,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,$A$ and $B$ are two $4\times4$ matrices such that $AB=0$. Why is $\operatorname{rank}(BA)<3$?,and  are two  matrices such that . Why is ?,A B 4\times4 AB=0 \operatorname{rank}(BA)<3,"Given two $4 \times 4$ matrices $A$ and $B$. Prove that if $AB=0$ then $\operatorname{rank}(BA)<3$, i.e. $\operatorname{rank}(BA)$ is $2$ or less. Now, i can definitely see the reason behind the conclusion, i tried many things to do in order to simplify AB its just too hard, so i tried to simplify the question. If $AB = 0$ then $BA$ will have at least 2 zero rows. From there i tried to get into conclusion that both matrixes must have at least 2 sets of 2 rows that they are must be similar (1 is multiplicity of the other) and its was a very long deduction. I am a little hopeless and need help, thanks.","Given two $4 \times 4$ matrices $A$ and $B$. Prove that if $AB=0$ then $\operatorname{rank}(BA)<3$, i.e. $\operatorname{rank}(BA)$ is $2$ or less. Now, i can definitely see the reason behind the conclusion, i tried many things to do in order to simplify AB its just too hard, so i tried to simplify the question. If $AB = 0$ then $BA$ will have at least 2 zero rows. From there i tried to get into conclusion that both matrixes must have at least 2 sets of 2 rows that they are must be similar (1 is multiplicity of the other) and its was a very long deduction. I am a little hopeless and need help, thanks.",,"['linear-algebra', 'matrices']"
1,expansion of logarithmic matrix,expansion of logarithmic matrix,,"Let $A,B$ be Hermitian matrices ($A$ has positive or zero eigenvalues and $Tr A=Tr[A+\lambda B]=1$), and $\lambda$ is infinitesimal constant. How to expand \begin{equation}   \ln(A+\lambda B) \end{equation} and  \begin{equation}   Tr[(A+\lambda B)\ln(A+\lambda B)] \end{equation} into powers of $\lambda$ as usually do in Taylor expansion of functions?","Let $A,B$ be Hermitian matrices ($A$ has positive or zero eigenvalues and $Tr A=Tr[A+\lambda B]=1$), and $\lambda$ is infinitesimal constant. How to expand \begin{equation}   \ln(A+\lambda B) \end{equation} and  \begin{equation}   Tr[(A+\lambda B)\ln(A+\lambda B)] \end{equation} into powers of $\lambda$ as usually do in Taylor expansion of functions?",,"['matrices', 'matrix-calculus']"
2,Trace $A$ is an integer.,Trace  is an integer.,A,If $A$ is a square matrix of order $n$ with real entries and $A^3 + I =0$. Then $\operatorname{trace}(A)$ is an integer. My attempt: Here $|A| = -1$. And $\operatorname{trace}(A^3) = -n$. Then I tried to draw a contradiction assuming that $\operatorname{trace}(A)$ isn't an integer (using definition of determinant). But nowhere near the solution.,If $A$ is a square matrix of order $n$ with real entries and $A^3 + I =0$. Then $\operatorname{trace}(A)$ is an integer. My attempt: Here $|A| = -1$. And $\operatorname{trace}(A^3) = -n$. Then I tried to draw a contradiction assuming that $\operatorname{trace}(A)$ isn't an integer (using definition of determinant). But nowhere near the solution.,,"['linear-algebra', 'matrices', 'matrix-equations']"
3,"How to construct $A$ and $b$, given the set of all solutions to $Ax=b$?","How to construct  and , given the set of all solutions to ?",A b Ax=b,"I want to find a matrix $A \in \mathbb{C}^{2x4}$ and    $b \in\mathbb{C}^{2}$ the solution of    $Ax=b$ is: $$L =  \left\{\pmatrix{1\\2\\0\\-1} + x_1\pmatrix{1\\-2\\2\\1} + x_2\pmatrix{2\\2\\-1\\1}\right\}$$ Therefore $\dim(A) = 4$, $\dim(\ker(A)) = 2$, $\dim(\operatorname{im}(A)) = 2$. $A$ and $b$ have the following format: $$ A=      \begin{pmatrix}         a_{11} & a_{12} & a_{13}  & a_{14}\\         a_{21} & a_{22} & a_{23}  & a_{24} \\         \end{pmatrix} b=      \pmatrix{b_1\\b_2} $$ My idea is to solve the following equation for A and b: $$A\,\pmatrix{1 + x_1 + 2x_2\\2 - 2x_1+ 2x_2\\2x_1 - x_2\\-1+x_1 + x_2} = b$$ Is this the right way to start? I get then a linear equation with 12 unknowns and only 2 equations. I know that there must be many solutions. Do I simply define some of them as 1 or 0?","I want to find a matrix $A \in \mathbb{C}^{2x4}$ and    $b \in\mathbb{C}^{2}$ the solution of    $Ax=b$ is: $$L =  \left\{\pmatrix{1\\2\\0\\-1} + x_1\pmatrix{1\\-2\\2\\1} + x_2\pmatrix{2\\2\\-1\\1}\right\}$$ Therefore $\dim(A) = 4$, $\dim(\ker(A)) = 2$, $\dim(\operatorname{im}(A)) = 2$. $A$ and $b$ have the following format: $$ A=      \begin{pmatrix}         a_{11} & a_{12} & a_{13}  & a_{14}\\         a_{21} & a_{22} & a_{23}  & a_{24} \\         \end{pmatrix} b=      \pmatrix{b_1\\b_2} $$ My idea is to solve the following equation for A and b: $$A\,\pmatrix{1 + x_1 + 2x_2\\2 - 2x_1+ 2x_2\\2x_1 - x_2\\-1+x_1 + x_2} = b$$ Is this the right way to start? I get then a linear equation with 12 unknowns and only 2 equations. I know that there must be many solutions. Do I simply define some of them as 1 or 0?",,"['linear-algebra', 'matrices', 'systems-of-equations', 'matrix-equations']"
4,"How to construct symmetric and positive definite $A,B,C$ such that $A+B+C=I$?",How to construct symmetric and positive definite  such that ?,"A,B,C A+B+C=I","In an attempt to formulate a answer to this (in)famous question How does one prove the matrix inequality $\det\left(6(A^3+B^3+C^3)+I_{n}\right)\ge 5^n\det(A^2+B^2+C^2)$? I'm trying to construct three $n\times n$ matrices $A,B,C$ that are (a) symmetric, (b) positive definite, (c) add to $I_n$ . Note that I've already decided to restrict attention to the reals and I have replaced Hermitian by symmetric (which IMO is difficult enough). My unsuccessful tries are a wild mixture of two extremes: Make a random square matrix $H$ and form $A = H^TH$ . Make another random square matrix $H$ and form $B = H^TH$ . In the same way, form $C = H^TH$ . Then $A,B,C$ are symmetric and positive definite. But in general $A+B+C \ne I$ . Generate random numbers for $A_{ij} = A_{ji}$ , $B_{ij} = B_{ji}$ and form $C_{ij} = C_{ji} = I_{ij}-A_{ij}-B_{ij}$ . Then $A,B,C$ are symmetric and $A+B+C = I$ , but it cannot be guaranteed that these are positive definite matrices. So the question is: how can the three requirements (a) , (b) , (c) be fulfilled at the same time, while keeping $A,B,C$ yet as random as possible? My plan is to do numerical experiments and eventually find a counter example. I have all the ingredients to do it, except this.","In an attempt to formulate a answer to this (in)famous question How does one prove the matrix inequality $\det\left(6(A^3+B^3+C^3)+I_{n}\right)\ge 5^n\det(A^2+B^2+C^2)$? I'm trying to construct three $n\times n$ matrices $A,B,C$ that are (a) symmetric, (b) positive definite, (c) add to $I_n$ . Note that I've already decided to restrict attention to the reals and I have replaced Hermitian by symmetric (which IMO is difficult enough). My unsuccessful tries are a wild mixture of two extremes: Make a random square matrix $H$ and form $A = H^TH$ . Make another random square matrix $H$ and form $B = H^TH$ . In the same way, form $C = H^TH$ . Then $A,B,C$ are symmetric and positive definite. But in general $A+B+C \ne I$ . Generate random numbers for $A_{ij} = A_{ji}$ , $B_{ij} = B_{ji}$ and form $C_{ij} = C_{ji} = I_{ij}-A_{ij}-B_{ij}$ . Then $A,B,C$ are symmetric and $A+B+C = I$ , but it cannot be guaranteed that these are positive definite matrices. So the question is: how can the three requirements (a) , (b) , (c) be fulfilled at the same time, while keeping $A,B,C$ yet as random as possible? My plan is to do numerical experiments and eventually find a counter example. I have all the ingredients to do it, except this.",,"['linear-algebra', 'matrices', 'inequality']"
5,Find the rational canonical form of $A=\left(\begin{smallmatrix} -1 & -2 & 6 \\ -1 & 0 & 3\\ -1 & -1 &4 \end{smallmatrix}\right)$,Find the rational canonical form of,A=\left(\begin{smallmatrix} -1 & -2 & 6 \\ -1 & 0 & 3\\ -1 & -1 &4 \end{smallmatrix}\right),"Attempt First, the characteristic polynomial of $A$ is $\chi_A=(t-1)^3$ and the minimum polynomial is $\mu_A=(t-1)^2$. Since the degree of the minimum polynomial is $2$, the dimension of the largest cyclic subspace cannot exceed 2. Now if $e_1=(1,0,0)^T$  then the cyclic subspace $C_{e_1}$ generated by $e_1$ is $$\mathrm{span}\{e_1, Ae_1\}.$$ Now the space $V$ can be decomposed into a direct sum of cyclic subspaces. so $$V=C_{e_1} \oplus C_x$$ for some $x$. we note that since $V=\mathbb{R}^2$, $C_x$ must be $1$ dimensional. So the minimum polynomial of $A$ restricted to $C_x$ is 1 dimensional. Since this polynomial divides $\mu_A$, we find that the minimum polynomial is $t-1$.  Thus there are two companion matrices, one corresponding to $(t-1)^2$ and one corresponding to $(t-1)$. Thus $$R=\begin{pmatrix} 0 & -1&0\\ 1 & 2 & 0\\ 0 & 0 & 1\end{pmatrix}.$$ Questions and concerns I want to make sure my reasoning is correct. Throughout the above argument I am implicitly using the assumption that the minimum polynomial of a cyclic subspace of dimension $n$ has degree $n$. Is this correct? In general, can you tell me properties of the rational canonical form of a matrix similar to the first answer here for Jordan canonical form that may help calculate the rational canonical form for matrices of small size?","Attempt First, the characteristic polynomial of $A$ is $\chi_A=(t-1)^3$ and the minimum polynomial is $\mu_A=(t-1)^2$. Since the degree of the minimum polynomial is $2$, the dimension of the largest cyclic subspace cannot exceed 2. Now if $e_1=(1,0,0)^T$  then the cyclic subspace $C_{e_1}$ generated by $e_1$ is $$\mathrm{span}\{e_1, Ae_1\}.$$ Now the space $V$ can be decomposed into a direct sum of cyclic subspaces. so $$V=C_{e_1} \oplus C_x$$ for some $x$. we note that since $V=\mathbb{R}^2$, $C_x$ must be $1$ dimensional. So the minimum polynomial of $A$ restricted to $C_x$ is 1 dimensional. Since this polynomial divides $\mu_A$, we find that the minimum polynomial is $t-1$.  Thus there are two companion matrices, one corresponding to $(t-1)^2$ and one corresponding to $(t-1)$. Thus $$R=\begin{pmatrix} 0 & -1&0\\ 1 & 2 & 0\\ 0 & 0 & 1\end{pmatrix}.$$ Questions and concerns I want to make sure my reasoning is correct. Throughout the above argument I am implicitly using the assumption that the minimum polynomial of a cyclic subspace of dimension $n$ has degree $n$. Is this correct? In general, can you tell me properties of the rational canonical form of a matrix similar to the first answer here for Jordan canonical form that may help calculate the rational canonical form for matrices of small size?",,"['linear-algebra', 'matrices', 'minimal-polynomials']"
6,Eigenvalue decomposition of $A = I - xx^T$ [duplicate],Eigenvalue decomposition of  [duplicate],A = I - xx^T,"This question already has answers here : Determinant of a rank $1$ update of a scalar matrix, or characteristic polynomial of a rank $1$ matrix (2 answers) Closed 7 years ago . Let $A = I - xx^T$, where $x \in \mathbb{R}^n$ and $I$ is the identity matrix of $\mathbb{R}^n$ We know that $A$ is a real symmetric matrix, therefore there exists an eigenvalue decomposition of $A$ such that $$A = Q^T\Lambda Q$$ Is it possible to find $Q$, $\Lambda$? $I - xx^T = Q^TQ - xQ^TQx^T = Q^TQ - (Q^Tx)^T(x^TQ)^T...$","This question already has answers here : Determinant of a rank $1$ update of a scalar matrix, or characteristic polynomial of a rank $1$ matrix (2 answers) Closed 7 years ago . Let $A = I - xx^T$, where $x \in \mathbb{R}^n$ and $I$ is the identity matrix of $\mathbb{R}^n$ We know that $A$ is a real symmetric matrix, therefore there exists an eigenvalue decomposition of $A$ such that $$A = Q^T\Lambda Q$$ Is it possible to find $Q$, $\Lambda$? $I - xx^T = Q^TQ - xQ^TQx^T = Q^TQ - (Q^Tx)^T(x^TQ)^T...$",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'matrix-decomposition']"
7,"If the matrices $A^3 = 0$, $B^3=0$ and $AB=BA$ then show this:","If the matrices ,  and  then show this:",A^3 = 0 B^3=0 AB=BA,"The question: If $A$ and $B$ are square matrices of the same type such that $A^3=0$, $B^3=0$ and $AB=BA$. Show that $$\left(I+A+\frac{A^2}{2!}\right)\left(I+B+\frac{B^2}{2!}\right)=I+(A+B)+\frac{(A+B)^2}{2!}+\frac{(A+B)^3}{3!}+\frac{(A+B)^4}{4!}$$ This is how I started.  $$\left(I+A+\frac{A^2}{2!}\right)\left(I+B+\frac{B^2}{2!}\right)=I+(A+B)+\frac{A^2+B^2}{2!}+AB+\frac{AB^2}{2!}+\frac{A^2B}{2!}+\frac{A^2B^2}{2!2!}$$  I tried to get  $$\frac{A^2+B^2}{2!}+AB+\frac{AB^2}{2!}+\frac{A^2B}{2!}+\frac{A^2B^2}{2!2!}$$ to be equal to $$\frac{(A+B)^2}{2!}+\frac{(A+B)^3}{3!}+\frac{(A+B)^4}{4!}$$ But I'm not getting there.","The question: If $A$ and $B$ are square matrices of the same type such that $A^3=0$, $B^3=0$ and $AB=BA$. Show that $$\left(I+A+\frac{A^2}{2!}\right)\left(I+B+\frac{B^2}{2!}\right)=I+(A+B)+\frac{(A+B)^2}{2!}+\frac{(A+B)^3}{3!}+\frac{(A+B)^4}{4!}$$ This is how I started.  $$\left(I+A+\frac{A^2}{2!}\right)\left(I+B+\frac{B^2}{2!}\right)=I+(A+B)+\frac{A^2+B^2}{2!}+AB+\frac{AB^2}{2!}+\frac{A^2B}{2!}+\frac{A^2B^2}{2!2!}$$  I tried to get  $$\frac{A^2+B^2}{2!}+AB+\frac{AB^2}{2!}+\frac{A^2B}{2!}+\frac{A^2B^2}{2!2!}$$ to be equal to $$\frac{(A+B)^2}{2!}+\frac{(A+B)^3}{3!}+\frac{(A+B)^4}{4!}$$ But I'm not getting there.",,"['linear-algebra', 'matrices', 'matrix-equations']"
8,Recover vector $x$ from rank-$1$ matrix $Q=xx^H$,Recover vector  from rank- matrix,x 1 Q=xx^H,"Let the matrix $Q \in\mathbb{C}^{n \times n}$ be known. It is also known that $Q=xx^H$ , where $x=[x_1,\ldots,x_n]^T$ and $x^H$ is its conjugate transpose. What is $x$ ? How to recover it?","Let the matrix be known. It is also known that , where and is its conjugate transpose. What is ? How to recover it?","Q \in\mathbb{C}^{n \times n} Q=xx^H x=[x_1,\ldots,x_n]^T x^H x","['linear-algebra', 'matrices', 'matrix-decomposition', 'rank-1-matrices']"
9,The largest field contained in $GL_2(\mathbb{R})$,The largest field contained in,GL_2(\mathbb{R}),"It seems pretty intuitive that the complex numbers can be represented as elements in $GL_2(\mathbb{R})$ by identifying $$ a+bi \equiv \begin{bmatrix} a & -b\\b & a \end{bmatrix}$$ And additionally including the matrix $\begin{bmatrix} 0&0\\0&0 \end{bmatrix} $ since it is not invertible. So it appears that this set in fact forms a field under matrix addition and matrix multiplication. My question is, can we say that this is the largest field we can obtain out of elements of $GL_2(\mathbb{R}) \cup \{0\} $? If yes, would there be some extension of such a procedure to higher dimensions for say $GL_n(\mathbb{R})$ in which we can get fields out of $\mathbb{R}$-vector spaces? Or can it be shown that it is not possible? Some initial thoughts on the case for $GL_2(\mathbb{R})$: I can see that $GL_2(\mathbb{R})$ itself does not form a field since it is not closed under addition and additional constraints would be needed to ensure $det(A + B)$ is non-zero given $det(A)$ and $det(B)$ are non-zero.  The condition that: $$det (A_1 + A_2) = det(\begin{bmatrix} a_1&b_1\\c_1&d_1 \end{bmatrix} + \begin{bmatrix} a_2&b_2\\c_2&d_2 \end{bmatrix}) \neq 0$$ Seems to give me: $$\det(A_1) + det(A_2) \neq -(a_1d_2 - b_1c_2 + a_2d_1 - b_2c_1)$$ Which seems like a pretty odd condition since it would have to be true regardless of choice of $A_1$ and $A_2$. Not sure if this is the right approach... Maybe we could obtain a lower bound on $det(A_1 + A_2)$ in terms of $det(A_1)$ and $det(A_2)$ by restricting ourselves to matrices of positive determinant along with some extra condition. Clearly skew symmetric matrices with identical diagonal entries works, so maybe positive entries on the diagonal plus skew-symmetric is a step forward?","It seems pretty intuitive that the complex numbers can be represented as elements in $GL_2(\mathbb{R})$ by identifying $$ a+bi \equiv \begin{bmatrix} a & -b\\b & a \end{bmatrix}$$ And additionally including the matrix $\begin{bmatrix} 0&0\\0&0 \end{bmatrix} $ since it is not invertible. So it appears that this set in fact forms a field under matrix addition and matrix multiplication. My question is, can we say that this is the largest field we can obtain out of elements of $GL_2(\mathbb{R}) \cup \{0\} $? If yes, would there be some extension of such a procedure to higher dimensions for say $GL_n(\mathbb{R})$ in which we can get fields out of $\mathbb{R}$-vector spaces? Or can it be shown that it is not possible? Some initial thoughts on the case for $GL_2(\mathbb{R})$: I can see that $GL_2(\mathbb{R})$ itself does not form a field since it is not closed under addition and additional constraints would be needed to ensure $det(A + B)$ is non-zero given $det(A)$ and $det(B)$ are non-zero.  The condition that: $$det (A_1 + A_2) = det(\begin{bmatrix} a_1&b_1\\c_1&d_1 \end{bmatrix} + \begin{bmatrix} a_2&b_2\\c_2&d_2 \end{bmatrix}) \neq 0$$ Seems to give me: $$\det(A_1) + det(A_2) \neq -(a_1d_2 - b_1c_2 + a_2d_1 - b_2c_1)$$ Which seems like a pretty odd condition since it would have to be true regardless of choice of $A_1$ and $A_2$. Not sure if this is the right approach... Maybe we could obtain a lower bound on $det(A_1 + A_2)$ in terms of $det(A_1)$ and $det(A_2)$ by restricting ourselves to matrices of positive determinant along with some extra condition. Clearly skew symmetric matrices with identical diagonal entries works, so maybe positive entries on the diagonal plus skew-symmetric is a step forward?",,"['linear-algebra', 'abstract-algebra', 'matrices', 'field-theory']"
10,How are matrices related to vectors?,How are matrices related to vectors?,,"I know it's a silly question, but there's something I feel like I'm missing in my understanding of matrices. I'm studying linear algebra, and much of what we covered in the first few topics related to vectors (vector spaces, linear independence, etc.), but then all of a sudden we started using matrices, loosely defining them as an ""array of numbers"". So I'm kind of confused, is a matrix supposed to be a list of vectors? And if so, are they the rows or the columns of the matrix?","I know it's a silly question, but there's something I feel like I'm missing in my understanding of matrices. I'm studying linear algebra, and much of what we covered in the first few topics related to vectors (vector spaces, linear independence, etc.), but then all of a sudden we started using matrices, loosely defining them as an ""array of numbers"". So I'm kind of confused, is a matrix supposed to be a list of vectors? And if so, are they the rows or the columns of the matrix?",,"['linear-algebra', 'matrices']"
11,Linear Algebra used to solve hotel room light switch question,Linear Algebra used to solve hotel room light switch question,,"There is a special suite of rooms designed with light switches in an odd way.  Our goal is to turn off all of the the lights. There are 25 rooms in the suite arranged in a square of five by five rooms: 1  2  3  4  5   6  7  8  9  10  21 22 13 14 15  16 17 18 19 20  21 22 23 24 25 If you switch the light switch in any room, it toggles the lights in the adjacent room.  For example, -assuming all of the lights are on and you switch the switch in room 1, the lights in room 2 and 6 are turned off and there is no other change in the status of the lights. -or assuming all of the light are on and you switch the switch in room 18, the lights in room 13, 17, 19, 23 are all turned off and there is no other change in the status of the lights. Assuming all of the lights are on, please advise on how to turn off all of the lights.  You can use a web app for row reduction. Assuming all of the lights in even number rooms are on, please advise on how to turn off all of the lights. After row reduction, we get a free column where, $$x_4 =  t$$ $$x_1 = -2 + t$$ $$x_2 =  2 - t$$ $$x_3 =  1 - t$$ Can anyone help out with the rest?","There is a special suite of rooms designed with light switches in an odd way.  Our goal is to turn off all of the the lights. There are 25 rooms in the suite arranged in a square of five by five rooms: 1  2  3  4  5   6  7  8  9  10  21 22 13 14 15  16 17 18 19 20  21 22 23 24 25 If you switch the light switch in any room, it toggles the lights in the adjacent room.  For example, -assuming all of the lights are on and you switch the switch in room 1, the lights in room 2 and 6 are turned off and there is no other change in the status of the lights. -or assuming all of the light are on and you switch the switch in room 18, the lights in room 13, 17, 19, 23 are all turned off and there is no other change in the status of the lights. Assuming all of the lights are on, please advise on how to turn off all of the lights.  You can use a web app for row reduction. Assuming all of the lights in even number rooms are on, please advise on how to turn off all of the lights. After row reduction, we get a free column where, $$x_4 =  t$$ $$x_1 = -2 + t$$ $$x_2 =  2 - t$$ $$x_3 =  1 - t$$ Can anyone help out with the rest?",,"['linear-algebra', 'matrices']"
12,Lower bounding the trace of $A^2$ using the trace of $A^T A$,Lower bounding the trace of  using the trace of,A^2 A^T A,"$\DeclareMathOperator{\tr}{tr}$For a real, square matrix $A$, I believe that one has a simple upper bound on the (absolute value of the) trace of its square in terms of the trace of its Gramian-type product, namely \begin{align} \left|\tr{A^2}\right| &= \left|\sum_i \lambda_i(A^2)\right|\\ &\leq \sum_i \left|\lambda_i(A^2)\right|\\ &\leq \sum_i \sigma_i (A^2) = \tr\left(A^T A\right), \end{align} where $\lambda_i(\,\cdot\,)$ and $\sigma_i(\,\cdot\,)$ denote the $i$th largest eigenvalue and singular value, respectively. [The equality at the end was obtained by noting that the singular values of $A^2$ are equal to the eigenvalues of $((A^2)^TA^2)^{1/2} = A^T A$.] My question is, are there any methods or theorems to establish a lower bound on $\tr{A^2}$ in terms of $\tr\left(A^T A\right)$ (potentially with constants, factors of its dimension/rank, or square roots)? (In particular, I am dealing with a class of matrices that satisfy $\tr{A^2} = \left(\tr{A}\right)^2$, and where $\tr{A}$ is strictly real, so I am equally interested in bounds on simply $\tr{A}$, if an inequality exists for that as well.)","$\DeclareMathOperator{\tr}{tr}$For a real, square matrix $A$, I believe that one has a simple upper bound on the (absolute value of the) trace of its square in terms of the trace of its Gramian-type product, namely \begin{align} \left|\tr{A^2}\right| &= \left|\sum_i \lambda_i(A^2)\right|\\ &\leq \sum_i \left|\lambda_i(A^2)\right|\\ &\leq \sum_i \sigma_i (A^2) = \tr\left(A^T A\right), \end{align} where $\lambda_i(\,\cdot\,)$ and $\sigma_i(\,\cdot\,)$ denote the $i$th largest eigenvalue and singular value, respectively. [The equality at the end was obtained by noting that the singular values of $A^2$ are equal to the eigenvalues of $((A^2)^TA^2)^{1/2} = A^T A$.] My question is, are there any methods or theorems to establish a lower bound on $\tr{A^2}$ in terms of $\tr\left(A^T A\right)$ (potentially with constants, factors of its dimension/rank, or square roots)? (In particular, I am dealing with a class of matrices that satisfy $\tr{A^2} = \left(\tr{A}\right)^2$, and where $\tr{A}$ is strictly real, so I am equally interested in bounds on simply $\tr{A}$, if an inequality exists for that as well.)",,"['linear-algebra', 'matrices', 'upper-lower-bounds', 'trace']"
13,"When a Markov chain converges to a steady state, what kind of convergence is it?","When a Markov chain converges to a steady state, what kind of convergence is it?",,"Let $A$ be a transition matrix, the steady state distribution $x$ satisfies the distribution $Ax = x$. One can prove that under certain circumstances, $$\lim_{n\rightarrow\infty}A^n q=x$$ where $q$ is a probability distribution for the states in Markov chain.  What kind of convergence is this? Is this convergence almost surely, in probability, or in distribution?","Let $A$ be a transition matrix, the steady state distribution $x$ satisfies the distribution $Ax = x$. One can prove that under certain circumstances, $$\lim_{n\rightarrow\infty}A^n q=x$$ where $q$ is a probability distribution for the states in Markov chain.  What kind of convergence is this? Is this convergence almost surely, in probability, or in distribution?",,"['matrices', 'probability-theory', 'convergence-divergence', 'markov-chains']"
14,Imaginary eigenvalues,Imaginary eigenvalues,,I'm still developing my skills in LinearAlgebra and I ponder just what are the main differences between effects of imaginary and real eigenvalues on linear operations specially taking into account their geometric interpretation ? Is there somewhere the list of these differences? Is it true that if we have imaginary eigenvalues then it is necessary for some  subspace of a space generated by a matrix $A$  that we have no preserved directions of vectors in this subspace as for example it is in the case of 2D i 3D rotations? If so how to apply these imaginary eigenvalues for generating this subspace? (in the case of rotations to generate the plane),I'm still developing my skills in LinearAlgebra and I ponder just what are the main differences between effects of imaginary and real eigenvalues on linear operations specially taking into account their geometric interpretation ? Is there somewhere the list of these differences? Is it true that if we have imaginary eigenvalues then it is necessary for some  subspace of a space generated by a matrix $A$  that we have no preserved directions of vectors in this subspace as for example it is in the case of 2D i 3D rotations? If so how to apply these imaginary eigenvalues for generating this subspace? (in the case of rotations to generate the plane),,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
15,minimum number of dependent rows in a matrix,minimum number of dependent rows in a matrix,,"Does the minimum number of dependent rows in a matrix have a specific name? (the way ""rank"" refers to the maximum number of independent rows). This comes up in calculating distances of codes. There are plenty of algorithms to calculate rank; are there any for this minimum other than brute force? Any reference to or description of the algorithm are appreciated; same for any sw package that might have that implemented.","Does the minimum number of dependent rows in a matrix have a specific name? (the way ""rank"" refers to the maximum number of independent rows). This comes up in calculating distances of codes. There are plenty of algorithms to calculate rank; are there any for this minimum other than brute force? Any reference to or description of the algorithm are appreciated; same for any sw package that might have that implemented.",,"['linear-algebra', 'matrices', 'coding-theory']"
16,"Computing the matrix of $Tp(x) = p'(x) + x^2 p''(x)$ relative to the basis $\{1, x, x^2\}$",Computing the matrix of  relative to the basis,"Tp(x) = p'(x) + x^2 p''(x) \{1, x, x^2\}","Show that the operator $T \colon P_2(\mathbb{R}) \to P_2(\mathbb{R})$ given by   $$   Tp(x) = p'(x) + x^2 p''(x) $$   is a linear operator. Compute the matrix $[T]_{B,B}$ of $T$ relative to the standard basis $B = \{1, x, x^2\}$ for $P_2(\mathbb{R})$. So I know that a polynomial in $P_2(\mathbb{R})$ is of the form $ax^2 + bx + c$, and $P'(x) = 2ax + b$, $P''(x) = 2a$. So $Tp(x) = 2ax+b + 2ax^2 = b + 2ax + 2ax^2$. But I am having trouble expressing it in terms of a matrix. Just to make sure I understand it, I have tried the question with a new basis $B=(1,x,x^2+x+\frac{1}{2}$) and I get the matrix: $$         \begin{pmatrix}         0 & 1 & 0 \\         0 & 0 & 0 \\         0 & 0 & 2 \\         \end{pmatrix} $$ Applying the operator $x^2+x+\frac{1}{2}$ gives me $2x^2+2x+1$, which is 2 lots of the basis. Comments: -A linear operator is always unique. -Since the only basis that changed was the last one, that means I can borrow the first 2 columns from my first matrix.","Show that the operator $T \colon P_2(\mathbb{R}) \to P_2(\mathbb{R})$ given by   $$   Tp(x) = p'(x) + x^2 p''(x) $$   is a linear operator. Compute the matrix $[T]_{B,B}$ of $T$ relative to the standard basis $B = \{1, x, x^2\}$ for $P_2(\mathbb{R})$. So I know that a polynomial in $P_2(\mathbb{R})$ is of the form $ax^2 + bx + c$, and $P'(x) = 2ax + b$, $P''(x) = 2a$. So $Tp(x) = 2ax+b + 2ax^2 = b + 2ax + 2ax^2$. But I am having trouble expressing it in terms of a matrix. Just to make sure I understand it, I have tried the question with a new basis $B=(1,x,x^2+x+\frac{1}{2}$) and I get the matrix: $$         \begin{pmatrix}         0 & 1 & 0 \\         0 & 0 & 0 \\         0 & 0 & 2 \\         \end{pmatrix} $$ Applying the operator $x^2+x+\frac{1}{2}$ gives me $2x^2+2x+1$, which is 2 lots of the basis. Comments: -A linear operator is always unique. -Since the only basis that changed was the last one, that means I can borrow the first 2 columns from my first matrix.",,"['linear-algebra', 'matrices']"
17,"If the determinant of a matrix goes to infinity, does it means it has no inverse?","If the determinant of a matrix goes to infinity, does it means it has no inverse?",,"Context I have a linear time-invariant (single-input, single-output) system in state space representation ( https://en.wikipedia.org/wiki/State-space_representation#Linear_systems ): $$ \mathbf{x'}(t) = \mathbf{A}*\mathbf{x}(t) + \mathbf{B}*{u(t)}$$ In which: $\mathbf{x}(t)$ is a $n$-order vector of the variables of the system; the output $\mathbf{y}(t)$ is the last variable of $\mathbf{x}(t)$, i.e., $\mathbf{x}_{n} = \mathbf{y}$; $\mathbf{x'}(t)$ is a $n$-order vector of first order derivatives; the $n$ x $n$ square matrix $\mathbf{A}$ is sparse; it's non-zero elements are in the: first column, first line and main diagonal; $$ \mathbf{A} =         \begin{bmatrix}         {a}_{11} & {a}_{12} & {a}_{13} & \cdots & {a}_{1n} \\         {a}_{21} & {a}_{22} & 0 & \cdots & 0 \\         {a}_{31} & 0 & {a}_{33} & \cdots & 0 \\         \vdots & \vdots & \vdots & \ddots & \vdots \\        {a}_{n1} & 0 & 0 & \cdots & {a}_{nn} \\         \end{bmatrix} $$ the input $u(t)$ is a scalar; $\mathbf{B}$ is a sparse $n$-order vector which the only non-zero element is the first. To solve the state-space, I thought of doing integration, line by line: $$\mathbf{x}(t) = {e}^{\mathbf{A}t}\mathbf{x}(0) + \int_{0}^{t}{e}^{\mathbf{A}(t-\tau)}\mathbf{B}u(\tau)d\tau $$ That would be very simple if ${\mathbf{A}}$ was a diagonal matrix. Because if it is, then so will be ${e}^{\mathbf{A}t}$; and every element of it will be ${e}^{{a}_{ii}t}$. Where ${a}_{ii}$ is an element of ${\mathbf{A}}$. The question arises So my question arised when I tried to diagonalize ${\mathbf{A}}$... For ${\mathbf{A}}$ to be diagonalizable, it must be invertible, i.e., $$det({\mathbf{A}}) \neq 0$$ I went to check the determinant of ${\mathbf{A}}$, and I got:  $${det(\mathbf{A})}=1.96*{10}^{16}$$ So, I am wondering: is ${\mathbf{A}}$ invertible? Can I diagonalize it? Why do I ask? Suppose that the determinant of ${\mathbf{A}}$ goes to infinity. Then the determinant of it's inverse will go to zero. As if: $$det({\mathbf{A}}) = \infty $$ then $$ det({\mathbf{A}^{-1}}) = 0 $$ Therefore, the inverse of ${\mathbf{A}}$ is not invertible (This apparent contradiction is what is bugging me). A note: Because the determinant was so high, the inverse will have it's elements with near-zero values. This will certainly cause numerical errors.","Context I have a linear time-invariant (single-input, single-output) system in state space representation ( https://en.wikipedia.org/wiki/State-space_representation#Linear_systems ): $$ \mathbf{x'}(t) = \mathbf{A}*\mathbf{x}(t) + \mathbf{B}*{u(t)}$$ In which: $\mathbf{x}(t)$ is a $n$-order vector of the variables of the system; the output $\mathbf{y}(t)$ is the last variable of $\mathbf{x}(t)$, i.e., $\mathbf{x}_{n} = \mathbf{y}$; $\mathbf{x'}(t)$ is a $n$-order vector of first order derivatives; the $n$ x $n$ square matrix $\mathbf{A}$ is sparse; it's non-zero elements are in the: first column, first line and main diagonal; $$ \mathbf{A} =         \begin{bmatrix}         {a}_{11} & {a}_{12} & {a}_{13} & \cdots & {a}_{1n} \\         {a}_{21} & {a}_{22} & 0 & \cdots & 0 \\         {a}_{31} & 0 & {a}_{33} & \cdots & 0 \\         \vdots & \vdots & \vdots & \ddots & \vdots \\        {a}_{n1} & 0 & 0 & \cdots & {a}_{nn} \\         \end{bmatrix} $$ the input $u(t)$ is a scalar; $\mathbf{B}$ is a sparse $n$-order vector which the only non-zero element is the first. To solve the state-space, I thought of doing integration, line by line: $$\mathbf{x}(t) = {e}^{\mathbf{A}t}\mathbf{x}(0) + \int_{0}^{t}{e}^{\mathbf{A}(t-\tau)}\mathbf{B}u(\tau)d\tau $$ That would be very simple if ${\mathbf{A}}$ was a diagonal matrix. Because if it is, then so will be ${e}^{\mathbf{A}t}$; and every element of it will be ${e}^{{a}_{ii}t}$. Where ${a}_{ii}$ is an element of ${\mathbf{A}}$. The question arises So my question arised when I tried to diagonalize ${\mathbf{A}}$... For ${\mathbf{A}}$ to be diagonalizable, it must be invertible, i.e., $$det({\mathbf{A}}) \neq 0$$ I went to check the determinant of ${\mathbf{A}}$, and I got:  $${det(\mathbf{A})}=1.96*{10}^{16}$$ So, I am wondering: is ${\mathbf{A}}$ invertible? Can I diagonalize it? Why do I ask? Suppose that the determinant of ${\mathbf{A}}$ goes to infinity. Then the determinant of it's inverse will go to zero. As if: $$det({\mathbf{A}}) = \infty $$ then $$ det({\mathbf{A}^{-1}}) = 0 $$ Therefore, the inverse of ${\mathbf{A}}$ is not invertible (This apparent contradiction is what is bugging me). A note: Because the determinant was so high, the inverse will have it's elements with near-zero values. This will certainly cause numerical errors.",,"['linear-algebra', 'matrices', 'determinant', 'numerical-linear-algebra']"
18,Why matrices are multiplied the way they are multiplied?,Why matrices are multiplied the way they are multiplied?,,"I couldn't see a specific reason for multiplying every row of A with every column of B. Is this an arbitrary property of multiplication function of matrices? Instead, why don't we simply multiply row#1 of matrix A with row#1 of matrix B, which would help us to do multiplication easier without all that confusion of what we are gonna multiply with what? In this case, product of A(mxn) and B(kxn) would be P(mxk). I know a lot of things would change in today's mathematics if we define multiplication this way. But would that approach cause any problem in the future?","I couldn't see a specific reason for multiplying every row of A with every column of B. Is this an arbitrary property of multiplication function of matrices? Instead, why don't we simply multiply row#1 of matrix A with row#1 of matrix B, which would help us to do multiplication easier without all that confusion of what we are gonna multiply with what? In this case, product of A(mxn) and B(kxn) would be P(mxk). I know a lot of things would change in today's mathematics if we define multiplication this way. But would that approach cause any problem in the future?",,[]
19,Determinant of a matrix will all off-diagonal elements equal,Determinant of a matrix will all off-diagonal elements equal,,Consider matrices of the following form: \begin{bmatrix} a & x & \cdots \\  x & b & x &\cdots \\ \vdots & x & c &\cdots \\   & \vdots & x & \ddots & \\ \end{bmatrix} where all $x$ are the same scalar. Is there anything that can be done to calculate the determinant easily? Obviously if $$x=a=b=c=\cdots $$ then the matrix is singular and the determinant is zero. Assume this is not the case. Do they have any other interesting properties?,Consider matrices of the following form: \begin{bmatrix} a & x & \cdots \\  x & b & x &\cdots \\ \vdots & x & c &\cdots \\   & \vdots & x & \ddots & \\ \end{bmatrix} where all $x$ are the same scalar. Is there anything that can be done to calculate the determinant easily? Obviously if $$x=a=b=c=\cdots $$ then the matrix is singular and the determinant is zero. Assume this is not the case. Do they have any other interesting properties?,,"['linear-algebra', 'matrices']"
20,An inequality on the rank of a block matrix,An inequality on the rank of a block matrix,,"Let $\mathbb F$ be a field, and let $r_1, r_2, s_1, s_2$ be positive integers. Consider the matrix $$X:=\left[\begin{array}{cc}  A & B \\  C & D  \end{array}  \right],$$ where $A \in \mathbb F^{r_1 \times s_1}$, $B\in \mathbb F^{r_1\times s_2}$, $C \in \mathbb F^{r_2\times s_1}$ and $D \in \mathbb F^{r_2\times s_2}$. Q1: Is the following inequality true?    $$rank [X] \geq rank \left[\begin{array}{c}  B \\  D  \end{array}  \right]+\max \left\{rank[A\;\; B]- rank[B], rank[C\;\;D]-rank[D] \right\}$$  If so, when does the equality hold? Q2: Is there some nice formula for $rank[X]$ depending only on the blocks? Thanks. Alessandro Edit: The answer to the first question is YES. Is there an elementary proof of it? I can prove it using using results on completions of partial matrices, but I believe there should be an easy way to prove it. For the equality I suspect that it holds if and only if $$rank \left[\begin{array}{c}  A \\  C  \end{array}  \right]=\max \left\{rank[A],rank[C] \right\},$$ but I'm not sure.","Let $\mathbb F$ be a field, and let $r_1, r_2, s_1, s_2$ be positive integers. Consider the matrix $$X:=\left[\begin{array}{cc}  A & B \\  C & D  \end{array}  \right],$$ where $A \in \mathbb F^{r_1 \times s_1}$, $B\in \mathbb F^{r_1\times s_2}$, $C \in \mathbb F^{r_2\times s_1}$ and $D \in \mathbb F^{r_2\times s_2}$. Q1: Is the following inequality true?    $$rank [X] \geq rank \left[\begin{array}{c}  B \\  D  \end{array}  \right]+\max \left\{rank[A\;\; B]- rank[B], rank[C\;\;D]-rank[D] \right\}$$  If so, when does the equality hold? Q2: Is there some nice formula for $rank[X]$ depending only on the blocks? Thanks. Alessandro Edit: The answer to the first question is YES. Is there an elementary proof of it? I can prove it using using results on completions of partial matrices, but I believe there should be an easy way to prove it. For the equality I suspect that it holds if and only if $$rank \left[\begin{array}{c}  A \\  C  \end{array}  \right]=\max \left\{rank[A],rank[C] \right\},$$ but I'm not sure.",,"['linear-algebra', 'matrices', 'inequality', 'finite-fields', 'matrix-rank']"
21,To obtain the condition for vanishing of the given determinant,To obtain the condition for vanishing of the given determinant,,"If $a,b,c$ are distinct real numbers obtain the condition under which the following determinant vanishes.  $$\left| \begin{array}{cc}  a & a^2 & 1+a^3\\ b & b^2 & 1+b^3\\ c & c^2 & 1+c^3\\ \end{array} \right|$$ My answer: After a little calculation I was able to show that $D=0$ reduces to $abc=-1$. Is there a simpler one line answer to this? especially since this matrix looks suspiciously similar to the Vandermonde matrix?","If $a,b,c$ are distinct real numbers obtain the condition under which the following determinant vanishes.  $$\left| \begin{array}{cc}  a & a^2 & 1+a^3\\ b & b^2 & 1+b^3\\ c & c^2 & 1+c^3\\ \end{array} \right|$$ My answer: After a little calculation I was able to show that $D=0$ reduces to $abc=-1$. Is there a simpler one line answer to this? especially since this matrix looks suspiciously similar to the Vandermonde matrix?",,"['real-analysis', 'linear-algebra', 'matrices', 'determinant']"
22,"If $A$ and $B$ are similar, can the same be said of $A^t$ and $B^t$? [duplicate]","If  and  are similar, can the same be said of  and ? [duplicate]",A B A^t B^t,"This question already has answers here : A and B is similar ⇒ $A^T$ is similar to $B^T$. (3 answers) Closed 8 years ago . If $A$ and $B$ are similar, can the same be said of $A^t$ and $B^t$? $A$ similar to $B$ $\iff \exists P$ such that  $A = PBP^{-1}$ Apply the transpose on both sides $A^t = (PBP^{-1})^t = (P^{-1})^tB^tP^t = (P^t)^{-1}B^tP^t$ Can I conclude that they are similar?","This question already has answers here : A and B is similar ⇒ $A^T$ is similar to $B^T$. (3 answers) Closed 8 years ago . If $A$ and $B$ are similar, can the same be said of $A^t$ and $B^t$? $A$ similar to $B$ $\iff \exists P$ such that  $A = PBP^{-1}$ Apply the transpose on both sides $A^t = (PBP^{-1})^t = (P^{-1})^tB^tP^t = (P^t)^{-1}B^tP^t$ Can I conclude that they are similar?",,"['linear-algebra', 'matrices', 'transpose']"
23,Is there a better way of solving this problem other than guessing what the $2 \times 2$ matrices are?,Is there a better way of solving this problem other than guessing what the  matrices are?,2 \times 2,"EDIT I am just wondering whether, in this type of question, we would be allowed to assume that the ground field is algebraically closed, e.g., the complex numbers, so that we can look the Jordan form of matrices?  Please feel free to comment on this, if you'd like. The Jordan form was the key point of the problem, for anyone interested in solving the below problem. This is an old exam question from linear algebra that I am working on, The problem statement is: Part 1 Can you find $2×2$ matrices $A$,$B$, and $C$, so that $A^2≠0$, $B^2≠0$ and $C^2≠0$ but $AB=0$,$BC=0$ and $CA=0$? Part 2 Can you find $2×2$ matrices $A$,$B$, and $C$, so that $A^2≠0$, $B^2≠0$ and $C^2≠0$ but $AB=0$,$AC=0$ and $BC=0$? So far, I have spent a lot of time on this problem by naively playing around with simple $2x2$ matrices, but no combination of $3$ has worked yet. Every time I think I've found a combination that works, it turns out that the square of one matrix is unfortunately the zero matrix, which disqualifies it from being used. What is the real intent of this problem?  What concepts are being tested?  What is a good strategy? Any ideas are welcome. Thanks,","EDIT I am just wondering whether, in this type of question, we would be allowed to assume that the ground field is algebraically closed, e.g., the complex numbers, so that we can look the Jordan form of matrices?  Please feel free to comment on this, if you'd like. The Jordan form was the key point of the problem, for anyone interested in solving the below problem. This is an old exam question from linear algebra that I am working on, The problem statement is: Part 1 Can you find $2×2$ matrices $A$,$B$, and $C$, so that $A^2≠0$, $B^2≠0$ and $C^2≠0$ but $AB=0$,$BC=0$ and $CA=0$? Part 2 Can you find $2×2$ matrices $A$,$B$, and $C$, so that $A^2≠0$, $B^2≠0$ and $C^2≠0$ but $AB=0$,$AC=0$ and $BC=0$? So far, I have spent a lot of time on this problem by naively playing around with simple $2x2$ matrices, but no combination of $3$ has worked yet. Every time I think I've found a combination that works, it turns out that the square of one matrix is unfortunately the zero matrix, which disqualifies it from being used. What is the real intent of this problem?  What concepts are being tested?  What is a good strategy? Any ideas are welcome. Thanks,",,"['linear-algebra', 'matrices', 'matrix-rank']"
24,Prove that the transpose of a symplectic matrix is also symplectic,Prove that the transpose of a symplectic matrix is also symplectic,,I worked out with symplectic matrices that the transpose is also symplectic for the $2\times 2$ case since the algebra was easy and the determinant of the matrix just needed to equal $1$. [The expression for determinant is easy for $2\times 2$ matrices.] and the transpose has equal determinant. But what about the $n\times n$ case. Is it also true?,I worked out with symplectic matrices that the transpose is also symplectic for the $2\times 2$ case since the algebra was easy and the determinant of the matrix just needed to equal $1$. [The expression for determinant is easy for $2\times 2$ matrices.] and the transpose has equal determinant. But what about the $n\times n$ case. Is it also true?,,"['linear-algebra', 'matrices', 'symplectic-linear-algebra']"
25,Determinant of an anti-diagonal block matrix,Determinant of an anti-diagonal block matrix,,"Is it true in general that if $A$ and $B$ are two $n \times n$ matrices, then the determinant of the anti-diagonal block matrix $$ J = \left[\begin{array}{cc} 0 & A  \\ B& 0  \end{array}\right] $$ is $\det(J)=\det(B)\det(A)$? It is simple to prove this if $n=2$, but I have no idea on how to generalize it.","Is it true in general that if $A$ and $B$ are two $n \times n$ matrices, then the determinant of the anti-diagonal block matrix $$ J = \left[\begin{array}{cc} 0 & A  \\ B& 0  \end{array}\right] $$ is $\det(J)=\det(B)\det(A)$? It is simple to prove this if $n=2$, but I have no idea on how to generalize it.",,"['matrices', 'determinant', 'block-matrices']"
26,"$A+B$ is non singular and $C=(A+B)^{-1}(A-B)$,then prove that $C^TAC=A$","is non singular and ,then prove that",A+B C=(A+B)^{-1}(A-B) C^TAC=A,"If $A$ is a symmetric and $B$ is a skew symmetric matrix and $A+B$ is non singular and $C=(A+B)^{-1}(A-B)$,then prove that $C^TAC=A$. My Attempt: $C^T=((A+B)^{-1}(A-B))^T=(A-B)^T((A+B)^{-1})^T=(A^T-B^T)((A+B)^T)^{-1}$ $C^T=(A+B)(A-B)^{-1}$ $C^TAC=(A+B)(A-B)^{-1}A(A+B)^{-1}(A-B)$ But i am stuck here and could not solve further.Please help me.Thanks.","If $A$ is a symmetric and $B$ is a skew symmetric matrix and $A+B$ is non singular and $C=(A+B)^{-1}(A-B)$,then prove that $C^TAC=A$. My Attempt: $C^T=((A+B)^{-1}(A-B))^T=(A-B)^T((A+B)^{-1})^T=(A^T-B^T)((A+B)^T)^{-1}$ $C^T=(A+B)(A-B)^{-1}$ $C^TAC=(A+B)(A-B)^{-1}A(A+B)^{-1}(A-B)$ But i am stuck here and could not solve further.Please help me.Thanks.",,['matrices']
27,"How to show that if two matrices have the same eigenvectors, then they commute?","How to show that if two matrices have the same eigenvectors, then they commute?",,"This is a problem from Quarteroni, Sacco, and Saleri's Numerical Mathematics .  Their question isn't any more specific than my phrasing of it.  I don't see how to do it without any additional assumptions - for example, that they have the same generalized eigenspaces.  I'm also not very experienced in linear algebra so I also couldn't think of counterexamples if the general case is false.","This is a problem from Quarteroni, Sacco, and Saleri's Numerical Mathematics .  Their question isn't any more specific than my phrasing of it.  I don't see how to do it without any additional assumptions - for example, that they have the same generalized eigenspaces.  I'm also not very experienced in linear algebra so I also couldn't think of counterexamples if the general case is false.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
28,$A$ is symmetric iff $A^T = A$,is symmetric iff,A A^T = A,"I saw this question in my linear algebra textbook: Show $A$ is symmetric if and only $A^T = A$. I started by doing the following using inner product: $$\begin{array}{rl}&A \text{ is symmetric}\\ \Leftrightarrow & g(Ax, y) = g(x, Ay)\\ \Leftrightarrow &(Ax)^Ty = x^T(Ay)\\ \Leftrightarrow &(A^Tx^T)y = x^T(Ay)\\ \Leftrightarrow &A^T(x^Ty) = A(x^Ty)\\ \Leftrightarrow &A^T = A \end{array}$$ I don't know how to do the reverse direction. Can someone help me out? I have read If the inner product is symmetric iff $A=A^{T}$ and Prove A is symmetric matrix iff $A$ is square and $x^T Ay = (Ax)^T y$ , but the arguments for the reverse direction don't have any real validity, hence I am asking here. Edit: My textbook's definition of a symmetric matrix: $$\begin{array}{rl}&A \text{ is symmetric}\\ \Leftrightarrow & g(Ax, y) = g(x, Ay)\\\end{array}$$","I saw this question in my linear algebra textbook: Show $A$ is symmetric if and only $A^T = A$. I started by doing the following using inner product: $$\begin{array}{rl}&A \text{ is symmetric}\\ \Leftrightarrow & g(Ax, y) = g(x, Ay)\\ \Leftrightarrow &(Ax)^Ty = x^T(Ay)\\ \Leftrightarrow &(A^Tx^T)y = x^T(Ay)\\ \Leftrightarrow &A^T(x^Ty) = A(x^Ty)\\ \Leftrightarrow &A^T = A \end{array}$$ I don't know how to do the reverse direction. Can someone help me out? I have read If the inner product is symmetric iff $A=A^{T}$ and Prove A is symmetric matrix iff $A$ is square and $x^T Ay = (Ax)^T y$ , but the arguments for the reverse direction don't have any real validity, hence I am asking here. Edit: My textbook's definition of a symmetric matrix: $$\begin{array}{rl}&A \text{ is symmetric}\\ \Leftrightarrow & g(Ax, y) = g(x, Ay)\\\end{array}$$",,"['linear-algebra', 'matrices', 'transpose']"
29,Relation between determinant of the cofactor matrix and the matrix itself?,Relation between determinant of the cofactor matrix and the matrix itself?,,If the cofactor matrix of A is $$\begin{bmatrix}1 & 2 & 3 \\ 0 & -2 & 4 \\ 0 & 0 & -2\end{bmatrix}$$ How can I find the determinant of $A$?,If the cofactor matrix of A is $$\begin{bmatrix}1 & 2 & 3 \\ 0 & -2 & 4 \\ 0 & 0 & -2\end{bmatrix}$$ How can I find the determinant of $A$?,,['matrices']
30,2-norm of a diagonal matrix and its relation to largest eigenvalue,2-norm of a diagonal matrix and its relation to largest eigenvalue,,"My problem is originated from the boxed inequality below. In the lemma, after Schur decomposition $D$ contains eigenvalues of $A$ along its diagonal. I knew that $\|A \|_{2} =\|D\|_{2}$ as shown Norm of a symmetric matrix equals spectral radius . And, $\|A \|_{2} = \sigma_{max}$ which is the maximum singular value of $A$. So, $\|D \|_{2} =\|A\|_{2}=\sigma_{max} < \sqrt{\lambda_{max}} < \lambda_{max}$ I am not sure my argument is correct or not because if $0<\lambda_{max}<1$, e.g. $\lambda_{max} = 0.8$, then $\sigma_{max} = \sqrt{\lambda_{max}}=\sqrt(0.8) \approx 0.8944 > \lambda_{max}$. So, my question is: I do not understand why $\|D\|_2$ can be replaced by largest eigenvalue of $A$ (or absolute of the largest element of $D$ itself because of the result of Schur decomposition.) Thanks in advance.","My problem is originated from the boxed inequality below. In the lemma, after Schur decomposition $D$ contains eigenvalues of $A$ along its diagonal. I knew that $\|A \|_{2} =\|D\|_{2}$ as shown Norm of a symmetric matrix equals spectral radius . And, $\|A \|_{2} = \sigma_{max}$ which is the maximum singular value of $A$. So, $\|D \|_{2} =\|A\|_{2}=\sigma_{max} < \sqrt{\lambda_{max}} < \lambda_{max}$ I am not sure my argument is correct or not because if $0<\lambda_{max}<1$, e.g. $\lambda_{max} = 0.8$, then $\sigma_{max} = \sqrt{\lambda_{max}}=\sqrt(0.8) \approx 0.8944 > \lambda_{max}$. So, my question is: I do not understand why $\|D\|_2$ can be replaced by largest eigenvalue of $A$ (or absolute of the largest element of $D$ itself because of the result of Schur decomposition.) Thanks in advance.",,"['linear-algebra', 'matrices']"
31,"$\mathrm{Z}(\mathfrak{gl}(2,\Bbb F))$ where the Lie bracket is $[X,Y]=XY-YX$",where the Lie bracket is,"\mathrm{Z}(\mathfrak{gl}(2,\Bbb F)) [X,Y]=XY-YX","I want to find $\mathrm{Z}(\mathfrak{gl}(2,\Bbb F))$ where the Lie bracket is $[X,Y]=XY-YX$ So then this will depend on the field, but no harm in direct computation for arbitrary matrices: $$x=\begin{bmatrix}a&b\\c&d\end{bmatrix},y=\begin{bmatrix}\alpha&\beta\\\gamma&\delta\end{bmatrix}$$ $$[x,y]=\begin{bmatrix}a\alpha+b\gamma-\alpha a - \beta c&a\beta+b\delta-\alpha b-\beta d\\c\alpha+d\gamma -\gamma a - \delta c&c\beta + d\delta - \gamma b-\delta d\end{bmatrix}$$ I want to find $x\in\mathfrak{gl}(2,\Bbb F)$, $[x,y]=0,\forall y$ In $\Bbb C$ or $\Bbb R$, the only possible elements are $\begin{bmatrix}0&0\\0&0\end{bmatrix},I$ In $\Bbb Z_2$, the top left position gives us $b=c=0$, so $$[x,y]=\begin{bmatrix}a\alpha-\alpha a&a\beta-\beta d\\d\gamma -\gamma a &  d\delta -\delta d\end{bmatrix}$$ That's easier to handle and we get $d-a=a-d=0$, which means the centre is: $$Z(\mathfrak{gl}(2,\Bbb Z_2))=\left\{\begin{bmatrix}0&0\\0&0\end{bmatrix},I\right\}$$ How would I go about checking the centre for all fields? Will this always be the same?","I want to find $\mathrm{Z}(\mathfrak{gl}(2,\Bbb F))$ where the Lie bracket is $[X,Y]=XY-YX$ So then this will depend on the field, but no harm in direct computation for arbitrary matrices: $$x=\begin{bmatrix}a&b\\c&d\end{bmatrix},y=\begin{bmatrix}\alpha&\beta\\\gamma&\delta\end{bmatrix}$$ $$[x,y]=\begin{bmatrix}a\alpha+b\gamma-\alpha a - \beta c&a\beta+b\delta-\alpha b-\beta d\\c\alpha+d\gamma -\gamma a - \delta c&c\beta + d\delta - \gamma b-\delta d\end{bmatrix}$$ I want to find $x\in\mathfrak{gl}(2,\Bbb F)$, $[x,y]=0,\forall y$ In $\Bbb C$ or $\Bbb R$, the only possible elements are $\begin{bmatrix}0&0\\0&0\end{bmatrix},I$ In $\Bbb Z_2$, the top left position gives us $b=c=0$, so $$[x,y]=\begin{bmatrix}a\alpha-\alpha a&a\beta-\beta d\\d\gamma -\gamma a &  d\delta -\delta d\end{bmatrix}$$ That's easier to handle and we get $d-a=a-d=0$, which means the centre is: $$Z(\mathfrak{gl}(2,\Bbb Z_2))=\left\{\begin{bmatrix}0&0\\0&0\end{bmatrix},I\right\}$$ How would I go about checking the centre for all fields? Will this always be the same?",,"['matrices', 'group-theory', 'lie-algebras', 'finite-fields']"
32,Proving space of skew-symmetric matrices is orthogonal complement of symmetric matrices,Proving space of skew-symmetric matrices is orthogonal complement of symmetric matrices,,"Problem: Prove that $\left\{ A \in \mathbb{R}^{n \times n} \mid A \text{ is symmetric}\right\}^{\bot} = \left\{ A \in \mathbb{R}^{n \times n} \mid  A \ \text{is skew-symmetric}\right\}$ with $\langle A, B \rangle = Tr(A^T B)$. Attempt at proof: Let $A$ be symmetric, and $B$ skew-symmetric. I want to prove that $\langle A, B \rangle = 0$. So this is what I had so far: \begin{align*} \langle A, B \rangle &= Tr(A^T B) \\ &= Tr(AB) \\ &= \sum_{i=1}^n (AB)_{ii} \\ &= \sum_{i=1}^n \sum_{k=1}^n (a_{ik} b_{ki}) \end{align*} Now I need to use somewhere the fact that $b_{ii} = 0$, i.e. the diagonal elements of a skew-symmetric matrix are zero. But I don't know how to split up the summations? Help would be appreciated!","Problem: Prove that $\left\{ A \in \mathbb{R}^{n \times n} \mid A \text{ is symmetric}\right\}^{\bot} = \left\{ A \in \mathbb{R}^{n \times n} \mid  A \ \text{is skew-symmetric}\right\}$ with $\langle A, B \rangle = Tr(A^T B)$. Attempt at proof: Let $A$ be symmetric, and $B$ skew-symmetric. I want to prove that $\langle A, B \rangle = 0$. So this is what I had so far: \begin{align*} \langle A, B \rangle &= Tr(A^T B) \\ &= Tr(AB) \\ &= \sum_{i=1}^n (AB)_{ii} \\ &= \sum_{i=1}^n \sum_{k=1}^n (a_{ik} b_{ki}) \end{align*} Now I need to use somewhere the fact that $b_{ii} = 0$, i.e. the diagonal elements of a skew-symmetric matrix are zero. But I don't know how to split up the summations? Help would be appreciated!",,"['linear-algebra', 'matrices', 'inner-products']"
33,A matrix as a point in $\mathbb{R}^{nm}$,A matrix as a point in,\mathbb{R}^{nm},"I just had a really quick question to ask. I was reading a book on linear algebra and have just been trying to wrap my head around what exactly a matrix represents. At one point, the book said ""In a more formal sense an $m × n$ matrix $A$ can be thought of as a point in $\mathbb R^{nm}$, with the agreement that the entries are ordered into rows and columns rather than a single row or single column."" I didn't really understand what this meant. What is a point in $\mathbb R^{nm}$ from an intuitive/not so abstract perspective? Does it mean literally the vector space $\mathbb R^{n × m}$ e.g a $2 × 3$ matrix represents a point in $\mathbb R^{6}$ except instead of the 6-tuple written with 6 entries in one row or 6 entries in one column they are written in both rows and columns?","I just had a really quick question to ask. I was reading a book on linear algebra and have just been trying to wrap my head around what exactly a matrix represents. At one point, the book said ""In a more formal sense an $m × n$ matrix $A$ can be thought of as a point in $\mathbb R^{nm}$, with the agreement that the entries are ordered into rows and columns rather than a single row or single column."" I didn't really understand what this meant. What is a point in $\mathbb R^{nm}$ from an intuitive/not so abstract perspective? Does it mean literally the vector space $\mathbb R^{n × m}$ e.g a $2 × 3$ matrix represents a point in $\mathbb R^{6}$ except instead of the 6-tuple written with 6 entries in one row or 6 entries in one column they are written in both rows and columns?",,"['linear-algebra', 'matrices', 'intuition']"
34,Exponential of a matrix with elements $\cos t \& \sin t$,Exponential of a matrix with elements,\cos t \& \sin t,"I want to calculate $e^{A}$ of the matrix $A$: $$\left ( \begin{array}{cc}  \cos t & \sin t \\ -\sin t & \cos t \end{array} \right )$$ I tried to use $e^{At}=P\ \mbox{diag}(e^{\lambda t}) P^{-1}$, but from there I obtain the eigenvalue as $\cos t-|-\sin t|$ which seemed wrong for the solution of the problem. Also I tried to use power series, however I couldn't get anything that makes sense. I don't know what to do, and kind of lost. Could you please help me?","I want to calculate $e^{A}$ of the matrix $A$: $$\left ( \begin{array}{cc}  \cos t & \sin t \\ -\sin t & \cos t \end{array} \right )$$ I tried to use $e^{At}=P\ \mbox{diag}(e^{\lambda t}) P^{-1}$, but from there I obtain the eigenvalue as $\cos t-|-\sin t|$ which seemed wrong for the solution of the problem. Also I tried to use power series, however I couldn't get anything that makes sense. I don't know what to do, and kind of lost. Could you please help me?",,['matrices']
35,Determinant of symplectic matrix,Determinant of symplectic matrix,,"A $2n \times 2n$ matrix $S$ is symplectic, if $SJ_{2n}S^T=J_{2n}$ where  \begin{equation} J_{2n} = \begin{bmatrix} 0 & I_n \\ -I_n & 0 \end{bmatrix}. \end{equation} My question is, how to prove that $\det S =1$, without using Pfaffian , as given in the wikipedia article .","A $2n \times 2n$ matrix $S$ is symplectic, if $SJ_{2n}S^T=J_{2n}$ where  \begin{equation} J_{2n} = \begin{bmatrix} 0 & I_n \\ -I_n & 0 \end{bmatrix}. \end{equation} My question is, how to prove that $\det S =1$, without using Pfaffian , as given in the wikipedia article .",,"['linear-algebra', 'matrices', 'symplectic-linear-algebra']"
36,Using SVD to approximate matrix-vector multiplication?,Using SVD to approximate matrix-vector multiplication?,,"Given some matrix A, is it possible to use Singular Value Decomposition to approximate Ax for some vector x within some error bound? According to Efficient low rank matrix-vector multiplication , it should be possible in O(m+n) time (where A is m by n). I'm not really sure how to get information from that answer though.","Given some matrix A, is it possible to use Singular Value Decomposition to approximate Ax for some vector x within some error bound? According to Efficient low rank matrix-vector multiplication , it should be possible in O(m+n) time (where A is m by n). I'm not really sure how to get information from that answer though.",,"['linear-algebra', 'matrices', 'approximation', 'svd', 'matrix-decomposition']"
37,What is the relationship between dimension of eigen space and multiplity of eigen value?,What is the relationship between dimension of eigen space and multiplity of eigen value?,,Is there a relationship between dimension of eigen space with respect to an eigen value $\lambda_i$ and multiplicity of eigen value $\lambda_i$ ( by multiplicity I mean if $(\lambda-2 )^3(\lambda -1)^2 = 0$ is the eigen value equation then multiplicity for eigen value 2 is 3 and for 1 is 2 )  for a general square matrix ?,Is there a relationship between dimension of eigen space with respect to an eigen value $\lambda_i$ and multiplicity of eigen value $\lambda_i$ ( by multiplicity I mean if $(\lambda-2 )^3(\lambda -1)^2 = 0$ is the eigen value equation then multiplicity for eigen value 2 is 3 and for 1 is 2 )  for a general square matrix ?,,"['linear-algebra', 'matrices']"
38,"If $A$ and $B$ are similar, why does $\text{rank}(A) = \text{rank}(B)$?","If  and  are similar, why does ?",A B \text{rank}(A) = \text{rank}(B),Suppose $A$ and $B$ are similar matrices over $\mathbb{C}^n$. Why do we have $\text{rank}(A) = \text{rank}(B)$?,Suppose $A$ and $B$ are similar matrices over $\mathbb{C}^n$. Why do we have $\text{rank}(A) = \text{rank}(B)$?,,"['linear-algebra', 'matrices', 'matrix-rank']"
39,Solution of differential lyapunov equation,Solution of differential lyapunov equation,,"How would I solve for following, else any implemented algorithms or solvers in matlab (even ways to solve it) for Lyapunov differential equation of form: $$P'(t) + A(t)^TP(t) + P(t)A(t) + Q(t) = 0,$$ where $Q(t)> 0$, with $P(t)$ being symmetric periodic positive definite, $A(t)$ is linearized dynamics of the system.","How would I solve for following, else any implemented algorithms or solvers in matlab (even ways to solve it) for Lyapunov differential equation of form: $$P'(t) + A(t)^TP(t) + P(t)A(t) + Q(t) = 0,$$ where $Q(t)> 0$, with $P(t)$ being symmetric periodic positive definite, $A(t)$ is linearized dynamics of the system.",,"['matrices', 'ordinary-differential-equations', 'dynamical-systems', 'matrix-equations', 'periodic-functions']"
40,Is $\left(\sum_{n=0}^\infty\frac{1}{n!}A^n\right)v=\sum_{n=0}^\infty\frac{1}{n!}(A^nv)$?,Is ?,\left(\sum_{n=0}^\infty\frac{1}{n!}A^n\right)v=\sum_{n=0}^\infty\frac{1}{n!}(A^nv),"Suppose we have a convergent power series of matrices   $$A=\sum_{n=0}^\infty a_nX^n,$$   for $X\in M_n(\mathbb{C})$. Is it true that if $v\in\mathbb{R}^n$ then   $$Av=\sum_{n=0}^\infty a_n(X^nv)?$$   If no, under what conditions is it true? I am asking this question because I am trying to prove that if $\lambda$ is an eigenvalue of $A$, then $e^\lambda$ is an eigenvalue of $\exp(A)$. Indeed, if $Av=\lambda v$ then $$\exp(A)v=\left(\sum_{n=0}^\infty\frac{1}{n!}A^n\right)v\stackrel{?}{=}\sum_{n=0}^\infty\frac{1}{n!}(A^nv)=\sum_{n=0}^\infty\frac{1}{n!}\lambda^nv=e^\lambda v.$$ But I am not sure if the second equality is justified. Otherwise, we can try to show that $$\det(\exp(A)-e^\lambda I)=0,$$ but this seems much harder.","Suppose we have a convergent power series of matrices   $$A=\sum_{n=0}^\infty a_nX^n,$$   for $X\in M_n(\mathbb{C})$. Is it true that if $v\in\mathbb{R}^n$ then   $$Av=\sum_{n=0}^\infty a_n(X^nv)?$$   If no, under what conditions is it true? I am asking this question because I am trying to prove that if $\lambda$ is an eigenvalue of $A$, then $e^\lambda$ is an eigenvalue of $\exp(A)$. Indeed, if $Av=\lambda v$ then $$\exp(A)v=\left(\sum_{n=0}^\infty\frac{1}{n!}A^n\right)v\stackrel{?}{=}\sum_{n=0}^\infty\frac{1}{n!}(A^nv)=\sum_{n=0}^\infty\frac{1}{n!}\lambda^nv=e^\lambda v.$$ But I am not sure if the second equality is justified. Otherwise, we can try to show that $$\det(\exp(A)-e^\lambda I)=0,$$ but this seems much harder.",,['matrices']
41,Are there real solutions to $\exp(X)=-I$?,Are there real solutions to ?,\exp(X)=-I,"As we know, the equation $$e^x=-1,\quad x\in\mathbb{C}$$ has no real solution (in fact $x=i\pi+2ki\pi$, $k\in\mathbb{Z}$). I am now considering the generalization of this question to $2\times 2$ matrices: Question: Is there a real matrix $X\in M_2(\mathbb{R})$ such that   $$\exp(X)=-I,$$   where $\exp$ is the matrix exponential ? I found that the (unreal) matrix $$X=\begin{pmatrix}i\pi & 0 \\ 0 & i\pi\end{pmatrix}$$ satisfy the equation. But I have no idea on how to show whether there are other real solutions.","As we know, the equation $$e^x=-1,\quad x\in\mathbb{C}$$ has no real solution (in fact $x=i\pi+2ki\pi$, $k\in\mathbb{Z}$). I am now considering the generalization of this question to $2\times 2$ matrices: Question: Is there a real matrix $X\in M_2(\mathbb{R})$ such that   $$\exp(X)=-I,$$   where $\exp$ is the matrix exponential ? I found that the (unreal) matrix $$X=\begin{pmatrix}i\pi & 0 \\ 0 & i\pi\end{pmatrix}$$ satisfy the equation. But I have no idea on how to show whether there are other real solutions.",,['matrices']
42,Why use homogeneous coordinates?,Why use homogeneous coordinates?,,"I am having trouble understand the use of homogeneous coordinates for when describing transformations in 3D space. From what I have seen, the only difference between a transformation matrix in standard coordinates, and homogeneous coordinates, is that a fourth row is added, of [0 0 0 1]. Then, when transforming a point, an additional row of [1] is added to the point vector. What is the point of this additional 1? And is it ever a different number? From what I have read, homogeneous coordinates enable perspective transformations to be achieved using matrices and linear algebra, but I don't see the connection....","I am having trouble understand the use of homogeneous coordinates for when describing transformations in 3D space. From what I have seen, the only difference between a transformation matrix in standard coordinates, and homogeneous coordinates, is that a fourth row is added, of [0 0 0 1]. Then, when transforming a point, an additional row of [1] is added to the point vector. What is the point of this additional 1? And is it ever a different number? From what I have read, homogeneous coordinates enable perspective transformations to be achieved using matrices and linear algebra, but I don't see the connection....",,"['linear-algebra', 'matrices']"
43,Commutativity of the square root of matrices,Commutativity of the square root of matrices,,"Let $A, B \in \mathbb{R}^{n \times n}$ two positive definite matrices such that $AB = BA$, that is $A$ commutes with $B$. It is easy to prove that $A^{1/2}$ commutes with $A$, indeed $AA^{1/2} = A^{3/2}=A^{1/2}A$, but I am wondering whether it is true that $A^{1/2}B = BA^{1/2}$, in other words that $AB = BA$ implies $A^{1/2}B = BA^{1/2}$. The best clue I have - and it isn't much - is that I cannot prove it.","Let $A, B \in \mathbb{R}^{n \times n}$ two positive definite matrices such that $AB = BA$, that is $A$ commutes with $B$. It is easy to prove that $A^{1/2}$ commutes with $A$, indeed $AA^{1/2} = A^{3/2}=A^{1/2}A$, but I am wondering whether it is true that $A^{1/2}B = BA^{1/2}$, in other words that $AB = BA$ implies $A^{1/2}B = BA^{1/2}$. The best clue I have - and it isn't much - is that I cannot prove it.",,['matrices']
44,Street Fighter: is the game balanced?,Street Fighter: is the game balanced?,,"Suppose that $A$ is a matrix that describes the matchup information of any pair of Street Fighter characters e.g., considering $3$ characters, assume that the first row/collumn is associated with a character like Ryu, the second row/collumn is associated with Chun-li, the third row/collumn with Dhalsim, etc. Then for a $3\times3$ matrix we have: $$A=\begin{pmatrix} 5 & 3 & 9 \\ 7 & 5 & 4 \\ 1 & 6 & 5 \end{pmatrix},$$ which tell us e.g., if Ryu fights Chun-li he wins $3$ out of $10$ matches. Which kind of processing can I do to $A$, so that I know more information about the game?","Suppose that $A$ is a matrix that describes the matchup information of any pair of Street Fighter characters e.g., considering $3$ characters, assume that the first row/collumn is associated with a character like Ryu, the second row/collumn is associated with Chun-li, the third row/collumn with Dhalsim, etc. Then for a $3\times3$ matrix we have: $$A=\begin{pmatrix} 5 & 3 & 9 \\ 7 & 5 & 4 \\ 1 & 6 & 5 \end{pmatrix},$$ which tell us e.g., if Ryu fights Chun-li he wins $3$ out of $10$ matches. Which kind of processing can I do to $A$, so that I know more information about the game?",,"['matrices', 'game-theory']"
45,Eigenvalues of a nxn matrix without calculations [duplicate],Eigenvalues of a nxn matrix without calculations [duplicate],,"This question already has answers here : Determinant of a rank $1$ update of a scalar matrix, or characteristic polynomial of a rank $1$ matrix (2 answers) Closed 9 years ago . I have a question about the following matrix: $$         \begin{bmatrix}         1 & 2 & 3 \\         1 & 2 & 3 \\         1 & 2 & 3 \\         \end{bmatrix} $$ Find the eigenvalues without calculations and define your answer. Now, I was thinking about this problem. And I thought, yeah ok if you try the vector (1,1,1), you can find 6 as one eigenvalue (and I know you have a double multiplicity 0 too). But than you are doing sort of guessing/calculation work. I see that the columns are linearly dependant. So I know the dimension of the column space and of the null space. Thank you in advance. EDIT: follow up question: Ok, so you find that the dimension of the null space is 2, so there are 2 eigenvectors when the eigenvalue is 0. Now my question is, can the dimension of the eigenspace be bigger than the amount of eigenvalues? I guess not. I know it can be smaller","This question already has answers here : Determinant of a rank $1$ update of a scalar matrix, or characteristic polynomial of a rank $1$ matrix (2 answers) Closed 9 years ago . I have a question about the following matrix: $$         \begin{bmatrix}         1 & 2 & 3 \\         1 & 2 & 3 \\         1 & 2 & 3 \\         \end{bmatrix} $$ Find the eigenvalues without calculations and define your answer. Now, I was thinking about this problem. And I thought, yeah ok if you try the vector (1,1,1), you can find 6 as one eigenvalue (and I know you have a double multiplicity 0 too). But than you are doing sort of guessing/calculation work. I see that the columns are linearly dependant. So I know the dimension of the column space and of the null space. Thank you in advance. EDIT: follow up question: Ok, so you find that the dimension of the null space is 2, so there are 2 eigenvectors when the eigenvalue is 0. Now my question is, can the dimension of the eigenspace be bigger than the amount of eigenvalues? I guess not. I know it can be smaller",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
46,Invertible matrices in commutative rings,Invertible matrices in commutative rings,,Let $A$ be a square matrix over a commutative ring $R$. Then $A$ has a left inverse iff it is invertible. Does there exist a elementary proof of this fact? (i.e. without using the determinant!),Let $A$ be a square matrix over a commutative ring $R$. Then $A$ has a left inverse iff it is invertible. Does there exist a elementary proof of this fact? (i.e. without using the determinant!),,"['abstract-algebra', 'matrices', 'commutative-algebra']"
47,"For a 2x2 matrix A satisfying $A^k=I$, compute $e^A$","For a 2x2 matrix A satisfying , compute",A^k=I e^A,"For a 2x2 matrix A satisfying $A^k=I$, compute $e^A$  Oh, the exponential of a matrix is: $e^A=\sum_{i=0}^\infty\frac{1}{i!}A^i$ I thought I'd solved the $e^A$ form but I actually did something really silly, now I'm a little stuck. If $A^k=I$ then $A^{k+1}=A$ and we have a cycle forming, so we will get: $$\sum^{k-1}_{i=0}\sum^{\infty}_{j=1}\frac{1}{(kj+i)!}A^i$$ (Or something like this form, I don't have paper to hand and just spotted that now) Is this what the question wants? It doesn't use the 2x2 property. I believe the answer lies in finding an expression for the inner summation, I can do this for two terms (think of $e+e^{-1}$ all the odd power leave)","For a 2x2 matrix A satisfying $A^k=I$, compute $e^A$  Oh, the exponential of a matrix is: $e^A=\sum_{i=0}^\infty\frac{1}{i!}A^i$ I thought I'd solved the $e^A$ form but I actually did something really silly, now I'm a little stuck. If $A^k=I$ then $A^{k+1}=A$ and we have a cycle forming, so we will get: $$\sum^{k-1}_{i=0}\sum^{\infty}_{j=1}\frac{1}{(kj+i)!}A^i$$ (Or something like this form, I don't have paper to hand and just spotted that now) Is this what the question wants? It doesn't use the 2x2 property. I believe the answer lies in finding an expression for the inner summation, I can do this for two terms (think of $e+e^{-1}$ all the odd power leave)",,"['linear-algebra', 'matrices']"
48,Matrices similar only to themselves,Matrices similar only to themselves,,"Find all matrices similar only to themselves, i.e., $PTP^{-1}=T$ for any invertible $P$. My attempt: $PT = TP$. Am I going about this correctly? If so, how do I find all matrices that are commutative (where $P$ is invertible)?","Find all matrices similar only to themselves, i.e., $PTP^{-1}=T$ for any invertible $P$. My attempt: $PT = TP$. Am I going about this correctly? If so, how do I find all matrices that are commutative (where $P$ is invertible)?",,"['linear-algebra', 'matrices']"
49,Rank of a matrix and dimension of the image,Rank of a matrix and dimension of the image,,"I'm teaching linear algebra to first year students, and I was recently asked why is the rank of a matrix, representing a linear application in a given basis, equal to the dimension of the image space of this application. If I think in terms of columns it is easy to see that applying it to the canonical basis vectors gives me a span of the image where every vector has for coordinates one column of the first matrix, thus the dimension of the image is the number of linearly independent columns. But is there an equivalent short argument for rows? I mean without using the fact that the rank is invariant by applying transposition, and possibly without introducing a dual space which could make things only more confusing for my students.","I'm teaching linear algebra to first year students, and I was recently asked why is the rank of a matrix, representing a linear application in a given basis, equal to the dimension of the image space of this application. If I think in terms of columns it is easy to see that applying it to the canonical basis vectors gives me a span of the image where every vector has for coordinates one column of the first matrix, thus the dimension of the image is the number of linearly independent columns. But is there an equivalent short argument for rows? I mean without using the fact that the rank is invariant by applying transposition, and possibly without introducing a dual space which could make things only more confusing for my students.",,"['linear-algebra', 'matrices', 'vector-spaces', 'matrix-rank']"
50,$A^2=-I$ and its similar matrices,and its similar matrices,A^2=-I,"Given $A \in M_n(\Bbb R)$ be a matrix such that $A^2 = -I$ . Prove that $A$ is similar over $\Bbb R$ to $$B=\begin{pmatrix}0&{-I_{n/2}}\\{I_{n/2}}&0\end{pmatrix}.$$ It's easy to see that $(\det A)^2 = (-1)^n$ , so $n$ is even. It's fine to show that there exists an invertible matrix $P$ such that $P^{-1}AP=B$ , but is there any explicit representation of $P$ ?","Given be a matrix such that . Prove that is similar over to It's easy to see that , so is even. It's fine to show that there exists an invertible matrix such that , but is there any explicit representation of ?",A \in M_n(\Bbb R) A^2 = -I A \Bbb R B=\begin{pmatrix}0&{-I_{n/2}}\\{I_{n/2}}&0\end{pmatrix}. (\det A)^2 = (-1)^n n P P^{-1}AP=B P,"['linear-algebra', 'matrices']"
51,Show that sum of elements of rows / columns of a matrix is equal to reciprocal of sum of elements of rows/colums of its inverse matrix,Show that sum of elements of rows / columns of a matrix is equal to reciprocal of sum of elements of rows/colums of its inverse matrix,,"Suppose $A=(a_{ij})_{n\times n}$ be a non singular matrix. Suppose sum of elements of each row is $k\neq 0$, then the sum of elements of rows of $A^{-1}$ is $\frac{1}{k}$. Let    $\,A^{-1}=(b_{ij})_{n\times n}$. Then \begin{align*} &A\left[ \begin{array}{c} 1\\ 1\\ 1\\\vdots \\1 \end{array}\right]=k\left[ \begin{array}{c} 1\\ 1\\ 1\\\vdots \\1 \end{array}\right]\\ \implies &\frac{1}{k}A^{-1}A\left[ \begin{array}{c} 1\\ 1\\ 1\\\vdots \\1 \end{array}\right]=\frac{1}{k}A^{-1}k\left[ \begin{array}{c} 1\\ 1\\ 1\\\vdots \\1 \end{array}\right]\\ \implies &\frac{1}{k}I_n\left[ \begin{array}{c} 1\\ 1\\ 1\\\vdots \\1 \end{array}\right]=A^{-1}\left[ \begin{array}{c} 1\\ 1\\ 1\\\vdots \\1 \end{array}\right]\\ \implies &\frac{1}{k}I_n\left[ \begin{array}{c} 1\\ 1\\ 1\\\vdots \\1 \end{array}\right]=(b_{ij})\left[ \begin{array}{c} 1\\ 1\\ 1\\\vdots \\1 \end{array}\right]\\ \implies& (b_{ij})\left[ \begin{array}{c} 1\\ 1\\ 1\\\vdots \\1 \end{array}\right]=\frac{1}{k}I_n\left[ \begin{array}{c} 1\\ 1\\ 1\\\vdots \\1 \end{array}\right]\\ \implies &(\displaystyle\sum_{r=1}^n b_{1r}, \displaystyle\sum_{r=1}^n b_{2r},\cdots, \displaystyle\sum_{r=1}^n b_{nr})=(\frac{1}{k}, \frac{1}{k}\cdots, \frac{1}{k})\\ \implies & \displaystyle\sum_{r=1}^n b_{ir}=\frac{1}{k}, \forall i=1,2,\cdots,n   \end{align*}","Suppose $A=(a_{ij})_{n\times n}$ be a non singular matrix. Suppose sum of elements of each row is $k\neq 0$, then the sum of elements of rows of $A^{-1}$ is $\frac{1}{k}$. Let    $\,A^{-1}=(b_{ij})_{n\times n}$. Then \begin{align*} &A\left[ \begin{array}{c} 1\\ 1\\ 1\\\vdots \\1 \end{array}\right]=k\left[ \begin{array}{c} 1\\ 1\\ 1\\\vdots \\1 \end{array}\right]\\ \implies &\frac{1}{k}A^{-1}A\left[ \begin{array}{c} 1\\ 1\\ 1\\\vdots \\1 \end{array}\right]=\frac{1}{k}A^{-1}k\left[ \begin{array}{c} 1\\ 1\\ 1\\\vdots \\1 \end{array}\right]\\ \implies &\frac{1}{k}I_n\left[ \begin{array}{c} 1\\ 1\\ 1\\\vdots \\1 \end{array}\right]=A^{-1}\left[ \begin{array}{c} 1\\ 1\\ 1\\\vdots \\1 \end{array}\right]\\ \implies &\frac{1}{k}I_n\left[ \begin{array}{c} 1\\ 1\\ 1\\\vdots \\1 \end{array}\right]=(b_{ij})\left[ \begin{array}{c} 1\\ 1\\ 1\\\vdots \\1 \end{array}\right]\\ \implies& (b_{ij})\left[ \begin{array}{c} 1\\ 1\\ 1\\\vdots \\1 \end{array}\right]=\frac{1}{k}I_n\left[ \begin{array}{c} 1\\ 1\\ 1\\\vdots \\1 \end{array}\right]\\ \implies &(\displaystyle\sum_{r=1}^n b_{1r}, \displaystyle\sum_{r=1}^n b_{2r},\cdots, \displaystyle\sum_{r=1}^n b_{nr})=(\frac{1}{k}, \frac{1}{k}\cdots, \frac{1}{k})\\ \implies & \displaystyle\sum_{r=1}^n b_{ir}=\frac{1}{k}, \forall i=1,2,\cdots,n   \end{align*}",,"['matrices', 'inverse']"
52,General formula for $\det(A+I)$ where I is identity. Worked it out for $2 \times 2$ and $3 \times 3$.,General formula for  where I is identity. Worked it out for  and .,\det(A+I) 2 \times 2 3 \times 3,"Does anybody know a general formula for $|A+I|$ where $A$ is a $\textbf{symmetric}$, real (square) matrix? For a $2\times2$ system I worked out: $|A+I| = |A|+\text{tr}(A)+1$. This is very friendly. For a $3\times3$ system I worked out: $|A+I| = |A|+\text{tr}(A) + 1 + \sum_{i=1}^{3} |M_{i,i}|$, where $M_{i,i}$ is the determinant of the i,i-th minor, i.e. the determinant of the matrix that ensues when we remove the i-th row and i-th column from the original matrix $A$. Now, I am wondering (and assuming) that this problem is not new and was wondering if anybody knew of a general formula. Because for a $4\times4$ system, this last formula does not hold. All your help is greatly appreciated!","Does anybody know a general formula for $|A+I|$ where $A$ is a $\textbf{symmetric}$, real (square) matrix? For a $2\times2$ system I worked out: $|A+I| = |A|+\text{tr}(A)+1$. This is very friendly. For a $3\times3$ system I worked out: $|A+I| = |A|+\text{tr}(A) + 1 + \sum_{i=1}^{3} |M_{i,i}|$, where $M_{i,i}$ is the determinant of the i,i-th minor, i.e. the determinant of the matrix that ensues when we remove the i-th row and i-th column from the original matrix $A$. Now, I am wondering (and assuming) that this problem is not new and was wondering if anybody knew of a general formula. Because for a $4\times4$ system, this last formula does not hold. All your help is greatly appreciated!",,"['linear-algebra', 'matrices']"
53,Maximal commutative subring of the ring of $2 \times 2$ matrices over the reals,Maximal commutative subring of the ring of  matrices over the reals,2 \times 2,"Motivated by complex numbers, I noticed that the set of all elements of the following forms a commutative sub-ring of $M_2(\mathbb{R})$: \begin{pmatrix} x & y\\ -y & x \end{pmatrix} Is this sub-ring maximal w.r.t commutativity? If this sub-ring is commutatively maximal, are there 'other' such maximally commutative sub-rings? P.S: 'other'=non-isomorphic.","Motivated by complex numbers, I noticed that the set of all elements of the following forms a commutative sub-ring of $M_2(\mathbb{R})$: \begin{pmatrix} x & y\\ -y & x \end{pmatrix} Is this sub-ring maximal w.r.t commutativity? If this sub-ring is commutatively maximal, are there 'other' such maximally commutative sub-rings? P.S: 'other'=non-isomorphic.",,['matrices']
54,Finding the closest matrix of a given form,Finding the closest matrix of a given form,,"Let's say I have a vector $(a_1\dots a_n)$ , where each component is between $-1$ and $1$ . Now from this vector I define a $n\times n$ matrix $M$ such that $$M_{ij} = \begin{cases} 1&\,& i = j\\ a_i\cdot a_j&\,& i\neq j\end{cases}$$ I now call $E$ the space of all matrices $M$ I can build with that process. Now, I have a given matrix $A$ that can be assumed to be symetrical, having $1$ on the diagonal and that is symetric definitive positive. Is there a way to find a matrix from the space $E$ that is the closest to $A$ ? By ""closest"" I mean with any norm that seems reasonable. I guess this can be difficult to solve, and I'm just trying to find direction for this problem. Alternatively, any iterative solution that can converge to the result could be fine as well. Any idea? Thanks!","Let's say I have a vector , where each component is between and . Now from this vector I define a matrix such that I now call the space of all matrices I can build with that process. Now, I have a given matrix that can be assumed to be symetrical, having on the diagonal and that is symetric definitive positive. Is there a way to find a matrix from the space that is the closest to ? By ""closest"" I mean with any norm that seems reasonable. I guess this can be difficult to solve, and I'm just trying to find direction for this problem. Alternatively, any iterative solution that can converge to the result could be fine as well. Any idea? Thanks!","(a_1\dots a_n) -1 1 n\times n M M_{ij} = \begin{cases} 1&\,& i = j\\ a_i\cdot a_j&\,& i\neq j\end{cases} E M A 1 E A","['linear-algebra', 'matrices', 'optimization']"
55,Function for diagonalizing a vector.,Function for diagonalizing a vector.,,"I was playing around whith the idea of what operation (function) should I perform (apply) over a vector $\mathbf{a} = (a_1,a_2, \ldots, a_N)^T \in \mathbb{R}^N$ to come up with the following matrix : $$ A =  \left( \begin{array}{ccc} a_1 & 0 & \cdots & 0 \\ 0  & a_2 & \cdots &  0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & a_N \end{array} \right)? $$ i.e, how can I diagonalize my vector $\mathbf{a}$. This is my approach, but first I must apologize for my very rusty linear-applications-knowhow. That being said, I was wondering that, if there exist the following relation between $\mathbf{a}$ and $A$ : $$ A \, \mathbf{1}  = \mathbf{a},$$ where $\mathbf{1}$ is a $n \times 1$ column vector filled with ones in its entries, then I can trivially solve for $A$ in terms of the pseudoinverse of $\mathbf{a}$ as follows: $$ A = \left[ \mathbf{1} \, \mathbf{a}^T \, (\mathbf{a} \, \mathbf{a}^T)^{-1} \right]^{-1}, $$ so this would become my desired $A = f(\mathbf{a})$. Unfortunately, Matlab has wisely warned me that $\det{(\mathbf{a}\, \mathbf{a}^T)} = 0$ (which I can't formally see why), so the above formula makes no sense. I'm run out of ideas right now. Someone can help me? Thanks in advance and Cheers! Edit 1 : of course, I should have mentioned that I'm aware of the existence of the command diag in Matlab.","I was playing around whith the idea of what operation (function) should I perform (apply) over a vector $\mathbf{a} = (a_1,a_2, \ldots, a_N)^T \in \mathbb{R}^N$ to come up with the following matrix : $$ A =  \left( \begin{array}{ccc} a_1 & 0 & \cdots & 0 \\ 0  & a_2 & \cdots &  0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & a_N \end{array} \right)? $$ i.e, how can I diagonalize my vector $\mathbf{a}$. This is my approach, but first I must apologize for my very rusty linear-applications-knowhow. That being said, I was wondering that, if there exist the following relation between $\mathbf{a}$ and $A$ : $$ A \, \mathbf{1}  = \mathbf{a},$$ where $\mathbf{1}$ is a $n \times 1$ column vector filled with ones in its entries, then I can trivially solve for $A$ in terms of the pseudoinverse of $\mathbf{a}$ as follows: $$ A = \left[ \mathbf{1} \, \mathbf{a}^T \, (\mathbf{a} \, \mathbf{a}^T)^{-1} \right]^{-1}, $$ so this would become my desired $A = f(\mathbf{a})$. Unfortunately, Matlab has wisely warned me that $\det{(\mathbf{a}\, \mathbf{a}^T)} = 0$ (which I can't formally see why), so the above formula makes no sense. I'm run out of ideas right now. Someone can help me? Thanks in advance and Cheers! Edit 1 : of course, I should have mentioned that I'm aware of the existence of the command diag in Matlab.",,"['matrices', 'algebra-precalculus', 'vector-spaces']"
56,Does an analog of the absolute value is greater than the real part inequality hold for square matrices?,Does an analog of the absolute value is greater than the real part inequality hold for square matrices?,,"Does the following hold for any square matrix $A$, $(AA^*)^{1/2}\geq (A+A^*)/2$, where the superscript $*$ denotes the Hermitian transpose. If not, does it hold for some types of matrices at least? Proof/any comment would be appreciated.","Does the following hold for any square matrix $A$, $(AA^*)^{1/2}\geq (A+A^*)/2$, where the superscript $*$ denotes the Hermitian transpose. If not, does it hold for some types of matrices at least? Proof/any comment would be appreciated.",,"['linear-algebra', 'matrices', 'inequality']"
57,Prove Derivative is sum of determinants,Prove Derivative is sum of determinants,,"Given $n^2$ functions $f_{ij}$, each differentiable on an interval (a,b), define $F(x) = det[f_{ij}(x)]$ for each $x$ in $(a,b)$. Prove that the derivative $F'(x)$ is the sum of the determinants, $$ F'(x) = \sum_{i=1}^n det A_i(x),$$ where $A_i(x)$ is the matrix obtained by differentiating the functions in the $ith$ row of $[f_{ij}(x)]$ Yeah, I have no clue what I'm supposed to do.","Given $n^2$ functions $f_{ij}$, each differentiable on an interval (a,b), define $F(x) = det[f_{ij}(x)]$ for each $x$ in $(a,b)$. Prove that the derivative $F'(x)$ is the sum of the determinants, $$ F'(x) = \sum_{i=1}^n det A_i(x),$$ where $A_i(x)$ is the matrix obtained by differentiating the functions in the $ith$ row of $[f_{ij}(x)]$ Yeah, I have no clue what I'm supposed to do.",,"['linear-algebra', 'matrices', 'derivatives', 'determinant']"
58,Show each eigenvalue of a companion matrix has geometric multiplicity $=1$.,Show each eigenvalue of a companion matrix has geometric multiplicity .,=1,"Given the differential equation $$x^{(n)}(t)+c_{n-1}x^{(n-1)}(t) + \dotsb + c_1x'(t) + c_0=0,$$ we can form a vector $\xi = (x, x', \dotsc, x^{(n-1)})$, and then we have $$\xi'(t) = A\xi,$$  where $A$ is the transpose of the companion matrix for the polynomial $$z^n + c_{n-1}z^{n-1}+\dotsb + c_1z + c_0.$$ A problem in Teschl's ODE book is to show that each eigenvalue of $A$ has geometric multiplicity $1.$ The hint it gives is ""can you find a cyclic vector for $A$? How does that help?"" (By a cyclic vector he means $v$ such that $\{A^{k}v\}\,\, (0\leq k <n)$ spans the vector space.) I see that $e_n$ is a cyclic vector for $A$, but I'm not seeing how that helps. Any ideas?","Given the differential equation $$x^{(n)}(t)+c_{n-1}x^{(n-1)}(t) + \dotsb + c_1x'(t) + c_0=0,$$ we can form a vector $\xi = (x, x', \dotsc, x^{(n-1)})$, and then we have $$\xi'(t) = A\xi,$$  where $A$ is the transpose of the companion matrix for the polynomial $$z^n + c_{n-1}z^{n-1}+\dotsb + c_1z + c_0.$$ A problem in Teschl's ODE book is to show that each eigenvalue of $A$ has geometric multiplicity $1.$ The hint it gives is ""can you find a cyclic vector for $A$? How does that help?"" (By a cyclic vector he means $v$ such that $\{A^{k}v\}\,\, (0\leq k <n)$ spans the vector space.) I see that $e_n$ is a cyclic vector for $A$, but I'm not seeing how that helps. Any ideas?",,"['linear-algebra', 'matrices', 'ordinary-differential-equations', 'eigenvalues-eigenvectors', 'companion-matrices']"
59,A Problem about Common Eigenvector,A Problem about Common Eigenvector,,"Question 1: Let $A$ , $B$ be two $n\times n$ complex matrix satisfy: $AB-BA=0$ . Then $A$ , $B$ have a common eigenvector. Question 2: Let $A$ , $B$ be two $n\times n$ complex matrix satisfy: $AB-BA=B$ . Then $A$ , $B$ have a common eigenvector. Question 1 It is easy to prove. Let $\lambda$ be a eigenvalue of $A$ and $V_\lambda$ be the eigensubspace. For any $x\in V_\lambda$ , we have $A(Bx)=\lambda(Bx)$ . So $V_\lambda$ is the invariant subspace of $B$ . Question 2 I have some ideas but fail to solve it all. Let $Ax=\lambda x$ . Then we have $A(Bx)=(\lambda+1)(Bx)$ . Assume $V_{\lambda_1}$ , $V_{\lambda_2}$ ,.., $V_{\lambda_s}$ are all eigensubspaces of $A$ and $n_0,...,n_s$ are the index: for any $i$ , there exists $x\in V_{\lambda_i}$ and we have $B^{n_i}x\not=0$ but for all $y\in V_{\lambda_i}$ , $B^{n_i}y=0$ . How is the next step? Or is there any different idea? Can someone help me? Thank you.","Question 1: Let , be two complex matrix satisfy: . Then , have a common eigenvector. Question 2: Let , be two complex matrix satisfy: . Then , have a common eigenvector. Question 1 It is easy to prove. Let be a eigenvalue of and be the eigensubspace. For any , we have . So is the invariant subspace of . Question 2 I have some ideas but fail to solve it all. Let . Then we have . Assume , ,.., are all eigensubspaces of and are the index: for any , there exists and we have but for all , . How is the next step? Or is there any different idea? Can someone help me? Thank you.","A B n\times n AB-BA=0 A B A B n\times n AB-BA=B A B \lambda A V_\lambda x\in V_\lambda A(Bx)=\lambda(Bx) V_\lambda B Ax=\lambda x A(Bx)=(\lambda+1)(Bx) V_{\lambda_1} V_{\lambda_2} V_{\lambda_s} A n_0,...,n_s i x\in V_{\lambda_i} B^{n_i}x\not=0 y\in V_{\lambda_i} B^{n_i}y=0","['linear-algebra', 'matrices']"
60,Distance from Point on Plane to Origin,Distance from Point on Plane to Origin,,"I have a practice question that is asking: Find the point on the plane $2x - 3y + z = 3$ closest to the origin. What is the distance from that point to the origin? Here is my work so far (if you can check to see if it is incorrect, please let me know): $d = (2,-3,1)$ Vector equation of line: $$\begin{align}x & = p + td \\ x &= (0,0,0) + t(2,-3,1)\end{align}$$ Parametric form: $$\begin{align}x  & =2t \\ y & =-3t \\ z & =t\end{align}$$ Equation: $$\begin{align}2x-3y+z & = 3 \\ 2(2t) - 3(-3t) + t & = 3 \\ 4t + 9t + t & = 3 \\ 14t & = 3 \\ t & = 3/14\end{align}$$ Therefore: $$\begin{align}& x = 2(3/14), \  & y &= -3(3/14), \  & z = 3/14 \\ & x = 6/14, \ & y &= -9/14, \ & z = 3/14\end{align}$$ Closest point to origin: $P(3/7, -9/14, 3/14)$ But now I have to find the distance from $P(3/7, -9/14, 3/14)$ to the origin. Can someone help me with that? And also can you please check to see what I did to find the point on the plane closest to the origin is correct ? Would greatly appreciate the help. It is a practice question for an upcoming exam so I want to get this correct. Thanks!","I have a practice question that is asking: Find the point on the plane closest to the origin. What is the distance from that point to the origin? Here is my work so far (if you can check to see if it is incorrect, please let me know): Vector equation of line: Parametric form: Equation: Therefore: Closest point to origin: But now I have to find the distance from to the origin. Can someone help me with that? And also can you please check to see what I did to find the point on the plane closest to the origin is correct ? Would greatly appreciate the help. It is a practice question for an upcoming exam so I want to get this correct. Thanks!","2x - 3y + z = 3 d = (2,-3,1) \begin{align}x & = p + td \\
x &= (0,0,0) + t(2,-3,1)\end{align} \begin{align}x  & =2t \\
y & =-3t \\
z & =t\end{align} \begin{align}2x-3y+z & = 3 \\
2(2t) - 3(-3t) + t & = 3 \\
4t + 9t + t & = 3 \\
14t & = 3 \\
t & = 3/14\end{align} \begin{align}& x = 2(3/14), \ 
& y &= -3(3/14), \ 
& z = 3/14 \\
& x = 6/14, \
& y &= -9/14, \
& z = 3/14\end{align} P(3/7, -9/14, 3/14) P(3/7, -9/14, 3/14)","['matrices', 'vector-spaces']"
61,Product of two symmetric matrices with eigenvalues all 0 or 1 is idempotent,Product of two symmetric matrices with eigenvalues all 0 or 1 is idempotent,,"Assume $A$, $V$ are symmetric and $V$ is positive definite. If $AV$ has eigenvalues that are all zero or one, show $AV$ is idempotent. My proof so far (haven't used the symmetric property or the fact that $V$ is positive definite - where do those come in?): If the eigenvalue $\lambda=0$, and assume there exists a nonzero $u$ such that $$AVu=\lambda u=0u=0$$ $$(AV)^2u=AVAVu=AV0=0$$ Then $(AV)^2u=AVu=0$ implies $(AV)^2=AV$ since $u\ne 0$. If $\lambda=1$, and assume there exists nonzero $u$ such that $$AVu=\lambda u=u$$ $$(AV)^2u=AVAVu=AVu$$ $$\implies (AV)^2=AV$$","Assume $A$, $V$ are symmetric and $V$ is positive definite. If $AV$ has eigenvalues that are all zero or one, show $AV$ is idempotent. My proof so far (haven't used the symmetric property or the fact that $V$ is positive definite - where do those come in?): If the eigenvalue $\lambda=0$, and assume there exists a nonzero $u$ such that $$AVu=\lambda u=0u=0$$ $$(AV)^2u=AVAVu=AV0=0$$ Then $(AV)^2u=AVu=0$ implies $(AV)^2=AV$ since $u\ne 0$. If $\lambda=1$, and assume there exists nonzero $u$ such that $$AVu=\lambda u=u$$ $$(AV)^2u=AVAVu=AVu$$ $$\implies (AV)^2=AV$$",,"['linear-algebra', 'matrices']"
62,Diagonalization of circulant matrices,Diagonalization of circulant matrices,,Why does the following hold?: $A$ circulant matrix iff it has a representation of the form $F^{-1}DF$ where $D$ is a diagonal matrix and $F$ is a discrete Fourier transformation. I get that $F^{-1}DF$ is circulant but what about the other direction?,Why does the following hold?: $A$ circulant matrix iff it has a representation of the form $F^{-1}DF$ where $D$ is a diagonal matrix and $F$ is a discrete Fourier transformation. I get that $F^{-1}DF$ is circulant but what about the other direction?,,"['matrices', 'analysis', 'fourier-analysis', 'diagonalization', 'circulant-matrices']"
63,"If $A\in M_n(R)$ and $\det A$ is not a zero divisor, what can we say about its entries?","If  and  is not a zero divisor, what can we say about its entries?",A\in M_n(R) \det A,"I am working on this proof and think I have a lemma that will get it for me. However I am not sure if this lemma is true and can not figure out how to prove it, if it is. Here goes Given some $A\in M_n(R)$, $R$ a commutative ring, if $\det A$ is not a zero divisor then none of its nonzero entries are zero divisors. I would also like to make this an if and only if statement. Possibly The $\det A$ is not a zero divisor if and only if $\det A\neq 0$ and none of its non zero entries are zero divisors. Any ideas on proving this lemma or rephrasing it to something more useful would be much appreciated.","I am working on this proof and think I have a lemma that will get it for me. However I am not sure if this lemma is true and can not figure out how to prove it, if it is. Here goes Given some $A\in M_n(R)$, $R$ a commutative ring, if $\det A$ is not a zero divisor then none of its nonzero entries are zero divisors. I would also like to make this an if and only if statement. Possibly The $\det A$ is not a zero divisor if and only if $\det A\neq 0$ and none of its non zero entries are zero divisors. Any ideas on proving this lemma or rephrasing it to something more useful would be much appreciated.",,"['abstract-algebra', 'matrices', 'determinant']"
64,Proof that $(\alpha I - A)$ invertible if $\alpha > \rho(A)$,Proof that  invertible if,(\alpha I - A) \alpha > \rho(A),"I want to proof that for $A \in \mathbb{R}^{n \times n}$ with $a_{ij}\geq 0, \forall i,j=1,...,n$: \begin{align} (\alpha I - A) \text{ is invertible if } \alpha > \rho(A) \end{align} where $\rho(A)$ ist the spectral radius of $A$, so the largest eigen value of A. I want to show that $\det(\alpha I - A) \neq 0$, but I don't know how. What I found out so far is that $\det(A)=\lambda_1^{r_i}...\lambda_n^{r_n} \leq \rho(A)^{r_i}...\rho(A)^{r_n}$ and that $\det(\alpha I)=\alpha^n$. Can I use this somehow? I know this has a lot to do with functional analysis. $(\alpha I - A)^{-1}$ is the resolvent formalism. But I never had anything to do with functional analysis, so I'm struggling. Thanks for you help!","I want to proof that for $A \in \mathbb{R}^{n \times n}$ with $a_{ij}\geq 0, \forall i,j=1,...,n$: \begin{align} (\alpha I - A) \text{ is invertible if } \alpha > \rho(A) \end{align} where $\rho(A)$ ist the spectral radius of $A$, so the largest eigen value of A. I want to show that $\det(\alpha I - A) \neq 0$, but I don't know how. What I found out so far is that $\det(A)=\lambda_1^{r_i}...\lambda_n^{r_n} \leq \rho(A)^{r_i}...\rho(A)^{r_n}$ and that $\det(\alpha I)=\alpha^n$. Can I use this somehow? I know this has a lot to do with functional analysis. $(\alpha I - A)^{-1}$ is the resolvent formalism. But I never had anything to do with functional analysis, so I'm struggling. Thanks for you help!",,"['linear-algebra', 'matrices', 'numerical-linear-algebra']"
65,How can we derive the pseudo inverse of a matrix from its Singular value decomposition?,How can we derive the pseudo inverse of a matrix from its Singular value decomposition?,,"For a matrix $M$ with its singular value decomposition $UΣV^T$, the pseudo inverse of $M$, i.e., $M^+$ is $VΣ^+U^T$. How can I derive the pseudo inverse(Moore–Penrose) $M^+$ from the singular value decomposition of a matrix $M$? From SVD, we know that $Σ$ is a diagonal matrix which contains the square roots of the eigen values of both $MM^T$ and $M^TM$ whereas $Σ^+$ is formed by replacing the non-zero diagonal elements of $Σ$ by its reciprocal. The diagonal matrix Σ is not always full rank so I assume that $ΣΣ^+$ cannot always be an Identity Matrix. How is it possible to prove that the pseudo inverse of $M$, i.e., $M^+$=$VΣ^+U^T$ holds when $ΣΣ^+$ cannot be reduced to Identity Matrix? Is there any other approach?","For a matrix $M$ with its singular value decomposition $UΣV^T$, the pseudo inverse of $M$, i.e., $M^+$ is $VΣ^+U^T$. How can I derive the pseudo inverse(Moore–Penrose) $M^+$ from the singular value decomposition of a matrix $M$? From SVD, we know that $Σ$ is a diagonal matrix which contains the square roots of the eigen values of both $MM^T$ and $M^TM$ whereas $Σ^+$ is formed by replacing the non-zero diagonal elements of $Σ$ by its reciprocal. The diagonal matrix Σ is not always full rank so I assume that $ΣΣ^+$ cannot always be an Identity Matrix. How is it possible to prove that the pseudo inverse of $M$, i.e., $M^+$=$VΣ^+U^T$ holds when $ΣΣ^+$ cannot be reduced to Identity Matrix? Is there any other approach?",,"['linear-algebra', 'matrices', 'inverse', 'svd', 'pseudoinverse']"
66,Why do all linear transformations have no restrictions on their natural domains?,Why do all linear transformations have no restrictions on their natural domains?,,"Some normal functions have restrictions on their natural domains, but linear functions don't. Why? Related: Is there a linear transformation who domain isn't all of $\mathbb{R}^n$? Why my question is not a duplicate of the above question : The above question asks if there asks if there is any function whose domain isn't $\mathbb{R}^n$. This function clearly exists by limiting the domain of any linear transformation. My question is asking about the natural domain of a linear transformation, which none of the answers from the question above address. Natural Domain: The largest domain where the transformation makes sense. In other words, the domain of a linear transformation, without any artificial ""restrictions"" put on it.","Some normal functions have restrictions on their natural domains, but linear functions don't. Why? Related: Is there a linear transformation who domain isn't all of $\mathbb{R}^n$? Why my question is not a duplicate of the above question : The above question asks if there asks if there is any function whose domain isn't $\mathbb{R}^n$. This function clearly exists by limiting the domain of any linear transformation. My question is asking about the natural domain of a linear transformation, which none of the answers from the question above address. Natural Domain: The largest domain where the transformation makes sense. In other words, the domain of a linear transformation, without any artificial ""restrictions"" put on it.",,"['linear-algebra', 'matrices', 'vector-spaces']"
67,A common eigenvector of $A^2$ and $A^{-1}$,A common eigenvector of  and,A^2 A^{-1},"Show that the eigenvector of $A^2$ is the same as the one of $A^{-1}$, where $$A = \begin{bmatrix} 2 & 1 \\ 0 & 2 \end{bmatrix}.$$ All that I can find is that $$A^2 = \begin{bmatrix} 4 & 4 \\ 0 & 4 \end{bmatrix}, \quad A^{-1} = \begin{bmatrix} 1/2 & -1/4 \\ 0 & 1/2 \end{bmatrix}.$$ The eigenvalue of $A^2$ is $4$, and of $A^{-1}$ is $1/2$. I don't know how to do the rest. *Edit According to my lecturer's note, $$(A-\lambda I)X = 0 \rightarrow \begin{bmatrix} a-\lambda & b \\ c & d-\lambda \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$$ If I use $A^2$ 's eigenvalue which is $\lambda = 4$. By using the note provided, $$\begin{bmatrix} 4-\lambda & 4 \\ 0 & 4-\lambda \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$$ $$\begin{bmatrix} 4-4 & 4 \\ 0 & 4-4 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$$ $$\begin{bmatrix} 0 & 4 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$$ Then I'm stuck here given what I know and what I've done is correct...","Show that the eigenvector of $A^2$ is the same as the one of $A^{-1}$, where $$A = \begin{bmatrix} 2 & 1 \\ 0 & 2 \end{bmatrix}.$$ All that I can find is that $$A^2 = \begin{bmatrix} 4 & 4 \\ 0 & 4 \end{bmatrix}, \quad A^{-1} = \begin{bmatrix} 1/2 & -1/4 \\ 0 & 1/2 \end{bmatrix}.$$ The eigenvalue of $A^2$ is $4$, and of $A^{-1}$ is $1/2$. I don't know how to do the rest. *Edit According to my lecturer's note, $$(A-\lambda I)X = 0 \rightarrow \begin{bmatrix} a-\lambda & b \\ c & d-\lambda \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$$ If I use $A^2$ 's eigenvalue which is $\lambda = 4$. By using the note provided, $$\begin{bmatrix} 4-\lambda & 4 \\ 0 & 4-\lambda \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$$ $$\begin{bmatrix} 4-4 & 4 \\ 0 & 4-4 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$$ $$\begin{bmatrix} 0 & 4 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$$ Then I'm stuck here given what I know and what I've done is correct...",,"['matrices', 'eigenvalues-eigenvectors']"
68,What am I doing wrong when trying to find a determinant of this 4x4,What am I doing wrong when trying to find a determinant of this 4x4,,I have to find the determinant of this 4x4 matrix: $ \begin{bmatrix}  5 & -7 & 2 & 2 \\  0 & 3 & 0 & -4 \\  -5 & -8 & 0 & 3 \\  0 & -5 & 0 & -6 \\ \end{bmatrix} $ Here is my working which seems wrong according to the solutions. What am i doing wrong?: And here is the solution:,I have to find the determinant of this 4x4 matrix: $ \begin{bmatrix}  5 & -7 & 2 & 2 \\  0 & 3 & 0 & -4 \\  -5 & -8 & 0 & 3 \\  0 & -5 & 0 & -6 \\ \end{bmatrix} $ Here is my working which seems wrong according to the solutions. What am i doing wrong?: And here is the solution:,,"['matrices', 'determinant']"
69,Projection and direct sum,Projection and direct sum,,I want to show that for every projection $A^2=A$ we have that there exists a subspace $U_1 \subset ker(A)$ and $U_2$ such that $A|_{U_2} = id$ such that $V = U_1 \oplus U_2$. Does anybody here have a hint how to show this?,I want to show that for every projection $A^2=A$ we have that there exists a subspace $U_1 \subset ker(A)$ and $U_2$ such that $A|_{U_2} = id$ such that $V = U_1 \oplus U_2$. Does anybody here have a hint how to show this?,,"['real-analysis', 'linear-algebra']"
70,Test for acyclic graph property based on adjacency matrix,Test for acyclic graph property based on adjacency matrix,,I am trying to solve a problem that I have but I lack the theoretical knowledge that might be necessary to solve it. I have a directed graph encoded as an adjacency matrix. Is it possible to test whether a graph is acyclic just by using algebra (operations/transformations/properties of that adjacency matrix)? Any help greatly appreciated. Thank you.,I am trying to solve a problem that I have but I lack the theoretical knowledge that might be necessary to solve it. I have a directed graph encoded as an adjacency matrix. Is it possible to test whether a graph is acyclic just by using algebra (operations/transformations/properties of that adjacency matrix)? Any help greatly appreciated. Thank you.,,"['matrices', 'graph-theory']"
71,Wikipedia Proof of Skolem-Noether Theorem,Wikipedia Proof of Skolem-Noether Theorem,,"Could someone help me understand the wikipedia proof of the Skolem-Noether theorem? In particular, I'm interested in the case where $A=B= M_n(\mathbb{C})$. So the theorem claims that given any two homomorphisms $f, g: A \to A$, there is a unit $k \in A$ so that for each $a$, $f(a)=kg(a)k^{-1}$. It seems this case is dealt with in the first two sentences of the wikipedia proof. They observe that $f, g$ give an action of $A$ on $\mathbb{C}^n$ via $a \cdot v=f(a)v$. This makes $\mathbb{C}^n$ into $A$ modules in two different ways which we can denote $V_f, V_g$. Now, there is a unique simple $A$-module $S$. Thus, $V_f \cong S^r, V_g \cong S^s$ for some $r ,s$. If $V_f$ and $V_g$ are isomorphic as $A$-modules, then such a map is also an isomorphism of $\mathbb{C}$-vector spaces and so there is some invertible matrix $b: V_f  \to V_g$ and I am done. But this is seemingly only true if $s=r$. What am I missing? Edit: This is the case since $V_f, V_g$ have the same dimension as vector spaces and so the dimensions of $S^r, S^s$ are the same.","Could someone help me understand the wikipedia proof of the Skolem-Noether theorem? In particular, I'm interested in the case where $A=B= M_n(\mathbb{C})$. So the theorem claims that given any two homomorphisms $f, g: A \to A$, there is a unit $k \in A$ so that for each $a$, $f(a)=kg(a)k^{-1}$. It seems this case is dealt with in the first two sentences of the wikipedia proof. They observe that $f, g$ give an action of $A$ on $\mathbb{C}^n$ via $a \cdot v=f(a)v$. This makes $\mathbb{C}^n$ into $A$ modules in two different ways which we can denote $V_f, V_g$. Now, there is a unique simple $A$-module $S$. Thus, $V_f \cong S^r, V_g \cong S^s$ for some $r ,s$. If $V_f$ and $V_g$ are isomorphic as $A$-modules, then such a map is also an isomorphism of $\mathbb{C}$-vector spaces and so there is some invertible matrix $b: V_f  \to V_g$ and I am done. But this is seemingly only true if $s=r$. What am I missing? Edit: This is the case since $V_f, V_g$ have the same dimension as vector spaces and so the dimensions of $S^r, S^s$ are the same.",,"['abstract-algebra', 'matrices', 'proof-explanation']"
72,Eigenvalues of a tridiagonal stochastic matrix,Eigenvalues of a tridiagonal stochastic matrix,,"I've tried to calculate the eigenvalues of this tridiagonal (stochastic) matrix of dimension $n \times n$, but I had some problems to find an explicit form. I only know that 1 is the largest eigenvalue. $$M=\dfrac{1}{2}\begin{pmatrix}  1& 1 &  &  &  & \\   1& 0 &1  &  &  & \\   & 1 & 0 &1  &  & \\   &  &  & \dots &  & \\   &  &  & 1 & 0 & 1\\   &  &  &  & 1 & 1 \end{pmatrix}$$ (Here, the matrix takes the value 0 where there is no number.) I'm interested in the value of the second largest eigenvalue as a function of the dimension $n$ of this matrix. I would appreciate any help.","I've tried to calculate the eigenvalues of this tridiagonal (stochastic) matrix of dimension $n \times n$, but I had some problems to find an explicit form. I only know that 1 is the largest eigenvalue. $$M=\dfrac{1}{2}\begin{pmatrix}  1& 1 &  &  &  & \\   1& 0 &1  &  &  & \\   & 1 & 0 &1  &  & \\   &  &  & \dots &  & \\   &  &  & 1 & 0 & 1\\   &  &  &  & 1 & 1 \end{pmatrix}$$ (Here, the matrix takes the value 0 where there is no number.) I'm interested in the value of the second largest eigenvalue as a function of the dimension $n$ of this matrix. I would appreciate any help.",,"['linear-algebra', 'matrices', 'stochastic-processes', 'eigenvalues-eigenvectors', 'diagonalization']"
73,"""If $A^2-2AB+B^2=0$, then $(A-B)^2=0$"" is true. How about $n$-th degree case?","""If , then "" is true. How about -th degree case?",A^2-2AB+B^2=0 (A-B)^2=0 n,"I've known the following question: Supposing that $A, B$ are the second degree square matrices whose elements are all complex numbers, then is the following true ? If $A^2-2AB+B^2=O,$ then $(A-B)^2=O$ where $O$ is a zero matrix. Surprisingly, the answer is Yes ! Proof : We'll use the fact that $\operatorname{trace} (AB)= \operatorname{trace}(BA)$. Letting $C=A-B$, then $B=A-C$. So, $$A^2-2AB+B^2=0\rightarrow C^2=CA-AC \tag{1}$$ Hence, $$\operatorname{trace} (C^2) = \operatorname{trace} (CA)-\operatorname{trace} (AC)=0 \tag{2}$$ Here, supposing that there exists $C^{-1}$, then multiplying $C^{-1}$ to $(1)$ from both sides leads  $$I=AC^{-1}-C^{-1}A$$ where $I$ is a unit matrix. Then trace $(I)=\operatorname{trace} (AC^{-1})- \operatorname{trace} (C^{-1}A)$ leads a contradiction $2=0$. Then, knowing that there doesn't exist $C^{-1}$, we get det$(C)=0$. By Cayley-Hamilton theorem, $$C^2-\operatorname{trace}(C)C+\det (C)I=O\rightarrow C^2=\operatorname{trace}(C)C \tag{3}$$ Hence $(2)$ leads $0=\operatorname{trace}(C^2)=\operatorname{trace}(C)\times\operatorname{trace}(C)\rightarrow\operatorname{trace}(C)=0$. Hence $(3)$ leads $C^2=O$. Now the proof is completed. Let us call the above question the $2$nd degree version question . I got interested in this question. So, I've been thinking the $3$rd degree version question. The $3$rd degree version question : Supposing that $A, B$ are the third degree square matrices whose elements are all complex numbers, then is the following true ? If $A^3-3A^2B-3AB^2-B^3=O,$ then $(A-B)^3=O$ where $O$ is a zero matrix. After struggling to solve this question, I got a counterexample. The answer for the $3$rd degree version question is NO . The following is the counterexample: $$A=\begin{pmatrix}         0 & 0 & 0 \\         1 & 0 & 0 \\         0 & 0 & 0 \\         \end{pmatrix} ,\ \ B=\begin{pmatrix}         0 & 1 & 0 \\         0 & 0 & 0 \\         0 & 0 & 0 \\         \end{pmatrix}.$$ I've tried to solve the $n$-th degree version question for $n\ge4$, but I'm facing difficulty. Then, here is my question. Question : Could you show me how to solve the $n$-th degree version question for $n\ge4$?","I've known the following question: Supposing that $A, B$ are the second degree square matrices whose elements are all complex numbers, then is the following true ? If $A^2-2AB+B^2=O,$ then $(A-B)^2=O$ where $O$ is a zero matrix. Surprisingly, the answer is Yes ! Proof : We'll use the fact that $\operatorname{trace} (AB)= \operatorname{trace}(BA)$. Letting $C=A-B$, then $B=A-C$. So, $$A^2-2AB+B^2=0\rightarrow C^2=CA-AC \tag{1}$$ Hence, $$\operatorname{trace} (C^2) = \operatorname{trace} (CA)-\operatorname{trace} (AC)=0 \tag{2}$$ Here, supposing that there exists $C^{-1}$, then multiplying $C^{-1}$ to $(1)$ from both sides leads  $$I=AC^{-1}-C^{-1}A$$ where $I$ is a unit matrix. Then trace $(I)=\operatorname{trace} (AC^{-1})- \operatorname{trace} (C^{-1}A)$ leads a contradiction $2=0$. Then, knowing that there doesn't exist $C^{-1}$, we get det$(C)=0$. By Cayley-Hamilton theorem, $$C^2-\operatorname{trace}(C)C+\det (C)I=O\rightarrow C^2=\operatorname{trace}(C)C \tag{3}$$ Hence $(2)$ leads $0=\operatorname{trace}(C^2)=\operatorname{trace}(C)\times\operatorname{trace}(C)\rightarrow\operatorname{trace}(C)=0$. Hence $(3)$ leads $C^2=O$. Now the proof is completed. Let us call the above question the $2$nd degree version question . I got interested in this question. So, I've been thinking the $3$rd degree version question. The $3$rd degree version question : Supposing that $A, B$ are the third degree square matrices whose elements are all complex numbers, then is the following true ? If $A^3-3A^2B-3AB^2-B^3=O,$ then $(A-B)^3=O$ where $O$ is a zero matrix. After struggling to solve this question, I got a counterexample. The answer for the $3$rd degree version question is NO . The following is the counterexample: $$A=\begin{pmatrix}         0 & 0 & 0 \\         1 & 0 & 0 \\         0 & 0 & 0 \\         \end{pmatrix} ,\ \ B=\begin{pmatrix}         0 & 1 & 0 \\         0 & 0 & 0 \\         0 & 0 & 0 \\         \end{pmatrix}.$$ I've tried to solve the $n$-th degree version question for $n\ge4$, but I'm facing difficulty. Then, here is my question. Question : Could you show me how to solve the $n$-th degree version question for $n\ge4$?",,['matrices']
74,Calculating determinant with real number on diagonal and units everywhere else,Calculating determinant with real number on diagonal and units everywhere else,,"I'm solving a problem and I'm having difficulties in calculation of the determinants of two matrices. There is two $N\times N$ matrices: $$\left(   \begin{array}{cccc}     a & 1 & \ldots & 1 \\    1 & a & \ldots & \vdots \\     \vdots & \ldots & \ddots & 1 \\     1 & \ldots & 1 & a \\   \end{array} \right)$$ where $a\in \mathbb{R}$ and $$\left(   \begin{array}{cccc}     a_1 & 1 & \ldots & 1 \\    1 & a_2 & \ldots & \vdots \\     \vdots & \ldots & \ddots & 1 \\     1 & \ldots & 1 & a_n \\   \end{array} \right)$$ where $a_1,\ldots,a_n\in \mathbb{R}$ I've tried to get recursive formula, but I have no result. If someone could help or give me any links I would be grateful.","I'm solving a problem and I'm having difficulties in calculation of the determinants of two matrices. There is two $N\times N$ matrices: $$\left(   \begin{array}{cccc}     a & 1 & \ldots & 1 \\    1 & a & \ldots & \vdots \\     \vdots & \ldots & \ddots & 1 \\     1 & \ldots & 1 & a \\   \end{array} \right)$$ where $a\in \mathbb{R}$ and $$\left(   \begin{array}{cccc}     a_1 & 1 & \ldots & 1 \\    1 & a_2 & \ldots & \vdots \\     \vdots & \ldots & \ddots & 1 \\     1 & \ldots & 1 & a_n \\   \end{array} \right)$$ where $a_1,\ldots,a_n\in \mathbb{R}$ I've tried to get recursive formula, but I have no result. If someone could help or give me any links I would be grateful.",,"['linear-algebra', 'matrices', 'determinant']"
75,Prove that the set of all diagonal matrices is a subring of $\operatorname{Mat}_n(R)$ which is isomorphic to $R \times\dots\times R$ ($n$ factors),Prove that the set of all diagonal matrices is a subring of  which is isomorphic to  ( factors),\operatorname{Mat}_n(R) R \times\dots\times R n,"Can someone tell me, is that diagonal matrices is a subring of $\operatorname{Mat}_n(R)$ which is (ring) isomorphic to $R \times · · · \times R$ (n factors) and why?.","Can someone tell me, is that diagonal matrices is a subring of $\operatorname{Mat}_n(R)$ which is (ring) isomorphic to $R \times · · · \times R$ (n factors) and why?.",,['linear-algebra']
76,Sum of principal minors,Sum of principal minors,,Is there any formula for the sum of principal minors? (note: $i^{th}$ principal minor which results from omitting the $i^{th}$ row and $i^{th}$ column),Is there any formula for the sum of principal minors? (note: $i^{th}$ principal minor which results from omitting the $i^{th}$ row and $i^{th}$ column),,"['linear-algebra', 'matrices']"
77,$A^TA+I$ is always invertible?,is always invertible?,A^TA+I,How to prove a general matrix invertible given by as below? How to prove that $A^TA+I$ is always invertible for $\forall A \in \mathbb{R}^{n\times n}$?,How to prove a general matrix invertible given by as below? How to prove that $A^TA+I$ is always invertible for $\forall A \in \mathbb{R}^{n\times n}$?,,['matrices']
78,Characteristic polynomial divides minimal polynomial if and only if all eigenspaces are one-dimensional [duplicate],Characteristic polynomial divides minimal polynomial if and only if all eigenspaces are one-dimensional [duplicate],,"This question already has answers here : When are minimal and characteristic polynomials the same? (4 answers) Closed 10 years ago . Prove that characteristic polynomial of a complex matrix $A$ divides its minimal polynomial if and only if all eigenspaces of $A$ are one-dimensional. As far as I can see I the only possible case is when minimal polynomial equals characteristic one. All distinct eigenvalues with multiplicity 1 grant us that the eigenspaces would be one-dimensional, I thought that this is the key to the solution, however we have the theorem stating that on the other side, eigenspace dimension could be less then or equal to its eigenvalue algebraic multiplicity. Everything now mixed up, will be thankful for any help.","This question already has answers here : When are minimal and characteristic polynomials the same? (4 answers) Closed 10 years ago . Prove that characteristic polynomial of a complex matrix $A$ divides its minimal polynomial if and only if all eigenspaces of $A$ are one-dimensional. As far as I can see I the only possible case is when minimal polynomial equals characteristic one. All distinct eigenvalues with multiplicity 1 grant us that the eigenspaces would be one-dimensional, I thought that this is the key to the solution, however we have the theorem stating that on the other side, eigenspace dimension could be less then or equal to its eigenvalue algebraic multiplicity. Everything now mixed up, will be thankful for any help.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
79,A question about matrices,A question about matrices,,"I was working on the McLain example about a characteristically simple group. McLain Unfortunaly I have no access to the original paper, so I was trying to make it out by myself. I found (on Google) that McLain use a division ring to create a group of automorphisms of a vector space which (I hope) has to be the characteristically simple one. Now I proved that this group has to be locally nilpotent and that every its element can be embedded in an unitriangular group (over the same division ring). So my question is the following one: Are unitriangular matrices, over a division ring, nilpotent groups? How can I show that? (Nilpotency will help me to make a further step towards the proof) (I know that this works for unitriangular matrix over a field.) (Obviously if you can tell to me some information on the McLain example it would be great too!) Thanks for the attention!","I was working on the McLain example about a characteristically simple group. McLain Unfortunaly I have no access to the original paper, so I was trying to make it out by myself. I found (on Google) that McLain use a division ring to create a group of automorphisms of a vector space which (I hope) has to be the characteristically simple one. Now I proved that this group has to be locally nilpotent and that every its element can be embedded in an unitriangular group (over the same division ring). So my question is the following one: Are unitriangular matrices, over a division ring, nilpotent groups? How can I show that? (Nilpotency will help me to make a further step towards the proof) (I know that this works for unitriangular matrix over a field.) (Obviously if you can tell to me some information on the McLain example it would be great too!) Thanks for the attention!",,"['linear-algebra', 'group-theory', 'matrices']"
80,Singular values: smallest perturbation to make a matrix singular,Singular values: smallest perturbation to make a matrix singular,,"A problem from Gilbert Strang's Linear text, after the section on the singular value decomposition: Suppose $A$ is $2\times 2$ and  invertible (with $\sigma_1 > \sigma_2 > 0$). Change $A$ by as small a matrix as possible to produce a singular matrix $A_0$. Hint: $U$ and $V$ don't change. Use $$A = \left [ \begin {matrix} u_1 & u_2\end {matrix}\right ] \left [ \begin {matrix} \sigma_1 &  \\ & \sigma_2 \end {matrix}\right ] \left [ \begin {matrix} v_1 & v_2\end {matrix}\right ]^T.$$ I think we can change $A$ by $$\left [ \begin {matrix} u_1 & u_2\end {matrix}\right ] \left [ \begin {matrix} 0 &  \\ & -\sigma_2 \end {matrix}\right ] \left [ \begin {matrix} v_1 & v_2\end {matrix}\right ]^T.$$ But how can we show this is the smallest possible alteration of $A$ to get a singular matrix? I think Strang is expecting a somewhat loose answer here, since he's not yet introduced the matrix norm, so it's not clear what he means by ""as small as possible.""  If we were to attempt it in a more rigorous way, I think we'd like two results: A result relating singular values to addition and subtraction, something like $\sigma_n(A) + \sigma_n(B) \geq \sigma_n(A \pm  B)\geq \sigma_n(A) - \sigma_1(B)$, where $\sigma_1, \dotsc , \sigma_n$ are arranged in descending order. A result relating singular values to matrix norm. I believe the largest singular value is the value of the 2-norm, but how can we relate this to the operator norm?","A problem from Gilbert Strang's Linear text, after the section on the singular value decomposition: Suppose $A$ is $2\times 2$ and  invertible (with $\sigma_1 > \sigma_2 > 0$). Change $A$ by as small a matrix as possible to produce a singular matrix $A_0$. Hint: $U$ and $V$ don't change. Use $$A = \left [ \begin {matrix} u_1 & u_2\end {matrix}\right ] \left [ \begin {matrix} \sigma_1 &  \\ & \sigma_2 \end {matrix}\right ] \left [ \begin {matrix} v_1 & v_2\end {matrix}\right ]^T.$$ I think we can change $A$ by $$\left [ \begin {matrix} u_1 & u_2\end {matrix}\right ] \left [ \begin {matrix} 0 &  \\ & -\sigma_2 \end {matrix}\right ] \left [ \begin {matrix} v_1 & v_2\end {matrix}\right ]^T.$$ But how can we show this is the smallest possible alteration of $A$ to get a singular matrix? I think Strang is expecting a somewhat loose answer here, since he's not yet introduced the matrix norm, so it's not clear what he means by ""as small as possible.""  If we were to attempt it in a more rigorous way, I think we'd like two results: A result relating singular values to addition and subtraction, something like $\sigma_n(A) + \sigma_n(B) \geq \sigma_n(A \pm  B)\geq \sigma_n(A) - \sigma_1(B)$, where $\sigma_1, \dotsc , \sigma_n$ are arranged in descending order. A result relating singular values to matrix norm. I believe the largest singular value is the value of the 2-norm, but how can we relate this to the operator norm?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
81,Find the determinant by using elementary row operations,Find the determinant by using elementary row operations,,I'm having a problem finding the determinant of the following matrix using elementary row operations. I know the determinant is -15 but confused on how to do it using the elementary row operations. Here is the matrix $$\begin{bmatrix} 2 &  3 & 10   \\ 1 & 2 & -2   \\ 1 & 1 & -3   \end{bmatrix}$$ Thank you,I'm having a problem finding the determinant of the following matrix using elementary row operations. I know the determinant is -15 but confused on how to do it using the elementary row operations. Here is the matrix $$\begin{bmatrix} 2 &  3 & 10   \\ 1 & 2 & -2   \\ 1 & 1 & -3   \end{bmatrix}$$ Thank you,,['matrices']
82,Solving for X in a simple matrix equation system.,Solving for X in a simple matrix equation system.,,"I am trying to solve for X in this simple matrix equation system: $$\begin{bmatrix}7 & 7\\2 & 4\\\end{bmatrix} - X\begin{bmatrix}5 & -1\\6 & -4\\\end{bmatrix} = E $$ where $E$ is the identity matrix. If I multiply $X$ with $\begin{bmatrix}5 & -1\\6 & -4\\\end{bmatrix}$ I get the following system: $$\begin{bmatrix}5x_1 & -1x_2\\6x_3 & -4x_4\\\end{bmatrix}$$ By subtracting this from $\begin{bmatrix}7 & 7\\2 & 4\\\end{bmatrix}$ I get $\begin{bmatrix}7 - 5x_1 & 7 + 1x_2\\2 - 6x_3 & 4 + 4x_4\\\end{bmatrix} = \begin{bmatrix}1 & 0\\0 & 1\\\end{bmatrix}$ Which gives me: $7-5x_1 = 1$ $7+1x_2 = 0$ $2-6x_3 = 0$ $4+4x_4 = 1$ These are not the correct answers, can anyone help me out here? Thank you!","I am trying to solve for X in this simple matrix equation system: $$\begin{bmatrix}7 & 7\\2 & 4\\\end{bmatrix} - X\begin{bmatrix}5 & -1\\6 & -4\\\end{bmatrix} = E $$ where $E$ is the identity matrix. If I multiply $X$ with $\begin{bmatrix}5 & -1\\6 & -4\\\end{bmatrix}$ I get the following system: $$\begin{bmatrix}5x_1 & -1x_2\\6x_3 & -4x_4\\\end{bmatrix}$$ By subtracting this from $\begin{bmatrix}7 & 7\\2 & 4\\\end{bmatrix}$ I get $\begin{bmatrix}7 - 5x_1 & 7 + 1x_2\\2 - 6x_3 & 4 + 4x_4\\\end{bmatrix} = \begin{bmatrix}1 & 0\\0 & 1\\\end{bmatrix}$ Which gives me: $7-5x_1 = 1$ $7+1x_2 = 0$ $2-6x_3 = 0$ $4+4x_4 = 1$ These are not the correct answers, can anyone help me out here? Thank you!",,"['linear-algebra', 'matrices']"
83,Commutative Matrix Multiplication of Invertible Matrices,Commutative Matrix Multiplication of Invertible Matrices,,"I know that in general, the only matrices that are multiplicatively commutative are those that are scalar multiples of $I$, the identity matrix. But what about matrices that are multiplicatively commutative with only invertible matrices?  Is it any different?  I don't think so, but I'm not certain, and am struggling to prove it. Simply, with $A$ and $B$ both being $n\times n$ matrices over the reals, what are all $A$ such that $AB = BA$ if $B$ is invertible? I suppose in group theory this could be phrased as the centre of the general linear group over the reals - $S(GL_n(\mathbb{R}))$.","I know that in general, the only matrices that are multiplicatively commutative are those that are scalar multiples of $I$, the identity matrix. But what about matrices that are multiplicatively commutative with only invertible matrices?  Is it any different?  I don't think so, but I'm not certain, and am struggling to prove it. Simply, with $A$ and $B$ both being $n\times n$ matrices over the reals, what are all $A$ such that $AB = BA$ if $B$ is invertible? I suppose in group theory this could be phrased as the centre of the general linear group over the reals - $S(GL_n(\mathbb{R}))$.",,"['group-theory', 'matrices']"
84,Eigenvalues of a block matrix,Eigenvalues of a block matrix,,"For $X=\left(\begin{array}{cc} A & B\\ C & 0\end{array}\right)$ , how are eigenvalues of $X$ related to the eigenvalues of $A$ ?","For , how are eigenvalues of related to the eigenvalues of ?",X=\left(\begin{array}{cc} A & B\\ C & 0\end{array}\right) X A,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'block-matrices']"
85,Does the pseudoinverse of $A\in\mathbb{R}^{n\times n}$ commute with $A$ when $A$ is normal?,Does the pseudoinverse of  commute with  when  is normal?,A\in\mathbb{R}^{n\times n} A A,Suppose that $A^+\in \mathbb{R}^{n\times n}$ is the (Moore-Penrose) pseudoinverse of $A$ and that $AA^t=A^tA$. Does there hold $$AA^+=A^+A?$$,Suppose that $A^+\in \mathbb{R}^{n\times n}$ is the (Moore-Penrose) pseudoinverse of $A$ and that $AA^t=A^tA$. Does there hold $$AA^+=A^+A?$$,,"['linear-algebra', 'matrices']"
86,Inverse of a symmetric tridiagonal filter matrix,Inverse of a symmetric tridiagonal filter matrix,,How to get the inverse of this matrix $$\begin{pmatrix} 2&-1\\-1&2&-1\\&-1&2&-1\\&&&\ddots\\&&&&\ddots\\&&&&-1&2&-1\\&&&&&-1&2 \end{pmatrix}$$ where the blank entries are all zero?,How to get the inverse of this matrix where the blank entries are all zero?,"\begin{pmatrix} 2&-1\\-1&2&-1\\&-1&2&-1\\&&&\ddots\\&&&&\ddots\\&&&&-1&2&-1\\&&&&&-1&2
\end{pmatrix}","['linear-algebra', 'matrices', 'inverse', 'tridiagonal-matrices', 'toeplitz-matrices']"
87,Largest and smallest eigenvalues of a hermitian matrix,Largest and smallest eigenvalues of a hermitian matrix,,How to show that the largest and smallest eigenvalues of a hermitian matrix $A \in \mathbb{C}^{n \times n} $ can be found as: $\displaystyle \lambda_{max} = \underset{x\neq0}{\max{\frac{x^*Ax}{x^*x}}}$ and $\displaystyle \lambda_{min} = \underset{x\neq0}{\min{\frac{x^*Ax}{x^*x}}}$,How to show that the largest and smallest eigenvalues of a hermitian matrix $A \in \mathbb{C}^{n \times n} $ can be found as: $\displaystyle \lambda_{max} = \underset{x\neq0}{\max{\frac{x^*Ax}{x^*x}}}$ and $\displaystyle \lambda_{min} = \underset{x\neq0}{\min{\frac{x^*Ax}{x^*x}}}$,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'numerical-linear-algebra']"
88,How to find this greatest common divisor limit?,How to find this greatest common divisor limit?,,"Let $A^n-I=\begin{bmatrix} a_{n}&b_{n}\\ c_{n}&d_{n} \end{bmatrix} $, where $A=\begin{bmatrix} 3&2\\ 4&3 \end{bmatrix}$, $I=\begin{bmatrix} 1&0\\ 0&1 \end{bmatrix}$ and let $f_{n}=\gcd(a_{n},b_{n},c_{n},d_{n})$. Show that   $$\lim_{n\to\infty}f_{n}=\infty$$ My idea: I think we first have to find $A^n$, and this has a nice solution? Thank you.","Let $A^n-I=\begin{bmatrix} a_{n}&b_{n}\\ c_{n}&d_{n} \end{bmatrix} $, where $A=\begin{bmatrix} 3&2\\ 4&3 \end{bmatrix}$, $I=\begin{bmatrix} 1&0\\ 0&1 \end{bmatrix}$ and let $f_{n}=\gcd(a_{n},b_{n},c_{n},d_{n})$. Show that   $$\lim_{n\to\infty}f_{n}=\infty$$ My idea: I think we first have to find $A^n$, and this has a nice solution? Thank you.",,"['linear-algebra', 'matrices', 'limits', 'contest-math']"
89,"If $X$ is an orthogonal matrix, why does $X^TX = I$?","If  is an orthogonal matrix, why does ?",X X^TX = I,"It's not immediately clear to me why this is true. My notes say that putting $n$ orthonormal vectors $ v_1, ..., v_n$ in the columns of $X$ gives $X^TX = I$, and it follows from this that the rows of $X$ are orthonormal. Can you explain this better to me? I tried the $2\times 2$ case, but it isn't clear to me why this is true.","It's not immediately clear to me why this is true. My notes say that putting $n$ orthonormal vectors $ v_1, ..., v_n$ in the columns of $X$ gives $X^TX = I$, and it follows from this that the rows of $X$ are orthonormal. Can you explain this better to me? I tried the $2\times 2$ case, but it isn't clear to me why this is true.",,"['linear-algebra', 'matrices', 'vector-spaces', 'inner-products']"
90,Nondiagonal $3 \times 3$ matrix,Nondiagonal  matrix,3 \times 3,"Can someone give an example of a nondiagonal, $3 \times 3$ matrix that is diagonalizable but is not invertible? Explanation would be appreciated.","Can someone give an example of a nondiagonal, $3 \times 3$ matrix that is diagonalizable but is not invertible? Explanation would be appreciated.",,"['matrices', 'diagonalization']"
91,Math hack for solving system of equations,Math hack for solving system of equations,,"Is it a ""standard"" Math/Numerical-Analysis hack to add a relatively small number e.g. 1*10E-5 to the diagonal of a squared matrix to ensure LU Decomposition (or whichever decomposition algorithm is applicable)?  As opposed to ""partially/totally pivoting""? [Edit] The problem: I am computing the regressors for a statistical regression model, and the constraints are that we can no longer bring in any open source software.  Hence I am drawing on my undergrad courses in Linear Algebra and Numerical Methods to implement the solution. I am currently using Croute's LU Decomposition algorithm ( http://en.wikipedia.org/wiki/Crout_matrix_decomposition ) with partial pivoting on the rows.","Is it a ""standard"" Math/Numerical-Analysis hack to add a relatively small number e.g. 1*10E-5 to the diagonal of a squared matrix to ensure LU Decomposition (or whichever decomposition algorithm is applicable)?  As opposed to ""partially/totally pivoting""? [Edit] The problem: I am computing the regressors for a statistical regression model, and the constraints are that we can no longer bring in any open source software.  Hence I am drawing on my undergrad courses in Linear Algebra and Numerical Methods to implement the solution. I am currently using Croute's LU Decomposition algorithm ( http://en.wikipedia.org/wiki/Crout_matrix_decomposition ) with partial pivoting on the rows.",,"['linear-algebra', 'matrices', 'numerical-methods', 'eigenvalues-eigenvectors', 'determinant']"
92,Why is this transformation's standard matrix the way it is?,Why is this transformation's standard matrix the way it is?,,"So I know for a simple rotation of $2$ vectors, I know that the vector $\left[\begin{matrix}1\\0\end{matrix}\right]$ rotates to $\left[\begin{matrix}\cos\phi\\\sin\phi\end{matrix}\right]$ and the vector $\left[\begin{matrix}0\\1\end{matrix}\right]$ rotates to $\left[\begin{matrix}-\sin\phi\\\cos\phi\end{matrix}\right]$. But why? I can see geometrically why the first vector rotates to where it does, but the second vector? Why? It just doesn't make sense to me.","So I know for a simple rotation of $2$ vectors, I know that the vector $\left[\begin{matrix}1\\0\end{matrix}\right]$ rotates to $\left[\begin{matrix}\cos\phi\\\sin\phi\end{matrix}\right]$ and the vector $\left[\begin{matrix}0\\1\end{matrix}\right]$ rotates to $\left[\begin{matrix}-\sin\phi\\\cos\phi\end{matrix}\right]$. But why? I can see geometrically why the first vector rotates to where it does, but the second vector? Why? It just doesn't make sense to me.",,"['linear-algebra', 'matrices']"
93,Tricky problem on skew-symmetric matrices,Tricky problem on skew-symmetric matrices,,"Problem: If $S$ is a skew-symmetric matrix, show that $(I+S)(I-S)^{-1}$ is orthogonal. This appeared on a list of standard questions asked of Princeton graduate students. It has been a while since I've studied linear algebra, and frankly I cannot even see why $(I-S)$ must be invertible.","Problem: If $S$ is a skew-symmetric matrix, show that $(I+S)(I-S)^{-1}$ is orthogonal. This appeared on a list of standard questions asked of Princeton graduate students. It has been a while since I've studied linear algebra, and frankly I cannot even see why $(I-S)$ must be invertible.",,"['linear-algebra', 'matrices']"
94,Solving matrix equation involving singular matrix,Solving matrix equation involving singular matrix,,"Given that $AB=AC$, $A=\begin{bmatrix} 3 & 6 \\ 1 & 2 \end{bmatrix}$ $B=\begin{bmatrix} 1 & 5 \\ 0 & 1 \end{bmatrix}$ Find a matrix $C$ whose elements are all non-zero and is $2\times 2$ matrix. I attempt to find the inverse of $A$ but it was unsuccessful as it is a singular matrix I have also attempted to solve the question $AB-AC=0$, $A(B-C)=0$ but still unsuccessful... Thank you","Given that $AB=AC$, $A=\begin{bmatrix} 3 & 6 \\ 1 & 2 \end{bmatrix}$ $B=\begin{bmatrix} 1 & 5 \\ 0 & 1 \end{bmatrix}$ Find a matrix $C$ whose elements are all non-zero and is $2\times 2$ matrix. I attempt to find the inverse of $A$ but it was unsuccessful as it is a singular matrix I have also attempted to solve the question $AB-AC=0$, $A(B-C)=0$ but still unsuccessful... Thank you",,['matrices']
95,Can an eigenvalue (of an $n$ by $n$ matrix A) with algebraic multiplicity $n$ have an eigenspace with fewer than $n$ dimensions?,Can an eigenvalue (of an  by  matrix A) with algebraic multiplicity  have an eigenspace with fewer than  dimensions?,n n n n,"Is it possible for a matrix with characteristic polynomial $(λ−a)^3$ to have an eigenline (one-dimensional eigenspace)? I know that geometric multiplicity can generally be smaller than algebraic multiplicity. But I was wondering if algebraic multiplicity $n$ might be a special case. This question is motivated by my earlier one The greatest possible geometric multiplicity of an eigenvalue , where I learnt that $A$ has an $n$-dimensional eigenspace iff. $A=\lambda I$.","Is it possible for a matrix with characteristic polynomial $(λ−a)^3$ to have an eigenline (one-dimensional eigenspace)? I know that geometric multiplicity can generally be smaller than algebraic multiplicity. But I was wondering if algebraic multiplicity $n$ might be a special case. This question is motivated by my earlier one The greatest possible geometric multiplicity of an eigenvalue , where I learnt that $A$ has an $n$-dimensional eigenspace iff. $A=\lambda I$.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
96,How to find a canonical member of an equivalence class of matrices under row and column swaps?,How to find a canonical member of an equivalence class of matrices under row and column swaps?,,"Call two matrices ""swap-equivalent"" if one matrix can be transformed into the other via some sequence of row swaps and column swaps. I'd like a computationally efficient algorithm that can transform a matrix into a canonical swap-equivalent matrix (so that all members of an equivalence class give the same result). I've been first sorting the rows and then sorting the columns, using the following comparison function: A list (row or column) $a$ is considered greater than a list $b$ if when sorted, $a$ comes before $b$ lexicographically, with ties broken by the lexicographic order of (unsorted) $a$ and $b$. For example: $\begin{matrix} 1 & 0 & 3 \\ 0 & 4 & 2 \\ 2 & 0 & 4 \end{matrix}$ becomes $\begin{matrix} 2 & 0 & 4 \\ 0 & 4 & 2 \\ 1 & 0 & 3 \end{matrix}$ after sorting the rows, and then $\begin{matrix} 4 & 0 & 2 \\ 2 & 4 & 0 \\ 3 & 0 & 1 \end{matrix}$ after sorting the columns. This is efficient enough, but I haven't been able to figure out if it's right. EDIT: Turns out this doesn't work. $\begin{matrix} 2 & 1 & 0 \\ 1 & 0 & 2 \end{matrix}$ gets transformed to $\begin{matrix} 2 & 0 & 1 \\ 1 & 2 & 0 \end{matrix}$ but $\begin{matrix} 2 & 1 & 0 \\ 0 & 2 & 1 \end{matrix}$ gets transformed to $\begin{matrix} 1 & 2 & 0 \\ 2 & 0 & 1 \end{matrix}$ even though they're equivalent. Sorting the rows again fixes it though.  Is that enough?  Maybe iterate until a stable state is reached?","Call two matrices ""swap-equivalent"" if one matrix can be transformed into the other via some sequence of row swaps and column swaps. I'd like a computationally efficient algorithm that can transform a matrix into a canonical swap-equivalent matrix (so that all members of an equivalence class give the same result). I've been first sorting the rows and then sorting the columns, using the following comparison function: A list (row or column) $a$ is considered greater than a list $b$ if when sorted, $a$ comes before $b$ lexicographically, with ties broken by the lexicographic order of (unsorted) $a$ and $b$. For example: $\begin{matrix} 1 & 0 & 3 \\ 0 & 4 & 2 \\ 2 & 0 & 4 \end{matrix}$ becomes $\begin{matrix} 2 & 0 & 4 \\ 0 & 4 & 2 \\ 1 & 0 & 3 \end{matrix}$ after sorting the rows, and then $\begin{matrix} 4 & 0 & 2 \\ 2 & 4 & 0 \\ 3 & 0 & 1 \end{matrix}$ after sorting the columns. This is efficient enough, but I haven't been able to figure out if it's right. EDIT: Turns out this doesn't work. $\begin{matrix} 2 & 1 & 0 \\ 1 & 0 & 2 \end{matrix}$ gets transformed to $\begin{matrix} 2 & 0 & 1 \\ 1 & 2 & 0 \end{matrix}$ but $\begin{matrix} 2 & 1 & 0 \\ 0 & 2 & 1 \end{matrix}$ gets transformed to $\begin{matrix} 1 & 2 & 0 \\ 2 & 0 & 1 \end{matrix}$ even though they're equivalent. Sorting the rows again fixes it though.  Is that enough?  Maybe iterate until a stable state is reached?",,"['matrices', 'algorithms', 'permutations']"
97,Find dimension when sum of the entries in the first row and the sum of the diagonal entries are both zero.,Find dimension when sum of the entries in the first row and the sum of the diagonal entries are both zero.,,"What is the dimension of the space of all $n \times n$ matrices with real entries   which are such that the sum of the entries in the first row and the sum of   the diagonal entries are both zero? I tried by finding number of independent entries. The number of independent entries on the diagonal is $n-1$. The number of upper triangular independent entries is $\frac{n(n-1)}{2}-1(n-1)$, and the number of lowertriangular independent entries is $\frac{n(n-1)}{2}$. Now adding them will give dimension. Am I right?","What is the dimension of the space of all $n \times n$ matrices with real entries   which are such that the sum of the entries in the first row and the sum of   the diagonal entries are both zero? I tried by finding number of independent entries. The number of independent entries on the diagonal is $n-1$. The number of upper triangular independent entries is $\frac{n(n-1)}{2}-1(n-1)$, and the number of lowertriangular independent entries is $\frac{n(n-1)}{2}$. Now adding them will give dimension. Am I right?",,"['linear-algebra', 'matrices']"
98,Can the product of two non-zero symmetric matrices be anti-symmetric?,Can the product of two non-zero symmetric matrices be anti-symmetric?,,"I'm trying to find an example to show that the product of two non-zero symmetric matrices can be anti-symmetric. I've proven that this is impossible for 2x2 matrices. For 3x3 matrices, I've formulated a set of linear equations in 12 variables and used MATLAB to try and find a solution, to no avail. So, is this possible, and if so, what is the best method to use to formulate an example? If not, what is the best way to prove that it is impossible for matrices of size n (n arbitrary natural number)? With very many thanks, Froskoy.","I'm trying to find an example to show that the product of two non-zero symmetric matrices can be anti-symmetric. I've proven that this is impossible for 2x2 matrices. For 3x3 matrices, I've formulated a set of linear equations in 12 variables and used MATLAB to try and find a solution, to no avail. So, is this possible, and if so, what is the best method to use to formulate an example? If not, what is the best way to prove that it is impossible for matrices of size n (n arbitrary natural number)? With very many thanks, Froskoy.",,"['linear-algebra', 'matrices']"
99,The compactness of the unit sphere in finite dimensional normed vector space,The compactness of the unit sphere in finite dimensional normed vector space,,"We define $ (\mathbb{R}^m, \|.\|)$ to be a finite dimensional normed vector space with $ \|.\|$ is defined to be any norm in $ \mathbb{R^m}$. Let $S = \lbrace x \in \mathbb{R}^m: \| x\| = 1 \rbrace.$ Prove that $S$ is compact in $ (\mathbb{R}^m, \|.\|).$","We define $ (\mathbb{R}^m, \|.\|)$ to be a finite dimensional normed vector space with $ \|.\|$ is defined to be any norm in $ \mathbb{R^m}$. Let $S = \lbrace x \in \mathbb{R}^m: \| x\| = 1 \rbrace.$ Prove that $S$ is compact in $ (\mathbb{R}^m, \|.\|).$",,"['real-analysis', 'linear-algebra', 'matrices', 'compactness']"
