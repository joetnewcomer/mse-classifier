,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Fractional part of $n\alpha$ is equidistributed,Fractional part of  is equidistributed,n\alpha,"Let $\alpha$ be an irrational number. Then the sequence $\{\{n\alpha\}\}$ is equidistributed. I am using the following definition of equidistribution. A sequence $\{a_i\}$ is equidistributed if $\frac1n\sum\limits_{i=1}^n f(a_i) \to \lambda(f):=\int_0^1 f(x)\,dx$ for all continuous $f:[0,1]\to \mathbb{R}_{\ge 0}$. In the proof I used the fact that if $T_g(x)=(g+x) \pmod 1$, then $\lambda(f\circ T_g)=\lambda(f)$ for all continuous $f:[0,1]\to \mathbb{R}_{\ge 0}$. Hence $\lambda$ is the unique Haar probability measure. Now suppose for $\theta=e^{2i\pi\alpha}$ I have proved $$\frac1n\sum\limits_{i=1}^n g(\theta^i) \to \hat{\lambda}(g)$$ for all continuous $g:T\to \mathbb{R}_{\ge 0}$ where $\hat{\lambda}$ is the unique haar measure in $T$ and $T$ is the circle group. Then how to deduce the fact that $n\alpha \pmod 1$ is equidistributed. Note that if we define the map $G: [0,1)\to T$ as $G(x)=e^{2i\pi x}$. Then $f=g\circ G$ does not covers all continuous functions.","Let $\alpha$ be an irrational number. Then the sequence $\{\{n\alpha\}\}$ is equidistributed. I am using the following definition of equidistribution. A sequence $\{a_i\}$ is equidistributed if $\frac1n\sum\limits_{i=1}^n f(a_i) \to \lambda(f):=\int_0^1 f(x)\,dx$ for all continuous $f:[0,1]\to \mathbb{R}_{\ge 0}$. In the proof I used the fact that if $T_g(x)=(g+x) \pmod 1$, then $\lambda(f\circ T_g)=\lambda(f)$ for all continuous $f:[0,1]\to \mathbb{R}_{\ge 0}$. Hence $\lambda$ is the unique Haar probability measure. Now suppose for $\theta=e^{2i\pi\alpha}$ I have proved $$\frac1n\sum\limits_{i=1}^n g(\theta^i) \to \hat{\lambda}(g)$$ for all continuous $g:T\to \mathbb{R}_{\ge 0}$ where $\hat{\lambda}$ is the unique haar measure in $T$ and $T$ is the circle group. Then how to deduce the fact that $n\alpha \pmod 1$ is equidistributed. Note that if we define the map $G: [0,1)\to T$ as $G(x)=e^{2i\pi x}$. Then $f=g\circ G$ does not covers all continuous functions.",,"['integration', 'measure-theory', 'lebesgue-measure', 'ergodic-theory', 'equidistribution']"
1,Convolution of measures on a measurable group is associative,Convolution of measures on a measurable group is associative,,"I've come across a statement in Kallenberg's Foundations of Modern Probability which claims this and only tells me to use Fubini's theorem. I am not very familiar with this topic and the text doesn't give any further detail, so I have several questions here: Do the measures need to be at least $\sigma$-finite (since the author tells me to use Fubini's)? Does the actual proof resemble this: $$ \begin{align} ((\mu_1 * \mu_2) * \mu_3)(B) &= \iint \chi_B(xz) (\mu_1*\mu_2)(dx) \mu_3(dz) \\ &= \iiint \chi_B(xyz) \mu_1(dx) \mu_2(dy) \mu_3(dz) \\ &= \iiint \chi_B(xyz) \mu_2(dy) \mu_3(dz) \mu_1(dx) \\ &= \iint \chi_B(xy) (\mu_2*\mu_3)(dy) \mu_1(dx) \\ &= \iint \chi_B(xy) (\mu_2*\mu_3)(dy) \mu_1(dx) \\& = \iint \chi_B(xy) \mu_1(dx)  (\mu_2*\mu_3)(dy) = (\mu_1 * (\mu_2 * \mu_3))(B) \end{align}$$ in which case it doesn't assume that the $\mu_i$ are invariant under the group operations at all. On the other hand, the answers in Convolution on group with measure claim that they need to be so, so am I misunderstanding this somehow?","I've come across a statement in Kallenberg's Foundations of Modern Probability which claims this and only tells me to use Fubini's theorem. I am not very familiar with this topic and the text doesn't give any further detail, so I have several questions here: Do the measures need to be at least $\sigma$-finite (since the author tells me to use Fubini's)? Does the actual proof resemble this: $$ \begin{align} ((\mu_1 * \mu_2) * \mu_3)(B) &= \iint \chi_B(xz) (\mu_1*\mu_2)(dx) \mu_3(dz) \\ &= \iiint \chi_B(xyz) \mu_1(dx) \mu_2(dy) \mu_3(dz) \\ &= \iiint \chi_B(xyz) \mu_2(dy) \mu_3(dz) \mu_1(dx) \\ &= \iint \chi_B(xy) (\mu_2*\mu_3)(dy) \mu_1(dx) \\ &= \iint \chi_B(xy) (\mu_2*\mu_3)(dy) \mu_1(dx) \\& = \iint \chi_B(xy) \mu_1(dx)  (\mu_2*\mu_3)(dy) = (\mu_1 * (\mu_2 * \mu_3))(B) \end{align}$$ in which case it doesn't assume that the $\mu_i$ are invariant under the group operations at all. On the other hand, the answers in Convolution on group with measure claim that they need to be so, so am I misunderstanding this somehow?",,"['group-theory', 'measure-theory']"
2,Definition of lebesgue integral with respect to measure $\mu$ [duplicate],Definition of lebesgue integral with respect to measure  [duplicate],\mu,"This question already has answers here : Lebesgue Integral of Non-Measurable Function (2 answers) Closed 8 years ago . In Rudin's Real and Complex Analysis, the Lebesgue integral is defined as: L et $(X,m,\mu)$ be a measure space, where $X$ is a set, $m$ is a $\sigma$ algebra on $X$ and $\mu$ is a measure. Then, if $f:X \to [0,\infty]$ and $E \in m$, we define $$\int_E f d\mu  = \sup \int_E s d\mu \tag{1}$$ where the supremum is taken over all simple functions $s, 0 \leq s \leq f$ I do not have much background in measure theory, and I am wondering why we assume $f$ to be measurable to define its integral. EVEN IF $f$ is not a measuarable function, the above definition (1) would still be well-defined. Why do we define integral only for measurable $f$?","This question already has answers here : Lebesgue Integral of Non-Measurable Function (2 answers) Closed 8 years ago . In Rudin's Real and Complex Analysis, the Lebesgue integral is defined as: L et $(X,m,\mu)$ be a measure space, where $X$ is a set, $m$ is a $\sigma$ algebra on $X$ and $\mu$ is a measure. Then, if $f:X \to [0,\infty]$ and $E \in m$, we define $$\int_E f d\mu  = \sup \int_E s d\mu \tag{1}$$ where the supremum is taken over all simple functions $s, 0 \leq s \leq f$ I do not have much background in measure theory, and I am wondering why we assume $f$ to be measurable to define its integral. EVEN IF $f$ is not a measuarable function, the above definition (1) would still be well-defined. Why do we define integral only for measurable $f$?",,"['measure-theory', 'lebesgue-integral']"
3,Does measurability really matter?,Does measurability really matter?,,"I am studying applied math  and I currently got stuck on proving that a function, which emerges in a model is measurable (Borel functon), so we can integrate it. I know, that there are examples of non-measurable sets w.r.t. the Lebesgue measure ( Vitali set ) so its characteristic(indicator) function will be non-measurable too. But my question is the following: Is there an example of real-life application where the verification of measurability really matters? For example a mechanism, for which we can try to derive its behavior (for example, stability properties), ignoring measurability check (just writing integrals mindlessly), but which do not follow our prediction exactly because we assumed some function within the model to be measurable but it is actually not? In the other words, I want historical ""proof"" of importance of this particular type of mathematical correctness.","I am studying applied math  and I currently got stuck on proving that a function, which emerges in a model is measurable (Borel functon), so we can integrate it. I know, that there are examples of non-measurable sets w.r.t. the Lebesgue measure ( Vitali set ) so its characteristic(indicator) function will be non-measurable too. But my question is the following: Is there an example of real-life application where the verification of measurability really matters? For example a mechanism, for which we can try to derive its behavior (for example, stability properties), ignoring measurability check (just writing integrals mindlessly), but which do not follow our prediction exactly because we assumed some function within the model to be measurable but it is actually not? In the other words, I want historical ""proof"" of importance of this particular type of mathematical correctness.",,"['real-analysis', 'measure-theory', 'lebesgue-measure', 'applications']"
4,"Concerning existence of subsequence of converging integrals on subsets of $[0,1]$ of a sequence $(f_n)\in[0,1]$",Concerning existence of subsequence of converging integrals on subsets of  of a sequence,"[0,1] (f_n)\in[0,1]","Problem Statement Let $\{f_n\}$ be a sequence of real-valued, measurable functions on $[0,1]$ that is uniformly bounded. Show that if $A$ is a Borel subset of $[0,1]$ then there exists subsequence $n_j$ such that $\int_A f_{n_j}(x) \ \mathrm{d}x$ converges. Show that if $(A_i)$ is a countable collection of Borel measurable subsets of $[0,1]$, then there exists a subsequence $n_j$ such that $\int_{A_i} f_{n_j}(x) \ \mathrm{d}x$ converges for each $i$. Show that there exists a subsequence $n_j$ such that $\int_A     f_{n_j}(x)$ converges for each Borel subset of $A$. Attempt At first I started thinking of using a diagonalization argument and to approach this problem step by step. Instead, I am wondering what might be wrong with the following naive approach. $f_n$ is uniformly bounded on $[0,1]$ so $\int_{[0,1]}f_n  \ \mathrm{d}x$ is an infinite sequence of real numbers on the compact set $[-2k,2k]$, where $|f_n|\leq k$. So there is a convergent subsequence $\int_{[0,1]}f_{n_k}\ \mathrm{d}x$. This subsequence also converges for any borel subset of $[0,1]$ so we have the result for all three of the above problems. Question What major concept(s) am I missing here? Note that I am not asking for a full solution to the problem but rather some feedback on my attempt at solving it. I'm sorry for this silly question but I find it hard to dig into a problem until I realize why my ""initial naive attempt"" fails.","Problem Statement Let $\{f_n\}$ be a sequence of real-valued, measurable functions on $[0,1]$ that is uniformly bounded. Show that if $A$ is a Borel subset of $[0,1]$ then there exists subsequence $n_j$ such that $\int_A f_{n_j}(x) \ \mathrm{d}x$ converges. Show that if $(A_i)$ is a countable collection of Borel measurable subsets of $[0,1]$, then there exists a subsequence $n_j$ such that $\int_{A_i} f_{n_j}(x) \ \mathrm{d}x$ converges for each $i$. Show that there exists a subsequence $n_j$ such that $\int_A     f_{n_j}(x)$ converges for each Borel subset of $A$. Attempt At first I started thinking of using a diagonalization argument and to approach this problem step by step. Instead, I am wondering what might be wrong with the following naive approach. $f_n$ is uniformly bounded on $[0,1]$ so $\int_{[0,1]}f_n  \ \mathrm{d}x$ is an infinite sequence of real numbers on the compact set $[-2k,2k]$, where $|f_n|\leq k$. So there is a convergent subsequence $\int_{[0,1]}f_{n_k}\ \mathrm{d}x$. This subsequence also converges for any borel subset of $[0,1]$ so we have the result for all three of the above problems. Question What major concept(s) am I missing here? Note that I am not asking for a full solution to the problem but rather some feedback on my attempt at solving it. I'm sorry for this silly question but I find it hard to dig into a problem until I realize why my ""initial naive attempt"" fails.",,"['measure-theory', 'lebesgue-integral']"
5,Nontrivial normed functional on the bounded functions from $\mathbb R^2$ into $\mathbb R$ invariant by isometries,Nontrivial normed functional on the bounded functions from  into  invariant by isometries,\mathbb R^2 \mathbb R,"I am trying to show that there exists a nontrivial normed functional on $\mathbb R^2$ invariant by isometries. That is: If $A$ is any set, let $\mathcal B_{A}=\{f: \mathbb R^2 \rightarrow \mathbb R: f \text{ is bounded}\}$. Let $\|f\|=\sup\{|f(x)|: x \in  A\}$. Notice that we are dealing with a vector space. I am looking for a linear functional satisfying: $I$ is normed, that is, $|I(f)|\leq \|f\|$ $I$ is nontrivial, that is, $I(1)=1$ If $s \in \mathbb R^2$, let $f_s(x)=f(x+s)$ for every $x \in X$ and let $f^-(x)=f(\bar x)$ (complex conjugation). There are the translations and reflections. If $|t|=1$, let $f^t(x)=f(tx)$ (complex product). We want that $I(f)=I(f^-)=I(f_s)=I(f^t)$. These are the rotations. I have already proved that there exists $I_0$ satisfies everything but the rotation thing. Let $T=\{z: |z|=1\}$. I also know that there exists a nontrivial normed linear functional $I_1: \mathcal B_T \rightarrow \mathbb R$ satisfying $I_1(f^t)=I(f)$ for every $t \in \mathbb T$, $f \in \mathcal B_T$. The book I am following tells me to define $I(f)=I_1(G_f)$, where $G_f: \mathcal B_t \rightarrow \mathbb R$ is given by $G_f(t)=I_0(f^t)$. This is a bounded function since $\|f^t\|=\|f\|$. It's easy to see that $I$ is a nontrivial normed linear functional. I have verified that $I$ is invariant by rotations and by translations, but I'm stuck on showing $I$ is invariant by the reflection. My attempt to show it was: Notice that $(f^-)^t(x)=f(\bar t \bar x)=(f^{\bar t})^-(x)$. Therefore, $G_{\bar f}(t)=G_f(\bar t)$ since $I_0$ is invariant by $\,^-$. I don't know how to proceed from here. I'm tagging this question as measure theory since I need to prove this in order to define a nice measure on $\mathbb R^2$.","I am trying to show that there exists a nontrivial normed functional on $\mathbb R^2$ invariant by isometries. That is: If $A$ is any set, let $\mathcal B_{A}=\{f: \mathbb R^2 \rightarrow \mathbb R: f \text{ is bounded}\}$. Let $\|f\|=\sup\{|f(x)|: x \in  A\}$. Notice that we are dealing with a vector space. I am looking for a linear functional satisfying: $I$ is normed, that is, $|I(f)|\leq \|f\|$ $I$ is nontrivial, that is, $I(1)=1$ If $s \in \mathbb R^2$, let $f_s(x)=f(x+s)$ for every $x \in X$ and let $f^-(x)=f(\bar x)$ (complex conjugation). There are the translations and reflections. If $|t|=1$, let $f^t(x)=f(tx)$ (complex product). We want that $I(f)=I(f^-)=I(f_s)=I(f^t)$. These are the rotations. I have already proved that there exists $I_0$ satisfies everything but the rotation thing. Let $T=\{z: |z|=1\}$. I also know that there exists a nontrivial normed linear functional $I_1: \mathcal B_T \rightarrow \mathbb R$ satisfying $I_1(f^t)=I(f)$ for every $t \in \mathbb T$, $f \in \mathcal B_T$. The book I am following tells me to define $I(f)=I_1(G_f)$, where $G_f: \mathcal B_t \rightarrow \mathbb R$ is given by $G_f(t)=I_0(f^t)$. This is a bounded function since $\|f^t\|=\|f\|$. It's easy to see that $I$ is a nontrivial normed linear functional. I have verified that $I$ is invariant by rotations and by translations, but I'm stuck on showing $I$ is invariant by the reflection. My attempt to show it was: Notice that $(f^-)^t(x)=f(\bar t \bar x)=(f^{\bar t})^-(x)$. Therefore, $G_{\bar f}(t)=G_f(\bar t)$ since $I_0$ is invariant by $\,^-$. I don't know how to proceed from here. I'm tagging this question as measure theory since I need to prove this in order to define a nice measure on $\mathbb R^2$.",,"['linear-algebra', 'measure-theory']"
6,Minimum Knowledges to precisely calculate PDEs (integral equations),Minimum Knowledges to precisely calculate PDEs (integral equations),,"Basically all I want to do is to calculate (or prove) precisely equations such as  $$ \frac{1}{n\alpha(n)r^{n-1}}\int_{\partial B(x,r)} u(y) dS(y) =  \frac{1}{n\alpha(n)} \int_{\partial B(x,r)} u(x+rz) dS(z). $$ (p. 26, Evans' PDE, 1st edition) where $u \in C^2 (U)$. There are quite rough discussions about it in the same spirit, one by DG, and the other by Hausdorff measure. But both are quite rough to me and I'm having trouble following the steps. So my questions are (1) both are really rigorous? (2) Will studying the textbooks they mention suffice? (for example, for DG approach, I may study Lee's SM and for Hausdorff measure approach I study Evans' measure textbook) Also refer","Basically all I want to do is to calculate (or prove) precisely equations such as  $$ \frac{1}{n\alpha(n)r^{n-1}}\int_{\partial B(x,r)} u(y) dS(y) =  \frac{1}{n\alpha(n)} \int_{\partial B(x,r)} u(x+rz) dS(z). $$ (p. 26, Evans' PDE, 1st edition) where $u \in C^2 (U)$. There are quite rough discussions about it in the same spirit, one by DG, and the other by Hausdorff measure. But both are quite rough to me and I'm having trouble following the steps. So my questions are (1) both are really rigorous? (2) Will studying the textbooks they mention suffice? (for example, for DG approach, I may study Lee's SM and for Hausdorff measure approach I study Evans' measure textbook) Also refer",,"['measure-theory', 'differential-geometry', 'reference-request', 'partial-differential-equations', 'soft-question']"
7,Transformation shift measurable,Transformation shift measurable,,"How to prove that this transformation is measurable? $\sigma:B(n)\rightarrow{B(n)} $ $\sigma(x)(k)=x(k+1)$ $\sigma(...,x_{-1},x_{0},x_{1},...)=(...,x_{0},x_{1},x_{2},...)$ where $B(n)$ with product topology of $Y=\left\{{0,1,2,...,n-1}\right\}$, i.e $B(n)=Y^{\mathbb{Z}}$ and $B(n)$ measurable space with $\sigma$-algebra generated by the base of all cylinders. Progress . First, this transformation shift $\sigma$ prove is continuous, because all function in topological spaces is measurable. But the form of cylinders is very cumbersome. Second,I think in use a Sub basis of $Y^{Z}$ ,for example if $\left\{B_{j}\right\}$ basis for $Y$, a sub basis of that topology  $Y^{Z}$ are $U_{i,j}=\pi_i^{-1}(B_j)$ $i\in I,j\in J$, but no more. Where $I =Y$.","How to prove that this transformation is measurable? $\sigma:B(n)\rightarrow{B(n)} $ $\sigma(x)(k)=x(k+1)$ $\sigma(...,x_{-1},x_{0},x_{1},...)=(...,x_{0},x_{1},x_{2},...)$ where $B(n)$ with product topology of $Y=\left\{{0,1,2,...,n-1}\right\}$, i.e $B(n)=Y^{\mathbb{Z}}$ and $B(n)$ measurable space with $\sigma$-algebra generated by the base of all cylinders. Progress . First, this transformation shift $\sigma$ prove is continuous, because all function in topological spaces is measurable. But the form of cylinders is very cumbersome. Second,I think in use a Sub basis of $Y^{Z}$ ,for example if $\left\{B_{j}\right\}$ basis for $Y$, a sub basis of that topology  $Y^{Z}$ are $U_{i,j}=\pi_i^{-1}(B_j)$ $i\in I,j\in J$, but no more. Where $I =Y$.",,['measure-theory']
8,Pointwise Convergence: No Diagonal Subsequence Exists?,Pointwise Convergence: No Diagonal Subsequence Exists?,,"Can anyone find a sequence of arbitrary functions $f_n : \mathbb{R} \to \mathbb{R}$ that converge pointwise to an arbitrary function $f : \mathbb{R} \to \mathbb{R}$, such that for each $n$, there is a sequence of arbitrary functions $f^{k}_{n}: \mathbb{R} \to \mathbb{R}$ that converges pointwise to $f_n$ as $k$ goes to $\infty$, but such that no subsequence of the $f^{k}_{n}$'s converges to $f$? Alternatively, can it be shown that such an example is impossible? (These functions cannot be measurable, otherwise there is a simple argument that shows that this cannot work.) The question arises as follows: suppose $A$ is set of functions $\mathbb{R} \to \mathbb{R}$, $B$ is a subset of $A$ such that every function in $A$ is the pointwise limit of functions from $B$, and $C$ is a subset of $B$ such that every function in $B$ is the pointwise limit of functions from $C$. Is every function in $A$ a pointwise limit of functions from $C$? The obvious approach is to use a `diagonal subsequence' argument. Note that this is not the same as showing that a dense subset of a dense subset is dense in the whole space in the topological sense, since the topology of pointwise convergence is not first-countable, and so the sequential closure differs from the `setwise' closure.","Can anyone find a sequence of arbitrary functions $f_n : \mathbb{R} \to \mathbb{R}$ that converge pointwise to an arbitrary function $f : \mathbb{R} \to \mathbb{R}$, such that for each $n$, there is a sequence of arbitrary functions $f^{k}_{n}: \mathbb{R} \to \mathbb{R}$ that converges pointwise to $f_n$ as $k$ goes to $\infty$, but such that no subsequence of the $f^{k}_{n}$'s converges to $f$? Alternatively, can it be shown that such an example is impossible? (These functions cannot be measurable, otherwise there is a simple argument that shows that this cannot work.) The question arises as follows: suppose $A$ is set of functions $\mathbb{R} \to \mathbb{R}$, $B$ is a subset of $A$ such that every function in $A$ is the pointwise limit of functions from $B$, and $C$ is a subset of $B$ such that every function in $B$ is the pointwise limit of functions from $C$. Is every function in $A$ a pointwise limit of functions from $C$? The obvious approach is to use a `diagonal subsequence' argument. Note that this is not the same as showing that a dense subset of a dense subset is dense in the whole space in the topological sense, since the topology of pointwise convergence is not first-countable, and so the sequential closure differs from the `setwise' closure.",,"['real-analysis', 'general-topology', 'measure-theory', 'examples-counterexamples']"
9,subset of a compact set in $\mathbb{R}$ with nonempty interior has positive outer measure,subset of a compact set in  with nonempty interior has positive outer measure,\mathbb{R},"Let $A\subset I=[a,b] \subset \mathbb{R}$, $a < b$ such that Int$(A) \neq \emptyset$. Show that $A$ has positive outer measure. What I have so far: Since Int$(A) \subseteq A$, by the monotonicity property of the outer measure we have $m^*(\text{Int}(A)) \leq m^*(A)$. Since Int$(A)$ is a nonempty open set in $\mathbb{R}$, Int$(A)$ is uncountable, thus $m^*(\text{Int}(A)) > 0$.","Let $A\subset I=[a,b] \subset \mathbb{R}$, $a < b$ such that Int$(A) \neq \emptyset$. Show that $A$ has positive outer measure. What I have so far: Since Int$(A) \subseteq A$, by the monotonicity property of the outer measure we have $m^*(\text{Int}(A)) \leq m^*(A)$. Since Int$(A)$ is a nonempty open set in $\mathbb{R}$, Int$(A)$ is uncountable, thus $m^*(\text{Int}(A)) > 0$.",,"['measure-theory', 'proof-verification']"
10,Characterizing a union of intervals.,Characterizing a union of intervals.,,"Find the measure of the set of real numbers in $(0,1)$ whose binary expansions contains zeroes in the odd positions, such that $x = 0.k_1k_2k_3\ldots$ Checking odd positions one at a time: If $k_1 = 0, x \in (0,\frac12)$. If $k_1,k_2 = 0, x \in (0,\frac{1}{8}) \cup (\frac{2}{8},\frac38)$. If $k_1,k_2,k_3 = 0, x \in (0,\frac{1}{32}) \cup (\frac{2}{32},\frac{3}{32}) \cup (\frac{8}{32},\frac{9}{32}) \cup (\frac{10}{32}.\frac{11}{32})$. I'm at a loss for characterizing this in a more concise way, akin to the Cantor set, to the union containing all $x$ such that $k_1,k_2,\ldots,k_n = 0$.","Find the measure of the set of real numbers in $(0,1)$ whose binary expansions contains zeroes in the odd positions, such that $x = 0.k_1k_2k_3\ldots$ Checking odd positions one at a time: If $k_1 = 0, x \in (0,\frac12)$. If $k_1,k_2 = 0, x \in (0,\frac{1}{8}) \cup (\frac{2}{8},\frac38)$. If $k_1,k_2,k_3 = 0, x \in (0,\frac{1}{32}) \cup (\frac{2}{32},\frac{3}{32}) \cup (\frac{8}{32},\frac{9}{32}) \cup (\frac{10}{32}.\frac{11}{32})$. I'm at a loss for characterizing this in a more concise way, akin to the Cantor set, to the union containing all $x$ such that $k_1,k_2,\ldots,k_n = 0$.",,"['measure-theory', 'cantor-set']"
11,Convergence in measure of product of convergent sequences [duplicate],Convergence in measure of product of convergent sequences [duplicate],,"This question already has answers here : Convergence in measure - product (4 answers) Closed 3 years ago . Let $(X,\Sigma,\mu)$ be a finite measurable space ($\mu(X)<\infty$). Suppose $f_n \xrightarrow{\mu} f$ and $g_n \xrightarrow{\mu} f$, prove that $f_ng_n \xrightarrow{\mu} fg$ I'll write what I could do up to now: Let $\lambda>0$, then $$\lambda<|f_ng_n(x)-fg(x)|$$$$=|f_ng_n-fg_n+fg_n-fg|$$$$\leq|g_n(x)||f_n(x)-f(x)|+|f(x)||g_n(x)-g(x)|$$ Let $S=\{x \in X: |f_ng_n(x)-fg(x)|>\lambda\}$, then $S \subset \{x \in X:|g_n(x)||f_n(x)-f(x)|>\dfrac{\lambda}{2}\} \cup  \{x \in X:|f(x)||g_n(x)-g(x)|>\dfrac{\lambda}{2}\}$ If I call $S_1$ and $S_2$ to the first set and second sets of the union respectively, then given $N>0$, $$S_1 \subset A_1 \cup A_2,$$where $$A_1=  \{|g_n| \geq N\dfrac{\sqrt{\lambda}}{4}\} \cap\{|f_n-f|<N^{-1}\dfrac{\sqrt{\lambda}}{4}\}$$ and$$A_2= \{|f_n-f| \geq N\dfrac{\sqrt{\lambda}}{4}\} \cap\{|g_n|<N^{-1}\dfrac{\sqrt{\lambda}}{4}\}$$ Similarly, $S_2 \subset B_1 \cup B_2$ with $$B_1= \{|f| \geq N\dfrac{\sqrt{\lambda}}{4}\} \cap\{|g_n-g|<N^{-1}\dfrac{\sqrt{\lambda}}{4}\}$$ and $$B_2=\{|g_n-g| \geq N\dfrac{\sqrt{\lambda}}{4}\} \cap\{|f|<N^{-1}\dfrac{\sqrt{\lambda}}{4}\}$$ I know how to find a bound for $A_2$ and $B_2$ using the fact that $f_n \xrightarrow{\mu} f$ and $g_n \xrightarrow{\mu} g$, but what about $A_1$ (or $B_1$)? Thanks in advance","This question already has answers here : Convergence in measure - product (4 answers) Closed 3 years ago . Let $(X,\Sigma,\mu)$ be a finite measurable space ($\mu(X)<\infty$). Suppose $f_n \xrightarrow{\mu} f$ and $g_n \xrightarrow{\mu} f$, prove that $f_ng_n \xrightarrow{\mu} fg$ I'll write what I could do up to now: Let $\lambda>0$, then $$\lambda<|f_ng_n(x)-fg(x)|$$$$=|f_ng_n-fg_n+fg_n-fg|$$$$\leq|g_n(x)||f_n(x)-f(x)|+|f(x)||g_n(x)-g(x)|$$ Let $S=\{x \in X: |f_ng_n(x)-fg(x)|>\lambda\}$, then $S \subset \{x \in X:|g_n(x)||f_n(x)-f(x)|>\dfrac{\lambda}{2}\} \cup  \{x \in X:|f(x)||g_n(x)-g(x)|>\dfrac{\lambda}{2}\}$ If I call $S_1$ and $S_2$ to the first set and second sets of the union respectively, then given $N>0$, $$S_1 \subset A_1 \cup A_2,$$where $$A_1=  \{|g_n| \geq N\dfrac{\sqrt{\lambda}}{4}\} \cap\{|f_n-f|<N^{-1}\dfrac{\sqrt{\lambda}}{4}\}$$ and$$A_2= \{|f_n-f| \geq N\dfrac{\sqrt{\lambda}}{4}\} \cap\{|g_n|<N^{-1}\dfrac{\sqrt{\lambda}}{4}\}$$ Similarly, $S_2 \subset B_1 \cup B_2$ with $$B_1= \{|f| \geq N\dfrac{\sqrt{\lambda}}{4}\} \cap\{|g_n-g|<N^{-1}\dfrac{\sqrt{\lambda}}{4}\}$$ and $$B_2=\{|g_n-g| \geq N\dfrac{\sqrt{\lambda}}{4}\} \cap\{|f|<N^{-1}\dfrac{\sqrt{\lambda}}{4}\}$$ I know how to find a bound for $A_2$ and $B_2$ using the fact that $f_n \xrightarrow{\mu} f$ and $g_n \xrightarrow{\mu} g$, but what about $A_1$ (or $B_1$)? Thanks in advance",,"['real-analysis', 'measure-theory']"
12,Prove that $\int (\delta x)=\delta^{-d} \int f$,Prove that,\int (\delta x)=\delta^{-d} \int f,"Let $f$ be a real-valued integrable function on $\mathbb{R}^d$. Prove that $$\int f(\delta x) = \delta^{-d} \int f.$$ I let $f(x)=\chi_E(x)=\begin{cases} 1 & \text{if }\delta x \in E \\ 0 & \text{if }\delta x \in E \end{cases} = \begin{cases} 1 & \text{if } x \in \delta^{-1} E \\ 0 & \text{if } x \in \delta^{-1}E \end{cases}$. Putting this together, I start with \begin{align*} m(\delta^{-1}E)&=\delta^{-d}m(E) \\ \sum_{k=1}^N a_k m(\delta^{-1}E)&=\delta^{-d} \sum_{k=1}^Na_km(E) \\ \end{align*} And as $N \to \infty$,  \begin{align*} \int \chi_{\delta^{-1}E}(x)&= \delta^{-d} \int \chi_E(x) \\ \int f(\delta x) &= \delta^{-d} \int f(x). \end{align*} Is this correct?","Let $f$ be a real-valued integrable function on $\mathbb{R}^d$. Prove that $$\int f(\delta x) = \delta^{-d} \int f.$$ I let $f(x)=\chi_E(x)=\begin{cases} 1 & \text{if }\delta x \in E \\ 0 & \text{if }\delta x \in E \end{cases} = \begin{cases} 1 & \text{if } x \in \delta^{-1} E \\ 0 & \text{if } x \in \delta^{-1}E \end{cases}$. Putting this together, I start with \begin{align*} m(\delta^{-1}E)&=\delta^{-d}m(E) \\ \sum_{k=1}^N a_k m(\delta^{-1}E)&=\delta^{-d} \sum_{k=1}^Na_km(E) \\ \end{align*} And as $N \to \infty$,  \begin{align*} \int \chi_{\delta^{-1}E}(x)&= \delta^{-d} \int \chi_E(x) \\ \int f(\delta x) &= \delta^{-d} \int f(x). \end{align*} Is this correct?",,"['real-analysis', 'measure-theory', 'proof-verification', 'lebesgue-integral']"
13,A strange Jensen's inequality for function of two variables,A strange Jensen's inequality for function of two variables,,"I am reading a paper where they use implicitly the following ""Jensen's inequality which i find quite strange. Moreover i did not find this result in any textbook so, i would like an opinion before i cite their result. Here is the ""Jensen's inequality"" they use : $f : \mathbb{R}^N \times \mathbb{R}^N \to \mathbb{R}$ a measurable map. $X$,$Y$ two measurable maps $X,Y : \Omega \to \mathbb{R}^N$ on a probability space $(\Omega,\mathcal{F}, \mathbb{P})$. $\mathcal{B}$ a sigma field (whose elements are in $\mathcal{F}$). Assuming for all $x$ $$y\to g(x,y)$$ is convex, is it true that $$E[f(X,Y) |\mathcal{B}] \geq f(X,E[Y|\mathcal{B}])$$ ? How can this be proved ? Do you have a counter example","I am reading a paper where they use implicitly the following ""Jensen's inequality which i find quite strange. Moreover i did not find this result in any textbook so, i would like an opinion before i cite their result. Here is the ""Jensen's inequality"" they use : $f : \mathbb{R}^N \times \mathbb{R}^N \to \mathbb{R}$ a measurable map. $X$,$Y$ two measurable maps $X,Y : \Omega \to \mathbb{R}^N$ on a probability space $(\Omega,\mathcal{F}, \mathbb{P})$. $\mathcal{B}$ a sigma field (whose elements are in $\mathcal{F}$). Assuming for all $x$ $$y\to g(x,y)$$ is convex, is it true that $$E[f(X,Y) |\mathcal{B}] \geq f(X,E[Y|\mathcal{B}])$$ ? How can this be proved ? Do you have a counter example",,"['probability', 'measure-theory']"
14,Defining Lebesgue measure on a subspace of $\mathbb{R}^n$,Defining Lebesgue measure on a subspace of,\mathbb{R}^n,"Let $\bar{w}_1,.., \bar{w}_k$ be linearly independent vectors in $\mathbb{R}^n$. Let $W$ be the subspace spanned by these $\bar{w}_i$'s. I know how the Lebesgue measure is defined on $\mathbb{R}^n$. How does one define Lebesgue measure on $W$ and assign volumes to measurable subsets of $W$? My attempt for clarification by an example: In $\mathbb{R}^3$, take $\bar{e}_1 = [1,0,0]$ and $\bar{e}_2 = [0,1,0]$. Let $V = Span \{\bar{e}_1,\bar{e}_2 \}$. The Lebesgue measure $m$ on $\mathbb{R}^3$ computes the volume of measurable sets in $\mathbb{R}^3$ and it gives $m(V) = 0$. But we can also define Lebesgue measure on $V$, which computes the area of measurable subsets of $V$. My question is about how one can generalize this example from $\mathbb{R}^3$ to $\mathbb{R}^n$ and to subspace $W$. PS This came up when I was reading something related to geometry of numbers where they computed volumes of certain parallelepipeds in the lattice of a subspace of $\mathbb{R}^n$. And I was wondering how it worked. Thank you! PPS I would also appreciate any reference.","Let $\bar{w}_1,.., \bar{w}_k$ be linearly independent vectors in $\mathbb{R}^n$. Let $W$ be the subspace spanned by these $\bar{w}_i$'s. I know how the Lebesgue measure is defined on $\mathbb{R}^n$. How does one define Lebesgue measure on $W$ and assign volumes to measurable subsets of $W$? My attempt for clarification by an example: In $\mathbb{R}^3$, take $\bar{e}_1 = [1,0,0]$ and $\bar{e}_2 = [0,1,0]$. Let $V = Span \{\bar{e}_1,\bar{e}_2 \}$. The Lebesgue measure $m$ on $\mathbb{R}^3$ computes the volume of measurable sets in $\mathbb{R}^3$ and it gives $m(V) = 0$. But we can also define Lebesgue measure on $V$, which computes the area of measurable subsets of $V$. My question is about how one can generalize this example from $\mathbb{R}^3$ to $\mathbb{R}^n$ and to subspace $W$. PS This came up when I was reading something related to geometry of numbers where they computed volumes of certain parallelepipeds in the lattice of a subspace of $\mathbb{R}^n$. And I was wondering how it worked. Thank you! PPS I would also appreciate any reference.",,"['measure-theory', 'reference-request', 'lebesgue-measure']"
15,Borel $\sigma$-algebra,Borel -algebra,\sigma,"Since the Borel $\sigma$-algebra is generated by the family of open sets, does that mean that every Borel set is essentially some countable union/intersection of open sets or a complement of open sets? The idea is quite similar to the concept of generated field extensions in field theory, where the field $K(\alpha)$ is a field extension  generated by $\alpha$ over $K$. and we see that every element is essentially a quotient of linear combinations of powers of $\alpha$.","Since the Borel $\sigma$-algebra is generated by the family of open sets, does that mean that every Borel set is essentially some countable union/intersection of open sets or a complement of open sets? The idea is quite similar to the concept of generated field extensions in field theory, where the field $K(\alpha)$ is a field extension  generated by $\alpha$ over $K$. and we see that every element is essentially a quotient of linear combinations of powers of $\alpha$.",,['measure-theory']
16,"For $f\in L^1_{loc} (\Omega)$, $f=0$ almost everywhere in $\Omega$ provided $\int_{\Omega}f(x)\Phi (x)dx=0 , \forall \Phi \in C_{c}^{\infty}(\Omega)$","For ,  almost everywhere in  provided","f\in L^1_{loc} (\Omega) f=0 \Omega \int_{\Omega}f(x)\Phi (x)dx=0 , \forall \Phi \in C_{c}^{\infty}(\Omega)","I need to show that $f=0$ almost everywhere in $\Omega$ provided $$\int_{\Omega}f(x)\Phi (x)dx=0 , \forall \Phi \in C_{c}^{\infty}(\Omega)$$ Here is how I have decided to proceed. Suppose there exists some measurable set $\bar{\Omega} \subseteq \Omega$ such that $f\neq0$ in $\bar{\Omega}$. I want to show that $\int_{\Omega}f(x)\Phi (x)dx\neq0$. My problem is coming from the construction of $\Phi$. It has been suggested to me that I approximate $\Phi(x)=\chi_{\bar{\Omega}} $ (indicator function) using convolution. So let $\Phi_{\epsilon}=\int_{\Omega}\eta_{\epsilon}(x-y)\Phi(y)dy$ I have that $\Phi_{\epsilon} \to \Phi$ a.e. as $\epsilon \to 0$ and that $\Phi_{\epsilon} \in C_{c}^{\infty}(\Omega_{\epsilon}), \forall \epsilon>0$ where $\Omega_{\epsilon}$ is the set with an $\epsilon$-slice removed near the boundary. Now, \begin{align} \int_{\Omega}f(x)\Phi_\epsilon (x)dx &=\int_{\Omega-\bar{\Omega}}f(x)\Phi_\epsilon (x)dx+\int_{\bar{\Omega}}f(x)\Phi_\epsilon (x)dx  \end{align} This is where I get stuck. I know the second integral is non-zero, and the first one should go to zero, but I am not sure how to finish off the proof here. Do I take the limit as $\epsilon \to 0$ ? What are things that stop me from just taking this limit? Thanks for any help! I've been stuck on this for a while.","I need to show that $f=0$ almost everywhere in $\Omega$ provided $$\int_{\Omega}f(x)\Phi (x)dx=0 , \forall \Phi \in C_{c}^{\infty}(\Omega)$$ Here is how I have decided to proceed. Suppose there exists some measurable set $\bar{\Omega} \subseteq \Omega$ such that $f\neq0$ in $\bar{\Omega}$. I want to show that $\int_{\Omega}f(x)\Phi (x)dx\neq0$. My problem is coming from the construction of $\Phi$. It has been suggested to me that I approximate $\Phi(x)=\chi_{\bar{\Omega}} $ (indicator function) using convolution. So let $\Phi_{\epsilon}=\int_{\Omega}\eta_{\epsilon}(x-y)\Phi(y)dy$ I have that $\Phi_{\epsilon} \to \Phi$ a.e. as $\epsilon \to 0$ and that $\Phi_{\epsilon} \in C_{c}^{\infty}(\Omega_{\epsilon}), \forall \epsilon>0$ where $\Omega_{\epsilon}$ is the set with an $\epsilon$-slice removed near the boundary. Now, \begin{align} \int_{\Omega}f(x)\Phi_\epsilon (x)dx &=\int_{\Omega-\bar{\Omega}}f(x)\Phi_\epsilon (x)dx+\int_{\bar{\Omega}}f(x)\Phi_\epsilon (x)dx  \end{align} This is where I get stuck. I know the second integral is non-zero, and the first one should go to zero, but I am not sure how to finish off the proof here. Do I take the limit as $\epsilon \to 0$ ? What are things that stop me from just taking this limit? Thanks for any help! I've been stuck on this for a while.",,"['real-analysis', 'measure-theory', 'partial-differential-equations']"
17,Marginals of (not necessarily finite) measures,Marginals of (not necessarily finite) measures,,"Consider a product of two measurable spaces, $(X,\mathcal{A})$ and $(Y,\mathcal{B})$, and a (not necessarily finite) measure, $\varrho$ on the product space $(X \times Y, \mathcal{A} \otimes \mathcal{B})$. I wonder, if there is any general notion for the ""marginals"" of $\varrho$, so if there is a way to find (or at least prove the existence of) a measure $\mu$ on $(X, \mathcal{A})$ and a measure $\nu$ on $(Y, \mathcal{B})$, for which $\varrho$ equals the product measure $\mu \times \nu$. I need it the case when $\varrho$ is atom-less and $\sigma$-finite, although I would be interested in the general case as well. (By the way, I just realised, that if there are such marginals, they can't be unique: actually, if I multiply $\mu$ with a positive constant $c$, and divide $\nu$ with the same $c$ constant, the results will still be marginals of $\varrho$. Although, I guess, they are uniqe, up to this manipulation.)","Consider a product of two measurable spaces, $(X,\mathcal{A})$ and $(Y,\mathcal{B})$, and a (not necessarily finite) measure, $\varrho$ on the product space $(X \times Y, \mathcal{A} \otimes \mathcal{B})$. I wonder, if there is any general notion for the ""marginals"" of $\varrho$, so if there is a way to find (or at least prove the existence of) a measure $\mu$ on $(X, \mathcal{A})$ and a measure $\nu$ on $(Y, \mathcal{B})$, for which $\varrho$ equals the product measure $\mu \times \nu$. I need it the case when $\varrho$ is atom-less and $\sigma$-finite, although I would be interested in the general case as well. (By the way, I just realised, that if there are such marginals, they can't be unique: actually, if I multiply $\mu$ with a positive constant $c$, and divide $\nu$ with the same $c$ constant, the results will still be marginals of $\varrho$. Although, I guess, they are uniqe, up to this manipulation.)",,['measure-theory']
18,"Show: $\int f \, d\nu = \int f \, d\mu$ for continuous $f$ implies $\nu=\mu$",Show:  for continuous  implies,"\int f \, d\nu = \int f \, d\mu f \nu=\mu","Consider two $\sigma$-finite measures $\mu$ and $\nu$, both defined on $(\mathbb{R}^{k},\mathscr{B}(\mathbb{R}^{k}))$. Prove that if $\int{f}d\mu=\int{f}d \nu$ for all continuous functions $f:\mathbb{R}^{k}\rightarrow \mathbb{R}$, then $\mu=\nu$. Can someone give me hints?","Consider two $\sigma$-finite measures $\mu$ and $\nu$, both defined on $(\mathbb{R}^{k},\mathscr{B}(\mathbb{R}^{k}))$. Prove that if $\int{f}d\mu=\int{f}d \nu$ for all continuous functions $f:\mathbb{R}^{k}\rightarrow \mathbb{R}$, then $\mu=\nu$. Can someone give me hints?",,['measure-theory']
19,"What are the $SL(2, \mathbb{R})$-invariant Baire measures on $\mathbb{R}^2$?",What are the -invariant Baire measures on ?,"SL(2, \mathbb{R}) \mathbb{R}^2","This was an interesting problem that came up in qual studying. $SL(2, \mathbb{R})$ acts on $\mathbb{R}^2$, and hence on any measure on $\mathbb{R}^2$. What are the Baire measures (i.e. Borel measures finite on compact sets) that are invariant under this action? Two obvious ones are the Lebesgue measure, and delta masses at the origin. I suspect that there are none which are not a linear combination of these. Intuitively, it seems like if we can find some set which has very small Lebesgue measure, then we should be able to squeeze many $SL(2, \mathbb{R})$-orbits of it into a compact set, but I haven't been able to formalize this.","This was an interesting problem that came up in qual studying. $SL(2, \mathbb{R})$ acts on $\mathbb{R}^2$, and hence on any measure on $\mathbb{R}^2$. What are the Baire measures (i.e. Borel measures finite on compact sets) that are invariant under this action? Two obvious ones are the Lebesgue measure, and delta masses at the origin. I suspect that there are none which are not a linear combination of these. Intuitively, it seems like if we can find some set which has very small Lebesgue measure, then we should be able to squeeze many $SL(2, \mathbb{R})$-orbits of it into a compact set, but I haven't been able to formalize this.",,"['real-analysis', 'measure-theory', 'group-actions']"
20,Upper Lebesgue sum with a new partition,Upper Lebesgue sum with a new partition,,"Assume we have a $f$ from  $R$ to  $[0, \infty)$, which is Lebesgue integrable. Show that there exists a sequence of bi-infinite partitions $Y_n$ of the $y$-axis for which the Lebesgue upper sum is finite and converge to $\int f$ as $n$ goes to $\infty$. The bi-infinite partition is defined as $Y$= $\left\{y_i: 0 < ...< y_{i-1} <y_i<..., \right\}$, with $y_i$ goes to $0$ as $i$ goes to $-\infty$ and $y_i$ goes to $\infty$ as $i$ goes to $\infty$. So how to construct the sequence of the bi-infinite partition? And how is such construction related to the $\liminf$ of the upper Legesgue sum? $***EDIT***$: Here is the question, from Pugh's Real Mathematical Analysis: Do your think whether such sequence of partitions work? $Y_1$=$\left\{y_i=i,  y_{-i}=1/2^i \right\}$ And we add $k/2^m$, where $k<2^m$, $m\leqslant n$ to the negative index part, and for the positive part we add $1+q$ for $Y_2$, and $1+q, 2+q$ for $Y_3$, and so on and so forth, where $q$ is rationals between 0 and 1. My guess is that by doing so, we are letting $y_i+1/y_i$ to approach 1 when $n$ is sufficiently large, which makes the Lebesgue upper sum approaching to the lower sum, and thus to the Lebesgue Integral.","Assume we have a $f$ from  $R$ to  $[0, \infty)$, which is Lebesgue integrable. Show that there exists a sequence of bi-infinite partitions $Y_n$ of the $y$-axis for which the Lebesgue upper sum is finite and converge to $\int f$ as $n$ goes to $\infty$. The bi-infinite partition is defined as $Y$= $\left\{y_i: 0 < ...< y_{i-1} <y_i<..., \right\}$, with $y_i$ goes to $0$ as $i$ goes to $-\infty$ and $y_i$ goes to $\infty$ as $i$ goes to $\infty$. So how to construct the sequence of the bi-infinite partition? And how is such construction related to the $\liminf$ of the upper Legesgue sum? $***EDIT***$: Here is the question, from Pugh's Real Mathematical Analysis: Do your think whether such sequence of partitions work? $Y_1$=$\left\{y_i=i,  y_{-i}=1/2^i \right\}$ And we add $k/2^m$, where $k<2^m$, $m\leqslant n$ to the negative index part, and for the positive part we add $1+q$ for $Y_2$, and $1+q, 2+q$ for $Y_3$, and so on and so forth, where $q$ is rationals between 0 and 1. My guess is that by doing so, we are letting $y_i+1/y_i$ to approach 1 when $n$ is sufficiently large, which makes the Lebesgue upper sum approaching to the lower sum, and thus to the Lebesgue Integral.",,"['real-analysis', 'measure-theory', 'proof-verification', 'lebesgue-integral']"
21,On the importance of the Riesz–Markov–Kakutani representation theorem.,On the importance of the Riesz–Markov–Kakutani representation theorem.,,"I am following big Rudin and I have arrived at the representation theorem. Before doing the full long proof I would like to know what results are based on this theorem that for completeness I state below: Let $X$ be a locally compact Hausdorff space. For any positive linear functional $\psi$ on $C_c(X)$, there is a unique regular Borel measure $μ$ on $X $such that: $$     \psi(f) = \int_X f(x) \, d \mu(x) \quad  $$ for all f in Cc(X). (Rudin proves it in a more general setting ). I apologize if I should already understand why this is important and what results are based on this but in my defence I am fairly new to measure theory.","I am following big Rudin and I have arrived at the representation theorem. Before doing the full long proof I would like to know what results are based on this theorem that for completeness I state below: Let $X$ be a locally compact Hausdorff space. For any positive linear functional $\psi$ on $C_c(X)$, there is a unique regular Borel measure $μ$ on $X $such that: $$     \psi(f) = \int_X f(x) \, d \mu(x) \quad  $$ for all f in Cc(X). (Rudin proves it in a more general setting ). I apologize if I should already understand why this is important and what results are based on this but in my defence I am fairly new to measure theory.",,"['complex-analysis', 'measure-theory', 'self-learning', 'riesz-representation-theorem']"
22,Riemannian Volume vs. Euclidean Volume,Riemannian Volume vs. Euclidean Volume,,"If $M$ is a Riemannian manifold and $\phi$ is a Borel measurable function into $M$, then what is the relationship between $\int_M \phi \,d\mu$ and $\int_{\mathbb{R}^d} \psi\circ \phi\, d\lambda$; where $\mu$ is the Riemannian measure on $M$, $\lambda$ is the Lebesgue measure on ${\mathbb{R}^d}$ and $\psi: M \rightarrow {\mathbb{R}^d}$ maps $M-\mathrm{Cutlocus}(p)$ (for some $p \in M$) to its normal coordinates? Specifically is $\int_{\mathbb{R}^d} \psi\circ \phi \,d\lambda\leq \int_M \phi\, d\mu$?","If $M$ is a Riemannian manifold and $\phi$ is a Borel measurable function into $M$, then what is the relationship between $\int_M \phi \,d\mu$ and $\int_{\mathbb{R}^d} \psi\circ \phi\, d\lambda$; where $\mu$ is the Riemannian measure on $M$, $\lambda$ is the Lebesgue measure on ${\mathbb{R}^d}$ and $\psi: M \rightarrow {\mathbb{R}^d}$ maps $M-\mathrm{Cutlocus}(p)$ (for some $p \in M$) to its normal coordinates? Specifically is $\int_{\mathbb{R}^d} \psi\circ \phi \,d\lambda\leq \int_M \phi\, d\mu$?",,"['integration', 'measure-theory', 'lebesgue-integral', 'riemannian-geometry']"
23,Is a compact subset of the space of Radon measures Polish?,Is a compact subset of the space of Radon measures Polish?,,"Denote by $M(\mathbb{R})$ the space of Radon measures on $\mathbb{R}$ considered as the topological dual of $C_c(\mathbb{R})$ equipped with the locally convex inductive limit topology (as is done by Bourbaki). Equip $M(\mathbb{R})$ with the weak-* topology, i.e. $\mu_n \to \mu$ iff $\int f \, d\mu_n \to \int f \, d\mu$ for all $f \in C_c(\mathbb{R})$. For a subset $H \subseteq M(\mathbb{R})$ the following are equivalent: $H$ is relatively compact iff $H$ is bounded iff $H$ is equicontinuous. It is known that $M(\mathbb{R})$ is not metrizable. The subspace of positive measures is metrizable and moreover Polish. I thought to have read somewhere that compact subsets of $M(\mathbb{R})$ are Polish, but I don't know whether this statement is really true. Does anyone know of a proof (maybe in a more general setup) or a counterexample? Moreover, is the space $M(\mathbb{R})$ of all Radon measures in some way close to Polish,  e.g. Lusin or Souslin?","Denote by $M(\mathbb{R})$ the space of Radon measures on $\mathbb{R}$ considered as the topological dual of $C_c(\mathbb{R})$ equipped with the locally convex inductive limit topology (as is done by Bourbaki). Equip $M(\mathbb{R})$ with the weak-* topology, i.e. $\mu_n \to \mu$ iff $\int f \, d\mu_n \to \int f \, d\mu$ for all $f \in C_c(\mathbb{R})$. For a subset $H \subseteq M(\mathbb{R})$ the following are equivalent: $H$ is relatively compact iff $H$ is bounded iff $H$ is equicontinuous. It is known that $M(\mathbb{R})$ is not metrizable. The subspace of positive measures is metrizable and moreover Polish. I thought to have read somewhere that compact subsets of $M(\mathbb{R})$ are Polish, but I don't know whether this statement is really true. Does anyone know of a proof (maybe in a more general setup) or a counterexample? Moreover, is the space $M(\mathbb{R})$ of all Radon measures in some way close to Polish,  e.g. Lusin or Souslin?",,"['functional-analysis', 'measure-theory']"
24,Do continuous linear functions between Banach spaces extend?,Do continuous linear functions between Banach spaces extend?,,"Just wondering... Let $E$, $G$ be Banach spaces, let $U\subset E$ be a subset of $E$, and let $f:U\rightarrow G$ be a continuous linear function. Can $f$ be extended to a continuous linear function on $E$, $F:E\rightarrow G$? For which cases does this happen?","Just wondering... Let $E$, $G$ be Banach spaces, let $U\subset E$ be a subset of $E$, and let $f:U\rightarrow G$ be a continuous linear function. Can $f$ be extended to a continuous linear function on $E$, $F:E\rightarrow G$? For which cases does this happen?",,"['general-topology', 'analysis', 'banach-spaces', 'topological-vector-spaces']"
25,Is there a monotonic function discontinuous over some dense set?,Is there a monotonic function discontinuous over some dense set?,,"Can we construct a monotonic function $f : \mathbb{R} \to \mathbb{R}$ such that there is a dense set in some interval $(a,b)$ for which $f$ is discontinuous at all points in the dense set?  What about a strictly monotonic function? My intuition tells me that such a function is impossible. Here is a rough sketch of an attempt at proving that such a function does not exist: we could suppose a function satisfies these conditions.  Take an $\epsilon > 0$ and two points $x,y$ in this dense set such that $x<y$.  Then, $f(x)<f(y)$ because if they are equal, then the function is constant at all points in between, and there is another element of $X$ between $x$ and $y$, which would be a contradiction.  Take $f(y)-f(x)$.  By the Archimedean property of the reals, $f(y)-f(x)<n\epsilon$ for some $n$. However, after this point, I am stuck.  Could we somehow partition $(x,y)$ into $n$ subintervals and conclude that there must be some point on the dense set that is continuous?","Can we construct a monotonic function $f : \mathbb{R} \to \mathbb{R}$ such that there is a dense set in some interval $(a,b)$ for which $f$ is discontinuous at all points in the dense set?  What about a strictly monotonic function? My intuition tells me that such a function is impossible. Here is a rough sketch of an attempt at proving that such a function does not exist: we could suppose a function satisfies these conditions.  Take an $\epsilon > 0$ and two points $x,y$ in this dense set such that $x<y$.  Then, $f(x)<f(y)$ because if they are equal, then the function is constant at all points in between, and there is another element of $X$ between $x$ and $y$, which would be a contradiction.  Take $f(y)-f(x)$.  By the Archimedean property of the reals, $f(y)-f(x)<n\epsilon$ for some $n$. However, after this point, I am stuck.  Could we somehow partition $(x,y)$ into $n$ subintervals and conclude that there must be some point on the dense set that is continuous?",,"['real-analysis', 'continuity']"
26,"can the emphasis on ""smallest"" in the monotone class theorem be ignored in applications?","can the emphasis on ""smallest"" in the monotone class theorem be ignored in applications?",,"The monotone class theorem states that for any algebra of sets $\cal A$ one can construct the smallest monotone class generated by this class ${\cal M}(\cal A)$. This smallest monotone class is also the smallest sigma-algebra $\Sigma(\cal A)$ generated by $\cal A$, and ${\cal M}(\cal A)=\Sigma(\cal A)$. However, I noticed that in applications of the theorem in various mathematical proofs I have studied authors ignore the fact that the monotone class should be the smallest. For example, assume that the goal is to prove that a certain class of sets, e.g. $\cal B$, is a sigma-algebra. Authors just prove that this group of sets is a monotone class, and then by invoking the monotone class theorem, without showing that $\cal B$ is indeed the smallest monotone class, just conclude that this class has to be also a sigma-algebra. Is it in general possible to ignore the ""smallest"" requirement when using the monotone class theorem, or are there circumstances one has to be aware of that make such use possible? EDIT: The background to the question can be found here . The post discusses a theorem where the ""smallest"" requirement is being ignored.  It provides an example.","The monotone class theorem states that for any algebra of sets $\cal A$ one can construct the smallest monotone class generated by this class ${\cal M}(\cal A)$. This smallest monotone class is also the smallest sigma-algebra $\Sigma(\cal A)$ generated by $\cal A$, and ${\cal M}(\cal A)=\Sigma(\cal A)$. However, I noticed that in applications of the theorem in various mathematical proofs I have studied authors ignore the fact that the monotone class should be the smallest. For example, assume that the goal is to prove that a certain class of sets, e.g. $\cal B$, is a sigma-algebra. Authors just prove that this group of sets is a monotone class, and then by invoking the monotone class theorem, without showing that $\cal B$ is indeed the smallest monotone class, just conclude that this class has to be also a sigma-algebra. Is it in general possible to ignore the ""smallest"" requirement when using the monotone class theorem, or are there circumstances one has to be aware of that make such use possible? EDIT: The background to the question can be found here . The post discusses a theorem where the ""smallest"" requirement is being ignored.  It provides an example.",,"['measure-theory', 'monotone-class-theorem']"
27,"If $f_k \to f$ a.e. and the $L^p$ norms converge, then $f_k \to f$ in $L^p$","If  a.e. and the  norms converge, then  in",f_k \to f L^p f_k \to f L^p,"Let $1\leq p < \infty$ . Suppose that $\{f_k, f\} \subset L^p$ (the domain here does not necessarily have to be finite), $f_k \to f$ almost everywhere, and $\|f_k\|_{L^p} \to \|f\|_{L^p}$ . Why is it the case that $$\|f_k - f\|_{L^p} \to 0?$$ A statement in the other direction (i.e. $\|f_k - f\|_{L^p} \to 0 \Rightarrow \|f_k\|_{L^p} \to \|f\|_{L^p}$ ) follows pretty easily and is the one that I've seen most of the time. I'm not how to show the result above though.","Let . Suppose that (the domain here does not necessarily have to be finite), almost everywhere, and . Why is it the case that A statement in the other direction (i.e. ) follows pretty easily and is the one that I've seen most of the time. I'm not how to show the result above though.","1\leq p < \infty \{f_k, f\} \subset L^p f_k \to f \|f_k\|_{L^p} \to \|f\|_{L^p} \|f_k - f\|_{L^p} \to 0? \|f_k - f\|_{L^p} \to 0 \Rightarrow \|f_k\|_{L^p} \to \|f\|_{L^p}","['real-analysis', 'functional-analysis', 'measure-theory', 'convergence-divergence', 'lp-spaces']"
28,Equivalent Vitali Covering Properties for Differentiation Bases,Equivalent Vitali Covering Properties for Differentiation Bases,,"N.B. In what follows below, we work exclusively with the Lebesgue measure on $\mathbb{R}^{n}$. Def 1 For each $x\in\mathbb{R}^{n}$, let $\mathcal{B}(x)$ be a collection of bounded open sets $R$ containing $x$ such that there exists a sequence $\left\{R_{k}\right\}\subset\mathcal{B}(x)$ tending to $\left\{x\right\}$. We say that $\mathcal{B}:=\bigcup\mathcal{B}(x)$ is a differentiation basis . If for $R\in\mathcal{B}$, $x\in R$ implies $R\in\mathcal{B}(x)$, we say that $\mathcal{B}$ is a Busemann-Feller differentiation basis . (Hereafter, all differentiation bases will be BF.) Def 2 Let $A\subset\mathbb{R}^{n}$ be a measurable set. $V\subset\mathcal{B}$ is a $\mathcal{B}$-Vitali covering of $A$ if for each $x\in A$ there exists a sequence $\left\{R_{k}\right\}\subset V$ such that $x\in R_{k}$ and $\text{diam }R_{k}\rightarrow 0$. Def 3 We say that a basis $\mathcal{B}$ has the $V_{q}$ property, where $1<q<\infty$, if there exists  constant $C$ such that for every measurable set $A$, for every $\mathcal{B}$-Vitali covering of $A$, and for every $\varepsilon>0$, there exists a countable subcollection $\left\{R_{k}\right\}\subset V$ satisfying $\left|A\setminus\bigcup R_{k}\right|=0$, $\left|\bigcup R_{k}\setminus A\right|\leq\varepsilon$ $\left\|\sum \chi_{R_{k}}\right\|_{L^{q}}\leq C\left|A\right|^{1/q}$ Define the upper and lower derivatives of $\int f$ at $x$ respectively by $$\overline{D}\left(\int f,x\right):=\sup_{\left\{R_{k}\right\}}\limsup_{k}\dfrac{1}{\left|R_{k}\right|}\int_{R_{k}}f, \ \underline{D}\left(\int f,x\right):=\inf_{\left\{R_{k}\right\}}\liminf_{k}\dfrac{1}{\left|R_{k}\right|}\int_{R_{k}}f$$ where the supremum and infimum are respectively taken over all sequences $\left\{R_{k}\right\}\subset\mathcal{B}(x)$ with $\text{diam} R_{k}\rightarrow 0$. It has been shown by A. Cordoba and R. Fefferman [Theorem 2, p. 2212] that the $V_{q}$ property is equivalent to the property that $\mathcal{B}$ differentiates $\int f$ a.e., for $f\in L_{loc}^{p}$, where $1/p+1/q=1$; i.e., $$\overline{D}\left(\int f,x\right)=\underline{D}\left(\int f,x\right)=f(x) \ a.e.,$$ It has also been shown by C.A. Hayes [Theorem 2.4, p. 178] that Definition 3 with condition 2 replaced by condition 2* $$\left\|\sum_{k}\chi_{R_{k}}-\chi_{\bigcup R_{k}}\right\|_{L^{q}}\leq\varepsilon$$ is equivalent to the differentiation property stated above. My question is whether there is a proof of the equivalence of Cordoba's $V_{q}$ property and Hayes's $V_{q}$ property, which does not use the full result that each is equivalent to the differentiation property.","N.B. In what follows below, we work exclusively with the Lebesgue measure on $\mathbb{R}^{n}$. Def 1 For each $x\in\mathbb{R}^{n}$, let $\mathcal{B}(x)$ be a collection of bounded open sets $R$ containing $x$ such that there exists a sequence $\left\{R_{k}\right\}\subset\mathcal{B}(x)$ tending to $\left\{x\right\}$. We say that $\mathcal{B}:=\bigcup\mathcal{B}(x)$ is a differentiation basis . If for $R\in\mathcal{B}$, $x\in R$ implies $R\in\mathcal{B}(x)$, we say that $\mathcal{B}$ is a Busemann-Feller differentiation basis . (Hereafter, all differentiation bases will be BF.) Def 2 Let $A\subset\mathbb{R}^{n}$ be a measurable set. $V\subset\mathcal{B}$ is a $\mathcal{B}$-Vitali covering of $A$ if for each $x\in A$ there exists a sequence $\left\{R_{k}\right\}\subset V$ such that $x\in R_{k}$ and $\text{diam }R_{k}\rightarrow 0$. Def 3 We say that a basis $\mathcal{B}$ has the $V_{q}$ property, where $1<q<\infty$, if there exists  constant $C$ such that for every measurable set $A$, for every $\mathcal{B}$-Vitali covering of $A$, and for every $\varepsilon>0$, there exists a countable subcollection $\left\{R_{k}\right\}\subset V$ satisfying $\left|A\setminus\bigcup R_{k}\right|=0$, $\left|\bigcup R_{k}\setminus A\right|\leq\varepsilon$ $\left\|\sum \chi_{R_{k}}\right\|_{L^{q}}\leq C\left|A\right|^{1/q}$ Define the upper and lower derivatives of $\int f$ at $x$ respectively by $$\overline{D}\left(\int f,x\right):=\sup_{\left\{R_{k}\right\}}\limsup_{k}\dfrac{1}{\left|R_{k}\right|}\int_{R_{k}}f, \ \underline{D}\left(\int f,x\right):=\inf_{\left\{R_{k}\right\}}\liminf_{k}\dfrac{1}{\left|R_{k}\right|}\int_{R_{k}}f$$ where the supremum and infimum are respectively taken over all sequences $\left\{R_{k}\right\}\subset\mathcal{B}(x)$ with $\text{diam} R_{k}\rightarrow 0$. It has been shown by A. Cordoba and R. Fefferman [Theorem 2, p. 2212] that the $V_{q}$ property is equivalent to the property that $\mathcal{B}$ differentiates $\int f$ a.e., for $f\in L_{loc}^{p}$, where $1/p+1/q=1$; i.e., $$\overline{D}\left(\int f,x\right)=\underline{D}\left(\int f,x\right)=f(x) \ a.e.,$$ It has also been shown by C.A. Hayes [Theorem 2.4, p. 178] that Definition 3 with condition 2 replaced by condition 2* $$\left\|\sum_{k}\chi_{R_{k}}-\chi_{\bigcup R_{k}}\right\|_{L^{q}}\leq\varepsilon$$ is equivalent to the differentiation property stated above. My question is whether there is a proof of the equivalence of Cordoba's $V_{q}$ property and Hayes's $V_{q}$ property, which does not use the full result that each is equivalent to the differentiation property.",,"['real-analysis', 'integration', 'measure-theory']"
29,Show that any element of a sigma algebra is the union of disjoint sets,Show that any element of a sigma algebra is the union of disjoint sets,,"Let $\mathscr{M}$ be a $\sigma$-algebra on $X$ generated by a finite family of sets. Prove that there exists a partition of $X$ into disjoint sets $E_1, E_2, \ldots, E_n$ such that $A$ is an element of $\mathscr{M}$ if and only if $A$ is the union of some sets $E_1, E_2,\ldots, E_n$. My work so far is Suppose $\mathscr{M}$ is generated by a finite collection say $\{B_i\}$ $i = 1,\ldots,n$. Then I define a partition $\mathscr{P}$ by  $$\mathscr{P} = \left\{\bigcap C_i: C_i = B_i  \text{ or } B_i^c, i = 1,\ldots,n\right\}$$ Let $\mathscr{L}$ be a collection of the arbitrary union of the sets in $\mathscr{P}$.  I claim that $\mathscr{L}$ is a $\sigma$-algebra.  If I can show that $\mathscr{L} = \mathscr{M}$, is this enough to prove the proposition? Thank you. Sorry about the typsetting.  I don't know how to type in the symbols.","Let $\mathscr{M}$ be a $\sigma$-algebra on $X$ generated by a finite family of sets. Prove that there exists a partition of $X$ into disjoint sets $E_1, E_2, \ldots, E_n$ such that $A$ is an element of $\mathscr{M}$ if and only if $A$ is the union of some sets $E_1, E_2,\ldots, E_n$. My work so far is Suppose $\mathscr{M}$ is generated by a finite collection say $\{B_i\}$ $i = 1,\ldots,n$. Then I define a partition $\mathscr{P}$ by  $$\mathscr{P} = \left\{\bigcap C_i: C_i = B_i  \text{ or } B_i^c, i = 1,\ldots,n\right\}$$ Let $\mathscr{L}$ be a collection of the arbitrary union of the sets in $\mathscr{P}$.  I claim that $\mathscr{L}$ is a $\sigma$-algebra.  If I can show that $\mathscr{L} = \mathscr{M}$, is this enough to prove the proposition? Thank you. Sorry about the typsetting.  I don't know how to type in the symbols.",,"['real-analysis', 'measure-theory']"
30,Pairwise and Mutually disjoint sets,Pairwise and Mutually disjoint sets,,What is the difference between Pairwise and Mutually disjoint sets? The context of this is measure theory.,What is the difference between Pairwise and Mutually disjoint sets? The context of this is measure theory.,,"['real-analysis', 'measure-theory', 'elementary-set-theory']"
31,$Y\subset\mathbb{R}$ negligible $\Rightarrow f(Y)\subset\mathbb{R}$ negligible for $f$ differentiable,negligible  negligible for  differentiable,Y\subset\mathbb{R} \Rightarrow f(Y)\subset\mathbb{R} f,"I'm trying to prove the following theorem: Let $f:\mathbb{R}\to\mathbb{R}$ be a differentiable function, then $Y\subset\mathbb{R} \text { negligible}\Rightarrow f(Y)\subset\mathbb{R} \text{ negligible}$. Using the following theorem(*): Let $Y\subset\mathbb{R}$ be Lebesgue-integrable and $f:\mathbb{R}\to\mathbb{R}$ a function s.t. $\forall y\in Y$ f differentiable at $y$ and $|f'(y)|<n$, then $\exists B\subset\mathbb{R}$ Borel s.t. $f(Y)\subset B$ and $\lambda(B)\leq n\lambda(Y)$. My proof is as follows: Let $Y\subset\mathbb{R}$ be negligible and define $Y_{n}\subset Y$ by $Y_{n}:=\{y\in Y:|f'(y)|<n\}$, then by applying theorem (*)$\ \exists B_{n}\subset \mathbb{R}$ Borel s.t. $f(Y_{n})\subset B_{n}$ and $\lambda(B_{n}) \leq n\lambda(Y_{n}) = n*0 = 0 \ \forall n\in\mathbb{N}$, thus $\lambda(f(Y_{n}))\leq\lambda(B_{n})\leq 0$, so that $\lambda(f(Y)) \leq \sum \lambda(f(Y_{n})) = \sum 0 = 0$ thus $f(Y)$ is negligible. However, the hint in the exercise says it's not as easy as it looks. So I'm wondering: assuming my proof is too short, what's wrong?","I'm trying to prove the following theorem: Let $f:\mathbb{R}\to\mathbb{R}$ be a differentiable function, then $Y\subset\mathbb{R} \text { negligible}\Rightarrow f(Y)\subset\mathbb{R} \text{ negligible}$. Using the following theorem(*): Let $Y\subset\mathbb{R}$ be Lebesgue-integrable and $f:\mathbb{R}\to\mathbb{R}$ a function s.t. $\forall y\in Y$ f differentiable at $y$ and $|f'(y)|<n$, then $\exists B\subset\mathbb{R}$ Borel s.t. $f(Y)\subset B$ and $\lambda(B)\leq n\lambda(Y)$. My proof is as follows: Let $Y\subset\mathbb{R}$ be negligible and define $Y_{n}\subset Y$ by $Y_{n}:=\{y\in Y:|f'(y)|<n\}$, then by applying theorem (*)$\ \exists B_{n}\subset \mathbb{R}$ Borel s.t. $f(Y_{n})\subset B_{n}$ and $\lambda(B_{n}) \leq n\lambda(Y_{n}) = n*0 = 0 \ \forall n\in\mathbb{N}$, thus $\lambda(f(Y_{n}))\leq\lambda(B_{n})\leq 0$, so that $\lambda(f(Y)) \leq \sum \lambda(f(Y_{n})) = \sum 0 = 0$ thus $f(Y)$ is negligible. However, the hint in the exercise says it's not as easy as it looks. So I'm wondering: assuming my proof is too short, what's wrong?",,"['measure-theory', 'lebesgue-measure']"
32,Understanding averaging of symplectic matrices via Haar measure,Understanding averaging of symplectic matrices via Haar measure,,"In McDuff and Salamon's Intro. to Symplectic Topology (2nd edition), there's a proof that  $U(n)$ is a maximal compact subgroup of $Sp(2n)$ which I'm trying to understand. The proof uses the Haar measure to average over matrices in a way that's unclear to me. The statement I'm having difficulties with is the following: Let $G\subset Sp(2n)$ be a compact subgroup. There exist a symmetric and positive definite matrix $P\in Sp(2n)$ such that $$\Psi^TP\Psi=P, \forall\Psi\in G$$ According to the authors, such a matrix can be obtained by averaging the matrices $\Psi^T\Psi$ over $\Psi\in G$ using the Haar measure $C(G,\mathbb R)\rightarrow \mathbb R$ for a compact Lie group. So, the question is, in its broadest sense, how to understand the averaging process here? (Bear with me, clearer question follows) Here are some details on my progress in making sense of this before submitting this question: So far I have came across these Notes on Compact Lie Groups by Salamon which detail the construction of said Haar measure. This does not explain how to obtain a measure, in the measure theoretic sense, from the functional built in Salamon's notes. So, further searching lead me to Royden's Real Analysis (3rd edition), where he explains in detail how to build a measure out of a functional ( Daniell Integral ) in such a way that the integral over the measure agrees with the functional. All of this is fine and well, but ultimately remains in the realm of functionals, where I'm looking for a process that'll result in the matrix $P$ as above. In a more intuitive manner, what I gather should happen is somewhere along these lines: Let $M:C(G,\mathbb R)\rightarrow\mathbb R$ be a Haar measure following Salamon's notations, and let $\mu$ be the measure corresponding to the functional $M$, constructed like in Royden. Put $P=\int_G \Phi^T\Phi\operatorname{d}\mu$, then we can calculate that $$\Psi^TP\Psi=\Psi^T\int_G\Phi^T\Phi\operatorname{d}\mu\Psi=\int_G\Psi^T\Phi^T\Phi\Psi\operatorname{d}\mu=\int_G\left(\Phi\Psi\right)^T\Phi\Psi\operatorname{d}\mu=\int_G \Phi^T\Phi\operatorname{d}\mu=P$$where the second to last transition follows from $\mu$ being right invariant. So, now my question boils down to this - In light of all that's said, seeing as the Daniell integral $M$ assigns a real number to a function, where we need to assign a matrix to a function, how can $\int_G\Phi^T\Phi\operatorname{d}\mu$ be understood? Also, I'm guessing the second transition in the calculation follows from some means of limiting and applying the equation to the limits. If I'm mistaken, I would very much appreciate clarification on that as well, assuming the entire maneuver makes sense. Note : I am aware of an errata for this edition which fixes the error here, namely, $P$ is not guaranteed to be symplectic, but that's besides the point.","In McDuff and Salamon's Intro. to Symplectic Topology (2nd edition), there's a proof that  $U(n)$ is a maximal compact subgroup of $Sp(2n)$ which I'm trying to understand. The proof uses the Haar measure to average over matrices in a way that's unclear to me. The statement I'm having difficulties with is the following: Let $G\subset Sp(2n)$ be a compact subgroup. There exist a symmetric and positive definite matrix $P\in Sp(2n)$ such that $$\Psi^TP\Psi=P, \forall\Psi\in G$$ According to the authors, such a matrix can be obtained by averaging the matrices $\Psi^T\Psi$ over $\Psi\in G$ using the Haar measure $C(G,\mathbb R)\rightarrow \mathbb R$ for a compact Lie group. So, the question is, in its broadest sense, how to understand the averaging process here? (Bear with me, clearer question follows) Here are some details on my progress in making sense of this before submitting this question: So far I have came across these Notes on Compact Lie Groups by Salamon which detail the construction of said Haar measure. This does not explain how to obtain a measure, in the measure theoretic sense, from the functional built in Salamon's notes. So, further searching lead me to Royden's Real Analysis (3rd edition), where he explains in detail how to build a measure out of a functional ( Daniell Integral ) in such a way that the integral over the measure agrees with the functional. All of this is fine and well, but ultimately remains in the realm of functionals, where I'm looking for a process that'll result in the matrix $P$ as above. In a more intuitive manner, what I gather should happen is somewhere along these lines: Let $M:C(G,\mathbb R)\rightarrow\mathbb R$ be a Haar measure following Salamon's notations, and let $\mu$ be the measure corresponding to the functional $M$, constructed like in Royden. Put $P=\int_G \Phi^T\Phi\operatorname{d}\mu$, then we can calculate that $$\Psi^TP\Psi=\Psi^T\int_G\Phi^T\Phi\operatorname{d}\mu\Psi=\int_G\Psi^T\Phi^T\Phi\Psi\operatorname{d}\mu=\int_G\left(\Phi\Psi\right)^T\Phi\Psi\operatorname{d}\mu=\int_G \Phi^T\Phi\operatorname{d}\mu=P$$where the second to last transition follows from $\mu$ being right invariant. So, now my question boils down to this - In light of all that's said, seeing as the Daniell integral $M$ assigns a real number to a function, where we need to assign a matrix to a function, how can $\int_G\Phi^T\Phi\operatorname{d}\mu$ be understood? Also, I'm guessing the second transition in the calculation follows from some means of limiting and applying the equation to the limits. If I'm mistaken, I would very much appreciate clarification on that as well, assuming the entire maneuver makes sense. Note : I am aware of an errata for this edition which fixes the error here, namely, $P$ is not guaranteed to be symplectic, but that's besides the point.",,"['integration', 'matrices', 'measure-theory', 'symplectic-linear-algebra']"
33,"""+""-Sets are measurable.","""+""-Sets are measurable.",,"$A$ is a subset of $\mathbb{R}^2$ that for every $(x,y) \in A$ there is a $\delta >0$ that $(x-\delta , x+\delta) \times \{y\}$ and $\{x\} \times (y-\delta , y+\delta)$ are subsets of $A$. prove that $A$ is lebesgue measurable. a set like $A$ may be not an open set, like the below set:","$A$ is a subset of $\mathbb{R}^2$ that for every $(x,y) \in A$ there is a $\delta >0$ that $(x-\delta , x+\delta) \times \{y\}$ and $\{x\} \times (y-\delta , y+\delta)$ are subsets of $A$. prove that $A$ is lebesgue measurable. a set like $A$ may be not an open set, like the below set:",,['measure-theory']
34,Ratio limit of moments for bounded functions,Ratio limit of moments for bounded functions,,"I would appriciate if someone could verify if my solution to the following problem is correct. Problem: Given the measure space $\left(X,\mathcal{F},\mu\right)$, where $\mu(X)<\infty  $ and a function $f\in L^{\infty}(\mu)  $, show that $\lim_{n}\frac{\int\left|f\right|^{n+1}d\mu}{\int\left|f\right|^{n}d\mu}=\left\Vert f\right\Vert _{\infty}  $. My solution: On one hand we have, $\frac{\int\left|f\right|^{n+1}d\mu}{\int\left|f\right|^{n}d\mu}\leq\frac{\int\left\Vert f\right\Vert _{\infty}\left|f\right|^{n}d\mu}{\int\left|f\right|^{n}d\mu}=\left\Vert f\right\Vert _{\infty}\cdot1 . $ But on the other hand we have, $\frac{\int\left|f\right|^{n+1}d\mu}{\int\left|f\right|^{n}d\mu}=\frac{\int\left|f\right|^{n\left(\frac{n+1}{n}\right)}d\mu/\mu(X)}{\int\left|f\right|^{n}d\mu/\mu(X)}\geq[Jensen]\geq\frac{\left(\int\left|f\right|^{n}d\mu/\mu(X)\right)^{^{\left(\frac{n+1}{n}\right)}}}{\int\left|f\right|^{n}d\mu/\mu(X)}=\left\Vert f\right\Vert _{n}\mu(X)^{-1/n}.  $ From which it follows that $\lim_{n}\frac{\int\left|f\right|^{n+1}d\mu}{\int\left|f\right|^{n}d\mu}\geq\lim_{n}\left\Vert f\right\Vert _{n}\mu(X)^{-1/n}=\lim_{n}\left\Vert f\right\Vert _{n}\lim_{n}\mu(X)^{-1/n}=\left\Vert f\right\Vert _{\infty}\cdot1  .$ Where we used the fact that $\lim_{n}\left\Vert f\right\Vert _{n}=\left\Vert f\right\Vert _{\infty}  .$","I would appriciate if someone could verify if my solution to the following problem is correct. Problem: Given the measure space $\left(X,\mathcal{F},\mu\right)$, where $\mu(X)<\infty  $ and a function $f\in L^{\infty}(\mu)  $, show that $\lim_{n}\frac{\int\left|f\right|^{n+1}d\mu}{\int\left|f\right|^{n}d\mu}=\left\Vert f\right\Vert _{\infty}  $. My solution: On one hand we have, $\frac{\int\left|f\right|^{n+1}d\mu}{\int\left|f\right|^{n}d\mu}\leq\frac{\int\left\Vert f\right\Vert _{\infty}\left|f\right|^{n}d\mu}{\int\left|f\right|^{n}d\mu}=\left\Vert f\right\Vert _{\infty}\cdot1 . $ But on the other hand we have, $\frac{\int\left|f\right|^{n+1}d\mu}{\int\left|f\right|^{n}d\mu}=\frac{\int\left|f\right|^{n\left(\frac{n+1}{n}\right)}d\mu/\mu(X)}{\int\left|f\right|^{n}d\mu/\mu(X)}\geq[Jensen]\geq\frac{\left(\int\left|f\right|^{n}d\mu/\mu(X)\right)^{^{\left(\frac{n+1}{n}\right)}}}{\int\left|f\right|^{n}d\mu/\mu(X)}=\left\Vert f\right\Vert _{n}\mu(X)^{-1/n}.  $ From which it follows that $\lim_{n}\frac{\int\left|f\right|^{n+1}d\mu}{\int\left|f\right|^{n}d\mu}\geq\lim_{n}\left\Vert f\right\Vert _{n}\mu(X)^{-1/n}=\lim_{n}\left\Vert f\right\Vert _{n}\lim_{n}\mu(X)^{-1/n}=\left\Vert f\right\Vert _{\infty}\cdot1  .$ Where we used the fact that $\lim_{n}\left\Vert f\right\Vert _{n}=\left\Vert f\right\Vert _{\infty}  .$",,"['real-analysis', 'measure-theory', 'solution-verification']"
35,Limit of Suprema of Integrals,Limit of Suprema of Integrals,,"This is my first post here.  I'm trying to play by the rules, so forgive me if it seems I'm asking too much. I've been working on the following problem: Let $\mu(\Omega)<\infty$, and $\ f_n:\Omega\to\mathbb{R}\ \ n\geq 1\ $, $\ f:\Omega\to\mathbb{R}$ be integrable functions such that $0 \leq f_n \to f$ a.e.,  and $\int_\Omega f_n d\mu \to \int_\Omega f d\mu$.  Show that  $$ \lim_{a\to\infty}\sup_{n\geq1}\int_{\{f_n\geq a\}}f_nd\mu=0. $$ Here's my initial attempt: By Egorov's theorem, we may obtain, for all $n\geq N$, $|f_n-f|<\epsilon_0$ on the set $\Omega \backslash A$, with $\mu(A)<\delta$, i.e. for all $n\geq N$ we have $f-\epsilon_0\leq f_n \leq f+\epsilon_0$ on $\Omega\backslash A$.  Now, we may choose an $a_1$ such that $$ \max_{1\leq n\leq N}\int_{\{f_n\geq a_1\}}f_nd\mu<\epsilon $$ by the integrability of $f_n$.  Noting that for $n\geq N$ we have $\{f_n>a\}\subset\{f+\epsilon_0>a\}$, similarly (and this is somewhat awkward) we may choose an $a_2$ such that  $$ \int_{\{f+\epsilon_0\geq a_2\}\backslash A}(f+\epsilon_0 )d\mu<\epsilon $$ where the integrability of $\epsilon_0$ comes from the finiteness of $\mu(\Omega)$.  This gives for all $n\geq N$ $$ \int_{\{f_n\geq a_2\}\backslash A} f_n d\mu \leq \int_{\{f+\epsilon_0\geq a_2\}\backslash A}(f+\epsilon_0 )d\mu<\epsilon. $$ Letting $a=\max\{a_1,a_2\}$, we have, for all $n$ $$ \int_{\{f_n\geq a\}}f_nd\mu = \int_{\{f_n\geq a\}\backslash A}f_nd\mu + \int_{\{f_n\geq a\}\cap A}f_nd\mu $$ $$ <\epsilon +  \int_{\{f_n\geq a\}\cap A}f_nd\mu $$ So this holds for suprema as well. Now, I'm trying to make use of the fact that $\mu(A)<\delta$ for the remaining integral, but I'm not sure how to proceed.  I imagine this is where I need to make use of convergence of the integrals.  I'm sure, given the awkwardness of this attempt, there is a better approach as well. Thanks! Edit I found a way to deal with the remaining integral, as I posted below, but I'm still curious to see if there is a more elegant approach to this problem. Any suggestions would be greatly appreciated.","This is my first post here.  I'm trying to play by the rules, so forgive me if it seems I'm asking too much. I've been working on the following problem: Let $\mu(\Omega)<\infty$, and $\ f_n:\Omega\to\mathbb{R}\ \ n\geq 1\ $, $\ f:\Omega\to\mathbb{R}$ be integrable functions such that $0 \leq f_n \to f$ a.e.,  and $\int_\Omega f_n d\mu \to \int_\Omega f d\mu$.  Show that  $$ \lim_{a\to\infty}\sup_{n\geq1}\int_{\{f_n\geq a\}}f_nd\mu=0. $$ Here's my initial attempt: By Egorov's theorem, we may obtain, for all $n\geq N$, $|f_n-f|<\epsilon_0$ on the set $\Omega \backslash A$, with $\mu(A)<\delta$, i.e. for all $n\geq N$ we have $f-\epsilon_0\leq f_n \leq f+\epsilon_0$ on $\Omega\backslash A$.  Now, we may choose an $a_1$ such that $$ \max_{1\leq n\leq N}\int_{\{f_n\geq a_1\}}f_nd\mu<\epsilon $$ by the integrability of $f_n$.  Noting that for $n\geq N$ we have $\{f_n>a\}\subset\{f+\epsilon_0>a\}$, similarly (and this is somewhat awkward) we may choose an $a_2$ such that  $$ \int_{\{f+\epsilon_0\geq a_2\}\backslash A}(f+\epsilon_0 )d\mu<\epsilon $$ where the integrability of $\epsilon_0$ comes from the finiteness of $\mu(\Omega)$.  This gives for all $n\geq N$ $$ \int_{\{f_n\geq a_2\}\backslash A} f_n d\mu \leq \int_{\{f+\epsilon_0\geq a_2\}\backslash A}(f+\epsilon_0 )d\mu<\epsilon. $$ Letting $a=\max\{a_1,a_2\}$, we have, for all $n$ $$ \int_{\{f_n\geq a\}}f_nd\mu = \int_{\{f_n\geq a\}\backslash A}f_nd\mu + \int_{\{f_n\geq a\}\cap A}f_nd\mu $$ $$ <\epsilon +  \int_{\{f_n\geq a\}\cap A}f_nd\mu $$ So this holds for suprema as well. Now, I'm trying to make use of the fact that $\mu(A)<\delta$ for the remaining integral, but I'm not sure how to proceed.  I imagine this is where I need to make use of convergence of the integrals.  I'm sure, given the awkwardness of this attempt, there is a better approach as well. Thanks! Edit I found a way to deal with the remaining integral, as I posted below, but I'm still curious to see if there is a more elegant approach to this problem. Any suggestions would be greatly appreciated.",,"['real-analysis', 'integration', 'measure-theory']"
36,Equality of measure sets of dynamical system,Equality of measure sets of dynamical system,,"This is a homework question I have been crunching my brains on for a lot of time, but unfortunately I'm stuck. I would greatly appreciate any help! The problem is as follows: We have some continuous map $f: A \rightarrow A$ of a compact metric space $(X, d)$. Denote $\tilde{A}$ the space $\{a \in A^{Z} | f(a)_n = (a)_{n+1} \}$. Then we can extend $f$ with $ \tilde{f} : \tilde{A} \rightarrow \tilde{A}$, where $\tilde{f}(a_{n}) = a_{n+1}$ for all $n$. (so a shift) $\tilde{A}$ has metric $\tilde{d}(a, b) = \sum_{Z} 2^{|-i|}d(a_i, b_i)$ Write $\pi : \tilde{A} \rightarrow A$  for the map sending $a$ to $a_0$. Now show that $\pi_* M(\tilde{f}(\tilde{A})) = M(f(A)$, where $M$ stands for the set of Borel probability measures on $A$. my thoughts: to start, I understand that $\pi$ is a topological factor map and that $\tilde{A}$ is some kind of an orbit space. Also I think that I have to construct a measure on $\tilde{A}$ given a measure on $A$that is compatible with $\pi$ So I thought this would be a good cylinder-like construction. Let $\mu$ be some measure on $A$. Now let $B \subset \tilde{A}$. Then define measure $\tilde{\mu}(B) := \prod_Z \mu(\cup_{b\in B}(b_i))$. because this measure is I think a probability measure, and also it is easy to go from a measure of this form to a measure on $A$. But how can I possibly show that measures are of such a product form? or is that even true? And how can I now show that this is an actual measure on all open sets? Now I only know how it behaves on cylinders. Or don't I even need an explicit construction? Please let me know if you have any thoughts, directions, hints etc! Thanks and kind regards, Derk van Willekenburg","This is a homework question I have been crunching my brains on for a lot of time, but unfortunately I'm stuck. I would greatly appreciate any help! The problem is as follows: We have some continuous map $f: A \rightarrow A$ of a compact metric space $(X, d)$. Denote $\tilde{A}$ the space $\{a \in A^{Z} | f(a)_n = (a)_{n+1} \}$. Then we can extend $f$ with $ \tilde{f} : \tilde{A} \rightarrow \tilde{A}$, where $\tilde{f}(a_{n}) = a_{n+1}$ for all $n$. (so a shift) $\tilde{A}$ has metric $\tilde{d}(a, b) = \sum_{Z} 2^{|-i|}d(a_i, b_i)$ Write $\pi : \tilde{A} \rightarrow A$  for the map sending $a$ to $a_0$. Now show that $\pi_* M(\tilde{f}(\tilde{A})) = M(f(A)$, where $M$ stands for the set of Borel probability measures on $A$. my thoughts: to start, I understand that $\pi$ is a topological factor map and that $\tilde{A}$ is some kind of an orbit space. Also I think that I have to construct a measure on $\tilde{A}$ given a measure on $A$that is compatible with $\pi$ So I thought this would be a good cylinder-like construction. Let $\mu$ be some measure on $A$. Now let $B \subset \tilde{A}$. Then define measure $\tilde{\mu}(B) := \prod_Z \mu(\cup_{b\in B}(b_i))$. because this measure is I think a probability measure, and also it is easy to go from a measure of this form to a measure on $A$. But how can I possibly show that measures are of such a product form? or is that even true? And how can I now show that this is an actual measure on all open sets? Now I only know how it behaves on cylinders. Or don't I even need an explicit construction? Please let me know if you have any thoughts, directions, hints etc! Thanks and kind regards, Derk van Willekenburg",,"['measure-theory', 'dynamical-systems']"
37,Hahn-Banach proof of existence of Haar measure,Hahn-Banach proof of existence of Haar measure,,"I'm reading these notes of Terry Tao on the Haar measure (and related topics) on a locally compact Hausdorff group $G$. When he goes through the construction of the Haar measure, he does so by way of the Riesz representation theorem: if we had a linear, translation-invariant functional that satisfied a positivity property, by the Riesz representation theorem we'd get our measure. To that end he constructs, for any $f_1, \ldots, f_n \in \mathcal{C}_c(G)^+$ and $\varepsilon > 0$, a functional $I = I_{f_1, \ldots, f_n, \varepsilon}$ that is Homogeneous, ""Almost linear"" on the functions we've chosen in the sense that $|I(f_i + f_j) - I(f_i) - I(f_j)| < \varepsilon$, Translation-invariant, Nontrivial in the sense that it is nonzero on a predetermined nonzero $f_0$, and Uniformly bounded (in the $f_i$ and $\varepsilon$) by a sublinear functional $K$. He then sketches a couple different arguments to show the existence of the functional that we want from here. One of the arguments he talks about uses the Hahn-Banach theorem. The gist, he says, is If one lets $\mathcal C$ be the space of all tuples $(f_1,\ldots,f_n,\epsilon)$, one can use the Hahn-Banach theorem to construct a bounded real linear functional $\lambda: \ell^\infty({\mathcal C}) \rightarrow {\mathbb R}$ that maps the constant sequence $1$ to $1$. If one then applies this functional to the $I_{f_1,\ldots,f_n,\epsilon}$ one can obtain a functional $I$ with the required properties. Could someone explain this argument or point me to a source where it's worked out in more detail? I can't even get started on how this would go.","I'm reading these notes of Terry Tao on the Haar measure (and related topics) on a locally compact Hausdorff group $G$. When he goes through the construction of the Haar measure, he does so by way of the Riesz representation theorem: if we had a linear, translation-invariant functional that satisfied a positivity property, by the Riesz representation theorem we'd get our measure. To that end he constructs, for any $f_1, \ldots, f_n \in \mathcal{C}_c(G)^+$ and $\varepsilon > 0$, a functional $I = I_{f_1, \ldots, f_n, \varepsilon}$ that is Homogeneous, ""Almost linear"" on the functions we've chosen in the sense that $|I(f_i + f_j) - I(f_i) - I(f_j)| < \varepsilon$, Translation-invariant, Nontrivial in the sense that it is nonzero on a predetermined nonzero $f_0$, and Uniformly bounded (in the $f_i$ and $\varepsilon$) by a sublinear functional $K$. He then sketches a couple different arguments to show the existence of the functional that we want from here. One of the arguments he talks about uses the Hahn-Banach theorem. The gist, he says, is If one lets $\mathcal C$ be the space of all tuples $(f_1,\ldots,f_n,\epsilon)$, one can use the Hahn-Banach theorem to construct a bounded real linear functional $\lambda: \ell^\infty({\mathcal C}) \rightarrow {\mathbb R}$ that maps the constant sequence $1$ to $1$. If one then applies this functional to the $I_{f_1,\ldots,f_n,\epsilon}$ one can obtain a functional $I$ with the required properties. Could someone explain this argument or point me to a source where it's worked out in more detail? I can't even get started on how this would go.",,"['functional-analysis', 'measure-theory', 'harmonic-analysis']"
38,derivative of cost function for Logistic Regression,derivative of cost function for Logistic Regression,,"I am going over the lectures on Machine Learning at Coursera. I am struggling with the following. How can the partial derivative of $$J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}y^{i}\log(h_\theta(x^{i}))+(1-y^{i})\log(1-h_\theta(x^{i}))$$ where $h_{\theta}(x)$ is defined as follows $$h_{\theta}(x)=g(\theta^{T}x)$$ $$g(z)=\frac{1}{1+e^{-z}}$$ be $$ \frac{\partial}{\partial\theta_{j}}J(\theta) =\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{i})-y^i)x_j^i$$ In other words, how would we go about calculating the partial derivative with respect to $\theta$ of the cost function (the logs are natural logarithms): $$J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}y^{i}\log(h_\theta(x^{i}))+(1-y^{i})\log(1-h_\theta(x^{i}))$$","I am going over the lectures on Machine Learning at Coursera. I am struggling with the following. How can the partial derivative of where is defined as follows be In other words, how would we go about calculating the partial derivative with respect to of the cost function (the logs are natural logarithms):",J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}y^{i}\log(h_\theta(x^{i}))+(1-y^{i})\log(1-h_\theta(x^{i})) h_{\theta}(x) h_{\theta}(x)=g(\theta^{T}x) g(z)=\frac{1}{1+e^{-z}}  \frac{\partial}{\partial\theta_{j}}J(\theta) =\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{i})-y^i)x_j^i \theta J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}y^{i}\log(h_\theta(x^{i}))+(1-y^{i})\log(1-h_\theta(x^{i})),"['statistics', 'regression', 'machine-learning', 'partial-derivative']"
39,How often does it happen that the oldest person alive dies?,How often does it happen that the oldest person alive dies?,,"Today, we are brought the sad news that Europe's oldest woman died. A little over a week ago the oldest person in the U.S. unfortunately died. Yesterday, the Netherlands' oldest man died peacefully. The Gerontology Research Group keeps records: Guinness World Records. If you live in a country with $N_{\text{country}}$ people, a continent with $N_{\text{continent}}$ people, and a world with $N_{\text{world}}$ people, during a year and on average, how often will you be notified (if you're paying attention to your quality tabloid) of the death of the oldest man/woman/person alive of your country/continent/world? (Note that a death will result in at most one notification.) Edit, Suggestions (due to comments; thanks!) only: Ultimately, I am looking for a realistic formula. That means that life tables are certainly allowed (but mind the ending , and note that some of the oldest people are older than the maximum in the tables). I guess that the Gompertz–Makeham law of mortality , or any plausible model of late-life mortality , is also fair game. However, these suggestions do not preclude some insight or realistic assumption that might not need any of those things. Further potentially useful assumptions (with regard to a subpopulation) may include (if reasonable w.r.t. the question): Time is discrete. The number of births at any time is constant. The number of deaths at any time is constant. The number of births is equal to the number of deaths at any time. Time is defined in such a way that, at any time, one birth and one death occur. In the limit to infinite age, the probability of a person of such age dying goes to $1$. At any time, the oldest person(s) alive is (are) the most likely to die. At any time, an older person(s) alive is (are) more likely to die than any younger person alive. (Although such assumptions aren't necessarily realistic, I don't immediately see how they would distort the outcome.)","Today, we are brought the sad news that Europe's oldest woman died. A little over a week ago the oldest person in the U.S. unfortunately died. Yesterday, the Netherlands' oldest man died peacefully. The Gerontology Research Group keeps records: Guinness World Records. If you live in a country with $N_{\text{country}}$ people, a continent with $N_{\text{continent}}$ people, and a world with $N_{\text{world}}$ people, during a year and on average, how often will you be notified (if you're paying attention to your quality tabloid) of the death of the oldest man/woman/person alive of your country/continent/world? (Note that a death will result in at most one notification.) Edit, Suggestions (due to comments; thanks!) only: Ultimately, I am looking for a realistic formula. That means that life tables are certainly allowed (but mind the ending , and note that some of the oldest people are older than the maximum in the tables). I guess that the Gompertz–Makeham law of mortality , or any plausible model of late-life mortality , is also fair game. However, these suggestions do not preclude some insight or realistic assumption that might not need any of those things. Further potentially useful assumptions (with regard to a subpopulation) may include (if reasonable w.r.t. the question): Time is discrete. The number of births at any time is constant. The number of deaths at any time is constant. The number of births is equal to the number of deaths at any time. Time is defined in such a way that, at any time, one birth and one death occur. In the limit to infinite age, the probability of a person of such age dying goes to $1$. At any time, the oldest person(s) alive is (are) the most likely to die. At any time, an older person(s) alive is (are) more likely to die than any younger person alive. (Although such assumptions aren't necessarily realistic, I don't immediately see how they would distort the outcome.)",,"['statistics', 'markov-chains', 'actuarial-science']"
40,Variance of sample variance?,Variance of sample variance?,,What is the variance of the sample variance? In other words I am looking for $\mathrm{Var}(S^2)$. I have started by expanding out $\mathrm{Var}(S^2)$ into $E(S^4) - [E(S^2)]^2$ I know that $[E(S^2)]^2$ is $\sigma$ to the power of 4. And that is as far as I got.,What is the variance of the sample variance? In other words I am looking for $\mathrm{Var}(S^2)$. I have started by expanding out $\mathrm{Var}(S^2)$ into $E(S^4) - [E(S^2)]^2$ I know that $[E(S^2)]^2$ is $\sigma$ to the power of 4. And that is as far as I got.,,['statistics']
41,"Density of sum of two independent uniform random variables on $[0,1]$",Density of sum of two independent uniform random variables on,"[0,1]","I am trying to understand an example from my textbook. Let's say $Z = X + Y$ , where $X$ and $Y$ are independent uniform random variables with range $[0,1]$ . Then the PDF is $$f(z) = \begin{cases} z & \text{for $0 < z < 1$} \\ 2-z & \text{for $1 \le z < 2$} \\ 0 & \text{otherwise.} \end{cases}$$ How was this PDF obtained? Thanks","I am trying to understand an example from my textbook. Let's say , where and are independent uniform random variables with range . Then the PDF is How was this PDF obtained? Thanks","Z = X + Y X Y [0,1] f(z) = \begin{cases}
z & \text{for 0 < z < 1} \\
2-z & \text{for 1 \le z < 2} \\
0 & \text{otherwise.}
\end{cases}","['statistics', 'probability-distributions', 'uniform-distribution', 'density-function']"
42,What's so special about standard deviation?,What's so special about standard deviation?,,"Equivalently, about variance? I realize it measures the spread of a distribution, but many other metrics could do the same (e.g., the average absolute deviation). What is its deeper significance? Does it have a particular geometric interpretation (in the sense, e.g., that the mean is the balancing point of a distribution)? any other intuitive interpretation that differentiates it from other possible measures of spread? What's so special about it that makes it act as a normalizing factor in all sorts of situations (for example, convert covariance to correlation)?","Equivalently, about variance? I realize it measures the spread of a distribution, but many other metrics could do the same (e.g., the average absolute deviation). What is its deeper significance? Does it have a particular geometric interpretation (in the sense, e.g., that the mean is the balancing point of a distribution)? any other intuitive interpretation that differentiates it from other possible measures of spread? What's so special about it that makes it act as a normalizing factor in all sorts of situations (for example, convert covariance to correlation)?",,['statistics']
43,How was the normal distribution derived?,How was the normal distribution derived?,,"Abraham de Moivre, when he came up with this formula, had to assure that the points of inflection were exactly one standard deviation away from the center, and so that it was bell-shaped, as well as make sure that the area under the curve was exactly equal to one. And somehow they came up with the standard normal distribution, which is as follows: $$\displaystyle\phi(x) = \frac{1}{\sqrt{2\pi}}e^{-\dfrac{1}{2}x^2}$$ And even cooler, he found the distribution for when the mean was not $0$ and the standard deviation was not $1$, and came up with: $$\displaystyle f(x) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\dfrac{(x - \mu)^2}{2\sigma^2}}$$ And so what I ask is, how? How was an equation come up with that fit all the aforementioned criteria? Moreover, how do the numbers $\pi$ and $e$ come into this?","Abraham de Moivre, when he came up with this formula, had to assure that the points of inflection were exactly one standard deviation away from the center, and so that it was bell-shaped, as well as make sure that the area under the curve was exactly equal to one. And somehow they came up with the standard normal distribution, which is as follows: $$\displaystyle\phi(x) = \frac{1}{\sqrt{2\pi}}e^{-\dfrac{1}{2}x^2}$$ And even cooler, he found the distribution for when the mean was not $0$ and the standard deviation was not $1$, and came up with: $$\displaystyle f(x) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\dfrac{(x - \mu)^2}{2\sigma^2}}$$ And so what I ask is, how? How was an equation come up with that fit all the aforementioned criteria? Moreover, how do the numbers $\pi$ and $e$ come into this?",,"['statistics', 'probability-distributions', 'math-history', 'normal-distribution']"
44,incremental computation of standard deviation,incremental computation of standard deviation,,"How can I compute the standard deviation in an incremental way (using the new value and the last computed mean and/or std deviation) ? for the non incremental way, I just do something like: $$S_N=\sqrt{\frac1N\sum_{i=1}^N(x_i-\overline{x})^2}.$$ mean = Mean(list) for i = 0 to list.size    stdev = stdev + (list[i] - mean)^2 stdev = sqrRoot( stdev / list.size )","How can I compute the standard deviation in an incremental way (using the new value and the last computed mean and/or std deviation) ? for the non incremental way, I just do something like: mean = Mean(list) for i = 0 to list.size    stdev = stdev + (list[i] - mean)^2 stdev = sqrRoot( stdev / list.size )",S_N=\sqrt{\frac1N\sum_{i=1}^N(x_i-\overline{x})^2}.,"['statistics', 'algorithms', 'computational-mathematics', 'standard-deviation']"
45,Why do we use a Least Squares fit?,Why do we use a Least Squares fit?,,"I've been wondering for a while now if there's any deep mathematical or statistical significance to finding the line that minimizes the square of the errors between the line and the data points. If we use a less common method like LAD, where we just consider the absolute deviation, then outliers make less difference to the final model, while if we take the cube of the error (or any other power higher than 2), then outliers are far more significant than with the least squares model. I suppose what I'm really asking is mathematically , is raising the error to the power of 2 really that special. Is it say more ""accurate"" in some sense than raising the error to the power of 1.95 or 2.05??? Thanks!","I've been wondering for a while now if there's any deep mathematical or statistical significance to finding the line that minimizes the square of the errors between the line and the data points. If we use a less common method like LAD, where we just consider the absolute deviation, then outliers make less difference to the final model, while if we take the cube of the error (or any other power higher than 2), then outliers are far more significant than with the least squares model. I suppose what I'm really asking is mathematically , is raising the error to the power of 2 really that special. Is it say more ""accurate"" in some sense than raising the error to the power of 1.95 or 2.05??? Thanks!",,"['statistics', 'regression']"
46,Why we consider log likelihood instead of Likelihood in Gaussian Distribution,Why we consider log likelihood instead of Likelihood in Gaussian Distribution,,"I am reading Gaussian Distribution from a machine learning book. It states that - We shall determine values for the unknown parameters $\mu$ and   $\sigma^2$ in the Gaussian by maximizing the likelihood function. In practice, it is more convenient to maximize the log of the likelihood   function. Because the logarithm is monotonically increasing function   of its argument, maximization of the log of a function is equivalent   to maximization of the function itself. Taking the log not only   simplifies the subsequent mathematical analysis, but it also helps   numerically because the product of a large number of small   probabilities can easily underflow the numerical precision of the   computer, and this is resolved by computing instead the sum of the log   probabilities. can anyone give me some intuition behind it with some example? Where the log likelihood is more convenient over likelihood. Please give me a practical example. Thanks in advance!","I am reading Gaussian Distribution from a machine learning book. It states that - We shall determine values for the unknown parameters $\mu$ and   $\sigma^2$ in the Gaussian by maximizing the likelihood function. In practice, it is more convenient to maximize the log of the likelihood   function. Because the logarithm is monotonically increasing function   of its argument, maximization of the log of a function is equivalent   to maximization of the function itself. Taking the log not only   simplifies the subsequent mathematical analysis, but it also helps   numerically because the product of a large number of small   probabilities can easily underflow the numerical precision of the   computer, and this is resolved by computing instead the sum of the log   probabilities. can anyone give me some intuition behind it with some example? Where the log likelihood is more convenient over likelihood. Please give me a practical example. Thanks in advance!",,"['statistics', 'normal-distribution', 'machine-learning']"
47,Sample Standard Deviation vs. Population Standard Deviation,Sample Standard Deviation vs. Population Standard Deviation,,"I have an HP 50g graphing calculator and I am using it to calculate the standard deviation of some data. In the statistics calculation there is a type which can have two values: Sample Population I didn't change it, but I kept getting the wrong results for the standard deviation. When I changed it to ""Population"" type, I started getting correct results! Why is that? As far as I know, there is only one type of standard deviation which is to calculate the root-mean-square of the values! Did I miss something?","I have an HP 50g graphing calculator and I am using it to calculate the standard deviation of some data. In the statistics calculation there is a type which can have two values: Sample Population I didn't change it, but I kept getting the wrong results for the standard deviation. When I changed it to ""Population"" type, I started getting correct results! Why is that? As far as I know, there is only one type of standard deviation which is to calculate the root-mean-square of the values! Did I miss something?",,"['statistics', 'standard-deviation']"
48,how does expectation maximization work?,how does expectation maximization work?,,"I'm reading a tutorial on expectation maximization which gives an example of a coin flipping experiment (the description is at http://www.nature.com/nbt/journal/v26/n8/full/nbt1406.html?pagewanted=all ). Could you please help me understand where the probabilities in step 2 of the process (i.e. in the middle of part b in the below illustration) come from? Thank you. EM starts with an initial guess of the parameters. 2. In the E-step, a probability distribution over possible completions is computed using the current parameters. The counts shown in the table are the expected numbers of heads and tails according to this distribution. 3. In the M-step, new parameters are determined using the current completions. 4. After several repetitions of the E-step and M-step, the algorithm converges.","I'm reading a tutorial on expectation maximization which gives an example of a coin flipping experiment (the description is at http://www.nature.com/nbt/journal/v26/n8/full/nbt1406.html?pagewanted=all ). Could you please help me understand where the probabilities in step 2 of the process (i.e. in the middle of part b in the below illustration) come from? Thank you. EM starts with an initial guess of the parameters. 2. In the E-step, a probability distribution over possible completions is computed using the current parameters. The counts shown in the table are the expected numbers of heads and tails according to this distribution. 3. In the M-step, new parameters are determined using the current completions. 4. After several repetitions of the E-step and M-step, the algorithm converges.",,['statistics']
49,Difference between Poisson and Binomial distributions. [closed],Difference between Poisson and Binomial distributions. [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question If both the Poisson and Binomial distribution are discrete, then why do we need two different distributions?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question If both the Poisson and Binomial distribution are discrete, then why do we need two different distributions?",,['statistics']
50,Why the sum of residuals equals 0 when we do a sample regression by OLS?,Why the sum of residuals equals 0 when we do a sample regression by OLS?,,"That's my question, I have looking round online and people post a formula by they don't explain the formula. Could anyone please give me a hand with that ? cheers","That's my question, I have looking round online and people post a formula by they don't explain the formula. Could anyone please give me a hand with that ? cheers",,"['statistics', 'statistical-inference']"
51,Why does Benford's Law (or Zipf's Law) hold?,Why does Benford's Law (or Zipf's Law) hold?,,"Both Benford's Law (if you take a list of values, the distribution of the most significant digit is rougly proportional to the logarithm of the digit) and Zipf's Law (given a corpus of natural language utterances, the frequency of any word is roughly inversely proportional to its rank in the frequency table) are not theorems in a mathematical sense, but they work quite good in the real life. Does anyone have an idea why this happens? (see also this question )","Both Benford's Law (if you take a list of values, the distribution of the most significant digit is rougly proportional to the logarithm of the digit) and Zipf's Law (given a corpus of natural language utterances, the frequency of any word is roughly inversely proportional to its rank in the frequency table) are not theorems in a mathematical sense, but they work quite good in the real life. Does anyone have an idea why this happens? (see also this question )",,"['statistics', 'soft-question', 'applications']"
52,Real life usage of Benford's Law,Real life usage of Benford's Law,,I recently discovered Benford's Law. I find it very fascinating. I'm wondering what are some of the real life uses of Benford's law. Specific examples would be great.,I recently discovered Benford's Law. I find it very fascinating. I'm wondering what are some of the real life uses of Benford's law. Specific examples would be great.,,"['soft-question', 'big-list', 'statistics', 'applications']"
53,Is there a mathematical reason why chocolate chip cookies have 37% (1/e) chocolate in them?,Is there a mathematical reason why chocolate chip cookies have 37% (1/e) chocolate in them?,,"Someone once briefly explained to me why it is that chocolate chip cookies have 37% chocolate in them. To the best of my memory it has to do with the way trying to place dots in a circle in a random and scattered way behave, which turns out to be $1/e$ or $\approx37%$ . There are 2 levels of validating this: The first is the theoretical side: Can you find a suitable definition for ""random and scattered"" in a circle that fits the use case and the $1/e$ behavior? The second level is the practical. How does the size of the dots and volume impact this phenomenon? Are there real-life constraints that force it to be $1/e$ ? Edit http://sarcasticresonance.wordpress.com/2012/03/11/1e-do-not-lie/ This blogger validated the fact these do have 37% chocolate, and after contacting him he provided me with the following partial explanation, yet for this to become a full answer some conversion of the problem is needed and practical considerations to be taken. Let us assume the following: In the factory the manufacturing process start with a chunk of chocolate the size of a cookie, which is made up from a million chocolate particles. After that there are a million robotic arms. Each arm chooses a chocolate particle randomly and replaces it with cookie dough, unfortunately there is no synchronization between the arms, and its possible for few arm to switch the same chocolate particle. Its obvious that not all million of the particles will be switched but less, and therefore we will have some mix, the question is what will the ratio be. Let us look on a specific particle, what is the probability of it being switched? hard to calculate directly since it may be chosen by some or all arms, but can be calculated throw elimination: an arm doesn't choose it if it happens to choose another particle. That means: $(N-1)/N = 1 - 1/N$ Is sum of all articles so the probability of no arm choosing it is N when: $(1-1/N)^N$ And that makes the probability of an arm to do choose any particle: $1 - (1-1/N)^N$ Well approach n to infinity, well use the know fact: $(1-1/N)^N ----> 1/e$ We will get that in average: $1-1/e = 0.63$ Which means 0.63 of the chocolate is being switch and there for 37% chocolate is left. The question remains on the following points: Can this be converted to scattering things in a circle? [this will make the next part easier] Does this fit to real life constraints? or is it not? edit 2 As requested and in reponse to the claim that 37% refers to the chocolate itself, iv'e added a picture of the back with the ingridients","Someone once briefly explained to me why it is that chocolate chip cookies have 37% chocolate in them. To the best of my memory it has to do with the way trying to place dots in a circle in a random and scattered way behave, which turns out to be or . There are 2 levels of validating this: The first is the theoretical side: Can you find a suitable definition for ""random and scattered"" in a circle that fits the use case and the behavior? The second level is the practical. How does the size of the dots and volume impact this phenomenon? Are there real-life constraints that force it to be ? Edit http://sarcasticresonance.wordpress.com/2012/03/11/1e-do-not-lie/ This blogger validated the fact these do have 37% chocolate, and after contacting him he provided me with the following partial explanation, yet for this to become a full answer some conversion of the problem is needed and practical considerations to be taken. Let us assume the following: In the factory the manufacturing process start with a chunk of chocolate the size of a cookie, which is made up from a million chocolate particles. After that there are a million robotic arms. Each arm chooses a chocolate particle randomly and replaces it with cookie dough, unfortunately there is no synchronization between the arms, and its possible for few arm to switch the same chocolate particle. Its obvious that not all million of the particles will be switched but less, and therefore we will have some mix, the question is what will the ratio be. Let us look on a specific particle, what is the probability of it being switched? hard to calculate directly since it may be chosen by some or all arms, but can be calculated throw elimination: an arm doesn't choose it if it happens to choose another particle. That means: Is sum of all articles so the probability of no arm choosing it is N when: And that makes the probability of an arm to do choose any particle: Well approach n to infinity, well use the know fact: We will get that in average: Which means 0.63 of the chocolate is being switch and there for 37% chocolate is left. The question remains on the following points: Can this be converted to scattering things in a circle? [this will make the next part easier] Does this fit to real life constraints? or is it not? edit 2 As requested and in reponse to the claim that 37% refers to the chocolate itself, iv'e added a picture of the back with the ingridients",1/e \approx37% 1/e 1/e (N-1)/N = 1 - 1/N (1-1/N)^N 1 - (1-1/N)^N (1-1/N)^N ----> 1/e 1-1/e = 0.63,['statistics']
54,Proof of upper-tail inequality for standard normal distribution,Proof of upper-tail inequality for standard normal distribution,,"$X \sim \mathcal{N}(0,1)$, then to show that for $x > 0$, $$ \mathbb{P}(X>x) \leq \frac{\exp(-x^2/2)}{x \sqrt{2 \pi}} \>. $$","$X \sim \mathcal{N}(0,1)$, then to show that for $x > 0$, $$ \mathbb{P}(X>x) \leq \frac{\exp(-x^2/2)}{x \sqrt{2 \pi}} \>. $$",,"['statistics', 'inequality']"
55,Intuitive Explanation of Bessel's Correction,Intuitive Explanation of Bessel's Correction,,When calculating a sample variance a factor of $N-1$ appears instead of $N$ (see this link ).  Does anybody have an intuitive way of explaining this to students who need to use this fact but maybe haven't taken a statistics course?,When calculating a sample variance a factor of appears instead of (see this link ).  Does anybody have an intuitive way of explaining this to students who need to use this fact but maybe haven't taken a statistics course?,N-1 N,"['statistics', 'intuition']"
56,Why does zero correlation not imply independence?,Why does zero correlation not imply independence?,,"Although independence implies zero correlation, zero correlation does not necessarily imply independence. While I understand the concept, I can't imagine a real world situation with zero correlation that did not also have independence. Can someone please give me an example so I can better understand this phenomenon?","Although independence implies zero correlation, zero correlation does not necessarily imply independence. While I understand the concept, I can't imagine a real world situation with zero correlation that did not also have independence. Can someone please give me an example so I can better understand this phenomenon?",,"['statistics', 'independence']"
57,Why is variance squared?,Why is variance squared?,,"The mean absolute deviation is: $$\dfrac{\sum_{i=1}^{n}|x_i-\bar x|}{n}$$ The variance is: $$\dfrac{\sum_{i=1}^{n}(x_i-\bar x)^2}{n-1}$$ So the mean deviation and the variance are measuring the same thing, yet variance requires squaring the difference. Why? Squaring always gives a non-negative value, but the absolute value is also a non-negative value. Why isn't it $|x_i-\bar x|^2$ , then? Squaring just enlarges, why do we need to do this? A similar question is here , but mine is a little different. Thanks.","The mean absolute deviation is: The variance is: So the mean deviation and the variance are measuring the same thing, yet variance requires squaring the difference. Why? Squaring always gives a non-negative value, but the absolute value is also a non-negative value. Why isn't it , then? Squaring just enlarges, why do we need to do this? A similar question is here , but mine is a little different. Thanks.",\dfrac{\sum_{i=1}^{n}|x_i-\bar x|}{n} \dfrac{\sum_{i=1}^{n}(x_i-\bar x)^2}{n-1} |x_i-\bar x|^2,"['statistics', 'standard-deviation', 'descriptive-statistics']"
58,Motivation behind standard deviation?,Motivation behind standard deviation?,,"Let's take the numbers 0-10.  Their mean is 5, and the individual deviations from 5 are -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5 And so the average (magnitude of) deviation from the mean is $30/11 \approx 2.72$. However, this is not the standard deviation.  The standard deviation is $\sqrt{10} \approx 3.16$. The first mean-deviation is a simpler and by far more intuitive definition of the ""standard-deviation"" , so I'm sure it's the first definition statisticians worked with.  However, for some reason they decided to adopt the second definition instead.  What is the reasoning behind that decision?","Let's take the numbers 0-10.  Their mean is 5, and the individual deviations from 5 are -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5 And so the average (magnitude of) deviation from the mean is $30/11 \approx 2.72$. However, this is not the standard deviation.  The standard deviation is $\sqrt{10} \approx 3.16$. The first mean-deviation is a simpler and by far more intuitive definition of the ""standard-deviation"" , so I'm sure it's the first definition statisticians worked with.  However, for some reason they decided to adopt the second definition instead.  What is the reasoning behind that decision?",,"['intuition', 'statistics', 'standard-deviation']"
59,Is it possible to have 2 different but equal size real number sets that have the same mean and standard deviation?,Is it possible to have 2 different but equal size real number sets that have the same mean and standard deviation?,,"By inspection I notice that Shifting does not change the standard deviation but change mean. {1,3,4} has the same standard deviation as {11,13,14} for example. Sets with the same (or reversed) sequence of adjacent difference have the same standard deviation. For example, {1,3,4} , {0,2,3} , {0,1,3} have the same standard deviation. But the means are different. My conjecture: There are no two distinct sets with the same length, mean and standard deviation. Question Is it possible to have 2 different but equal size real number sets that have the same mean and standard deviation?","By inspection I notice that Shifting does not change the standard deviation but change mean. {1,3,4} has the same standard deviation as {11,13,14} for example. Sets with the same (or reversed) sequence of adjacent difference have the same standard deviation. For example, {1,3,4} , {0,2,3} , {0,1,3} have the same standard deviation. But the means are different. My conjecture: There are no two distinct sets with the same length, mean and standard deviation. Question Is it possible to have 2 different but equal size real number sets that have the same mean and standard deviation?",,"['statistics', 'standard-deviation', 'means']"
60,Proof of $\frac{(n-1)S^2}{\sigma^2} \sim \chi^2_{n-1}$,Proof of,\frac{(n-1)S^2}{\sigma^2} \sim \chi^2_{n-1},"It's a standard result that given $X_1,\cdots ,X_n $ random sample from $N(\mu,\sigma^2)$, the random variable $$\frac{(n-1)S^2}{\sigma^2}$$ has a chi-square distribution with $(n-1)$ degrees of freedom, where $$S^2=\frac{1}{n-1}\sum^{n}_{i=1}(X_i-\bar{X})^2.$$ I would like help in proving the above result. Thanks.","It's a standard result that given $X_1,\cdots ,X_n $ random sample from $N(\mu,\sigma^2)$, the random variable $$\frac{(n-1)S^2}{\sigma^2}$$ has a chi-square distribution with $(n-1)$ degrees of freedom, where $$S^2=\frac{1}{n-1}\sum^{n}_{i=1}(X_i-\bar{X})^2.$$ I would like help in proving the above result. Thanks.",,"['statistics', 'probability-distributions']"
61,maximum estimator method more known as MLE of a uniform distribution [closed],maximum estimator method more known as MLE of a uniform distribution [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Let  $ X_1, ... X_n $ a sample of independent random variables with uniform distribution $(0,$$ \theta  $$ ) $ Find a $ $$ \widehat\theta  $$  $ estimator for theta using the maximun estimator method more known as MLE","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Let  $ X_1, ... X_n $ a sample of independent random variables with uniform distribution $(0,$$ \theta  $$ ) $ Find a $ $$ \widehat\theta  $$  $ estimator for theta using the maximun estimator method more known as MLE",,['statistics']
62,What is the use of moments in statistics?,What is the use of moments in statistics?,,Can anyone give me a simple explanation (i.e. without too many equations) of what is the use of moments in statistics? Why do we need moments? What can we learn from them?,Can anyone give me a simple explanation (i.e. without too many equations) of what is the use of moments in statistics? Why do we need moments? What can we learn from them?,,"['statistics', 'probability-distributions']"
63,Why get the sum of squares instead of the sum of absolute values?,Why get the sum of squares instead of the sum of absolute values?,,"I'm self-studying machine learning and getting into the basics of linear regression models. From what I understand so far, a good regression model minimizes the sum of the squared differences between predicted values $h(x)$ and actual values $y$. Something like the following: $$\sum_{i=1}^m (h(x_i)-y_i)^2$$ Why do we square the differences? On one hand, it seems squaring them will allow us to get a positive number when the expected value is less than the actual value. But why can't this just be accounted for by taking the sum of the absolute values? Like so: $$\sum_{i=1}^m |h(x_i)-y_i|$$","I'm self-studying machine learning and getting into the basics of linear regression models. From what I understand so far, a good regression model minimizes the sum of the squared differences between predicted values $h(x)$ and actual values $y$. Something like the following: $$\sum_{i=1}^m (h(x_i)-y_i)^2$$ Why do we square the differences? On one hand, it seems squaring them will allow us to get a positive number when the expected value is less than the actual value. But why can't this just be accounted for by taking the sum of the absolute values? Like so: $$\sum_{i=1}^m |h(x_i)-y_i|$$",,['statistics']
64,"Guessing the length of a playlist on ""shuffle random?""","Guessing the length of a playlist on ""shuffle random?""",,"The other night I was hanging out with some friends and someone put on a playlist on shuffle random, where the songs are drawn uniformly at random from a fixed playlist. The person who put the playlist together forgot how many songs were in it, so the topic came up of how to estimate the size of the playlist purely based on what we hearing. We came up with a few high-level ideas for how to do this. For example, using ideas from the Birthday Paradox, we thought we could listen until we heard a song repeated for the first time, then use that to make an educated guess about how many songs were on the playlist in total. We also thought that we could listen for a long time and build up a frequency histogram of the number of times each song was played, then use the fact that it should look somewhat normally distributed to get the mean and variance and from there estimate the total number of songs on the list. None of us are statisticians or have a lot of training in machine learning, but I suspect that this is probably a well-studied problem and that there are some really nice techniques we can use to estimate the playlist size. Are there a good family of techniques for estimating the playlist size? From a practical perspective, would any of these techniques be something that would be relatively easy to work out without a computer or calculator? Thanks!","The other night I was hanging out with some friends and someone put on a playlist on shuffle random, where the songs are drawn uniformly at random from a fixed playlist. The person who put the playlist together forgot how many songs were in it, so the topic came up of how to estimate the size of the playlist purely based on what we hearing. We came up with a few high-level ideas for how to do this. For example, using ideas from the Birthday Paradox, we thought we could listen until we heard a song repeated for the first time, then use that to make an educated guess about how many songs were on the playlist in total. We also thought that we could listen for a long time and build up a frequency histogram of the number of times each song was played, then use the fact that it should look somewhat normally distributed to get the mean and variance and from there estimate the total number of songs on the list. None of us are statisticians or have a lot of training in machine learning, but I suspect that this is probably a well-studied problem and that there are some really nice techniques we can use to estimate the playlist size. Are there a good family of techniques for estimating the playlist size? From a practical perspective, would any of these techniques be something that would be relatively easy to work out without a computer or calculator? Thanks!",,"['statistics', 'recreational-mathematics', 'birthday']"
65,Calculate variance from a stream of sample values,Calculate variance from a stream of sample values,,"I'd like to calculate a standard deviation for a very large (but known) number of sample values, with the highest accuracy possible.  The number of samples is larger than can be efficiently stored in memory. The basic variance formula is: $\sigma^2 = \frac{1}{N}\sum (x - \mu)^2$ ... but this formulation depends on knowing the value of $\mu$ already. $\mu$ can be calculated cumulatively -- that is, you can calculate the mean without storing every sample value.  You just have to store their sum. But to calculate the variance, is it necessary to store every sample value?  Given a stream of samples, can I accumulate a calculation of the variance, without a need for memory of each sample?  Put another way, is there a formulation of the variance which doesn't depend on foreknowledge of the exact value of $\mu$ before the whole sample set has been seen?","I'd like to calculate a standard deviation for a very large (but known) number of sample values, with the highest accuracy possible.  The number of samples is larger than can be efficiently stored in memory. The basic variance formula is: $\sigma^2 = \frac{1}{N}\sum (x - \mu)^2$ ... but this formulation depends on knowing the value of $\mu$ already. $\mu$ can be calculated cumulatively -- that is, you can calculate the mean without storing every sample value.  You just have to store their sum. But to calculate the variance, is it necessary to store every sample value?  Given a stream of samples, can I accumulate a calculation of the variance, without a need for memory of each sample?  Put another way, is there a formulation of the variance which doesn't depend on foreknowledge of the exact value of $\mu$ before the whole sample set has been seen?",,"['statistics', 'algorithms']"
66,"""Normalize"" values to sum 1 but keeping their weights","""Normalize"" values to sum 1 but keeping their weights",,"I am not really sure what this operation might be called, but I have some numbers, for example: 40 10 I need to format these numbers so that they form the sum 1, but they should keep their ""weight"". In this specific case: 40 would become 0.80 10 would become 0.2 But if I have more values (like 40, 10, 25, 5 for example), I am really lost because I don't know the formula. If anybody can help, could they please reply in words (for example: ""Sum up all values then divide through..."", and not in a formula? I am really not good at reading formulas at all.","I am not really sure what this operation might be called, but I have some numbers, for example: 40 10 I need to format these numbers so that they form the sum 1, but they should keep their ""weight"". In this specific case: 40 would become 0.80 10 would become 0.2 But if I have more values (like 40, 10, 25, 5 for example), I am really lost because I don't know the formula. If anybody can help, could they please reply in words (for example: ""Sum up all values then divide through..."", and not in a formula? I am really not good at reading formulas at all.",,['statistics']
67,Maximum Likelihood Estimator of parameters of multinomial distribution,Maximum Likelihood Estimator of parameters of multinomial distribution,,"Suppose that 50 measuring scales made by a machine are selected at random from the production of the machine and their lengths and widths are measured. It was found that 45 had both measurements within the tolerance limits, 2 had satisfactory length but unsatisfactory width, 2 had satisfactory width but unsatisfactory length, 1 had both length and width unsatisfactory. Each scale may be regarded as a drawing from a multinomial population with density $$ \pi_{11}^{x_{11}} \pi_{12}^{x_{12}} \pi_{21}^{x_{21}}(1-\pi_{11}-\pi_{12}-\pi_{21})^{x_{22}} $$ Obtain the maximum likelihood estimates of the parameters. I have tried this by the following way: the likelihood function is \begin{align} L & =L(\pi_{11},\pi_{12},\pi_{21},(1-\pi_{11}-\pi_{12}-\pi_{21})) \\[8pt] & =\prod_{i=1}^{50}[\pi_{11}^{x_{11}} \pi_{12}^{x_{12}} \pi_{21}^{x_{21}}(1-\pi_{11}-\pi_{12}-\pi_{21})^{x_{22}}] \\[8pt] & =[\pi_{11}^{x_{11}} \pi_{12}^{x_{12}} \pi_{21}^{x_{21}}(1-\pi_{11}-\pi_{12}-\pi_{21})^{x_{22}}]^{50} \\[8pt] & =[\pi_{11}^{45}\pi_{12}^{2} \pi_{21}^{2}(1-\pi_{11}-\pi_{12}-\pi_{21})^{1}]^{50} \\[8pt] & =\pi_{11}^{2250}\pi_{12}^{100} \pi_{21}^{100}(1-\pi_{11}-\pi_{12}-\pi_{21})^{50} \end{align} Taking logarithm of the likelihood function yields, \begin{align} L^*& =\log L=\log \left[\pi_{11}^{2250} \pi_{12}^{100} \pi_{21}^{100}(1-\pi_{11}-\pi_{12}-\pi_{21})^{50}\right] \\[8pt] & =2250\log [\pi_{11}]+100\log [\pi_{12}]+100\log [\pi_{21}]+50\log (1-\pi_{11}-\pi_{12}-\pi_{21}) \end{align} Now taking the first derivative of $L^*$ with respect to $\pi_{11}$ $\frac{\partial L^*}{\partial \pi_{11}}$ $=\frac{2250}{\pi_{11}}-\frac{50}{(1-\pi_{11}-\pi_{12}-\pi_{21})}$ setting $\frac{\partial L^*}{\partial \pi_{11}}$ equal to $0$ , $$\frac{\partial L^*}{\partial \hat\pi_{11}}=0$$ $$\Rightarrow\frac{2250}{\hat\pi_{11}}-\frac{50}{(1-\hat\pi_{11}-\hat\pi_{12}-\hat\pi_{21})}=0$$ $$\Rightarrow \hat\pi_{11}=\frac{45(1-\hat\pi_{12}-\hat\pi_{21})}{44}$$ $\bullet$ Are the procedure and estimate of $\pi_{11}$ correct? $\bullet$ I have another question that if it is multinomial then where the term $\binom{n}{x_{11}x_{12}x_{21}x_{22}}=\binom{50}{45,2,2,1}$ ?","Suppose that 50 measuring scales made by a machine are selected at random from the production of the machine and their lengths and widths are measured. It was found that 45 had both measurements within the tolerance limits, 2 had satisfactory length but unsatisfactory width, 2 had satisfactory width but unsatisfactory length, 1 had both length and width unsatisfactory. Each scale may be regarded as a drawing from a multinomial population with density Obtain the maximum likelihood estimates of the parameters. I have tried this by the following way: the likelihood function is Taking logarithm of the likelihood function yields, Now taking the first derivative of with respect to setting equal to , Are the procedure and estimate of correct? I have another question that if it is multinomial then where the term ?","
\pi_{11}^{x_{11}}
\pi_{12}^{x_{12}}
\pi_{21}^{x_{21}}(1-\pi_{11}-\pi_{12}-\pi_{21})^{x_{22}}
 \begin{align}
L & =L(\pi_{11},\pi_{12},\pi_{21},(1-\pi_{11}-\pi_{12}-\pi_{21})) \\[8pt]
& =\prod_{i=1}^{50}[\pi_{11}^{x_{11}} \pi_{12}^{x_{12}}
\pi_{21}^{x_{21}}(1-\pi_{11}-\pi_{12}-\pi_{21})^{x_{22}}] \\[8pt]
& =[\pi_{11}^{x_{11}} \pi_{12}^{x_{12}}
\pi_{21}^{x_{21}}(1-\pi_{11}-\pi_{12}-\pi_{21})^{x_{22}}]^{50} \\[8pt]
& =[\pi_{11}^{45}\pi_{12}^{2}
\pi_{21}^{2}(1-\pi_{11}-\pi_{12}-\pi_{21})^{1}]^{50} \\[8pt]
& =\pi_{11}^{2250}\pi_{12}^{100}
\pi_{21}^{100}(1-\pi_{11}-\pi_{12}-\pi_{21})^{50}
\end{align} \begin{align}
L^*& =\log L=\log \left[\pi_{11}^{2250} \pi_{12}^{100}
\pi_{21}^{100}(1-\pi_{11}-\pi_{12}-\pi_{21})^{50}\right] \\[8pt]
& =2250\log [\pi_{11}]+100\log [\pi_{12}]+100\log [\pi_{21}]+50\log (1-\pi_{11}-\pi_{12}-\pi_{21})
\end{align} L^* \pi_{11} \frac{\partial L^*}{\partial \pi_{11}} =\frac{2250}{\pi_{11}}-\frac{50}{(1-\pi_{11}-\pi_{12}-\pi_{21})} \frac{\partial L^*}{\partial \pi_{11}} 0 \frac{\partial L^*}{\partial \hat\pi_{11}}=0 \Rightarrow\frac{2250}{\hat\pi_{11}}-\frac{50}{(1-\hat\pi_{11}-\hat\pi_{12}-\hat\pi_{21})}=0 \Rightarrow \hat\pi_{11}=\frac{45(1-\hat\pi_{12}-\hat\pi_{21})}{44} \bullet \pi_{11} \bullet \binom{n}{x_{11}x_{12}x_{21}x_{22}}=\binom{50}{45,2,2,1}","['statistics', 'probability-distributions', 'statistical-inference', 'maximum-likelihood', 'parameter-estimation']"
68,Is there a simple test for uniform distributions?,Is there a simple test for uniform distributions?,,"I have a function that (more or less) is supposed to select a small number $m$ of random numbers from the range $[1,n]$ (for some $n \gg m$) and I need to test that it work. Is there an easy to implement test that will give good confidence that it is working correctly? (For reference the full on chi squared test is not simple enough.) Edit: $m$ in $[5,500]$, $n$ is small enough I can use normal int/float types for most math. I can get as many sets as I'm willing to wait for ($>100$s per second) The reason I'm looking for something simple is that the code I'm checking isn't that complicated. If the test code is more complex than the code under test, then the potential for the test being wrong becomes a real problem. Edit 2: I took another look at Chi-squared and it's not as bad as I remembered... as long as I have a fixed number of buckets and a fixed significance threshold. (Or a canned CDF, and I don't think I do.)","I have a function that (more or less) is supposed to select a small number $m$ of random numbers from the range $[1,n]$ (for some $n \gg m$) and I need to test that it work. Is there an easy to implement test that will give good confidence that it is working correctly? (For reference the full on chi squared test is not simple enough.) Edit: $m$ in $[5,500]$, $n$ is small enough I can use normal int/float types for most math. I can get as many sets as I'm willing to wait for ($>100$s per second) The reason I'm looking for something simple is that the code I'm checking isn't that complicated. If the test code is more complex than the code under test, then the potential for the test being wrong becomes a real problem. Edit 2: I took another look at Chi-squared and it's not as bad as I remembered... as long as I have a fixed number of buckets and a fixed significance threshold. (Or a canned CDF, and I don't think I do.)",,['statistics']
69,What situation calls for dividing the standard deviation by $\sqrt n$?,What situation calls for dividing the standard deviation by ?,\sqrt n,"While doing my homework and checking my answers with the book's answers I noticed that sometimes the standard deviation is divided by $\sqrt n$ where $n$ is the sample size. I'm a little confused. For my current problem I am trying to find the estimated standard error of the estimator. I had found in a previous part of the problem that $\hat \sigma=.33853$ and the sample consists of $16$ measurements. Now the standard error is $.084633$ which is indeed $\frac{\hat \sigma}{\sqrt{16}}$. When I found the standard deviation I didn't divide by $4$, so whats different this time?","While doing my homework and checking my answers with the book's answers I noticed that sometimes the standard deviation is divided by $\sqrt n$ where $n$ is the sample size. I'm a little confused. For my current problem I am trying to find the estimated standard error of the estimator. I had found in a previous part of the problem that $\hat \sigma=.33853$ and the sample consists of $16$ measurements. Now the standard error is $.084633$ which is indeed $\frac{\hat \sigma}{\sqrt{16}}$. When I found the standard deviation I didn't divide by $4$, so whats different this time?",,"['statistics', 'statistical-inference']"
70,Mean vs. Median: When to Use?,Mean vs. Median: When to Use?,,"I know the difference between the mean and the median . The mean of a set of numbers is the sum of all the numbers divided by the cardinality. The median of a set of numbers is the middle number, when the set is organized in ascending or descending order (and, when the set has an even cardinality, the mean of the middle two numbers). It seems to me that they're often used interchangeably, both to give a sense of what's going on in same data. Do they mean (pun intended ) different things? When should one be used over the other?","I know the difference between the mean and the median . The mean of a set of numbers is the sum of all the numbers divided by the cardinality. The median of a set of numbers is the middle number, when the set is organized in ascending or descending order (and, when the set has an even cardinality, the mean of the middle two numbers). It seems to me that they're often used interchangeably, both to give a sense of what's going on in same data. Do they mean (pun intended ) different things? When should one be used over the other?",,"['statistics', 'soft-question', 'terminology', 'means', 'median']"
71,Statistics Primer for the Unwary Mathematician,Statistics Primer for the Unwary Mathematician,,"I have a new position in a biology department (after being housed in a maths department) working on cognitive and population modeling. People in my lab are asking for help with applying statistical tests to their data set, but when people say ""chi-squared"", I say, ""wikipedia"". I'd like to pick up a statistics book that is aimed at teaching someone with a fairly solid background in math and probability theory how to apply standard statistical tests to real data sets, and that goes into some of the practical issues with doing so. Ideally I'd like a book that focuses on frequentist statistics -- my own research deals with Bayesian modeling, and (ironicly) I have a far easier time understanding and working with the more complex Bayesian data analyses that people bring to me than simpler frequentist analyses e.g. ANOVA hybrids","I have a new position in a biology department (after being housed in a maths department) working on cognitive and population modeling. People in my lab are asking for help with applying statistical tests to their data set, but when people say ""chi-squared"", I say, ""wikipedia"". I'd like to pick up a statistics book that is aimed at teaching someone with a fairly solid background in math and probability theory how to apply standard statistical tests to real data sets, and that goes into some of the practical issues with doing so. Ideally I'd like a book that focuses on frequentist statistics -- my own research deals with Bayesian modeling, and (ironicly) I have a far easier time understanding and working with the more complex Bayesian data analyses that people bring to me than simpler frequentist analyses e.g. ANOVA hybrids",,"['statistics', 'reference-request', 'applications', 'book-recommendation']"
72,Proving $E[X^4]=3σ^4$,Proving,E[X^4]=3σ^4,"Given a random variable $X\sim\mathcal N(0,\sigma^2)$, how can we prove that $E[X^4]=3\sigma^4$? I am having trouble even starting with the proof.","Given a random variable $X\sim\mathcal N(0,\sigma^2)$, how can we prove that $E[X^4]=3\sigma^4$? I am having trouble even starting with the proof.",,"['statistics', 'normal-distribution', 'expectation']"
73,What is degree of freedom in statistics?,What is degree of freedom in statistics?,,"In statistics, degree of  freedom is widely used in regression analysis, ANOVA and so on. But, what is degree of  freedom ? Wikipedia said that The number of degrees of freedom is the number of values in the final  calculation of a statistic that are free to vary. Mathematically, degrees of freedom is the number of dimension of the  domain of a random vector, or essentially the number of 'free'    components: how many components need to be known before the vector is   fully determined. However, it is still hard for me to understand the concept intuitively . Question: Could anyone provide a intuitive explain to the concept and anything can help me understand? Thanks! Update: I'm NOT asking how to calculate degree of freedom . Let me give an example: For Chi-squared distribution, different degree of freedom produces different probability density function. Could you explain it intuitively ?","In statistics, degree of  freedom is widely used in regression analysis, ANOVA and so on. But, what is degree of  freedom ? Wikipedia said that The number of degrees of freedom is the number of values in the final  calculation of a statistic that are free to vary. Mathematically, degrees of freedom is the number of dimension of the  domain of a random vector, or essentially the number of 'free'    components: how many components need to be known before the vector is   fully determined. However, it is still hard for me to understand the concept intuitively . Question: Could anyone provide a intuitive explain to the concept and anything can help me understand? Thanks! Update: I'm NOT asking how to calculate degree of freedom . Let me give an example: For Chi-squared distribution, different degree of freedom produces different probability density function. Could you explain it intuitively ?",,['statistics']
74,Unbiased Estimator for a Uniform Variable Support,Unbiased Estimator for a Uniform Variable Support,,"Let $ x_i $ be iid observations in a sample from a uniform distribution over $ \left[ 0, \theta \right] $.  Now I need to estimate $ \theta $ based on $N$ observations and I want the estimator to be unbiased. I thought about simple estimator $ \hat{\theta} = \max \left( x_i \right) $. Based on simulation it is not biased, yet I couldn't show it analytically. Could anyone, please, show it is unbiased? BTW, I could easily find another, easy to prove, unbiased estimator, $ \hat{\theta} = 2 \mathrm{mean} \left( {x}_{i} \right) $","Let $ x_i $ be iid observations in a sample from a uniform distribution over $ \left[ 0, \theta \right] $.  Now I need to estimate $ \theta $ based on $N$ observations and I want the estimator to be unbiased. I thought about simple estimator $ \hat{\theta} = \max \left( x_i \right) $. Based on simulation it is not biased, yet I couldn't show it analytically. Could anyone, please, show it is unbiased? BTW, I could easily find another, easy to prove, unbiased estimator, $ \hat{\theta} = 2 \mathrm{mean} \left( {x}_{i} \right) $",,['statistics']
75,How to calculate standard deviation with streaming inputs?,How to calculate standard deviation with streaming inputs?,,Is there a formula that is capable of operating on streaming inputs and approximating standard  deviation of the set of numbers?,Is there a formula that is capable of operating on streaming inputs and approximating standard  deviation of the set of numbers?,,['statistics']
76,Proof of the independence of the sample mean and sample variance,Proof of the independence of the sample mean and sample variance,,"I've been trying to establish that the sample mean and the sample variance are independent. One motivation is to try and write the sample variance, $S^{2}$ as a function of $\left\{ X_{2}-\bar{X},X_{3}-\bar{X},\cdots,X_{n}-\bar{X}\right\} =A$ only. Then we proceed by showing that $A$ and $\bar{X}$ are independent ( which I'm unable to show ), which then implies the independence of $S^{2}$ and $\bar{X}.$ I would appreciate it if the good people of M.SE would help guide me in the right direction. Thanks. Edit: The random samples $X_1,\cdots,X_n$ are from an $N(\mu, \sigma)$ distribution.","I've been trying to establish that the sample mean and the sample variance are independent. One motivation is to try and write the sample variance, $S^{2}$ as a function of $\left\{ X_{2}-\bar{X},X_{3}-\bar{X},\cdots,X_{n}-\bar{X}\right\} =A$ only. Then we proceed by showing that $A$ and $\bar{X}$ are independent ( which I'm unable to show ), which then implies the independence of $S^{2}$ and $\bar{X}.$ I would appreciate it if the good people of M.SE would help guide me in the right direction. Thanks. Edit: The random samples $X_1,\cdots,X_n$ are from an $N(\mu, \sigma)$ distribution.",,['statistics']
77,Intuitive Way To Understand Principal Component Analysis,Intuitive Way To Understand Principal Component Analysis,,I know that this is meant to explain variance but the description on Wikipedia stinks and it is not clear how you can explain variance using this technique Can anyone explain it in a simple way?,I know that this is meant to explain variance but the description on Wikipedia stinks and it is not clear how you can explain variance using this technique Can anyone explain it in a simple way?,,"['statistics', 'terminology', 'intuition', 'visualization', 'descriptive-statistics']"
78,What is the equation used to calculate a linear trendline?,What is the equation used to calculate a linear trendline?,,"What are the equations to calculate a linear trendline over a set of points? EDIT: ""In excel it is done automatically but how to manually calculate a linear trendline over a set of points"" was originally the question. At first I asked this question because I was simply doing it with excel all the time and couldn't figure out how it computed the result. People tend to focus on the excel part instead of the actual question so I just removed this mention.","What are the equations to calculate a linear trendline over a set of points? EDIT: ""In excel it is done automatically but how to manually calculate a linear trendline over a set of points"" was originally the question. At first I asked this question because I was simply doing it with excel all the time and couldn't figure out how it computed the result. People tend to focus on the excel part instead of the actual question so I just removed this mention.",,['statistics']
79,Why is the geometric mean less sensitive to outliers than the arithmetic mean?,Why is the geometric mean less sensitive to outliers than the arithmetic mean?,,"It’s well known that the geometric mean of a set of positive numbers is less sensitive to outliers than the arithmetic mean. It’s easy to see this by example, but is there a deeper theoretical reason for this? How would I go about “proving” that this is true? Would it make sense to compare the variances of the GM and AM of a sequence of random variables?","It’s well known that the geometric mean of a set of positive numbers is less sensitive to outliers than the arithmetic mean. It’s easy to see this by example, but is there a deeper theoretical reason for this? How would I go about “proving” that this is true? Would it make sense to compare the variances of the GM and AM of a sequence of random variables?",,"['statistics', 'means', 'descriptive-statistics']"
80,Is Standard Deviation the same as Entropy?,Is Standard Deviation the same as Entropy?,,"We know that standard deviation (SD) represents the level of dispersion of a distribution. Thus a distribution with only one value (e.g., 1,1,1,1) has SD equals to zero. Similarly, such a distribution requires little information to be defined. On the other hand, a distribution with high SD requires many bits of information to be defined, therefore we can say its entropy level is high. So my question: is SD the same as entropy? If not, which relationship exist between these two measurements?","We know that standard deviation (SD) represents the level of dispersion of a distribution. Thus a distribution with only one value (e.g., 1,1,1,1) has SD equals to zero. Similarly, such a distribution requires little information to be defined. On the other hand, a distribution with high SD requires many bits of information to be defined, therefore we can say its entropy level is high. So my question: is SD the same as entropy? If not, which relationship exist between these two measurements?",,"['statistics', 'information-theory', 'standard-deviation', 'entropy']"
81,What is the expectation of $ X^2$ where $ X$ is distributed normally?,What is the expectation of  where  is distributed normally?, X^2  X,"I know that if $X$ were distributed as a standard normal, then $X^2$ would be distributed as chi-squared , and hence have expectation $1$, but I'm not sure about for a general normal. Thanks","I know that if $X$ were distributed as a standard normal, then $X^2$ would be distributed as chi-squared , and hence have expectation $1$, but I'm not sure about for a general normal. Thanks",,"['statistics', 'normal-distribution']"
82,what does `ensemble average` mean?,what does `ensemble average` mean?,,"I'm studying this paper and somewhere in the conclusion part is written: ""Since this rotation of the coherency matrix is carried out based on the ensemble average of polarimetric scattering characteristics in a selected imaging window, we obtain the rotation angle as a result of second-order statistics."" Also I've seen the term ensemble average in several other papers of this context. Now I want to understand the exact mathematical or statistical definition of ensemble averaging not only in this context but the exact meaning and use of ensemble averaging in statistics and mathematics. I googled the term ensemble average and here in wikipedia we have the definition as ""In statistical mechanics, the ensemble average is defined as the mean of a quantity that is a function of the microstate of a system (the ensemble of possible states), according to the distribution of the system on its microstates in this ensemble."" But I didn't understand this definition because I don't even know what does the microstate of a system or possible states of system mean in mathematics. Could you please give me a simple definition with some examples for ensemble averaging ? Compare time averaging and ensemble averaging ? And also introduce me some good resources to study more especially resources that can be helpful in image processing too?","I'm studying this paper and somewhere in the conclusion part is written: ""Since this rotation of the coherency matrix is carried out based on the ensemble average of polarimetric scattering characteristics in a selected imaging window, we obtain the rotation angle as a result of second-order statistics."" Also I've seen the term ensemble average in several other papers of this context. Now I want to understand the exact mathematical or statistical definition of ensemble averaging not only in this context but the exact meaning and use of ensemble averaging in statistics and mathematics. I googled the term ensemble average and here in wikipedia we have the definition as ""In statistical mechanics, the ensemble average is defined as the mean of a quantity that is a function of the microstate of a system (the ensemble of possible states), according to the distribution of the system on its microstates in this ensemble."" But I didn't understand this definition because I don't even know what does the microstate of a system or possible states of system mean in mathematics. Could you please give me a simple definition with some examples for ensemble averaging ? Compare time averaging and ensemble averaging ? And also introduce me some good resources to study more especially resources that can be helpful in image processing too?",,"['statistics', 'stochastic-processes', 'average', 'order-statistics']"
83,How do I combine standard deviations of two groups?,How do I combine standard deviations of two groups?,,"I have 2 groups of people. I'm working with the data about their age. I know the means, the standard deviations and the number of people. I don't know the data of each person in the groups. Group 1 : Mean = 35 years old; SD = 14; n = 137 people Group 2 : Mean = 31 years old; SD = 11; n = 112 people I want to combine those 2 groups to obtain a new mean and SD. It's easy for the mean, but is it possible for the SD? I do not know the distribution of those samples, and I can't assume those are normal distributions. Is there a formula for distributions that aren't necessarily normal?","I have 2 groups of people. I'm working with the data about their age. I know the means, the standard deviations and the number of people. I don't know the data of each person in the groups. Group 1 : Mean = 35 years old; SD = 14; n = 137 people Group 2 : Mean = 31 years old; SD = 11; n = 112 people I want to combine those 2 groups to obtain a new mean and SD. It's easy for the mean, but is it possible for the SD? I do not know the distribution of those samples, and I can't assume those are normal distributions. Is there a formula for distributions that aren't necessarily normal?",,"['statistics', 'standard-deviation']"
84,Distribution of Squared Euclidean Norm of Gaussian Vector,Distribution of Squared Euclidean Norm of Gaussian Vector,,"If $\mathbf{X} \sim \mathcal{N}_N(\mathbf{m}, \mathbf{C})$ is an $N$-dimensional gaussian vector, where $\mathbf{m} \in \mathbb{R}^{N}$ and $\mathbf{C} \in \mathbb{R}^{N \times N}$, what is the distribution of $$ Y = \| \mathbf{X} \|^2 $$ where $\| \cdot \|$ denotes the $L_2$-norm (Euclidean norm) ? It may be useful to know that the mean can be easily calculated via $$ \mathbb{E}[ \| \mathbf{X} \|^2 ] = \mathbb{E}\left[\sum_{i=1}^N X_i^2 \right] = \sum_{i=1}^N \mathbb{E}[X_i^2] = \sum_{i=1}^N (\sigma^2_i + m_i^2) = \sum_{i=1}^N\sigma_i^2 + \sum_{i=1}^N m_i^2 = \mathrm{tr}(\mathbf{C}) + \| \mathbf{m} \|^2 $$ where $\mathrm{tr}(\cdot)$ denotes the trace of a matrix. EDIT: Related question: link .","If $\mathbf{X} \sim \mathcal{N}_N(\mathbf{m}, \mathbf{C})$ is an $N$-dimensional gaussian vector, where $\mathbf{m} \in \mathbb{R}^{N}$ and $\mathbf{C} \in \mathbb{R}^{N \times N}$, what is the distribution of $$ Y = \| \mathbf{X} \|^2 $$ where $\| \cdot \|$ denotes the $L_2$-norm (Euclidean norm) ? It may be useful to know that the mean can be easily calculated via $$ \mathbb{E}[ \| \mathbf{X} \|^2 ] = \mathbb{E}\left[\sum_{i=1}^N X_i^2 \right] = \sum_{i=1}^N \mathbb{E}[X_i^2] = \sum_{i=1}^N (\sigma^2_i + m_i^2) = \sum_{i=1}^N\sigma_i^2 + \sum_{i=1}^N m_i^2 = \mathrm{tr}(\mathbf{C}) + \| \mathbf{m} \|^2 $$ where $\mathrm{tr}(\cdot)$ denotes the trace of a matrix. EDIT: Related question: link .",,"['statistics', 'probability-distributions', 'normal-distribution']"
85,Statistics: Why doesn't the probability of an accurate medical test equal the probability of you having disease?,Statistics: Why doesn't the probability of an accurate medical test equal the probability of you having disease?,,"Suppose there is a test for Disease A that is correct 90% of the time. You had this test done, and it came out positive. I understand that the chance that this test is right is 90%, but I thought this would mean the chance that you have a disease should be 90% too. However, according to Bayes' rule, your chance of disease depends on the percentage of the population that has this disease. It sounds absurd: If the test is correct then you have it, and if it's not then you don't, 90% of the time- so there should be 90% chance that the results are right for you... But on other hand, say 100% of population has it. Then regardless of the chance the test says you have it, let it be 90% or 30%, your chance is still 100%... now all of a sudden it doesn't sound absurd. Please avoid using weird symbols as I'm not statistics expert. It just deludes things for me and buries the insight.","Suppose there is a test for Disease A that is correct 90% of the time. You had this test done, and it came out positive. I understand that the chance that this test is right is 90%, but I thought this would mean the chance that you have a disease should be 90% too. However, according to Bayes' rule, your chance of disease depends on the percentage of the population that has this disease. It sounds absurd: If the test is correct then you have it, and if it's not then you don't, 90% of the time- so there should be 90% chance that the results are right for you... But on other hand, say 100% of population has it. Then regardless of the chance the test says you have it, let it be 90% or 30%, your chance is still 100%... now all of a sudden it doesn't sound absurd. Please avoid using weird symbols as I'm not statistics expert. It just deludes things for me and buries the insight.",,['statistics']
86,What is the purpose of subtracting the mean from data when standardizing?,What is the purpose of subtracting the mean from data when standardizing?,,What is the purpose of subtracting the mean from data when standardizing?  and What is the purpose of dividing by the standard deviation?,What is the purpose of subtracting the mean from data when standardizing?  and What is the purpose of dividing by the standard deviation?,,"['statistics', 'standard-deviation', 'mean-square-error']"
87,Distribution of $-\log X$ if $X$ is uniform.,Distribution of  if  is uniform.,-\log X X,"For $X$ and $Y$ random variables; $X$ follows the uniform distribution. (1): if $Y=-\log X$ (2): then it can be shown that $-\log X$ is distributed as $\exp(1)$ {i.e. exponential with mean 1}. Why is this so? Intuitively statement (2) make sense to me. But i'd like a mathematical proof. -Probably wrong working: (1) seems to imply $\exp(-Y) = X$ (which is like saying $X$ is exponentially distributed, which is a contradiction, since its actually uniform!); or is it incorrect for me to do this since $X$ and $Y$ are random variables? Ultimately how do I prove (2)? Thanks","For $X$ and $Y$ random variables; $X$ follows the uniform distribution. (1): if $Y=-\log X$ (2): then it can be shown that $-\log X$ is distributed as $\exp(1)$ {i.e. exponential with mean 1}. Why is this so? Intuitively statement (2) make sense to me. But i'd like a mathematical proof. -Probably wrong working: (1) seems to imply $\exp(-Y) = X$ (which is like saying $X$ is exponentially distributed, which is a contradiction, since its actually uniform!); or is it incorrect for me to do this since $X$ and $Y$ are random variables? Ultimately how do I prove (2)? Thanks",,['statistics']
88,Strange distribution of movie ratings,Strange distribution of movie ratings,,"I like math but I also like movies. I have been collecting movies all my life. My collection is rather huge: almost 25.000 movies. Being also a developer I was able to create my own online catalogue and pull various statistics from the database. There is one thing that puzzles me. Movies have ratings and I did not invent mine: I have copied them from IMDb. As you probably already know, IMDb ratings go from 1 to 10, with 1 being the lowest. I have created a histogram representing ratings distribution and it looks like this: I expected to see something like normal distribution, but my histogram has a funny dip around rating 7.0. Is this a known phenomenon in statistics? Has anyone seen something like this in other data?","I like math but I also like movies. I have been collecting movies all my life. My collection is rather huge: almost 25.000 movies. Being also a developer I was able to create my own online catalogue and pull various statistics from the database. There is one thing that puzzles me. Movies have ratings and I did not invent mine: I have copied them from IMDb. As you probably already know, IMDb ratings go from 1 to 10, with 1 being the lowest. I have created a histogram representing ratings distribution and it looks like this: I expected to see something like normal distribution, but my histogram has a funny dip around rating 7.0. Is this a known phenomenon in statistics? Has anyone seen something like this in other data?",,"['statistics', 'statistical-inference', 'descriptive-statistics']"
89,The law of the unconscious statistician,The law of the unconscious statistician,,"In Casella and Berger's Statistical Inference (2nd edition) it says at the start of section 2.2 (page 55) when defining expectations that If $ \mathrm{E} \,|g(X)| = \infty $ we say that $ \mathrm{E} \,g(X) $ does not exists. (Ross 1988 refers to this as the ""law of the unconscious statistician."" We do not find this amusing.) Why would one call this the ""law of the unconscious statistician""? Perhaps it is that I'm not a native speaker of English, but I have really no idea what being ""unconscious"" has to do with defining existence of expectations. can this be (or not be) considered amusing?","In Casella and Berger's Statistical Inference (2nd edition) it says at the start of section 2.2 (page 55) when defining expectations that If $ \mathrm{E} \,|g(X)| = \infty $ we say that $ \mathrm{E} \,g(X) $ does not exists. (Ross 1988 refers to this as the ""law of the unconscious statistician."" We do not find this amusing.) Why would one call this the ""law of the unconscious statistician""? Perhaps it is that I'm not a native speaker of English, but I have really no idea what being ""unconscious"" has to do with defining existence of expectations. can this be (or not be) considered amusing?",,['statistics']
90,How to accurately calculate the error function $\operatorname{erf}(x)$ with a computer?,How to accurately calculate the error function  with a computer?,\operatorname{erf}(x),"I am looking for an accurate algorithm to calculate the error function $$\operatorname{erf}(x)=\frac{2}{\sqrt{\pi}}\int_0^x e^{-t^2}\ dt$$ I have tried using the following formula, ( Handbook of Mathematical Functions , formula 7.1.26 ), but the results are not accurate enough for the application.","I am looking for an accurate algorithm to calculate the error function $$\operatorname{erf}(x)=\frac{2}{\sqrt{\pi}}\int_0^x e^{-t^2}\ dt$$ I have tried using the following formula, ( Handbook of Mathematical Functions , formula 7.1.26 ), but the results are not accurate enough for the application.",,"['statistics', 'algorithms', 'numerical-methods', 'special-functions']"
91,Estimating the entropy,Estimating the entropy,,"Given a discrete random variable $X$, I would like to estimate the entropy of $Y=f(X)$ by sampling. I can sample uniformly from $X$. The samples are just random vectors of length $n$ where the entries are $0$ or $1$.  For each sample vector $x_i$, I can then compute the function $f(x_i)$ which itself is a vector.  A naive method is to run this process for as long as time allows and then to take the collection of $f(x_i)$ vectors and compute its entropy by making a histogram of how frequently each vector has occurred. This however doesn't seem a good estimate. In particular, the sample space for $Y$ is exponential in $n$ and so I am very likely never to have seen any samples with low probability.  This will mean I may grossly underestimate the entropy I think. The size of the vectors $n$ will typically be at most $100$ and is known. Is there an unbiased estimator for the entropy? Or alternatively, Is there an estimator with lower variance?","Given a discrete random variable $X$, I would like to estimate the entropy of $Y=f(X)$ by sampling. I can sample uniformly from $X$. The samples are just random vectors of length $n$ where the entries are $0$ or $1$.  For each sample vector $x_i$, I can then compute the function $f(x_i)$ which itself is a vector.  A naive method is to run this process for as long as time allows and then to take the collection of $f(x_i)$ vectors and compute its entropy by making a histogram of how frequently each vector has occurred. This however doesn't seem a good estimate. In particular, the sample space for $Y$ is exponential in $n$ and so I am very likely never to have seen any samples with low probability.  This will mean I may grossly underestimate the entropy I think. The size of the vectors $n$ will typically be at most $100$ and is known. Is there an unbiased estimator for the entropy? Or alternatively, Is there an estimator with lower variance?",,['statistics']
92,"Mean, Expected Value, or Expectation of a Constant?","Mean, Expected Value, or Expectation of a Constant?",,"According to Wikipedia, ""the expected value of a constant is equal to the constant itself; i.e., if $c$ is a constant, then $\mathbb E[c] = c$."" I am currently having a hard time picturing what this means. If I have a random variable $X$ that represents the number of times a coin lands on heads, then I can see how $\mathbb  E(X)$ is $0.5$. But $\mathbb  E(0.5)$ seems ""meaningless."" Can someone please explain the significance of the expected value of a constant?","According to Wikipedia, ""the expected value of a constant is equal to the constant itself; i.e., if $c$ is a constant, then $\mathbb E[c] = c$."" I am currently having a hard time picturing what this means. If I have a random variable $X$ that represents the number of times a coin lands on heads, then I can see how $\mathbb  E(X)$ is $0.5$. But $\mathbb  E(0.5)$ seems ""meaningless."" Can someone please explain the significance of the expected value of a constant?",,"['statistics', 'intuition']"
93,Correlation Coefficient and Determination Coefficient,Correlation Coefficient and Determination Coefficient,,"I'm new to linear regression and am trying to teach myself. In my textbook there's a problem that asks ""why is $R^{2}$ in the regression of $Y$ on $X$ equal to the square of the sample correlation between X and Y?"" I've been throwing my head against this for a while and I keep getting stuck because in the correlation coefficient there is a $X$ and $\bar{X}$ term, whilst in the $R^{2}$ term there is no such thing. Can anyone provide a derivation as to why $R^{2}$ is the correlation coefficient squared? Thanks!","I'm new to linear regression and am trying to teach myself. In my textbook there's a problem that asks ""why is in the regression of on equal to the square of the sample correlation between X and Y?"" I've been throwing my head against this for a while and I keep getting stuck because in the correlation coefficient there is a and term, whilst in the term there is no such thing. Can anyone provide a derivation as to why is the correlation coefficient squared? Thanks!",R^{2} Y X X \bar{X} R^{2} R^{2},"['statistics', 'regression', 'correlation']"
94,Why does the normalized z-score introduce a square root? (And some more confusion),Why does the normalized z-score introduce a square root? (And some more confusion),,"I am having a little trouble getting answers from the articles I have been reading. Referring to the ""Standardizing in mathematical statistics"" section here . Why does the substitution of $x$ with $\bar{x}$ result in a square root multiplier. Where does this come from? What does it do? When calculating the z-score using means , why must the formula be $z=\frac{\bar{x}-\mu}{SE}$ instead of $z=\frac{\bar{x}-\mu}{\sigma}$? (where $\sigma$ is standard deviation and $SE$ is $\sigma/\sqrt{n}$.) Why must the units be in standard error rather than in standard deviation as used with non-mean values in the second section of the article? Must this always be the case when using $\bar{x}$ instead of $x$? Are $SE$ and $\sigma$ always based on the entire population or on the sample itself? What does the narrator mean when he says "" we divide that by the standard deviation of the sampling distribution "" here @1:38. Does he mean the standard deviation of the sample or the standard deviation of the entire population?","I am having a little trouble getting answers from the articles I have been reading. Referring to the ""Standardizing in mathematical statistics"" section here . Why does the substitution of $x$ with $\bar{x}$ result in a square root multiplier. Where does this come from? What does it do? When calculating the z-score using means , why must the formula be $z=\frac{\bar{x}-\mu}{SE}$ instead of $z=\frac{\bar{x}-\mu}{\sigma}$? (where $\sigma$ is standard deviation and $SE$ is $\sigma/\sqrt{n}$.) Why must the units be in standard error rather than in standard deviation as used with non-mean values in the second section of the article? Must this always be the case when using $\bar{x}$ instead of $x$? Are $SE$ and $\sigma$ always based on the entire population or on the sample itself? What does the narrator mean when he says "" we divide that by the standard deviation of the sampling distribution "" here @1:38. Does he mean the standard deviation of the sample or the standard deviation of the entire population?",,[]
95,Why is the Kullback-Leibler divergence not symmetric?,Why is the Kullback-Leibler divergence not symmetric?,,"As known the Kullback-Leibler Divergence: $$\operatorname{KL}=\sum_{i=1}^n \ln(\frac{P(i)}{Q(i)})P(i)$$ is not symmetric. I would like to know how this can be seen from the formula. I am aware that I could just try it out with exchaning Q and P for some special case, but I would like to know the mathematical reason behind it. Also, is actually here ""i"" the random variable? Thanks a lot Miau","As known the Kullback-Leibler Divergence: $$\operatorname{KL}=\sum_{i=1}^n \ln(\frac{P(i)}{Q(i)})P(i)$$ is not symmetric. I would like to know how this can be seen from the formula. I am aware that I could just try it out with exchaning Q and P for some special case, but I would like to know the mathematical reason behind it. Also, is actually here ""i"" the random variable? Thanks a lot Miau",,['statistics']
96,Recursive formula for variance,Recursive formula for variance,,"I'd like to know how I can recursively (iteratively) compute variance, so that I may calculate the standard deviation of a very large dataset in javascript. The input is a sorted array of positive integers.","I'd like to know how I can recursively (iteratively) compute variance, so that I may calculate the standard deviation of a very large dataset in javascript. The input is a sorted array of positive integers.",,"['statistics', 'standard-deviation']"
97,Why John Tukey set 1.5 IQR to detect outliers instead of 1 or 2?,Why John Tukey set 1.5 IQR to detect outliers instead of 1 or 2?,,"To define outliers, why we cannot use: Lower Limit: Q1-1xIQR Upper Limit: Q3+1xIQR OR Lower Limit: Q1-2xIQR Upper Limit: Q3+2xIQR","To define outliers, why we cannot use: Lower Limit: Q1-1xIQR Upper Limit: Q3+1xIQR OR Lower Limit: Q1-2xIQR Upper Limit: Q3+2xIQR",,['statistics']
98,What is lag in a time series?,What is lag in a time series?,,"I am curious about what a lagging time series is. On investopedia, I saw an article that said that: ""Autocorrelation is degree of similarity between time series and a lagged version of itself over successive intervals."" Someone please explain to me what ""lagged"" means, and why autocorrelation matters in relation to time series analysis. Does autocorrelation mean that the time series will perform like the past? Thanks! Edit: Thanks for everyone's answers, especially the 2 thumbs-up answer earlier. That was very helpful. Now I am wondering why autocorrelation even matters. Sure a function may correlate with a shifted version of itself, but who says that that function will perform like that? Is it just through correlation? Why does this matter in context of autoregressive models, and how did we develop this autocorrelation, then ARM/ARIMA kinda thing to model time series in the first place. Who developed time series?","I am curious about what a lagging time series is. On investopedia, I saw an article that said that: ""Autocorrelation is degree of similarity between time series and a lagged version of itself over successive intervals."" Someone please explain to me what ""lagged"" means, and why autocorrelation matters in relation to time series analysis. Does autocorrelation mean that the time series will perform like the past? Thanks! Edit: Thanks for everyone's answers, especially the 2 thumbs-up answer earlier. That was very helpful. Now I am wondering why autocorrelation even matters. Sure a function may correlate with a shifted version of itself, but who says that that function will perform like that? Is it just through correlation? Why does this matter in context of autoregressive models, and how did we develop this autocorrelation, then ARM/ARIMA kinda thing to model time series in the first place. Who developed time series?",,"['statistics', 'time-series']"
99,unbiased estimate of the covariance,unbiased estimate of the covariance,,"How can I prove that $$ \frac 1 {n-1} \sum_{i=1}^n (X_i - \bar X)(Y_i-\bar Y) $$ is an unbiased estimate of the covariance $\operatorname{Cov}(X, Y)$ where $\bar X = \dfrac 1 n \sum_{i=1}^n X_i$ and $\bar Y = \dfrac 1 n \sum_{i=1}^n Y_i$ and $(X_1, Y_1), \ldots ,(X_n, Y_n)$ an independent sample from random vector  $(X, Y)$?","How can I prove that $$ \frac 1 {n-1} \sum_{i=1}^n (X_i - \bar X)(Y_i-\bar Y) $$ is an unbiased estimate of the covariance $\operatorname{Cov}(X, Y)$ where $\bar X = \dfrac 1 n \sum_{i=1}^n X_i$ and $\bar Y = \dfrac 1 n \sum_{i=1}^n Y_i$ and $(X_1, Y_1), \ldots ,(X_n, Y_n)$ an independent sample from random vector  $(X, Y)$?",,"['statistics', 'covariance']"
