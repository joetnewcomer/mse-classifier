,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"How to solve the following differential equation? If not, state why?","How to solve the following differential equation? If not, state why?",,"$$x'(t)=(x-1)^2$$ My professor said that we can't solve the equation and find a function that satisfies the differential equation. I don't really get what he means. What I've done so far is $$ \frac{dx}{dt}= (x-1)^2 $$ $$ \int dx = \int(x-1)^2dt$$ $$ x= \frac{(-2t-1)+((2t+1)^2-4t(t+C))^\frac{1}{2}}{-2t}$$ OR $$x= \frac{(-2t-1)-((2t+1)^2-4t(t+C))^\frac{1}{2}}{-2t}$$ Is it correct? If not, do you think I can solve it or find a function that satisfies the DE?","My professor said that we can't solve the equation and find a function that satisfies the differential equation. I don't really get what he means. What I've done so far is OR Is it correct? If not, do you think I can solve it or find a function that satisfies the DE?",x'(t)=(x-1)^2  \frac{dx}{dt}= (x-1)^2   \int dx = \int(x-1)^2dt  x= \frac{(-2t-1)+((2t+1)^2-4t(t+C))^\frac{1}{2}}{-2t} x= \frac{(-2t-1)-((2t+1)^2-4t(t+C))^\frac{1}{2}}{-2t},['ordinary-differential-equations']
1,Steady states of $u_t= u_{xx}+\pi^2u$,Steady states of,u_t= u_{xx}+\pi^2u,"I just put the following one-dimensional reaction-diffusion equation in Mathematica: $$u_t= u_{xx}+au$$ with $\Omega=(0,1)$ with Dirichlet boundary conditions. When $a<9$ , no matter the initial condition I choose, the solution decays to $0$ : (time interval=[0,20]) But for $a>10$ the solution grows to infinity: However , when I choose exactly $a=\pi^2$ , every smooth initial condition I tried seems to give rise to a constant solution, here's for example $u_0(x)=-x^2+x$ : It gives the same result for all functions I tried like $u_0(x)=-3x^2+3x$ or $u_0(x)=\sin(\pi x)$ . I tried to find analytically the steady states of $u_t= u_{xx}+\pi^2u$ and I found all the functions of the form $u_0(x)=B\sin(\pi x)$ . But why a function like $u_0(x)=-x^2+x$ seems to be also a steady state in the simulation? Could it be that $u_0(x)=-x^2+x$ gives rise to a solution that converges immediately to a $B\sin(\pi x)$ function?","I just put the following one-dimensional reaction-diffusion equation in Mathematica: with with Dirichlet boundary conditions. When , no matter the initial condition I choose, the solution decays to : (time interval=[0,20]) But for the solution grows to infinity: However , when I choose exactly , every smooth initial condition I tried seems to give rise to a constant solution, here's for example : It gives the same result for all functions I tried like or . I tried to find analytically the steady states of and I found all the functions of the form . But why a function like seems to be also a steady state in the simulation? Could it be that gives rise to a solution that converges immediately to a function?","u_t= u_{xx}+au \Omega=(0,1) a<9 0 a>10 a=\pi^2 u_0(x)=-x^2+x u_0(x)=-3x^2+3x u_0(x)=\sin(\pi x) u_t= u_{xx}+\pi^2u u_0(x)=B\sin(\pi x) u_0(x)=-x^2+x u_0(x)=-x^2+x B\sin(\pi x)","['ordinary-differential-equations', 'partial-differential-equations', 'heat-equation', 'steady-state']"
2,"Finding a good Lyapunov function for $ \{ x'= y , y'= -4 x + 5x^3 - x^5 \}$",Finding a good Lyapunov function for," \{ x'= y , y'= -4 x + 5x^3 - x^5 \}","For the system: $$\begin{cases}x'= y \\y'= -4 x + 5x^3 - x^5 \end{cases} $$ I am trying to determine the stability of $(x,y)=(0,0)$ by means of a Lyapunov function. I am trying to find a good one, the regular $V(x,y)=ax^2 + by^2$ does not help me as I get odd-powered terms and products of $x$ and $y$ that do not cancel. Specifically: $$ \dot{V}(x,y) = 2axy + b xy(-1+5x^2-x^4)$$ Does someone have a better suggestion, what is the general approach in finding such a function for a given problem? I want to somehow use the fact that these odd powers of $x$ and $y$ appear in this system of equations, I haven't figured out how to do this in an effective manner.","For the system: I am trying to determine the stability of by means of a Lyapunov function. I am trying to find a good one, the regular does not help me as I get odd-powered terms and products of and that do not cancel. Specifically: Does someone have a better suggestion, what is the general approach in finding such a function for a given problem? I want to somehow use the fact that these odd powers of and appear in this system of equations, I haven't figured out how to do this in an effective manner.","\begin{cases}x'= y \\y'= -4 x + 5x^3 - x^5 \end{cases}  (x,y)=(0,0) V(x,y)=ax^2 + by^2 x y  \dot{V}(x,y) = 2axy + b xy(-1+5x^2-x^4) x y",['ordinary-differential-equations']
3,Solution to equation $-u''=\cos(x)$,Solution to equation,-u''=\cos(x),"The question tells us to solve $-u''=\cos(x)$ $u(0)=0, u'(0)=1$ I have not solved these types of problems in a long time so my first attempt is that $u''=d^2u/dx^2$ . I'm not sure if that's what's actually implied here but using that I simply integrated twice, $u''=-\cos(x)$ $u'=-\sin(x)+c$ $u=\cos(x)+cx+d$ Using initial conditions: $u(0)=1+d=0, d=-1$ $u'(0)=c=1$ So my answer is, $u(x)=\cos(x)+x-1$ Is this actually the correct way to solve this?","The question tells us to solve I have not solved these types of problems in a long time so my first attempt is that . I'm not sure if that's what's actually implied here but using that I simply integrated twice, Using initial conditions: So my answer is, Is this actually the correct way to solve this?","-u''=\cos(x) u(0)=0, u'(0)=1 u''=d^2u/dx^2 u''=-\cos(x) u'=-\sin(x)+c u=\cos(x)+cx+d u(0)=1+d=0, d=-1 u'(0)=c=1 u(x)=\cos(x)+x-1","['ordinary-differential-equations', 'derivatives']"
4,Fourier Transform of Airy Equation,Fourier Transform of Airy Equation,,"I am trying to find $Y(k)$ of the equation $y''(x)-xy(x)=0$ and hence show that $$y(x)=\sqrt{\frac{2}{\pi}}\int_0^{\infty}\cos\left(\frac{k^3}{3}+kx\right) \ dk,$$ given $Y(0)=1$ . Here, we use the following definition of the Fourier transform: $$F(k)=\mathcal{F}(f(x))=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{-ikx}f(x) \ dx.$$ It is easy to show that $$\mathcal{F}(xy(x))=i\frac{dY(k)}{dk},$$ where $Y(k)=\mathcal{F}(y(x))$ . My working is as follows: \begin{align} \mathcal{F}(y''(x))-\mathcal{F}(xy(x))&=0 \\ -k^2\mathcal{F}(y(x))-i\frac{dY(k)}{dk}&=0 \\ i\frac{dY(k)}{dk}+k^2Y(k)&=0 \\ \implies Y(k)&=Ae^{ik^2} \\ \implies Y(k)&=e^{ik^2} \\  y(x)&=\mathcal{F}^{-1}(e^{ik^2}) \\ y(x)&=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{i(k^2+kx)} \ dk \\ y(x)&=\sqrt{\frac{2}{\pi}}\int_0^{\infty}\cos(k^2+kx) \ dk \ \ \text{(sine is odd)} \end{align} I don't know where/if I've made an error in the argument of $\cos$ .","I am trying to find of the equation and hence show that given . Here, we use the following definition of the Fourier transform: It is easy to show that where . My working is as follows: I don't know where/if I've made an error in the argument of .","Y(k) y''(x)-xy(x)=0 y(x)=\sqrt{\frac{2}{\pi}}\int_0^{\infty}\cos\left(\frac{k^3}{3}+kx\right) \ dk, Y(0)=1 F(k)=\mathcal{F}(f(x))=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{-ikx}f(x) \ dx. \mathcal{F}(xy(x))=i\frac{dY(k)}{dk}, Y(k)=\mathcal{F}(y(x)) \begin{align}
\mathcal{F}(y''(x))-\mathcal{F}(xy(x))&=0 \\
-k^2\mathcal{F}(y(x))-i\frac{dY(k)}{dk}&=0 \\
i\frac{dY(k)}{dk}+k^2Y(k)&=0 \\
\implies Y(k)&=Ae^{ik^2} \\
\implies Y(k)&=e^{ik^2} \\ 
y(x)&=\mathcal{F}^{-1}(e^{ik^2}) \\
y(x)&=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{i(k^2+kx)} \ dk \\
y(x)&=\sqrt{\frac{2}{\pi}}\int_0^{\infty}\cos(k^2+kx) \ dk \ \ \text{(sine is odd)}
\end{align} \cos","['calculus', 'ordinary-differential-equations', 'proof-verification', 'fourier-transform']"
5,Can we cancel the equality mark here?,Can we cancel the equality mark here?,,"Problem Let $f(x)$ satisfy that $f(1)=1$ and $f'(x)=\dfrac{1}{x^2+f^2(x)}$ . Prove that $\lim\limits_{x \to +\infty}f(x)$ exists and is less than $1+\dfrac{\pi}{4}.$ Proof Since $f'(x)=\dfrac{1}{x^2+f'(x)}>0$ , $f(x)$ is strictly increasing. Thus, $f(x)>f(1)=1$ holds for all $x>1$ , and $\lim\limits_{x \to +\infty}f(x)$ equals either the positive infinity or some finite value. Notice that, $\forall x>1:$ \begin{align*}  f(x)-f(1)&=\int_1^x f'(t){\rm d}t=\int_1^x \frac{1}{t^2+f^2(t)}{\rm d}t<\int_1^x\frac{1}{t^2+1}{\rm d}t=\arctan x-\frac{\pi}{4}. \end{align*} Therefore $$f(x)<\arctan x-\frac{\pi}{4}+1<\frac{\pi}{2}-\frac{\pi}{4}+1=1+\frac{\pi}{4},$$ which implies that $f(x)$ is bounded upward. Thus, $\lim\limits_{x \to +\infty}f(x)$ exists. Take the limits as $x \to +\infty$ , we have $\lim\limits_{x \to +\infty}f(x)\leq 1+\dfrac{\pi}{4}.$ Can we cancel the equality mark here? In another word, can we obtain $\lim\limits_{x \to +\infty}f(x)<1+\dfrac{\pi}{4}$ ?","Problem Let satisfy that and . Prove that exists and is less than Proof Since , is strictly increasing. Thus, holds for all , and equals either the positive infinity or some finite value. Notice that, Therefore which implies that is bounded upward. Thus, exists. Take the limits as , we have Can we cancel the equality mark here? In another word, can we obtain ?","f(x) f(1)=1 f'(x)=\dfrac{1}{x^2+f^2(x)} \lim\limits_{x \to +\infty}f(x) 1+\dfrac{\pi}{4}. f'(x)=\dfrac{1}{x^2+f'(x)}>0 f(x) f(x)>f(1)=1 x>1 \lim\limits_{x \to +\infty}f(x) \forall x>1: \begin{align*}
 f(x)-f(1)&=\int_1^x f'(t){\rm d}t=\int_1^x \frac{1}{t^2+f^2(t)}{\rm d}t<\int_1^x\frac{1}{t^2+1}{\rm d}t=\arctan x-\frac{\pi}{4}.
\end{align*} f(x)<\arctan x-\frac{\pi}{4}+1<\frac{\pi}{2}-\frac{\pi}{4}+1=1+\frac{\pi}{4}, f(x) \lim\limits_{x \to +\infty}f(x) x \to +\infty \lim\limits_{x \to +\infty}f(x)\leq 1+\dfrac{\pi}{4}. \lim\limits_{x \to +\infty}f(x)<1+\dfrac{\pi}{4}","['calculus', 'ordinary-differential-equations', 'limits']"
6,"What does $(dy)(x, \Delta x)$ mean?",What does  mean?,"(dy)(x, \Delta x)","I am reading ordinary differential equations from a book which says Hence, $dy = f'(x) \Delta x$ , we call $dy$ the differential of $y$ . As the differential $dy$ is a function of two independent variables $x$ and $\Delta x$ , we indicate this dependency by $(dy)(x, \Delta x)$ . Excuse me but what the hell is this? It goes even further by saying The differential of $y$ , written as $dy$ (or $df$ ) is defined by $$(dy)(x, \Delta x) = f'(x) \Delta x$$ And now I am completely confused. Do we define $dy$ twice? Or what is $(dy)(x, \Delta x)$ ? Is it a sort of relation as it says ""we write a dependency in this way"", so it is a relation in terms of set theory? What is it exactly? Why do $dy$ and $(dy)(x, \Delta x)$ have the same definitions? Can we say that $dy = (dy)(x, \Delta x)$ then? If so, what is the point of this mess? Thank you if you read up to this point and I will be more thankful if you help me to understand what I am missing.","I am reading ordinary differential equations from a book which says Hence, , we call the differential of . As the differential is a function of two independent variables and , we indicate this dependency by . Excuse me but what the hell is this? It goes even further by saying The differential of , written as (or ) is defined by And now I am completely confused. Do we define twice? Or what is ? Is it a sort of relation as it says ""we write a dependency in this way"", so it is a relation in terms of set theory? What is it exactly? Why do and have the same definitions? Can we say that then? If so, what is the point of this mess? Thank you if you read up to this point and I will be more thankful if you help me to understand what I am missing.","dy = f'(x) \Delta x dy y dy x \Delta x (dy)(x, \Delta x) y dy df (dy)(x, \Delta x) = f'(x) \Delta x dy (dy)(x, \Delta x) dy (dy)(x, \Delta x) dy = (dy)(x, \Delta x)","['ordinary-differential-equations', 'notation', 'terminology']"
7,How to find Sturm-Liouville problem eigenvalue and function?,How to find Sturm-Liouville problem eigenvalue and function?,,"So I have the following Sturm-Liouville problem: $$  y'' + \lambda y = 0  $$ Such that $ \lambda > 0 $ and the initial conditions are as follows: $$ y (0) + y'(0) = 0 $$ $$ y(1) + y'(1) = 0 $$ So my attempt at this goes something like this: I know that the $\lambda$ is positive so the solution must be: $$ y(t) = A\cos(\sqrt(\lambda)t) + B\sin(\sqrt(\lambda)t)$$ and: $$ y'(t) = -A\sqrt{\lambda}\sin(\sqrt{\lambda}t) + B\sqrt{\lambda}\cos(\sqrt{\lambda})t)$$ Such that $A$ and $B$ are constants. So I can evaluate the solution at the first initial condition: \begin{align} &=A\cos(\sqrt{\lambda}0) + B\sin(\sqrt{\lambda}0) + -A\sqrt{\lambda})\sin(\sqrt{\lambda}0) + B\sqrt{\lambda}\cos(\sqrt{\lambda}0)\\ &=A  + B\sqrt{\lambda} \end{align} Thus I know: $$ A = -B\sqrt{\lambda}$$ Evaluating at the second initial condition: \begin{align} &=A\cos(\sqrt{\lambda}1) + B\sin(\sqrt{\lambda}1)  -A\sqrt{\lambda}\sin(\sqrt{\lambda}1) + B\sqrt{\lambda}\cos(\sqrt{\lambda}1)\\ &=A\cos(\sqrt{\lambda})  + B\sin(\sqrt{\lambda}) -A\sqrt{\lambda}\sin(\sqrt{\lambda}) + B\sqrt{\lambda}\cos(\sqrt{\lambda}) \end{align} I'm not sure where to go from here, I factored the second initial condition by cos and sin but that led me to a trivial solution for A and B like this: $$(A + B\sqrt{\lambda})\cos(\sqrt{\lambda}) + (B - A\sqrt{\lambda})\sin(\sqrt{\lambda}) = 0$$ Plugging in $$ A = -B\sqrt{\lambda}$$ . $$(-B\sqrt{\lambda} + B\sqrt{\lambda})\cos(\sqrt{\lambda}) + (B + B\sqrt{\lambda}\sqrt{\lambda})\sin(\sqrt{\lambda}) = 0$$ $$ (B + B\lambda)\sin(\sqrt{\lambda}) = 0$$ So assuming $B$ isn't $0$ , then I know $\lambda = (n\pi)^2$ but how do I find the value of B? Any guidance would be greatly appreciated! Thank you.","So I have the following Sturm-Liouville problem: Such that and the initial conditions are as follows: So my attempt at this goes something like this: I know that the is positive so the solution must be: and: Such that and are constants. So I can evaluate the solution at the first initial condition: Thus I know: Evaluating at the second initial condition: I'm not sure where to go from here, I factored the second initial condition by cos and sin but that led me to a trivial solution for A and B like this: Plugging in . So assuming isn't , then I know but how do I find the value of B? Any guidance would be greatly appreciated! Thank you.","
 y'' + \lambda y = 0 
  \lambda > 0   y (0) + y'(0) = 0   y(1) + y'(1) = 0  \lambda  y(t) = A\cos(\sqrt(\lambda)t) + B\sin(\sqrt(\lambda)t)  y'(t) = -A\sqrt{\lambda}\sin(\sqrt{\lambda}t) + B\sqrt{\lambda}\cos(\sqrt{\lambda})t) A B \begin{align}
&=A\cos(\sqrt{\lambda}0) + B\sin(\sqrt{\lambda}0) + -A\sqrt{\lambda})\sin(\sqrt{\lambda}0) + B\sqrt{\lambda}\cos(\sqrt{\lambda}0)\\
&=A  + B\sqrt{\lambda}
\end{align}  A = -B\sqrt{\lambda} \begin{align}
&=A\cos(\sqrt{\lambda}1) + B\sin(\sqrt{\lambda}1)  -A\sqrt{\lambda}\sin(\sqrt{\lambda}1) + B\sqrt{\lambda}\cos(\sqrt{\lambda}1)\\
&=A\cos(\sqrt{\lambda})  + B\sin(\sqrt{\lambda}) -A\sqrt{\lambda}\sin(\sqrt{\lambda}) + B\sqrt{\lambda}\cos(\sqrt{\lambda})
\end{align} (A + B\sqrt{\lambda})\cos(\sqrt{\lambda}) + (B - A\sqrt{\lambda})\sin(\sqrt{\lambda}) = 0  A = -B\sqrt{\lambda} (-B\sqrt{\lambda} + B\sqrt{\lambda})\cos(\sqrt{\lambda}) + (B + B\sqrt{\lambda}\sqrt{\lambda})\sin(\sqrt{\lambda}) = 0  (B + B\lambda)\sin(\sqrt{\lambda}) = 0 B 0 \lambda = (n\pi)^2","['ordinary-differential-equations', 'partial-differential-equations', 'eigenfunctions', 'sturm-liouville']"
8,Why is the method of separation of variables so widely used in physics?,Why is the method of separation of variables so widely used in physics?,,Why is the method of separation of variables is the only method used in physics for solving partial differential equations despite that they are not the most general solutions? Do these solutions form some kind of complete set? Is there a proof?,Why is the method of separation of variables is the only method used in physics for solving partial differential equations despite that they are not the most general solutions? Do these solutions form some kind of complete set? Is there a proof?,,"['ordinary-differential-equations', 'partial-differential-equations', 'partial-derivative', 'mathematical-physics']"
9,Show that $\alpha \int_0^{\pi}X^2dx = \int_0^{\pi}(X')^2dx$ holds for $\alpha > 0$,Show that  holds for,\alpha \int_0^{\pi}X^2dx = \int_0^{\pi}(X')^2dx \alpha > 0,"Consider the equation in the form $X + \alpha X = 0$ , with Dirichlet boundary conditions at $x = 0$ and $x = π$ . a) Multiply the equation by X and integrate from 0 to π, then integrate the first term, $XX$ , by parts to obtain the identity $$\alpha \int_0^{\pi}X^2dx=-[X'X]\biggr|_0^{\pi} + \int_0^{\pi} (X')^2dx$$ b) Apply the Boundary Conditions to conclude that either $\alpha >0$ or ( $\alpha = 0$ and X is constant) I have obtained part a) and when applying the boundary conditions $X(0)=0=X(\pi )$ I get the following equation $$\alpha \int_0^{\pi}X^2dx = \int_0^{\pi}(X')^2dx$$ It is clear to me why ( $\alpha = 0$ and X constant) satisfy this equation but I'm having trouble explicitly showing that this equation is also true for $\alpha > 0$ . Could someone walk me through this conclusion? Thanks!","Consider the equation in the form , with Dirichlet boundary conditions at and . a) Multiply the equation by X and integrate from 0 to π, then integrate the first term, , by parts to obtain the identity b) Apply the Boundary Conditions to conclude that either or ( and X is constant) I have obtained part a) and when applying the boundary conditions I get the following equation It is clear to me why ( and X constant) satisfy this equation but I'm having trouble explicitly showing that this equation is also true for . Could someone walk me through this conclusion? Thanks!",X + \alpha X = 0 x = 0 x = π XX \alpha \int_0^{\pi}X^2dx=-[X'X]\biggr|_0^{\pi} + \int_0^{\pi} (X')^2dx \alpha >0 \alpha = 0 X(0)=0=X(\pi ) \alpha \int_0^{\pi}X^2dx = \int_0^{\pi}(X')^2dx \alpha = 0 \alpha > 0,"['integration', 'ordinary-differential-equations', 'dirichlet-series']"
10,Solving the ODE $\frac{\mathrm{d}x}{\mathrm{d}t} = x^{-n} -2x$,Solving the ODE,\frac{\mathrm{d}x}{\mathrm{d}t} = x^{-n} -2x,"I'm preparing for my mock exams, and one of the past questions was to Solve $$\frac{\mathrm{d}x}{\mathrm{d}t}  = x^{-n} -2x$$ for $0<x<\infty$ where $n>0$ , subject to $x(1)=1$ . My work so far is as follows:  Multiplying both sides of the ODE by $x^{n}$ gives $$  x^n\frac{\mathrm{d}x}{\mathrm{d}t}  = 1 -2x^{n+1}.  $$ Put $p = x^{n+1}$ . Then $$ \frac{\mathrm{d}p}{\mathrm{d}t} = (n+1)x^{n}\frac{\mathrm{d}x}{\mathrm{d}t}, $$ or $$ x^{n}\frac{\mathrm{d}x}{\mathrm{d}t} = \frac{1}{n+1}\frac{\mathrm{d}p}{\mathrm{d}t}. $$ Therefore, the ODE in question may be rewritten in the form of $$  \frac{1}{n+1}\frac{\mathrm{d}p}{\mathrm{d}t}  = 1 -2p. $$ Assume, for the time being, that $1-2p\neq 0$ . Then separating variables leads to $$ \frac{1}{n+1}\frac{\mathrm{d}p}{1 -2p} = \mathrm{d}t, $$ and integrating both sides gives $$ -\frac{1}{2}\frac{1}{n+1}\ln{|1-2p|} = t+C $$ or $$ -\frac{1}{2}\frac{1}{n+1}\ln{|1-2x^{n+1}|} = t+C. $$ The condition $x(1)=1$ leads to $$ -\frac{1}{2}\frac{1}{n+1}\ln{|1-2\cdot 1|}=-\frac{1}{2}\frac{1}{n+1}\ln{1}= 0  = 1+C, $$ so that $C = -1$ and $$ -\frac{1}{2}\frac{1}{n+1}\ln{|1-2x^{n+1}|} = t - 1, $$ and from here we get $$ |1-2x^{n+1}| = e^{2(n+1)(1-t)}. $$ Now, I wanted to get $x(t)$ explicitly. I obtained that $1-2x^{n+1}\ge 0$ for $x\in\left(0, \frac{1}{2^{n+1}}\right]$ and $1-2x^{n+1}\le 0$ for $x\in\left[\frac{1}{2^{n+1}}, \infty \right)$ . This leads to ""solutions""( ?? ) $$ \begin{align} x(t) = \begin{cases} &\displaystyle\sqrt[n+1]{\displaystyle\frac{1 - e^{2(n+1)(1-t)}}{2}}\quad\text{ for }x\in\left(0, \displaystyle\frac{1}{2^{n+1}}\right]\\\ &\displaystyle \sqrt[n+1]{\displaystyle\frac{1 + e^{2(n+1)(1-t)}}{2}}\quad\text{ for }x\in\left[\displaystyle\frac{1}{2^{n+1}}, \infty \right). \end{cases} \end{align}$$ but this kinda doesn't make sense, because we're giving a formula for $x$ as a function of $t$ and at the same time imposing restrictions as to which interval has $x$ to be to be given by a particular formula... My question(a) is(are): Is there a simpler way to get $x(t)$ ? How can we reconcile the restrictions on $x$ (so that $1-2x^{n+1}$ is $><0$ ) with the formulas for $x$ as a function of $t$ ? And frankly, I'm not sure how to word my question(s); tl;dr would be how to get explicit formulae for $x(t)$ from the ODE - and not implicit relation between $|1-2x^{n+1}|$ and some $f(t)$ )","I'm preparing for my mock exams, and one of the past questions was to Solve for where , subject to . My work so far is as follows:  Multiplying both sides of the ODE by gives Put . Then or Therefore, the ODE in question may be rewritten in the form of Assume, for the time being, that . Then separating variables leads to and integrating both sides gives or The condition leads to so that and and from here we get Now, I wanted to get explicitly. I obtained that for and for . This leads to ""solutions""( ?? ) but this kinda doesn't make sense, because we're giving a formula for as a function of and at the same time imposing restrictions as to which interval has to be to be given by a particular formula... My question(a) is(are): Is there a simpler way to get ? How can we reconcile the restrictions on (so that is ) with the formulas for as a function of ? And frankly, I'm not sure how to word my question(s); tl;dr would be how to get explicit formulae for from the ODE - and not implicit relation between and some )","\frac{\mathrm{d}x}{\mathrm{d}t}  = x^{-n} -2x 0<x<\infty n>0 x(1)=1 x^{n} 
 x^n\frac{\mathrm{d}x}{\mathrm{d}t}  = 1 -2x^{n+1}.
  p = x^{n+1} 
\frac{\mathrm{d}p}{\mathrm{d}t} = (n+1)x^{n}\frac{\mathrm{d}x}{\mathrm{d}t},
 
x^{n}\frac{\mathrm{d}x}{\mathrm{d}t} = \frac{1}{n+1}\frac{\mathrm{d}p}{\mathrm{d}t}.
 
 \frac{1}{n+1}\frac{\mathrm{d}p}{\mathrm{d}t}  = 1 -2p.
 1-2p\neq 0 
\frac{1}{n+1}\frac{\mathrm{d}p}{1 -2p} = \mathrm{d}t,
 
-\frac{1}{2}\frac{1}{n+1}\ln{|1-2p|} = t+C
 
-\frac{1}{2}\frac{1}{n+1}\ln{|1-2x^{n+1}|} = t+C.
 x(1)=1 
-\frac{1}{2}\frac{1}{n+1}\ln{|1-2\cdot 1|}=-\frac{1}{2}\frac{1}{n+1}\ln{1}= 0  = 1+C,
 C = -1 
-\frac{1}{2}\frac{1}{n+1}\ln{|1-2x^{n+1}|} = t - 1,
 
|1-2x^{n+1}| = e^{2(n+1)(1-t)}.
 x(t) 1-2x^{n+1}\ge 0 x\in\left(0, \frac{1}{2^{n+1}}\right] 1-2x^{n+1}\le 0 x\in\left[\frac{1}{2^{n+1}}, \infty \right)  \begin{align}
x(t) = \begin{cases}
&\displaystyle\sqrt[n+1]{\displaystyle\frac{1 - e^{2(n+1)(1-t)}}{2}}\quad\text{ for }x\in\left(0, \displaystyle\frac{1}{2^{n+1}}\right]\\\
&\displaystyle \sqrt[n+1]{\displaystyle\frac{1 + e^{2(n+1)(1-t)}}{2}}\quad\text{ for }x\in\left[\displaystyle\frac{1}{2^{n+1}}, \infty \right).
\end{cases}
\end{align} x t x x(t) x 1-2x^{n+1} ><0 x t x(t) |1-2x^{n+1}| f(t)",[]
11,Show that $e^{tA} = \sum\limits_{k=0}^{n-1} f_k(t)A^k$,Show that,e^{tA} = \sum\limits_{k=0}^{n-1} f_k(t)A^k,"Let $A$ be a $n\times n$ matrix such that the characteristic polynomial of $A$ is $$P(\lambda)=\lambda^n+a_{n-1}\lambda^{n-1}+...+a_1\lambda+a_0$$ Now consider the nth order differential equation $$\frac{d^nx}{dt^n}+a_{n-1}\frac{d^{n-1}x}{dt^{n-1}}+...+a_1\frac{dx}{dt}+a_0x=0$$ and let $f_0(t),\: f_1(t),\: ...,\: f_{n-1}(t)$ be the unique solutions satisfying the inital conditions $$f_j^l(0)= \begin{cases} 1 & j=l \\ 0 & j \neq l\end{cases}$$ for $0 \leq j,\: l\leq n-1$ Show that $e^{tA} = f_0(t)I+f_1(t)A+f_2(t)A^2+...+f_{n-1}(t)A^{n-1}$ I know that $e^{tA}$ is the function $M(t)$ such that $\frac{dM}{dt}=AM$ and $M(0) = I$ . So all I need to do is show that the right hand side of the equation satisfies these two conditions. The second one is easy enough to prove; however, the first one presents some trouble. My plan is to show that $\frac{d^n}{dt^n}f_j(t) = A^nf_j(t)$ for every unique solution $f_j$ , and based on the definitions of $A$ and the solutions to the differential equations, I suspect that Cayley-Hamilton theorem could be used here. However, I haven't been able to engineer it in a way to prove the above statement.","Let be a matrix such that the characteristic polynomial of is Now consider the nth order differential equation and let be the unique solutions satisfying the inital conditions for Show that I know that is the function such that and . So all I need to do is show that the right hand side of the equation satisfies these two conditions. The second one is easy enough to prove; however, the first one presents some trouble. My plan is to show that for every unique solution , and based on the definitions of and the solutions to the differential equations, I suspect that Cayley-Hamilton theorem could be used here. However, I haven't been able to engineer it in a way to prove the above statement.","A n\times n A P(\lambda)=\lambda^n+a_{n-1}\lambda^{n-1}+...+a_1\lambda+a_0 \frac{d^nx}{dt^n}+a_{n-1}\frac{d^{n-1}x}{dt^{n-1}}+...+a_1\frac{dx}{dt}+a_0x=0 f_0(t),\: f_1(t),\: ...,\: f_{n-1}(t) f_j^l(0)= \begin{cases} 1 & j=l \\ 0 & j \neq l\end{cases} 0 \leq j,\: l\leq n-1 e^{tA} = f_0(t)I+f_1(t)A+f_2(t)A^2+...+f_{n-1}(t)A^{n-1} e^{tA} M(t) \frac{dM}{dt}=AM M(0) = I \frac{d^n}{dt^n}f_j(t) = A^nf_j(t) f_j A","['linear-algebra', 'ordinary-differential-equations', 'cayley-hamilton']"
12,Solve a system of ordinary differential equations,Solve a system of ordinary differential equations,,"I have the following set of ordinary differential equations: $$ \begin{cases} x_1'(s) &= -e^{-s} (1-x_1(s)) - x_2(s) + x_1(s) x_3(s)\\ x_2'(s) &= -x_2(s) + x_1(s)^2\\ x_3'(s) &= -x_3(s) + x_1(s), \end{cases} $$ with boundary condition $x_2(0)=x_3(0)=0$ . I am looking for a method to find an exact solution for this type of ODE. In particular I am interested in finding a solution for $x_1$ which satisfies $x_1(\infty) = 0$ , but it already excelent if the solution of $x_1$ is simply given in function of a general boundary condition $x_1(0) = a$ . EDIT I have found this link of methods for solving this type of problems, but my problem does not seem to fit in any of the suggested methods.","I have the following set of ordinary differential equations: with boundary condition . I am looking for a method to find an exact solution for this type of ODE. In particular I am interested in finding a solution for which satisfies , but it already excelent if the solution of is simply given in function of a general boundary condition . EDIT I have found this link of methods for solving this type of problems, but my problem does not seem to fit in any of the suggested methods.","
\begin{cases}
x_1'(s) &= -e^{-s} (1-x_1(s)) - x_2(s) + x_1(s) x_3(s)\\
x_2'(s) &= -x_2(s) + x_1(s)^2\\
x_3'(s) &= -x_3(s) + x_1(s),
\end{cases}
 x_2(0)=x_3(0)=0 x_1 x_1(\infty) = 0 x_1 x_1(0) = a","['real-analysis', 'ordinary-differential-equations']"
13,Find Order And Degree of a Differential Equation,Find Order And Degree of a Differential Equation,,$\left\{ 1 + \left( \frac { d y } { d x } \right) ^ { 2 } \right\} ^ { \frac { 3 } { 2 } } = \frac { d ^ { 2 } y } { d x ^ { 2 } }$ what is the degree and order for above equation well according to my knowledge the order be should $2$ and degree should be $\frac { 3 } { 2 }$ is my answer right or wrong ?,$\left\{ 1 + \left( \frac { d y } { d x } \right) ^ { 2 } \right\} ^ { \frac { 3 } { 2 } } = \frac { d ^ { 2 } y } { d x ^ { 2 } }$ what is the degree and order for above equation well according to my knowledge the order be should $2$ and degree should be $\frac { 3 } { 2 }$ is my answer right or wrong ?,,"['ordinary-differential-equations', 'differential']"
14,Lipschitz and Uniqueness of an IVP,Lipschitz and Uniqueness of an IVP,,"Consider the IVP $$\frac{dx}{dt}=1+x^2, \ \ \ x(0)=1.$$ I am trying to show that $f(x)=1+x^2$ is Lipschitz and hence the above IVP has a unique solution. My attempt: I considered $f(x)=1+x^2$. Now if $f$ is Lipschitz, then $\exists L\in\mathbb{R}, \ \text{such that} \ \forall x,y\in\mathbb{R}$, $$|f(x)-f(y)|\leq L|x-y|.$$ Now, \begin{align} L&\geq \frac{|1+x^2-1-y^2|}{|x-y|} \\ &=\frac{|(x-y)(x+y)|}{|x-y|} \\ &=|x+y| \end{align} I'm a bit unsure of how to proceed. How can I use the initial value conditions to further my answer?","Consider the IVP $$\frac{dx}{dt}=1+x^2, \ \ \ x(0)=1.$$ I am trying to show that $f(x)=1+x^2$ is Lipschitz and hence the above IVP has a unique solution. My attempt: I considered $f(x)=1+x^2$. Now if $f$ is Lipschitz, then $\exists L\in\mathbb{R}, \ \text{such that} \ \forall x,y\in\mathbb{R}$, $$|f(x)-f(y)|\leq L|x-y|.$$ Now, \begin{align} L&\geq \frac{|1+x^2-1-y^2|}{|x-y|} \\ &=\frac{|(x-y)(x+y)|}{|x-y|} \\ &=|x+y| \end{align} I'm a bit unsure of how to proceed. How can I use the initial value conditions to further my answer?",,['ordinary-differential-equations']
15,Interpretation of Differential Algebraic Equation,Interpretation of Differential Algebraic Equation,,"Considering the Differential Algebraic Equation (DAE) of the form $$\dot{x}=f(x,y)$$ $$0=g(x,y)$$ We can use the Implicit function theorem to conclude that as long as the Jacobian $  \frac{\partial g(x,y)}{\partial y}$ is non-singular, $\dot{y}$ can be written as a function of $x$ and $y$, and thus, we can use the local equivalent ODE version of this DAE. (i)- Under what condition we can interpret the above DAE as an ODE on the manifold $S:=\{ g(x,y)=0 \}$? (ii)- What if the DAE is of the form $\dot{x}=f(x)$, $g(x)=0$ (i.e., when we have a complete ODE plus a set of algebraic equations)?","Considering the Differential Algebraic Equation (DAE) of the form $$\dot{x}=f(x,y)$$ $$0=g(x,y)$$ We can use the Implicit function theorem to conclude that as long as the Jacobian $  \frac{\partial g(x,y)}{\partial y}$ is non-singular, $\dot{y}$ can be written as a function of $x$ and $y$, and thus, we can use the local equivalent ODE version of this DAE. (i)- Under what condition we can interpret the above DAE as an ODE on the manifold $S:=\{ g(x,y)=0 \}$? (ii)- What if the DAE is of the form $\dot{x}=f(x)$, $g(x)=0$ (i.e., when we have a complete ODE plus a set of algebraic equations)?",,"['ordinary-differential-equations', 'manifolds', 'dynamical-systems', 'smooth-manifolds', 'control-theory']"
16,Why can’t we apply Method of separation of variables to second order Diff equations?,Why can’t we apply Method of separation of variables to second order Diff equations?,,Suppose we are solving the following ODE $${\mathrm{d}y\over \mathrm{d}x}={y\over x}$$ then we can solve it by seperation of variables method. But if we have to solve $${\mathrm{d}^2y\over \mathrm{d}x^2} = {y\over x}$$ then why cant we seperate $\mathrm{d}^2y\over \mathrm{d}x^2$?,Suppose we are solving the following ODE $${\mathrm{d}y\over \mathrm{d}x}={y\over x}$$ then we can solve it by seperation of variables method. But if we have to solve $${\mathrm{d}^2y\over \mathrm{d}x^2} = {y\over x}$$ then why cant we seperate $\mathrm{d}^2y\over \mathrm{d}x^2$?,,['ordinary-differential-equations']
17,"Find the integrating factor of $(x^2y-2xy^2)\,dx+(x^3-3x^2y)\,dy=0$",Find the integrating factor of,"(x^2y-2xy^2)\,dx+(x^3-3x^2y)\,dy=0","Find the integrating factor of the differential equation: $$(x^2y-2xy^2)\,dx+(x^3-3x^2y)\,dy=0$$ What I tried: This is a homogeneous equation. Therefore, $$I.F=\frac{1}{Mx+Ny}=\frac{1}{(x^2y-2xy^2)x+(x^3-3x^2y)y}=\frac{1}{x^3y-2x^2y^2+x^3y-3x^2y^2}$$ However, the given answer is: $$I.F=\frac{1}{x^2y^2}$$","Find the integrating factor of the differential equation: $$(x^2y-2xy^2)\,dx+(x^3-3x^2y)\,dy=0$$ What I tried: This is a homogeneous equation. Therefore, $$I.F=\frac{1}{Mx+Ny}=\frac{1}{(x^2y-2xy^2)x+(x^3-3x^2y)y}=\frac{1}{x^3y-2x^2y^2+x^3y-3x^2y^2}$$ However, the given answer is: $$I.F=\frac{1}{x^2y^2}$$",,"['ordinary-differential-equations', 'integrating-factor']"
18,solving $y'-x^2y+y^2=2x$,solving,y'-x^2y+y^2=2x,"Find a general solution to the equation  $$y'-x^2y+y^2=2x$$ I managed to guess a solution $y=x^2$ $$(x^2)'-x^2\cdot x^2+(x^2)^2=2x-x^4+x^4=2x$$ To get the general solution I tried plugging $y=z(x)+x^2$ into the equation, which gives $$z'+x^2z+z^2=0$$ The equation is not exact $$dz+(x^2z+z^2)dx=0$$ and I failed in the search for an integrating factor. Moreover, the solution in Wolfram is not so welcoming. Maybe there is some implicit solution...","Find a general solution to the equation  $$y'-x^2y+y^2=2x$$ I managed to guess a solution $y=x^2$ $$(x^2)'-x^2\cdot x^2+(x^2)^2=2x-x^4+x^4=2x$$ To get the general solution I tried plugging $y=z(x)+x^2$ into the equation, which gives $$z'+x^2z+z^2=0$$ The equation is not exact $$dz+(x^2z+z^2)dx=0$$ and I failed in the search for an integrating factor. Moreover, the solution in Wolfram is not so welcoming. Maybe there is some implicit solution...",,['ordinary-differential-equations']
19,Reciprocal solutions of a differential equation,Reciprocal solutions of a differential equation,,"I need to show that if $a$ is a constant and $b(x)$ is a function, then $$y''+\frac{b'(x)}{b(x)}y'-\frac{a^2}{[b(x)]^2}y=0$$ has a pair of linearly independent solutions which are reciprocal and then find them. I would have thought I could just substitute in $y=u(x)+\dfrac{1}{u(x)}$ but I seem to get nowhere. Anyway, we have $y'=u'-\dfrac{u'}{u^2}$ and $y''=u''-\dfrac{u''}{u^2}+\dfrac{2(u')^2}{u^3}$ .  Therefore, we get $$u''-\frac{u''}{u^2}+\frac{2(u')^2}{u^3}+(\frac{b'(x)}{b(x)})(u'-\frac{u'}{u^2})-(\frac{a^2}{[b(x)]^2})(u(x)+\frac{1}{u(x)})=0\,.$$ tidying gives $$-\frac{u''}{u^2}+\frac{2(u')^2}{u^3}+(\frac{b'(x)}{b(x)})(-\frac{u'}{u^2})-(\frac{a^2}{[b(x)]^2})(\frac{1}{u(x)})=0\,.$$ multiply by $-u^2$ to get $$u''-\frac{2(u')^2}{u}+(\frac{b'(x)}{b(x)})(u')+(\frac{a^2}{[b(x)]^2})(u(x))=0\,.$$","I need to show that if is a constant and is a function, then has a pair of linearly independent solutions which are reciprocal and then find them. I would have thought I could just substitute in but I seem to get nowhere. Anyway, we have and .  Therefore, we get tidying gives multiply by to get","a b(x) y''+\frac{b'(x)}{b(x)}y'-\frac{a^2}{[b(x)]^2}y=0 y=u(x)+\dfrac{1}{u(x)} y'=u'-\dfrac{u'}{u^2} y''=u''-\dfrac{u''}{u^2}+\dfrac{2(u')^2}{u^3} u''-\frac{u''}{u^2}+\frac{2(u')^2}{u^3}+(\frac{b'(x)}{b(x)})(u'-\frac{u'}{u^2})-(\frac{a^2}{[b(x)]^2})(u(x)+\frac{1}{u(x)})=0\,. -\frac{u''}{u^2}+\frac{2(u')^2}{u^3}+(\frac{b'(x)}{b(x)})(-\frac{u'}{u^2})-(\frac{a^2}{[b(x)]^2})(\frac{1}{u(x)})=0\,. -u^2 u''-\frac{2(u')^2}{u}+(\frac{b'(x)}{b(x)})(u')+(\frac{a^2}{[b(x)]^2})(u(x))=0\,.","['calculus', 'ordinary-differential-equations']"
20,"A twice differentiable function $f$ satisfies $f′′(x)+f(x)=−xg(x)f′(x)$, $\forall x\ge 0$.","A twice differentiable function  satisfies , .",f f′′(x)+f(x)=−xg(x)f′(x) \forall x\ge 0,"Consider $f\in C^2$ so that $$f''(x)+f(x)=-x\,g(x)f'(x), \ \forall x\ge0 $$ where $g(x)\ge 0$. Then ($\forall x\ge 0$) (A) $f(x)^2+f'(x)^2$ is non-increasing, (B) $f(x)^2<3f(0)^2+(2f'(0))^2$, (C) $|f(x)|\le\alpha$, where $\alpha$ is a fixed real constant. (D) $\lim_{x\to\infty} f(x)\sin\left(\dfrac{1}{x}\right)$ exists. original task description Answer Given: (A),(B),(C),(D) What I have tried : I differentiated option (A) and then with the help of the given equation in the question I was able to show that  it is less than zero . Hence I was able to infer that option (A) is correct . Now coming to option (B).  I assumed the function in option (A) to be $h(x)$. $$ h(x) = f(x)^2 + f'(x)^2 $$ Since $h'(x)<0$ ,therefore $h(x)$ is a decreasing function hence for all $x\ge 0$ $h(x)< h(0)$ which implies $(f(x))^2 + (f'(x))^2<(f(0))^2 + (f'(0))^2$ From the above expression we can get the second option . Is my method correct ?. If not , please show the right method . I am unable to get the the 3rd and 4th option .","Consider $f\in C^2$ so that $$f''(x)+f(x)=-x\,g(x)f'(x), \ \forall x\ge0 $$ where $g(x)\ge 0$. Then ($\forall x\ge 0$) (A) $f(x)^2+f'(x)^2$ is non-increasing, (B) $f(x)^2<3f(0)^2+(2f'(0))^2$, (C) $|f(x)|\le\alpha$, where $\alpha$ is a fixed real constant. (D) $\lim_{x\to\infty} f(x)\sin\left(\dfrac{1}{x}\right)$ exists. original task description Answer Given: (A),(B),(C),(D) What I have tried : I differentiated option (A) and then with the help of the given equation in the question I was able to show that  it is less than zero . Hence I was able to infer that option (A) is correct . Now coming to option (B).  I assumed the function in option (A) to be $h(x)$. $$ h(x) = f(x)^2 + f'(x)^2 $$ Since $h'(x)<0$ ,therefore $h(x)$ is a decreasing function hence for all $x\ge 0$ $h(x)< h(0)$ which implies $(f(x))^2 + (f'(x))^2<(f(0))^2 + (f'(0))^2$ From the above expression we can get the second option . Is my method correct ?. If not , please show the right method . I am unable to get the the 3rd and 4th option .",,"['calculus', 'ordinary-differential-equations', 'derivatives']"
21,Show that the following is asymptotically stable - Liapunov function,Show that the following is asymptotically stable - Liapunov function,,"I am trying to figure out how to show that the zero solution to the following is asymptotically stable. $$x'=xy-x^3$$ $$y'=-y+x^3y$$ I was trying to use a liapunov function:  $$V(x,y)=\frac{1}{2}[x^2+y^2]$$ Which is always positive. This gives us that  $$V'(x,y)=xx'+yy'$$ $$=x(xy-x^3)+y(-y+x^3y)$$ $$=x^2y-x^4-y^2+x^3y^2$$ I know that I need this to be strictly negative for some domain surrounding and including the origin. I am having a hard time showing this. I have tried a couple other liapunov functions as well, but none have been successful either. Thanks for any help! I am also open to other methods if there is something easier. When I linearized the system though, it seemed to be inconclusive. I found the linearization to be  $$ \begin{bmatrix}     x'\\     y'\\ \end{bmatrix}= \begin{bmatrix}     y && -\frac{x^3}{y}\\ -\frac{y}{x} && x^3 \\ \end{bmatrix} \begin{bmatrix}     x\\     y\\ \end{bmatrix} $$ So I can't plug in $x=y=0$ to find the eigenvalues. Unless I am not doing that right...","I am trying to figure out how to show that the zero solution to the following is asymptotically stable. $$x'=xy-x^3$$ $$y'=-y+x^3y$$ I was trying to use a liapunov function:  $$V(x,y)=\frac{1}{2}[x^2+y^2]$$ Which is always positive. This gives us that  $$V'(x,y)=xx'+yy'$$ $$=x(xy-x^3)+y(-y+x^3y)$$ $$=x^2y-x^4-y^2+x^3y^2$$ I know that I need this to be strictly negative for some domain surrounding and including the origin. I am having a hard time showing this. I have tried a couple other liapunov functions as well, but none have been successful either. Thanks for any help! I am also open to other methods if there is something easier. When I linearized the system though, it seemed to be inconclusive. I found the linearization to be  $$ \begin{bmatrix}     x'\\     y'\\ \end{bmatrix}= \begin{bmatrix}     y && -\frac{x^3}{y}\\ -\frac{y}{x} && x^3 \\ \end{bmatrix} \begin{bmatrix}     x\\     y\\ \end{bmatrix} $$ So I can't plug in $x=y=0$ to find the eigenvalues. Unless I am not doing that right...",,"['ordinary-differential-equations', 'stability-in-odes', 'stability-theory']"
22,Finding integration factor for an ODE,Finding integration factor for an ODE,,I'm trying to get the following system of equation to exact differential equation form: $$ \left\{  \begin{array} \dot \dot x =y^2-x \\  \dot y = 2y \end{array} \right.  $$ What I tried: $$\frac{dx}{dy}=\frac{y^2-x}{2y} \quad \Rightarrow \quad 2ydx+(x-y^2)dy=0 $$ Next I tried to find an integration factor: $$\frac{N_x-M_y}{M}=-\frac{1}{2y} $$ $$\Rightarrow \quad \mu(y)=e^{\int-\frac{1}{2y}dy}=e^{-\frac{1}{2}ln|y|}=\frac{1}{\sqrt{|y|}}$$ Is there a way I can get rid of the absolute value? omitting it gives an exact differential equation but only for part of the domain.,I'm trying to get the following system of equation to exact differential equation form: $$ \left\{  \begin{array} \dot \dot x =y^2-x \\  \dot y = 2y \end{array} \right.  $$ What I tried: $$\frac{dx}{dy}=\frac{y^2-x}{2y} \quad \Rightarrow \quad 2ydx+(x-y^2)dy=0 $$ Next I tried to find an integration factor: $$\frac{N_x-M_y}{M}=-\frac{1}{2y} $$ $$\Rightarrow \quad \mu(y)=e^{\int-\frac{1}{2y}dy}=e^{-\frac{1}{2}ln|y|}=\frac{1}{\sqrt{|y|}}$$ Is there a way I can get rid of the absolute value? omitting it gives an exact differential equation but only for part of the domain.,,"['integration', 'ordinary-differential-equations']"
23,I need a function that abides by this definition: $F(x+1)-F(x)=x$,I need a function that abides by this definition:,F(x+1)-F(x)=x,"I am trying to find a solution to the differential equation: $$\frac{\mathrm{d}y}{\mathrm{d}x}=\lfloor{x}\rfloor.$$ And one solution is $$F(x+1)-F(x)=x.$$ I do not know any strategies to determine the definition of the function $F(x)$. I have seen that some Riemann functions have some functional definitions like this, and I was wondering if anyone knows a function that has the definition: $F(x+1)-F(x)=x$.","I am trying to find a solution to the differential equation: $$\frac{\mathrm{d}y}{\mathrm{d}x}=\lfloor{x}\rfloor.$$ And one solution is $$F(x+1)-F(x)=x.$$ I do not know any strategies to determine the definition of the function $F(x)$. I have seen that some Riemann functions have some functional definitions like this, and I was wondering if anyone knows a function that has the definition: $F(x+1)-F(x)=x$.",,['ordinary-differential-equations']
24,Get the solution to the differential equation $\cos y \sin2x dx +(cos^2y - cos^2x)dy = 0$,Get the solution to the differential equation,\cos y \sin2x dx +(cos^2y - cos^2x)dy = 0,"I was helping somebody on some math problems when a wild question appears. It goes like this: Get the solution to the differential equation $$\cos y \sin2x dx +(\cos^2y - \cos^2x)dy = 0$$ My work I was able to search the internet on how to get the solution to the differential equation shown above but I do not understand the way. We could perhaps rearrange the above differential equation into the form $$M(x,y)dx + N(x,y)dy = 0$$ With the form above, we could see if the given differential equation is variable-separable, homogenous or an exact-type. Testing the given differential equation as variable-separable: No. We can't. The differential equation above isn't variable-separable. Testing the given differential equation as homogenous: $F(x,y)$ is a homogenous function of degree $n$ in $x$ and $y$ provided $F(kx,ky)$ = $k^nF(x,y)$. For example, $x^2 + y^2$ and $x^2 \sin \left(\frac{y}{x}\right)$ are homogenous functions of degree $2$ in $x$ and $y$. With that in mind... Looking at the first term $\cos y \sin2x$: $$F(x,y) = \cos y \sin2x$$ $$F(kx,ky) = \cos (ky) \sin(2kx)$$ Getting deeper, we conclude that $$F(kx,ky) \neq k^nF(x,y)$$ Looking at the second term $\cos^2y - \cos^2x$: $$F(x,y) = \cos^2y - \cos^2x$$ $$F(kx,ky) = \cos^2(ky) - \cos^2(kx)$$ Getting deeper, we conclude that $$F(kx,ky) \neq k^nF(x,y)$$ The terms themselves must be homogenous if the differential equations are homogenous. No. We can't. The differential equation above isn't homogenous. Testing the given differential equation if it is an exact-type: A necessary condition for a differential equation to be exact is $$\frac{\partial M}{\partial y} = \frac{\partial N}{\partial x}$$ So getting the expression for $\frac{\partial M}{\partial y}$: $$\frac{\partial M}{\partial y} = \frac{\partial}{\partial y}(\cos y \sin2x) $$ $$\frac{\partial M}{\partial y} = \sin 2x\frac{\partial}{\partial y}(\cos y) $$ $$\frac{\partial M}{\partial y} = \sin 2x(-\sin y)$$ $$\frac{\partial M}{\partial y} = -\sin y \sin 2x$$ So getting the expression for $\frac{\partial N}{\partial x}$: $$\frac{\partial N}{\partial x} = \frac{\partial}{\partial x}(\cos^2y - \cos^2x) $$ $$\frac{\partial N}{\partial x} = 0 - \frac{\partial}{\partial x}(\cos^2x) $$ $$\frac{\partial N}{\partial x} = 0 - \frac{\partial}{\partial x}\left(\frac{1}{2} + \frac{1}{2}\cos 2x\right) $$ $$\frac{\partial N}{\partial x} = \sin 2x $$ We see that $\frac{\partial M}{\partial y} \neq \frac{\partial N}{\partial x}$, so the given differential equation isn't exact either. Let's try converting it into linear differential equation. Let's try rearranging the given differential equation above into a Bernoulli equation, having the form $$\frac{dy}{dx} + P(x)y = Q(x)$$ Then... $$\cos y \sin2x dx +(\cos^2y - \cos^2x)dy = 0$$ $$\frac{\cos y \sin2x dx}{dx} +\frac{(\cos^2y - \cos^2x)dy}{dx} = \frac{0}{dx}$$ $$\cos y \sin2x + (\cos^2y - \cos^2x)\frac{dy}{dx} = 0$$ $$\frac{\cos y \sin2x}{\cos^2y - \cos^2x} + \frac{\cos^2y - \cos^2x}{\cos^2y - \cos^2x}\frac{dy}{dx} = \frac{0}{\cos^2y - \cos^2x}$$ $$\frac{\cos y \sin2x}{\cos^2y - \cos^2x} + \frac{dy}{dx} = 0$$ $$\frac{dy}{dx} + \frac{\cos y \sin2x}{\cos^2y - \cos^2x} = 0$$ Screw that. We couldn't even discern what is the expression of $P(x)y$ because the expression $\frac{\cos y \sin2x}{\cos^2y - \cos^2x}$ is irreducible. We can't separate $x$'s from $y$'s. How do we get the solution  of the differential equation above?","I was helping somebody on some math problems when a wild question appears. It goes like this: Get the solution to the differential equation $$\cos y \sin2x dx +(\cos^2y - \cos^2x)dy = 0$$ My work I was able to search the internet on how to get the solution to the differential equation shown above but I do not understand the way. We could perhaps rearrange the above differential equation into the form $$M(x,y)dx + N(x,y)dy = 0$$ With the form above, we could see if the given differential equation is variable-separable, homogenous or an exact-type. Testing the given differential equation as variable-separable: No. We can't. The differential equation above isn't variable-separable. Testing the given differential equation as homogenous: $F(x,y)$ is a homogenous function of degree $n$ in $x$ and $y$ provided $F(kx,ky)$ = $k^nF(x,y)$. For example, $x^2 + y^2$ and $x^2 \sin \left(\frac{y}{x}\right)$ are homogenous functions of degree $2$ in $x$ and $y$. With that in mind... Looking at the first term $\cos y \sin2x$: $$F(x,y) = \cos y \sin2x$$ $$F(kx,ky) = \cos (ky) \sin(2kx)$$ Getting deeper, we conclude that $$F(kx,ky) \neq k^nF(x,y)$$ Looking at the second term $\cos^2y - \cos^2x$: $$F(x,y) = \cos^2y - \cos^2x$$ $$F(kx,ky) = \cos^2(ky) - \cos^2(kx)$$ Getting deeper, we conclude that $$F(kx,ky) \neq k^nF(x,y)$$ The terms themselves must be homogenous if the differential equations are homogenous. No. We can't. The differential equation above isn't homogenous. Testing the given differential equation if it is an exact-type: A necessary condition for a differential equation to be exact is $$\frac{\partial M}{\partial y} = \frac{\partial N}{\partial x}$$ So getting the expression for $\frac{\partial M}{\partial y}$: $$\frac{\partial M}{\partial y} = \frac{\partial}{\partial y}(\cos y \sin2x) $$ $$\frac{\partial M}{\partial y} = \sin 2x\frac{\partial}{\partial y}(\cos y) $$ $$\frac{\partial M}{\partial y} = \sin 2x(-\sin y)$$ $$\frac{\partial M}{\partial y} = -\sin y \sin 2x$$ So getting the expression for $\frac{\partial N}{\partial x}$: $$\frac{\partial N}{\partial x} = \frac{\partial}{\partial x}(\cos^2y - \cos^2x) $$ $$\frac{\partial N}{\partial x} = 0 - \frac{\partial}{\partial x}(\cos^2x) $$ $$\frac{\partial N}{\partial x} = 0 - \frac{\partial}{\partial x}\left(\frac{1}{2} + \frac{1}{2}\cos 2x\right) $$ $$\frac{\partial N}{\partial x} = \sin 2x $$ We see that $\frac{\partial M}{\partial y} \neq \frac{\partial N}{\partial x}$, so the given differential equation isn't exact either. Let's try converting it into linear differential equation. Let's try rearranging the given differential equation above into a Bernoulli equation, having the form $$\frac{dy}{dx} + P(x)y = Q(x)$$ Then... $$\cos y \sin2x dx +(\cos^2y - \cos^2x)dy = 0$$ $$\frac{\cos y \sin2x dx}{dx} +\frac{(\cos^2y - \cos^2x)dy}{dx} = \frac{0}{dx}$$ $$\cos y \sin2x + (\cos^2y - \cos^2x)\frac{dy}{dx} = 0$$ $$\frac{\cos y \sin2x}{\cos^2y - \cos^2x} + \frac{\cos^2y - \cos^2x}{\cos^2y - \cos^2x}\frac{dy}{dx} = \frac{0}{\cos^2y - \cos^2x}$$ $$\frac{\cos y \sin2x}{\cos^2y - \cos^2x} + \frac{dy}{dx} = 0$$ $$\frac{dy}{dx} + \frac{\cos y \sin2x}{\cos^2y - \cos^2x} = 0$$ Screw that. We couldn't even discern what is the expression of $P(x)y$ because the expression $\frac{\cos y \sin2x}{\cos^2y - \cos^2x}$ is irreducible. We can't separate $x$'s from $y$'s. How do we get the solution  of the differential equation above?",,['ordinary-differential-equations']
25,Differential equation $y''=y^2$ [duplicate],Differential equation  [duplicate],y''=y^2,"This question already has answers here : Solve $y''=y^2$ (5 answers) Closed 3 months ago . I must solve the differential equation $y''=y^2$. Clearly, the function $y=0$ is a solution. So, assume that $y$ is not identically zero. Unless I'm not mistaken, there is a trick to solve equations of this kind. If I multiply for $2y'$, the left-hand member is $2y'y''$, that is the derivative of $(y')^2$, while the right-hand member is $2y^2 y'$. At this point, how can I continue this exercise?","This question already has answers here : Solve $y''=y^2$ (5 answers) Closed 3 months ago . I must solve the differential equation $y''=y^2$. Clearly, the function $y=0$ is a solution. So, assume that $y$ is not identically zero. Unless I'm not mistaken, there is a trick to solve equations of this kind. If I multiply for $2y'$, the left-hand member is $2y'y''$, that is the derivative of $(y')^2$, while the right-hand member is $2y^2 y'$. At this point, how can I continue this exercise?",,['ordinary-differential-equations']
26,What is the degree of $(y')^{-2}+5y'=0 $?,What is the degree of ?,(y')^{-2}+5y'=0 ,"In the video(time stamp present) https://youtu.be/L61hIm_WoC8?t=2944 , the differential equation  $(y')^{-2}+5y'=0$ is said to have no degree referring that it is not polynomial in derivative(y'), ""The teacher says that the equation is not polynomial in derivative so the degree can't be calculated and puts a cross on the right side of the equation."" but I guess multiplying the equation with $(y')^2$, it becomes $1+5(y')^3=0$ which is polynomial in derivative(y'), it has degree 3. So, which one is correct?","In the video(time stamp present) https://youtu.be/L61hIm_WoC8?t=2944 , the differential equation  $(y')^{-2}+5y'=0$ is said to have no degree referring that it is not polynomial in derivative(y'), ""The teacher says that the equation is not polynomial in derivative so the degree can't be calculated and puts a cross on the right side of the equation."" but I guess multiplying the equation with $(y')^2$, it becomes $1+5(y')^3=0$ which is polynomial in derivative(y'), it has degree 3. So, which one is correct?",,"['calculus', 'ordinary-differential-equations']"
27,How to show that all trajectories of this dynamical system end in a disk?,How to show that all trajectories of this dynamical system end in a disk?,,"Consider the system: $$\begin{alignat}{2} 			x_{1}' &=&~ -ax_2 &+ x_{1}\left(1-x_{1}^2-x_{2}^2\right),\\ 			x_{2}' &=&   ax_1 &+ x_{2}\left(1-x_{1}^2-x_{2}^2\right)-b, 	\end{alignat} $$ where $a,b$ are real numbers. The task is to show that there is a disk which eventually contains every orbit of the system and that there is a limit cycle only if $b$ is zero. Up to this point, I was trying to construct a function $V(x_1,x_2)$ with negative derivative along the solution curves on a disk but it’s not getting anywhere. Maybe a proof by assuming the opposite plus a theoretical argument would do the job, but I would welcome any hints here.","Consider the system: $$\begin{alignat}{2} 			x_{1}' &=&~ -ax_2 &+ x_{1}\left(1-x_{1}^2-x_{2}^2\right),\\ 			x_{2}' &=&   ax_1 &+ x_{2}\left(1-x_{1}^2-x_{2}^2\right)-b, 	\end{alignat} $$ where $a,b$ are real numbers. The task is to show that there is a disk which eventually contains every orbit of the system and that there is a limit cycle only if $b$ is zero. Up to this point, I was trying to construct a function $V(x_1,x_2)$ with negative derivative along the solution curves on a disk but it’s not getting anywhere. Maybe a proof by assuming the opposite plus a theoretical argument would do the job, but I would welcome any hints here.",,"['ordinary-differential-equations', 'dynamical-systems']"
28,Confusion about Green's functions for inhomogeneous boundary conditions but no forcing.,Confusion about Green's functions for inhomogeneous boundary conditions but no forcing.,,"I have recently seen Green's functions for the first time but for some reason can't seem to get my head around them. I have been told we can use them to solve differential equations up to an integral, however I am having difficulties seeing why they should work (probably from my inexperience). For example if we are asked to solve $\mathcal{L}(u(x)) = f(x)$ on say $[0,1]$ with $u(0) = u(1) = 1$ where $\mathcal{L}$ is any second order Sturm-Liouville operator (say take $\mathcal{L} = \frac{d^2}{dx^2}$) then with Green's functions we would have $G(x,\xi)$ satisfies $\mathcal{L}(G) = \delta(x-\xi)$ and our solution for $u$ is $u(x) = \int\limits_0^1 G(x,\xi) f(\xi)\ d\xi$. Now consider the case where $f = 0$ everywhere. This means that in our integral we have $f(\xi) = 0$ everywhere so we get $u(x) = 0$ everywhere as $G$ has to be bounded (at least that is what I think it has to based on all the examples I have seen so far) which is wrong. I will be glad if someone could point out to me where I am making my mistake. Thank you in advance.","I have recently seen Green's functions for the first time but for some reason can't seem to get my head around them. I have been told we can use them to solve differential equations up to an integral, however I am having difficulties seeing why they should work (probably from my inexperience). For example if we are asked to solve $\mathcal{L}(u(x)) = f(x)$ on say $[0,1]$ with $u(0) = u(1) = 1$ where $\mathcal{L}$ is any second order Sturm-Liouville operator (say take $\mathcal{L} = \frac{d^2}{dx^2}$) then with Green's functions we would have $G(x,\xi)$ satisfies $\mathcal{L}(G) = \delta(x-\xi)$ and our solution for $u$ is $u(x) = \int\limits_0^1 G(x,\xi) f(\xi)\ d\xi$. Now consider the case where $f = 0$ everywhere. This means that in our integral we have $f(\xi) = 0$ everywhere so we get $u(x) = 0$ everywhere as $G$ has to be bounded (at least that is what I think it has to based on all the examples I have seen so far) which is wrong. I will be glad if someone could point out to me where I am making my mistake. Thank you in advance.",,"['ordinary-differential-equations', 'dirac-delta', 'greens-function', 'sturm-liouville']"
29,"Find all $C^{1}$ functions $f: (0,+\infty) \to (0, +\infty)$ such that $f(x)^{f'(x)}=x$, $f(1)=1$.","Find all  functions  such that , .","C^{1} f: (0,+\infty) \to (0, +\infty) f(x)^{f'(x)}=x f(1)=1","As the question title says, I'm trying to find all $C^1$ functions $f:(0, +\infty) \to (0, +\infty)$ which satisfy $f(x)^{f'(x)} = x$, and $f(1)=1$. I know that $f(x)=x$ is one solution. When I put everything into the exponent, I get $f'(x) \ln{f(x)} = \ln{x}$, which gives me the implicit solution $f(x)(\ln{f(x)}-1) = x(\ln{x}-1)+C$, $C \in \mathbb{R}$. By inserting $(1,1)$ into the implicit solution, I get that the solution must satisfy $f(x)(\ln{f(x)}-1) = x(\ln{x}-1)$. The problem here is that I can't use Picard's theorem and claim uniqueness, because the expression $f'(x) = \frac{\ln{x}}{\ln{f(x)}}$ isn't defined for $(x, f(x))=(1,1)$. Is there a different way to prove uniqueness, or is there another solution to this equation?","As the question title says, I'm trying to find all $C^1$ functions $f:(0, +\infty) \to (0, +\infty)$ which satisfy $f(x)^{f'(x)} = x$, and $f(1)=1$. I know that $f(x)=x$ is one solution. When I put everything into the exponent, I get $f'(x) \ln{f(x)} = \ln{x}$, which gives me the implicit solution $f(x)(\ln{f(x)}-1) = x(\ln{x}-1)+C$, $C \in \mathbb{R}$. By inserting $(1,1)$ into the implicit solution, I get that the solution must satisfy $f(x)(\ln{f(x)}-1) = x(\ln{x}-1)$. The problem here is that I can't use Picard's theorem and claim uniqueness, because the expression $f'(x) = \frac{\ln{x}}{\ln{f(x)}}$ isn't defined for $(x, f(x))=(1,1)$. Is there a different way to prove uniqueness, or is there another solution to this equation?",,['ordinary-differential-equations']
30,How to solve the differential equation $\cos^2(x) \frac{d^2 y}{d x^2} -2 y = -\cos(x)$.,How to solve the differential equation .,\cos^2(x) \frac{d^2 y}{d x^2} -2 y = -\cos(x),"Solve the following differential equation:   $$\cos^2(x) \frac{d^2 y}{d x^2} -2 y = -\cos(x).$$ We were asked not to solve this by the method of variation of parameters, so except that method we have tried to reduce the equation as But after this point, since the RHS of the equation is too complex to do anything, we could not proceed. Addition to that, we have used the method of differential operator method, but it led to a complex integral which we or any online applet couldn't take the integral, so we are basically stuck. So, how can we solve this differential equation ? Note: Any hind also is appreciated. Edit: We would like to solve this ODE by using some methods, and not just guessing the particular solution and moving to the corresponding homogeneous equation, since the very purpose of this question is to learn how to solve such a ODE.","Solve the following differential equation:   $$\cos^2(x) \frac{d^2 y}{d x^2} -2 y = -\cos(x).$$ We were asked not to solve this by the method of variation of parameters, so except that method we have tried to reduce the equation as But after this point, since the RHS of the equation is too complex to do anything, we could not proceed. Addition to that, we have used the method of differential operator method, but it led to a complex integral which we or any online applet couldn't take the integral, so we are basically stuck. So, how can we solve this differential equation ? Note: Any hind also is appreciated. Edit: We would like to solve this ODE by using some methods, and not just guessing the particular solution and moving to the corresponding homogeneous equation, since the very purpose of this question is to learn how to solve such a ODE.",,['ordinary-differential-equations']
31,Differential equation: $y''\cdot y'\cdot y=1$,Differential equation:,y''\cdot y'\cdot y=1,"I've been playing around with differential equations. I can easily solve the differential equation $$y'\cdot y=1$$ for $y:\mathbb R\mapsto\mathbb R$, and I can also solve $$y''\cdot y=1$$ using substitution into the previous example. However, I cannot figure out this differential equation: $$y''\cdot y'\cdot y=1$$ Does anybody have any ideas about how to solve this? So far, the only technique that I know that seems valid for this differential equation is the use of Taylor Series, but that gets too messy for me to get anything useful out of it.","I've been playing around with differential equations. I can easily solve the differential equation $$y'\cdot y=1$$ for $y:\mathbb R\mapsto\mathbb R$, and I can also solve $$y''\cdot y=1$$ using substitution into the previous example. However, I cannot figure out this differential equation: $$y''\cdot y'\cdot y=1$$ Does anybody have any ideas about how to solve this? So far, the only technique that I know that seems valid for this differential equation is the use of Taylor Series, but that gets too messy for me to get anything useful out of it.",,"['ordinary-differential-equations', 'derivatives']"
32,Unstable solutions when numerically integrating a system of ODE's with increasing step size,Unstable solutions when numerically integrating a system of ODE's with increasing step size,,"Given is the following system of linear ordinary differential equations: $$ a'(t)= -a(t)+0.1b(t)+0.5c(t)+0.1d(t) $$ $$ b'(t)=0.1a(t)-b(t)+0.2c(t)+0.4d(t)$$ $$ c'(t)=0.5a(t)+0.2b(t)-c(t)+0.3d(t)$$ $$ d'(t)=0.1a(t)+0.4b(t)+0.3c(t)-d(t)$$ with initial conditions $$ a(0)=1, b(0)=c(0)=d(0)=0 $$ I numerically solved this system with the explicit Euler method at a step size of 0.1 and then increased the step size. Here is the solution at step size 0.1: However, with increasing step size, the solution becomes more and more unstable and eventually ""explodes"". Here is a graph with step size 1.25: And here with step size 1.35: Can somebody explain to me why the increase in step size results in such a behavior?","Given is the following system of linear ordinary differential equations: $$ a'(t)= -a(t)+0.1b(t)+0.5c(t)+0.1d(t) $$ $$ b'(t)=0.1a(t)-b(t)+0.2c(t)+0.4d(t)$$ $$ c'(t)=0.5a(t)+0.2b(t)-c(t)+0.3d(t)$$ $$ d'(t)=0.1a(t)+0.4b(t)+0.3c(t)-d(t)$$ with initial conditions $$ a(0)=1, b(0)=c(0)=d(0)=0 $$ I numerically solved this system with the explicit Euler method at a step size of 0.1 and then increased the step size. Here is the solution at step size 0.1: However, with increasing step size, the solution becomes more and more unstable and eventually ""explodes"". Here is a graph with step size 1.25: And here with step size 1.35: Can somebody explain to me why the increase in step size results in such a behavior?",,"['ordinary-differential-equations', 'numerical-methods', 'systems-of-equations']"
33,Solve the differential equation $\left(\arctan(xy)+\frac{xy-2xy^{2}}{1+x^{2}y^{2}}\right)dx+\left(\frac{x^{2}-2x^{2}y}{1+x^{2}y^{2}}\right)dy=0$,Solve the differential equation,\left(\arctan(xy)+\frac{xy-2xy^{2}}{1+x^{2}y^{2}}\right)dx+\left(\frac{x^{2}-2x^{2}y}{1+x^{2}y^{2}}\right)dy=0,"This is problem 9, exercise 10 from Tannebaum and Pollard's ODE book. I have deduced that the differential equation is exact, but I can't find all the integrable combinations. Any hints that would help me to move forward would be great! Solve the differential equation  $\left(\arctan(xy)+\frac{xy-2xy^{2}}{1+x^{2}y^{2}}\right)dx+\left(\frac{x^{2}-2x^{2}y}{1+x^{2}y^{2}}\right)dy=0$ Solution. We have, $\begin{align} P&=\arctan(xy)+\frac{xy-2xy^{2}}{1+x^{2}y^{2}}\\ \frac{\partial P}{\partial y}&=\frac{x}{1+x^{2}y^{2}}+\left[\frac{(1+x^{2}y^{2})(x-4xy)-(xy-2xy^{2})(2x^{2}y)}{(1+x^{2}y^{2})^{2}}\right]\\ &=\frac{x}{1+x^{2}y^{2}}+\left[\frac{x-4xy+x^{3}y^{2}-4x^{3}y^{3}-2x^{3}y^{2}+4x^{3}y^{3}}{(1+x^{2}y^{2})^{2}}\right]\\ &=\frac{x}{1+x^{2}y^{2}}+\left[\frac{x-4xy-x^{3}y^{2}}{(1+x^{2}y^{2})^{2}}\right]\\ &=\frac{x(1+x^{2}y^{2})+x-4xy-x^{3}y^{2}}{(1+x^{2}y^{2})^{2}}\\ &=\frac{x+x^{3}y^{2}+x-4xy-x^{3}y^{2}}{(1+x^{2}y^{2})^{2}}\\ &=\frac{2x-4xy}{(1+x^{2}y^{2})^{2}}\\ Q&=\frac{x^{2}-2x^{2}y}{1+x^{2}y^{2}}\\ \frac{\partial Q}{\partial x}&=\frac{(1+x^{2}y^{2})(2x-4xy)-(x^{2}-2x^{2}y)(2xy^{2})}{(1+x^{2}y^{2})^{2}}\\ &=\frac{2x-4xy+2x^{3}y^{2}-4x^{3}y^{3}-2x^{3}y^{2}+4x^{3}y^{3}}{(1+x^{2}y^{2})^{2}}\\ &=\frac{2x-4xy}{(1+x^{2}y^{2})^{2}} \end{align}$ Since $\partial{P}/\partial{y}=\partial{Q}/\partial{x}$, this is an exact differential equation. We know that- $\begin{align} d(x\cdot \arctan(xy))&=\arctan(xy)dx+x\frac{1}{1+x^{2}y^{2}}(xdy+ydx)\\ &=\arctan(xy)dx+\frac{x^{2}dy}{1+x^{2}y^{2}}+\frac{xydx}{1+x^{2}y^{2}}  \end{align}$ I am not able to find an integrable combination for the remaining two terms.","This is problem 9, exercise 10 from Tannebaum and Pollard's ODE book. I have deduced that the differential equation is exact, but I can't find all the integrable combinations. Any hints that would help me to move forward would be great! Solve the differential equation  $\left(\arctan(xy)+\frac{xy-2xy^{2}}{1+x^{2}y^{2}}\right)dx+\left(\frac{x^{2}-2x^{2}y}{1+x^{2}y^{2}}\right)dy=0$ Solution. We have, $\begin{align} P&=\arctan(xy)+\frac{xy-2xy^{2}}{1+x^{2}y^{2}}\\ \frac{\partial P}{\partial y}&=\frac{x}{1+x^{2}y^{2}}+\left[\frac{(1+x^{2}y^{2})(x-4xy)-(xy-2xy^{2})(2x^{2}y)}{(1+x^{2}y^{2})^{2}}\right]\\ &=\frac{x}{1+x^{2}y^{2}}+\left[\frac{x-4xy+x^{3}y^{2}-4x^{3}y^{3}-2x^{3}y^{2}+4x^{3}y^{3}}{(1+x^{2}y^{2})^{2}}\right]\\ &=\frac{x}{1+x^{2}y^{2}}+\left[\frac{x-4xy-x^{3}y^{2}}{(1+x^{2}y^{2})^{2}}\right]\\ &=\frac{x(1+x^{2}y^{2})+x-4xy-x^{3}y^{2}}{(1+x^{2}y^{2})^{2}}\\ &=\frac{x+x^{3}y^{2}+x-4xy-x^{3}y^{2}}{(1+x^{2}y^{2})^{2}}\\ &=\frac{2x-4xy}{(1+x^{2}y^{2})^{2}}\\ Q&=\frac{x^{2}-2x^{2}y}{1+x^{2}y^{2}}\\ \frac{\partial Q}{\partial x}&=\frac{(1+x^{2}y^{2})(2x-4xy)-(x^{2}-2x^{2}y)(2xy^{2})}{(1+x^{2}y^{2})^{2}}\\ &=\frac{2x-4xy+2x^{3}y^{2}-4x^{3}y^{3}-2x^{3}y^{2}+4x^{3}y^{3}}{(1+x^{2}y^{2})^{2}}\\ &=\frac{2x-4xy}{(1+x^{2}y^{2})^{2}} \end{align}$ Since $\partial{P}/\partial{y}=\partial{Q}/\partial{x}$, this is an exact differential equation. We know that- $\begin{align} d(x\cdot \arctan(xy))&=\arctan(xy)dx+x\frac{1}{1+x^{2}y^{2}}(xdy+ydx)\\ &=\arctan(xy)dx+\frac{x^{2}dy}{1+x^{2}y^{2}}+\frac{xydx}{1+x^{2}y^{2}}  \end{align}$ I am not able to find an integrable combination for the remaining two terms.",,['ordinary-differential-equations']
34,Existence and uniqueness of the differential equation $\frac{dy}{dx}=\sqrt{xy} $,Existence and uniqueness of the differential equation,\frac{dy}{dx}=\sqrt{xy} ,"I am trying to find the regions for the differential equation $\frac{dy}{dx}=\sqrt{xy} $ for which the solution is unique, with graph passing through the point $(x_0, y_0)$ in these regions. I tried to applied Picard theorem for existence and uniqueness. The function $\frac{\partial f}{\partial y}(x,y) = \frac{\sqrt x}{2 \sqrt y}$ is defined when $xy\geq 0$ and $y \neq 0$ thus we have existence and uniqueness in the regions $R = [0, \infty)\times (0, \infty)$ and $R = (-\infty, 0]\times (-\infty, 0)$ for any $(x_0, y_0)  \in R$. However, answer is $R = (0, \infty)\times (0, \infty)$  and $R = (-\infty, 0)\times (-\infty, 0)$. My question is why not $x=0$ is included in the region of existence and uniquness? Thank you","I am trying to find the regions for the differential equation $\frac{dy}{dx}=\sqrt{xy} $ for which the solution is unique, with graph passing through the point $(x_0, y_0)$ in these regions. I tried to applied Picard theorem for existence and uniqueness. The function $\frac{\partial f}{\partial y}(x,y) = \frac{\sqrt x}{2 \sqrt y}$ is defined when $xy\geq 0$ and $y \neq 0$ thus we have existence and uniqueness in the regions $R = [0, \infty)\times (0, \infty)$ and $R = (-\infty, 0]\times (-\infty, 0)$ for any $(x_0, y_0)  \in R$. However, answer is $R = (0, \infty)\times (0, \infty)$  and $R = (-\infty, 0)\times (-\infty, 0)$. My question is why not $x=0$ is included in the region of existence and uniquness? Thank you",,['ordinary-differential-equations']
35,Is my solution of a differential equation problem right?,Is my solution of a differential equation problem right?,,"I tried to solve the following problem but I feel that I must work a little more specially for the second limit. Please let me know if I need to do more. Problem: Solve the following Cauchy problem. $y'(x)=\frac{(y(x))^{2}}{1-(y(x))^{2}}\;\; and\;\; y(0)=1/2 .$ Compute the following limits $$\lim\limits_{x\to 1/2-}y(x)\;\; and\;\; \lim\limits_{x\to 1/2-}y'(x).$$ Here is my solution. At the first step, we have $$\frac{1-y^{2}}{y^{2}}dy=dx\;\Rightarrow\; 1/y+y=-x-c.$$ Since  $y(0)=1/2\;$ so $\;c=-5/2.$ Now, we put  $x=1/2\;$ and $\;c=-5/2\;$ then we have  $y=1$ by the  $y+1/y=-1/2+5/2.$ Since  $f(x,y)=\frac{1}{y}+y+x-\frac{5}{2}=0$ is continuous at every point except  $y\neq 0.$ We have  $\lim\limits_{x\to 1/2-}y(x)=1.$ Since we have  $y'(x)=\frac{(y(x))^{2}}{1-(y(x))^{2}},$ therefore,  $\lim\limits_{x\to 1/2-}y'(x)=\infty.$ Edit after comments: $$\frac{1}{y}+y+x-\frac{5}{2}=0 \Rightarrow y^{2}+(x-5/2)y+1=0 $$ $$\Rightarrow y=\dfrac{-(x-5/2)\pm\sqrt{(x-5/2)^{2}-4}}{2} $$ $$\Rightarrow \lim\limits_{x\to 1/2-}y(x)=1 $$ Also, $$\Rightarrow \lim\limits_{x\to 1/2-}y'(x)=+\infty $$","I tried to solve the following problem but I feel that I must work a little more specially for the second limit. Please let me know if I need to do more. Problem: Solve the following Cauchy problem. $y'(x)=\frac{(y(x))^{2}}{1-(y(x))^{2}}\;\; and\;\; y(0)=1/2 .$ Compute the following limits $$\lim\limits_{x\to 1/2-}y(x)\;\; and\;\; \lim\limits_{x\to 1/2-}y'(x).$$ Here is my solution. At the first step, we have $$\frac{1-y^{2}}{y^{2}}dy=dx\;\Rightarrow\; 1/y+y=-x-c.$$ Since  $y(0)=1/2\;$ so $\;c=-5/2.$ Now, we put  $x=1/2\;$ and $\;c=-5/2\;$ then we have  $y=1$ by the  $y+1/y=-1/2+5/2.$ Since  $f(x,y)=\frac{1}{y}+y+x-\frac{5}{2}=0$ is continuous at every point except  $y\neq 0.$ We have  $\lim\limits_{x\to 1/2-}y(x)=1.$ Since we have  $y'(x)=\frac{(y(x))^{2}}{1-(y(x))^{2}},$ therefore,  $\lim\limits_{x\to 1/2-}y'(x)=\infty.$ Edit after comments: $$\frac{1}{y}+y+x-\frac{5}{2}=0 \Rightarrow y^{2}+(x-5/2)y+1=0 $$ $$\Rightarrow y=\dfrac{-(x-5/2)\pm\sqrt{(x-5/2)^{2}-4}}{2} $$ $$\Rightarrow \lim\limits_{x\to 1/2-}y(x)=1 $$ Also, $$\Rightarrow \lim\limits_{x\to 1/2-}y'(x)=+\infty $$",,"['calculus', 'ordinary-differential-equations', 'limits']"
36,"How does proof for ""differentiability imply continuity"" work?","How does proof for ""differentiability imply continuity"" work?",,"Here's the well known proof: $$\lim_{x \to c} \left[f(x) - f(c) \right] = \lim_{x \to c} (x - c) \frac{f(x) - f(c)}{x - c}$$ $$ = \left[\lim_{x \to c} (x - c)\right]\left[ \lim_{x \to c} \frac{f(x) - f(c)}{x - c}\right]$$  $$ = 0 \cdot f ^{\prime} (c)$$ $$ = 0$$ a) Therefore, $\lim_{x \to c} f(x) = f(c)$, $f$ is continuous at $c$. The problem I know how the equation works, but I don't get how it proves explicitly that differentiability implies continuity. To me, as long as $\lim_{x \to c} (x - c) = 0$, then why should $f ^{\prime} (x)$ even matter? a) would be remain true anyways, right? I must be looking at it wrong. To me, it just looks like an equation saying that continuity implies continuity. As long as $\lim_{x \to c} (x - c) = 0$, which is the continuity equation, then of course the equation would imply that f(c) is continuous. But how does differentiability explicitly imply continuity? It just looks like modifications were made to the continuity equation to make the differentiability equation appear, then it just went back to the continuity equation. But how exactly does the differentiability equation that popped up imply continuity? I mean, it just looks like the continuity equation proving itself, but I don't see how the differentiability equation is ""proving"" anything. It started from the continuity equation and it went back to the continuity equation? I'm so frustrated with this. I keep attending the same classes and the teachers seem to see something I can't.","Here's the well known proof: $$\lim_{x \to c} \left[f(x) - f(c) \right] = \lim_{x \to c} (x - c) \frac{f(x) - f(c)}{x - c}$$ $$ = \left[\lim_{x \to c} (x - c)\right]\left[ \lim_{x \to c} \frac{f(x) - f(c)}{x - c}\right]$$  $$ = 0 \cdot f ^{\prime} (c)$$ $$ = 0$$ a) Therefore, $\lim_{x \to c} f(x) = f(c)$, $f$ is continuous at $c$. The problem I know how the equation works, but I don't get how it proves explicitly that differentiability implies continuity. To me, as long as $\lim_{x \to c} (x - c) = 0$, then why should $f ^{\prime} (x)$ even matter? a) would be remain true anyways, right? I must be looking at it wrong. To me, it just looks like an equation saying that continuity implies continuity. As long as $\lim_{x \to c} (x - c) = 0$, which is the continuity equation, then of course the equation would imply that f(c) is continuous. But how does differentiability explicitly imply continuity? It just looks like modifications were made to the continuity equation to make the differentiability equation appear, then it just went back to the continuity equation. But how exactly does the differentiability equation that popped up imply continuity? I mean, it just looks like the continuity equation proving itself, but I don't see how the differentiability equation is ""proving"" anything. It started from the continuity equation and it went back to the continuity equation? I'm so frustrated with this. I keep attending the same classes and the teachers seem to see something I can't.",,"['calculus', 'ordinary-differential-equations']"
37,Fundamental theorem of calculus for multivariable function,Fundamental theorem of calculus for multivariable function,,"In a proof, they have written: $$\frac{f(x,y+t)-f(x,y)}{t}=\int_0^1 f_y(x,y+st)ds$$ But doesnt the fundamental theorem of calculus say $$\int_0^1 f_y(x,y+st)ds=f(x,y+t)-f(x,y)?$$ I'm sure the answer is obvious, but for the life of me I can't see where this extra $1/t$ coming from. Thanks!","In a proof, they have written: $$\frac{f(x,y+t)-f(x,y)}{t}=\int_0^1 f_y(x,y+st)ds$$ But doesnt the fundamental theorem of calculus say $$\int_0^1 f_y(x,y+st)ds=f(x,y+t)-f(x,y)?$$ I'm sure the answer is obvious, but for the life of me I can't see where this extra $1/t$ coming from. Thanks!",,"['calculus', 'real-analysis', 'integration', 'ordinary-differential-equations']"
38,Trouble applying the fundamental theorem of Calculus,Trouble applying the fundamental theorem of Calculus,,"Let $\psi(t)=\phi(t_0)*\exp(\int_{t_0}^t b(r)dr) + \int_{t_0}^t a(s) \exp(\int_{s}^t b(r)dr)ds$ Now I know already ( because this is from a differential equation example in a book ) that $$\psi'(t)= a(t)+ b(t)*\psi(t)$$ but I can't seem to differentiate correctly. If I differentiate $\psi(t)$  I should get  $$\psi'(t)=\phi(t_0)*\exp(\int_{t_0}^t b(r)dr)* b(t)+ \frac{d}{dt}\int_{t_0}^t a(s) \exp(\int_{s}^t b(r)dr)ds$$ and then use FTC for the last summand, but ""plugging in"" $t$ for every $s$ is wrong here because of the $t$ in the last integral and it surely gives me a wrong result if I tried to. I think I have to use some kind of chain rule here, but I need some help with the details! How do I differentiate the last summand?","Let $\psi(t)=\phi(t_0)*\exp(\int_{t_0}^t b(r)dr) + \int_{t_0}^t a(s) \exp(\int_{s}^t b(r)dr)ds$ Now I know already ( because this is from a differential equation example in a book ) that $$\psi'(t)= a(t)+ b(t)*\psi(t)$$ but I can't seem to differentiate correctly. If I differentiate $\psi(t)$  I should get  $$\psi'(t)=\phi(t_0)*\exp(\int_{t_0}^t b(r)dr)* b(t)+ \frac{d}{dt}\int_{t_0}^t a(s) \exp(\int_{s}^t b(r)dr)ds$$ and then use FTC for the last summand, but ""plugging in"" $t$ for every $s$ is wrong here because of the $t$ in the last integral and it surely gives me a wrong result if I tried to. I think I have to use some kind of chain rule here, but I need some help with the details! How do I differentiate the last summand?",,"['calculus', 'real-analysis', 'ordinary-differential-equations']"
39,On reducing complex ODE's to Bessel's form using Kummer's series,On reducing complex ODE's to Bessel's form using Kummer's series,,"I am trying to reduce the following ODE to Bessel's ODE form and solve it: $$x^{2}y''(x)+x(4x^{3}-3)y'(x)+(4x^{8}-5x^{2}+3)y(x)=0\tag{1} \, .$$ I tried to solve it via the standard method, i.e., by comparing it with a generalised ODE form and finding the solution from then on. The general form (as given in Mary L. Boas- Mathematical Methods in Physical Sciences ) is: $$y''(x)+\frac{1-2a}{x}y'(x)+\left((bcx^{c-1})^{2}+\frac{a^{2}-p^{2}c^{2}}{x^{2}}\right)y(x)=0\tag{2} \, ,$$   and the solution:$$y(x)=x^{a}Z_{p}(bx^{c})\tag{3} \, .$$ But I am unable to get the answer via this method. The solution which is as follows: $$y(x)=x^{2}e^{-\frac{x^{4}}{2}}[AI_{1}(\sqrt{5}x)+BK_{1}(\sqrt{5}x)]\tag{4}$$ Is obtained using comparison with another standard form which is given as follows: $$x^{2}y''(x)+x(a+2bx^{p})y'(x)+[c+dx^{2q}+b(a+p-1)x^{p}+b^{2}x^{2p})y(x)=0\tag{5} \, ,$$   and the solution as:   $$y(x)=x^{\alpha}e^{-\beta x^{p}}[AJ_{\nu}(\lambda x^{q})+BY_{\nu}(\lambda x^{q})]\tag{6} \, .$$ Where: $\alpha=\frac{1-a}{2}$, $\beta=\frac{b}{p}$, $\lambda=\frac{\sqrt{d}}{q}$, $\nu=\frac{\sqrt{(1-a)^{2}-4c)}}{2q}$ If I divide through the ode by $x^{2}$, I would get the Fuchasian form: $$y''(x)+f(x)y'(x)+g(x)y(x)=0$$ The terms of $xf(x)$ and $x^{2}g(x)$ are expandable in convergent power series $\sum_{n=0}^{\infty}a_{n}x^{n}$, hence there exists a nonessential singularity at the origin. But I am unable to solve via the Frobenius method. Hence, my question- How is the generalised form of equation $(5)$ arrived at and why can't I use $(2)$ instead?  Rather than bringing this ODE to a non-standard form as given in equation $(5)$, is there a way to derive the equation itself (and deduce the general solution)? Any help is appreciated. Edit: I found the following form in a book: $$x^{2}y''(x)+x(a+2bx^{p})y'(x)+[c+dx^{2q}+fx^{q}+b(a+p-1)x^{p}+b^{2}x^{2p})y(x)=0\tag{7} \, .$$ The only difference between the above and equation $(6)$ is the extra term:$fx^{q}$ Now if I substitute $y=we^{-\frac{bx^{p}}{p}}$ in equation $(7)$, it simplifies to the following linear equation: $$x^{2}w''(x)+axw'(x)+(dx^{2q}+fx^{q}+c)w(x)=0\tag{8}\,.$$ Now using the transformation $z=x^{q}$, and $y=wz^{k}$, where $k$ is the root of the following quadratic equation: $q^{2}k^{2}+q(a-1)k+c=0$; leads to a further simplified and linear form: $$q^{2}zy''(z)+[qbz+2kq^{2}+q(q-1+a)]y'(z)+(dz+kqb+f)y(z)=0\tag{9}\,.$$   This equation has the solution: $y(x)=e^{kx}w(z)$, where $w(z)$ is the solution to the hypergeometric equation as given below Now, let a function $\Omega(b,a;x)$ be an arbitrary solution to the degenerate hypergeometric equation: $$xy''(x)+(a-x)y'(x)-by(x)=0\tag{10}\,.$$ And $Z_{\nu}(x)$ be an arbitrary solution of the Bessel equation. Now in equation $(10)$, if $b\neq0,-1,-2,-3,...$, the solution is given by the Kummer's series as: $$\Phi(b,a;x)=1+\sum_{k=1}^{\infty}\frac{(b)_{k}x^{k}}{(a)_{k}k!}$$ Where:$(b)_{k}=b(b+1)...(b+k-1)$ When $a$ is not an integer, the solution can be written as: $$y=C_{1}\Phi(b,a;x)+C_{2}x^{1-a}\Phi(b-a+1,2-a;x)$$ Make the following replacements: $b=2n$ and $a=n$ Now the series becomes: $$\Phi(n,2n;x)=\Gamma\left(n+\frac{1}{2}\right)e^{\frac{x}{2}}\left(\frac{x}{4}\right)^{(-n+\frac{1}{2})}I_{n-\frac{1}{2}}(\frac{x}{2})$$   And   $$\Phi(-n,-2n;x)=\frac{1}{\sqrt{\pi}}e^{\frac{x}{2}}\left(x\right)^{(-n+\frac{1}{2})}K_{n-\frac{1}{2}}(x)$$ Substituting the above in the solution of equation $(9)$, the general solution becomes: $$y=e^{x(k+\frac{1}{2})}\left[C_{1}\Gamma\left(n+\frac{1}{2}\right)\left(\frac{x}{4}\right)^{(-n+\frac{1}{2})}I_{n+\frac{1}{2}}(x)+C_{2}\frac{1}{\sqrt{\pi}}\left(x\right)^{(-n+\frac{1}{2})}K_{n+\frac{1}{2}}(x)\right]$$ Which simplifies to: $$y(x)=\left(x\right)^{(-n+\frac{1}{2})}e^{x(k+\frac{1}{2})}\left[C_{1}\Gamma\left(n+\frac{1}{2}\right)\left(\frac{1}{4}\right)^{(-n+\frac{1}{2})}I_{n+\frac{1}{2}}(x)+C_{2}\frac{1}{\sqrt{\pi}}K_{n+\frac{1}{2}}(x)\right]$$ Which is the final solution. I tried to do the same for equation $(6)$, but did not get the solution. Any help is appreciated.","I am trying to reduce the following ODE to Bessel's ODE form and solve it: $$x^{2}y''(x)+x(4x^{3}-3)y'(x)+(4x^{8}-5x^{2}+3)y(x)=0\tag{1} \, .$$ I tried to solve it via the standard method, i.e., by comparing it with a generalised ODE form and finding the solution from then on. The general form (as given in Mary L. Boas- Mathematical Methods in Physical Sciences ) is: $$y''(x)+\frac{1-2a}{x}y'(x)+\left((bcx^{c-1})^{2}+\frac{a^{2}-p^{2}c^{2}}{x^{2}}\right)y(x)=0\tag{2} \, ,$$   and the solution:$$y(x)=x^{a}Z_{p}(bx^{c})\tag{3} \, .$$ But I am unable to get the answer via this method. The solution which is as follows: $$y(x)=x^{2}e^{-\frac{x^{4}}{2}}[AI_{1}(\sqrt{5}x)+BK_{1}(\sqrt{5}x)]\tag{4}$$ Is obtained using comparison with another standard form which is given as follows: $$x^{2}y''(x)+x(a+2bx^{p})y'(x)+[c+dx^{2q}+b(a+p-1)x^{p}+b^{2}x^{2p})y(x)=0\tag{5} \, ,$$   and the solution as:   $$y(x)=x^{\alpha}e^{-\beta x^{p}}[AJ_{\nu}(\lambda x^{q})+BY_{\nu}(\lambda x^{q})]\tag{6} \, .$$ Where: $\alpha=\frac{1-a}{2}$, $\beta=\frac{b}{p}$, $\lambda=\frac{\sqrt{d}}{q}$, $\nu=\frac{\sqrt{(1-a)^{2}-4c)}}{2q}$ If I divide through the ode by $x^{2}$, I would get the Fuchasian form: $$y''(x)+f(x)y'(x)+g(x)y(x)=0$$ The terms of $xf(x)$ and $x^{2}g(x)$ are expandable in convergent power series $\sum_{n=0}^{\infty}a_{n}x^{n}$, hence there exists a nonessential singularity at the origin. But I am unable to solve via the Frobenius method. Hence, my question- How is the generalised form of equation $(5)$ arrived at and why can't I use $(2)$ instead?  Rather than bringing this ODE to a non-standard form as given in equation $(5)$, is there a way to derive the equation itself (and deduce the general solution)? Any help is appreciated. Edit: I found the following form in a book: $$x^{2}y''(x)+x(a+2bx^{p})y'(x)+[c+dx^{2q}+fx^{q}+b(a+p-1)x^{p}+b^{2}x^{2p})y(x)=0\tag{7} \, .$$ The only difference between the above and equation $(6)$ is the extra term:$fx^{q}$ Now if I substitute $y=we^{-\frac{bx^{p}}{p}}$ in equation $(7)$, it simplifies to the following linear equation: $$x^{2}w''(x)+axw'(x)+(dx^{2q}+fx^{q}+c)w(x)=0\tag{8}\,.$$ Now using the transformation $z=x^{q}$, and $y=wz^{k}$, where $k$ is the root of the following quadratic equation: $q^{2}k^{2}+q(a-1)k+c=0$; leads to a further simplified and linear form: $$q^{2}zy''(z)+[qbz+2kq^{2}+q(q-1+a)]y'(z)+(dz+kqb+f)y(z)=0\tag{9}\,.$$   This equation has the solution: $y(x)=e^{kx}w(z)$, where $w(z)$ is the solution to the hypergeometric equation as given below Now, let a function $\Omega(b,a;x)$ be an arbitrary solution to the degenerate hypergeometric equation: $$xy''(x)+(a-x)y'(x)-by(x)=0\tag{10}\,.$$ And $Z_{\nu}(x)$ be an arbitrary solution of the Bessel equation. Now in equation $(10)$, if $b\neq0,-1,-2,-3,...$, the solution is given by the Kummer's series as: $$\Phi(b,a;x)=1+\sum_{k=1}^{\infty}\frac{(b)_{k}x^{k}}{(a)_{k}k!}$$ Where:$(b)_{k}=b(b+1)...(b+k-1)$ When $a$ is not an integer, the solution can be written as: $$y=C_{1}\Phi(b,a;x)+C_{2}x^{1-a}\Phi(b-a+1,2-a;x)$$ Make the following replacements: $b=2n$ and $a=n$ Now the series becomes: $$\Phi(n,2n;x)=\Gamma\left(n+\frac{1}{2}\right)e^{\frac{x}{2}}\left(\frac{x}{4}\right)^{(-n+\frac{1}{2})}I_{n-\frac{1}{2}}(\frac{x}{2})$$   And   $$\Phi(-n,-2n;x)=\frac{1}{\sqrt{\pi}}e^{\frac{x}{2}}\left(x\right)^{(-n+\frac{1}{2})}K_{n-\frac{1}{2}}(x)$$ Substituting the above in the solution of equation $(9)$, the general solution becomes: $$y=e^{x(k+\frac{1}{2})}\left[C_{1}\Gamma\left(n+\frac{1}{2}\right)\left(\frac{x}{4}\right)^{(-n+\frac{1}{2})}I_{n+\frac{1}{2}}(x)+C_{2}\frac{1}{\sqrt{\pi}}\left(x\right)^{(-n+\frac{1}{2})}K_{n+\frac{1}{2}}(x)\right]$$ Which simplifies to: $$y(x)=\left(x\right)^{(-n+\frac{1}{2})}e^{x(k+\frac{1}{2})}\left[C_{1}\Gamma\left(n+\frac{1}{2}\right)\left(\frac{1}{4}\right)^{(-n+\frac{1}{2})}I_{n+\frac{1}{2}}(x)+C_{2}\frac{1}{\sqrt{\pi}}K_{n+\frac{1}{2}}(x)\right]$$ Which is the final solution. I tried to do the same for equation $(6)$, but did not get the solution. Any help is appreciated.",,"['ordinary-differential-equations', 'bessel-functions', 'hypergeometric-function', 'frobenius-method']"
40,Arnold Diffusion,Arnold Diffusion,,"Let $H(I_1,I_2,\varphi_1,\varphi_2,t) = H_0 + H_1$ where $I_j$ is the conjugate variable of $\varphi_j$ for $j =1 ,2$ and $H_0 =\frac{1}{2} I_1^2 + \varepsilon (\cos\varphi_1 - 1)$ and $H_1 = \frac{1}{2} I_2^2$. I am trying to find the stable/unstable manifold of this system. The paper I am reading claims it is the three dimensional manifold described by $H_0 = 0$ , $H_1 = \frac{1}{2}\omega^2$ for some irrational $\omega$. I simply can't see how he got this. I see that it is 3 dimensional as $\varphi_2,t$ can vary without affecting H. If it helps: notice that $H_0$ is hamiltonian of the pendulum: $$\dot{I_1} = - \varepsilon \sin\varphi_1 \text{ and } \dot{\varphi_1} = I_1$$ and so $\ Just to give a bit of background: I am trying to fill in the details that Arnold left out in his 1964 paper ""Instability of dynamical systems with several degrees of freedom"". I can't seem to find any articles or references that aids with the rigorously checking out the details. So also if anyone could recommend anything to help that they have come across.","Let $H(I_1,I_2,\varphi_1,\varphi_2,t) = H_0 + H_1$ where $I_j$ is the conjugate variable of $\varphi_j$ for $j =1 ,2$ and $H_0 =\frac{1}{2} I_1^2 + \varepsilon (\cos\varphi_1 - 1)$ and $H_1 = \frac{1}{2} I_2^2$. I am trying to find the stable/unstable manifold of this system. The paper I am reading claims it is the three dimensional manifold described by $H_0 = 0$ , $H_1 = \frac{1}{2}\omega^2$ for some irrational $\omega$. I simply can't see how he got this. I see that it is 3 dimensional as $\varphi_2,t$ can vary without affecting H. If it helps: notice that $H_0$ is hamiltonian of the pendulum: $$\dot{I_1} = - \varepsilon \sin\varphi_1 \text{ and } \dot{\varphi_1} = I_1$$ and so $\ Just to give a bit of background: I am trying to fill in the details that Arnold left out in his 1964 paper ""Instability of dynamical systems with several degrees of freedom"". I can't seem to find any articles or references that aids with the rigorously checking out the details. So also if anyone could recommend anything to help that they have come across.",,"['ordinary-differential-equations', 'partial-differential-equations', 'dynamical-systems']"
41,"If $f'(x):\mathbb R^n \to \mathbb R^n$ is a isometry, then $f(x)=T(x)+a$.","If  is a isometry, then .",f'(x):\mathbb R^n \to \mathbb R^n f(x)=T(x)+a,"Hi folks, I'm trying to solve this one problem: Let $f:\mathbb R^n \to \mathbb R^n$ $\in \mathrm C^1$ such that for all $x \in \mathbb R^n$, $f'(x):\mathbb R^n \to \mathbb R^n$ is a isometry (i.e. $||f'(x)\cdot v||=||v||$) with respect to the Euclidean norm. Show that there is a Linear transformation $T:\mathbb R^n \to \mathbb R^n$ and a vector $a\in\mathbb R^n$ such that $$f(x)=T(x)+a, \forall x\in \mathbb R^n$$ I don't know if this information will be useful, but I've already showed that $f$ is also a isometry (i.e. $||f(x)-f(y)||=||x-y||$)","Hi folks, I'm trying to solve this one problem: Let $f:\mathbb R^n \to \mathbb R^n$ $\in \mathrm C^1$ such that for all $x \in \mathbb R^n$, $f'(x):\mathbb R^n \to \mathbb R^n$ is a isometry (i.e. $||f'(x)\cdot v||=||v||$) with respect to the Euclidean norm. Show that there is a Linear transformation $T:\mathbb R^n \to \mathbb R^n$ and a vector $a\in\mathbb R^n$ such that $$f(x)=T(x)+a, \forall x\in \mathbb R^n$$ I don't know if this information will be useful, but I've already showed that $f$ is also a isometry (i.e. $||f(x)-f(y)||=||x-y||$)",,"['real-analysis', 'ordinary-differential-equations', 'isometry']"
42,Parametric solution of the Brachistochrone problem,Parametric solution of the Brachistochrone problem,,"In my attempt to find the parametric solution to the Brachistochrone problem I write the differential equation resulting from the calculus of variations treatment as $$y(1+y'^2) = k^2 \Rightarrow y' = \sqrt{\frac{k^2 - y}{y}}$$ Then attempt a parameterization of the form $$\tan\phi = \frac{\mathrm{d}y}{\mathrm{d}x} = y' = \frac{\sqrt{k^2-y}}{\sqrt{y}}$$ and so $$\cos\phi = \frac{\sqrt{y}}{k} \Rightarrow y = k^2\cos^2\phi \Rightarrow \frac{\mathrm{d}y}{\mathrm{d}\phi} = -2k^2\cos\phi\sin\phi,$$ and $$\frac{\mathrm{d}x}{\mathrm{d}\phi} = \frac{\mathrm{d}x}{\mathrm{d}y}\frac{\mathrm{d}y}{\mathrm{d}\phi} = \cot\phi\cdot(-2k^2\cos\phi\sin\phi) = -k^2(\cos 2\phi + 1)$$ which I integrate to $$ x = -k^2(\frac{1}{2}\sin 2\phi + \phi) + A. $$ Now I'm stuck because this doesn't look like the equation of a cycloid that I recognise, and I don't know what range of values $\phi$ should take. The substitution $\theta = -2\phi$ almost works: $$ x = \frac{k^2}{2}(\sin\theta + \theta + A), y = \frac{k^2}{2}(1+\cos\theta), $$ but how do I find $A$ given the start and end points $P_1 = (0,0)$ and $P_2 = (x_2, y_2)$?","In my attempt to find the parametric solution to the Brachistochrone problem I write the differential equation resulting from the calculus of variations treatment as $$y(1+y'^2) = k^2 \Rightarrow y' = \sqrt{\frac{k^2 - y}{y}}$$ Then attempt a parameterization of the form $$\tan\phi = \frac{\mathrm{d}y}{\mathrm{d}x} = y' = \frac{\sqrt{k^2-y}}{\sqrt{y}}$$ and so $$\cos\phi = \frac{\sqrt{y}}{k} \Rightarrow y = k^2\cos^2\phi \Rightarrow \frac{\mathrm{d}y}{\mathrm{d}\phi} = -2k^2\cos\phi\sin\phi,$$ and $$\frac{\mathrm{d}x}{\mathrm{d}\phi} = \frac{\mathrm{d}x}{\mathrm{d}y}\frac{\mathrm{d}y}{\mathrm{d}\phi} = \cot\phi\cdot(-2k^2\cos\phi\sin\phi) = -k^2(\cos 2\phi + 1)$$ which I integrate to $$ x = -k^2(\frac{1}{2}\sin 2\phi + \phi) + A. $$ Now I'm stuck because this doesn't look like the equation of a cycloid that I recognise, and I don't know what range of values $\phi$ should take. The substitution $\theta = -2\phi$ almost works: $$ x = \frac{k^2}{2}(\sin\theta + \theta + A), y = \frac{k^2}{2}(1+\cos\theta), $$ but how do I find $A$ given the start and end points $P_1 = (0,0)$ and $P_2 = (x_2, y_2)$?",,"['calculus', 'ordinary-differential-equations', 'parametric']"
43,Slope of the Orthogonal Trajectory in Polar Coordinates (Versus the Slope of the Orthogonal Trajectory in the $xy$-plane),Slope of the Orthogonal Trajectory in Polar Coordinates (Versus the Slope of the Orthogonal Trajectory in the -plane),xy,"When finding the orthogonal trajectories of a family of curves in the $xy$ plane, we do the following: Differentiate the equation of the family of curves with respect to the independent variables, which gives us the slope of the family of curves; Eliminate the parameter (if it didn't already get eliminated during differentiation); Rearrange to get the form $\dfrac{dy}{dx} = f(x, y)$; Take the negative reciprocal of $\dfrac{dy}{dx} \implies \dfrac{-dx}{dy} = f(x, y)$, which gives us the slope of the orthogonal trajectories; Solve the differential equation using separation of variables or some other method. This gives us the equation of the orthogonal trajectories. In my previous (related) question , I mentioned a peculiar problem in my textbook, "" Differential Equations with Applications and Historical Notes, 3rd edition "", by Simmons and Finlay, where the authors used polar coordinates to solve the orthogonal trajectories problem. In the aforementioned question, I discovered that the reason for my confusion was because the slope of the orthogonal trajectories in polar form is different to the slope of the orthogonal trajectories in the xy plane : The slope of the orthogonal trajectories in the xy plane , as previously mentioned, is the negative reciprocal of $\dfrac{dy}{dx} \implies \dfrac{-dx}{dy} = f(x, y)$, whilst the slope of the orthogonal trajectories in polar form is the negative reciprocal of $\dfrac{1}{r}\dfrac{\mathrm dr}{\mathrm d\theta} \implies -r\dfrac{\mathrm d\theta}{\mathrm dr} = f(r, \theta)$. In other words, and more generally, we can see that the way in which we get the slope of the orthogonal (normal) trajectories (vectors) in polar form is different from that in the $xy$-plane. My original confusion stemmed from this fact that, with polar coordinates, we do not only take the negative reciprocal of the operator $\dfrac{\mathrm dr}{\mathrm d\theta}$, but also the negative reciprocal of $\dfrac{1}{r}$ along with it. This difference was not mentioned in the textbook; the only case that was mentioned was that of dealing with the operator $\dfrac{dy}{dx} = f(x, y)$ -- coordinates of the xy-plane. I would greatly appreciate it if people could please take the time to post a step-by-step proof that clearly shows that, unlike the orthogonal trajectory in the $xy$-plane, the orthogonal trajectory in polar form is found by taking the negative reciprocal of $\dfrac{dr}{rd\theta} \implies -r\dfrac{\mathrm d\theta}{\mathrm dr} = f(r, \theta)$. My goal is to convince myself that this is true -- that the way we get the slope for the orthogonal trajectories is different between equations using coordinates in the xy-plane $\left( \dfrac{dy}{dx} \implies \dfrac{-dx}{dy} = f(x, y) \right)$ and those using coordinates in polar form $\left( \dfrac{1}{r}\dfrac{\mathrm dr}{\mathrm d\theta}\ \implies -r\dfrac{\mathrm d\theta}{\mathrm dr} = f(r, \theta) \right)$.","When finding the orthogonal trajectories of a family of curves in the $xy$ plane, we do the following: Differentiate the equation of the family of curves with respect to the independent variables, which gives us the slope of the family of curves; Eliminate the parameter (if it didn't already get eliminated during differentiation); Rearrange to get the form $\dfrac{dy}{dx} = f(x, y)$; Take the negative reciprocal of $\dfrac{dy}{dx} \implies \dfrac{-dx}{dy} = f(x, y)$, which gives us the slope of the orthogonal trajectories; Solve the differential equation using separation of variables or some other method. This gives us the equation of the orthogonal trajectories. In my previous (related) question , I mentioned a peculiar problem in my textbook, "" Differential Equations with Applications and Historical Notes, 3rd edition "", by Simmons and Finlay, where the authors used polar coordinates to solve the orthogonal trajectories problem. In the aforementioned question, I discovered that the reason for my confusion was because the slope of the orthogonal trajectories in polar form is different to the slope of the orthogonal trajectories in the xy plane : The slope of the orthogonal trajectories in the xy plane , as previously mentioned, is the negative reciprocal of $\dfrac{dy}{dx} \implies \dfrac{-dx}{dy} = f(x, y)$, whilst the slope of the orthogonal trajectories in polar form is the negative reciprocal of $\dfrac{1}{r}\dfrac{\mathrm dr}{\mathrm d\theta} \implies -r\dfrac{\mathrm d\theta}{\mathrm dr} = f(r, \theta)$. In other words, and more generally, we can see that the way in which we get the slope of the orthogonal (normal) trajectories (vectors) in polar form is different from that in the $xy$-plane. My original confusion stemmed from this fact that, with polar coordinates, we do not only take the negative reciprocal of the operator $\dfrac{\mathrm dr}{\mathrm d\theta}$, but also the negative reciprocal of $\dfrac{1}{r}$ along with it. This difference was not mentioned in the textbook; the only case that was mentioned was that of dealing with the operator $\dfrac{dy}{dx} = f(x, y)$ -- coordinates of the xy-plane. I would greatly appreciate it if people could please take the time to post a step-by-step proof that clearly shows that, unlike the orthogonal trajectory in the $xy$-plane, the orthogonal trajectory in polar form is found by taking the negative reciprocal of $\dfrac{dr}{rd\theta} \implies -r\dfrac{\mathrm d\theta}{\mathrm dr} = f(r, \theta)$. My goal is to convince myself that this is true -- that the way we get the slope for the orthogonal trajectories is different between equations using coordinates in the xy-plane $\left( \dfrac{dy}{dx} \implies \dfrac{-dx}{dy} = f(x, y) \right)$ and those using coordinates in polar form $\left( \dfrac{1}{r}\dfrac{\mathrm dr}{\mathrm d\theta}\ \implies -r\dfrac{\mathrm d\theta}{\mathrm dr} = f(r, \theta) \right)$.",,"['ordinary-differential-equations', 'polar-coordinates']"
44,Symmetry-finding with SAGE,Symmetry-finding with SAGE,,"On pp. 152-3 of Hydon's Symmetry Methods for Differential Equations (2000 ed.), he lists some computer packages for symmetry-finding. This related Mathematica StackExchange question mentions the SYM Mathematica package and Maple's DEtools/symgen . Does SAGE have anything similar for doing symmetry-finding? This question also posted on ask.sagemath.org","On pp. 152-3 of Hydon's Symmetry Methods for Differential Equations (2000 ed.), he lists some computer packages for symmetry-finding. This related Mathematica StackExchange question mentions the SYM Mathematica package and Maple's DEtools/symgen . Does SAGE have anything similar for doing symmetry-finding? This question also posted on ask.sagemath.org",,"['ordinary-differential-equations', 'lie-groups', 'symmetry', 'sagemath']"
45,If $f$ bounded and $f''>f$ then $f$ decreases exponentially,If  bounded and  then  decreases exponentially,f f''>f f,"Let $f$ be a function of class $C^2$ from $ \mathbb{R^{+,*}}$ to $\mathbb{R^{+, *}}$ (where  $\mathbb{R^{+,*}}$ denotes the set of nonnegative reals) such that $f''>f$ and $f$ is bounded above. Show that $f \le f(0)e^{-x}$.","Let $f$ be a function of class $C^2$ from $ \mathbb{R^{+,*}}$ to $\mathbb{R^{+, *}}$ (where  $\mathbb{R^{+,*}}$ denotes the set of nonnegative reals) such that $f''>f$ and $f$ is bounded above. Show that $f \le f(0)e^{-x}$.",,"['calculus', 'real-analysis', 'ordinary-differential-equations']"
46,What boundary condition is imposed when Fourier transform is used for solving differential equation on infinite domain?,What boundary condition is imposed when Fourier transform is used for solving differential equation on infinite domain?,,"It's a question that has puzzled me for a long time. Every PDE textbook I've ever seen tells me that, Fourier transform can be used to solve linear constant-coefficient differential equations on an infinite domain, but none of them includes an explanation about what boundary condition is actually used when Fourier transform ""kills"" the derivative. Some materials, for example this seems to suggest that, the boundary condition is $0$ at $\pm\infty$, but it's not true. A counter example is $$y'(x)+y(x)=\sin (x)$$ The general solution of this equation is (* Here's the corresponding Mathematica code *) DSolve[y'[x] + y[x] == Sin[x], y[x], x] $$y(x)= c_1 e^{-x}+\frac{1}{2} (\sin (x)-\cos (x))$$ while the solution given by Fourier transform and inverse Fourier transform is (* Here's the corresponding Mathematica code *) fou = FourierTransform[#, x, w] &; fou[y'[x]] + fou@y[x] == fou@Sin[x] /. HoldPattern@FourierTransform[__] :> Y[w] Solve[%, Y[w]][[1, 1, -1]] InverseFourierTransform[%, w, x] $$y(x)=\frac{1}{2} (\sin (x)-\cos (x))$$ Clearly the boundary condition isn't $y(\pm\infty)=0$ or $y'(\pm\infty)=0$. What boundary condition / restriction is imposed when Fourier transform is used for solving differential equations?","It's a question that has puzzled me for a long time. Every PDE textbook I've ever seen tells me that, Fourier transform can be used to solve linear constant-coefficient differential equations on an infinite domain, but none of them includes an explanation about what boundary condition is actually used when Fourier transform ""kills"" the derivative. Some materials, for example this seems to suggest that, the boundary condition is $0$ at $\pm\infty$, but it's not true. A counter example is $$y'(x)+y(x)=\sin (x)$$ The general solution of this equation is (* Here's the corresponding Mathematica code *) DSolve[y'[x] + y[x] == Sin[x], y[x], x] $$y(x)= c_1 e^{-x}+\frac{1}{2} (\sin (x)-\cos (x))$$ while the solution given by Fourier transform and inverse Fourier transform is (* Here's the corresponding Mathematica code *) fou = FourierTransform[#, x, w] &; fou[y'[x]] + fou@y[x] == fou@Sin[x] /. HoldPattern@FourierTransform[__] :> Y[w] Solve[%, Y[w]][[1, 1, -1]] InverseFourierTransform[%, w, x] $$y(x)=\frac{1}{2} (\sin (x)-\cos (x))$$ Clearly the boundary condition isn't $y(\pm\infty)=0$ or $y'(\pm\infty)=0$. What boundary condition / restriction is imposed when Fourier transform is used for solving differential equations?",,"['ordinary-differential-equations', 'partial-differential-equations', 'fourier-analysis', 'fourier-transform']"
47,Show that $\sup_{0<x<\infty} (\mathrm{cos}x+\mathrm{sin}\sqrt{2}x)=2$.,Show that .,\sup_{0<x<\infty} (\mathrm{cos}x+\mathrm{sin}\sqrt{2}x)=2,"This is a problem in V Arnold's Ordinary Differential Equations. By plotting the function $f(x)=\mathrm{cos}x+\mathrm{sin}\sqrt{2}x$ (the first graph below), I see that the graph is bounded from -2 to 2. But the boundary points $\{2,-2\}$ are not attained by $f(x)$. Intuitively, it seems true that $f(x)$ approaches 2 when both $\mathrm{cos}x$ and $\mathrm{sin}\sqrt{2}x$ approach 1. From the graph of both $\mathrm{cos}x$ and $\mathrm{sin}\sqrt{2}x$ (the second graph below), one can see that around $x=19$, both $\mathrm{cos}x$ and $\mathrm{sin}\sqrt{2}x$ approach 1. But this doesn't seem like much of a proof. Is there another approach to show this equality? Also, I fail to find an ODE-related approach to this problem.","This is a problem in V Arnold's Ordinary Differential Equations. By plotting the function $f(x)=\mathrm{cos}x+\mathrm{sin}\sqrt{2}x$ (the first graph below), I see that the graph is bounded from -2 to 2. But the boundary points $\{2,-2\}$ are not attained by $f(x)$. Intuitively, it seems true that $f(x)$ approaches 2 when both $\mathrm{cos}x$ and $\mathrm{sin}\sqrt{2}x$ approach 1. From the graph of both $\mathrm{cos}x$ and $\mathrm{sin}\sqrt{2}x$ (the second graph below), one can see that around $x=19$, both $\mathrm{cos}x$ and $\mathrm{sin}\sqrt{2}x$ approach 1. But this doesn't seem like much of a proof. Is there another approach to show this equality? Also, I fail to find an ODE-related approach to this problem.",,"['ordinary-differential-equations', 'trigonometry', 'dynamical-systems', 'supremum-and-infimum']"
48,Determine a stability region?,Determine a stability region?,,"The question I'm trying to solve is the following: The implicit midpoint method is defined as: $y_{n+1} = y_n + hf(t_{n+1/2},(y_n + y_{n+1}/2),$ where $t_{n+1/2} = t_n + {h/2}$ What is the stability region for the equation $y' = \lambda y$? In other words, for what values $\bar{h} = h \lambda \space \in \space \mathbb{C}$ is the method stable? I really have no idea on where to even begin with this. What I tried to do was plug in $\lambda y_n$ into the trapezoid method so I can obtain $y_n$ as a function of it and $\bar{h}$, but I wasn't really getting any where. Honestly, I'm not even sure if there's supposed to be an explicit way of getting the answer here. Does anybody have any idea on how to do this? I'd really appreciate any help.","The question I'm trying to solve is the following: The implicit midpoint method is defined as: $y_{n+1} = y_n + hf(t_{n+1/2},(y_n + y_{n+1}/2),$ where $t_{n+1/2} = t_n + {h/2}$ What is the stability region for the equation $y' = \lambda y$? In other words, for what values $\bar{h} = h \lambda \space \in \space \mathbb{C}$ is the method stable? I really have no idea on where to even begin with this. What I tried to do was plug in $\lambda y_n$ into the trapezoid method so I can obtain $y_n$ as a function of it and $\bar{h}$, but I wasn't really getting any where. Honestly, I'm not even sure if there's supposed to be an explicit way of getting the answer here. Does anybody have any idea on how to do this? I'd really appreciate any help.",,"['ordinary-differential-equations', 'numerical-methods', 'computational-mathematics', 'stability-in-odes', 'stability-theory']"
49,Ito's product rule: Three processes,Ito's product rule: Three processes,,"I stumbled upon the following problem, I want to compute the stochastic differential of the following 3 processes: $dX_t = \mu_{X,t}dt + \sigma_{X,t}dW_t$ $dY_t = \mu_{Y,t}dt + \sigma_{Y,t}dW_t$ $dZ_t = \mu_{Z,t}dt + \sigma_{Z,t}dW_t$ That is, I want to calculate $d(X_tY_tZ_t)$ using Ito's product rule Now from the aforementioned product rule I know that $d(X_tY_t) = X_tdY_t + Y_tdX_t + \sigma_{X,t}\sigma_{Y,t}dt$. I tried to write $P_t = X_tY_t$ and substitute to calculate $d(P_tZ_t)$. I obtained the process $d(X_tY_tZ_t) = X_tY_tdZ_t + X_tZ_tdY_t + Y_tZ_tdX_t + Z_t\sigma_{X,t}\sigma_{Y,t}dt + \sigma_{P,t}\sigma_{Z,t}dt$ but do not know how to work out the last volatility product. The correct process is apparently equal to $d(X_tY_tZ_t) = X_tY_tdZ_t + X_tZ_tdY_t + Y_tZ_tdX_t + Z_t\sigma_{X,t}\sigma_{Y,t}dt + X_t\sigma_{Y,t}\sigma_{Z,t}dt+Y_t\sigma_{X,t}\sigma_{Z,t}dt$ I was hoping if somebody could help me arriving at the above equation. Moreover, I am also curious to find out how to calculate the SDE $d(X_t/Y_t)$, given that the processes $X_t$ and $Y_t$ are the same as given above. If somebody knows how to tackle this one, I would be very grateful for your help. Many thanks in advance.","I stumbled upon the following problem, I want to compute the stochastic differential of the following 3 processes: $dX_t = \mu_{X,t}dt + \sigma_{X,t}dW_t$ $dY_t = \mu_{Y,t}dt + \sigma_{Y,t}dW_t$ $dZ_t = \mu_{Z,t}dt + \sigma_{Z,t}dW_t$ That is, I want to calculate $d(X_tY_tZ_t)$ using Ito's product rule Now from the aforementioned product rule I know that $d(X_tY_t) = X_tdY_t + Y_tdX_t + \sigma_{X,t}\sigma_{Y,t}dt$. I tried to write $P_t = X_tY_t$ and substitute to calculate $d(P_tZ_t)$. I obtained the process $d(X_tY_tZ_t) = X_tY_tdZ_t + X_tZ_tdY_t + Y_tZ_tdX_t + Z_t\sigma_{X,t}\sigma_{Y,t}dt + \sigma_{P,t}\sigma_{Z,t}dt$ but do not know how to work out the last volatility product. The correct process is apparently equal to $d(X_tY_tZ_t) = X_tY_tdZ_t + X_tZ_tdY_t + Y_tZ_tdX_t + Z_t\sigma_{X,t}\sigma_{Y,t}dt + X_t\sigma_{Y,t}\sigma_{Z,t}dt+Y_t\sigma_{X,t}\sigma_{Z,t}dt$ I was hoping if somebody could help me arriving at the above equation. Moreover, I am also curious to find out how to calculate the SDE $d(X_t/Y_t)$, given that the processes $X_t$ and $Y_t$ are the same as given above. If somebody knows how to tackle this one, I would be very grateful for your help. Many thanks in advance.",,"['ordinary-differential-equations', 'stochastic-calculus']"
50,Exercise problems in Arnold's ODE concerning direction/vector fields under diffeomorphism,Exercise problems in Arnold's ODE concerning direction/vector fields under diffeomorphism,,"I am stuck with some problems in Arnold's Ordinary Differential Equation. In Page 75, Problem 5, Arnold asked Can a diffeomorphism of the plane map the direction field of the differential equation $\dot{x}=x^2$ into a field of parallel lines? and offers an Answer. It is possible, though an explicit formula is difficult to write out. But I can't see why it's possible. The diffeomorphism needs to map the line $x=0$ to some line $\ell$ in the plane. Since $x=0$ is the asymptote of the integral curves of $\dot{x}=x^2$, while the image of these curves are a collection of parallel lines. So $\ell$ should be the asymptote of these parallel lines, which means they are all the same line, a contradiction. What's wrong with my reasoning above? And why is it possible to find the diffeomorphism? Any help would be appreciated. Additional problem. Our ODE class has specified Arnold as the textbook, but unfortunately, our teacher seems to be not so good at presenting the ideas in the book clearly. To make things worse, exercises in Arnold's book frequently seems confusing(may partly due to translation, which makes sentences unsmooth), and our teacher never explains those exercises. So I'm wondering whether there are sources available about the solution of problems in Arnold's ODE. I googled with no relevant results. Thanks again for any help!","I am stuck with some problems in Arnold's Ordinary Differential Equation. In Page 75, Problem 5, Arnold asked Can a diffeomorphism of the plane map the direction field of the differential equation $\dot{x}=x^2$ into a field of parallel lines? and offers an Answer. It is possible, though an explicit formula is difficult to write out. But I can't see why it's possible. The diffeomorphism needs to map the line $x=0$ to some line $\ell$ in the plane. Since $x=0$ is the asymptote of the integral curves of $\dot{x}=x^2$, while the image of these curves are a collection of parallel lines. So $\ell$ should be the asymptote of these parallel lines, which means they are all the same line, a contradiction. What's wrong with my reasoning above? And why is it possible to find the diffeomorphism? Any help would be appreciated. Additional problem. Our ODE class has specified Arnold as the textbook, but unfortunately, our teacher seems to be not so good at presenting the ideas in the book clearly. To make things worse, exercises in Arnold's book frequently seems confusing(may partly due to translation, which makes sentences unsmooth), and our teacher never explains those exercises. So I'm wondering whether there are sources available about the solution of problems in Arnold's ODE. I googled with no relevant results. Thanks again for any help!",,"['ordinary-differential-equations', 'reference-request']"
51,How to interpret the differential equation (z+x)dx + (x+z)dy + (x+y)dz = 0?,How to interpret the differential equation (z+x)dx + (x+z)dy + (x+y)dz = 0?,,"Since $\frac{dy}{dx}$ is not a ratio but a limit, the differential equation  $\frac{d^2y}{dx^2} -3 \frac{dy}{dx} + 2y = 0$ can be interpreted as a function $y$ whose derivatives first order and second order satisfy the equation and not like differentials $dx, dy$ satisfy the equation. But how to interpret a differential equation of the form $(z+x)dx + (x+z)dy + (x+y)dz = 0$ and also is there any rigorous way of expressing the equation above. Since a derivative must be expressed as $\frac{dy}{dx} = \lim_{h\to 0} \frac{f(x+h)- f(x)}{h}$? Thanks in advance.","Since $\frac{dy}{dx}$ is not a ratio but a limit, the differential equation  $\frac{d^2y}{dx^2} -3 \frac{dy}{dx} + 2y = 0$ can be interpreted as a function $y$ whose derivatives first order and second order satisfy the equation and not like differentials $dx, dy$ satisfy the equation. But how to interpret a differential equation of the form $(z+x)dx + (x+z)dy + (x+y)dz = 0$ and also is there any rigorous way of expressing the equation above. Since a derivative must be expressed as $\frac{dy}{dx} = \lim_{h\to 0} \frac{f(x+h)- f(x)}{h}$? Thanks in advance.",,['ordinary-differential-equations']
52,"Modified Wave Equation: Bound $\int u^2 \, dx$",Modified Wave Equation: Bound,"\int u^2 \, dx","I'm studying for a qualifying exam and I can't figure this problem out: Suppose $B \subset \mathbb R^n$ is the unit ball centered at the origin and  that $u$ is a smooth solution of \begin{align*} u_{tt} + a(x) u_t - \Delta u &= 0 \,\,\,\,\,\,\,\,\,\,\,\,\, \text{ in } B \times (0,\infty) \\ u(x,t) &= 0 \,\,\,\,\,\,\,\,\,\,\,\, \text{ on } \partial B\times (0,\infty) \\ u(x,0) = g(x), \,\,\, u_t(x,0) &=h(x) \,\,\,\,\, \text{ in } B\end{align*} where $a(x),g(x),h(x)$ are smooth with $g(x) = h(x) = 0$ on $\partial B$ and $0 < \lambda < a(x)$ in $B$. Show that $\int_B u(x,t)^2 dx \le C e^{-\lambda t}$ for all $t > 0$ where $C$ is a constant depending on $g,h$ and the dimension $n.$ This problem shouts 'energy methods' to me, but I can't make it work. First, I defined $$E(t) = \int_B u(x,t)^2 dx$$ since this is what we are trying to bound but this energy doesn't lend itself naturally to the wave equation and indeed differentiating gives nothing too useful (as far as I can see). Next, I tried the natural energy for the wave equation: $$E(t) = \int_B u_t^2 + \lvert \nabla u \rvert^2 dx$$ but this doesn't seem to help since it doesn't include $u^2$. Finally, I tried the energy $$E(t) =\int_B u_t^2 + \lvert \nabla u \rvert^2 + a(x) u^2 dx$$ but again, this didn't seem to lead to anything useful. Am I simply not seeing the correct energy to use or is there some other method that I should be trying? Any help would be appreciated.","I'm studying for a qualifying exam and I can't figure this problem out: Suppose $B \subset \mathbb R^n$ is the unit ball centered at the origin and  that $u$ is a smooth solution of \begin{align*} u_{tt} + a(x) u_t - \Delta u &= 0 \,\,\,\,\,\,\,\,\,\,\,\,\, \text{ in } B \times (0,\infty) \\ u(x,t) &= 0 \,\,\,\,\,\,\,\,\,\,\,\, \text{ on } \partial B\times (0,\infty) \\ u(x,0) = g(x), \,\,\, u_t(x,0) &=h(x) \,\,\,\,\, \text{ in } B\end{align*} where $a(x),g(x),h(x)$ are smooth with $g(x) = h(x) = 0$ on $\partial B$ and $0 < \lambda < a(x)$ in $B$. Show that $\int_B u(x,t)^2 dx \le C e^{-\lambda t}$ for all $t > 0$ where $C$ is a constant depending on $g,h$ and the dimension $n.$ This problem shouts 'energy methods' to me, but I can't make it work. First, I defined $$E(t) = \int_B u(x,t)^2 dx$$ since this is what we are trying to bound but this energy doesn't lend itself naturally to the wave equation and indeed differentiating gives nothing too useful (as far as I can see). Next, I tried the natural energy for the wave equation: $$E(t) = \int_B u_t^2 + \lvert \nabla u \rvert^2 dx$$ but this doesn't seem to help since it doesn't include $u^2$. Finally, I tried the energy $$E(t) =\int_B u_t^2 + \lvert \nabla u \rvert^2 + a(x) u^2 dx$$ but again, this didn't seem to lead to anything useful. Am I simply not seeing the correct energy to use or is there some other method that I should be trying? Any help would be appreciated.",,"['real-analysis', 'ordinary-differential-equations', 'partial-differential-equations', 'wave-equation']"
53,"Algebra of Linear differential operators, question on Commutativity and Association","Algebra of Linear differential operators, question on Commutativity and Association",,"The following is a discussion on the following second differential equation $$  \frac{dy^2}{dx} - y = 0 $$ So, let us introduce the following, convention and definition, represent the derivative operator by $$ D = \frac{d}{dx} $$ So that our first equation can be represented as $$ \left( D^2 - 1 \right) y(x) = 0 $$ My professor discussed on the idea, on ""factoring"" (The space on which the derivative acts?) the operators, his discussion lead to the apparent nonuniqueness of factoring as either $$ (D - \tanh(x))(D + \tanh(x)) y = 0 \quad \textrm{and } \quad (D - 1)(D + 1) = 0 $$ My first question is that how is that  $ (D - \tanh(x))(D + \tanh(x))  ``="" (D^2 - 1) $ and how is unfoiled?, because I think is not commutative because $$ f(x)\frac{d}{dx}y \neq \frac{d}{dx}f(x)y(x) $$ One is first derive something and multiply by $f$, and the other is derive the product. Second, my professor move to the general $$ y'' + a(x)y' + b(x) = 0 $$ or $$ \big(D^2 + a(x)D + b(x)\big)y(x) = 0 $$ and he wants to factor the above as $$ \big( D + A(x)\big)\big(D + B(x)\big)y(x) = 0 $$ So then he unfoils the above equation as: $$ \big( D^2 + AD + AB + B' + BD \big)y = 0 $$ and he argues that the terms $B'$ and $BD$ comes from the fact that either we take the derivative of $B$ or take the derivative and multiply by B (Why that doesn't apply to the terms $ AD  $ and we should add $ A'$ ? Also, do you happen to know where to find reference on solving ODEs by this approach? Thanks","The following is a discussion on the following second differential equation $$  \frac{dy^2}{dx} - y = 0 $$ So, let us introduce the following, convention and definition, represent the derivative operator by $$ D = \frac{d}{dx} $$ So that our first equation can be represented as $$ \left( D^2 - 1 \right) y(x) = 0 $$ My professor discussed on the idea, on ""factoring"" (The space on which the derivative acts?) the operators, his discussion lead to the apparent nonuniqueness of factoring as either $$ (D - \tanh(x))(D + \tanh(x)) y = 0 \quad \textrm{and } \quad (D - 1)(D + 1) = 0 $$ My first question is that how is that  $ (D - \tanh(x))(D + \tanh(x))  ``="" (D^2 - 1) $ and how is unfoiled?, because I think is not commutative because $$ f(x)\frac{d}{dx}y \neq \frac{d}{dx}f(x)y(x) $$ One is first derive something and multiply by $f$, and the other is derive the product. Second, my professor move to the general $$ y'' + a(x)y' + b(x) = 0 $$ or $$ \big(D^2 + a(x)D + b(x)\big)y(x) = 0 $$ and he wants to factor the above as $$ \big( D + A(x)\big)\big(D + B(x)\big)y(x) = 0 $$ So then he unfoils the above equation as: $$ \big( D^2 + AD + AB + B' + BD \big)y = 0 $$ and he argues that the terms $B'$ and $BD$ comes from the fact that either we take the derivative of $B$ or take the derivative and multiply by B (Why that doesn't apply to the terms $ AD  $ and we should add $ A'$ ? Also, do you happen to know where to find reference on solving ODEs by this approach? Thanks",,"['linear-algebra', 'ordinary-differential-equations', 'derivatives', 'operator-theory']"
54,The significance of failure of uniqueness in differential equations,The significance of failure of uniqueness in differential equations,,"The nonlinear ODE: $y'(t)=y(t)^{1/2}$ with initial condition $y(0)=1$ has two solutions.  Non-uniqueness is not surprising because of the failure of Lipschitz continuity in the $y$ term.  While this is formally true, what if any, is the practical significance of the failure of uniqueness of solutions?  For example, if this ODE (or a similar PDE) modelled some biological or physical phenomenon, would non-uniqueness mean anything? Edit: Thanks for the responses so far!  I found a passage in Fung and Tong's Classical and Computational Solid Mechanics in Chapter 21 on page 849 which reads: One of the distinct characteristics of nonlinear problems is that the   solution may not be unique. In the nonuniqueness, there lies much of   nature’s secret. Examples in solid mechanics are the buckling of thin   shells, self-equilibrating residual stresses and strains,   three-dimensional solutions in bodies with apparently two-dimensional   boundary conditions, and many problems in plasticity. I am not so familiar with these models: perhaps someone wiser than myself might have some specific insight?","The nonlinear ODE: $y'(t)=y(t)^{1/2}$ with initial condition $y(0)=1$ has two solutions.  Non-uniqueness is not surprising because of the failure of Lipschitz continuity in the $y$ term.  While this is formally true, what if any, is the practical significance of the failure of uniqueness of solutions?  For example, if this ODE (or a similar PDE) modelled some biological or physical phenomenon, would non-uniqueness mean anything? Edit: Thanks for the responses so far!  I found a passage in Fung and Tong's Classical and Computational Solid Mechanics in Chapter 21 on page 849 which reads: One of the distinct characteristics of nonlinear problems is that the   solution may not be unique. In the nonuniqueness, there lies much of   nature’s secret. Examples in solid mechanics are the buckling of thin   shells, self-equilibrating residual stresses and strains,   three-dimensional solutions in bodies with apparently two-dimensional   boundary conditions, and many problems in plasticity. I am not so familiar with these models: perhaps someone wiser than myself might have some specific insight?",,"['ordinary-differential-equations', 'intuition']"
55,Problem 5 of chapter 6 of Evans PDE 1st.,Problem 5 of chapter 6 of Evans PDE 1st.,,"I don't know how to start it. I try to compute the $Lv$ , but nothing I get.Maybe, I think some hint is suitable for me . Thanks.","I don't know how to start it. I try to compute the $Lv$ , but nothing I get.Maybe, I think some hint is suitable for me . Thanks.",,"['ordinary-differential-equations', 'partial-differential-equations', 'elliptic-equations']"
56,Find the differential equation of all non-horizontal lines in a plane.,Find the differential equation of all non-horizontal lines in a plane.,,"Find the differential equation of all non-horizontal lines in a plane. This question is also present here . But I do not understand that solution. Let $y=mx+c,m\ne0$ be the equation of all non-horizontal lines in a plane. As it has two arbitrary constants $m$ and $c$,we need to differentiate it two times. $y=mx+c$ $\frac{dy}{dx}=m$ $\frac{d^2y}{dx^2}=0$ But my book says answer is $\frac{d^2x}{dy^2}=0$. Book gives this solution. Let $ax+by+c=0,a\ne0$ be the equation of all non-horizontal lines in a plane. Differentiating both sides wrt $y$, $a\frac{dx}{dy}+b=0$ $a\frac{d^2x}{dy^2}=0$ As $a\ne0,\frac{d^2x}{dy^2}=0$ My answer and book answer are completely different.I do not understand what is wrong in my answer?","Find the differential equation of all non-horizontal lines in a plane. This question is also present here . But I do not understand that solution. Let $y=mx+c,m\ne0$ be the equation of all non-horizontal lines in a plane. As it has two arbitrary constants $m$ and $c$,we need to differentiate it two times. $y=mx+c$ $\frac{dy}{dx}=m$ $\frac{d^2y}{dx^2}=0$ But my book says answer is $\frac{d^2x}{dy^2}=0$. Book gives this solution. Let $ax+by+c=0,a\ne0$ be the equation of all non-horizontal lines in a plane. Differentiating both sides wrt $y$, $a\frac{dx}{dy}+b=0$ $a\frac{d^2x}{dy^2}=0$ As $a\ne0,\frac{d^2x}{dy^2}=0$ My answer and book answer are completely different.I do not understand what is wrong in my answer?",,['ordinary-differential-equations']
57,Inverse Laplace Transform (Natural Logarithm Case),Inverse Laplace Transform (Natural Logarithm Case),,"I have a problem about Inverse Laplace Transform, I would be appreciated to get your help for solving this problem (It took me about several hours to think but didn't come up with any solution). Please find the inverse laplace transform of : $$\ln\left(1+\frac{a^2}{s^2}\right)$$ where ""$a$"" is an constant.","I have a problem about Inverse Laplace Transform, I would be appreciated to get your help for solving this problem (It took me about several hours to think but didn't come up with any solution). Please find the inverse laplace transform of : $$\ln\left(1+\frac{a^2}{s^2}\right)$$ where ""$a$"" is an constant.",,"['ordinary-differential-equations', 'laplace-transform']"
58,"Lie group question: If $\gamma^{-1}\dot{\gamma}\in\mathfrak{g}$ everywhere, does $\gamma(t)\in G$?","Lie group question: If  everywhere, does ?",\gamma^{-1}\dot{\gamma}\in\mathfrak{g} \gamma(t)\in G,"Let $G$ be a Lie subgroup of $GL(n,\Bbb R)$ and $\mathfrak{g}\subseteq M(n,\Bbb R)$ its Lie algebra. Suppose that we have a smooth curve $$\gamma:\Bbb R\to G$$ with $\gamma(0)=I$. Then, it induces a curve on the Lie algebra $$\alpha:\Bbb R\to \mathfrak{g},\quad \alpha(t)=\gamma(t)^{-1}\frac{d\gamma}{dt}(t).$$ I am wondering about the converse of this fact: Question: Let $\gamma:\Bbb R\to M(n,\Bbb R)$ be a smooth curve with $\gamma(0)=I$ and suppose that   $$\gamma(t)^{-1}\frac{d\gamma}{dt}(t)\in\mathfrak{g},\quad\forall t\in\Bbb R.$$   Does $\gamma(t)\in G$ for all $t\in\Bbb R$? If true, it would have the following implication: Let $\alpha:\Bbb R\to\mathfrak{g}$ be a smooth curve and consider the initial value problem $$\frac{d\gamma}{dt}(t)=\gamma(t)\alpha(t),\quad \gamma(0)=I.$$ Since it is a first order linear system of ODEs, it has a unique solution $$\gamma: \Bbb R\to M(n,\Bbb R),$$ and in fact $\gamma(t)\in G$ for all $t\in\Bbb R$.","Let $G$ be a Lie subgroup of $GL(n,\Bbb R)$ and $\mathfrak{g}\subseteq M(n,\Bbb R)$ its Lie algebra. Suppose that we have a smooth curve $$\gamma:\Bbb R\to G$$ with $\gamma(0)=I$. Then, it induces a curve on the Lie algebra $$\alpha:\Bbb R\to \mathfrak{g},\quad \alpha(t)=\gamma(t)^{-1}\frac{d\gamma}{dt}(t).$$ I am wondering about the converse of this fact: Question: Let $\gamma:\Bbb R\to M(n,\Bbb R)$ be a smooth curve with $\gamma(0)=I$ and suppose that   $$\gamma(t)^{-1}\frac{d\gamma}{dt}(t)\in\mathfrak{g},\quad\forall t\in\Bbb R.$$   Does $\gamma(t)\in G$ for all $t\in\Bbb R$? If true, it would have the following implication: Let $\alpha:\Bbb R\to\mathfrak{g}$ be a smooth curve and consider the initial value problem $$\frac{d\gamma}{dt}(t)=\gamma(t)\alpha(t),\quad \gamma(0)=I.$$ Since it is a first order linear system of ODEs, it has a unique solution $$\gamma: \Bbb R\to M(n,\Bbb R),$$ and in fact $\gamma(t)\in G$ for all $t\in\Bbb R$.",,"['ordinary-differential-equations', 'differential-geometry', 'lie-groups', 'lie-algebras']"
59,Matrix Differential equation x'(t) = Ax(t)+b solution defined for non-invertible values,Matrix Differential equation x'(t) = Ax(t)+b solution defined for non-invertible values,,"The problem I am trying to find a solution for is this linear ODE, where A is an $n x n$ matrix and b is an $nx1$ matrix. $$x'(t) = Ax(t) + b$$ One solution for this equation is provided by a wikipedia article: https://en.wikipedia.org/wiki/Matrix_differential_equation#Stability_and_steady_state_of_the_matrix_system $$x^* = -A^{-1} b$$ $$x(t) = x^* + e^{At}(x(0) - x^*)$$ The biggest trouble I am running into is that, in my case, $A$ may be singular. In which case, this solution won't work. One thing I noticed: if this were a scalar problem, not a matrix problem, and A were non-invertible, than A would have to be zero. If I set A to zero in my original equation, I get the following. $$x'(t) = 0x(t) + b$$ $$x'(t) = b$$ $$x(t) = bt + x(0)$$ This tells me that the solution I seek should resolve to $x(t) = bt + x(0)$ if $A$ is the zero matrix. How can I determine the solution to this equation when $A$ is singular, but not zero? Is there a way to take the limit of $x(t)=x^∗+e^{At}(x(0)−x^∗)$ so that it is defined when $A$ is singular?","The problem I am trying to find a solution for is this linear ODE, where A is an $n x n$ matrix and b is an $nx1$ matrix. $$x'(t) = Ax(t) + b$$ One solution for this equation is provided by a wikipedia article: https://en.wikipedia.org/wiki/Matrix_differential_equation#Stability_and_steady_state_of_the_matrix_system $$x^* = -A^{-1} b$$ $$x(t) = x^* + e^{At}(x(0) - x^*)$$ The biggest trouble I am running into is that, in my case, $A$ may be singular. In which case, this solution won't work. One thing I noticed: if this were a scalar problem, not a matrix problem, and A were non-invertible, than A would have to be zero. If I set A to zero in my original equation, I get the following. $$x'(t) = 0x(t) + b$$ $$x'(t) = b$$ $$x(t) = bt + x(0)$$ This tells me that the solution I seek should resolve to $x(t) = bt + x(0)$ if $A$ is the zero matrix. How can I determine the solution to this equation when $A$ is singular, but not zero? Is there a way to take the limit of $x(t)=x^∗+e^{At}(x(0)−x^∗)$ so that it is defined when $A$ is singular?",,"['calculus', 'ordinary-differential-equations', 'limits']"
60,Can all differential equations be solved using power series?,Can all differential equations be solved using power series?,,"To elaborate, does the differential equation have to be of some form to be solvable by power series.  More specifically, if I wanted to solve this equation by power series would I be able to? $$ (1-x^2)y''(x)-3xy'(x)+n(n+2)y(x)=0 $$ Also, $n$ is a constant in the stated equation.","To elaborate, does the differential equation have to be of some form to be solvable by power series.  More specifically, if I wanted to solve this equation by power series would I be able to? $$ (1-x^2)y''(x)-3xy'(x)+n(n+2)y(x)=0 $$ Also, $n$ is a constant in the stated equation.",,"['ordinary-differential-equations', 'power-series']"
61,"Finding the ""lost"" and ""spurious'' solutions to an ODE","Finding the ""lost"" and ""spurious'' solutions to an ODE",,"Consider the differential equation:   $$ {dy \over dx}=x(y-2)^{1 \over 2} .$$ We can solve the equation by separating the variables. \begin{aligned}   {dy \over dx}&=x(y-2)^{1 \over 2}\\   dy&=x\sqrt{y-2}dx\\   \int (y-2)^{-{1 \over 2}} dy&=\int x dx\\   2(y-2)^{1 \over 2}&={x^2 \over 2}+C\\  (y-2)^{1 \over 2}&={x^2 \over 4}+{C \over 2}\\  y-2&=\left({x^2 \over 4}+{C \over 2}\right)^2\\  y&=\left({x^2 \over 4}+{C \over 2}\right)^2+2\\  \end{aligned} Then, my question is: What are the 'lost' solutions and the 'spurious' solutions?","Consider the differential equation:   $$ {dy \over dx}=x(y-2)^{1 \over 2} .$$ We can solve the equation by separating the variables. \begin{aligned}   {dy \over dx}&=x(y-2)^{1 \over 2}\\   dy&=x\sqrt{y-2}dx\\   \int (y-2)^{-{1 \over 2}} dy&=\int x dx\\   2(y-2)^{1 \over 2}&={x^2 \over 2}+C\\  (y-2)^{1 \over 2}&={x^2 \over 4}+{C \over 2}\\  y-2&=\left({x^2 \over 4}+{C \over 2}\right)^2\\  y&=\left({x^2 \over 4}+{C \over 2}\right)^2+2\\  \end{aligned} Then, my question is: What are the 'lost' solutions and the 'spurious' solutions?",,"['calculus', 'integration', 'ordinary-differential-equations']"
62,Why Hartman-Grobman theorem does not work when one of the eigenvalues is purely imaginary?,Why Hartman-Grobman theorem does not work when one of the eigenvalues is purely imaginary?,,I would be very grateful if someone clever explained to me why Hartman-Grobman theorem does not work when one of the eigenvalues of linearized system is purely imaginary? Is there any intuition behind this?,I would be very grateful if someone clever explained to me why Hartman-Grobman theorem does not work when one of the eigenvalues of linearized system is purely imaginary? Is there any intuition behind this?,,"['ordinary-differential-equations', 'dynamical-systems']"
63,Phase portrait with one eigenvalue equal to zero?,Phase portrait with one eigenvalue equal to zero?,,"In my differential equations classes this semester we have been learning how to sketch phase portraits given a solution to a system of equations including eigenvalues and eigenvectors. The cases we have learnt are Real and distinct eigenvalues (nodal sink, source or saddle depending on signs) Repeated eigenvalues (proper or improper node depending on the number of eigenvectors) Purely complex (ellipses) And complex with a real part (spiral) So you can see they haven't taught us about zero eigenvalues. But I'd like to know what the general form of the phase portrait would look like in the case that there was a zero eigenvalue. Is the a general case like the above? If so, what is it?","In my differential equations classes this semester we have been learning how to sketch phase portraits given a solution to a system of equations including eigenvalues and eigenvectors. The cases we have learnt are Real and distinct eigenvalues (nodal sink, source or saddle depending on signs) Repeated eigenvalues (proper or improper node depending on the number of eigenvectors) Purely complex (ellipses) And complex with a real part (spiral) So you can see they haven't taught us about zero eigenvalues. But I'd like to know what the general form of the phase portrait would look like in the case that there was a zero eigenvalue. Is the a general case like the above? If so, what is it?",,"['ordinary-differential-equations', 'eigenvalues-eigenvectors']"
64,Separable Solutions vs Integrating Factors,Separable Solutions vs Integrating Factors,,"I am looking to better understand the key differences between these two methods of solving ODEs. I know they are very different, but I will give an example to show what I am a bit confused about. For example, in one of my previous problems I had $$\frac{dQ}{dt}=3-0.7Q$$ Now, when I first saw this, It appeared to be linear to me. I thought maybe the best way to solve it would be to rewrite it as $$\frac{dQ}{dt}+0.7Q=3$$ and then use the 0.7 to solve for the integrating factor. However, I was told that the correct answer is to look for separable solutions. Which I also understand how to do. But I am just a bit worried that I will try to solve something by using integrating factors when separation of variables should be used. Is this the case? How can I know? Thank you all!","I am looking to better understand the key differences between these two methods of solving ODEs. I know they are very different, but I will give an example to show what I am a bit confused about. For example, in one of my previous problems I had Now, when I first saw this, It appeared to be linear to me. I thought maybe the best way to solve it would be to rewrite it as and then use the 0.7 to solve for the integrating factor. However, I was told that the correct answer is to look for separable solutions. Which I also understand how to do. But I am just a bit worried that I will try to solve something by using integrating factors when separation of variables should be used. Is this the case? How can I know? Thank you all!",\frac{dQ}{dt}=3-0.7Q \frac{dQ}{dt}+0.7Q=3,['ordinary-differential-equations']
65,Are there functions that are not of exponential order for which you can define a Laplace transform?,Are there functions that are not of exponential order for which you can define a Laplace transform?,,"I'am in a course of Introduction to Linear Differential Equations and teacher made us this question in class. we work in $\mathbb{R}$, and any help to answer this is welcome","I'am in a course of Introduction to Linear Differential Equations and teacher made us this question in class. we work in $\mathbb{R}$, and any help to answer this is welcome",,"['ordinary-differential-equations', 'laplace-transform']"
66,what can I say about the solution $y(x)$ of the ODE?,what can I say about the solution  of the ODE?,y(x),"Let $y:\mathbb R\to \mathbb R$ be differentiable and satisfy the ODE: $$\frac{dy}{dx} =f(y),x\in\mathbb R$$ $$y(0)=y(1)=0$$ where $f:\mathbb R\to \mathbb R$ is a Lipschitz continuous function. Then $y(x)=0$ if and only if $x\in\ ${$0,1$} $y$ is bounded $y$ is strictly increasing $\frac{dy}{dx}$ is unbounded. I've got this question from an exam paper, and I cannot understand how to solve it. How should I use the condition of Lipschitz continuity of $f$ to solve it? Please help. Thanks in advance.","Let $y:\mathbb R\to \mathbb R$ be differentiable and satisfy the ODE: $$\frac{dy}{dx} =f(y),x\in\mathbb R$$ $$y(0)=y(1)=0$$ where $f:\mathbb R\to \mathbb R$ is a Lipschitz continuous function. Then $y(x)=0$ if and only if $x\in\ ${$0,1$} $y$ is bounded $y$ is strictly increasing $\frac{dy}{dx}$ is unbounded. I've got this question from an exam paper, and I cannot understand how to solve it. How should I use the condition of Lipschitz continuity of $f$ to solve it? Please help. Thanks in advance.",,['ordinary-differential-equations']
67,Why does the midpoint method have error $O(h^2)$ [duplicate],Why does the midpoint method have error  [duplicate],O(h^2),"This question already has answers here : Numerical Approximation of Differential Equations with Midpoint Method (3 answers) Closed 9 years ago . In solving an ode $$ y'(t) = f(t, y(t)), \quad y(t_0) = y_0 $$ the midpoint method estimates $$y_{n+1} = y_n + hf\left(t_n+\frac{h}{2},y_n+\frac{h}{2}f(t_n, y_n)\right)$$ But why is the error $O(h^2)$? The estimate for Euler's method is straightforward since it's just the Taylor approximation, but here it doesn't look like a Taylor approximation.","This question already has answers here : Numerical Approximation of Differential Equations with Midpoint Method (3 answers) Closed 9 years ago . In solving an ode $$ y'(t) = f(t, y(t)), \quad y(t_0) = y_0 $$ the midpoint method estimates $$y_{n+1} = y_n + hf\left(t_n+\frac{h}{2},y_n+\frac{h}{2}f(t_n, y_n)\right)$$ But why is the error $O(h^2)$? The estimate for Euler's method is straightforward since it's just the Taylor approximation, but here it doesn't look like a Taylor approximation.",,"['ordinary-differential-equations', 'numerical-methods']"
68,What is the purpose of Wronskian (linear independence/variation of parameters)?,What is the purpose of Wronskian (linear independence/variation of parameters)?,,"So as I understand, the Wronskian determinant can be used to show linear independence. Why is that? Also, how does this fit into understanding variation of parameters for solving a differential equation?","So as I understand, the Wronskian determinant can be used to show linear independence. Why is that? Also, how does this fit into understanding variation of parameters for solving a differential equation?",,"['linear-algebra', 'ordinary-differential-equations']"
69,Variation of parameters: $3y''+4y'+4=(\sin(t))e^{-t}$,Variation of parameters:,3y''+4y'+4=(\sin(t))e^{-t},"Solve the following initial-value problem: $$3y''+4y'+4=(\sin(t))e^{-t}$$ where $y(0)=1, y'(0)=0$ Here are my steps: I started out with the homogeneous equation: $$3y''+4y'+4=0$$ and found the roots to be $\frac{-1}{3}$ and $-1$ So that means two of the solutions are: $y_1(t)=e^{\frac{-t}{3}}$ $y_2(t)=e^{-t}$ I was about to use the wronskian but then I just decided to overlook my book and realize that the author put a note saying: How to make a note that the coefficient of $y''$ is 3. How will that affect me?","Solve the following initial-value problem: $$3y''+4y'+4=(\sin(t))e^{-t}$$ where $y(0)=1, y'(0)=0$ Here are my steps: I started out with the homogeneous equation: $$3y''+4y'+4=0$$ and found the roots to be $\frac{-1}{3}$ and $-1$ So that means two of the solutions are: $y_1(t)=e^{\frac{-t}{3}}$ $y_2(t)=e^{-t}$ I was about to use the wronskian but then I just decided to overlook my book and realize that the author put a note saying: How to make a note that the coefficient of $y''$ is 3. How will that affect me?",,['ordinary-differential-equations']
70,How could we show that $u=0$?,How could we show that ?,u=0,"In my notes there is the following example about the energy method. We want to show that the problem $$w_{tt}(x, t)-w_{xxtt}(x, t)-w_{xx}(x, t)=f(x, t), 0<x<1, t>0$$  $$w(x, 0)=\phi(x) \\ w_t(x, 0)=\psi(x) \\ w_x(0, t)=h(t), t>0 \\ w_x(1, t)=g(t)$$  has an unique solution. We suppose that $w_1, w_2$ are two distinct solutions. Then $u=w_1-w_2$ solves the problem :  $$u_{tt}(x, t)-u_{xxtt}(x, t)-u_{xx}(x, t)=0, \, 0<x<1, \, t>0$$ $$u(x, 0)=0 \\ u_t(x, 0)=0 \\ u_x(0, t)=0 \\ u_x(1, t)=0$$ $$$$ To find the energy we do the following: $$\int_0^1(u_tu_{tt}-u_tu_{xxtt}-u_tu_{xx})dx=0 \tag 1$$ $$\int_0^1 u_tu_{tt}dx=\int_0^1\frac{1}{2}(u_t^2)_tdx=\frac{d}{dt}\int_0^1 \frac{1}{2}u_t^2dx$$ $$\int_0^1 u_t u_{xxtt}dx=-\int_0^1 u_{tx}u_{xtt}dx+[u_t u_{xtt}]_0^1=-\int_0^1\frac{1}{2}(u_{tx}^2)_tdx$$ $$\int_0^1 u_t u_{xx}dx=-\int_0^1 u_{tx}u_x dx+[u_t u_x]_0^1=-\frac{1}{2} \frac{d}{dt} \int_0^1 u_x^2dx$$ $$(1) \Rightarrow \frac{d}{dt}\int_0^1 \frac{1}{2}u_t^2dx+\frac{d}{dt}\frac{1}{2}\int_0^1 u_{tx}^2dx+\frac{d}{dt} \frac{1}{2} \int_0^1 u_x^2dx=0$$ The energy of the system is $$E(t)=\frac{1}{2}\int_0^1 (u_t^2(x, t)+u_{tx}^2(x, t)+u_{x}^2(x, t))dx$$ $$\Rightarrow E'(t)=\int_0^1 (u_t u_{tt}+u_{tx}u_{xtt}+u_xu_{xy})dx= \dots =0$$ (The energy is always positive.) $$\Rightarrow 0 \leq E(t) = E(0)=0 \Rightarrow u_t=0 \Rightarrow u(x, t)=u(x, 0)=0 \text{ Contradiction}$$ So,  the initial problem has an inuque solution. $$$$ $$$$ I wanted to apply this at the following : $$v_{tt}(x, t)-v_{xt}(x, t)=f(x, t), x \in \mathbb{R}, t>0 \\ v(x, 0)=g(x), x \in \mathbb{R} \\ v_t(x, 0)=h(x), x \in \mathbb{R}$$ and I have done the following: We suppose that $v_1, v_2$ are two distinct solutions. Then $u=v_1-v_2$ solve the problem:  $$u_{tt}-u_{xt}=0, x \in \mathbb{R}, t>0 \\ v(x, 0)=0, x \in \mathbb{R} \\ v_t=0, x \in \mathbb{R}$$  In this case since $x\in \mathbb{R}$ we are looking for the characteristic curves to use them as the limits of the integral. $$u_{tt}-u_{xt}=f $$ The characteristic curves are $$x=x_0 \text{ AND } x+t=x_0+t_0$$ $$\int_{x_0}^{x_0+t_0-t}(u_tu_{tt}-u_tu_{xt})dx=0 \tag 2$$ $$\int_{x_0}^{x_0+t_0-t}u_tu_{tt}dx=\int_{x_0}^{x_0+t_0-t}\frac{\partial}{\partial{t}}\left (\frac{1}{2}u_t^2\right )dx=\frac{d}{dt}\int_{x_0}^{x_0+t_0-t}\frac{1}{2}u_t^2dx$$ \begin{align}  \int_{x_0}^{x_0+t_0-t}u_tu_{xt}dx &= \int_{x_0}^{x_0+t_0-t}u_t\left [\frac{\partial}{\partial{x}}u_t\right ]dx=[u_t^2]_{x=x_0}^{x_0+t_0-t}-\int_{x_0}^{x_0+t_0-t}u_{xt}u_tdx \\ &\Rightarrow \int_{x_0}^{x_0+t_0-t}u_tu_{xt}dx=\frac{1}{2}\left (u_t^2(x_0+t_0-t, t)-u_t^2(x_0, t)\right ) \end{align} $$(2) \Rightarrow \frac{d}{dt}\int_{x_0}^{x_0+t_0-t}\frac{1}{2}u_t^2dx-\frac{1}{2}\left (u_t^2(x_0+t_0-t, t)-u_t^2(x_0, t)\right )=0 \Rightarrow \frac{d}{dt}\int_{x_0}^{x_0+t_0-t}\frac{1}{2}u_t^2dx=\frac{1}{2}\left (u_t^2(x_0+t_0-t, t)-u_t^2(x_0, t)\right )$$ The energy of the system is $$E(t)=\int_{x_0}^{x_0+t_0-t}\frac{1}{2}u_t^2dx$$ $$E'(t)=\frac{d}{dt}\int_{x_0}^{x_0+t_0-t}\frac{1}{2}u_t^2dx \Rightarrow E'(t)=\frac{1}{2}\left (u_t^2(x_0+t_0-t, t)-u_t^2(x_0, t)\right )$$ We have also that $E(0)=0, E'(0)=0$, right?? How could we continue to show that $u=0$ ?? We could show that if we would know that $E'(t) \leq 0$, but how could we get this inequality??","In my notes there is the following example about the energy method. We want to show that the problem $$w_{tt}(x, t)-w_{xxtt}(x, t)-w_{xx}(x, t)=f(x, t), 0<x<1, t>0$$  $$w(x, 0)=\phi(x) \\ w_t(x, 0)=\psi(x) \\ w_x(0, t)=h(t), t>0 \\ w_x(1, t)=g(t)$$  has an unique solution. We suppose that $w_1, w_2$ are two distinct solutions. Then $u=w_1-w_2$ solves the problem :  $$u_{tt}(x, t)-u_{xxtt}(x, t)-u_{xx}(x, t)=0, \, 0<x<1, \, t>0$$ $$u(x, 0)=0 \\ u_t(x, 0)=0 \\ u_x(0, t)=0 \\ u_x(1, t)=0$$ $$$$ To find the energy we do the following: $$\int_0^1(u_tu_{tt}-u_tu_{xxtt}-u_tu_{xx})dx=0 \tag 1$$ $$\int_0^1 u_tu_{tt}dx=\int_0^1\frac{1}{2}(u_t^2)_tdx=\frac{d}{dt}\int_0^1 \frac{1}{2}u_t^2dx$$ $$\int_0^1 u_t u_{xxtt}dx=-\int_0^1 u_{tx}u_{xtt}dx+[u_t u_{xtt}]_0^1=-\int_0^1\frac{1}{2}(u_{tx}^2)_tdx$$ $$\int_0^1 u_t u_{xx}dx=-\int_0^1 u_{tx}u_x dx+[u_t u_x]_0^1=-\frac{1}{2} \frac{d}{dt} \int_0^1 u_x^2dx$$ $$(1) \Rightarrow \frac{d}{dt}\int_0^1 \frac{1}{2}u_t^2dx+\frac{d}{dt}\frac{1}{2}\int_0^1 u_{tx}^2dx+\frac{d}{dt} \frac{1}{2} \int_0^1 u_x^2dx=0$$ The energy of the system is $$E(t)=\frac{1}{2}\int_0^1 (u_t^2(x, t)+u_{tx}^2(x, t)+u_{x}^2(x, t))dx$$ $$\Rightarrow E'(t)=\int_0^1 (u_t u_{tt}+u_{tx}u_{xtt}+u_xu_{xy})dx= \dots =0$$ (The energy is always positive.) $$\Rightarrow 0 \leq E(t) = E(0)=0 \Rightarrow u_t=0 \Rightarrow u(x, t)=u(x, 0)=0 \text{ Contradiction}$$ So,  the initial problem has an inuque solution. $$$$ $$$$ I wanted to apply this at the following : $$v_{tt}(x, t)-v_{xt}(x, t)=f(x, t), x \in \mathbb{R}, t>0 \\ v(x, 0)=g(x), x \in \mathbb{R} \\ v_t(x, 0)=h(x), x \in \mathbb{R}$$ and I have done the following: We suppose that $v_1, v_2$ are two distinct solutions. Then $u=v_1-v_2$ solve the problem:  $$u_{tt}-u_{xt}=0, x \in \mathbb{R}, t>0 \\ v(x, 0)=0, x \in \mathbb{R} \\ v_t=0, x \in \mathbb{R}$$  In this case since $x\in \mathbb{R}$ we are looking for the characteristic curves to use them as the limits of the integral. $$u_{tt}-u_{xt}=f $$ The characteristic curves are $$x=x_0 \text{ AND } x+t=x_0+t_0$$ $$\int_{x_0}^{x_0+t_0-t}(u_tu_{tt}-u_tu_{xt})dx=0 \tag 2$$ $$\int_{x_0}^{x_0+t_0-t}u_tu_{tt}dx=\int_{x_0}^{x_0+t_0-t}\frac{\partial}{\partial{t}}\left (\frac{1}{2}u_t^2\right )dx=\frac{d}{dt}\int_{x_0}^{x_0+t_0-t}\frac{1}{2}u_t^2dx$$ \begin{align}  \int_{x_0}^{x_0+t_0-t}u_tu_{xt}dx &= \int_{x_0}^{x_0+t_0-t}u_t\left [\frac{\partial}{\partial{x}}u_t\right ]dx=[u_t^2]_{x=x_0}^{x_0+t_0-t}-\int_{x_0}^{x_0+t_0-t}u_{xt}u_tdx \\ &\Rightarrow \int_{x_0}^{x_0+t_0-t}u_tu_{xt}dx=\frac{1}{2}\left (u_t^2(x_0+t_0-t, t)-u_t^2(x_0, t)\right ) \end{align} $$(2) \Rightarrow \frac{d}{dt}\int_{x_0}^{x_0+t_0-t}\frac{1}{2}u_t^2dx-\frac{1}{2}\left (u_t^2(x_0+t_0-t, t)-u_t^2(x_0, t)\right )=0 \Rightarrow \frac{d}{dt}\int_{x_0}^{x_0+t_0-t}\frac{1}{2}u_t^2dx=\frac{1}{2}\left (u_t^2(x_0+t_0-t, t)-u_t^2(x_0, t)\right )$$ The energy of the system is $$E(t)=\int_{x_0}^{x_0+t_0-t}\frac{1}{2}u_t^2dx$$ $$E'(t)=\frac{d}{dt}\int_{x_0}^{x_0+t_0-t}\frac{1}{2}u_t^2dx \Rightarrow E'(t)=\frac{1}{2}\left (u_t^2(x_0+t_0-t, t)-u_t^2(x_0, t)\right )$$ We have also that $E(0)=0, E'(0)=0$, right?? How could we continue to show that $u=0$ ?? We could show that if we would know that $E'(t) \leq 0$, but how could we get this inequality??",,"['ordinary-differential-equations', 'partial-differential-equations']"
71,Exact solution for ODE: $yy' + y + f(x) = 0$,Exact solution for ODE:,yy' + y + f(x) = 0,"Is there is exactly solution for ODE in the form: $yy'+y+f(x)=0$.  Thanks. If there is no such solution for general $f$, does it ease the problem if $f(x)=Ax+B$ for some constants $A$ and $B$? Clarification: solutions below are just particular solution.  What I was looking for is the general solution that works for arbitrary initial condition.","Is there is exactly solution for ODE in the form: $yy'+y+f(x)=0$.  Thanks. If there is no such solution for general $f$, does it ease the problem if $f(x)=Ax+B$ for some constants $A$ and $B$? Clarification: solutions below are just particular solution.  What I was looking for is the general solution that works for arbitrary initial condition.",,['ordinary-differential-equations']
72,Differential equation solved as a separable equation and the solution y=constant,Differential equation solved as a separable equation and the solution y=constant,,"I have a doubt in the definition of the solution of the differential equations in general. For this question we can use the following differential equation: $$ y'(x)=3x^2y^2 $$ We can solve it as a separable equation, where: $$ y'(x)=a(x)b(y) $$ And in our case: $$ a(x)=3x^3, b(y)=y^2 $$ Now, when I solve the differential equation I get: $$ \int \frac{1}{y^2}dy=\int3x^2dx \Rightarrow -\frac{1}{y(x)}=x^3+C \Rightarrow y(x)=-\frac{1}{x^3+C} $$ But if we go back to the original differential equation we can see that $ b(0)=0 $  and $y'(0)=0$. So that is it right to say that the differential equation solutions are: $$ y(x)=-\frac{1}{x^3+C};y(x)=0 $$ I have this doubt because on some books this step is important but for example, when I check my exercises on Wolfram Alpha the ""constant solution"" is ignored.","I have a doubt in the definition of the solution of the differential equations in general. For this question we can use the following differential equation: $$ y'(x)=3x^2y^2 $$ We can solve it as a separable equation, where: $$ y'(x)=a(x)b(y) $$ And in our case: $$ a(x)=3x^3, b(y)=y^2 $$ Now, when I solve the differential equation I get: $$ \int \frac{1}{y^2}dy=\int3x^2dx \Rightarrow -\frac{1}{y(x)}=x^3+C \Rightarrow y(x)=-\frac{1}{x^3+C} $$ But if we go back to the original differential equation we can see that $ b(0)=0 $  and $y'(0)=0$. So that is it right to say that the differential equation solutions are: $$ y(x)=-\frac{1}{x^3+C};y(x)=0 $$ I have this doubt because on some books this step is important but for example, when I check my exercises on Wolfram Alpha the ""constant solution"" is ignored.",,"['calculus', 'ordinary-differential-equations']"
73,Solve $y' = \frac{1}{2}\sqrt{x} + \sqrt[3]{y}$,Solve,y' = \frac{1}{2}\sqrt{x} + \sqrt[3]{y},Please help with this $$y' = \frac{1}{2}\sqrt{x} + \sqrt[3]{y}$$ Tried making $t=\sqrt[3]{y}$. Then $3t^{2}t'_{x} = \frac{1}{2}x^\frac{1}{2} + t$. $p=t'$. And then expressed $x$ and differentiated with respect to $t$. But can't see solution.,Please help with this $$y' = \frac{1}{2}\sqrt{x} + \sqrt[3]{y}$$ Tried making $t=\sqrt[3]{y}$. Then $3t^{2}t'_{x} = \frac{1}{2}x^\frac{1}{2} + t$. $p=t'$. And then expressed $x$ and differentiated with respect to $t$. But can't see solution.,,['ordinary-differential-equations']
74,"Finding General Solutions to 2nd Order Differential Equations, Am I on the right track?","Finding General Solutions to 2nd Order Differential Equations, Am I on the right track?",,"I'm studying Differential Equations, and I'm working on figuring out how to solve using Undetermined Coefficients.  We've been assured that on the test we will only have to solve equations with a 2nd order at most.  That being said, I'm trying to figure out what my general solutions should look like for each possible combination of roots. I've read through a lot of very verbose generalized general solutions, that I'm having a lot of trouble really comprehending.  Here's what I've taken from them. Two Real Roots (non-repeating) $$ Y_c = c_1e^{m_1x}+c_2e^{m_2x} $$ Repeated Real Roots $$ Y_c = c_1e^{x}+c_2xe^{x} $$ Imaginary Roots (I'm really lost on this one) $$ Y_c = c_1\cos+c_2\sin $$ My Questions is this: Am I on the right track with the first two? What am I doing wrong with the last one? *[We're using Dennis G Zill's A First Course in Differential Equations ]*","I'm studying Differential Equations, and I'm working on figuring out how to solve using Undetermined Coefficients.  We've been assured that on the test we will only have to solve equations with a 2nd order at most.  That being said, I'm trying to figure out what my general solutions should look like for each possible combination of roots. I've read through a lot of very verbose generalized general solutions, that I'm having a lot of trouble really comprehending.  Here's what I've taken from them. Two Real Roots (non-repeating) $$ Y_c = c_1e^{m_1x}+c_2e^{m_2x} $$ Repeated Real Roots $$ Y_c = c_1e^{x}+c_2xe^{x} $$ Imaginary Roots (I'm really lost on this one) $$ Y_c = c_1\cos+c_2\sin $$ My Questions is this: Am I on the right track with the first two? What am I doing wrong with the last one? *[We're using Dennis G Zill's A First Course in Differential Equations ]*",,['ordinary-differential-equations']
75,Why Runge-Kutta methods cannot find the solution of Lorenz system?,Why Runge-Kutta methods cannot find the solution of Lorenz system?,,"The solution of the following Lorenz system s=10; r=28; b=8/3; f = @(t,y) [-s*y(1)+s*y(2); -y(1)*y(3)+r*y(1)-y(2); y(1)*y(2)-b*y(3)]; in the interval $[0,8]$ with initial values $y_1(0)=1$,$y_2(0)=0$,$y_3(0)=0$ using MATLAB ode45() function are >> [t y] = ode45(f,[0 8],[1 0 0]); >> [t(end) y(end,:)] ans =     8.0000   -7.3560   -5.6838   27.8372 But the results are very poor with RK4 even with one million sub-interval (i.e. h=(8-0)/1e6 ) ans =      8.0000   -7.4199   -5.6639   28.0052 My questions are: Why the results are different? Is ode45() the best function for Lorenz system? How can I improve the RK4 accuracy? Is Lorenz system an stiff equation ?","The solution of the following Lorenz system s=10; r=28; b=8/3; f = @(t,y) [-s*y(1)+s*y(2); -y(1)*y(3)+r*y(1)-y(2); y(1)*y(2)-b*y(3)]; in the interval $[0,8]$ with initial values $y_1(0)=1$,$y_2(0)=0$,$y_3(0)=0$ using MATLAB ode45() function are >> [t y] = ode45(f,[0 8],[1 0 0]); >> [t(end) y(end,:)] ans =     8.0000   -7.3560   -5.6838   27.8372 But the results are very poor with RK4 even with one million sub-interval (i.e. h=(8-0)/1e6 ) ans =      8.0000   -7.4199   -5.6639   28.0052 My questions are: Why the results are different? Is ode45() the best function for Lorenz system? How can I improve the RK4 accuracy? Is Lorenz system an stiff equation ?",,"['ordinary-differential-equations', 'numerical-methods']"
76,General solution to $f^{(n)}=f$ but $f^{(k)}\ne f$ for $k<n$,General solution to  but  for,f^{(n)}=f f^{(k)}\ne f k<n,We know that $$\frac{d}{dx}e^x=e^x$$ and $$\frac{d^4}{dx^4}\sin(x)=\sin(x)$$ What is the general solution $f$ to $$\begin{equation} \begin{split} \frac{d^n}{dx^n}f(x)&=f(x) \\ \frac{d^k}{dx^k}f(x)&\ne f(x)\quad \mathrm{for}\>\>k<n \quad? \end{split} \end{equation}$$,We know that $$\frac{d}{dx}e^x=e^x$$ and $$\frac{d^4}{dx^4}\sin(x)=\sin(x)$$ What is the general solution $f$ to $$\begin{equation} \begin{split} \frac{d^n}{dx^n}f(x)&=f(x) \\ \frac{d^k}{dx^k}f(x)&\ne f(x)\quad \mathrm{for}\>\>k<n \quad? \end{split} \end{equation}$$,,['calculus']
77,"Ordinary differential equations of the form $M(x,y)dx+N(x,y)dy=0$ question",Ordinary differential equations of the form  question,"M(x,y)dx+N(x,y)dy=0","An ODE of the form $M(x,y)dx+N(x,y)dy=0$ is called ""good"" if $\frac{\partial (M(x,y))}{\partial y}=\frac{\partial (N(x,y))}{\partial x}$ We are given the differential equation $(3x^2y+2xy+y^3)dx+(x^2+y^2)dy=0$. This ODE is not ""good"". We are asked to find $\mu (x,y)$ such that: $$\mu (x,y)(3x^2y+2xy+y^3)dx+\mu (x,y)(x^2+y^2)dy=0,  (*)$$  is ""good"". What I did: if the equation $(*)$ is good then $\mu_y (x,y) M(x,y)+\mu (x,y)M_y (x,y)=\mu_x (x,y)N(x,y)+\mu (x,y)N_x (x,y)$ so we get $\mu_y(x,y)(3x^2y+2xy+y^3)+\mu(x,y)(3x^2+2x+3y^2)=\mu_x(x,y) (x^2+y^2)+2x \mu(x,y)$ And now I'm stuck. Even if we were to guess $\mu_x(x,y)=0$ or $\mu_y(x,y)=0$ we will never get something like $\frac{\mu_y}{\mu}=\phi(y)$ or $\frac{\mu_x}{\mu}=\psi(x)$. $\mu$ seems to depend on both variables and unless the above restrictions apply (which they don't here) I don't know how to find $\mu$. Please help.","An ODE of the form $M(x,y)dx+N(x,y)dy=0$ is called ""good"" if $\frac{\partial (M(x,y))}{\partial y}=\frac{\partial (N(x,y))}{\partial x}$ We are given the differential equation $(3x^2y+2xy+y^3)dx+(x^2+y^2)dy=0$. This ODE is not ""good"". We are asked to find $\mu (x,y)$ such that: $$\mu (x,y)(3x^2y+2xy+y^3)dx+\mu (x,y)(x^2+y^2)dy=0,  (*)$$  is ""good"". What I did: if the equation $(*)$ is good then $\mu_y (x,y) M(x,y)+\mu (x,y)M_y (x,y)=\mu_x (x,y)N(x,y)+\mu (x,y)N_x (x,y)$ so we get $\mu_y(x,y)(3x^2y+2xy+y^3)+\mu(x,y)(3x^2+2x+3y^2)=\mu_x(x,y) (x^2+y^2)+2x \mu(x,y)$ And now I'm stuck. Even if we were to guess $\mu_x(x,y)=0$ or $\mu_y(x,y)=0$ we will never get something like $\frac{\mu_y}{\mu}=\phi(y)$ or $\frac{\mu_x}{\mu}=\psi(x)$. $\mu$ seems to depend on both variables and unless the above restrictions apply (which they don't here) I don't know how to find $\mu$. Please help.",,"['calculus', 'integration', 'ordinary-differential-equations', 'derivatives']"
78,Finding eigenfunctions and eigenvalues to Sturm-Liouville operator,Finding eigenfunctions and eigenvalues to Sturm-Liouville operator,,"I'm struggling to understand how to find the associated eigenfunctions and eigenvalues of a differential operator in Sturm-Liouville form. For instance, one question that I am trying to solve is the following: By substituting $x=e^{t}$ find the eigenfunctions and eigenvalues of the operator $\hat{\mathcal{L}}$, defined by: $$\hat{\mathcal{L}}[y(x)]=x^{2}y''(x)+2xy'(x)+\frac{1}{4}y(x)$$  With boundary conditions $y(1)=y(e)=0$ I can set up the eigensystem: $$x^2 y_{n}''(x)+2xy'_{n}(x)+\frac{1}{4}y_{n}(x)=\lambda_{n}y_{n}(x)$$ In Sturm-Lioville form, we have: $$\frac{\mathrm{d}}{\mathrm{d}x}\left(x^{2}\frac{\mathrm{d}y_{n}}{\mathrm{d}x}\right)+\frac{1}{4}y_{n}(x)=\lambda_{n}y_{n}(x)$$ However, I'm not sure how to solve this equation as Rodriguez' conditions are not met and if I attempt an ansatz series solution: $y_{n}=\sum_{n=0}^{\infty}\alpha_{n}x^{n}$, I end up with the following equation: $$\sum_{n=0}^{\infty}\alpha_{n}\cdot n(n-1)x^{n}+\sum_{n=0}^{\infty}2\alpha_{n}\cdot n x^{n}+\sum_{n=0}^{\infty}\frac{\alpha_{n}}{4}x^{n}=\sum_{n=0}^{\infty}\lambda \alpha_{n}x^{n} $$ However, this does not give me a recursion relationship as I would expect, instead I simply get: $$\lambda = k^{2} + k + \frac{1}{4} \qquad \exists k \in \mathbb{N} \cup \{0\}$$ I tried substituting $x=e^{t}$, giving me: $$\frac{\mathrm{d}^{2}}{\mathrm{d}t^{2}}\left(y_{n}(e^{t})\right)+\frac{1}{4}y_{n}(e^{t})=\lambda_{n}y_{n}(e^{t})$$ But I wasn't sure how to solve this for the eigenfunctions and eigenvalue either? I'm missing something big here, so I'd appreciate it if someone could enlighten me! Thanks in advance!","I'm struggling to understand how to find the associated eigenfunctions and eigenvalues of a differential operator in Sturm-Liouville form. For instance, one question that I am trying to solve is the following: By substituting $x=e^{t}$ find the eigenfunctions and eigenvalues of the operator $\hat{\mathcal{L}}$, defined by: $$\hat{\mathcal{L}}[y(x)]=x^{2}y''(x)+2xy'(x)+\frac{1}{4}y(x)$$  With boundary conditions $y(1)=y(e)=0$ I can set up the eigensystem: $$x^2 y_{n}''(x)+2xy'_{n}(x)+\frac{1}{4}y_{n}(x)=\lambda_{n}y_{n}(x)$$ In Sturm-Lioville form, we have: $$\frac{\mathrm{d}}{\mathrm{d}x}\left(x^{2}\frac{\mathrm{d}y_{n}}{\mathrm{d}x}\right)+\frac{1}{4}y_{n}(x)=\lambda_{n}y_{n}(x)$$ However, I'm not sure how to solve this equation as Rodriguez' conditions are not met and if I attempt an ansatz series solution: $y_{n}=\sum_{n=0}^{\infty}\alpha_{n}x^{n}$, I end up with the following equation: $$\sum_{n=0}^{\infty}\alpha_{n}\cdot n(n-1)x^{n}+\sum_{n=0}^{\infty}2\alpha_{n}\cdot n x^{n}+\sum_{n=0}^{\infty}\frac{\alpha_{n}}{4}x^{n}=\sum_{n=0}^{\infty}\lambda \alpha_{n}x^{n} $$ However, this does not give me a recursion relationship as I would expect, instead I simply get: $$\lambda = k^{2} + k + \frac{1}{4} \qquad \exists k \in \mathbb{N} \cup \{0\}$$ I tried substituting $x=e^{t}$, giving me: $$\frac{\mathrm{d}^{2}}{\mathrm{d}t^{2}}\left(y_{n}(e^{t})\right)+\frac{1}{4}y_{n}(e^{t})=\lambda_{n}y_{n}(e^{t})$$ But I wasn't sure how to solve this for the eigenfunctions and eigenvalue either? I'm missing something big here, so I'd appreciate it if someone could enlighten me! Thanks in advance!",,"['ordinary-differential-equations', 'eigenvalues-eigenvectors', 'operator-theory']"
79,How to solve Sturm-Liouville problems $y''-2y'+(\lambda+1)y = 0$?,How to solve Sturm-Liouville problems ?,y''-2y'+(\lambda+1)y = 0,"I'm currently having a class at university that discusses Sturm Liouville Problems. We have to solve a few problems one of which I can't seem to solve. We use Advanced Engineering Mathematics by Erwin Kreyszig. The problem is a Sturm-Liouville problem: $$y''-2y'+(\lambda+1)y=0\ ,$$ with boundary conditions $y(0)=0$ and $y(1)=0$. The problem says the following: Find the eigenvalues and eigenfunctions. Verify orthogonality. Start by writing the ODE in the form of $$[p(x) y']' + [q(x) + \lambda r(x)]y=0$$ using problem 6. Problem 6 states the following: Show that $$y''+f y'+(g+\lambda h)y=0$$ takes the form $$[p(x) y']' + [q(x) + \lambda r(x)]y=0$$ if you set $$\begin{align} p&=e^{\int f\ dx}\ ,\cr q&=p g\ ,\cr r&=p h\ . \end{align}$$ Why would you do such a transformation? Now the problems are that I can't seem to understand why such a transformation is useful, and I can't seem to find the solutions of the differential equation. According to problem 6 I find the following: $$\begin{align} f&=-2\cr g&=1\cr h&=1 \end{align}$$ so: $$ \begin{align} p(x) &= e^{-2x}\cr q(x) &= e^{-2x}\cr r(x) &= e^{-2x}\ . \end{align} $$ Thus I get $$[e^{x}y']' + [1+\lambda]e^{-2x}y = 0$$ where $y(0)=0$ and $y(1)=0$. But I can't seem to get any further with this.","I'm currently having a class at university that discusses Sturm Liouville Problems. We have to solve a few problems one of which I can't seem to solve. We use Advanced Engineering Mathematics by Erwin Kreyszig. The problem is a Sturm-Liouville problem: $$y''-2y'+(\lambda+1)y=0\ ,$$ with boundary conditions $y(0)=0$ and $y(1)=0$. The problem says the following: Find the eigenvalues and eigenfunctions. Verify orthogonality. Start by writing the ODE in the form of $$[p(x) y']' + [q(x) + \lambda r(x)]y=0$$ using problem 6. Problem 6 states the following: Show that $$y''+f y'+(g+\lambda h)y=0$$ takes the form $$[p(x) y']' + [q(x) + \lambda r(x)]y=0$$ if you set $$\begin{align} p&=e^{\int f\ dx}\ ,\cr q&=p g\ ,\cr r&=p h\ . \end{align}$$ Why would you do such a transformation? Now the problems are that I can't seem to understand why such a transformation is useful, and I can't seem to find the solutions of the differential equation. According to problem 6 I find the following: $$\begin{align} f&=-2\cr g&=1\cr h&=1 \end{align}$$ so: $$ \begin{align} p(x) &= e^{-2x}\cr q(x) &= e^{-2x}\cr r(x) &= e^{-2x}\ . \end{align} $$ Thus I get $$[e^{x}y']' + [1+\lambda]e^{-2x}y = 0$$ where $y(0)=0$ and $y(1)=0$. But I can't seem to get any further with this.",,"['ordinary-differential-equations', 'sturm-liouville']"
80,Numerical way to deal with Dirac delta.,Numerical way to deal with Dirac delta.,,I have been wondering about this: I have a differential equation $y'(t) = y(t) + n \delta(t) y(t)$ with $y(-1) :=y_0$ Thus I want to apply a short delta pulse at some particular point $0$ to my system with a strength controlled by some sort of coefficient $n$. My question is: How do I simulate this numerically?- If I want to apply some canonical numerical ODE solver to this ODE( is this even possible here?).,I have been wondering about this: I have a differential equation $y'(t) = y(t) + n \delta(t) y(t)$ with $y(-1) :=y_0$ Thus I want to apply a short delta pulse at some particular point $0$ to my system with a strength controlled by some sort of coefficient $n$. My question is: How do I simulate this numerically?- If I want to apply some canonical numerical ODE solver to this ODE( is this even possible here?).,,['ordinary-differential-equations']
81,2nd order nonlinear ODE question,2nd order nonlinear ODE question,,"I am looking for help to solve the following $F(x,y(x),y'(x),y''(x))=0$ equation: $$ xy''(x)-y'(x)-(x^2)y(x)y'(x)=0 $$ Very much appreciated.","I am looking for help to solve the following $F(x,y(x),y'(x),y''(x))=0$ equation: $$ xy''(x)-y'(x)-(x^2)y(x)y'(x)=0 $$ Very much appreciated.",,['ordinary-differential-equations']
82,Solve a PDE: $ x(y^2+z)p-y(x^2+z)q=(x^2-y^2)z$,Solve a PDE:, x(y^2+z)p-y(x^2+z)q=(x^2-y^2)z,"Solve the PDE $$ x(y^2+z)p-y(x^2+z)q=(x^2-y^2)z$$ where, $ p=\displaystyle \frac{\partial z}{\partial x}$ and $ q=\displaystyle \frac{\partial z}{\partial y}$ My attempt: I start with Lagrange's auxiliary equation $$\frac{dx}{x(y^2+z)}=\frac{dy}{-y(x^2+z)}=\frac{dz}{z(x^2-y^2)}$$ Relation 1: $$\frac{x \, dx+y \, dy- dz}{x^2y^2+x^2z-y^2x^2-y^2z-x^2z+y^2z} = \frac{x \, dx+y \, dy-dz}{0} $$ Integrating $x \, dx+y \, dy-dz=0$ $$\frac{x^2}{2}+\frac{y^2}{2}-z=c \implies x^2+y^2-2z=c_1 $$ I can't see/find a second relation for me to solve the problem. Please help.","Solve the PDE where, and My attempt: I start with Lagrange's auxiliary equation Relation 1: Integrating I can't see/find a second relation for me to solve the problem. Please help."," x(y^2+z)p-y(x^2+z)q=(x^2-y^2)z  p=\displaystyle \frac{\partial z}{\partial x}  q=\displaystyle \frac{\partial z}{\partial y} \frac{dx}{x(y^2+z)}=\frac{dy}{-y(x^2+z)}=\frac{dz}{z(x^2-y^2)} \frac{x \, dx+y \, dy- dz}{x^2y^2+x^2z-y^2x^2-y^2z-x^2z+y^2z} = \frac{x \, dx+y \, dy-dz}{0}  x \, dx+y \, dy-dz=0 \frac{x^2}{2}+\frac{y^2}{2}-z=c \implies x^2+y^2-2z=c_1 ","['calculus', 'ordinary-differential-equations', 'partial-differential-equations']"
83,Solve $x^2+tx'+x=0$,Solve,x^2+tx'+x=0,"Solve $x^2+tx'+x=0$ this is clearly a Bernoulli's equation so I make a substitution $z=\frac 1 x$ $$x=\frac 1z$$ $$x'=\frac {-z'}{z^2}$$ $$\frac {1}{z^2}-\frac {tz'}{z^2}+\frac 1 z=0$$ $$1-tz'+z=0$$ $$z+1=tz'$$ $$log|z+1|=log|t|+C$$ $$z=Ct+1$$ $$x=\frac {1}{Ct+1}$$ But the actual answer to this is  $$x=\frac {e^C}{t-e^C}$$ Wolfram alpha also shows the second answer, so my question is where did I make a mistake, and if I didnt then why these two solutions differ so much?","Solve $x^2+tx'+x=0$ this is clearly a Bernoulli's equation so I make a substitution $z=\frac 1 x$ $$x=\frac 1z$$ $$x'=\frac {-z'}{z^2}$$ $$\frac {1}{z^2}-\frac {tz'}{z^2}+\frac 1 z=0$$ $$1-tz'+z=0$$ $$z+1=tz'$$ $$log|z+1|=log|t|+C$$ $$z=Ct+1$$ $$x=\frac {1}{Ct+1}$$ But the actual answer to this is  $$x=\frac {e^C}{t-e^C}$$ Wolfram alpha also shows the second answer, so my question is where did I make a mistake, and if I didnt then why these two solutions differ so much?",,['ordinary-differential-equations']
84,Initial value problem for 2nd order ODE $y''+ 4y = 8x$,Initial value problem for 2nd order ODE,y''+ 4y = 8x,How can I go about solving this equation $y''+ 4y = 8x$ ? Progress I found  the general solution for its homogeneous form. What I don't know is how to find its particular solution.,How can I go about solving this equation ? Progress I found  the general solution for its homogeneous form. What I don't know is how to find its particular solution.,y''+ 4y = 8x,"['calculus', 'ordinary-differential-equations']"
85,To Solve an ODE,To Solve an ODE,,"To Solve: $\displaystyle (1+x^2)\frac{d^2y}{dx^2}+1+\left(\frac{dy}{dx}\right)^2=0$ My Attempt: Take $\displaystyle \frac{dy}{dx}=p$ Now we have: $\displaystyle (1+x^2)\frac{dp}{dx}+1+p^2=0$ $\displaystyle \frac{dp}{1+p^2}=-\frac{dx}{(1+x^2)}$ Integrating, $\displaystyle \tan^{-1}p=-\tan^{-1}x$ So now, can we take this as $\displaystyle p = -x$ ? If I can,we end with: $\displaystyle \frac{dy}{dx}=-x+c_1$ The answer seems different: $\displaystyle y=c_1x+(c_1^2+1)\log(x-c_1)+c_2$ Where am I going wrong?","To Solve: $\displaystyle (1+x^2)\frac{d^2y}{dx^2}+1+\left(\frac{dy}{dx}\right)^2=0$ My Attempt: Take $\displaystyle \frac{dy}{dx}=p$ Now we have: $\displaystyle (1+x^2)\frac{dp}{dx}+1+p^2=0$ $\displaystyle \frac{dp}{1+p^2}=-\frac{dx}{(1+x^2)}$ Integrating, $\displaystyle \tan^{-1}p=-\tan^{-1}x$ So now, can we take this as $\displaystyle p = -x$ ? If I can,we end with: $\displaystyle \frac{dy}{dx}=-x+c_1$ The answer seems different: $\displaystyle y=c_1x+(c_1^2+1)\log(x-c_1)+c_2$ Where am I going wrong?",,['ordinary-differential-equations']
86,ODE with additional term,ODE with additional term,,"In an application I encountered the ODE  $$ \left( x^2-1 \right) \frac {{\rm d}^{2}}{{\rm d} x^2} f  ( x ) +x \left( \frac {\rm d}{{\rm d}x} f (x)  \right) ( 8x^2-7 ) -4 (C+1) f( x ) =0. $$  I don't see how I can find any solution analytically. Does anybody here know whether solutions to this function are known or whether there are any solutions that we can write down? Has this particular ODE ever been studied? I am greatful to every comment. In particular, just a few more solution are helpful too, if you cannot identify them all.","In an application I encountered the ODE  $$ \left( x^2-1 \right) \frac {{\rm d}^{2}}{{\rm d} x^2} f  ( x ) +x \left( \frac {\rm d}{{\rm d}x} f (x)  \right) ( 8x^2-7 ) -4 (C+1) f( x ) =0. $$  I don't see how I can find any solution analytically. Does anybody here know whether solutions to this function are known or whether there are any solutions that we can write down? Has this particular ODE ever been studied? I am greatful to every comment. In particular, just a few more solution are helpful too, if you cannot identify them all.",,"['calculus', 'real-analysis']"
87,Unsure with second order complex differential equations,Unsure with second order complex differential equations,,"Solve $$y'' - 4y' + 5y = 0 $$ Where $y(0) = 0 \ , \ y'(0) = 2$. So I solve this as a second degree polynomial (no idea why) $$\frac{4 \pm \sqrt{16-20}}{2} = 2 \pm 2i$$ So the CASE III solution as my book calls it is: $$Ae^{kt} \cos(wt) + B e^{kt} \sin(wt)$$ Where $k = Re$ and $w = Im$. So anyhow, $$y(0) = A \cos(0) + B \sin (0) \Rightarrow A = 0$$ $$y'(0) = -A \sin(0) + B \cos (0) = 2 \Rightarrow B = 2$$ So the solution is thus $$y(t) = 2 \cos(t)$$ Am I even doing this right? I have no idea what I am doing and it seems that differential equations are just taught this way. Plug this and that into these magical formulas.","Solve $$y'' - 4y' + 5y = 0 $$ Where $y(0) = 0 \ , \ y'(0) = 2$. So I solve this as a second degree polynomial (no idea why) $$\frac{4 \pm \sqrt{16-20}}{2} = 2 \pm 2i$$ So the CASE III solution as my book calls it is: $$Ae^{kt} \cos(wt) + B e^{kt} \sin(wt)$$ Where $k = Re$ and $w = Im$. So anyhow, $$y(0) = A \cos(0) + B \sin (0) \Rightarrow A = 0$$ $$y'(0) = -A \sin(0) + B \cos (0) = 2 \Rightarrow B = 2$$ So the solution is thus $$y(t) = 2 \cos(t)$$ Am I even doing this right? I have no idea what I am doing and it seems that differential equations are just taught this way. Plug this and that into these magical formulas.",,"['calculus', 'ordinary-differential-equations', 'complex-numbers']"
88,Existence and Uniqueness of solution for an IVP,Existence and Uniqueness of solution for an IVP,,"Consider the following IVP: $$ y'(x)=1+y^{2/3},\quad y(0)=0, $$ where the flux function is $f(x,y)=1+y^{2/3}$. According to Picard-Lindelöf Theorem, since $f_{y}$ is not continuous in any interval containing $x_{0}=0$, we can not guarantee existence of a solution for the above problem. On the other hand, using the separation of variables, we can find the following solution $$ 3\Big(y^{1/3}-\tan^{-1}(y^{1/3})\Big)=x. $$  It seems that this solution is unique for our ivp. My Question: How we can find an interval of uniqueness without solving the problem?","Consider the following IVP: $$ y'(x)=1+y^{2/3},\quad y(0)=0, $$ where the flux function is $f(x,y)=1+y^{2/3}$. According to Picard-Lindelöf Theorem, since $f_{y}$ is not continuous in any interval containing $x_{0}=0$, we can not guarantee existence of a solution for the above problem. On the other hand, using the separation of variables, we can find the following solution $$ 3\Big(y^{1/3}-\tan^{-1}(y^{1/3})\Big)=x. $$  It seems that this solution is unique for our ivp. My Question: How we can find an interval of uniqueness without solving the problem?",,"['analysis', 'ordinary-differential-equations']"
89,Discretize differential equation by finite differences. What is the matrix?,Discretize differential equation by finite differences. What is the matrix?,,"I have a differential equation: $$ -u''(x) + \sigma u'(x) = f(x), \quad 0<x<1$$ with boundary conditions $u(0) = \alpha$ and $u(1) = \beta$. I've discretized equation using symmetric (finite) differences: $$ \frac{-u_{i-1} + 2u_i - u_{i+1}}{h^2} + \sigma \frac{u_{i+1} - u_i}{h} = f_i $$ If $n$ is the number of discretization points,  $h = \frac{1}{n+1}$. My question is how would I get that in a matrix form $Ax = f$ ?","I have a differential equation: $$ -u''(x) + \sigma u'(x) = f(x), \quad 0<x<1$$ with boundary conditions $u(0) = \alpha$ and $u(1) = \beta$. I've discretized equation using symmetric (finite) differences: $$ \frac{-u_{i-1} + 2u_i - u_{i+1}}{h^2} + \sigma \frac{u_{i+1} - u_i}{h} = f_i $$ If $n$ is the number of discretization points,  $h = \frac{1}{n+1}$. My question is how would I get that in a matrix form $Ax = f$ ?",,"['ordinary-differential-equations', 'numerical-methods']"
90,Forced nonlinear oscillator - analytical methods,Forced nonlinear oscillator - analytical methods,,"This is an example from Kovacic & Brennan (2011) . Consider the following equation of motion for a forced, non-linear oscillator (Duffing's equation): $$ \ddot x + 2 \zeta \dot x + \alpha x + \gamma x^3 = F \cos \Omega t, $$ where $\zeta, \alpha, \gamma, F$ and $\Omega$ are parameters. We assume ""the steady-state harmonic solution"" as $$ x = Y\cos{(\Omega t - \theta)}. $$ My goal is to obtain the following equation for $Y$: $$ Y^2 = \frac{F^2}{4\zeta^2\Omega^2 + \left( \Omega^2 - \alpha -  % \frac{3}{4}\gamma Y^2 \right)^2}. $$ This is done by 1) substituting the second equation into the first, and 2) equating the coefficients of $\sin\Omega t$ and $\cos\Omega t$ from both sides. The substitution part seems straightforward, but I can't quite wrap my head around the last step. Especially the ""$\cos(\Omega t-\theta)$"" on the right hand side is causing headaches. Suggestions are welcome!","This is an example from Kovacic & Brennan (2011) . Consider the following equation of motion for a forced, non-linear oscillator (Duffing's equation): $$ \ddot x + 2 \zeta \dot x + \alpha x + \gamma x^3 = F \cos \Omega t, $$ where $\zeta, \alpha, \gamma, F$ and $\Omega$ are parameters. We assume ""the steady-state harmonic solution"" as $$ x = Y\cos{(\Omega t - \theta)}. $$ My goal is to obtain the following equation for $Y$: $$ Y^2 = \frac{F^2}{4\zeta^2\Omega^2 + \left( \Omega^2 - \alpha -  % \frac{3}{4}\gamma Y^2 \right)^2}. $$ This is done by 1) substituting the second equation into the first, and 2) equating the coefficients of $\sin\Omega t$ and $\cos\Omega t$ from both sides. The substitution part seems straightforward, but I can't quite wrap my head around the last step. Especially the ""$\cos(\Omega t-\theta)$"" on the right hand side is causing headaches. Suggestions are welcome!",,"['ordinary-differential-equations', 'nonlinear-system']"
91,Can we express all doubly periodic functions as one of doubly periodic function?,Can we express all doubly periodic functions as one of doubly periodic function?,,"Singly Periodic Functions $e^{x},\cos(x),\sin(x),\tan(x), .. etc.$ Euler's identity is $$e^{i\alpha}=\cos(\alpha)+i\sin(\alpha)$$ $$e^{-i\alpha}=\cos(\alpha)-i\sin(\alpha)$$ Thus, we can express all trigonometric functions via  $e^{x}$ function $$\cos(\alpha)=\frac{e^{i\alpha}+e^{-i\alpha}}{2}$$ $$\sin(\alpha)=\frac{e^{i\alpha}-e^{-i\alpha}}{2i}$$ $$\tan(\alpha)=\frac{e^{i\alpha}-e^{-i\alpha}}{i(e^{i\alpha}+e^{-i\alpha})}$$ $$\cot(\alpha)=\frac{i(e^{i\alpha}+e^{-i\alpha})}{e^{i\alpha}-e^{-i\alpha}}$$ $$.$$ $$.$$ And also we know that  periodic functions or periodic signals into the sum of a (possibly infinite) set of simple oscillating functions  are can be expressed via sines and cosines (or complex exponentials). wiki reference $$ f(x) = \frac{a_0}{2} + \sum_{n=1}^\infty \, [a_n \cos(\tfrac{2\pi nx}{P}) + b_n \sin(\tfrac{2\pi nx}{P})]\\ = \sum_{n=-\infty}^\infty c_n\cdot e^{i \tfrac{2\pi nx}{P}}$$ where $ f(x)$ is any periodic function and $P$ is period. Elliptic functions are doubly periodic functions The elliptic functions are inversions of the elliptic integrals. The two standard forms of these functions are known as Jacobi elliptic functions and Weierstrass elliptic functions. Jacobi elliptic functions arise as solutions to differential equations of the form $$ y''(x)=A+By(x)+Cy^2(x)+Dy^3(x)$$ and Weierstrass elliptic functions arise as solutions to differential equations of the form $$ y''(x)=A+By(x)+Cy^2(x)$$ wolfram Reference My questions: Can we express all doubly periodic functions as one of doubly periodic function similarly as we do in singly periodic functions? Is there any research on such base doubly periodic function that can be used  to express all type of doubly periodic functions or signals in series? Please share references If you know any. In other words,For example: $y(x)=e^{x}$ is a solution of $$ y'(x)=y(x)$$  and $e^{x}$ has been used as base function to express all singly Periodic Functions. Maybe we can define a $f{(x)}$ as the solution of  $$ y''(x)=1+y(x)+y^2(x)+y^3(x)$$ .$f{(x)}$ can be used to  express all of other  doubly periodic functions . I do not know if it is possible or not? Thanks for answers and comments","Singly Periodic Functions $e^{x},\cos(x),\sin(x),\tan(x), .. etc.$ Euler's identity is $$e^{i\alpha}=\cos(\alpha)+i\sin(\alpha)$$ $$e^{-i\alpha}=\cos(\alpha)-i\sin(\alpha)$$ Thus, we can express all trigonometric functions via  $e^{x}$ function $$\cos(\alpha)=\frac{e^{i\alpha}+e^{-i\alpha}}{2}$$ $$\sin(\alpha)=\frac{e^{i\alpha}-e^{-i\alpha}}{2i}$$ $$\tan(\alpha)=\frac{e^{i\alpha}-e^{-i\alpha}}{i(e^{i\alpha}+e^{-i\alpha})}$$ $$\cot(\alpha)=\frac{i(e^{i\alpha}+e^{-i\alpha})}{e^{i\alpha}-e^{-i\alpha}}$$ $$.$$ $$.$$ And also we know that  periodic functions or periodic signals into the sum of a (possibly infinite) set of simple oscillating functions  are can be expressed via sines and cosines (or complex exponentials). wiki reference $$ f(x) = \frac{a_0}{2} + \sum_{n=1}^\infty \, [a_n \cos(\tfrac{2\pi nx}{P}) + b_n \sin(\tfrac{2\pi nx}{P})]\\ = \sum_{n=-\infty}^\infty c_n\cdot e^{i \tfrac{2\pi nx}{P}}$$ where $ f(x)$ is any periodic function and $P$ is period. Elliptic functions are doubly periodic functions The elliptic functions are inversions of the elliptic integrals. The two standard forms of these functions are known as Jacobi elliptic functions and Weierstrass elliptic functions. Jacobi elliptic functions arise as solutions to differential equations of the form $$ y''(x)=A+By(x)+Cy^2(x)+Dy^3(x)$$ and Weierstrass elliptic functions arise as solutions to differential equations of the form $$ y''(x)=A+By(x)+Cy^2(x)$$ wolfram Reference My questions: Can we express all doubly periodic functions as one of doubly periodic function similarly as we do in singly periodic functions? Is there any research on such base doubly periodic function that can be used  to express all type of doubly periodic functions or signals in series? Please share references If you know any. In other words,For example: $y(x)=e^{x}$ is a solution of $$ y'(x)=y(x)$$  and $e^{x}$ has been used as base function to express all singly Periodic Functions. Maybe we can define a $f{(x)}$ as the solution of  $$ y''(x)=1+y(x)+y^2(x)+y^3(x)$$ .$f{(x)}$ can be used to  express all of other  doubly periodic functions . I do not know if it is possible or not? Thanks for answers and comments",,"['ordinary-differential-equations', 'fourier-series', 'elliptic-functions', 'elliptic-integrals']"
92,Bifurcation Diagram,Bifurcation Diagram,,"If we are drawing a pitchfork bifurcation of $x'=\lambda x-x^3$ then my concern is that what is the difference if $\lambda$ is piece-wise constant? That is what is difference between the original bifurcation diagram for general $\lambda$ and the bifurcation diagram where λ varies with time? Piece-wise constant means $\lambda$ takes a value for some period of time and it will have another value for another period of time. A similar problem can be found in Ordinary Differential Equations with Applications (Texts in Applied Mathematics) by Carmen Chicone, Second Edition on page number 13 Exercise 1.16 and my question is exactly part (c) of this exercise. Thanks and hope to get your help soon.","If we are drawing a pitchfork bifurcation of $x'=\lambda x-x^3$ then my concern is that what is the difference if $\lambda$ is piece-wise constant? That is what is difference between the original bifurcation diagram for general $\lambda$ and the bifurcation diagram where λ varies with time? Piece-wise constant means $\lambda$ takes a value for some period of time and it will have another value for another period of time. A similar problem can be found in Ordinary Differential Equations with Applications (Texts in Applied Mathematics) by Carmen Chicone, Second Edition on page number 13 Exercise 1.16 and my question is exactly part (c) of this exercise. Thanks and hope to get your help soon.",,[]
93,"Show that all the roots of $\frac{dx}{dt}=A(t)x$ are bounded in $[t_0, \infty)$.",Show that all the roots of  are bounded in .,"\frac{dx}{dt}=A(t)x [t_0, \infty)","For real system of equations$$\frac{dx}{dt}=A(t)x,(1)$$ where $A(t) \in C[t_0, +\infty)$. Prove that if $\int_{t_0}^{\infty} \|A(t_1)+A^T(t_1)\|< +\infty$ then all the roots of (1) are bounded in $[t_0, \infty)$. Can anyone help me? Thanks.","For real system of equations$$\frac{dx}{dt}=A(t)x,(1)$$ where $A(t) \in C[t_0, +\infty)$. Prove that if $\int_{t_0}^{\infty} \|A(t_1)+A^T(t_1)\|< +\infty$ then all the roots of (1) are bounded in $[t_0, \infty)$. Can anyone help me? Thanks.",,"['ordinary-differential-equations', 'control-theory']"
94,"When solving $3xy'+y=12x$, why can absolute sign in ln |x| can be ignored?","When solving , why can absolute sign in ln |x| can be ignored?",3xy'+y=12x,"So the following is the differential equation: $3xy'+y=12x$ We can solve this by recasting the equation to $y'+y/3x=4$ and finding an integration factor. Integration factor is: $e^{\int 1/3x \,\, dx} = e^{1/3 \ln |x|} = |x|^{1/3}$ But often it seems that absolute signs are ignored. Why is it the case? Does the result not change whether absolute sign is there or not?","So the following is the differential equation: $3xy'+y=12x$ We can solve this by recasting the equation to $y'+y/3x=4$ and finding an integration factor. Integration factor is: $e^{\int 1/3x \,\, dx} = e^{1/3 \ln |x|} = |x|^{1/3}$ But often it seems that absolute signs are ignored. Why is it the case? Does the result not change whether absolute sign is there or not?",,['ordinary-differential-equations']
95,How to solve vector-valued first order linear pde?,How to solve vector-valued first order linear pde?,,"Is there an analytical solution to the pde system? $$\frac{\partial f}{\partial x} + \frac{\partial g}{\partial y} = 0$$ $$\frac{\partial f}{\partial y} - \frac{\partial g}{\partial x} = 0$$ More generally, how about $$a\frac{\partial f}{\partial x} + b\frac{\partial g}{\partial y} = 0$$ $$c\frac{\partial f}{\partial y} - d\frac{\partial g}{\partial x} = 0$$ where $a,b,c,d$ are all constants. Maybe now analytical solution depends on these coefficients and thus be hard or unclear, in which case a proof of existence is wanted. ( Does here Frobenius theorem help? )","Is there an analytical solution to the pde system? $$\frac{\partial f}{\partial x} + \frac{\partial g}{\partial y} = 0$$ $$\frac{\partial f}{\partial y} - \frac{\partial g}{\partial x} = 0$$ More generally, how about $$a\frac{\partial f}{\partial x} + b\frac{\partial g}{\partial y} = 0$$ $$c\frac{\partial f}{\partial y} - d\frac{\partial g}{\partial x} = 0$$ where $a,b,c,d$ are all constants. Maybe now analytical solution depends on these coefficients and thus be hard or unclear, in which case a proof of existence is wanted. ( Does here Frobenius theorem help? )",,"['ordinary-differential-equations', 'partial-differential-equations']"
96,Modelling with exact differential equations?,Modelling with exact differential equations?,,"I'm teaching some very elementary differential equations to engineering students, and their constant question to me is ""What's the use of this?"" or alternatively ""Where would we use this?""  Now, I'm not an applied mathematician, but most of the ""standard"" applications of de's you see in texts seem to include population growth, radioactive decay, cooling, mixing, predator-prey, all of which can be modelled with separable or linear de's. So what I'm trying to find is a fairly simple example - suitable to first year students with limited mathematics - of a practical ""real-world"" application which is most easily modelled by an exact differential equation.  My web searching has drawn a blank, so I'm open to suggestions!","I'm teaching some very elementary differential equations to engineering students, and their constant question to me is ""What's the use of this?"" or alternatively ""Where would we use this?""  Now, I'm not an applied mathematician, but most of the ""standard"" applications of de's you see in texts seem to include population growth, radioactive decay, cooling, mixing, predator-prey, all of which can be modelled with separable or linear de's. So what I'm trying to find is a fairly simple example - suitable to first year students with limited mathematics - of a practical ""real-world"" application which is most easily modelled by an exact differential equation.  My web searching has drawn a blank, so I'm open to suggestions!",,"['ordinary-differential-equations', 'education', 'applications']"
97,Confusion regarding change of variables in ODEs,Confusion regarding change of variables in ODEs,,"Consider the following exercise: Using the Laplace transform, find a solution $y(x)$ of the following initial value problem: $$\begin{cases}  y'' +y  = x + 1, \quad x > \pi \\ y(\pi) = \pi^2 \\ y'(\pi) = 2\pi \end{cases}$$ Suggestion : Make the change of variables $t = x - \pi$. Of course, this equation is pretty easy to solve without the Laplace transform, but the whole point of the exercise is to use it. That's not the problem, though. I'm having some trouble with the change of variables. I know that for the derivatives you have to use the chain rule; in this case, since $dt = dx$ there's no difference. My main issue is with the initial conditions. I'm not very sure how to restate them in terms of $t$, and I think that's because I don't really know a precise definition of change of variables. How is it done? Do we define a new function, something like $g(t) = y(x-\pi)$, if that even makes sense? What would be a general method to make sure one does things like this carefully? I apologize if the question is rather vague. Just to be clear, I'm not asking how to solve this particular differential equation; I've realized I get confused in general when using change of variables in an ODE.","Consider the following exercise: Using the Laplace transform, find a solution $y(x)$ of the following initial value problem: $$\begin{cases}  y'' +y  = x + 1, \quad x > \pi \\ y(\pi) = \pi^2 \\ y'(\pi) = 2\pi \end{cases}$$ Suggestion : Make the change of variables $t = x - \pi$. Of course, this equation is pretty easy to solve without the Laplace transform, but the whole point of the exercise is to use it. That's not the problem, though. I'm having some trouble with the change of variables. I know that for the derivatives you have to use the chain rule; in this case, since $dt = dx$ there's no difference. My main issue is with the initial conditions. I'm not very sure how to restate them in terms of $t$, and I think that's because I don't really know a precise definition of change of variables. How is it done? Do we define a new function, something like $g(t) = y(x-\pi)$, if that even makes sense? What would be a general method to make sure one does things like this carefully? I apologize if the question is rather vague. Just to be clear, I'm not asking how to solve this particular differential equation; I've realized I get confused in general when using change of variables in an ODE.",,['ordinary-differential-equations']
98,$e^A=\lim_{j\to +\infty}(I+\frac{1}{j}A)^j$,,e^A=\lim_{j\to +\infty}(I+\frac{1}{j}A)^j,I'm wondering if this is true: $e^A=\lim_{j\to +\infty}(I+\frac{1}{j}A)^j$ I need help thanks a lot.,I'm wondering if this is true: $e^A=\lim_{j\to +\infty}(I+\frac{1}{j}A)^j$ I need help thanks a lot.,,['ordinary-differential-equations']
99,If $f$ is even and $y'=f(y)$ then $y$ is odd,If  is even and  then  is odd,f y'=f(y) y,"Let $f\in C^1(\mathbb{R}, \mathbb{R})$ be an even function. Consider the maximal solution $y\colon\left]\alpha ,\beta\right[\to \mathbb{R}$ of the IVP $$y'=f(y),\ y(0)=0$$ Prove that $y$ is an odd function and $\beta =-\alpha$ . To be able to prove that $y$ is odd, I first need its domain to be symmetric ( $x\in \left]\alpha ,\beta\right[\implies -x\in \left]\alpha ,\beta\right[$ ), from here I can conclude that $\alpha=-\beta$ . But how to prove the domain is symmetric? And how to prove that $y(-x)=-y(x)$ for all $x\in \left]\alpha,\beta\right[$ ? I suspect it has something to do with the fact that the derivative of an even function is odd and $y'=f(y)$ , but I can't see how to get the desired result.","Let be an even function. Consider the maximal solution of the IVP Prove that is an odd function and . To be able to prove that is odd, I first need its domain to be symmetric ( ), from here I can conclude that . But how to prove the domain is symmetric? And how to prove that for all ? I suspect it has something to do with the fact that the derivative of an even function is odd and , but I can't see how to get the desired result.","f\in C^1(\mathbb{R}, \mathbb{R}) y\colon\left]\alpha ,\beta\right[\to \mathbb{R} y'=f(y),\ y(0)=0 y \beta =-\alpha y x\in \left]\alpha ,\beta\right[\implies -x\in \left]\alpha ,\beta\right[ \alpha=-\beta y(-x)=-y(x) x\in \left]\alpha,\beta\right[ y'=f(y)","['ordinary-differential-equations', 'functional-equations']"
