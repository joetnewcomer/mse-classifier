,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Solve $\frac{dy}{dx} (xy)+ 4x^2 + y^2 = 0$ using the substitution $w=\frac yx$,Solve  using the substitution,\frac{dy}{dx} (xy)+ 4x^2 + y^2 = 0 w=\frac yx,"Solve $\frac{dy}{dx} (xy)+ 4x^2 + y^2 = 0$ using the substitution $w=\frac yx$ I have done most of this question but i'm unsure how to write the solution. This is what I have so far: $$\frac{dy}{dx} + 4(x/y) + \frac yx = 0$$ $$\frac{dy}{dx} + \frac4w + w=0$$ As $w=\dfrac yx$, it follows that $y=xw$ so $\dfrac{dy}{dx} = w$. so we now have: $$w +\dfrac4w + w = 0$$ $$2w^2 + 4 = 0$$ Which gives the solution  $\pm\sqrt{2i}$ This is now the point where I get confused.  Is this next part correct? $$w = C_1 \cos(x\sqrt2) + C_2 \sin(x\sqrt2)$$ Therefore: $$y =  x[ C_1\cos(x\sqrt2) + C_2\sin(x\sqrt2)]$$ Where $C_1$ and $C_2$ are constants. I have a feeling this isn't correct?","Solve $\frac{dy}{dx} (xy)+ 4x^2 + y^2 = 0$ using the substitution $w=\frac yx$ I have done most of this question but i'm unsure how to write the solution. This is what I have so far: $$\frac{dy}{dx} + 4(x/y) + \frac yx = 0$$ $$\frac{dy}{dx} + \frac4w + w=0$$ As $w=\dfrac yx$, it follows that $y=xw$ so $\dfrac{dy}{dx} = w$. so we now have: $$w +\dfrac4w + w = 0$$ $$2w^2 + 4 = 0$$ Which gives the solution  $\pm\sqrt{2i}$ This is now the point where I get confused.  Is this next part correct? $$w = C_1 \cos(x\sqrt2) + C_2 \sin(x\sqrt2)$$ Therefore: $$y =  x[ C_1\cos(x\sqrt2) + C_2\sin(x\sqrt2)]$$ Where $C_1$ and $C_2$ are constants. I have a feeling this isn't correct?",,"['ordinary-differential-equations', 'substitution']"
1,"Solving a coupled second order differential equation, for a particles path.","Solving a coupled second order differential equation, for a particles path.",,"I was tasked with finding the path a particle takes through this potential function. $$U(x)=x^2+xy+y^2$$ I then took the gradient, and this produced a pair of differential equations. $$\frac{d^2x}{dt^2}=\frac{1}{m}(-2x-y)$$ $$\frac{d^2y}{dt^2}=\frac{1}{m}(-2y-x)$$ I have yet to encounter a problem of this form. It looks simple, but it has proven to be very tricky to solve. I tried a couple substitution techniques that initially seemed promising but inevitably failed. I even used Laplace transforms, but that got very messy and I eventually gave up. Any thoughts? Thank you!","I was tasked with finding the path a particle takes through this potential function. $$U(x)=x^2+xy+y^2$$ I then took the gradient, and this produced a pair of differential equations. $$\frac{d^2x}{dt^2}=\frac{1}{m}(-2x-y)$$ $$\frac{d^2y}{dt^2}=\frac{1}{m}(-2y-x)$$ I have yet to encounter a problem of this form. It looks simple, but it has proven to be very tricky to solve. I tried a couple substitution techniques that initially seemed promising but inevitably failed. I even used Laplace transforms, but that got very messy and I eventually gave up. Any thoughts? Thank you!",,"['ordinary-differential-equations', 'systems-of-equations', 'dynamical-systems', 'physics']"
2,Where are the constant solutions of this separable ODE $y'=y^2-4$.?,Where are the constant solutions of this separable ODE .?,y'=y^2-4,"Let $y'=y^2-4$. We can solve this equation by separation of variables, and thus we reach the solutions $$y(x) = -\frac{2 (e^{4 c_1 + 4 x} - 1)}{e^{4 c_1 + 4 x} + 1}$$ (Checked using Wolfram Alpha ) Now, we can see plainly that the constant solutions $y=\pm 2$ satisfy the equations. Are this solutions hidden in this formula for a particular value of $c_1$? If not, how can I make the integration such that the constant solutions are not lost?","Let $y'=y^2-4$. We can solve this equation by separation of variables, and thus we reach the solutions $$y(x) = -\frac{2 (e^{4 c_1 + 4 x} - 1)}{e^{4 c_1 + 4 x} + 1}$$ (Checked using Wolfram Alpha ) Now, we can see plainly that the constant solutions $y=\pm 2$ satisfy the equations. Are this solutions hidden in this formula for a particular value of $c_1$? If not, how can I make the integration such that the constant solutions are not lost?",,['ordinary-differential-equations']
3,Conservation law(s) of chemical reaction $\rm A \longrightarrow B + 2C$,Conservation law(s) of chemical reaction,\rm A \longrightarrow B + 2C,"I am trying to find the conservation law for the following chemical reaction: $$\rm A \longrightarrow B + 2C$$ where: A converts into $B$ and $C$ at a rate $k (k>0)$. $[A]_0$, $[B]_0$, $[C]_0$ are initial concentrations of $A$, $B$ and $C$ $[B]_0$ and $[C]_0$ are initially $\rm 0\  M$. I am thinking the conservation laws are: $[B] + [A] = [A]_0$ $[C] + 2[A] = 2[A]_0$ Am I correct? Or am I getting it wrong?","I am trying to find the conservation law for the following chemical reaction: $$\rm A \longrightarrow B + 2C$$ where: A converts into $B$ and $C$ at a rate $k (k>0)$. $[A]_0$, $[B]_0$, $[C]_0$ are initial concentrations of $A$, $B$ and $C$ $[B]_0$ and $[C]_0$ are initially $\rm 0\  M$. I am thinking the conservation laws are: $[B] + [A] = [A]_0$ $[C] + 2[A] = 2[A]_0$ Am I correct? Or am I getting it wrong?",,"['ordinary-differential-equations', 'dynamical-systems', 'chemistry']"
4,analytic solution of a zombie model,analytic solution of a zombie model,,"in the following Paper a ODE model of a zombie infection is given. \begin{equation} \begin{aligned} 	\frac{dS}{d\tau} &= -\frac{SZ}{N} \\ 	\frac{dZ}{d\tau} &= (1-\alpha)\frac{SZ}{N}\\ 	\frac{dR}{d\tau} &= \alpha\frac{SZ}{N} \\\\                   N &= S + Z + R = const \quad \dots\\ \text{initial condition: } S(0) &= S_0,\, Z(0) = Z_0,\, R(0) = 0 \end{aligned} \end{equation} The system is non-linear and there exits a analytical solution. \begin{equation*} \begin{aligned} 	P &\equiv Z_0 + (1-\alpha)S_0 \\ 	\mu &\equiv \frac{P}{Z_0}-1\\ 	f(\tau) &\equiv \frac{P\mu}{\exp{(\tau P/N)} + \mu}\\ 	Z(\tau) &= P - f(\tau) \\ 	S(\tau) &= \frac{f(\tau)}{1-\alpha} \end{aligned} \end{equation*} Unfortunatly I don't know how to solve the system to get the analytical solution. My Idea is to substitute X = SZ but if I calculate $\dot{X}$ I can't eliminate S or Z from the equation. \begin{equation} \dot{X} = \dot{S}Z + \dot{Z}S \end{equation}  I'm out of ideas how to solve the system and I hope for some suggestions","in the following Paper a ODE model of a zombie infection is given. \begin{equation} \begin{aligned} 	\frac{dS}{d\tau} &= -\frac{SZ}{N} \\ 	\frac{dZ}{d\tau} &= (1-\alpha)\frac{SZ}{N}\\ 	\frac{dR}{d\tau} &= \alpha\frac{SZ}{N} \\\\                   N &= S + Z + R = const \quad \dots\\ \text{initial condition: } S(0) &= S_0,\, Z(0) = Z_0,\, R(0) = 0 \end{aligned} \end{equation} The system is non-linear and there exits a analytical solution. \begin{equation*} \begin{aligned} 	P &\equiv Z_0 + (1-\alpha)S_0 \\ 	\mu &\equiv \frac{P}{Z_0}-1\\ 	f(\tau) &\equiv \frac{P\mu}{\exp{(\tau P/N)} + \mu}\\ 	Z(\tau) &= P - f(\tau) \\ 	S(\tau) &= \frac{f(\tau)}{1-\alpha} \end{aligned} \end{equation*} Unfortunatly I don't know how to solve the system to get the analytical solution. My Idea is to substitute X = SZ but if I calculate $\dot{X}$ I can't eliminate S or Z from the equation. \begin{equation} \dot{X} = \dot{S}Z + \dot{Z}S \end{equation}  I'm out of ideas how to solve the system and I hope for some suggestions",,"['ordinary-differential-equations', 'nonlinear-system']"
5,"If $\alpha'=f(\alpha)$ for some periodic $f:\Bbb R\to\Bbb R_{>0}$, then $\alpha(x+\ell)=\alpha(x)+A$ for some fixed $\ell$.","If  for some periodic , then  for some fixed .",\alpha'=f(\alpha) f:\Bbb R\to\Bbb R_{>0} \alpha(x+\ell)=\alpha(x)+A \ell,"Let $f:\Bbb R\to\Bbb R_{>0}$ be some periodic Lipschitz function, $A>0$ some constant, and suppose that $f$ is $A$-periodic. Then the theory of ODEs tells us that we have global solutions $\alpha:\Bbb R\to\Bbb R$ to $$\alpha'(s)=f(\alpha(s))$$ for all initial values. Now, I suspect that if we set $$\ell=\int_x^{x+A}\frac{\mathrm{d}u}{f(u)}$$ for any $x\in\Bbb R$, then $\alpha(x+\ell)=\alpha(x)+A$ for all $x\in\Bbb R$ (this problem comes from the theory of curves, in which we try to find a unit-length parametrization for a closed immersed plane curve). But why is this true?","Let $f:\Bbb R\to\Bbb R_{>0}$ be some periodic Lipschitz function, $A>0$ some constant, and suppose that $f$ is $A$-periodic. Then the theory of ODEs tells us that we have global solutions $\alpha:\Bbb R\to\Bbb R$ to $$\alpha'(s)=f(\alpha(s))$$ for all initial values. Now, I suspect that if we set $$\ell=\int_x^{x+A}\frac{\mathrm{d}u}{f(u)}$$ for any $x\in\Bbb R$, then $\alpha(x+\ell)=\alpha(x)+A$ for all $x\in\Bbb R$ (this problem comes from the theory of curves, in which we try to find a unit-length parametrization for a closed immersed plane curve). But why is this true?",,"['real-analysis', 'ordinary-differential-equations']"
6,Definition of a non-linear first order Partial differential equation,Definition of a non-linear first order Partial differential equation,,"Actually I am a little bit confused about the definition. I have read two three articles but I could not find out what type of equations are called a non-linear partial differential equation. Articles are following. https://en.wikiversity.org/wiki/Partial_differential_equations https://www.slideshare.net/jayanshugundaniya9/advanced-engineering-mathematics-first-order-nonlinear-partial-differential-equation-its-applications https://mat.iitm.ac.in/home/sryedida/public_html/caimna/pde/forth/forth.html $pq = 0$ will be a first order non linear Partial differential equation? p,q are usual notation in PDE. Please don' downvote. I know it is a silly question. But I am really confused. Please help me. I am looking forward to ur reply.","Actually I am a little bit confused about the definition. I have read two three articles but I could not find out what type of equations are called a non-linear partial differential equation. Articles are following. https://en.wikiversity.org/wiki/Partial_differential_equations https://www.slideshare.net/jayanshugundaniya9/advanced-engineering-mathematics-first-order-nonlinear-partial-differential-equation-its-applications https://mat.iitm.ac.in/home/sryedida/public_html/caimna/pde/forth/forth.html will be a first order non linear Partial differential equation? p,q are usual notation in PDE. Please don' downvote. I know it is a silly question. But I am really confused. Please help me. I am looking forward to ur reply.",pq = 0,"['ordinary-differential-equations', 'partial-differential-equations', 'partial-derivative']"
7,"Method of Frobenius, constants replaced by power series","Method of Frobenius, constants replaced by power series",,"The source I am using is Differential Equations with Applications and Historical Notes (Simmons). I appologize in advance if I did not include enough information from the text about my question. The method of Frobenius is motivated with the $z = log(x)$ substitution in the Euler equation $$x^2y'' + pxy' + qy = 0,$$ which has solutions $x^{m_1}$ and $x^{m_2}$ (or $x^{m_1}\log{x}$ ) corresponding to $m^2 + (p-1)m + q = 0$ . It then says that if $p$ and $q$ are replaced by power series, $$y'' + \frac{(p_0 + p_1x + p_2x^2 + \dots)}{x}y' + \frac{(q_0 + q_1x + q_2x^2 + \dots)}{x^2}y=0$$ then it is natural to guess that the solutions (to the equations with power series) might be found by replacing the original solutions with solutions of the form $$y_1 = x^m(a_0 + a_1x + a_2x^2 + \dots),\\ y_2 = x^m\log{x}(a_0 + a_1x + a_2x^2 + \dots)$$ Why would this be a natural guess? I assume that there is more to this than simply multiplying our solutions by a power series simply because power series now appear in the differential equation. But I also feel like there is something really obvious that I am missing. Thanks in advance.","The source I am using is Differential Equations with Applications and Historical Notes (Simmons). I appologize in advance if I did not include enough information from the text about my question. The method of Frobenius is motivated with the substitution in the Euler equation which has solutions and (or ) corresponding to . It then says that if and are replaced by power series, then it is natural to guess that the solutions (to the equations with power series) might be found by replacing the original solutions with solutions of the form Why would this be a natural guess? I assume that there is more to this than simply multiplying our solutions by a power series simply because power series now appear in the differential equation. But I also feel like there is something really obvious that I am missing. Thanks in advance.","z = log(x) x^2y'' + pxy' + qy = 0, x^{m_1} x^{m_2} x^{m_1}\log{x} m^2 + (p-1)m + q = 0 p q y'' + \frac{(p_0 + p_1x + p_2x^2 + \dots)}{x}y' + \frac{(q_0 + q_1x + q_2x^2 + \dots)}{x^2}y=0 y_1 = x^m(a_0 + a_1x + a_2x^2 + \dots),\\ y_2 = x^m\log{x}(a_0 + a_1x + a_2x^2 + \dots)","['ordinary-differential-equations', 'frobenius-method']"
8,Flow inside and outside Cylinder,Flow inside and outside Cylinder,,"Incompressible fluid with constant density ρ fills the three-dimensional domain below the free surface z = η(r) in cylindrical polar coordinates. The flow is axisymmetric and steady, and the only non-zero velocity component is $u_θ$. Gravity acts upon the fluid. Suppose the fluid in r < a rotates rigidly about the z-axis with angular velocity Ω, and the fluid in r 􏰌 a is irrotational. Show that the velocity in $r \ge$􏰌 a is given by $$u_θ = \frac{{Ωa^2}}{r}$$ I got stuck about how to show rigidly. My idea: For incompressible: $$\nabla\bullet u= \frac{1}{r}\frac{∂u_θ}{∂_θ}=0$$ For irrotational: $$\frac{∂u_θ}{∂_r}=0$$ For as $r \rightarrow$ a, $u_0$ approaches the tangential velocity Ωr. the second part of the problem is suppose the free surface position satisfies  η$\rightarrow$ 0 as r $\rightarrow \infty$. Show that the free surface position in $r\ge a$ is $$η= -\frac{Ω^2a^4}{2gr^2}$$ My idea is to use the Bernoulli's theorem which is expressed in the boundary condition of the free surface: $\frac{∂\phi }{∂t}+ \frac{1}{2}|\nabla\phi|^2+gη=0$  at y =η $\phi$ is the velocity potential, then it is $\frac{Ωa^2z}{r}$ $\frac{Ωa^2z}{r}$=-gη we can get η. But how to get z from the condition?","Incompressible fluid with constant density ρ fills the three-dimensional domain below the free surface z = η(r) in cylindrical polar coordinates. The flow is axisymmetric and steady, and the only non-zero velocity component is $u_θ$. Gravity acts upon the fluid. Suppose the fluid in r < a rotates rigidly about the z-axis with angular velocity Ω, and the fluid in r 􏰌 a is irrotational. Show that the velocity in $r \ge$􏰌 a is given by $$u_θ = \frac{{Ωa^2}}{r}$$ I got stuck about how to show rigidly. My idea: For incompressible: $$\nabla\bullet u= \frac{1}{r}\frac{∂u_θ}{∂_θ}=0$$ For irrotational: $$\frac{∂u_θ}{∂_r}=0$$ For as $r \rightarrow$ a, $u_0$ approaches the tangential velocity Ωr. the second part of the problem is suppose the free surface position satisfies  η$\rightarrow$ 0 as r $\rightarrow \infty$. Show that the free surface position in $r\ge a$ is $$η= -\frac{Ω^2a^4}{2gr^2}$$ My idea is to use the Bernoulli's theorem which is expressed in the boundary condition of the free surface: $\frac{∂\phi }{∂t}+ \frac{1}{2}|\nabla\phi|^2+gη=0$  at y =η $\phi$ is the velocity potential, then it is $\frac{Ωa^2z}{r}$ $\frac{Ωa^2z}{r}$=-gη we can get η. But how to get z from the condition?",,"['ordinary-differential-equations', 'fluid-dynamics', 'cylindrical-coordinates']"
9,"Standard hyperbolic solution to 2nd order ODE, equivalent forms.","Standard hyperbolic solution to 2nd order ODE, equivalent forms.",,"Consider the following 2nd order ODE: $$\frac{d^2u}{dx^2}-\gamma^2u=0.$$ This equation has solutions of the form $$u(x)=Ae^{\gamma x}+Be^{-\gamma x},$$ or equivalently $$u(x)=(A+B)\cosh{\gamma x}+(A-B)\sinh{\gamma x}\equiv C_1\cosh{\gamma x}+C_2\sinh{\gamma x}.$$ I would like to demonstrate, that both are equivalent to $$u(x)=D\cosh{(\gamma x +x_0)}.$$ How do you go about doing this (if it is indeed possible)? I tried to do it the same way you do it for regular trig functions but cosh is undefined for values less than 1.","Consider the following 2nd order ODE: $$\frac{d^2u}{dx^2}-\gamma^2u=0.$$ This equation has solutions of the form $$u(x)=Ae^{\gamma x}+Be^{-\gamma x},$$ or equivalently $$u(x)=(A+B)\cosh{\gamma x}+(A-B)\sinh{\gamma x}\equiv C_1\cosh{\gamma x}+C_2\sinh{\gamma x}.$$ I would like to demonstrate, that both are equivalent to $$u(x)=D\cosh{(\gamma x +x_0)}.$$ How do you go about doing this (if it is indeed possible)? I tried to do it the same way you do it for regular trig functions but cosh is undefined for values less than 1.",,"['ordinary-differential-equations', 'trigonometry']"
10,Do springs and projectiles under gravity exist in the same family?,Do springs and projectiles under gravity exist in the same family?,,"Background A 1D vertical spring subject to gravity satisfies Hook's Law: $m x''(t) = -kx(t) + g$ where $m$ is the mass at the end of the spring, $x(t)$ is the position of the mass at time $t$, $x''(t)$ is its acceleration, $k$ is the stiffness coefficient, and $g$ is the gravitational force. Solving this ODE (I used maple, shame on me), I get: $x \left( t \right) =\sin(\frac{\sqrt{k}t}{\sqrt {m}})c_1+\cos(\frac{\sqrt {k}t}{\sqrt {m}})c_2 + {\frac {g}{k}},$ where $c_1$ and $c_2$ are integration constants. If my mass were detached from the spring, then this amounts to setting $k=0$ and the mass falls according to: $mx''(t) = g$ and solving this produces– as expected –a quadratically increasing displacement: $x(t) = \frac{g}{2m}t^2 + c_3 t + c_4$ where $c_3$ and $c_4$ are integration constants. With initial conditions: We can simplify this problem further by assuming homogenous initial conditions in both cases: $x(0) = x'(0) = 0$. For the spring solution we get: $x(t) = -\frac{g}{k}cos(\frac{\sqrt{k}}{\sqrt{m}}t) + \frac{g}{k}$ and for the falling object we get: $x(t) = \frac{g}{2m}t^2$. Question Because of the divide by $k$ I cannot simply consider $k=0$ to get from the spring solution to the falling object solution. Naively taking the limit seems to produce $x(t) = \infty$. Is there a correct way to take the limit or to solve the ODE so that I get a solution that transitions from the oscillatory spring into the falling object as $k\rightarrow 0$? Note: the conversation in the comments here mention similar behavior but without a solution to the specific question. Bonus points for convincing maple to do this since my actual problem is a more complicated of this simple scenario.","Background A 1D vertical spring subject to gravity satisfies Hook's Law: $m x''(t) = -kx(t) + g$ where $m$ is the mass at the end of the spring, $x(t)$ is the position of the mass at time $t$, $x''(t)$ is its acceleration, $k$ is the stiffness coefficient, and $g$ is the gravitational force. Solving this ODE (I used maple, shame on me), I get: $x \left( t \right) =\sin(\frac{\sqrt{k}t}{\sqrt {m}})c_1+\cos(\frac{\sqrt {k}t}{\sqrt {m}})c_2 + {\frac {g}{k}},$ where $c_1$ and $c_2$ are integration constants. If my mass were detached from the spring, then this amounts to setting $k=0$ and the mass falls according to: $mx''(t) = g$ and solving this produces– as expected –a quadratically increasing displacement: $x(t) = \frac{g}{2m}t^2 + c_3 t + c_4$ where $c_3$ and $c_4$ are integration constants. With initial conditions: We can simplify this problem further by assuming homogenous initial conditions in both cases: $x(0) = x'(0) = 0$. For the spring solution we get: $x(t) = -\frac{g}{k}cos(\frac{\sqrt{k}}{\sqrt{m}}t) + \frac{g}{k}$ and for the falling object we get: $x(t) = \frac{g}{2m}t^2$. Question Because of the divide by $k$ I cannot simply consider $k=0$ to get from the spring solution to the falling object solution. Naively taking the limit seems to produce $x(t) = \infty$. Is there a correct way to take the limit or to solve the ODE so that I get a solution that transitions from the oscillatory spring into the falling object as $k\rightarrow 0$? Note: the conversation in the comments here mention similar behavior but without a solution to the specific question. Bonus points for convincing maple to do this since my actual problem is a more complicated of this simple scenario.",,"['ordinary-differential-equations', 'limits', 'physics', 'maple', 'projectile-motion']"
11,How can we apply the method of separation of variables?,How can we apply the method of separation of variables?,,"I want to check if the method of separation of variables can be used for the replacement of the following given partial differential equation from a pair of ordinary differential equations. If so, I want to find the equations. $u_{xx}+(x+y) u_{yy}=0$ Suppose that $u$ is of the form $u(x,y)=X(x) Y(y)$. Then  $u_{xx}+(x+y) u_{yy}=0 \Rightarrow X''(x) Y(y)+(x+y) X(x) Y''(y)=0 $. So we see that we cannot use the method. But in order to apply the method, we could set $z=x+y$. But then how do we proceed? Do we find the derivative of z as or x? EDIT :Let $z=x+y$. We have that $$\frac{dX}{dx}=\frac{dX}{dz}\cdot \frac{dz}{dx}=\frac{dX}{dz}$$ and $$\frac{d^2X}{dx^2}=\frac{d}{dx}\left (\frac{dX}{dz}\right )=\frac{d}{dz}\frac{dX}{dz}\cdot \frac{dz}{dx}=\frac{d^2X}{dz^2}$$ Then we have $$\frac{d^2X}{dx^2}\cdot Y+(x+y)\cdot X\cdot \frac{d^2Y}{dy^2}=0 \\ \Rightarrow \frac{d^2X}{dz^2}\cdot Y+z\cdot X\cdot \frac{d^2Y}{dy^2}=0 \\ \Rightarrow \frac{d^2X}{dz^2}\cdot Y=-z\cdot X\cdot \frac{d^2Y}{dy^2} \\ \Rightarrow \frac{1}{z\cdot X}\cdot \frac{d^2X}{dz^2}=- \frac{1}{Y}\cdot \frac{d^2Y}{dy^2}$$ But won't $X$ be a variable of both $y$ and $z$ since $x=z-y$? Or do we get somehow that $X$ will depend only on $z$?","I want to check if the method of separation of variables can be used for the replacement of the following given partial differential equation from a pair of ordinary differential equations. If so, I want to find the equations. $u_{xx}+(x+y) u_{yy}=0$ Suppose that $u$ is of the form $u(x,y)=X(x) Y(y)$. Then  $u_{xx}+(x+y) u_{yy}=0 \Rightarrow X''(x) Y(y)+(x+y) X(x) Y''(y)=0 $. So we see that we cannot use the method. But in order to apply the method, we could set $z=x+y$. But then how do we proceed? Do we find the derivative of z as or x? EDIT :Let $z=x+y$. We have that $$\frac{dX}{dx}=\frac{dX}{dz}\cdot \frac{dz}{dx}=\frac{dX}{dz}$$ and $$\frac{d^2X}{dx^2}=\frac{d}{dx}\left (\frac{dX}{dz}\right )=\frac{d}{dz}\frac{dX}{dz}\cdot \frac{dz}{dx}=\frac{d^2X}{dz^2}$$ Then we have $$\frac{d^2X}{dx^2}\cdot Y+(x+y)\cdot X\cdot \frac{d^2Y}{dy^2}=0 \\ \Rightarrow \frac{d^2X}{dz^2}\cdot Y+z\cdot X\cdot \frac{d^2Y}{dy^2}=0 \\ \Rightarrow \frac{d^2X}{dz^2}\cdot Y=-z\cdot X\cdot \frac{d^2Y}{dy^2} \\ \Rightarrow \frac{1}{z\cdot X}\cdot \frac{d^2X}{dz^2}=- \frac{1}{Y}\cdot \frac{d^2Y}{dy^2}$$ But won't $X$ be a variable of both $y$ and $z$ since $x=z-y$? Or do we get somehow that $X$ will depend only on $z$?",,"['ordinary-differential-equations', 'partial-differential-equations']"
12,Find a function f(u) such that the following ODE becomes exact.,Find a function f(u) such that the following ODE becomes exact.,,"So I'm a bit puzzled with this question: Find a function f(u) such that this function becomes exact: $$\ f(x+y)+\ln(x)+(e^{x+y}+y^2)y'=0,$$ my initial thoughts were setting $$\ P(x,y):= f(x+y)+\ln(x); Q(x,y):=e^{x+y}+y^2,$$ then$$\dfrac{\partial P}{\partial y}=f'(x+y)=e^{x+y}=\dfrac{\partial Q}{\partial x}, $$ ${}{}{}$ so $$\ f(x+y)=e^{x+y}, $$ Is this right? Thanks in advance. EDIT: to show correct answer, thanks everyone!","So I'm a bit puzzled with this question: Find a function f(u) such that this function becomes exact: $$\ f(x+y)+\ln(x)+(e^{x+y}+y^2)y'=0,$$ my initial thoughts were setting $$\ P(x,y):= f(x+y)+\ln(x); Q(x,y):=e^{x+y}+y^2,$$ then$$\dfrac{\partial P}{\partial y}=f'(x+y)=e^{x+y}=\dfrac{\partial Q}{\partial x}, $$ ${}{}{}$ so $$\ f(x+y)=e^{x+y}, $$ Is this right? Thanks in advance. EDIT: to show correct answer, thanks everyone!",,['ordinary-differential-equations']
13,Integral transform in finite range,Integral transform in finite range,,"Fourier transform is an integral transform in infinite range, it can be used for solving constant coefficient linear differential equations defined in $(-\infty,+\infty)$. Laplace transform is an integral transform in semi-infinite range, it can be used for solving initial value problem (IVP) of constant coefficient linear differential equations defined in $[0,+\infty)$. Then does there exist integral transform(s) in finite range so I can use it for solving boundary value problem (BVP) of constant coefficient linear differential equations defined in e.g. $[a,b]$? To make the question more specific, can I solve the following BVP $$\frac{\partial ^2u(x,y)}{\partial x^2}+\frac{\partial ^2u(x,y)}{\partial y^2}=1$$ $$u(0,y)=0,\ u(1,y)=0$$ $$u(x,0)=0,\ u(x,2)=0$$ in $[0,1]×[0,2]$ with some kind of integral transform? (Yeah I don't want to use separation of variable. )","Fourier transform is an integral transform in infinite range, it can be used for solving constant coefficient linear differential equations defined in $(-\infty,+\infty)$. Laplace transform is an integral transform in semi-infinite range, it can be used for solving initial value problem (IVP) of constant coefficient linear differential equations defined in $[0,+\infty)$. Then does there exist integral transform(s) in finite range so I can use it for solving boundary value problem (BVP) of constant coefficient linear differential equations defined in e.g. $[a,b]$? To make the question more specific, can I solve the following BVP $$\frac{\partial ^2u(x,y)}{\partial x^2}+\frac{\partial ^2u(x,y)}{\partial y^2}=1$$ $$u(0,y)=0,\ u(1,y)=0$$ $$u(x,0)=0,\ u(x,2)=0$$ in $[0,1]×[0,2]$ with some kind of integral transform? (Yeah I don't want to use separation of variable. )",,"['ordinary-differential-equations', 'partial-differential-equations', 'integral-transforms']"
14,"Quotient Rule, finding a special case","Quotient Rule, finding a special case",,Quotient rule for functions is straightforward: $g(x)/h(x)$ Can I find a pair of functions not constant where the quotient of the derivative is the derivative of the quotient?,Quotient rule for functions is straightforward: $g(x)/h(x)$ Can I find a pair of functions not constant where the quotient of the derivative is the derivative of the quotient?,,"['calculus', 'ordinary-differential-equations', 'derivatives']"
15,The reverse of finding the arc length,The reverse of finding the arc length,,"As we know, finding the arc length of a function $f(x)$ from $x=a$ to $x=b$ is straightforward and can be implemented numerically. For a particular function $f(x)$ that I have, suppose that I have numerically computed its arclength to be approximately $L$ using Simpson's rule. Now, for a set of values $\ell_k$ such that $0 \le \ell_k \le L$, I want to find $x_k\in [a,b]$ for which the arc length of $f(x)$ from $x=a$ to $x = x_k$ is $\ell_k$. I am not sure what's the best way to do this numerically but I followed the suggestion here . That is, if $L(x)$ is the arclength function, $L(x) = \int_a^x \sqrt{1+f'(x)^2} dx$ which yields a differential equation $\frac{dx}{dL} = \frac{1}{\sqrt{1+f'(x)^2}}$. Solving this differential equation numerically is then not much of a problem. I used the initial condition that the $x-$value corresponding to arclength 0 is $x=a$. Now, inevitably, numerical errors arise. That is, after solving the differential equation numerically, the $x-value$ corresponding to length $L$ is not exactly $b$. Oftentimes, it goes slightly beyond $b$. I used Matlab's ode45 to solve the differential equation. I am wondering if I can impose two conditions on this equation, i.e. $x(0) = a$ and $x(L) = b$. If so, how can I solve the ODE numerically? It seems like bvp methods don't apply. Any suggestions?","As we know, finding the arc length of a function $f(x)$ from $x=a$ to $x=b$ is straightforward and can be implemented numerically. For a particular function $f(x)$ that I have, suppose that I have numerically computed its arclength to be approximately $L$ using Simpson's rule. Now, for a set of values $\ell_k$ such that $0 \le \ell_k \le L$, I want to find $x_k\in [a,b]$ for which the arc length of $f(x)$ from $x=a$ to $x = x_k$ is $\ell_k$. I am not sure what's the best way to do this numerically but I followed the suggestion here . That is, if $L(x)$ is the arclength function, $L(x) = \int_a^x \sqrt{1+f'(x)^2} dx$ which yields a differential equation $\frac{dx}{dL} = \frac{1}{\sqrt{1+f'(x)^2}}$. Solving this differential equation numerically is then not much of a problem. I used the initial condition that the $x-$value corresponding to arclength 0 is $x=a$. Now, inevitably, numerical errors arise. That is, after solving the differential equation numerically, the $x-value$ corresponding to length $L$ is not exactly $b$. Oftentimes, it goes slightly beyond $b$. I used Matlab's ode45 to solve the differential equation. I am wondering if I can impose two conditions on this equation, i.e. $x(0) = a$ and $x(L) = b$. If so, how can I solve the ODE numerically? It seems like bvp methods don't apply. Any suggestions?",,"['calculus', 'ordinary-differential-equations', 'numerical-methods', 'matlab', 'arc-length']"
16,Doubt related to formation of differential equation,Doubt related to formation of differential equation,,"Question: Find the order of the differential equation of $$y=C_1\sin^2x+C_2\cos 2x+C_3$$ I read in my book that the order of the differential equation is equal to the number of arbitrary constants but the answer given is $2$. Attempt: Here are two methods I tried: I calculated up to 3rd differential and obtained a differential equation. $$\begin{align} y&=C_1\sin^2x+C_2\cos2x+C_3\\ y'&=C_1\sin2x-2C_2\sin2x\\ y''&=2C_1\cos2x-4C_2\cos2x\\ y'''&=-4C_1\sin2x+8C_2\sin2x\\ &=-4(C_1\sin2x-2C_2\sin2x)\\ &=-4y' \end{align}$$ I differentiated both sides w.r.t. $x$ and then sent the $\sin2x$ term, which I was getting in RHS, to LHS and wrote $\frac1{\sin2x}$ as $\operatorname{cosec}2x$. Then I differentiated both sides again w.r.t. $x$. In this way, both $C_1$ and $C_2$ which remained after calculating 1st derivative become zero. $$\begin{align} y&=C_1\sin^2x+C_2\cos2x+C_3\\ y'&=C_1\sin2x-2C_2\sin2x\\ \operatorname{cosec}2xy'&=C_1-2C_2\\ 2\operatorname{cosec}2x\cot2xy'+\operatorname{cosec}2xy''&=0\\ \implies2\cot2xy'+y''&=0 \end{align}$$ Which one is the correct method? If it's neither, what's the right method?","Question: Find the order of the differential equation of $$y=C_1\sin^2x+C_2\cos 2x+C_3$$ I read in my book that the order of the differential equation is equal to the number of arbitrary constants but the answer given is $2$. Attempt: Here are two methods I tried: I calculated up to 3rd differential and obtained a differential equation. $$\begin{align} y&=C_1\sin^2x+C_2\cos2x+C_3\\ y'&=C_1\sin2x-2C_2\sin2x\\ y''&=2C_1\cos2x-4C_2\cos2x\\ y'''&=-4C_1\sin2x+8C_2\sin2x\\ &=-4(C_1\sin2x-2C_2\sin2x)\\ &=-4y' \end{align}$$ I differentiated both sides w.r.t. $x$ and then sent the $\sin2x$ term, which I was getting in RHS, to LHS and wrote $\frac1{\sin2x}$ as $\operatorname{cosec}2x$. Then I differentiated both sides again w.r.t. $x$. In this way, both $C_1$ and $C_2$ which remained after calculating 1st derivative become zero. $$\begin{align} y&=C_1\sin^2x+C_2\cos2x+C_3\\ y'&=C_1\sin2x-2C_2\sin2x\\ \operatorname{cosec}2xy'&=C_1-2C_2\\ 2\operatorname{cosec}2x\cot2xy'+\operatorname{cosec}2xy''&=0\\ \implies2\cot2xy'+y''&=0 \end{align}$$ Which one is the correct method? If it's neither, what's the right method?",,"['calculus', 'ordinary-differential-equations']"
17,Solving a differential equation with a factor $2$ in the argument of the unknown function.,Solving a differential equation with a factor  in the argument of the unknown function.,2,"The well-known trig identity $$2\sin(x)\cos(x) = \sin(2x)$$ gives use the differential equation (which I'm not even sure qualifies as an ODE, given the factor in the argument?) $$2f(x)f'(x) = f(2x)$$ Assuming one had no idea that $\sin(x)$ is a solution to this equation, I was wondering if there was a way to derive the solution anyway. My attempt Assume that $f(x)$ has a power series expansion valid for all values of $x$ that we are interested in, that is $$f(x) = \sum_{n=0}^{\infty}a_nx^n$$ for some coefficients $a_n$. Plugging this in gives us $$\sum_{n=0}^\infty 2^na_nx^n=2\left(\sum_{n=0}^\infty a_nx^n\right)\left(\sum_{n=0}^\infty (n+1)a_{n+1}x^n\right) \tag1$$ Assuming absolute convergence, we can rewrite the RHS using the Cauchy product formula: \begin{align} \left(\sum_{n=0}^\infty a_nx^n\right)\left(\sum_{n=0}^\infty (n+1)a_{n+1}x^n\right)&=\sum_{n=0}^\infty\sum_{k=0}^n(k+1)a_{k+1}x^ka_{n-k}x^{n-k}\\ &=\sum_{n=0}^\infty x^n\sum_{k=0}^n(k+1)a_{k+1}a_{n-k} \end{align} Plugging this into $(1)$ and comparing coefficients of powers of $x$, we get: $$a_n=2^{1-n}\sum_{k=0}^n(k+1)a_{k+1}a_{n-k}$$ However, I have no idea how to proceed with this recurrence relation for the coefficients of the power series that I am searching for. Therefore, my two questions are: $1.$ Is my approach correct and is there a way to solve this recurrence relation for the known coefficients of the power series expansion of $\sin(x)?$ $2.$ Is there a general approach that is maybe less cumbersome to this kind of problem?","The well-known trig identity $$2\sin(x)\cos(x) = \sin(2x)$$ gives use the differential equation (which I'm not even sure qualifies as an ODE, given the factor in the argument?) $$2f(x)f'(x) = f(2x)$$ Assuming one had no idea that $\sin(x)$ is a solution to this equation, I was wondering if there was a way to derive the solution anyway. My attempt Assume that $f(x)$ has a power series expansion valid for all values of $x$ that we are interested in, that is $$f(x) = \sum_{n=0}^{\infty}a_nx^n$$ for some coefficients $a_n$. Plugging this in gives us $$\sum_{n=0}^\infty 2^na_nx^n=2\left(\sum_{n=0}^\infty a_nx^n\right)\left(\sum_{n=0}^\infty (n+1)a_{n+1}x^n\right) \tag1$$ Assuming absolute convergence, we can rewrite the RHS using the Cauchy product formula: \begin{align} \left(\sum_{n=0}^\infty a_nx^n\right)\left(\sum_{n=0}^\infty (n+1)a_{n+1}x^n\right)&=\sum_{n=0}^\infty\sum_{k=0}^n(k+1)a_{k+1}x^ka_{n-k}x^{n-k}\\ &=\sum_{n=0}^\infty x^n\sum_{k=0}^n(k+1)a_{k+1}a_{n-k} \end{align} Plugging this into $(1)$ and comparing coefficients of powers of $x$, we get: $$a_n=2^{1-n}\sum_{k=0}^n(k+1)a_{k+1}a_{n-k}$$ However, I have no idea how to proceed with this recurrence relation for the coefficients of the power series that I am searching for. Therefore, my two questions are: $1.$ Is my approach correct and is there a way to solve this recurrence relation for the known coefficients of the power series expansion of $\sin(x)?$ $2.$ Is there a general approach that is maybe less cumbersome to this kind of problem?",,"['ordinary-differential-equations', 'power-series']"
18,"""Proving"" the definition of the Laplace Transform from two properties","""Proving"" the definition of the Laplace Transform from two properties",,"Suppose $\mathbf{G}$ is the class of piecewise continuous functions of exponential order from $[0,\infty)$ to $\mathbb{R}$ and that $\mathcal{L}$ is a transformation defined on elements of $\mathbf{G}$ satisfying the two properties For $f,\,g\in\mathbf{G}$ and $c_1,\,c_2\in\mathbb{R}, \mathcal{L}\{c_1f+c_2g\}=c_1\mathcal{L}\{f\}+c_2\mathcal{L}\{g\}$ $\mathcal{L}\{f^\prime(t)\}=s\mathcal{L}\{f(t)\}-f(0)$ Certain elementary properties of Laplace transforms follow easily from these two. For example: $\mathcal{L}\{0\}$ follows from property ($1$). Let $f(t)=1$ for $t\ge0$. Then $\mathcal{L}\{1^\prime\}=s\mathcal{L}\{1\}-1$ therefore $$\mathcal{L}\{1\}=\dfrac{1}{s}\tag{1}$$ Using this as a basis step and using property ($2$) to prove that for $n\ge0$ $$ \mathcal{L}\{t^{n+1}\}=\dfrac{s}{n+1}\mathcal{L}\{t^n\}$$ one may establish the inductive step showing that $$\mathcal{L}\{t^n\}=\dfrac{n!}{s^{n+1}}\tag{2}$$ Also, since property ($2$) gives $\mathcal{L}\{\left(e^{at}\right)^\prime\}=s\mathcal{L}\{e^{at}\}-e^0$ we immediately get $$ \mathcal{L}\{e^{at}\}=\dfrac{1}{s-a}\tag{3}$$ From $\mathcal{L}\{(\cos t)^\prime\}=s\mathcal{L}\{\cos t\}-1$ we obtain $$ \mathcal{L}\{\sin t\}+s\mathcal{L}\{\cos t\}=1$$ and from $\mathcal{L}\{(\sin t)^\prime\}=s\mathcal{L}\{\sin t\}-0$ we obtain $$ s\mathcal{L}\{\sin t\}-\mathcal{L}\{\cos t\}=0$$ And the solution of these two gives $$ \mathcal{L}\{\sin at\}=\dfrac{a}{s^2+a^2}\tag{4}$$ $$ \mathcal{L}\{\cos at\}=\dfrac{s}{s^2+a^2}\tag{5}$$ When I first adopted this approach decades ago in a differential equations class I believed I also had a proof that $$ \mathcal{L}\{f(t)\}=\int_0^\infty e^{-st}f(t)\,dt$$ but I cannot recall it so I may have been mistaken. Can anyone prove this usual definition of the Laplace transform from the two properties at the top of this post?","Suppose $\mathbf{G}$ is the class of piecewise continuous functions of exponential order from $[0,\infty)$ to $\mathbb{R}$ and that $\mathcal{L}$ is a transformation defined on elements of $\mathbf{G}$ satisfying the two properties For $f,\,g\in\mathbf{G}$ and $c_1,\,c_2\in\mathbb{R}, \mathcal{L}\{c_1f+c_2g\}=c_1\mathcal{L}\{f\}+c_2\mathcal{L}\{g\}$ $\mathcal{L}\{f^\prime(t)\}=s\mathcal{L}\{f(t)\}-f(0)$ Certain elementary properties of Laplace transforms follow easily from these two. For example: $\mathcal{L}\{0\}$ follows from property ($1$). Let $f(t)=1$ for $t\ge0$. Then $\mathcal{L}\{1^\prime\}=s\mathcal{L}\{1\}-1$ therefore $$\mathcal{L}\{1\}=\dfrac{1}{s}\tag{1}$$ Using this as a basis step and using property ($2$) to prove that for $n\ge0$ $$ \mathcal{L}\{t^{n+1}\}=\dfrac{s}{n+1}\mathcal{L}\{t^n\}$$ one may establish the inductive step showing that $$\mathcal{L}\{t^n\}=\dfrac{n!}{s^{n+1}}\tag{2}$$ Also, since property ($2$) gives $\mathcal{L}\{\left(e^{at}\right)^\prime\}=s\mathcal{L}\{e^{at}\}-e^0$ we immediately get $$ \mathcal{L}\{e^{at}\}=\dfrac{1}{s-a}\tag{3}$$ From $\mathcal{L}\{(\cos t)^\prime\}=s\mathcal{L}\{\cos t\}-1$ we obtain $$ \mathcal{L}\{\sin t\}+s\mathcal{L}\{\cos t\}=1$$ and from $\mathcal{L}\{(\sin t)^\prime\}=s\mathcal{L}\{\sin t\}-0$ we obtain $$ s\mathcal{L}\{\sin t\}-\mathcal{L}\{\cos t\}=0$$ And the solution of these two gives $$ \mathcal{L}\{\sin at\}=\dfrac{a}{s^2+a^2}\tag{4}$$ $$ \mathcal{L}\{\cos at\}=\dfrac{s}{s^2+a^2}\tag{5}$$ When I first adopted this approach decades ago in a differential equations class I believed I also had a proof that $$ \mathcal{L}\{f(t)\}=\int_0^\infty e^{-st}f(t)\,dt$$ but I cannot recall it so I may have been mistaken. Can anyone prove this usual definition of the Laplace transform from the two properties at the top of this post?",,"['ordinary-differential-equations', 'laplace-transform']"
19,Solve non-linear differential equation with boundary conditions,Solve non-linear differential equation with boundary conditions,,"Consider this problem with a differential equation and border conditions. I have to find a differentiable function $f:[0,1]\rightarrow \mathbb{R}$ such that it will satisfy the following conditions: $$f(0)=a,f(1)=b,\ddot f=\dfrac{\dot f^2-c^2f^4}{f}$$ with $a,b,c\in \mathbb{R}$ given. Differential equations are really not my field and I really don't know how to find this function $f$ (in particular how to solve this differential equation), so I hope someone will point me out a way to proceed. As I said in the comments this problem arises from riemannian geometry: these are the conditions that a component of a geodesic must satisfy. I've tried to solve the differential equation imposing $v:=\dot f$. In this way I get $$\dfrac{\partial v}{\partial f}v=\dfrac{v^2-c^2f^4}{f}$$ but this isn't an ordinary separable differential equation and unfortunately I don't know how to proceed. Thank you. EDIT: user zwim found a solution $f=\frac{k}{c\cdot ch(kx+\phi)}$ which checks out. My question now is: how can I be sure that there aren't other solutions? If there are other solutions, which are they?","Consider this problem with a differential equation and border conditions. I have to find a differentiable function $f:[0,1]\rightarrow \mathbb{R}$ such that it will satisfy the following conditions: $$f(0)=a,f(1)=b,\ddot f=\dfrac{\dot f^2-c^2f^4}{f}$$ with $a,b,c\in \mathbb{R}$ given. Differential equations are really not my field and I really don't know how to find this function $f$ (in particular how to solve this differential equation), so I hope someone will point me out a way to proceed. As I said in the comments this problem arises from riemannian geometry: these are the conditions that a component of a geodesic must satisfy. I've tried to solve the differential equation imposing $v:=\dot f$. In this way I get $$\dfrac{\partial v}{\partial f}v=\dfrac{v^2-c^2f^4}{f}$$ but this isn't an ordinary separable differential equation and unfortunately I don't know how to proceed. Thank you. EDIT: user zwim found a solution $f=\frac{k}{c\cdot ch(kx+\phi)}$ which checks out. My question now is: how can I be sure that there aren't other solutions? If there are other solutions, which are they?",,"['analysis', 'ordinary-differential-equations', 'partial-differential-equations']"
20,How to solve laplace inverse using convolution,How to solve laplace inverse using convolution,,"The problem is find the laplace inverse of: $Y(s) =\frac{18}{s(s-6)} + \frac{2}{s-6}$. Are you able to solve this problem using convolution, by breaking the terms into: $18 * \mathscr{L} (\frac{1}{s}) * \mathscr{L}(\frac{1}{s-6})$ I tried to solve it using this method, but did not get the same solution as solving through partial fractions.   Thanks in advance for the help! The answer I found through partial fractions is: $5e^{6t}-3$ My solution is shown in the image below!","The problem is find the laplace inverse of: $Y(s) =\frac{18}{s(s-6)} + \frac{2}{s-6}$. Are you able to solve this problem using convolution, by breaking the terms into: $18 * \mathscr{L} (\frac{1}{s}) * \mathscr{L}(\frac{1}{s-6})$ I tried to solve it using this method, but did not get the same solution as solving through partial fractions.   Thanks in advance for the help! The answer I found through partial fractions is: $5e^{6t}-3$ My solution is shown in the image below!",,['ordinary-differential-equations']
21,Solving the differential equation $dy/dx=\frac{6}{x+y}$,Solving the differential equation,dy/dx=\frac{6}{x+y},"Let $ \text{  }\dfrac{dy}{dx}=\frac{6}{x+y}$ where $y(0)=0$ . Find the value of $y$ when $x+y=6$ . Let $x+y=v$ . Thus $$ \text{  }\dfrac{dy}{dx}=\dfrac{dv}{dx}-1$$ Therefore $$ \dfrac{dv}{dx}=\dfrac{6+v}{v}$$ On separating the variables and integrating, I get $$y=6\ln(x+y+6 )+C$$ Could somebody please show me where I've gone wrong?","Let where . Find the value of when . Let . Thus Therefore On separating the variables and integrating, I get Could somebody please show me where I've gone wrong?"," \text{  }\dfrac{dy}{dx}=\frac{6}{x+y} y(0)=0 y x+y=6 x+y=v  \text{  }\dfrac{dy}{dx}=\dfrac{dv}{dx}-1  \dfrac{dv}{dx}=\dfrac{6+v}{v} y=6\ln(x+y+6
)+C","['calculus', 'ordinary-differential-equations']"
22,Orthogonal trajectory of $ xy = 2$,Orthogonal trajectory of, xy = 2,I have to find orthogonal trajectory of $xy = 2$ hyperbola. Differentiating wrt x: $$x\dfrac{dy}{dx}+y = 0$$ $$\dfrac{dy}{dx} = \dfrac{-y}{x} \ \ \ \ \ ... (1) \ \ \ \ \ \ \\=\dfrac{-2}{x^2}\ \ \ \ \ ...(2)$$ Now we replace $\dfrac{dy}{dx}$ with $\dfrac{-dx}{dy}$ and solve. The problem is that eqn (1) gives wrong curve but eqn  (2) gives correct curve. Edit: From 1 and 2 here we get two different answers. Are they both correct?,I have to find orthogonal trajectory of $xy = 2$ hyperbola. Differentiating wrt x: $$x\dfrac{dy}{dx}+y = 0$$ $$\dfrac{dy}{dx} = \dfrac{-y}{x} \ \ \ \ \ ... (1) \ \ \ \ \ \ \\=\dfrac{-2}{x^2}\ \ \ \ \ ...(2)$$ Now we replace $\dfrac{dy}{dx}$ with $\dfrac{-dx}{dy}$ and solve. The problem is that eqn (1) gives wrong curve but eqn  (2) gives correct curve. Edit: From 1 and 2 here we get two different answers. Are they both correct?,,['ordinary-differential-equations']
23,Convexity of the solution of an o.d.e.,Convexity of the solution of an o.d.e.,,"Are there any results on proving the convexity of a function defined implicitly through an ode? More specifically I'm interested in proving convexity of a function $f(t,u):\mathbb{R}_{+}^{2}\rightarrow\mathbb{R}_{+}$ in $u$ ($0 \leq u  \leq 1$) for all $t$ given that it satisfies the following ode: $$ \frac{\partial f(t,u)}{\partial t}=a-\min(u,f(t,u)), $$ with initial condition $f(0)$. Update 2: Here is a more basic question (which can help answering the original one): can we show that $f(t,u)$ is nonincreasing in $u$ for all $t$? (without solving the ode) This is very intuitive: one can think of $f$ as the content of a buffer/tank at time $t$ with input rate $a$ and output rate $\min(u,f(t,u))$. The output rate is at most $u$ but also cannot exceed the current content level. By increasing $u$ we are basically increasing the potential output rate and hence it must be that $f$ is nonincreasing in $u$. Update 1: Eventually I'm interested in showing convexity when $a$ is also a function of $t$, i.e., $f$ solves $$ \frac{\partial f(t,u)}{\partial t}=a(t)-\min(u,f(t,u)). $$ This is why I'm looking for an implicit approach to prove convexity (rather than solving the ode).  When $a(t)=a$ the ode can be solved (as @daw mentioned) by considering the two cases $u<f(t,u)$ and $u \geq f(t,u)$. This leads to two ode's with solutions given by $$ f_{1}(t,u)=f_{1}(s)+(t-s)(\lambda-u),\\ f_{2}(t,u)=(f_{2}(s)-a) \exp(-(t-s))+a, $$ which characterize the solution of the original ode: Given an initial condition $f(0)$ the solution is given by one of the equations above with the possibility of crossing over to the other equation at most once. Here is a plot of the trajectory of $f$ for a fixed $u$: As you see given initial condition $f(0)=1$ the solution is given by the first (linear) equation $f_{1}$ but once it reaches $u=0.9$ it takes the exponential form of $f_{2}$. The solution is also continuous in $u$ and I'm pretty sure convex in $u$ for all $t$ even when $a=a(t)$. Here is a plot of $f(t,u)$ as a function of $u$ for differnet values of $t$: I believe something can be done by differentiating the ode with respect to $u$ and trying to show the monotonicity of the derivative, but I haven't had any luck yet.","Are there any results on proving the convexity of a function defined implicitly through an ode? More specifically I'm interested in proving convexity of a function $f(t,u):\mathbb{R}_{+}^{2}\rightarrow\mathbb{R}_{+}$ in $u$ ($0 \leq u  \leq 1$) for all $t$ given that it satisfies the following ode: $$ \frac{\partial f(t,u)}{\partial t}=a-\min(u,f(t,u)), $$ with initial condition $f(0)$. Update 2: Here is a more basic question (which can help answering the original one): can we show that $f(t,u)$ is nonincreasing in $u$ for all $t$? (without solving the ode) This is very intuitive: one can think of $f$ as the content of a buffer/tank at time $t$ with input rate $a$ and output rate $\min(u,f(t,u))$. The output rate is at most $u$ but also cannot exceed the current content level. By increasing $u$ we are basically increasing the potential output rate and hence it must be that $f$ is nonincreasing in $u$. Update 1: Eventually I'm interested in showing convexity when $a$ is also a function of $t$, i.e., $f$ solves $$ \frac{\partial f(t,u)}{\partial t}=a(t)-\min(u,f(t,u)). $$ This is why I'm looking for an implicit approach to prove convexity (rather than solving the ode).  When $a(t)=a$ the ode can be solved (as @daw mentioned) by considering the two cases $u<f(t,u)$ and $u \geq f(t,u)$. This leads to two ode's with solutions given by $$ f_{1}(t,u)=f_{1}(s)+(t-s)(\lambda-u),\\ f_{2}(t,u)=(f_{2}(s)-a) \exp(-(t-s))+a, $$ which characterize the solution of the original ode: Given an initial condition $f(0)$ the solution is given by one of the equations above with the possibility of crossing over to the other equation at most once. Here is a plot of the trajectory of $f$ for a fixed $u$: As you see given initial condition $f(0)=1$ the solution is given by the first (linear) equation $f_{1}$ but once it reaches $u=0.9$ it takes the exponential form of $f_{2}$. The solution is also continuous in $u$ and I'm pretty sure convex in $u$ for all $t$ even when $a=a(t)$. Here is a plot of $f(t,u)$ as a function of $u$ for differnet values of $t$: I believe something can be done by differentiating the ode with respect to $u$ and trying to show the monotonicity of the derivative, but I haven't had any luck yet.",,"['real-analysis', 'ordinary-differential-equations', 'convex-analysis', 'dynamical-systems', 'convex-optimization']"
24,Bounds on a system of coupled ODEs,Bounds on a system of coupled ODEs,,"Suppose we have a $1$-dimensional differential inequality $$\frac{dx}{dt} \leq x - x^3 $$ We can apply the Comparison principle to claim that if $y(t)$ is the solution to $\frac{dy}{dt} = y - y^3$, then $x(t) \leq y(t)$ (assuming $x(0) \leq y(0)$. Can we extend similar argument to a $2$-dimensional system? For example, let us consider the following system of equations $$\frac{dx_1}{dt}=x_1-x_2, \quad \quad \quad \frac{dx_2}{dt} \leq x_1 + x_2 - \frac{x_2^4}{x_1^2}$$ Is the solution to following $$\frac{dy_1}{dt}=y_1-y_2, \quad \quad \quad \frac{dy_2}{dt} = y_1 + y_2 - \frac{y_2^4}{y_1^2}$$ related with the solution of the original problem. Specifically, can we apply the Comparison principle to first say that $x_2(t) \leq y_2 (t)$ and then subsequently use it to claim $x_1(t) \geq y_1(t)$?","Suppose we have a $1$-dimensional differential inequality $$\frac{dx}{dt} \leq x - x^3 $$ We can apply the Comparison principle to claim that if $y(t)$ is the solution to $\frac{dy}{dt} = y - y^3$, then $x(t) \leq y(t)$ (assuming $x(0) \leq y(0)$. Can we extend similar argument to a $2$-dimensional system? For example, let us consider the following system of equations $$\frac{dx_1}{dt}=x_1-x_2, \quad \quad \quad \frac{dx_2}{dt} \leq x_1 + x_2 - \frac{x_2^4}{x_1^2}$$ Is the solution to following $$\frac{dy_1}{dt}=y_1-y_2, \quad \quad \quad \frac{dy_2}{dt} = y_1 + y_2 - \frac{y_2^4}{y_1^2}$$ related with the solution of the original problem. Specifically, can we apply the Comparison principle to first say that $x_2(t) \leq y_2 (t)$ and then subsequently use it to claim $x_1(t) \geq y_1(t)$?",,"['real-analysis', 'ordinary-differential-equations', 'inequality', 'control-theory', 'nonlinear-system']"
25,Leibniz rule; Solving differential equations,Leibniz rule; Solving differential equations,,"Could you help me with a question? I get stuck at ii), Define the function $$I(x):=\frac{1}{\pi} \int^\pi_0 \cos(x\sin\theta) d\theta$$ i) Via application of Leibniz rule (or otherwise) calculate $I'$ and $I''$. ii) Thus, determine non-zero value(s) of $k$ for which $I$ will be a solution to the differential equation $$k^2x^2I''+xI'+x^2I=0$$ iii) Write down the values of $I(0)$ and $I'(0)$. So far what I have is  $$I'(x)=\frac{1}{\pi} \int^\pi_0 -\sin\theta \sin(x\sin\theta) d\theta$$ $$I''(x)=\frac{1}{\pi}\int^\pi_0 -\sin^2\theta \cos(x\sin\theta) d\theta$$ No idea where to go from there :S","Could you help me with a question? I get stuck at ii), Define the function $$I(x):=\frac{1}{\pi} \int^\pi_0 \cos(x\sin\theta) d\theta$$ i) Via application of Leibniz rule (or otherwise) calculate $I'$ and $I''$. ii) Thus, determine non-zero value(s) of $k$ for which $I$ will be a solution to the differential equation $$k^2x^2I''+xI'+x^2I=0$$ iii) Write down the values of $I(0)$ and $I'(0)$. So far what I have is  $$I'(x)=\frac{1}{\pi} \int^\pi_0 -\sin\theta \sin(x\sin\theta) d\theta$$ $$I''(x)=\frac{1}{\pi}\int^\pi_0 -\sin^2\theta \cos(x\sin\theta) d\theta$$ No idea where to go from there :S",,"['ordinary-differential-equations', 'differential']"
26,Drawing conclusions from a differential inequality,Drawing conclusions from a differential inequality,,"Let $f(x)$ be a smooth real function defined on $x>0$. It is given that: $f$ is an increasing function ($f'(x)>0$ for all $x>0$). $x \cdot f'(x)$ is a decreasing function. I am trying to prove that: $$ \lim_{x\to 0}f(x) = -\infty $$ EXAMPLE: $f(x) = -x^{-q}$, for some constant $q>0$. Then $f$ is increasing, $x\cdot f'(x) = q x^{-q}$ is decreasing, and indeed $ \lim_{x\to 0}f(x) = -\infty $. If this is not true, what other conditions are required to make it true?","Let $f(x)$ be a smooth real function defined on $x>0$. It is given that: $f$ is an increasing function ($f'(x)>0$ for all $x>0$). $x \cdot f'(x)$ is a decreasing function. I am trying to prove that: $$ \lim_{x\to 0}f(x) = -\infty $$ EXAMPLE: $f(x) = -x^{-q}$, for some constant $q>0$. Then $f$ is increasing, $x\cdot f'(x) = q x^{-q}$ is decreasing, and indeed $ \lim_{x\to 0}f(x) = -\infty $. If this is not true, what other conditions are required to make it true?",,"['real-analysis', 'ordinary-differential-equations']"
27,Solution of $\frac{dy}{dx}=\frac{1}{xy(x^2 \sin y^2+1)}$,Solution of,\frac{dy}{dx}=\frac{1}{xy(x^2 \sin y^2+1)},Find the solution of following differential equation: $$\frac{dy}{dx}=\frac{1}{xy(x^2 \sin (y^2)+1)}$$ Could someone hint me something to get through this problem?,Find the solution of following differential equation: $$\frac{dy}{dx}=\frac{1}{xy(x^2 \sin (y^2)+1)}$$ Could someone hint me something to get through this problem?,,"['calculus', 'integration', 'ordinary-differential-equations']"
28,Zero's of an ODE.,Zero's of an ODE.,,"I like to prove that any non-trivial solution of the linear differential equation$$y^{''}-q(x)y=0$$ (where $q(x)$ is positive monotonically increasing continuous function of $x$) has most one zero. It is clear that $y$ can not have repeated root as it is non trivial solution. Now what happens if $y$ has two distinct roots say $x_{0},y_{0}$ i.e $y(x_{0})=0=y(y_{0})$ which gives $y^{'}(c)=0$ for some $c\in(x_{0},y_{0}).$ Now i am stuck. I also like to find the nature of $y$ and $y'$ as $x\rightarrow\infty$.  Please help me. Thanks.","I like to prove that any non-trivial solution of the linear differential equation$$y^{''}-q(x)y=0$$ (where $q(x)$ is positive monotonically increasing continuous function of $x$) has most one zero. It is clear that $y$ can not have repeated root as it is non trivial solution. Now what happens if $y$ has two distinct roots say $x_{0},y_{0}$ i.e $y(x_{0})=0=y(y_{0})$ which gives $y^{'}(c)=0$ for some $c\in(x_{0},y_{0}).$ Now i am stuck. I also like to find the nature of $y$ and $y'$ as $x\rightarrow\infty$.  Please help me. Thanks.",,['ordinary-differential-equations']
29,Boundary layers: approximately satisfying BC,Boundary layers: approximately satisfying BC,,"I am working on a boundary layer problem for a second order linear ODE. A simpler problem which I think still illustrates the issue I am having is $$\varepsilon y''-y'+y=0,y(0)=0,y(1)=1$$ where $\varepsilon > 0$ is a small parameter. This problem (unlike my actual problem) in fact admits an exact solution, namely $$\frac{e^{\lambda_2 x}-e^{\lambda_1 x}}{e^{\lambda_2}-e^{\lambda_1}}$$ where $$\lambda_1,\lambda_2=\frac{1 \pm \sqrt{1-4\varepsilon}}{2 \varepsilon}.$$ For small $\varepsilon > 0$, $\lambda_1$ is a large positive number, namely $\frac{1}{\varepsilon}+O(1)$, while $\lambda_2$ is a positive number of order $1$, namely $1+O(\varepsilon)$. Upon sorting out minus signs, this means that the solution above grows very fast near $1$ and is very small far away from $1$. Accordingly, I've developed an ""inner"" boundary layer solution near $x=1$. This goes through the usual way by changing variables to $z=\frac{x-1}{\varepsilon}$, assuming that $\frac{d^2 y}{dz^2},\frac{dy}{dz}$ and $y$ are all $O(1)$ near $x=1$, and balancing the leading order terms. From this I get a leading order solution of $e^{\frac{x-1}{\varepsilon}}$ for $|x-1|=O(\varepsilon)$. Now my problem arises on the rest of the interval. For the leading order outer solution, we consider $y'=y$, and get $y=Ce^x$. My problem is now essentially in identifying $C$. I can take it to be $0$ to match the left boundary condition, but this doesn't seem right, because the composite solution from this procedure is $e^{\frac{x-1}{\varepsilon}}$ over the whole interval, which fails to satisfy the left boundary condition (at least exactly). I can subtract off $e^{-\frac{1}{\varepsilon}}$ to force the left boundary condition to hold (which has a negligible impact on the error in the DE itself), but then the right boundary condition doesn't hold. Should I be thinking about this differently? Do I need to go to higher order in order to get a reasonable result? It seems to me that I will, because a ""reasonable"" first order linear ODE beginning with value zero will always just stay at zero. So to start at zero and not stay there I need to consider a truly second order problem. But this is more difficult in my actual problem, and it seems that the method should be more or less universal.","I am working on a boundary layer problem for a second order linear ODE. A simpler problem which I think still illustrates the issue I am having is $$\varepsilon y''-y'+y=0,y(0)=0,y(1)=1$$ where $\varepsilon > 0$ is a small parameter. This problem (unlike my actual problem) in fact admits an exact solution, namely $$\frac{e^{\lambda_2 x}-e^{\lambda_1 x}}{e^{\lambda_2}-e^{\lambda_1}}$$ where $$\lambda_1,\lambda_2=\frac{1 \pm \sqrt{1-4\varepsilon}}{2 \varepsilon}.$$ For small $\varepsilon > 0$, $\lambda_1$ is a large positive number, namely $\frac{1}{\varepsilon}+O(1)$, while $\lambda_2$ is a positive number of order $1$, namely $1+O(\varepsilon)$. Upon sorting out minus signs, this means that the solution above grows very fast near $1$ and is very small far away from $1$. Accordingly, I've developed an ""inner"" boundary layer solution near $x=1$. This goes through the usual way by changing variables to $z=\frac{x-1}{\varepsilon}$, assuming that $\frac{d^2 y}{dz^2},\frac{dy}{dz}$ and $y$ are all $O(1)$ near $x=1$, and balancing the leading order terms. From this I get a leading order solution of $e^{\frac{x-1}{\varepsilon}}$ for $|x-1|=O(\varepsilon)$. Now my problem arises on the rest of the interval. For the leading order outer solution, we consider $y'=y$, and get $y=Ce^x$. My problem is now essentially in identifying $C$. I can take it to be $0$ to match the left boundary condition, but this doesn't seem right, because the composite solution from this procedure is $e^{\frac{x-1}{\varepsilon}}$ over the whole interval, which fails to satisfy the left boundary condition (at least exactly). I can subtract off $e^{-\frac{1}{\varepsilon}}$ to force the left boundary condition to hold (which has a negligible impact on the error in the DE itself), but then the right boundary condition doesn't hold. Should I be thinking about this differently? Do I need to go to higher order in order to get a reasonable result? It seems to me that I will, because a ""reasonable"" first order linear ODE beginning with value zero will always just stay at zero. So to start at zero and not stay there I need to consider a truly second order problem. But this is more difficult in my actual problem, and it seems that the method should be more or less universal.",,"['ordinary-differential-equations', 'asymptotics', 'perturbation-theory']"
30,Pharmacokinetics differential equations; with equal absorption and elimination constants.,Pharmacokinetics differential equations; with equal absorption and elimination constants.,,"There are three closely related questions for the single compartment model applied to oral dosing in pharmacokinetics: The time evolution of blood concentrations, $t=0$ to $t=\infty$, given one and only one dose to the stomach at $t=0$; and, The equilibrium state time evolution of blood concentrations in between doses, $t=0$ at the instant a dose is taken and $t=\tau$ at the instant the next dose is taken (which then sets $t=0$ again), assuming regularly spaced doses, given that enough time has passed to reach that equilibrium state ($t \to \infty$); and, The fuller time evolution of blood concentrations, $t=0$ to $t=\infty$, also assuming regularly spaced doses, and also given that the first dose starts at $t=0$ (each dose occurs at $t=n\tau$, where integer $n \ge 0$ and where $\tau$ is the dose interval.) In each of the above cases, there is an assumed single absorption rate ($k_a$) to model the transfer from the stomach (mouth/tongue, esophagus, stomach, and the rest of the gut) to the blood stream and another assumed single elimination rate ($k_e$) to model the removal from the blood stream. Let's call $A$ the amount of the dose remaining in the gut and awaiting absorption into the blood and $E$ the amount residing in the blood stream and awaiting elimination. (We'll avoid confounding issues about blood volume and concentration, effectiveness, etc.) The set up for case 1 above is simple: \begin{equation} dA = -k_a \cdot A \, dt \\ dE = k_a\cdot A \, dt - k_e \cdot E \, dt \end{equation} and I'm able to solve this when $k_a \ne k_e$ and when $k_a = k_e$. Just to dot the i, regarding case 1, here's the logic I've applied. (I apologize for doing the obvious here.) Assume that D is the single dose amount, which occurs at $t=0$. So the initial condition is $A_0 = D$. For the stomach: \begin{equation} dA = -k_a \cdot A \, dt, \,\,\,\text{where $A_0=D$} \\ \frac{dA}{A} = -k_a \, dt \\ \int \frac{dA}{A} = \int -k_a \, dt \\ ln(A_t) = -k_a \cdot t + C_0 \\ A_t = D \cdot e^{-k_a \cdot t} \end{equation} For the blood: \begin{equation} dE = k_a\cdot A \, dt - k_e \cdot E \, dt, \,\,\,\text{where $E_0=0$} \\ dE = k_a\cdot D \cdot e^{-k_a \cdot t} \, dt - k_e \cdot E \, dt \\ \frac{dE}{dt} = k_a\cdot D \cdot e^{-k_a \cdot t} - k_e \cdot E \\ \frac{dE}{dt} + k_e \cdot E = k_a\cdot D \cdot e^{-k_a \cdot t} \\ \text{setting integrating factor $\mu=e^{\int k_e \, dt}=e^{k_e \cdot t}$}, then \\ E = \frac{1}{\mu} \int_0^t \mu \cdot k_a \cdot D \cdot e^{-k_a \cdot s} \, ds \\ E = D\cdot e^{-k_e \cdot t}\cdot k_a \int_0^t e^{\left(k_e-k_a\right) \cdot s} \, ds \\ \text{which resolves one of two ways,} \\ E_t = D \cdot k \cdot e^{-k_e \cdot t} \cdot t, \,\,\,\text{where $k = k_a = k_e$} \\ E_t = D \cdot \frac{k_a}{k_a - k_e} \cdot \left( e^{-k_e \cdot t} - e^{-k_a \cdot t} \right), \,\,\,\text{where $k_a \ne k_e$} \end{equation} I get that far without difficulty. Per the above development, I've also found two related questions have already been asked and answered on stackexchange . These are the relatively clearly stated single compartment model, single dose, absorption rate equals elimination rate and the somewhat more confused question on the same single compartment model, single dose, absorption and elimination rates not necessarily the same . Unfortunately, neither of these address case 2 and case 3. Since I already understand case 1, those cases on stackexchange don't further my understanding at all. I'm particularly interested in developing case 3 above for the situation where $k_a = k_e$. (I have the book answer, but not solution method, when $k_a \ne k_e$, so if I had those solution steps I could probably intercede and find the other case.) I tap out approximately where I'm able to find the equilibrium equation over time for the stomach value, $A$, for case 2, but not the equilibrium equation for $E$ (though I know what it should be, again from stock answers I've already found.) I also understand there isn't a single approach, but actually several areas in mathematics which can be applied depending upon where my comfort level is at. For these purposes, assume ONE year of undergrad mathematics: basically MTH 251, 252, and 253. This means NO serious use of Laplace transforms (though a glancing familiarity with the idea), no familiarity with variations of parameters methods, and also very little experience with the application of matrix methods here (though I understand them as applied to solving combinations of finite linear equations.) I need help both with the setup and the solution process for case 3 but case 2 might be used to prepare for it or else as a check to see if it falls out of the solution for case 3 where $t \to \infty$. (Case 1, of course, should match up with case 3's equation where $0 \le t < \tau$.) I apologize in advance for any perceived lack of effort or clarity in posing this question. Please accept my assurance that I've spent dozens of hours testing my own skills and attempting to find an appropriate resource that I could learn from before posting here. I'm now at the point of hoping there is someone interested enough to help educate me about solving this case-3 problem. For those needing/wanting to dig into my motivation: This pharmacokinetics question actually develops because my daughter suffers from grand mal seizures and takes drugs to help manage them. This is my personal interest here and has nothing whatever to do with homework! (It's been several decades since my first year calculus coursework.) I've taken the time to read what I could over many days' worth of searches (and hand to paper work as well, of course) and still find that I'm out of my depth in being able to develop the solutions myself. That's frustrating because, while I can find some answers stated in tables for certain questions, I can't create my own solutions to related questions as I simply lack the insight either in properly forming the problems or else find I'm lacking the methods in solving them. It may help that I have found elsewhere (without knowing how it is achieved) that a solution for case 3, where $k_a \ne k_e$, is: \begin{equation} E_t = D \cdot \frac{k_a}{k_a - k_e} \cdot \left[ \left( \frac{1 - e^{-n \cdot k_e \cdot \tau}}{1 - e^{-k_e \cdot \tau}} \right) \cdot e^{-k_e \cdot t} - \left( \frac{1 - e^{-n \cdot k_a \cdot \tau}}{1 - e^{-k_a \cdot \tau}} \right) \cdot e^{-k_a \cdot t} \right] \end{equation} where $n$ is the number of doses. In the case where $n=1$, you can see how this nicely resolves into: \begin{equation} E_t = D \cdot \frac{k_a}{k_a - k_e} \cdot \left( e^{-k_e \cdot t} - e^{-k_a \cdot t} \right) \end{equation} which is the solution for case 1 where $k_a \ne k_e$ (but not where $k_a = k_e$, where it is instead $E_t = D \cdot t \cdot e^{-k \cdot t}$, where $k = k_a = k_e$) In the case where $n \to \infty$, you can see how this nicely resolves into: \begin{equation} E_t = D \cdot \frac{k_a}{k_a - k_e} \cdot \left[ \frac{e^{-k_e \cdot t}}{1 - e^{-k_e \cdot \tau}}  - \frac{e^{-k_a \cdot t}}{1 - e^{-k_a \cdot \tau}}  \right] \end{equation} which is the solution for case 2 where $k_a \ne k_e$ (but not where $k_a = k_e$.) Note that this case 3 situation (where I can't develop my own work, sadly) only works where $k_a \ne k_e$. I'd like to find the answer where $k_a = k_e$. But I'd also, of course, like to know how to solve it either way. Not just the answers, but the approach.","There are three closely related questions for the single compartment model applied to oral dosing in pharmacokinetics: The time evolution of blood concentrations, $t=0$ to $t=\infty$, given one and only one dose to the stomach at $t=0$; and, The equilibrium state time evolution of blood concentrations in between doses, $t=0$ at the instant a dose is taken and $t=\tau$ at the instant the next dose is taken (which then sets $t=0$ again), assuming regularly spaced doses, given that enough time has passed to reach that equilibrium state ($t \to \infty$); and, The fuller time evolution of blood concentrations, $t=0$ to $t=\infty$, also assuming regularly spaced doses, and also given that the first dose starts at $t=0$ (each dose occurs at $t=n\tau$, where integer $n \ge 0$ and where $\tau$ is the dose interval.) In each of the above cases, there is an assumed single absorption rate ($k_a$) to model the transfer from the stomach (mouth/tongue, esophagus, stomach, and the rest of the gut) to the blood stream and another assumed single elimination rate ($k_e$) to model the removal from the blood stream. Let's call $A$ the amount of the dose remaining in the gut and awaiting absorption into the blood and $E$ the amount residing in the blood stream and awaiting elimination. (We'll avoid confounding issues about blood volume and concentration, effectiveness, etc.) The set up for case 1 above is simple: \begin{equation} dA = -k_a \cdot A \, dt \\ dE = k_a\cdot A \, dt - k_e \cdot E \, dt \end{equation} and I'm able to solve this when $k_a \ne k_e$ and when $k_a = k_e$. Just to dot the i, regarding case 1, here's the logic I've applied. (I apologize for doing the obvious here.) Assume that D is the single dose amount, which occurs at $t=0$. So the initial condition is $A_0 = D$. For the stomach: \begin{equation} dA = -k_a \cdot A \, dt, \,\,\,\text{where $A_0=D$} \\ \frac{dA}{A} = -k_a \, dt \\ \int \frac{dA}{A} = \int -k_a \, dt \\ ln(A_t) = -k_a \cdot t + C_0 \\ A_t = D \cdot e^{-k_a \cdot t} \end{equation} For the blood: \begin{equation} dE = k_a\cdot A \, dt - k_e \cdot E \, dt, \,\,\,\text{where $E_0=0$} \\ dE = k_a\cdot D \cdot e^{-k_a \cdot t} \, dt - k_e \cdot E \, dt \\ \frac{dE}{dt} = k_a\cdot D \cdot e^{-k_a \cdot t} - k_e \cdot E \\ \frac{dE}{dt} + k_e \cdot E = k_a\cdot D \cdot e^{-k_a \cdot t} \\ \text{setting integrating factor $\mu=e^{\int k_e \, dt}=e^{k_e \cdot t}$}, then \\ E = \frac{1}{\mu} \int_0^t \mu \cdot k_a \cdot D \cdot e^{-k_a \cdot s} \, ds \\ E = D\cdot e^{-k_e \cdot t}\cdot k_a \int_0^t e^{\left(k_e-k_a\right) \cdot s} \, ds \\ \text{which resolves one of two ways,} \\ E_t = D \cdot k \cdot e^{-k_e \cdot t} \cdot t, \,\,\,\text{where $k = k_a = k_e$} \\ E_t = D \cdot \frac{k_a}{k_a - k_e} \cdot \left( e^{-k_e \cdot t} - e^{-k_a \cdot t} \right), \,\,\,\text{where $k_a \ne k_e$} \end{equation} I get that far without difficulty. Per the above development, I've also found two related questions have already been asked and answered on stackexchange . These are the relatively clearly stated single compartment model, single dose, absorption rate equals elimination rate and the somewhat more confused question on the same single compartment model, single dose, absorption and elimination rates not necessarily the same . Unfortunately, neither of these address case 2 and case 3. Since I already understand case 1, those cases on stackexchange don't further my understanding at all. I'm particularly interested in developing case 3 above for the situation where $k_a = k_e$. (I have the book answer, but not solution method, when $k_a \ne k_e$, so if I had those solution steps I could probably intercede and find the other case.) I tap out approximately where I'm able to find the equilibrium equation over time for the stomach value, $A$, for case 2, but not the equilibrium equation for $E$ (though I know what it should be, again from stock answers I've already found.) I also understand there isn't a single approach, but actually several areas in mathematics which can be applied depending upon where my comfort level is at. For these purposes, assume ONE year of undergrad mathematics: basically MTH 251, 252, and 253. This means NO serious use of Laplace transforms (though a glancing familiarity with the idea), no familiarity with variations of parameters methods, and also very little experience with the application of matrix methods here (though I understand them as applied to solving combinations of finite linear equations.) I need help both with the setup and the solution process for case 3 but case 2 might be used to prepare for it or else as a check to see if it falls out of the solution for case 3 where $t \to \infty$. (Case 1, of course, should match up with case 3's equation where $0 \le t < \tau$.) I apologize in advance for any perceived lack of effort or clarity in posing this question. Please accept my assurance that I've spent dozens of hours testing my own skills and attempting to find an appropriate resource that I could learn from before posting here. I'm now at the point of hoping there is someone interested enough to help educate me about solving this case-3 problem. For those needing/wanting to dig into my motivation: This pharmacokinetics question actually develops because my daughter suffers from grand mal seizures and takes drugs to help manage them. This is my personal interest here and has nothing whatever to do with homework! (It's been several decades since my first year calculus coursework.) I've taken the time to read what I could over many days' worth of searches (and hand to paper work as well, of course) and still find that I'm out of my depth in being able to develop the solutions myself. That's frustrating because, while I can find some answers stated in tables for certain questions, I can't create my own solutions to related questions as I simply lack the insight either in properly forming the problems or else find I'm lacking the methods in solving them. It may help that I have found elsewhere (without knowing how it is achieved) that a solution for case 3, where $k_a \ne k_e$, is: \begin{equation} E_t = D \cdot \frac{k_a}{k_a - k_e} \cdot \left[ \left( \frac{1 - e^{-n \cdot k_e \cdot \tau}}{1 - e^{-k_e \cdot \tau}} \right) \cdot e^{-k_e \cdot t} - \left( \frac{1 - e^{-n \cdot k_a \cdot \tau}}{1 - e^{-k_a \cdot \tau}} \right) \cdot e^{-k_a \cdot t} \right] \end{equation} where $n$ is the number of doses. In the case where $n=1$, you can see how this nicely resolves into: \begin{equation} E_t = D \cdot \frac{k_a}{k_a - k_e} \cdot \left( e^{-k_e \cdot t} - e^{-k_a \cdot t} \right) \end{equation} which is the solution for case 1 where $k_a \ne k_e$ (but not where $k_a = k_e$, where it is instead $E_t = D \cdot t \cdot e^{-k \cdot t}$, where $k = k_a = k_e$) In the case where $n \to \infty$, you can see how this nicely resolves into: \begin{equation} E_t = D \cdot \frac{k_a}{k_a - k_e} \cdot \left[ \frac{e^{-k_e \cdot t}}{1 - e^{-k_e \cdot \tau}}  - \frac{e^{-k_a \cdot t}}{1 - e^{-k_a \cdot \tau}}  \right] \end{equation} which is the solution for case 2 where $k_a \ne k_e$ (but not where $k_a = k_e$.) Note that this case 3 situation (where I can't develop my own work, sadly) only works where $k_a \ne k_e$. I'd like to find the answer where $k_a = k_e$. But I'd also, of course, like to know how to solve it either way. Not just the answers, but the approach.",,"['calculus', 'ordinary-differential-equations']"
31,Double Pendulum Cuspiness,Double Pendulum Cuspiness,,Does the curve traced out by the tip of a double pendulum have cusps?,Does the curve traced out by the tip of a double pendulum have cusps?,,"['ordinary-differential-equations', 'classical-mechanics']"
32,Jump Diffusion Infinitesimal generator,Jump Diffusion Infinitesimal generator,,"I have this difussion process $dX(t)=\mu X(t)dt+\sigma X(t)dW(t)+u X(t)  dN(t),\qquad X(0)=x > 0$ where $W(t)$ is a Brownian Motion and $N(t)$ is a Poisson process. And I need to know the infinitesimal generator but I can't . Can someone help me? $\mu,\sigma,u$ are constants. Thank you so much","I have this difussion process $dX(t)=\mu X(t)dt+\sigma X(t)dW(t)+u X(t)  dN(t),\qquad X(0)=x > 0$ where $W(t)$ is a Brownian Motion and $N(t)$ is a Poisson process. And I need to know the infinitesimal generator but I can't . Can someone help me? $\mu,\sigma,u$ are constants. Thank you so much",,"['ordinary-differential-equations', 'stochastic-processes', 'stochastic-differential-equations']"
33,Some clarification on this textbook's definition of linear ODEs?,Some clarification on this textbook's definition of linear ODEs?,,"The textbook I am reading (Zill's ""A First Course in Differential Equations with Modeling Applications) describes classifying ODEs as linear vs. nonlinear with the following statement: An nth-order ordinary differential equation $$F(x,y,y',...,y^n) = 0$$ is said to be linear if F is linear in $$y,y',...,y^n$$ This means that an nth-order ODE is linear when it is $$a_n(x)y^{(n)}+a_{n-1}(x)y^{n-1}+...+a_1(x)y'+a_0(x)y - g(x) = 0$$ I understand that the intuitive definition of linearity is that an ODE is linear if: 1) the highest degree of the dependent variable (y) and all of its derivatives is 1 2) the coefficient of the dependent variable and its derivatives is either a constant, or some term with at most the independent variable (x) 3) the dependent variable and its derivatives aren't inside of other functions like sin(y) However, the mathematical general definition is almost incomprehensible to me. I don't understand what the word ""in"" means when the author says that the function F is ""linear in"" followed by y and its derivatives separated by commas. And it does not say what g(x) means, what the collection of a(x) functions mean, etc.","The textbook I am reading (Zill's ""A First Course in Differential Equations with Modeling Applications) describes classifying ODEs as linear vs. nonlinear with the following statement: An nth-order ordinary differential equation $$F(x,y,y',...,y^n) = 0$$ is said to be linear if F is linear in $$y,y',...,y^n$$ This means that an nth-order ODE is linear when it is $$a_n(x)y^{(n)}+a_{n-1}(x)y^{n-1}+...+a_1(x)y'+a_0(x)y - g(x) = 0$$ I understand that the intuitive definition of linearity is that an ODE is linear if: 1) the highest degree of the dependent variable (y) and all of its derivatives is 1 2) the coefficient of the dependent variable and its derivatives is either a constant, or some term with at most the independent variable (x) 3) the dependent variable and its derivatives aren't inside of other functions like sin(y) However, the mathematical general definition is almost incomprehensible to me. I don't understand what the word ""in"" means when the author says that the function F is ""linear in"" followed by y and its derivatives separated by commas. And it does not say what g(x) means, what the collection of a(x) functions mean, etc.",,"['ordinary-differential-equations', 'definition']"
34,A linear but intractable PDE,A linear but intractable PDE,,"I have a PDE of the following form, from a physics problem: $$ y \left(\alpha \frac{\partial }{\partial y}+x \frac{\partial^2 }{\partial x \partial y} \right)f(x,y) = \left( z_1 + z_2 x^\alpha y^{-2} \right) f(x,y) $$ Function $f(x,y)$ is a real-space real-valued function and  $z_{1,2},\alpha$ are real numbers, generally irrational.   The latter, specifically the $z_2$ coefficient term, seems to make all of the textbook methods (characteristics, Froebnius, Fourier transform) fail.   Does any one know weather a method exists to solve this?  Apologies if this is a simple question but, well, I am a theoretical physicist and it is not simple for me.","I have a PDE of the following form, from a physics problem: $$ y \left(\alpha \frac{\partial }{\partial y}+x \frac{\partial^2 }{\partial x \partial y} \right)f(x,y) = \left( z_1 + z_2 x^\alpha y^{-2} \right) f(x,y) $$ Function $f(x,y)$ is a real-space real-valued function and  $z_{1,2},\alpha$ are real numbers, generally irrational.   The latter, specifically the $z_2$ coefficient term, seems to make all of the textbook methods (characteristics, Froebnius, Fourier transform) fail.   Does any one know weather a method exists to solve this?  Apologies if this is a simple question but, well, I am a theoretical physicist and it is not simple for me.",,['ordinary-differential-equations']
35,Show $\int_{s}^{\infty} f(x)dx = \mathcal{L} \{\frac{F(t)}{t}\}$ given $f(x) = \int_{0}^{\infty} e^{-xt}F(t)dt$,Show  given,\int_{s}^{\infty} f(x)dx = \mathcal{L} \{\frac{F(t)}{t}\} f(x) = \int_{0}^{\infty} e^{-xt}F(t)dt,I'm trying to derive this to show that $$\int_{0}^{\infty} f(x)dx = \int_{0}^{\infty} \frac{F(t)}{t} dt$$ and use that to prove $$\int_{0}^{\infty} \frac{\sin t}{t} = \frac{\pi}{2}$$ How do I go about proving that $$\int_{s}^{\infty} f(x)dx = \mathcal{L} \{\frac{F(t)}{t}\}$$ given $$f(x) = \int_{0}^{\infty} e^{-xt}F(t)dt$$,I'm trying to derive this to show that $$\int_{0}^{\infty} f(x)dx = \int_{0}^{\infty} \frac{F(t)}{t} dt$$ and use that to prove $$\int_{0}^{\infty} \frac{\sin t}{t} = \frac{\pi}{2}$$ How do I go about proving that $$\int_{s}^{\infty} f(x)dx = \mathcal{L} \{\frac{F(t)}{t}\}$$ given $$f(x) = \int_{0}^{\infty} e^{-xt}F(t)dt$$,,"['ordinary-differential-equations', 'definite-integrals', 'improper-integrals', 'laplace-transform']"
36,Solving Burgers' equation $u_y+uu_x=0$ with arbitrary initial values,Solving Burgers' equation  with arbitrary initial values,u_y+uu_x=0,"Consider the PDE $$u_y+uu_x=0$$ with the condition $u(x,0)=f(x)$. I want to solve this using the method of characteristics. The first thing I've done was to solve the characteristic system. In that case the system is $$\dfrac{dx}{dt}=u, \quad \dfrac{dy}{dt}=1, \quad \dfrac{du}{dt}=0.$$ In that case we get, with $c_1,c_2,c_3$ constants, $u=c_3$, $x=c_1+c_3t$ and $y=t+c_2$. Now as I understand, the initial condition can be thought of as a curve $\sigma(s)=(s, 0, f(s))$ which cuts the characteristics. The idea then is to write $u$ as a function os $s$ and $t$ that is $s$ picks a characteristic and $t$ the value of $u$ at some point of it. In that case if we imagine that give $s$ the characteristic is intercepted at $t=0$ we find $c_2=0$, $c_1=s$ and $c_3=f(s)$. Now for any $t$ on this characteristic $u$ is the same so $u(s,t)=f(s)$. Now we have to express $s$ and $t$ in terms of $x,y$. The point is that we easily get $t=y$, but $s=x-yf(s)$. In that case we would have $$u(x,y) = f(x-yf(s))$$ which is obviously wrong. What is wrong here? How is the right way to do this?","Consider the PDE $$u_y+uu_x=0$$ with the condition $u(x,0)=f(x)$. I want to solve this using the method of characteristics. The first thing I've done was to solve the characteristic system. In that case the system is $$\dfrac{dx}{dt}=u, \quad \dfrac{dy}{dt}=1, \quad \dfrac{du}{dt}=0.$$ In that case we get, with $c_1,c_2,c_3$ constants, $u=c_3$, $x=c_1+c_3t$ and $y=t+c_2$. Now as I understand, the initial condition can be thought of as a curve $\sigma(s)=(s, 0, f(s))$ which cuts the characteristics. The idea then is to write $u$ as a function os $s$ and $t$ that is $s$ picks a characteristic and $t$ the value of $u$ at some point of it. In that case if we imagine that give $s$ the characteristic is intercepted at $t=0$ we find $c_2=0$, $c_1=s$ and $c_3=f(s)$. Now for any $t$ on this characteristic $u$ is the same so $u(s,t)=f(s)$. Now we have to express $s$ and $t$ in terms of $x,y$. The point is that we easily get $t=y$, but $s=x-yf(s)$. In that case we would have $$u(x,y) = f(x-yf(s))$$ which is obviously wrong. What is wrong here? How is the right way to do this?",,"['ordinary-differential-equations', 'partial-differential-equations', 'characteristics']"
37,Construct a Second Order ODE given the fundamental solutions,Construct a Second Order ODE given the fundamental solutions,,"I need to construct a second order linear differential equation for which $\{ \sin (x), x \sin (x) \}$ is the set of fundamental solutions. I am completely lost on this problem and have been trying different approaches for days now. First, I tried simple variations of $ y'' + y = 0$ but I couldn't come up with anything successful. Then I tried using the identity $$ \dfrac{dW}{dx} + PW = 0 $$ and solving for $~P~$ , knowing that the Wronskian $~W~$ of the solution set is $ \sin^2 (x) $ . I got something like $ P = -\cot(x) $ so I tried variations of $$ y'' -\cot(x)y' + y = 0 $$ but again nothing worked. So I started looking through my book over and over to see what types of ODE's could produce such a solution set, and from what I can tell, there are no constant-coefficient ODE's that can possibly produce this solution set. The only way I learned to solve variable coefficient second order equations was by Cauchy-Euler's method, but I don't see how that can output such a solution set either. I've never come across a second order ODE that has the solution $ \sin(x) $ without $ \cos(x) $ .  Any ideas?","I need to construct a second order linear differential equation for which is the set of fundamental solutions. I am completely lost on this problem and have been trying different approaches for days now. First, I tried simple variations of but I couldn't come up with anything successful. Then I tried using the identity and solving for , knowing that the Wronskian of the solution set is . I got something like so I tried variations of but again nothing worked. So I started looking through my book over and over to see what types of ODE's could produce such a solution set, and from what I can tell, there are no constant-coefficient ODE's that can possibly produce this solution set. The only way I learned to solve variable coefficient second order equations was by Cauchy-Euler's method, but I don't see how that can output such a solution set either. I've never come across a second order ODE that has the solution without .  Any ideas?","\{ \sin (x), x \sin (x) \}  y'' + y = 0  \dfrac{dW}{dx} + PW = 0  ~P~ ~W~  \sin^2 (x)   P = -\cot(x)   y'' -\cot(x)y' + y = 0   \sin(x)   \cos(x) ","['ordinary-differential-equations', 'wronskian']"
38,second order differential equation to hypergeometric equation,second order differential equation to hypergeometric equation,,I know how to transform a general second order differential equation of the form $$\frac{d^2w}{dz^2} + \left(\frac{A}{z-\xi}+\frac{B}{z-\eta}\right)\frac{dw}{dz} + \frac{1}{z-\xi}\frac{1}{z-\eta}\left(\frac{D}{z-\xi}+\frac{E}{z-\eta}\right)w=0$$ into a hypergeometric equation. But now I have to solve $$\frac{d^2w}{dz^2} + \left(\frac{A}{z-\xi}+\frac{B}{z-\eta}\right)\frac{dw}{dz} + \frac{1}{z-\xi}\frac{1}{z-\eta}\left(\frac{D}{z-\xi}+\frac{E}{z-\eta}+C\right)w=0$$ in terms of hypergeometric functions and I can't seem to find the change of variables adequate to obtain an equation like the first one. I'm sure I'm missing something very simple here.,I know how to transform a general second order differential equation of the form $$\frac{d^2w}{dz^2} + \left(\frac{A}{z-\xi}+\frac{B}{z-\eta}\right)\frac{dw}{dz} + \frac{1}{z-\xi}\frac{1}{z-\eta}\left(\frac{D}{z-\xi}+\frac{E}{z-\eta}\right)w=0$$ into a hypergeometric equation. But now I have to solve $$\frac{d^2w}{dz^2} + \left(\frac{A}{z-\xi}+\frac{B}{z-\eta}\right)\frac{dw}{dz} + \frac{1}{z-\xi}\frac{1}{z-\eta}\left(\frac{D}{z-\xi}+\frac{E}{z-\eta}+C\right)w=0$$ in terms of hypergeometric functions and I can't seem to find the change of variables adequate to obtain an equation like the first one. I'm sure I'm missing something very simple here.,,"['ordinary-differential-equations', 'hypergeometric-function']"
39,The Green's function of the beam deflection equation,The Green's function of the beam deflection equation,,"This is a problem in a textbook used in my class: Suppose we have an infinite elastic beam, where the deflection $u(x)$   satisfies the differential equation $$\frac{d^4 u}{dx^4}+k^4 u = > f(x),$$ where $k^4$ is a positive constant regarded as known, and   $f(x)$ is a load. For part (a) of the question, we assume that the load is a unit   concentrated load at $x = \xi$, so that it satisfies we equation   $$\frac{d^4 u}{dx^4}+k^4 u = \delta(x - \xi),$$ where $\delta$ is the   Dirac-delta equation. So, for this problem I would like to find the deflection (which is equal to the free space Green's function). I've looked in several mechanics textbooks to find a detailed approach as to how to calculate this, but only found solutions for different problems.","This is a problem in a textbook used in my class: Suppose we have an infinite elastic beam, where the deflection $u(x)$   satisfies the differential equation $$\frac{d^4 u}{dx^4}+k^4 u = > f(x),$$ where $k^4$ is a positive constant regarded as known, and   $f(x)$ is a load. For part (a) of the question, we assume that the load is a unit   concentrated load at $x = \xi$, so that it satisfies we equation   $$\frac{d^4 u}{dx^4}+k^4 u = \delta(x - \xi),$$ where $\delta$ is the   Dirac-delta equation. So, for this problem I would like to find the deflection (which is equal to the free space Green's function). I've looked in several mechanics textbooks to find a detailed approach as to how to calculate this, but only found solutions for different problems.",,"['ordinary-differential-equations', 'distribution-theory', 'greens-function', 'fundamental-solution']"
40,Prove that ALL solutions of the differential equation y'=y are of the form y=c*e^x,Prove that ALL solutions of the differential equation y'=y are of the form y=c*e^x,,"This is my first post. :) I'm learning about solving differential equations by separation of variables and I have questions about the rigourousness of the method. As stated, I'm solving $${dy\over dx} = y$$ The method that I've been taught goes like this. Case 1: Assume $y(x) = 0$ for all $x$. Check if $y(x) = 0$ is a solution. It is. Case 2: Assume $y(x) = 0$ for no $x$. We divide by $y$ on both sides to convert into separable form. This division is justified because $y \not= 0$. We get $dy/y = dx$. ""Integrating"" both sides, we get $\ln|y| = x+c$. Rearrange to isolate $y$, we get $y = \pm e^xe^c$, and realize that $\pm e^c$ is a nonzero free constant. $\square$ The above is the approach I've been taught. The problem is that we're missing a case. How about this? Case 3: Assume $y(x) = 0$ for some $x$. I feel that a completely rigourous solution also needs to prove that there are no solutions under this case. But I don't know how to prove it. If you possess a proof, I invite you to present your answer here. I'd be delighted to read it. I've been reading a textbook that tried to prove this but I can't fill in all the missing steps myself. It goes something like this: Suppose we're not in Case 1, so that $y(x) \not= 0$ for some $x$. Since $y$ is continuous, there is some open interval around $x$ such that $y(x) \not= 0$ for all $x$ within the interval. Then all the steps in case 2 apply within this interval. But then I don't understand why this implies that $y(x) \not= 0$ outside of the interval as well. Anyway, this is only one possible proof method and I'm open to hearing others. Ideally, I'm interested a proof method that easily generalizes to other separable differential equations as well, because dividing by $y$ or a function of $y$ is quite common during the separation process.","This is my first post. :) I'm learning about solving differential equations by separation of variables and I have questions about the rigourousness of the method. As stated, I'm solving $${dy\over dx} = y$$ The method that I've been taught goes like this. Case 1: Assume $y(x) = 0$ for all $x$. Check if $y(x) = 0$ is a solution. It is. Case 2: Assume $y(x) = 0$ for no $x$. We divide by $y$ on both sides to convert into separable form. This division is justified because $y \not= 0$. We get $dy/y = dx$. ""Integrating"" both sides, we get $\ln|y| = x+c$. Rearrange to isolate $y$, we get $y = \pm e^xe^c$, and realize that $\pm e^c$ is a nonzero free constant. $\square$ The above is the approach I've been taught. The problem is that we're missing a case. How about this? Case 3: Assume $y(x) = 0$ for some $x$. I feel that a completely rigourous solution also needs to prove that there are no solutions under this case. But I don't know how to prove it. If you possess a proof, I invite you to present your answer here. I'd be delighted to read it. I've been reading a textbook that tried to prove this but I can't fill in all the missing steps myself. It goes something like this: Suppose we're not in Case 1, so that $y(x) \not= 0$ for some $x$. Since $y$ is continuous, there is some open interval around $x$ such that $y(x) \not= 0$ for all $x$ within the interval. Then all the steps in case 2 apply within this interval. But then I don't understand why this implies that $y(x) \not= 0$ outside of the interval as well. Anyway, this is only one possible proof method and I'm open to hearing others. Ideally, I'm interested a proof method that easily generalizes to other separable differential equations as well, because dividing by $y$ or a function of $y$ is quite common during the separation process.",,"['calculus', 'analysis', 'ordinary-differential-equations']"
41,Solve $y'=(x+y)^2$,Solve,y'=(x+y)^2,$y'=(x+y)^2$ The equation above is in the form $y'=P(x)y^2+Q(X)y+C(x)$ which is known as Ricatti equation. I set $z=x+y$  so ${dz\over dx}={dy\over dx}+1 \implies {dy\over dx}={dz\over dx}-1 \qquad(1)  $ From the initial equation I get ${dy\over dx}=z^2  \qquad   (2)$ Finally $(1)=(2) \implies {dz\over dx}-1=z^2 \implies {1\over z^2+1}dz=dx $ If I integrate both sides with respect to x I get $atan(z)=x+c$ where c constant Then $z=tan(x+c) \implies y=tan(x+c) -x$ Is the logic above solid or am I mistaken somewhere?,$y'=(x+y)^2$ The equation above is in the form $y'=P(x)y^2+Q(X)y+C(x)$ which is known as Ricatti equation. I set $z=x+y$  so ${dz\over dx}={dy\over dx}+1 \implies {dy\over dx}={dz\over dx}-1 \qquad(1)  $ From the initial equation I get ${dy\over dx}=z^2  \qquad   (2)$ Finally $(1)=(2) \implies {dz\over dx}-1=z^2 \implies {1\over z^2+1}dz=dx $ If I integrate both sides with respect to x I get $atan(z)=x+c$ where c constant Then $z=tan(x+c) \implies y=tan(x+c) -x$ Is the logic above solid or am I mistaken somewhere?,,['ordinary-differential-equations']
42,Using contraction mapping theorem to prove existence/uniqueness of solutions of Linear first order ODEs,Using contraction mapping theorem to prove existence/uniqueness of solutions of Linear first order ODEs,,"In class we used the contraction mapping theorem to prove the existence and uniqueness of solutions to a first order (not necessarily linear) ODE on some interval [0,h]. The method we used was this: First convert the linear ODE into an integral equation of the form $u=f(t)+\int_0^s k(t,s)g(t,u(s)) \ ds$. and define an operator $Tu=f(t)+\int_0^s k(t,s)g(t,u(s)) \ ds$. Then use the contraction mapping theorem to ensure $T$ has a fixed point (and thus the integral equation is uniquely solved) on some small enough interval $[0,h]$. My question is this: Suppose I restrict the ODE to be a linear ODE only. That is, the operator is now  $T=f(t)+\int_0^s k(t,s)u(s) \ ds$. The above contraction mapping still gives us a unique solution on $[0,h]$. Using this fact, how can I show that there is a unique solution for $[h,2h]$ and, therefore, for all intervals $[0,k]$?","In class we used the contraction mapping theorem to prove the existence and uniqueness of solutions to a first order (not necessarily linear) ODE on some interval [0,h]. The method we used was this: First convert the linear ODE into an integral equation of the form $u=f(t)+\int_0^s k(t,s)g(t,u(s)) \ ds$. and define an operator $Tu=f(t)+\int_0^s k(t,s)g(t,u(s)) \ ds$. Then use the contraction mapping theorem to ensure $T$ has a fixed point (and thus the integral equation is uniquely solved) on some small enough interval $[0,h]$. My question is this: Suppose I restrict the ODE to be a linear ODE only. That is, the operator is now  $T=f(t)+\int_0^s k(t,s)u(s) \ ds$. The above contraction mapping still gives us a unique solution on $[0,h]$. Using this fact, how can I show that there is a unique solution for $[h,2h]$ and, therefore, for all intervals $[0,k]$?",,['ordinary-differential-equations']
43,"Trouble solving this differential equation: $x'=3(x-2)$, $x(0)=-1$.","Trouble solving this differential equation: , .",x'=3(x-2) x(0)=-1,Find the solution of the differential equation $x'=3(x-2)$ given an initial value condition of $x(0)=-1$. This is my attempt: $$x'=3(x-2) \iff \frac{dx}{dt} = 3(x-2) \iff \frac{dx}{x} - 2 = 3dt$$ $$\iff \int\bigg(\frac{dx}{x}\bigg)-2 = \int(3dt + c) \iff ln|x-2| = 3 + C$$ $$\iff |x-2| = e^3 e^{c_1} \iff |x-2| = e^3 c_2$$ $$|x-2| = e^3 C$$ $$|-3| = e^3 C \iff \frac{3}{e^3} = C$$ Plug back in: $$|x-2|=e^3 C$$ Is what I have done correct?,Find the solution of the differential equation $x'=3(x-2)$ given an initial value condition of $x(0)=-1$. This is my attempt: $$x'=3(x-2) \iff \frac{dx}{dt} = 3(x-2) \iff \frac{dx}{x} - 2 = 3dt$$ $$\iff \int\bigg(\frac{dx}{x}\bigg)-2 = \int(3dt + c) \iff ln|x-2| = 3 + C$$ $$\iff |x-2| = e^3 e^{c_1} \iff |x-2| = e^3 c_2$$ $$|x-2| = e^3 C$$ $$|-3| = e^3 C \iff \frac{3}{e^3} = C$$ Plug back in: $$|x-2|=e^3 C$$ Is what I have done correct?,,"['calculus', 'integration', 'ordinary-differential-equations']"
44,Why is the solution of an ordinary differential equation required to be defined on an interval? [duplicate],Why is the solution of an ordinary differential equation required to be defined on an interval? [duplicate],,"This question already has answers here : Why is it differential equations exist on an interval instead of a domain? (2 answers) Closed 3 years ago . I am reading A First Course in Differential Equations with Modeling Applications (10th Edition) and here is a definition: Any function $\phi$, defined on an interval $I$ and possessing at least $n$ derivatives that are continuous on $I$, which when substituted into an $n$th-order ordinary differential equation reduces the equation to the identity, is said to be a solution of the equation on the interval. I am wondering why the condition ""on an interval $I$"" is important (this has been emphasized in other places of the book). For example, why it is not okay to say that $$y=x^{-1}, x\neq 0$$ is a solution to the differential equation $y'=-x^{-2}$?","This question already has answers here : Why is it differential equations exist on an interval instead of a domain? (2 answers) Closed 3 years ago . I am reading A First Course in Differential Equations with Modeling Applications (10th Edition) and here is a definition: Any function $\phi$, defined on an interval $I$ and possessing at least $n$ derivatives that are continuous on $I$, which when substituted into an $n$th-order ordinary differential equation reduces the equation to the identity, is said to be a solution of the equation on the interval. I am wondering why the condition ""on an interval $I$"" is important (this has been emphasized in other places of the book). For example, why it is not okay to say that $$y=x^{-1}, x\neq 0$$ is a solution to the differential equation $y'=-x^{-2}$?",,"['ordinary-differential-equations', 'definition']"
45,What mathematics topics pertain more towards applied mathematics?,What mathematics topics pertain more towards applied mathematics?,,"I'm entering my second year of undergrad (majoring in mathematics), and I've found that I am really bad at Linear Algebra, but very good at Calculus and Differential Equations. I'm hoping to venture onto Sci. Computing/Applied Maths, but I'm worried my inadequacy (as quite personally, unfortunate lack of interest) for Lin. Alg. will prevent me from being successful in topics such as Numerical Analysis, Algebra, as well as Scientific Computing. Does anyone in the applied maths field/experience with applied maths have any advice on what I should do? That is, what else is there like Calculus/DEs that will help me in this field? Or do i just need to buck up and get on my Lin. Alg. horse in order to get remotely close to where I want to go? I appreciate any and all input.","I'm entering my second year of undergrad (majoring in mathematics), and I've found that I am really bad at Linear Algebra, but very good at Calculus and Differential Equations. I'm hoping to venture onto Sci. Computing/Applied Maths, but I'm worried my inadequacy (as quite personally, unfortunate lack of interest) for Lin. Alg. will prevent me from being successful in topics such as Numerical Analysis, Algebra, as well as Scientific Computing. Does anyone in the applied maths field/experience with applied maths have any advice on what I should do? That is, what else is there like Calculus/DEs that will help me in this field? Or do i just need to buck up and get on my Lin. Alg. horse in order to get remotely close to where I want to go? I appreciate any and all input.",,"['linear-algebra', 'ordinary-differential-equations', 'numerical-linear-algebra', 'applications', 'computational-mathematics']"
46,Solving ODE rigorously,Solving ODE rigorously,,"I am given the ODE $$(f''(r)+\frac{f'(r)}{r})(1+f'(r)^2)-f'(r)^2f''(r)=0$$ and want to solve it rigorously for $r>0.$ So especially, I don't want to loose any solutions. $\textbf{Derivation of the solutions}$ First I multiply by $r$ $$(rf''(r)+f'(r))(1+f'(r)^2) =rf'(r)^2f''(r)$$ which is nothing but $$\frac{d}{dr}(rf'(r))(1+f'(r)^2) =rf'(r)^2f''(r)$$ now assuming $f'(r) \neq 0$ (Problem 1) I can rewrite this as $$\frac{1}{r f'(r)} \frac{d}{dr}(rf'(r)) = \frac{f'(r)f''(r)}{1+f'(r)^2}$$ Assuming $f'(r)>0$ (Problem 2) we get $$\frac{d}{dr}\ln(rf'(r)) = \frac{1}{2} \frac{d}{dr} \ln(1+f'(r)^2)$$ which gives after integrating $$\ln(r^2f'(r)^2) = ln(1+f'(r)^2)+C$$ for some constant $C.$ Exponentiating this gives $$r^2 f'(r)^2 = \underbrace{e^C}_{=:C_1^2>0} (1+f'(r)^2).$$ or alternatively $$f'(r)^2(r^2-C_1^2) = C_1^2$$ Problem (3) $r \neq C_1$ and Problem (4) (taking $\pm$) $$f'(r) = \frac{ \pm C_1 }{\sqrt{r^2-C_1^2}} = \frac{ \pm 1 }{\sqrt{\frac{r^2}{C_1^2}-1}}$$ Integrating gives then $f(r) = \pm C_1 \cosh^{-1} (\frac{r}{C_1})+d$ where $d$ is arbitrary and $C_1 >0.$ $\textbf{Regarding the problems:}$ Now, I want to understand whether I can get rid of the 4 problems (small inaccuracies in the way I am solving this ODE.) Regarding problem 1: The question is: Do I really have to assume $f'(r) \neq 0$ or can I avoid this somehow? Regarding problem 2: If I assume $f'(r)<0$ somwhere, it is quite obvious that we can rewrite the left hand side as $$\frac{1}{-rf'(r)} \frac{d}{dr}(-rf'(r)) = \ln(-rf'(r))$$ which would be well-defined now. Now, since we eventually square the argument of the $\ln $anyway (two equations below the statement of the problem in the derivation), we see that even for $f'(r)<0$ we would end up with the same equation as for $f'(r)>0$. Thus, problem 2 is apparently not a problem. Problem 3: Is it really necessary to assume $ r \neq C_1$? I guess yes, as otherwise we would get $0=f'(r)^2(r^2-C_1^2) = C_1^2=r^2$ which is a contradiction to $r \neq 0.$ Am I right? Problem 4: I don't like this $\pm $ there, as it could be that a solution has both positive and negative $f'(r)$, so the way I write the solution is not that general.","I am given the ODE $$(f''(r)+\frac{f'(r)}{r})(1+f'(r)^2)-f'(r)^2f''(r)=0$$ and want to solve it rigorously for $r>0.$ So especially, I don't want to loose any solutions. $\textbf{Derivation of the solutions}$ First I multiply by $r$ $$(rf''(r)+f'(r))(1+f'(r)^2) =rf'(r)^2f''(r)$$ which is nothing but $$\frac{d}{dr}(rf'(r))(1+f'(r)^2) =rf'(r)^2f''(r)$$ now assuming $f'(r) \neq 0$ (Problem 1) I can rewrite this as $$\frac{1}{r f'(r)} \frac{d}{dr}(rf'(r)) = \frac{f'(r)f''(r)}{1+f'(r)^2}$$ Assuming $f'(r)>0$ (Problem 2) we get $$\frac{d}{dr}\ln(rf'(r)) = \frac{1}{2} \frac{d}{dr} \ln(1+f'(r)^2)$$ which gives after integrating $$\ln(r^2f'(r)^2) = ln(1+f'(r)^2)+C$$ for some constant $C.$ Exponentiating this gives $$r^2 f'(r)^2 = \underbrace{e^C}_{=:C_1^2>0} (1+f'(r)^2).$$ or alternatively $$f'(r)^2(r^2-C_1^2) = C_1^2$$ Problem (3) $r \neq C_1$ and Problem (4) (taking $\pm$) $$f'(r) = \frac{ \pm C_1 }{\sqrt{r^2-C_1^2}} = \frac{ \pm 1 }{\sqrt{\frac{r^2}{C_1^2}-1}}$$ Integrating gives then $f(r) = \pm C_1 \cosh^{-1} (\frac{r}{C_1})+d$ where $d$ is arbitrary and $C_1 >0.$ $\textbf{Regarding the problems:}$ Now, I want to understand whether I can get rid of the 4 problems (small inaccuracies in the way I am solving this ODE.) Regarding problem 1: The question is: Do I really have to assume $f'(r) \neq 0$ or can I avoid this somehow? Regarding problem 2: If I assume $f'(r)<0$ somwhere, it is quite obvious that we can rewrite the left hand side as $$\frac{1}{-rf'(r)} \frac{d}{dr}(-rf'(r)) = \ln(-rf'(r))$$ which would be well-defined now. Now, since we eventually square the argument of the $\ln $anyway (two equations below the statement of the problem in the derivation), we see that even for $f'(r)<0$ we would end up with the same equation as for $f'(r)>0$. Thus, problem 2 is apparently not a problem. Problem 3: Is it really necessary to assume $ r \neq C_1$? I guess yes, as otherwise we would get $0=f'(r)^2(r^2-C_1^2) = C_1^2=r^2$ which is a contradiction to $r \neq 0.$ Am I right? Problem 4: I don't like this $\pm $ there, as it could be that a solution has both positive and negative $f'(r)$, so the way I write the solution is not that general.",,"['calculus', 'real-analysis', 'analysis', 'ordinary-differential-equations', 'dynamical-systems']"
47,Slightly different results to an ODE system - hand calculation vs Mathematica,Slightly different results to an ODE system - hand calculation vs Mathematica,,"This has been driving me mad for the last few days. I have a a pair of ODEs: $$\frac{d^2 M_N}{d x^2}=\lambda_{N}^2 M_N$$ $$\frac{d^2 M_{N-1}}{d x^2}=\lambda_{N-1}^2 M_{N-1}-\frac{f}{d_{N-1}}M_N$$ With BCs $$M_N(1)=M_{N-1}(1)=0,\ \frac{d M_{N}}{d x}(0)=h,\ \frac{d M_{N-1}}{d x}(0)=0$$ The solution to $M_N$ is: $$M_N(x)=\frac{h}{\lambda_N}\text{sech}(\lambda_N)\sinh(\lambda_N(x-1))$$ No surprises there. The trouble comes when I solve $M_{N-1}$. I get: $$M_{N-1}=\frac{fh}{d_{N-1}\lambda_{N}^2}\left(\frac{1}{\lambda_N}\text{sech}(\lambda_N)\sinh(\lambda_N(x-1))-\frac{1}{\lambda_{N-1}}\text{sech}(\lambda_{N-1})\sinh(\lambda_{N-1}(x-1))\right)$$ However, Mathematica gets $$M_{N-1}=\frac{fh}{d_{N-1}(\lambda_{N-1}^2-\lambda_{N}^2)}\left(\frac{1}{\lambda_N}\text{sech}(\lambda_N)\sinh(\lambda_N(x-1))-\frac{1}{\lambda_{N-1}}\text{sech}(\lambda_{N-1})\sinh(\lambda_{N-1}(x-1))\right)$$ Now, I can say for certain that $|\lambda_N| \ne |\lambda_{N-1}|$. I realise Mathematica doesn't know this, and I suspect that could have something to do with this. But I simply can't find a way to come up with the same result by hand. I know this is a selfish question, but can someone step through the process and show me how to come up with Mathematica's answer please!","This has been driving me mad for the last few days. I have a a pair of ODEs: $$\frac{d^2 M_N}{d x^2}=\lambda_{N}^2 M_N$$ $$\frac{d^2 M_{N-1}}{d x^2}=\lambda_{N-1}^2 M_{N-1}-\frac{f}{d_{N-1}}M_N$$ With BCs $$M_N(1)=M_{N-1}(1)=0,\ \frac{d M_{N}}{d x}(0)=h,\ \frac{d M_{N-1}}{d x}(0)=0$$ The solution to $M_N$ is: $$M_N(x)=\frac{h}{\lambda_N}\text{sech}(\lambda_N)\sinh(\lambda_N(x-1))$$ No surprises there. The trouble comes when I solve $M_{N-1}$. I get: $$M_{N-1}=\frac{fh}{d_{N-1}\lambda_{N}^2}\left(\frac{1}{\lambda_N}\text{sech}(\lambda_N)\sinh(\lambda_N(x-1))-\frac{1}{\lambda_{N-1}}\text{sech}(\lambda_{N-1})\sinh(\lambda_{N-1}(x-1))\right)$$ However, Mathematica gets $$M_{N-1}=\frac{fh}{d_{N-1}(\lambda_{N-1}^2-\lambda_{N}^2)}\left(\frac{1}{\lambda_N}\text{sech}(\lambda_N)\sinh(\lambda_N(x-1))-\frac{1}{\lambda_{N-1}}\text{sech}(\lambda_{N-1})\sinh(\lambda_{N-1}(x-1))\right)$$ Now, I can say for certain that $|\lambda_N| \ne |\lambda_{N-1}|$. I realise Mathematica doesn't know this, and I suspect that could have something to do with this. But I simply can't find a way to come up with the same result by hand. I know this is a selfish question, but can someone step through the process and show me how to come up with Mathematica's answer please!",,"['ordinary-differential-equations', 'systems-of-equations']"
48,Derivative of solution of differential equation with respect to parameter,Derivative of solution of differential equation with respect to parameter,,"Find derivative with respect to $A$ of solution of differential equation $$ \ddot x = {\dot x}^2 + x^3\tag1\label1 $$ with initial conditions $\{x(0)=0,\,\dot x(0)=A \}$ at $A=0$. My attempt We can find direct solution of $\eqref{1}$, but only implicitly (using substitution $p=\dot x^2$). Anyway, I prefer series: $$ x=x_0 + Ax_1 + \ldots,\tag2\label2 $$ where $x_0, x_1, \ldots$ are functions of $t$. Now the question is posed as to find $x_1$. From initial conditions, $x_0(0)=0, x_1(0)=0, \dot x_0(0)=0, \dot x_1(0)=1$. Substituting $\eqref2$ into $\eqref1$, we have $$ \ddot x_0 = \dot x_0^2 + x_0^3,\\ \ddot x_1 = 2\dot x_0\dot x_1 + 3x_0^2 x_1 \tag3 $$ But now I stuck. I can find $x_0$ only implicitly, so I cannot solve second equation for $x_1$. How to proceed from this ot how to use another methods?","Find derivative with respect to $A$ of solution of differential equation $$ \ddot x = {\dot x}^2 + x^3\tag1\label1 $$ with initial conditions $\{x(0)=0,\,\dot x(0)=A \}$ at $A=0$. My attempt We can find direct solution of $\eqref{1}$, but only implicitly (using substitution $p=\dot x^2$). Anyway, I prefer series: $$ x=x_0 + Ax_1 + \ldots,\tag2\label2 $$ where $x_0, x_1, \ldots$ are functions of $t$. Now the question is posed as to find $x_1$. From initial conditions, $x_0(0)=0, x_1(0)=0, \dot x_0(0)=0, \dot x_1(0)=1$. Substituting $\eqref2$ into $\eqref1$, we have $$ \ddot x_0 = \dot x_0^2 + x_0^3,\\ \ddot x_1 = 2\dot x_0\dot x_1 + 3x_0^2 x_1 \tag3 $$ But now I stuck. I can find $x_0$ only implicitly, so I cannot solve second equation for $x_1$. How to proceed from this ot how to use another methods?",,['ordinary-differential-equations']
49,How do I find invariant lines for a system of differential equations?,How do I find invariant lines for a system of differential equations?,,How do I find invariant lines for the following system of differential equations: $$x' = 2x - xy + x^3$$ $$y' = y - xy$$,How do I find invariant lines for the following system of differential equations: $$x' = 2x - xy + x^3$$ $$y' = y - xy$$,,['ordinary-differential-equations']
50,How can i find the basis solutions of homogeneous linear ODE?,How can i find the basis solutions of homogeneous linear ODE?,,"Second order linear differential equation is given below. $y''+\frac{2}{x}y'+k^2y=0,$ where $k$ is constant and $x\neq 0$ I already know that the basis are $y_1=\frac{e^{-ikx}}{x}$ and $y_2=\frac{e^{ikx}}{x}$. (from book) But i don't know how to find the basis of that ODE...","Second order linear differential equation is given below. $y''+\frac{2}{x}y'+k^2y=0,$ where $k$ is constant and $x\neq 0$ I already know that the basis are $y_1=\frac{e^{-ikx}}{x}$ and $y_2=\frac{e^{ikx}}{x}$. (from book) But i don't know how to find the basis of that ODE...",,"['linear-algebra', 'ordinary-differential-equations', 'homogeneous-equation']"
51,How to address multiple cases in this BVP? (Laplace equation in quarter-annulus),How to address multiple cases in this BVP? (Laplace equation in quarter-annulus),,"The original problem:   $$\nabla^2 u =0 \ \ \ \   for \ \ \ 0<a<r<b\ \ \  ,\ \ \ 0<\theta <\frac \pi 2$$   $$u(r,0)=0,\ \  u(r,\frac \pi 2)=f(r),\ \  u(a,\theta)=u(b,\theta)=0$$ My question pertains to the ODE for $G(r)$, found after separation $u=G(r) \Theta (\theta)$ and how this affects the solution to the overall PDE. $$r^2 G_{rr} + rG_r - \lambda G = 0$$ $$\lambda = -(\frac{n\pi}{ln b/a})^2$$ where $\lambda$ has been derived in the post. I've run into a wall with this separated radial ODE. I've actually finished this entire problem and reached the back of the book answer, but I found some beguiling cases that seem to flaw the solution. To see the cases, please scroll down. The answer in the back of the book does not seem to take into account these other cases, and I want to see if all cases simplify into the same answer or not. Here's my general solution to the ODE with $\lambda$ substituted in the second line: $$G(r) = A\cos\left(\sqrt{|\lambda|} \ln r\right) + B\sin\left(\sqrt{|\lambda|} \ln r\right)$$ $$ = A\cos\left(\frac{n\pi}{\ln b/a} \ln r\right) + B\sin\left(\frac{n\pi}{\ln b/a} \ln r\right)$$ Here are the boundary conditions, $G(b)=G(a)=0$: $$A\cos\left(\frac{n\pi}{\ln b/a} \ln a\right) + B\sin\left(\frac{n\pi}{\ln b/a} \ln a\right) = 0$$ $$A\cos\left(\frac{n\pi}{\ln b/a} \ln b\right) + B\sin\left(\frac{n\pi}{\ln b/a} \ln b\right) = 0$$ EDIT2: Solving $\lambda$, where $\lambda <0$ proven elsewhere. From the BCs above note that $$ \begin{bmatrix}     \cos\left(\sqrt{|\lambda|}\ln a\right) & \sin\left(\sqrt{|\lambda|} \ln a\right) \\      \cos\left(\sqrt{|\lambda|} \ln b\right) & \sin\left(\sqrt{|\lambda|} \ln b\right) \end{bmatrix} \begin{bmatrix}     A\\ B \end{bmatrix} =0 $$ $$\implies \sin(\sqrt{|\lambda|} ln(b/a)) = 0 \implies \lambda = -(\frac{n\pi}{ln b/a})^2$$ Here's where the issue arises: I need to resolve the coeffcients of G(r). Can I put $A$ in terms of $B$ without creating multiple cases? If the terms were nonzero, it'd be easy. We'd use either equation (say the top one) to derive $$A = -B\tan\left(\frac{n\pi}{\ln b/a} \ln a\right)$$ which eventually leads to $$G(r) = C\sin\left(\frac{n\pi}{\ln b/a} \ln r/a\right)$$ This last line leads to the back of the book answer (scroll down, see CASE1). But it's not clear if we are actually permitted to divide our terms and get $A = -B\tan\left(\frac{n\pi}{\ln b/a} \ln a\right)$! What if a term was zero? The terms cannot be simultaneously zero, since sine and cosine are orthogonal, but suppose one of them, say $\cos\left(\frac{n\pi}{\ln b/a} \ln a\right)=0$. Then $B=0$ and we get CASE2 $$G(r) = A\cos\left(\frac{n\pi}{\ln b/a} \ln r\right) $$ But then the remaining BC gives a funny result $$G(b) = A\cos\left(\frac{n\pi}{\ln b/a} \ln b\right) = 0$$ $\implies n\pi\frac{\ln b}{\ln b/a} = \frac{\pi}{2}(2k+1)$ for a nontrivial solution. But... that depends on the value of $b$!!! It doesn't appear that $b$ always has to satisfy this... does it? This has me scratching my head. UPDATE: SRX's answer clarifies this point. This makes sense. There is similarly a CASE3 with $G(r) = B\sin\left(\frac{n\pi}{\ln b/a} \ln r\right) $. What's left to find out is whether or not these cases simplify to the same overall solution. Or, whether we need cases at all! The solution to the azimuthal ODE is $\Theta (\theta) = \sinh(\frac{n \pi \theta}{\ln b/a})$. Take my word for it. This gives three cases of solutions: CASE1: $$u_1(r,\theta) = \sum_{n=0}^{\infty} C_n\sin\left(\frac{n\pi}{\ln b/a} \ln r/a\right) \sinh \left(\frac{n \pi \theta}{\ln b/a}\right)$$   $$C_n \sinh \left(\frac{n \pi^2}{2\ln (b/a)}\right) = \frac{2}{\ln(b/a)} \int^b_a f(r) \sin \left(n \pi \frac{\ln (r/a)}{\ln (b/a)}\right) \frac{dr}{r}$$ CASE2: $$u_2(r,\theta) = \sum_{n=0}^{\infty} A_n\cos\left(\frac{n\pi}{\ln b/a} \ln r\right) \sinh \left(\frac{n \pi \theta}{\ln b/a}\right)$$ CASE3 $$u_3(r,\theta) = \sum_{n=0}^{\infty} B_n\sin\left(\frac{n\pi}{\ln b/a} \ln r\right) \sinh \left(\frac{n \pi \theta}{\ln b/a}\right)$$ As mentioned earlier, only CASE1 appears in the back of the textbook as an answer. The coefficients were found after applying the final BC $u(r,\frac \pi 2) = f(r)$ and taking a weighted inner product with weight $1/r$. I'll exclude the work for now and only cite the result for CASE1. The coefficient relations for $A_n$ and $B_n$ don't seem to work out very nicely. I had issues with them and will need to re-explore those. Am I crazy, or does this problem split into these three separate solutions? I see how, physically speaking, CASE1 is of greatest interest as it would occur more commonly, but what about CASES 2 and 3?","The original problem:   $$\nabla^2 u =0 \ \ \ \   for \ \ \ 0<a<r<b\ \ \  ,\ \ \ 0<\theta <\frac \pi 2$$   $$u(r,0)=0,\ \  u(r,\frac \pi 2)=f(r),\ \  u(a,\theta)=u(b,\theta)=0$$ My question pertains to the ODE for $G(r)$, found after separation $u=G(r) \Theta (\theta)$ and how this affects the solution to the overall PDE. $$r^2 G_{rr} + rG_r - \lambda G = 0$$ $$\lambda = -(\frac{n\pi}{ln b/a})^2$$ where $\lambda$ has been derived in the post. I've run into a wall with this separated radial ODE. I've actually finished this entire problem and reached the back of the book answer, but I found some beguiling cases that seem to flaw the solution. To see the cases, please scroll down. The answer in the back of the book does not seem to take into account these other cases, and I want to see if all cases simplify into the same answer or not. Here's my general solution to the ODE with $\lambda$ substituted in the second line: $$G(r) = A\cos\left(\sqrt{|\lambda|} \ln r\right) + B\sin\left(\sqrt{|\lambda|} \ln r\right)$$ $$ = A\cos\left(\frac{n\pi}{\ln b/a} \ln r\right) + B\sin\left(\frac{n\pi}{\ln b/a} \ln r\right)$$ Here are the boundary conditions, $G(b)=G(a)=0$: $$A\cos\left(\frac{n\pi}{\ln b/a} \ln a\right) + B\sin\left(\frac{n\pi}{\ln b/a} \ln a\right) = 0$$ $$A\cos\left(\frac{n\pi}{\ln b/a} \ln b\right) + B\sin\left(\frac{n\pi}{\ln b/a} \ln b\right) = 0$$ EDIT2: Solving $\lambda$, where $\lambda <0$ proven elsewhere. From the BCs above note that $$ \begin{bmatrix}     \cos\left(\sqrt{|\lambda|}\ln a\right) & \sin\left(\sqrt{|\lambda|} \ln a\right) \\      \cos\left(\sqrt{|\lambda|} \ln b\right) & \sin\left(\sqrt{|\lambda|} \ln b\right) \end{bmatrix} \begin{bmatrix}     A\\ B \end{bmatrix} =0 $$ $$\implies \sin(\sqrt{|\lambda|} ln(b/a)) = 0 \implies \lambda = -(\frac{n\pi}{ln b/a})^2$$ Here's where the issue arises: I need to resolve the coeffcients of G(r). Can I put $A$ in terms of $B$ without creating multiple cases? If the terms were nonzero, it'd be easy. We'd use either equation (say the top one) to derive $$A = -B\tan\left(\frac{n\pi}{\ln b/a} \ln a\right)$$ which eventually leads to $$G(r) = C\sin\left(\frac{n\pi}{\ln b/a} \ln r/a\right)$$ This last line leads to the back of the book answer (scroll down, see CASE1). But it's not clear if we are actually permitted to divide our terms and get $A = -B\tan\left(\frac{n\pi}{\ln b/a} \ln a\right)$! What if a term was zero? The terms cannot be simultaneously zero, since sine and cosine are orthogonal, but suppose one of them, say $\cos\left(\frac{n\pi}{\ln b/a} \ln a\right)=0$. Then $B=0$ and we get CASE2 $$G(r) = A\cos\left(\frac{n\pi}{\ln b/a} \ln r\right) $$ But then the remaining BC gives a funny result $$G(b) = A\cos\left(\frac{n\pi}{\ln b/a} \ln b\right) = 0$$ $\implies n\pi\frac{\ln b}{\ln b/a} = \frac{\pi}{2}(2k+1)$ for a nontrivial solution. But... that depends on the value of $b$!!! It doesn't appear that $b$ always has to satisfy this... does it? This has me scratching my head. UPDATE: SRX's answer clarifies this point. This makes sense. There is similarly a CASE3 with $G(r) = B\sin\left(\frac{n\pi}{\ln b/a} \ln r\right) $. What's left to find out is whether or not these cases simplify to the same overall solution. Or, whether we need cases at all! The solution to the azimuthal ODE is $\Theta (\theta) = \sinh(\frac{n \pi \theta}{\ln b/a})$. Take my word for it. This gives three cases of solutions: CASE1: $$u_1(r,\theta) = \sum_{n=0}^{\infty} C_n\sin\left(\frac{n\pi}{\ln b/a} \ln r/a\right) \sinh \left(\frac{n \pi \theta}{\ln b/a}\right)$$   $$C_n \sinh \left(\frac{n \pi^2}{2\ln (b/a)}\right) = \frac{2}{\ln(b/a)} \int^b_a f(r) \sin \left(n \pi \frac{\ln (r/a)}{\ln (b/a)}\right) \frac{dr}{r}$$ CASE2: $$u_2(r,\theta) = \sum_{n=0}^{\infty} A_n\cos\left(\frac{n\pi}{\ln b/a} \ln r\right) \sinh \left(\frac{n \pi \theta}{\ln b/a}\right)$$ CASE3 $$u_3(r,\theta) = \sum_{n=0}^{\infty} B_n\sin\left(\frac{n\pi}{\ln b/a} \ln r\right) \sinh \left(\frac{n \pi \theta}{\ln b/a}\right)$$ As mentioned earlier, only CASE1 appears in the back of the textbook as an answer. The coefficients were found after applying the final BC $u(r,\frac \pi 2) = f(r)$ and taking a weighted inner product with weight $1/r$. I'll exclude the work for now and only cite the result for CASE1. The coefficient relations for $A_n$ and $B_n$ don't seem to work out very nicely. I had issues with them and will need to re-explore those. Am I crazy, or does this problem split into these three separate solutions? I see how, physically speaking, CASE1 is of greatest interest as it would occur more commonly, but what about CASES 2 and 3?",,"['ordinary-differential-equations', 'partial-differential-equations']"
52,Find minimizer of the functional $l(u)= \int_{-1} ^1 u(t) \mathbb d t$,Find minimizer of the functional,l(u)= \int_{-1} ^1 u(t) \mathbb d t,Find minimizer of the functional  $ l(u)= \int \limits _{-1} ^1 u(t) \mathbb d t $ with $u(-1)=u(1)=0 $ subject to $g(u)=\int \limits _{-1} ^1 \sqrt{1+u'(t)} \mathbb d t=π $. I solved it using Lagrange's equations and I found $u(t)=\sqrt{\lambda ^2 -(t+c)^2 }+c$. First I started by $l^*=l- \lambda g$ then I used the Euler-Lagrange equation ($l_u -\frac{d}{dt}l_u'=0$) or first integral ($l-u'l_u'=c$). My problem is how to find value of $c$.,Find minimizer of the functional  $ l(u)= \int \limits _{-1} ^1 u(t) \mathbb d t $ with $u(-1)=u(1)=0 $ subject to $g(u)=\int \limits _{-1} ^1 \sqrt{1+u'(t)} \mathbb d t=π $. I solved it using Lagrange's equations and I found $u(t)=\sqrt{\lambda ^2 -(t+c)^2 }+c$. First I started by $l^*=l- \lambda g$ then I used the Euler-Lagrange equation ($l_u -\frac{d}{dt}l_u'=0$) or first integral ($l-u'l_u'=c$). My problem is how to find value of $c$.,,"['ordinary-differential-equations', 'calculus-of-variations']"
53,Backwards Heat Equation $ u_{t} = -\lambda^2 u_{xx}$,Backwards Heat Equation, u_{t} = -\lambda^2 u_{xx},"Problem Consider the backwards heat equation of the form    $$ \left\{ \begin{aligned} u_{t} & = \lambda^2 u_{xx}, & x\in[0,L], \quad t\in[0,T]\\ u(0,t) &= u(L,t) = 0 \\ u(x,T) &= f(x), \end{aligned} \right.\tag{*}\label{*}$$   Establish whether solution is unique and analyze its stability. Attempt of ( dis )proving stability My attempt to answer the stability question is provided in this post . Attempt of proving uniqueness I know that it is possible to use the energy functional method for proving uniqueness of the solution of backwards heat equation in a way we do that for a regular heat equation, with some additional tweaks. Assume there are two different solutions $u_1$ and $u_2$ of $\eqref{*}$ and define the discrepancy as $w(x,t) : = u_1(x,t) - u_2(x,t)$. By superposition principle $w(x,t)$ is also a solution of \eqref{*}. Define the energy  $$ E (t):=\int_0^Lw^2(x,t)\,dx \ge 0, $$ then $$\dot{E}(t) := \frac{dE}{dt} = 2\int_0^L w  w_t\,dx, = 2 \int_0^L w  w_{xx}\,dx = 2ww_x\big|_{0}^L - 2\int_0^L w_x^2\,dx=-2\int_0^L w_x^2\,dx,$$ $$ \ddot{E}(t) = \frac{d }{dt}\Big(\dot{E}(t) \Big) = -2\frac{d }{dt}\Bigg(\int_0^L w_x^2\,dx \Bigg) = -4\int_0^L w_x w_{xt}\,dx = 4\int_0^L w_{xx} w_{t}\,dx = 4\int_0^L w_{xx}^2 \,dx $$ By Cauchy-Schwarz we have  $$ \dot{E}^2 = 4\Bigg(\int_0^L w  w_{xx}\Bigg)^2\le 4\bigg(\int_0^Lw^2\,dx \bigg)\cdot \bigg(\int_0^Lw_{xx}^2\,dx\bigg)  = E\cdot \ddot{E}$$ What should be the next step in the proof? EDIT : Thanks to this answer , I was able to get the following: $$\dot{E}^2\le E \ddot{E} $$ Define $F(t) := \ln\big(E(t)\big)$, then $$ \dot{F} = \frac{\dot{E}}{E}, \quad \ddot{F} = \frac{\ddot{E}E - \dot{E}^2}{E^2} > 0, $$ so that $F(t)$ is convex . By definition of convexity,  $$ \forall t_1, t_2 \in [0,T], \ \forall \theta\in [0,1] \quad F\big(\theta t_1 + (1-\theta) t_2 \big) \le \theta F(t_1) + (1-\theta) F(t_2 )  \implies \\ \ln\Big(E\big(\theta t_1 + (1-\theta) t_2 \big) \Big) \le \theta \ln\big(E(t_1)\big) + (1-\theta)\ln\big(E(t_2)\big) =  \ln \Big(E^\theta\left( t_1 \right) E^{(1-\theta)}\left( t_2 \right)  \Big) \\ E\big(\theta t_1 + (1-\theta) t_2 \big) \le E^\theta\left( t_1 \right) E^{(1-\theta)}\left( t_2 \right)   $$ Choosing $t_2 = T$ and assuming arbitrary $t_1 = t$, we get  $$E\big(\theta t + (1-\theta) T \big) \le E^\theta\left( t \right) \underbrace{E^{(1-\theta)}\left( T \right)}_{=0}  =0 \quad \forall \theta \in [0,1]$$ Since $w(T) = u_1(x,T) - u_2(x,T) = 0$, we know that  $E(T) =0 $. But then  $$ 0 \le E\big(\theta t_1 + (1-\theta) t_2 \big) \le E^\theta (t)\! \cdot\!0 \implies  \\   \forall t\in (0,T) \quad E(t) \equiv 0 \ \implies w(x,t) \equiv 0  \iff \\  u_1(x,t) \equiv u_2(x,t). $$ Q.E.D.","Problem Consider the backwards heat equation of the form    $$ \left\{ \begin{aligned} u_{t} & = \lambda^2 u_{xx}, & x\in[0,L], \quad t\in[0,T]\\ u(0,t) &= u(L,t) = 0 \\ u(x,T) &= f(x), \end{aligned} \right.\tag{*}\label{*}$$   Establish whether solution is unique and analyze its stability. Attempt of ( dis )proving stability My attempt to answer the stability question is provided in this post . Attempt of proving uniqueness I know that it is possible to use the energy functional method for proving uniqueness of the solution of backwards heat equation in a way we do that for a regular heat equation, with some additional tweaks. Assume there are two different solutions $u_1$ and $u_2$ of $\eqref{*}$ and define the discrepancy as $w(x,t) : = u_1(x,t) - u_2(x,t)$. By superposition principle $w(x,t)$ is also a solution of \eqref{*}. Define the energy  $$ E (t):=\int_0^Lw^2(x,t)\,dx \ge 0, $$ then $$\dot{E}(t) := \frac{dE}{dt} = 2\int_0^L w  w_t\,dx, = 2 \int_0^L w  w_{xx}\,dx = 2ww_x\big|_{0}^L - 2\int_0^L w_x^2\,dx=-2\int_0^L w_x^2\,dx,$$ $$ \ddot{E}(t) = \frac{d }{dt}\Big(\dot{E}(t) \Big) = -2\frac{d }{dt}\Bigg(\int_0^L w_x^2\,dx \Bigg) = -4\int_0^L w_x w_{xt}\,dx = 4\int_0^L w_{xx} w_{t}\,dx = 4\int_0^L w_{xx}^2 \,dx $$ By Cauchy-Schwarz we have  $$ \dot{E}^2 = 4\Bigg(\int_0^L w  w_{xx}\Bigg)^2\le 4\bigg(\int_0^Lw^2\,dx \bigg)\cdot \bigg(\int_0^Lw_{xx}^2\,dx\bigg)  = E\cdot \ddot{E}$$ What should be the next step in the proof? EDIT : Thanks to this answer , I was able to get the following: $$\dot{E}^2\le E \ddot{E} $$ Define $F(t) := \ln\big(E(t)\big)$, then $$ \dot{F} = \frac{\dot{E}}{E}, \quad \ddot{F} = \frac{\ddot{E}E - \dot{E}^2}{E^2} > 0, $$ so that $F(t)$ is convex . By definition of convexity,  $$ \forall t_1, t_2 \in [0,T], \ \forall \theta\in [0,1] \quad F\big(\theta t_1 + (1-\theta) t_2 \big) \le \theta F(t_1) + (1-\theta) F(t_2 )  \implies \\ \ln\Big(E\big(\theta t_1 + (1-\theta) t_2 \big) \Big) \le \theta \ln\big(E(t_1)\big) + (1-\theta)\ln\big(E(t_2)\big) =  \ln \Big(E^\theta\left( t_1 \right) E^{(1-\theta)}\left( t_2 \right)  \Big) \\ E\big(\theta t_1 + (1-\theta) t_2 \big) \le E^\theta\left( t_1 \right) E^{(1-\theta)}\left( t_2 \right)   $$ Choosing $t_2 = T$ and assuming arbitrary $t_1 = t$, we get  $$E\big(\theta t + (1-\theta) T \big) \le E^\theta\left( t \right) \underbrace{E^{(1-\theta)}\left( T \right)}_{=0}  =0 \quad \forall \theta \in [0,1]$$ Since $w(T) = u_1(x,T) - u_2(x,T) = 0$, we know that  $E(T) =0 $. But then  $$ 0 \le E\big(\theta t_1 + (1-\theta) t_2 \big) \le E^\theta (t)\! \cdot\!0 \implies  \\   \forall t\in (0,T) \quad E(t) \equiv 0 \ \implies w(x,t) \equiv 0  \iff \\  u_1(x,t) \equiv u_2(x,t). $$ Q.E.D.",,"['calculus', 'analysis', 'ordinary-differential-equations', 'heat-equation']"
54,Why are the eigenfunctions linear independent?,Why are the eigenfunctions linear independent?,,"At a Sturm-Liouville problem how do we know that the two eigenfunctions that we have found are linear independent?? For example we have the following problem : $$X''+\lambda X=0 \\ X(0)=X(2\pi) \\ X'(0)=X^\circ(2\pi)$$ We find that for $\lambda=k^2>0$ we have $X(x)=c_1\cos (kx)+c_2\sin (kx)$. We have two linear independent eigenfunctions $\cos (kx), \sin (kx)$. How do we know these two eigenfunctions are linear independent??","At a Sturm-Liouville problem how do we know that the two eigenfunctions that we have found are linear independent?? For example we have the following problem : $$X''+\lambda X=0 \\ X(0)=X(2\pi) \\ X'(0)=X^\circ(2\pi)$$ We find that for $\lambda=k^2>0$ we have $X(x)=c_1\cos (kx)+c_2\sin (kx)$. We have two linear independent eigenfunctions $\cos (kx), \sin (kx)$. How do we know these two eigenfunctions are linear independent??",,"['ordinary-differential-equations', 'partial-differential-equations']"
55,How to argue Existence of Unique solution of an IVP,How to argue Existence of Unique solution of an IVP,,"How do I show that there exists a unique solution for this given IVP without solving? Just mere argument. $$\frac{dy}{dx} = \frac{1}{2y\sqrt{1-x^2}}, \qquad y(0)=3$$ I'm having problems with this kind of question because we're not taught of proving without solving. Please help.","How do I show that there exists a unique solution for this given IVP without solving? Just mere argument. $$\frac{dy}{dx} = \frac{1}{2y\sqrt{1-x^2}}, \qquad y(0)=3$$ I'm having problems with this kind of question because we're not taught of proving without solving. Please help.",,['ordinary-differential-equations']
56,Solving system of delay differential equations,Solving system of delay differential equations,,"Are there any numerical methods for solving systems of delay differential equations with time-dependent delays? For example, I have a system: $$\frac{dP_1}{dt}  = f_1(t) P_2(t-\tau(t)) P_3(t)$$ $$\frac{dP_2}{dt}  = f_2(t) P_3(t-\tau(t)) P_1(t)$$ $$\frac{dP_3}{dt}  = f_3(t) P_1(t-\tau(t)) P_2(t)$$ I thought about applying method of steps together with Runge-Kutta method, but it leads to loss of information, because Runge-Kutta method requires values of RHS at $(x+1/2 h)$, where $h$ is a step. So, I'm interested if any other methods except this one exist for solving such systems. And, if they exist, could you please tell me where I can read about them, because I didn't find useful information.","Are there any numerical methods for solving systems of delay differential equations with time-dependent delays? For example, I have a system: $$\frac{dP_1}{dt}  = f_1(t) P_2(t-\tau(t)) P_3(t)$$ $$\frac{dP_2}{dt}  = f_2(t) P_3(t-\tau(t)) P_1(t)$$ $$\frac{dP_3}{dt}  = f_3(t) P_1(t-\tau(t)) P_2(t)$$ I thought about applying method of steps together with Runge-Kutta method, but it leads to loss of information, because Runge-Kutta method requires values of RHS at $(x+1/2 h)$, where $h$ is a step. So, I'm interested if any other methods except this one exist for solving such systems. And, if they exist, could you please tell me where I can read about them, because I didn't find useful information.",,"['ordinary-differential-equations', 'reference-request', 'numerical-methods', 'delay-differential-equations']"
57,"If $\|u\|^2 = c^2$, then $\|\dot x\|^2 \to a $ for $\ddot x = -\dot x + u(t)$?","If , then  for ?",\|u\|^2 = c^2 \|\dot x\|^2 \to a  \ddot x = -\dot x + u(t),"I am dealing with the next simple equation $$ \ddot x = -\dot x + u(t), $$ where $u, x\in\mathbb{R}^m$, with $m \geq 1$, and I am wondering if for $\|u\|^2 = c^2 > 0$ then $\|\dot x\|^2\to a$, where $a\in\mathbb R^+$. For the trivial case of having $m = 1$ it is clear, however I do not know whether is true for $m > 1$. Any hints or references? Edit The next computations show me a bound for $||\dot x(t)||$, but I do not know how to show that it converges to a constant (if this is indeed the case). Take $y = \dot x$, then $$ y(t) = e^{-t}y(0)+\int_0^\tau e^{-I(t-\tau)}u(\tau)d\tau \\ |y(t)| \leq e^{-t}|y(0)|+\int_0^\tau \left|e^{-I(t-\tau)}\right| \, \left|u(\tau)\right|d\tau \\ |y(t)| \leq e^{-t}|y(0)|+ \sup (|u(\sigma)|)_{0\leq\sigma\leq\tau} \int_0^\tau \left|e^{-I(t-\tau)}\right| d\tau, $$ and knowing that $|u(t)| = c, \forall t$, then $$ |y(t)| \leq e^{-t}|y(0)| + c \\ $$ Then if I take the limit $t\to\infty$ in the before expression. I can conclude that $|y(t)| \leq c$, so $|\dot x(t)| \leq c$ when $t\to\infty$. How to continue from here? I suspect that it is related with energy. If we are pumping constant energy and the system can only disipate certain energy per unit of time, they will reach an equilibrium in the sense of energy, i.e. $\dot x(t)$ might not be constant but its norm yes.","I am dealing with the next simple equation $$ \ddot x = -\dot x + u(t), $$ where $u, x\in\mathbb{R}^m$, with $m \geq 1$, and I am wondering if for $\|u\|^2 = c^2 > 0$ then $\|\dot x\|^2\to a$, where $a\in\mathbb R^+$. For the trivial case of having $m = 1$ it is clear, however I do not know whether is true for $m > 1$. Any hints or references? Edit The next computations show me a bound for $||\dot x(t)||$, but I do not know how to show that it converges to a constant (if this is indeed the case). Take $y = \dot x$, then $$ y(t) = e^{-t}y(0)+\int_0^\tau e^{-I(t-\tau)}u(\tau)d\tau \\ |y(t)| \leq e^{-t}|y(0)|+\int_0^\tau \left|e^{-I(t-\tau)}\right| \, \left|u(\tau)\right|d\tau \\ |y(t)| \leq e^{-t}|y(0)|+ \sup (|u(\sigma)|)_{0\leq\sigma\leq\tau} \int_0^\tau \left|e^{-I(t-\tau)}\right| d\tau, $$ and knowing that $|u(t)| = c, \forall t$, then $$ |y(t)| \leq e^{-t}|y(0)| + c \\ $$ Then if I take the limit $t\to\infty$ in the before expression. I can conclude that $|y(t)| \leq c$, so $|\dot x(t)| \leq c$ when $t\to\infty$. How to continue from here? I suspect that it is related with energy. If we are pumping constant energy and the system can only disipate certain energy per unit of time, they will reach an equilibrium in the sense of energy, i.e. $\dot x(t)$ might not be constant but its norm yes.",,['ordinary-differential-equations']
58,Fredholm Integral Equations,Fredholm Integral Equations,,"I'm having problems obtaining the solution of the homogeneous Fredholm Integral Equation of the 2nd kind, with separable kernel. I always get a zero if I use the normal method I was taught for the non homogeneous type. I have an example: $$y(x) = \lambda \int_{-1}^1(x+z)y(z)dz$$. Can you help me please?","I'm having problems obtaining the solution of the homogeneous Fredholm Integral Equation of the 2nd kind, with separable kernel. I always get a zero if I use the normal method I was taught for the non homogeneous type. I have an example: $$y(x) = \lambda \int_{-1}^1(x+z)y(z)dz$$. Can you help me please?",,['ordinary-differential-equations']
59,What does a standalone $dx$ mean?,What does a standalone  mean?,dx,"Some literature uses $dx$, in the context of differential equations, in a confusing way without defining what it really stands for: $Mdx + Ndy = 0$ Does it mean one of the following or something else entirely? $\int Mdx + \int Ndy = 0$ $Md/dx + Nd/dy = 0$ $Mx' + Ny' = 0$","Some literature uses $dx$, in the context of differential equations, in a confusing way without defining what it really stands for: $Mdx + Ndy = 0$ Does it mean one of the following or something else entirely? $\int Mdx + \int Ndy = 0$ $Md/dx + Nd/dy = 0$ $Mx' + Ny' = 0$",,"['calculus', 'ordinary-differential-equations', 'notation', 'differential-forms']"
60,Solve an ODE $\frac{d^2y}{dx^2}+\cot x\frac{dy}{dx}+4y\csc^2x=0$,Solve an ODE,\frac{d^2y}{dx^2}+\cot x\frac{dy}{dx}+4y\csc^2x=0,$$\frac{d^2y}{dx^2}+\cot x\frac{dy}{dx}+4y\csc^2x=0$$ I could not find any examples as to how to approach 2nd order ODEs with trigonometric coefficients. How do I solve the auxiliary equation and get the complementary function ? Please assist. There are no initial conditions given. The answer given is: $\displaystyle y=k_1\cos\left(2\log\tan\frac{x}{2}+k_2\right)$. Wolframalpha gives a different answer: http://www.wolframalpha.com/input/?i=y%27%27[x]+%2B+Cot[x]+y%27[x]+%2B+4+y[x]+Csc[x] ^2%3D0,$$\frac{d^2y}{dx^2}+\cot x\frac{dy}{dx}+4y\csc^2x=0$$ I could not find any examples as to how to approach 2nd order ODEs with trigonometric coefficients. How do I solve the auxiliary equation and get the complementary function ? Please assist. There are no initial conditions given. The answer given is: $\displaystyle y=k_1\cos\left(2\log\tan\frac{x}{2}+k_2\right)$. Wolframalpha gives a different answer: http://www.wolframalpha.com/input/?i=y%27%27[x]+%2B+Cot[x]+y%27[x]+%2B+4+y[x]+Csc[x] ^2%3D0,,['ordinary-differential-equations']
61,Differential Equations Question involving a spring-mass system,Differential Equations Question involving a spring-mass system,,"I'm working through a Differential Equations book and came across a question that puzzles me: ""Consider a spring-mass system where there are two masses connected in series by springs, with each spring having a different constant ($k_1$ and $k_2$)"" (set up like this): |<><>($m_1$)<><>($m_2$) Where $m_1$ and $m_2$ are the masses, and $k_1$ and $k_2$ would be the spring constants. The position of each mass is given by $x_1$ and $x_2$. It asks to show that (using $F=ma$) the system can be described by this pair of DEs: \begin{align} m_1\ddot{x}_1 &= -(k_1+k_2)x_1 + k_2x_2\\ m_2\ddot{x}_2 &= k_2x_1 - k_2x_2 \end{align} and then reduce these to a single equation for the position of mass 2 ($x_2$). Any suggestions on how to get these equations? Been a while since my Physics days but I think it's primarily a mathematics question. Thanks in advance","I'm working through a Differential Equations book and came across a question that puzzles me: ""Consider a spring-mass system where there are two masses connected in series by springs, with each spring having a different constant ($k_1$ and $k_2$)"" (set up like this): |<><>($m_1$)<><>($m_2$) Where $m_1$ and $m_2$ are the masses, and $k_1$ and $k_2$ would be the spring constants. The position of each mass is given by $x_1$ and $x_2$. It asks to show that (using $F=ma$) the system can be described by this pair of DEs: \begin{align} m_1\ddot{x}_1 &= -(k_1+k_2)x_1 + k_2x_2\\ m_2\ddot{x}_2 &= k_2x_1 - k_2x_2 \end{align} and then reduce these to a single equation for the position of mass 2 ($x_2$). Any suggestions on how to get these equations? Been a while since my Physics days but I think it's primarily a mathematics question. Thanks in advance",,"['ordinary-differential-equations', 'physics', 'classical-mechanics']"
62,A basic question on omega limit sets equilibrium points,A basic question on omega limit sets equilibrium points,,Consider the following O.D.E $$\dot{x}(t)=h(x(t))$$ with $h$ being lipschitz. consider a trajectory of it. Assume that its omega limit sets are finite. Then I have read in a paper that its  omega limit set necessarily consists of equilibrium points of the O.D.E. Why ?,Consider the following O.D.E $$\dot{x}(t)=h(x(t))$$ with $h$ being lipschitz. consider a trajectory of it. Assume that its omega limit sets are finite. Then I have read in a paper that its  omega limit set necessarily consists of equilibrium points of the O.D.E. Why ?,,['ordinary-differential-equations']
63,Limit of solution of linear system of ODEs as $t\to \infty$,Limit of solution of linear system of ODEs as,t\to \infty,"I am completely stuck on the following problem: Consider the linear system: $x'(t)=A(t)x(t)$ where $A(t)$ is an $n$ by $n$ matrix. Assume that $\lim_{t\to \infty}A(t)=B$. Suppose that each eigenvalue of $B$ has strictly negative real part. Let $y(t)$ be a solution to the linear system. Does anyone how to show: $\lim_{t\to \infty}|y(t)|=0$ I know that if $y(t)$ is a solution to the linear system and all eigenvalues of $A$ have  strictly negative real parts, then $y(t)\to 0$ as $t\to \infty$. However, this is true when $A$ is independent of $t$. The issue for this problem is that $A$ is dependent on $t$, and it's the limiting matrix $B$ that has eigenvalues with strictly negative real parts. I don't know how to approach this problem. Thanks to anyone who help me out!","I am completely stuck on the following problem: Consider the linear system: $x'(t)=A(t)x(t)$ where $A(t)$ is an $n$ by $n$ matrix. Assume that $\lim_{t\to \infty}A(t)=B$. Suppose that each eigenvalue of $B$ has strictly negative real part. Let $y(t)$ be a solution to the linear system. Does anyone how to show: $\lim_{t\to \infty}|y(t)|=0$ I know that if $y(t)$ is a solution to the linear system and all eigenvalues of $A$ have  strictly negative real parts, then $y(t)\to 0$ as $t\to \infty$. However, this is true when $A$ is independent of $t$. The issue for this problem is that $A$ is dependent on $t$, and it's the limiting matrix $B$ that has eigenvalues with strictly negative real parts. I don't know how to approach this problem. Thanks to anyone who help me out!",,"['real-analysis', 'linear-algebra', 'analysis', 'ordinary-differential-equations']"
64,Help with Implicit Differentiation: Finding an equation for a tangent to a given point on a curve,Help with Implicit Differentiation: Finding an equation for a tangent to a given point on a curve,,"When working through a problem set containing Implicit Differentiation problems, I've found that I keep getting the wrong answer compared to the one listed at the back of my book. The problem is given as such: Use implicit differentiation to find the equation of the tangent line to the curve at a given point x^2 + xy + y^2 = 3 With given point (1, 1) . I also am told that it is an ellipse. To solve this, I evidently must differentiate both sides of the problem: 1: dy/dx ( x^2 + xy + Y^2 ) = dy/dx(3) 2: dy/dx (2x + 1y'+ 2yy') = 0 3: 1y' + 2yy' = 0 - 2x 4: y'(1+2y) = -2x 5: y' = -2x/(1+2y) Hurray, so now since I have the first derivative of Y. I can use it to find the slope at the point. Slope at Point (1,1)= -2( 1 ) / (1+2( 1 ) Slope at Point (1,1)= -2/3 So now that I've got my slope, I know the equation of the tangent will be in the form: y=mx+b So, given I now know the slope: y=-2/3x + b Substitute in the known point: 1 = -2/3(1) + b b = 5/3 So the final answer I get is: y = -2/3x + 5/3 But according to the answer, it is supposed to be: -x + 2 , I don't know where I went wrong, and I've done it twice to make sure I'm getting the same answer. Could someone please help me?","When working through a problem set containing Implicit Differentiation problems, I've found that I keep getting the wrong answer compared to the one listed at the back of my book. The problem is given as such: Use implicit differentiation to find the equation of the tangent line to the curve at a given point x^2 + xy + y^2 = 3 With given point (1, 1) . I also am told that it is an ellipse. To solve this, I evidently must differentiate both sides of the problem: 1: dy/dx ( x^2 + xy + Y^2 ) = dy/dx(3) 2: dy/dx (2x + 1y'+ 2yy') = 0 3: 1y' + 2yy' = 0 - 2x 4: y'(1+2y) = -2x 5: y' = -2x/(1+2y) Hurray, so now since I have the first derivative of Y. I can use it to find the slope at the point. Slope at Point (1,1)= -2( 1 ) / (1+2( 1 ) Slope at Point (1,1)= -2/3 So now that I've got my slope, I know the equation of the tangent will be in the form: y=mx+b So, given I now know the slope: y=-2/3x + b Substitute in the known point: 1 = -2/3(1) + b b = 5/3 So the final answer I get is: y = -2/3x + 5/3 But according to the answer, it is supposed to be: -x + 2 , I don't know where I went wrong, and I've done it twice to make sure I'm getting the same answer. Could someone please help me?",,"['ordinary-differential-equations', 'derivatives', 'implicit-differentiation']"
65,Solving the ODE $(x^2 - 1) y''- 2xy' + 2y = (x^2 - 1)^2$,Solving the ODE,(x^2 - 1) y''- 2xy' + 2y = (x^2 - 1)^2,"I want to solve this ODE: $$(x^2 - 1)y'' - 2xy' + 2y = (x^2 - 1)^2.$$ I found out that $y_1 = x$ and $y_2 = x^2+1$ are solutions of the associated  homogeneous equation, $x$ by inspecting, and $x^2+1$ by multiplying $x$ by a function and solving. Now, I know that a particular solution of the problem is given by $y_p = k_1x + k_2(x^2+1)$, for some functions $k_1,k_2$, which must satisfy the following relations: $$\left\{ \begin{array}{l} k_1'x + k_2'(x²+1) = 0 \\k_1' + k_2'(2x) = (x^2-1)^2 \end{array}\right.$$ From the second equation, I have that $k_1' = (x^2-1)^2 - 2xk_2'$, and in the first equation we get that: $$ x(x^2-1)^2 - 2x^2k_2' + (x^2+1)k_2' = 0 \implies (-x^2+1)k_2' = -x(x^2-1)^2$$ and so $k_2' = x(x^2-1) = x³ - x$. Back in the second equation, we obtain: $$k_1' = (x^2-1)^2 - 2x^2(x^2-1) \implies k_1' = (x^2-1)(-x²-1) = -x⁴ + 1.$$ By simple integration, we get $k_1 = -\frac{x^5}{5}+x$ and $k_2 = \frac{x^4}{4} - \frac{x^2}{2}$. By this calculation, the particular solution would be: $$y_p =-\frac{x^6}{5} + x^2 + \frac{x^6}{4} -\frac{x^4}{2} + \frac{x^4}{4} - \frac{x^2}{2} \\ \implies y_p = \frac{x^6}{20} - \frac{x^4}{4} + \frac{x^2}{2}.$$ Differentiating, we get $y_p' = \frac{3x^5}{10} - x^3 + x$ and $y_p'' = \frac{3x^4}{2} - 3x^2 + 1$. Notice that the only constant term on the left side will be a lonely $-1$. But this can't be right, since it'll have to appear a $+1$ term on the right side, to match the $+1$ in $x^4-2x^2 + 1$ on the right side. I don't have a single clue of why this isn't working. This is bothering me since yesterday, and I can't think of anything. Can someone help me?","I want to solve this ODE: $$(x^2 - 1)y'' - 2xy' + 2y = (x^2 - 1)^2.$$ I found out that $y_1 = x$ and $y_2 = x^2+1$ are solutions of the associated  homogeneous equation, $x$ by inspecting, and $x^2+1$ by multiplying $x$ by a function and solving. Now, I know that a particular solution of the problem is given by $y_p = k_1x + k_2(x^2+1)$, for some functions $k_1,k_2$, which must satisfy the following relations: $$\left\{ \begin{array}{l} k_1'x + k_2'(x²+1) = 0 \\k_1' + k_2'(2x) = (x^2-1)^2 \end{array}\right.$$ From the second equation, I have that $k_1' = (x^2-1)^2 - 2xk_2'$, and in the first equation we get that: $$ x(x^2-1)^2 - 2x^2k_2' + (x^2+1)k_2' = 0 \implies (-x^2+1)k_2' = -x(x^2-1)^2$$ and so $k_2' = x(x^2-1) = x³ - x$. Back in the second equation, we obtain: $$k_1' = (x^2-1)^2 - 2x^2(x^2-1) \implies k_1' = (x^2-1)(-x²-1) = -x⁴ + 1.$$ By simple integration, we get $k_1 = -\frac{x^5}{5}+x$ and $k_2 = \frac{x^4}{4} - \frac{x^2}{2}$. By this calculation, the particular solution would be: $$y_p =-\frac{x^6}{5} + x^2 + \frac{x^6}{4} -\frac{x^4}{2} + \frac{x^4}{4} - \frac{x^2}{2} \\ \implies y_p = \frac{x^6}{20} - \frac{x^4}{4} + \frac{x^2}{2}.$$ Differentiating, we get $y_p' = \frac{3x^5}{10} - x^3 + x$ and $y_p'' = \frac{3x^4}{2} - 3x^2 + 1$. Notice that the only constant term on the left side will be a lonely $-1$. But this can't be right, since it'll have to appear a $+1$ term on the right side, to match the $+1$ in $x^4-2x^2 + 1$ on the right side. I don't have a single clue of why this isn't working. This is bothering me since yesterday, and I can't think of anything. Can someone help me?",,['ordinary-differential-equations']
66,how to solve this differential equation which cannot be reduced to homogeneous using standard methods,how to solve this differential equation which cannot be reduced to homogeneous using standard methods,,"I came across this interesting question. $$(y^2 + xy + 1)\,dx + (x^2 + xy + 1)\,dy =0.$$ I tried to make it homogeneous by using an integrating factor but could not proceed through. NOTE: I am asking a question first time in this forum and don't know the exact format on how to ask a good question.","I came across this interesting question. $$(y^2 + xy + 1)\,dx + (x^2 + xy + 1)\,dy =0.$$ I tried to make it homogeneous by using an integrating factor but could not proceed through. NOTE: I am asking a question first time in this forum and don't know the exact format on how to ask a good question.",,['ordinary-differential-equations']
67,Why always the number of arbitrary constant is equal to order of a Differential Equation? [closed],Why always the number of arbitrary constant is equal to order of a Differential Equation? [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Why always the number of arbitrary constant is equal to order of a Differential Equation?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Why always the number of arbitrary constant is equal to order of a Differential Equation?",,['ordinary-differential-equations']
68,Solution of differential equations with discontinuity,Solution of differential equations with discontinuity,,"Suppose that we have scalar differential equation \begin{equation} \dot{x}(t)=u(t) \end{equation} Here $u(t)$ is a piecewise constant function with discontinuity. If the points of discontinuity is infinitely many, how would it affect the solution of the ordinary differential equation above?","Suppose that we have scalar differential equation \begin{equation} \dot{x}(t)=u(t) \end{equation} Here $u(t)$ is a piecewise constant function with discontinuity. If the points of discontinuity is infinitely many, how would it affect the solution of the ordinary differential equation above?",,"['analysis', 'ordinary-differential-equations', 'continuity']"
69,Differential Equations with Discontinuous Forcing Functions,Differential Equations with Discontinuous Forcing Functions,,"$$ y''+y'+1.25y = g(t), \quad t > 0, $$ $$y(0) = 0, \quad  y'(0) = 0 $$ $$g(t) = \left\{ \begin{array}{ll} \sin{t}  &  0 \le t < \pi  \\  0        &  t \ge \pi \end{array}\right.$$ Solve the initial value problem. Draw the graphs of the solution and of the forcing function; explain how they are related. I am confused about what a forcing function is and how to solve it. I attempted to solve it piece wise, finding that the solution for $y'' + y' + 1.25y = \sin{t}$  is $\frac{4}{17} \sin{t} - \frac{16}{17} \cos{t}$. I got the general solution for $y'' + y' + 1.25y = 0$ as $y = ce^{-1/2t}cos(t) + ce^{-1/2t} \sin{t}$. I wasn't sure what to do next. Was I supposed to use laplace transform instead?","$$ y''+y'+1.25y = g(t), \quad t > 0, $$ $$y(0) = 0, \quad  y'(0) = 0 $$ $$g(t) = \left\{ \begin{array}{ll} \sin{t}  &  0 \le t < \pi  \\  0        &  t \ge \pi \end{array}\right.$$ Solve the initial value problem. Draw the graphs of the solution and of the forcing function; explain how they are related. I am confused about what a forcing function is and how to solve it. I attempted to solve it piece wise, finding that the solution for $y'' + y' + 1.25y = \sin{t}$  is $\frac{4}{17} \sin{t} - \frac{16}{17} \cos{t}$. I got the general solution for $y'' + y' + 1.25y = 0$ as $y = ce^{-1/2t}cos(t) + ce^{-1/2t} \sin{t}$. I wasn't sure what to do next. Was I supposed to use laplace transform instead?",,"['ordinary-differential-equations', 'laplace-transform']"
70,General solution of $\frac{\partial^2}{\partial t^2} x(t) + \omega^2 x(t) = 0$,General solution of,\frac{\partial^2}{\partial t^2} x(t) + \omega^2 x(t) = 0,"Consider $$\frac{\partial^2}{\partial t^2} x(t) + \omega^2 x(t) = 0$$ 1) Show that $\left(\frac{\partial x}{\partial t}\right)^2 + \omega^2 x^2$ is constant in $t$, and 2) deduce that the general solution is $x(t) = a \cos \omega t + b \sin \omega t$ Differentiating $\left(\frac{\partial x}{\partial t}\right)^2 + \omega^2 x^2$ with respect to $t$ shows the first part, since $$2 \frac{\partial x}{\partial t}\left(\frac{\partial^2 x}{\partial^2 t} + \omega^2 x\right) = 0$$ hence the derivative is $0$ and the function is constant. Now I want to deduce the second part, but am not sure how to use the first part to show this.","Consider $$\frac{\partial^2}{\partial t^2} x(t) + \omega^2 x(t) = 0$$ 1) Show that $\left(\frac{\partial x}{\partial t}\right)^2 + \omega^2 x^2$ is constant in $t$, and 2) deduce that the general solution is $x(t) = a \cos \omega t + b \sin \omega t$ Differentiating $\left(\frac{\partial x}{\partial t}\right)^2 + \omega^2 x^2$ with respect to $t$ shows the first part, since $$2 \frac{\partial x}{\partial t}\left(\frac{\partial^2 x}{\partial^2 t} + \omega^2 x\right) = 0$$ hence the derivative is $0$ and the function is constant. Now I want to deduce the second part, but am not sure how to use the first part to show this.",,['ordinary-differential-equations']
71,Quick question about solutions of $y'-y+y^8=0$,Quick question about solutions of,y'-y+y^8=0,"I have one small question about solving $y'-y+y^8=0$ but I don't know where. Here's what I do: Let $u(x) = y(x)^{1-8} = y^{-7}$, then $y=u^{- \frac{1}{7}}$ $\frac{du}{dx} = -7 y^{-8} \frac{dy}{dx} = -7 u^{\frac{8}{7}} (u^{-\frac{1}{7}} - u^{-\frac{8}{7}})= -7u+7$ $u=Ce^{-7x}+1, \ \ C>0$ $y=(Ce^{-7x}+1)^{- \frac{1}{7}}$ And here's where I have doubts. Wolfram says that the solutions of this equation are $\frac{?}{\sqrt[7]{C + e^{7x}}}$ Where $? = e^x, \ \sqrt[7]{-1} \cdot e^x, \ (- 1 )^{2/7} e^x, ..., (- 1 )^{6/7} e^x$. My question is, should I include the $n$-th roots of $1$ in my solutions or is not always necessary?","I have one small question about solving $y'-y+y^8=0$ but I don't know where. Here's what I do: Let $u(x) = y(x)^{1-8} = y^{-7}$, then $y=u^{- \frac{1}{7}}$ $\frac{du}{dx} = -7 y^{-8} \frac{dy}{dx} = -7 u^{\frac{8}{7}} (u^{-\frac{1}{7}} - u^{-\frac{8}{7}})= -7u+7$ $u=Ce^{-7x}+1, \ \ C>0$ $y=(Ce^{-7x}+1)^{- \frac{1}{7}}$ And here's where I have doubts. Wolfram says that the solutions of this equation are $\frac{?}{\sqrt[7]{C + e^{7x}}}$ Where $? = e^x, \ \sqrt[7]{-1} \cdot e^x, \ (- 1 )^{2/7} e^x, ..., (- 1 )^{6/7} e^x$. My question is, should I include the $n$-th roots of $1$ in my solutions or is not always necessary?",,['ordinary-differential-equations']
72,How solve inhomogeneous ODE with boundary conditions?,How solve inhomogeneous ODE with boundary conditions?,,"I'm trying to solve this ODE: $$y''-2xy'+3y=x^3 $$ With the conditions: $$\lim_{x\to\pm\infty}e^{-x^2/2}y(x)=\lim_{x\to \pm\infty}e^{-x^2/2}y'(x)=0$$ The homogeneous part is Hermite's equation for noninteger $n$. I tried multiplying by the exponential and take the limit: $$\lim_{x\to\pm\infty} \left( e^{-x^2/2}y'' - 2e^{-x^2/2}xy'= e^{-x^2/2}x^3\right)$$ But I think this leads nowhere. I was told that I should use the generating function : $g(x,t)=e^{-x^2+2tx}$, which I don't see how could it be helpful. So, where's the trick? I would appreciate just some hints.","I'm trying to solve this ODE: $$y''-2xy'+3y=x^3 $$ With the conditions: $$\lim_{x\to\pm\infty}e^{-x^2/2}y(x)=\lim_{x\to \pm\infty}e^{-x^2/2}y'(x)=0$$ The homogeneous part is Hermite's equation for noninteger $n$. I tried multiplying by the exponential and take the limit: $$\lim_{x\to\pm\infty} \left( e^{-x^2/2}y'' - 2e^{-x^2/2}xy'= e^{-x^2/2}x^3\right)$$ But I think this leads nowhere. I was told that I should use the generating function : $g(x,t)=e^{-x^2+2tx}$, which I don't see how could it be helpful. So, where's the trick? I would appreciate just some hints.",,"['ordinary-differential-equations', 'boundary-value-problem']"
73,Proving Nonexistence of Periodic Solutions in a Planar System,Proving Nonexistence of Periodic Solutions in a Planar System,,"I'm asked to prove in two distinctly different ways, that the following system has no periodic solution in the phase plane: \begin{alignat*}{2} \frac{dx}{dt} &= -x+4y \\ \frac{dy}{dt} &= -x-y^{3} \\ \end{alignat*} For the first proof, I simply apply the Bendixson criterion. Let \begin{equation*} f = \left[ \begin{matrix} f_{1} \\ f_{2} \end{matrix} \right]. \end{equation*} We have that \begin{alignat*}{2} f_{1} &= \dot{x} = -x+4y \\ f_{2} &= \dot{y} = -x-y^{3} \end{alignat*} $\therefore$ \begin{alignat*}{2} \frac{\partial f_{1}}{\partial x} &= -1 \\ \frac{\partial f_{2}}{\partial y} &= -3y^{2} \end{alignat*} $\therefore$ \begin{alignat*}{2} \frac{\partial f_{1}}{\partial x} + \frac{\partial f_{2}}{\partial y} &= -1 -2y^{2} \\ &< 0 &&\forall x,y \in \mathbb{R}. \end{alignat*} By the Bendixson criterion, there are no periodic orbits (other than singular points) in a simply connected domain when $f$ is continuously differentiable and $\text{div}(f)$ does not change sign nor does it vanish identically.  Thus, the system has no periodic orbits. I find myself at an impasse for a distinct alternative proof.  First, I tried a change to polar coordinates.  The result I got was \begin{alignat*}{2} \left[ \begin{matrix} \dot{r}\\ \dot{\theta} \end{matrix} \right] &=  \left[ \begin{matrix} 3r\cos\theta\sin\theta+r\cos^{2}\theta-r^{3}\sin^{4}\theta \\ \cos\theta\sin\theta - 4\sin^{2}\theta -\cos^{2}\theta-r^{2}\sin^{3}\theta\cos\theta\end{matrix} \right]. \end{alignat*} From here I did not see a clear way of solving for $r$ and $\theta$  or analyzing $\dot{r}$ to rule out periodicity. Another approach that I considered was to use the theory associated with the Poincare-Bendixson theorem.  For instance, if I could show that every solution crosses a transversal an infinite number of times then I could rule out a periodic solution.  However, I was not sure how to construct such an argument from the given system. I also wonder if an eigenvalue analysis of a linearization could be used here as well?  I would be happy with any distinctly different approach or a hint towards one that I have tried so far.  Thanks in advance.","I'm asked to prove in two distinctly different ways, that the following system has no periodic solution in the phase plane: \begin{alignat*}{2} \frac{dx}{dt} &= -x+4y \\ \frac{dy}{dt} &= -x-y^{3} \\ \end{alignat*} For the first proof, I simply apply the Bendixson criterion. Let \begin{equation*} f = \left[ \begin{matrix} f_{1} \\ f_{2} \end{matrix} \right]. \end{equation*} We have that \begin{alignat*}{2} f_{1} &= \dot{x} = -x+4y \\ f_{2} &= \dot{y} = -x-y^{3} \end{alignat*} $\therefore$ \begin{alignat*}{2} \frac{\partial f_{1}}{\partial x} &= -1 \\ \frac{\partial f_{2}}{\partial y} &= -3y^{2} \end{alignat*} $\therefore$ \begin{alignat*}{2} \frac{\partial f_{1}}{\partial x} + \frac{\partial f_{2}}{\partial y} &= -1 -2y^{2} \\ &< 0 &&\forall x,y \in \mathbb{R}. \end{alignat*} By the Bendixson criterion, there are no periodic orbits (other than singular points) in a simply connected domain when $f$ is continuously differentiable and $\text{div}(f)$ does not change sign nor does it vanish identically.  Thus, the system has no periodic orbits. I find myself at an impasse for a distinct alternative proof.  First, I tried a change to polar coordinates.  The result I got was \begin{alignat*}{2} \left[ \begin{matrix} \dot{r}\\ \dot{\theta} \end{matrix} \right] &=  \left[ \begin{matrix} 3r\cos\theta\sin\theta+r\cos^{2}\theta-r^{3}\sin^{4}\theta \\ \cos\theta\sin\theta - 4\sin^{2}\theta -\cos^{2}\theta-r^{2}\sin^{3}\theta\cos\theta\end{matrix} \right]. \end{alignat*} From here I did not see a clear way of solving for $r$ and $\theta$  or analyzing $\dot{r}$ to rule out periodicity. Another approach that I considered was to use the theory associated with the Poincare-Bendixson theorem.  For instance, if I could show that every solution crosses a transversal an infinite number of times then I could rule out a periodic solution.  However, I was not sure how to construct such an argument from the given system. I also wonder if an eigenvalue analysis of a linearization could be used here as well?  I would be happy with any distinctly different approach or a hint towards one that I have tried so far.  Thanks in advance.",,['ordinary-differential-equations']
74,Solving a 2nd order ODE with trigonometric coefficients,Solving a 2nd order ODE with trigonometric coefficients,,"How would one solve the following eigenvalue problem? $y'' + (\cot(x) - \tan(x)) y' = \lambda y$ for $\lambda$ an arbitrary constant, $x \in [0, \pi/2]$, and boundary conditions $y(0) = y(\pi/2) = 0$","How would one solve the following eigenvalue problem? $y'' + (\cot(x) - \tan(x)) y' = \lambda y$ for $\lambda$ an arbitrary constant, $x \in [0, \pi/2]$, and boundary conditions $y(0) = y(\pi/2) = 0$",,['ordinary-differential-equations']
75,Linear independence of a second-order ODE and solutions with min/max properties,Linear independence of a second-order ODE and solutions with min/max properties,,"Let $y=\phi(x)$ and $y=\psi(x)$ be linearly independent solutions of the ODE $y^{\prime\prime} + p(x)y^{\prime} + q(x)y = 0$, where $p$ and $q$ are continuous on an open interval $I$. I am looking at a problem that asks for a proof of the following statement: Suppose that $x_{0}\in I$ is a zero of $\phi$, then $\phi$ cannot have a relative extremum value at $x_{0}$. This problem also gives a hint by suggesting consideration of the Wronskian. I know that $n$-th order linear homogeneous differential equation always has $n$ linearly independent solutions. If $\phi$, $\psi$ are two linearly independent solutions of the ODE  $y^{\prime\prime} + p(x)y^{\prime} + q(x)y = 0$, then $y = c_{1}\phi + c_{2}\psi$ is the general solution $\forall x \in I$ where $y=0$ only when $c_{1}=c_{2}=0$.  However, if $\phi(x_{0})$ is a relative extremum, then  $\phi^{\prime}(x_{0}) = \phi(x_{0}) = 0$ and the Wronskian, \begin{alignat*}{2} W(x_{0}) &= \left| \begin{matrix} \phi(x_{0}) & \psi(x_{0}) \\ \phi^{\prime}(x_{0}) & \psi^{\prime}(x_{0}) \\ \end{matrix} \right| &= \phi(x_{0})\psi^{\prime}(x_{0}) - \psi(x_{0})\phi^{\prime}(x_{0}) \end{alignat*} vanishes for this particular value. I don't see why this is impossible. I know that the Wronskian can be used to show that a set of differentiable functions is linearly independent on an interval by showing that it does not vanish identically, but my understanding is that the converse is not true without any extra conditions like analyticity.","Let $y=\phi(x)$ and $y=\psi(x)$ be linearly independent solutions of the ODE $y^{\prime\prime} + p(x)y^{\prime} + q(x)y = 0$, where $p$ and $q$ are continuous on an open interval $I$. I am looking at a problem that asks for a proof of the following statement: Suppose that $x_{0}\in I$ is a zero of $\phi$, then $\phi$ cannot have a relative extremum value at $x_{0}$. This problem also gives a hint by suggesting consideration of the Wronskian. I know that $n$-th order linear homogeneous differential equation always has $n$ linearly independent solutions. If $\phi$, $\psi$ are two linearly independent solutions of the ODE  $y^{\prime\prime} + p(x)y^{\prime} + q(x)y = 0$, then $y = c_{1}\phi + c_{2}\psi$ is the general solution $\forall x \in I$ where $y=0$ only when $c_{1}=c_{2}=0$.  However, if $\phi(x_{0})$ is a relative extremum, then  $\phi^{\prime}(x_{0}) = \phi(x_{0}) = 0$ and the Wronskian, \begin{alignat*}{2} W(x_{0}) &= \left| \begin{matrix} \phi(x_{0}) & \psi(x_{0}) \\ \phi^{\prime}(x_{0}) & \psi^{\prime}(x_{0}) \\ \end{matrix} \right| &= \phi(x_{0})\psi^{\prime}(x_{0}) - \psi(x_{0})\phi^{\prime}(x_{0}) \end{alignat*} vanishes for this particular value. I don't see why this is impossible. I know that the Wronskian can be used to show that a set of differentiable functions is linearly independent on an interval by showing that it does not vanish identically, but my understanding is that the converse is not true without any extra conditions like analyticity.",,['ordinary-differential-equations']
76,Find general solution of first order non-linear in a transcendental function,Find general solution of first order non-linear in a transcendental function,,"I have the function $$\frac{dV}{dT}=1-V^2$$ Just looking to see if my working is okay. $$dV=1-V^2dT$$ $$\frac{1}{1-V^2}dV=dT$$ Integrate $$\int{}\frac{1}{1-V^2}dV=\int{}dT$$ Let $V=\tanh(x)$ $\frac{dV}{dx}=sech^2(x)$ $dV=sech^2(x)dx$ $$\int{}\frac{1}{1-\tanh^2(x)}sech^2(x)dx=\int{}dT$$ $1-tanh^2(x)=sech^2(x)$ $$\int{}\frac{1}{sech^2(x)}sech^2(x)dx=\int{}dT$$ $$\int{}\frac{sech^2(x)}{sech^2(x)}dx=\int{}dT$$ $$\int{}\ dx=\int{}\ dT$$ $$x=\int{}dT$$ $V=\tanh(x)$ so take $\tanh^{-1}$ of both sides to find x $$\tanh^{-1}(V)=x$$ $$\tanh^{-1}(V)=\int{}dT$$ $$=T+C$$ Take tanh of both side $$V=\tanh(T+C)$$ Therefore the general solution of $\frac{dV}{dT}=1-V^2$ is $V=\tanh(T+C)$ It is then asking for the particular solution of this for when t=0, v=0 for V is a function of T. I have no idea what they mean.","I have the function $$\frac{dV}{dT}=1-V^2$$ Just looking to see if my working is okay. $$dV=1-V^2dT$$ $$\frac{1}{1-V^2}dV=dT$$ Integrate $$\int{}\frac{1}{1-V^2}dV=\int{}dT$$ Let $V=\tanh(x)$ $\frac{dV}{dx}=sech^2(x)$ $dV=sech^2(x)dx$ $$\int{}\frac{1}{1-\tanh^2(x)}sech^2(x)dx=\int{}dT$$ $1-tanh^2(x)=sech^2(x)$ $$\int{}\frac{1}{sech^2(x)}sech^2(x)dx=\int{}dT$$ $$\int{}\frac{sech^2(x)}{sech^2(x)}dx=\int{}dT$$ $$\int{}\ dx=\int{}\ dT$$ $$x=\int{}dT$$ $V=\tanh(x)$ so take $\tanh^{-1}$ of both sides to find x $$\tanh^{-1}(V)=x$$ $$\tanh^{-1}(V)=\int{}dT$$ $$=T+C$$ Take tanh of both side $$V=\tanh(T+C)$$ Therefore the general solution of $\frac{dV}{dT}=1-V^2$ is $V=\tanh(T+C)$ It is then asking for the particular solution of this for when t=0, v=0 for V is a function of T. I have no idea what they mean.",,"['calculus', 'integration', 'ordinary-differential-equations', 'indefinite-integrals', 'transcendental-equations']"
77,Help in Measuring Error on Estimates of Differential Equations,Help in Measuring Error on Estimates of Differential Equations,,"I am working on a project for class where I have to estimate the solutions to a damped harmonic oscillator ($x''+2 \gamma x'+ \omega^2 x=0$) and compare three methods for doing so (Third Order Runge-Kutta, Conformal Explicit Leap-Frog, Conformal Implicit Midpoint) Part of this is comparing what error looks like at each step (using the easily obtainable exact solution) but I am having trouble coming up with a way to measure error. Normally there is always the fallback on real error, but this is the first project I've done where the solution has a possibility of being 0 (such that real error would explode when the solution is near zero) so that is not a possibility. Absolute error doesn't help either since the solution is going to 0, the graph of error looks like the dampened harmonic graph itself. So I am wondering what might be a good way to calculate error? I am working in MATLAB and have been given the code for all 3 methods so I am confident they work. I know that Runge-Kutta should be better than the other two in shorter intervals, but the other two are better in the long run. Thanks for any help","I am working on a project for class where I have to estimate the solutions to a damped harmonic oscillator ($x''+2 \gamma x'+ \omega^2 x=0$) and compare three methods for doing so (Third Order Runge-Kutta, Conformal Explicit Leap-Frog, Conformal Implicit Midpoint) Part of this is comparing what error looks like at each step (using the easily obtainable exact solution) but I am having trouble coming up with a way to measure error. Normally there is always the fallback on real error, but this is the first project I've done where the solution has a possibility of being 0 (such that real error would explode when the solution is near zero) so that is not a possibility. Absolute error doesn't help either since the solution is going to 0, the graph of error looks like the dampened harmonic graph itself. So I am wondering what might be a good way to calculate error? I am working in MATLAB and have been given the code for all 3 methods so I am confident they work. I know that Runge-Kutta should be better than the other two in shorter intervals, but the other two are better in the long run. Thanks for any help",,"['ordinary-differential-equations', 'error-propagation']"
78,Solving Wave Equations with different Boundary Conditions,Solving Wave Equations with different Boundary Conditions,,"Right now I'm studying the wave equation and how to solve it with different boundary conditions (i.e. $u(x,0);u(0,t);u_t(x,0);u_x(x,0);u(x,x);u_t(x,x)...$) I know how to solve it when the boundary conditions are $u(x,0)=f(x)$ and $u_t(x,0)=g(x)$ with d'Alambert's formula. But I don't know how to solve it in any other case. Does anyone know of any book, notes, etc. where I can find an explanation on how to solve the wave equation with many kinds of boundary conditions? Examples: $$u_{tt}-4u_{xx}=16$$ $$u(0,t)=t;u_x(0,t)=0$$ or $$u_{tt}-4u_{xx}=2$$ $$u(x,x)=x^2;u_t(x,x)=x$$ I'm talking about the wave equation with many kinds of initial conditions, not just only the ones in d'Alambert's solution. Thanks.","Right now I'm studying the wave equation and how to solve it with different boundary conditions (i.e. $u(x,0);u(0,t);u_t(x,0);u_x(x,0);u(x,x);u_t(x,x)...$) I know how to solve it when the boundary conditions are $u(x,0)=f(x)$ and $u_t(x,0)=g(x)$ with d'Alambert's formula. But I don't know how to solve it in any other case. Does anyone know of any book, notes, etc. where I can find an explanation on how to solve the wave equation with many kinds of boundary conditions? Examples: $$u_{tt}-4u_{xx}=16$$ $$u(0,t)=t;u_x(0,t)=0$$ or $$u_{tt}-4u_{xx}=2$$ $$u(x,x)=x^2;u_t(x,x)=x$$ I'm talking about the wave equation with many kinds of initial conditions, not just only the ones in d'Alambert's solution. Thanks.",,"['ordinary-differential-equations', 'partial-differential-equations', 'boundary-value-problem', 'wave-equation']"
79,What does it mean for a solution to a Linear DE to be homogeneous?,What does it mean for a solution to a Linear DE to be homogeneous?,,"I'm a physics student and I've just being going over some definitions for differential equations. I don't think I fully understand what a homogeneous equation is and the Wikipedia article says that a Linear DE has homogeneous solutions which add to form other homogeneous solutions. What does it mean for a solution to be homogeneous? Usually, a DE is said to be homogeneous if it is equal to zero. eg: $$u'' + u' -4u = 0$$ But that's not the full story is it? This definition of ""homogeneous"" is making me think that a homogeneous solution is equal to zero. Alas, mathematicians, please enlighten me.","I'm a physics student and I've just being going over some definitions for differential equations. I don't think I fully understand what a homogeneous equation is and the Wikipedia article says that a Linear DE has homogeneous solutions which add to form other homogeneous solutions. What does it mean for a solution to be homogeneous? Usually, a DE is said to be homogeneous if it is equal to zero. eg: $$u'' + u' -4u = 0$$ But that's not the full story is it? This definition of ""homogeneous"" is making me think that a homogeneous solution is equal to zero. Alas, mathematicians, please enlighten me.",,"['ordinary-differential-equations', 'homogeneous-equation']"
80,Minimize an integral of a function and its derivative,Minimize an integral of a function and its derivative,,"I'm trying to find the function $x(t)$ that minimizes the quantity $$\int_0^1 (x^2 +\dot{x}^2)dt$$ given that $x(0)=1$ and $x(1)$ is free (so I'm expecting the result to be a function of $x(1)$ itself). I am however utterly lost on this. One could say that the smallest value the integral could take is $0$, and therefore solve $x^2 +\dot{x}^2 = 0 \Rightarrow \dot{x}^2 = -x^2$, but this is an ODE that I find particularly weird (and even if it does make sense, I have no idea how to solve it). Any insight or solution would be greatly appreciated.","I'm trying to find the function $x(t)$ that minimizes the quantity $$\int_0^1 (x^2 +\dot{x}^2)dt$$ given that $x(0)=1$ and $x(1)$ is free (so I'm expecting the result to be a function of $x(1)$ itself). I am however utterly lost on this. One could say that the smallest value the integral could take is $0$, and therefore solve $x^2 +\dot{x}^2 = 0 \Rightarrow \dot{x}^2 = -x^2$, but this is an ODE that I find particularly weird (and even if it does make sense, I have no idea how to solve it). Any insight or solution would be greatly appreciated.",,['ordinary-differential-equations']
81,Stable and unstable manifolds,Stable and unstable manifolds,,"I would like to know how to finish this problem, and if what I have done so far is correct. Problem: Determine the stable and unstable manifolds for the rest point of the system $$\dot{x}=2x-(2+y)e^y, \dot{y}=-y.$$ Attempt and outline: The rest point of the system is $(1,0).$ Now, I changed the coordinates of the system to be centered at the origin, giving me the system $$\dot{x}=2(x+1)-(2+y)e^y, \dot{y}=-y.$$ I have found the solutions of the (shifted) system to be $$x=e^{{c_1}e^{-t}}+c_2e^{2t}-1, y=c_1e^{-t}$$ Then I computed $Df(x_0)$ and obtained the eigenvalues of the associated linear system as $\lambda_1 = 2, \lambda_2 = -1$. My question is: Now that I have the stable and unstable eigenspaces (manifolds) for the linear system, how can I find them explicitly for the nonlinear system? Thanks for any help.","I would like to know how to finish this problem, and if what I have done so far is correct. Problem: Determine the stable and unstable manifolds for the rest point of the system $$\dot{x}=2x-(2+y)e^y, \dot{y}=-y.$$ Attempt and outline: The rest point of the system is $(1,0).$ Now, I changed the coordinates of the system to be centered at the origin, giving me the system $$\dot{x}=2(x+1)-(2+y)e^y, \dot{y}=-y.$$ I have found the solutions of the (shifted) system to be $$x=e^{{c_1}e^{-t}}+c_2e^{2t}-1, y=c_1e^{-t}$$ Then I computed $Df(x_0)$ and obtained the eigenvalues of the associated linear system as $\lambda_1 = 2, \lambda_2 = -1$. My question is: Now that I have the stable and unstable eigenspaces (manifolds) for the linear system, how can I find them explicitly for the nonlinear system? Thanks for any help.",,[]
82,differential equation of a harmonic function,differential equation of a harmonic function,,"Let $v$ be a smooth harmonic function on $R^n$. If $r^2=\sum_{i=1}^{n}|x_i|^2$, where $x=(x_1,x_2,....x_n) \in R^n$ and if $v$ is a radial function i.e $v(x)=v(r)$, write down the differential equation satisfied by $v$.","Let $v$ be a smooth harmonic function on $R^n$. If $r^2=\sum_{i=1}^{n}|x_i|^2$, where $x=(x_1,x_2,....x_n) \in R^n$ and if $v$ is a radial function i.e $v(x)=v(r)$, write down the differential equation satisfied by $v$.",,['ordinary-differential-equations']
83,Tricky Differential Equation Problem,Tricky Differential Equation Problem,,"I am unsure of how to tackle the following differential equation: $$ dx+ x\,dy = e^{-y}\sec^2y\,dy$$ I have done the following so far: $$dx + x\,dy = e^{-y} \sec^2y \, dy$$ $$=>dx = e^{-y} \sec^2y \, dy - x \, dy$$ $$=>dx = (e^{-y} \sec^2y - x) \, dy$$ $$=>dx/dy = (e^{-y}) \sec^2y - x $$ Is this the correct approach? How can the problem be solved after this? (Not homework: preparing for a test)","I am unsure of how to tackle the following differential equation: $$ dx+ x\,dy = e^{-y}\sec^2y\,dy$$ I have done the following so far: $$dx + x\,dy = e^{-y} \sec^2y \, dy$$ $$=>dx = e^{-y} \sec^2y \, dy - x \, dy$$ $$=>dx = (e^{-y} \sec^2y - x) \, dy$$ $$=>dx/dy = (e^{-y}) \sec^2y - x $$ Is this the correct approach? How can the problem be solved after this? (Not homework: preparing for a test)",,"['calculus', 'integration', 'ordinary-differential-equations', 'multivariable-calculus']"
84,system of 2 linear differential equations with variable coefficients,system of 2 linear differential equations with variable coefficients,,"I have a system of 2 linear diff equations but with a variable coefficients: $$f''(x)+af'(x)+(1+x)g'(x)+bg(x)=0\\g''(x)+ag'(x)+(1-x)f'(x)-bf(x)=0$$ where $a,b$ are some positive constants. I have no idea how to solve this system (or even if it has an anayltic solution), I'll appreciate any help or some reference.","I have a system of 2 linear diff equations but with a variable coefficients: $$f''(x)+af'(x)+(1+x)g'(x)+bg(x)=0\\g''(x)+ag'(x)+(1-x)f'(x)-bf(x)=0$$ where $a,b$ are some positive constants. I have no idea how to solve this system (or even if it has an anayltic solution), I'll appreciate any help or some reference.",,"['ordinary-differential-equations', 'systems-of-equations']"
85,closed form solution to the heat equation,closed form solution to the heat equation,,"Let smooth functions $f(x) , g(t)$ are given solve the heat equation on the semi infinite domain $(a,\infty) \times (0,T)$. for simplicity, we can let $a = 0$. \begin{eqnarray} &&u_t(x,t) = u_{xx}(x,t) \quad a<x<\infty , \quad 0<t<T \\ &&u(x,0) = f(x), \quad a<x\\ &&u(a,t) = g(t),  \quad 0<t<T \\ && lim_{x\rightarrow \infty} u(x,t) = 0 . \end{eqnarray} i need the  closed form solution to the problem subject to $f(x) , g(t)$ Thanks for any help in advance","Let smooth functions $f(x) , g(t)$ are given solve the heat equation on the semi infinite domain $(a,\infty) \times (0,T)$. for simplicity, we can let $a = 0$. \begin{eqnarray} &&u_t(x,t) = u_{xx}(x,t) \quad a<x<\infty , \quad 0<t<T \\ &&u(x,0) = f(x), \quad a<x\\ &&u(a,t) = g(t),  \quad 0<t<T \\ && lim_{x\rightarrow \infty} u(x,t) = 0 . \end{eqnarray} i need the  closed form solution to the problem subject to $f(x) , g(t)$ Thanks for any help in advance",,"['ordinary-differential-equations', 'partial-differential-equations', 'mathematical-physics']"
86,"The trace-determinant plane, classification of equilibria of differential equations","The trace-determinant plane, classification of equilibria of differential equations",,What are some easy ways to remember each of the different behaviors of general solutions of ordinary differential equations in the trace-determinant plane? For differential equations of the form $\frac{dY}{dt} = AY$ where $A$ is a $2 \times 2$-matrix and $Y$ is the column vector $(x\ y)$.,What are some easy ways to remember each of the different behaviors of general solutions of ordinary differential equations in the trace-determinant plane? For differential equations of the form $\frac{dY}{dt} = AY$ where $A$ is a $2 \times 2$-matrix and $Y$ is the column vector $(x\ y)$.,,"['linear-algebra', 'ordinary-differential-equations', 'determinant', 'trace']"
87,How to identify this power series as $k\sin(k/x)$?,How to identify this power series as ?,k\sin(k/x),"In this question , a functional equation is solved for functions with a power series. We find a recursive formula: (copied from the answer by user achille hui) \begin{align} ( 2^1 - 3 ) a_2 &= 0\\ ( 2^2 - 4 ) a_3 &= 2 a_2 a_2 \iff 0 = 0 \text{ ( i.e. } a_3\;\text{ is arbitrary )}\\ ( 2^3 - 5 ) a_4 &= 3 a_3 a_2 + 2 a_2 a_3\\ ( 2^4 - 6 ) a_5 &= 4 a_4 a_2 + 3 a_3 a_3 + 2 a_2 a_4\\ &\;\vdots \end{align} According to the answer, it turns out that $\sum a_i x^i$ is equal to $k \sin(x/k)$ or $k \sinh(x/k)$, depending on the sign of $a_3$. How does one see this? A messy inductive proof will probably prove equality, but that is not what I am looking for. What methods/tricks are there to find a closed form for the coefficents $a_i$ from the recursion, and then recognize it as the power series of $\sin$ or $\sinh$?","In this question , a functional equation is solved for functions with a power series. We find a recursive formula: (copied from the answer by user achille hui) \begin{align} ( 2^1 - 3 ) a_2 &= 0\\ ( 2^2 - 4 ) a_3 &= 2 a_2 a_2 \iff 0 = 0 \text{ ( i.e. } a_3\;\text{ is arbitrary )}\\ ( 2^3 - 5 ) a_4 &= 3 a_3 a_2 + 2 a_2 a_3\\ ( 2^4 - 6 ) a_5 &= 4 a_4 a_2 + 3 a_3 a_3 + 2 a_2 a_4\\ &\;\vdots \end{align} According to the answer, it turns out that $\sum a_i x^i$ is equal to $k \sin(x/k)$ or $k \sinh(x/k)$, depending on the sign of $a_3$. How does one see this? A messy inductive proof will probably prove equality, but that is not what I am looking for. What methods/tricks are there to find a closed form for the coefficents $a_i$ from the recursion, and then recognize it as the power series of $\sin$ or $\sinh$?",,"['real-analysis', 'ordinary-differential-equations', 'power-series', 'generating-functions', 'functional-equations']"
88,Where do power series solutions of differential equation exist?,Where do power series solutions of differential equation exist?,,"Without actually solving the differential equation $$(\cos x)y'' + y' + 5y = 0,$$ find lower bounds for the radii of convergence of the power series solutions about $x=0$ and $x=1$. Any idea guys? I thinks singular points are required to answer this. But the problem is I don't understand singular points really well.  Can you help me?","Without actually solving the differential equation $$(\cos x)y'' + y' + 5y = 0,$$ find lower bounds for the radii of convergence of the power series solutions about $x=0$ and $x=1$. Any idea guys? I thinks singular points are required to answer this. But the problem is I don't understand singular points really well.  Can you help me?",,['ordinary-differential-equations']
89,Software for solving fractional differential equations numerically,Software for solving fractional differential equations numerically,,"I have been trying to find information on how to solve fractional differential equations numerically with the usual maths software (Mathematica, Maple, Matlab, etc). Or to find an alternative program to do that. I had no success. Thanks.","I have been trying to find information on how to solve fractional differential equations numerically with the usual maths software (Mathematica, Maple, Matlab, etc). Or to find an alternative program to do that. I had no success. Thanks.",,"['ordinary-differential-equations', 'numerical-methods', 'math-software', 'fractional-calculus']"
90,Solve $x=y\frac{dy}{dx}-\left(\frac{dy}{dx}\right)^{2}$,Solve,x=y\frac{dy}{dx}-\left(\frac{dy}{dx}\right)^{2},"I've recently been learning about differential equations, and my teacher has been giving some particularly difficult examples out for those of us who finish early. He gave us the following differential equation: $$x=y\frac{dy}{dx}-\left(\frac{dy}{dx}\right)^{2}$$ With the hint ""Differentiate with respect to $y$, then let $\frac{dy}{dx}=p$"". I've solved it after the lesson and would like someone to check whether or not I've done so correctly. Following the hint, $$\frac{dx}{dy}=\frac{1}{p}=p+y\frac{dp}{dy}-2p\frac{dp}{dy}$$ Which rearranges to $$p-\frac{1}{p}+y\frac{dp}{dy}=2p\frac{dp}{dy}$$ Multiplying through by $\frac{dy}{dp}$, $$\left(p-\frac{1}{p}\right)\frac{dy}{dp}+y=2p$$ Rearranging into the form $\frac{dy}{dp}+f(p)y=g(p)$, and writing $p-\frac{1}{p}=\frac{p^{2}-1}{p}$ $$\frac{dy}{dp}+\frac{p}{p^{2}-1}y=\frac{2p^{2}}{p^{2}-1}$$ Our integrating factor is $$e^{\int\frac{p}{p^{2}-1}dp}=e^{\frac{1}{2}\ln(p^{2}-1)}=\sqrt{p^{2}-1}$$ So we have $$\sqrt{p^{2}-1}\frac{dy}{dp}+\frac{p}{\sqrt{p^{2}-1}}y=\frac{2p^{2}}{\sqrt{p^{2}-1}}$$ The left hand side is, by design, $\frac{d}{dp}(y\sqrt{p^{2}-1})$, and the right hand side can be integrated as follows: $$\int \frac{2p^{2}}{\sqrt{p^{2}-1}}dp=\int\frac{2(p^{2}-1)+2}{\sqrt{p^{2}-1}}dp=\int 2\sqrt{p^{2}-1}dp+\int \frac{2}{\sqrt{p^{2}-1}} dp$$ These can both be solved using the substitution $p=\cosh(u)$, and gives $$y\sqrt{p^{2}-1}=p\sqrt{p^{2}+1}-\cosh^{-1}(p)+\alpha$$ Where $\alpha$ is an arbitrary constant. Dividing, we finally have $$y=p+\frac{\cosh^{-1}(p)+\alpha}{\sqrt{p^{2}-1}}$$ Here I was stuck for a long time, until I realised that we also have the equation we began with: $x=py-p^{2}$, which we can solve as a quadratic in $p$ to also get $$p=\frac{y\pm \sqrt{y^{2}-4x}}{2} \implies p^{2}=\frac{y^{2}+y^{2}-4x \pm 2y\sqrt{y^{2}-4x}}{4}$$ Thus, $$y=\frac{y\pm \sqrt{y^{2}-4x}}{2}+\frac{\cosh^{-1}\left(\frac{y\pm \sqrt{y^{2}-4x}}{2}\right)+\alpha}{\sqrt{\frac{y^{2}-2x \pm y\sqrt{y^{2}-4x}}{2}-1}}$$ This is as good as I can do - we have a relation between $y$ and $x$ - am I done? Is this the correct answer? Is there an easier way? Thank you for your time. Following the suggestion by @Valentin $$\frac{dy}{dp}=1+\frac{\frac{1}{\sqrt{p^{2}-1}}\sqrt{p^{2}-1}-p\frac{\cosh^{-1}(p)+\alpha}{\sqrt{p^{2}-1}}}{p^{2}-1}=1+p\frac{1-\frac{\cosh^{-1}(p)+\alpha}{\sqrt{p^{2}-1}}}{p^{2}-1}$$ Hence, $$dx=\frac{dy}{p}=dp\left(\frac{1}{p}+\frac{1-\frac{\cosh^{-1}(p)+\alpha}{\sqrt{p^{2}-1}}}{p^{2}-1}\right)$$","I've recently been learning about differential equations, and my teacher has been giving some particularly difficult examples out for those of us who finish early. He gave us the following differential equation: $$x=y\frac{dy}{dx}-\left(\frac{dy}{dx}\right)^{2}$$ With the hint ""Differentiate with respect to $y$, then let $\frac{dy}{dx}=p$"". I've solved it after the lesson and would like someone to check whether or not I've done so correctly. Following the hint, $$\frac{dx}{dy}=\frac{1}{p}=p+y\frac{dp}{dy}-2p\frac{dp}{dy}$$ Which rearranges to $$p-\frac{1}{p}+y\frac{dp}{dy}=2p\frac{dp}{dy}$$ Multiplying through by $\frac{dy}{dp}$, $$\left(p-\frac{1}{p}\right)\frac{dy}{dp}+y=2p$$ Rearranging into the form $\frac{dy}{dp}+f(p)y=g(p)$, and writing $p-\frac{1}{p}=\frac{p^{2}-1}{p}$ $$\frac{dy}{dp}+\frac{p}{p^{2}-1}y=\frac{2p^{2}}{p^{2}-1}$$ Our integrating factor is $$e^{\int\frac{p}{p^{2}-1}dp}=e^{\frac{1}{2}\ln(p^{2}-1)}=\sqrt{p^{2}-1}$$ So we have $$\sqrt{p^{2}-1}\frac{dy}{dp}+\frac{p}{\sqrt{p^{2}-1}}y=\frac{2p^{2}}{\sqrt{p^{2}-1}}$$ The left hand side is, by design, $\frac{d}{dp}(y\sqrt{p^{2}-1})$, and the right hand side can be integrated as follows: $$\int \frac{2p^{2}}{\sqrt{p^{2}-1}}dp=\int\frac{2(p^{2}-1)+2}{\sqrt{p^{2}-1}}dp=\int 2\sqrt{p^{2}-1}dp+\int \frac{2}{\sqrt{p^{2}-1}} dp$$ These can both be solved using the substitution $p=\cosh(u)$, and gives $$y\sqrt{p^{2}-1}=p\sqrt{p^{2}+1}-\cosh^{-1}(p)+\alpha$$ Where $\alpha$ is an arbitrary constant. Dividing, we finally have $$y=p+\frac{\cosh^{-1}(p)+\alpha}{\sqrt{p^{2}-1}}$$ Here I was stuck for a long time, until I realised that we also have the equation we began with: $x=py-p^{2}$, which we can solve as a quadratic in $p$ to also get $$p=\frac{y\pm \sqrt{y^{2}-4x}}{2} \implies p^{2}=\frac{y^{2}+y^{2}-4x \pm 2y\sqrt{y^{2}-4x}}{4}$$ Thus, $$y=\frac{y\pm \sqrt{y^{2}-4x}}{2}+\frac{\cosh^{-1}\left(\frac{y\pm \sqrt{y^{2}-4x}}{2}\right)+\alpha}{\sqrt{\frac{y^{2}-2x \pm y\sqrt{y^{2}-4x}}{2}-1}}$$ This is as good as I can do - we have a relation between $y$ and $x$ - am I done? Is this the correct answer? Is there an easier way? Thank you for your time. Following the suggestion by @Valentin $$\frac{dy}{dp}=1+\frac{\frac{1}{\sqrt{p^{2}-1}}\sqrt{p^{2}-1}-p\frac{\cosh^{-1}(p)+\alpha}{\sqrt{p^{2}-1}}}{p^{2}-1}=1+p\frac{1-\frac{\cosh^{-1}(p)+\alpha}{\sqrt{p^{2}-1}}}{p^{2}-1}$$ Hence, $$dx=\frac{dy}{p}=dp\left(\frac{1}{p}+\frac{1-\frac{\cosh^{-1}(p)+\alpha}{\sqrt{p^{2}-1}}}{p^{2}-1}\right)$$",,"['ordinary-differential-equations', 'integration']"
91,Differential equation $y'=\frac{1-xy}{y-x^2}$,Differential equation,y'=\frac{1-xy}{y-x^2},I tried many thing but I could not find a method to solve the  differential equation $$y'=\frac{1-xy}{y-x^2}$$ $$y'=-x+\frac{1-x^3}{y-x^2}$$ $Z=y-x^2$ $$Z'+2x=-x+\frac{1-x^3}{Z}$$ $$Z(Z'+3x)=1-x^3$$ Could you please give me hint which method can be used for such first order differential equations? Thanks a lot for advice and answers,I tried many thing but I could not find a method to solve the  differential equation $$y'=\frac{1-xy}{y-x^2}$$ $$y'=-x+\frac{1-x^3}{y-x^2}$$ $Z=y-x^2$ $$Z'+2x=-x+\frac{1-x^3}{Z}$$ $$Z(Z'+3x)=1-x^3$$ Could you please give me hint which method can be used for such first order differential equations? Thanks a lot for advice and answers,,['ordinary-differential-equations']
92,Applying the fundamental theorem of calculus,Applying the fundamental theorem of calculus,,"If $f:\mathbb{R}^n \to \mathbb{R}$ is differentiable and $f(0)=0$, prove that exist continuous $g_i:\mathbb{R}^n \to \mathbb{R}$ such that $$f(x)=\sum_{i=1}^{n} x_i g_i(x).$$ Hint. Write $f(x)-f(0)=f(tx)|_{t=0}^1$ and apply the fundamental theorem of calculus. It says apply the fundamental theorem of calculus? but how do i do it exactly?","If $f:\mathbb{R}^n \to \mathbb{R}$ is differentiable and $f(0)=0$, prove that exist continuous $g_i:\mathbb{R}^n \to \mathbb{R}$ such that $$f(x)=\sum_{i=1}^{n} x_i g_i(x).$$ Hint. Write $f(x)-f(0)=f(tx)|_{t=0}^1$ and apply the fundamental theorem of calculus. It says apply the fundamental theorem of calculus? but how do i do it exactly?",,"['calculus', 'ordinary-differential-equations']"
93,Differential Equation $y'' - 4y' + 4y = 0$,Differential Equation,y'' - 4y' + 4y = 0,"[1] $y'' - 4y' + 4y = 0$ Usually problem like these will have the answer in the form $C_1e^a + C_2e^b ... $ where $a $ and $b$ are the roots of the characteristic equation $e^{rt}$ $$ y = e^{rt} $$ $$ y' = re^{rt}.. y'' = r^2 e^{rt} $$  $$ r^2e^{rt} - 4e^{rt} + 4e^{rt} = 0$$ $$ e^{rt}(r-2)(r-2) = 0$$ $$ y= C_1e^{2t} + C_2e^{2t}$$ However, this is not correct as the answer is $ y = C_1e^{2t} + C_2te^{2t}$ ! I just don't know why. [2] For a similar problem , $y'' + 3y' - 4y = 0 $ I did the exact same thing and the answer is $$y = c_1e^t + c_2e^{-4t} $$ How are [1] and [2] different? They look the same, why does [2]'s solution have an extra factor of t.","[1] $y'' - 4y' + 4y = 0$ Usually problem like these will have the answer in the form $C_1e^a + C_2e^b ... $ where $a $ and $b$ are the roots of the characteristic equation $e^{rt}$ $$ y = e^{rt} $$ $$ y' = re^{rt}.. y'' = r^2 e^{rt} $$  $$ r^2e^{rt} - 4e^{rt} + 4e^{rt} = 0$$ $$ e^{rt}(r-2)(r-2) = 0$$ $$ y= C_1e^{2t} + C_2e^{2t}$$ However, this is not correct as the answer is $ y = C_1e^{2t} + C_2te^{2t}$ ! I just don't know why. [2] For a similar problem , $y'' + 3y' - 4y = 0 $ I did the exact same thing and the answer is $$y = c_1e^t + c_2e^{-4t} $$ How are [1] and [2] different? They look the same, why does [2]'s solution have an extra factor of t.",,['ordinary-differential-equations']
94,How do I squeeze a $\theta(t)$ and $\varphi(t)$ out of this?,How do I squeeze a  and  out of this?,\theta(t) \varphi(t),"A ball attached to a fixed-length massless rod swings about under gravity. Mathematically: $$L=T-U=\frac{MR^2}{2}(\sin^2(\theta)\dot{\varphi}^2+\dot{\theta}^2)+MgR \cos(\theta)$$ $$\frac{d}{dt}\left(\frac{\partial L}{\partial \dot{\theta}}\right)=\frac{\partial L}{\partial \theta}$$ $$MR^2 \ddot{\theta}=MR^2\sin(\theta)\cos(\theta)\dot{\varphi}^2-MgR \sin(\theta)$$ $$MR^2 \ddot{\theta}=\frac{MR^2}{2}\sin(2\theta)\dot{\varphi}^2-MgR \sin(\theta)\tag{1}$$ $$\frac{d}{dt}\left(\frac{\partial L}{\partial \dot{\varphi}}\right)=\frac{\partial L}{\partial \varphi}$$ $$\frac{d}{dt}(MR^2 \sin^2(\theta) \dot{\varphi})=0$$ $$MR^2( \sin^2(\theta) \ddot{\varphi}+2\sin(\theta)\cos(\theta) \dot{\varphi}\dot{\theta})=0$$ $$MR^2( \sin^2(\theta) \ddot{\varphi}+\sin(2\theta) \dot{\varphi}\dot{\theta})=0\tag{2}.$$ Any ideas as to the domestication of these equations? Any approximative tricks? Edit: I forget that it's useful if I post some of my own insights to aid answerers $\sin(\theta)=\theta+o(\theta^3)$, so approximate $\sin(\theta) \approx \theta$ For cosine, it's not so easy, because $cos(\theta)=1+o(\theta^2)$ doesn't hold for as long, so perhaps $cos(\theta) \approx 1-\frac{1}{2}\theta^2$ could work, but there's enough trouble already with the equations being nonlinear before adding a squared term.","A ball attached to a fixed-length massless rod swings about under gravity. Mathematically: $$L=T-U=\frac{MR^2}{2}(\sin^2(\theta)\dot{\varphi}^2+\dot{\theta}^2)+MgR \cos(\theta)$$ $$\frac{d}{dt}\left(\frac{\partial L}{\partial \dot{\theta}}\right)=\frac{\partial L}{\partial \theta}$$ $$MR^2 \ddot{\theta}=MR^2\sin(\theta)\cos(\theta)\dot{\varphi}^2-MgR \sin(\theta)$$ $$MR^2 \ddot{\theta}=\frac{MR^2}{2}\sin(2\theta)\dot{\varphi}^2-MgR \sin(\theta)\tag{1}$$ $$\frac{d}{dt}\left(\frac{\partial L}{\partial \dot{\varphi}}\right)=\frac{\partial L}{\partial \varphi}$$ $$\frac{d}{dt}(MR^2 \sin^2(\theta) \dot{\varphi})=0$$ $$MR^2( \sin^2(\theta) \ddot{\varphi}+2\sin(\theta)\cos(\theta) \dot{\varphi}\dot{\theta})=0$$ $$MR^2( \sin^2(\theta) \ddot{\varphi}+\sin(2\theta) \dot{\varphi}\dot{\theta})=0\tag{2}.$$ Any ideas as to the domestication of these equations? Any approximative tricks? Edit: I forget that it's useful if I post some of my own insights to aid answerers $\sin(\theta)=\theta+o(\theta^3)$, so approximate $\sin(\theta) \approx \theta$ For cosine, it's not so easy, because $cos(\theta)=1+o(\theta^2)$ doesn't hold for as long, so perhaps $cos(\theta) \approx 1-\frac{1}{2}\theta^2$ could work, but there's enough trouble already with the equations being nonlinear before adding a squared term.",,"['ordinary-differential-equations', 'mathematical-physics']"
95,Structure of the solution set of a 2nd order ODE,Structure of the solution set of a 2nd order ODE,,"Let $V \in C^1(\mathbb{R}^d)$, and let $S$ denote the solution set for the problem $$ \ddot{x}+\nabla V(x)=0,\quad x(T)-x(0)=0=\dot{x}(T)- \dot{x}(0). $$ When $d=1, T=2\pi$, and $V(x)=\frac12|x|^2$ it is clear that $S$ is the two-dimensional vector space  $$ \{\alpha\cos+\beta\sin:\ \alpha,\beta\in \mathbb{R}\} \simeq \mathbb{R}^2. $$ For general $V$ (when the ODE is non-linear), does $S$ have some kind of manifold structure?","Let $V \in C^1(\mathbb{R}^d)$, and let $S$ denote the solution set for the problem $$ \ddot{x}+\nabla V(x)=0,\quad x(T)-x(0)=0=\dot{x}(T)- \dot{x}(0). $$ When $d=1, T=2\pi$, and $V(x)=\frac12|x|^2$ it is clear that $S$ is the two-dimensional vector space  $$ \{\alpha\cos+\beta\sin:\ \alpha,\beta\in \mathbb{R}\} \simeq \mathbb{R}^2. $$ For general $V$ (when the ODE is non-linear), does $S$ have some kind of manifold structure?",,['ordinary-differential-equations']
96,correct combination of differentiation rules,correct combination of differentiation rules,,"I am trying to calculate the derivative of a rather complex function for my homework. I think I have found the solution, it just seems too bulky for my taste. See the bottom for specific questions I have regarding my solution. $ f(x)=\frac{\overbrace{\sin x}^\text{u(x)}\cdot \overbrace{e^x+x^3}^\text{v(x)}}{\underbrace{x^3+2x+2}_\text{w(x)}} $ $ u'(x)=\cos x $, $ v'(x)=e^x+3x^2 $, $ w'(x)=3x^2+4x $ $ \begin{align*} u'v'(x)&=(\sin x\cdot e^x+3x^2)+(\cos x\cdot x^3)\\ &= e^x\cdot\sin x+3x\cdot\sin x+x^3\cdot\cos x \end{align*} $ $ \begin{align*} f'(x)&= \frac{u'v'(x)\cdot w(x)-w'(x)\cdot uv(x)}{w(x)^2}\\ &= \frac{(e^x\cdot\sin x+3x\cdot\sin x+x^3\cdot\cos x)\cdot (x^3+2x^2+2)-(3x^2+4x)(\sin x\cdot e^x+x^3)}{(x^3+2x^2+2)^2} \end{align*} $ The obvious question: Is this derivative correct? Especially: Is it really possible to calculate the derivative by splitting the function into part-functions and calculating them together following the differentiation rules, the way I did it? Is it possible to reduce the summands by applying some rule I am not aware of? E.g. reducing $(e^x\cdot\sin x+3x\cdot\sin x+x^3\cdot\cos x)$ to something with just one $\sin$ or something?","I am trying to calculate the derivative of a rather complex function for my homework. I think I have found the solution, it just seems too bulky for my taste. See the bottom for specific questions I have regarding my solution. $ f(x)=\frac{\overbrace{\sin x}^\text{u(x)}\cdot \overbrace{e^x+x^3}^\text{v(x)}}{\underbrace{x^3+2x+2}_\text{w(x)}} $ $ u'(x)=\cos x $, $ v'(x)=e^x+3x^2 $, $ w'(x)=3x^2+4x $ $ \begin{align*} u'v'(x)&=(\sin x\cdot e^x+3x^2)+(\cos x\cdot x^3)\\ &= e^x\cdot\sin x+3x\cdot\sin x+x^3\cdot\cos x \end{align*} $ $ \begin{align*} f'(x)&= \frac{u'v'(x)\cdot w(x)-w'(x)\cdot uv(x)}{w(x)^2}\\ &= \frac{(e^x\cdot\sin x+3x\cdot\sin x+x^3\cdot\cos x)\cdot (x^3+2x^2+2)-(3x^2+4x)(\sin x\cdot e^x+x^3)}{(x^3+2x^2+2)^2} \end{align*} $ The obvious question: Is this derivative correct? Especially: Is it really possible to calculate the derivative by splitting the function into part-functions and calculating them together following the differentiation rules, the way I did it? Is it possible to reduce the summands by applying some rule I am not aware of? E.g. reducing $(e^x\cdot\sin x+3x\cdot\sin x+x^3\cdot\cos x)$ to something with just one $\sin$ or something?",,"['ordinary-differential-equations', 'derivatives']"
97,Legendre's Equation,Legendre's Equation,,"I'm given two solutions to Legendre's equation: $$P_1=x$$ $$Q_0=\frac{1}{2} \ln\left(\frac{1+x}{1-x}\right)$$ I'm trying to explain why their overlap integral (i.e. $\int_{-1}^{1} P_1 Q_0 dx$) is non-zero.  I computed it and it is indeed non-zero, but I'm having a difficult time justifying why that is. I'm thinking it has something to do with that fact that the $P_n$ and $Q_n$ solutions are constructed w.r.t different weight functions. Or perhaps it has something to do with the completeness of solutions. Any thoughts?","I'm given two solutions to Legendre's equation: $$P_1=x$$ $$Q_0=\frac{1}{2} \ln\left(\frac{1+x}{1-x}\right)$$ I'm trying to explain why their overlap integral (i.e. $\int_{-1}^{1} P_1 Q_0 dx$) is non-zero.  I computed it and it is indeed non-zero, but I'm having a difficult time justifying why that is. I'm thinking it has something to do with that fact that the $P_n$ and $Q_n$ solutions are constructed w.r.t different weight functions. Or perhaps it has something to do with the completeness of solutions. Any thoughts?",,['ordinary-differential-equations']
98,system of implicit nonlinear differential equations,system of implicit nonlinear differential equations,,"Here I have a system of nonlinear differential equations: $ (M+2m)\ddot{x} + m(l_1 \ddot{\theta}_1\cos\theta_1 - l_1\dot{\theta}_1^2\sin\theta_1) + m(l_2\ddot{\theta}_2\cos\theta_2-l_2\dot{\theta}_2^2\sin\theta_2) = F $ $ l_1\ddot{\theta}_1 + \ddot{x}\cos\theta_1 - g\sin\theta_1 = 0 $ $ l_2\ddot{\theta}_2 + \ddot{x}\cos\theta_2 - g\sin\theta_2 = 0 $ States are defined by me like this: $x_1 = \theta_1 , x_2 = \dot{x_1} = \dot{\theta}_1,x_3 = \theta_2, x_4 = \dot{x_3} = \dot{\theta}_2,x_5 = x, x_6 = \dot{x_5} = \dot{x}$ I mean actually what I need to do is to linearize this system about all the states are equal to $0$. But I cannot find the $\cdots$ places below $ \dot{x_1} = x_2 \\ \dot{x_2} = \cdots \\ \dot{x_3} = x_4 \\ \dot{x_4} = \cdots \\ \dot{x_5} = x_6 \\ \dot{x_6} = \cdots \\ $ How can I find $\dot{x_2},\dot{x_4},\dot{x_6}$? Thanks","Here I have a system of nonlinear differential equations: $ (M+2m)\ddot{x} + m(l_1 \ddot{\theta}_1\cos\theta_1 - l_1\dot{\theta}_1^2\sin\theta_1) + m(l_2\ddot{\theta}_2\cos\theta_2-l_2\dot{\theta}_2^2\sin\theta_2) = F $ $ l_1\ddot{\theta}_1 + \ddot{x}\cos\theta_1 - g\sin\theta_1 = 0 $ $ l_2\ddot{\theta}_2 + \ddot{x}\cos\theta_2 - g\sin\theta_2 = 0 $ States are defined by me like this: $x_1 = \theta_1 , x_2 = \dot{x_1} = \dot{\theta}_1,x_3 = \theta_2, x_4 = \dot{x_3} = \dot{\theta}_2,x_5 = x, x_6 = \dot{x_5} = \dot{x}$ I mean actually what I need to do is to linearize this system about all the states are equal to $0$. But I cannot find the $\cdots$ places below $ \dot{x_1} = x_2 \\ \dot{x_2} = \cdots \\ \dot{x_3} = x_4 \\ \dot{x_4} = \cdots \\ \dot{x_5} = x_6 \\ \dot{x_6} = \cdots \\ $ How can I find $\dot{x_2},\dot{x_4},\dot{x_6}$? Thanks",,"['calculus', 'ordinary-differential-equations', 'classical-mechanics']"
99,Just a differential equation.,Just a differential equation.,,"This eqn came toward the end of a much bigger problem, and I'm a bit rusty with these differential equations.. But maybe I got it right (probably not .. ) Anyway.. $$\ddot{Z}(t)=A+Bcos(\omega t)$$ to the best of my knowledge this is a Second Order inhomogenous non-linear ordinary differential equation (quite a mouthful) and can be solved as follows Soln to homogenous part: $$\ddot{Z}=0   \ \ \ => \ \ \ Z=Ct+D$$ Then the soln to the particular case I wasn't quite as sure but this is what I tried: let $Z = pt^2 + qcos(\omega t)$ where p, q are arbitrary then $$\dot Z = 2pt - q\omega sin(\omega t)$$ $$\ddot Z = 2p - q(\omega)^2 cos(\omega t)$$ and thus: $$2p - q(\omega)^2 cos(\omega t) = A+Bcos(\omega t)$$ $=>$ $$p=A/2 ; q = \frac{-B}{(\omega)^2}$$ which would give us our soln: $$Z =Ct + D + (A/2)t^2 - \frac{B}{(\omega)^2}cos(\omega t)$$ Is this right ?! and if so, is this the most efficient method of solving this ODE? ..... If this is right, I have an intial condition that : $$ t=0, => \dot Z = 0 $$ which solves to $C=0$ and $$Z = D + (A/2)t^2 - \frac{B}{(\omega)^2}cos(\omega t)$$ Which is obviously not unique, and I was wondering about the significance of the undetermined parameter D. Does it just mean that the set of functions that satisfy $\ddot{Z}=A+Bcos(\omega t)$ and $ t=0, => \dot Z = 0 $ are all equivalent with only a translation up the Z axis. Thanks a lot $:))$","This eqn came toward the end of a much bigger problem, and I'm a bit rusty with these differential equations.. But maybe I got it right (probably not .. ) Anyway.. $$\ddot{Z}(t)=A+Bcos(\omega t)$$ to the best of my knowledge this is a Second Order inhomogenous non-linear ordinary differential equation (quite a mouthful) and can be solved as follows Soln to homogenous part: $$\ddot{Z}=0   \ \ \ => \ \ \ Z=Ct+D$$ Then the soln to the particular case I wasn't quite as sure but this is what I tried: let $Z = pt^2 + qcos(\omega t)$ where p, q are arbitrary then $$\dot Z = 2pt - q\omega sin(\omega t)$$ $$\ddot Z = 2p - q(\omega)^2 cos(\omega t)$$ and thus: $$2p - q(\omega)^2 cos(\omega t) = A+Bcos(\omega t)$$ $=>$ $$p=A/2 ; q = \frac{-B}{(\omega)^2}$$ which would give us our soln: $$Z =Ct + D + (A/2)t^2 - \frac{B}{(\omega)^2}cos(\omega t)$$ Is this right ?! and if so, is this the most efficient method of solving this ODE? ..... If this is right, I have an intial condition that : $$ t=0, => \dot Z = 0 $$ which solves to $C=0$ and $$Z = D + (A/2)t^2 - \frac{B}{(\omega)^2}cos(\omega t)$$ Which is obviously not unique, and I was wondering about the significance of the undetermined parameter D. Does it just mean that the set of functions that satisfy $\ddot{Z}=A+Bcos(\omega t)$ and $ t=0, => \dot Z = 0 $ are all equivalent with only a translation up the Z axis. Thanks a lot $:))$",,['ordinary-differential-equations']
