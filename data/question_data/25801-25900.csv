,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Rows of a matrix over an arbitrary ring,Rows of a matrix over an arbitrary ring,,"I didn't know what to put on my search query so it's possible that this has already been asked before. If so, please give me a link to the answer and I'll delete this question. In linear algebra over fields, it is easy to see that if a matrix has more columns than rows, then there is a nontrivial relation between the columns. This result is easily expandable to commutative integral domains, since they can be embedded into fields, and I would think that proving the same result for division rings and hence for any integral domains in general wouldn't be such a problem. However I have read somewhere that this holds for matrices over arbitrary rings, but couldn't find a proof. Intuitively, this seems normal because removing integrity causes more things to be zero, more easily (for instance if the ring has trivial multiplication, then this is obviously true), and so should make creating non trivial relations easier. But when the ring is not ""regular"" enough, a theory of dimension over this ring is harder to put into place and so it wouldn't have surprised me that this result were actually false. So my question is : is this theorem true for arbitrary rings, and if so, how do you prove it ?","I didn't know what to put on my search query so it's possible that this has already been asked before. If so, please give me a link to the answer and I'll delete this question. In linear algebra over fields, it is easy to see that if a matrix has more columns than rows, then there is a nontrivial relation between the columns. This result is easily expandable to commutative integral domains, since they can be embedded into fields, and I would think that proving the same result for division rings and hence for any integral domains in general wouldn't be such a problem. However I have read somewhere that this holds for matrices over arbitrary rings, but couldn't find a proof. Intuitively, this seems normal because removing integrity causes more things to be zero, more easily (for instance if the ring has trivial multiplication, then this is obviously true), and so should make creating non trivial relations easier. But when the ring is not ""regular"" enough, a theory of dimension over this ring is harder to put into place and so it wouldn't have surprised me that this result were actually false. So my question is : is this theorem true for arbitrary rings, and if so, how do you prove it ?",,"['linear-algebra', 'abstract-algebra', 'matrices', 'ring-theory']"
1,Prove that the largest number of $1$'s in the $n\times n$ invertible matrix $A$ with entries $0$ or $1$ is $n^2-n+1$,Prove that the largest number of 's in the  invertible matrix  with entries  or  is,1 n\times n A 0 1 n^2-n+1,"Prove that the largest number of $1$'s in the $n\times n$ invertible matrix $A$ with entries $0$ or $1$ is $n^2-n+1$. My approach. If we denote $M=$number of $1$ in $A$ and $N=$number of $0$ . Then $M+N=n^2$. We are required to prove $M\leq n^2-n+1 $ alternatively $N\geq n-1$. Assume $n-1 >N$. Claim 1: There will be at least one column with all entries $1$. Proof: There are $n$ columns and if each were to contain at least one $0$ then number of $N\geq n$. Contrary to assumption that $n-1>N$. Proved. Place this column with all entries $1$ in the first column. This can always be done by column interchange without affecting the invertiblity of $A$. Claim 2: Now there will be a second column with at most one $0$. Proof: As we already selected the first column we are left with $n-1$ columns. If all of them contains more than one 0 then there will be more than $n-1$ zero contradicting the assumption again. Now place this column to second column. Now two things can happen to second column. As this column contains at  most one $0$ either (1) The second column contains all $1$ (2) The second column contains exactly one $0$ and $n-1$ numbers of $1$ . In (1) We have 1st and 2nd column equal . So their Determinant $0$. (2) $1$st column contains all $1$ and $2$nd column contains exactly one zero. Wlog assume this zero is in the first row. (This can be achieved by row interchange.) Now the 1st columns contains all 1 and second column contains $0$ in the first component ad rest are all $1$. Now subtract $2$nd column from $1$st column in the 1st column. This will leave the $1$st column with $1$ in the $(1,1)$ position of the matrix and all entries $0$. Now expand by coffactors. We see that required deteminant is now equal to a determinant of $(n-1)\times(n-1)$ whose all entries in the $1$st column equal to $1$. Now we are back to again where we started. If one keeps doing this we eventually reach determinant $0$. Hence proved. This how i proved it. I'm sure about the correctness of (1) but not so much about (2) . So i would like to hear your opinion about it. If you have any other proof please provide it. Thank you.","Prove that the largest number of $1$'s in the $n\times n$ invertible matrix $A$ with entries $0$ or $1$ is $n^2-n+1$. My approach. If we denote $M=$number of $1$ in $A$ and $N=$number of $0$ . Then $M+N=n^2$. We are required to prove $M\leq n^2-n+1 $ alternatively $N\geq n-1$. Assume $n-1 >N$. Claim 1: There will be at least one column with all entries $1$. Proof: There are $n$ columns and if each were to contain at least one $0$ then number of $N\geq n$. Contrary to assumption that $n-1>N$. Proved. Place this column with all entries $1$ in the first column. This can always be done by column interchange without affecting the invertiblity of $A$. Claim 2: Now there will be a second column with at most one $0$. Proof: As we already selected the first column we are left with $n-1$ columns. If all of them contains more than one 0 then there will be more than $n-1$ zero contradicting the assumption again. Now place this column to second column. Now two things can happen to second column. As this column contains at  most one $0$ either (1) The second column contains all $1$ (2) The second column contains exactly one $0$ and $n-1$ numbers of $1$ . In (1) We have 1st and 2nd column equal . So their Determinant $0$. (2) $1$st column contains all $1$ and $2$nd column contains exactly one zero. Wlog assume this zero is in the first row. (This can be achieved by row interchange.) Now the 1st columns contains all 1 and second column contains $0$ in the first component ad rest are all $1$. Now subtract $2$nd column from $1$st column in the 1st column. This will leave the $1$st column with $1$ in the $(1,1)$ position of the matrix and all entries $0$. Now expand by coffactors. We see that required deteminant is now equal to a determinant of $(n-1)\times(n-1)$ whose all entries in the $1$st column equal to $1$. Now we are back to again where we started. If one keeps doing this we eventually reach determinant $0$. Hence proved. This how i proved it. I'm sure about the correctness of (1) but not so much about (2) . So i would like to hear your opinion about it. If you have any other proof please provide it. Thank you.",,"['linear-algebra', 'matrices', 'proof-verification', 'linear-transformations']"
2,Invariant subspace of a diagonal matrix [duplicate],Invariant subspace of a diagonal matrix [duplicate],,"This question already has answers here : Diagonalizable transformation restricted to an invariant subspace is diagonalizable (3 answers) Closed 7 years ago . Suppose $T:V\to V$ is a diagonalizable linear transformation with no repeated eigenvalues, and denote $\{e_i:i\in I\}$ an eigenbasis of $V$ . Suppose $W$ is an invariant subspace of $T$ . Is it true that $W=\mathrm{span}\{e_i:J\subset I\}$ for some $J$ ? Why?","This question already has answers here : Diagonalizable transformation restricted to an invariant subspace is diagonalizable (3 answers) Closed 7 years ago . Suppose is a diagonalizable linear transformation with no repeated eigenvalues, and denote an eigenbasis of . Suppose is an invariant subspace of . Is it true that for some ? Why?",T:V\to V \{e_i:i\in I\} V W T W=\mathrm{span}\{e_i:J\subset I\} J,"['linear-algebra', 'eigenvalues-eigenvectors']"
3,Finding the dimension of a vector space over different fields?,Finding the dimension of a vector space over different fields?,,"We were given this question for my linear algebra module: We view $\Bbb C ^2$ as a vector space over $\Bbb C $,$\Bbb R$ and $\Bbb Q$. Let $$\mathbf x_1 := \begin{pmatrix} i \\ 0 \end{pmatrix}, \mathbf x_2 := \begin{pmatrix} \sqrt 2 \\ \sqrt 5 \end{pmatrix}, \mathbf x_3 := \begin{pmatrix} 0 \\ 1 \end{pmatrix}, \mathbf x_4 := \begin{pmatrix} i\sqrt 3 \\ \sqrt 3 \end{pmatrix}, \mathbf x_5 := \begin{pmatrix} 1 \\ 3 \end{pmatrix}  \in \Bbb C ^2 $$ Find dim$_F$(Span$_F$($ \mathbf x_1,\mathbf x_2,\mathbf x_3,\mathbf x_4,\mathbf x_5 $)) for F=$\Bbb C $,$\Bbb R$ and $\Bbb Q$. So I have found that for F=$\Bbb C $, the dimension is 2, but I'm struggling with the other fields. Whats been confusing me is say I want to find a basis for Span$_F$($ \mathbf x_1,\mathbf x_2,\mathbf x_3,\mathbf x_4,\mathbf x_5 $) over $\Bbb R$. If I want to apply Gaussian elimination to obtain the minimal spanning set, which scalars am I allowed to used say when scaling the rows when applying row ops. Do the scalars always have to the field elements, say in this case just $\Bbb R $? Sorry if this seems trivial but couldn't really find any good sources online and lecturer didn't cover it in lectures. Thanks a lot!","We were given this question for my linear algebra module: We view $\Bbb C ^2$ as a vector space over $\Bbb C $,$\Bbb R$ and $\Bbb Q$. Let $$\mathbf x_1 := \begin{pmatrix} i \\ 0 \end{pmatrix}, \mathbf x_2 := \begin{pmatrix} \sqrt 2 \\ \sqrt 5 \end{pmatrix}, \mathbf x_3 := \begin{pmatrix} 0 \\ 1 \end{pmatrix}, \mathbf x_4 := \begin{pmatrix} i\sqrt 3 \\ \sqrt 3 \end{pmatrix}, \mathbf x_5 := \begin{pmatrix} 1 \\ 3 \end{pmatrix}  \in \Bbb C ^2 $$ Find dim$_F$(Span$_F$($ \mathbf x_1,\mathbf x_2,\mathbf x_3,\mathbf x_4,\mathbf x_5 $)) for F=$\Bbb C $,$\Bbb R$ and $\Bbb Q$. So I have found that for F=$\Bbb C $, the dimension is 2, but I'm struggling with the other fields. Whats been confusing me is say I want to find a basis for Span$_F$($ \mathbf x_1,\mathbf x_2,\mathbf x_3,\mathbf x_4,\mathbf x_5 $) over $\Bbb R$. If I want to apply Gaussian elimination to obtain the minimal spanning set, which scalars am I allowed to used say when scaling the rows when applying row ops. Do the scalars always have to the field elements, say in this case just $\Bbb R $? Sorry if this seems trivial but couldn't really find any good sources online and lecturer didn't cover it in lectures. Thanks a lot!",,"['linear-algebra', 'gaussian-elimination']"
4,Trace of a nilpotent operator,Trace of a nilpotent operator,,"Let $V$ be a finite-dimensional vector space over a field $\mathbb{F}$, and $T:\,V \longmapsto V$ a linear map. The trace of $T$, $tr(T)$, is the trace of any matrix $A \in M_n(\mathbb{F})$ related to $T$ with respect any basis of $V$. I have to show that Lemma If $T$ is a nilpotent linear map, then $tr(T)=0$. proof. We know that if $T:\,V \longmapsto V$ a linear nilpotent map of a a finite-dimensional vector space, then then there exists a basis of $V$ such that the matrix representation of $T$ is upper triangular with zero diagonal elements. Then $tr(T)=0$. QUESTION: My teacher uses a different proof of this lemma, but I think there is something missing... proof.2 First of all it is easy to show that the trace of a linear transformation is also equal to the sum of the roots of his characteristic polynomial $p_T$ in a splitting field $K$ of $p_t$ over $\mathbb{F}$. Also if $T:\,V \longmapsto V$ a linear nilpotent map then its only eigenvalue is 0. Then $tr(T)=0$. I think this proof is true only if the field $\mathbb{F}$ is algebraically closed: if so, $\mathbb{F}$ contains all the roots of $p_T$. Then every root of $p_T$ is an eigenvalue of $T$ and so it must be zero. But if $\mathbb{F}$ is not algebraically closed, it is possible that there are some roots of $p_T$ that are not in $\mathbb{F}$. So we are not sure that every root of $p_T$ is an eigenvalue of $T$, and then I cannot use the fact that every eigenvalue of $T$ is 0 to show that $tr(T)=0$. Am I wrong?? Thanks a lot!!","Let $V$ be a finite-dimensional vector space over a field $\mathbb{F}$, and $T:\,V \longmapsto V$ a linear map. The trace of $T$, $tr(T)$, is the trace of any matrix $A \in M_n(\mathbb{F})$ related to $T$ with respect any basis of $V$. I have to show that Lemma If $T$ is a nilpotent linear map, then $tr(T)=0$. proof. We know that if $T:\,V \longmapsto V$ a linear nilpotent map of a a finite-dimensional vector space, then then there exists a basis of $V$ such that the matrix representation of $T$ is upper triangular with zero diagonal elements. Then $tr(T)=0$. QUESTION: My teacher uses a different proof of this lemma, but I think there is something missing... proof.2 First of all it is easy to show that the trace of a linear transformation is also equal to the sum of the roots of his characteristic polynomial $p_T$ in a splitting field $K$ of $p_t$ over $\mathbb{F}$. Also if $T:\,V \longmapsto V$ a linear nilpotent map then its only eigenvalue is 0. Then $tr(T)=0$. I think this proof is true only if the field $\mathbb{F}$ is algebraically closed: if so, $\mathbb{F}$ contains all the roots of $p_T$. Then every root of $p_T$ is an eigenvalue of $T$ and so it must be zero. But if $\mathbb{F}$ is not algebraically closed, it is possible that there are some roots of $p_T$ that are not in $\mathbb{F}$. So we are not sure that every root of $p_T$ is an eigenvalue of $T$, and then I cannot use the fact that every eigenvalue of $T$ is 0 to show that $tr(T)=0$. Am I wrong?? Thanks a lot!!",,"['linear-algebra', 'eigenvalues-eigenvectors', 'nilpotence']"
5,From the Spectral Theorem in Operator Theory to the Spectral Theorem in Linear Algebra,From the Spectral Theorem in Operator Theory to the Spectral Theorem in Linear Algebra,,"I'm still a bit confused about the very general Spectral Theorem in Operator Theory, since it's very abstract. So I thought it might be a good idea to apply the general theorem to the finite-dimensional case. Here's the theorem I got in my notes: Spectral Theorem: Let $H$ be a complex Hilbert-space and $A: H \to H$ a normal operator. There is a unique spectral measure $\Phi$ on the spectrum $\sigma(A)$ such that $$f(A) = \int_{\sigma(A)} f d \Phi$$ for any continuous function $f$ on the spectrum $\sigma(A)$. Moreover $\Phi(U) \neq 0$ for all non-empty open subsets $U \subset \sigma(A)$, and for every bounded operator $B : H \to H$ we have $BA = AB$ if and only if $B \Phi(U)  =  \Phi(U) B$ for all $U$. Let's now assume that $H := \mathbf{C}^n$ and that $A$ is self-adjoint. The spectrum of $A$ is real and finite, hence discrete. I want to show that there exists a orthonormal basis of eigenvectors of $A$. So I figured to take the function $f := \mathbf{1}_{\{\lambda\}}$ for a $\lambda \in \sigma(A)$. We get $f(A) = \Phi(\{\lambda\})$. The spectral theorem then gives me a finite family of pairwise orthogonal projections $\{ \Phi(\{\lambda \})\}_{\lambda \in \sigma(A)}$, such that $\sum_\lambda \Phi(\{\lambda\})= \text{Id}$. But I don't see how this does lead anywhere close to the Spectral Theorem in Linear Algebra. Can anyone help? Thanks!","I'm still a bit confused about the very general Spectral Theorem in Operator Theory, since it's very abstract. So I thought it might be a good idea to apply the general theorem to the finite-dimensional case. Here's the theorem I got in my notes: Spectral Theorem: Let $H$ be a complex Hilbert-space and $A: H \to H$ a normal operator. There is a unique spectral measure $\Phi$ on the spectrum $\sigma(A)$ such that $$f(A) = \int_{\sigma(A)} f d \Phi$$ for any continuous function $f$ on the spectrum $\sigma(A)$. Moreover $\Phi(U) \neq 0$ for all non-empty open subsets $U \subset \sigma(A)$, and for every bounded operator $B : H \to H$ we have $BA = AB$ if and only if $B \Phi(U)  =  \Phi(U) B$ for all $U$. Let's now assume that $H := \mathbf{C}^n$ and that $A$ is self-adjoint. The spectrum of $A$ is real and finite, hence discrete. I want to show that there exists a orthonormal basis of eigenvectors of $A$. So I figured to take the function $f := \mathbf{1}_{\{\lambda\}}$ for a $\lambda \in \sigma(A)$. We get $f(A) = \Phi(\{\lambda\})$. The spectral theorem then gives me a finite family of pairwise orthogonal projections $\{ \Phi(\{\lambda \})\}_{\lambda \in \sigma(A)}$, such that $\sum_\lambda \Phi(\{\lambda\})= \text{Id}$. But I don't see how this does lead anywhere close to the Spectral Theorem in Linear Algebra. Can anyone help? Thanks!",,"['linear-algebra', 'functional-analysis', 'operator-theory']"
6,"From $p_{1},p_{2},p_{3}$ choose a basis $B$ for $W$",From  choose a basis  for,"p_{1},p_{2},p_{3} B W","I'm very confused because we haven't got vectors or matrices here. So I really have no idea how to solve that task. I thought about converting these to one matrix somehow but it doesn't seem to work. This is no homework, it's a task from an old exam. Let $W= \text{span}(p_{1},p_{2},p_{3}), W \subseteq R_{2}[x]$ $p_{1}(x)= 4x^2+3x^3$ $p_{2}(x)= 1+2x^2+3x^3$ $p_{3}(x) = 3-2x^2+3x^3$ From $p_{1},p_{2},p_{3}$ choose a basis $B$ for $W$ and state why $B$   is a basis for $W$. And what does this $R_{2}[x]$ mean? I really hope you can give a detailled answer, I will also reward that answer with a bounty because I need to know how to solve tasks like that!","I'm very confused because we haven't got vectors or matrices here. So I really have no idea how to solve that task. I thought about converting these to one matrix somehow but it doesn't seem to work. This is no homework, it's a task from an old exam. Let $W= \text{span}(p_{1},p_{2},p_{3}), W \subseteq R_{2}[x]$ $p_{1}(x)= 4x^2+3x^3$ $p_{2}(x)= 1+2x^2+3x^3$ $p_{3}(x) = 3-2x^2+3x^3$ From $p_{1},p_{2},p_{3}$ choose a basis $B$ for $W$ and state why $B$   is a basis for $W$. And what does this $R_{2}[x]$ mean? I really hope you can give a detailled answer, I will also reward that answer with a bounty because I need to know how to solve tasks like that!",,"['linear-algebra', 'matrices', 'elementary-number-theory', 'vector-spaces']"
7,Backwards stability of QR vs SVD,Backwards stability of QR vs SVD,,"I've been reading Trefethen & Bau's book on Numerical Linear Algebra, and they have this one question whose answer does not entirely make sense to me.  In particular, they imply that the SVD algorithm (the computation of the SVD, not the solution of $Ax = b$ by SVD) is not backwards stable.  The suggestion is that this has to do with the fact that SVD maps from an $m\times n$ matrix into the space of triples of $m\times m$, $m\times n$, and $n\times n$ for $U$, $\Sigma$, and $V$.  They have a comment, with regards to the outer product computation, that since that, too, maps from a smaller dimensional space into a larger one, the computation should not be expected to be backwards stable.  At the same time, Householder triangularization ($QR$), is backwards stable, but this too maps from a smaller dimensional space into a larger dimensional space.  Is Householder just an exceptional case, or is there more to this?","I've been reading Trefethen & Bau's book on Numerical Linear Algebra, and they have this one question whose answer does not entirely make sense to me.  In particular, they imply that the SVD algorithm (the computation of the SVD, not the solution of $Ax = b$ by SVD) is not backwards stable.  The suggestion is that this has to do with the fact that SVD maps from an $m\times n$ matrix into the space of triples of $m\times m$, $m\times n$, and $n\times n$ for $U$, $\Sigma$, and $V$.  They have a comment, with regards to the outer product computation, that since that, too, maps from a smaller dimensional space into a larger one, the computation should not be expected to be backwards stable.  At the same time, Householder triangularization ($QR$), is backwards stable, but this too maps from a smaller dimensional space into a larger dimensional space.  Is Householder just an exceptional case, or is there more to this?",,"['linear-algebra', 'numerical-linear-algebra']"
8,Function that satisfies $f(x+ y) = f(x) + f(y)$ but not $f(cx)=cf(x)$ [duplicate],Function that satisfies  but not  [duplicate],f(x+ y) = f(x) + f(y) f(cx)=cf(x),"This question already has an answer here : Overview of basic facts about Cauchy functional equation (1 answer) Closed 7 years ago . Is there a function from $ \Bbb R^3 \to \Bbb R^3$ such that $$f(x + y) = f(x) + f(y)$$ but not $$f(cx) = cf(x)$$ for some scalar $c$? Is there one such function even in one dimension? I so, what is it? If not, why? I came across a function from $\Bbb R^3$ to $\Bbb R^3$ such that $$f(cx) = cf(x)$$ but not $$f(x + y) = f(x) + f(y)$$, and I was wondering whether there is one with converse. Although there is another post titled Overview of the Basic Facts of Cauchy valued functions, I do not understand it. If someone can explain in simplest terms the function that satisfy my question and why, that would be great.","This question already has an answer here : Overview of basic facts about Cauchy functional equation (1 answer) Closed 7 years ago . Is there a function from $ \Bbb R^3 \to \Bbb R^3$ such that $$f(x + y) = f(x) + f(y)$$ but not $$f(cx) = cf(x)$$ for some scalar $c$? Is there one such function even in one dimension? I so, what is it? If not, why? I came across a function from $\Bbb R^3$ to $\Bbb R^3$ such that $$f(cx) = cf(x)$$ but not $$f(x + y) = f(x) + f(y)$$, and I was wondering whether there is one with converse. Although there is another post titled Overview of the Basic Facts of Cauchy valued functions, I do not understand it. If someone can explain in simplest terms the function that satisfy my question and why, that would be great.",,"['linear-algebra', 'nonlinear-system']"
9,Orthogonal group acts on symmetric matrices,Orthogonal group acts on symmetric matrices,,"Let $O_n(\mathbb{R})$ act on $Sym_n(\mathbb{R})$, the symmetric matrices with real entries, via $S \mapsto A^T SA$ for $A \in O_2(\mathbb{R})$ and $Sym_n(\mathbb{R})$. What is the space of orbits $O_n(\mathbb{R})/Sym_n(\mathbb{R})$ as a set (and what is a basis for the topology)? I know that we can diagonalize a symmetric matrix $A$ with $Q\in O_n(\mathbb{R})$ such that $QSQ^{-1}$ is diagnonal but I don't know how to continue. Thanks a lot for your help!","Let $O_n(\mathbb{R})$ act on $Sym_n(\mathbb{R})$, the symmetric matrices with real entries, via $S \mapsto A^T SA$ for $A \in O_2(\mathbb{R})$ and $Sym_n(\mathbb{R})$. What is the space of orbits $O_n(\mathbb{R})/Sym_n(\mathbb{R})$ as a set (and what is a basis for the topology)? I know that we can diagonalize a symmetric matrix $A$ with $Q\in O_n(\mathbb{R})$ such that $QSQ^{-1}$ is diagnonal but I don't know how to continue. Thanks a lot for your help!",,"['linear-algebra', 'general-topology']"
10,Solving $ {L}_{1} $ Regularized Least Squares Over Complex Domain,Solving  Regularized Least Squares Over Complex Domain, {L}_{1} ,"I would like to solve the following Regularized Least Squares Problem (Very Similar to LASSO ): $$ \arg \min_{x} \frac{1}{2} {\left\| A x - b \right\|}_{2}^{2} + \lambda {\left\| x \right\|}_{1} $$ Where $ A \in {\mathbb{R}}^{m \times n} $ and $ b \in {\mathbb{R}}^{m} $. For simplicity one could define $ f \left( x \right) = \frac{1}{2} {\left\| A x - b \right\|}_{2}^{2} $ and $ g \left( x \right) = \lambda {\left\| x \right\|}_{1} $. For $ x \in {\mathbb{R}}^{n} $ the solution can be achieved using Sub Gradient Method or Proximal Gradient Method . My question is, how can it be solved for $ x \in {\mathbb{C}}^{n} $ (Assuming $ A \in {\mathbb{C}}^{m \times n} $ and $ b \in {\mathbb{C}}^{m} $)? Namely if the problem is over the complex domain. For instance, what is the Sub Gradient? What is the Prox (Shrinkage of Complex Number)? Thank You. My Attempt for Solution 001 The Gradient of $ f \left( x \right) $ is given by: $$ {\nabla}_{x} f \left( x \right) = {A}^{H} \left( A x - b \right) $$ The Sub Gradient of $ g \left( x \right) $ is given by: $$ {\partial}_{x} g \left( x \right) = \lambda \operatorname{sgn} \left( x \right) = \lambda \begin{cases} \frac{x}{ \left| x \right| } & \text{ if } x \neq 0 \\  0 & \text{ if } x = 0 \end{cases} $$ Namely it is the Complex Sign Function . Then, the Sub Gradient Method is given by: $$ {x}^{k + 1} = {x}^{k} - {\alpha}_{k} \left( {A}^{H} \left( A {x}^{k} - b \right) + \lambda \operatorname{sgn} \left( {x}^{k} \right) \right) $$ Where $ {\alpha}_{k} $ is the step size. Yet it won't converge to CVX Solution for this problem. Remark on Attempt 001 I think I understood why it doesn't work well. The Absolute Value Function in the Complex Domain is (Quoted from Wikipedia Absolute Value Derivative Section ):","I would like to solve the following Regularized Least Squares Problem (Very Similar to LASSO ): $$ \arg \min_{x} \frac{1}{2} {\left\| A x - b \right\|}_{2}^{2} + \lambda {\left\| x \right\|}_{1} $$ Where $ A \in {\mathbb{R}}^{m \times n} $ and $ b \in {\mathbb{R}}^{m} $. For simplicity one could define $ f \left( x \right) = \frac{1}{2} {\left\| A x - b \right\|}_{2}^{2} $ and $ g \left( x \right) = \lambda {\left\| x \right\|}_{1} $. For $ x \in {\mathbb{R}}^{n} $ the solution can be achieved using Sub Gradient Method or Proximal Gradient Method . My question is, how can it be solved for $ x \in {\mathbb{C}}^{n} $ (Assuming $ A \in {\mathbb{C}}^{m \times n} $ and $ b \in {\mathbb{C}}^{m} $)? Namely if the problem is over the complex domain. For instance, what is the Sub Gradient? What is the Prox (Shrinkage of Complex Number)? Thank You. My Attempt for Solution 001 The Gradient of $ f \left( x \right) $ is given by: $$ {\nabla}_{x} f \left( x \right) = {A}^{H} \left( A x - b \right) $$ The Sub Gradient of $ g \left( x \right) $ is given by: $$ {\partial}_{x} g \left( x \right) = \lambda \operatorname{sgn} \left( x \right) = \lambda \begin{cases} \frac{x}{ \left| x \right| } & \text{ if } x \neq 0 \\  0 & \text{ if } x = 0 \end{cases} $$ Namely it is the Complex Sign Function . Then, the Sub Gradient Method is given by: $$ {x}^{k + 1} = {x}^{k} - {\alpha}_{k} \left( {A}^{H} \left( A {x}^{k} - b \right) + \lambda \operatorname{sgn} \left( {x}^{k} \right) \right) $$ Where $ {\alpha}_{k} $ is the step size. Yet it won't converge to CVX Solution for this problem. Remark on Attempt 001 I think I understood why it doesn't work well. The Absolute Value Function in the Complex Domain is (Quoted from Wikipedia Absolute Value Derivative Section ):",,"['linear-algebra', 'optimization', 'convex-optimization', 'least-squares', 'regularization']"
11,Need help finding the kernel of a linear transformation $P^2 \to \mathbb{R}$,Need help finding the kernel of a linear transformation,P^2 \to \mathbb{R},The question asks to find the kernel of $S: P^2 \to \mathbb{R}$ defined by $S(a+bx+cx^2) = a+b+c$. I know how to find the kernel of a matrix transformation (it's just the null space of the matrix) but I can't conceptualize a transformation from two different types of vector spaces. How would I go about finding a basis ker(S)?,The question asks to find the kernel of $S: P^2 \to \mathbb{R}$ defined by $S(a+bx+cx^2) = a+b+c$. I know how to find the kernel of a matrix transformation (it's just the null space of the matrix) but I can't conceptualize a transformation from two different types of vector spaces. How would I go about finding a basis ker(S)?,,"['linear-algebra', 'linear-transformations']"
12,How do I find if two matrices are unitarily equivalent and the corresponding unitary matrix?,How do I find if two matrices are unitarily equivalent and the corresponding unitary matrix?,,"I'm trying to solve some exercises on linear operators and I came acroos the notion of unitary equivalence recently. I'm having problems solving the following exercise. I have two matrices- $ A=\begin{bmatrix}     0&0&1\\0&0&0\\1&0&0 \end{bmatrix}$ and $ B=\begin{bmatrix}     0.5&0.5&0\\0.5&0.5&0\\0&0&-1 \end{bmatrix}$. The question is to check if the matrices are unitarily similar and if so, to find the unitary matrix $C$ such that $B=C^{-1}AC$. I have calculated the eigenvalues and both have the same set of eigenvalues-$0,1,-1$. Since the eigenvalues are distinct and the same, the matrices are similar. But how do I check if they are unitarily similar? And if so, what will be my matrix $C$? I have a feeling that I need to do something with an orthonormal basis of eigenvectors of one of the matrices, but here I'm not sure. I would appreciate some help. Thanks.","I'm trying to solve some exercises on linear operators and I came acroos the notion of unitary equivalence recently. I'm having problems solving the following exercise. I have two matrices- $ A=\begin{bmatrix}     0&0&1\\0&0&0\\1&0&0 \end{bmatrix}$ and $ B=\begin{bmatrix}     0.5&0.5&0\\0.5&0.5&0\\0&0&-1 \end{bmatrix}$. The question is to check if the matrices are unitarily similar and if so, to find the unitary matrix $C$ such that $B=C^{-1}AC$. I have calculated the eigenvalues and both have the same set of eigenvalues-$0,1,-1$. Since the eigenvalues are distinct and the same, the matrices are similar. But how do I check if they are unitarily similar? And if so, what will be my matrix $C$? I have a feeling that I need to do something with an orthonormal basis of eigenvectors of one of the matrices, but here I'm not sure. I would appreciate some help. Thanks.",,"['linear-algebra', 'linear-transformations']"
13,$AA^T$ invertible or not?,invertible or not?,AA^T,"Let $A$ be an $m\times n$ matrix. Suppose that the columns of $A$ are linearly independent. (a) Is $A^TA$ invertible? Explain. (b) Is $AA^T$ invertible? Explain I have a solution for A but not sure about B. a) $A$ is a $m\times n$ matrix, so $A^T$ is a $n\times m$ matrix. So $A^TA$ is a $n\times n$ square matrix. So a square matrix with linearly independent columns -> RREF will have $n$ pivot columns be $n\times n$. $I_K$ is invertible. Vector $v$ in $N(A^TA) \Longrightarrow v^Ta^TAv = 0v^T = 0v = 0$ $v^TA^T=(Av)^T$ $(Av)^TAv=0 \Longrightarrow (Av)(Av) = 0 \Longrightarrow ||Av||^2 = 0 $ $Av= 0 $ If vector $v$ is in $N(A^TA)$ then $v$ is in $N(A)$. $N(A^TA)=N(A)=0$ $(A^TA)x=0$ only solution is $x=0$. Columns of $A^TA$ are linearly independent and a square matrix. So the RREF of $(A^TA) = I_K$. Therefor $A^TA$ is invertible. Can someone help me with question b?","Let $A$ be an $m\times n$ matrix. Suppose that the columns of $A$ are linearly independent. (a) Is $A^TA$ invertible? Explain. (b) Is $AA^T$ invertible? Explain I have a solution for A but not sure about B. a) $A$ is a $m\times n$ matrix, so $A^T$ is a $n\times m$ matrix. So $A^TA$ is a $n\times n$ square matrix. So a square matrix with linearly independent columns -> RREF will have $n$ pivot columns be $n\times n$. $I_K$ is invertible. Vector $v$ in $N(A^TA) \Longrightarrow v^Ta^TAv = 0v^T = 0v = 0$ $v^TA^T=(Av)^T$ $(Av)^TAv=0 \Longrightarrow (Av)(Av) = 0 \Longrightarrow ||Av||^2 = 0 $ $Av= 0 $ If vector $v$ is in $N(A^TA)$ then $v$ is in $N(A)$. $N(A^TA)=N(A)=0$ $(A^TA)x=0$ only solution is $x=0$. Columns of $A^TA$ are linearly independent and a square matrix. So the RREF of $(A^TA) = I_K$. Therefor $A^TA$ is invertible. Can someone help me with question b?",,"['linear-algebra', 'proof-verification', 'proof-writing', 'proof-explanation']"
14,Analogoue of a determinant test for $3$-tensors,Analogoue of a determinant test for -tensors,3,"Suppose we have a matrix $H$ and I want to find out whether there exists a vector $r\ne 0$ such that $H^{ij}r_i r_j=0$.  To this end we could simply compute if the determinant $\det(H)$ is zero, which is computationally inexpensive and explicit (we don't have to find such $r$ explicitly). Is there an analogue of such a test to see if there exists a vector $r\ne0$ such that $G^{ijk}r_i r_j r_k=0$ for a $3$-tensor $G$? I didn't specify the number of dimensions of the vector space, but if it's relevant $r\in \mathbb R^3$. This question could be related to Determinant of a tensor .","Suppose we have a matrix $H$ and I want to find out whether there exists a vector $r\ne 0$ such that $H^{ij}r_i r_j=0$.  To this end we could simply compute if the determinant $\det(H)$ is zero, which is computationally inexpensive and explicit (we don't have to find such $r$ explicitly). Is there an analogue of such a test to see if there exists a vector $r\ne0$ such that $G^{ijk}r_i r_j r_k=0$ for a $3$-tensor $G$? I didn't specify the number of dimensions of the vector space, but if it's relevant $r\in \mathbb R^3$. This question could be related to Determinant of a tensor .",,"['linear-algebra', 'determinant', 'tensors']"
15,"Let A be skew-symmetric, and denote its singular values by $\sigma_1\geq \sigma_2\geq \dots \sigma_n\geq0$.","Let A be skew-symmetric, and denote its singular values by .",\sigma_1\geq \sigma_2\geq \dots \sigma_n\geq0,"Let A be skew-symmetric, and denote its singular values by $\sigma_1\geq \sigma_2\geq \dots \sigma_n\geq0$. Show that a) If n is even, then $\sigma_{2k}=\sigma_{2k-1}\geq 0, k= 1,2,\dots n/2.$ If n is odd, then the same relationship holds up to $k=(n-1)/2$ and also $\sigma_n=0$. b) The eigenvalues $\lambda_j=(-1)^ji\sigma_j$, $j=1,2,\dots,n$. I know that skew symmetric means $-A=A^T$ and I know that the eigenvalues of a skew-symmetric matrix are either purely imaginary or zero. I am not able to get this though and I have been trying all week... Thanks for your time.","Let A be skew-symmetric, and denote its singular values by $\sigma_1\geq \sigma_2\geq \dots \sigma_n\geq0$. Show that a) If n is even, then $\sigma_{2k}=\sigma_{2k-1}\geq 0, k= 1,2,\dots n/2.$ If n is odd, then the same relationship holds up to $k=(n-1)/2$ and also $\sigma_n=0$. b) The eigenvalues $\lambda_j=(-1)^ji\sigma_j$, $j=1,2,\dots,n$. I know that skew symmetric means $-A=A^T$ and I know that the eigenvalues of a skew-symmetric matrix are either purely imaginary or zero. I am not able to get this though and I have been trying all week... Thanks for your time.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
16,Finding $\det(B)$ in terms of $\det(A) $ for matrices $A$ and $B$,Finding  in terms of  for matrices  and,\det(B) \det(A)  A B,"Q: Let the rows of $A \in M_{n \hspace{1mm}\mathbb x \hspace{1mm}n }$ ( $\mathbb F)$ be $a_1,a_2,...,a_n$ , and let $B$ be the matrix in which the rows are $a_n,a_{n-1} ,...,a_1$ . Calculate $\det(B)$ in terms of $\det(A)$ . A: $\det(B) = (-1)^{\frac{n(n-1)}{2}}\det(A)$ . I thought of applying row interchange but in doing so I can't see how I can derive te desired result as given above. Any suggestions as to how I can approach this question?","Q: Let the rows of ( be , and let be the matrix in which the rows are . Calculate in terms of . A: . I thought of applying row interchange but in doing so I can't see how I can derive te desired result as given above. Any suggestions as to how I can approach this question?","A \in M_{n \hspace{1mm}\mathbb x \hspace{1mm}n } \mathbb F) a_1,a_2,...,a_n B a_n,a_{n-1} ,...,a_1 \det(B) \det(A) \det(B) = (-1)^{\frac{n(n-1)}{2}}\det(A)","['linear-algebra', 'matrices']"
17,Uniqueness of orthogonal projector,Uniqueness of orthogonal projector,,"Let $S \subseteq \mathbb{R}^n$ be a subspace of $\mathbb{R}^n$, and let $P_1$, $P_2$ be arbitrary orthogonal projectors onto $S$. How can I prove that the orthogonal projector onto $S$ is unique, i.e. that $P_1 = P_2$?","Let $S \subseteq \mathbb{R}^n$ be a subspace of $\mathbb{R}^n$, and let $P_1$, $P_2$ be arbitrary orthogonal projectors onto $S$. How can I prove that the orthogonal projector onto $S$ is unique, i.e. that $P_1 = P_2$?",,['linear-algebra']
18,"If $X$ is symmetric, show $k(X^2)$ = $k(X)^2$","If  is symmetric, show  =",X k(X^2) k(X)^2,"Suppose we have a symmetric invertible matrix $X$. How can I find $k(X^2)$ in terms of $k(X)$? Note that $k$ represents the condition number operation, that is $k(X) = \|X\|\,\|X^{-1}\|$. So, after messing around in Matlab for a while, I think $$k(X^2) = k(X)^2$$ although I dont know how to prove it. I think I need to take advantage of the fact that symmetric matrices can be diagonalized by orthogonal matrices , but I wouldn't know where to go from there. Does anybody know how to show this?","Suppose we have a symmetric invertible matrix $X$. How can I find $k(X^2)$ in terms of $k(X)$? Note that $k$ represents the condition number operation, that is $k(X) = \|X\|\,\|X^{-1}\|$. So, after messing around in Matlab for a while, I think $$k(X^2) = k(X)^2$$ although I dont know how to prove it. I think I need to take advantage of the fact that symmetric matrices can be diagonalized by orthogonal matrices , but I wouldn't know where to go from there. Does anybody know how to show this?",,"['linear-algebra', 'matrices', 'numerical-methods', 'normed-spaces', 'matlab']"
19,How to find the transformation matrix given two vectors and their particular transformations?,How to find the transformation matrix given two vectors and their particular transformations?,,"I know this is probably a pretty simple thing to do, but I can't really wrap my head around it: I have two vectors in $\mathbb{R}^2$ , $〈1,2〉$ and $〈1,0〉$ , whose transformations are $〈1,2,0〉$ and $〈3,0,1〉$ respectively. How do I find out the transformation matrix from this information? I know I could manually write down a new system of equations using the elements of the matrix as my unknowns, but I supposed that's too tedious to be the right solution. Also, do basis transformation matrices have anything to do with this? I thought I could use a new matrix consisting of my two column vectors in $\mathbb{R}^2$ (representing a change of basis from the standard basis vectors), but I'm not sure if doing the same thing to my transformed vectors in $\mathbb{R}^3$ makes any sense (can I group them up too? Would that be a change of basis in the range?). I'm sorry if I'm mixing up two unrelated things, but I had a hunch the problem might be related to that. Thanks for the help!","I know this is probably a pretty simple thing to do, but I can't really wrap my head around it: I have two vectors in , and , whose transformations are and respectively. How do I find out the transformation matrix from this information? I know I could manually write down a new system of equations using the elements of the matrix as my unknowns, but I supposed that's too tedious to be the right solution. Also, do basis transformation matrices have anything to do with this? I thought I could use a new matrix consisting of my two column vectors in (representing a change of basis from the standard basis vectors), but I'm not sure if doing the same thing to my transformed vectors in makes any sense (can I group them up too? Would that be a change of basis in the range?). I'm sorry if I'm mixing up two unrelated things, but I had a hunch the problem might be related to that. Thanks for the help!","\mathbb{R}^2 〈1,2〉 〈1,0〉 〈1,2,0〉 〈3,0,1〉 \mathbb{R}^2 \mathbb{R}^3","['linear-algebra', 'linear-transformations']"
20,"Show $\dim(U) = d_1^2 + d_2^2 + \cdots + d_k^2$, where $U$ is the set of matrices that commute with a diagonalizable matrix $A$","Show , where  is the set of matrices that commute with a diagonalizable matrix",\dim(U) = d_1^2 + d_2^2 + \cdots + d_k^2 U A,"I asked this question last night, but I have made some additional progress where I have another question. Let $A$ be an $n \times n$ diagonalizable matrix with distinct eigenvalues $\lambda_1, \ldots, \lambda_k$ with corresponding multiplicities $d_1, \ldots, d_k$. Show that $$\dim(U) = d_1^2 + d_2^2 + \cdots + d_k^2,$$ where $U = \{B \in M_n \,|\, AB = BA \}$. PROOF: Let $A$ be an $n \times n$ diagonalizable matrix with eigenvalues $\lambda_1, \ldots, \lambda_k$ and respective multiplicities $d_1, \ldots, d_k$. Since $A$ is diagonalizable, $D = P^{-1}AP$ is a diagonal matrix. Observe that for any $B \in U$, $$P^{-1}AP = D \implies P^{-1}ABP = D(P^{-1}BP)$$ and likewise $$P^{-1}BAP = (P^{-1}BP)D.$$ This implies that we need only determine matrices $C$ such that $CD = DC$. Let $$D = \begin{pmatrix}\lambda_1I_1 & & & \\ & \lambda_2I_2 & & \\ & & \ddots & \\ & & & \lambda_kI_k\end{pmatrix}.$$ Note that $I_i$ is the identity matrix of size $d_i \times d_i$. Partition $C$ as such: $$C = \begin{pmatrix}C_{11} & C_{12} & \cdots & C_{1k} \\ C_{21} & C_{22} & & C_{2k} \\ \vdots & \vdots & \ddots & \vdots \\ C_{k1} & C_{k2} & \cdots & C_{kk}\end{pmatrix}$$ Because $C$ and $D$ should commute, $CD = DC$ and so for any $i, j = 1, \ldots, k$, we have $$\lambda_iC_{ij} = C_{ij}\lambda_j \implies (\lambda_i - \lambda_j)C_{ij} = O.$$ Because each eigenvalue is distinct, it is not possible that $\lambda_i - \lambda_j = 0$ (for $i = j$), which leaves that $C_{ij} = O$ where $i \ne j$. So $C$ is block diagonal. Here's where I'm stuck. I'm not sure what comes next to deduce $\dim(U)$. I think I need to find the dimension of each $C_{ii}$. How can I find the dimension of each $C_{ii}$? Surely it isn't as easy as saying that each $C_{ii}$ is $d_i \times d_i$, is it?","I asked this question last night, but I have made some additional progress where I have another question. Let $A$ be an $n \times n$ diagonalizable matrix with distinct eigenvalues $\lambda_1, \ldots, \lambda_k$ with corresponding multiplicities $d_1, \ldots, d_k$. Show that $$\dim(U) = d_1^2 + d_2^2 + \cdots + d_k^2,$$ where $U = \{B \in M_n \,|\, AB = BA \}$. PROOF: Let $A$ be an $n \times n$ diagonalizable matrix with eigenvalues $\lambda_1, \ldots, \lambda_k$ and respective multiplicities $d_1, \ldots, d_k$. Since $A$ is diagonalizable, $D = P^{-1}AP$ is a diagonal matrix. Observe that for any $B \in U$, $$P^{-1}AP = D \implies P^{-1}ABP = D(P^{-1}BP)$$ and likewise $$P^{-1}BAP = (P^{-1}BP)D.$$ This implies that we need only determine matrices $C$ such that $CD = DC$. Let $$D = \begin{pmatrix}\lambda_1I_1 & & & \\ & \lambda_2I_2 & & \\ & & \ddots & \\ & & & \lambda_kI_k\end{pmatrix}.$$ Note that $I_i$ is the identity matrix of size $d_i \times d_i$. Partition $C$ as such: $$C = \begin{pmatrix}C_{11} & C_{12} & \cdots & C_{1k} \\ C_{21} & C_{22} & & C_{2k} \\ \vdots & \vdots & \ddots & \vdots \\ C_{k1} & C_{k2} & \cdots & C_{kk}\end{pmatrix}$$ Because $C$ and $D$ should commute, $CD = DC$ and so for any $i, j = 1, \ldots, k$, we have $$\lambda_iC_{ij} = C_{ij}\lambda_j \implies (\lambda_i - \lambda_j)C_{ij} = O.$$ Because each eigenvalue is distinct, it is not possible that $\lambda_i - \lambda_j = 0$ (for $i = j$), which leaves that $C_{ij} = O$ where $i \ne j$. So $C$ is block diagonal. Here's where I'm stuck. I'm not sure what comes next to deduce $\dim(U)$. I think I need to find the dimension of each $C_{ii}$. How can I find the dimension of each $C_{ii}$? Surely it isn't as easy as saying that each $C_{ii}$ is $d_i \times d_i$, is it?",,"['linear-algebra', 'vector-spaces', 'proof-writing', 'eigenvalues-eigenvectors']"
21,How can we parametrise this matricial hypersphere?,How can we parametrise this matricial hypersphere?,,"What I call a matricial hypersphere for lack of a recognised name is the set in $\mathbb{R}^{p\times k}$ defined by $$\mathfrak{H}=\left\{ a_1,\ldots,a_k\in \mathbb{R}^{p};\ \sum_{i=1}^k a_i a_i^\text{T} = \mathbf{A} \right\}$$ where $\mathbf{A}$ is a $p\times p$ symmetric positive semi-definite matrix of rank $k$ $(k\le p)$. My questions are Is this a well-known object? Given the matrix $\mathbf{A}$ is there a completion of $\mathbf{A}$ into an object in bijection with $\{a_1,\ldots,a_k\}$, which is my meaning of parameterisation ? what is the size or dimension of $\mathfrak{H}$? Note: This object does not stem out of nowhere. It appears in linear   regression, where the $a_i$ vector is a collection of regression   coefficients,  and in connection with Wishart distributions, where the   $a_i$'s are Normal variates. I actually need to find a   reparameterisation of the $a_i$'s given $A$ to proceed a research   problem.","What I call a matricial hypersphere for lack of a recognised name is the set in $\mathbb{R}^{p\times k}$ defined by $$\mathfrak{H}=\left\{ a_1,\ldots,a_k\in \mathbb{R}^{p};\ \sum_{i=1}^k a_i a_i^\text{T} = \mathbf{A} \right\}$$ where $\mathbf{A}$ is a $p\times p$ symmetric positive semi-definite matrix of rank $k$ $(k\le p)$. My questions are Is this a well-known object? Given the matrix $\mathbf{A}$ is there a completion of $\mathbf{A}$ into an object in bijection with $\{a_1,\ldots,a_k\}$, which is my meaning of parameterisation ? what is the size or dimension of $\mathfrak{H}$? Note: This object does not stem out of nowhere. It appears in linear   regression, where the $a_i$ vector is a collection of regression   coefficients,  and in connection with Wishart distributions, where the   $a_i$'s are Normal variates. I actually need to find a   reparameterisation of the $a_i$'s given $A$ to proceed a research   problem.",,"['linear-algebra', 'multivariable-calculus', 'algebraic-geometry', 'random-matrices']"
22,Proof that the intersection of basis' is a basis for the intersection of their respective subspaces,Proof that the intersection of basis' is a basis for the intersection of their respective subspaces,,"I'm doing some linear algebra review and I'm trying to prove two things. Let $U$ and $W$ be subspaces of $V$. Firstly, suppose $U+W$ has finite dimensions. Let $B_1$ be a basis for $U$ and Let $B_2$ be a basis for $W$. I need to show that I can choose $B_1$ and $B_2$ such that $B_1 \cap B_2$ is a basis for $U \cap W$. Secondly, I need to show that the previous statement is not true in general. I think a counter example would suffice but I can't think of any.","I'm doing some linear algebra review and I'm trying to prove two things. Let $U$ and $W$ be subspaces of $V$. Firstly, suppose $U+W$ has finite dimensions. Let $B_1$ be a basis for $U$ and Let $B_2$ be a basis for $W$. I need to show that I can choose $B_1$ and $B_2$ such that $B_1 \cap B_2$ is a basis for $U \cap W$. Secondly, I need to show that the previous statement is not true in general. I think a counter example would suffice but I can't think of any.",,"['linear-algebra', 'vector-spaces']"
23,Complexification of a self-adjoint operator is self-adjoint on the complexified space,Complexification of a self-adjoint operator is self-adjoint on the complexified space,,"In ""Linear algebra done right"" 9.b.4 : Suppose $V$ is a real inner product space and $T \in \mathcal{L}(V)$ is self-adjoint. Show that $T_\mathbb{C}$ is a self-adjoint operator on the inner product space $V_\mathbb{C}$. My way to do it is as follow: Since $T$ is self-adjoint under real inner product vector space, by real spectral theorem, there exists an orthonormal basis $\mathcal{B}$ such that $\mathcal{M}(T,\mathcal{B},\mathcal{B})$ is diagonal matrix and all $\lambda_i \in \mathbb{R}$ are on the diagonal. W.r.t the same basis $\mathcal{B}$, $\mathcal{M}(T_\mathbb{C},\mathcal{B},\mathcal{B})$ is the same as $\mathcal{M} (T,\mathcal{B},\mathcal{B})$, and because $\lambda_i$ are all real, we have $\mathcal{M}(T_\mathbb{C}^*,\mathcal{B},\mathcal{B})$=$(\overline{\mathcal{M}(T_\mathbb{C},\mathcal{B},\mathcal{B})})^t$=$\mathcal{M}(T_\mathbb{C},\mathcal{B},\mathcal{B})$, which implies $T_\mathbb{C}$ is self-adjoint. Can someone tell me is there anything wrong in this proof?","In ""Linear algebra done right"" 9.b.4 : Suppose $V$ is a real inner product space and $T \in \mathcal{L}(V)$ is self-adjoint. Show that $T_\mathbb{C}$ is a self-adjoint operator on the inner product space $V_\mathbb{C}$. My way to do it is as follow: Since $T$ is self-adjoint under real inner product vector space, by real spectral theorem, there exists an orthonormal basis $\mathcal{B}$ such that $\mathcal{M}(T,\mathcal{B},\mathcal{B})$ is diagonal matrix and all $\lambda_i \in \mathbb{R}$ are on the diagonal. W.r.t the same basis $\mathcal{B}$, $\mathcal{M}(T_\mathbb{C},\mathcal{B},\mathcal{B})$ is the same as $\mathcal{M} (T,\mathcal{B},\mathcal{B})$, and because $\lambda_i$ are all real, we have $\mathcal{M}(T_\mathbb{C}^*,\mathcal{B},\mathcal{B})$=$(\overline{\mathcal{M}(T_\mathbb{C},\mathcal{B},\mathcal{B})})^t$=$\mathcal{M}(T_\mathbb{C},\mathcal{B},\mathcal{B})$, which implies $T_\mathbb{C}$ is self-adjoint. Can someone tell me is there anything wrong in this proof?",,['linear-algebra']
24,"If two vector spaces $V$ and $W$ are isomorphic and $V$ is F-D then $W$ is F-D. Furthermore, $\text{dim} V = \text{dim} W$.","If two vector spaces  and  are isomorphic and  is F-D then  is F-D. Furthermore, .",V W V W \text{dim} V = \text{dim} W,"Is the following statement true? Conjecture . If two vector spaces $V$ and $W$ are isomorphic and $V$ is finite dimensional (F-D) then $W$ is finite dimensional. Furthermore, $\text{dim} V = \text{dim} W$. and if YES , then how it can be proved? I can prove the following which is slightly different from the conjecture Theorem . Two finite dimensional vector spaces $V$ and $W$ are isomorphic if and only if they have the same dimension. but it does not help to deduce the conjecture from it. However, I strongly feel that the conjecture should be true. I would be thankful if you provide a hint.  :) Motivation of the Question While I was reading Linear Algebra Done Right by Sheldon Axler I encountered this theorem However, I was not able to prove the red underlined part by the references made! It seems that a little point was overlooked by the author! The references are where the notations used are $\Bbb{F}$ is the field $\Bbb{R}$ or $\Bbb{C}$. $\Bbb{F}^{m,n}$ is the vector space  of $m \times n$ matrices. $\mathcal{L}(V,W)$ is the vector space of linear maps from $V$ to $W$. $\mathcal{M}$ is a linear map from $\mathcal{L}(V,W)$ to $\Bbb{F}^{m,n}$ which gives the corresponding matrix of a linear map belonging to $\mathcal{L}(V,W)$.","Is the following statement true? Conjecture . If two vector spaces $V$ and $W$ are isomorphic and $V$ is finite dimensional (F-D) then $W$ is finite dimensional. Furthermore, $\text{dim} V = \text{dim} W$. and if YES , then how it can be proved? I can prove the following which is slightly different from the conjecture Theorem . Two finite dimensional vector spaces $V$ and $W$ are isomorphic if and only if they have the same dimension. but it does not help to deduce the conjecture from it. However, I strongly feel that the conjecture should be true. I would be thankful if you provide a hint.  :) Motivation of the Question While I was reading Linear Algebra Done Right by Sheldon Axler I encountered this theorem However, I was not able to prove the red underlined part by the references made! It seems that a little point was overlooked by the author! The references are where the notations used are $\Bbb{F}$ is the field $\Bbb{R}$ or $\Bbb{C}$. $\Bbb{F}^{m,n}$ is the vector space  of $m \times n$ matrices. $\mathcal{L}(V,W)$ is the vector space of linear maps from $V$ to $W$. $\mathcal{M}$ is a linear map from $\mathcal{L}(V,W)$ to $\Bbb{F}^{m,n}$ which gives the corresponding matrix of a linear map belonging to $\mathcal{L}(V,W)$.",,['linear-algebra']
25,Linear Algebra Textbook,Linear Algebra Textbook,,"I'm looking for a textbook on Linear Algebra and I seem to have narrowed down the list to: Linear Algebra by Hoffman and Kunze; and Linear Algebra by Friedberg, Insel and Spence. I'm not quite sure which textbook to commit to since I can't seem to distinguish between them based on their individual merits and demerits. It'd be great if someone could weigh out the merits and demerits (exercises, content, depth etc.) of both books. Thanks.","I'm looking for a textbook on Linear Algebra and I seem to have narrowed down the list to: Linear Algebra by Hoffman and Kunze; and Linear Algebra by Friedberg, Insel and Spence. I'm not quite sure which textbook to commit to since I can't seem to distinguish between them based on their individual merits and demerits. It'd be great if someone could weigh out the merits and demerits (exercises, content, depth etc.) of both books. Thanks.",,"['linear-algebra', 'reference-request']"
26,"Subspaces of $\mathbb{R}^{(0,3)}$",Subspaces of,"\mathbb{R}^{(0,3)}","I'm working my way through Axler's ""Linear Algebra Done Right"", and I've run into a proposition that I don't understand.  Namely: The set of differentiable real-valued functions $f$ on the interval (0, 3) such that $f'(2)=b$ is a subspace of $\mathbb{R}^{(0,3)}$ if and only if $b=0$ . My intution is that this has something to do with the fact that a subspace requires an additive identity, but that because the domain of the differentiable functions is open and exludes 0, we have to make some concessions (i.e. $f'(2) = 0$ ), though I'm unsure of how to work up a proof that backs this. Why must $f'(2) = 0$ to form a subspace of $\mathbb{R}^{(0,3)}$ ?","I'm working my way through Axler's ""Linear Algebra Done Right"", and I've run into a proposition that I don't understand.  Namely: The set of differentiable real-valued functions on the interval (0, 3) such that is a subspace of if and only if . My intution is that this has something to do with the fact that a subspace requires an additive identity, but that because the domain of the differentiable functions is open and exludes 0, we have to make some concessions (i.e. ), though I'm unsure of how to work up a proof that backs this. Why must to form a subspace of ?","f f'(2)=b \mathbb{R}^{(0,3)} b=0 f'(2) = 0 f'(2) = 0 \mathbb{R}^{(0,3)}",['linear-algebra']
27,Find a matrix $E$ such that $EA= B$,Find a matrix  such that,E EA= B,"I am asked to find a matrix $E$ such that $EA= B$. I am given matrix $A$ which is $4\times 4$ and matrix $B$ $4\times4$. Would I find $E$ the following way or is incorrect? $$EA=B$$ $A^{-1} [EA = B]$ Multiply by $A^{-1}$ on both sides $E = BA^{-1}$. E = $A^{-1} B$ (Not sure if this step is correct by matrix multiplication) So, therefore I would find matrix $E$ by finding the inverse of $A$ and then multiplying it by matrix $B$? Is that correct?","I am asked to find a matrix $E$ such that $EA= B$. I am given matrix $A$ which is $4\times 4$ and matrix $B$ $4\times4$. Would I find $E$ the following way or is incorrect? $$EA=B$$ $A^{-1} [EA = B]$ Multiply by $A^{-1}$ on both sides $E = BA^{-1}$. E = $A^{-1} B$ (Not sure if this step is correct by matrix multiplication) So, therefore I would find matrix $E$ by finding the inverse of $A$ and then multiplying it by matrix $B$? Is that correct?",,"['linear-algebra', 'matrices']"
28,Invertibility of a linear transformation without knowing its matrix,Invertibility of a linear transformation without knowing its matrix,,"Let $\mathbb{V}$ be a finite-dimensional inner product space, and let $\mathbb{W} \subset \mathbb{V}$ be a subspace. Define $T:\mathbb{V} \rightarrow \mathbb{V}$ by $$T(\overrightarrow v)=\overrightarrow v + Proj_{W}\overrightarrow v$$ Show that $T$ is invertible. My approach is to show that $T$ is injective and since $T$ goes from $\mathbb{V}$ to $\mathbb{V}$, that would imply that it is bjective and therefore invertible. I let an arbitrary vector $\overrightarrow v \in KerT$. $T(\overrightarrow v)=\overrightarrow 0$ $\Rightarrow$ $\overrightarrow v=-Proj_{W}\overrightarrow v$ This would mean that $\overrightarrow v$ is a linear combination of vectors which pertain to a basis of $\mathbb{W}$. $\therefore Proj_{W}\overrightarrow v=\overrightarrow v$ $\therefore \overrightarrow v=-\overrightarrow v \Rightarrow \overrightarrow v=\overrightarrow 0$ Therefore $T$ is bijective and invertible. Is my approach correct?","Let $\mathbb{V}$ be a finite-dimensional inner product space, and let $\mathbb{W} \subset \mathbb{V}$ be a subspace. Define $T:\mathbb{V} \rightarrow \mathbb{V}$ by $$T(\overrightarrow v)=\overrightarrow v + Proj_{W}\overrightarrow v$$ Show that $T$ is invertible. My approach is to show that $T$ is injective and since $T$ goes from $\mathbb{V}$ to $\mathbb{V}$, that would imply that it is bjective and therefore invertible. I let an arbitrary vector $\overrightarrow v \in KerT$. $T(\overrightarrow v)=\overrightarrow 0$ $\Rightarrow$ $\overrightarrow v=-Proj_{W}\overrightarrow v$ This would mean that $\overrightarrow v$ is a linear combination of vectors which pertain to a basis of $\mathbb{W}$. $\therefore Proj_{W}\overrightarrow v=\overrightarrow v$ $\therefore \overrightarrow v=-\overrightarrow v \Rightarrow \overrightarrow v=\overrightarrow 0$ Therefore $T$ is bijective and invertible. Is my approach correct?",,"['linear-algebra', 'inverse']"
29,Jordan canonical form of the transpose of a matrix,Jordan canonical form of the transpose of a matrix,,"Let $A \in M_{n \times n}(\mathbb C)$. Prove that $A$ and $A^\text{T}$ have the same Jordan canonical form, and conclude that $A$ and $A^\text{T}$ are similar. Intuitively, I know that I need to flip each cycle of generalised eigenvectors of $A^\text{T}$, but how to prove it? Moreover, I've tried Mathematical Induction but was stuck...","Let $A \in M_{n \times n}(\mathbb C)$. Prove that $A$ and $A^\text{T}$ have the same Jordan canonical form, and conclude that $A$ and $A^\text{T}$ are similar. Intuitively, I know that I need to flip each cycle of generalised eigenvectors of $A^\text{T}$, but how to prove it? Moreover, I've tried Mathematical Induction but was stuck...",,"['linear-algebra', 'jordan-normal-form']"
30,Is there a way to cut a an ellipsoid with a plane such that it gives a circle?,Is there a way to cut a an ellipsoid with a plane such that it gives a circle?,,"I'm trying to answer this In $\Bbb {R^3} $ consider the ellipsoid:   $2x^2+3y^2+4z^2=1$ It exists a subspace of dimension 2 which intersection with the ellipsoid is a circle. Justify any answer. I know that I must search for a plane since these are the only linear subspaces of $\Bbb {R^3} $ of dimension 2, but I don't know what else to do to check if such a plane exists. any hint would be much appreciated. Thanks!","I'm trying to answer this In $\Bbb {R^3} $ consider the ellipsoid:   $2x^2+3y^2+4z^2=1$ It exists a subspace of dimension 2 which intersection with the ellipsoid is a circle. Justify any answer. I know that I must search for a plane since these are the only linear subspaces of $\Bbb {R^3} $ of dimension 2, but I don't know what else to do to check if such a plane exists. any hint would be much appreciated. Thanks!",,"['linear-algebra', 'geometry', 'vector-spaces']"
31,Coordinate free Geometric Algebra vs. Linear Algebra,Coordinate free Geometric Algebra vs. Linear Algebra,,"I think I know what coordinate free means. But I never found in ANY text a good explanation of it or something like: This is the problem solved with coordinates and this is the problem solved without coordinates, etc. Since the philosophy of GA is that everything should be coordinate free, I would like to see an example of something that can be done in GA without coordinates but you have to use coordinates with usual linear algebra. To be specific, in GA you can make something like this in, let's say $\mathbb{G_3}$: $c=(bab)I$ The vector $a$ is rotated about the vector $b$, and then you take the dual plane of it. Is there an efficient way to do this without coordinates in linear algebra? This is just an example that I made up spontaneously; maybe there are better examples.","I think I know what coordinate free means. But I never found in ANY text a good explanation of it or something like: This is the problem solved with coordinates and this is the problem solved without coordinates, etc. Since the philosophy of GA is that everything should be coordinate free, I would like to see an example of something that can be done in GA without coordinates but you have to use coordinates with usual linear algebra. To be specific, in GA you can make something like this in, let's say $\mathbb{G_3}$: $c=(bab)I$ The vector $a$ is rotated about the vector $b$, and then you take the dual plane of it. Is there an efficient way to do this without coordinates in linear algebra? This is just an example that I made up spontaneously; maybe there are better examples.",,"['linear-algebra', 'clifford-algebras', 'geometric-algebras']"
32,Minimum number of real variables required to describe all n by n matrices that are their own inverse,Minimum number of real variables required to describe all n by n matrices that are their own inverse,,"This is related to Is the identity matrix the only matrix which is its own inverse? , but that question does not contain an answer to this one. I am attempting to find - probably by using a learning algorithm - the linear transformation in $\mathbb{R}^{300}$ that most closely maps a set of pairs of vectors to each other. Naively, this would require me to perform gradient descent (or some other learning algorithm) on $300\cdot 300 = 90000$ variables - one variable for each cell in my matrix. However, since I know that the matrix must be its own inverse, I suspect I can use a smaller number of variables. Is this the case? Is there some function of fewer than 90000 reals whose range is (or at least contains) all 300 by 300 matrices who are their own inverse? If so, what is that function, and what is the minimum number of reals required in its input?","This is related to Is the identity matrix the only matrix which is its own inverse? , but that question does not contain an answer to this one. I am attempting to find - probably by using a learning algorithm - the linear transformation in $\mathbb{R}^{300}$ that most closely maps a set of pairs of vectors to each other. Naively, this would require me to perform gradient descent (or some other learning algorithm) on $300\cdot 300 = 90000$ variables - one variable for each cell in my matrix. However, since I know that the matrix must be its own inverse, I suspect I can use a smaller number of variables. Is this the case? Is there some function of fewer than 90000 reals whose range is (or at least contains) all 300 by 300 matrices who are their own inverse? If so, what is that function, and what is the minimum number of reals required in its input?",,"['linear-algebra', 'matrices']"
33,Orthogonal Projection of a function,Orthogonal Projection of a function,,"If $C[-2,2]$, let $W:= span\{x,e^x\}.$ How would I go about figuring out the orthogonal projection of $x+1$ on $W$? encountered this problem and it really has me stumped. I was told that the Grahm-Schmidt Process is the correct route but am a bit confused on how to even go about that with this type of problem.","If $C[-2,2]$, let $W:= span\{x,e^x\}.$ How would I go about figuring out the orthogonal projection of $x+1$ on $W$? encountered this problem and it really has me stumped. I was told that the Grahm-Schmidt Process is the correct route but am a bit confused on how to even go about that with this type of problem.",,"['linear-algebra', 'orthogonal-polynomials']"
34,What are some books for infinite dimensional linear algebra?,What are some books for infinite dimensional linear algebra?,,"From the linear algebra books that I've encountered, they either discuss exclusively about finite-dimensional vector spaces, or assume that the reader already knows about infinite-dimensional vector space, Hamel basis, etc. What books explain the concept of infinite-dimensional vector space and its structures?","From the linear algebra books that I've encountered, they either discuss exclusively about finite-dimensional vector spaces, or assume that the reader already knows about infinite-dimensional vector space, Hamel basis, etc. What books explain the concept of infinite-dimensional vector space and its structures?",,"['linear-algebra', 'vector-spaces', 'book-recommendation']"
35,"Understanding Eigenvalues, Eigenfunctions and Eigenstates","Understanding Eigenvalues, Eigenfunctions and Eigenstates",,"Please could somebody explain the meaning and uses of Eigenvalues, eigenfunctions and eigenstates for me. I have taken 3 years of physics and math classes at university and never fully grasped the concept/ never had a satisfactory answer. I used eigenstates a lot in Quantum mechanics yet I did not understand their significance and it still bothers me to this day. If possible please include some basic examples or analogies.","Please could somebody explain the meaning and uses of Eigenvalues, eigenfunctions and eigenstates for me. I have taken 3 years of physics and math classes at university and never fully grasped the concept/ never had a satisfactory answer. I used eigenstates a lot in Quantum mechanics yet I did not understand their significance and it still bothers me to this day. If possible please include some basic examples or analogies.",,['linear-algebra']
36,Additivity Implies Homogeneity of Rational Scalars,Additivity Implies Homogeneity of Rational Scalars,,"I did my best to search for this question on the site- but I did not find it. Here it is: If a function $f:\mathbb{R}^2\to\mathbb{R}$ satisfies $f(u+v)=f(u)+f(v)$ for all $u,v\in\mathbb{R}^2$, then $f(\frac{p}{q}v)=\frac{p}{q}f(v)$ for all $p\in \mathbb{Z}$, $q\in \mathbb{Z}\setminus \{0\}$. So far, I have tried to use argument substitutions and ""abuse"" of additivity to achieve a homogeneity condition. For example: $$ f\bigg(\frac{p}{q}v\bigg)=f\bigg(\frac{p-q}{q}v+v\bigg)=f(v)+f\bigg(\frac{p-q}{q}v\bigg)$$ and other similar things- but I have been unable to show this. Note: I only want a gentle nudge in the right direction - I explicitly request that no one give me the answer.","I did my best to search for this question on the site- but I did not find it. Here it is: If a function $f:\mathbb{R}^2\to\mathbb{R}$ satisfies $f(u+v)=f(u)+f(v)$ for all $u,v\in\mathbb{R}^2$, then $f(\frac{p}{q}v)=\frac{p}{q}f(v)$ for all $p\in \mathbb{Z}$, $q\in \mathbb{Z}\setminus \{0\}$. So far, I have tried to use argument substitutions and ""abuse"" of additivity to achieve a homogeneity condition. For example: $$ f\bigg(\frac{p}{q}v\bigg)=f\bigg(\frac{p-q}{q}v+v\bigg)=f(v)+f\bigg(\frac{p-q}{q}v\bigg)$$ and other similar things- but I have been unable to show this. Note: I only want a gentle nudge in the right direction - I explicitly request that no one give me the answer.",,"['linear-algebra', 'irrational-numbers']"
37,preservation of extreme points under linear transformation,preservation of extreme points under linear transformation,,"Suppose $\{e_1,...,e_N\}$ is the set of all extreme points of a compact convex subset $X\subset\mathbb R^n$.   $L: \mathbb R^n\to \mathbb R^m$ is a linear transformation. $L$ is surjective but is not injective. Let $Y= L(X)$. Would it hold that for every  $1\leq i\leq N$,   $L(e_i)$ must be an extreme point of  $Y$? Is there any   characterization on $L$ such that this property holds? Thanks.","Suppose $\{e_1,...,e_N\}$ is the set of all extreme points of a compact convex subset $X\subset\mathbb R^n$.   $L: \mathbb R^n\to \mathbb R^m$ is a linear transformation. $L$ is surjective but is not injective. Let $Y= L(X)$. Would it hold that for every  $1\leq i\leq N$,   $L(e_i)$ must be an extreme point of  $Y$? Is there any   characterization on $L$ such that this property holds? Thanks.",,"['real-analysis', 'linear-algebra', 'convex-analysis', 'linear-transformations']"
38,range of $T$ equals range of $T^2$ if and only if the intersection of the range and kernel of $T$ is trivial,range of  equals range of  if and only if the intersection of the range and kernel of  is trivial,T T^2 T,"Let $V$ be a finite-dimensional vector space over a field $F$ and let $T$ be an operator on $V$. Prove $\text{range}(T^2) = \text{range}(T)$ if and only if $\text{range}(T) \cap \ker(T) = \{0 \}$. Proof. Assume $v \in \text{range}(T) \cap \ker(T) = \text{range}(T^2) \cap \ker(T)$. Then there are vectors $v_1,v_2 \in V$ such that $(\star) \hspace{1mm} v = T^2(v_1)=T(v_2)$ and $T(v)=0$ Thus $T(v) = T^3(v_1) = T^2(v_2) = 0$, implying that $$T(v) = T^3(v_1)+T^2(v_2)= T(T^2(v_1)+T(v_2)) \Longleftrightarrow v = T^2(v_1)+T(v_2)= v + v = 2v$$ $$\hspace{5.6cm} \Longleftrightarrow v = 2v \Rightarrow v = 0.$$ Conversely, assume that the intersection of $\text{range}(T)$ and $\ker(T)$ is trivial and let $v \in \text{range}(T^2)$. Then there exists $u \in V$ such that $v = T^2(u)=T(T(u))$. Since $T$ is an operator, $T(u) \in V$, implying that there exists a vector $w \in V$ such that $w = T(u)$. Hence $T^2(u)=T(w) = v$, implying that $v \in \text{range}(T)$, and so $\text{range}(T^2) \subset \text{range} (T)$. This is where I get stuck. I'm trying to show that $\dim \text{range} (T^2) = \dim \text{range}(T)$, so the above inclusion implies equality, but I am having trouble. I know we probably need to apply the rank-nullity theorem combined with our assumption that the intersection of the range and kernel is trivial, but I am not sure how.","Let $V$ be a finite-dimensional vector space over a field $F$ and let $T$ be an operator on $V$. Prove $\text{range}(T^2) = \text{range}(T)$ if and only if $\text{range}(T) \cap \ker(T) = \{0 \}$. Proof. Assume $v \in \text{range}(T) \cap \ker(T) = \text{range}(T^2) \cap \ker(T)$. Then there are vectors $v_1,v_2 \in V$ such that $(\star) \hspace{1mm} v = T^2(v_1)=T(v_2)$ and $T(v)=0$ Thus $T(v) = T^3(v_1) = T^2(v_2) = 0$, implying that $$T(v) = T^3(v_1)+T^2(v_2)= T(T^2(v_1)+T(v_2)) \Longleftrightarrow v = T^2(v_1)+T(v_2)= v + v = 2v$$ $$\hspace{5.6cm} \Longleftrightarrow v = 2v \Rightarrow v = 0.$$ Conversely, assume that the intersection of $\text{range}(T)$ and $\ker(T)$ is trivial and let $v \in \text{range}(T^2)$. Then there exists $u \in V$ such that $v = T^2(u)=T(T(u))$. Since $T$ is an operator, $T(u) \in V$, implying that there exists a vector $w \in V$ such that $w = T(u)$. Hence $T^2(u)=T(w) = v$, implying that $v \in \text{range}(T)$, and so $\text{range}(T^2) \subset \text{range} (T)$. This is where I get stuck. I'm trying to show that $\dim \text{range} (T^2) = \dim \text{range}(T)$, so the above inclusion implies equality, but I am having trouble. I know we probably need to apply the rank-nullity theorem combined with our assumption that the intersection of the range and kernel is trivial, but I am not sure how.",,['linear-algebra']
39,How to rationalize denominator?,How to rationalize denominator?,,"Suppose $c$ is not a complete square integer, ${a_0},{a_1} \in \mathbb{Q}$, we have $$ \frac{1}{{{a_0} + {a_1}\sqrt c }}  = \frac{{{a_0} - {a_1}\sqrt c }}{{a_0^2 - a_1^2c}}. $$ We need to show ${{a_0} + {a_1}\sqrt c } = 0$ iff ${a_0} = {a_1} = 0$. I know it's not hard. Suppose $c$ is not a complete cube integer, ${a_0},{a_1},{a_2} \in \mathbb{Q}$, how can we deal with $$ \frac{1}{{{a_0} + {a_1}\sqrt[3]{c} + {a_2}\sqrt[3]{{{c^2}}}}} $$ similarly? Any help will be appreciated.","Suppose $c$ is not a complete square integer, ${a_0},{a_1} \in \mathbb{Q}$, we have $$ \frac{1}{{{a_0} + {a_1}\sqrt c }}  = \frac{{{a_0} - {a_1}\sqrt c }}{{a_0^2 - a_1^2c}}. $$ We need to show ${{a_0} + {a_1}\sqrt c } = 0$ iff ${a_0} = {a_1} = 0$. I know it's not hard. Suppose $c$ is not a complete cube integer, ${a_0},{a_1},{a_2} \in \mathbb{Q}$, how can we deal with $$ \frac{1}{{{a_0} + {a_1}\sqrt[3]{c} + {a_2}\sqrt[3]{{{c^2}}}}} $$ similarly? Any help will be appreciated.",,"['calculus', 'linear-algebra', 'algebra-precalculus', 'number-theory']"
40,Frobenius Norm inequality,Frobenius Norm inequality,,Is there anyway the following inequality can be proved without using SVD and Unitary matrices properties of the norms? $||AB||_F \le ||A||_2 ||B||_F$,Is there anyway the following inequality can be proved without using SVD and Unitary matrices properties of the norms? $||AB||_F \le ||A||_2 ||B||_F$,,"['linear-algebra', 'matrices', 'normed-spaces']"
41,How do I figure out the features of a cube in 4 dimensions?,How do I figure out the features of a cube in 4 dimensions?,,"This is a question from Gilbert Strang's Introduction to Linear Algebra and MIT OCW How many corners does a cube have in $4$ dimensions? How many $3$D faces? How many edges? A typical corner is $(0,0,1,0)$. A typical edge goes to $(0,1,0,0)$. I know that it will have $16$ corners, but I don't know how to figure out the rest. So far, I have only learned a little about vectors, their components, and linear combinations. How can I apply this knowledge to figure out the features of this cube? The worked example in the book (for this problem) uses matrices and it is also in the middle of the next chapter. Hints only, please.","This is a question from Gilbert Strang's Introduction to Linear Algebra and MIT OCW How many corners does a cube have in $4$ dimensions? How many $3$D faces? How many edges? A typical corner is $(0,0,1,0)$. A typical edge goes to $(0,1,0,0)$. I know that it will have $16$ corners, but I don't know how to figure out the rest. So far, I have only learned a little about vectors, their components, and linear combinations. How can I apply this knowledge to figure out the features of this cube? The worked example in the book (for this problem) uses matrices and it is also in the middle of the next chapter. Hints only, please.",,['linear-algebra']
42,Prove that as a $\Bbb Q$ vector space $\Bbb R^n$ is isomorphic to $\Bbb R$,Prove that as a  vector space  is isomorphic to,\Bbb Q \Bbb R^n \Bbb R,"Prove that as a $\mathbb{Q}$ vector space, $\mathbb{R}^n$ is isomorphic to $\mathbb{R}.$ I came across this statement somewhere and I have been trying to prove it ever since, but I just don't know how to start or what to define as the map.","Prove that as a $\mathbb{Q}$ vector space, $\mathbb{R}^n$ is isomorphic to $\mathbb{R}.$ I came across this statement somewhere and I have been trying to prove it ever since, but I just don't know how to start or what to define as the map.",,"['linear-algebra', 'vector-spaces']"
43,"Is there any neat way to show $T$ is $ \mathbb R$-linear, if $T\left(x^2+T(y)\right)=y+T(x)^2$?","Is there any neat way to show  is -linear, if ?",T  \mathbb R T\left(x^2+T(y)\right)=y+T(x)^2,"Let $T: \mathbb R \to  \mathbb R$ be the map which satisfies the following functional equation $T\left(x^2+T(y)\right)=y+T(x)^2$ , $ \forall x,y \in \mathbb R$ . Is there any neat way to show that $T$ is $ \mathbb R$ -linear? After substituting some ""special values"" and doing a little bit algebraic manipulation it turned out that identity map on $\mathbb R$ is the only map which satisfies the above functional equation so $T$ must be $ \mathbb R$ -linear. But Is there any other way (without computing all possible choices for $T$ ) to show that $T$ is $ \mathbb R$ -linear?","Let be the map which satisfies the following functional equation , . Is there any neat way to show that is -linear? After substituting some ""special values"" and doing a little bit algebraic manipulation it turned out that identity map on is the only map which satisfies the above functional equation so must be -linear. But Is there any other way (without computing all possible choices for ) to show that is -linear?","T: \mathbb R \to  \mathbb R T\left(x^2+T(y)\right)=y+T(x)^2  \forall x,y \in \mathbb R T  \mathbb R \mathbb R T  \mathbb R T T  \mathbb R","['linear-algebra', 'functions', 'functional-equations', 'linear-transformations']"
44,Find a homogeneous system whose solution space is spanned by the given vectors,Find a homogeneous system whose solution space is spanned by the given vectors,,"find a homogeneous system whose solution space is spanned by the following set of 3 vectors:  $$(1,-2,0,3,-1) , (2,-3,2,5,-3), (1,-2,1,2,-2)$$ Please help, I've only seen similar questions where there are 4 unknowns not 5","find a homogeneous system whose solution space is spanned by the following set of 3 vectors:  $$(1,-2,0,3,-1) , (2,-3,2,5,-3), (1,-2,1,2,-2)$$ Please help, I've only seen similar questions where there are 4 unknowns not 5",,['linear-algebra']
45,Is there a formula for the inverse of this bordered matrix?,Is there a formula for the inverse of this bordered matrix?,,"Suppose I have a matrix $\mathbf{H}$ of size $n\times n$, and that I know its inverse $\mathbf{W}=\mathbf{H}^{-1}$. Then I add a column and a row to $\mathbf{H}$ to obtain a new matrix $\mathbf{G}$. That is $\mathbf{G}$ is given by $$\mathbf{G}=\left( \begin{array}{c|c}   r_1 & \begin{array}{ccc} r_2 & \cdots & r_n \end{array} \\ \hline \begin{array}{c} c_2 \\ \vdots \\ c_n \end{array}  & {\Huge{\mathbf{H}}} \end{array} \right) $$ Is there a relation between $\mathbf{W}$ and $\mathbf{G}$ and $\mathbf{G}^{-1}$?","Suppose I have a matrix $\mathbf{H}$ of size $n\times n$, and that I know its inverse $\mathbf{W}=\mathbf{H}^{-1}$. Then I add a column and a row to $\mathbf{H}$ to obtain a new matrix $\mathbf{G}$. That is $\mathbf{G}$ is given by $$\mathbf{G}=\left( \begin{array}{c|c}   r_1 & \begin{array}{ccc} r_2 & \cdots & r_n \end{array} \\ \hline \begin{array}{c} c_2 \\ \vdots \\ c_n \end{array}  & {\Huge{\mathbf{H}}} \end{array} \right) $$ Is there a relation between $\mathbf{W}$ and $\mathbf{G}$ and $\mathbf{G}^{-1}$?",,['linear-algebra']
46,Show that $\log \det(A\cdot B^{-1})=-\int_{0}^\infty tr(e^{-t\cdot A}-e^{-t\cdot B}){dt\over t}$,Show that,\log \det(A\cdot B^{-1})=-\int_{0}^\infty tr(e^{-t\cdot A}-e^{-t\cdot B}){dt\over t},"Let $$A,B:V\to V$$ be positive definite operators in complex linear space with inner product $V$ such that $\text{dim}V<\infty$ Show that $$\log \left(\det(A\cdot B^{-1})\right)=-\int_{0}^\infty tr(e^{-t\cdot A}-e^{-t\cdot B}){dt\over t}$$ Could anyone give some hints how to prove this?","Let $$A,B:V\to V$$ be positive definite operators in complex linear space with inner product $V$ such that $\text{dim}V<\infty$ Show that $$\log \left(\det(A\cdot B^{-1})\right)=-\int_{0}^\infty tr(e^{-t\cdot A}-e^{-t\cdot B}){dt\over t}$$ Could anyone give some hints how to prove this?",,"['calculus', 'linear-algebra']"
47,"Cauchy-Schwarz Inequality without using $\langle a x,y\rangle=a\langle x,y\rangle$",Cauchy-Schwarz Inequality without using,"\langle a x,y\rangle=a\langle x,y\rangle","Let $V$ be a vector space and define a function $\langle .,.\rangle:V\times V\to\mathbb{C}$ such that $$\begin{align} & \langle x,y\rangle=\overline{\langle y,x\rangle }\,\,\,\forall x,y\in V\\ & \langle x+y,z\rangle=\langle x,z\rangle+\langle y,z\rangle \,\,\,\,\,\forall x,y,z\in V\\  & \langle x,x\rangle\ge0\,\,\,\,\,\forall x\in V\,\,\,\text{and equality holds iff}\,\,\,\ x=0\\  \end{align}$$ Does Cauchy-Schwarz inequality $$\color{Green}{|\langle x,y\rangle|^2\le\langle x,x\rangle\langle y,y\rangle}$$still vallied? (Without the condition $\langle a x,y\rangle=a\langle x,y\rangle,\,\,\,\forall a\in\mathbb{C},\,\,\,\forall x,y\in V.$) If the answer is ""NO"", can we prove it?","Let $V$ be a vector space and define a function $\langle .,.\rangle:V\times V\to\mathbb{C}$ such that $$\begin{align} & \langle x,y\rangle=\overline{\langle y,x\rangle }\,\,\,\forall x,y\in V\\ & \langle x+y,z\rangle=\langle x,z\rangle+\langle y,z\rangle \,\,\,\,\,\forall x,y,z\in V\\  & \langle x,x\rangle\ge0\,\,\,\,\,\forall x\in V\,\,\,\text{and equality holds iff}\,\,\,\ x=0\\  \end{align}$$ Does Cauchy-Schwarz inequality $$\color{Green}{|\langle x,y\rangle|^2\le\langle x,x\rangle\langle y,y\rangle}$$still vallied? (Without the condition $\langle a x,y\rangle=a\langle x,y\rangle,\,\,\,\forall a\in\mathbb{C},\,\,\,\forall x,y\in V.$) If the answer is ""NO"", can we prove it?",,"['real-analysis', 'linear-algebra', 'inequality', 'vector-spaces', 'inner-products']"
48,"Does $GL(n,\mathbb{C})$ inject into $GL^+(2n, \mathbb{R})$ for all $n$?",Does  inject into  for all ?,"GL(n,\mathbb{C}) GL^+(2n, \mathbb{R}) n","I know that $GL(n, \mathbb{C}) \hookrightarrow GL^+(2n,\mathbb{R})$ for n = 1 and 2. I just did these with pen and paper (and mathematica). But is this statement true in general? What is a proof of this? Thanks for the help. Oh, how about since as Lie groups, $GL(n,\mathbb{C}) \subset GL(2n, \mathbb{R})$ and $GL(n,\mathbb{C})$ is connected but $GL(2n, \mathbb{R})$ has two connected components, one for positive determinant and one for negative determinant? And the identity has positive determinant, so it must lie in that component. Is there an algebraic proof though?","I know that $GL(n, \mathbb{C}) \hookrightarrow GL^+(2n,\mathbb{R})$ for n = 1 and 2. I just did these with pen and paper (and mathematica). But is this statement true in general? What is a proof of this? Thanks for the help. Oh, how about since as Lie groups, $GL(n,\mathbb{C}) \subset GL(2n, \mathbb{R})$ and $GL(n,\mathbb{C})$ is connected but $GL(2n, \mathbb{R})$ has two connected components, one for positive determinant and one for negative determinant? And the identity has positive determinant, so it must lie in that component. Is there an algebraic proof though?",,"['linear-algebra', 'abstract-algebra', 'lie-groups']"
49,Kernel of a matrix pencil,Kernel of a matrix pencil,,"Let $A,B$ be $n\times n$ singular real matrices such $ker A\cap ker B=\{0\}$, how could I show that there exists $x\in \mathbb R$ such that $ker (A+xB)=\{0\}$?","Let $A,B$ be $n\times n$ singular real matrices such $ker A\cap ker B=\{0\}$, how could I show that there exists $x\in \mathbb R$ such that $ker (A+xB)=\{0\}$?",,['linear-algebra']
50,Determinant proof using its properties,Determinant proof using its properties,,Prove without expanding: \begin{equation} \begin{vmatrix}bc&a^2&a^2\\b^2&ac&b^2\\c^2&c^2 & ab\end{vmatrix} = \begin{vmatrix}ac&bc&ab\\bc&ab&ac\\ab&ac&bc\end{vmatrix} \end{equation} Tried to multiply by 'abc' in all rows then take common factors. Tried to expand determinant into two determinants. I used the determinant properties shown here: http://www.vitutor.com/alg/determinants/properties_determinants.html,Prove without expanding: \begin{equation} \begin{vmatrix}bc&a^2&a^2\\b^2&ac&b^2\\c^2&c^2 & ab\end{vmatrix} = \begin{vmatrix}ac&bc&ab\\bc&ab&ac\\ab&ac&bc\end{vmatrix} \end{equation} Tried to multiply by 'abc' in all rows then take common factors. Tried to expand determinant into two determinants. I used the determinant properties shown here: http://www.vitutor.com/alg/determinants/properties_determinants.html,,"['linear-algebra', 'matrices', 'determinant']"
51,Largest entry in symmetric positive definite matrix,Largest entry in symmetric positive definite matrix,,"I know why in a symmetric positive definite matrix every entry on the trace is positive entry $a_{ii}>0$. However I don't how to show that the largest value of the matrix is also on it's trace, meaning a $z$ exist with $a_{zz} = max \vert a_{ij} \vert $ for  $1\le i,j\le n$. Thanks for help.","I know why in a symmetric positive definite matrix every entry on the trace is positive entry $a_{ii}>0$. However I don't how to show that the largest value of the matrix is also on it's trace, meaning a $z$ exist with $a_{zz} = max \vert a_{ij} \vert $ for  $1\le i,j\le n$. Thanks for help.",,['linear-algebra']
52,Invariant Subspace containing linear combination of eigenvectors,Invariant Subspace containing linear combination of eigenvectors,,"Let $$T:V\to V$$ be a linear transformation. Suppose that $v_1, v_2, \cdots, v_k \in V$ are eigenvectors of $T$ that correspond to distinct eigenvalues. Assume that $W$ is a $T$-invariant subspace of $V$ that contains the vector $v_1 + v_2 + \cdots + v_k$. Show that $W$ contains each of $v_1, v_2, \cdots, v_k$.","Let $$T:V\to V$$ be a linear transformation. Suppose that $v_1, v_2, \cdots, v_k \in V$ are eigenvectors of $T$ that correspond to distinct eigenvalues. Assume that $W$ is a $T$-invariant subspace of $V$ that contains the vector $v_1 + v_2 + \cdots + v_k$. Show that $W$ contains each of $v_1, v_2, \cdots, v_k$.",,"['linear-algebra', 'vector-spaces', 'eigenvalues-eigenvectors']"
53,Proving direct sum when field is NOT of characteristic $2$.,Proving direct sum when field is NOT of characteristic .,2,"Let $\mathbb{F}$ be a field that is not of characteristic $2$. Define $W_1 = \{ A \in M_{n \times n} (\mathbb{F}) : A_{ij} = 0$ whenever $i \leq j\}$ and $W_2$ to be the set of all symmetric $n \times n$ matrices with entries from $\mathbb{F}$. Both $W_1$ and $W_2$ are subspaces of $M_{n \times n} (\mathbb{F})$. Prove that $M_{n \times n} (\mathbb{F})=W_1 \oplus W_2$. I know how to prove that $M_{n \times n} (\mathbb{F})=W_1 \oplus W_2$ for any arbitrary field $\mathbb{F}$ (by showing $W_1 \cap W_2 = \{0\}$ and any element of $M_{n \times n} (\mathbb{F})$ is sum of elements of $W_1$ and $W_2$). But what does ""$\mathbb{F}$ be a field that is NOT of characteristic $2$"" mean here (I am aware of definition of characteristic of a field), i.e. how does the characteristic of field affect the elements in $M_{n \times n} (\mathbb{F})$? And how does it change the proof? Thanks.","Let $\mathbb{F}$ be a field that is not of characteristic $2$. Define $W_1 = \{ A \in M_{n \times n} (\mathbb{F}) : A_{ij} = 0$ whenever $i \leq j\}$ and $W_2$ to be the set of all symmetric $n \times n$ matrices with entries from $\mathbb{F}$. Both $W_1$ and $W_2$ are subspaces of $M_{n \times n} (\mathbb{F})$. Prove that $M_{n \times n} (\mathbb{F})=W_1 \oplus W_2$. I know how to prove that $M_{n \times n} (\mathbb{F})=W_1 \oplus W_2$ for any arbitrary field $\mathbb{F}$ (by showing $W_1 \cap W_2 = \{0\}$ and any element of $M_{n \times n} (\mathbb{F})$ is sum of elements of $W_1$ and $W_2$). But what does ""$\mathbb{F}$ be a field that is NOT of characteristic $2$"" mean here (I am aware of definition of characteristic of a field), i.e. how does the characteristic of field affect the elements in $M_{n \times n} (\mathbb{F})$? And how does it change the proof? Thanks.",,"['linear-algebra', 'matrices']"
54,"Find the standard matrix representation of the linear transformation T in M2,2","Find the standard matrix representation of the linear transformation T in M2,2",,"let  $T: M_{2,2} \rightarrow M_{2,2}$ be a linear transformation defined by: $$T \left(\begin{bmatrix}         a & b\\         c & d\\         \end{bmatrix}\right) = \begin{bmatrix}a + b& b + a \\ c - d&d+b\end{bmatrix} $$ Find the standard matrix for $S$ by using the standard basis of $M_{2,2}$ This is what i've done so far: $$T \left(\begin{bmatrix}         1 & 0\\         0 & 0\\         \end{bmatrix}\right) = \begin{bmatrix}1& 1 \\ 0&0\end{bmatrix} = x_1 $$ $$T \left(\begin{bmatrix}         0 & 1\\         0 & 0\\         \end{bmatrix}\right) = \begin{bmatrix}1& 1 \\ 0&1\end{bmatrix} = x_2 $$ $$T \left(\begin{bmatrix}         0 & 0\\         1 & 0\\         \end{bmatrix}\right) = \begin{bmatrix}0&0\\ 1&0\end{bmatrix} = x_3 $$ $$T \left(\begin{bmatrix}         0 & 0\\         0 & 1\\         \end{bmatrix}\right) = \begin{bmatrix}0& 0\\ -1&1\end{bmatrix} = x_4 $$ But I'm not too sure where to go from here. All I know that these need to somehow become part of a larger matrix","let  $T: M_{2,2} \rightarrow M_{2,2}$ be a linear transformation defined by: $$T \left(\begin{bmatrix}         a & b\\         c & d\\         \end{bmatrix}\right) = \begin{bmatrix}a + b& b + a \\ c - d&d+b\end{bmatrix} $$ Find the standard matrix for $S$ by using the standard basis of $M_{2,2}$ This is what i've done so far: $$T \left(\begin{bmatrix}         1 & 0\\         0 & 0\\         \end{bmatrix}\right) = \begin{bmatrix}1& 1 \\ 0&0\end{bmatrix} = x_1 $$ $$T \left(\begin{bmatrix}         0 & 1\\         0 & 0\\         \end{bmatrix}\right) = \begin{bmatrix}1& 1 \\ 0&1\end{bmatrix} = x_2 $$ $$T \left(\begin{bmatrix}         0 & 0\\         1 & 0\\         \end{bmatrix}\right) = \begin{bmatrix}0&0\\ 1&0\end{bmatrix} = x_3 $$ $$T \left(\begin{bmatrix}         0 & 0\\         0 & 1\\         \end{bmatrix}\right) = \begin{bmatrix}0& 0\\ -1&1\end{bmatrix} = x_4 $$ But I'm not too sure where to go from here. All I know that these need to somehow become part of a larger matrix",,"['linear-algebra', 'matrices', 'linear-transformations']"
55,Prove that if G has a faithful complex irreducible representation,Prove that if G has a faithful complex irreducible representation,,"I am struggling with the proof that if G has a faithful complex irreducible representation then $Z(G)$ is cyclic: Let $\rho:G \rightarrow GL(V)$ be a faithful complex irreducible representation. Let $z \in Z(G)$ . Consider the map $\phi_z: v \mapsto zv$ for all $v \in V$ . This is a G-endomorphism on $V$ , hence is multiplication by a scalar $\mu_z$ "" I keep coming across the term G-homomorphism. For instance in Schur's Lemma ...""Then any G-homomorphism $\theta:V \rightarrow W$ is 0 or an isomorphism"". What exactly is G-homomorphism? Then the map $Z(G) \rightarrow \mathbb{C}^\times, z \mapsto \mu_z$ , is a representation of $Z$ and is faithful (since $\rho$ is). Thus $Z(G)$ is isomorphic to a finite subgroup of $\mathbb{C}^\times$ , hence is cyclic. What is the justification for this last sentence?","I am struggling with the proof that if G has a faithful complex irreducible representation then is cyclic: Let be a faithful complex irreducible representation. Let . Consider the map for all . This is a G-endomorphism on , hence is multiplication by a scalar "" I keep coming across the term G-homomorphism. For instance in Schur's Lemma ...""Then any G-homomorphism is 0 or an isomorphism"". What exactly is G-homomorphism? Then the map , is a representation of and is faithful (since is). Thus is isomorphic to a finite subgroup of , hence is cyclic. What is the justification for this last sentence?","Z(G) \rho:G \rightarrow GL(V) z \in Z(G) \phi_z: v \mapsto zv v \in V V \mu_z \theta:V \rightarrow W Z(G) \rightarrow \mathbb{C}^\times, z \mapsto \mu_z Z \rho Z(G) \mathbb{C}^\times","['linear-algebra', 'group-theory', 'representation-theory', 'group-homomorphism']"
56,"Is a square matrix with positive determinant, positive diagonal entries and negative off-diagonal entries an M-matrix?","Is a square matrix with positive determinant, positive diagonal entries and negative off-diagonal entries an M-matrix?",,"I'm trying to determine if a certain class of matrices are M-matrices in general. I'm considering square matrices $A$ with the following properties: $\det(A) > 0$ (strictly), all the diagonal entries are positive, all the off diagonal entries are negative. An M-matrix can be characterized in many ways. I've tried proving this (or finding a counterexample) by looking at the principal minors and have found that $A$ is an M-matrix if it has dimension 2 or 3, but it's hard to make any sort of induction with that. Right now I'm trying two other definitions (they're equivalent) of M-matrices There is a positive vector such that $Ax > 0$ (component-wise). $A$ is monotone (i.e. $Ax \geq 0$ implies $x \geq 0$). Again, 1 isn't hard to show if the matrix is small, but this is hard to generalize, so I thought an easier approach might be using 2 and try to proceed by contradiction. Does anyone here have some suggestions? This is an outside project for a class I'm working on so I don't know if these matrices are or are not M-matrices in general - mostly just looking for tips here.","I'm trying to determine if a certain class of matrices are M-matrices in general. I'm considering square matrices $A$ with the following properties: $\det(A) > 0$ (strictly), all the diagonal entries are positive, all the off diagonal entries are negative. An M-matrix can be characterized in many ways. I've tried proving this (or finding a counterexample) by looking at the principal minors and have found that $A$ is an M-matrix if it has dimension 2 or 3, but it's hard to make any sort of induction with that. Right now I'm trying two other definitions (they're equivalent) of M-matrices There is a positive vector such that $Ax > 0$ (component-wise). $A$ is monotone (i.e. $Ax \geq 0$ implies $x \geq 0$). Again, 1 isn't hard to show if the matrix is small, but this is hard to generalize, so I thought an easier approach might be using 2 and try to proceed by contradiction. Does anyone here have some suggestions? This is an outside project for a class I'm working on so I don't know if these matrices are or are not M-matrices in general - mostly just looking for tips here.",,"['linear-algebra', 'matrices']"
57,"$T,S: V\to V$, prove that $TS$ and $ST$ have the same eigenvalues",", prove that  and  have the same eigenvalues","T,S: V\to V TS ST","hey I was trying to prove this proposition by dividing to cases and this is what I've got so far: let's assume without loss of generality that T is invertible: ST = [T][S] TS = [S][T] Pst(x) = |[T][S]-גI| = |[T][S]-גI|*|[T][T^-1]| = |[T^-1]|*|[T][S]-גI|*|[T]| = |[S]-[T^-1]גI|*|[T]| = |[S]-ג[T^-1]|*|[T]| = |[S][T]-גI| = Pts(x) they have the same characteristic polynomial, thus the same eigenvalues. but what about the case when both are non-invertible? would appreciate any kind of help.","hey I was trying to prove this proposition by dividing to cases and this is what I've got so far: let's assume without loss of generality that T is invertible: ST = [T][S] TS = [S][T] Pst(x) = |[T][S]-גI| = |[T][S]-גI|*|[T][T^-1]| = |[T^-1]|*|[T][S]-גI|*|[T]| = |[S]-[T^-1]גI|*|[T]| = |[S]-ג[T^-1]|*|[T]| = |[S][T]-גI| = Pts(x) they have the same characteristic polynomial, thus the same eigenvalues. but what about the case when both are non-invertible? would appreciate any kind of help.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'linear-transformations']"
58,Positive semidefinite matrix proof,Positive semidefinite matrix proof,,Let $e_i$ the $i$-th column of the identity matrix. Is there an easy way to prove that the matrix  $$\left[\matrix{\mathbb{I}_n & e_1e_2^T &  \cdots & e_1e_n^T\\ e_2e_1^T & \mathbb{I}_n & \cdots & e_2e_n^T \\ \vdots & \vdots & \ddots & \vdots\\ e_ne_1^T & e_ne_2^T &\cdots & \mathbb{I}_n}\right]$$ is positive semidefinite?,Let $e_i$ the $i$-th column of the identity matrix. Is there an easy way to prove that the matrix  $$\left[\matrix{\mathbb{I}_n & e_1e_2^T &  \cdots & e_1e_n^T\\ e_2e_1^T & \mathbb{I}_n & \cdots & e_2e_n^T \\ \vdots & \vdots & \ddots & \vdots\\ e_ne_1^T & e_ne_2^T &\cdots & \mathbb{I}_n}\right]$$ is positive semidefinite?,,"['linear-algebra', 'matrices']"
59,Clarification between a module and a vector space?,Clarification between a module and a vector space?,,"I'm reading Kenneth Hoffman's Linear Algebra, Ed2. In $\S5.5$ it talks about Module and Vector Spaces: (1) ￼If $K$ is a commutative ring with identity, a module over $K$ ( or a $K$-module ) is an algebraic system which behaves like a   vector space, with $K$ playing the role of the scalar field. (2) A basis for the module $V$ is a linearly independent subset   which spans (or generates) the module. This is the same definition   which we gave for vector spaces; and, the important property of a   basis $\mathscr B$ is that each element of $V$ can be expressed uniquely as a linear combination of (some finite number of) elements of $\mathscr B$. (3) The reader is well aware that a basis exists in any vector space   which is spanned by a finite number of vectors. But this is not the   case for modules. (4) Definition. The $K$-module $V$ is called a free module if it   has a basis. If $V$ has a finite basis containing $n$ elements, then   $V$ is called a free $K$-module with $n$ generators . (5) Definition. The module $V$ is finitely generated if it   contains a finite subset which spans $V$. The rank of a finitely   generated module is the smallest integer $k$ such that some $k$   elements span $V$. (6) We repeat that a module may be finitely generated without having a   finite basis. (7) Theorem 5. Let $K$ be a commutative ring with identity. If $V$ is   a free $K$-module with $n$ generators, then the rank of $V$ is $n$. (8) From Theorem 5 we know that ‘free module of rank $n$’ is the same   as ‘free module with $n$ generators.’ I'm quite lost here -- Q1: (3) and (6) seem are talking about the same thing, that a module $V$ could be spanned by a finite number of elements $\{\beta_1, \dots, \beta_n\}$, but $V$ might not have a finite basis. Does this mean that in module $V$ there might be some element $\alpha$, that it can be expressed as a linear combination of $\{\beta_1, \dots, \beta_n\}$, but in two different ways? (So this violates the "" uniquely "" requirement in the basis definition?) If so, could you pls give me some examples of such a $V$? Q2: however, (8) says: ‘free module of rank $n$’ is the same as ‘free module with $n$ generators.’ Seems this conflicts with (3) and (6)?","I'm reading Kenneth Hoffman's Linear Algebra, Ed2. In $\S5.5$ it talks about Module and Vector Spaces: (1) ￼If $K$ is a commutative ring with identity, a module over $K$ ( or a $K$-module ) is an algebraic system which behaves like a   vector space, with $K$ playing the role of the scalar field. (2) A basis for the module $V$ is a linearly independent subset   which spans (or generates) the module. This is the same definition   which we gave for vector spaces; and, the important property of a   basis $\mathscr B$ is that each element of $V$ can be expressed uniquely as a linear combination of (some finite number of) elements of $\mathscr B$. (3) The reader is well aware that a basis exists in any vector space   which is spanned by a finite number of vectors. But this is not the   case for modules. (4) Definition. The $K$-module $V$ is called a free module if it   has a basis. If $V$ has a finite basis containing $n$ elements, then   $V$ is called a free $K$-module with $n$ generators . (5) Definition. The module $V$ is finitely generated if it   contains a finite subset which spans $V$. The rank of a finitely   generated module is the smallest integer $k$ such that some $k$   elements span $V$. (6) We repeat that a module may be finitely generated without having a   finite basis. (7) Theorem 5. Let $K$ be a commutative ring with identity. If $V$ is   a free $K$-module with $n$ generators, then the rank of $V$ is $n$. (8) From Theorem 5 we know that ‘free module of rank $n$’ is the same   as ‘free module with $n$ generators.’ I'm quite lost here -- Q1: (3) and (6) seem are talking about the same thing, that a module $V$ could be spanned by a finite number of elements $\{\beta_1, \dots, \beta_n\}$, but $V$ might not have a finite basis. Does this mean that in module $V$ there might be some element $\alpha$, that it can be expressed as a linear combination of $\{\beta_1, \dots, \beta_n\}$, but in two different ways? (So this violates the "" uniquely "" requirement in the basis definition?) If so, could you pls give me some examples of such a $V$? Q2: however, (8) says: ‘free module of rank $n$’ is the same as ‘free module with $n$ generators.’ Seems this conflicts with (3) and (6)?",,"['linear-algebra', 'vector-spaces', 'modules']"
60,Prove $\min_{i}|\lambda_i| \leq |r_{jj}| \leq \max_{i}|\lambda_i|$,Prove,\min_{i}|\lambda_i| \leq |r_{jj}| \leq \max_{i}|\lambda_i|,"Let A be a normal $n \times n$ matrix with the eigenvalues $\lambda_1,...,\lambda_n$ |A| = |QR|, $|Q^HQ| = I$, $|R| = [r_{ik}]$ upper triangular matrix. Prove: $$\min_{i}|\lambda_i| \leq |r_{jj}| \leq \max_{i}|\lambda_i|, \; j = 1,...,n$$ In this problem, I know that somehow we have to use Cholesky decomposition, which is $A = LL^H$, but I don't know how can I do it to prove.","Let A be a normal $n \times n$ matrix with the eigenvalues $\lambda_1,...,\lambda_n$ |A| = |QR|, $|Q^HQ| = I$, $|R| = [r_{ik}]$ upper triangular matrix. Prove: $$\min_{i}|\lambda_i| \leq |r_{jj}| \leq \max_{i}|\lambda_i|, \; j = 1,...,n$$ In this problem, I know that somehow we have to use Cholesky decomposition, which is $A = LL^H$, but I don't know how can I do it to prove.",,"['linear-algebra', 'matrices', 'numerical-methods', 'numerical-linear-algebra']"
61,Proximal Operator of $ f \left( U \right) = -\log \det \left( U \right) $,Proximal Operator of, f \left( U \right) = -\log \det \left( U \right) ,"This is an assignment problem which I failed to solve in a couple of days. Denote the set of all $n \times n$ symmetric matrices and the set of all $n \times n$ symmetric positive definite matrices by $\mathbb{S}^n$ and $\mathbb{S}^n_{++}$ respectively. Let $f: \mathbb{S}^n_{++} \rightarrow \mathbb{R}$ be defined by $$f(U) = -\log \det (U) \text{ for } U \in \mathbb{S}^n_{++}$$ We are asked to find the proximal mapping $\text{prox}_f$ of $f$. My question is: What is the domain of $\text{prox}_f$? It is not mentioned in the problem. I suppose it is $\mathbb{S}^n$ and I will explain it later. The function $g(U;X) = -\log \det(U) + \frac{1}{2}\|U - X\|_F^2$ is differentiable in each entry $U_{ij}$ of $U$. If I did not make mistakes in the calculation, the minimization problem  $$ \text{prox}_f(X) = \text{argmin}_{U \in \mathbb{S}^n_{++}} g(U;X) $$ can be formulated as $$ 0 = \frac{\partial g(U;X)}{\partial U_{ij}} = -U^{-1}_{ji} + U_{ij} - X_{ij} = -U^{-1}_{ij} + U_{ij} - X_{ij}$$ and therefore $U = \text{prox}_f(X) \in \mathbb{S}^n_{++}$ should satisfy $$U - U^{-1} = X$$ This also sees why $X$ has to be symmetric. But I cannot proceed from here: How do we calculate $U$ from $X$ in practice? Thanks in advance. Any comments or hints are welcome.","This is an assignment problem which I failed to solve in a couple of days. Denote the set of all $n \times n$ symmetric matrices and the set of all $n \times n$ symmetric positive definite matrices by $\mathbb{S}^n$ and $\mathbb{S}^n_{++}$ respectively. Let $f: \mathbb{S}^n_{++} \rightarrow \mathbb{R}$ be defined by $$f(U) = -\log \det (U) \text{ for } U \in \mathbb{S}^n_{++}$$ We are asked to find the proximal mapping $\text{prox}_f$ of $f$. My question is: What is the domain of $\text{prox}_f$? It is not mentioned in the problem. I suppose it is $\mathbb{S}^n$ and I will explain it later. The function $g(U;X) = -\log \det(U) + \frac{1}{2}\|U - X\|_F^2$ is differentiable in each entry $U_{ij}$ of $U$. If I did not make mistakes in the calculation, the minimization problem  $$ \text{prox}_f(X) = \text{argmin}_{U \in \mathbb{S}^n_{++}} g(U;X) $$ can be formulated as $$ 0 = \frac{\partial g(U;X)}{\partial U_{ij}} = -U^{-1}_{ji} + U_{ij} - X_{ij} = -U^{-1}_{ij} + U_{ij} - X_{ij}$$ and therefore $U = \text{prox}_f(X) \in \mathbb{S}^n_{++}$ should satisfy $$U - U^{-1} = X$$ This also sees why $X$ has to be symmetric. But I cannot proceed from here: How do we calculate $U$ from $X$ in practice? Thanks in advance. Any comments or hints are welcome.",,"['linear-algebra', 'matrices', 'optimization', 'convex-optimization', 'proximal-operators']"
62,What is an 'independent parameter' of a matrix?,What is an 'independent parameter' of a matrix?,,I saw the following question a book of mine: Show that an $n\times n$ orthogonal matrix has $n(n-1)/2$ independent parameters. I have no idea what an independent parameter is. Could you explain it to me?,I saw the following question a book of mine: Show that an $n\times n$ orthogonal matrix has $n(n-1)/2$ independent parameters. I have no idea what an independent parameter is. Could you explain it to me?,,"['linear-algebra', 'matrices']"
63,Inner Product on $\mathbb{R}$ and on $\mathbb{C}$,Inner Product on  and on,\mathbb{R} \mathbb{C},"This is a question of the book Linear Algebra of Kenneth Hoffman. Describe explicitly all inner products on $\mathbb{R}$ and on $\mathbb{C}$. I think that if $<,>$ is one inner product on $\mathbb{R}$ (or $\mathbb{C}$) then $< ,>=K[,]$, where $[,]$ is the standard inner product. Is that correct?","This is a question of the book Linear Algebra of Kenneth Hoffman. Describe explicitly all inner products on $\mathbb{R}$ and on $\mathbb{C}$. I think that if $<,>$ is one inner product on $\mathbb{R}$ (or $\mathbb{C}$) then $< ,>=K[,]$, where $[,]$ is the standard inner product. Is that correct?",,['linear-algebra']
64,"Graphing linear, affine, and convex combinations","Graphing linear, affine, and convex combinations",,"For the vectors (2, 1) and (1, 3), how would I graph each of the three combinations? Here are my thoughts (sorry might be totally wrong): linear - plane connecting the two points affine - infinite line connecting the two points convex - triangle with vertices at origin and the two points","For the vectors (2, 1) and (1, 3), how would I graph each of the three combinations? Here are my thoughts (sorry might be totally wrong): linear - plane connecting the two points affine - infinite line connecting the two points convex - triangle with vertices at origin and the two points",,"['linear-algebra', 'combinations']"
65,$|\det (A+B)|\ge |\det B|$ for all $B$ such that $AB=BA$ iff $A^2=O$,for all  such that  iff,|\det (A+B)|\ge |\det B| B AB=BA A^2=O,"Let $A\in M_2(\mathbb{C})$. $Z(A)$ is the set of all $B\in M_2(\mathbb{C})$ such that $AB=BA$. Prove that $|\det(A+B)|\ge |\det B|$ for all $B\in Z(A)$ if and only if $A^2=O$. If $A^2=O$ and $A\neq O$, suppose $\lambda$ is an eigenvalue of $A+B$. Then $(A+B)x=\lambda x$, then $A(A+B)x=\lambda(Ax)$, or $ABx=\lambda (Ax)$, or $B(Ax)=\lambda(Ax)$. Then $\lambda$ is also an eigenvalue of $B$. Thus $\det (A+B)=\det B$. The problem is I don't know how to deal with the converse, that is if $|\det (A+B)|\ge |\det B|$ for all $B\in Z(A)$, then $A^2=O$. Thanks in advance.","Let $A\in M_2(\mathbb{C})$. $Z(A)$ is the set of all $B\in M_2(\mathbb{C})$ such that $AB=BA$. Prove that $|\det(A+B)|\ge |\det B|$ for all $B\in Z(A)$ if and only if $A^2=O$. If $A^2=O$ and $A\neq O$, suppose $\lambda$ is an eigenvalue of $A+B$. Then $(A+B)x=\lambda x$, then $A(A+B)x=\lambda(Ax)$, or $ABx=\lambda (Ax)$, or $B(Ax)=\lambda(Ax)$. Then $\lambda$ is also an eigenvalue of $B$. Thus $\det (A+B)=\det B$. The problem is I don't know how to deal with the converse, that is if $|\det (A+B)|\ge |\det B|$ for all $B\in Z(A)$, then $A^2=O$. Thanks in advance.",,['linear-algebra']
66,For which values of $a$ the matrix is diagonalizable,For which values of  the matrix is diagonalizable,a,"Given the following matrix: $$B=\begin{bmatrix} 1 & 0 & 0 \\ 1 & 0 & a^2 \\ 1 & 1 & 0 \end{bmatrix}$$ I tried to find for which values of $a$, the matrix $B$ is diagonalizable. I found that the characteristic polynomial is: $P_B(x) = (1-x)(x+a)(x-a)$. ( Stop reading here and skip to the Edit section below ) Therefore I tried to find the eigenspace for each eigenvalue, but eventually concluded that: for $a=(-1)$, the eigenspaces are linearly dependent. for $a=1$, the trace of the diagonal form matrix (call it D ) isn't equal to the trace of the matrix that's composed of the eigenvectors (call it Q ). (' a ' must be 1 or (-1) according to the homogeneous equations with which I found the eigenspaces) The diagonal form matrix that I have found ( D ): $$D=\begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & -1 \end{bmatrix}$$ The matrix that's composed of the eigenvectors that I have found ( Q ): $$Q=\begin{bmatrix} 0 & -1 & 0 \\ 1 & 2 & -1 \\ 1 & 1 & 1 \end{bmatrix}$$ Edit: I have found a mistake in the row reduction process of the matrices... So now we have: The diagonal form matrix ( D ): $$D=\begin{bmatrix} 1 & 0 & 0 \\ 0 & a & 0 \\ 0 & 0 & -a \end{bmatrix}$$ The matrix that's composed of the eigenvectors ( Q ): $$Q=\begin{bmatrix} 1-\frac{a^2+1}{2} & 0 & 0 \\ \frac{a^2+1}{2} & a & -a \\ 0 & 1 & 1 \end{bmatrix}$$ D and Q should be similar, thus by comparing their trace, I have found that: $a_1=(1+\sqrt{2})$ and $a_2=(1-\sqrt{2})$. Does it make sense? Edit2 - To conclude: I was confused about the relations between the matrices $B,Q \text{ and }D$: At first I thought that matrices $Q \text{ and }D$ must be similar, but that isn't necessarily true! Only matrices $B \text{ and }D$ must be similar. Edit 3 - Response to Marc: I understood everything until the last sentence. Also I tried an example using Wolfram Alpha on this matrix . Does it have something to do with a nilpotent characteristic of the matrix? Indeed I can compute $(B-pI)(B-qI)$ and also could've seen with the example that it's true, but don't understand the rules (or characteristics) which allow this to be true.","Given the following matrix: $$B=\begin{bmatrix} 1 & 0 & 0 \\ 1 & 0 & a^2 \\ 1 & 1 & 0 \end{bmatrix}$$ I tried to find for which values of $a$, the matrix $B$ is diagonalizable. I found that the characteristic polynomial is: $P_B(x) = (1-x)(x+a)(x-a)$. ( Stop reading here and skip to the Edit section below ) Therefore I tried to find the eigenspace for each eigenvalue, but eventually concluded that: for $a=(-1)$, the eigenspaces are linearly dependent. for $a=1$, the trace of the diagonal form matrix (call it D ) isn't equal to the trace of the matrix that's composed of the eigenvectors (call it Q ). (' a ' must be 1 or (-1) according to the homogeneous equations with which I found the eigenspaces) The diagonal form matrix that I have found ( D ): $$D=\begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & -1 \end{bmatrix}$$ The matrix that's composed of the eigenvectors that I have found ( Q ): $$Q=\begin{bmatrix} 0 & -1 & 0 \\ 1 & 2 & -1 \\ 1 & 1 & 1 \end{bmatrix}$$ Edit: I have found a mistake in the row reduction process of the matrices... So now we have: The diagonal form matrix ( D ): $$D=\begin{bmatrix} 1 & 0 & 0 \\ 0 & a & 0 \\ 0 & 0 & -a \end{bmatrix}$$ The matrix that's composed of the eigenvectors ( Q ): $$Q=\begin{bmatrix} 1-\frac{a^2+1}{2} & 0 & 0 \\ \frac{a^2+1}{2} & a & -a \\ 0 & 1 & 1 \end{bmatrix}$$ D and Q should be similar, thus by comparing their trace, I have found that: $a_1=(1+\sqrt{2})$ and $a_2=(1-\sqrt{2})$. Does it make sense? Edit2 - To conclude: I was confused about the relations between the matrices $B,Q \text{ and }D$: At first I thought that matrices $Q \text{ and }D$ must be similar, but that isn't necessarily true! Only matrices $B \text{ and }D$ must be similar. Edit 3 - Response to Marc: I understood everything until the last sentence. Also I tried an example using Wolfram Alpha on this matrix . Does it have something to do with a nilpotent characteristic of the matrix? Indeed I can compute $(B-pI)(B-qI)$ and also could've seen with the example that it's true, but don't understand the rules (or characteristics) which allow this to be true.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'diagonalization', 'trace']"
67,"Given $n$ linear functionals $f_k(x_1,\dotsc,x_n) = \sum_{j=1}^n (k-j)x_j$, what is the dimension of the subspace they annihilate?","Given  linear functionals , what is the dimension of the subspace they annihilate?","n f_k(x_1,\dotsc,x_n) = \sum_{j=1}^n (k-j)x_j","Let $F$ be a subfield of the complex numbers. We define $n$ linear functionals on $F^n$ ($n \geq 2$) by $f_k(x_1, \dotsc, x_n) = \sum_{j=1}^n (k-j) x_j$, $1 \leq k \leq n$. What is the dimension of the subspace annihilated by $f_1, \dotsc, f_n$? Approaches I've tried so far: Construct a matrix $A$ whose $k$th row's entries are the coefficients of $f_k$, i.e., $A_{ij} = i - j$, and compute the rank of the matrix. Empirically, the resulting matrix has rank 2 for $n = 2$ to $n = 6$, but I don't see a convenient pattern to follow for a row reduction type proof for the general case. Observe that $f_k$, $k \geq 2$ annihilates $x = (x_1,\dotsc, x_n)$ iff $$\sum_{j=1}^{k-1} (k-j)x_j + \sum_{j = k+1}^n (j-k)x_j = 0$$ iff $$k\left(\sum_{j=1}^{k-1} x_j - \sum_{j=k+1}^n x_j\right) = \sum_{j=1}^{k-1} jx_j - \sum_{j=k+1}^n jx_j,$$ and go from there, but I do not see how to proceed. Note: This is Exercise 10 in Section 3.5 (""Linear Functionals"") in Linear Algebra by Hoffman and Kunze; eigenvalues and determinants have not yet been introduced and so I am looking for direction towards an elementary proof.","Let $F$ be a subfield of the complex numbers. We define $n$ linear functionals on $F^n$ ($n \geq 2$) by $f_k(x_1, \dotsc, x_n) = \sum_{j=1}^n (k-j) x_j$, $1 \leq k \leq n$. What is the dimension of the subspace annihilated by $f_1, \dotsc, f_n$? Approaches I've tried so far: Construct a matrix $A$ whose $k$th row's entries are the coefficients of $f_k$, i.e., $A_{ij} = i - j$, and compute the rank of the matrix. Empirically, the resulting matrix has rank 2 for $n = 2$ to $n = 6$, but I don't see a convenient pattern to follow for a row reduction type proof for the general case. Observe that $f_k$, $k \geq 2$ annihilates $x = (x_1,\dotsc, x_n)$ iff $$\sum_{j=1}^{k-1} (k-j)x_j + \sum_{j = k+1}^n (j-k)x_j = 0$$ iff $$k\left(\sum_{j=1}^{k-1} x_j - \sum_{j=k+1}^n x_j\right) = \sum_{j=1}^{k-1} jx_j - \sum_{j=k+1}^n jx_j,$$ and go from there, but I do not see how to proceed. Note: This is Exercise 10 in Section 3.5 (""Linear Functionals"") in Linear Algebra by Hoffman and Kunze; eigenvalues and determinants have not yet been introduced and so I am looking for direction towards an elementary proof.",,['linear-algebra']
68,Find Jordan form of a $3\times 3$ matrix,Find Jordan form of a  matrix,3\times 3,"$$\left( \begin{array}{ccc} 0 & 1 & 2 \\ -5 &-3 & -7 \\  1 & 0 & 0 \end{array} \right) $$ I figured out the eigenvalues are all -1 from the characteristic polynomial, but I'm not sure how to find the 1's on the subdiagonal. I know they're 1, but I'm not sure how that's determined. I also know that the eigenspace is 0 on the matrix where we subtract -1 on the diagonal. Thank you.","$$\left( \begin{array}{ccc} 0 & 1 & 2 \\ -5 &-3 & -7 \\  1 & 0 & 0 \end{array} \right) $$ I figured out the eigenvalues are all -1 from the characteristic polynomial, but I'm not sure how to find the 1's on the subdiagonal. I know they're 1, but I'm not sure how that's determined. I also know that the eigenspace is 0 on the matrix where we subtract -1 on the diagonal. Thank you.",,"['linear-algebra', 'eigenvalues-eigenvectors', 'jordan-normal-form']"
69,What is the relationship between vector and its associated skew symmetric matrix?,What is the relationship between vector and its associated skew symmetric matrix?,,"This is my first post in this forum, so hello everyone! I am working with geometries (i.e. areas, volumes and inertias of polygons and polyhedrons in 3D space). For doing that, I to use both the Cross Product and the Parallel Axis Theorem , among others. According to wikipedia, these can be defined, by using the [...] skew symmetric matrix associated with the position vector [...] and then operating on them (see links) So, my question is: What is the connection between $ \vec{r} = (x,y,z)$ and $ [r]=\left( \begin{array}{ccc} 0 & -z & y \\ z & 0 & -x \\ -y & x & 0 \end{array} \right)$ ? After googling and looking for an answer for this for a while, I have not been able to find an answer. I guess it may have something to do with the eigenvectors, but I am just not sure. Any help will be appreciated! Thanks!","This is my first post in this forum, so hello everyone! I am working with geometries (i.e. areas, volumes and inertias of polygons and polyhedrons in 3D space). For doing that, I to use both the Cross Product and the Parallel Axis Theorem , among others. According to wikipedia, these can be defined, by using the [...] skew symmetric matrix associated with the position vector [...] and then operating on them (see links) So, my question is: What is the connection between $ \vec{r} = (x,y,z)$ and $ [r]=\left( \begin{array}{ccc} 0 & -z & y \\ z & 0 & -x \\ -y & x & 0 \end{array} \right)$ ? After googling and looking for an answer for this for a while, I have not been able to find an answer. I guess it may have something to do with the eigenvectors, but I am just not sure. Any help will be appreciated! Thanks!",,"['linear-algebra', 'geometry']"
70,Any invertible linear map is homotopic to a composition of reflections,Any invertible linear map is homotopic to a composition of reflections,,"I am trying to solve a problem in Hatcher. I reduced my problem to showing that if $f:\mathbb{R}^n\to\mathbb{R}^n$ is an invertible linear map, then $f$ is homotopic to a composition of reflections, i.e there is a path of invertible matrices between $f$ and a diagonal matrix with only $\pm 1$ on the diagonal. The hint is to use Gaussian elimination. Please help.","I am trying to solve a problem in Hatcher. I reduced my problem to showing that if $f:\mathbb{R}^n\to\mathbb{R}^n$ is an invertible linear map, then $f$ is homotopic to a composition of reflections, i.e there is a path of invertible matrices between $f$ and a diagonal matrix with only $\pm 1$ on the diagonal. The hint is to use Gaussian elimination. Please help.",,"['linear-algebra', 'abstract-algebra', 'algebraic-topology']"
71,The inverse of AR structure correlation matrix / Kac-Murdock-Szegő matrix,The inverse of AR structure correlation matrix / Kac-Murdock-Szegő matrix,,"I want to find the inverse of the following matrix: $$ R_{k-1} =     \begin{pmatrix}     1 &\rho &\rho^2 & \dots &\rho^{k-2} \\     \rho &1 &\rho   & \dots  &\rho^{k-3} \\     \rho^2 &\rho &1 & \dots&\rho^{k-4} \\     \vdots &\vdots &\vdots &\ddots &\vdots\\      \rho^{k-2} & \rho^{k-3} & \rho^{k-4} & \dots & 1\end{pmatrix}$$ Let $A_{i,j}$ be the $i,j$ minor of $R_{k-1}$ . By considering the pattern of the above matrix and its symmetrical properties, we can conclude that: $\det(A_{11})  = \det(A_{k-1,k-1})= |R_{k-2}|$ $\det(A_{i,j}) = \det((A_{j,i})^T)$ $\det(A_{i,j}) = 0$ for $|i-j|\le2$ which means that the inverse of $R_{k-1}$ is a tridiagonal symmetric matrix. I've tried to find the inverse using the fact I've described above and using $A^{-1}A=A A^{-1}=I$ . But I couldn't find it, since there are more variables than equations. Did i miss something? or may be is there any other easier way to find the inverse?","I want to find the inverse of the following matrix: Let be the minor of . By considering the pattern of the above matrix and its symmetrical properties, we can conclude that: for which means that the inverse of is a tridiagonal symmetric matrix. I've tried to find the inverse using the fact I've described above and using . But I couldn't find it, since there are more variables than equations. Did i miss something? or may be is there any other easier way to find the inverse?"," R_{k-1} = 
   \begin{pmatrix}
    1 &\rho &\rho^2 & \dots &\rho^{k-2} \\
    \rho &1 &\rho   & \dots  &\rho^{k-3} \\
    \rho^2 &\rho &1 & \dots&\rho^{k-4} \\
    \vdots &\vdots &\vdots &\ddots &\vdots\\ 
    \rho^{k-2} & \rho^{k-3} & \rho^{k-4} & \dots & 1\end{pmatrix} A_{i,j} i,j R_{k-1} \det(A_{11})  = \det(A_{k-1,k-1})= |R_{k-2}| \det(A_{i,j}) = \det((A_{j,i})^T) \det(A_{i,j}) = 0 |i-j|\le2 R_{k-1} A^{-1}A=A A^{-1}=I","['linear-algebra', 'matrices', 'inverse', 'correlation', 'toeplitz-matrices']"
72,Is $\operatorname{End}_K(V)$ self-opposite?,Is  self-opposite?,\operatorname{End}_K(V),Let $V$ be a vector space over a field $K$. Is $\operatorname{End}_K(V)$ a self-opposite $K$-algebra?,Let $V$ be a vector space over a field $K$. Is $\operatorname{End}_K(V)$ a self-opposite $K$-algebra?,,"['linear-algebra', 'ring-theory']"
73,Prove transpose of pseudoinverse commutes,Prove transpose of pseudoinverse commutes,,"How can I show that $(A^T)^+=(A^+)^T$, where $A^+$ is Moore-Penrose Inverse? I know there are 4 properties of the Moore-Penrose Generalized inverse, for example: $$AA^+A=A^+. $$ To prove it, could I take the transpose of the above $$(AA^+A)^T=(A^+)^T$$ and somehow simplify the LHS so it looks like the LHS of the original statement? Is this on the right track at all? I can get the LHS to be:  $$A^+A(A^+)^T$$ but this does not seem to help me. Any hints please?","How can I show that $(A^T)^+=(A^+)^T$, where $A^+$ is Moore-Penrose Inverse? I know there are 4 properties of the Moore-Penrose Generalized inverse, for example: $$AA^+A=A^+. $$ To prove it, could I take the transpose of the above $$(AA^+A)^T=(A^+)^T$$ and somehow simplify the LHS so it looks like the LHS of the original statement? Is this on the right track at all? I can get the LHS to be:  $$A^+A(A^+)^T$$ but this does not seem to help me. Any hints please?",,"['linear-algebra', 'pseudoinverse']"
74,Jacobi vs. Gauss-Seidel: convergence,Jacobi vs. Gauss-Seidel: convergence,,"I know that for tridiagonal matrices the two iterative methods for linear system solving, the Gauss-Seidel method and the Jacobi one, either both converge or neither converges, and the Gauss-Seidel method converges twice as fast as the Jacobi one. Are there theorems relating the convergence speeds of these two methods when the system's matrix is not tridiagonal?","I know that for tridiagonal matrices the two iterative methods for linear system solving, the Gauss-Seidel method and the Jacobi one, either both converge or neither converges, and the Gauss-Seidel method converges twice as fast as the Jacobi one. Are there theorems relating the convergence speeds of these two methods when the system's matrix is not tridiagonal?",,"['linear-algebra', 'numerical-linear-algebra']"
75,Conic section: What is the coordinate matrix of its bilinear form?,Conic section: What is the coordinate matrix of its bilinear form?,,"Given is the conic section $x^2 + xy + y^2 + 2x +3y - 3 = 0$. I need to find the coordinate matrix $M_\beta(s)$ of the bilinear form $s: \mathbb{R}^2 \times \mathbb{R}^2 -> \mathbb{R}$. I read this article first: http://en.wikipedia.org/wiki/Matrix_representation_of_conic_sections So I simply put $A_Q = \begin{pmatrix} 1 & 0.5 & 1 \\ 0.5 & 1 & 1.5 \\ 1 & 1.5 & -3 \end{pmatrix}$ (Since A=B=C = 1, D=2, E=3 and F=-3). So $A_Q$ is the coordinate matrix?","Given is the conic section $x^2 + xy + y^2 + 2x +3y - 3 = 0$. I need to find the coordinate matrix $M_\beta(s)$ of the bilinear form $s: \mathbb{R}^2 \times \mathbb{R}^2 -> \mathbb{R}$. I read this article first: http://en.wikipedia.org/wiki/Matrix_representation_of_conic_sections So I simply put $A_Q = \begin{pmatrix} 1 & 0.5 & 1 \\ 0.5 & 1 & 1.5 \\ 1 & 1.5 & -3 \end{pmatrix}$ (Since A=B=C = 1, D=2, E=3 and F=-3). So $A_Q$ is the coordinate matrix?",,"['linear-algebra', 'matrices', 'conic-sections', 'bilinear-form']"
76,Cube of an integer,Cube of an integer,,"$\frac{x}{y}+\frac{y}{z}+\frac{z}{x}=k$ and $x, y, z, k$ are integers. Prove that $xyz$ is cube of some integer number. I was wondering about giving a parametrization for the rational points on the elliptic curve in $\mathbb{P}^2(\mathbb{R})$ $$ x^2 y + y^2 z + z^2 x = k xyz$$ but I am not too confident in the topic, so I am asking you to shed some light. Thank you for your help.","$\frac{x}{y}+\frac{y}{z}+\frac{z}{x}=k$ and $x, y, z, k$ are integers. Prove that $xyz$ is cube of some integer number. I was wondering about giving a parametrization for the rational points on the elliptic curve in $\mathbb{P}^2(\mathbb{R})$ $$ x^2 y + y^2 z + z^2 x = k xyz$$ but I am not too confident in the topic, so I am asking you to shed some light. Thank you for your help.",,"['linear-algebra', 'elliptic-curves']"
77,Prove that $BA^{-1} B \not=-B$ if $A + B$ is invertible for $A$ invertible and $B$ non-zero matrix,Prove that  if  is invertible for  invertible and  non-zero matrix,BA^{-1} B \not=-B A + B A B,"Let $A$ and $B$ be $n×n$ real square matrices. Matrix $A$ is an invertible and $B$ is a non-zero matrix. a)Prove that $BA^{-1} B \not=-B$ if $A + B$ is invertible b) Let $B= uv^T$ for $u,v \in \Bbb R^n$. Prove that $BA^{-1}B=-B$ when  $v^TA^{-1}u= -1$ I'm kind of left in the dark with this problem. I tried applying trace and using the determinants (as $\det(A + B) = 0$ ), however I don't know how to proceed,","Let $A$ and $B$ be $n×n$ real square matrices. Matrix $A$ is an invertible and $B$ is a non-zero matrix. a)Prove that $BA^{-1} B \not=-B$ if $A + B$ is invertible b) Let $B= uv^T$ for $u,v \in \Bbb R^n$. Prove that $BA^{-1}B=-B$ when  $v^TA^{-1}u= -1$ I'm kind of left in the dark with this problem. I tried applying trace and using the determinants (as $\det(A + B) = 0$ ), however I don't know how to proceed,",,"['linear-algebra', 'matrices', 'inverse']"
78,When a complex matrix is similar to a real matrix?,When a complex matrix is similar to a real matrix?,,Suppose I have a matrix whose entries are in $\mathbb{C}$. How easy or hard is it to tell in general if a matrix $M$ is similar to a real matrix? Rasmus pointed out in the comments that in general a matrix has no similar real counterpart. What about for simple matrices like diagonal ones?,Suppose I have a matrix whose entries are in $\mathbb{C}$. How easy or hard is it to tell in general if a matrix $M$ is similar to a real matrix? Rasmus pointed out in the comments that in general a matrix has no similar real counterpart. What about for simple matrices like diagonal ones?,,['linear-algebra']
79,"If the 2-norm of a matrix is small, the trace of the matrix is also small","If the 2-norm of a matrix is small, the trace of the matrix is also small",,"Is it true that If the 2-norm of a symmetric real matrix is small, then the trace of the matrix is also small? I played around with some matrices in MATLAB and discovered this phenomenon. Does there exist any theorem that relates the 2-norm of a symmetric real matrix to the magnitude of its trace? Thanks.","Is it true that If the 2-norm of a symmetric real matrix is small, then the trace of the matrix is also small? I played around with some matrices in MATLAB and discovered this phenomenon. Does there exist any theorem that relates the 2-norm of a symmetric real matrix to the magnitude of its trace? Thanks.",,"['linear-algebra', 'matrices']"
80,Why generalize the derivative for multivariable functions? [duplicate],Why generalize the derivative for multivariable functions? [duplicate],,"This question already has answers here : Derivative of function with 2 variables (3 answers) Closed 9 years ago . Sorry if this is a dupe (did a search, couldn't find anything). In single variable calculus, if the following limit exists: $$\lim_{h\rightarrow 0}\frac{f(x+h)-f(x)}{h},$$ then this expression itself is the derivative of $f$ at $x$. This is nicely motivated geometrically and otherwise. This definition gives of $f'(x)$ which gives the best linear approximation $\tilde{f}$ to $f$ at $x$ by $$\tilde{f}(t) = f'(x)t + f(x).$$ For functions of several variables, most of the time I see the derivative defined in terms of the best linear approximation to the function explicitly. Specifically, the derivative of a multivariable function $g$ at $\mathbf{y}$ is some linear operator $L(\mathbf{y})$. Concretely, if the following expression is satisfied: $$\lim_{\mathbf{h} \rightarrow 0}\frac{\|g(\mathbf{y} + \mathbf{h}) - g(\mathbf{y}) + L(\mathbf{y})\|}{\|\mathbf{h}\|} = 0,$$ then $L(\mathbf{y})$ is the derivative of $g$ at $\mathbf{y}$. We can show that $L(\mathbf{y})$ is unique in this case. My question is why is this generalization necessary? What is the problem with simply defining is analogously to the single variable case as $$L(\mathbf{y}) = \lim_{\mathbf{h} \rightarrow 0}\frac{g(\mathbf{y} + \mathbf{h}) - g(\mathbf{y})}{\|\mathbf{h}\|}?$$","This question already has answers here : Derivative of function with 2 variables (3 answers) Closed 9 years ago . Sorry if this is a dupe (did a search, couldn't find anything). In single variable calculus, if the following limit exists: $$\lim_{h\rightarrow 0}\frac{f(x+h)-f(x)}{h},$$ then this expression itself is the derivative of $f$ at $x$. This is nicely motivated geometrically and otherwise. This definition gives of $f'(x)$ which gives the best linear approximation $\tilde{f}$ to $f$ at $x$ by $$\tilde{f}(t) = f'(x)t + f(x).$$ For functions of several variables, most of the time I see the derivative defined in terms of the best linear approximation to the function explicitly. Specifically, the derivative of a multivariable function $g$ at $\mathbf{y}$ is some linear operator $L(\mathbf{y})$. Concretely, if the following expression is satisfied: $$\lim_{\mathbf{h} \rightarrow 0}\frac{\|g(\mathbf{y} + \mathbf{h}) - g(\mathbf{y}) + L(\mathbf{y})\|}{\|\mathbf{h}\|} = 0,$$ then $L(\mathbf{y})$ is the derivative of $g$ at $\mathbf{y}$. We can show that $L(\mathbf{y})$ is unique in this case. My question is why is this generalization necessary? What is the problem with simply defining is analogously to the single variable case as $$L(\mathbf{y}) = \lim_{\mathbf{h} \rightarrow 0}\frac{g(\mathbf{y} + \mathbf{h}) - g(\mathbf{y})}{\|\mathbf{h}\|}?$$",,"['linear-algebra', 'multivariable-calculus', 'derivatives']"
81,Constructing a vector space of dimension $\beth_\omega$,Constructing a vector space of dimension,\beth_\omega,"I'm trying to solve Exercise I.13.34 of Kunen's Set Theory , which goes as follows (paraphrased): Let $F$ be a field with $|F| < \beth_\omega$, and $W_0$ a vector space over $F$ with $\aleph_0 \le \dim W_0 < \beth_\omega$.  Recursively let $W_{n+1} = W_n^{**}$ so that $W_n$ is naturally identified with a subspace of $W_{n+1}$.  Then let $W_\omega = \bigcup_n W_n$.  Show that $|W_\omega| = \dim W_{\omega} = \beth_\omega$. Some useful facts: If $W$ is a vector space over $F$ with basis $B$, there is an obvious bijection between $W^*$ and ${}^{B}F$ (i.e. the set of functions from $F$ to $B$, denoted this way to avoid ambiguity with cardinal exponentiation).  Hence $|W^*| = |F|^{\dim W}$. Asaf Karagila showed in this answer that $|W| = \max(\dim W, |F|)$. By the ""dual basis"" construction we have $\dim W^* \ge \dim W$.  (There's an assertion on Wikipedia that the inequality is strict whenever $\dim W$ is infinite, but I don't immediately see how to prove that.) One inequality is pretty easy.  Using Fact 1, we get $|W^*| = |F|^{\dim W} \le |F|^{|W|}$.  Now thanks to the simple fact that ${}^{\beth_n} \beth_m \subset \mathcal{P}(\mathcal{P}(\beth_m \times \beth_n))$ we have $\beth_m^{\beth_n} \le \beth_{\max(m,n)+2}$.  So by induction it follows that $|W_n| < \beth_\omega$ for each $n$, and hence (using Kunen's Theorem 1.12.14) we get  $|W_\omega| \le \beth_\omega$. For the other direction, if $\dim W \ge |F|$ then Fact 3 gives us $\dim W^* \ge |F|$ and hence by Facts 1 and 2 $$\dim W^* = \max(\dim W^*, |F|) = |W^*| = |F|^{\dim W} \ge 2^{\dim W}.$$ So if $\dim W_0 \ge |F|$ then by induction we get $\dim W_n \ge \beth_{2n}$ and therefore $\dim W_\omega \ge \beth_\omega$.  Since $|W_\omega| \ge \dim W_\omega$ we must have equality throughout. But I am stuck on the case $\aleph_0 \le \dim W_0 < |F|$.  Intuitively it still seems like $\dim W^*$ should be ""much larger"" than $\dim W$.  We shouldn't really need to go through the cardinalities of the spaces themselves, but I can't see what to do.  Any hints?","I'm trying to solve Exercise I.13.34 of Kunen's Set Theory , which goes as follows (paraphrased): Let $F$ be a field with $|F| < \beth_\omega$, and $W_0$ a vector space over $F$ with $\aleph_0 \le \dim W_0 < \beth_\omega$.  Recursively let $W_{n+1} = W_n^{**}$ so that $W_n$ is naturally identified with a subspace of $W_{n+1}$.  Then let $W_\omega = \bigcup_n W_n$.  Show that $|W_\omega| = \dim W_{\omega} = \beth_\omega$. Some useful facts: If $W$ is a vector space over $F$ with basis $B$, there is an obvious bijection between $W^*$ and ${}^{B}F$ (i.e. the set of functions from $F$ to $B$, denoted this way to avoid ambiguity with cardinal exponentiation).  Hence $|W^*| = |F|^{\dim W}$. Asaf Karagila showed in this answer that $|W| = \max(\dim W, |F|)$. By the ""dual basis"" construction we have $\dim W^* \ge \dim W$.  (There's an assertion on Wikipedia that the inequality is strict whenever $\dim W$ is infinite, but I don't immediately see how to prove that.) One inequality is pretty easy.  Using Fact 1, we get $|W^*| = |F|^{\dim W} \le |F|^{|W|}$.  Now thanks to the simple fact that ${}^{\beth_n} \beth_m \subset \mathcal{P}(\mathcal{P}(\beth_m \times \beth_n))$ we have $\beth_m^{\beth_n} \le \beth_{\max(m,n)+2}$.  So by induction it follows that $|W_n| < \beth_\omega$ for each $n$, and hence (using Kunen's Theorem 1.12.14) we get  $|W_\omega| \le \beth_\omega$. For the other direction, if $\dim W \ge |F|$ then Fact 3 gives us $\dim W^* \ge |F|$ and hence by Facts 1 and 2 $$\dim W^* = \max(\dim W^*, |F|) = |W^*| = |F|^{\dim W} \ge 2^{\dim W}.$$ So if $\dim W_0 \ge |F|$ then by induction we get $\dim W_n \ge \beth_{2n}$ and therefore $\dim W_\omega \ge \beth_\omega$.  Since $|W_\omega| \ge \dim W_\omega$ we must have equality throughout. But I am stuck on the case $\aleph_0 \le \dim W_0 < |F|$.  Intuitively it still seems like $\dim W^*$ should be ""much larger"" than $\dim W$.  We shouldn't really need to go through the cardinalities of the spaces themselves, but I can't see what to do.  Any hints?",,"['linear-algebra', 'set-theory', 'cardinals']"
82,Finding the basis of an intersection of subspaces,Finding the basis of an intersection of subspaces,,"We have subspaces in $\mathbb R^4: $ $w_1= \operatorname{sp} \left\{  \begin{pmatrix} 1\\  1 \\ 0 \\1 \end{pmatrix} , \begin{pmatrix} 1\\  0 \\ 2 \\0 \end{pmatrix}, \begin{pmatrix} 0\\  2 \\ 1 \\1 \end{pmatrix} \right\}$,    $w_2= \operatorname{sp} \left\{ \begin{pmatrix} 1\\  1 \\ 1 \\1 \end{pmatrix} , \begin{pmatrix} 3\\  2 \\ 3 \\2 \end{pmatrix}, \begin{pmatrix} 2\\  -1 \\ 2 \\0 \end{pmatrix} \right\}$ Find the basis of $w_1+w_2$ and the basis of $w_1\cap w_2$. So in order to find the basis for $w_1+w_2$, I need to make a $4\times 6$ matrix of all the six vectors, bring it to RREF and see which vector is LD and the basis would be the LI vectors. But the intersection of these 2 spans seems empty, or are they the LD vectors that I should've found before ? In general, how is the intersection of subspaces defined ?","We have subspaces in $\mathbb R^4: $ $w_1= \operatorname{sp} \left\{  \begin{pmatrix} 1\\  1 \\ 0 \\1 \end{pmatrix} , \begin{pmatrix} 1\\  0 \\ 2 \\0 \end{pmatrix}, \begin{pmatrix} 0\\  2 \\ 1 \\1 \end{pmatrix} \right\}$,    $w_2= \operatorname{sp} \left\{ \begin{pmatrix} 1\\  1 \\ 1 \\1 \end{pmatrix} , \begin{pmatrix} 3\\  2 \\ 3 \\2 \end{pmatrix}, \begin{pmatrix} 2\\  -1 \\ 2 \\0 \end{pmatrix} \right\}$ Find the basis of $w_1+w_2$ and the basis of $w_1\cap w_2$. So in order to find the basis for $w_1+w_2$, I need to make a $4\times 6$ matrix of all the six vectors, bring it to RREF and see which vector is LD and the basis would be the LI vectors. But the intersection of these 2 spans seems empty, or are they the LD vectors that I should've found before ? In general, how is the intersection of subspaces defined ?",,['linear-algebra']
83,Understanding vector projection,Understanding vector projection,,"I'm learning about vector projection. I understand how to perform it, but I still can't understand what it actually means and what it gives me. Here is a common definition: Vector projection of a onto b , gives the component   of a that is in the same direction of b . What does this mean? Vectors are straight lines. a can't have a part of it in the direction of b and a part of it that isn't. A vector has one direction, it's a straight line. I just can't understand this. I hope with some help here it'll finally click. Thanks","I'm learning about vector projection. I understand how to perform it, but I still can't understand what it actually means and what it gives me. Here is a common definition: Vector projection of a onto b , gives the component   of a that is in the same direction of b . What does this mean? Vectors are straight lines. a can't have a part of it in the direction of b and a part of it that isn't. A vector has one direction, it's a straight line. I just can't understand this. I hope with some help here it'll finally click. Thanks",,"['linear-algebra', 'analytic-geometry']"
84,$f: SL_2(\mathbb{R}) \to GL_4(\mathbb{R})$ show that $Im(f)=SL_4(\mathbb{R})$,show that,f: SL_2(\mathbb{R}) \to GL_4(\mathbb{R}) Im(f)=SL_4(\mathbb{R}),"I was struggling with the following problem (from linear algebra): Let $V$ be the vector space of the $2 \times 2$ matrices with real coefficients. Consider the action of the group $SL_2(\mathbb{R})$ on $V$, namely for any matrix $T \in SL_2(\mathbb{R})$ and for every matrx $A \in V$ we define $\Phi_T(A)=T\cdot A\cdot T^{-1}$. Prove that for every $T\in SL_2(\mathbb{R})$, $\Phi_T$ is an endomorphism of $V$. Then starting from a basis of $V$, write the corresponding homomorphism $SL_2(\mathbb{R}) \to GL_4(\mathbb{R})$, then show that the image lies inside $SL_4(\mathbb{R})$ First of all. Does anybody know the origin of this problem? Is there a book with standard solutions to this kind of problem? And is my solution any good? Apparently it is generalizable to many more dimensions, giving a nontrivial result on the determinant of such matrices. And what does it happen when the trace is $0$? Can we show an homomorphism $SL_2(\mathbb{R}) \to SL_3(\mathbb{R})$ in the same way?","I was struggling with the following problem (from linear algebra): Let $V$ be the vector space of the $2 \times 2$ matrices with real coefficients. Consider the action of the group $SL_2(\mathbb{R})$ on $V$, namely for any matrix $T \in SL_2(\mathbb{R})$ and for every matrx $A \in V$ we define $\Phi_T(A)=T\cdot A\cdot T^{-1}$. Prove that for every $T\in SL_2(\mathbb{R})$, $\Phi_T$ is an endomorphism of $V$. Then starting from a basis of $V$, write the corresponding homomorphism $SL_2(\mathbb{R}) \to GL_4(\mathbb{R})$, then show that the image lies inside $SL_4(\mathbb{R})$ First of all. Does anybody know the origin of this problem? Is there a book with standard solutions to this kind of problem? And is my solution any good? Apparently it is generalizable to many more dimensions, giving a nontrivial result on the determinant of such matrices. And what does it happen when the trace is $0$? Can we show an homomorphism $SL_2(\mathbb{R}) \to SL_3(\mathbb{R})$ in the same way?",,"['linear-algebra', 'abstract-algebra']"
85,How does a row of zeros make a free variable in linear systems of equations?,How does a row of zeros make a free variable in linear systems of equations?,,"I don't understand how a row of zeros gives a free variable when solving systems of linear equations. Here's an example matrix and let us say that we're trying to solve Ax=0 : $$\left[         \begin{matrix}         2 & -3 & 0 \\         3 & 5 & 0 \\         0 & 1 & 0 \\         \end{matrix} \right]$$ This makes sense to me - you have a COLUMN of numbers corresponding to the number of times the variable $x_3$ is used for each equation and since they are all zeros, $x_3$ could have been any real number because we never get to manipulate our equations to determine a value for it. Hence, it is a free variable as it is not subject to the constraints of the equations. $$\left[         \begin{matrix}         2 & -3 & 5 \\         0 & 5 & 1 \\         0 & 0 & 0 \\         \end{matrix} \right]$$ Now for this second example, $x_3$ would still be a free variable. Why is this so? $x_3$ is being used in the other two equations where you could certainly come up with a finite set of answers for this variable rather than saying ""It could've been anything!"" right? Also is it entirely arbitrary that $x_3$ is the free variable or could it be decided that $x_1$ or $x_2$ is free instead? Could someone explain to me in a more layman or simplified form on why a row of zeros magically makes a free variable? Please help me :(","I don't understand how a row of zeros gives a free variable when solving systems of linear equations. Here's an example matrix and let us say that we're trying to solve Ax=0 : $$\left[         \begin{matrix}         2 & -3 & 0 \\         3 & 5 & 0 \\         0 & 1 & 0 \\         \end{matrix} \right]$$ This makes sense to me - you have a COLUMN of numbers corresponding to the number of times the variable $x_3$ is used for each equation and since they are all zeros, $x_3$ could have been any real number because we never get to manipulate our equations to determine a value for it. Hence, it is a free variable as it is not subject to the constraints of the equations. $$\left[         \begin{matrix}         2 & -3 & 5 \\         0 & 5 & 1 \\         0 & 0 & 0 \\         \end{matrix} \right]$$ Now for this second example, $x_3$ would still be a free variable. Why is this so? $x_3$ is being used in the other two equations where you could certainly come up with a finite set of answers for this variable rather than saying ""It could've been anything!"" right? Also is it entirely arbitrary that $x_3$ is the free variable or could it be decided that $x_1$ or $x_2$ is free instead? Could someone explain to me in a more layman or simplified form on why a row of zeros magically makes a free variable? Please help me :(",,"['linear-algebra', 'matrices', 'systems-of-equations']"
86,Charactristic polynomial of a F-linear transformation with respect to Galois group,Charactristic polynomial of a F-linear transformation with respect to Galois group,,"Let $K$ be a Galois extension of $F$, and let $a \in K$. Let $L_a : K \to K$ be the $F$-linear transformation defined by $L_a(b)=ab$. Show that the characteristic polynomial of $L_a$ is $\prod_{\sigma \in \operatorname{Gal}(L/K)}(x- \sigma (a))$ and the minimal polynomial of $L_a$ is $\min(F,a).$","Let $K$ be a Galois extension of $F$, and let $a \in K$. Let $L_a : K \to K$ be the $F$-linear transformation defined by $L_a(b)=ab$. Show that the characteristic polynomial of $L_a$ is $\prod_{\sigma \in \operatorname{Gal}(L/K)}(x- \sigma (a))$ and the minimal polynomial of $L_a$ is $\min(F,a).$",,"['linear-algebra', 'galois-theory']"
87,The periodic nature of the fibonacci sequence modulo $m$,The periodic nature of the fibonacci sequence modulo,m,"Let $x_n$ denote the $n$-th element of the fibonacci sequence and $$A:=\begin{pmatrix} 0&1\\1&1 \end{pmatrix}$$ It's easy to show, that it holds: $$A^n=\begin{pmatrix} F_{n-1}&F_n\\F_n&F_{n+1}  \end{pmatrix}$$ However, I want to show that $$(F_n\text{ mod }m)_n\;\;\;\;\;(m\in\mathbb{N})$$ is a periodic sequence. Therefor, it's sufficient to show, that $$(A^n\text{ mod }m)_n$$ is periodic. In other words: We need to show, that $A$ is an element of finite order in $\text{GL}(2,\mathbb{Z}/m\mathbb{Z})$. What's the most elegant way to do that? PS: I know that it might be better to choose $A$ and thereby $A^n$ in an other way, but I'm asked to show the statement for the given choice of $A$.","Let $x_n$ denote the $n$-th element of the fibonacci sequence and $$A:=\begin{pmatrix} 0&1\\1&1 \end{pmatrix}$$ It's easy to show, that it holds: $$A^n=\begin{pmatrix} F_{n-1}&F_n\\F_n&F_{n+1}  \end{pmatrix}$$ However, I want to show that $$(F_n\text{ mod }m)_n\;\;\;\;\;(m\in\mathbb{N})$$ is a periodic sequence. Therefor, it's sufficient to show, that $$(A^n\text{ mod }m)_n$$ is periodic. In other words: We need to show, that $A$ is an element of finite order in $\text{GL}(2,\mathbb{Z}/m\mathbb{Z})$. What's the most elegant way to do that? PS: I know that it might be better to choose $A$ and thereby $A^n$ in an other way, but I'm asked to show the statement for the given choice of $A$.",,"['linear-algebra', 'group-theory', 'matrices', 'finite-groups']"
88,Rank-one perturbation proof,Rank-one perturbation proof,,"I wrote a proof for a problem in my textbook. Can someone please verify it or offer suggestions for improvement? $\textbf{Problem:} $If $u$ and $v$ are $m$-vectors, the matrix $A = I+uv^*$ is known as the rank-one perturbation of the identity. Show that if $A$ is nonsingular, then its inverse has the form $A^{-1}=I+\alpha u v^*$ for some scalar $\alpha$, and give an expression for $\alpha$. For which $u$ and $v$ is $A$ singular? If it is singular, what is null($A$)? Suppose $A$ is singular. Then, there exists a nonzero $x \in \mathbb{C}^m$ such that $Ax=0$. That is, $(I+uv^*)x=0$. This implies that $u(v^*x) = -x$. For this to be true, $x$ has to be a multiple of $u$. So, let $x = c_1u$ for some nonzero scalar $c_1$. Then, $Ax = c_1u(v^*u) = -c_1u$. Note that $u = 0$ if $v^*u = 0$. Therefore, $A$ is singular if and only if $v^*u \neq 0$, and null($A$) $=$ span($u$) if it is singular. Now, set $\alpha = -1$. By the reasoning in the above paragraph, $v^*u = 0$ for $A$ to be invertible. Now, $(I+uv^*)(I-uv^*) = I - uv^*uv^* = I - u(v^*u)v^* = I$. Therefore, $A^{-1} = I - uv^*$.","I wrote a proof for a problem in my textbook. Can someone please verify it or offer suggestions for improvement? $\textbf{Problem:} $If $u$ and $v$ are $m$-vectors, the matrix $A = I+uv^*$ is known as the rank-one perturbation of the identity. Show that if $A$ is nonsingular, then its inverse has the form $A^{-1}=I+\alpha u v^*$ for some scalar $\alpha$, and give an expression for $\alpha$. For which $u$ and $v$ is $A$ singular? If it is singular, what is null($A$)? Suppose $A$ is singular. Then, there exists a nonzero $x \in \mathbb{C}^m$ such that $Ax=0$. That is, $(I+uv^*)x=0$. This implies that $u(v^*x) = -x$. For this to be true, $x$ has to be a multiple of $u$. So, let $x = c_1u$ for some nonzero scalar $c_1$. Then, $Ax = c_1u(v^*u) = -c_1u$. Note that $u = 0$ if $v^*u = 0$. Therefore, $A$ is singular if and only if $v^*u \neq 0$, and null($A$) $=$ span($u$) if it is singular. Now, set $\alpha = -1$. By the reasoning in the above paragraph, $v^*u = 0$ for $A$ to be invertible. Now, $(I+uv^*)(I-uv^*) = I - uv^*uv^* = I - u(v^*u)v^* = I$. Therefore, $A^{-1} = I - uv^*$.",,"['linear-algebra', 'proof-verification']"
89,Show that a Hilbert space with two inner products has a basis that is orthogonal with respect to both inner products,Show that a Hilbert space with two inner products has a basis that is orthogonal with respect to both inner products,,"Let $\mathcal{H}$ be a complex, $n$-dimensional Hilbert space with two inner products $\langle \cdot, \cdot  \rangle_1$, $\langle \cdot, \cdot  \rangle_2$. Show that there exists a basis $ X = x_1, \dots, x_n$ of $\mathcal{H}$ such that for $i \neq j$, $0 = \langle x_i, x_j  \rangle_1 = \langle x_i,   x_j \rangle_2$. That is, we want to find a basis that is orthogonal with respect to both inner products. Using Gram Schmidt, I know that we can find an orthonormal basis for each inner product separately. The question is how we then construct a basis that is orthogonal with respect to both $\langle \cdot, \cdot  \rangle_1$ and $\langle \cdot, \cdot  \rangle_2$ at the same time. I have thought about trying to use an induction argument on $n$. The $n=1$ case is trivial. For $n > 1$, we can write $\mathcal{H}$ as a direct sum $\mathcal{H} = V \bigoplus \mathbb{C} x $, where $V$ is an $n-1$ dimensional Hilbert space. We can then find a basis $Y$ of $V$ that is orthogonal with respect to both $\langle \cdot, \cdot  \rangle_1$ and $\langle \cdot, \cdot  \rangle_2$. The question would then be how we make $x$ orthogonal to the rest of the basis elements in $Y$. Hints or solutions are greatly appreciated.","Let $\mathcal{H}$ be a complex, $n$-dimensional Hilbert space with two inner products $\langle \cdot, \cdot  \rangle_1$, $\langle \cdot, \cdot  \rangle_2$. Show that there exists a basis $ X = x_1, \dots, x_n$ of $\mathcal{H}$ such that for $i \neq j$, $0 = \langle x_i, x_j  \rangle_1 = \langle x_i,   x_j \rangle_2$. That is, we want to find a basis that is orthogonal with respect to both inner products. Using Gram Schmidt, I know that we can find an orthonormal basis for each inner product separately. The question is how we then construct a basis that is orthogonal with respect to both $\langle \cdot, \cdot  \rangle_1$ and $\langle \cdot, \cdot  \rangle_2$ at the same time. I have thought about trying to use an induction argument on $n$. The $n=1$ case is trivial. For $n > 1$, we can write $\mathcal{H}$ as a direct sum $\mathcal{H} = V \bigoplus \mathbb{C} x $, where $V$ is an $n-1$ dimensional Hilbert space. We can then find a basis $Y$ of $V$ that is orthogonal with respect to both $\langle \cdot, \cdot  \rangle_1$ and $\langle \cdot, \cdot  \rangle_2$. The question would then be how we make $x$ orthogonal to the rest of the basis elements in $Y$. Hints or solutions are greatly appreciated.",,"['linear-algebra', 'hilbert-spaces']"
90,On the canonical isomorphism between $V$ and $V^{**}$,On the canonical isomorphism between  and,V V^{**},"I am trying to understand more about the Bidualspace (or double dual space). The whole idea is that $V$ and $V^{**}$ are canonically isomorphic to one another, which means that they are isomorphic without the choice of a basis , which means there exists an isomorphism between them which does not depend on choosing some basis (on either of the spaces) . (suggestion by @DonAntonio) Let $V$ be a finite dimensional Vectorspace with Basis $\beta = \lbrace v_1, \dots , v_n \rbrace $ (the infinite dimensional case is discussed at Canonical Isomorphism Between $\mathbf{V}$ and $(\mathbf{V}^*)^*$ ). I do understand that although we make a choice of Basis here, it will later on somehow be obsolete (which I don't see why) Define : \begin{align} \begin{matrix} V & \overset{\Phi}{\longrightarrow}& V^* & \overset{\Phi^*}{\longrightarrow}&V^{**} \\  \sum_{i=1}^n \lambda_i v_i & \longmapsto & \sum_{i=1}^n \lambda_i v_i^* & \longmapsto & \sum_{i=1}^n \lambda_i v_i^{**} \end{matrix} \end{align} Discussion : I know and have already shown that $V$ and $V^*$ are isomorphic (not canonically isomorphic!) to one another, meaning that $\lbrace v_1^*, \dots , v_n^* \rbrace$ defines a Basis for $V^*$. Although I did not show it I am at peace with the statement that the mapping $\Phi^*$ introduces another isomorphism between $V^*$ and $V^{**}$. It is the canonical isomorphism between $V$ and $V^{**}$ that bothers me. My tutors reasoning : In the following I will use $\checkmark$ to highlight whether or not I understand something. $v_i^{**} \in V^{**}=\hom(V^{*},k)  \checkmark$, sure nothing to add here $v_i^{**} (\sum_{j=1}^n \lambda_j v_j^{*})=\lambda_i \checkmark$, should be completely analogous to $v_i^*(v_j)=\lambda_i$ $\Phi^* \circ \Phi (v) =: \iota_v$ $$ \iota: \begin{cases} V & \longrightarrow V^{**} \\ v & \longmapsto \iota_v \end{cases}$$ where $\iota_v( \varphi)= \varphi(v)$ and $\varphi \in V^*$ is a linear functional. My tutor said that $\iota$ is 'suddenly' a canonical isomorphism, independent of the choice of the Basis $\beta$ because $\Phi$ and $\Phi^*$ are isomorphisms. This is the step where I understand nothing about. What happened next (two more equations) : I told my tutor that I don't see why $\iota$ is independent of a basis choice, because at $\Phi^* \circ \Phi= \iota$ we still make a choice for a basis at $\Phi$ namely $\beta$. My tutor said that this seems to be the case at first sight and made two more calculations which I in fact ""understand"": $$ \Phi^* \circ \Phi \left( \sum_{i=1}^n \lambda_i v_i \right) ( \varphi) = \Phi^* \left( \sum_{i=1}^n \lambda_i v_i^* \right) ( \varphi) = \left( \sum_{i=1}^n \lambda_i v_i^{**} \right)(\varphi) \\ = \sum_{i=1}^n \sum_{j=1}^n \lambda_i \mu_i \underbrace{v_i^{**}(v_j^*)}_{= \delta_{ij}} = \sum_{i=1}^n \lambda_i \mu_i $$ I understand this calculation, it's mainly an application of the above introduced definitions and the Kronecker-Delta. Apparently I am supposed to have an ""aha"" moment here, which unfortunately didn't occur yet.  The next calculation he did was $$ \varphi (v) = \left(\sum_{j=1}^n \mu_j v_j^{*} \right) \left( \sum_{i=1}^n \lambda_i v_i \right)= \sum_{j=1}^n \sum_{i=1}^n \mu_j \lambda_i \underbrace{v_j^*( v_i)}_{\delta_{ij}} = \sum_{i=1}^n \lambda_i \mu_i $$ which evidently gives the same result as above. I would appreciate some wording on how those two equations (which calculations I understand) help me to see why there appears to be a canonical isomorphism between $V$ and $V^{**}$","I am trying to understand more about the Bidualspace (or double dual space). The whole idea is that $V$ and $V^{**}$ are canonically isomorphic to one another, which means that they are isomorphic without the choice of a basis , which means there exists an isomorphism between them which does not depend on choosing some basis (on either of the spaces) . (suggestion by @DonAntonio) Let $V$ be a finite dimensional Vectorspace with Basis $\beta = \lbrace v_1, \dots , v_n \rbrace $ (the infinite dimensional case is discussed at Canonical Isomorphism Between $\mathbf{V}$ and $(\mathbf{V}^*)^*$ ). I do understand that although we make a choice of Basis here, it will later on somehow be obsolete (which I don't see why) Define : \begin{align} \begin{matrix} V & \overset{\Phi}{\longrightarrow}& V^* & \overset{\Phi^*}{\longrightarrow}&V^{**} \\  \sum_{i=1}^n \lambda_i v_i & \longmapsto & \sum_{i=1}^n \lambda_i v_i^* & \longmapsto & \sum_{i=1}^n \lambda_i v_i^{**} \end{matrix} \end{align} Discussion : I know and have already shown that $V$ and $V^*$ are isomorphic (not canonically isomorphic!) to one another, meaning that $\lbrace v_1^*, \dots , v_n^* \rbrace$ defines a Basis for $V^*$. Although I did not show it I am at peace with the statement that the mapping $\Phi^*$ introduces another isomorphism between $V^*$ and $V^{**}$. It is the canonical isomorphism between $V$ and $V^{**}$ that bothers me. My tutors reasoning : In the following I will use $\checkmark$ to highlight whether or not I understand something. $v_i^{**} \in V^{**}=\hom(V^{*},k)  \checkmark$, sure nothing to add here $v_i^{**} (\sum_{j=1}^n \lambda_j v_j^{*})=\lambda_i \checkmark$, should be completely analogous to $v_i^*(v_j)=\lambda_i$ $\Phi^* \circ \Phi (v) =: \iota_v$ $$ \iota: \begin{cases} V & \longrightarrow V^{**} \\ v & \longmapsto \iota_v \end{cases}$$ where $\iota_v( \varphi)= \varphi(v)$ and $\varphi \in V^*$ is a linear functional. My tutor said that $\iota$ is 'suddenly' a canonical isomorphism, independent of the choice of the Basis $\beta$ because $\Phi$ and $\Phi^*$ are isomorphisms. This is the step where I understand nothing about. What happened next (two more equations) : I told my tutor that I don't see why $\iota$ is independent of a basis choice, because at $\Phi^* \circ \Phi= \iota$ we still make a choice for a basis at $\Phi$ namely $\beta$. My tutor said that this seems to be the case at first sight and made two more calculations which I in fact ""understand"": $$ \Phi^* \circ \Phi \left( \sum_{i=1}^n \lambda_i v_i \right) ( \varphi) = \Phi^* \left( \sum_{i=1}^n \lambda_i v_i^* \right) ( \varphi) = \left( \sum_{i=1}^n \lambda_i v_i^{**} \right)(\varphi) \\ = \sum_{i=1}^n \sum_{j=1}^n \lambda_i \mu_i \underbrace{v_i^{**}(v_j^*)}_{= \delta_{ij}} = \sum_{i=1}^n \lambda_i \mu_i $$ I understand this calculation, it's mainly an application of the above introduced definitions and the Kronecker-Delta. Apparently I am supposed to have an ""aha"" moment here, which unfortunately didn't occur yet.  The next calculation he did was $$ \varphi (v) = \left(\sum_{j=1}^n \mu_j v_j^{*} \right) \left( \sum_{i=1}^n \lambda_i v_i \right)= \sum_{j=1}^n \sum_{i=1}^n \mu_j \lambda_i \underbrace{v_j^*( v_i)}_{\delta_{ij}} = \sum_{i=1}^n \lambda_i \mu_i $$ which evidently gives the same result as above. I would appreciate some wording on how those two equations (which calculations I understand) help me to see why there appears to be a canonical isomorphism between $V$ and $V^{**}$",,"['linear-algebra', 'vector-spaces', 'definition', 'self-learning']"
91,Rank of a Matrix under certain conditions,Rank of a Matrix under certain conditions,,I am a little confused about the rank of a matrix. When does the rank of a matrix equals to zero? Is rank of a matrix equal to zero when it is a zero matrix or the matrix has no elements in it?  Thank you.,I am a little confused about the rank of a matrix. When does the rank of a matrix equals to zero? Is rank of a matrix equal to zero when it is a zero matrix or the matrix has no elements in it?  Thank you.,,"['linear-algebra', 'definition', 'matrix-rank']"
92,"Find a formula in terms of k for the entries of $A^k$, where A is the diagonalizable matrix:","Find a formula in terms of k for the entries of , where A is the diagonalizable matrix:",A^k,"Find a formula in terms of k for the entries of $A^k$, where A is the diagonalizable matrix This is my 2x2 matrix (sorry for formatting): [$-5$ $8$] [$-4$ $7$] I've tried this question a million times and I always get the answer the wrong. To start off, I found my eigenvalues and eigenvectors which are: eigenvalue1: -1 eigenvalue2: 3, eigenvector1: (1,1), (2,1). Can someone please show me a step by step without ommitting any calculations? I have the answer, but I do not know how to arrive at it. Thanks a lot.","Find a formula in terms of k for the entries of $A^k$, where A is the diagonalizable matrix This is my 2x2 matrix (sorry for formatting): [$-5$ $8$] [$-4$ $7$] I've tried this question a million times and I always get the answer the wrong. To start off, I found my eigenvalues and eigenvectors which are: eigenvalue1: -1 eigenvalue2: 3, eigenvector1: (1,1), (2,1). Can someone please show me a step by step without ommitting any calculations? I have the answer, but I do not know how to arrive at it. Thanks a lot.",,"['linear-algebra', 'eigenvalues-eigenvectors']"
93,Infinite Sequences on Arbitrary Field - Countable Basis?,Infinite Sequences on Arbitrary Field - Countable Basis?,,"Please note that this is a homework question, I've been thinking about this for a couple days and I haven't had much luck! Thought I'd get some other input. Let $V$ be the vector space of all infinite sequences with entries in an arbitrary field $\mathbb{F}$ .  (The goal is to show it doesn't have a countable basis) First show that if $V$ admits a countable basis then there is a nested sequence of subspaces $W_{1} \subset W_{2} \subset ...$ such that $dim(W_{j}) = j$ and $V = \cup W_{j}$ . This proof was fairly obvious because you are given the fact that there is a countable basis so you can just take spans.  However, then ext part says We can represent an infinite sequence as a concatenation of sequences $(s_{2}, ... )$ where $s_{j}$ is a sequence of length $j$ .  For each $j \geq 2$ show that there exists a sequence $S_{j}$ with length $j$ and the property that $s_{j} = S_{j} \rightarrow (s_{2}, ... S_{j}, .. ) \not\in W_{j-1}$ For me what this is trying to get at that given any element in your vector space you can find a sequence of length $j$ such that if you shove it in the $j$ th spot of the sequence representation of the element in the vector space than it isn't in $W_{j-1}$ . My first idea was thinking about picking our ""countable basis""(which we know doesn't exist) to be the element whose ith entry is 1 and the rest is 0.  Then we could just pick up a diagonal argument to ensure that it wouldn't be in $W_{j-1}$ but I can't quite wrap my head about how to apply the same idea to an arbitrary countable basis where we don't know what the elements look like! Lastly it is to show that there's an element which doesn't belong to any of the $W_{j}$ which follows from the above just by picking all those finite length sequences and putting them together. So what gives, I feel as though I'm getting too focused around the idea that I know this not to be the case that I haven't been able to quite get it. Thanks!","Please note that this is a homework question, I've been thinking about this for a couple days and I haven't had much luck! Thought I'd get some other input. Let be the vector space of all infinite sequences with entries in an arbitrary field .  (The goal is to show it doesn't have a countable basis) First show that if admits a countable basis then there is a nested sequence of subspaces such that and . This proof was fairly obvious because you are given the fact that there is a countable basis so you can just take spans.  However, then ext part says We can represent an infinite sequence as a concatenation of sequences where is a sequence of length .  For each show that there exists a sequence with length and the property that For me what this is trying to get at that given any element in your vector space you can find a sequence of length such that if you shove it in the th spot of the sequence representation of the element in the vector space than it isn't in . My first idea was thinking about picking our ""countable basis""(which we know doesn't exist) to be the element whose ith entry is 1 and the rest is 0.  Then we could just pick up a diagonal argument to ensure that it wouldn't be in but I can't quite wrap my head about how to apply the same idea to an arbitrary countable basis where we don't know what the elements look like! Lastly it is to show that there's an element which doesn't belong to any of the which follows from the above just by picking all those finite length sequences and putting them together. So what gives, I feel as though I'm getting too focused around the idea that I know this not to be the case that I haven't been able to quite get it. Thanks!","V \mathbb{F} V W_{1} \subset W_{2} \subset ... dim(W_{j}) = j V = \cup W_{j} (s_{2}, ... ) s_{j} j j \geq 2 S_{j} j s_{j} = S_{j} \rightarrow (s_{2}, ... S_{j}, .. ) \not\in W_{j-1} j j W_{j-1} W_{j-1} W_{j}","['linear-algebra', 'abstract-algebra']"
94,"Prove: If A is invertible, then adj(A) is invertible and $[\operatorname{adj}(A)]^{-1}=\frac1{\det(A)}A=\operatorname{adj}(A^{-1})$","Prove: If A is invertible, then adj(A) is invertible and",[\operatorname{adj}(A)]^{-1}=\frac1{\det(A)}A=\operatorname{adj}(A^{-1}),"Prove: $\newcommand{\adj}{\operatorname{adj}}$ If $A$ is invertible, then $\adj(A)$ is invertible and $[\adj(A)]^{-1}=\frac{1}{\det(A)}A=\adj(A^{-1})$ I can show the left side: \begin{align*} A^{-1}&=\frac{1}{\det(A)}\adj(A)\\ \implies AA^{-1}&=\frac{1}{\det(A)}A \cdot \adj(A)\\ \implies I&=\frac{1}{\det(A)}A\cdot \adj(A), \end{align*} and, \begin{align*} A^{-1}A&=\adj(A)\frac{1}{\det(A)}A\\ \implies I&=\adj(A)\frac{1}{\det(A)}A. \end{align*} So, $$[\adj(A)]^{-1}=\frac{1}{\det(A)}A.$$ But I'm not sure how to show: $$\frac{1}{\det(A)}A=\adj(A^{-1}).$$","Prove: If is invertible, then is invertible and I can show the left side: and, So, But I'm not sure how to show:","\newcommand{\adj}{\operatorname{adj}} A \adj(A) [\adj(A)]^{-1}=\frac{1}{\det(A)}A=\adj(A^{-1}) \begin{align*}
A^{-1}&=\frac{1}{\det(A)}\adj(A)\\
\implies AA^{-1}&=\frac{1}{\det(A)}A \cdot \adj(A)\\
\implies I&=\frac{1}{\det(A)}A\cdot \adj(A),
\end{align*} \begin{align*}
A^{-1}A&=\adj(A)\frac{1}{\det(A)}A\\
\implies I&=\adj(A)\frac{1}{\det(A)}A.
\end{align*} [\adj(A)]^{-1}=\frac{1}{\det(A)}A. \frac{1}{\det(A)}A=\adj(A^{-1}).","['linear-algebra', 'abstract-algebra', 'matrices', 'vector-spaces', 'determinant']"
95,Does element-wise exponentiation of a matrix preserve its positive semi-definiteness?,Does element-wise exponentiation of a matrix preserve its positive semi-definiteness?,,"Given a real symmetric positive semi-definite matrix $\mathbf{A}$, is $\mathbf{A}^{\cdot k},k\in\mathbb{N}$ also positive semi-definite? Matrix $\mathbf{A}\in\mathbb{R}^{n\times n}$ is positive semi-definite if $\mathbf{x}^T\mathbf{A}\mathbf{x}\geq0,\forall\mathbf{x}\in\mathbb{R}^n$. The elements of $\mathbf{A}^{\cdot k}$ are $\{\mathbf{A}^{\cdot k}\}_{ij}=A_{ij}^k,1\leq i,j \leq n$.","Given a real symmetric positive semi-definite matrix $\mathbf{A}$, is $\mathbf{A}^{\cdot k},k\in\mathbb{N}$ also positive semi-definite? Matrix $\mathbf{A}\in\mathbb{R}^{n\times n}$ is positive semi-definite if $\mathbf{x}^T\mathbf{A}\mathbf{x}\geq0,\forall\mathbf{x}\in\mathbb{R}^n$. The elements of $\mathbf{A}^{\cdot k}$ are $\{\mathbf{A}^{\cdot k}\}_{ij}=A_{ij}^k,1\leq i,j \leq n$.",,"['linear-algebra', 'matrices']"
96,Vector spaces with unique basis [duplicate],Vector spaces with unique basis [duplicate],,This question already has an answer here : What kind of vector spaces have exactly one basis? (1 answer) Closed 7 years ago . Vector spaces like $\mathbb{R}^n$ can have different bases and we can change the basis with a matrix to get a new one. This made me wonder: Are there any vector spaces with $dim>1$ that have only one basis?,This question already has an answer here : What kind of vector spaces have exactly one basis? (1 answer) Closed 7 years ago . Vector spaces like $\mathbb{R}^n$ can have different bases and we can change the basis with a matrix to get a new one. This made me wonder: Are there any vector spaces with $dim>1$ that have only one basis?,,"['linear-algebra', 'vector-spaces']"
97,How to solve the six elements equations below?,How to solve the six elements equations below?,,"How to get the exact or numerical solutions of the six elements equations below? $$\begin{cases} \frac{c_1}{1-x_1}+\frac{c_2}{1-x_2}+\frac{c_3}{1-x_3}=0\\ \frac{c_1}{1-x_4}+\frac{c_2}{1-x_5}+\frac{c_3}{1-x_6}=0\\ c_1\ln{\frac{x_1}{1-x_1}}+c_2\ln{\frac{x_2}{1-x_2}} +c_3\ln{\frac{x_3}{1-x_3}}=0\\ c_1\ln{\frac{x_4}{1-x_4}}+c_2\ln{\frac{x_5}{1-x_5}} +c_3\ln{\frac{x_6}{1-x_6}}=0\\ \frac{x_1(1-x_1)}{x_4(1-x_4)}=\frac{x_2(1-x_2)}{x_5(1-x_5)}=\frac{x_3(1-x_3)}{x_6(1-x_6)} \end{cases}$$ where $ c_1,c_2,c_3 $ are constants, and $x_i\in(0,1), i=1,2,3,4,5,6$. Using $y_i=1-x_i$ instead of $x_i$ makes the equations a little more succinct $$\begin{cases} \frac{c_1}{y_1}+\frac{c_2}{y_2}+\frac{c_3}{y_3}=0\\ \frac{c_1}{y_4}+\frac{c_2}{y_5}+\frac{c_3}{y_6}=0\\ c_1\ln{\frac{1-y_1}{y_1}}+c_2\ln{\frac{1-y_2}{y_2}} +c_3\ln{\frac{1-y_3}{y_3}}=0\\ c_1\ln{\frac{1-y_4}{y_4}}+c_2\ln{\frac{1-y_5}{y_5}} +c_3\ln{\frac{1-y_6}{y_6}}=0\\ \frac{y_1(1-y_1)}{y_4(1-y_4)}=\frac{y_2(1-y_2)}{y_5(1-y_5)}=\frac{y_3(1-y_3)}{y_6(1-y_6)} \end{cases}$$","How to get the exact or numerical solutions of the six elements equations below? $$\begin{cases} \frac{c_1}{1-x_1}+\frac{c_2}{1-x_2}+\frac{c_3}{1-x_3}=0\\ \frac{c_1}{1-x_4}+\frac{c_2}{1-x_5}+\frac{c_3}{1-x_6}=0\\ c_1\ln{\frac{x_1}{1-x_1}}+c_2\ln{\frac{x_2}{1-x_2}} +c_3\ln{\frac{x_3}{1-x_3}}=0\\ c_1\ln{\frac{x_4}{1-x_4}}+c_2\ln{\frac{x_5}{1-x_5}} +c_3\ln{\frac{x_6}{1-x_6}}=0\\ \frac{x_1(1-x_1)}{x_4(1-x_4)}=\frac{x_2(1-x_2)}{x_5(1-x_5)}=\frac{x_3(1-x_3)}{x_6(1-x_6)} \end{cases}$$ where $ c_1,c_2,c_3 $ are constants, and $x_i\in(0,1), i=1,2,3,4,5,6$. Using $y_i=1-x_i$ instead of $x_i$ makes the equations a little more succinct $$\begin{cases} \frac{c_1}{y_1}+\frac{c_2}{y_2}+\frac{c_3}{y_3}=0\\ \frac{c_1}{y_4}+\frac{c_2}{y_5}+\frac{c_3}{y_6}=0\\ c_1\ln{\frac{1-y_1}{y_1}}+c_2\ln{\frac{1-y_2}{y_2}} +c_3\ln{\frac{1-y_3}{y_3}}=0\\ c_1\ln{\frac{1-y_4}{y_4}}+c_2\ln{\frac{1-y_5}{y_5}} +c_3\ln{\frac{1-y_6}{y_6}}=0\\ \frac{y_1(1-y_1)}{y_4(1-y_4)}=\frac{y_2(1-y_2)}{y_5(1-y_5)}=\frac{y_3(1-y_3)}{y_6(1-y_6)} \end{cases}$$",,"['linear-algebra', 'algebra-precalculus', 'numerical-methods', 'systems-of-equations']"
98,Show that if a function $f : \mathbb{R}^n \to \mathbb{R}^m$ is differentiable with differentiable inverse then $m = n$,Show that if a function  is differentiable with differentiable inverse then,f : \mathbb{R}^n \to \mathbb{R}^m m = n,"So far I have: $\boldsymbol{f^{-1}} \circ \boldsymbol{f}(\boldsymbol{a}) = \boldsymbol{a} \implies [\boldsymbol{D}(\boldsymbol{f^{-1}}(\boldsymbol{a}) \circ \boldsymbol{f}(\boldsymbol{a}))] = I_n \implies [\boldsymbol{D}\boldsymbol{f^{-1}}(\boldsymbol{f}(\boldsymbol{a}))][\boldsymbol{D}\boldsymbol{f}(\boldsymbol{a})] = I_n $ by the chain rule. Interpreting $[\boldsymbol{D}\boldsymbol{f^{-1}}(\boldsymbol{f}(\boldsymbol{a}))]$ as the matrix composed of row-reduction operations, $[\boldsymbol{D}\boldsymbol{f}(\boldsymbol{a})]$ row-reduces to $I_n$. Now $[\boldsymbol{D}\boldsymbol{f}(\boldsymbol{a})]$ is a $m \times n$ matrix, therefore we have $m \le n$: Is this convincing? How do I make it rigorous? This looks like a similar question to Existence of an inverse to differentiable function","So far I have: $\boldsymbol{f^{-1}} \circ \boldsymbol{f}(\boldsymbol{a}) = \boldsymbol{a} \implies [\boldsymbol{D}(\boldsymbol{f^{-1}}(\boldsymbol{a}) \circ \boldsymbol{f}(\boldsymbol{a}))] = I_n \implies [\boldsymbol{D}\boldsymbol{f^{-1}}(\boldsymbol{f}(\boldsymbol{a}))][\boldsymbol{D}\boldsymbol{f}(\boldsymbol{a})] = I_n $ by the chain rule. Interpreting $[\boldsymbol{D}\boldsymbol{f^{-1}}(\boldsymbol{f}(\boldsymbol{a}))]$ as the matrix composed of row-reduction operations, $[\boldsymbol{D}\boldsymbol{f}(\boldsymbol{a})]$ row-reduces to $I_n$. Now $[\boldsymbol{D}\boldsymbol{f}(\boldsymbol{a})]$ is a $m \times n$ matrix, therefore we have $m \le n$: Is this convincing? How do I make it rigorous? This looks like a similar question to Existence of an inverse to differentiable function",,"['real-analysis', 'linear-algebra', 'multivariable-calculus']"
99,Gram-Schmidt for uncountable sets?,Gram-Schmidt for uncountable sets?,,"I know that Gram Schmidt can be applied to countable linear independent sets on Hilbert spaces, but what happens if we apply it on uncountable sets? Obviously this process has to fail then (at least sometimes) cause we cannot have uncountable orthonormal systems in separable Hilbert spaces. But where exactly does it crack? And is there any structure in the set we construct via Gram Schmidt for uncountable systems?","I know that Gram Schmidt can be applied to countable linear independent sets on Hilbert spaces, but what happens if we apply it on uncountable sets? Obviously this process has to fail then (at least sometimes) cause we cannot have uncountable orthonormal systems in separable Hilbert spaces. But where exactly does it crack? And is there any structure in the set we construct via Gram Schmidt for uncountable systems?",,"['linear-algebra', 'functional-analysis']"
