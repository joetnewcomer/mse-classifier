,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Probability of probability with certainty?,Probability of probability with certainty?,,"Let $x$ be a uniformly distributed variable across the interval [0, 0.1], inclusive, where x represents the probability of a particular event occurring during a trial. If $528174$ trials occur, and in each of these trials the event does not occur, what is the smallest real $y$ so that that $x < y$ with at least $95%$ certainty?","Let be a uniformly distributed variable across the interval [0, 0.1], inclusive, where x represents the probability of a particular event occurring during a trial. If trials occur, and in each of these trials the event does not occur, what is the smallest real so that that with at least certainty?",x 528174 y x < y 95%,['probability']
1,Checking whether the minimal sufficient statistic is complete or not for negative exponential distribution,Checking whether the minimal sufficient statistic is complete or not for negative exponential distribution,,"Let $X_1, X_2..., X_n$ follows iid negative exponential distribution with pdf $$f(x) = \frac{1}{\theta^2} \: e^{-\frac{(x-\theta)}{\theta^2}} \: \: I_{(x>\theta)} $$ I have to show whether the minimal sufficient statistic for this pdf is complete or not? I have found that the minimal sufficient statistic is $T=\left( X_{(1)}, \sum_{i=1}^{n} (X_i - X_{(1)}) \right)$ . If this minimal sufficient statistic is not complete then there exists a function $h(T)$ of the minimal sufficient statistic such that $E_\theta [h(T)] =0$ for all $\theta>0$ where $h(T)$ is not identically zero. Is this minimal sufficient complete or not? How can I find the function $h(T)$ of the minimal sufficient statistic? Note that, $X_{(1)} $ is the first order statistic i.e., $min\{X_1,..X_n\}$ . I have calculated the pdf of $X_{(1)}$ . Let $Y= X_{(1)}$ then the pdf of $Y$ is given by, $$ f(y) = \frac{n}{\theta^2} \: e^{-\frac{n(y-\theta)}{\theta^2}} \: \: I_{(y>\theta)} $$ I have also calculated $$E(X)= \theta^2 + \theta $$ and $$E(Y) = \frac{\theta^2}{n} + \theta$$ Now, please help me to find out $h(T)$ for which $E_\theta[h(T)] = 0$ for all $\theta>0$ if the minimal sufficient statistic is not complete.","Let follows iid negative exponential distribution with pdf I have to show whether the minimal sufficient statistic for this pdf is complete or not? I have found that the minimal sufficient statistic is . If this minimal sufficient statistic is not complete then there exists a function of the minimal sufficient statistic such that for all where is not identically zero. Is this minimal sufficient complete or not? How can I find the function of the minimal sufficient statistic? Note that, is the first order statistic i.e., . I have calculated the pdf of . Let then the pdf of is given by, I have also calculated and Now, please help me to find out for which for all if the minimal sufficient statistic is not complete.","X_1, X_2..., X_n f(x) = \frac{1}{\theta^2} \: e^{-\frac{(x-\theta)}{\theta^2}} \: \: I_{(x>\theta)}  T=\left( X_{(1)}, \sum_{i=1}^{n} (X_i - X_{(1)}) \right) h(T) E_\theta [h(T)] =0 \theta>0 h(T) h(T) X_{(1)}  min\{X_1,..X_n\} X_{(1)} Y= X_{(1)} Y  f(y) = \frac{n}{\theta^2} \: e^{-\frac{n(y-\theta)}{\theta^2}} \: \: I_{(y>\theta)}  E(X)= \theta^2 + \theta  E(Y) = \frac{\theta^2}{n} + \theta h(T) E_\theta[h(T)] = 0 \theta>0","['statistics', 'statistical-inference', 'order-statistics']"
2,"Bias and MSE of $\hat{\theta} = \min(X_1, \ldots, X_n)$",Bias and MSE of,"\hat{\theta} = \min(X_1, \ldots, X_n)","Let $X_1, ... X_n$ iid with pdf given by $$p_{\theta, r} = r \theta^r x^{- (r+1)} \mathbb{1}\{x \geq \theta\}$$ for $\theta > 0$ , and some $r > 2$ that is known. Then $\hat{\theta} = \min(X_1, \ldots, X_n) = X_{(1)}$ . I want to determine the bias and MSE of $\hat{\theta}$ , so I need the pdf of $\hat{\theta}$ . If my calculations are correct, the pdf of $\hat{\theta}$ is given by: $$f_{X_{(1)}} = n(r+1)r^n \theta^{rn} x^{-n(r+1) - 1} \mathbb{1}\{x \geq \theta\}.$$ Wondering if this pdf is correct, and how one would calculate the bias and MSE using this variance? I know that the bias is given by $E[\hat{\theta}] - \theta$ , but I end up with a complicated expression, so I believe I am doing something wrong.","Let iid with pdf given by for , and some that is known. Then . I want to determine the bias and MSE of , so I need the pdf of . If my calculations are correct, the pdf of is given by: Wondering if this pdf is correct, and how one would calculate the bias and MSE using this variance? I know that the bias is given by , but I end up with a complicated expression, so I believe I am doing something wrong.","X_1, ... X_n p_{\theta, r} = r \theta^r x^{- (r+1)} \mathbb{1}\{x \geq \theta\} \theta > 0 r > 2 \hat{\theta} = \min(X_1, \ldots, X_n) = X_{(1)} \hat{\theta} \hat{\theta} \hat{\theta} f_{X_{(1)}} = n(r+1)r^n \theta^{rn} x^{-n(r+1) - 1} \mathbb{1}\{x \geq \theta\}. E[\hat{\theta}] - \theta","['probability', 'statistics', 'probability-distributions', 'mean-square-error']"
3,About equivalent form of fisher information,About equivalent form of fisher information,,"i'm confused about the hypothesies of the following statement: if $\log f(x,\theta)$ is twice differentiable respect $\theta$ , and under certain regularity conditions, then the Fisher information may also be written as $$\mathbb{E} \big[ \big(\frac{\partial}{\partial \theta}\log f(X,\theta)\big)^2 \big]=-\mathbb{E}\big[\frac{\partial^2}{\partial\theta^2}\log f(X,\theta) \big]$$ where $X:\Omega\rightarrow\mathbb{R}$ is a r.v. and $(\Omega,\mathcal{F},\mathbb{P})$ a probability space which are these regularity conditions? surely $\log f(x,\theta)$ must be twice differentiable and both the function in the equality must be integrable but taking the demonstration: $$\mathbb{E}\big[\frac{\partial^2}{\partial\theta^2}\log f(X,\theta) \big]=\int_{\mathbb{R}}f(x,\theta)* \frac{\partial^2}{\partial\theta^2}\log f(x,\theta) dx= ... \\ ... = \int_{\mathbb{R}}\frac{\partial^2}{\partial \theta^2}f(x,\theta)dx - \int_{\mathbb{R}}\frac{\big( \frac{\partial}{\partial \theta}f(x,\theta) \big)^2}{f(x,\theta)}dx$$ now $$\int_{\mathbb{R}}\frac{\big( \frac{\partial}{\partial \theta}f(x,\theta) \big)^2}{f(x,\theta)}dx = \int_{\mathbb{R}}\big( \frac{ \frac{\partial}{\partial \theta}f(x,\theta)}{f(x,\theta)}\big)^2*f(x,\theta)dx=\mathbb{E} \big[ \big(\frac{\partial}{\partial \theta}\log f(X,\theta)\big)^2 \big]$$ the thesis is obtained having $$\int_{\mathbb{R}}\frac{\partial^2}{\partial \theta^2}f(x,\theta)dx=0$$ to prove the previous equality i think is sufficent that $$\frac{\partial}{\partial \theta}f(x,\theta)\space \mathrm{and} \space f(x,\theta)$$ both satisfy the theorem of exchange of integral and derivative. Do you agree? moreover there are some way to relax the hypothesis?","i'm confused about the hypothesies of the following statement: if is twice differentiable respect , and under certain regularity conditions, then the Fisher information may also be written as where is a r.v. and a probability space which are these regularity conditions? surely must be twice differentiable and both the function in the equality must be integrable but taking the demonstration: now the thesis is obtained having to prove the previous equality i think is sufficent that both satisfy the theorem of exchange of integral and derivative. Do you agree? moreover there are some way to relax the hypothesis?","\log f(x,\theta) \theta \mathbb{E} \big[ \big(\frac{\partial}{\partial \theta}\log f(X,\theta)\big)^2 \big]=-\mathbb{E}\big[\frac{\partial^2}{\partial\theta^2}\log f(X,\theta) \big] X:\Omega\rightarrow\mathbb{R} (\Omega,\mathcal{F},\mathbb{P}) \log f(x,\theta) \mathbb{E}\big[\frac{\partial^2}{\partial\theta^2}\log f(X,\theta) \big]=\int_{\mathbb{R}}f(x,\theta)* \frac{\partial^2}{\partial\theta^2}\log f(x,\theta) dx= ... \\ ... = \int_{\mathbb{R}}\frac{\partial^2}{\partial \theta^2}f(x,\theta)dx - \int_{\mathbb{R}}\frac{\big( \frac{\partial}{\partial \theta}f(x,\theta) \big)^2}{f(x,\theta)}dx \int_{\mathbb{R}}\frac{\big( \frac{\partial}{\partial \theta}f(x,\theta) \big)^2}{f(x,\theta)}dx = \int_{\mathbb{R}}\big( \frac{ \frac{\partial}{\partial \theta}f(x,\theta)}{f(x,\theta)}\big)^2*f(x,\theta)dx=\mathbb{E} \big[ \big(\frac{\partial}{\partial \theta}\log f(X,\theta)\big)^2 \big] \int_{\mathbb{R}}\frac{\partial^2}{\partial \theta^2}f(x,\theta)dx=0 \frac{\partial}{\partial \theta}f(x,\theta)\space \mathrm{and} \space f(x,\theta)","['probability', 'statistics', 'statistical-inference', 'fisher-information']"
4,Use the Chernoff Inequality to bound a probability.,Use the Chernoff Inequality to bound a probability.,,"A coin is equally likely to be either $B_{1/3}$ (Bernoulli distributed with $p=1/3$ ) or $B_{2/3}$ . To figure out the bias, we toss the coin 99 times and declare $B_{1/3}$ if the number of heads is less than 49.5 and $B_{2/3}$ otherwise. Bound the error probability using the Chernoff bound. The Chernoff inequality or bound states: $P(X\geq a) = P(e^{tX}\geq e^{ta}) \leq \frac{E[e^{tX}]}{e^{ta}}$ In the special case of a binomial distribrution the lower bound of the Chernoff inequality is given by: $P(X\leq(1-\delta)\mu) = e^{-\frac{\delta^2}{2}\mu}$ From the question I understand that we are looking for the bound prob that (1) choosing $B_{1/3}$ is wrong if we see 49.5 or less heads after the 99 tosses. (2) Or we choose $B_{2/3}$ if we see 49.5 or more heads after the 99 tosses. To calculate the first part (1): I tried using the binomial case of the Chernoff inequality. First, calculating $\delta$ $$ P(X\leq(1-\delta)\mu) = P(49.5 \leq (1-\delta)E(X_{2/3}))\\ E(X_{2/3})=\frac{2}{3}99 = 66\\  \delta = 1-\frac{49.5}{66} = 0.25\\ $$ Then I plugged delta into the inequality to get the lower bound probability: $$ P(X\leq(1-0.25)66)\leq e^{-\frac{0.25^2}{2}66} =  0.1271 $$ for the second part (2): first we need to use the binomial Chernoff upper bound inequality, which is: $P(X\geq(1+\delta)\mu) = e^{-\frac{\delta^2}{2+\delta}\mu}$ Again I calculte delta: $$ P(X\geq(1+\delta)\mu) = P(49.5 \geq (1-\delta)E(X_{1/3}))\\ E(X_{1/3})=\frac{1}{3}99 = 33\\  \delta = \frac{49.5}{33}-1 = 0.5\\ $$ Then I plugged delta into the inequality to get the upper bound probability: $$ P(X\geq(1+0.5)33)\leq e^{-\frac{0.5^2}{2.5}33} =  0.03688 $$ Finally, I add both upper and lower bound $P(wrong) = P(X\geq49.5)+P(X\leq49.5) = 0.1640189 $ However, this answer is wrong, and I am not sure what I am doing wrong. Maybe is the interpretation of the question? Any help or hint would be very appreciated!","A coin is equally likely to be either (Bernoulli distributed with ) or . To figure out the bias, we toss the coin 99 times and declare if the number of heads is less than 49.5 and otherwise. Bound the error probability using the Chernoff bound. The Chernoff inequality or bound states: In the special case of a binomial distribrution the lower bound of the Chernoff inequality is given by: From the question I understand that we are looking for the bound prob that (1) choosing is wrong if we see 49.5 or less heads after the 99 tosses. (2) Or we choose if we see 49.5 or more heads after the 99 tosses. To calculate the first part (1): I tried using the binomial case of the Chernoff inequality. First, calculating Then I plugged delta into the inequality to get the lower bound probability: for the second part (2): first we need to use the binomial Chernoff upper bound inequality, which is: Again I calculte delta: Then I plugged delta into the inequality to get the upper bound probability: Finally, I add both upper and lower bound However, this answer is wrong, and I am not sure what I am doing wrong. Maybe is the interpretation of the question? Any help or hint would be very appreciated!","B_{1/3} p=1/3 B_{2/3} B_{1/3} B_{2/3} P(X\geq a) = P(e^{tX}\geq e^{ta}) \leq \frac{E[e^{tX}]}{e^{ta}} P(X\leq(1-\delta)\mu) = e^{-\frac{\delta^2}{2}\mu} B_{1/3} B_{2/3} \delta 
P(X\leq(1-\delta)\mu) = P(49.5 \leq (1-\delta)E(X_{2/3}))\\
E(X_{2/3})=\frac{2}{3}99 = 66\\ 
\delta = 1-\frac{49.5}{66} = 0.25\\
 
P(X\leq(1-0.25)66)\leq e^{-\frac{0.25^2}{2}66} =  0.1271
 P(X\geq(1+\delta)\mu) = e^{-\frac{\delta^2}{2+\delta}\mu} 
P(X\geq(1+\delta)\mu) = P(49.5 \geq (1-\delta)E(X_{1/3}))\\
E(X_{1/3})=\frac{1}{3}99 = 33\\ 
\delta = \frac{49.5}{33}-1 = 0.5\\
 
P(X\geq(1+0.5)33)\leq e^{-\frac{0.5^2}{2.5}33} =  0.03688
 P(wrong) = P(X\geq49.5)+P(X\leq49.5) = 0.1640189 ","['probability', 'statistics', 'random-variables', 'binomial-distribution']"
5,Why the pdf always exists in statistics textbooks?,Why the pdf always exists in statistics textbooks?,,"I just jumped into statistics, but what confuses me is that all textbooks seem to assume existence of the probability densify function (pdf) of a R.V.? For instance, at the beginning, they say there is a dominating measure (Lebesgue measure or counting measure, depending on continuous or discrete cases), and a probability measure. Then they implicitly use the Radon-Nikodym theorem to write the pdf as the R-N derivative. From real analysis, I know the condition that R-N theorem holds is that the probability measure must be absolutely continuous w.r.t. the dominating measure. But those textbooks do not discuss it. So my question is why we can use the R-N theorem while no discussion of absolute continuity between the two measures? Is there some point I missed? Thanks!","I just jumped into statistics, but what confuses me is that all textbooks seem to assume existence of the probability densify function (pdf) of a R.V.? For instance, at the beginning, they say there is a dominating measure (Lebesgue measure or counting measure, depending on continuous or discrete cases), and a probability measure. Then they implicitly use the Radon-Nikodym theorem to write the pdf as the R-N derivative. From real analysis, I know the condition that R-N theorem holds is that the probability measure must be absolutely continuous w.r.t. the dominating measure. But those textbooks do not discuss it. So my question is why we can use the R-N theorem while no discussion of absolute continuity between the two measures? Is there some point I missed? Thanks!",,"['real-analysis', 'probability', 'probability-theory', 'statistics', 'random-variables']"
6,Mapping a single uniform random variable to a uniform product random variable,Mapping a single uniform random variable to a uniform product random variable,,"I have a single random variable $X\sim \operatorname{Uniform}(0,1)$ . I would like a function $f: \mathbb{R} \to \mathbb{R}$ such that $F = f(X)$ has the distribution \begin{equation} p_F(y) = \begin{cases} {(-\log y)^n \over n!} & \text{if } 0 < y \leq 1 \\ 0 & \text{otherwise} \end{cases} \end{equation} for some given $n$ (i.e., the product distribution for $n+1$ independent random variables). Does such an $f$ exist? If so what is it? I tried reverse-engineering one directly but became hopelessly tangled in incomplete gamma functions and the like. Thanks!","I have a single random variable . I would like a function such that has the distribution for some given (i.e., the product distribution for independent random variables). Does such an exist? If so what is it? I tried reverse-engineering one directly but became hopelessly tangled in incomplete gamma functions and the like. Thanks!","X\sim \operatorname{Uniform}(0,1) f: \mathbb{R} \to \mathbb{R} F = f(X) \begin{equation}
p_F(y) = \begin{cases}
{(-\log y)^n \over n!} & \text{if } 0 < y \leq 1 \\
0 & \text{otherwise}
\end{cases}
\end{equation} n n+1 f","['probability', 'statistics', 'probability-distributions']"
7,The Frobenius norm,The Frobenius norm,,"Let P be a non zero projector. Is ( $||P||_F\geq 1$ with equality if and only if P is an orthogonal projector) true? My question is about the the Frobenius norm of P, is this true $||P||_F\geq 1$ with equality if and only if P is an orthogonal projector. I know it is happen in $2$ -norm.","Let P be a non zero projector. Is ( with equality if and only if P is an orthogonal projector) true? My question is about the the Frobenius norm of P, is this true with equality if and only if P is an orthogonal projector. I know it is happen in -norm.",||P||_F\geq 1 ||P||_F\geq 1 2,"['linear-algebra', 'statistics']"
8,Intuition behind sub-Gaussian and sub-exponential random variables,Intuition behind sub-Gaussian and sub-exponential random variables,,"As I was reading about Gaussian random variable (RV) and Chernoff Bounds, I came across sub-Gaussian & sub-exponential RVs. After much analysis, I still couldn't figure what purpose sub-Gaussian & sub-exponential serve in the broader realm of statistics. So my question is: How do sub-Gaussian & sub-exponential RVs relate to Gaussian RV in an intuitive sense? How should I decide when sub-Gaussian/sub-exponential RVs are suitable for a particular scenario? An answer combining intuition and mathematics would be helpful.","As I was reading about Gaussian random variable (RV) and Chernoff Bounds, I came across sub-Gaussian & sub-exponential RVs. After much analysis, I still couldn't figure what purpose sub-Gaussian & sub-exponential serve in the broader realm of statistics. So my question is: How do sub-Gaussian & sub-exponential RVs relate to Gaussian RV in an intuitive sense? How should I decide when sub-Gaussian/sub-exponential RVs are suitable for a particular scenario? An answer combining intuition and mathematics would be helpful.",,"['probability', 'probability-theory', 'statistics', 'random-variables', 'concentration-of-measure']"
9,How to prove that UMVUE does not exist for non-constant function?,How to prove that UMVUE does not exist for non-constant function?,,"This is problem 1.9 of chapter 2 from Lehmann and Casella: Let $X_1, \dots, X_n$ be a sample (assuming i.i.d.) from a discrete distribution which assigns probability $1/3$ to each of the points $\theta-1, \theta, \theta+1$ . $\theta$ ranges over all integers. Prove that no non-constant function of $\theta$ has a UMVUE. My approach: Assume the simplest case $n=1$ . We can use the relationship between UMVUE and unbiased estimator of zero to tackle this issue. Suppose $T(x)$ is a UMVUE for $g(\theta)$ for some function $g$ . Then for an unbiased estimator of zero $U(x)$ , we have $$ E_\theta U = 1/3(U(\theta-1) + U(\theta) + U(\theta+1)) = 0, \ \forall \theta \in \mathbb{Z}, $$ i.e., $$ U(\theta-1) + U(\theta) + U(\theta+1) = 0, \ \forall \theta \in \mathbb{Z}. $$ Then we have $$ U(\theta-1) = U(\theta + 2), \ \forall \theta \in \mathbb{Z}. $$ Now we use the fact that if $T$ is UMVUE, then for any $U$ that is an unbiased estimator of zero, we have $$ E_\theta[TU] = 0. $$ This implies that $TU$ is also an unbiased estimator of zero. Hence, for any unbiased estimator of zero $U$ : $$ T(\theta-1)U(\theta-1) = T(\theta + 2)U(\theta+2), \ \forall \theta \in \mathbb{Z}. $$ This means that $$ T(\theta-1) = T(\theta+2), \ \forall \theta \in \mathbb{Z}. $$ This gives us $$ 1/3(T(\theta-1) + T(\theta) + T(\theta+1)) = 1/3(T(\theta) + T(\theta+1) + T(\theta+2)), \ \forall \theta \in \mathbb{Z}. $$ Now the RHS is $g(\theta)$ while the LHS is $g(\theta+1)$ . Therefore, $$ g(\theta) = g(\theta + 1), \ \forall \theta \in \mathbb{Z}. $$ This shows that $g$ is a constant function of $\theta$ . That is, UMVUE does not exist for non-constant function of $\theta$ . Now the part I am stuck at is how do I prove this for $n > 1$ ? I don't see an immediate induction-type argument or something else and carrying out direct derivation through brute force seems extremely complicated. Any hint would be helpful. Thank you very much!","This is problem 1.9 of chapter 2 from Lehmann and Casella: Let be a sample (assuming i.i.d.) from a discrete distribution which assigns probability to each of the points . ranges over all integers. Prove that no non-constant function of has a UMVUE. My approach: Assume the simplest case . We can use the relationship between UMVUE and unbiased estimator of zero to tackle this issue. Suppose is a UMVUE for for some function . Then for an unbiased estimator of zero , we have i.e., Then we have Now we use the fact that if is UMVUE, then for any that is an unbiased estimator of zero, we have This implies that is also an unbiased estimator of zero. Hence, for any unbiased estimator of zero : This means that This gives us Now the RHS is while the LHS is . Therefore, This shows that is a constant function of . That is, UMVUE does not exist for non-constant function of . Now the part I am stuck at is how do I prove this for ? I don't see an immediate induction-type argument or something else and carrying out direct derivation through brute force seems extremely complicated. Any hint would be helpful. Thank you very much!","X_1, \dots, X_n 1/3 \theta-1, \theta, \theta+1 \theta \theta n=1 T(x) g(\theta) g U(x) 
E_\theta U = 1/3(U(\theta-1) + U(\theta) + U(\theta+1)) = 0, \ \forall \theta \in \mathbb{Z},
 
U(\theta-1) + U(\theta) + U(\theta+1) = 0, \ \forall \theta \in \mathbb{Z}.
 
U(\theta-1) = U(\theta + 2), \ \forall \theta \in \mathbb{Z}.
 T U 
E_\theta[TU] = 0.
 TU U 
T(\theta-1)U(\theta-1) = T(\theta + 2)U(\theta+2), \ \forall \theta \in \mathbb{Z}.
 
T(\theta-1) = T(\theta+2), \ \forall \theta \in \mathbb{Z}.
 
1/3(T(\theta-1) + T(\theta) + T(\theta+1)) = 1/3(T(\theta) + T(\theta+1) + T(\theta+2)), \ \forall \theta \in \mathbb{Z}.
 g(\theta) g(\theta+1) 
g(\theta) = g(\theta + 1), \ \forall \theta \in \mathbb{Z}.
 g \theta \theta n > 1","['statistics', 'estimation', 'parameter-estimation']"
10,"Nontrivial lower-bound for $\lim\sup_n P(|X_n| / (Y_n^2+Z_n^2) \le t) $ if $X_n,Y_n,Z_n \to N(0,1)$ in distribution",Nontrivial lower-bound for  if  in distribution,"\lim\sup_n P(|X_n| / (Y_n^2+Z_n^2) \le t)  X_n,Y_n,Z_n \to N(0,1)","Let $(X_n)_n$ , $(Y_n)_n$ , $(Z_n)_n$ be sequence of random variables (no assumptions of independence whatsoever). Suppose that $Y_n \to N(0, 1)$ , $Y_n \to N(0, 1)$ , and $Z_n \to N(0,1)$ in distribution. Let $\epsilon \ge 0$ . Question. What is a nontrivial lower-bound for $p := \lim\sup_n P(|X_n| / (Y_n^2+Z_n^2) \le \epsilon)$ ? Note. If I had $Y_n \to y$ and $Z_n \to z$ in probability (for constants $y,z \in \mathbb R$ ), then I could use Slutsky's theorem to get $p = P(|N(0,1)| \le \epsilon)$ , and then concentration arguments would give $p \ge 1 - 2e^{-\epsilon^2/2}$ .","Let , , be sequence of random variables (no assumptions of independence whatsoever). Suppose that , , and in distribution. Let . Question. What is a nontrivial lower-bound for ? Note. If I had and in probability (for constants ), then I could use Slutsky's theorem to get , and then concentration arguments would give .","(X_n)_n (Y_n)_n (Z_n)_n Y_n \to N(0, 1) Y_n \to N(0, 1) Z_n \to N(0,1) \epsilon \ge 0 p := \lim\sup_n P(|X_n| / (Y_n^2+Z_n^2) \le \epsilon) Y_n \to y Z_n \to z y,z \in \mathbb R p = P(|N(0,1)| \le \epsilon) p \ge 1 - 2e^{-\epsilon^2/2}","['probability', 'probability-theory', 'statistics', 'law-of-large-numbers', 'concentration-of-measure']"
11,Glivenko-Cantelli theorem proof,Glivenko-Cantelli theorem proof,,"Glivenko-Cantelli theorem states that: $$\sup_{x\in \Bbb R}|F_n(x)-F(x)|\to 0 \quad\text{almost surely}\,,$$ where $F_n(x)$ is an empirical CDF. There is a LINK with a proof for discrete random variable, but I don't really get this line: $$\sup_{x\in \Bbb R}|F_n(x)-F(x)|\leq \max_{j \in \{1,\ldots,m\}}|F_n (x_j)-F(x_j)| + \frac{1}{m}$$ Could anyone please explain me why that is true?","Glivenko-Cantelli theorem states that: where is an empirical CDF. There is a LINK with a proof for discrete random variable, but I don't really get this line: Could anyone please explain me why that is true?","\sup_{x\in \Bbb R}|F_n(x)-F(x)|\to 0 \quad\text{almost surely}\,, F_n(x) \sup_{x\in \Bbb R}|F_n(x)-F(x)|\leq \max_{j \in \{1,\ldots,m\}}|F_n (x_j)-F(x_j)| + \frac{1}{m}","['probability-theory', 'statistics', 'proof-explanation']"
12,Confidence Intervals over time on expectation of having covid$-19$?,Confidence Intervals over time on expectation of having covid?,-19,"I have the random indicator variable $X$ , which takes $1$ if the person has covid and $0$ otherwise. I have calculated $Prob(X_i = 1)$ (so the ${E[X_i]}$ or expectation of $X_i$ ) over the course of $137$ days. The next question is to ""Derive confidence intervals for your estimates of $Prob(X_0 = 1)$ using the CLT and Chebyshev Inequality and plot these confidence intervals vs time, e.g. as error bars about the estimates of $Prob(X_0 = 1)$ ."" So I got my array of $E[X_i]$ values, and for each day I calculated the mean and standard deviation based on that day and all previous days and then did my calculations. This leads to $95\%$ confidence intervals that seem to grow and grow ( graph ) Is this correct? what does this imply? Is it not incorrect to create confidence intervals on this time based sampling anyways???","I have the random indicator variable , which takes if the person has covid and otherwise. I have calculated (so the or expectation of ) over the course of days. The next question is to ""Derive confidence intervals for your estimates of using the CLT and Chebyshev Inequality and plot these confidence intervals vs time, e.g. as error bars about the estimates of ."" So I got my array of values, and for each day I calculated the mean and standard deviation based on that day and all previous days and then did my calculations. This leads to confidence intervals that seem to grow and grow ( graph ) Is this correct? what does this imply? Is it not incorrect to create confidence intervals on this time based sampling anyways???",X 1 0 Prob(X_i = 1) {E[X_i]} X_i 137 Prob(X_0 = 1) Prob(X_0 = 1) E[X_i] 95\%,"['statistics', 'confidence-interval']"
13,"Difference between random variables, random samples, and observations","Difference between random variables, random samples, and observations",,"Here there is my (difficult to be phrased without looking silly) question: is there an actual difference between a random variable, a random sample, and an observation? [For what below, the books I am reading are De Groot and Schervish's ""Probability and Statistics"" and Spiegel's ""Statistics"" (Schaum's)] Before jumping to the conclusion that I am missing some very basic stuff (which could be, of course), give me a second to explain my problem and read below the example, which could lead to a slightly more precise formulation of the question. Example Given a population of 100 students in a campus, we want to gather some information concerning their heights, such as mean, variance, etc. Thus, on Monday we get a sample of 5 students, on Tuesday we get a sample of other 5 students, and so on until Sunday: we end up having 7 samples, with 5 observations each. In the example above, for what would we use the notation $X_i$ ? I know that notation is just notation, but my point is that $X_i$ is used for many things in statistics and sometime I feel I lose sight of the actual object behind the notation. What I mean is that, by taking the example, my first feeling is that the r.v. is the height, every student is an observation, and we have the 7 samples (this would be my feeling after reading Spiegel). But then, after De Groot and Schervish, I get the feeling that a student is a r.v. in its own right, call it, the $X_i$ random variable. What above becomes even muddier to me when we move to flipping coins (a favourite example of books): if we flip one coin $n$ times and we do it $k$ times to get if it is fair, what is the r.v.? I would say that we have just one r.v., which is the behavior of the coin; also, we have $k$ samples of the behavior of this one coin, with $n$ observations each. Thus, $X_i$ would be reserved for the sample? However, it seems to me that quite often (always), one observation (say the $i$ th) of a head or tail of that one coin is considered a r.v. in itself, thus getting the $X_i$ notation. If you are wondering why this question, simply, I want to get this, because I have the feeling the confusion I have is hampering my possibility of gettings some things right in statistics. Below, some links that - unfortunately - did not help me in getting this right: Random Variable vs data vs random sample https://stats.stackexchange.com/questions/239500/what-is-the-difference-between-random-variable-and-random-sample Concerning the second link, the comment of hanugm to the accepted answer (which has been left unanswered) really summarizes what my problem is all about.","Here there is my (difficult to be phrased without looking silly) question: is there an actual difference between a random variable, a random sample, and an observation? [For what below, the books I am reading are De Groot and Schervish's ""Probability and Statistics"" and Spiegel's ""Statistics"" (Schaum's)] Before jumping to the conclusion that I am missing some very basic stuff (which could be, of course), give me a second to explain my problem and read below the example, which could lead to a slightly more precise formulation of the question. Example Given a population of 100 students in a campus, we want to gather some information concerning their heights, such as mean, variance, etc. Thus, on Monday we get a sample of 5 students, on Tuesday we get a sample of other 5 students, and so on until Sunday: we end up having 7 samples, with 5 observations each. In the example above, for what would we use the notation ? I know that notation is just notation, but my point is that is used for many things in statistics and sometime I feel I lose sight of the actual object behind the notation. What I mean is that, by taking the example, my first feeling is that the r.v. is the height, every student is an observation, and we have the 7 samples (this would be my feeling after reading Spiegel). But then, after De Groot and Schervish, I get the feeling that a student is a r.v. in its own right, call it, the random variable. What above becomes even muddier to me when we move to flipping coins (a favourite example of books): if we flip one coin times and we do it times to get if it is fair, what is the r.v.? I would say that we have just one r.v., which is the behavior of the coin; also, we have samples of the behavior of this one coin, with observations each. Thus, would be reserved for the sample? However, it seems to me that quite often (always), one observation (say the th) of a head or tail of that one coin is considered a r.v. in itself, thus getting the notation. If you are wondering why this question, simply, I want to get this, because I have the feeling the confusion I have is hampering my possibility of gettings some things right in statistics. Below, some links that - unfortunately - did not help me in getting this right: Random Variable vs data vs random sample https://stats.stackexchange.com/questions/239500/what-is-the-difference-between-random-variable-and-random-sample Concerning the second link, the comment of hanugm to the accepted answer (which has been left unanswered) really summarizes what my problem is all about.",X_i X_i X_i n k k n X_i i X_i,"['probability', 'statistics', 'random-variables', 'sampling']"
14,Find exact confidence interval for uniform distribution,Find exact confidence interval for uniform distribution,,"In my homework I have $X_1,...,X_n$ which are all uniformly distributed on $(0,\theta)$ I have concluded that the $MLE=\hat{\theta}=max(X_1,...,X_n)$ because: $L_x(\theta)= \frac{1}{\theta^n}$ so the likelihood is a decreasing function. However we also know that $\theta > \max\left(X_1,...,X_n\right)$ , hence $\hat{\theta}=\max\left(X_1,...,X_n\right)$ But I need to find an exact $95$ % confidence interval for $\theta$ . I think the book wants me to find a pivot I have tried to set $P\left(X \in \left(0,X_n\right)\right)=\left(\frac{X_n}{\theta}\right)^n$ for some $\theta>X_n$ Therefore $P(X \in (X_n,\theta))=1-\left(\frac{X_n}{\theta}\right)^n=0.95$ which gives me $\theta=\frac{X_n}{0.05^{1/n}}$ However I am pretty sure this is not the way to go and it is not a standard method. There should be some more standard method but using $L,\ell_X, \ell_X'=0$ just gives me MLE=0 which is of no use Any help/hint would be appreciated","In my homework I have which are all uniformly distributed on I have concluded that the because: so the likelihood is a decreasing function. However we also know that , hence But I need to find an exact % confidence interval for . I think the book wants me to find a pivot I have tried to set for some Therefore which gives me However I am pretty sure this is not the way to go and it is not a standard method. There should be some more standard method but using just gives me MLE=0 which is of no use Any help/hint would be appreciated","X_1,...,X_n (0,\theta) MLE=\hat{\theta}=max(X_1,...,X_n) L_x(\theta)= \frac{1}{\theta^n} \theta > \max\left(X_1,...,X_n\right) \hat{\theta}=\max\left(X_1,...,X_n\right) 95 \theta P\left(X \in \left(0,X_n\right)\right)=\left(\frac{X_n}{\theta}\right)^n \theta>X_n P(X \in (X_n,\theta))=1-\left(\frac{X_n}{\theta}\right)^n=0.95 \theta=\frac{X_n}{0.05^{1/n}} L,\ell_X, \ell_X'=0","['statistics', 'uniform-distribution', 'confidence-interval']"
15,Statistics 101: T test vs Z test. Sample proportion vs Sample mean,Statistics 101: T test vs Z test. Sample proportion vs Sample mean,,"Sorry, I'm just on Khan academy and can't seem to grasp the essence of statistics. Hope to find some help out here. Both are samples, but why when looking for confidence interval of a: sample proportion, we take $\hat{p} \pm Z^*\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}$ sample mean, we take take $\bar{x} \pm t^*{\frac{S}{\sqrt{n}}}$ What I understand: That both are trying to determine the confidence level that Population mean falls between an interval. I can see from google that: Z-scores are based on your knowledge about the population's standard deviation and mean.  T-scores are used when the conversion is made without knowledge of the population standard deviation and mean. But I don't think I properly understand it: what throws me off the statement is that the confidence interval formula $\bar{x} \pm t^*{\frac{S}{\sqrt{n}}}$ still uses S(sampling standard deviation). Question: Why can't we just take $\bar{x} \pm Z^*\sqrt{\frac{\bar{x}(1-\bar{x})}{n}}$ as we did $\hat{p} \pm Z^*\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}$ ? Both are samples, why does one estimate with $\hat{p}(1-\hat{p})$ while the other uses S (sample standard deviation)? What is the difference between sample proportion and sample mean? Is the former Bernuli distribution and another not? Is the sample proportion confidence interval of a single sample VS sample mean which is the average of multiple samples ? Both are samples, why do we n-1 during t test, but not z test? I get that samples need to -1 to compensate for accuracy, but why didn't we do it for z tests? Don't feel pressure to answer all the questions, just what you can, perhaps the collective can help me out of this rut. Appreciate it your time reading this!","Sorry, I'm just on Khan academy and can't seem to grasp the essence of statistics. Hope to find some help out here. Both are samples, but why when looking for confidence interval of a: sample proportion, we take sample mean, we take take What I understand: That both are trying to determine the confidence level that Population mean falls between an interval. I can see from google that: Z-scores are based on your knowledge about the population's standard deviation and mean.  T-scores are used when the conversion is made without knowledge of the population standard deviation and mean. But I don't think I properly understand it: what throws me off the statement is that the confidence interval formula still uses S(sampling standard deviation). Question: Why can't we just take as we did ? Both are samples, why does one estimate with while the other uses S (sample standard deviation)? What is the difference between sample proportion and sample mean? Is the former Bernuli distribution and another not? Is the sample proportion confidence interval of a single sample VS sample mean which is the average of multiple samples ? Both are samples, why do we n-1 during t test, but not z test? I get that samples need to -1 to compensate for accuracy, but why didn't we do it for z tests? Don't feel pressure to answer all the questions, just what you can, perhaps the collective can help me out of this rut. Appreciate it your time reading this!",\hat{p} \pm Z^*\sqrt{\frac{\hat{p}(1-\hat{p})}{n}} \bar{x} \pm t^*{\frac{S}{\sqrt{n}}} \bar{x} \pm t^*{\frac{S}{\sqrt{n}}} \bar{x} \pm Z^*\sqrt{\frac{\bar{x}(1-\bar{x})}{n}} \hat{p} \pm Z^*\sqrt{\frac{\hat{p}(1-\hat{p})}{n}} \hat{p}(1-\hat{p}),"['statistics', 'sampling', 'confidence-interval', 'standard-error']"
16,Is the set of all random $n$-vectors with arbitrary expectations an inner product space?,Is the set of all random -vectors with arbitrary expectations an inner product space?,n,"Let $(\Omega, \mathcal A, Prob)$ be a probability space. Let $V$ be a set of all random $n$ -vectors $X$ of the form $X = [x_1 \cdots x_n]^T$ , where each $x_i: \Omega \rightarrow \mathbb R$ is in turn an arbitrary random variable with finite second moment, i.e., $e\left[X^T X\right] < \infty$ . Is $V$ a real inner product space without requiring that expectation $E[X] = 0$ for each element $X$ of $V$ ? If yes, what is the simplest inner product definition that does the job? (Note: $E$ is the expectation operator on $V$ whereas $e$ is the expectation operator on the vector space of all random variables $x: \Omega \rightarrow \mathbb R$ .) It is clear to me that $V$ satisfies the axioms of a real vector space. I can also see that if $V$ were to consist of only those random $n$ -vectors $X$ that have zero expectation, then the inner product definition $\langle X, Y \rangle = e\left[X^TY\right]$ transforms $V$ into a real inner product space. However I am unsure about the general case of $V$ as mentioned in the question. Would appreciate some insight, and a simple example of an inner product definition, if the answer to the question is ""yes"". Thanks.","Let be a probability space. Let be a set of all random -vectors of the form , where each is in turn an arbitrary random variable with finite second moment, i.e., . Is a real inner product space without requiring that expectation for each element of ? If yes, what is the simplest inner product definition that does the job? (Note: is the expectation operator on whereas is the expectation operator on the vector space of all random variables .) It is clear to me that satisfies the axioms of a real vector space. I can also see that if were to consist of only those random -vectors that have zero expectation, then the inner product definition transforms into a real inner product space. However I am unsure about the general case of as mentioned in the question. Would appreciate some insight, and a simple example of an inner product definition, if the answer to the question is ""yes"". Thanks.","(\Omega, \mathcal A, Prob) V n X X = [x_1 \cdots x_n]^T x_i: \Omega \rightarrow \mathbb R e\left[X^T X\right] < \infty V E[X] = 0 X V E V e x: \Omega \rightarrow \mathbb R V V n X \langle X, Y \rangle = e\left[X^TY\right] V V","['probability-theory', 'statistics', 'vector-spaces', 'random-variables', 'inner-products']"
17,Book recommendation on maximum likelihood estimation with proofs in general settings,Book recommendation on maximum likelihood estimation with proofs in general settings,,"I am looking for a book (or serious notes) that contains a chapter on maximum likelihood estimation with all proofs on consistency, functional invariance, efficiency, second-order efficiency after correction for bias... preferably from a measure theoretic point of view. Any recommendation ?","I am looking for a book (or serious notes) that contains a chapter on maximum likelihood estimation with all proofs on consistency, functional invariance, efficiency, second-order efficiency after correction for bias... preferably from a measure theoretic point of view. Any recommendation ?",,"['probability-theory', 'statistics', 'soft-question', 'book-recommendation', 'maximum-likelihood']"
18,Hoeffding's Inequality Assumptions,Hoeffding's Inequality Assumptions,,"I'm looking for the assumptions of the Hoeffding's inequality to check it is applicable to my problem. So far the only assumptions I can find are the variables $Z_i$ are IID and bounded. However, Im wondering if there is some assumption I am missing that assumes the underlying distribution is Gaussian or Sub-Gaussian. Am I missing any assumptions or is Gaussian or sub-Gaussian implied?","I'm looking for the assumptions of the Hoeffding's inequality to check it is applicable to my problem. So far the only assumptions I can find are the variables are IID and bounded. However, Im wondering if there is some assumption I am missing that assumes the underlying distribution is Gaussian or Sub-Gaussian. Am I missing any assumptions or is Gaussian or sub-Gaussian implied?",Z_i,"['probability-theory', 'statistics', 'inequality', 'upper-lower-bounds']"
19,Hoeffding/ANOVA decomposition for vector valued $U$-statistics,Hoeffding/ANOVA decomposition for vector valued -statistics,U,"Consider a random variable $Z = f(X_1, \dots, X_n)$ , where $f: {\bf R}^n \to {\bf R}$ and $X_1, \dots, X_n$ IID random variables, and assume that $Z$ and $X_1, \dots, X_n$ all have finite variance. It is well known that there exists a collection of projections $\{\pi_A : L^2 \to L^2\}$ , indexed by subsets $A \subseteq \{1, \dots, n\}$ , such that $$Z = \sum_A \pi_A Z$$ and $\pi_A(Z)$ and $\pi_B(Z)$ are uncorrelated ( $\mathop{{\bf Cov}}[\pi_A(Z), \pi_B(Z)] = 0$ ) whenever $A \neq B$ .   This is the Hoeffding decomposition of $Z$ . Is there an analogous decomposition if $f$ is vector valued, that is, if $Z \in {\bf R}^p$ ? That is, does there exist a set of projections $\{\pi_A\}$ such that $Z = \sum \pi_A Z$ and $$ \mathop{{\bf Cov}}[\pi_A(Z), \pi_B(Z)] = \mathbf{0} \in \mathbf{R}^{p \times p} $$ for all $A \neq B$ ?","Consider a random variable , where and IID random variables, and assume that and all have finite variance. It is well known that there exists a collection of projections , indexed by subsets , such that and and are uncorrelated ( ) whenever .   This is the Hoeffding decomposition of . Is there an analogous decomposition if is vector valued, that is, if ? That is, does there exist a set of projections such that and for all ?","Z = f(X_1, \dots, X_n) f: {\bf R}^n \to {\bf R} X_1, \dots, X_n Z X_1, \dots, X_n \{\pi_A : L^2 \to L^2\} A \subseteq \{1, \dots, n\} Z = \sum_A \pi_A Z \pi_A(Z) \pi_B(Z) \mathop{{\bf Cov}}[\pi_A(Z), \pi_B(Z)] = 0 A \neq B Z f Z \in {\bf R}^p \{\pi_A\} Z = \sum \pi_A Z 
\mathop{{\bf Cov}}[\pi_A(Z), \pi_B(Z)] = \mathbf{0} \in \mathbf{R}^{p \times p}
 A \neq B","['probability', 'statistics', 'anova']"
20,Linear regression with non-fixed regressors and some properties,Linear regression with non-fixed regressors and some properties,,"I was talking to my teacher the other day about the OLS and linear regression model $Y = \beta X + \varepsilon$ . If the regressors X are fixed numbers and I don't have the normal condition on the errors, then in order to have the asymptotic distribution of the OLS estimator (CLT) I need the condition (this also appears in Thomas Ferguson) \begin{equation} \frac{\max_i (X_i - \bar{X})^2}{n} \longrightarrow 0, n\rightarrow \infty. \end{equation} But then in an e-mail I was asking about the case where $X$ is random, and I get the response If the errors are not normal and $X$ has finite fourth moment, since $X$ is random we automatically have $$\frac{\max_i (X_i - \bar{X})^2}{n} \longrightarrow 0, n\rightarrow \infty.$$ I'd like to understand this response without asking in another e-mail. Why do we have this property based on the randomness of the $X$ ?","I was talking to my teacher the other day about the OLS and linear regression model . If the regressors X are fixed numbers and I don't have the normal condition on the errors, then in order to have the asymptotic distribution of the OLS estimator (CLT) I need the condition (this also appears in Thomas Ferguson) But then in an e-mail I was asking about the case where is random, and I get the response If the errors are not normal and has finite fourth moment, since is random we automatically have I'd like to understand this response without asking in another e-mail. Why do we have this property based on the randomness of the ?","Y = \beta X + \varepsilon \begin{equation}
\frac{\max_i (X_i - \bar{X})^2}{n} \longrightarrow 0, n\rightarrow \infty.
\end{equation} X X X \frac{\max_i (X_i - \bar{X})^2}{n} \longrightarrow 0, n\rightarrow \infty. X","['probability', 'probability-theory', 'statistics', 'random-variables', 'linear-regression']"
21,Simple question related to Correlation & Covariance,Simple question related to Correlation & Covariance,,"$X$ and $Y$ are two random variables with $\Bbb E[X] = \Bbb E[Y] = 1$ and $\Bbb E[X^2] = \Bbb E[Y^2] = 2$ . Which of the following is not possible: $\Bbb E[XY] > 0$ $\Bbb E[XY] < 0$ $\Bbb E[XY] = 0$ $\Bbb E[XY] \le 2$ I reached the following conclusions: $\operatorname{Cov}(X,Y) = \Bbb E[XY] - \Bbb E[X]\Bbb E[Y] = \Bbb E[XY]-1$ $\operatorname{Var}(X) = \Bbb E[X^2] - \Bbb E[X]^2 = \operatorname{Var}(Y) = 1$ $\operatorname{Corr}(X,Y) = \operatorname{Cov}(X,Y)$ I noticed that this value is the maximum value for both correlation and covariance since $$\operatorname{Corr}(X,Y)=\frac{\operatorname{Cov}(X,Y)}{\sqrt{\operatorname{Var}(X)\operatorname{Var}(Y)}}$$ and $\operatorname{Corr}(X,Y) = \Bbb E[XY]-1$ but I still did not reach a conclusion regarding $\Bbb E[XY]$ . Any help is appreciated!",and are two random variables with and . Which of the following is not possible: I reached the following conclusions: I noticed that this value is the maximum value for both correlation and covariance since and but I still did not reach a conclusion regarding . Any help is appreciated!,"X Y \Bbb E[X] = \Bbb E[Y] = 1 \Bbb E[X^2] = \Bbb E[Y^2] = 2 \Bbb E[XY] > 0 \Bbb E[XY] < 0 \Bbb E[XY] = 0 \Bbb E[XY] \le 2 \operatorname{Cov}(X,Y) = \Bbb E[XY] - \Bbb E[X]\Bbb E[Y] = \Bbb E[XY]-1 \operatorname{Var}(X) = \Bbb E[X^2] - \Bbb E[X]^2 = \operatorname{Var}(Y) = 1 \operatorname{Corr}(X,Y) = \operatorname{Cov}(X,Y) \operatorname{Corr}(X,Y)=\frac{\operatorname{Cov}(X,Y)}{\sqrt{\operatorname{Var}(X)\operatorname{Var}(Y)}} \operatorname{Corr}(X,Y) = \Bbb E[XY]-1 \Bbb E[XY]","['statistics', 'random-variables', 'covariance', 'correlation']"
22,How to mathematically explain why the median kill/death ratio is always lower than the mean in battle royale games?,How to mathematically explain why the median kill/death ratio is always lower than the mean in battle royale games?,,"Firstly this is a theory that I'm pretty certain must be true at least under certain conditions, but I don't know how to explain it mathematically. I understand there are many variables I'm not accounting for. FYI: In a battle royale game (BR) players drop in to a specified area that closes over time, fighting till the last man standing, with no respawning. Outside of game-specific mechanics like fall-deaths, the total kill-death ratio (kdr) is a fraction below 1, because the last player doesn't die. Here's my logic now: From the stats I've seen, as well as experience, the median kd should be lower than the mean, I'd guess around 0.7-0.85 for most BR games. It's quite obvious why that is. The top players can sometimes kill 1/4 of the lobby. But I wanted to explain it more mathematically. If you convert the battle royale into a 1v1 single elimination tournament, the distribution is obvious. If there are 8 players, 4 get no kills, 2 get 1, 1 gets 2, and 1 gets 3. From my understanding you can't calculate the mean kd from this because the winner doesn't die, but the median is 0.5. To get the mean I'd add a second game, where all the players who didn't get a kill get on the scoreboard. The KDs would now be 4 with 0.5, 2 with 1 and 2 with 3 (the mean being 1.5 and the median being 0.75). So even with the skill levels being completely flattened (the placing reversed after the first game), the median is still 50% of the mean. If you added skill, and assumed the same players would place highly most of the time, the median would be even lower. Obvious counters to this as a relevant model: yes, players don't fight each other one at a time, and some players will camp for placement. IMO the former would actually accentuate the skewedness of the distribution, as good players are more able to take advantage of third parties. The latter would have no/little effect on kd distribution, as camping till the end to win won't help you if you have to fight good players, and doesn't necessarily improve your kd or harm a high-kd player's anyway. So here's an alternative model: players kill each other in a circle, p1 kills p2, p8 kills p1... and p3 kills p4 to win. Playing twice assuming equal skill you would end up with a distribution like {0, 0.5, 1, 1, 1, 1, 2, 3} the mean being 1.1875 and the median being 1}. Even though I think this is less similar to how BRs play out, it still would give a lower mean than median. I was wondering how I could improve upon the logic and make it more useful.  Is it a convincing argument? Is there a way I could combine the first and second models? And also if there are any general theorems that would apply.","Firstly this is a theory that I'm pretty certain must be true at least under certain conditions, but I don't know how to explain it mathematically. I understand there are many variables I'm not accounting for. FYI: In a battle royale game (BR) players drop in to a specified area that closes over time, fighting till the last man standing, with no respawning. Outside of game-specific mechanics like fall-deaths, the total kill-death ratio (kdr) is a fraction below 1, because the last player doesn't die. Here's my logic now: From the stats I've seen, as well as experience, the median kd should be lower than the mean, I'd guess around 0.7-0.85 for most BR games. It's quite obvious why that is. The top players can sometimes kill 1/4 of the lobby. But I wanted to explain it more mathematically. If you convert the battle royale into a 1v1 single elimination tournament, the distribution is obvious. If there are 8 players, 4 get no kills, 2 get 1, 1 gets 2, and 1 gets 3. From my understanding you can't calculate the mean kd from this because the winner doesn't die, but the median is 0.5. To get the mean I'd add a second game, where all the players who didn't get a kill get on the scoreboard. The KDs would now be 4 with 0.5, 2 with 1 and 2 with 3 (the mean being 1.5 and the median being 0.75). So even with the skill levels being completely flattened (the placing reversed after the first game), the median is still 50% of the mean. If you added skill, and assumed the same players would place highly most of the time, the median would be even lower. Obvious counters to this as a relevant model: yes, players don't fight each other one at a time, and some players will camp for placement. IMO the former would actually accentuate the skewedness of the distribution, as good players are more able to take advantage of third parties. The latter would have no/little effect on kd distribution, as camping till the end to win won't help you if you have to fight good players, and doesn't necessarily improve your kd or harm a high-kd player's anyway. So here's an alternative model: players kill each other in a circle, p1 kills p2, p8 kills p1... and p3 kills p4 to win. Playing twice assuming equal skill you would end up with a distribution like {0, 0.5, 1, 1, 1, 1, 2, 3} the mean being 1.1875 and the median being 1}. Even though I think this is less similar to how BRs play out, it still would give a lower mean than median. I was wondering how I could improve upon the logic and make it more useful.  Is it a convincing argument? Is there a way I could combine the first and second models? And also if there are any general theorems that would apply.",,"['probability', 'statistics', 'ratio', 'infinite-games']"
23,"What is the Covariance of Two Bernoulli Random Variables coming from the same sample, under design-based approach?","What is the Covariance of Two Bernoulli Random Variables coming from the same sample, under design-based approach?",,"The scenario for the question is as follows: We draw a sample of size n from a population of size N. Assume a design-based approach (meaning the variable of interest $y_i$ (where $i$ represent the $i^{th}$ unit in the population) has fixed value and is unknown. The random variable $Z_1, Z_2, Z_3, ....Z_i..., Z_N$ represent if the $i_{th}$ unit in the population is in sample or not. In other words: $Z_i = 1$ if unit i is in the sample, and equals $0$ otherwise. We choose an Simple Random Sample (SRS) of size n out of the N population units, and the $Z_i$ 's are identically distributions Bernoulli random variable with $p_i=P(Z_i=1)=P(select\space unit\space i \space\ in\space the \space sample)=n/N$ and $P(Z_i=0)=P(select\space unit\space i \space\ in\space the \space sample)=(1-(n/N))$ The question involves finding the value for $E[Z_iZ_j]$ and $Cov(Z_i, Z_j)$ My questions are the following: 1.) What is the meaning of the random variable $Z_iZ_j$ ? From what I understand, it is a function tha maps from the set of all possible outcomes to the real numbers. There are 4 possible outcomes concerning the combination of the two random variables $Z_i$ and $Z_j$ : 1.) both are in sample, 2.) both are not in sample, 3.) i is in sample, j is not in sample, 4.) i is not in sample, j is in sample. The caluclation of $E[ZiZj]$ given in the textbook (Sampling: Design and Analysis, by  Sharon Lohr, second edition, page 52, if you happen to have the book) considers only the conditional probability of both units in sample when calculating the expectation. I can find the probability for each of the 4 scenarios listed above, but i do not understand what value should they take on. The calculation in the textbook is as follows: $E[Z_iZ_j]=P(Z_i=1 \space and \space Z_j=1) = P(Z_i=1 \space | \space Z_j=1)*P(Z_i =1) = (\frac{n-1}{N-1})*(\frac{n}{N})$ Why is the other scenarios not considered? And why is the value for this scenario set to 1? Second Question: the calculation for Covariance is given as follows: $Cov(Z_i, Z_j)=E[Z_i Z_j]-E[Z_i]E[Z_j]=(\frac{n-1}{N-1})*(\frac{n}{N}) - (\frac{n}{N})^2=-(\frac{1}{N-1})*(1-\frac{n}{N})*\frac{n}{N}$ How did the derivation from the second last line to the last line happen? Thank you.","The scenario for the question is as follows: We draw a sample of size n from a population of size N. Assume a design-based approach (meaning the variable of interest (where represent the unit in the population) has fixed value and is unknown. The random variable represent if the unit in the population is in sample or not. In other words: if unit i is in the sample, and equals otherwise. We choose an Simple Random Sample (SRS) of size n out of the N population units, and the 's are identically distributions Bernoulli random variable with and The question involves finding the value for and My questions are the following: 1.) What is the meaning of the random variable ? From what I understand, it is a function tha maps from the set of all possible outcomes to the real numbers. There are 4 possible outcomes concerning the combination of the two random variables and : 1.) both are in sample, 2.) both are not in sample, 3.) i is in sample, j is not in sample, 4.) i is not in sample, j is in sample. The caluclation of given in the textbook (Sampling: Design and Analysis, by  Sharon Lohr, second edition, page 52, if you happen to have the book) considers only the conditional probability of both units in sample when calculating the expectation. I can find the probability for each of the 4 scenarios listed above, but i do not understand what value should they take on. The calculation in the textbook is as follows: Why is the other scenarios not considered? And why is the value for this scenario set to 1? Second Question: the calculation for Covariance is given as follows: How did the derivation from the second last line to the last line happen? Thank you.","y_i i i^{th} Z_1, Z_2, Z_3, ....Z_i..., Z_N i_{th} Z_i = 1 0 Z_i p_i=P(Z_i=1)=P(select\space unit\space i \space\ in\space the \space sample)=n/N P(Z_i=0)=P(select\space unit\space i \space\ in\space the \space sample)=(1-(n/N)) E[Z_iZ_j] Cov(Z_i, Z_j) Z_iZ_j Z_i Z_j E[ZiZj] E[Z_iZ_j]=P(Z_i=1 \space and \space Z_j=1) = P(Z_i=1 \space | \space Z_j=1)*P(Z_i =1)
= (\frac{n-1}{N-1})*(\frac{n}{N}) Cov(Z_i, Z_j)=E[Z_i Z_j]-E[Z_i]E[Z_j]=(\frac{n-1}{N-1})*(\frac{n}{N}) - (\frac{n}{N})^2=-(\frac{1}{N-1})*(1-\frac{n}{N})*\frac{n}{N}","['statistics', 'random-variables', 'expected-value', 'covariance', 'sampling']"
24,"In which sense is this an ""irreducibility"" condition on a Markov semigroup?","In which sense is this an ""irreducibility"" condition on a Markov semigroup?",,"Let $E$ be a $\mathbb R$ -Banach space, $\rho$ be a metric on $E$ , $\delta_x$ denote the Dirac measure on $(E,\mathcal B(E))$ at $x$ for $x\in E$ and $(\kappa_t)_{t\ge0}$ be a Markov semigroup on $(E,\mathcal B(E))$ satisfying the following property: If $\delta>0$ , there is a $t_0\ge0$ so that for all $t\ge t_0$ there is a $a>0$ with $$\inf_{\substack{(x,\:y)\:\in\:E^2\\\left\|x\right\|_E,\:\left\|y\right\|_E\:\le\:c}}\sup_{\substack{\gamma\\\gamma\text{ is a coupling of }\delta_x\kappa_t\text{ and }\delta_y\kappa_t}}\gamma\left(\left\{\rho<\delta\right\}\right)\ge a\tag1.$$ In which sense is this an ""irreducibility"" condition on $(\kappa_t)_{t\ge0}$ ? If $(\Omega,\mathcal A,\operatorname P)$ is a probability space and $X:\Omega\times[0,\infty)\times E$ is a stochastic flow, $X^x_t(\omega):=X(\omega,t,x)$ for $(\omega,t,x)\in\Omega\times[0,\infty)\times E$ and $$\kappa_t(x,B)=\operatorname P\left[X^x_t\in B\right]\;\;\;\text{for all }(x,B)\in E\times\mathcal B(E)\text{ and }t\ge0\tag2,$$ how can we describe $(1)$ in plain English. I've got some problems to understand the intuitive meaning of $(1)$ for the flow.","Let be a -Banach space, be a metric on , denote the Dirac measure on at for and be a Markov semigroup on satisfying the following property: If , there is a so that for all there is a with In which sense is this an ""irreducibility"" condition on ? If is a probability space and is a stochastic flow, for and how can we describe in plain English. I've got some problems to understand the intuitive meaning of for the flow.","E \mathbb R \rho E \delta_x (E,\mathcal B(E)) x x\in E (\kappa_t)_{t\ge0} (E,\mathcal B(E)) \delta>0 t_0\ge0 t\ge t_0 a>0 \inf_{\substack{(x,\:y)\:\in\:E^2\\\left\|x\right\|_E,\:\left\|y\right\|_E\:\le\:c}}\sup_{\substack{\gamma\\\gamma\text{ is a coupling of }\delta_x\kappa_t\text{ and }\delta_y\kappa_t}}\gamma\left(\left\{\rho<\delta\right\}\right)\ge a\tag1. (\kappa_t)_{t\ge0} (\Omega,\mathcal A,\operatorname P) X:\Omega\times[0,\infty)\times E X^x_t(\omega):=X(\omega,t,x) (\omega,t,x)\in\Omega\times[0,\infty)\times E \kappa_t(x,B)=\operatorname P\left[X^x_t\in B\right]\;\;\;\text{for all }(x,B)\in E\times\mathcal B(E)\text{ and }t\ge0\tag2, (1) (1)","['probability-theory', 'statistics', 'measure-theory', 'reference-request', 'markov-process']"
25,Existence of non-randomized most powerful test,Existence of non-randomized most powerful test,,"Let $\Omega = \mathbb{R}^n$ and probability measures $\mathbb{P}\small{1}, \mathbb{P}\small{2}$ are absolutely continuous with regard to the Lebesgue measure. Does a non-randomized most powerful test exist if we test a hypothesis $H\small1 = \{\mathbb{P} = \mathbb{P}\small{1}\}$ with an alternative hypothesis $H\small2 = \{\mathbb{P} = \mathbb{P}\small{2}\}$ for every significance level $\alpha$ ?",Let and probability measures are absolutely continuous with regard to the Lebesgue measure. Does a non-randomized most powerful test exist if we test a hypothesis with an alternative hypothesis for every significance level ?,"\Omega = \mathbb{R}^n \mathbb{P}\small{1}, \mathbb{P}\small{2} H\small1 = \{\mathbb{P} = \mathbb{P}\small{1}\} H\small2 = \{\mathbb{P} = \mathbb{P}\small{2}\} \alpha","['probability-theory', 'statistics', 'lebesgue-measure']"
26,Deriving hyperparameter updates in Online Interactive Collaborative Filtering,Deriving hyperparameter updates in Online Interactive Collaborative Filtering,,"I've been going through "" Online Interactive Collaborative Filtering Using Multi-Armed Bandit with Dependent Arms "" by Wang et al. and am unable to understand how the update equations for the hyperparameters (section 4.3, equation set (23)) were derived. I'd deeply appreciate it if anyone could provide a full or partial derivation of the updates. Any general suggestions regarding how to proceed with the derivation would also be appreciated. ICTR Graphical Model The variables are sampled as below $$\mathbb{p}_m|\lambda \sim \text{Dirichlet}(\lambda)$$ $$\sigma^2_n|\alpha,\beta \sim \text{Inverse-Gamma}(\alpha,\beta)$$ $$\mathbb{q}_n |\mu_{\mathbb{q}}, \Sigma_{\mathbb{q}}, \sigma_n^2 \sim \mathcal{N}(\mu_{\mathbb{q}}, \sigma_n^2\Sigma_{\mathbb{q}})$$ $$\mathbb{\Phi}_k |\eta \sim \text{Dirichlet}(\eta)$$ $$z_{m,t} | \mathbb{p}_m \sim \text{Multinomial}(\mathbb{p}_m)$$ $$x_{m,t} | \mathbb{\Phi}_k \sim \text{Multinomial}(\mathbb{\Phi}_k) $$ $$y_{m,t} \sim \mathcal{N}(\mathbb{p}_m^T\mathbb{q}_n, \sigma_n^2)$$ And the update equations are below","I've been going through "" Online Interactive Collaborative Filtering Using Multi-Armed Bandit with Dependent Arms "" by Wang et al. and am unable to understand how the update equations for the hyperparameters (section 4.3, equation set (23)) were derived. I'd deeply appreciate it if anyone could provide a full or partial derivation of the updates. Any general suggestions regarding how to proceed with the derivation would also be appreciated. ICTR Graphical Model The variables are sampled as below And the update equations are below","\mathbb{p}_m|\lambda \sim \text{Dirichlet}(\lambda) \sigma^2_n|\alpha,\beta \sim \text{Inverse-Gamma}(\alpha,\beta) \mathbb{q}_n |\mu_{\mathbb{q}}, \Sigma_{\mathbb{q}}, \sigma_n^2 \sim \mathcal{N}(\mu_{\mathbb{q}}, \sigma_n^2\Sigma_{\mathbb{q}}) \mathbb{\Phi}_k |\eta \sim \text{Dirichlet}(\eta) z_{m,t} | \mathbb{p}_m \sim \text{Multinomial}(\mathbb{p}_m) x_{m,t} | \mathbb{\Phi}_k \sim \text{Multinomial}(\mathbb{\Phi}_k)  y_{m,t} \sim \mathcal{N}(\mathbb{p}_m^T\mathbb{q}_n, \sigma_n^2)","['statistics', 'optimization', 'machine-learning', 'bayesian-network']"
27,Hypothesis testing with an exponential distribution,Hypothesis testing with an exponential distribution,,"I have the following problem: Given the data $X_1, X_2, \ldots, X_{15}$ which we consider as a sample from a distribution with a probability density of $\exp(-(x-\theta))$ for $x\ge\theta$ . We test the $H_0: \theta=0$ against the $H_1: \theta>0$ . As test statistic $T$ we take $T = \min\{x_1, x_2, \ldots, x_{15}\}$ . Big values for $T$ indicate the $H_1$ . Assume the observed value of $T$ equals $t=0.1$ . What is the p-value of this test? Hint: If $X_1, X_2,\ldots,X_n$ is a sample from an $\operatorname{Exp}(\lambda)$ distribution, than $\min\{X_1,  X_2,\ldots,X_n\}$ has an $\operatorname{Exp}(n\lambda)$ distribution. The solution says 0.22. I know that the first question you have to ask youself regarding the p-value is: ""What is the probability that the H0 would generate a sample >0?"" So I assume H0 is true and take  = 0. The probability-density function becomes: f(x) = Exp(-x). I take up the hint, so I make it f(x) = Exp(-nx) This is where I get stuck. I don't know how to proceed with the information given: Assume the observed value of T equals t=0.1. Can I have feedback on this problem? Thanks, Ter","I have the following problem: Given the data which we consider as a sample from a distribution with a probability density of for . We test the against the . As test statistic we take . Big values for indicate the . Assume the observed value of equals . What is the p-value of this test? Hint: If is a sample from an distribution, than has an distribution. The solution says 0.22. I know that the first question you have to ask youself regarding the p-value is: ""What is the probability that the H0 would generate a sample >0?"" So I assume H0 is true and take  = 0. The probability-density function becomes: f(x) = Exp(-x). I take up the hint, so I make it f(x) = Exp(-nx) This is where I get stuck. I don't know how to proceed with the information given: Assume the observed value of T equals t=0.1. Can I have feedback on this problem? Thanks, Ter","X_1, X_2, \ldots, X_{15} \exp(-(x-\theta)) x\ge\theta H_0: \theta=0 H_1: \theta>0 T T = \min\{x_1, x_2, \ldots, x_{15}\} T H_1 T t=0.1 X_1, X_2,\ldots,X_n \operatorname{Exp}(\lambda) \min\{X_1,  X_2,\ldots,X_n\} \operatorname{Exp}(n\lambda)","['probability', 'statistics', 'probability-distributions', 'hypothesis-testing', 'exponential-distribution']"
28,Questions on the Kantorovich-Rubinstein duality,Questions on the Kantorovich-Rubinstein duality,,"Let $\mu,\nu$ be probability measures on a metric space $(E,d)$ endowed with the Borel $\sigma$ -algebra and $$\operatorname W_d(\mu,\nu):=\inf_{\gamma\in\mathcal C(\mu,\:\nu)}\int d\:{\rm d}\gamma,$$ where $\mathcal C(\mu,\nu)$ denotes the set of couplings of $\mu$ and $\nu$ . The Kantorovich-Rubinstein duality states that $$\operatorname W_d(\mu,\nu)=\sup_{\substack{f\::\:E\:\to\:\mathbb R\\|f|_{\operatorname{Lip}(d)}\:\le\:1}}\int f\:{\rm d}(\mu-\nu)\tag1,$$ where $$|f|_{\operatorname{Lip}(d)}:=\frac{|f(x)-f(y)|}{d(x,y)}\;\;\;\text{for }f:E\to\mathbb R.$$ The "" $\ge$ "" part is almost trivial. Is there an elegant way to prove the other inequality? In any case, I've got some questions on the statement. Does it matter whether we state it in the given form or if we replace the integral on the right-hand side by its absolute value? If $f:E\to\mathbb R$ satisfies $|f|_{\operatorname{Lip}(d)}<\infty$ , then $f$ is clearly continuous and hence Borel measurable. But in order for the integral on the right-hand side of $(1)$ to be well-defined, it needs to be integrable (or at least quasi-integrable) as well. Is this the case? Clearly, by the reverse triangle inequality, $$\int|f|\:{\rm d}(\mu-\nu)\le|f|_{\operatorname{Lip}(d)}\int d\:{\rm d}\gamma\tag2,$$ where $\gamma$ is any coupling of $\mu$ and $\nu$ . So, if we could show that there exists a coupling such that the integral on the right-hand side of $(2)$ is finite, we could conclude that $f$ is integrable. Is this the case? In any case, is $(1)$ equivalent to the same statement with requiring $f$ to be bounded as well?","Let be probability measures on a metric space endowed with the Borel -algebra and where denotes the set of couplings of and . The Kantorovich-Rubinstein duality states that where The "" "" part is almost trivial. Is there an elegant way to prove the other inequality? In any case, I've got some questions on the statement. Does it matter whether we state it in the given form or if we replace the integral on the right-hand side by its absolute value? If satisfies , then is clearly continuous and hence Borel measurable. But in order for the integral on the right-hand side of to be well-defined, it needs to be integrable (or at least quasi-integrable) as well. Is this the case? Clearly, by the reverse triangle inequality, where is any coupling of and . So, if we could show that there exists a coupling such that the integral on the right-hand side of is finite, we could conclude that is integrable. Is this the case? In any case, is equivalent to the same statement with requiring to be bounded as well?","\mu,\nu (E,d) \sigma \operatorname W_d(\mu,\nu):=\inf_{\gamma\in\mathcal C(\mu,\:\nu)}\int d\:{\rm d}\gamma, \mathcal C(\mu,\nu) \mu \nu \operatorname W_d(\mu,\nu)=\sup_{\substack{f\::\:E\:\to\:\mathbb R\\|f|_{\operatorname{Lip}(d)}\:\le\:1}}\int f\:{\rm d}(\mu-\nu)\tag1, |f|_{\operatorname{Lip}(d)}:=\frac{|f(x)-f(y)|}{d(x,y)}\;\;\;\text{for }f:E\to\mathbb R. \ge f:E\to\mathbb R |f|_{\operatorname{Lip}(d)}<\infty f (1) \int|f|\:{\rm d}(\mu-\nu)\le|f|_{\operatorname{Lip}(d)}\int d\:{\rm d}\gamma\tag2, \gamma \mu \nu (2) f (1) f","['probability-theory', 'statistics', 'markov-process', 'lipschitz-functions', 'coupling']"
29,Why linear regression choose to minimize the residual on y-axis but not the absolute error between prediction and measurement?,Why linear regression choose to minimize the residual on y-axis but not the absolute error between prediction and measurement?,,"Does anyone know why linear regression choose to minimize only $(y_{predict} - y_{measured})^2$ , but not $(y_{predict} - y_{measured})^2 + (x_{predict} - x_{measured})^2 $ ?","Does anyone know why linear regression choose to minimize only , but not ?",(y_{predict} - y_{measured})^2 (y_{predict} - y_{measured})^2 + (x_{predict} - x_{measured})^2 ,"['statistics', 'linear-regression']"
30,Confidence Interval. Bernoulli Distribution,Confidence Interval. Bernoulli Distribution,,"I am reviewing the construction of confidence intervals for a random sample with Bernoulli distribution. The book uses the statistics of the central limit theorem that distributes $N(0,1)$ to estimate the interval : $$Z_n = \frac{X_1 + X_2 + \cdots + X_n -n\mu}{\sigma \sqrt{n}}$$ Why are the intervals constructed from these statistics symmetrical around the origin? The book says: ""Since it is desirable that the length of the interval be as small as possible and since the standard normal distribution is symmetrical around the origin, it turns out that the minimum length interval must also be symmetric around the origin"", but I don't understand this.","I am reviewing the construction of confidence intervals for a random sample with Bernoulli distribution. The book uses the statistics of the central limit theorem that distributes to estimate the interval : Why are the intervals constructed from these statistics symmetrical around the origin? The book says: ""Since it is desirable that the length of the interval be as small as possible and since the standard normal distribution is symmetrical around the origin, it turns out that the minimum length interval must also be symmetric around the origin"", but I don't understand this.","N(0,1) Z_n = \frac{X_1 + X_2 + \cdots + X_n -n\mu}{\sigma \sqrt{n}}",['probability']
31,How can I compute the correlation coefficient iteratively?,How can I compute the correlation coefficient iteratively?,,"I'd like to compute the Pearson Correlation Coefficient (PCC) on-line i.e, deriving the PCC for n samples, then updating it with observation n+1.","I'd like to compute the Pearson Correlation Coefficient (PCC) on-line i.e, deriving the PCC for n samples, then updating it with observation n+1.",,"['statistics', 'correlation']"
32,Event Schedule 12 Teams / 4 Activities / Play Each Activity 3X / Play Each Team Once,Event Schedule 12 Teams / 4 Activities / Play Each Activity 3X / Play Each Team Once,,Anyone know how to schedule an event where you have 12 teams and 4 different activities. The different activities all have the ability to host two of the same activities simultaneous. The different activities will have each team plays each event 3 times and plays each team once. I added a picture of what I am working with but as you get towards the end I can't solve it. Thanks,Anyone know how to schedule an event where you have 12 teams and 4 different activities. The different activities all have the ability to host two of the same activities simultaneous. The different activities will have each team plays each event 3 times and plays each team once. I added a picture of what I am working with but as you get towards the end I can't solve it. Thanks,,"['combinatorics', 'statistics', 'permutations', 'combinations']"
33,Poissonization use for sample size,Poissonization use for sample size,,"(Cross-post from Stats Stack Exchange) I'm interested in using the Poissonization trick to solve the following problem, which I made up: Suppose I have a categorical random variable $X$ taking values $1$ , $2$ , and $3$ , with probability distribution $(\pi_1, \pi_2, \pi_3)$ . How many samples from $X$ should I take before I have at least 5 samples from one of the categories, with probability $.9$ ? Or at least 5 samples from every category? If $Y_1, Y_2, Y_3$ are independent Poisson random variables with rates $\pi_1, \pi_2, \pi_3$ , then the Poissonization trick tells us that $(Y_1, Y_2, Y_3 \mid \sum Y_i = k)$ has distribution $\sim \operatorname{Mult}(k; \pi_1, \pi_2, \pi_3)$ on $\mathbb{N}^3$ . If $A\subset \mathbb{N}^3$ is the set $\{x_1, x_2, x_3 \leq 4\}$ (I'm interested in the complement of $A$ ), then $$\mathbb{P}\{(Y_1, Y_2, Y_3)\in A\}$$ $$ = \sum \mathbb{P}\{(Y_1, Y_2, Y_3)\in A \mid \sum Y_i =k \} \cdot \mathbb{P}(\sum Y_i = k)$$ And (this is where I get a bit shaky) I should expand in a sort of fake variable $\lambda$ , representing the rate of $Y_1 + Y_2 + Y_3$ (though I know that rate is actually one), and then $\mathbb{P}\{(Y_1, Y_2, Y_3)\in A\}$ is $$e^{\pi_1 \lambda}(1 + \frac{(\pi_1\lambda)^1}{1!} + \dotsb + \frac{(\pi_1\lambda)^4}{4!})\, \cdot \, $$ $$e^{\pi_2 \lambda}(1 + \frac{(\pi_2\lambda)^1}{1!} + \dotsb + \frac{(\pi_2\lambda)^4}{4!})\, \cdot \,  $$ $$e^{\pi_3 \lambda}(1 + \frac{(\pi_3\lambda)^1}{1!} + \dotsb + \frac{(\pi_3\lambda)^4}{4!})\, \cdot \,  $$ And now I should multiply this by $e^{-\lambda}$ , expand in $\lambda$ somehow, and look for the coefficient on $\frac{\lambda^k}{k!}$ ? Is that right?","(Cross-post from Stats Stack Exchange) I'm interested in using the Poissonization trick to solve the following problem, which I made up: Suppose I have a categorical random variable taking values , , and , with probability distribution . How many samples from should I take before I have at least 5 samples from one of the categories, with probability ? Or at least 5 samples from every category? If are independent Poisson random variables with rates , then the Poissonization trick tells us that has distribution on . If is the set (I'm interested in the complement of ), then And (this is where I get a bit shaky) I should expand in a sort of fake variable , representing the rate of (though I know that rate is actually one), and then is And now I should multiply this by , expand in somehow, and look for the coefficient on ? Is that right?","X 1 2 3 (\pi_1, \pi_2, \pi_3) X .9 Y_1, Y_2, Y_3 \pi_1, \pi_2, \pi_3 (Y_1, Y_2, Y_3 \mid \sum Y_i = k) \sim \operatorname{Mult}(k; \pi_1, \pi_2, \pi_3) \mathbb{N}^3 A\subset \mathbb{N}^3 \{x_1, x_2, x_3 \leq 4\} A \mathbb{P}\{(Y_1, Y_2, Y_3)\in A\}  = \sum \mathbb{P}\{(Y_1, Y_2, Y_3)\in A \mid \sum Y_i =k \} \cdot \mathbb{P}(\sum Y_i = k) \lambda Y_1 + Y_2 + Y_3 \mathbb{P}\{(Y_1, Y_2, Y_3)\in A\} e^{\pi_1 \lambda}(1 + \frac{(\pi_1\lambda)^1}{1!} + \dotsb + \frac{(\pi_1\lambda)^4}{4!})\, \cdot \,  e^{\pi_2 \lambda}(1 + \frac{(\pi_2\lambda)^1}{1!} + \dotsb + \frac{(\pi_2\lambda)^4}{4!})\, \cdot \,   e^{\pi_3 \lambda}(1 + \frac{(\pi_3\lambda)^1}{1!} + \dotsb + \frac{(\pi_3\lambda)^4}{4!})\, \cdot \,   e^{-\lambda} \lambda \frac{\lambda^k}{k!}","['probability', 'statistics', 'approximation', 'poisson-distribution']"
34,Expectation over an expected value in ergodic process,Expectation over an expected value in ergodic process,,"From ergodic theorem we can see that: $\frac{1}{N} \sum_{i}f(x_{i}) \to {E}[f(x)]$ as $n \to \infty$ Does this imply that in ergodic process we can similarly write: $\frac{1}{N} \sum_{i}E[f(x_{i})] \to \lim_{n \to \infty}{E}[f(x_{n})]$ If this statement is right, then I am a little bit confused about how can we write this?","From ergodic theorem we can see that: as Does this imply that in ergodic process we can similarly write: If this statement is right, then I am a little bit confused about how can we write this?",\frac{1}{N} \sum_{i}f(x_{i}) \to {E}[f(x)] n \to \infty \frac{1}{N} \sum_{i}E[f(x_{i})] \to \lim_{n \to \infty}{E}[f(x_{n})],"['probability', 'statistics', 'statistical-inference']"
35,Is there an optimal sampling algorithm to determine the group with highest mean?,Is there an optimal sampling algorithm to determine the group with highest mean?,,"Suppose i have N types of flowers and I want to find which flower people find the most beautiful. Each day, I have a new judge come in to rate (from 0 to 1) a flower of my choice. I have M>>N days to decide estimate which flower is the most beautiful. If I let each flower be judged the same number of times, then in the end, the MLE estimate for the most beautiful flower would be the flower with the highest average rating. However, are there any superior algorithms to determining which flower to get the judges to rate if we're doing this in sequence? What if we changed the question to be: which flowers have beautifulness > 0.8? I feel like there should be a way but Im also unsure how. Distribution wise, let us assume the rating of the flower follows a beta distribution. EDIT: I define a superior algorithm as one that, under the above assumptions, estimates the most beautiful flower, with lowest cross entropy loss. Or replace cross entropy with something similar if it leads to a mathematically simpler solution.","Suppose i have N types of flowers and I want to find which flower people find the most beautiful. Each day, I have a new judge come in to rate (from 0 to 1) a flower of my choice. I have M>>N days to decide estimate which flower is the most beautiful. If I let each flower be judged the same number of times, then in the end, the MLE estimate for the most beautiful flower would be the flower with the highest average rating. However, are there any superior algorithms to determining which flower to get the judges to rate if we're doing this in sequence? What if we changed the question to be: which flowers have beautifulness > 0.8? I feel like there should be a way but Im also unsure how. Distribution wise, let us assume the rating of the flower follows a beta distribution. EDIT: I define a superior algorithm as one that, under the above assumptions, estimates the most beautiful flower, with lowest cross entropy loss. Or replace cross entropy with something similar if it leads to a mathematically simpler solution.",,"['probability', 'probability-theory', 'statistics', 'statistical-inference']"
36,"MLE for normal distribution with mean and variance unknown, consistency and histograms","MLE for normal distribution with mean and variance unknown, consistency and histograms",,"Consider $X_1, X_2, \ldots, X_n$ i.i.d $N(\mu, \sigma^2),$ where both parameters are unknown, and consider estimation of $\sigma^2.$ Consider the MLE for $\sigma^2.$ We know it is, $$\hat{\sigma^2_n} = \frac{1}{n} \sum_{i=1}^n \left(X_i-\bar{X} \right)^2.$$ I have shown $\hat{\sigma^2_n}$ is biased but consistent for $\sigma^2.$ Now, I want to add a concrete example to visualize the property of consistency in $\hat{\sigma^2_n}$ and ran into the following hurdle. I considered $X_1, X_2, \ldots, X_n$ i.i.d $N(2, 3^2)$ . Took random samples of size $n$ . Have a plot that shows how as $n$ increases, $\hat{\sigma}^2_n \to 9$ in probability. This makes sense since $\hat{\sigma^2_n}$ is consistent. But then I did the following: I repeatedly ( $100,000$ times) took samples of size $n=20$ , calculated $\hat{\sigma}^2_n$ , and visualized with histogram to get: I repeatedly ( $100,000$ times) took samples of size $n=10,000$ , calculated $\hat{\sigma}^2_n$ , and visualized with histogram to get: My question is, are these histograms showing that $\hat{\sigma^2_n}$ is asymptotically unbiased (not always true for all consistent estimators but true for this one)? Or is it showing that $\hat{\sigma^2_n}$ is also an efficient estimator (what is the Cramer Rao Lower Bound when $\mu$ is also unknown, and is the asymptotic variance of $\hat{\sigma^2_n}$ equal to it)?","Consider i.i.d where both parameters are unknown, and consider estimation of Consider the MLE for We know it is, I have shown is biased but consistent for Now, I want to add a concrete example to visualize the property of consistency in and ran into the following hurdle. I considered i.i.d . Took random samples of size . Have a plot that shows how as increases, in probability. This makes sense since is consistent. But then I did the following: I repeatedly ( times) took samples of size , calculated , and visualized with histogram to get: I repeatedly ( times) took samples of size , calculated , and visualized with histogram to get: My question is, are these histograms showing that is asymptotically unbiased (not always true for all consistent estimators but true for this one)? Or is it showing that is also an efficient estimator (what is the Cramer Rao Lower Bound when is also unknown, and is the asymptotic variance of equal to it)?","X_1, X_2, \ldots, X_n N(\mu, \sigma^2), \sigma^2. \sigma^2. \hat{\sigma^2_n} = \frac{1}{n} \sum_{i=1}^n \left(X_i-\bar{X} \right)^2. \hat{\sigma^2_n} \sigma^2. \hat{\sigma^2_n} X_1, X_2, \ldots, X_n N(2, 3^2) n n \hat{\sigma}^2_n \to 9 \hat{\sigma^2_n} 100,000 n=20 \hat{\sigma}^2_n 100,000 n=10,000 \hat{\sigma}^2_n \hat{\sigma^2_n} \hat{\sigma^2_n} \mu \hat{\sigma^2_n}","['statistics', 'statistical-inference', 'maximum-likelihood', 'parameter-estimation']"
37,Covariance basic properties and understanding,Covariance basic properties and understanding,,"Let's consider two random variables $A$ and $B$ having joint probability distribution $P(A,B)$ . I would like to understand better the limit cases of the correlation coefficient $r=\frac{C(A,B)}{\sigma(A)\sigma(B)}$ I first show what I understand and where I struggle. In the case $A$ and $B$ are not correlated, it means that $P(b|a)=P(b)$ (or equivalently $P(a|b)=P(a)$ ). In this case: $\int da db P(a,b) *a*b = \int da P(a)*a \int db P(b)*b$ which implies $C(A,B)=0$ and thus $r=0$ . In the case $A$ and $B$ are perfectly correlated, I can imagine a specific case which is: $P(b|a)=\delta(b-a)$ ( $b$ is always equal to $a$ ). Then we have: $$P(a) = \int db P(a,b) = \int da P(a) \delta(b-a) = P(b)$$ And finally: $$C(A,B)=\int da db P(a,b) a*b = \int da db P(a) \delta(b-a) * a * b - \int da P(a)*a \int db P(b)*b=Var(A)$$ And as $P(a)=P(b)$ I have $Var(A)=\sigma(A)^2=\sigma(A) \sigma(B)$ which proves $r=1$ . My question is the following: In the case of perfect correlations, the case $P(b|a)=\delta(b-a)$ is very specific. I would imagine a more general case like $P(b|a)=\delta(b-f(a))$ meaning that if I know $a$ I can deduce for sure $b$ but it is not necesserally the same as $a$ . I tried to derive something in this more general case but I didn't manage to find anything. Are there some properties here ? I also saw the case of ""anti correlation"" case for which $r=-1$ . To which $P(b|a)$ does that correspond to ? $P(b|a)=\delta(b+a)$ ?","Let's consider two random variables and having joint probability distribution . I would like to understand better the limit cases of the correlation coefficient I first show what I understand and where I struggle. In the case and are not correlated, it means that (or equivalently ). In this case: which implies and thus . In the case and are perfectly correlated, I can imagine a specific case which is: ( is always equal to ). Then we have: And finally: And as I have which proves . My question is the following: In the case of perfect correlations, the case is very specific. I would imagine a more general case like meaning that if I know I can deduce for sure but it is not necesserally the same as . I tried to derive something in this more general case but I didn't manage to find anything. Are there some properties here ? I also saw the case of ""anti correlation"" case for which . To which does that correspond to ? ?","A B P(A,B) r=\frac{C(A,B)}{\sigma(A)\sigma(B)} A B P(b|a)=P(b) P(a|b)=P(a) \int da db P(a,b) *a*b = \int da P(a)*a \int db P(b)*b C(A,B)=0 r=0 A B P(b|a)=\delta(b-a) b a P(a) = \int db P(a,b) = \int da P(a) \delta(b-a) = P(b) C(A,B)=\int da db P(a,b) a*b = \int da db P(a) \delta(b-a) * a * b - \int da P(a)*a \int db P(b)*b=Var(A) P(a)=P(b) Var(A)=\sigma(A)^2=\sigma(A) \sigma(B) r=1 P(b|a)=\delta(b-a) P(b|a)=\delta(b-f(a)) a b a r=-1 P(b|a) P(b|a)=\delta(b+a)",['statistics']
38,"Given $A \in \mathbb R^{m \times n}$, find upper bound for $\mathbb E\|Az\|_q$ for $z$ drawn uniformly at random on the sphere $\{\|z\|_p = 1\}$","Given , find upper bound for  for  drawn uniformly at random on the sphere",A \in \mathbb R^{m \times n} \mathbb E\|Az\|_q z \{\|z\|_p = 1\},"Let $m$ and $n$ be positive integers and $p,q \in [1,\infty]$ . Consider the finite-dimensiaonal normed vector spaces $X = (\mathbb R^m,\|\cdot\|_p)$ and $Y = (\mathbb R^n,\|\cdot\|_q)$ , where $$ \|x\|_p := \begin{cases}(\sum_{i=1}^m|x_i|^p)^{1/p},&\mbox{ if }1 \le p < \infty,&\\\max_{i=1}^m|x_i|,&\mbox{ else.}\end{cases} $$ Let $A:X \rightarrow Y$ be a linear operator, and $z$ be uniformly distributed on the unit sphere in $m$ -sphere of $X$ , and define $\Delta_{p,q}(A):= E[\|Az\|_q]$ . Question. What are good upper bounds for the quantity $\Delta_{p,q}(A)$ in terms of spectral properties of $A$ ( $m$ , $n$ , singular values, etc.) ? Note. I'm particularly interested in the cases where $p,q \in \{1,2,\infty\}$ . Solution for the euclidean case $p=q=2$ Suppose $p=q=2$ . Then $z$ is uniform on the euclidean unit $m$ -sphere, and so has mean $\mu = 0$ and covariance matrix $\Sigma = (1/m)I_m$ . By Jensen's inequality and standard formula for expectation of quadratic forms , one computes $$ \begin{split} \Delta_{2,2}(A)^2 &:= (E\|Az\|_2)^2 \le E\|Az\|_2^2 = \text{Trace}(A^TA\Sigma) + \|A\mu\|_2^2 = (1/m)\text{Trace}(A^TA)\\ & = (1/m)\|A\|_{\text{Fro}}^2, \end{split} $$ from which we get $\Delta_{2,2}(A) \le m^{-1/2}\|A\|_{\text{Fro}}$ . Edit: Solution for the case $p \in \{2,\infty\}$ By symmetry, $z$ has mean 0 and covariance $$ \Sigma_p = \begin{cases}\frac{1}{m}I_m,&\mbox{ if }p = 2,\\\frac{1}{3}I_m + \mathcal O(m^2e^{-Cm}),&\mbox{ if }p=\infty.\end{cases} $$ Thus, one computes $$ E\|Az\|_2^2 = \text{Trace}(A^TA\Sigma_p) = \begin{cases}\frac{1}{m}\text{Trace}(A^TA) = \|A\|_F^2,&\mbox{ if }p = 2,\\\frac{1}{3}\|A\|_F^2+ \mathcal O(m^2e^{-Cm}),&\mbox{ if }p=\infty.\end{cases} \tag{1} $$ On the other hand, we have by classical equaivalence bounds for norms on finite-dimensional spaces, one has $$ \|Az\|_q \le \begin{cases}n^{1/q-1/2}\|Az\|_2,&\mbox{ if }1 \le q < 2,\\\|Az\|_2, &\mbox{ if }2 \le q \le \infty.\end{cases}. $$ Taking expectations on both sides and using (1), we then get $$ \begin{split} \frac{\Delta_{p,q}(A)}{\|A\|_F} &\le \begin{cases}m^{-1/2}n^{1/q-1/2},&\mbox{ if }p=2,\;1 \le q < 2,\\m^{-1/2}, &\mbox{ if }p=2,\;2 \le q \le \infty,\\\dfrac{n^{1/q-1/2}}{3},&\mbox{ if }p=\infty,\; 1 \le q < 2,\\\frac{1}{3},&\mbox{ if }p=\infty,\; 2 \le p \le \infty.\end{cases}\\ \end{split} $$ which can be written more compactly as $$ \frac{\Delta_{p,q}(A)}{\|A\|_F} \le  \begin{cases}m^{-1/2}n^{(1/q-1/2)_+},&\mbox{ if }p=2,\\\dfrac{n^{(1/q-1/2)_+}}{3},&\mbox{ if }p=\infty.\end{cases} $$","Let and be positive integers and . Consider the finite-dimensiaonal normed vector spaces and , where Let be a linear operator, and be uniformly distributed on the unit sphere in -sphere of , and define . Question. What are good upper bounds for the quantity in terms of spectral properties of ( , , singular values, etc.) ? Note. I'm particularly interested in the cases where . Solution for the euclidean case Suppose . Then is uniform on the euclidean unit -sphere, and so has mean and covariance matrix . By Jensen's inequality and standard formula for expectation of quadratic forms , one computes from which we get . Edit: Solution for the case By symmetry, has mean 0 and covariance Thus, one computes On the other hand, we have by classical equaivalence bounds for norms on finite-dimensional spaces, one has Taking expectations on both sides and using (1), we then get which can be written more compactly as","m n p,q \in [1,\infty] X = (\mathbb R^m,\|\cdot\|_p) Y = (\mathbb R^n,\|\cdot\|_q) 
\|x\|_p := \begin{cases}(\sum_{i=1}^m|x_i|^p)^{1/p},&\mbox{ if }1 \le p < \infty,&\\\max_{i=1}^m|x_i|,&\mbox{ else.}\end{cases}
 A:X \rightarrow Y z m X \Delta_{p,q}(A):= E[\|Az\|_q] \Delta_{p,q}(A) A m n p,q \in \{1,2,\infty\} p=q=2 p=q=2 z m \mu = 0 \Sigma = (1/m)I_m 
\begin{split}
\Delta_{2,2}(A)^2 &:= (E\|Az\|_2)^2 \le E\|Az\|_2^2 = \text{Trace}(A^TA\Sigma) + \|A\mu\|_2^2 = (1/m)\text{Trace}(A^TA)\\
& = (1/m)\|A\|_{\text{Fro}}^2,
\end{split}
 \Delta_{2,2}(A) \le m^{-1/2}\|A\|_{\text{Fro}} p \in \{2,\infty\} z 
\Sigma_p = \begin{cases}\frac{1}{m}I_m,&\mbox{ if }p = 2,\\\frac{1}{3}I_m + \mathcal O(m^2e^{-Cm}),&\mbox{ if }p=\infty.\end{cases}
 
E\|Az\|_2^2 = \text{Trace}(A^TA\Sigma_p) = \begin{cases}\frac{1}{m}\text{Trace}(A^TA) = \|A\|_F^2,&\mbox{ if }p = 2,\\\frac{1}{3}\|A\|_F^2+ \mathcal O(m^2e^{-Cm}),&\mbox{ if }p=\infty.\end{cases}
\tag{1}
 
\|Az\|_q \le \begin{cases}n^{1/q-1/2}\|Az\|_2,&\mbox{ if }1 \le q < 2,\\\|Az\|_2, &\mbox{ if }2 \le q \le \infty.\end{cases}.
 
\begin{split}
\frac{\Delta_{p,q}(A)}{\|A\|_F} &\le \begin{cases}m^{-1/2}n^{1/q-1/2},&\mbox{ if }p=2,\;1 \le q < 2,\\m^{-1/2}, &\mbox{ if }p=2,\;2 \le q \le \infty,\\\dfrac{n^{1/q-1/2}}{3},&\mbox{ if }p=\infty,\; 1 \le q < 2,\\\frac{1}{3},&\mbox{ if }p=\infty,\; 2 \le p \le \infty.\end{cases}\\
\end{split}
 
\frac{\Delta_{p,q}(A)}{\|A\|_F} \le  \begin{cases}m^{-1/2}n^{(1/q-1/2)_+},&\mbox{ if }p=2,\\\dfrac{n^{(1/q-1/2)_+}}{3},&\mbox{ if }p=\infty.\end{cases}
","['probability', 'statistics', 'random-matrices', 'geometric-probability', 'concentration-of-measure']"
39,"Derive Likelihood Ratio test for testing $H_0: \mu=\sigma^2$, $H_1: \mu \not= \sigma^2$.","Derive Likelihood Ratio test for testing , .",H_0: \mu=\sigma^2 H_1: \mu \not= \sigma^2,"Let $X_1, . . . , X_n$ be independent $N(, ^2)$ random variables. Derive the LRT for testing $H_0: \mu=\sigma^2$ , $H_1: \mu \not= \sigma^2$ . As per usual I found MLE of $\theta \in \Theta$ and $\theta \in \Theta_0$ to calculate: $$\Lambda=\frac{\sup_{\theta \in \Theta_0} L(\theta)}{\sup_{\theta \in \Theta}L(\theta)}=\frac{L(a,a)}{L(\bar{X},S^2)}$$ Where $a=\sqrt{\bar{X^2}+1/4}-1/2$ and $S^2=(1/n)\sum (X_i-\bar{X})^2$ . $L(a,a)=(2\pi a)^{-n/2}exp(\frac{-1}{2a}\sum (X_i-a)^2)$ $L(\bar{X},S^2)=(2\pi S^2)^{-n/2}exp(\frac{-1}{2S^2}\sum (X_i-\bar{X})^2)=(2\pi S^2)^{-n/2}exp(\frac{-1}{2S^2}nS^2)=(2\pi S^2)^{-n/2}exp(\frac{-n}{2})$ So I'm getting $$\Lambda=(\frac{eS^2}{a})^{n/2}\exp\{{\frac{-1}{2a}\sum(X_i-a)^2}\}$$ And here I'm stuck. Do I (can I even) simplify this more or can I just shove this into the LRT and move on? In the examples I'm given; they typically find the distribution of the statistic or find an expression in terms of some known distribution. (1) Did I make a mistake? (2) Can I simplify this further? In other words; how do I finish the problem that isn't just ''The LRT is $\chi[\Lambda \leq  c]$ ''","Let be independent random variables. Derive the LRT for testing , . As per usual I found MLE of and to calculate: Where and . So I'm getting And here I'm stuck. Do I (can I even) simplify this more or can I just shove this into the LRT and move on? In the examples I'm given; they typically find the distribution of the statistic or find an expression in terms of some known distribution. (1) Did I make a mistake? (2) Can I simplify this further? In other words; how do I finish the problem that isn't just ''The LRT is ''","X_1, . . . , X_n N(, ^2) H_0: \mu=\sigma^2 H_1: \mu \not= \sigma^2 \theta \in \Theta \theta \in \Theta_0 \Lambda=\frac{\sup_{\theta \in \Theta_0} L(\theta)}{\sup_{\theta \in \Theta}L(\theta)}=\frac{L(a,a)}{L(\bar{X},S^2)} a=\sqrt{\bar{X^2}+1/4}-1/2 S^2=(1/n)\sum (X_i-\bar{X})^2 L(a,a)=(2\pi a)^{-n/2}exp(\frac{-1}{2a}\sum (X_i-a)^2) L(\bar{X},S^2)=(2\pi S^2)^{-n/2}exp(\frac{-1}{2S^2}\sum (X_i-\bar{X})^2)=(2\pi S^2)^{-n/2}exp(\frac{-1}{2S^2}nS^2)=(2\pi S^2)^{-n/2}exp(\frac{-n}{2}) \Lambda=(\frac{eS^2}{a})^{n/2}\exp\{{\frac{-1}{2a}\sum(X_i-a)^2}\} \chi[\Lambda \leq  c]","['probability', 'statistics', 'solution-verification', 'hypothesis-testing']"
40,"Asymptotic distribution of sum of independent, not identically distributed, bernoulli distribution with poisson tail probability","Asymptotic distribution of sum of independent, not identically distributed, bernoulli distribution with poisson tail probability",,"I'm trying to find an asymptotic distribution of the follwing random variable $$Z_n=\sum_{i=1}^n Y_i$$ where $Y_i = I[T_i<t]$ with $T_i \text{~} Gamma(i, \lambda)$ . Here $t$ is a fixed number. My initial trial was using the Lyapunov CLT ( or Linderberg CLT). First I noticed that by using integral by parts, $$E[Y_i]=P[Y_i=1]=P[T_i<t]=\sum_{k=i}^\infty \frac{(t/\lambda)^ie^{-t/\lambda}}{i!}$$ which is the tail probabilty of the poisson random variable with mean $t/\lambda$ . Let $p_i:=E[Y_i], R_i:=Y_i-p_i, S_n:=\sum_{i=1}^nR_i, \sigma_n^2:=Var(S_n)=\sum_{i=1}^np_i(1-p_i)$ . I'm trying to show that $R_i$ satisfies the Lyapunov condition where $$ \sigma_n^{-(2+\delta)}\sum_{i=1}^nE[|R_i|^{(2+\delta)}]\to0\ \text{as $n\to \infty$ }$$ . With $\delta=1$ ,   I get $$E[|R_i|^3]=(1-p_i)^3p_i + p_i^3(1-p_i)^3\le p_i(1-p_i)$$ therefore $$\sigma_n^{-3}\sum_{i=1}^nE[|R_i|^{3}] \le \sigma_n^{-1}.$$ As long as I can show that $$\sigma_n \to \infty \ \text{as $n\to \infty$ }$$ I'm done with the proof. But I'm stuck with showing the divergence of $\sigma_n$ . In other words, I need to show that $$ \sum_{i=1}^n p_i(1-p_i) \to \infty \ \text{as $n\to \infty$ }.$$ I sincerely ask to help me with the proof of the last statement. Or Am i doing something wrong? Thank you.","I'm trying to find an asymptotic distribution of the follwing random variable where with . Here is a fixed number. My initial trial was using the Lyapunov CLT ( or Linderberg CLT). First I noticed that by using integral by parts, which is the tail probabilty of the poisson random variable with mean . Let . I'm trying to show that satisfies the Lyapunov condition where . With ,   I get therefore As long as I can show that I'm done with the proof. But I'm stuck with showing the divergence of . In other words, I need to show that I sincerely ask to help me with the proof of the last statement. Or Am i doing something wrong? Thank you.","Z_n=\sum_{i=1}^n Y_i Y_i = I[T_i<t] T_i \text{~} Gamma(i, \lambda) t E[Y_i]=P[Y_i=1]=P[T_i<t]=\sum_{k=i}^\infty \frac{(t/\lambda)^ie^{-t/\lambda}}{i!} t/\lambda p_i:=E[Y_i], R_i:=Y_i-p_i, S_n:=\sum_{i=1}^nR_i, \sigma_n^2:=Var(S_n)=\sum_{i=1}^np_i(1-p_i) R_i  \sigma_n^{-(2+\delta)}\sum_{i=1}^nE[|R_i|^{(2+\delta)}]\to0\ \text{as n\to \infty } \delta=1 E[|R_i|^3]=(1-p_i)^3p_i + p_i^3(1-p_i)^3\le p_i(1-p_i) \sigma_n^{-3}\sum_{i=1}^nE[|R_i|^{3}] \le \sigma_n^{-1}. \sigma_n \to \infty \ \text{as n\to \infty } \sigma_n  \sum_{i=1}^n p_i(1-p_i) \to \infty \ \text{as n\to \infty }.","['probability', 'statistics', 'asymptotics', 'poisson-distribution', 'central-limit-theorem']"
41,Finding the MLE of $\theta$,Finding the MLE of,\theta,"Working through this given problem on maximum likelihood estimation (MLE). The density is given as $$f(x \mid \theta) = \frac{1}{2\theta} e^{-|x|/\theta}, -\infty < x <\infty $$ I get $$L(\theta)= \frac{1}{(2\theta)^{n}} e^{- \frac{1}{2\theta} \sum\limits_{i=1}^{n}|X_i|}$$ Applying the ln step $$\ln L(\theta)= -n\ln(2\theta) - \frac{1}{\theta}\sum\limits_{i=1}^{n}|X_i|$$ Taking the derivative $$\frac{d}{d\theta} = \frac{-n}{\theta} \frac{\sum\limits_{i=1}^{n}|X_i|}{\theta^{2}}$$ Setting the derivative to 0 and solving for $\theta$ $$\frac{-n}{\theta} \frac{\sum\limits_{i=1}^{n}|X_i|}{\theta^{2}} = 0 $$ $$\hat{\theta}= \frac{\sum\limits_{i=1}^{n}|X_i|}{n}$$ Is this correct or have i done a something wrong and im getting the wrong mle",Working through this given problem on maximum likelihood estimation (MLE). The density is given as I get Applying the ln step Taking the derivative Setting the derivative to 0 and solving for Is this correct or have i done a something wrong and im getting the wrong mle,"f(x \mid \theta) = \frac{1}{2\theta} e^{-|x|/\theta}, -\infty < x <\infty  L(\theta)= \frac{1}{(2\theta)^{n}} e^{- \frac{1}{2\theta} \sum\limits_{i=1}^{n}|X_i|} \ln L(\theta)= -n\ln(2\theta) - \frac{1}{\theta}\sum\limits_{i=1}^{n}|X_i| \frac{d}{d\theta} = \frac{-n}{\theta} \frac{\sum\limits_{i=1}^{n}|X_i|}{\theta^{2}} \theta \frac{-n}{\theta} \frac{\sum\limits_{i=1}^{n}|X_i|}{\theta^{2}} = 0  \hat{\theta}= \frac{\sum\limits_{i=1}^{n}|X_i|}{n}","['statistics', 'statistical-inference', 'maximum-likelihood']"
42,Any probability density can be written as a sum of Gaussian,Any probability density can be written as a sum of Gaussian,,"In Gaussian sum mixture model, any probability density function (pdf) can be written as a sum of Gaussian. Lets consider here any $n$ -dimensional vector $x$ follows Gaussian distribution with mean $\hat{x} \in \mathbb{R}^n$ , and covariance $P \in \mathbb{R}^{n\times n}$ can be written as \begin{equation} p(x) = \mathcal{N}(x;\, \hat{x},\, P) = \sum_{i=1}^{N_f} w_{i} \mathcal{N}(x;\, \hat{x}_i, \, P_i),    \end{equation} where $w_i$ represents the weight of the $i$ -th Gaussian pdf, and $\sum_{i=1}^{N_f} w_{i}=1$ . $\hat{x}_i$ and $P_i$ are the $i$ -th mean and covariance respectively. Here my specific question is how to generate $w_i$ , $\hat{x}_i$ and $P_i$ for any $N_f$ . Any kind of suggestion will be of great help.","In Gaussian sum mixture model, any probability density function (pdf) can be written as a sum of Gaussian. Lets consider here any -dimensional vector follows Gaussian distribution with mean , and covariance can be written as where represents the weight of the -th Gaussian pdf, and . and are the -th mean and covariance respectively. Here my specific question is how to generate , and for any . Any kind of suggestion will be of great help.","n x \hat{x} \in \mathbb{R}^n P \in \mathbb{R}^{n\times n} \begin{equation}
p(x) = \mathcal{N}(x;\, \hat{x},\, P) = \sum_{i=1}^{N_f} w_{i} \mathcal{N}(x;\, \hat{x}_i, \, P_i),   
\end{equation} w_i i \sum_{i=1}^{N_f} w_{i}=1 \hat{x}_i P_i i w_i \hat{x}_i P_i N_f","['probability', 'probability-theory', 'statistics', 'estimation', 'gaussian']"
43,"Asymptotic distribution of U-statistic based ""log-likelihood""","Asymptotic distribution of U-statistic based ""log-likelihood""",,"This is question 4.2c from John Duchi's course Stats 300b at Stanford: http://web.stanford.edu/class/stats300b/Exercises/all-exercises.pdf Consider the U-statistic-based ""Log-likelihood"" type objective: $$L_n(\theta) = {n \choose 2}^{-1} \sum_{i,j}^n 1\{Y_i > Y_j\} \log P_{\theta}(Y_i > Y_j | x_i, x_j) $$ where we have $n$ samples of $(Y_i, x_i)$ such that: $$Y_i = x_i^T \theta + \epsilon_i$$ $$\epsilon_i  N(0, 1)$$ $$E[x_i] = 0, Cov[x_i] = \Sigma$$ Let $\hat{\theta}_n = argmax_{\theta} L_n(\theta)$ and assuming that $\hat{\theta}_n$ is consistent for the true $\theta$ : Find the asymptotic distribution of $\hat{\theta}_n$ . My approach has been (using theorem 5.23 from Asymptotic Statistics by Van der Vaart) to seek the asymptotic distribution of $\sqrt{n}(\hat\theta_n - \theta)$ which under suitable regularity conditions is asymptotically normal with mean $0$ and variance: $$V_\theta^{-1}E[\nabla L_n \nabla L_n^T]V_\theta^{-1}$$ where $V_\theta^{-1} = E[\nabla^2L_n]$ and expectation is taken with respect to data and derivatives are taken with respect to $\theta$ . My difficulty has been in evaluating these expectations to determine the variance of this estimator. $\nabla L_n$ looks like a U-statistic with kernel $h(x_1, x_2) = 1\{Y_i > Y_j\}\nabla_\theta \log P_{\theta}(Y_1 > Y_2 | x_1, x_2)$ . So, $\sqrt{n}(\nabla L_n - E[\nabla L_n])$ is asymptotically $N(0, 2^2\xi_1)$ where $\xi_1$ is the variance of : $$\xi_1 = Cov(h(X, X_i), h(X, X_j)) = E(h(X, X_i)h(X, X_j)) - E(h(X, X_i)) E(h(X, X_j))$$ But I'm also having trouble evaluating this covariance. EDIT: A friend showed me a few tricks, but I dont seem much closer to having a neat closed form (if there is one). At a closer look, $L_N$ is not a log-likelihood, so let's go after $V_\theta$ and $E[\nabla L_n \nabla L_n^T]$ to apply theorem 5.23 directly. First, we have: $$\log P_{\theta}(Y_i > Y_j | x_i, x_j) = \log P_{\theta}(\epsilon_j - \epsilon_i < (x_i - x_j)^T\theta | x_i, x_j) = \log \Phi \bigg(\frac{(x_i-x_j)^T\theta}{\sqrt2}\bigg)$$ Evaluating $\xi_1$ we have for the first moment: $$E(h(X, X_i)) = E [1_i\frac{(X - X_i)\phi(\gamma_i)}{\Phi(\gamma_i)}]$$ where $\gamma_i = \frac{(X - X_i)^T\theta}{\sqrt{2}}$ and $1_i = 1\{Y > Y_i\}$ . By conditioning on the event in the indicator and applying the tower property of expectation: $$ = E [(X - X_i)\phi(\gamma_i)] $$ which is equal to $0$ by symmetry. So, $$\xi_1 = E [(1_i\frac{(X - X_i)\phi(\gamma_i)}{\Phi(\gamma_i)})(1_j\frac{(X - X_j)\phi(\gamma_j)}{\Phi(\gamma_j)})^T]$$ Simplifying as before: $$= E [\phi(\gamma_i)\phi(\gamma_j)(X - X_i)(X - X_i)^T$$ Which I cannot simplify any further. Applying similar simplifications to $\nabla^2 L_n(\theta)$ we have: $$\nabla^2 L_n(\theta) = -\frac{\phi(\gamma_{ij})^2}{\Phi(\gamma_{ij})}(X_i-X_j)(X_i-X_j)^T/2$$ where $\gamma = \frac{(X_i-X_j)^T\theta}{\sqrt2}$ . EDIT: Using this expression to simulate the asymptotic variance, we get what we expect intuitively, this estimator works better than the least-squares estimator when the variance of the noise term $\epsilon$ is smaller than that of the covariates $x_i$ . The following log-log plot shows the relationship for the 1-D case between the variance and $var(\epsilon)$ where $x$ is sampled from $N(0,1)$ . The relationship is even more dramatic when $x\sim  cauchy$ . An interesting followup question would be: can we find a variance stabilizing transform, such that the asymptotic variance no longer depends on the parameter in question?","This is question 4.2c from John Duchi's course Stats 300b at Stanford: http://web.stanford.edu/class/stats300b/Exercises/all-exercises.pdf Consider the U-statistic-based ""Log-likelihood"" type objective: where we have samples of such that: Let and assuming that is consistent for the true : Find the asymptotic distribution of . My approach has been (using theorem 5.23 from Asymptotic Statistics by Van der Vaart) to seek the asymptotic distribution of which under suitable regularity conditions is asymptotically normal with mean and variance: where and expectation is taken with respect to data and derivatives are taken with respect to . My difficulty has been in evaluating these expectations to determine the variance of this estimator. looks like a U-statistic with kernel . So, is asymptotically where is the variance of : But I'm also having trouble evaluating this covariance. EDIT: A friend showed me a few tricks, but I dont seem much closer to having a neat closed form (if there is one). At a closer look, is not a log-likelihood, so let's go after and to apply theorem 5.23 directly. First, we have: Evaluating we have for the first moment: where and . By conditioning on the event in the indicator and applying the tower property of expectation: which is equal to by symmetry. So, Simplifying as before: Which I cannot simplify any further. Applying similar simplifications to we have: where . EDIT: Using this expression to simulate the asymptotic variance, we get what we expect intuitively, this estimator works better than the least-squares estimator when the variance of the noise term is smaller than that of the covariates . The following log-log plot shows the relationship for the 1-D case between the variance and where is sampled from . The relationship is even more dramatic when . An interesting followup question would be: can we find a variance stabilizing transform, such that the asymptotic variance no longer depends on the parameter in question?","L_n(\theta) = {n \choose 2}^{-1} \sum_{i,j}^n 1\{Y_i > Y_j\} \log P_{\theta}(Y_i > Y_j | x_i, x_j)  n (Y_i, x_i) Y_i = x_i^T \theta + \epsilon_i \epsilon_i  N(0, 1) E[x_i] = 0, Cov[x_i] = \Sigma \hat{\theta}_n = argmax_{\theta} L_n(\theta) \hat{\theta}_n \theta \hat{\theta}_n \sqrt{n}(\hat\theta_n - \theta) 0 V_\theta^{-1}E[\nabla L_n \nabla L_n^T]V_\theta^{-1} V_\theta^{-1} = E[\nabla^2L_n] \theta \nabla L_n h(x_1, x_2) = 1\{Y_i > Y_j\}\nabla_\theta \log P_{\theta}(Y_1 > Y_2 | x_1, x_2) \sqrt{n}(\nabla L_n - E[\nabla L_n]) N(0, 2^2\xi_1) \xi_1 \xi_1 = Cov(h(X, X_i), h(X, X_j)) = E(h(X, X_i)h(X, X_j)) - E(h(X, X_i)) E(h(X, X_j)) L_N V_\theta E[\nabla L_n \nabla L_n^T] \log P_{\theta}(Y_i > Y_j | x_i, x_j) = \log P_{\theta}(\epsilon_j - \epsilon_i < (x_i - x_j)^T\theta | x_i, x_j) = \log \Phi \bigg(\frac{(x_i-x_j)^T\theta}{\sqrt2}\bigg) \xi_1 E(h(X, X_i)) = E [1_i\frac{(X - X_i)\phi(\gamma_i)}{\Phi(\gamma_i)}] \gamma_i = \frac{(X - X_i)^T\theta}{\sqrt{2}} 1_i = 1\{Y > Y_i\}  = E [(X - X_i)\phi(\gamma_i)]  0 \xi_1 = E [(1_i\frac{(X - X_i)\phi(\gamma_i)}{\Phi(\gamma_i)})(1_j\frac{(X - X_j)\phi(\gamma_j)}{\Phi(\gamma_j)})^T] = E [\phi(\gamma_i)\phi(\gamma_j)(X - X_i)(X - X_i)^T \nabla^2 L_n(\theta) \nabla^2 L_n(\theta) = -\frac{\phi(\gamma_{ij})^2}{\Phi(\gamma_{ij})}(X_i-X_j)(X_i-X_j)^T/2 \gamma = \frac{(X_i-X_j)^T\theta}{\sqrt2} \epsilon x_i var(\epsilon) x N(0,1) x\sim  cauchy","['probability-theory', 'statistics', 'asymptotics', 'weak-convergence']"
44,Averaging Datasets with Inconsistent Time Points,Averaging Datasets with Inconsistent Time Points,,"I am trying to find the equivalent of a ""mean"" and ""standard deviation"" of some time-dependent datasets, with the added complication that not all datasets were taken at consistent time points. Say, for example, you set 3 separate ovens to 350 degrees and measure the temperature response. All the ovens follow a similar temperature profile, but each timepoint is distinct. Additionally, most of the datapoints were taken at inconsistent times (e.g. only oven 1 has data at 6 min, only oven 2 has data at 5 min, and only oven 3 has data at 5.5 min). I want to know the ""average"" temperature profile as the ovens approach 350, and the spread in these profiles. It would be excellent if I could construct an average curve with time-dependent error bars. I'm not sure how to do this, but there must be some procedure. I thought about picking some easy time points (e.g. 1,2,3...), interpolating the curves that don't have data at these times, and averaging like normal from these calculated values, but that seems to lack rigor. Is there a better option? Sorry for the somewhat contrived example and the excel graph, but I think it's demonstrative of the kind of dataset I'm thinking about.","I am trying to find the equivalent of a ""mean"" and ""standard deviation"" of some time-dependent datasets, with the added complication that not all datasets were taken at consistent time points. Say, for example, you set 3 separate ovens to 350 degrees and measure the temperature response. All the ovens follow a similar temperature profile, but each timepoint is distinct. Additionally, most of the datapoints were taken at inconsistent times (e.g. only oven 1 has data at 6 min, only oven 2 has data at 5 min, and only oven 3 has data at 5.5 min). I want to know the ""average"" temperature profile as the ovens approach 350, and the spread in these profiles. It would be excellent if I could construct an average curve with time-dependent error bars. I'm not sure how to do this, but there must be some procedure. I thought about picking some easy time points (e.g. 1,2,3...), interpolating the curves that don't have data at these times, and averaging like normal from these calculated values, but that seems to lack rigor. Is there a better option? Sorry for the somewhat contrived example and the excel graph, but I think it's demonstrative of the kind of dataset I'm thinking about.",,"['statistics', 'standard-deviation', 'data-analysis']"
45,Can any one recommend a statistics book?,Can any one recommend a statistics book?,,"I am currently a second-year graduate student in statistics. I have taken the course with the textbook Statistical inference by George Casella and Roger L. Berger. I have learned some basic concepts such as sufficient statistics,  complete sufficient statistics,  UMVUE and etc. Though I found all these concepts are in one-dimensional case in this book.  Can anyone recommend a book that covers these concepts in a multivariate or high dimensional case?  Especially, a textbook with practice questions and a solution manual is preferred.","I am currently a second-year graduate student in statistics. I have taken the course with the textbook Statistical inference by George Casella and Roger L. Berger. I have learned some basic concepts such as sufficient statistics,  complete sufficient statistics,  UMVUE and etc. Though I found all these concepts are in one-dimensional case in this book.  Can anyone recommend a book that covers these concepts in a multivariate or high dimensional case?  Especially, a textbook with practice questions and a solution manual is preferred.",,"['probability', 'statistics', 'reference-request', 'statistical-inference', 'book-recommendation']"
46,Determine or find an upper bound for the limsup of a function with random variables,Determine or find an upper bound for the limsup of a function with random variables,,"Determine or find an upper bound for $$\limsup_{n \to \infty} \left|\frac{\frac{2}{n} (l(\beta) - l(\hat{\beta})) - (\beta - \hat{\beta})^T \nabla^2 l(\hat{\beta})(\beta - \hat{\beta})}{(\beta - \hat{\beta})^T \nabla^2 l(\hat{\beta}) (\beta - \hat{\beta})}\right|$$ where $l$ is the log-likelihood function in a model of logistic regression with $n$ samples of the form $(y_i,X_i)$ , where $y_i \in \{0,1\}$ and $X_i \in R^p$ , $\beta \in R^p$ is an unknown vector of parameters and $\hat{\beta}$ is the MLE. W.l.o.g. I assume that $y_i = 0$ for all $i = 1,\dots,n$ and rearrange the expression to $$\left|\limsup_{n \to \infty} \frac{\frac{2}{n} \sum_{i=1}^n \text{log}\left(\frac{1+e^{X_i^T\beta}}{1+e^{X_i^T\hat{\beta}}}\right) - (\beta - \hat{\beta})^T X^T \text{diag}\left(\frac{e^{X_1\hat{\beta}}}{(1+e^{X_1\hat{\beta}})^2},\dots,\frac{e^{X_n\hat{\beta}}}{\left(1+e^{X_n\hat{\beta}}\right)^2}\right)X(\beta - \hat{\beta})}{(\beta - \hat{\beta})^T X^T \text{diag}\left(\frac{e^{X_1\hat{\beta}}}{\left(1+e^{X_1\hat{\beta}}\right)^2},\dots,\frac{e^{X_n\hat{\beta}}}{\left(1+e^{X_n\hat{\beta}}\right)^2}\right)X(\beta - \hat{\beta})}\right|$$ $$= \left|\limsup_{n \to \infty} \frac{\frac{2}{n}\sum_{i=1}^n \text{log}\left(\frac{1+e^{\sum_{j=1}^p x_{ij}\beta_j}}{1+e^{\sum_{j=1}^p x_{ij}\hat{\beta}_j}}\right) - \sum_{s=1}^p \sum_{m=1}^p \sum_{i=1}^n \frac{e^{\sum_{j=1}^p x_{ij}\beta_j}}{\left(1+e^{\sum_{j=1}^p x_{ij}\hat{\beta_j}}\right)^2}x_{im}x_{is}(\beta_m - \hat{\beta}_m)(\beta_s - \hat{\beta}_s)}{\sum_{s=1}^p \sum_{m=1}^p \sum_{i=1}^n \frac{e^{\sum_{j=1}^p x_{ij}\beta_j}}{\left(1+e^{\sum_{j=1}^p x_{ij}\hat{\beta_j}}\right)^2}x_{im}x_{is}(\beta_m - \hat{\beta}_m)(\beta_s - \hat{\beta}_s)}\right|$$ An idea would be to use an application of the Borel-Cantelli lemma Let $a \in R$ be such that $$ \sum_{n=1}^{\infty} P(X_n > a) < \infty.$$ Then $$\limsup_{n \to \infty} X_n \leq a$$ almost surely. For my purposes it would be awesome if I could show this either for $a = 0$ or for $a = \frac{l(\beta)-l(\hat{\beta}) - \frac{1}{2} (\beta -\hat{\beta})^T \nabla^2 l(\hat{\beta}) (\beta - \hat{\beta})}{\lVert\beta - \hat{\beta}\rVert^2}$ . Another idea would be to use the law of large numbers by showing that $$\mathbb{E}\left[\frac{1}{n} \sum_{i=1}^n \text{log}\left(\frac{1+e^{X_i^T\beta}}{1+e^{X_i^T\hat{\beta}}}\right)\right] = \frac{1}{2}(\beta - \hat{\beta})^T X^T \text{diag}\left(\frac{e^{X_1\hat{\beta}}}{(1+e^{X_1\hat{\beta}})^2},\dots,\frac{e^{X_n\hat{\beta}}}{\left(1+e^{X_n\hat{\beta}}\right)^2}\right)X(\beta - \hat{\beta})$$ but I wouldnt know how, especially since the distribution of $X_i$ is not specified. Both ideas would only deliver almost sure statements though but that would be better than nothing.","Determine or find an upper bound for where is the log-likelihood function in a model of logistic regression with samples of the form , where and , is an unknown vector of parameters and is the MLE. W.l.o.g. I assume that for all and rearrange the expression to An idea would be to use an application of the Borel-Cantelli lemma Let be such that Then almost surely. For my purposes it would be awesome if I could show this either for or for . Another idea would be to use the law of large numbers by showing that but I wouldnt know how, especially since the distribution of is not specified. Both ideas would only deliver almost sure statements though but that would be better than nothing.","\limsup_{n \to \infty} \left|\frac{\frac{2}{n} (l(\beta) - l(\hat{\beta})) - (\beta - \hat{\beta})^T \nabla^2 l(\hat{\beta})(\beta - \hat{\beta})}{(\beta - \hat{\beta})^T \nabla^2 l(\hat{\beta}) (\beta - \hat{\beta})}\right| l n (y_i,X_i) y_i \in \{0,1\} X_i \in R^p \beta \in R^p \hat{\beta} y_i = 0 i = 1,\dots,n \left|\limsup_{n \to \infty} \frac{\frac{2}{n} \sum_{i=1}^n \text{log}\left(\frac{1+e^{X_i^T\beta}}{1+e^{X_i^T\hat{\beta}}}\right) - (\beta - \hat{\beta})^T X^T \text{diag}\left(\frac{e^{X_1\hat{\beta}}}{(1+e^{X_1\hat{\beta}})^2},\dots,\frac{e^{X_n\hat{\beta}}}{\left(1+e^{X_n\hat{\beta}}\right)^2}\right)X(\beta - \hat{\beta})}{(\beta - \hat{\beta})^T X^T \text{diag}\left(\frac{e^{X_1\hat{\beta}}}{\left(1+e^{X_1\hat{\beta}}\right)^2},\dots,\frac{e^{X_n\hat{\beta}}}{\left(1+e^{X_n\hat{\beta}}\right)^2}\right)X(\beta - \hat{\beta})}\right| = \left|\limsup_{n \to \infty} \frac{\frac{2}{n}\sum_{i=1}^n \text{log}\left(\frac{1+e^{\sum_{j=1}^p x_{ij}\beta_j}}{1+e^{\sum_{j=1}^p x_{ij}\hat{\beta}_j}}\right) - \sum_{s=1}^p \sum_{m=1}^p \sum_{i=1}^n \frac{e^{\sum_{j=1}^p x_{ij}\beta_j}}{\left(1+e^{\sum_{j=1}^p x_{ij}\hat{\beta_j}}\right)^2}x_{im}x_{is}(\beta_m - \hat{\beta}_m)(\beta_s - \hat{\beta}_s)}{\sum_{s=1}^p \sum_{m=1}^p \sum_{i=1}^n \frac{e^{\sum_{j=1}^p x_{ij}\beta_j}}{\left(1+e^{\sum_{j=1}^p x_{ij}\hat{\beta_j}}\right)^2}x_{im}x_{is}(\beta_m - \hat{\beta}_m)(\beta_s - \hat{\beta}_s)}\right| a \in R  \sum_{n=1}^{\infty} P(X_n > a) < \infty. \limsup_{n \to \infty} X_n \leq a a = 0 a = \frac{l(\beta)-l(\hat{\beta}) - \frac{1}{2} (\beta -\hat{\beta})^T \nabla^2 l(\hat{\beta}) (\beta - \hat{\beta})}{\lVert\beta - \hat{\beta}\rVert^2} \mathbb{E}\left[\frac{1}{n} \sum_{i=1}^n \text{log}\left(\frac{1+e^{X_i^T\beta}}{1+e^{X_i^T\hat{\beta}}}\right)\right] = \frac{1}{2}(\beta - \hat{\beta})^T X^T \text{diag}\left(\frac{e^{X_1\hat{\beta}}}{(1+e^{X_1\hat{\beta}})^2},\dots,\frac{e^{X_n\hat{\beta}}}{\left(1+e^{X_n\hat{\beta}}\right)^2}\right)X(\beta - \hat{\beta}) X_i","['limits', 'statistics', 'limsup-and-liminf', 'law-of-large-numbers', 'borel-cantelli-lemmas']"
47,Means and entropy: what's the relationship?,Means and entropy: what's the relationship?,,"Economist here. In my current research I often encounter the concepts of entropy and means, and I find some connections interesting (for my purposes). The context is a set of $n$ nonnegative numbers $\{a_1,\dots,a_n\}$ , which can be normalized by the sum of the $a_i$ 's to get $\{x_1,\dots,x_n\}$ . These $x_i$ 's serve as weights in the context of means, and they serve as probabilities out of which entropy can be calculated. I find relationships between entropy and means, particularly between \begin{align*} \mathcal{A}\left(a_{i},\frac{1}{n}\right) & =\frac{1}{n}\sum a_{i} & \underset{\textrm{(i.e. equal weights }\frac{1}{n})}{\textrm{unweighted arithmetic mean}}\\ \mathcal{G}\left(a_{i},x_{i}\right) & =\prod a_{i}^{x_{i}} & \underset{\textrm{(with weights }x_{i}\textrm{ summing to one)}}{\textrm{weighted geometric mean}}\\ H(x_{i}) & =-\sum_{i=1}^{n}x_{i}\ln x_{i} & \underset{\textrm{(same }x_{i}\textrm{ as above, i.e. probabilities)}}{\textrm{Shannon entropy}} \end{align*} Questions: In general, how are entropy and means related? I can't think I'm the first one to notice. The only thing I can think of is the inequality $\mathcal{A}\ge\mathcal{G}$ between the arithmetic and geometric means; if I am right, the difference or divergence between these two means relates to the Shannon entropy. But not so fast: the inequality of means is valid when both the arithmetic and geometric means have the same weights (either $\frac{1}{n}$ or $x_i$ )--which is not the case here. Any thoughts greatly appreciated.","Economist here. In my current research I often encounter the concepts of entropy and means, and I find some connections interesting (for my purposes). The context is a set of nonnegative numbers , which can be normalized by the sum of the 's to get . These 's serve as weights in the context of means, and they serve as probabilities out of which entropy can be calculated. I find relationships between entropy and means, particularly between Questions: In general, how are entropy and means related? I can't think I'm the first one to notice. The only thing I can think of is the inequality between the arithmetic and geometric means; if I am right, the difference or divergence between these two means relates to the Shannon entropy. But not so fast: the inequality of means is valid when both the arithmetic and geometric means have the same weights (either or )--which is not the case here. Any thoughts greatly appreciated.","n \{a_1,\dots,a_n\} a_i \{x_1,\dots,x_n\} x_i \begin{align*}
\mathcal{A}\left(a_{i},\frac{1}{n}\right) & =\frac{1}{n}\sum a_{i} & \underset{\textrm{(i.e. equal weights }\frac{1}{n})}{\textrm{unweighted arithmetic mean}}\\
\mathcal{G}\left(a_{i},x_{i}\right) & =\prod a_{i}^{x_{i}} & \underset{\textrm{(with weights }x_{i}\textrm{ summing to one)}}{\textrm{weighted geometric mean}}\\
H(x_{i}) & =-\sum_{i=1}^{n}x_{i}\ln x_{i} & \underset{\textrm{(same }x_{i}\textrm{ as above, i.e. probabilities)}}{\textrm{Shannon entropy}}
\end{align*} \mathcal{A}\ge\mathcal{G} \frac{1}{n} x_i","['statistics', 'means', 'entropy']"
48,"Number of times for which a random process reaches a ""trough""","Number of times for which a random process reaches a ""trough""",,"Let $W_t$ be the Wiener process (Brownian motion). I am interested in the following distributions/probabilities What is the expected number of intervals on $[0,1]$ on which $W_t<a$ , where $a<0$ is a given number? More precisely: since $W_t$ is continuous in $t$ , the set $U=\{t\in[0,1]:W_t<a\}$ is open. Therefore, $U$ can be expressed as at most countably many open intervals. Write $U=\bigcup_{k=1}^n B_k$ , where $B_k$ are disjoint open intervals. We may have $n=\infty$ . My question is: what is the expected value of $n$ ? What distribution does $n$ follow? Let $m$ be number of intervals in $\{B_k\}$ with length at least $b$ . $m$ is always finite. What is the expected value of $m$ ? I do not find these in my random process book, but I think they are quite natural questions to ask. I would really appreciate it, if anyone could provide me with some reference about those problems, or write an answer to them.","Let be the Wiener process (Brownian motion). I am interested in the following distributions/probabilities What is the expected number of intervals on on which , where is a given number? More precisely: since is continuous in , the set is open. Therefore, can be expressed as at most countably many open intervals. Write , where are disjoint open intervals. We may have . My question is: what is the expected value of ? What distribution does follow? Let be number of intervals in with length at least . is always finite. What is the expected value of ? I do not find these in my random process book, but I think they are quite natural questions to ask. I would really appreciate it, if anyone could provide me with some reference about those problems, or write an answer to them.","W_t [0,1] W_t<a a<0 W_t t U=\{t\in[0,1]:W_t<a\} U U=\bigcup_{k=1}^n B_k B_k n=\infty n n m \{B_k\} b m m","['statistics', 'stochastic-processes', 'brownian-motion']"
49,Pseudo Inverses of covariance matrices that are close to each other,Pseudo Inverses of covariance matrices that are close to each other,,"I am trying to understand inverses of covariance matrices that are close to each other. I start with a positive semidefinite matrix $\mathbf{\Sigma^{-1}}$ , then I take the pseudo-inverse to get covariance matrix $\mathbf{\Sigma}$ . Then this is transform into correlation matrix, $$\mathbf{R = D^{-1}\ \Sigma\ D^{-1}} \ \ \text{where}\ \ \mathbf{D} =  \sqrt{\operatorname{diag}(\mathbf{\Sigma})}$$ and then I generate correlated random variables $\mathbf{x}$ which have correlation matrix $\mathbf{R^{'}}$ , which is not exact same as $\mathbf{R}$ but is very close. I then convert $\mathbf{R^{'}}$ as above to get $\mathbf{\Sigma^{'}}$ which is close to original $\mathbf{\Sigma}$ . I am trying now to invert this $\mathbf{\Sigma^{'}}$ to get the $\mathbf{\Sigma^{'-1}}$ which I would expect is good approximation of $\mathbf{\Sigma}$ . However when I actually perform this procedure, I get a $\mathbf{\Sigma^{'}}$ that is indeed close to $\mathbf{\Sigma}$ but the inverse $\mathbf{\Sigma^{'-1}}$ is completely different from $\mathbf{\Sigma^{-1}}$ . It is not only the non-zero entries that are different, the pattern of zero is also completely different. I am doing the above to test out part of an algorithm for coursework assignment, but am not able to proceed because $\mathbf{\Sigma^{'-1}}$ is clearly not even close to $\mathbf{\Sigma^{-1}}$ . I understand from other questions on this site that it is not guaranteed that matrices close to each other will always produce inverses that is close. In this case is there procedure to invert $\mathbf{\Sigma^{'}}$ which will give a good approximation for $\mathbf{\Sigma^{-1}}$ ?","I am trying to understand inverses of covariance matrices that are close to each other. I start with a positive semidefinite matrix , then I take the pseudo-inverse to get covariance matrix . Then this is transform into correlation matrix, and then I generate correlated random variables which have correlation matrix , which is not exact same as but is very close. I then convert as above to get which is close to original . I am trying now to invert this to get the which I would expect is good approximation of . However when I actually perform this procedure, I get a that is indeed close to but the inverse is completely different from . It is not only the non-zero entries that are different, the pattern of zero is also completely different. I am doing the above to test out part of an algorithm for coursework assignment, but am not able to proceed because is clearly not even close to . I understand from other questions on this site that it is not guaranteed that matrices close to each other will always produce inverses that is close. In this case is there procedure to invert which will give a good approximation for ?",\mathbf{\Sigma^{-1}} \mathbf{\Sigma} \mathbf{R = D^{-1}\ \Sigma\ D^{-1}} \ \ \text{where}\ \ \mathbf{D} =  \sqrt{\operatorname{diag}(\mathbf{\Sigma})} \mathbf{x} \mathbf{R^{'}} \mathbf{R} \mathbf{R^{'}} \mathbf{\Sigma^{'}} \mathbf{\Sigma} \mathbf{\Sigma^{'}} \mathbf{\Sigma^{'-1}} \mathbf{\Sigma} \mathbf{\Sigma^{'}} \mathbf{\Sigma} \mathbf{\Sigma^{'-1}} \mathbf{\Sigma^{-1}} \mathbf{\Sigma^{'-1}} \mathbf{\Sigma^{-1}} \mathbf{\Sigma^{'}} \mathbf{\Sigma^{-1}},"['linear-algebra', 'statistics', 'numerical-linear-algebra', 'pseudoinverse']"
50,Is the following distribution a form of another well-known one?,Is the following distribution a form of another well-known one?,,"In my research, I derived the following distribution. However, it does not seem to resemble the form of any well-known distributions. I wanted to see if anyone is seeing something I am not or if I am probably correct. Thanks for taking a look! \begin{equation} p(X=x)  = \left(1-e^{-x^2/2}\right)    e^{-x+\mathrm{erf}\left(x/\sqrt{2}\right) \sqrt{\pi/2}}    I_{[0,\infty)}(x) \end{equation}","In my research, I derived the following distribution. However, it does not seem to resemble the form of any well-known distributions. I wanted to see if anyone is seeing something I am not or if I am probably correct. Thanks for taking a look!","\begin{equation}
p(X=x)
 = \left(1-e^{-x^2/2}\right)
   e^{-x+\mathrm{erf}\left(x/\sqrt{2}\right) \sqrt{\pi/2}}
   I_{[0,\infty)}(x)
\end{equation}","['probability', 'statistics', 'probability-distributions']"
51,Intuition of law of total variance.,Intuition of law of total variance.,,"General question: The law of total variance is given by $Var(Y)=E[Var(Y|X)]+Var(E[Y|X])$ Question: I know variance is how spread out variables are, but the formula doesn't make sense intuitively, can some one explain from the ground up? Thanks.","General question: The law of total variance is given by Question: I know variance is how spread out variables are, but the formula doesn't make sense intuitively, can some one explain from the ground up? Thanks.",Var(Y)=E[Var(Y|X)]+Var(E[Y|X]),"['probability', 'probability-theory', 'statistics']"
52,Construct a random walk on $\mathbb{R}$,Construct a random walk on,\mathbb{R},"Construct a random walk on $\mathbb{R}$ such that it is returnable and the set of its values is everywhere dense in $\mathbb{R}$ . I think that random walk $S_{n} = \xi_{1} + ... + \xi_{n}$ , where random variable $\xi$ with values $\frac{1}{n}$ , $n \in \mathbb{Z}, n \neq 0 $ and $P(\xi = \frac{1}{n}) = \frac{p(1-p)^{n-1}}{2} , P(\xi = -\frac{1}{n}) = \frac{p(1-p)^{n-1}}{2}$ , could be the solution of this task. But I don't know how to prove that it's returnable.","Construct a random walk on such that it is returnable and the set of its values is everywhere dense in . I think that random walk , where random variable with values , and , could be the solution of this task. But I don't know how to prove that it's returnable.","\mathbb{R} \mathbb{R} S_{n} = \xi_{1} + ... + \xi_{n} \xi \frac{1}{n} n \in \mathbb{Z}, n \neq 0  P(\xi = \frac{1}{n}) = \frac{p(1-p)^{n-1}}{2} , P(\xi = -\frac{1}{n}) = \frac{p(1-p)^{n-1}}{2}","['probability', 'statistics', 'stochastic-processes', 'random-walk']"
53,Approximate confidence interval for MLE,Approximate confidence interval for MLE,,"This is a question in a practice exam handed out by my professor in preparation for the final. Suppose that $\{X_i\}_{i=1}^{n}$ , are i.i.d. with common density $f(X; \theta) = \theta X^{\theta1}I(0 < x < 1)$ where $\theta \in (0, \infty)$ Assume that n is large. Derive an approximate 95% confidence interval for $\theta$ based on the m.l.e estimate. $\textbf{Step 1:}$ Find MLE. $$\prod_{i=1}^{n}f(X_i;\theta)=\theta^{n} \prod_{i=1}^{n}f(X_i;\theta)X^{\theta1}I(0 < x < 1)$$ I derived the MLE to be $\hat{\theta}=\frac{n}{-\sum_{i=1}^{n}ln(X_i)}$ . Define $S_i=-ln(X_i)$ then $\hat{\theta}=\frac{1}{\bar{S}}$ $\textbf{Step 2:}$ Find Distribution of MLE. $$S_i\sim exp(\theta)\Rightarrow\sum_{i=1}^{n}S_i\sim gam(n,\theta)\Rightarrow\frac{1}{\sum_{i=1}^{n}S_i}\sim InvGama(n,\theta)$$ $\textbf{Step 3:}$ Find Asymptotic distribution of MLE. By CLT $$\sqrt{n}(\bar{S}-\frac{1}{\theta})\stackrel{\text{d}}{\rightarrow}N(0,\frac{1}{\theta^2})$$ Consider: $$h(t)=\frac{1}{t} \hspace{4mm} \text{and} \hspace{4mm} h^{'}(t)=-\frac{1}{t^2}$$ Since $h^{'}(\frac{1}{\theta})=-\theta^2\neq0$ by assumption $\theta\in(0,\infty)$ By the delta method: $$\sqrt{n}(h(\bar{S})-h\bigg(\frac{1}{\theta}\bigg))\stackrel{\text{d}}{\rightarrow}N(0,\frac{1}{\theta^2}*\bigg[h^{'}\bigg(\frac{1}{\theta}\bigg)\bigg]^2)=N(0,\theta^2)$$ Then for large enough n, $$P\bigg(Z_{\frac{\alpha}{2}}<\frac{\sqrt{n}(\frac{1}{\bar{S}}-\theta)}{\theta}<Z_{1-\frac{\alpha}{2}}\bigg)\approx 1-\alpha$$ Then the confidence interval is( $Z$ represent the quantiles of a standard normal. $$\bigg(\frac{\sqrt{n}}{(Z_{1-\frac{\alpha}{2}}+\sqrt{n})\bar{S}},\frac{\sqrt{n}}{(Z_{\frac{\alpha}{2}}+\sqrt{n})\bar{S}}\bigg)$$ Is my solution correct? Is there an easier way to approach this problem?","This is a question in a practice exam handed out by my professor in preparation for the final. Suppose that , are i.i.d. with common density where Assume that n is large. Derive an approximate 95% confidence interval for based on the m.l.e estimate. Find MLE. I derived the MLE to be . Define then Find Distribution of MLE. Find Asymptotic distribution of MLE. By CLT Consider: Since by assumption By the delta method: Then for large enough n, Then the confidence interval is( represent the quantiles of a standard normal. Is my solution correct? Is there an easier way to approach this problem?","\{X_i\}_{i=1}^{n} f(X; \theta) = \theta X^{\theta1}I(0 < x < 1) \theta \in (0, \infty) \theta \textbf{Step 1:} \prod_{i=1}^{n}f(X_i;\theta)=\theta^{n} \prod_{i=1}^{n}f(X_i;\theta)X^{\theta1}I(0 < x < 1) \hat{\theta}=\frac{n}{-\sum_{i=1}^{n}ln(X_i)} S_i=-ln(X_i) \hat{\theta}=\frac{1}{\bar{S}} \textbf{Step 2:} S_i\sim exp(\theta)\Rightarrow\sum_{i=1}^{n}S_i\sim gam(n,\theta)\Rightarrow\frac{1}{\sum_{i=1}^{n}S_i}\sim InvGama(n,\theta) \textbf{Step 3:} \sqrt{n}(\bar{S}-\frac{1}{\theta})\stackrel{\text{d}}{\rightarrow}N(0,\frac{1}{\theta^2}) h(t)=\frac{1}{t} \hspace{4mm} \text{and} \hspace{4mm} h^{'}(t)=-\frac{1}{t^2} h^{'}(\frac{1}{\theta})=-\theta^2\neq0 \theta\in(0,\infty) \sqrt{n}(h(\bar{S})-h\bigg(\frac{1}{\theta}\bigg))\stackrel{\text{d}}{\rightarrow}N(0,\frac{1}{\theta^2}*\bigg[h^{'}\bigg(\frac{1}{\theta}\bigg)\bigg]^2)=N(0,\theta^2) P\bigg(Z_{\frac{\alpha}{2}}<\frac{\sqrt{n}(\frac{1}{\bar{S}}-\theta)}{\theta}<Z_{1-\frac{\alpha}{2}}\bigg)\approx 1-\alpha Z \bigg(\frac{\sqrt{n}}{(Z_{1-\frac{\alpha}{2}}+\sqrt{n})\bar{S}},\frac{\sqrt{n}}{(Z_{\frac{\alpha}{2}}+\sqrt{n})\bar{S}}\bigg)","['statistics', 'statistical-inference', 'confidence-interval']"
54,Expectation of product of matrix from iid gaussian,Expectation of product of matrix from iid gaussian,,"I have a matrix X which is of size $(5,25)$ and the elements of the matrix are drawn from the standard normal distribution. I need to derive $\mathbb E(X'X)$ i.e. the expectation of the product of $X$ times of itself. Can you all please help me out. Thanks!",I have a matrix X which is of size and the elements of the matrix are drawn from the standard normal distribution. I need to derive i.e. the expectation of the product of times of itself. Can you all please help me out. Thanks!,"(5,25) \mathbb E(X'X) X","['linear-algebra', 'probability', 'matrices', 'statistics', 'expected-value']"
55,Vapnik-Chervonenkis inequalities,Vapnik-Chervonenkis inequalities,,"I kown of two Vapnik-Chervonenkis inequalities. The original one, by Vapnik and Chervonenkis, $$P\left(\sup_{A \in \mathcal{A}} \left |\nu_n(A) - \nu(A) \right | >\varepsilon \right) \leq 8\, S(\mathcal{A},n)\, e^{-n\varepsilon^2/32}$$ and the following (I am not sure whom to credit for - I read it in Devroye,Lugosi Combinatorial methods in density estimation ) $$\mathbb{E}\left[\sup_{A \in \mathcal{A}} \left |\nu_n(A) - \nu(A) \right | \right] \leq 2 \sqrt{\dfrac{\log 2\,S(\mathcal{A},n) + \log 2}{n}}$$ How do they compare?","I kown of two Vapnik-Chervonenkis inequalities. The original one, by Vapnik and Chervonenkis, and the following (I am not sure whom to credit for - I read it in Devroye,Lugosi Combinatorial methods in density estimation ) How do they compare?","P\left(\sup_{A \in \mathcal{A}} \left |\nu_n(A) - \nu(A) \right | >\varepsilon \right) \leq 8\, S(\mathcal{A},n)\, e^{-n\varepsilon^2/32} \mathbb{E}\left[\sup_{A \in \mathcal{A}} \left |\nu_n(A) - \nu(A) \right | \right] \leq 2 \sqrt{\dfrac{\log 2\,S(\mathcal{A},n) + \log 2}{n}}","['probability', 'probability-theory', 'statistics', 'machine-learning']"
56,Is it possible to find the distribution of $\frac{X_1-\bar{X}}{s^2}$?,Is it possible to find the distribution of ?,\frac{X_1-\bar{X}}{s^2},"Let $X_i \sim N(\mu,\sigma^2)$ where both $\mu,\sigma$ are unknown. Is it possible to find the distribution of $\frac{X_1-\bar{X}}{s^2}$ , where $s^2$ is the sample variance. It seems difficult to guess the nature of the distribution as $X_1-\bar{X} $ has a normal distribution while $s^2$ has chi-squared distribution and both are not independent. But is there a way to compute the distribution?","Let where both are unknown. Is it possible to find the distribution of , where is the sample variance. It seems difficult to guess the nature of the distribution as has a normal distribution while has chi-squared distribution and both are not independent. But is there a way to compute the distribution?","X_i \sim N(\mu,\sigma^2) \mu,\sigma \frac{X_1-\bar{X}}{s^2} s^2 X_1-\bar{X}  s^2","['statistics', 'probability-distributions', 'normal-distribution']"
57,Fisher's exact test two sided confusion,Fisher's exact test two sided confusion,,"Consider the guessing milk tea example $$\begin{array}{c|c|c|}   & \text{Guess Milk} & \text{Guess Tea} \\ \hline \text{Milk} & 3 & 1 \\ \hline \text{Tea} & 1 & 3 \\ \hline \end{array}$$ I want to test that $H_0: \theta  = 1$ (independent) vs. $H_a: \theta  \neq  1$ (associated) The formula for $P(n_{11} = t) = \frac{{n_{1+} \choose t} {n_{2+} \choose n_{+1}-t}}{{n \choose n_{+1}}}$ It's fixed on $n_{11}$ The range is given by $m_- = max(0, n_{11} - n_{22}) = max(0,0) = 0$ and $m_+ = min(n_{11}+n_{12}, n_{11}+n_{21}) = min(4,4) = 4$ . $$0 \leq n_{11} \leq 4$$ This is what my notes did after that $P(n_{11} = 0) = 0.0143, P(n_{11} = 1) = 0.2285, P(n_{11} = 2) = 0.5143, P(n_{11} = 3) = 0.2285, P(n_{11} = 4) = 0.0143$ Thus, the two sided p-value is $$P(n_{11} = 0) + P(n_{11} = 1) + P(n_{11} = 3) + P(n_{11} = 4) = 0.4857$$ . Why was $P(n_{11} = 2)$ excluded ?","Consider the guessing milk tea example I want to test that (independent) vs. (associated) The formula for It's fixed on The range is given by and . This is what my notes did after that Thus, the two sided p-value is . Why was excluded ?","\begin{array}{c|c|c|} 
 & \text{Guess Milk} & \text{Guess Tea} \\ \hline
\text{Milk} & 3 & 1 \\ \hline
\text{Tea} & 1 & 3 \\ \hline
\end{array} H_0: \theta  = 1 H_a: \theta  \neq  1 P(n_{11} = t) = \frac{{n_{1+} \choose t} {n_{2+} \choose n_{+1}-t}}{{n \choose n_{+1}}} n_{11} m_- = max(0, n_{11} - n_{22}) = max(0,0) = 0 m_+ = min(n_{11}+n_{12}, n_{11}+n_{21}) = min(4,4) = 4 0 \leq n_{11} \leq 4 P(n_{11} = 0) = 0.0143, P(n_{11} = 1) = 0.2285, P(n_{11} = 2) = 0.5143, P(n_{11} = 3) = 0.2285, P(n_{11} = 4) = 0.0143 P(n_{11} = 0) + P(n_{11} = 1) + P(n_{11} = 3) + P(n_{11} = 4) = 0.4857 P(n_{11} = 2)","['statistics', 'hypothesis-testing']"
58,Infinite monkey theorem independent of number of monkeys,Infinite monkey theorem independent of number of monkeys,,"I was just thinking about the infinite monkey theorem and arrived at a creepy conclussion. Let's begin with the probability that one monkey types a set of $N$ letters with a chosen order, which is given by $P = \left( \frac{1}{N_k} \right)^N$ , being $N_k$ the number of keys available in a hypotetical typewriter (for example, the probability of a monkey typing ""MAMA"" in a 26-key typewriter would be $P = \left( \frac{1}{26} \right)^4 \simeq 2\times 10^{-6}$ ). If we have $N_m$ monkeys typewriting, we could think that each monkey $i$ would be required to type only a fraction $N_i = N/N_m$ of the $N$ ordered letters, so that we could arrange later the $N_m$ fractions and create the $N$ -letters original sequence. The total probability of every monkey achieving its task is given by the product of each monkey's probability, $P =  \left( \left( \frac{1}{N_k} \right)^{N_i} \right) ^{N_m} =  \left( \frac{1}{N_k} \right)^N$ , which is independent of $N_m$ . UPDATE: Now I know I have done nothing wrong, but I would like to derive the relation between $P$ and $N_m$ in order to analytically see how the $P$ improves as $N_m$ gets bigger.","I was just thinking about the infinite monkey theorem and arrived at a creepy conclussion. Let's begin with the probability that one monkey types a set of letters with a chosen order, which is given by , being the number of keys available in a hypotetical typewriter (for example, the probability of a monkey typing ""MAMA"" in a 26-key typewriter would be ). If we have monkeys typewriting, we could think that each monkey would be required to type only a fraction of the ordered letters, so that we could arrange later the fractions and create the -letters original sequence. The total probability of every monkey achieving its task is given by the product of each monkey's probability, , which is independent of . UPDATE: Now I know I have done nothing wrong, but I would like to derive the relation between and in order to analytically see how the improves as gets bigger.",N P = \left( \frac{1}{N_k} \right)^N N_k P = \left( \frac{1}{26} \right)^4 \simeq 2\times 10^{-6} N_m i N_i = N/N_m N N_m N P =  \left( \left( \frac{1}{N_k} \right)^{N_i} \right) ^{N_m} =  \left( \frac{1}{N_k} \right)^N N_m P N_m P N_m,"['combinatorics', 'statistics', 'combinations']"
59,Multinomial Maximum Likelihood Estimation,Multinomial Maximum Likelihood Estimation,,"Let $(X_1, X_2, X_3, X_4)$ be a random vector from a multinomial $$\left[n,\frac{1-2\theta+\theta^2}{5},\frac{\theta(2-\theta)}{5},\frac{\theta(2-\theta)}{5},\frac{(1-\theta)^2}{5}\right]$$ find the ML estimate of $\theta$ This is a homework problem, and I don't want the answer, but I'm hoping for guidance on how to obtain the ML. The trouble I am running into is after taking the partial derivative wrt $\theta$ and attempting to solve for the parameter. I get a monster of an equation and I'm unable to algebraically isolate $\theta$ . If anyone has run into a problem like this before, could you please provide insight on a different way to obtain the parameter estimate? Thanks!","Let be a random vector from a multinomial find the ML estimate of This is a homework problem, and I don't want the answer, but I'm hoping for guidance on how to obtain the ML. The trouble I am running into is after taking the partial derivative wrt and attempting to solve for the parameter. I get a monster of an equation and I'm unable to algebraically isolate . If anyone has run into a problem like this before, could you please provide insight on a different way to obtain the parameter estimate? Thanks!","(X_1, X_2, X_3, X_4) \left[n,\frac{1-2\theta+\theta^2}{5},\frac{\theta(2-\theta)}{5},\frac{\theta(2-\theta)}{5},\frac{(1-\theta)^2}{5}\right] \theta \theta \theta","['statistics', 'maximum-likelihood', 'multinomial-distribution']"
60,Necessary and sufficient condition for two components of least square estimate to be negatively correlated,Necessary and sufficient condition for two components of least square estimate to be negatively correlated,,"Consider a linear model in a matrix form $Y = X\beta + \epsilon$ where $Y$ is a response vector, $X$ is a $n$ by $p$ ( $p < n$ ) full rank matrix of predictors, $\beta$ is a parameter vector, and $\epsilon$ is an error vector whose components are i.i.d. with a variance of $\sigma^2 > 0$ . (a) Derive the variance-covariance matrix of $\hat{\beta}$ , the least square estimator of $\beta$ . (b) Let $X$ be given by the matrix \begin{bmatrix} 1 & x_1\\ 1 & x_2\\ . & .\\ . & .\\ . & .\\ 1 & x_n \end{bmatrix} Determine the necessary and sufficient condition for the two components of $\hat{\beta}$ being negatively correlated. Here are my thoughts so far: Part (a) has been asked on this network quite a bit, so sparing the details, one can work out that the variance-covariance matrix of $\hat{\beta}$ is  given by $(X'X)^{-1} \cdot \sigma^2$ (where the prime symbol here represents the transpose). Now, for part (b), let $\beta = (\beta_1, \beta_2)'$ , and let $\hat{\beta} = (\hat{\beta_1}, \hat{\beta_2})'$ be the LSE of $\beta$ . Then we have covariance matrix given by $(X'X)^{-1} \cdot \sigma^2 = \begin{bmatrix} n & \sum_{i = 1}^nx_i\\ \sum_{i = 1}^nx_i & \sum_{i = 1}^n x_i^2 \end{bmatrix}\sigma^2 = \begin{bmatrix} Var(\hat{\beta_1}) & Cov(\hat{\beta_1} \hat{\beta_2})\\ Cov(\hat{\beta_1} \hat{\beta_2}) & Var(\hat{\beta_2}) \end{bmatrix}\sigma^2$ Now, I believe that the two components $\hat{\beta_1}$ and $\hat{\beta_2}$ of $\hat{\beta}$ are negatively correlated if and only if $Cov(\hat{\beta_1}, \hat{\beta_2})$ is negative. Following this thought, since $\sigma^2$ is positive, this means that $\hat{\beta_1}$ and $\hat{\beta_2}$ are negatively correlated in our case if and only if $\sum_{i = 1}^n x_i < 0$ . Thus, the necessary and sufficient condition for the two components of $\hat{\beta}$ being negatively correlated is $\sum_{i = 1}^n x_i < 0$ . Is this a correct solution ? Or can I take this further to express a more desirable condition for my solution ? Thanks for your time !","Consider a linear model in a matrix form where is a response vector, is a by ( ) full rank matrix of predictors, is a parameter vector, and is an error vector whose components are i.i.d. with a variance of . (a) Derive the variance-covariance matrix of , the least square estimator of . (b) Let be given by the matrix Determine the necessary and sufficient condition for the two components of being negatively correlated. Here are my thoughts so far: Part (a) has been asked on this network quite a bit, so sparing the details, one can work out that the variance-covariance matrix of is  given by (where the prime symbol here represents the transpose). Now, for part (b), let , and let be the LSE of . Then we have covariance matrix given by Now, I believe that the two components and of are negatively correlated if and only if is negative. Following this thought, since is positive, this means that and are negatively correlated in our case if and only if . Thus, the necessary and sufficient condition for the two components of being negatively correlated is . Is this a correct solution ? Or can I take this further to express a more desirable condition for my solution ? Thanks for your time !","Y = X\beta + \epsilon Y X n p p < n \beta \epsilon \sigma^2 > 0 \hat{\beta} \beta X \begin{bmatrix}
1 & x_1\\
1 & x_2\\
. & .\\
. & .\\
. & .\\
1 & x_n
\end{bmatrix} \hat{\beta} \hat{\beta} (X'X)^{-1} \cdot \sigma^2 \beta = (\beta_1, \beta_2)' \hat{\beta} = (\hat{\beta_1}, \hat{\beta_2})' \beta (X'X)^{-1} \cdot \sigma^2 = \begin{bmatrix}
n & \sum_{i = 1}^nx_i\\
\sum_{i = 1}^nx_i & \sum_{i = 1}^n x_i^2
\end{bmatrix}\sigma^2 = \begin{bmatrix}
Var(\hat{\beta_1}) & Cov(\hat{\beta_1} \hat{\beta_2})\\
Cov(\hat{\beta_1} \hat{\beta_2}) & Var(\hat{\beta_2})
\end{bmatrix}\sigma^2 \hat{\beta_1} \hat{\beta_2} \hat{\beta} Cov(\hat{\beta_1}, \hat{\beta_2}) \sigma^2 \hat{\beta_1} \hat{\beta_2} \sum_{i = 1}^n x_i < 0 \hat{\beta} \sum_{i = 1}^n x_i < 0","['linear-algebra', 'statistics', 'variance', 'covariance', 'least-squares']"
61,Portmanteau Theorem in Multiple Dimensions,Portmanteau Theorem in Multiple Dimensions,,"I have a significant issue in trying to prove the claim highlighted in red (see picture below), which is in Van der Vaart's ""Asymptotic Statistics"" text, where we assume $X_n = (X_{n,1}, ..., X_{n,d})^T $ and $X = (X_1, ..., X_d)^T$ are both in $\mathbb{R}^d$ and $P(X\le x) = P(X_1 \leq x_1, ..., X_d \leq x_d)$ - elementwise vector inequalities. Let $a = (a_1, ..., a_d)^T$ and $b = (b_1, ..., b_d)^T$ be in the set of continuity points for $P(X \leq x)$ with $a_i < b_i \forall i$ . $I = \prod_{i=1}^d (a_i, b_i]$ in this case. If $d=1$ , the claim is trivial (as $P(X_n \in I) = P(X_n \leq b_1) - P(X_n \leq a_1)$ which converges to the required quantity by assumption), but for $d > 1$ , I am completely lost. My attempt at proving it is as follows: $ P(X_n \in I) = P(X_n \leq b, X_n > a)$ $= P(X_n \leq b) - P(X_n \leq b \cap (\cup_{i=1}^d \{X_{n,i} \leq a_i\}))$ $= P(X_n \leq b) - P(\cup_{i=1}^d \cap_{j=1}^d \{X_{n,j} \leq b_j \} \cap \{X_{n,i} \leq a_i\})$ $= P(X_n \leq b) - P(\cup_{i=1}^d \{X_{n,i} \leq a_i \} \cap_{j \ne i} \{X_{n,j} \leq b_j \} )$ (this is all just applying definitions) Obviously the first term converges to $P(X \leq b)$ but I have no clue how to show that the second term converges to $P(\cup_{i=1}^d \{X_{i} \leq a_i \} \cap_{j \ne i} \{X_{j} \leq b_j \} )$ Can anyone offer any help?","I have a significant issue in trying to prove the claim highlighted in red (see picture below), which is in Van der Vaart's ""Asymptotic Statistics"" text, where we assume and are both in and - elementwise vector inequalities. Let and be in the set of continuity points for with . in this case. If , the claim is trivial (as which converges to the required quantity by assumption), but for , I am completely lost. My attempt at proving it is as follows: (this is all just applying definitions) Obviously the first term converges to but I have no clue how to show that the second term converges to Can anyone offer any help?","X_n = (X_{n,1}, ..., X_{n,d})^T  X = (X_1, ..., X_d)^T \mathbb{R}^d P(X\le x) = P(X_1 \leq x_1, ..., X_d \leq x_d) a = (a_1, ..., a_d)^T b = (b_1, ..., b_d)^T P(X \leq x) a_i < b_i \forall i I = \prod_{i=1}^d (a_i, b_i] d=1 P(X_n \in I) = P(X_n \leq b_1) - P(X_n \leq a_1) d > 1  P(X_n \in I) = P(X_n \leq b, X_n > a) = P(X_n \leq b) - P(X_n \leq b \cap (\cup_{i=1}^d \{X_{n,i} \leq a_i\})) = P(X_n \leq b) - P(\cup_{i=1}^d \cap_{j=1}^d \{X_{n,j} \leq b_j \} \cap \{X_{n,i} \leq a_i\}) = P(X_n \leq b) - P(\cup_{i=1}^d \{X_{n,i} \leq a_i \} \cap_{j \ne i} \{X_{n,j} \leq b_j \} ) P(X \leq b) P(\cup_{i=1}^d \{X_{i} \leq a_i \} \cap_{j \ne i} \{X_{j} \leq b_j \} )","['probability', 'statistics', 'weak-convergence']"
62,Minimally Sufficient Statistics Partition Intuition,Minimally Sufficient Statistics Partition Intuition,,"I am trying to understand the intuitive idea of a minimally sufficient statistic. It is my understanding that a statistic $T$ is minimally sufficient for $\theta$ for a family of populations $X\sim P_\theta$ if any other sufficient statistic $S$ of $\theta$ is of the form $T=h(S)$ where $h$ is a borel measurable function. This makes sense in a data reduction sense as this will produce the smallest $\sigma$ -algebra generated by a sufficient statistic that is a subset of the $\sigma$ -algebra generated by $X$ . Where I get confused is in the sense of a partition by a minimally sufficient statistic. It is my understanding that any Statistic creates a partition of the support of $X$ , where the minimally sufficient partition produces classes where the probability within each class is independent of $\theta$ . What is so special about the minimally sufficient partition? I know that if we use the relation $X R Y$ iff $P_\theta(X)=G(X,Y)P_\theta(Y)$ where $G(X,Y)$ is independent of $\theta$ , equivalence classes are created, but why is an equivalent criterion for minimally sufficiency be that the ratio $G(X,Y)=\frac{P_\theta(X)}{P_\theta(Y)}$ does not depend on $\theta$ $\leftrightarrow T(X)=T(Y).$ How is minimally sufficient implied by this equivalent criterion? I understand the proof but I am looking more for an intuitive understanding.","I am trying to understand the intuitive idea of a minimally sufficient statistic. It is my understanding that a statistic is minimally sufficient for for a family of populations if any other sufficient statistic of is of the form where is a borel measurable function. This makes sense in a data reduction sense as this will produce the smallest -algebra generated by a sufficient statistic that is a subset of the -algebra generated by . Where I get confused is in the sense of a partition by a minimally sufficient statistic. It is my understanding that any Statistic creates a partition of the support of , where the minimally sufficient partition produces classes where the probability within each class is independent of . What is so special about the minimally sufficient partition? I know that if we use the relation iff where is independent of , equivalence classes are created, but why is an equivalent criterion for minimally sufficiency be that the ratio does not depend on How is minimally sufficient implied by this equivalent criterion? I understand the proof but I am looking more for an intuitive understanding.","T \theta X\sim P_\theta S \theta T=h(S) h \sigma \sigma X X \theta X R Y P_\theta(X)=G(X,Y)P_\theta(Y) G(X,Y) \theta G(X,Y)=\frac{P_\theta(X)}{P_\theta(Y)} \theta \leftrightarrow T(X)=T(Y).","['statistics', 'statistical-inference', 'data-analysis', 'sufficient-statistics']"
63,Normal distribution tail probability inequality,Normal distribution tail probability inequality,,I am trying to show that $$P(X>t)\leq \frac{1}{2}e^\frac{-t^2}{2}$$ for $t>0$ where $X$ is a standard normal random variable.  Perhaps this is simple.  I have been starting with $$ \int_{t}^{\infty} \frac{1}{\sqrt{2\pi}} e^\frac{-x^2}{2}dx \leq \int_{t}^{\infty}\frac{x}{t}\frac{1}{\sqrt{2\pi}}e^\frac{-x^2}{2}dx = \frac{1}{t\sqrt{2\pi}}e^\frac{-t^2}{2}$$ but this is not what I want...I am looking for a much stronger inequality.   Any help in the direction of getting the $1/2$?,I am trying to show that $$P(X>t)\leq \frac{1}{2}e^\frac{-t^2}{2}$$ for $t>0$ where $X$ is a standard normal random variable.  Perhaps this is simple.  I have been starting with $$ \int_{t}^{\infty} \frac{1}{\sqrt{2\pi}} e^\frac{-x^2}{2}dx \leq \int_{t}^{\infty}\frac{x}{t}\frac{1}{\sqrt{2\pi}}e^\frac{-x^2}{2}dx = \frac{1}{t\sqrt{2\pi}}e^\frac{-t^2}{2}$$ but this is not what I want...I am looking for a much stronger inequality.   Any help in the direction of getting the $1/2$?,,"['inequality', 'normal-distribution']"
64,Minimally Sufficient Statistics,Minimally Sufficient Statistics,,"I'm trying to find the minimally sufficient statistic where $\{X_i\}_{i=1}^{n}$ are iid from the following family of populations: $$P=\{U(0,\theta): \theta>0\}$$ I looked at the ratio of the likelihood of two observations in the support of the family of populations: $$\frac{\frac{1}{\theta}^n\chi_{(x_{(n)},\infty)}(\theta)}{\frac{1}{\theta}^n\chi_{(y_{(n)},\infty)}(\theta)}=\frac{\chi_{(x_{(n)},\infty)}(\theta)}{\chi_{(x_{(n)},\infty)}(\theta)}$$ This will not be a function of $\theta$ iff $X_{(n)}=Y_{(n)}$ . One can conclude that $X_{(n)}$ is a minimally sufficient statistic. I am questioning my answer because it is my understanding that the ratio cannot include $\theta$ , I have only  shown  is  that it will not  depend on $\theta$ if $X_{(n)}=Y_{(n)}$ . Is this the same or am I  missing anything?","I'm trying to find the minimally sufficient statistic where are iid from the following family of populations: I looked at the ratio of the likelihood of two observations in the support of the family of populations: This will not be a function of iff . One can conclude that is a minimally sufficient statistic. I am questioning my answer because it is my understanding that the ratio cannot include , I have only  shown  is  that it will not  depend on if . Is this the same or am I  missing anything?","\{X_i\}_{i=1}^{n} P=\{U(0,\theta): \theta>0\} \frac{\frac{1}{\theta}^n\chi_{(x_{(n)},\infty)}(\theta)}{\frac{1}{\theta}^n\chi_{(y_{(n)},\infty)}(\theta)}=\frac{\chi_{(x_{(n)},\infty)}(\theta)}{\chi_{(x_{(n)},\infty)}(\theta)} \theta X_{(n)}=Y_{(n)} X_{(n)} \theta \theta X_{(n)}=Y_{(n)}","['statistics', 'statistical-inference', 'order-statistics', 'sufficient-statistics']"
65,Forecasting time series,Forecasting time series,,"I am trying to solve the following problem. Let the time series $S_t$ , $t\in \{1,...N\}$ and consider its corresponding return $R_t$ defined as \begin{equation} R_t=\log(\frac{S_t}{S_{t-1}})=\mu_t+\sigma_t \end{equation} Let $\Psi=\{\Psi_t\}$ be a $\sigma$ -algebra where $\Psi_t=\sigma(\{S1,..,S_t\})=\sigma(\{R_1,..,R_t\})$ . The model is described as follow: \begin{equation} \mu_t=c+\theta_1R_{t-1}+\theta_2R_{t-2} \end{equation} \begin{equation} \sigma_t=\sigma_{t,1}+\sigma_{t,2} \end{equation} where \begin{equation}\sigma_{t,1}|\Psi_{t-1}\sim N(0,h) \;\;\;\;\;\;\;\sigma_{t,2}=\sum_{k=1}^{N_t}V_{t,k}-\lambda\phi \end{equation} with $N_t \sim Poiss(\lambda)$ and $V_{t,k} \sim N(\phi,\theta^2)$ i.i.d. for $k=1,2...$ . $\;\;$ So $E[\sigma_{t,2}|\Psi_{t-1}]=0$ . The model parameters are $\{c,\theta_1,\theta_2,h,\lambda,\phi,\theta\}$ and  I have already estimated them by maximizing the log-likelihood function. Now, I would like to compute a forecast for $\hat{S_t}=\hat{S_{t-1}}e^{\hat{R_t}}$ , $t\in\{N+1,...,N'\}$ with $N'>N$ , but I don't know how to do it. I have already tried taking conditional expectation $E[S_t|\Psi_{t-1}]$ but the result is a straight line and I think it is a too rough solution. Any kind of help would be appreciated.","I am trying to solve the following problem. Let the time series , and consider its corresponding return defined as Let be a -algebra where . The model is described as follow: where with and i.i.d. for . So . The model parameters are and  I have already estimated them by maximizing the log-likelihood function. Now, I would like to compute a forecast for , with , but I don't know how to do it. I have already tried taking conditional expectation but the result is a straight line and I think it is a too rough solution. Any kind of help would be appreciated.","S_t t\in \{1,...N\} R_t \begin{equation}
R_t=\log(\frac{S_t}{S_{t-1}})=\mu_t+\sigma_t
\end{equation} \Psi=\{\Psi_t\} \sigma \Psi_t=\sigma(\{S1,..,S_t\})=\sigma(\{R_1,..,R_t\}) \begin{equation}
\mu_t=c+\theta_1R_{t-1}+\theta_2R_{t-2}
\end{equation} \begin{equation}
\sigma_t=\sigma_{t,1}+\sigma_{t,2}
\end{equation} \begin{equation}\sigma_{t,1}|\Psi_{t-1}\sim N(0,h) \;\;\;\;\;\;\;\sigma_{t,2}=\sum_{k=1}^{N_t}V_{t,k}-\lambda\phi
\end{equation} N_t \sim Poiss(\lambda) V_{t,k} \sim N(\phi,\theta^2) k=1,2... \;\; E[\sigma_{t,2}|\Psi_{t-1}]=0 \{c,\theta_1,\theta_2,h,\lambda,\phi,\theta\} \hat{S_t}=\hat{S_{t-1}}e^{\hat{R_t}} t\in\{N+1,...,N'\} N'>N E[S_t|\Psi_{t-1}]","['probability', 'statistics', 'statistical-inference', 'conditional-probability']"
66,Log-Likelihood function of log-Normal distribution with right censored observations and regression,Log-Likelihood function of log-Normal distribution with right censored observations and regression,,"EXERCISE Find the log-likelihood function for the regression model of log-Normal Distribution considering the right-censored observations ATTEMPT: So, we know that the probability density function of log-normal distribution is: $$f(t)=\dfrac{1}{\sqrt{2^2}\cdot t}\cdot exp(-\frac{(lnt-)^2}{2^2})$$ At first, we have to show that $S(t)$ (reliability function) is in this form: $$S(t;x)=S_0(tg(x))$$ where $S_0$ is a basic function and we usually have $g(x)=exp(-'x)$ . In other words we have to show that we have an accelerated life model My first question is how can I show this, considering that I have to use the log-Normal distribution Afterwards, we take the log-likelihood function with right-censored observations $$L(,)=\prod\limits_{i=1}^n f(t_i)^{_i}S(t_i)^{1-_i} \Rightarrow l(,)=lnL(,)=\sum\limits_{i=1}^n( _ilnf(t_i)+(1-_i)lnS(t_i)	)$$ I also know that I have to use that in accelerated life models, we have that: $$lnT_x=_0+'x+$$ So, if $T_x$ follows Normal distribution, $lnT_x$ will follow log-Normal distribution. So I have to use that we speak about regression model but actually I do not have any idea how to use this. Can anyone give me a thorough solution/explanation of this problem because I do not have previous experience in this type of problems for any distribution. Thanks, in advance!","EXERCISE Find the log-likelihood function for the regression model of log-Normal Distribution considering the right-censored observations ATTEMPT: So, we know that the probability density function of log-normal distribution is: At first, we have to show that (reliability function) is in this form: where is a basic function and we usually have . In other words we have to show that we have an accelerated life model My first question is how can I show this, considering that I have to use the log-Normal distribution Afterwards, we take the log-likelihood function with right-censored observations I also know that I have to use that in accelerated life models, we have that: So, if follows Normal distribution, will follow log-Normal distribution. So I have to use that we speak about regression model but actually I do not have any idea how to use this. Can anyone give me a thorough solution/explanation of this problem because I do not have previous experience in this type of problems for any distribution. Thanks, in advance!","f(t)=\dfrac{1}{\sqrt{2^2}\cdot t}\cdot exp(-\frac{(lnt-)^2}{2^2}) S(t) S(t;x)=S_0(tg(x)) S_0 g(x)=exp(-'x) L(,)=\prod\limits_{i=1}^n f(t_i)^{_i}S(t_i)^{1-_i}
\Rightarrow l(,)=lnL(,)=\sum\limits_{i=1}^n( _ilnf(t_i)+(1-_i)lnS(t_i)	) lnT_x=_0+'x+ T_x lnT_x","['statistics', 'regression', 'reliability']"
67,Determinant as measure of overall orthogonality of columns,Determinant as measure of overall orthogonality of columns,,"My understanding is that the determinant of a matrix can be thought of as the volume of a parallelepiped formed by the column vectors and copies of them linked to themselves. From this I can conclude making the vectors longer, or more orthogonal should increase the determinant. So if were to divide each column separately by its length (to get rid of the effect from longer vectors) and take the determinant of the resulting matrix, it should give me a sort of overall orthogonality score  the closer to $1$ the more orthogonal all the column vectors are to one another. Is there a name for this technique/number? I could see it being useful in statistics where each column is a feature and rows are observations, but when I try to Google it I just get results about orthogonal matrices, the end case where the determinant is $1$ , rather than discussion of this sort of score.","My understanding is that the determinant of a matrix can be thought of as the volume of a parallelepiped formed by the column vectors and copies of them linked to themselves. From this I can conclude making the vectors longer, or more orthogonal should increase the determinant. So if were to divide each column separately by its length (to get rid of the effect from longer vectors) and take the determinant of the resulting matrix, it should give me a sort of overall orthogonality score  the closer to the more orthogonal all the column vectors are to one another. Is there a name for this technique/number? I could see it being useful in statistics where each column is a feature and rows are observations, but when I try to Google it I just get results about orthogonal matrices, the end case where the determinant is , rather than discussion of this sort of score.",1 1,"['linear-algebra', 'matrices', 'statistics', 'determinant', 'orthogonality']"
68,Hoeffding-Type Bounds for Noncentered Variables,Hoeffding-Type Bounds for Noncentered Variables,,"Hoeffding's Tail Bound is well-known for subgaussian variables. It can be written in the following way: Assume $X_i$ for $1\leq i\leq n$ satisfies: $$ \mathbb{P}(|X_i-\mu|>t)\leq 2\exp\left(\frac{-t^2}{2\sigma_i^2}\right) $$ Then for $t\geq 0$ , we have that: $$ \mathbb{P}(|\frac{\sum_i X_i}{n}-\mu|>t)\leq 2\exp\left(\frac{-n^2t^2}{2\sum_i \sigma^2}\right) $$ I am wondering if this can be generalized to the case when the constant is not the mean. i.e. assume that for $X_i$ we have the bounds: $$ \mathbb{P}(|X_i-a_i|>t)\leq 2\exp\left(\frac{-t^2}{2\sigma_i^2}\right) $$ Where $\mathbb{E}(X_i)\neq a_i$ . Then can we bound: $$ \mathbb{P}(|\frac{\sum_i X_i-a_i}{n}|>t) $$ similarly to Hoeffding's bound? I feel like I am missing something obvious but I am quite stuck.","Hoeffding's Tail Bound is well-known for subgaussian variables. It can be written in the following way: Assume for satisfies: Then for , we have that: I am wondering if this can be generalized to the case when the constant is not the mean. i.e. assume that for we have the bounds: Where . Then can we bound: similarly to Hoeffding's bound? I feel like I am missing something obvious but I am quite stuck.","X_i 1\leq i\leq n 
\mathbb{P}(|X_i-\mu|>t)\leq 2\exp\left(\frac{-t^2}{2\sigma_i^2}\right)
 t\geq 0 
\mathbb{P}(|\frac{\sum_i X_i}{n}-\mu|>t)\leq 2\exp\left(\frac{-n^2t^2}{2\sum_i \sigma^2}\right)
 X_i 
\mathbb{P}(|X_i-a_i|>t)\leq 2\exp\left(\frac{-t^2}{2\sigma_i^2}\right)
 \mathbb{E}(X_i)\neq a_i 
\mathbb{P}(|\frac{\sum_i X_i-a_i}{n}|>t)
","['probability', 'statistics', 'random-variables', 'distribution-tails']"
69,Show that $\operatorname{Pr}(Z-X \geq 0)$ converges to one,Show that  converges to one,\operatorname{Pr}(Z-X \geq 0),"Suppose that $V_i$ , for $i \in \mathbb{N}$ , are i.i.d. standard normal random variables and $Y_i = \sum_{k=1}^i V_k$ for $i \in \mathbb{N}$ with $Y_0 = 0$ . Let $X_n = (\sum_{i=1}^n V_i Y_{i-1})^2 Y_n^2$ and $Z_n = (\sum_{i=1}^n Y_{i-1}^2)^2$ . How can I show that $\lim_{n \rightarrow \infty} p(Z_n - X_n \geq 0) = 1$ ? From simulations, I have seen that this probability converges to 1 very fast. Intuitively, this makes sense because the we have $E[Z_n] = \Theta(n^4)$ and $\sigma(Z_n) = \Theta(n^4)$ but $E[X_n] = \Theta(n^3)$ and $\sigma(Z_n) = \Theta(n^3)$ . Chebyshev inequality implies that $X_n$ samples are mostly around $n^3$ but $Z_n$ samples are mostly around $n^4$ . Thus, there is a higher chance that $Z_n$ is greater than $X_n$ .","Suppose that , for , are i.i.d. standard normal random variables and for with . Let and . How can I show that ? From simulations, I have seen that this probability converges to 1 very fast. Intuitively, this makes sense because the we have and but and . Chebyshev inequality implies that samples are mostly around but samples are mostly around . Thus, there is a higher chance that is greater than .",V_i i \in \mathbb{N} Y_i = \sum_{k=1}^i V_k i \in \mathbb{N} Y_0 = 0 X_n = (\sum_{i=1}^n V_i Y_{i-1})^2 Y_n^2 Z_n = (\sum_{i=1}^n Y_{i-1}^2)^2 \lim_{n \rightarrow \infty} p(Z_n - X_n \geq 0) = 1 E[Z_n] = \Theta(n^4) \sigma(Z_n) = \Theta(n^4) E[X_n] = \Theta(n^3) \sigma(Z_n) = \Theta(n^3) X_n n^3 Z_n n^4 Z_n X_n,"['probability', 'statistics', 'normal-distribution', 'upper-lower-bounds', 'probability-limit-theorems']"
70,"Relationship between $\mathbb P_{x \sim \mathcal N(x_0,\sigma^2 I)}(x \in A)$ and distance of $x_0$ to $A$",Relationship between  and distance of  to,"\mathbb P_{x \sim \mathcal N(x_0,\sigma^2 I)}(x \in A) x_0 A","Let $A$ be a non-empty measurable subset of euclidean $\mathbb R^n$ and $x_0 \in \mathbb R^n\setminus A$ . Define $d(x_0,A) := \inf_{a \in A} d(x_0,a)$ , the distance of $x_0$ from $A$ . Finally, let $\sigma > 0$ . Question 1. Are there any interesting relationships (functional equalities, inequalities, etc.) between $d(x_0,A)$ and $p(x_0,A) := \mathbb P_{x \sim \mathcal N(x_0,\sigma^2I)}(x \in A)$ ? Notes I'm fine with partial answers to Question 1 which make ""reasonable"" assumptions on $A$ (convex, closed, smoothness, curvature, etc.) Ultimately, I'm interested in estimating $p(x_0,A)$ . The sought-for connection with $d(x_0,A)$ is just a rough guess (inspired by the case of half-spaces worked-out below). It may turn out that other quantities are petinent for the estimation are in fact, not $d(x_0,A)$ in general. Motivating example: half-spaces Say $b \in \mathbb R$ , $u \in \mathbb R^n$ with $\|u\| = 1$ , and $A:=\{x \in \mathbb R^n \mid x^Tu \le b\}$ , a half-space, and let $x_0 \in \mathbb R^n\setminus A$ . Note that $d(x_0,A)=x_0^Tu-b$ . Let $\Phi$ be the CDF of the standard 1D Gaussian distribution. Then one easily computes $$ \begin{split} p(x_0,A) &= \mathbb P_{x \sim \mathcal N(x_0,\sigma^2I)}(x \in A) = \mathbb P_{x \sim \mathcal N(x_0,\sigma^2I)}(x^Tu \le b)\\ &= \mathbb P_{z \sim \mathcal N(0,I)}(z^Tu \le (b-x_0^Tu)/\sigma)=\Phi(-d(x_0,A)/\sigma), \end{split} \tag{*} $$ or equivalently, $d(x_0,A) = -\sigma\Phi^{-1}(p(x_0,A))=\sigma\Phi^{-1}(1-p(x_0,A))$ . Edit: rough upper-bound on $p(x_0,A)$ via closed convex hulls Suppose $A$ is closed convex nonempty subset of $\mathbb R^n$ . Let $H_0$ be the half-space containing $A$ , bordered by the hyper-plane with outward normal  pointing in the direction of $x_0-a_0$ , where $a_0$ is the (unique!) point of $A$ such that $d(x_0,A) = d(x_0,a_0)$ . In other words, $$ H_0 := \{x \in \mathbb R^n \mid (x-a_0)^Tu_0 \le 0\} = \{x \in \mathbb R^n \mid x^Tu_0 \le b_0\}, $$ where $u_0 := (x_0-a_0)/d(x_0,A)$ , a unit vector in $\mathbb R^n$ and $b_0 := a_0^Tu_0 \in \mathbb R$ . Note that $d(x_0,A) = d(x_0,H_0) = x_0^Tu_0-b_0$ . Now, if $x \sim \mathcal N(x_0,\sigma^2 I)$ , we have $$ \mathbb P(x \in A) \le \mathbb P(x \in H_0) \overset{*}{=} \Phi(-d(x_0,A)/\sigma), $$ i.e $p(x_0,A) \le \Phi(-d(x_0,A)/\sigma)$ , or equivalent, $d(x_0,A) \ge \sigma\Phi^{-1}(1-p(x_0,A))$ . The following theorem follows. Lemma. Let $A$ be a nonempty measurable subset of $\mathbb R^n$ , and let $\overline{co}(A)$ denote is the closure of it's convex hall. Let $x_0 \in \mathbb R^n\setminus \overline{co}(A)$ . Then $$ d(x_0,A) \ge d(x_0,\overline{co}(A)) \ge \sigma\Phi^{-1}(1-p(x_0,\overline{co}(A))). $$","Let be a non-empty measurable subset of euclidean and . Define , the distance of from . Finally, let . Question 1. Are there any interesting relationships (functional equalities, inequalities, etc.) between and ? Notes I'm fine with partial answers to Question 1 which make ""reasonable"" assumptions on (convex, closed, smoothness, curvature, etc.) Ultimately, I'm interested in estimating . The sought-for connection with is just a rough guess (inspired by the case of half-spaces worked-out below). It may turn out that other quantities are petinent for the estimation are in fact, not in general. Motivating example: half-spaces Say , with , and , a half-space, and let . Note that . Let be the CDF of the standard 1D Gaussian distribution. Then one easily computes or equivalently, . Edit: rough upper-bound on via closed convex hulls Suppose is closed convex nonempty subset of . Let be the half-space containing , bordered by the hyper-plane with outward normal  pointing in the direction of , where is the (unique!) point of such that . In other words, where , a unit vector in and . Note that . Now, if , we have i.e , or equivalent, . The following theorem follows. Lemma. Let be a nonempty measurable subset of , and let denote is the closure of it's convex hall. Let . Then","A \mathbb R^n x_0 \in \mathbb R^n\setminus A d(x_0,A) := \inf_{a \in A} d(x_0,a) x_0 A \sigma > 0 d(x_0,A) p(x_0,A) := \mathbb P_{x \sim \mathcal N(x_0,\sigma^2I)}(x \in A) A p(x_0,A) d(x_0,A) d(x_0,A) b \in \mathbb R u \in \mathbb R^n \|u\| = 1 A:=\{x \in \mathbb R^n \mid x^Tu \le b\} x_0 \in \mathbb R^n\setminus A d(x_0,A)=x_0^Tu-b \Phi 
\begin{split}
p(x_0,A) &= \mathbb P_{x \sim \mathcal N(x_0,\sigma^2I)}(x \in A) = \mathbb P_{x \sim \mathcal N(x_0,\sigma^2I)}(x^Tu \le b)\\
&= \mathbb P_{z \sim \mathcal N(0,I)}(z^Tu \le (b-x_0^Tu)/\sigma)=\Phi(-d(x_0,A)/\sigma),
\end{split}
\tag{*}
 d(x_0,A) = -\sigma\Phi^{-1}(p(x_0,A))=\sigma\Phi^{-1}(1-p(x_0,A)) p(x_0,A) A \mathbb R^n H_0 A x_0-a_0 a_0 A d(x_0,A) = d(x_0,a_0) 
H_0 := \{x \in \mathbb R^n \mid (x-a_0)^Tu_0 \le 0\} = \{x \in \mathbb R^n \mid x^Tu_0 \le b_0\},
 u_0 := (x_0-a_0)/d(x_0,A) \mathbb R^n b_0 := a_0^Tu_0 \in \mathbb R d(x_0,A) = d(x_0,H_0) = x_0^Tu_0-b_0 x \sim \mathcal N(x_0,\sigma^2 I) 
\mathbb P(x \in A) \le \mathbb P(x \in H_0) \overset{*}{=} \Phi(-d(x_0,A)/\sigma),
 p(x_0,A) \le \Phi(-d(x_0,A)/\sigma) d(x_0,A) \ge \sigma\Phi^{-1}(1-p(x_0,A)) A \mathbb R^n \overline{co}(A) x_0 \in \mathbb R^n\setminus \overline{co}(A) 
d(x_0,A) \ge d(x_0,\overline{co}(A)) \ge \sigma\Phi^{-1}(1-p(x_0,\overline{co}(A))).
","['probability', 'statistics', 'hypothesis-testing', 'convex-geometry']"
71,Bounded-Lipschitz (aka Dudley) metric between Gaussians,Bounded-Lipschitz (aka Dudley) metric between Gaussians,,"Let $X=(X,d)$ be a (Polish) metric space equipped with the Borel sigma-algebra, and let $\mathcal P(X)$ be the set of probability distributions thereupon. For a function , $f:X \rightarrow \mathbb R$ , let $\|f\|_{BL} := \|f\|_\infty + \|f\|_L$ , where $\|f\| := \sup_{x \in X}|f(x)|$ and $\|f\|_L := \underset{x,x' \in X,\;x' \ne x}{\sup}\; \frac{|f(x')-f(x)|}{d(x',x)}$ . The bounded-Lipschitz metric (aka Dudley metric ) on $\mathcal P(X)$ is defined by $$ d_{BL}(\mu,\nu) := \sup_{\|f\|_{BL} \le 1}\mathbb E_\mu[f] - \mathbb E_\nu [f]. $$ Now, let $X = \mathbb R^n$ equipped with the euclidean metric. Let $c_1,c_2 \in \mathbb R^n$ and $\Sigma_1,\Sigma_2$ be positive-definite matricies of size $n$ . Question 1. $d_{BL}(\mathcal N(c_1,\Sigma_1),\mathcal N(c_2,\Sigma_2)) = ?$ Question 2. Same question with $\Sigma_2 = \Sigma_1$ . Note that $$ d_{BL}(\mathcal N(c_1,\Sigma),\mathcal N(c_2,\Sigma)) \le W_1(\mathcal N(c_1,\Sigma),\mathcal N(c_2,\Sigma)) = \|c_1-c_2\|_2, $$ where $ W_1(\mu,\nu) := \sup_{\|f\|_L \le 1} \mathbb E_\mu[f] - \mathbb E_\nu[f]$ , is the Wasserstein distance between $\mu$ and $\nu$ .","Let be a (Polish) metric space equipped with the Borel sigma-algebra, and let be the set of probability distributions thereupon. For a function , , let , where and . The bounded-Lipschitz metric (aka Dudley metric ) on is defined by Now, let equipped with the euclidean metric. Let and be positive-definite matricies of size . Question 1. Question 2. Same question with . Note that where , is the Wasserstein distance between and .","X=(X,d) \mathcal P(X) f:X \rightarrow \mathbb R \|f\|_{BL} := \|f\|_\infty + \|f\|_L \|f\| := \sup_{x \in X}|f(x)| \|f\|_L := \underset{x,x' \in X,\;x' \ne x}{\sup}\; \frac{|f(x')-f(x)|}{d(x',x)} \mathcal P(X) 
d_{BL}(\mu,\nu) := \sup_{\|f\|_{BL} \le 1}\mathbb E_\mu[f] - \mathbb E_\nu [f].
 X = \mathbb R^n c_1,c_2 \in \mathbb R^n \Sigma_1,\Sigma_2 n d_{BL}(\mathcal N(c_1,\Sigma_1),\mathcal N(c_2,\Sigma_2)) = ? \Sigma_2 = \Sigma_1 
d_{BL}(\mathcal N(c_1,\Sigma),\mathcal N(c_2,\Sigma)) \le W_1(\mathcal N(c_1,\Sigma),\mathcal N(c_2,\Sigma))
= \|c_1-c_2\|_2,
  W_1(\mu,\nu) := \sup_{\|f\|_L \le 1} \mathbb E_\mu[f] - \mathbb E_\nu[f] \mu \nu","['probability', 'functional-analysis', 'statistics', 'reference-request', 'geometric-probability']"
72,"Is there a name for the property, such as for f-means, that the result lies in the closed interval between the minimum and maximum of the input set?","Is there a name for the property, such as for f-means, that the result lies in the closed interval between the minimum and maximum of the input set?",,"All the possible f-means (a broad generalization of the arithmetic mean ) have the property that the result is in the closed interval between min and max of the input set. I've found this property mentioned in several places, but it is never given a name. Is there a name for this? I'd consider ""convexity"" (since the result is a convex combination of the inputs), but that word already has a different meaning when it comes to functions.","All the possible f-means (a broad generalization of the arithmetic mean ) have the property that the result is in the closed interval between min and max of the input set. I've found this property mentioned in several places, but it is never given a name. Is there a name for this? I'd consider ""convexity"" (since the result is a convex combination of the inputs), but that word already has a different meaning when it comes to functions.",,"['statistics', 'functions', 'terminology', 'average', 'means']"
73,"Find UMVU estimator for P((0,1))","Find UMVU estimator for P((0,1))",,"Suppose a statistical model comprises all continuous distributions on $R^1$ . Based on $n$ samples $x_i$ , find a UMVU estimtor for $P((0,1))$ , where $P$ is the true distribution. I have three questions: In textbooks, I learned that a statistical model is denoted as $P_\theta$ , indexed by $\theta\in \Omega$ . Usually, $\theta$ is a scalar or vector value. In this question should I understand $\theta$ as an index for (uncountable) continuous distributions? In this case ( $\theta$ is not a value), do we have something like MLE for inferring $\theta$ ? Intuitively, the solution to the question is $\frac{\sum_i 1(x_i\in (0,1))}{n}$ . I can compute the variance ( $\frac{p(1-p)}{n}$ , where $p=P((0,1)$ ). How can I prove it's UMVU?","Suppose a statistical model comprises all continuous distributions on . Based on samples , find a UMVU estimtor for , where is the true distribution. I have three questions: In textbooks, I learned that a statistical model is denoted as , indexed by . Usually, is a scalar or vector value. In this question should I understand as an index for (uncountable) continuous distributions? In this case ( is not a value), do we have something like MLE for inferring ? Intuitively, the solution to the question is . I can compute the variance ( , where ). How can I prove it's UMVU?","R^1 n x_i P((0,1)) P P_\theta \theta\in \Omega \theta \theta \theta \theta \frac{\sum_i 1(x_i\in (0,1))}{n} \frac{p(1-p)}{n} p=P((0,1)","['statistics', 'maximum-likelihood']"
74,sample means with distributions that almost have a true mean,sample means with distributions that almost have a true mean,,"The Cauchy distribution, $\frac 1 {\pi(1+x^2)}$ , has no mean. The distribution of the sample mean $\bar X$ is the original Cauchy distribution so you end up where you started, no matter how many samples you take. This is in contract to the ""usual"" behavior where standard deviation of the sample mean scales as $\frac 1 {\sqrt N}$ . But the Cauchy distribution is close . Changing the distribution to $\frac C {(1+|x|^{2+\epsilon})}$ for any small $\epsilon>0$ gives X a true mean of zero which $\bar X$ converges to. But what about a smaller change? $X $ ~ $ \frac C {1+x^{2}ln|x|}$ . For large x , the 1 on the denominator becomes irrelevant: $\int \frac {Cx} {1+x^{2}ln|x|} \approx \int \frac {Cx} {x^{2}ln|x|} = C ln(ln(x)) + (const),\ \ x >>0$ . This grows much more slowly than $ln(x)$ but it still diverges, and we have no mean. But will the sample mean converge for this modified $X$ ? Convergence in this case would mean that the sample mean gets within a given $\epsilon$ of zero with probability 1 as $N$ goes to infinity.","The Cauchy distribution, , has no mean. The distribution of the sample mean is the original Cauchy distribution so you end up where you started, no matter how many samples you take. This is in contract to the ""usual"" behavior where standard deviation of the sample mean scales as . But the Cauchy distribution is close . Changing the distribution to for any small gives X a true mean of zero which converges to. But what about a smaller change? ~ . For large x , the 1 on the denominator becomes irrelevant: . This grows much more slowly than but it still diverges, and we have no mean. But will the sample mean converge for this modified ? Convergence in this case would mean that the sample mean gets within a given of zero with probability 1 as goes to infinity.","\frac 1 {\pi(1+x^2)} \bar X \frac 1 {\sqrt N} \frac C {(1+|x|^{2+\epsilon})} \epsilon>0 \bar X X   \frac C {1+x^{2}ln|x|} \int \frac {Cx} {1+x^{2}ln|x|} \approx \int \frac {Cx} {x^{2}ln|x|} = C ln(ln(x)) + (const),\ \ x >>0 ln(x) X \epsilon N","['statistics', 'convergence-divergence']"
75,Tetrahedral dice throw,Tetrahedral dice throw,,"If we threw $30$ tetrahedral dice and summed the outcomes, how many values in the    distribution would have a non-zero probability? a) If we calculate the distribution of the sum of two thrown tetrahedral dice, how many values have a non-zero probability?","If we threw tetrahedral dice and summed the outcomes, how many values in the    distribution would have a non-zero probability? a) If we calculate the distribution of the sum of two thrown tetrahedral dice, how many values have a non-zero probability?",30,['statistics']
76,Meaning of a term in stochastic gradient descent,Meaning of a term in stochastic gradient descent,,"This is in reference to the first two pages of Robbins-Monro ""A stochastic approximation method,"" https://projecteuclid.org/euclid.aoms/1177729586 . What is the meaning of the RHS in (8)? As I understand it, there is a measure space $(\Sigma,\mu)$ and $x_n$ and $y_n$ are real-valued measurable functions on $\Sigma.$ The LHS of (8) is a conditional probability, and so for each number $y$ it is a function whose domain is $x_n(\Sigma)\subset\mathbb{R}.$ However, for each number $x$ and $y$ , $H(y\mid x)$ is a number, and so it seems that for each number $y$ , the RHS $H(y\mid x_n)$ is a measurable function on $\Sigma.$ So I don't understand the meaning of the equality.","This is in reference to the first two pages of Robbins-Monro ""A stochastic approximation method,"" https://projecteuclid.org/euclid.aoms/1177729586 . What is the meaning of the RHS in (8)? As I understand it, there is a measure space and and are real-valued measurable functions on The LHS of (8) is a conditional probability, and so for each number it is a function whose domain is However, for each number and , is a number, and so it seems that for each number , the RHS is a measurable function on So I don't understand the meaning of the equality.","(\Sigma,\mu) x_n y_n \Sigma. y x_n(\Sigma)\subset\mathbb{R}. x y H(y\mid x) y H(y\mid x_n) \Sigma.","['probability', 'statistics', 'measure-theory', 'stochastic-processes', 'stochastic-approximation']"
77,Hypothesis testing of a binomial variable,Hypothesis testing of a binomial variable,,"There's this problem I'm trying to solve and I can't wrap my head around it; Let X be a binomial variable. The number $x_k$ of times we observed $k$ successes in 96 repetitions of the experiment is given by this table: $$\begin{array}{|c|c|} \hline \text{k} & 0 & 1 & 2 & 3 & 4 \\ \hline \text{$x_k$} & 5 & 22 & 35 & 28 & 6 \\ \hline \end{array}$$ Should we accept with $\alpha = 0.10$ the hypothesis $H_0$ that $X\sim Bin(4,\frac{1}{2})$ From what I understand, what we're testing is $$H_0: p=\frac{1}{2}    \space \space \space H_1: p \neq \frac{1}{2} \space\space\space when \space n=4$$ But I can't find what value I should test it with... Just from looking at the data it looks about right but... Any help would be greatly appreciated!","There's this problem I'm trying to solve and I can't wrap my head around it; Let X be a binomial variable. The number of times we observed successes in 96 repetitions of the experiment is given by this table: Should we accept with the hypothesis that From what I understand, what we're testing is But I can't find what value I should test it with... Just from looking at the data it looks about right but... Any help would be greatly appreciated!","x_k k \begin{array}{|c|c|} \hline
\text{k} & 0 & 1 & 2 & 3 & 4 \\ \hline
\text{x_k} & 5 & 22 & 35 & 28 & 6 \\ \hline
\end{array} \alpha = 0.10 H_0 X\sim Bin(4,\frac{1}{2}) H_0: p=\frac{1}{2}    \space \space \space H_1: p \neq \frac{1}{2} \space\space\space when \space n=4","['statistics', 'statistical-inference', 'binomial-distribution', 'hypothesis-testing']"
78,CDF of ratio of independent standard normal distributions,CDF of ratio of independent standard normal distributions,,"Question; Given two independent standard normally distributed random variables $X,Y,$ find the cumulative density function of $\frac{X}{Y}.$ My attempt: Note that \begin{align*} F(x) & = P(\frac{X}{Y}\leq x) \\ & = P(\frac{X}{Y}\leq x | Y >0)P(Y>0) + P(\frac{X}{Y}\leq x | Y<0) P(Y<0) \\ & = \frac{1}{2} \left[ P(X\leq xY | Y >0) + P(X\geq xY | Y<0) \right]. \end{align*} However, I have trouble evaluating $P(X\leq xY | Y >0).$ I know the final answer is that the distribution of $\frac{X}{Y}$ is Cauchy. I am trying to prove it by considering CDF and then differentiating CDF to obtain PDF.","Question; Given two independent standard normally distributed random variables find the cumulative density function of My attempt: Note that However, I have trouble evaluating I know the final answer is that the distribution of is Cauchy. I am trying to prove it by considering CDF and then differentiating CDF to obtain PDF.","X,Y, \frac{X}{Y}. \begin{align*}
F(x) & = P(\frac{X}{Y}\leq x) \\
& = P(\frac{X}{Y}\leq x | Y >0)P(Y>0) + P(\frac{X}{Y}\leq x | Y<0) P(Y<0) \\
& = \frac{1}{2} \left[ P(X\leq xY | Y >0) + P(X\geq xY | Y<0) \right].
\end{align*} P(X\leq xY | Y >0). \frac{X}{Y}","['probability', 'statistics', 'normal-distribution', 'cumulative-distribution-functions']"
79,What is the sufficient condition for the consistency and normality of MLE?,What is the sufficient condition for the consistency and normality of MLE?,,"What is the sufficient condition for the consistency and normality of MLE? I read several such regular conditions for the consistency and normality of MLE. The latest one I have read state is one the by Ferguson ""a course in the large sample"" Sufficient Condition 1 Let $X_1, X_2, \dots$ be i.i.d with density $f(x|\theta)$ (with respect t0 dv), and let $\theta_0$ denoate the true value of the parameter. If (1) $\Theta$ is an open subset of $R^k$ (2)second partial derivatives of $f(x|\theta)$ with respect to $\theta$ exist and are continuous for all x and may be passed under the integral sign in $\int f(x|\theta)dv(x)$ (3)There exists a function K(x) such that $E_{\theta_0}K(X)<\infty$ and each $\frac{d^2\theta f(x|\theta)}{d\theta^2 }$ is  bounded in absolute value by K(x) uniformly in some neigborhood of $\theta_0$ (4) $I(\theta_0)=-E_{\theta_0}\frac{d\theta f(x|\theta)}{d\theta}$ is positive definite (5) $f(x|\theta)=f(x|\theta_0)$ a.e.dv, thus $\theta=\theta_0$ Then there exists a strongly consistent sequence of $\hat{\theta}$ of roots of the likelihood equation such that $\sqrt{n}(\hat{\theta}_n-{\theta}_0)\rightarrow ^{d} N(0,I(\theta_0)^{-1})$ Can not make sure the condition (3)? The details of sufficient conditions are never stated in some elementary statistics courses. Some books may state that For consistency, some books may state that $(1)X=(X_1,X_2,\cdots,X_n)$ follows $ f(x|\theta)$ i.i.d with respect to u with some common support (2)The true parameter value $\theta_0$ is an interior point of the parameter space Similar situation also happens in asymptotic normality: They told me the MLE can be asymptotic normality or can be not? And then they just show some examples. I am not so good at real analysis and mathematics. So it is not easy for me to understand the proof in Ferguson. (Especially inconsistent part, it use Uniform Law of Large Number, and based on some semiconscious and compact assumption) Can anyone give much more simple read proof? And also a simple sufficient statement for asymptotic normality?  I know some advanced probability and measure theory. Though I am a beginner. I try to find a much clear statement for this consistency of MLE. One note from Stanford looks like that (see page 2) https://web.stanford.edu/class/archive/stats/stats200/stats200.1172/Lecture14.pdf Sufficient Condition 2 (1)all pdfs /pmfs $f(x|\theta)$ in the model have the same support, (2) $\theta_0$ is an interior point(not one the boundary) (3)The loglikelihood $l(\theta)$ is differentiable in $\theta$ (4) $\hat{\theta}$ is the unique value of $\theta \in \Omega$ that solves the equation $0=l'(\theta)$ My question: (1) It seems that in the sencond item of first sufficient condition :second partial derivatives of $f(x|\theta)$ with respect to $\theta$ exist and are continuous for all x and may be passed under the integral sign in $\int f(x|\theta)dv(x)$ This statement seems very important. Though I never find it in Sufficient Condition 2. (2) It can be shown that Let $X_1,\cdots, X_n$ follow $U(0,\theta)$ . The MLE is $\hat{\theta}=X_{(n)}$ . Though, $\hat{\theta}$ is asymptotic exp(1). Which items in sufficient conditions are not satisfied, both in sufficient conditions  one and two? Many thanks!","What is the sufficient condition for the consistency and normality of MLE? I read several such regular conditions for the consistency and normality of MLE. The latest one I have read state is one the by Ferguson ""a course in the large sample"" Sufficient Condition 1 Let be i.i.d with density (with respect t0 dv), and let denoate the true value of the parameter. If (1) is an open subset of (2)second partial derivatives of with respect to exist and are continuous for all x and may be passed under the integral sign in (3)There exists a function K(x) such that and each is  bounded in absolute value by K(x) uniformly in some neigborhood of (4) is positive definite (5) a.e.dv, thus Then there exists a strongly consistent sequence of of roots of the likelihood equation such that Can not make sure the condition (3)? The details of sufficient conditions are never stated in some elementary statistics courses. Some books may state that For consistency, some books may state that follows i.i.d with respect to u with some common support (2)The true parameter value is an interior point of the parameter space Similar situation also happens in asymptotic normality: They told me the MLE can be asymptotic normality or can be not? And then they just show some examples. I am not so good at real analysis and mathematics. So it is not easy for me to understand the proof in Ferguson. (Especially inconsistent part, it use Uniform Law of Large Number, and based on some semiconscious and compact assumption) Can anyone give much more simple read proof? And also a simple sufficient statement for asymptotic normality?  I know some advanced probability and measure theory. Though I am a beginner. I try to find a much clear statement for this consistency of MLE. One note from Stanford looks like that (see page 2) https://web.stanford.edu/class/archive/stats/stats200/stats200.1172/Lecture14.pdf Sufficient Condition 2 (1)all pdfs /pmfs in the model have the same support, (2) is an interior point(not one the boundary) (3)The loglikelihood is differentiable in (4) is the unique value of that solves the equation My question: (1) It seems that in the sencond item of first sufficient condition :second partial derivatives of with respect to exist and are continuous for all x and may be passed under the integral sign in This statement seems very important. Though I never find it in Sufficient Condition 2. (2) It can be shown that Let follow . The MLE is . Though, is asymptotic exp(1). Which items in sufficient conditions are not satisfied, both in sufficient conditions  one and two? Many thanks!","X_1, X_2, \dots f(x|\theta) \theta_0 \Theta R^k f(x|\theta) \theta \int f(x|\theta)dv(x) E_{\theta_0}K(X)<\infty \frac{d^2\theta f(x|\theta)}{d\theta^2 } \theta_0 I(\theta_0)=-E_{\theta_0}\frac{d\theta f(x|\theta)}{d\theta} f(x|\theta)=f(x|\theta_0) \theta=\theta_0 \hat{\theta} \sqrt{n}(\hat{\theta}_n-{\theta}_0)\rightarrow ^{d} N(0,I(\theta_0)^{-1}) (1)X=(X_1,X_2,\cdots,X_n)  f(x|\theta) \theta_0 f(x|\theta) \theta_0 l(\theta) \theta \hat{\theta} \theta \in \Omega 0=l'(\theta) f(x|\theta) \theta \int f(x|\theta)dv(x) X_1,\cdots, X_n U(0,\theta) \hat{\theta}=X_{(n)} \hat{\theta}","['statistics', 'measure-theory', 'statistical-inference']"
80,Correlation vs Derivative,Correlation vs Derivative,,"Is a statistical correlation and derivative (in calculus) the same thing? I ask because a teacher gave an example in which the derivative between happiness and coffee was positive. In other words, as coffee consumption goes up so should happiness. This sounds like a correlation but I've never heard anyone mention the connection between statistics and calculus.","Is a statistical correlation and derivative (in calculus) the same thing? I ask because a teacher gave an example in which the derivative between happiness and coffee was positive. In other words, as coffee consumption goes up so should happiness. This sounds like a correlation but I've never heard anyone mention the connection between statistics and calculus.",,"['calculus', 'statistics']"
81,Minimal sufficient statistic implies any complete statistic is also minimal sufficient,Minimal sufficient statistic implies any complete statistic is also minimal sufficient,,"I am reading about data reduction of Casella book. There is a theorem establishing the next: If a minimal sufficient statistic exists, then any complete statistic is also a minimal sufficient statistic. A proof is no provided by the author; is there a reference for a formal proof of this fact? Any kind of help is thanked in advanced.","I am reading about data reduction of Casella book. There is a theorem establishing the next: If a minimal sufficient statistic exists, then any complete statistic is also a minimal sufficient statistic. A proof is no provided by the author; is there a reference for a formal proof of this fact? Any kind of help is thanked in advanced.",,"['statistics', 'reference-request', 'statistical-inference']"
82,"Understanding of the definition ""statistical experiment"" (Blackwell, 1951)","Understanding of the definition ""statistical experiment"" (Blackwell, 1951)",,"I'm interesting for the understanding of the definition ""statistical experiment"". Formally statistical experiment , statistical model or just experiment was defined as a triple $$\mathscr{P}=(\Omega,\mathscr{F}, \{ P_{\theta}:\theta \in \Theta \}),$$ where $\Omega,\mathscr{F}$ is a sample space , $\Theta$ is a parameter spase and $\{ P_{\theta}:\theta \in \Theta \}$ is family of probability measures , defined on the given sample space. My question is: why it is a family of probability measures? What is/are an example(s) of this abstaction? Especially we could interesting for the parameters estimation. Let me give an example as the (real-valued) mean estimation problem of the random variables, that are normally distributed with known variance equals one. More precise: we have some i.i.d. r.v.'s $X_1,X_2,...,X_n$ (given observations) with $X_1 \sim \mathscr{N}(\theta, 1)$ . Then we could formulate related statistical experiment as $$\mathscr{P}=(\Bbb{R}^n,\mathscr{B}(\Bbb{R}^n), \{ P_{\theta}^{\times{n}}:\theta \in \Theta=\Bbb{R} \}).$$ So, we define our statistical experiment as the infinite set of possible distributions (i.e. as a set $\{\mathscr{N}(\theta, 1)^{\times{n}}:\theta \in \Bbb{R}\}$ ). Why do we need a set? Why we can not say, that we are looking for some estimation of the ""true"" distribution, which is unique?","I'm interesting for the understanding of the definition ""statistical experiment"". Formally statistical experiment , statistical model or just experiment was defined as a triple where is a sample space , is a parameter spase and is family of probability measures , defined on the given sample space. My question is: why it is a family of probability measures? What is/are an example(s) of this abstaction? Especially we could interesting for the parameters estimation. Let me give an example as the (real-valued) mean estimation problem of the random variables, that are normally distributed with known variance equals one. More precise: we have some i.i.d. r.v.'s (given observations) with . Then we could formulate related statistical experiment as So, we define our statistical experiment as the infinite set of possible distributions (i.e. as a set ). Why do we need a set? Why we can not say, that we are looking for some estimation of the ""true"" distribution, which is unique?","\mathscr{P}=(\Omega,\mathscr{F}, \{ P_{\theta}:\theta \in \Theta \}), \Omega,\mathscr{F} \Theta \{ P_{\theta}:\theta \in \Theta \} X_1,X_2,...,X_n X_1 \sim \mathscr{N}(\theta, 1) \mathscr{P}=(\Bbb{R}^n,\mathscr{B}(\Bbb{R}^n), \{ P_{\theta}^{\times{n}}:\theta \in \Theta=\Bbb{R} \}). \{\mathscr{N}(\theta, 1)^{\times{n}}:\theta \in \Bbb{R}\}","['statistics', 'descriptive-statistics', 'experimental-mathematics']"
83,Multiplying the Fisher Information by n to find the asymptotic variance,Multiplying the Fisher Information by n to find the asymptotic variance,,"I've noticed that in some places, the asymptotic variance of a Maximum Likelihood Estimator (MLE) under certain regularity conditions is listed as $\frac{1}{I(\Theta )}$ . However, it's also commonly listed as $\frac{1}{nI(\Theta )}$ in other texts. Why is the fisher information multiplied by $n$ in some contexts but not in others? Example: This question from stackexchange: Proof of asymptotic normality of Maximum Likelihood Estimator (MLE) versus Page 6 from these notes: https://ocw.mit.edu/courses/mathematics/18-443-statistics-for-applications-fall-2006/lecture-notes/lecture3.pdf","I've noticed that in some places, the asymptotic variance of a Maximum Likelihood Estimator (MLE) under certain regularity conditions is listed as . However, it's also commonly listed as in other texts. Why is the fisher information multiplied by in some contexts but not in others? Example: This question from stackexchange: Proof of asymptotic normality of Maximum Likelihood Estimator (MLE) versus Page 6 from these notes: https://ocw.mit.edu/courses/mathematics/18-443-statistics-for-applications-fall-2006/lecture-notes/lecture3.pdf",\frac{1}{I(\Theta )} \frac{1}{nI(\Theta )} n,"['statistics', 'normal-distribution', 'parametric', 'central-limit-theorem', 'maximum-likelihood']"
84,Find natural parameter of Gamma,Find natural parameter of Gamma,,"We start by proving that the gamma distribution is a member of the exponential family. It looks a bit different if either $\alpha$ or $\beta$ are known but below I showcase my proof if both are unknown $$ pdf: p(x|\beta, \alpha) = \frac{\beta^\alpha}{\Gamma (\alpha)} x^{\alpha-1} e^{-\beta x}$$ $$ log [p(x|\beta, \alpha)] = \alpha log \beta - log\Gamma (\alpha) + (\alpha - 1)logx - \beta x $$ $$ p(x|\beta, \alpha) = \frac{1}{\Gamma (\alpha) }exp [\alpha log \beta + (\alpha - 1)logx - \beta x] $$ If we define a member of the exponential family as $$ p_(x|\theta) = \frac{h(x)}{z(\theta)} exp(\eta(\theta)^T \cdot s(x)) \quad \quad \quad (1) $$ Then we can see that the gamma distribution is a member of the exponential family, since $$ h(x)=1, \quad z(\alpha, \beta) = \Gamma(\alpha), \quad \eta_1(\alpha, \beta)= \alpha log \beta, \quad s_1 (x) = 1 \\ \eta_2(\alpha, \beta)=(\alpha-1), \quad s_2(x)=logx, \quad \eta_3(\alpha, \beta) = -\beta, \quad s_3(x)=x $$ My question: Now, we are to find the natural parameter and describe the natural parameter space. Any help here would be greatly appreciated, since I have a hard time understanding the concept of natural parameters, how to calculate it and why we are interested in it. EDIT: I've tried reading up a bit, and my guess at the moment is that when I successfully write a p.d.f in terms of equation (1), then I can determine that the corresponding distribution is a member of the exponential family, and the function $\eta(\theta)$ is the natural parameter.","We start by proving that the gamma distribution is a member of the exponential family. It looks a bit different if either or are known but below I showcase my proof if both are unknown If we define a member of the exponential family as Then we can see that the gamma distribution is a member of the exponential family, since My question: Now, we are to find the natural parameter and describe the natural parameter space. Any help here would be greatly appreciated, since I have a hard time understanding the concept of natural parameters, how to calculate it and why we are interested in it. EDIT: I've tried reading up a bit, and my guess at the moment is that when I successfully write a p.d.f in terms of equation (1), then I can determine that the corresponding distribution is a member of the exponential family, and the function is the natural parameter.","\alpha \beta  pdf: p(x|\beta, \alpha) = \frac{\beta^\alpha}{\Gamma (\alpha)} x^{\alpha-1} e^{-\beta x}  log [p(x|\beta, \alpha)] = \alpha log \beta - log\Gamma (\alpha) + (\alpha - 1)logx - \beta x   p(x|\beta, \alpha) = \frac{1}{\Gamma (\alpha) }exp [\alpha log \beta + (\alpha - 1)logx - \beta x]   p_(x|\theta) = \frac{h(x)}{z(\theta)} exp(\eta(\theta)^T \cdot s(x)) \quad \quad \quad (1)   h(x)=1, \quad z(\alpha, \beta) = \Gamma(\alpha), \quad \eta_1(\alpha, \beta)= \alpha log \beta, \quad s_1 (x) = 1 \\ \eta_2(\alpha, \beta)=(\alpha-1), \quad s_2(x)=logx, \quad \eta_3(\alpha, \beta) = -\beta, \quad s_3(x)=x  \eta(\theta)","['probability-theory', 'statistics', 'gamma-distribution']"
85,What is the expected value of this Markov chain - Secret Santa problem?,What is the expected value of this Markov chain - Secret Santa problem?,,"How this problem came about: There's 10 of us participating in a Secret Santa gift exchange. The issue is that someone might draw his/her own name and we start over. My solution: One person at a time draws a name from the ""bowl"" and that person needs to make sure that he/she has not drawn himself/herself before the next person's turn. If someone draws his/her own name, half of those who already drew a name should replace their drawn names back to the ""bowl"". For example, say I am the sixth person to draw a name and I draw my own, then I'll ask 2 of those who already drew a name to put theirs back and take part in the drawing once more. (Round down the half of 5 to get 2.) Definition: redraw - each time that a person draws his/her name and others put their names back What I found out: Whenever I simulate this process 1,000,000 times, the expected number of redraws is 1.36 or 1.37. See https://onlinegdb.com/ryuUmT-6S . Can you prove this algebraically?","How this problem came about: There's 10 of us participating in a Secret Santa gift exchange. The issue is that someone might draw his/her own name and we start over. My solution: One person at a time draws a name from the ""bowl"" and that person needs to make sure that he/she has not drawn himself/herself before the next person's turn. If someone draws his/her own name, half of those who already drew a name should replace their drawn names back to the ""bowl"". For example, say I am the sixth person to draw a name and I draw my own, then I'll ask 2 of those who already drew a name to put theirs back and take part in the drawing once more. (Round down the half of 5 to get 2.) Definition: redraw - each time that a person draws his/her name and others put their names back What I found out: Whenever I simulate this process 1,000,000 times, the expected number of redraws is 1.36 or 1.37. See https://onlinegdb.com/ryuUmT-6S . Can you prove this algebraically?",,"['probability', 'statistics', 'stochastic-processes', 'markov-chains', 'mathematical-modeling']"
86,Using Chebychev's inequality to prove...,Using Chebychev's inequality to prove...,,"The number $C_n$ of disjoint cycles in a uniformly chosen random permutation of $\{1, 2, 3, \ldots , n\}$ has the representation $C_n = \sum_{k=1}^n X_k$ where $X_1, X_2, \ldots, X_n$ are independent random variables satisfying $P(X_k = 1) = 1  P(X_k = 0) = 1/k $ . Use Chebyshevs inequality to prove that, for any $\varepsilon > 0$ , $P(|(C_n\log(n)) -1| \ge \varepsilon ) \to 0$ as $n  $ I am unsure where even to begin this, however I am aware that the EulerMascheroni constant may be of some use here. Any help appreciated!","The number of disjoint cycles in a uniformly chosen random permutation of has the representation where are independent random variables satisfying . Use Chebyshevs inequality to prove that, for any , as I am unsure where even to begin this, however I am aware that the EulerMascheroni constant may be of some use here. Any help appreciated!","C_n \{1, 2, 3, \ldots , n\} C_n = \sum_{k=1}^n X_k X_1, X_2, \ldots, X_n P(X_k = 1) = 1  P(X_k = 0) = 1/k  \varepsilon > 0 P(|(C_n\log(n)) -1| \ge \varepsilon ) \to 0 n  ","['probability', 'statistics']"
87,"Why is the sum of x1...xn uniformly distributed over [0,1] equal to 1/n!?","Why is the sum of x1...xn uniformly distributed over [0,1] equal to 1/n!?",,"Given x1, x2,...,xn is iid over a uniform [0,1], what is the probability that x1+x2+...+xn < 1? Every explanation I've found seems to use simplex/n-dimension geometry which I can't understand. What's a probability/intuitive way of understand the result of $\frac{1}{n!}$ ? My questionable logic is that 1/2 + 1/4 + 1/8 ... = 1 towards infinity. So for the sum of finite n < 1, the probability has to use that logic. So $\sum \frac{1}{2^i}$ , then since order doesn't matter, multiply it by n to get $n\sum \frac{1}{2^i}$ . Now out of the (probably) many issues, the most first is that I can't get the probability of any point in a continuous distribution. That first 1/2 is based off of the mean.","Given x1, x2,...,xn is iid over a uniform [0,1], what is the probability that x1+x2+...+xn < 1? Every explanation I've found seems to use simplex/n-dimension geometry which I can't understand. What's a probability/intuitive way of understand the result of ? My questionable logic is that 1/2 + 1/4 + 1/8 ... = 1 towards infinity. So for the sum of finite n < 1, the probability has to use that logic. So , then since order doesn't matter, multiply it by n to get . Now out of the (probably) many issues, the most first is that I can't get the probability of any point in a continuous distribution. That first 1/2 is based off of the mean.",\frac{1}{n!} \sum \frac{1}{2^i} n\sum \frac{1}{2^i},"['probability', 'statistics', 'probability-distributions']"
88,Conditional independence for 3-way contingency tables,Conditional independence for 3-way contingency tables,,"In class we went over a proof to show that $$(X_{1}, X_{2}) \perp X_{3} \equiv X_{1} \perp X_{3} | X_{2}$$ for 3-way contingency tables. I understand the rest of the proof, but can't see how these were derived: $$LHS \Longrightarrow P(X_{1} = i, X_{2} = j, X_{3} = k) = P(X_{1} = i, X_{2} = j)P(X_{3} = k)$$ and $$RHS \Longrightarrow \frac{P(X_{1} = i, X_{2} = j, X_{3} = k)}{P(X_{2} = j)} = \frac{P(X_{1} = i, X_{2} = j)}{P(X_{2} = j)} \frac{P(X_{2} = j, X_{3} = k)}{P(X_{2} = j)}$$ I assume Bayes formula has been used, but am unsure where or how. Any help would be awesome!","In class we went over a proof to show that for 3-way contingency tables. I understand the rest of the proof, but can't see how these were derived: and I assume Bayes formula has been used, but am unsure where or how. Any help would be awesome!","(X_{1}, X_{2}) \perp X_{3} \equiv X_{1} \perp X_{3} | X_{2} LHS \Longrightarrow P(X_{1} = i, X_{2} = j, X_{3} = k) = P(X_{1} = i, X_{2} = j)P(X_{3} = k) RHS \Longrightarrow \frac{P(X_{1} = i, X_{2} = j, X_{3} = k)}{P(X_{2} = j)} = \frac{P(X_{1} = i, X_{2} = j)}{P(X_{2} = j)} \frac{P(X_{2} = j, X_{3} = k)}{P(X_{2} = j)}","['statistics', 'proof-explanation', 'conditional-probability', 'independence']"
89,How to modify normal distribution to take into account properties of Mahonian numbers,How to modify normal distribution to take into account properties of Mahonian numbers,,"According to Richard Stanley's answer from normal approximation to $inv(\pi)$ : $$ \left| P\left( \frac{\mathrm{inv}(\pi)-\frac 12{n\choose 2}}{\sqrt{n(n-1)(2n+5)/72}}\leq x\right)-\Phi(x)\right| \leq \frac{C}{\sqrt{n}}, $$ where $\Phi(x)$ denotes the standard normal distribution we get that Raw maxima $M(n)$ of Mahonian numbers ( $T(n,k)$ is the number of permutations of ${1..n}$ with $k$ inversions): $M(n+1)/M(n)=n-\frac 12+o(1)$ for large $n$ . To be more precise the asymptotic is like $M(n+1)/M(n)=n-\frac {1}{2}+O(\frac {1}{n^{1-\epsilon}})$ I wonder how to modify the normal distribution (for eg. its mean and variance) to take into account the precise information about the $M(n)$ numbers? What will be the new variance, mean? I suppose that we'll have the same mean but a different variance in normal distribution. I understand we cannot improve the inequality without further assumptions about the numbers. Thank you for explanations and ideas.","According to Richard Stanley's answer from normal approximation to : where denotes the standard normal distribution we get that Raw maxima of Mahonian numbers ( is the number of permutations of with inversions): for large . To be more precise the asymptotic is like I wonder how to modify the normal distribution (for eg. its mean and variance) to take into account the precise information about the numbers? What will be the new variance, mean? I suppose that we'll have the same mean but a different variance in normal distribution. I understand we cannot improve the inequality without further assumptions about the numbers. Thank you for explanations and ideas.","inv(\pi)  \left| P\left( \frac{\mathrm{inv}(\pi)-\frac 12{n\choose 2}}{\sqrt{n(n-1)(2n+5)/72}}\leq x\right)-\Phi(x)\right| \leq \frac{C}{\sqrt{n}},  \Phi(x) M(n) T(n,k) {1..n} k M(n+1)/M(n)=n-\frac 12+o(1) n M(n+1)/M(n)=n-\frac {1}{2}+O(\frac {1}{n^{1-\epsilon}}) M(n)","['real-analysis', 'statistics', 'inequality', 'probability-distributions', 'permutations']"
90,2-Wasserstein distance between normal distribution and other distribution,2-Wasserstein distance between normal distribution and other distribution,,"For 2-Wasserstein distance between two normal distributions, there is a nice closed-form formula . Now, I want to find the distance between a normal distribution $\mathcal{N}(\mu, \sigma^2)$ and, say, an exponential distribution $\exp(\lambda)$ . There's a formula to calculate the 2-Wasserstein distance between continuous one dimensional distributions. However, it involves integral over the inverse CDF of a random variable, which cannot be computed for normal distribution. If I follow the definition of Wasserstein distance, then the problem is about maximizing $E[XY]$ , for which I have no clue about how to solve. I want to ask: is there a way to compute 2-Wasserstein distance between normal distribution and other continuous random variable, say something from exponential family? Is it possible to generalize the computation to high-dimensional distributions? Any suggestion is appreciated. Thanks!","For 2-Wasserstein distance between two normal distributions, there is a nice closed-form formula . Now, I want to find the distance between a normal distribution and, say, an exponential distribution . There's a formula to calculate the 2-Wasserstein distance between continuous one dimensional distributions. However, it involves integral over the inverse CDF of a random variable, which cannot be computed for normal distribution. If I follow the definition of Wasserstein distance, then the problem is about maximizing , for which I have no clue about how to solve. I want to ask: is there a way to compute 2-Wasserstein distance between normal distribution and other continuous random variable, say something from exponential family? Is it possible to generalize the computation to high-dimensional distributions? Any suggestion is appreciated. Thanks!","\mathcal{N}(\mu, \sigma^2) \exp(\lambda) E[XY]","['probability', 'statistics', 'probability-distributions', 'optimal-transport']"
91,Describing bursts of Poisson arrivals: help verify whether these two perspective or models the same.,Describing bursts of Poisson arrivals: help verify whether these two perspective or models the same.,,"Introduction: I currently have a sensor that picks up the inter-arrival times of vehicles. These inter-arrival times occur in aggregated bursts because there is a traffic light up-stream. So essentially, we have very short inter-arrival times clustered together with mean batch size $N$ which are separated by long inter-arrival times. Problem Description I am interested in two different perspectives that are taken to describe these batches of short inter-arrival times. They are as follows: We have a Poisson process with rate $\mu$ such that vehicles are separated on average by $\frac{1}{\mu}$ amount of time within this batch. This Poisson process is only allowed to be active for an Exponential distributed amount of time $\frac{1}{\lambda}$ . This is like a continuous-time Markov process that dictates how long the active Poisson process may run for. We take a Renewal style approach to this. We have i.i.d. Exponential distributed inter-arrival times of length $\frac{1}{\mu}$ . We observe on average $N$ of these inter-arrivals. We would observe $N(t)$ to be a Poisson Process with rate $\lambda$ . I would like to show that these two perspectives are the same. My attempt: To show that they are the same, it will suffice to show that $P(N(t)=k)$ is the same for both. Here $t$ is the duration of the batch/burst/activity and N(t) is the amount of vehicles or inter-arrivals that are found in the batch. Case 1: We have a Poisson process with rate $\mu$ that is only allowed to run for $t$ amount of time. However, $E[t] = \frac{1}{\lambda}$ . Hence, $$ P(N(t)=k) = \frac{(\mu t)^k e^{-(\mu t)}}{k!} $$ $$\therefore P(N(t)=k) = \frac{(\frac{\mu}{\lambda})^k e^{-(\frac{\mu}{\lambda})}}{k!} = P \left( N \left( \frac{1}{\lambda} \right) =k  \right)   $$ We must note that $\mu > \lambda$ such that $t_{burst} > t_{arrival}$ . We can thus say $N = \max\{n: n \leq \frac{\mu}{\lambda} \} $ Case 2: This perspective is tricky for me. A Poisson process usually has to be observed for a fixed interval of time. We have an expected fixed interval of time which is the length of a burst. So our Poisson data/counts gives rise to a rate in the form of $n$ counts per $E[t]$ interval of time. Here $E[t]$ should be the sum of $N$ inter-arrivals each of which have mean length $\frac{1}{\mu}$ . Intuitively, we then have $N \times \frac{1}{\mu}$ as the the length of the burst. But $E[N]=\lambda$ such that $E[t] = \frac{\lambda}{\mu}$ . Hence the overall process is Poisson with rate $\frac{\mu}{\lambda}$ as in case 1. I try to describe the intuitive explanation in more Math-like terms. Let $$ S_{N(t)} = \sum_{i=1}^{N(t)} X_i $$ $S_{N(t)}$ would then be a burst length. Furthermore, it has an Erlang probability distribution function. Now we have $$P(N(t)=k) = P(S_k \leq E[t]; X_{k+1} \geq E[t] - S_k)$$ $$ \therefore P(N(t)=k) = P(S_k \leq \frac{1}{\lambda}; X_{k+1} \geq \frac{1}{\lambda} - S_k)$$ $$  \therefore P(N(t)=k) = \int_0^{\frac{1}{\lambda}} \int_{\frac{1}{\lambda}-s}^{\infty} \left(  \frac{\mu^k s^{k-1} e^{-\mu s}}{(k-1)!}  \right) \mu e^{-\mu x} \, dx \, ds $$ $$ \therefore P(N(t)=k) = \frac{\mu^k}{(k-1)!} \int_0^{\frac{1}{\lambda}} (s^{k-1} e^{-\mu s}) e^{-\mu (\frac{1}{\lambda} - s)} \, ds $$ $$ \therefore P(N(t)=k) = \frac{(\frac{\mu}{\lambda})^k e^{-(\frac{\mu}{\lambda})}}{k!}   $$ This is the same as in above. Conclusion: Would you agree with what I have done? I am not too confident in the reasoning of what I have used. I've tried to solve this problem in between a lot of chaos hence the lack of confidence in what has been provided. Please verify if you believe it to be correct and please correct me where wrong. Ideally, I would love for more explanations/solutions of a more elegant nature to be provided. Thank you for your time","Introduction: I currently have a sensor that picks up the inter-arrival times of vehicles. These inter-arrival times occur in aggregated bursts because there is a traffic light up-stream. So essentially, we have very short inter-arrival times clustered together with mean batch size which are separated by long inter-arrival times. Problem Description I am interested in two different perspectives that are taken to describe these batches of short inter-arrival times. They are as follows: We have a Poisson process with rate such that vehicles are separated on average by amount of time within this batch. This Poisson process is only allowed to be active for an Exponential distributed amount of time . This is like a continuous-time Markov process that dictates how long the active Poisson process may run for. We take a Renewal style approach to this. We have i.i.d. Exponential distributed inter-arrival times of length . We observe on average of these inter-arrivals. We would observe to be a Poisson Process with rate . I would like to show that these two perspectives are the same. My attempt: To show that they are the same, it will suffice to show that is the same for both. Here is the duration of the batch/burst/activity and N(t) is the amount of vehicles or inter-arrivals that are found in the batch. Case 1: We have a Poisson process with rate that is only allowed to run for amount of time. However, . Hence, We must note that such that . We can thus say Case 2: This perspective is tricky for me. A Poisson process usually has to be observed for a fixed interval of time. We have an expected fixed interval of time which is the length of a burst. So our Poisson data/counts gives rise to a rate in the form of counts per interval of time. Here should be the sum of inter-arrivals each of which have mean length . Intuitively, we then have as the the length of the burst. But such that . Hence the overall process is Poisson with rate as in case 1. I try to describe the intuitive explanation in more Math-like terms. Let would then be a burst length. Furthermore, it has an Erlang probability distribution function. Now we have This is the same as in above. Conclusion: Would you agree with what I have done? I am not too confident in the reasoning of what I have used. I've tried to solve this problem in between a lot of chaos hence the lack of confidence in what has been provided. Please verify if you believe it to be correct and please correct me where wrong. Ideally, I would love for more explanations/solutions of a more elegant nature to be provided. Thank you for your time","N \mu \frac{1}{\mu} \frac{1}{\lambda} \frac{1}{\mu} N N(t) \lambda P(N(t)=k) t \mu t E[t] = \frac{1}{\lambda}  P(N(t)=k) = \frac{(\mu t)^k e^{-(\mu t)}}{k!}  \therefore P(N(t)=k) = \frac{(\frac{\mu}{\lambda})^k e^{-(\frac{\mu}{\lambda})}}{k!} = P \left( N \left( \frac{1}{\lambda} \right) =k  \right)    \mu > \lambda t_{burst} > t_{arrival} N = \max\{n: n \leq \frac{\mu}{\lambda} \}  n E[t] E[t] N \frac{1}{\mu} N \times \frac{1}{\mu} E[N]=\lambda E[t] = \frac{\lambda}{\mu} \frac{\mu}{\lambda}  S_{N(t)} = \sum_{i=1}^{N(t)} X_i  S_{N(t)} P(N(t)=k) = P(S_k \leq E[t]; X_{k+1} \geq E[t] - S_k)  \therefore P(N(t)=k) = P(S_k \leq \frac{1}{\lambda}; X_{k+1} \geq \frac{1}{\lambda} - S_k)   \therefore P(N(t)=k) = \int_0^{\frac{1}{\lambda}} \int_{\frac{1}{\lambda}-s}^{\infty} \left(  \frac{\mu^k s^{k-1} e^{-\mu s}}{(k-1)!}  \right) \mu e^{-\mu x} \, dx \, ds   \therefore P(N(t)=k) = \frac{\mu^k}{(k-1)!} \int_0^{\frac{1}{\lambda}} (s^{k-1} e^{-\mu s}) e^{-\mu (\frac{1}{\lambda} - s)} \, ds   \therefore P(N(t)=k) = \frac{(\frac{\mu}{\lambda})^k e^{-(\frac{\mu}{\lambda})}}{k!}   ","['statistics', 'stochastic-processes', 'poisson-process', 'exponential-distribution', 'renewal-processes']"
92,Cross-entropy and determinant of cross-covariance matrix,Cross-entropy and determinant of cross-covariance matrix,,Can the cross-entropy between two multivariate gaussians be expressed using the determinant of cross-covariance matrices?,Can the cross-entropy between two multivariate gaussians be expressed using the determinant of cross-covariance matrices?,,"['statistics', 'information-theory', 'entropy']"
93,A family of distributions induced by a metric,A family of distributions induced by a metric,,"I'm interested in the family of distribution which can be expressed in the following form $f(x|\mu)=C \exp(-d(x,\mu))$ where $\mu$ is a parameter of the distribution and $d(*,*)$ is a metric.  Normal distribution and Laplace distribution satisfy this property. I was wondering if this family has already been studied and what is its name.",I'm interested in the family of distribution which can be expressed in the following form where is a parameter of the distribution and is a metric.  Normal distribution and Laplace distribution satisfy this property. I was wondering if this family has already been studied and what is its name.,"f(x|\mu)=C \exp(-d(x,\mu)) \mu d(*,*)",['statistics']
94,Expected number of balls to be thrown until one of $N$ urns has $\beta$ more balls than all the other urns [closed],Expected number of balls to be thrown until one of  urns has  more balls than all the other urns [closed],N \beta,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Given $N$ urns, what is the expected number of balls that need to be thrown until one of the urns has $\beta$ more balls than all the other urns?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Given urns, what is the expected number of balls that need to be thrown until one of the urns has more balls than all the other urns?",N \beta,"['probability', 'combinatorics', 'statistics', 'discrete-mathematics']"
95,Prove this estimator is not sufficient,Prove this estimator is not sufficient,,"I have the following problem. Let $X_1,...,X_n$ be a random sample of i.i.d random variables with density $$f_\theta=\begin{cases} \theta x +\frac{1}{2},&\text{if }-1\le x\le 1\\ 0 ,&\text{elsewhere} \end{cases}$$ Let $\hat\theta(X)=\frac{3\bar{X}}{2}$ an estimator of $\theta$ . I want to prove that $\hat\theta(X)$ is not sufficient by giving a specific example where $P(X=x|\hat\theta(X)=t)$ depends on $\theta$ . Any help would be appreciated.",I have the following problem. Let be a random sample of i.i.d random variables with density Let an estimator of . I want to prove that is not sufficient by giving a specific example where depends on . Any help would be appreciated.,"X_1,...,X_n f_\theta=\begin{cases}
\theta x +\frac{1}{2},&\text{if }-1\le x\le 1\\
0 ,&\text{elsewhere}
\end{cases} \hat\theta(X)=\frac{3\bar{X}}{2} \theta \hat\theta(X) P(X=x|\hat\theta(X)=t) \theta","['statistics', 'estimation', 'maximum-likelihood', 'parameter-estimation']"
96,The four-penny problem,The four-penny problem,,"Suppose you have four pennies, say $P_1$ , $P_2$ , $P_3$ , and $P_4$ , a weighing balance with two pans, and a set of weights. You are allowed to make only four weighings of the pennies. In each weighing, you may place some (possibly none) of the pennies in the left pan, some (possibly none) in the right pan, and leave the remaining pennies (if any) off the balance. The problem is this: How should the four  weighings be designed so as to produce the most accurate determination of the weights of the four pennies, in the sense that the sum of the variances of the four weight estimators should be as small as possible? Wee assume that weighings are independent and that measuring errors have mean $0$ and a common variance, say $\sigma^2$ , that does not depend on the configuration of pennies on the scale. Let $\beta_1$ , $\beta_2$ , $\beta_3$ , $\beta_4$ be the unknown weights of the four pennies. For $i = 1,...,4$ , let $W_i$ be the random variable recording the result of the $i^th$ weighting; adopt the convention that weights placed in the left (respectively, right) pan are treated as being positive (respectively, negative). The assumptions of the problem imply that for any given weighing design, the random vector $W=(W_1 ,W_2 ,W_3 ,W_4 )^T$ is weakly spherical and for each $i$ , $$E(W_i)=\sum^4_{j=1}x_{ij}\beta_j$$ where $$ x_{ij}= \begin{cases} -1,& \text{if penny} ~j~ \text{is to be placed in the left pan}\\ 0,& \text{if penny} ~j~ \text{is to be left off the balance}\\ 1,& \text{if penny} ~j~ \text{is to be placed in the right pan} \end{cases} $$ in the $i^th$ weighing. The four-penny problem then be restated as follows: For what weighing design, that is, for what $4\times 4$ (nonsingular) design matrix $X=(x_{ij})$ having for its elements only $1$ s, $0$ s, and $1$ s, is $$\sum^4_{j=1} Var(\hat{\beta}_j)$$ the smallest? Where $\hat\beta_j=\langle cv(\beta_j),W\rangle$ a Gauss-Markov estimator of $\beta_j$ , with the coefficient vector $cv(\beta_i)$ .","Suppose you have four pennies, say , , , and , a weighing balance with two pans, and a set of weights. You are allowed to make only four weighings of the pennies. In each weighing, you may place some (possibly none) of the pennies in the left pan, some (possibly none) in the right pan, and leave the remaining pennies (if any) off the balance. The problem is this: How should the four  weighings be designed so as to produce the most accurate determination of the weights of the four pennies, in the sense that the sum of the variances of the four weight estimators should be as small as possible? Wee assume that weighings are independent and that measuring errors have mean and a common variance, say , that does not depend on the configuration of pennies on the scale. Let , , , be the unknown weights of the four pennies. For , let be the random variable recording the result of the weighting; adopt the convention that weights placed in the left (respectively, right) pan are treated as being positive (respectively, negative). The assumptions of the problem imply that for any given weighing design, the random vector is weakly spherical and for each , where in the weighing. The four-penny problem then be restated as follows: For what weighing design, that is, for what (nonsingular) design matrix having for its elements only s, s, and s, is the smallest? Where a Gauss-Markov estimator of , with the coefficient vector .","P_1 P_2 P_3 P_4 0 \sigma^2 \beta_1 \beta_2 \beta_3 \beta_4 i = 1,...,4 W_i i^th W=(W_1 ,W_2 ,W_3 ,W_4 )^T i E(W_i)=\sum^4_{j=1}x_{ij}\beta_j 
x_{ij}=
\begin{cases}
-1,& \text{if penny} ~j~ \text{is to be placed in the left pan}\\
0,& \text{if penny} ~j~ \text{is to be left off the balance}\\
1,& \text{if penny} ~j~ \text{is to be placed in the right pan}
\end{cases}
 i^th 4\times 4 X=(x_{ij}) 1 0 1 \sum^4_{j=1} Var(\hat{\beta}_j) \hat\beta_j=\langle cv(\beta_j),W\rangle \beta_j cv(\beta_i)","['linear-algebra', 'statistics']"
97,Low-rank covariance matrix,Low-rank covariance matrix,,"I'm reading a paper in the context of high-dimensional data where the authors propose to estimate a $p\times p$ unknown covariance matrix $\Sigma$ with a $p\times p$ matrix $\tilde{\Sigma}$ that has low rank $r<p$ . Despite doing this, they don't give an intuition about the motivation of this approach. What are the consequences (or benefits) of having a low-rank estimation of $\Sigma$ ? All that I can think of is the connection with PCA: computing the eigenvectors of the low-rank estimation $\tilde{\Sigma}$ I will have the principal components. But why is this useful when I can compute the classical estimator $\hat{\Sigma}=n^{-1}X^{T}X$ (for $n\times p$ centered data $X$ ) and then compute the eigenvectors?","I'm reading a paper in the context of high-dimensional data where the authors propose to estimate a unknown covariance matrix with a matrix that has low rank . Despite doing this, they don't give an intuition about the motivation of this approach. What are the consequences (or benefits) of having a low-rank estimation of ? All that I can think of is the connection with PCA: computing the eigenvectors of the low-rank estimation I will have the principal components. But why is this useful when I can compute the classical estimator (for centered data ) and then compute the eigenvectors?",p\times p \Sigma p\times p \tilde{\Sigma} r<p \Sigma \tilde{\Sigma} \hat{\Sigma}=n^{-1}X^{T}X n\times p X,"['linear-algebra', 'statistics', 'statistical-inference']"
98,Is there a way to obtain a strict total order of sets of ordinal rankings?,Is there a way to obtain a strict total order of sets of ordinal rankings?,,"Consider a set of three ordinal rankings of three elements $A,B,C$ ; that is, a set of the form $\{\sigma_1,\sigma_2,\sigma_3\}$ where, for example, $\sigma_1 = ABC, \sigma_2 = ACB, \sigma_3 = CAB$ . (Here, $ABC$ is shorthand for $A\succ B\succ C$ , meaning that $A$ is preferable to $B$ , and $B$ is preferable to $C$ .) It's easy to see that we always can relabel the elements such that all sets can be written on the form $\{ABC,\sigma_2,\sigma_3\}$ , and thus reducing the number of sets to $(3!)^2 = 36$ , and since the order of elements in a set doesn't matter, these can in turn be reduced to 10 equivalence classes (two sets are equivalent if one can be obtained from the other by relabeling the elements, e.g. $\{ABC,ABC,ABC\}$ and $\{BAC,BAC,BAC\}$ ): $$P_1 = \{ABC,ABC,ABC\}, P_2 = \{ABC,ABC,ACB\},P_3 = \{ABC,ABC,BAC\},\\ P_4 = \{ABC,ABC,BCA\}, P_5 = \{ABC,ABC,CAB\}, P_6 = \{ABC,ABC,CBA\}\\ P_7 = \{ABC,ACB,BAC\}, P_8 = \{ABC,ACB,BCA\},\\ P_9 = \{ABC,BAC,CAB\},P_{10} = \{ABC,BCA,CAB\}.$$ My aim is to obtain a strict total order between these sets, with respect to ""internal agreement"": For instance, if I compare $P_1 = \{ABC, ABC, ABC\}$ with $P_2 = \{ABC,ABC,ACB\}$ and $P_{10} = \{ABC, BCA, CAB\}$ , we intuitively think that $P_1$ has maximal internal agreement, since the rankings are identical, but in $P_{10}$ there is very little internal agreement. And $P_2$ falls somewhere in between. Therefore, under some yet to be defined strict total order $<$ , one would like to have $$P_1 < P_2< P_{10}.$$ Is there a suitable way to define such an order? It seems that we ought to take the position of the elements into account (analagous to weighted Kendall's $\tau$ for distances between ordinal rankings), so that for example, $$P_1<P_2<P_3,$$ since it should be more ""costly"" to swap the first two elements in any ordering than to swap the last two elements. In short: Can $P_1,\dots,P_{10}$ be strictly totally ordered (no ties) with respect to ""internal agreement"" as discussed above? Any suggestions would be much appreciated. (Even other orders with relaxed conditions, or distance functions with an strict total order of the distances, might be valuable.)","Consider a set of three ordinal rankings of three elements ; that is, a set of the form where, for example, . (Here, is shorthand for , meaning that is preferable to , and is preferable to .) It's easy to see that we always can relabel the elements such that all sets can be written on the form , and thus reducing the number of sets to , and since the order of elements in a set doesn't matter, these can in turn be reduced to 10 equivalence classes (two sets are equivalent if one can be obtained from the other by relabeling the elements, e.g. and ): My aim is to obtain a strict total order between these sets, with respect to ""internal agreement"": For instance, if I compare with and , we intuitively think that has maximal internal agreement, since the rankings are identical, but in there is very little internal agreement. And falls somewhere in between. Therefore, under some yet to be defined strict total order , one would like to have Is there a suitable way to define such an order? It seems that we ought to take the position of the elements into account (analagous to weighted Kendall's for distances between ordinal rankings), so that for example, since it should be more ""costly"" to swap the first two elements in any ordering than to swap the last two elements. In short: Can be strictly totally ordered (no ties) with respect to ""internal agreement"" as discussed above? Any suggestions would be much appreciated. (Even other orders with relaxed conditions, or distance functions with an strict total order of the distances, might be valuable.)","A,B,C \{\sigma_1,\sigma_2,\sigma_3\} \sigma_1 = ABC, \sigma_2 = ACB, \sigma_3 = CAB ABC A\succ B\succ C A B B C \{ABC,\sigma_2,\sigma_3\} (3!)^2 = 36 \{ABC,ABC,ABC\} \{BAC,BAC,BAC\} P_1 = \{ABC,ABC,ABC\}, P_2 = \{ABC,ABC,ACB\},P_3 = \{ABC,ABC,BAC\},\\
P_4 = \{ABC,ABC,BCA\}, P_5 = \{ABC,ABC,CAB\}, P_6 = \{ABC,ABC,CBA\}\\
P_7 = \{ABC,ACB,BAC\}, P_8 = \{ABC,ACB,BCA\},\\
P_9 = \{ABC,BAC,CAB\},P_{10} = \{ABC,BCA,CAB\}. P_1 = \{ABC, ABC, ABC\} P_2 = \{ABC,ABC,ACB\} P_{10} = \{ABC, BCA, CAB\} P_1 P_{10} P_2 < P_1 < P_2< P_{10}. \tau P_1<P_2<P_3, P_1,\dots,P_{10}","['combinatorics', 'statistics']"
99,Variance of EWMA (Exponentially Weighted Moving Average),Variance of EWMA (Exponentially Weighted Moving Average),,"It is assumed that the quality characteristic of interest, denoted by $Y_i$ , follows a Normal distribution with mean $$ and variance $^2$ . We take a sample of size $n$ at time $t$ and measure its quality characteristic.  The EWMA statistic at time $t$ , denoted by $M_t$ , is \begin{align}  (Eq.1):   M_t = (1-)M_{t-1} + \bar{Y_t} + k(\bar{Y_t}-\bar{Y_{t-1}}) \end{align} (Eq.1) where $\bar{Y_t}$ is the sample mean at time t and k is a constant. Attempt \begin{gather*}    E[M_t] = (1-)E[M_{t-1}] + E[\bar{Y_t}] + k(E[\bar{Y_t}]] - E [\bar{Y_{t-1}}]) \end{gather*} Q1 : I now assume $E[M_t] = E[M_{t-1}]$ , is this correct? Therefore, \begin{gather*}    E[M_t] = \frac{1}{} + k(-) =  \end{gather*} And for the variance, \begin{gather*}    V[M_t] = (1-)^2V[M_{t-1}] + ^2V[\bar{Y_t}] + k^2(V[\bar{Y_t}] + V[\bar{Y_{t-1}}]) + 2(1-)(Cov(M_{t-1},\bar{Y_t}) + k( Cov(M_{t-1},\bar{Y_t}) - Cov(M_{t-1},\bar{Y_{t-1}}) )) \end{gather*} Q2 : Above, I asumed $\bar{Y_t}$ and $\bar{Y_{t-1}}$ are independent, since $Y_{i,t}$ are independent. Q3 : Is $M_{t-1}$ independent from $\bar{Y_t}$ and $\bar{Y_{t-1}}$ ? For my third question, my answer would be that $M_{t-1}$ is dependent on the sample means since there is a relation between them (Eq.1). I don't know if this is trivial but I wanted to make sure I am not missing something here. Thank you.","It is assumed that the quality characteristic of interest, denoted by , follows a Normal distribution with mean and variance . We take a sample of size at time and measure its quality characteristic.  The EWMA statistic at time , denoted by , is (Eq.1) where is the sample mean at time t and k is a constant. Attempt Q1 : I now assume , is this correct? Therefore, And for the variance, Q2 : Above, I asumed and are independent, since are independent. Q3 : Is independent from and ? For my third question, my answer would be that is dependent on the sample means since there is a relation between them (Eq.1). I don't know if this is trivial but I wanted to make sure I am not missing something here. Thank you.","Y_i  ^2 n t t M_t \begin{align}
 (Eq.1):   M_t = (1-)M_{t-1} + \bar{Y_t} + k(\bar{Y_t}-\bar{Y_{t-1}})
\end{align} \bar{Y_t} \begin{gather*}
   E[M_t] = (1-)E[M_{t-1}] + E[\bar{Y_t}] + k(E[\bar{Y_t}]] - E [\bar{Y_{t-1}}])
\end{gather*} E[M_t] = E[M_{t-1}] \begin{gather*}
   E[M_t] = \frac{1}{} + k(-) = 
\end{gather*} \begin{gather*}
   V[M_t] = (1-)^2V[M_{t-1}] + ^2V[\bar{Y_t}] + k^2(V[\bar{Y_t}] + V[\bar{Y_{t-1}}]) + 2(1-)(Cov(M_{t-1},\bar{Y_t}) + k( Cov(M_{t-1},\bar{Y_t}) - Cov(M_{t-1},\bar{Y_{t-1}}) ))
\end{gather*} \bar{Y_t} \bar{Y_{t-1}} Y_{i,t} M_{t-1} \bar{Y_t} \bar{Y_{t-1}} M_{t-1}","['statistics', 'variance']"
