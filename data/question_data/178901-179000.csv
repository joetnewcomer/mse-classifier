,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Solving $\iint_{[0,1]^2}\frac{1}{(\sqrt{1+x^2+y^2})^3}$",Solving,"\iint_{[0,1]^2}\frac{1}{(\sqrt{1+x^2+y^2})^3}","Solving $$\iint_{[0,1]^2}\frac{1}{(\sqrt{1+x^2+y^2})^3}$$ Thinking of using $$x=r\cos\varphi \\ y=r\sin\varphi \\ J=r$$ But this is a square, so I just, am guessing need to find the range of $r$, what it is, and why?","Solving $$\iint_{[0,1]^2}\frac{1}{(\sqrt{1+x^2+y^2})^3}$$ Thinking of using $$x=r\cos\varphi \\ y=r\sin\varphi \\ J=r$$ But this is a square, so I just, am guessing need to find the range of $r$, what it is, and why?",,"['calculus', 'real-analysis', 'integration', 'multivariable-calculus', 'definite-integrals']"
1,"Total derivative of the function $F(x,y)=\langle Ax,y \rangle$, where $A$ is a matrix","Total derivative of the function , where  is a matrix","F(x,y)=\langle Ax,y \rangle A","Let $F:\mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R}$. Defined as $F(x,y)=\langle Ax,y \rangle$ where $A$ is an $n \times n$ matrix. Then show that $$(DF(x,y))(u,v)=\langle Au,y \rangle +\langle Ax,v \rangle,$$ where $D $ is the total derivative. We have just been introduced to multivariate calculus this semester. I have been trying to crack this one since morning. I am sure there is a key idea that I am missing.","Let $F:\mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R}$. Defined as $F(x,y)=\langle Ax,y \rangle$ where $A$ is an $n \times n$ matrix. Then show that $$(DF(x,y))(u,v)=\langle Au,y \rangle +\langle Ax,v \rangle,$$ where $D $ is the total derivative. We have just been introduced to multivariate calculus this semester. I have been trying to crack this one since morning. I am sure there is a key idea that I am missing.",,"['real-analysis', 'multivariable-calculus']"
2,League of Legends optimal items,League of Legends optimal items,,"In the popular game League of Legends, your effective amount of hit points ($E$) against physical damage is a function of your actual hit points ($H$) and the amount of armor ($A$) you have. $$E = H\left(\frac{A+100}{100}\right)$$ You can increase your hit points and armor by buying items. $20$ gold per $1$ armor $\frac 83$ gold per $1$ hit point Given a state $(h,a,x)$ where $h$ is your current amount of hit points, $a$ is your current amount of armor, and $x$ is the amount of gold you have, how much of each (armor and hit points) should you buy to maximize your effective hit points $e$? I was told that this problem requires Lagrange multipliers, but I haven't learned that yet so I don't know what to do.","In the popular game League of Legends, your effective amount of hit points ($E$) against physical damage is a function of your actual hit points ($H$) and the amount of armor ($A$) you have. $$E = H\left(\frac{A+100}{100}\right)$$ You can increase your hit points and armor by buying items. $20$ gold per $1$ armor $\frac 83$ gold per $1$ hit point Given a state $(h,a,x)$ where $h$ is your current amount of hit points, $a$ is your current amount of armor, and $x$ is the amount of gold you have, how much of each (armor and hit points) should you buy to maximize your effective hit points $e$? I was told that this problem requires Lagrange multipliers, but I haven't learned that yet so I don't know what to do.",,"['multivariable-calculus', 'optimization']"
3,One partial derivative is continuous (at a single point) implies differentiable?,One partial derivative is continuous (at a single point) implies differentiable?,,"Let $f:\mathbb{R}^2\to\mathbb{R}$ and $(p,q)\in\mathbb{R}^2$ such that both $f_x$ and $f_y$ exists at $(p,q)$. Assume that $f_x$ is continuous at $(p,q)$. How do we prove/disprove that $f$ is differentiable at $(p,q)$? I do note that it is similar to this question: Continuity of one partial derivative implies differentiability However the critical difference is that for my case, I only have $f_x$ continuous at a single point $(p,q)$, not even in a neighborhood, hence I believe that the approach of Fundamental Theorem of Calculus used in the other question cannot work. Thanks for any help!","Let $f:\mathbb{R}^2\to\mathbb{R}$ and $(p,q)\in\mathbb{R}^2$ such that both $f_x$ and $f_y$ exists at $(p,q)$. Assume that $f_x$ is continuous at $(p,q)$. How do we prove/disprove that $f$ is differentiable at $(p,q)$? I do note that it is similar to this question: Continuity of one partial derivative implies differentiability However the critical difference is that for my case, I only have $f_x$ continuous at a single point $(p,q)$, not even in a neighborhood, hence I believe that the approach of Fundamental Theorem of Calculus used in the other question cannot work. Thanks for any help!",,"['real-analysis', 'multivariable-calculus']"
4,Continuous for each variables does not implies continuous,Continuous for each variables does not implies continuous,,"Prove or disprove the following statement: Statement. Continuous for each variables, when other variables are fixed, implies continuous? More clearly, prove or disprove the following problem: Let $\displaystyle f:\left[ a,b \right]\times \left[ c,d \right]\to \mathbb{R}$ for which: For every $\displaystyle {{x}_{0}}\in \left[ a,b \right]$, $\displaystyle f\left( {{x}_{0}},y \right)$ is continuous on $\displaystyle \left[ c,d \right]$ respect to variable $ \displaystyle y$. For every $ \displaystyle {{y}_{0}}\in \left[ c,d \right]$, $ \displaystyle f\left( x,{{y}_{0}} \right)$ is continuous on $ \displaystyle \left[ a,b \right]$ respect to variable $\displaystyle x$. Then $\displaystyle f\left( x,y \right)$ is continuous on $ \displaystyle \left[ a,b \right]\times \left[ c,d \right]$. ? https://hongnguyenquanba.wordpress.com/2016/05/12/problem-6/","Prove or disprove the following statement: Statement. Continuous for each variables, when other variables are fixed, implies continuous? More clearly, prove or disprove the following problem: Let $\displaystyle f:\left[ a,b \right]\times \left[ c,d \right]\to \mathbb{R}$ for which: For every $\displaystyle {{x}_{0}}\in \left[ a,b \right]$, $\displaystyle f\left( {{x}_{0}},y \right)$ is continuous on $\displaystyle \left[ c,d \right]$ respect to variable $ \displaystyle y$. For every $ \displaystyle {{y}_{0}}\in \left[ c,d \right]$, $ \displaystyle f\left( x,{{y}_{0}} \right)$ is continuous on $ \displaystyle \left[ a,b \right]$ respect to variable $\displaystyle x$. Then $\displaystyle f\left( x,y \right)$ is continuous on $ \displaystyle \left[ a,b \right]\times \left[ c,d \right]$. ? https://hongnguyenquanba.wordpress.com/2016/05/12/problem-6/",,"['functions', 'multivariable-calculus', 'continuity', 'examples-counterexamples']"
5,Integral of bounded function with limit zero at $\pm \infty$,Integral of bounded function with limit zero at,\pm \infty,"Very simple question here, I almost feel bad for asking it.. Lets say we have a function bounded between $0$ and $1$. This function is high dimensional: $0<f(X) \le1, ~~~ X \in \mathbb{R}^D$ Now, we calculate the limit for all elements of $X$ going to plus and minus infinity. We find out that they are zero. Can we say that the integral of the function over the entire domain of $X$ is finite? Can we say that if we get even non-zero limit? Finally, if the zero limit is insufficient, is there some other condition that suffices?","Very simple question here, I almost feel bad for asking it.. Lets say we have a function bounded between $0$ and $1$. This function is high dimensional: $0<f(X) \le1, ~~~ X \in \mathbb{R}^D$ Now, we calculate the limit for all elements of $X$ going to plus and minus infinity. We find out that they are zero. Can we say that the integral of the function over the entire domain of $X$ is finite? Can we say that if we get even non-zero limit? Finally, if the zero limit is insufficient, is there some other condition that suffices?",,"['calculus', 'integration', 'multivariable-calculus', 'vector-spaces', 'potential-theory']"
6,integrate $\int_{0}^{1}\int_{3y}^{3} e^{x^2}dxdy$,integrate,\int_{0}^{1}\int_{3y}^{3} e^{x^2}dxdy,$$\int_{0}^{1}\int_{3y}^{3}e^{x^2}dxdy$$ I understand I need to change the integration limits but just changing the order does not help to solve the integral. $\int_{3y}^{3}\int_{0}^{1}e^{x^2}dydx=\int_{3y}^{3}[ye^{x^2}]_{0}^{1}dx=\int_{3y}^{3}e^{x^2}dx$ How should I approach it?,$$\int_{0}^{1}\int_{3y}^{3}e^{x^2}dxdy$$ I understand I need to change the integration limits but just changing the order does not help to solve the integral. $\int_{3y}^{3}\int_{0}^{1}e^{x^2}dydx=\int_{3y}^{3}[ye^{x^2}]_{0}^{1}dx=\int_{3y}^{3}e^{x^2}dx$ How should I approach it?,,"['integration', 'multivariable-calculus']"
7,"$f$ differentiable with f(0)=0, 1 isn't eigenvalue of $f'(0)$. There's a neighborhood of 0 in wich $f(x)\neq x$.","differentiable with f(0)=0, 1 isn't eigenvalue of . There's a neighborhood of 0 in wich .",f f'(0) f(x)\neq x,"Question: Let $f:\mathbb{R}^m\to \mathbb{R}^m$ be differentiable with $f(0)=0$. If $1$ isn't eigenvalue of $f'(0)$ then there is a neighborhood $V$ of $0$ in $\mathbb{R}^m$ such that $f(x)\neq x$ for all $x\in V\setminus\{0\}$. I tried three approaches: We know that $f'(0)x=\lim\limits_{t\to 0}\frac{f(tx)}{t}\neq x$. Also, $\lim\limits_{h\to 0}\frac{f(h)-f'(0)h}{||h||}=0$. And $f(x)=f'(0)x+r(x)$ where $\lim\limits_{x\to 0}\frac{r(x)}{||x||}=0.$ And there's the fact that $f'(0)x\neq x$, $\forall x\in V\setminus \{0\}$. But I don't know how to use theses facts here.","Question: Let $f:\mathbb{R}^m\to \mathbb{R}^m$ be differentiable with $f(0)=0$. If $1$ isn't eigenvalue of $f'(0)$ then there is a neighborhood $V$ of $0$ in $\mathbb{R}^m$ such that $f(x)\neq x$ for all $x\in V\setminus\{0\}$. I tried three approaches: We know that $f'(0)x=\lim\limits_{t\to 0}\frac{f(tx)}{t}\neq x$. Also, $\lim\limits_{h\to 0}\frac{f(h)-f'(0)h}{||h||}=0$. And $f(x)=f'(0)x+r(x)$ where $\lim\limits_{x\to 0}\frac{r(x)}{||x||}=0.$ And there's the fact that $f'(0)x\neq x$, $\forall x\in V\setminus \{0\}$. But I don't know how to use theses facts here.",,"['real-analysis', 'analysis', 'multivariable-calculus']"
8,Partial derivatives exist and derivative does not,Partial derivatives exist and derivative does not,,"Wikipedia states that partial derivatives of this function exist, while derivative doesn't. I understand that the problem lies at $x=-1$. $$f(x,y) = \begin{cases}x & \text{if }y \ne x^2 \\ 0 & \text{if }y = x^2\end{cases}$$ The partial derivatives for this function are $$f(x,y)'_x = \begin{cases}1 & \text{if }y \ne x^2 \\ 0 & \text{if }y = x^2\end{cases}$$ $$f(x,y)'_y = \begin{cases}0 & \text{if }y \ne x^2 \\ 0 & \text{if }y = x^2\end{cases}$$ Does the problem with the derivative arise because of the jump at $x = -1$? EDIT: Wikipedia link see section ""Differentiability in higher dimensions""","Wikipedia states that partial derivatives of this function exist, while derivative doesn't. I understand that the problem lies at $x=-1$. $$f(x,y) = \begin{cases}x & \text{if }y \ne x^2 \\ 0 & \text{if }y = x^2\end{cases}$$ The partial derivatives for this function are $$f(x,y)'_x = \begin{cases}1 & \text{if }y \ne x^2 \\ 0 & \text{if }y = x^2\end{cases}$$ $$f(x,y)'_y = \begin{cases}0 & \text{if }y \ne x^2 \\ 0 & \text{if }y = x^2\end{cases}$$ Does the problem with the derivative arise because of the jump at $x = -1$? EDIT: Wikipedia link see section ""Differentiability in higher dimensions""",,"['multivariable-calculus', 'derivatives', 'partial-derivative']"
9,Problem 19 chapter 9 from PMA Rudin,Problem 19 chapter 9 from PMA Rudin,,"Show that the system of equations $$\begin{cases} 3x + y - z+u^2=0 \\ x - y + 2z+u=0 \\ 2x + 2y - 3z+2u=0 \end{cases}$$ can be solved for $x,y,u$ in terms of $z$; for $x,z,u$ in terms of $y$; for $y,z,u$ in terms of $x$; but not for $x,y,z$ in terms of $u$. Proof: Let $\mathbf{f}:\mathbb{R}^4\to \mathbb{R}^3$ defined by $$\mathbf{f}(x,y,z,u)=(3x + y - z+u^2, x - y + 2z+u, 2x + 2y - 3z+2u).$$ Also we see that $\mathbf{f}(0,0,0,0)=(0,0,0)$ and $\mathbf{f}'(0,0,0,0)=A$ where $[A]$ has the following form (relative to the standard basis) $$[A]=\begin{bmatrix} 3 & 1 & -1 & 0 \\ 1 & -1 & 2 & 1 \\ 2 & 2 & -3 & 2  \end{bmatrix}$$ Our linear operator $A\in L(\mathbb{R}^4,\mathbb{R}^3)$ can be written as: $A(x,y,z,u)=A_1(x,y,u)+A_2(z)$ where $A_1(x,y,u)=A(x,y,0,u)$ and $A_2(z)=A(0,0,z,0)$. And $$[A_1]=\begin{bmatrix} 3 & 1  & 0 \\ 1 & -1  & 1 \\ 2 & 2 & 2  \end{bmatrix}$$ Since $\det[A_1]=-12\neq 0$ then $A_1$ is invertible. Then by implicit function theorem exists open neighborhood $W\in \mathbb{R}$ and $U\in \mathbb{R}^4$ of $0$ and $(0,0,0,0)$ respectively. Also to every $z\in W$ exists a unique $(x(z),y(z), u(z))$ such that $$(x(z),y(z), u(z),z)\in U \quad\text{and}\quad \mathbf{f}(x(z),y(z), u(z),z)=0.$$ Hence the system of equations can be solved for $x,y,u$ in terms of $z$. Analogous reasoning can be applied for another cases. How to show rigorously that above system can not be solved for $x,y,z$ in terms of $u$? Can anyone give the full answer? I would be very grateful for help!","Show that the system of equations $$\begin{cases} 3x + y - z+u^2=0 \\ x - y + 2z+u=0 \\ 2x + 2y - 3z+2u=0 \end{cases}$$ can be solved for $x,y,u$ in terms of $z$; for $x,z,u$ in terms of $y$; for $y,z,u$ in terms of $x$; but not for $x,y,z$ in terms of $u$. Proof: Let $\mathbf{f}:\mathbb{R}^4\to \mathbb{R}^3$ defined by $$\mathbf{f}(x,y,z,u)=(3x + y - z+u^2, x - y + 2z+u, 2x + 2y - 3z+2u).$$ Also we see that $\mathbf{f}(0,0,0,0)=(0,0,0)$ and $\mathbf{f}'(0,0,0,0)=A$ where $[A]$ has the following form (relative to the standard basis) $$[A]=\begin{bmatrix} 3 & 1 & -1 & 0 \\ 1 & -1 & 2 & 1 \\ 2 & 2 & -3 & 2  \end{bmatrix}$$ Our linear operator $A\in L(\mathbb{R}^4,\mathbb{R}^3)$ can be written as: $A(x,y,z,u)=A_1(x,y,u)+A_2(z)$ where $A_1(x,y,u)=A(x,y,0,u)$ and $A_2(z)=A(0,0,z,0)$. And $$[A_1]=\begin{bmatrix} 3 & 1  & 0 \\ 1 & -1  & 1 \\ 2 & 2 & 2  \end{bmatrix}$$ Since $\det[A_1]=-12\neq 0$ then $A_1$ is invertible. Then by implicit function theorem exists open neighborhood $W\in \mathbb{R}$ and $U\in \mathbb{R}^4$ of $0$ and $(0,0,0,0)$ respectively. Also to every $z\in W$ exists a unique $(x(z),y(z), u(z))$ such that $$(x(z),y(z), u(z),z)\in U \quad\text{and}\quad \mathbf{f}(x(z),y(z), u(z),z)=0.$$ Hence the system of equations can be solved for $x,y,u$ in terms of $z$. Analogous reasoning can be applied for another cases. How to show rigorously that above system can not be solved for $x,y,z$ in terms of $u$? Can anyone give the full answer? I would be very grateful for help!",,"['linear-algebra', 'multivariable-calculus']"
10,Helmholtz theorem,Helmholtz theorem,,"I have been told that the Helmholtz decomposition theorem says that every smooth vector field $\boldsymbol{F}$ [where I am not sure what precise assumptions are needed on $\boldsymbol{F}$] on an opportune   region $V\subset\mathbb{R}^3$ [satisfying certain conditions for whose precisation I would be very grateful to any answerer] can be expressed as $$  \boldsymbol{F}(\boldsymbol{x})=-\nabla\left[\int_{V}\frac{\nabla'\cdot \boldsymbol{F}(\boldsymbol{x}')}{4\pi\|\boldsymbol{x}-\boldsymbol{x}'\|}dV'-\oint_{\partial V}\frac{\boldsymbol{F}(\boldsymbol{x}')\cdot\hat{\boldsymbol{n}}(\boldsymbol{x}')}{4\pi\|\boldsymbol{x}-\boldsymbol{x}'\|}dS'\right]$$ $$+\nabla\times\left[\int_{V}\frac{\nabla'\times \boldsymbol{F}(\boldsymbol{x}')}{4\pi\|\boldsymbol{x}-\boldsymbol{x}'\|}dV'+\oint_{\partial V}\frac{\boldsymbol{F}(\boldsymbol{x}')\times\hat{\boldsymbol{n}}(\boldsymbol{x}')}{4\pi\|\boldsymbol{x}-\boldsymbol{x}'\|}dS'\right]$$[where I suppose that the $\int_V$ integrals are intended as limits of Riemann integrals or Lebesgue integrals]. I think I have been able to prove it ( below ), interpretating the integrals as Lebesgue integrals with $dV'=d\mu'$ where $\mu'$ is the usual tridimensional Lebesgue measure, for a compactly supported $\boldsymbol{F}\in C^2(\mathbb{R}^3)$ with $\boldsymbol{x}\in \mathring{V}$ and $V$ satisfying the hypothesis of Gauss's divergence theorem. Nevertheless, I am also interested in proofs of it under less strict assumptions on $\boldsymbol{F}$. What are the usual assumptions -I am particularly interested in the assumptions done in physics- on $\boldsymbol{F}$ and how can the theorem proved in that case? I heartily thank any answerer. I think that it would be interesting to generalise it to some space containing $C_c^2(\mathbb{R}^3)$ whose functions have ""smoothness"" properties usually considered true in physics, where the Helmholtz decomposition is much used, but I am not able to find such a space and prove the desired generalisation.","I have been told that the Helmholtz decomposition theorem says that every smooth vector field $\boldsymbol{F}$ [where I am not sure what precise assumptions are needed on $\boldsymbol{F}$] on an opportune   region $V\subset\mathbb{R}^3$ [satisfying certain conditions for whose precisation I would be very grateful to any answerer] can be expressed as $$  \boldsymbol{F}(\boldsymbol{x})=-\nabla\left[\int_{V}\frac{\nabla'\cdot \boldsymbol{F}(\boldsymbol{x}')}{4\pi\|\boldsymbol{x}-\boldsymbol{x}'\|}dV'-\oint_{\partial V}\frac{\boldsymbol{F}(\boldsymbol{x}')\cdot\hat{\boldsymbol{n}}(\boldsymbol{x}')}{4\pi\|\boldsymbol{x}-\boldsymbol{x}'\|}dS'\right]$$ $$+\nabla\times\left[\int_{V}\frac{\nabla'\times \boldsymbol{F}(\boldsymbol{x}')}{4\pi\|\boldsymbol{x}-\boldsymbol{x}'\|}dV'+\oint_{\partial V}\frac{\boldsymbol{F}(\boldsymbol{x}')\times\hat{\boldsymbol{n}}(\boldsymbol{x}')}{4\pi\|\boldsymbol{x}-\boldsymbol{x}'\|}dS'\right]$$[where I suppose that the $\int_V$ integrals are intended as limits of Riemann integrals or Lebesgue integrals]. I think I have been able to prove it ( below ), interpretating the integrals as Lebesgue integrals with $dV'=d\mu'$ where $\mu'$ is the usual tridimensional Lebesgue measure, for a compactly supported $\boldsymbol{F}\in C^2(\mathbb{R}^3)$ with $\boldsymbol{x}\in \mathring{V}$ and $V$ satisfying the hypothesis of Gauss's divergence theorem. Nevertheless, I am also interested in proofs of it under less strict assumptions on $\boldsymbol{F}$. What are the usual assumptions -I am particularly interested in the assumptions done in physics- on $\boldsymbol{F}$ and how can the theorem proved in that case? I heartily thank any answerer. I think that it would be interesting to generalise it to some space containing $C_c^2(\mathbb{R}^3)$ whose functions have ""smoothness"" properties usually considered true in physics, where the Helmholtz decomposition is much used, but I am not able to find such a space and prove the desired generalisation.",,"['real-analysis', 'multivariable-calculus', 'lebesgue-integral', 'vector-analysis', 'vector-fields']"
11,Demonstrate the existence of following limit using squeeze theorem,Demonstrate the existence of following limit using squeeze theorem,,"$$\lim_{(x,y)\to(0,0)} \sqrt[3]x \cdot e^\frac{-y^2}{x^2}$$ I have not idea what function to put in the inequality. Some aid?","$$\lim_{(x,y)\to(0,0)} \sqrt[3]x \cdot e^\frac{-y^2}{x^2}$$ I have not idea what function to put in the inequality. Some aid?",,"['limits', 'multivariable-calculus']"
12,Prove that $f:\mathbb{R}^n\to\mathbb{R}$ is continuous,Prove that  is continuous,f:\mathbb{R}^n\to\mathbb{R},"Let $f:\mathbb{R}^n\to\mathbb{R}$ such that for every continuous curve, $\gamma:[0,1]\to\mathbb{R}^n$: $f\circ\gamma$ is continuous. Prove that $f$ is continuous. So I know we shall prove it by a contradiction. Let's assume that $f$ isn't continuous at $x_0$. Then, there's a sequence, $\{x_k\}$ converging to $x_0$ such that $\lim_{k\to\infty} f(x_k) \ne f(x_0)$. Now, I need to have some curve in order to get a contradiction. I'd be glad to get help with that. Thanks.","Let $f:\mathbb{R}^n\to\mathbb{R}$ such that for every continuous curve, $\gamma:[0,1]\to\mathbb{R}^n$: $f\circ\gamma$ is continuous. Prove that $f$ is continuous. So I know we shall prove it by a contradiction. Let's assume that $f$ isn't continuous at $x_0$. Then, there's a sequence, $\{x_k\}$ converging to $x_0$ such that $\lim_{k\to\infty} f(x_k) \ne f(x_0)$. Now, I need to have some curve in order to get a contradiction. I'd be glad to get help with that. Thanks.",,"['calculus', 'general-topology', 'multivariable-calculus', 'curves']"
13,To find the volume of a certain solid cone,To find the volume of a certain solid cone,,A solid cone is obtained by connecting (with a line segment in $3$ dimensional Euclidean space ) every point of a plane region $S$ with a vertex not in the plane $S$ . Let $A$ denote the area of $S$ and let $h$ denote the altitude of the cone . Then what is the volume of the cone ? Please help ; thanks in advance,A solid cone is obtained by connecting (with a line segment in $3$ dimensional Euclidean space ) every point of a plane region $S$ with a vertex not in the plane $S$ . Let $A$ denote the area of $S$ and let $h$ denote the altitude of the cone . Then what is the volume of the cone ? Please help ; thanks in advance,,['multivariable-calculus']
14,"The function $f(r,\theta)=(r\cos\theta,r\sin\theta).$",The function,"f(r,\theta)=(r\cos\theta,r\sin\theta).","Consider the function $f:\mathbb{R}^{2}\rightarrow\mathbb{R^2}$ given by $$f(r,\theta)=(r\cos\theta,r\sin\theta)$$ I like to show that $f$ is one-to-one in some neighborhood of any non zero point $(r,\theta).$ I tried as $(r\cos\theta,r\sin\theta)=(s\cos\phi,s\sin\phi)$  Which gives $r=s$ and $\theta=\phi+2n\pi,$ How to show that $\theta=\phi?$ Please help. Thanks.","Consider the function $f:\mathbb{R}^{2}\rightarrow\mathbb{R^2}$ given by $$f(r,\theta)=(r\cos\theta,r\sin\theta)$$ I like to show that $f$ is one-to-one in some neighborhood of any non zero point $(r,\theta).$ I tried as $(r\cos\theta,r\sin\theta)=(s\cos\phi,s\sin\phi)$  Which gives $r=s$ and $\theta=\phi+2n\pi,$ How to show that $\theta=\phi?$ Please help. Thanks.",,"['calculus', 'multivariable-calculus', 'polar-coordinates']"
15,Change of variables problem.,Change of variables problem.,,"We're given a normal xy - plane , with the help of the transformations $ u = x^{2} - y^{2}$ and $v = 2xy$ , we need to plot the corresponding image in the $uv$-plane. First we need to find $x$ and $y$ in terms of $u$ and $v$ , By completing the square , I got : $ (x^{2} + y^{2})^{2} = u^{2} + v^{2}$ => $ x^{2} + y^{2} = \sqrt{u^{2} + v^{2}}$. If I try to find equations for $x$ and $y$ from this , the situation becomes messy .. Can anyone suggest a better way ? Update : Using the equations $y = 2x$ and $y=4$ ,  I got the equations in $u$ and $v$ as : $v= (\dfrac{-4}{3}) u$ and $v^{2} = 64(u + 16)$ , and I was able to draw a graph as follows : (I know that doesn't look like a parabola , but still.. ) Now I am not able to find the image of the line $ y = 2x - 10$ , I tried solving $ u = x^{2} - y^{2}$ and $v = 2xy$ for the given $y$ but the equation ends up in terms of $x$ only.. Could anyone help ?","We're given a normal xy - plane , with the help of the transformations $ u = x^{2} - y^{2}$ and $v = 2xy$ , we need to plot the corresponding image in the $uv$-plane. First we need to find $x$ and $y$ in terms of $u$ and $v$ , By completing the square , I got : $ (x^{2} + y^{2})^{2} = u^{2} + v^{2}$ => $ x^{2} + y^{2} = \sqrt{u^{2} + v^{2}}$. If I try to find equations for $x$ and $y$ from this , the situation becomes messy .. Can anyone suggest a better way ? Update : Using the equations $y = 2x$ and $y=4$ ,  I got the equations in $u$ and $v$ as : $v= (\dfrac{-4}{3}) u$ and $v^{2} = 64(u + 16)$ , and I was able to draw a graph as follows : (I know that doesn't look like a parabola , but still.. ) Now I am not able to find the image of the line $ y = 2x - 10$ , I tried solving $ u = x^{2} - y^{2}$ and $v = 2xy$ for the given $y$ but the equation ends up in terms of $x$ only.. Could anyone help ?",,"['calculus', 'multivariable-calculus']"
16,Local maximum implies derivative is $0$,Local maximum implies derivative is,0,"If $f$ is a differentiable real function in an open set $E \subset \Bbb R^n$ and $f$ has a local maximum at a point $\textbf{x} = (x_1, x_2, \cdots , x_n) \in E$, show $f'(\textbf{x}) = 0$. I think it would be nice if I could show that each component is $0$, which I think means showing the partial derivative at each component is $0$, but I don't know how to show this.","If $f$ is a differentiable real function in an open set $E \subset \Bbb R^n$ and $f$ has a local maximum at a point $\textbf{x} = (x_1, x_2, \cdots , x_n) \in E$, show $f'(\textbf{x}) = 0$. I think it would be nice if I could show that each component is $0$, which I think means showing the partial derivative at each component is $0$, but I don't know how to show this.",,"['real-analysis', 'multivariable-calculus', 'derivatives']"
17,A curve that will be perpendicular to all $c \sin x$,A curve that will be perpendicular to all,c \sin x,"I want to find a parametric curve that would be perpendicular to all curves $y=c \sin x$ I can see that these curves will be straight lines when $x=\frac{2n+1}2\pi$ and they should become tiny circles as $x\rightarrow n\pi$, but I do not see how I would do this mathematically or what the answer would look like. The only thing that came to my mind so far is to think of a function $F$ and equate its derivative to be $-\frac{1}{c \cos x}$ $$-\frac{\frac{\partial F}{\partial x}}{\frac{\partial F}{\partial y}}=-\frac{1}{c \cos x}$$ $$c\cos x\frac{\partial F}{\partial x}=\frac{\partial F}{\partial y}$$ A separable solution to this would be $$\exp \bigg(\frac{2 k \tanh ^{-1}\left(\tan \left(\frac{x}{2}\right)\right)}{c}+ky\bigg)$$ I want the parametric curves to be perpendicular for all $c$. But I do not know if this makes sense and how I would continue. Intuitively, I am expecting to see concentric ellipses centered at $x=n\pi$","I want to find a parametric curve that would be perpendicular to all curves $y=c \sin x$ I can see that these curves will be straight lines when $x=\frac{2n+1}2\pi$ and they should become tiny circles as $x\rightarrow n\pi$, but I do not see how I would do this mathematically or what the answer would look like. The only thing that came to my mind so far is to think of a function $F$ and equate its derivative to be $-\frac{1}{c \cos x}$ $$-\frac{\frac{\partial F}{\partial x}}{\frac{\partial F}{\partial y}}=-\frac{1}{c \cos x}$$ $$c\cos x\frac{\partial F}{\partial x}=\frac{\partial F}{\partial y}$$ A separable solution to this would be $$\exp \bigg(\frac{2 k \tanh ^{-1}\left(\tan \left(\frac{x}{2}\right)\right)}{c}+ky\bigg)$$ I want the parametric curves to be perpendicular for all $c$. But I do not know if this makes sense and how I would continue. Intuitively, I am expecting to see concentric ellipses centered at $x=n\pi$",,"['multivariable-calculus', 'curves']"
18,Intuitive meaning of smooth curve,Intuitive meaning of smooth curve,,"A curve ,let's say $(x(t),y(t))$ is said to be smooth if $x'(t)$ and $y'(t)$ both exist and are continuous.(Am I not right?) A function differentiable at a point intuitively means that its graph on coordinate plane has a unique tangent(no corner) at that point. So what does a smooth curve mean intuitively ? Thanks in advance!","A curve ,let's say $(x(t),y(t))$ is said to be smooth if $x'(t)$ and $y'(t)$ both exist and are continuous.(Am I not right?) A function differentiable at a point intuitively means that its graph on coordinate plane has a unique tangent(no corner) at that point. So what does a smooth curve mean intuitively ? Thanks in advance!",,"['calculus', 'complex-analysis', 'multivariable-calculus']"
19,Intersection of two non-linear equations?,Intersection of two non-linear equations?,,"Graphically it is clear that $$1 + 2e^{{(x-y)}^2}(x-y) = 0$$ $$e^{{(x-y)}^2} - y = 0$$ has a unique solution, but how do I solve this analytically? If this is not possible, then what would be an appropriate technique to try and estimate the answer.","Graphically it is clear that $$1 + 2e^{{(x-y)}^2}(x-y) = 0$$ $$e^{{(x-y)}^2} - y = 0$$ has a unique solution, but how do I solve this analytically? If this is not possible, then what would be an appropriate technique to try and estimate the answer.",,"['algebra-precalculus', 'multivariable-calculus', 'nonlinear-system']"
20,Why don't we consider $\mathbb{R}^3$ to be an affine space?,Why don't we consider  to be an affine space?,\mathbb{R}^3,"When we're introduced to $\mathbb{R}^3$ in multivariable calculus, we first think of it as a collection of points. Then we're taught that you can have these things called vectors , which are (equivalence classes of) arrows that start at one point and end up at another. At this point $\mathbb{R}^3$ is an affine space, not a vector space: for two points $x, y \in \mathbb{R}^3$, the operation $x + y$ is meaningless (my professor likes to say: ""You can't add Chicago and New York!"") but the operation $x - y$ gives a vector (the vector which points from New York to Chicago). You can also add a point and a vector, which gives you a translated point. The distinction between the point $(0, 1, 2)$ and the vector $\langle 0, 1, 2 \rangle$ is sometimes made. But then we quickly move on to treating $\mathbb{R}^3$ as a vector space, where instead of a point $A$, you have vectors starting at the origin with their tip at $A$. For example, parameterized curves such as $$r(t) = (t, t^2, 3t)$$ are called ""vector-valued functions"" and not ""point-valued functions"". So, my question is, what is the reason that we historically don't define two spaces -- $\mathbb{R}^3$ and $\mathbb{R}^3_{\text{affine}}$? (I'm sure there's better notation). For example, my ""point-valued function"" $r(t)$ would be a function $\mathbb{R} \rightarrow \mathbb{R}^3_{\text{affine}}$, but its derivative $r'(t)$ (the velocity vector ) would be a function $\mathbb{R} \rightarrow \mathbb{R}^3$. What would this make more difficult? In particular, I know that $\mathbb{R}^3 \iff \mathbb{R}^3_{\text{affine}}$ is a bijection, and that we use this sometimes, but how often in multivariable calculus? If we are using it all the time, then it wouldn't make sense to emphasize the distinction.","When we're introduced to $\mathbb{R}^3$ in multivariable calculus, we first think of it as a collection of points. Then we're taught that you can have these things called vectors , which are (equivalence classes of) arrows that start at one point and end up at another. At this point $\mathbb{R}^3$ is an affine space, not a vector space: for two points $x, y \in \mathbb{R}^3$, the operation $x + y$ is meaningless (my professor likes to say: ""You can't add Chicago and New York!"") but the operation $x - y$ gives a vector (the vector which points from New York to Chicago). You can also add a point and a vector, which gives you a translated point. The distinction between the point $(0, 1, 2)$ and the vector $\langle 0, 1, 2 \rangle$ is sometimes made. But then we quickly move on to treating $\mathbb{R}^3$ as a vector space, where instead of a point $A$, you have vectors starting at the origin with their tip at $A$. For example, parameterized curves such as $$r(t) = (t, t^2, 3t)$$ are called ""vector-valued functions"" and not ""point-valued functions"". So, my question is, what is the reason that we historically don't define two spaces -- $\mathbb{R}^3$ and $\mathbb{R}^3_{\text{affine}}$? (I'm sure there's better notation). For example, my ""point-valued function"" $r(t)$ would be a function $\mathbb{R} \rightarrow \mathbb{R}^3_{\text{affine}}$, but its derivative $r'(t)$ (the velocity vector ) would be a function $\mathbb{R} \rightarrow \mathbb{R}^3$. What would this make more difficult? In particular, I know that $\mathbb{R}^3 \iff \mathbb{R}^3_{\text{affine}}$ is a bijection, and that we use this sometimes, but how often in multivariable calculus? If we are using it all the time, then it wouldn't make sense to emphasize the distinction.",,['multivariable-calculus']
21,"Show using the definition of limit that $\lim_{ (x,y)\to(0,0)}\frac{ (1-\cos(xy))\sin y}{(x^2+y^2) }= 0$",Show using the definition of limit that,"\lim_{ (x,y)\to(0,0)}\frac{ (1-\cos(xy))\sin y}{(x^2+y^2) }= 0","can you help me with this excercise. Show using the definition of limit that $$\lim_{ (x,y)\to(0,0)}\frac{ (1-\cos(xy))\sin y}{(x^2+y^2) }= 0$$ Definition of limit: $\lim_{(x,y)\to(a,b)} f(x,y) =L$ if and only if for every $\epsilon >0$ exist $\delta>0$ such that if $\sqrt{(x-a)^2+(y-b)^2}<\delta$ then $|f(x,y)-L|<\epsilon$. Hi  I´ve tried this, For taylor series. Given $\epsilon>0$, find $\delta>0$ such that if $$\sqrt{x^2+y^2}<\delta$$ then $$\bigg|\frac{(1-\cos xy)\sin y}{x^2+y^2}\bigg|<\epsilon$$ \begin{align*} \bigg|\frac{(1-\cos xy)\sin y}{x^2+y^2}\bigg| & = \bigg|\frac{(1-1-\frac{x^2y^2}{2}+\frac{x^4y^4}{24})(y-\frac{y^3}{6}+\frac{y^5}{120})}{x^2+y^2}\bigg|\\ & =\bigg|\frac{(-\frac{x^2y^2}{2}+\frac{x^4y^4}{24})(y-\frac{y^3}{6}+\frac{y^5}{120})}{x^2+y^2}\bigg| \end{align*} Then what do I do?","can you help me with this excercise. Show using the definition of limit that $$\lim_{ (x,y)\to(0,0)}\frac{ (1-\cos(xy))\sin y}{(x^2+y^2) }= 0$$ Definition of limit: $\lim_{(x,y)\to(a,b)} f(x,y) =L$ if and only if for every $\epsilon >0$ exist $\delta>0$ such that if $\sqrt{(x-a)^2+(y-b)^2}<\delta$ then $|f(x,y)-L|<\epsilon$. Hi  I´ve tried this, For taylor series. Given $\epsilon>0$, find $\delta>0$ such that if $$\sqrt{x^2+y^2}<\delta$$ then $$\bigg|\frac{(1-\cos xy)\sin y}{x^2+y^2}\bigg|<\epsilon$$ \begin{align*} \bigg|\frac{(1-\cos xy)\sin y}{x^2+y^2}\bigg| & = \bigg|\frac{(1-1-\frac{x^2y^2}{2}+\frac{x^4y^4}{24})(y-\frac{y^3}{6}+\frac{y^5}{120})}{x^2+y^2}\bigg|\\ & =\bigg|\frac{(-\frac{x^2y^2}{2}+\frac{x^4y^4}{24})(y-\frac{y^3}{6}+\frac{y^5}{120})}{x^2+y^2}\bigg| \end{align*} Then what do I do?",,"['limits', 'multivariable-calculus']"
22,"Showing that $\lim_{(x,y) \to (0, 0)}\frac{xy^2}{x^2+y^2} = 0$",Showing that,"\lim_{(x,y) \to (0, 0)}\frac{xy^2}{x^2+y^2} = 0","Show that $$\lim_{(x,y) \to (0, 0)}\frac{xy^2}{x^2+y^2} = 0$$ I have tried switching to polar coordinates but I'm not getting a single term.  This is what I did. Putting $$x=r\sin θ,\quad  y=r\cos θ$$ we obtain $$\left|\frac{xy^2}{x^2+y^2}\right|=|r\cos^2 θ \sin θ|                        =|r\sin θ(1-\sin^2 θ)|                        = |r\sin θ-r\sin^3 θ|$$","Show that $$\lim_{(x,y) \to (0, 0)}\frac{xy^2}{x^2+y^2} = 0$$ I have tried switching to polar coordinates but I'm not getting a single term.  This is what I did. Putting $$x=r\sin θ,\quad  y=r\cos θ$$ we obtain $$\left|\frac{xy^2}{x^2+y^2}\right|=|r\cos^2 θ \sin θ|                        =|r\sin θ(1-\sin^2 θ)|                        = |r\sin θ-r\sin^3 θ|$$",,"['limits', 'multivariable-calculus']"
23,How is the Directional Derivative a linear transform?,How is the Directional Derivative a linear transform?,,"So I know basically what a directional derivative is and how to calculate it using the gradient vector, but I'm a bit lost on the more advanced approach of looking at it as a linear transform. I've read that multivariable calculus is about approximating nonlinear maps with linear ones, and I know that the Jacobian is the matrix associated with the directional derivative. However, I still don't really understand the directional derivative as a linear transform. For example, what is the input of the transform? Also, what is meant by approximating nonlinear maps by linear maps? I only know a little linear algebra so that might be why I'm so confused.","So I know basically what a directional derivative is and how to calculate it using the gradient vector, but I'm a bit lost on the more advanced approach of looking at it as a linear transform. I've read that multivariable calculus is about approximating nonlinear maps with linear ones, and I know that the Jacobian is the matrix associated with the directional derivative. However, I still don't really understand the directional derivative as a linear transform. For example, what is the input of the transform? Also, what is meant by approximating nonlinear maps by linear maps? I only know a little linear algebra so that might be why I'm so confused.",,['multivariable-calculus']
24,Div$f$ is invariant under an orthogonal change of coordinates,Div is invariant under an orthogonal change of coordinates,f,"Let $f: \mathbb{R^n} \to \mathbb{R^n}$ and $Df$ exists. I need to show that div$f$ is invariant under an orthogonal change of coordinates. Let $T:\mathbb{R^n} \to \mathbb{R^n}$ be an orthogonal transformation. Then Suppose that $$f(x_1,x_2,..,x_n)=(f_1(x_1,x_2,..,x_n),f_2(x_1,x_2,..,x_n),..,f_n(x_1,x_2,..,x_n))$$ Also suppose that $T(x_1,x_2,..,x_n)=(x'_1,x'_2,..,x'_n),T^{t}T=TT^{t}=I$ Let $X=(x_1,x_2,..x_n)$ and $X'=(x'_1,x'_2,..,x'_n)$ Now I want to use the fact that $divf=trace(Df)$. So I let $$g(X')=f(T(X))$$(considering them as a function of $X$ and $X'$ respectively) This is where I am stuck. Somehow on differentiating two sides I should have $DT$ and $(DT)^{-1}$ and that should do it for me. Thanks for the help!!","Let $f: \mathbb{R^n} \to \mathbb{R^n}$ and $Df$ exists. I need to show that div$f$ is invariant under an orthogonal change of coordinates. Let $T:\mathbb{R^n} \to \mathbb{R^n}$ be an orthogonal transformation. Then Suppose that $$f(x_1,x_2,..,x_n)=(f_1(x_1,x_2,..,x_n),f_2(x_1,x_2,..,x_n),..,f_n(x_1,x_2,..,x_n))$$ Also suppose that $T(x_1,x_2,..,x_n)=(x'_1,x'_2,..,x'_n),T^{t}T=TT^{t}=I$ Let $X=(x_1,x_2,..x_n)$ and $X'=(x'_1,x'_2,..,x'_n)$ Now I want to use the fact that $divf=trace(Df)$. So I let $$g(X')=f(T(X))$$(considering them as a function of $X$ and $X'$ respectively) This is where I am stuck. Somehow on differentiating two sides I should have $DT$ and $(DT)^{-1}$ and that should do it for me. Thanks for the help!!",,"['calculus', 'multivariable-calculus', 'vectors']"
25,Evaluate $\oint_{C} e^{-x} \sin y \;dx+e^{-x} \cos y\;dy$,Evaluate,\oint_{C} e^{-x} \sin y \;dx+e^{-x} \cos y\;dy,"I need to evaluate the following integral using Green's theorem $$\oint_{C} e^{-x} \sin y \;dx+e^{-x} \cos y\;dy$$   $C$: from point $E \to F\to G\to H$   $$E=(0,0)\,,F=(\pi,0)\;,G=(\pi,\frac{\pi}{2}),\;H=(0,\frac{\pi}{2})$$ My attempt: $$\oint_{C} \underbrace{e^{-x} \sin y}_{P} \;dx+\underbrace{e^{-x} \cos y}_{Q}\;dy$$ $$=\iint\bigg(\frac{\partial(e^{-x} \cos y)}{\partial x} -\frac{\partial(e^{-x} \sin y)}{\partial y}\bigg)dxdy$$ $$=\iint\bigg(-e^{-x}\cos y-e^{-x}\cos y\bigg)dxdy$$ $$=\iint\bigg(-2e^{-x}\cos y\bigg)dxdy$$ $$=\int_{y=0}^{y=\pi/2}\bigg(\int_{0}^{\pi}[-2e^{-x}\cos y] dx\bigg)dy$$ $$=\int_{y=0}^{y=\pi/2}\bigg([2e^{-x}\cos y] \bigg|_0^{\pi}\bigg)dy$$ $$=\int_{y=0}^{y=\pi/2}\bigg([2e^{-\pi}-2] \bigg)dy$$ $$=2\cdot \frac{\pi}{2}e^{-\pi}-2\frac{\pi}{2}$$ $$=\boxed{\pi e^{-\pi}-\pi}$$ Is it correct? EDIT: After using @mickep answer $$=\int_{y=0}^{y=\pi/2}\bigg([2e^{-x}\cos y] \bigg|_0^{\pi}\bigg)dy$$ $$=\int_{y=0}^{y=\pi/2}\bigg([2e^{-\pi}\cos y-2\cos y] \bigg)dy$$ $$2e^{-\pi}\sin y -2\sin y\bigg|_0^{\pi / 2}$$ $$=\boxed{2e^{-\pi}-2}$$","I need to evaluate the following integral using Green's theorem $$\oint_{C} e^{-x} \sin y \;dx+e^{-x} \cos y\;dy$$   $C$: from point $E \to F\to G\to H$   $$E=(0,0)\,,F=(\pi,0)\;,G=(\pi,\frac{\pi}{2}),\;H=(0,\frac{\pi}{2})$$ My attempt: $$\oint_{C} \underbrace{e^{-x} \sin y}_{P} \;dx+\underbrace{e^{-x} \cos y}_{Q}\;dy$$ $$=\iint\bigg(\frac{\partial(e^{-x} \cos y)}{\partial x} -\frac{\partial(e^{-x} \sin y)}{\partial y}\bigg)dxdy$$ $$=\iint\bigg(-e^{-x}\cos y-e^{-x}\cos y\bigg)dxdy$$ $$=\iint\bigg(-2e^{-x}\cos y\bigg)dxdy$$ $$=\int_{y=0}^{y=\pi/2}\bigg(\int_{0}^{\pi}[-2e^{-x}\cos y] dx\bigg)dy$$ $$=\int_{y=0}^{y=\pi/2}\bigg([2e^{-x}\cos y] \bigg|_0^{\pi}\bigg)dy$$ $$=\int_{y=0}^{y=\pi/2}\bigg([2e^{-\pi}-2] \bigg)dy$$ $$=2\cdot \frac{\pi}{2}e^{-\pi}-2\frac{\pi}{2}$$ $$=\boxed{\pi e^{-\pi}-\pi}$$ Is it correct? EDIT: After using @mickep answer $$=\int_{y=0}^{y=\pi/2}\bigg([2e^{-x}\cos y] \bigg|_0^{\pi}\bigg)dy$$ $$=\int_{y=0}^{y=\pi/2}\bigg([2e^{-\pi}\cos y-2\cos y] \bigg)dy$$ $$2e^{-\pi}\sin y -2\sin y\bigg|_0^{\pi / 2}$$ $$=\boxed{2e^{-\pi}-2}$$",,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals', 'proof-verification']"
26,Change of variable (Fourier Transform related),Change of variable (Fourier Transform related),,"Consider a problem below... The solution offered to this particular question (1)a)) simply state the change of variable ksi to by to yield the result, I'm failing miserably to see how.","Consider a problem below... The solution offered to this particular question (1)a)) simply state the change of variable ksi to by to yield the result, I'm failing miserably to see how.",,['multivariable-calculus']
27,"Proof: $f(x,y)=\sqrt{4x^2+y^2}$ is continuous at $(0,0)$",Proof:  is continuous at,"f(x,y)=\sqrt{4x^2+y^2} (0,0)","Prove $f(x,y)=\sqrt{4x^2+y^2}$ is continuous at $(0,0)$. Attempt I need to find a $\delta(\epsilon)$: $$\forall \epsilon>0\exists \delta>0: 0<\sqrt{x^2+y^2}<δ \implies |\sqrt{4x^2+y^2}|<\epsilon $$ I set $\delta=\dfrac \epsilon 2$ so $$x^2+y^2<\dfrac {\epsilon^2}4 \implies 4x^2+y^2\leq 4x^2+4y^2< \epsilon^2 \implies f(x,y)=\sqrt{4x^2+y^2}<\epsilon$$ The only thing missing is the absolute value, but as I'm taking the square root of squares (I assume real numbers, problem doesn't mention it), $f \geq 0$ so $f=|f|$ so I'm done, is this correct? A more important question, if this is correct, is how would you find a value for $\delta$? I did this by trying out many values, but don't know how to arrive at an expresion $\delta =g(\epsilon)$ in a ""deductive"" fashion. E: Forgot to add this when I posted this question: Is it possible to generalize this method to prove continuity of this function in $\Bbb R^2$?","Prove $f(x,y)=\sqrt{4x^2+y^2}$ is continuous at $(0,0)$. Attempt I need to find a $\delta(\epsilon)$: $$\forall \epsilon>0\exists \delta>0: 0<\sqrt{x^2+y^2}<δ \implies |\sqrt{4x^2+y^2}|<\epsilon $$ I set $\delta=\dfrac \epsilon 2$ so $$x^2+y^2<\dfrac {\epsilon^2}4 \implies 4x^2+y^2\leq 4x^2+4y^2< \epsilon^2 \implies f(x,y)=\sqrt{4x^2+y^2}<\epsilon$$ The only thing missing is the absolute value, but as I'm taking the square root of squares (I assume real numbers, problem doesn't mention it), $f \geq 0$ so $f=|f|$ so I'm done, is this correct? A more important question, if this is correct, is how would you find a value for $\delta$? I did this by trying out many values, but don't know how to arrive at an expresion $\delta =g(\epsilon)$ in a ""deductive"" fashion. E: Forgot to add this when I posted this question: Is it possible to generalize this method to prove continuity of this function in $\Bbb R^2$?",,"['multivariable-calculus', 'epsilon-delta']"
28,Using Green's theorem to find an area.,Using Green's theorem to find an area.,,I wish to find out the area enclosed by the ellipse $C:=2x^2+3y^2=2y$ using Green's theorem. I know how to parametrize the ellipse and understand Green's theorem I just don't understand how it is useful in this case. Looking at my notes it says $$Area=\int_C x~dy$$ but it isn't at all obvious where this comes from and that this is even true. Could anyone clarify.,I wish to find out the area enclosed by the ellipse $C:=2x^2+3y^2=2y$ using Green's theorem. I know how to parametrize the ellipse and understand Green's theorem I just don't understand how it is useful in this case. Looking at my notes it says $$Area=\int_C x~dy$$ but it isn't at all obvious where this comes from and that this is even true. Could anyone clarify.,,['multivariable-calculus']
29,$\Delta u$ is bounded. Can we say $u\in C^1$?,is bounded. Can we say ?,\Delta u u\in C^1,"Let $\Omega\subset\mathbb{R}^n$ be a bounded open set. Let us say it has a Lipschitz boundary. Consider the Laplacian $\Delta$ in the classical sense. Suppose $\Delta u=\frac{\partial^2}{\partial x_1^2}u+\dotsb+\frac{\partial^2}{\partial x_n^2}u$ is bounded. Q: Can we say $u\in C^1(\Omega)$? Does it depend on the dimension $n$? Can we claim the smoothness recursively, i.e., if $\Delta^m u$: bounded, then,...etc? I was pondering about the relations of partial differentiability and continuity, and got confused. Bounded partial derivatives imply continuity says ""If all partial derivatives of f are bounded, then f is continuous on E."", but we cannot apply this argument recursively as we do not have the ""cross term"" $\frac{\partial^2}{\partial x_i\partial x_j}$. We have $\Delta u\in L^2(\Omega)$ as $\Delta u$ is bounded on a bounded region $\Omega$. However, we cannot use this result Sobolov Space $W^{2,2}\cap W^{1,2}_0$ norm equivalence and say $u\in H^2(\Omega)$, because 1. $u$ does not necessarily vanish on the boundary, and 2. we are not sure if $\frac{\partial^2}{\partial x_1^2}u+\dotsb+\frac{\partial^2}{\partial x_n^2}u+(\text{partial derivatives of cross terms})u$ are bounded. Aha, from Equivalent Norms on Sobolev Spaces , $\Delta u\in L^2(\Omega)$ is enough to say $u\in H^2(\Omega)$. But one thing is I do not know if we have the same kind of equivalence for $m>3$, and another thing is resorting to Sobolev embedding does not sound like a good idea as it depends on the dimension heavily. I wonder I could show this directly.","Let $\Omega\subset\mathbb{R}^n$ be a bounded open set. Let us say it has a Lipschitz boundary. Consider the Laplacian $\Delta$ in the classical sense. Suppose $\Delta u=\frac{\partial^2}{\partial x_1^2}u+\dotsb+\frac{\partial^2}{\partial x_n^2}u$ is bounded. Q: Can we say $u\in C^1(\Omega)$? Does it depend on the dimension $n$? Can we claim the smoothness recursively, i.e., if $\Delta^m u$: bounded, then,...etc? I was pondering about the relations of partial differentiability and continuity, and got confused. Bounded partial derivatives imply continuity says ""If all partial derivatives of f are bounded, then f is continuous on E."", but we cannot apply this argument recursively as we do not have the ""cross term"" $\frac{\partial^2}{\partial x_i\partial x_j}$. We have $\Delta u\in L^2(\Omega)$ as $\Delta u$ is bounded on a bounded region $\Omega$. However, we cannot use this result Sobolov Space $W^{2,2}\cap W^{1,2}_0$ norm equivalence and say $u\in H^2(\Omega)$, because 1. $u$ does not necessarily vanish on the boundary, and 2. we are not sure if $\frac{\partial^2}{\partial x_1^2}u+\dotsb+\frac{\partial^2}{\partial x_n^2}u+(\text{partial derivatives of cross terms})u$ are bounded. Aha, from Equivalent Norms on Sobolev Spaces , $\Delta u\in L^2(\Omega)$ is enough to say $u\in H^2(\Omega)$. But one thing is I do not know if we have the same kind of equivalence for $m>3$, and another thing is resorting to Sobolev embedding does not sound like a good idea as it depends on the dimension heavily. I wonder I could show this directly.",,"['calculus', 'multivariable-calculus', 'sobolev-spaces', 'partial-derivative']"
30,Difference between a Fréchet derivative and a total derivative,Difference between a Fréchet derivative and a total derivative,,"I've heard many times that they are somehow similar and in some cases mean the same thing. Consider this function: $$f(x,y)=x^2y$$ I have to calculate the Fréchet derivative $f'(x_0,y_0)$ and some ""remainder"" $r(\binom{h}{k})$. I've been told that, in this case, Fréchet derivative and total derivative mean the same thing. However, I can't seem to remember that this remainder is also present in the total derivative. Therefore, what exactly is the difference between Fréchet and total derivative? Are they the same thing in my example?","I've heard many times that they are somehow similar and in some cases mean the same thing. Consider this function: $$f(x,y)=x^2y$$ I have to calculate the Fréchet derivative $f'(x_0,y_0)$ and some ""remainder"" $r(\binom{h}{k})$. I've been told that, in this case, Fréchet derivative and total derivative mean the same thing. However, I can't seem to remember that this remainder is also present in the total derivative. Therefore, what exactly is the difference between Fréchet and total derivative? Are they the same thing in my example?",,"['calculus', 'multivariable-calculus', 'derivatives']"
31,"$\nabla \times \left(\frac{\mathbf{A \times r}}{r^3}\right)$, where $\mathbf{A}$ is independent of $\displaystyle\nabla \times$",", where  is independent of",\nabla \times \left(\frac{\mathbf{A \times r}}{r^3}\right) \mathbf{A} \displaystyle\nabla \times,"The curl is just over $\mathbf{r}$ and $r$. I've been trying to pull the vector $\displaystyle \mathbf{A}$ out of the way, in order to get a expression much easier to deal with, but I have no idea how to do it without expanding the whole expression (I'm not sure whether what I did is correct either). This is my attempt: $$\nabla \times \left(\frac{\mathbf{A \times r}}{r^3}\right) = \frac{\nabla \times (\mathbf{A \times r})r^3+ (\mathbf{A \times r)}\nabla r^3}{r^6}$$ In the second term of the numerator: $\displaystyle \nabla r^3 = 3r^2\mathbf{\hat{r}}$ For the nastiest part of the first term: \begin{align*} \nabla \times (\mathbf{A \times r}) &= \mathbf{A}(\nabla \cdot \mathbf{r}) - \mathbf{r}(\nabla \cdot \mathbf{A})+ (\mathbf{r}\cdot \nabla ) \mathbf{A} - (\mathbf{A} \cdot \nabla)\mathbf{r}\\ &= 3\mathbf{A}- 0 +  (\mathbf{r}\cdot \nabla) \mathbf{A} - (\mathbf{A} \cdot \nabla)\mathbf{r} \end{align*} According to me, $\displaystyle \nabla \cdot \mathbf{A} = 0$. And I'm stuck there, I don't know what to do with the directional derivatives, but it doesn't seem to be correct so far. Thanks!!","The curl is just over $\mathbf{r}$ and $r$. I've been trying to pull the vector $\displaystyle \mathbf{A}$ out of the way, in order to get a expression much easier to deal with, but I have no idea how to do it without expanding the whole expression (I'm not sure whether what I did is correct either). This is my attempt: $$\nabla \times \left(\frac{\mathbf{A \times r}}{r^3}\right) = \frac{\nabla \times (\mathbf{A \times r})r^3+ (\mathbf{A \times r)}\nabla r^3}{r^6}$$ In the second term of the numerator: $\displaystyle \nabla r^3 = 3r^2\mathbf{\hat{r}}$ For the nastiest part of the first term: \begin{align*} \nabla \times (\mathbf{A \times r}) &= \mathbf{A}(\nabla \cdot \mathbf{r}) - \mathbf{r}(\nabla \cdot \mathbf{A})+ (\mathbf{r}\cdot \nabla ) \mathbf{A} - (\mathbf{A} \cdot \nabla)\mathbf{r}\\ &= 3\mathbf{A}- 0 +  (\mathbf{r}\cdot \nabla) \mathbf{A} - (\mathbf{A} \cdot \nabla)\mathbf{r} \end{align*} According to me, $\displaystyle \nabla \cdot \mathbf{A} = 0$. And I'm stuck there, I don't know what to do with the directional derivatives, but it doesn't seem to be correct so far. Thanks!!",,['multivariable-calculus']
32,The derivative of a recurrence relation of functions,The derivative of a recurrence relation of functions,,"I am unsure of how to take the derivative of a recurrence relation of functions. For example consider the following recurrence relation: \begin{equation} \left\{ \begin{array}{cl} f_n(x) &= a_n\cdot f_{n-1}^2(x) \\ f_0(x) &= x \end{array} \right. \end{equation} with $0 \leq n\leq L \in \mathbb{N}$ such that $F = f_L$. How do I notate, and thereby compute, the partial derivative of the relation with respect to some $a_m$; that is, $$\frac{\partial F}{\partial a_m} =\;?$$ Does this result in a recursive definition of partial derivatives (i.e. $D_{a_m} f_n = c_n(x) D_{a_m}f_{n-1}$ where $D_x g$ is the partial derivative of $g$ with respect to $x$)? If so, can I solve this recurrence relation to a closed form of $m$ using the methods of homogeneous recurrence relations? Lastly, can I construct a recurrence relation for $D_{a_m} F$ which acts in the reverse direction; that is, can I define a sequence such that $D_{a_m} F$ is defined by $D_{a_{m+1}}$, which is defined by $D_{a_{m+2}}, \dots,$ which is defined by $D_{a_{L}}$? Thanks","I am unsure of how to take the derivative of a recurrence relation of functions. For example consider the following recurrence relation: \begin{equation} \left\{ \begin{array}{cl} f_n(x) &= a_n\cdot f_{n-1}^2(x) \\ f_0(x) &= x \end{array} \right. \end{equation} with $0 \leq n\leq L \in \mathbb{N}$ such that $F = f_L$. How do I notate, and thereby compute, the partial derivative of the relation with respect to some $a_m$; that is, $$\frac{\partial F}{\partial a_m} =\;?$$ Does this result in a recursive definition of partial derivatives (i.e. $D_{a_m} f_n = c_n(x) D_{a_m}f_{n-1}$ where $D_x g$ is the partial derivative of $g$ with respect to $x$)? If so, can I solve this recurrence relation to a closed form of $m$ using the methods of homogeneous recurrence relations? Lastly, can I construct a recurrence relation for $D_{a_m} F$ which acts in the reverse direction; that is, can I define a sequence such that $D_{a_m} F$ is defined by $D_{a_{m+1}}$, which is defined by $D_{a_{m+2}}, \dots,$ which is defined by $D_{a_{L}}$? Thanks",,"['calculus', 'multivariable-calculus', 'derivatives', 'recurrence-relations', 'partial-derivative']"
33,Evaluate the integral by type1 or type 2,Evaluate the integral by type1 or type 2,,"Evaluate $\displaystyle\int_{0}^{2} \int_{0}^{\log(x)}(x-1)\sqrt{1+e^y}\,dy\,dx$. I have tried integration by substitution but can't connect to type 1 or type 2. Any help.","Evaluate $\displaystyle\int_{0}^{2} \int_{0}^{\log(x)}(x-1)\sqrt{1+e^y}\,dy\,dx$. I have tried integration by substitution but can't connect to type 1 or type 2. Any help.",,"['multivariable-calculus', 'improper-integrals']"
34,"$f,g$ diffirentiable function at point $(x_0, y_0)$ how to show that $fg$ diffirentiable function at point $(x_0, y_0)$?",diffirentiable function at point  how to show that  diffirentiable function at point ?,"f,g (x_0, y_0) fg (x_0, y_0)","I guess there is pretty simple way of showing the statement below.. I tried using definition but it seem complicated. Suppose $f, g: \Bbb R^{2} \to \Bbb R$.  Prove if $f, g$ are differentiable at $(x_{0}, y_{0})$, then the product, $fg$, is differentiable at $(x_{0}, y_{0})$.","I guess there is pretty simple way of showing the statement below.. I tried using definition but it seem complicated. Suppose $f, g: \Bbb R^{2} \to \Bbb R$.  Prove if $f, g$ are differentiable at $(x_{0}, y_{0})$, then the product, $fg$, is differentiable at $(x_{0}, y_{0})$.",,"['calculus', 'multivariable-calculus']"
35,Whats is the meaning of the derivative of a vector function?,Whats is the meaning of the derivative of a vector function?,,"Assuming we have a continuous and well behaved vector function  $$ R(t) = \langle f(t), g(t), h(t) \rangle $$ then its derivative at an arbitrary point a is $R'(a)$, which can be computed (I'm not using the definition not to clutter the post) as  $$ R'(x) = \langle f'(a), g'(a), h'(a)\rangle $$ This gives the vector tangent to the vector function curve, at point a. But what does the function defined by R'(t) describes? I have tried to think about it and search it on the books that I have, but I have found no answer.","Assuming we have a continuous and well behaved vector function  $$ R(t) = \langle f(t), g(t), h(t) \rangle $$ then its derivative at an arbitrary point a is $R'(a)$, which can be computed (I'm not using the definition not to clutter the post) as  $$ R'(x) = \langle f'(a), g'(a), h'(a)\rangle $$ This gives the vector tangent to the vector function curve, at point a. But what does the function defined by R'(t) describes? I have tried to think about it and search it on the books that I have, but I have found no answer.",,"['calculus', 'multivariable-calculus', 'vectors']"
36,How calculate definite double integral $\int_0^1\int_0^1\mathrm{ln}(|x-y|)\;\mathrm{d}x\;\mathrm{d}y$?,How calculate definite double integral ?,\int_0^1\int_0^1\mathrm{ln}(|x-y|)\;\mathrm{d}x\;\mathrm{d}y,"I want to calculate the integral:$$\int_0^1\int_0^1\mathrm{ln}(|x-y|)\;\mathrm{d}x\;\mathrm{d}y,$$ but as you can see when $x=y$ the integrand goes to $-\infty$. Does this integral has a solution? Could you give me any hint about how to solve it? Thanks!","I want to calculate the integral:$$\int_0^1\int_0^1\mathrm{ln}(|x-y|)\;\mathrm{d}x\;\mathrm{d}y,$$ but as you can see when $x=y$ the integrand goes to $-\infty$. Does this integral has a solution? Could you give me any hint about how to solve it? Thanks!",,"['multivariable-calculus', 'definite-integrals']"
37,"If the integral of a vector field over a closed curve equals zero, is the field conservative?","If the integral of a vector field over a closed curve equals zero, is the field conservative?",,"If a vector field has a potential, then the integral of that vector field over every closed curve is zero. If the integral of a vector field over a closed curve equals zero, does that imply that the vector field is conservative?","If a vector field has a potential, then the integral of that vector field over every closed curve is zero. If the integral of a vector field over a closed curve equals zero, does that imply that the vector field is conservative?",,"['calculus', 'multivariable-calculus']"
38,"Why is this True ? It's probability question, but really an multivariable integration question","Why is this True ? It's probability question, but really an multivariable integration question",,"I'm given that $X$ is a non-negative continuous random variable show that $$E(X) = \int^\infty_0 [1-F(x)]\,dx$$ $F(x)$is a cdf The solution is following $$\int^\infty_0 (1-F(x)) \, dx = \int^\infty_0 P(X > x) \, dx = \int^\infty_0 \int^\infty_x f_x(t) \, dt\,dx = \int^\infty_0 f(t) \left(\int^t_0 dx\right) \, dt$$ I don't understand why is it true for the last two equality, when the order $dx$ and $dy$ been changed. What's the theorem behind it ? (i only know basic double integration calculation)","I'm given that $X$ is a non-negative continuous random variable show that $$E(X) = \int^\infty_0 [1-F(x)]\,dx$$ $F(x)$is a cdf The solution is following $$\int^\infty_0 (1-F(x)) \, dx = \int^\infty_0 P(X > x) \, dx = \int^\infty_0 \int^\infty_x f_x(t) \, dt\,dx = \int^\infty_0 f(t) \left(\int^t_0 dx\right) \, dt$$ I don't understand why is it true for the last two equality, when the order $dx$ and $dy$ been changed. What's the theorem behind it ? (i only know basic double integration calculation)",,"['probability', 'multivariable-calculus']"
39,Show that $T(t)$ and $N(t)$ are Orthogonal,Show that  and  are Orthogonal,T(t) N(t),"If $r(t)$ is the smooth parametrization of a curve $C$ in 3-space, then the unit tangent and unit normal vectors are denoted as $T$ and $N$, and are given by: $$T(t)=\frac{r'(t)}{||r'(t)||},N(t)=\frac{T'(t)}{||T'(t)||}$$ How do I show that $T(t)$ and $N(t)$ are orthogonal for all $t$ at which they are defined. I cannot assume anything about the binormal vector $B$, so I want to know how to do this without assuming so.","If $r(t)$ is the smooth parametrization of a curve $C$ in 3-space, then the unit tangent and unit normal vectors are denoted as $T$ and $N$, and are given by: $$T(t)=\frac{r'(t)}{||r'(t)||},N(t)=\frac{T'(t)}{||T'(t)||}$$ How do I show that $T(t)$ and $N(t)$ are orthogonal for all $t$ at which they are defined. I cannot assume anything about the binormal vector $B$, so I want to know how to do this without assuming so.",,"['multivariable-calculus', 'differential-geometry']"
40,Surface integral over parabolic cylinder that lies inside another cylinder,Surface integral over parabolic cylinder that lies inside another cylinder,,"To be precise, I'm given the following: Find $\iint_K {xdS}$ over the part of parabolic cylinder $z = \frac{{{x^2}}}{2}$  that lies inside the first octant part of the cylinder $x^2+y^2=1$. In my attempt, I tried to parametrize as following: $x=r\cos(\theta),y=r\sin(\theta),z=r^2\cos^2(\theta)/2$. The surface element turned out to be $dS=r\sqrt{r^2\cos^2(\theta)+1}drd\theta$, which means that considering the substitution, the integral (now, a double integral) is: $$\iint\limits_D {{r^2}\cos (\theta )\sqrt {{r^2}{{\cos }^2}(\theta ) + 1} drd\theta },\qquad D = \{ 0 \leqslant r \leqslant 1,0 \leqslant \theta  \leqslant \pi /2\} $$ Numerically, this turns out to be $\pi/8$, but I don't think it is clear how this can be solved analytically. Is there any way to solve these kinds of double integrals where the variables are bounded in such a way, or should a different parametrization be used instead?","To be precise, I'm given the following: Find $\iint_K {xdS}$ over the part of parabolic cylinder $z = \frac{{{x^2}}}{2}$  that lies inside the first octant part of the cylinder $x^2+y^2=1$. In my attempt, I tried to parametrize as following: $x=r\cos(\theta),y=r\sin(\theta),z=r^2\cos^2(\theta)/2$. The surface element turned out to be $dS=r\sqrt{r^2\cos^2(\theta)+1}drd\theta$, which means that considering the substitution, the integral (now, a double integral) is: $$\iint\limits_D {{r^2}\cos (\theta )\sqrt {{r^2}{{\cos }^2}(\theta ) + 1} drd\theta },\qquad D = \{ 0 \leqslant r \leqslant 1,0 \leqslant \theta  \leqslant \pi /2\} $$ Numerically, this turns out to be $\pi/8$, but I don't think it is clear how this can be solved analytically. Is there any way to solve these kinds of double integrals where the variables are bounded in such a way, or should a different parametrization be used instead?",,"['calculus', 'multivariable-calculus', 'surface-integrals']"
41,"Evaluating the triple integral $\iiint \limits_R ze^{-(x^2+y^2+z^2)} \, \, dV$",Evaluating the triple integral,"\iiint \limits_R ze^{-(x^2+y^2+z^2)} \, \, dV","Evaluate the following triple integrals as a repeated integral using an appropriate coordinate systems: $$\iiint\limits_R ze^{-(x^2+y^2+z^2)} \, \, dV ,$$ where $$R=\{ (x,y,z): \, x,y \in (-\infty, \infty), \, 0 \leq z \leq 1 \}.$$ It is simple to integrate after using cylindrical coordinates but how do you figure out the limits of $r$ and $\theta$?","Evaluate the following triple integrals as a repeated integral using an appropriate coordinate systems: $$\iiint\limits_R ze^{-(x^2+y^2+z^2)} \, \, dV ,$$ where $$R=\{ (x,y,z): \, x,y \in (-\infty, \infty), \, 0 \leq z \leq 1 \}.$$ It is simple to integrate after using cylindrical coordinates but how do you figure out the limits of $r$ and $\theta$?",,['multivariable-calculus']
42,Compute the wedge product n times,Compute the wedge product n times,,"Let $\omega$ be a 2-differential form in $\mathbb{R}^{2n}$ given by $$\displaystyle \omega=dx^1\wedge dx^2+dx^3\wedge dx^4 + \cdots + dx^{2n-1}\wedge dx^{2n}$$ Compute: $$\displaystyle \overbrace{\omega\wedge\omega\wedge\cdots\wedge\omega}^{\text{n times}}.$$ Then I have already done the case n=1,2,3 so I have in general that 1) $\omega\wedge\omega$ is 4-form 2) $\omega\wedge\omega\wedge\omega$ is a 6 -form $\vdots$ n-1) ($\omega\wedge\omega\wedge\cdots\wedge\omega$) is a 2n-form therefore we have the relation as in the n step we have a (2k+1)-form where k is the number of wedges that apear, so we have that  product will be zero since we have all the 2n members of the basis ($dx^i$) in the for of the step n-1 so the product should be zero. Am I right? or How can I do this in a better way? Thanks a lot for your help in advance :).","Let $\omega$ be a 2-differential form in $\mathbb{R}^{2n}$ given by $$\displaystyle \omega=dx^1\wedge dx^2+dx^3\wedge dx^4 + \cdots + dx^{2n-1}\wedge dx^{2n}$$ Compute: $$\displaystyle \overbrace{\omega\wedge\omega\wedge\cdots\wedge\omega}^{\text{n times}}.$$ Then I have already done the case n=1,2,3 so I have in general that 1) $\omega\wedge\omega$ is 4-form 2) $\omega\wedge\omega\wedge\omega$ is a 6 -form $\vdots$ n-1) ($\omega\wedge\omega\wedge\cdots\wedge\omega$) is a 2n-form therefore we have the relation as in the n step we have a (2k+1)-form where k is the number of wedges that apear, so we have that  product will be zero since we have all the 2n members of the basis ($dx^i$) in the for of the step n-1 so the product should be zero. Am I right? or How can I do this in a better way? Thanks a lot for your help in advance :).",,"['analysis', 'multivariable-calculus', 'proof-verification', 'differential-forms']"
43,Intuition of Greens Theorem in the plane,Intuition of Greens Theorem in the plane,,"I'm trying to understand a special case of Greens Theorem. Let $V: \Omega \to \mathbb{R}^2$ be a $C^1$ vector field defined an open set  $\Omega \subseteq \mathbb{R}^2$. Let $\gamma$ be a $C^1$-kurve, that is closed and has no loops and runs in the positive direction. Then $$ \int_{\gamma} V \cdot \mathrm{d}r = \int_E \left( \dfrac{\partial V_2}{\partial x}(x,y) - \dfrac{\partial V_1}{\partial y}(x,y)\right) \mathrm{d}(x,y) $$ where $E$ is the area enclosed by $\gamma$. If $V$ is a closed vector field, meaning that $\dfrac{\partial V_1}{\partial y} = \dfrac{\partial V_2}{\partial x}$ then what does Greens theorem state? Now I can see that I'm integrating $0$ but I don't understand whats going on here. I'm very new to integrals in higher dimensions, so I'm lacking a severe amount of intuition.","I'm trying to understand a special case of Greens Theorem. Let $V: \Omega \to \mathbb{R}^2$ be a $C^1$ vector field defined an open set  $\Omega \subseteq \mathbb{R}^2$. Let $\gamma$ be a $C^1$-kurve, that is closed and has no loops and runs in the positive direction. Then $$ \int_{\gamma} V \cdot \mathrm{d}r = \int_E \left( \dfrac{\partial V_2}{\partial x}(x,y) - \dfrac{\partial V_1}{\partial y}(x,y)\right) \mathrm{d}(x,y) $$ where $E$ is the area enclosed by $\gamma$. If $V$ is a closed vector field, meaning that $\dfrac{\partial V_1}{\partial y} = \dfrac{\partial V_2}{\partial x}$ then what does Greens theorem state? Now I can see that I'm integrating $0$ but I don't understand whats going on here. I'm very new to integrals in higher dimensions, so I'm lacking a severe amount of intuition.",,"['multivariable-calculus', 'vector-analysis']"
44,Minimize : $\sqrt{(1+{1\over a})(1+{1\over b})}$ subject to $a+b=\lambda$.,Minimize :  subject to .,\sqrt{(1+{1\over a})(1+{1\over b})} a+b=\lambda,"Given positive real variables $a$ and $b$, find the minimum of  $$f(a,b)=\sqrt{\left(1+{1\over a}\right)\left(1+{1\over b}\right)}$$  subject to  $a+b=\lambda$ where $\lambda$ is a constant . [ISI Sample Papers] Method $1$ : Substitute $b=\lambda - a$ and then compute ${\partial \over \partial a }f(a,b)$. But, the calculations get a bit messy. Method $2$ : Actually this is what I want to know. Is there an easier approach using some inequalities like the AM-GM inequality ? I tried this but was not able to got lost in between. Method $3$ : Lagrange multipliers. I have not tried this and kept it as a last option. What is the best way to solve this problem ?","Given positive real variables $a$ and $b$, find the minimum of  $$f(a,b)=\sqrt{\left(1+{1\over a}\right)\left(1+{1\over b}\right)}$$  subject to  $a+b=\lambda$ where $\lambda$ is a constant . [ISI Sample Papers] Method $1$ : Substitute $b=\lambda - a$ and then compute ${\partial \over \partial a }f(a,b)$. But, the calculations get a bit messy. Method $2$ : Actually this is what I want to know. Is there an easier approach using some inequalities like the AM-GM inequality ? I tried this but was not able to got lost in between. Method $3$ : Lagrange multipliers. I have not tried this and kept it as a last option. What is the best way to solve this problem ?",,"['multivariable-calculus', 'inequality', 'optimization', 'problem-solving', 'lagrange-multiplier']"
45,"Can someone provide a simple example of the ""pre-image theorem"" in differential geometry?","Can someone provide a simple example of the ""pre-image theorem"" in differential geometry?",,"I only have a background in engineering calculus. A problem I am currently working on relates to something called a ""pre-image theorem"" The theorem roughly states: Let $f: N \to R^{m}$ be a $C^{\infty} $ map on a manifold N of   dimension n. Then a nonempty regular level set $S = f^{-1}(c)$ is a   regular submanifold of dimension n-m of N I am confounded by the language used in this theorem, but I really wish to understand this. Can someone translate this into a simple case where $f$ is some three dimensional function i.e. $f = x^2 + y^2 + z^2$ can show how this theorem applies? Thanks!","I only have a background in engineering calculus. A problem I am currently working on relates to something called a ""pre-image theorem"" The theorem roughly states: Let $f: N \to R^{m}$ be a $C^{\infty} $ map on a manifold N of   dimension n. Then a nonempty regular level set $S = f^{-1}(c)$ is a   regular submanifold of dimension n-m of N I am confounded by the language used in this theorem, but I really wish to understand this. Can someone translate this into a simple case where $f$ is some three dimensional function i.e. $f = x^2 + y^2 + z^2$ can show how this theorem applies? Thanks!",,"['calculus', 'multivariable-calculus', 'differential-geometry']"
46,Line integral (not using Stokes theorem),Line integral (not using Stokes theorem),,"Evaluate $$\int_C Fdr$$ $$F=<-y^2,x,z^2>$$ $C$ is the curve of intersection of the plane $y+z=2$ and the cylinder $x^2+y^2=1$ I can parametrize the curve using cylindrical coordinates but I don't know exactly how I would do that when $x$ is missing from the plane equation. Any ideas?","Evaluate $$\int_C Fdr$$ $$F=<-y^2,x,z^2>$$ $C$ is the curve of intersection of the plane $y+z=2$ and the cylinder $x^2+y^2=1$ I can parametrize the curve using cylindrical coordinates but I don't know exactly how I would do that when $x$ is missing from the plane equation. Any ideas?",,['multivariable-calculus']
47,Finding potential function for a vector field,Finding potential function for a vector field,,"Let $\mathbf{F}(x,y,z) = y \hat{i} + x \hat{j} + z^2 \hat{k}$ be a vector field. Determine if its conservative, and find a potential if it is. Attempt at solution: We have that $\frac{\partial F_1}{\partial y} = 1 = \frac{\partial F_2}{\partial x} $, $\frac{\partial F_1}{\partial z} = 0 = \frac{\partial F_3}{\partial x}$, $\frac{\partial F_2}{\partial z} = 0 =  \frac{\partial F_3}{\partial y}$, so the potential might exist. Now we need to find a function $f$ such that $\nabla f = \mathbf{F}$. For the first component, this means that $\frac{\partial f(x,y,z)}{\partial x} = y $, or after integrating, $f(x,y,z) = yx + C(y,z)$. Now I don't know how to determine the constant of integration $C(y,z)$, and I don't understand if I should add another constant when I integrate the second component. For the second component, we have that $f(x,y,z) = xy + D(x,z)$, and for the third $f(x,y,z) = \frac{z^3}{3} + E(x,y)$. What now? Any help please? In my textbook this is explained really in a terrible way.","Let $\mathbf{F}(x,y,z) = y \hat{i} + x \hat{j} + z^2 \hat{k}$ be a vector field. Determine if its conservative, and find a potential if it is. Attempt at solution: We have that $\frac{\partial F_1}{\partial y} = 1 = \frac{\partial F_2}{\partial x} $, $\frac{\partial F_1}{\partial z} = 0 = \frac{\partial F_3}{\partial x}$, $\frac{\partial F_2}{\partial z} = 0 =  \frac{\partial F_3}{\partial y}$, so the potential might exist. Now we need to find a function $f$ such that $\nabla f = \mathbf{F}$. For the first component, this means that $\frac{\partial f(x,y,z)}{\partial x} = y $, or after integrating, $f(x,y,z) = yx + C(y,z)$. Now I don't know how to determine the constant of integration $C(y,z)$, and I don't understand if I should add another constant when I integrate the second component. For the second component, we have that $f(x,y,z) = xy + D(x,z)$, and for the third $f(x,y,z) = \frac{z^3}{3} + E(x,y)$. What now? Any help please? In my textbook this is explained really in a terrible way.",,"['calculus', 'multivariable-calculus', 'vector-fields']"
48,Is the gradient of a convex paraboloid pointing up or down?,Is the gradient of a convex paraboloid pointing up or down?,,Intuitively the gradient is the vector pointing to the maximum rate of change. But this can be either up or down. How would the gradient point on this surface?,Intuitively the gradient is the vector pointing to the maximum rate of change. But this can be either up or down. How would the gradient point on this surface?,,"['multivariable-calculus', 'gradient-flows']"
49,A challenging improper integral,A challenging improper integral,,"The integral is $$\int_0^1\frac{dx}{\sqrt{-\ln x}}.$$ Not sure if it helps, but it is in the same problem section as $$\int_0^\infty e^{-x^2}dx.$$","The integral is $$\int_0^1\frac{dx}{\sqrt{-\ln x}}.$$ Not sure if it helps, but it is in the same problem section as $$\int_0^\infty e^{-x^2}dx.$$",,"['multivariable-calculus', 'definite-integrals', 'improper-integrals']"
50,Calculate $\int_0^1\int_0^1\frac{x-y}{(x+y)^3}dydx$,Calculate,\int_0^1\int_0^1\frac{x-y}{(x+y)^3}dydx,"$$\int_0^1\int_0^1\frac{x-y}{(x+y)^3}dydx$$ The only way I can think of doing this, is to do integration by parts. However, this will get messy very quickly. Is there a better way of doing it?","$$\int_0^1\int_0^1\frac{x-y}{(x+y)^3}dydx$$ The only way I can think of doing this, is to do integration by parts. However, this will get messy very quickly. Is there a better way of doing it?",,"['integration', 'multivariable-calculus']"
51,"Find the domain of this function in the x,y,z graph","Find the domain of this function in the x,y,z graph",,"Find the domain of this function in set notation: $f(x,y)=\dfrac{\sqrt{x^2+y^2+4}}v$ $v=y-x$ Could I say that $x^2+y^2+4 \geq 0$ and $y-x \geq 0$ and based off those statements put that into set notation? and get: $D= \{(x,y)|x^2+y^2+4 \neq 0$ and $y-x \neq 0\}$","Find the domain of this function in set notation: $f(x,y)=\dfrac{\sqrt{x^2+y^2+4}}v$ $v=y-x$ Could I say that $x^2+y^2+4 \geq 0$ and $y-x \geq 0$ and based off those statements put that into set notation? and get: $D= \{(x,y)|x^2+y^2+4 \neq 0$ and $y-x \neq 0\}$",,"['multivariable-calculus', 'functions']"
52,Find the Range and Domain of the following function,Find the Range and Domain of the following function,,"The function is: $f(x,y) = \frac{2}{\sqrt{3-x}} + \frac{1}{\sqrt{4-y}}$ I have found the domain and the Range intuitively. But how would I formally prove that my assumption of the Range and Domain is true?","The function is: $f(x,y) = \frac{2}{\sqrt{3-x}} + \frac{1}{\sqrt{4-y}}$ I have found the domain and the Range intuitively. But how would I formally prove that my assumption of the Range and Domain is true?",,"['geometry', 'multivariable-calculus', '3d']"
53,Finding minimum value of a function of two variables,Finding minimum value of a function of two variables,,"I am given function $$ f(x,y)=Ax^2+2Bxy+Cy^2+2Dx+2Ey+F,\quad\text{where }A>0\text{ and }B^2<AC . $$ Prove that a point $(a,b)$ exists which $f$ has a minimum. I figured out that there is no stationary point for this equation. So, I am trying to change quadratic parts to sum of squares (as the ""hint"" says). But I failed to change it. Also, Why $f(a,b)=Da+Eb+F$ is at this minimum..?","I am given function $$ f(x,y)=Ax^2+2Bxy+Cy^2+2Dx+2Ey+F,\quad\text{where }A>0\text{ and }B^2<AC . $$ Prove that a point $(a,b)$ exists which $f$ has a minimum. I figured out that there is no stationary point for this equation. So, I am trying to change quadratic parts to sum of squares (as the ""hint"" says). But I failed to change it. Also, Why $f(a,b)=Da+Eb+F$ is at this minimum..?",,"['algebra-precalculus', 'multivariable-calculus', 'optimization']"
54,Area of ellipse not in xy-plane,Area of ellipse not in xy-plane,,"I've got a problem in which I'm trying to find the area of an ellipse which is given by the intersection of an elliptic cylinder with a plane. Nothing here is parallel to the coordinate axes, which is making it kind of annoying to work with. The plane is given by the equation $x+ay+a^2z=0$, and the cylinder is given by $(x-a^2z)^2-a(x-a^2z)(y-az)+a^2(y-az)^2=L^2$. I can think of some complicated ways to do it with integrals, and I'm wondering if there's something simpler that I'm missing. If I could transform into some coordinates in the plane I'm trying to work with, and I knew that the coordinate transformation would preserve areas, then I would be good, because I know how to get the area of an ellipse in the form $Ax^2+Bxy+Cy^2=1$. I'm really not sure how to do this transformation, though, and whether this is even the best way to proceed. Maybe I should be using Langrange multipliers with two constraints to just obtain the lengths of the semi-major and semi-minor axes? That sounds like a pain, but doable. At least we're centered at the origin. Thanks in advance for any assistance. Edit: If I take $u=(x-a^2z)$ and $v=(y-az)$, then my cylinder becomes $u^2-auv+a^2v^2=L^2$, which is pretty great, but then I've basically cut the cylinder with a plane parallel to $z=0$. In that case, my given plane becomes $u+av+3a^2z=0$, and I'm not sure how that's helpful.","I've got a problem in which I'm trying to find the area of an ellipse which is given by the intersection of an elliptic cylinder with a plane. Nothing here is parallel to the coordinate axes, which is making it kind of annoying to work with. The plane is given by the equation $x+ay+a^2z=0$, and the cylinder is given by $(x-a^2z)^2-a(x-a^2z)(y-az)+a^2(y-az)^2=L^2$. I can think of some complicated ways to do it with integrals, and I'm wondering if there's something simpler that I'm missing. If I could transform into some coordinates in the plane I'm trying to work with, and I knew that the coordinate transformation would preserve areas, then I would be good, because I know how to get the area of an ellipse in the form $Ax^2+Bxy+Cy^2=1$. I'm really not sure how to do this transformation, though, and whether this is even the best way to proceed. Maybe I should be using Langrange multipliers with two constraints to just obtain the lengths of the semi-major and semi-minor axes? That sounds like a pain, but doable. At least we're centered at the origin. Thanks in advance for any assistance. Edit: If I take $u=(x-a^2z)$ and $v=(y-az)$, then my cylinder becomes $u^2-auv+a^2v^2=L^2$, which is pretty great, but then I've basically cut the cylinder with a plane parallel to $z=0$. In that case, my given plane becomes $u+av+3a^2z=0$, and I'm not sure how that's helpful.",,"['multivariable-calculus', 'analytic-geometry', 'conic-sections']"
55,Show that $\oint f\nabla g \cdot d\alpha = -\oint g \nabla f \cdot d\alpha $ for every piecewise smooth Jordan curve $C$ in $S$.,Show that  for every piecewise smooth Jordan curve  in .,\oint f\nabla g \cdot d\alpha = -\oint g \nabla f \cdot d\alpha  C S,"If $f$ and $g$ are continuously differntiable in an open connected set $S$ in the plane, show that $\oint f\nabla g \cdot d\alpha = -\oint g \nabla f \cdot d\alpha $ for every piecewise smooth Jordan curve $C$ in $S$. Attempt :$\oint f\nabla g \cdot d\alpha + \oint g \nabla g \cdot d\alpha  = f ~d(g(\alpha)) + g~d(f(\alpha)) = \iint_R ( \dfrac {\partial g}{\partial f}- \dfrac {\partial f}{\partial g}    ) df~dg $ . ( By Green's Theorem ) How do I proceed now? We need to prove that $\dfrac {\partial g(\alpha)}{\partial f(\alpha)}= \dfrac {\partial f(\alpha)}{\partial g(\alpha)}$ ? I just have a connecting idea that in an open connected space, a gradient follows the same behavior what we are looking to prove, but then, nothing is mentioned about the presence of a gradient function either. Thank you very much for the help.","If $f$ and $g$ are continuously differntiable in an open connected set $S$ in the plane, show that $\oint f\nabla g \cdot d\alpha = -\oint g \nabla f \cdot d\alpha $ for every piecewise smooth Jordan curve $C$ in $S$. Attempt :$\oint f\nabla g \cdot d\alpha + \oint g \nabla g \cdot d\alpha  = f ~d(g(\alpha)) + g~d(f(\alpha)) = \iint_R ( \dfrac {\partial g}{\partial f}- \dfrac {\partial f}{\partial g}    ) df~dg $ . ( By Green's Theorem ) How do I proceed now? We need to prove that $\dfrac {\partial g(\alpha)}{\partial f(\alpha)}= \dfrac {\partial f(\alpha)}{\partial g(\alpha)}$ ? I just have a connecting idea that in an open connected space, a gradient follows the same behavior what we are looking to prove, but then, nothing is mentioned about the presence of a gradient function either. Thank you very much for the help.",,"['multivariable-calculus', 'vector-analysis']"
56,is the Jacobian Determinant continuous,is the Jacobian Determinant continuous,,Is the Determinant of the Jacobian a continuous function? i.e.  $$f:\mathbb{R}^n \rightarrow \mathbb{R}^n $$ $$ \forall \varepsilon >0 \quad \exists \delta >0 : |x-x_0 |<\delta \Longrightarrow |det(Jf(x))-det(Jf(x_0))|<\varepsilon $$ How would I go about showing this? Does the $\varepsilon - \delta$ definition help in this case or would it be more appropriate to use the inverse function theorem? Any suggestions appreciated,Is the Determinant of the Jacobian a continuous function? i.e.  $$f:\mathbb{R}^n \rightarrow \mathbb{R}^n $$ $$ \forall \varepsilon >0 \quad \exists \delta >0 : |x-x_0 |<\delta \Longrightarrow |det(Jf(x))-det(Jf(x_0))|<\varepsilon $$ How would I go about showing this? Does the $\varepsilon - \delta$ definition help in this case or would it be more appropriate to use the inverse function theorem? Any suggestions appreciated,,"['multivariable-calculus', 'determinant']"
57,"Inconclusive second derivative test at (0,0) for $x^{4} + y^{4} - 2x^{2} - 2y^{2} +4xy $","Inconclusive second derivative test at (0,0) for",x^{4} + y^{4} - 2x^{2} - 2y^{2} +4xy ,"Second derivative test is inconclusive here , given f( x, y) is $x^{4} + y^{4} - 2x^{2} - 2y^{2} +4xy $ At (0,0) how do i check nature ? Also i would like to know general tactics when things  like these happen .Thank You","Second derivative test is inconclusive here , given f( x, y) is $x^{4} + y^{4} - 2x^{2} - 2y^{2} +4xy $ At (0,0) how do i check nature ? Also i would like to know general tactics when things  like these happen .Thank You",,['multivariable-calculus']
58,Teaching myself multivariable calculus,Teaching myself multivariable calculus,,"I want to learn multivariable calculus and I need a book suitable for self-study. I looked around on Amazon and found two books that seem to contain the right material: Clark Bray: Multivariable Calculus and Larson/Edwards: Multivariable Calculus Unfortunately, the first book is described as targeted at engineering majors (by the author himself on his website). And the second book seems to be too colorful and fancy so that I suspect it's also for engineering majors. I major in pure maths. Can I (a maths major) use either of these books (for engineering) to learn multivariable calculus? And if not, which multivariable calculus book is suitable for a maths   major? I am looking for an undergraduate low-level text with lots of exercises and  solutions. I want to practice the material as well as study proofs. Edit Why I am worried that these books might be no good to me: One thing is that since they seem to be written for engineers I am worried that there are no or only few proofs. The other thing that I'm worried about is that they don't cover the same scope of material like a book targeted at maths majors. Edit 2 After some more searching I found Edwards: Advanced Calculus of several variables It looks good topic-wise but its title suggests that it's advanced and I only know first year real analysis in one variable. This book contains a chapter about differential forms (isn't that rather advanced?) Also, it does have exercises but no solutions. I am worried that this book is too advanced for me. Does anyone have any experience with this book?","I want to learn multivariable calculus and I need a book suitable for self-study. I looked around on Amazon and found two books that seem to contain the right material: Clark Bray: Multivariable Calculus and Larson/Edwards: Multivariable Calculus Unfortunately, the first book is described as targeted at engineering majors (by the author himself on his website). And the second book seems to be too colorful and fancy so that I suspect it's also for engineering majors. I major in pure maths. Can I (a maths major) use either of these books (for engineering) to learn multivariable calculus? And if not, which multivariable calculus book is suitable for a maths   major? I am looking for an undergraduate low-level text with lots of exercises and  solutions. I want to practice the material as well as study proofs. Edit Why I am worried that these books might be no good to me: One thing is that since they seem to be written for engineers I am worried that there are no or only few proofs. The other thing that I'm worried about is that they don't cover the same scope of material like a book targeted at maths majors. Edit 2 After some more searching I found Edwards: Advanced Calculus of several variables It looks good topic-wise but its title suggests that it's advanced and I only know first year real analysis in one variable. This book contains a chapter about differential forms (isn't that rather advanced?) Also, it does have exercises but no solutions. I am worried that this book is too advanced for me. Does anyone have any experience with this book?",,"['multivariable-calculus', 'self-learning', 'education', 'book-recommendation']"
59,Optimize over unit circle to prove $|ax + by| \le \sqrt{a^2 + b^2}$,Optimize over unit circle to prove,|ax + by| \le \sqrt{a^2 + b^2},"I have the following problem which, straight off the shelf, seems totally approachable.  It's been giving me difficulty however: Let $a,b,x,y \in \mathbb{R}$, and suppose that $x^2 + y^2 =1$. Prove that $|ax + by| \le \sqrt{a^2 + b^2}$. I have started by considering the function $f(x,y) = ax + by$, and by defining a constraint function $g(x,y) = x^2 + y^2$. Then if I can maximize and minimize $f(x,y)$ subject to the constraint $g(x,y) = 1$, I should be done. By both methods (the method of Lagrange multipliers, and the method of evaluating $f(x,y)$ on the unit circle, finding critical points, etc.) I am having difficulties. For the Lagrange multiplier method, we know that we will maximize/minimize $f(x,y)$ subject to the constraint $g(x,y) = 1$ if the system $\nabla g = \lambda \nabla f$, $g(x,y) = 1$ is satisfied. In terms of our particular example, we obtain the system of three equations \begin{align*} 2 \lambda x &= a\\ 2 \lambda y &= b\\   x^2 + y^2 &=1. \end{align*} I'm not sure how to solve this system. I considered using the function $h(x,y) = (f(x,y))^2 = (ax + by)^2$ instead of $f$, but then things don't work out nicely for the minimum values of $f(x,y)$ and $h(x,y)$. For the other method, I'm even less sure of what I'm doing.  Any help would be greatly appreciated.","I have the following problem which, straight off the shelf, seems totally approachable.  It's been giving me difficulty however: Let $a,b,x,y \in \mathbb{R}$, and suppose that $x^2 + y^2 =1$. Prove that $|ax + by| \le \sqrt{a^2 + b^2}$. I have started by considering the function $f(x,y) = ax + by$, and by defining a constraint function $g(x,y) = x^2 + y^2$. Then if I can maximize and minimize $f(x,y)$ subject to the constraint $g(x,y) = 1$, I should be done. By both methods (the method of Lagrange multipliers, and the method of evaluating $f(x,y)$ on the unit circle, finding critical points, etc.) I am having difficulties. For the Lagrange multiplier method, we know that we will maximize/minimize $f(x,y)$ subject to the constraint $g(x,y) = 1$ if the system $\nabla g = \lambda \nabla f$, $g(x,y) = 1$ is satisfied. In terms of our particular example, we obtain the system of three equations \begin{align*} 2 \lambda x &= a\\ 2 \lambda y &= b\\   x^2 + y^2 &=1. \end{align*} I'm not sure how to solve this system. I considered using the function $h(x,y) = (f(x,y))^2 = (ax + by)^2$ instead of $f$, but then things don't work out nicely for the minimum values of $f(x,y)$ and $h(x,y)$. For the other method, I'm even less sure of what I'm doing.  Any help would be greatly appreciated.",,"['multivariable-calculus', 'lagrange-multiplier']"
60,To check continuity of Multivariable functions,To check continuity of Multivariable functions,,"To check continuity of function at origin given by  $$F (x, y) = \begin{cases}\dfrac{xy^{2}}{x^{2} + y ^{4}}&;& \mbox{otherwise},\\ 0&;&\mbox{ at origin}. \end{cases}$$","To check continuity of function at origin given by  $$F (x, y) = \begin{cases}\dfrac{xy^{2}}{x^{2} + y ^{4}}&;& \mbox{otherwise},\\ 0&;&\mbox{ at origin}. \end{cases}$$",,"['limits', 'multivariable-calculus', 'continuity']"
61,Proving that $\left( \frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial y^2} \right) \lvert f(z) \rvert^2 = 4 \lvert f'(z) \rvert^2$,Proving that,\left( \frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial y^2} \right) \lvert f(z) \rvert^2 = 4 \lvert f'(z) \rvert^2,"Given $f$ entire show that $$ \left( \frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial y^2} \right) \lvert f(z) \rvert^2 = 4 \lvert f'(z) \rvert^2 $$ I've come close to getting the exact answer by writing $f(z)$ as $u(x,y)+iv(x,y)$ and realizing the Laplacian of f is equal to zero. This leads to a lot of cancellations but I'm still making mistakes in my computations and I can't figure out where. Any help is appreciated.","Given $f$ entire show that $$ \left( \frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial y^2} \right) \lvert f(z) \rvert^2 = 4 \lvert f'(z) \rvert^2 $$ I've come close to getting the exact answer by writing $f(z)$ as $u(x,y)+iv(x,y)$ and realizing the Laplacian of f is equal to zero. This leads to a lot of cancellations but I'm still making mistakes in my computations and I can't figure out where. Any help is appreciated.",,"['complex-analysis', 'multivariable-calculus', 'derivatives', 'partial-derivative', 'laplacian']"
62,"Give an example of a function $f:D\subseteq \mathbb R^2 \to \mathbb R$ so that $lim_{x\to x_0}f(x,y)$ does not exist",Give an example of a function  so that  does not exist,"f:D\subseteq \mathbb R^2 \to \mathbb R lim_{x\to x_0}f(x,y)","Give an example of a function $f:D\subseteq \mathbb R^2 \to \mathbb R$ so that $$lim_{y\to y_0}f(x,y)$$ and $$lim_{x\to x_0}(lim_{y\rightarrow y_0}f(x,y))$$ exists but $lim_{x\to x_0}f(x,y)$ does not exist for a fixed $(x_0,y_0)\in D$ I haven´t been able to give an example. Can you please help me? I would really appreciate it :)","Give an example of a function $f:D\subseteq \mathbb R^2 \to \mathbb R$ so that $$lim_{y\to y_0}f(x,y)$$ and $$lim_{x\to x_0}(lim_{y\rightarrow y_0}f(x,y))$$ exists but $lim_{x\to x_0}f(x,y)$ does not exist for a fixed $(x_0,y_0)\in D$ I haven´t been able to give an example. Can you please help me? I would really appreciate it :)",,"['limits', 'multivariable-calculus', 'examples-counterexamples']"
63,"differentiability of $f(x,y)=xy\sin\left({1\over x^2+y^2}\right)$",differentiability of,"f(x,y)=xy\sin\left({1\over x^2+y^2}\right)","Let $f(x,y)=xy\sin\left({1\over x^2+y^2}\right)$ if $(x,y)\neq (0,0)$ and $0$ if $(x,y)=(0,0)$. Determine the points in which $f$ is differentiable I know that $f(x,y)$ is differentiable at $(x_0,y_0)$ iff there exists a linear function $l:\mathbb R^2 \to \mathbb R$ so that: $$\lim_{(x,y)\to (x_0,y_0)}{f(x,y)-f(x_0,y_0)-l(x-x_0,y-y_0)\over \|\vec x - \vec x_0\|}=0$$ If I want to see the points in which $f$ is differentiable then I need to pick an arbitry point $(x_0,y_0)$ but then the last expression becomes very complicated; Is there an easy way do it? I would really appreciate your help :)","Let $f(x,y)=xy\sin\left({1\over x^2+y^2}\right)$ if $(x,y)\neq (0,0)$ and $0$ if $(x,y)=(0,0)$. Determine the points in which $f$ is differentiable I know that $f(x,y)$ is differentiable at $(x_0,y_0)$ iff there exists a linear function $l:\mathbb R^2 \to \mathbb R$ so that: $$\lim_{(x,y)\to (x_0,y_0)}{f(x,y)-f(x_0,y_0)-l(x-x_0,y-y_0)\over \|\vec x - \vec x_0\|}=0$$ If I want to see the points in which $f$ is differentiable then I need to pick an arbitry point $(x_0,y_0)$ but then the last expression becomes very complicated; Is there an easy way do it? I would really appreciate your help :)",,"['multivariable-calculus', 'approximation-theory']"
64,Simplifying Double Integrals to Single-Variable Integrals,Simplifying Double Integrals to Single-Variable Integrals,,"Let D be a subset of $\mathbb{R}^2$ defined by $ |x| + |y| \leq 1$, and let $f$ be a continuous single-variable function on the interval $[-1,1]$. Show that $$ \iint\limits_D \,f(x+y) \,  \mathrm{d}x \, \mathrm{d}y = \int_{-1}^{-1} \, f(u) \,  \mathrm{d}u $$ This makes sense when you consider the region D since the values of x and y essentially range from -1 to 1 but I can't figure out a first solid step into the proof. Intuitively it looks plausible to me but that's it. Any help?","Let D be a subset of $\mathbb{R}^2$ defined by $ |x| + |y| \leq 1$, and let $f$ be a continuous single-variable function on the interval $[-1,1]$. Show that $$ \iint\limits_D \,f(x+y) \,  \mathrm{d}x \, \mathrm{d}y = \int_{-1}^{-1} \, f(u) \,  \mathrm{d}u $$ This makes sense when you consider the region D since the values of x and y essentially range from -1 to 1 but I can't figure out a first solid step into the proof. Intuitively it looks plausible to me but that's it. Any help?",,['multivariable-calculus']
65,Volume between a cone and and an Hyperboloid,Volume between a cone and and an Hyperboloid,,"I'm trying to use integration in several variables to find put what is the volume between  the cone $x^2+y^2=z^2$ and the hyperboloid $x^2+y^2=3+z^2$ I'm having a hard time with this problem, as the two surfaces actually never meet. I'm open to your suggestions, maybe even a convenient variable change.","I'm trying to use integration in several variables to find put what is the volume between  the cone $x^2+y^2=z^2$ and the hyperboloid $x^2+y^2=3+z^2$ I'm having a hard time with this problem, as the two surfaces actually never meet. I'm open to your suggestions, maybe even a convenient variable change.",,"['real-analysis', 'integration', 'multivariable-calculus']"
66,Solve this triple integral,Solve this triple integral,,"I am trying to solve this triple integral: $$\int_0^{ 2\pi} \int_0^4 \int_3^\sqrt{25-r^2} r \: dz \: dr \: d \theta$$. I get stuck at  $$\int_0^{ 2\pi} \int_0^4 \ r(\sqrt{25-r^2}-3)  \: dr \: d \theta$$ Should I use u-sub to find the rest? And if so, how?","I am trying to solve this triple integral: $$\int_0^{ 2\pi} \int_0^4 \int_3^\sqrt{25-r^2} r \: dz \: dr \: d \theta$$. I get stuck at  $$\int_0^{ 2\pi} \int_0^4 \ r(\sqrt{25-r^2}-3)  \: dr \: d \theta$$ Should I use u-sub to find the rest? And if so, how?",,"['calculus', 'multivariable-calculus']"
67,Frobenius theorem for 2-plane fields on some open set in $\mathbb{R}^3$,Frobenius theorem for 2-plane fields on some open set in,\mathbb{R}^3,"I need help with this two part question. I am rather confused by it. let $f(x, y, z)$, $g(x, y, z)$ be smooth on $U \subset \mathbb{R}^3$ with $f^2 + g^2 > 0$ on $U$. Define the differential form $$\omega = f(x, y, z)\,dx + g(x, y, z)\,dy$$ on $U$. Establish necessary and sufficient conditions on $f$, $g$ for the plane field defined by $\omega$ on $U$ to be integrable. Explain the condition you find geometrically. What do the integral surfaces look like? Let $a(z)$, $b(x)$, $c(y)$ be smooth functions on $U \subset \mathbb{R}^3$, not all vanishing. Repeat the first part for $$\omega = a(z)\,dx + b(x)\,dy + c(y)\,dz.$$ I understand that for the first part the necessary and sufficient condition for integrability is $\omega \wedge d\omega = 0$, and that the right thing to do for the second part is to forget about $U$ and assume that the functions are smooth on all of $\mathbb{R}^3$.","I need help with this two part question. I am rather confused by it. let $f(x, y, z)$, $g(x, y, z)$ be smooth on $U \subset \mathbb{R}^3$ with $f^2 + g^2 > 0$ on $U$. Define the differential form $$\omega = f(x, y, z)\,dx + g(x, y, z)\,dy$$ on $U$. Establish necessary and sufficient conditions on $f$, $g$ for the plane field defined by $\omega$ on $U$ to be integrable. Explain the condition you find geometrically. What do the integral surfaces look like? Let $a(z)$, $b(x)$, $c(y)$ be smooth functions on $U \subset \mathbb{R}^3$, not all vanishing. Repeat the first part for $$\omega = a(z)\,dx + b(x)\,dy + c(y)\,dz.$$ I understand that for the first part the necessary and sufficient condition for integrability is $\omega \wedge d\omega = 0$, and that the right thing to do for the second part is to forget about $U$ and assume that the functions are smooth on all of $\mathbb{R}^3$.",,['multivariable-calculus']
68,Why does $\frac{\partial F}{\partial x_i}(\pmb{a}) = \lim_{h\rightarrow 0} \frac{F(\pmb{a}+h\pmb{e}_i)-F(\pmb{a})}{h} $ gives a vector?,Why does  gives a vector?,\frac{\partial F}{\partial x_i}(\pmb{a}) = \lim_{h\rightarrow 0} \frac{F(\pmb{a}+h\pmb{e}_i)-F(\pmb{a})}{h} ,"I am reading about partial derivatives in some notes on Multivariable Calculus . On page 2, it says: Definition: Partial Derivatives Let $U \subset \mathbb R^m$ be open, let $F: U \to \mathbb R^m$ , and let $\mathbf a \in U$ . If $i \in \{1, \ldots , m\}$ , the partial derivative of $F$ with respect to $x_i$ at $\mathbf a$ is defined as follows: $$\frac{\partial F}{\partial x_i}(\mathbf a) = \lim_{h \to 0} \frac{F(\mathbf a + h\mathbf e_i) - F(\mathbf a)}{h}.$$ Here $\mathbf e_i$ denotes a unit vector in the xi direction. Then, later it says: As we have defined it, the partial derivative $(\partial F)/(\partial x_i)(\mathbf a)$ is a vector. Specifically, if $F(\mathbf x) = (f_1(\mathbf x), \ldots, f_n(\mathbf x))$ , then $$\frac{\partial F}{\partial x_i}(\mathbf a) = \left(\frac{\partial f_1}{\partial x_i}(\mathbf a), \ldots, \frac{\partial f_n}{\partial x_i}(\mathbf a)\right)$$ I don't see how the limit expression can be a vector. Based on my observation, the numerator will be a $n\times 1$ and the denominator will be a $m \times 1$ vector. Therefore we have division by vectors which is not defined. Can I have a step by step explanation of how the limit expression results the vector described in point 2? I've taken a course on multi variable calculus but have not encountered taking the limit of vectors.","I am reading about partial derivatives in some notes on Multivariable Calculus . On page 2, it says: Definition: Partial Derivatives Let be open, let , and let . If , the partial derivative of with respect to at is defined as follows: Here denotes a unit vector in the xi direction. Then, later it says: As we have defined it, the partial derivative is a vector. Specifically, if , then I don't see how the limit expression can be a vector. Based on my observation, the numerator will be a and the denominator will be a vector. Therefore we have division by vectors which is not defined. Can I have a step by step explanation of how the limit expression results the vector described in point 2? I've taken a course on multi variable calculus but have not encountered taking the limit of vectors.","U \subset \mathbb R^m F: U \to \mathbb R^m \mathbf a \in U i \in \{1, \ldots , m\} F x_i \mathbf a \frac{\partial F}{\partial x_i}(\mathbf a) = \lim_{h \to 0} \frac{F(\mathbf a + h\mathbf e_i) - F(\mathbf a)}{h}. \mathbf e_i (\partial F)/(\partial x_i)(\mathbf a) F(\mathbf x) = (f_1(\mathbf x), \ldots, f_n(\mathbf x)) \frac{\partial F}{\partial x_i}(\mathbf a) = \left(\frac{\partial f_1}{\partial x_i}(\mathbf a), \ldots, \frac{\partial f_n}{\partial x_i}(\mathbf a)\right) n\times 1 m \times 1","['multivariable-calculus', 'partial-derivative']"
69,A bounded function on $\mathbb{R}^2$,A bounded function on,\mathbb{R}^2,"How to prove that the function $f(x,y)=\displaystyle\frac{xy^2}{x^2+y^4}$ if $(0,0)\not = (0,0)$ and $f(x,y)=0$ is bounded on $\mathbb{R}^2$? I like some advice to this problem. Thanks!","How to prove that the function $f(x,y)=\displaystyle\frac{xy^2}{x^2+y^4}$ if $(0,0)\not = (0,0)$ and $f(x,y)=0$ is bounded on $\mathbb{R}^2$? I like some advice to this problem. Thanks!",,['multivariable-calculus']
70,What do we mean by Derivative of linear function is a constant function.,What do we mean by Derivative of linear function is a constant function.,,"I've the text below given in my notes: Derivative of linear function: Let $R:X\to Y$ be a linear function .Then $R':X\to L(X,Y)$ is a constant function with the constant value $R\in L(X,Y)$ i.e. $R'(a)=R$ for all a $\in X$.That is , $$R'(a)=R$$ for all a,x $\in X$. Can anyone explain the above definition with help of an example clearly stating what it means?","I've the text below given in my notes: Derivative of linear function: Let $R:X\to Y$ be a linear function .Then $R':X\to L(X,Y)$ is a constant function with the constant value $R\in L(X,Y)$ i.e. $R'(a)=R$ for all a $\in X$.That is , $$R'(a)=R$$ for all a,x $\in X$. Can anyone explain the above definition with help of an example clearly stating what it means?",,"['multivariable-calculus', 'linear-transformations']"
71,Finding the bounds on a triple integral,Finding the bounds on a triple integral,,"Problem: Find the volume enclosed by the cone $$x^2 + y^2 = z^2$$ and the plane $$2z - y -2 = 0$$ So I know that I need to do a triple integral over this region, and the integrand will be 1. My problem is with finding the bounds for the integral. I set the $z$s equal to each other and found the intersection is $x^2 + \frac{3}{4}(y-\frac{4}{9})^2 = \frac{13}{9}$ (although I may have made a mistake here.) I believe that the bound for $z$ is from $\sqrt{x^2 + y^2}$ to $\frac{y+2}{2}$? Now I am not sure what to do. How can I find the bounds for $x$ and $y$? Also, I think I should do a change of variable so that the plane lies flat across the cone instead of slanted, to make the upper bound for $z$ constant. Is this a good idea? How could I do it? I am really lost with this problem.","Problem: Find the volume enclosed by the cone $$x^2 + y^2 = z^2$$ and the plane $$2z - y -2 = 0$$ So I know that I need to do a triple integral over this region, and the integrand will be 1. My problem is with finding the bounds for the integral. I set the $z$s equal to each other and found the intersection is $x^2 + \frac{3}{4}(y-\frac{4}{9})^2 = \frac{13}{9}$ (although I may have made a mistake here.) I believe that the bound for $z$ is from $\sqrt{x^2 + y^2}$ to $\frac{y+2}{2}$? Now I am not sure what to do. How can I find the bounds for $x$ and $y$? Also, I think I should do a change of variable so that the plane lies flat across the cone instead of slanted, to make the upper bound for $z$ constant. Is this a good idea? How could I do it? I am really lost with this problem.",,"['calculus', 'integration', 'geometry', 'multivariable-calculus', 'volume']"
72,"Compute a multiple integral$\iint_{[0,1]^2} (xy)^{xy} dxdy$",Compute a multiple integral,"\iint_{[0,1]^2} (xy)^{xy} dxdy","$$\text{Compute} :\iint_{[0,1]^2} (xy)^{xy} dxdy$$ I am thinking about changing the variable, $x=u,y={v \over u}$.But it doesn't work. I just found that the answer is$\int_0^1 t^t dt$.Maybe my idea is right?","$$\text{Compute} :\iint_{[0,1]^2} (xy)^{xy} dxdy$$ I am thinking about changing the variable, $x=u,y={v \over u}$.But it doesn't work. I just found that the answer is$\int_0^1 t^t dt$.Maybe my idea is right?",,"['integration', 'multivariable-calculus']"
73,Are the conditions for multivariable integrability the same?,Are the conditions for multivariable integrability the same?,,"For a single variable function, the function needs to have a finite number of discontinuities and must be bounded over the interval of integration for it to be Riemann integrable over that interval. Are the conditions the same for the multivariable case? In other words, is it true that if a multivariable function is bounded over a region and has a finite number of discontinuities it is necessarily Riemann integrable?","For a single variable function, the function needs to have a finite number of discontinuities and must be bounded over the interval of integration for it to be Riemann integrable over that interval. Are the conditions the same for the multivariable case? In other words, is it true that if a multivariable function is bounded over a region and has a finite number of discontinuities it is necessarily Riemann integrable?",,"['real-analysis', 'multivariable-calculus']"
74,Integration by parts on all of $\mathbb{R}^n$ with $n>1$,Integration by parts on all of  with,\mathbb{R}^n n>1,"So this came up as I was thinking about the uniqueness of solutions to the wave equation. I have seen proofs for uniqueness on all of $\mathbb{R}$ or on bounded subsets of $\mathbb{R}^n$, but never $\textit{explicitly}$ on all of $\mathbb{R}^n$. I tried to use the energy approach method to see if I could show uniqueness on all of $\mathbb{R}^n$, however, I ran into trouble when I started doing the integration by parts. i.e., I realized that I don't know the validity of integration by parts on an unbounded domain in dimensions greater than 1. So is this valid? It seems like a proof would look something like below $\lim_{R \rightarrow \infty} \int_{B(0,R)} u\cdot \nabla v d\bar{x} = \lim_{R \rightarrow \infty} [\int_{\partial B(0,R)} vu\cdot \hat{n} dS\bar{x}- \int_{B(0,R)} v \nabla \cdot u  d\bar{x}]$ However, is $\lim_{R \rightarrow \infty} \int_{\partial B(0,R)} vu\cdot \hat{n} dS\bar{x}$ a well defined expression? Part of my worry is I looked up multidimensional integration by parts on wikipedia but it explicitly stated the expression for bounded domains and didn't mention unbounded. From experience past experience there isn't necessarily problems with taking integrals over infinite domains, but I haven't encountered many infinite integrals over a surface. Can anybody offer some illumination?","So this came up as I was thinking about the uniqueness of solutions to the wave equation. I have seen proofs for uniqueness on all of $\mathbb{R}$ or on bounded subsets of $\mathbb{R}^n$, but never $\textit{explicitly}$ on all of $\mathbb{R}^n$. I tried to use the energy approach method to see if I could show uniqueness on all of $\mathbb{R}^n$, however, I ran into trouble when I started doing the integration by parts. i.e., I realized that I don't know the validity of integration by parts on an unbounded domain in dimensions greater than 1. So is this valid? It seems like a proof would look something like below $\lim_{R \rightarrow \infty} \int_{B(0,R)} u\cdot \nabla v d\bar{x} = \lim_{R \rightarrow \infty} [\int_{\partial B(0,R)} vu\cdot \hat{n} dS\bar{x}- \int_{B(0,R)} v \nabla \cdot u  d\bar{x}]$ However, is $\lim_{R \rightarrow \infty} \int_{\partial B(0,R)} vu\cdot \hat{n} dS\bar{x}$ a well defined expression? Part of my worry is I looked up multidimensional integration by parts on wikipedia but it explicitly stated the expression for bounded domains and didn't mention unbounded. From experience past experience there isn't necessarily problems with taking integrals over infinite domains, but I haven't encountered many infinite integrals over a surface. Can anybody offer some illumination?",,"['multivariable-calculus', 'partial-differential-equations', 'improper-integrals', 'vector-analysis', 'wave-equation']"
75,Notation on partial deriviatives,Notation on partial deriviatives,,"If I need to find $$\frac{\partial ^{2}g}{\partial u \partial v}$$ Then do I want to perform $$ \frac{\partial} { \partial v}\ \big( \frac{\partial g}{\partial u} \big) $$  or  $$ \frac{\partial g} {\partial u} * \frac{\partial g}{\partial v} $$ I'm thinking the first one, but I just want to check my intuition. edit: Notation Blunder *facepalm *","If I need to find $$\frac{\partial ^{2}g}{\partial u \partial v}$$ Then do I want to perform $$ \frac{\partial} { \partial v}\ \big( \frac{\partial g}{\partial u} \big) $$  or  $$ \frac{\partial g} {\partial u} * \frac{\partial g}{\partial v} $$ I'm thinking the first one, but I just want to check my intuition. edit: Notation Blunder *facepalm *",,"['multivariable-calculus', 'notation']"
76,Doubts on Differentiation in $\mathbb{R}^p$,Doubts on Differentiation in,\mathbb{R}^p,"I am currently reading R.G Bartle's ""Elements of Real Analysis"" for a one semester course in Advanced Real Analysis. In the chapter on Differentiation in $\mathbb{R}^p$, I am confused regarding the terminologies and the notations used. I list the basic doubts : $1$. What do you mean when you say derivative of $f:A\subseteq\mathbb{R}^p\to\mathbb{R}^q$ is a linear map L from $\mathbb{R}^p\to \mathbb{R}^q $ ? $2.$ What does the notation $Df(c)(u)$ mean ? $3$. What do you mean when you say partial derivative of f with respect to an arbitratry vector $u\in \mathbb{R}^p $ ? $4.$ Given $f(x) = \begin{cases} \dfrac{xy^2}{x^2+y^2}, &\text{if }(x,y)\ne(0,0) \\ 0, &\text{if }(x,y)=(0,0). \end{cases}$ How is $D_{(a,b)}f(0,0)=\dfrac{ab^2}{a^2+b^2},(a,b)\ne(0,0 )?$ I need help in understanding these.  Thanks","I am currently reading R.G Bartle's ""Elements of Real Analysis"" for a one semester course in Advanced Real Analysis. In the chapter on Differentiation in $\mathbb{R}^p$, I am confused regarding the terminologies and the notations used. I list the basic doubts : $1$. What do you mean when you say derivative of $f:A\subseteq\mathbb{R}^p\to\mathbb{R}^q$ is a linear map L from $\mathbb{R}^p\to \mathbb{R}^q $ ? $2.$ What does the notation $Df(c)(u)$ mean ? $3$. What do you mean when you say partial derivative of f with respect to an arbitratry vector $u\in \mathbb{R}^p $ ? $4.$ Given $f(x) = \begin{cases} \dfrac{xy^2}{x^2+y^2}, &\text{if }(x,y)\ne(0,0) \\ 0, &\text{if }(x,y)=(0,0). \end{cases}$ How is $D_{(a,b)}f(0,0)=\dfrac{ab^2}{a^2+b^2},(a,b)\ne(0,0 )?$ I need help in understanding these.  Thanks",,"['real-analysis', 'multivariable-calculus']"
77,Tractrix exercise,Tractrix exercise,,"Exercise: Let  $$\begin{align*}\gamma:(0,\pi) &\to \mathbb R^2\\ t &\mapsto \gamma(t)=(\sin t,\ \cos t+\log \left(\tan(\frac{t}{2}) \right), \end{align*}$$ be the parametrized curve of the tractrix. Let $P$ be a point on the tractrix, $L$ the tangent line that passes through $P$, and $Q$ the intersection of $L$ with the $y$ axis. Prove that the distance between $P$ and $Q$ is $1$. My attempt and questions: If $P=\gamma(t_0)$, with $t_0 \in (0,\pi)$, then the tangent line $L$ is $$L: \gamma(t_0)+\lambda\gamma'(t_0).$$ The intersection of the line and the $y$ axists is the point $Q=(0,\gamma_2(t_0)-\dfrac{\gamma_2'(t_0)}{\gamma_1'(t_0)}\gamma_1(t_0)$. I've tried to find the point of intersection between the tangent line and the $y$ axis and then calculate $\|P-Q \|^2$ but I got stuck. $$\|P-Q\|^2=\|(\gamma_1(t_0),-\frac{\gamma_2'(t_0)}{\gamma_1'(t_0)}\gamma_1(t_0))\|^2$$ I've calculated  $$\gamma'(t_0)=\left(\cos(t),-\sin(t)+\frac{\sec^2(\frac{t_0}{2})}{2\tan(\frac{t_0}{2})} \right),$$  to know $\gamma_1'(t_0),\gamma_2'(t_0)$ but I couldn't arrive to anything. Also, what if $\gamma_1'(t_0)=0$?, This can perfectly be the case, for example take $t=\frac{\pi}{2}$. How can I analyze that case separately from the rest? Any suggestions, ideas, answers would be appreciated.","Exercise: Let  $$\begin{align*}\gamma:(0,\pi) &\to \mathbb R^2\\ t &\mapsto \gamma(t)=(\sin t,\ \cos t+\log \left(\tan(\frac{t}{2}) \right), \end{align*}$$ be the parametrized curve of the tractrix. Let $P$ be a point on the tractrix, $L$ the tangent line that passes through $P$, and $Q$ the intersection of $L$ with the $y$ axis. Prove that the distance between $P$ and $Q$ is $1$. My attempt and questions: If $P=\gamma(t_0)$, with $t_0 \in (0,\pi)$, then the tangent line $L$ is $$L: \gamma(t_0)+\lambda\gamma'(t_0).$$ The intersection of the line and the $y$ axists is the point $Q=(0,\gamma_2(t_0)-\dfrac{\gamma_2'(t_0)}{\gamma_1'(t_0)}\gamma_1(t_0)$. I've tried to find the point of intersection between the tangent line and the $y$ axis and then calculate $\|P-Q \|^2$ but I got stuck. $$\|P-Q\|^2=\|(\gamma_1(t_0),-\frac{\gamma_2'(t_0)}{\gamma_1'(t_0)}\gamma_1(t_0))\|^2$$ I've calculated  $$\gamma'(t_0)=\left(\cos(t),-\sin(t)+\frac{\sec^2(\frac{t_0}{2})}{2\tan(\frac{t_0}{2})} \right),$$  to know $\gamma_1'(t_0),\gamma_2'(t_0)$ but I couldn't arrive to anything. Also, what if $\gamma_1'(t_0)=0$?, This can perfectly be the case, for example take $t=\frac{\pi}{2}$. How can I analyze that case separately from the rest? Any suggestions, ideas, answers would be appreciated.",,"['multivariable-calculus', 'plane-curves']"
78,What is the meaning of $d\vec S$ in a surface integral?,What is the meaning of  in a surface integral?,d\vec S,"Can someone explain if I have a surface $z= 9-x^2-y^2$ What would $\vec{n}$ be? What would $d\vec{S}$ be? Why is $d\vec{S}$ $(2x,2y,1)$ and not $(2x,2y,1)/\sqrt{4x^2+4y^2+1}$? Thanks!","Can someone explain if I have a surface $z= 9-x^2-y^2$ What would $\vec{n}$ be? What would $d\vec{S}$ be? Why is $d\vec{S}$ $(2x,2y,1)$ and not $(2x,2y,1)/\sqrt{4x^2+4y^2+1}$? Thanks!",,"['integration', 'multivariable-calculus', 'surfaces']"
79,Using Stokes theorem to integrate $\vec{F}=5y \vec{\imath} −5x \vec{\jmath} +4(y−x) \vec{k}$ over a circle,Using Stokes theorem to integrate  over a circle,\vec{F}=5y \vec{\imath} −5x \vec{\jmath} +4(y−x) \vec{k},"Find $\oint_C \vec{F} \cdot d \vec{r}$ where $C$ is a circle of radius $2$ in the plane $x+y+z=3$ , centered at $(2,4,−3)$ and oriented clockwise when viewed from the origin, if $\vec{F}=5y \vec{\imath} −5x \vec{\jmath} +4(y−x) \vec{k}$ Relevant equations: Stokes theorem: $$\int_S \operatorname{curl}{F} \cdot \mathbf{n} \, dS = \oint_{\partial S} F \cdot d\mathbf{r}$$ My attempt: For the curl I get $(4,4,-10)$ . For $d\vec{S}$ I get $(1,1,1)$ from $z = 3-x-y$ Dotted together its $-2$ . So: $-2 \iint_S dA$ . Area of circle is $4\pi$ . My answer would be $-8\pi$ but the online homework system says it's not correct. Please help!","Find where is a circle of radius in the plane , centered at and oriented clockwise when viewed from the origin, if Relevant equations: Stokes theorem: My attempt: For the curl I get . For I get from Dotted together its . So: . Area of circle is . My answer would be but the online homework system says it's not correct. Please help!","\oint_C \vec{F} \cdot d \vec{r} C 2 x+y+z=3 (2,4,−3) \vec{F}=5y \vec{\imath} −5x \vec{\jmath} +4(y−x) \vec{k} \int_S \operatorname{curl}{F} \cdot \mathbf{n} \, dS = \oint_{\partial S} F \cdot d\mathbf{r} (4,4,-10) d\vec{S} (1,1,1) z = 3-x-y -2 -2 \iint_S dA 4\pi -8\pi","['calculus', 'integration', 'multivariable-calculus', 'definite-integrals', 'vector-analysis']"
80,Why does this vector derivation hold?,Why does this vector derivation hold?,,"I have the following variables/matrices: $$A \in \mathbb{R}^{m \times n} , \quad p \in \mathbb{R}^{n}, \quad \Sigma \in \mathbb{R}^{m \times m}, \quad w \in \mathbb{R}^{m}$$ where $\Sigma$ is a diagonal matrix. With these we define function $S(p)$ as $$S(p) = (w + Ap)^{T} \Sigma^{-1} (w + Ap)$$ Since we would like to find the minimum of $S(p)$ we compute the first derivation with respect to $p$, according to my master's solution this is $$\nabla S(p) = 2(Ap + w)^{T} \Sigma^{-1} A \overset{!}{=} 0$$ However I don't understand how they arrive at this solution, could somebody please explain the intermediary steps?","I have the following variables/matrices: $$A \in \mathbb{R}^{m \times n} , \quad p \in \mathbb{R}^{n}, \quad \Sigma \in \mathbb{R}^{m \times m}, \quad w \in \mathbb{R}^{m}$$ where $\Sigma$ is a diagonal matrix. With these we define function $S(p)$ as $$S(p) = (w + Ap)^{T} \Sigma^{-1} (w + Ap)$$ Since we would like to find the minimum of $S(p)$ we compute the first derivation with respect to $p$, according to my master's solution this is $$\nabla S(p) = 2(Ap + w)^{T} \Sigma^{-1} A \overset{!}{=} 0$$ However I don't understand how they arrive at this solution, could somebody please explain the intermediary steps?",,"['linear-algebra', 'multivariable-calculus', 'optimization', 'partial-derivative', 'matrix-calculus']"
81,Trouble computing this double integral,Trouble computing this double integral,,"$$\iint_R xe^{xy}~\mathrm{d}A \qquad 0\le x\le 2 \quad 0 \le y \le 1$$ Today I started learning about double integrals on a class I am taking, had good understanding on single-variable integrals but I simply have no idea on what to do here. I am only able to do some simple excercises were I can obviously separate the terms without ""mixing"" the variables, then move them out of one integral, for example: $$\iint_R (x^2y) ~\mathrm{d}y~\mathrm{d}x \qquad 0\le y\le 1 \quad 1 \le x \le2$$ $$=\int_1^2 \left[ x^2\int_0^1 y~\mathrm{d}y \right]~\mathrm{d}x$$ And then it gets easy to do. But on that first one I don't know how to separate them. Could anyone give me a hint on what the next step is?","$$\iint_R xe^{xy}~\mathrm{d}A \qquad 0\le x\le 2 \quad 0 \le y \le 1$$ Today I started learning about double integrals on a class I am taking, had good understanding on single-variable integrals but I simply have no idea on what to do here. I am only able to do some simple excercises were I can obviously separate the terms without ""mixing"" the variables, then move them out of one integral, for example: $$\iint_R (x^2y) ~\mathrm{d}y~\mathrm{d}x \qquad 0\le y\le 1 \quad 1 \le x \le2$$ $$=\int_1^2 \left[ x^2\int_0^1 y~\mathrm{d}y \right]~\mathrm{d}x$$ And then it gets easy to do. But on that first one I don't know how to separate them. Could anyone give me a hint on what the next step is?",,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals']"
82,Typed version of Newton's Principia Mathematica,Typed version of Newton's Principia Mathematica,,I need a typed pdf version of Newton's Principia. Is it available for free online? And I also need the proof of universal law of gravity and the elliptical orbits of planets(If there's no typed version).,I need a typed pdf version of Newton's Principia. Is it available for free online? And I also need the proof of universal law of gravity and the elliptical orbits of planets(If there's no typed version).,,"['multivariable-calculus', 'reference-request']"
83,When is $x\mapsto |x|^{s-1}x$ a diffeomorphism?,When is  a diffeomorphism?,x\mapsto |x|^{s-1}x,"Consider the function $f:B^n\rightarrow B^n$ from the disk to itself $$f(x)=\vert x\vert^{s-1}x$$ where $s>0$ and we are considering the euclidean norm (we define the function to be $0$ in the origin if $s<1$). This function defines a homemorphism from the disk to itself. My question is why if $f$ is a diffeomorphism, then $s=1$? This is part of an exercise in Lee's Introduction to Smooth Manifolds chapter 1.","Consider the function $f:B^n\rightarrow B^n$ from the disk to itself $$f(x)=\vert x\vert^{s-1}x$$ where $s>0$ and we are considering the euclidean norm (we define the function to be $0$ in the origin if $s<1$). This function defines a homemorphism from the disk to itself. My question is why if $f$ is a diffeomorphism, then $s=1$? This is part of an exercise in Lee's Introduction to Smooth Manifolds chapter 1.",,"['multivariable-calculus', 'differential-geometry', 'smooth-manifolds']"
84,What is a Hyper-Sphere?,What is a Hyper-Sphere?,,"I am interesting about the geometric properties of 3-D spheres and I know nothing about hyper-spheres. Please can you describe me, what is a hyper-sphere?","I am interesting about the geometric properties of 3-D spheres and I know nothing about hyper-spheres. Please can you describe me, what is a hyper-sphere?",,['multivariable-calculus']
85,Area of $\frac{x}{a} + \frac{y}{b} + \frac{z}{c} = 1$,Area of,\frac{x}{a} + \frac{y}{b} + \frac{z}{c} = 1,"Could you tell me how to calculate the area of part of the plane:  $\frac{x}{a} + \frac{y}{b} + \frac{z}{c} = 1$, $a, b, c >0$ where all coordinates of a point are positive?","Could you tell me how to calculate the area of part of the plane:  $\frac{x}{a} + \frac{y}{b} + \frac{z}{c} = 1$, $a, b, c >0$ where all coordinates of a point are positive?",,"['real-analysis', 'integration', 'multivariable-calculus', 'area']"
86,simple question about $\nabla r$,simple question about,\nabla r,"In my physics notes, it says $\nabla r = \underline{e_r} = \frac{\underline{r}}{r}$ and $\nabla \frac{1}{r} = - \frac{\underline{r}}{r^3} = - \frac{1}{r^2} \underline{e_r}$ I don't quite understand why this is? I know that $\nabla \varphi = (\frac{\delta\varphi}{\delta x},\frac{\delta\varphi}{\delta y},\frac{\delta\varphi}{\delta z})$ but I do not understand the above. Thanks.","In my physics notes, it says $\nabla r = \underline{e_r} = \frac{\underline{r}}{r}$ and $\nabla \frac{1}{r} = - \frac{\underline{r}}{r^3} = - \frac{1}{r^2} \underline{e_r}$ I don't quite understand why this is? I know that $\nabla \varphi = (\frac{\delta\varphi}{\delta x},\frac{\delta\varphi}{\delta y},\frac{\delta\varphi}{\delta z})$ but I do not understand the above. Thanks.",,"['calculus', 'multivariable-calculus', 'physics']"
87,"How to tell if $T(x,y,z) = (y \sin x, z \cos y, xy)$ is one-to-one and/or onto?",How to tell if  is one-to-one and/or onto?,"T(x,y,z) = (y \sin x, z \cos y, xy)","$T(x,y,z) = (y \sin x, z \cos y, xy)$ from $\mathbb{R^3} \rightarrow \mathbb{R^3}$ To show 1-to-1, we want to show: $$y \sin x = y' \sin x' \\ z \cos y = z' \cos y' \\ xy = x'y'$$ I'm not sure what to do here algebraically, since we can't divide anything since we don't know if $x, y, z, x', y', z'$ might be a $0$ value. For onto, we want to solve for $x, y,$ and $z$: $$y \sin x = a \\ z \cos y = b \\ xy = c $$ Again, we don't know if any of these values may be a $0$, so we can't be too careful in dividing by $0$. What should I do from here?","$T(x,y,z) = (y \sin x, z \cos y, xy)$ from $\mathbb{R^3} \rightarrow \mathbb{R^3}$ To show 1-to-1, we want to show: $$y \sin x = y' \sin x' \\ z \cos y = z' \cos y' \\ xy = x'y'$$ I'm not sure what to do here algebraically, since we can't divide anything since we don't know if $x, y, z, x', y', z'$ might be a $0$ value. For onto, we want to solve for $x, y,$ and $z$: $$y \sin x = a \\ z \cos y = b \\ xy = c $$ Again, we don't know if any of these values may be a $0$, so we can't be too careful in dividing by $0$. What should I do from here?",,['multivariable-calculus']
88,Integral/Vector calculus $\oint_{\partial S} u \vec \nabla v \cdot d \vec \lambda=\int_S (\vec \nabla u)\times (\vec \nabla v)\cdot d\vec S.$,Integral/Vector calculus,\oint_{\partial S} u \vec \nabla v \cdot d \vec \lambda=\int_S (\vec \nabla u)\times (\vec \nabla v)\cdot d\vec S.,I am trying to show that $$ \oint_{\partial S} u \vec \nabla v \cdot d \vec \lambda=\int_S (\vec \nabla u)\times (\vec \nabla v)\cdot d\vec S $$ using Levi Cevita notation methods only.  The Levi Cevita tensor is given by $\epsilon_{ijk}$ and is a totally anti symmetric tensor. The functions u and v are dependent on the radius vector in 3 dimensions ($\vec r$).  I am stuck on this I know  the general idea of using this notation is to write things like $$ (\vec \nabla \times \vec A)_i= \epsilon_{ijk} \partial_j A_k $$ but I still am stuck.  Thank you,I am trying to show that $$ \oint_{\partial S} u \vec \nabla v \cdot d \vec \lambda=\int_S (\vec \nabla u)\times (\vec \nabla v)\cdot d\vec S $$ using Levi Cevita notation methods only.  The Levi Cevita tensor is given by $\epsilon_{ijk}$ and is a totally anti symmetric tensor. The functions u and v are dependent on the radius vector in 3 dimensions ($\vec r$).  I am stuck on this I know  the general idea of using this notation is to write things like $$ (\vec \nabla \times \vec A)_i= \epsilon_{ijk} \partial_j A_k $$ but I still am stuck.  Thank you,,"['calculus', 'integration', 'multivariable-calculus', 'vector-analysis', 'tensors']"
89,Which Cross Product for the Desired Orientation of a Hyperboloid ? [Stewart P1103 16.9.8],Which Cross Product for the Desired Orientation of a Hyperboloid ? [Stewart P1103 16.9.8],,"P1103 16.9.$8.$ Evaluate the surface integral $\iint_S \mathbf{F} \cdot d\mathbf{S}$.   $\mathbf{F} = (x^3y,-x^2y^2,-x^2yz)$ and $S$ is the surface of the solid bounded by the hyperboloid $x^2 + y^2 -z^2 = 1$, and the planes $z = -2$ and $z = 2$. Parametrisation for the open middle piece of the paraboloid: $\mathbf{r}(u,v) = (1\cosh u \cos v, 1 \sinh u \sin v, 1 \sinh u) \, \forall \, -1 \le u \le 1, 0 \le v \le 2\pi$. How does one determine: $\partial_{\huge{u}}\mathbf{r} \times \partial_{\huge{v}}\mathbf{r}$ effects the right orientation? If a significant geometric or visual argument is necessary, would you please provide a picture? Addendum: The question as printed only asks for a computation with Divergence Theorem, but I don't want to use Divergence Thm here; I only want to solve this with piecewise surface integrals.","P1103 16.9.$8.$ Evaluate the surface integral $\iint_S \mathbf{F} \cdot d\mathbf{S}$.   $\mathbf{F} = (x^3y,-x^2y^2,-x^2yz)$ and $S$ is the surface of the solid bounded by the hyperboloid $x^2 + y^2 -z^2 = 1$, and the planes $z = -2$ and $z = 2$. Parametrisation for the open middle piece of the paraboloid: $\mathbf{r}(u,v) = (1\cosh u \cos v, 1 \sinh u \sin v, 1 \sinh u) \, \forall \, -1 \le u \le 1, 0 \le v \le 2\pi$. How does one determine: $\partial_{\huge{u}}\mathbf{r} \times \partial_{\huge{v}}\mathbf{r}$ effects the right orientation? If a significant geometric or visual argument is necessary, would you please provide a picture? Addendum: The question as printed only asks for a computation with Divergence Theorem, but I don't want to use Divergence Thm here; I only want to solve this with piecewise surface integrals.",,['multivariable-calculus']
90,Learning Advanced Mathematics,Learning Advanced Mathematics,,"I'm a 12th grade student and I've recently developed a passion for mathematics . Currently my knowledge in this particular area is comprised by : single-variable calculus , trigonometry , geometry , basic notions of linear algebra and set theory . I'm particularly interested in calculus and I need some advice as I intend on coupling my future math skills with the study of Quantum Physics . Where should I go from here in order to understand multivariable calculus ?Can anyone recommend an interesting advanced calculus textbook  ? (Hope the prerequisites needed for understanding the textbooks you suggest match my current mathematical skills and knowledge)","I'm a 12th grade student and I've recently developed a passion for mathematics . Currently my knowledge in this particular area is comprised by : single-variable calculus , trigonometry , geometry , basic notions of linear algebra and set theory . I'm particularly interested in calculus and I need some advice as I intend on coupling my future math skills with the study of Quantum Physics . Where should I go from here in order to understand multivariable calculus ?Can anyone recommend an interesting advanced calculus textbook  ? (Hope the prerequisites needed for understanding the textbooks you suggest match my current mathematical skills and knowledge)",,"['multivariable-calculus', 'reference-request', 'mathematical-physics', 'advice']"
91,Electric charge is distributed over the disk $x^2 + y^2 \leq 5$...Find the total charge on the disk.,Electric charge is distributed over the disk ...Find the total charge on the disk.,x^2 + y^2 \leq 5,"Electric charge is distributed over the disk    $x^2 + y^2 \leq 5$ so that the charge density at $(x,y$) is $\sigma(x,y) = 2 + x^2 + y^2$ coulombs per square meter. Find the total charge on the disk. $$\int_0^{2\pi}\int_0^5(2+r^2)\space r\space dr\space d\theta=\frac{725\pi}2$$ is not the right answer.  I don't know why. Any help?","Electric charge is distributed over the disk    $x^2 + y^2 \leq 5$ so that the charge density at $(x,y$) is $\sigma(x,y) = 2 + x^2 + y^2$ coulombs per square meter. Find the total charge on the disk. $$\int_0^{2\pi}\int_0^5(2+r^2)\space r\space dr\space d\theta=\frac{725\pi}2$$ is not the right answer.  I don't know why. Any help?",,['multivariable-calculus']
92,Finding maximal and minimal temperature on barbed wire,Finding maximal and minimal temperature on barbed wire,,"A barbed wire has circle form $x^2+y^2-2y=0$. The temperature on a point on the wire is given by the function $t(x,y)=2x^2+3y$. Find minimal and maximal temperature points on the wire. How does one solve this? Thanks!","A barbed wire has circle form $x^2+y^2-2y=0$. The temperature on a point on the wire is given by the function $t(x,y)=2x^2+3y$. Find minimal and maximal temperature points on the wire. How does one solve this? Thanks!",,['multivariable-calculus']
93,Use Lagrange Multipliers to find the absolute extrema,Use Lagrange Multipliers to find the absolute extrema,,"Use Lagrange Multipliers to find the absolute extrema (if any) of: $f(x,y) = 4x^2 + 9y^2$; subject to $2x +3y = 6$. Using Lagrange I end up with one point: $(\frac{3}{2}, 1)$ I'm just not sure how to show if that point is a max or a min?","Use Lagrange Multipliers to find the absolute extrema (if any) of: $f(x,y) = 4x^2 + 9y^2$; subject to $2x +3y = 6$. Using Lagrange I end up with one point: $(\frac{3}{2}, 1)$ I'm just not sure how to show if that point is a max or a min?",,"['calculus', 'multivariable-calculus', 'optimization', 'lagrange-multiplier']"
94,Vector valued function in $\mathbb{R}^2$ with non-finite arc length,Vector valued function in  with non-finite arc length,\mathbb{R}^2,"Find a curve $\mathcal{C}$ : x = g (t) , $a \leq t \leq b$ in $\mathbb{R}^2$, where g $\in \mathcal{C}[a,b]$ such that $\mathcal{C}$ does NOT have finite arc length","Find a curve $\mathcal{C}$ : x = g (t) , $a \leq t \leq b$ in $\mathbb{R}^2$, where g $\in \mathcal{C}[a,b]$ such that $\mathcal{C}$ does NOT have finite arc length",,"['multivariable-calculus', 'vector-analysis']"
95,"Prove $\lim_{(x,y) \to (0,0)} \frac{x^3y}{x^4+y^2} = 0$ [duplicate]",Prove  [duplicate],"\lim_{(x,y) \to (0,0)} \frac{x^3y}{x^4+y^2} = 0","This question already has answers here : How to prove that $\lim_{(x,y) \to (0,0)} \frac{x^3y}{x^4+y^2} = 0?$ [duplicate] (5 answers) Closed 3 years ago . How would you prove the following limit? $$\lim_{(x,y) \to (0,0)} \frac{x^3y}{x^4+y^2} = 0$$ I think the best way is using the squeeze theorem but I can't find left expression. $$0 \le \frac{x^3y}{x^4 + y^2} \le \frac{x^3y}{x^4} \le \frac{x^3y}{x^3} \le y = 0.$$ But I'm not sure I'm right (especially at $\frac{x^3y}{x^4} \le \frac{x^3y}{x^3}).$ If I'm right - I'd glad if you can accept it. If I'm wrong - can you please correct me? Thanks in advance!","This question already has answers here : How to prove that $\lim_{(x,y) \to (0,0)} \frac{x^3y}{x^4+y^2} = 0?$ [duplicate] (5 answers) Closed 3 years ago . How would you prove the following limit? $$\lim_{(x,y) \to (0,0)} \frac{x^3y}{x^4+y^2} = 0$$ I think the best way is using the squeeze theorem but I can't find left expression. $$0 \le \frac{x^3y}{x^4 + y^2} \le \frac{x^3y}{x^4} \le \frac{x^3y}{x^3} \le y = 0.$$ But I'm not sure I'm right (especially at $\frac{x^3y}{x^4} \le \frac{x^3y}{x^3}).$ If I'm right - I'd glad if you can accept it. If I'm wrong - can you please correct me? Thanks in advance!",,"['limits', 'multivariable-calculus', 'arithmetic']"
96,Directional derivative in a Sobolev-like inequality,Directional derivative in a Sobolev-like inequality,,"I am trying to do the following problem: Let $\Omega \subset \subset \overline{\mathbb{R}_{+}^{n+1}} = \{ (x, x_{n+1}); \, x_{n+1}\geq 0\}$ (i.e., $\Omega$ is bounded inside the closed upper half-space). (By definition, smooth functions on $\overline{\mathbb{R}_{+}^{n+1}}$ are functions which are restrictions of smooth functions on an open neighborhood of $\overline{\mathbb{R}_{+}^{n+1}}$ in $\mathbb{R}^{n+1}$.) Let $u \in C_{0}^{\infty}(\Omega)$. Show that for any $p$ such that $1\leq p \leq \infty$,  there is a constant $C_{\Omega}$ independent of $u$ so that,  $$\|u(\cdot,0)\|_{L^{p}(\mathbb{R}^{n})}\leq C_{\Omega}\|\partial_{n+1}u\|_{L^{p}(\Omega)}.$$ Now, I've been told that the notation $\partial_{n+1}u$ means ""the directional derivative of $u$ in the direction of $n+1$"". What does that even mean/look like? I feel like I won't be able to do this problem until I find out what this actually means, so if anyone could shed some light on the subject, it would be much appreciated!","I am trying to do the following problem: Let $\Omega \subset \subset \overline{\mathbb{R}_{+}^{n+1}} = \{ (x, x_{n+1}); \, x_{n+1}\geq 0\}$ (i.e., $\Omega$ is bounded inside the closed upper half-space). (By definition, smooth functions on $\overline{\mathbb{R}_{+}^{n+1}}$ are functions which are restrictions of smooth functions on an open neighborhood of $\overline{\mathbb{R}_{+}^{n+1}}$ in $\mathbb{R}^{n+1}$.) Let $u \in C_{0}^{\infty}(\Omega)$. Show that for any $p$ such that $1\leq p \leq \infty$,  there is a constant $C_{\Omega}$ independent of $u$ so that,  $$\|u(\cdot,0)\|_{L^{p}(\mathbb{R}^{n})}\leq C_{\Omega}\|\partial_{n+1}u\|_{L^{p}(\Omega)}.$$ Now, I've been told that the notation $\partial_{n+1}u$ means ""the directional derivative of $u$ in the direction of $n+1$"". What does that even mean/look like? I feel like I won't be able to do this problem until I find out what this actually means, so if anyone could shed some light on the subject, it would be much appreciated!",,"['analysis', 'functional-analysis']"
97,Multivariable Calculus Divergence Theorem,Multivariable Calculus Divergence Theorem,,"I have a cylinder where the top is given by $ z=-2$ the bottom is $z=2$ and the ""lateral surface"" is $x^2+y^2=1$. I have to find$\iint_{s}(4x+3y+z^2) \ dS$. I know this involves using the divergence theorem and then plugging in the answer into the volume of a cylinder but I have no idea how to set up the limits on the triple integral once I have found the partials and split up the integral for the divergence theorem. Thanks.","I have a cylinder where the top is given by $ z=-2$ the bottom is $z=2$ and the ""lateral surface"" is $x^2+y^2=1$. I have to find$\iint_{s}(4x+3y+z^2) \ dS$. I know this involves using the divergence theorem and then plugging in the answer into the volume of a cylinder but I have no idea how to set up the limits on the triple integral once I have found the partials and split up the integral for the divergence theorem. Thanks.",,"['multivariable-calculus', 'volume']"
98,What does $\Delta$ mean in context of vector calculus?,What does  mean in context of vector calculus?,\Delta,"I'm reading an article that has a formula for $\Delta \phi(x)$, where $\phi : \mathbf{R}^2 \rightarrow \mathbf{R}$ and $x \in \mathbf{R}^2$ and $\Delta \phi(x) : \mathbf{R}^2 \rightarrow \mathbf{R}$ From context I think it has to do with the gradient of $\phi$.  Specifically I strongly suspect it means $|\nabla \phi(x)|$, but I've never seen $\Delta$ used to mean that.  Is that a common practice? The paper specifically is Transfinite mean value interpolation , bottom of page 17.","I'm reading an article that has a formula for $\Delta \phi(x)$, where $\phi : \mathbf{R}^2 \rightarrow \mathbf{R}$ and $x \in \mathbf{R}^2$ and $\Delta \phi(x) : \mathbf{R}^2 \rightarrow \mathbf{R}$ From context I think it has to do with the gradient of $\phi$.  Specifically I strongly suspect it means $|\nabla \phi(x)|$, but I've never seen $\Delta$ used to mean that.  Is that a common practice? The paper specifically is Transfinite mean value interpolation , bottom of page 17.",,"['calculus', 'multivariable-calculus', 'vector-analysis']"
99,Multivariable limit of $\frac {xy}{e^{x^2y^2}}$,Multivariable limit of,\frac {xy}{e^{x^2y^2}},"We want to find $$ \large \lim_{x^2 + y^2 \to \infty } \frac {xy}{ e^{x^2y^2} }$$ It looks like it goes to zero but if we let $y = \frac 1x$then the limit is equal to $\frac 1e$ i.e. the function becomes constant when the domain is restricted to the curve $y = 1/x$. By changing to polar coordinates, $x = r \cos \theta, y = r \sin \theta$ we get $$ \large \lim_{r^2 \to \infty} \frac { r^2 \sin \theta \cos \theta }{e^{ r^4 \sin^2 \theta \cos^2 \theta }} = \frac 12 \frac { r^2 \sin 2\theta  }{e^{ \frac 14 r^4 \sin^2 2\theta  }}   $$ Now one would say that since $e^{r^4}$ grows faster than $r^2$, the limit goes to zero. But as clearly demonstrated earlier, the limit does not exist. So, how can we now show from expression above that this limit is not necessarily zero?","We want to find $$ \large \lim_{x^2 + y^2 \to \infty } \frac {xy}{ e^{x^2y^2} }$$ It looks like it goes to zero but if we let $y = \frac 1x$then the limit is equal to $\frac 1e$ i.e. the function becomes constant when the domain is restricted to the curve $y = 1/x$. By changing to polar coordinates, $x = r \cos \theta, y = r \sin \theta$ we get $$ \large \lim_{r^2 \to \infty} \frac { r^2 \sin \theta \cos \theta }{e^{ r^4 \sin^2 \theta \cos^2 \theta }} = \frac 12 \frac { r^2 \sin 2\theta  }{e^{ \frac 14 r^4 \sin^2 2\theta  }}   $$ Now one would say that since $e^{r^4}$ grows faster than $r^2$, the limit goes to zero. But as clearly demonstrated earlier, the limit does not exist. So, how can we now show from expression above that this limit is not necessarily zero?",,['multivariable-calculus']
