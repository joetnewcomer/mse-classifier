,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"How to prove $\sum_{1 \leq i,j \leq n} {\frac{x_i x_j}{x_i^2+x_j^2}} \geq 0$ without using integral",How to prove  without using integral,"\sum_{1 \leq i,j \leq n} {\frac{x_i x_j}{x_i^2+x_j^2}} \geq 0","Let $x_1,x_2,...x_n$ be nonzero real number. Prove that $$\sum_{1 \leq i,j \leq n} {\frac{x_i x_j}{x_i^2+x_j^2}} \geq 0.$$ I think this problem by Prove that $E\left(\frac{XY}{X^2+Y^2}\right) \geqslant 0$ for i.i.d. $X$ and $Y$ . But I want to find proof without using integral . How to solve this? Edit : As @qfwfq pointed out, a proof without using integral is given in Proof that $\sum\limits_{j,k=1}^N\frac{a_ja_k}{j+k}\ge0$ . @Ramil's comment there: I'm also pretty interested in more elementary proof so that even an eighth-or-ninth-grader from high school student could understand it. Is there any way to do without determinants, integrals or any tools of higher (advanced) math ? Maybe just using a couple of standard (or not quite standard) inequalities, mathematical induction, algebra","Let be nonzero real number. Prove that I think this problem by Prove that $E\left(\frac{XY}{X^2+Y^2}\right) \geqslant 0$ for i.i.d. $X$ and $Y$ . But I want to find proof without using integral . How to solve this? Edit : As @qfwfq pointed out, a proof without using integral is given in Proof that $\sum\limits_{j,k=1}^N\frac{a_ja_k}{j+k}\ge0$ . @Ramil's comment there: I'm also pretty interested in more elementary proof so that even an eighth-or-ninth-grader from high school student could understand it. Is there any way to do without determinants, integrals or any tools of higher (advanced) math ? Maybe just using a couple of standard (or not quite standard) inequalities, mathematical induction, algebra","x_1,x_2,...x_n \sum_{1 \leq i,j \leq n} {\frac{x_i x_j}{x_i^2+x_j^2}} \geq 0.","['real-analysis', 'inequality']"
1,Is the following proof of the continuity of a function correct?,Is the following proof of the continuity of a function correct?,,"I just started learning analysis and encountered my first task on continuity today, namely proving that $f: \mathbb{R}\to \mathbb{R}, \space f(x) = \inf\{|x-k\rvert \space \mid k\in \mathbb{Z} \}$ is continuous in every point. I attempted to prove this as follows: Let $x_{0} \in \mathbb{R}$ be arbitrary. Let $x \in \mathbb{R}$ . By the triangle inequality we have $\left|x_{0}-k\right| \leq\left|x_{0}-x\right|+|x-k|$ for all $k \in \mathbb{Z}$ . Thus, one obtains $$ \inf \left\{\left|x_{0}-k\right| \mid k \in \mathbb{Z}\right\} \leq\left|x_{0}-x\right|+\inf \{|x-k| \mid k \in \mathbb{Z}\} $$ so by the definition of $f$ $$ f\left(x_{0}\right)-f(x) \leq\left|x_{0}-x\right| $$ so we obtain $f(x)-f\left(x_{0}\right) \leq$ $\left|x_{0}-x\right|$ (applying the same argumentation). We get $$ \left|f\left(x_{0}\right)-f(x)\right| \leq\left|x_{0}-x\right| $$ Now, let $\epsilon>0$ be arbitrary. To show that $f$ is continuous in $x_{0}$ we must find some $\delta>0$ , s.t. for all $x \in \mathbb{R}$ with $\left|x-x_{0}\right|<\delta$ the inequality $\left|f\left(x_{0}\right)-f(x)\right|<\epsilon$ holds. From the above we can clearly see that $\delta=\epsilon$ is a sufficient choice is. Since $x_{0}$ was arbitrary, we are done. Does this attempt seem valid? Also, what are other ways one could prove this (assuming that the above works). I assume that one often can just plug in the definition of the respective function and then rearrange the inequality, but this didn‘t quite work for me in this case.","I just started learning analysis and encountered my first task on continuity today, namely proving that is continuous in every point. I attempted to prove this as follows: Let be arbitrary. Let . By the triangle inequality we have for all . Thus, one obtains so by the definition of so we obtain (applying the same argumentation). We get Now, let be arbitrary. To show that is continuous in we must find some , s.t. for all with the inequality holds. From the above we can clearly see that is a sufficient choice is. Since was arbitrary, we are done. Does this attempt seem valid? Also, what are other ways one could prove this (assuming that the above works). I assume that one often can just plug in the definition of the respective function and then rearrange the inequality, but this didn‘t quite work for me in this case.","f: \mathbb{R}\to \mathbb{R}, \space f(x) = \inf\{|x-k\rvert \space \mid k\in \mathbb{Z} \} x_{0} \in \mathbb{R} x \in \mathbb{R} \left|x_{0}-k\right| \leq\left|x_{0}-x\right|+|x-k| k \in \mathbb{Z} 
\inf \left\{\left|x_{0}-k\right| \mid k \in \mathbb{Z}\right\} \leq\left|x_{0}-x\right|+\inf \{|x-k| \mid k \in \mathbb{Z}\}
 f 
f\left(x_{0}\right)-f(x) \leq\left|x_{0}-x\right|
 f(x)-f\left(x_{0}\right) \leq \left|x_{0}-x\right| 
\left|f\left(x_{0}\right)-f(x)\right| \leq\left|x_{0}-x\right|
 \epsilon>0 f x_{0} \delta>0 x \in \mathbb{R} \left|x-x_{0}\right|<\delta \left|f\left(x_{0}\right)-f(x)\right|<\epsilon \delta=\epsilon x_{0}","['real-analysis', 'continuity', 'solution-verification']"
2,Bounding the difference between consecutive terms in the sequence $a_n=(1+1/n)^n$,Bounding the difference between consecutive terms in the sequence,a_n=(1+1/n)^n,"I'm pretty sure the following estimate holds for $C>1$ , but I can't prove it: $$a_n:=\left(1+\frac{1}{n}\right)^n-\left(1+\frac{1}{n-1}\right)^{n-1}\leq\frac{C}{n^2}$$ For context, I was looking for some such bound in order to prove that $na_n\to0$ . I tried using the fact that $$\left(1+\frac{1}{n}\right)^n\leq\color{red}{1}+\color{red}{1}+\color{red}{\frac{1}{2}}+\cdots+\color{red}{\frac{1}{n!}}:=S_n$$ because, by some algebra with the binomial theorem, $$\left(1+\frac{1}{n}\right)^n=\color{red}{1}+\color{red}{1}+\color{red}{\frac{1}{2}}\color{blue}{\left(1-\frac{1}{n}\right)}+\cdots+\color{red}{\frac{1}{n!}}\color{blue}{\left(1-\frac{1}{n}\right)\cdots\left(1-\frac{n-1}{n}\right)}$$ where the blue factors are all less than one. But I also need a good lower bound on the terms $(1+n^{-1})^{n}$ , and I can't see how to pick the right lower bound to get, say, $$a_n\leq S_n-\text{lower bound applied to the term $\left(1+\frac{1}{n-1}\right)^{n-1}$}\leq\frac{C}{n^2}$$","I'm pretty sure the following estimate holds for , but I can't prove it: For context, I was looking for some such bound in order to prove that . I tried using the fact that because, by some algebra with the binomial theorem, where the blue factors are all less than one. But I also need a good lower bound on the terms , and I can't see how to pick the right lower bound to get, say,",C>1 a_n:=\left(1+\frac{1}{n}\right)^n-\left(1+\frac{1}{n-1}\right)^{n-1}\leq\frac{C}{n^2} na_n\to0 \left(1+\frac{1}{n}\right)^n\leq\color{red}{1}+\color{red}{1}+\color{red}{\frac{1}{2}}+\cdots+\color{red}{\frac{1}{n!}}:=S_n \left(1+\frac{1}{n}\right)^n=\color{red}{1}+\color{red}{1}+\color{red}{\frac{1}{2}}\color{blue}{\left(1-\frac{1}{n}\right)}+\cdots+\color{red}{\frac{1}{n!}}\color{blue}{\left(1-\frac{1}{n}\right)\cdots\left(1-\frac{n-1}{n}\right)} (1+n^{-1})^{n} a_n\leq S_n-\text{lower bound applied to the term \left(1+\frac{1}{n-1}\right)^{n-1}}\leq\frac{C}{n^2},"['real-analysis', 'limits', 'inequality', 'upper-lower-bounds']"
3,Proof that a sequence $(x_n)_{n \ge 1}$ such that $|x_{n+1} - x_n| < \frac{1}{n}$ converges.,Proof that a sequence  such that  converges.,(x_n)_{n \ge 1} |x_{n+1} - x_n| < \frac{1}{n},"I'm studying for a test and I came across this exercise: Let $(x_n)_{n \ge 1}$ be a sequence such that $x_{2n-1} \le x_{2n+1} \le x_{2n+2} \le x_{2n}, \hspace4ex |x_{n+1} - x_n| < \frac{1}{n}$ Prove that the sequence converges. I've been stuck on this question for a while now, by doing some arithmetics I've gotten a few inequalities, namely: (1) $x_{2n} - x_{2n+1} < \frac{1}{2n}$ (2) $x_{2n+2} - x_{2n+1} < \frac{1}{2n+1}$ (3) $x_{2n} - x_{2n-1} < \frac{1}{2n-1}$ But I have no idea where to go from here, or even if those inequalities are useful at all.","I'm studying for a test and I came across this exercise: Let be a sequence such that Prove that the sequence converges. I've been stuck on this question for a while now, by doing some arithmetics I've gotten a few inequalities, namely: (1) (2) (3) But I have no idea where to go from here, or even if those inequalities are useful at all.","(x_n)_{n \ge 1} x_{2n-1} \le x_{2n+1} \le x_{2n+2} \le x_{2n}, \hspace4ex |x_{n+1} - x_n| < \frac{1}{n} x_{2n} - x_{2n+1} < \frac{1}{2n} x_{2n+2} - x_{2n+1} < \frac{1}{2n+1} x_{2n} - x_{2n-1} < \frac{1}{2n-1}","['real-analysis', 'sequences-and-series']"
4,Do derivative and Integral really cancel out so easily here?,Do derivative and Integral really cancel out so easily here?,,"In a paper I came across the following equation where the loss function $\mathcal{L}$ must be minimized with respect to the reconstruction function $r_\sigma$ . With $\bar{x} := x + \eta \quad \eta \sim \mathcal{N}(\sigma^2I)$ : $$ \begin{align*} \mathcal{L}_{\textrm{DAE}}(\theta) &=  \mathbb{E}_{f(\eta, x)} \left[ \|x - r_{\sigma}(\bar{x}) \|^2_2  \right]  \\  &= \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} p(\bar{x}-\eta)p(\eta|\bar{x}) \|r_{\sigma}(\bar{x}) - \bar{x} + \eta  \|^2_2 \;  d\eta d\bar{x} \\  &= \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} p(\bar{x}-\eta)p(\eta) \|r_{\sigma}(\bar{x}) - \bar{x} + \eta  \|^2_2  \; d\eta d\bar{x} \\ &=  \int_{\mathbb{R}^d} \mathbb{E}_{\eta} \left[ p(\bar{x}-\eta) \left\|r_{\sigma}(\bar{x}) - \bar{x} + \eta \right \|^2_2 \right] d\bar{x} \quad (1)  \end{align*} $$ We minimize this equation by differentiating it with respect to $r_{\sigma}(\bar{x})$ and setting it equal to $0$ . Let $r_\sigma^*(\bar{x})$ denote the the minimum: $$ \begin{align*} 0 &= \mathbb{E}_{\eta } \left[ p(\bar{x}-\eta) \left(r_\sigma^*(\bar{x}) - \bar{x} + \eta \right) \right] \quad(2) \end{align*} \\ ... $$ My problem is the step between (1) and (2). Although it looks correct, I failed to do this step more formally. Any help is much appreciated! It is the first part of the proof 6.1 from https://arxiv.org/pdf/1211.4246.pdf Edit Thanks to Milo Brandt's answer I now understand why the integral vanishes. However I am very unsure about the correctness of the  following steps: Lets fix $\bar{x}$ and try to find a $r$ which minimizes $\mathbb{E}_{\eta} \left[ p(\bar{x}-\eta) \left\|r_{\sigma}(\bar{x}) - \bar{x} + \eta \right \|^2_2 \right] $ for this arbitrary $\bar{x}$ . To do so we take the derivativ with respect $r$ and get the minima $r^*$ by setting it to 0. $$ \begin{align} 0=&\frac{d}{dr}  \mathbb{E}_{\eta} \left[ p(\bar{x}-\eta) \left\|r_{\sigma}^*(\bar{x}) - \bar{x} + \eta \right \|^2_2 \right] \qquad (3) \\ 0= &  \mathbb{E}_{\eta} \left[ \frac{d}{dr}  p(\bar{x}-\eta) \left\|r_{\sigma}^*(\bar{x}) - \bar{x} + \eta \right \|^2_2 \right] \qquad (4)\\ 0 &= \mathbb{E}_{\eta } \left[ p(\bar{x}-\eta) \left(r_\sigma^*(\bar{x}) - \bar{x} + \eta \right) \right] \quad(4) \end{align} $$","In a paper I came across the following equation where the loss function must be minimized with respect to the reconstruction function . With : We minimize this equation by differentiating it with respect to and setting it equal to . Let denote the the minimum: My problem is the step between (1) and (2). Although it looks correct, I failed to do this step more formally. Any help is much appreciated! It is the first part of the proof 6.1 from https://arxiv.org/pdf/1211.4246.pdf Edit Thanks to Milo Brandt's answer I now understand why the integral vanishes. However I am very unsure about the correctness of the  following steps: Lets fix and try to find a which minimizes for this arbitrary . To do so we take the derivativ with respect and get the minima by setting it to 0.","\mathcal{L} r_\sigma \bar{x} := x + \eta \quad \eta \sim \mathcal{N}(\sigma^2I) 
\begin{align*}
\mathcal{L}_{\textrm{DAE}}(\theta) &= 
\mathbb{E}_{f(\eta, x)} \left[ \|x - r_{\sigma}(\bar{x}) \|^2_2  \right]  \\
 &= \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} p(\bar{x}-\eta)p(\eta|\bar{x}) \|r_{\sigma}(\bar{x}) - \bar{x} + \eta  \|^2_2 \;  d\eta d\bar{x} \\
 &= \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} p(\bar{x}-\eta)p(\eta) \|r_{\sigma}(\bar{x}) - \bar{x} + \eta  \|^2_2  \; d\eta d\bar{x} \\
&=  \int_{\mathbb{R}^d} \mathbb{E}_{\eta} \left[ p(\bar{x}-\eta) \left\|r_{\sigma}(\bar{x}) - \bar{x} + \eta \right \|^2_2 \right] d\bar{x} \quad (1) 
\end{align*}
 r_{\sigma}(\bar{x}) 0 r_\sigma^*(\bar{x}) 
\begin{align*}
0 &= \mathbb{E}_{\eta } \left[ p(\bar{x}-\eta) \left(r_\sigma^*(\bar{x}) - \bar{x} + \eta \right) \right] \quad(2)
\end{align*} \\
...
 \bar{x} r \mathbb{E}_{\eta} \left[ p(\bar{x}-\eta) \left\|r_{\sigma}(\bar{x}) - \bar{x} + \eta \right \|^2_2 \right]  \bar{x} r r^* 
\begin{align}
0=&\frac{d}{dr}  \mathbb{E}_{\eta} \left[ p(\bar{x}-\eta) \left\|r_{\sigma}^*(\bar{x}) - \bar{x} + \eta \right \|^2_2 \right] \qquad (3) \\
0= &  \mathbb{E}_{\eta} \left[ \frac{d}{dr}  p(\bar{x}-\eta) \left\|r_{\sigma}^*(\bar{x}) - \bar{x} + \eta \right \|^2_2 \right] \qquad (4)\\
0 &= \mathbb{E}_{\eta } \left[ p(\bar{x}-\eta) \left(r_\sigma^*(\bar{x}) - \bar{x} + \eta \right) \right] \quad(4)
\end{align}
","['real-analysis', 'integration', 'derivatives']"
5,"Prove function $f:[a, b] \rightarrow \mathbb{R}$ is Riemann integrable if and only if $f$ is bounded and almost everywhere continuous.",Prove function  is Riemann integrable if and only if  is bounded and almost everywhere continuous.,"f:[a, b] \rightarrow \mathbb{R} f","I wonder how can we prove the backward direction (function $f:[a, b] \rightarrow \mathbb{R}$ is Riemann integrable, then $f$ is bounded and almost everywhere continuous.) without using the measure theory. My attempt: If $f\in R(\alpha)$ on [a,b], there exists a partition $U(P,f,\alpha)-L(P,f,\alpha)<\epsilon$ . Consider an arbitrary partition $P: {a=x_0<x_1<\ldots<x_n=b}$ . By theorem 6.7(Rudin), If $s,t$ are arbitrary points in $[x_{i-1},x_i]$ , then $\sum_{i=1}^n|f(s)-f(t)|\Delta\alpha_i<\epsilon$ . Since $U(P,f,\alpha)-L(P,f,\alpha)=\sum_{i=1}^n(M_i-m_i)\Delta\alpha_i<\epsilon$ by definition and $f$ is bounded, $M_i-m_i\leq\eta$ where $\eta>0$ . Thus, given $\eta > 0,\exists \delta>0$ s.t. when $|s-t|<\delta$ , $|f(s)-f(t)|<\eta.$ Thus, $f$ is continuous on each $[x_{i-1},x_i]$ . I felt the part "" $M_i-m_i\leq\epsilon$ where $\epsilon>0$ . Thus, given $\epsilon > 0,\exists \delta>0$ "" weird and not sure if what I'm doing is correct. Any help appreciated! Edit: -------------------------- The original problem is : Let $a_{1}, a_{2}, \ldots$ be a strictly increasing sequence in $(a, b],$ and let $p_{n}>0$ be such that $\sum_{n=1}^{\infty} p_{n}=1$ Define $\alpha:[a, b] \rightarrow \mathbb{R}$ as follows: $\alpha(x)=0$ if $a \leq x<a_{1}$ $$ \alpha(x)=\sum_{k=1}^{n} p_{k} \text { if } a_{n} \leq x<a_{n+1} $$ and $\alpha(x)=1$ if $\sup _{n} a_{n} \leq x \leq b$ I need to Show that a bounded function $f:[a, b] \rightarrow \mathbb{R}$ is Riemann-Stieltjes integrable with respect to $\alpha$ if and only if $f\left(a_{n}^{-}\right)=\lim _{x \rightarrow a_{n}^{-}} f(x)$ exists and $f\left(a_{n}^{-}\right)=f\left(a_{n}\right)$ for all $n,$ and that in this case $$ \int_{a}^{b} f d \alpha=\sum_{n=1}^{\infty} f\left(a_{n}\right) p_{n} $$ I'm stuck on proving "" $f:[a, b] \rightarrow \mathbb{R}$ is Riemann-Stieltjes integrable, then it's left continuous"" .","I wonder how can we prove the backward direction (function is Riemann integrable, then is bounded and almost everywhere continuous.) without using the measure theory. My attempt: If on [a,b], there exists a partition . Consider an arbitrary partition . By theorem 6.7(Rudin), If are arbitrary points in , then . Since by definition and is bounded, where . Thus, given s.t. when , Thus, is continuous on each . I felt the part "" where . Thus, given "" weird and not sure if what I'm doing is correct. Any help appreciated! Edit: -------------------------- The original problem is : Let be a strictly increasing sequence in and let be such that Define as follows: if and if I need to Show that a bounded function is Riemann-Stieltjes integrable with respect to if and only if exists and for all and that in this case I'm stuck on proving "" is Riemann-Stieltjes integrable, then it's left continuous"" .","f:[a, b] \rightarrow \mathbb{R} f f\in R(\alpha) U(P,f,\alpha)-L(P,f,\alpha)<\epsilon P: {a=x_0<x_1<\ldots<x_n=b} s,t [x_{i-1},x_i] \sum_{i=1}^n|f(s)-f(t)|\Delta\alpha_i<\epsilon U(P,f,\alpha)-L(P,f,\alpha)=\sum_{i=1}^n(M_i-m_i)\Delta\alpha_i<\epsilon f M_i-m_i\leq\eta \eta>0 \eta > 0,\exists \delta>0 |s-t|<\delta |f(s)-f(t)|<\eta. f [x_{i-1},x_i] M_i-m_i\leq\epsilon \epsilon>0 \epsilon > 0,\exists \delta>0 a_{1}, a_{2}, \ldots (a, b], p_{n}>0 \sum_{n=1}^{\infty} p_{n}=1 \alpha:[a, b] \rightarrow \mathbb{R} \alpha(x)=0 a \leq x<a_{1} 
\alpha(x)=\sum_{k=1}^{n} p_{k} \text { if } a_{n} \leq x<a_{n+1}
 \alpha(x)=1 \sup _{n} a_{n} \leq x \leq b f:[a, b] \rightarrow \mathbb{R} \alpha f\left(a_{n}^{-}\right)=\lim _{x \rightarrow a_{n}^{-}} f(x) f\left(a_{n}^{-}\right)=f\left(a_{n}\right) n, 
\int_{a}^{b} f d \alpha=\sum_{n=1}^{\infty} f\left(a_{n}\right) p_{n}
 f:[a, b] \rightarrow \mathbb{R}","['real-analysis', 'integration', 'analysis', 'riemann-integration']"
6,Showing $\frac{f(a+2b)+f(a-2b)}{2f(a)}\leq\sqrt{1-\left(\frac{b}{a(1-a)}\right)^2}$ for $f(x)=\sqrt{x(1-x)}$ with some constraints,Showing  for  with some constraints,\frac{f(a+2b)+f(a-2b)}{2f(a)}\leq\sqrt{1-\left(\frac{b}{a(1-a)}\right)^2} f(x)=\sqrt{x(1-x)},"My question is from a inequality that is not proved (it is just implicitly mentioned I guess) in a book. Specifically, let $a \in (0,1)$ and $b \in (0,1)$ with $a - 2b \geq 0$ and $a + 2b \leq 1$ . Then with $f(x) := \sqrt{x(1-x)}$ we are asked to show that $$\frac{f(a+2b)+f(a-2b)}{2f(a)} \leq \sqrt{1-\left(\frac{b}{a(1-a)}\right)^2}$$ I am really stuck at this, the only related inequality that comes to mind is that $$\frac{\sqrt{1+2\alpha}+\sqrt{1-2\alpha}}{2} \leq \sqrt{1-\alpha^2},\quad \forall \alpha \in [0,\tfrac{1}{2}]$$ But I still cannot prove the desired inequality. Any help is greatly appreciated!","My question is from a inequality that is not proved (it is just implicitly mentioned I guess) in a book. Specifically, let and with and . Then with we are asked to show that I am really stuck at this, the only related inequality that comes to mind is that But I still cannot prove the desired inequality. Any help is greatly appreciated!","a \in (0,1) b \in (0,1) a - 2b \geq 0 a + 2b \leq 1 f(x) := \sqrt{x(1-x)} \frac{f(a+2b)+f(a-2b)}{2f(a)} \leq \sqrt{1-\left(\frac{b}{a(1-a)}\right)^2} \frac{\sqrt{1+2\alpha}+\sqrt{1-2\alpha}}{2} \leq \sqrt{1-\alpha^2},\quad \forall \alpha \in [0,\tfrac{1}{2}]","['real-analysis', 'calculus', 'inequality']"
7,Proving $\underset{n\to \infty }{\text{lim}}\frac{n!}{n^{n+\frac{1}{2}} \ e^{-n}}=\sqrt{2 \pi }$,Proving,\underset{n\to \infty }{\text{lim}}\frac{n!}{n^{n+\frac{1}{2}} \ e^{-n}}=\sqrt{2 \pi },"This question is the last part of a problem leading to proof of Stirling's approximation. I've already proved that $\underset{n\to \infty }{\text{lim}}\frac{n!}{n^{n+\frac{1}{2}} \ e^{-n}}$ exists and that $\underset{n\to \infty }{\text{lim}}\frac{2^{4 n} (n!)^4}{((2 n)!)^2 \ (2 n+1)}=\frac{\pi }{2}$ . Hence, the question asks to assume $\underset{n\to \infty }{\text{lim}}\frac{n!}{n^{n+\frac{1}{2}} \ e^{-n}}$ exists, and then asks to use $\underset{n\to \infty }{\text{lim}}\frac{2^{4 n} (n!)^4}{((2 n)!)^2 \ (2 n+1)}=\frac{\pi }{2}$ to show $\underset{n\to \infty }{\text{lim}}\frac{n!}{n^{n+\frac{1}{2}} \ e^{-n}}=\sqrt{2 \pi }$ . My attempt goes like this: Since $\sqrt{x}$ is continuous, we can use $\sqrt{\underset{n\to \infty }{\text{lim}}f(n)}=\underset{n\to \ \infty }{\text{lim}}\sqrt{f(n)}$ to get $\underset{n\to \infty }{\text{lim}}\frac{2^{2 n} (n!)^2}{(2 n)! \ \sqrt{2 n+1}}=\sqrt{\frac{\pi }{2}}$ . Then we can eliminate $2^{2 n}n!$ to get $$\frac{2^{2 n} (n!)^2}{\sqrt{1+2 n} (2 n)!}=\frac{n!}{\sqrt{1+2 n} \left(n-\frac{1}{2}\right) \left(n-\frac{3}{2}\right) \cdots  \ \frac{3}{2}\frac{1}{2}}$$ Then factor out $n^n$ and adjust $\sqrt{1+2n}$ to get $$\frac{2^{2 n} (n!)^2}{\sqrt{1+2 n} (2 n)!}=\frac{n!}{n^{n+\frac{1}{2}} \sqrt{2+\frac{1}{n}} \left(1-\frac{1}{2n}\right) \left(1-\frac{3}{2n}\right) \cdots  \ \frac{3}{2n}\frac{1}{2n}}$$ The $\sqrt{2+\frac{1}{n}}$ factor will give a $\sqrt{2}$ , so the remaining $\prod _k^n \left(1-\frac{2 k-1}{2 n}\right)$ must somehow relate to $\sqrt{2} e^{-n}$ . However, I'm not sure how to do this.","This question is the last part of a problem leading to proof of Stirling's approximation. I've already proved that exists and that . Hence, the question asks to assume exists, and then asks to use to show . My attempt goes like this: Since is continuous, we can use to get . Then we can eliminate to get Then factor out and adjust to get The factor will give a , so the remaining must somehow relate to . However, I'm not sure how to do this.","\underset{n\to \infty }{\text{lim}}\frac{n!}{n^{n+\frac{1}{2}} \ e^{-n}} \underset{n\to \infty }{\text{lim}}\frac{2^{4 n} (n!)^4}{((2 n)!)^2 \ (2 n+1)}=\frac{\pi }{2} \underset{n\to \infty }{\text{lim}}\frac{n!}{n^{n+\frac{1}{2}} \ e^{-n}} \underset{n\to \infty }{\text{lim}}\frac{2^{4 n} (n!)^4}{((2 n)!)^2 \ (2 n+1)}=\frac{\pi }{2} \underset{n\to \infty }{\text{lim}}\frac{n!}{n^{n+\frac{1}{2}} \ e^{-n}}=\sqrt{2 \pi } \sqrt{x} \sqrt{\underset{n\to \infty }{\text{lim}}f(n)}=\underset{n\to \
\infty }{\text{lim}}\sqrt{f(n)} \underset{n\to \infty }{\text{lim}}\frac{2^{2 n} (n!)^2}{(2 n)! \
\sqrt{2 n+1}}=\sqrt{\frac{\pi }{2}} 2^{2 n}n! \frac{2^{2 n} (n!)^2}{\sqrt{1+2 n} (2 n)!}=\frac{n!}{\sqrt{1+2 n} \left(n-\frac{1}{2}\right) \left(n-\frac{3}{2}\right) \cdots  \
\frac{3}{2}\frac{1}{2}} n^n \sqrt{1+2n} \frac{2^{2 n} (n!)^2}{\sqrt{1+2 n} (2 n)!}=\frac{n!}{n^{n+\frac{1}{2}} \sqrt{2+\frac{1}{n}} \left(1-\frac{1}{2n}\right) \left(1-\frac{3}{2n}\right) \cdots  \
\frac{3}{2n}\frac{1}{2n}} \sqrt{2+\frac{1}{n}} \sqrt{2} \prod _k^n \left(1-\frac{2 k-1}{2 n}\right) \sqrt{2} e^{-n}",['real-analysis']
8,Bound on approximation error of Bernstein polynomial,Bound on approximation error of Bernstein polynomial,,"Consider a continuous function $f: [0,1] \to [0,1]$ . Let $B_n$ be its $n$ -th order Bernstein polynomial , $$ B_n(x) = \sum_{k=0}^n f\left(\frac{k}{n}\right) \binom{n}{k}x^k (1-x)^{n-k}. $$ As is well known, $B_n(x) \rightarrow f(x)$ uniformly on $[0,1]$ as $n \rightarrow \infty$ . I am interested in bounding the approximation error $B_n(x)-f(x)$ . This reference , section 4, contains one such bound: $$ |B_n(x)-f(x)| \leq \left( 1 + \frac{1}{4n^2} \right) \omega(n^{-1/2}) $$ where $\omega$ is the modulus of continuity of $f$ , that is, $\omega(\delta) = \sup_{|x-x'|<\delta} |f(x)-f(x')|$ . My questions are Is there any reference or proof to that result? Are there any similar results that provide a bound on $|B_n(x)-f(x)|$ ?","Consider a continuous function . Let be its -th order Bernstein polynomial , As is well known, uniformly on as . I am interested in bounding the approximation error . This reference , section 4, contains one such bound: where is the modulus of continuity of , that is, . My questions are Is there any reference or proof to that result? Are there any similar results that provide a bound on ?","f: [0,1] \to [0,1] B_n n 
B_n(x) = \sum_{k=0}^n f\left(\frac{k}{n}\right) \binom{n}{k}x^k (1-x)^{n-k}.
 B_n(x) \rightarrow f(x) [0,1] n \rightarrow \infty B_n(x)-f(x) 
|B_n(x)-f(x)| \leq \left( 1 + \frac{1}{4n^2} \right) \omega(n^{-1/2})
 \omega f \omega(\delta) = \sup_{|x-x'|<\delta} |f(x)-f(x')| |B_n(x)-f(x)|","['real-analysis', 'functions', 'polynomials', 'reference-request', 'approximation-theory']"
9,When is the $\lim\sup(a_n+b_n)$ strictly less than $\lim \sup (a_n)+\lim\sup(b_n)$,When is the  strictly less than,\lim\sup(a_n+b_n) \lim \sup (a_n)+\lim\sup(b_n),"So I recently proved this inequality in my real analysis class: $$\lim\sup(a_n+b_n)\leq \lim\sup(a_n) + \lim\sup(b_n)$$ However I am wondering when this inequality is strictly less than. I've tried out a bunch of sequences and here is my thought process so far: $$$$ To get a view of when this inequality is unequal look at the two sequences: $$a_n=(0,1,0,1,0,....)$$ $$b_n=(1,0,1,0,1,....)$$ Clearly: $$\lim \sup (a_n+b_n)=1$$ However: $$\lim \sup (a_n)+\lim\sup(b_n)=2$$ This seems to imply that if the limit of a sequence does not exist then we have an inequality. Furthermore what if: $$a_n=(1,1,1,1,....)$$ $$b_n=(0,1,0,1,....)$$ Well then clearly: $$\lim \sup (a_n+b_n)=2$$ $$\lim \sup(a_n)+\lim \sup (b_n)=2$$ And we have equality, which contradicts our earlier hypothesis. Perhaps then it is if both limits do not exist, well let $b_n$ be defined as previously and $a_n=cb_n$ , for some $c\in\mathbb{R}$ both these limits of the sequence do not exist however we obtain: $$\lim \sup (a_n+b_n)=c+1$$ $$\lim\sup(a_n)+\lim\sup(b_n)=c+1$$ And we have equality again. Well then perhaps it must be that both limits do not exist, and that the sequences cannot be scalar multiples of each other. Well then define: $$a_n=(0,1,0,1,0,....)$$ $$b_n-(0,0,0,1,0,0,0,1...)$$ Clearly: $$\lim \sup (a_n+b_n)=2$$ $$\lim \sup(a_n)+\lim \sup (b_n)=2$$ From here I tried testing some cases where $\sup(a_n)\neq \lim\sup(a_n)$ but I still continued to get equalities. So I really can't figure out for what conditions inequality holds. I guess if $a_n$ and $b_n$ look period but are shifted by a non even $n$ then that would make sense but I feel like there's more to it than that.","So I recently proved this inequality in my real analysis class: However I am wondering when this inequality is strictly less than. I've tried out a bunch of sequences and here is my thought process so far: To get a view of when this inequality is unequal look at the two sequences: Clearly: However: This seems to imply that if the limit of a sequence does not exist then we have an inequality. Furthermore what if: Well then clearly: And we have equality, which contradicts our earlier hypothesis. Perhaps then it is if both limits do not exist, well let be defined as previously and , for some both these limits of the sequence do not exist however we obtain: And we have equality again. Well then perhaps it must be that both limits do not exist, and that the sequences cannot be scalar multiples of each other. Well then define: Clearly: From here I tried testing some cases where but I still continued to get equalities. So I really can't figure out for what conditions inequality holds. I guess if and look period but are shifted by a non even then that would make sense but I feel like there's more to it than that.","\lim\sup(a_n+b_n)\leq \lim\sup(a_n) + \lim\sup(b_n)  a_n=(0,1,0,1,0,....) b_n=(1,0,1,0,1,....) \lim \sup (a_n+b_n)=1 \lim \sup (a_n)+\lim\sup(b_n)=2 a_n=(1,1,1,1,....) b_n=(0,1,0,1,....) \lim \sup (a_n+b_n)=2 \lim \sup(a_n)+\lim \sup (b_n)=2 b_n a_n=cb_n c\in\mathbb{R} \lim \sup (a_n+b_n)=c+1 \lim\sup(a_n)+\lim\sup(b_n)=c+1 a_n=(0,1,0,1,0,....) b_n-(0,0,0,1,0,0,0,1...) \lim \sup (a_n+b_n)=2 \lim \sup(a_n)+\lim \sup (b_n)=2 \sup(a_n)\neq \lim\sup(a_n) a_n b_n n","['real-analysis', 'sequences-and-series', 'limsup-and-liminf']"
10,Is there a way to compute $\sum_{n=0}^\infty 1/(1+n!)$?,Is there a way to compute ?,\sum_{n=0}^\infty 1/(1+n!),"Is there a way to compute the exact value of the following series? \begin{equation} \sum_{n=0}^\infty\frac{1}{1+n!} \end{equation} I know that it converges to a number less than $e$ , since $e=\sum_{n=0}^\infty 1/n!$ . I also know that the approximated value is $1.52607$ . But can I express the exact value using known constants or functions, such as $e, \pi, \Gamma$ ?","Is there a way to compute the exact value of the following series? I know that it converges to a number less than , since . I also know that the approximated value is . But can I express the exact value using known constants or functions, such as ?","\begin{equation}
\sum_{n=0}^\infty\frac{1}{1+n!}
\end{equation} e e=\sum_{n=0}^\infty 1/n! 1.52607 e, \pi, \Gamma","['real-analysis', 'sequences-and-series']"
11,Holder's Inequality for integrals (non-negative functions),Holder's Inequality for integrals (non-negative functions),,"Let's recall Young's Inequality . Statement: Let $u, v \geqslant 0$ , and $p, q \in (0, \infty)$ such that $$ uv \leqslant \frac{u^p}{p} + \frac{v^q}{q}$$ $\blacksquare~$ Problem: Let $p,q$ (Holder Conjgates) be positive real numbers satisfying $$\frac{1}{p} + \frac{1}{q} =1 $$ Then prove the following If $f,g$ are Riemann integrable non-negative functions, then $$ \int_a^b fg~ \mathrm{d}x  \leqslant \left\{\int_a^b f^p ~\mathrm{d}x\right\}^{\frac{1}{p}} \left\{\int_a^b g^q ~\mathrm{d}x\right \}^{\frac{1}{q}} $$ $\blacksquare~$ Solution: The problem is trivial (equality holds) when the value of both integrals is $0$ . Then let's consider the first case (reduced) as $\bullet~$ Case $1$ : If $$ \int_a^b f^p ~\mathrm{d}x = \int_a^b g^q ~\mathrm{d}x = 1 \implies \left\{\int_a^b f^p ~\mathrm{d}x \right\}^\frac{1}{p} \left\{ \int_a^b g^q ~\mathrm{d}x \right\}^\frac{1}{q} = 1  $$ then from Young's Inequality we have that \begin{equation*}     \begin{split}         fg &\leqslant \frac{f^p}{p} + \frac{g^q}{q}\\       \implies   \int_a^b fg ~\mathrm{d}x &\leqslant \frac{1}{p} \int_a^b f^p~\mathrm{d}x + \frac{1}{q} \int_a^b g^q~\mathrm{d}x\\       \implies \int_a^b fg~\mathrm{d}x &\leqslant \frac{1}{p} + \frac{1}{q} = 1 = \left\{\int_a^b f^p ~\mathrm{d}x \right\}^\frac{1}{p} \left\{ \int_a^b g^q ~\mathrm{d}x \right\}^\frac{1}{q}     \end{split} \end{equation*} Hence, we have established the inequality for $\textbf{Case 1.}$ $\bullet~$ Case $2$ : The general case, i.e., $$ \int_a^b f^p \mathrm{d}x ~\text{ and }~ \int_a^b g^q \mathrm{d}x \neq 1$$ Then let's assume that \begin{align} \label{equation 4}     \int_a^b f^p \mathrm{d}x = \alpha^p ~\text{ and }~ \int_a^b g^q \mathrm{d}x = \beta^q \quad \text{for }~\alpha, \beta \in \mathbb{R}^{+}  \end{align} Thus we can easily see that \begin{align} \label{general case}     \int_a^b \left(\frac{f}{\alpha}\right)^p \mathrm{d}x = 1 ~\text{ and }~ \int_a^b \left(\frac{g}{\beta}\right)^q \mathrm{d}x = 1 \quad \text{for }~\alpha, \beta \in \mathbb{R}^{+} \end{align} Then from Young's Inequality we have that \begin{equation*}     \begin{split}         \left( \frac{fg}{\alpha \beta} \right) &\leqslant \left( \frac{f^p}{p \cdot \alpha^p} \right) + \left( \frac{g^q}{q \cdot \beta^q} \right)\\       \implies   \int_a^b \left( \frac{fg}{\alpha \beta} \right) ~\mathrm{d}x &\leqslant \frac{1}{p} \int_a^b \left(\frac{f}{\alpha}\right)^p~\mathrm{d}x + \frac{1}{q} \int_a^b \left( \frac{g}{\beta} \right)^q~\mathrm{d}x  \\       \implies \frac{1}{\alpha \beta} \int_a^b fg~\mathrm{d}x &\leqslant \frac{1}{p} + \frac{1}{q} = 1 \quad  \\       \implies \int_a^b fg ~\mathrm{d}x &\leqslant \alpha \beta = (\alpha^p)^{\frac{1}{p}} \cdot (\beta^q)^\frac{1}{q} \\       \implies \int_a^b fg ~\mathrm{d}x &\leqslant \left\{\int_a^b f^p ~\mathrm{d}x \right\}^\frac{1}{p} \left\{ \int_a^b g^q ~\mathrm{d}x \right\}^\frac{1}{q} \quad [\text{From our construction}]     \end{split} \end{equation*} Thus, we have proved $\textbf{Case 2.}$ The equality holds when $\beta^q \cdot f^p = \alpha^p \cdot g^q$ . Hence we are done! Please check for glitches and if it's fine, it'll be great if I get another way of solution. Thanks in Advance Guys! :)","Let's recall Young's Inequality . Statement: Let , and such that Problem: Let (Holder Conjgates) be positive real numbers satisfying Then prove the following If are Riemann integrable non-negative functions, then Solution: The problem is trivial (equality holds) when the value of both integrals is . Then let's consider the first case (reduced) as Case : If then from Young's Inequality we have that Hence, we have established the inequality for Case : The general case, i.e., Then let's assume that Thus we can easily see that Then from Young's Inequality we have that Thus, we have proved The equality holds when . Hence we are done! Please check for glitches and if it's fine, it'll be great if I get another way of solution. Thanks in Advance Guys! :)","u, v \geqslant 0 p, q \in (0, \infty)  uv \leqslant \frac{u^p}{p} + \frac{v^q}{q} \blacksquare~ p,q \frac{1}{p} + \frac{1}{q} =1  f,g 
\int_a^b fg~ \mathrm{d}x  \leqslant \left\{\int_a^b f^p ~\mathrm{d}x\right\}^{\frac{1}{p}} \left\{\int_a^b g^q ~\mathrm{d}x\right
\}^{\frac{1}{q}}
 \blacksquare~ 0 \bullet~ 1  \int_a^b f^p ~\mathrm{d}x = \int_a^b g^q ~\mathrm{d}x = 1 \implies \left\{\int_a^b f^p ~\mathrm{d}x \right\}^\frac{1}{p} \left\{ \int_a^b g^q ~\mathrm{d}x \right\}^\frac{1}{q} = 1   \begin{equation*}
    \begin{split}
        fg &\leqslant \frac{f^p}{p} + \frac{g^q}{q}\\
      \implies   \int_a^b fg ~\mathrm{d}x &\leqslant \frac{1}{p} \int_a^b f^p~\mathrm{d}x + \frac{1}{q} \int_a^b g^q~\mathrm{d}x\\
      \implies \int_a^b fg~\mathrm{d}x &\leqslant \frac{1}{p} + \frac{1}{q} = 1 = \left\{\int_a^b f^p ~\mathrm{d}x \right\}^\frac{1}{p} \left\{ \int_a^b g^q ~\mathrm{d}x \right\}^\frac{1}{q}
    \end{split}
\end{equation*} \textbf{Case 1.} \bullet~ 2  \int_a^b f^p \mathrm{d}x ~\text{ and }~ \int_a^b g^q \mathrm{d}x \neq 1 \begin{align}
\label{equation 4}
    \int_a^b f^p \mathrm{d}x = \alpha^p ~\text{ and }~ \int_a^b g^q \mathrm{d}x = \beta^q \quad \text{for }~\alpha, \beta \in \mathbb{R}^{+} 
\end{align} \begin{align}
\label{general case}
    \int_a^b \left(\frac{f}{\alpha}\right)^p \mathrm{d}x = 1 ~\text{ and }~ \int_a^b \left(\frac{g}{\beta}\right)^q \mathrm{d}x = 1 \quad \text{for }~\alpha, \beta \in \mathbb{R}^{+}
\end{align} \begin{equation*}
    \begin{split}
        \left( \frac{fg}{\alpha \beta} \right) &\leqslant \left( \frac{f^p}{p \cdot \alpha^p} \right) + \left( \frac{g^q}{q \cdot \beta^q} \right)\\
      \implies   \int_a^b \left( \frac{fg}{\alpha \beta} \right) ~\mathrm{d}x &\leqslant \frac{1}{p} \int_a^b \left(\frac{f}{\alpha}\right)^p~\mathrm{d}x + \frac{1}{q} \int_a^b \left( \frac{g}{\beta} \right)^q~\mathrm{d}x  \\
      \implies \frac{1}{\alpha \beta} \int_a^b fg~\mathrm{d}x &\leqslant \frac{1}{p} + \frac{1}{q} = 1 \quad  \\
      \implies \int_a^b fg ~\mathrm{d}x &\leqslant \alpha \beta = (\alpha^p)^{\frac{1}{p}} \cdot (\beta^q)^\frac{1}{q} \\
      \implies \int_a^b fg ~\mathrm{d}x &\leqslant \left\{\int_a^b f^p ~\mathrm{d}x \right\}^\frac{1}{p} \left\{ \int_a^b g^q ~\mathrm{d}x \right\}^\frac{1}{q} \quad [\text{From our construction}]
    \end{split}
\end{equation*} \textbf{Case 2.} \beta^q \cdot f^p = \alpha^p \cdot g^q","['real-analysis', 'definite-integrals', 'solution-verification', 'integral-inequality', 'holder-inequality']"
12,Combining Cayley transform and Fourier series,Combining Cayley transform and Fourier series,,"If one has a function $F(x)$ defined on the real line ( $x \in \mathbb{R}$ ) then one can study it by means of its Fourier transform. Because $\mathbb{R}$ is not compact one has a Fourier integral rather than a Fourier series (assuming $F$ is sufficiently nice that it can be expressed as such). However $\mathbb{R}$ can be mapped by the Cayley transform to the unit circle $C: x \mapsto \frac{i-x}{i+x}$ and so composing $F$ with the Cayley transform one can define $F$ on the unit circle by $F \circ C^{-1}$ . One then can compute Fourier coefficients $$a_n = \frac{1}{2\pi} \int_{\mathbb{T}} F \circ C^{-1}(e^{i\theta}) e^{-in\theta} \, d\theta = \frac{1}{\pi} \int_{\mathbb{R}} F(x) \left( \frac{i+x}{i-x} \right)^n \frac{1}{1+x^2} \, dx $$ $F \circ C^{-1}$ is of course not defined at $-1$ but $\{ -1 \}$ is a set of measure zero and so we could let $F \circ C^{-1}$ take some arbitrary value at this point. If $F$ is continuously differentiable, and if $F(\infty) = F(-\infty)$ and $F^\prime(\infty) = F^\prime(-\infty)$ then it would seem the Fourier series of $F \circ C^{-1}$ converges pointwise uniformly on the circle and so the series $$\sum_{n \in \mathbb{Z}} a_n \left( \frac{i-x}{i+x} \right)^n$$ should converge pointwise uniformly to $F(x)$ on $\mathbb{R}$ . Is this analysis correct and has this approach ever been studied before?","If one has a function defined on the real line ( ) then one can study it by means of its Fourier transform. Because is not compact one has a Fourier integral rather than a Fourier series (assuming is sufficiently nice that it can be expressed as such). However can be mapped by the Cayley transform to the unit circle and so composing with the Cayley transform one can define on the unit circle by . One then can compute Fourier coefficients is of course not defined at but is a set of measure zero and so we could let take some arbitrary value at this point. If is continuously differentiable, and if and then it would seem the Fourier series of converges pointwise uniformly on the circle and so the series should converge pointwise uniformly to on . Is this analysis correct and has this approach ever been studied before?","F(x) x \in \mathbb{R} \mathbb{R} F \mathbb{R} C: x \mapsto \frac{i-x}{i+x} F F F \circ C^{-1} a_n = \frac{1}{2\pi} \int_{\mathbb{T}} F \circ C^{-1}(e^{i\theta}) e^{-in\theta} \, d\theta = \frac{1}{\pi} \int_{\mathbb{R}} F(x) \left( \frac{i+x}{i-x} \right)^n \frac{1}{1+x^2} \, dx  F \circ C^{-1} -1 \{ -1 \} F \circ C^{-1} F F(\infty) = F(-\infty) F^\prime(\infty) = F^\prime(-\infty) F \circ C^{-1} \sum_{n \in \mathbb{Z}} a_n \left( \frac{i-x}{i+x} \right)^n F(x) \mathbb{R}","['real-analysis', 'fourier-analysis', 'fourier-series']"
13,Is this “limit” of a sequence of $L^2$ functions in $L^2$?,Is this “limit” of a sequence of  functions in ?,L^2 L^2,"Suppose we have a sequence $\{f_n\}$ in $L^2([0,1])$ and a Lebesgue measurable $f$ such that $$\int_E f_ndx\rightarrow\int_E fdx$$ as $n\rightarrow\infty$ for every Lebesgue measurable subset $E\subseteq[0,1]$ . If $\sup_n\|f_n\|_2<\infty$ , then do we necessarily have $f\in L^2([0,1])$ ? I’m not seeing how to show this, one way or the other. I tried constructing a counterexample along the lines of $f_n(x)=x^{\frac{1}{n}-\frac{1}{2}}$ , since then $f(x)=x^{-1/2}$ isn’t square-integrable, but this doesn’t satisfy either condition. I’m inclined to think that we do have $f\in L^2([0,1])$ , since it doesn’t seem like you can make $\int_{[0,1]}|f|^2dx$ blow up without either of $\int_{[0,1]}|f_n|^2dx$ or $\int f dx$ blowing up, but I don’t see a way to formalize or justify this instinct.","Suppose we have a sequence in and a Lebesgue measurable such that as for every Lebesgue measurable subset . If , then do we necessarily have ? I’m not seeing how to show this, one way or the other. I tried constructing a counterexample along the lines of , since then isn’t square-integrable, but this doesn’t satisfy either condition. I’m inclined to think that we do have , since it doesn’t seem like you can make blow up without either of or blowing up, but I don’t see a way to formalize or justify this instinct.","\{f_n\} L^2([0,1]) f \int_E f_ndx\rightarrow\int_E fdx n\rightarrow\infty E\subseteq[0,1] \sup_n\|f_n\|_2<\infty f\in L^2([0,1]) f_n(x)=x^{\frac{1}{n}-\frac{1}{2}} f(x)=x^{-1/2} f\in L^2([0,1]) \int_{[0,1]}|f|^2dx \int_{[0,1]}|f_n|^2dx \int f dx","['real-analysis', 'functional-analysis', 'lebesgue-measure']"
14,"Divergence of $\sum_{n=1}^{\infty}\prod_{k=1}^n q_k$ for some enumeration $(q_n)_{n}$ of $\mathbb{Q}\cap (0,1)$",Divergence of  for some enumeration  of,"\sum_{n=1}^{\infty}\prod_{k=1}^n q_k (q_n)_{n} \mathbb{Q}\cap (0,1)","Given an enumeration $(q_n)_{n}$ of $\mathbb{Q}\cap (0,1)$ , let us consider the series $$\sum_{n=1}^{\infty}\prod_{k=1}^n q_k.$$ Find an enumeration such that the series is convergent. Find an enumeration such that the series is divergent. is rather easy: take any enumeration $(a_n)_{n}$ of $\mathbb{Q}\cap (0,1/2]$ and any enumeration $(b_n)_{n}$ of $\mathbb{Q}\cap (1/2,1)$ . By letting $(q_n)_n=a_1,b_1,a_2,b_2,\dots$ , it follows that $$\sum_{n=1}^{\infty}\prod_{k=1}^n q_k< 2\sum_{n=1}^{\infty}\frac{1}{2^n}=2.$$ seems to be more challenging and I did not solve it so far. Any hints? I read about this problem a few years ago, but I can't remember the source. If someone find it please let me know! Bonus question 1'. Is there any enumeration $(q_n)_{n}$ of $\mathbb{Q}^+$ such that the series is convergent?","Given an enumeration of , let us consider the series Find an enumeration such that the series is convergent. Find an enumeration such that the series is divergent. is rather easy: take any enumeration of and any enumeration of . By letting , it follows that seems to be more challenging and I did not solve it so far. Any hints? I read about this problem a few years ago, but I can't remember the source. If someone find it please let me know! Bonus question 1'. Is there any enumeration of such that the series is convergent?","(q_n)_{n} \mathbb{Q}\cap (0,1) \sum_{n=1}^{\infty}\prod_{k=1}^n q_k. (a_n)_{n} \mathbb{Q}\cap (0,1/2] (b_n)_{n} \mathbb{Q}\cap (1/2,1) (q_n)_n=a_1,b_1,a_2,b_2,\dots \sum_{n=1}^{\infty}\prod_{k=1}^n q_k< 2\sum_{n=1}^{\infty}\frac{1}{2^n}=2. (q_n)_{n} \mathbb{Q}^+","['real-analysis', 'sequences-and-series']"
15,Completing the proof using strong induction for $E = \bigcap_{n=1}^\infty E_n $,Completing the proof using strong induction for,E = \bigcap_{n=1}^\infty E_n ,"I want to follow up my previous question . My original question was: Fix that $E$ is the set of real numbers $x \in [0,1]$ whose decimal expansion contains only the digits $4$ and $7$ . Let $S_n$ be the set consisting of all natural numbers not exceeding $10^n$ whose digits consists only of $4$ or $7$ . For example, \begin{equation*}     \begin{split}         S_1 &= \{4, 7\} \\         S_2 &= \{44, 77, 47, 74\} \\         S_3 &= \{444, 744, 474, 447, 774, 747, 477, 777\} \\         \vdots     \end{split} \end{equation*} I want to prove that $E$ can be defined as: \begin{equation*}     E = \bigcap_{n=1}^\infty E_n, \textrm{ where } E_n = \cup_{a \in S_n} \left[\frac{a}{10^n}, \frac{a+1}{10^{n}}\right]   \end{equation*} For instance, \begin{equation*}     \begin{split}         E_1 &= [0.4, 0.5] \cup [0.7, 0.8] \\         E_2 &= [0.44, 0.45] \cup [0.77, 0.78] \cup [0.47, 0.48] \cup [0.74, 0.75] \\ E_3 &= [0.444, 0.445] \cup [0.447, 0.448] \cup [0.474, 0.475] \cup [0.477, 0.478]\\         &\cup [0.744, 0.745] \cup [0.747, 0.748] \cup [0.774, 0.775] \cup [0.777, 0.778]  \\         &\vdots     \end{split}     \end{equation*} and I had no idea how I could prove $\bigcap_{n=1}^\infty E_n \subseteq E$ . My original question also got a wonderful response, but I eventually came up with an alternative proof. Here is that proof: Let $y \in \bigcap_{n=1}^\infty E_n$ . Then, $y \in E_n$ for each $n$ which implies that $y$ is in exactly one of the closed intervals $\left[\frac{a_n}{10^n}, \frac{a_n+1}{10^{n}}\right]$ . Define the decimal expansion of $y$ as $y=0.d_1d_2d_3\ldots\;$ . First we show that $d_1$ is either $4$ or $7$ . STTC that $d_1 \notin \{4, 7\}$ . If $d_1 \in \{0, 1, 2, 3\}$ , then $y\le0.4$ . If $y<0.4$ , then $y\notin E_1$ , which is not possible. If $y=0.4$ , then $y\notin E_2$ , which is also not possible. If $d_1 \in \{5, 6\}$ , then $0.5\le y \le 0.7$ . If $0.5< y < 0.7$ , then $y\notin E_1$ , which is not possible. If $y=0.5$ or $y=0.7$ , then $y \notin E_2$ , which is not possible. If $d_1 \in \{8, 9\}$ , then $0.8 \le y< 1$ . If $0.8 <y< 1$ , then $y\notin E_1$ , which is not possible. If $y =0.8$ , then $y\notin E_2$ , which is also not possible. Thus, $d_1 \in \{4, 7\}$ . Similarly, suppose to the contrary that $d_2 \notin \{4, 7\}$ . My idea is that I want to show that if $d_2 \notin \{4, 7\}$ , then that would force that either $y \notin E_2$ or $y \notin E_3$ , which would signal a definite pattern, which is all I want (no need for a formal induction). Thus: If $d_2 \in \{0, 1, 2, 3\}$ , then $0.400 \le y \le 0.740$ . If $0.400 \le y < 0.440$ , then $y \notin E_2$ which is not possible. If $0.440\le y <0.444$ , then $y \notin E_3$ . If $0.444 \le y < \dots$ , If $d_2 \in \{5, 6\}$ , then $0.450 \le y \le 0.770$ . If $y = 0.45$ , then $y \notin E_3$ . If $0.45 < y < 0.47 $ , then $y \notin E_2$ . If $0.47 \le y < 0.474$ , then $y \notin E_3$ . If $0.474 \le y \dots$ , If $d_2 \in \{8, 9\}$ , then $0.480 \le y \le 0.80$ . I haven't developed bullet 3. for $d_2$ because I couldn't even complete the argument in the first two bullets for $d_2$ . Can someone please suggest how the argument for $d_2$ can be completed? (Again, no need for formal induction. I just want to develop an argument for $d_2$ that is similar to $d_1$ .)","I want to follow up my previous question . My original question was: Fix that is the set of real numbers whose decimal expansion contains only the digits and . Let be the set consisting of all natural numbers not exceeding whose digits consists only of or . For example, I want to prove that can be defined as: For instance, and I had no idea how I could prove . My original question also got a wonderful response, but I eventually came up with an alternative proof. Here is that proof: Let . Then, for each which implies that is in exactly one of the closed intervals . Define the decimal expansion of as . First we show that is either or . STTC that . If , then . If , then , which is not possible. If , then , which is also not possible. If , then . If , then , which is not possible. If or , then , which is not possible. If , then . If , then , which is not possible. If , then , which is also not possible. Thus, . Similarly, suppose to the contrary that . My idea is that I want to show that if , then that would force that either or , which would signal a definite pattern, which is all I want (no need for a formal induction). Thus: If , then . If , then which is not possible. If , then . If , If , then . If , then . If , then . If , then . If , If , then . I haven't developed bullet 3. for because I couldn't even complete the argument in the first two bullets for . Can someone please suggest how the argument for can be completed? (Again, no need for formal induction. I just want to develop an argument for that is similar to .)","E x \in [0,1] 4 7 S_n 10^n 4 7 \begin{equation*}
    \begin{split}
        S_1 &= \{4, 7\} \\
        S_2 &= \{44, 77, 47, 74\} \\
        S_3 &= \{444, 744, 474, 447, 774, 747, 477, 777\} \\
        \vdots
    \end{split}
\end{equation*} E \begin{equation*}
    E = \bigcap_{n=1}^\infty E_n, \textrm{ where } E_n = \cup_{a \in S_n} \left[\frac{a}{10^n}, \frac{a+1}{10^{n}}\right]  
\end{equation*} \begin{equation*}
    \begin{split}
        E_1 &= [0.4, 0.5] \cup [0.7, 0.8] \\
        E_2 &= [0.44, 0.45] \cup [0.77, 0.78] \cup [0.47, 0.48] \cup [0.74, 0.75] \\
E_3 &= [0.444, 0.445] \cup [0.447, 0.448] \cup [0.474, 0.475] \cup [0.477, 0.478]\\
        &\cup [0.744, 0.745] \cup [0.747, 0.748] \cup [0.774, 0.775] \cup [0.777, 0.778]  \\
        &\vdots
    \end{split}
    \end{equation*} \bigcap_{n=1}^\infty E_n \subseteq E y \in \bigcap_{n=1}^\infty E_n y \in E_n n y \left[\frac{a_n}{10^n}, \frac{a_n+1}{10^{n}}\right] y y=0.d_1d_2d_3\ldots\; d_1 4 7 d_1 \notin \{4, 7\} d_1 \in \{0, 1, 2, 3\} y\le0.4 y<0.4 y\notin E_1 y=0.4 y\notin E_2 d_1 \in \{5, 6\} 0.5\le y \le 0.7 0.5< y < 0.7 y\notin E_1 y=0.5 y=0.7 y \notin E_2 d_1 \in \{8, 9\} 0.8 \le y< 1 0.8 <y< 1 y\notin E_1 y =0.8 y\notin E_2 d_1 \in \{4, 7\} d_2 \notin \{4, 7\} d_2 \notin \{4, 7\} y \notin E_2 y \notin E_3 d_2 \in \{0, 1, 2, 3\} 0.400 \le y \le 0.740 0.400 \le y < 0.440 y \notin E_2 0.440\le y <0.444 y \notin E_3 0.444 \le y < \dots d_2 \in \{5, 6\} 0.450 \le y \le 0.770 y = 0.45 y \notin E_3 0.45 < y < 0.47  y \notin E_2 0.47 \le y < 0.474 y \notin E_3 0.474 \le y \dots d_2 \in \{8, 9\} 0.480 \le y \le 0.80 d_2 d_2 d_2 d_2 d_1","['real-analysis', 'general-topology', 'induction', 'alternative-proof']"
16,Proof of the change of variables formula without using the Monotone Convergence Theorem,Proof of the change of variables formula without using the Monotone Convergence Theorem,,"I recently encountered the problem Exercise 36 in Tao's An Introduction to Measure Theory . The link of an online version of this problem is here . Now I quote this problem as follows: Exercise 36 (Change of variables formula) Let $(X, \mathcal{B}, \mu)$ be a measure space, and let $\phi: X \rightarrow Y$ be a measurable morphism (as defined in Remark 8 from $(X, \mathcal{B})$ to another measurable space $(Y, \mathcal{C}). $ Define the pushforward $\phi_{*} \mu: \mathcal{C} \rightarrow[0,+\infty]$ of $\mu$ by $\phi$ by the formula $\phi_{*} \mu(E):=\mu\left(\phi^{-1}(E)\right)$ Show that $\phi_{*} \mu$ is a measure on $\mathcal{C},$ so that $\left(Y, \mathcal{C}, \phi_{*} \mu\right)$ is a measure space. If $f: Y \rightarrow[0,+\infty]$ is measurable, show that $\int_{Y} f d \phi_{*} \mu=\int_{X}(f \circ \phi) d \mu$ (Hint: the quickest proof here is via the monotone convergence theorem below, but it is also possible to prove the exercise without this theorem. ) I really eager about how to prove the second statement WITHOUT the Monotone Convergence Theorem, in order to follow the procedure of the book. I tried hard and figured out only the case that $f$ is a simple function. How can we prove the case that $f$ is a general unsigned (nonnegative) function? The author have not provide the solution yet. Any help is appreciated.","I recently encountered the problem Exercise 36 in Tao's An Introduction to Measure Theory . The link of an online version of this problem is here . Now I quote this problem as follows: Exercise 36 (Change of variables formula) Let be a measure space, and let be a measurable morphism (as defined in Remark 8 from to another measurable space Define the pushforward of by by the formula Show that is a measure on so that is a measure space. If is measurable, show that (Hint: the quickest proof here is via the monotone convergence theorem below, but it is also possible to prove the exercise without this theorem. ) I really eager about how to prove the second statement WITHOUT the Monotone Convergence Theorem, in order to follow the procedure of the book. I tried hard and figured out only the case that is a simple function. How can we prove the case that is a general unsigned (nonnegative) function? The author have not provide the solution yet. Any help is appreciated.","(X, \mathcal{B}, \mu) \phi: X \rightarrow Y (X, \mathcal{B}) (Y, \mathcal{C}).  \phi_{*} \mu: \mathcal{C} \rightarrow[0,+\infty] \mu \phi \phi_{*} \mu(E):=\mu\left(\phi^{-1}(E)\right) \phi_{*} \mu \mathcal{C}, \left(Y, \mathcal{C}, \phi_{*} \mu\right) f: Y \rightarrow[0,+\infty] \int_{Y} f d \phi_{*} \mu=\int_{X}(f \circ \phi) d \mu f f","['real-analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
17,If $a_0=1$ and $a_{n+1}=a_n +e^{-a_n}$ then does the limit of $a_n-\log{n}$ exist and if so then what is it?,If  and  then does the limit of  exist and if so then what is it?,a_0=1 a_{n+1}=a_n +e^{-a_n} a_n-\log{n},I don't  want the full answer but hints for solving this question.My idea to attempt the question so far is the following by back tracing if we calculate the limit of $\dfrac{e^{a_n}}{n}$ we will get our answer just by applying logarithm.As $\dfrac{e^{a_n}}{n}$ might be useful in the sense that we can use the given conditions of the problem and with Taylor series expression.But how should I proceed to get to $\dfrac{e^{a_n}}{n}$ . I think we need to construct a new sequence.Is my approach correct or I need to think differently? Hints required,I don't  want the full answer but hints for solving this question.My idea to attempt the question so far is the following by back tracing if we calculate the limit of we will get our answer just by applying logarithm.As might be useful in the sense that we can use the given conditions of the problem and with Taylor series expression.But how should I proceed to get to . I think we need to construct a new sequence.Is my approach correct or I need to think differently? Hints required,\dfrac{e^{a_n}}{n} \dfrac{e^{a_n}}{n} \dfrac{e^{a_n}}{n},"['real-analysis', 'sequences-and-series']"
18,"$(\frac{1}{3},1)\cup(\frac{1}{5},\frac{1}{3})\cup(\frac{1}{7},\frac{1}{5})\cup\ldots$ is not a finite union of open intervals",is not a finite union of open intervals,"(\frac{1}{3},1)\cup(\frac{1}{5},\frac{1}{3})\cup(\frac{1}{7},\frac{1}{5})\cup\ldots","Let $X=(\frac{1}{3},1)\cup(\frac{1}{5},\frac{1}{3})\cup(\frac{1}{7},\frac{1}{5})\cup\ldots \subseteq \mathbb{R}$ I have tried to show that $X$ is not a finite union of open intervals. Does the following argument make sense? Suppose, for contradiction, that $$X=(a_1,b_1)\cup\ldots\cup(a_n,b_n)$$ where we assume that $a_1\leq a_2 \leq \ldots \leq a_n.$ If $a_1<0,$ then there is a negative number in $X$ - which is a contradiction. If $a_1=0,$ then $X$ contains $\frac{1}{2m+1}$ for sufficiently large $m \in \mathbb{N}$ - which is a contradiction. If $a_1>0,$ then $X$ does not contain $\frac{1}{2m}$ for sufficiently large $m \in \mathbb{N}$ - which is a contradiction.","Let I have tried to show that is not a finite union of open intervals. Does the following argument make sense? Suppose, for contradiction, that where we assume that If then there is a negative number in - which is a contradiction. If then contains for sufficiently large - which is a contradiction. If then does not contain for sufficiently large - which is a contradiction.","X=(\frac{1}{3},1)\cup(\frac{1}{5},\frac{1}{3})\cup(\frac{1}{7},\frac{1}{5})\cup\ldots \subseteq \mathbb{R} X X=(a_1,b_1)\cup\ldots\cup(a_n,b_n) a_1\leq a_2 \leq \ldots \leq a_n. a_1<0, X a_1=0, X \frac{1}{2m+1} m \in \mathbb{N} a_1>0, X \frac{1}{2m} m \in \mathbb{N}","['real-analysis', 'general-topology']"
19,Topology of open and closed sets,Topology of open and closed sets,,"Since any open set A is basically the interior of the set A itself, and the boundary of a set is basically the end points of the set being that it could be in the set A or the complement of A, then is A-BdA = IntA and is A-IntA= BdA","Since any open set A is basically the interior of the set A itself, and the boundary of a set is basically the end points of the set being that it could be in the set A or the complement of A, then is A-BdA = IntA and is A-IntA= BdA",,"['real-analysis', 'general-topology']"
20,"$\varphi$ monotone,continuous on $[0,1]$. Show $\varphi$ is AC, iff for any borel $f_n\to f$ in $L^1$,$\{\varphi(f_n)\}$ converges in measure.","monotone,continuous on . Show  is AC, iff for any borel  in , converges in measure.","\varphi [0,1] \varphi f_n\to f L^1 \{\varphi(f_n)\}","Let $\varphi$ be monotonic and continuous on $[0,1]$ . Show it is AC, iff for any borel sequence $f_n$ that converges in $L^1$ the sequences $\varphi(f_n)$ converges in measure. The forward direction follows from the fact that $f_n \to f$ in $L^1$ implies convergence in measure and continuity preserves convergence in measure on finite measures. The backwards direction is the one that is giving me a little more trouble. Since $\varphi$ is monotonic (WLOG its increasing) all we have to show is that $\nexists x$ s. t. $\int_{0}^{x}\varphi'\lt\varphi(x)-\varphi(0)$ . However I do not really know how to do that. Any hints would be highly appreciated.","Let be monotonic and continuous on . Show it is AC, iff for any borel sequence that converges in the sequences converges in measure. The forward direction follows from the fact that in implies convergence in measure and continuity preserves convergence in measure on finite measures. The backwards direction is the one that is giving me a little more trouble. Since is monotonic (WLOG its increasing) all we have to show is that s. t. . However I do not really know how to do that. Any hints would be highly appreciated.","\varphi [0,1] f_n L^1 \varphi(f_n) f_n \to f L^1 \varphi \nexists x \int_{0}^{x}\varphi'\lt\varphi(x)-\varphi(0)","['real-analysis', 'measure-theory', 'lebesgue-integral']"
21,What is the size of the marked area (calculate without computer)?,What is the size of the marked area (calculate without computer)?,,"Recently I scribbled a representation of squares which should challenge the imagination of the infiniteness of the sum $$s_1=1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+...$$ and of the finiteness of the sum $$s_2=1+\frac{1}{2^2}+\frac{1}{3^2}+\frac{1}{4^2}+...$$ The following picture puts together the infinite length of a line constructed of the partial series of $s_1$ (using the length unit ""cm"" for example) and the finite area constructed of the partial series of $s_2$ (in ""cm²"") which is enclosed by a border of infinite length: Ok, I mean this picture gives a nice composition (of a finite area limited by an infinite line) for some interesting amateur. After that, I thought: what if I smooth that figure by connecting the edges of the squares with straight lines? What is the size of the newly introduced area (yellow)? Is a purely geometrical solution possible? Challenge1: find a formula for the yellow area without the use of a computer. I think I needed about an hour to find the (very nice) solution with pen&paper (feeling a bit rusty needing such a long time... ). Challenge2: If the edges are connected with a smooth interpolating curve instead of straight pieces - what could be a formula with the target of simpleness and of straightness of the curve? (If I got the terminology correct, the ideal were a ""absolute monotonous"" function, see wikipedia .) As a ""poor man's approach"" I thought one might rotate the picture by 45 deg first towards the x-axis. Then the new area could be expressed by the integral for the curve's formula. Give such an integral and the area. My first approach was to ""generalize"" the formula for polynomial interpolation using the heights of the upper edges (when the diagonal of the figure is rotated to the x-axis) towards an infinite number of interpolation-points; one problem with this is that the x-coordinates of the interpolation-points are not equidistant. (I've no solution so far, possibly one can employ the $\psi()$ -function as interpolation of the harmonic numbers.)","Recently I scribbled a representation of squares which should challenge the imagination of the infiniteness of the sum and of the finiteness of the sum The following picture puts together the infinite length of a line constructed of the partial series of (using the length unit ""cm"" for example) and the finite area constructed of the partial series of (in ""cm²"") which is enclosed by a border of infinite length: Ok, I mean this picture gives a nice composition (of a finite area limited by an infinite line) for some interesting amateur. After that, I thought: what if I smooth that figure by connecting the edges of the squares with straight lines? What is the size of the newly introduced area (yellow)? Is a purely geometrical solution possible? Challenge1: find a formula for the yellow area without the use of a computer. I think I needed about an hour to find the (very nice) solution with pen&paper (feeling a bit rusty needing such a long time... ). Challenge2: If the edges are connected with a smooth interpolating curve instead of straight pieces - what could be a formula with the target of simpleness and of straightness of the curve? (If I got the terminology correct, the ideal were a ""absolute monotonous"" function, see wikipedia .) As a ""poor man's approach"" I thought one might rotate the picture by 45 deg first towards the x-axis. Then the new area could be expressed by the integral for the curve's formula. Give such an integral and the area. My first approach was to ""generalize"" the formula for polynomial interpolation using the heights of the upper edges (when the diagonal of the figure is rotated to the x-axis) towards an infinite number of interpolation-points; one problem with this is that the x-coordinates of the interpolation-points are not equidistant. (I've no solution so far, possibly one can employ the -function as interpolation of the harmonic numbers.)",s_1=1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+... s_2=1+\frac{1}{2^2}+\frac{1}{3^2}+\frac{1}{4^2}+... s_1 s_2 \psi(),"['real-analysis', 'recreational-mathematics']"
22,How to create sequence of functions,How to create sequence of functions,,"I'm sorry about this question but the more I think about this, the more I feel ignorant. How can I, by hand, create a sequence of functions $(f_h)_h$ , let's say in $L^2((01))$ , coverging, under the $\left\lVert \cdot\right\rVert_{L^1}$ norm, to $\frac{1}{\sqrt{x}}$ , for example. This specific example was meant to show that $L^2((0,1))$ is not closed in $L^1((0,1))$ , but this question would like to be way general, as I find myself always in trouble in creating explicitely sequences of functions. I'd really like to see the method for which create sequences, rather than simply read a specific answer as it won't help me grow, I think. Any solution, hint or reference would be much appreciate, thanks in advance.","I'm sorry about this question but the more I think about this, the more I feel ignorant. How can I, by hand, create a sequence of functions , let's say in , coverging, under the norm, to , for example. This specific example was meant to show that is not closed in , but this question would like to be way general, as I find myself always in trouble in creating explicitely sequences of functions. I'd really like to see the method for which create sequences, rather than simply read a specific answer as it won't help me grow, I think. Any solution, hint or reference would be much appreciate, thanks in advance.","(f_h)_h L^2((01)) \left\lVert \cdot\right\rVert_{L^1} \frac{1}{\sqrt{x}} L^2((0,1)) L^1((0,1))",['real-analysis']
23,Is there a closed form for the polygamma function?,Is there a closed form for the polygamma function?,,"Mathematica gave me that $$ \sum_{k=n}^\infty \frac1{k^2} = \texttt{PolyGamma[1,n]}. $$ However, in all my attempts to simplify and approximate the number as a decimal, it kept leaving it in terms of the ""PolyGamma"" function. I understand that \begin{align} \texttt{PolyGamma[1,1]} &= \psi^{(1)}(1) \\ &= (-1)^2 \int_0^\infty \frac{te^{-t}}{1-e^{-t}}\ \mathsf dt\\ &= \frac{\pi^2}6, \end{align} and in general \begin{align} \texttt{PolyGamma[1,n]} &= \psi^{(1)}(n)\\ &=  \int_0^\infty \frac{te^{-nt}}{1-e^{-t}}\ \mathsf dt\\ &= -\int_0^1 \frac{t^{n-1}}{1-t}\log t\ \mathsf dt. \end{align} Is this just not an integral that cannot be solved analytically? Wolfram Alpha gives me the series expansion $$ \frac1{n^2} + \frac{\pi^2}6 + n \psi^{(2)}(1) + \frac{\pi^4n^2}{30} + O(n^3), $$ which is fairly useless considering $n$ as a continuous variable, as this is the series expansion about $0$ . If this integral cannot be computed analytically, then what is a good approximation for $\texttt{PolyGamma[1,n]}$ as a function of $n$ ? I see that it can be computed exactly for any fixed $n$ . But I am interested in the map $n\mapsto \psi^{(1)}(n)$ . Edit: From Wikipedia it appears that an easy way to approximate the trigamma function is to take the derivative of the series expansion of the digamma function: $$ \psi^{(1)}(n) = \frac1n + \frac1{2x^3} + \frac1{6x^3} - \frac1{30x^5} + \frac1{42x^7} - \frac1{30x^9} + \frac5{66x^{11}} - \frac{691}{2730x^{13}} + \frac7{6x^{15}} + O(n^{17}) $$ However I see no clear pattern as to the coefficients in this Laurent series.","Mathematica gave me that However, in all my attempts to simplify and approximate the number as a decimal, it kept leaving it in terms of the ""PolyGamma"" function. I understand that and in general Is this just not an integral that cannot be solved analytically? Wolfram Alpha gives me the series expansion which is fairly useless considering as a continuous variable, as this is the series expansion about . If this integral cannot be computed analytically, then what is a good approximation for as a function of ? I see that it can be computed exactly for any fixed . But I am interested in the map . Edit: From Wikipedia it appears that an easy way to approximate the trigamma function is to take the derivative of the series expansion of the digamma function: However I see no clear pattern as to the coefficients in this Laurent series.","
\sum_{k=n}^\infty \frac1{k^2} = \texttt{PolyGamma[1,n]}.
 \begin{align}
\texttt{PolyGamma[1,1]} &= \psi^{(1)}(1) \\
&= (-1)^2 \int_0^\infty \frac{te^{-t}}{1-e^{-t}}\ \mathsf dt\\
&= \frac{\pi^2}6,
\end{align} \begin{align}
\texttt{PolyGamma[1,n]} &= \psi^{(1)}(n)\\ &=  \int_0^\infty \frac{te^{-nt}}{1-e^{-t}}\ \mathsf dt\\ &= -\int_0^1 \frac{t^{n-1}}{1-t}\log t\ \mathsf dt.
\end{align} 
\frac1{n^2} + \frac{\pi^2}6 + n \psi^{(2)}(1) + \frac{\pi^4n^2}{30} + O(n^3),
 n 0 \texttt{PolyGamma[1,n]} n n n\mapsto \psi^{(1)}(n) 
\psi^{(1)}(n) = \frac1n + \frac1{2x^3} + \frac1{6x^3} - \frac1{30x^5} + \frac1{42x^7} - \frac1{30x^9} + \frac5{66x^{11}} - \frac{691}{2730x^{13}} + \frac7{6x^{15}} + O(n^{17})
","['real-analysis', 'closed-form', 'digamma-function']"
24,"Computing $\|f\|$ when $f(x,0,z,0,0,...)=2x-z$.",Computing  when .,"\|f\| f(x,0,z,0,0,...)=2x-z","Let $1\leq p<\infty$ and let $B$ be the subspace of $(l_{p}(\mathbb{N}),\|\cdot\|_{p})$ given by $B=\{(x,0,z,0,0,...) : x,y \in \mathbb{C}\}$ . Let $f : B \to \mathbb{C}$ be given by $f(x,0,z,0,0,...)=2x-z$ . I'd like to compute $\|f\|$ . My attempt: Assume $B$ is finite-dimensional. Write $\textbf{x}=(x,0,z,0,0,...,0)$ . $\|f(\textbf{x})\| =|2x-z|\leq 2|x|+|z| \leq 2\|\textbf{x}\|_{1} \leq 2C\|\textbf{x}\|_{p}$ for some $C>0$ , since any two norms are equivalent, when $B$ is finite-dimensional. Also, by Hölder's inequality, $\|\textbf{x}\|_{1}\leq 2^{1-1/p}\|\textbf{x}\|_{p}$ (is happy to provide the details if needed). Hence, $\|f(\textbf{x})\|\leq 2\|\textbf{x}\|_{1}\leq 2^{2-1/p}\|\textbf{x}\|_{p}$ . Now, if I can find $\textbf{x}$ s.t. we get equality, then $\|f\|=2^{2-1/p}$ . This, however, is where I am stuck. Perhaps there is a smaller bound on $\|f(\textbf{x})\|$ that I have missed?","Let and let be the subspace of given by . Let be given by . I'd like to compute . My attempt: Assume is finite-dimensional. Write . for some , since any two norms are equivalent, when is finite-dimensional. Also, by Hölder's inequality, (is happy to provide the details if needed). Hence, . Now, if I can find s.t. we get equality, then . This, however, is where I am stuck. Perhaps there is a smaller bound on that I have missed?","1\leq p<\infty B (l_{p}(\mathbb{N}),\|\cdot\|_{p}) B=\{(x,0,z,0,0,...) : x,y \in \mathbb{C}\} f : B \to \mathbb{C} f(x,0,z,0,0,...)=2x-z \|f\| B \textbf{x}=(x,0,z,0,0,...,0) \|f(\textbf{x})\| =|2x-z|\leq 2|x|+|z| \leq 2\|\textbf{x}\|_{1} \leq 2C\|\textbf{x}\|_{p} C>0 B \|\textbf{x}\|_{1}\leq 2^{1-1/p}\|\textbf{x}\|_{p} \|f(\textbf{x})\|\leq 2\|\textbf{x}\|_{1}\leq 2^{2-1/p}\|\textbf{x}\|_{p} \textbf{x} \|f\|=2^{2-1/p} \|f(\textbf{x})\|","['real-analysis', 'functional-analysis', 'banach-spaces', 'normed-spaces', 'dual-spaces']"
25,"$| \int fg|\leq 1$ for any $g\in C_{0}^{\infty}$, $\|g\|_{L^{2}}=1$ then $\|f\|_{L^{2}}\leq1$","for any ,  then",| \int fg|\leq 1 g\in C_{0}^{\infty} \|g\|_{L^{2}}=1 \|f\|_{L^{2}}\leq1,"I have the following question: Let $f$ be a continuous function in an open, bounded, smooth domain $\Omega$ in $\mathbb{R}^{n}$ such that $|  \int_{\Omega} fg|\leq 1$ for any $g\in C_{0}^{\infty}(\Omega)$ , $\|g\|_{L^{2}(\Omega)}=1$ . Here the measure and the integral are w.r.t. Lebesgue measure.  And $C_{0}^{\infty}(\Omega)$ is the set of smooth functions of compact support in $\Omega$ . Can we conclude that $f\in L^{2}(\Omega)$ ? Thanks for any hint.","I have the following question: Let be a continuous function in an open, bounded, smooth domain in such that for any , . Here the measure and the integral are w.r.t. Lebesgue measure.  And is the set of smooth functions of compact support in . Can we conclude that ? Thanks for any hint.",f \Omega \mathbb{R}^{n} |  \int_{\Omega} fg|\leq 1 g\in C_{0}^{\infty}(\Omega) \|g\|_{L^{2}(\Omega)}=1 C_{0}^{\infty}(\Omega) \Omega f\in L^{2}(\Omega),"['real-analysis', 'lebesgue-integral', 'lebesgue-measure']"
26,Differentiability of $\sum_{k=0}^{\infty}c_{k}e^{ikt}$ given $\lim_{k\to\infty}k^{m}c_{k}=0$,Differentiability of  given,\sum_{k=0}^{\infty}c_{k}e^{ikt} \lim_{k\to\infty}k^{m}c_{k}=0,"Consider a Fourier Series $$S(t) = \sum_{k=0}^{\infty}c_{k}e^{ikt}$$ where $c_{k}$ are complex coefficients such that the sum $\sum |c_{k}|$ is finite. I am also given that $\lim_{k\to\infty}k^{m}c_{k}=0$ for some fixed $m>0$ . Question: What can we say about the differentiability of $S$ ? What I tried: If I can prove that $\sum k|c_{k}|<\infty$ , then the sequence of derivatives of the partial sums $$f_{n}'(t)=\sum_{k=0}^{n}ikc_{k}e^{ikt}$$ must converge uniformly to a continuous limit, then $S$ would be differentiable. However, I am not sure how to apply the fact that $\lim_{k\to\infty}k^{m}c_{k}=0$ for some fixed $m>0$ - certainly $\sum k|c_{k}|<\infty$ implies the terms should go to 0 but how do I know the converse is true?","Consider a Fourier Series where are complex coefficients such that the sum is finite. I am also given that for some fixed . Question: What can we say about the differentiability of ? What I tried: If I can prove that , then the sequence of derivatives of the partial sums must converge uniformly to a continuous limit, then would be differentiable. However, I am not sure how to apply the fact that for some fixed - certainly implies the terms should go to 0 but how do I know the converse is true?",S(t) = \sum_{k=0}^{\infty}c_{k}e^{ikt} c_{k} \sum |c_{k}| \lim_{k\to\infty}k^{m}c_{k}=0 m>0 S \sum k|c_{k}|<\infty f_{n}'(t)=\sum_{k=0}^{n}ikc_{k}e^{ikt} S \lim_{k\to\infty}k^{m}c_{k}=0 m>0 \sum k|c_{k}|<\infty,"['real-analysis', 'fourier-series', 'uniform-convergence']"
27,"Find the value of $\sup \int_a^b \max\{f,g\}$",Find the value of,"\sup \int_a^b \max\{f,g\}","Assume $a$ and $b$ are real numbers such that $0<a<b$ , and let $\textsf{K}_{a,b}$ the set of all non-negative, monotonically decreasing functions $f$ satisfying $$\int_a^b f(t) dt=1$$ and that $af(a)=bf(b)$ on the interval $[a,b]$ . Find the value of $$\sup \left\{ \int_a^b\max\{f(t),g(t)\}dt \, : \, f,g\in \textsf{K}_{a,b} \right\}$$ Em, I basically have no idea. How to deal with the $\max$ function? Using inequality? I don't know... Somehow it looks like a variational problem? Everything is thankful.","Assume and are real numbers such that , and let the set of all non-negative, monotonically decreasing functions satisfying and that on the interval . Find the value of Em, I basically have no idea. How to deal with the function? Using inequality? I don't know... Somehow it looks like a variational problem? Everything is thankful.","a b 0<a<b \textsf{K}_{a,b} f \int_a^b f(t) dt=1 af(a)=bf(b) [a,b] \sup \left\{ \int_a^b\max\{f(t),g(t)\}dt \, : \, f,g\in \textsf{K}_{a,b} \right\} \max","['real-analysis', 'calculus', 'integration', 'functions']"
28,"Find $\frac{\partial^2 f}{\partial x \partial y}(0, 0)$ given that $\lim_{(x, y) \to (0,0)} \frac{f(x, y) - \tan{(x)}\sin{(y)}}{x^2 + y^2} = 0$.",Find  given that .,"\frac{\partial^2 f}{\partial x \partial y}(0, 0) \lim_{(x, y) \to (0,0)} \frac{f(x, y) - \tan{(x)}\sin{(y)}}{x^2 + y^2} = 0","$f \in C^2(\mathbb{R^2})$ satisfies $$\lim_{(x, y) \to (0, 0)} \frac{f(x, y) - \tan{(x)}\sin{(y)}}{x^2 + y^2} = 0.$$ Find $\frac{\partial^2 f}{\partial x \partial y}(0, 0).$ I've tried to deduce something from the limit and definition of partial derivatives but with no effect.",satisfies Find I've tried to deduce something from the limit and definition of partial derivatives but with no effect.,"f \in C^2(\mathbb{R^2}) \lim_{(x, y) \to (0, 0)} \frac{f(x, y) - \tan{(x)}\sin{(y)}}{x^2 + y^2} = 0. \frac{\partial^2 f}{\partial x \partial y}(0, 0).","['real-analysis', 'limits', 'multivariable-calculus', 'partial-derivative']"
29,Does there exist an $n \in \mathbb{N}$ such that the improper Riemann integral $\int_0^{\infty}|\sin(x^{n})|$ converges?,Does there exist an  such that the improper Riemann integral  converges?,n \in \mathbb{N} \int_0^{\infty}|\sin(x^{n})|,I know that $\int_0^{\infty}\sin(x^{\alpha})$ converges for $|\alpha| \geq 1$ which can be attributed to cancellation of areas and $\int_0^{\infty}|\sin(x^{n})|$ diverges for $n = 2$ since there is no cancellation of areas. It seems unlikely but is there a $n$ such that the area of the oscillations can be made so small that it can be approximated by a geometric series and hence show that this integral converges?,I know that converges for which can be attributed to cancellation of areas and diverges for since there is no cancellation of areas. It seems unlikely but is there a such that the area of the oscillations can be made so small that it can be approximated by a geometric series and hence show that this integral converges?,\int_0^{\infty}\sin(x^{\alpha}) |\alpha| \geq 1 \int_0^{\infty}|\sin(x^{n})| n = 2 n,"['real-analysis', 'calculus']"
30,Evaluating sum $\sum_{m=0}^{\infty}\frac{(2-\delta_m^0)(-1)^m \lambda_0}{a(\lambda_0^2 -(\frac{m\pi}{a}))}\cos(m\pi x/a)$,Evaluating sum,\sum_{m=0}^{\infty}\frac{(2-\delta_m^0)(-1)^m \lambda_0}{a(\lambda_0^2 -(\frac{m\pi}{a}))}\cos(m\pi x/a),How Can I evaluate the following sum $$\sum_{m=0}^{\infty}\frac{2-\delta_m^0}{a}\frac{(-1)^m \lambda_0}{\lambda_0^2 -(\frac{m\pi}{a})}\cos\left(\frac{m\pi x}{a}\right)=\frac{\cos(\lambda_0 x)}{\sin(\lambda_0 a)}$$ I have read this in a research paper I have tried evaluating the sum using finite cosine transform  We have $$\frac{2}{a}\frac{1}{\lambda_0}+\frac{2}{a}\sum_{m=0}^{\infty}\frac{(-1)^m \lambda_0}{\lambda_0^2 -(\frac{m\pi}{a})}\cos\left(\frac{m\pi x}{a}\right)=f(x)$$ So $$\frac{(-1)^m \lambda_0}{\lambda_0^2 -(\frac{m\pi}{a})}=\int_{0}^{a} f(x)\cos\left(\frac{m\pi x}{a}\right) dx $$ How to find $f(x)$ ? And Is there any other way to evaluate the sum ? Thanks In advance.,How Can I evaluate the following sum I have read this in a research paper I have tried evaluating the sum using finite cosine transform  We have So How to find ? And Is there any other way to evaluate the sum ? Thanks In advance.,\sum_{m=0}^{\infty}\frac{2-\delta_m^0}{a}\frac{(-1)^m \lambda_0}{\lambda_0^2 -(\frac{m\pi}{a})}\cos\left(\frac{m\pi x}{a}\right)=\frac{\cos(\lambda_0 x)}{\sin(\lambda_0 a)} \frac{2}{a}\frac{1}{\lambda_0}+\frac{2}{a}\sum_{m=0}^{\infty}\frac{(-1)^m \lambda_0}{\lambda_0^2 -(\frac{m\pi}{a})}\cos\left(\frac{m\pi x}{a}\right)=f(x) \frac{(-1)^m \lambda_0}{\lambda_0^2 -(\frac{m\pi}{a})}=\int_{0}^{a} f(x)\cos\left(\frac{m\pi x}{a}\right) dx  f(x),"['real-analysis', 'sequences-and-series', 'definite-integrals', 'summation', 'fourier-series']"
31,"ACL characterization of functions in $W^{1,\infty}(\Omega)$",ACL characterization of functions in,"W^{1,\infty}(\Omega)","I am reading ""A First Course in Sobolev Space"" by Leoni. Theorem 10.35 states that for $1\leq p<\infty$ , a function $u\in L^p(\Omega)$ belongs to $W^{1,p}(\Omega)$ if and only it has a representative $\bar{u}$ that is absolutely continuous on $a.e.$ line segments of $\Omega$ that are parallel to the coordinate axes. The proof in the book is not valid for $p=\infty$ . However, according to Wikipedia, this theorem also holds for $p=\infty$ . Can anyone provide a proof?","I am reading ""A First Course in Sobolev Space"" by Leoni. Theorem 10.35 states that for , a function belongs to if and only it has a representative that is absolutely continuous on line segments of that are parallel to the coordinate axes. The proof in the book is not valid for . However, according to Wikipedia, this theorem also holds for . Can anyone provide a proof?","1\leq p<\infty u\in L^p(\Omega) W^{1,p}(\Omega) \bar{u} a.e. \Omega p=\infty p=\infty","['real-analysis', 'functional-analysis', 'partial-differential-equations']"
32,Difficulty in understanding corollary of Archimedean property.,Difficulty in understanding corollary of Archimedean property.,,"Question prove that $$\forall x \in R              \exists n\in Z $$ such that $$n \leq x < n+1$$ (such $n$ is unique) My Attempt The prove given in my book(see the corollary 5) is very brief. I did not understand what it has to do with Archimedean property. As Archimedean property is established upon completeness I would rather try to prove it from that. Consider $A=\{m|m<x,\forall m\in R\}$ Now a is bounded hence sup exists(say $\alpha$ . So $\forall \epsilon>0 \exists a\in A$ such that $$\alpha-\epsilon < a \leq \alpha $$ But taking $\epsilon=1$ does not really prove the required statement. We have to show that $\alpha$ and $\alpha-1$ to be integers. So this approach failed. My book's proof Proof consists of two parts 1)To show $x \geq n$ 2) To show $n+1>x$ It proves (1) with constructing a set $$\{m:m<x,\forall m\in Z\}$$ and says that it is bounded above so it has suprema say $n,n\in Z$ thus $x\geq n$ . 2) To prove (2) it does not give any explanation. I think it has been done using Archimedean property ie. $\forall x\in R \exists m\in Z$ such that $m>x$ But my question is how to prove $m=n+1$ ? I need following help 1) Please can you write the second part of proof given in my book in a detailed or explanatory manner ? 2) I want to know whether my approach was reasonable or not.",Question prove that such that (such is unique) My Attempt The prove given in my book(see the corollary 5) is very brief. I did not understand what it has to do with Archimedean property. As Archimedean property is established upon completeness I would rather try to prove it from that. Consider Now a is bounded hence sup exists(say . So such that But taking does not really prove the required statement. We have to show that and to be integers. So this approach failed. My book's proof Proof consists of two parts 1)To show 2) To show It proves (1) with constructing a set and says that it is bounded above so it has suprema say thus . 2) To prove (2) it does not give any explanation. I think it has been done using Archimedean property ie. such that But my question is how to prove ? I need following help 1) Please can you write the second part of proof given in my book in a detailed or explanatory manner ? 2) I want to know whether my approach was reasonable or not.,"\forall x \in R              \exists n\in Z  n \leq x < n+1 n A=\{m|m<x,\forall m\in R\} \alpha \forall \epsilon>0 \exists a\in A \alpha-\epsilon < a \leq \alpha  \epsilon=1 \alpha \alpha-1 x \geq n n+1>x \{m:m<x,\forall m\in Z\} n,n\in Z x\geq n \forall x\in R \exists m\in Z m>x m=n+1","['real-analysis', 'elementary-set-theory']"
33,Riemann Integrability of Composite functions.,Riemann Integrability of Composite functions.,,"Function $f$ is continuous and strictly increasing on $[a,b]$ and $g$ is Riemann integrable such that $g \circ f$ is defined. Is $g \circ f$ Riemann integrable? I was able to show that if $f$ additionally satisfies: $x_{1}<x_{2} \in [a,b] \Rightarrow f(x_2)-f(x_1) \geq x_{2}-x_{1}$ , then $g \circ f$ is Riemann integrable.","Function is continuous and strictly increasing on and is Riemann integrable such that is defined. Is Riemann integrable? I was able to show that if additionally satisfies: , then is Riemann integrable.","f [a,b] g g \circ f g \circ f f x_{1}<x_{2} \in [a,b] \Rightarrow f(x_2)-f(x_1) \geq x_{2}-x_{1} g \circ f","['real-analysis', 'riemann-integration']"
34,Continuous function with $f(x^m+y^n) \le f(x+y) $,Continuous function with,f(x^m+y^n) \le f(x+y) ,"Let $m>n\ge 1$ be integers. If $m$ is even and $f:\mathbb{R} \to \mathbb{R} $ is continuous, nonconstant, with $f(x^m+y^n) \le f(x+y) $ , $\forall x, y \in \mathbb{R} $ , prove that $n$ is even. I think that we should suppose $n$ is odd and reach some kind of contradiction. By taking $x=0$ we get that $f(y) \ge f(y^n), \forall y\in \mathbb{R} $ , but I don't know how to use this. EDIT: Is there any chance that the problem is wrong? An user in the comments pointed out that the fact that $m$ is even is redundant. This makes me doubt a little the correctness of the problem, but I can't make any progress in this direction either. Bump","Let be integers. If is even and is continuous, nonconstant, with , , prove that is even. I think that we should suppose is odd and reach some kind of contradiction. By taking we get that , but I don't know how to use this. EDIT: Is there any chance that the problem is wrong? An user in the comments pointed out that the fact that is even is redundant. This makes me doubt a little the correctness of the problem, but I can't make any progress in this direction either. Bump","m>n\ge 1 m f:\mathbb{R} \to \mathbb{R}  f(x^m+y^n) \le f(x+y)  \forall x, y \in \mathbb{R}  n n x=0 f(y) \ge f(y^n), \forall y\in \mathbb{R}  m","['real-analysis', 'functional-analysis', 'functions', 'continuity']"
35,"Given a function $\alpha$ of bounded variation, show that: $\lim_{n \rightarrow \infty} \int_{0}^{1} x^n d\alpha(x) =\alpha(1) - \alpha(1-)$","Given a function  of bounded variation, show that:",\alpha \lim_{n \rightarrow \infty} \int_{0}^{1} x^n d\alpha(x) =\alpha(1) - \alpha(1-),"Given a function $\alpha$ of bounded variation on $[0,1]$ , show that: $\lim_{n \rightarrow \infty} \int_{0}^{1} x^n d\alpha(x) =\alpha(1) - \alpha(1-)$ .   Where $\alpha(1-)$ is the left sided limit. First off, I can't see how this limit is not $0$ , I know it is zero if $\alpha$ is continuous. I'm still trying to find an example of a discontinuous $\alpha$ for which this limit is different than zero. Now, this is what I've attempted for this limit: Since $\alpha$ is of bounded variation, $\alpha = \beta - \gamma$ for $\beta$ and $\gamma$ increasing functions. So it suffices to show this for an increasing function, say $\beta$ . So, $\int_{0}^{1} x^n d\beta(x) = \int_{0}^{1-\epsilon} x^n d\beta(x) + \int_{1-\epsilon}^{1} x^n d\beta(x)$ for all $\epsilon \in (0,1)$ . Then, $0\le \int_{0}^{\epsilon}x^nd\beta(x) \le \epsilon^n(\beta(\epsilon) - \beta(0))$ , so that $\lim_{n \rightarrow \infty} \int_{0}^{1-\epsilon} x^n d\beta(x) = 0$ . This means that $\lim_{n \rightarrow \infty} \int_{1-\epsilon}^{1} x^n d\beta(x)$ should be equal to $\alpha(1) - \alpha(1-)$ . But I still don't know how to prove this.","Given a function of bounded variation on , show that: .   Where is the left sided limit. First off, I can't see how this limit is not , I know it is zero if is continuous. I'm still trying to find an example of a discontinuous for which this limit is different than zero. Now, this is what I've attempted for this limit: Since is of bounded variation, for and increasing functions. So it suffices to show this for an increasing function, say . So, for all . Then, , so that . This means that should be equal to . But I still don't know how to prove this.","\alpha [0,1] \lim_{n \rightarrow \infty} \int_{0}^{1} x^n d\alpha(x) =\alpha(1) - \alpha(1-) \alpha(1-) 0 \alpha \alpha \alpha \alpha = \beta - \gamma \beta \gamma \beta \int_{0}^{1} x^n d\beta(x) = \int_{0}^{1-\epsilon} x^n d\beta(x) + \int_{1-\epsilon}^{1} x^n d\beta(x) \epsilon \in (0,1) 0\le \int_{0}^{\epsilon}x^nd\beta(x) \le \epsilon^n(\beta(\epsilon) - \beta(0)) \lim_{n \rightarrow \infty} \int_{0}^{1-\epsilon} x^n d\beta(x) = 0 \lim_{n \rightarrow \infty} \int_{1-\epsilon}^{1} x^n d\beta(x) \alpha(1) - \alpha(1-)","['real-analysis', 'integration']"
36,Show that $\inf \{ \| f-P \|_{\infty}\mid P \in P_n \} \geq \delta_n$ for any decreasing sequence $\delta_n \to 0$,Show that  for any decreasing sequence,\inf \{ \| f-P \|_{\infty}\mid P \in P_n \} \geq \delta_n \delta_n \to 0,"I'm trying to show that given any decreasing sequence $\delta_n \to 0$ , we can find a continuous function $f: [-1,1] \to \mathbb{R}$ such that $$\inf\{\|f-P \|_{\infty}\mid P \text{ a polynomial of degree at most } n  \} \geq \delta_n.$$ The hint I've been given is to show that, if $\gamma_j$ is a sequence of non-negative numbers with $\sum_{j \geq 1} \gamma_j$ convergent and $T_j$ is the $j$ th Chebyshev polynomial, then show that $\displaystyle \sum_{j \geqslant 1} \gamma_j T_{3^j}$ converges uniformly on $[-1,1]$ to a continuous function $f$ and that if $P_n = \sum_{j=1}^n \gamma_j T_{3^j}$ then we can find points $$-1 \leqslant x_0 \leqslant \cdots \leqslant x_{3^{n+1}} \leqslant 1$$ with $$f(x_k) - P_n(x_k) = (-1)^{k+1} \sum_{j=n+1}^{\infty} \gamma_j.$$ I haven't even managed to do much with the very first part of the hint, I'm having trouble showing that it converges uniformly. Given that I can't come up with a candidate limit, I suppose I need to show that its uniformly Cauchy (assuming that partial sums of $\gamma_j$ are Cauchy) but I'm not sure how to work with the Chebyshev polynomials. In any case, I'm uncertain as to how the hint even applies to the question, I can see that $\sum_{j=n+1}^\infty \gamma_j$ is a decreasing sequence, which is more or less the only link I can think of between the parts, but again, doesn't help in any way.","I'm trying to show that given any decreasing sequence , we can find a continuous function such that The hint I've been given is to show that, if is a sequence of non-negative numbers with convergent and is the th Chebyshev polynomial, then show that converges uniformly on to a continuous function and that if then we can find points with I haven't even managed to do much with the very first part of the hint, I'm having trouble showing that it converges uniformly. Given that I can't come up with a candidate limit, I suppose I need to show that its uniformly Cauchy (assuming that partial sums of are Cauchy) but I'm not sure how to work with the Chebyshev polynomials. In any case, I'm uncertain as to how the hint even applies to the question, I can see that is a decreasing sequence, which is more or less the only link I can think of between the parts, but again, doesn't help in any way.","\delta_n \to 0 f: [-1,1] \to \mathbb{R} \inf\{\|f-P \|_{\infty}\mid P \text{ a polynomial of degree at most } n  \} \geq \delta_n. \gamma_j \sum_{j \geq 1} \gamma_j T_j j \displaystyle \sum_{j \geqslant 1} \gamma_j T_{3^j} [-1,1] f P_n = \sum_{j=1}^n \gamma_j T_{3^j} -1 \leqslant x_0 \leqslant \cdots \leqslant x_{3^{n+1}} \leqslant 1 f(x_k) - P_n(x_k) = (-1)^{k+1} \sum_{j=n+1}^{\infty} \gamma_j. \gamma_j \sum_{j=n+1}^\infty \gamma_j","['real-analysis', 'uniform-convergence', 'chebyshev-polynomials']"
37,An inequality about linear PDE,An inequality about linear PDE,,"I am trying to solve the following problem: Let $\Omega$ be a bounded smooth domain in $\mathbb{R}^n, \ n\geq 2$ . Let $u\in C^2(\overline{\Omega})$ be a solution of $$\left\{\begin{array}{ll}u_t-\Delta u=f(x)& \text{in } \Omega\times(0,\infty)\\ u=0&\text{on }\partial \Omega\times (0,\infty) \\ u=g(x) & \text{on } \Omega\times\{0\}\end{array}\right.$$ Show that $$\max_{0\leq t\leq T} \int_\Omega u^2(x,t)dx+\int_0^T\int_\Omega|\nabla u(x,t)|^2dx\, dt\leq C\left(\int_\Omega g^2(x)dx+\int_0^T|f(x)|^2 dx\,dt\right)$$ for some constant $C$ independent of $f,\ g$ and $u$ . My attempt: Multiplying the first equation by $u$ and taking integration on $\Omega$ , we have $$\frac{1}{2}\frac{d}{dt}||u||^2_{L^2}+\int_\Omega \nabla u\cdot \nabla u=\int_\Omega fu.$$ (Here we also use the Green's identity and the boundary condition of $u$ ) Taking integration on $[0,s]$ where $s\leq T$ . Then we have $$\frac{1}{2}||u(x,s)||^2_{L^2}+\int_0^s\int_\Omega|\nabla u(x,t)|^2dx\, dt= \frac{1}{2} \int_\Omega g^2(x)dx+\int_0^s\int_\Omega fu\ dt$$ where $$\int_\Omega fu\leq ||f||_{L^2} ||u||_{L^2}\leq \epsilon ||u||_{L^2}^2 + C(\epsilon ) ||f||_{L^2}$$ Then we have $$(\frac{1}{2}-s\epsilon )||u(x,s)||^2_{L^2}+\int_0^s\int_\Omega|\nabla u(x,t)|^2dx\, dt \leq \frac{1}{2} \int_\Omega g^2(x)dx+\int_0^s C(\epsilon ) ||f||_{L^2} dt$$ Then I was stuck here. I don't know where can I derive the part $$\max_{0\leq t\leq T}||u(x,t)||.$$ Also, I am struggling how to make the two coefficients before the two terms on the left the same so that we can get the desired inequality.","I am trying to solve the following problem: Let be a bounded smooth domain in . Let be a solution of Show that for some constant independent of and . My attempt: Multiplying the first equation by and taking integration on , we have (Here we also use the Green's identity and the boundary condition of ) Taking integration on where . Then we have where Then we have Then I was stuck here. I don't know where can I derive the part Also, I am struggling how to make the two coefficients before the two terms on the left the same so that we can get the desired inequality.","\Omega \mathbb{R}^n, \ n\geq 2 u\in C^2(\overline{\Omega}) \left\{\begin{array}{ll}u_t-\Delta u=f(x)& \text{in } \Omega\times(0,\infty)\\ u=0&\text{on }\partial \Omega\times (0,\infty) \\ u=g(x) & \text{on } \Omega\times\{0\}\end{array}\right. \max_{0\leq t\leq T} \int_\Omega u^2(x,t)dx+\int_0^T\int_\Omega|\nabla u(x,t)|^2dx\, dt\leq C\left(\int_\Omega g^2(x)dx+\int_0^T|f(x)|^2 dx\,dt\right) C f,\ g u u \Omega \frac{1}{2}\frac{d}{dt}||u||^2_{L^2}+\int_\Omega \nabla u\cdot \nabla u=\int_\Omega fu. u [0,s] s\leq T \frac{1}{2}||u(x,s)||^2_{L^2}+\int_0^s\int_\Omega|\nabla u(x,t)|^2dx\, dt= \frac{1}{2} \int_\Omega g^2(x)dx+\int_0^s\int_\Omega fu\ dt \int_\Omega fu\leq ||f||_{L^2} ||u||_{L^2}\leq \epsilon ||u||_{L^2}^2 + C(\epsilon ) ||f||_{L^2} (\frac{1}{2}-s\epsilon )||u(x,s)||^2_{L^2}+\int_0^s\int_\Omega|\nabla u(x,t)|^2dx\, dt
\leq \frac{1}{2} \int_\Omega g^2(x)dx+\int_0^s C(\epsilon ) ||f||_{L^2} dt \max_{0\leq t\leq T}||u(x,t)||.","['real-analysis', 'inequality', 'partial-differential-equations']"
38,For which values of $a$ is $f$ primitivable?,For which values of  is  primitivable?,a f,"Let $a \in \mathbb{R}$ and $p,q$ be natural numbers with $p \geq q+2.$ For which values of $a$ is the function $$f(x) =  \begin{cases} \frac{1}{x}\sin \frac{1}{x^p}\sin \frac{1}{x^q}, &x \neq 0 \\ a, &x=0 \end{cases} $$ primitivable? I noticed that $\frac{1}{x}\sin \frac{1}{x^p}\sin \frac{1}{x^q}$ doesn't have an elementary antiderivative, so I tried to obtain it from the derivative of some function and work this from there. But those powers in the denominator are really getting in the way of any attempt. I also thought of using the formula $\sin a \sin b = \frac{1}{2}(\cos(a-b)-\cos(a+b))$ and so I split the function like  this: $$f(x)= \frac{1}{2}\begin{cases} \frac{1}{x}\cos(\frac{1}{x^p}-\frac{1}{x^q}), &x \neq 0 \\ a, &x=0 \end{cases} - \frac{1}{2}\begin{cases} \frac{1}{x}\cos(\frac{1}{x^p}+\frac{1}{x^q}), &x \neq 0 \\ a, &x=0 \end{cases} $$ and this definitely looks more promising, but I don't know how to proceed.","Let and be natural numbers with For which values of is the function primitivable? I noticed that doesn't have an elementary antiderivative, so I tried to obtain it from the derivative of some function and work this from there. But those powers in the denominator are really getting in the way of any attempt. I also thought of using the formula and so I split the function like  this: and this definitely looks more promising, but I don't know how to proceed.","a \in \mathbb{R} p,q p \geq q+2. a f(x) = 
\begin{cases}
\frac{1}{x}\sin \frac{1}{x^p}\sin \frac{1}{x^q}, &x \neq 0 \\
a, &x=0
\end{cases}
 \frac{1}{x}\sin \frac{1}{x^p}\sin \frac{1}{x^q} \sin a \sin b = \frac{1}{2}(\cos(a-b)-\cos(a+b)) f(x)=
\frac{1}{2}\begin{cases}
\frac{1}{x}\cos(\frac{1}{x^p}-\frac{1}{x^q}), &x \neq 0 \\
a, &x=0
\end{cases} -
\frac{1}{2}\begin{cases}
\frac{1}{x}\cos(\frac{1}{x^p}+\frac{1}{x^q}), &x \neq 0 \\
a, &x=0
\end{cases}
","['calculus', 'real-analysis', 'integration', 'indefinite-integrals']"
39,What are the conditions on $f$ for $\sup f$ and $f(\sup)$ be interchangeable?,What are the conditions on  for  and  be interchangeable?,f \sup f f(\sup),"Let $f:\mathbb{R}^n \to \mathbb{R}$ . Say that $f$ is magic if for every nonempty set $X$ and every functions $g_1,\dots,g_n : X \to \mathbb{R}$ the following holds: $$\sup_{(x_1,\dots,x_n) \in X^n} f(g_1(x_1),\dots,g_n(x_n)) = f \left(\sup_{x \in X} g_1(x),\dots,\sup_{x \in X} g_n(x) \right)$$ I am looking for a characterization of magicness . In other words, I am looking for necessary and sufficient conditions for magicness , a theorem that looks like: $f$ is magic if and only if [some conditions here] Intuitively, it looks like being a linear operator with non-negative coefficients* is sufficient, but (1) I could be wrong and (2) it seems too strong to be a necessary condition anyway. * By linear operator with non-negative coefficients I mean that $f$ has the form $f(x_1, \dots, x_n) = a_1 x_1 + \dots + a_n x_n$ where every $a_i$ is non-negative.","Let . Say that is magic if for every nonempty set and every functions the following holds: I am looking for a characterization of magicness . In other words, I am looking for necessary and sufficient conditions for magicness , a theorem that looks like: is magic if and only if [some conditions here] Intuitively, it looks like being a linear operator with non-negative coefficients* is sufficient, but (1) I could be wrong and (2) it seems too strong to be a necessary condition anyway. * By linear operator with non-negative coefficients I mean that has the form where every is non-negative.","f:\mathbb{R}^n \to \mathbb{R} f X g_1,\dots,g_n : X \to \mathbb{R} \sup_{(x_1,\dots,x_n) \in X^n} f(g_1(x_1),\dots,g_n(x_n)) = f \left(\sup_{x \in X} g_1(x),\dots,\sup_{x \in X} g_n(x) \right) f f f(x_1, \dots, x_n) = a_1 x_1 + \dots + a_n x_n a_i","['real-analysis', 'supremum-and-infimum']"
40,"How to solve $\int_0^\infty \left(\sqrt{1-\gamma(t)}e^{c\gamma(t)}-1\right)dt$, where $\gamma(t) = \frac{e^{-at}}{b+e^{-at}}$?","How to solve , where ?",\int_0^\infty \left(\sqrt{1-\gamma(t)}e^{c\gamma(t)}-1\right)dt \gamma(t) = \frac{e^{-at}}{b+e^{-at}},"$$\int_0^\infty \left(\sqrt{1-\gamma(t)}e^{c\gamma(t)}-1\right)dt,$$ where $$\gamma(t) = \frac{e^{-at}}{b+e^{-at}}.$$ I'll be glad if anyone can help me with this. I already tried some obvious substitutions, unsuccessfully of course. I rather have an exact result, however, if this can only be solved approximatively I would ideally have an expansion in $b$ .","where I'll be glad if anyone can help me with this. I already tried some obvious substitutions, unsuccessfully of course. I rather have an exact result, however, if this can only be solved approximatively I would ideally have an expansion in .","\int_0^\infty \left(\sqrt{1-\gamma(t)}e^{c\gamma(t)}-1\right)dt, \gamma(t) = \frac{e^{-at}}{b+e^{-at}}. b","['real-analysis', 'integration']"
41,Proving that the series with the general term $u_n = \int_{0}^{\frac{\pi}{2}} \sin^n(x)dx$ diverges.,Proving that the series with the general term  diverges.,u_n = \int_{0}^{\frac{\pi}{2}} \sin^n(x)dx,"$$u_n = \int_{0}^{\frac{\pi}{2}} \sin^n(x)dx$$ for $ n \in \mathbb{N}^*$ . Prove that $(u_n)$ is convergent toward $0$ . Prove that the series with the general term $(-1)^n u_n$ converges. Prove that $$\sum_{n=0}^{\infty} (-1)^n u_n = \int_{0}^{\frac{\pi}{2}} \frac{dx}{1 + \sin(x)}dx$$ Compute $$\int_{0}^{\frac{\pi}{2}} \frac{dx}{1 + \sin(x)}dx$$ Hint: You can start by proving $$\int_{0}^{\frac{\pi}{2}} \frac{dx}{1 + \sin(x)}dx = \int_{0}^{\frac{\pi}{2}} \frac{dx}{1 + \cos(x)}dx$$ Prove that the series with the general term $u_n$ diverges. Hint: You can start by proving $$u_n \geq \frac{1}{n + 1}$$ For $ 0 \leq x \leq \frac{\pi}{2}$ we have: $0 \leq u_n \leq \frac{\pi}{2}$ wich I can't derive the convergence from it. How can I prove it is convergent toward 0? I am stuck with question 4. and 5, I could not see how to prove the hints.","for . Prove that is convergent toward . Prove that the series with the general term converges. Prove that Compute Hint: You can start by proving Prove that the series with the general term diverges. Hint: You can start by proving For we have: wich I can't derive the convergence from it. How can I prove it is convergent toward 0? I am stuck with question 4. and 5, I could not see how to prove the hints.",u_n = \int_{0}^{\frac{\pi}{2}} \sin^n(x)dx  n \in \mathbb{N}^* (u_n) 0 (-1)^n u_n \sum_{n=0}^{\infty} (-1)^n u_n = \int_{0}^{\frac{\pi}{2}} \frac{dx}{1 + \sin(x)}dx \int_{0}^{\frac{\pi}{2}} \frac{dx}{1 + \sin(x)}dx \int_{0}^{\frac{\pi}{2}} \frac{dx}{1 + \sin(x)}dx = \int_{0}^{\frac{\pi}{2}} \frac{dx}{1 + \cos(x)}dx u_n u_n \geq \frac{1}{n + 1}  0 \leq x \leq \frac{\pi}{2} 0 \leq u_n \leq \frac{\pi}{2},"['real-analysis', 'sequences-and-series']"
42,How can we prove the scaling property of the Dirac delta function rigorously?,How can we prove the scaling property of the Dirac delta function rigorously?,,"Let $(\Omega,\mathcal A)$ be a measurable space $\omega\in\Omega$ $\delta_\omega$ denote the Dirac measureat $\omega$ on $(\Omega,\mathcal A)$ $E$ be a $\mathbb R$ -Banach space $\mathcal M$ denote the set of strongly $\mathcal A$ -measurable $f:\Omega\to E$ If $f\in\mathcal M$ , $$\langle\delta_\omega,f\rangle :=\int f\:{\rm d}\delta_\omega=f(\omega)$$ is well-defined. In that sense, $\delta_\omega$ can be thought of as an element of the algebraic dual space $\mathcal M^\ast$ of $\mathcal M$ . Now, assume $(\Omega,\mathcal A)=(\mathbb R,\mathcal B(\mathbb R))$ $E=\mathbb R$ In the context of distribution theory, we can find the so-called scaling property of the Dirac delta function $$\langle\delta(a\;\cdot\;),f\rangle=\frac1{\left|a\right|}\left\langle\delta,f\left(\frac{\;\cdot\;}a\right)\right\rangle=\frac1{\left|a\right|}f(0),\tag1$$ where $a\in\mathbb R\setminus\left\{0\right\}$ and $f\in C^\infty(\mathbb R,\mathbb C)$ . My problem is that I don't understand how $\delta(a\;\cdot\;)$ is defined. Moreover, I'm aware of the usual proof of $(1)$ relying on the substitution rule . However, since the Dirac delta function is not a function, I don't understand why that rule is applicable. It seems like one is assuming that there is a function $\delta$ such that the measure $\delta_x$ is Radon-Nikodým differentiable with respect to the Lebesuge measure $\lambda$ on $(\mathbb R,\mathcal B(\mathbb R))$ with $$\delta(\;\cdot\;-x)=\frac{{\rm d}\delta_x}{{\rm d}\lambda}\tag2$$ for all $x\in\mathbb R$ . With that assumption, it's easy to see that $$\int f(y)\delta(a(y-x))\:\lambda({\rm d}y)=\frac1{|a|}\int f\left(\frac{\;\cdot\;}a\right)\delta(\;\cdot\;-ax)\:{\rm d}\lambda=\frac1{|a|}f(x)\tag3$$ for all $x\in\mathbb R$ . However, since $\delta_x$ is singular with respect to $\lambda$ for all $x\in\mathbb R$ , $(2)$ is not well-defined. So, how can we state and prove $(1)$ in a rigorous way? Please note that I know almost nothing about distribution theory. So, it would be great if there would be a purely measure theoretic answer.","Let be a measurable space denote the Dirac measureat on be a -Banach space denote the set of strongly -measurable If , is well-defined. In that sense, can be thought of as an element of the algebraic dual space of . Now, assume In the context of distribution theory, we can find the so-called scaling property of the Dirac delta function where and . My problem is that I don't understand how is defined. Moreover, I'm aware of the usual proof of relying on the substitution rule . However, since the Dirac delta function is not a function, I don't understand why that rule is applicable. It seems like one is assuming that there is a function such that the measure is Radon-Nikodým differentiable with respect to the Lebesuge measure on with for all . With that assumption, it's easy to see that for all . However, since is singular with respect to for all , is not well-defined. So, how can we state and prove in a rigorous way? Please note that I know almost nothing about distribution theory. So, it would be great if there would be a purely measure theoretic answer.","(\Omega,\mathcal A) \omega\in\Omega \delta_\omega \omega (\Omega,\mathcal A) E \mathbb R \mathcal M \mathcal A f:\Omega\to E f\in\mathcal M \langle\delta_\omega,f\rangle :=\int f\:{\rm d}\delta_\omega=f(\omega) \delta_\omega \mathcal M^\ast \mathcal M (\Omega,\mathcal A)=(\mathbb R,\mathcal B(\mathbb R)) E=\mathbb R \langle\delta(a\;\cdot\;),f\rangle=\frac1{\left|a\right|}\left\langle\delta,f\left(\frac{\;\cdot\;}a\right)\right\rangle=\frac1{\left|a\right|}f(0),\tag1 a\in\mathbb R\setminus\left\{0\right\} f\in C^\infty(\mathbb R,\mathbb C) \delta(a\;\cdot\;) (1) \delta \delta_x \lambda (\mathbb R,\mathcal B(\mathbb R)) \delta(\;\cdot\;-x)=\frac{{\rm d}\delta_x}{{\rm d}\lambda}\tag2 x\in\mathbb R \int f(y)\delta(a(y-x))\:\lambda({\rm d}y)=\frac1{|a|}\int f\left(\frac{\;\cdot\;}a\right)\delta(\;\cdot\;-ax)\:{\rm d}\lambda=\frac1{|a|}f(x)\tag3 x\in\mathbb R \delta_x \lambda x\in\mathbb R (2) (1)","['real-analysis', 'functional-analysis', 'measure-theory', 'distribution-theory', 'dirac-delta']"
43,Whether 'min' and 'division' are metric,Whether 'min' and 'division' are metric,,"Let $(X,d_1)$ and ( $X,d_2)$ be metric spaces. Whether the following are again metrics on $X$ ? a) $d(x,y)=\text{min}\;\{d_1(x,y),d_2(x,y)\}$ b) $h(x,y)=\Big(\frac{d_1}{d_2}\Big)(x,y)$ where $x \neq y$ and $h(x,x)=0$ Actually the answer for the first option is already available in this site. But I mention here is to check my example. Take $X=\Bbb{R}$ and $d_1(x,y)=\vert x -y\vert$ and $d_2(x,y)=\vert x^3-y^3\vert$ and take $x=0, y=1/2 ,z=1$ Now $$d(0,1)=\text{min}\;\{1,1\}=1$$ $$d(0,1/2)=\text{min}\;\{1/2,1/8\}=1/8$$ $$d(1/2,1)=\text{min}\;\{1/2,7/8\}=1/2$$ But $1=d(0,1) \leq d(0,1/2)+d(1/2,1)=1/8+1/2=0.625$ does't hold. Hence $d$ is not a metric! Is this correct? and what about b? Any help?",Let and ( be metric spaces. Whether the following are again metrics on ? a) b) where and Actually the answer for the first option is already available in this site. But I mention here is to check my example. Take and and and take Now But does't hold. Hence is not a metric! Is this correct? and what about b? Any help?,"(X,d_1) X,d_2) X d(x,y)=\text{min}\;\{d_1(x,y),d_2(x,y)\} h(x,y)=\Big(\frac{d_1}{d_2}\Big)(x,y) x \neq y h(x,x)=0 X=\Bbb{R} d_1(x,y)=\vert x -y\vert d_2(x,y)=\vert x^3-y^3\vert x=0, y=1/2 ,z=1 d(0,1)=\text{min}\;\{1,1\}=1 d(0,1/2)=\text{min}\;\{1/2,1/8\}=1/8 d(1/2,1)=\text{min}\;\{1/2,7/8\}=1/2 1=d(0,1) \leq d(0,1/2)+d(1/2,1)=1/8+1/2=0.625 d",[]
44,Show that $e^{-t}\int_{0}^{t}\frac{e^{x}}{\sqrt{x}}dx$ is strictly decreasing for $t\geq1$,Show that  is strictly decreasing for,e^{-t}\int_{0}^{t}\frac{e^{x}}{\sqrt{x}}dx t\geq1,"I want to prove that $e^{-t}\int_{0}^{t}\frac{e^{x}}{\sqrt{x}}dx$ is decreasing on $[1,\infty[$. First of all numerical experiments verify this. I am trying the first derivative test, but stuck with showing that the sign of the derivative is negative, which is equivalent to showing that  $$\int_{0}^{t}\frac{e^{x}}{\sqrt{x}}dx> \frac{e^{t}}{\sqrt{t}},\qquad t\geq 1$$. Any ideas?","I want to prove that $e^{-t}\int_{0}^{t}\frac{e^{x}}{\sqrt{x}}dx$ is decreasing on $[1,\infty[$. First of all numerical experiments verify this. I am trying the first derivative test, but stuck with showing that the sign of the derivative is negative, which is equivalent to showing that  $$\int_{0}^{t}\frac{e^{x}}{\sqrt{x}}dx> \frac{e^{t}}{\sqrt{t}},\qquad t\geq 1$$. Any ideas?",,"['calculus', 'real-analysis', 'inequality']"
45,Factoring $x^n + 1$.,Factoring .,x^n + 1,"By the Fundamental Theorem of Algebra, every polynomial of degree $n$ can be factored into a product of $n$ linear polynomials. As an example, since the polynomial $ x^5 +1$ has the five complex roots $$\tag{1} -1,\quad e^{\frac{\pi i}{5}}, \quad  e^{\frac{-\pi i}{5}}, \quad  e^{\frac{3\pi i}{5}}, \quad  e^{\frac{-3\pi i}{5}},$$ we can write $$\tag{2} x^5+1=(x+1)(x- e^{\frac{\pi i}{5}})(x- e^{\frac{-\pi i}{5}})(x- e^{\frac{3\pi i}{5}})(x- e^{\frac{-3\pi i}{5}}).$$ Multiplying several terms in $(2)$ gives $$\tag{3} x^5+1=(x+1)(x^2 -2\cos{\frac{\pi}{5}}x + 1)(x^2 -2\cos{ \frac{3\pi}{5}}x+1).$$ Now, $(3)$ is of course a specific example of the general theorem (which also relies on the structure of $\mathbb{C}$): Any polynomial with real coefficients can be factored into a product of real linear and real quadratic polynomials. Generalising $(3)$ to any odd value  $n\in \mathbb{N}$, gives the following formula: $$\tag{4} x^n +1 = (x+1) \left(x^2 -2\big(\cos{\frac{\pi}{n}}\big)x + 1\right) \left(x^2 -2\big(\cos{\frac{3\pi}{n}}\big)x + 1\right)\cdots \left(x^2 -2\big(\cos{\frac{(n-2)\pi}{n}}\big)x + 1\right) .$$ This result is interesting to me, as it suggests that even in $\mathbb{R}$ the cosine function is, in some vague sense, built in to exponentiation. Question: I am wondering if either the general formula $(4)$ or a specific case, such as $(3)$, could be derived using only real analytical tools (and maybe some ring theory?): i.e., without complex numbers and Euler’s formula?","By the Fundamental Theorem of Algebra, every polynomial of degree $n$ can be factored into a product of $n$ linear polynomials. As an example, since the polynomial $ x^5 +1$ has the five complex roots $$\tag{1} -1,\quad e^{\frac{\pi i}{5}}, \quad  e^{\frac{-\pi i}{5}}, \quad  e^{\frac{3\pi i}{5}}, \quad  e^{\frac{-3\pi i}{5}},$$ we can write $$\tag{2} x^5+1=(x+1)(x- e^{\frac{\pi i}{5}})(x- e^{\frac{-\pi i}{5}})(x- e^{\frac{3\pi i}{5}})(x- e^{\frac{-3\pi i}{5}}).$$ Multiplying several terms in $(2)$ gives $$\tag{3} x^5+1=(x+1)(x^2 -2\cos{\frac{\pi}{5}}x + 1)(x^2 -2\cos{ \frac{3\pi}{5}}x+1).$$ Now, $(3)$ is of course a specific example of the general theorem (which also relies on the structure of $\mathbb{C}$): Any polynomial with real coefficients can be factored into a product of real linear and real quadratic polynomials. Generalising $(3)$ to any odd value  $n\in \mathbb{N}$, gives the following formula: $$\tag{4} x^n +1 = (x+1) \left(x^2 -2\big(\cos{\frac{\pi}{n}}\big)x + 1\right) \left(x^2 -2\big(\cos{\frac{3\pi}{n}}\big)x + 1\right)\cdots \left(x^2 -2\big(\cos{\frac{(n-2)\pi}{n}}\big)x + 1\right) .$$ This result is interesting to me, as it suggests that even in $\mathbb{R}$ the cosine function is, in some vague sense, built in to exponentiation. Question: I am wondering if either the general formula $(4)$ or a specific case, such as $(3)$, could be derived using only real analytical tools (and maybe some ring theory?): i.e., without complex numbers and Euler’s formula?",,"['real-analysis', 'polynomials', 'complex-numbers', 'polynomial-rings']"
46,Is there any sequence of functions containing functions growing slower than any smooth function?,Is there any sequence of functions containing functions growing slower than any smooth function?,,"The function $f(x)= \begin{cases}        e^{-\frac{1}{x}} & x > 0 \\       0 & x\leq 0 \\    \end{cases}$ is a smooth function which grows, at $0$, slower than any function of the form $x^n$. My question is, does there exist a sequence of functions $(f_n)$, infinitely differentiable at $0$ and with $f_n(0)=0$ and $f_n(x)\neq 0$ when $x\neq 0$ for all $n$, with the following property? For any function $f$ infinitely differentiable at 0 where $f(0)=0$ and $f(x)\neq 0$ when $x\neq 0$ there exists a natural number $N$ such that for all $n\geq N$, we have $\lim_{x\rightarrow 0}\frac{f_n(x)}{f(x)}=0$. Or is there no such sequence of functions, i.e. for any sequence of functions does there exist a function which grows more slowly than all the functions in the sequence?","The function $f(x)= \begin{cases}        e^{-\frac{1}{x}} & x > 0 \\       0 & x\leq 0 \\    \end{cases}$ is a smooth function which grows, at $0$, slower than any function of the form $x^n$. My question is, does there exist a sequence of functions $(f_n)$, infinitely differentiable at $0$ and with $f_n(0)=0$ and $f_n(x)\neq 0$ when $x\neq 0$ for all $n$, with the following property? For any function $f$ infinitely differentiable at 0 where $f(0)=0$ and $f(x)\neq 0$ when $x\neq 0$ there exists a natural number $N$ such that for all $n\geq N$, we have $\lim_{x\rightarrow 0}\frac{f_n(x)}{f(x)}=0$. Or is there no such sequence of functions, i.e. for any sequence of functions does there exist a function which grows more slowly than all the functions in the sequence?",,"['calculus', 'real-analysis', 'limits']"
47,Approximation from within by sets in a generating algebra,Approximation from within by sets in a generating algebra,,"Let $(\Omega, \mathcal{F}, \mu)$ be a finite measure space, and let $\mathcal{A}$ be an algebra generating $\mathcal{F}$. It is well-known that $\mu$ enjoys the following approximation property: For all $\epsilon>0$ and $F\in\mathcal{F}$, there exists $A \in \mathcal{A}$ such that $\mu(F \triangle A) < \epsilon$. My general question is When does the following, stronger approximation property hold? For all $\epsilon>0$ and $F \in \mathcal{F}$, there exists $A \in \mathcal{A}$, $A \subseteq F$, such that $P(F-A)< \epsilon$. More specifically, I'm mainly interested in products of finite spaces. That is, for each $n \in \mathbb{N}$, let $A_n$ be a finite set equipped with discrete topology. Let $\Omega = \prod_n A_n$. Let $\mathcal{F}$ be the Borel $\sigma$-algebra in the product topology, and let $\mathcal{A}$ be the algebra of cylinder sets of the form $$B_1 \times B_2 \times..., \ B_n \subseteq A_n, \ \text{and} \ B_n = A_n \ \text{for all but finitely many $n$}.$$ In this case, one result that is pretty close to what I want is that $\mu$ can be approximated from within by compact sets. See this . But I don't think that compact sets are necessarily members of $\mathcal{A}$, so it doesn't quite answer my question. What got me thinking about this was this post . In the presence of finite additivity, approximation from within by a compact class is sufficient for countable additivity. The present question, in the special case of products of finite spaces, is a sort of converse: When is countable additivity sufficient for approximation by the compact class that generates $\mathcal{F}$?","Let $(\Omega, \mathcal{F}, \mu)$ be a finite measure space, and let $\mathcal{A}$ be an algebra generating $\mathcal{F}$. It is well-known that $\mu$ enjoys the following approximation property: For all $\epsilon>0$ and $F\in\mathcal{F}$, there exists $A \in \mathcal{A}$ such that $\mu(F \triangle A) < \epsilon$. My general question is When does the following, stronger approximation property hold? For all $\epsilon>0$ and $F \in \mathcal{F}$, there exists $A \in \mathcal{A}$, $A \subseteq F$, such that $P(F-A)< \epsilon$. More specifically, I'm mainly interested in products of finite spaces. That is, for each $n \in \mathbb{N}$, let $A_n$ be a finite set equipped with discrete topology. Let $\Omega = \prod_n A_n$. Let $\mathcal{F}$ be the Borel $\sigma$-algebra in the product topology, and let $\mathcal{A}$ be the algebra of cylinder sets of the form $$B_1 \times B_2 \times..., \ B_n \subseteq A_n, \ \text{and} \ B_n = A_n \ \text{for all but finitely many $n$}.$$ In this case, one result that is pretty close to what I want is that $\mu$ can be approximated from within by compact sets. See this . But I don't think that compact sets are necessarily members of $\mathcal{A}$, so it doesn't quite answer my question. What got me thinking about this was this post . In the presence of finite additivity, approximation from within by a compact class is sufficient for countable additivity. The present question, in the special case of products of finite spaces, is a sort of converse: When is countable additivity sufficient for approximation by the compact class that generates $\mathcal{F}$?",,"['real-analysis', 'measure-theory']"
48,"Inequivalence of Compactness and Sequential Compactness, Especially for the Weak* Topology?","Inequivalence of Compactness and Sequential Compactness, Especially for the Weak* Topology?",,"I know that for topological spaces, compactness and sequential compactness are generally not equivalent. From wikipedia: ""there exist sequentially compact spaces that are not compact (such as the first uncountable ordinal with the order topology), and compact spaces that are not sequentially compact (such as the product of ${\displaystyle 2^{\aleph _{0}}={\mathfrak {c}}}$ copies of the closed unit interval [e.g., the space of functions $f:\mathbb{R} \to [0,1]$ with pointwise convergence])."" For the weak topology on a normed space, compactness and sequential compactness are equivalent: Theorem (Eberlein-Smulian) Let $X$ be a normed space and let $A$ be a subset of $X$. Then $A$ is weakly compact if and only $A$ is weakly sequentially compact. I'm interested in examples where compactness and sequential compactness are not equivalent in the weak* topology. Example. The closed unit ball $B_{*}$ of $X^{*} = (\ell^{\infty})^{*}$ is weak* compact (by Banach-Alaoglu) but is not weak* sequentially compact (the sequence of functionals $f_n(x_1,x_2,\ldots,)=x_n$ has no weak* convergent subsequence). Question 1. What is an example of a normed space $X$ and an $A \subseteq X^{*}$ that is weak* sequentially compact but not weak* compact? Question 2. Is there a normed space $X$ such that (i) there is a set $A \subseteq X^{*}$ that is weak* sequentially compact but not weak* compact and (ii) the closed unit ball $B_{*} \subseteq X^{*}$ is weak* compact but not weak* sequentially compact. Question 3. (Easier than Question 2). Is there a topological space $X$ such that (i) there is a set $A \subseteq X$ that is sequentially compact but not compact and (ii) there is a set $B \subseteq X$ that is compact but not sequentially compact. Edit: Question 3 turned out to be much easier than I thought (see Henno Brandsma's answer below).","I know that for topological spaces, compactness and sequential compactness are generally not equivalent. From wikipedia: ""there exist sequentially compact spaces that are not compact (such as the first uncountable ordinal with the order topology), and compact spaces that are not sequentially compact (such as the product of ${\displaystyle 2^{\aleph _{0}}={\mathfrak {c}}}$ copies of the closed unit interval [e.g., the space of functions $f:\mathbb{R} \to [0,1]$ with pointwise convergence])."" For the weak topology on a normed space, compactness and sequential compactness are equivalent: Theorem (Eberlein-Smulian) Let $X$ be a normed space and let $A$ be a subset of $X$. Then $A$ is weakly compact if and only $A$ is weakly sequentially compact. I'm interested in examples where compactness and sequential compactness are not equivalent in the weak* topology. Example. The closed unit ball $B_{*}$ of $X^{*} = (\ell^{\infty})^{*}$ is weak* compact (by Banach-Alaoglu) but is not weak* sequentially compact (the sequence of functionals $f_n(x_1,x_2,\ldots,)=x_n$ has no weak* convergent subsequence). Question 1. What is an example of a normed space $X$ and an $A \subseteq X^{*}$ that is weak* sequentially compact but not weak* compact? Question 2. Is there a normed space $X$ such that (i) there is a set $A \subseteq X^{*}$ that is weak* sequentially compact but not weak* compact and (ii) the closed unit ball $B_{*} \subseteq X^{*}$ is weak* compact but not weak* sequentially compact. Question 3. (Easier than Question 2). Is there a topological space $X$ such that (i) there is a set $A \subseteq X$ that is sequentially compact but not compact and (ii) there is a set $B \subseteq X$ that is compact but not sequentially compact. Edit: Question 3 turned out to be much easier than I thought (see Henno Brandsma's answer below).",,"['real-analysis', 'general-topology', 'functional-analysis']"
49,Lipschitz constant of limit of functions,Lipschitz constant of limit of functions,,"Consider two metric spaces $(X,d_X)$ and $(Y,d_Y)$ and define the lipschitz constant of every continuous function $f:X\rightarrow Y$ as $$Lip(f):=\sup\limits_{x\neq y}\frac{d_Y(f(x),f(y))}{d_X(x,y)}$$ Consider a sequence of continuous functions $f_n:X\rightarrow Y$ such that there is a $k>1$ such that for every $n\in \mathbb{N}$ it is $Lip(f_n)\le k$ $\{f_n\}$ has limit $f:X\rightarrow Y$ for the uniform convergence on  compact sets (this means that for every $K\subset X$ compact it results $\lim\limits_{n\rightarrow \infty}\sup\limits_{x\in K}d_Y(f(x),f_n(x))=0$) $Lip(f_n)\rightarrow 1$ Question: does it follow $Lip(f)=1$? Can you motivate your answer? I am sorry if I do not show my reasoning, but I can not even understand if the statement is true or not","Consider two metric spaces $(X,d_X)$ and $(Y,d_Y)$ and define the lipschitz constant of every continuous function $f:X\rightarrow Y$ as $$Lip(f):=\sup\limits_{x\neq y}\frac{d_Y(f(x),f(y))}{d_X(x,y)}$$ Consider a sequence of continuous functions $f_n:X\rightarrow Y$ such that there is a $k>1$ such that for every $n\in \mathbb{N}$ it is $Lip(f_n)\le k$ $\{f_n\}$ has limit $f:X\rightarrow Y$ for the uniform convergence on  compact sets (this means that for every $K\subset X$ compact it results $\lim\limits_{n\rightarrow \infty}\sup\limits_{x\in K}d_Y(f(x),f_n(x))=0$) $Lip(f_n)\rightarrow 1$ Question: does it follow $Lip(f)=1$? Can you motivate your answer? I am sorry if I do not show my reasoning, but I can not even understand if the statement is true or not",,"['real-analysis', 'functional-analysis', 'analysis', 'lipschitz-functions']"
50,"Comparing some numbers $a=2016^\sqrt{2014}, b=2015^\sqrt{2015},c=2014^\sqrt{2016}$",Comparing some numbers,"a=2016^\sqrt{2014}, b=2015^\sqrt{2015},c=2014^\sqrt{2016}","compare $a=2016^\sqrt{2014}, b=2015^\sqrt{2015},c=2014^\sqrt{2016}$ I took 2 functions to solve this question: $f(x)=\sqrt{x+1}\ln(x-1)-\sqrt{x}\ln x$ and $g(x)=\sqrt{x}\ln x - \sqrt{x-1}\ln(x+1).$ $f(x)$ has a local maxima at a point of abscissa $x_1\in(62,63)$ and $\lim_{x\to\infty}f(x)=0$ and $f'(x)<0$ on $[x_1,\infty).\implies f(2015)>0\implies c>b.$ $g(x)$ has a local maxima in the point of absicssa $x_2\in(45,46), \lim_{x\to\infty}g(x)=0$ and $g'(x)<0$ on $[x_2,\infty)\implies g(2015)>0\implies b > a.$ Leaving us with the answer $c > b > a.$ However, I was thinking of a way of verifying this (comparing these numbers) using only $1$ function and I was thinking maybe something like $f(x)=\sqrt{x}\ln(4030+x)...$ but it doesn't really work out... how could i compare these numbers using only one function?","compare $a=2016^\sqrt{2014}, b=2015^\sqrt{2015},c=2014^\sqrt{2016}$ I took 2 functions to solve this question: $f(x)=\sqrt{x+1}\ln(x-1)-\sqrt{x}\ln x$ and $g(x)=\sqrt{x}\ln x - \sqrt{x-1}\ln(x+1).$ $f(x)$ has a local maxima at a point of abscissa $x_1\in(62,63)$ and $\lim_{x\to\infty}f(x)=0$ and $f'(x)<0$ on $[x_1,\infty).\implies f(2015)>0\implies c>b.$ $g(x)$ has a local maxima in the point of absicssa $x_2\in(45,46), \lim_{x\to\infty}g(x)=0$ and $g'(x)<0$ on $[x_2,\infty)\implies g(2015)>0\implies b > a.$ Leaving us with the answer $c > b > a.$ However, I was thinking of a way of verifying this (comparing these numbers) using only $1$ function and I was thinking maybe something like $f(x)=\sqrt{x}\ln(4030+x)...$ but it doesn't really work out... how could i compare these numbers using only one function?",,"['calculus', 'real-analysis', 'inequality', 'exponential-function', 'radicals']"
51,Is a Sobolev map with smooth minors smooth?,Is a Sobolev map with smooth minors smooth?,,"$\newcommand{\Cof}{\text{cof}}$ Let $d>2$. Let $f \in W^{1,p}(\Omega,\mathbb{R}^d)$ where $\Omega$ is an open subset of $\mathbb{R}^d$. Let $2 \le k \le d-1$ be fixed. Suppose that $\det df>0$ a.e. and that $\bigwedge^k df$ is smooth. Is $f$ smooth? Partial answer: If $k,d$ are not both even and $\bigwedge^k df \in \text{GL}(\bigwedge^{k}\mathbb{R}^d)$ , the answer is positive. (see details below). I am not sure  the answer remains positive when $k,d$ are both even, since in that case, $\bigwedge^k A=\bigwedge^k (-A)$ and both $A,-A \in \text{GL}^+(\mathbb{R}^d)$. Thus the minors cannot distinguish beween a map and its negative, so theoretically $df$ could ""switch"" between ""something"" and its negative, thus violating smoothness. It would be interesting to find a concrete counter example for smoothness in this case. The smallest dimensions are $d=4,k=2$. Every possible counter-example must have non-continuous weak derivatives . When $k,d$ are not both even, $\bigwedge^k df$ uniquely determines $df$ (assuming $\det df>0$). That is, if $A,B \in \text{GL}^+(\mathbb{R}^d)$ and $\bigwedge^k A=\bigwedge^k B$, then $A=B$. Indeed, write $S=AB^{-1}$. Then we have $\bigwedge^k S=\text{Id}_{\bigwedge^k \mathbb{R}^d}$. This implies, any $k$-dimensional subspace of $\mathbb{R}^d$ is $S$-invariant, hence $S$ is a multiple of the identity , i.e. $S=\lambda \text{Id}$, which then forces $\lambda^k=1$, so $\lambda=\pm 1$. If $k$ is odd, then $\lambda=1$, and $S=\text{Id}$. If $d$ is odd, then the requirement $S \in \text{GL}^+(\mathbb{R}^n)$ forces $\lambda^d=\det S >0$, so again $S=\text{Id}$. We showed that the map $\psi: A \to \bigwedge^k A$ is a smooth injective homomorphism of Lie groups $\text{GL}^+(V) \to \text{GL}(\bigwedge^{k}V)$. Set $ S=\text{Image} (\psi)$. Since $S$ is an embedded submanifold , $\psi:\text{GL}^+(V) \to S$ is a diffeomorphism. Now composing $x \to \bigwedge^k df_x$ with the smooth inverse of $\psi$ finishes the job. Some more details are provided in my answer below.","$\newcommand{\Cof}{\text{cof}}$ Let $d>2$. Let $f \in W^{1,p}(\Omega,\mathbb{R}^d)$ where $\Omega$ is an open subset of $\mathbb{R}^d$. Let $2 \le k \le d-1$ be fixed. Suppose that $\det df>0$ a.e. and that $\bigwedge^k df$ is smooth. Is $f$ smooth? Partial answer: If $k,d$ are not both even and $\bigwedge^k df \in \text{GL}(\bigwedge^{k}\mathbb{R}^d)$ , the answer is positive. (see details below). I am not sure  the answer remains positive when $k,d$ are both even, since in that case, $\bigwedge^k A=\bigwedge^k (-A)$ and both $A,-A \in \text{GL}^+(\mathbb{R}^d)$. Thus the minors cannot distinguish beween a map and its negative, so theoretically $df$ could ""switch"" between ""something"" and its negative, thus violating smoothness. It would be interesting to find a concrete counter example for smoothness in this case. The smallest dimensions are $d=4,k=2$. Every possible counter-example must have non-continuous weak derivatives . When $k,d$ are not both even, $\bigwedge^k df$ uniquely determines $df$ (assuming $\det df>0$). That is, if $A,B \in \text{GL}^+(\mathbb{R}^d)$ and $\bigwedge^k A=\bigwedge^k B$, then $A=B$. Indeed, write $S=AB^{-1}$. Then we have $\bigwedge^k S=\text{Id}_{\bigwedge^k \mathbb{R}^d}$. This implies, any $k$-dimensional subspace of $\mathbb{R}^d$ is $S$-invariant, hence $S$ is a multiple of the identity , i.e. $S=\lambda \text{Id}$, which then forces $\lambda^k=1$, so $\lambda=\pm 1$. If $k$ is odd, then $\lambda=1$, and $S=\text{Id}$. If $d$ is odd, then the requirement $S \in \text{GL}^+(\mathbb{R}^n)$ forces $\lambda^d=\det S >0$, so again $S=\text{Id}$. We showed that the map $\psi: A \to \bigwedge^k A$ is a smooth injective homomorphism of Lie groups $\text{GL}^+(V) \to \text{GL}(\bigwedge^{k}V)$. Set $ S=\text{Image} (\psi)$. Since $S$ is an embedded submanifold , $\psi:\text{GL}^+(V) \to S$ is a diffeomorphism. Now composing $x \to \bigwedge^k df_x$ with the smooth inverse of $\psi$ finishes the job. Some more details are provided in my answer below.",,"['real-analysis', 'differential-geometry', 'sobolev-spaces', 'regularity-theory-of-pdes', 'jacobian']"
52,Let $X$ be Banach space and $X^*$ be its dual space. Let $S\subset X$ be a convex set containing the origin. Prove that $\bar S = S^{**}.$,Let  be Banach space and  be its dual space. Let  be a convex set containing the origin. Prove that,X X^* S\subset X \bar S = S^{**}.,"I am learning the Dual Space in functional analysis. I have trouble proving this statement. Let $X$ be Banach space and $X^*$ be its dual space. Let $S\subset X$ be a convex set containing the origin. Define $$S^*=\{\phi\in X^*: \phi(x)\le 1\ \;\forall x\in S\},$$ $$S^{**}=\{x\in X;\phi(x)\le 1, \text{for all}~\phi\in S^*\}.$$ Prove that $\bar S = S^{**},\bar S~\text{is the closure of} ~S.$ My attempt: I want to show $\bar S\subset S^{**}$ and $S^{**} \subset \bar S$. $\forall x,y\in S,(1-a)x+ay\in S$. $\phi((1-a)x+ay)=(1-a)\phi(x)+a\phi(y) \subset\phi(S)$. I can show that $\phi(S)$ is a convex set. But I don't know what I can do next to prove the statement.","I am learning the Dual Space in functional analysis. I have trouble proving this statement. Let $X$ be Banach space and $X^*$ be its dual space. Let $S\subset X$ be a convex set containing the origin. Define $$S^*=\{\phi\in X^*: \phi(x)\le 1\ \;\forall x\in S\},$$ $$S^{**}=\{x\in X;\phi(x)\le 1, \text{for all}~\phi\in S^*\}.$$ Prove that $\bar S = S^{**},\bar S~\text{is the closure of} ~S.$ My attempt: I want to show $\bar S\subset S^{**}$ and $S^{**} \subset \bar S$. $\forall x,y\in S,(1-a)x+ay\in S$. $\phi((1-a)x+ay)=(1-a)\phi(x)+a\phi(y) \subset\phi(S)$. I can show that $\phi(S)$ is a convex set. But I don't know what I can do next to prove the statement.",,"['real-analysis', 'linear-algebra', 'functional-analysis', 'banach-spaces', 'dual-spaces']"
53,How to calculate the Fourier transform of the Poisson kernel,How to calculate the Fourier transform of the Poisson kernel,,"We know that the Fourier transform of the Poisson kernel $P(x,t)$ \begin{equation} \frac{\Gamma(\frac{(n+1)}{2})}{\pi^{\frac{(n+1)}{2}}}\frac{t}{(t^2+\lvert x\rvert^2)^{\frac{(n+1)}{2}}} \end{equation} is the Abel kernel $K(x,t)$ \begin{equation} e^{-2\pi t \lvert \xi \rvert}. \end{equation} However, I have just seen one method of proving it from Stein's Introduction to Fourier Analysis on Euclidean Spaces. The key of the proof is to use \begin{equation} e^{-\beta}=\frac{1}{\sqrt \pi} \int_0^\infty \frac{e^{-u}}{\sqrt u} e^{-\frac{\beta^2}{4u}} \, \mathrm{d} u. \end{equation} And it start with the Abel kernel to Poisson kernel. But I feel that this proof is a little trick. So is there any other proof of it? Thank you very much!","We know that the Fourier transform of the Poisson kernel $P(x,t)$ \begin{equation} \frac{\Gamma(\frac{(n+1)}{2})}{\pi^{\frac{(n+1)}{2}}}\frac{t}{(t^2+\lvert x\rvert^2)^{\frac{(n+1)}{2}}} \end{equation} is the Abel kernel $K(x,t)$ \begin{equation} e^{-2\pi t \lvert \xi \rvert}. \end{equation} However, I have just seen one method of proving it from Stein's Introduction to Fourier Analysis on Euclidean Spaces. The key of the proof is to use \begin{equation} e^{-\beta}=\frac{1}{\sqrt \pi} \int_0^\infty \frac{e^{-u}}{\sqrt u} e^{-\frac{\beta^2}{4u}} \, \mathrm{d} u. \end{equation} And it start with the Abel kernel to Poisson kernel. But I feel that this proof is a little trick. So is there any other proof of it? Thank you very much!",,"['calculus', 'real-analysis', 'integration', 'harmonic-analysis']"
54,"How to prove that the span of $\cos((n+1/2)x)$ and $\sin(nx)$ is dense in $L^2(-\pi,\pi)$?",How to prove that the span of  and  is dense in ?,"\cos((n+1/2)x) \sin(nx) L^2(-\pi,\pi)","Consider the following functions in $L^2(-\pi,\pi)$: $$ f_n(x)=\cos\big((n+1/2)x\big), g_k(x)=\sin(kx), n=0,1,\dots, k=1,2,\dots.$$ I am trying to prove $\text{span}(f_n \cup g_k)$ is dense in $L^2(-\pi,\pi)$. I already know the standard trigonometric polynomials are dense, that is $\cos(nx),\sin(nx) $ form a complete orthonormal system. Is there any easy way to reduce the problem to this well-known fact? One way to prove the density of the trigonometric polynomials is to use Stone-Weierstrass theorem, but here our set of ""modified"" trigonometric polynomials do not form an algebra, so we can't use the theorem directly. Any advice? (The reason I am interested in this specific set of generators is that they arise naturally as eigenfunctions of a differential operator).","Consider the following functions in $L^2(-\pi,\pi)$: $$ f_n(x)=\cos\big((n+1/2)x\big), g_k(x)=\sin(kx), n=0,1,\dots, k=1,2,\dots.$$ I am trying to prove $\text{span}(f_n \cup g_k)$ is dense in $L^2(-\pi,\pi)$. I already know the standard trigonometric polynomials are dense, that is $\cos(nx),\sin(nx) $ form a complete orthonormal system. Is there any easy way to reduce the problem to this well-known fact? One way to prove the density of the trigonometric polynomials is to use Stone-Weierstrass theorem, but here our set of ""modified"" trigonometric polynomials do not form an algebra, so we can't use the theorem directly. Any advice? (The reason I am interested in this specific set of generators is that they arise naturally as eigenfunctions of a differential operator).",,"['real-analysis', 'functional-analysis', 'fourier-analysis', 'hilbert-spaces', 'fourier-series']"
55,How can I prove that $f(x)=\sum_{k=1}^{\infty}\frac{\{kx\}}{k^{2}}$ is continuous?,How can I prove that  is continuous?,f(x)=\sum_{k=1}^{\infty}\frac{\{kx\}}{k^{2}},"Let $f:\mathbb{R}\to\mathbb{R}$ be defined as $$f(x)=\sum_{k=1}^{\infty}\frac{\{kx\}}{k^{2}},$$ where $$\{x\}=\begin{cases}0; & x = \dfrac{n}{2}\ \text{for some odd integer } n\\x - m(x);& \text{otherwise}\end{cases}$$ and $m(x)$ is the nearest integer to $x$. I have already proved that $f$ is continuos at $x=0$ (actually I have proved that it is at any integer and, maybe, at rational numbers of the form $\dfrac{q}{2^{n}}$, with $q$ odd). To see this it is enough to see that if $x\in\mathbb{R}$ is such that $|x|<\dfrac{1}{2^{N}}<ε$ then $$\left|\sum_{k=1}^{\infty}\frac{\{kx\}}{k^{2}}\right|\leq \left|\sum_{k=1}^{S}\frac{\{kx\}}{k^{2}}\right|+\left|\sum_{k=S+1}^{\infty}\frac{\{kx\}}{k^{2}}\right|<\left|\sum_{k=1}^{S}\frac{\{kx\}}{k^{2}}\right|+ε=\left|\sum_{k=1}^{S}\frac{kx}{k^{2}}\right|+ε.$$ Τhese last steps are true because $f(x)$ converges and then there is $J\in \mathbb{N}$ such that $$\left|\sum_{k=n}^{\infty}\frac{\{kx\}}{k^{2}}\right|<ε$$ for all $n\geq J$ and then, if $S=\max\{N,J\}$, $|kx|<\dfrac{k}{2^{S}}<\dfrac{1}{2}$ for all $k\leq S$, and so $|f(x)|<{\mit Γ}ε + ε$ for some constant $\mit Γ$ and therefore $f(x)$ is continuous at $0$. This same argument can be used to prove that $f(x)$ is continuous at any integer and (maybe) at any rational of the form $\dfrac{q}{2^{n}}$. Can I generalize for any real number? Or how can I find where $f(x)$ is not continuous?","Let $f:\mathbb{R}\to\mathbb{R}$ be defined as $$f(x)=\sum_{k=1}^{\infty}\frac{\{kx\}}{k^{2}},$$ where $$\{x\}=\begin{cases}0; & x = \dfrac{n}{2}\ \text{for some odd integer } n\\x - m(x);& \text{otherwise}\end{cases}$$ and $m(x)$ is the nearest integer to $x$. I have already proved that $f$ is continuos at $x=0$ (actually I have proved that it is at any integer and, maybe, at rational numbers of the form $\dfrac{q}{2^{n}}$, with $q$ odd). To see this it is enough to see that if $x\in\mathbb{R}$ is such that $|x|<\dfrac{1}{2^{N}}<ε$ then $$\left|\sum_{k=1}^{\infty}\frac{\{kx\}}{k^{2}}\right|\leq \left|\sum_{k=1}^{S}\frac{\{kx\}}{k^{2}}\right|+\left|\sum_{k=S+1}^{\infty}\frac{\{kx\}}{k^{2}}\right|<\left|\sum_{k=1}^{S}\frac{\{kx\}}{k^{2}}\right|+ε=\left|\sum_{k=1}^{S}\frac{kx}{k^{2}}\right|+ε.$$ Τhese last steps are true because $f(x)$ converges and then there is $J\in \mathbb{N}$ such that $$\left|\sum_{k=n}^{\infty}\frac{\{kx\}}{k^{2}}\right|<ε$$ for all $n\geq J$ and then, if $S=\max\{N,J\}$, $|kx|<\dfrac{k}{2^{S}}<\dfrac{1}{2}$ for all $k\leq S$, and so $|f(x)|<{\mit Γ}ε + ε$ for some constant $\mit Γ$ and therefore $f(x)$ is continuous at $0$. This same argument can be used to prove that $f(x)$ is continuous at any integer and (maybe) at any rational of the form $\dfrac{q}{2^{n}}$. Can I generalize for any real number? Or how can I find where $f(x)$ is not continuous?",,"['real-analysis', 'analysis', 'continuity']"
56,"Prove $\frac{1}{x^2}$ is uniformly continuous on $[1,\infty)$ but not on $(0,1)$.",Prove  is uniformly continuous on  but not on .,"\frac{1}{x^2} [1,\infty) (0,1)","Prove $\frac{1}{x^2}$ is uniformly continuous on $[1,\infty)$ but not on $(0,1)$. Proof On $[1,\infty)$: $$\left|f(x) - f(y)\right| = \left| \frac{1}{x^2} - \frac{1}{y^2} \right| = \frac{(x+y)\left|x -y\right|}{x^2y^2} = \left(\frac{1}{xy^2} + \frac{1}{x^2y}\right)\left|x-y\right|\leq 2 \left|x-y\right| $$ Therefore $f$ is a Lipschitz function on $[1,\infty)$ which implies uniform continuity. On $(0,1)$ : Choose $\epsilon_0 = 1$ and consider the sequences defined by $x_n = \frac{1}{\sqrt{n}}$ and $y_n = \frac{1}{\sqrt{n+1}}$. We see that $\lim(x_n - y_n) = 0$ but $$\left|f(x_n) - f(y_n)\right| = \left|n - (n+1)\right| = 1$$ therefore $f$ is not uniformly continuous on $(0,1)$. Please comment on the validity, readability, and/or style. Thank you.","Prove $\frac{1}{x^2}$ is uniformly continuous on $[1,\infty)$ but not on $(0,1)$. Proof On $[1,\infty)$: $$\left|f(x) - f(y)\right| = \left| \frac{1}{x^2} - \frac{1}{y^2} \right| = \frac{(x+y)\left|x -y\right|}{x^2y^2} = \left(\frac{1}{xy^2} + \frac{1}{x^2y}\right)\left|x-y\right|\leq 2 \left|x-y\right| $$ Therefore $f$ is a Lipschitz function on $[1,\infty)$ which implies uniform continuity. On $(0,1)$ : Choose $\epsilon_0 = 1$ and consider the sequences defined by $x_n = \frac{1}{\sqrt{n}}$ and $y_n = \frac{1}{\sqrt{n+1}}$. We see that $\lim(x_n - y_n) = 0$ but $$\left|f(x_n) - f(y_n)\right| = \left|n - (n+1)\right| = 1$$ therefore $f$ is not uniformly continuous on $(0,1)$. Please comment on the validity, readability, and/or style. Thank you.",,['real-analysis']
57,Indeterminate form: $0^0$,Indeterminate form:,0^0,"It is known that $\lim_{x\to 0^+}x^x=1$, $\lim_{x\to 0^+}0^x=0$ and $\lim_{x\to 0}x^0=1$. So sometimes $0^0$ is left undefined, sometimes defined as $1$. A question then come to my mind: Given $(s_n)_{n\in\mathbb N},(t_n)_{n\in\mathbb N}$ are sequences$,\forall n\in\mathbb N,(s_n)\gt0\land(t_n)\gt0$, $\lim_n s_n=\lim_n t_n=0$. Given $a\in[0,1]$,When will $(s_n^{t_n})\to a$ ? Even giving more example may help. I have found some example, 1)$\forall n\in\mathbb N, s_n=t_n=\frac{1}{n},(s_n^{t_n})\to 1$ 2)$\forall a\in(0,1), \forall n\in\mathbb N, s_n=a^n\land t_n=\frac{1}{n},(s_n^{t_n})\to a$ 3)$\forall n\in\mathbb N, s_n=\frac{1}{n^n}\land t_n=\frac{1}{n},(s_n^{t_n})\to 0$ I think it sounds very 'easy' for $(s_n^{t_n})\to 1$, but are there similarity between the examples? Or Can I even ask for: If $\lim_{x\to 0^+}f(x)=\lim_{x\to 0^+}g(x)=0, a\in\mathbb R$, when will $\lim_{x\to 0^+}f(x)^{g(x)}=a$? Thanks.","It is known that $\lim_{x\to 0^+}x^x=1$, $\lim_{x\to 0^+}0^x=0$ and $\lim_{x\to 0}x^0=1$. So sometimes $0^0$ is left undefined, sometimes defined as $1$. A question then come to my mind: Given $(s_n)_{n\in\mathbb N},(t_n)_{n\in\mathbb N}$ are sequences$,\forall n\in\mathbb N,(s_n)\gt0\land(t_n)\gt0$, $\lim_n s_n=\lim_n t_n=0$. Given $a\in[0,1]$,When will $(s_n^{t_n})\to a$ ? Even giving more example may help. I have found some example, 1)$\forall n\in\mathbb N, s_n=t_n=\frac{1}{n},(s_n^{t_n})\to 1$ 2)$\forall a\in(0,1), \forall n\in\mathbb N, s_n=a^n\land t_n=\frac{1}{n},(s_n^{t_n})\to a$ 3)$\forall n\in\mathbb N, s_n=\frac{1}{n^n}\land t_n=\frac{1}{n},(s_n^{t_n})\to 0$ I think it sounds very 'easy' for $(s_n^{t_n})\to 1$, but are there similarity between the examples? Or Can I even ask for: If $\lim_{x\to 0^+}f(x)=\lim_{x\to 0^+}g(x)=0, a\in\mathbb R$, when will $\lim_{x\to 0^+}f(x)^{g(x)}=a$? Thanks.",,"['real-analysis', 'sequences-and-series', 'limits', 'examples-counterexamples']"
58,Heat equation - Step function initial condition,Heat equation - Step function initial condition,,"Suppose you want to solve the usual heat equation on the real line $[-\infty ,+\infty ]$ \begin{equation} \begin{cases}  \partial_t u(x,t)= \partial_{xx} u(x,t)\\  u(x,0)=f(x)\\ \end{cases}, \end{equation} where the initial condition is given by a step function \begin{equation} f(x)=  \begin{cases} 0 \ \ \ \ \text{if} \ \ x<0\\ 1 \ \ \ \ \text{if} \ \ x>0\\ \end{cases}. \end{equation} By solving the equation using Green's function method it's possible to find a solution of the form \begin{equation} u(x,t)=\frac{1}{2}\left( 1+\mathrm{erf} \left( \frac{x}{\sqrt{4t}} \right) \right) \end{equation} A short derivation of this result can be found at this link . It seems to me that the limit $(x,t)\to (0,0)$ of this function does not exist (in the sense of limit of functions); this is of course in conflict with the IC imposed. Why is this happening? Am I forgetting something while calculating the solution? Or am I misinterpreting the word ""limit"" for this particular case?","Suppose you want to solve the usual heat equation on the real line $[-\infty ,+\infty ]$ \begin{equation} \begin{cases}  \partial_t u(x,t)= \partial_{xx} u(x,t)\\  u(x,0)=f(x)\\ \end{cases}, \end{equation} where the initial condition is given by a step function \begin{equation} f(x)=  \begin{cases} 0 \ \ \ \ \text{if} \ \ x<0\\ 1 \ \ \ \ \text{if} \ \ x>0\\ \end{cases}. \end{equation} By solving the equation using Green's function method it's possible to find a solution of the form \begin{equation} u(x,t)=\frac{1}{2}\left( 1+\mathrm{erf} \left( \frac{x}{\sqrt{4t}} \right) \right) \end{equation} A short derivation of this result can be found at this link . It seems to me that the limit $(x,t)\to (0,0)$ of this function does not exist (in the sense of limit of functions); this is of course in conflict with the IC imposed. Why is this happening? Am I forgetting something while calculating the solution? Or am I misinterpreting the word ""limit"" for this particular case?",,"['real-analysis', 'partial-differential-equations']"
59,$X$ is the space of all continuous functions with the norm $||f||=\sup_{t\ge0}e^{kt}|f(t)|<\infty.$ Is $X$ a Banach space?,is the space of all continuous functions with the norm  Is  a Banach space?,X ||f||=\sup_{t\ge0}e^{kt}|f(t)|<\infty. X,"Fix $k\in \mathbb{R}$ and let $X$ be the space of all continuous functions $f:[0,\infty) \to\mathbb{R}$ s.t. $$\|f\|=\sup_{t\ge0}e^{kt}|f(t)|<\infty.$$ Is $X$ a Banach Space? I think $X$ is Banach if $k\ge0$ and I had a brief proof. But I have no idea whether $X$ is Banach when $k<0$. Can someone give a brief proof or a counterexample?","Fix $k\in \mathbb{R}$ and let $X$ be the space of all continuous functions $f:[0,\infty) \to\mathbb{R}$ s.t. $$\|f\|=\sup_{t\ge0}e^{kt}|f(t)|<\infty.$$ Is $X$ a Banach Space? I think $X$ is Banach if $k\ge0$ and I had a brief proof. But I have no idea whether $X$ is Banach when $k<0$. Can someone give a brief proof or a counterexample?",,"['real-analysis', 'functional-analysis', 'analysis', 'banach-spaces']"
60,Estimating the limit $x_{n+1} =x_n - x_{n}^{n+1} $,Estimating the limit,x_{n+1} =x_n - x_{n}^{n+1} ,"I wonder whether there is a general method for accurately estimating the limit of the sequence: \begin{equation} x_{n+1} = x_n - x_{n}^{n+1}, \forall x_1 \in (0,1) \end{equation} After showing that the limit exists, since $ x_n $ is decreasing and bounded, I managed to derive a lower-bound. In particular, I used the fact that: \begin{equation} \frac{x_{n+1}}{x_n} = 1-x_{n}^n \tag{1} \end{equation} Using $(1)$ we obtain: \begin{equation} \frac{x_N}{x_{N-1}}...\frac{x_2}{x_1}=\prod_{n=1}^{N} (1-x_{n}^n)=\frac{x_N}{x_1} \tag{2} \end{equation} From this we deduce: \begin{equation} \begin{split} \lim_{N \to \infty} x_N & = \lim_{N \to \infty}x_1 \prod_{n=1}^{N} (1-x_{n}^n) \\  & = x_1 (\lim_{N \to \infty} \prod_{n=1}^{N} e^{\ln (1-x_{n}^n)}) \\ & = x_1 (\lim_{N \to \infty} e^{\sum_{n=1}^N\ln (1-x_{n}^n)}) \end{split} \tag{3}\end{equation} Using the following facts: \begin{cases} \sum_{n=1}^{N} \ln(1-x_{n}^n) \geq \sum_{n=1}^{N} \ln(1-x_{1}^n),\\ x \approx 0 \implies \ln(1+x) \approx x \\ \tag{4}\end{cases} We may deduce that for $M$ sufficiently large: \begin{equation} \sum_{n=1}^{\infty}\ln (1-x_{n}^n) \geq \sum_{n=1}^{M} \ln(1-x_{1}^n)-\sum_{n=M}^\infty x_{1}^n \tag{5} \end{equation} And using $(5)$ we have a useful lower-bound. However, I wonder whether there's a more direct integration technique which can give me a good approximation to $(3)$.","I wonder whether there is a general method for accurately estimating the limit of the sequence: \begin{equation} x_{n+1} = x_n - x_{n}^{n+1}, \forall x_1 \in (0,1) \end{equation} After showing that the limit exists, since $ x_n $ is decreasing and bounded, I managed to derive a lower-bound. In particular, I used the fact that: \begin{equation} \frac{x_{n+1}}{x_n} = 1-x_{n}^n \tag{1} \end{equation} Using $(1)$ we obtain: \begin{equation} \frac{x_N}{x_{N-1}}...\frac{x_2}{x_1}=\prod_{n=1}^{N} (1-x_{n}^n)=\frac{x_N}{x_1} \tag{2} \end{equation} From this we deduce: \begin{equation} \begin{split} \lim_{N \to \infty} x_N & = \lim_{N \to \infty}x_1 \prod_{n=1}^{N} (1-x_{n}^n) \\  & = x_1 (\lim_{N \to \infty} \prod_{n=1}^{N} e^{\ln (1-x_{n}^n)}) \\ & = x_1 (\lim_{N \to \infty} e^{\sum_{n=1}^N\ln (1-x_{n}^n)}) \end{split} \tag{3}\end{equation} Using the following facts: \begin{cases} \sum_{n=1}^{N} \ln(1-x_{n}^n) \geq \sum_{n=1}^{N} \ln(1-x_{1}^n),\\ x \approx 0 \implies \ln(1+x) \approx x \\ \tag{4}\end{cases} We may deduce that for $M$ sufficiently large: \begin{equation} \sum_{n=1}^{\infty}\ln (1-x_{n}^n) \geq \sum_{n=1}^{M} \ln(1-x_{1}^n)-\sum_{n=M}^\infty x_{1}^n \tag{5} \end{equation} And using $(5)$ we have a useful lower-bound. However, I wonder whether there's a more direct integration technique which can give me a good approximation to $(3)$.",,['real-analysis']
61,"Two non-empty, compact, perfect sets in real line, with empty interior, can be mapped into each other by an order preserving isomorphism of real line","Two non-empty, compact, perfect sets in real line, with empty interior, can be mapped into each other by an order preserving isomorphism of real line",,"Let $A,B$ be non-empty, dense in itself (i.e. every point is a limit point), compact subsets of $\mathbb R$ with empty interior. Then how to show that there exists order preserving isomorphism  $f: \mathbb R \to \mathbb R$ such that $f(A)=B$ ? I can see that both $A,B$ are perfect sets ( https://en.wikipedia.org/wiki/Perfect_set ) , so has cardinality of continuum. Also, it is enough to take one of $A$ or $B$ to be the Cantor set. I can show that $A, B$ are order homeomorphic with the cantor set, but I don't know if such a homeomorphism can be extended to whole real line. I am unable to derive anything else. Please help . Thanks in advance","Let $A,B$ be non-empty, dense in itself (i.e. every point is a limit point), compact subsets of $\mathbb R$ with empty interior. Then how to show that there exists order preserving isomorphism  $f: \mathbb R \to \mathbb R$ such that $f(A)=B$ ? I can see that both $A,B$ are perfect sets ( https://en.wikipedia.org/wiki/Perfect_set ) , so has cardinality of continuum. Also, it is enough to take one of $A$ or $B$ to be the Cantor set. I can show that $A, B$ are order homeomorphic with the cantor set, but I don't know if such a homeomorphism can be extended to whole real line. I am unable to derive anything else. Please help . Thanks in advance",,"['real-analysis', 'general-topology']"
62,Toeplitz Theorem.,Toeplitz Theorem.,,"Theorem: Let $a_n$ be a real sequence convergent to $a \in \mathbb{R}$. Let   $c_{k,n}$ (where $1\le k \le n$) be a sequence such that: $$\quad \forall k \lim_{n \to \infty}c_{k,n} = 0$$ $$\quad \lim_{n \to > \infty} \sum_{k=1}^n c_{k,n} = 1$$ $$\quad \exists M>0 : \forall n\ \ > \sum_{k=1}^n |c_{k,n}| \le M$$ Then $\lim_{n \to \infty}s_n =a$, where $$s_n \equiv \sum_{k=1}^n  c_{k,n} \cdot a_k.$$ The author of my textbook begins by observing that if $a_n$ is a constant sequence then $$s_n=a\sum_{k=1}^n  c_{k,n}$$ implying that $\lim_{n\to \infty}s_n=a.$ He then remarks that it is enough to consider the case when the sequence is equal to zero. I don't understand the reasoning behind this argument and would, therefore, be grateful if someone could explain this step in the proof. Here is the complete proof by the way.","Theorem: Let $a_n$ be a real sequence convergent to $a \in \mathbb{R}$. Let   $c_{k,n}$ (where $1\le k \le n$) be a sequence such that: $$\quad \forall k \lim_{n \to \infty}c_{k,n} = 0$$ $$\quad \lim_{n \to > \infty} \sum_{k=1}^n c_{k,n} = 1$$ $$\quad \exists M>0 : \forall n\ \ > \sum_{k=1}^n |c_{k,n}| \le M$$ Then $\lim_{n \to \infty}s_n =a$, where $$s_n \equiv \sum_{k=1}^n  c_{k,n} \cdot a_k.$$ The author of my textbook begins by observing that if $a_n$ is a constant sequence then $$s_n=a\sum_{k=1}^n  c_{k,n}$$ implying that $\lim_{n\to \infty}s_n=a.$ He then remarks that it is enough to consider the case when the sequence is equal to zero. I don't understand the reasoning behind this argument and would, therefore, be grateful if someone could explain this step in the proof. Here is the complete proof by the way.",,['real-analysis']
63,"Prove there is an increasing function on closed bounded interval that is continuous only at points in $[a,b] \setminus C$",Prove there is an increasing function on closed bounded interval that is continuous only at points in,"[a,b] \setminus C","Let $C$ be a countable subset of $(a,b)$ . Then there is an increasing function on $(a,b)$ that is continuous only on $(a,b)\setminus C$ This is an example from Royden's real analysis book. The function defined on $(a,b)$ is $f(x)=\sum_{\{n:q_n \le x\}}\frac{1}{2^n}$ where $\{q_n\}$ is an enumeration of $C$ . To show continuity on $(a,b)\setminus C$ he says let $x_0$ be in $(a,b)\setminus C$ and let $n$ be any natural number. Then there exists an interval, $I$ , that contains $x_0$ . In addition, $q_n$ is not in $I$ for $1\le k \le n$ . Then this implys that $|f(x)-f(x_0)| < \frac{1}{2^n}$ for $x \in I$ . I understand this part. Then there is a problem right after this proof says: Let $C$ be a countable subset of the nondegenerate closed bounded interval $[a,b]$ . Then there is an increasing  function on $[a,b]$ that is continuous only on $[a,b]\setminus C$ . I was wondering, if we define $$f(a)=0 \text{ and } f(b)=\sum^{\infty}_{n=1}\frac{1}{2^n}=1$$ Is the proof still hold? Did I miss something here? Are the proof of these two problems similar? Thank you! Thank you for the solution I accepted, and I added something new here. Show that there is a strictly increasing function on $[0,1]$ that is continuous only at the irrationals in $[0,1]$ . Let $f$ be a monotone function on a subset $E$ of $\mathbb R$ . Show that $f$ is continuous except possibly at a countable number of points n $E$ . Let $E$ be a subset of $\mathbb R$ and $C$ a countable subset of $E$ . Is there a monotone function on $E$ that is continuous only at points $E \setminus C$ ? These are four successive problems on Page 109 from Royden's real analysis book. So I think for problem 2, we can just use the result from problem 1 and let $C$ be the rationals. Did I miss something here? And for 3 and 4, what are the difference between them? I mean if 3 is true which means we do have this function. Or did I misunderstand something here? Thank you!","Let be a countable subset of . Then there is an increasing function on that is continuous only on This is an example from Royden's real analysis book. The function defined on is where is an enumeration of . To show continuity on he says let be in and let be any natural number. Then there exists an interval, , that contains . In addition, is not in for . Then this implys that for . I understand this part. Then there is a problem right after this proof says: Let be a countable subset of the nondegenerate closed bounded interval . Then there is an increasing  function on that is continuous only on . I was wondering, if we define Is the proof still hold? Did I miss something here? Are the proof of these two problems similar? Thank you! Thank you for the solution I accepted, and I added something new here. Show that there is a strictly increasing function on that is continuous only at the irrationals in . Let be a monotone function on a subset of . Show that is continuous except possibly at a countable number of points n . Let be a subset of and a countable subset of . Is there a monotone function on that is continuous only at points ? These are four successive problems on Page 109 from Royden's real analysis book. So I think for problem 2, we can just use the result from problem 1 and let be the rationals. Did I miss something here? And for 3 and 4, what are the difference between them? I mean if 3 is true which means we do have this function. Or did I misunderstand something here? Thank you!","C (a,b) (a,b) (a,b)\setminus C (a,b) f(x)=\sum_{\{n:q_n \le x\}}\frac{1}{2^n} \{q_n\} C (a,b)\setminus C x_0 (a,b)\setminus C n I x_0 q_n I 1\le k \le n |f(x)-f(x_0)| < \frac{1}{2^n} x \in I C [a,b] [a,b] [a,b]\setminus C f(a)=0 \text{ and } f(b)=\sum^{\infty}_{n=1}\frac{1}{2^n}=1 [0,1] [0,1] f E \mathbb R f E E \mathbb R C E E E \setminus C C","['calculus', 'real-analysis', 'functional-analysis']"
64,Prove that if $\lim \frac{x_{n+1}}{x_n}=L<1$ then it converges to $0$.,Prove that if  then it converges to .,\lim \frac{x_{n+1}}{x_n}=L<1 0,"I'm stuck with this problem from ""An Introduction to Analysis by James Kirkwood"" on page 48 exercise 24. Let $\{x_n\}$ be a sequence of real positive numbers such that $\displaystyle\lim_{n\to \infty} \frac{x_{n+1}}{x_n}=L$ exists. Prove that if $L<1$ then $\{x_n\}$ converges to $0$. I'm thinking (I got it from this similar question ) By definiton of limit I have that for any $\epsilon>0$ in particular for $0<\epsilon<1$ such that $L+\epsilon < 1$, exists $N_{\epsilon}\in \mathbb{N}$ such that $n>N_{\epsilon}$ implies that, $$|\frac{x_{n+1}}{x_n}-L|<\epsilon \Rightarrow \frac{x_{n+1}}{x_n}<L+\epsilon \Rightarrow x_{n+1}<(L+\epsilon)x_n<(L+\epsilon)^{n}x_n \\ \text{This will always happen even as $n \rightarrow \infty$, as long as $n>N_{\epsilon}$} $$ Since $\lim_{ N_{\epsilon}\to\infty }r^{N_{\epsilon}}=0$, when $0<r<1$, and because $0<(L+\epsilon)<1$ we must have that $x_{x+1}<0$ however because $x_n$ is a sequence of real positive numbers we have that $x_{n+1}$ must also be always greater than $0$. So $\{x_{n+1}\}$ converges to $0$, and therefore $\{x_n\}$ converges to $0$. But I'm not sure about this thinking, since I think a possible contradiction might arise with $L$ being zero or something. Your thoughts?","I'm stuck with this problem from ""An Introduction to Analysis by James Kirkwood"" on page 48 exercise 24. Let $\{x_n\}$ be a sequence of real positive numbers such that $\displaystyle\lim_{n\to \infty} \frac{x_{n+1}}{x_n}=L$ exists. Prove that if $L<1$ then $\{x_n\}$ converges to $0$. I'm thinking (I got it from this similar question ) By definiton of limit I have that for any $\epsilon>0$ in particular for $0<\epsilon<1$ such that $L+\epsilon < 1$, exists $N_{\epsilon}\in \mathbb{N}$ such that $n>N_{\epsilon}$ implies that, $$|\frac{x_{n+1}}{x_n}-L|<\epsilon \Rightarrow \frac{x_{n+1}}{x_n}<L+\epsilon \Rightarrow x_{n+1}<(L+\epsilon)x_n<(L+\epsilon)^{n}x_n \\ \text{This will always happen even as $n \rightarrow \infty$, as long as $n>N_{\epsilon}$} $$ Since $\lim_{ N_{\epsilon}\to\infty }r^{N_{\epsilon}}=0$, when $0<r<1$, and because $0<(L+\epsilon)<1$ we must have that $x_{x+1}<0$ however because $x_n$ is a sequence of real positive numbers we have that $x_{n+1}$ must also be always greater than $0$. So $\{x_{n+1}\}$ converges to $0$, and therefore $\{x_n\}$ converges to $0$. But I'm not sure about this thinking, since I think a possible contradiction might arise with $L$ being zero or something. Your thoughts?",,"['real-analysis', 'sequences-and-series']"
65,"If $\sum\limits_i\sum\limits_j\alpha_{ij}x_iy_j$ converges for every square integrable $(x_n)$ and $(y_n)$, then the order of the sums commutes","If  converges for every square integrable  and , then the order of the sums commutes",\sum\limits_i\sum\limits_j\alpha_{ij}x_iy_j (x_n) (y_n),"Let $(\alpha_{ij})$ be a square infinite matrix such that for all $x=(\xi_{n}),y=(\eta_{n}) \in \ell ^{2}$ we have that ${\displaystyle \sum_{i=0}^{\infty}\sum_{j=0}^{\infty}\alpha_{ij}\xi_{i}\eta_{j}}$ converges. The question: Is it correct to say that $$\sum_{i=0}^{\infty}\sum_{j=0}^{\infty}\alpha_{ij}\xi_{i}\eta_{j}=\sum_{j=0}^{\infty}\sum_{i=0}^{\infty}\alpha_{ij}\xi_{i}\eta_{j} \:\:?. \tag{$\bigstar$}$$ Remark: We know that if the serie converges absolutely, then every rearrangement  converges. I do not know if in this case it is necessary to show that the series converges absolutely to be able to demonstrate $(\bigstar)$, however, I have not been able to show this absolute convergence.","Let $(\alpha_{ij})$ be a square infinite matrix such that for all $x=(\xi_{n}),y=(\eta_{n}) \in \ell ^{2}$ we have that ${\displaystyle \sum_{i=0}^{\infty}\sum_{j=0}^{\infty}\alpha_{ij}\xi_{i}\eta_{j}}$ converges. The question: Is it correct to say that $$\sum_{i=0}^{\infty}\sum_{j=0}^{\infty}\alpha_{ij}\xi_{i}\eta_{j}=\sum_{j=0}^{\infty}\sum_{i=0}^{\infty}\alpha_{ij}\xi_{i}\eta_{j} \:\:?. \tag{$\bigstar$}$$ Remark: We know that if the serie converges absolutely, then every rearrangement  converges. I do not know if in this case it is necessary to show that the series converges absolutely to be able to demonstrate $(\bigstar)$, however, I have not been able to show this absolute convergence.",,"['real-analysis', 'sequences-and-series', 'functional-analysis', 'convergence-divergence', 'infinite-matrices']"
66,Prove that an elementary function is non-decreasing,Prove that an elementary function is non-decreasing,,"For $p > 0$, define : $f(p) := \int_0^1{p (1-x)^{p^2 + 2p}e^{\frac{(1+p)^2}{2} x(x+2)}}dx$ The question is : how to prove that $f$ is a non-decreasing function of p? It seems to be the case numerically. Note that one can show that $p \mapsto f(p)/p$ is a non-increasing function because $f(p)/p = \int_0^1{\frac{1}{1-x}e^{(1+p)^2[\ln{(1-x)} + \frac{x(x+2)}{2}]}dx}$ and $\ln{(1-x)} + \frac{x(x+2)}{2}$ is always negative. So the difficulty here is to understand why does the factor $p$ help! Thanks a lot for any clue!","For $p > 0$, define : $f(p) := \int_0^1{p (1-x)^{p^2 + 2p}e^{\frac{(1+p)^2}{2} x(x+2)}}dx$ The question is : how to prove that $f$ is a non-decreasing function of p? It seems to be the case numerically. Note that one can show that $p \mapsto f(p)/p$ is a non-increasing function because $f(p)/p = \int_0^1{\frac{1}{1-x}e^{(1+p)^2[\ln{(1-x)} + \frac{x(x+2)}{2}]}dx}$ and $\ln{(1-x)} + \frac{x(x+2)}{2}$ is always negative. So the difficulty here is to understand why does the factor $p$ help! Thanks a lot for any clue!",,"['real-analysis', 'integration']"
67,When does a limit of a sequence equal both its lim sup and lim inf?,When does a limit of a sequence equal both its lim sup and lim inf?,,"Suppose $(a_n)_{n\ge1}$ is a positive (so non-zero), real sequence with $\lim_{n\rightarrow\infty}a_n=L$, where $L\in[0,\infty]$. Is this equivalent with $\limsup_{n\rightarrow\infty}a_n=\liminf_{n\rightarrow\infty}a_n=L$? In particular, does this hold for the infinity case? If so, why? Thank you!","Suppose $(a_n)_{n\ge1}$ is a positive (so non-zero), real sequence with $\lim_{n\rightarrow\infty}a_n=L$, where $L\in[0,\infty]$. Is this equivalent with $\limsup_{n\rightarrow\infty}a_n=\liminf_{n\rightarrow\infty}a_n=L$? In particular, does this hold for the infinity case? If so, why? Thank you!",,"['real-analysis', 'sequences-and-series', 'limits', 'limsup-and-liminf']"
68,Continuity of ball measure in metric spaces,Continuity of ball measure in metric spaces,,"Let $(X,d)$ a metric space and $\mu$ a Borelian probability non atomic measure on $X$. Fixed $x\in X$, the following application $r\in[0,1]\mapsto\mu(B(x,r))$ is continuous? where $B(x,r)=\{y\in X:d(x,y)<r\}$. I would appreciate some counterexample or some suggestion for the proof.","Let $(X,d)$ a metric space and $\mu$ a Borelian probability non atomic measure on $X$. Fixed $x\in X$, the following application $r\in[0,1]\mapsto\mu(B(x,r))$ is continuous? where $B(x,r)=\{y\in X:d(x,y)<r\}$. I would appreciate some counterexample or some suggestion for the proof.",,"['real-analysis', 'metric-spaces', 'continuity']"
69,How is continuity different between a sequence of points in $\mathbb{R}^n$ and a sequence of functions in a function space?,How is continuity different between a sequence of points in  and a sequence of functions in a function space?,\mathbb{R}^n,"For any continuous function $f(x)$ we have that $$ (*) \quad \quad \lim_{x\to x_0} f(x) =  f(\lim_{x\to x_0}x) = f(x_0). $$ Because of this, I always believed that by the continuity of the norm function, for a sequence of functions $f_n$ converging to some function $f$, that we could say $$ (**) \quad \quad \lim_{n\to \infty} ||f_n||_p =  || \lim_{n\to \infty} f_n||_p = ||f||_p. $$ So is there a difference between $(*)$ and $(**)$? Are we not allowed to perform the switching of limits in $(**)$, even though the norm is continuous? On a related note I also always thought we could do the following for a sequence of functions in a Hilbert space: $$ \lim_{n\to \infty} \langle f_n, g \rangle = \langle   \lim_{n\to \infty} f_n, g \rangle = \langle f, g \rangle. $$ Is this also not true?","For any continuous function $f(x)$ we have that $$ (*) \quad \quad \lim_{x\to x_0} f(x) =  f(\lim_{x\to x_0}x) = f(x_0). $$ Because of this, I always believed that by the continuity of the norm function, for a sequence of functions $f_n$ converging to some function $f$, that we could say $$ (**) \quad \quad \lim_{n\to \infty} ||f_n||_p =  || \lim_{n\to \infty} f_n||_p = ||f||_p. $$ So is there a difference between $(*)$ and $(**)$? Are we not allowed to perform the switching of limits in $(**)$, even though the norm is continuous? On a related note I also always thought we could do the following for a sequence of functions in a Hilbert space: $$ \lim_{n\to \infty} \langle f_n, g \rangle = \langle   \lim_{n\to \infty} f_n, g \rangle = \langle f, g \rangle. $$ Is this also not true?",,"['real-analysis', 'functional-analysis', 'measure-theory', 'normed-spaces']"
70,Baby rudin theorem 3.6: proof verification,Baby rudin theorem 3.6: proof verification,,"Theorem 3.6 in Rudin's principles of mathematical analysis says the following thing: (a) If $(p_n)$ is a sequence in a compact metric space $X$, then some   subsequence of $(p_n)$ converges to a point of $X$ I will now provide the worked out proofs, and I wonder if someone could verify whether I understood the details left out correctly. Before I start the proof, let me introduce the notation $N_r(p)$, which is the open ball (or neighborhood) with center $p$ and radius $r$ Proof : (a) Define $E:= \{p_n \in X\mid n \in \mathbb{N}\}$ We consider two cases: $\boxed{1}$ $E$ is finite In this case, since a sequence is infinite, it must follow that there is an element $p \in E$ that occurs an infinite amount of times in the sequence. So, there are integers $n_1 <n_2 <\dots$ such that $p_{n_i} = p$ for $i = 1,2,3, \dots$. So, the sequence $(p_{n_i})_{i }$ is a constant subsequence with limit $p \in X$ $\boxed{2}$ $E$ is infinite By a theorem that is already proven (theorem 2.37), the set $E$ has a limit point $p \in X$, since $X$ is compact. Now, we construct a subsequence that converges to $p$. Since $p$ is a limit point of E, for any neighborhood $N$ of $p$, there are infinitely many point in the intersection $E \cap N$. Because of this, we can find an element in $N_1(p) \cap E$, meaning that we can pick an element of the sequence on distance less than $1$ from $p$.  Call this element $p_{n_1}$. Now, pick an element of the sequence on distance less than $1/2$ from $p$. Moreover, pick this element such that it occurs after $p_{n_1}$ in the sequence. This is certainly possible, because there are an infinite elements on such a distance and only a finite amount of elements  occur before $p_{n_1}$ in the sequence. Call this element $p_{n_2}$ Continuing in this way (i.e. every time, we pick a point $p_{n_i}$ such that $d(p,p_{n_i}) < 1/i$, the argumentation to make this rigorous can be found in the previous paragraphe), we obtain the existence of a subsequence $(p_{n_i})_i$ with $d(p,p_{n_i}) < 1/i$. We now prove that $p_{n_i} \to p$ . For this, let $\epsilon > 0$. By the archimedian property of the real numbers, there is a positive integer $I$ such that $1/I < \epsilon$. So, let $i > I$, then $1/i < 1/I < \epsilon$, such that $d(p,p_{n_i}) < \epsilon$. This shows that $p_{n_i} \to p \quad \square$ Questions: Have I filled up the details correctly?   Is the last paragraph correct?","Theorem 3.6 in Rudin's principles of mathematical analysis says the following thing: (a) If $(p_n)$ is a sequence in a compact metric space $X$, then some   subsequence of $(p_n)$ converges to a point of $X$ I will now provide the worked out proofs, and I wonder if someone could verify whether I understood the details left out correctly. Before I start the proof, let me introduce the notation $N_r(p)$, which is the open ball (or neighborhood) with center $p$ and radius $r$ Proof : (a) Define $E:= \{p_n \in X\mid n \in \mathbb{N}\}$ We consider two cases: $\boxed{1}$ $E$ is finite In this case, since a sequence is infinite, it must follow that there is an element $p \in E$ that occurs an infinite amount of times in the sequence. So, there are integers $n_1 <n_2 <\dots$ such that $p_{n_i} = p$ for $i = 1,2,3, \dots$. So, the sequence $(p_{n_i})_{i }$ is a constant subsequence with limit $p \in X$ $\boxed{2}$ $E$ is infinite By a theorem that is already proven (theorem 2.37), the set $E$ has a limit point $p \in X$, since $X$ is compact. Now, we construct a subsequence that converges to $p$. Since $p$ is a limit point of E, for any neighborhood $N$ of $p$, there are infinitely many point in the intersection $E \cap N$. Because of this, we can find an element in $N_1(p) \cap E$, meaning that we can pick an element of the sequence on distance less than $1$ from $p$.  Call this element $p_{n_1}$. Now, pick an element of the sequence on distance less than $1/2$ from $p$. Moreover, pick this element such that it occurs after $p_{n_1}$ in the sequence. This is certainly possible, because there are an infinite elements on such a distance and only a finite amount of elements  occur before $p_{n_1}$ in the sequence. Call this element $p_{n_2}$ Continuing in this way (i.e. every time, we pick a point $p_{n_i}$ such that $d(p,p_{n_i}) < 1/i$, the argumentation to make this rigorous can be found in the previous paragraphe), we obtain the existence of a subsequence $(p_{n_i})_i$ with $d(p,p_{n_i}) < 1/i$. We now prove that $p_{n_i} \to p$ . For this, let $\epsilon > 0$. By the archimedian property of the real numbers, there is a positive integer $I$ such that $1/I < \epsilon$. So, let $i > I$, then $1/i < 1/I < \epsilon$, such that $d(p,p_{n_i}) < \epsilon$. This shows that $p_{n_i} \to p \quad \square$ Questions: Have I filled up the details correctly?   Is the last paragraph correct?",,"['real-analysis', 'sequences-and-series']"
71,Differentiation and asymptotic equivalents.,Differentiation and asymptotic equivalents.,,"Following this post on Meta , I am going to (semi) regularly ask questions from competitive mathematics exams, on a variety of topics; and provide a solution a few days later. The goal is not only to list interesting (I hope) exercises for the sake of self-study, but also to obtain (again, hopefully) a variety of techniques to solve them. Differentiating asymptotic equivalents. Let $f\colon\mathbb{R}\to\mathbb{R}$ be continuous and non-decreasing. For $x\in\mathbb{R}$ , let $F(x)=\int_0^x  f$ . Suppose there exists $\alpha > 0$ such that $F(x)\displaystyle\operatorname*{\sim}_{x\to\infty}\frac{x^\alpha}{\alpha}$ . Show that $f(x)\displaystyle\operatorname*{\sim}_{x\to\infty}x^{\alpha-1}$ . Now, assume that $F(x)=\frac{x^2}{2}+o(x)$ when $x\to\infty$ . Show that $f(x) = x+o(\sqrt{x})$ . Give counterexamples for 1. (and 2.) when $f$ is not assumed monotone. Reference: Exercise 4.58 in Exercices de mathématiques: oraux X-ENS (Analyse I) , by Francinou, Gianella, and Nicolas (2014) ISBN 978-2842252137.","Following this post on Meta , I am going to (semi) regularly ask questions from competitive mathematics exams, on a variety of topics; and provide a solution a few days later. The goal is not only to list interesting (I hope) exercises for the sake of self-study, but also to obtain (again, hopefully) a variety of techniques to solve them. Differentiating asymptotic equivalents. Let be continuous and non-decreasing. For , let . Suppose there exists such that . Show that . Now, assume that when . Show that . Give counterexamples for 1. (and 2.) when is not assumed monotone. Reference: Exercise 4.58 in Exercices de mathématiques: oraux X-ENS (Analyse I) , by Francinou, Gianella, and Nicolas (2014) ISBN 978-2842252137.",f\colon\mathbb{R}\to\mathbb{R} x\in\mathbb{R} F(x)=\int_0^x  f \alpha > 0 F(x)\displaystyle\operatorname*{\sim}_{x\to\infty}\frac{x^\alpha}{\alpha} f(x)\displaystyle\operatorname*{\sim}_{x\to\infty}x^{\alpha-1} F(x)=\frac{x^2}{2}+o(x) x\to\infty f(x) = x+o(\sqrt{x}) f,"['real-analysis', 'derivatives', 'asymptotics']"
72,Diffeomorphisms from spheres in $\mathbb{R}^n$,Diffeomorphisms from spheres in,\mathbb{R}^n,"I'm stuck  in the following exercise: Let $f:B_r (x_0) \subset \mathbb{R}^n \to \mathbb{R}^n$ be a diffeomorphism from $B_r (x_0)$ onto $f(B_r (x_0))$. If $\|f'(x)^{-1}\|\leq M$ for all $x\in B_r (x_0)$ and $|f(x_0)|\leq r/M$, then $0\in f(B_r (x_0))$. I've tried using the mean-value inequality to find a sequence converging to zero, but it doesn't seem to be any hypothesis on the bound $M$, so I'm not sure if that's the right way to prove it. Since the only point of the domain I know something about is the center $x_0$, I guess I should try to show the origin belongs to some neighborhood of $f(x_0)$, but I'm not sure how to do it.  Could someone help me?","I'm stuck  in the following exercise: Let $f:B_r (x_0) \subset \mathbb{R}^n \to \mathbb{R}^n$ be a diffeomorphism from $B_r (x_0)$ onto $f(B_r (x_0))$. If $\|f'(x)^{-1}\|\leq M$ for all $x\in B_r (x_0)$ and $|f(x_0)|\leq r/M$, then $0\in f(B_r (x_0))$. I've tried using the mean-value inequality to find a sequence converging to zero, but it doesn't seem to be any hypothesis on the bound $M$, so I'm not sure if that's the right way to prove it. Since the only point of the domain I know something about is the center $x_0$, I guess I should try to show the origin belongs to some neighborhood of $f(x_0)$, but I'm not sure how to do it.  Could someone help me?",,"['real-analysis', 'inverse-function', 'diffeomorphism']"
73,"Theorem 6.12 (e) in Baby Rudin: If $f \in \mathscr{R}(\alpha)$ and $c > 0$, then $\ldots$","Theorem 6.12 (e) in Baby Rudin: If  and , then",f \in \mathscr{R}(\alpha) c > 0 \ldots,"If $f \in \mathscr{R}(\alpha)$ on $[a, b]$ and if $c$ is a positive constant, then $f \in \mathscr{R}(c \alpha)$ , and $$ \int_a^b f d (c \alpha) = c \int_a^b f d \alpha.$$ This is part of Theorem 6.12 (e) in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: My proof: Here is my Math Stack Exchange post on the definition of integral as used in Baby Rudin. As $c > 0$ and as $\alpha$ is a monotonically increasing function on $[a, b]$ , so $c \alpha$ is also  monotonically increasing on $[a, b]$ . And, for any partition $P = \left\{ \ x_0, x_1, \ldots, x_n \ \right\}$ of $[a, b]$ , we also have $$  \begin{align} L(P, f, c \alpha) &= \sum_{i=1}^n \left[ \left(  \inf_{x_{i-1} \leq x \leq x_i} f(x) \right) \left[ c \alpha\left( x_i \right) - c \alpha\left( x_{i-1} \right) \right] \right] \\  &=  c \sum_{i=1}^n \left(  \inf_{x_{i-1} \leq x \leq x_i} f(x) \right) \left[  \alpha\left( x_i \right) -  \alpha\left( x_{i-1} \right) \right]  \\  &= c L(P, f, \alpha); \end{align} $$ that is, $$  L(P, f, c \alpha) = c L(P, f, \alpha ) \ \mbox{ and similarly} \   U(P, f, c \alpha) = c U(P, f, \alpha). \tag{1} $$ Now as $f \in \mathscr{R}(\alpha)$ on $[a, b]$ , so for every real number $\varepsilon > 0$ we can find a partition $P$ of $[a, b]$ such that $$ U(P, f, \alpha) - L(P, f, \alpha) < { \varepsilon \over c }, $$ and so from (1) we conclude that for this same partition $P$ of $[a, b]$ , we have $$ U(P, f, c \alpha) - L(P, f, c \alpha) < \varepsilon, \tag{2} $$ from which it follows that $f \in \mathscr{R}( c \alpha)$ . Also from (1) and  (2) we see that \begin{align} \int_a^b f d (c \alpha) &\leq U(P, f, c \alpha) \\ &<  L(P, f, c \alpha ) + \varepsilon \\ &= c L(P, f, \alpha) + \varepsilon \\  &\leq c \int_a^b f d \alpha + \varepsilon \end{align} for every real number $\varepsilon > 0$ , which implies that $$ \int_a^b f d (c \alpha) \leq c \int_a^b f d \alpha. \tag{A}$$ And, again from (1) and  (2) we also have \begin{align} c \int_a^b f d \alpha & \leq c U(P, f, \alpha) \\ &= U(P, f, c \alpha) \\ &< L(P, f, c \alpha) + \varepsilon \\ &\leq \int_a^b f d (c \alpha) + \varepsilon  \end{align} for every real number $\varepsilon > 0$ , which implies that $$ c \int_a^b f d \alpha \leq \int_a^b f d (c \alpha). \tag{B}$$ From (A) and (B) the required result follows. Is my proof satisfactory enough?","If on and if is a positive constant, then , and This is part of Theorem 6.12 (e) in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: My proof: Here is my Math Stack Exchange post on the definition of integral as used in Baby Rudin. As and as is a monotonically increasing function on , so is also  monotonically increasing on . And, for any partition of , we also have that is, Now as on , so for every real number we can find a partition of such that and so from (1) we conclude that for this same partition of , we have from which it follows that . Also from (1) and  (2) we see that for every real number , which implies that And, again from (1) and  (2) we also have for every real number , which implies that From (A) and (B) the required result follows. Is my proof satisfactory enough?","f \in \mathscr{R}(\alpha) [a, b] c f \in \mathscr{R}(c \alpha)  \int_a^b f d (c \alpha) = c \int_a^b f d \alpha. c > 0 \alpha [a, b] c \alpha [a, b] P = \left\{ \ x_0, x_1, \ldots, x_n \ \right\} [a, b]  
\begin{align}
L(P, f, c \alpha) &= \sum_{i=1}^n \left[ \left(  \inf_{x_{i-1} \leq x \leq x_i} f(x) \right) \left[ c \alpha\left( x_i \right) - c \alpha\left( x_{i-1} \right) \right] \right] \\ 
&=  c \sum_{i=1}^n \left(  \inf_{x_{i-1} \leq x \leq x_i} f(x) \right) \left[  \alpha\left( x_i \right) -  \alpha\left( x_{i-1} \right) \right]  \\
 &= c L(P, f, \alpha);
\end{align}
   L(P, f, c \alpha) = c L(P, f, \alpha ) \ \mbox{ and similarly} \   U(P, f, c \alpha) = c U(P, f, \alpha). \tag{1}  f \in \mathscr{R}(\alpha) [a, b] \varepsilon > 0 P [a, b]  U(P, f, \alpha) - L(P, f, \alpha) < { \varepsilon \over c },  P [a, b]  U(P, f, c \alpha) - L(P, f, c \alpha) < \varepsilon, \tag{2}  f \in \mathscr{R}( c \alpha) \begin{align}
\int_a^b f d (c \alpha) &\leq U(P, f, c \alpha) \\
&<  L(P, f, c \alpha ) + \varepsilon \\
&= c L(P, f, \alpha) + \varepsilon \\ 
&\leq c \int_a^b f d \alpha + \varepsilon
\end{align} \varepsilon > 0  \int_a^b f d (c \alpha) \leq c \int_a^b f d \alpha. \tag{A} \begin{align}
c \int_a^b f d \alpha & \leq c U(P, f, \alpha) \\
&= U(P, f, c \alpha) \\
&< L(P, f, c \alpha) + \varepsilon \\
&\leq \int_a^b f d (c \alpha) + \varepsilon 
\end{align} \varepsilon > 0  c \int_a^b f d \alpha \leq \int_a^b f d (c \alpha). \tag{B}","['real-analysis', 'integration', 'analysis', 'definite-integrals', 'solution-verification']"
74,"Is $f$ identically zero on $[0, 1]$?",Is  identically zero on ?,"f [0, 1]","Let $f$ be a continuous real-valued function on $[0,1]$ such that there is $K>0$ for which $|f(x)|\le K \int_0^x|f(t)|dt$ for all $x\in [0,1]$. Does it follow that $f=0$ on $[0,1]$? What I know is just $f(0)=0.$","Let $f$ be a continuous real-valued function on $[0,1]$ such that there is $K>0$ for which $|f(x)|\le K \int_0^x|f(t)|dt$ for all $x\in [0,1]$. Does it follow that $f=0$ on $[0,1]$? What I know is just $f(0)=0.$",,"['real-analysis', 'integration', 'analysis']"
75,"$K(x,y)$ continuous on $S=[0,1]\times[0,1]$ and $\phi \in C([0,1])$, define $T\phi$ on $[0,1]$ by $T\phi(x)=\int_0^xK(x,y)\phi(y)dy,~~x\in[0,1].$","continuous on  and , define  on  by","K(x,y) S=[0,1]\times[0,1] \phi \in C([0,1]) T\phi [0,1] T\phi(x)=\int_0^xK(x,y)\phi(y)dy,~~x\in[0,1].","Let $K(x,y)$ be continuous on $S=[0,1]\times[0,1]$ and $\phi \in C([0,1])$, define $T\phi$ on $[0,1]$ by  $$T\phi(x)=\int_0^xK(x,y)\phi(y)dy,~~x\in[0,1].$$ (a) Show that $T\phi \in C([0,1])$ (b) Show that $T$ is continuous. (C) If $\{\phi_n\}_{n=1}^\infty$ is a bounded sequence in $C([0,1]),$ show that the sequence $\{T\phi_n\}_{n=1}^\infty$ has a convergent subsequence. My attempt: (a) By assumption, we have that $K(x,y)$ is uniformly continuous. So,  $\exists~\delta>0$ s.t. $|K(x,y)-K(x',y')|<\frac{\epsilon}{1+\|\phi\|}$ whenever $\sqrt{(x-x')^2+(y-y')^2}<0$. $|\int_0^xK(x,y)\phi(y)~dy-\int_0^z K(z,y)\phi(y)~dy| \\ \leq |\int_0^z K(x,y)\phi(y)~dy-\int_0^z K(z,y)\phi(y)~dy +\int_z^x K(x,y)\phi(y)~dy| \\ \leq |\int_0^z (K(x,y)-K(z,y))\phi(y)~dy~| + |\int_z^xK(x,y)\phi(y)~dy| \\   \leq \|\phi\|\frac{\epsilon}{1+\|\phi\|}+|\int_z^xK(x,y)\phi(y)~dy|$ How to estimate this term $|\int_z^xK(x,y)\phi(y)~dy|$ ? (b) $|\int_0^x K(x,y)\phi(y)~dy|\leq \int_0^1 |K(x,y)\phi(y)|~dy \\ \Rightarrow \|T\phi\|=\displaystyle\sup_{x\in[0,1]}|\int_0^x K(x,y)\phi(y)~dy|\leq \|K\|\cdot\|\phi\|=C\|\phi\|~~~(C=\|K\|)$ So, $\|T\phi_1-T\phi_2\|=\|T(\phi_1-\phi_2)\| \leq C\|\phi_1-\phi_2\|$ for $\phi_1,\phi_2 \in C([0,1])$. So, $T$ is continuous. (c) I have no idea about this problem. Maybe I should show that  $\{T\phi_n\}_{n=1}^\infty$ is equicontinuous. I know the theorem that if  $\{T\phi_n\}_{n=1}^\infty$ is pointwise bounded and equicontinuous on $K$, then  $\{T\phi_n\}_{n=1}^\infty$ contains a uniformly convergent subsequece.","Let $K(x,y)$ be continuous on $S=[0,1]\times[0,1]$ and $\phi \in C([0,1])$, define $T\phi$ on $[0,1]$ by  $$T\phi(x)=\int_0^xK(x,y)\phi(y)dy,~~x\in[0,1].$$ (a) Show that $T\phi \in C([0,1])$ (b) Show that $T$ is continuous. (C) If $\{\phi_n\}_{n=1}^\infty$ is a bounded sequence in $C([0,1]),$ show that the sequence $\{T\phi_n\}_{n=1}^\infty$ has a convergent subsequence. My attempt: (a) By assumption, we have that $K(x,y)$ is uniformly continuous. So,  $\exists~\delta>0$ s.t. $|K(x,y)-K(x',y')|<\frac{\epsilon}{1+\|\phi\|}$ whenever $\sqrt{(x-x')^2+(y-y')^2}<0$. $|\int_0^xK(x,y)\phi(y)~dy-\int_0^z K(z,y)\phi(y)~dy| \\ \leq |\int_0^z K(x,y)\phi(y)~dy-\int_0^z K(z,y)\phi(y)~dy +\int_z^x K(x,y)\phi(y)~dy| \\ \leq |\int_0^z (K(x,y)-K(z,y))\phi(y)~dy~| + |\int_z^xK(x,y)\phi(y)~dy| \\   \leq \|\phi\|\frac{\epsilon}{1+\|\phi\|}+|\int_z^xK(x,y)\phi(y)~dy|$ How to estimate this term $|\int_z^xK(x,y)\phi(y)~dy|$ ? (b) $|\int_0^x K(x,y)\phi(y)~dy|\leq \int_0^1 |K(x,y)\phi(y)|~dy \\ \Rightarrow \|T\phi\|=\displaystyle\sup_{x\in[0,1]}|\int_0^x K(x,y)\phi(y)~dy|\leq \|K\|\cdot\|\phi\|=C\|\phi\|~~~(C=\|K\|)$ So, $\|T\phi_1-T\phi_2\|=\|T(\phi_1-\phi_2)\| \leq C\|\phi_1-\phi_2\|$ for $\phi_1,\phi_2 \in C([0,1])$. So, $T$ is continuous. (c) I have no idea about this problem. Maybe I should show that  $\{T\phi_n\}_{n=1}^\infty$ is equicontinuous. I know the theorem that if  $\{T\phi_n\}_{n=1}^\infty$ is pointwise bounded and equicontinuous on $K$, then  $\{T\phi_n\}_{n=1}^\infty$ contains a uniformly convergent subsequece.",,"['real-analysis', 'functional-analysis', 'analysis']"
76,Composition of Borel-measurable function and continuous function,Composition of Borel-measurable function and continuous function,,"Let $g: \mathbb{R} \rightarrow \mathbb{R}$ be continuous and $h: \mathbb{R} \rightarrow \mathbb{R}$ be Borel measurable. Is $(h \circ g): \mathbb{R} \rightarrow \mathbb{R}$ necessarily Borel measurable? For any $a \in \mathbb{R}$: $(h \circ g)^{-1}(a, \infty) = g^{-1}(h^{-1}(a, \infty))$, and $h^{-1}(a, \infty)$ is Borel since $h$ is Borel, so let's write $B = h^{-1}(a, \infty)$. The question is now whether or not $g^{-1}(B)$ is Borel. I see no reason for $g^{-1}(B)$ to be Borel. I understand that continuity of $g$ implies that $g$ is also Borel measurable, but $B$ is not necessarily open. I am failing to see why $g^{-1}(B)$ would be Borel. If the statement that I'm trying to prove is false, I would really appreciate a counter-example. Thanks in advance.","Let $g: \mathbb{R} \rightarrow \mathbb{R}$ be continuous and $h: \mathbb{R} \rightarrow \mathbb{R}$ be Borel measurable. Is $(h \circ g): \mathbb{R} \rightarrow \mathbb{R}$ necessarily Borel measurable? For any $a \in \mathbb{R}$: $(h \circ g)^{-1}(a, \infty) = g^{-1}(h^{-1}(a, \infty))$, and $h^{-1}(a, \infty)$ is Borel since $h$ is Borel, so let's write $B = h^{-1}(a, \infty)$. The question is now whether or not $g^{-1}(B)$ is Borel. I see no reason for $g^{-1}(B)$ to be Borel. I understand that continuity of $g$ implies that $g$ is also Borel measurable, but $B$ is not necessarily open. I am failing to see why $g^{-1}(B)$ would be Borel. If the statement that I'm trying to prove is false, I would really appreciate a counter-example. Thanks in advance.",,"['real-analysis', 'measure-theory', 'lebesgue-measure', 'borel-sets']"
77,An analytical proof for the punctured plane is not simply connected?,An analytical proof for the punctured plane is not simply connected?,,"I am trying to make a ""challenge problem"" for my (undergraduate) real analysis students. Currently, the students knows about connectedness, compactness in $\mathbb R^n$, functional limits and continuous functions from $\mathbb R^m$ to $\mathbb R^n$. They are also familiar with all the standard real analysis results on $\mathbb R$. The goal for this problem is to show that the punctured plane is not simply connected. I set up the problem by defining the terms ""path"", ""loop"", ""path homotopy"", and ""null-homotopic"". But I could not come up with a proof accessible to real analysis students showing that the path parameterized by $\alpha(t)  = (\cos(2 \pi t),\sin(2 \pi t))$ is NOT null-homotopic. Can someone help? Things I definitely want to avoid: fundamental groups, Brouwer fixed point theorem, residue theorem. Things I wish to avoid: There is a proof using Green's theorem, which I guess has the same flavor as the residue theorem in complex analysis. I think this is something students are able to understand. But since we have not talked about vector calculus in this course, it would be better if the proof I write down as solution does not involve Green's theorem.","I am trying to make a ""challenge problem"" for my (undergraduate) real analysis students. Currently, the students knows about connectedness, compactness in $\mathbb R^n$, functional limits and continuous functions from $\mathbb R^m$ to $\mathbb R^n$. They are also familiar with all the standard real analysis results on $\mathbb R$. The goal for this problem is to show that the punctured plane is not simply connected. I set up the problem by defining the terms ""path"", ""loop"", ""path homotopy"", and ""null-homotopic"". But I could not come up with a proof accessible to real analysis students showing that the path parameterized by $\alpha(t)  = (\cos(2 \pi t),\sin(2 \pi t))$ is NOT null-homotopic. Can someone help? Things I definitely want to avoid: fundamental groups, Brouwer fixed point theorem, residue theorem. Things I wish to avoid: There is a proof using Green's theorem, which I guess has the same flavor as the residue theorem in complex analysis. I think this is something students are able to understand. But since we have not talked about vector calculus in this course, it would be better if the proof I write down as solution does not involve Green's theorem.",,['real-analysis']
78,Proof of a proposition regarding recursive definitions (from Terence Tao's Analysis I),Proof of a proposition regarding recursive definitions (from Terence Tao's Analysis I),,"I'm currently reading Terence Tao's book Analysis I , and I got stuck on his proposition 2.1.16 about recursive definitions ; actually, I don't understand his (informal) proof of it . First, here are the five Peano axioms he's using : Axiom 2.1. $0$ is a natural number. Axiom 2.2. If $n$ is a natural number, then $n{++}$ is also a natural number. Axiom 2.3. $0$ is not the successor of any natural number ; i.e., we have $n{++} \ne 0$ for every natural number $n$ . Axiom 2.4. Different natural numbers must have different successors ; i.e., if $n, m$ are natural numbers and $n \ne m$ , then $n{++} \ne m{++}$ . Equivalently, if $n{++} = m{++}$ , then we must have $n=m$ . Axiom 2.5. (Principle of mathematical induction). Let $P(n)$ be any property pertaining to a natural number $n$ . Suppose that $P(0)$ is true, and suppose that whenever $P(n)$ is true, $P(n{++})$ is also true. Then $P(n)$ is true for every natural number $n$ . And here is the proposition, and the proof I'm stuck on : Proposition 2.1.16 (Recursive definitions). Suppose for each natural number $n$ , we have some function $f_{n} : \mathbb{N} \rightarrow \mathbb{N}$ from the natural numbers to the natural numbers. Let $c$ be a natural number. Then we can assign a unique natural number $a_{n}$ to each natural number $n$ , such that $a_{0} = c$ and $a_{n{++}} = f_{n} (a_{n})$ for each natural number $n$ . Proof . (Informal) We use induction. We first observe that this procedure gives a single value to $a_0$ , namely $c$ . (None of the other definitions $a_{n{++}}:= f_{n} (a_n)$ will redefine the value of $a_0$ , because of Axiom 2.3.) Now suppose inductively that the procedure gives a single value to $a_n$ . Then it gives a single value to $a_{n{++}}$ , namely $a_{n{++}}:= f_{n} (a_n)$ . (None of the other definitions $a_{m{++}}:= f_{m} (a_{m})$ will redefine the value of $a_{n{++}}$ , because of Axiome 2.4.) This completes the induction, and so $a_n$ is defined for each natural number $n$ , with a single value assigned to each $a_n$ . I don't understand how does that prove anything. I feel like all he's saying is that « all natural numbers are different from each others » ; I don't understand why we couldn't have, for example, $a_0 = a_1 = c$ (for instance, if we consider the Fibonacci sequence : $0, 1, 1, 2, ...$ , we have $a_1 = a_2 = 1$ ). I actually found another proof of uniqueness on Wikipedia (regarding the Recursion theorem) and I think I understood it, but I really would like to understand Tao's point of view as well. Finally, shouldn't we prove that such a function exists as well, instead of just supposing it exists ? I found this document which gives a proof for the existence of such a function (at pages 2-3), but I'm a little bit confused about the way they do it: can we really ""imagine"" or ""create"" such sets as $S$ and $g$ , like they do, and draw conclusions from them? Another thing that disturbs me regarding their approach is that Tao doesn't talk about sets before chapter 3, and the proposition above comes from chapter 2... so the question here is: how could we prove the existence of such functions as the ones described in the recursion theorem without knowing anything about sets and set theory? So, as you can see... I'm pretty confused about a lot of things. Any help would be greatly appreciated!","I'm currently reading Terence Tao's book Analysis I , and I got stuck on his proposition 2.1.16 about recursive definitions ; actually, I don't understand his (informal) proof of it . First, here are the five Peano axioms he's using : Axiom 2.1. is a natural number. Axiom 2.2. If is a natural number, then is also a natural number. Axiom 2.3. is not the successor of any natural number ; i.e., we have for every natural number . Axiom 2.4. Different natural numbers must have different successors ; i.e., if are natural numbers and , then . Equivalently, if , then we must have . Axiom 2.5. (Principle of mathematical induction). Let be any property pertaining to a natural number . Suppose that is true, and suppose that whenever is true, is also true. Then is true for every natural number . And here is the proposition, and the proof I'm stuck on : Proposition 2.1.16 (Recursive definitions). Suppose for each natural number , we have some function from the natural numbers to the natural numbers. Let be a natural number. Then we can assign a unique natural number to each natural number , such that and for each natural number . Proof . (Informal) We use induction. We first observe that this procedure gives a single value to , namely . (None of the other definitions will redefine the value of , because of Axiom 2.3.) Now suppose inductively that the procedure gives a single value to . Then it gives a single value to , namely . (None of the other definitions will redefine the value of , because of Axiome 2.4.) This completes the induction, and so is defined for each natural number , with a single value assigned to each . I don't understand how does that prove anything. I feel like all he's saying is that « all natural numbers are different from each others » ; I don't understand why we couldn't have, for example, (for instance, if we consider the Fibonacci sequence : , we have ). I actually found another proof of uniqueness on Wikipedia (regarding the Recursion theorem) and I think I understood it, but I really would like to understand Tao's point of view as well. Finally, shouldn't we prove that such a function exists as well, instead of just supposing it exists ? I found this document which gives a proof for the existence of such a function (at pages 2-3), but I'm a little bit confused about the way they do it: can we really ""imagine"" or ""create"" such sets as and , like they do, and draw conclusions from them? Another thing that disturbs me regarding their approach is that Tao doesn't talk about sets before chapter 3, and the proposition above comes from chapter 2... so the question here is: how could we prove the existence of such functions as the ones described in the recursion theorem without knowing anything about sets and set theory? So, as you can see... I'm pretty confused about a lot of things. Any help would be greatly appreciated!","0 n n{++} 0 n{++} \ne 0 n n, m n \ne m n{++} \ne m{++} n{++} = m{++} n=m P(n) n P(0) P(n) P(n{++}) P(n) n n f_{n} : \mathbb{N} \rightarrow \mathbb{N} c a_{n} n a_{0} = c a_{n{++}} = f_{n} (a_{n}) n a_0 c a_{n{++}}:= f_{n} (a_n) a_0 a_n a_{n{++}} a_{n{++}}:= f_{n} (a_n) a_{m{++}}:= f_{m} (a_{m}) a_{n{++}} a_n n a_n a_0 = a_1 = c 0, 1, 1, 2, ... a_1 = a_2 = 1 S g","['real-analysis', 'proof-explanation', 'peano-axioms']"
79,explanation of a triple integral $\iiint _{x^2+y^2+z^2<R^2}\left|xz\right|y^2dxdydz\:$,explanation of a triple integral,\iiint _{x^2+y^2+z^2<R^2}\left|xz\right|y^2dxdydz\:,"I'm trying to figure out how the my book solve this exercise: $$\iiint _{x^2+y^2+z^2<R^2}\left|xz\right|y^2dxdydz\:$$ BOOK SOLUTION: $$\iiint _{x^2+y^2+z^2<R^2}\left|xz\right|y^2dxdydz\: = 8\iiint _Sxzy^2dxdydz\:$$ with: $$S=\{(x,y,z):x^2+y^2+z^2<R^2, x\ge0, y\ge0, z\ge0\}$$ We transform in spherical coordinates: $$S=\{(\rho,\phi,\theta): \rho \in [0,R], \phi\in[0,\frac{\pi}{2}], \theta \in [0,\frac{\pi}{2}]\}, dxdydz=\rho^2\sin\phi d\phi d\theta d\rho$$ then, the exercise continues with the triple integral solution after the transformation (which is not difficult). MY DOUBT i will report below, what I am not clear (outlined in red) $$\iiint _{x^2+y^2+z^2<R^2}\left|xz\right|y^2dxdydz\: = \color{red}{8\iiint_Cxzy^2dxdydz\:}$$ I just can not understand how these two integrals can be equal. Then: $$S=\{(\rho,\phi,\theta): \rho \in [0,R], \color{red}{\phi\in[0,\frac{\pi}{2}]}, \color{red}{\theta \in [0,\frac{\pi}{2}]}\}, dxdydz=\rho^2\sin\phi d\phi d\theta d\rho$$ why in that range? Thanks","I'm trying to figure out how the my book solve this exercise: $$\iiint _{x^2+y^2+z^2<R^2}\left|xz\right|y^2dxdydz\:$$ BOOK SOLUTION: $$\iiint _{x^2+y^2+z^2<R^2}\left|xz\right|y^2dxdydz\: = 8\iiint _Sxzy^2dxdydz\:$$ with: $$S=\{(x,y,z):x^2+y^2+z^2<R^2, x\ge0, y\ge0, z\ge0\}$$ We transform in spherical coordinates: $$S=\{(\rho,\phi,\theta): \rho \in [0,R], \phi\in[0,\frac{\pi}{2}], \theta \in [0,\frac{\pi}{2}]\}, dxdydz=\rho^2\sin\phi d\phi d\theta d\rho$$ then, the exercise continues with the triple integral solution after the transformation (which is not difficult). MY DOUBT i will report below, what I am not clear (outlined in red) $$\iiint _{x^2+y^2+z^2<R^2}\left|xz\right|y^2dxdydz\: = \color{red}{8\iiint_Cxzy^2dxdydz\:}$$ I just can not understand how these two integrals can be equal. Then: $$S=\{(\rho,\phi,\theta): \rho \in [0,R], \color{red}{\phi\in[0,\frac{\pi}{2}]}, \color{red}{\theta \in [0,\frac{\pi}{2}]}\}, dxdydz=\rho^2\sin\phi d\phi d\theta d\rho$$ why in that range? Thanks",,"['real-analysis', 'integration', 'multivariable-calculus']"
80,Does Euclidean geometry require a complete metric space?,Does Euclidean geometry require a complete metric space?,,"Note: I'm not sure how to tag this question so please fix tags as appropriate and necessary. This question relates to Tarski's axioms of Euclidean geometry, not Hilbert's axioms. It says on the same page (Theorem 3.1 -- see also this related question on Math.SE ) that: For every model of Euclidean Plane Geometry (using Tarski's axioms) there is a real closed field $R$ such that $M \cong R \times R$ as models. From what I gather from nLab's page on real closed fields , these are strictly more general than the real numbers, for example the real algebraic numbers is also a real closed field. However, the real algebraic numbers are not metrically complete . (I think.) In other words, not every Cauchy sequence converges. Question: How can a real closed field, which is not metrically complete, serve as a model for Euclidean plane geometry? In particular, how can the real algebraic numbers serve as a model for Euclidean geometry? For example, such a field cannot express the ratio of the circumference of a circle to its diameter in Euclidean plane geometry since $\pi$ is transcendental and thus only contained in the metric completion of the rational numbers, but not the (real) algebraic completion. One of Tarski's axioms is described on the nLab page as being a ""Dedekind cut axiom expressed in first order terms"". If I remember real analysis correctly, Dedekind cuts allow one to construct the metric completion of the rational numbers (the real numbers). So if one of Tarski's axioms implies metric completeness (does it not? if not, why doesn't it?), then how can the real algebraic numbers, which aren't metrically complete (I think) serve as a model? I'm not sure if the Cantor-Dedekind axiom is one of Tarski's axioms or the same as the aforementioned ""Dedekind cut axiom"". If Tarski's axioms don't require a metrically complete space, then is the Cantor-Dedekind axiom not generally valid for models of Tarski's axioms? The answers to this related question seem to suggest that $\mathbb{Q}$ is insufficient for Euclidean geometry, which makes sense since $\mathbb{Q}$ is not real closed. However, it is unclear from the answers given if the problems with $\mathbb{Q}$ arise only because of the non-existence of algebraic irrational numbers (e.g. $\sqrt{2}$ ) or also because of the non-existence of transcendental irrational numbers, $\pi$ . The latter would imply that the real algebraic numbers are also insufficient for Euclidean geometry. Also, one of the axioms given for Euclidean plane geometry (on p. 171, M.2) in Agricola, Friedrich's Elementary Geometry is that the plane is a complete metric space. I had assumed that these axioms were just a (possibly less rigorous) rewording of Tarski's axioms, but if non-complete metric spaces also serve as models for Euclidean geometry, then it would seem that this axiom is an invention/addition of the authors, which perhaps should have been mentioned explicitly.","Note: I'm not sure how to tag this question so please fix tags as appropriate and necessary. This question relates to Tarski's axioms of Euclidean geometry, not Hilbert's axioms. It says on the same page (Theorem 3.1 -- see also this related question on Math.SE ) that: For every model of Euclidean Plane Geometry (using Tarski's axioms) there is a real closed field such that as models. From what I gather from nLab's page on real closed fields , these are strictly more general than the real numbers, for example the real algebraic numbers is also a real closed field. However, the real algebraic numbers are not metrically complete . (I think.) In other words, not every Cauchy sequence converges. Question: How can a real closed field, which is not metrically complete, serve as a model for Euclidean plane geometry? In particular, how can the real algebraic numbers serve as a model for Euclidean geometry? For example, such a field cannot express the ratio of the circumference of a circle to its diameter in Euclidean plane geometry since is transcendental and thus only contained in the metric completion of the rational numbers, but not the (real) algebraic completion. One of Tarski's axioms is described on the nLab page as being a ""Dedekind cut axiom expressed in first order terms"". If I remember real analysis correctly, Dedekind cuts allow one to construct the metric completion of the rational numbers (the real numbers). So if one of Tarski's axioms implies metric completeness (does it not? if not, why doesn't it?), then how can the real algebraic numbers, which aren't metrically complete (I think) serve as a model? I'm not sure if the Cantor-Dedekind axiom is one of Tarski's axioms or the same as the aforementioned ""Dedekind cut axiom"". If Tarski's axioms don't require a metrically complete space, then is the Cantor-Dedekind axiom not generally valid for models of Tarski's axioms? The answers to this related question seem to suggest that is insufficient for Euclidean geometry, which makes sense since is not real closed. However, it is unclear from the answers given if the problems with arise only because of the non-existence of algebraic irrational numbers (e.g. ) or also because of the non-existence of transcendental irrational numbers, . The latter would imply that the real algebraic numbers are also insufficient for Euclidean geometry. Also, one of the axioms given for Euclidean plane geometry (on p. 171, M.2) in Agricola, Friedrich's Elementary Geometry is that the plane is a complete metric space. I had assumed that these axioms were just a (possibly less rigorous) rewording of Tarski's axioms, but if non-complete metric spaces also serve as models for Euclidean geometry, then it would seem that this axiom is an invention/addition of the authors, which perhaps should have been mentioned explicitly.",R M \cong R \times R \pi \mathbb{Q} \mathbb{Q} \mathbb{Q} \sqrt{2} \pi,"['real-analysis', 'soft-question', 'euclidean-geometry', 'model-theory', 'axioms']"
81,How do I differentiate under the integral sign here.,How do I differentiate under the integral sign here.,,"Assume we are working on the interval $[a,b]$, we have that $f \in L^1  [a,b]$. Let $\gamma \in (0,1)$. We then have that $$\int_a^uf(t)(u-t)^{1-\gamma}dt, u \in [a,b]$$ is well-defined. Because $(u-t)^{1-\gamma}$ is continuous on $[a,u]$,   and hence bounded. A book I am reading states that: $$\frac{d}{du }\int_a^uf(t)(u-t)^{1-\gamma}dt=(1-\gamma)\int_a^uf(t)(u-t)^{-\gamma}dt.$$ But why is this the case? I tried using the leibniz integral rule . But from what I see I can not use it because the derivative $(u-t)^{-\gamma}$ does not behave well when $t=u$. Another problem I have is that by Fubini/Tonelli we may have that $(1-\gamma)\int_a^uf(t)(u-t)^{-\gamma}dt$ is well defined only  a.e. on $[a,b]$? But do you see how we can prove that identity if $(1-\gamma)\int_a^uf(t)(u-t)^{-\gamma}dt$ is well-defined? I tried writing the definition of the derivative, and then taking limits etc.. But I do not get that the fraction is bounded(so I can use the dominated convergence theorem), because $(u-t)^{-\gamma}$ is not bounded. If I am supposed to solve it using this I get: $\frac{\int_a^{u+\Delta u}f(t)(u+\Delta u-t)^{1-\gamma}dt-\int_a^{u}f(t)(u-t)^{1-\gamma}dt}{\Delta u}=\int_a^u\frac{f(t)[(u+\Delta u-t)^{1-\gamma}-(u-t)^{1-\gamma}]dt}{\Delta u}+\int_u^{u+\Delta u}\frac{f(t)(u+\Delta u-t)^{1-\gamma}dt}{\Delta u}.$ Do you see what happens with the last terms when $\Delta u$ goes to zero? (If what I wrote in the start is correct, the last term is supposed to go to zero. And in the second last term we are supposed to be able to write the derivative inside, but how do I argue for this using DCT?) ANSWER For those who are interested I think I found the answer. The book I was reading was a book containg articles, and I think the article my quesiton is based on was badly written. I looked at the source for the article and it seemed better. Note first if we can show that $\int_a^u f(t)(u-t)^{1-\gamma}dt=(1-\gamma)\int_a^u[\int_a^t\frac{f(s)ds}{(t-s)^\gamma}]dt , H(t)=\int_a^t\frac{f(s)ds}{(t-s)^\gamma}\in L^1[a,b]$, we are done, because then the result follows from the fundamental theorem of calculus. First we show that $H$ is well defined, and simultaneously show that we can use Fubini to interchange limits.: $\int_a^b\int_a^t\frac{|f(s)|ds}{(t-s)^\gamma}dt=\int_a^b \int_s^b|f(s)|(t-s)^{-\gamma} dtds=\int_a^b |f(s)|(b-s)^{1-\gamma} \cdot1/(1-\gamma)ds<\infty$. And now the same calculuation with $u$ instead of b, and $f(s)$ instead of $|f(s)|$ tells us that $\int_a^u f(t)(u-t)^{1-\gamma}dt=(1-\gamma)\int_a^u[\int_a^t\frac{f(s)ds}{(t-s)^\gamma}]dt$.","Assume we are working on the interval $[a,b]$, we have that $f \in L^1  [a,b]$. Let $\gamma \in (0,1)$. We then have that $$\int_a^uf(t)(u-t)^{1-\gamma}dt, u \in [a,b]$$ is well-defined. Because $(u-t)^{1-\gamma}$ is continuous on $[a,u]$,   and hence bounded. A book I am reading states that: $$\frac{d}{du }\int_a^uf(t)(u-t)^{1-\gamma}dt=(1-\gamma)\int_a^uf(t)(u-t)^{-\gamma}dt.$$ But why is this the case? I tried using the leibniz integral rule . But from what I see I can not use it because the derivative $(u-t)^{-\gamma}$ does not behave well when $t=u$. Another problem I have is that by Fubini/Tonelli we may have that $(1-\gamma)\int_a^uf(t)(u-t)^{-\gamma}dt$ is well defined only  a.e. on $[a,b]$? But do you see how we can prove that identity if $(1-\gamma)\int_a^uf(t)(u-t)^{-\gamma}dt$ is well-defined? I tried writing the definition of the derivative, and then taking limits etc.. But I do not get that the fraction is bounded(so I can use the dominated convergence theorem), because $(u-t)^{-\gamma}$ is not bounded. If I am supposed to solve it using this I get: $\frac{\int_a^{u+\Delta u}f(t)(u+\Delta u-t)^{1-\gamma}dt-\int_a^{u}f(t)(u-t)^{1-\gamma}dt}{\Delta u}=\int_a^u\frac{f(t)[(u+\Delta u-t)^{1-\gamma}-(u-t)^{1-\gamma}]dt}{\Delta u}+\int_u^{u+\Delta u}\frac{f(t)(u+\Delta u-t)^{1-\gamma}dt}{\Delta u}.$ Do you see what happens with the last terms when $\Delta u$ goes to zero? (If what I wrote in the start is correct, the last term is supposed to go to zero. And in the second last term we are supposed to be able to write the derivative inside, but how do I argue for this using DCT?) ANSWER For those who are interested I think I found the answer. The book I was reading was a book containg articles, and I think the article my quesiton is based on was badly written. I looked at the source for the article and it seemed better. Note first if we can show that $\int_a^u f(t)(u-t)^{1-\gamma}dt=(1-\gamma)\int_a^u[\int_a^t\frac{f(s)ds}{(t-s)^\gamma}]dt , H(t)=\int_a^t\frac{f(s)ds}{(t-s)^\gamma}\in L^1[a,b]$, we are done, because then the result follows from the fundamental theorem of calculus. First we show that $H$ is well defined, and simultaneously show that we can use Fubini to interchange limits.: $\int_a^b\int_a^t\frac{|f(s)|ds}{(t-s)^\gamma}dt=\int_a^b \int_s^b|f(s)|(t-s)^{-\gamma} dtds=\int_a^b |f(s)|(b-s)^{1-\gamma} \cdot1/(1-\gamma)ds<\infty$. And now the same calculuation with $u$ instead of b, and $f(s)$ instead of $|f(s)|$ tells us that $\int_a^u f(t)(u-t)^{1-\gamma}dt=(1-\gamma)\int_a^u[\int_a^t\frac{f(s)ds}{(t-s)^\gamma}]dt$.",,"['calculus', 'real-analysis', 'derivatives', 'definite-integrals']"
82,$\forall x\in\mathbb{R}\setminus\mathbb{Q}:f'(x)=0$. Does it follow that f is constant?,. Does it follow that f is constant?,\forall x\in\mathbb{R}\setminus\mathbb{Q}:f'(x)=0,Function $f:\mathbb{R}\rightarrow\mathbb{R}$ is continuous and $\forall x\in\mathbb{R}\setminus\mathbb{Q}:f'(x)=0$. Does it follow that f is constant?,Function $f:\mathbb{R}\rightarrow\mathbb{R}$ is continuous and $\forall x\in\mathbb{R}\setminus\mathbb{Q}:f'(x)=0$. Does it follow that f is constant?,,"['real-analysis', 'functions', 'derivatives']"
83,Limit of $\left(1-\frac{1}{2^2}\right)\left(1-\frac{1}{3^3}\right)\dots \left(1-\frac{1}{n^n}\right)$ as $n\to \infty$,Limit of  as,\left(1-\frac{1}{2^2}\right)\left(1-\frac{1}{3^3}\right)\dots \left(1-\frac{1}{n^n}\right) n\to \infty,"So I'm trying to solve the following limit: $$\lim_{n \to \infty}\left(1-\frac{1}{2^2}\right)\left(1-\frac{1}{3^3}\right)\dots \left(1-\frac{1}{n^n}\right)$$ Now, I tried getting the squeeze theorem around this one, since it does feel like something for the squeeze theorem. The upper bound is obviously $1$, but since each term decreases the product, it may seem like this approaches zero?","So I'm trying to solve the following limit: $$\lim_{n \to \infty}\left(1-\frac{1}{2^2}\right)\left(1-\frac{1}{3^3}\right)\dots \left(1-\frac{1}{n^n}\right)$$ Now, I tried getting the squeeze theorem around this one, since it does feel like something for the squeeze theorem. The upper bound is obviously $1$, but since each term decreases the product, it may seem like this approaches zero?",,"['real-analysis', 'limits']"
84,"Show that $f(x) = 0$ for all $x \in [a,b]$ given $|f'(x)| \leq C|f(x)| $",Show that  for all  given,"f(x) = 0 x \in [a,b] |f'(x)| \leq C|f(x)| ","Suppose for real numbers $a<b$ one has a function with continuous derivative $f:[a,b] \to \mathbb{R}$ such that $f(a) = 0$ and there exists a real number $C$ with $$|f'(x)| \leq C|f(x)|$$ for all $x \in [a,b]$ . Show that $f(x) = 0$ for all $x \in [a,b]$ . Well, since $f(a) = 0$ , we have that $|f'(a)| \leq C|0|$ , so $|f'(a)| = 0$ . Since $f$ has a continuous derivative, we also know that $f$ is continuous. Since $f$ is continuous on a compact interval, $f$ obtains a maximum, say at $\xi \in [a,b]$ . So, $|f'(x)| \leq C|f(\xi)|$ . Since the derivative is bounded, we obtain that $f$ is Lipschitz, so $f$ is also uniformly continuous. Suppose to the contrary that $f(\xi) > 0$ ? The above is pretty much everything I could figure out about $f$ , so I'm not sure what to try next. This one is also from an old qual and possibly uses methods from beyond our course. I think maybe I should try to show that $|f'(x)| = 0$ for all $x$ , but I don't know how.","Suppose for real numbers one has a function with continuous derivative such that and there exists a real number with for all . Show that for all . Well, since , we have that , so . Since has a continuous derivative, we also know that is continuous. Since is continuous on a compact interval, obtains a maximum, say at . So, . Since the derivative is bounded, we obtain that is Lipschitz, so is also uniformly continuous. Suppose to the contrary that ? The above is pretty much everything I could figure out about , so I'm not sure what to try next. This one is also from an old qual and possibly uses methods from beyond our course. I think maybe I should try to show that for all , but I don't know how.","a<b f:[a,b] \to \mathbb{R} f(a) = 0 C |f'(x)| \leq C|f(x)| x \in [a,b] f(x) = 0 x \in [a,b] f(a) = 0 |f'(a)| \leq C|0| |f'(a)| = 0 f f f f \xi \in [a,b] |f'(x)| \leq C|f(\xi)| f f f(\xi) > 0 f |f'(x)| = 0 x","['real-analysis', 'analysis']"
85,An Application using Arzela-Ascoli Theorem,An Application using Arzela-Ascoli Theorem,,"I am looking for a good application for undergraduates (Analysis I) that uses the AA-theorem. Perhaps, a proof of some surprising result that is not so easy to prove otherwise? I do realize that AA-theorem is usually used as a theoretical tool in other proofs. But I was wondering if anyone has any insightful examples that may make the students appreciate it?","I am looking for a good application for undergraduates (Analysis I) that uses the AA-theorem. Perhaps, a proof of some surprising result that is not so easy to prove otherwise? I do realize that AA-theorem is usually used as a theoretical tool in other proofs. But I was wondering if anyone has any insightful examples that may make the students appreciate it?",,"['real-analysis', 'functional-analysis', 'analysis']"
86,The Minkowski inequality for fractional order?,The Minkowski inequality for fractional order?,,"Let $u\in C^\infty(\bar I)$ be given where $I=(0,1)$. Define $$ t(\alpha):=\left(\int_I\int_I \frac{|u(x)-u(y)|^\alpha}{|x-y|^{1+s\alpha}}\right)^{\frac1\alpha} $$ where $1<\alpha<2$, $0<s<1$ is fixed. Note the $t(\alpha)$ above defines the fractional order sobolev seminorm. See here , roll down for the section ""Sobolev spaces with non-integer $k$"". We know that for $L^p$ space, if $p<q$ then $\|u\|_{L^p(I)}\leq \|u\|_{L^q(I)}$ by using holder inequality. So I am wondering whether similar properties hold for sobolev space with non-integer $k$.(of course, for integer case it is again a Holder inequality) That is, I am wondering for $1<\alpha_1<\alpha_2<2$, do we have $$ t(\alpha_1)\leq Ct(\alpha_2) $$ hold, where $C$ is a constant does not depends on $u$. Just like what we usually have for $L^p$ norm. However, I tried to prove it by using minkowski inequality or sth like holder, but I can't do it... I think the domain $I=(0,1)$ would be important and I also tried to use Jensen inequality, but no lucky... Any ideas? Thank you!","Let $u\in C^\infty(\bar I)$ be given where $I=(0,1)$. Define $$ t(\alpha):=\left(\int_I\int_I \frac{|u(x)-u(y)|^\alpha}{|x-y|^{1+s\alpha}}\right)^{\frac1\alpha} $$ where $1<\alpha<2$, $0<s<1$ is fixed. Note the $t(\alpha)$ above defines the fractional order sobolev seminorm. See here , roll down for the section ""Sobolev spaces with non-integer $k$"". We know that for $L^p$ space, if $p<q$ then $\|u\|_{L^p(I)}\leq \|u\|_{L^q(I)}$ by using holder inequality. So I am wondering whether similar properties hold for sobolev space with non-integer $k$.(of course, for integer case it is again a Holder inequality) That is, I am wondering for $1<\alpha_1<\alpha_2<2$, do we have $$ t(\alpha_1)\leq Ct(\alpha_2) $$ hold, where $C$ is a constant does not depends on $u$. Just like what we usually have for $L^p$ norm. However, I tried to prove it by using minkowski inequality or sth like holder, but I can't do it... I think the domain $I=(0,1)$ would be important and I also tried to use Jensen inequality, but no lucky... Any ideas? Thank you!",,"['real-analysis', 'measure-theory', 'inequality', 'lebesgue-integral', 'sobolev-spaces']"
87,Solving $x\log x=1$ without numerical methods,Solving  without numerical methods,x\log x=1,"I tried solving $x\log x=1$ where $x \in (1,e) \subset \mathbb{R} $ but couldn't find a nice way to solve it. Given that $x \in (1,e)$, I tried $x:=e^\alpha$ for $\alpha \in [0,1]$ so that I obtain $\alpha e^\alpha=1$. This resembles the familiar exponential function $f(t)=e^{\alpha t}$. I tried to recover some useful relations and I found the following: a) $\forall t \in [0,1]\quad f^n(t)f^{n+1}(t)=\alpha^{2n}$ b) Using the mean-value theorem, I can show that: $\int_{0}^{1} f^n(t)dt=\alpha^{n-1}(\frac{1}{\alpha}-1)$ which isn't too difficult to show given that $\exists c_n \in (0,1) \quad f^{n+1}(c_n)=f^n(1)-f^n(0)=\alpha^n f'(c_0)$ I found that $c_0 = \frac{1}{\alpha}\log(\frac{1}{\alpha^2}-\frac{1}{\alpha})$ But, after all this work I still couldn't figure out how to find $\alpha$. Perhaps there's an additional relation which I simply failed to discover? Anyway, at this point I tried Newton's method: Using $\large t_{n+1}=t_n-\frac{f(t_n)}{f'(t_{n+1)}}=t_n-\frac{t_n\ln(t_n)+t_n^2}{1+t_n}$ I found that $\alpha \approx 0.567143$. But, I don't consider this a nice solution as I haven't found the exact value of $\alpha$. Might there be a better way that doesn't use numerical methods?","I tried solving $x\log x=1$ where $x \in (1,e) \subset \mathbb{R} $ but couldn't find a nice way to solve it. Given that $x \in (1,e)$, I tried $x:=e^\alpha$ for $\alpha \in [0,1]$ so that I obtain $\alpha e^\alpha=1$. This resembles the familiar exponential function $f(t)=e^{\alpha t}$. I tried to recover some useful relations and I found the following: a) $\forall t \in [0,1]\quad f^n(t)f^{n+1}(t)=\alpha^{2n}$ b) Using the mean-value theorem, I can show that: $\int_{0}^{1} f^n(t)dt=\alpha^{n-1}(\frac{1}{\alpha}-1)$ which isn't too difficult to show given that $\exists c_n \in (0,1) \quad f^{n+1}(c_n)=f^n(1)-f^n(0)=\alpha^n f'(c_0)$ I found that $c_0 = \frac{1}{\alpha}\log(\frac{1}{\alpha^2}-\frac{1}{\alpha})$ But, after all this work I still couldn't figure out how to find $\alpha$. Perhaps there's an additional relation which I simply failed to discover? Anyway, at this point I tried Newton's method: Using $\large t_{n+1}=t_n-\frac{f(t_n)}{f'(t_{n+1)}}=t_n-\frac{t_n\ln(t_n)+t_n^2}{1+t_n}$ I found that $\alpha \approx 0.567143$. But, I don't consider this a nice solution as I haven't found the exact value of $\alpha$. Might there be a better way that doesn't use numerical methods?",,['calculus']
88,Is $f(C)$ a convex set if $f$ and $C$ are convex?,Is  a convex set if  and  are convex?,f(C) f C,"The complete claim is, Let $F$ be an ordered field and $\bar F=F\bigcup\{+\infty,-\infty\}$ . Let $f:A \to \bar F$ be a convex map from affine space $(A,V)$ to affine space $(F,F)$ , then $f(C)$ is a convex set if $C$ is convex. Here $F$ can be treated as $\Bbb R$ except for that we do not use its topology and metric. $A$ is a point set, and $V$ is a vector space. A convex function is defined using epigraph, $f$ is a convex set if its epigraph $\text{epi} f=\{(x,y)\in A\times F:y\ge f(x)\}$ is a convex set. I am not sure if this claim is right but it seems so. I find difficulty in proving it. The following is an attempt, but no contradiction is found. If $f(C)$ is not convex, then there exists $y_1,y_2\in f(C)$ st. $y_1>y_2$ and $\theta y_1 + (1-\theta) y_2 \notin f(C)$ for some $\theta \in (0,1)$ . Let $y_3=\theta y_1 + (1-\theta) y_2$ , and suppose $f(x_1)=y_1, f(x_2) = y_2$ for some $x_1,x_2\in C$ . Let $x_3 =\theta x_1 + (1-\theta) x_2)$ , then $(x_1,y_1),(x_2,y_2)\in \text{epi} f \Rightarrow (\theta x_1 + (1-\theta) x_2, \theta y_1 + (1-\theta) y_2) = (x_3,y_3) \in \text{epi} f \Rightarrow y_3 > f(x_3)$ The last inequality $y_3>x_3$ does not contradict with Jensen's inequality. I need help with this. If it is not comfortable to deal with affine space, we might just assume $f:\Bbb R^n \to \Bbb R$ , and just try not to use topology information (such like continuity) and metric in the proof. I add ""real analysis"" as a tag of this question for this reason. Again, note the claim might not be true. If it is not true, hope someone could come up with a counterexample . Thank you!","The complete claim is, Let be an ordered field and . Let be a convex map from affine space to affine space , then is a convex set if is convex. Here can be treated as except for that we do not use its topology and metric. is a point set, and is a vector space. A convex function is defined using epigraph, is a convex set if its epigraph is a convex set. I am not sure if this claim is right but it seems so. I find difficulty in proving it. The following is an attempt, but no contradiction is found. If is not convex, then there exists st. and for some . Let , and suppose for some . Let , then The last inequality does not contradict with Jensen's inequality. I need help with this. If it is not comfortable to deal with affine space, we might just assume , and just try not to use topology information (such like continuity) and metric in the proof. I add ""real analysis"" as a tag of this question for this reason. Again, note the claim might not be true. If it is not true, hope someone could come up with a counterexample . Thank you!","F \bar F=F\bigcup\{+\infty,-\infty\} f:A \to \bar F (A,V) (F,F) f(C) C F \Bbb R A V f \text{epi} f=\{(x,y)\in A\times F:y\ge f(x)\} f(C) y_1,y_2\in f(C) y_1>y_2 \theta y_1 + (1-\theta) y_2 \notin f(C) \theta \in (0,1) y_3=\theta y_1 + (1-\theta) y_2 f(x_1)=y_1, f(x_2) = y_2 x_1,x_2\in C x_3 =\theta x_1 + (1-\theta) x_2) (x_1,y_1),(x_2,y_2)\in \text{epi} f \Rightarrow (\theta x_1 + (1-\theta) x_2, \theta y_1 + (1-\theta) y_2) = (x_3,y_3) \in \text{epi} f \Rightarrow y_3 > f(x_3) y_3>x_3 f:\Bbb R^n \to \Bbb R","['real-analysis', 'convex-analysis', 'convex-optimization']"
89,Apostol's proof of Arzelà's Bounded Convergence Theorem,Apostol's proof of Arzelà's Bounded Convergence Theorem,,"In the first edition of Apostol's Mathematical Analysis, there is a proof of Arzelà's bounded convergence theorem: $\textbf{Theorem 13-17}\,(\textit{Arzel}\grave{a})\textbf{.}$ Assume that $\lbrace f_{n}\rbrace$ is a uniformly bounded, pointwise convergent on $[a,b]$ and suppose that each $f_{n}$ is Riemann-integrable on $[a,b]$. Suppose also that the limit function $f(x):=\lim_{n}f_{n}(x)$ is Riemann-integrable on $[a,b]$. Then, $$ \lim_{n}\int_{[a,b]}f_{n}=\int_{[a,b]}\lim_{n}f_{n}=\int_{[a,b]}f. $$ $\textit{Proof}.$ Let $g_{n}(x)=\left|f_{n}(x)-f(x)\right|$. We will prove that $$ \lim_{n}\int_{[a,b]}g_{n}=0. $$ For this purpose we define a new sequence of functions $\lbrace h_{n}\rbrace$ as follows: $$ h_{n}(x)=\sup_{m\ge n}f_{m}(x),\qquad\text{ if }x\in[a,b]\text{ and }n\in\mathbf{N}. $$ Note that, for each fixed $x$, we have $$ 0\le g_{n}(x)\le h_{n}(x),\qquad h_{n+1}(x)\le h_{n}(x),\qquad \lim_{n} h_{n}(x)=0. $$ Hence, $0\le\int_{[a,b]}g_{n}=\mathop{\underline{\int}}_{[a,b]}g_{n}\le\mathop{\underline{\int}}_{[a,b]}h_{n}.$ (The lower integral $\mathop{\underline{\int}}_{[a,b]} h_{n}$ exists because each $h_{n}$ is bounded on $[a,b]$. However, $h_{n}$ need not to be Riemann-integrable. On the other hand, $g_{n}$ $\textit{is}$ Riemann-integrable, since both $f_{n}$ and $f$ are integrable.) Let $I_{n} = \mathop{\underline{\int}}_{[a,b]} h_{n}$. The proof will be complete if we can show that $I_{n}\to 0$ as $n\to\infty$. Observe that the sequence $\lbrace I_{n}\rbrace$ convergs to $\textit{some}$ non-negative limit. Let $I=\lim_{n}I_{n}$. We will show that the inequality $I>0$ leads to a contradiction. Assume that $I>0$. Since $I_{n}\ge I > I/2$ for each $n$, there is a partition $P_{n}$ of $[a,b]$ and a lower Riemann sum $L(P_{n},h_{n})$ such that $L(P_{n},h_{n})>I/2.$ Let $\varepsilon=\tfrac{1}{2}I/(M+b-a)$, where $M$ is a uniform bound for $\lbrace h_{n}\rbrace$ on $[a,b]$. The lower sum $L(P_{n},h_{n})$ can be split into two parts as follows: $$ L(P_{n},h_{n})=\sum_{i\in A_{n}}m_{i}(h_{n})\Delta x_{i}+\sum_{i\in B_{n}}m_{i}(h_{n})\Delta x_{i}, $$ where $m_{i}(h_{n})$ denotes the $\inf$ of $h_{n}$ in the $i$th subinterval of $P_{n}$ and $$ A_{n}=\lbrace i: m_{i}(h_{n})>\varepsilon\rbrace,\quad B_{n}=\lbrace i:m_{i}(h_{n})\le\varepsilon\rbrace. $$ The inequality $L(P_{n},h_{n}) >I/2$ implies $$ \tfrac{1}{2} I <\sum_{i\in A_{n}}M\Delta x_{i}+\varepsilon\sum_{i\in B_{n}}\Delta x_{i}\le M\sum_{i\in A_{n}}\Delta x_{i}+\varepsilon(b-a), $$ from which we obtain $\sum_{i\in A_{n}}>\varepsilon$. Since refinement of partitions increases lower sums, there is no loss in generality in assuming that the $P_{n}$ are such that $P_{n}\subset P_{n+1}.$ Now let $S_{n}$ denote the union of these subintervals $[x_{i-1},x_{i}]$ of $P_{n}$ for which $i\in A_{n}$. Then $S_{n}$ is a closed set and its Jordan content is $$ c(S_{n})=\sum_{i\in A_{n}}\Delta x_{i}>\varepsilon. $$ This implies that there is ate least one $x$ belonging to infinitely many of the sets $S_{n}$. Hence, for this $x$ we have $h_{n}(x)>\varepsilon$ infinitely often, contradicting the fact that $h_{n}(x)\to 0$ as $n\to\infty$. Thus, $I=0$. Why does $c(S_{n})>\varepsilon$ for all $n$ imply that there exists some $x$ such that $x\in S_{n}$ for infinitely many $n$? Apostol doesn't prove this, but this claim doesn't seem to be entirely obvious. I would appreciate any help.","In the first edition of Apostol's Mathematical Analysis, there is a proof of Arzelà's bounded convergence theorem: $\textbf{Theorem 13-17}\,(\textit{Arzel}\grave{a})\textbf{.}$ Assume that $\lbrace f_{n}\rbrace$ is a uniformly bounded, pointwise convergent on $[a,b]$ and suppose that each $f_{n}$ is Riemann-integrable on $[a,b]$. Suppose also that the limit function $f(x):=\lim_{n}f_{n}(x)$ is Riemann-integrable on $[a,b]$. Then, $$ \lim_{n}\int_{[a,b]}f_{n}=\int_{[a,b]}\lim_{n}f_{n}=\int_{[a,b]}f. $$ $\textit{Proof}.$ Let $g_{n}(x)=\left|f_{n}(x)-f(x)\right|$. We will prove that $$ \lim_{n}\int_{[a,b]}g_{n}=0. $$ For this purpose we define a new sequence of functions $\lbrace h_{n}\rbrace$ as follows: $$ h_{n}(x)=\sup_{m\ge n}f_{m}(x),\qquad\text{ if }x\in[a,b]\text{ and }n\in\mathbf{N}. $$ Note that, for each fixed $x$, we have $$ 0\le g_{n}(x)\le h_{n}(x),\qquad h_{n+1}(x)\le h_{n}(x),\qquad \lim_{n} h_{n}(x)=0. $$ Hence, $0\le\int_{[a,b]}g_{n}=\mathop{\underline{\int}}_{[a,b]}g_{n}\le\mathop{\underline{\int}}_{[a,b]}h_{n}.$ (The lower integral $\mathop{\underline{\int}}_{[a,b]} h_{n}$ exists because each $h_{n}$ is bounded on $[a,b]$. However, $h_{n}$ need not to be Riemann-integrable. On the other hand, $g_{n}$ $\textit{is}$ Riemann-integrable, since both $f_{n}$ and $f$ are integrable.) Let $I_{n} = \mathop{\underline{\int}}_{[a,b]} h_{n}$. The proof will be complete if we can show that $I_{n}\to 0$ as $n\to\infty$. Observe that the sequence $\lbrace I_{n}\rbrace$ convergs to $\textit{some}$ non-negative limit. Let $I=\lim_{n}I_{n}$. We will show that the inequality $I>0$ leads to a contradiction. Assume that $I>0$. Since $I_{n}\ge I > I/2$ for each $n$, there is a partition $P_{n}$ of $[a,b]$ and a lower Riemann sum $L(P_{n},h_{n})$ such that $L(P_{n},h_{n})>I/2.$ Let $\varepsilon=\tfrac{1}{2}I/(M+b-a)$, where $M$ is a uniform bound for $\lbrace h_{n}\rbrace$ on $[a,b]$. The lower sum $L(P_{n},h_{n})$ can be split into two parts as follows: $$ L(P_{n},h_{n})=\sum_{i\in A_{n}}m_{i}(h_{n})\Delta x_{i}+\sum_{i\in B_{n}}m_{i}(h_{n})\Delta x_{i}, $$ where $m_{i}(h_{n})$ denotes the $\inf$ of $h_{n}$ in the $i$th subinterval of $P_{n}$ and $$ A_{n}=\lbrace i: m_{i}(h_{n})>\varepsilon\rbrace,\quad B_{n}=\lbrace i:m_{i}(h_{n})\le\varepsilon\rbrace. $$ The inequality $L(P_{n},h_{n}) >I/2$ implies $$ \tfrac{1}{2} I <\sum_{i\in A_{n}}M\Delta x_{i}+\varepsilon\sum_{i\in B_{n}}\Delta x_{i}\le M\sum_{i\in A_{n}}\Delta x_{i}+\varepsilon(b-a), $$ from which we obtain $\sum_{i\in A_{n}}>\varepsilon$. Since refinement of partitions increases lower sums, there is no loss in generality in assuming that the $P_{n}$ are such that $P_{n}\subset P_{n+1}.$ Now let $S_{n}$ denote the union of these subintervals $[x_{i-1},x_{i}]$ of $P_{n}$ for which $i\in A_{n}$. Then $S_{n}$ is a closed set and its Jordan content is $$ c(S_{n})=\sum_{i\in A_{n}}\Delta x_{i}>\varepsilon. $$ This implies that there is ate least one $x$ belonging to infinitely many of the sets $S_{n}$. Hence, for this $x$ we have $h_{n}(x)>\varepsilon$ infinitely often, contradicting the fact that $h_{n}(x)\to 0$ as $n\to\infty$. Thus, $I=0$. Why does $c(S_{n})>\varepsilon$ for all $n$ imply that there exists some $x$ such that $x\in S_{n}$ for infinitely many $n$? Apostol doesn't prove this, but this claim doesn't seem to be entirely obvious. I would appreciate any help.",,"['real-analysis', 'analysis']"
90,Measurable function and the Mean Value Theorem,Measurable function and the Mean Value Theorem,,"Let $\,f:[a,b]\to \mathbb{R}\,$ be continuous on $[a,b]$ and derivable on $(a,b)$. By the mean value property, for all $\,x\in (a,b)\,$ there exists $\,\xi_x\in (a,x)\,$ such that $\,f(x)-f(a)=f'\left(\xi_x\right)(x-a)$. Let $\,h:(a,b)\to\mathbb{R}\,$ be defined as $\,h(x)=\xi_x$. Is the function $h$ measurable? Is $\,f'\!\circ h\,$ measurable? The motivation for this question is the following: in an exercise in class, I estimated $\,\int_{\mathbb{R}} \left\lvert F(x)-F(-x)\right\rvert dx\,$ for a certain ""good"" function $\,F$ using $\,\leq \int_{\mathbb{R}} \left\lvert F'\left(\xi_x\right)\right\rvert 2\left\lvert x\right\rvert dx,\,$ but the professor told me to be careful, as he wasn't sure of the measurability of $\,x\mapsto F'\left(\xi_x\right)$. Edit: Thank you for your answers. Assuming that $x\mapsto \xi_x$ is well-defined (taking $\xi_x$ as the smallest possible), is it measurable?","Let $\,f:[a,b]\to \mathbb{R}\,$ be continuous on $[a,b]$ and derivable on $(a,b)$. By the mean value property, for all $\,x\in (a,b)\,$ there exists $\,\xi_x\in (a,x)\,$ such that $\,f(x)-f(a)=f'\left(\xi_x\right)(x-a)$. Let $\,h:(a,b)\to\mathbb{R}\,$ be defined as $\,h(x)=\xi_x$. Is the function $h$ measurable? Is $\,f'\!\circ h\,$ measurable? The motivation for this question is the following: in an exercise in class, I estimated $\,\int_{\mathbb{R}} \left\lvert F(x)-F(-x)\right\rvert dx\,$ for a certain ""good"" function $\,F$ using $\,\leq \int_{\mathbb{R}} \left\lvert F'\left(\xi_x\right)\right\rvert 2\left\lvert x\right\rvert dx,\,$ but the professor told me to be careful, as he wasn't sure of the measurability of $\,x\mapsto F'\left(\xi_x\right)$. Edit: Thank you for your answers. Assuming that $x\mapsto \xi_x$ is well-defined (taking $\xi_x$ as the smallest possible), is it measurable?",,"['real-analysis', 'measure-theory']"
91,"Let $f_n$ integrable in $[a,b]$ for all $n$. Show that if $(f_n)\to f$ uniformly in $[a,b]$ then $f$ is integrable in $[a,b]$",Let  integrable in  for all . Show that if  uniformly in  then  is integrable in,"f_n [a,b] n (f_n)\to f [a,b] f [a,b]","I want a check of this proof because I'm not completely sure about the manipulation in some inequalities. Let $f_n$ integrable in $[a,b]$ for all $n$ . Show that if $(f_n)\to f$ uniformly in $[a,b]$ then $f$ is integrable in $[a,b]$ Using Darboux upper and lower sums if $f_n$ is integrable in $[a,b]$ this mean $$\forall \varepsilon>0,\exists P\in\mathcal P: \sum_{j=1}^{H} (M_j-m_j)\Delta x_j<\varepsilon\tag{1}$$ where $M_j$ and $m_j$ are the supremum and infimum of $f_n(x)$ in $[x_j,x_{j+1}]$ , and where $\mathcal P$ is the set of all partitions of $[a,b]$ . And we have that if $f(x)\to f$ uniformly in $[a,b]$ then $$\forall\varepsilon>0,\exists N\in\Bbb N,\forall x\in[a,b]:|f_n(x)-f(x)|<\varepsilon,\,\forall n\ge N\tag{2}$$ Then due to $(2)$ there exists $N$ such that $|f_n(x)-f(x)|<\frac{\varepsilon}{3(b-a)},\forall n\ge N$ and $\forall x\in[a,b]$ . And due to $(1)$ for $\frac{\varepsilon}{3}$ and $f_n$ exists some $P_{\varepsilon,n}\in\mathcal P$ such that $$\sum_{j=1}^{H} (M_{n,j}-m_{n,j})\Delta x_j <\frac{\varepsilon}{3}$$ If $\sum_{j=1}^{H} (M_{j}-m_{j})\Delta x_j$ is the difference between upper and lower sum of $f$ using the partition $P_{\varepsilon,n}$ then $$\sum_{j=1}^{H} (M_{n,j}-m_{n,j})\Delta x_j - \sum_{j=1}^{H} (M_{j}-m_{j})\Delta x_j+\sum_{j=1}^{H} (M_{j}-m_{j})\Delta x_j<\frac{\varepsilon}{3}$$ $$\sum_{j=1}^{H} (M_{j}-m_{j})\Delta x_j<\frac{\varepsilon}{3}-\sum_{j=1}^{H} (M_{n,j}-m_{n,j})\Delta x_j + \sum_{j=1}^{H} (M_{j}-m_{j})\Delta x_j$$ $$\sum_{j=1}^{H} (M_{j}-m_{j})\Delta x_j<\frac{\varepsilon}{3} + \sum_{j=1}^{H} ((M_{j}-M_{n,j})+(m_{n,j}-m_{j}))\Delta x_j$$ $$\sum_{j=1}^{H} (M_{j}-m_{j})\Delta x_j<\frac{\varepsilon}{3} + \sum_{j=1}^{H} (|M_{j}-M_{n,j}|+|m_{n,j}-m_{j}|)\Delta x_j$$ $$\sum_{j=1}^{H} (M_{j}-m_{j})\Delta x_j<\frac{\varepsilon}{3} + \sum_{j=1}^{H} \frac{2\varepsilon}{3(b-a)}\Delta x_j=\frac{\varepsilon}{3} + \frac{2\varepsilon}{3(b-a)}\sum_{j=1}^{H}\Delta x_j$$ $$\sum_{j=1}^{H} (M_{j}-m_{j})\Delta x_j<\frac{\varepsilon}{3} + \frac{2\varepsilon}{3(b-a)}(b-a)=\varepsilon$$ Then for $f$ exists partitions where the difference between upper and lower sum is arbitrarily close to zero, so $f$ is integrable in $[a,b]$ .","I want a check of this proof because I'm not completely sure about the manipulation in some inequalities. Let integrable in for all . Show that if uniformly in then is integrable in Using Darboux upper and lower sums if is integrable in this mean where and are the supremum and infimum of in , and where is the set of all partitions of . And we have that if uniformly in then Then due to there exists such that and . And due to for and exists some such that If is the difference between upper and lower sum of using the partition then Then for exists partitions where the difference between upper and lower sum is arbitrarily close to zero, so is integrable in .","f_n [a,b] n (f_n)\to f [a,b] f [a,b] f_n [a,b] \forall \varepsilon>0,\exists P\in\mathcal P: \sum_{j=1}^{H} (M_j-m_j)\Delta x_j<\varepsilon\tag{1} M_j m_j f_n(x) [x_j,x_{j+1}] \mathcal P [a,b] f(x)\to f [a,b] \forall\varepsilon>0,\exists N\in\Bbb N,\forall x\in[a,b]:|f_n(x)-f(x)|<\varepsilon,\,\forall n\ge N\tag{2} (2) N |f_n(x)-f(x)|<\frac{\varepsilon}{3(b-a)},\forall n\ge N \forall x\in[a,b] (1) \frac{\varepsilon}{3} f_n P_{\varepsilon,n}\in\mathcal P \sum_{j=1}^{H} (M_{n,j}-m_{n,j})\Delta x_j <\frac{\varepsilon}{3} \sum_{j=1}^{H} (M_{j}-m_{j})\Delta x_j f P_{\varepsilon,n} \sum_{j=1}^{H} (M_{n,j}-m_{n,j})\Delta x_j - \sum_{j=1}^{H} (M_{j}-m_{j})\Delta x_j+\sum_{j=1}^{H} (M_{j}-m_{j})\Delta x_j<\frac{\varepsilon}{3} \sum_{j=1}^{H} (M_{j}-m_{j})\Delta x_j<\frac{\varepsilon}{3}-\sum_{j=1}^{H} (M_{n,j}-m_{n,j})\Delta x_j + \sum_{j=1}^{H} (M_{j}-m_{j})\Delta x_j \sum_{j=1}^{H} (M_{j}-m_{j})\Delta x_j<\frac{\varepsilon}{3} + \sum_{j=1}^{H} ((M_{j}-M_{n,j})+(m_{n,j}-m_{j}))\Delta x_j \sum_{j=1}^{H} (M_{j}-m_{j})\Delta x_j<\frac{\varepsilon}{3} + \sum_{j=1}^{H} (|M_{j}-M_{n,j}|+|m_{n,j}-m_{j}|)\Delta x_j \sum_{j=1}^{H} (M_{j}-m_{j})\Delta x_j<\frac{\varepsilon}{3} + \sum_{j=1}^{H} \frac{2\varepsilon}{3(b-a)}\Delta x_j=\frac{\varepsilon}{3} + \frac{2\varepsilon}{3(b-a)}\sum_{j=1}^{H}\Delta x_j \sum_{j=1}^{H} (M_{j}-m_{j})\Delta x_j<\frac{\varepsilon}{3} + \frac{2\varepsilon}{3(b-a)}(b-a)=\varepsilon f f [a,b]","['real-analysis', 'proof-verification']"
92,Showing that $\prod_{n=1}^{\infty}\left(1+\frac{1}{F_{2^n+1}L_{2^n+1}}\right)=\frac{3}{\phi^2}$,Showing that,\prod_{n=1}^{\infty}\left(1+\frac{1}{F_{2^n+1}L_{2^n+1}}\right)=\frac{3}{\phi^2},"Infinite product $F_{n}:=[1,1,2,3,5,8,\cdots]$ and $L_{n}:=[1,3,4,7,\cdots]$ for $n=1,2,3,\cdots$ respectively. $\frac{1+\sqrt5}{2}=\phi$ Show that, $$\prod_{n=1}^{\infty}\left(1+\frac{1}{F_{2^n+1}L_{2^n+1}}\right)=\frac{3}{\phi^2}$$ We took the idea from this site Expand the product (1) $$\left(1+\frac{1}{F_{3}L_{3}}\right)\cdot\left(1+\frac{1}{F_{5}L_{5}}\right)\cdot\left(1+\frac{1}{F_{9}L_{9}}\right)\cdots=\frac{3}{\phi^2}$$ $F_{n}=\frac{\phi^n-(-\phi)^{-n}}{\sqrt5}$ $L_{n}=\phi^n+(-\phi)^{-n}$ $F_{n}L_{n}=\frac{\phi^{2n}-(-\phi)^{-2n}}{\sqrt5}=F_{2n}$ Rewrite (1) $$\left(1+\frac{1}{F_{6}}\right)\cdot\left(1+\frac{1}{F_{10}}\right)\cdot\left(1+\frac{1}{F_{18}}\right)\cdots=\frac{3}{\phi^2}$$ Still doesn't help much to get from LHS to RHS. We have $F_{2n}=F^2_{n+1}-F^2_{n-1}$ I have substituted in, but the formula seem too messy and more complicated. Can anybody please give a hand?","Infinite product $F_{n}:=[1,1,2,3,5,8,\cdots]$ and $L_{n}:=[1,3,4,7,\cdots]$ for $n=1,2,3,\cdots$ respectively. $\frac{1+\sqrt5}{2}=\phi$ Show that, $$\prod_{n=1}^{\infty}\left(1+\frac{1}{F_{2^n+1}L_{2^n+1}}\right)=\frac{3}{\phi^2}$$ We took the idea from this site Expand the product (1) $$\left(1+\frac{1}{F_{3}L_{3}}\right)\cdot\left(1+\frac{1}{F_{5}L_{5}}\right)\cdot\left(1+\frac{1}{F_{9}L_{9}}\right)\cdots=\frac{3}{\phi^2}$$ $F_{n}=\frac{\phi^n-(-\phi)^{-n}}{\sqrt5}$ $L_{n}=\phi^n+(-\phi)^{-n}$ $F_{n}L_{n}=\frac{\phi^{2n}-(-\phi)^{-2n}}{\sqrt5}=F_{2n}$ Rewrite (1) $$\left(1+\frac{1}{F_{6}}\right)\cdot\left(1+\frac{1}{F_{10}}\right)\cdot\left(1+\frac{1}{F_{18}}\right)\cdots=\frac{3}{\phi^2}$$ Still doesn't help much to get from LHS to RHS. We have $F_{2n}=F^2_{n+1}-F^2_{n-1}$ I have substituted in, but the formula seem too messy and more complicated. Can anybody please give a hand?",,"['calculus', 'real-analysis']"
93,Find this limit $\frac{a_n}{b_n}$,Find this limit,\frac{a_n}{b_n},"Suppose that $a_0 = \sqrt5$, $a_{n+1} = 2a_n^2-1$. Define $b_n :=2^na_0a_1...a_{n-1}$. Show that $\lim_{n\to\infty}\frac{a_n}{b_n} = 2$. I can show that this limit exists by showing that the sequence is contractive, after which I have no clue how to find its value. Could someone point me in the right direction? The limit sort of makes sense because both $a_n$ and $b_n$ are more or less doubling in value each time, with the former one 'step' ahead of the latter","Suppose that $a_0 = \sqrt5$, $a_{n+1} = 2a_n^2-1$. Define $b_n :=2^na_0a_1...a_{n-1}$. Show that $\lim_{n\to\infty}\frac{a_n}{b_n} = 2$. I can show that this limit exists by showing that the sequence is contractive, after which I have no clue how to find its value. Could someone point me in the right direction? The limit sort of makes sense because both $a_n$ and $b_n$ are more or less doubling in value each time, with the former one 'step' ahead of the latter",,"['real-analysis', 'limits']"
94,$n$-th derivative of Beta function,-th derivative of Beta function,n,"We pretty much know nothing about the high order derivatives of the Beta function. Well, we known for the example some recursive formulae for $\Gamma^{(n)}(1)$ as well as $\Gamma^{(n)}\left(\frac{1}{2}\right)$ but that is all. Yesterday , I encountered the following integral: $$\int_{0}^{\infty} \frac{\log^n x}{\left ( 1+x^2 \right )^m}\, {\rm d}x$$ As it can easily be seen , we have that: $$\int_{0}^{\infty} \frac{\log x}{\left ( 1+x^2 \right )^m}\, {\rm d}x ={\rm B}^{(1)} \left ( \frac{1}{2}, m- \frac{1}{2} \right ) \implies  \int_{0}^{\infty} \frac{\log^n x}{\left ( 1+x^2 \right )^m}\, {\rm d}x = {\rm B}^{(n)} \left ( \frac{1}{2}, m- \frac{1}{2} \right )$$ The question is whether or not we can express that last $n$-th derivative in a closed form - most likely in an inductive form. I guess it will be difficult but with mathematics you never know.","We pretty much know nothing about the high order derivatives of the Beta function. Well, we known for the example some recursive formulae for $\Gamma^{(n)}(1)$ as well as $\Gamma^{(n)}\left(\frac{1}{2}\right)$ but that is all. Yesterday , I encountered the following integral: $$\int_{0}^{\infty} \frac{\log^n x}{\left ( 1+x^2 \right )^m}\, {\rm d}x$$ As it can easily be seen , we have that: $$\int_{0}^{\infty} \frac{\log x}{\left ( 1+x^2 \right )^m}\, {\rm d}x ={\rm B}^{(1)} \left ( \frac{1}{2}, m- \frac{1}{2} \right ) \implies  \int_{0}^{\infty} \frac{\log^n x}{\left ( 1+x^2 \right )^m}\, {\rm d}x = {\rm B}^{(n)} \left ( \frac{1}{2}, m- \frac{1}{2} \right )$$ The question is whether or not we can express that last $n$-th derivative in a closed form - most likely in an inductive form. I guess it will be difficult but with mathematics you never know.",,"['calculus', 'real-analysis', 'improper-integrals', 'special-functions']"
95,"$m+ni+k\lambda,\,\Re(\lambda),\Im(\lambda)\notin \mathbb{Q}$ is dense in $\mathbb{C}$!",is dense in !,"m+ni+k\lambda,\,\Re(\lambda),\Im(\lambda)\notin \mathbb{Q} \mathbb{C}","As said in the comments below, it's needed to suppose $\{1,\Re(\lambda),\Im(\lambda)\}$ linearly independent over $\mathbb{Q}$, otherwise the result is false, according to Christian's example. This is an exercise of the book Funções de uma Variável Complexa, by Alcides Lins Neto and, well, let's see it before I explain my doubt: Consider $\lambda\in \mathbb{C}$ such that $\Re(\lambda)$ and $\Im(\lambda)$ are NOT rational numbers. Prove that the set   $$\{m+ni+k\lambda; m,n,k\in \mathbb{Z}\}$$   is a dense subset of $\mathbb{C}$. This is exercise 19 of the very first section of exercises in the book. At exercise 18 is given a suggestion (which, I suppose, may also be used at the 19th) that is: If $a\notin\mathbb{Q}$ then the set $\{m+na;m,n\in \mathbb{Z}\}$ is dense in $\mathbb{R}$. Let's see what I've tried. Let $z_0$ be any complex number and $\varepsilon>0$ be any ""radius"". In order to show that that set is dense in $\mathbb{C}$ is sufficient to show that there is an element of it in the square $$R_\varepsilon(z_0)=\left\{u+iv\in \mathbb{C};|u-\Re(z_0)|<\varepsilon,\,\,\, |v-\Im(z_0)|<\varepsilon\right\}.$$ It is sufficient because the collection of ""open squares"" is a basis for topology on $\mathbb{C}\cong\mathbb{R}^2$. Now, using the suggestion, we have that there are $m_1,n_1,m_2,n_2\in \mathbb{Z}$ such that $$\begin{array}{c}\Re(z_0)-\varepsilon<m_1+n_1\Re(\lambda)<\Re(z_0)+\varepsilon\\ \Im(z_0)-\varepsilon<m_2+n_2\Im(\lambda)<\Im(z_0)+\varepsilon\end{array}.$$ So we have that the number $u+iv$ given by $$\begin{array}{rcl}u&=&m_1+n_1\Re(\lambda),\\ v&=&m_2+n_2\Im(\lambda),\end{array}$$ is in the square $R_\varepsilon(z_0)$ ""near"" $z_0$. But now we have the following: $$u+iv=m_1+m_2i+n_1\Re(\lambda)+n_2\Im(\lambda)i,$$ and this IS NOT NECESSARILY a point of the form $m+ni+k\lambda$, since $n_1$ mights be different from $n_2$. After this I've tried to think something about least common multiple, tried to find a pair to which $n_1=n_2$, or tried to fix $n_1=n_2$ and then to search for suitable $m_1$ and $m_2$: all of them without any success... How to garantee $n_1=n_2$?? Or maybe there is a completely different way of proving it... Thank you guys!","As said in the comments below, it's needed to suppose $\{1,\Re(\lambda),\Im(\lambda)\}$ linearly independent over $\mathbb{Q}$, otherwise the result is false, according to Christian's example. This is an exercise of the book Funções de uma Variável Complexa, by Alcides Lins Neto and, well, let's see it before I explain my doubt: Consider $\lambda\in \mathbb{C}$ such that $\Re(\lambda)$ and $\Im(\lambda)$ are NOT rational numbers. Prove that the set   $$\{m+ni+k\lambda; m,n,k\in \mathbb{Z}\}$$   is a dense subset of $\mathbb{C}$. This is exercise 19 of the very first section of exercises in the book. At exercise 18 is given a suggestion (which, I suppose, may also be used at the 19th) that is: If $a\notin\mathbb{Q}$ then the set $\{m+na;m,n\in \mathbb{Z}\}$ is dense in $\mathbb{R}$. Let's see what I've tried. Let $z_0$ be any complex number and $\varepsilon>0$ be any ""radius"". In order to show that that set is dense in $\mathbb{C}$ is sufficient to show that there is an element of it in the square $$R_\varepsilon(z_0)=\left\{u+iv\in \mathbb{C};|u-\Re(z_0)|<\varepsilon,\,\,\, |v-\Im(z_0)|<\varepsilon\right\}.$$ It is sufficient because the collection of ""open squares"" is a basis for topology on $\mathbb{C}\cong\mathbb{R}^2$. Now, using the suggestion, we have that there are $m_1,n_1,m_2,n_2\in \mathbb{Z}$ such that $$\begin{array}{c}\Re(z_0)-\varepsilon<m_1+n_1\Re(\lambda)<\Re(z_0)+\varepsilon\\ \Im(z_0)-\varepsilon<m_2+n_2\Im(\lambda)<\Im(z_0)+\varepsilon\end{array}.$$ So we have that the number $u+iv$ given by $$\begin{array}{rcl}u&=&m_1+n_1\Re(\lambda),\\ v&=&m_2+n_2\Im(\lambda),\end{array}$$ is in the square $R_\varepsilon(z_0)$ ""near"" $z_0$. But now we have the following: $$u+iv=m_1+m_2i+n_1\Re(\lambda)+n_2\Im(\lambda)i,$$ and this IS NOT NECESSARILY a point of the form $m+ni+k\lambda$, since $n_1$ mights be different from $n_2$. After this I've tried to think something about least common multiple, tried to find a pair to which $n_1=n_2$, or tried to fix $n_1=n_2$ and then to search for suitable $m_1$ and $m_2$: all of them without any success... How to garantee $n_1=n_2$?? Or maybe there is a completely different way of proving it... Thank you guys!",,"['real-analysis', 'general-topology', 'complex-analysis', 'complex-numbers']"
96,Prove expansive function on a compact set is surjective.,Prove expansive function on a compact set is surjective.,,"Let $M$ be a compact set and $(M,d)$ be a metric space, define function $f:M\to M$ such that for all $\,p,q\in M$ $$d(f(p),f(q))\ge d(p,q)$$   Prove $f$ is surjective. I observed that compactness of $M$ is crucial, supposing $M$ being only closed or bounded would have counterexamples of $f$. Also I found another problem which might be related somehow, I couldn't help this but guessing we have to prove $f$ is isometric. @TommasoSeneci Thank you very much for the efforts, but as we see problem is not completely solved, so any clarification of showing $Y$(discussed in the below answer) is closed or other ways maybe, would be so appreciated!","Let $M$ be a compact set and $(M,d)$ be a metric space, define function $f:M\to M$ such that for all $\,p,q\in M$ $$d(f(p),f(q))\ge d(p,q)$$   Prove $f$ is surjective. I observed that compactness of $M$ is crucial, supposing $M$ being only closed or bounded would have counterexamples of $f$. Also I found another problem which might be related somehow, I couldn't help this but guessing we have to prove $f$ is isometric. @TommasoSeneci Thank you very much for the efforts, but as we see problem is not completely solved, so any clarification of showing $Y$(discussed in the below answer) is closed or other ways maybe, would be so appreciated!",,"['real-analysis', 'functions', 'metric-spaces', 'compactness']"
97,Part of proof of Prime Number Theorem,Part of proof of Prime Number Theorem,,"If $x\ge 1$, let $\pi(x) = \sum_{p\le x} 1$ denote the number of primes $\le x$. The prime number theorem states that $\pi(x) \sim {x\over \log(x)}$ This is usually proved by studying the related function $\theta (x)=\sum_{p\le x} \log (p)$. I want to show that the prime number theorem is equivalent to the relation $\theta(x) \sim x$. I already derived the following equation:  $$\theta(x)= \pi(x)\log(x)-\int_{x}^2 {\pi(t)\over t}dt $$ $$\pi(x)={\theta(x)\over \log(x)} + \int_{x}^2 {\theta(t)\over {t \log^2t}}dt$$ It is also known that $\theta(x) \sim x$, i.e. $\lim_{x\to \infty}{ {\theta(x)\over x}}=1$, and $$\int_{x}^2{1\over \log(y)} dy={x\over \log(x)} + o({x\over \log(x)})$$ I also derived: $$\int_{x}^2{1\over \log^2(y)}dy = \textrm{Li}(x) - {x\over \log(x)} + {2\over \log(2)}$$, where $Li(x) = \int_{x}^2 {dt\over \log(t)}$, $o$ is the Hardy's little o notation. Assuming these pieces of information, I'm having trouble establishing the prime number theorem. Could someone complete the proof please? Thanks.","If $x\ge 1$, let $\pi(x) = \sum_{p\le x} 1$ denote the number of primes $\le x$. The prime number theorem states that $\pi(x) \sim {x\over \log(x)}$ This is usually proved by studying the related function $\theta (x)=\sum_{p\le x} \log (p)$. I want to show that the prime number theorem is equivalent to the relation $\theta(x) \sim x$. I already derived the following equation:  $$\theta(x)= \pi(x)\log(x)-\int_{x}^2 {\pi(t)\over t}dt $$ $$\pi(x)={\theta(x)\over \log(x)} + \int_{x}^2 {\theta(t)\over {t \log^2t}}dt$$ It is also known that $\theta(x) \sim x$, i.e. $\lim_{x\to \infty}{ {\theta(x)\over x}}=1$, and $$\int_{x}^2{1\over \log(y)} dy={x\over \log(x)} + o({x\over \log(x)})$$ I also derived: $$\int_{x}^2{1\over \log^2(y)}dy = \textrm{Li}(x) - {x\over \log(x)} + {2\over \log(2)}$$, where $Li(x) = \int_{x}^2 {dt\over \log(t)}$, $o$ is the Hardy's little o notation. Assuming these pieces of information, I'm having trouble establishing the prime number theorem. Could someone complete the proof please? Thanks.",,"['real-analysis', 'integration', 'prime-numbers']"
98,Can someone confirm this sketch of a proof for the existence of smooth partitions of unity on an unbounded convex open euclidean set?,Can someone confirm this sketch of a proof for the existence of smooth partitions of unity on an unbounded convex open euclidean set?,,"EDIT!!  The problem originally described (see below) has been reduced to the correctness of a simple extension of an argument from Rudin's PMA.  Feel free to skip to the proposed solution, below. As part of a much needed calculus review, I've been working through Rudin's Principles of Mathematical Analysis.  Besides the following, I've found the prospective on the material therein to be rather illuminating -- which leads me to believe I've missed something, in this case. The proof of a lemma (10.38) to be used in the proof of Poincare's Lemma (10.39) cites the following fact: (to paraphrase) $V=\{(x,y)|x\in\Bbb{R^{p-1}, y\in\Bbb{R}}\}; p>1$; V convex open.  $U=\{x|(x,y)\in V \text{ for some y}\}$.  It follows that there is a continuously differentiable function on U, let's call it $\alpha$, such that $(x,\alpha(x))\in V.$ This is one of very few proof details in Rudin explicitly left as an exercise.  The hint is that $\alpha$ may be taken to be a constant if V is a ball; it should also be noted that the same $\alpha$ notation is conspicuously used in the proof of the existence of partitions of unity on compact sets. Now, ignoring the hints and using inf, sup, and handling the special case of extended values, I think I can find a continuous $\alpha$ easily enough, but the resulting function is not necessarily smooth. I've seen a totally different solution involving countable open coverings, compact exhaustions, and smooth partitions of unity on open sets, but, although this solution makes use of the hints, I'm apprehensive about this scheme because it relies on so much machinery not mentioned in the text or exercises elsewhere in the book.  It would be very uncharacteristic of the author if this were the intended solution (I say this having worked hundreds of other exercises from his books). With that in mind, can someone propose a solution scheme that does not rely on so much beyond-the-text machinery? EDIT!!  In particular, after examining Rudin's proof of the existence of partitions of unity on compact sets, I believe I have found an extension of his argument that works in this case.  Can anyone verify this?  (See below for my proposed answer).","EDIT!!  The problem originally described (see below) has been reduced to the correctness of a simple extension of an argument from Rudin's PMA.  Feel free to skip to the proposed solution, below. As part of a much needed calculus review, I've been working through Rudin's Principles of Mathematical Analysis.  Besides the following, I've found the prospective on the material therein to be rather illuminating -- which leads me to believe I've missed something, in this case. The proof of a lemma (10.38) to be used in the proof of Poincare's Lemma (10.39) cites the following fact: (to paraphrase) $V=\{(x,y)|x\in\Bbb{R^{p-1}, y\in\Bbb{R}}\}; p>1$; V convex open.  $U=\{x|(x,y)\in V \text{ for some y}\}$.  It follows that there is a continuously differentiable function on U, let's call it $\alpha$, such that $(x,\alpha(x))\in V.$ This is one of very few proof details in Rudin explicitly left as an exercise.  The hint is that $\alpha$ may be taken to be a constant if V is a ball; it should also be noted that the same $\alpha$ notation is conspicuously used in the proof of the existence of partitions of unity on compact sets. Now, ignoring the hints and using inf, sup, and handling the special case of extended values, I think I can find a continuous $\alpha$ easily enough, but the resulting function is not necessarily smooth. I've seen a totally different solution involving countable open coverings, compact exhaustions, and smooth partitions of unity on open sets, but, although this solution makes use of the hints, I'm apprehensive about this scheme because it relies on so much machinery not mentioned in the text or exercises elsewhere in the book.  It would be very uncharacteristic of the author if this were the intended solution (I say this having worked hundreds of other exercises from his books). With that in mind, can someone propose a solution scheme that does not rely on so much beyond-the-text machinery? EDIT!!  In particular, after examining Rudin's proof of the existence of partitions of unity on compact sets, I believe I have found an extension of his argument that works in this case.  Can anyone verify this?  (See below for my proposed answer).",,"['real-analysis', 'differential-geometry', 'convex-analysis', 'smooth-manifolds']"
99,When does a linear combination of trigonometric functions have an axis of symmetry?,When does a linear combination of trigonometric functions have an axis of symmetry?,,"I am trying to find out when a linear combination of $\sin(ax)$ and $\cos(bx)$ has an axis of symmetry. Clearly, $\sin(x)+\cos(x)$ has an axis of symmetry at $\pi/4$. It seems as if $\sin(3 x)+\cos(x)$ does not have an axis of symmetry (could not prove it, but a plot suggests it). My question is: Is it possible to claim some conditions on the parameters $a$ and $b$ guaranteeing that a linear combination of $\sin(a x)$ and $\cos(b x)$ has an axis of symmetry?","I am trying to find out when a linear combination of $\sin(ax)$ and $\cos(bx)$ has an axis of symmetry. Clearly, $\sin(x)+\cos(x)$ has an axis of symmetry at $\pi/4$. It seems as if $\sin(3 x)+\cos(x)$ does not have an axis of symmetry (could not prove it, but a plot suggests it). My question is: Is it possible to claim some conditions on the parameters $a$ and $b$ guaranteeing that a linear combination of $\sin(a x)$ and $\cos(b x)$ has an axis of symmetry?",,"['real-analysis', 'trigonometry', 'periodic-functions', 'symmetry']"
