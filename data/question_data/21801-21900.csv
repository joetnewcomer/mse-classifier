,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,When does $S \cap GL_4(\mathbb R)$ have precisely two connected components where $S \subset M_4(\mathbb R)$ is a linear subspace?,When does  have precisely two connected components where  is a linear subspace?,S \cap GL_4(\mathbb R) S \subset M_4(\mathbb R),"This is related to the question How many connected components for the intersection $S \cap GL_n(\mathbb R)$ where $S \subset M_n(\mathbb R)$ is a linear subspace? I asked. There is a nice example in the answer to show the intersection does not need to have two connected components as $GL_n(\mathbb R)$. The comment below by Travis is also very helpful. Let $S \subset M_n(\mathbb R^n)$ be a linear subspace. What property should $S$ process such that $S \cap GL_n(\mathbb R)$ has two connected components? I realized the question I asked before (crossed out above) might be too general to answer. Since it has not been answered, let me ask the specific question I am considering. The question is now cross-posted here at MO. Let $A \in M_4(\mathbb R)$ and $A = (e_2, x, e_4, y)$ where $e_2, e_4$ are standard basis in $\mathbb R^4$ and $x,y$ are undetermined variables. Let $\phi, \psi: M_4(\mathbb R) \to \mathbb R^4$ be linear maps defined by   \begin{align*} &\phi: B \mapsto (AB-BA) e_1, \\ &\psi: B \mapsto (AB-BA)e_3. \end{align*}   The subspace $S$ I am interested in is the intersection of kernels of the two linear maps, i.e., $S :=\text{ker}(\phi) \cap \text{ker}{\psi}$. In other words, the elements in $S \cap GL_4(\mathbb R)$ would preserve the structure of first and third columns of $A$ by conjugation, i.e., $(B^{-1}AB) e_1 = e_2, (B^{-1}AB)e_3 = e_4$ for $B \in S \cap GL_4(\mathbb R)$. I would like to determine: whether there exists $A$ with eigenvalues all lying on the left open half plane of $\mathbb C$, i.e., with negative real parts ( we can freely choose $x, y$) such that $S \cap GL_4(\mathbb R)$ has precisely two connected components or precisely one component. If there exists $A$, such that $\{V^{-1} A V: V \in S \cap GL_4(\mathbb R)\}$ is connected. Edit 1: If $S \cap GL_4(\mathbb R)$ has precisely two connected components, I guess they should be $S \cap GL_4(\mathbb R)_+$ and $S \cap GL_4(\mathbb R)_-$. So if $V \in S$ and $\det(V) > 0$, $V$ should be path-connected with $I$. It is not hard to check the condition implies $V = (v_1, Av_1, v_3, Av_3)$. Since $e_2 = Ae_1, e_4 = Ae_3$, the question is can we continuously change $v_1, v_3$ to $e_1, e_3$ such that $(v_1, Av_1, v_3, Av_3)$ stay linearly independent during the process. Edit 2: If the intersection only have one component, then the $2^{\text{nd}}$ question is immediate. Or if $A$ has two components but with a real eigenvalue, then $2$ should hold too. However, it is possible $2$ can be solve directly which I could not see. Edit 3: I crossed out the restrictions I put on $A$ although I feel this should not matter too much. The second question is newly added which is actually my end question. Before I had a feeling there should be some ""special"" $A$ such that the intersection would only give $1$ or $2$ components. As mentioned above, it's highly possible we can directly attack the second question.","This is related to the question How many connected components for the intersection $S \cap GL_n(\mathbb R)$ where $S \subset M_n(\mathbb R)$ is a linear subspace? I asked. There is a nice example in the answer to show the intersection does not need to have two connected components as $GL_n(\mathbb R)$. The comment below by Travis is also very helpful. Let $S \subset M_n(\mathbb R^n)$ be a linear subspace. What property should $S$ process such that $S \cap GL_n(\mathbb R)$ has two connected components? I realized the question I asked before (crossed out above) might be too general to answer. Since it has not been answered, let me ask the specific question I am considering. The question is now cross-posted here at MO. Let $A \in M_4(\mathbb R)$ and $A = (e_2, x, e_4, y)$ where $e_2, e_4$ are standard basis in $\mathbb R^4$ and $x,y$ are undetermined variables. Let $\phi, \psi: M_4(\mathbb R) \to \mathbb R^4$ be linear maps defined by   \begin{align*} &\phi: B \mapsto (AB-BA) e_1, \\ &\psi: B \mapsto (AB-BA)e_3. \end{align*}   The subspace $S$ I am interested in is the intersection of kernels of the two linear maps, i.e., $S :=\text{ker}(\phi) \cap \text{ker}{\psi}$. In other words, the elements in $S \cap GL_4(\mathbb R)$ would preserve the structure of first and third columns of $A$ by conjugation, i.e., $(B^{-1}AB) e_1 = e_2, (B^{-1}AB)e_3 = e_4$ for $B \in S \cap GL_4(\mathbb R)$. I would like to determine: whether there exists $A$ with eigenvalues all lying on the left open half plane of $\mathbb C$, i.e., with negative real parts ( we can freely choose $x, y$) such that $S \cap GL_4(\mathbb R)$ has precisely two connected components or precisely one component. If there exists $A$, such that $\{V^{-1} A V: V \in S \cap GL_4(\mathbb R)\}$ is connected. Edit 1: If $S \cap GL_4(\mathbb R)$ has precisely two connected components, I guess they should be $S \cap GL_4(\mathbb R)_+$ and $S \cap GL_4(\mathbb R)_-$. So if $V \in S$ and $\det(V) > 0$, $V$ should be path-connected with $I$. It is not hard to check the condition implies $V = (v_1, Av_1, v_3, Av_3)$. Since $e_2 = Ae_1, e_4 = Ae_3$, the question is can we continuously change $v_1, v_3$ to $e_1, e_3$ such that $(v_1, Av_1, v_3, Av_3)$ stay linearly independent during the process. Edit 2: If the intersection only have one component, then the $2^{\text{nd}}$ question is immediate. Or if $A$ has two components but with a real eigenvalue, then $2$ should hold too. However, it is possible $2$ can be solve directly which I could not see. Edit 3: I crossed out the restrictions I put on $A$ although I feel this should not matter too much. The second question is newly added which is actually my end question. Before I had a feeling there should be some ""special"" $A$ such that the intersection would only give $1$ or $2$ components. As mentioned above, it's highly possible we can directly attack the second question.",,"['linear-algebra', 'abstract-algebra', 'general-topology', 'matrices', 'connectedness']"
1,"A ""geometric''-ish infinite sum of matrices","A ""geometric''-ish infinite sum of matrices",,"Suppose I have full rank $n\times n$ matrix $A$ with $\rho(A) < 1$ and I want to find an expression for $$S = X +  A^\top X A + A^{2\top} X A^2 + A^{3\top} X A^3 + \dots$$ where $X$ is an $n\times n$ positive definite matrix. Thus, $$ S = X + A^\top S A$$ Following this answer to a similar problem, we can make an eigenvalue decomposition of $A$ such that $A=UDU^{-1}$ with $D = \text{diag}(\lambda_1,\dots,\lambda_n)$. Then $$ S = X + U^{-\top}DU^\top S UDU^{-1}$$ and $$ Z = U^\top S U = U^\top X U + DU^\top S UD$$ If we then define $T = U^\top XU$ then $$ Z = T + DZD = T + DTD + D^2TD^2 + D^3TD^3 + \dots$$ which implies that $(i,j)$'th entry of $Z$ is $$ Z_{ij} = \frac{t_{ij}}{1 - \lambda_i\lambda_j}$$ Having obtained $Z$ then we can find S through $$S = U^{-\top}ZU^{-1}$$ My questions are Is the above correct? Is the it a good (fast and numerically stable way) way to compute it? R code confirming the above in one case # assign matrices A <- matrix(c(.8, .4, .1, .5), 2, 2) X <- matrix(c( 1, .5, .5,  2), 2)  # compute with above formulas eg  <- eigen(A) U   <- eg$vectors U_t <- t(U) las <- eg$values T.  <- crossprod(U, X %*% U) Z   <- T. / (1 - tcrossprod(las)) S   <- solve(U_t, t(solve(U_t, t(Z))))  # naive solution nai <- X for(i in 1:1000)   nai <- crossprod(A, nai %*% A) + X  nai - S #R           [,1]      [,2] #R [1,] -3.55e-15 -4.44e-16 #R [2,] -8.88e-16  0.00e+00","Suppose I have full rank $n\times n$ matrix $A$ with $\rho(A) < 1$ and I want to find an expression for $$S = X +  A^\top X A + A^{2\top} X A^2 + A^{3\top} X A^3 + \dots$$ where $X$ is an $n\times n$ positive definite matrix. Thus, $$ S = X + A^\top S A$$ Following this answer to a similar problem, we can make an eigenvalue decomposition of $A$ such that $A=UDU^{-1}$ with $D = \text{diag}(\lambda_1,\dots,\lambda_n)$. Then $$ S = X + U^{-\top}DU^\top S UDU^{-1}$$ and $$ Z = U^\top S U = U^\top X U + DU^\top S UD$$ If we then define $T = U^\top XU$ then $$ Z = T + DZD = T + DTD + D^2TD^2 + D^3TD^3 + \dots$$ which implies that $(i,j)$'th entry of $Z$ is $$ Z_{ij} = \frac{t_{ij}}{1 - \lambda_i\lambda_j}$$ Having obtained $Z$ then we can find S through $$S = U^{-\top}ZU^{-1}$$ My questions are Is the above correct? Is the it a good (fast and numerically stable way) way to compute it? R code confirming the above in one case # assign matrices A <- matrix(c(.8, .4, .1, .5), 2, 2) X <- matrix(c( 1, .5, .5,  2), 2)  # compute with above formulas eg  <- eigen(A) U   <- eg$vectors U_t <- t(U) las <- eg$values T.  <- crossprod(U, X %*% U) Z   <- T. / (1 - tcrossprod(las)) S   <- solve(U_t, t(solve(U_t, t(Z))))  # naive solution nai <- X for(i in 1:1000)   nai <- crossprod(A, nai %*% A) + X  nai - S #R           [,1]      [,2] #R [1,] -3.55e-15 -4.44e-16 #R [2,] -8.88e-16  0.00e+00",,['linear-algebra']
2,Number of simultaneously solvable linear equations with nonnegative variables,Number of simultaneously solvable linear equations with nonnegative variables,,"Let $N$ variables $x_i \ge 0$ but not all of them be zero. One may fix $\sum_{i=1}^N x_i = 1$. There are $P$ equations which need to be solved, with coefficients $a^k_i$ indexed with superscripts $k = 1 ... P$ and subscripts $i = 1 ... N$: $$ a^1_1 x_1 + a^1_2 x_2 + \cdots + a^1_N x_N  = 0\\ a^2_1 x_1 + a^2_2 x_2 + \cdots + a^2_N x_N  = 0\\ \vdots \\ a^P_1 x_1 + a^P_2 x_2 + \cdots + a^P_N x_N  = 0\\ $$ Let $P$ and $N$ be sufficiently large to come up with probabilistic statements (see below). Allow only binary choices $a_i^k = \pm 1$. If all $P \cdot N$ many coefficients are chosen randomly with probability 0.5 for $\pm 1$, and if all coefficient vectors are in general position , then it is known [Thomas Cover, [Cover65]] that variables $x_i$ can be found which solve the equations for at least 50 % of all such random choices of coefficients, as long as $2 P <  N$. [Note: this is half as many equations than can be solved if there were no restrictions on the $x_i$. ] Now define a different generative procedure for the binary coefficients as follows.  Select randomly the coefficients in the first equation $a^1_i$. Let the other coefficients be $a^k_i = \prod_{j=1}^k  a^1_{(1+[(j+i-1) \! \! \! \mod \! \! N])}$ (which are then also binary). To illustrate, the first three equations are  $$ a^1_1 x_1 + a^1_2 x_2 + \cdots + a^1_N x_N  = 0\\ a^1_1 a^1_2 x_1 + a^1_2 a^1_3 x_2 + \cdots + a^1_N a^1_1 x_N  = 0\\ a^1_1 a^1_2 a^1_3 x_1 + a^1_2 a^1_3a^1_4 x_2 + \cdots + a^1_N a^1_1 a^1_2  x_N  = 0\\ \vdots $$ Then numerical simulations indicate that variables $x_i$ can be found in the same sense as above as long as $1.7 P <  N$. Already for small values, like $N =10$, the effect can be seen in simulations. I.e. with this choice of coefficients, the equations are ""easier"" in the sense that more of them can be simultaneously solved. Can anybody give a hint why this is so? (and possibly also how the ratio 1.7 can be calculated?)  I was reading about equations with nonnegative coefficients but didn't get a handle on  this problem. Also, a link might exist to elementary symmetric polynomials but I couldn't find material which elucidates this question. [Cover65] Thomas M. Cover: ""Geometrical and Statistical Properties of Systems of Linear Inequalities with Applications in Pattern Recognition"". IEEE TRANSACTIONS ON ELECTRONIC COMPUTERS, Volume: EC-14, Issue: 3, June 1965, pp. 326 - 334.","Let $N$ variables $x_i \ge 0$ but not all of them be zero. One may fix $\sum_{i=1}^N x_i = 1$. There are $P$ equations which need to be solved, with coefficients $a^k_i$ indexed with superscripts $k = 1 ... P$ and subscripts $i = 1 ... N$: $$ a^1_1 x_1 + a^1_2 x_2 + \cdots + a^1_N x_N  = 0\\ a^2_1 x_1 + a^2_2 x_2 + \cdots + a^2_N x_N  = 0\\ \vdots \\ a^P_1 x_1 + a^P_2 x_2 + \cdots + a^P_N x_N  = 0\\ $$ Let $P$ and $N$ be sufficiently large to come up with probabilistic statements (see below). Allow only binary choices $a_i^k = \pm 1$. If all $P \cdot N$ many coefficients are chosen randomly with probability 0.5 for $\pm 1$, and if all coefficient vectors are in general position , then it is known [Thomas Cover, [Cover65]] that variables $x_i$ can be found which solve the equations for at least 50 % of all such random choices of coefficients, as long as $2 P <  N$. [Note: this is half as many equations than can be solved if there were no restrictions on the $x_i$. ] Now define a different generative procedure for the binary coefficients as follows.  Select randomly the coefficients in the first equation $a^1_i$. Let the other coefficients be $a^k_i = \prod_{j=1}^k  a^1_{(1+[(j+i-1) \! \! \! \mod \! \! N])}$ (which are then also binary). To illustrate, the first three equations are  $$ a^1_1 x_1 + a^1_2 x_2 + \cdots + a^1_N x_N  = 0\\ a^1_1 a^1_2 x_1 + a^1_2 a^1_3 x_2 + \cdots + a^1_N a^1_1 x_N  = 0\\ a^1_1 a^1_2 a^1_3 x_1 + a^1_2 a^1_3a^1_4 x_2 + \cdots + a^1_N a^1_1 a^1_2  x_N  = 0\\ \vdots $$ Then numerical simulations indicate that variables $x_i$ can be found in the same sense as above as long as $1.7 P <  N$. Already for small values, like $N =10$, the effect can be seen in simulations. I.e. with this choice of coefficients, the equations are ""easier"" in the sense that more of them can be simultaneously solved. Can anybody give a hint why this is so? (and possibly also how the ratio 1.7 can be calculated?)  I was reading about equations with nonnegative coefficients but didn't get a handle on  this problem. Also, a link might exist to elementary symmetric polynomials but I couldn't find material which elucidates this question. [Cover65] Thomas M. Cover: ""Geometrical and Statistical Properties of Systems of Linear Inequalities with Applications in Pattern Recognition"". IEEE TRANSACTIONS ON ELECTRONIC COMPUTERS, Volume: EC-14, Issue: 3, June 1965, pp. 326 - 334.",,"['linear-algebra', 'probability', 'combinatorics', 'inequality', 'optimization']"
3,Is entropy being maximized and the norm being minimized by the same (unique) probability vector somehow related?,Is entropy being maximized and the norm being minimized by the same (unique) probability vector somehow related?,,"Consider the set of probability vectors $$ \mathcal P_n=\Big\lbrace x\in[0,1]^n\,\Big|\,\sum_{i=1}^n x_i=1\Big\rbrace\subset\mathbb C^n $$ for any $n\in\mathbb N$ where $x_i$ is the $i$ -th component of $x$ . Since $\mathcal P_n$ is a non-empty convex and closed set, there exists a unique vector of minimal norm which, unsurprisingly, turns out to be the equilibrium $\frac1n(1,\ldots,1)$ as a simple consequence of Cauchy-Schwarz. On the other hand given $x\in \mathcal P_n$ we can define the entropy as in quantum information via $$ S:\mathcal P_n\to\mathbb R_0^+\qquad x\mapsto-\sum_{i=1}^n x_i\log(x_i). $$ It is well known that the entropy is maximal if and only if $x=\frac1n(1,\ldots,1)$ . Question: I wondered if the entropy being maximized and the norm being minimized by the same unique vector $\frac1n(1,\ldots,1)$ is somehow related? I don't see a direct connection since the logarithm obviously is not linear but I still feel like there might be some kind of link between those two results. Thanks in advance for any answer or comment!","Consider the set of probability vectors for any where is the -th component of . Since is a non-empty convex and closed set, there exists a unique vector of minimal norm which, unsurprisingly, turns out to be the equilibrium as a simple consequence of Cauchy-Schwarz. On the other hand given we can define the entropy as in quantum information via It is well known that the entropy is maximal if and only if . Question: I wondered if the entropy being maximized and the norm being minimized by the same unique vector is somehow related? I don't see a direct connection since the logarithm obviously is not linear but I still feel like there might be some kind of link between those two results. Thanks in advance for any answer or comment!","
\mathcal P_n=\Big\lbrace x\in[0,1]^n\,\Big|\,\sum_{i=1}^n x_i=1\Big\rbrace\subset\mathbb C^n
 n\in\mathbb N x_i i x \mathcal P_n \frac1n(1,\ldots,1) x\in \mathcal P_n 
S:\mathcal P_n\to\mathbb R_0^+\qquad x\mapsto-\sum_{i=1}^n x_i\log(x_i).
 x=\frac1n(1,\ldots,1) \frac1n(1,\ldots,1)","['linear-algebra', 'entropy', 'quantum-information']"
4,$\left[\begin{array}{cc} P & A^T\\ A & 0\end{array}\right]$ is non-singular if and only if $\mathcal N(P) \cap \mathcal N(A)=\{0\}?$,is non-singular if and only if,\left[\begin{array}{cc} P & A^T\\ A & 0\end{array}\right] \mathcal N(P) \cap \mathcal N(A)=\{0\}?,"Suppose $P\succeq 0$ and $A$ is of full row rank.  I want to show that $\left[\begin{array}{cc} P & A^T\\ A & 0\end{array}\right]$ is nonsingular if and only if $\mathcal N(P) \cap \mathcal N(A)=\{0\},$ where $\mathcal N(\bullet)$ denotes the null space of a matrix. The only-if part is straightforward, because $\left[\begin{array}{c} P \\ A \end{array}\right]$ would not even be full-rank otherwise. I think I managed to show the if part, but would appreciate it if someone can confirm it or point out where I'm mistaken.  I'd also appreciate an alternative, simpler or more intuitive proof/comments.  Here's my attempted proof: ($0$ denotes a scalar or a matrix below, depending on the context.) Suppose $\left[\begin{array}{cc} P & A^T \\ A & 0\end{array}\right] \left[\begin{array}{c} x \\ y\end{array}\right]=0.$  Then we have $Ax=0$ and $Px+A^Ty=0.$  Therefore, $x^TPx+x^TA^Ty=0$. Since $Ax=0$, this implies that $x^TPx=0$, hence $P^{1/2}x=0$ and $Px=0.$  It then follows that $x=0$, since $\mathcal N(P) \cap \mathcal N(A)=\{0\}.$ As a result, $y=0$ since $A^T$ is of full column rank. Q.E.D.","Suppose $P\succeq 0$ and $A$ is of full row rank.  I want to show that $\left[\begin{array}{cc} P & A^T\\ A & 0\end{array}\right]$ is nonsingular if and only if $\mathcal N(P) \cap \mathcal N(A)=\{0\},$ where $\mathcal N(\bullet)$ denotes the null space of a matrix. The only-if part is straightforward, because $\left[\begin{array}{c} P \\ A \end{array}\right]$ would not even be full-rank otherwise. I think I managed to show the if part, but would appreciate it if someone can confirm it or point out where I'm mistaken.  I'd also appreciate an alternative, simpler or more intuitive proof/comments.  Here's my attempted proof: ($0$ denotes a scalar or a matrix below, depending on the context.) Suppose $\left[\begin{array}{cc} P & A^T \\ A & 0\end{array}\right] \left[\begin{array}{c} x \\ y\end{array}\right]=0.$  Then we have $Ax=0$ and $Px+A^Ty=0.$  Therefore, $x^TPx+x^TA^Ty=0$. Since $Ax=0$, this implies that $x^TPx=0$, hence $P^{1/2}x=0$ and $Px=0.$  It then follows that $x=0$, since $\mathcal N(P) \cap \mathcal N(A)=\{0\}.$ As a result, $y=0$ since $A^T$ is of full column rank. Q.E.D.",,"['linear-algebra', 'matrices']"
5,Linear Algebra Done Right Exercise 3.F.26,Linear Algebra Done Right Exercise 3.F.26,,"Suppose $V$ is finite-dimensional and $\Gamma$ is a subspace of $V'$. Show that   $$\Gamma = \{v\in V: \phi(v) = 0\ \forall \phi\in\Gamma\}^0$$ It's straightforward to show that $\Gamma\subset \{v\in V: \phi(v) = 0\ \forall \phi\in\Gamma\}^0$, but I can't come up with the reverse direction.","Suppose $V$ is finite-dimensional and $\Gamma$ is a subspace of $V'$. Show that   $$\Gamma = \{v\in V: \phi(v) = 0\ \forall \phi\in\Gamma\}^0$$ It's straightforward to show that $\Gamma\subset \{v\in V: \phi(v) = 0\ \forall \phi\in\Gamma\}^0$, but I can't come up with the reverse direction.",,['linear-algebra']
6,A Matrix Norm Inequality $\|A^{1/2}B^{1/2}(A+B)^{-1/2}\|_F \geq \|A^{1/2}(A+B)^{-1/2}B^{1/2}\|_F$,A Matrix Norm Inequality,\|A^{1/2}B^{1/2}(A+B)^{-1/2}\|_F \geq \|A^{1/2}(A+B)^{-1/2}B^{1/2}\|_F,"Let $\|X\|_F:= \sqrt{\text{Tr} \left(XX^\dagger\right) }$ denote the Frobenius norm. Does anyone know how to show the norm inequality: $\left\|A^{\frac12}B^{\frac12}(A+B)^{-\frac12}\right\|_F \geq   \left\|A^{\frac12}(A+B)^{-\frac12}B^{\frac12}\right\|_F$ for any positive definite matrices $A$ and $B$ ? We have numerical evidence for this inequality to be true. The inequality obviously does not hold if we replace $(A+B)^{-\frac12}$ by arbitrary positive definite matrix $Z$, i.e. $\left\|A^{\frac12}B^{\frac12}Z\right\|_F \geq  \left\|A^{\frac12}ZB^{\frac12}\right\|_F$. Hence, it seems that the sum $(A+B)$ admits some nice structures to ensure the norm inequality. Actually, we expect that the inequality: $\left\|A^{\frac12}B^{\frac12}(A+B)^{-1}B^{\frac12}A^{\frac12}\right\|  \geq  \left\|A^{\frac12}(A+B)^{-\frac12}B(A+B)^{-\frac12}A^{\frac12}\right\|$ holds for all unitarily-invariant norms $\|\cdot\|$. I would appreciate any ideas.","Let $\|X\|_F:= \sqrt{\text{Tr} \left(XX^\dagger\right) }$ denote the Frobenius norm. Does anyone know how to show the norm inequality: $\left\|A^{\frac12}B^{\frac12}(A+B)^{-\frac12}\right\|_F \geq   \left\|A^{\frac12}(A+B)^{-\frac12}B^{\frac12}\right\|_F$ for any positive definite matrices $A$ and $B$ ? We have numerical evidence for this inequality to be true. The inequality obviously does not hold if we replace $(A+B)^{-\frac12}$ by arbitrary positive definite matrix $Z$, i.e. $\left\|A^{\frac12}B^{\frac12}Z\right\|_F \geq  \left\|A^{\frac12}ZB^{\frac12}\right\|_F$. Hence, it seems that the sum $(A+B)$ admits some nice structures to ensure the norm inequality. Actually, we expect that the inequality: $\left\|A^{\frac12}B^{\frac12}(A+B)^{-1}B^{\frac12}A^{\frac12}\right\|  \geq  \left\|A^{\frac12}(A+B)^{-\frac12}B(A+B)^{-\frac12}A^{\frac12}\right\|$ holds for all unitarily-invariant norms $\|\cdot\|$. I would appreciate any ideas.",,"['linear-algebra', 'matrices', 'inequality', 'normed-spaces']"
7,"Why, historically, do we multiply matrices as we do?","Why, historically, do we multiply matrices as we do?",,"Multiplication of matrices — taking the dot product of the $i$th row of the first matrix and the $j$th column of the second to yield the $ij$th entry of the product — is not a very intuitive operation: if you were to ask someone how to mutliply two matrices, he probably would not think of that method. Of course, it turns out to be very useful: matrix multiplication is precisely the operation that represents composition of transformations. But it's not intuitive. So my question is where it came from. Who thought of multiplying matrices in that way, and why? (Was it perhaps multiplication of a matrix and a vector first? If so, who thought of multiplying them in that way, and why?) My question is intact no matter whether matrix multiplication was done this way only after it was used as representation of composition of transformations, or whether, on the contrary, matrix multiplication came first. (Again, I'm not asking about the utility of multiplying matrices as we do: this is clear to me. I'm asking a question about history.)","Multiplication of matrices — taking the dot product of the $i$th row of the first matrix and the $j$th column of the second to yield the $ij$th entry of the product — is not a very intuitive operation: if you were to ask someone how to mutliply two matrices, he probably would not think of that method. Of course, it turns out to be very useful: matrix multiplication is precisely the operation that represents composition of transformations. But it's not intuitive. So my question is where it came from. Who thought of multiplying matrices in that way, and why? (Was it perhaps multiplication of a matrix and a vector first? If so, who thought of multiplying them in that way, and why?) My question is intact no matter whether matrix multiplication was done this way only after it was used as representation of composition of transformations, or whether, on the contrary, matrix multiplication came first. (Again, I'm not asking about the utility of multiplying matrices as we do: this is clear to me. I'm asking a question about history.)",,"['linear-algebra', 'matrices', 'math-history']"
8,Exactly $n-1$ nonzero elements if $\det(A)=0$ for every arrangement,Exactly  nonzero elements if  for every arrangement,n-1 \det(A)=0,"Let $x_1,x_2,\dots,x_{n^2}\in\mathbb{R}$ with the property that any $n\times n$ matrix with exactly these elements has determinant $0$ . Suppose also that there are at least $n$ distinct elements. Do these conditions imply that exactly $n-1$ elements are nonzero? This is equivalent to asking if exactly $n^2-n+1$ of the elements must be zero. There is some reason to think so - if you have less than $n$ nonzero elements, some row has all $0$ s and the determinant is $0$ for every matrix, while if you have exactly $n$ nonzero elements, then the diagonal matrix will always have nonzero determinant. However, perhaps it is possible to have zero determinant for every arrangement if we have more nonzero elements (though this appears unreasonable). The $n$ distinct elements condition is to avoid just using the pigeonhole principle to force two rows to be the same for every arrangement.","Let with the property that any matrix with exactly these elements has determinant . Suppose also that there are at least distinct elements. Do these conditions imply that exactly elements are nonzero? This is equivalent to asking if exactly of the elements must be zero. There is some reason to think so - if you have less than nonzero elements, some row has all s and the determinant is for every matrix, while if you have exactly nonzero elements, then the diagonal matrix will always have nonzero determinant. However, perhaps it is possible to have zero determinant for every arrangement if we have more nonzero elements (though this appears unreasonable). The distinct elements condition is to avoid just using the pigeonhole principle to force two rows to be the same for every arrangement.","x_1,x_2,\dots,x_{n^2}\in\mathbb{R} n\times n 0 n n-1 n^2-n+1 n 0 0 n n","['linear-algebra', 'matrices']"
9,Let $\mathbb{K} $ be a field of characteristic $p>0$ and $\mathbb{F} | \mathbb{K} $ a finite and separable extension.,Let  be a field of characteristic  and  a finite and separable extension.,\mathbb{K}  p>0 \mathbb{F} | \mathbb{K} ,"Let $\mathbb{K}$ be a field of characteristic $p>0$ and $\mathbb{F}/ \mathbb{K}$ a finite and separable extension. Show that if $B=\{\alpha_1,\dots,\alpha_n\}$ is a basis, then $C=\{\alpha_1^p,\dots,\alpha_n^p\}$ is a basis. Well, my first idea is to use the following map: $T:\mathbb{F}\longrightarrow\mathbb{F}$ where $T(\alpha_i)=\alpha_i^p$ with $i=1,\dots,n$ Proving that $T$ is isomorphism between vector spaces the problem is solved, since that $T$ carries basis to basis. $T(k\alpha_i)=kT(\alpha_i)$ for $k\in\mathbb{K}$. Now i have to prove $T(\alpha_i+\alpha_j)=\alpha_i^p + \alpha_j^p$, surjectivity and injectivity of $T$, but at first, it does not look true. Maybe I'm in the wrong way, because I'm not using the fact that $\mathbb{F}/\mathbb{K}$ is separable.","Let $\mathbb{K}$ be a field of characteristic $p>0$ and $\mathbb{F}/ \mathbb{K}$ a finite and separable extension. Show that if $B=\{\alpha_1,\dots,\alpha_n\}$ is a basis, then $C=\{\alpha_1^p,\dots,\alpha_n^p\}$ is a basis. Well, my first idea is to use the following map: $T:\mathbb{F}\longrightarrow\mathbb{F}$ where $T(\alpha_i)=\alpha_i^p$ with $i=1,\dots,n$ Proving that $T$ is isomorphism between vector spaces the problem is solved, since that $T$ carries basis to basis. $T(k\alpha_i)=kT(\alpha_i)$ for $k\in\mathbb{K}$. Now i have to prove $T(\alpha_i+\alpha_j)=\alpha_i^p + \alpha_j^p$, surjectivity and injectivity of $T$, but at first, it does not look true. Maybe I'm in the wrong way, because I'm not using the fact that $\mathbb{F}/\mathbb{K}$ is separable.",,"['linear-algebra', 'field-theory', 'extension-field']"
10,Can we determine the determinant?,Can we determine the determinant?,,Could someone prove that this determinant is not zero? $$\left|       \begin{array}{cccc}         1^n & 2^n & \cdots & (n+1)^n \\         2^n & 3^n & \cdots & (n+2)^n \\         \vdots & \vdots &  & \vdots \\         (n+1)^n & (n+2)^n & \cdots & (2n+1)^n \\       \end{array}     \right|\neq0$$ How can we compute it?,Could someone prove that this determinant is not zero? $$\left|       \begin{array}{cccc}         1^n & 2^n & \cdots & (n+1)^n \\         2^n & 3^n & \cdots & (n+2)^n \\         \vdots & \vdots &  & \vdots \\         (n+1)^n & (n+2)^n & \cdots & (2n+1)^n \\       \end{array}     \right|\neq0$$ How can we compute it?,,"['linear-algebra', 'determinant']"
11,Connections and dependences between topological and algebraic basis in topological vector space,Connections and dependences between topological and algebraic basis in topological vector space,,"On my last functional analysis exam, one of the tasks was to show that if normed vector space $X$ have countable Hamel basis , then $X$ is separable space (over field $\mathbb{R}$). I am not sure if what I did was correct, but I started from using fact (and sketched proof of it) that in metric spaces separability is equivalent to existence of countable (topological) base. Then, I was trying to show that if normed space have countable Hamel base  $\mathscr{B}$, then it have countable open (topological) base. I wanted to do something like this: Let's consider arbitrary $ x \in Lin \{ \mathscr{B} \} \cap {???}$  (there is more than countable many of them in $Lin \{ \mathscr{B} \}$... but maybe I can somehow fix it, for example taking only linear combinations for rational coefficients, or intersect it with some countable $???$ set?) and arbitrary positive rational $r \in \mathbb{Q}$. Then, we have (we have not (!!), but we will have after fixing what I mentioned above) countable number of balls    $$B(x,r) = \{ y \in Lin \{ \mathscr{B} \} \cap {???} : ||x-y || < r \}$$   and all these balls makes topological basis of our vector space $X$. I know that this sketch above looks a little chaotic, but can we accomplish something like that? (build countable number of balls, which make topological base of our space, using countable Hamel base in normed space?) If yes, how should I correctly construct them? Or there's also some other and easier way to do this task? And here's where my second question, even more important (stronger) arrives. We can even say that this is the main purpose of this almost ""essay"" I just wrote above. After the exam I realized that I know nothing about connections between topological and algebraic bases in linear topological spaces. I also don't recall to hear anything about it on my course. I was trying to check in the internet (uncle Google), but cannot find anything useful (or I just lack awareness about it usefulness). Do you know any dependences between algebraic and topological bases of the same space? Do they always must to have equal powers? Are there some interesting properties, connections, or possible differences in some of these properties between them? How much does it differ between linear topological spaces and linear topological spaces which are metrizable, normable, etc.? I would be glad to hear about any of things mentioned above or to receive some references, like some chapters in books, links, publications or other materials where I can find this matter discussed or explained. Or maybe I am just exaggerating and there's not really much to tell about this all? There was nothing said about this on my course nor I cannot find this in my books or in internet, so I would be very glad to find something, which can give me full picture about this matter. /-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/- I would want to better understand and find out something regarding second part of my question (about topological and algebraic bases) (in generality, not focusing only on Banach spaces), so I add 50 rep bounty .  I know it is not much and I don't have too much, but I hope it will encourage someone who can know about that or can point to where I can find some constructive results on this matter ^^.","On my last functional analysis exam, one of the tasks was to show that if normed vector space $X$ have countable Hamel basis , then $X$ is separable space (over field $\mathbb{R}$). I am not sure if what I did was correct, but I started from using fact (and sketched proof of it) that in metric spaces separability is equivalent to existence of countable (topological) base. Then, I was trying to show that if normed space have countable Hamel base  $\mathscr{B}$, then it have countable open (topological) base. I wanted to do something like this: Let's consider arbitrary $ x \in Lin \{ \mathscr{B} \} \cap {???}$  (there is more than countable many of them in $Lin \{ \mathscr{B} \}$... but maybe I can somehow fix it, for example taking only linear combinations for rational coefficients, or intersect it with some countable $???$ set?) and arbitrary positive rational $r \in \mathbb{Q}$. Then, we have (we have not (!!), but we will have after fixing what I mentioned above) countable number of balls    $$B(x,r) = \{ y \in Lin \{ \mathscr{B} \} \cap {???} : ||x-y || < r \}$$   and all these balls makes topological basis of our vector space $X$. I know that this sketch above looks a little chaotic, but can we accomplish something like that? (build countable number of balls, which make topological base of our space, using countable Hamel base in normed space?) If yes, how should I correctly construct them? Or there's also some other and easier way to do this task? And here's where my second question, even more important (stronger) arrives. We can even say that this is the main purpose of this almost ""essay"" I just wrote above. After the exam I realized that I know nothing about connections between topological and algebraic bases in linear topological spaces. I also don't recall to hear anything about it on my course. I was trying to check in the internet (uncle Google), but cannot find anything useful (or I just lack awareness about it usefulness). Do you know any dependences between algebraic and topological bases of the same space? Do they always must to have equal powers? Are there some interesting properties, connections, or possible differences in some of these properties between them? How much does it differ between linear topological spaces and linear topological spaces which are metrizable, normable, etc.? I would be glad to hear about any of things mentioned above or to receive some references, like some chapters in books, links, publications or other materials where I can find this matter discussed or explained. Or maybe I am just exaggerating and there's not really much to tell about this all? There was nothing said about this on my course nor I cannot find this in my books or in internet, so I would be very glad to find something, which can give me full picture about this matter. /-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/- I would want to better understand and find out something regarding second part of my question (about topological and algebraic bases) (in generality, not focusing only on Banach spaces), so I add 50 rep bounty .  I know it is not much and I don't have too much, but I hope it will encourage someone who can know about that or can point to where I can find some constructive results on this matter ^^.",,"['linear-algebra', 'general-topology', 'functional-analysis', 'topological-vector-spaces']"
12,Constructing a directed graph from its spectrum,Constructing a directed graph from its spectrum,,"This is related to the following question from cs theory stack exchange: https://cstheory.stackexchange.com/questions/3742/reverse-graph-spectra-problem So it seems as if given a sequence of real numbers, it is not always possible to generate a 0-1 symmetric matrix (with zeros on the diagonal) whose spectrum is the given sequence. This is because not only is it true that not all sequences can occur as eigenvalues of such matrices, but also because there exist non-isomorphic graphs with the spectrum. This leads to the question of characterizing n-sequences for which such a reverse reconstruction exists and whether such a reverse reconstruction is possible in more specific cases. Consider the case of simple digraphs (directed graph) and their line digraphs. In this case, the adjacency matrices are again 0-1 matrices with zeroes on the diagonal, but not the matrix need not be symmetric. The following paper shows a clean relationship between the spectrum of a digraph and that of its line digraph: http://www.academia.edu/1462821/Algebraic_properties_of_line_digraphs What I noticed in the introduction was that: ""As it is expected, it is not possible, to reconstruct the adjacency matrix of a digraph from the spectrum or the characteristic polynomial only. However we shall prove that this is possible for the case of line digraphs."" The paper claims to do this using a relation between the Jordan normal forms of the adjacency matrices of the digraph and its line digraph, which I am firstly unable to see clearly from the paper. Secondly, given any graph, I could convert it into a digraph in the natural way by having two arcs for each of its edges. Then given the spectrum of a graph, we could construct the spectrum of its line digraph, from which I could reconstruct the line digraph (accordin to the paper's claim), from which I could in turn reconstruct the digraph (this correspondence between a digraph and its line digraph is well-understood in the case of digraphs with no sources or sinks- from the result of Harary and Norman) from which we could construct the underlying graph. What am I missing here?","This is related to the following question from cs theory stack exchange: https://cstheory.stackexchange.com/questions/3742/reverse-graph-spectra-problem So it seems as if given a sequence of real numbers, it is not always possible to generate a 0-1 symmetric matrix (with zeros on the diagonal) whose spectrum is the given sequence. This is because not only is it true that not all sequences can occur as eigenvalues of such matrices, but also because there exist non-isomorphic graphs with the spectrum. This leads to the question of characterizing n-sequences for which such a reverse reconstruction exists and whether such a reverse reconstruction is possible in more specific cases. Consider the case of simple digraphs (directed graph) and their line digraphs. In this case, the adjacency matrices are again 0-1 matrices with zeroes on the diagonal, but not the matrix need not be symmetric. The following paper shows a clean relationship between the spectrum of a digraph and that of its line digraph: http://www.academia.edu/1462821/Algebraic_properties_of_line_digraphs What I noticed in the introduction was that: ""As it is expected, it is not possible, to reconstruct the adjacency matrix of a digraph from the spectrum or the characteristic polynomial only. However we shall prove that this is possible for the case of line digraphs."" The paper claims to do this using a relation between the Jordan normal forms of the adjacency matrices of the digraph and its line digraph, which I am firstly unable to see clearly from the paper. Secondly, given any graph, I could convert it into a digraph in the natural way by having two arcs for each of its edges. Then given the spectrum of a graph, we could construct the spectrum of its line digraph, from which I could reconstruct the line digraph (accordin to the paper's claim), from which I could in turn reconstruct the digraph (this correspondence between a digraph and its line digraph is well-understood in the case of digraphs with no sources or sinks- from the result of Harary and Norman) from which we could construct the underlying graph. What am I missing here?",,"['linear-algebra', 'graph-theory', 'algebraic-graph-theory', 'spectral-graph-theory']"
13,What can we say about two graphs if they have similar adjacency matrices?,What can we say about two graphs if they have similar adjacency matrices?,,"Suppose we have two (finite, simple, undirected) graphs, what can we say about these graphs if they have similar adjacency matrices ? Observations to begin with: If $G_1$ and $G_2$ are isomorphic, then they have similar adjacency matrices, $A_1$ and $A_2$ .  In fact, they are similar in an even stronger sense: they satisfy $A_1=PA_2P^{-1}$ , where $P$ is a permutation matrix. The following non-isomorphic graphs, have similar adjacency matrices: $\qquad\qquad\qquad$ Similarity of adjacency matrices is an equivalence relation on the set of $n$ -vertex graphs. Graphs with similar adjacency matrices must be isospectral graphs .","Suppose we have two (finite, simple, undirected) graphs, what can we say about these graphs if they have similar adjacency matrices ? Observations to begin with: If and are isomorphic, then they have similar adjacency matrices, and .  In fact, they are similar in an even stronger sense: they satisfy , where is a permutation matrix. The following non-isomorphic graphs, have similar adjacency matrices: Similarity of adjacency matrices is an equivalence relation on the set of -vertex graphs. Graphs with similar adjacency matrices must be isospectral graphs .",G_1 G_2 A_1 A_2 A_1=PA_2P^{-1} P \qquad\qquad\qquad n,"['linear-algebra', 'graph-theory', 'spectral-graph-theory', 'adjacency-matrix', 'similar-matrices']"
14,Linearity of the determinant,Linearity of the determinant,,"I'd like to prove the following properties of the determinant map. $\det I = 1$ $\det$ is linear in the rows of the input matrix The determinant map is defined on $n\times n$ matrices $A$ by: $$\det \begin{bmatrix}a\end{bmatrix} = a$$$$\det A = a_{11}\det A_{11}-a_{21}\det A_{21}\pm \dots\pm a_{n1}\det A_{n1}$$ Where  $A_{xy}$ is the matrix obtained from $A$ by removing the $xth$ row and the $yth$ column and $a_{xy}$ are the entries of the matrix $A$. Proof. $\det I_n = 1\times \det I_{n-1}+0=\det I_{n-2}=\dots=det I_1 = 1$ The proof is by induction on the size of the matrix. For $2\times 2$ matrices: $\det \begin{bmatrix}a & b\\ c+x & d+y\end{bmatrix}=a(d+y)-b(c+x)=(ad-bc)+(ay-bx)=\det \begin{bmatrix}a & b\\ c & d\end{bmatrix}+\det \begin{bmatrix}a & b\\ x & y\end{bmatrix}$ Let $ C=\begin{bmatrix} \dots\\ R+S\\ \dots \end{bmatrix}$ be a $n\times n$ matrix for $n\gt 2$, we compute $\det C$ in terms of $\det R$ and $\det S$ where $R = \begin{bmatrix}\dots\\R\\\dots\end{bmatrix}$ and $S = \begin{bmatrix}\dots\\S\\\dots\end{bmatrix}$. Let $i$ denote the index of the rows $R, S, R+S$. $$\det C = \sum_{j\neq i}\pm c_{j1} \det C_{j1} \pm c_{i1} \det C_{i1}$$. By the induction hypothesis, we can write:  $$\det C = \sum_{j\neq i}\pm c_{j1}(\det R_{j1}+\det S_{j1}) \pm c_{i1}\det C_{i1}$$ We know that $\det C_{i1}=\det R_{i1}=\det S_{i1}$ and $c_{i1}=r_{i1}+s_{i1}$ and $c_{j1}=r_{j1}=s_{j1}$ for $j\neq i$. So  $$\det C = \sum_{j\neq i}\pm r_{j1}\det R_{j1} + \sum_{j\neq i}\pm s_{j1}\det S_{j1} \pm (r_{i1} + s_{i1})\det C_{i1}= \sum_{j\neq i}\pm r_{j1}\det R_{j1} + \sum_{j\neq i}\pm s_{j1}\det S_{j1} \pm ( r_{i1}\det C_{i1} + s_{i1}\det C_{i1})= \sum_{j\neq i}\pm r_{j1}\det R_{j1} + \sum_{j\neq i}\pm s_{j1}\det S_{j1} \pm (r_{i1}\det R_{i1} + s_{i1}\det S_{i1})= \sum_{j}\pm r_{j1}\det R_{j1} + \sum_{j}\pm s_{j1}\det S_{j1}=\det R + \det S$$. A simular induction argument shows that $$\det \begin{bmatrix}\dots \\ c\times R \\ \dots\end{bmatrix}= c\times \det \begin{bmatrix}\dots \\ R \\ \dots\end{bmatrix}$$ Is my proof correct?","I'd like to prove the following properties of the determinant map. $\det I = 1$ $\det$ is linear in the rows of the input matrix The determinant map is defined on $n\times n$ matrices $A$ by: $$\det \begin{bmatrix}a\end{bmatrix} = a$$$$\det A = a_{11}\det A_{11}-a_{21}\det A_{21}\pm \dots\pm a_{n1}\det A_{n1}$$ Where  $A_{xy}$ is the matrix obtained from $A$ by removing the $xth$ row and the $yth$ column and $a_{xy}$ are the entries of the matrix $A$. Proof. $\det I_n = 1\times \det I_{n-1}+0=\det I_{n-2}=\dots=det I_1 = 1$ The proof is by induction on the size of the matrix. For $2\times 2$ matrices: $\det \begin{bmatrix}a & b\\ c+x & d+y\end{bmatrix}=a(d+y)-b(c+x)=(ad-bc)+(ay-bx)=\det \begin{bmatrix}a & b\\ c & d\end{bmatrix}+\det \begin{bmatrix}a & b\\ x & y\end{bmatrix}$ Let $ C=\begin{bmatrix} \dots\\ R+S\\ \dots \end{bmatrix}$ be a $n\times n$ matrix for $n\gt 2$, we compute $\det C$ in terms of $\det R$ and $\det S$ where $R = \begin{bmatrix}\dots\\R\\\dots\end{bmatrix}$ and $S = \begin{bmatrix}\dots\\S\\\dots\end{bmatrix}$. Let $i$ denote the index of the rows $R, S, R+S$. $$\det C = \sum_{j\neq i}\pm c_{j1} \det C_{j1} \pm c_{i1} \det C_{i1}$$. By the induction hypothesis, we can write:  $$\det C = \sum_{j\neq i}\pm c_{j1}(\det R_{j1}+\det S_{j1}) \pm c_{i1}\det C_{i1}$$ We know that $\det C_{i1}=\det R_{i1}=\det S_{i1}$ and $c_{i1}=r_{i1}+s_{i1}$ and $c_{j1}=r_{j1}=s_{j1}$ for $j\neq i$. So  $$\det C = \sum_{j\neq i}\pm r_{j1}\det R_{j1} + \sum_{j\neq i}\pm s_{j1}\det S_{j1} \pm (r_{i1} + s_{i1})\det C_{i1}= \sum_{j\neq i}\pm r_{j1}\det R_{j1} + \sum_{j\neq i}\pm s_{j1}\det S_{j1} \pm ( r_{i1}\det C_{i1} + s_{i1}\det C_{i1})= \sum_{j\neq i}\pm r_{j1}\det R_{j1} + \sum_{j\neq i}\pm s_{j1}\det S_{j1} \pm (r_{i1}\det R_{i1} + s_{i1}\det S_{i1})= \sum_{j}\pm r_{j1}\det R_{j1} + \sum_{j}\pm s_{j1}\det S_{j1}=\det R + \det S$$. A simular induction argument shows that $$\det \begin{bmatrix}\dots \\ c\times R \\ \dots\end{bmatrix}= c\times \det \begin{bmatrix}\dots \\ R \\ \dots\end{bmatrix}$$ Is my proof correct?",,"['linear-algebra', 'determinant']"
15,Approximating commuting matrices by commuting diagonalizable matrices,Approximating commuting matrices by commuting diagonalizable matrices,,"Suppose the matrices $A$ and $B$ commute. Do there exists sequences $A_n$ and $B_n$ of matrices such that $A_n \rightarrow A$, $B_n \rightarrow B$. Each $A_n$ is diagonalizable  and the same for each $B_n$. For every $n$, $A_n$ commutes with $B_n$. Moreover, it would be nice if the following property was additionally satisfied: if $A,B$ are real, then $A_n,B_n$ can be chosen to be real as well.","Suppose the matrices $A$ and $B$ commute. Do there exists sequences $A_n$ and $B_n$ of matrices such that $A_n \rightarrow A$, $B_n \rightarrow B$. Each $A_n$ is diagonalizable  and the same for each $B_n$. For every $n$, $A_n$ commutes with $B_n$. Moreover, it would be nice if the following property was additionally satisfied: if $A,B$ are real, then $A_n,B_n$ can be chosen to be real as well.",,['linear-algebra']
16,Continuous choice of basis for subspaces,Continuous choice of basis for subspaces,,"Consider the flag variety (or flag manifold, depending on who you are) $V=\mathrm {Fl} (3,\mathbb C)$ of complete flags of subspaces of $\mathbb C^3$. That is, an element of M is a tuple (L , P) consisting of line, and plane in $\mathbb C^3$, so that the line lies in the plane. Suppose that we have a fixed choice of basis for every plane in $\mathbb C^3$. Then we have a consistent isomorphism $P\to\mathbb C^2$ for every plane. Then choice of a flag breaks apart into choice of a plane (an element of $\mathrm{Gr}(2,3)\cong\mathrm{Gr}(1,3)\cong\mathbb P^2 (\mathbb C)$), and then a choice of a line in any plane (an element of $\mathrm{Gr}(1,2)\cong\mathbb P^1 (\mathbb C)$) which can be isomorphically embedded into that plane. In other words, every choice of basis for all of our planes (map sending points of $\mathrm{Gr}(2,3)$ to pairs of linearly independent vectors lying in that plane) gives us a bijection $\mathrm {Fl} (3,\mathbb C) \to \mathbb P^1\times \mathbb P^2$, or in fact of any flag variety (over any field) to a product of projective spaces (over that field). (We are using the relation that $\mathrm{Gr}(k,n)\cong\mathrm{Gr}(n-k,n)$, by sending k-dimensional subspaces to their orthogonal complements.) The natural next question is whether we can make a choice of basis so that this bijection is in fact a homeomorphism (or hopefully an isomorphism of varieties/diffeomorphism of manifolds.) It would appear that this is possible if and only if we can make a ""continuous choice of basis"" for the planes which make up $\mathrm{Gr}(2,3)$: that is a continuous map $\mathrm{Gr}(2,3)\to\mathbb C^3\times \mathbb C^3$ so that the two resulting vectors are a basis for the plane we started with. Such a map is necessarily injective. Thinking about this spatially, many ""natural-looking"" choices of basis will fail to be continuous. (Hold your hand out with thumb and forefinger at a right-angle. Then rotate their spanned plane about the axisyour thumb. After rotating 180$^{\circ}$, you get the same plane, but now you've negated one of your basis vectors: this choice of basis is not continuous.) Hence, my question is this: does there exist a continuous choice of basis for planes in a finite-dimensional $\mathbb C$-vector space in general? Over $\mathbb R$ (/other topological fields)? For particular dimensions (even/odd)? This problem appears to be a bit algebraic-topological in nature, and I'm not sure how to approach it. (Gaussian elimination, for instance, fails to give a continuous choice.) Ultimately, of course, the question I am interested in is whether this bijection (and hopefully homeomorphism) can be an isomorphism of varieties. There are obviously other ways to approach that question, but continuous choice of basis seems an interesting question in its own right.","Consider the flag variety (or flag manifold, depending on who you are) $V=\mathrm {Fl} (3,\mathbb C)$ of complete flags of subspaces of $\mathbb C^3$. That is, an element of M is a tuple (L , P) consisting of line, and plane in $\mathbb C^3$, so that the line lies in the plane. Suppose that we have a fixed choice of basis for every plane in $\mathbb C^3$. Then we have a consistent isomorphism $P\to\mathbb C^2$ for every plane. Then choice of a flag breaks apart into choice of a plane (an element of $\mathrm{Gr}(2,3)\cong\mathrm{Gr}(1,3)\cong\mathbb P^2 (\mathbb C)$), and then a choice of a line in any plane (an element of $\mathrm{Gr}(1,2)\cong\mathbb P^1 (\mathbb C)$) which can be isomorphically embedded into that plane. In other words, every choice of basis for all of our planes (map sending points of $\mathrm{Gr}(2,3)$ to pairs of linearly independent vectors lying in that plane) gives us a bijection $\mathrm {Fl} (3,\mathbb C) \to \mathbb P^1\times \mathbb P^2$, or in fact of any flag variety (over any field) to a product of projective spaces (over that field). (We are using the relation that $\mathrm{Gr}(k,n)\cong\mathrm{Gr}(n-k,n)$, by sending k-dimensional subspaces to their orthogonal complements.) The natural next question is whether we can make a choice of basis so that this bijection is in fact a homeomorphism (or hopefully an isomorphism of varieties/diffeomorphism of manifolds.) It would appear that this is possible if and only if we can make a ""continuous choice of basis"" for the planes which make up $\mathrm{Gr}(2,3)$: that is a continuous map $\mathrm{Gr}(2,3)\to\mathbb C^3\times \mathbb C^3$ so that the two resulting vectors are a basis for the plane we started with. Such a map is necessarily injective. Thinking about this spatially, many ""natural-looking"" choices of basis will fail to be continuous. (Hold your hand out with thumb and forefinger at a right-angle. Then rotate their spanned plane about the axisyour thumb. After rotating 180$^{\circ}$, you get the same plane, but now you've negated one of your basis vectors: this choice of basis is not continuous.) Hence, my question is this: does there exist a continuous choice of basis for planes in a finite-dimensional $\mathbb C$-vector space in general? Over $\mathbb R$ (/other topological fields)? For particular dimensions (even/odd)? This problem appears to be a bit algebraic-topological in nature, and I'm not sure how to approach it. (Gaussian elimination, for instance, fails to give a continuous choice.) Ultimately, of course, the question I am interested in is whether this bijection (and hopefully homeomorphism) can be an isomorphism of varieties. There are obviously other ways to approach that question, but continuous choice of basis seems an interesting question in its own right.",,"['linear-algebra', 'general-topology', 'algebraic-geometry', 'algebraic-topology']"
17,Generalized Eigenvalue Problem with one matrix having low rank,Generalized Eigenvalue Problem with one matrix having low rank,,"I have a specific Generalized Eigenvalue Problem (GEVP) where i am primary not interested in solving this problem but concluding from a standard EVP the spectrum of the GEVP. The Problem Let $A$ be a $n\times x$ possibly complex matrix and $B$ a diagonal, real $n\times n$ matrix with maximal rank of $n-1$ (e.g. the matrix $B$ has at minimum 1 zero column and row). Solving $(B\lambda-A)\cdot v=0$ with $|v|=1$, so that we have $n+1$ equations for $n+1$ unknown is the GEVP. The GEVP can not be reformulated as EVP because $det(B)=0$ and therefore $B$ is not invertible. As I said the goal is not just solving this problem (this could be done by solving $det(\lambda B I-A)=0$ to obtain the eigenvalues) but to conclude eigenvalues for the stated GEVP from the following, already solved, EVP (the $n$ eigenvalues $\mu_1\leq\mu_2\leq\dots\leq \mu_n$ of $A$ are known): $(I\mu-A)\cdot w=0$. What I have already learned *As $A$ and $B$ in general do not commutate it is not possible to diagonalize $A$ and $B$ simultanously. Therefore the spectra will be different. *If the EVP results in eigenvalues $\mu=0$, so there will be the same number of eigenvalues $\lambda=0$ in the GEVP. (Because in both cases $det(A)=0$ must be fullfilled and the geometric multiplicity comes from the dimension of $kern(A)$.) *For every zero-row in $B$ the number of eigenvalues $\mu$ is one less then in the EVP. This is because the order of the characteristic polynom (CP) goes one down for every zero-row in $B$ compared to the order of the CP in the EVP. Questions *Can be said which eigenvalues (in addition to the zeros) of the EVP are also eigenvalues of the GEVP (the eigenvectors may not be the same in both cases, but the eigenvalues). *Is there a pertubation theory? Can I somehow make a taylor series of the CP in the GEVP where the zeroth-term is the CP of the EVP? *The number of eigenvalues in the GEVP is less than in the EVP, can be concluded which eigenvalues vanish? In case anybody wants to know, where my question emerges from (this is not essential for my questions but possibly from general interest): If one wants to conclude the stability of a fix point $x^*$ of ODEs one needs to solve the variational ODE $\dot{\delta x}(t)=D_xf(x^*)\delta x(t)$. Where $\delta x$ is a small pertubation away from the fix point: $\delta x(t)=x(t)-x^*$. Solving this with $\delta x(t)=\delta x_0 e^{\mu t}$ results in the EVP $\mu\delta x_0 = D_x f(x^*) \delta x_0$. Using $D_xf(x^*)=A$ and $w=\delta x_0$ results in the stated EVP. If one has additional constraints in a implicit way $g(x(t))=0$ the stability of a fix point in the ODE may change (e.g. the constraint acts in a unstable direction. The eigenvalue of $A$ in this direction is still greater zero (obviously the matrix $A$ does not change if constraints are imposed) but it is a ""forbidden"" direction as the corrisponding eigenvector is in a direction which is not allowed due to the constraint). Taking the time derivative of $g$ results in $D_x g(x)\cdot \dot{x}(t)=0$. Inserting the pertubation away from the fix point results in $(D_xg(x^*)+D_x(D_xg(x^*))\cdot \delta x)\cdot \dot{\delta x}(t)+\dots=0$ $D_xg(x^*)\cdot \dot{\delta x}(t)+O(\delta x^2)=0$ Inserting the exp-ansatz results in $D_xg(x^*)\cdot \delta x_0\mu\approx0$ This means that the small pertubations need to be orthogonal to the gradient on the invariant manifold near the fix point (e.g. they are inside the invariant manifold). One possible way to go to study the change of stability of the fix point when  constraints are imposed, is to solve the EVP and then to check consistency with the last equation. I want to include the constraint directly in the EVP, which leads to the GEVP by simply adding the last equation to the EVP (with $D_x g(x^*)=\hat{B}$ and $w=\delta x_0$): $(\hat{B}\mu+I\mu-A)\cdot w=0$ and with $\hat{B}+I=B$ $(B\mu-A)\cdot w=0$ The criterion of ""low rank $B$"" comes from the generic constraints like $g(x_1,\dots,x_n)=x_0^1-x_1\rightarrow D_xg=(-1,0,\dots,0)\rightarrow B=\mbox{diag}(0,1,\dots,1)$.","I have a specific Generalized Eigenvalue Problem (GEVP) where i am primary not interested in solving this problem but concluding from a standard EVP the spectrum of the GEVP. The Problem Let $A$ be a $n\times x$ possibly complex matrix and $B$ a diagonal, real $n\times n$ matrix with maximal rank of $n-1$ (e.g. the matrix $B$ has at minimum 1 zero column and row). Solving $(B\lambda-A)\cdot v=0$ with $|v|=1$, so that we have $n+1$ equations for $n+1$ unknown is the GEVP. The GEVP can not be reformulated as EVP because $det(B)=0$ and therefore $B$ is not invertible. As I said the goal is not just solving this problem (this could be done by solving $det(\lambda B I-A)=0$ to obtain the eigenvalues) but to conclude eigenvalues for the stated GEVP from the following, already solved, EVP (the $n$ eigenvalues $\mu_1\leq\mu_2\leq\dots\leq \mu_n$ of $A$ are known): $(I\mu-A)\cdot w=0$. What I have already learned *As $A$ and $B$ in general do not commutate it is not possible to diagonalize $A$ and $B$ simultanously. Therefore the spectra will be different. *If the EVP results in eigenvalues $\mu=0$, so there will be the same number of eigenvalues $\lambda=0$ in the GEVP. (Because in both cases $det(A)=0$ must be fullfilled and the geometric multiplicity comes from the dimension of $kern(A)$.) *For every zero-row in $B$ the number of eigenvalues $\mu$ is one less then in the EVP. This is because the order of the characteristic polynom (CP) goes one down for every zero-row in $B$ compared to the order of the CP in the EVP. Questions *Can be said which eigenvalues (in addition to the zeros) of the EVP are also eigenvalues of the GEVP (the eigenvectors may not be the same in both cases, but the eigenvalues). *Is there a pertubation theory? Can I somehow make a taylor series of the CP in the GEVP where the zeroth-term is the CP of the EVP? *The number of eigenvalues in the GEVP is less than in the EVP, can be concluded which eigenvalues vanish? In case anybody wants to know, where my question emerges from (this is not essential for my questions but possibly from general interest): If one wants to conclude the stability of a fix point $x^*$ of ODEs one needs to solve the variational ODE $\dot{\delta x}(t)=D_xf(x^*)\delta x(t)$. Where $\delta x$ is a small pertubation away from the fix point: $\delta x(t)=x(t)-x^*$. Solving this with $\delta x(t)=\delta x_0 e^{\mu t}$ results in the EVP $\mu\delta x_0 = D_x f(x^*) \delta x_0$. Using $D_xf(x^*)=A$ and $w=\delta x_0$ results in the stated EVP. If one has additional constraints in a implicit way $g(x(t))=0$ the stability of a fix point in the ODE may change (e.g. the constraint acts in a unstable direction. The eigenvalue of $A$ in this direction is still greater zero (obviously the matrix $A$ does not change if constraints are imposed) but it is a ""forbidden"" direction as the corrisponding eigenvector is in a direction which is not allowed due to the constraint). Taking the time derivative of $g$ results in $D_x g(x)\cdot \dot{x}(t)=0$. Inserting the pertubation away from the fix point results in $(D_xg(x^*)+D_x(D_xg(x^*))\cdot \delta x)\cdot \dot{\delta x}(t)+\dots=0$ $D_xg(x^*)\cdot \dot{\delta x}(t)+O(\delta x^2)=0$ Inserting the exp-ansatz results in $D_xg(x^*)\cdot \delta x_0\mu\approx0$ This means that the small pertubations need to be orthogonal to the gradient on the invariant manifold near the fix point (e.g. they are inside the invariant manifold). One possible way to go to study the change of stability of the fix point when  constraints are imposed, is to solve the EVP and then to check consistency with the last equation. I want to include the constraint directly in the EVP, which leads to the GEVP by simply adding the last equation to the EVP (with $D_x g(x^*)=\hat{B}$ and $w=\delta x_0$): $(\hat{B}\mu+I\mu-A)\cdot w=0$ and with $\hat{B}+I=B$ $(B\mu-A)\cdot w=0$ The criterion of ""low rank $B$"" comes from the generic constraints like $g(x_1,\dots,x_n)=x_0^1-x_1\rightarrow D_xg=(-1,0,\dots,0)\rightarrow B=\mbox{diag}(0,1,\dots,1)$.",,"['linear-algebra', 'eigenvalues-eigenvectors']"
18,Name for a non-square matrix with ones along the main diagonal? [closed],Name for a non-square matrix with ones along the main diagonal? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Is there a name for a matrix that would be the identity matrix, except that it's not square?  In other words, it has ones along the main diagonal, and zeroes off of the main diagonal.  But as it's not a square matrix, it is not the identity matrix.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Is there a name for a matrix that would be the identity matrix, except that it's not square?  In other words, it has ones along the main diagonal, and zeroes off of the main diagonal.  But as it's not a square matrix, it is not the identity matrix.",,"['linear-algebra', 'matrices']"
19,A question about linear transformations mapping straight lines to straight lines,A question about linear transformations mapping straight lines to straight lines,,"Obviously, a linear transformation over a space maps all straight lines to other straight lines. My question is: is the converse true? That is, if we're looking at a space after some transformation and we're observing that all straight lines were mapped to straight lines, does that imply a linear transformation? (Lines are understood as finite and infinite collections of points that are aligned, well, in a line) EDIT: The comments mention affine transformations which fullfil the above condition but are not linear since they also involve translation. I think I'll reask then: Is every transformation that maps ALL lines to lines necessarily affine?","Obviously, a linear transformation over a space maps all straight lines to other straight lines. My question is: is the converse true? That is, if we're looking at a space after some transformation and we're observing that all straight lines were mapped to straight lines, does that imply a linear transformation? (Lines are understood as finite and infinite collections of points that are aligned, well, in a line) EDIT: The comments mention affine transformations which fullfil the above condition but are not linear since they also involve translation. I think I'll reask then: Is every transformation that maps ALL lines to lines necessarily affine?",,['linear-algebra']
20,Proof: The inverse of the inverse matrix is the matrix. [closed],Proof: The inverse of the inverse matrix is the matrix. [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved Improve this question If $A$ is a square matrix such that it is not singular, then  $(A^{-1})^{-1} = A$ How can I prove this property? I would appreciate it if somebody can help me.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved Improve this question If $A$ is a square matrix such that it is not singular, then  $(A^{-1})^{-1} = A$ How can I prove this property? I would appreciate it if somebody can help me.",,"['linear-algebra', 'matrices', 'inverse']"
21,Why does this description of vector spaces include so many tedious/ seemingly obvious criteria?,Why does this description of vector spaces include so many tedious/ seemingly obvious criteria?,,"I'm a CS student. I commonly notice that when I'm learning math-heavy topics (like machine learning) that the descriptions often seem overly tedious and unintuitive, like the one below. Vector spaces are the basic setting in which linear algebra happens. A vector space $V$ is a set (the elements of which are called vectors) on which two operations are defined: vectors can be added together, and vectors can be multiplied by real numbers called scalars. $V$ must satisfy There exists an additive identity (written $0$ ) in $V$ such that $x + 0 = x$ for all $x\in V$ . For each $x\in V$ , there exists an additive inverse (written $−x$ ) such that $x + (−x) = 0$ . There exists a multiplicative identity (written $1$ ) in $\mathbb{R}$ such that $1x = x$ for all $x\in V$ . Commutativity: $x + y = y + x$ for all $x,y\in V$ . Associativity: $(x + y) + z = x + (y + z)$ and $\alpha(\beta x) = (\alpha \beta)x$ for all $x,y,z\in V$ and $\alpha,\beta\in\mathbb{R}$ . Distributivity: $\alpha(x + y) = \alpha x + \alpha y$ and $(\alpha+\beta)x = \alpha x + \beta x$ for all $x,y\in V$ and $\alpha,\beta\in\mathbb{R}$ . Why do we even need to specify these rules? It seems fairly obvious that in 1., $x +0 =x$ . If fact, I can't think of a case where that wouldn't be true.","I'm a CS student. I commonly notice that when I'm learning math-heavy topics (like machine learning) that the descriptions often seem overly tedious and unintuitive, like the one below. Vector spaces are the basic setting in which linear algebra happens. A vector space is a set (the elements of which are called vectors) on which two operations are defined: vectors can be added together, and vectors can be multiplied by real numbers called scalars. must satisfy There exists an additive identity (written ) in such that for all . For each , there exists an additive inverse (written ) such that . There exists a multiplicative identity (written ) in such that for all . Commutativity: for all . Associativity: and for all and . Distributivity: and for all and . Why do we even need to specify these rules? It seems fairly obvious that in 1., . If fact, I can't think of a case where that wouldn't be true.","V V 0 V x + 0 = x x\in V x\in V −x x + (−x) = 0 1 \mathbb{R} 1x = x x\in V x + y = y + x x,y\in V (x + y) + z = x + (y + z) \alpha(\beta x) = (\alpha \beta)x x,y,z\in V \alpha,\beta\in\mathbb{R} \alpha(x + y) = \alpha x + \alpha y (\alpha+\beta)x = \alpha x + \beta x x,y\in V \alpha,\beta\in\mathbb{R} x +0 =x","['linear-algebra', 'vectors', 'machine-learning', 'axioms']"
22,Is it possible that $\ker(T) = \operatorname{im}(T)$ for some linear transformation $T:V \to V$?,Is it possible that  for some linear transformation ?,\ker(T) = \operatorname{im}(T) T:V \to V,Help would be very appreciated. It it possible that $\ker(T) = \operatorname{im}(T)$ for some linear transformation $T:V \to V$?,Help would be very appreciated. It it possible that $\ker(T) = \operatorname{im}(T)$ for some linear transformation $T:V \to V$?,,['linear-algebra']
23,Prove that $A$ cannot be invertible if $A^2=0$,Prove that  cannot be invertible if,A A^2=0,"Let $A$ be an $n\times n$ matrix for which $A^2=0$. Prove that $A$ can not be invertible. My attempt: Given $A^2 = 0$, this means that $A = 0$. If $A$ is invertible, there must be an $n \times n$ matrix $B$ such that $AB = I$. However, because $A = 0$, this is not possible, thus $A$ is not invertible.","Let $A$ be an $n\times n$ matrix for which $A^2=0$. Prove that $A$ can not be invertible. My attempt: Given $A^2 = 0$, this means that $A = 0$. If $A$ is invertible, there must be an $n \times n$ matrix $B$ such that $AB = I$. However, because $A = 0$, this is not possible, thus $A$ is not invertible.",,"['linear-algebra', 'matrices']"
24,"Show that if $B^2 x = 0_n$ for some vector $x \neq 0_n$, then $B$ is not invertible","Show that if  for some vector , then  is not invertible",B^2 x = 0_n x \neq 0_n B,"If $B \in M_{n \times n}(\mathbb{R})$ and $B^2 x = 0_n$ for some vector $x \neq 0_n$, then $B$ is not invertible. I get that $$\mbox{rank} ( B^2 ) < n$$ but I can't seem to be able to link it to $B$ . Perhaps I need to use diagonalization to deal with the power, but that only works if $B$ is diagonalizable. Any hints would be appreciated.","If $B \in M_{n \times n}(\mathbb{R})$ and $B^2 x = 0_n$ for some vector $x \neq 0_n$, then $B$ is not invertible. I get that $$\mbox{rank} ( B^2 ) < n$$ but I can't seem to be able to link it to $B$ . Perhaps I need to use diagonalization to deal with the power, but that only works if $B$ is diagonalizable. Any hints would be appreciated.",,"['linear-algebra', 'matrices']"
25,"Dimension of $GL(n, \mathbb{R})$",Dimension of,"GL(n, \mathbb{R})","Why is the group $GL(n, \mathbb{R})$ of dimension $n^{2}$?","Why is the group $GL(n, \mathbb{R})$ of dimension $n^{2}$?",,"['linear-algebra', 'matrices']"
26,Calculating Eigenvectors: Is my book wrong?,Calculating Eigenvectors: Is my book wrong?,,"I have a covariance matrix: $$    S= \begin{pmatrix}     16 & 10 \\     10 & 25     \end{pmatrix} $$ I calculate my eigenvalues correctly (the same as what the book finds); $\lambda_1 = 31.47$ , $\lambda_2 = 9.53$ But now it comes to calculating eigenvectors: I do everything as I was taught way back in Elementary Linear Algebra. $S X = \lambda v$ {where v is the eigenvector} $(S - I \lambda)v$ Get Row-Echelon Form But when I do this I get the following reduced matrix: $$     \begin{pmatrix}     1 & -.646412 & 0 \\     0 & 0 &0      \end{pmatrix} $$ But this result doesn't seem consistent with my textbook which says that the eigenvectors are; $(0.54 , 0.84)^T$ and $(0.84 , -0.54)$ I looked online for calculators and found one consistent with the book and a few consistent with my result: Consistent with Book: http://comnuan.com/cmnn01002/ Consistent with Me: http://www.arndt-bruenner.de/mathe/scripts/engl_eigenwert2.htm Any ideas? Additional Information: This problem stems from Principal Component Analysis","I have a covariance matrix: $$    S= \begin{pmatrix}     16 & 10 \\     10 & 25     \end{pmatrix} $$ I calculate my eigenvalues correctly (the same as what the book finds); $\lambda_1 = 31.47$ , $\lambda_2 = 9.53$ But now it comes to calculating eigenvectors: I do everything as I was taught way back in Elementary Linear Algebra. $S X = \lambda v$ {where v is the eigenvector} $(S - I \lambda)v$ Get Row-Echelon Form But when I do this I get the following reduced matrix: $$     \begin{pmatrix}     1 & -.646412 & 0 \\     0 & 0 &0      \end{pmatrix} $$ But this result doesn't seem consistent with my textbook which says that the eigenvectors are; $(0.54 , 0.84)^T$ and $(0.84 , -0.54)$ I looked online for calculators and found one consistent with the book and a few consistent with my result: Consistent with Book: http://comnuan.com/cmnn01002/ Consistent with Me: http://www.arndt-bruenner.de/mathe/scripts/engl_eigenwert2.htm Any ideas? Additional Information: This problem stems from Principal Component Analysis",,"['linear-algebra', 'eigenvalues-eigenvectors']"
27,Finding an equation of circle which passes through three points,Finding an equation of circle which passes through three points,,"How to find the equation of a circle which passes through these points $(5,10), (-5,0),(9,-6)$ using the formula  $(x-q)^2 + (y-p)^2 = r^2$. I know i need to use that formula but have no idea how to start, I have tried to start but don't think my answer is right.","How to find the equation of a circle which passes through these points $(5,10), (-5,0),(9,-6)$ using the formula  $(x-q)^2 + (y-p)^2 = r^2$. I know i need to use that formula but have no idea how to start, I have tried to start but don't think my answer is right.",,"['linear-algebra', 'algebra-precalculus', 'matrices', 'geometry', 'analytic-geometry']"
28,Proving $A$ is invertible if $A + A^2 = I$.,Proving  is invertible if .,A A + A^2 = I,"I'm trying to prove $A$ is invertible by proving there is an $A'$, for $AA' = I$. So I got to this stage $A(I + A) = I$, now I determine that $A' = I + A$,  and from that I get $AA' = I$. I wanted to know if this is valid,  casue it doesn't seem to make sense,and I can't find a 'real' solution for such equation. Note:  $A$ is $n \times n$ and  $I$ represents the identity matrix and is also of the same dimensions.","I'm trying to prove $A$ is invertible by proving there is an $A'$, for $AA' = I$. So I got to this stage $A(I + A) = I$, now I determine that $A' = I + A$,  and from that I get $AA' = I$. I wanted to know if this is valid,  casue it doesn't seem to make sense,and I can't find a 'real' solution for such equation. Note:  $A$ is $n \times n$ and  $I$ represents the identity matrix and is also of the same dimensions.",,"['linear-algebra', 'matrices']"
29,How to find a 3x3 matrix with determinant =0 from which I can delete random column and random row to make it nonzero?,How to find a 3x3 matrix with determinant =0 from which I can delete random column and random row to make it nonzero?,,I need to find a $3 \times 3$ matrix and the determinant of this matrix has to be $0$. I also need to be able to delete randomly chosen column and row to make the determinant nonzero?  Is it even possible? Thank you.,I need to find a $3 \times 3$ matrix and the determinant of this matrix has to be $0$. I also need to be able to delete randomly chosen column and row to make the determinant nonzero?  Is it even possible? Thank you.,,"['linear-algebra', 'matrices', 'determinant']"
30,How to prove $I + t X$ is invertible for small enough $ | t | ?$,How to prove  is invertible for small enough,I + t X  | t | ?,"Let $X \in \text{GL}_n(\mathbb{R})$ be an arbitrary real $n\times n$ matrix. How can we prove rigorously:  $$ \underset{b>0} {\exists} : \underset{|t|\le b} {\forall} : \det (I + t X) \neq 0 $$ If necessary, we could also assume that $t \ge 0.$","Let $X \in \text{GL}_n(\mathbb{R})$ be an arbitrary real $n\times n$ matrix. How can we prove rigorously:  $$ \underset{b>0} {\exists} : \underset{|t|\le b} {\forall} : \det (I + t X) \neq 0 $$ If necessary, we could also assume that $t \ge 0.$",,"['linear-algebra', 'matrices', 'determinant']"
31,Doubts about a question I asked a long time ago (eigenvalues),Doubts about a question I asked a long time ago (eigenvalues),,"Here I posted a question about the eigenvalues of the matrix $A:=vv^t$ (where $v\in\mathbb{R}^n$). The question was answered but I think (after some time) that I am not satisfied. Can someone please expand the answer? I don't understand why $A$ has rank at most $1$ and why this fact implies that $\lambda=\sum x_i^2$ is the unique eigenvalue. In addition, can I conclude that $A$ is diagonalizable?","Here I posted a question about the eigenvalues of the matrix $A:=vv^t$ (where $v\in\mathbb{R}^n$). The question was answered but I think (after some time) that I am not satisfied. Can someone please expand the answer? I don't understand why $A$ has rank at most $1$ and why this fact implies that $\lambda=\sum x_i^2$ is the unique eigenvalue. In addition, can I conclude that $A$ is diagonalizable?",,"['linear-algebra', 'eigenvalues-eigenvectors', 'diagonalization']"
32,Find an expression for $A^n = \left( \begin{array}{cc} 1 & 4 \\ 2 & 3 \end{array} \right)^n$,Find an expression for,A^n = \left( \begin{array}{cc} 1 & 4 \\ 2 & 3 \end{array} \right)^n,"We want to find an expression for $A^n = \left( \begin{array}{cc} 1 & 4 \\ 2 & 3 \end{array} \right)^n$ for an arbitrary ""n"". I have tried writing out a few elements of the sequence as $n \to \infty$: $A^2 =  \left( \begin{array}{cc} 9 & 16 \\ 8 & 17 \end{array} \right)$ $A^3 = \left( \begin{array}{cc} 41 & 84 \\ 36 & 51 \end{array} \right)$ However, a pattern doesn't seem to appear. This is where I want to ask my question: if we put this matrix into reduced-row echelon form, would an expression of the $(reduced matrix)^n$ work as an expression for the original matrix $A$? i.e. reduced-row matrix $ = \left( \begin{array}{cc} 1 & 0 \\ 0 & 1 \end{array} \right)^n$. Then, we know that any diagonal matrix to the $n^{th}$ is just the diagonal entries to the $n^{th}$ and this would make an expression easy to come up with. Thank you!","We want to find an expression for $A^n = \left( \begin{array}{cc} 1 & 4 \\ 2 & 3 \end{array} \right)^n$ for an arbitrary ""n"". I have tried writing out a few elements of the sequence as $n \to \infty$: $A^2 =  \left( \begin{array}{cc} 9 & 16 \\ 8 & 17 \end{array} \right)$ $A^3 = \left( \begin{array}{cc} 41 & 84 \\ 36 & 51 \end{array} \right)$ However, a pattern doesn't seem to appear. This is where I want to ask my question: if we put this matrix into reduced-row echelon form, would an expression of the $(reduced matrix)^n$ work as an expression for the original matrix $A$? i.e. reduced-row matrix $ = \left( \begin{array}{cc} 1 & 0 \\ 0 & 1 \end{array} \right)^n$. Then, we know that any diagonal matrix to the $n^{th}$ is just the diagonal entries to the $n^{th}$ and this would make an expression easy to come up with. Thank you!",,"['linear-algebra', 'matrices']"
33,Eigenvalues of adjoint operator [General Case],Eigenvalues of adjoint operator [General Case],,"I am asked to prove that if $T: V \rightarrow V$ is a linear operator over a complex inner product space $(V,\langle,\rangle)$, then $\overline{\lambda}$ is an eigenvalue of $T^*$ where $\lambda$ is an eigenvalue of $T$. The problem is easily reduced to the cases where either $V$ is a finite dimensional space or $T$ is a normal operator, but I am stuck with the general case.","I am asked to prove that if $T: V \rightarrow V$ is a linear operator over a complex inner product space $(V,\langle,\rangle)$, then $\overline{\lambda}$ is an eigenvalue of $T^*$ where $\lambda$ is an eigenvalue of $T$. The problem is easily reduced to the cases where either $V$ is a finite dimensional space or $T$ is a normal operator, but I am stuck with the general case.",,"['linear-algebra', 'eigenvalues-eigenvectors', 'operator-theory']"
34,"Proving that $\sqrt[3] {2} ,\sqrt[3] {4},1$ are linearly independent over rationals",Proving that  are linearly independent over rationals,"\sqrt[3] {2} ,\sqrt[3] {4},1","I was trying to prove that $\sqrt[3] {2} ,\sqrt[3] {4}$ and  $1$ are linearly independent using elementary knowledge of rational numbers. I also saw this which was in a way close to the question I was thinking about. But I could not come up with any proof using simple arguments. So if someone could give a simple proof, it would be great. My try: $a \sqrt[3] {2}+b\sqrt[3] {4}+c=0$ Then taking $c$ to the other side cubing on both sides we get $2a^3+4b^3+6ab(a+b)=-c^3$. I could not proceed further from here. Apart from the above question i was also wondering how one would prove that $\sqrt{2},\sqrt{3},\sqrt{5},\sqrt{7},\sqrt{11},\sqrt{13}$ are linearly independent. Here assuming $a\sqrt{2}+b\sqrt{3}+c\sqrt{5}+...=0$ and solving seems to get complicated. So how does one solve problems of this type?","I was trying to prove that $\sqrt[3] {2} ,\sqrt[3] {4}$ and  $1$ are linearly independent using elementary knowledge of rational numbers. I also saw this which was in a way close to the question I was thinking about. But I could not come up with any proof using simple arguments. So if someone could give a simple proof, it would be great. My try: $a \sqrt[3] {2}+b\sqrt[3] {4}+c=0$ Then taking $c$ to the other side cubing on both sides we get $2a^3+4b^3+6ab(a+b)=-c^3$. I could not proceed further from here. Apart from the above question i was also wondering how one would prove that $\sqrt{2},\sqrt{3},\sqrt{5},\sqrt{7},\sqrt{11},\sqrt{13}$ are linearly independent. Here assuming $a\sqrt{2}+b\sqrt{3}+c\sqrt{5}+...=0$ and solving seems to get complicated. So how does one solve problems of this type?",,"['linear-algebra', 'radicals']"
35,Invertible Matrices within a Matrix,Invertible Matrices within a Matrix,,"Suppose A, B are invertible matrices of the same size. Show that $$M = \begin{bmatrix} 0& A\\ B& 0\end{bmatrix}$$ is invertible. I don't understand how I could show this. I have learned about linear combinations and spanning in my college class, but I don't know how that would help in this case.","Suppose A, B are invertible matrices of the same size. Show that is invertible. I don't understand how I could show this. I have learned about linear combinations and spanning in my college class, but I don't know how that would help in this case.",M = \begin{bmatrix} 0& A\\ B& 0\end{bmatrix},"['linear-algebra', 'matrices', 'inverse', 'block-matrices']"
36,Question about definition: what is an affine linear space?,Question about definition: what is an affine linear space?,,In an article I am reading it says : Let $H$ be an affine linear space of codimension $m$... Could someone please explain me what is meant by affine linear space? Thanks!,In an article I am reading it says : Let $H$ be an affine linear space of codimension $m$... Could someone please explain me what is meant by affine linear space? Thanks!,,"['linear-algebra', 'definition']"
37,Show that the vector space of all continuous real-valued functions is infinite-dimensional,Show that the vector space of all continuous real-valued functions is infinite-dimensional,,"Show that the vector space $C(\Bbb R)$ of all continuous functions defined on the real line is infinite-dimensional. I get that if $C(\Bbb R)$ contains an infinite-dimensional subspace, then it is infinite-dimensional, but how do I prove that? Obviously $\Bbb R$ is infinite…","Show that the vector space $C(\Bbb R)$ of all continuous functions defined on the real line is infinite-dimensional. I get that if $C(\Bbb R)$ contains an infinite-dimensional subspace, then it is infinite-dimensional, but how do I prove that? Obviously $\Bbb R$ is infinite…",,['linear-algebra']
38,"can $\cos(x)$ be written as a linear combination of $e^x$ and $e^{-x}$ using the interval $[0,1]$?",can  be written as a linear combination of  and  using the interval ?,"\cos(x) e^x e^{-x} [0,1]","Consider $C[0,1]$: the vector space of all continuous functions on the interval $[0,1]$.  Let $S$ be a subspace of $C[0,1]$ where $S =$ the span of $\{e^x, e^{-x}\}$ does the following function: $\cos(x)$ belong to $S$?  In other words, can $\cos(x)$ be rewritten as a linear combination of $e^x$ and $e^{-x}$ when working with the interval $[0,1]$? My intuition is yes, since these functions arent discontinuous, there will always be some real numbers a and b such that satisfy the following equation: $a\cdot e^x + b\cdot e^{-x} = \cos(x)$ for all $x$ in $[0,1]$.  I just dont know how to prove that..","Consider $C[0,1]$: the vector space of all continuous functions on the interval $[0,1]$.  Let $S$ be a subspace of $C[0,1]$ where $S =$ the span of $\{e^x, e^{-x}\}$ does the following function: $\cos(x)$ belong to $S$?  In other words, can $\cos(x)$ be rewritten as a linear combination of $e^x$ and $e^{-x}$ when working with the interval $[0,1]$? My intuition is yes, since these functions arent discontinuous, there will always be some real numbers a and b such that satisfy the following equation: $a\cdot e^x + b\cdot e^{-x} = \cos(x)$ for all $x$ in $[0,1]$.  I just dont know how to prove that..",,"['linear-algebra', 'vector-spaces']"
39,"Polynomials identity, factoring $x^{2^n}-1$","Polynomials identity, factoring",x^{2^n}-1,"There is a proof that I can't solve. Show that for any integer $k$, the following identity holds: $$(1+x)(1+x^2)(1+x^4)\cdots(1+x^{2^{k-1}})=1+x+x^2+x^3+\cdots+x^{2^k-1}$$ Thanks for your help.","There is a proof that I can't solve. Show that for any integer $k$, the following identity holds: $$(1+x)(1+x^2)(1+x^4)\cdots(1+x^{2^{k-1}})=1+x+x^2+x^3+\cdots+x^{2^k-1}$$ Thanks for your help.",,"['linear-algebra', 'abstract-algebra', 'polynomials']"
40,"Showing that $\{ 1, \cos t, \cos^2 t, \dots, \cos^6 t \}$ is a linearly independent set",Showing that  is a linearly independent set,"\{ 1, \cos t, \cos^2 t, \dots, \cos^6 t \}","The problem is: Show that $\{ 1, \cos t, \cos^2 t, \dots, \cos^6 t \}$ is a linearly independent set of functions defined on $\mathbb{R}$. The problem expects the student to use a computer program such as Matlab. To solve the problem I created a matrix in Matlab with $t$ ranging from $1$ to $7$, then row reduced it and got the identity matrix as result. I.e. the columns of the matrix is linearly independent. Is it enough to just show for $t=1\to 7$; could it not possibly break down at some other number? And is there a more elegant way of solving this without the use of ""brute force""? Here's the Matlab code if necessary: for t = 1:7 A(t,:) = [1 cos(t) (cos(t))^2 (cos(t))^3 (cos(t))^4 (cos(t))^5 (cos(t))^6]; end","The problem is: Show that $\{ 1, \cos t, \cos^2 t, \dots, \cos^6 t \}$ is a linearly independent set of functions defined on $\mathbb{R}$. The problem expects the student to use a computer program such as Matlab. To solve the problem I created a matrix in Matlab with $t$ ranging from $1$ to $7$, then row reduced it and got the identity matrix as result. I.e. the columns of the matrix is linearly independent. Is it enough to just show for $t=1\to 7$; could it not possibly break down at some other number? And is there a more elegant way of solving this without the use of ""brute force""? Here's the Matlab code if necessary: for t = 1:7 A(t,:) = [1 cos(t) (cos(t))^2 (cos(t))^3 (cos(t))^4 (cos(t))^5 (cos(t))^6]; end",,['linear-algebra']
41,How to compute the SVD of a symmetric matrix?,How to compute the SVD of a symmetric matrix?,,"If I have only the upper triangular part of a symmetric matrix $A$, how could I compute the SVD? $$\begin{pmatrix} 1 & 22 & 13 & 14 \\  & 1 & 45 & 24 \\  &  & 1 & 34 \\  &  &  & 1\end{pmatrix}$$ Does having this upper triangular make the computing easier?","If I have only the upper triangular part of a symmetric matrix $A$, how could I compute the SVD? $$\begin{pmatrix} 1 & 22 & 13 & 14 \\  & 1 & 45 & 24 \\  &  & 1 & 34 \\  &  &  & 1\end{pmatrix}$$ Does having this upper triangular make the computing easier?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'svd', 'symmetric-matrices']"
42,A pattern in determinants of Fibonacci numbers?,A pattern in determinants of Fibonacci numbers?,,"Let $F_n$ denote the $n$ th Fibonacci number , adopting the convention $F_1=1$ , $F_2=1$ and so on. Consider the $n\times n$ matrix defined by $$\mathbf M_n:=\begin{bmatrix}F_1&F_2&\dots&F_n\\F_{n+1}&F_{n+2}&\dots&F_{2n}\\\vdots&\vdots&\ddots&\vdots\\F_{n^2-n+1}&F_{n^2-n+2}&\dots&F_{n^2}\end{bmatrix}.$$ I have the following conjecture: Conjecture. For all integers $n\geq3$ , $\det\mathbf M_n=0$ . I have used some Python code to test this conjecture for $n$ up to $9$ , but I cannot go further. Note that $\det\mathbf M_1=\det\mathbf M_2=1$ . Due to the elementary nature of this problem I have to assume that it has been discussed before, perhaps even on this site. But I couldn't find any reference on it, by Googling or searching here. Can someone shed light onto whether the conjecture is true, and a proof of it if so?","Let denote the th Fibonacci number , adopting the convention , and so on. Consider the matrix defined by I have the following conjecture: Conjecture. For all integers , . I have used some Python code to test this conjecture for up to , but I cannot go further. Note that . Due to the elementary nature of this problem I have to assume that it has been discussed before, perhaps even on this site. But I couldn't find any reference on it, by Googling or searching here. Can someone shed light onto whether the conjecture is true, and a proof of it if so?",F_n n F_1=1 F_2=1 n\times n \mathbf M_n:=\begin{bmatrix}F_1&F_2&\dots&F_n\\F_{n+1}&F_{n+2}&\dots&F_{2n}\\\vdots&\vdots&\ddots&\vdots\\F_{n^2-n+1}&F_{n^2-n+2}&\dots&F_{n^2}\end{bmatrix}. n\geq3 \det\mathbf M_n=0 n 9 \det\mathbf M_1=\det\mathbf M_2=1,"['linear-algebra', 'determinant', 'fibonacci-numbers']"
43,Solve equation in determinant,Solve equation in determinant,,"Let $ a,b,c,m,n,p\in \mathbb{R}^{*} $, $ a+m+n=p+b+c $. Solve the equation: $$  \begin{vmatrix} x & a & b &c \\  a & x & b &c \\  m &n  & x &p \\   m&  n&  p& x \end{vmatrix} =0 $$ I had used the Schur complement ($\det(M)=\det(A)\cdot (D-C\cdot A^{-1}\cdot B)$, for $ M= \begin{bmatrix} A &B \\  C & D \end{bmatrix}) $ but it didn't help me.","Let $ a,b,c,m,n,p\in \mathbb{R}^{*} $, $ a+m+n=p+b+c $. Solve the equation: $$  \begin{vmatrix} x & a & b &c \\  a & x & b &c \\  m &n  & x &p \\   m&  n&  p& x \end{vmatrix} =0 $$ I had used the Schur complement ($\det(M)=\det(A)\cdot (D-C\cdot A^{-1}\cdot B)$, for $ M= \begin{bmatrix} A &B \\  C & D \end{bmatrix}) $ but it didn't help me.",,"['linear-algebra', 'matrices', 'determinant']"
44,How do I calculate the gradient of a function in a $n$-dimensional space?,How do I calculate the gradient of a function in a -dimensional space?,n,"$q(x)=x^TAx+b^Tx+c$ $A$ is matrix. $x,b\in \mathbb{R}^n$ and  $c\in \mathbb{R}$ I really don't know how to calculate it for this function.","$q(x)=x^TAx+b^Tx+c$ $A$ is matrix. $x,b\in \mathbb{R}^n$ and  $c\in \mathbb{R}$ I really don't know how to calculate it for this function.",,"['linear-algebra', 'matrices']"
45,How do you compute eigenvalues/vectors of big $n\times n$ matrix?,How do you compute eigenvalues/vectors of big  matrix?,n\times n,"The product of the non-zero eigenvalues of the matrix is ____. $$\pmatrix{1&0&0&0&1\\0&1&1&1&0\\0&1&1&1&0\\0&1&1&1&0\\1&0&0&0&1}$$ My attempt: Well, answer is $6$ . It's a big matrix (but not more), so finding eigenvalues by characteristics equation will be lengthy process. I'm trying for any short trick to find eigenvalues of a big $n \times n$ matrix. This explained "" eigenvalues by inspection "". I'm not getting properly. Can you explain, eigenvalues and eigenvectors by inspection for this matrix in steps please? Can you explain in steps.","The product of the non-zero eigenvalues of the matrix is ____. My attempt: Well, answer is . It's a big matrix (but not more), so finding eigenvalues by characteristics equation will be lengthy process. I'm trying for any short trick to find eigenvalues of a big matrix. This explained "" eigenvalues by inspection "". I'm not getting properly. Can you explain, eigenvalues and eigenvectors by inspection for this matrix in steps please? Can you explain in steps.",\pmatrix{1&0&0&0&1\\0&1&1&1&0\\0&1&1&1&0\\0&1&1&1&0\\1&0&0&0&1} 6 n \times n,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
46,Video Lectures for Linear Algebra,Video Lectures for Linear Algebra,,"Are there any video lectures for linear algebra other than Strang's? . I'm looking for something more pure. Ideally, from a professor who follows a book similar to Hoffman and Kunze or Axler. Thanks!","Are there any video lectures for linear algebra other than Strang's? . I'm looking for something more pure. Ideally, from a professor who follows a book similar to Hoffman and Kunze or Axler. Thanks!",,"['linear-algebra', 'online-resources']"
47,What is Modern Mathematics? Is this an exact concept with a clear meaning? [closed],What is Modern Mathematics? Is this an exact concept with a clear meaning? [closed],,"It's difficult to tell what is being asked here. This question is ambiguous, vague, incomplete, overly broad, or rhetorical and cannot be reasonably answered in its current form. For help clarifying this question so that it can be reopened, visit the help center . Closed 13 years ago . Motivated by this question I would like to know whether there is an exact definition of modern mathematics. In which point in time  (century, decade) does one think, when speaking about modern mathematics. Does it refer to the Abstract Algebra? Edit: I got -1. If this is not a correct question, please state why. And it can be closed if the question is not proper. This is a genuine doubt and ignorance of mine! Edit 2: In my humble opinion instead of closing, should be better to tag it as a community wiki. Anyway I do not have any no objections against in closing it. Edit 3: Now is a community wiki. Edit 4. I have learned from the answers and comments, including the explanation for closing!","It's difficult to tell what is being asked here. This question is ambiguous, vague, incomplete, overly broad, or rhetorical and cannot be reasonably answered in its current form. For help clarifying this question so that it can be reopened, visit the help center . Closed 13 years ago . Motivated by this question I would like to know whether there is an exact definition of modern mathematics. In which point in time  (century, decade) does one think, when speaking about modern mathematics. Does it refer to the Abstract Algebra? Edit: I got -1. If this is not a correct question, please state why. And it can be closed if the question is not proper. This is a genuine doubt and ignorance of mine! Edit 2: In my humble opinion instead of closing, should be better to tag it as a community wiki. Anyway I do not have any no objections against in closing it. Edit 3: Now is a community wiki. Edit 4. I have learned from the answers and comments, including the explanation for closing!",,"['linear-algebra', 'abstract-algebra', 'terminology', 'math-history']"
48,"For vector spaces $V,W$ over $\mathbf{k}$, is every additive $\phi: V \to W$ also $\mathbf{k}$-linear?","For vector spaces  over , is every additive  also -linear?","V,W \mathbf{k} \phi: V \to W \mathbf{k}","Since every map of vector spaces is a map of abelian groups, I was wondering if the converse also holds: Given an additive map $\phi: V \to W$ between two vector spaces, does it follow that $\phi$ is also $\mathbf{k}-$ linear? I'm interested in the case of $\mathbf{k}$ having characteristic zero, specially if $\mathbf{k}$ is a famous field like rational or real or complex numbers. I'm guessing it's false, but I tried to come up with counter-examples for $\mathbf{k} = \mathbf{Q}, \mathbf{R}$ and couldn't find any. Finding a counter-example in characteristic $p>0$ may not be that hard, for instance since taking $p-$ th power is additive. However that's not the case I care most about. Appreciate any help!","Since every map of vector spaces is a map of abelian groups, I was wondering if the converse also holds: Given an additive map between two vector spaces, does it follow that is also linear? I'm interested in the case of having characteristic zero, specially if is a famous field like rational or real or complex numbers. I'm guessing it's false, but I tried to come up with counter-examples for and couldn't find any. Finding a counter-example in characteristic may not be that hard, for instance since taking th power is additive. However that's not the case I care most about. Appreciate any help!","\phi: V \to W \phi \mathbf{k}- \mathbf{k} \mathbf{k} \mathbf{k} = \mathbf{Q}, \mathbf{R} p>0 p-","['linear-algebra', 'abstract-algebra', 'vector-spaces']"
49,Is this a correct way to prove T is not a linear transformation?,Is this a correct way to prove T is not a linear transformation?,,"I have the following transformation $T:\mathbb{R}^2 \longrightarrow \mathbb{R}^3$ defined by $T\left( x, y \right) = \left( y, x, x^2 + y^2 \right).$ I know the transformation is not linear but would like to prove it, so I deviced the following ""proof."" We know every linear transformation $T$ has a unique matrix representation for the standard basis of $\mathbb{R}^2,$ which is given by $$A = \left[ \begin{array}{ccc} T(\mathbf{e}_1) & T(\mathbf{e}_2) \\\end{array} \right],$$ and this matrix $A$ would move me back to the linear transformation by $T\left( \mathbf{x} \right) = A \mathbf{x}.$ So, I assume $T$ is a linear transformation and construct it standard matrix representation, which would be $$A = \left( \begin{array}{ccc} 0 & 1 \\ 1 & 0 \\ 1 & 1 \end{array} \right).$$ Now, to get my original transfomation back I would have to do $$T\left( \mathbf{x} \right) = A \mathbf{x} = \left( \begin{array}{ccc} 0 & 1 \\ 1 & 0 \\ 1 & 1 \end{array} \right) \cdot \left( \begin{array}{ccc} x \\ y \end{array} \right) = \left( \begin{array}{ccc} y \\ x \\ x+y \end{array} \right).$$ Since this transformation I got is not the original one, I conclude $T$ is not a linear transformation. My question is, the above reasoning is correct? And in general, can I apply this method to prove or disprove any transformation is a linear transformation? EDIT: Please do not sugegst alternative methods of proof; I know them well. All I need is to know if the method described works.","I have the following transformation $T:\mathbb{R}^2 \longrightarrow \mathbb{R}^3$ defined by $T\left( x, y \right) = \left( y, x, x^2 + y^2 \right).$ I know the transformation is not linear but would like to prove it, so I deviced the following ""proof."" We know every linear transformation $T$ has a unique matrix representation for the standard basis of $\mathbb{R}^2,$ which is given by $$A = \left[ \begin{array}{ccc} T(\mathbf{e}_1) & T(\mathbf{e}_2) \\\end{array} \right],$$ and this matrix $A$ would move me back to the linear transformation by $T\left( \mathbf{x} \right) = A \mathbf{x}.$ So, I assume $T$ is a linear transformation and construct it standard matrix representation, which would be $$A = \left( \begin{array}{ccc} 0 & 1 \\ 1 & 0 \\ 1 & 1 \end{array} \right).$$ Now, to get my original transfomation back I would have to do $$T\left( \mathbf{x} \right) = A \mathbf{x} = \left( \begin{array}{ccc} 0 & 1 \\ 1 & 0 \\ 1 & 1 \end{array} \right) \cdot \left( \begin{array}{ccc} x \\ y \end{array} \right) = \left( \begin{array}{ccc} y \\ x \\ x+y \end{array} \right).$$ Since this transformation I got is not the original one, I conclude $T$ is not a linear transformation. My question is, the above reasoning is correct? And in general, can I apply this method to prove or disprove any transformation is a linear transformation? EDIT: Please do not sugegst alternative methods of proof; I know them well. All I need is to know if the method described works.",,"['linear-algebra', 'proof-writing', 'linear-transformations']"
50,Is there a scalar product s.t. the following list is orthogonal?,Is there a scalar product s.t. the following list is orthogonal?,,"Let $A_1=\begin{pmatrix}1&0\\0&0\end{pmatrix}\quad A_2=\begin{pmatrix}1&1\\0&0\end{pmatrix},\quad A_3=\begin{pmatrix}1&1\\1&0\end{pmatrix},\quad A_4=\begin{pmatrix}1&1\\1&1\end{pmatrix}$. Is there a scalar product s.t. $\|A_k\|=k$ for $k=1,2,3,4$ and $A_i\perp A_j$ for $i\neq j$ ? With $\langle A, B \rangle = \mbox{Tr}(A^\top B)$ we have that $\|A_i\|=i$, but unfortunately it's not orthogonal. So, how can we conclude?","Let $A_1=\begin{pmatrix}1&0\\0&0\end{pmatrix}\quad A_2=\begin{pmatrix}1&1\\0&0\end{pmatrix},\quad A_3=\begin{pmatrix}1&1\\1&0\end{pmatrix},\quad A_4=\begin{pmatrix}1&1\\1&1\end{pmatrix}$. Is there a scalar product s.t. $\|A_k\|=k$ for $k=1,2,3,4$ and $A_i\perp A_j$ for $i\neq j$ ? With $\langle A, B \rangle = \mbox{Tr}(A^\top B)$ we have that $\|A_i\|=i$, but unfortunately it's not orthogonal. So, how can we conclude?",,['linear-algebra']
51,Can we say that $\det(A+B) = \det(A) + \det(B) +\operatorname{tr}(A) \operatorname{tr}(B) - \operatorname{tr}(AB)$?,Can we say that ?,\det(A+B) = \det(A) + \det(B) +\operatorname{tr}(A) \operatorname{tr}(B) - \operatorname{tr}(AB),"Let $A,B \in M_n$ . Is this formula true? $$\det(A+B) = \det(A) + \det(B) + \operatorname{tr}(A) \operatorname{tr}(B) - \operatorname{tr}(AB).$$",Let . Is this formula true?,"A,B \in M_n \det(A+B) = \det(A) + \det(B) + \operatorname{tr}(A) \operatorname{tr}(B) - \operatorname{tr}(AB).","['linear-algebra', 'matrices', 'determinant', 'trace']"
52,Eigenvalues of $AB$ and $BA$ where $A$ and $B$ are rectangular matrices,Eigenvalues of  and  where  and  are rectangular matrices,AB BA A B,"This question is a generalisation of Eigenvalues of $AB$ and $BA$ where $A$ and $B$ are square matrices . Let $A$ and $B$ be $m\times n$ and $n\times m$ complex matrices, respectively, with $m < n$. If the eigenvalues of $AB$ are $\lambda_1, \ldots, \lambda_m$, what are the eigenvalues of $BA$? If the matrices were square, then the conclusion would follow  from the fact that $AB$ and $BA$ have  the same characteristic polynomial . With rectangular matrices this is not going to happen; how to proceed then?","This question is a generalisation of Eigenvalues of $AB$ and $BA$ where $A$ and $B$ are square matrices . Let $A$ and $B$ be $m\times n$ and $n\times m$ complex matrices, respectively, with $m < n$. If the eigenvalues of $AB$ are $\lambda_1, \ldots, \lambda_m$, what are the eigenvalues of $BA$? If the matrices were square, then the conclusion would follow  from the fact that $AB$ and $BA$ have  the same characteristic polynomial . With rectangular matrices this is not going to happen; how to proceed then?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
53,Prove that if $ u \cdot v = u \cdot w $ then $v = w$,Prove that if  then, u \cdot v = u \cdot w  v = w,"I've tried putting it up as: $$ [u_1 v_1 + \ldots + u_n v_n] = [u_1 w_1 + \ldots + u_n w_n] $$ But this doesn't make it immediately clear...I can't simply divide by $u_1 + \ldots + u_n$ as these ($u$, $v$ and $w$) are vectors... Any hints?","I've tried putting it up as: $$ [u_1 v_1 + \ldots + u_n v_n] = [u_1 w_1 + \ldots + u_n w_n] $$ But this doesn't make it immediately clear...I can't simply divide by $u_1 + \ldots + u_n$ as these ($u$, $v$ and $w$) are vectors... Any hints?",,['linear-algebra']
54,"for a $3 \times 3$ matrix A ,value of $ A^{50} $ is","for a  matrix A ,value of  is",3 \times 3  A^{50} ,I f $$A= \begin{pmatrix}1& 0 & 0 \\  1 & 0 & 1\\ 0 & 1 & 0 \end{pmatrix}$$  then $ A^{50} $  is $$ \begin{pmatrix}1& 0 & 0 \\  50 & 1 & 0\\ 50 & 0 & 1 \end{pmatrix}$$ $$\begin{pmatrix}1& 0 & 0 \\  48 & 1 & 0\\ 48 & 0 & 1 \end{pmatrix}$$ $$\begin{pmatrix}1& 0 & 0 \\  25 & 1 & 0\\ 25 & 0 & 1 \end{pmatrix}$$ $$\begin{pmatrix}1& 0 & 0 \\  24 & 1 & 0\\ 24 & 0 & 1\end{pmatrix}$$ I am stuck on this problem. Can anyone help me please...............,I f $$A= \begin{pmatrix}1& 0 & 0 \\  1 & 0 & 1\\ 0 & 1 & 0 \end{pmatrix}$$  then $ A^{50} $  is $$ \begin{pmatrix}1& 0 & 0 \\  50 & 1 & 0\\ 50 & 0 & 1 \end{pmatrix}$$ $$\begin{pmatrix}1& 0 & 0 \\  48 & 1 & 0\\ 48 & 0 & 1 \end{pmatrix}$$ $$\begin{pmatrix}1& 0 & 0 \\  25 & 1 & 0\\ 25 & 0 & 1 \end{pmatrix}$$ $$\begin{pmatrix}1& 0 & 0 \\  24 & 1 & 0\\ 24 & 0 & 1\end{pmatrix}$$ I am stuck on this problem. Can anyone help me please...............,,['linear-algebra']
55,Is the product of two invertible symmetric matrices always diagonalizable?,Is the product of two invertible symmetric matrices always diagonalizable?,,"I have two symmetric matrices $A$ and $B$ , which are both invertible. Their eigenvalues are obviously real, but not necessarily positive. I know that if one matrix were positive definite, we could use ( Product of two symmetric matrices is similar to a symmetric matrix ) to show that $AB$ is similar to a symmetric matrix (with real eigenvalues). In my cases, the eigenvalues of $AB$ are generally complex, but can one adjust the proof to show that $AB$ is always diagonalizable?","I have two symmetric matrices and , which are both invertible. Their eigenvalues are obviously real, but not necessarily positive. I know that if one matrix were positive definite, we could use ( Product of two symmetric matrices is similar to a symmetric matrix ) to show that is similar to a symmetric matrix (with real eigenvalues). In my cases, the eigenvalues of are generally complex, but can one adjust the proof to show that is always diagonalizable?",A B AB AB AB,"['linear-algebra', 'diagonalization']"
56,"If $N$ is nilpotent, then $N \sim N^2 \Longleftrightarrow N=0$","If  is nilpotent, then",N N \sim N^2 \Longleftrightarrow N=0,"Given that $N$ is nilpotent, then $N$ is similar to $N^2$ if and only if $N=0$ ( $N$ is the zero matrix).  The "" $\Longleftarrow$ "" direction is easy enough, because $N = 0 = N^2$ so they're equal and trivially similar. But showing $N \text{ nilpotent and } N \sim N^2\Longrightarrow N=0$ is turning out to be more difficult.  I'm comfortable using well-known facts about $n \times n$ nilpotent matrices such as: \begin{align} N \text{ nilpotent } &\Longleftrightarrow \text {all eigenvalues }=0\\ &\Longleftrightarrow p_N = \lambda^n\\ &\Longleftrightarrow m_N =\lambda^k\,,\, \text{ for some k, }1\leq k \leq n\\ &\Longleftrightarrow {\rm tr}N^q = 0\,,\, \text{ where }q\in \mathbb{N}\text{, minimal, s.t. } N^q=0 \end{align} Note that $p_N$ above is the characteristic and $m_N$ is the minimal polynomial.  My hunch is to use that trace equals 0 for some power $q$ and the fact that each entry $c_{ij}$ of $N^2$ is \begin{align} c_{ij} = \sum_{k=1}^n n_{ik} n_{kj} \end{align} to get some cancellations but I can't quite see it yet.  Any thoughts?","Given that is nilpotent, then is similar to if and only if ( is the zero matrix).  The "" "" direction is easy enough, because so they're equal and trivially similar. But showing is turning out to be more difficult.  I'm comfortable using well-known facts about nilpotent matrices such as: Note that above is the characteristic and is the minimal polynomial.  My hunch is to use that trace equals 0 for some power and the fact that each entry of is to get some cancellations but I can't quite see it yet.  Any thoughts?","N N N^2 N=0 N \Longleftarrow N = 0 = N^2 N \text{ nilpotent and } N \sim N^2\Longrightarrow N=0 n \times n \begin{align}
N \text{ nilpotent } &\Longleftrightarrow \text {all eigenvalues }=0\\
&\Longleftrightarrow p_N = \lambda^n\\
&\Longleftrightarrow m_N =\lambda^k\,,\, \text{ for some k, }1\leq k \leq n\\
&\Longleftrightarrow {\rm tr}N^q = 0\,,\, \text{ where }q\in \mathbb{N}\text{, minimal, s.t. } N^q=0
\end{align} p_N m_N q c_{ij} N^2 \begin{align}
c_{ij} = \sum_{k=1}^n n_{ik} n_{kj}
\end{align}","['linear-algebra', 'matrices', 'nilpotence']"
57,rank(AB) = rank(A) if B is invertible [duplicate],rank(AB) = rank(A) if B is invertible [duplicate],,"This question already has an answer here : Prove $\operatorname{rank}(AB) = \operatorname{rank}(B)$ (1 answer) Closed 7 years ago . If $B$ is invertible, show that rank($AB$) = rank($A$). I've seen this question asked elsewhere but all had answers I didn't understand. I know how to solve the following problem If $A$ is invertible, then rank($AB$) = rank($B$) Because if $Bx=0$, then $ABx = A0 = 0$, and when $ABx=0$ then $Bx=0$ because $A$ is invertible, so null($AB$)=null($A$), and by the rank-nullity theorem, rank($A$) = rank($AB$). However when $B$ is invertible, as in the problem we have to tackle, I don't know how to use that fact. $ABx = 0$, but $B$ is in the middle so we can't simply get rid of it to get a meaningful expression. Does someone know how to tackle this?","This question already has an answer here : Prove $\operatorname{rank}(AB) = \operatorname{rank}(B)$ (1 answer) Closed 7 years ago . If $B$ is invertible, show that rank($AB$) = rank($A$). I've seen this question asked elsewhere but all had answers I didn't understand. I know how to solve the following problem If $A$ is invertible, then rank($AB$) = rank($B$) Because if $Bx=0$, then $ABx = A0 = 0$, and when $ABx=0$ then $Bx=0$ because $A$ is invertible, so null($AB$)=null($A$), and by the rank-nullity theorem, rank($A$) = rank($AB$). However when $B$ is invertible, as in the problem we have to tackle, I don't know how to use that fact. $ABx = 0$, but $B$ is in the middle so we can't simply get rid of it to get a meaningful expression. Does someone know how to tackle this?",,"['linear-algebra', 'matrices', 'matrix-rank']"
58,"$\det (e^A) = e^{\text{Tr}(A)}$ without Jordan canonical form, Schur decomposition?","without Jordan canonical form, Schur decomposition?",\det (e^A) = e^{\text{Tr}(A)},"Let $V$ be a finite-dimensional vector space over $\mathbb{C}$ . How do I show that $$\det\,e^A = e^{\text{Tr}\,A}$$ for any $A \in \text{End}\,V$ without invoking Jordan canonical form or Schur decomposition?",Let be a finite-dimensional vector space over . How do I show that for any without invoking Jordan canonical form or Schur decomposition?,"V \mathbb{C} \det\,e^A = e^{\text{Tr}\,A} A \in \text{End}\,V","['linear-algebra', 'matrices']"
59,"How prove $A^2=0$,if $AB-BA=A$","How prove ,if",A^2=0 AB-BA=A,"let $A_{2\times 2}$ matrix, and  The matrix $B$ is  order  square,such   $$AB-BA=A$$ show that   $$A^2=0$$ My idea: since $$Tr(AB)=Tr(BA)$$ so $$Tr(A)=Tr(AB-BA)=Tr(AB)-Tr(BA)=0$$ Question:2 if $A_{n\times n}$ matrix,and the matrix $B$ is order square,such $$AB-BA=A$$ then we also have $$A^2=0?$$ and then I can't Continue .Thank you","let $A_{2\times 2}$ matrix, and  The matrix $B$ is  order  square,such   $$AB-BA=A$$ show that   $$A^2=0$$ My idea: since $$Tr(AB)=Tr(BA)$$ so $$Tr(A)=Tr(AB-BA)=Tr(AB)-Tr(BA)=0$$ Question:2 if $A_{n\times n}$ matrix,and the matrix $B$ is order square,such $$AB-BA=A$$ then we also have $$A^2=0?$$ and then I can't Continue .Thank you",,['linear-algebra']
60,Determine the value of a second determinant based on the first,Determine the value of a second determinant based on the first,,"I know the theory of determinants, but I have no idea how to apply it to this problem. Suppose $$\det\begin{bmatrix}a&b&c\\ d&e&f\\ g&h&i \end{bmatrix} = 6$$ What is the value of $$\det\begin{bmatrix}g - 2a&h - 2b&i - 2c\\ 3a&3b&3c\\2d&2e&2f \end{bmatrix}$$ The options for the answers are: 4 36 24 -24","I know the theory of determinants, but I have no idea how to apply it to this problem. Suppose $$\det\begin{bmatrix}a&b&c\\ d&e&f\\ g&h&i \end{bmatrix} = 6$$ What is the value of $$\det\begin{bmatrix}g - 2a&h - 2b&i - 2c\\ 3a&3b&3c\\2d&2e&2f \end{bmatrix}$$ The options for the answers are: 4 36 24 -24",,"['linear-algebra', 'matrices', 'determinant']"
61,Finding matrix $B$ is not zero matrix where $AB= 0$,Finding matrix  is not zero matrix where,B AB= 0,"$A$ is defined as an $m\times m$ matrix which is not invertible. How can i show that there is an $m\times m$ matrix $B$ where $AB = 0$ but $B$ is not equal to $0$? For the solution of this question I think giving an example is not enough because it is too easy to solve this by giving an example, so how can I show that $B$ is not the $0$ matrix?","$A$ is defined as an $m\times m$ matrix which is not invertible. How can i show that there is an $m\times m$ matrix $B$ where $AB = 0$ but $B$ is not equal to $0$? For the solution of this question I think giving an example is not enough because it is too easy to solve this by giving an example, so how can I show that $B$ is not the $0$ matrix?",,"['linear-algebra', 'matrices']"
62,Why isn't 'intuitive' vector multiplication useful? [duplicate],Why isn't 'intuitive' vector multiplication useful? [duplicate],,"This question already has answers here : why don't we define vector multiplication component-wise? (6 answers) Closed 6 years ago . In Axler's Linear Algebra Done Right , in the introduction to vectors it says: We could define a multiplication on $\mathbf{F}^n$ in a similar fashion [to addition], starting with two elements in $\mathbf{F}^n$ and getting another element of $\mathbf{F}^n$ by multiplying corresponding coordinates. Experience shows us that this definition is not useful for our purposes. In what ways was this definition tested? And how did it fail? Is there some fundamental reason why this definition of multiplication is bad for a vector space? Are there any examples of successful application this definition of multiplication?","This question already has answers here : why don't we define vector multiplication component-wise? (6 answers) Closed 6 years ago . In Axler's Linear Algebra Done Right , in the introduction to vectors it says: We could define a multiplication on $\mathbf{F}^n$ in a similar fashion [to addition], starting with two elements in $\mathbf{F}^n$ and getting another element of $\mathbf{F}^n$ by multiplying corresponding coordinates. Experience shows us that this definition is not useful for our purposes. In what ways was this definition tested? And how did it fail? Is there some fundamental reason why this definition of multiplication is bad for a vector space? Are there any examples of successful application this definition of multiplication?",,"['linear-algebra', 'vector-spaces']"
63,What is the maximum value of $x^T Ax$?,What is the maximum value of ?,x^T Ax,"Let $$A = \begin{bmatrix}3 &1 \\  1&2\end{bmatrix}$$ What is the maximum value of $x^T Ax$ where the maximum is taken over all $x$ that are the unit eigenvectors of $A$ ? $5$ $\frac{(5 + \sqrt{5})}{2}$ $3$ $\frac{(5 - \sqrt{5})}{2}$ The eigenvalues of $A$ are $\frac{5 \pm \sqrt{5}}{2}$ . What is $x^T Ax$ ? Can you explain a little bit, please?","Let What is the maximum value of where the maximum is taken over all that are the unit eigenvectors of ? The eigenvalues of are . What is ? Can you explain a little bit, please?",A = \begin{bmatrix}3 &1 \\  1&2\end{bmatrix} x^T Ax x A 5 \frac{(5 + \sqrt{5})}{2} 3 \frac{(5 - \sqrt{5})}{2} A \frac{5 \pm \sqrt{5}}{2} x^T Ax,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'quadratic-forms']"
64,First Course in Linear Algebra book suggestions? [duplicate],First Course in Linear Algebra book suggestions? [duplicate],,This question already has answers here : Where to start learning Linear Algebra? [closed] (16 answers) Closed 11 years ago . I'm starting maths degree this September and hoping to do some reading on Linear Algebra before I start. What are some good introductory books? Is there something that starts from the very beginning but also covers stuff in depth? (Is there a book that will be useful for both 1st year and 2nd year linear algebra modules?) It would also be great if the book has lots of exercises (proof questions!) and solutions given as well. Thanks in advance.,This question already has answers here : Where to start learning Linear Algebra? [closed] (16 answers) Closed 11 years ago . I'm starting maths degree this September and hoping to do some reading on Linear Algebra before I start. What are some good introductory books? Is there something that starts from the very beginning but also covers stuff in depth? (Is there a book that will be useful for both 1st year and 2nd year linear algebra modules?) It would also be great if the book has lots of exercises (proof questions!) and solutions given as well. Thanks in advance.,,"['linear-algebra', 'reference-request']"
65,Is $\mathbb{Z}(p^{\infty})$ a vector space over some field $\mathbb{F}$?,Is  a vector space over some field ?,\mathbb{Z}(p^{\infty}) \mathbb{F},"I don't know how to write in good English, so I will follow Hungerford's word from his book Algebra . The following relation on the additive group $\mathbb{Q}$ of rational numbers is a congruence relation $a\sim b \leftrightarrow a-b\in \mathbb{Z}$. Denote for $\mathbb{Q}/\mathbb{Z}$  the set of all those equivalence classes. Let $p$ be a prime and $$\mathbb{Z}(p^{\infty})=\{\overline{a/b}\in \mathbb{Q}/\mathbb{Z}\;|\; a,b \in \mathbb{Z}\text{ and }b=p^{i}\text{ for some } i\geq 0\}.$$ I am trying to define a vector space structure on $\mathbb{Z}(p^{\infty})$ over a field $\mathbb{F}$, but I think it is impossible. For example, if $\mathbb{F}$ is uncountable, it can't be done. But I don't know to answer this question when $\mathbb{F}=\mathbb{Q}$ or $\mathrm{char}(\mathbb{F})=q$, where $q$ is a prime number. Thanks for your help!","I don't know how to write in good English, so I will follow Hungerford's word from his book Algebra . The following relation on the additive group $\mathbb{Q}$ of rational numbers is a congruence relation $a\sim b \leftrightarrow a-b\in \mathbb{Z}$. Denote for $\mathbb{Q}/\mathbb{Z}$  the set of all those equivalence classes. Let $p$ be a prime and $$\mathbb{Z}(p^{\infty})=\{\overline{a/b}\in \mathbb{Q}/\mathbb{Z}\;|\; a,b \in \mathbb{Z}\text{ and }b=p^{i}\text{ for some } i\geq 0\}.$$ I am trying to define a vector space structure on $\mathbb{Z}(p^{\infty})$ over a field $\mathbb{F}$, but I think it is impossible. For example, if $\mathbb{F}$ is uncountable, it can't be done. But I don't know to answer this question when $\mathbb{F}=\mathbb{Q}$ or $\mathrm{char}(\mathbb{F})=q$, where $q$ is a prime number. Thanks for your help!",,['linear-algebra']
66,Basis for a Subspace of Polynomials of Degree 5,Basis for a Subspace of Polynomials of Degree 5,,"I have the following question: ""Consider the subspace $W$ of $P_5(\mathbb{R})$ (the set of polynomials of at most degree five) given by $$W=\{p(x)\in P_5(\mathbb{R})|p(1)=p(-1)=0\}.$$ Find a basis for $W$, and compute its dimension."" I have been working on this for a bit, and I know that the standard basis for $P_5(\mathbb{R})$ won't work, so I came up with $$\{x^5-x^3,x^4-2x^2+1,x^3-x,x^2-1\}.$$ However, I'm  afraid that the set doesn't span $P_5(\mathbb{R})$, but I think that it's linearly independent.  Once I have all this, I can easily find the dimension.  Thank you in advance.","I have the following question: ""Consider the subspace $W$ of $P_5(\mathbb{R})$ (the set of polynomials of at most degree five) given by $$W=\{p(x)\in P_5(\mathbb{R})|p(1)=p(-1)=0\}.$$ Find a basis for $W$, and compute its dimension."" I have been working on this for a bit, and I know that the standard basis for $P_5(\mathbb{R})$ won't work, so I came up with $$\{x^5-x^3,x^4-2x^2+1,x^3-x,x^2-1\}.$$ However, I'm  afraid that the set doesn't span $P_5(\mathbb{R})$, but I think that it's linearly independent.  Once I have all this, I can easily find the dimension.  Thank you in advance.",,"['linear-algebra', 'polynomials', 'vector-spaces']"
67,In terms of functions why am I allowed to separate $f$ and $g$ from $x$?,In terms of functions why am I allowed to separate  and  from ?,f g x,"While studying Linear Algebra I came across a function notation that looked like this: $(f + g)(x)$. I also saw that this was the same thing as saying $f(x) + g(x)$. I understand the function notation $f(x)$ and $g(x)$ (which are literally functions) but I do not understand the notation $(f + g)(x)$. Could anyone please fill me in with some introductory info of what this notation means? My guess is that it is a composite function. However I saw that a composite function does not have a plus but a circle in it so I may be wrong. Also from an algebraic perspective, it sort of looks like you can apply the distributive law on $(f + g)(x)$ to make it into $f(x) + g(x)$. That sounds a bit strange to me because notation wise it doesn't make sense. I have heard that this means $(f + g)$ ""evaluated at $x$"", which I don't understand. I would greatly appreciate it if someone gave an overview of the $(f + g)(x)$ notation. Thank you in advance everyone.","While studying Linear Algebra I came across a function notation that looked like this: $(f + g)(x)$. I also saw that this was the same thing as saying $f(x) + g(x)$. I understand the function notation $f(x)$ and $g(x)$ (which are literally functions) but I do not understand the notation $(f + g)(x)$. Could anyone please fill me in with some introductory info of what this notation means? My guess is that it is a composite function. However I saw that a composite function does not have a plus but a circle in it so I may be wrong. Also from an algebraic perspective, it sort of looks like you can apply the distributive law on $(f + g)(x)$ to make it into $f(x) + g(x)$. That sounds a bit strange to me because notation wise it doesn't make sense. I have heard that this means $(f + g)$ ""evaluated at $x$"", which I don't understand. I would greatly appreciate it if someone gave an overview of the $(f + g)(x)$ notation. Thank you in advance everyone.",,"['linear-algebra', 'functions']"
68,Circulant determinants,Circulant determinants,,"Suppose that $a_1,a_2,\ldots,a_n$ are $n$ distinct real numbers; is the following statement true? There is a permutation of $a_1,a_2,\ldots,a_n$, namely $b_1,b_2,\ldots,b_n$, such that the determinant of the following matrix is nonzero: $$ \begin{bmatrix} b_1&b_2&\cdots&b_n\\ b_2&b_3&\cdots&b_1\\ \vdots&\vdots&\ddots&\vdots\\ b_n&b_1&\cdots&b_{n-1}\\ \end{bmatrix} $$ (Such a matrix is called a circulant matrix .)","Suppose that $a_1,a_2,\ldots,a_n$ are $n$ distinct real numbers; is the following statement true? There is a permutation of $a_1,a_2,\ldots,a_n$, namely $b_1,b_2,\ldots,b_n$, such that the determinant of the following matrix is nonzero: $$ \begin{bmatrix} b_1&b_2&\cdots&b_n\\ b_2&b_3&\cdots&b_1\\ \vdots&\vdots&\ddots&\vdots\\ b_n&b_1&\cdots&b_{n-1}\\ \end{bmatrix} $$ (Such a matrix is called a circulant matrix .)",,"['linear-algebra', 'matrices', 'determinant', 'circulant-matrices']"
69,Derivative of inner product,Derivative of inner product,,"If the inner product of some vector $\mathbf{x}$ can be expressed as $$\langle \mathbf{x}, \mathbf{x}\rangle_G = \mathbf{x}^T G\mathbf{x}$$ where $G$ is some symmetric matrix, if I want the derivative of this inner product with respect to $\mathbf{x}$ , I should get a vector as a result since this is the derivative of a scalar function by a vector ( https://en.wikipedia.org/wiki/Matrix_calculus#Scalar-by-vector ). Nevertheless, this formula tells me that I should get a row-vector, and not a normal vector. $$\frac{\mathrm{d}}{\mathrm{d} \mathbf{x}} (\mathbf{x}^TG\mathbf{x}) = 2\mathbf{x}^T G$$ ( http://www.cs.huji.ac.il/~csip/tirgul3_derivatives.pdf ) which is a row-vector. Why do I get this contradiction?","If the inner product of some vector can be expressed as where is some symmetric matrix, if I want the derivative of this inner product with respect to , I should get a vector as a result since this is the derivative of a scalar function by a vector ( https://en.wikipedia.org/wiki/Matrix_calculus#Scalar-by-vector ). Nevertheless, this formula tells me that I should get a row-vector, and not a normal vector. ( http://www.cs.huji.ac.il/~csip/tirgul3_derivatives.pdf ) which is a row-vector. Why do I get this contradiction?","\mathbf{x} \langle \mathbf{x}, \mathbf{x}\rangle_G = \mathbf{x}^T G\mathbf{x} G \mathbf{x} \frac{\mathrm{d}}{\mathrm{d} \mathbf{x}} (\mathbf{x}^TG\mathbf{x}) = 2\mathbf{x}^T G","['linear-algebra', 'derivatives', 'vectors', 'inner-products']"
70,Proof verification for Identity matrices,Proof verification for Identity matrices,,"So I have the following question: Analyze the following 'Claim' (which may or may not be true) and the corresponding 'Proof', by writing 'TRUE' or 'FALSE' (together with the reason) for each step. [Note: $I_n$ is the $n \times n$ identity matrix.] Claim: Let $A$ be any $n \times n$ matrix satisfying $A^2=I_n$ . Then either $A=I_n$ or $A=-I_n$ . 'Proof'. Step 1: $A$ satisfies $A^2-I_n = 0$ (True or False) True. My reasoning: Clearly, this is true. $A^2=I_n$ is not always true, but because it is true, I should have no problem moving the Identity matrix the the LHS. Step 2: So $(A+I_n)(A-I_n)=0$ (True or false) True. My reasoning: Because $I_n$ is the identity matrix, there should be no issues with factoring just like normal algebra. Step 3: $A+I_n=0$ or $A-I_n=0$ I'm not sure about this part. I'm very tempted to say this is fine but I am not sure how I can justify this, if I even can. Therefore $A=-I_n$ or $A=I_n$ . (End of 'Proof'.) Is what I am doing right so far or am I messing up somewhere?","So I have the following question: Analyze the following 'Claim' (which may or may not be true) and the corresponding 'Proof', by writing 'TRUE' or 'FALSE' (together with the reason) for each step. [Note: is the identity matrix.] Claim: Let be any matrix satisfying . Then either or . 'Proof'. Step 1: satisfies (True or False) True. My reasoning: Clearly, this is true. is not always true, but because it is true, I should have no problem moving the Identity matrix the the LHS. Step 2: So (True or false) True. My reasoning: Because is the identity matrix, there should be no issues with factoring just like normal algebra. Step 3: or I'm not sure about this part. I'm very tempted to say this is fine but I am not sure how I can justify this, if I even can. Therefore or . (End of 'Proof'.) Is what I am doing right so far or am I messing up somewhere?",I_n n \times n A n \times n A^2=I_n A=I_n A=-I_n A A^2-I_n = 0 A^2=I_n (A+I_n)(A-I_n)=0 I_n A+I_n=0 A-I_n=0 A=-I_n A=I_n,"['linear-algebra', 'matrices', 'proof-verification']"
71,Supplementary Linear Algebra textbook,Supplementary Linear Algebra textbook,,"We are using Elementary Linear Algebra by Howard Anton in the class and I’m not happy with it. At times there is many pages of writing, yet, there is very little information contained. I really like vector spaces but to find appreciation of them I have to scavenge online sources. What would be a good supplement, maybe substitution? I don’t have knowledge beyond Calculus 3, and I really want to learn how to understand and write proofs. At the same time, I’d like to be able to follow at least half of the time what the author is trying to convey.","We are using Elementary Linear Algebra by Howard Anton in the class and I’m not happy with it. At times there is many pages of writing, yet, there is very little information contained. I really like vector spaces but to find appreciation of them I have to scavenge online sources. What would be a good supplement, maybe substitution? I don’t have knowledge beyond Calculus 3, and I really want to learn how to understand and write proofs. At the same time, I’d like to be able to follow at least half of the time what the author is trying to convey.",,"['linear-algebra', 'reference-request', 'book-recommendation']"
72,Prove diagonal entries of positive definite matrices cannot be smaller than the eigenvalues,Prove diagonal entries of positive definite matrices cannot be smaller than the eigenvalues,,"The aim is to prove that the diagonal entries of a positive definite matrix cannot be smaller than any of the eigenvalues. I know a positive definite matrix must have eigenvalues that are > 0, and that just because a matrix has all positive values, does not make it a positive definite matrix.  I've also looked at the wikipedia for Positive-definite matrices and understand the definition given there, but am having a hard time convincing myself that the diagonal entries have to be greater than the eigenvalues. The starting point of the proof should be to consider $A−a_{ii}I$, where $A=A^T$, and A is the positive definite matrix. Can anyone help push me in the right direction to complete the proof?","The aim is to prove that the diagonal entries of a positive definite matrix cannot be smaller than any of the eigenvalues. I know a positive definite matrix must have eigenvalues that are > 0, and that just because a matrix has all positive values, does not make it a positive definite matrix.  I've also looked at the wikipedia for Positive-definite matrices and understand the definition given there, but am having a hard time convincing myself that the diagonal entries have to be greater than the eigenvalues. The starting point of the proof should be to consider $A−a_{ii}I$, where $A=A^T$, and A is the positive definite matrix. Can anyone help push me in the right direction to complete the proof?",,"['linear-algebra', 'matrices', 'positive-definite']"
73,What would the determinant of the following matrix be?,What would the determinant of the following matrix be?,,""" If $$\det\begin{pmatrix}a&1&d\\ b&1&e\\ c&1&f\end{pmatrix}=4$$ and $$\det \begin{pmatrix}a&1&d\\ b&2&e\\ c&3&f\end{pmatrix}=3$$ What is $$\det \begin{pmatrix}a&-1&d\\ b&-3&e\\ c&-5&f\end{pmatrix}?$$"" This does not seem to fit into any of the regular changes in the values of matrices. A row does not seem to be multiplied. It does not seem like a row is being added to another. So how would I find the required determinant?",""" If $$\det\begin{pmatrix}a&1&d\\ b&1&e\\ c&1&f\end{pmatrix}=4$$ and $$\det \begin{pmatrix}a&1&d\\ b&2&e\\ c&3&f\end{pmatrix}=3$$ What is $$\det \begin{pmatrix}a&-1&d\\ b&-3&e\\ c&-5&f\end{pmatrix}?$$"" This does not seem to fit into any of the regular changes in the values of matrices. A row does not seem to be multiplied. It does not seem like a row is being added to another. So how would I find the required determinant?",,"['linear-algebra', 'matrices', 'determinant']"
74,Alternative definition for tensor product which nowhere occurs (to my knowledge),Alternative definition for tensor product which nowhere occurs (to my knowledge),,"Every vector space $V$ could be embedded into $V^{\ast}$ (see here ) after choosing a basis, for a given vector $v \in V$ denote this embedding by $v^{\ast}\in V^{\ast}$. Now for given vector spaces $V_1, \ldots, V_k$ over some field $F$, let $V = \{ \varphi : V_1 \times \ldots \times V_k \to F \mbox{ multilinear } \}$. Why not define the tensor product  of $V_1, \ldots, V_k$ simply as $T = \{ \varphi^{\ast} \mid \varphi\in V\}$. Then the universal property is obviously fulfilled, for if we define $\pi : V_1 \times \ldots \times V_k \to T$ by $\pi(v_1, \ldots, v_k) = \Phi \in V^{\ast}$ with $$  \Phi(\varphi) = \varphi(v_1, \ldots, v_k). $$ Then if we have some multilinear $\varphi : V_1 \times \ldots \times V_k \to F$ define the linear map $h_{\varphi} : T \to F$ by $$  h_{\varphi}(\Phi) = \Phi(\varphi) $$ and we have $$  h_{\varphi}(\pi(v_1, \ldots, v_k)) = \varphi(v_1, \ldots, v_k) $$ i.e. it factors through $T$ by $\pi$ and $h_{\varphi}$. Then everything works out quite easily, no nasty ""quotient constructions"", it even appears too simple for me... I have nowhere seen this definition? So why not define it that way? Have I overlooked something? Note that we do not rely on reflexivity here, as $T$ does not has to be all of $V^{\ast}$, but just those elements that arise from elements of $V$ (the image of the embedding). Maybe the universal property breaks down because the linear map is not unique, but I do not see other choices for it?","Every vector space $V$ could be embedded into $V^{\ast}$ (see here ) after choosing a basis, for a given vector $v \in V$ denote this embedding by $v^{\ast}\in V^{\ast}$. Now for given vector spaces $V_1, \ldots, V_k$ over some field $F$, let $V = \{ \varphi : V_1 \times \ldots \times V_k \to F \mbox{ multilinear } \}$. Why not define the tensor product  of $V_1, \ldots, V_k$ simply as $T = \{ \varphi^{\ast} \mid \varphi\in V\}$. Then the universal property is obviously fulfilled, for if we define $\pi : V_1 \times \ldots \times V_k \to T$ by $\pi(v_1, \ldots, v_k) = \Phi \in V^{\ast}$ with $$  \Phi(\varphi) = \varphi(v_1, \ldots, v_k). $$ Then if we have some multilinear $\varphi : V_1 \times \ldots \times V_k \to F$ define the linear map $h_{\varphi} : T \to F$ by $$  h_{\varphi}(\Phi) = \Phi(\varphi) $$ and we have $$  h_{\varphi}(\pi(v_1, \ldots, v_k)) = \varphi(v_1, \ldots, v_k) $$ i.e. it factors through $T$ by $\pi$ and $h_{\varphi}$. Then everything works out quite easily, no nasty ""quotient constructions"", it even appears too simple for me... I have nowhere seen this definition? So why not define it that way? Have I overlooked something? Note that we do not rely on reflexivity here, as $T$ does not has to be all of $V^{\ast}$, but just those elements that arise from elements of $V$ (the image of the embedding). Maybe the universal property breaks down because the linear map is not unique, but I do not see other choices for it?",,"['linear-algebra', 'definition', 'tensor-products', 'tensors', 'multilinear-algebra']"
75,If $A$ and $B$ are two matrices such that $AB=B$ and $BA=A$ then how to show that $A^2+B^2$ equals $A+B$?,If  and  are two matrices such that  and  then how to show that  equals ?,A B AB=B BA=A A^2+B^2 A+B,"If $A$ and $B$ are two matrices such that $AB=B$ and $BA=A$ then  $A^2+B^2$ equals ? (a) $2AB$ (b) $2BA$ (c) $A+B$ (d) $AB$ I tried  $(A+B)^2=A^2+B^2+AB+BA$ or,$A^2+B^2=(A+B)^2-AB-BA$ $=(A+B)^2-A-B$ $ =(A+B)^2-(A+B)=(A+B)(A+B-1)$ How to reach the answer from here?","If $A$ and $B$ are two matrices such that $AB=B$ and $BA=A$ then  $A^2+B^2$ equals ? (a) $2AB$ (b) $2BA$ (c) $A+B$ (d) $AB$ I tried  $(A+B)^2=A^2+B^2+AB+BA$ or,$A^2+B^2=(A+B)^2-AB-BA$ $=(A+B)^2-A-B$ $ =(A+B)^2-(A+B)=(A+B)(A+B-1)$ How to reach the answer from here?",,['linear-algebra']
76,Determinant of a matrix with $t$ in all off-diagonal entries.,Determinant of a matrix with  in all off-diagonal entries.,t,"It seems from playing around with small values of $n$ that $$ \det \left( \begin{array}{ccccc} -1 & t & t & \dots & t\\ t & -1 & t & \dots & t\\ t & t & -1 & \dots & t\\ \vdots & \vdots & \vdots & \ddots & \vdots\\ t  & t & t & \dots& -1 \end{array}\right) = (-1)^{n-1}(t+1)^{n-1}((n-1)t-1) $$ where $n$ is the size of the matrix. How would one approach deriving (or at least proving) this formally? Motivation This came up when someone asked what is the general solution to: $$\frac{a}{b+c}=\frac{b}{c+a}=\frac{c}{a+b},$$ and for non-trivial solutions, the matrix above (with $n=3$) must be singular. In this case either $t=-1\implies a+b+c=1$ or $t=\frac{1}{2}\implies a=b=c$. So I wanted to ensure that these are also the only solutions for the case with more variables.","It seems from playing around with small values of $n$ that $$ \det \left( \begin{array}{ccccc} -1 & t & t & \dots & t\\ t & -1 & t & \dots & t\\ t & t & -1 & \dots & t\\ \vdots & \vdots & \vdots & \ddots & \vdots\\ t  & t & t & \dots& -1 \end{array}\right) = (-1)^{n-1}(t+1)^{n-1}((n-1)t-1) $$ where $n$ is the size of the matrix. How would one approach deriving (or at least proving) this formally? Motivation This came up when someone asked what is the general solution to: $$\frac{a}{b+c}=\frac{b}{c+a}=\frac{c}{a+b},$$ and for non-trivial solutions, the matrix above (with $n=3$) must be singular. In this case either $t=-1\implies a+b+c=1$ or $t=\frac{1}{2}\implies a=b=c$. So I wanted to ensure that these are also the only solutions for the case with more variables.",,"['linear-algebra', 'matrices', 'polynomials', 'determinant']"
77,How to denote the opposite case of the Kronecker Delta?,How to denote the opposite case of the Kronecker Delta?,,"The Kronecker delta is defined as link to wikipedia :  $$\delta_{l,m} = \begin{cases} 1 & \text{if }m=l,\\ 0 & \text{if }m\neq l. \end{cases}$$ I would like to denote the case where: $$ = \begin{cases} 0 & \text{if }m=l,\\ 1 & \text{if }m\neq l. \end{cases}$$ How should this be done?","The Kronecker delta is defined as link to wikipedia :  $$\delta_{l,m} = \begin{cases} 1 & \text{if }m=l,\\ 0 & \text{if }m\neq l. \end{cases}$$ I would like to denote the case where: $$ = \begin{cases} 0 & \text{if }m=l,\\ 1 & \text{if }m\neq l. \end{cases}$$ How should this be done?",,"['linear-algebra', 'numerical-methods', 'power-series']"
78,"Let $N$ be a $2× 2$ complex matrix such that $N^2=0$. how could I show $N=0$, or $N$ is similar over the matrix.","Let  be a  complex matrix such that . how could I show , or  is similar over the matrix.",N 2× 2 N^2=0 N=0 N,"Let $N$ be a $2× 2$ complex matrix such that $N^2=0$. how could I show $N=0$, or $N$ is similar over $\mathbb{C}$ to \begin{bmatrix}0 & 0\\1 & 0\end{bmatrix}","Let $N$ be a $2× 2$ complex matrix such that $N^2=0$. how could I show $N=0$, or $N$ is similar over $\mathbb{C}$ to \begin{bmatrix}0 & 0\\1 & 0\end{bmatrix}",,"['linear-algebra', 'complex-analysis']"
79,Are groups and rings more difficult algebraic structures to understand than vector spaces? [closed],Are groups and rings more difficult algebraic structures to understand than vector spaces? [closed],,"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 3 years ago . Improve this question I have read multiple posts on here and in other places where most people seem to recommend to learning linear algebra before abstract algebra. Is that because vector spaces are simpler to understand than groups and rings? I am having some challenges with understanding how certain aspects of vector spaces work, I was wondering if learning about rings and/or groups can help me better understand how vector spaces work?","Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 3 years ago . Improve this question I have read multiple posts on here and in other places where most people seem to recommend to learning linear algebra before abstract algebra. Is that because vector spaces are simpler to understand than groups and rings? I am having some challenges with understanding how certain aspects of vector spaces work, I was wondering if learning about rings and/or groups can help me better understand how vector spaces work?",,"['linear-algebra', 'abstract-algebra', 'group-theory', 'vector-spaces', 'soft-question']"
80,If and when a linear system has exactly three solutions,If and when a linear system has exactly three solutions,,"Does the following system have exactly three solutions?   $$\left\{ \begin{array}{l} 2x - y +3z = 1 \\ x + 4y - 2z = -7 \\ 3x + y -z = 4 \\ \end{array} \right.$$ I marked this answer as True.  I proceeded to row reduce it and obtained the resultant matrix as - $$\left[ \begin{array}{ccc|c} 1 & -1/2 & 3/2 & 4 \\ 0 & 1 & 9 & 13/9 \\ 0 & 0 & 1 & 10/28 \\ \end{array} \right]$$ I'm pretty sure I made some mistake while row-reducing it. I answered this question in an exam setting. Then I gave the explanation as follows - Form row reduction, we know that the system is consistent and the rank of the matrix is 3 which is equal to the number of variables. So, the system of equations has 3 distinct solutions. Can someone point out where I went wrong?","Does the following system have exactly three solutions?   $$\left\{ \begin{array}{l} 2x - y +3z = 1 \\ x + 4y - 2z = -7 \\ 3x + y -z = 4 \\ \end{array} \right.$$ I marked this answer as True.  I proceeded to row reduce it and obtained the resultant matrix as - $$\left[ \begin{array}{ccc|c} 1 & -1/2 & 3/2 & 4 \\ 0 & 1 & 9 & 13/9 \\ 0 & 0 & 1 & 10/28 \\ \end{array} \right]$$ I'm pretty sure I made some mistake while row-reducing it. I answered this question in an exam setting. Then I gave the explanation as follows - Form row reduction, we know that the system is consistent and the rank of the matrix is 3 which is equal to the number of variables. So, the system of equations has 3 distinct solutions. Can someone point out where I went wrong?",,"['linear-algebra', 'systems-of-equations']"
81,Error in the book? Or wrong logic?,Error in the book? Or wrong logic?,,"In the book ""Elementary linear algebra with supplemental applications"", 11th edition, page 81,task 127, the following task appears: Show that if $A$ is a square matrix such that $A^K=0$ for some positive integer $K$, then the matrix $A$ is invertible and the inverse of the matrix $(I-A)$ is equal to $I + A + A^2 + ... + A^{K-1}$. I have no difficulty in arriving at the identity given that $A^K = 0$, however I think there is something wrong here... Consider the following: Let $A$ be a square matrix not equal to 0 , such that $A^K=0$ for some nonnegative integer $K \ge 2$, and assume for contradiction that $A^{-1}$ exists. Then we can deduce the following: $$A^K=0 \implies A^K A^{-1}=0 \implies A^{K-1} A A^{-1} = 0 \implies A^{K-1} I = 0$$ and we are left with $A^{K-1} = 0$. Thus, $A^K=0 \implies A^{K-1} = 0$, and using the same logic (multiplying by $A^{-1}$), we deduce $A^K=0 \implies A^{K-1} = 0 \implies \cdots \implies A^2 = 0 \implies A = 0$, a contradiction. Thus, the three statements ""$A$ does not equal $0$"", ""$A^{-1}$ exists"" and ""$A^K=0$ for some nonnegative integer $K$ greater than or equal to $2$"" cannot all be true at the same time. And thus the task in the book is inncorrect since it states that $A^{-1}$ exists. Is the book wrong, or is there something wrong with my use of logic here? :)","In the book ""Elementary linear algebra with supplemental applications"", 11th edition, page 81,task 127, the following task appears: Show that if $A$ is a square matrix such that $A^K=0$ for some positive integer $K$, then the matrix $A$ is invertible and the inverse of the matrix $(I-A)$ is equal to $I + A + A^2 + ... + A^{K-1}$. I have no difficulty in arriving at the identity given that $A^K = 0$, however I think there is something wrong here... Consider the following: Let $A$ be a square matrix not equal to 0 , such that $A^K=0$ for some nonnegative integer $K \ge 2$, and assume for contradiction that $A^{-1}$ exists. Then we can deduce the following: $$A^K=0 \implies A^K A^{-1}=0 \implies A^{K-1} A A^{-1} = 0 \implies A^{K-1} I = 0$$ and we are left with $A^{K-1} = 0$. Thus, $A^K=0 \implies A^{K-1} = 0$, and using the same logic (multiplying by $A^{-1}$), we deduce $A^K=0 \implies A^{K-1} = 0 \implies \cdots \implies A^2 = 0 \implies A = 0$, a contradiction. Thus, the three statements ""$A$ does not equal $0$"", ""$A^{-1}$ exists"" and ""$A^K=0$ for some nonnegative integer $K$ greater than or equal to $2$"" cannot all be true at the same time. And thus the task in the book is inncorrect since it states that $A^{-1}$ exists. Is the book wrong, or is there something wrong with my use of logic here? :)",,['linear-algebra']
82,"If $A$ is a symmetric matrix, then $A^2$ is also symmetric","If  is a symmetric matrix, then  is also symmetric",A A^2,"I first tried if the claim was true by testing it with a symmetric matrix, and I got that if I have a symmetric square matrix $A$ then $A^2$ is also symmetric. So to prove this for a general case I did: First of all I take a general square matrix $$A=   \left[ {\begin{array}{ccc}    a_{11} & a_{12} & ...a_{1n}\\    a_{21} & a_{12} & ...a_{1n}\\    ... & ... & ...\\    a_{n1} & a_{n2} & ...a_{nn}\\   \end{array} } \right]$$ we can see that the matrix above is symmetric because it is equal to its transpose. Then I calculate $$A^2=   \left[ {\begin{array}{ccc}    \sum\limits_{k=1}^n a_{1k}a_{k1} & \sum\limits_{k=1}^n a_{1k}a_{k2} & ...\sum\limits_{k=1}^n a_{1k}a_{kn}\\    \sum\limits_{k=1}^n a_{2k}a_{k1} & \sum\limits_{k=1}^n a_{2k}a_{k2} & ...\sum\limits_{k=1}^n a_{2k}a_{kn}\\    ... & ... & ...\\    \sum\limits_{k=1}^n a_{nk}a_{k1} & \sum\limits_{k=1}^n a_{nk}a_{k2} & ...\sum\limits_{k=1}^n a_{nk}a_{kn}\\   \end{array} } \right]$$ so I get that $A^2$ is symmetric because it is equal to its transpose $(A^2)^T$ or we can say that because $a_{ij}a_{ji}=a_{ji}a_{ij}$ for all $1\le i,j\le n$ . Do you think this is a good proof or how can I improve it? Thanks","I first tried if the claim was true by testing it with a symmetric matrix, and I got that if I have a symmetric square matrix then is also symmetric. So to prove this for a general case I did: First of all I take a general square matrix we can see that the matrix above is symmetric because it is equal to its transpose. Then I calculate so I get that is symmetric because it is equal to its transpose or we can say that because for all . Do you think this is a good proof or how can I improve it? Thanks","A A^2 A=
  \left[ {\begin{array}{ccc}
   a_{11} & a_{12} & ...a_{1n}\\
   a_{21} & a_{12} & ...a_{1n}\\
   ... & ... & ...\\
   a_{n1} & a_{n2} & ...a_{nn}\\
  \end{array} } \right] A^2=
  \left[ {\begin{array}{ccc}
   \sum\limits_{k=1}^n a_{1k}a_{k1} & \sum\limits_{k=1}^n a_{1k}a_{k2} & ...\sum\limits_{k=1}^n a_{1k}a_{kn}\\
   \sum\limits_{k=1}^n a_{2k}a_{k1} & \sum\limits_{k=1}^n a_{2k}a_{k2} & ...\sum\limits_{k=1}^n a_{2k}a_{kn}\\
   ... & ... & ...\\
   \sum\limits_{k=1}^n a_{nk}a_{k1} & \sum\limits_{k=1}^n a_{nk}a_{k2} & ...\sum\limits_{k=1}^n a_{nk}a_{kn}\\
  \end{array} } \right] A^2 (A^2)^T a_{ij}a_{ji}=a_{ji}a_{ij} 1\le i,j\le n","['linear-algebra', 'abstract-algebra', 'matrices']"
83,How do you know a solution space contains the origin?,How do you know a solution space contains the origin?,,"In a linear system $$x - 2y + 3z = 0 \\ -3x + 7y -8z = 0 \\ -2x + 4y -6z = 0 $$ The solution space is $\{(2s -3t,s,t) \mid s,t \in \mathbb{R}\} = \operatorname{span}\{(2,1,0),(-3,0,1)\}$ How do we know that the plane in $\mathbb{R}^3$ contains the origin?","In a linear system $$x - 2y + 3z = 0 \\ -3x + 7y -8z = 0 \\ -2x + 4y -6z = 0 $$ The solution space is $\{(2s -3t,s,t) \mid s,t \in \mathbb{R}\} = \operatorname{span}\{(2,1,0),(-3,0,1)\}$ How do we know that the plane in $\mathbb{R}^3$ contains the origin?",,['linear-algebra']
84,Finding a 2x2 Matrix raised to the power of 1000,Finding a 2x2 Matrix raised to the power of 1000,,Let $A= \pmatrix{1&4\\ 3&2}$. Find $A^{1000}$. Does this problem have to do with eigenvalues or is there another formula that is specific to 2x2 matrices?,Let $A= \pmatrix{1&4\\ 3&2}$. Find $A^{1000}$. Does this problem have to do with eigenvalues or is there another formula that is specific to 2x2 matrices?,,"['linear-algebra', 'matrices', 'diagonalization']"
85,"$A$ and $B$ are $3\times 3$ real matrices such that $\operatorname{rank}(AB)=1$, then $\operatorname{rank}(BA$) can not be which of the following?","and  are  real matrices such that , then ) can not be which of the following?",A B 3\times 3 \operatorname{rank}(AB)=1 \operatorname{rank}(BA,"I was thinking about the problem that says: If $A$ and $B$ are $3\times 3$ real matrices such that $\operatorname{rank}(AB)=1$, then $\operatorname{rank}(BA)$ can not be which of the following? (a) $0$ (b) $1$ (c) $2$ (d) $3$. My attempt: I have chosen suitable $3 \times 3 $ matrices for $A$ and $B$ keeping in mind that $\operatorname{rank}(AB)=1$. Say for example if I take $A$ and $B$ to be  $$A = \begin{pmatrix} 1 &2  &0 \\  0 & 0 &0 \\  0 & 0 &0  \end{pmatrix}$$  and  $$B = \begin{pmatrix} -2 &1  &0 \\  1 & 0 &0 \\  0 & 0 &0  \end{pmatrix}$$ respectively, then I see $\operatorname{rank}(AB) = \operatorname{rank}(BA) = 1$. So, option (b) can not be correct. Do I have to keep choosing the matrices and then observe which of the option holds good. Is this kind of approach right to tackle the problem? I am looking for a direct way (e.g. application to some theorem) which can give me the result. I have also noticed that $AB$ and $BA$ are similar matrices as we see that $A^{-1}(AB)A=BA$. Is this observation going to help me in any way? Thanks in advance for your time.","I was thinking about the problem that says: If $A$ and $B$ are $3\times 3$ real matrices such that $\operatorname{rank}(AB)=1$, then $\operatorname{rank}(BA)$ can not be which of the following? (a) $0$ (b) $1$ (c) $2$ (d) $3$. My attempt: I have chosen suitable $3 \times 3 $ matrices for $A$ and $B$ keeping in mind that $\operatorname{rank}(AB)=1$. Say for example if I take $A$ and $B$ to be  $$A = \begin{pmatrix} 1 &2  &0 \\  0 & 0 &0 \\  0 & 0 &0  \end{pmatrix}$$  and  $$B = \begin{pmatrix} -2 &1  &0 \\  1 & 0 &0 \\  0 & 0 &0  \end{pmatrix}$$ respectively, then I see $\operatorname{rank}(AB) = \operatorname{rank}(BA) = 1$. So, option (b) can not be correct. Do I have to keep choosing the matrices and then observe which of the option holds good. Is this kind of approach right to tackle the problem? I am looking for a direct way (e.g. application to some theorem) which can give me the result. I have also noticed that $AB$ and $BA$ are similar matrices as we see that $A^{-1}(AB)A=BA$. Is this observation going to help me in any way? Thanks in advance for your time.",,"['linear-algebra', 'matrices']"
86,Proving $\operatorname{rank}A =\operatorname{rank}B$ when $AB = 2A + 3B$,Proving  when,\operatorname{rank}A =\operatorname{rank}B AB = 2A + 3B,$A$ and $B$ are two square matrices such that $AB = 2A + 3B$ . Show that $\operatorname{rank}A =\operatorname{rank}B$ . I managed to prove that the matrices $A-3I$ and $B-2I$ are invertible and that $AB=BA$ . Also if $A$ is invertible then $B$ is invertible because otherwise determinat of $2A$ would be $0$ which is false. I don't know what to do when $A$ is not invertible.,and are two square matrices such that . Show that . I managed to prove that the matrices and are invertible and that . Also if is invertible then is invertible because otherwise determinat of would be which is false. I don't know what to do when is not invertible.,A B AB = 2A + 3B \operatorname{rank}A =\operatorname{rank}B A-3I B-2I AB=BA A B 2A 0 A,"['linear-algebra', 'matrices', 'matrix-rank']"
87,What does a having pivot in every row tell us? What about a pivot in every column?,What does a having pivot in every row tell us? What about a pivot in every column?,,"Given a matrix $A_1$ as part of the equation $A\vec{x}=\vec{b}$ : $$ \begin{bmatrix} P & f & f & f\\ 0 & P & f & f\\ 0 & 0 & P & f \end{bmatrix} $$ What do we know based on the fact that there is a pivot in every row ? Given a matrix $A_2$ as part of the equation $A\vec{x}=\vec{b}$ : $$ \begin{bmatrix} P & f & f\\ 0 & P & f\\ 0 & 0 & P\\ 0 & 0 & 0 \end{bmatrix} $$ What do we know based on the fact that there is a pivot in every column ? My understanding is that a pivot in every row (as in $A_1$ ) tells us that the columns of $A_1$ span $\mathbb{R}^m$ . And that a pivot in every column (as in $A_2$ ) tells us that the columns are linearly independent. Are these understandings correct? I'm sure that we know a lot about a matrix given the conditions listed above, but I'm just looking for the most obvious or helpful information.","Given a matrix as part of the equation : What do we know based on the fact that there is a pivot in every row ? Given a matrix as part of the equation : What do we know based on the fact that there is a pivot in every column ? My understanding is that a pivot in every row (as in ) tells us that the columns of span . And that a pivot in every column (as in ) tells us that the columns are linearly independent. Are these understandings correct? I'm sure that we know a lot about a matrix given the conditions listed above, but I'm just looking for the most obvious or helpful information.","A_1 A\vec{x}=\vec{b} 
\begin{bmatrix}
P & f & f & f\\
0 & P & f & f\\
0 & 0 & P & f
\end{bmatrix}
 A_2 A\vec{x}=\vec{b} 
\begin{bmatrix}
P & f & f\\
0 & P & f\\
0 & 0 & P\\
0 & 0 & 0
\end{bmatrix}
 A_1 A_1 \mathbb{R}^m A_2","['linear-algebra', 'matrices']"
88,Product of two consecutive integers.,Product of two consecutive integers.,,"I have a small algebra problem If the sum of two consecutive integers is $x$, what would be their product? I've made equation.$$n+(n+1)=x$$ But, don't understand how to find their sum.","I have a small algebra problem If the sum of two consecutive integers is $x$, what would be their product? I've made equation.$$n+(n+1)=x$$ But, don't understand how to find their sum.",,"['linear-algebra', 'algebra-precalculus']"
89,"Prove that if Ax = b has a solution for every b, then A is invertible","Prove that if Ax = b has a solution for every b, then A is invertible",,"I am interested in the case that $A$ is a matrix over a commutative ring, not necessarily a field. Is it still true that if $Ax = b$ has a solution for every $b$, then $A$ is invertible? I know that in the general setting, $A$ having the trivial nullspace does not imply that it is invertible. However, I cannot seem to find a counterexample to the fact in the title of the question, so I am starting to believe it is true. Any ideas how to prove it?","I am interested in the case that $A$ is a matrix over a commutative ring, not necessarily a field. Is it still true that if $Ax = b$ has a solution for every $b$, then $A$ is invertible? I know that in the general setting, $A$ having the trivial nullspace does not imply that it is invertible. However, I cannot seem to find a counterexample to the fact in the title of the question, so I am starting to believe it is true. Any ideas how to prove it?",,['linear-algebra']
90,"Show that $1, (x-5)^2, (x-5)^3$ is a basis of the subspace $U$ of $\mathcal P_3(\Bbb R)$",Show that  is a basis of the subspace  of,"1, (x-5)^2, (x-5)^3 U \mathcal P_3(\Bbb R)","In Axler's LADR book example 2.41: Show that $1, (x-5)^2, (x-5)^3$ is a basis of the subspace $U$ of $\mathcal P_3(\Bbb R)$ defined by: $$U = \{p \in P_3(\Bbb R):p'(5) = 0\}$$ Book shows that $1, (x-5)^2, (x-5)^3$ is linear independent, hence $\dim U \ge 3$ . It also must be $\dim U \le 4$ . But I dont understand this part: However, $\dim U$ cannot equal $4$ , because otherwise when we extend a basis of $U$ to a basis of $P_3(\Bbb R)$ we would get a list with length greater than 4. Why we will get basis with length greater than $4$ ?","In Axler's LADR book example 2.41: Show that is a basis of the subspace of defined by: Book shows that is linear independent, hence . It also must be . But I dont understand this part: However, cannot equal , because otherwise when we extend a basis of to a basis of we would get a list with length greater than 4. Why we will get basis with length greater than ?","1, (x-5)^2, (x-5)^3 U \mathcal P_3(\Bbb R) U = \{p \in P_3(\Bbb R):p'(5) = 0\} 1, (x-5)^2, (x-5)^3 \dim U \ge 3 \dim U \le 4 \dim U 4 U P_3(\Bbb R) 4","['linear-algebra', 'polynomials']"
91,How to multiply a vector from the left side with matrix?,How to multiply a vector from the left side with matrix?,,"I have always dealt with vector - matrix multiplication where the vector is the right multiplicand, but I am not sure how to apply the product between a matrix and a vector when the vector is the left multiplicand. I have the following example $$\beta = \begin{pmatrix} \beta_0 & \beta_1 \end{pmatrix} \in \mathbb{R}^{1 \times 2}$$ and a general matrix $$A = \begin{pmatrix} a_{11} & a_{12} \\ a_{21} & a_{22}\end{pmatrix} \in \mathbb{R}^{2 \times 2}$$ What would be the algorithm to multiply $\beta \cdot A$? Of course the result is a $1 \times 2$ row vector.","I have always dealt with vector - matrix multiplication where the vector is the right multiplicand, but I am not sure how to apply the product between a matrix and a vector when the vector is the left multiplicand. I have the following example $$\beta = \begin{pmatrix} \beta_0 & \beta_1 \end{pmatrix} \in \mathbb{R}^{1 \times 2}$$ and a general matrix $$A = \begin{pmatrix} a_{11} & a_{12} \\ a_{21} & a_{22}\end{pmatrix} \in \mathbb{R}^{2 \times 2}$$ What would be the algorithm to multiply $\beta \cdot A$? Of course the result is a $1 \times 2$ row vector.",,"['linear-algebra', 'matrices']"
92,Show that a vector that is orthogonal to every other vector is the zero vector,Show that a vector that is orthogonal to every other vector is the zero vector,,"I have the following question, and I'd like to get some tips on how to write the proof. I know why it is, but I'm still not so great at writing it mathematically. If $u$ is a vector in $\mathbb{R}^n$ that is orthogonal to every vector in $\mathbb{R}^n$, then $u$ must be the zero vector. Why? I'm starting off like this, but I don't know if it's the right way to do it, or if it is and I just don't know how to continue. \begin{align} (\exists u\in\mathbb{R}^n)(\forall v\in\mathbb{R}^n)[\text{u is orthogonal to v}]&\iff u\cdot v=0\\ &\iff ? \end{align} From here, instinctively I want to divide both sides by $v$, but I don't know if there is such a thing as dividing a dot product.","I have the following question, and I'd like to get some tips on how to write the proof. I know why it is, but I'm still not so great at writing it mathematically. If $u$ is a vector in $\mathbb{R}^n$ that is orthogonal to every vector in $\mathbb{R}^n$, then $u$ must be the zero vector. Why? I'm starting off like this, but I don't know if it's the right way to do it, or if it is and I just don't know how to continue. \begin{align} (\exists u\in\mathbb{R}^n)(\forall v\in\mathbb{R}^n)[\text{u is orthogonal to v}]&\iff u\cdot v=0\\ &\iff ? \end{align} From here, instinctively I want to divide both sides by $v$, but I don't know if there is such a thing as dividing a dot product.",,"['linear-algebra', 'proof-writing']"
93,What is the significance of the order in an ordered basis/basis?,What is the significance of the order in an ordered basis/basis?,,"Throughout my Linear Algebra course I heard reference to the fact that a set must be ordered in some way to be a basis for a space, but never managed to see the importance of this - what is it? What would the consequences be of re-ordering our basis (apart from having to write matrix/vector representations in the some permuted form)? The motivation of this question is that I've come to a problem where I am to find the dual bases corresponding to a certain bases of some space V. In doing so I've had to take care to order the dual bases carefully with respect to the respective bases of V, which made me wonder where else that the order of a basis becomes significant.","Throughout my Linear Algebra course I heard reference to the fact that a set must be ordered in some way to be a basis for a space, but never managed to see the importance of this - what is it? What would the consequences be of re-ordering our basis (apart from having to write matrix/vector representations in the some permuted form)? The motivation of this question is that I've come to a problem where I am to find the dual bases corresponding to a certain bases of some space V. In doing so I've had to take care to order the dual bases carefully with respect to the respective bases of V, which made me wonder where else that the order of a basis becomes significant.",,['linear-algebra']
94,Eigenvalues of the differentiation operator,Eigenvalues of the differentiation operator,,I have a linear operator $T_1$ which acts on the vector space of polynomials in this way: $$T_1(p(x))=p'(x).$$ How can I find its eigenvalues and how can I know whether it is diagonalizable or not?,I have a linear operator $T_1$ which acts on the vector space of polynomials in this way: $$T_1(p(x))=p'(x).$$ How can I find its eigenvalues and how can I know whether it is diagonalizable or not?,,['linear-algebra']
95,Complex eigenvalues of a matrix in conjugate pairs (or not),Complex eigenvalues of a matrix in conjugate pairs (or not),,"I have learnt that in a matrix, if there are complex eigenvalues, they should come as conjugate pairs. Also, I know that, in a diagonal matrix, eigenvalues are the diagonal elements. So how about the following matrix? $$\begin{pmatrix} i & 0\\   0& 2 \end{pmatrix}$$ Shouldn't the eigenvalues be $i$ and $2$ , where it doesn't have a conjugate pair?! I appreciate your help to clarify my mistake.","I have learnt that in a matrix, if there are complex eigenvalues, they should come as conjugate pairs. Also, I know that, in a diagonal matrix, eigenvalues are the diagonal elements. So how about the following matrix? Shouldn't the eigenvalues be and , where it doesn't have a conjugate pair?! I appreciate your help to clarify my mistake.","\begin{pmatrix}
i & 0\\ 
 0& 2
\end{pmatrix} i 2","['linear-algebra', 'matrices', 'algebra-precalculus', 'complex-numbers', 'eigenvalues-eigenvectors']"
96,Tensor Product with Trivial Vector Space,Tensor Product with Trivial Vector Space,,"This question seems obvious and yet I can't seem to find a good answer anywhere. Let $V$ be a finite dimensional vector space, and let $0$ denote the trivial vector space. Is $V \otimes 0 = 0$ or $V \otimes 0 = V$ ? My gut tells me that it is the second case, but in thinking about dimension, tensor product should multiply dimension in which case I think it is the first case.","This question seems obvious and yet I can't seem to find a good answer anywhere. Let be a finite dimensional vector space, and let denote the trivial vector space. Is or ? My gut tells me that it is the second case, but in thinking about dimension, tensor product should multiply dimension in which case I think it is the first case.",V 0 V \otimes 0 = 0 V \otimes 0 = V,"['linear-algebra', 'vector-spaces', 'tensor-products']"
97,Reducible and Irreducible polynomials are confusing me,Reducible and Irreducible polynomials are confusing me,,"The definition claims that a polynomial in a field of positive degree is a reducible polynomial when it can be written as the product of $2$ polynomials in the field with positive degrees. Other wise it is irreducible. So if a polynomial $f(x)$ can be written as the product of say $41(x^2 + x)$ , is that considered not reducible because $41$ is really $41x^0$ , and $0$ isn't technically positive, but by the definition of a polynomial in a field it is a polynomial if $a_n$ isn't $0$ for the highest degree $n$ where $n \geq 0$ . So would the example of the polynomial example I gave be reducible or irreducible?","The definition claims that a polynomial in a field of positive degree is a reducible polynomial when it can be written as the product of polynomials in the field with positive degrees. Other wise it is irreducible. So if a polynomial can be written as the product of say , is that considered not reducible because is really , and isn't technically positive, but by the definition of a polynomial in a field it is a polynomial if isn't for the highest degree where . So would the example of the polynomial example I gave be reducible or irreducible?",2 f(x) 41(x^2 + x) 41 41x^0 0 a_n 0 n n \geq 0,['linear-algebra']
98,Is $A^T A$ similar to $AA^T$?,Is  similar to ?,A^T A AA^T,"I saw in a proof somewhere that a square matrix $AA^T$ is similar to $A^T A$, so I thought about it and I don't know why (or whether) it's true. I tried using the fact that every matrix is similar to its transpose and maybe transpose the entire expression $AA^T$ but what I get is $(AA^T)^T=A^{T^T} A^T=AA^T$ which is obvious because $AA^T$ is symmetric. I tried to run some examples like  $$ A = \begin{bmatrix}     1       & 4   \\     3       & 2   \end{bmatrix} \qquad \qquad \qquad  A^T = \begin{bmatrix}     1       & 3   \\     4       & 2   \end{bmatrix}  $$ And I get that $AA^T$ and $A^TA$ have the same characteristic polynomial so obviously they have the same trace, eigenvalues and determinant. But is it true for the general case?","I saw in a proof somewhere that a square matrix $AA^T$ is similar to $A^T A$, so I thought about it and I don't know why (or whether) it's true. I tried using the fact that every matrix is similar to its transpose and maybe transpose the entire expression $AA^T$ but what I get is $(AA^T)^T=A^{T^T} A^T=AA^T$ which is obvious because $AA^T$ is symmetric. I tried to run some examples like  $$ A = \begin{bmatrix}     1       & 4   \\     3       & 2   \end{bmatrix} \qquad \qquad \qquad  A^T = \begin{bmatrix}     1       & 3   \\     4       & 2   \end{bmatrix}  $$ And I get that $AA^T$ and $A^TA$ have the same characteristic polynomial so obviously they have the same trace, eigenvalues and determinant. But is it true for the general case?",,"['linear-algebra', 'matrices']"
99,Dual space of exterior power and exterior power of dual space,Dual space of exterior power and exterior power of dual space,,Let $V$ be a finite-dimensional vector space. Is there an isomorphism between $\Lambda^k(V^\ast)$ and $\left(\Lambda^k(V)\right)^\ast$? I was able to prove this with the additional requirement of an inner product on $V$ (and thus subsequently on $\Lambda^k(V)$) via  $$ \require{AMScd} \begin{CD} \left(\Lambda^k(V)\right)^\ast @>\mathcal{J}^{-1}>> \Lambda^k(V) @>\Lambda^kJ>> \Lambda^k(V^\ast)   \end{CD} $$ where $J: V \to V^\ast$ and $\mathcal{J}: \Lambda^k(V) \to \left(\Lambda^k(V)\right)^\ast$ are the isomorphisms given by the Riesz representation theorem and $\Lambda^kJ$ is the map given by $v_1\wedge \cdots \wedge v_k \mapsto J(v_1) \wedge \cdots \wedge J(v_k)$. Is there another way to identify these two spaces without the requirement of an inner product on $V$? I read Qiaochu Yuan's comment to his answer on a similar question but did not really understand it I fear.  Thank you very much.,Let $V$ be a finite-dimensional vector space. Is there an isomorphism between $\Lambda^k(V^\ast)$ and $\left(\Lambda^k(V)\right)^\ast$? I was able to prove this with the additional requirement of an inner product on $V$ (and thus subsequently on $\Lambda^k(V)$) via  $$ \require{AMScd} \begin{CD} \left(\Lambda^k(V)\right)^\ast @>\mathcal{J}^{-1}>> \Lambda^k(V) @>\Lambda^kJ>> \Lambda^k(V^\ast)   \end{CD} $$ where $J: V \to V^\ast$ and $\mathcal{J}: \Lambda^k(V) \to \left(\Lambda^k(V)\right)^\ast$ are the isomorphisms given by the Riesz representation theorem and $\Lambda^kJ$ is the map given by $v_1\wedge \cdots \wedge v_k \mapsto J(v_1) \wedge \cdots \wedge J(v_k)$. Is there another way to identify these two spaces without the requirement of an inner product on $V$? I read Qiaochu Yuan's comment to his answer on a similar question but did not really understand it I fear.  Thank you very much.,,"['linear-algebra', 'multilinear-algebra', 'exterior-algebra']"
