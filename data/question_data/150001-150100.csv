,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,MVT/Rolle's Theorem on twice differentiable function.,MVT/Rolle's Theorem on twice differentiable function.,,"Suppose $ f : \mathbb R\to\mathbb R$ is differentiable two times (so both $f$ and $f′$ are differentiable on $\mathbb R$ ) and $f′′(x)$ > 0 for all $x ∈ \mathbb R$. Show that for any $y ∈ \mathbb R$ there exist at most two distinct values $x_1,x_2 ∈ \mathbb R$ such that $f(x_1) = f(x_2) = y$ I believe you have to use MVT/Rolle's Theorem to try and obtain a contradiction somewhere but I am struggling to figure out how. Help would be very much appreciated.","Suppose $ f : \mathbb R\to\mathbb R$ is differentiable two times (so both $f$ and $f′$ are differentiable on $\mathbb R$ ) and $f′′(x)$ > 0 for all $x ∈ \mathbb R$. Show that for any $y ∈ \mathbb R$ there exist at most two distinct values $x_1,x_2 ∈ \mathbb R$ such that $f(x_1) = f(x_2) = y$ I believe you have to use MVT/Rolle's Theorem to try and obtain a contradiction somewhere but I am struggling to figure out how. Help would be very much appreciated.",,"['real-analysis', 'analysis']"
1,Intuition behind the Banach fixed-point theorem,Intuition behind the Banach fixed-point theorem,,The theorem appeared as an exercise in my real analysis book and only considered functions in $\mathbb{R}$ but the proof of the general theorem seems to be almost identical after looking up the wikipedia article. Is there a somewhat intuitive way of thinking about this theorem? I understand the proof and what it entails but I don't see why the result ought to hold given the necessary conditions. Is there a way of convincing someone that the theorem ought to be true without actually proving it? Some vague geometric intuition would be nice to have a picture in my head of what's going on.,The theorem appeared as an exercise in my real analysis book and only considered functions in $\mathbb{R}$ but the proof of the general theorem seems to be almost identical after looking up the wikipedia article. Is there a somewhat intuitive way of thinking about this theorem? I understand the proof and what it entails but I don't see why the result ought to hold given the necessary conditions. Is there a way of convincing someone that the theorem ought to be true without actually proving it? Some vague geometric intuition would be nice to have a picture in my head of what's going on.,,"['real-analysis', 'sequences-and-series', 'analysis', 'functions', 'fixed-point-theorems']"
2,Another version of mean value theorem,Another version of mean value theorem,,"Let $f$ be a real valued differentiable $n+1$-times on $\Bbb R. $ Show that for each $a<b$ such that, $$\ln\left(\frac{f(b)+f'(b)+\cdots+ f^{(n)}(b)}{f(a)+f'(a)+\cdots+ f^{(n)}(a)}\right)=b-a$$ there exists $c\in (a,b)$ such that $$f^{(n+1)}(c)= f(c)$$ I have the feeling one should apply the classical mean value theorem. But I don't know to what exact function I should apply it. Can someone provide me with a hint?","Let $f$ be a real valued differentiable $n+1$-times on $\Bbb R. $ Show that for each $a<b$ such that, $$\ln\left(\frac{f(b)+f'(b)+\cdots+ f^{(n)}(b)}{f(a)+f'(a)+\cdots+ f^{(n)}(a)}\right)=b-a$$ there exists $c\in (a,b)$ such that $$f^{(n+1)}(c)= f(c)$$ I have the feeling one should apply the classical mean value theorem. But I don't know to what exact function I should apply it. Can someone provide me with a hint?",,"['calculus', 'real-analysis', 'analysis', 'derivatives', 'logarithms']"
3,What is the condition $|f(x)| \le f(|x|)$ called?,What is the condition  called?,|f(x)| \le f(|x|),Suppose I have a function $f: \mathbb R \to \mathbb R$ so that for all $x\in \mathbb R$ it holds that $|f(x)| \le f(|x|).$ Does this condition have a name?,Suppose I have a function $f: \mathbb R \to \mathbb R$ so that for all $x\in \mathbb R$ it holds that $|f(x)| \le f(|x|).$ Does this condition have a name?,,"['real-analysis', 'analysis', 'inequality', 'terminology', 'normed-spaces']"
4,Continuous group homomorphism between normed vector spaces are linear?,Continuous group homomorphism between normed vector spaces are linear?,,"The following is exercise III.1.13 from Analysis I by Amann and Escher. Suppose that $V$ and $W$ are normed vector spaces and $f : V → W$ is a continuous   group homomorphism from $(V, +)$ to $(W, +)$. Prove that $f$ is linear. (Hint: If $\mathbb{K = R}$, $x \in V$ and $q ∈ \mathbb{Q}$, then $f(qx) = qf(x)$. See also Exercise $6$.) Here $\mathbb{K}$ means the base field for $V$ and $W$ and $\mathbb{K=C}$ or $\mathbb{R}$ (which is a convention stated in the text). Now if $\mathbb{K=C}$ then by the hint we can only show that $f(a+bi)=af(1)+bf(i)$ for all $a,b\in\mathbb{R}$, but I think $f(i)$ need not equal $if(1)$, and so the result is false. For instance, take $V=W=\mathbb{C}$, regarded as vector spaces over $\mathbb{C}$ and equipped with the usual Euclidean norm. Let $f$ be the conjugation, i.e., $f:a+bi\mapsto a-bi$. I do think this is a continuous group homomorphism, but it isn't linear. Am I right?","The following is exercise III.1.13 from Analysis I by Amann and Escher. Suppose that $V$ and $W$ are normed vector spaces and $f : V → W$ is a continuous   group homomorphism from $(V, +)$ to $(W, +)$. Prove that $f$ is linear. (Hint: If $\mathbb{K = R}$, $x \in V$ and $q ∈ \mathbb{Q}$, then $f(qx) = qf(x)$. See also Exercise $6$.) Here $\mathbb{K}$ means the base field for $V$ and $W$ and $\mathbb{K=C}$ or $\mathbb{R}$ (which is a convention stated in the text). Now if $\mathbb{K=C}$ then by the hint we can only show that $f(a+bi)=af(1)+bf(i)$ for all $a,b\in\mathbb{R}$, but I think $f(i)$ need not equal $if(1)$, and so the result is false. For instance, take $V=W=\mathbb{C}$, regarded as vector spaces over $\mathbb{C}$ and equipped with the usual Euclidean norm. Let $f$ be the conjugation, i.e., $f:a+bi\mapsto a-bi$. I do think this is a continuous group homomorphism, but it isn't linear. Am I right?",,"['analysis', 'continuity', 'linear-transformations', 'normed-spaces', 'group-homomorphism']"
5,equivalent definitions of limit superior,equivalent definitions of limit superior,,"Definition I Let $\{ s_n \}$ be a sequence of real numbers. Let $E$ be the set of numbers $x$ (in the extended real number system) such that $s_{n_k} \rightarrow x$ for some subsequence $\{s_{n_k}\}$. This set $E$ contains all subsequential limits, plus possibly the numbers $+\infty$, $-\infty$. Then the limit superior $s^*$ is the unique number with the properties: (a) $s^* \in E$. (b) If $x> s^*$, there is an integer $N$ such that $n \geq N$ implies $s_n < x$. Definition II Let $(a_k)$ be a sequence of real numbers. Define $s_m =\sup\{a_k\mid k\ge m\}. $ Then the limit superior is defined to be $s^*=\lim\limits_{m\rightarrow \infty}s_m.$ I have show these two definitions are equivalent. Attempt : Let $(a_k)$ be a bounded sequence of real numbers. Define $s_m =\sup\{a_k\mid k\ge m\}.$ Since $(s_m)$ is monotonic and bounded, $(s_m)$ converges. Denote $U =\lim\limits_{m\rightarrow \infty}s_m $ so that $$\forall \epsilon >0, \exists N\in \mathbb{N} \text{ such that } |s_m-U|<\epsilon, \forall m\ge N.$$ If we show that $U$ satisfies the properties mentioned in definition I, we are done. So, Claim : If $y>U,$ then $\exists m\in \mathbb{N}$ such that $k\ge m$ implies $a_k < y.$ Choose $\epsilon := y-U$ This implies $\exists N\in \mathbb{N}$ such that  $|s_m-U|<y-U, \forall m\ge N.$ $$\implies s_m-U<y-U, \forall m\ge N.$$ $$\implies s_m<y, \forall m\ge N.$$ Thus for $k\ge m,$ we have $a_k\le s_m < y, \forall m\ge N.$ Hence, If $y>U,$ then $\exists m\in \mathbb{N}$ such that $k\ge m$ implies $a_k < y$ and the claim is verified. Question : How do you show that $U \in E$ i.e., there exists a subsequence $(a_{n_k})$ which converges to $U.$","Definition I Let $\{ s_n \}$ be a sequence of real numbers. Let $E$ be the set of numbers $x$ (in the extended real number system) such that $s_{n_k} \rightarrow x$ for some subsequence $\{s_{n_k}\}$. This set $E$ contains all subsequential limits, plus possibly the numbers $+\infty$, $-\infty$. Then the limit superior $s^*$ is the unique number with the properties: (a) $s^* \in E$. (b) If $x> s^*$, there is an integer $N$ such that $n \geq N$ implies $s_n < x$. Definition II Let $(a_k)$ be a sequence of real numbers. Define $s_m =\sup\{a_k\mid k\ge m\}. $ Then the limit superior is defined to be $s^*=\lim\limits_{m\rightarrow \infty}s_m.$ I have show these two definitions are equivalent. Attempt : Let $(a_k)$ be a bounded sequence of real numbers. Define $s_m =\sup\{a_k\mid k\ge m\}.$ Since $(s_m)$ is monotonic and bounded, $(s_m)$ converges. Denote $U =\lim\limits_{m\rightarrow \infty}s_m $ so that $$\forall \epsilon >0, \exists N\in \mathbb{N} \text{ such that } |s_m-U|<\epsilon, \forall m\ge N.$$ If we show that $U$ satisfies the properties mentioned in definition I, we are done. So, Claim : If $y>U,$ then $\exists m\in \mathbb{N}$ such that $k\ge m$ implies $a_k < y.$ Choose $\epsilon := y-U$ This implies $\exists N\in \mathbb{N}$ such that  $|s_m-U|<y-U, \forall m\ge N.$ $$\implies s_m-U<y-U, \forall m\ge N.$$ $$\implies s_m<y, \forall m\ge N.$$ Thus for $k\ge m,$ we have $a_k\le s_m < y, \forall m\ge N.$ Hence, If $y>U,$ then $\exists m\in \mathbb{N}$ such that $k\ge m$ implies $a_k < y$ and the claim is verified. Question : How do you show that $U \in E$ i.e., there exists a subsequence $(a_{n_k})$ which converges to $U.$",,"['real-analysis', 'analysis']"
6,A rarely seen form of substitution of definitite integral theorem: $\int_\alpha^\beta f(u(t))dt=\int_{u(\alpha)}^{u(\beta)}f(x)\cdot(u^{-1})'(x)dx$,A rarely seen form of substitution of definitite integral theorem:,\int_\alpha^\beta f(u(t))dt=\int_{u(\alpha)}^{u(\beta)}f(x)\cdot(u^{-1})'(x)dx,"Below is a theorem about substitution of definite integral that I found today. However, I had never seen this form in analysis books. How to understand its meaning and usage? Is it really made used in practice? The characters for $u^{-1}$, $f(u(t))$ is messy to me. (Though I know the classic form of such theorem.) Let $J=[\alpha,\beta],~u:J\to\Bbb R$ be a $C^1$ function and   $u'(x)\neq 0$ for all $x\in J$, $I$ be an interval and   $u(J)\subseteq I$, $f:I\to\Bbb R$ be continuous. Then   $$\int_\alpha^\beta f(u(t))dt=\int_{u(\alpha)}^{u(\beta)}f(x)\cdot(u^{-1})'(x)dx$$ Edit: We are looking for explicitly examples where this method is usefully for computing integrals.","Below is a theorem about substitution of definite integral that I found today. However, I had never seen this form in analysis books. How to understand its meaning and usage? Is it really made used in practice? The characters for $u^{-1}$, $f(u(t))$ is messy to me. (Though I know the classic form of such theorem.) Let $J=[\alpha,\beta],~u:J\to\Bbb R$ be a $C^1$ function and   $u'(x)\neq 0$ for all $x\in J$, $I$ be an interval and   $u(J)\subseteq I$, $f:I\to\Bbb R$ be continuous. Then   $$\int_\alpha^\beta f(u(t))dt=\int_{u(\alpha)}^{u(\beta)}f(x)\cdot(u^{-1})'(x)dx$$ Edit: We are looking for explicitly examples where this method is usefully for computing integrals.",,"['real-analysis', 'analysis', 'riemann-integration']"
7,How to prove this recursive convergence by induction,How to prove this recursive convergence by induction,,"I am having difficulties in proving that the following recursive sequence converges: $$a_{n+1}=\frac{2a_{n}+1}{a_{n}+2} :a_{1}=0$$ I have tried proving this by mathematical induction by first proving it is increasing and secondly that it has an upper bound. Proving it is increasing: Since $a_{2}\geq a_{1}$ then we assume that $a_{k+1}\geq a_{k}$ and try to show that it implies $a_{k+2}\geq a_{k+1}$. I try to show this below and get stuck quite quickly: $$a_{k+1}\geq a_{k} \Leftrightarrow 2a_{k+1}+1 \geq 2a_{k}+1$$ As you can see I cannot divide by $a_{n}+2$ to show that $a_{k+1}\geq a_{k} \implies a_{k+2}\geq a_{k+1}$. My question is therefore if I am doing something worng or if this cannot be proven by induction, and if not, which other methods exist to prove this.","I am having difficulties in proving that the following recursive sequence converges: $$a_{n+1}=\frac{2a_{n}+1}{a_{n}+2} :a_{1}=0$$ I have tried proving this by mathematical induction by first proving it is increasing and secondly that it has an upper bound. Proving it is increasing: Since $a_{2}\geq a_{1}$ then we assume that $a_{k+1}\geq a_{k}$ and try to show that it implies $a_{k+2}\geq a_{k+1}$. I try to show this below and get stuck quite quickly: $$a_{k+1}\geq a_{k} \Leftrightarrow 2a_{k+1}+1 \geq 2a_{k}+1$$ As you can see I cannot divide by $a_{n}+2$ to show that $a_{k+1}\geq a_{k} \implies a_{k+2}\geq a_{k+1}$. My question is therefore if I am doing something worng or if this cannot be proven by induction, and if not, which other methods exist to prove this.",,"['calculus', 'analysis', 'induction']"
8,Hyperbolic set such that its set of periodic points is not dense in it,Hyperbolic set such that its set of periodic points is not dense in it,,"Let $f : M \rightarrow M$ be a $C^r$ diffeomorphism, $r \geq 1$, where $M$ is a Riemannian manifold. We say that an invariant set $\Lambda \subset M$ is hyperbolic if At every point $p$ in $\Lambda$ the tangent space $T_p M$ splits as a direct sum $T_p M = E_p^s \oplus E_p^u$. Both $E_p^s$ and $E_p^s$ are $Df_p$-invariant, that is, $Df_p(E_p^s) = E_p^s$ and $Df_p(E_p^u) = E_p^u$ There are constants $0<\lambda< 1$ and $C \geq 1$ independent of $p$ such that for all $n \geq 0$ $||Df^n_p(v)|| \leq C\lambda^n||v||$ for $v \in E_p^s$ and $||Df^{-n}_p(v)|| \leq C\lambda^n||v||$ for $v \in E_p^u$. Many examples of hyperbolic sets, such as the Cantor sets for the quadratic family $F_{\mu} = \mu x(1-x)$ for $\mu > 4$ and the linear toral automorphisms have the property that they have a dense set of periodic points. Is there a hyperbolic set $\Lambda$ such that its set of periodic points is not dense in $\Lambda$?","Let $f : M \rightarrow M$ be a $C^r$ diffeomorphism, $r \geq 1$, where $M$ is a Riemannian manifold. We say that an invariant set $\Lambda \subset M$ is hyperbolic if At every point $p$ in $\Lambda$ the tangent space $T_p M$ splits as a direct sum $T_p M = E_p^s \oplus E_p^u$. Both $E_p^s$ and $E_p^s$ are $Df_p$-invariant, that is, $Df_p(E_p^s) = E_p^s$ and $Df_p(E_p^u) = E_p^u$ There are constants $0<\lambda< 1$ and $C \geq 1$ independent of $p$ such that for all $n \geq 0$ $||Df^n_p(v)|| \leq C\lambda^n||v||$ for $v \in E_p^s$ and $||Df^{-n}_p(v)|| \leq C\lambda^n||v||$ for $v \in E_p^u$. Many examples of hyperbolic sets, such as the Cantor sets for the quadratic family $F_{\mu} = \mu x(1-x)$ for $\mu > 4$ and the linear toral automorphisms have the property that they have a dense set of periodic points. Is there a hyperbolic set $\Lambda$ such that its set of periodic points is not dense in $\Lambda$?",,"['analysis', 'dynamical-systems']"
9,Intersection of two algebras on different spaces not an algebra,Intersection of two algebras on different spaces not an algebra,,"Reviewing my lecture notes, I have trouble understanding why the intersection of the following two algebras behaves as it does: $$ \mathcal{A} = \left\{A \subseteq \mathbb{N}: |A| < \infty \text{ or } |A^c| < \infty  \right\}$$ $$ \mathcal{B} = \left\{B \subseteq \mathbb{Z}: |B| < \infty \text{ or } |B^c| < \infty  \right\},$$ where $A^c$ and $B^c$ denote the complements of $A$ and $B$, respectively. I was told that the intersection of $\mathcal{A}$ and $\mathcal{B}$ is equal to $\left\{A \subseteq \mathbb{N}: |A| < \infty \right\}$. Why isn't $\mathcal{A} \cap \mathcal{B}$ equal to $\mathcal{A}$ itself? Why isn't it an algebra? Thanks for enlightening me! Much obliged for any input!","Reviewing my lecture notes, I have trouble understanding why the intersection of the following two algebras behaves as it does: $$ \mathcal{A} = \left\{A \subseteq \mathbb{N}: |A| < \infty \text{ or } |A^c| < \infty  \right\}$$ $$ \mathcal{B} = \left\{B \subseteq \mathbb{Z}: |B| < \infty \text{ or } |B^c| < \infty  \right\},$$ where $A^c$ and $B^c$ denote the complements of $A$ and $B$, respectively. I was told that the intersection of $\mathcal{A}$ and $\mathcal{B}$ is equal to $\left\{A \subseteq \mathbb{N}: |A| < \infty \right\}$. Why isn't $\mathcal{A} \cap \mathcal{B}$ equal to $\mathcal{A}$ itself? Why isn't it an algebra? Thanks for enlightening me! Much obliged for any input!",,"['real-analysis', 'analysis', 'measure-theory']"
10,prove that every non-empty open set contains an open sphere disjoint from $A$.,prove that every non-empty open set contains an open sphere disjoint from .,A,"If $A$ is nowhere dense in $(X,d)$ then prove that every non-empty open set contains an open sphere disjoint from $A$. Suppose that $A$ is n.w.d. in $X$ , and every non-empty open set say , $B$ contains all open sphere $S_r(x),x\in X,r>0$ such that $S_r(x) \cap A\not= \emptyset.$ Then where it contradicts ?","If $A$ is nowhere dense in $(X,d)$ then prove that every non-empty open set contains an open sphere disjoint from $A$. Suppose that $A$ is n.w.d. in $X$ , and every non-empty open set say , $B$ contains all open sphere $S_r(x),x\in X,r>0$ such that $S_r(x) \cap A\not= \emptyset.$ Then where it contradicts ?",,"['real-analysis', 'analysis', 'metric-spaces']"
11,"If T and S are topologically conjugate homeomorphisms, T on X is minimal iff S on Y is minimal. (Definitions inside)","If T and S are topologically conjugate homeomorphisms, T on X is minimal iff S on Y is minimal. (Definitions inside)",,"Let $X$ and $Y$ be compact metric spaces, $T:X \to X$ and $S: Y \to Y$ homeomorphisms. We say $T$ and $S$ are topologically conjugate if there exists a homeomorphism $\phi: X \to Y$ such that $S(\phi(x))= \phi(T(x))$ for all $x \in X$. We say a homeomorphism $T$ is minimal if for every $x \in X$, the set $\{T^k x: x\in \mathbb{Z}\}$ is dense in $X$. The claim is that T is minimal iff S is minimal. I have tried to show this but to no avail, as I get stuck trying to compare a ball around x with a ball around $\phi(x)$ which unless its an isometry I can not do anything interesting with to get my result. Thanks in advance!","Let $X$ and $Y$ be compact metric spaces, $T:X \to X$ and $S: Y \to Y$ homeomorphisms. We say $T$ and $S$ are topologically conjugate if there exists a homeomorphism $\phi: X \to Y$ such that $S(\phi(x))= \phi(T(x))$ for all $x \in X$. We say a homeomorphism $T$ is minimal if for every $x \in X$, the set $\{T^k x: x\in \mathbb{Z}\}$ is dense in $X$. The claim is that T is minimal iff S is minimal. I have tried to show this but to no avail, as I get stuck trying to compare a ball around x with a ball around $\phi(x)$ which unless its an isometry I can not do anything interesting with to get my result. Thanks in advance!",,"['real-analysis', 'general-topology']"
12,Does there exist an English translation of the book Théorie Des Distributions by Laurent Schwartz? [closed],Does there exist an English translation of the book Théorie Des Distributions by Laurent Schwartz? [closed],,"Closed. This question is off-topic . It is not currently accepting answers. Closed 6 years ago . This question is not about mathematics, within the scope defined in the help center . This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Improve this question Does there exist an English translation of the book Théorie Des Distributions by Laurent Schwartz? The theory of Distributions as it is presented in modern books is very much evolved and for a beginner it is difficult to comprehend the motivation behind it. The father of the subject; Laurent Schwartz wrote his treatise in french. A few pages of the original book was translated into english. It is much easier to comprehend the motivation behind the various concepts,in this book. That's why I wanted to know if there is a complete translated version of the book.","Closed. This question is off-topic . It is not currently accepting answers. Closed 6 years ago . This question is not about mathematics, within the scope defined in the help center . This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Improve this question Does there exist an English translation of the book Théorie Des Distributions by Laurent Schwartz? The theory of Distributions as it is presented in modern books is very much evolved and for a beginner it is difficult to comprehend the motivation behind it. The father of the subject; Laurent Schwartz wrote his treatise in french. A few pages of the original book was translated into english. It is much easier to comprehend the motivation behind the various concepts,in this book. That's why I wanted to know if there is a complete translated version of the book.",,"['analysis', 'reference-request', 'book-recommendation']"
13,How do I show convergence in the 2-adics?,How do I show convergence in the 2-adics?,,"How do I show convergence in the 2-adics in general, but particularly for the series given below? If, to discriminate between convergence in different $p$-adics I define $\infty_p=\lim_{n\to\infty} p^n$ then it would seem reasonable to state $\infty_2=0_2$ in the 2-adics.  I'm unsure of however, whether other series are convergent in the 2-adics such as whether $\infty_3=0_2$ I have an infinite sum which is divergent in the integers but in the 2-adics it converges to $\infty_2=0$. The case I have in mind is; show that for every $x_0\in3\mathbb{N}_{>0}$: $$f(x_0)=\lim_{n\to\infty} \left(3^n\left(x_m-2^{v(x_0)}\right)+\sum_{k=0}^n3^{n-k}2^{v(x_{k})} \right)=0_2$$ where $v(\cdot)$ is the (additive) 2-adic valuation $v:\Bbb Q_2^\times\to\Bbb Z$ and $x_{m+1}=2x_m+v(x_m$). It actually converges to $2^r$ for some $r$ in finite steps for all $x_m$ and lands on every alternate power of $2$ thereafter. How might one typically identify convergence in such a series?  Straight off the bat I can see that it's sufficient to show its value converges to the inverse of its 2-adic norm and therefore if $x$ converges to $0_2$, then $\frac{x}{2^{v(x)}}=1$ I can also see that this problem is equivalent to stating that for any given $x_0$, for sufficiently large $r$, there exists a series $x_0+x_{1}+\ldots$ in which $x_{m+1}=2x'_m+2^{v(x)}$ which sums to either $2^r$ or $2^{r+1}$ where $x'_m$ denotes the partial sum to $x_m$.  I mention this partly to highlight the alternating nature of the series in that for sufficiently high $2^r$ it lands on alternate powers of $2$ and not on every power of $2$. I'm not necessarily asking you to solve this problem (although that would be welcome!) as it's clearly a challenging one but guidance as to a typical method would be appreciated.  There is much I am unsure of.","How do I show convergence in the 2-adics in general, but particularly for the series given below? If, to discriminate between convergence in different $p$-adics I define $\infty_p=\lim_{n\to\infty} p^n$ then it would seem reasonable to state $\infty_2=0_2$ in the 2-adics.  I'm unsure of however, whether other series are convergent in the 2-adics such as whether $\infty_3=0_2$ I have an infinite sum which is divergent in the integers but in the 2-adics it converges to $\infty_2=0$. The case I have in mind is; show that for every $x_0\in3\mathbb{N}_{>0}$: $$f(x_0)=\lim_{n\to\infty} \left(3^n\left(x_m-2^{v(x_0)}\right)+\sum_{k=0}^n3^{n-k}2^{v(x_{k})} \right)=0_2$$ where $v(\cdot)$ is the (additive) 2-adic valuation $v:\Bbb Q_2^\times\to\Bbb Z$ and $x_{m+1}=2x_m+v(x_m$). It actually converges to $2^r$ for some $r$ in finite steps for all $x_m$ and lands on every alternate power of $2$ thereafter. How might one typically identify convergence in such a series?  Straight off the bat I can see that it's sufficient to show its value converges to the inverse of its 2-adic norm and therefore if $x$ converges to $0_2$, then $\frac{x}{2^{v(x)}}=1$ I can also see that this problem is equivalent to stating that for any given $x_0$, for sufficiently large $r$, there exists a series $x_0+x_{1}+\ldots$ in which $x_{m+1}=2x'_m+2^{v(x)}$ which sums to either $2^r$ or $2^{r+1}$ where $x'_m$ denotes the partial sum to $x_m$.  I mention this partly to highlight the alternating nature of the series in that for sufficiently high $2^r$ it lands on alternate powers of $2$ and not on every power of $2$. I'm not necessarily asking you to solve this problem (although that would be welcome!) as it's clearly a challenging one but guidance as to a typical method would be appreciated.  There is much I am unsure of.",,"['analysis', 'convergence-divergence', 'p-adic-number-theory']"
14,Help with proof by contradiction,Help with proof by contradiction,,"I'm relatively new with proofs and am trying to self-teach. I'm currently going through questions that unfortunately have no solutions... I've been doing well until I struck this one: If l, m, and n are consecutive integers, then 12 does not divide $l^2 + m^2 + n ^2 +1$. I know that proof by contradiction is  p and (not q) => C. So to start off, I assume that it IS divisible by 12 and I have $l$, $m=l+1$, n=$l+2$. Therefore $l^2 + m^2 + n ^2 +1 = 3l^2+6l+6 = 3(l^2+2l+2)$. This is as far as a got. Any help or hints would be appreciated.","I'm relatively new with proofs and am trying to self-teach. I'm currently going through questions that unfortunately have no solutions... I've been doing well until I struck this one: If l, m, and n are consecutive integers, then 12 does not divide $l^2 + m^2 + n ^2 +1$. I know that proof by contradiction is  p and (not q) => C. So to start off, I assume that it IS divisible by 12 and I have $l$, $m=l+1$, n=$l+2$. Therefore $l^2 + m^2 + n ^2 +1 = 3l^2+6l+6 = 3(l^2+2l+2)$. This is as far as a got. Any help or hints would be appreciated.",,['analysis']
15,An Integral Inequality for $C_c^{\infty}(\mathbb{R}^3)$,An Integral Inequality for,C_c^{\infty}(\mathbb{R}^3),"I stumbled upon the following inequality where $f \in C_c^{\infty}(\mathbb{R}^3)$: $$ \left( \int \int_{\mathbb{R}^6} \frac{\overline{f(x)} f(y)}{\vert x-y \vert}dxdy \right) \Vert\nabla f\Vert_2^2 \geq 4 \pi \Vert f\Vert_2^4 $$ where $\overline{f(x)}$ denotes complex conjugation. Unfortunately, I have no idea how to prove it. I don't even know why the left-hand side should be non-negative. The symmetry of the left-hand side and the $4 \pi$ on the right suggests some kind of transformation into a polar coordinates or use of the co-area formula. I'm also curious about generalizations to $\mathbb{R}^d$, i.e. if something like $$ \left( \int \int_{\mathbb{R}^{2d}} \frac{\overline{f(x)} f(y)}{\vert x-y \vert}dxdy \right) \Vert\nabla f\Vert_2^2 \geq \omega_n \Vert f\Vert_2^4 $$ holds for $f \in C_c^{\infty}(\mathbb{R}^d)$, where $\omega_n$ is the surface area of the $d$-dimensional sphere.","I stumbled upon the following inequality where $f \in C_c^{\infty}(\mathbb{R}^3)$: $$ \left( \int \int_{\mathbb{R}^6} \frac{\overline{f(x)} f(y)}{\vert x-y \vert}dxdy \right) \Vert\nabla f\Vert_2^2 \geq 4 \pi \Vert f\Vert_2^4 $$ where $\overline{f(x)}$ denotes complex conjugation. Unfortunately, I have no idea how to prove it. I don't even know why the left-hand side should be non-negative. The symmetry of the left-hand side and the $4 \pi$ on the right suggests some kind of transformation into a polar coordinates or use of the co-area formula. I'm also curious about generalizations to $\mathbb{R}^d$, i.e. if something like $$ \left( \int \int_{\mathbb{R}^{2d}} \frac{\overline{f(x)} f(y)}{\vert x-y \vert}dxdy \right) \Vert\nabla f\Vert_2^2 \geq \omega_n \Vert f\Vert_2^4 $$ holds for $f \in C_c^{\infty}(\mathbb{R}^d)$, where $\omega_n$ is the surface area of the $d$-dimensional sphere.",,"['integration', 'analysis', 'inequality', 'fourier-analysis', 'integral-inequality']"
16,Calculate $\int_{-\infty}^{\infty} \frac{\sin^2(\pi Rx)}{R(\pi x)^2}dx$ for $R>0$.,Calculate  for .,\int_{-\infty}^{\infty} \frac{\sin^2(\pi Rx)}{R(\pi x)^2}dx R>0,"Calculate $$\int_{-\infty}^{\infty} \frac{\sin^2(\pi Rx)}{R(\pi x)^2}dx$$ for $R>0$.  The value is $1$, but I don't know which integration technique I need to use to calculate this. I would greatly appreciate any help.","Calculate $$\int_{-\infty}^{\infty} \frac{\sin^2(\pi Rx)}{R(\pi x)^2}dx$$ for $R>0$.  The value is $1$, but I don't know which integration technique I need to use to calculate this. I would greatly appreciate any help.",,"['calculus', 'integration', 'analysis', 'definite-integrals']"
17,Find the supremum and infimum of the set $S = \{ \sqrt {n^2 + 1} - n: n \in \mathbb{N}\}$,Find the supremum and infimum of the set,S = \{ \sqrt {n^2 + 1} - n: n \in \mathbb{N}\},Find the supremum and infimum of the set $S = \{ \sqrt {n^2 + 1} - n: n \in   \mathbb{N} \}.$ I know that the supremum is $\sqrt{2} - 1$ but what about the infimum is it $0$? Could anyone tell me if I am right or wrong?,Find the supremum and infimum of the set $S = \{ \sqrt {n^2 + 1} - n: n \in   \mathbb{N} \}.$ I know that the supremum is $\sqrt{2} - 1$ but what about the infimum is it $0$? Could anyone tell me if I am right or wrong?,,['real-analysis']
18,Big -O time complexity of an algorithm.,Big -O time complexity of an algorithm.,,"I need to analyze this pseudo code, do_something() is O(1) and you may assume N = 2^k for some positive integer k. I will write on each line the answer I think is correct. Set n ← N   **1** For i ← 1 to n  **n**   set counter ← 1  **1**   While counter ≤ n   **logn**     For j ← counter to 1 **Im not sure about it, maybe 2^n -1**       do_something()  **1**     end     counter ← 2 * counter **1**   end end Is my analysis correct? And how I translate it to big O notation? Thank you.","I need to analyze this pseudo code, do_something() is O(1) and you may assume N = 2^k for some positive integer k. I will write on each line the answer I think is correct. Set n ← N   **1** For i ← 1 to n  **n**   set counter ← 1  **1**   While counter ≤ n   **logn**     For j ← counter to 1 **Im not sure about it, maybe 2^n -1**       do_something()  **1**     end     counter ← 2 * counter **1**   end end Is my analysis correct? And how I translate it to big O notation? Thank you.",,"['analysis', 'algorithms', 'asymptotics', 'computer-science']"
19,Proving limit of sin(1/x)cos(1/x) doesn't exist as x goes to 0,Proving limit of sin(1/x)cos(1/x) doesn't exist as x goes to 0,,"Just a quick question, this may or may not be a duplicate by the way. I've seen the proof of the trig functions not existing separately but I couldn't seem to find them multiplied together like in this problem. My question is could I just separate the two functions, prove that both limits do not exist and say the overall limit doesn't exist or is that the completely wrong way of approaching the problem? EDIT: Here's my attempt with the use of $\frac{sin(\frac{2}{x})}{2}$ hint. Proof: Note that $\sin(\frac{1}{x})\cos(\frac{1}{x})$ =  $\frac{\sin(\frac{2}{x})}{2}$. Let f(x) =  $\frac{\sin(\frac{2}{x})}{2}$. $\sin \theta = 1$ when $\theta = \frac{4k+1 \pi}{2}$ for $k \in \mathbb{Z}$ $\sin \theta = -1$ when $\theta = \frac{4k+1 \pi}{2}$ for $k \in \mathbb{Z}$ so $\sin(2/x_n) = 1$ when $x_n = \frac{4}{(4n+1)\pi}$ which equals $0$ when $n$ goes to infinity and $f(x_n) = 1$ for all $n$ $\sin(2/y_n) = 1$ when $y_n = \frac{4}{(4n-1)\pi}$ which equals $0$ when $n$ goes to infinity and $f(y_n) = -1$ for all $n$ It then follows that: $\sin(2/x_n)/2 \implies f(x_n) = 1/2$ $\sin(2/y_n)/2 \implies f(y_n) = -1/2$ Since $\{x_n\}$ goes to zero $x_n \neq 0$ and $\{y_n\}$ goes to zero $y_n \neq 0$ but $\{f(x_n)\} \rightarrow 1/2 \neq -1/2 \leftarrow\{f(y_n)\}$ $\lim_{x \to 0} f(x)$ does not exist by the sequential criterion. Is this totally off?","Just a quick question, this may or may not be a duplicate by the way. I've seen the proof of the trig functions not existing separately but I couldn't seem to find them multiplied together like in this problem. My question is could I just separate the two functions, prove that both limits do not exist and say the overall limit doesn't exist or is that the completely wrong way of approaching the problem? EDIT: Here's my attempt with the use of $\frac{sin(\frac{2}{x})}{2}$ hint. Proof: Note that $\sin(\frac{1}{x})\cos(\frac{1}{x})$ =  $\frac{\sin(\frac{2}{x})}{2}$. Let f(x) =  $\frac{\sin(\frac{2}{x})}{2}$. $\sin \theta = 1$ when $\theta = \frac{4k+1 \pi}{2}$ for $k \in \mathbb{Z}$ $\sin \theta = -1$ when $\theta = \frac{4k+1 \pi}{2}$ for $k \in \mathbb{Z}$ so $\sin(2/x_n) = 1$ when $x_n = \frac{4}{(4n+1)\pi}$ which equals $0$ when $n$ goes to infinity and $f(x_n) = 1$ for all $n$ $\sin(2/y_n) = 1$ when $y_n = \frac{4}{(4n-1)\pi}$ which equals $0$ when $n$ goes to infinity and $f(y_n) = -1$ for all $n$ It then follows that: $\sin(2/x_n)/2 \implies f(x_n) = 1/2$ $\sin(2/y_n)/2 \implies f(y_n) = -1/2$ Since $\{x_n\}$ goes to zero $x_n \neq 0$ and $\{y_n\}$ goes to zero $y_n \neq 0$ but $\{f(x_n)\} \rightarrow 1/2 \neq -1/2 \leftarrow\{f(y_n)\}$ $\lim_{x \to 0} f(x)$ does not exist by the sequential criterion. Is this totally off?",,['analysis']
20,"Prob. 25(b), Chap. 4 in Baby Rudin: The set $C_1 + C_2$ need not be closed in $\mathbb{R}$ even for closed sets $C_1$ and $C_2$","Prob. 25(b), Chap. 4 in Baby Rudin: The set  need not be closed in  even for closed sets  and",C_1 + C_2 \mathbb{R} C_1 C_2,"Here is Prob. 25, Chap. 4 in Principles of Mathematical Analysis by Walter Rudin, 3rd edition: If $A \subset \mathbb{R}^k$ and $B \subset \mathbb{R}^k$, define $A + B$ to be the set of all sums $x+y$ with $x \in A$, $y \in B$. (a) If $K$ is compact and $C$ is closed in $\mathbb{R}^k$, prove that $K+C$ is closed. [This I think I've managed to prove.] Hint: . . . (b) Let $\alpha$ be an irrational real number. Let $C_1$ be the set of all integers, let $C_2$ be the set of all $n \alpha$ with $n \in C_1$. Show that $C_1$ and $C_2$ are closed subsets of $\mathbb{R}^1$ whose sum $C_1 + C_2$ is not closed, by by showing that $C_1 + C_2$ is a countable dense subset of $\mathbb{R}^1$. My effort: Let $p$ be a real number that is not an integer, and let $\delta$ be any real number such that $$0 < \delta < \min \left\{ \ p - \lfloor p \rfloor, \ \lceil p \rceil - p \ \right\}. $$    Then the $\delta$-neighborhood of $p$ in $\mathbb{R}^1$ does not intersect the set of integers at all, showing that every point $p$ of $\mathbb{R}^1 - C_1$ is an interior point and hence that $C_1$ is closed in $\mathbb{R}^1$. Am I right? For any two distinct points $x$ and $y$ of $C_2$, we note that    $$\vert x-y \vert \geq \vert \alpha \vert > 0.$$    So if $p$ is any real number that is not in $C_2$ and if $\delta$ is any real number such that    $$0 < \delta < \min \left\{ \ \left\vert p - \alpha \lfloor \frac{p}{\alpha} \rfloor \right\vert, \ \left\vert \alpha \lceil \frac{p}{\alpha} \rceil - p \right\vert \ \right\},$$    then the $\delta$-neighborhood of $p$ in $\mathbb{$}^1$ --- which equals the segment $(p-\delta, p+\delta) --- does not intersect the set $C_2$ at all, for is $x \in \mathbb{R}$^1$ and $$ p-\delta < x < p+\delta, $$   then we must have    $$ \alpha \lfloor \frac{p}{\alpha} \rfloor < x < \alpha \lceil \frac{p}{\alpha} \rceil,$$    and $\lfloor \frac{p}{\alpha} \rfloor$ and $\lceil \frac{p}{\alpha} \rceil$ are two successive integers, which implies that $x$ lies strictly in between two successive elements of $C_2$. Thus every point $p$ of $\mathbb{R}^1- C_2$ is an interior point, from which it follows that $C_2$ is closed. Am I right? Moreover, we also note that if $n$ is non-zero, then $n \alpah$ is irrational, by Prob. 1, Chap. 1 in Baby Rudin, 3rd edition. So the sets $C_1$ and $C_2$ intersect in $0$ only. Am I right? Now the set $C_1 + C_2$ is given by    $$C_1 + C_2 = \left\{ \ m + n\alpha \ \colon \ m \mbox{ and } n \mbox{ are integers } \ \right\}.$$ Am I right? Now the map $m + n \alpha \ \mapsto \ (m, n)$ is an injective mapping of $C_1 + C_2$ into the Cartesian product $C_1 \times C_1$, and this Cartesian product is of course countable. So $C_1 + C_2$ is countable. Am I right? Now how to show that $C_1 + C_2$ is dense in $\mathbb{R}^1$? Once we have shown this then we know that $C_1 + C_2$ cannot be closed, because if $F$ is a closed set in a metric space $X$ and if $F$ is dense in $X$, then we have    $$X = \overline{F} = F,$$   but $C_1 + C_2$, being a countable subset of $\mathbb{R}^1$, is of course a proper subset of $\mathbb{R}^1$, since $\mathbb{R}^1$ is uncountable. Am I right? So, if what I've established so far is correct, then my only question is how to (rigorously) show that $C_1 + C_2$ is dense in $\mathbb{R}^1$?","Here is Prob. 25, Chap. 4 in Principles of Mathematical Analysis by Walter Rudin, 3rd edition: If $A \subset \mathbb{R}^k$ and $B \subset \mathbb{R}^k$, define $A + B$ to be the set of all sums $x+y$ with $x \in A$, $y \in B$. (a) If $K$ is compact and $C$ is closed in $\mathbb{R}^k$, prove that $K+C$ is closed. [This I think I've managed to prove.] Hint: . . . (b) Let $\alpha$ be an irrational real number. Let $C_1$ be the set of all integers, let $C_2$ be the set of all $n \alpha$ with $n \in C_1$. Show that $C_1$ and $C_2$ are closed subsets of $\mathbb{R}^1$ whose sum $C_1 + C_2$ is not closed, by by showing that $C_1 + C_2$ is a countable dense subset of $\mathbb{R}^1$. My effort: Let $p$ be a real number that is not an integer, and let $\delta$ be any real number such that $$0 < \delta < \min \left\{ \ p - \lfloor p \rfloor, \ \lceil p \rceil - p \ \right\}. $$    Then the $\delta$-neighborhood of $p$ in $\mathbb{R}^1$ does not intersect the set of integers at all, showing that every point $p$ of $\mathbb{R}^1 - C_1$ is an interior point and hence that $C_1$ is closed in $\mathbb{R}^1$. Am I right? For any two distinct points $x$ and $y$ of $C_2$, we note that    $$\vert x-y \vert \geq \vert \alpha \vert > 0.$$    So if $p$ is any real number that is not in $C_2$ and if $\delta$ is any real number such that    $$0 < \delta < \min \left\{ \ \left\vert p - \alpha \lfloor \frac{p}{\alpha} \rfloor \right\vert, \ \left\vert \alpha \lceil \frac{p}{\alpha} \rceil - p \right\vert \ \right\},$$    then the $\delta$-neighborhood of $p$ in $\mathbb{$}^1$ --- which equals the segment $(p-\delta, p+\delta) --- does not intersect the set $C_2$ at all, for is $x \in \mathbb{R}$^1$ and $$ p-\delta < x < p+\delta, $$   then we must have    $$ \alpha \lfloor \frac{p}{\alpha} \rfloor < x < \alpha \lceil \frac{p}{\alpha} \rceil,$$    and $\lfloor \frac{p}{\alpha} \rfloor$ and $\lceil \frac{p}{\alpha} \rceil$ are two successive integers, which implies that $x$ lies strictly in between two successive elements of $C_2$. Thus every point $p$ of $\mathbb{R}^1- C_2$ is an interior point, from which it follows that $C_2$ is closed. Am I right? Moreover, we also note that if $n$ is non-zero, then $n \alpah$ is irrational, by Prob. 1, Chap. 1 in Baby Rudin, 3rd edition. So the sets $C_1$ and $C_2$ intersect in $0$ only. Am I right? Now the set $C_1 + C_2$ is given by    $$C_1 + C_2 = \left\{ \ m + n\alpha \ \colon \ m \mbox{ and } n \mbox{ are integers } \ \right\}.$$ Am I right? Now the map $m + n \alpha \ \mapsto \ (m, n)$ is an injective mapping of $C_1 + C_2$ into the Cartesian product $C_1 \times C_1$, and this Cartesian product is of course countable. So $C_1 + C_2$ is countable. Am I right? Now how to show that $C_1 + C_2$ is dense in $\mathbb{R}^1$? Once we have shown this then we know that $C_1 + C_2$ cannot be closed, because if $F$ is a closed set in a metric space $X$ and if $F$ is dense in $X$, then we have    $$X = \overline{F} = F,$$   but $C_1 + C_2$, being a countable subset of $\mathbb{R}^1$, is of course a proper subset of $\mathbb{R}^1$, since $\mathbb{R}^1$ is uncountable. Am I right? So, if what I've established so far is correct, then my only question is how to (rigorously) show that $C_1 + C_2$ is dense in $\mathbb{R}^1$?",,"['calculus', 'real-analysis', 'analysis', 'metric-spaces']"
21,Uniqueness of limit point of a Cauchy Sequence,Uniqueness of limit point of a Cauchy Sequence,,"I'm trying to show that the closure of a Cauchy sequence is countable, and the hint for the question is to show that if $(x_n)$ is Cauchy, then it has at most one limit point. I'm a little bit confused between the idea between the idea of a limit point of a set and a limit of a sequence. I know that if $\{x_1,x_2,...\}$ is a set, then $x$ is a limit point if $\forall r >0, \exists x_k \neq x $ such that $x_k \in B_r(x)$. Now I also know what a limit of a sequence is, but since this isn't necessarily a complete metric space we don't know if $(x_n)$ converges. My first thought was to use the fact that Cauchy sequences are bounded, but I don't know where to go with that. Would appreciate any hints!","I'm trying to show that the closure of a Cauchy sequence is countable, and the hint for the question is to show that if $(x_n)$ is Cauchy, then it has at most one limit point. I'm a little bit confused between the idea between the idea of a limit point of a set and a limit of a sequence. I know that if $\{x_1,x_2,...\}$ is a set, then $x$ is a limit point if $\forall r >0, \exists x_k \neq x $ such that $x_k \in B_r(x)$. Now I also know what a limit of a sequence is, but since this isn't necessarily a complete metric space we don't know if $(x_n)$ converges. My first thought was to use the fact that Cauchy sequences are bounded, but I don't know where to go with that. Would appreciate any hints!",,"['analysis', 'cauchy-sequences']"
22,"Continuous function on a closed, bounded set","Continuous function on a closed, bounded set",,"I am having some trouble dealing with the following Let $\Omega \subseteq \mathbb{R}^m$ be closed and bounded, and $f:\Omega \to \mathbb{R}^n$ continuous in $\Omega$. Without using Heine-Borel theorem, show that $f(\Omega)$ is closed. Can I find some sequence $(f(\mathbf{x}_k))$ such that $f(\mathbf{x}_k) \to f(\mathbf{x})$, with $f(\mathbf{x}) \in f(\Omega)$? Is it the right approach? If so, how do I build it or show it exists? Thanks!","I am having some trouble dealing with the following Let $\Omega \subseteq \mathbb{R}^m$ be closed and bounded, and $f:\Omega \to \mathbb{R}^n$ continuous in $\Omega$. Without using Heine-Borel theorem, show that $f(\Omega)$ is closed. Can I find some sequence $(f(\mathbf{x}_k))$ such that $f(\mathbf{x}_k) \to f(\mathbf{x})$, with $f(\mathbf{x}) \in f(\Omega)$? Is it the right approach? If so, how do I build it or show it exists? Thanks!",,"['analysis', 'continuity', 'proof-writing']"
23,Infinitely differentiable functions and their bounds,Infinitely differentiable functions and their bounds,,"We know that if $f(x)$ is analytic, i.e $f(x) \in C^\infty$ in an open set $D$ and $f(x)$ has a convergent Taylor series at any point $x_{0}\in D$ for $x$ in some neighborhood of $x_{0}$, we can write $\left|{\frac {d^{k}f}{dx^{k}}}(x)\right|\leq C^{k+1}k!$ Is there a counterpart for the derivative boundedness for infinite differentiable functions $g(x) \in C^\infty$? $\left|{\frac {d^{k}g}{dx^{k}}}(x)\right|\leq h(k)?$","We know that if $f(x)$ is analytic, i.e $f(x) \in C^\infty$ in an open set $D$ and $f(x)$ has a convergent Taylor series at any point $x_{0}\in D$ for $x$ in some neighborhood of $x_{0}$, we can write $\left|{\frac {d^{k}f}{dx^{k}}}(x)\right|\leq C^{k+1}k!$ Is there a counterpart for the derivative boundedness for infinite differentiable functions $g(x) \in C^\infty$? $\left|{\frac {d^{k}g}{dx^{k}}}(x)\right|\leq h(k)?$",,"['calculus', 'real-analysis', 'analysis', 'asymptotics']"
24,"Riemann-Stieltjes integral problem: $\int_{a} ^{b} g\, d\beta=\int_{a} ^{b} fg\, d\alpha$",Riemann-Stieltjes integral problem:,"\int_{a} ^{b} g\, d\beta=\int_{a} ^{b} fg\, d\alpha","Help, I've been stuck with this for hours, so far I've tried expanding the $\alpha$ integral using the definition of upper and lower integrals U and L but it doesn't seem to be a good way. Let be $\alpha,\ f,\ g\ :[a,b]\to\mathbb{R}$ continous, $\alpha$ non- decreasing and $f(x) \ge 0$. Let be $\beta(x) = \int_a^x f\, d\alpha$. Show $\int_a^b g\ d\beta = \int_a^b gf\, d\alpha$.","Help, I've been stuck with this for hours, so far I've tried expanding the $\alpha$ integral using the definition of upper and lower integrals U and L but it doesn't seem to be a good way. Let be $\alpha,\ f,\ g\ :[a,b]\to\mathbb{R}$ continous, $\alpha$ non- decreasing and $f(x) \ge 0$. Let be $\beta(x) = \int_a^x f\, d\alpha$. Show $\int_a^b g\ d\beta = \int_a^b gf\, d\alpha$.",,"['real-analysis', 'analysis']"
25,Interchanging sums with inner sum in terms of outer sum variable,Interchanging sums with inner sum in terms of outer sum variable,,"I've got a double sum of the form $$\sum_{k=0}^\infty \left( \sum_{i=0}^k a_{i,k}  \right)    $$ and I'm trying to work out how you interchange these two sums. I remember seeing a formula for this in one of my courses, but I can't remember it (nor can I find my notes). As far as I remember, it comes out as two infinite sums. I know when the inner sum isn't in terms of $k$, you can apply Fubini-Tonelli if the inner summands are all non-negative, but here that obviously doesn't make much sense. I tried to draw a grid with the entries and count them in a different order, but I keep getting the sum indexed by $i$ on the inside so I'm a little lost. So, if anyone could prod me in the right direction, that would be great.","I've got a double sum of the form $$\sum_{k=0}^\infty \left( \sum_{i=0}^k a_{i,k}  \right)    $$ and I'm trying to work out how you interchange these two sums. I remember seeing a formula for this in one of my courses, but I can't remember it (nor can I find my notes). As far as I remember, it comes out as two infinite sums. I know when the inner sum isn't in terms of $k$, you can apply Fubini-Tonelli if the inner summands are all non-negative, but here that obviously doesn't make much sense. I tried to draw a grid with the entries and count them in a different order, but I keep getting the sum indexed by $i$ on the inside so I'm a little lost. So, if anyone could prod me in the right direction, that would be great.",,"['sequences-and-series', 'analysis', 'summation']"
26,Radius of convergence and convergence sets of $\sum\limits_n\frac{2n+1}{(n-1)^2}x^n$ and $\sum\limits_n(-1)^n(\sqrt{n+1}-\sqrt{n})x^n$,Radius of convergence and convergence sets of  and,\sum\limits_n\frac{2n+1}{(n-1)^2}x^n \sum\limits_n(-1)^n(\sqrt{n+1}-\sqrt{n})x^n,I want to find the radius of convergence of the following series and the set of $x\in \mathbb{R}$ in which the series converge. $$\sum_{n=2}^{\infty}\frac{2n+1}{(n-1)^2}x^n$$ $$\sum_{n=0}^{\infty}(-1)^n(\sqrt{n+1}-\sqrt{n})x^n$$ To find the radius of convergence we have to compute the limit $\lim_{n\rightarrow \infty}\sqrt[n]{|a_n|}$. Do we do something elese at the first case where the sum starts from $2$ and not from $0$ ? I have done the following: $$\lim_{n\rightarrow \infty}\sqrt[n]{|a_n|}=\lim_{n\rightarrow \infty}\sqrt[n]{\left|\frac{2n+1}{(n-1)^2}x^n\right|}=|x|\lim_{n\rightarrow \infty}\sqrt[n]{\left|\frac{2n+1}{(n-1)^2}\right|}=|x|\lim_{n\rightarrow \infty}\sqrt[n]{\left|\frac{2n+1}{n^2-2n+1}\right|}=|x|\lim_{n\rightarrow \infty}\sqrt[n]{\left|\frac{\frac{2}{n}+\frac{1}{n^2}}{1-\frac{2}{n^2}+\frac{1}{n^2}}\right|}$$ $$\lim_{n\rightarrow \infty}\sqrt[n]{|a_n|}=\lim_{n\rightarrow \infty}\sqrt[n]{\left|(-1)^n(\sqrt{n+1}-\sqrt{n})x^n\right|}=|x|\lim_{n\rightarrow \infty}\sqrt[n]{\left|\sqrt{n+1}-\sqrt{n}\right|}=|x|\lim_{n\rightarrow \infty}\sqrt[n]{\left|\frac{(\sqrt{n+1}-\sqrt{n})(\sqrt{n+1}+\sqrt{n})}{\sqrt{n+1}+\sqrt{n}}\right|}=|x|\lim_{n\rightarrow \infty}\sqrt[n]{\left|\frac{n+1-n}{\sqrt{n+1}+\sqrt{n}}\right|}=|x|\lim_{n\rightarrow \infty}\sqrt[n]{\left|\frac{1}{\sqrt{n+1}+\sqrt{n}}\right|}=|x|\lim_{n\rightarrow \infty}\sqrt[n]{\left|\frac{1}{\sqrt{n+1}+\sqrt{n}}\right|}$$ $$$$ Is this correct so far? How could we continue?,I want to find the radius of convergence of the following series and the set of $x\in \mathbb{R}$ in which the series converge. $$\sum_{n=2}^{\infty}\frac{2n+1}{(n-1)^2}x^n$$ $$\sum_{n=0}^{\infty}(-1)^n(\sqrt{n+1}-\sqrt{n})x^n$$ To find the radius of convergence we have to compute the limit $\lim_{n\rightarrow \infty}\sqrt[n]{|a_n|}$. Do we do something elese at the first case where the sum starts from $2$ and not from $0$ ? I have done the following: $$\lim_{n\rightarrow \infty}\sqrt[n]{|a_n|}=\lim_{n\rightarrow \infty}\sqrt[n]{\left|\frac{2n+1}{(n-1)^2}x^n\right|}=|x|\lim_{n\rightarrow \infty}\sqrt[n]{\left|\frac{2n+1}{(n-1)^2}\right|}=|x|\lim_{n\rightarrow \infty}\sqrt[n]{\left|\frac{2n+1}{n^2-2n+1}\right|}=|x|\lim_{n\rightarrow \infty}\sqrt[n]{\left|\frac{\frac{2}{n}+\frac{1}{n^2}}{1-\frac{2}{n^2}+\frac{1}{n^2}}\right|}$$ $$\lim_{n\rightarrow \infty}\sqrt[n]{|a_n|}=\lim_{n\rightarrow \infty}\sqrt[n]{\left|(-1)^n(\sqrt{n+1}-\sqrt{n})x^n\right|}=|x|\lim_{n\rightarrow \infty}\sqrt[n]{\left|\sqrt{n+1}-\sqrt{n}\right|}=|x|\lim_{n\rightarrow \infty}\sqrt[n]{\left|\frac{(\sqrt{n+1}-\sqrt{n})(\sqrt{n+1}+\sqrt{n})}{\sqrt{n+1}+\sqrt{n}}\right|}=|x|\lim_{n\rightarrow \infty}\sqrt[n]{\left|\frac{n+1-n}{\sqrt{n+1}+\sqrt{n}}\right|}=|x|\lim_{n\rightarrow \infty}\sqrt[n]{\left|\frac{1}{\sqrt{n+1}+\sqrt{n}}\right|}=|x|\lim_{n\rightarrow \infty}\sqrt[n]{\left|\frac{1}{\sqrt{n+1}+\sqrt{n}}\right|}$$ $$$$ Is this correct so far? How could we continue?,,"['real-analysis', 'analysis', 'convergence-divergence']"
27,Let $a_n \rightarrow 0$. Show $\sum\limits_{n=0}^\infty (a_n - a_{n+1}) = a_0$.,Let . Show .,a_n \rightarrow 0 \sum\limits_{n=0}^\infty (a_n - a_{n+1}) = a_0,"Let $a_n \rightarrow 0$. Show $\sum\limits_{n=0}^\infty (a_n - a_{n+1}) = a_0$. In the case that $\sum\limits_{n=0}^\infty a_n$ is convergent, I was able to show that  $$\sum\limits_{n=0}^\infty (a_{n} - a_{n+1}) = \sum\limits_{n=0}^\infty a_n - \sum\limits_{n=0}^\infty a_{n+1} = a_0 + \sum\limits_{n=1}^\infty a_n - \sum\limits_{n=0}^\infty a_{n+1} = a_0 + \sum\limits_{n=0}^\infty a_{n+1} - \sum\limits_{n=0}^\infty a_{n+1} = a_0. $$ For the case where  $\sum\limits_{n=0}^\infty a_n$ is divergent, I am not really sure what to do. I was thinking that since $a_n \rightarrow 0$, we have that for all $\epsilon > 0$, there exists $k \in \mathbb{N}$ such that $|a_n| < \epsilon$ for all $n \geq k$. So, then we get that $$\sum\limits_{n=0}^\infty (a_n - a_{n+1}) = a_0 - a_k + \sum\limits_{n=k}^\infty (a_n - a_{n+1}).$$ This converges if and only if the series of partial sums converges, and we have that  $$S_k =  a_0 - a_k + \sum\limits_{n=k}^K (a_n - a_{n+1}).$$ So, taking the limit as $k$ goes to infinity, we get $a_0$? Why would the last term go away?","Let $a_n \rightarrow 0$. Show $\sum\limits_{n=0}^\infty (a_n - a_{n+1}) = a_0$. In the case that $\sum\limits_{n=0}^\infty a_n$ is convergent, I was able to show that  $$\sum\limits_{n=0}^\infty (a_{n} - a_{n+1}) = \sum\limits_{n=0}^\infty a_n - \sum\limits_{n=0}^\infty a_{n+1} = a_0 + \sum\limits_{n=1}^\infty a_n - \sum\limits_{n=0}^\infty a_{n+1} = a_0 + \sum\limits_{n=0}^\infty a_{n+1} - \sum\limits_{n=0}^\infty a_{n+1} = a_0. $$ For the case where  $\sum\limits_{n=0}^\infty a_n$ is divergent, I am not really sure what to do. I was thinking that since $a_n \rightarrow 0$, we have that for all $\epsilon > 0$, there exists $k \in \mathbb{N}$ such that $|a_n| < \epsilon$ for all $n \geq k$. So, then we get that $$\sum\limits_{n=0}^\infty (a_n - a_{n+1}) = a_0 - a_k + \sum\limits_{n=k}^\infty (a_n - a_{n+1}).$$ This converges if and only if the series of partial sums converges, and we have that  $$S_k =  a_0 - a_k + \sum\limits_{n=k}^K (a_n - a_{n+1}).$$ So, taking the limit as $k$ goes to infinity, we get $a_0$? Why would the last term go away?",,"['sequences-and-series', 'analysis']"
28,Supremum and infimum of an intersection of bounded sets proof.,Supremum and infimum of an intersection of bounded sets proof.,,"Given A, B are bounded subsets of R. Prove the following: $\sup(A∩B) ≤ \min(\sup A,\sup B)$ $\inf(A∩B) ≥ \max(\inf A,\inf B)$ They're quite similar so just solving/proving one of them is good enough for me. I am unable to come up with a ""rigorous"" proof as to why this is true.","Given A, B are bounded subsets of R. Prove the following: They're quite similar so just solving/proving one of them is good enough for me. I am unable to come up with a ""rigorous"" proof as to why this is true.","\sup(A∩B) ≤ \min(\sup A,\sup B) \inf(A∩B) ≥ \max(\inf A,\inf B)","['real-analysis', 'analysis', 'elementary-set-theory']"
29,How can I prove that $\mu$ is a measure? (finite additivity and infinite subadditivity),How can I prove that  is a measure? (finite additivity and infinite subadditivity),\mu,"Let $\Omega$ be a set and $\mathcal{A}$ the $\sigma$-algebra to $\Omega$. Let $\mu : \mathcal{A} \rightarrow [0,\infty]$ be a function with $\mu(\emptyset)=0$, and for sets $A, B \in \mathcal{A}, A \cap B = \emptyset$ let \begin{equation} \mu(A \cup B)=\mu(A)+\mu(B). \end{equation} Also let $\mu$ be $\sigma$-subadditiv for disjoint sets, that means for $(A_{k})_{k\in \mathbb{N}} \subset \mathcal{A}, A_{i} \cap A_{j}=\emptyset, i\neq j $ let \begin{equation} \mu(\bigcup\limits_{k \in \mathbb{N}}A_{k})\le\sum\limits_{k \in \mathbb{N}}\mu(A_{k}). \end{equation} Prove that $\mu$ is a measure.","Let $\Omega$ be a set and $\mathcal{A}$ the $\sigma$-algebra to $\Omega$. Let $\mu : \mathcal{A} \rightarrow [0,\infty]$ be a function with $\mu(\emptyset)=0$, and for sets $A, B \in \mathcal{A}, A \cap B = \emptyset$ let \begin{equation} \mu(A \cup B)=\mu(A)+\mu(B). \end{equation} Also let $\mu$ be $\sigma$-subadditiv for disjoint sets, that means for $(A_{k})_{k\in \mathbb{N}} \subset \mathcal{A}, A_{i} \cap A_{j}=\emptyset, i\neq j $ let \begin{equation} \mu(\bigcup\limits_{k \in \mathbb{N}}A_{k})\le\sum\limits_{k \in \mathbb{N}}\mu(A_{k}). \end{equation} Prove that $\mu$ is a measure.",,"['analysis', 'measure-theory']"
30,$\int_0^1 |\sin n x| dx \ge C >0$?,?,\int_0^1 |\sin n x| dx \ge C >0,I want to prove that  $$ \int_0^1 |\sin n x| dx \ge C >0 $$ for any $n\in \mathbb N$ with $C$ not depending on $n$. This seems so difficult for me to prove.,I want to prove that  $$ \int_0^1 |\sin n x| dx \ge C >0 $$ for any $n\in \mathbb N$ with $C$ not depending on $n$. This seems so difficult for me to prove.,,"['calculus', 'real-analysis', 'integration', 'analysis']"
31,How can I know whether the point is a maximum or minimum without much calculation?,How can I know whether the point is a maximum or minimum without much calculation?,,"Find the maximum and minimum of this function and state whether they   are local or global: $$f: \mathbb{R} \ni x \mapsto \frac{x}{x^{2}+x+1} \in \mathbb{R}$$ \begin{align*}  f'(x)&= \frac{-x^{2}+x}{\left(x^{2}+x+1\right)^{2}}\\ f'(x)&=0 \iff \frac{-x^{2}+x}{\left(x^{2}+x+1\right)^{2}}=0 \iff -x^2+x=0 \iff x(1-x)=0, \end{align*} which gives $x_{1}=0, x_{2}=1$. Here comes the disturbing part, we need to know if these are maximum or minimum and for this we usually used the second derivative. But this would be soo exhausting, I don't even  want think of doing it. There must be an easier way and I remember someone here has even recommended me using monotony somehow. But how can we do this here? Please do tell me, at home I got enough time to use second derivative but surely not in the exam : /","Find the maximum and minimum of this function and state whether they   are local or global: $$f: \mathbb{R} \ni x \mapsto \frac{x}{x^{2}+x+1} \in \mathbb{R}$$ \begin{align*}  f'(x)&= \frac{-x^{2}+x}{\left(x^{2}+x+1\right)^{2}}\\ f'(x)&=0 \iff \frac{-x^{2}+x}{\left(x^{2}+x+1\right)^{2}}=0 \iff -x^2+x=0 \iff x(1-x)=0, \end{align*} which gives $x_{1}=0, x_{2}=1$. Here comes the disturbing part, we need to know if these are maximum or minimum and for this we usually used the second derivative. But this would be soo exhausting, I don't even  want think of doing it. There must be an easier way and I remember someone here has even recommended me using monotony somehow. But how can we do this here? Please do tell me, at home I got enough time to use second derivative but surely not in the exam : /",,"['calculus', 'analysis', 'functions', 'derivatives', 'curves']"
32,"Inverse of the sum $\sum\limits_{j=1}^k (-1)^{k-j}\binom{k}{j} j^{\,k} a_j$",Inverse of the sum,"\sum\limits_{j=1}^k (-1)^{k-j}\binom{k}{j} j^{\,k} a_j","$k\in\mathbb{N}$ The inverse of the sum $$b_k:=\sum\limits_{j=1}^k (-1)^{k-j}\binom{k}{j} j^{\,k} a_j$$ is obviously  $$a_k=\sum\limits_{j=1}^k \binom{k-1}{j-1}\frac{b_j}{k^j}$$ . How can one proof it (in a clear manner)? Thanks in advance. Background of the question: It’s  $$\sum\limits_{k=1}^\infty \frac{b_k}{k!}\int\limits_0^\infty \left(\frac{t}{e^t-1}\right)^k dt =\sum\limits_{k=1}^\infty \frac{a_k}{k}$$  with $\,\displaystyle b_k:=\sum\limits_{j=1}^k (-1)^{k-j}\binom{k}{j}j^{\,k}a_j $. Note: A special case is $\displaystyle a_k:=\frac{1}{k^n}$ with $n\in\mathbb{N}$ and therefore $\,\displaystyle b_k=\sum\limits_{j=1}^k (-1)^{k-j}\binom{k}{j}j^{\,k-n}$ (see Stirling numbers of the second kind)  $$\sum\limits_{k=1}^n \frac{b_k}{k!}\int\limits_0^\infty \left(\frac{t}{e^t-1}\right)^k dt =\zeta(n+1)$$ and the invers equation can be found in A formula for $\int\limits_0^\infty (\frac{x}{e^x-1})^n dx$ .","$k\in\mathbb{N}$ The inverse of the sum $$b_k:=\sum\limits_{j=1}^k (-1)^{k-j}\binom{k}{j} j^{\,k} a_j$$ is obviously  $$a_k=\sum\limits_{j=1}^k \binom{k-1}{j-1}\frac{b_j}{k^j}$$ . How can one proof it (in a clear manner)? Thanks in advance. Background of the question: It’s  $$\sum\limits_{k=1}^\infty \frac{b_k}{k!}\int\limits_0^\infty \left(\frac{t}{e^t-1}\right)^k dt =\sum\limits_{k=1}^\infty \frac{a_k}{k}$$  with $\,\displaystyle b_k:=\sum\limits_{j=1}^k (-1)^{k-j}\binom{k}{j}j^{\,k}a_j $. Note: A special case is $\displaystyle a_k:=\frac{1}{k^n}$ with $n\in\mathbb{N}$ and therefore $\,\displaystyle b_k=\sum\limits_{j=1}^k (-1)^{k-j}\binom{k}{j}j^{\,k-n}$ (see Stirling numbers of the second kind)  $$\sum\limits_{k=1}^n \frac{b_k}{k!}\int\limits_0^\infty \left(\frac{t}{e^t-1}\right)^k dt =\zeta(n+1)$$ and the invers equation can be found in A formula for $\int\limits_0^\infty (\frac{x}{e^x-1})^n dx$ .",,"['analysis', 'summation', 'binomial-coefficients', 'analytic-combinatorics']"
33,Checking if differential form $y^2xdx + ydy$ is exact,Checking if differential form  is exact,y^2xdx + ydy,"I would like to check if the differential form $w = y^2xdx + ydy$ is exact. If $w$ would be exact we would have a function $f$ with $\partial f/\partial y =y$, thus $f = 1/2 y^2 +c$. $f$ would need to satisfy $\partial f/\partial x =  y^2x$ as well, so $c = 1/2 y^2 x^2 + c'$, where $c'$ may depend on $y$. Now differentiating our found expression for $f$ with respect to $y$ again we could say how $c'$ must look like, since we know that the differentiative has to be $y$ once again, and so on. I am struggling to get this argument to an end and to conclude, that $w$ cannot be exact. Any ideas to finish this? As an alternative I think the following should work: $w$ is not exact, since $w$ is not closed: $\partial w_1 /\partial y = 2yx \neq 0 = \partial w_2 / \partial x$ (for $x,y \neq 0$). Is this correct?","I would like to check if the differential form $w = y^2xdx + ydy$ is exact. If $w$ would be exact we would have a function $f$ with $\partial f/\partial y =y$, thus $f = 1/2 y^2 +c$. $f$ would need to satisfy $\partial f/\partial x =  y^2x$ as well, so $c = 1/2 y^2 x^2 + c'$, where $c'$ may depend on $y$. Now differentiating our found expression for $f$ with respect to $y$ again we could say how $c'$ must look like, since we know that the differentiative has to be $y$ once again, and so on. I am struggling to get this argument to an end and to conclude, that $w$ cannot be exact. Any ideas to finish this? As an alternative I think the following should work: $w$ is not exact, since $w$ is not closed: $\partial w_1 /\partial y = 2yx \neq 0 = \partial w_2 / \partial x$ (for $x,y \neq 0$). Is this correct?",,"['real-analysis', 'analysis']"
34,Show that the following equation has got exactly one solution for each $C>0$,Show that the following equation has got exactly one solution for each,C>0,"Show that the equation $$C=\left ( 1+x+\frac{1}{2}x^{2} \right)*e^{-x}$$ has got exactly one solution for each $C>0$. Alright so I did it like that but not sure if it's correct: $0<\left ( 1+x+\frac{1}{2}x^{2} \right)*e^{-x}$ |: $e^{-x}$ $0<\left ( 1+x+\frac{1}{2}x^{2} \right)$ | *$2$ $0<\left ( 2+2x+x^{2} \right)$ $0<x^{2}+2x+2$ $0 < \left ( x+1 \right)^{2}+1$ $0 < x+1+\sqrt{1}$ $x > -2$ To be honest, I'm not sure if my preparation is correct at all. ""show(...) exactly one solution for each $C>0$"" confusing me. Again, this is no homework, only practice for me. If anyone wants I can upload the pdf (example for our exam) here.","Show that the equation $$C=\left ( 1+x+\frac{1}{2}x^{2} \right)*e^{-x}$$ has got exactly one solution for each $C>0$. Alright so I did it like that but not sure if it's correct: $0<\left ( 1+x+\frac{1}{2}x^{2} \right)*e^{-x}$ |: $e^{-x}$ $0<\left ( 1+x+\frac{1}{2}x^{2} \right)$ | *$2$ $0<\left ( 2+2x+x^{2} \right)$ $0<x^{2}+2x+2$ $0 < \left ( x+1 \right)^{2}+1$ $0 < x+1+\sqrt{1}$ $x > -2$ To be honest, I'm not sure if my preparation is correct at all. ""show(...) exactly one solution for each $C>0$"" confusing me. Again, this is no homework, only practice for me. If anyone wants I can upload the pdf (example for our exam) here.",,"['calculus', 'analysis', 'functions']"
35,Show that there is a subsequence of $(f_n)_n$ that converges to $f$ almost everywhere.,Show that there is a subsequence of  that converges to  almost everywhere.,(f_n)_n f,"Let $(X,\mathcal{B}, \mu)$ be a measure space and assume the sequence $(f_n)_n$ converges to $f$ in $L^p(\mu)$, where $1\leq p<\infty$. Show that there is a subsequence of $(f_n)_n$ that converges to $f$ almost everywhere. Isn't it true that for all subsequence of $(f_n)_n$? Attempt: Since $f_n\to f$ in $L^p$, for any $\epsilon>0$, there exists $N\in\mathbb{N}$ such that for all $n,m\leq N$, $\|f_m-f_n\|_p<\epsilon /2$ or $\|f_n-f\|_p<\epsilon /2$ Let $(f_{n_k})_k$ be any subsequence of $(f_n)_n$. Then $$\|f_{n_k}-f\|_p\leq \|f_{n_k}-f_n\|_p+\|f_n-f\|_p< \epsilon /2+\epsilon /2=\epsilon.$$ I don't know what the wrong is here. Can anyone check my proof? Thanks!","Let $(X,\mathcal{B}, \mu)$ be a measure space and assume the sequence $(f_n)_n$ converges to $f$ in $L^p(\mu)$, where $1\leq p<\infty$. Show that there is a subsequence of $(f_n)_n$ that converges to $f$ almost everywhere. Isn't it true that for all subsequence of $(f_n)_n$? Attempt: Since $f_n\to f$ in $L^p$, for any $\epsilon>0$, there exists $N\in\mathbb{N}$ such that for all $n,m\leq N$, $\|f_m-f_n\|_p<\epsilon /2$ or $\|f_n-f\|_p<\epsilon /2$ Let $(f_{n_k})_k$ be any subsequence of $(f_n)_n$. Then $$\|f_{n_k}-f\|_p\leq \|f_{n_k}-f_n\|_p+\|f_n-f\|_p< \epsilon /2+\epsilon /2=\epsilon.$$ I don't know what the wrong is here. Can anyone check my proof? Thanks!",,"['real-analysis', 'analysis', 'measure-theory']"
36,harmonic series as product over primes,harmonic series as product over primes,,"If we consider the harmonic series written as such $$\sum_{n=1}^\infty\,\frac{1}{n} = \prod_{\substack{p\text{ prime}}} (1+\frac{1}p+\frac{1}{p^2}+\frac{1}{p^3}+…) \tag{1}\label{1} $$ I don’t understand how it can be written in this form $$\prod_{\substack{p\text{ prime}}} \frac{1}{1−\frac{1}p} \tag{2}\label{2}$$ In the first infinite product (\ref{1}) we have a bunch of power of primes, which are needed to write all the possible integer as a product of primes, but these power of prime don’t seem to appear in (\ref{2}). How do you get from (\ref{1}) to (\ref{2})?","If we consider the harmonic series written as such $$\sum_{n=1}^\infty\,\frac{1}{n} = \prod_{\substack{p\text{ prime}}} (1+\frac{1}p+\frac{1}{p^2}+\frac{1}{p^3}+…) \tag{1}\label{1} $$ I don’t understand how it can be written in this form $$\prod_{\substack{p\text{ prime}}} \frac{1}{1−\frac{1}p} \tag{2}\label{2}$$ In the first infinite product (\ref{1}) we have a bunch of power of primes, which are needed to write all the possible integer as a product of primes, but these power of prime don’t seem to appear in (\ref{2}). How do you get from (\ref{1}) to (\ref{2})?",,"['calculus', 'analysis']"
37,"How to show that $x \sin \frac{\pi}{x} > \pi \cos \frac{\pi}{x}$ for $x \in (1, \infty)$?",How to show that  for ?,"x \sin \frac{\pi}{x} > \pi \cos \frac{\pi}{x} x \in (1, \infty)","How could I prove that $x \sin \frac{\pi}{x} > \pi \cos \frac{\pi}{x}$ for $x \in (1, \infty)$? Dividing both sides through by $x \sin \frac{\pi}{x}$ and letting $y = \frac{\pi}{x}$ gives the inequality $1> y \cot y$, if $y \in (0,\pi)$, but then I don't know how to go ahead and actually prove that?","How could I prove that $x \sin \frac{\pi}{x} > \pi \cos \frac{\pi}{x}$ for $x \in (1, \infty)$? Dividing both sides through by $x \sin \frac{\pi}{x}$ and letting $y = \frac{\pi}{x}$ gives the inequality $1> y \cot y$, if $y \in (0,\pi)$, but then I don't know how to go ahead and actually prove that?",,"['calculus', 'analysis']"
38,$\lim\limits_{n\to \infty}\sum_{k=1}^{\infty}2^{-k}\sin(k/n)=0$,,\lim\limits_{n\to \infty}\sum_{k=1}^{\infty}2^{-k}\sin(k/n)=0,"I want to show that $$\lim\limits_{n\to \infty}\sum_{k=1}^{\infty}2^{-k}\sin(k/n)=0$$ I first thought if I can change the order of limit, it can be easy to show that. But I found that there exists counter example here . I know that $\sum_{k=1}^{\infty}2^{-k}\sin(k/n)$ absolutely converges, but how to show the result?","I want to show that $$\lim\limits_{n\to \infty}\sum_{k=1}^{\infty}2^{-k}\sin(k/n)=0$$ I first thought if I can change the order of limit, it can be easy to show that. But I found that there exists counter example here . I know that $\sum_{k=1}^{\infty}2^{-k}\sin(k/n)$ absolutely converges, but how to show the result?",,"['sequences-and-series', 'analysis']"
39,Convergent + divergent $\to$ divergent,Convergent + divergent  divergent,\to,"Given sequences $(x_n)$, convergent, but $(y_n)$ is divergent, then $(x_n + y_n)$ is divergent. I am confident that it is true, but having trouble getting the formalities correct. I have tried proof by contradiction, i.e. assuming  $\forall \varepsilon > 0 \; \exists N \in \mathbf{N}:$ $$ |(x_n + y_n) - L| < \varepsilon \qquad  \forall n > N.$$ This seems to lead me nowhere. Equivalently, trying to find an $\varepsilon$ such that for all $N$ $|(x_n + y_n) - L| \geq \varepsilon$ have not gotten me any further.  Any hints are appreciated.","Given sequences $(x_n)$, convergent, but $(y_n)$ is divergent, then $(x_n + y_n)$ is divergent. I am confident that it is true, but having trouble getting the formalities correct. I have tried proof by contradiction, i.e. assuming  $\forall \varepsilon > 0 \; \exists N \in \mathbf{N}:$ $$ |(x_n + y_n) - L| < \varepsilon \qquad  \forall n > N.$$ This seems to lead me nowhere. Equivalently, trying to find an $\varepsilon$ such that for all $N$ $|(x_n + y_n) - L| \geq \varepsilon$ have not gotten me any further.  Any hints are appreciated.",,['real-analysis']
40,Integrating over a sequence of sets $A_n$ with $\mu(A_n)\to0$,Integrating over a sequence of sets  with,A_n \mu(A_n)\to0,"I am going through the proof of the following. Let $(X,\mu)$ be a measure space and $f\colon X\to\overline{\mathbb R}$ be a measurable function with finite integral. If $A_1,A_2,\dots$ are $\mu$-measurable and $\lim_{n\to\infty}\mu(A_n)=0$, then $$\lim_{n\to\infty}\int_{A_n}f\,d\mu=0.$$ Proof. Let $E_N=\{x\mid f(x)\leq N\}$ and define $f_N=\chi_{E_N}f$. Clearly   $f_N\leq f$ and since $f$ has finite integral it is finite a.e., and   it follows that $\lim_{N\to\infty} f_N=f$ a.e.; then,   $\lim_{N\to\infty}(f-f_N)=0$ a.e.. Now $f-f_N$ is dominated by   $|f|+|f_N|$ which has finite integral by additivity and hence by the   DCT we get $\int (f-f_N)\,d\mu\to0$ as $N\to\infty$. Now by additivity and the fact that $f_N\leq N$ for all $N$ we have    $$\int_{A_n}f\,d\mu=\int_{A_n}(f-f_N)\,d\mu+\int_{A_n} f_N\,d\mu\leq  \int_{A_n}(f-f_N)\,d\mu+N\mu(A_n)$$ Taking $N\to\infty$ and noting that $\int_{A_n}(f-f_N)\,d\mu\leq \int(f-f_N)\, d\mu$ gives $$\int_{A_n}f\,d\mu\leq 0+\infty\cdot\mu(A_n).$$ Taking $n\to\infty$ gives the result because $\infty\cdot 0=0$. Now I don't really see how the last line proves anything. How do we know that $f$ is not negative? As I see, we have only shown that the limit is not positive. It also makes me a little uncomfortable that we write $\infty\cdot\mu(A_n)$, I mean I know that in the extended reals $0\cdot \infty=0$ but is this really rigorous? Any ideas?","I am going through the proof of the following. Let $(X,\mu)$ be a measure space and $f\colon X\to\overline{\mathbb R}$ be a measurable function with finite integral. If $A_1,A_2,\dots$ are $\mu$-measurable and $\lim_{n\to\infty}\mu(A_n)=0$, then $$\lim_{n\to\infty}\int_{A_n}f\,d\mu=0.$$ Proof. Let $E_N=\{x\mid f(x)\leq N\}$ and define $f_N=\chi_{E_N}f$. Clearly   $f_N\leq f$ and since $f$ has finite integral it is finite a.e., and   it follows that $\lim_{N\to\infty} f_N=f$ a.e.; then,   $\lim_{N\to\infty}(f-f_N)=0$ a.e.. Now $f-f_N$ is dominated by   $|f|+|f_N|$ which has finite integral by additivity and hence by the   DCT we get $\int (f-f_N)\,d\mu\to0$ as $N\to\infty$. Now by additivity and the fact that $f_N\leq N$ for all $N$ we have    $$\int_{A_n}f\,d\mu=\int_{A_n}(f-f_N)\,d\mu+\int_{A_n} f_N\,d\mu\leq  \int_{A_n}(f-f_N)\,d\mu+N\mu(A_n)$$ Taking $N\to\infty$ and noting that $\int_{A_n}(f-f_N)\,d\mu\leq \int(f-f_N)\, d\mu$ gives $$\int_{A_n}f\,d\mu\leq 0+\infty\cdot\mu(A_n).$$ Taking $n\to\infty$ gives the result because $\infty\cdot 0=0$. Now I don't really see how the last line proves anything. How do we know that $f$ is not negative? As I see, we have only shown that the limit is not positive. It also makes me a little uncomfortable that we write $\infty\cdot\mu(A_n)$, I mean I know that in the extended reals $0\cdot \infty=0$ but is this really rigorous? Any ideas?",,"['real-analysis', 'analysis', 'measure-theory', 'proof-explanation']"
41,"$\mu$ is a $\sigma-$finite measure on and $\{E_n\}$ measurable sets. When $\nu(E)=\sum \mu(E\cap E_n)$, is $\nu$ is $\sigma$-finite?","is a finite measure on and  measurable sets. When , is  is -finite?",\mu \sigma- \{E_n\} \nu(E)=\sum \mu(E\cap E_n) \nu \sigma,"My question stems from the following problem. Suppose $\mu$ is a $\sigma-$finite measure on $(X,M)$ and $\{E_n\}$ a sequence of measurable sets. Define $\nu$ on $M$ by $\nu(E)=\sum \mu(E\cap E_n)$. Find the Radon-Nikodym derivative $\frac{d\nu}{d\mu}$. For all measurable set $E$, $\nu(E)=\sum \mu(E\cap E_n)=\sum \int \chi_{E\cap E_n} d\mu = \sum \int_E \chi_{E_n}d\mu=\int (\sum \chi_{E_n}) d\mu$, so that $\frac{d\nu}{d\mu}=\sum \chi_{E_n}$. However, for the Radon-Nikodym derivative to exist, I need the fact that $\nu$ is $\sigma-$finite, but I can't show how this is possible. Is $\nu$ sigma finite even when the $E_n$'s are not disjoint? And how can I show this in that case? I would greatly appreciate any help.","My question stems from the following problem. Suppose $\mu$ is a $\sigma-$finite measure on $(X,M)$ and $\{E_n\}$ a sequence of measurable sets. Define $\nu$ on $M$ by $\nu(E)=\sum \mu(E\cap E_n)$. Find the Radon-Nikodym derivative $\frac{d\nu}{d\mu}$. For all measurable set $E$, $\nu(E)=\sum \mu(E\cap E_n)=\sum \int \chi_{E\cap E_n} d\mu = \sum \int_E \chi_{E_n}d\mu=\int (\sum \chi_{E_n}) d\mu$, so that $\frac{d\nu}{d\mu}=\sum \chi_{E_n}$. However, for the Radon-Nikodym derivative to exist, I need the fact that $\nu$ is $\sigma-$finite, but I can't show how this is possible. Is $\nu$ sigma finite even when the $E_n$'s are not disjoint? And how can I show this in that case? I would greatly appreciate any help.",,"['real-analysis', 'analysis', 'measure-theory', 'derivatives']"
42,Test whether $f$ is constant or not.,Test whether  is constant or not.,f,"Let , $f:[0,1]\to \mathbb R$ be continuous and $f(x^2)=f(x)$ for all $x\in [0,1]$. Then which are TRUE ? (A) $f$ is a constant function. (B) $f$ is differentiable function. (C) $f$ is uniformly continuous. (D) $f(x)\ge 0$ for all $x\in [0,1]$. As the domain $[0,1]$ is compact so the function $f$ is uniformly continuous. But I am unable to test the other options..Please help.","Let , $f:[0,1]\to \mathbb R$ be continuous and $f(x^2)=f(x)$ for all $x\in [0,1]$. Then which are TRUE ? (A) $f$ is a constant function. (B) $f$ is differentiable function. (C) $f$ is uniformly continuous. (D) $f(x)\ge 0$ for all $x\in [0,1]$. As the domain $[0,1]$ is compact so the function $f$ is uniformly continuous. But I am unable to test the other options..Please help.",,"['real-analysis', 'analysis', 'continuity']"
43,Where do these p-adic identities come from?,Where do these p-adic identities come from?,,"I was reading this article ( http://www.asiapacific-mathnews.com/03/0304/0001_0006.pdf ) to see some applications of $p$-adic numbers outside mathematics, and came across these two identities: $\sum_{n=0}^\infty (-1)^n n!(n+2) = 1$ and $\sum_{n=0}^\infty (-1)^n n!(n^2-5) = -3$. Since no $p$ was stated, I figured there must be some trick that shows these are the values in any $\mathbb{Q}_p$, but I couldn't find anything by just looking at some partial sums, unless I missed something. Also, where do these kinds of $p$-adic identities come from? It's easy enough to write down a series that converges in $\mathbb{Q}_p$, but writing down one with a nice limit seems a bit harder.","I was reading this article ( http://www.asiapacific-mathnews.com/03/0304/0001_0006.pdf ) to see some applications of $p$-adic numbers outside mathematics, and came across these two identities: $\sum_{n=0}^\infty (-1)^n n!(n+2) = 1$ and $\sum_{n=0}^\infty (-1)^n n!(n^2-5) = -3$. Since no $p$ was stated, I figured there must be some trick that shows these are the values in any $\mathbb{Q}_p$, but I couldn't find anything by just looking at some partial sums, unless I missed something. Also, where do these kinds of $p$-adic identities come from? It's easy enough to write down a series that converges in $\mathbb{Q}_p$, but writing down one with a nice limit seems a bit harder.",,"['analysis', 'p-adic-number-theory']"
44,Find sum of $1+\frac{1}{3}+\frac{1}{5}-\frac{1}{2}-\frac{1}{4}-\frac{1}{6}+\frac{1}{7}+\frac{1}{9}+\frac{1}{11}-\cdots$,Find sum of,1+\frac{1}{3}+\frac{1}{5}-\frac{1}{2}-\frac{1}{4}-\frac{1}{6}+\frac{1}{7}+\frac{1}{9}+\frac{1}{11}-\cdots,"Find the sum of the following series :  $$1-\frac{1}{2}-\frac{1}{4}+\frac{1}{3}-\frac{1}{6}-\frac{1}{8}+\frac{1}{5}-\frac{1}{10}-\frac{1}{12}+\cdots$$ and  $$1+\frac{1}{3}+\frac{1}{5}-\frac{1}{2}-\frac{1}{4}-\frac{1}{6}+\frac{1}{7}+\frac{1}{9}+\frac{1}{11}-\cdots$$ For the first series , we have , $$(1-\frac{1}{2})-\frac{1}{4}+(\frac{1}{3}-\frac{1}{6})-\frac{1}{8}+(\frac{1}{5}-\frac{1}{10})-\frac{1}{12}+\cdots$$ $$=\frac{1}{2}-\frac{1}{4}+\frac{1}{6}-\frac{1}{8}+\cdots=\frac{1}{2}\log 2$$ But I am unable to set the second series to find its sum..Please help.","Find the sum of the following series :  $$1-\frac{1}{2}-\frac{1}{4}+\frac{1}{3}-\frac{1}{6}-\frac{1}{8}+\frac{1}{5}-\frac{1}{10}-\frac{1}{12}+\cdots$$ and  $$1+\frac{1}{3}+\frac{1}{5}-\frac{1}{2}-\frac{1}{4}-\frac{1}{6}+\frac{1}{7}+\frac{1}{9}+\frac{1}{11}-\cdots$$ For the first series , we have , $$(1-\frac{1}{2})-\frac{1}{4}+(\frac{1}{3}-\frac{1}{6})-\frac{1}{8}+(\frac{1}{5}-\frac{1}{10})-\frac{1}{12}+\cdots$$ $$=\frac{1}{2}-\frac{1}{4}+\frac{1}{6}-\frac{1}{8}+\cdots=\frac{1}{2}\log 2$$ But I am unable to set the second series to find its sum..Please help.",,"['real-analysis', 'sequences-and-series', 'analysis']"
45,Prove $S^1$ is not homeomorphic to $S^2$ using connectedness,Prove  is not homeomorphic to  using connectedness,S^1 S^2,"I have to prove that the unit circle $S^1$ is not homeomorphic to the sphere $S^2$ using connectedness. Intuitively I know this is true, but I'm not sure how to prove this.. Can someone help me?","I have to prove that the unit circle $S^1$ is not homeomorphic to the sphere $S^2$ using connectedness. Intuitively I know this is true, but I'm not sure how to prove this.. Can someone help me?",,"['general-topology', 'analysis', 'continuity', 'connectedness']"
46,Finding a one-form $\lambda$ such that $d\lambda = \omega$,Finding a one-form  such that,\lambda d\lambda = \omega,Let $\omega = 2xz dy\wedge dz + dz\wedge dx -(z^2 + e^x)dx\wedge dy$. We have just started out with differential forms and need to find a one-form $\lambda$ so that $d\lambda = \omega$. \begin{gather} \lambda = a_1dx + a_2dy + a_3dz\\ d\lambda = da_1\wedge dx + da_2\wedge dx + da_3\wedge dx \end{gather} I just cant figure out suitable functions $a_i$. I tried guessing $\lambda$ but I would like to know how to find $\lambda$ in general.,Let $\omega = 2xz dy\wedge dz + dz\wedge dx -(z^2 + e^x)dx\wedge dy$. We have just started out with differential forms and need to find a one-form $\lambda$ so that $d\lambda = \omega$. \begin{gather} \lambda = a_1dx + a_2dy + a_3dz\\ d\lambda = da_1\wedge dx + da_2\wedge dx + da_3\wedge dx \end{gather} I just cant figure out suitable functions $a_i$. I tried guessing $\lambda$ but I would like to know how to find $\lambda$ in general.,,"['analysis', 'differential-forms']"
47,Baby Rudin exercise 1.6: Is this the proof Rudin expects?,Baby Rudin exercise 1.6: Is this the proof Rudin expects?,,"$\bf Exercise\, 1.6$ Fix $b>1.$ Prove that if $m,n,p,q$ are integers, $n>0,q>0$ and $r=m/n=p/q$, then $$ (b^m)^{1/n}=(b^p)^{1/q}. $$ I'm not really sure what I can assume and what  I can't assume, I think that all I need is $(x^y)^z=x^{yz}$ for integers $y,z$, but I'm not sure how to prove this (I don't even know what the expected definition of exponentiation is!). Attempt $$ \begin{align} \left((b^m)^{1/n}\right)^n&=b^m\quad \text{By Theorem 1.21 (I think).}\\ \left((b^m)^{1/n}\right)^{nq}&=b^{mq}\quad \text{Here I use $(x^y)^z=x^{yz}$.}\\ \left((b^m)^{1/n}\right)^{nq}&=b^{np}\quad \text{As $mq=np$.}\\ \left((b^m)^{1/n}\right)^{qn}&=b^{pn}\quad  \end{align} $$ Then, taking $n$-th and then $q$-th roots, we get our desired result (I think this is possible, again, by theorem $1.21$, but I'm not sure). Could someone check my proof and tell me which facts about exponentiation we are allowed to assume and use for these kind of proofs?","$\bf Exercise\, 1.6$ Fix $b>1.$ Prove that if $m,n,p,q$ are integers, $n>0,q>0$ and $r=m/n=p/q$, then $$ (b^m)^{1/n}=(b^p)^{1/q}. $$ I'm not really sure what I can assume and what  I can't assume, I think that all I need is $(x^y)^z=x^{yz}$ for integers $y,z$, but I'm not sure how to prove this (I don't even know what the expected definition of exponentiation is!). Attempt $$ \begin{align} \left((b^m)^{1/n}\right)^n&=b^m\quad \text{By Theorem 1.21 (I think).}\\ \left((b^m)^{1/n}\right)^{nq}&=b^{mq}\quad \text{Here I use $(x^y)^z=x^{yz}$.}\\ \left((b^m)^{1/n}\right)^{nq}&=b^{np}\quad \text{As $mq=np$.}\\ \left((b^m)^{1/n}\right)^{qn}&=b^{pn}\quad  \end{align} $$ Then, taking $n$-th and then $q$-th roots, we get our desired result (I think this is possible, again, by theorem $1.21$, but I'm not sure). Could someone check my proof and tell me which facts about exponentiation we are allowed to assume and use for these kind of proofs?",,"['real-analysis', 'analysis', 'proof-verification']"
48,Transforming the integral $\int_0^\infty e^{-x^2}\sin(x) dx$ into a specific sum,Transforming the integral  into a specific sum,\int_0^\infty e^{-x^2}\sin(x) dx,"Using the series representation of $\sin x$, I want to prove that: $$\int_0^\infty e^{-x^2} \sin(x) dx = \frac{1}{2} \sum_{k=0}^\infty (-1)^k \frac{k!}{(2k+1)!}$$ My attempt: I've started by substituting $\sin$ with his power series representation: $$\int_0^\infty e^{-x^2} \sin(x) dx = \int_0^\infty e^{-x^2} \sum_{k=0}^\infty \frac{(-1)^k x^{1+2k}}{(1+2k)!} dx \space\space\space\space\space\space\space(1)$$ I don't really know how to continue from there. It seems obvious that I also need to replace $e^{-x^2}$ with it's power series representation $\sum_{k=0}^\infty \frac{({-x^2})^k}{k!}$, but should I pull $e^{-x^2}$ into the sum that I replaced $\sin$ with, before I insert this second sum? Or should I rather keep these two sums seperate? Either way, I'll end up with two sums. One of the theorems that I can use states that, given a sequence $f_n: \mathbb{R} \to [0, ∞]$ of measurable functions, I have that $$\int_\mathbb{R} (\sum_{k=0}^\infty f_n) d\mu = \sum_{k=0}^\infty \int_\mathbb{R} f_n d\mu$$ So what I would want to do, is rearrange the expression $(1)$ so that I can use this theorem and pull the integral into the sum, in order to then evaluate the integral by integrating each of the $f_n$. But the problem is that $\frac{(-1)^k x^{1+2k}}{(1+2k)!}$ is negative for each odd $k \in \mathbb{N}$, and therefore, $\frac{(-1)^k x^{1+2k}}{(1+2k)!}$ isn't always $≥ 0$, as I needed to be, if I wanted to use the theorem. (And since $e^{-x^2} ≥ 0 \forall x \in \mathbb{R}$, the product of $\frac{(-1)^k x^{1+2k}}{(1+2k)!}$ and $e^{-x^2}$ would give me the same problem, would it?) Thanks in advance.","Using the series representation of $\sin x$, I want to prove that: $$\int_0^\infty e^{-x^2} \sin(x) dx = \frac{1}{2} \sum_{k=0}^\infty (-1)^k \frac{k!}{(2k+1)!}$$ My attempt: I've started by substituting $\sin$ with his power series representation: $$\int_0^\infty e^{-x^2} \sin(x) dx = \int_0^\infty e^{-x^2} \sum_{k=0}^\infty \frac{(-1)^k x^{1+2k}}{(1+2k)!} dx \space\space\space\space\space\space\space(1)$$ I don't really know how to continue from there. It seems obvious that I also need to replace $e^{-x^2}$ with it's power series representation $\sum_{k=0}^\infty \frac{({-x^2})^k}{k!}$, but should I pull $e^{-x^2}$ into the sum that I replaced $\sin$ with, before I insert this second sum? Or should I rather keep these two sums seperate? Either way, I'll end up with two sums. One of the theorems that I can use states that, given a sequence $f_n: \mathbb{R} \to [0, ∞]$ of measurable functions, I have that $$\int_\mathbb{R} (\sum_{k=0}^\infty f_n) d\mu = \sum_{k=0}^\infty \int_\mathbb{R} f_n d\mu$$ So what I would want to do, is rearrange the expression $(1)$ so that I can use this theorem and pull the integral into the sum, in order to then evaluate the integral by integrating each of the $f_n$. But the problem is that $\frac{(-1)^k x^{1+2k}}{(1+2k)!}$ is negative for each odd $k \in \mathbb{N}$, and therefore, $\frac{(-1)^k x^{1+2k}}{(1+2k)!}$ isn't always $≥ 0$, as I needed to be, if I wanted to use the theorem. (And since $e^{-x^2} ≥ 0 \forall x \in \mathbb{R}$, the product of $\frac{(-1)^k x^{1+2k}}{(1+2k)!}$ and $e^{-x^2}$ would give me the same problem, would it?) Thanks in advance.",,"['real-analysis', 'integration', 'analysis', 'improper-integrals']"
49,"Show that if $0\lt L\lt R\lt+\infty$, then $\int_L^R\frac{\sin x}x dx\lt \frac2L$","Show that if , then",0\lt L\lt R\lt+\infty \int_L^R\frac{\sin x}x dx\lt \frac2L,"Show that if $0\lt L\lt R\lt+\infty$, then $\int_L^R\frac{\sin x}x dx\lt \frac2L$ My Work: a) Integration by parts: $$\int_L^R\frac{\sin x}x dx$$ using the formula $\int a'b=ab-\int ab'$, there is no viable selection for a or b because no amount of integration or derivation will make one of the two functions cancel out b) Substitution: $u(x)=\frac1x$ $$\int_{u(L)}^{u(R)}u \sin{\frac1u} dx$$ From here we can try IBP again, but I believe that this will also prove to be fruitless because of the term $\sin{\frac1u}$ I don't know of any other method I could use to integrate this function and I'm rather puzzled on what to do next. Any help is appreciated!!","Show that if $0\lt L\lt R\lt+\infty$, then $\int_L^R\frac{\sin x}x dx\lt \frac2L$ My Work: a) Integration by parts: $$\int_L^R\frac{\sin x}x dx$$ using the formula $\int a'b=ab-\int ab'$, there is no viable selection for a or b because no amount of integration or derivation will make one of the two functions cancel out b) Substitution: $u(x)=\frac1x$ $$\int_{u(L)}^{u(R)}u \sin{\frac1u} dx$$ From here we can try IBP again, but I believe that this will also prove to be fruitless because of the term $\sin{\frac1u}$ I don't know of any other method I could use to integrate this function and I'm rather puzzled on what to do next. Any help is appreciated!!",,"['calculus', 'integration', 'analysis', 'inequality']"
50,Why is this claim about limits true,Why is this claim about limits true,,"I didn't understand why this claim from wikipedia is true ...More specifically, when $f$ is applied to any input sufficiently close to $p$, the output value is forced arbitrarily close to $L$. Is not the contrary? we have formally from the definition of limits: for every $\epsilon>0$ we have one $\delta>0$ such that   $0<|x-p|<\delta\implies |f(x)-L| < \epsilon$. So I think, the best claim is ... More specifically, any output sufficiently close to $L$, the input   value is forced arbitrarily close to $p$. So my question is why the wikipedia claim is true?","I didn't understand why this claim from wikipedia is true ...More specifically, when $f$ is applied to any input sufficiently close to $p$, the output value is forced arbitrarily close to $L$. Is not the contrary? we have formally from the definition of limits: for every $\epsilon>0$ we have one $\delta>0$ such that   $0<|x-p|<\delta\implies |f(x)-L| < \epsilon$. So I think, the best claim is ... More specifically, any output sufficiently close to $L$, the input   value is forced arbitrarily close to $p$. So my question is why the wikipedia claim is true?",,"['calculus', 'analysis']"
51,Convergence of series of positive terms: $\sum_{n=2}^{\infty}\frac{1}{{(\log n)}^{p}}$,Convergence of series of positive terms:,\sum_{n=2}^{\infty}\frac{1}{{(\log n)}^{p}},"I have applied Cauchy condensation test for to test the convergence the series $\sum_{n=2}^{\infty}\frac{1}{{(\log n)}^{p}}$, where p is constant, I got $\frac{1}{{\log 2}^{p}}\sum_{k=1}^{\infty}\frac{2^{k}}{k^{p}}$ . I do not understand for which value of p such that  the original series is convergent. Also have used Cauchy integral test but did not solve the improper integral $\int_{2}^{\infty} {\frac{1}{(\log{x})^{p}}}dx$.  I do not understand the convergent or not, if it convergent what is the value of p will be.   Please some one help me. Thanks","I have applied Cauchy condensation test for to test the convergence the series $\sum_{n=2}^{\infty}\frac{1}{{(\log n)}^{p}}$, where p is constant, I got $\frac{1}{{\log 2}^{p}}\sum_{k=1}^{\infty}\frac{2^{k}}{k^{p}}$ . I do not understand for which value of p such that  the original series is convergent. Also have used Cauchy integral test but did not solve the improper integral $\int_{2}^{\infty} {\frac{1}{(\log{x})^{p}}}dx$.  I do not understand the convergent or not, if it convergent what is the value of p will be.   Please some one help me. Thanks",,"['calculus', 'sequences-and-series', 'analysis', 'divergent-series', 'monotone-functions']"
52,Sequential continuity between metric and topological space,Sequential continuity between metric and topological space,,"I'm trying to prove that a map between a metric and topological space is continuous at x iff for every converging sequence in the materic space, the map of the sequence converges in the topological space. I was able to prove the forward case without too much trouble, but I'm having trouble proving that converging sequences imply continuity. I've assumed for contradiction that the map is not continuous and then invoked the definition of continuity to say there is an open neighborhood of x whose preimage is closed, but I'm having trouble piecing together a contradiction on convergence from this.","I'm trying to prove that a map between a metric and topological space is continuous at x iff for every converging sequence in the materic space, the map of the sequence converges in the topological space. I was able to prove the forward case without too much trouble, but I'm having trouble proving that converging sequences imply continuity. I've assumed for contradiction that the map is not continuous and then invoked the definition of continuity to say there is an open neighborhood of x whose preimage is closed, but I'm having trouble piecing together a contradiction on convergence from this.",,"['general-topology', 'analysis']"
53,Give an open cover with no finite subcover.,Give an open cover with no finite subcover.,,"I am looking for an example of an open cover of $\mathbb Q \cap [0,1]$ (with the metric induced from the usual metric on $\mathbb R$) that has no finite subcover. I'm at a loss because of the intersection with $\mathbb Q$.  I originally thought of $(-\frac 1n, 1+ \frac 1n$) for $n \in \mathbb N$ but then I think this has a finite subcover, so I'm not sure where to go from here.","I am looking for an example of an open cover of $\mathbb Q \cap [0,1]$ (with the metric induced from the usual metric on $\mathbb R$) that has no finite subcover. I'm at a loss because of the intersection with $\mathbb Q$.  I originally thought of $(-\frac 1n, 1+ \frac 1n$) for $n \in \mathbb N$ but then I think this has a finite subcover, so I'm not sure where to go from here.",,"['general-topology', 'analysis', 'metric-spaces']"
54,Inequality between Hausdorff measure and spherical Hausdorff measure,Inequality between Hausdorff measure and spherical Hausdorff measure,,"I have a doubt on spherical Hausdorff measure. Given $k, \delta \in (0,\infty)$, the $\delta$-Hausdorff premeasure is defined for $E\subset \mathbb R^n$ as: $$\mathcal H^k_\delta(E):=\inf\{\sum_j\alpha_k \frac{\text{ diam}E_j^k}{2^k}: E\subset \bigcup E_j, \text{ diam}E_j\leq \delta\},$$ where $\alpha_k=\frac{2 \pi^\frac{k}{2}}{k\Gamma(\frac{m}{2})}.$ The spherical Hausdorff $\delta$-premeasure $\mathcal S^k_\delta$ differs from the last one in the fact that coverings made only by balls are allowed. With this definition it is clear that $\mathcal H^k_\delta \leq \mathcal S^k_\delta$. But it is also true that $\mathcal S^k_\delta\leq 2^k \mathcal H^k_\delta.$ My question is: why is this true? I've though that, for any $\epsilon>0$, there exists a $\delta$-covering $\{E_j\}$ of $E$ such that $$\sum_j \alpha _k\frac{\text{ diam}E_j^k}{2^k}\leq\sum_j \alpha^k \text{ diam}E_j^k\leq 2^k\mathcal H^k_\delta(E)+2^k\epsilon.$$  Now, I would finished the proof if I could assume that the covering $\{E_j\}$ is made by balls, because I would have $$\mathcal S^k_\delta(E)\leq\sum_j\alpha_k \frac{\text{ diam}E_j^k}{2^k}\leq\sum_j \alpha^k \text{ diam}E_j^k\leq 2^k\mathcal H^k_\delta(E)+2^k\epsilon,$$ and by the arbitrariety of $\epsilon$ it would follow $\mathcal S^k_\delta\leq 2^k \mathcal H^k_\delta.$ Can I assume this? Also, why and where spherical Hausdorff measure is useful?","I have a doubt on spherical Hausdorff measure. Given $k, \delta \in (0,\infty)$, the $\delta$-Hausdorff premeasure is defined for $E\subset \mathbb R^n$ as: $$\mathcal H^k_\delta(E):=\inf\{\sum_j\alpha_k \frac{\text{ diam}E_j^k}{2^k}: E\subset \bigcup E_j, \text{ diam}E_j\leq \delta\},$$ where $\alpha_k=\frac{2 \pi^\frac{k}{2}}{k\Gamma(\frac{m}{2})}.$ The spherical Hausdorff $\delta$-premeasure $\mathcal S^k_\delta$ differs from the last one in the fact that coverings made only by balls are allowed. With this definition it is clear that $\mathcal H^k_\delta \leq \mathcal S^k_\delta$. But it is also true that $\mathcal S^k_\delta\leq 2^k \mathcal H^k_\delta.$ My question is: why is this true? I've though that, for any $\epsilon>0$, there exists a $\delta$-covering $\{E_j\}$ of $E$ such that $$\sum_j \alpha _k\frac{\text{ diam}E_j^k}{2^k}\leq\sum_j \alpha^k \text{ diam}E_j^k\leq 2^k\mathcal H^k_\delta(E)+2^k\epsilon.$$  Now, I would finished the proof if I could assume that the covering $\{E_j\}$ is made by balls, because I would have $$\mathcal S^k_\delta(E)\leq\sum_j\alpha_k \frac{\text{ diam}E_j^k}{2^k}\leq\sum_j \alpha^k \text{ diam}E_j^k\leq 2^k\mathcal H^k_\delta(E)+2^k\epsilon,$$ and by the arbitrariety of $\epsilon$ it would follow $\mathcal S^k_\delta\leq 2^k \mathcal H^k_\delta.$ Can I assume this? Also, why and where spherical Hausdorff measure is useful?",,"['real-analysis', 'analysis', 'measure-theory', 'geometric-measure-theory']"
55,The decay conditions of the Poisson summation formula,The decay conditions of the Poisson summation formula,,"The Poisson summation formula states that for a function $f$ satisfying  $$ |f(x)|\leq A(1+|x|)^{-n-\delta},~|\hat{f}(\xi)|\leq A(1+|\xi|)^{-n-\delta} $$ for $\delta>0,$ then the equality  $$ \sum_{\nu\in\mathbb{Z}^n}f(x+\nu)=\sum_{\nu\in\mathbb{Z}^n}\hat{f}(\nu)e^{2\pi ix\cdot \nu} $$ holds. Depending on how you interpret the theorem, it can suggest that two ways of making a function periodic are identical. My question is why are the boundedness/decay conditions necessary in the theorem? Is it solely to ensure that $f$ and $\hat{f}$ do not blow up to infinity, or is there something deeper? Thank you in advance for your help and comments.","The Poisson summation formula states that for a function $f$ satisfying  $$ |f(x)|\leq A(1+|x|)^{-n-\delta},~|\hat{f}(\xi)|\leq A(1+|\xi|)^{-n-\delta} $$ for $\delta>0,$ then the equality  $$ \sum_{\nu\in\mathbb{Z}^n}f(x+\nu)=\sum_{\nu\in\mathbb{Z}^n}\hat{f}(\nu)e^{2\pi ix\cdot \nu} $$ holds. Depending on how you interpret the theorem, it can suggest that two ways of making a function periodic are identical. My question is why are the boundedness/decay conditions necessary in the theorem? Is it solely to ensure that $f$ and $\hat{f}$ do not blow up to infinity, or is there something deeper? Thank you in advance for your help and comments.",,['analysis']
56,Sum of two harmonic alternating series,Sum of two harmonic alternating series,,"Evaluate the series $$\sum_{n=1}^\infty (-1)^{n+1}\frac{2n+1}{n(n+1)}.$$ I've simplified it to the form $$\sum_{n=1}^\infty (-1)^{n+1}\frac{1}{n+1} + \sum_{n=1}^\infty (-1)^{n+1}\frac{1}{n}$$  and I've proved that both parts converge. However, I'm having trouble finding the limit. Writing out the terms as $(1+\frac 1 2 - \frac 1 2 + \frac 1 3 - \frac 1 3 ... )$ suggest their sum is one. However when I look up the sums of the two parts, they are $-\ln(2)$ and $\ln(2)$ respectively, which suggests the sum of the overall series is $0$. I'm aware that if a series is not absolutely convergent then its terms can be rearranged to converge to any number, but we haven't covered that topic yet so I feel like that shouldn't be a consideration in solving this.","Evaluate the series $$\sum_{n=1}^\infty (-1)^{n+1}\frac{2n+1}{n(n+1)}.$$ I've simplified it to the form $$\sum_{n=1}^\infty (-1)^{n+1}\frac{1}{n+1} + \sum_{n=1}^\infty (-1)^{n+1}\frac{1}{n}$$  and I've proved that both parts converge. However, I'm having trouble finding the limit. Writing out the terms as $(1+\frac 1 2 - \frac 1 2 + \frac 1 3 - \frac 1 3 ... )$ suggest their sum is one. However when I look up the sums of the two parts, they are $-\ln(2)$ and $\ln(2)$ respectively, which suggests the sum of the overall series is $0$. I'm aware that if a series is not absolutely convergent then its terms can be rearranged to converge to any number, but we haven't covered that topic yet so I feel like that shouldn't be a consideration in solving this.",,"['real-analysis', 'sequences-and-series', 'analysis']"
57,Is this correct for Rudin exercise 3.7? Prove the series is convergent,Is this correct for Rudin exercise 3.7? Prove the series is convergent,,"This is Baby Rudin exercise 7 of Chapter 3. Prove that the convergence of $\sum{a_n}$ implies the convergence of $\sum{\sqrt {a_k} \over k}$ if $a_n \ge 0$. Proof:  I will attempt to show that the cauchy criterion is met so $${\sum_{k=n}^m {\sqrt {a_k} \over k}} \le \epsilon $$ for all $n,m \le N_1$. Im disregarding the absolute value since its all positive terms. Since $\sum{a_n}$ converges, we know that the $\lim \limits_{n \to \infty} a_n =0$ So there exists some $N_2$ such that for all $n \ge N_2$, $$   a_n \lt \ {{(m-n)}^2k^2(\epsilon)^2} $$ I am again disregarding the absolute values because its always positive Let N=max{$N_1,N_2$} then for all $n,m \ge N$ $$\sum_{k=n}^m {\sqrt {a_k} \over k} \le \sum_{k=n}^m {\sqrt {{(m-n)}^2k^2(\epsilon)^2} \over k} \le \sum_{k=n}^m{(m-n)(\epsilon)} = \epsilon $$ Maybe this is just crazy, should I just use Cauchy-Schwarz inequality and forget about this or could I possibly make this method work?","This is Baby Rudin exercise 7 of Chapter 3. Prove that the convergence of $\sum{a_n}$ implies the convergence of $\sum{\sqrt {a_k} \over k}$ if $a_n \ge 0$. Proof:  I will attempt to show that the cauchy criterion is met so $${\sum_{k=n}^m {\sqrt {a_k} \over k}} \le \epsilon $$ for all $n,m \le N_1$. Im disregarding the absolute value since its all positive terms. Since $\sum{a_n}$ converges, we know that the $\lim \limits_{n \to \infty} a_n =0$ So there exists some $N_2$ such that for all $n \ge N_2$, $$   a_n \lt \ {{(m-n)}^2k^2(\epsilon)^2} $$ I am again disregarding the absolute values because its always positive Let N=max{$N_1,N_2$} then for all $n,m \ge N$ $$\sum_{k=n}^m {\sqrt {a_k} \over k} \le \sum_{k=n}^m {\sqrt {{(m-n)}^2k^2(\epsilon)^2} \over k} \le \sum_{k=n}^m{(m-n)(\epsilon)} = \epsilon $$ Maybe this is just crazy, should I just use Cauchy-Schwarz inequality and forget about this or could I possibly make this method work?",,"['real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence', 'cauchy-sequences']"
58,Show the following are not connected in $\mathbb{R}^n$,Show the following are not connected in,\mathbb{R}^n,"Is the interior, boundary and closure of a connected set in $\mathbb{R}^n$ connected? I know the interior is not connected we can show it by a counterexample but I am not quite sure for the closure and boundary","Is the interior, boundary and closure of a connected set in $\mathbb{R}^n$ connected? I know the interior is not connected we can show it by a counterexample but I am not quite sure for the closure and boundary",,['analysis']
59,Differential forms in Physics,Differential forms in Physics,,"In the context of physics, I just read about the the symplectic 2-form $\omega$ on a symplectic manifold $M$ of dimension $2n$.  Unfortunately, I could not follow a few arguments. I.e. it was said that since $\omega$ is non-degenerate, the $n$-fold wedge-sum $\omega \wedge \cdots \wedge \omega$ is a volume form and if $M$ would be a closed manifold $M$, then $\int_M \omega \wedge \cdots \wedge \omega \neq 0.$ Could anybody explain to me where these two conclusion actually come from, i.e. I see that the wedge sum is a top-degree volume form, but I don't see why this means that it is nowhere vanishing and especially why the integral is non-zero.","In the context of physics, I just read about the the symplectic 2-form $\omega$ on a symplectic manifold $M$ of dimension $2n$.  Unfortunately, I could not follow a few arguments. I.e. it was said that since $\omega$ is non-degenerate, the $n$-fold wedge-sum $\omega \wedge \cdots \wedge \omega$ is a volume form and if $M$ would be a closed manifold $M$, then $\int_M \omega \wedge \cdots \wedge \omega \neq 0.$ Could anybody explain to me where these two conclusion actually come from, i.e. I see that the wedge sum is a top-degree volume form, but I don't see why this means that it is nowhere vanishing and especially why the integral is non-zero.",,"['real-analysis', 'analysis', 'differential-geometry', 'differential-topology', 'differential-forms']"
60,Prove there exists a $c$ such that $-f'(c)=\frac{f(c)}c$,Prove there exists a  such that,c -f'(c)=\frac{f(c)}c,"Let $f: \Bbb{R} \longrightarrow \Bbb{R}$ be a continuous function on $[0,2]$ and differentiable on $(0,2)$. $f(2)=0$. Prove there exists a $c \in (0,2)$ such that $-f'(c)=\frac{f(c)}c$. What I did: 1) From MVT: $\exists c_2 :f'(c_2)=\frac{f(2)-f(0)}{2-0}=\frac{-f(0)}2$ As $-f'(c)=\frac{f(c)}c \iff -f'(c_2)=\frac{f(c_2)}{c_2}$ I can replace $f'(c_2)$ to get: $\frac{f(0)}{2}=\frac{f(c_2)}{c_2}$ . 2) This last statement is equivalent to prove that: $$ g(x)=xf(0)-2f(x) $$ Has a root on $(0,2)$ As: $g(0)=-2f(0)$ and $g(2)=2f(0)$, if $f(0)\neq0$ I can say that due to Rolle's theorem has a root, but what if $f(0)=0$? Is my reasoning so far correct?","Let $f: \Bbb{R} \longrightarrow \Bbb{R}$ be a continuous function on $[0,2]$ and differentiable on $(0,2)$. $f(2)=0$. Prove there exists a $c \in (0,2)$ such that $-f'(c)=\frac{f(c)}c$. What I did: 1) From MVT: $\exists c_2 :f'(c_2)=\frac{f(2)-f(0)}{2-0}=\frac{-f(0)}2$ As $-f'(c)=\frac{f(c)}c \iff -f'(c_2)=\frac{f(c_2)}{c_2}$ I can replace $f'(c_2)$ to get: $\frac{f(0)}{2}=\frac{f(c_2)}{c_2}$ . 2) This last statement is equivalent to prove that: $$ g(x)=xf(0)-2f(x) $$ Has a root on $(0,2)$ As: $g(0)=-2f(0)$ and $g(2)=2f(0)$, if $f(0)\neq0$ I can say that due to Rolle's theorem has a root, but what if $f(0)=0$? Is my reasoning so far correct?",,"['analysis', 'proof-verification']"
61,Let $x \in \mathbb Q \setminus \{0\}$ and $y \in \mathbb R\setminus \mathbb Q$. Prove that $\frac{x}{y} \in \mathbb R \setminus \mathbb Q$,Let  and . Prove that,x \in \mathbb Q \setminus \{0\} y \in \mathbb R\setminus \mathbb Q \frac{x}{y} \in \mathbb R \setminus \mathbb Q,Let $x \in \mathbb Q\setminus  \{0\}$ and $y \in \mathbb R\setminus \mathbb Q.$ Prove that $\frac{x}{y} \in \mathbb R \setminus \mathbb Q$ I saw this question in a basic analysis test but it confuses me because intuitively it makes sense but how do you show mathematically that the set of rationals is not in the solution space?,Let $x \in \mathbb Q\setminus  \{0\}$ and $y \in \mathbb R\setminus \mathbb Q.$ Prove that $\frac{x}{y} \in \mathbb R \setminus \mathbb Q$ I saw this question in a basic analysis test but it confuses me because intuitively it makes sense but how do you show mathematically that the set of rationals is not in the solution space?,,"['real-analysis', 'analysis']"
62,Why doesn't $\sum_{n=1}^\infty \frac{1}{n^{1+\frac{1}{n}}}$ converge?,Why doesn't  converge?,\sum_{n=1}^\infty \frac{1}{n^{1+\frac{1}{n}}},$\sum_{n=1}^\infty \frac{1}{n^{1+\frac{1}{n}}} = \infty$. Is there a comparison that works well to prove this?,$\sum_{n=1}^\infty \frac{1}{n^{1+\frac{1}{n}}} = \infty$. Is there a comparison that works well to prove this?,,"['sequences-and-series', 'analysis', 'divergent-series']"
63,"Problem 5, sec. 13, in Munkres' Topology, 2nd ed.: How to prove the assertion if $\mathscr{A}$ is a subbasis?","Problem 5, sec. 13, in Munkres' Topology, 2nd ed.: How to prove the assertion if  is a subbasis?",\mathscr{A},"Show that if $\mathscr{A}$ is a basis for a topology on $X$ , then the topology generated by $\mathscr{A}$ equals the intersection of all topologies on $X$ that contain $\mathscr{A}$ . This is what I've managed. However, the following stumps me: Prove the same if $\mathscr{A}$ is a subbasis. That is, how to show that, the topology generated by $\mathscr{A}$ as a subbasis is the same as the intersection of all topologies on $X$ that contain $\mathscr{A}$ ? Let $\mathscr{T}$ be the topology that consists of all unions of finite intersections of sets in $\mathscr{A}$ , and let $\mathscr{T}^\prime$ be the topology that is the intersection of all topologies that contain $\mathscr{A}$ . Now $\mathscr{A}$ is a collection of subsets of $X$ whose union is all of $X$ , and each set $A \in \mathscr{A}$ is in both $\mathscr{T}$ and $\mathscr{T}^\prime$ . Am I right so far? And if so, then what next?","Show that if is a basis for a topology on , then the topology generated by equals the intersection of all topologies on that contain . This is what I've managed. However, the following stumps me: Prove the same if is a subbasis. That is, how to show that, the topology generated by as a subbasis is the same as the intersection of all topologies on that contain ? Let be the topology that consists of all unions of finite intersections of sets in , and let be the topology that is the intersection of all topologies that contain . Now is a collection of subsets of whose union is all of , and each set is in both and . Am I right so far? And if so, then what next?",\mathscr{A} X \mathscr{A} X \mathscr{A} \mathscr{A} \mathscr{A} X \mathscr{A} \mathscr{T} \mathscr{A} \mathscr{T}^\prime \mathscr{A} \mathscr{A} X X A \in \mathscr{A} \mathscr{T} \mathscr{T}^\prime,"['general-topology', 'analysis']"
64,"Example 4.21 in Baby Rudin: How is the map $f^{-1}$ not continuous at the point $(1,0) = f(0)$?",Example 4.21 in Baby Rudin: How is the map  not continuous at the point ?,"f^{-1} (1,0) = f(0)","Let $f \colon [0,2\pi ) \to \{ (x,y) \in \mathbb{R}^2 \mid x^2 + y^2 = 1\}$ be defined as  $$f(t) = ( \cos t , \sin t) \ \ \mbox{ for all } \ t \in [0, 2\pi).$$  Then the map $f$ is bijective and continuous, as asserted by Rudin. But how do we rigorously show that the inverse map is not continuous at the point $(1,0) = f(0)$? Intuitively, it is obvious since if we approach the point $(1,0)$ along that part of the unit circle which lies in the fourth quadrant, we can make the  distance in $\mathbb{R}^2$ between $f(t)$ and $f(0)$ as small as we please, but the distance in $\mathbb{R}$ between $t= f^{-1}\left(f(t)\right)$ and $0= f^{-1}\left(f(0)\right) = f^{-1}\left( (1,0) \right)$ approaches $2\pi$. But how to formulate this into a rigorous, $\epsilon$-$\delta$ argument?","Let $f \colon [0,2\pi ) \to \{ (x,y) \in \mathbb{R}^2 \mid x^2 + y^2 = 1\}$ be defined as  $$f(t) = ( \cos t , \sin t) \ \ \mbox{ for all } \ t \in [0, 2\pi).$$  Then the map $f$ is bijective and continuous, as asserted by Rudin. But how do we rigorously show that the inverse map is not continuous at the point $(1,0) = f(0)$? Intuitively, it is obvious since if we approach the point $(1,0)$ along that part of the unit circle which lies in the fourth quadrant, we can make the  distance in $\mathbb{R}^2$ between $f(t)$ and $f(0)$ as small as we please, but the distance in $\mathbb{R}$ between $t= f^{-1}\left(f(t)\right)$ and $0= f^{-1}\left(f(0)\right) = f^{-1}\left( (1,0) \right)$ approaches $2\pi$. But how to formulate this into a rigorous, $\epsilon$-$\delta$ argument?",,"['real-analysis', 'analysis', 'metric-spaces', 'continuity', 'uniform-continuity']"
65,Finding $ \lbrace a_{n}\rbrace $ s.t. $\mathop {\lim }\limits_{n \to \infty }a_{n}=1$ and $\mathop {\lim }\limits_{n \to \infty }a_{n}^{n}=2015$,Finding  s.t.  and, \lbrace a_{n}\rbrace  \mathop {\lim }\limits_{n \to \infty }a_{n}=1 \mathop {\lim }\limits_{n \to \infty }a_{n}^{n}=2015,The following problem appears in our analysis assignment. Find a sequence $ \lbrace a_{n}\rbrace $ of real numbers such that $$\mathop {\lim }\limits_{n \to \infty }a_{n}=1\text{ and }\mathop {\lim }\limits_{n \to \infty }a_{n}^{n}=2015.$$ Could anyone give me some help to find such a sequence ? Any hints/ideas are much appreciated. Thanks in advance for any replies.,The following problem appears in our analysis assignment. Find a sequence $ \lbrace a_{n}\rbrace $ of real numbers such that $$\mathop {\lim }\limits_{n \to \infty }a_{n}=1\text{ and }\mathop {\lim }\limits_{n \to \infty }a_{n}^{n}=2015.$$ Could anyone give me some help to find such a sequence ? Any hints/ideas are much appreciated. Thanks in advance for any replies.,,"['real-analysis', 'sequences-and-series', 'analysis']"
66,"Prob. 6 (d), Chap. 3, in Baby Rudin, 3rd ed: What about the convergence of this series for $\vert z \vert >1$?","Prob. 6 (d), Chap. 3, in Baby Rudin, 3rd ed: What about the convergence of this series for ?",\vert z \vert >1,"Let $a_n \colon= {1 \over {1+z^n}} $ for $n = 1, 2, 3, \ldots$, where $z$ is a given complex number. Then what about the convergence of the series $\sum a_n$? My effort: When $\vert z \vert \leq 1$, we have  $$\vert a_n \vert = {1 \over \vert 1+z^n \vert } \geq {1 \over {1 + \vert z \vert^n } } \geq {1 \over 2 }$$ for all $n$ so that  $$ \lim_{n\to \infty} a_n \neq 0,$$  and the series fails to converge. What if $\vert z \vert > 1$? In this case, for all $n$, we have  $$\vert a_n  \vert =  {1 \over \vert 1 + z^n \vert } \leq {1 \over {\vert z \vert^n - 1}}. $$ What next? (How) can we compare this series with a geometric series?","Let $a_n \colon= {1 \over {1+z^n}} $ for $n = 1, 2, 3, \ldots$, where $z$ is a given complex number. Then what about the convergence of the series $\sum a_n$? My effort: When $\vert z \vert \leq 1$, we have  $$\vert a_n \vert = {1 \over \vert 1+z^n \vert } \geq {1 \over {1 + \vert z \vert^n } } \geq {1 \over 2 }$$ for all $n$ so that  $$ \lim_{n\to \infty} a_n \neq 0,$$  and the series fails to converge. What if $\vert z \vert > 1$? In this case, for all $n$, we have  $$\vert a_n  \vert =  {1 \over \vert 1 + z^n \vert } \leq {1 \over {\vert z \vert^n - 1}}. $$ What next? (How) can we compare this series with a geometric series?",,"['calculus', 'real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence']"
67,$a_{n+1}=a_n+\cos a_n$ for all $n \in N$,for all,a_{n+1}=a_n+\cos a_n n \in N,"Define the sequence $a_n$ recursively by $a_{n+1}=a_n+\cos a_n$ for all $n\in \mathbb N$ with $a_1=1$ I've just begun with introduction to analysis and have witnessed these two sequences from the text. However, from the knowledge I've learned so far, I don't know how to prove that these sequeces converge and to what values they converge. How can I prove the convergence of these sequences?","Define the sequence $a_n$ recursively by $a_{n+1}=a_n+\cos a_n$ for all $n\in \mathbb N$ with $a_1=1$ I've just begun with introduction to analysis and have witnessed these two sequences from the text. However, from the knowledge I've learned so far, I don't know how to prove that these sequeces converge and to what values they converge. How can I prove the convergence of these sequences?",,"['real-analysis', 'sequences-and-series', 'analysis', 'cauchy-sequences']"
68,Convergence for Conjuguate gradient method,Convergence for Conjuguate gradient method,,"I am trying to probe this corollary in a numerical PDE book: If $A\in \mathbb{R^{n\times n}}$  is symmetric and positive definite, then the conjugate gradient method reaches the exact solution in at most $n$ iterations.","I am trying to probe this corollary in a numerical PDE book: If $A\in \mathbb{R^{n\times n}}$  is symmetric and positive definite, then the conjugate gradient method reaches the exact solution in at most $n$ iterations.",,"['linear-algebra', 'analysis', 'numerical-methods', 'numerical-linear-algebra']"
69,Is $\|x\|^6 \sin^6 \|x\|^6$ harmonic?,Is  harmonic?,\|x\|^6 \sin^6 \|x\|^6,"Suppose the function  $$ u(x)=\|x\|^6 \sin^6 \|x\|^6$$ for $x \in \mathbb{R}^d$, where  $$\|x\| = \sqrt{x_1^2 + \ldots +x_d^2}.$$  How can I decide if the function $u$ is harmonic in the unit ball $B_1(0)$ without calculating $\Delta u$?","Suppose the function  $$ u(x)=\|x\|^6 \sin^6 \|x\|^6$$ for $x \in \mathbb{R}^d$, where  $$\|x\| = \sqrt{x_1^2 + \ldots +x_d^2}.$$  How can I decide if the function $u$ is harmonic in the unit ball $B_1(0)$ without calculating $\Delta u$?",,"['calculus', 'real-analysis', 'analysis', 'harmonic-analysis', 'harmonic-functions']"
70,This improper integral $\int_{1}^{+\infty}x\sin{x}\sin{x^4}dx$ is absolutely convergent?,This improper integral  is absolutely convergent?,\int_{1}^{+\infty}x\sin{x}\sin{x^4}dx,Discuss the improper integral $$\int_{1}^{+\infty}x\sin{x}\sin{x^4}dx$$ absolute convergence? My idea: since $$\sin{x}=x-\dfrac{x^3}{6}+\cdots$$ $$\sin{x^4}=x^4-\dfrac{x^{12}}{6}+\cdots$$ so $$x\sin{x}\sin{x^4}=x^6-\cdots$$ then I can't sure this integral is absolutely convergent. Thank you,Discuss the improper integral $$\int_{1}^{+\infty}x\sin{x}\sin{x^4}dx$$ absolute convergence? My idea: since $$\sin{x}=x-\dfrac{x^3}{6}+\cdots$$ $$\sin{x^4}=x^4-\dfrac{x^{12}}{6}+\cdots$$ so $$x\sin{x}\sin{x^4}=x^6-\cdots$$ then I can't sure this integral is absolutely convergent. Thank you,,"['integration', 'analysis']"
71,How prove this integral inequality $\int_{0}^{\infty}(f(t))^2t^{-\delta}dt\le\frac{4}{(1-\delta)^2}\int_{0}^{\infty}(f'(t))^2t^{2-\delta}dt$?,How prove this integral inequality ?,\int_{0}^{\infty}(f(t))^2t^{-\delta}dt\le\frac{4}{(1-\delta)^2}\int_{0}^{\infty}(f'(t))^2t^{2-\delta}dt,"Question: let $\delta\in(0,1)$, and $f\in C_{0}^{1}(R_{+})$,show that   $$\int_{0}^{\infty}(f(t))^2t^{-\delta}dt\le\dfrac{4}{(1-\delta)^2}\int_{0}^{\infty}(f'(t))^2t^{2-\delta}dt$$ My idea: I think we must use Cauchy-Schwarz inequality $$\int_{0}^{\infty}(f'(t))^2t^{2-\delta}dt\int_{0}^{\infty}t^{-2-\delta}dt \ge\left(\int_{0}^{\infty}f'(t)t^{-\delta}dt\right)^2$$ But I can't know this  coefficient  $\dfrac{4}{(1-\delta)^2}$ How do have it?  Thank you","Question: let $\delta\in(0,1)$, and $f\in C_{0}^{1}(R_{+})$,show that   $$\int_{0}^{\infty}(f(t))^2t^{-\delta}dt\le\dfrac{4}{(1-\delta)^2}\int_{0}^{\infty}(f'(t))^2t^{2-\delta}dt$$ My idea: I think we must use Cauchy-Schwarz inequality $$\int_{0}^{\infty}(f'(t))^2t^{2-\delta}dt\int_{0}^{\infty}t^{-2-\delta}dt \ge\left(\int_{0}^{\infty}f'(t)t^{-\delta}dt\right)^2$$ But I can't know this  coefficient  $\dfrac{4}{(1-\delta)^2}$ How do have it?  Thank you",,"['analysis', 'integral-inequality']"
72,Is the number of subsequential limits of a sequence always countable,Is the number of subsequential limits of a sequence always countable,,I know that a sequence can have many different subsequential limits but is the number of subsequential limits always countable? How do we know?,I know that a sequence can have many different subsequential limits but is the number of subsequential limits always countable? How do we know?,,['analysis']
73,Formula for $\sum\cos(\pi kt)/(1+k^2)$,Formula for,\sum\cos(\pi kt)/(1+k^2),Is there an explicit formula for the sum $$F = \sum_{k=0}^\infty \frac{1}{1+k^2} \cos(\pi k t)$$ This is the green function for the operator $1 + \Delta$ on the circle.,Is there an explicit formula for the sum $$F = \sum_{k=0}^\infty \frac{1}{1+k^2} \cos(\pi k t)$$ This is the green function for the operator $1 + \Delta$ on the circle.,,"['sequences-and-series', 'analysis']"
74,Sequence and Limit,Sequence and Limit,,"If $\lim\limits_{n\to\infty}a_n=1$, $0\leq a_n\leq 1 \; \forall n\in N,$ then what about the $\lim\limits_{n\to\infty}(a_n)^n$. Is this limit exists? If yes then what is the value of this limit?","If $\lim\limits_{n\to\infty}a_n=1$, $0\leq a_n\leq 1 \; \forall n\in N,$ then what about the $\lim\limits_{n\to\infty}(a_n)^n$. Is this limit exists? If yes then what is the value of this limit?",,"['calculus', 'analysis']"
75,"Isomorphism isometries between finite subsets , implies isomorphism isometry between compact metric spaces","Isomorphism isometries between finite subsets , implies isomorphism isometry between compact metric spaces",,"Let's $(X_1,d_1), (X_2,d_2)$ be compact metric spaces such that for every finite subset of $X_1$ like $A$ (respectively any finite subset of $X_2$ like $B$ ) there exists a finite subset of $X_2$ like $B$ ( respectively exists a finite subset of $X_1$ like $A$ ) such that there exists a isomorphism isometry between $A$ and $B$. Show that there exists an isomorphism isometry between $X_1$ and $X_2$","Let's $(X_1,d_1), (X_2,d_2)$ be compact metric spaces such that for every finite subset of $X_1$ like $A$ (respectively any finite subset of $X_2$ like $B$ ) there exists a finite subset of $X_2$ like $B$ ( respectively exists a finite subset of $X_1$ like $A$ ) such that there exists a isomorphism isometry between $A$ and $B$. Show that there exists an isomorphism isometry between $X_1$ and $X_2$",,"['general-topology', 'analysis', 'metric-spaces', 'compactness']"
76,variation of a function over countable intervals,variation of a function over countable intervals,,"Let $f$ be a function of bounded variation on $[0,1]$. Let $\{[a_n,b_n]\}_{n=1}^\infty$  such that $(a_n,b_n)$ are pairwise disjoint and $\cup_{n=1}^\infty [a_n,b_n]=[0,1]$. (for example, $[1/2, 1], [0,1/3], [1/3,1/3+1/3^2], \cdots$ ) Can we write $$ \operatorname{Var}_{[0,1]} f=\sum_{k=1}^\infty \operatorname{Var}_{[a_k,b_k]} f? $$","Let $f$ be a function of bounded variation on $[0,1]$. Let $\{[a_n,b_n]\}_{n=1}^\infty$  such that $(a_n,b_n)$ are pairwise disjoint and $\cup_{n=1}^\infty [a_n,b_n]=[0,1]$. (for example, $[1/2, 1], [0,1/3], [1/3,1/3+1/3^2], \cdots$ ) Can we write $$ \operatorname{Var}_{[0,1]} f=\sum_{k=1}^\infty \operatorname{Var}_{[a_k,b_k]} f? $$",,"['calculus', 'real-analysis', 'integration', 'analysis', 'measure-theory']"
77,uniform convergence of a functional sequence,uniform convergence of a functional sequence,,"Is this sequence of functions  $$f_n(x)=n^3x(1-x)^n$$  converges uniformly for $x\in[0,1]$. I need some help on this.","Is this sequence of functions  $$f_n(x)=n^3x(1-x)^n$$  converges uniformly for $x\in[0,1]$. I need some help on this.",,"['real-analysis', 'sequences-and-series', 'analysis']"
78,if $f(x) \sim g(x)$ is $ \sum f(k) \sim \sum g(k)$ [duplicate],if  is  [duplicate],f(x) \sim g(x)  \sum f(k) \sim \sum g(k),"This question already has answers here : Does $a_n \sim b_n$ imply $\sum_n a_n \sim \sum_n b_n$ for $a_n, b_n>0$? (2 answers) Closed 7 years ago . if $f(x) \sim g(x)$ as $x \to \infty$ then is $\sum_{k=1}^N f(k) \sim \sum_{k=1}^N g(k)$ as $N \to \infty$? Intuitively, i should think so because as $k$ gets larger $f$ and $g$ get closer so it should 'even out'?","This question already has answers here : Does $a_n \sim b_n$ imply $\sum_n a_n \sim \sum_n b_n$ for $a_n, b_n>0$? (2 answers) Closed 7 years ago . if $f(x) \sim g(x)$ as $x \to \infty$ then is $\sum_{k=1}^N f(k) \sim \sum_{k=1}^N g(k)$ as $N \to \infty$? Intuitively, i should think so because as $k$ gets larger $f$ and $g$ get closer so it should 'even out'?",,"['analysis', 'asymptotics']"
79,What's the most elegant way to show $x_{n+1}=\frac{2x_n^3+a}{3x_n^2}$ converges against $\sqrt[3]{a}$?,What's the most elegant way to show  converges against ?,x_{n+1}=\frac{2x_n^3+a}{3x_n^2} \sqrt[3]{a},"Let $1<a\in\mathbb{R}$, $x_0>\sqrt[3]{a}$ and $$\displaystyle x_{n+1}=\frac{2x_n^3+a}{3x_n^2}\;\;\;\;\;(n\in\mathbb{N}_0)$$ It's easy to show that it holds: $x_n>\sqrt[3]{a}$ $x_{n+1}<x_n$ Thus, the sequence $(x_n)_n$ converges, since it's monotonically decreasing and lower bounded. However, I want to show that it converges against $\sqrt[3]{a}$. What's the most elegant way to do that?","Let $1<a\in\mathbb{R}$, $x_0>\sqrt[3]{a}$ and $$\displaystyle x_{n+1}=\frac{2x_n^3+a}{3x_n^2}\;\;\;\;\;(n\in\mathbb{N}_0)$$ It's easy to show that it holds: $x_n>\sqrt[3]{a}$ $x_{n+1}<x_n$ Thus, the sequence $(x_n)_n$ converges, since it's monotonically decreasing and lower bounded. However, I want to show that it converges against $\sqrt[3]{a}$. What's the most elegant way to do that?",,"['real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence']"
80,What is the sufficient and necessary condition for changing the order of summation?,What is the sufficient and necessary condition for changing the order of summation?,,"What is the necessary and sufficient condition for $\sum\limits_{i=0}^\infty \sum\limits_{j=0}^\infty a_{ij}=\sum\limits_{j=0}^\infty \sum\limits_{i=0}^\infty a_{ij}$? Suppose that both sides are convergent. As we know, the absolute convergence of any side is sufficient. My textbook says there is a necessary and sufficient condition found by Markov. But I can't find it on the Internet. Can anyone tell me what is it?","What is the necessary and sufficient condition for $\sum\limits_{i=0}^\infty \sum\limits_{j=0}^\infty a_{ij}=\sum\limits_{j=0}^\infty \sum\limits_{i=0}^\infty a_{ij}$? Suppose that both sides are convergent. As we know, the absolute convergence of any side is sufficient. My textbook says there is a necessary and sufficient condition found by Markov. But I can't find it on the Internet. Can anyone tell me what is it?",,"['sequences-and-series', 'analysis']"
81,Determine best possible Lipschitz constant,Determine best possible Lipschitz constant,,"I'm slightly confused by a homework problem here...I've been given the function: $ f(u) = log(u) $ With the bounds: $ 2 \leq u \lt \infty $ Now I thought I understood what the Lipschitz Condition required, but that was a function of two variables, but here I only have $u$. Believe it or not, I'm not a math major and I'm confused as to how to proceed using the definition of the Lipschitz constant.","I'm slightly confused by a homework problem here...I've been given the function: $ f(u) = log(u) $ With the bounds: $ 2 \leq u \lt \infty $ Now I thought I understood what the Lipschitz Condition required, but that was a function of two variables, but here I only have $u$. Believe it or not, I'm not a math major and I'm confused as to how to proceed using the definition of the Lipschitz constant.",,"['analysis', 'numerical-methods', 'continuity']"
82,Is this set a manifold?,Is this set a manifold?,,"For which $ ( \alpha , \beta ) \in \Bbb R^2$ set: $\{ (x_1,x_2,x_3,x_4) \in \Bbb R^4 | x_1+x_4= \alpha, x_1 x_4 - x_2x_3 = \beta \}$ is a manifold? I made a Jacobian matrix: $         \begin{bmatrix}         1 & 0 & 0& 1 \\         x_4 & -x_3 & -x_2 & x_1 \\         \end{bmatrix}$ Now I think that something must happen with this matrix for my set to be a manifold, but don't know what it is.. Can anybody help me?","For which $ ( \alpha , \beta ) \in \Bbb R^2$ set: $\{ (x_1,x_2,x_3,x_4) \in \Bbb R^4 | x_1+x_4= \alpha, x_1 x_4 - x_2x_3 = \beta \}$ is a manifold? I made a Jacobian matrix: $         \begin{bmatrix}         1 & 0 & 0& 1 \\         x_4 & -x_3 & -x_2 & x_1 \\         \end{bmatrix}$ Now I think that something must happen with this matrix for my set to be a manifold, but don't know what it is.. Can anybody help me?",,"['analysis', 'manifolds']"
83,"If I only know $f'(x) = e^{x^2}$, how do I evaluate $\lim_{x \to \infty} \frac{f'(x)}{f(x)}$?","If I only know , how do I evaluate ?",f'(x) = e^{x^2} \lim_{x \to \infty} \frac{f'(x)}{f(x)},"Specifically, I need to show that it equals $\infty$. I remember writing the solution down somewhere, though I can't find it and can't remember it, so I'm mostly looking for an outline of how to solve it. Thanks in advance.","Specifically, I need to show that it equals $\infty$. I remember writing the solution down somewhere, though I can't find it and can't remember it, so I'm mostly looking for an outline of how to solve it. Thanks in advance.",,['analysis']
84,If $\lim_{x\rightarrow\infty}(f(x+1)-f(x))=L$ prove that $f=O(x)$.,If  prove that .,\lim_{x\rightarrow\infty}(f(x+1)-f(x))=L f=O(x),"Let $f:\mathbb{R}\rightarrow\mathbb{R}$ such that  $$\lim_{x\rightarrow\infty}(f(x+1)-f(x))=L$$ Prove that $$\lim_{x\rightarrow \infty}\dfrac{f(x)}{x}=L$$ This was an exam question that I was given and got nowhere on it. Going back now, I don't think I'm any closer. This is my idea so far. We know that  $$\lim_{x\rightarrow\infty}\dfrac{f(x+1)-f(x)}{x}=0$$ I think I'm supposed to add the apprapraite $0$ to $$\left\vert \dfrac{f(x+1)-f(x)}{x}\right\vert$$ but I just keep getting a lower bound.  A hint would be much appreciated. Thanks","Let $f:\mathbb{R}\rightarrow\mathbb{R}$ such that  $$\lim_{x\rightarrow\infty}(f(x+1)-f(x))=L$$ Prove that $$\lim_{x\rightarrow \infty}\dfrac{f(x)}{x}=L$$ This was an exam question that I was given and got nowhere on it. Going back now, I don't think I'm any closer. This is my idea so far. We know that  $$\lim_{x\rightarrow\infty}\dfrac{f(x+1)-f(x)}{x}=0$$ I think I'm supposed to add the apprapraite $0$ to $$\left\vert \dfrac{f(x+1)-f(x)}{x}\right\vert$$ but I just keep getting a lower bound.  A hint would be much appreciated. Thanks",,"['real-analysis', 'analysis']"
85,Homotopy vs Conservative,Homotopy vs Conservative,,"I learned about conservative fields in multivariable calculus. I'm always curious about finding other or more fundamental methods of describing a concept (or concepts) in math, and better understanding how these methods relate. Question: To what extent and in what ways do ""homotopy"" and the concept of conservative relate? Mini question about terminology: In regards to homotopy and conservative, can each term be used to describe functions, fields, or both functions and fields? (Specific kinds of functions or fields? e.g. flux field)","I learned about conservative fields in multivariable calculus. I'm always curious about finding other or more fundamental methods of describing a concept (or concepts) in math, and better understanding how these methods relate. Question: To what extent and in what ways do ""homotopy"" and the concept of conservative relate? Mini question about terminology: In regards to homotopy and conservative, can each term be used to describe functions, fields, or both functions and fields? (Specific kinds of functions or fields? e.g. flux field)",,"['real-analysis', 'general-topology', 'analysis', 'multivariable-calculus', 'algebraic-topology']"
86,Identify when $f(x) = 0$.,Identify when .,f(x) = 0,"I have the following problem to solve. My attempt a. $\int_0^{\pi} x^n f(x) dx =0$ $\forall$ $n \ge 0$ gives $ x^n f(x) dx =0$ almost everywhere in $[0,\pi]$ $\forall$ $n \ge 0$. Putting $n = 0$ we shall get $f(x) = 0$ almost everywhere. As $f(x) \in C[0,\pi]$ we shall say $f(x) = 0$ $\forall$ $x \in [0,\pi]$ b. It is same as a. We shall put $n = 0$ and $\cos(nx) = 1$ $\Rightarrow$ $f(x) = 0$ $\forall$ $x \in [0,\pi]$ c. $\int_0^{\pi} f(x) \sin(nx) dx =0$ $\forall$ $n \ge 1$. Now  $f(x)\sin(nx) = 0$ almost everywhere. But I am not getting any more here. Integrals of b and c are looking like Fourier coefficients of the function $f(x)$. Can we say anything from it? Thank you for your help.","I have the following problem to solve. My attempt a. $\int_0^{\pi} x^n f(x) dx =0$ $\forall$ $n \ge 0$ gives $ x^n f(x) dx =0$ almost everywhere in $[0,\pi]$ $\forall$ $n \ge 0$. Putting $n = 0$ we shall get $f(x) = 0$ almost everywhere. As $f(x) \in C[0,\pi]$ we shall say $f(x) = 0$ $\forall$ $x \in [0,\pi]$ b. It is same as a. We shall put $n = 0$ and $\cos(nx) = 1$ $\Rightarrow$ $f(x) = 0$ $\forall$ $x \in [0,\pi]$ c. $\int_0^{\pi} f(x) \sin(nx) dx =0$ $\forall$ $n \ge 1$. Now  $f(x)\sin(nx) = 0$ almost everywhere. But I am not getting any more here. Integrals of b and c are looking like Fourier coefficients of the function $f(x)$. Can we say anything from it? Thank you for your help.",,['analysis']
87,Compute $\int_0^{\infty}\frac{\cos(\pi t/2)}{1-t^2}dt$,Compute,\int_0^{\infty}\frac{\cos(\pi t/2)}{1-t^2}dt,Compute $$\int_0^{\infty}\frac{\cos(\pi t/2)}{1-t^2}dt$$ The answer is $\pi/2$. The discontinuities at $\pm1$ are removable since the limit exists at those points.,Compute $$\int_0^{\infty}\frac{\cos(\pi t/2)}{1-t^2}dt$$ The answer is $\pi/2$. The discontinuities at $\pm1$ are removable since the limit exists at those points.,,"['real-analysis', 'integration', 'analysis', 'fourier-analysis', 'improper-integrals']"
88,"How to determine the sum of the series $\,\sum_{n=1}^{\infty}\frac{n+1}{2^n}$ [duplicate]",How to determine the sum of the series  [duplicate],"\,\sum_{n=1}^{\infty}\frac{n+1}{2^n}","This question already has answers here : How can I evaluate $\sum_{n=0}^\infty(n+1)x^n$? (24 answers) Closed 10 years ago . I am stuck on the following problem: I have to determine the sum of the series  $$\sum_{n=1}^{\infty}\frac{n+1}{2^n}$$ My Attempt : $$\sum_{n=0}^{\infty}\frac{n+1}{2^n}=\sum_{n=0}^{\infty}\frac{1}{2^n}+\sum_{n=0}^{\infty}\frac{n}{2^n}=\frac{1}{1-\frac12}+\sum_{n=0}^{\infty}\frac{n}{2^n}=2+\sum_{n=0}^{\infty}\frac{n}{2^n}$$. So,I am stuck on determining the value of $\,\,\sum_{n=0}^{\infty}\frac{n}{2^n}$. Can someone point me in the right direction? Thanks in advance for your time.","This question already has answers here : How can I evaluate $\sum_{n=0}^\infty(n+1)x^n$? (24 answers) Closed 10 years ago . I am stuck on the following problem: I have to determine the sum of the series  $$\sum_{n=1}^{\infty}\frac{n+1}{2^n}$$ My Attempt : $$\sum_{n=0}^{\infty}\frac{n+1}{2^n}=\sum_{n=0}^{\infty}\frac{1}{2^n}+\sum_{n=0}^{\infty}\frac{n}{2^n}=\frac{1}{1-\frac12}+\sum_{n=0}^{\infty}\frac{n}{2^n}=2+\sum_{n=0}^{\infty}\frac{n}{2^n}$$. So,I am stuck on determining the value of $\,\,\sum_{n=0}^{\infty}\frac{n}{2^n}$. Can someone point me in the right direction? Thanks in advance for your time.",,['analysis']
89,Solution to a tricky inequality (math analysis),Solution to a tricky inequality (math analysis),,"Let $p>1$ and put $q=\frac{p}{p-1}$, so $1/p+1/q=1$. Show that for any $x>0$ and $y>0$, we have  $$ xy \le \frac{x^p}{p}+\frac{y^q}{q}$$ And find where the equality holds. So far, I have simply tried to multiply through the RHS of the above expression and see what would happen, plugged in for $q$ and I got this: $$ pxy \le  x^p+(p-1)y^\frac{p}{p-1} $$ We also know that $q>1$ by its definition and using $p>1$, but I am not quite sure how to proceed. Any suggestions?  Thank you for the help","Let $p>1$ and put $q=\frac{p}{p-1}$, so $1/p+1/q=1$. Show that for any $x>0$ and $y>0$, we have  $$ xy \le \frac{x^p}{p}+\frac{y^q}{q}$$ And find where the equality holds. So far, I have simply tried to multiply through the RHS of the above expression and see what would happen, plugged in for $q$ and I got this: $$ pxy \le  x^p+(p-1)y^\frac{p}{p-1} $$ We also know that $q>1$ by its definition and using $p>1$, but I am not quite sure how to proceed. Any suggestions?  Thank you for the help",,['analysis']
90,Prove that $x_{n}$ is convergent if $|x_{n+1}-x_n|\leq(\frac45)^n$,Prove that  is convergent if,x_{n} |x_{n+1}-x_n|\leq(\frac45)^n,"The question says; prove the sequence $x_{n}$ given by: $\left | x_{n+1} - x_{n}\right| \leq (\frac{4}{5})^n \ \forall \ n \in \mathbb{N} $ is convergent. Here is how I approached the problem: Since the difference between the terms applies to all $n \in \mathbb{N}$, then we know that it is a Cauchy sequence (specifically, a contractile sequence) where we can assume that $\left| x_2 - x_1 \right|=1 $ then $K = \frac{4}{5} <1$ Also since for large values of $n$, $\lim_{n\rightarrow \infty} (\frac{4}{5})^n = 0$ so the constant $(\frac{4}{5})^n$ can replace $\varepsilon$ in the criteria for contractive sequence: $\left | x_{n+1} - x_{n}\right| \leq \varepsilon $ Then we can have, $\lim_{n\rightarrow \infty} \left|x_{n+1} - x_{n} \right| = 0$ which is a convergent sequence using the contraction principle. However, it seems like I should have used another method that uses similar idea but explicitly shows that the difference between $\left|x_{n+1}-x_{n} \right| \leq \frac{4}{5}\left|x_{n}-x_{n-1} \right|$ and so on until I get to $\left | x_{n+1} - x_{n}\right| \leq (\frac{4}{5})^n \left| x_2 - x_1 \right|$. I would like to get some feedback and the what are the major flaws in my solution. Suggestions are very welcome","The question says; prove the sequence $x_{n}$ given by: $\left | x_{n+1} - x_{n}\right| \leq (\frac{4}{5})^n \ \forall \ n \in \mathbb{N} $ is convergent. Here is how I approached the problem: Since the difference between the terms applies to all $n \in \mathbb{N}$, then we know that it is a Cauchy sequence (specifically, a contractile sequence) where we can assume that $\left| x_2 - x_1 \right|=1 $ then $K = \frac{4}{5} <1$ Also since for large values of $n$, $\lim_{n\rightarrow \infty} (\frac{4}{5})^n = 0$ so the constant $(\frac{4}{5})^n$ can replace $\varepsilon$ in the criteria for contractive sequence: $\left | x_{n+1} - x_{n}\right| \leq \varepsilon $ Then we can have, $\lim_{n\rightarrow \infty} \left|x_{n+1} - x_{n} \right| = 0$ which is a convergent sequence using the contraction principle. However, it seems like I should have used another method that uses similar idea but explicitly shows that the difference between $\left|x_{n+1}-x_{n} \right| \leq \frac{4}{5}\left|x_{n}-x_{n-1} \right|$ and so on until I get to $\left | x_{n+1} - x_{n}\right| \leq (\frac{4}{5})^n \left| x_2 - x_1 \right|$. I would like to get some feedback and the what are the major flaws in my solution. Suggestions are very welcome",,"['real-analysis', 'analysis']"
91,Show that the Fourier transform of a radial function $ L^1 (\mathbb{R}) $ is also radial,Show that the Fourier transform of a radial function  is also radial, L^1 (\mathbb{R}) ,How do I prove that the Fourier transform of a radial function $ f \in L^1 (\mathbb{R}) $ is also radial function? I tried by polar coordinates but I dont got.,How do I prove that the Fourier transform of a radial function is also radial function? I tried by polar coordinates but I dont got., f \in L^1 (\mathbb{R}) ,"['real-analysis', 'analysis', 'fourier-analysis']"
92,Convergence of the Zeta and Phi functions,Convergence of the Zeta and Phi functions,,I want to show that the following functions (in the picture) are absolutely and locally uniformly convergent if real part of complex number $s$ is bigger than 1. Absolute part for zeta function is easy but I am not sure about the absolute convergence of phi function so i tried to prove it as in picture please check it for me. Is it the right name of the theorem that i used in the end. I am not sure about this theorem. And also tell me how we will check the local uniform convergence for both of them. Thanks,I want to show that the following functions (in the picture) are absolutely and locally uniformly convergent if real part of complex number $s$ is bigger than 1. Absolute part for zeta function is easy but I am not sure about the absolute convergence of phi function so i tried to prove it as in picture please check it for me. Is it the right name of the theorem that i used in the end. I am not sure about this theorem. And also tell me how we will check the local uniform convergence for both of them. Thanks,,"['real-analysis', 'analysis', 'complex-numbers', 'analytic-number-theory', 'riemann-zeta']"
93,estimating the error of $\sin(x) = x$ with Taylor's Theorem,estimating the error of  with Taylor's Theorem,\sin(x) = x,"I want to calculate the numerical error in approximating $\sin(x)=x$ with Taylor's Theorem. Furthermore, what values of $x$ will this approximation be correct to within $7$ decimal places? Here is what I have done: $\sin(x) = \sum\limits_{k=0}^n (-1)^k\dfrac{x^{2k+1}}{(2k+1)!} + E_n(x)$ Where $E_n(x) =\dfrac{f^{(n+1)}(\xi)}{(n+1)!}x^{n+1}$, $x\in (-\infty, \infty)$ and $\xi$ is between $x$ and $0$. (This is just Taylor's Theorem with Lagrange remainder ) Let $\xi = 0$ (why not). I am unsure of how I made $E_n(x)$: \begin{align} \left|\sin(x)-x\right| =& \sum\limits_{k=0}^n (-1)^k\dfrac{x^{2k+1}}{(2k+1)!} + (-1)^{2n+3}\dfrac{x^{2n+3}}{(2n+3)!} -x \\ \left|\sin(x)-x -\sum\limits_{k=0}^n (-1)^k\dfrac{x^{2k+1}}{(2k+1)!}\right|  =& \left|(-1)^{2n+3}\dfrac{x^{2n+3}}{(2n+3)!} -x\right| \\ =&\left|\dfrac{x^{2n+3}}{(2n+3)!}-x\right| \end{align} Continuing in this way find the values of $n$ and $x$ that solve: \begin{equation} \left|\dfrac{x^{2n+3}}{(2n+3)!}-x\right| \leq 10^{-7} \end{equation} Intuitively this seems off because $(2n+3)!$ grows faster than $x^{2n+3}$, and so taking the limit as $n\to\infty$ we get $\left|-x\right|=x$ and thus any value of $x \leq 10^{-7}$ would work. This just does not seem right. All help is greatly appreciated!","I want to calculate the numerical error in approximating $\sin(x)=x$ with Taylor's Theorem. Furthermore, what values of $x$ will this approximation be correct to within $7$ decimal places? Here is what I have done: $\sin(x) = \sum\limits_{k=0}^n (-1)^k\dfrac{x^{2k+1}}{(2k+1)!} + E_n(x)$ Where $E_n(x) =\dfrac{f^{(n+1)}(\xi)}{(n+1)!}x^{n+1}$, $x\in (-\infty, \infty)$ and $\xi$ is between $x$ and $0$. (This is just Taylor's Theorem with Lagrange remainder ) Let $\xi = 0$ (why not). I am unsure of how I made $E_n(x)$: \begin{align} \left|\sin(x)-x\right| =& \sum\limits_{k=0}^n (-1)^k\dfrac{x^{2k+1}}{(2k+1)!} + (-1)^{2n+3}\dfrac{x^{2n+3}}{(2n+3)!} -x \\ \left|\sin(x)-x -\sum\limits_{k=0}^n (-1)^k\dfrac{x^{2k+1}}{(2k+1)!}\right|  =& \left|(-1)^{2n+3}\dfrac{x^{2n+3}}{(2n+3)!} -x\right| \\ =&\left|\dfrac{x^{2n+3}}{(2n+3)!}-x\right| \end{align} Continuing in this way find the values of $n$ and $x$ that solve: \begin{equation} \left|\dfrac{x^{2n+3}}{(2n+3)!}-x\right| \leq 10^{-7} \end{equation} Intuitively this seems off because $(2n+3)!$ grows faster than $x^{2n+3}$, and so taking the limit as $n\to\infty$ we get $\left|-x\right|=x$ and thus any value of $x \leq 10^{-7}$ would work. This just does not seem right. All help is greatly appreciated!",,"['analysis', 'numerical-methods', 'taylor-expansion']"
94,"Sequence of functions convergence, interchanging limit/integral","Sequence of functions convergence, interchanging limit/integral",,"This seems like a simple enough problem, but I'm having some trouble. Here it is: Suppose $f:[0,\infty)\rightarrow \mathbb{R} $ is continuous, and $\lim\limits_{x\rightarrow \infty}f(x)=L.$ Then $\lim\limits_{n \rightarrow \infty}\int_0^1 f(nx)dx=L.$ My thoughts: First, I think since $f(x)$ is continuous on $[0,\infty)$, $f(nx)$ is continuous on $[0,1]$ (I haven't proved this), and hence integrable on $[0,1]$. Then, if we can show $f(nx)$ converges to $L$ uniformly on $[0,1]$, then we may interchange the limit and integral as $$\lim\limits_{n \rightarrow \infty}\int_0^1 f(nx)dx=\int_0^1 \lim\limits_{n \rightarrow \infty}f(nx)dx=\int_0^1Ldx=L. $$ Your thoughts? Thanks in advance for your suggestions. (This is not homework)","This seems like a simple enough problem, but I'm having some trouble. Here it is: Suppose $f:[0,\infty)\rightarrow \mathbb{R} $ is continuous, and $\lim\limits_{x\rightarrow \infty}f(x)=L.$ Then $\lim\limits_{n \rightarrow \infty}\int_0^1 f(nx)dx=L.$ My thoughts: First, I think since $f(x)$ is continuous on $[0,\infty)$, $f(nx)$ is continuous on $[0,1]$ (I haven't proved this), and hence integrable on $[0,1]$. Then, if we can show $f(nx)$ converges to $L$ uniformly on $[0,1]$, then we may interchange the limit and integral as $$\lim\limits_{n \rightarrow \infty}\int_0^1 f(nx)dx=\int_0^1 \lim\limits_{n \rightarrow \infty}f(nx)dx=\int_0^1Ldx=L. $$ Your thoughts? Thanks in advance for your suggestions. (This is not homework)",,"['real-analysis', 'analysis', 'uniform-convergence']"
95,Rudin Real and complex analysis question[Differentiation],Rudin Real and complex analysis question[Differentiation],,"At the beginning of the chapter on differentiation, the following theorem is stated without proof. Apparently it is so trivial that it does not require justification. I however don't find it so trivial and would appreciate if someone could assist me in proving it. The theorem is the following: Let $\mu$ be a complex borel measure on $\Bbb R$ and define $f(x):=\mu((-\infty,x))$ . Following statements are equivalent: 1.) $f$ is differentiable at $x$ and $f'(x)=A$ 2.) For every $\epsilon>0$ there exists $\delta>0$ so that for all open segments $I$ containing $x$ with length $<\delta$ the inequality $|\frac{\mu(I)}{m(I)}-A|<\epsilon$ ,where $m$ denotes lebesgue measure. Thanks in advance!","At the beginning of the chapter on differentiation, the following theorem is stated without proof. Apparently it is so trivial that it does not require justification. I however don't find it so trivial and would appreciate if someone could assist me in proving it. The theorem is the following: Let be a complex borel measure on and define . Following statements are equivalent: 1.) is differentiable at and 2.) For every there exists so that for all open segments containing with length the inequality ,where denotes lebesgue measure. Thanks in advance!","\mu \Bbb R f(x):=\mu((-\infty,x)) f x f'(x)=A \epsilon>0 \delta>0 I x <\delta |\frac{\mu(I)}{m(I)}-A|<\epsilon m","['real-analysis', 'analysis', 'measure-theory', 'derivatives']"
96,Inequality between product measure and its projection,Inequality between product measure and its projection,,"$\newcommand{\smin}{\setminus} \newcommand{\sset}{\subseteq}$If $\mu$ is a measure on $X$, $ \nu $ a measure on $Y, \gamma $ a measure on $X \times Y$ s.t. $ \gamma(A \times Y) = \mu (A) $ and $\gamma(X \times B) = \nu(B)$, $ \forall A \sset X $ $\mu$-measurable and $B \sset Y$ $\nu$-measurable then  \begin{array}\\  &\gamma ( X \times Y \smin A \times B) \\ & \leq \gamma(((X \smin A) \times Y) \cup (X \times (Y \smin B))) \\ &\leq \gamma((X \smin A) \times Y) + \gamma((X \times Y) \smin B)\\ &=  \mu(X \smin A) + \nu (Y \smin B) \end{array} Does this make sense?","$\newcommand{\smin}{\setminus} \newcommand{\sset}{\subseteq}$If $\mu$ is a measure on $X$, $ \nu $ a measure on $Y, \gamma $ a measure on $X \times Y$ s.t. $ \gamma(A \times Y) = \mu (A) $ and $\gamma(X \times B) = \nu(B)$, $ \forall A \sset X $ $\mu$-measurable and $B \sset Y$ $\nu$-measurable then  \begin{array}\\  &\gamma ( X \times Y \smin A \times B) \\ & \leq \gamma(((X \smin A) \times Y) \cup (X \times (Y \smin B))) \\ &\leq \gamma((X \smin A) \times Y) + \gamma((X \times Y) \smin B)\\ &=  \mu(X \smin A) + \nu (Y \smin B) \end{array} Does this make sense?",,['analysis']
97,How to find all polynomials with rational coefficients s.t $\forall r\notin\mathbb Q :f(r)\notin\mathbb Q$,How to find all polynomials with rational coefficients s.t,\forall r\notin\mathbb Q :f(r)\notin\mathbb Q,"How to find all polynomials with rational coefficients$f(x)=a_nx^n+\cdots+a_1x+a_0$, $a_i\in \mathbb Q$, such that $$\forall r\in\mathbb R\setminus\mathbb Q,\quad f(r)\in\mathbb R\setminus\mathbb Q.$$ thanks in advance","How to find all polynomials with rational coefficients$f(x)=a_nx^n+\cdots+a_1x+a_0$, $a_i\in \mathbb Q$, such that $$\forall r\in\mathbb R\setminus\mathbb Q,\quad f(r)\in\mathbb R\setminus\mathbb Q.$$ thanks in advance",,['analysis']
98,Error estimate for sequence defining e,Error estimate for sequence defining e,,Is there an estimate for error of $e-\left(1+\frac1n\right)^n$ ?thanks,Is there an estimate for error of $e-\left(1+\frac1n\right)^n$ ?thanks,,"['real-analysis', 'analysis']"
99,Showing that $\lim_{\delta \to 0^+} \frac{1}{\delta} \int_x^{x + \delta} f(t) \ \mathrm{d}t = f(x)$,Showing that,\lim_{\delta \to 0^+} \frac{1}{\delta} \int_x^{x + \delta} f(t) \ \mathrm{d}t = f(x),"I'm working on proving the following equation: $\lim_{\delta \to 0^+} \frac{1}{\delta} \int_x^{x + \delta} f(t) \ \mathrm{d}t = f(x)$, where $f$ is given to be Riemann integrable and continuous on [0,1]. Attempted Proof I thought about applying the Fundamental Theorem of Calculus to arrive at, for the LHS, something like, $\lim_{\delta \to 0^+} \frac{F(x + \delta) - F(x)}{\delta}$, where $F$ is the antiderivative of $f(t).$ However, this feels very circular to me, since this expression of course equals $F'(x) = f(x)$, and we would then have equality on $(0, 1).$ I'd be grateful for any direction. Thanks so much.","I'm working on proving the following equation: $\lim_{\delta \to 0^+} \frac{1}{\delta} \int_x^{x + \delta} f(t) \ \mathrm{d}t = f(x)$, where $f$ is given to be Riemann integrable and continuous on [0,1]. Attempted Proof I thought about applying the Fundamental Theorem of Calculus to arrive at, for the LHS, something like, $\lim_{\delta \to 0^+} \frac{F(x + \delta) - F(x)}{\delta}$, where $F$ is the antiderivative of $f(t).$ However, this feels very circular to me, since this expression of course equals $F'(x) = f(x)$, and we would then have equality on $(0, 1).$ I'd be grateful for any direction. Thanks so much.",,"['real-analysis', 'analysis', 'integration']"
