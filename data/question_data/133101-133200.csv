,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Solving systems of differential equations with laplace transform,Solving systems of differential equations with laplace transform,,"I'm working on the system: $dx/dt=x-2y$, $x(0)=-1$ $dy/dt=5x-y$, $y(0)=6$ This seems like it should be easy to solve and yet I'm struggling. I am trying to use the Laplace transform and I have done this to every term but I get stuck when you I need to put everything in terms of $X(s)$ and $Y(s)$.","I'm working on the system: $dx/dt=x-2y$, $x(0)=-1$ $dy/dt=5x-y$, $y(0)=6$ This seems like it should be easy to solve and yet I'm struggling. I am trying to use the Laplace transform and I have done this to every term but I get stuck when you I need to put everything in terms of $X(s)$ and $Y(s)$.",,['ordinary-differential-equations']
1,Inequality with cosine and sine,Inequality with cosine and sine,,"Let $A:=f^2+g^2$, where $f,g$ are functions of $x$ such that $$f'=(c-1)(f\cos(x)\sin(x)+g\sin^2(x)),$$ $$g'=-(c-1)(f\cos^2(x)+g\cos(x)\sin(x)),$$ for some constant $c$. (Note: $f'\cos(x)+g'\sin(x)=0$) How do I show that $A'\leq4|c-1|A?$ I see that \begin{align} A'&=2ff'+2gg'\\ &=2(c-1)\left(f^2\cos(x)\sin(x)+fg\sin^2(x)-fg\cos^2(x)-g^2\cos(x)\sin(x)\right)\\ &=2(c-1)(f\cos(x)+g\sin(x))(f\sin(x)-g\cos(x)). \end{align} But how does the desired inequality follow?","Let $A:=f^2+g^2$, where $f,g$ are functions of $x$ such that $$f'=(c-1)(f\cos(x)\sin(x)+g\sin^2(x)),$$ $$g'=-(c-1)(f\cos^2(x)+g\cos(x)\sin(x)),$$ for some constant $c$. (Note: $f'\cos(x)+g'\sin(x)=0$) How do I show that $A'\leq4|c-1|A?$ I see that \begin{align} A'&=2ff'+2gg'\\ &=2(c-1)\left(f^2\cos(x)\sin(x)+fg\sin^2(x)-fg\cos^2(x)-g^2\cos(x)\sin(x)\right)\\ &=2(c-1)(f\cos(x)+g\sin(x))(f\sin(x)-g\cos(x)). \end{align} But how does the desired inequality follow?",,"['ordinary-differential-equations', 'trigonometry']"
2,ODE with modulus,ODE with modulus,,"I want to solve the following ODE: $$y''=|y|$$ However I don't really know how to deal with this modulus! I tried to mess with the particular cases for $y>0$ and $y<0$ which have simple solutions. Well, of course if $y\geq 0$ then $y(x)=Ae^x+Be^{−x}$ is an solution and if $y<0$ $y(x)=C\cos x+D\sin x$ is an solution. But I couldn't find constraints on these constants and neither could I find a general solution I would appreciate any help.","I want to solve the following ODE: $$y''=|y|$$ However I don't really know how to deal with this modulus! I tried to mess with the particular cases for $y>0$ and $y<0$ which have simple solutions. Well, of course if $y\geq 0$ then $y(x)=Ae^x+Be^{−x}$ is an solution and if $y<0$ $y(x)=C\cos x+D\sin x$ is an solution. But I couldn't find constraints on these constants and neither could I find a general solution I would appreciate any help.",,['ordinary-differential-equations']
3,Second order PDE with mixed derivative,Second order PDE with mixed derivative,,"In my study of QFT I've faced the following PDE in Peskin&Schroeder textbook: $$\frac{\partial^2 g(x,\xi)}{\partial w \partial \xi}=A g(x,\xi),$$ where $w=\ln(1/x)$ and $A$ is a constant. The book provides a solution only in the limit $w\xi\gg1$ for some reason. $$g=K(Q^2)exp(\left(4w(\xi-\xi_0)\right)^{1/2}),$$ where $\xi=\ln \ln \left(Q^2\right)$, and $K(Q^2)$ is a function determined from initial condition. So, my questions are: Why dont they use separation of variables for this PDE? Is there any method to obtain general solution? Or can you explain how they get this approximate form?","In my study of QFT I've faced the following PDE in Peskin&Schroeder textbook: $$\frac{\partial^2 g(x,\xi)}{\partial w \partial \xi}=A g(x,\xi),$$ where $w=\ln(1/x)$ and $A$ is a constant. The book provides a solution only in the limit $w\xi\gg1$ for some reason. $$g=K(Q^2)exp(\left(4w(\xi-\xi_0)\right)^{1/2}),$$ where $\xi=\ln \ln \left(Q^2\right)$, and $K(Q^2)$ is a function determined from initial condition. So, my questions are: Why dont they use separation of variables for this PDE? Is there any method to obtain general solution? Or can you explain how they get this approximate form?",,"['ordinary-differential-equations', 'multivariable-calculus', 'partial-differential-equations']"
4,Solving the differential equation $y^{(4)} + 4y = 0$,Solving the differential equation,y^{(4)} + 4y = 0,"I have trouble solving the differential equation $y^{(4)} + 4y = 0$ This is a linear differential equation with constant coefficients. Solving for the roots of the associated polynomial gives: $$r^4 + 4r = 0 \iff r(r^3 + 4) = 0 \iff r(r+\sqrt[3]{4})(r^2 - \sqrt[3]{4}r + \sqrt[3]{4}^2) = 0$$ I get very ugly solutions for the roots of this quadratic equation. My book says the answer should be $$y = e^x(c_1\cos(x) + c_2\sin(x))+ e^{-x}(c_3\cos(x) + c_4\sin(x))$$ and wolfram alpha confirms this. But, in general, if $a + bi$ is a complex root of the polynomial of multiplicity $1$ then $e^{ax}(\cos(bx) + \sin(bx))$ will be a solution, so it seems I won't be able to reach that solution. Any help will be greatly appreciated!","I have trouble solving the differential equation $y^{(4)} + 4y = 0$ This is a linear differential equation with constant coefficients. Solving for the roots of the associated polynomial gives: $$r^4 + 4r = 0 \iff r(r^3 + 4) = 0 \iff r(r+\sqrt[3]{4})(r^2 - \sqrt[3]{4}r + \sqrt[3]{4}^2) = 0$$ I get very ugly solutions for the roots of this quadratic equation. My book says the answer should be $$y = e^x(c_1\cos(x) + c_2\sin(x))+ e^{-x}(c_3\cos(x) + c_4\sin(x))$$ and wolfram alpha confirms this. But, in general, if $a + bi$ is a complex root of the polynomial of multiplicity $1$ then $e^{ax}(\cos(bx) + \sin(bx))$ will be a solution, so it seems I won't be able to reach that solution. Any help will be greatly appreciated!",,[]
5,Solve the ODE $\frac{\mathrm dx}{\mathrm dt}=(x+t)t$,Solve the ODE,\frac{\mathrm dx}{\mathrm dt}=(x+t)t,"$\def\d{\mathrm{d}}$How to solve this ODE? (From a real analysis course, existence and uniqueness of ODE) $$\frac{\d x}{\d t}=(x+t)t. \quad \forall t\in [0,1], \quad x(0)=0$$ My attempt: $$\dot{x}=\frac{\d x}{\d t}=V(x(t),t)=(x+t)t$$ So we can use $\phi_v(x)$ such that $$\phi_v^1(x,t)=x(0)+\int_{0}^{t}(x+t)t\,\d t= \frac{t^3}{3}+\frac{t^2x}{2}$$ with $x(0)=0$. Applying $\phi_v$ to approximate $\phi^n_v$: $$ \phi_v^2(x,t)=\int_{0}^{t} \left( \frac{t^3x}{2}+\frac{t^4}{3}+t^2 \right) \, \d t= \frac{t^4x}{8}+\frac{t^5}{15}+\frac{t^3}{3},\\ \phi_v^3(x,t)=\int_0^t \left( \frac{t^5x}{8}+\frac{t^5}{15}+t^4 \right) \,\d t= \frac{t^6x}{48} +\frac{t^7}{106}+\frac{t^3}{15}+\frac{t^3}{3}. $$ Am I on the right track? How should I finish this?","$\def\d{\mathrm{d}}$How to solve this ODE? (From a real analysis course, existence and uniqueness of ODE) $$\frac{\d x}{\d t}=(x+t)t. \quad \forall t\in [0,1], \quad x(0)=0$$ My attempt: $$\dot{x}=\frac{\d x}{\d t}=V(x(t),t)=(x+t)t$$ So we can use $\phi_v(x)$ such that $$\phi_v^1(x,t)=x(0)+\int_{0}^{t}(x+t)t\,\d t= \frac{t^3}{3}+\frac{t^2x}{2}$$ with $x(0)=0$. Applying $\phi_v$ to approximate $\phi^n_v$: $$ \phi_v^2(x,t)=\int_{0}^{t} \left( \frac{t^3x}{2}+\frac{t^4}{3}+t^2 \right) \, \d t= \frac{t^4x}{8}+\frac{t^5}{15}+\frac{t^3}{3},\\ \phi_v^3(x,t)=\int_0^t \left( \frac{t^5x}{8}+\frac{t^5}{15}+t^4 \right) \,\d t= \frac{t^6x}{48} +\frac{t^7}{106}+\frac{t^3}{15}+\frac{t^3}{3}. $$ Am I on the right track? How should I finish this?",,['ordinary-differential-equations']
6,Solution of differential equation which is quadratic in $\frac{dy}{dx}$,Solution of differential equation which is quadratic in,\frac{dy}{dx},Consider the following differential equation $$ x\frac{dy}{dx} + y = x^4(\frac{dy}{dx})^2$$ I used the quadratic formula and got $\frac{dy}{dx} = \frac{x \pm \sqrt {x^2 + 4yx^4}}{2x^4}$. Now how to proceed or is there any other method? Edit: I proceeded further this way $\frac{dy}{dx} = \frac{1 \pm \sqrt {1 +4yx^2}}{2x^3}$ Let $1 + 4yx^4 = t^2$ then $8yx + 4x^2\frac{dy}{dx} = 2t\frac{dt}{dx}$ multiplying throughout with $x$ I got $4yx^3 +2x^3\frac{dy}{dx} = tx\frac{dt}{dx}$ now putting the value of $2x^3\frac{dy}{dx}$ and $t^2$ in 1st equation I got $tx\frac{dt}{dx} - t^2 = \pm t$ which gave 2 solutions $$t = 0$$ $$and$$ $$x\frac{dt}{dx} = t \pm 1$$  Using $t  = 0$ I got one solution $$1 + 4x^2y = 0$$ and the other by solving the differential equation $x\frac{dt}{dx} = t \pm 1$ as $$c^2x^2 + 4y^2x^2 = 4cyx^2 + 2c$$ Now the answer to this question which I have got is $$4x^2y + 1 = 0$$ $$and$$ $$xy - c^2x + c = 0$$ Why I couldn't get the second solution right?,Consider the following differential equation $$ x\frac{dy}{dx} + y = x^4(\frac{dy}{dx})^2$$ I used the quadratic formula and got $\frac{dy}{dx} = \frac{x \pm \sqrt {x^2 + 4yx^4}}{2x^4}$. Now how to proceed or is there any other method? Edit: I proceeded further this way $\frac{dy}{dx} = \frac{1 \pm \sqrt {1 +4yx^2}}{2x^3}$ Let $1 + 4yx^4 = t^2$ then $8yx + 4x^2\frac{dy}{dx} = 2t\frac{dt}{dx}$ multiplying throughout with $x$ I got $4yx^3 +2x^3\frac{dy}{dx} = tx\frac{dt}{dx}$ now putting the value of $2x^3\frac{dy}{dx}$ and $t^2$ in 1st equation I got $tx\frac{dt}{dx} - t^2 = \pm t$ which gave 2 solutions $$t = 0$$ $$and$$ $$x\frac{dt}{dx} = t \pm 1$$  Using $t  = 0$ I got one solution $$1 + 4x^2y = 0$$ and the other by solving the differential equation $x\frac{dt}{dx} = t \pm 1$ as $$c^2x^2 + 4y^2x^2 = 4cyx^2 + 2c$$ Now the answer to this question which I have got is $$4x^2y + 1 = 0$$ $$and$$ $$xy - c^2x + c = 0$$ Why I couldn't get the second solution right?,,['ordinary-differential-equations']
7,Relation between numbers of stable and unstable fixed points or manifolds,Relation between numbers of stable and unstable fixed points or manifolds,,"In 1D dynamical systems, it is well-known that in general between any two stable fixed points there is an unstable fixed point. How does this result generalize to higher dimensions? Are there general theorems that establish a connection between the number of stable fixed points versus unstable fixed points? Of course in higher dimension the extra complication is that we could have higher dimensional manifolds defined by $\frac{dx}{dt} = 0$, e.g. lines or surfaces. Are there general results for their stability in relation to the number of stable and unstable lines / surfaces / higher dimensional manifolds? How about systems defined not on $\mathbb{R}^n$ but on some manifold?","In 1D dynamical systems, it is well-known that in general between any two stable fixed points there is an unstable fixed point. How does this result generalize to higher dimensions? Are there general theorems that establish a connection between the number of stable fixed points versus unstable fixed points? Of course in higher dimension the extra complication is that we could have higher dimensional manifolds defined by $\frac{dx}{dt} = 0$, e.g. lines or surfaces. Are there general results for their stability in relation to the number of stable and unstable lines / surfaces / higher dimensional manifolds? How about systems defined not on $\mathbb{R}^n$ but on some manifold?",,"['ordinary-differential-equations', 'differential-geometry', 'dynamical-systems']"
8,Inverse Laplace transform of $e^{-\sqrt s}$,Inverse Laplace transform of,e^{-\sqrt s},How could one possibly find the inverse Laplace transform of $e^{-\sqrt{s}}$ using a table of Laplace transforms?,How could one possibly find the inverse Laplace transform of $e^{-\sqrt{s}}$ using a table of Laplace transforms?,,"['ordinary-differential-equations', 'partial-differential-equations', 'laplace-transform']"
9,Solving the equation $(x+1)xy''+(x+2)y'-y=0$,Solving the equation,(x+1)xy''+(x+2)y'-y=0,Find the general solution to the equation $(x+1)xy''+(x+2)y'-y=0$ given that one of the solutions is a polynomial. Here's what I did: plugging in $y=Ax^2+Bx+C$ we find that $y_1=x+2$ solves the equation. Then we can try to find a solution of the form $y_2=y_1 z = (x+2) z$. From the original differential equation we obtain $z''+\left ( \frac{2}{x+2}+\frac{x+2}{x(x+1)} \right)z'=0$. Then we must do fraction decomposition and the substitution $w=z'$ so we obtain $w=\frac{x+1}{x^2 (x+1)^2}$. Then we need to integrate the function $\frac{x+1}{x^2 (x+1)^2}$ which requires fraction decomposition once again and we arrive at the result $z=-\frac{1}{2x(x+2)}$ which in turn gives us $y_2=-1/(2x)$. Although I got the correct result I wonder whether there is a simpler way of arriving at this solution (avoiding so many fraction decompositions and integrations which I omitted here). Maybe there's some better substitution that will work?,Find the general solution to the equation $(x+1)xy''+(x+2)y'-y=0$ given that one of the solutions is a polynomial. Here's what I did: plugging in $y=Ax^2+Bx+C$ we find that $y_1=x+2$ solves the equation. Then we can try to find a solution of the form $y_2=y_1 z = (x+2) z$. From the original differential equation we obtain $z''+\left ( \frac{2}{x+2}+\frac{x+2}{x(x+1)} \right)z'=0$. Then we must do fraction decomposition and the substitution $w=z'$ so we obtain $w=\frac{x+1}{x^2 (x+1)^2}$. Then we need to integrate the function $\frac{x+1}{x^2 (x+1)^2}$ which requires fraction decomposition once again and we arrive at the result $z=-\frac{1}{2x(x+2)}$ which in turn gives us $y_2=-1/(2x)$. Although I got the correct result I wonder whether there is a simpler way of arriving at this solution (avoiding so many fraction decompositions and integrations which I omitted here). Maybe there's some better substitution that will work?,,['ordinary-differential-equations']
10,Why is a homogeneous function called homogeneous?,Why is a homogeneous function called homogeneous?,,"Why is a homogeneous function called homogeneous? When I ask this, I don't mean, ""Show me how to algebraically manipulate a function whose input has been multiplied by a constant to get the original function multiplied by the same constant."" I mean--why do we use the word ""homogeneous""? That word in particular must have been chosen for a reason; what is it meant to communicate in this context?","Why is a homogeneous function called homogeneous? When I ask this, I don't mean, ""Show me how to algebraically manipulate a function whose input has been multiplied by a constant to get the original function multiplied by the same constant."" I mean--why do we use the word ""homogeneous""? That word in particular must have been chosen for a reason; what is it meant to communicate in this context?",,"['linear-algebra', 'ordinary-differential-equations', 'terminology']"
11,Solving $\frac{dy}{dx} = \frac{ay+b}{cy+d}$,Solving,\frac{dy}{dx} = \frac{ay+b}{cy+d},"I'm on the section of my book about separable equations, and it asks me to solve this: $$\frac{dy}{dx} = \frac{ay+b}{cy+d}$$ So I must separate it into something like: $f(y)\frac{dy}{dx} + g(x) = constant$ *note that there are no $g(x)$ but I don't think it's possible. Is there something I'm missing?","I'm on the section of my book about separable equations, and it asks me to solve this: $$\frac{dy}{dx} = \frac{ay+b}{cy+d}$$ So I must separate it into something like: $f(y)\frac{dy}{dx} + g(x) = constant$ *note that there are no $g(x)$ but I don't think it's possible. Is there something I'm missing?",,"['calculus', 'integration', 'ordinary-differential-equations']"
12,solving $y' - yy'x^2-x=0$,solving,y' - yy'x^2-x=0,"How can i solve this? $$y' - yy'x^2-x=0$$ I only got to the homogeneous solution wich I found is (I just divided by $y'$) $$y=\frac{1}{x^2}$$ But I don't know how to get the particular solution, I have for certain that it's not a constant as I tried to find it in every way possibile, could anybody help me? Thanks.","How can i solve this? $$y' - yy'x^2-x=0$$ I only got to the homogeneous solution wich I found is (I just divided by $y'$) $$y=\frac{1}{x^2}$$ But I don't know how to get the particular solution, I have for certain that it's not a constant as I tried to find it in every way possibile, could anybody help me? Thanks.",,['ordinary-differential-equations']
13,Are all solutions to the ODE $ay''(t) + by'(t) + cy(t) = 0$ of the form $y(t)= \alpha e^{(\beta + i\gamma)t}$?,Are all solutions to the ODE  of the form ?,ay''(t) + by'(t) + cy(t) = 0 y(t)= \alpha e^{(\beta + i\gamma)t},"Let $a$ $b$ and $c$ be complex numbers. Consider the complex solution of the ODE $$ay''(t) + by'(t) + cy(t) = 0.$$ If there exist solutions to this, are they necessarily of the form $$y(t)= \alpha e^{(\beta + i\gamma)t}$$ for some constants $\alpha, \beta, \gamma$? Here $i$ is the imaginary unit. In every example I've seen, this has been the case. But I am not sure if it necessarily the case.","Let $a$ $b$ and $c$ be complex numbers. Consider the complex solution of the ODE $$ay''(t) + by'(t) + cy(t) = 0.$$ If there exist solutions to this, are they necessarily of the form $$y(t)= \alpha e^{(\beta + i\gamma)t}$$ for some constants $\alpha, \beta, \gamma$? Here $i$ is the imaginary unit. In every example I've seen, this has been the case. But I am not sure if it necessarily the case.",,"['analysis', 'ordinary-differential-equations', 'complex-numbers']"
14,Why does the coordinate transformation from Cartesian coordinates leads to an additional term in the biharmonic operator in spherical coordinates,Why does the coordinate transformation from Cartesian coordinates leads to an additional term in the biharmonic operator in spherical coordinates,,"I am trying to solve a problem in physics where the biharmonic operator is involved. I think that the bihahmonic operator can be obtained by taking twice the Laplace operator, such that $\nabla^4 f = \nabla^2 (\nabla^2 f)$.  Here $f$ is a function of the polar angle $\phi$ only (i.e. in the axisymmetric case for the sake of simplicity.) In this way, the Laplace operator reads $$ \nabla^2 f(\phi) = \frac{1}{a^2 \sin\phi} \frac{\partial}{\partial \phi} \left( \sin\phi \frac{\partial f}{\partial \phi} \right) \, , $$ where $a$ being the sphere radius. In principle, the biharmonic operator is obtained after applying twice the above operation, to obtain $$ \nabla^4 f(\phi) = \frac{1}{a^4}  \left(  \frac{\partial^4 f}{\partial \phi^4}  + 2\cot\phi \frac{\partial^3 f}{\partial \phi^3} -(2+\cot^2 \phi) \frac{\partial^2 f}{\partial \phi^2} +\cot\phi (1+\cot^2\phi) \frac{\partial f}{\partial \phi} \right) \, . $$ However, if we start from the biharmonic equation in Cartesian coordinates, namely $$ \nabla^4 f(x,y,z) = \frac{\partial^4 f}{\partial x^4}  + \frac{\partial^4 f}{\partial y^4}  + \frac{\partial^4 f}{\partial z^4} + 2 \left(  \frac{\partial^4 f}{\partial x^2 \partial y^2} + \frac{\partial^4 f}{\partial y^2 \partial z^2} + \frac{\partial^4 f}{\partial x^2 \partial z^2} \right) \, , $$ and apply the coordinate transformation to spherical, e.g. using Maple PDEchangecoords and of course drop the dependence on $r$ and $\theta$, I get the following biharmonic $$ \nabla^4 f(\phi) = \frac{1}{a^4}  \left(  \frac{\partial^4 f}{\partial \phi^4}  + 2\cot\phi \frac{\partial^3 f}{\partial \phi^3} -\cot^2 \phi\frac{\partial^2 f}{\partial \phi^2} +\cot\phi (3+\cot^2\phi) \frac{\partial f}{\partial \phi} \right) \, , $$ i.e. the following additional terms appear $$ \frac{2}{a^4} \left( \frac{\partial^2 f}{\partial \phi^2} + \frac{\partial f}{\partial \phi} \cot \phi \right) \, ,  $$ My question is which resulting biharmonic operator is correct? What is the reason behind the discrepancy between the two results. Thank you, a","I am trying to solve a problem in physics where the biharmonic operator is involved. I think that the bihahmonic operator can be obtained by taking twice the Laplace operator, such that $\nabla^4 f = \nabla^2 (\nabla^2 f)$.  Here $f$ is a function of the polar angle $\phi$ only (i.e. in the axisymmetric case for the sake of simplicity.) In this way, the Laplace operator reads $$ \nabla^2 f(\phi) = \frac{1}{a^2 \sin\phi} \frac{\partial}{\partial \phi} \left( \sin\phi \frac{\partial f}{\partial \phi} \right) \, , $$ where $a$ being the sphere radius. In principle, the biharmonic operator is obtained after applying twice the above operation, to obtain $$ \nabla^4 f(\phi) = \frac{1}{a^4}  \left(  \frac{\partial^4 f}{\partial \phi^4}  + 2\cot\phi \frac{\partial^3 f}{\partial \phi^3} -(2+\cot^2 \phi) \frac{\partial^2 f}{\partial \phi^2} +\cot\phi (1+\cot^2\phi) \frac{\partial f}{\partial \phi} \right) \, . $$ However, if we start from the biharmonic equation in Cartesian coordinates, namely $$ \nabla^4 f(x,y,z) = \frac{\partial^4 f}{\partial x^4}  + \frac{\partial^4 f}{\partial y^4}  + \frac{\partial^4 f}{\partial z^4} + 2 \left(  \frac{\partial^4 f}{\partial x^2 \partial y^2} + \frac{\partial^4 f}{\partial y^2 \partial z^2} + \frac{\partial^4 f}{\partial x^2 \partial z^2} \right) \, , $$ and apply the coordinate transformation to spherical, e.g. using Maple PDEchangecoords and of course drop the dependence on $r$ and $\theta$, I get the following biharmonic $$ \nabla^4 f(\phi) = \frac{1}{a^4}  \left(  \frac{\partial^4 f}{\partial \phi^4}  + 2\cot\phi \frac{\partial^3 f}{\partial \phi^3} -\cot^2 \phi\frac{\partial^2 f}{\partial \phi^2} +\cot\phi (3+\cot^2\phi) \frac{\partial f}{\partial \phi} \right) \, , $$ i.e. the following additional terms appear $$ \frac{2}{a^4} \left( \frac{\partial^2 f}{\partial \phi^2} + \frac{\partial f}{\partial \phi} \cot \phi \right) \, ,  $$ My question is which resulting biharmonic operator is correct? What is the reason behind the discrepancy between the two results. Thank you, a",,"['ordinary-differential-equations', 'differential-geometry', 'partial-differential-equations', 'harmonic-analysis']"
15,Periodic solutions of $x'=x^2-1-\cos t$,Periodic solutions of,x'=x^2-1-\cos t,"Consider $x'=x^2-1-\cos t$. What can be said about the existence of periodic solutions for this equation? I'm not sure if periodic solutions exist, but if they do, they must have period equal to $ 2\pi$ and $x(0)=x(2k\pi)$ for $\forall k\in\mathbb Z$. New Edit: I guess that may use the following lemma: Lemma: Consider the differential equation $x' = f (t , x)$ where $f(t,   x)$ is continuously differentiable in $t$ and $x$. Suppose that    $f (t + T, x) = f (t , x)$     for all t . Suppose there are constants $p$, $q$ such that     $f (t , p) > 0, f (t , q) < 0$     for all $t$ then there is a periodic solution $x(t )$ for this equation  with      $p < x(0) < q$. Realy, I consider $p=2$ and $q=0$ but $f(t,q)=-1-\cos t\leq 0$ and this inequality is not strictly.","Consider $x'=x^2-1-\cos t$. What can be said about the existence of periodic solutions for this equation? I'm not sure if periodic solutions exist, but if they do, they must have period equal to $ 2\pi$ and $x(0)=x(2k\pi)$ for $\forall k\in\mathbb Z$. New Edit: I guess that may use the following lemma: Lemma: Consider the differential equation $x' = f (t , x)$ where $f(t,   x)$ is continuously differentiable in $t$ and $x$. Suppose that    $f (t + T, x) = f (t , x)$     for all t . Suppose there are constants $p$, $q$ such that     $f (t , p) > 0, f (t , q) < 0$     for all $t$ then there is a periodic solution $x(t )$ for this equation  with      $p < x(0) < q$. Realy, I consider $p=2$ and $q=0$ but $f(t,q)=-1-\cos t\leq 0$ and this inequality is not strictly.",,"['ordinary-differential-equations', 'dynamical-systems']"
16,Zero's of non trivial solution of an second order ordinary differential equation,Zero's of non trivial solution of an second order ordinary differential equation,,"What can we say about the zero's of  any non-trivial solution of the linear differential equation$$y^{''}+q(x)y=0$$ (where $q(x)$ is positive monotonically increasing continuous function of $x$). Can we say that it must have infinitely many zeros in $\mathbb{R}?$ For example $y^{''}+y=0$ has $sin(x)$ as a non trivial solution, which has infinitely many zero's. Please help me to prove the general result about infinitely many zero's. Thanks in advance.","What can we say about the zero's of  any non-trivial solution of the linear differential equation$$y^{''}+q(x)y=0$$ (where $q(x)$ is positive monotonically increasing continuous function of $x$). Can we say that it must have infinitely many zeros in $\mathbb{R}?$ For example $y^{''}+y=0$ has $sin(x)$ as a non trivial solution, which has infinitely many zero's. Please help me to prove the general result about infinitely many zero's. Thanks in advance.",,['ordinary-differential-equations']
17,A thief and a policeman,A thief and a policeman,,"A policeman desperately tries to catch a thief that is $a$ meters away. The thief has the constant velocity $v$ , and the policeman has the constant velocity $k\cdot v$ , with $k > 1$ . The policeman starts at $(0, 0)$ and the thief at $(0, a)$ . The thief never changes its direction (not a very smart thief) and always runs straight to the right. The policeman is a bit smarter and always looks directly at the thief while running. At $(a, a)$ , the thief is caught. For illustration, see this image: Find $k$ . I found a solution (numerical approximation) but the answer is very unexpected :) I used Excel and the method of small steps. For a very small step and about $10^6$ iterations, I got the first six digits of the golden ratio. I want to find a proof but I have no clue at all.","A policeman desperately tries to catch a thief that is meters away. The thief has the constant velocity , and the policeman has the constant velocity , with . The policeman starts at and the thief at . The thief never changes its direction (not a very smart thief) and always runs straight to the right. The policeman is a bit smarter and always looks directly at the thief while running. At , the thief is caught. For illustration, see this image: Find . I found a solution (numerical approximation) but the answer is very unexpected :) I used Excel and the method of small steps. For a very small step and about iterations, I got the first six digits of the golden ratio. I want to find a proof but I have no clue at all.","a v k\cdot v k > 1 (0, 0) (0, a) (a, a) k 10^6","['calculus', 'ordinary-differential-equations', 'vector-analysis']"
18,how to bring the PDE $u_{tt}-u_{xx} = x^2 -t^2$ to the canonical form,how to bring the PDE  to the canonical form,u_{tt}-u_{xx} = x^2 -t^2,"How to bring to the canonical form and solve the below PDE? $$u_{tt}-u_{xx} = x^2 -t^2$$ I recognize that it is a hyperbolic PDE, as the $b^2-4ac=(-4(1)(-1))=4 > 0$. I don't know how to proceed further to get the canonical form. I know how to deal with something like $u_{tt}-u_{xx} = 0$. With $\ RHS =0 \ $ I would use the equation for characteristic  $R (\frac{\partial^2 dy}{\partial dx})-2S (\frac{\partial^2 dy}{\partial dx})+T=0$ , define the $\xi$ and $\eta$ in terms of $x$ and $y$, calculate the first and second partial derivatives and substitute them into the initial equation. Here the function on the right hand side $x^2 -t^2$ complicates matter. How the RHS=X^2-t^2 changes the standard wave equation $u_{tt}−u_{xx}=0$ in terms of interpretation?","How to bring to the canonical form and solve the below PDE? $$u_{tt}-u_{xx} = x^2 -t^2$$ I recognize that it is a hyperbolic PDE, as the $b^2-4ac=(-4(1)(-1))=4 > 0$. I don't know how to proceed further to get the canonical form. I know how to deal with something like $u_{tt}-u_{xx} = 0$. With $\ RHS =0 \ $ I would use the equation for characteristic  $R (\frac{\partial^2 dy}{\partial dx})-2S (\frac{\partial^2 dy}{\partial dx})+T=0$ , define the $\xi$ and $\eta$ in terms of $x$ and $y$, calculate the first and second partial derivatives and substitute them into the initial equation. Here the function on the right hand side $x^2 -t^2$ complicates matter. How the RHS=X^2-t^2 changes the standard wave equation $u_{tt}−u_{xx}=0$ in terms of interpretation?",,"['ordinary-differential-equations', 'multivariable-calculus', 'partial-differential-equations']"
19,Bifurcation diagram and bifurcation value,Bifurcation diagram and bifurcation value,,"Determine the bifurcation values of $\dot{x} = x(x-r^2)$, and sketch the bifurcation diagram. My attempt: First, we see that if $f(x_0, r_0) = Df(x_0, r_0) = 0$, then $x_0$ is a non-hyperbolic critical point and $r_0$ is a bifurcation value. We see that this only occurs when $(x_0, r_0) = (0,0)$, so this is the only bifurcation value. Now, for each $r_0\neq 0$, the solution would increase without bound for $x_0 > \sqrt{r_0}$ and $x_0 < 0$, and decrease for $0 < x_0< \sqrt{r_0}$. I don't know how to demonstrate these information on the bifurcation diagram though:( Can someone please help?","Determine the bifurcation values of $\dot{x} = x(x-r^2)$, and sketch the bifurcation diagram. My attempt: First, we see that if $f(x_0, r_0) = Df(x_0, r_0) = 0$, then $x_0$ is a non-hyperbolic critical point and $r_0$ is a bifurcation value. We see that this only occurs when $(x_0, r_0) = (0,0)$, so this is the only bifurcation value. Now, for each $r_0\neq 0$, the solution would increase without bound for $x_0 > \sqrt{r_0}$ and $x_0 < 0$, and decrease for $0 < x_0< \sqrt{r_0}$. I don't know how to demonstrate these information on the bifurcation diagram though:( Can someone please help?",,"['ordinary-differential-equations', 'bifurcation']"
20,Finding a particular solution for $\frac{\mathrm{d}^2R}{\mathrm{d}r^2}+\frac1r\frac{\mathrm{d}R}{\mathrm{d}r}+\alpha^2R=J_0(\alpha r)$,Finding a particular solution for,\frac{\mathrm{d}^2R}{\mathrm{d}r^2}+\frac1r\frac{\mathrm{d}R}{\mathrm{d}r}+\alpha^2R=J_0(\alpha r),"Motivation I have the following non-homogeneous Bessel differential equation $$\frac{\mathrm{d}^2R}{\mathrm{d}r^2}+\frac1r\frac{\mathrm{d}R}{\mathrm{d}r}+\alpha^2R=J_0(\alpha r)$$ I want to find the general solution for this ODE. I know that the general solution can be written as $$R=R_h+R_p$$ where $R_h$ is the basis for the homogeneous ODE and is known to be $$R_h=C_1 \, J_0(\alpha r) + C_2 \, Y_0(\alpha r).$$ Next, for finding the particular solution $R_p$ one can use the method of variation of parameters to obtain a particular solution using the homogeneous ones. However, this will lead to some messy solution with hard integrals ! As the MAPLE or WOLFRAM both use this technique their result is not valuable for me. I know that $$R_p=\frac1{2 \alpha^2}\left[\alpha rJ_1(\alpha r)\right] \tag{*}$$ and it can be verified by putting it into the ODE. In fact, I saw this in some published paper. Question Is there an elegant method to obtain such a particular solution mentioned in $(*)$ ?","Motivation I have the following non-homogeneous Bessel differential equation I want to find the general solution for this ODE. I know that the general solution can be written as where is the basis for the homogeneous ODE and is known to be Next, for finding the particular solution one can use the method of variation of parameters to obtain a particular solution using the homogeneous ones. However, this will lead to some messy solution with hard integrals ! As the MAPLE or WOLFRAM both use this technique their result is not valuable for me. I know that and it can be verified by putting it into the ODE. In fact, I saw this in some published paper. Question Is there an elegant method to obtain such a particular solution mentioned in ?","\frac{\mathrm{d}^2R}{\mathrm{d}r^2}+\frac1r\frac{\mathrm{d}R}{\mathrm{d}r}+\alpha^2R=J_0(\alpha r) R=R_h+R_p R_h R_h=C_1 \, J_0(\alpha r) + C_2 \, Y_0(\alpha r). R_p R_p=\frac1{2 \alpha^2}\left[\alpha rJ_1(\alpha r)\right] \tag{*} (*)","['ordinary-differential-equations', 'special-functions', 'bessel-functions']"
21,Topics to master (be literate at) before differential equations?,Topics to master (be literate at) before differential equations?,,"Good evening, I'm really enthusiastic about learning differential equations because it was said that D.E. is the most important tool of mathematics "" can be used for modelling real-world physical occurrences "". I've taken courses in Differential and Integral Calculus (including numerical techniques of evaluation), and had self-studied Multivariable Calculus (only from partial differentiation, multiple integration, Vector Integration Stoke's, Green's), Series (taylor/maclaurin, covergence, divergence), and a little bit of linear algebra (before vector spaces). I have a book on Differential Equations but it seems that I can't understand the way it proves and explains the rules and theorems (I can solve separable first order and the one with dy/dx + y = c, but I'm stuck after it, specifically on the uniqueness and existence of solutions). So for the people who are adept at D.E., what mathematical techniques/knowledge should I study to further understand Differential Equations? (My plan now is to finish my linear algebra book) PS. I am an engineering major :), though my math courses are focused on the application and problem solving, I keep it to a point that I know its basis and that I can derive it.","Good evening, I'm really enthusiastic about learning differential equations because it was said that D.E. is the most important tool of mathematics "" can be used for modelling real-world physical occurrences "". I've taken courses in Differential and Integral Calculus (including numerical techniques of evaluation), and had self-studied Multivariable Calculus (only from partial differentiation, multiple integration, Vector Integration Stoke's, Green's), Series (taylor/maclaurin, covergence, divergence), and a little bit of linear algebra (before vector spaces). I have a book on Differential Equations but it seems that I can't understand the way it proves and explains the rules and theorems (I can solve separable first order and the one with dy/dx + y = c, but I'm stuck after it, specifically on the uniqueness and existence of solutions). So for the people who are adept at D.E., what mathematical techniques/knowledge should I study to further understand Differential Equations? (My plan now is to finish my linear algebra book) PS. I am an engineering major :), though my math courses are focused on the application and problem solving, I keep it to a point that I know its basis and that I can derive it.",,['ordinary-differential-equations']
22,ODE $2yy'' - 3(y')^2 = 4 y^2$,ODE,2yy'' - 3(y')^2 = 4 y^2,I'm trying to solve the equation by using these substitutions (how it was suggested in my textbook): $$ y = e^{z(x)} \implies y' = z'y \implies y'' = y((z')^2 + z'') $$  The result is: $$ 2y^2((z')^2 + z'') - 3y^2(z')^2 = 4y^2 \implies 2z'' - (z')^2 =  4$$ Here I'm stuck and I can't figure out a way to simplify it or reduce the order of the equation. What should I do then?,I'm trying to solve the equation by using these substitutions (how it was suggested in my textbook): $$ y = e^{z(x)} \implies y' = z'y \implies y'' = y((z')^2 + z'') $$  The result is: $$ 2y^2((z')^2 + z'') - 3y^2(z')^2 = 4y^2 \implies 2z'' - (z')^2 =  4$$ Here I'm stuck and I can't figure out a way to simplify it or reduce the order of the equation. What should I do then?,,['ordinary-differential-equations']
23,Solving a differential equation by using Laplace transform,Solving a differential equation by using Laplace transform,,"I need to solve this equations by using laplace-transform. I tried to solve it but when I reach to the point that it's needed to use partial fraction expansion in order to transform the laplace inverse, I get $\Delta<0$ and I don't know what to do next? because I always got $\Delta\ge0$, can you help me dear friends?  $$y''+9y=\sin2t$$ $$y(0)=1$$ $$y'(0)=1$$ This is what I tried to do:  $$L[y]=F(s);L[y']=S.F(s)-f(0);L[y'']=s^2.F(s) -s.f(0)-f'(0)  \Rightarrow  L[y'' + 9y] = L[\sin^2] \Rightarrow s^2.F(s)-s-1+9.F(s)={s^2\over s^2+4} \Rightarrow {2+(s^2+4)(s^2+1)\over (s^2+4)(s^2+9)}  \Rightarrow $$   $${A\over s^2+4}+{B\over s^2+9}$$  that's it,here, I can't find $A,B$.","I need to solve this equations by using laplace-transform. I tried to solve it but when I reach to the point that it's needed to use partial fraction expansion in order to transform the laplace inverse, I get $\Delta<0$ and I don't know what to do next? because I always got $\Delta\ge0$, can you help me dear friends?  $$y''+9y=\sin2t$$ $$y(0)=1$$ $$y'(0)=1$$ This is what I tried to do:  $$L[y]=F(s);L[y']=S.F(s)-f(0);L[y'']=s^2.F(s) -s.f(0)-f'(0)  \Rightarrow  L[y'' + 9y] = L[\sin^2] \Rightarrow s^2.F(s)-s-1+9.F(s)={s^2\over s^2+4} \Rightarrow {2+(s^2+4)(s^2+1)\over (s^2+4)(s^2+9)}  \Rightarrow $$   $${A\over s^2+4}+{B\over s^2+9}$$  that's it,here, I can't find $A,B$.",,"['ordinary-differential-equations', 'laplace-transform']"
24,How do I solve for $y$ in this differential equation?,How do I solve for  in this differential equation?,y,$y'(t)= 3ty$ where $y(0)=-1$. I have attempted to solve for $y$ by; $$\frac{1}{y}\space dy=3t\space dt$$ $$\int\frac{1}{y}\space dy=\int3t\space dt$$ $$\implies \ln(y)=\frac{3t^{2}}{2} + c$$ $$\implies y=e^{\frac{3t^{2}}{2}} \cdot e^{c}$$ $$\implies-1=e^{c}$$ Hence I have gone wrong somewhere but I am not sure what it is. Can someone explain please?,$y'(t)= 3ty$ where $y(0)=-1$. I have attempted to solve for $y$ by; $$\frac{1}{y}\space dy=3t\space dt$$ $$\int\frac{1}{y}\space dy=\int3t\space dt$$ $$\implies \ln(y)=\frac{3t^{2}}{2} + c$$ $$\implies y=e^{\frac{3t^{2}}{2}} \cdot e^{c}$$ $$\implies-1=e^{c}$$ Hence I have gone wrong somewhere but I am not sure what it is. Can someone explain please?,,['ordinary-differential-equations']
25,Finding the characteristic timescale of a first-order nonlinear ODE,Finding the characteristic timescale of a first-order nonlinear ODE,,"I know that to find the timescale of a first order linear equation $$\frac{dX(t)}{dt} + aX(t) = b$$ you just take the inverse of the integrating factor, so $$t_x = \frac{1}{a}$$ Henning and joriki provide mathematical definitions of the characteristic timescale . I have a system of coupled ODEs. I want to find the timescale for the generation of one of the species, $X$, whose derivative is of the form $$\frac{dX(t)}{dt} + aX(t) = bY(t)Z(t)$$ Firstly, am I right in saying this is a first order non-linear ODE? Secondly, does anyone have any pointers for how to find its timescale? Many thanks.","I know that to find the timescale of a first order linear equation $$\frac{dX(t)}{dt} + aX(t) = b$$ you just take the inverse of the integrating factor, so $$t_x = \frac{1}{a}$$ Henning and joriki provide mathematical definitions of the characteristic timescale . I have a system of coupled ODEs. I want to find the timescale for the generation of one of the species, $X$, whose derivative is of the form $$\frac{dX(t)}{dt} + aX(t) = bY(t)Z(t)$$ Firstly, am I right in saying this is a first order non-linear ODE? Secondly, does anyone have any pointers for how to find its timescale? Many thanks.",,"['ordinary-differential-equations', 'nonlinear-system']"
26,Question about this ODE? $\frac{dy}{dx} = \frac{2x-y}{x+2y}$,Question about this ODE?,\frac{dy}{dx} = \frac{2x-y}{x+2y},"Am I being dumb, or is this question actually hard?  I made the substitution $u=y/x \implies y = ux$, so then I get: $x \cdot \dfrac{du}{dx} + u = \dfrac{2x-ux}{x+2ux} \implies x \cdot \dfrac{du}{dx} + u = \dfrac{2-u}{1+2u} $.  Then I simplified this to $x \cdot \dfrac{du}{dx} + \dfrac{2u^2+2u-2}{1+2u}=0$... Now I have no idea how to solve this, back to where I started.  Was my initial substitution wrong? It's the one my teacher recommended so I thought it would work out a little better... Thanks!","Am I being dumb, or is this question actually hard?  I made the substitution $u=y/x \implies y = ux$, so then I get: $x \cdot \dfrac{du}{dx} + u = \dfrac{2x-ux}{x+2ux} \implies x \cdot \dfrac{du}{dx} + u = \dfrac{2-u}{1+2u} $.  Then I simplified this to $x \cdot \dfrac{du}{dx} + \dfrac{2u^2+2u-2}{1+2u}=0$... Now I have no idea how to solve this, back to where I started.  Was my initial substitution wrong? It's the one my teacher recommended so I thought it would work out a little better... Thanks!",,['ordinary-differential-equations']
27,How to bound error when approximating ODE,How to bound error when approximating ODE,,"I have a question regarding how to bound the error, if one changes the ""right hand side"" of an ODE. For example, the equation of a simple pendulum in polar coordinates is something like $$\ddot{\theta}= k\sin\theta$$ The common adjustment is to let $$\sin\theta \sim\theta$$ for small enough $\theta$. I'll try to state my question in general terms now. Take the IVP: $$\left\{\begin{align}\dot x  = f(x) \\ x(0) = x_0\end{align}\right.$$ with $$f:B_{r}(0)\subset\Bbb R^n\to\Bbb R^n$$ Now, suppose that $\tilde f:B_{\tilde r}(0)\to\Bbb R^n$ is such that $\|f(x)-\tilde f(x)\| \lt \varepsilon$ for $x\in B_{\tilde r}(0)$ if $\tilde r < \delta \leq r$. Under which conditions, and then how, can we place a bound on $$\|\varphi(t) - \tilde\varphi(t)\|$$ where $\varphi$ solves the original IVP, and $\tilde\varphi$ solves the new IVP formed by replacing $f$ with $\tilde f$? Obviously, the conditions must at least guarantee existence and uniqueness for the question to make sense, but what else (if anything)? I'm not sure if this has to do with perturbation theory proper, so let me know if it's wrongly tagged.","I have a question regarding how to bound the error, if one changes the ""right hand side"" of an ODE. For example, the equation of a simple pendulum in polar coordinates is something like $$\ddot{\theta}= k\sin\theta$$ The common adjustment is to let $$\sin\theta \sim\theta$$ for small enough $\theta$. I'll try to state my question in general terms now. Take the IVP: $$\left\{\begin{align}\dot x  = f(x) \\ x(0) = x_0\end{align}\right.$$ with $$f:B_{r}(0)\subset\Bbb R^n\to\Bbb R^n$$ Now, suppose that $\tilde f:B_{\tilde r}(0)\to\Bbb R^n$ is such that $\|f(x)-\tilde f(x)\| \lt \varepsilon$ for $x\in B_{\tilde r}(0)$ if $\tilde r < \delta \leq r$. Under which conditions, and then how, can we place a bound on $$\|\varphi(t) - \tilde\varphi(t)\|$$ where $\varphi$ solves the original IVP, and $\tilde\varphi$ solves the new IVP formed by replacing $f$ with $\tilde f$? Obviously, the conditions must at least guarantee existence and uniqueness for the question to make sense, but what else (if anything)? I'm not sure if this has to do with perturbation theory proper, so let me know if it's wrongly tagged.",,"['ordinary-differential-equations', 'continuity', 'perturbation-theory']"
28,How to solve this recurrence Relation - Varying Coefficient,How to solve this recurrence Relation - Varying Coefficient,,"Sir,I have two questions related to this recurrence relation. It has been messing with me for long. Because of this I couldn't proceed my work for some time .This contains a polynomial term n+2 in sequence. I tried almost all known methods to me. I tried generating functions,general methods all etc . So it makes difficult for solving it. Would you provide the solution for it $ R_{n}=\frac{1}{n} \{C_1 R_{n-1} +C_2 R_{n-2}\}, R_0 = A_0 ,R_1 = A_1  $ Here $C_1$  and $C_2 $ are constants Question 1 ::a) What is the solution of this in terms of n for $R_n $ b) What is the summation($ \sum_{n=0}^{n= \infty}  R_n$) of it, in-terms of n if possible provided $R_0,R_1,C_1,C_2, R_n  $ are numerical constants of singular matrices. Means normal case,numerical constants Question 2 ::a) What is the solution of this in terms of n for $R_n$ and What is the summation($ \sum_{n=0}^{n= \infty}  R_n$) of it, in-terms of n if possible provided $ R_0,R_1,C_1,C_2 $ are   matrices of order $3\times 3 $?","Sir,I have two questions related to this recurrence relation. It has been messing with me for long. Because of this I couldn't proceed my work for some time .This contains a polynomial term n+2 in sequence. I tried almost all known methods to me. I tried generating functions,general methods all etc . So it makes difficult for solving it. Would you provide the solution for it $ R_{n}=\frac{1}{n} \{C_1 R_{n-1} +C_2 R_{n-2}\}, R_0 = A_0 ,R_1 = A_1  $ Here $C_1$  and $C_2 $ are constants Question 1 ::a) What is the solution of this in terms of n for $R_n $ b) What is the summation($ \sum_{n=0}^{n= \infty}  R_n$) of it, in-terms of n if possible provided $R_0,R_1,C_1,C_2, R_n  $ are numerical constants of singular matrices. Means normal case,numerical constants Question 2 ::a) What is the solution of this in terms of n for $R_n$ and What is the summation($ \sum_{n=0}^{n= \infty}  R_n$) of it, in-terms of n if possible provided $ R_0,R_1,C_1,C_2 $ are   matrices of order $3\times 3 $?",,"['ordinary-differential-equations', 'algorithms', 'generating-functions', 'recursive-algorithms', 'recurrence-relations']"
29,"Inverse Laplace Transform,","Inverse Laplace Transform,",,"I have been stuck on this problem for quite a bit, have tried to look at similar answers on website but no help... The original questions is, Solve the IVP $\ y''+y=\sin(t);y(0)=1;y'(0)=0$ I applied Laplace to both sides and ended up with this $\ Y(s) = \frac {s}{s^2+1} - \frac {1}{s^2+1} + \frac {1}{(s^2+1)^2}$ where $\ L(y(t)) = Y(s) $ so $\ y(t) = \cos(t) - \sin(t) +L(\frac {1}{(s^2+1)^2}) $ the first to fractions on the RHS of the equation are simple to solve, what I am having trouble with is the last fraction. I have tried partial fractions, but got nothing out of it. I am not sure where to go with this? I tried using this solution, but wasn't sure how to apply it to my problem; Finding the inverse Laplace transform of $\frac{s^2-4s-4}{s^4+8s^2+16}$","I have been stuck on this problem for quite a bit, have tried to look at similar answers on website but no help... The original questions is, Solve the IVP $\ y''+y=\sin(t);y(0)=1;y'(0)=0$ I applied Laplace to both sides and ended up with this $\ Y(s) = \frac {s}{s^2+1} - \frac {1}{s^2+1} + \frac {1}{(s^2+1)^2}$ where $\ L(y(t)) = Y(s) $ so $\ y(t) = \cos(t) - \sin(t) +L(\frac {1}{(s^2+1)^2}) $ the first to fractions on the RHS of the equation are simple to solve, what I am having trouble with is the last fraction. I have tried partial fractions, but got nothing out of it. I am not sure where to go with this? I tried using this solution, but wasn't sure how to apply it to my problem; Finding the inverse Laplace transform of $\frac{s^2-4s-4}{s^4+8s^2+16}$",,"['ordinary-differential-equations', 'laplace-transform']"
30,Use Green's function to find solutions for the boundary value problem,Use Green's function to find solutions for the boundary value problem,,"Find a solution using Green's functions $$y''+y=t;  y(0)=0, y(1)=1$$ So far I have $$x(t)=c_1 \cos(t)+c_2 \sin(t)$$ so $$y_1=\cos(t), y_2=\sin(t)$$ and $$W(y_1,y_2)=-1$$ When I put that in the integral for Green's function I get $$(x)t=\int^t_0 (s\cos(t) \sin(s))\:\mathrm{d}s - \sin(t) \int^1_t (s\cos(s))\:\mathrm{d}s$$  so I end up getting $$-\cos(t)\sin(t)+t\cos^2 (t)-\sin(t)\cos(1)-\sin(t)\sin(1)+\sin(t)\cos(t)+t\sin^2 (t)$$ I think I did something wrong at the beginning, but I am not sure what. I do not think I should be getting $\sin(1)$ or $\cos(1)$ in my answer.","Find a solution using Green's functions $$y''+y=t;  y(0)=0, y(1)=1$$ So far I have $$x(t)=c_1 \cos(t)+c_2 \sin(t)$$ so $$y_1=\cos(t), y_2=\sin(t)$$ and $$W(y_1,y_2)=-1$$ When I put that in the integral for Green's function I get $$(x)t=\int^t_0 (s\cos(t) \sin(s))\:\mathrm{d}s - \sin(t) \int^1_t (s\cos(s))\:\mathrm{d}s$$  so I end up getting $$-\cos(t)\sin(t)+t\cos^2 (t)-\sin(t)\cos(1)-\sin(t)\sin(1)+\sin(t)\cos(t)+t\sin^2 (t)$$ I think I did something wrong at the beginning, but I am not sure what. I do not think I should be getting $\sin(1)$ or $\cos(1)$ in my answer.",,"['ordinary-differential-equations', 'boundary-value-problem']"
31,A first-order non-linear ordinary differential equation containing various squares,A first-order non-linear ordinary differential equation containing various squares,,"The Equation: Find all differentiable functions $f: I \rightarrow \mathbb{R}$ satisfies: $$\big(\,f(x)-x\,f'(x)\big)^2 = \big(\,f'(x)\big)^2 + 1 \; \; \; \; \; \text{for all}\,\,\, x \in I,$$ where $I$ is an open interval. This is not an assignment. And, thanks in advance!","The Equation: Find all differentiable functions $f: I \rightarrow \mathbb{R}$ satisfies: $$\big(\,f(x)-x\,f'(x)\big)^2 = \big(\,f'(x)\big)^2 + 1 \; \; \; \; \; \text{for all}\,\,\, x \in I,$$ where $I$ is an open interval. This is not an assignment. And, thanks in advance!",,"['real-analysis', 'analysis', 'ordinary-differential-equations']"
32,how to solve $x^3y′′−xy′+y=0$,how to solve,x^3y′′−xy′+y=0,"I tried to use Frobenius method to solve $$ x^{3}{\rm y}′′\left(x\right) − x\,{\rm y}′\left(x\right) + {\rm y}\left(x\right)=0, $$ but it does not work. And the solution most be $y_{1} = ax + b$. I tried also using first change of variables $\left(~s = 1/x~\right)$, and then applied the power series method but I did not get the $y_{1} = ax + b$ solution. Does anyone know how this equation can be solved ?.","I tried to use Frobenius method to solve $$ x^{3}{\rm y}′′\left(x\right) − x\,{\rm y}′\left(x\right) + {\rm y}\left(x\right)=0, $$ but it does not work. And the solution most be $y_{1} = ax + b$. I tried also using first change of variables $\left(~s = 1/x~\right)$, and then applied the power series method but I did not get the $y_{1} = ax + b$ solution. Does anyone know how this equation can be solved ?.",,['ordinary-differential-equations']
33,"How to find the orthogonal trajectories of the family of all the circles through the points $(1,1)$ and $(-1,-1)$?",How to find the orthogonal trajectories of the family of all the circles through the points  and ?,"(1,1) (-1,-1)","I'm trying to find the orthogonal trajectories of the family of circles through the points $(1,1)$ and $(-1, -1)$. Now such a family can be given by an equation of the form $$ x^2 + y^2 + 2g(x-y) - 2 = 0, $$ where $g$ is a parameter. Now upon differentiation with respect to x, we obtain $$ 2x + 2y y^\prime + 2g (1 - y^\prime ) = 0, $$ where $y^\prime$ denotes the derivative of $y$ with respect to $x$. Upon dividing out by $2$, we arrive at $$ x + y y^\prime + g(1 - y^\prime ) = 0, $$ from which we get $$ g = \frac{x + y y^\prime}{y^\prime - 1}. $$ Putting this value of $g$ into the equation of the family of circles, we get $$ x^2 + y^2 +2 \frac{x + y y^\prime}{y^\prime - 1} ( x - y ) - 2 = 0, $$ so $$ (x^2 + y^2 -2 ) (y^\prime - 1 ) + 2 (x + y y^\prime ) (x - y)  = 0$$ or $$ (x^2 + y^2 - 2 + 2xy - 2y^2 ) y^\prime + (2x^2 - 2xy - x^2 - y^2 + 2) = 0  $$ or $$ ( x^2 + 2xy - y^2 - 2) y^\prime +  (x^2 - 2xy  - y^2 + 2) = 0, $$  from which we get $$ y^\prime = - \frac{ x^2 - 2xy  - y^2 + 2}{ x^2 + 2xy - y^2 - 2}. $$ Now for the orthogonal trajectories, we get $$ y^\prime = \frac{x^2 + 2xy - y^2 - 2}{x^2 - 2xy  - y^2 + 2}. $$ How to solve this differential equation?","I'm trying to find the orthogonal trajectories of the family of circles through the points $(1,1)$ and $(-1, -1)$. Now such a family can be given by an equation of the form $$ x^2 + y^2 + 2g(x-y) - 2 = 0, $$ where $g$ is a parameter. Now upon differentiation with respect to x, we obtain $$ 2x + 2y y^\prime + 2g (1 - y^\prime ) = 0, $$ where $y^\prime$ denotes the derivative of $y$ with respect to $x$. Upon dividing out by $2$, we arrive at $$ x + y y^\prime + g(1 - y^\prime ) = 0, $$ from which we get $$ g = \frac{x + y y^\prime}{y^\prime - 1}. $$ Putting this value of $g$ into the equation of the family of circles, we get $$ x^2 + y^2 +2 \frac{x + y y^\prime}{y^\prime - 1} ( x - y ) - 2 = 0, $$ so $$ (x^2 + y^2 -2 ) (y^\prime - 1 ) + 2 (x + y y^\prime ) (x - y)  = 0$$ or $$ (x^2 + y^2 - 2 + 2xy - 2y^2 ) y^\prime + (2x^2 - 2xy - x^2 - y^2 + 2) = 0  $$ or $$ ( x^2 + 2xy - y^2 - 2) y^\prime +  (x^2 - 2xy  - y^2 + 2) = 0, $$  from which we get $$ y^\prime = - \frac{ x^2 - 2xy  - y^2 + 2}{ x^2 + 2xy - y^2 - 2}. $$ Now for the orthogonal trajectories, we get $$ y^\prime = \frac{x^2 + 2xy - y^2 - 2}{x^2 - 2xy  - y^2 + 2}. $$ How to solve this differential equation?",,"['calculus', 'ordinary-differential-equations', 'analytic-geometry']"
34,How to solve the ODE: $\frac{d^2y}{dx^2}=y\big(\frac{dy}{dx}\big)^2$,How to solve the ODE:,\frac{d^2y}{dx^2}=y\big(\frac{dy}{dx}\big)^2,I am trying to solve the non-linear ODE  $ \frac{d^2y}{dx^2}=y\big(\frac{dy}{dx}\big)^2$ and I would appreciate some suggestions on how to approach this equation. This is actually part of a more complex PDE which I managed to separate but I am puzzled when it comes to solve the above non-linear problem. Thank you.,I am trying to solve the non-linear ODE  $ \frac{d^2y}{dx^2}=y\big(\frac{dy}{dx}\big)^2$ and I would appreciate some suggestions on how to approach this equation. This is actually part of a more complex PDE which I managed to separate but I am puzzled when it comes to solve the above non-linear problem. Thank you.,,['ordinary-differential-equations']
35,How can I prove that the DE $y'=y^\alpha$ has infinitely many solutions?,How can I prove that the DE  has infinitely many solutions?,y'=y^\alpha,"I need to show that the DE $y'=y^{\alpha}$, where $\alpha$ is a constant with $0<\alpha<1$, has infinitely many solutions passing through the point $(0,0)$. Also I need four of such solutions. Thank you for your help! Klara","I need to show that the DE $y'=y^{\alpha}$, where $\alpha$ is a constant with $0<\alpha<1$, has infinitely many solutions passing through the point $(0,0)$. Also I need four of such solutions. Thank you for your help! Klara",,"['real-analysis', 'ordinary-differential-equations']"
36,Green's function for periodic boundary condition,Green's function for periodic boundary condition,,"How to find a explicit Green's function for the problem $$-u''(x)+q(x)u(x)=g(x),$$ $x\in (0,1)$ with conditions $u(0)=u(1)$ and $u'(0)=u'(1)$? Here $q$ has whatever property you want except being constant. All I found about it is for separated end-point conditions saying ""Yes, there is a green's function"".","How to find a explicit Green's function for the problem $$-u''(x)+q(x)u(x)=g(x),$$ $x\in (0,1)$ with conditions $u(0)=u(1)$ and $u'(0)=u'(1)$? Here $q$ has whatever property you want except being constant. All I found about it is for separated end-point conditions saying ""Yes, there is a green's function"".",,['ordinary-differential-equations']
37,ODE existence and uniqueness,ODE existence and uniqueness,,"For $u''(x) + u'(x) = f(x)$, and $u'(0) = u(0) = 1/2\bigl(u'(l) + u(l)\bigr)$, with $f$ a given function, is the solution unique, and why/why not? Also, does a solution necessarily exist or is there a condition that $f$ must satisfy for existence? I'm thinking of an integrating factor but that didn't quite get me anywhere. Thanks","For $u''(x) + u'(x) = f(x)$, and $u'(0) = u(0) = 1/2\bigl(u'(l) + u(l)\bigr)$, with $f$ a given function, is the solution unique, and why/why not? Also, does a solution necessarily exist or is there a condition that $f$ must satisfy for existence? I'm thinking of an integrating factor but that didn't quite get me anywhere. Thanks",,['ordinary-differential-equations']
38,Quaternion Differentiation,Quaternion Differentiation,,"I have an application that tracks an image and estimates its position and orientation. The orientation is given by a quaternion, and it is modified by an angular velocity every frame. To predict the orientation I calculate the differential quaternion basing on the angular rate $\vec \omega$ and the previous quaternion $\vec q$. I found these equations. $$q_x=\frac{1}{2}(w_x q_w+w_y q_z-w_z q_y) $$ $$q_y=\frac{1}{2}(w_y q_w+w_z q_x-w_x q_z) $$ $$q_z=\frac{1}{2}(w_z q_w+w_x q_y-w_y1 q_x)$$  $$q_w=-\frac{1}{2}(w_x q_x+w_y q_y-w_z q_z)$$ Is this approach correct? Should I use $\vec \omega$ or do I need to take into account the time interval between frames $\vec \omega\Delta t $? After this, the predicted quaternion $\hat q$ would be the sum of the previous one and the differentiation, wouldn't it?","I have an application that tracks an image and estimates its position and orientation. The orientation is given by a quaternion, and it is modified by an angular velocity every frame. To predict the orientation I calculate the differential quaternion basing on the angular rate $\vec \omega$ and the previous quaternion $\vec q$. I found these equations. $$q_x=\frac{1}{2}(w_x q_w+w_y q_z-w_z q_y) $$ $$q_y=\frac{1}{2}(w_y q_w+w_z q_x-w_x q_z) $$ $$q_z=\frac{1}{2}(w_z q_w+w_x q_y-w_y1 q_x)$$  $$q_w=-\frac{1}{2}(w_x q_x+w_y q_y-w_z q_z)$$ Is this approach correct? Should I use $\vec \omega$ or do I need to take into account the time interval between frames $\vec \omega\Delta t $? After this, the predicted quaternion $\hat q$ would be the sum of the previous one and the differentiation, wouldn't it?",,"['ordinary-differential-equations', 'quaternions']"
39,Third order Cauchy-Euler differential equation,Third order Cauchy-Euler differential equation,,"I need to solve this: $$x^3y''' - x^2y'' + 2xy' - 2y = x^3$$ I know that first I have to solve: $$E = x^3y''' - x^2y'' + 2xy' - 2y = 0$$ I choose $y = x^r$. By feeding that to $E$ it will lead me to the following characteristic equation: $$(r-2)(r-1)^2=0$$ Now if all roots would be distinct the solution would be simple, but with repeating roots how do I approach this equation? I know that the correct answer must be: $$y(x) = c_3 x^2+c_1 x+c_2 x ln(x)+x^3/4$$ I don't know how to get to the $c_2xln(x)$ part. I know that I have to use Wronskian matrix to get the $x^3/4$ part.","I need to solve this: $$x^3y''' - x^2y'' + 2xy' - 2y = x^3$$ I know that first I have to solve: $$E = x^3y''' - x^2y'' + 2xy' - 2y = 0$$ I choose $y = x^r$. By feeding that to $E$ it will lead me to the following characteristic equation: $$(r-2)(r-1)^2=0$$ Now if all roots would be distinct the solution would be simple, but with repeating roots how do I approach this equation? I know that the correct answer must be: $$y(x) = c_3 x^2+c_1 x+c_2 x ln(x)+x^3/4$$ I don't know how to get to the $c_2xln(x)$ part. I know that I have to use Wronskian matrix to get the $x^3/4$ part.",,['ordinary-differential-equations']
40,Stochastic predator-prey,Stochastic predator-prey,,"My system is a simple $P$ vs $I$ foxes- vs rabbits model given by: $$ \begin{cases} \frac{\mathrm{d}I}{\mathrm{d}t}=& \alpha_I+\lambda_IP- \gamma_II -\delta_IPI;\\ \frac{\mathrm{d}P}{\mathrm{d}t}=&\alpha_P+\theta_PP-\delta_PPI \end{cases} $$ with a parameter set:  $$ \begin{cases} \theta_P &=0.15 \\  \delta_P&=0.01 \end{cases}\quad\text{ and }\quad  \begin{cases} \alpha_I&=0.4  \\ \gamma_I&=0.1 \\ \delta_I&=0.05 \\ \lambda_I&=0.05 \end{cases} $$ but a condition on the initial introduction of $D$ rabbits ($P$) over a specific timeframe $T$. $\alpha_P = \begin{cases} &\dfrac{D}{T} &\mbox{if } t\leq T \\ \\ &0 & \mbox{otherwise.} \end{cases}$ METHOD I'm using is: over any small timestep $\delta t<<1$, one of the following can occur: I=I+1, with probability $\alpha_I+P\lambda_I$ I=I-1, with probability $I\gamma_I-PI\delta_I$ P=P+1, with probability $ P\theta_P$ P=P-1, with probability $ PI\delta_P$ QUESTION: But is this true when $t<T$? EDIT 18/04/2013: Consider that P is actually Pathogens, and I is actually immunity cells in the human body. A literature search finds: (where Innoculation time is $t<T$) Pujol 2009 -The Effect of Ongoing Exposure Dynamics in Dose  Response Relationships (free access)","My system is a simple $P$ vs $I$ foxes- vs rabbits model given by: $$ \begin{cases} \frac{\mathrm{d}I}{\mathrm{d}t}=& \alpha_I+\lambda_IP- \gamma_II -\delta_IPI;\\ \frac{\mathrm{d}P}{\mathrm{d}t}=&\alpha_P+\theta_PP-\delta_PPI \end{cases} $$ with a parameter set:  $$ \begin{cases} \theta_P &=0.15 \\  \delta_P&=0.01 \end{cases}\quad\text{ and }\quad  \begin{cases} \alpha_I&=0.4  \\ \gamma_I&=0.1 \\ \delta_I&=0.05 \\ \lambda_I&=0.05 \end{cases} $$ but a condition on the initial introduction of $D$ rabbits ($P$) over a specific timeframe $T$. $\alpha_P = \begin{cases} &\dfrac{D}{T} &\mbox{if } t\leq T \\ \\ &0 & \mbox{otherwise.} \end{cases}$ METHOD I'm using is: over any small timestep $\delta t<<1$, one of the following can occur: I=I+1, with probability $\alpha_I+P\lambda_I$ I=I-1, with probability $I\gamma_I-PI\delta_I$ P=P+1, with probability $ P\theta_P$ P=P-1, with probability $ PI\delta_P$ QUESTION: But is this true when $t<T$? EDIT 18/04/2013: Consider that P is actually Pathogens, and I is actually immunity cells in the human body. A literature search finds: (where Innoculation time is $t<T$) Pujol 2009 -The Effect of Ongoing Exposure Dynamics in Dose  Response Relationships (free access)",,"['ordinary-differential-equations', 'stochastic-processes', 'mathematical-modeling']"
41,Examples of parameter dependent ODEs,Examples of parameter dependent ODEs,,"I would like to have some examples of simple parameter dependent ODEs. I would like the solutions to have some physical meaning. I'll give one example so it clear what I am after: Example 1: The Lane-Emden equation from Astrophysics can be rewritten so that the first root of the solution becomes an unknown. Let $v$ be the unknown first root then the parameter-dependent ODE becomes, $ u'' +(2/x)u' + v^2u =0, u(0)=0, u'(0)=1, u(1)=0. $ The Lane-Emden equation model stellar formation and the parameter $v$ determines the polytropic region. I would like to have perhaps two/three interesting examples. Thanks in advance.","I would like to have some examples of simple parameter dependent ODEs. I would like the solutions to have some physical meaning. I'll give one example so it clear what I am after: Example 1: The Lane-Emden equation from Astrophysics can be rewritten so that the first root of the solution becomes an unknown. Let $v$ be the unknown first root then the parameter-dependent ODE becomes, $ u'' +(2/x)u' + v^2u =0, u(0)=0, u'(0)=1, u(1)=0. $ The Lane-Emden equation model stellar formation and the parameter $v$ determines the polytropic region. I would like to have perhaps two/three interesting examples. Thanks in advance.",,"['ordinary-differential-equations', 'examples-counterexamples']"
42,Solving differential equations with signum,Solving differential equations with signum,,Is there a general solution to: $A x''+ B x' + C \mathop{\rm sgn}(x')+ D x=0$  where $\mathop{\rm sgn}(x')$ is the sign of $x'$,Is there a general solution to: $A x''+ B x' + C \mathop{\rm sgn}(x')+ D x=0$  where $\mathop{\rm sgn}(x')$ is the sign of $x'$,,['ordinary-differential-equations']
43,Solve the ODE $y(x+y)dx+(1+xy)dy=0$,Solve the ODE,y(x+y)dx+(1+xy)dy=0,"I need to solve the following ODE: $$y(x+y)dx+(1+xy)dy=0$$ I can see that this is not an exact equation, and when I tried to multiply it by the integration factor $\mu(x)$ or $\mu(y)$ I got nothing. So I started to ""play"" with it. $$y(x+y)dx+(1+xy)dy=0$$ $$y(x+y)+(1+xy)y'=0$$ $$y'=-\frac{yx+y^2}{1+xy}=-\left( \frac{xy+1+y^2-1}{1+xy} \right)=-\left(1+\frac{y^2-1}{1+xy}\right)$$ What can I do from here?","I need to solve the following ODE: I can see that this is not an exact equation, and when I tried to multiply it by the integration factor or I got nothing. So I started to ""play"" with it. What can I do from here?",y(x+y)dx+(1+xy)dy=0 \mu(x) \mu(y) y(x+y)dx+(1+xy)dy=0 y(x+y)+(1+xy)y'=0 y'=-\frac{yx+y^2}{1+xy}=-\left( \frac{xy+1+y^2-1}{1+xy} \right)=-\left(1+\frac{y^2-1}{1+xy}\right),['ordinary-differential-equations']
44,Group laws of the flow of time-dependent vector field,Group laws of the flow of time-dependent vector field,,"I am reading the fundamental theorem on time-dependent flows from the book Introduction to Smooth Manifolds, written by John Lee ( see page 237 ). The last line of the theorem (i.e., equation $9.18$ ) says that if $p\in M_{t_1,t_0}$ and $\psi_{t_1,t_0}(p)\in M_{t_2,t_1}$ , then $p\in M_{t_2,t_0}$ and $$\psi_{t_2,t_1}\circ \psi_{t_1,t_0}(p)=\psi_{t_2,t_0}(p).$$ Notice that the left-hand side of this equation depends on $t_1$ , but the right-hand side doesn't. Should it not be $$\psi_{t_2,t_1}\circ \psi_{t_0,t_1}(p)=\psi_{t_2+t_0,t_1}(p)?$$ The question arises while checking the group laws (i.e., equation $9.3$ on page 209 ) of the flow of each vector field $V_t\colon M\to TM$ defined by $V_t(p)=V(t,p)$ .","I am reading the fundamental theorem on time-dependent flows from the book Introduction to Smooth Manifolds, written by John Lee ( see page 237 ). The last line of the theorem (i.e., equation ) says that if and , then and Notice that the left-hand side of this equation depends on , but the right-hand side doesn't. Should it not be The question arises while checking the group laws (i.e., equation on page 209 ) of the flow of each vector field defined by .","9.18 p\in M_{t_1,t_0} \psi_{t_1,t_0}(p)\in M_{t_2,t_1} p\in M_{t_2,t_0} \psi_{t_2,t_1}\circ \psi_{t_1,t_0}(p)=\psi_{t_2,t_0}(p). t_1 \psi_{t_2,t_1}\circ \psi_{t_0,t_1}(p)=\psi_{t_2+t_0,t_1}(p)? 9.3 V_t\colon M\to TM V_t(p)=V(t,p)","['ordinary-differential-equations', 'dynamical-systems', 'smooth-manifolds', 'group-actions', 'vector-fields']"
45,Finding the function that satisfies $\lim_{t\to x} \frac{t^2f(x)-x^2f(t)}{f(t)-f(x)}=1$,Finding the function that satisfies,\lim_{t\to x} \frac{t^2f(x)-x^2f(t)}{f(t)-f(x)}=1,"I have been provided that a function satisfies this given condition: $$\lim_{t\to x} \frac{t^2f(x)-x^2f(t)}{f(t)-f(x)}=1$$ I need to find the value of $$L=\lim_{x\to 1}\frac{\ln(f(x)-\ln2)}{x-1}$$ where the limit has a non zero finite value. My attempt Since this is a $\frac00$ indeterminate form, I apply L'Hôpital's rule and obtain $$\lim_{t\to x}\frac{2tf(x)-x^2f^\prime(t)}{f'(t)}=1$$ I write this as $$\lim_{t\to x}\frac{2tf(x)}{f'(t)}-x^2=1$$ Putting $t=x$ $$\frac{2xf(x)}{f'(x)}-x^2=1$$ Putting f(x)=y and $f'(x)=\frac{dy}{dx}$ , we obtain $$\frac{dy}{dx}=\frac{2xy}{1+x^2}$$ $$\frac{dy}{y}=\frac{2xdx}{1+x^2}$$ Integrating on both sides $$\int\frac{dy}{y}=\int\frac{2xdx}{1+x^2}$$ $$\ln(y)=\ln(1+x^2)+c$$ $$y=e^{\ln(1+x^2)+c}$$ $$y=k(1+x^2)$$ Since $L$ is a non zero finite value, and the denominator tends to $0,$ the argument inside the logarithm should tend to $1$ $$\begin{align}f(1)-\ln2=1 &\implies 2k=1+\ln2\\&\implies f(x)=\frac{(1+\ln2)}{2}(1+x^2).\end{align}$$ Applying L'Hôpital's rule on $L$ $$L=\lim_{x\to 1}\frac{f'(x)}{f(x)-\ln2}$$ $$L=\frac{f'(1)}{f(1)-\ln2}$$ $$L=1+\ln2$$ However according to the answer key, the answer is $1.$ Can anyone tell me at what point I made a mistake in my solution, or if maybe the answer key is incorrect? Thank you. PS: This is is my first question on this site so I'm sorry for any formatting issues in advance and would gladly fix them if you point them out to me.","I have been provided that a function satisfies this given condition: I need to find the value of where the limit has a non zero finite value. My attempt Since this is a indeterminate form, I apply L'Hôpital's rule and obtain I write this as Putting Putting f(x)=y and , we obtain Integrating on both sides Since is a non zero finite value, and the denominator tends to the argument inside the logarithm should tend to Applying L'Hôpital's rule on However according to the answer key, the answer is Can anyone tell me at what point I made a mistake in my solution, or if maybe the answer key is incorrect? Thank you. PS: This is is my first question on this site so I'm sorry for any formatting issues in advance and would gladly fix them if you point them out to me.","\lim_{t\to x} \frac{t^2f(x)-x^2f(t)}{f(t)-f(x)}=1 L=\lim_{x\to 1}\frac{\ln(f(x)-\ln2)}{x-1} \frac00 \lim_{t\to x}\frac{2tf(x)-x^2f^\prime(t)}{f'(t)}=1 \lim_{t\to x}\frac{2tf(x)}{f'(t)}-x^2=1 t=x \frac{2xf(x)}{f'(x)}-x^2=1 f'(x)=\frac{dy}{dx} \frac{dy}{dx}=\frac{2xy}{1+x^2} \frac{dy}{y}=\frac{2xdx}{1+x^2} \int\frac{dy}{y}=\int\frac{2xdx}{1+x^2} \ln(y)=\ln(1+x^2)+c y=e^{\ln(1+x^2)+c} y=k(1+x^2) L 0, 1 \begin{align}f(1)-\ln2=1 &\implies 2k=1+\ln2\\&\implies f(x)=\frac{(1+\ln2)}{2}(1+x^2).\end{align} L L=\lim_{x\to 1}\frac{f'(x)}{f(x)-\ln2} L=\frac{f'(1)}{f(1)-\ln2} L=1+\ln2 1.","['ordinary-differential-equations', 'limits', 'functions']"
46,Proving that Coefficients of Homogeneous Linear Differential Equations have to be constant if $f^{\prime}$ and $f$ are solutions,Proving that Coefficients of Homogeneous Linear Differential Equations have to be constant if  and  are solutions,f^{\prime} f,"Let $a_0$ and $a_1$ be differentiable functions such that for each solution $f$ of the homogeneous linear differential equation $$ y^{\prime \prime}+a_1 y^{\prime}+a_0 y=0 $$ also $f^{\prime}$ is a solution. Then $a_0$ and $a_1$ are constant. Assuming $f$ is a solution, then $$ f^{\prime \prime}+a_1 f^{\prime}+a_0 f=0 $$ and $$ (f^{\prime})^{\prime \prime}+a_1 (f^{\prime})^{\prime}+a_0 (f^{\prime})=f^{\prime \prime}+a_1 f^{\prime \prime}+a_0 f^{\prime}=0. $$ Hence we have got $$ f^{\prime\prime \prime}+(a_1-1)f^{\prime\prime}+(a_0-a_1)f^{\prime}-a_0f=0. $$ and $$ f^{\prime\prime \prime}+a_1(f^{\prime\prime}-f^{\prime})+a_0(f^\prime-f)=0. $$ But I do not know how this can be helpful. Any hint would be appreciated.","Let and be differentiable functions such that for each solution of the homogeneous linear differential equation also is a solution. Then and are constant. Assuming is a solution, then and Hence we have got and But I do not know how this can be helpful. Any hint would be appreciated.","a_0 a_1 f 
y^{\prime \prime}+a_1 y^{\prime}+a_0 y=0
 f^{\prime} a_0 a_1 f 
f^{\prime \prime}+a_1 f^{\prime}+a_0 f=0
 
(f^{\prime})^{\prime \prime}+a_1 (f^{\prime})^{\prime}+a_0 (f^{\prime})=f^{\prime \prime}+a_1 f^{\prime \prime}+a_0 f^{\prime}=0.
 
f^{\prime\prime \prime}+(a_1-1)f^{\prime\prime}+(a_0-a_1)f^{\prime}-a_0f=0.
 
f^{\prime\prime \prime}+a_1(f^{\prime\prime}-f^{\prime})+a_0(f^\prime-f)=0.
",['ordinary-differential-equations']
47,Solving the differential $\frac{y'y'''}{y''} = x$,Solving the differential,\frac{y'y'''}{y''} = x,"I've been trying to solve the differential equation: $\frac{f'(x)f'''(x)}{f''(x)} = x$ , $x\in \mathbb{R}$ My initial attempt was to integrate by parts, something like this: $$\int_{}^{} \frac{f'(x)f'''(x)}{f''(x)} dx = \int_{}^{}x dx \Leftrightarrow \\ ln(f''(x))f'(x) - \int_{}^{} \ln(f''(x)) f''(x)dx  = \frac{x^2}{2} + C1$$ but I soon came to realize that there was absolutely no way to proceed. Even if I try to reduce it to a second-order nonlinear ODE by letting $u(x) = f'(x)$ , I can't seem to solve it by simple integration. After some trial and error, I managed to figure out that $f(x) = \frac{x^3}{3} + C ,x\in \mathbb{R}$ (as noted by @Vasili) is indeed a solution (if not the only solution) to this differential. And here I ask you: How do we mathematically prove that $f(x) = \frac{x^3}{3}$ + C is a solution? How do we reach that conclusion without simply making guesses? Thanks in advance for any insight!","I've been trying to solve the differential equation: , My initial attempt was to integrate by parts, something like this: but I soon came to realize that there was absolutely no way to proceed. Even if I try to reduce it to a second-order nonlinear ODE by letting , I can't seem to solve it by simple integration. After some trial and error, I managed to figure out that (as noted by @Vasili) is indeed a solution (if not the only solution) to this differential. And here I ask you: How do we mathematically prove that + C is a solution? How do we reach that conclusion without simply making guesses? Thanks in advance for any insight!","\frac{f'(x)f'''(x)}{f''(x)} = x x\in \mathbb{R} \int_{}^{} \frac{f'(x)f'''(x)}{f''(x)} dx = \int_{}^{}x dx \Leftrightarrow \\ ln(f''(x))f'(x) - \int_{}^{} \ln(f''(x)) f''(x)dx  = \frac{x^2}{2} + C1 u(x) = f'(x) f(x) = \frac{x^3}{3} + C ,x\in \mathbb{R} f(x) = \frac{x^3}{3}","['calculus', 'integration', 'ordinary-differential-equations', 'analysis', 'differential']"
48,Solve differential equation: $t\cdot\frac{dx}{dt}=x(\ln x - \ln t)$,Solve differential equation:,t\cdot\frac{dx}{dt}=x(\ln x - \ln t),"Solve the equation $$t\cdot\frac{dx}{dt}=x(\ln x - \ln t)$$ My try: We consider the equation for $x, t >0$ . $$tx'=x(\ln x - \ln t)$$ $$t \cdot \frac{dx}{dt}=x(\ln x - \ln t)$$ $$\frac{dx}{x}=(\ln x - \ln t)\frac{dt}t$$ $$\int \frac{1}{x}dx=\int \ln (\frac xt) \cdot \frac 1t dt$$ $\int \ln (\frac xt) \cdot \frac 1t dt=\begin{cases} u=\ln (\frac xt) \\ du = -\frac 1t dt \end{cases} = -\int u du = -\frac{u^2}2 +C = -\frac 12 \ln ^2 (\frac xt) +C$ We go back to our equation: $$\ln x =-\frac 12 \ln ^2 (\frac xt) +C$$ However, I think my way of solving is not the best, because I came to an equation from which it is difficult to determine $x(t)$ .","Solve the equation My try: We consider the equation for . We go back to our equation: However, I think my way of solving is not the best, because I came to an equation from which it is difficult to determine .","t\cdot\frac{dx}{dt}=x(\ln x - \ln t) x, t >0 tx'=x(\ln x - \ln t) t \cdot \frac{dx}{dt}=x(\ln x - \ln t) \frac{dx}{x}=(\ln x - \ln t)\frac{dt}t \int \frac{1}{x}dx=\int \ln (\frac xt) \cdot \frac 1t dt \int \ln (\frac xt) \cdot \frac 1t dt=\begin{cases} u=\ln (\frac xt) \\ du = -\frac 1t dt \end{cases} = -\int u du = -\frac{u^2}2 +C = -\frac 12 \ln ^2 (\frac xt) +C \ln x =-\frac 12 \ln ^2 (\frac xt) +C x(t)","['ordinary-differential-equations', 'absolute-value', 'initial-value-problems']"
49,Find the invariant manifolds of the equilibrium,Find the invariant manifolds of the equilibrium,,"Consider the system $$\begin{cases}\dot{x}=x+y\cos(y)\\ \dot{y}=-y \end{cases}$$ which has the unique equilibrium point $(0,0)$ . I want to find the invariant manifolds for this system. The Jacobian at the origin is $$\begin{pmatrix} 1&1\\ 0&-1 \end{pmatrix}$$ which has eigenvalues $\lambda_1=1$ and $\lambda_2=-1$ . An eigenvector for $\lambda_1$ is $v_1=(1,0)^\intercal$ and for $\lambda_2$ , an eigenvector is $v_2=(1,-2)^\intercal$ . It is clear that the unstable manifold is the unstable subspace $\operatorname{span}(v_1)$ aka the $x$ -axis. However, I am completely lost on how to find the stable manifold. How does one go about finding this?","Consider the system which has the unique equilibrium point . I want to find the invariant manifolds for this system. The Jacobian at the origin is which has eigenvalues and . An eigenvector for is and for , an eigenvector is . It is clear that the unstable manifold is the unstable subspace aka the -axis. However, I am completely lost on how to find the stable manifold. How does one go about finding this?","\begin{cases}\dot{x}=x+y\cos(y)\\
\dot{y}=-y
\end{cases} (0,0) \begin{pmatrix}
1&1\\
0&-1
\end{pmatrix} \lambda_1=1 \lambda_2=-1 \lambda_1 v_1=(1,0)^\intercal \lambda_2 v_2=(1,-2)^\intercal \operatorname{span}(v_1) x","['ordinary-differential-equations', 'dynamical-systems', 'set-invariance']"
50,$Au_{xx} + Bu = 0 $ then $u = 0$,then,Au_{xx} + Bu = 0  u = 0,"Let $A,B > 0$ and $0 < a < b < \infty$ . Consider $u \in C^{1}([0,b])$ , $u = 0$ in $[0,a)$ and $$ Au_{xx} + Bu = 0 \ \  \text{in} \ \ (a,b) $$ with $u(a) = u(b) = 0$ . Then $u = 0$ in $[a,b)$ . My ideia: How $u \in C^{1}([a,b])$ and $u= 0$ in $[0,a)$ , then $u(0) = u (a) = u_{x}(a) = 0$ . By system, we have $$ u(x) = c_{1}\cos\bigg(\sqrt{\frac{A}{B}}x\bigg) + c_{2}\sin\bigg(\sqrt{\frac{A}{B}}x\bigg), \ \ \text{in} \ \ (a,b) $$ But, $u(0) = u (a) = u_{x}(a) = 0$ , then $$ u(x) =  c_{2}\sin\bigg(\sqrt{\frac{A}{B}}x\bigg) $$ with $$ c_{2}\cos\bigg(\sqrt{\frac{A}{B}}a\bigg) = c_{2}\sin\bigg(\sqrt{\frac{A}{B}}a\bigg) = 0 $$ Soon, $\frac{A}{B}a = \frac{\pi}{4} + \pi n$ or $c_{2} = 0$ . And now? I can't continue.","Let and . Consider , in and with . Then in . My ideia: How and in , then . By system, we have But, , then with Soon, or . And now? I can't continue.","A,B > 0 0 < a < b < \infty u \in C^{1}([0,b]) u = 0 [0,a) 
Au_{xx} + Bu = 0 \ \  \text{in} \ \ (a,b)
 u(a) = u(b) = 0 u = 0 [a,b) u \in C^{1}([a,b]) u= 0 [0,a) u(0) = u (a) = u_{x}(a) = 0 
u(x) = c_{1}\cos\bigg(\sqrt{\frac{A}{B}}x\bigg) + c_{2}\sin\bigg(\sqrt{\frac{A}{B}}x\bigg), \ \ \text{in} \ \ (a,b)
 u(0) = u (a) = u_{x}(a) = 0 
u(x) =  c_{2}\sin\bigg(\sqrt{\frac{A}{B}}x\bigg)
 
c_{2}\cos\bigg(\sqrt{\frac{A}{B}}a\bigg) = c_{2}\sin\bigg(\sqrt{\frac{A}{B}}a\bigg) = 0
 \frac{A}{B}a = \frac{\pi}{4} + \pi n c_{2} = 0",['ordinary-differential-equations']
51,Examples of ODEs that are toy models for complicated phenomena in PDEs,Examples of ODEs that are toy models for complicated phenomena in PDEs,,"Let me elaborate on what I mean by the title of this question. In studying (specifically non-linear) PDEs, one finds that each term in the equations contributes a certain amount to the behavior of the solution and the resulting behavior of the said solution depends on the tension between these effects. Such effects are for example non-linearity, transport, diffusion, dispersion, and dissipation. I am interested in example where these phenomena are manifested in simpler ODEs, and these examples would then be considered toy models. One example would be a tension between linear dissipation (the first term) and non-linear growth (the second term) manifested in this ODE: $$ \dot{y}(t)=-y+y^2, y(0)=y_0\neq 0, 1. $$ Let $y_0>0$ . This is well manifested in the solution, which takes the form $$y(t)=\left(1+e^t\left(\frac{1-y_0}{y_0}\right)\right)^{-1}$$ is globally defined for $t\ge 0$ if $y_0<1$ (the linear term dominates), and in fact $y(t)\to 0$ as $t\to\infty$ . However if $y_0>1$ , then the solution blows up at $t=-\log(1-1/y_0)$ , originating from the fact that the non-linear term is dominating. I am looking for similar examples where the solution to an ODE qualitatively represents the tension between effects seen in more complicated PDEs.","Let me elaborate on what I mean by the title of this question. In studying (specifically non-linear) PDEs, one finds that each term in the equations contributes a certain amount to the behavior of the solution and the resulting behavior of the said solution depends on the tension between these effects. Such effects are for example non-linearity, transport, diffusion, dispersion, and dissipation. I am interested in example where these phenomena are manifested in simpler ODEs, and these examples would then be considered toy models. One example would be a tension between linear dissipation (the first term) and non-linear growth (the second term) manifested in this ODE: Let . This is well manifested in the solution, which takes the form is globally defined for if (the linear term dominates), and in fact as . However if , then the solution blows up at , originating from the fact that the non-linear term is dominating. I am looking for similar examples where the solution to an ODE qualitatively represents the tension between effects seen in more complicated PDEs.","
\dot{y}(t)=-y+y^2, y(0)=y_0\neq 0, 1.
 y_0>0 y(t)=\left(1+e^t\left(\frac{1-y_0}{y_0}\right)\right)^{-1} t\ge 0 y_0<1 y(t)\to 0 t\to\infty y_0>1 t=-\log(1-1/y_0)","['real-analysis', 'ordinary-differential-equations', 'partial-differential-equations']"
52,Does the solution of $y' = (x^2 + y^2) e^{-(x^2+y^2)}$ have a limit for $x \to \infty$?,Does the solution of  have a limit for ?,y' = (x^2 + y^2) e^{-(x^2+y^2)} x \to \infty,"An old exam problem I am trying to solve is as follows: Given the cauchy problem $y' = (x^2 + y^2) e^{-(x^2+y^2)}, y(x_0) = y_0$ , do the following: Show that there is a unique solution for all $x \in \mathbb{R}$ Does the limit $ \lim_{x \to \infty} y(x) $ exist? Hint: evaluate the limit $ \lim_{x \to \infty} (x^2 + y^2) e^{-(x^2+y^2)} e^x$ For part (1), I managed to bound the functions $f(x,y) = (x^2 + y^2) e^{-(x^2+y^2)}$ and $f_y(x,y)$ under some fixed value. Then, I concluded, that we have a solution at some interval $[x_0 - \varepsilon, x_0 + \varepsilon]$ and we can extend this to $\mathbb{R}$ by moving to the right and left and applying the same result. I had quite a hard time bounding the function $f_y(x,y)$ , so first of all, I would be glad if someone shows me a quick and elegant way to do so. Secondly, and more importantly, I don't know how to approach part (2). I suppose I should somehow bound the integral $\int  (x^2 + y^2) e^{-(x^2+y^2)}$ by $(x^2 + y^2) e^{-(x^2+y^2)} e^x$ and then show that this limit tends to $0$ . But I have been so far unsuccesful with showing either of these two claims to be true. So any help here would be much appreciated!","An old exam problem I am trying to solve is as follows: Given the cauchy problem , do the following: Show that there is a unique solution for all Does the limit exist? Hint: evaluate the limit For part (1), I managed to bound the functions and under some fixed value. Then, I concluded, that we have a solution at some interval and we can extend this to by moving to the right and left and applying the same result. I had quite a hard time bounding the function , so first of all, I would be glad if someone shows me a quick and elegant way to do so. Secondly, and more importantly, I don't know how to approach part (2). I suppose I should somehow bound the integral by and then show that this limit tends to . But I have been so far unsuccesful with showing either of these two claims to be true. So any help here would be much appreciated!","y' = (x^2 + y^2) e^{-(x^2+y^2)}, y(x_0) = y_0 x \in \mathbb{R}  \lim_{x \to \infty} y(x)   \lim_{x \to \infty} (x^2 + y^2) e^{-(x^2+y^2)} e^x f(x,y) = (x^2 + y^2) e^{-(x^2+y^2)} f_y(x,y) [x_0 - \varepsilon, x_0 + \varepsilon] \mathbb{R} f_y(x,y) \int  (x^2 + y^2) e^{-(x^2+y^2)} (x^2 + y^2) e^{-(x^2+y^2)} e^x 0","['real-analysis', 'ordinary-differential-equations', 'analysis', 'cauchy-problem']"
53,How to obtain all solutions of separable differential equation where the existence and uniqueness theorem does not apply?,How to obtain all solutions of separable differential equation where the existence and uniqueness theorem does not apply?,,"From blackpenredpen 's February 2017 video on the existence & uniqueness theorem, the separable differential equation $$\frac{{\rm d} y}{{\rm d}x}= x\sqrt{y-3} \tag{1}$$ with the initial condition $y(4)=3$ has the only solutions $$y=\left(\frac{x^2}{4}-4\right)^2+3 \tag{2}$$ $$y(x)=3 \tag{3}$$ where the valid interval can be the whole real line. The solution is obtained by dividing both sides of $(1)$ by $\sqrt{y-3}$ and then taking the integral of both sides. For me, it seems that it has not been proven that there aren't any more solutions $y$ to $(1)$ such that there exists a $x$ for which $y(x)=3$ . I'm thinking when integrating both sides we first assume that $y(x) \neq 3$ so we can divide both sides. When this happens, we can not say we have covered all solutions $y$ of $(1)$ such that there is a $x$ for which $y(x)=3$ . This is evident from $(3)$ . I'm assuming obtaining $(2)$ was just a happy accident. In short the proof has 2 cases. First case assumes $y\neq3$ for all $x$ and covers all those solutions and the second case is the constant solution $y=3$ . I'm asking where the justification is that we don't need a third case for the solutions $y$ such that only for some $x$ in the valid interval is $y(x)=3$ . Background: Solving separable differential equations","From blackpenredpen 's February 2017 video on the existence & uniqueness theorem, the separable differential equation with the initial condition has the only solutions where the valid interval can be the whole real line. The solution is obtained by dividing both sides of by and then taking the integral of both sides. For me, it seems that it has not been proven that there aren't any more solutions to such that there exists a for which . I'm thinking when integrating both sides we first assume that so we can divide both sides. When this happens, we can not say we have covered all solutions of such that there is a for which . This is evident from . I'm assuming obtaining was just a happy accident. In short the proof has 2 cases. First case assumes for all and covers all those solutions and the second case is the constant solution . I'm asking where the justification is that we don't need a third case for the solutions such that only for some in the valid interval is . Background: Solving separable differential equations",\frac{{\rm d} y}{{\rm d}x}= x\sqrt{y-3} \tag{1} y(4)=3 y=\left(\frac{x^2}{4}-4\right)^2+3 \tag{2} y(x)=3 \tag{3} (1) \sqrt{y-3} y (1) x y(x)=3 y(x) \neq 3 y (1) x y(x)=3 (3) (2) y\neq3 x y=3 y x y(x)=3,"['ordinary-differential-equations', 'initial-value-problems']"
54,Constant of integration for solving differential equation,Constant of integration for solving differential equation,,"As I am progressing differential equations practice, I found myself at somewhat of a roadblock. The roadblock is essentially that let's say we have the following equation: $$\int x^2\,dx=\int y\,dy.$$ Now when we put the constants down after integration, such as: $$\frac{x^3}{3}+c_1=\frac{y^2}{2}+c_2,$$ would $c_1$ and $c_2$ not have to have the same value? Of course, the problem then becomes that the constant would cancel out, and there would be no constant in the solution. However, I was just wondering why the constants could be different, especially since we need to ensure that both left and right antiderivatives are the same function? In the case that the constants can be different, would someone be able to explain why that can be the case? Thanks so much.","As I am progressing differential equations practice, I found myself at somewhat of a roadblock. The roadblock is essentially that let's say we have the following equation: Now when we put the constants down after integration, such as: would and not have to have the same value? Of course, the problem then becomes that the constant would cancel out, and there would be no constant in the solution. However, I was just wondering why the constants could be different, especially since we need to ensure that both left and right antiderivatives are the same function? In the case that the constants can be different, would someone be able to explain why that can be the case? Thanks so much.","\int x^2\,dx=\int y\,dy. \frac{x^3}{3}+c_1=\frac{y^2}{2}+c_2, c_1 c_2","['integration', 'ordinary-differential-equations']"
55,Prove that a solution to differential equation is unbounded,Prove that a solution to differential equation is unbounded,,"Consider the following system: $$ \begin{cases} x'=x-6y\\ y'=-2x-y \end{cases} $$ I want to prove that the solution to the system which satisfies $$ x\left(0\right)=1,\thinspace\thinspace y\left(0\right)=0 $$ is unbounded, Without solving the equation. My work so far: Notice that this system is hamiltonian system, meaning: $$ \begin{cases} x'=\frac{\partial}{\partial y}H\left(x,y\right)\\ y'=-\frac{\partial}{\partial x}H\left(x,y\right) \end{cases} $$ Where $ H\left(x,y\right)=x^{2}+xy-3y^{2} $ and the solution $\varphi$ which satisfies $\varphi(0)=(1,0)$ also satisfies $$ \varphi\left(0\right)\in\Lambda_{1}=\left\{ \left(x,y\right)\thinspace:\thinspace H\left(x,y\right)=1\right\} =\left\{ \left(x,y\right)\thinspace:\thinspace x^{2}+xy-3y^{2}=1\right\}  $$ And the level curve $\Lambda_1$ is hyperbola (which means it is unbounded). Now, I tried to assume by contradiction that $\varphi(t) $ indeed is a bounded solution, which means that both $x(t)$ and $y(t)$ are bounded, but I cannot find how to reach a contradiction. Any help would be appreciated.","Consider the following system: I want to prove that the solution to the system which satisfies is unbounded, Without solving the equation. My work so far: Notice that this system is hamiltonian system, meaning: Where and the solution which satisfies also satisfies And the level curve is hyperbola (which means it is unbounded). Now, I tried to assume by contradiction that indeed is a bounded solution, which means that both and are bounded, but I cannot find how to reach a contradiction. Any help would be appreciated."," \begin{cases}
x'=x-6y\\
y'=-2x-y
\end{cases}   x\left(0\right)=1,\thinspace\thinspace y\left(0\right)=0   \begin{cases}
x'=\frac{\partial}{\partial y}H\left(x,y\right)\\
y'=-\frac{\partial}{\partial x}H\left(x,y\right)
\end{cases}   H\left(x,y\right)=x^{2}+xy-3y^{2}  \varphi \varphi(0)=(1,0)  \varphi\left(0\right)\in\Lambda_{1}=\left\{ \left(x,y\right)\thinspace:\thinspace H\left(x,y\right)=1\right\} =\left\{ \left(x,y\right)\thinspace:\thinspace x^{2}+xy-3y^{2}=1\right\}   \Lambda_1 \varphi(t)  x(t) y(t)","['calculus', 'ordinary-differential-equations']"
56,$y''+2\tan x y'-y=0$,,y''+2\tan x y'-y=0,"Question: Use the variation of parameter method to find the general solution of the following differential equation $$(\cos x) y''+(2\sin x) y'-(\cos x) y =0\;\;\;\;,\;\;\;\;0<x<1$$ My Try: I think the question is wrong, since the right hand side term is 0, so the particular integral will also be zero. Thus, the general solution will be equal to homogenous solution. So, I think no use of using the variation of parameter formulas since $y_p(x)=0$ always. I reduced the equation as in the subject or title and then used integrating factor $$y=(\cos x )z$$ to eliminate the term $y'$ but I got another difficult DE as $z''-2\sec^2xz=0$ Please help with any suggestions or do you think question is correct. Is there a way to solve it ?","Question: Use the variation of parameter method to find the general solution of the following differential equation My Try: I think the question is wrong, since the right hand side term is 0, so the particular integral will also be zero. Thus, the general solution will be equal to homogenous solution. So, I think no use of using the variation of parameter formulas since always. I reduced the equation as in the subject or title and then used integrating factor to eliminate the term but I got another difficult DE as Please help with any suggestions or do you think question is correct. Is there a way to solve it ?","(\cos x) y''+(2\sin x) y'-(\cos x) y =0\;\;\;\;,\;\;\;\;0<x<1 y_p(x)=0 y=(\cos x )z y' z''-2\sec^2xz=0","['calculus', 'integration', 'ordinary-differential-equations']"
57,A simple special case of Gronwall's inequality for Dini derivatives,A simple special case of Gronwall's inequality for Dini derivatives,,"Let $I=[t_0,t_1)\subset \Bbb R$ an interval and $a,b,c\ge0$ with $a>c$ . Assume that $f\colon I\to\Bbb R$ is a continuous function with $$\tag{1} f(t)-f(s)\le \int_s^t\left( -af(r)+be^{-cr} \right) dr \quad \text{for all $s,t\in I$.}  $$ If $f$ were differentiable, dividing by $(t-s)$ and letting $s\to t$ would imply $$ f'(t) \le -af(t)+be^{-ct}  \quad \text{for all $t\in I$}, $$ and hence one could apply the differential version of Gronwall's Lemma and arrive at $$\tag{2} f(t) \le f(t_0)e^{-a(t-t_0)}+\frac{b}{a-c}e^{-at}\left(e^{(a-c)t}-e^{(a-c)t_0}\right)  \quad \text{for all $t\in I$}. $$ Note that, contrary to the classical integral version of Gronwall's Lemma, (1) holds for all $s,t\in I$ and not just for $s=t_0$ and $t\in I$ . Hence even if $f$ is not differentiable (but continuous!), the inequality (1) still implies $$\tag{3} Df(t) \le -af(t)+be^{-ct}  \quad \text{for all $t\in I$}, $$ where $D$ denotes any Dini derivative. However, I do not understand completely how to continue from here in order to prove that (2) holds. In the case $b=0$ , (3) implies $D^+(\log f)\le-a$ and so (2) follows immediately. I am not sure how to deal with the term $be^{-ct}$ if $b$ does not vanish, though. So, does (1) (or (3)) imply (2) also for $b\neq0$ and if yes, how can we prove it?","Let an interval and with . Assume that is a continuous function with If were differentiable, dividing by and letting would imply and hence one could apply the differential version of Gronwall's Lemma and arrive at Note that, contrary to the classical integral version of Gronwall's Lemma, (1) holds for all and not just for and . Hence even if is not differentiable (but continuous!), the inequality (1) still implies where denotes any Dini derivative. However, I do not understand completely how to continue from here in order to prove that (2) holds. In the case , (3) implies and so (2) follows immediately. I am not sure how to deal with the term if does not vanish, though. So, does (1) (or (3)) imply (2) also for and if yes, how can we prove it?","I=[t_0,t_1)\subset \Bbb R a,b,c\ge0 a>c f\colon I\to\Bbb R \tag{1}
f(t)-f(s)\le \int_s^t\left( -af(r)+be^{-cr} \right) dr \quad \text{for all s,t\in I.} 
 f (t-s) s\to t 
f'(t) \le -af(t)+be^{-ct}  \quad \text{for all t\in I},
 \tag{2}
f(t) \le f(t_0)e^{-a(t-t_0)}+\frac{b}{a-c}e^{-at}\left(e^{(a-c)t}-e^{(a-c)t_0}\right)  \quad \text{for all t\in I}.
 s,t\in I s=t_0 t\in I f \tag{3}
Df(t) \le -af(t)+be^{-ct}  \quad \text{for all t\in I},
 D b=0 D^+(\log f)\le-a be^{-ct} b b\neq0","['real-analysis', 'ordinary-differential-equations', 'integral-inequality', 'dini-derivative']"
58,Solve Non-Linear DOE with Dirac Delta Function in the denominator,Solve Non-Linear DOE with Dirac Delta Function in the denominator,,"I found a differential ordinary equation that uses Diric Delta Function that I don't know how to solve. Thank you if you can help me. The ODE is like that $$ z' = \rho \cdot \sqrt{1+z^2} + w \cdot \delta(x-x_0) $$ And my domain is $\left[a, \ b\right]$ with $a < x_0 < b$ . With $\rho$ and $w$ constants. How can I solve it? The solution I tried was: $$ \dfrac{z'}{\rho \cdot \sqrt{1+z^2} + w \cdot \delta(x-x_0)} = 1 $$ $$ I = \int_{a}^{b} \dfrac{z'}{\rho \sqrt{1+z^2}+w\delta} dx = \int_{a}^{b} 1 \ dx = b-a $$ Now I divide the domain $\left[a, \ b\right]$ in three domains $\left[a, \ x_0 - \varepsilon\right] \cup \left[x_0 - \varepsilon, \ x_0 + \varepsilon\right] \cup \left[x_0 + \varepsilon, \ b\right]$ with $\varepsilon > 0$ small and then make three integrals separately $I_1$ , $I_2$ and $I_3$ . $$ I = \lim_{\varepsilon \to 0^{+}} I_{1}(\varepsilon)+I_{2}(\varepsilon)+I_{3}(\varepsilon) = \lim_{\varepsilon \to 0^{+}} \left(\underbrace{\int_{a}^{x_0 - \varepsilon} \square \ dx}_{I_1(\varepsilon)} + \underbrace{\int_{x_0 - \varepsilon}^{x_0 + \varepsilon} \square \ dx}_{I_2(\varepsilon)} + \underbrace{\int_{x_0 + \varepsilon}^{b} \square \ dx}_{I_3(\varepsilon)}\right) $$ With \begin{align*} I_{1}(\varepsilon) & = \int_{a}^{x_0-\varepsilon} \dfrac{z'}{\rho \sqrt{1+z^2}+w\underbrace{\delta}_{0}} dx  = \dfrac{1}{\rho}\int_{z(a)}^{z(x_0-\varepsilon)} \dfrac{dz}{\sqrt{1+z^2}} = \dfrac{1}{\rho} \ln \left(z + \sqrt{z^2-1}\right)_{z(a)}^{z(x_0-\varepsilon)} \\ I_2(\varepsilon) & = \int_{x_0-\varepsilon}^{x_0+\varepsilon} \dfrac{z'}{\rho \sqrt{1+z^2}+w\delta} dx  \overset{\underset{\mathrm{?}}{}}{=}  \int_{x_0-\varepsilon}^{x_0+\varepsilon} 1  \ dx = 2\varepsilon  \\ I_{3}(\varepsilon) & = \int_{x_0+\varepsilon}^{b} \dfrac{z'}{\rho \sqrt{1+z^2}+w\underbrace{\delta}_{0}} dx  = \dfrac{1}{\rho}\int_{z(x_0+\varepsilon)}^{z(b)} \dfrac{dz}{\sqrt{1+z^2}} = \dfrac{1}{\rho} \ln \left(z + \sqrt{z^2-1}\right)_{z(x_0+\varepsilon)}^{z(b)} \end{align*} And therefore \begin{align*} b - a = I & = \lim_{\varepsilon \to 0^{+}} I_1 + I_2 + I_3 \\  & = \lim_{\varepsilon^{+}} \dfrac{1}{\rho} \ln \left(z + \sqrt{z^2-1}\right)_{z(a)}^{z(x_0-\varepsilon)}  + 2 \varepsilon + \dfrac{1}{\rho} \ln \left(z + \sqrt{z^2-1}\right)_{z(x_0+\varepsilon)}^{z(b)} \\ & = \dfrac{1}{\rho} \ln \left(z + \sqrt{z^2-1}\right)_{z(a)}^{z(b)} + \lim_{\varepsilon \to 0^{+}} \dfrac{1}{\rho} \ln \left(z + \sqrt{z^2-1}\right)_{z(x_0-\varepsilon)}^{z(x_0+\varepsilon)} \\ & \overset{\underset{\mathrm{?}}{}}{=} \dfrac{1}{\rho} \ln \left(z + \sqrt{z^2-1}\right)_{z(a)}^{z(b)} \end{align*} I thought about using another function in the place of $\delta$ , like $$ \delta_\varepsilon(x-x_0) = \begin{cases} 0 \ \ \ \text{if} \ x < x_0 - \varepsilon \\ \frac{1}{2\varepsilon} \ \ \ \text{if} \ x_0 - \varepsilon < x <  x_0 + \varepsilon \\ 0 \ \ \ \text{if} \ x_0 + \varepsilon < x \\ \end{cases} $$ or even a continuous function, but it's harder(or impossible) to calculate the integral: $$ \delta_\varepsilon(x-x_0) = \dfrac{1}{\varepsilon \sqrt{\pi}} \exp\left(-\dfrac{1}{\varepsilon^2}(x-x_0)^2\right) $$ Unfortunately, the physical problem (of this ODE) suggests that $z$ has a linear term of Heaviside Function at the point $x_0$ .","I found a differential ordinary equation that uses Diric Delta Function that I don't know how to solve. Thank you if you can help me. The ODE is like that And my domain is with . With and constants. How can I solve it? The solution I tried was: Now I divide the domain in three domains with small and then make three integrals separately , and . With And therefore I thought about using another function in the place of , like or even a continuous function, but it's harder(or impossible) to calculate the integral: Unfortunately, the physical problem (of this ODE) suggests that has a linear term of Heaviside Function at the point .","
z' = \rho \cdot \sqrt{1+z^2} + w \cdot \delta(x-x_0)
 \left[a, \ b\right] a < x_0 < b \rho w 
\dfrac{z'}{\rho \cdot \sqrt{1+z^2} + w \cdot \delta(x-x_0)} = 1
 
I = \int_{a}^{b} \dfrac{z'}{\rho \sqrt{1+z^2}+w\delta} dx = \int_{a}^{b} 1 \ dx = b-a
 \left[a, \ b\right] \left[a, \ x_0 - \varepsilon\right] \cup \left[x_0 - \varepsilon, \ x_0 + \varepsilon\right] \cup \left[x_0 + \varepsilon, \ b\right] \varepsilon > 0 I_1 I_2 I_3 
I = \lim_{\varepsilon \to 0^{+}} I_{1}(\varepsilon)+I_{2}(\varepsilon)+I_{3}(\varepsilon) = \lim_{\varepsilon \to 0^{+}} \left(\underbrace{\int_{a}^{x_0 - \varepsilon} \square \ dx}_{I_1(\varepsilon)} + \underbrace{\int_{x_0 - \varepsilon}^{x_0 + \varepsilon} \square \ dx}_{I_2(\varepsilon)} + \underbrace{\int_{x_0 + \varepsilon}^{b} \square \ dx}_{I_3(\varepsilon)}\right)
 \begin{align*}
I_{1}(\varepsilon) & = \int_{a}^{x_0-\varepsilon} \dfrac{z'}{\rho \sqrt{1+z^2}+w\underbrace{\delta}_{0}} dx  = \dfrac{1}{\rho}\int_{z(a)}^{z(x_0-\varepsilon)} \dfrac{dz}{\sqrt{1+z^2}} = \dfrac{1}{\rho} \ln \left(z + \sqrt{z^2-1}\right)_{z(a)}^{z(x_0-\varepsilon)} \\
I_2(\varepsilon) & = \int_{x_0-\varepsilon}^{x_0+\varepsilon} \dfrac{z'}{\rho \sqrt{1+z^2}+w\delta} dx  \overset{\underset{\mathrm{?}}{}}{=}  \int_{x_0-\varepsilon}^{x_0+\varepsilon} 1  \ dx = 2\varepsilon  \\
I_{3}(\varepsilon) & = \int_{x_0+\varepsilon}^{b} \dfrac{z'}{\rho \sqrt{1+z^2}+w\underbrace{\delta}_{0}} dx  = \dfrac{1}{\rho}\int_{z(x_0+\varepsilon)}^{z(b)} \dfrac{dz}{\sqrt{1+z^2}} = \dfrac{1}{\rho} \ln \left(z + \sqrt{z^2-1}\right)_{z(x_0+\varepsilon)}^{z(b)}
\end{align*} \begin{align*}
b - a = I & = \lim_{\varepsilon \to 0^{+}} I_1 + I_2 + I_3 \\ 
& = \lim_{\varepsilon^{+}} \dfrac{1}{\rho} \ln \left(z + \sqrt{z^2-1}\right)_{z(a)}^{z(x_0-\varepsilon)}  + 2 \varepsilon + \dfrac{1}{\rho} \ln \left(z + \sqrt{z^2-1}\right)_{z(x_0+\varepsilon)}^{z(b)} \\
& = \dfrac{1}{\rho} \ln \left(z + \sqrt{z^2-1}\right)_{z(a)}^{z(b)} + \lim_{\varepsilon \to 0^{+}} \dfrac{1}{\rho} \ln \left(z + \sqrt{z^2-1}\right)_{z(x_0-\varepsilon)}^{z(x_0+\varepsilon)} \\
& \overset{\underset{\mathrm{?}}{}}{=} \dfrac{1}{\rho} \ln \left(z + \sqrt{z^2-1}\right)_{z(a)}^{z(b)}
\end{align*} \delta 
\delta_\varepsilon(x-x_0) =
\begin{cases}
0 \ \ \ \text{if} \ x < x_0 - \varepsilon \\
\frac{1}{2\varepsilon} \ \ \ \text{if} \ x_0 - \varepsilon < x <  x_0 + \varepsilon \\
0 \ \ \ \text{if} \ x_0 + \varepsilon < x \\
\end{cases}
 
\delta_\varepsilon(x-x_0) =
\dfrac{1}{\varepsilon \sqrt{\pi}} \exp\left(-\dfrac{1}{\varepsilon^2}(x-x_0)^2\right)
 z x_0","['ordinary-differential-equations', 'definite-integrals', 'dirac-delta']"
59,How can I make this 1st order ODE separable?,How can I make this 1st order ODE separable?,,"$$ y (2+3xy) \,{\rm d} x = x (2-3xy) \,{\rm d} y $$ I tried using the substitution $y=\frac{v}{x}$ , but that didn't get me far. Then I tried using the substitution $y=vx$ , but that didn't work either. Any help, please?","I tried using the substitution , but that didn't get me far. Then I tried using the substitution , but that didn't work either. Any help, please?"," y (2+3xy) \,{\rm d} x = x (2-3xy) \,{\rm d} y  y=\frac{v}{x} y=vx","['ordinary-differential-equations', 'substitution']"
60,Instability of a parameter varying system whose parameters belong to a compact set,Instability of a parameter varying system whose parameters belong to a compact set,,"Suppose, there is a system $$\dot{x}=f(t, \gamma_p(t), x)$$ with $x\in\mathbb{R}^2$ . For my specific case, parameter vector $\gamma_p$ is a scalar and known monotonic function with a compact image set (for example $\gamma_p(t) = e^{-t}$ , thus $\gamma_p \in [1,0)$ ). I would like to show instability of this system. Well known techniques (specifically, averaging) are applicable to show system's instability for all frozen parameter values, that is a simplified system $\tilde{f}$ with $$\dot{x}=f^\star(t, x) = f(t, \gamma_p(t^\star), x) \quad \forall \; t^\star \in [0, \infty).$$ Can instability of all frozen parameter cases let me conclude anything about the instability of the original system? Specifics of my problem $f$ takes the form $$ f(t,\gamma_p(t),x) = \varepsilon \begin{bmatrix}\tfrac{\alpha(1-\gamma_p(t))}{\gamma_p(t)} \\ 1\end{bmatrix} L\left(t, x_2, (x_1-\alpha x_2)\gamma_p(t)+\alpha x_2\right)$$ where $\varepsilon > 0$ is a small parameter of the system, $\alpha$ is a positive coefficient and $L$ is a $2\pi$ periodic function over only the first argument $t$ (thus not necessarily periodic when $\gamma_p$ is changing) with $$\mathrm{sign}\left[\int_{0}^{2\pi}L(t, x_2, \underbrace{(x_1-\alpha x_2)\gamma_p(t^\star)+\alpha x_2}_{y(t^\star)})\mathrm{d}t\right] = \mathrm{sign}\left[y(t^\star)\right]\quad \forall \; t^\star \in [0,\infty).$$ Substituting $\gamma_p(t^\star)$ gives the frozen parameter system $$ f^\star(t,x) = \varepsilon \begin{bmatrix}\tfrac{\alpha(1-\gamma_p(t^\star))}{\gamma_p(t^\star)} \\ 1\end{bmatrix} L\left(t, x_2, (x_1-\alpha x_2)\gamma_p(t^\star)+\alpha x_2\right)$$ With $\gamma_p(t^\star)$ now fixed, dynamics become periodic with period $2\pi$ which allows me to apply averaging. Specifically, we can determine the stability/instability through the averaged system $$ \dot{x}\approx\tilde{f}^\star(x) = \varepsilon \begin{bmatrix}\tfrac{\alpha(1-\gamma_p(t^\star))}{\gamma_p(t^\star)} \\ 1\end{bmatrix}\int_{0}^{2\pi}L(t, x_2, (x_1-\alpha x_2)\gamma_p(t^\star)+\alpha x_2)\mathrm{d}t$$ Using further details of the system, I can show the system grows unboundedly in the direction $[\gamma_p(t^\star), (1-\gamma_p(t^\star)]$ in the space spanned by $(x_1,x_2)$ .","Suppose, there is a system with . For my specific case, parameter vector is a scalar and known monotonic function with a compact image set (for example , thus ). I would like to show instability of this system. Well known techniques (specifically, averaging) are applicable to show system's instability for all frozen parameter values, that is a simplified system with Can instability of all frozen parameter cases let me conclude anything about the instability of the original system? Specifics of my problem takes the form where is a small parameter of the system, is a positive coefficient and is a periodic function over only the first argument (thus not necessarily periodic when is changing) with Substituting gives the frozen parameter system With now fixed, dynamics become periodic with period which allows me to apply averaging. Specifically, we can determine the stability/instability through the averaged system Using further details of the system, I can show the system grows unboundedly in the direction in the space spanned by .","\dot{x}=f(t, \gamma_p(t), x) x\in\mathbb{R}^2 \gamma_p \gamma_p(t) = e^{-t} \gamma_p \in [1,0) \tilde{f} \dot{x}=f^\star(t, x) = f(t, \gamma_p(t^\star), x) \quad \forall \; t^\star \in [0, \infty). f  f(t,\gamma_p(t),x) = \varepsilon \begin{bmatrix}\tfrac{\alpha(1-\gamma_p(t))}{\gamma_p(t)} \\ 1\end{bmatrix} L\left(t, x_2, (x_1-\alpha x_2)\gamma_p(t)+\alpha x_2\right) \varepsilon > 0 \alpha L 2\pi t \gamma_p \mathrm{sign}\left[\int_{0}^{2\pi}L(t, x_2, \underbrace{(x_1-\alpha x_2)\gamma_p(t^\star)+\alpha x_2}_{y(t^\star)})\mathrm{d}t\right] = \mathrm{sign}\left[y(t^\star)\right]\quad \forall \; t^\star \in [0,\infty). \gamma_p(t^\star)  f^\star(t,x) = \varepsilon \begin{bmatrix}\tfrac{\alpha(1-\gamma_p(t^\star))}{\gamma_p(t^\star)} \\ 1\end{bmatrix} L\left(t, x_2, (x_1-\alpha x_2)\gamma_p(t^\star)+\alpha x_2\right) \gamma_p(t^\star) 2\pi  \dot{x}\approx\tilde{f}^\star(x) = \varepsilon \begin{bmatrix}\tfrac{\alpha(1-\gamma_p(t^\star))}{\gamma_p(t^\star)} \\ 1\end{bmatrix}\int_{0}^{2\pi}L(t, x_2, (x_1-\alpha x_2)\gamma_p(t^\star)+\alpha x_2)\mathrm{d}t [\gamma_p(t^\star), (1-\gamma_p(t^\star)] (x_1,x_2)","['ordinary-differential-equations', 'control-theory', 'stability-in-odes', 'stability-theory', 'nonlinear-dynamics']"
61,Does this algebraic relation between the functions imply linear dependence?,Does this algebraic relation between the functions imply linear dependence?,,"While working on a geometric problem, I reached to the following uniqueness question: Let $s(t),\tilde s(t), b(t),\tilde b(t)$ be continuous* real-valued functions, and suppose that $$ \bigg(\frac{\tilde s}{s}\bigg)^2+\bigg(\frac{s}{\tilde s}\bigg)^2+\bigg( \frac{\tilde b}{s}-\frac{b}{\tilde  s}+\frac{c}{s}\bigg)^2 $$ is independent of $t$ , where $c \in \mathbb{R}$ is some given non-zero constant. Is it true that $\tilde b = \alpha b, \tilde s = \frac{1}{\alpha} s$ for some constant scalar $\alpha$ ? If this is the case, then one easily sees that $s(t)$ must be constant. *I am fine with assuming higher regularity of the functions, i.e. that all the functions are $C^1$ . I guess we could differentiate the equation, but this doesn't look too simple.","While working on a geometric problem, I reached to the following uniqueness question: Let be continuous* real-valued functions, and suppose that is independent of , where is some given non-zero constant. Is it true that for some constant scalar ? If this is the case, then one easily sees that must be constant. *I am fine with assuming higher regularity of the functions, i.e. that all the functions are . I guess we could differentiate the equation, but this doesn't look too simple.","s(t),\tilde s(t), b(t),\tilde b(t) 
\bigg(\frac{\tilde s}{s}\bigg)^2+\bigg(\frac{s}{\tilde s}\bigg)^2+\bigg( \frac{\tilde b}{s}-\frac{b}{\tilde  s}+\frac{c}{s}\bigg)^2
 t c \in \mathbb{R} \tilde b = \alpha b, \tilde s = \frac{1}{\alpha} s \alpha s(t) C^1","['real-analysis', 'ordinary-differential-equations', 'systems-of-equations', 'symmetry']"
62,Saddle-node behaviour of planar systems,Saddle-node behaviour of planar systems,,"I'm teaching a course on ODEs using Lawrence Perko's book. Having assigned Q3(c) of section 2.11, and both working out the solution using theorems in the sections and consulting the solution manual, I get an answer that seems to conflict with other calculations. The details are as follows: The system is: $$ \dot{x} = y, \qquad \dot{y} = x^4 + xy. $$ According to a Thm 2.11.3 in Perko (see screenshot of Thms 2.11.2 and 2.11.3 below), taken from Andronov--Leontovich--Gordon--Maier , the critical point $(0,0)$ is a saddle-node, ""because"" the lowest order monomial of the form $x^k$ in the $\dot{y}$ equation is of even degree, the lowest order monomial of the form $x^ny$ has $n = 1$ , with coefficient $b_1 = 1 \not= 0$ , and $4/2 > 1$ . The solution manual concurs. Now from the equation, $\dot{x} > 0$ on $\{y > 0\}$ , $\dot{x} < 0$ on $\{y < 0\}$ . Moreover $\dot{y} = xy + O(|x,y|^4)$ , which means $\dot{y} > 0$ in the 1st and 3rd quadrants, and $\dot{y} < 0$ in the 2nd and 4th quadrants close to $(0,0)$ . And any way I draw this, it looks like a cusp near the critical point. Finally, winplot confirms this: . So my question is, is it known that Thm 2.11.3 is in fact not a theorem, or have I missed something?","I'm teaching a course on ODEs using Lawrence Perko's book. Having assigned Q3(c) of section 2.11, and both working out the solution using theorems in the sections and consulting the solution manual, I get an answer that seems to conflict with other calculations. The details are as follows: The system is: According to a Thm 2.11.3 in Perko (see screenshot of Thms 2.11.2 and 2.11.3 below), taken from Andronov--Leontovich--Gordon--Maier , the critical point is a saddle-node, ""because"" the lowest order monomial of the form in the equation is of even degree, the lowest order monomial of the form has , with coefficient , and . The solution manual concurs. Now from the equation, on , on . Moreover , which means in the 1st and 3rd quadrants, and in the 2nd and 4th quadrants close to . And any way I draw this, it looks like a cusp near the critical point. Finally, winplot confirms this: . So my question is, is it known that Thm 2.11.3 is in fact not a theorem, or have I missed something?","
\dot{x} = y, \qquad \dot{y} = x^4 + xy.
 (0,0) x^k \dot{y} x^ny n = 1 b_1 = 1 \not= 0 4/2 > 1 \dot{x} > 0 \{y > 0\} \dot{x} < 0 \{y < 0\} \dot{y} = xy + O(|x,y|^4) \dot{y} > 0 \dot{y} < 0 (0,0)","['ordinary-differential-equations', 'fixed-points']"
63,An interesting ODE,An interesting ODE,,"I've been thinking about approaches to solve $$\frac{\textrm{d}^{3}y}{\textrm{d}x^{3}}=\textrm{e}^{-y(x)}.$$ My initial thought was to set $y(x)=\ln(z(x))$ in order to obtain an ODE relating $z$ to $x$ . However, this in fact makes things much worse (or at least much messier). Any insights much appreciated. Thanks","I've been thinking about approaches to solve My initial thought was to set in order to obtain an ODE relating to . However, this in fact makes things much worse (or at least much messier). Any insights much appreciated. Thanks",\frac{\textrm{d}^{3}y}{\textrm{d}x^{3}}=\textrm{e}^{-y(x)}. y(x)=\ln(z(x)) z x,"['calculus', 'integration', 'ordinary-differential-equations']"
64,Existence of periodic solution in an ODE system,Existence of periodic solution in an ODE system,,"Let the ODE system, \begin{cases}x'(t)=x^3+x^5 \\ y'(t)=y+y^7 \end{cases} defined in $\mathbb{R}^2$ . I need to say if this system supports periodic solution with period $T>0$ . This is a question of Analysis in $\mathbb{R}^n$ not ODE, that is, I need to demonstrate this using concepts presented in Analysis in $\mathbb{R}^n$ , my textbook is Analysis on Manifolds by Munkres. The hint is, A periodic solution of the system above is a closed curve $\gamma(t) = (x(t), y (t))$ that satisfies the system. I believe I have to use Stokes, but I am not able to proceed with the solution.","Let the ODE system, defined in . I need to say if this system supports periodic solution with period . This is a question of Analysis in not ODE, that is, I need to demonstrate this using concepts presented in Analysis in , my textbook is Analysis on Manifolds by Munkres. The hint is, A periodic solution of the system above is a closed curve that satisfies the system. I believe I have to use Stokes, but I am not able to proceed with the solution.","\begin{cases}x'(t)=x^3+x^5 \\ y'(t)=y+y^7 \end{cases} \mathbb{R}^2 T>0 \mathbb{R}^n \mathbb{R}^n \gamma(t) = (x(t), y (t))","['real-analysis', 'ordinary-differential-equations', 'analysis']"
65,Find the general solution to the ODE:,Find the general solution to the ODE:,,"I was asked to find the general solution to these ODEs. $$\frac{dy}{dx}=\frac{(-8x+3y-31)}{(-3x+y-11)}$$ I tried by rearranging to the form $M(x,y)+N(x,y)\frac{dy}{dx}=0$ and let $y=vx$ , but what I got was $$(v^2x-6vx-11v+8x+31)dx+x(-3x+vx-11)dv=0$$ and I could not separate $x$ and $v$ for integration. $$\frac{dy}{dx}=\frac{y}{x}+\frac{9}{2}xexp(-\frac{2y}{x})+\frac{9}{2}xexp(\frac{2y}{x})$$ I have totally no idea which method to be used in Question 2. May I know if I'm on the wrong track, and how should I solve them? Thanks.","I was asked to find the general solution to these ODEs. I tried by rearranging to the form and let , but what I got was and I could not separate and for integration. I have totally no idea which method to be used in Question 2. May I know if I'm on the wrong track, and how should I solve them? Thanks.","\frac{dy}{dx}=\frac{(-8x+3y-31)}{(-3x+y-11)} M(x,y)+N(x,y)\frac{dy}{dx}=0 y=vx (v^2x-6vx-11v+8x+31)dx+x(-3x+vx-11)dv=0 x v \frac{dy}{dx}=\frac{y}{x}+\frac{9}{2}xexp(-\frac{2y}{x})+\frac{9}{2}xexp(\frac{2y}{x})","['ordinary-differential-equations', 'homogeneous-equation']"
66,Multiple Solutions to an ODE,Multiple Solutions to an ODE,,"I want to find the solution to the IVP: $$y' = 2\cos(x)\sqrt{y-1},\;\; y\geq1 $$ with initial condition $y(0)=2$ . I used separation of variables to get the general solution $$y =\left(\sin(x)+C\right)^2 +1.$$ When I use the initial condition to try and find a specific solution, I get that $C = \pm 1$ . Both of these values of $C$ work when subbed back into the original differential equation. But since the ODE is continuous for all $y\geq 1$ and the $y$ partial derivative is continuous for all $y>1$ , Picard's theorem says that there should be a unique solution for the initial condition $y(0)=2$ . Is there any way that I can further test the two solutions to see which one is invalid?","I want to find the solution to the IVP: with initial condition . I used separation of variables to get the general solution When I use the initial condition to try and find a specific solution, I get that . Both of these values of work when subbed back into the original differential equation. But since the ODE is continuous for all and the partial derivative is continuous for all , Picard's theorem says that there should be a unique solution for the initial condition . Is there any way that I can further test the two solutions to see which one is invalid?","y' = 2\cos(x)\sqrt{y-1},\;\; y\geq1
 y(0)=2 y =\left(\sin(x)+C\right)^2 +1. C = \pm 1 C y\geq 1 y y>1 y(0)=2","['ordinary-differential-equations', 'initial-value-problems']"
67,Why isn't a jagged orbit ever observed in the two-body problem?,Why isn't a jagged orbit ever observed in the two-body problem?,,The two-body problem deals with two planets revolving around a common center relative to one another. Why doesn't the model ever exhibit a jagged orbit and it is always smoothly elliptical? Is there something about the math actively enforcing the smooth orbit?,The two-body problem deals with two planets revolving around a common center relative to one another. Why doesn't the model ever exhibit a jagged orbit and it is always smoothly elliptical? Is there something about the math actively enforcing the smooth orbit?,,"['ordinary-differential-equations', 'celestial-mechanics']"
68,Why is the flow generated by a smooth vector field smooth?,Why is the flow generated by a smooth vector field smooth?,,"Suppose $X$ is a smooth vector field on a smooth manifold $M$ . For any $p \in M$ , the theorems of existence and uniqueness to solutions of ODEs show that there is a unique differentiable function $\theta ^ {(p)} :J \to M$ , where $J$ is an open interval containing $0$ , such that $\theta ^ {(p)} (0) = p$ and $\frac{d}{dt}\theta ^ {(p)} (s)=X_{\theta^{(p)}(s)}$ . Then we can define a function $\theta$ on an appropriate subset of $M \times \mathbb{R}$ by $\theta(p,s)=\theta^{(p)}(s)$ . $\theta$ is the flow generated by $X$ . I am trying to understand why $\theta$ is smooth, and I don't even see why it has to be continuous. I thought of using the explicit construction from the proof of the existence theorem, but it seems to be quite involved so I wonder if there is a simpler way.","Suppose is a smooth vector field on a smooth manifold . For any , the theorems of existence and uniqueness to solutions of ODEs show that there is a unique differentiable function , where is an open interval containing , such that and . Then we can define a function on an appropriate subset of by . is the flow generated by . I am trying to understand why is smooth, and I don't even see why it has to be continuous. I thought of using the explicit construction from the proof of the existence theorem, but it seems to be quite involved so I wonder if there is a simpler way.","X M p \in M \theta ^ {(p)} :J \to M J 0 \theta ^ {(p)} (0) = p \frac{d}{dt}\theta ^ {(p)} (s)=X_{\theta^{(p)}(s)} \theta M \times \mathbb{R} \theta(p,s)=\theta^{(p)}(s) \theta X \theta","['ordinary-differential-equations', 'differential-geometry', 'smooth-manifolds', 'vector-fields']"
69,Analysis of frequency and amplitude at Hopf bifurcation,Analysis of frequency and amplitude at Hopf bifurcation,,"I am analyzing the following system, where $I_{in}$ is a scalar parameter: $$ \begin{aligned} &\dot{V} = 10 \left( V - \frac{V^3}{3} - R + I_{in} \right) \\ &\dot{R} = 0.8 \left( -R +1.25V + 1.5 \right) \end{aligned} $$ It is a simplified version of the Fitzhugh-Nagumo equations for neuronal excitability (reference to book below). There is a single equilibrium, that varies with $I_{in}$ , so we need to calculate the Jacobian at those equilibrium values and perform a stability analysis. Such an analysis reveals that as $I_{in}$ increases from zero to around 1.5, the system undergoes a supercritical Hopf bifurcation [ edit : it undergoes what I intuitively thought was a supercritical bifurcation]: We go from a stable center to an unstable center at a critical value (zero real eigenvalue) at $I_{crit}=0.966064$ . Note that I calculated the limit cycle boundaries in that diagram by just getting the minimum and maximum values of V for each loop through the limit cycle (examples of such loops are shown below in Figures 3 and 4). ( Edit : I added my derivation of $I_{crit}$ below). You can see the nature of the transition in the trace-determinant diagram in the following Figure 2: As $I_{in}$ increases, the equilibrium point turns from a sink to a (stable) spiral, and then we hit the critical point at $I_{crit}$ , after which we have a spiral source surrounded by a (stable) limit cycle, and eventually a stable source also surrounded by a limit cycle. So far, so good, I think. This all seems pretty straightforward. So what is the problem? At this point I am confused about a couple of things. In my book it says the following two facts (corollaries of the Hopf Bifurcation theorem) should be true near $I_{crit}$ : The amplitude of oscillations will be very small. The frequency of oscillation should be close to $\omega/2\pi$ Hz, where $\omega$ is the imaginary part of the eigenvalue. It seems that neither of these facts is true here. First, the oscillation amplitude starts out very large, as you can see in the bifurcation diagram in Figure 1. There is none of that textbook ramping up of the amplitude. Indeed, even when $I_{in}$ is less than $I_{crit}$ , there already a large, stable, limit-cycle-like orbit in this phase space! The following figure shows some full orbits in the phase space (left), and a couple of V trajectories on the right. This is for $I_{in}=I_{crit}-0.00874$ : That is, we have lots of large-amplitude stable orbits cycling around some stable centers (such damped oscillations occur only for orbits that are close to the equilibrium point). So not only does the limit cycle start out with a large amplitude past $I_{crit}$ , it seems there is already a kind of harbinger of a limit cycle with a large amplitude even before the bifurcation. That said, the above two facts do seem to apply to the damped spirals in Figure 2: the amplitude of the spiral is very small (tending toward zero), and its frequency is basically exactly $\omega/2\pi$ -- it is basically double the frequency of the large-amplitude pseudo-limit cycle that encloses it. Could that be what my text is referring to? This brings me explicitly to the second fact above: at $I_{crit}$ the eigenvalues are $\pm 3.05i$ . Hence, the frequency of oscillation should be about 0.5 Hz, a period of 2 s. But instead I see a period of 4 seconds (0.25 Hz), as the following diagram of V versus time for $I=I_{crit}+0.000001$ shows: I calculate the period based on the distance between the red X's. I'll mention again, though, if we were to do the same analysis of the oscillations of the damped spirals (as in Figure 3) the frequencies of those damped oscillations would basically be right -- it is the full limit cycles that seem off (though their order of magnitude is right). Overall, this system is supposed to be approachable because of its simplicity but I've already spent about a week hitting my head against it, still not sure of some of the most basic facts about Hopf Bifurcations. Derivation of the critical value Note the Jacobian of the system is: $$ J =  \begin{bmatrix} \frac{\partial F_1}{\partial V} & \frac{\partial F_1}{\partial R}\\ \frac{\partial F_2}{\partial V} & \frac{\partial F_2}{\partial R} \end{bmatrix} =  \begin{bmatrix} 10(1-V^2) & -10 \\ 1 & -0.8 \end{bmatrix} $$ Our task is essentially to determine the system's (V,R) equilibria for different values of $I_{in}$ . Then, we can plug these equilibrium values into the Jacobian for our stability analysis, and find the coefficient matrix where the real part of the eigenvalues go to zero. How do we find this? First, I found the equilibrium value of V that would yield purely imaginary eigenvalues, and I did this using the trace. That is, the sum of the eigenvalues is the same as the sum of the values in the coefficient matrix (the trace). From the equation for the Jacobian above, we know the trace is zero when: $$ 9.2 - 10V^2 = 0 \implies V = \pm \sqrt{0.92} = \pm 0.959 $$ Focusing on the negative root for now, this implies that our critical value of $I_{in}$ will be the one that generates $V_{eqm}=-0.959$ . How do we find this value of $I_{in}$ ? I did it by substitution, using the nullcline equations of our system. Namely, the equations for the nullclines of our system are given by: $$ \begin{aligned} &R = V - \frac{V^3}{3} + I_{input}\\ &R = 1.25V + 1.5 \end{aligned} $$ So, if given a value $V_{eqm}$ we can plug the second nullcline equation $R(V)$ into the first, and solve for $I_{in}$ in terms of $V$ . Namely, given a value of $V_{eqm}$ , the $I_{in}$ that produces that will be: $$ I_{in}=\frac{V^3}{3} + 0.25V + 1.5 $$ So, looping back to our question, if we plug in $V_{eqm}=-0.959$ into this equation, that yields $I_{crit}=0.966$ . Note also that plugging this $I_{crit}$ into the original system of equations and numerically solving for the equilibrium using Python's fsolve() yields the equilibrium point (V, R) = (-0.959, 0.301) , which gives secondary confirmation of our result. The Jacobian at this equilibrium point is: $$ J =  \begin{bmatrix} 0.8 & -10 \\ 1 & -0.8 \end{bmatrix} $$ This coefficient matrix has purely imaginary eigenvalues $\pm3.06i$ , as expected. So it seems we have a critical value where the real part of the eigenvalues reaches zero, as originally claimed. QED, maybe? To address a question from a comment: when $I=0.866$ the equilibrum point is $(V, R) = (-1.04, 0.20)$ , and the eigenvalues of the Jacobian are $-0.8\pm3.16i$ . This, coupled with the secondary confirmation of the calculations from the trace-discriminant curve (Figure 2 above), make me think there isn't a mistake in the calculation of $I_{crit}$ above. That said, I have definitely made worse mistakes in my lifetime, and thought I was right, so we definitely shouldn't exclude this possibility. Different question about same equations Hopf bifurcation and limit cycles Reference Wilson (1999) Spikes, decisions, and actions: dynamical foundations of neuroscience . OUP.","I am analyzing the following system, where is a scalar parameter: It is a simplified version of the Fitzhugh-Nagumo equations for neuronal excitability (reference to book below). There is a single equilibrium, that varies with , so we need to calculate the Jacobian at those equilibrium values and perform a stability analysis. Such an analysis reveals that as increases from zero to around 1.5, the system undergoes a supercritical Hopf bifurcation [ edit : it undergoes what I intuitively thought was a supercritical bifurcation]: We go from a stable center to an unstable center at a critical value (zero real eigenvalue) at . Note that I calculated the limit cycle boundaries in that diagram by just getting the minimum and maximum values of V for each loop through the limit cycle (examples of such loops are shown below in Figures 3 and 4). ( Edit : I added my derivation of below). You can see the nature of the transition in the trace-determinant diagram in the following Figure 2: As increases, the equilibrium point turns from a sink to a (stable) spiral, and then we hit the critical point at , after which we have a spiral source surrounded by a (stable) limit cycle, and eventually a stable source also surrounded by a limit cycle. So far, so good, I think. This all seems pretty straightforward. So what is the problem? At this point I am confused about a couple of things. In my book it says the following two facts (corollaries of the Hopf Bifurcation theorem) should be true near : The amplitude of oscillations will be very small. The frequency of oscillation should be close to Hz, where is the imaginary part of the eigenvalue. It seems that neither of these facts is true here. First, the oscillation amplitude starts out very large, as you can see in the bifurcation diagram in Figure 1. There is none of that textbook ramping up of the amplitude. Indeed, even when is less than , there already a large, stable, limit-cycle-like orbit in this phase space! The following figure shows some full orbits in the phase space (left), and a couple of V trajectories on the right. This is for : That is, we have lots of large-amplitude stable orbits cycling around some stable centers (such damped oscillations occur only for orbits that are close to the equilibrium point). So not only does the limit cycle start out with a large amplitude past , it seems there is already a kind of harbinger of a limit cycle with a large amplitude even before the bifurcation. That said, the above two facts do seem to apply to the damped spirals in Figure 2: the amplitude of the spiral is very small (tending toward zero), and its frequency is basically exactly -- it is basically double the frequency of the large-amplitude pseudo-limit cycle that encloses it. Could that be what my text is referring to? This brings me explicitly to the second fact above: at the eigenvalues are . Hence, the frequency of oscillation should be about 0.5 Hz, a period of 2 s. But instead I see a period of 4 seconds (0.25 Hz), as the following diagram of V versus time for shows: I calculate the period based on the distance between the red X's. I'll mention again, though, if we were to do the same analysis of the oscillations of the damped spirals (as in Figure 3) the frequencies of those damped oscillations would basically be right -- it is the full limit cycles that seem off (though their order of magnitude is right). Overall, this system is supposed to be approachable because of its simplicity but I've already spent about a week hitting my head against it, still not sure of some of the most basic facts about Hopf Bifurcations. Derivation of the critical value Note the Jacobian of the system is: Our task is essentially to determine the system's (V,R) equilibria for different values of . Then, we can plug these equilibrium values into the Jacobian for our stability analysis, and find the coefficient matrix where the real part of the eigenvalues go to zero. How do we find this? First, I found the equilibrium value of V that would yield purely imaginary eigenvalues, and I did this using the trace. That is, the sum of the eigenvalues is the same as the sum of the values in the coefficient matrix (the trace). From the equation for the Jacobian above, we know the trace is zero when: Focusing on the negative root for now, this implies that our critical value of will be the one that generates . How do we find this value of ? I did it by substitution, using the nullcline equations of our system. Namely, the equations for the nullclines of our system are given by: So, if given a value we can plug the second nullcline equation into the first, and solve for in terms of . Namely, given a value of , the that produces that will be: So, looping back to our question, if we plug in into this equation, that yields . Note also that plugging this into the original system of equations and numerically solving for the equilibrium using Python's fsolve() yields the equilibrium point (V, R) = (-0.959, 0.301) , which gives secondary confirmation of our result. The Jacobian at this equilibrium point is: This coefficient matrix has purely imaginary eigenvalues , as expected. So it seems we have a critical value where the real part of the eigenvalues reaches zero, as originally claimed. QED, maybe? To address a question from a comment: when the equilibrum point is , and the eigenvalues of the Jacobian are . This, coupled with the secondary confirmation of the calculations from the trace-discriminant curve (Figure 2 above), make me think there isn't a mistake in the calculation of above. That said, I have definitely made worse mistakes in my lifetime, and thought I was right, so we definitely shouldn't exclude this possibility. Different question about same equations Hopf bifurcation and limit cycles Reference Wilson (1999) Spikes, decisions, and actions: dynamical foundations of neuroscience . OUP.","I_{in} 
\begin{aligned}
&\dot{V} = 10 \left( V - \frac{V^3}{3} - R + I_{in} \right) \\
&\dot{R} = 0.8 \left( -R +1.25V + 1.5 \right)
\end{aligned}
 I_{in} I_{in} I_{crit}=0.966064 I_{crit} I_{in} I_{crit} I_{crit} \omega/2\pi \omega I_{in} I_{crit} I_{in}=I_{crit}-0.00874 I_{crit} \omega/2\pi I_{crit} \pm 3.05i I=I_{crit}+0.000001 
J = 
\begin{bmatrix}
\frac{\partial F_1}{\partial V} & \frac{\partial F_1}{\partial R}\\
\frac{\partial F_2}{\partial V} & \frac{\partial F_2}{\partial R}
\end{bmatrix} = 
\begin{bmatrix}
10(1-V^2) & -10 \\
1 & -0.8
\end{bmatrix}
 I_{in} 
9.2 - 10V^2 = 0 \implies V = \pm \sqrt{0.92} = \pm 0.959
 I_{in} V_{eqm}=-0.959 I_{in} 
\begin{aligned}
&R = V - \frac{V^3}{3} + I_{input}\\
&R = 1.25V + 1.5
\end{aligned}
 V_{eqm} R(V) I_{in} V V_{eqm} I_{in} 
I_{in}=\frac{V^3}{3} + 0.25V + 1.5
 V_{eqm}=-0.959 I_{crit}=0.966 I_{crit} 
J = 
\begin{bmatrix}
0.8 & -10 \\
1 & -0.8
\end{bmatrix}
 \pm3.06i I=0.866 (V, R) = (-1.04, 0.20) -0.8\pm3.16i I_{crit}","['ordinary-differential-equations', 'eigenvalues-eigenvectors', 'dynamical-systems', 'nonlinear-system', 'bifurcation']"
70,"Solve the initial value problem: $\frac{dy}{dx} = e^{x+y}$, given $y(0)=0$. [duplicate]","Solve the initial value problem: , given . [duplicate]",\frac{dy}{dx} = e^{x+y} y(0)=0,"This question already has answers here : Find the general solution of $y'= a^{x + y}$ where y is the function (2 answers) Closed 3 years ago . I have attempted the question several times so far, and I have always reached the same answer that differs from the solution, any advice would help greatly! My attempt $$\frac{dy}{e^y} = e^x dx$$ Taking the integral, I got $$-e^{-y} = e^x + C.$$ Solving for $y$ , I got $$y=-\ln(C-e^x).$$ After subbing in $y(0)=0$ , I got $$0=-\ln(C-1)$$ and solving for $C$ , I got $$C=2.$$ Thus, I got $$y=-\ln(2-e^x).$$ However, the solutions have $y=-\ln(1-e^x)$ as the answer. Have I done something wrong? Thanks in advance!","This question already has answers here : Find the general solution of $y'= a^{x + y}$ where y is the function (2 answers) Closed 3 years ago . I have attempted the question several times so far, and I have always reached the same answer that differs from the solution, any advice would help greatly! My attempt Taking the integral, I got Solving for , I got After subbing in , I got and solving for , I got Thus, I got However, the solutions have as the answer. Have I done something wrong? Thanks in advance!",\frac{dy}{e^y} = e^x dx -e^{-y} = e^x + C. y y=-\ln(C-e^x). y(0)=0 0=-\ln(C-1) C C=2. y=-\ln(2-e^x). y=-\ln(1-e^x),"['ordinary-differential-equations', 'initial-value-problems']"
71,Solving a third order Euler-Cauchy ODE,Solving a third order Euler-Cauchy ODE,,"I have been given the following ODE: $$(2x+3)^3 y''' + 3 (2x+3) y' - 6 y=0$$ and I have to solve it using Euler's method, which I am fairly familiar with. Now, I let $ 2x+3 = e^t$ and $y=e^{λt}$ After differentiating $y$ , I get that $$y''' = \frac{y_t'''-3y_t''+2y_t'}{e^{3t}}$$ and $y'$ is $$\frac{y_t'}{e^t}$$ Now after substituting in the given equation I get $$e^{3t} \frac{y_t'''-3y_t''+2y_t'}{e^{3t}} + 3e^t \frac{y_t'}{e^t} -6y=0 $$ After which I am left with the following homogeneous equation: $$y''' - 3y'' + 5y' -6y =0$$ Which can be easily solved and the solutions are (I checked in wolframalpha): $$C_1 e^{2t} + e^{\frac{t}{2}}(C_2 \cos(\frac{\sqrt {11}}{2} t) + C_3 \sin(\frac{\sqrt {11}}{2} t))$$ When I plug $2x+3=e^t$ back in, I get: $$y(x) = C_1(2x+3)^2 + C_2 \sqrt{2x+3}   \cos(\frac{\sqrt {11}}{2}\ln(2x+3)) +  C_3 \sqrt{2x+3}   \sin(\frac{\sqrt {11}}{2}\ln(2x+3))$$ But the wolframalpha solution for the whole eqauation is $$C_2(2x+3)^{\frac{3}{2}} + C_3(2x+3) + C_1\sqrt{2x+3}$$ Now, I am new to ODES so I can't rule out that I made a silly mistake. What I did when substituting back is essentially $e^t = 2x+3$ and $t=\ln(2x+3)$ Can anyone point out my mistakes?","I have been given the following ODE: and I have to solve it using Euler's method, which I am fairly familiar with. Now, I let and After differentiating , I get that and is Now after substituting in the given equation I get After which I am left with the following homogeneous equation: Which can be easily solved and the solutions are (I checked in wolframalpha): When I plug back in, I get: But the wolframalpha solution for the whole eqauation is Now, I am new to ODES so I can't rule out that I made a silly mistake. What I did when substituting back is essentially and Can anyone point out my mistakes?",(2x+3)^3 y''' + 3 (2x+3) y' - 6 y=0  2x+3 = e^t y=e^{λt} y y''' = \frac{y_t'''-3y_t''+2y_t'}{e^{3t}} y' \frac{y_t'}{e^t} e^{3t} \frac{y_t'''-3y_t''+2y_t'}{e^{3t}} + 3e^t \frac{y_t'}{e^t} -6y=0  y''' - 3y'' + 5y' -6y =0 C_1 e^{2t} + e^{\frac{t}{2}}(C_2 \cos(\frac{\sqrt {11}}{2} t) + C_3 \sin(\frac{\sqrt {11}}{2} t)) 2x+3=e^t y(x) = C_1(2x+3)^2 + C_2 \sqrt{2x+3}   \cos(\frac{\sqrt {11}}{2}\ln(2x+3)) +  C_3 \sqrt{2x+3}   \sin(\frac{\sqrt {11}}{2}\ln(2x+3)) C_2(2x+3)^{\frac{3}{2}} + C_3(2x+3) + C_1\sqrt{2x+3} e^t = 2x+3 t=\ln(2x+3),['ordinary-differential-equations']
72,"Solving Differential Equation : $(1+\tan y)(\,dx - \,dy) + 2x\,dy = 0$",Solving Differential Equation :,"(1+\tan y)(\,dx - \,dy) + 2x\,dy = 0","The Question is: $$(1+\tan y)(\,dx - \,dy) + 2x\,dy = 0$$ I tried the problem by: $$(\tan y+1)\left( 1 - \frac{\,dy}{\,dx} \right) +2x\frac{dy}{dx} = 0$$ Then I put $\tan y = u $ $\frac{du}{dx}.\frac{1}{1+u^2} = \frac{dy}{dx}$ However going ahead doesn't yield any useful expressions. Also I don't see any clear product rule expressions. How would you solve this ?",The Question is: I tried the problem by: Then I put However going ahead doesn't yield any useful expressions. Also I don't see any clear product rule expressions. How would you solve this ?,"(1+\tan y)(\,dx - \,dy) + 2x\,dy = 0 (\tan y+1)\left( 1 - \frac{\,dy}{\,dx} \right) +2x\frac{dy}{dx} = 0 \tan y = u  \frac{du}{dx}.\frac{1}{1+u^2} = \frac{dy}{dx}",['ordinary-differential-equations']
73,Integrating an ODE in terms of elliptic functions,Integrating an ODE in terms of elliptic functions,,"Consider the ODE for $w:\mathbb{C}\to \mathbb{C}$ \begin{align} w''=2w^3+Aw+B &&(1) \end{align} We can multiply it by $w'$ and then integrate to get \begin{align} w'^2=w^4+Aw^2+2Bw+C &&(2) \end{align} ( $A,B,C$ are complex constants) My question is how would one then integrate (2) - i.e. obtain an explicit expression for the solution $w$ ? In particular, equation (1) is found in `Ordinary Differential Equations' by Ince. It's listed as equation VIII in section 14.316. Ince states that it is ""integrable in terms of elliptic functions"". Unfortunately, I cannot see how this would be done. As an example of what I'm looking for, in a problem related to the one above, one is able to transform the ODE \begin{align} P'^2-P^4+\lambda P^2+\frac{B^2}{P^2}+2A=0 && (3) \end{align} into the ODE defining the Weierstrass elliptic function $$\wp'^2=4\wp^3-g_2\wp-g_3$$ by taking $$\wp(x):=P(x)^2-\frac{\lambda}{3},$$ where $g_2, g_3$ are constants depending on $A,B,\lambda$ . And so the general solution to (3) is $$P(x)=\pm\sqrt{\wp(x)+\frac{\lambda}{3}}$$","Consider the ODE for We can multiply it by and then integrate to get ( are complex constants) My question is how would one then integrate (2) - i.e. obtain an explicit expression for the solution ? In particular, equation (1) is found in `Ordinary Differential Equations' by Ince. It's listed as equation VIII in section 14.316. Ince states that it is ""integrable in terms of elliptic functions"". Unfortunately, I cannot see how this would be done. As an example of what I'm looking for, in a problem related to the one above, one is able to transform the ODE into the ODE defining the Weierstrass elliptic function by taking where are constants depending on . And so the general solution to (3) is","w:\mathbb{C}\to \mathbb{C} \begin{align}
w''=2w^3+Aw+B &&(1)
\end{align} w' \begin{align}
w'^2=w^4+Aw^2+2Bw+C &&(2)
\end{align} A,B,C w \begin{align}
P'^2-P^4+\lambda P^2+\frac{B^2}{P^2}+2A=0 && (3)
\end{align} \wp'^2=4\wp^3-g_2\wp-g_3 \wp(x):=P(x)^2-\frac{\lambda}{3}, g_2, g_3 A,B,\lambda P(x)=\pm\sqrt{\wp(x)+\frac{\lambda}{3}}","['integration', 'ordinary-differential-equations', 'complex-integration', 'nonlinear-system', 'elliptic-functions']"
74,Differential Equation $dy/dx=y^{1/3}$ and condition $y(x_0)=y_0$ has infinite solutions,Differential Equation  and condition  has infinite solutions,dy/dx=y^{1/3} y(x_0)=y_0,"Prove that the differential equation $$\frac{dy}{dx}= y^{1/3}$$ with the initial value of $y(x_0)=y_0$ has infinite solutions. I don't really understand the problem if I have to show that there are infinite solutions depending on the initial conditions or if it is something like if I proposed $(x_0,y_0)=(0,0)$ and prove that for that case the equation has infinite solutions. if you think is the first one could you explain how to do it.",Prove that the differential equation with the initial value of has infinite solutions. I don't really understand the problem if I have to show that there are infinite solutions depending on the initial conditions or if it is something like if I proposed and prove that for that case the equation has infinite solutions. if you think is the first one could you explain how to do it.,"\frac{dy}{dx}= y^{1/3} y(x_0)=y_0 (x_0,y_0)=(0,0)","['real-analysis', 'calculus', 'ordinary-differential-equations']"
75,Can smooth ODE converge to its equilibrium in finite time?,Can smooth ODE converge to its equilibrium in finite time?,,"Consider the following nonliner system: \begin{align} \dot{x}=f(x) \end{align} where $x\in\mathbb{R}^n$ and $f(x)\in\mathbb{R}^n$ is sufficiently smooth and Lipschitz in $x$ . Then the system is smooth and admits a unique solution. Suppose $x^*\in\mathbb{R}^n$ is an equilibrium of the system, i.e., $f(x^*)=0$ . Is it possible that, for some initial condition $x(0)=x_0$ , the solution of the system satisfies \begin{align} \lim_{t\to T}x(t)=x^{*}, \end{align} that is, the solution reaches the equilibrium $x^*$ in some finite time $T$ . If it is not possible, is there a way to show that, the solution of a smooth system will take an infinite amount of time to converge to an equilibrium? Update：Assume that $x_0\neq x^{*}$ .","Consider the following nonliner system: where and is sufficiently smooth and Lipschitz in . Then the system is smooth and admits a unique solution. Suppose is an equilibrium of the system, i.e., . Is it possible that, for some initial condition , the solution of the system satisfies that is, the solution reaches the equilibrium in some finite time . If it is not possible, is there a way to show that, the solution of a smooth system will take an infinite amount of time to converge to an equilibrium? Update：Assume that .","\begin{align}
\dot{x}=f(x)
\end{align} x\in\mathbb{R}^n f(x)\in\mathbb{R}^n x x^*\in\mathbb{R}^n f(x^*)=0 x(0)=x_0 \begin{align}
\lim_{t\to T}x(t)=x^{*},
\end{align} x^* T x_0\neq x^{*}","['ordinary-differential-equations', 'dynamical-systems', 'finite-duration']"
76,How can I solve $\int \frac{P_1(t)}{P_2(t)e^{P_3(t)}} dt$,How can I solve,\int \frac{P_1(t)}{P_2(t)e^{P_3(t)}} dt,"The other day I found an integral on the form: $$\int \frac{P_1(t)}{P_2(t)e^{P_3(t)}} dt$$ where $P_1,P_2,P_3$ all are polynomials. Does there exist any particular technique to approach solving such integrals? My own work is mostly limited to quite fruitless experimentation with logarithmic derivative. One idea I had was to rewrite it as $$I(t_1) = \int_{-\infty}^{t_1} \frac{P_1(t)}{P_2(t)e^{P_3(t)}} dt$$ Then, by fundamental theorem of calculus: $$\frac{\partial}{\partial t_1} I(t_1) = \frac{P_1(t_1)}{P_2(t_1)e^{P_3(t_1)}}$$ and: $$P_2(t_1)e^{P_3(t_1)} \frac{\partial}{\partial t_1} I(t_1) = {P_1(t_1)}$$ This would then be solved in power series with some machinery which could solve for example an expanded version of: this , allowing $P_k$ to be arbitrary power series instead of polynomials.","The other day I found an integral on the form: where all are polynomials. Does there exist any particular technique to approach solving such integrals? My own work is mostly limited to quite fruitless experimentation with logarithmic derivative. One idea I had was to rewrite it as Then, by fundamental theorem of calculus: and: This would then be solved in power series with some machinery which could solve for example an expanded version of: this , allowing to be arbitrary power series instead of polynomials.","\int \frac{P_1(t)}{P_2(t)e^{P_3(t)}} dt P_1,P_2,P_3 I(t_1) = \int_{-\infty}^{t_1} \frac{P_1(t)}{P_2(t)e^{P_3(t)}} dt \frac{\partial}{\partial t_1} I(t_1) = \frac{P_1(t_1)}{P_2(t_1)e^{P_3(t_1)}} P_2(t_1)e^{P_3(t_1)} \frac{\partial}{\partial t_1} I(t_1) = {P_1(t_1)} P_k","['integration', 'ordinary-differential-equations', 'analysis', 'power-series']"
77,Can’t see that an ODE is equivalent to a Bessel equation,Can’t see that an ODE is equivalent to a Bessel equation,,"I can solve the following differential equation without any trouble using the method of Frobenius $$ x^2 y’’ - (2 + 3x) y = 0. $$ When I put the differential equation in Mathematica, it gives me the solutions in terms of modified Bessel functions of order 3 $$ y(x) = A \sqrt{x} I_3\big(2\sqrt{3x}\big) + B \sqrt{x} K_3\big(2\sqrt{3x}\big). $$ I cannot for the life of me see how to put the given ODE into the form of a modified Bessel equation. Can anyone point me in the right direction? Some Added Information Generally, the equation $$ \frac{d}{dx}\left(x^a \frac{dy}{dx}\right) + b x^c y = 0, $$ can be transformed to a Bessel equation with solution $$ y(x) = x^{\nu/\alpha} Z_\nu \left(\alpha\sqrt{|b|} x^{1/\alpha}\right), $$ where $Z_\nu$ is any Bessel function solution of the transformed equation, if we choose $$ \alpha = \frac{2}{c-a+2} \quad \text{and} \quad \nu = \frac{1-a}{c-a+2}, $$ Considering the equations for $\alpha$ and $\nu$ , I can see from Mathematica’s solution that $\nu = 3$ and $\alpha = 2$ , so I can solve them to find $a = -2$ and $c = -3$ . Putting those into the ODE above, we get $$ \frac{d}{dx}\left(x^{-2} \frac{dy}{dx}\right) + b x^{-3} y = 0. $$ Expanding this out, I get $$ \frac{1}{x^2}y’’ - \frac{2}{x^3} y’ - \frac{b}{x^3} y = 0 \quad \text{or} \quad x^2 y’’ - 2x y’ - 3xy = 0. $$ This is my ODE if the $y’$ were instead a $y$ . :-( So, I am stuck.","I can solve the following differential equation without any trouble using the method of Frobenius When I put the differential equation in Mathematica, it gives me the solutions in terms of modified Bessel functions of order 3 I cannot for the life of me see how to put the given ODE into the form of a modified Bessel equation. Can anyone point me in the right direction? Some Added Information Generally, the equation can be transformed to a Bessel equation with solution where is any Bessel function solution of the transformed equation, if we choose Considering the equations for and , I can see from Mathematica’s solution that and , so I can solve them to find and . Putting those into the ODE above, we get Expanding this out, I get This is my ODE if the were instead a . :-( So, I am stuck.","
x^2 y’’ - (2 + 3x) y = 0.
 
y(x) = A \sqrt{x} I_3\big(2\sqrt{3x}\big) + B \sqrt{x} K_3\big(2\sqrt{3x}\big).
 
\frac{d}{dx}\left(x^a \frac{dy}{dx}\right) + b x^c y = 0,
 
y(x) = x^{\nu/\alpha} Z_\nu \left(\alpha\sqrt{|b|} x^{1/\alpha}\right),
 Z_\nu 
\alpha = \frac{2}{c-a+2} \quad \text{and} \quad \nu = \frac{1-a}{c-a+2},
 \alpha \nu \nu = 3 \alpha = 2 a = -2 c = -3 
\frac{d}{dx}\left(x^{-2} \frac{dy}{dx}\right) + b x^{-3} y = 0.
 
\frac{1}{x^2}y’’ - \frac{2}{x^3} y’ - \frac{b}{x^3} y = 0 \quad \text{or} \quad x^2 y’’ - 2x y’ - 3xy = 0.
 y’ y","['ordinary-differential-equations', 'bessel-functions', 'frobenius-method']"
78,"Does $dx/dt=\{1$ if $x\notin \mathbb Q$, $0$ otherwise$\}$ have solutions?","Does  if ,  otherwise have solutions?",dx/dt=\{1 x\notin \mathbb Q 0 \},"Consider the following system: \begin{align*}  \dot{x} & = f(x)\\ x(0) & = x_0, \end{align*} where $f(x) = \begin{cases} 1 & \text{if } x \notin \Bbb Q \\ 0 & \text{if } x \in \Bbb Q \end{cases}$ . Do there exist solutions? I think the following is a solution: $$ x(t) = x_0 + t, $$ because it satisfies the Lebesgue integral equation $$ x(t) = x_0 + \int_0^t f(x(\tau))\, d\mu $$ In particular, $$ \begin{aligned} x_0 + \int_0^t f(x(\tau)) \,d\mu & = x_0 + \int_{[0,t]\cap\Bbb Q} f(x(\tau))\, d\mu + \int_{[0,t]\setminus\Bbb Q} f(x(\tau))\, d\mu \\ & = x_0 + \int_{[0,t]\setminus\Bbb Q} d\mu \\ & = x_0 + t; \end{aligned} $$ This is because $\Bbb Q$ has measure zero... right?","Consider the following system: where . Do there exist solutions? I think the following is a solution: because it satisfies the Lebesgue integral equation In particular, This is because has measure zero... right?","\begin{align*} 
\dot{x} & = f(x)\\
x(0) & = x_0,
\end{align*} f(x) = \begin{cases}
1 & \text{if } x \notin \Bbb Q \\
0 & \text{if } x \in \Bbb Q \end{cases}  x(t) = x_0 + t,   x(t) = x_0 + \int_0^t f(x(\tau))\, d\mu   \begin{aligned}
x_0 + \int_0^t f(x(\tau)) \,d\mu
& = x_0 + \int_{[0,t]\cap\Bbb Q} f(x(\tau))\, d\mu + \int_{[0,t]\setminus\Bbb Q} f(x(\tau))\, d\mu \\
& = x_0 + \int_{[0,t]\setminus\Bbb Q} d\mu \\
& = x_0 + t;
\end{aligned}  \Bbb Q",['ordinary-differential-equations']
79,linearly independent solution to second order ODE.,linearly independent solution to second order ODE.,,Let $y(t)$ be a nontrivial solution for the second order differential equation $\ddot{x}+a(t)\dot{x}+b(t)x=0$ to determine a solution that is linearly independent from $y$ we set $z(t)=y(t)v(t)$ . Show that this leads to a first order differential equation for $\dot{v}=w$ What do they even mean with linearly independent from $y$ ? Is it meant in the sense that the solution shouldn't be just different by a constant from $y$ like in linear algebra ? Also how would I go on showing that this leads to a solution of first order ? I'm kind of lacking any idea where to start.,Let be a nontrivial solution for the second order differential equation to determine a solution that is linearly independent from we set . Show that this leads to a first order differential equation for What do they even mean with linearly independent from ? Is it meant in the sense that the solution shouldn't be just different by a constant from like in linear algebra ? Also how would I go on showing that this leads to a solution of first order ? I'm kind of lacking any idea where to start.,y(t) \ddot{x}+a(t)\dot{x}+b(t)x=0 y z(t)=y(t)v(t) \dot{v}=w y y,"['calculus', 'ordinary-differential-equations', 'proof-writing']"
80,Find function $f$ such that $(p*f)(x) = xf(x)$,Find function  such that,f (p*f)(x) = xf(x),"Question: The function $p(x)$ is defined as $p(x) = e^{-x}$ for $x>0$ and $p(x) = 0$ for $x<0$ .   Find the Fourier Transform of $p(x)$ and use the convolution theorem to find $f(x)$ such that: $$\int_0^{\infty} p(y)f(x-y) dy = xf(x)$$ My attempt: I have found $\hat{p}(k) = \frac{1}{1 + ik}$ . I took the convolution of both sides and obtained the ODE $$\hat{f}(k) = i(1 + ik)\hat{f}'(k)$$ which I solved to yield $$\hat{f}(k) = \frac{1}{A(1 + ik)}$$ where $A$ is an arbitrary constant. After inverting this using the expression for $\hat{p}(k)$ earlier, and substituting into the original question, I get the left hand side integral diverging to infinity. I cannot find my mistake and am unsure how to progress. Could someone please point me in the right direction?Thank you.","Question: The function is defined as for and for .   Find the Fourier Transform of and use the convolution theorem to find such that: My attempt: I have found . I took the convolution of both sides and obtained the ODE which I solved to yield where is an arbitrary constant. After inverting this using the expression for earlier, and substituting into the original question, I get the left hand side integral diverging to infinity. I cannot find my mistake and am unsure how to progress. Could someone please point me in the right direction?Thank you.",p(x) p(x) = e^{-x} x>0 p(x) = 0 x<0 p(x) f(x) \int_0^{\infty} p(y)f(x-y) dy = xf(x) \hat{p}(k) = \frac{1}{1 + ik} \hat{f}(k) = i(1 + ik)\hat{f}'(k) \hat{f}(k) = \frac{1}{A(1 + ik)} A \hat{p}(k),"['ordinary-differential-equations', 'fourier-analysis', 'fourier-transform', 'convolution']"
81,Find the general solution of $4y′′+ 3y′−y=e^{−x}+x$,Find the general solution of,4y′′+ 3y′−y=e^{−x}+x,"Given $$4y′′+ 3y′−y = e^{−x} + x$$ Find the general solution I identified this as a non-homogeneous linear equation so I assumed that my solution, $$ y = y_h  + y_p$$ $$ 4\lambda^2 + 3\lambda - 1 = 0$$ $$ \lambda = -1 \quad\text{and}\quad \lambda =1/4$$ $$ y_h  = Ae^{x/4} + Be^{-x}$$ However, I am not too sure what should I do to find $y_p$ I do know that my $G(x) = e^{-x} + x$ but am not too sure how to find $y_p$ .","Given Find the general solution I identified this as a non-homogeneous linear equation so I assumed that my solution, However, I am not too sure what should I do to find I do know that my but am not too sure how to find .",4y′′+ 3y′−y = e^{−x} + x  y = y_h  + y_p  4\lambda^2 + 3\lambda - 1 = 0  \lambda = -1 \quad\text{and}\quad \lambda =1/4  y_h  = Ae^{x/4} + Be^{-x} y_p G(x) = e^{-x} + x y_p,"['calculus', 'ordinary-differential-equations']"
82,On the smooth dependence on initial conditions for the following ODE,On the smooth dependence on initial conditions for the following ODE,,"Consider the following ODE: $$ \frac{d^2}{dx^2}f(x)+\frac{3}{2x}f(x)+x^{7/2}\sin f(x) =0\,. $$ This seems to have a non-continuous dependence on its initial conditions. For example, choosing \begin{align} f(0.1) &= \lambda \,, \\ f'(0.1) &= \mu \quad \,, \end{align} and letting $\lambda \le 2.21918, \mu = 1$ , produces the following solution and letting $\lambda \ge 2.21919, \mu = 1$ , completely changes the form of the solution. The critical point (for example, $\lambda = 2.21918$ ) depends on the choice of $\mu$ (in this example, $\mu = 1$ ). I would like to know what is responsible for this behaviour, and in generic situations, how we can predict whether a given ODE has smooth dependence on its initial conditions or not.","Consider the following ODE: This seems to have a non-continuous dependence on its initial conditions. For example, choosing and letting , produces the following solution and letting , completely changes the form of the solution. The critical point (for example, ) depends on the choice of (in this example, ). I would like to know what is responsible for this behaviour, and in generic situations, how we can predict whether a given ODE has smooth dependence on its initial conditions or not.","
\frac{d^2}{dx^2}f(x)+\frac{3}{2x}f(x)+x^{7/2}\sin f(x) =0\,.
 \begin{align}
f(0.1) &= \lambda \,, \\
f'(0.1) &= \mu \quad \,,
\end{align} \lambda \le 2.21918, \mu = 1 \lambda \ge 2.21919, \mu = 1 \lambda = 2.21918 \mu \mu = 1","['real-analysis', 'ordinary-differential-equations', 'continuity', 'initial-value-problems']"
83,Deriving the central Euler method and intuition,Deriving the central Euler method and intuition,,"My professor (Dutch) asked us to determine, among other things, the truncation error of the central Euler method. First of all, this is probably not the correct term, since there are very few results for ""Central Euler"", so that made looking things up a hassle. To determine the truncation error, I thought I had to first know how to derive this central Euler method, given by: $$ u_{k+1}=u_{k-1} + 2h\cdot f(u_k) $$ (It also states $u_0$ and $u_1$ are given/known) In which $f(x)$ gives the slope at point $x$ (I think). I tried to derive this using Taylor expansions, but I didn't get close. I also thought I had to get an intuitive notion of what this means, and I figured out it's this: We add two times the slope (times the step size) at $x=u_k$ to the the y-coordinate at $u_{k-1}$ to get the y coordinate at $x=u_{k+1}$ . The two times is because the length between $k-1$ and $k+1$ is equal to $2h$ . So my question is: How do I derive this method, what's it called, and is the derivation a good step in figuring out the truncation error? Note: It was given that the truncation error is of order $h^3$ .","My professor (Dutch) asked us to determine, among other things, the truncation error of the central Euler method. First of all, this is probably not the correct term, since there are very few results for ""Central Euler"", so that made looking things up a hassle. To determine the truncation error, I thought I had to first know how to derive this central Euler method, given by: (It also states and are given/known) In which gives the slope at point (I think). I tried to derive this using Taylor expansions, but I didn't get close. I also thought I had to get an intuitive notion of what this means, and I figured out it's this: We add two times the slope (times the step size) at to the the y-coordinate at to get the y coordinate at . The two times is because the length between and is equal to . So my question is: How do I derive this method, what's it called, and is the derivation a good step in figuring out the truncation error? Note: It was given that the truncation error is of order .","
u_{k+1}=u_{k-1} + 2h\cdot f(u_k)
 u_0 u_1 f(x) x x=u_k u_{k-1} x=u_{k+1} k-1 k+1 2h h^3","['ordinary-differential-equations', 'numerical-methods', 'truncation-error']"
84,Empirical error proof Runge-Kutta algorithm when not knowing exact solution,Empirical error proof Runge-Kutta algorithm when not knowing exact solution,,"I'm implementing a RK solver for calculating the solution to the Lorenz system: \begin{equation} \begin{cases} x'(t) = \sigma(y-x) \\ y'(t) = rx-y-z \\ z'(t) = xy-bz \end{cases} \end{equation} The implemented RK method is of order 3 with some random Butcher tableau (but satisfies the conditions neccesary for it to have global error of order $\mathcal{O}(h^k)$ , for $k$ the order of the method and local truncation error of $\mathcal{O}(h^{k+1})$ ). One of the questions I'm being asked is to prove empirically that my implemented RK achieves these orders of error. My first attempt was to estimate the local error ( $\delta_L$ ) as: $$\delta_L(t_n) = x_n(t_n)-x_n(t_n-1)$$ Where $x_n$ is the numerical solution of $x$ and $t_n$ is the time step. Following this approach, I get a very oscillatory error, ranging from $-2.5$ to 2.5 with mean $0.0012$ , suggesting that the way I calculate the error must not be appropiate.","I'm implementing a RK solver for calculating the solution to the Lorenz system: The implemented RK method is of order 3 with some random Butcher tableau (but satisfies the conditions neccesary for it to have global error of order , for the order of the method and local truncation error of ). One of the questions I'm being asked is to prove empirically that my implemented RK achieves these orders of error. My first attempt was to estimate the local error ( ) as: Where is the numerical solution of and is the time step. Following this approach, I get a very oscillatory error, ranging from to 2.5 with mean , suggesting that the way I calculate the error must not be appropiate.","\begin{equation}
\begin{cases}
x'(t) = \sigma(y-x) \\
y'(t) = rx-y-z \\
z'(t) = xy-bz
\end{cases}
\end{equation} \mathcal{O}(h^k) k \mathcal{O}(h^{k+1}) \delta_L \delta_L(t_n) = x_n(t_n)-x_n(t_n-1) x_n x t_n -2.5 0.0012","['ordinary-differential-equations', 'numerical-methods', 'runge-kutta-methods']"
85,Separation of Variables in Integration. Why is it necessary?,Separation of Variables in Integration. Why is it necessary?,,"I'm quite a novice with differential equations and such and english isn't my first language so I ask to please explain everything in simple english terms. I am currently studying differentiation equations in school and one of the tricks is ""separation of variables"". The trick is very simple and I get how its used. You replace $y'$ with $dy/dx$ and juggle the equation around until all the $y$ 's are on one side and all the $x$ 's are on the other. Then you can integrate However what I do NOT understand is how this is necessary in a mathematical sense. $y'$ is not $y$ . It is the function of change for $y$ . So why can't I just integrate right away? I know it won't work but I don't get why it wouldn't. Let's say we got the following function: $$y' = -\frac{x}{y}$$ I understand that if I don't separate the variables that $y$ will remain in the solution so essentially the solution of $y$ will have $y$ in it, but in a mathematical sense I still don't get the connection. Is it really just a cheap trick or is there actual logic behind whats happening here other than just ""doing it to get rid of $y$ "".","I'm quite a novice with differential equations and such and english isn't my first language so I ask to please explain everything in simple english terms. I am currently studying differentiation equations in school and one of the tricks is ""separation of variables"". The trick is very simple and I get how its used. You replace with and juggle the equation around until all the 's are on one side and all the 's are on the other. Then you can integrate However what I do NOT understand is how this is necessary in a mathematical sense. is not . It is the function of change for . So why can't I just integrate right away? I know it won't work but I don't get why it wouldn't. Let's say we got the following function: I understand that if I don't separate the variables that will remain in the solution so essentially the solution of will have in it, but in a mathematical sense I still don't get the connection. Is it really just a cheap trick or is there actual logic behind whats happening here other than just ""doing it to get rid of "".",y' dy/dx y x y' y y y' = -\frac{x}{y} y y y y,"['integration', 'ordinary-differential-equations']"
86,Resolution of first order differential equation,Resolution of first order differential equation,,"I have difficulties to solve these two differential equations: 1) $ y'(x)=\frac{x-y(x)}{x+y(x)} $ with the initial condition $ y(1)=1 $ .I'm arrived to prove that $$ y=x(\sqrt{2-e^{-2(\ln x+c)}}-1) $$ but I don't know if it's correct. If it's right, how do I find the constant $ c $ ? Because WolframAlpha says that the solution is $ y(x)=\sqrt{2}\sqrt{x^2+1}-x $ . 2) $ y'(x)=\frac{2y(x)-x}{2x-y(x)} $ . I'm arrived to prove that $ \frac{z-1}{(z+1)^3}=e^{2c}x^{2} $ but I don't know if it's correct. If it's right, how do I explain $ z $ to substitute it in $ y=xz $ ? Then, how do I find the constant $ c $ ? Thanks for any help!","I have difficulties to solve these two differential equations: 1) with the initial condition .I'm arrived to prove that but I don't know if it's correct. If it's right, how do I find the constant ? Because WolframAlpha says that the solution is . 2) . I'm arrived to prove that but I don't know if it's correct. If it's right, how do I explain to substitute it in ? Then, how do I find the constant ? Thanks for any help!", y'(x)=\frac{x-y(x)}{x+y(x)}   y(1)=1   y=x(\sqrt{2-e^{-2(\ln x+c)}}-1)   c   y(x)=\sqrt{2}\sqrt{x^2+1}-x   y'(x)=\frac{2y(x)-x}{2x-y(x)}   \frac{z-1}{(z+1)^3}=e^{2c}x^{2}   z   y=xz   c ,"['integration', 'ordinary-differential-equations', 'indefinite-integrals']"
87,Roots of the Indicial Polynomial of the Legendre equation $(1-z^2)u''-2zu'+v(v+1)u=0$,Roots of the Indicial Polynomial of the Legendre equation,(1-z^2)u''-2zu'+v(v+1)u=0,"Consider the Legendre equation $$(1-z^2)u''-2zu'+v(v+1)u=0.$$ Find the roots of the indicial polynomial if we apply the Frobenius method about $z=1$. My attempt: Let \begin{align}u=\sum_{k=0}^{\infty} A_k(z-1)^{k+r}&\implies u'=\sum_{k=0}^{\infty} (k+r) A_k(z-1)^{k+r-1}\\  &\implies u''=(k+r)(k+r-1)\sum_{k=0}^{\infty} A_k(z-1)^{k+r-2} \end{align} Substituting this into the Legendre equation, I get: $$\sum_{k=-2}^{\infty} (k+r+1)(k+r+2)A_{k+2}(z-1)^{k+r}+\sum_{k=0}^{\infty} \left((v^2+v)-(k+r)(k+r-1)-2(k+r)\right)A_k(z-1)^{k+r}=0$$ Now what do I do? edit I have found the roots of the indicial polynomial are $r=0, 1$, which contradicts the answer provided, which states that $0$ is a repeated root. Any advice would be greatly appreciated.","Consider the Legendre equation $$(1-z^2)u''-2zu'+v(v+1)u=0.$$ Find the roots of the indicial polynomial if we apply the Frobenius method about $z=1$. My attempt: Let \begin{align}u=\sum_{k=0}^{\infty} A_k(z-1)^{k+r}&\implies u'=\sum_{k=0}^{\infty} (k+r) A_k(z-1)^{k+r-1}\\  &\implies u''=(k+r)(k+r-1)\sum_{k=0}^{\infty} A_k(z-1)^{k+r-2} \end{align} Substituting this into the Legendre equation, I get: $$\sum_{k=-2}^{\infty} (k+r+1)(k+r+2)A_{k+2}(z-1)^{k+r}+\sum_{k=0}^{\infty} \left((v^2+v)-(k+r)(k+r-1)-2(k+r)\right)A_k(z-1)^{k+r}=0$$ Now what do I do? edit I have found the roots of the indicial polynomial are $r=0, 1$, which contradicts the answer provided, which states that $0$ is a repeated root. Any advice would be greatly appreciated.",,['ordinary-differential-equations']
88,"Qualitative study of $ \dot x = - y^2$, $\dot y= x^2 $","Qualitative study of ,", \dot x = - y^2 \dot y= x^2 ,"Make the qualitative study of the ODE on the plane $$ \begin{cases}\dot x = - y^2\\ \dot y= x^2 \end{cases} $$ and determine how many solutions satisfy $y(0) = x(1) = 0$. My attempt. Let $f(x,y):=\begin{pmatrix}-y^2\\x^2 \end{pmatrix}$ be the vectorial field. It is well defined on all plane and $C^1$, then every Cauchy Problem has a unique local solution and every one of these solution can be extended in unique way to a maximal solution (not necessary globally defined). Now I note that the function $$E(x,y):=\frac{x^3}{3}+\frac{y^3}{3}$$ is a constant of motion, i.e. $\dot E(x(t),y(t))=c$, where $(x(t),y(t))$ is a solution. The orbits are the level sets of $E$, that are open curves like this: https://m.wolframalpha.com/input/?i=plot+x%5E3+%2B+y%5E3+%3D+10 , or https://m.wolframalpha.com/input/?i=plot+x%5E3+%2B+y%5E3+%3D+-10 . It is all correct? Do you think that it is enough ? The task ""Make the qualitative study"" is very generic. What would you add? I don't know how to determine how many solutions satisfy $y(0) = x(1) = 0$.","Make the qualitative study of the ODE on the plane $$ \begin{cases}\dot x = - y^2\\ \dot y= x^2 \end{cases} $$ and determine how many solutions satisfy $y(0) = x(1) = 0$. My attempt. Let $f(x,y):=\begin{pmatrix}-y^2\\x^2 \end{pmatrix}$ be the vectorial field. It is well defined on all plane and $C^1$, then every Cauchy Problem has a unique local solution and every one of these solution can be extended in unique way to a maximal solution (not necessary globally defined). Now I note that the function $$E(x,y):=\frac{x^3}{3}+\frac{y^3}{3}$$ is a constant of motion, i.e. $\dot E(x(t),y(t))=c$, where $(x(t),y(t))$ is a solution. The orbits are the level sets of $E$, that are open curves like this: https://m.wolframalpha.com/input/?i=plot+x%5E3+%2B+y%5E3+%3D+10 , or https://m.wolframalpha.com/input/?i=plot+x%5E3+%2B+y%5E3+%3D+-10 . It is all correct? Do you think that it is enough ? The task ""Make the qualitative study"" is very generic. What would you add? I don't know how to determine how many solutions satisfy $y(0) = x(1) = 0$.",,"['ordinary-differential-equations', 'proof-verification', 'dynamical-systems']"
89,Understanding Galerkin method of weighted residuals,Understanding Galerkin method of weighted residuals,,"I have a puzzlement regarding the Galerkin method of weighted residuals. The following is taken from the book A Finite Element Primer for Beginners , from chapter 1.1. If I have a one dimensional differential equation $A(u)=f$, and an approximate solution $U^N = \sum_{i=1}^N a_i \phi_i(x) $, and the residual $r^N = A(u^N)-f$. The Galerkin method is to enforce that each of the individual approximation functions $\phi_i$ will be orthogonal to the residual $r^N$. So in mathematical formulation is reads: $$ \int_0^L r^N (x) a_i \phi_i(x) dx = a_i \int_0^L r^N (x)  \phi_i(x) dx =0    \Rightarrow \int_0^L r^N (x)  \phi_i(x) dx =0 \, .$$ Then, in the above equation we have to solve $N$ equations for $N$ unknowns, to find the $a_i$. But if $a_i$ are canceled here, how do I solve for them?","I have a puzzlement regarding the Galerkin method of weighted residuals. The following is taken from the book A Finite Element Primer for Beginners , from chapter 1.1. If I have a one dimensional differential equation $A(u)=f$, and an approximate solution $U^N = \sum_{i=1}^N a_i \phi_i(x) $, and the residual $r^N = A(u^N)-f$. The Galerkin method is to enforce that each of the individual approximation functions $\phi_i$ will be orthogonal to the residual $r^N$. So in mathematical formulation is reads: $$ \int_0^L r^N (x) a_i \phi_i(x) dx = a_i \int_0^L r^N (x)  \phi_i(x) dx =0    \Rightarrow \int_0^L r^N (x)  \phi_i(x) dx =0 \, .$$ Then, in the above equation we have to solve $N$ equations for $N$ unknowns, to find the $a_i$. But if $a_i$ are canceled here, how do I solve for them?",,"['ordinary-differential-equations', 'numerical-methods', 'finite-element-method', 'galerkin-methods', 'weighted-least-squares']"
90,How do I factorize this exact differential?,How do I factorize this exact differential?,,I know for a fact that $$(x+y)dx + (x-y)dy$$ is an exact differential since their partial derivatives are the same (both equates to 1). How do I find that $df$ that could capture this entire equation? I can't see how since there is a $-1$ to the $y$. My closest guess is $\frac{1}{2}(x+y)^2$ but I cannot get the negative $y$.,I know for a fact that $$(x+y)dx + (x-y)dy$$ is an exact differential since their partial derivatives are the same (both equates to 1). How do I find that $df$ that could capture this entire equation? I can't see how since there is a $-1$ to the $y$. My closest guess is $\frac{1}{2}(x+y)^2$ but I cannot get the negative $y$.,,"['ordinary-differential-equations', 'derivatives', 'partial-differential-equations']"
91,"Lyapunov function and an open disk inside the basin of $(0,0)$",Lyapunov function and an open disk inside the basin of,"(0,0)","a)Find a strict Lyapunov function for the equilibrium point $(0,0)$ of $$x'=-2x-y^2$$ $$y'=-y-x^2$$. b)Find $\delta>0$ as large as possible so that the open disk of radius $\delta$ and center $(0,0)$ is contained in the basin of $(0,0)$ Solution a) is done. Consider the Lyapunov function $L(x,y)=x^2+y^2$. $L'(x,y)=-2x^2(2+y)-2y^2(1+x)$ is strictly negative when x and y are near from zero. Switching into polar coordinates: $L'=-2r^3(1+cos^2\theta)[\frac{1}{r}+\frac{cos\theta sin\theta(cos\theta+sin\theta)}{1+cos^2\theta}]$ Now since $\color{blue}{\frac{cos\theta sin\theta(cos\theta+sin\theta)}{1+cos^2\theta}>-\frac{1}{2},}$ as long as $r<2$ the quantity of the brackets is positive. Thus $L'<0$ is in the open disk of radius 2 with center (0,0). Moreover, there are $\color{blue}{no}$ solutions on which $L$ is constant except for the equilibrium at $(0, 0). $ This implies, by the Lasalle invariance principle, that the circle of radius 2, centered at the origin is contained in the basin of attraction. To say no solutions on which L is constant means that there are no other equilibrium points , i.e the only point is $(0,0)$? I don't understand the blue part of the solution and also the italic text. Any kind of help is greatly appreciated.😊","a)Find a strict Lyapunov function for the equilibrium point $(0,0)$ of $$x'=-2x-y^2$$ $$y'=-y-x^2$$. b)Find $\delta>0$ as large as possible so that the open disk of radius $\delta$ and center $(0,0)$ is contained in the basin of $(0,0)$ Solution a) is done. Consider the Lyapunov function $L(x,y)=x^2+y^2$. $L'(x,y)=-2x^2(2+y)-2y^2(1+x)$ is strictly negative when x and y are near from zero. Switching into polar coordinates: $L'=-2r^3(1+cos^2\theta)[\frac{1}{r}+\frac{cos\theta sin\theta(cos\theta+sin\theta)}{1+cos^2\theta}]$ Now since $\color{blue}{\frac{cos\theta sin\theta(cos\theta+sin\theta)}{1+cos^2\theta}>-\frac{1}{2},}$ as long as $r<2$ the quantity of the brackets is positive. Thus $L'<0$ is in the open disk of radius 2 with center (0,0). Moreover, there are $\color{blue}{no}$ solutions on which $L$ is constant except for the equilibrium at $(0, 0). $ This implies, by the Lasalle invariance principle, that the circle of radius 2, centered at the origin is contained in the basin of attraction. To say no solutions on which L is constant means that there are no other equilibrium points , i.e the only point is $(0,0)$? I don't understand the blue part of the solution and also the italic text. Any kind of help is greatly appreciated.😊",,"['ordinary-differential-equations', 'dynamical-systems', 'stability-in-odes', 'lyapunov-functions', 'basins-of-attraction']"
92,$x''+(x^2+2x'^2-1)x'+x=0$ has a non trivial periodic solution.,has a non trivial periodic solution.,x''+(x^2+2x'^2-1)x'+x=0,"I'm trying to prove that $x''+(x^2+2x'^2-1)x'+x=0$ has a non trivial periodic solution. I've written it as a 2x2 system and found that the only equilibrium point $(0,0)$ is strictly unstable. Therefore I'm trying to find a invariant set that contains the origin so I can apply Poincaré-Bendixson, but I haven't had any luck. Any suggestions? also, do you know of a better way to prove this? Thank you!","I'm trying to prove that $x''+(x^2+2x'^2-1)x'+x=0$ has a non trivial periodic solution. I've written it as a 2x2 system and found that the only equilibrium point $(0,0)$ is strictly unstable. Therefore I'm trying to find a invariant set that contains the origin so I can apply Poincaré-Bendixson, but I haven't had any luck. Any suggestions? also, do you know of a better way to prove this? Thank you!",,"['ordinary-differential-equations', 'dynamical-systems']"
93,Differentiable functions $f'(x)=f(-x)^4f(x)$,Differentiable functions,f'(x)=f(-x)^4f(x),"Find all differentiable functions $f\colon \mathbb{R}\to\mathbb{R}$ with $f(0)=1$ such that $f'(x)=f(-x)^4f(x)$, for all $x \in \mathbb{R}$.","Find all differentiable functions $f\colon \mathbb{R}\to\mathbb{R}$ with $f(0)=1$ such that $f'(x)=f(-x)^4f(x)$, for all $x \in \mathbb{R}$.",,"['real-analysis', 'ordinary-differential-equations']"
94,differential equations solve for exponents,differential equations solve for exponents,,"Consider $x' = A(t)x$, $x \in \mathbb{R}^n$ where $A$ is $2\pi$-periodic. $$A(t) = \begin{bmatrix} 1+\sin(t)&0&0\\ 0&3&4\\ 0&1&3\end{bmatrix}$$ The question is to find the Floquet exponents and it also asked to find Lyapunov exponents. I am kind of new to ordinary differential equations, and it would be very nice to show step by steps to solve this problem to firmly understand the concept.","Consider $x' = A(t)x$, $x \in \mathbb{R}^n$ where $A$ is $2\pi$-periodic. $$A(t) = \begin{bmatrix} 1+\sin(t)&0&0\\ 0&3&4\\ 0&1&3\end{bmatrix}$$ The question is to find the Floquet exponents and it also asked to find Lyapunov exponents. I am kind of new to ordinary differential equations, and it would be very nice to show step by steps to solve this problem to firmly understand the concept.",,['ordinary-differential-equations']
95,First-order linear differential inequality [closed],First-order linear differential inequality [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Let $x$ be a function of  $C^1(I,R)$ where $I\subset \mathbb{R}$ , such that  $$x'(t)\leq a(t) x(t)+b(t),$$ where $a$ and $b$ are continuous functions on  $I$ in $R$ then    $$ x(t)\leq x(t_0) \exp\left(\int_{t_0}^{t}a(s)ds\right)+\int_{t_0}^{t}\exp\left(\int_{s}^t a(\sigma)d\sigma\right)b(s)ds$$ How to prove this proposition please ? Thank you","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Let $x$ be a function of  $C^1(I,R)$ where $I\subset \mathbb{R}$ , such that  $$x'(t)\leq a(t) x(t)+b(t),$$ where $a$ and $b$ are continuous functions on  $I$ in $R$ then    $$ x(t)\leq x(t_0) \exp\left(\int_{t_0}^{t}a(s)ds\right)+\int_{t_0}^{t}\exp\left(\int_{s}^t a(\sigma)d\sigma\right)b(s)ds$$ How to prove this proposition please ? Thank you",,"['real-analysis', 'ordinary-differential-equations']"
96,Regular and irregular points,Regular and irregular points,,"In the following 2 examples, I am trying to find all the singular points of the given equations and determine whether each one is regular or irregular I can determine the singular points of the equations but I am having some trouble determining if they are regular or irregular. 1)  $$x^2 (1-x^2)y'' + \frac {2}{x}y'+4y =0$$ The singular points of the equation are $x=\pm 1, 0$ The answer is that $0$ is irregular and $\pm 1$ regular but I am not sure why 2) $$xy'' + (1-x)y' +xy =0$$ The singular point is $0$ but why is this considered regular? Do I have to consider the $p (x)$ term in each equation?","In the following 2 examples, I am trying to find all the singular points of the given equations and determine whether each one is regular or irregular I can determine the singular points of the equations but I am having some trouble determining if they are regular or irregular. 1)  $$x^2 (1-x^2)y'' + \frac {2}{x}y'+4y =0$$ The singular points of the equation are $x=\pm 1, 0$ The answer is that $0$ is irregular and $\pm 1$ regular but I am not sure why 2) $$xy'' + (1-x)y' +xy =0$$ The singular point is $0$ but why is this considered regular? Do I have to consider the $p (x)$ term in each equation?",,['ordinary-differential-equations']
97,Differential Equation with Delta Dirac,Differential Equation with Delta Dirac,,"This is my first question, and it was my last solution, since no article could help me solve this differential equation. The equation is in the following form: $$\dfrac{d^2 f(x)}{dx^2}-Af(x)+B\delta(x-C)f(x) = 0 \quad x \in [0,L]$$ where $$\delta(x-C)= \infty\quad if \quad x=C$$or$$ \delta(x-C)=0 \quad if\quad x\neq C$$ What I'm Asking is the solution of $f(x).$ Ignoring the delta results in Exponential solutions, but delta function makes it difficult to calculate $f(x)$ P.S. : Had Kronecker instead of Dirac, which was TOTALLY wrong, that's why the 1st comments are kind of ""strange"" now.","This is my first question, and it was my last solution, since no article could help me solve this differential equation. The equation is in the following form: $$\dfrac{d^2 f(x)}{dx^2}-Af(x)+B\delta(x-C)f(x) = 0 \quad x \in [0,L]$$ where $$\delta(x-C)= \infty\quad if \quad x=C$$or$$ \delta(x-C)=0 \quad if\quad x\neq C$$ What I'm Asking is the solution of $f(x).$ Ignoring the delta results in Exponential solutions, but delta function makes it difficult to calculate $f(x)$ P.S. : Had Kronecker instead of Dirac, which was TOTALLY wrong, that's why the 1st comments are kind of ""strange"" now.",,"['ordinary-differential-equations', 'dirac-delta']"
98,Fourier Series of $f(x) = \sin x$ on $-\pi \leq x \leq \pi$,Fourier Series of  on,f(x) = \sin x -\pi \leq x \leq \pi,"Find the Fourier series of, $$f(x) = \sin (x)$$ on the interval $- \pi \leq x \leq \pi. $ I am not quite sure if my workings are correct or if I have chose the right formulas to use since this is my first encounter of Fourier series of trigonometric functions. My logic here is that $f(x)$ is a period function so I have to use the formula, $$f(x) = \frac{A_0}{2} + \sum _ {n=1}^{\infty}\Big(A_n\cos(nx) + B_n\sin (nx)\Big)$$ where, $$A_0 = \frac{1}{\pi} \int^{\pi}_{-\pi} f(x)dx$$ $$A_n = \frac{1}{\pi} \int^{\pi}_{-\pi} f(x)\cos (nx)dx$$ $$B_n = \frac{1}{\pi} \int^{\pi}_{-\pi} f(x)\sin (nx)dx$$ Looking at the function $f(x) = \sin x$ this is a odd function so $A_0$ is a odd function, so $A_0=0$. Next looking at $A_n$, $f(x)$ is a odd function at $\cos(nx)$ is a even function, so a even function multiplied by a odd function is a odd function and therefore $A_n = 0 $ Finally considering $B_n$, $$B_n = \frac{1}{\pi } \int^{\pi}_{-\pi} \sin (x) \sin(nx) = \frac{cos(x)sin(nx)-nsin(x)cos(nx)}{n^2-1}\Bigg|_{-\pi}^{\pi} = - \frac{2sin(\pi n )}{n^2-1}$$ Therefore, the Fourier series of $f(x) = \sin x $ on $-\pi \leq x \leq \pi$ is, $$f(x) = \sum_{n=1}^{\infty} - \frac{2sin(\pi n )}{n^2-1}\sin(nx) = \sum _{n=1}^{\infty} \sin x$$","Find the Fourier series of, $$f(x) = \sin (x)$$ on the interval $- \pi \leq x \leq \pi. $ I am not quite sure if my workings are correct or if I have chose the right formulas to use since this is my first encounter of Fourier series of trigonometric functions. My logic here is that $f(x)$ is a period function so I have to use the formula, $$f(x) = \frac{A_0}{2} + \sum _ {n=1}^{\infty}\Big(A_n\cos(nx) + B_n\sin (nx)\Big)$$ where, $$A_0 = \frac{1}{\pi} \int^{\pi}_{-\pi} f(x)dx$$ $$A_n = \frac{1}{\pi} \int^{\pi}_{-\pi} f(x)\cos (nx)dx$$ $$B_n = \frac{1}{\pi} \int^{\pi}_{-\pi} f(x)\sin (nx)dx$$ Looking at the function $f(x) = \sin x$ this is a odd function so $A_0$ is a odd function, so $A_0=0$. Next looking at $A_n$, $f(x)$ is a odd function at $\cos(nx)$ is a even function, so a even function multiplied by a odd function is a odd function and therefore $A_n = 0 $ Finally considering $B_n$, $$B_n = \frac{1}{\pi } \int^{\pi}_{-\pi} \sin (x) \sin(nx) = \frac{cos(x)sin(nx)-nsin(x)cos(nx)}{n^2-1}\Bigg|_{-\pi}^{\pi} = - \frac{2sin(\pi n )}{n^2-1}$$ Therefore, the Fourier series of $f(x) = \sin x $ on $-\pi \leq x \leq \pi$ is, $$f(x) = \sum_{n=1}^{\infty} - \frac{2sin(\pi n )}{n^2-1}\sin(nx) = \sum _{n=1}^{\infty} \sin x$$",,"['ordinary-differential-equations', 'partial-differential-equations', 'fourier-series']"
99,Green's Function / Impulse Response Confusion,Green's Function / Impulse Response Confusion,,"From: https://en.wikipedia.org/wiki/Green%27s_function ""In mathematics, a Green's function is the impulse response of an inhomogeneous linear differential equation defined on a domain, with specified initial conditions or boundary conditions. A Green's function, $G(x,s)$, of a linear differential operator $L = L(x)$ acting on distributions over a subset of the Euclidean space $ℝn$, at a point $s$, is any solution of $$LG(x,s)=\delta(x-s)$$ where $δ$ is the Dirac delta function."" But it seems to me that the impulse response, $I$, would be $$L(\delta(x-s))=I(x,s)$$ Then $I$ and $G$ are not the same.  That is, $I$ is the output from $L$ due to an impulsive input and $G$ is the input to $L$ that produces an impulsive output. My Question: How can the impulse response be the same as the Green's function given these definitions?","From: https://en.wikipedia.org/wiki/Green%27s_function ""In mathematics, a Green's function is the impulse response of an inhomogeneous linear differential equation defined on a domain, with specified initial conditions or boundary conditions. A Green's function, $G(x,s)$, of a linear differential operator $L = L(x)$ acting on distributions over a subset of the Euclidean space $ℝn$, at a point $s$, is any solution of $$LG(x,s)=\delta(x-s)$$ where $δ$ is the Dirac delta function."" But it seems to me that the impulse response, $I$, would be $$L(\delta(x-s))=I(x,s)$$ Then $I$ and $G$ are not the same.  That is, $I$ is the output from $L$ due to an impulsive input and $G$ is the input to $L$ that produces an impulsive output. My Question: How can the impulse response be the same as the Green's function given these definitions?",,"['ordinary-differential-equations', 'linear-transformations', 'greens-function']"
