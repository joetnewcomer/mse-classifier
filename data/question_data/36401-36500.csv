,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Probability problem : A lot of 100 bulbs from a manufacturing process is known to contain 10 defective and 90 non ...,Probability problem : A lot of 100 bulbs from a manufacturing process is known to contain 10 defective and 90 non ...,,"Problem : A lot of 100 bulbs from a manufacturing process is known to contain 10 defective and 90 non defective bulbs. If 8 bulbs are selected at random, what is the probability that there will be at least one defective bulb. Method I : Probability that no bulb will be defective , $P(X =0) $ $$P(0) ={}^8C_0 p^0 q^8 =q^8 = (\frac{9}{10})^8$$ where q probability that a bulb selected is non defective $$\therefore q = 1-p = 1-\frac{1}{10} = \frac{9}{10}$$ and p ( probability of bulb drawn is a defective ) = $$\frac{10}{100} = \frac{1}{10}$$ Now probability that at least one bulb is defective = $$1 -P(0) = 1-(\frac{9}{10})^8$$ Method II : But I want to find the answer in another way, Probability of drawing non defective bulb $$=  \frac{90}{100} \times \frac{89}{99} \times \frac{88}{98} \times...........\times \frac{82}{92} ..........(i)$$ Now the probability of drawing at least one bulb will be defective $$= 1-(i)  = 1-   \frac{90}{100} \times \frac{89}{99} \times \frac{88}{98} \times...........\times \frac{82}{92}$$ But this is not the correct answer ... please suggest the correction here..... thanks...","Problem : A lot of 100 bulbs from a manufacturing process is known to contain 10 defective and 90 non defective bulbs. If 8 bulbs are selected at random, what is the probability that there will be at least one defective bulb. Method I : Probability that no bulb will be defective , $P(X =0) $ $$P(0) ={}^8C_0 p^0 q^8 =q^8 = (\frac{9}{10})^8$$ where q probability that a bulb selected is non defective $$\therefore q = 1-p = 1-\frac{1}{10} = \frac{9}{10}$$ and p ( probability of bulb drawn is a defective ) = $$\frac{10}{100} = \frac{1}{10}$$ Now probability that at least one bulb is defective = $$1 -P(0) = 1-(\frac{9}{10})^8$$ Method II : But I want to find the answer in another way, Probability of drawing non defective bulb $$=  \frac{90}{100} \times \frac{89}{99} \times \frac{88}{98} \times...........\times \frac{82}{92} ..........(i)$$ Now the probability of drawing at least one bulb will be defective $$= 1-(i)  = 1-   \frac{90}{100} \times \frac{89}{99} \times \frac{88}{98} \times...........\times \frac{82}{92}$$ But this is not the correct answer ... please suggest the correction here..... thanks...",,['probability']
1,"Angle between $(X,Y)$ and $(E(X), E(Y)) $ where X and Y are independent random variables.",Angle between  and  where X and Y are independent random variables.,"(X,Y) (E(X), E(Y)) ","Suppose that $X$ and $Y$ are two independent random variables with known (different/same) probability distribution functions. Now consider the vector $(X,Y)$, I want to find the angle between $(X,Y)$ and $(E(X), E(Y))$. I am guessing that again the angle should be a random variable, but I don't know how to find its PDF. In addition, suppose that $E(X)$ and $E(Y)$ are not zero at the same time. Any help would be appreciated, Thanks.","Suppose that $X$ and $Y$ are two independent random variables with known (different/same) probability distribution functions. Now consider the vector $(X,Y)$, I want to find the angle between $(X,Y)$ and $(E(X), E(Y))$. I am guessing that again the angle should be a random variable, but I don't know how to find its PDF. In addition, suppose that $E(X)$ and $E(Y)$ are not zero at the same time. Any help would be appreciated, Thanks.",,"['probability', 'statistics', 'probability-theory']"
2,Interchanging probability and limits,Interchanging probability and limits,,"I am trying to prove that $\mathbb{P}(\cup A_n)=1$ if $\mathbb{P}(\limsup A_n)=1$ when $A_1...A_n$ are independent events and $\mathbb{P}(A_n)<1$. My question is, while trying to prove this, can I interchange the limit and probability and can I do that always or are there conditions when I can't ? $\mathbb{P}(\limsup A_n)=\lim_{n \to \infty}(\mathbb{P}(\cup_{m\geq n} A_m))$ . Also if $\mathbb{P}(A_n)<1$ then why is $\sum \mathbb{P}(A_n) = \infty$ ?","I am trying to prove that $\mathbb{P}(\cup A_n)=1$ if $\mathbb{P}(\limsup A_n)=1$ when $A_1...A_n$ are independent events and $\mathbb{P}(A_n)<1$. My question is, while trying to prove this, can I interchange the limit and probability and can I do that always or are there conditions when I can't ? $\mathbb{P}(\limsup A_n)=\lim_{n \to \infty}(\mathbb{P}(\cup_{m\geq n} A_m))$ . Also if $\mathbb{P}(A_n)<1$ then why is $\sum \mathbb{P}(A_n) = \infty$ ?",,"['probability', 'probability-theory']"
3,Binomial Coefficient Identities,Binomial Coefficient Identities,,"I am trying to simplify the following fraction, which I think is equal to 1 but I am not sure. $$\frac{\frac{\left(\begin{array}{c} b-1\\ k-1 \end{array}\right)\left(\begin{array}{c} r\\ n-k \end{array}\right)}{\left(\begin{array}{c} r+b-1\\ n-1 \end{array}\right)}}{\frac{\left(\begin{array}{c} b\\ k \end{array}\right)\left(\begin{array}{c} r\\ n-k \end{array}\right)}{\left(\begin{array}{c} r+b\\ n \end{array}\right)}}$$ I tried to use the identity  $$\left(\begin{array}{c} n\\ r \end{array}\right)=\left(\begin{array}{c} n-1\\ r-1 \end{array}\right)+\left(\begin{array}{c} n-1\\ r \end{array}\right)  $$ I have done the following Step 1:  $$\frac{\left(\begin{array}{c} b-1\\ k-1 \end{array}\right)\left(\begin{array}{c} r\\ n-k \end{array}\right)}{\left(\begin{array}{c} r+b-1\\ n-1 \end{array}\right)}\cdot\frac{\left(\begin{array}{c} r+b\\ n \end{array}\right)}{\left(\begin{array}{c} b\\ k \end{array}\right)\left(\begin{array}{c} r\\ n-k \end{array}\right)}$$ Step 2: $$\frac{\left(\begin{array}{c} b-1\\ k-1 \end{array}\right)}{\left(\begin{array}{c} b\\ k \end{array}\right)}\cdot\frac{\left(\begin{array}{c} r+b\\ n \end{array}\right)}{\left(\begin{array}{c} r+b-1\\ n-1 \end{array}\right)}$$ From there I get stuck here $$\frac{\left[\left(\begin{array}{c} b\\ k \end{array}\right)-\left(\begin{array}{c} b-1\\ k \end{array}\right)\right]}{\left(\begin{array}{c} b\\ k \end{array}\right)}\cdot\frac{\left(\begin{array}{c} r+b\\ n \end{array}\right)}{\left[\left(\begin{array}{c} r+b\\ n \end{array}\right)-\left(\begin{array}{c} r+b-1\\ n \end{array}\right)\right]}$$ Is there any other identity that would be more useful for this problem? If not does anyone have a useful hint for where to proceed from here?","I am trying to simplify the following fraction, which I think is equal to 1 but I am not sure. $$\frac{\frac{\left(\begin{array}{c} b-1\\ k-1 \end{array}\right)\left(\begin{array}{c} r\\ n-k \end{array}\right)}{\left(\begin{array}{c} r+b-1\\ n-1 \end{array}\right)}}{\frac{\left(\begin{array}{c} b\\ k \end{array}\right)\left(\begin{array}{c} r\\ n-k \end{array}\right)}{\left(\begin{array}{c} r+b\\ n \end{array}\right)}}$$ I tried to use the identity  $$\left(\begin{array}{c} n\\ r \end{array}\right)=\left(\begin{array}{c} n-1\\ r-1 \end{array}\right)+\left(\begin{array}{c} n-1\\ r \end{array}\right)  $$ I have done the following Step 1:  $$\frac{\left(\begin{array}{c} b-1\\ k-1 \end{array}\right)\left(\begin{array}{c} r\\ n-k \end{array}\right)}{\left(\begin{array}{c} r+b-1\\ n-1 \end{array}\right)}\cdot\frac{\left(\begin{array}{c} r+b\\ n \end{array}\right)}{\left(\begin{array}{c} b\\ k \end{array}\right)\left(\begin{array}{c} r\\ n-k \end{array}\right)}$$ Step 2: $$\frac{\left(\begin{array}{c} b-1\\ k-1 \end{array}\right)}{\left(\begin{array}{c} b\\ k \end{array}\right)}\cdot\frac{\left(\begin{array}{c} r+b\\ n \end{array}\right)}{\left(\begin{array}{c} r+b-1\\ n-1 \end{array}\right)}$$ From there I get stuck here $$\frac{\left[\left(\begin{array}{c} b\\ k \end{array}\right)-\left(\begin{array}{c} b-1\\ k \end{array}\right)\right]}{\left(\begin{array}{c} b\\ k \end{array}\right)}\cdot\frac{\left(\begin{array}{c} r+b\\ n \end{array}\right)}{\left[\left(\begin{array}{c} r+b\\ n \end{array}\right)-\left(\begin{array}{c} r+b-1\\ n \end{array}\right)\right]}$$ Is there any other identity that would be more useful for this problem? If not does anyone have a useful hint for where to proceed from here?",,"['probability', 'combinatorics']"
4,Almost sure equal probability,Almost sure equal probability,,"I have a problem that reads: Let X be a random variable such that $X(\omega) = \omega $ and a random variable Y such that $Y(\omega)$ = \begin{cases} \omega,  & \text{if $\omega$ $\neq$ 1/2} \\ 2, & \text{if $\omega$ = 1/2}  \\ \end{cases} Then it provides the answer as P($X(\omega)$ is not equal to $Y(\omega)$) = P(1/2) = 0. P here refers to Lebesgue measure. My question is how P($X(\omega) \neq$ $Y(\omega)$) = P(1/2) = 0 ? need some explanation on this.","I have a problem that reads: Let X be a random variable such that $X(\omega) = \omega $ and a random variable Y such that $Y(\omega)$ = \begin{cases} \omega,  & \text{if $\omega$ $\neq$ 1/2} \\ 2, & \text{if $\omega$ = 1/2}  \\ \end{cases} Then it provides the answer as P($X(\omega)$ is not equal to $Y(\omega)$) = P(1/2) = 0. P here refers to Lebesgue measure. My question is how P($X(\omega) \neq$ $Y(\omega)$) = P(1/2) = 0 ? need some explanation on this.",,"['probability', 'random-variables']"
5,Calculations for a random variable given density function,Calculations for a random variable given density function,,"Studying for a statistics course an stumped on how to go bout solving a problem. The following is a problem I have solved - the problem I'm having trouble with is related to this one: Impurities in the batch of final product of a chemical process often reflect a serious problem. From considerable plant data gathered, it is known that the proportion $y$ of impurities in a batch has a density function given by $$f(y) = 10(1−y)^9,\quad 0 ≤ y ≤ 1.$$ A batch is considered not acceptable if the percentage of impurities exceeds $60\%$ . With the current quality of the process, what is the percentage of batches that are not acceptable? I found this quantity as such: We can take integral $\int_{0.60}^1 10(1-y)^9dy$ and use transformation to see that it is equivalent to the negative integral $\int_{0.4}^0 10x^9dx$ , which, then, is equivalent to $\int_0^{0.4}10x^9\, dx$ . I perform the integration and get the value $$0.00010408576 \quad \text{or} \quad 0.01048576\%~\text{of batches!}$$ That's all fine and good. But what of the follow-up questions? For the random variable Y in the previous problem a. Find the expected percentage of impurities. b. Find the expected value of the proportion of quality material (i.e., $E[1-y]$ ). What is meant by this? Did we not just find the $\%$ of batches impure? What exactly is being asked by a? I imagine once we have a, finding b my be simple. Thoughts? Direction?","Studying for a statistics course an stumped on how to go bout solving a problem. The following is a problem I have solved - the problem I'm having trouble with is related to this one: Impurities in the batch of final product of a chemical process often reflect a serious problem. From considerable plant data gathered, it is known that the proportion of impurities in a batch has a density function given by A batch is considered not acceptable if the percentage of impurities exceeds . With the current quality of the process, what is the percentage of batches that are not acceptable? I found this quantity as such: We can take integral and use transformation to see that it is equivalent to the negative integral , which, then, is equivalent to . I perform the integration and get the value That's all fine and good. But what of the follow-up questions? For the random variable Y in the previous problem a. Find the expected percentage of impurities. b. Find the expected value of the proportion of quality material (i.e., ). What is meant by this? Did we not just find the of batches impure? What exactly is being asked by a? I imagine once we have a, finding b my be simple. Thoughts? Direction?","y f(y) = 10(1−y)^9,\quad 0 ≤ y ≤ 1. 60\% \int_{0.60}^1 10(1-y)^9dy \int_{0.4}^0 10x^9dx \int_0^{0.4}10x^9\, dx 0.00010408576 \quad \text{or} \quad 0.01048576\%~\text{of batches!} E[1-y] \%","['probability', 'statistics', 'functions']"
6,"Is this a theorem in Statistics? If not, can anyone explain why this seems to be true?","Is this a theorem in Statistics? If not, can anyone explain why this seems to be true?",,"It seems to me that the following is true: If $c$ is a random variable with probability $p(c)$ and cumulative probability $P(c)$, then the probability of $P(c)$ is constant. I have taken $N$ random values consistent with a probability function (in my test case it was a simple gaussian $p(x) = \frac{2}{\sqrt{\pi}}Exp(-x^2)$) for which the cumulative PDF is $P(x) = Erf(x)$. When I create the $N$ random variables, I plot the histogram and get back the gaussian which lets me know that the $p(x)$ is correct. Then, for each value of $c$ which I have created, I evaluate $P(c)$ and then plot the histogram of $P(c)$ which seems to be constant (~$1/N$). The more points I create, the closer it becomes a perfectly (less noisy) horizontal line and for deviations from the true $p(c)$, it moves away from a straight line. I have included the matlab code below. Note that as we let $N$ become larger (on line 1), the resulting graph in figure 2 becomes more and more perfectly straight. If we, however, change line 13 and allow for rv = 0.5*rand() , then the resulting $p(c)$ deviates from the true gaussian and that deviation is reflected in figure 2 as well. 1 N = 50000;  2  3 cs = zeros(N,1);  4 maxPx = 0.0;  5 for i = 1:N  6   while(cs(i) == 0)  7     x = 10*(2*rand()-1);  8     Px = (2/sqrt(pi))*exp(-x^2);  9     if(Px > maxPx) 10       maxPx = Px; 11     end 12     rv = (2/sqrt(pi))*rand(); 13 %    rv = 0.5*rand(); 14     if(rv < Px) 15       cs(i) = x; 16     else 17     end 18   end 19 end 20 21 figure(1); 22 [ns, xs] = hist(cs, 100); 23  24 plot(xs, ns, 'k.'); 25 axis([min(xs) max(xs) 0 max(ns)]); 26 27 Ps = zeros(N,1); 28 for i = 1:N 29   x = cs(i); 30   Ps(i) = erf(x); 31 end 32 33 figure(2); 34 [ns, xs] = hist(Ps, 100); 35 plot(xs, ns, 'k.'); 36 axis([min(xs) max(xs) 0 max(ns)]); EDIT: included images (1) taking N = 50,000 The gaussian distribution is: for which the related probability is: (2) Letting N grow larger to 5,000,000 we have The gaussian distribution: and the related probability: (3) Finally, when we change the original function from the gaussian distribution we have for the distribution: and the probability:","It seems to me that the following is true: If $c$ is a random variable with probability $p(c)$ and cumulative probability $P(c)$, then the probability of $P(c)$ is constant. I have taken $N$ random values consistent with a probability function (in my test case it was a simple gaussian $p(x) = \frac{2}{\sqrt{\pi}}Exp(-x^2)$) for which the cumulative PDF is $P(x) = Erf(x)$. When I create the $N$ random variables, I plot the histogram and get back the gaussian which lets me know that the $p(x)$ is correct. Then, for each value of $c$ which I have created, I evaluate $P(c)$ and then plot the histogram of $P(c)$ which seems to be constant (~$1/N$). The more points I create, the closer it becomes a perfectly (less noisy) horizontal line and for deviations from the true $p(c)$, it moves away from a straight line. I have included the matlab code below. Note that as we let $N$ become larger (on line 1), the resulting graph in figure 2 becomes more and more perfectly straight. If we, however, change line 13 and allow for rv = 0.5*rand() , then the resulting $p(c)$ deviates from the true gaussian and that deviation is reflected in figure 2 as well. 1 N = 50000;  2  3 cs = zeros(N,1);  4 maxPx = 0.0;  5 for i = 1:N  6   while(cs(i) == 0)  7     x = 10*(2*rand()-1);  8     Px = (2/sqrt(pi))*exp(-x^2);  9     if(Px > maxPx) 10       maxPx = Px; 11     end 12     rv = (2/sqrt(pi))*rand(); 13 %    rv = 0.5*rand(); 14     if(rv < Px) 15       cs(i) = x; 16     else 17     end 18   end 19 end 20 21 figure(1); 22 [ns, xs] = hist(cs, 100); 23  24 plot(xs, ns, 'k.'); 25 axis([min(xs) max(xs) 0 max(ns)]); 26 27 Ps = zeros(N,1); 28 for i = 1:N 29   x = cs(i); 30   Ps(i) = erf(x); 31 end 32 33 figure(2); 34 [ns, xs] = hist(Ps, 100); 35 plot(xs, ns, 'k.'); 36 axis([min(xs) max(xs) 0 max(ns)]); EDIT: included images (1) taking N = 50,000 The gaussian distribution is: for which the related probability is: (2) Letting N grow larger to 5,000,000 we have The gaussian distribution: and the related probability: (3) Finally, when we change the original function from the gaussian distribution we have for the distribution: and the probability:",,"['probability', 'statistics', 'matlab']"
7,Poisson process and an independent Wiener process,Poisson process and an independent Wiener process,,Let $\{W_t\}_{t\geq 0}$ be a standard one-dimensional Wiener process and $\{N_t\}_{t\geq 0}$ an independent rate-1 Poisson process. Define $T$ to be the first time (if ever) $t \geq 0$ such that $W_t \geq N_t + 1$. I want to find $P\{T < \infty\}$. It is not difficult to find an expression involving expectations of some functions of hitting times of SBM. But I heard from some source that it would be the solution to a transcendental algebraic equation. Any suggestions?,Let $\{W_t\}_{t\geq 0}$ be a standard one-dimensional Wiener process and $\{N_t\}_{t\geq 0}$ an independent rate-1 Poisson process. Define $T$ to be the first time (if ever) $t \geq 0$ such that $W_t \geq N_t + 1$. I want to find $P\{T < \infty\}$. It is not difficult to find an expression involving expectations of some functions of hitting times of SBM. But I heard from some source that it would be the solution to a transcendental algebraic equation. Any suggestions?,,"['probability', 'stochastic-processes']"
8,Expected number of trials until n-th success urn problem without replacement,Expected number of trials until n-th success urn problem without replacement,,"I have a close relative of the urn problem, but with a slight twist. Given an urn containing $r$ red balls and $b$ blue balls, how many balls should I expect to pull out of the urn before I have pulled $d \leq b$ blue balls out of the urn? In the standard version of this problem you are only interested in the number of balls one has to pull out before the FIRST success (and there are a number of places where one can look for that piece of information), but I am instead interested the expected number of balls one has to pull out of the urn before the $n$-th success.","I have a close relative of the urn problem, but with a slight twist. Given an urn containing $r$ red balls and $b$ blue balls, how many balls should I expect to pull out of the urn before I have pulled $d \leq b$ blue balls out of the urn? In the standard version of this problem you are only interested in the number of balls one has to pull out before the FIRST success (and there are a number of places where one can look for that piece of information), but I am instead interested the expected number of balls one has to pull out of the urn before the $n$-th success.",,['probability']
9,Proving that the expectation is always negative,Proving that the expectation is always negative,,"I am interested in if the expected value $$E_{X_3}\left[\log\frac{f_1(y)}{f_2(y)}\right]<0$$ is always be negative or not. Here, I have $3$ random variables $X_1,X_2,X_3$, corresponding to the densities $f_1,f_2,f_3$ with their Cumulative distribution functions satifying $F_1(Y)>F_2(Y)>F_3(Y)$. If I rewrite the expectation, I have $$\int_{\mathbb{R}}f_3(y) \log\frac{f_1(y)}{f_2(y)}\mathrm{d}y$$ If I take the expectation with respect to $X_1$ then I have $$E_{X_1}\left[\log\frac{f_1(y)}{f_2(y)}\right]>0$$ If I take the expectation with respect to $X_2$ then I have $$E_{X_2}\left[\log\frac{f_1(y)}{f_2(y)}\right]<0$$ due to stochastical ordering ${X_3}_{ST}>{X_2}_{ST}>{X_1}_{ST}$ since $F_1(Y)>F_2(Y)>F_3(Y)$ I suppose that I must have $$E_{X_3}\left[\log\frac{f_1(y)}{f_2(y)}\right]<0$$ do you have any ideas how I can prove this? Thanks for your help. Additional Information : If $F_1(Y)>F_2(Y)>F_3(Y)$ then $E[X_3]>E[X_2]>E[X_1]$","I am interested in if the expected value $$E_{X_3}\left[\log\frac{f_1(y)}{f_2(y)}\right]<0$$ is always be negative or not. Here, I have $3$ random variables $X_1,X_2,X_3$, corresponding to the densities $f_1,f_2,f_3$ with their Cumulative distribution functions satifying $F_1(Y)>F_2(Y)>F_3(Y)$. If I rewrite the expectation, I have $$\int_{\mathbb{R}}f_3(y) \log\frac{f_1(y)}{f_2(y)}\mathrm{d}y$$ If I take the expectation with respect to $X_1$ then I have $$E_{X_1}\left[\log\frac{f_1(y)}{f_2(y)}\right]>0$$ If I take the expectation with respect to $X_2$ then I have $$E_{X_2}\left[\log\frac{f_1(y)}{f_2(y)}\right]<0$$ due to stochastical ordering ${X_3}_{ST}>{X_2}_{ST}>{X_1}_{ST}$ since $F_1(Y)>F_2(Y)>F_3(Y)$ I suppose that I must have $$E_{X_3}\left[\log\frac{f_1(y)}{f_2(y)}\right]<0$$ do you have any ideas how I can prove this? Thanks for your help. Additional Information : If $F_1(Y)>F_2(Y)>F_3(Y)$ then $E[X_3]>E[X_2]>E[X_1]$",,"['probability', 'probability-theory', 'probability-distributions']"
10,Calculating the conditional probability Prob(A|B),Calculating the conditional probability Prob(A|B),,"There are 6 women philosophy graduate students in the philosophy   department, 12 women philosophers, and 20 philosophy graduate   students. What is the probability that a philosopher is a graduate   student, given that she is a woman? What is the probability that a   philosopher is a woman, given than (s)he is a graduate student? I am having trouble understanding how to calculate this. For the first question, do I divide 6 (the event of being a philosophy graduate student) into 18 (the event of being a woman)? Or do I divide 20 (the event of being a graduate student) into 18 (the event of being a woman)? For the second question, do I divide 18 (the event of being a woman) into 20 (the event of being a graduate)? Why? Or am I completely wrong and need to calculate the probability of being a woman philosopher (18/38) and the probability of being a grad student (20/38) separately? How would I solve this problem???","There are 6 women philosophy graduate students in the philosophy   department, 12 women philosophers, and 20 philosophy graduate   students. What is the probability that a philosopher is a graduate   student, given that she is a woman? What is the probability that a   philosopher is a woman, given than (s)he is a graduate student? I am having trouble understanding how to calculate this. For the first question, do I divide 6 (the event of being a philosophy graduate student) into 18 (the event of being a woman)? Or do I divide 20 (the event of being a graduate student) into 18 (the event of being a woman)? For the second question, do I divide 18 (the event of being a woman) into 20 (the event of being a graduate)? Why? Or am I completely wrong and need to calculate the probability of being a woman philosopher (18/38) and the probability of being a grad student (20/38) separately? How would I solve this problem???",,"['probability', 'conditional-probability']"
11,Inequality on Shannon's entropy,Inequality on Shannon's entropy,,"Let $P$ be a set of probabilities s.t. $\sum_{p_i \in P} p_i = 1$. Moreover, let $H(P)$ the Shannon's entropy of the set of probabilities $P$: $$ H(P) = -\sum_{p_i \in P} p_i \log_2 p_i $$ I define a set of actions $a_1, \ldots, a_N$ which modify the set of probabilities $P$. Performing an action. When $a_i$ is asked: Some of the probabilities are removed from $P$ $P$ is normalized so that its summation is still 1 In this context, I define $H_{i}(P)$ as the Shannon's entropy of the new set $P$. Performing two actions. When $a_j$ is asked after $a_i$: Some of the probabilities are removed from $P$ as a consequence of $a_i$ $P$ is normalized so that its summation is still 1 Some of the probabilities are removed from the new $P$ as a consequence of $a_j$ $P$ is normalized again The final Shannon's entropy is $H_{j}(P | a_i)$. Proof? Now, suppose I choose two actions $a_1$ and $a_2$ such that:  $$ H_1(P) \leq H_2(P). $$  How to prove whether the following is true (or, if it is not, whether there is a subset of cases in which it is true)? $$ \forall a_i, i \neq 1,2: H_i(P|a_1) \leq H_i(P|a_2) $$","Let $P$ be a set of probabilities s.t. $\sum_{p_i \in P} p_i = 1$. Moreover, let $H(P)$ the Shannon's entropy of the set of probabilities $P$: $$ H(P) = -\sum_{p_i \in P} p_i \log_2 p_i $$ I define a set of actions $a_1, \ldots, a_N$ which modify the set of probabilities $P$. Performing an action. When $a_i$ is asked: Some of the probabilities are removed from $P$ $P$ is normalized so that its summation is still 1 In this context, I define $H_{i}(P)$ as the Shannon's entropy of the new set $P$. Performing two actions. When $a_j$ is asked after $a_i$: Some of the probabilities are removed from $P$ as a consequence of $a_i$ $P$ is normalized so that its summation is still 1 Some of the probabilities are removed from the new $P$ as a consequence of $a_j$ $P$ is normalized again The final Shannon's entropy is $H_{j}(P | a_i)$. Proof? Now, suppose I choose two actions $a_1$ and $a_2$ such that:  $$ H_1(P) \leq H_2(P). $$  How to prove whether the following is true (or, if it is not, whether there is a subset of cases in which it is true)? $$ \forall a_i, i \neq 1,2: H_i(P|a_1) \leq H_i(P|a_2) $$",,"['probability', 'proof-writing', 'entropy']"
12,variance of number of divisors,variance of number of divisors,,"Let $d(i)$ be the number of divisors of $i$. I know that $\frac{1}{n}\sum_{1\le i \le n} d(i)= \ln n+\Theta(1)$ as $n$ grows, this can be seen by asking, for each $j$, how many $i$ are there such that $j$ contributes to $d(i)$. This statement can be interpreted as computing the average number of divisors of a random number between 1 and $n$. I'm wondering how to study $\frac{1}{n}\sum_{1\le i \le n} d(i)^2$, as a way to compute the variance of the number of divisors of a random number between 1 and $n$. This worksheet (see Exercise 2.10) indicates that $\frac{1}{n}\sum_{1\le i \le n} d(i)^2=\Theta(\ln^3 n)$. Is there an easy way to see this?","Let $d(i)$ be the number of divisors of $i$. I know that $\frac{1}{n}\sum_{1\le i \le n} d(i)= \ln n+\Theta(1)$ as $n$ grows, this can be seen by asking, for each $j$, how many $i$ are there such that $j$ contributes to $d(i)$. This statement can be interpreted as computing the average number of divisors of a random number between 1 and $n$. I'm wondering how to study $\frac{1}{n}\sum_{1\le i \le n} d(i)^2$, as a way to compute the variance of the number of divisors of a random number between 1 and $n$. This worksheet (see Exercise 2.10) indicates that $\frac{1}{n}\sum_{1\le i \le n} d(i)^2=\Theta(\ln^3 n)$. Is there an easy way to see this?",,"['probability', 'elementary-number-theory']"
13,Maximum of absolute value of linear combinations with i.i.d random variables,Maximum of absolute value of linear combinations with i.i.d random variables,,"Suppose $x_{1},\dots,x_{n}$ are i.i.d random variables with density $p(x_{i})=exp(-|x_{i}|)/2$. Denote column vector $x=(x_{1},\dots,x_{n})^{T}$ Let $C\in\mathbb{R}^{n\times n}$ be a matrix with unit euclidean norm rows $c_{i}^T$ representing linear combinations. Define $w(C)=\mathbb{E}_{x}\{\max_{j\in[n]}|c_{j}^Tx|\}$ Which $C$ maximizes $w(C)$?","Suppose $x_{1},\dots,x_{n}$ are i.i.d random variables with density $p(x_{i})=exp(-|x_{i}|)/2$. Denote column vector $x=(x_{1},\dots,x_{n})^{T}$ Let $C\in\mathbb{R}^{n\times n}$ be a matrix with unit euclidean norm rows $c_{i}^T$ representing linear combinations. Define $w(C)=\mathbb{E}_{x}\{\max_{j\in[n]}|c_{j}^Tx|\}$ Which $C$ maximizes $w(C)$?",,"['probability', 'statistics', 'optimization', 'random-variables']"
14,How can I approximate $\sum\limits_{k=4}^{\infty}\Pr(X=k)[{\Pr(X\le k)}^6 - {\Pr(X\le k-4)}^6]$ for $\lambda \to +\infty$?,How can I approximate  for ?,\sum\limits_{k=4}^{\infty}\Pr(X=k)[{\Pr(X\le k)}^6 - {\Pr(X\le k-4)}^6] \lambda \to +\infty,"$X$ is a Poisson random variable and the probability mass function is given by: $$\Pr(X = k) = e^{-\lambda}\frac{{\lambda}^k}{k!}$$ I’ve got a probability function $f(\lambda)$ $$f(\lambda) = \sum\limits_{k=4}^{\infty}\Pr(X=k)[{\Pr(X\le k)}^6 - {\Pr(X\le k-4)}^6]$$ To date, I only find that ${\Pr(X\le k)}^6 - {\Pr(X\le k-4)}^6$ can be factorized as  \begin{align*} &{\Pr(X\le k)}^6 - {\Pr(X\le k-4)}^6 \\&= [\Pr(X=k)+ \Pr(X=k-1) + \Pr(X=k-2) + \Pr(X=k-3)]\cdot[ {\Pr(X\le k)}^5+{\Pr(X\le k)}^4{\Pr(X\le k-4)}+…+ {\Pr(X\le k-4)}^5] \end{align*} But I have no idea what to do next… Can I assume that $\Pr(X=k) \approx \Pr(X=k-1)$ if $\lambda \to +\infty$? Are there any better approximation for $f(\lambda)$? If there is a simple expression for $ f(\lambda) $ about $\lambda \to +\infty$ that would be best, but I’m open to whatever can be suggested. Thank you in advance!","$X$ is a Poisson random variable and the probability mass function is given by: $$\Pr(X = k) = e^{-\lambda}\frac{{\lambda}^k}{k!}$$ I’ve got a probability function $f(\lambda)$ $$f(\lambda) = \sum\limits_{k=4}^{\infty}\Pr(X=k)[{\Pr(X\le k)}^6 - {\Pr(X\le k-4)}^6]$$ To date, I only find that ${\Pr(X\le k)}^6 - {\Pr(X\le k-4)}^6$ can be factorized as  \begin{align*} &{\Pr(X\le k)}^6 - {\Pr(X\le k-4)}^6 \\&= [\Pr(X=k)+ \Pr(X=k-1) + \Pr(X=k-2) + \Pr(X=k-3)]\cdot[ {\Pr(X\le k)}^5+{\Pr(X\le k)}^4{\Pr(X\le k-4)}+…+ {\Pr(X\le k-4)}^5] \end{align*} But I have no idea what to do next… Can I assume that $\Pr(X=k) \approx \Pr(X=k-1)$ if $\lambda \to +\infty$? Are there any better approximation for $f(\lambda)$? If there is a simple expression for $ f(\lambda) $ about $\lambda \to +\infty$ that would be best, but I’m open to whatever can be suggested. Thank you in advance!",,"['probability', 'analysis', 'asymptotics']"
15,Identically distributed and correlated systems of random variables,Identically distributed and correlated systems of random variables,,"I have a rectangular grid of $n \times m$ lightbulbs on a torus, each situated at a grid point. That is, the lightbulbs' coordinates range from $(0,0)$ to $(n-1,m-1)$ and the lightbulb $(0,0)$ has neighbors $(0,1)$, $(1,0)$, $(0,m-1)$, and $(n-1,0)$. For each edge, I flip a coin that gives heads with probability $p$. Any lightbulb that has 1 or 3 of its edges heads is on; otherwise it is off. Let $X_{i,j}$ be a random variable that is 1 if the lightbulb at $(i,j)$ is on, and 0 otherwise. Clearly the $X_{i,j}$ are identically distributed. However, they are not independent. I would like to describe the distribution of the system of $X_{i,j}$ (whatever that means; the individual $X_{i,j}$ are tame enough). Assume that $n=m$ if that helps. Formally, what I am asking is: if $X$ is the matrix whose $(i,j)$th entry is $X_{i,j}$, how is $X$ distributed? It is not the same as flipping a coin for each entry. I am rusty enough in my probability theory that I don't really know where to begin. I notice that this has a similarity to the square lattice Ising model, but I believe it is only a resemblance -- there is no value of $p$, for instance, that leads to nearly all of the lightbulbs being on. If this is too difficult to answer, I would appreciate references to toy systems of this nature so that I could analyze mine by analogy.","I have a rectangular grid of $n \times m$ lightbulbs on a torus, each situated at a grid point. That is, the lightbulbs' coordinates range from $(0,0)$ to $(n-1,m-1)$ and the lightbulb $(0,0)$ has neighbors $(0,1)$, $(1,0)$, $(0,m-1)$, and $(n-1,0)$. For each edge, I flip a coin that gives heads with probability $p$. Any lightbulb that has 1 or 3 of its edges heads is on; otherwise it is off. Let $X_{i,j}$ be a random variable that is 1 if the lightbulb at $(i,j)$ is on, and 0 otherwise. Clearly the $X_{i,j}$ are identically distributed. However, they are not independent. I would like to describe the distribution of the system of $X_{i,j}$ (whatever that means; the individual $X_{i,j}$ are tame enough). Assume that $n=m$ if that helps. Formally, what I am asking is: if $X$ is the matrix whose $(i,j)$th entry is $X_{i,j}$, how is $X$ distributed? It is not the same as flipping a coin for each entry. I am rusty enough in my probability theory that I don't really know where to begin. I notice that this has a similarity to the square lattice Ising model, but I believe it is only a resemblance -- there is no value of $p$, for instance, that leads to nearly all of the lightbulbs being on. If this is too difficult to answer, I would appreciate references to toy systems of this nature so that I could analyze mine by analogy.",,"['probability', 'reference-request']"
16,Recovering random variable from its moments,Recovering random variable from its moments,,"The problem is: can you recover a distribution of random variable if you know all its moments? My first guess was to use moment-generating function (MGF). It is known that if two random variables have the same MGF, then they have equal distributions. So if $M(X) = 1 + \sum_{n = 1}^\infty \mathbb{E} X^n \frac{t^n}{n!}$ converges, then it uniquely corresponds to some distribution; we can apply inverse Laplace transform and get the PDF of X, which solves the problem. But there are some issues: MGF may not exist. Even if MGF exists, the approach with Laplace transform works only with continuous random variables. Can you help me with this problem? Thank you!","The problem is: can you recover a distribution of random variable if you know all its moments? My first guess was to use moment-generating function (MGF). It is known that if two random variables have the same MGF, then they have equal distributions. So if $M(X) = 1 + \sum_{n = 1}^\infty \mathbb{E} X^n \frac{t^n}{n!}$ converges, then it uniquely corresponds to some distribution; we can apply inverse Laplace transform and get the PDF of X, which solves the problem. But there are some issues: MGF may not exist. Even if MGF exists, the approach with Laplace transform works only with continuous random variables. Can you help me with this problem? Thank you!",,['probability']
17,Neyman-Pearson lemma on Normal distribution,Neyman-Pearson lemma on Normal distribution,,"We've got a random sample of iid $X_1,\dots,X_n$. We're testing the mean of $X \sim \mathcal{N}(\mu,\sigma^2)$, where $\sigma^2$ is known. The size of the test $\alpha=0.05$. $H_0: \mu=0$ $H_1: \mu=v$ By the Neyman-Pearson lemma the Most Powerful test is $\phi(X) = \mathbf{1}_A$ where the set $A =\{ x: \prod_{i=1}^n \frac{f(\mu_1,\sigma^2)}{f(\mu_0,\sigma^2)} > k \} $ simplyfying we can reduce the test to: \begin{equation} \frac{1}{n}\sum_{i=1}^n X_i > \frac{2\sigma^2(\log k+\mu_1^2 n)}{n \mu_1} \end{equation} where $\mu_1 = v$. Calculating the critical value $k$ we evaluate: $$ \begin{align} \alpha=\mathbb{E}[\phi(X) |H_0] &= \frac{1}{\sqrt{2 \pi \sigma^2}} \int_{-\infty}^{\infty}\phi(x) e^{-\frac{x^2}{2\sigma^2}} dx\\ &=\frac{1}{\sqrt{2 \pi \sigma^2}} \int_{-\infty}^{\infty}\mathbf{1}_A e^{-\frac{x^2}{2\sigma^2}} dx\\ &= \frac{1}{\sqrt{2 \pi \sigma^2}} \int_{k}^{\infty}e^{-\frac{(x-\mu_1)^2}{2\sigma^2}} dx\\ &= \frac{1}{\sqrt{2 \pi }} \int_{\frac{k-\mu_1}{\sigma}}^{\frac{\infty-\mu_1}{\sigma}}e^{-\frac{x^2}{2}} dx\\ &=1-\Phi\Bigg(\frac{k-\mu_1}{\sigma} \Bigg) \end{align} $$ therefore we can derive $k=\mu_1 + \sigma \Phi^{-1}(1-\alpha)$ Is my approach correct, or did I mess up the calculation of the critical value?","We've got a random sample of iid $X_1,\dots,X_n$. We're testing the mean of $X \sim \mathcal{N}(\mu,\sigma^2)$, where $\sigma^2$ is known. The size of the test $\alpha=0.05$. $H_0: \mu=0$ $H_1: \mu=v$ By the Neyman-Pearson lemma the Most Powerful test is $\phi(X) = \mathbf{1}_A$ where the set $A =\{ x: \prod_{i=1}^n \frac{f(\mu_1,\sigma^2)}{f(\mu_0,\sigma^2)} > k \} $ simplyfying we can reduce the test to: \begin{equation} \frac{1}{n}\sum_{i=1}^n X_i > \frac{2\sigma^2(\log k+\mu_1^2 n)}{n \mu_1} \end{equation} where $\mu_1 = v$. Calculating the critical value $k$ we evaluate: $$ \begin{align} \alpha=\mathbb{E}[\phi(X) |H_0] &= \frac{1}{\sqrt{2 \pi \sigma^2}} \int_{-\infty}^{\infty}\phi(x) e^{-\frac{x^2}{2\sigma^2}} dx\\ &=\frac{1}{\sqrt{2 \pi \sigma^2}} \int_{-\infty}^{\infty}\mathbf{1}_A e^{-\frac{x^2}{2\sigma^2}} dx\\ &= \frac{1}{\sqrt{2 \pi \sigma^2}} \int_{k}^{\infty}e^{-\frac{(x-\mu_1)^2}{2\sigma^2}} dx\\ &= \frac{1}{\sqrt{2 \pi }} \int_{\frac{k-\mu_1}{\sigma}}^{\frac{\infty-\mu_1}{\sigma}}e^{-\frac{x^2}{2}} dx\\ &=1-\Phi\Bigg(\frac{k-\mu_1}{\sigma} \Bigg) \end{align} $$ therefore we can derive $k=\mu_1 + \sigma \Phi^{-1}(1-\alpha)$ Is my approach correct, or did I mess up the calculation of the critical value?",,"['probability', 'statistics']"
18,Basic understanding of sampling from a continuous distribution.,Basic understanding of sampling from a continuous distribution.,,"For continuous distribution (on R ) the probability of a single point is $0$. So I'm not sure what does it mean to sample $M$ elements from a continuous distribution. Let say there is a continuous distribution D and there is a number z and a function f such that: $f(x)=1$ for $x < z$ except of a finite number of cases. $f(x)=0$ for $x \ge z$ except of a finite number of cases. $D(x < z) > 0  , D(x \ge z) > 0$ So if I have a random sample  $(X_1, \dots ,X_m)$ from $D$, And assume $X_1>z$ , can I conclude that $f(X_1)=0$ ? And assume $f(X_2)=1$ , can I conclude that $X_2 < z$ ?","For continuous distribution (on R ) the probability of a single point is $0$. So I'm not sure what does it mean to sample $M$ elements from a continuous distribution. Let say there is a continuous distribution D and there is a number z and a function f such that: $f(x)=1$ for $x < z$ except of a finite number of cases. $f(x)=0$ for $x \ge z$ except of a finite number of cases. $D(x < z) > 0  , D(x \ge z) > 0$ So if I have a random sample  $(X_1, \dots ,X_m)$ from $D$, And assume $X_1>z$ , can I conclude that $f(X_1)=0$ ? And assume $f(X_2)=1$ , can I conclude that $X_2 < z$ ?",,"['probability', 'statistics', 'probability-distributions', 'machine-learning']"
19,Calculating Probabilities for Substitution Ciphers using Frequency Analysis,Calculating Probabilities for Substitution Ciphers using Frequency Analysis,,"I have been trying to put together a tool that can take in cipher text encrypted via a simple substitution cipher and calculate the most likely ""key"" (that is, how the plain text letters were mapped to the cipher text letters). I think I've had some success with an approach I've been using, but I suspect it could be a lot better. What I've been doing to this point: For a given mystery letter (that is, that which appeared in the cipher text), I calculated that it actually decrypts to 'a', given the frequency that this mystery letter appears in the cipher text. I did that by using Bayes formula to render it as the likelihood that 'a' appears in the cipher text the number of times that the mystery letter does (knowing usual frequency of 'a' in English). In other words, if we denote $X$ as the mystery letter and $F_X$ denotes the frequency that $X$ appears, then: $$P(X = a \mid F_{X} ) = \frac{ P(F_X \mid X = a) P(X = a) }{ P(F_X) }$$ $P(F_X \mid X = a)$ can be easily calculated by using the binomial distribution mass function knowing the general frequency that 'a' appears in the English language. Without any other information, I assume that $P(X = a)$ equals $1 \over 26$. I neglected to divide by the bottom for reasons which I'll explain in a moment. Then I did the same for every other letter; that is $P(X = b \mid F_X), \ldots, P(X = z \mid F_X)$. I summed up all of what I had calculated for this mystery letter, and then divided each one by the sum, to complete the calculation for each possibility. Then I did the same for every other mystery letter. So, what I ended up with was a 26-by-26 matrix of probabilities that a mystery letter corresponded to a plaintext letter. It then becomes an assignment problem of selecting 26 elements, exactly one from each column and one from each row and trying to maximize probability for a particular key. What I don't like about this (beyond that I'm generally inexperienced and uneasy on this topic), is that I feel that I could definitely do better. Each probability calculated for a mystery letter only takes into account the frequency of that particular mystery letter, and not the frequency of other mystery letters. I've thought about trying to do this as a Bayesian Network, but I feel unsure if that is valid, and I'm having a hard time getting my mind around it. Thanks for any help and insight!","I have been trying to put together a tool that can take in cipher text encrypted via a simple substitution cipher and calculate the most likely ""key"" (that is, how the plain text letters were mapped to the cipher text letters). I think I've had some success with an approach I've been using, but I suspect it could be a lot better. What I've been doing to this point: For a given mystery letter (that is, that which appeared in the cipher text), I calculated that it actually decrypts to 'a', given the frequency that this mystery letter appears in the cipher text. I did that by using Bayes formula to render it as the likelihood that 'a' appears in the cipher text the number of times that the mystery letter does (knowing usual frequency of 'a' in English). In other words, if we denote $X$ as the mystery letter and $F_X$ denotes the frequency that $X$ appears, then: $$P(X = a \mid F_{X} ) = \frac{ P(F_X \mid X = a) P(X = a) }{ P(F_X) }$$ $P(F_X \mid X = a)$ can be easily calculated by using the binomial distribution mass function knowing the general frequency that 'a' appears in the English language. Without any other information, I assume that $P(X = a)$ equals $1 \over 26$. I neglected to divide by the bottom for reasons which I'll explain in a moment. Then I did the same for every other letter; that is $P(X = b \mid F_X), \ldots, P(X = z \mid F_X)$. I summed up all of what I had calculated for this mystery letter, and then divided each one by the sum, to complete the calculation for each possibility. Then I did the same for every other mystery letter. So, what I ended up with was a 26-by-26 matrix of probabilities that a mystery letter corresponded to a plaintext letter. It then becomes an assignment problem of selecting 26 elements, exactly one from each column and one from each row and trying to maximize probability for a particular key. What I don't like about this (beyond that I'm generally inexperienced and uneasy on this topic), is that I feel that I could definitely do better. Each probability calculated for a mystery letter only takes into account the frequency of that particular mystery letter, and not the frequency of other mystery letters. I've thought about trying to do this as a Bayesian Network, but I feel unsure if that is valid, and I'm having a hard time getting my mind around it. Thanks for any help and insight!",,"['probability', 'cryptography', 'bayesian']"
20,Standard error and p-values,Standard error and p-values,,"I am a scientist who is currently doing a BSc. I did some stats a few years ago but since haven't touched it. I'm not sure how to calculate p-values for my data. A bit of background is that I am assessing replication of a virus when I mutate it. The hypothesis is the mutant still replicates, but less efficiently. I'm looking to see if there's statistical significance at the 95% confidence level and then also working out a p-value. So I measure replication and I get this for my completely normal (a.k.a wild type) virus (expressed as a percentage of maximal replication possible) Wild type: 65%, 67%, 70% My mutant form gives me these results: 50%, 60%, 53% For standard error I use the sample standard error formula and for 95% confidence intervals I multiply this by 1.96 for each so I get intervals for both the normal/wild type form and for my mutant. However how do I calculate a p-value? I thought I would calculate the mean and standard deviation of wild type, then compare the mean for my mutant using z-scores etc but I don't think that's right.","I am a scientist who is currently doing a BSc. I did some stats a few years ago but since haven't touched it. I'm not sure how to calculate p-values for my data. A bit of background is that I am assessing replication of a virus when I mutate it. The hypothesis is the mutant still replicates, but less efficiently. I'm looking to see if there's statistical significance at the 95% confidence level and then also working out a p-value. So I measure replication and I get this for my completely normal (a.k.a wild type) virus (expressed as a percentage of maximal replication possible) Wild type: 65%, 67%, 70% My mutant form gives me these results: 50%, 60%, 53% For standard error I use the sample standard error formula and for 95% confidence intervals I multiply this by 1.96 for each so I get intervals for both the normal/wild type form and for my mutant. However how do I calculate a p-value? I thought I would calculate the mean and standard deviation of wild type, then compare the mean for my mutant using z-scores etc but I don't think that's right.",,"['probability', 'statistics', 'standard-deviation']"
21,The Starry Rebound,The Starry Rebound,,"An (infinitely small) ball starting out in the middle of a 5 pointed star table (outer 5 points 10m radius, inner 5 points 5m radius) has a starting angle of a random value from 0 to 360 degrees. The ball is now set loose and travels around the table. On average, how many sides will have been hit once the ball has traveled 1000 m ?","An (infinitely small) ball starting out in the middle of a 5 pointed star table (outer 5 points 10m radius, inner 5 points 5m radius) has a starting angle of a random value from 0 to 360 degrees. The ball is now set loose and travels around the table. On average, how many sides will have been hit once the ball has traveled 1000 m ?",,"['probability', 'geometry', 'dynamical-systems']"
22,Biased random walk away from cliff,Biased random walk away from cliff,,"A drunken probabilist stands $n$ steps from a cliff's edge. He takes random steps, either towards or away from the cliff, each step independent of the past. At any point, the probability of taking a step away is $2/3$, or a step toward, $1/3$. What are his chances of eventually falling off the cliff? The textbook has no answer. This is how I did it: Let $p(i)$ represent the probability of death if you start $i$ steps from the cliff. I first start with the case where $i=1$. Clearly, $$p(1)=\frac{1}{3} + \frac{2}{3}p(2)$$ and $p(2)=(p(1))^2$ so we have the equation $$2(p(1))^2 + 1 - 3(p(1))=0$$ which we can easily solve. Then to get the case where $i=n$ simply raise $p(1)$ to the $n^\text{th}$ power. Is this correct? It seems a bit counterintuitive, since it seems as if no matter how far away the drunkard gets, sooner or later some fluke of chance will send him walking straight and steady down the cliff, with probability $1$.","A drunken probabilist stands $n$ steps from a cliff's edge. He takes random steps, either towards or away from the cliff, each step independent of the past. At any point, the probability of taking a step away is $2/3$, or a step toward, $1/3$. What are his chances of eventually falling off the cliff? The textbook has no answer. This is how I did it: Let $p(i)$ represent the probability of death if you start $i$ steps from the cliff. I first start with the case where $i=1$. Clearly, $$p(1)=\frac{1}{3} + \frac{2}{3}p(2)$$ and $p(2)=(p(1))^2$ so we have the equation $$2(p(1))^2 + 1 - 3(p(1))=0$$ which we can easily solve. Then to get the case where $i=n$ simply raise $p(1)$ to the $n^\text{th}$ power. Is this correct? It seems a bit counterintuitive, since it seems as if no matter how far away the drunkard gets, sooner or later some fluke of chance will send him walking straight and steady down the cliff, with probability $1$.",,"['probability', 'statistics']"
23,Probability to draw cards,Probability to draw cards,,"The problem: Given a pack of cards of 10 different kinds, the probability to draw a card of type $1$ is $\frac{1}{3}$. The probability to draw a card of type $i$, for every $i=2,...,10$ is $\frac{2}{27}$. We can assume that the cards types are independent. What is the probability that the $15$th card being drawn is of a type we haven't drawn yet? I'm kind of lost here. Since the probability to draw card of type $1$ and $2,...,10$ is different, I don't know how to treat this data. Help will be much appreciated!","The problem: Given a pack of cards of 10 different kinds, the probability to draw a card of type $1$ is $\frac{1}{3}$. The probability to draw a card of type $i$, for every $i=2,...,10$ is $\frac{2}{27}$. We can assume that the cards types are independent. What is the probability that the $15$th card being drawn is of a type we haven't drawn yet? I'm kind of lost here. Since the probability to draw card of type $1$ and $2,...,10$ is different, I don't know how to treat this data. Help will be much appreciated!",,"['probability', 'combinatorics']"
24,Is it necessary to state that $y_i \leq 1$,Is it necessary to state that,y_i \leq 1,"In a class test for Linear Programming, my professor deducted some marks because I missed the condition $y_i \leq 1$ in the mixed strategy games solution. $ y_i $ stands for the probability of any step a player can make. My exact statement for $y_i$ was: $$ \sum_{i = 1}^{n}{y_i} = 1 \tag{$y_i \geq 0\ \  \text{ where } \ \  i = 1,2, \cdots, n$} $$ I kept on saying that since all probabilities are non-zero and summing to $1$, none of them can ever exceed the value $1$ itself. Was I wrong in declaring like that? Or is it wrong to skip mentioning that $$ 0 \leq y_i \leq 1 \tag{$i = 1,2, \cdots, n$} $$","In a class test for Linear Programming, my professor deducted some marks because I missed the condition $y_i \leq 1$ in the mixed strategy games solution. $ y_i $ stands for the probability of any step a player can make. My exact statement for $y_i$ was: $$ \sum_{i = 1}^{n}{y_i} = 1 \tag{$y_i \geq 0\ \  \text{ where } \ \  i = 1,2, \cdots, n$} $$ I kept on saying that since all probabilities are non-zero and summing to $1$, none of them can ever exceed the value $1$ itself. Was I wrong in declaring like that? Or is it wrong to skip mentioning that $$ 0 \leq y_i \leq 1 \tag{$i = 1,2, \cdots, n$} $$",,"['probability', 'notation', 'linear-programming', 'game-theory']"
25,"I have a bunch of sets. Some sets contain bad values. I know which sets have them, but not which values are bad.","I have a bunch of sets. Some sets contain bad values. I know which sets have them, but not which values are bad.",,"My company sends email on behalf of many other companies. Hotmail tells us when we start sending spammy messages, but they only say ""some of the emails this giant batch of messages had spammy stuff"", and not specifically which emails were spammy. Those batches of emails contain stuff from lots of different clients we have, and I need to narrow down which clients are sending the spammy messages. So, given a bunch of sets of emails we sent, and Hotmail giving us a ""yay"" or ""nay"" on each set, and given that emails and clients are 1:1 so it's easy to which client sent any given email in the sets, how can I tell which clients are probably the spammers? (P.S.- I'm not the brightest math whiz in the box, so please explain your answer in layman's terms).","My company sends email on behalf of many other companies. Hotmail tells us when we start sending spammy messages, but they only say ""some of the emails this giant batch of messages had spammy stuff"", and not specifically which emails were spammy. Those batches of emails contain stuff from lots of different clients we have, and I need to narrow down which clients are sending the spammy messages. So, given a bunch of sets of emails we sent, and Hotmail giving us a ""yay"" or ""nay"" on each set, and given that emails and clients are 1:1 so it's easy to which client sent any given email in the sets, how can I tell which clients are probably the spammers? (P.S.- I'm not the brightest math whiz in the box, so please explain your answer in layman's terms).",,"['probability', 'statistics', 'statistical-inference']"
26,Equivalence of measures and $L^1$ functions,Equivalence of measures and  functions,L^1,"Suppose we have two probability measures $\mu$ and $\delta$ on $(X, \mathcal{B})$ such that $ \delta <<\mu << \delta $. How can I prove that $f \in L^1(X,\mathcal{B}, \mu)$ iff $f \in L^1(X,\mathcal{B}, \delta)$? My idea was to use that the Radon Nikodym theorem. So we know there exist $g$ $\mu$-measurable and $g^{-1}$ $\delta$-measurable such that for all $B \in \mathcal{B}$: $$\delta(B) = \int_B g d \mu$$ and $$\mu(B) = \int_B g^{-1} d \delta$$. Then if $f$ is in $L^1(X,\mathcal{B}, \mu)$, we have $$\int_X |f| d\delta = \int_X |f g| d\mu$$ and so $f \in L^1(X,\mathcal{B}, \delta)$ if we can claim that  $fg \in L^1(X,\mathcal{B}, \delta)$. However, we only know that $g$ is $\mu$-measurable! So I am not sure if this works... Any thoughts? Of course one can use a Holder type inequality for probability spaces to conclude that $\int_X |f g| d\mu \leq \int_X |f|  d\mu \int_X |g|  d\mu$ but can I say that $\int_X |g|  d\mu < \infty$??","Suppose we have two probability measures $\mu$ and $\delta$ on $(X, \mathcal{B})$ such that $ \delta <<\mu << \delta $. How can I prove that $f \in L^1(X,\mathcal{B}, \mu)$ iff $f \in L^1(X,\mathcal{B}, \delta)$? My idea was to use that the Radon Nikodym theorem. So we know there exist $g$ $\mu$-measurable and $g^{-1}$ $\delta$-measurable such that for all $B \in \mathcal{B}$: $$\delta(B) = \int_B g d \mu$$ and $$\mu(B) = \int_B g^{-1} d \delta$$. Then if $f$ is in $L^1(X,\mathcal{B}, \mu)$, we have $$\int_X |f| d\delta = \int_X |f g| d\mu$$ and so $f \in L^1(X,\mathcal{B}, \delta)$ if we can claim that  $fg \in L^1(X,\mathcal{B}, \delta)$. However, we only know that $g$ is $\mu$-measurable! So I am not sure if this works... Any thoughts? Of course one can use a Holder type inequality for probability spaces to conclude that $\int_X |f g| d\mu \leq \int_X |f|  d\mu \int_X |g|  d\mu$ but can I say that $\int_X |g|  d\mu < \infty$??",,"['probability', 'analysis', 'measure-theory']"
27,Is this probability theorem correct?,Is this probability theorem correct?,,"Let me preface my question by saying that I am not a formally trained mathematician, so please forgive my informal statement of my problem. I have the following 'theorem' of an idea that I believe to be pretty evident in probability. Given an event E Given the Set of all real positive integers R and Given the set of all possible outcomes of a random Event N Given the number of favorable outcomes n where E occurs. p(E) = n/N for 0 < p(E) <= 1 the probability that E will occur in R trials is 1. I have three questions: Is there something logically incorrect about my theorem? Is this a well known theorem, and if so what is it called? How can I write this theorem, more succinctly using symbolic or mathematical notation?","Let me preface my question by saying that I am not a formally trained mathematician, so please forgive my informal statement of my problem. I have the following 'theorem' of an idea that I believe to be pretty evident in probability. Given an event E Given the Set of all real positive integers R and Given the set of all possible outcomes of a random Event N Given the number of favorable outcomes n where E occurs. p(E) = n/N for 0 < p(E) <= 1 the probability that E will occur in R trials is 1. I have three questions: Is there something logically incorrect about my theorem? Is this a well known theorem, and if so what is it called? How can I write this theorem, more succinctly using symbolic or mathematical notation?",,"['probability', 'probability-theory']"
28,Probability with Markov chains,Probability with Markov chains,,"I need some hint about Markov chains. So here is my homework. Let $\{ X_t : t = 0,1, 2, 3, \ldots, n\}? $ be a Markov chain. What is $P(X_0 =i\mid X_n=j)$? So I need to calculate if it's $j$ in the time of $n$ what is the probability of being $i$ at the start? Is it : Since if we start from state 0, then the probability to go to next state 1 is $1/2$, and to state 2 is $1/4$, etc. $$ P_i = \left(\frac {1} {2}\right)^n \cdot \frac {1} {2}$$ Is that right ?","I need some hint about Markov chains. So here is my homework. Let $\{ X_t : t = 0,1, 2, 3, \ldots, n\}? $ be a Markov chain. What is $P(X_0 =i\mid X_n=j)$? So I need to calculate if it's $j$ in the time of $n$ what is the probability of being $i$ at the start? Is it : Since if we start from state 0, then the probability to go to next state 1 is $1/2$, and to state 2 is $1/4$, etc. $$ P_i = \left(\frac {1} {2}\right)^n \cdot \frac {1} {2}$$ Is that right ?",,"['probability', 'markov-chains']"
29,"How to determine the expected value of the $f(x,y)$?",How to determine the expected value of the ?,"f(x,y)","How to determine the expected value of the $f(x,y)$ defined as: f(x,y): $\quad$   for i = 1 to y $\quad$  $\quad$    do x = R(x) $\quad$ $\quad$  return x where $R(N)$ returns any integer in the range $[0,N)$ with equal probability and $R(0) = 0$ I have absolutely no clue on how to approach this one, any ideas?","How to determine the expected value of the $f(x,y)$ defined as: f(x,y): $\quad$   for i = 1 to y $\quad$  $\quad$    do x = R(x) $\quad$ $\quad$  return x where $R(N)$ returns any integer in the range $[0,N)$ with equal probability and $R(0) = 0$ I have absolutely no clue on how to approach this one, any ideas?",,"['probability', 'probability-theory']"
30,Two gamblers' ruin,Two gamblers' ruin,,"I'm trying to work out the solution to a variant of the gambler's ruin. Here's my version: There are two very unlucky but friendly gamblers A and B who decide to pool their money together to form a common budget with starting amount $b$, a positive integer. They roll a weighted die to decide who will play the next game. Therefore, gambler A will play a round with probability $p_A$. Likewise gambler B plays with probability $p_B=1-p_A$. Now, A and B are bad at gambling and either break even or lose money whenever they play—say \$1. So their pool of money can only decrease. However, they are not equally unlucky. Gambler A breaks even (does not lose or make money) $q_A$ of the time and loses otherwise. And gambler B breaks even $q_B$ of the time and loses otherwise. When they've totally exhausted their funds, I want to know how much money each gambler is individually responsible for losing. For example, say they started off with \$1000 and gambler A plays 1/3 of the time, and breaks even 1/3 of the times he plays. Gambler B plays (therefore) 2/3 of the time and breaks even 1/2 of the time she plays. (By simulation) gambler A is likely responsible for about \$400 lost and gambler B is responsible the remaining \$600. I'd appreciate any hints.","I'm trying to work out the solution to a variant of the gambler's ruin. Here's my version: There are two very unlucky but friendly gamblers A and B who decide to pool their money together to form a common budget with starting amount $b$, a positive integer. They roll a weighted die to decide who will play the next game. Therefore, gambler A will play a round with probability $p_A$. Likewise gambler B plays with probability $p_B=1-p_A$. Now, A and B are bad at gambling and either break even or lose money whenever they play—say \$1. So their pool of money can only decrease. However, they are not equally unlucky. Gambler A breaks even (does not lose or make money) $q_A$ of the time and loses otherwise. And gambler B breaks even $q_B$ of the time and loses otherwise. When they've totally exhausted their funds, I want to know how much money each gambler is individually responsible for losing. For example, say they started off with \$1000 and gambler A plays 1/3 of the time, and breaks even 1/3 of the times he plays. Gambler B plays (therefore) 2/3 of the time and breaks even 1/2 of the time she plays. (By simulation) gambler A is likely responsible for about \$400 lost and gambler B is responsible the remaining \$600. I'd appreciate any hints.",,"['probability', 'random-walk']"
31,unbiased estimator of sample variance using two samples,unbiased estimator of sample variance using two samples,,"I have a couple questions, I'm hoping someone can help! Let $X_1...X_n$ is a random i.i.d. sample from a $N(\mu,\sigma^2)$ distribution, and $Y_1...Y_m$ is a random i.i.d. sample from a $N(2\mu,\sigma^2)$ distribution, and further let the two samples be independent (and the quantities $\mu$ and $\sigma^2$ be unknown).  I'm trying to do the following: construct an unbiased estimator of $\mu$ ($\hat{\mu}$) using both samples, calculate $Var(\hat{\mu})$, and then use both samples to obtain an unbiased estimator for $\sigma^2$. I think I understand the first two parts: we know $\large E(\frac{X_1+...+X_n}{n}) = \mu$, and $\large E(\frac{Y_1+...+Y_m}{m}) = 2\mu$, so I believe $\large \frac{X_1+...+X_n}{2n}+\frac{Y_1+...+Y_m}{4m}$ should provide an unbiased estimator for $\mu$, and from that it follows $\large Var(\hat{\mu})=\sigma^2(\frac{1}{4n}+\frac{1}{16m})$. What I'm not clear on is how to construct an unbiased estimator for the variance. I'm aware that $\large\frac{1}{n-1}\sum_{i=1}^n(X_i-\bar{X})^2$ provides an unbiased estimator for $\sigma^2$ (the proof is on wikipedia). From this, it seems like $\large\frac{1}{2}\cdot\frac{1}{n-1}\sum_{i=1}^n(X_i-\bar{X})^2$+$\large\frac{1}{2}\cdot\frac{1}{n-1}\sum_{i=1}^m(Y_i-\bar{Y})^2$ would yield $\frac{\sigma}{2}+\frac{\sigma}{2}=\sigma$, but something about it makes me nervous, and I feel like is approach may be inherently flawed? Any help/suggestions would be greatly appreciated! Thanks","I have a couple questions, I'm hoping someone can help! Let $X_1...X_n$ is a random i.i.d. sample from a $N(\mu,\sigma^2)$ distribution, and $Y_1...Y_m$ is a random i.i.d. sample from a $N(2\mu,\sigma^2)$ distribution, and further let the two samples be independent (and the quantities $\mu$ and $\sigma^2$ be unknown).  I'm trying to do the following: construct an unbiased estimator of $\mu$ ($\hat{\mu}$) using both samples, calculate $Var(\hat{\mu})$, and then use both samples to obtain an unbiased estimator for $\sigma^2$. I think I understand the first two parts: we know $\large E(\frac{X_1+...+X_n}{n}) = \mu$, and $\large E(\frac{Y_1+...+Y_m}{m}) = 2\mu$, so I believe $\large \frac{X_1+...+X_n}{2n}+\frac{Y_1+...+Y_m}{4m}$ should provide an unbiased estimator for $\mu$, and from that it follows $\large Var(\hat{\mu})=\sigma^2(\frac{1}{4n}+\frac{1}{16m})$. What I'm not clear on is how to construct an unbiased estimator for the variance. I'm aware that $\large\frac{1}{n-1}\sum_{i=1}^n(X_i-\bar{X})^2$ provides an unbiased estimator for $\sigma^2$ (the proof is on wikipedia). From this, it seems like $\large\frac{1}{2}\cdot\frac{1}{n-1}\sum_{i=1}^n(X_i-\bar{X})^2$+$\large\frac{1}{2}\cdot\frac{1}{n-1}\sum_{i=1}^m(Y_i-\bar{Y})^2$ would yield $\frac{\sigma}{2}+\frac{\sigma}{2}=\sigma$, but something about it makes me nervous, and I feel like is approach may be inherently flawed? Any help/suggestions would be greatly appreciated! Thanks",,"['probability', 'statistics', 'sampling']"
32,"Given two different ranges, probability of a number in one range being greater than the other","Given two different ranges, probability of a number in one range being greater than the other",,"It has been a long time since I last studied probability and statistics but I am trying to solve a tricky (for me) problem. If I have any two number ranges, what is the calculation for the probability of a random value in one range being higher than the other? Simple example... Range 1: 10-19 Range 2: 20-39 Result: 0% probability that a number in Range 1 is greater than Range 2. Trickier example... Range 1: 10-20 Range 2: 15-24 What is the probability that a random value in Range 1 is greater than a random value in Range 2?","It has been a long time since I last studied probability and statistics but I am trying to solve a tricky (for me) problem. If I have any two number ranges, what is the calculation for the probability of a random value in one range being higher than the other? Simple example... Range 1: 10-19 Range 2: 20-39 Result: 0% probability that a number in Range 1 is greater than Range 2. Trickier example... Range 1: 10-20 Range 2: 15-24 What is the probability that a random value in Range 1 is greater than a random value in Range 2?",,"['probability', 'probability-distributions']"
33,shoes probability for finding exact k pairs of shoes,shoes probability for finding exact k pairs of shoes,,"The closet has $10$ pairs of shoes. The no. of ways in which $6$ shoes can be choosen from it show that There will be (i) no Complete pair (ii) exactly one complete pair (iii) 2 complete pair (iv) 3 complete pair My Try: Total no. of selecting $6$ shoes from $20$ shoes is $\displaystyle = \binom{20}{6}$ Now for first part... First we will select $6$ pairs from $10$ pairs is $ \displaystyle = \binom{10}{6}$ Like  $6$ pairs  is $(A_{1},A_{2})\;\;,(B_{1},B_{2})\;\;,(C_{1},C_{2})\;,(D_{1},D_{2})\;,(E_{1},E_{2})\;,(F_{1},F_{2})$ Noew select $1$ shoes from each pair is $ = \displaystyle \binom{2}{1}\times  \binom{2}{1} \times  \binom{2}{1} \times  \binom{2}{1}\times  \binom{2}{1}\times  \binom{2}{1}=2^6$ So Required probability for first is $ \displaystyle \frac{\binom{10}{6}.2^6}{\binom{20}{6}}$ Now can anyone explain me how can i solve other parts","The closet has $10$ pairs of shoes. The no. of ways in which $6$ shoes can be choosen from it show that There will be (i) no Complete pair (ii) exactly one complete pair (iii) 2 complete pair (iv) 3 complete pair My Try: Total no. of selecting $6$ shoes from $20$ shoes is $\displaystyle = \binom{20}{6}$ Now for first part... First we will select $6$ pairs from $10$ pairs is $ \displaystyle = \binom{10}{6}$ Like  $6$ pairs  is $(A_{1},A_{2})\;\;,(B_{1},B_{2})\;\;,(C_{1},C_{2})\;,(D_{1},D_{2})\;,(E_{1},E_{2})\;,(F_{1},F_{2})$ Noew select $1$ shoes from each pair is $ = \displaystyle \binom{2}{1}\times  \binom{2}{1} \times  \binom{2}{1} \times  \binom{2}{1}\times  \binom{2}{1}\times  \binom{2}{1}=2^6$ So Required probability for first is $ \displaystyle \frac{\binom{10}{6}.2^6}{\binom{20}{6}}$ Now can anyone explain me how can i solve other parts",,['probability']
34,Is it that trivial to see that a sequence of random variables is mutually independent?,Is it that trivial to see that a sequence of random variables is mutually independent?,,"Grinstead and Snells book, Introduction to Probability, page 144: Here is a number of short questions I have about this text: 0) The authors say that they consider ""special classes of random variables"", one such classe being the class of indepedent trails. I think this is imprecise: They should have said ""classes of sequences of random variables"". What do you think ? 1) The $X_j$ are functions $X_j:R\times R\times \ldots \times R \rightarrow \mathbb{R}$, right ? 2) They should have specified that $R\subseteq \mathbb{R}$ since otherwise the $j$-th projection isn't well defined: $X_j(\Omega)\subseteq \mathbb{R}$, but it $R\ni \omega_j \not\in \mathbb{R}$. 3) On the second line from below shouldn't it say ""outcome $(\omega_1,\ldots,\omega_n)$, rather then $(r_1,\ldots,r_n)$ ? (The $r$'s are also already used to define $R=\{r_1,\ldots,r_s\}$) 4) Most important Is it that trivial to see (penultimate line) that the random variables $X_1,\ldots,X_n$ form an independent trials process ? It is indeed easy to see, that they have the same distribution, but proving that they are mutually independent does require some work!","Grinstead and Snells book, Introduction to Probability, page 144: Here is a number of short questions I have about this text: 0) The authors say that they consider ""special classes of random variables"", one such classe being the class of indepedent trails. I think this is imprecise: They should have said ""classes of sequences of random variables"". What do you think ? 1) The $X_j$ are functions $X_j:R\times R\times \ldots \times R \rightarrow \mathbb{R}$, right ? 2) They should have specified that $R\subseteq \mathbb{R}$ since otherwise the $j$-th projection isn't well defined: $X_j(\Omega)\subseteq \mathbb{R}$, but it $R\ni \omega_j \not\in \mathbb{R}$. 3) On the second line from below shouldn't it say ""outcome $(\omega_1,\ldots,\omega_n)$, rather then $(r_1,\ldots,r_n)$ ? (The $r$'s are also already used to define $R=\{r_1,\ldots,r_s\}$) 4) Most important Is it that trivial to see (penultimate line) that the random variables $X_1,\ldots,X_n$ form an independent trials process ? It is indeed easy to see, that they have the same distribution, but proving that they are mutually independent does require some work!",,"['probability', 'probability-theory']"
35,How many random samplings are required to solve a Rubik's cube?,How many random samplings are required to solve a Rubik's cube?,,"Consider the permutation group of the Rubik's cube $G$: http://en.wikipedia.org/wiki/Rubik%27s_Cube_group We have that $$\lVert G\rVert = 2^{27}\times 3^{14}\times 5^3\times 7^2\times 11=43,252,003,274,489,856,000\approx 4.325\times 10^{19}$$ Say we randomly sample cube states.  More specifically, we take a cube, tie a blindfold on ourselves, and make some arbitrarily large number of random moves, then untie the blindfold and look at the cube state (ignoring everything during mixing).  The idea is that we want to uniformly sample cube states, and not simply take a random walk on the cube's Cayley graph where all sorts of correlations will crop up based on previous states. Is the probability of solving the cube during any given random sampling $\frac{1}{||G||}$?  Or is the space larger because $||G||$ does not include automorhisms, reflections, or other symmetries that might decrease the probability of observing the solution state?  What is the true probability of seeing a cube solution during any given sampling event?  Can every cube solution be realized (in terms of the nearest-neighbor graph for colored faces)?  Is it, for example, true that every state is unique up to the automorphisms of the cube, or do some states have higher probabilities than others because they are more or less symmetric?","Consider the permutation group of the Rubik's cube $G$: http://en.wikipedia.org/wiki/Rubik%27s_Cube_group We have that $$\lVert G\rVert = 2^{27}\times 3^{14}\times 5^3\times 7^2\times 11=43,252,003,274,489,856,000\approx 4.325\times 10^{19}$$ Say we randomly sample cube states.  More specifically, we take a cube, tie a blindfold on ourselves, and make some arbitrarily large number of random moves, then untie the blindfold and look at the cube state (ignoring everything during mixing).  The idea is that we want to uniformly sample cube states, and not simply take a random walk on the cube's Cayley graph where all sorts of correlations will crop up based on previous states. Is the probability of solving the cube during any given random sampling $\frac{1}{||G||}$?  Or is the space larger because $||G||$ does not include automorhisms, reflections, or other symmetries that might decrease the probability of observing the solution state?  What is the true probability of seeing a cube solution during any given sampling event?  Can every cube solution be realized (in terms of the nearest-neighbor graph for colored faces)?  Is it, for example, true that every state is unique up to the automorphisms of the cube, or do some states have higher probabilities than others because they are more or less symmetric?",,"['probability', 'combinatorics']"
36,Probability - Balls and Buckets; variance question,Probability - Balls and Buckets; variance question,,"I've been working on this problem for a while and its giving me no end of trouble! The question is this: Suppose we have 2k buckets, numbered 1 through 2k. We throw x black balls and y white balls, at random, towards the buckets. A black ball has twice the chance of landing in an odd bucket as landing in an even one, and a white ball has twice the chance of landing in an even bucket as landing in an odd one. Aside from this, the chances of a ball falling into a certain bucket is evenly distributed (i.e. a black ball is just as likely to land in bucket 1 as bucket 15, etc). I'm trying to compute $E(z)$ and $Var(z)$, where $z$ is the number of buckets that have at least one ball in them. It seems to me that it may be easier to look at the chance that bucket i is empty, as follows: Pr(bucket i is empty|bucket is odd) = $(\frac{3k-2}{3k})^x(\frac{3k-1}{3k})^y$, and Pr(bucket i is empty|bucket is even) = $(\frac{3k-2}{3k})^y(\frac{3k-1}{3k})^x$. If we let $Z_i$ equal 1 if the bin is empty and 0 otherwise, our expected value should be $((\frac{3k-2}{3k})^x(\frac{3k-1}{3k})^y)\cdot k$ $+$ $((\frac{3k-2}{3k})^y(\frac{3k-1}{3k})^x)\cdot k$. This expression seems really unwieldy though, and calculating variance and covariance with it seems an almost insurmountable task - am I going about this in an incorrect fashion? Thanks so much for your help! Edit: Attempting to continue down this path, running into some issues? If I go along with this, I'm looking for $\sum E(z_i^2) + \sum\sum_{i\neq j}E(z_iz_j)+E(z)^2$, where the first sum is equal to $\sum E(z_i)$, since $z$ takes on values either 1 or 0. There are four cases for the covariance term in the middle: i is odd, j is odd; i is odd, j is even; i is even, j is odd; i is even, j is even. If i and j are both even or odd, there are $k^2$ possibilities for [both odd] and for [both even]. Otherwise, there are $k^2-k$ possibilities for each case. So I must calculate probabilities of four cases: 1.) Given: i is odd, j is odd Pr(no balls in i)$x$Pr(no balls in j|no balls in  i): $\large (k^2-k)*((\frac{3k-2}{3k})^x\cdot (\frac{3k-1}{3k})^y\cdot (\frac{3k-4}{3k-2})^x\cdot (\frac{3k-2}{3k-1})^y)$ The first half of the expression is just the probability that nobody gets off at floor i. The second half needs to take into account that one of the odd buckets will not have balls in it. The rest of the cases are basically the same, with minor tweaks. Assuming this is correct, do I just attempt to plug in these expressions in the variance expression above and do my best to simplify? thanks for the help!","I've been working on this problem for a while and its giving me no end of trouble! The question is this: Suppose we have 2k buckets, numbered 1 through 2k. We throw x black balls and y white balls, at random, towards the buckets. A black ball has twice the chance of landing in an odd bucket as landing in an even one, and a white ball has twice the chance of landing in an even bucket as landing in an odd one. Aside from this, the chances of a ball falling into a certain bucket is evenly distributed (i.e. a black ball is just as likely to land in bucket 1 as bucket 15, etc). I'm trying to compute $E(z)$ and $Var(z)$, where $z$ is the number of buckets that have at least one ball in them. It seems to me that it may be easier to look at the chance that bucket i is empty, as follows: Pr(bucket i is empty|bucket is odd) = $(\frac{3k-2}{3k})^x(\frac{3k-1}{3k})^y$, and Pr(bucket i is empty|bucket is even) = $(\frac{3k-2}{3k})^y(\frac{3k-1}{3k})^x$. If we let $Z_i$ equal 1 if the bin is empty and 0 otherwise, our expected value should be $((\frac{3k-2}{3k})^x(\frac{3k-1}{3k})^y)\cdot k$ $+$ $((\frac{3k-2}{3k})^y(\frac{3k-1}{3k})^x)\cdot k$. This expression seems really unwieldy though, and calculating variance and covariance with it seems an almost insurmountable task - am I going about this in an incorrect fashion? Thanks so much for your help! Edit: Attempting to continue down this path, running into some issues? If I go along with this, I'm looking for $\sum E(z_i^2) + \sum\sum_{i\neq j}E(z_iz_j)+E(z)^2$, where the first sum is equal to $\sum E(z_i)$, since $z$ takes on values either 1 or 0. There are four cases for the covariance term in the middle: i is odd, j is odd; i is odd, j is even; i is even, j is odd; i is even, j is even. If i and j are both even or odd, there are $k^2$ possibilities for [both odd] and for [both even]. Otherwise, there are $k^2-k$ possibilities for each case. So I must calculate probabilities of four cases: 1.) Given: i is odd, j is odd Pr(no balls in i)$x$Pr(no balls in j|no balls in  i): $\large (k^2-k)*((\frac{3k-2}{3k})^x\cdot (\frac{3k-1}{3k})^y\cdot (\frac{3k-4}{3k-2})^x\cdot (\frac{3k-2}{3k-1})^y)$ The first half of the expression is just the probability that nobody gets off at floor i. The second half needs to take into account that one of the odd buckets will not have balls in it. The rest of the cases are basically the same, with minor tweaks. Assuming this is correct, do I just attempt to plug in these expressions in the variance expression above and do my best to simplify? thanks for the help!",,"['probability', 'balls-in-bins']"
37,"Combinatorics, listing all pairs","Combinatorics, listing all pairs",,"In a group of six people $p_1,\ldots,p_6$, two people are chosen to win a prize (=holiday on Tahiti). List all pairs we can make. This is a sample space $S$. Is the answer $$S=\{\{p_i,p_j\}\},1\leq i<j\leq 6$$ or $$S=\{(p_i,p_j)\},1\leq i,j\leq 6,i\ne j$$?","In a group of six people $p_1,\ldots,p_6$, two people are chosen to win a prize (=holiday on Tahiti). List all pairs we can make. This is a sample space $S$. Is the answer $$S=\{\{p_i,p_j\}\},1\leq i<j\leq 6$$ or $$S=\{(p_i,p_j)\},1\leq i,j\leq 6,i\ne j$$?",,"['probability', 'combinatorics']"
38,Continuous uniformly distributed variables,Continuous uniformly distributed variables,,"Let $L_k$ uniform (discrete) i.i.d. variables in $\{0,1\}$. How to prove $$X:=\sum_{k=1}^\infty \frac{L_k}{2^k}$$ is uniformly distributed in $[0,1]$? Of course I have to show that the cdf is the same, which means I have to prove for all $n \in \mathbb{N}$, $$P\left(X \leq \frac{j}{2^n}\right) = \frac{j}{2^n}.$$ I have no idea how to continue after $$P\left(X \leq \frac{j}{2^n} \right) = P\left(\sum_{k=1}^\infty \frac{L_k}{2^k}\leq \frac{j}{2^n}  \right)$$ Can sb. give me a hint?","Let $L_k$ uniform (discrete) i.i.d. variables in $\{0,1\}$. How to prove $$X:=\sum_{k=1}^\infty \frac{L_k}{2^k}$$ is uniformly distributed in $[0,1]$? Of course I have to show that the cdf is the same, which means I have to prove for all $n \in \mathbb{N}$, $$P\left(X \leq \frac{j}{2^n}\right) = \frac{j}{2^n}.$$ I have no idea how to continue after $$P\left(X \leq \frac{j}{2^n} \right) = P\left(\sum_{k=1}^\infty \frac{L_k}{2^k}\leq \frac{j}{2^n}  \right)$$ Can sb. give me a hint?",,"['probability', 'probability-distributions']"
39,Books of Random Numbers. How were the numbers generated?,Books of Random Numbers. How were the numbers generated?,,"In the mid 1940's I believe, the RAND corporation published a book with a million random numbers (from a normal distribution). This was before Marsaglia, so considering the primitive state of their knowledge in testing for randomness, how did they generate these numbers and how did they test them? I ask this b/c I always get the sense that people in the field seem to regard this source as the ""gold standard"", although it is not obvious at all to me that this should be the case. Thanks, Jack","In the mid 1940's I believe, the RAND corporation published a book with a million random numbers (from a normal distribution). This was before Marsaglia, so considering the primitive state of their knowledge in testing for randomness, how did they generate these numbers and how did they test them? I ask this b/c I always get the sense that people in the field seem to regard this source as the ""gold standard"", although it is not obvious at all to me that this should be the case. Thanks, Jack",,"['probability', 'probability-theory', 'stochastic-processes']"
40,Bound for the variance,Bound for the variance,,"I am trying to show that the second moment is bounded by 1 from above. Let $x=(x_1, \ldots, x_n)$ be real vector such that $\|x\|_2=1$. Let $\pi(\cdot)$ be a permutation on the set $\{1,...,n\}$  with a uniform distribution. I would like to show that $E\left|\sum_{i=1}^kx_{\pi(i)}-\sum_{i=k+1}^{n}x_{\pi(i)}\right|^2\leq 1,$ $1\leq k\leq n$. $$ $$ As was shown in Expectation of the difference of sums Denoting by $A=\sum_{i=1}^{n}x_i\quad\text{and}\quad B=\sum_{i=1}^{n}x_i^2$ we get that  for every $i$ and every $i\ne j$, $$ \mathbb E(x_{\pi(i)}^2)=\frac{B}{n},\quad\mathbb E(x_{\pi(i)}x_{\pi(j)})=\frac{A^2-B}{n(n-1)}. $$ Now, expanding the square in the expectation, we obtain: $$ E\left|\sum_{i=1}^kx_{\pi(i)}-\sum_{i=k+1}^{n}x_{\pi(i)}\right|^2=B+\frac{(A^2-B)(k(k-1)+(n-k)(n-k-1)-2kn)}{n(n-1)}. $$ But now I am stuck. How do I show that the last expression is less than or equal to one? Thank you.","I am trying to show that the second moment is bounded by 1 from above. Let $x=(x_1, \ldots, x_n)$ be real vector such that $\|x\|_2=1$. Let $\pi(\cdot)$ be a permutation on the set $\{1,...,n\}$  with a uniform distribution. I would like to show that $E\left|\sum_{i=1}^kx_{\pi(i)}-\sum_{i=k+1}^{n}x_{\pi(i)}\right|^2\leq 1,$ $1\leq k\leq n$. $$ $$ As was shown in Expectation of the difference of sums Denoting by $A=\sum_{i=1}^{n}x_i\quad\text{and}\quad B=\sum_{i=1}^{n}x_i^2$ we get that  for every $i$ and every $i\ne j$, $$ \mathbb E(x_{\pi(i)}^2)=\frac{B}{n},\quad\mathbb E(x_{\pi(i)}x_{\pi(j)})=\frac{A^2-B}{n(n-1)}. $$ Now, expanding the square in the expectation, we obtain: $$ E\left|\sum_{i=1}^kx_{\pi(i)}-\sum_{i=k+1}^{n}x_{\pi(i)}\right|^2=B+\frac{(A^2-B)(k(k-1)+(n-k)(n-k-1)-2kn)}{n(n-1)}. $$ But now I am stuck. How do I show that the last expression is less than or equal to one? Thank you.",,"['probability', 'combinatorics', 'statistics']"
41,Maximize expected heads with biased coins.,Maximize expected heads with biased coins.,,"Say I have n biased coins. Coin $i$ lands on heads with probability $p_i$ which comes from a uniform prior probability distribution over $[0, 1]$. At times $t = 1, 2, ..., k$ I must select one of the coins to flip (Assume k > n). What strategy would give a maximal expected number of heads over the k flips? This problem seems so deceptively simple... You obviously need to make some trade-off between flipping the coin that has given the best return so far and trying out others to see if they're better. I'm not sure how to do that.","Say I have n biased coins. Coin $i$ lands on heads with probability $p_i$ which comes from a uniform prior probability distribution over $[0, 1]$. At times $t = 1, 2, ..., k$ I must select one of the coins to flip (Assume k > n). What strategy would give a maximal expected number of heads over the k flips? This problem seems so deceptively simple... You obviously need to make some trade-off between flipping the coin that has given the best return so far and trying out others to see if they're better. I'm not sure how to do that.",,"['probability', 'statistics']"
42,Solving a multiple choice question paper,Solving a multiple choice question paper,,"Firstly, I am not sure whether this is a mathematical problem. Please inform me in case it is not. My question is as follows I have a online game with 14 questions. It's a kind of treasure hunt. Each question has 4 choices. The correct answer choices are unevenly distributed. Choices B & C constitute 5 answers each and choice A & D constitute 2 answers each. Imagine a man without any knowledge about the quiz subject, what would be the best way to clear the quiz to go to the next level? Once the user submits the answers, the system gives him reference links for questions he answered wrongly. But they will not be numbered and he will not know which references are for which question. The references will be in order and the user has unlimited attempts to crack the paper. The user has to get 10 answers correct to crack the quiz. What is the best approach to follow to go to next level.  The problem is a practical problem and I am not sure under which topic of mathematics this falls under.","Firstly, I am not sure whether this is a mathematical problem. Please inform me in case it is not. My question is as follows I have a online game with 14 questions. It's a kind of treasure hunt. Each question has 4 choices. The correct answer choices are unevenly distributed. Choices B & C constitute 5 answers each and choice A & D constitute 2 answers each. Imagine a man without any knowledge about the quiz subject, what would be the best way to clear the quiz to go to the next level? Once the user submits the answers, the system gives him reference links for questions he answered wrongly. But they will not be numbered and he will not know which references are for which question. The references will be in order and the user has unlimited attempts to crack the paper. The user has to get 10 answers correct to crack the quiz. What is the best approach to follow to go to next level.  The problem is a practical problem and I am not sure under which topic of mathematics this falls under.",,['probability']
43,"The probability of a ""double supremum"" of random variable","The probability of a ""double supremum"" of random variable",,"Let $X_1,X_2,X_3,\ldots$ be IID r.v. with \begin{equation} P(X_i<-1)=0 \end{equation} \begin{equation} P(X_i<0)>0 \end{equation} \begin{equation} P(X_i>0)>0. \end{equation} Define \begin{equation} F_t = \prod_{i=1}^t(1+\frac{1}{2}X_i) \end{equation} \begin{equation} G_t = \prod_{i=1}^t(1+\frac{1}{4}X_i). \end{equation} How can we show, for some integer $S>0$, that \begin{equation} P\left(\sup_{s\in[1,S]} \left[\sup_{t\in[1,s]} \frac{F_t-F_s}{F_t}\right] > \frac{1}{3}\right) > P\left(\sup_{s\in[1,S]} \left[\sup_{t\in[1,s]} \frac{G_t-G_s}{G_t}\right] > \frac{1}{3}\right) \end{equation} Thanks in advance for any hints to get me started, or possibly a draft of a solution. I simply have no clue about how to proceed. Update: I have been thinking. Instinctvly, this problem seems to hold true because for any negative $X_i$, this holds: \begin{equation} (1+\frac{1}{2}X_i) < (1+\frac{1}{4}X_i). \end{equation} This should mean that $F_t$ will usually be dropping faster than $G_t$. I still wonder how to formalize these instincts.","Let $X_1,X_2,X_3,\ldots$ be IID r.v. with \begin{equation} P(X_i<-1)=0 \end{equation} \begin{equation} P(X_i<0)>0 \end{equation} \begin{equation} P(X_i>0)>0. \end{equation} Define \begin{equation} F_t = \prod_{i=1}^t(1+\frac{1}{2}X_i) \end{equation} \begin{equation} G_t = \prod_{i=1}^t(1+\frac{1}{4}X_i). \end{equation} How can we show, for some integer $S>0$, that \begin{equation} P\left(\sup_{s\in[1,S]} \left[\sup_{t\in[1,s]} \frac{F_t-F_s}{F_t}\right] > \frac{1}{3}\right) > P\left(\sup_{s\in[1,S]} \left[\sup_{t\in[1,s]} \frac{G_t-G_s}{G_t}\right] > \frac{1}{3}\right) \end{equation} Thanks in advance for any hints to get me started, or possibly a draft of a solution. I simply have no clue about how to proceed. Update: I have been thinking. Instinctvly, this problem seems to hold true because for any negative $X_i$, this holds: \begin{equation} (1+\frac{1}{2}X_i) < (1+\frac{1}{4}X_i). \end{equation} This should mean that $F_t$ will usually be dropping faster than $G_t$. I still wonder how to formalize these instincts.",,"['calculus', 'probability', 'random-walk']"
44,question involving Markov chain,question involving Markov chain,,"Let $S_{2m}$ be the group of all permutations $\pi$ of $\{1, 2, \ldots, 2m\}$. The following transition kernel $S$ generates the random transposition walk $$ Ch(\pi, \pi')= \begin{cases} \frac{1}{2m} & \pi'=\pi\\[10pt] \frac{2}{(2m)^2} & \pi'=\tau \pi\  \text{ for some transposition $\tau$}\\[10pt] 0, & \text{otherwise} \end{cases} $$ It is known that with symmetric probability measure $\mu$, the pair $(Ch, \mu)$ defines a reversible Markov chain. Let $\tau=(I, J)$ be a random transposition, with $I, J$ chosen independentely and uniformly from $\{1, 2, \ldots, 2m\}$. Multiplication by $\tau$ results in taking a step in the chain defined by $Ch$. All this structure is given in  "" The Concentration of Measure Phenomenon "" by M. Ledoux. Let $c=(c_1, c_2, \ldots, c_{2m})$ be a vector in $R^{2m}$. Define function $f:S_{2m}\longrightarrow R$ as $f(\pi):=|\sum_{k=0}^mc_{\pi(k)}-\sum_{k=m+1}^{2m}c_{\pi(k)}|$. Question: Find the upper bound of $|f(\pi)-f(\tau \pi)|$. Thank you for your help.","Let $S_{2m}$ be the group of all permutations $\pi$ of $\{1, 2, \ldots, 2m\}$. The following transition kernel $S$ generates the random transposition walk $$ Ch(\pi, \pi')= \begin{cases} \frac{1}{2m} & \pi'=\pi\\[10pt] \frac{2}{(2m)^2} & \pi'=\tau \pi\  \text{ for some transposition $\tau$}\\[10pt] 0, & \text{otherwise} \end{cases} $$ It is known that with symmetric probability measure $\mu$, the pair $(Ch, \mu)$ defines a reversible Markov chain. Let $\tau=(I, J)$ be a random transposition, with $I, J$ chosen independentely and uniformly from $\{1, 2, \ldots, 2m\}$. Multiplication by $\tau$ results in taking a step in the chain defined by $Ch$. All this structure is given in  "" The Concentration of Measure Phenomenon "" by M. Ledoux. Let $c=(c_1, c_2, \ldots, c_{2m})$ be a vector in $R^{2m}$. Define function $f:S_{2m}\longrightarrow R$ as $f(\pi):=|\sum_{k=0}^mc_{\pi(k)}-\sum_{k=m+1}^{2m}c_{\pi(k)}|$. Question: Find the upper bound of $|f(\pi)-f(\tau \pi)|$. Thank you for your help.",,"['probability', 'statistics', 'permutations', 'markov-chains']"
45,Probability that no side of a dice is rolled more than $k$ times,Probability that no side of a dice is rolled more than  times,k,"Suppose, we roll a fair die $n$ times. The expected value of how many times each side of the die will appear is $\frac{n}{6}$. The question is, what is the probability that any side of the die will come up more than $k$ times, where $n>k>\frac{n}{6}$? Or you could put it this way: what is the probability that no side will come up more than $k$ times? Clearly these two probabilities add up to 1.","Suppose, we roll a fair die $n$ times. The expected value of how many times each side of the die will appear is $\frac{n}{6}$. The question is, what is the probability that any side of the die will come up more than $k$ times, where $n>k>\frac{n}{6}$? Or you could put it this way: what is the probability that no side will come up more than $k$ times? Clearly these two probabilities add up to 1.",,"['probability', 'combinatorics', 'dice']"
46,Erdős-Rényi Random Graph Triangle Number,Erdős-Rényi Random Graph Triangle Number,,"Let $G(n,p)$ be the usual Erdős-Rényi random graph. Let $T$ be the number of triangles in a realization of such a random graph. After counting $T$, let $T'$ be the number of triangles after selecting a pair of vertices, uniformly at random and then redrawing the edge with the same probability $p$. Are there any asymptotic results in $n$ of $P(T'\mid T)$? If it helps, one can assume that $T$ is chosen to be larger than the expected number of triangles. Moreover, notice that choosing a vertex pair uniformly is equivalent to just fixing two vertices and subsequently redrawing the edge. References would be greatly appreciated!","Let $G(n,p)$ be the usual Erdős-Rényi random graph. Let $T$ be the number of triangles in a realization of such a random graph. After counting $T$, let $T'$ be the number of triangles after selecting a pair of vertices, uniformly at random and then redrawing the edge with the same probability $p$. Are there any asymptotic results in $n$ of $P(T'\mid T)$? If it helps, one can assume that $T$ is chosen to be larger than the expected number of triangles. Moreover, notice that choosing a vertex pair uniformly is equivalent to just fixing two vertices and subsequently redrawing the edge. References would be greatly appreciated!",,"['probability', 'probability-theory', 'graph-theory']"
47,How to measure the clustering property of a sequence,How to measure the clustering property of a sequence,,"For a sequence of numbers with increasing order, $a_1 < a_2, \dots < a_n$, I want to know a measure to describe the the extend of clustering in the sequences. For instance, a sequence like $1, 2,3,4, 100, 101,102,103$ and another sequence $10, 12, 32, 45, 66,77, 89,102$, it is clear to see the first sequence have higher clustering property.","For a sequence of numbers with increasing order, $a_1 < a_2, \dots < a_n$, I want to know a measure to describe the the extend of clustering in the sequences. For instance, a sequence like $1, 2,3,4, 100, 101,102,103$ and another sequence $10, 12, 32, 45, 66,77, 89,102$, it is clear to see the first sequence have higher clustering property.",,"['probability', 'combinatorics', 'sequences-and-series']"
48,Find a pdf of a function of a random variable,Find a pdf of a function of a random variable,,Let $Y$ be an exponential random variable with parameter $\frac12$.   Let $X=e^{-Y/2}$. Determine the pdf of $X$. $$f(t)=\frac{d}{dt}P(X\le t)=\frac{d}{dt}P(e^{-Y/2}\le t)\\=\frac{d}{dt}P(Y\ge-2\ln t)=\frac{d}{dt}(1-P(Y&lt-2\ln t))=-\frac12e^{\ln t}=-\frac{t}2$$ How come I end up with a negative value? What's wrong?,Let $Y$ be an exponential random variable with parameter $\frac12$.   Let $X=e^{-Y/2}$. Determine the pdf of $X$. $$f(t)=\frac{d}{dt}P(X\le t)=\frac{d}{dt}P(e^{-Y/2}\le t)\\=\frac{d}{dt}P(Y\ge-2\ln t)=\frac{d}{dt}(1-P(Y&lt-2\ln t))=-\frac12e^{\ln t}=-\frac{t}2$$ How come I end up with a negative value? What's wrong?,,['probability']
49,Help with fixing a proof for an upper bound of $\int_{0}^{\infty} \exp\left( - \frac{x^2}{2y^{2r}} - \frac{y^2}{2}\right) \frac{dy}{y^r}$,Help with fixing a proof for an upper bound of,\int_{0}^{\infty} \exp\left( - \frac{x^2}{2y^{2r}} - \frac{y^2}{2}\right) \frac{dy}{y^r},"This is a follow up to Help with removing singularities involving $ \int_{1}^{\infty} \exp\left( - \frac{x^2}{2y^{2r}} - \frac{y^2}{2}\right) \frac{dy}{y^r}$ I am getting a little confused at the argument that was suggested to me which was based on replacing the limits in an integral using the fact that the singularity at the origin of our integrand in not a problem (the proof of removing the singularity is essentially contained in this post Why is $ \frac{\exp{\left( -\frac{x^2}{4y^{2r}} \right)}}{y^r} $ bounded on $[0,1]$? ) Let $ 0 < r < 1$, fix $x > 1$ and consider the integral $$ I_{1}(x) = \int_{0}^{\infty} \exp\left( - \frac{x^2}{2y^{2r}} - \frac{y^2}{2}\right) \frac{dy}{y^r}.$$ Fix a constant $c^* = r^{\frac{1}{2r+2}} $ and let $x^* = x^{\frac{1}{1+r}}$. Write $f(y) =   \frac{x^2}{2y^{2r}} + \frac{y^2}{2}$ and note $c^* x^*$ is a local minimum of $f(y)$ so that it is a global max for $-f(y)$ on $[0, \infty)$. (*) I am interested in bounds of the form $I_1 (x) \leq c_1(r) \exp( - f(c^* x^*))$  ($c_1(r)$ is constant depending only on $r$) The argument presented to me said that ""Without Loss of Generality"" we can replace  $\int_{0}^{\infty} \exp\left( - \frac{x^2}{2y^{2r}} - \frac{y^2}{2}\right) \frac{dy}{y^r}$ by $ \int_{\epsilon}^{\infty}\exp\left( - \frac{x^2}{2y^{2r}} - \frac{y^2}{2}\right) \frac{dy}{y^r} $ for some $\epsilon >0$ (the proof is based on the second post I cited) and then we just get the bound in (*) by observing that $I_1(x) \leq \exp( - f(c^* x^*)) \int_{\epsilon}^{\infty} y^{-r} dr$... This argument feels flawed to me is there any hopes of salvaging it? Bonus Question: is there a way to get the upper bound in (*) than the one outlined in 2)?","This is a follow up to Help with removing singularities involving $ \int_{1}^{\infty} \exp\left( - \frac{x^2}{2y^{2r}} - \frac{y^2}{2}\right) \frac{dy}{y^r}$ I am getting a little confused at the argument that was suggested to me which was based on replacing the limits in an integral using the fact that the singularity at the origin of our integrand in not a problem (the proof of removing the singularity is essentially contained in this post Why is $ \frac{\exp{\left( -\frac{x^2}{4y^{2r}} \right)}}{y^r} $ bounded on $[0,1]$? ) Let $ 0 < r < 1$, fix $x > 1$ and consider the integral $$ I_{1}(x) = \int_{0}^{\infty} \exp\left( - \frac{x^2}{2y^{2r}} - \frac{y^2}{2}\right) \frac{dy}{y^r}.$$ Fix a constant $c^* = r^{\frac{1}{2r+2}} $ and let $x^* = x^{\frac{1}{1+r}}$. Write $f(y) =   \frac{x^2}{2y^{2r}} + \frac{y^2}{2}$ and note $c^* x^*$ is a local minimum of $f(y)$ so that it is a global max for $-f(y)$ on $[0, \infty)$. (*) I am interested in bounds of the form $I_1 (x) \leq c_1(r) \exp( - f(c^* x^*))$  ($c_1(r)$ is constant depending only on $r$) The argument presented to me said that ""Without Loss of Generality"" we can replace  $\int_{0}^{\infty} \exp\left( - \frac{x^2}{2y^{2r}} - \frac{y^2}{2}\right) \frac{dy}{y^r}$ by $ \int_{\epsilon}^{\infty}\exp\left( - \frac{x^2}{2y^{2r}} - \frac{y^2}{2}\right) \frac{dy}{y^r} $ for some $\epsilon >0$ (the proof is based on the second post I cited) and then we just get the bound in (*) by observing that $I_1(x) \leq \exp( - f(c^* x^*)) \int_{\epsilon}^{\infty} y^{-r} dr$... This argument feels flawed to me is there any hopes of salvaging it? Bonus Question: is there a way to get the upper bound in (*) than the one outlined in 2)?",,"['calculus', 'real-analysis', 'probability', 'analysis']"
50,"What are the $X_1, X_2, ..., X_n$ in the Sampling Distribution of $\bar{X}=\frac { 1 }{ n } \sum _{ i=1 }^{ n }{ X_{ j } } $?",What are the  in the Sampling Distribution of ?,"X_1, X_2, ..., X_n \bar{X}=\frac { 1 }{ n } \sum _{ i=1 }^{ n }{ X_{ j } } ","The sampling distribution of $\bar{X}$ defined in a book that I am reading is $\bar{X}=\frac { 1 }{ n } \sum _{ i=1 }^{ n }{ X_{ j } } $. I know $X_1, X_2, ..., X_n$ are random variables. But what is confusing to me is that should $n$ here be seen as the number of observations or the number of trials in each observation? If the $n$ here is the number of observations, then, does that mean that the $X_1, X_2, ..., X_n$ are mean values of their own individual trials? For example, $X_1$ is the average of say 10 trials. So $X_1$ itself is $X_1=\frac { 1 }{ 10 } \sum _{ i=1 }^{ 10 }{ Y_{ i } } $, where $Y_{1...10}$ are the trials made in the observation set of $X_1$. In this case, however, $X_1, X_2, ..., X_n$ are more of like constant instead of random variables any more. Then it shouldn't be just $X_1, X_2, ..., X_n$ but $\bar{X_1}, \bar{X_2}, ..., \bar{X_n}$ and $\bar{X}=\bar{X_1}, \bar{X_2}, ..., \bar{X_n}$. But this doesn't look like how it is defined. Then, if the $n$ here is the number of trials in each observation, then $\bar{X}$ is just the mean value of the trials in this single observation, which again doesn't make a lot of sense because this is just average of one set of observation. From my understanding, Sampling Distribution is the ""average of the averages of $n$ sets of observations"" and so this interpretation doesn't align with my understanding too. Which of my interpretations is right? What is the right way to look at the definition of the sampling distribution of $\bar{X}$ as $\bar{X}=\frac { 1 }{ n } \sum _{ i=1 }^{ n }{ X_{ j } } $ and what are the $X_1, X_2, ..., X_n$?","The sampling distribution of $\bar{X}$ defined in a book that I am reading is $\bar{X}=\frac { 1 }{ n } \sum _{ i=1 }^{ n }{ X_{ j } } $. I know $X_1, X_2, ..., X_n$ are random variables. But what is confusing to me is that should $n$ here be seen as the number of observations or the number of trials in each observation? If the $n$ here is the number of observations, then, does that mean that the $X_1, X_2, ..., X_n$ are mean values of their own individual trials? For example, $X_1$ is the average of say 10 trials. So $X_1$ itself is $X_1=\frac { 1 }{ 10 } \sum _{ i=1 }^{ 10 }{ Y_{ i } } $, where $Y_{1...10}$ are the trials made in the observation set of $X_1$. In this case, however, $X_1, X_2, ..., X_n$ are more of like constant instead of random variables any more. Then it shouldn't be just $X_1, X_2, ..., X_n$ but $\bar{X_1}, \bar{X_2}, ..., \bar{X_n}$ and $\bar{X}=\bar{X_1}, \bar{X_2}, ..., \bar{X_n}$. But this doesn't look like how it is defined. Then, if the $n$ here is the number of trials in each observation, then $\bar{X}$ is just the mean value of the trials in this single observation, which again doesn't make a lot of sense because this is just average of one set of observation. From my understanding, Sampling Distribution is the ""average of the averages of $n$ sets of observations"" and so this interpretation doesn't align with my understanding too. Which of my interpretations is right? What is the right way to look at the definition of the sampling distribution of $\bar{X}$ as $\bar{X}=\frac { 1 }{ n } \sum _{ i=1 }^{ n }{ X_{ j } } $ and what are the $X_1, X_2, ..., X_n$?",,"['probability', 'statistics']"
51,expectation and group of permutations,expectation and group of permutations,,"Let $r_i, i=1,\ldots,m$ be random variables with $P(r_i=1)=P(r_i=-1)=1/2$. let $b_i, i=1,\ldots,m$ be real numbers. I should calculate $E\left(|\sum_{i=1}^m b_ir_i|^4| \sum_{i=1}^m r_i=0\right)$ using the following hint: Let $$X=\left\{r\in\{1, -1\}^m \quad | \quad r_i= \begin{cases} 1,&\quad \text{if} \quad i \quad\text{is in }Y\\[4mm] -1,&\quad \text{if}\quad i \quad\text{is not in }Y \end{cases} \right\},$$ where $Y\subset\{1,\ldots,m\}, \operatorname{card}(Y)=m/2$. For $X$ we put into correspondence the group $\Pi_m$ of all permutations of the set $\{1,\ldots,m\}$ as follows $$ \pi(\cdot)\longleftrightarrow r_i= \begin{cases} 1,&\quad \text{if} \quad \pi(i)\leq \frac m2\\[4mm] -1,&\quad \text{if} \quad \pi(i)>\frac m2 \end{cases}. $$ On the group $\Pi_m$ I consider the normalized counting measure $\mu_m(A)=\operatorname{card}(A)/m!$ for $A\subset \Pi_m$ and the normalized metric $d_m(\pi_1, \pi_2)=\frac 1m \#\{i:\pi_1(i)\neq \pi_2(i), \quad \pi_1, \pi_2 \in \Pi_m\}$. It is known that $\Pi_m$ is a normal Levy family and  for $A_\epsilon=\{\pi\quad| \exists \pi'\in A: d_m(\pi, \pi')\leq \epsilon\}$ we have $$ \inf_{\mu_m(A)\geq 1/2}\mu_m(A_\epsilon)\geq 1-2\exp(-c\epsilon^2m), \quad \text{$c>0$ is a constant}. $$ It is known that in a Levy family we have phenomenon of concentration of measure around one value of a function. So, if $f:\Pi_m\longrightarrow R$ is a function with modulus of continuity $\omega_f(\epsilon)=\sup_{d_m(\pi_1, \pi_2)}|f(\pi_1)-f(\pi_2)|$ and with median $M_f$, then $$ \mu\left(|f-M_f|\leq \omega_f(\epsilon)\right)\geq 2\inf_{\mu_m(A)\geq 1/2}\mu_m(A_\epsilon)-1. $$ But now I am confused with the next step. What can I say about expectation which I need to find? Thank you for your help.","Let $r_i, i=1,\ldots,m$ be random variables with $P(r_i=1)=P(r_i=-1)=1/2$. let $b_i, i=1,\ldots,m$ be real numbers. I should calculate $E\left(|\sum_{i=1}^m b_ir_i|^4| \sum_{i=1}^m r_i=0\right)$ using the following hint: Let $$X=\left\{r\in\{1, -1\}^m \quad | \quad r_i= \begin{cases} 1,&\quad \text{if} \quad i \quad\text{is in }Y\\[4mm] -1,&\quad \text{if}\quad i \quad\text{is not in }Y \end{cases} \right\},$$ where $Y\subset\{1,\ldots,m\}, \operatorname{card}(Y)=m/2$. For $X$ we put into correspondence the group $\Pi_m$ of all permutations of the set $\{1,\ldots,m\}$ as follows $$ \pi(\cdot)\longleftrightarrow r_i= \begin{cases} 1,&\quad \text{if} \quad \pi(i)\leq \frac m2\\[4mm] -1,&\quad \text{if} \quad \pi(i)>\frac m2 \end{cases}. $$ On the group $\Pi_m$ I consider the normalized counting measure $\mu_m(A)=\operatorname{card}(A)/m!$ for $A\subset \Pi_m$ and the normalized metric $d_m(\pi_1, \pi_2)=\frac 1m \#\{i:\pi_1(i)\neq \pi_2(i), \quad \pi_1, \pi_2 \in \Pi_m\}$. It is known that $\Pi_m$ is a normal Levy family and  for $A_\epsilon=\{\pi\quad| \exists \pi'\in A: d_m(\pi, \pi')\leq \epsilon\}$ we have $$ \inf_{\mu_m(A)\geq 1/2}\mu_m(A_\epsilon)\geq 1-2\exp(-c\epsilon^2m), \quad \text{$c>0$ is a constant}. $$ It is known that in a Levy family we have phenomenon of concentration of measure around one value of a function. So, if $f:\Pi_m\longrightarrow R$ is a function with modulus of continuity $\omega_f(\epsilon)=\sup_{d_m(\pi_1, \pi_2)}|f(\pi_1)-f(\pi_2)|$ and with median $M_f$, then $$ \mu\left(|f-M_f|\leq \omega_f(\epsilon)\right)\geq 2\inf_{\mu_m(A)\geq 1/2}\mu_m(A_\epsilon)-1. $$ But now I am confused with the next step. What can I say about expectation which I need to find? Thank you for your help.",,"['probability', 'combinatorics', 'statistics', 'measure-theory']"
52,understanding a proof of the hitting time theorem for a right-continuous random walk using generating functions,understanding a proof of the hitting time theorem for a right-continuous random walk using generating functions,,"This is particularly directed at those who have Grimmett & Stirzaker, Probability and random processes (2005), at hand. It pertains to the proof step prior to equation (10), p. 166. For others: $X_i$ are i.i.d. integer-valued random variables with $\mathbb{P}(X_i \leq 1) = 1$ and $\mathbb{P}(X_i = 1) > 0$. $T_b=\min\{n:\sum_{i=1}^n X_i = b\}>0$ is the first hitting time of the point $b$. $G(z) = \mathbb{E}\left(z^{-X_1}\right)  = \sum_{n=-\infty}^1 z^{-n} \mathbb{P}(X_1=n)$ $F_b(z) = \mathbb{E}\left(z^{T_b}\right)  = \sum_{n=0}^\infty z^n \mathbb{P}(T_b=n)$ equation (9): $F_b(z) = F_1(z)^b$ for $b \geq 1$ I'm not understanding how to prove $$ \mathbb{E}(\mathbb{E}(z^{T_1}|X_1))  = \mathbb{E}(z^{1+T_{1-X_1}})  = z \mathbb{E}\left(F_{1-X_1}(z)\right)  = z \mathbb{E}\left(F_1(z)^{1-X_1}\right)  = z F_1(z)G(F_1(z)) $$ In J. G. Wendel, ""Left-continuous random walk and the Lagrange expansion"" (1975), he argues in essence that $$ \begin{align*} \mathbb{E}(\mathbb{E}(z^{T_1}|X_1))  & = \sum_{n=-1}^\infty \mathbb{E}(z^{T_1}|X_1=-n)\ \mathbb{P}(X_1=-n) \\  & = z \mathbb{P}(X_1=1) + \sum_{n=0}^\infty \mathbb{E}(z^{T_1}|X_1=-n)\ \mathbb{P}(X_1=-n)  & T_1=1 \Leftrightarrow X_1=1 \\  & = z \mathbb{P}(X_1=1) + \sum_{n=0}^\infty \mathbb{E}(z^{1+T_{1+n}})\ \mathbb{P}(X_1=-n)  & \textrm{homogeneity} \\  & = z \mathbb{P}(X_1=1) + z \sum_{n=0}^\infty F_{1+n}(z)\ \mathbb{P}(X_1=-n)  & \textrm{definition of }F \\  & = z \mathbb{P}(X_1=1) + z \sum_{n=0}^\infty F_1(z)^{1+n}\ \mathbb{P}(X_1=-n)  & \textrm{equation (9)} \\  & = z F_1(z) \sum_{n=-1}^\infty F_1(z)^n\ \mathbb{P}(X_1=-n) \\  & = z F_1(z) G(F_1(z))  & \textrm{definition of }G \end{align*} $$ In the second line, $n=-1$ is treated specially, because the temporal and spatial homogeneity assumptions don't apply to it. Specifically, the further time required to hit 1 is 0; applying the homogeneity assumptions would imply $T_0=0$, which isn't allowed, hitting times are positive. In the fifth line, applying equation (9), you can see that $n=-1$ needs to be treated specially there as well. So although $\mathbb{E}(\mathbb{E}(z^{T_1}|X_1)) = z F_1(z)G(F_1(z))$, it seems that Grimmett's intermediate steps are nonsensical. Or can you see his logic? Incidentally, $$ \mathbb{E}\left(F_1(z)^{1-X_1}\right)  = \sum_{n=0}^\infty F_1(z)^n\ \mathbb{P}(1-X_1=n)  = \sum_{n=-1}^\infty F_1(z)^{1+n}\ \mathbb{P}(X_1=-n)  = F_1(z)G(F_1(z)) $$ so the subsequent application of Lagrange's inversion formula on p. 166 is still done correctly.","This is particularly directed at those who have Grimmett & Stirzaker, Probability and random processes (2005), at hand. It pertains to the proof step prior to equation (10), p. 166. For others: $X_i$ are i.i.d. integer-valued random variables with $\mathbb{P}(X_i \leq 1) = 1$ and $\mathbb{P}(X_i = 1) > 0$. $T_b=\min\{n:\sum_{i=1}^n X_i = b\}>0$ is the first hitting time of the point $b$. $G(z) = \mathbb{E}\left(z^{-X_1}\right)  = \sum_{n=-\infty}^1 z^{-n} \mathbb{P}(X_1=n)$ $F_b(z) = \mathbb{E}\left(z^{T_b}\right)  = \sum_{n=0}^\infty z^n \mathbb{P}(T_b=n)$ equation (9): $F_b(z) = F_1(z)^b$ for $b \geq 1$ I'm not understanding how to prove $$ \mathbb{E}(\mathbb{E}(z^{T_1}|X_1))  = \mathbb{E}(z^{1+T_{1-X_1}})  = z \mathbb{E}\left(F_{1-X_1}(z)\right)  = z \mathbb{E}\left(F_1(z)^{1-X_1}\right)  = z F_1(z)G(F_1(z)) $$ In J. G. Wendel, ""Left-continuous random walk and the Lagrange expansion"" (1975), he argues in essence that $$ \begin{align*} \mathbb{E}(\mathbb{E}(z^{T_1}|X_1))  & = \sum_{n=-1}^\infty \mathbb{E}(z^{T_1}|X_1=-n)\ \mathbb{P}(X_1=-n) \\  & = z \mathbb{P}(X_1=1) + \sum_{n=0}^\infty \mathbb{E}(z^{T_1}|X_1=-n)\ \mathbb{P}(X_1=-n)  & T_1=1 \Leftrightarrow X_1=1 \\  & = z \mathbb{P}(X_1=1) + \sum_{n=0}^\infty \mathbb{E}(z^{1+T_{1+n}})\ \mathbb{P}(X_1=-n)  & \textrm{homogeneity} \\  & = z \mathbb{P}(X_1=1) + z \sum_{n=0}^\infty F_{1+n}(z)\ \mathbb{P}(X_1=-n)  & \textrm{definition of }F \\  & = z \mathbb{P}(X_1=1) + z \sum_{n=0}^\infty F_1(z)^{1+n}\ \mathbb{P}(X_1=-n)  & \textrm{equation (9)} \\  & = z F_1(z) \sum_{n=-1}^\infty F_1(z)^n\ \mathbb{P}(X_1=-n) \\  & = z F_1(z) G(F_1(z))  & \textrm{definition of }G \end{align*} $$ In the second line, $n=-1$ is treated specially, because the temporal and spatial homogeneity assumptions don't apply to it. Specifically, the further time required to hit 1 is 0; applying the homogeneity assumptions would imply $T_0=0$, which isn't allowed, hitting times are positive. In the fifth line, applying equation (9), you can see that $n=-1$ needs to be treated specially there as well. So although $\mathbb{E}(\mathbb{E}(z^{T_1}|X_1)) = z F_1(z)G(F_1(z))$, it seems that Grimmett's intermediate steps are nonsensical. Or can you see his logic? Incidentally, $$ \mathbb{E}\left(F_1(z)^{1-X_1}\right)  = \sum_{n=0}^\infty F_1(z)^n\ \mathbb{P}(1-X_1=n)  = \sum_{n=-1}^\infty F_1(z)^{1+n}\ \mathbb{P}(X_1=-n)  = F_1(z)G(F_1(z)) $$ so the subsequent application of Lagrange's inversion formula on p. 166 is still done correctly.",,"['probability', 'random-walk']"
53,source of proof for a characterization of normal distribution,source of proof for a characterization of normal distribution,,"I want to know the proof of the following statement about normal distribution: If the sample mean and sample variance are independent for a population, then the distribution of the population is normal. I cannot find the proof in any probability/statistics textbook that I have. Please help.","I want to know the proof of the following statement about normal distribution: If the sample mean and sample variance are independent for a population, then the distribution of the population is normal. I cannot find the proof in any probability/statistics textbook that I have. Please help.",,"['probability', 'reference-request', 'statistics']"
54,Monotone Likelihood Ratio and Stochastic Dominance,Monotone Likelihood Ratio and Stochastic Dominance,,http://en.wikipedia.org/wiki/Monotone_likelihood_ratio says neither monotone hazard rates nor stochastic dominance imply the MLRP what is an example? Is there any necessary and sufficient condition for stochastic dominance?,http://en.wikipedia.org/wiki/Monotone_likelihood_ratio says neither monotone hazard rates nor stochastic dominance imply the MLRP what is an example? Is there any necessary and sufficient condition for stochastic dominance?,,['probability']
55,Probability of familiarity with a popular item?,Probability of familiarity with a popular item?,,"I want to know if there's a method for determining the likelihood that a person is familiar with the most popular item in a set, given the number of items in said set they are familiar with. Here's the best example I can think of: Andrew walks into a room with 20 dogs. He's played with 3 of these dogs, but we don't know which dogs in advance. More people have played with Spike than any other dog in the room. Let's say that Andrew's circle of friends is 10 people (including himself), and 5 of them have played with Spike. What are the odds that Andrew has played with Spike? I'm sure there's an existing formula for this type of problem, but there are so many probability formulae that I can't find the one I need.","I want to know if there's a method for determining the likelihood that a person is familiar with the most popular item in a set, given the number of items in said set they are familiar with. Here's the best example I can think of: Andrew walks into a room with 20 dogs. He's played with 3 of these dogs, but we don't know which dogs in advance. More people have played with Spike than any other dog in the room. Let's say that Andrew's circle of friends is 10 people (including himself), and 5 of them have played with Spike. What are the odds that Andrew has played with Spike? I'm sure there's an existing formula for this type of problem, but there are so many probability formulae that I can't find the one I need.",,['probability']
56,random walk along edges of tetrahedron --- which face gets hit last?,random walk along edges of tetrahedron --- which face gets hit last?,,"Suppose we have a tetrahedron $abcd$, and start at edge $ab$.  Now walk to any ""adjacent"" edge (i.e. in this case any edge other than $cd$), each with equal probability $1/4$.  This gives a stationary Markov process on the edges. Walking from one edge to another uniquely determines the face that they share.  In this case, we say the face was ""hit."" For example, if you start at edge $ab$ and walk to edge $bd$ then face $abd$ has been hit.  As we continue the random walk indefinitely, the probability approaches $1$ that every face has been hit at least once. What is the probability that face $abc$ is the last of the four faces to be hit?  This tells us the probability for every face, since the probability for $abc$ and $abd$ being the last face hit is the same by symmetry (since we start at edge $ab$), and similarly the probability of $bcd$ and $acd$ being hit last is also the same. The answer that I'd most like is that the probability of being hit last is the same for every face $1/4$, but I don't know if that's true.","Suppose we have a tetrahedron $abcd$, and start at edge $ab$.  Now walk to any ""adjacent"" edge (i.e. in this case any edge other than $cd$), each with equal probability $1/4$.  This gives a stationary Markov process on the edges. Walking from one edge to another uniquely determines the face that they share.  In this case, we say the face was ""hit."" For example, if you start at edge $ab$ and walk to edge $bd$ then face $abd$ has been hit.  As we continue the random walk indefinitely, the probability approaches $1$ that every face has been hit at least once. What is the probability that face $abc$ is the last of the four faces to be hit?  This tells us the probability for every face, since the probability for $abc$ and $abd$ being the last face hit is the same by symmetry (since we start at edge $ab$), and similarly the probability of $bcd$ and $acd$ being hit last is also the same. The answer that I'd most like is that the probability of being hit last is the same for every face $1/4$, but I don't know if that's true.",,"['probability', 'combinatorics', 'markov-chains']"
57,Probability / Discrete Random Variables,Probability / Discrete Random Variables,,"(a) Let $X$ and $Y$ be two independent discrete random variables. Derive a formula for expressing the distribution of the sum $S = X + Y$ in terms of the distributions of $X$ and of $Y$. (b) Use your formula in part (a) to compute the distribution of $S = X +Y$ if $X$ and $Y$ are both discrete and uniformly distributed on $\{1,\dots,K\}.$ (c) Suppose now $X$ and $Y$ are continuous random variables with densities $f$ and $g$ respectively ($X,Y$ still independent). Based on part (a) and your understanding of continuous random variables, give an educated guess for the formula of the density of $S = X +Y$ in terms of $f$ and $g$. (d) Use your formula in part (c) to compute the density of $S$ if $X$ and $Y$ have both uniform densities on $[0, a]$. (e) Show that if $X$ and $Y$ are independent normally distributed variables, then $X +Y$ is also a normally distributed variable. Added from comment: What I think I have figured out so far is: a) distribution of $S$ is $$\text{mean}(S) = \text{mean}(X) + \text{mean}(Y)$$ and $$\text{stddev}(S) = \sqrt{\text{stddev}(X)^2 + \text{stddev}(Y)^2}$$ b) stuck a little on b... help please","(a) Let $X$ and $Y$ be two independent discrete random variables. Derive a formula for expressing the distribution of the sum $S = X + Y$ in terms of the distributions of $X$ and of $Y$. (b) Use your formula in part (a) to compute the distribution of $S = X +Y$ if $X$ and $Y$ are both discrete and uniformly distributed on $\{1,\dots,K\}.$ (c) Suppose now $X$ and $Y$ are continuous random variables with densities $f$ and $g$ respectively ($X,Y$ still independent). Based on part (a) and your understanding of continuous random variables, give an educated guess for the formula of the density of $S = X +Y$ in terms of $f$ and $g$. (d) Use your formula in part (c) to compute the density of $S$ if $X$ and $Y$ have both uniform densities on $[0, a]$. (e) Show that if $X$ and $Y$ are independent normally distributed variables, then $X +Y$ is also a normally distributed variable. Added from comment: What I think I have figured out so far is: a) distribution of $S$ is $$\text{mean}(S) = \text{mean}(X) + \text{mean}(Y)$$ and $$\text{stddev}(S) = \sqrt{\text{stddev}(X)^2 + \text{stddev}(Y)^2}$$ b) stuck a little on b... help please",,['probability']
58,Probability of two independent standard normal random variables,Probability of two independent standard normal random variables,,"Suppose we have $X,Y$, two independent standard normal random variables. How can we calculate $P(|\min(X,Y)|<1)$. I am still learning multivariables probability, and I also realize there are a lot of nice properties of two standard normal r.vs but I am not sure how to use them.","Suppose we have $X,Y$, two independent standard normal random variables. How can we calculate $P(|\min(X,Y)|<1)$. I am still learning multivariables probability, and I also realize there are a lot of nice properties of two standard normal r.vs but I am not sure how to use them.",,['probability']
59,"One container contains 6 red and 4 white balls, while a second container contains 7 red and 3 white balls","One container contains 6 red and 4 white balls, while a second container contains 7 red and 3 white balls",,"One container contains 6 red and 4 white balls, while a second container contains 7 red and 3 white balls. A ball is chosen at random from the first container and placed in the second.  Then a ball is chosen at random from the second container and placed in first. a. What is the probability to choose a red ball of the second container? b. What is the probability the ball was red chosen from the first container if know that from the second selected white?","One container contains 6 red and 4 white balls, while a second container contains 7 red and 3 white balls. A ball is chosen at random from the first container and placed in the second.  Then a ball is chosen at random from the second container and placed in first. a. What is the probability to choose a red ball of the second container? b. What is the probability the ball was red chosen from the first container if know that from the second selected white?",,['probability']
60,Convergence in distribution of product of two sequences of random variables,Convergence in distribution of product of two sequences of random variables,,"For random variables $\{X_n\}$, $\{Y_n\}$ that converge in distribution to X and Y, what would it mean if $\{(X_n,Y_n)\}$ converges in distribution to (X,Y)?","For random variables $\{X_n\}$, $\{Y_n\}$ that converge in distribution to X and Y, what would it mean if $\{(X_n,Y_n)\}$ converges in distribution to (X,Y)?",,['probability']
61,A Boolean function with total influence 1 must be a dictatorship,A Boolean function with total influence 1 must be a dictatorship,,"Let $f:\{-1,1\}^n\to\{-1,1\}$ be a boolean function. Define the influence of the $i$'th coordinate of $f$ as follows: $$\operatorname{Inf}_i(f)=\Pr_{x}[f(x)\neq f(\hat x_i)]$$ where $x$ is uniformly picked from $\{-1,1\}^n$, and $\hat x_i$ is $x$ with its $i$'th coordinate flipped (e.g., say $x=(1,1,1,1,-1)$, then $\hat x_3=(1,1,-1,1,-1)$). How do I show that if $f$ is balanced ($\mathbb{E}_x[f(x)]=0$ ) and holds $\displaystyle\sum_{i=1}^n{Inf}_i(f)=1$ then $f$ must be a dictatorship function ($dict_i(x_1,\ldots,x_i,\ldots,x_n)=x_i$) up to a sign, i.e., $\pm dict_i$ ?","Let $f:\{-1,1\}^n\to\{-1,1\}$ be a boolean function. Define the influence of the $i$'th coordinate of $f$ as follows: $$\operatorname{Inf}_i(f)=\Pr_{x}[f(x)\neq f(\hat x_i)]$$ where $x$ is uniformly picked from $\{-1,1\}^n$, and $\hat x_i$ is $x$ with its $i$'th coordinate flipped (e.g., say $x=(1,1,1,1,-1)$, then $\hat x_3=(1,1,-1,1,-1)$). How do I show that if $f$ is balanced ($\mathbb{E}_x[f(x)]=0$ ) and holds $\displaystyle\sum_{i=1}^n{Inf}_i(f)=1$ then $f$ must be a dictatorship function ($dict_i(x_1,\ldots,x_i,\ldots,x_n)=x_i$) up to a sign, i.e., $\pm dict_i$ ?",,"['probability', 'combinatorics']"
62,"Bound 1D gaussian domain in the interval $[-3\sigma, 3\sigma]$ so it still is a probability density function",Bound 1D gaussian domain in the interval  so it still is a probability density function,"[-3\sigma, 3\sigma]","I need to bound a 1D gaussian/normal (or similar) probability density function in the domain interval $[-3\sigma, 3\sigma]$ in a way that still integrates to 1. I would need something like this: $$ p(x) = \begin{cases} N(x;\mu, \sigma) &\text{if } -3\sigma \leq x \leq 3\sigma\\ 0 & \text{otherwise } \end{cases}  $$ This is NOT a probability density function but how could I get a bounded distribution that is similar to the gaussian case? Thanks in advance, Federico","I need to bound a 1D gaussian/normal (or similar) probability density function in the domain interval $[-3\sigma, 3\sigma]$ in a way that still integrates to 1. I would need something like this: $$ p(x) = \begin{cases} N(x;\mu, \sigma) &\text{if } -3\sigma \leq x \leq 3\sigma\\ 0 & \text{otherwise } \end{cases}  $$ This is NOT a probability density function but how could I get a bounded distribution that is similar to the gaussian case? Thanks in advance, Federico",,"['probability', 'functions', 'probability-distributions']"
63,Diverging random walk,Diverging random walk,,"I have a process $X_{n+1} = X_n\xi_n$ where $\xi_n\sim\mathcal N(1,1)$ and $\xi_n$ is independent of $X_n$. I need to prove that if $X_0\neq0$ then $$ \mathsf P\{|X_n|>1\text{ for some }n\geq0\} = 1. $$ From this I construct a random walk: $Y_n = \log|X_n|$ so $$ Y_{n+1} = Y_n+\eta_n $$ where $\eta_n = \log|\xi_n|$. I guess that from here I should apply the Law of Large Numbers - but I'm stacked with it. Could you help me? For now I should prove that $Y_n$ will eventually be positive a.s. starting from any point. On the other hand, $X_n$ is a martingale which maybe also useful for deriving the desired result. If it helps, one can take $\xi_n\sim\mathcal N(m,1)$ for some $m\geq1$.","I have a process $X_{n+1} = X_n\xi_n$ where $\xi_n\sim\mathcal N(1,1)$ and $\xi_n$ is independent of $X_n$. I need to prove that if $X_0\neq0$ then $$ \mathsf P\{|X_n|>1\text{ for some }n\geq0\} = 1. $$ From this I construct a random walk: $Y_n = \log|X_n|$ so $$ Y_{n+1} = Y_n+\eta_n $$ where $\eta_n = \log|\xi_n|$. I guess that from here I should apply the Law of Large Numbers - but I'm stacked with it. Could you help me? For now I should prove that $Y_n$ will eventually be positive a.s. starting from any point. On the other hand, $X_n$ is a martingale which maybe also useful for deriving the desired result. If it helps, one can take $\xi_n\sim\mathcal N(m,1)$ for some $m\geq1$.",,"['probability', 'stochastic-processes']"
64,ML estimate vs expected value?,ML estimate vs expected value?,,"I have a random vector $x=[x_1,x_2,...,x_n]^T$ with prior distribution Normal(0, I). I have $m$ linear constraints summarized in matrix form $Ax=y$ where $A$ is an $m$ by $n$ matrix and $y$ is an $m$ by 1 matrix.  I used Lagrange multipliers technique and derived that given the constraints, the minimum for $\sum_{j=0}^{n}x_{j}^{2}$ occurs at $x_{opt}=Wy$ where $W=A^TQ\Lambda^{-1}Q^T$ and $AA^T=Q\Lambda Q^T$,  $Q^TQ=I$, and  $\Lambda$  diagonal.  My question is: what is $x_{opt}$ called? Does it make sense if I call it $x_{ML}$ ( ML for Maximum Likelihood) ?  Is the conditional distribution of $x$ given the constraints also normal ? if yes, isn't $x_{opt}$ the mean of the conditional distribution. So shall I call $x_{opt}$, the expected value for $x$ ? please see this for a related problem: https://stats.stackexchange.com/questions/9071/intuitive-explanation-of-contribution-to-sum-of-two-normally-distributed-random-v","I have a random vector $x=[x_1,x_2,...,x_n]^T$ with prior distribution Normal(0, I). I have $m$ linear constraints summarized in matrix form $Ax=y$ where $A$ is an $m$ by $n$ matrix and $y$ is an $m$ by 1 matrix.  I used Lagrange multipliers technique and derived that given the constraints, the minimum for $\sum_{j=0}^{n}x_{j}^{2}$ occurs at $x_{opt}=Wy$ where $W=A^TQ\Lambda^{-1}Q^T$ and $AA^T=Q\Lambda Q^T$,  $Q^TQ=I$, and  $\Lambda$  diagonal.  My question is: what is $x_{opt}$ called? Does it make sense if I call it $x_{ML}$ ( ML for Maximum Likelihood) ?  Is the conditional distribution of $x$ given the constraints also normal ? if yes, isn't $x_{opt}$ the mean of the conditional distribution. So shall I call $x_{opt}$, the expected value for $x$ ? please see this for a related problem: https://stats.stackexchange.com/questions/9071/intuitive-explanation-of-contribution-to-sum-of-two-normally-distributed-random-v",,"['probability', 'statistics', 'probability-theory', 'probability-distributions']"
65,"probability of at least one person having a gem of type $n$, etc.","probability of at least one person having a gem of type , etc.",n,"(Probability Question) I have a number of $N$ types of gems that I have handed out to $M$ people, using some random probability function for each person. Thus:  $$ \begin{align} P_1^1 + P_2^1 + P_3^1 + P_4^1 + \dotsb + P_n^1 &= 1  \\ P_1^2 + P_2^2 + P_3^2 + P_4^2 + \dotsb + P_n^2 &= 1  \\ P_1^3 + P_2^3 + P_3^3 + P_4^3 + \dotsb + P_n^3 &= 1  \\ P_1^4 + P_2^4 + P_3^4 + P_4^4 + \dotsb + P_n^4 &= 1  \\ P_1^5 + P_2^5 + P_3^5 + P_4^5 + \dotsb + P_n^5 &= 1  \\ \vdots & \\ P_1^m + P_2^m + P_3^m + P_4^m + \dotsb + P_n^m &= 1  \end{align} $$ Thus, each person has a type of gem with a certain probability (each person can only have one type of gem). An example for $N = 3$ , $M = 3$ would be (each row is a different person, each column a gem of  type different type): $0.3 ~0.5~ 0.2$ $0.8 ~0.1~ 0.1$ $0.4 ~0.3~ 0.3$ Is there a way of finding out the probability of at least one person having a gem of type $n$, at least one person having a gem of type $n-1$, but none having one of $n$, at least one person having a gem of type $n-2$, but none having a gem of type $(n-1)$ and $n$ etc., without enumerating all the joint probabilities ($N^M$ possible combinations) ?","(Probability Question) I have a number of $N$ types of gems that I have handed out to $M$ people, using some random probability function for each person. Thus:  $$ \begin{align} P_1^1 + P_2^1 + P_3^1 + P_4^1 + \dotsb + P_n^1 &= 1  \\ P_1^2 + P_2^2 + P_3^2 + P_4^2 + \dotsb + P_n^2 &= 1  \\ P_1^3 + P_2^3 + P_3^3 + P_4^3 + \dotsb + P_n^3 &= 1  \\ P_1^4 + P_2^4 + P_3^4 + P_4^4 + \dotsb + P_n^4 &= 1  \\ P_1^5 + P_2^5 + P_3^5 + P_4^5 + \dotsb + P_n^5 &= 1  \\ \vdots & \\ P_1^m + P_2^m + P_3^m + P_4^m + \dotsb + P_n^m &= 1  \end{align} $$ Thus, each person has a type of gem with a certain probability (each person can only have one type of gem). An example for $N = 3$ , $M = 3$ would be (each row is a different person, each column a gem of  type different type): $0.3 ~0.5~ 0.2$ $0.8 ~0.1~ 0.1$ $0.4 ~0.3~ 0.3$ Is there a way of finding out the probability of at least one person having a gem of type $n$, at least one person having a gem of type $n-1$, but none having one of $n$, at least one person having a gem of type $n-2$, but none having a gem of type $(n-1)$ and $n$ etc., without enumerating all the joint probabilities ($N^M$ possible combinations) ?",,['probability']
66,How can I get better at this certain kind of probability problem?,How can I get better at this certain kind of probability problem?,,"I'm studying for an onsite interview with Google for a product manager position. While looking at interview questions online, I've realized that I really need to brush up on the probability and statistics that I once knew well in college -- marbles, cards, dice, etc. I'm expecting questions like this one: If the probability of observing a car in 30 minutes on a highway is 0.95, what is the probability of observing a car in 10 minutes (assuming constant default probability)? What's a good way to find a lot of problems at this level? I'd like to try to do several per day between now and the interview (which is a couple weeks away). Are there any good books of problems like this? I don't think I need to study permutations/combinations or types of distributions or anything like that, so a general prob/stat textbook isn't really what I need. I don't really know how to characterize those problems well enough to search for them. Thanks!","I'm studying for an onsite interview with Google for a product manager position. While looking at interview questions online, I've realized that I really need to brush up on the probability and statistics that I once knew well in college -- marbles, cards, dice, etc. I'm expecting questions like this one: If the probability of observing a car in 30 minutes on a highway is 0.95, what is the probability of observing a car in 10 minutes (assuming constant default probability)? What's a good way to find a lot of problems at this level? I'd like to try to do several per day between now and the interview (which is a couple weeks away). Are there any good books of problems like this? I don't think I need to study permutations/combinations or types of distributions or anything like that, so a general prob/stat textbook isn't really what I need. I don't really know how to characterize those problems well enough to search for them. Thanks!",,['probability']
67,A question about linear regression,A question about linear regression,,"Select $n$ numbers from a set $\{1,2,...,U\}$, $y_i$ is the $i$th number selected, and $x_i$ is the rank of $y_i$ in the $n$ numbers. The rank is the order of the a number after the $n$ numbers are sorted in ascending order. We can get $n$ data points $(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)$, And a best fit line for these data points can be found by linear regression.  $r_{xy}$ (correlation coefficient) is the goodness of the fit line, I want to calculate $E(r_{xy})$ or $E(r_{xy}^2)$ (correlation of determination).","Select $n$ numbers from a set $\{1,2,...,U\}$, $y_i$ is the $i$th number selected, and $x_i$ is the rank of $y_i$ in the $n$ numbers. The rank is the order of the a number after the $n$ numbers are sorted in ascending order. We can get $n$ data points $(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)$, And a best fit line for these data points can be found by linear regression.  $r_{xy}$ (correlation coefficient) is the goodness of the fit line, I want to calculate $E(r_{xy})$ or $E(r_{xy}^2)$ (correlation of determination).",,"['probability', 'combinatorics']"
68,Probability value in a skewed data,Probability value in a skewed data,,"A question goes like this: Given that there 2 questions out of 3 questions to pass an exam and follow are details of number of people who attended the questions correctly, what is the maximum number of people who could have qualified the exam. 1. 2 2 2 The answer here is 3. Consider ab bc ca. It can also be derived as 6/2 = (total number who cleared / number of questions which are required to clear). If I give another sample data 2. 245 272 238 Even here I can say floor(755/2) = 377 are the maximum number of people who have passed the exam. But consider this data. 3. 100 10 1 Now, the average formula would not work. By reasoning, I can say that 11 people could have passed the exam. But mathematically, how do I derive this as in for any M tests (and we are given the number of people have cleared those M tests indivually) and minimum requirement is N tests, how we can find the maximum number of people who could have cleared the exam. I hope, I explained it properly.","A question goes like this: Given that there 2 questions out of 3 questions to pass an exam and follow are details of number of people who attended the questions correctly, what is the maximum number of people who could have qualified the exam. 1. 2 2 2 The answer here is 3. Consider ab bc ca. It can also be derived as 6/2 = (total number who cleared / number of questions which are required to clear). If I give another sample data 2. 245 272 238 Even here I can say floor(755/2) = 377 are the maximum number of people who have passed the exam. But consider this data. 3. 100 10 1 Now, the average formula would not work. By reasoning, I can say that 11 people could have passed the exam. But mathematically, how do I derive this as in for any M tests (and we are given the number of people have cleared those M tests indivually) and minimum requirement is N tests, how we can find the maximum number of people who could have cleared the exam. I hope, I explained it properly.",,"['probability', 'average']"
69,Large Deviation Properties of a function of a geometric random variable,Large Deviation Properties of a function of a geometric random variable,,"Suppose I have a variable $s$ that has a geometric distribution with success parameter x. So the probability of success on  trial $s$ is $p_s = (1 - x)^{s - 1} x$, Consider the following function of $s$ $V(s) = \delta^s$, where $\delta < 1 $.  Then the expected value of this function is given by $\sum_ 1^{\infty} V(s) p_s = \frac{\delta x} {1 - \delta (1 - x)}$. Consider now the deviation of V(s) from its mean : $(V(s) - \frac{\delta x} {1 - \delta (1 - x)}) $ What distribution does this have? What I am really interested in is the behavior of the follow ordinary stochastic differential equation: $\dot{V} =  \delta^s  - V$, stochastic because the law of motion depends on the random variable $s$.  The stochastic approximation technique allows me to focus on the mean dynamics, which are given by $\dot{V} =  \frac{\delta x} {1 - \delta (1 - x)}  - V$, whose equilibrium is simply $\hat{V} = \frac{\delta x} {1 - \delta (1 - x)}$. I would like to characterize the large deviation properties of the original ODE, IE calculate the exponential likelihood $V$ crosses a particular boundary $c$. For example, if $\delta = .95$ and $x= .01$, $\hat{V} = 0.159664$.  If we set $V_0 = 0.159664$, and let $V_t = V_{t-1} + .2 (\delta^s - V_{t-1})$, and $s$ has the above distribution, how do I calculate the expected time to $V_t$ crossing, say, $c=.4$?  What is the associated rate function? Editing to give the background to the problem: I am interested in the following stochastic dynamic system: $V_t = V_{t-1} + \gamma (\delta^{S_{t-1}} - V_{t-1})$, where $\delta <1$, $\gamma<1$, both positive, and each $S_t$ is an i.i.d. geometric random variable with success parameter $x$.  What this models is agents who have to wait a geometric length of time to get a reward of value 1, and they have time discount factor $\delta$, and they are learning  the expected value of their reward using a constant gain adaptive  learning procedure, with gain $\gamma$. So considering $S$ as following a Poisson, rather than Geometric, makes little difference to the sense of the problem.  The goal is Find the equilibria of the learning dynamics Characterize the mean time to escape from this equilibrium, the large deviation properties. So, for large deviation properties, I should be able to use something like Cramer's theorem, I think.  As Mike says below, iterating the dynamics gives $V_t = (1−γ)^tV_0+γ\sum_{k=0}^{t−1}(1−γ)^{t−1−k}δ^{S_k}$ So it all depends on $\sum_{k=0}^{t−1}(1−γ)^{t−1−k}δ^{S_k}$, the discounted sum of some random variables. The Probability that $V_t >c$ is then given by $Pr(V_t > c ) = Pr(\sum_{k=0}^{t−1}(1−γ)^{t−1−k}δ^{S_k} > \frac{c-(1−γ)^tV_0}{\gamma})$ Let $Z:=\frac{c-(1−γ)^tV_0}{\gamma}$.  so we need the distribution of $\sum_{k=0}^{t−1}(1−γ)^{t−1−k}δ^{S_k}$, which as Mike says is a discounted sum of independent random variables, and so we have that this is approximately normal, for large $t$. Turning now to the question of large deviations, if things were not discounted by powers of $(1-\gamma)$, we could appeal directly to Cramer's Theorem; $ Pr(\sum \delta^{S_k} > n a) \leq Exp[-n(r^* a - Log(E[Exp(r^* \delta^S)])] $ where $r^* $ is chosen to maximize $r a - Log(E[Exp(r \delta^S)]$.  The problem would be to simply calculate the moment generating function of  $\delta^S$.  But, I don't exactly want $Pr(\sum \delta^{S_k} > n a)$, I need $ Pr(\sum (1-\gamma)^{t-1-k} \delta^{S_k} > a)$; so not an average of $\delta^{S_k}$s, but a discounted sum.  so it is not quite a direct application.","Suppose I have a variable $s$ that has a geometric distribution with success parameter x. So the probability of success on  trial $s$ is $p_s = (1 - x)^{s - 1} x$, Consider the following function of $s$ $V(s) = \delta^s$, where $\delta < 1 $.  Then the expected value of this function is given by $\sum_ 1^{\infty} V(s) p_s = \frac{\delta x} {1 - \delta (1 - x)}$. Consider now the deviation of V(s) from its mean : $(V(s) - \frac{\delta x} {1 - \delta (1 - x)}) $ What distribution does this have? What I am really interested in is the behavior of the follow ordinary stochastic differential equation: $\dot{V} =  \delta^s  - V$, stochastic because the law of motion depends on the random variable $s$.  The stochastic approximation technique allows me to focus on the mean dynamics, which are given by $\dot{V} =  \frac{\delta x} {1 - \delta (1 - x)}  - V$, whose equilibrium is simply $\hat{V} = \frac{\delta x} {1 - \delta (1 - x)}$. I would like to characterize the large deviation properties of the original ODE, IE calculate the exponential likelihood $V$ crosses a particular boundary $c$. For example, if $\delta = .95$ and $x= .01$, $\hat{V} = 0.159664$.  If we set $V_0 = 0.159664$, and let $V_t = V_{t-1} + .2 (\delta^s - V_{t-1})$, and $s$ has the above distribution, how do I calculate the expected time to $V_t$ crossing, say, $c=.4$?  What is the associated rate function? Editing to give the background to the problem: I am interested in the following stochastic dynamic system: $V_t = V_{t-1} + \gamma (\delta^{S_{t-1}} - V_{t-1})$, where $\delta <1$, $\gamma<1$, both positive, and each $S_t$ is an i.i.d. geometric random variable with success parameter $x$.  What this models is agents who have to wait a geometric length of time to get a reward of value 1, and they have time discount factor $\delta$, and they are learning  the expected value of their reward using a constant gain adaptive  learning procedure, with gain $\gamma$. So considering $S$ as following a Poisson, rather than Geometric, makes little difference to the sense of the problem.  The goal is Find the equilibria of the learning dynamics Characterize the mean time to escape from this equilibrium, the large deviation properties. So, for large deviation properties, I should be able to use something like Cramer's theorem, I think.  As Mike says below, iterating the dynamics gives $V_t = (1−γ)^tV_0+γ\sum_{k=0}^{t−1}(1−γ)^{t−1−k}δ^{S_k}$ So it all depends on $\sum_{k=0}^{t−1}(1−γ)^{t−1−k}δ^{S_k}$, the discounted sum of some random variables. The Probability that $V_t >c$ is then given by $Pr(V_t > c ) = Pr(\sum_{k=0}^{t−1}(1−γ)^{t−1−k}δ^{S_k} > \frac{c-(1−γ)^tV_0}{\gamma})$ Let $Z:=\frac{c-(1−γ)^tV_0}{\gamma}$.  so we need the distribution of $\sum_{k=0}^{t−1}(1−γ)^{t−1−k}δ^{S_k}$, which as Mike says is a discounted sum of independent random variables, and so we have that this is approximately normal, for large $t$. Turning now to the question of large deviations, if things were not discounted by powers of $(1-\gamma)$, we could appeal directly to Cramer's Theorem; $ Pr(\sum \delta^{S_k} > n a) \leq Exp[-n(r^* a - Log(E[Exp(r^* \delta^S)])] $ where $r^* $ is chosen to maximize $r a - Log(E[Exp(r \delta^S)]$.  The problem would be to simply calculate the moment generating function of  $\delta^S$.  But, I don't exactly want $Pr(\sum \delta^{S_k} > n a)$, I need $ Pr(\sum (1-\gamma)^{t-1-k} \delta^{S_k} > a)$; so not an average of $\delta^{S_k}$s, but a discounted sum.  so it is not quite a direct application.",,"['probability', 'probability-theory', 'ordinary-differential-equations', 'approximation', 'dynamical-systems']"
70,Estimating Population Growth with Limited Information,Estimating Population Growth with Limited Information,,"The bounty expires in 2 days . Answers to this question are eligible for a +50 reputation bounty. konofoso is looking for an answer from a reputable source . I have been thinking about these problems for a while and think I might have found a way to partly answer them. These problems deal with estimating the birth and death rate of a system in different scenarios. Scenario 1: Suppose at time=0 my population is $k_1$ . At some final time =t, my population is $k_2$ ( $k_1, k_2$ are positive integers) At each time point from (0,T), I assume that there is a probability of $p_1$ that the current population can increase by $j$ units and a probability of $p_2$ that the current population decrease by $l$ units (assume $p_1, p_2, j, l$ are all constant) I only observe the system at time=0 and time=t  (i.e. I know the exact values of $k_1, k_2$ . Assuming that the population can never go below 0, what are the most likely values of $p_1, p_2, j, l$ ? Scenario 2: Suppose at time=0 my population is $k_1$ . At some final time =t, my population is $k_2$ ( $k_1, k_2$ are positive integers) At each time point from (0,T), I assume that there is a probability of $p_1$ that the current population can increase by $j$ units and a probability of $p_2$ that the current population decrease by $l$ units (assume $p_1, p_2, j, l$ are all constant) I observe the population of time =0 and time=T ... as well as at some times $t_i$ (e.g. suppose I have intervals 0,1,2,3,4,5,6,7,8,9,10. I observe the population at times=0, 5,6,9,10 ... at time=0 population is $k_1$ , at time=5 population is $a$ , time= 6 population is $b$ , time = 9 population is $c$ , time=10 population is $k_2$ ) Assuming that the population can never go below 0, what are the most likely values of $p_1, p_2, j, l$ ? My Answers: For Scenario 1, I used a latent variable approach (e.g. EM algorithm/Gaussian Mixture) wrote: $$ L(p_1, p_2, j, l) = \sum_{j=0}^{J} \sum_{l=0}^{L} \sum_{n_1=0}^{T} \sum_{n_2=0}^{T} \left[ p_1^{n_1} \cdot (1 - p_1)^{T - n_1} \cdot p_2^{n_2} \cdot (1 - p_2)^{T - n_2} \right] \cdot I(n_1 \cdot j - n_2 \cdot l = k_2 - k_1) $$ $p_1^{n_1}$ represents the probability of $n_1$ increases in the population. $p_2^{n_2}$ represents the probability of $n_2$ decreases in the population. $(1 - p_1 - p_2)^{T - n_1 - n_2}$ represents the probability of the remaining time intervals during which the population neither increases nor decreases. This happens with probability $1 - p_1 - p_2$ , and there are $T - n_1 - n_2$ such intervals. $I(n_1 \cdot j - n_2 \cdot l = k_2 - k_1)$ is an indicator function that takes values 1 if the condition is true else 0. This is to ensure that the total number of decreases and increases respect the initial and final population However, I am not sure if this Likelihood Function prevents the population from going below 0 and some intermediate time point. I am also not sure if a combinatorial/multinomial term is needed in the likelihood I think its more difficult to write the likelihood function for Scenario 2 , even though Scenario 2 has more information compared to Scenario 1. I think this question can be broken down into two parts: A likelihood function for the times where we have full information, and a likelihood function for the times where we have missing information: $$ L(p_1, p_2, j, l) = L_{obs}(p_1, p_2, j, l) \cdot L_{unobs}(p_1, p_2, j, l) $$ From this point on, I think we can use a similar approach as in Scenario 1: $$ L_{obs}(p_1, p_2, j, l) = \prod_{i=0}^{N_{obs}-1} \sum_{n_1=0}^{t_{o_{i+1}} - t_{o_i}} \sum_{n_2=0}^{t_{o_{i+1}} - t_{o_i}} \sum_{j=0}^{J} \sum_{l=0}^{L} \left[ p_1^{n_1} \cdot (1 - p_1)^{t_{o_{i+1}} - t_{o_i - n_1}} \cdot p_2^{n_2} \cdot (1 - p_2)^{t_{o_{i+1}} - t_{o_i - n_2}} \right] \cdot I(n_1 \cdot j - n_2 \cdot l = k_{t_{o_{i+1}}} - k_{t_{o_i}}) $$ $$ L_{unobs}(p_1, p_2, j, l) = \prod_{i=0}^{N_{unobs}-1} \sum_{n_1=0}^{t_{u_{i+1}} - t_{u_i}} \sum_{n_2=0}^{t_{u_{i+1}} - t_{u_i}} \sum_{j=0}^{J} \sum_{l=0}^{L} \left[ p_1^{n_1} \cdot (1 - p_1)^{t_{u_{i+1}} - t_{u_i - n_1}} \cdot p_2^{n_2} \cdot (1 - p_2)^{t_{u_{i+1}} - t_{u_i - n_2}} \right] \cdot I(n_1 \cdot j - n_2 \cdot l = k_{t_{u_{i+1}}} - k_{t_{u_i}}) $$ I have been thinking about how to correctly write the likelihood functions for both Scenario 1 and Scenario 2 for quite some time and find myself getting lost/confused. Can someone please help me write these correctly?","The bounty expires in 2 days . Answers to this question are eligible for a +50 reputation bounty. konofoso is looking for an answer from a reputable source . I have been thinking about these problems for a while and think I might have found a way to partly answer them. These problems deal with estimating the birth and death rate of a system in different scenarios. Scenario 1: Suppose at time=0 my population is . At some final time =t, my population is ( are positive integers) At each time point from (0,T), I assume that there is a probability of that the current population can increase by units and a probability of that the current population decrease by units (assume are all constant) I only observe the system at time=0 and time=t  (i.e. I know the exact values of . Assuming that the population can never go below 0, what are the most likely values of ? Scenario 2: Suppose at time=0 my population is . At some final time =t, my population is ( are positive integers) At each time point from (0,T), I assume that there is a probability of that the current population can increase by units and a probability of that the current population decrease by units (assume are all constant) I observe the population of time =0 and time=T ... as well as at some times (e.g. suppose I have intervals 0,1,2,3,4,5,6,7,8,9,10. I observe the population at times=0, 5,6,9,10 ... at time=0 population is , at time=5 population is , time= 6 population is , time = 9 population is , time=10 population is ) Assuming that the population can never go below 0, what are the most likely values of ? My Answers: For Scenario 1, I used a latent variable approach (e.g. EM algorithm/Gaussian Mixture) wrote: represents the probability of increases in the population. represents the probability of decreases in the population. represents the probability of the remaining time intervals during which the population neither increases nor decreases. This happens with probability , and there are such intervals. is an indicator function that takes values 1 if the condition is true else 0. This is to ensure that the total number of decreases and increases respect the initial and final population However, I am not sure if this Likelihood Function prevents the population from going below 0 and some intermediate time point. I am also not sure if a combinatorial/multinomial term is needed in the likelihood I think its more difficult to write the likelihood function for Scenario 2 , even though Scenario 2 has more information compared to Scenario 1. I think this question can be broken down into two parts: A likelihood function for the times where we have full information, and a likelihood function for the times where we have missing information: From this point on, I think we can use a similar approach as in Scenario 1: I have been thinking about how to correctly write the likelihood functions for both Scenario 1 and Scenario 2 for quite some time and find myself getting lost/confused. Can someone please help me write these correctly?","k_1 k_2 k_1, k_2 p_1 j p_2 l p_1, p_2, j, l k_1, k_2 p_1, p_2, j, l k_1 k_2 k_1, k_2 p_1 j p_2 l p_1, p_2, j, l t_i k_1 a b c k_2 p_1, p_2, j, l  L(p_1, p_2, j, l) = \sum_{j=0}^{J} \sum_{l=0}^{L} \sum_{n_1=0}^{T} \sum_{n_2=0}^{T} \left[ p_1^{n_1} \cdot (1 - p_1)^{T - n_1} \cdot p_2^{n_2} \cdot (1 - p_2)^{T - n_2} \right] \cdot I(n_1 \cdot j - n_2 \cdot l = k_2 - k_1)  p_1^{n_1} n_1 p_2^{n_2} n_2 (1 - p_1 - p_2)^{T - n_1 - n_2} 1 - p_1 - p_2 T - n_1 - n_2 I(n_1 \cdot j - n_2 \cdot l = k_2 - k_1)  L(p_1, p_2, j, l) = L_{obs}(p_1, p_2, j, l) \cdot L_{unobs}(p_1, p_2, j, l)   L_{obs}(p_1, p_2, j, l) = \prod_{i=0}^{N_{obs}-1} \sum_{n_1=0}^{t_{o_{i+1}} - t_{o_i}} \sum_{n_2=0}^{t_{o_{i+1}} - t_{o_i}} \sum_{j=0}^{J} \sum_{l=0}^{L} \left[ p_1^{n_1} \cdot (1 - p_1)^{t_{o_{i+1}} - t_{o_i - n_1}} \cdot p_2^{n_2} \cdot (1 - p_2)^{t_{o_{i+1}} - t_{o_i - n_2}} \right] \cdot I(n_1 \cdot j - n_2 \cdot l = k_{t_{o_{i+1}}} - k_{t_{o_i}})   L_{unobs}(p_1, p_2, j, l) = \prod_{i=0}^{N_{unobs}-1} \sum_{n_1=0}^{t_{u_{i+1}} - t_{u_i}} \sum_{n_2=0}^{t_{u_{i+1}} - t_{u_i}} \sum_{j=0}^{J} \sum_{l=0}^{L} \left[ p_1^{n_1} \cdot (1 - p_1)^{t_{u_{i+1}} - t_{u_i - n_1}} \cdot p_2^{n_2} \cdot (1 - p_2)^{t_{u_{i+1}} - t_{u_i - n_2}} \right] \cdot I(n_1 \cdot j - n_2 \cdot l = k_{t_{u_{i+1}}} - k_{t_{u_i}}) ","['probability', 'combinatorics']"
71,Why don't we apply the principle of indifference here?,Why don't we apply the principle of indifference here?,,"The Problem You and friend play a game where you both select an integer 1-100. The winner receives 1$ from the loser. The winner is the one who chooses a number that is either (i) exactly two less than the other player's number, or (ii) strictly greater than the other player's number (if they choose a number that is two less than your, they win) Assume you and your friend play optimally. What is the optimal strategy here, and what is $Var(X)$ , where $X$ is the random value you choose according to this optimal distribution? My Questions (i) I haven't yet been exposed to too many of these questions regarding ""optimal strategies"". So I'm a little unsure on how to think when I see one. Are these really probability questions? If so, can someone tell me in a general way how I should think about these kinds of problems? (ii) The only question I've seen that was similar to this one (in that it was an ""optimal strategy"" question as well) was the popular problem 3.4 in Mark Joshi's book ( Fail to understand 'The indifference criterion means that $1p_1=2p_2=3p_3$.' ). It utilized the indifference criterion. Why can't the same principle be applied here? What's different between that problem and this problem? In case you don't know which problem I'm talking about, I will reproduce it here: We play a game: I pick a number from 1 to 100. If you guess correctly, I pay you $n and zero otherwise. How much would you pay to play this game?","The Problem You and friend play a game where you both select an integer 1-100. The winner receives 1$ from the loser. The winner is the one who chooses a number that is either (i) exactly two less than the other player's number, or (ii) strictly greater than the other player's number (if they choose a number that is two less than your, they win) Assume you and your friend play optimally. What is the optimal strategy here, and what is , where is the random value you choose according to this optimal distribution? My Questions (i) I haven't yet been exposed to too many of these questions regarding ""optimal strategies"". So I'm a little unsure on how to think when I see one. Are these really probability questions? If so, can someone tell me in a general way how I should think about these kinds of problems? (ii) The only question I've seen that was similar to this one (in that it was an ""optimal strategy"" question as well) was the popular problem 3.4 in Mark Joshi's book ( Fail to understand 'The indifference criterion means that $1p_1=2p_2=3p_3$.' ). It utilized the indifference criterion. Why can't the same principle be applied here? What's different between that problem and this problem? In case you don't know which problem I'm talking about, I will reproduce it here: We play a game: I pick a number from 1 to 100. If you guess correctly, I pay you $n and zero otherwise. How much would you pay to play this game?",Var(X) X,"['probability', 'game-theory']"
72,Existence of a uniform measure on the group of permutation of $\mathbb N.$,Existence of a uniform measure on the group of permutation of,\mathbb N.,"If $G$ is the group of permutations of $\mathbb N,$ is there a non-trivial probability measure $\mu$ on $G$ such that, when $S\subset G$ is measurable, and $g\in G,$ then $gS=\{gs\mid s\in S\}$ is also measurable and $\mu(S)=\mu(gS).$ By ""non-trivial,"" I am hoping for at least one measurable set which is neither countable nor co-countable. A measurable set with measure strictly between $0$ and $1.$ All of my attempts reduce to finding a uniform probability measure on $\mathbb N,$ which is impossible. In particular, if you treat $G$ as a subset of $\mathbb N^{\mathbb N}$ with the product topology, and treat the basis of your $\sigma$ -algebra as the open sets, then you get $S_{n,m}=\{\pi\in G\mid \pi(n)=m\}$ must all have the same measure, and $\bigcup_m S_{0.m}=G$ is a disjoint union, which is impossible. So we'd need some different $\sigma-$ algebra. If there isn't a probability measure, maybe there is a non-trivial uniform measure with $\mu(\{g\})=0$ for all $g\in G$ ? We'd still need $S_{m,n}$ either unmeasurable or have infinite measure for all $n,m.$ Otherwise, we'd have $$S_{0,0}=\bigcup_{n\geq 1} \left(S_{0,0}\cap S_{1,n}\right)$$ would reach the same contradiction - a uniform probability measure on $\mathbb N.$ Indeed, we'd need any open set from the subspace of the product topology on $\mathbb N^{\mathbb N}$ to be unmeasurable or have infinite measure.","If is the group of permutations of is there a non-trivial probability measure on such that, when is measurable, and then is also measurable and By ""non-trivial,"" I am hoping for at least one measurable set which is neither countable nor co-countable. A measurable set with measure strictly between and All of my attempts reduce to finding a uniform probability measure on which is impossible. In particular, if you treat as a subset of with the product topology, and treat the basis of your -algebra as the open sets, then you get must all have the same measure, and is a disjoint union, which is impossible. So we'd need some different algebra. If there isn't a probability measure, maybe there is a non-trivial uniform measure with for all ? We'd still need either unmeasurable or have infinite measure for all Otherwise, we'd have would reach the same contradiction - a uniform probability measure on Indeed, we'd need any open set from the subspace of the product topology on to be unmeasurable or have infinite measure.","G \mathbb N, \mu G S\subset G g\in G, gS=\{gs\mid s\in S\} \mu(S)=\mu(gS). 0 1. \mathbb N, G \mathbb N^{\mathbb N} \sigma S_{n,m}=\{\pi\in G\mid \pi(n)=m\} \bigcup_m S_{0.m}=G \sigma- \mu(\{g\})=0 g\in G S_{m,n} n,m. S_{0,0}=\bigcup_{n\geq 1} \left(S_{0,0}\cap S_{1,n}\right) \mathbb N. \mathbb N^{\mathbb N}","['probability', 'measure-theory', 'topological-groups']"
73,Prove a Result about Optimization and Exponential Distribution: Intuitive but Challenging,Prove a Result about Optimization and Exponential Distribution: Intuitive but Challenging,,"Let $v=(v_1,v_2,...v_N)$ be a random vector with $N$ elements. $v_i \perp v_j$ for any $i\neq j$ . For each $i$ , the CDF of $v_i$ is: $F_i(v_i)=1-e^{-(v_i-a_i)/\theta}$ , where $v_i\in [a_i,\infty)$ . Assume $0<\theta<a_1<a_2<...<a_N$ and $c>0$ . Let $x(v)=(x_1(v),x_2(v)...x_N(v))$ be the solution of the following problem \begin{equation} \max_{x(v)} \sum_{i=1}^N x_i (v_i-\theta- cx_i) \end{equation} subject to $x_i\geq 0$ for all $i$ , and $\sum_{i=1}^{N}x_i\leq1$ . Since $v$ is random, $x(v)$ is also a random vector. Prove : $\mathbb{E}(x_N)-\mathbb{E}(x_1)$ is a decreasing function of $c$ . My attempts : This result is quite intuitive and I've verified using MATLAB for different parameters. If $N=2$ , this can be proved. The solution for each $x_i$ is the following: if $\sum_{i=1}^N \frac{v_i-\theta}{2c}\leq1$ , then $x_i=\frac{v_i-\theta}{2c}$ ; if $\sum_{i=1}^N \frac{v_i-\theta}{2c}>1$ , then $x_i=\frac{[v_i-\theta-\lambda(v)]^+}{2c}$ , and $\sum_{i=1}^N x_i=1$ for some Lagrange multiplier $\lambda(v)>0$ . Note that $\lambda$ is symmetric, i.e., $\lambda(v_i,v_j,v_{-{i,j}})$ = $\lambda(v_j,v_i,v_{-{i,j}})$ for all $i\neq j$ . I think the challenge is the $[\quad]^+$ sign in the solution of $x_i$ , if we remove this, the result is obviuosly true. But the $[\quad]^+$ sign makes it messy. Maybe Mathematical Induction is useful, I've tried, but still didn't prove it.","Let be a random vector with elements. for any . For each , the CDF of is: , where . Assume and . Let be the solution of the following problem subject to for all , and . Since is random, is also a random vector. Prove : is a decreasing function of . My attempts : This result is quite intuitive and I've verified using MATLAB for different parameters. If , this can be proved. The solution for each is the following: if , then ; if , then , and for some Lagrange multiplier . Note that is symmetric, i.e., = for all . I think the challenge is the sign in the solution of , if we remove this, the result is obviuosly true. But the sign makes it messy. Maybe Mathematical Induction is useful, I've tried, but still didn't prove it.","v=(v_1,v_2,...v_N) N v_i \perp v_j i\neq j i v_i F_i(v_i)=1-e^{-(v_i-a_i)/\theta} v_i\in [a_i,\infty) 0<\theta<a_1<a_2<...<a_N c>0 x(v)=(x_1(v),x_2(v)...x_N(v)) \begin{equation}
\max_{x(v)} \sum_{i=1}^N x_i (v_i-\theta- cx_i)
\end{equation} x_i\geq 0 i \sum_{i=1}^{N}x_i\leq1 v x(v) \mathbb{E}(x_N)-\mathbb{E}(x_1) c N=2 x_i \sum_{i=1}^N \frac{v_i-\theta}{2c}\leq1 x_i=\frac{v_i-\theta}{2c} \sum_{i=1}^N \frac{v_i-\theta}{2c}>1 x_i=\frac{[v_i-\theta-\lambda(v)]^+}{2c} \sum_{i=1}^N x_i=1 \lambda(v)>0 \lambda \lambda(v_i,v_j,v_{-{i,j}}) \lambda(v_j,v_i,v_{-{i,j}}) i\neq j [\quad]^+ x_i [\quad]^+","['probability', 'convex-optimization', 'exponential-distribution']"
74,Is there a way to calculate this probability cleanly without brute force?,Is there a way to calculate this probability cleanly without brute force?,,"Let $L$ and $a$ be positive integer constants. I will flip a coin $L$ times. Let $n_1, \ldots, n_s$ be the count of consecutive runs of the same flip. For example, if $L = 5$ and I flipped $HHTHT$ , then $n_1 = 2, n_2 = 1, n_3 = 1, n_4 = 1$ . Note $L = \sum n_i$ . Let $X$ denote the random variable representing $(n_1 + 1) (n_2 + 1) \cdots (n_s+1) - 1$ . What is $Pr(X < a)$ in terms of a and L? I would prefer something of the form $Pr(X < a) = $ , but if it is a bound (please no Markov lol) that's also fine.","Let and be positive integer constants. I will flip a coin times. Let be the count of consecutive runs of the same flip. For example, if and I flipped , then . Note . Let denote the random variable representing . What is in terms of a and L? I would prefer something of the form , but if it is a bound (please no Markov lol) that's also fine.","L a L n_1, \ldots, n_s L = 5 HHTHT n_1 = 2, n_2 = 1, n_3 = 1, n_4 = 1 L = \sum n_i X (n_1 + 1) (n_2 + 1) \cdots (n_s+1) - 1 Pr(X < a) Pr(X < a) = ","['probability', 'combinatorics', 'random-variables']"
75,Solving the Secretary Problem using Simple Math,Solving the Secretary Problem using Simple Math,,"In our computer coding class, we learned about the Secretary Problem ( https://en.wikipedia.org/wiki/Secretary_problem ). The goal of this problem is to find out the ""optimal cutoff"" for an interview process: What is the least number of candidates you need to interview that maximizes your chance of finding the best candidate? If there are $n$ candidates (assume their skill level is randomly distributed) and you rank the first $1/e$ % of candidates (i.e 37%) and then pick the candidate better than the best of the $1/e$ set, there is a $1/e$ % chance you will pick the best candidate in total. We learned how to write a simulation for this: library(ggplot2) n <- 100 n_simulations <- 1000 first_set_size <- round(n / exp(1)) results <- data.frame(iteration = integer(), category = character(), candidate = integer())  for (i in 1:n_simulations) {     candidates <- sample(1:n)     first_set <- candidates[1:first_set_size]     remaining_set <- candidates[(first_set_size + 1):n]          best_in_first_set <- max(first_set)     stopping_rule_candidate <- remaining_set[which.max(remaining_set > best_in_first_set)]          results <- rbind(results,                       data.frame(iteration = i, category = ""Best Candidate"", candidate = n),                      data.frame(iteration = i, category = ""Best in First Set"", candidate = best_in_first_set),                      data.frame(iteration = i, category = ""Stopping Rule Candidate"", candidate = stopping_rule_candidate)) }   average_best_picked <- mean(results $candidate[results$ category == ""Stopping Rule Candidate""] == n)   ggplot(results, aes(x = iteration, y = candidate, color = category)) +     geom_jitter(width = 0.3) +     labs(x = ""Iteration"", y = ""Candidate Number"",           title = paste(""Average times best candidate picked:"", round(average_best_picked, 2)),          colour = ""Legend"") +  # Change the legend title here     scale_color_manual(values = c(""Best Candidate"" = ""red"", ""Best in First Set"" = ""blue"", ""Stopping Rule Candidate"" = ""green"")) +     theme_minimal() +     theme(legend.position = ""bottom"") I am now trying to understand the mathematical solution to this problem but I am getting confused how to do this. I came up with the following logic for $N$ candidates. Suppose we have an integer number line from $0$ to $N$ that looks like this: $$[(0,R) , (R+1, N)]$$ If we find the 2nd best candidate between $(0,R)$ , we are guaranteed to find the best candidate between $(R,N)$ . Condition 1: Relative to the entire set of candidates, the probability of finding the 2nd best candidate between $(0,R)$ is $\frac{R}{N}$ Condition 2: The probability of finding the overall best candidate between $(R+1, N)$ is $1 - \frac{R}{N}$ . My logic is that $\frac{R}{N}$ is the probability of finding the overall best candidate in $R$ trials. So the probability of finding the overall best candidate outside of this set is $1 - \frac{R}{N}$ To find the overall best candidate according to the rules of this problem, both of these conditions have to be simultaneously true (i.e. multiply them by each other, joint probability) Our goal is now to find out the optimal $R$ . Let's call this optimal value as $R^*$ . We can write a sum to express this as: $$ P(R = R^*) = \sum_{R} \left[ \frac{R}{N} \cdot (1- \frac{R}{N} \right)] $$ However from here I am stuck. I think a derivative needs to be taken to find out which value of $R$ optimizes this equation. But I am not sure how to then prove that there will be a $1/e$ chance of finding the best candidate after reject the first $1/e$ % of all candidates. Any ideas what I can do? PS: Our prof showed us a simulation which shows what happens when you try different ""Stopping Rules"" (we can see that they are all suboptimal compared to 1/e): n <- 100 n_simulations <- 10000 results <- data.frame(stopping_rule = integer(), success_rate = numeric())  for (rule in 1:100) {     success_count <- 0     for (i in 1:n_simulations) {         candidates <- sample(1:n)         first_set_size <- round(n * rule / 100)         first_set <- candidates[1:first_set_size]         remaining_set <- candidates[(first_set_size + 1):n]                  best_in_first_set <- max(first_set)         stopping_rule_candidate <- remaining_set[which.max(remaining_set > best_in_first_set)]                  if (!is.na(stopping_rule_candidate) && stopping_rule_candidate == n) {             success_count <- success_count + 1         }     }     success_rate <- success_count / n_simulations     results <- rbind(results, data.frame(stopping_rule = rule, success_rate = success_rate)) }  best_rule <- results $stopping_rule[which.max(results$ success_rate)] best_rate <- max(results$success_rate)   results $color <- ifelse(results$ stopping_rule == best_rule, ""red"", ""black"")  ggplot(results, aes(x = stopping_rule, y = success_rate, color = color)) +     geom_point() +     geom_text(data = subset(results, stopping_rule == best_rule),                aes(label = paste(""Best Rule: "", best_rule, ""%"")),                vjust = -1) +     scale_color_identity() +     labs(x = ""Stopping Rule (%)"", y = ""Success Rate"",          title = paste(""Success Rate of Different Stopping Rules\nBest Rule: "", best_rule, ""% with Success Rate: "", round(best_rate, 2))) +     theme_minimal()","In our computer coding class, we learned about the Secretary Problem ( https://en.wikipedia.org/wiki/Secretary_problem ). The goal of this problem is to find out the ""optimal cutoff"" for an interview process: What is the least number of candidates you need to interview that maximizes your chance of finding the best candidate? If there are candidates (assume their skill level is randomly distributed) and you rank the first % of candidates (i.e 37%) and then pick the candidate better than the best of the set, there is a % chance you will pick the best candidate in total. We learned how to write a simulation for this: library(ggplot2) n <- 100 n_simulations <- 1000 first_set_size <- round(n / exp(1)) results <- data.frame(iteration = integer(), category = character(), candidate = integer())  for (i in 1:n_simulations) {     candidates <- sample(1:n)     first_set <- candidates[1:first_set_size]     remaining_set <- candidates[(first_set_size + 1):n]          best_in_first_set <- max(first_set)     stopping_rule_candidate <- remaining_set[which.max(remaining_set > best_in_first_set)]          results <- rbind(results,                       data.frame(iteration = i, category = ""Best Candidate"", candidate = n),                      data.frame(iteration = i, category = ""Best in First Set"", candidate = best_in_first_set),                      data.frame(iteration = i, category = ""Stopping Rule Candidate"", candidate = stopping_rule_candidate)) }   average_best_picked <- mean(results category == ""Stopping Rule Candidate""] == n)   ggplot(results, aes(x = iteration, y = candidate, color = category)) +     geom_jitter(width = 0.3) +     labs(x = ""Iteration"", y = ""Candidate Number"",           title = paste(""Average times best candidate picked:"", round(average_best_picked, 2)),          colour = ""Legend"") +  # Change the legend title here     scale_color_manual(values = c(""Best Candidate"" = ""red"", ""Best in First Set"" = ""blue"", ""Stopping Rule Candidate"" = ""green"")) +     theme_minimal() +     theme(legend.position = ""bottom"") I am now trying to understand the mathematical solution to this problem but I am getting confused how to do this. I came up with the following logic for candidates. Suppose we have an integer number line from to that looks like this: If we find the 2nd best candidate between , we are guaranteed to find the best candidate between . Condition 1: Relative to the entire set of candidates, the probability of finding the 2nd best candidate between is Condition 2: The probability of finding the overall best candidate between is . My logic is that is the probability of finding the overall best candidate in trials. So the probability of finding the overall best candidate outside of this set is To find the overall best candidate according to the rules of this problem, both of these conditions have to be simultaneously true (i.e. multiply them by each other, joint probability) Our goal is now to find out the optimal . Let's call this optimal value as . We can write a sum to express this as: However from here I am stuck. I think a derivative needs to be taken to find out which value of optimizes this equation. But I am not sure how to then prove that there will be a chance of finding the best candidate after reject the first % of all candidates. Any ideas what I can do? PS: Our prof showed us a simulation which shows what happens when you try different ""Stopping Rules"" (we can see that they are all suboptimal compared to 1/e): n <- 100 n_simulations <- 10000 results <- data.frame(stopping_rule = integer(), success_rate = numeric())  for (rule in 1:100) {     success_count <- 0     for (i in 1:n_simulations) {         candidates <- sample(1:n)         first_set_size <- round(n * rule / 100)         first_set <- candidates[1:first_set_size]         remaining_set <- candidates[(first_set_size + 1):n]                  best_in_first_set <- max(first_set)         stopping_rule_candidate <- remaining_set[which.max(remaining_set > best_in_first_set)]                  if (!is.na(stopping_rule_candidate) && stopping_rule_candidate == n) {             success_count <- success_count + 1         }     }     success_rate <- success_count / n_simulations     results <- rbind(results, data.frame(stopping_rule = rule, success_rate = success_rate)) }  best_rule <- results success_rate)] best_rate <- max(results$success_rate)   results stopping_rule == best_rule, ""red"", ""black"")  ggplot(results, aes(x = stopping_rule, y = success_rate, color = color)) +     geom_point() +     geom_text(data = subset(results, stopping_rule == best_rule),                aes(label = paste(""Best Rule: "", best_rule, ""%"")),                vjust = -1) +     scale_color_identity() +     labs(x = ""Stopping Rule (%)"", y = ""Success Rate"",          title = paste(""Success Rate of Different Stopping Rules\nBest Rule: "", best_rule, ""% with Success Rate: "", round(best_rate, 2))) +     theme_minimal()","n 1/e 1/e 1/e candidate[results N 0 N [(0,R) , (R+1, N)] (0,R) (R,N) (0,R) \frac{R}{N} (R+1, N) 1 - \frac{R}{N} \frac{R}{N} R 1 - \frac{R}{N} R R^*  P(R = R^*) = \sum_{R} \left[ \frac{R}{N} \cdot (1- \frac{R}{N} \right)]  R 1/e 1/e stopping_rule[which.max(results color <- ifelse(results","['calculus', 'probability', 'derivatives', 'optimization']"
76,A question involving the expectation of the random variable $2^X$,A question involving the expectation of the random variable,2^X,"Let $X$ be a random variable with mgf $M_X(t)=a+be^{2t}$ .Given that mean of the random variable $X$ is $1.5$ . Then find the expected value of the random variable $2^X.$ My attempt: We know that $E(X^r)=\frac{d^r}{dt^r}M_X(t) \rvert_{t=0}$ . So, we have that $E(X) = 2b = 1.5$ . This means that $b=0.75$ . Also, we know that $M_X(0)=1.$ So, we get $a=0.25$ . I am now going to do some operation here. I want to know whether it is valid or not. Now, set $t= \ln 2$ . So, we have that $e^{(\ln 2) X} = \big( e^{\ln2 } \big)^X = 2^X.$ From the definition $M_X(t)=E(e^{tX})$ , we get $M_X(\ln 2) = E(e^{\ln 2 X}) =  E(2^X) = 0.25 + 0.75 e ^ {2 \times \ln 2} = 3.25$ . Hence, we have that $E(2^X)=3.25$ . Have I gone wrong somewhere ? If yes, please let me know where I have gone wrong and what is the correct approach. If not, then suggest what property of the random variables or distributions I am using here. Please help. Thank you very much.","Let be a random variable with mgf .Given that mean of the random variable is . Then find the expected value of the random variable My attempt: We know that . So, we have that . This means that . Also, we know that So, we get . I am now going to do some operation here. I want to know whether it is valid or not. Now, set . So, we have that From the definition , we get . Hence, we have that . Have I gone wrong somewhere ? If yes, please let me know where I have gone wrong and what is the correct approach. If not, then suggest what property of the random variables or distributions I am using here. Please help. Thank you very much.",X M_X(t)=a+be^{2t} X 1.5 2^X. E(X^r)=\frac{d^r}{dt^r}M_X(t) \rvert_{t=0} E(X) = 2b = 1.5 b=0.75 M_X(0)=1. a=0.25 t= \ln 2 e^{(\ln 2) X} = \big( e^{\ln2 } \big)^X = 2^X. M_X(t)=E(e^{tX}) M_X(\ln 2) = E(e^{\ln 2 X}) =  E(2^X) = 0.25 + 0.75 e ^ {2 \times \ln 2} = 3.25 E(2^X)=3.25,"['probability', 'random-variables', 'moment-generating-functions']"
77,Calculating Expected Throws for All Die Faces to Appear Twice [duplicate],Calculating Expected Throws for All Die Faces to Appear Twice [duplicate],,"This question already has answers here : Expectation time for at least 2 balls in each bin (1 answer) Coupon collector problem for collecting set k times. (2 answers) Closed 4 months ago . I'm working on a problem where I need to find the expected number of throws required for each face of a six-sided die to appear at least twice. I've conceptualized the problem using a triplet $(a, b, c)$ to represent the state of the system: $a$ : numbers drawn twice, $b$ : numbers drawn once, $c$ : numbers not drawn, with the initial state being $(0, 0, 6)$ . The transitions are: From $c$ to $b$ with probability $\frac{c}{6}$ , From $b$ to $a$ with probability $\frac{b}{6}$ . And then you can use brute force and write an algorithm to solve it using DP. But I’m wondering if there is any clever method. I know a lot of theorems have been studied on coupon collections issues. But this is a particular simple example so I’m curious about some simpler but clever strategy free of those existing theories that are intended for a more general scenario.","This question already has answers here : Expectation time for at least 2 balls in each bin (1 answer) Coupon collector problem for collecting set k times. (2 answers) Closed 4 months ago . I'm working on a problem where I need to find the expected number of throws required for each face of a six-sided die to appear at least twice. I've conceptualized the problem using a triplet to represent the state of the system: : numbers drawn twice, : numbers drawn once, : numbers not drawn, with the initial state being . The transitions are: From to with probability , From to with probability . And then you can use brute force and write an algorithm to solve it using DP. But I’m wondering if there is any clever method. I know a lot of theorems have been studied on coupon collections issues. But this is a particular simple example so I’m curious about some simpler but clever strategy free of those existing theories that are intended for a more general scenario.","(a, b, c) a b c (0, 0, 6) c b \frac{c}{6} b a \frac{b}{6}","['probability', 'combinatorics', 'expected-value', 'dice', 'coupon-collector']"
78,Probability that one random number among many has a unique prime factor,Probability that one random number among many has a unique prime factor,,"If I sample $N+1$ integers $x, x_1, \ldots, x_N$ uniformly and independently from $\{1, \ldots, M=2^k\}$ , what is the probability that $x$ contains a prime divisor that does not divide any of the $\{x_i\}$ ? Upper/lower bounds and/or asymptotic approximations would also be of interest. For prime $p$ and $x, x_1, \ldots, x_N$ as above, say $x$ is $p$ -unique if $p | x$ and $p \nmid x_1, \ldots, p \nmid x_N$ . For any fixed and sufficiently small prime $p$ (say, $p \leq \sqrt{M}$ ), the probability that $x$ is $p$ -unique is $\frac{1}{p} \cdot \left(1-\frac{1}{p}\right)^N$ . One can try to lower bound $\Pr[\vee_p \hspace{2pt} \mbox{$x$ is $p$-unique}]$ using the inclusion-exclusion principle and treating the events that $x$ is $p_1$ -unique and $p_2$ -unique as independent when $p_1, p_2$ are both sufficiently small (and distinct). Doing so gives a lower bound of $\sum_{p} \frac{1}{p} \left(1-\frac{1}{p}\right)^N - \sum_{p_1 \neq p_2} \frac{1}{p_1p_2} \left(1-\frac{1}{p_1}\right)^N \left(1-\frac{1}{p_2}\right)^N$ (where the sums are over sufficiently small primes). But I didn't know where to go from there. This seems like a problem that would have been studied before in the context of multiplicative number theory but I have not managed to find any references.","If I sample integers uniformly and independently from , what is the probability that contains a prime divisor that does not divide any of the ? Upper/lower bounds and/or asymptotic approximations would also be of interest. For prime and as above, say is -unique if and . For any fixed and sufficiently small prime (say, ), the probability that is -unique is . One can try to lower bound using the inclusion-exclusion principle and treating the events that is -unique and -unique as independent when are both sufficiently small (and distinct). Doing so gives a lower bound of (where the sums are over sufficiently small primes). But I didn't know where to go from there. This seems like a problem that would have been studied before in the context of multiplicative number theory but I have not managed to find any references.","N+1 x, x_1, \ldots, x_N \{1, \ldots, M=2^k\} x \{x_i\} p x, x_1, \ldots, x_N x p p | x p \nmid x_1, \ldots, p \nmid x_N p p \leq \sqrt{M} x p \frac{1}{p} \cdot \left(1-\frac{1}{p}\right)^N \Pr[\vee_p \hspace{2pt} \mbox{x is p-unique}] x p_1 p_2 p_1, p_2 \sum_{p} \frac{1}{p} \left(1-\frac{1}{p}\right)^N - \sum_{p_1 \neq p_2} \frac{1}{p_1p_2} \left(1-\frac{1}{p_1}\right)^N \left(1-\frac{1}{p_2}\right)^N","['probability', 'number-theory', 'prime-numbers', 'analytic-number-theory', 'multiplicative-function']"
79,Probability of Streaks of length $\lg n - 2 \lg \lg n$ in a Fair Coin [CLRS],Probability of Streaks of length  in a Fair Coin [CLRS],\lg n - 2 \lg \lg n,"I'm trying to solve Exercise 5.4-8 in the fourth edition of CLRS: $\star$ 5.4-8 Sharpen the lower bound on streak length by showing that in $n$ flips of a fair coin, the probability is at least $1 - 1/n$ that a streak of length $\lg n- 2\lg \lg n$ consecutive heads occurs. While there is a closed form solution for the probability of a streak , I found it difficult to use for getting the bound needed here. I also found a supposed solution (listed as 5.4-7 there), using a similar approach to the text, where the sequence is partitioned into $\lfloor{n/s} \rfloor$ subsequences of length $s$ each. However, I think there are some issues with this solution. For starters, I believe a streak of length $s=\lg n - 2 \lg \lg n$ actually means $s = \lceil\lg n - 2 \lg \lg n \rceil$ . Also the floor at the exponent $n/s$ appears to be missing. I tried bounding the floors and ceilings more carefully and ended up with the inequality $$\left(1-\frac{\log _2^2(n)}{2 n}\right){}^{\frac{n}{\log _2(n)-2 \log _2\left(\log _2(n)\right)}-1} \leq \frac{1}{n} $$ which unfortunately fails around $n=2 \times 10^{11}$ . I cannot see how to remedy the proof in the link, nor how to get this bound in any other way. I still believe the statement in the exercise is true, and would greatly appreciate help proving it. Thanks.","I'm trying to solve Exercise 5.4-8 in the fourth edition of CLRS: 5.4-8 Sharpen the lower bound on streak length by showing that in flips of a fair coin, the probability is at least that a streak of length consecutive heads occurs. While there is a closed form solution for the probability of a streak , I found it difficult to use for getting the bound needed here. I also found a supposed solution (listed as 5.4-7 there), using a similar approach to the text, where the sequence is partitioned into subsequences of length each. However, I think there are some issues with this solution. For starters, I believe a streak of length actually means . Also the floor at the exponent appears to be missing. I tried bounding the floors and ceilings more carefully and ended up with the inequality which unfortunately fails around . I cannot see how to remedy the proof in the link, nor how to get this bound in any other way. I still believe the statement in the exercise is true, and would greatly appreciate help proving it. Thanks.",\star n 1 - 1/n \lg n- 2\lg \lg n \lfloor{n/s} \rfloor s s=\lg n - 2 \lg \lg n s = \lceil\lg n - 2 \lg \lg n \rceil n/s \left(1-\frac{\log _2^2(n)}{2 n}\right){}^{\frac{n}{\log _2(n)-2 \log _2\left(\log _2(n)\right)}-1} \leq \frac{1}{n}  n=2 \times 10^{11},"['probability', 'probability-theory', 'inequality']"
80,No two adjacent bulbs on,No two adjacent bulbs on,,"The problem is to count number of configuration of $9$ bulbs on a $3\times 3$ grid, where no two bulbs that are adjacent are switched on. I solved this problem in a very ad-hoc kind of manner, the answer is $63$ , here's how (I am representing configuration row wise as : $\mathtt{row_1}$ , $\mathtt{row_2}$ , $\mathtt{row_3}$ ) These are the only unique configurations of the bulbs $101|010|101$ or $010|101|010$ or $100|001|100$ * 4(depending on the $4$ possible position of 2nd row bulb on middle cell of every boundary) or $100|001|010$ * 4 (depending on the position of row 1 bulb on corners) So a total of $2^5 + (2^4-1) + 4\times 1 + (2^2-1)\times 4$ (subtraction is done to ensure no double counting) My question is, is there a more systematic way of solving this problem ?","The problem is to count number of configuration of bulbs on a grid, where no two bulbs that are adjacent are switched on. I solved this problem in a very ad-hoc kind of manner, the answer is , here's how (I am representing configuration row wise as : , , ) These are the only unique configurations of the bulbs or or * 4(depending on the possible position of 2nd row bulb on middle cell of every boundary) or * 4 (depending on the position of row 1 bulb on corners) So a total of (subtraction is done to ensure no double counting) My question is, is there a more systematic way of solving this problem ?",9 3\times 3 63 \mathtt{row_1} \mathtt{row_2} \mathtt{row_3} 101|010|101 010|101|010 100|001|100 4 100|001|010 2^5 + (2^4-1) + 4\times 1 + (2^2-1)\times 4,"['probability', 'combinatorics', 'puzzle']"
81,"How to interpret the ""independent probability"" of an event which may only happen once at most?","How to interpret the ""independent probability"" of an event which may only happen once at most?",,"Good evening, everyone. I'm currently reading Davy Paindaveine & Philippe Spindel (2023) Revisiting the Name Variant of the Two-Children Problem, The American Statistician, 77:4, 401-405, DOI: 10.1080/00031305.2023.2173293 , and, working through some of the preliminary probabilities, I got stuck on something I can't resolve: The second variant rather asks: for a two-children family having at least a girl whose name is Florida, what is the probability that the other child is a boy? See, for example, Mlodinow (2008) or Marks and Smith (2011). If two sisters may be given the same name, then this variant is strictly equivalent to the previous one: more precisely, if it is assumed that girls are independently named Florida with probability r, then the probability that the other child is a boy is 2/(4 − r). To make the second variant of interest, one therefore needs to assume that two sisters may not be given the same name, in which case, under the assumptions associated with what we will call Model A below, the probability that the other child is a boy is 1/2, irrespective of r I'm trying to reason through the italicized part of the second variant as follows. The siblings are listed in order where $B$ is a boy, $G_F$ is a girl named Florida, and $G_F^C$ is a girl not named Florida: $$\frac{P\{(B,G_F)\cup(G_F,B)\}}{P\{(B,G_F)\cup(G_F,B)\cup(G_F^C,G_F)\cup(G_F,G_F^C)\}}=\frac{2\times.5^2r}{2\times .5^2r+P\{(G_F^C,G_F)\cup(G_F,G_F^C)\}}$$ Now, where I'm getting stuck is resolving $P\{(G_F^C,G_F)\cup(G_F,G_F^C)\}$ . If I reason as follows, I can get the right answer: The probability neither sister is named Florida is $(1-r)^2$ So the probability at least one sister is named Florida is $1-(1-r)^2$ But I don't want to count the probability both are named Florida So I subtract $r^2$ , leaving $1-(1-r)^2-r^2=2r$ So the probability of two girls, one named Florida, is $.5^2\times 2r$ . What's confusing me is that I also feel like I should be able to do it this way: $$P\{(G_F^C,G_F)\cup(G_F,G_F^C)\}=P(G_F^C,G_F)+P(G_F,G_F^C)=P(G_F^C|G_F)P(G_F)+P(G_F|G_F^C)P(G_F^C)$$ $$=.5^2[1r+r(1-r)]=.5^2[2r-r^2]$$ Where is this $-r^2$ term coming from? What's weird is that I was able to confirm that, reasoning this way, $P(G_F^C,G_F)+P(G_F,G_F^C)+P(G_F^C,G_F^C)=.5^2(1)$ . Somehow the $(G_F,G_F)$ event has probability 0 like it's supposed to, but the probability of ""exactly one sister named Florida"" is off by exactly what $P(G_F,G_F)$ would've been if possible. I feel like I'm missing something fundamental here. The only explanation I have is that, as soon as we defined an independent probability $r$ of a girl being named Florida, we lost the right to treat ""two sisters named Florida"" as an impossibility excluded from the overall sample space. That would necessitate the first strategy I used, while invalidating my $P(G_F|G_F^C)=.5r$ and $P(G_F^C|G_F)=.5(1)$ conditionals in the second strategy. ""May not be given the same name"" appears to mean ""exclude the $(G_F,G_F)$ event from consideration"", and not "" $P(G_F,G_F)=0$ "". However, even if this is correct, I still can't explain the fact that the second strategy gave a complete probability distribution. Can anyone shed some light on this?","Good evening, everyone. I'm currently reading Davy Paindaveine & Philippe Spindel (2023) Revisiting the Name Variant of the Two-Children Problem, The American Statistician, 77:4, 401-405, DOI: 10.1080/00031305.2023.2173293 , and, working through some of the preliminary probabilities, I got stuck on something I can't resolve: The second variant rather asks: for a two-children family having at least a girl whose name is Florida, what is the probability that the other child is a boy? See, for example, Mlodinow (2008) or Marks and Smith (2011). If two sisters may be given the same name, then this variant is strictly equivalent to the previous one: more precisely, if it is assumed that girls are independently named Florida with probability r, then the probability that the other child is a boy is 2/(4 − r). To make the second variant of interest, one therefore needs to assume that two sisters may not be given the same name, in which case, under the assumptions associated with what we will call Model A below, the probability that the other child is a boy is 1/2, irrespective of r I'm trying to reason through the italicized part of the second variant as follows. The siblings are listed in order where is a boy, is a girl named Florida, and is a girl not named Florida: Now, where I'm getting stuck is resolving . If I reason as follows, I can get the right answer: The probability neither sister is named Florida is So the probability at least one sister is named Florida is But I don't want to count the probability both are named Florida So I subtract , leaving So the probability of two girls, one named Florida, is . What's confusing me is that I also feel like I should be able to do it this way: Where is this term coming from? What's weird is that I was able to confirm that, reasoning this way, . Somehow the event has probability 0 like it's supposed to, but the probability of ""exactly one sister named Florida"" is off by exactly what would've been if possible. I feel like I'm missing something fundamental here. The only explanation I have is that, as soon as we defined an independent probability of a girl being named Florida, we lost the right to treat ""two sisters named Florida"" as an impossibility excluded from the overall sample space. That would necessitate the first strategy I used, while invalidating my and conditionals in the second strategy. ""May not be given the same name"" appears to mean ""exclude the event from consideration"", and not "" "". However, even if this is correct, I still can't explain the fact that the second strategy gave a complete probability distribution. Can anyone shed some light on this?","B G_F G_F^C \frac{P\{(B,G_F)\cup(G_F,B)\}}{P\{(B,G_F)\cup(G_F,B)\cup(G_F^C,G_F)\cup(G_F,G_F^C)\}}=\frac{2\times.5^2r}{2\times .5^2r+P\{(G_F^C,G_F)\cup(G_F,G_F^C)\}} P\{(G_F^C,G_F)\cup(G_F,G_F^C)\} (1-r)^2 1-(1-r)^2 r^2 1-(1-r)^2-r^2=2r .5^2\times 2r P\{(G_F^C,G_F)\cup(G_F,G_F^C)\}=P(G_F^C,G_F)+P(G_F,G_F^C)=P(G_F^C|G_F)P(G_F)+P(G_F|G_F^C)P(G_F^C) =.5^2[1r+r(1-r)]=.5^2[2r-r^2] -r^2 P(G_F^C,G_F)+P(G_F,G_F^C)+P(G_F^C,G_F^C)=.5^2(1) (G_F,G_F) P(G_F,G_F) r P(G_F|G_F^C)=.5r P(G_F^C|G_F)=.5(1) (G_F,G_F) P(G_F,G_F)=0","['probability', 'conditional-probability']"
82,Uniform law of large numbers in an infinite dimensional set,Uniform law of large numbers in an infinite dimensional set,,"Most properties on the uniform law of large numbers impose that parameter lies in a compact set. Let $X_1, \dots, X_n$ be ergodic variables such that, for any $\theta \in \Theta$ , $S_n(\theta) = (1/n)\sum_{i = 1}^n f(X_i, \theta)$ converges in probability to $S_0(\theta) = \lim_{n \to \infty}(1/n)\sum_{i = 1}^n \mathbb{E}(f(X_i, \theta))$ , for some measurable function $f$ continuous in $\theta$ . To generalize the convergence to the uniform convergence, we typically need $\Theta$ to be compact and $\mathbb{E}(\sup_{\theta}f(X_i, \theta)) < \infty$ . Under these conditions, we can say that $sup_{\theta}\lVert S_n(\theta) - S_0(\theta)\rVert = o_p(1)$ . What if $\Theta$ is not compact (an infinite dimensional set)?  For example, Let $u_1, \dots, u_n$ be a nonstochastic sequence, such that $u_i \in \mathcal{N}_i$ , where $\mathcal{N}_i$ is a neighborhood subset in $\Theta$ . Assume that $T_n (u_1, \dots, u_n) = (1/n)\sum_{i = 1}^n f(X_i, u_i)$ converges in probability to $\lim \mathbb {E}(T_n (u_1, \dots, u_n))$ . Intuitively, it is as if I replace $\theta$ with a function. Which conditions should I impose to show that $$\sup_{u_1\in\mathcal{N}_1,\dots,u_n\in\mathcal{N}_n}\lVert T_n (u_1, \dots, u_n) - {E}(T_n (u_1, \dots, u_n))\rVert = o_p(1)?$$","Most properties on the uniform law of large numbers impose that parameter lies in a compact set. Let be ergodic variables such that, for any , converges in probability to , for some measurable function continuous in . To generalize the convergence to the uniform convergence, we typically need to be compact and . Under these conditions, we can say that . What if is not compact (an infinite dimensional set)?  For example, Let be a nonstochastic sequence, such that , where is a neighborhood subset in . Assume that converges in probability to . Intuitively, it is as if I replace with a function. Which conditions should I impose to show that","X_1, \dots, X_n \theta \in \Theta S_n(\theta) = (1/n)\sum_{i = 1}^n f(X_i, \theta) S_0(\theta) = \lim_{n \to \infty}(1/n)\sum_{i = 1}^n \mathbb{E}(f(X_i, \theta)) f \theta \Theta \mathbb{E}(\sup_{\theta}f(X_i, \theta)) < \infty sup_{\theta}\lVert S_n(\theta) - S_0(\theta)\rVert = o_p(1) \Theta u_1, \dots, u_n u_i \in \mathcal{N}_i \mathcal{N}_i \Theta T_n (u_1, \dots, u_n) = (1/n)\sum_{i = 1}^n f(X_i, u_i) \lim \mathbb {E}(T_n (u_1, \dots, u_n)) \theta \sup_{u_1\in\mathcal{N}_1,\dots,u_n\in\mathcal{N}_n}\lVert T_n (u_1, \dots, u_n) - {E}(T_n (u_1, \dots, u_n))\rVert = o_p(1)?","['probability', 'probability-theory', 'measure-theory', 'uniform-convergence', 'probability-limit-theorems']"
83,A textbook with a lot of intuitive explanations in advanced probability theory,A textbook with a lot of intuitive explanations in advanced probability theory,,"I am studying Durrett's Advanced Probability Theory, 5th ed. However, I find it very helpful to have more intuitive explanations like this . May I ask if there a textbook or online course or online video or any other resources like this?","I am studying Durrett's Advanced Probability Theory, 5th ed. However, I find it very helpful to have more intuitive explanations like this . May I ask if there a textbook or online course or online video or any other resources like this?",,"['probability', 'reference-request', 'intuition', 'book-recommendation', 'education']"
84,Solving $\operatorname{argmin}_\alpha \|I-\alpha \operatorname{diag} h + 2 \alpha^2 \operatorname{diag} h^2 +\alpha^2 h\otimes h\|$,Solving,\operatorname{argmin}_\alpha \|I-\alpha \operatorname{diag} h + 2 \alpha^2 \operatorname{diag} h^2 +\alpha^2 h\otimes h\|,"Suppose $h^*=\left(1,\frac{1}{2},\frac{1}{3},\ldots,\frac{1}{d}\right)$ and $h=h^*/\|h^*\|_1$ . Can someone see a way to approximate the following quantity for $d\approx 10^6$ ? The problem is to minimize the norm of a symmetric diagonal+rank1 matrix: $$\operatorname{argmin}_\alpha \|I-\alpha \operatorname{diag} h + 2 \alpha^2 (\operatorname{diag} h)^2 +\alpha^2 (h\otimes h)\|$$ For $d=2000$ I can compute it using brute-force to be $\approx 1.48$ and the hypothesis is that it goes to $2$ as $d\to\infty$ Motivation: this gives optimal step size for SGD used to solve $0=wx_i$ where $x_i$ are drawn from Gaussian with eigenvalues $h$ .",Suppose and . Can someone see a way to approximate the following quantity for ? The problem is to minimize the norm of a symmetric diagonal+rank1 matrix: For I can compute it using brute-force to be and the hypothesis is that it goes to as Motivation: this gives optimal step size for SGD used to solve where are drawn from Gaussian with eigenvalues .,"h^*=\left(1,\frac{1}{2},\frac{1}{3},\ldots,\frac{1}{d}\right) h=h^*/\|h^*\|_1 d\approx 10^6 \operatorname{argmin}_\alpha \|I-\alpha \operatorname{diag} h + 2 \alpha^2 (\operatorname{diag} h)^2 +\alpha^2 (h\otimes h)\| d=2000 \approx 1.48 2 d\to\infty 0=wx_i x_i h","['linear-algebra', 'probability', 'semidefinite-programming']"
85,Girsanov Transformation for discrete Random walks,Girsanov Transformation for discrete Random walks,,"currently I am trying to solve the following exercise/idea: Setup : Take $\Omega = \{ \omega \, \colon \mathbb{N}_0 \to \mathbb Z \mid \omega(0) = 0 \}$ the state space of all $\mathbb Z$ -valued random walks starting at $0$ together with a filtration of $(\Omega,\mathcal{F})$ given by $\mathcal{F}_n = \sigma(X_1,\dots,X_n)$ . Next consider the canocial projection given by $X_n(\omega) = \omega(n)$ and suppose that we are given a family of probability measures $P_p$ with $0 < p < 1$ such that the increments $\xi_n = X_n - X_{n-1}$ are i.i.d. $p$ -Rademacher distributed, i.e. $$ P_p(\xi_n = 1) = p \quad \text{ and } P_p(\xi_n = -1) = 1-p. $$ In other words we restrict our case to the case of simple random walks on $\mathbb Z$ . Question : Now I want to find an explicit formula for the Radon-Nikodym derivative given by $$ M_n = \frac{P_{p}\vert_{\mathcal{F}_n}}{P_{1/2}\vert_{\mathcal{F}_n}} $$ in terms of $n$ and $X_n$ . Using this I want to show that $M_n$ is a $P_{1/2}$ martingale with respect to $\mathcal{F}_{\mathbb{N}_0}$ . My attempt : Basically since we already know that $P_p$ only assigns positive mass to random walks that are simple it is enough to restrict $\omega \in \Omega$ where the $\xi_k \in \{-1,1\}$ (all the other walks have $0$ -probability). Let us start with the easy case: Easy case : Suppose that $X$ is a $1/2$ -Rademacher on $(\Omega,\mathcal{A},P_{1/2})$ and that $Y$ is $p$ -Rademacher for $0<p<1$ , which Radon-Nikodym derivative can we apply such that we change the law of $X$ to the law of $Y$ . Well if one takes $f \colon \{-1,1\} \to [0,\infty)$ given by $f(\omega) = 2p \, \delta_{\omega = 1} + 2(1-p) \, \delta_{\omega = -1}$ where $\delta$ denotes the dirac mass. Then observe that $$ P_p(X = 1) = \int_{\{1\}} f(\omega) dP_{1/2}(\omega) = p, $$ and $P_p(X = -1)=1-p$ respectively, thus by definition we know that $f$ is the right Radon-Nikodym derivative. Simple Random Walks : Following the same logic and using that the increments are i.i.d. I believe that the Radon-Nikodym derivative should be given by $$ M_n = \prod^{n}_{k=1} 2p \delta_{X_k = 1} + 2(1-p) \delta_{X_k=-1}. $$ Now using the same trick as above we should have $$ E_{1/2}[M_n \vert \mathcal{F}_{n-1}] = M_{n-1} E_{1/2}[\frac{dP_{p}}{dP_{1/2}}] = M_{n-1} E_p[1] = M_{n-1}, $$ and this finishes the proof however I am not 100% sure if this is correct.","currently I am trying to solve the following exercise/idea: Setup : Take the state space of all -valued random walks starting at together with a filtration of given by . Next consider the canocial projection given by and suppose that we are given a family of probability measures with such that the increments are i.i.d. -Rademacher distributed, i.e. In other words we restrict our case to the case of simple random walks on . Question : Now I want to find an explicit formula for the Radon-Nikodym derivative given by in terms of and . Using this I want to show that is a martingale with respect to . My attempt : Basically since we already know that only assigns positive mass to random walks that are simple it is enough to restrict where the (all the other walks have -probability). Let us start with the easy case: Easy case : Suppose that is a -Rademacher on and that is -Rademacher for , which Radon-Nikodym derivative can we apply such that we change the law of to the law of . Well if one takes given by where denotes the dirac mass. Then observe that and respectively, thus by definition we know that is the right Radon-Nikodym derivative. Simple Random Walks : Following the same logic and using that the increments are i.i.d. I believe that the Radon-Nikodym derivative should be given by Now using the same trick as above we should have and this finishes the proof however I am not 100% sure if this is correct.","\Omega = \{ \omega \, \colon \mathbb{N}_0 \to \mathbb Z \mid \omega(0) = 0 \} \mathbb Z 0 (\Omega,\mathcal{F}) \mathcal{F}_n = \sigma(X_1,\dots,X_n) X_n(\omega) = \omega(n) P_p 0 < p < 1 \xi_n = X_n - X_{n-1} p 
P_p(\xi_n = 1) = p \quad \text{ and } P_p(\xi_n = -1) = 1-p.
 \mathbb Z 
M_n = \frac{P_{p}\vert_{\mathcal{F}_n}}{P_{1/2}\vert_{\mathcal{F}_n}}
 n X_n M_n P_{1/2} \mathcal{F}_{\mathbb{N}_0} P_p \omega \in \Omega \xi_k \in \{-1,1\} 0 X 1/2 (\Omega,\mathcal{A},P_{1/2}) Y p 0<p<1 X Y f \colon \{-1,1\} \to [0,\infty) f(\omega) = 2p \, \delta_{\omega = 1} + 2(1-p) \, \delta_{\omega = -1} \delta 
P_p(X = 1) = \int_{\{1\}} f(\omega) dP_{1/2}(\omega) = p,
 P_p(X = -1)=1-p f 
M_n = \prod^{n}_{k=1} 2p \delta_{X_k = 1} + 2(1-p) \delta_{X_k=-1}.
 
E_{1/2}[M_n \vert \mathcal{F}_{n-1}] = M_{n-1} E_{1/2}[\frac{dP_{p}}{dP_{1/2}}] = M_{n-1} E_p[1] = M_{n-1},
","['probability', 'probability-theory', 'stochastic-processes']"
86,Expected number of points inside a disc with center inside a square,Expected number of points inside a disc with center inside a square,,"Let's say we have $n$ points that are uniformly, identically, and independently distributed in a square of side length $l$ . Now, we take a disk of radius $r\leq l/2$ with the center randomly placed inside the square. How many points are expected to be inside the disk? Initially, I was making an approximation based on the areas of the disk and square: $$\frac{\pi r^2 n}{l^2}.$$ However, this ignores the fact that when the center of the disk is closer to the edge, we would have a lesser number of points. So, I tried dividing the main square into parts, as shown in the figure below. AKME is a square of side of side $r$ . Now, the previous result holds true for the innermost square region, with reduced side length. I will need to find out the expected area for one of the smaller squares and one of the side rectangles, and then I can multiply by 4. Now, the side rectangles can be treated as the expected area if the center lies on the line KM, and then we can multiply by the side length MN. I used calculus and circular segments to compute it as (hopefully correctly): $$ (l-2r)\left(\pi r^2 r - \int_0^\pi \frac{r^2}{2}(\theta-\sin \theta)d\theta\right) = (l-2r)\left(\pi r^3 - \frac{r^2 (\pi^2-4)}{4}\right).$$ I guess I could do something similar with the corner squares as well, which would require double integration. Is the approach correct? Is there a simpler way? Would I be counting the area on the intermediate sides such as MN multiple times?","Let's say we have points that are uniformly, identically, and independently distributed in a square of side length . Now, we take a disk of radius with the center randomly placed inside the square. How many points are expected to be inside the disk? Initially, I was making an approximation based on the areas of the disk and square: However, this ignores the fact that when the center of the disk is closer to the edge, we would have a lesser number of points. So, I tried dividing the main square into parts, as shown in the figure below. AKME is a square of side of side . Now, the previous result holds true for the innermost square region, with reduced side length. I will need to find out the expected area for one of the smaller squares and one of the side rectangles, and then I can multiply by 4. Now, the side rectangles can be treated as the expected area if the center lies on the line KM, and then we can multiply by the side length MN. I used calculus and circular segments to compute it as (hopefully correctly): I guess I could do something similar with the corner squares as well, which would require double integration. Is the approach correct? Is there a simpler way? Would I be counting the area on the intermediate sides such as MN multiple times?",n l r\leq l/2 \frac{\pi r^2 n}{l^2}. r  (l-2r)\left(\pi r^2 r - \int_0^\pi \frac{r^2}{2}(\theta-\sin \theta)d\theta\right) = (l-2r)\left(\pi r^3 - \frac{r^2 (\pi^2-4)}{4}\right).,"['probability', 'geometry']"
87,Smallest eigenvalue of random walk on a graph,Smallest eigenvalue of random walk on a graph,,"Say $A$ is an $n \times n$ adjacency matrix for a connected undirected graph with all diagonals 1 (i.e. all self loops are present). I consider the uniform random walk on the graph according to transition matrix $D^{-1}A$ . The rate of convergence to equilibrium depends on $\lambda_* = \max(|\lambda_2|,|\lambda_n|)$ where $\lambda_i \geq \lambda_{i+1}$ are the eigenvalues of $D^{-1}A$ , so I am interested in finding under what conditions $\lambda_* = \lambda_2$ . Intuitively it feels like this should be the case in a ""typical"" example of $A$ , since $\lambda_n$ near -1 would mean near-periodicity whereas $\lambda_2$ near 1 is the more likely case of near-unconnectedness, if I understand correctly. However I haven't been able to find much conrete analysis of when $\lambda_2 = \lambda_*$ - are there any well known results?","Say is an adjacency matrix for a connected undirected graph with all diagonals 1 (i.e. all self loops are present). I consider the uniform random walk on the graph according to transition matrix . The rate of convergence to equilibrium depends on where are the eigenvalues of , so I am interested in finding under what conditions . Intuitively it feels like this should be the case in a ""typical"" example of , since near -1 would mean near-periodicity whereas near 1 is the more likely case of near-unconnectedness, if I understand correctly. However I haven't been able to find much conrete analysis of when - are there any well known results?","A n \times n D^{-1}A \lambda_* = \max(|\lambda_2|,|\lambda_n|) \lambda_i \geq \lambda_{i+1} D^{-1}A \lambda_* = \lambda_2 A \lambda_n \lambda_2 \lambda_2 = \lambda_*","['probability', 'graph-theory', 'markov-chains']"
88,Notions of measurability restricted to an event.,Notions of measurability restricted to an event.,,"Suppose on a probability space $(\Omega,\mathcal{A},\mathbb{P})$ we have $\mathcal{F}$ and $\mathcal{G}$ , independent sub-sigma-algebras of $\mathcal{A}$ . Consider $F\cap G\in \sigma(\mathcal{F},\mathcal{G})$ where $F\in \sigma(\mathcal{F},\mathcal{G})$ and $G\in\mathcal{G}$ . If we can show, due to the structure of $\mathcal{G}$ , that $F\cap G\in \mathcal{F}\cap G$ (where $\mathcal{F}\cap G$ denotes the trace sigma-algebra on the event $G$ ) can we conclude that $F\in\mathcal{F}$ ? Although I do not think so, to give some context, I came across two papers that make essentially the above reasoning, with the sole difference that the final claim is that $\mathbb{1}_F$ is $\mathcal{F}$ -measurable 'on $G$ ' (a claim that does not seem to make sense to me, as I have never met a notion of measurability restricted to an event, and therefore I interpreted as given above) and then go on to use the fact in computing $\mathbb{E}(X\mathbb{1}_F\mathbb{1}_G|\mathcal{F})=\mathbb{P}(G)X\mathbb{1}_F$ , having exploited that $X$ is a $\mathcal{F}$ -measurable random variable. Clearly this step is what suggest my interpretation of their claim. However, I failed to prove the theoretical fact above. Any ideas?","Suppose on a probability space we have and , independent sub-sigma-algebras of . Consider where and . If we can show, due to the structure of , that (where denotes the trace sigma-algebra on the event ) can we conclude that ? Although I do not think so, to give some context, I came across two papers that make essentially the above reasoning, with the sole difference that the final claim is that is -measurable 'on ' (a claim that does not seem to make sense to me, as I have never met a notion of measurability restricted to an event, and therefore I interpreted as given above) and then go on to use the fact in computing , having exploited that is a -measurable random variable. Clearly this step is what suggest my interpretation of their claim. However, I failed to prove the theoretical fact above. Any ideas?","(\Omega,\mathcal{A},\mathbb{P}) \mathcal{F} \mathcal{G} \mathcal{A} F\cap G\in \sigma(\mathcal{F},\mathcal{G}) F\in \sigma(\mathcal{F},\mathcal{G}) G\in\mathcal{G} \mathcal{G} F\cap G\in \mathcal{F}\cap G \mathcal{F}\cap G G F\in\mathcal{F} \mathbb{1}_F \mathcal{F} G \mathbb{E}(X\mathbb{1}_F\mathbb{1}_G|\mathcal{F})=\mathbb{P}(G)X\mathbb{1}_F X \mathcal{F}","['probability', 'probability-theory', 'measure-theory', 'conditional-probability', 'conditional-expectation']"
89,CDF of a convergent positive series,CDF of a convergent positive series,,"Let $Y_0, Y_1, \ldots$ be an i.i.d. random sequence such that $$ \mathbb{P}(Y_k = 0) \;=\; 1 - \mathbb{P}(Y_k = 1) \;=\; p \qquad \text{for each $k\ge 0$}. $$ I am interested in the following random sequences $$ U_k = \prod_{i=0}^{k-1}\gamma^{Y_i} \theta^{1 - Y_i}, $$ $$ Q_n = \sum_{k=1}^n U_k, $$ where $0<\theta<1<\gamma$ . I have proved that $Q_\infty = \sum_{k=1}^\infty U_k$ converges a.s. when $p> \log \gamma/\log (\theta^{-1}\gamma)$ . That means under this case, $Q_\infty$ is a random variable and it makes sense to consider its CDF (cumulative distribution function). Let us use $F_n$ and $F_\infty$ to denote the CDFs of $Q_n$ and $Q_\infty$ , respectively. I also proved that actually $F_n \downarrow F_\infty$ when $n\to\infty$ . Now I am curious about what is exactly $F_\infty$ . Can we have a beautiful explicit formula for it? Or can we have a tight lower bound of it? I also use MATLAB to draw the pictures of $F_n$ as follows (I set $\gamma=2$ , $\theta=0.5$ , and $p=0.51$ ), but I can only have $n$ around 20 since the time of computing this $F_n$ is $\mathcal{O}(2^n)$ . Any comments are welcome.","Let be an i.i.d. random sequence such that I am interested in the following random sequences where . I have proved that converges a.s. when . That means under this case, is a random variable and it makes sense to consider its CDF (cumulative distribution function). Let us use and to denote the CDFs of and , respectively. I also proved that actually when . Now I am curious about what is exactly . Can we have a beautiful explicit formula for it? Or can we have a tight lower bound of it? I also use MATLAB to draw the pictures of as follows (I set , , and ), but I can only have around 20 since the time of computing this is . Any comments are welcome.","Y_0, Y_1, \ldots  \mathbb{P}(Y_k = 0) \;=\; 1 - \mathbb{P}(Y_k = 1) \;=\; p \qquad \text{for each k\ge 0}.   U_k = \prod_{i=0}^{k-1}\gamma^{Y_i} \theta^{1 - Y_i},   Q_n = \sum_{k=1}^n U_k,  0<\theta<1<\gamma Q_\infty = \sum_{k=1}^\infty U_k p> \log \gamma/\log (\theta^{-1}\gamma) Q_\infty F_n F_\infty Q_n Q_\infty F_n \downarrow F_\infty n\to\infty F_\infty F_n \gamma=2 \theta=0.5 p=0.51 n F_n \mathcal{O}(2^n)","['probability', 'limits', 'pointwise-convergence', 'cumulative-distribution-functions']"
90,Challenging integral evaluation,Challenging integral evaluation,,"I computed the following integral and got that $$\int_S [x_1y_3+x_2(y_2+y_3)+x_3(y_1+y_2)]^n =O\Big(\frac{\log(n)}{n^3}\Big).$$ The integral is over the set $S=\{x_1+x_2+x_3=1, y_1+y_2+y_3=1 | x_i,y_i\geq 0\}$ . I now want evaluate a somewhat different integral, of the form: $$\int\limits_S  \sum\limits_k      \binom{n}{k}\frac1k       [x_1y_3+x_2(y_2+y_3)+x_3(y_1+y_2)]^{n-k}(x_3y_3)^k$$ over the same domain. So far I have found an upper bound of the form $O\Big(\frac{\log^2(n)}{n^2}\Big)$ , which is not good for me. I believe that this upper bound can be improved to $o\Big(\frac1{n^2}\Big)$ but cannot prove this currently. Is there any good way to evaluate this integral? Any way to prove\disprove that it's $o\Big(\frac1{n^2}\Big)$ ?","I computed the following integral and got that The integral is over the set . I now want evaluate a somewhat different integral, of the form: over the same domain. So far I have found an upper bound of the form , which is not good for me. I believe that this upper bound can be improved to but cannot prove this currently. Is there any good way to evaluate this integral? Any way to prove\disprove that it's ?","\int_S [x_1y_3+x_2(y_2+y_3)+x_3(y_1+y_2)]^n =O\Big(\frac{\log(n)}{n^3}\Big). S=\{x_1+x_2+x_3=1, y_1+y_2+y_3=1 | x_i,y_i\geq 0\} \int\limits_S
 \sum\limits_k
     \binom{n}{k}\frac1k
      [x_1y_3+x_2(y_2+y_3)+x_3(y_1+y_2)]^{n-k}(x_3y_3)^k O\Big(\frac{\log^2(n)}{n^2}\Big) o\Big(\frac1{n^2}\Big) o\Big(\frac1{n^2}\Big)","['probability', 'integration', 'combinatorics', 'hypergeometric-function', 'beta-function']"
91,Expected time until you reach the top of staircase in a dice game,Expected time until you reach the top of staircase in a dice game,,"I received this question during an online assessment and it continues to bug me. The setup is the following: you have a staircase with $100$ stairs in front of you, and at each time step $t$ you roll a fair six-sided die. You climb the same number of stairs as your roll except if you roll the same amount as the previous time step. If you roll the same number at time $t$ as you did in time $t - 1$ then you take a step back. For example, if you start on the $0$ th step and roll a $6$ , you go to stair $6$ . Then if you roll a $6$ again, you take one step back to be on stair $5$ . What is the expected number of rolls until you reach the top of the staircase? This was my reasoning: let $X_t$ be a random process denoting the position of the player after t rolls and let $d_t$ be the roll at time t. $E[d_t] = 3.5$ and we can calculate the expectation of $X_t$ using the law of iterated expectations and the law of total expectation. But first we consider the term $E[X_{t}|d_t \neq d_{t-1}] = X_{t-1} + E[d_t|d_t \neq d_{t-1}] = X_{t-1} + \frac{(1+2+...+6 - d_{t-1})}{5} = X_{t-1} + \frac{21-d_{t-1}}{5}.$ Then, \begin{align*} E[X_t] &= E[E[X_t|d_{t-1}]] = E\left[\frac{1}{6}(X_{t-1} - 1) + \frac{5}{6}(X_{t-1} + \frac{21-d_{t-1}}{5})\right] \\ & = E\left[X_{t-1} -\frac{d_{t-1}}{6} + \frac{20}{6} \right] = E[X_{t-1}] + \frac{11}{4} \end{align*} So it seems that at each time step, we are advancing on average $2.5$ stairs. Since $X_t$ is not a martingale , I cannot simply use the optional stopping theorem to calculate the stopping time. This is where I doubt my calculation. $E[X_t]$ is a recurrence relation with a solution of $E[X_t] = 3.5 + 2.75(t-1)$ , so what if we simply calculate the time until the expected position is at least $100$ ? Doing that we find $t = 36.0909...$ so would our answer then be $t = 37$ ? I suppose another way to do it is by using random variables $t_{i}$ to denote the time until you reach the end of the staircase starting at staircase $i$ . Then of course $E[t_{100}] = 0$ , $E[t_{99}] = \frac{5}{6}(1) + \frac{1}{6}E[t_{98}]$ but this soon gets complex as to calculate $E[t_{98}]$ we have to keep track of $d_{98}$ and $d_{97}$ and so on and so on.","I received this question during an online assessment and it continues to bug me. The setup is the following: you have a staircase with stairs in front of you, and at each time step you roll a fair six-sided die. You climb the same number of stairs as your roll except if you roll the same amount as the previous time step. If you roll the same number at time as you did in time then you take a step back. For example, if you start on the th step and roll a , you go to stair . Then if you roll a again, you take one step back to be on stair . What is the expected number of rolls until you reach the top of the staircase? This was my reasoning: let be a random process denoting the position of the player after t rolls and let be the roll at time t. and we can calculate the expectation of using the law of iterated expectations and the law of total expectation. But first we consider the term Then, So it seems that at each time step, we are advancing on average stairs. Since is not a martingale , I cannot simply use the optional stopping theorem to calculate the stopping time. This is where I doubt my calculation. is a recurrence relation with a solution of , so what if we simply calculate the time until the expected position is at least ? Doing that we find so would our answer then be ? I suppose another way to do it is by using random variables to denote the time until you reach the end of the staircase starting at staircase . Then of course , but this soon gets complex as to calculate we have to keep track of and and so on and so on.","100 t t t - 1 0 6 6 6 5 X_t d_t E[d_t] = 3.5 X_t E[X_{t}|d_t \neq d_{t-1}] = X_{t-1} + E[d_t|d_t \neq d_{t-1}] = X_{t-1} + \frac{(1+2+...+6 - d_{t-1})}{5} = X_{t-1} + \frac{21-d_{t-1}}{5}. \begin{align*}
E[X_t] &= E[E[X_t|d_{t-1}]] = E\left[\frac{1}{6}(X_{t-1} - 1) + \frac{5}{6}(X_{t-1} + \frac{21-d_{t-1}}{5})\right] \\
& = E\left[X_{t-1} -\frac{d_{t-1}}{6} + \frac{20}{6} \right] = E[X_{t-1}] + \frac{11}{4}
\end{align*} 2.5 X_t E[X_t] E[X_t] = 3.5 + 2.75(t-1) 100 t = 36.0909... t = 37 t_{i} i E[t_{100}] = 0 E[t_{99}] = \frac{5}{6}(1) + \frac{1}{6}E[t_{98}] E[t_{98}] d_{98} d_{97}","['probability', 'markov-chains', 'puzzle']"
92,Probability of a survivor,Probability of a survivor,,"In a room stand n armed and angry people. At each chime of a clock, everyone simultaneously spins around and shoots a random other person. The persons shot fall dead and the survivors spin and shoot again at the next chime. Eventually, either everyone is dead or there is a single survivor. As n grows, what is the limiting probability that there will be a survivor? I have been trying to solve this problem for a while but couldn't find a good approach. Any insight is appreciated! Here are the probabilites for the small values of $n$ : $n = 2$ -> $0$ $n = 3$ -> $3 / 4$ $n = 4$ -> $48/81$ For much larger values of n, I ran some computer simulation and found that the answer gets close to $1 / 2$ .","In a room stand n armed and angry people. At each chime of a clock, everyone simultaneously spins around and shoots a random other person. The persons shot fall dead and the survivors spin and shoot again at the next chime. Eventually, either everyone is dead or there is a single survivor. As n grows, what is the limiting probability that there will be a survivor? I have been trying to solve this problem for a while but couldn't find a good approach. Any insight is appreciated! Here are the probabilites for the small values of : -> -> -> For much larger values of n, I ran some computer simulation and found that the answer gets close to .",n n = 2 0 n = 3 3 / 4 n = 4 48/81 1 / 2,"['probability', 'random-graphs']"
93,Probability in Pokémon Speedrunning,Probability in Pokémon Speedrunning,,"Background Hey folks.  So I am, in addition to being a casual math and physics nerd, a Pokémon speedrunner.  I run the game Pokémon Red Version and the category ""Any % Glitchless (Classic).""  For those of you who don't know speedrunning terminology, ""Any %"" means that the only goal is to finish the game (i.e. you are allowed to skip any optional content) and ""glitchless"" means that you may not use any glitches.  In the ""glitchless (classic)"" category, you are also not permitted to ""hard reset"" the game (power it off and back on again), which is necessary to manipulate the game's random number generator function, so for runners of this category, the RNG is functionally truly random. Of course, one of the main purposes that we want to manipulate the RNG function is to control whether we get encounters.  The encounter mechanic in Pokémon (simplified) is that there is a certain probability in any given region of the game that each step in an encounter generating tile* will give an encounter.  If any of y'all have played Generation I Pokémon, you'll know that Mount Moon is the absolute most obnoxious area in the game and tends to generate an absolute torrent of encounters.  This is because, in ""cave"" areas, every tile on the map* is an encounter generating tile, whereas in other areas, only grassy tiles can generate encounters, so grassy tiles can be avoided.  Famously, one Pokémon speedrunner with the handle ""ExtraTricky"" got an absurd 18 encounters in Mount Moon in a previous world record. Naturally, I was wondering what the average number of encounters in Mount Moon should be. Simplified Problem You'll notice that I asterisked a few of the notes about the encounter generation function.  We're going to ignore certain complexities of this function for the purposes of the simplified problem.  In the full problem, we'll reintroduce those complexities. Like I said previously, every region of the map has an ""encounter rate"" that determines the probability that you will generate an encounter when stepping in an encounter-generating tile on the map.  In Mount Moon, that encounter rate is $\frac{10}{256}$ , which maybe doesn't seem that bad, but optimal movement through the area requires 276 steps (for any glitchless classic runners, this assumes no Hiker strats).  This version of the problem is actually pretty simple: it's a binomal distribution.  Thus, we can state the probability function as follows: Let $r$ be the number of encounters. $$ \mathbb P (r) = {276 \choose r} \cdot \bigg ( \frac{10}{256} \bigg )^r \cdot \bigg ( \frac{246}{256} \bigg ) ^{276-r}$$ I even made a graph! The average just ends up being the weighted sum, which I plugged into a spreadsheet, so: $$ \bar r = \sum_{n=0}^{276} n \mathbb P (n) \approx 10.7813$$ Full Problem There are two confounding factors that I want to add in here: Factor 1: Nido DVs and Catch Level So the main Poké that you use in Gen 1 is Nidoking, which means you will catch a Nidoran♂ in Route 22.  You can encounter this Nidoran♂ at levels 2, 3, or 4.  We do not catch level 2 Nidorans, so those may be ignored.  The player is, in random encounters, twice as likely to encounter a level 3 Nidoran♂ as a level 4 Nidoran♂.**  And, of course, the strats differ when using a level 3 Nidoran♂ vs. a level 4 Nidoran♂. Then, each individual Poké in Generation 1 has a ""determinant value"" for each stat that determines how high that stat will be compared so another Poké of the same species and level, which can be any 4 bit number (i.e. 0–15).  This also determines what strats we use.  In particular, if the Nidoran♂ has an attack DV under 5, we reset (we do not continue the run), so those may be ignore.  With a level 3 Nidoran♂  whose attack DV is $5\leq x \leq10$ we take an extra 20 steps in Mount Moon.  With a level 4 Nidoran♂ whose attack DV is $5\leq x \leq 8$ , we also take those extra 20 steps.  These are the above-referenced ""Hiker Strats"".  Assume all DVs for attack are equally likely.  We want to account for how these DV and catch level strats will affect our average. Of course, this isn't actually that hard.  We'll just need to have two separate probability functions based on whether we use Hiker strats or not and multiply those results times the probability of any given Nidoran♂'s catch level and attack DV. Factor Two: The Two-Step Rule This is the asterisk I put above with the encounter rates.  The first two steps upon entering Mount Moon and the first two steps after any battle (trainer or wild encounter) will not generate encounters.  This means that the maximum number of encounters is not, in fact, 276 because you will not generate an encounter until your third step after a battle, meaning the theorhetical maximum number of encounters is $\Big \lfloor \frac{276}{3} \Big \rfloor = 92$ .  In the above step count, I excluded the first two steps after entering Mt. Moon and after every trainer battle, but I did not exclude steps after random encounters. This is the part that has me a little stumped.  I suppose I could some kind of script or program that uses brute force to calculate every possible encounter profile given this two-step rule, but I'm wondering if there is a more elegant way to do it that I'm not seeing. And yes, I know the binomial distribution estimate is probably close enough, but I'm curious if there is a non-ridiculous way to calculate the average number of encounters taking the two-step rule into account. ** I'm not actually sure this is accurate.  In the part of the game where we catch the Nidoran♂, we have a method of only searching for wild encounters when the RNG is likely to give us a Nidoran♂ (called ""DSUMming"") and I don't know if this is biased towards a level 3 or a level 4 and/or by how much if so.","Background Hey folks.  So I am, in addition to being a casual math and physics nerd, a Pokémon speedrunner.  I run the game Pokémon Red Version and the category ""Any % Glitchless (Classic).""  For those of you who don't know speedrunning terminology, ""Any %"" means that the only goal is to finish the game (i.e. you are allowed to skip any optional content) and ""glitchless"" means that you may not use any glitches.  In the ""glitchless (classic)"" category, you are also not permitted to ""hard reset"" the game (power it off and back on again), which is necessary to manipulate the game's random number generator function, so for runners of this category, the RNG is functionally truly random. Of course, one of the main purposes that we want to manipulate the RNG function is to control whether we get encounters.  The encounter mechanic in Pokémon (simplified) is that there is a certain probability in any given region of the game that each step in an encounter generating tile* will give an encounter.  If any of y'all have played Generation I Pokémon, you'll know that Mount Moon is the absolute most obnoxious area in the game and tends to generate an absolute torrent of encounters.  This is because, in ""cave"" areas, every tile on the map* is an encounter generating tile, whereas in other areas, only grassy tiles can generate encounters, so grassy tiles can be avoided.  Famously, one Pokémon speedrunner with the handle ""ExtraTricky"" got an absurd 18 encounters in Mount Moon in a previous world record. Naturally, I was wondering what the average number of encounters in Mount Moon should be. Simplified Problem You'll notice that I asterisked a few of the notes about the encounter generation function.  We're going to ignore certain complexities of this function for the purposes of the simplified problem.  In the full problem, we'll reintroduce those complexities. Like I said previously, every region of the map has an ""encounter rate"" that determines the probability that you will generate an encounter when stepping in an encounter-generating tile on the map.  In Mount Moon, that encounter rate is , which maybe doesn't seem that bad, but optimal movement through the area requires 276 steps (for any glitchless classic runners, this assumes no Hiker strats).  This version of the problem is actually pretty simple: it's a binomal distribution.  Thus, we can state the probability function as follows: Let be the number of encounters. I even made a graph! The average just ends up being the weighted sum, which I plugged into a spreadsheet, so: Full Problem There are two confounding factors that I want to add in here: Factor 1: Nido DVs and Catch Level So the main Poké that you use in Gen 1 is Nidoking, which means you will catch a Nidoran♂ in Route 22.  You can encounter this Nidoran♂ at levels 2, 3, or 4.  We do not catch level 2 Nidorans, so those may be ignored.  The player is, in random encounters, twice as likely to encounter a level 3 Nidoran♂ as a level 4 Nidoran♂.**  And, of course, the strats differ when using a level 3 Nidoran♂ vs. a level 4 Nidoran♂. Then, each individual Poké in Generation 1 has a ""determinant value"" for each stat that determines how high that stat will be compared so another Poké of the same species and level, which can be any 4 bit number (i.e. 0–15).  This also determines what strats we use.  In particular, if the Nidoran♂ has an attack DV under 5, we reset (we do not continue the run), so those may be ignore.  With a level 3 Nidoran♂  whose attack DV is we take an extra 20 steps in Mount Moon.  With a level 4 Nidoran♂ whose attack DV is , we also take those extra 20 steps.  These are the above-referenced ""Hiker Strats"".  Assume all DVs for attack are equally likely.  We want to account for how these DV and catch level strats will affect our average. Of course, this isn't actually that hard.  We'll just need to have two separate probability functions based on whether we use Hiker strats or not and multiply those results times the probability of any given Nidoran♂'s catch level and attack DV. Factor Two: The Two-Step Rule This is the asterisk I put above with the encounter rates.  The first two steps upon entering Mount Moon and the first two steps after any battle (trainer or wild encounter) will not generate encounters.  This means that the maximum number of encounters is not, in fact, 276 because you will not generate an encounter until your third step after a battle, meaning the theorhetical maximum number of encounters is .  In the above step count, I excluded the first two steps after entering Mt. Moon and after every trainer battle, but I did not exclude steps after random encounters. This is the part that has me a little stumped.  I suppose I could some kind of script or program that uses brute force to calculate every possible encounter profile given this two-step rule, but I'm wondering if there is a more elegant way to do it that I'm not seeing. And yes, I know the binomial distribution estimate is probably close enough, but I'm curious if there is a non-ridiculous way to calculate the average number of encounters taking the two-step rule into account. ** I'm not actually sure this is accurate.  In the part of the game where we catch the Nidoran♂, we have a method of only searching for wild encounters when the RNG is likely to give us a Nidoran♂ (called ""DSUMming"") and I don't know if this is biased towards a level 3 or a level 4 and/or by how much if so.",\frac{10}{256} r  \mathbb P (r) = {276 \choose r} \cdot \bigg ( \frac{10}{256} \bigg )^r \cdot \bigg ( \frac{246}{256} \bigg ) ^{276-r}  \bar r = \sum_{n=0}^{276} n \mathbb P (n) \approx 10.7813 5\leq x \leq10 5\leq x \leq 8 \Big \lfloor \frac{276}{3} \Big \rfloor = 92,['probability']
94,Comparing the Sample Variance with the Population Variance,Comparing the Sample Variance with the Population Variance,,"I read in the following paper ( https://iopscience.iop.org/article/10.1088/0026-1394/41/3/004/pdf ) on page 133 "" The conditional standard deviation (...) is necessarily an underestimate of its unconditional standard deviation. "" . I am trying to understand why this statement is true. I will now outline my understanding of this in 3 parts. Part 1: In this link ( Proving that Sample Variance is an unbiased estimator of Population Variance ), a proof is given that shows the sample variance is an unbiased estimator of the population variance: $$E(S^2) = \frac{n-1}{n}E(X_1-Y_1)^2 = \frac{n-1}{n}\text{var}(X_1-Y_1) = \frac{n-1}{n}\left(\sigma^2 + \frac{\sigma^2}{n-1}\right) = \sigma^2$$ Part 2: In this link ( https://stats.stackexchange.com/questions/496424/how-to-prove-s2-is-a-consistent-estimator-of-sigma2 ), a proof is given that shows the sample variance is a consistent estimator of the population variance: \begin{align*} &\mathbb{P}(\mid s^2 - \sigma^2 \mid > \varepsilon )\\ &= \mathbb{P}(\mid s^2 - \mathbb{E}(s^2) \mid > \varepsilon )\\ &\leqslant \dfrac{\text{var}(s^2)}{\varepsilon^2}\\ &=\dfrac{1}{(n-1)^2}\cdot \text{var}\left[\sum (X_i - \overline{X})^2)\right]\\ &=\dfrac{\sigma^4}{(n-1)^2}\cdot \text{var}\left[\frac{\sum (X_i - \overline{X})^2}{\sigma^2}\right]\\ &=\dfrac{\sigma^4}{(n-1)^2}\cdot\text{var}(Z_n)\\ &=\dfrac{\sigma^4}{(n-1)^2}\cdot 2(n-1) = \dfrac{2\sigma^4}{n-1} \stackrel{n\to\infty}{\longrightarrow} 0 \end{align*} Thus, $ \displaystyle\lim_{n\to\infty} \mathbb{P}(\mid s^2 - \sigma^2 \mid > \varepsilon ) = 0$ , i.e. $ s^2 \stackrel{\mathbb{P}}{\longrightarrow} \sigma^2 $ as $n\to\infty$ , which tells us that $s^2$ is a consistent estimator of $\sigma^2$ . Part 3: Using some algebraic manipulation, I can see that the sample variance appears to always be less than the population variance : The sample variance is defined as: $$s^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2$$ where $n$ is the sample size, $x_i$ are the individual observations, and $\bar{x}$ is the sample mean and the population variance is denoted as $\sigma^2$ . OLS (Ordinary Least Squares) tell us that $\bar{x}$ minimizes the sum of squared deviations - thus: $$\sum_{i=1}^n (x_i - \bar{x})^2 \leq \sum_{i=1}^n (x_i - \mu)^2$$ Dividing both sides by $n-1$ : $$\frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2 \leq \frac{1}{n-1} \sum_{i=1}^n (x_i - \mu)^2$$ Further simplifying and substituting (for large enough $n$ ): $$\sigma^2 = \frac{1}{n} \sum_{i=1}^n (x_i - \mu)^2$$ $$\frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2 \leq \frac{1}{n-1} n\sigma^2 = \sigma^2$$ $$s^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2 \leq \sigma^2$$ This proves that the sample variance $s^2$ is always less than or equal to the population variance $\sigma^2$ . On another note, using informal logic, an argument can be made that the sample variance might not include extreme outliers, whereas the population would include these extreme outliers. Extreme outliers have large deviations from the mean - thus, the presence of extreme outliers would increase the variance calculations. Thus, the sample variance can technically never be smaller than the population variance. My Question: How can all 3 parts simultaneously be correct at the same time? If the sample variance is said to estimate the population variance without any bias, the sample variance is said to converge to the population variance for large samples - then how can the sample variance always be guaranteed to be less than the population variance ?  Is this not a contradiction? Thanks! References: Why does sample standard deviation underestimate population standard deviation?","I read in the following paper ( https://iopscience.iop.org/article/10.1088/0026-1394/41/3/004/pdf ) on page 133 "" The conditional standard deviation (...) is necessarily an underestimate of its unconditional standard deviation. "" . I am trying to understand why this statement is true. I will now outline my understanding of this in 3 parts. Part 1: In this link ( Proving that Sample Variance is an unbiased estimator of Population Variance ), a proof is given that shows the sample variance is an unbiased estimator of the population variance: Part 2: In this link ( https://stats.stackexchange.com/questions/496424/how-to-prove-s2-is-a-consistent-estimator-of-sigma2 ), a proof is given that shows the sample variance is a consistent estimator of the population variance: Thus, , i.e. as , which tells us that is a consistent estimator of . Part 3: Using some algebraic manipulation, I can see that the sample variance appears to always be less than the population variance : The sample variance is defined as: where is the sample size, are the individual observations, and is the sample mean and the population variance is denoted as . OLS (Ordinary Least Squares) tell us that minimizes the sum of squared deviations - thus: Dividing both sides by : Further simplifying and substituting (for large enough ): This proves that the sample variance is always less than or equal to the population variance . On another note, using informal logic, an argument can be made that the sample variance might not include extreme outliers, whereas the population would include these extreme outliers. Extreme outliers have large deviations from the mean - thus, the presence of extreme outliers would increase the variance calculations. Thus, the sample variance can technically never be smaller than the population variance. My Question: How can all 3 parts simultaneously be correct at the same time? If the sample variance is said to estimate the population variance without any bias, the sample variance is said to converge to the population variance for large samples - then how can the sample variance always be guaranteed to be less than the population variance ?  Is this not a contradiction? Thanks! References: Why does sample standard deviation underestimate population standard deviation?","E(S^2) = \frac{n-1}{n}E(X_1-Y_1)^2 = \frac{n-1}{n}\text{var}(X_1-Y_1) = \frac{n-1}{n}\left(\sigma^2 + \frac{\sigma^2}{n-1}\right) = \sigma^2 \begin{align*}
&\mathbb{P}(\mid s^2 - \sigma^2 \mid > \varepsilon )\\
&= \mathbb{P}(\mid s^2 - \mathbb{E}(s^2) \mid > \varepsilon )\\
&\leqslant \dfrac{\text{var}(s^2)}{\varepsilon^2}\\
&=\dfrac{1}{(n-1)^2}\cdot \text{var}\left[\sum (X_i - \overline{X})^2)\right]\\
&=\dfrac{\sigma^4}{(n-1)^2}\cdot \text{var}\left[\frac{\sum (X_i - \overline{X})^2}{\sigma^2}\right]\\
&=\dfrac{\sigma^4}{(n-1)^2}\cdot\text{var}(Z_n)\\
&=\dfrac{\sigma^4}{(n-1)^2}\cdot 2(n-1) = \dfrac{2\sigma^4}{n-1} \stackrel{n\to\infty}{\longrightarrow} 0
\end{align*}  \displaystyle\lim_{n\to\infty} \mathbb{P}(\mid s^2 - \sigma^2 \mid > \varepsilon ) = 0  s^2 \stackrel{\mathbb{P}}{\longrightarrow} \sigma^2  n\to\infty s^2 \sigma^2 s^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2 n x_i \bar{x} \sigma^2 \bar{x} \sum_{i=1}^n (x_i - \bar{x})^2 \leq \sum_{i=1}^n (x_i - \mu)^2 n-1 \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2 \leq \frac{1}{n-1} \sum_{i=1}^n (x_i - \mu)^2 n \sigma^2 = \frac{1}{n} \sum_{i=1}^n (x_i - \mu)^2 \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2 \leq \frac{1}{n-1} n\sigma^2 = \sigma^2 s^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2 \leq \sigma^2 s^2 \sigma^2",['probability']
95,What's the probability that the only coin left is fair given that exactly $n$ tosses were required to obtain heads with each of the first two coins?,What's the probability that the only coin left is fair given that exactly  tosses were required to obtain heads with each of the first two coins?,n,"In the box there are $3$ coins. The probabilities of getting heads while tossing are equal to: $\frac{1}{2}$ on the first one, $\frac{1}{3}$ on the second one, $\frac{1}{4}$ on the third one. A man chose randomly $1$ of those $3$ coins and tossed it until getting heads. Then chose randomly another one and similarly - tossed it until getting heads. What's the probability that in the box the only coin that is left is the symmetric one if we know that the man tossed exactly $n$ times? I thought about counting the possibilities for $3 \cdot 2$ pairs of chosen coins in both orders and then the pair that we need in both orders. The first of two expressions would be put in the denominator and the other in the numerator. However, when I started with that approach I got to a point where calculating that all got too complicated. That's why I got to think that there must be more clever way to approach that problem. Any help would be much appreciated. Edit: Using the hint given by @lulu in the comments, I got something like this: $$\mathbb{P} \left( \text{coins were: } \ \frac{1}{3} \text{ and } \frac{1}{4} \ \Big| \ \text{there were n toses} \right) =$$ $$ = \frac{\mathbb{P}(\text{there were n toses} \ \Big| \ \text{2 coins were:} \ \frac{1}{3} \text{ and } \frac{1}{4}) \cdot \mathbb{P}(\text{coins were: } \ \frac{1}{3} \text{ and } \frac{1}{4})}{\mathbb{P}(\text{there were n toses})}$$ $\mathbb{P}(\text{there were n toses}) = $ $\mathbb{P} \left( \text{there were n toses} \ \Big| \ \text{2 coins chosen were those of prob. } \ \frac{1}{2} \text{ and } \frac{1}{3} \right) +$ $\mathbb{P} \left( \text{there were n toses} \ \Big| \ \text{2 coins chosen were those of prob. } \ \frac{1}{3} \text{ and } \frac{1}{4} \right) +$ $\mathbb{P} \left( \text{there were n toses} \ \Big| \ \text{2 coins chosen were those of prob. } \ \frac{1}{2} \text{ and } \frac{1}{4} \right)$ We get: ( $k$ is the number of failed tosses on one coin and $n-k-2$ are failed tosses on another) $1. = \displaystyle \sum_{k=0}^{n-1} \left( \frac{1}{2} \right)^k \frac{1}{2} \left( \frac{2}{3} \right)^{n-k-2} \frac{1}{3}$ $2. = \displaystyle \sum_{k=0}^{n-1} \left( \frac{2}{3} \right)^k \frac{1}{3} \left( \frac{3}{4} \right)^{n-k-2} \frac{1}{4}$ $3. = \displaystyle \sum_{k=0}^{n-1} \left( \frac{1}{2} \right)^k \frac{1}{2} \left( \frac{3}{4} \right)^{n-k-2} \frac{1}{4}$ It looks all good, but now, how do I express $1, 2, 3$ in terms of $n$ , losing $k$ in the process?","In the box there are coins. The probabilities of getting heads while tossing are equal to: on the first one, on the second one, on the third one. A man chose randomly of those coins and tossed it until getting heads. Then chose randomly another one and similarly - tossed it until getting heads. What's the probability that in the box the only coin that is left is the symmetric one if we know that the man tossed exactly times? I thought about counting the possibilities for pairs of chosen coins in both orders and then the pair that we need in both orders. The first of two expressions would be put in the denominator and the other in the numerator. However, when I started with that approach I got to a point where calculating that all got too complicated. That's why I got to think that there must be more clever way to approach that problem. Any help would be much appreciated. Edit: Using the hint given by @lulu in the comments, I got something like this: We get: ( is the number of failed tosses on one coin and are failed tosses on another) It looks all good, but now, how do I express in terms of , losing in the process?","3 \frac{1}{2} \frac{1}{3} \frac{1}{4} 1 3 n 3 \cdot 2 \mathbb{P} \left( \text{coins were: } \ \frac{1}{3} \text{ and } \frac{1}{4} \ \Big| \ \text{there were n toses} \right) =  = \frac{\mathbb{P}(\text{there were n toses} \ \Big| \ \text{2 coins were:} \ \frac{1}{3} \text{ and } \frac{1}{4}) \cdot \mathbb{P}(\text{coins were: } \ \frac{1}{3} \text{ and } \frac{1}{4})}{\mathbb{P}(\text{there were n toses})} \mathbb{P}(\text{there were n toses}) =  \mathbb{P} \left( \text{there were n toses} \ \Big| \ \text{2 coins chosen were those of prob. } \ \frac{1}{2} \text{ and } \frac{1}{3} \right) + \mathbb{P} \left( \text{there were n toses} \ \Big| \ \text{2 coins chosen were those of prob. } \ \frac{1}{3} \text{ and } \frac{1}{4} \right) + \mathbb{P} \left( \text{there were n toses} \ \Big| \ \text{2 coins chosen were those of prob. } \ \frac{1}{2} \text{ and } \frac{1}{4} \right) k n-k-2 1. = \displaystyle \sum_{k=0}^{n-1} \left( \frac{1}{2} \right)^k \frac{1}{2} \left( \frac{2}{3} \right)^{n-k-2} \frac{1}{3} 2. = \displaystyle \sum_{k=0}^{n-1} \left( \frac{2}{3} \right)^k \frac{1}{3} \left( \frac{3}{4} \right)^{n-k-2} \frac{1}{4} 3. = \displaystyle \sum_{k=0}^{n-1} \left( \frac{1}{2} \right)^k \frac{1}{2} \left( \frac{3}{4} \right)^{n-k-2} \frac{1}{4} 1, 2, 3 n k","['probability', 'conditional-probability']"
96,Recognizing a Probability Theorem,Recognizing a Probability Theorem,,"I am reading this article here https://www.jstor.org/stable/3001633 and on page 7, the following probability distribution function is given: Theorem : If $x_1, \dots, x_k$ are independently distributed with density functions: $$f_{n_i}(x_i) = \left(\frac{n_i}{2}\right)^{\frac{n_i}{2}} \frac{x_i^{\frac{n_i}{2}-1}e^{-\frac{n_ix_i}{2}}}{\Gamma\left(\frac{n_i}{2}\right)}$$ for $0 \leq x_i < \infty$ and $R(x_1,\dots,x_k)$ is a rational function with no singularities for $0 < x_1,\dots,x_k < \infty$ , then $\text{Ave}\{R(x_1,\dots,x_k)\}$ can be expanded in an asymptotic series in $\frac{1}{n_i}$ . In particular: $$\text{Ave}\{R(x_1,\dots,x_k)\} = R(1,\dots,1) + \sum_{i=1}^k \frac{1}{n_i} \left.\frac{\partial^2 R}{\partial x_i^2}\right|_{(1,\dots,1)} + O\left(\sum \frac{1}{n_i^2}\right)$$ My Question: It looks like in this theorem, the probability distribution $f_{n_i}(x_i)$ is a Chi-Square Distribution ( https://en.wikipedia.org/wiki/Chi-squared_distribution ) - but does anyone know any more information about this theorem? Does it have a name? I have been trying to find more information about it to learn where it comes from, why it is useful and why it is true (i.e. proof). Thanks! Note: Screenshot of the paper in case I transcribed it incorrectly :","I am reading this article here https://www.jstor.org/stable/3001633 and on page 7, the following probability distribution function is given: Theorem : If are independently distributed with density functions: for and is a rational function with no singularities for , then can be expanded in an asymptotic series in . In particular: My Question: It looks like in this theorem, the probability distribution is a Chi-Square Distribution ( https://en.wikipedia.org/wiki/Chi-squared_distribution ) - but does anyone know any more information about this theorem? Does it have a name? I have been trying to find more information about it to learn where it comes from, why it is useful and why it is true (i.e. proof). Thanks! Note: Screenshot of the paper in case I transcribed it incorrectly :","x_1, \dots, x_k f_{n_i}(x_i) = \left(\frac{n_i}{2}\right)^{\frac{n_i}{2}} \frac{x_i^{\frac{n_i}{2}-1}e^{-\frac{n_ix_i}{2}}}{\Gamma\left(\frac{n_i}{2}\right)} 0 \leq x_i < \infty R(x_1,\dots,x_k) 0 < x_1,\dots,x_k < \infty \text{Ave}\{R(x_1,\dots,x_k)\} \frac{1}{n_i} \text{Ave}\{R(x_1,\dots,x_k)\} = R(1,\dots,1) + \sum_{i=1}^k \frac{1}{n_i} \left.\frac{\partial^2 R}{\partial x_i^2}\right|_{(1,\dots,1)} + O\left(\sum \frac{1}{n_i^2}\right) f_{n_i}(x_i)","['calculus', 'probability', 'sequences-and-series', 'proof-explanation', 'taylor-expansion']"
97,How do you calculate the Expectation $E(X^4)$? [duplicate],How do you calculate the Expectation ? [duplicate],E(X^4),"This question already has answers here : Methods for Finding Raw Moments of the Normal Distribution (4 answers) Closed last year . Suppose $X$ is a normal random variable with, $\mu_x\ {\ne}\ 0\ \text{or}\ 1$ and $\sigma_x\ {\ne}\ 0\ \text{or}\ 1$ . I would like to show that the expectation $E(X^4)$ has the following equality: $E(X^4)\ {=}\ \mu_x^4+6\sigma_x^2\mu_x^2+3\sigma_x^4$ . I tried to prove this by calculating $E[(x-\mu_x)^4]$ which leads me to $E[(x-\mu_x)^4]\ {=}\ E(x^4-4x^3\mu_x+6x^2\mu_x^2-4x\mu_x^3+\mu_x^4)$ $E(x^4)\ {=}\ 4\mu_xE(x^3)-6\mu_x^2\sigma_x^2-3\mu_x^4+E[(x-\mu_x)^4]$ where I used $E(x^2)\ {=}\ \sigma_x^2+\mu_x^2$ and $E(x)\ {=}\ \mu_x$ to combine terms. This result didn't seem to help. I've looked into using the generating function approach but for $\mu_x\ {\ne}\ 0\ \text{or}\ 1$ and $\sigma_x\ {\ne}\ 0\ \text{or}\ 1$ these calculations become tedious very quickly. Is there an easy and quick way to show this?","This question already has answers here : Methods for Finding Raw Moments of the Normal Distribution (4 answers) Closed last year . Suppose is a normal random variable with, and . I would like to show that the expectation has the following equality: . I tried to prove this by calculating which leads me to where I used and to combine terms. This result didn't seem to help. I've looked into using the generating function approach but for and these calculations become tedious very quickly. Is there an easy and quick way to show this?",X \mu_x\ {\ne}\ 0\ \text{or}\ 1 \sigma_x\ {\ne}\ 0\ \text{or}\ 1 E(X^4) E(X^4)\ {=}\ \mu_x^4+6\sigma_x^2\mu_x^2+3\sigma_x^4 E[(x-\mu_x)^4] E[(x-\mu_x)^4]\ {=}\ E(x^4-4x^3\mu_x+6x^2\mu_x^2-4x\mu_x^3+\mu_x^4) E(x^4)\ {=}\ 4\mu_xE(x^3)-6\mu_x^2\sigma_x^2-3\mu_x^4+E[(x-\mu_x)^4] E(x^2)\ {=}\ \sigma_x^2+\mu_x^2 E(x)\ {=}\ \mu_x \mu_x\ {\ne}\ 0\ \text{or}\ 1 \sigma_x\ {\ne}\ 0\ \text{or}\ 1,"['probability', 'probability-theory', 'statistics', 'random-variables']"
98,Filtration modelling a random number of observation,Filtration modelling a random number of observation,,"Suppose we have i.i.d. r.v.'s $\{X_i\}$ and a random number of observations $N$ we are allowed, following some distribution, independent of the $\{X_i\}$ . So essentially we observe $X_1,\ldots,X_N$ . I am interested in constructing a filtration for the process of observing these values, and stopping according to some stopping rule as usual. However, what could happen is that my stopping rule could find out, AFTER an observation and not having stopped at that observation, that it was the last one allowed. We assign the value of infinity to the stopping time, in this case. Now my question is: what filtration models what we know at a step $n\leq N$ in this game? My guess would be $\sigma(X_1, \ldots, X_n, 1_{\{N=0\}},\ldots, 1_{\{N=n-1\}})$ . This makes sense when inspected step by step: if the game starts (this is expressed by $1_{\{N=0\}}=0$ ), at time 1, when receiving performing the first observation, I will only know that it is not the case that there were no observations at all, but I do not know yet if the observation I am taking will be the last: thus $\sigma(X_1,I_{\{N=0\}})$ is what I know. After making the decision whether to stop or continue, I will know $\sigma(X_1,I_{\{N=0\}},I_{\{N=1\}})$ , that is I will know whether $X_1$ was the last observation ( $I_{\{N=1\}}=1$ ) or not ( $I_{\{N=1\}}=0$ ). Next $\sigma(X_1,I_{\{N=0\}},I_{\{N=1\}}, X_2)$ and so on. What confuses me is that I am used to filtration expressing all the information available at time $n$ , in the sense of all that could have happened by that time, but my construction seems a little different and I suspect that there is a mistake. In fact, say we are at time 3 in the game, the information is supposed to be $\sigma(X_1,I_{\{N=0\}},I_{\{N=1\}}, X_2)$ . But clearly, if I am at time 3 in the game, just before I am inspecting $X_3$ , I know that $I_{\{N=0\}}=I_{\{N=1\}}=0$ , otherwise I would have not inspected $X_2$ at the second step. What I mean is that, focusing on the indicators, only sequences of zeroes and sequences of zeroes with one at the end are allowed, in terms of information, not every possible string of zero and ones. As soon as a one appears, the game stops and we set our stopping time to infinity. (If instead we stop by our decision, we have the stopped sigma algebra to model that.) Does this mean that the construction is redundant or even wrong? Thanks for any help.","Suppose we have i.i.d. r.v.'s and a random number of observations we are allowed, following some distribution, independent of the . So essentially we observe . I am interested in constructing a filtration for the process of observing these values, and stopping according to some stopping rule as usual. However, what could happen is that my stopping rule could find out, AFTER an observation and not having stopped at that observation, that it was the last one allowed. We assign the value of infinity to the stopping time, in this case. Now my question is: what filtration models what we know at a step in this game? My guess would be . This makes sense when inspected step by step: if the game starts (this is expressed by ), at time 1, when receiving performing the first observation, I will only know that it is not the case that there were no observations at all, but I do not know yet if the observation I am taking will be the last: thus is what I know. After making the decision whether to stop or continue, I will know , that is I will know whether was the last observation ( ) or not ( ). Next and so on. What confuses me is that I am used to filtration expressing all the information available at time , in the sense of all that could have happened by that time, but my construction seems a little different and I suspect that there is a mistake. In fact, say we are at time 3 in the game, the information is supposed to be . But clearly, if I am at time 3 in the game, just before I am inspecting , I know that , otherwise I would have not inspected at the second step. What I mean is that, focusing on the indicators, only sequences of zeroes and sequences of zeroes with one at the end are allowed, in terms of information, not every possible string of zero and ones. As soon as a one appears, the game stops and we set our stopping time to infinity. (If instead we stop by our decision, we have the stopped sigma algebra to model that.) Does this mean that the construction is redundant or even wrong? Thanks for any help.","\{X_i\} N \{X_i\} X_1,\ldots,X_N n\leq N \sigma(X_1, \ldots, X_n, 1_{\{N=0\}},\ldots, 1_{\{N=n-1\}}) 1_{\{N=0\}}=0 \sigma(X_1,I_{\{N=0\}}) \sigma(X_1,I_{\{N=0\}},I_{\{N=1\}}) X_1 I_{\{N=1\}}=1 I_{\{N=1\}}=0 \sigma(X_1,I_{\{N=0\}},I_{\{N=1\}}, X_2) n \sigma(X_1,I_{\{N=0\}},I_{\{N=1\}}, X_2) X_3 I_{\{N=0\}}=I_{\{N=1\}}=0 X_2","['probability', 'probability-theory', 'measure-theory', 'probability-distributions', 'independence']"
99,Inverse Hoeffding inequality for Rademacher random variables,Inverse Hoeffding inequality for Rademacher random variables,,"Let $Y_1,\dots,Y_k$ be independent and identically distributed Rademacher random variables, i.e. $\mathbb{P}(Y_1=1)=\mathbb{P}(Y_1=-1)=\frac{1}{2}$ , and take $p\in \mathbb{R}^k$ with $||p||_2=1$ . I want to prove the following inequality: $\mathbb{P}(\sum^k_{i=1}Y_ip_i>x)\geq e^{-cx^2}$ , where $x\geq a$ and $x\leq b(||p||_{\infty})^{-1}$ and $a$ and $b$ are some positive constants. I know the Hoeffding inequality and that this inequality is basically the inverse version. How could I approach this problem?","Let be independent and identically distributed Rademacher random variables, i.e. , and take with . I want to prove the following inequality: , where and and and are some positive constants. I know the Hoeffding inequality and that this inequality is basically the inverse version. How could I approach this problem?","Y_1,\dots,Y_k \mathbb{P}(Y_1=1)=\mathbb{P}(Y_1=-1)=\frac{1}{2} p\in \mathbb{R}^k ||p||_2=1 \mathbb{P}(\sum^k_{i=1}Y_ip_i>x)\geq e^{-cx^2} x\geq a x\leq b(||p||_{\infty})^{-1} a b","['probability', 'probability-theory', 'inequality']"
