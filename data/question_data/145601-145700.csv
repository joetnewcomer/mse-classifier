,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,How to find $\ \lim_{n \to \infty} \frac{\log n}{\sqrt n}$? [duplicate],How to find ? [duplicate],\ \lim_{n \to \infty} \frac{\log n}{\sqrt n},"This question already has answers here : How do I take the limit as $n$ goes to $\infty$ of $\frac{\sqrt{n}}{\log(n)}$? (3 answers) Closed 10 years ago . Find the limit of the following expression: $$\lim_{n \to \infty} \frac{\log n}{\sqrt n}$$ As I plot this function, it seems that this goes to $1$. Is it correct?","This question already has answers here : How do I take the limit as $n$ goes to $\infty$ of $\frac{\sqrt{n}}{\log(n)}$? (3 answers) Closed 10 years ago . Find the limit of the following expression: $$\lim_{n \to \infty} \frac{\log n}{\sqrt n}$$ As I plot this function, it seems that this goes to $1$. Is it correct?",,"['calculus', 'limits']"
1,Is this epsilon-delta proof correct?,Is this epsilon-delta proof correct?,,"Consider the function $f:\mathbb{R}\rightarrow\mathbb{R}$ $$f(x)=\begin{cases}x,\ x\in\mathbb{Q} \\ -x,\ x \notin \mathbb{Q}.\end{cases}$$ I'm trying to prove that for all $a \neq 0$, $\lim_{x \to a}f(x)$ does not exist. I tried to do this by contradiction, so my first step was to suppose that $$\lim_{x \to a}f(x)=A,$$ for some $A \in \mathbb{R}$. Then this implies that $$\forall \varepsilon>0\ \exists\delta:\ 0<|x-a|<\delta\implies|f(x)-A| < \varepsilon.$$ So I said, consider some $p \in \mathbb{Q}:0<|p-a|<\delta$, and therefore $|f(p) - A|=|p-a|<\varepsilon_1$. Also consider a $q \in \mathbb{R}, q \notin \mathbb{Q}:0<|q-a|<\delta$, and therefore $|f(q)-A|=|-q-A|=|q+a|<\varepsilon_2$. Since the epsilon-delta definition allows us to choose whatever $\varepsilon$ we want, I chose to let $\varepsilon_1 = p$ and $\varepsilon_2 = q$.  This then implies the following $$|p-A|<\varepsilon_1=p \implies p-A<p \implies A>0,$$ $$|q+A|<\varepsilon_2=q \implies q+A<q \implies A<0.$$ Since we cannot have $A>0$ and $A < 0 $, this is a contradiction, and therefore the limit does not exist. $\square$ Does this proof seem correct? In particular I'm concerned that I never made use of the fact $a \neq 0$, so I was hoping someone could review it and let me know if there are any errors. This problem is from Spivak's Calculus , 4th ed., and the proof give in the solution book is quite different than mine, so I wasn't able to check my answer that way. Thanks. EDIT: Here's the corrected proof. Assume for some $a > 0$ that $\lim_{x \to a} f(x) = A$, for some $A > 0$. Then from the epsilon-delta definition there is some delta such that $0 < |x-a| < \delta \implies |f(x) - A| < A$. Choose some irrational $q > 0$ such that $0 < |q - a| < \delta$, which implies that $|f(q) - A) = |-q - A| = |q + A| < A$, but this would mean that $q + A < A \implies q < 0$, a contradiction. To prove that $f$ does not approach a negative limit either, let $\varepsilon = -A$, and pick some $p \in \mathbb{Q}$ such that $0 < |p - a| < \delta$, so therefore $|f(p) - A| = |p - A| < -A$. This implies that $p - A < -A \implies p < 0$, again a contradiction. Finally, to prove that $f$ does not approach a $0$ limit, let $\varepsilon = a$, and then pick some $y > a$ such that $0 < |y-a| < \delta$. This would imply that $|f(y)| = y < a$, which is again a contradiction, so $f$ does not approach a limit for $a > 0$, and therefore $a < 0$. $\square$","Consider the function $f:\mathbb{R}\rightarrow\mathbb{R}$ $$f(x)=\begin{cases}x,\ x\in\mathbb{Q} \\ -x,\ x \notin \mathbb{Q}.\end{cases}$$ I'm trying to prove that for all $a \neq 0$, $\lim_{x \to a}f(x)$ does not exist. I tried to do this by contradiction, so my first step was to suppose that $$\lim_{x \to a}f(x)=A,$$ for some $A \in \mathbb{R}$. Then this implies that $$\forall \varepsilon>0\ \exists\delta:\ 0<|x-a|<\delta\implies|f(x)-A| < \varepsilon.$$ So I said, consider some $p \in \mathbb{Q}:0<|p-a|<\delta$, and therefore $|f(p) - A|=|p-a|<\varepsilon_1$. Also consider a $q \in \mathbb{R}, q \notin \mathbb{Q}:0<|q-a|<\delta$, and therefore $|f(q)-A|=|-q-A|=|q+a|<\varepsilon_2$. Since the epsilon-delta definition allows us to choose whatever $\varepsilon$ we want, I chose to let $\varepsilon_1 = p$ and $\varepsilon_2 = q$.  This then implies the following $$|p-A|<\varepsilon_1=p \implies p-A<p \implies A>0,$$ $$|q+A|<\varepsilon_2=q \implies q+A<q \implies A<0.$$ Since we cannot have $A>0$ and $A < 0 $, this is a contradiction, and therefore the limit does not exist. $\square$ Does this proof seem correct? In particular I'm concerned that I never made use of the fact $a \neq 0$, so I was hoping someone could review it and let me know if there are any errors. This problem is from Spivak's Calculus , 4th ed., and the proof give in the solution book is quite different than mine, so I wasn't able to check my answer that way. Thanks. EDIT: Here's the corrected proof. Assume for some $a > 0$ that $\lim_{x \to a} f(x) = A$, for some $A > 0$. Then from the epsilon-delta definition there is some delta such that $0 < |x-a| < \delta \implies |f(x) - A| < A$. Choose some irrational $q > 0$ such that $0 < |q - a| < \delta$, which implies that $|f(q) - A) = |-q - A| = |q + A| < A$, but this would mean that $q + A < A \implies q < 0$, a contradiction. To prove that $f$ does not approach a negative limit either, let $\varepsilon = -A$, and pick some $p \in \mathbb{Q}$ such that $0 < |p - a| < \delta$, so therefore $|f(p) - A| = |p - A| < -A$. This implies that $p - A < -A \implies p < 0$, again a contradiction. Finally, to prove that $f$ does not approach a $0$ limit, let $\varepsilon = a$, and then pick some $y > a$ such that $0 < |y-a| < \delta$. This would imply that $|f(y)| = y < a$, which is again a contradiction, so $f$ does not approach a limit for $a > 0$, and therefore $a < 0$. $\square$",,"['calculus', 'limits']"
2,I can't solve this limit...,I can't solve this limit...,,I tried to solve it as difference of two squares. But I guess I can't move any longer from that. Please help...,I tried to solve it as difference of two squares. But I guess I can't move any longer from that. Please help...,,['limits']
3,Question about polynomial $\sum_{j=1}^n j^k$,Question about polynomial,\sum_{j=1}^n j^k,"How could I prove that $ 1^k + 2^k + \cdots + n^k \in \Theta(n^{k+1}) $ or, equivalently, $$ 0 < \lim_{n\to\infty}\frac{\sum_{i=1}^n i^k}{n^{k+1}} < \infty? $$ I would appreciate a hint rather than a solution. Thanks in advance. ( I am sorry if this question is duplicate, I've searched but didn't found anything similar )","How could I prove that $ 1^k + 2^k + \cdots + n^k \in \Theta(n^{k+1}) $ or, equivalently, $$ 0 < \lim_{n\to\infty}\frac{\sum_{i=1}^n i^k}{n^{k+1}} < \infty? $$ I would appreciate a hint rather than a solution. Thanks in advance. ( I am sorry if this question is duplicate, I've searched but didn't found anything similar )",,"['real-analysis', 'limits', 'polynomials', 'computational-complexity']"
4,Finding the Limit $\lim_{x\to 0} \frac{\sin x(1 - \cos x)}{x^2}$,Finding the Limit,\lim_{x\to 0} \frac{\sin x(1 - \cos x)}{x^2},Here's the problem. $$\lim_{x\to 0} \frac{\sin x(1 - \cos x)}{x^2}$$ I really don't know where to start with this. Please help.,Here's the problem. $$\lim_{x\to 0} \frac{\sin x(1 - \cos x)}{x^2}$$ I really don't know where to start with this. Please help.,,"['calculus', 'limits', 'trigonometry']"
5,Why does $\lim_{x\to 0+} \sqrt{\frac {1}{x}+2}-\sqrt{\frac {1}{x}}$ equal zero?,Why does  equal zero?,\lim_{x\to 0+} \sqrt{\frac {1}{x}+2}-\sqrt{\frac {1}{x}},Limit $$\lim_{x\to 0+} \sqrt{\frac {1}{x}+2}-\sqrt{\frac {1}{x}}$$ equals zero. Could you help me prove it?,Limit $$\lim_{x\to 0+} \sqrt{\frac {1}{x}+2}-\sqrt{\frac {1}{x}}$$ equals zero. Could you help me prove it?,,['limits']
6,Limit of a function with factorial [duplicate],Limit of a function with factorial [duplicate],,"This question already has answers here : Prove that $\lim \limits_{n \to \infty} \frac{x^n}{n!} = 0$, $x \in \Bbb R$. (15 answers) Closed 10 years ago . $\lim_{n \rightarrow \infty} (x^n/n!)=0$. prove. x is finite whereas n is infinite. But increasing n means also increasing $x^n$. It is understandable that if n is too large n! will exceed $x^n$. How it can be proved in a mathematically precise way?","This question already has answers here : Prove that $\lim \limits_{n \to \infty} \frac{x^n}{n!} = 0$, $x \in \Bbb R$. (15 answers) Closed 10 years ago . $\lim_{n \rightarrow \infty} (x^n/n!)=0$. prove. x is finite whereas n is infinite. But increasing n means also increasing $x^n$. It is understandable that if n is too large n! will exceed $x^n$. How it can be proved in a mathematically precise way?",,"['limits', 'factorial']"
7,Limit: $\lim_{x \to 0}{[1+\ln{(1+x)}+\ln(1+2x)+\ldots+\ln(1+nx)]}^{1/x}.$,Limit:,\lim_{x \to 0}{[1+\ln{(1+x)}+\ln(1+2x)+\ldots+\ln(1+nx)]}^{1/x}.,How can I find the following limit? $$\lim_{x \to 0}{[1+\ln{(1+x)}+\ln(1+2x)+\ldots+\ln(1+nx)]}^{1/x}.$$ It's a limit of type $\displaystyle 1^{\infty}$ and if I note with $\displaystyle f(x)=\ln{(1+x)}+\ln(1+2x)+\ldots+\ln(1+nx)$ then I must do: $$\displaystyle \lim_{x \to 0}{(1+f(x))}^{1/x}=\lim_{x \to 0}[(1+f(x))^{\frac{1}{f(x)}}]^{\frac{f(x)}{x}}. $$ Is ok? Thanks :),How can I find the following limit? $$\lim_{x \to 0}{[1+\ln{(1+x)}+\ln(1+2x)+\ldots+\ln(1+nx)]}^{1/x}.$$ It's a limit of type $\displaystyle 1^{\infty}$ and if I note with $\displaystyle f(x)=\ln{(1+x)}+\ln(1+2x)+\ldots+\ln(1+nx)$ then I must do: $$\displaystyle \lim_{x \to 0}{(1+f(x))}^{1/x}=\lim_{x \to 0}[(1+f(x))^{\frac{1}{f(x)}}]^{\frac{f(x)}{x}}. $$ Is ok? Thanks :),,"['analysis', 'limits']"
8,Determine if a sequence converges using the number e,Determine if a sequence converges using the number e,,"Knowing that the number $e =\lim_{n\to\infty}\left(1+{1\over{n}}\right)^n$ solve $a_n=\left({n+1}\over{n+3}\right)^n$ So (...) $$\lim_{n\to\infty}\left({n+1}\over{n+3}\right)^n = \lim_{n\to\infty}\left({n+1+2-2}\over{n+3}\right)^n = \lim_{n\to\infty}\left(1-{2\over{n+3}}\right)^n = \lim_{n\to\infty}\left({1+{{1\over{-n-3}}\over2}}\right)^n = \lim_{n\to\infty}\left({1+{{1\over{-n-3}}\over2}}\right)^{{-n-3\over2}.{2\over{-n-3}}.n} = e^{\lim_{n\to\infty}{2n\over{-n-3}}}=e^{-2}$$ I know to solve it in this way, but my teacher told me that the in exams I can NOT solve it in this way.  The problem is that I can not put the denominator in the exponent and then multiply by the inverse to make it 1 (note that this does not alter the exercise). Is there another way to do this?","Knowing that the number $e =\lim_{n\to\infty}\left(1+{1\over{n}}\right)^n$ solve $a_n=\left({n+1}\over{n+3}\right)^n$ So (...) $$\lim_{n\to\infty}\left({n+1}\over{n+3}\right)^n = \lim_{n\to\infty}\left({n+1+2-2}\over{n+3}\right)^n = \lim_{n\to\infty}\left(1-{2\over{n+3}}\right)^n = \lim_{n\to\infty}\left({1+{{1\over{-n-3}}\over2}}\right)^n = \lim_{n\to\infty}\left({1+{{1\over{-n-3}}\over2}}\right)^{{-n-3\over2}.{2\over{-n-3}}.n} = e^{\lim_{n\to\infty}{2n\over{-n-3}}}=e^{-2}$$ I know to solve it in this way, but my teacher told me that the in exams I can NOT solve it in this way.  The problem is that I can not put the denominator in the exponent and then multiply by the inverse to make it 1 (note that this does not alter the exercise). Is there another way to do this?",,"['sequences-and-series', 'limits', 'convergence-divergence']"
9,To calculate $ \lim_{n\to \infty} \Big(\sum_{m=1}^r (a_m)^n\Big)^{1/n}$,To calculate, \lim_{n\to \infty} \Big(\sum_{m=1}^r (a_m)^n\Big)^{1/n},"Let $a_1 , a_2 , ..., a_r$ be positive real numbers such that $a_1 > a_2 > ... > a_r$. Without any more information given , can we exactly calculate $$ \lim_{n\to \infty} \Bigg(\sum_{m=1}^r (a_m)^n\Bigg)^{1/n}\ ?$$","Let $a_1 , a_2 , ..., a_r$ be positive real numbers such that $a_1 > a_2 > ... > a_r$. Without any more information given , can we exactly calculate $$ \lim_{n\to \infty} \Bigg(\sum_{m=1}^r (a_m)^n\Bigg)^{1/n}\ ?$$",,"['real-analysis', 'sequences-and-series', 'limits']"
10,Find the value of $\lim_{x \rightarrow \infty}\left(\frac{\pi}{2}-\tan^{-1}x\right)^{\Large\frac{1}{x}}$,Find the value of,\lim_{x \rightarrow \infty}\left(\frac{\pi}{2}-\tan^{-1}x\right)^{\Large\frac{1}{x}},Can this limit be solved without using L'Hopital's rule : $$\lim_{x \rightarrow \infty}\left(\frac{\pi}{2}-\tan^{-1}x\right)^{\Large\frac{1}{x}}$$ Answer of this limit is : $1$,Can this limit be solved without using L'Hopital's rule : $$\lim_{x \rightarrow \infty}\left(\frac{\pi}{2}-\tan^{-1}x\right)^{\Large\frac{1}{x}}$$ Answer of this limit is : $1$,,"['calculus', 'limits']"
11,Confusion over a limit. Different ways of solving give different answers?,Confusion over a limit. Different ways of solving give different answers?,,"Qn: If it is given that $$ \lim_{x\to\infty} \frac{x^2 - x - 2}{x + 1} - ax - b = 1 $$ then a and b must be? Now, I tried doing this by 2 methods. Method 1: $$ \frac{x^2 - x - 2}{x + 1} - ax - b $$ $$ = (x - 2) - ax - b $$ Since the limit is finite, $a$ must be $= 1$ and so, $b = -3$ Method 2: $$ \frac{x^2 - x - 2}{x + 1} - ax - b $$ $$ = \frac{x^2(1 - \frac 1 x - \frac2 {x^2})}{x(1 + \frac1x)} - ax - b $$ $$ = \frac{x - 1 - \frac2x}{(1 + \frac1x)} - ax - b $$ as $x \to \infty$, we have the above expression $$ = x - ax - b$$ So, $a = 1$ and $b = -1$ Which of the above is correct?","Qn: If it is given that $$ \lim_{x\to\infty} \frac{x^2 - x - 2}{x + 1} - ax - b = 1 $$ then a and b must be? Now, I tried doing this by 2 methods. Method 1: $$ \frac{x^2 - x - 2}{x + 1} - ax - b $$ $$ = (x - 2) - ax - b $$ Since the limit is finite, $a$ must be $= 1$ and so, $b = -3$ Method 2: $$ \frac{x^2 - x - 2}{x + 1} - ax - b $$ $$ = \frac{x^2(1 - \frac 1 x - \frac2 {x^2})}{x(1 + \frac1x)} - ax - b $$ $$ = \frac{x - 1 - \frac2x}{(1 + \frac1x)} - ax - b $$ as $x \to \infty$, we have the above expression $$ = x - ax - b$$ So, $a = 1$ and $b = -1$ Which of the above is correct?",,"['calculus', 'limits']"
12,Limit of Binomial distribution,Limit of Binomial distribution,,"In showing us that Binomial distribution: $$B_{N,p}(n) := \binom {N}{n} p^n(1-p)^{N-n}$$ tends to Poisson's: $$P_ \lambda (n) = \dfrac {\lambda ^n}{n!}e^{-\lambda}$$where I guess lambda should be defined as $\lambda:=\lim _N Np$ (it is the limit of the expected value of $\cal B$), my (mechanics) teacher did something i don't understand: he substituted $p=\frac {\lambda}{N}$ before evaluating the limit as $N$ goes to infty. Is this correct? If $\lambda$ is defined as I said above, then I think it isn't, for it must be calculated inside the limit substituting $p=\frac {Np}{N}$. Also, the simplification makes the computation of the limit much easier, so maybe that's why he did it. Note: I'm not looking for a full demonstration, I just want to know if the procedure of my teacher is correct. Related question : How would you prove $\lim _N (1-p)^N=e^{-\lambda}$, given $ \lim _N Np =\lambda \in \mathbb R $? Using the result: $$\lim _{N\rightarrow +\infty} (1+\frac{1}{N})^N = e$$ my attempt was: $$(1-p)^N=\left[\left(1-\dfrac{Np}{N}\right)^{-\frac{N}{Np}}\right]^{-Np}=\exp\left\{-Np\cdot\ln\left[\left(1-\dfrac{Np}{N}\right)^{-\frac{N}{Np}}\right]\right\}$$ $$\ell = \exp\left\{-\lim_{N\rightarrow+\infty}Np\cdot\ln\left[\lim_{N\rightarrow+\infty}\left(1-\dfrac{Np}{N}\right)^{-\frac{N}{Np}}\right]\right\}=e^{-\lambda}.$$ Notice that this is not immediate since $p=f(n)$.","In showing us that Binomial distribution: $$B_{N,p}(n) := \binom {N}{n} p^n(1-p)^{N-n}$$ tends to Poisson's: $$P_ \lambda (n) = \dfrac {\lambda ^n}{n!}e^{-\lambda}$$where I guess lambda should be defined as $\lambda:=\lim _N Np$ (it is the limit of the expected value of $\cal B$), my (mechanics) teacher did something i don't understand: he substituted $p=\frac {\lambda}{N}$ before evaluating the limit as $N$ goes to infty. Is this correct? If $\lambda$ is defined as I said above, then I think it isn't, for it must be calculated inside the limit substituting $p=\frac {Np}{N}$. Also, the simplification makes the computation of the limit much easier, so maybe that's why he did it. Note: I'm not looking for a full demonstration, I just want to know if the procedure of my teacher is correct. Related question : How would you prove $\lim _N (1-p)^N=e^{-\lambda}$, given $ \lim _N Np =\lambda \in \mathbb R $? Using the result: $$\lim _{N\rightarrow +\infty} (1+\frac{1}{N})^N = e$$ my attempt was: $$(1-p)^N=\left[\left(1-\dfrac{Np}{N}\right)^{-\frac{N}{Np}}\right]^{-Np}=\exp\left\{-Np\cdot\ln\left[\left(1-\dfrac{Np}{N}\right)^{-\frac{N}{Np}}\right]\right\}$$ $$\ell = \exp\left\{-\lim_{N\rightarrow+\infty}Np\cdot\ln\left[\lim_{N\rightarrow+\infty}\left(1-\dfrac{Np}{N}\right)^{-\frac{N}{Np}}\right]\right\}=e^{-\lambda}.$$ Notice that this is not immediate since $p=f(n)$.",,"['limits', 'probability-distributions', 'fake-proofs']"
13,Finding a limit where binomial coefficients appear as powers.,Finding a limit where binomial coefficients appear as powers.,,This one is for my mate that is 2 years older than me. Could you help please? $$\lim_{n\to\infty}\frac{\sqrt[\uproot{3}\Large 2^n]{n^{\textstyle\binom{n}{0}}\cdot (n+1)^{\textstyle\binom{n}{1\vphantom{0}}}\cdots(n+n)^{\textstyle\binom{n}{n\vphantom{0}}} }}{n}$$,This one is for my mate that is 2 years older than me. Could you help please? $$\lim_{n\to\infty}\frac{\sqrt[\uproot{3}\Large 2^n]{n^{\textstyle\binom{n}{0}}\cdot (n+1)^{\textstyle\binom{n}{1\vphantom{0}}}\cdots(n+n)^{\textstyle\binom{n}{n\vphantom{0}}} }}{n}$$,,"['sequences-and-series', 'limits']"
14,Is $f(x)=\frac { x }{ { e }^{ x }+1 } $ considered uniformly continuous?,Is  considered uniformly continuous?,f(x)=\frac { x }{ { e }^{ x }+1 } ,"According to a property we proved in my course: $f(x)$ is uniformly continuous on $(a,b) \Leftrightarrow  \exists { L }_{ 1 },{ L }_{ 2 }\in \mathbb R \ s.t.\ lim _{ x\rightarrow a  }{ f(x)={ L }_{ 1 },\lim _{ x\rightarrow b  }{ f(x)={ L }_{ 2 } }  } $ (and to be clearer: $\infty ,-\infty \notin \mathbb R$ ) I saw in some book that $\cfrac { x }{ { e }^{ x }+1 }$ is uniformly continuous and it makes sense because it's $\approx f(x) = x$ in the negatives and converges to $0$ on the positives. Just to make sure - that definition is only when $a,b\in\mathbb R$ ? I mean, even $f(x)=x$ is a counter example ...","According to a property we proved in my course: is uniformly continuous on (and to be clearer: ) I saw in some book that is uniformly continuous and it makes sense because it's in the negatives and converges to on the positives. Just to make sure - that definition is only when ? I mean, even is a counter example ...","f(x) (a,b) \Leftrightarrow  \exists { L }_{ 1 },{ L }_{ 2 }\in \mathbb R \ s.t.\ lim _{ x\rightarrow a  }{ f(x)={ L }_{ 1 },\lim _{ x\rightarrow b  }{ f(x)={ L }_{ 2 } }  }  \infty ,-\infty \notin \mathbb R \cfrac { x }{ { e }^{ x }+1 } \approx f(x) = x 0 a,b\in\mathbb R f(x)=x","['real-analysis', 'analysis', 'limits', 'uniform-continuity']"
15,"Finding the limit $\lim\limits_{x\to 0}\;\frac{f(x)}{\sin x}$ if $f(0) = 0$, $f'(0) = k > 0$ and $0 < f''(x) < f(x)$ for $x \in (0, \pi)$ [closed]","Finding the limit  if ,  and  for  [closed]","\lim\limits_{x\to 0}\;\frac{f(x)}{\sin x} f(0) = 0 f'(0) = k > 0 0 < f''(x) < f(x) x \in (0, \pi)","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question I would appreciate if somebody could help me with the following problem: $f(x)$ satisfies the  conditions $1,2,3$ : $f(0)=0$ $f'(0)=k>0$ $0<x<\pi$ $\Rightarrow$ $0\leq f''(x)\leq f(x)$ Find $\lim\limits_{x\to 0}\;\dfrac{f(x)}{\sin x}$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question I would appreciate if somebody could help me with the following problem: satisfies the  conditions : Find","f(x) 1,2,3 f(0)=0 f'(0)=k>0 0<x<\pi \Rightarrow 0\leq f''(x)\leq f(x) \lim\limits_{x\to 0}\;\dfrac{f(x)}{\sin x}","['calculus', 'limits']"
16,Help with limits algebraically,Help with limits algebraically,,"The question I am working on is evaluating $$ \lim_{x \rightarrow 5}\; \frac{x^2 - 6x + 5}{x - 5}$$ While I understand that the first step is to get the $x - 5$ out of the denominator so $5$ can be plugged in, I do not know how to manipulate the formula  to do that. Any information manipulating formulas would be extremely helpful","The question I am working on is evaluating $$ \lim_{x \rightarrow 5}\; \frac{x^2 - 6x + 5}{x - 5}$$ While I understand that the first step is to get the $x - 5$ out of the denominator so $5$ can be plugged in, I do not know how to manipulate the formula  to do that. Any information manipulating formulas would be extremely helpful",,"['calculus', 'limits']"
17,Is this limit correct?,Is this limit correct?,,"In $\mathbb R$, Suppose $\{x_n\}\to 0 $ and $\{y_n\}$ is bounded. In order to prove $\lim_{n\to \infty}x_ny_n=0$, I proved: $\lim_{n\to \infty}|x_ny_n|\leq M\lim_{n\to\infty}x=0$, and conclude the limit without absolute value exists, Is it correct?","In $\mathbb R$, Suppose $\{x_n\}\to 0 $ and $\{y_n\}$ is bounded. In order to prove $\lim_{n\to \infty}x_ny_n=0$, I proved: $\lim_{n\to \infty}|x_ny_n|\leq M\lim_{n\to\infty}x=0$, and conclude the limit without absolute value exists, Is it correct?",,"['sequences-and-series', 'limits']"
18,Evaluate $\lim_{n\to\infty}n\sin(2\pi \mathrm en!)$ [duplicate],Evaluate  [duplicate],\lim_{n\to\infty}n\sin(2\pi \mathrm en!),This question already has answers here : What is the limit of $n \sin (2 \pi \cdot e \cdot n!)$ as $n$ goes to infinity? (3 answers) Closed 10 years ago . Evaluate $$\lim_{n\to\infty}n\sin(2\pi \mathrm en!)$$,This question already has answers here : What is the limit of $n \sin (2 \pi \cdot e \cdot n!)$ as $n$ goes to infinity? (3 answers) Closed 10 years ago . Evaluate $$\lim_{n\to\infty}n\sin(2\pi \mathrm en!)$$,,"['real-analysis', 'analysis', 'limits']"
19,How do I apply an epsilon delta proof to the following problem?,How do I apply an epsilon delta proof to the following problem?,,"Any help in solving the following problem would be greatly appreciated: Let $f, g_1, g_2$ be functions from $\mathbb R$ to $\mathbb R$, with   $g_1(x) \leq f(x) \leq  g_2(x)$, for all $x \in \mathbb R$. Suppose   that, for some $p \in \mathbb R$, we have $\lim_{x \rightarrow p} g_1(x) = \lim_{x \rightarrow p} g_2(x) = c$. Show that $ \lim_{x\rightarrow p} f(x) = c,$    as well. I've spent hours on it and haven't come up with anything worth posting.","Any help in solving the following problem would be greatly appreciated: Let $f, g_1, g_2$ be functions from $\mathbb R$ to $\mathbb R$, with   $g_1(x) \leq f(x) \leq  g_2(x)$, for all $x \in \mathbb R$. Suppose   that, for some $p \in \mathbb R$, we have $\lim_{x \rightarrow p} g_1(x) = \lim_{x \rightarrow p} g_2(x) = c$. Show that $ \lim_{x\rightarrow p} f(x) = c,$    as well. I've spent hours on it and haven't come up with anything worth posting.",,"['limits', 'continuity']"
20,Is $\lim_{n \to \infty} \frac{n}{(n!)^\frac{1}{n}} = e$ any easier than Stirling? [duplicate],Is  any easier than Stirling? [duplicate],\lim_{n \to \infty} \frac{n}{(n!)^\frac{1}{n}} = e,"This question already has answers here : Closed 11 years ago . Possible Duplicate: Showing that $\frac{\sqrt[n]{n!}}{n}$ $\rightarrow \frac{1}{e}$ Stirling's approximation says that $$ \lim_{n \to \infty} \frac{n^n \sqrt{n}}{n!  e^n }  = \frac{1}{\sqrt{2 \pi}}.$$ Since $\lim_{n \to \infty} x^\frac{1}{n} \to 1$ uniformly on a neighbourhood of $\frac{1}{\sqrt{ 2 \pi}}$, it follows that $$\lim_{n \to \infty} \left( \frac{n^n \sqrt{n}}{n!  e^n } \right)^\frac{1}{n} = \lim_{n \to \infty} \frac{n}{(n!)^\frac{1}{n}} \cdot \frac{n^\frac{1}{2n}}{e} = 1.$$ Since $\lim_{n \to \infty} n^\frac{1}{2n} = 1$, we get the limit in the title $$\lim_{n \to \infty} \frac{n}{(n!)^\frac{1}{n}} = e.$$ Question : Is Stirling's approximation is really needed to derive the above limit? Or is there an easier way to reach the same conclusion? Motivation: The radius of convergence $R$ of a power series $\sum_{n=0}^\infty a_nx^n$  is given by Hadamard's formula $$\frac{1}{R} = \limsup_{n \to \infty} |a_n|^\frac{1}{n}.$$ If we know ahead of time that $R > 0$ then the coefficients are given by  $$ a_n = \frac{f^{(n)}(0)}{n!}$$ where $f$ is the function defined by the power series. Then we get $$\frac{1}{R} = \limsup_{n \to \infty} |a_n|^\frac{1}{n} = \limsup_{n \to \infty} \frac{|f^{(n)}(0)|^\frac{1}{n}}{n} \cdot \frac{n}{(n!)^\frac{1}{n}} = e \cdot \limsup_{n \to \infty} \frac{|f^{(n)}(0)|^\frac{1}{n}}{n}.$$ So one use of the limit is to clean up the formula for the radius of convergence of a power series in terms of the derivatives of the corresponding function.","This question already has answers here : Closed 11 years ago . Possible Duplicate: Showing that $\frac{\sqrt[n]{n!}}{n}$ $\rightarrow \frac{1}{e}$ Stirling's approximation says that $$ \lim_{n \to \infty} \frac{n^n \sqrt{n}}{n!  e^n }  = \frac{1}{\sqrt{2 \pi}}.$$ Since $\lim_{n \to \infty} x^\frac{1}{n} \to 1$ uniformly on a neighbourhood of $\frac{1}{\sqrt{ 2 \pi}}$, it follows that $$\lim_{n \to \infty} \left( \frac{n^n \sqrt{n}}{n!  e^n } \right)^\frac{1}{n} = \lim_{n \to \infty} \frac{n}{(n!)^\frac{1}{n}} \cdot \frac{n^\frac{1}{2n}}{e} = 1.$$ Since $\lim_{n \to \infty} n^\frac{1}{2n} = 1$, we get the limit in the title $$\lim_{n \to \infty} \frac{n}{(n!)^\frac{1}{n}} = e.$$ Question : Is Stirling's approximation is really needed to derive the above limit? Or is there an easier way to reach the same conclusion? Motivation: The radius of convergence $R$ of a power series $\sum_{n=0}^\infty a_nx^n$  is given by Hadamard's formula $$\frac{1}{R} = \limsup_{n \to \infty} |a_n|^\frac{1}{n}.$$ If we know ahead of time that $R > 0$ then the coefficients are given by  $$ a_n = \frac{f^{(n)}(0)}{n!}$$ where $f$ is the function defined by the power series. Then we get $$\frac{1}{R} = \limsup_{n \to \infty} |a_n|^\frac{1}{n} = \limsup_{n \to \infty} \frac{|f^{(n)}(0)|^\frac{1}{n}}{n} \cdot \frac{n}{(n!)^\frac{1}{n}} = e \cdot \limsup_{n \to \infty} \frac{|f^{(n)}(0)|^\frac{1}{n}}{n}.$$ So one use of the limit is to clean up the formula for the radius of convergence of a power series in terms of the derivatives of the corresponding function.",,"['limits', 'power-series']"
21,How to understand limit,How to understand limit,,"Let $\chi(t)$ be the Heaviside function, i.e. $\chi(t) = 1$ for $t > 0$ and $\chi(t) = 0$ if $t \leq 0$. Reading a paper I faced with a statement that $$    \frac{t^{p-1}}{\Gamma(p)}\chi(t) \to \delta^{(k)} $$ when $p \to -k$, where $k = 1,2,\ldots$ What does it mean? I can't integrate a function on the left when $\Re p < 0$ on intervals containing $0$.","Let $\chi(t)$ be the Heaviside function, i.e. $\chi(t) = 1$ for $t > 0$ and $\chi(t) = 0$ if $t \leq 0$. Reading a paper I faced with a statement that $$    \frac{t^{p-1}}{\Gamma(p)}\chi(t) \to \delta^{(k)} $$ when $p \to -k$, where $k = 1,2,\ldots$ What does it mean? I can't integrate a function on the left when $\Re p < 0$ on intervals containing $0$.",,"['limits', 'distribution-theory']"
22,Absolute value and limit reasoning.,Absolute value and limit reasoning.,,"I am trying to develop my reasoning ability with absolute value. So, I wanted to know if the following reasoning is correct: Find $\lim_{x \to -6}\dfrac{2x+12}{|x+6|}$ By definition of absolute value we have $|x| = x$ when $x > 0$ and $|x| = -x$ when $x<0$ So for the above limit we can consider the limit from the left and the limit from the right: $(x+6)<0$ and $(x+6)>0$: Case $(x+6)<0$: $\dfrac{2x+12}{-(x+6)} = \dfrac{2(x+6)}{-(x+6)} = -2$ Case $(x+6) > 0$: $\dfrac{2x+12}{(x+6)} = \dfrac{2(x+6)}{(x+6)} = 2$ Hence, the limits from the left and right are not and equal and we conclude that the limit does not exist. If we were not considering the limit at $-6$ we could just evaluate the function at any point since the function is continuous everywhere else I graphed this function and I see that I have a vertical asymptote at $x=-6$. What do these left and right limits evaluating to $-2$ and $2$ mean then?","I am trying to develop my reasoning ability with absolute value. So, I wanted to know if the following reasoning is correct: Find $\lim_{x \to -6}\dfrac{2x+12}{|x+6|}$ By definition of absolute value we have $|x| = x$ when $x > 0$ and $|x| = -x$ when $x<0$ So for the above limit we can consider the limit from the left and the limit from the right: $(x+6)<0$ and $(x+6)>0$: Case $(x+6)<0$: $\dfrac{2x+12}{-(x+6)} = \dfrac{2(x+6)}{-(x+6)} = -2$ Case $(x+6) > 0$: $\dfrac{2x+12}{(x+6)} = \dfrac{2(x+6)}{(x+6)} = 2$ Hence, the limits from the left and right are not and equal and we conclude that the limit does not exist. If we were not considering the limit at $-6$ we could just evaluate the function at any point since the function is continuous everywhere else I graphed this function and I see that I have a vertical asymptote at $x=-6$. What do these left and right limits evaluating to $-2$ and $2$ mean then?",,"['calculus', 'real-analysis', 'limits']"
23,Sequence's Limit when it goes to infinity,Sequence's Limit when it goes to infinity,,"Let $\{a_n\}$ be a sequnce. Then $a_n \to -\infty$ if $\forall K < 0 \;\exists N \;\forall n \ge N:a_n < K$ Show that : If $a_n → -\infty$, $a_n \ne 0$, then $1/a_n→0$ ; and If $a_n < 0$, $a_n → 0$, then $1/a_n→−∞$ From the highschool I know that this is true. But do not know how to prove it. Can you help me?","Let $\{a_n\}$ be a sequnce. Then $a_n \to -\infty$ if $\forall K < 0 \;\exists N \;\forall n \ge N:a_n < K$ Show that : If $a_n → -\infty$, $a_n \ne 0$, then $1/a_n→0$ ; and If $a_n < 0$, $a_n → 0$, then $1/a_n→−∞$ From the highschool I know that this is true. But do not know how to prove it. Can you help me?",,"['sequences-and-series', 'limits']"
24,Use $\epsilon\ -\ \delta$ definition to prove $\lim\limits_{x \rightarrow x_0}\sqrt[3]{f(x)} = \sqrt[3]{A}$,Use  definition to prove,\epsilon\ -\ \delta \lim\limits_{x \rightarrow x_0}\sqrt[3]{f(x)} = \sqrt[3]{A},"It's known that $\lim\limits_{x \rightarrow x_0}f(x) = A$, how to prove $\lim\limits_{x \rightarrow x_0}\sqrt[3]{f(x)} = \sqrt[3]{A}$? Here's what I've got now: When $A = 0$, to prove $\lim\limits_{x \rightarrow x_0}\sqrt[3]{f(x)} = 0$: Since we have $\lim\limits_{x \rightarrow x_0}f(x) = A = 0$, so $|f(x)| < \epsilon$. => $|\sqrt[3]{f(x)}| < \epsilon_0^3 < \epsilon$ When $A \ne 0$, $|\sqrt[3]{f(x)} - \sqrt[3]{A}| = \frac{|f(x) - A|}{|f(x)^{\frac{2}{3}}+(f(x)A)^{\frac{1}{3}} + A^{\frac{2}{3}}|}$... How can I deal with $(f(x)A)^{\frac{1}{3}}$? Thanks.","It's known that $\lim\limits_{x \rightarrow x_0}f(x) = A$, how to prove $\lim\limits_{x \rightarrow x_0}\sqrt[3]{f(x)} = \sqrt[3]{A}$? Here's what I've got now: When $A = 0$, to prove $\lim\limits_{x \rightarrow x_0}\sqrt[3]{f(x)} = 0$: Since we have $\lim\limits_{x \rightarrow x_0}f(x) = A = 0$, so $|f(x)| < \epsilon$. => $|\sqrt[3]{f(x)}| < \epsilon_0^3 < \epsilon$ When $A \ne 0$, $|\sqrt[3]{f(x)} - \sqrt[3]{A}| = \frac{|f(x) - A|}{|f(x)^{\frac{2}{3}}+(f(x)A)^{\frac{1}{3}} + A^{\frac{2}{3}}|}$... How can I deal with $(f(x)A)^{\frac{1}{3}}$? Thanks.",,"['real-analysis', 'limits']"
25,Limit of Integral with a Limit,Limit of Integral with a Limit,,"Here's a little question I saw in a book recently, which I can see but can't set out my own formal proof and it's annoying me. Say $\lim_{x\to\infty} g(x) = a$ and $g$ is continuous, so now prove that $\lim_{x\to\infty} \frac{1}{x} \int_0^x g(y) \mathrm{d}y = a$ . I can see that if we define $\int_0^x g(y)\mathrm{d}y  = G(x) - G(0)$ then $\frac{1}{x} \int_0^x g(y) \mathrm{d}y = \frac{G(x) - G(0)}{x}$ which clearly looks a lot like the limit of a differential, but I'm not sure how to handle the limit?!","Here's a little question I saw in a book recently, which I can see but can't set out my own formal proof and it's annoying me. Say $\lim_{x\to\infty} g(x) = a$ and $g$ is continuous, so now prove that $\lim_{x\to\infty} \frac{1}{x} \int_0^x g(y) \mathrm{d}y = a$ . I can see that if we define $\int_0^x g(y)\mathrm{d}y  = G(x) - G(0)$ then $\frac{1}{x} \int_0^x g(y) \mathrm{d}y = \frac{G(x) - G(0)}{x}$ which clearly looks a lot like the limit of a differential, but I'm not sure how to handle the limit?!",,"['integration', 'limits']"
26,How can I solve this limit?,How can I solve this limit?,,$\lim_{x\to 1} \frac{\sin (x-1)}{x-1}$ I know the answer equals $1$ because $\lim_{x\to 0} \frac{\sin (x)}{x} = 1$ and in the following question $x-1$ gets arbitrary close to 0 so the same thing is happening. What I need is some steps to basically show that the question was not solved by a calculator. I tried to use $\sin(A-B) = \sin A \mathrm{cos}B  - \sin B \cos A $ but I had a $\frac {0}0$ which is obviously wrong. Any help/tip would be great.,$\lim_{x\to 1} \frac{\sin (x-1)}{x-1}$ I know the answer equals $1$ because $\lim_{x\to 0} \frac{\sin (x)}{x} = 1$ and in the following question $x-1$ gets arbitrary close to 0 so the same thing is happening. What I need is some steps to basically show that the question was not solved by a calculator. I tried to use $\sin(A-B) = \sin A \mathrm{cos}B  - \sin B \cos A $ but I had a $\frac {0}0$ which is obviously wrong. Any help/tip would be great.,,"['trigonometry', 'limits']"
27,Techniques for solving limit with multi-variables,Techniques for solving limit with multi-variables,,"I am learning to solve limit with multi-variables by myself. but I don't understand some concepts when we apply the technique. Example (1) : $f(x,y)=\frac{x+y}{x^2+y^2}$ , $g(x,y,z)=\frac{xyz}{x^2+y^2+z^2}$ , $h(x,y)=\frac{xy}{x^2+y^2}$ Firstly, when we solve $\lim_{(x,y)\to(0,0)}f(x,y)$ , why can we apply the following technique in case of $f(x,y)$ $\lim_{x\to0}f(x,0)\neq\lim_{y\to0}f(0,y) \Rightarrow \lim_{(x,y)\to(0,0)}f(x,y)$ doesn't exist , but we can't apply the technique in case of $g(x,y,z)$. Secondly, In example (1) , which function(s) can we apply polar , cylindrical and spherical coordinate system to replace the variables and please explain briefly. Thirdly, $h(x,y)$ , we can consider $(x,y)\to(0,0)$ along the line $y=mx$ can we apply the same concept in case of $f(x,y)$ ?? And what kinds of the limit can apply the technique ?? Thanks for your help :)","I am learning to solve limit with multi-variables by myself. but I don't understand some concepts when we apply the technique. Example (1) : $f(x,y)=\frac{x+y}{x^2+y^2}$ , $g(x,y,z)=\frac{xyz}{x^2+y^2+z^2}$ , $h(x,y)=\frac{xy}{x^2+y^2}$ Firstly, when we solve $\lim_{(x,y)\to(0,0)}f(x,y)$ , why can we apply the following technique in case of $f(x,y)$ $\lim_{x\to0}f(x,0)\neq\lim_{y\to0}f(0,y) \Rightarrow \lim_{(x,y)\to(0,0)}f(x,y)$ doesn't exist , but we can't apply the technique in case of $g(x,y,z)$. Secondly, In example (1) , which function(s) can we apply polar , cylindrical and spherical coordinate system to replace the variables and please explain briefly. Thirdly, $h(x,y)$ , we can consider $(x,y)\to(0,0)$ along the line $y=mx$ can we apply the same concept in case of $f(x,y)$ ?? And what kinds of the limit can apply the technique ?? Thanks for your help :)",,"['limits', 'multivariable-calculus']"
28,Finding the limit of the following expression,Finding the limit of the following expression,,"Given that $f$ is continuous and non-negative on $[0,1]$, I was asked to find the limit of: $$\displaystyle\lim_{n \rightarrow \infty}\left(\int_0^1(f(x))^{n} dx\right)^{\frac{1}{n}}$$","Given that $f$ is continuous and non-negative on $[0,1]$, I was asked to find the limit of: $$\displaystyle\lim_{n \rightarrow \infty}\left(\int_0^1(f(x))^{n} dx\right)^{\frac{1}{n}}$$",,"['integration', 'limits']"
29,"Prove $\lim_{x \to +\infty} \frac{f(x)}{(1 + x^2)}\ = 0$ for all $f(x)$ uniformly continuous on $[0, \infty)$.",Prove  for all  uniformly continuous on .,"\lim_{x \to +\infty} \frac{f(x)}{(1 + x^2)}\ = 0 f(x) [0, \infty)","This question is from a bank of past master's exams. Here is my initial, albeit handwavy, intuition. In essence, I try to show that the uniform continuity condition on $f(x)$ prevents it from growing faster than the denominator. Let $g(x) = x^2 + 1$. Let $\epsilon > 0$ be given. $f$ is uniformly continuous on $[0, \infty)$, so there exists a $\delta_\epsilon$ such that $$|f(x) - f(y)| < \epsilon$$ when $$|x - y| < \delta_\epsilon$$ for all $x, y\in [0, \infty)$. Turning our attention now to the denominator, let $x = y + \frac{\delta_\epsilon}{2}$. Then $|x - y| < \delta_\epsilon$, but $$|(x^2 - 1) - (y^2 - 1)| =$$ $$|x^2 - y^2| =$$ $$|x -y||x + y| <$$ $$\delta_\epsilon |x + y| =$$ $$\delta_\epsilon|2y + \frac{\delta_\epsilon}{2}| =$$ $$\delta_\epsilon(2y + \frac{\delta_\epsilon}{2}), $$ since $y, \delta_\epsilon > 0$. Now, this last expression is greater than $\epsilon$ when $y > \frac{2\epsilon - \delta_\epsilon^2}{4\delta_\epsilon}$. Let $\Delta_{x,y} g = |g(x) - g(y)|$ and $\Delta_{x,y} f = |f(x) - f(y)|$. We have shown that, for the values of $x$ and $y$ chosen above, $$\Delta_{x,y} f < \Delta_{x,y} g.$$ Recall that this inequality only holds because we chose a certain value of $y$, one dependent exclusively on $\epsilon$. But, since $\epsilon$ was arbitrary and $x$ was chosen to be some function of $y$, we have that $\Delta_{x,y} f < \Delta_{x,y} g$ on every interval $[x, y]$ (this is one part that I'm unsure of). And, presto changeo, this implies the desired limit. Is this intuition correct? Is there a better way?","This question is from a bank of past master's exams. Here is my initial, albeit handwavy, intuition. In essence, I try to show that the uniform continuity condition on $f(x)$ prevents it from growing faster than the denominator. Let $g(x) = x^2 + 1$. Let $\epsilon > 0$ be given. $f$ is uniformly continuous on $[0, \infty)$, so there exists a $\delta_\epsilon$ such that $$|f(x) - f(y)| < \epsilon$$ when $$|x - y| < \delta_\epsilon$$ for all $x, y\in [0, \infty)$. Turning our attention now to the denominator, let $x = y + \frac{\delta_\epsilon}{2}$. Then $|x - y| < \delta_\epsilon$, but $$|(x^2 - 1) - (y^2 - 1)| =$$ $$|x^2 - y^2| =$$ $$|x -y||x + y| <$$ $$\delta_\epsilon |x + y| =$$ $$\delta_\epsilon|2y + \frac{\delta_\epsilon}{2}| =$$ $$\delta_\epsilon(2y + \frac{\delta_\epsilon}{2}), $$ since $y, \delta_\epsilon > 0$. Now, this last expression is greater than $\epsilon$ when $y > \frac{2\epsilon - \delta_\epsilon^2}{4\delta_\epsilon}$. Let $\Delta_{x,y} g = |g(x) - g(y)|$ and $\Delta_{x,y} f = |f(x) - f(y)|$. We have shown that, for the values of $x$ and $y$ chosen above, $$\Delta_{x,y} f < \Delta_{x,y} g.$$ Recall that this inequality only holds because we chose a certain value of $y$, one dependent exclusively on $\epsilon$. But, since $\epsilon$ was arbitrary and $x$ was chosen to be some function of $y$, we have that $\Delta_{x,y} f < \Delta_{x,y} g$ on every interval $[x, y]$ (this is one part that I'm unsure of). And, presto changeo, this implies the desired limit. Is this intuition correct? Is there a better way?",,['real-analysis']
30,Calculating limits of a function of 2 or 3 variables,Calculating limits of a function of 2 or 3 variables,,"I have to calculate these two limits, and have no idea where to start from. Your guidance for how should I start working with it can help me a lot. 1) $\lim\limits_{(x,y,z)\rightarrow (0,0,0)} (1+xyz)^{(x^2+y^2+z^2)^{-1}}$ 2)$\lim\limits_{(x,y)\rightarrow (0,0)} \dfrac{4y^2+3xy^2+2x^2}{x^2+2y^2}$ Thanks in advance.","I have to calculate these two limits, and have no idea where to start from. Your guidance for how should I start working with it can help me a lot. 1) $\lim\limits_{(x,y,z)\rightarrow (0,0,0)} (1+xyz)^{(x^2+y^2+z^2)^{-1}}$ 2)$\lim\limits_{(x,y)\rightarrow (0,0)} \dfrac{4y^2+3xy^2+2x^2}{x^2+2y^2}$ Thanks in advance.",,['limits']
31,Why does $\frac{s}{s-1} > \zeta(s) > \frac{1}{s-1}$ imply $\lim_{s \to 1^{+}}(s-1)\zeta(s)=1$?,Why does  imply ?,\frac{s}{s-1} > \zeta(s) > \frac{1}{s-1} \lim_{s \to 1^{+}}(s-1)\zeta(s)=1,"I am reading the paper Dirichlet's theorem: a real variable approach by Robin Chapman. In this paper, he constructs a proof via real analysis rather than complex analysis that $\zeta(s)$ is convergent if and only if $s>1$. However, this is a standard fact known about $\zeta(s)$. What confuses me is this: He states as a consequence of the inequality $$\frac{s}{s-1} > \zeta(s) > \frac{1}{s-1},$$ the following limit is true: $$\lim_{s \to 1^{+}}(s-1)\zeta(s)=1.$$ I know this is probably a stupid question, but I'm not that great with limits. I can't quite see where this reasoning is derived from. Is this the case because of the equivalent inequality $$s>(s-1)\zeta(s)>1$$ where $s >1$? If so, how? Could anyone care to elucidate this rudimentary step in logic for me?","I am reading the paper Dirichlet's theorem: a real variable approach by Robin Chapman. In this paper, he constructs a proof via real analysis rather than complex analysis that $\zeta(s)$ is convergent if and only if $s>1$. However, this is a standard fact known about $\zeta(s)$. What confuses me is this: He states as a consequence of the inequality $$\frac{s}{s-1} > \zeta(s) > \frac{1}{s-1},$$ the following limit is true: $$\lim_{s \to 1^{+}}(s-1)\zeta(s)=1.$$ I know this is probably a stupid question, but I'm not that great with limits. I can't quite see where this reasoning is derived from. Is this the case because of the equivalent inequality $$s>(s-1)\zeta(s)>1$$ where $s >1$? If so, how? Could anyone care to elucidate this rudimentary step in logic for me?",,"['limits', 'riemann-zeta']"
32,Computing a limit which i suspect should evaluate to $\dfrac{\sin x}{x}$,Computing a limit which i suspect should evaluate to,\dfrac{\sin x}{x},"Firstly i confess i came across this limit trying to solve a question which has been answered in yet another manner, but I was curious and want to learn if this limit could be computed. I could have made a mistake (although i did double check before posting) or my approach may not yield due to following a red herring, but i suppose it is all part of the learning process.I was hoping some one could shed some light on this.  $$\lim _{n\rightarrow \infty }\dfrac {1} {n}\left( \dfrac {\sin \dfrac {x} {2}\cos \dfrac {x} {2}\left(\dfrac {1} {n}-1\right)} {\sin \dfrac {x} {2n}}\right) $$ I suspect this should evaluate to $\dfrac{\sin x}{x}$ though i am unsure how to proceed i think the denominators are the source of my troubles. My apologies if this is trivial","Firstly i confess i came across this limit trying to solve a question which has been answered in yet another manner, but I was curious and want to learn if this limit could be computed. I could have made a mistake (although i did double check before posting) or my approach may not yield due to following a red herring, but i suppose it is all part of the learning process.I was hoping some one could shed some light on this.  $$\lim _{n\rightarrow \infty }\dfrac {1} {n}\left( \dfrac {\sin \dfrac {x} {2}\cos \dfrac {x} {2}\left(\dfrac {1} {n}-1\right)} {\sin \dfrac {x} {2n}}\right) $$ I suspect this should evaluate to $\dfrac{\sin x}{x}$ though i am unsure how to proceed i think the denominators are the source of my troubles. My apologies if this is trivial",,"['real-analysis', 'limits']"
33,How should I deal with this two-dimensional $\frac{0}{0}$ limit?,How should I deal with this two-dimensional  limit?,\frac{0}{0},"Here is my question : Does the following limit exist?   $$ \lim_{y\to\xi}\frac{(\xi_i-y_i)(\xi_j-y_j)({{\xi}-y})\cdot n(y)}{|\xi-y|^5},\quad 1\leq i,j\leq 3,\tag{*} $$   where $S\subset{\mathbb R}^3$ is a surface which has a continuously varying normal vector, $\xi=(\xi_1,\xi_2,\xi_3)\in S$, $y=(y_1,y_2,y_3)\in S$, $n(y)$ is the [EDITED: unit] normal vector at point $y$. Here $(\xi-y)\cdot n(y)$ is the dot product. In the spirit of Polya, I find a simpler case where $S$ is a unit sphere. Then we have $n(y)=y$. But I don't have a strategy to go on.","Here is my question : Does the following limit exist?   $$ \lim_{y\to\xi}\frac{(\xi_i-y_i)(\xi_j-y_j)({{\xi}-y})\cdot n(y)}{|\xi-y|^5},\quad 1\leq i,j\leq 3,\tag{*} $$   where $S\subset{\mathbb R}^3$ is a surface which has a continuously varying normal vector, $\xi=(\xi_1,\xi_2,\xi_3)\in S$, $y=(y_1,y_2,y_3)\in S$, $n(y)$ is the [EDITED: unit] normal vector at point $y$. Here $(\xi-y)\cdot n(y)$ is the dot product. In the spirit of Polya, I find a simpler case where $S$ is a unit sphere. Then we have $n(y)=y$. But I don't have a strategy to go on.",,['differential-geometry']
34,A question regarding limits,A question regarding limits,,"I have this limit: $$\lim_{x \to 1} \frac{2x^2-x-6}{x(x-1)^3}$$ This can be written as: $$\lim_{x \to 1^+} \approx \frac{-5}{1\times\mbox{tiny positive}} \to - \infty$$ Why is that? I mean, let's plug in some numbers.  $-5/1.0000000001$ is almost $-5$ , the greater the denominator becomes the close the number to $-5$ Can anybody tell me why the book says it goes to -infinity? Thanks a lot","I have this limit: $$\lim_{x \to 1} \frac{2x^2-x-6}{x(x-1)^3}$$ This can be written as: $$\lim_{x \to 1^+} \approx \frac{-5}{1\times\mbox{tiny positive}} \to - \infty$$ Why is that? I mean, let's plug in some numbers.  $-5/1.0000000001$ is almost $-5$ , the greater the denominator becomes the close the number to $-5$ Can anybody tell me why the book says it goes to -infinity? Thanks a lot",,"['calculus', 'limits']"
35,Continuity of $f(x)$ involving infinity,Continuity of  involving infinity,f(x),"$f(x)= \frac{\sin(\pi x)}{x(1-x)}$ How can I define $f(0)$ and $f(1)$ to make $f(x)$ continuous on $[0,1]$? I've found that the limit at $0 = \pi$, and the limit from the left at $1 = \infty$. I understand that if $f(0)=\pi$ then $f(x)$ is continuous on $[0,1)$, but must I define $f(1)=\infty$? Would that make $f(x)$ continuous? EDIT: Also, can someone explain why $\lim\limits_{x \to 0} \frac{\sin\pi x}{x} = \pi$?","$f(x)= \frac{\sin(\pi x)}{x(1-x)}$ How can I define $f(0)$ and $f(1)$ to make $f(x)$ continuous on $[0,1]$? I've found that the limit at $0 = \pi$, and the limit from the left at $1 = \infty$. I understand that if $f(0)=\pi$ then $f(x)$ is continuous on $[0,1)$, but must I define $f(1)=\infty$? Would that make $f(x)$ continuous? EDIT: Also, can someone explain why $\lim\limits_{x \to 0} \frac{\sin\pi x}{x} = \pi$?",,"['calculus', 'limits', 'infinity']"
36,Finding the largest possible delta in a epsilon-delta limit?,Finding the largest possible delta in a epsilon-delta limit?,,"$$\lim_{x\to0}\frac{x^2+2x}{x^2-3x} = - \frac{2}{3}$$ Given that $\epsilon = 0.01$, what is the largest possible $\delta$ according to the epsilon-delta definition of the limit? Here's what I've done by myself, but I'm getting stuck: $$\left|\frac{x^2+2x}{x^2-3x}+\frac{2}{3}\right|<\epsilon\ \text{ if }0 <x<\delta$$ $$\left|\frac{3x^2+6x+2x^2-6x}{3x^2-9x}\right|<\epsilon\ \text{ if }0 <x<\delta$$ $$\left|\frac{5x^2}{3x^2-9x}\right|<\epsilon\ \text{ if }0 <x<\delta$$ $$\left|\frac{5x}{3x-9}\right|<\epsilon\ \text{ if }0 <x<\delta$$ I could split the absolute value into two separate ones and multiply the denominator up to the other side, but that wouldn't get me any closer to isolating x. Even plugging in 0.01 for $\epsilon$ doesn't seem to help. What am I doing wrong?","$$\lim_{x\to0}\frac{x^2+2x}{x^2-3x} = - \frac{2}{3}$$ Given that $\epsilon = 0.01$, what is the largest possible $\delta$ according to the epsilon-delta definition of the limit? Here's what I've done by myself, but I'm getting stuck: $$\left|\frac{x^2+2x}{x^2-3x}+\frac{2}{3}\right|<\epsilon\ \text{ if }0 <x<\delta$$ $$\left|\frac{3x^2+6x+2x^2-6x}{3x^2-9x}\right|<\epsilon\ \text{ if }0 <x<\delta$$ $$\left|\frac{5x^2}{3x^2-9x}\right|<\epsilon\ \text{ if }0 <x<\delta$$ $$\left|\frac{5x}{3x-9}\right|<\epsilon\ \text{ if }0 <x<\delta$$ I could split the absolute value into two separate ones and multiply the denominator up to the other side, but that wouldn't get me any closer to isolating x. Even plugging in 0.01 for $\epsilon$ doesn't seem to help. What am I doing wrong?",,"['calculus', 'limits']"
37,"Is the book wrong about this left-hand limit with absolute value? (But, my delta depends on x.)","Is the book wrong about this left-hand limit with absolute value? (But, my delta depends on x.)",,"The book says that $$\lim_{x \rightarrow 0^{-}} \left( \frac{1}{x} - \frac{1}{|x|} \right) \mbox{does not exist}$$ But, given any $M \lt 0$ of large magnitude, if I choose $\delta = \frac{-x^{2}M}{2}$ then any value of x where $|x-0|< \delta$ and $x <0$ (as we are coming from the left) will lead to $\left( \frac{1}{x} - \frac{1}{|x|} \right) < M$. To me, that says that my text book is incorrect in saying that this limit ""d.n.e."" I'm a little bothered that my $\delta$ depends on $x$, but I tried a few numerical examples and it worked fine. Perhaps the function is not uniformly continuous when $x \lt 0$? I have not done enough work to answer that question yet. Maybe the book meant to say $$\lim_{x \rightarrow 0} \left( \frac{1}{x} - \frac{1}{|x|} \right) \mbox{does not exist?}$$ Or maybe I have missed something elementary.","The book says that $$\lim_{x \rightarrow 0^{-}} \left( \frac{1}{x} - \frac{1}{|x|} \right) \mbox{does not exist}$$ But, given any $M \lt 0$ of large magnitude, if I choose $\delta = \frac{-x^{2}M}{2}$ then any value of x where $|x-0|< \delta$ and $x <0$ (as we are coming from the left) will lead to $\left( \frac{1}{x} - \frac{1}{|x|} \right) < M$. To me, that says that my text book is incorrect in saying that this limit ""d.n.e."" I'm a little bothered that my $\delta$ depends on $x$, but I tried a few numerical examples and it worked fine. Perhaps the function is not uniformly continuous when $x \lt 0$? I have not done enough work to answer that question yet. Maybe the book meant to say $$\lim_{x \rightarrow 0} \left( \frac{1}{x} - \frac{1}{|x|} \right) \mbox{does not exist?}$$ Or maybe I have missed something elementary.",,"['calculus', 'limits', 'absolute-value']"
38,Convergence of a Non-linear Recursive Sequence with Fractional Exponent,Convergence of a Non-linear Recursive Sequence with Fractional Exponent,,"I am exploring a recursively defined sequence involving non-integer exponents, and I aim to find or confirm the limit of $\left\{b_n\right\} \text { as } n \rightarrow \infty \text {. } $ The sequence is defined as follows: Let $x_1, x_2, \ldots, x_t$ be initial fixed positive real numbers, and define the sequence $\{b_n\}$ by: For $n \leq t$ , $b_n = x_n$ . For $n > t$ , $$   b_n = b_{n-1}^p + b_{n-2}^p + \cdots + b_{n-k}^p,   $$ where $p$ is a real number with $0 < p < 1$ . Progress and Approach: Given the concavity of the function $f(x) = x^p$ for $x > 0$ and $0 < p < 1$ , we observe that $f(x) \leq x$ for $x \geq 1$ . This suggests that each term in the sequence contributes progressively less as it grows larger, potentially indicating boundedness or stabilization of $\{b_n\}$ . Assuming the sequence might converge to a limit $L$ , we can expect $L$ to satisfy the steady-state condition: $$   L = k L^p.   $$ Solving this leads to: $$   L = k^{1/(1-p)}.   $$ This suggests a fixed point, which we hypothesize might act as an attractor for the sequence. To further validate this, I considered the function $f : [0, \infty)^k \to [0, \infty)$ defined by: $$   f(x_1, \ldots, x_k) = x_1^p + \ldots + x_k^p.   $$ Given that the derivative $\frac{d}{dx}(x^p) = p x^{p-1}$ is less than 1 for all $x > 1$ and $p < 1$ , I aimed to examine if $f$ acts as a contraction mapping under a suitable metric. Defining the metric as: $$   d((x_1, \ldots, x_j), (y_1, \ldots, y_j)) = \max |x_i - y_i|,   $$ one needs to show that: $$   |f(x_1, \ldots, x_j) - f(y_1, \ldots, y_j)| \leq C \max |x_i - y_i|,   $$ where $C < 1$ . This would imply that $f$ gradually brings terms closer together, suggesting convergence towards the fixed point $L$ . Initially, setting $M = \max(x_1, \ldots, x_t)$ , it leads to: $$   b_{t+1} \leq k M^p   $$ and following the recurrence: $$   b_{t+2} \leq k \cdot (k M^p)^p = k^{1+p} M^{p^2}   $$ indicating each term might be bounded by a decreasing sequence as $M^{p^n}$ approaches zero for large $n$ , given $p^n$ ’s exponential decay. Considering these, how can I rigorously prove the hypothesized convergence towards $L = k^{1/(1-p)}$ , or assess how the sequence will converge/bound instead? I would greatly appreciate any insights, corrections, or suggestions on methods to rigorously confirm whether $\{b_n\}$ converges and to what limit.","I am exploring a recursively defined sequence involving non-integer exponents, and I aim to find or confirm the limit of The sequence is defined as follows: Let be initial fixed positive real numbers, and define the sequence by: For , . For , where is a real number with . Progress and Approach: Given the concavity of the function for and , we observe that for . This suggests that each term in the sequence contributes progressively less as it grows larger, potentially indicating boundedness or stabilization of . Assuming the sequence might converge to a limit , we can expect to satisfy the steady-state condition: Solving this leads to: This suggests a fixed point, which we hypothesize might act as an attractor for the sequence. To further validate this, I considered the function defined by: Given that the derivative is less than 1 for all and , I aimed to examine if acts as a contraction mapping under a suitable metric. Defining the metric as: one needs to show that: where . This would imply that gradually brings terms closer together, suggesting convergence towards the fixed point . Initially, setting , it leads to: and following the recurrence: indicating each term might be bounded by a decreasing sequence as approaches zero for large , given ’s exponential decay. Considering these, how can I rigorously prove the hypothesized convergence towards , or assess how the sequence will converge/bound instead? I would greatly appreciate any insights, corrections, or suggestions on methods to rigorously confirm whether converges and to what limit.","\left\{b_n\right\} \text { as } n \rightarrow \infty \text {. }
 x_1, x_2, \ldots, x_t \{b_n\} n \leq t b_n = x_n n > t 
  b_n = b_{n-1}^p + b_{n-2}^p + \cdots + b_{n-k}^p,
   p 0 < p < 1 f(x) = x^p x > 0 0 < p < 1 f(x) \leq x x \geq 1 \{b_n\} L L 
  L = k L^p.
   
  L = k^{1/(1-p)}.
   f : [0, \infty)^k \to [0, \infty) 
  f(x_1, \ldots, x_k) = x_1^p + \ldots + x_k^p.
   \frac{d}{dx}(x^p) = p x^{p-1} x > 1 p < 1 f 
  d((x_1, \ldots, x_j), (y_1, \ldots, y_j)) = \max |x_i - y_i|,
   
  |f(x_1, \ldots, x_j) - f(y_1, \ldots, y_j)| \leq C \max |x_i - y_i|,
   C < 1 f L M = \max(x_1, \ldots, x_t) 
  b_{t+1} \leq k M^p
   
  b_{t+2} \leq k \cdot (k M^p)^p = k^{1+p} M^{p^2}
   M^{p^n} n p^n L = k^{1/(1-p)} \{b_n\}","['sequences-and-series', 'limits', 'convergence-divergence', 'upper-lower-bounds']"
39,Limit and System of Equation,Limit and System of Equation,,"if $\;f(x) = ax^2 + bx + c\;$ intercept at y-axis point $(0,1)$ and $\lim_{x\to 1} \frac{f(x)}{x-1} = -4$ , then the value of $\;\frac{b+c}{a}\;$ is ... So, I tried by plug in $x = 0\;$ and $\;y = 0$ . $$\;f(0) = a(0)^2 + b(0) + c\;$$ $$c = 1$$ Then, using derivative to evaluate the limit $$\lim_{x\to 1} \;\frac{f(x)}{x-1} = -4$$ $$\frac{2ax + b}{1} = -4$$ $$2a + b = -4$$ Is $\;2a + b = -4\;$ and $\;c = 1\;$ equation enough to form $\;\frac{b+c}{a}\;$ equation? or we have to get the $a$ and $b$ value too? But, in what way we could get the $a$ and $b$ value? Edit (after @Khosrotash'answer) So, because of $$\lim_{x\to 1} \;\frac{f(x)}{x-1} = -4$$ is actually $\;0/0$ $$\lim_{x\to 1} \;\frac{f(1)}{1-1} = \frac{0}{0}$$ $$f(1) = 0$$ $$a + b + c = 0$$ $$a + b + 1 = 0$$ $$a + b = -1$$ By eliminating the equation, I get $a = -3$ , $b = 2$ , and $c = 1$ . So, the value of $\;\frac{b+c}{a}\;$ is equal to $-1$ . But I'm still confused, how do you know that $\lim_{x→1}\frac{f(x)}{x−1}=−4$ is $0/0$ not $∞/∞$ ? is it because the $x−1$ value is $0$ , so it can be assume that $\lim_{x→1}\frac{f(x)}{x−1}=−4$ is $0/0$ ? –","if intercept at y-axis point and , then the value of is ... So, I tried by plug in and . Then, using derivative to evaluate the limit Is and equation enough to form equation? or we have to get the and value too? But, in what way we could get the and value? Edit (after @Khosrotash'answer) So, because of is actually By eliminating the equation, I get , , and . So, the value of is equal to . But I'm still confused, how do you know that is not ? is it because the value is , so it can be assume that is ? –","\;f(x) = ax^2 + bx + c\; (0,1) \lim_{x\to 1} \frac{f(x)}{x-1} = -4 \;\frac{b+c}{a}\; x = 0\; \;y = 0 \;f(0) = a(0)^2 + b(0) + c\; c = 1 \lim_{x\to 1} \;\frac{f(x)}{x-1} = -4 \frac{2ax + b}{1} = -4 2a + b = -4 \;2a + b = -4\; \;c = 1\; \;\frac{b+c}{a}\; a b a b \lim_{x\to 1} \;\frac{f(x)}{x-1} = -4 \;0/0 \lim_{x\to 1} \;\frac{f(1)}{1-1} = \frac{0}{0} f(1) = 0 a + b + c = 0 a + b + 1 = 0 a + b = -1 a = -3 b = 2 c = 1 \;\frac{b+c}{a}\; -1 \lim_{x→1}\frac{f(x)}{x−1}=−4 0/0 ∞/∞ x−1 0 \lim_{x→1}\frac{f(x)}{x−1}=−4 0/0","['limits', 'solution-verification', 'systems-of-equations']"
40,Can a ratio $\lim_{x \to c} f'(x)/f(x)$ of a derivative and a function be defined at $c$ when both are zero?,Can a ratio  of a derivative and a function be defined at  when both are zero?,\lim_{x \to c} f'(x)/f(x) c,"Given a differentiable function $f$ where $f(c) = 0$ for some $c$ , can $\lim_{x \to c} \frac{f'(x)}{f(x)}$ have a defined value? I guess $f'(c)$ should also be zero for this to possibly have a value, but I'm stuck here. (This is not a homework problem; one of my colleagues studying NN asked me)","Given a differentiable function where for some , can have a defined value? I guess should also be zero for this to possibly have a value, but I'm stuck here. (This is not a homework problem; one of my colleagues studying NN asked me)",f f(c) = 0 c \lim_{x \to c} \frac{f'(x)}{f(x)} f'(c),"['calculus', 'limits', 'analysis']"
41,"On the asymptotic behavior of a sequence[Bulgaria MO 2024 Regional Round, 12,2]","On the asymptotic behavior of a sequence[Bulgaria MO 2024 Regional Round, 12,2]",,"Let $N$ be a positive integer. The sequence $x_1, x_2, \ldots$ of non-negative reals is defined by $$x_n^2=\sum_{i=1}^{n-1} \sqrt{x_ix_{n-i}}$$ for all positive integers $n>N$ . We aim to prove that : $$x_{n}=\frac{\pi}{8}n+ o(n)$$ For this let $y_{n}:= \frac{x_{n}}{n}$ . We have that : $$y_{n}^{2}= \frac{1}{n} \sum_{i=1}^{n-1} \sqrt{y_{i}\cdot y_{n-i}} \sqrt{\frac{i}{n} \cdot \frac{n-i}{n}}~~(1)$$ We can notice from here that due to Rieman sum : $$\frac{1}{n} \sum_{i=1}^{n-1} \sqrt{\frac{i}{n} \cdot \frac{n-i}{n}} ~~\to_{n \to +\infty} \int_{0}^{1} \sqrt{x\cdot (1-x)} = \frac{\pi}{8}$$ Now we are left to prove that: $(y_{n})$ converges Taking the limit in $(1)$ lead to : $$\lim~~ y_{n} = \lim ~~\frac{1}{n} \sum_{i=1}^{n-1} \sqrt{\frac{i}{n} \cdot \frac{n-i}{n}}=\frac{\pi}{8} $$ For this let $u_{n}:= \frac{1}{n} \sum_{i=1}^{n-1} \sqrt{\frac{i}{n} \cdot \frac{n-i}{n}}$ and $l=\lim u_{n}$ . Consider $r_{n}=\frac{y_{n}}{l}$ we aim to prove that $\lim ~~r_{n}=1$ . For this we have from $(1)$ : $$u_{n}\cdot \min\{y_{k} : k< n\} \leq y_{n}^{2}  \leq u_{n} \cdot \max\{y_{k}: k < n\}$$ But here I can find ways to conclude. Any help or reference if such exercice is standard would be welcome. Thanks.",Let be a positive integer. The sequence of non-negative reals is defined by for all positive integers . We aim to prove that : For this let . We have that : We can notice from here that due to Rieman sum : Now we are left to prove that: converges Taking the limit in lead to : For this let and . Consider we aim to prove that . For this we have from : But here I can find ways to conclude. Any help or reference if such exercice is standard would be welcome. Thanks.,"N x_1, x_2, \ldots x_n^2=\sum_{i=1}^{n-1} \sqrt{x_ix_{n-i}} n>N x_{n}=\frac{\pi}{8}n+ o(n) y_{n}:= \frac{x_{n}}{n} y_{n}^{2}= \frac{1}{n} \sum_{i=1}^{n-1} \sqrt{y_{i}\cdot y_{n-i}} \sqrt{\frac{i}{n} \cdot \frac{n-i}{n}}~~(1) \frac{1}{n} \sum_{i=1}^{n-1} \sqrt{\frac{i}{n} \cdot \frac{n-i}{n}} ~~\to_{n \to +\infty} \int_{0}^{1} \sqrt{x\cdot (1-x)} = \frac{\pi}{8} (y_{n}) (1) \lim~~ y_{n} = \lim ~~\frac{1}{n} \sum_{i=1}^{n-1} \sqrt{\frac{i}{n} \cdot \frac{n-i}{n}}=\frac{\pi}{8}  u_{n}:= \frac{1}{n} \sum_{i=1}^{n-1} \sqrt{\frac{i}{n} \cdot \frac{n-i}{n}} l=\lim u_{n} r_{n}=\frac{y_{n}}{l} \lim ~~r_{n}=1 (1) u_{n}\cdot \min\{y_{k} : k< n\} \leq y_{n}^{2}  \leq u_{n} \cdot \max\{y_{k}: k < n\}","['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
42,Solving a cubic limit without using rationalization,Solving a cubic limit without using rationalization,,Starting from $$ \lim_{{x \to \infty}} \left( x^3 + 5x^2 + 8x \right)^{1/3} - x= $$ $$ =\lim_{{x \to \infty}} \left( x \left(1 + \frac{5}{x} + \frac{8}{x^2}\right)^{1/3} - x \right) $$ Is there a way to continue the limit without using rationalization?,Starting from Is there a way to continue the limit without using rationalization?,"
\lim_{{x \to \infty}} \left( x^3 + 5x^2 + 8x \right)^{1/3} - x=
 
=\lim_{{x \to \infty}} \left( x \left(1 + \frac{5}{x} + \frac{8}{x^2}\right)^{1/3} - x \right)
","['calculus', 'limits']"
43,Evaluate the integral $ \lim_{n \to ∞ } \int_{0}^{1} \frac{n \sin(e^x)}{1+n^2x^2}dx$,Evaluate the integral, \lim_{n \to ∞ } \int_{0}^{1} \frac{n \sin(e^x)}{1+n^2x^2}dx,Evaluate $$ \lim_{n \to ∞ } \int_{0}^{1} \frac{n \sin(e^x)}{1+n^2x^2}dx$$ I could just show that is bounded by $$ \lim_{n \to ∞ } \int_{0}^{1} \frac{-n}{1+n^2x^2}dx ≤ \lim_{n \to ∞ } \int_{0}^{1} \frac{n \sin(e^x)}{1+n^2x^2}dx ≤  \lim_{n \to ∞ } \int_{0}^{1} \frac{n}{1+n^2x^2}dx $$ I looked up if I can swipe the $\lim_{n \to ∞}$ inside the integral and make $∞/∞$ form to apply L'Hôpital's but I found that we can do so if the function inside integral converges uniformly (& uniform convergence is not a part of my syllabus) even I don't know if it is uniformly convergent & even after applying L'Hôpital's it is seems the same tough. I think there is some other approach to this. Kindly help. Thank you.,Evaluate I could just show that is bounded by I looked up if I can swipe the inside the integral and make form to apply L'Hôpital's but I found that we can do so if the function inside integral converges uniformly (& uniform convergence is not a part of my syllabus) even I don't know if it is uniformly convergent & even after applying L'Hôpital's it is seems the same tough. I think there is some other approach to this. Kindly help. Thank you., \lim_{n \to ∞ } \int_{0}^{1} \frac{n \sin(e^x)}{1+n^2x^2}dx  \lim_{n \to ∞ } \int_{0}^{1} \frac{-n}{1+n^2x^2}dx ≤ \lim_{n \to ∞ } \int_{0}^{1} \frac{n \sin(e^x)}{1+n^2x^2}dx ≤  \lim_{n \to ∞ } \int_{0}^{1} \frac{n}{1+n^2x^2}dx  \lim_{n \to ∞} ∞/∞,"['calculus', 'integration', 'limits', 'definite-integrals']"
44,limit of $\frac{x^2}{\ln{|x|}}$,limit of,\frac{x^2}{\ln{|x|}},"For unknown reasons I have more problems solving this limit than expected. I want to determine the limit: $\lim_{x \to 0} \sqrt[3]{\dfrac{x^2}{\ln(|x|)}}$ . Using graphic plotter, the limit exists and has to be 0. Well, being rough $\frac{0}{\infty} = 0$ . But that is not very mathematical. Also, L'Hospital does not work for this. I tried using $t(x) = \frac{1}{x}$ , which means $\lim_{t \to \infty} \sqrt[3]{\dfrac{1}{t^2\ln(|\frac{1}{t}|)}}$ , maybe in order to use L'Hospital here. But nope.... Can someone help me? Really appreciate your help.","For unknown reasons I have more problems solving this limit than expected. I want to determine the limit: . Using graphic plotter, the limit exists and has to be 0. Well, being rough . But that is not very mathematical. Also, L'Hospital does not work for this. I tried using , which means , maybe in order to use L'Hospital here. But nope.... Can someone help me? Really appreciate your help.",\lim_{x \to 0} \sqrt[3]{\dfrac{x^2}{\ln(|x|)}} \frac{0}{\infty} = 0 t(x) = \frac{1}{x} \lim_{t \to \infty} \sqrt[3]{\dfrac{1}{t^2\ln(|\frac{1}{t}|)}},['limits']
45,"Calculate $\lim_{t\to \infty} \mathbb E(1-e^{-e^{B_t}}),$ for Brownian motion.",Calculate  for Brownian motion.,"\lim_{t\to \infty} \mathbb E(1-e^{-e^{B_t}}),","Calculate the limit $$\lim_{t\to \infty} \mathbb E(1-e^{-e^{B_t}}),$$ where $B_t$ - standard Brownian motion. $\lim_{t\to \infty} \mathbb E(1-e^{-e^{B_t}})=1-\lim_{t\to \infty} \mathbb E(e^{-e^{B_t}})$ . I think that next we should use the property of Brownian motion. Perhaps it is useful to know that $B_t \sim \mathcal N(0,t)$ or that $B_t \sim \sqrt t W_0$ . However I have a problem to find this limit.",Calculate the limit where - standard Brownian motion. . I think that next we should use the property of Brownian motion. Perhaps it is useful to know that or that . However I have a problem to find this limit.,"\lim_{t\to \infty} \mathbb E(1-e^{-e^{B_t}}), B_t \lim_{t\to \infty} \mathbb E(1-e^{-e^{B_t}})=1-\lim_{t\to \infty} \mathbb E(e^{-e^{B_t}}) B_t \sim \mathcal N(0,t) B_t \sim \sqrt t W_0","['limits', 'stochastic-processes', 'brownian-motion']"
46,Computing a limit $ \lim_{n \to \infty} \frac{1}{n}\sum_{k=1}^{n} \cos(\sin(\frac{1}{k})) $,Computing a limit, \lim_{n \to \infty} \frac{1}{n}\sum_{k=1}^{n} \cos(\sin(\frac{1}{k})) ,Find $$ \lim_{n \to \infty} \frac{1}{n}\sum_{k=1}^{n} \cos\left(\sin\left(\frac{1}{k}\right)\right) $$ I found the limit by approximating it from its graph. I could not get a proper solution I drew the graph in desmos from which anyone can approximate the limit. I also tried taylor expansion of $\cos(x)$ to its first 3 terms as higer powers will be negligible which gives $$\lim_{n \to \infty} \left(1 - \frac{1}{n} \sum_{k=1}^{n} \frac{\sin ^2{\frac{1}{k}}}{2!} + \frac{\sin ^4{\frac{1}{k}}}{4!}\right)$$ In which I think the summation goes to zero since $\sin ^2{x}$ is a bounded function,Find I found the limit by approximating it from its graph. I could not get a proper solution I drew the graph in desmos from which anyone can approximate the limit. I also tried taylor expansion of to its first 3 terms as higer powers will be negligible which gives In which I think the summation goes to zero since is a bounded function, \lim_{n \to \infty} \frac{1}{n}\sum_{k=1}^{n} \cos\left(\sin\left(\frac{1}{k}\right)\right)  \cos(x) \lim_{n \to \infty} \left(1 - \frac{1}{n} \sum_{k=1}^{n} \frac{\sin ^2{\frac{1}{k}}}{2!} + \frac{\sin ^4{\frac{1}{k}}}{4!}\right) \sin ^2{x},"['calculus', 'limits']"
47,Calculate limit of $\frac{\sin(2x)}{x(\sin(x)-1)}$ as x goes to +infinity,Calculate limit of  as x goes to +infinity,\frac{\sin(2x)}{x(\sin(x)-1)},"I tried like this: $$\frac{-1}{x(\sin(x)-1)}\leq\frac{\sin(2x)}{x(\sin(x)-1)}\leq\frac{1}{x(\sin(x)-1)}$$ for each $x$ in domain. And now, Wolfram-Alpha says that $\lim_{x \to +\infty}\frac{-1}{x(\sin(x)-1)}=\lim_{x \to +\infty}\frac{1}{x(\sin(x)-1)}=0$ , therefore $\lim_{x \to +\infty}\frac{\sin(2x)}{x(\sin(x)-1)}=0$ , but I do not know how to show that the following is true: $$\lim_{x \to +\infty}\frac{-1}{x(\sin(x)-1)}=\lim_{x \to +\infty}\frac{1}{x(\sin(x)-1)}=0.$$ I would be grateful for any hints.","I tried like this: for each in domain. And now, Wolfram-Alpha says that , therefore , but I do not know how to show that the following is true: I would be grateful for any hints.",\frac{-1}{x(\sin(x)-1)}\leq\frac{\sin(2x)}{x(\sin(x)-1)}\leq\frac{1}{x(\sin(x)-1)} x \lim_{x \to +\infty}\frac{-1}{x(\sin(x)-1)}=\lim_{x \to +\infty}\frac{1}{x(\sin(x)-1)}=0 \lim_{x \to +\infty}\frac{\sin(2x)}{x(\sin(x)-1)}=0 \lim_{x \to +\infty}\frac{-1}{x(\sin(x)-1)}=\lim_{x \to +\infty}\frac{1}{x(\sin(x)-1)}=0.,"['calculus', 'limits']"
48,Is delta-epsilon proof necessary for multivariable limits?,Is delta-epsilon proof necessary for multivariable limits?,,"The specific problem is from James Stewart's 7th edition of Multivariable Calculus. The problem is $$\lim _{(x,y)\to (0,0)}\left(\frac{x^2y}{\:x^2+y^2}\right)$$ To evaluate this limit, do I need to use the delta epsilon proof? Is there no algebraic way to do this?","The specific problem is from James Stewart's 7th edition of Multivariable Calculus. The problem is To evaluate this limit, do I need to use the delta epsilon proof? Is there no algebraic way to do this?","\lim _{(x,y)\to (0,0)}\left(\frac{x^2y}{\:x^2+y^2}\right)","['limits', 'multivariable-calculus']"
49,Convergence of series with terms $\frac{\ln(n)}{n^2}$,Convergence of series with terms,\frac{\ln(n)}{n^2},"So, I'm currently a TA for a calculus class, and I came across the following problem: Use the limit comparison test to determine that $$\sum_{n=1}^\infty \frac{\ln(n)}{n^2}$$ converges (I'm currently grading this exact problem on an exam students have already taken). I didn't initially think this was a problem since I was thinking they could compare this series with a series whose terms are $b_n = n^{-3/2}$ , but the issue is that the version of the limit comparison test that students learned in this class required having $$0< \lim_{n \to \infty} \frac{a_n}{b_n} < \infty$$ so that $\sum a_n$ converges $\iff \sum b_n$ converges. I've been trying to find an appropriate choice of $b_n$ for awhile now given the strict lower bound on the limit, but I haven't had any success. Would anyone happen to have any suggestions? Is this even possible? I will say none of the students in the class of $130+$ have provided a convincing answer, and I can see the difficulty myself haha.","So, I'm currently a TA for a calculus class, and I came across the following problem: Use the limit comparison test to determine that converges (I'm currently grading this exact problem on an exam students have already taken). I didn't initially think this was a problem since I was thinking they could compare this series with a series whose terms are , but the issue is that the version of the limit comparison test that students learned in this class required having so that converges converges. I've been trying to find an appropriate choice of for awhile now given the strict lower bound on the limit, but I haven't had any success. Would anyone happen to have any suggestions? Is this even possible? I will say none of the students in the class of have provided a convincing answer, and I can see the difficulty myself haha.",\sum_{n=1}^\infty \frac{\ln(n)}{n^2} b_n = n^{-3/2} 0< \lim_{n \to \infty} \frac{a_n}{b_n} < \infty \sum a_n \iff \sum b_n b_n 130+,"['calculus', 'sequences-and-series', 'limits', 'convergence-divergence']"
50,Asymptotics of $\left(\sum_{k=0}^\infty\frac {k^n}{k!}\right)^\frac1n$ at $n\to\infty$,Asymptotics of  at,\left(\sum_{k=0}^\infty\frac {k^n}{k!}\right)^\frac1n n\to\infty,"Solving the problem on Quora , I tried also to get a reasonable approximation of $$ (S_n)^\frac1n=\left(\sum_{k=0}^\infty\frac {k^n}{k!}\right)^\frac1n\,\text{at}\,n\to\infty$$ Using Laplace method (which, frankly speaking, is not formally applicable in this case), I got $$(S_n)^\frac1n\sim \frac {n\,e^{\frac1{W(n)}}}{e\,W(n)}$$ where $W(n)$ is Lambert function. The approximation works rather well - in spite of arbitrary usage of the method: $\displaystyle n=100 \quad\frac{\ln n}n\left(S_n\right)^\frac1n\bigg|_{n=100}=0.667365...\,;\quad \frac {\ln n\,\,e^{\frac1{W(n)}}}{e\,W(n)}\bigg|_{n=100}=0.672337...$ $\displaystyle n=1000 \quad\frac{\ln n}n\left(S_n\right)^\frac1n\bigg|_{n=1000}=0.585123...\,;\quad \frac {\ln n\,\,e^{\frac1{W(n)}}}{e\,W(n)}\bigg|_{n=1000}=0.585692...$ etc. My question is whether we can prove the result in a rigorous way, or provide a more accurate and justified asymptotic, using some other evaluation technics? Follow up The asymptotic of Bell numbers solves the problem; @Command Master and @Cactus - thank you for the information. One of the ways of evaluation (by means of complex integration) can be found in this pape . It is interesting to note that the correct first term of the asymptotics can be obtained in one line by simply applying the Laplace method. I got $$\sum_{k=0}^\infty\frac {k^n}{k!}=\frac1{\sqrt{W(n)+1}}\left(\frac {n\,e^{\frac1{W(n)}}}{e\,W(n)}\right)^n$$ what coincides with the first term of the asymptotics in the paper. The formula in Wikipedia is not quite accurate (contains $\frac1{\sqrt{W(n)}}$ instead of $\frac1{\sqrt{W(n)+1}}$ ). The formula gives good approximation already for small $n\sim 10$ .","Solving the problem on Quora , I tried also to get a reasonable approximation of Using Laplace method (which, frankly speaking, is not formally applicable in this case), I got where is Lambert function. The approximation works rather well - in spite of arbitrary usage of the method: etc. My question is whether we can prove the result in a rigorous way, or provide a more accurate and justified asymptotic, using some other evaluation technics? Follow up The asymptotic of Bell numbers solves the problem; @Command Master and @Cactus - thank you for the information. One of the ways of evaluation (by means of complex integration) can be found in this pape . It is interesting to note that the correct first term of the asymptotics can be obtained in one line by simply applying the Laplace method. I got what coincides with the first term of the asymptotics in the paper. The formula in Wikipedia is not quite accurate (contains instead of ). The formula gives good approximation already for small ."," (S_n)^\frac1n=\left(\sum_{k=0}^\infty\frac {k^n}{k!}\right)^\frac1n\,\text{at}\,n\to\infty (S_n)^\frac1n\sim \frac {n\,e^{\frac1{W(n)}}}{e\,W(n)} W(n) \displaystyle n=100 \quad\frac{\ln n}n\left(S_n\right)^\frac1n\bigg|_{n=100}=0.667365...\,;\quad \frac {\ln n\,\,e^{\frac1{W(n)}}}{e\,W(n)}\bigg|_{n=100}=0.672337... \displaystyle n=1000 \quad\frac{\ln n}n\left(S_n\right)^\frac1n\bigg|_{n=1000}=0.585123...\,;\quad \frac {\ln n\,\,e^{\frac1{W(n)}}}{e\,W(n)}\bigg|_{n=1000}=0.585692... \sum_{k=0}^\infty\frac {k^n}{k!}=\frac1{\sqrt{W(n)+1}}\left(\frac {n\,e^{\frac1{W(n)}}}{e\,W(n)}\right)^n \frac1{\sqrt{W(n)}} \frac1{\sqrt{W(n)+1}} n\sim 10","['sequences-and-series', 'limits', 'asymptotics']"
51,Proving the limit $\lim_{x \to -2} \frac{\sqrt[3]{x}+\sqrt[3]{2}}{x+2}=\frac{\sqrt[3]{2}}{6}$ using the epsilon-delta definition.,Proving the limit  using the epsilon-delta definition.,\lim_{x \to -2} \frac{\sqrt[3]{x}+\sqrt[3]{2}}{x+2}=\frac{\sqrt[3]{2}}{6},Prove $\lim_{x \to -2} \frac{\sqrt[3]{x}+\sqrt[3]{2}}{x+2}=\frac{\sqrt[3]{2}}{6}$ . Solution Attempt: $\left| f(x) - L \right| < \epsilon$ $\left|f(x)-L \right|=\left| \frac{\sqrt[3]{x}+\sqrt[3]{2}}{x+2} - \frac{\sqrt[3]{2}}{6} \right|$ $=\left| \frac{6\left( \sqrt[3]{x}+\sqrt[3]{2}\right)-\sqrt[3]{2} (x+2)}{6(x+2)} \right|$ $=\left| \frac{6\sqrt[3]{x}+4\sqrt[3]{2}-\sqrt[3]{2}x}{6(x+2)}\right|$ $=\frac{1}{6}\left| \frac{6\sqrt[3]{x}+4\sqrt[3]{2}-\sqrt[3]{2}x}{x+2}\right|$ $\left| \frac{6\sqrt[3]{x}+4\sqrt[3]{2}-\sqrt[3]{2}x}{x+2}\right|<6\epsilon$ I am now stuck at this part of the solution. Can someone assist me?,Prove . Solution Attempt: I am now stuck at this part of the solution. Can someone assist me?,\lim_{x \to -2} \frac{\sqrt[3]{x}+\sqrt[3]{2}}{x+2}=\frac{\sqrt[3]{2}}{6} \left| f(x) - L \right| < \epsilon \left|f(x)-L \right|=\left| \frac{\sqrt[3]{x}+\sqrt[3]{2}}{x+2} - \frac{\sqrt[3]{2}}{6} \right| =\left| \frac{6\left( \sqrt[3]{x}+\sqrt[3]{2}\right)-\sqrt[3]{2} (x+2)}{6(x+2)} \right| =\left| \frac{6\sqrt[3]{x}+4\sqrt[3]{2}-\sqrt[3]{2}x}{6(x+2)}\right| =\frac{1}{6}\left| \frac{6\sqrt[3]{x}+4\sqrt[3]{2}-\sqrt[3]{2}x}{x+2}\right| \left| \frac{6\sqrt[3]{x}+4\sqrt[3]{2}-\sqrt[3]{2}x}{x+2}\right|<6\epsilon,"['limits', 'epsilon-delta']"
52,If $ \prod_{k=0}^n (x+\frac{1}{2^k}) = \sum_{k=0} ^n a_kx^k $find $\lim_{x\to \infty}\frac{a_{n-2}}{a_{n-4}} = ?$,If find, \prod_{k=0}^n (x+\frac{1}{2^k}) = \sum_{k=0} ^n a_kx^k  \lim_{x\to \infty}\frac{a_{n-2}}{a_{n-4}} = ?,If $$ \prod_{k=1}^n (x+\frac{1}{2^k}) = \sum_{k=0} ^n a_kx^k  $$ then find $$\lim_{n\to \infty}\frac{a_{n-2}}{a_{n-4}} = ?$$ I’m able to find $a_{n-2}$ easily but finding $a_{n-4}$ is quite difficult for me. I think involving binomial theorem or principle of inclusion and exclusion may work but still I’m not able to proceed with that.,If then find I’m able to find easily but finding is quite difficult for me. I think involving binomial theorem or principle of inclusion and exclusion may work but still I’m not able to proceed with that.," \prod_{k=1}^n (x+\frac{1}{2^k}) = \sum_{k=0} ^n a_kx^k 
 \lim_{n\to \infty}\frac{a_{n-2}}{a_{n-4}} = ? a_{n-2} a_{n-4}","['sequences-and-series', 'limits', 'binomial-coefficients']"
53,Limit question regarding logarthmic function.,Limit question regarding logarthmic function.,,$\displaystyle{\lim_{x \to 0}}f(x)$ where $f(x)$ = $\frac{\ln(1+x^2+x)+\ln(1+x^2-x)}{secx-cosx}$ What I found was the two ways Way 1 $L$ = $\frac{\frac{\ln(1+x^2+x)(x+x^2)}{(x+x^2)}+\frac{\ln(1+x^2-x)(x^2-x)}{(x^2-x)}}{secx-cosx}$ $L$ = $\frac{2x^2}{secx-cosx}$$=$$2$ Way2- $\frac{\ln(1+x^2+x)+\ln(1+x^2-x)}{secx-cosx}$ $L$ = $\frac{\ln((1+x^2+x^4)}{secx-cosx}$ $L$ = $\frac{(x^2+x^4)}{secx-cosx}$ =1 Which method is correct and why?,where = What I found was the two ways Way 1 = = Way2- = = =1 Which method is correct and why?,\displaystyle{\lim_{x \to 0}}f(x) f(x) \frac{\ln(1+x^2+x)+\ln(1+x^2-x)}{secx-cosx} L \frac{\frac{\ln(1+x^2+x)(x+x^2)}{(x+x^2)}+\frac{\ln(1+x^2-x)(x^2-x)}{(x^2-x)}}{secx-cosx} L \frac{2x^2}{secx-cosx}=2 \frac{\ln(1+x^2+x)+\ln(1+x^2-x)}{secx-cosx} L \frac{\ln((1+x^2+x^4)}{secx-cosx} L \frac{(x^2+x^4)}{secx-cosx},['limits']
54,Calculating $ \lim_{n \to \infty} \sum_{i=0}^n \frac{i\pi}{n^2}\sin{(\frac{i^2\pi^2}{4n^2})} $,Calculating, \lim_{n \to \infty} \sum_{i=0}^n \frac{i\pi}{n^2}\sin{(\frac{i^2\pi^2}{4n^2})} ,"The problem: $$ \lim_{n \to \infty} \sum_{i=0}^n \frac{i\pi}{n^2}\sin{(\frac{i^2\pi^2}{4n^2})} $$ My attempt: Removed constants $$ \pi\lim_{n \to \infty} \frac{1}{n^2}\sum_{i=0}^n i\sin{(\frac{i^2\pi^2}{4n^2})} $$ Considered l'Hospital's Rule since $n^2 \to \infty$ and $\sum_{i=0}^n i\sin{(\frac{i^2\pi^2}{4n^2})}$ does not converge by the limit convergence test. L'Hospital's Rule gives $$ \pi\lim_{n \to \infty} \frac{1}{2n}\sum_{i=0}^n i\sin{(\frac{i^2\pi^2}{4n^2})}(-2)\frac{i^2\pi^2}{4n^3} $$ $$ =\frac{-\pi^3}{4}\lim_{n \to \infty} \frac{1}{n^4} \sum_{i=0}^n i^3\cos{\frac{i^2\pi^2}{4n^2}} $$ Which once again has an indeterminant form. I don't see myself getting anywhere with l'Hospital's Rule again, and I'm not really sure what I can do. Any tips would help tons! Thanks.","The problem: My attempt: Removed constants Considered l'Hospital's Rule since and does not converge by the limit convergence test. L'Hospital's Rule gives Which once again has an indeterminant form. I don't see myself getting anywhere with l'Hospital's Rule again, and I'm not really sure what I can do. Any tips would help tons! Thanks.","
\lim_{n \to \infty} \sum_{i=0}^n \frac{i\pi}{n^2}\sin{(\frac{i^2\pi^2}{4n^2})}
 
\pi\lim_{n \to \infty} \frac{1}{n^2}\sum_{i=0}^n i\sin{(\frac{i^2\pi^2}{4n^2})}
 n^2 \to \infty \sum_{i=0}^n i\sin{(\frac{i^2\pi^2}{4n^2})} 
\pi\lim_{n \to \infty} \frac{1}{2n}\sum_{i=0}^n i\sin{(\frac{i^2\pi^2}{4n^2})}(-2)\frac{i^2\pi^2}{4n^3}
 
=\frac{-\pi^3}{4}\lim_{n \to \infty} \frac{1}{n^4} \sum_{i=0}^n i^3\cos{\frac{i^2\pi^2}{4n^2}}
","['calculus', 'limits', 'summation']"
55,Epsilon-Delta proof for Multivariate Limit in Polar Coordinates,Epsilon-Delta proof for Multivariate Limit in Polar Coordinates,,"I'm trying to construct an $\varepsilon-\delta$ proof for the following limit, $$\lim_{(x, y) \rightarrow (0, 0)}\frac{4xy}{x^2 + y^2 + 2} = 0,$$ and I would like to see if my proof is sound having converted cartesian coordinates to polar coordinates. My proof is as follows: Let $x = r\cos\theta$ , $y = r\sin\theta$ with $r = \sqrt{x^2 + y^2}$ , then $$\lim_{(x, y) \rightarrow (0, 0)}\frac{4xy}{x^2 + y^2 + 2} = \lim_{r \rightarrow 0}\frac{4r^2\cos\theta\sin\theta}{r^2+2}$$ . We note that $\forall\theta\in\mathbb{R}, 0 \leq|\cos\theta\sin\theta| < 1$ . Now, suppose that $0 < |r| < \delta$ , for $\delta > 0$ . We take $\delta < \frac{\varepsilon}{4}$ , with $\varepsilon > 0$ , then $0 < |r| < \delta < \frac{\varepsilon}{4}$ . $$ \begin{align*}   \left|\frac{4r^2\cos\theta\sin\theta}{r^2+2} - 0\right| &< \left|\frac{4r^2\cos\theta\sin\theta}{r^2}\right|\\ &= 4|\cos\theta\sin\theta| \\ &<4|r||\cos\theta\sin\theta|\\ &<4|r| \\ &<4 \cdot \frac{\varepsilon}{4} \\ &= \varepsilon \end{align*}$$ Thus for $0<|r|<\delta$ , we have $\left|\frac{4r^2\cos\theta\sin\theta}{r^2+2}\right| < \varepsilon$ . Hence, we conclude that $$\lim_{(x, y) \rightarrow (0, 0)}\frac{4xy}{x^2 + y^2 + 2} = \lim_{r \rightarrow 0}\frac{4r^2\cos\theta\sin\theta}{r^2+2}$$ exists and is equal to $0$ . Anyways, any feedback of how I could improve the proof (if it is correct) or correct anything would be greatly appreciated.","I'm trying to construct an proof for the following limit, and I would like to see if my proof is sound having converted cartesian coordinates to polar coordinates. My proof is as follows: Let , with , then . We note that . Now, suppose that , for . We take , with , then . Thus for , we have . Hence, we conclude that exists and is equal to . Anyways, any feedback of how I could improve the proof (if it is correct) or correct anything would be greatly appreciated.","\varepsilon-\delta \lim_{(x, y) \rightarrow (0, 0)}\frac{4xy}{x^2 + y^2 + 2} = 0, x = r\cos\theta y = r\sin\theta r = \sqrt{x^2 + y^2} \lim_{(x, y) \rightarrow (0, 0)}\frac{4xy}{x^2 + y^2 + 2} = \lim_{r \rightarrow 0}\frac{4r^2\cos\theta\sin\theta}{r^2+2} \forall\theta\in\mathbb{R}, 0 \leq|\cos\theta\sin\theta| < 1 0 < |r| < \delta \delta > 0 \delta < \frac{\varepsilon}{4} \varepsilon > 0 0 < |r| < \delta < \frac{\varepsilon}{4} 
\begin{align*}
  \left|\frac{4r^2\cos\theta\sin\theta}{r^2+2} - 0\right| &< \left|\frac{4r^2\cos\theta\sin\theta}{r^2}\right|\\ &= 4|\cos\theta\sin\theta| \\
&<4|r||\cos\theta\sin\theta|\\
&<4|r| \\
&<4 \cdot \frac{\varepsilon}{4} \\
&= \varepsilon
\end{align*} 0<|r|<\delta \left|\frac{4r^2\cos\theta\sin\theta}{r^2+2}\right| < \varepsilon \lim_{(x, y) \rightarrow (0, 0)}\frac{4xy}{x^2 + y^2 + 2} = \lim_{r \rightarrow 0}\frac{4r^2\cos\theta\sin\theta}{r^2+2} 0","['real-analysis', 'limits', 'multivariable-calculus', 'polar-coordinates']"
56,Evaluation of a limit involving gamma function,Evaluation of a limit involving gamma function,,"Basically I try to answer this question . I am almost able to prove it straight on, but I hit a roadblock at the very last step. Namely, the limit $$ \lim_{n\rightarrow \infty} \left( \sqrt[n+1]{\Gamma \left( n+1+\frac{1}{2} \right)}-\sqrt[n]{\Gamma \left( n+\frac{1}{2} \right)} \right)  $$ It should evaluate to $1/e$ through numerical estimation. One possible direction here is to justify that we just plug in Stirling's formula, due to $$ \sqrt[n]{\Gamma \left( n+\frac{1}{2} \right)}\sim \frac{n}{e} $$ we have $$ \lim_{n\rightarrow \infty} \left( \sqrt[n+1]{\Gamma \left( n+1+\frac{1}{2} \right)}-\sqrt[n]{\Gamma \left( n+\frac{1}{2} \right)} \right) =\lim_{n\rightarrow \infty} \left( \frac{n+1}{e}-\frac{n}{e} \right) =\frac{1}{e} $$","Basically I try to answer this question . I am almost able to prove it straight on, but I hit a roadblock at the very last step. Namely, the limit It should evaluate to through numerical estimation. One possible direction here is to justify that we just plug in Stirling's formula, due to we have","
\lim_{n\rightarrow \infty} \left( \sqrt[n+1]{\Gamma \left( n+1+\frac{1}{2} \right)}-\sqrt[n]{\Gamma \left( n+\frac{1}{2} \right)} \right) 
 1/e 
\sqrt[n]{\Gamma \left( n+\frac{1}{2} \right)}\sim \frac{n}{e}
 
\lim_{n\rightarrow \infty} \left( \sqrt[n+1]{\Gamma \left( n+1+\frac{1}{2} \right)}-\sqrt[n]{\Gamma \left( n+\frac{1}{2} \right)} \right) =\lim_{n\rightarrow \infty} \left( \frac{n+1}{e}-\frac{n}{e} \right) =\frac{1}{e}
","['calculus', 'limits', 'analysis', 'gamma-function']"
57,$\lim_{x \to 0} (\cos x)^{\cot x}$,,\lim_{x \to 0} (\cos x)^{\cot x},The following question is from cengage calculus . Illustration 2.95 but the explanation isn't clear to me $\lim_{x \to 0} (\cos x)^{\cot x}$ It is to be solved by using the identity : $\lim_{x \to 0} (1+x)^{\frac{1}{x}} = e$,The following question is from cengage calculus . Illustration 2.95 but the explanation isn't clear to me It is to be solved by using the identity :,\lim_{x \to 0} (\cos x)^{\cot x} \lim_{x \to 0} (1+x)^{\frac{1}{x}} = e,"['limits', 'limits-without-lhopital']"
58,How to evaluate the limit $\lim\limits_{{q}\to {1}^{-}} \prod\limits_{k=0}^{+\infty} \dfrac{1-{q}^{a+k}x}{1-{q}^{k}x} \quad (a \in \mathbb{C})$,How to evaluate the limit,\lim\limits_{{q}\to {1}^{-}} \prod\limits_{k=0}^{+\infty} \dfrac{1-{q}^{a+k}x}{1-{q}^{k}x} \quad (a \in \mathbb{C}),"I want to evaluate this limit expression. I have this problem when I explored a corollary of infinity q-binomial theorem. $ \lim\limits_{{q}\to {1}^{-}} \prod\limits_{k=0}^{+\infty} \dfrac{1-{q}^{a+k}x}{1-{q}^{k}x} \\ x \in {\mathbb{R}} \quad , \quad \left|x\right|<1 \quad , \quad a \in \mathbb{C} $ When $a\in{\mathbb{R}^{+}}$ , it is clear that $ \lim\limits_{{q}\to {1}^{-}} \prod\limits_{k=0}^{+\infty} \dfrac{1-{q}^{a+k}x}{1-{q}^{k}x}= \lim\limits_{{q}\to {1}^{-}} \prod\limits_{k=0}^{a-1} \dfrac{1}{1-{q}^{k}x}= \dfrac{1}{{(1-x)}^{a}} $ Now I need to extend this conclusion to the broader situation that $a \in \mathbb{C}$ . Here the trouble is that I don't know how to do with it. So I ask for help here.","I want to evaluate this limit expression. I have this problem when I explored a corollary of infinity q-binomial theorem. When , it is clear that Now I need to extend this conclusion to the broader situation that . Here the trouble is that I don't know how to do with it. So I ask for help here.","
\lim\limits_{{q}\to {1}^{-}} \prod\limits_{k=0}^{+\infty} \dfrac{1-{q}^{a+k}x}{1-{q}^{k}x} \\
x \in {\mathbb{R}} \quad , \quad \left|x\right|<1 \quad , \quad a \in \mathbb{C}
 a\in{\mathbb{R}^{+}} 
\lim\limits_{{q}\to {1}^{-}} \prod\limits_{k=0}^{+\infty} \dfrac{1-{q}^{a+k}x}{1-{q}^{k}x}=
\lim\limits_{{q}\to {1}^{-}} \prod\limits_{k=0}^{a-1} \dfrac{1}{1-{q}^{k}x}=
\dfrac{1}{{(1-x)}^{a}}
 a \in \mathbb{C}","['limits', 'analysis', 'complex-numbers', 'infinite-product']"
59,Understanding the precise definition of limit,Understanding the precise definition of limit,,"I'm attempting to understand the exact definition of a limit, but I'm struggling to do so. For instance, why is it incorrect to say that the limit of x as it approaches 2 is equal to 10? $\lim_{x\rightarrow 2}x=10$ $$\forall\epsilon>0,\exists\delta>0,0<|x-c|<\delta\longrightarrow|f(x)-L|<\epsilon$$ I choose $\delta=100$ . So why the following statement is not correct? $$0<|x-2|<100\longrightarrow|x-10|<\epsilon,\quad\forall \epsilon>0$$","I'm attempting to understand the exact definition of a limit, but I'm struggling to do so. For instance, why is it incorrect to say that the limit of x as it approaches 2 is equal to 10? I choose . So why the following statement is not correct?","\lim_{x\rightarrow 2}x=10 \forall\epsilon>0,\exists\delta>0,0<|x-c|<\delta\longrightarrow|f(x)-L|<\epsilon \delta=100 0<|x-2|<100\longrightarrow|x-10|<\epsilon,\quad\forall \epsilon>0","['calculus', 'limits', 'functions', 'solution-verification']"
60,A limit problem That may be Solvable by using Probability Theory,A limit problem That may be Solvable by using Probability Theory,,"$$\lim_{n \to \infty}  \frac{\sum_{ k= 0}^{n-1} \prod_{i=0}^k (1-\frac{i}{n})  }{\sqrt{n}}$$ This problem arises from a probability question, which was inappropriately approximated into this form. After conducting computer simulations, the approximate result for this problem is around 1.25289. My classmate suggested that this can be viewed as randomly selecting one box out of n boxes without replacement until an empty box is chosen. They claimed that the expected number of balls can be transformed into an integral, possibly something like $$\int_{0}^{\infty} e^{-\frac{t^2}{2}} dt $$ , which resembles $\sqrt{\frac{\pi}{2}}$ . The computer simulations yielded a similar result. I would like to know if this is correct or if there is a more rigorous approach.","This problem arises from a probability question, which was inappropriately approximated into this form. After conducting computer simulations, the approximate result for this problem is around 1.25289. My classmate suggested that this can be viewed as randomly selecting one box out of n boxes without replacement until an empty box is chosen. They claimed that the expected number of balls can be transformed into an integral, possibly something like , which resembles . The computer simulations yielded a similar result. I would like to know if this is correct or if there is a more rigorous approach.",\lim_{n \to \infty}  \frac{\sum_{ k= 0}^{n-1} \prod_{i=0}^k (1-\frac{i}{n})  }{\sqrt{n}} \int_{0}^{\infty} e^{-\frac{t^2}{2}} dt  \sqrt{\frac{\pi}{2}},"['probability', 'limits', 'analysis']"
61,"If $f(x+y)=f(x)+f(y)$ and $\lim_{x \rightarrow 0} f(x)$ exists, prove $\lim_{x \rightarrow 0} f(x) = 0$ and $\lim_{x \rightarrow p} f(x) \ \forall p$","If  and  exists, prove  and",f(x+y)=f(x)+f(y) \lim_{x \rightarrow 0} f(x) \lim_{x \rightarrow 0} f(x) = 0 \lim_{x \rightarrow p} f(x) \ \forall p,"I'm trying to solve the problem 19 in Manfred Stoll's ""Introduction to Real Analysis"", Exercises 4.1 Let $f : \mathbb{R} \rightarrow \mathbb{R}$ satisfy $f(x+y)=f(x)+f(y) \ \forall x, y \in \mathbb{R}$ . If $\lim\limits_{x \rightarrow 0} f(x)$ exists, prove (a) $\lim\limits_{x \rightarrow 0} f(x) = 0$ and (b) $\lim\limits_{x \rightarrow p} f(x)$ exists $\forall p \in \mathbb{R}$ . I solved (a) by $\begin{align} f(2x) & = f(x)+f(x) \\ \lim_{x \rightarrow 0} f(2x) & = \lim_{x \rightarrow 0} f(x) + \lim_{x \rightarrow 0} f(x) \\ \lim_{x \rightarrow 0} f(x) = \lim_{2x \rightarrow 0} f(2x) & = \lim_{x \rightarrow 0} f(x) + \lim_{x \rightarrow 0} f(x) \\ \therefore \lim_{x \rightarrow 0} f(x) & = 0, \end{align}$ but I have no idea to solve (b). Is there any hint for (b)?","I'm trying to solve the problem 19 in Manfred Stoll's ""Introduction to Real Analysis"", Exercises 4.1 Let satisfy . If exists, prove (a) and (b) exists . I solved (a) by but I have no idea to solve (b). Is there any hint for (b)?","f : \mathbb{R} \rightarrow \mathbb{R} f(x+y)=f(x)+f(y) \ \forall x, y \in \mathbb{R} \lim\limits_{x \rightarrow 0} f(x) \lim\limits_{x \rightarrow 0} f(x) = 0 \lim\limits_{x \rightarrow p} f(x) \forall p \in \mathbb{R} \begin{align}
f(2x) & = f(x)+f(x) \\
\lim_{x \rightarrow 0} f(2x) & = \lim_{x \rightarrow 0} f(x) + \lim_{x \rightarrow 0} f(x) \\
\lim_{x \rightarrow 0} f(x) = \lim_{2x \rightarrow 0} f(2x) & = \lim_{x \rightarrow 0} f(x) + \lim_{x \rightarrow 0} f(x) \\
\therefore \lim_{x \rightarrow 0} f(x) & = 0,
\end{align}","['real-analysis', 'limits']"
62,Is limit about functions?,Is limit about functions?,,"When talking about limit subject, all the time we point functions. But in derivative definition we use $\frac{\Delta Y}{\Delta X}$ . Is $\frac{\Delta Y}{\Delta X}$ a kind of function? I have this question about integral formula as well.","When talking about limit subject, all the time we point functions. But in derivative definition we use . Is a kind of function? I have this question about integral formula as well.",\frac{\Delta Y}{\Delta X} \frac{\Delta Y}{\Delta X},"['calculus', 'limits']"
63,What is the sum of the series $\sum_{r = 1}^{n-1}{1 \over r \sqrt[a]{\log \big(\frac{\log n}{\log r}\big)}}$?,What is the sum of the series ?,\sum_{r = 1}^{n-1}{1 \over r \sqrt[a]{\log \big(\frac{\log n}{\log r}\big)}},"We have Euler's well known result $$ \sum_{r = 1}^n\frac{1}{r} = \log n + \gamma + O\Big(\frac{1}{n}\Big) $$ where $\gamma$ is the Euler-mascheroni constant. I experimentally observed a generalisation of the above. We have for $a > 1$ , $$ \sum_{r = 1}^{n-1}{1 \over r \sqrt[a]{\log \big(\frac{\log n}{\log r}\big)}} =   \Gamma\Big(1 - \frac{1}{a}\Big)\log n + C_a + O\Big(\frac{1}{n}\Big) $$ where $C_a$ is a constant that depends only on $a$ . If this is true than Euler's result will correspond to the special case $a \to \infty$ . Question : Can this be proved or disproved?","We have Euler's well known result where is the Euler-mascheroni constant. I experimentally observed a generalisation of the above. We have for , where is a constant that depends only on . If this is true than Euler's result will correspond to the special case . Question : Can this be proved or disproved?","
\sum_{r = 1}^n\frac{1}{r} = \log n + \gamma + O\Big(\frac{1}{n}\Big)
 \gamma a > 1 
\sum_{r = 1}^{n-1}{1 \over r \sqrt[a]{\log \big(\frac{\log n}{\log r}\big)}} =   \Gamma\Big(1 - \frac{1}{a}\Big)\log n + C_a + O\Big(\frac{1}{n}\Big)
 C_a a a \to \infty","['sequences-and-series', 'limits', 'number-theory', 'elementary-number-theory', 'gamma-function']"
64,Evaluation of a Trigonometric Limit...,Evaluation of a Trigonometric Limit...,,$$\lim_{x\to\tfrac{\pi}{4}} \frac{(\cos x + \sin x)^3 - 2\sqrt2}{1 - \sin 2x}$$ I tried solving this question without using L'Hospital's Rule but couldn't find a satisfactory way of approach to solve this particular question. I would be highly obliged if I could get a solution from anyone of you regarding how we can evaluate this limit without using L'Hospital's Rule. Thank you in advance!,I tried solving this question without using L'Hospital's Rule but couldn't find a satisfactory way of approach to solve this particular question. I would be highly obliged if I could get a solution from anyone of you regarding how we can evaluate this limit without using L'Hospital's Rule. Thank you in advance!,\lim_{x\to\tfrac{\pi}{4}} \frac{(\cos x + \sin x)^3 - 2\sqrt2}{1 - \sin 2x},"['calculus', 'limits', 'limits-without-lhopital']"
65,On the limit defined by $A + B(A + B(A + B (A + B(\cdots))))$,On the limit defined by,A + B(A + B(A + B (A + B(\cdots)))),"Suppose $A$ and $B$ are some constant ( $A,B\in\mathbb{R}$ ) Is there a simple expression for $x$ , where $x$ is: $$ x=A+B[A+B[A+B[\cdots]]]] $$ ""..."" indicates the pattern repeats forever. In other words, it is recursive: $$ x=A+Bx $$ It is very tempting to just do: $$ x=\frac{A}{1-B} $$ However, is this correct? I don't think is that simple. For context, I come from an engineering background and my math isn't as rigorous. Please enlighten me. Does this wiki resembles this kind of series? Edit 1: I don't think the expression for $x$ is simple, because: $$ x\neq \frac{A}{1-B} $$","Suppose and are some constant ( ) Is there a simple expression for , where is: ""..."" indicates the pattern repeats forever. In other words, it is recursive: It is very tempting to just do: However, is this correct? I don't think is that simple. For context, I come from an engineering background and my math isn't as rigorous. Please enlighten me. Does this wiki resembles this kind of series? Edit 1: I don't think the expression for is simple, because:","A B A,B\in\mathbb{R} x x 
x=A+B[A+B[A+B[\cdots]]]]
 
x=A+Bx
 
x=\frac{A}{1-B}
 x 
x\neq \frac{A}{1-B}
","['sequences-and-series', 'limits', 'recurrence-relations', 'telescopic-series']"
66,Finding $\lim_{x \to \infty} (1 - \frac{e}{x})^{x^2}$. [duplicate],Finding . [duplicate],\lim_{x \to \infty} (1 - \frac{e}{x})^{x^2},"This question already has answers here : Limit of exponential and logarithm function (4 answers) Closed last year . I tried using using the logarithm, \begin{align} L &=\lim_{x \to \infty} (1 - \frac{e}{x})^{x^2},\\ \ln L &= \ln \lim_{x \to \infty} (1 - \frac{e}{x})^{x^2}\\ &= \lim_{x \to \infty} \ln (1 - \frac{e}{x})^{x^2} \\ &= \lim_{x \to \infty} x^2 \ln (1 - \frac{e}{x})\\ &= \lim_{x \to \infty} x^2 \cdot \lim_{x \to \infty} \ln (1 - \frac{e}{x})\\ &= \lim_{x \to \infty} x^2 \cdot \lim_{x \to \infty} \ln (1) \\ &= \lim_{x \to \infty} x^2 \cdot \lim_{x \to \infty} \ln (1) \\ &= \lim_{x \to \infty} x^2 \cdot 0 .\\ &= \infty \cdot 0 \end{align} but get an indeterminate expression at the end. Then, after some research, I found out about Euler's number as a limit , that is, $$ e = \lim_{n \to 0} (1 + n)^{\frac{1}{n}} = \lim_{n \to \infty} (1 + \frac{1}{n})^n $$ and tried using both identities to see if one would work. However, after a substantial amount of working, I realised as the given limit had a power of $x$ , there would always be some ""residue"" left: Using the left-most identity: $$ \text{Let $n = -\frac{e}{x}$, hence $x = -\frac{e}{n}$ and $x^2 = \frac{e^2}{n^2}$. So, } \\ \lim_{x \to \infty} (1 - \frac{e}{x})^{x^2} = \lim_{x \to \infty} (1 + n)^{\frac{e^2}.{n^2}}\\ \text{Since $n = -\frac{e}{x}$, as $x \to \infty$, $n \to 0$. So, }\\ \lim_{x \to \infty} (1 + n)^{\frac{e^2}{n^2}} = \lim_{n \to 0} (1 + n)^{\frac{e^2}{n^2}}.\\ \lim_{n \to 0} (1 + n)^{\frac{e^2}{n^2}} = \lim_{n \to 0} (1 + n)^{\frac{1}{n}\frac{e^2}{n}}\\ = e^{\frac{e^2}{n}} $$ If you notice, there's a ""residue"" variable $n$ left in the denominator of the exponent. Using the other identity yields the same result: there's a residue variable left in the result. I have exhausted every technique I know and tried researching for similar questions, but none that use Euler's identities have the variable in the exponent raised to any power. They all have a multiple of $x$ , e.g. $10x$ , or some linear expression $x + c$ , e.g., $7x + 8$ , allowing the end result to be some constant. Any sort of help or pointing to some direction would be greatly appreciated.","This question already has answers here : Limit of exponential and logarithm function (4 answers) Closed last year . I tried using using the logarithm, but get an indeterminate expression at the end. Then, after some research, I found out about Euler's number as a limit , that is, and tried using both identities to see if one would work. However, after a substantial amount of working, I realised as the given limit had a power of , there would always be some ""residue"" left: Using the left-most identity: If you notice, there's a ""residue"" variable left in the denominator of the exponent. Using the other identity yields the same result: there's a residue variable left in the result. I have exhausted every technique I know and tried researching for similar questions, but none that use Euler's identities have the variable in the exponent raised to any power. They all have a multiple of , e.g. , or some linear expression , e.g., , allowing the end result to be some constant. Any sort of help or pointing to some direction would be greatly appreciated.","\begin{align}
L &=\lim_{x \to \infty} (1 - \frac{e}{x})^{x^2},\\
\ln L
&= \ln \lim_{x \to \infty} (1 - \frac{e}{x})^{x^2}\\
&= \lim_{x \to \infty} \ln (1 - \frac{e}{x})^{x^2} \\
&= \lim_{x \to \infty} x^2 \ln (1 - \frac{e}{x})\\
&= \lim_{x \to \infty} x^2 \cdot \lim_{x \to \infty} \ln (1 - \frac{e}{x})\\
&= \lim_{x \to \infty} x^2 \cdot \lim_{x \to \infty} \ln (1) \\
&= \lim_{x \to \infty} x^2 \cdot \lim_{x \to \infty} \ln (1) \\
&= \lim_{x \to \infty} x^2 \cdot 0 .\\
&= \infty \cdot 0
\end{align} 
e = \lim_{n \to 0} (1 + n)^{\frac{1}{n}} = \lim_{n \to \infty} (1 + \frac{1}{n})^n
 x 
\text{Let n = -\frac{e}{x}, hence x = -\frac{e}{n} and x^2 = \frac{e^2}{n^2}. So, } \\
\lim_{x \to \infty} (1 - \frac{e}{x})^{x^2} = \lim_{x \to \infty} (1 + n)^{\frac{e^2}.{n^2}}\\
\text{Since n = -\frac{e}{x}, as x \to \infty, n \to 0. So, }\\
\lim_{x \to \infty} (1 + n)^{\frac{e^2}{n^2}} = \lim_{n \to 0} (1 + n)^{\frac{e^2}{n^2}}.\\
\lim_{n \to 0} (1 + n)^{\frac{e^2}{n^2}} = \lim_{n \to 0} (1 + n)^{\frac{1}{n}\frac{e^2}{n}}\\
= e^{\frac{e^2}{n}}
 n x 10x x + c 7x + 8","['calculus', 'limits']"
67,$\lim_{\kappa \to \infty} \frac{R(\kappa)}{\kappa^2}=\frac{12}{\pi^2}$,,\lim_{\kappa \to \infty} \frac{R(\kappa)}{\kappa^2}=\frac{12}{\pi^2},"[ Rational Points on Elliptic Curves - Joseph H. Silverman, ex 3.1] Given a rational number $x=\frac mn$ in its lowest terms, define $$H(x):=\max\left\{|m|,|n|\right\}.$$ Let $R(\kappa)$ be the number of rational numbers $x$ with $H(x)$ less than $\kappa$ . First prove that $R(\kappa)\le 2\kappa^2+\kappa$ and then show that $$\lim_{\kappa \to \infty} \frac{R(\kappa)}{\kappa^2}=\frac{12}{\pi^2}$$ The first part is quite easy since the number of choices for the numerator is clearly bounded by $2\kappa+1$ and the denominator is bounded by $\kappa$ . But, I have no idea how to proceed with the second part.","[ Rational Points on Elliptic Curves - Joseph H. Silverman, ex 3.1] Given a rational number in its lowest terms, define Let be the number of rational numbers with less than . First prove that and then show that The first part is quite easy since the number of choices for the numerator is clearly bounded by and the denominator is bounded by . But, I have no idea how to proceed with the second part.","x=\frac mn H(x):=\max\left\{|m|,|n|\right\}. R(\kappa) x H(x) \kappa R(\kappa)\le 2\kappa^2+\kappa \lim_{\kappa \to \infty} \frac{R(\kappa)}{\kappa^2}=\frac{12}{\pi^2} 2\kappa+1 \kappa","['limits', 'rational-numbers', 'pi']"
68,Request an explain for a limit [closed],Request an explain for a limit [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question $$\lim_{x\to 0+}\exp\left(\frac{\ln \left( \frac{\ln(1+\tan 4x)}{4x} \right)}{\tan x}\right)=\lim_{x\to 0+}\exp\left(\frac{ \frac{\ln(1+\tan 4x)}{4x} -1}{x}\right)$$ Can anybody explain above equation? I can't understand right hand side of the equation. Where is $\tan x$ and how pop up $-1$ ? Thank you. Sorry, infact I solved a question linked my question using Taylor expansion and I'm looking for a solution that not using Taylor and I saw the solution I have mentioned but I can't get it.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question Can anybody explain above equation? I can't understand right hand side of the equation. Where is and how pop up ? Thank you. Sorry, infact I solved a question linked my question using Taylor expansion and I'm looking for a solution that not using Taylor and I saw the solution I have mentioned but I can't get it.",\lim_{x\to 0+}\exp\left(\frac{\ln \left( \frac{\ln(1+\tan 4x)}{4x} \right)}{\tan x}\right)=\lim_{x\to 0+}\exp\left(\frac{ \frac{\ln(1+\tan 4x)}{4x} -1}{x}\right) \tan x -1,['limits']
69,Do limits of (continuous) functions have a meaning in the distribution sense?,Do limits of (continuous) functions have a meaning in the distribution sense?,,"This question came to my mind when working with distributions. We know that locally integrable functions can be realized as distributions. Particularly, if $f$ is locally integrable on $\Omega$ , then for any $\phi \in C^{\infty}_c ( \Omega )$ , we have the action of $f$ on $\phi$ given by $$f ( \phi ) = \int\limits_{\Omega} f ( x ) \phi ( x )\, \mathrm{d}x.$$ Now, we also know that continuous functions are locally integrable, and hence can be realized as distributions. My question is whether we can make sense of the point values $f ( x )$ as distributions? Further, can we make sense of "" $\lim\limits_{x \rightarrow x_0} f ( x ) = f ( x_0 )$ "" in the sense of distributions? Any insights into this are appreciable!","This question came to my mind when working with distributions. We know that locally integrable functions can be realized as distributions. Particularly, if is locally integrable on , then for any , we have the action of on given by Now, we also know that continuous functions are locally integrable, and hence can be realized as distributions. My question is whether we can make sense of the point values as distributions? Further, can we make sense of "" "" in the sense of distributions? Any insights into this are appreciable!","f \Omega \phi \in C^{\infty}_c ( \Omega ) f \phi f ( \phi ) = \int\limits_{\Omega} f ( x ) \phi ( x )\, \mathrm{d}x. f ( x ) \lim\limits_{x \rightarrow x_0} f ( x ) = f ( x_0 )","['real-analysis', 'limits', 'distribution-theory']"
70,Examine for conditional and absolute convergence,Examine for conditional and absolute convergence,,"The problem: $$\sum_{n=1}^\infty \sqrt{n+1} - \sqrt[4]{n^2+n+1} $$ What I've done trying the root ratio test: $$\lim_{n\to \infty} |\frac{\sqrt{n+2} - \sqrt[4]{n^2+2n+1+n+2}}{\sqrt{n+1} - \sqrt[4]{n^2+n+1}}|<1 $$ $$\lim_{n\to \infty} |\frac{\sqrt{n+2} - \sqrt[4]{n^2+3n+3}}{\sqrt{n+1} - \sqrt[4]{n^2+n+1}} |<1$$ $$\lim_{n\to \infty} |\frac{e^{\frac{1}{2}ln(n+2)} - e^{\frac{1}{4}ln(n^2+3n+3)}}{e^{\frac{1}{2}ln(n+2)} - e^{\frac{1}{4}ln(n^2+3n+3)}}|<1 $$ The question: I don't know where to go from here. Maybe the criteria I am using is wrong. I think it could be solved if we compare it with something, but I don't know what.","The problem: What I've done trying the root ratio test: The question: I don't know where to go from here. Maybe the criteria I am using is wrong. I think it could be solved if we compare it with something, but I don't know what.",\sum_{n=1}^\infty \sqrt{n+1} - \sqrt[4]{n^2+n+1}  \lim_{n\to \infty} |\frac{\sqrt{n+2} - \sqrt[4]{n^2+2n+1+n+2}}{\sqrt{n+1} - \sqrt[4]{n^2+n+1}}|<1  \lim_{n\to \infty} |\frac{\sqrt{n+2} - \sqrt[4]{n^2+3n+3}}{\sqrt{n+1} - \sqrt[4]{n^2+n+1}} |<1 \lim_{n\to \infty} |\frac{e^{\frac{1}{2}ln(n+2)} - e^{\frac{1}{4}ln(n^2+3n+3)}}{e^{\frac{1}{2}ln(n+2)} - e^{\frac{1}{4}ln(n^2+3n+3)}}|<1 ,"['calculus', 'sequences-and-series', 'limits', 'convergence-divergence']"
71,Example of functions where L'Hospital fails with limit at finite value due to nonexistence of limits of derivatives but the actual limit exists.,Example of functions where L'Hospital fails with limit at finite value due to nonexistence of limits of derivatives but the actual limit exists.,,"I've seen that L'Hospital's Rule fails for functions such as $$\lim_{x \to \infty} \frac{x}{x+\sin(x)}$$ since the limit of the derivatives oscillates. But all the examples I've seen so far have been limits at infinity. Are there any examples of functions where the actual limit exists, the limit is evaluated at a finite value but the limit of the derivatives is not equal to the actual limit? I've been thinking about the functions $f(x)=1/x+\sin(1/x)$ and $g(x)=1/x$ and taking the limit as $x\to 0$ but clearly these functions are not differentiable at an interval containing zero so it doesn't work.","I've seen that L'Hospital's Rule fails for functions such as since the limit of the derivatives oscillates. But all the examples I've seen so far have been limits at infinity. Are there any examples of functions where the actual limit exists, the limit is evaluated at a finite value but the limit of the derivatives is not equal to the actual limit? I've been thinking about the functions and and taking the limit as but clearly these functions are not differentiable at an interval containing zero so it doesn't work.",\lim_{x \to \infty} \frac{x}{x+\sin(x)} f(x)=1/x+\sin(1/x) g(x)=1/x x\to 0,"['real-analysis', 'calculus', 'limits']"
72,Help with the study of the function $f(x) = \frac{-2}{5x-\ln\vert x \vert}$,Help with the study of the function,f(x) = \frac{-2}{5x-\ln\vert x \vert},"I'm having problems in understanding few things about this function, also because some of my calculations do not match the plot. $$f(x) = \dfrac{-2}{5x-\ln\vert x \vert}$$ Here is what I did. First of all, the domain is $\Omega: x\in (-\infty, x^*)\cup(x^*, 0)\cup(0, +\infty)$ where $x^*$ is the point that makes $5x - \ln\vert x \vert = 0$ . I understood through a very easy analysis that it's negative. The domain is a union of open sets to there is no garancy we have max or min but we go on the same. I calculated these behaviours: $$\lim_{x\to \pm\infty} f(x) = 0^{\mp}$$ $$\lim_{x\to 0^{\pm}} f(x) = 0$$ $$\lim_{x\to x^{*+}} f(x) = -\infty$$ $$\lim_{x\to x^{*-}} f(x) = +\infty$$ So thanks to this I sketched the plot a bit, understanding that there is a vertical asymptote at $x^*$ and a horizontal asymptote at zero. At $x = 0$ the function is continuos (removable singularity) but not differentiable (it might be a cusp point), for $$f'(x) = \dfrac{2}{(5x-\ln\vert x\vert)^2}\left(5 - \dfrac{1}{\vert x \vert}\right)$$ Where we see the non differentiability at $x^*$ and at $0$ . Also $f(x)$ is not continuous at $x^*$ . Troubles start now, where I have to study the monotonicity of the function. Splitting it into positive and negative $x$ I have $$f'(x) = \begin{cases}  \dfrac{2}{(5x-\ln(x))^2}\left(5 - \dfrac{1}{x}\right) \qquad & x > 0 \\ \dfrac{2}{(5x-\ln(-x))^2}\left(5 + \dfrac{1}{x}\right) \qquad & x < 0  \end{cases} $$ When studying $f'(x) > 0$ I recognize the left term is always positive, but the term in the round brackets in the right changes. I get for $x > 0$ $$f'(x) > 0 \rightarrow x > 1/5 $$ And the function is decreasing for $0 < x < 1/5$ . This matches the plot. But for $x < 0$ I get $$f'(x) > 0 \rightarrow x\in(-\infty, -1/5)$$ And it's decreasing for $-1/5 < x < 0$ The problems here are: -- The function is actually increasing for $x\in (-\infty, x^*)$ and for $x\in(x^*, 0)$ -- There is no decreasing part for $x < 0$ -- Also the limit for $f(x)$ at zero seems like to not be zero... Hence here I got stuck. Can someone please help? Thank you!","I'm having problems in understanding few things about this function, also because some of my calculations do not match the plot. Here is what I did. First of all, the domain is where is the point that makes . I understood through a very easy analysis that it's negative. The domain is a union of open sets to there is no garancy we have max or min but we go on the same. I calculated these behaviours: So thanks to this I sketched the plot a bit, understanding that there is a vertical asymptote at and a horizontal asymptote at zero. At the function is continuos (removable singularity) but not differentiable (it might be a cusp point), for Where we see the non differentiability at and at . Also is not continuous at . Troubles start now, where I have to study the monotonicity of the function. Splitting it into positive and negative I have When studying I recognize the left term is always positive, but the term in the round brackets in the right changes. I get for And the function is decreasing for . This matches the plot. But for I get And it's decreasing for The problems here are: -- The function is actually increasing for and for -- There is no decreasing part for -- Also the limit for at zero seems like to not be zero... Hence here I got stuck. Can someone please help? Thank you!","f(x) = \dfrac{-2}{5x-\ln\vert x \vert} \Omega: x\in (-\infty, x^*)\cup(x^*, 0)\cup(0, +\infty) x^* 5x - \ln\vert x \vert = 0 \lim_{x\to \pm\infty} f(x) = 0^{\mp} \lim_{x\to 0^{\pm}} f(x) = 0 \lim_{x\to x^{*+}} f(x) = -\infty \lim_{x\to x^{*-}} f(x) = +\infty x^* x = 0 f'(x) = \dfrac{2}{(5x-\ln\vert x\vert)^2}\left(5 - \dfrac{1}{\vert x \vert}\right) x^* 0 f(x) x^* x f'(x) = \begin{cases}
 \dfrac{2}{(5x-\ln(x))^2}\left(5 - \dfrac{1}{x}\right) \qquad & x > 0 \\
\dfrac{2}{(5x-\ln(-x))^2}\left(5 + \dfrac{1}{x}\right) \qquad & x < 0 
\end{cases}
 f'(x) > 0 x > 0 f'(x) > 0 \rightarrow x > 1/5  0 < x < 1/5 x < 0 f'(x) > 0 \rightarrow x\in(-\infty, -1/5) -1/5 < x < 0 x\in (-\infty, x^*) x\in(x^*, 0) x < 0 f(x)","['limits', 'analysis', 'functions', 'continuity', 'monotone-functions']"
73,$\lim_{n \to \infty} \frac{1}{\sqrt{n^k}} (1 + \frac{1}{\sqrt{2}} + \frac{1}{\sqrt{3}} + \cdots + \frac{1}{\sqrt{n}})^k$ by Stolz–Cesàro,by Stolz–Cesàro,\lim_{n \to \infty} \frac{1}{\sqrt{n^k}} (1 + \frac{1}{\sqrt{2}} + \frac{1}{\sqrt{3}} + \cdots + \frac{1}{\sqrt{n}})^k,"$$\lim_{n \to \infty} \frac{1}{\sqrt{n^k}} (1 + \frac{1}{\sqrt{2}} + \frac{1}{\sqrt{3}} + \cdots + \frac{1}{\sqrt{n}})^k, k \in \mathbb{N}$$ Putting $\sqrt{n^k}$ in denominator, we get: $$\lim_{n \to \infty} \frac{(1 + \frac{1}{\sqrt{2}} + \frac{1}{\sqrt{3}} + \cdots + \frac{1}{\sqrt{n}})^k}{\sqrt{n^k}}$$ Apllying Stolz–Cesàro theorem: $$\lim_{n \to \infty} \frac{(1 + \frac{1}{\sqrt{2}} + \frac{1}{\sqrt{3}} + \cdots + \frac{1}{\sqrt{n}})^k - (1 + \frac{1}{\sqrt{2}} + \frac{1}{\sqrt{3}} + \cdots + \frac{1}{\sqrt{n-1}})^k }{\sqrt{n^k} - \sqrt{(n-1)^k}}$$ I don't know how to use properly multinomial formula, but the biggest number in numerator among denominators in first brackets with $k$ degree is $\sqrt{n^k}$ , in second brackets $\sqrt{(n-1)^k}$ , their coefficients are $1$ $$\lim_{n \to \infty} \frac{ \frac{1}{\sqrt{n^k}} - \frac{1}{\sqrt{(n-1)^k}} + \frac{1}{\sqrt{(n-1)^k}} - \cdots + 1 - 1 + \cdots}{\sqrt{n^k} - \sqrt{(n-1)^k}}$$ Multiplying numbers in first brackets by $\frac{1}{\sqrt{n}}$ gives us $\frac{1}{\sqrt{2^{k-1} \times n}} + \frac{1}{\sqrt{3^{k-1} \times n}} + \cdots + \frac{1}{\sqrt{(n-1)^{k-1} \times n}}$ etc., but I suspect we can't get same numbers in second brackets. My guess, after evaluating sum (or difference) in numerator, we need to divide each number of our expression by $\sqrt{n^k}$ . In denominator we get 0, because $$\sqrt{\frac{n^k}{n^k}} - \sqrt{\frac{(n-1)^k}{n^k}} = 1 - \sqrt{(\frac{n-1}{n})^k} = 1 - 1 = 0, n \to \infty$$ Multiplication and division of denominator by conjugate ( $\sqrt{n^k} + \sqrt{(n-1)^k}$ ) gives ""slightly"" different picture (unless I've done several big mistakes in a row): $$\frac{ n^k - (n-1)^k }{ \sqrt{n^k} + \sqrt{(n-1)^k} } = \frac{ n^k - n^k + kn^{k-1} - \cdots }{ \sqrt{n^k} + \sqrt{(n-1)^k} } = \frac{kn^{k-1} - {k\choose 2}n^{k-2} +\cdots }{ \sqrt{n^k} + \sqrt{(n-1)^k} }, n \to \infty$$ Dividing by $n^{k-1}$ : $$\frac{k - 0 + 0}{ \sqrt{\frac{n^k}{n^{2(k-1)}  } } + \sqrt{ \frac{(n-1)^k}{n^{2(k-1)}}  }} = \frac{k - 0 + 0 - \cdots}{0 + 0}$$ So my guess was a bad guess. Unfortunately, by this point I'm pretty much lost. Hence, I need some help... Thank you!","Putting in denominator, we get: Apllying Stolz–Cesàro theorem: I don't know how to use properly multinomial formula, but the biggest number in numerator among denominators in first brackets with degree is , in second brackets , their coefficients are Multiplying numbers in first brackets by gives us etc., but I suspect we can't get same numbers in second brackets. My guess, after evaluating sum (or difference) in numerator, we need to divide each number of our expression by . In denominator we get 0, because Multiplication and division of denominator by conjugate ( ) gives ""slightly"" different picture (unless I've done several big mistakes in a row): Dividing by : So my guess was a bad guess. Unfortunately, by this point I'm pretty much lost. Hence, I need some help... Thank you!","\lim_{n \to \infty} \frac{1}{\sqrt{n^k}} (1 + \frac{1}{\sqrt{2}} + \frac{1}{\sqrt{3}} + \cdots + \frac{1}{\sqrt{n}})^k, k \in \mathbb{N} \sqrt{n^k} \lim_{n \to \infty} \frac{(1 + \frac{1}{\sqrt{2}} + \frac{1}{\sqrt{3}} + \cdots + \frac{1}{\sqrt{n}})^k}{\sqrt{n^k}} \lim_{n \to \infty} \frac{(1 + \frac{1}{\sqrt{2}} + \frac{1}{\sqrt{3}} + \cdots + \frac{1}{\sqrt{n}})^k - (1 + \frac{1}{\sqrt{2}} + \frac{1}{\sqrt{3}} + \cdots + \frac{1}{\sqrt{n-1}})^k }{\sqrt{n^k} - \sqrt{(n-1)^k}} k \sqrt{n^k} \sqrt{(n-1)^k} 1 \lim_{n \to \infty} \frac{ \frac{1}{\sqrt{n^k}} - \frac{1}{\sqrt{(n-1)^k}} + \frac{1}{\sqrt{(n-1)^k}} - \cdots + 1 - 1 + \cdots}{\sqrt{n^k} - \sqrt{(n-1)^k}} \frac{1}{\sqrt{n}} \frac{1}{\sqrt{2^{k-1} \times n}} + \frac{1}{\sqrt{3^{k-1} \times n}} + \cdots + \frac{1}{\sqrt{(n-1)^{k-1} \times n}} \sqrt{n^k} \sqrt{\frac{n^k}{n^k}} - \sqrt{\frac{(n-1)^k}{n^k}} = 1 - \sqrt{(\frac{n-1}{n})^k} = 1 - 1 = 0, n \to \infty \sqrt{n^k} + \sqrt{(n-1)^k} \frac{ n^k - (n-1)^k }{ \sqrt{n^k} + \sqrt{(n-1)^k} } = \frac{ n^k - n^k + kn^{k-1} - \cdots }{ \sqrt{n^k} + \sqrt{(n-1)^k} } = \frac{kn^{k-1} - {k\choose 2}n^{k-2} +\cdots }{ \sqrt{n^k} + \sqrt{(n-1)^k} }, n \to \infty n^{k-1} \frac{k - 0 + 0}{ \sqrt{\frac{n^k}{n^{2(k-1)}  } } + \sqrt{ \frac{(n-1)^k}{n^{2(k-1)}}  }} = \frac{k - 0 + 0 - \cdots}{0 + 0}","['real-analysis', 'calculus', 'limits']"
74,Find slope of the tangent line of $4\sqrt x + 2e^\frac {3x-12}{x+2}$ at $ x_0$ [closed],Find slope of the tangent line of  at  [closed],4\sqrt x + 2e^\frac {3x-12}{x+2}  x_0,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question Find the slope and the equation of the tangent line to the graph $y = f(x)$ at $x_0=4$ , $$4\sqrt x + 2e^\frac {3x-12}{x+2} $$ $$\lim_{h\to 0}\tfrac{4\sqrt {4+h} + 2e^\frac {12+3h-12}{4+h+2} - 10}{h} = \lim_{h\to 0}\frac{4\sqrt {4+h} + 2e^\frac {3h}{6+h} - 10}{h} $$ I am stuck at this part","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question Find the slope and the equation of the tangent line to the graph at , I am stuck at this part",y = f(x) x_0=4 4\sqrt x + 2e^\frac {3x-12}{x+2}  \lim_{h\to 0}\tfrac{4\sqrt {4+h} + 2e^\frac {12+3h-12}{4+h+2} - 10}{h} = \lim_{h\to 0}\frac{4\sqrt {4+h} + 2e^\frac {3h}{6+h} - 10}{h} ,"['calculus', 'limits', 'slope']"
75,Difference integral of continuous function,Difference integral of continuous function,,"Let $f:\mathbb R\rightarrow \mathbb R$ be a continuous function such that $$\lim_{t\rightarrow -\infty}f(t)=l_1,\hspace{0.4cm} \lim_{t\rightarrow +\infty}f(t)=l_2$$ Evaluate $$\int_{-\infty}^{+\infty}\left[f(t+1)-f(t)\right]dt$$ I thought of variable change : We have $$\int_{-\infty}^{+\infty}f(t+1)dt-\int_{-\infty}^{+\infty}f(t)dt$$ By substituting $u=t+1$ in the first integral, we obtain $$\int_{-\infty}^{+\infty}f(u)du-\int_{-\infty}^{+\infty}f(t)dt$$ My mind says it's the same integral so it must be equal to $0$ . But we still have no idea about the convergence of the integrals. I tried also to parametrize the integral : Let $$I(\alpha)=\int_{-\infty}^{+\infty}t^\alpha f(t)dt$$ Which in the end will lead to $I(0)-I(0)=0$ But here I still hesitate whether I defined $I$ as a diverging function","Let be a continuous function such that Evaluate I thought of variable change : We have By substituting in the first integral, we obtain My mind says it's the same integral so it must be equal to . But we still have no idea about the convergence of the integrals. I tried also to parametrize the integral : Let Which in the end will lead to But here I still hesitate whether I defined as a diverging function","f:\mathbb R\rightarrow \mathbb R \lim_{t\rightarrow -\infty}f(t)=l_1,\hspace{0.4cm} \lim_{t\rightarrow +\infty}f(t)=l_2 \int_{-\infty}^{+\infty}\left[f(t+1)-f(t)\right]dt \int_{-\infty}^{+\infty}f(t+1)dt-\int_{-\infty}^{+\infty}f(t)dt u=t+1 \int_{-\infty}^{+\infty}f(u)du-\int_{-\infty}^{+\infty}f(t)dt 0 I(\alpha)=\int_{-\infty}^{+\infty}t^\alpha f(t)dt I(0)-I(0)=0 I","['integration', 'limits', 'convergence-divergence']"
76,Equality between a set and the limit superior of a sequence of sets,Equality between a set and the limit superior of a sequence of sets,,"I have been asked to discuss whether $\{x : \varlimsup f_n(x) \gt a\}$ is equal to $\varlimsup \{x : f_n(x) \gt a \}$ I know there are many missing details but this is the way my professor has given the class the exercise. I assume $a\in\mathbb{R}$ and $\forall n\in\mathbb{N} , f_n : \mathbb{R}\to\mathbb{R}$ . The professor told us that the sets are not equal because we have $\{x : \varlimsup f_n(x) > a\} \subset  \varlimsup \{x : f_n(x) > a \}$ but $\{x : \varlimsup f_n(x) > a\}  \not\supset  \varlimsup \{x : f_n(x) > a \}$ I think I understand why the first statement is true. But the professor told us that the second is true because $y\in\varlimsup \{x : f_n(x) > a \}$ only implies $y\in\{x : \varlimsup f_n(x) \ge a\}$ which I don't understand. Could someone explain it to me ? Similarly, we have been told that $\{x : \varlimsup f_n(x) \ge a\}$ is not equal to $\varlimsup \{x : f_n(x) \ge a \}$ Is it for the same reasons ? Thanks in advance!","I have been asked to discuss whether is equal to I know there are many missing details but this is the way my professor has given the class the exercise. I assume and . The professor told us that the sets are not equal because we have but I think I understand why the first statement is true. But the professor told us that the second is true because only implies which I don't understand. Could someone explain it to me ? Similarly, we have been told that is not equal to Is it for the same reasons ? Thanks in advance!","\{x : \varlimsup f_n(x) \gt a\} \varlimsup \{x : f_n(x) \gt a \} a\in\mathbb{R} \forall n\in\mathbb{N} , f_n : \mathbb{R}\to\mathbb{R} \{x : \varlimsup f_n(x) > a\} \subset 
\varlimsup \{x : f_n(x) > a \} \{x : \varlimsup f_n(x) > a\}  \not\supset 
\varlimsup \{x : f_n(x) > a \} y\in\varlimsup \{x : f_n(x) > a \} y\in\{x : \varlimsup f_n(x) \ge a\} \{x : \varlimsup f_n(x) \ge a\} \varlimsup \{x : f_n(x) \ge a \}","['sequences-and-series', 'limits', 'functions']"
77,Here is a linear algebra problem from an interview and I have no clue.,Here is a linear algebra problem from an interview and I have no clue.,,"Matrix C of size n $\times$ n is symmetric . Zero is a simple eigenvalue of C. The associated eigenvector is q. For $\epsilon$ >0, the equation $Cx+\epsilon x=d$ in x, where x and d are n-dimensional Column vectors and d is known, has a solution that depends on $\epsilon$ . Call this solution $x(\epsilon)$ . Express $ \lim\limits_{\epsilon \to 0^+} \epsilon x(\epsilon)$ in terms of vectors q and d. I have some inspirations: When $\epsilon \to 0$ , $\epsilon q$ also comes to $0$ . So we have: $C(x+q)+\epsilon (x+q)=d$ Since C is a symmetrix matrix, we use diagonalization like: $U\Lambda U^Hx+\epsilon x=d$ But to be honest, I have no clue on this and it bothers me a lot. I can't figure out how to use these conditions we have. Thank you for reading my question! Could you give me some clues on this problem?","Matrix C of size n n is symmetric . Zero is a simple eigenvalue of C. The associated eigenvector is q. For >0, the equation in x, where x and d are n-dimensional Column vectors and d is known, has a solution that depends on . Call this solution . Express in terms of vectors q and d. I have some inspirations: When , also comes to . So we have: Since C is a symmetrix matrix, we use diagonalization like: But to be honest, I have no clue on this and it bothers me a lot. I can't figure out how to use these conditions we have. Thank you for reading my question! Could you give me some clues on this problem?",\times \epsilon Cx+\epsilon x=d \epsilon x(\epsilon)  \lim\limits_{\epsilon \to 0^+} \epsilon x(\epsilon) \epsilon \to 0 \epsilon q 0 C(x+q)+\epsilon (x+q)=d U\Lambda U^Hx+\epsilon x=d,"['linear-algebra', 'limits', 'eigenvalues-eigenvectors']"
78,Tips to solve limits of 2 variables,Tips to solve limits of 2 variables,,"Anyone got a tips on solving this type of limits $$ \lim_{(x^2 + y^2) \to \infty} \frac{e^{x^2+xy+y^2}-1}{e^{x^2+y^2}-1}$$ I'm always having a trouble finding a good method to solve for limits of 2 variables approaching infinity, usually I'd either use Taylor (mostly if limit approaches 0) or Polar coordinates, but I tried both here without any results, what is the way to think here?","Anyone got a tips on solving this type of limits I'm always having a trouble finding a good method to solve for limits of 2 variables approaching infinity, usually I'd either use Taylor (mostly if limit approaches 0) or Polar coordinates, but I tried both here without any results, what is the way to think here?", \lim_{(x^2 + y^2) \to \infty} \frac{e^{x^2+xy+y^2}-1}{e^{x^2+y^2}-1},"['calculus', 'limits', 'multivariable-calculus', 'taylor-expansion', 'polar-coordinates']"
79,Does there exist a real $\alpha$ such that series $\{n^\alpha x_n\}$ converges to a non-zero number?,Does there exist a real  such that series  converges to a non-zero number?,\alpha \{n^\alpha x_n\},"If $x_{n+1}=x_n(1-x_n^2)$ , $x_1 \in (0,1)$ , does there exist a real $\alpha$ such that series $\{n^\alpha x_n\}$ converges to non-zero number? I have written a program to check if the series converges. Plugging in $\alpha=1/2$ it seems that $\lim_{n\to\infty}{(\sqrt{n}~x_n)}$ always converges to $\frac{\sqrt{2}}{2}$ , but I couldn't find an elegant way to prove that. So far, I have proved that $\lim{\frac{x_{n+1}}{x_n}}=1$ and $x_{n+1}<x_n$ , but I didn't find that very useful. Any help is appreciated!","If , , does there exist a real such that series converges to non-zero number? I have written a program to check if the series converges. Plugging in it seems that always converges to , but I couldn't find an elegant way to prove that. So far, I have proved that and , but I didn't find that very useful. Any help is appreciated!","x_{n+1}=x_n(1-x_n^2) x_1 \in (0,1) \alpha \{n^\alpha x_n\} \alpha=1/2 \lim_{n\to\infty}{(\sqrt{n}~x_n)} \frac{\sqrt{2}}{2} \lim{\frac{x_{n+1}}{x_n}}=1 x_{n+1}<x_n","['real-analysis', 'sequences-and-series', 'limits']"
80,Find $\lim_{x \to \infty} x^{(2-\sin(\frac{2}{x}))}(x\sin(\frac{2}{x})-2)$ with L'Hospital's Rule,Find  with L'Hospital's Rule,\lim_{x \to \infty} x^{(2-\sin(\frac{2}{x}))}(x\sin(\frac{2}{x})-2),"I try to find $\lim_{x \to \infty} x^{(2-\sin(\frac{2}{x}))}(x\sin(\frac{2}{x})-2)$ with L'Hospital's Rule but get stuck. Here is my attempt. Let substitute $y = \dfrac{1}{x}$ \begin{align*} \lim_{x \to \infty} x^{(2-\sin(\frac{2}{x}))}(x\sin(\frac{2}{x})-2) &= \lim_{y \to 0} \frac{\left(\frac{\sin(2y)}{y}-2\right)}{y^{(2-\sin(2y))}} \\ &= \lim_{y \to 0} \frac{\sin(2y)-2y}{y^{(3-\sin(2y))}} \\ \text{(L'Hospital's Rule)} &= \lim_{y \to 0} \frac{2\cos(2y)-2}{-y^{(2-\sin(2y))}(\sin(2y) + 2y\ln(y)cos(2y) - 3)} \end{align*} After I use L'Hospital's Rule, everything seem to get messier. I think the next step is using L'Hospital's Rule again but everything will only get messier. I think I go in the wrong way since L'Hospital's Rule should not make everything worse. Where do I do wrong ?","I try to find with L'Hospital's Rule but get stuck. Here is my attempt. Let substitute After I use L'Hospital's Rule, everything seem to get messier. I think the next step is using L'Hospital's Rule again but everything will only get messier. I think I go in the wrong way since L'Hospital's Rule should not make everything worse. Where do I do wrong ?","\lim_{x \to \infty} x^{(2-\sin(\frac{2}{x}))}(x\sin(\frac{2}{x})-2) y = \dfrac{1}{x} \begin{align*}
\lim_{x \to \infty} x^{(2-\sin(\frac{2}{x}))}(x\sin(\frac{2}{x})-2) &= \lim_{y \to 0} \frac{\left(\frac{\sin(2y)}{y}-2\right)}{y^{(2-\sin(2y))}} \\
&= \lim_{y \to 0} \frac{\sin(2y)-2y}{y^{(3-\sin(2y))}} \\
\text{(L'Hospital's Rule)} &= \lim_{y \to 0} \frac{2\cos(2y)-2}{-y^{(2-\sin(2y))}(\sin(2y) + 2y\ln(y)cos(2y) - 3)}
\end{align*}",['limits']
81,$\{\log n-n(1-na_n)\}$ is convergent for $a_{n+1}=a_n(1-a_n)$ and $0<a_1<1$ (without using asymptotic analysis),is convergent for  and  (without using asymptotic analysis),\{\log n-n(1-na_n)\} a_{n+1}=a_n(1-a_n) 0<a_1<1,"Let a sequence $\{a_n\}_{n=1}^\infty$ satisfy $0<a_1<1$ and $a_{n+1}=a_n(1-a_n)$ for $n\geq 1$ . Prove that $\{\log n-n(1-na_n)\}$ is convergent (without using asymptotic analysis). This problem came to me when I was preparing my exercise lessons on Analysis(I) course, as a teaching assistant. The original problem asks students to prove $\lim_{n\to\infty}na_n=1$ . I'm curious about the asymptotic behaviour of this sequence. Using the method of asymptotic analysis, I found that $$a_n= \frac1n-\frac{\log n}{n^2}+\frac C{n^2}+o\left(\frac1{n^2}\right),\qquad n\to\infty,$$ where $C$ is a constant probably depending on the value of $a_1$ . After that, I tried to prove this asymptotic relation, by directly proving the following three properties: $\displaystyle\lim_{n\to\infty}na_n=1$ . $\displaystyle\lim_{n\to\infty}\frac{n}{\log n}(1-na_n)=1$ . $\displaystyle\lim_{n\to\infty}\log n\left(1-\frac{n}{\log n}(1-na_n)\right)=\lim_{n\to\infty}\left(\log n-n(1-na_n)\right)$ exists. We can easily show that $a_n$ is decreasing to $0$ . The first two properties can be proved using $\frac1{a_{n+1}}=\frac1{a_n}+\frac1{1-a_n}$ and Stolz–Cesàro theorem : for the first one, we apply Stolz–Cesàro theorem to $\frac{n}{1/a_n}$ ; and for the second one, we apply Stolz–Cesàro theorem to $$\frac{\frac{1-na_n}{a_n}}{\log n}=\frac{\frac1{a_n}-n}{\log n}.$$ However, I don't know how to prove the third one. I don't believe that we can prove it by directly using Stolz–Cesàro theorem. Because Stolz–Cesàro theorem will give a limit not depending on $a_1$ . Any help would be appreciated!","Let a sequence satisfy and for . Prove that is convergent (without using asymptotic analysis). This problem came to me when I was preparing my exercise lessons on Analysis(I) course, as a teaching assistant. The original problem asks students to prove . I'm curious about the asymptotic behaviour of this sequence. Using the method of asymptotic analysis, I found that where is a constant probably depending on the value of . After that, I tried to prove this asymptotic relation, by directly proving the following three properties: . . exists. We can easily show that is decreasing to . The first two properties can be proved using and Stolz–Cesàro theorem : for the first one, we apply Stolz–Cesàro theorem to ; and for the second one, we apply Stolz–Cesàro theorem to However, I don't know how to prove the third one. I don't believe that we can prove it by directly using Stolz–Cesàro theorem. Because Stolz–Cesàro theorem will give a limit not depending on . Any help would be appreciated!","\{a_n\}_{n=1}^\infty 0<a_1<1 a_{n+1}=a_n(1-a_n) n\geq 1 \{\log n-n(1-na_n)\} \lim_{n\to\infty}na_n=1 a_n= \frac1n-\frac{\log n}{n^2}+\frac C{n^2}+o\left(\frac1{n^2}\right),\qquad n\to\infty, C a_1 \displaystyle\lim_{n\to\infty}na_n=1 \displaystyle\lim_{n\to\infty}\frac{n}{\log n}(1-na_n)=1 \displaystyle\lim_{n\to\infty}\log n\left(1-\frac{n}{\log n}(1-na_n)\right)=\lim_{n\to\infty}\left(\log n-n(1-na_n)\right) a_n 0 \frac1{a_{n+1}}=\frac1{a_n}+\frac1{1-a_n} \frac{n}{1/a_n} \frac{\frac{1-na_n}{a_n}}{\log n}=\frac{\frac1{a_n}-n}{\log n}. a_1","['real-analysis', 'sequences-and-series', 'limits', 'analysis', 'recurrence-relations']"
82,Definition of the omega-limit set,Definition of the omega-limit set,,"The $\omega$ -limit set of the set $B$ is defined as $\omega(B) = \bigcap_{n \geq 0} \overline{\bigcup_{k \geq n} T^k(B) }$ , where $T$ is some continuous mapping. What is the intuition behind this definition? I found this image in the Wiggins's book ""Introduction to Applied Nonlinear Dynamical Systems and Chaos"" and it shows that the $\omega$ -limit set of point $x$ is a point $x_0$ on the curve $\gamma$ ): To me, it is clear  that $y \in \omega(B) $ iff there exists sequence $x_j \in B$ and sequence of integers $n_j \rightarrow \infty$ such that $T^{n_j} x_j  \rightarrow y  $ as $j \rightarrow \infty$ . But I don't completely understand the definition from beginning, why do we need union and then intersection? Thanks a lot in advance.","The -limit set of the set is defined as , where is some continuous mapping. What is the intuition behind this definition? I found this image in the Wiggins's book ""Introduction to Applied Nonlinear Dynamical Systems and Chaos"" and it shows that the -limit set of point is a point on the curve ): To me, it is clear  that iff there exists sequence and sequence of integers such that as . But I don't completely understand the definition from beginning, why do we need union and then intersection? Thanks a lot in advance.",\omega B \omega(B) = \bigcap_{n \geq 0} \overline{\bigcup_{k \geq n} T^k(B) } T \omega x x_0 \gamma y \in \omega(B)  x_j \in B n_j \rightarrow \infty T^{n_j} x_j  \rightarrow y   j \rightarrow \infty,"['calculus', 'limits', 'dynamical-systems']"
83,How to prove $\lim_{x\to1} \log(x)=0$ in a fair way?,How to prove  in a fair way?,\lim_{x\to1} \log(x)=0,"I am trying to prove the following limit with the definition: $$\lim_{x\to1} \log(x)=0$$ That is - I must prove that for all $\epsilon>0$ , there exists $\delta >0$ such that: $$ 0<|x-1|< \delta \implies |\log x - 0|< \epsilon$$ So - my trouble is to understand what is ""fair"" to use here. I know the following: $x<e^x \implies \log(x)<x$ And it would be nice if we could find that $\log(x)\leq x-1$ From this I think we get: $$|\log(x)|\leq|x-1| \text{ for }x\in[1,\infty]\qquad |\log(x)|\geq|x-1| \text{ for }x\in[-\infty,1]\tag{$\star$}$$ The trouble for me is that I know $\log(x)\leq x-1$ is true because of two things: Because $\log(1)=1-1=0$ and hence as $\log(x)$ and $x-1$ start in the same point and $\log'(x)=\frac{1}{x}$ and $(x-1)'=1$ then $\log (x)$ increases slower than $x-1$ when $x>1$ and decreases faster than $x-1$ when $x<1$ . But is it fair to use derivatives? In the book I am reading, we didn't even reached derivatives yet. I looked a plot of both functions, which also doesn't seems fair. Also, is it fair to assume we know that $\log(1)=0$ ? For me, knowing this seems to defeat the purpose of the problem somehow and the inequalities  I obtained in $(\star)$ , the first would be helpful but I don't see how the second would help me. So how can we proceed to prove this in a ""fair"" way?","I am trying to prove the following limit with the definition: That is - I must prove that for all , there exists such that: So - my trouble is to understand what is ""fair"" to use here. I know the following: And it would be nice if we could find that From this I think we get: The trouble for me is that I know is true because of two things: Because and hence as and start in the same point and and then increases slower than when and decreases faster than when . But is it fair to use derivatives? In the book I am reading, we didn't even reached derivatives yet. I looked a plot of both functions, which also doesn't seems fair. Also, is it fair to assume we know that ? For me, knowing this seems to defeat the purpose of the problem somehow and the inequalities  I obtained in , the first would be helpful but I don't see how the second would help me. So how can we proceed to prove this in a ""fair"" way?","\lim_{x\to1} \log(x)=0 \epsilon>0 \delta >0  0<|x-1|< \delta \implies |\log x - 0|< \epsilon x<e^x \implies \log(x)<x \log(x)\leq x-1 |\log(x)|\leq|x-1| \text{ for }x\in[1,\infty]\qquad |\log(x)|\geq|x-1| \text{ for }x\in[-\infty,1]\tag{\star} \log(x)\leq x-1 \log(1)=1-1=0 \log(x) x-1 \log'(x)=\frac{1}{x} (x-1)'=1 \log (x) x-1 x>1 x-1 x<1 \log(1)=0 (\star)",['limits']
84,1954 Miklos Schweitzer - Limit Distribution,1954 Miklos Schweitzer - Limit Distribution,,"Source : 1954 Miklos Schweitzer, Problem 5 Let $\xi _{1},\xi _{2},\dots ,\xi _{n},... $ be independent random variables of uniform distribution in $(0,1)$ . Show that the distribution of the random variable $$\eta _{n}= \sqrt[]{n}\prod_{k=1}^{n}(1-\frac{\xi _{k}}{k}) (n= 1,2,...)$$ tends to a limit distribution for $n \to \infty $ . Attempt : First I factor out a $\frac{1}{k}$ term from the product to get $$\sqrt[]{n}\prod_{k=1}^{n}(1-\frac{\xi _{k}}{k}) = \frac{1}{\sqrt{n}(n-1)!}\prod\limits_{k=1}^n (k-\xi_k).$$ Here, we have $k-\xi_k\sim \text{Unif}(k-1,k)$ and so then with $Y_k=-\log(k-\xi_k)$ and $\eta_n=-\log(\sqrt{n}(n-1)!)-\sum_{k=1}^n Y_k$ , we have $$f_{Y_k}(y)=\frac{1}{k-(k-1)}\cdot |-e^{-y}|=e^{-y}.$$ I thought this may be promising as it looks exponential, but the support doesn't match up - here we have $y\in[-\log(k),-\log(k-1)]$ . Additionally, I took the approach I did because I wanted to make this appear as exponentials whose sum is Gamma, but I realized this is dumb as we have an infinite number of them so even with aligned support, the resulting random variable is infinite WP1. The (n-1)! term on the denominator made me think of the Erlang distribution, but the exponential-looking random variables are not identically distributed and again this is limiting, not finite. Alternatively, I figured by inspection, it looks like we could apply the CLT and obtain something normal-looking, but I got tired looking at the problem. Edit : As per Brian's suggestion, we could take the log first $$\log(\eta_n) = \log(\sqrt{n})+\sum\limits_{k=1}^n \log(1-\frac{\xi_k}{k}).$$ Then letting $X_k=\log(1-\frac{\xi_k}{k})$ , we have for $x\in(\log(1-\frac{1}{k}),0)$ , $$f_{X_k}(x)=ke^x.$$ Taking the Laplace transform gives us $$\mathcal{L}(f_{X_k})=\mathbb{E}\left[e^{-tX_k}\right]=\int\limits_\Omega e^{-tx}\cdot ke^x\;dx$$ $$=\frac{k}{t+1}\left[\left(1-\frac{1}{k}\right)^{-(t+1)}-1\right].$$ This looks a bit horrid (possible I messed up the algebra), then with this, the idea is we'd have $$\mathbb{E}\left[e^{-t\sum_{k=1}^n X_k}\right]=\prod\limits_{k=1}^n \frac{k}{t+1}\left[\left(1-\frac{1}{k}\right)^{-(t+1)}-1\right]$$ by independence and we'd be able to identify this distribution. But two problems with this for me: 1) there's the lingering $\sqrt{n}$ term I don't know what to do with and 2) not sure what distribution this looks like. Question : I am all for hints on how to approach this correctly - thanks.","Source : 1954 Miklos Schweitzer, Problem 5 Let be independent random variables of uniform distribution in . Show that the distribution of the random variable tends to a limit distribution for . Attempt : First I factor out a term from the product to get Here, we have and so then with and , we have I thought this may be promising as it looks exponential, but the support doesn't match up - here we have . Additionally, I took the approach I did because I wanted to make this appear as exponentials whose sum is Gamma, but I realized this is dumb as we have an infinite number of them so even with aligned support, the resulting random variable is infinite WP1. The (n-1)! term on the denominator made me think of the Erlang distribution, but the exponential-looking random variables are not identically distributed and again this is limiting, not finite. Alternatively, I figured by inspection, it looks like we could apply the CLT and obtain something normal-looking, but I got tired looking at the problem. Edit : As per Brian's suggestion, we could take the log first Then letting , we have for , Taking the Laplace transform gives us This looks a bit horrid (possible I messed up the algebra), then with this, the idea is we'd have by independence and we'd be able to identify this distribution. But two problems with this for me: 1) there's the lingering term I don't know what to do with and 2) not sure what distribution this looks like. Question : I am all for hints on how to approach this correctly - thanks.","\xi _{1},\xi _{2},\dots ,\xi _{n},...  (0,1) \eta _{n}= \sqrt[]{n}\prod_{k=1}^{n}(1-\frac{\xi _{k}}{k}) (n= 1,2,...) n \to \infty  \frac{1}{k} \sqrt[]{n}\prod_{k=1}^{n}(1-\frac{\xi _{k}}{k}) = \frac{1}{\sqrt{n}(n-1)!}\prod\limits_{k=1}^n (k-\xi_k). k-\xi_k\sim \text{Unif}(k-1,k) Y_k=-\log(k-\xi_k) \eta_n=-\log(\sqrt{n}(n-1)!)-\sum_{k=1}^n Y_k f_{Y_k}(y)=\frac{1}{k-(k-1)}\cdot |-e^{-y}|=e^{-y}. y\in[-\log(k),-\log(k-1)] \log(\eta_n) = \log(\sqrt{n})+\sum\limits_{k=1}^n \log(1-\frac{\xi_k}{k}). X_k=\log(1-\frac{\xi_k}{k}) x\in(\log(1-\frac{1}{k}),0) f_{X_k}(x)=ke^x. \mathcal{L}(f_{X_k})=\mathbb{E}\left[e^{-tX_k}\right]=\int\limits_\Omega e^{-tx}\cdot ke^x\;dx =\frac{k}{t+1}\left[\left(1-\frac{1}{k}\right)^{-(t+1)}-1\right]. \mathbb{E}\left[e^{-t\sum_{k=1}^n X_k}\right]=\prod\limits_{k=1}^n \frac{k}{t+1}\left[\left(1-\frac{1}{k}\right)^{-(t+1)}-1\right] \sqrt{n}","['probability', 'limits']"
85,"$ f(x) = f(x + 1), \, \forall x \in \mathbb{R} \land \displaystyle \lim_{x \to \infty} f(x) = L \Rightarrow f(x) = L, \forall x \in \mathbb{R}$",," f(x) = f(x + 1), \, \forall x \in \mathbb{R} \land \displaystyle \lim_{x \to \infty} f(x) = L \Rightarrow f(x) = L, \forall x \in \mathbb{R}","I want to prove that $ f $ is continuous $\,  \land f(x) = f(x + 1), \, \forall x \in \mathbb{R} \land \displaystyle \lim_{x \to \infty} f(x) = L \Rightarrow f(x) = L, \forall x \in \mathbb{R}$ , is my proof valid? Assume $ \exists x \in \mathbb{R} $ such that $ f\left(x\right) \neq L $ . Choose $ \epsilon = \left|f\left(x\right) - L\right| \neq 0 $ , because the absolute value function is a metric. Therefore, $ \exists N \gt 0 : \forall z \gt N, \left|f\left(z\right) - L\right| \lt \epsilon $ . Choose $ z = x + \left|\lceil Nx \rceil\right| \gt N \Rightarrow \left|f\left(z\right) - L\right| =  \left|f\left(x\right) - L \right| \lt \epsilon$ - contradiction.","I want to prove that is continuous , is my proof valid? Assume such that . Choose , because the absolute value function is a metric. Therefore, . Choose - contradiction."," f  \,  \land f(x) = f(x + 1), \, \forall x \in \mathbb{R} \land \displaystyle \lim_{x \to \infty} f(x) = L \Rightarrow f(x) = L, \forall x \in \mathbb{R}  \exists x \in \mathbb{R}   f\left(x\right) \neq L   \epsilon = \left|f\left(x\right) - L\right| \neq 0   \exists N \gt 0 : \forall z \gt N, \left|f\left(z\right) - L\right| \lt \epsilon   z = x + \left|\lceil Nx \rceil\right| \gt N \Rightarrow \left|f\left(z\right) - L\right| =  \left|f\left(x\right) - L \right| \lt \epsilon","['real-analysis', 'calculus', 'limits']"
86,Evaluate $\displaystyle\lim_{n\to\infty} \frac{a_n}{n^2}$,Evaluate,\displaystyle\lim_{n\to\infty} \frac{a_n}{n^2},"Problem : Let $f\colon[0,\infty)\to\mathbb{R}\quad f(x)=\sin (\pi x)$ . For each $n\in\mathbb{N}$ , let $x_1,x_2,\cdots,x_k$ denotes roots of $$ f(x)=\frac{x}{2n}$$ Evaluate $$\lim_{n\to\infty} \frac{a_n}{n^2}$$ where $a_n$ is sum of all $x_1, x_2, \cdots, x_k$ . My Attempt I think $\frac{a_n}{n^2}\to 2$ . From observation, the equation $$f(x)=\frac{x}{2n}$$ has $2n$ distinct real roots for each $n\in\mathbb{N}$ . Rewrite this roots as $x_1, x_2, \cdots, x_{2n}$ . If $n\to\infty$ , $\frac{x}{2n}\to 0$ . This means the equation will be changed to : $f(x)=0$ under $n\to\infty$ . And, real roots of $f(x)=0$ is well known : $0, 1, 2, \cdots$ So I think $$\left\{x_1,x_2,\cdots,x_{2n}\right\}\to\left\{0,1,\cdots,2n-1\right\}.$$ This means $$a_n \approx \sum_{k=0}^{2n-1}k = n(2n-1)$$ And finally $$\lim_{n\to\infty} \frac{a_n}{n^2} \approx \lim_{n\to\infty}\frac{n(2n-1)}{n^2}=2$$ I have two questions : Is this valid? Is there any nice approach like squeeze theorem? (I think it's hard to represent $x_n$ with $\arcsin$ , so I think squeeze theorem will be the best approach.)","Problem : Let . For each , let denotes roots of Evaluate where is sum of all . My Attempt I think . From observation, the equation has distinct real roots for each . Rewrite this roots as . If , . This means the equation will be changed to : under . And, real roots of is well known : So I think This means And finally I have two questions : Is this valid? Is there any nice approach like squeeze theorem? (I think it's hard to represent with , so I think squeeze theorem will be the best approach.)","f\colon[0,\infty)\to\mathbb{R}\quad f(x)=\sin (\pi x) n\in\mathbb{N} x_1,x_2,\cdots,x_k  f(x)=\frac{x}{2n} \lim_{n\to\infty} \frac{a_n}{n^2} a_n x_1, x_2, \cdots, x_k \frac{a_n}{n^2}\to 2 f(x)=\frac{x}{2n} 2n n\in\mathbb{N} x_1, x_2, \cdots, x_{2n} n\to\infty \frac{x}{2n}\to 0 f(x)=0 n\to\infty f(x)=0 0, 1, 2, \cdots \left\{x_1,x_2,\cdots,x_{2n}\right\}\to\left\{0,1,\cdots,2n-1\right\}. a_n \approx \sum_{k=0}^{2n-1}k = n(2n-1) \lim_{n\to\infty} \frac{a_n}{n^2} \approx \lim_{n\to\infty}\frac{n(2n-1)}{n^2}=2 x_n \arcsin","['calculus', 'limits', 'solution-verification']"
87,Prove that $\sum_{k=1}^{n} a_k \left(1-\frac{k^2}{n^2}\right) \to 1$,Prove that,\sum_{k=1}^{n} a_k \left(1-\frac{k^2}{n^2}\right) \to 1,"Problem : Given $a_n$ which satisfies $\displaystyle\sum a_n = 1$ , prove that $$\lim_{n\to\infty}\sum_{k=1}^na_k\left(1-\frac{k^2}{n^2}\right) = 1.$$ My Attempt From $\displaystyle\sum a_n = 1$ , I changed this problem to show $\displaystyle\lim\sum\frac{a_kk^2}{n^2}\to 0$ . From given condition, I know that $a_n \to 0$ and it means that $a_n$ is bounded. So, for arbitrary large $n$ , $|a_n|<1$ , and I tried to bound an original sum $$\left|\sum\frac{a_kk^2}{n^2}\right|\le \frac{1}{n^2}\left|\sum a_kk^2\right|\le \frac{1}{n^2}\sum k^2$$ but I failed because the right term goes to infinity. Thanks for help.","Problem : Given which satisfies , prove that My Attempt From , I changed this problem to show . From given condition, I know that and it means that is bounded. So, for arbitrary large , , and I tried to bound an original sum but I failed because the right term goes to infinity. Thanks for help.",a_n \displaystyle\sum a_n = 1 \lim_{n\to\infty}\sum_{k=1}^na_k\left(1-\frac{k^2}{n^2}\right) = 1. \displaystyle\sum a_n = 1 \displaystyle\lim\sum\frac{a_kk^2}{n^2}\to 0 a_n \to 0 a_n n |a_n|<1 \left|\sum\frac{a_kk^2}{n^2}\right|\le \frac{1}{n^2}\left|\sum a_kk^2\right|\le \frac{1}{n^2}\sum k^2,"['limits', 'summation']"
88,"Find $\lim\limits_{(x,y)\to(0,0)}\frac{x^6+y^6}{x^3+y^3}$ using the $\epsilon-\delta$ definition?",Find  using the  definition?,"\lim\limits_{(x,y)\to(0,0)}\frac{x^6+y^6}{x^3+y^3} \epsilon-\delta","My textbook asks the question $$f(x,y) = \frac{x^6+y^6}{x^3+y^3}$$ Does $f(x,y)$ have a limit as $(x,y) \rightarrow (0,0)$ ? I used polar coordinates instead of solving explicitly in $\mathbb R^2 $ , and it went as the following: $$ x = r \cos \theta, \qquad y = r\sin\theta $$ Hence, $$\lim_{(x,y) \to (0,0)} \frac{x^6+y^6}{x^3+y^3} = \lim_{r \to 0}\frac{{r^6\cos^6\theta + r^6\sin^6\theta}}{r^3\cos^3\theta + r^3\sin^3\theta}$$ This simplifies to, $$ \lim_{r \to 0} \frac{r^3({\cos^6\theta + \sin^6\theta})}{\cos^3\theta + \sin^3\theta}$$ Now from the above, we find that as r→ $0$ the limit is $0$ . So now i have got $0$ as a possible limit.I have tried to procceed further using $\epsilon-\delta$ definition but I cannot get anywhere. Any thoughts on how to prove that $0$ is in fact the limit?","My textbook asks the question Does have a limit as ? I used polar coordinates instead of solving explicitly in , and it went as the following: Hence, This simplifies to, Now from the above, we find that as r→ the limit is . So now i have got as a possible limit.I have tried to procceed further using definition but I cannot get anywhere. Any thoughts on how to prove that is in fact the limit?","f(x,y) = \frac{x^6+y^6}{x^3+y^3} f(x,y) (x,y) \rightarrow (0,0) \mathbb R^2   x = r \cos \theta, \qquad y = r\sin\theta  \lim_{(x,y) \to (0,0)} \frac{x^6+y^6}{x^3+y^3} = \lim_{r \to 0}\frac{{r^6\cos^6\theta + r^6\sin^6\theta}}{r^3\cos^3\theta + r^3\sin^3\theta}  \lim_{r \to 0} \frac{r^3({\cos^6\theta + \sin^6\theta})}{\cos^3\theta + \sin^3\theta} 0 0 0 \epsilon-\delta 0","['limits', 'multivariable-calculus', 'polar-coordinates', 'epsilon-delta']"
89,"let curve $y=\dfrac{x^2}{4}$ and point $F(0,1)$. Let points $A_1(x_1,y_1),A_2(x_2,y_2),...,A_n(x_n,y_n)$ be n points on curve such that",let curve  and point . Let points  be n points on curve such that,"y=\dfrac{x^2}{4} F(0,1) A_1(x_1,y_1),A_2(x_2,y_2),...,A_n(x_n,y_n)","let curve $y=\dfrac{x^2}{4}$ and point $F(0,1)$ . Let points $A_1(x_1,y_1),A_2(x_2,y_2),...,A_n(x_n,y_n)$ be n points on curve such that $x_k>0$ and $\angle{OFA_k}=\dfrac{k\pi}{2n}, (k=1,2,3....,n)$ then find $\lim_{n\to \infty}\dfrac{1}{n} \sum_{k=1}^{n} FA_k$ I am not able to get how to solve. Can anyone help. Thanks",let curve and point . Let points be n points on curve such that and then find I am not able to get how to solve. Can anyone help. Thanks,"y=\dfrac{x^2}{4} F(0,1) A_1(x_1,y_1),A_2(x_2,y_2),...,A_n(x_n,y_n) x_k>0 \angle{OFA_k}=\dfrac{k\pi}{2n}, (k=1,2,3....,n) \lim_{n\to \infty}\dfrac{1}{n} \sum_{k=1}^{n} FA_k","['limits', 'conic-sections']"
90,I've recently discovered DCT and I'm wondering how one would solve this limit : [duplicate],I've recently discovered DCT and I'm wondering how one would solve this limit : [duplicate],,"This question already has answers here : Evaluate $\lim_{n\to \infty}n\int_2^e{(\ln x)^n}dx$ (2 answers) Closed 2 years ago . The limit: $$ \lim_{n \to \infty}  n\int_{2}^{e} [\ln(x)]^n \mathrm dx $$ The book from where I took this exercise offered these as possible results : $e$ $0$ $1$ $\ln(2)$ infinity I was able to pinpoint the solution ( $e$ ) by substituting $x=e^t$ and then building the integral from $\ln(2)\le t\le1$ , and in the end reaching to $2\le n\int_{2}^{e} [\ln(x)]^n \mathrm dx \le e$ . The only solution possible, considering the answers the book gave, was $e$ . The problem is, I still didn't solve the problem. I am still not sure how to reach the correct result so in my attempt to find a way, I discovered DCT. I tried to understand as much as I can but there are still a lot of empty gaps. In order to use DCT, I tried bringing the $n$ inside the integral and then use the substitution $t=(\ln(x))^n$ which gives us $ \lim_{n \to \infty}  \int_{(\ln(2))^n}^{1} t^\frac{1}{n} \mathrm dt $ . Because the lower bound depends on $n$ , I basically got stuck. So with all of this said, is there a way to use DCT to solve this limit?","This question already has answers here : Evaluate $\lim_{n\to \infty}n\int_2^e{(\ln x)^n}dx$ (2 answers) Closed 2 years ago . The limit: The book from where I took this exercise offered these as possible results : infinity I was able to pinpoint the solution ( ) by substituting and then building the integral from , and in the end reaching to . The only solution possible, considering the answers the book gave, was . The problem is, I still didn't solve the problem. I am still not sure how to reach the correct result so in my attempt to find a way, I discovered DCT. I tried to understand as much as I can but there are still a lot of empty gaps. In order to use DCT, I tried bringing the inside the integral and then use the substitution which gives us . Because the lower bound depends on , I basically got stuck. So with all of this said, is there a way to use DCT to solve this limit?", \lim_{n \to \infty}  n\int_{2}^{e} [\ln(x)]^n \mathrm dx  e 0 1 \ln(2) e x=e^t \ln(2)\le t\le1 2\le n\int_{2}^{e} [\ln(x)]^n \mathrm dx \le e e n t=(\ln(x))^n  \lim_{n \to \infty}  \int_{(\ln(2))^n}^{1} t^\frac{1}{n} \mathrm dt  n,"['integration', 'limits', 'definite-integrals']"
91,Testing whether limit exists,Testing whether limit exists,,"I have the following function: $$f:\mathbb{R^{2}}\to \mathbb{R},\;\;f(x,y):=\frac{x-y}{x^{2}+y^{2}},\;\;(x,y)\neq (0,0).$$ I would like to test whether its limit exists at $(0,0)$ . $$\lim_{x\to 0}\left(\lim_{y\to 0}\frac{x-y}{x^{2}+y^{2}}\right) = \lim_{x\to 0}\left(\frac{x}{x^{2}}\right) = \lim_{x\to 0}\frac{1}{x} = +\infty$$ $$\lim_{y \to 0}\left(\lim_{x\to 0}\frac{x-y}{x^{2}+y^{2}}\right) = -\lim_{y\to 0}\left(\frac{y}{y^{2}}\right) = -\lim_{y\to 0}\frac{1}{y} = -\infty$$ The divergent nature of the two limits indicates that the limit $$\lim_{(x,y)\to (0,0)}f(x,y)$$ does not exist. It would have been sufficient to show that at least one of the first two limits diverge.",I have the following function: I would like to test whether its limit exists at . The divergent nature of the two limits indicates that the limit does not exist. It would have been sufficient to show that at least one of the first two limits diverge.,"f:\mathbb{R^{2}}\to \mathbb{R},\;\;f(x,y):=\frac{x-y}{x^{2}+y^{2}},\;\;(x,y)\neq (0,0). (0,0) \lim_{x\to 0}\left(\lim_{y\to 0}\frac{x-y}{x^{2}+y^{2}}\right) = \lim_{x\to 0}\left(\frac{x}{x^{2}}\right) = \lim_{x\to 0}\frac{1}{x} = +\infty \lim_{y
\to 0}\left(\lim_{x\to 0}\frac{x-y}{x^{2}+y^{2}}\right) = -\lim_{y\to 0}\left(\frac{y}{y^{2}}\right) = -\lim_{y\to 0}\frac{1}{y} = -\infty \lim_{(x,y)\to (0,0)}f(x,y)","['real-analysis', 'limits', 'analysis', 'multivariable-calculus', 'solution-verification']"
92,Asymptotic behavior of integral,Asymptotic behavior of integral,,"I have the following expression: \begin{equation} \left.\frac{\partial U}{\partial(B/J)}\right|_{JB} = -N \sqrt{JB} \int_{-\pi}^\pi \frac{\text{d}k}{2\pi} \frac{B/J - J/B}{\sqrt{4\sin^2k/2 + (J/B - B/J)^2}} \end{equation} I am looking at the asymptotic behavior as $B/J - J/B \to 0$ . To me, this seems equivalent to: \begin{equation} \lim_{X \to 0}\left[-N \sqrt{Y} \int_{-\pi}^\pi \frac{\text{d}k}{2\pi} \frac{X}{\sqrt{4\sin^2k/2 + X^2}}\right] = \left[-N \sqrt{Y} \int_{-\pi}^\pi \frac{\text{d}k}{2\pi} \frac{0}{\sqrt{4\sin^2k/2}}\right] = 0 \end{equation} But I am doubting the results because $k=0$ is within the range of integration. What is the appropriate way to look at the asymptotic behavior?","I have the following expression: I am looking at the asymptotic behavior as . To me, this seems equivalent to: But I am doubting the results because is within the range of integration. What is the appropriate way to look at the asymptotic behavior?","\begin{equation}
\left.\frac{\partial U}{\partial(B/J)}\right|_{JB} = -N \sqrt{JB} \int_{-\pi}^\pi \frac{\text{d}k}{2\pi} \frac{B/J - J/B}{\sqrt{4\sin^2k/2 + (J/B - B/J)^2}}
\end{equation} B/J - J/B \to 0 \begin{equation}
\lim_{X \to 0}\left[-N \sqrt{Y} \int_{-\pi}^\pi \frac{\text{d}k}{2\pi} \frac{X}{\sqrt{4\sin^2k/2 + X^2}}\right] = \left[-N \sqrt{Y} \int_{-\pi}^\pi \frac{\text{d}k}{2\pi} \frac{0}{\sqrt{4\sin^2k/2}}\right] = 0
\end{equation} k=0","['integration', 'limits']"
93,Limit of $\sum_{i=1}^k a_iz_i^n$ as $n\to\infty$ where $|z_i| = 1$,Limit of  as  where,\sum_{i=1}^k a_iz_i^n n\to\infty |z_i| = 1,"Let $z_1, ..., z_k \in S^1\setminus\{1\}$ , $a_1, ..., a_k\in\mathbb{C}\setminus \{0\}$ with $z_i\neq z_j$ for $i\neq j$ , and consider the sequence of the form $b_n = \sum_{i=1}^k a_iz_i^n$ . Is it true that $\lim_{n\to\infty} b_n$ can never exist? I think that if we assume $z_1, ..., z_k$ are all roots of unity, then $b_n$ must be non-constant and periodic, so the limit doesn't exist. If $z$ isn't a root of unity, it's known that $\{z^n : n\in\mathbb{N}\}$ is dense in $S^1$ , so I think that in this case the limit also won't exist, but I'm not sure if the density of two such terms won't ""cancel out"".","Let , with for , and consider the sequence of the form . Is it true that can never exist? I think that if we assume are all roots of unity, then must be non-constant and periodic, so the limit doesn't exist. If isn't a root of unity, it's known that is dense in , so I think that in this case the limit also won't exist, but I'm not sure if the density of two such terms won't ""cancel out"".","z_1, ..., z_k \in S^1\setminus\{1\} a_1, ..., a_k\in\mathbb{C}\setminus \{0\} z_i\neq z_j i\neq j b_n = \sum_{i=1}^k a_iz_i^n \lim_{n\to\infty} b_n z_1, ..., z_k b_n z \{z^n : n\in\mathbb{N}\} S^1","['sequences-and-series', 'limits', 'complex-numbers']"
94,"Partial derivative of $f(x, y) = x ^ {x ^ {x ^ {x ^ y}}} + \ln(x)[\tan^{-1} (\tan^{-1}(\tan^ {-1}(\sin(\cos xy)-\ln(x+y)))]$",Partial derivative of,"f(x, y) = x ^ {x ^ {x ^ {x ^ y}}} + \ln(x)[\tan^{-1} (\tan^{-1}(\tan^ {-1}(\sin(\cos xy)-\ln(x+y)))]","If $$f(x, y) = x ^ {x ^ {x ^ {x ^ y}}} + \ln(x)[\tan^{-1} (\tan^{-1}(\tan^ {-1}(\sin(\cos xy)-\ln(x+y)))]$$ Then what are the values of partial derivative $f_x(1,2)$ and $f_x(1,5)?$ I tried my best but couldn't fight with such a difficult calculations involved in finding partial derivative. Is there any short and easy approach to find partial derivative of such a typical function?",If Then what are the values of partial derivative and I tried my best but couldn't fight with such a difficult calculations involved in finding partial derivative. Is there any short and easy approach to find partial derivative of such a typical function?,"f(x, y) = x ^ {x ^ {x ^ {x ^ y}}} + \ln(x)[\tan^{-1} (\tan^{-1}(\tan^ {-1}(\sin(\cos xy)-\ln(x+y)))] f_x(1,2) f_x(1,5)?","['calculus', 'limits', 'functions', 'derivatives', 'partial-derivative']"
95,I need help with this exercise on limits at infinity.,I need help with this exercise on limits at infinity.,,"The exercise I propose is the following. Let $a_0, a_1, ...., a_p$ be real numbers whose sum is equal to zero. Justify that $\lim \limits_{n \to \infty} (a_0\sqrt{n}+a_1\sqrt{n+1}+a_2\sqrt{n+2}+...+a_p\sqrt{n+p}) = 0$ I tried to get a common factor $\sqrt{n}$ and then try to group the terms together $a_0, a_1, a_p$ to add them. However, I can't come up with anything concrete to give me an idea of the limit. Any help?","The exercise I propose is the following. Let be real numbers whose sum is equal to zero. Justify that I tried to get a common factor and then try to group the terms together to add them. However, I can't come up with anything concrete to give me an idea of the limit. Any help?","a_0, a_1, ...., a_p \lim \limits_{n \to \infty} (a_0\sqrt{n}+a_1\sqrt{n+1}+a_2\sqrt{n+2}+...+a_p\sqrt{n+p}) = 0 \sqrt{n} a_0, a_1, a_p","['calculus', 'sequences-and-series', 'limits']"
96,Evaluate $\lim_{n \to \infty} \prod_{k=n+1}^{2n} k^{1/k}$,Evaluate,\lim_{n \to \infty} \prod_{k=n+1}^{2n} k^{1/k},"Problem: evaluate $\lim_{n \to \infty} \prod_{k=n+1}^{2n} k^{1/k}$ . My work: since $n+1 \le k \le 2n$ , it is $\frac{1}{2n} \le \frac{1}{k} \le \frac{1}{n+1}$ . Since $k \ge 1$ , the exponential with base $k$ is increasing and so, from $\frac{1}{2n} \le \frac{1}{k} \le \frac{1}{n+1}$ , it follows that $k^{1/2n} \le k^{1/k} \le k^{1/(n+1)}$ . Since the $\alpha$ -th ( $\alpha$ real) power is increasing when its base is nonnegative and $k \ge 1$ , it follows that $(n+1)^{1/2n} \le k^{1/2n}$ and $k^{1/(n+1)} \le (2n)^{1/(n+1)}$ ; in conclusion, it is $(n+1)^{1/2n} \le k^{1/k}\le(2n)^{1/(n+1)}$ for any $n+1 \le k \le 2n$ and for any $n\in\mathbb{N}$ . Hence $$\lim_{n \to \infty} \prod_{k=n+1}^{2n} k^{1/k} \ge \lim_{n \to \infty} \prod_{k=n+1}^{2n} (n+1)^{1/2n}=\lim_{n\to\infty}\sqrt{n+1}=\infty$$ $$\implies \lim_{n \to \infty} \prod_{k=n+1}^{2n} k^{1/k}=\infty$$ Is this correct? I'm not sure that what I've written about the estimation I get using the fact that the exponential and the real power are increasing.","Problem: evaluate . My work: since , it is . Since , the exponential with base is increasing and so, from , it follows that . Since the -th ( real) power is increasing when its base is nonnegative and , it follows that and ; in conclusion, it is for any and for any . Hence Is this correct? I'm not sure that what I've written about the estimation I get using the fact that the exponential and the real power are increasing.",\lim_{n \to \infty} \prod_{k=n+1}^{2n} k^{1/k} n+1 \le k \le 2n \frac{1}{2n} \le \frac{1}{k} \le \frac{1}{n+1} k \ge 1 k \frac{1}{2n} \le \frac{1}{k} \le \frac{1}{n+1} k^{1/2n} \le k^{1/k} \le k^{1/(n+1)} \alpha \alpha k \ge 1 (n+1)^{1/2n} \le k^{1/2n} k^{1/(n+1)} \le (2n)^{1/(n+1)} (n+1)^{1/2n} \le k^{1/k}\le(2n)^{1/(n+1)} n+1 \le k \le 2n n\in\mathbb{N} \lim_{n \to \infty} \prod_{k=n+1}^{2n} k^{1/k} \ge \lim_{n \to \infty} \prod_{k=n+1}^{2n} (n+1)^{1/2n}=\lim_{n\to\infty}\sqrt{n+1}=\infty \implies \lim_{n \to \infty} \prod_{k=n+1}^{2n} k^{1/k}=\infty,"['real-analysis', 'limits', 'solution-verification', 'infinite-product']"
97,Determining $A$ such that $\lim \limits_{x \to\infty }(\sqrt{Ax^2+2x}−5x)$ exists and is finite,Determining  such that  exists and is finite,A \lim \limits_{x \to\infty }(\sqrt{Ax^2+2x}−5x),"Determine the value of the real number $A$ so that a finite limit exists, and then compute the limit: $$\lim \limits_{x \to \infty}(\sqrt{Ax^2+2x}−5x)$$ Where would I start with this? I thought I could factor the highest degree variable from each term but that lets any real A work with it.","Determine the value of the real number so that a finite limit exists, and then compute the limit: Where would I start with this? I thought I could factor the highest degree variable from each term but that lets any real A work with it.",A \lim \limits_{x \to \infty}(\sqrt{Ax^2+2x}−5x),"['calculus', 'limits']"
98,How to solve limit of $(n-1)\left[1-\left(1-\left(\frac{\lambda}{n}\right)^2\right)^{n-1}\right]$?,How to solve limit of ?,(n-1)\left[1-\left(1-\left(\frac{\lambda}{n}\right)^2\right)^{n-1}\right],"I'm trying to find the solution to the following limit: $$ \lim_{n\rightarrow\infty}(n-1)\left[1-\left(1-\left(\frac{\lambda}{n}\right)^2\right)^{n-1}\right]\ \mathrm{.} $$ I've tried decomposing the limit and writing it as $$ \lim_{n\rightarrow\infty}(n-1)\left[1-\left(1-\frac{\lambda}{n}\right)^{n}\left(1+\frac{\lambda}{n}\right)^n\left(1-\left(\frac{\lambda}{n}\right)^2\right)^{-1}\right]\ \mathrm{,} $$ so I can use that $$ \lim_{n\rightarrow\infty}\left(1\pm\frac{\lambda}{n}\right)^{n}=e^{\pm\lambda}\mathrm{,} $$ but the problem seems to be that I cannot split the products or the sums within the limit into separate limits as $\lim_{n\rightarrow\infty}(n-1)$ does not converge. The answer should be $\lambda^2$ (this is also what Wolfram gives me), but I don't see a way to get to that answer myself. I'd rather not resort to the Laurent series, as this is also a nightmare to compute. Can anyone help?","I'm trying to find the solution to the following limit: I've tried decomposing the limit and writing it as so I can use that but the problem seems to be that I cannot split the products or the sums within the limit into separate limits as does not converge. The answer should be (this is also what Wolfram gives me), but I don't see a way to get to that answer myself. I'd rather not resort to the Laurent series, as this is also a nightmare to compute. Can anyone help?","
\lim_{n\rightarrow\infty}(n-1)\left[1-\left(1-\left(\frac{\lambda}{n}\right)^2\right)^{n-1}\right]\ \mathrm{.}
 
\lim_{n\rightarrow\infty}(n-1)\left[1-\left(1-\frac{\lambda}{n}\right)^{n}\left(1+\frac{\lambda}{n}\right)^n\left(1-\left(\frac{\lambda}{n}\right)^2\right)^{-1}\right]\ \mathrm{,}
 
\lim_{n\rightarrow\infty}\left(1\pm\frac{\lambda}{n}\right)^{n}=e^{\pm\lambda}\mathrm{,}
 \lim_{n\rightarrow\infty}(n-1) \lambda^2","['calculus', 'limits']"
99,"Solution Verification - find values of $a,b,c$ such that $\lim_{h\to 0} \frac{a\cdot f(x+h)+b\cdot f(x) + c\cdot f(x-h)}{h^2}$ exists",Solution Verification - find values of  such that  exists,"a,b,c \lim_{h\to 0} \frac{a\cdot f(x+h)+b\cdot f(x) + c\cdot f(x-h)}{h^2}","Let $f$ be any twice differentiable function. Find values of $a,b,c$ such that $$\lim_{h\to 0} \frac{a\cdot f(x+h)+b\cdot f(x) + c\cdot f(x-h)}{h^2}\tag{1}$$ exists for all $x$ . For $(1)$ to exist, we require $$\lim_{h\to 0}  \left(a\cdot f(x+h)+b\cdot f(x) + c\cdot f(x-h)\right)=0\tag{$*$}$$ (I've used this statement intuitively. I do not have a rigorous/proper proof for this statement.) Which gives $a+b+c=0$ . Hence applying L'Hôpital's rule on $(1)$ , we get $$\lim_{h\to 0} \frac{a\cdot f(x+h)+b\cdot f(x) + c\cdot f(x-h)}{h^2}=\lim_{h\to 0} \frac{a\cdot f'(x+h) - c\cdot f'(x-h)}{2h}\tag{2}$$ For $(2)$ to exist, we require $$\lim_{h\to 0} \left(a\cdot f'(x+h) - c\cdot f'(x-h)\right)=0\tag{$**$}$$ (I've used this statement intuitively. I do not have a rigorous/proper proof for this statement.) Which gives $a-c=0$ . Hence applying L'Hôpital's rule on $(2)$ , we get $$\lim_{h\to 0} \frac{a\cdot f'(x+h) - c\cdot f'(x-h)}{2h}=\lim_{h\to 0} \frac{a\cdot f''(x+h) + c\cdot f''(x-h)}{2}  \tag{3}$$ $$\implies\lim_{h\to 0} \frac{a\cdot f''(x+h) + c\cdot f''(x-h)}{2} = \frac{a+ c}{2}\cdot f''(x) \tag{4}$$ My questions are: Is the above solution correct? Can you provide a proper proof for the two statements that I have marked with asterisks? i.e  if it is given that $\lim_{x\to 0}\phi_2(x)=0$ , then for $\lim_{x\to 0}\frac{\phi_1(x)}{\phi_2(x)}$ to exist is it necessary that $\lim_{x\to 0}\phi_1(x)=0$ Edit:-","Let be any twice differentiable function. Find values of such that exists for all . For to exist, we require (I've used this statement intuitively. I do not have a rigorous/proper proof for this statement.) Which gives . Hence applying L'Hôpital's rule on , we get For to exist, we require (I've used this statement intuitively. I do not have a rigorous/proper proof for this statement.) Which gives . Hence applying L'Hôpital's rule on , we get My questions are: Is the above solution correct? Can you provide a proper proof for the two statements that I have marked with asterisks? i.e  if it is given that , then for to exist is it necessary that Edit:-","f a,b,c \lim_{h\to 0} \frac{a\cdot f(x+h)+b\cdot f(x) + c\cdot f(x-h)}{h^2}\tag{1} x (1) \lim_{h\to 0} 
\left(a\cdot f(x+h)+b\cdot f(x) + c\cdot f(x-h)\right)=0\tag{*} a+b+c=0 (1) \lim_{h\to 0} \frac{a\cdot f(x+h)+b\cdot f(x) + c\cdot f(x-h)}{h^2}=\lim_{h\to 0} \frac{a\cdot f'(x+h) - c\cdot f'(x-h)}{2h}\tag{2} (2) \lim_{h\to 0} \left(a\cdot f'(x+h) - c\cdot f'(x-h)\right)=0\tag{**} a-c=0 (2) \lim_{h\to 0} \frac{a\cdot f'(x+h) - c\cdot f'(x-h)}{2h}=\lim_{h\to 0} \frac{a\cdot f''(x+h) + c\cdot f''(x-h)}{2}  \tag{3} \implies\lim_{h\to 0} \frac{a\cdot f''(x+h) + c\cdot f''(x-h)}{2} = \frac{a+ c}{2}\cdot f''(x) \tag{4} \lim_{x\to 0}\phi_2(x)=0 \lim_{x\to 0}\frac{\phi_1(x)}{\phi_2(x)} \lim_{x\to 0}\phi_1(x)=0","['limits', 'derivatives']"
