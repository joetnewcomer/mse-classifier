,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Gambling device: What's my probability to win at 5 dollars before going bankrupt?,Gambling device: What's my probability to win at 5 dollars before going bankrupt?,,"I'm going gambling and I have ten dollars. I have a gambling device that costs 1 dollar per game to play. I win with a probability of $\frac{1}{5}$ and each win gives me back four dollars (to a net profit of three) while lost games give nothing back for a net loss of one dollar. I'm going to play the game until I go bankrupt (net wins and net losses sum up to $-10$ or less) or I make a profit of five dollars (net wins and net losses sum up to $5$ or more). I want to know the probability of the game ending in me winning. At first I tried analyzing this using the binomial distribution but I'd need to know the total number of games in advance, but it is unbounded. Ideas on how to approach this are warmly welcome.","I'm going gambling and I have ten dollars. I have a gambling device that costs 1 dollar per game to play. I win with a probability of $\frac{1}{5}$ and each win gives me back four dollars (to a net profit of three) while lost games give nothing back for a net loss of one dollar. I'm going to play the game until I go bankrupt (net wins and net losses sum up to $-10$ or less) or I make a profit of five dollars (net wins and net losses sum up to $5$ or more). I want to know the probability of the game ending in me winning. At first I tried analyzing this using the binomial distribution but I'd need to know the total number of games in advance, but it is unbounded. Ideas on how to approach this are warmly welcome.",,['probability']
1,Find the moment generating function of the sum of exponential random variables $S=X_1+X_2+X_3+X_4$,Find the moment generating function of the sum of exponential random variables,S=X_1+X_2+X_3+X_4,"Let $X_1+X_2+X_3+X_4$ be iid exponential random variables with parameter λ, and $S=X_1+X_2+X_3+X_4$ S follows the gamma distribution with parameters $\lambda$ and $r=4$. We know that an exponential random variable X with parameter $\lambda$ has moment generating function $E[e^{tX}]=\frac{\lambda}{\lambda-t}$ if $t<\lambda$ and $+\infty$ if $t \geq \lambda$ a) find MGF $M_S(t)$ (don't forget to declare the domain) b) find the 1st and 2nd moment of S, then find the variance I know that for b, you can just find $M_S'(0)$ and $M_S''(0)$ but for a, I'm not sure on how to get started on finding the moment generating function","Let $X_1+X_2+X_3+X_4$ be iid exponential random variables with parameter λ, and $S=X_1+X_2+X_3+X_4$ S follows the gamma distribution with parameters $\lambda$ and $r=4$. We know that an exponential random variable X with parameter $\lambda$ has moment generating function $E[e^{tX}]=\frac{\lambda}{\lambda-t}$ if $t<\lambda$ and $+\infty$ if $t \geq \lambda$ a) find MGF $M_S(t)$ (don't forget to declare the domain) b) find the 1st and 2nd moment of S, then find the variance I know that for b, you can just find $M_S'(0)$ and $M_S''(0)$ but for a, I'm not sure on how to get started on finding the moment generating function",,"['probability', 'moment-generating-functions']"
2,"Sheldon Ross vs My TA, what answer is wrong?","Sheldon Ross vs My TA, what answer is wrong?",,"I have the solution of this problem, 1) The game of Clue involves 6 suspects, 6 weapons, and 9 rooms. One of each is chosen randomly and the object of the game is to guess the chosen three. In one version of the game, the selection is made and then each of the players is randomly given three of the remaining cards. Let S, W, and R be, respectively, the number of suspects, weapons, and rooms in the set of three cards given to a specified player. Also, let X denote the number of solutions that are possible after that player observes his or her three cards. I need to find: (c) Find E[X]. Solution given by my TA: Solution given by Sheldon Ross - A first Course in Probability (Seven Edition). My question, is of what answer is wrong and what answer is correct, and why?","I have the solution of this problem, 1) The game of Clue involves 6 suspects, 6 weapons, and 9 rooms. One of each is chosen randomly and the object of the game is to guess the chosen three. In one version of the game, the selection is made and then each of the players is randomly given three of the remaining cards. Let S, W, and R be, respectively, the number of suspects, weapons, and rooms in the set of three cards given to a specified player. Also, let X denote the number of solutions that are possible after that player observes his or her three cards. I need to find: (c) Find E[X]. Solution given by my TA: Solution given by Sheldon Ross - A first Course in Probability (Seven Edition). My question, is of what answer is wrong and what answer is correct, and why?",,"['probability', 'probability-theory', 'statistics']"
3,"An unfair ""fair game.""","An unfair ""fair game.""",,"This is problem 2.2.8 from Durrett's Probability Theory and Examples 4th edition, I am using the version of this book that can be found on his website. Let $p_k=\frac{1}{2^k k (k+1)}, \ k=1,2,\dots$ and $p_0=1-\sum_{k\geq 1}p_k.$   $$ \sum\limits_{k=1}^\infty 2^k p_k = \left(1-\frac{1}{2}\right)+\left(\frac{1}{2}- \frac{1}{3}\right)+\dots = 1 $$   so if we let $X_1, X_2, \dots$ be i.i.d. with $P(X_n = -1)=p_0$ and $P(X_n=2^k-1)=p_k$ for $k\geq 1$ then $E(X_n)=0.$ Let $S_n=X_1 + \dots + X_n.$ Use the Weak Law for Triangular Arrays to conclude that    $$ \frac{S_n}{n/\log_2(n)} \rightarrow -1 \ \text{in probability}. $$ The weak law of triangular arrays is as follows: For each $n$ let $X_{n,k}, 1\leq k \leq n$, be independent. Let $b_n>0$ with $b_n \rightarrow \infty$, and let $\overline{X_{n,k}}=X_{n,k}1_{(|X_{n,k}|\leq b_n)}. $ Suppose that as $n\rightarrow \infty$ (i) $\sum_{k=1}^n P(|X_{n,k}|>b_n) \rightarrow 0$ (ii) $b_n^{-2} \sum_{k=1}^n E(\overline{X_{n,k}}^2)\rightarrow 0.$ If we let $S_n=X_{n,1}+\dots +X_{n,n}$ and put $a_n=\sum_{k=1}^n E(\overline{X_{n,k}})$ then   $$\frac{S_n-a_n}{b_n}\rightarrow 0 \ \text{in probability}. $$ I am letting $b_n=n/\log_2(n)$. Durrett suggests using something else, but this seems more natural to me. I have that all the $X_n$ are independent and that $b_n\rightarrow \infty$ as $n\rightarrow 0$. What I am having a hard time checking is why (i) and (ii) hold for our $X_n$. I see that $X_n=-1,1,3,7,\dots$, in case that helps. I appreciate any help.","This is problem 2.2.8 from Durrett's Probability Theory and Examples 4th edition, I am using the version of this book that can be found on his website. Let $p_k=\frac{1}{2^k k (k+1)}, \ k=1,2,\dots$ and $p_0=1-\sum_{k\geq 1}p_k.$   $$ \sum\limits_{k=1}^\infty 2^k p_k = \left(1-\frac{1}{2}\right)+\left(\frac{1}{2}- \frac{1}{3}\right)+\dots = 1 $$   so if we let $X_1, X_2, \dots$ be i.i.d. with $P(X_n = -1)=p_0$ and $P(X_n=2^k-1)=p_k$ for $k\geq 1$ then $E(X_n)=0.$ Let $S_n=X_1 + \dots + X_n.$ Use the Weak Law for Triangular Arrays to conclude that    $$ \frac{S_n}{n/\log_2(n)} \rightarrow -1 \ \text{in probability}. $$ The weak law of triangular arrays is as follows: For each $n$ let $X_{n,k}, 1\leq k \leq n$, be independent. Let $b_n>0$ with $b_n \rightarrow \infty$, and let $\overline{X_{n,k}}=X_{n,k}1_{(|X_{n,k}|\leq b_n)}. $ Suppose that as $n\rightarrow \infty$ (i) $\sum_{k=1}^n P(|X_{n,k}|>b_n) \rightarrow 0$ (ii) $b_n^{-2} \sum_{k=1}^n E(\overline{X_{n,k}}^2)\rightarrow 0.$ If we let $S_n=X_{n,1}+\dots +X_{n,n}$ and put $a_n=\sum_{k=1}^n E(\overline{X_{n,k}})$ then   $$\frac{S_n-a_n}{b_n}\rightarrow 0 \ \text{in probability}. $$ I am letting $b_n=n/\log_2(n)$. Durrett suggests using something else, but this seems more natural to me. I have that all the $X_n$ are independent and that $b_n\rightarrow \infty$ as $n\rightarrow 0$. What I am having a hard time checking is why (i) and (ii) hold for our $X_n$. I see that $X_n=-1,1,3,7,\dots$, in case that helps. I appreciate any help.",,"['probability', 'probability-theory']"
4,Uniformly Random Tuples,Uniformly Random Tuples,,"Consider a multiset of natural numbers. As an example take $$ M = \{1, 2, 2, 3, 3, 3\} $$ If we treat copies of the same number as indistinguishable, there are 8 distinct 2-tuples we can form from this, by not using any number more often than it appears in $M$: $$ \{ (1,2), (2,1), (1,3), (3,1), (2,3), (3,2), (2,2), (3,3) \} $$ Is there an algorithm which generates one of these tuples with uniform probability, without having to generate all the tuples up front and selecting one at random? Note that the uniformity is in reference to the list of tuples, so $(3,3)$ should appear with the same probability as $(2,2)$ or $(1,2)$, respectively. The algorithm should work for arbitrary non-empty $M$ and $n \leq |M|$, where the goal is to generate $n$-tuples. Ideally, I'm looking for an algorithm that is not based on rejection, but if that is not possible, I'd also be interested in how I can minimise the number of necessary rejections to generate the tuples efficiently. If special cases (i.e. small, fixed $n$) yield good algorithms, I'd still be interested in those, even if they don't generalise to larger $n$.","Consider a multiset of natural numbers. As an example take $$ M = \{1, 2, 2, 3, 3, 3\} $$ If we treat copies of the same number as indistinguishable, there are 8 distinct 2-tuples we can form from this, by not using any number more often than it appears in $M$: $$ \{ (1,2), (2,1), (1,3), (3,1), (2,3), (3,2), (2,2), (3,3) \} $$ Is there an algorithm which generates one of these tuples with uniform probability, without having to generate all the tuples up front and selecting one at random? Note that the uniformity is in reference to the list of tuples, so $(3,3)$ should appear with the same probability as $(2,2)$ or $(1,2)$, respectively. The algorithm should work for arbitrary non-empty $M$ and $n \leq |M|$, where the goal is to generate $n$-tuples. Ideally, I'm looking for an algorithm that is not based on rejection, but if that is not possible, I'd also be interested in how I can minimise the number of necessary rejections to generate the tuples efficiently. If special cases (i.e. small, fixed $n$) yield good algorithms, I'd still be interested in those, even if they don't generalise to larger $n$.",,"['probability', 'algorithms', 'random', 'multisets']"
5,Occupying seats in a classroom,Occupying seats in a classroom,,"Here's a nice probability puzzle I have thought about for a class I'm TAing, I'm curious to see different solutions :) It goes like this: We have a classroom with $n$ seats available and $m \leq n$ incoming students. Each student has an (ordered) list of $k \leq n$ preferences for the seat he is going to take, where $k$ is some fixed positive integer. If at the moment of his arrival, a person's $k$ favorite seats are already taken, then he randomly chooses a seat from the remaining $n-k$. What is the probability that everyone occupies one of his favorite $k$ seats?","Here's a nice probability puzzle I have thought about for a class I'm TAing, I'm curious to see different solutions :) It goes like this: We have a classroom with $n$ seats available and $m \leq n$ incoming students. Each student has an (ordered) list of $k \leq n$ preferences for the seat he is going to take, where $k$ is some fixed positive integer. If at the moment of his arrival, a person's $k$ favorite seats are already taken, then he randomly chooses a seat from the remaining $n-k$. What is the probability that everyone occupies one of his favorite $k$ seats?",,"['probability', 'discrete-mathematics']"
6,Probability of a m-long cycle,Probability of a m-long cycle,,"You have a deck of numbered cards from $1$ to $n$. After shuffling the cards randomly, you put them in numbered boxes, from $1$ to $n$, i.e. $1$ card per box. So box $1$ can contain any one card from $1$ to $n$, etc. You now go to box $1$ and look at the card inside. If the card is, for example, a $4$, you go to box number $4$ and look at the card inside. That card tells you where to go next. You continue, until you reach card $1$ (which would tell you to go to box $1$, which is empty). The number of cards (or steps) is the number of a cycle you just discovered. You then go to the first box with a card inside, and repeat the process. You end up with $1$ to $n$ cycles of $1$ to $n$ cards. For example, you could have one very big cycle with all the cards, or one big cycle with half of the cards and lots of smaller cycles. If you look into box number $7$ and you find the card with a $7$, that is a $1$-cycle. What is the probability that no cycle is longer than ${n \over 2}$? Thank you!","You have a deck of numbered cards from $1$ to $n$. After shuffling the cards randomly, you put them in numbered boxes, from $1$ to $n$, i.e. $1$ card per box. So box $1$ can contain any one card from $1$ to $n$, etc. You now go to box $1$ and look at the card inside. If the card is, for example, a $4$, you go to box number $4$ and look at the card inside. That card tells you where to go next. You continue, until you reach card $1$ (which would tell you to go to box $1$, which is empty). The number of cards (or steps) is the number of a cycle you just discovered. You then go to the first box with a card inside, and repeat the process. You end up with $1$ to $n$ cycles of $1$ to $n$ cards. For example, you could have one very big cycle with all the cards, or one big cycle with half of the cards and lots of smaller cycles. If you look into box number $7$ and you find the card with a $7$, that is a $1$-cycle. What is the probability that no cycle is longer than ${n \over 2}$? Thank you!",,['probability']
7,Kelly criterion for 3 outcomes,Kelly criterion for 3 outcomes,,"I have been exploring the Kelly criterion for optimizing the bet size for a two outcome bet situation. I'm having trouble applying this to a three outcome bet. I may refer to this excellent thread: Kelly criterion with more than two outcomes , answered by David Speyer. The thing that troubles me is that the result of his answer is a single number for all three bets combined. In my mind, there should be three different ""optimal"" bet sizes, depending which outcome you bet on. Say the odds for a match is the following: Team1 wins, 1.54 - probability 65% Team2 wins, 4.00 - probability 25% Draw, 10.00 - probability 10% You can only bet on one outcome, and there is only one of the outcomes that happens. Explanation much apreciated. Thanks!","I have been exploring the Kelly criterion for optimizing the bet size for a two outcome bet situation. I'm having trouble applying this to a three outcome bet. I may refer to this excellent thread: Kelly criterion with more than two outcomes , answered by David Speyer. The thing that troubles me is that the result of his answer is a single number for all three bets combined. In my mind, there should be three different ""optimal"" bet sizes, depending which outcome you bet on. Say the odds for a match is the following: Team1 wins, 1.54 - probability 65% Team2 wins, 4.00 - probability 25% Draw, 10.00 - probability 10% You can only bet on one outcome, and there is only one of the outcomes that happens. Explanation much apreciated. Thanks!",,"['probability', 'probability-theory']"
8,Why is the probability of drawing a king and then a heart the same as drawing the king of hearts?,Why is the probability of drawing a king and then a heart the same as drawing the king of hearts?,,"I calculated that the probability of drawing a king and then a heart from a deck of cards is $\frac{1(12) + 3(13)}{\text{Permutation}(52,2)}=\frac{1}{52}$ However, I also noticed that this is the same as the probability of drawing the king of hearts, which is also $\frac{1}{52}$ Is this just a coincidence or is there a reason why this happens?","I calculated that the probability of drawing a king and then a heart from a deck of cards is $\frac{1(12) + 3(13)}{\text{Permutation}(52,2)}=\frac{1}{52}$ However, I also noticed that this is the same as the probability of drawing the king of hearts, which is also $\frac{1}{52}$ Is this just a coincidence or is there a reason why this happens?",,"['probability', 'card-games']"
9,Prove that a constant multiplied by a Poisson random variable is not Poisson,Prove that a constant multiplied by a Poisson random variable is not Poisson,,"Does the following constitute a proof that the multiplication of a Poisson random variable $K$ with an integer constant $a$ is not itself Poisson? That is, $f_K(k) = \frac{\lambda^k}{k!} e^{-\lambda}$ $L = aK$ Does not imply $L$ is a Poisson random variable. Intuitively, multiplication of an integer-valued distribution leaves ""gaps"" on the number line, which would mean $aK$ cannot be poisson. Here is my attempted proof: $E[K] = \lambda$ $E[L] = a\lambda$ $Var(L) = a^2 \lambda$ If $L$ is Poisson, then $E[L] = Var(L)$ Which is a contradiction. Additionally, does that mean that $L$ is poisson iff. $a = 1$?","Does the following constitute a proof that the multiplication of a Poisson random variable $K$ with an integer constant $a$ is not itself Poisson? That is, $f_K(k) = \frac{\lambda^k}{k!} e^{-\lambda}$ $L = aK$ Does not imply $L$ is a Poisson random variable. Intuitively, multiplication of an integer-valued distribution leaves ""gaps"" on the number line, which would mean $aK$ cannot be poisson. Here is my attempted proof: $E[K] = \lambda$ $E[L] = a\lambda$ $Var(L) = a^2 \lambda$ If $L$ is Poisson, then $E[L] = Var(L)$ Which is a contradiction. Additionally, does that mean that $L$ is poisson iff. $a = 1$?",,"['probability', 'random-variables']"
10,Coding Theory Problem to save Humanity,Coding Theory Problem to save Humanity,,"For starters, this problem doesn't originate from me, it's a friend's coding theory problem and I got interested, thinking about it, but I can't think of any as I only have very basic coding theory knowledge. (I'm not taking that course btw) So the problem goes as follows~ A perverse extra terrestrial force has captured 7 humans, out of desire to get a clue about the intelligence of the human race. The next day, they will be put to a test; if they succeed, they will be returned home safely and humanity will probably not be bothered anymore, however if they fail, they will all be disposed of, and humans will be seen as weak and suitable for enslaving.... They are told the following: The next day, each of them will get a hair-coloring, at random, either blond or black, without knowing which. After hair-coloring, they will be all brought together. At that moment, each person will be able to see everyone's hair-color except their own. Then, every person is required to either guess their own hair-color by writing the color on a piece of paper, or to refrain from guessing. All pieces of paper are collected; if all guessers guess correct, with at least one person guessing , they succeed the test, but if at least one guesser guesses it wrong, they fail. Then they are sent off to discuss their strategy... What should they do? An obvious strategy is to design one person to randomly guess a color; that will save the humanity half of the times. However, they can do (much) better. Imagine you are one of the seven, provide the team with a good strategy.... (Hint: Think of the coloring as an unknown binary word, and think of the missing color as a possible error) So, what could possibly be a good method?","For starters, this problem doesn't originate from me, it's a friend's coding theory problem and I got interested, thinking about it, but I can't think of any as I only have very basic coding theory knowledge. (I'm not taking that course btw) So the problem goes as follows~ A perverse extra terrestrial force has captured 7 humans, out of desire to get a clue about the intelligence of the human race. The next day, they will be put to a test; if they succeed, they will be returned home safely and humanity will probably not be bothered anymore, however if they fail, they will all be disposed of, and humans will be seen as weak and suitable for enslaving.... They are told the following: The next day, each of them will get a hair-coloring, at random, either blond or black, without knowing which. After hair-coloring, they will be all brought together. At that moment, each person will be able to see everyone's hair-color except their own. Then, every person is required to either guess their own hair-color by writing the color on a piece of paper, or to refrain from guessing. All pieces of paper are collected; if all guessers guess correct, with at least one person guessing , they succeed the test, but if at least one guesser guesses it wrong, they fail. Then they are sent off to discuss their strategy... What should they do? An obvious strategy is to design one person to randomly guess a color; that will save the humanity half of the times. However, they can do (much) better. Imagine you are one of the seven, provide the team with a good strategy.... (Hint: Think of the coloring as an unknown binary word, and think of the missing color as a possible error) So, what could possibly be a good method?",,"['probability', 'recreational-mathematics', 'coding-theory']"
11,Mean and variance of truncated generalized Beta distribution,Mean and variance of truncated generalized Beta distribution,,"The generalized Beta probability density function is given by: $$f(x) = \frac{(x-A)^{\alpha - 1} (B-x)^{\beta - 1}}{(B-A)^{\alpha + \beta - 1} \mathrm{B}(\alpha ,\beta)}$$ for $A<x<B$, and $f(x) = 0$ otherwise. Here $\alpha>0$ and $\beta>0$. Let $g(x)$ be a  truncated version of this distribution in the interval $[a,b]$, where $A\le a \le b \le B$. That is, $g(x)\propto f(x)$ if $a\le x\le b$, with a proportionality constant that normalizes $g(x)$, and $g(x)=0$ otherwise. Write explicit formulas (without integrations or infinite sums; you can use special functions) for the first and second moments of $g(x)$: $$\langle x \rangle_g = \int_{-\infty}^\infty x g(x)\mathrm{d}x$$ $$\langle x^2 \rangle_g = \int_{-\infty}^\infty x^2 g(x)\mathrm{d}x$$","The generalized Beta probability density function is given by: $$f(x) = \frac{(x-A)^{\alpha - 1} (B-x)^{\beta - 1}}{(B-A)^{\alpha + \beta - 1} \mathrm{B}(\alpha ,\beta)}$$ for $A<x<B$, and $f(x) = 0$ otherwise. Here $\alpha>0$ and $\beta>0$. Let $g(x)$ be a  truncated version of this distribution in the interval $[a,b]$, where $A\le a \le b \le B$. That is, $g(x)\propto f(x)$ if $a\le x\le b$, with a proportionality constant that normalizes $g(x)$, and $g(x)=0$ otherwise. Write explicit formulas (without integrations or infinite sums; you can use special functions) for the first and second moments of $g(x)$: $$\langle x \rangle_g = \int_{-\infty}^\infty x g(x)\mathrm{d}x$$ $$\langle x^2 \rangle_g = \int_{-\infty}^\infty x^2 g(x)\mathrm{d}x$$",,"['probability', 'special-functions']"
12,"Measure theory, probability, tail events.","Measure theory, probability, tail events.",,"I have a problem with tail events. At the top of page 19 of Stefan Grosskinsky's lecture notes , it is pointed out that $A := \{\omega: \lim_{n\rightarrow\infty}X_n(\omega) \text{ exists}\}$ is a tail event, where $(X_n)_{n \in \mathbb{N}}$ are random variables defined on probably space $(\Omega, \mathcal{A}, \mathbb{P})$, taking values in $(\mathbb{R}, \mathcal{B})$, where $\mathcal{B}$ is the Borel $\sigma$-algebra on $\mathbb{R}$. Refer to bottom of page 18 for the definition of tail events. While this certainly sounds intuitive, I wanted to see if I can show more rigorously how this is true. I need to show that $$\forall M \in \mathbb{N}, \quad A \in \mathcal{T}_M,$$ where $\mathcal{T}_M := \sigma(X_{M+1}^{-1}(\mathcal{B}), X_{M+2}^{-1}(\mathcal{B}), \ldots)$. I realise the following. \begin{equation*} \begin{split} A &= \{\omega: \exists c \in \mathbb{R}, \forall \epsilon > 0, \exists N \in \mathbb{N}, \forall n > N, X_n(\omega) \in (c-\epsilon, c+\epsilon)\} \\ &= \bigcup_{c \in \mathbb{R}} \bigcap_{\epsilon > 0} \bigcup_{N \in \mathbb{N}} \bigcap_{n > N} X_n^{-1}(c-\epsilon, c+\epsilon). \end{split} \end{equation*} After a long night's struggle I managed to convince myself of the following. I will skim the details for now but please do ask me for more detail if you're interested. Certainly $X_n^{-1} (c-\epsilon, c+\epsilon) \in X_n^{-1}(\mathcal{B})$ for each particular $n \in \mathbb{N}$. So for a particular $N \in \mathbb{N}$, we have $\bigcap_{n>N}X_n^{-1}(c-\epsilon, c+\epsilon) \in \sigma(X_{N+1}^{-1}(\mathcal{B}), X_{N+2}^{-1}(\mathcal{B}), \ldots) := \mathcal{T}_N$, by definition of $\sigma$-algebra (in particular the closure of $\sigma$-algebrae under countable intersections). Notice that $\bigcap_{n>N}X_n^{-1}(c-\epsilon, c+\epsilon)$ tends \emph{up} as $N \rightarrow \infty$. So $$\bigcup_{N\in\mathbb{N}} \bigcap_{n>N}X_n^{-1}(c-\epsilon, c+\epsilon) = \lim_{N \rightarrow \infty} \bigcap_{n>N}X_n^{-1}(c-\epsilon, c+\epsilon),$$ and so it is certainly a member of $\mathcal{T}_M$ for any finite $M \in \mathbb{N}$. So far so good, we have shown that $\forall M \in \mathbb{N}$, $\bigcup_{N\in\mathbb{N}} \bigcap_{n>N}X_n^{-1}(c-\epsilon, c+\epsilon) \in \mathcal{T}_M$. Now we only need to deal with the $\bigcap_{\epsilon>0}$ and the $\bigcup_{c \in \mathbb{R}}$, bearing in mind however that $\sigma$-algebrae are closed under \emph{countable} intersections and unions, and that $\bigcap_{\epsilon>0}$ and $\bigcup_{c \in \mathbb{R}}$ are uncountable union and intersection. I can see that the intersection $\bigcap_{\epsilon > 0}$ can be changed to a countable intersection $\bigcap_{\epsilon \in \mathbb{Q}, \epsilon > 0}$ without changing the set. That is, for a fixed $c \in \mathbb{R}$, we have $$\bigcap_{\epsilon > 0} \bigcup_{N \in \mathbb{N}} \bigcap_{n > N} X_n^{-1}(c-\epsilon, c+\epsilon) = \bigcap_{\epsilon \in \mathbb{Q}, \epsilon > 0} \bigcup_{N \in \mathbb{N}} \bigcap_{n > N} X_n^{-1}(c-\epsilon, c+\epsilon).$$ Again, please ask me for details if you're interested but I won't go into it here. Now comes the key point. First (for short-hand notation) define $$A_c := \bigcap_{\epsilon > 0} \bigcup_{N \in \mathbb{N}} \bigcap_{n > N} X_n^{-1}(c-\epsilon, c+\epsilon)$$ for each $c \in \mathbb{R}$. I can't turn the \emph{uncountable} union $\bigcup_{c \in \mathbb{R}}$ into a \emph{countable} one. That is, it seems to me that $$\bigcup_{c \in \mathbb{R}} A_c \neq \bigcup_{c \in \mathbb{Q}} A_c.$$ For example, think about the event that $X_n$ tend to an irrational number. So it seems to me that, at least from this analysis, we cannot say that $A := \{\omega: \lim_{n\rightarrow\infty}X_n(\omega) \text{ exists}\}$ is a member of $\mathcal{T}_M$ (or indeed the tail $\sigma$-algebra $\mathcal{T}$). At most we can say that $\{\omega: \lim_{n\rightarrow\infty}X_n(\omega) \text{ exists in }\mathbb{Q}\}$ is a member of $\mathcal{T}_M$ for every $M \in \mathbb{N}$ (and so is in $\mathcal{T}$). Am I wrong? Is my analysis wrong? Please feel free to ask me to expand on any part of this. This is a long and complicated (tedious rather) question I know. I'd really really appreciate any help. Thank you very much for your time!","I have a problem with tail events. At the top of page 19 of Stefan Grosskinsky's lecture notes , it is pointed out that $A := \{\omega: \lim_{n\rightarrow\infty}X_n(\omega) \text{ exists}\}$ is a tail event, where $(X_n)_{n \in \mathbb{N}}$ are random variables defined on probably space $(\Omega, \mathcal{A}, \mathbb{P})$, taking values in $(\mathbb{R}, \mathcal{B})$, where $\mathcal{B}$ is the Borel $\sigma$-algebra on $\mathbb{R}$. Refer to bottom of page 18 for the definition of tail events. While this certainly sounds intuitive, I wanted to see if I can show more rigorously how this is true. I need to show that $$\forall M \in \mathbb{N}, \quad A \in \mathcal{T}_M,$$ where $\mathcal{T}_M := \sigma(X_{M+1}^{-1}(\mathcal{B}), X_{M+2}^{-1}(\mathcal{B}), \ldots)$. I realise the following. \begin{equation*} \begin{split} A &= \{\omega: \exists c \in \mathbb{R}, \forall \epsilon > 0, \exists N \in \mathbb{N}, \forall n > N, X_n(\omega) \in (c-\epsilon, c+\epsilon)\} \\ &= \bigcup_{c \in \mathbb{R}} \bigcap_{\epsilon > 0} \bigcup_{N \in \mathbb{N}} \bigcap_{n > N} X_n^{-1}(c-\epsilon, c+\epsilon). \end{split} \end{equation*} After a long night's struggle I managed to convince myself of the following. I will skim the details for now but please do ask me for more detail if you're interested. Certainly $X_n^{-1} (c-\epsilon, c+\epsilon) \in X_n^{-1}(\mathcal{B})$ for each particular $n \in \mathbb{N}$. So for a particular $N \in \mathbb{N}$, we have $\bigcap_{n>N}X_n^{-1}(c-\epsilon, c+\epsilon) \in \sigma(X_{N+1}^{-1}(\mathcal{B}), X_{N+2}^{-1}(\mathcal{B}), \ldots) := \mathcal{T}_N$, by definition of $\sigma$-algebra (in particular the closure of $\sigma$-algebrae under countable intersections). Notice that $\bigcap_{n>N}X_n^{-1}(c-\epsilon, c+\epsilon)$ tends \emph{up} as $N \rightarrow \infty$. So $$\bigcup_{N\in\mathbb{N}} \bigcap_{n>N}X_n^{-1}(c-\epsilon, c+\epsilon) = \lim_{N \rightarrow \infty} \bigcap_{n>N}X_n^{-1}(c-\epsilon, c+\epsilon),$$ and so it is certainly a member of $\mathcal{T}_M$ for any finite $M \in \mathbb{N}$. So far so good, we have shown that $\forall M \in \mathbb{N}$, $\bigcup_{N\in\mathbb{N}} \bigcap_{n>N}X_n^{-1}(c-\epsilon, c+\epsilon) \in \mathcal{T}_M$. Now we only need to deal with the $\bigcap_{\epsilon>0}$ and the $\bigcup_{c \in \mathbb{R}}$, bearing in mind however that $\sigma$-algebrae are closed under \emph{countable} intersections and unions, and that $\bigcap_{\epsilon>0}$ and $\bigcup_{c \in \mathbb{R}}$ are uncountable union and intersection. I can see that the intersection $\bigcap_{\epsilon > 0}$ can be changed to a countable intersection $\bigcap_{\epsilon \in \mathbb{Q}, \epsilon > 0}$ without changing the set. That is, for a fixed $c \in \mathbb{R}$, we have $$\bigcap_{\epsilon > 0} \bigcup_{N \in \mathbb{N}} \bigcap_{n > N} X_n^{-1}(c-\epsilon, c+\epsilon) = \bigcap_{\epsilon \in \mathbb{Q}, \epsilon > 0} \bigcup_{N \in \mathbb{N}} \bigcap_{n > N} X_n^{-1}(c-\epsilon, c+\epsilon).$$ Again, please ask me for details if you're interested but I won't go into it here. Now comes the key point. First (for short-hand notation) define $$A_c := \bigcap_{\epsilon > 0} \bigcup_{N \in \mathbb{N}} \bigcap_{n > N} X_n^{-1}(c-\epsilon, c+\epsilon)$$ for each $c \in \mathbb{R}$. I can't turn the \emph{uncountable} union $\bigcup_{c \in \mathbb{R}}$ into a \emph{countable} one. That is, it seems to me that $$\bigcup_{c \in \mathbb{R}} A_c \neq \bigcup_{c \in \mathbb{Q}} A_c.$$ For example, think about the event that $X_n$ tend to an irrational number. So it seems to me that, at least from this analysis, we cannot say that $A := \{\omega: \lim_{n\rightarrow\infty}X_n(\omega) \text{ exists}\}$ is a member of $\mathcal{T}_M$ (or indeed the tail $\sigma$-algebra $\mathcal{T}$). At most we can say that $\{\omega: \lim_{n\rightarrow\infty}X_n(\omega) \text{ exists in }\mathbb{Q}\}$ is a member of $\mathcal{T}_M$ for every $M \in \mathbb{N}$ (and so is in $\mathcal{T}$). Am I wrong? Is my analysis wrong? Please feel free to ask me to expand on any part of this. This is a long and complicated (tedious rather) question I know. I'd really really appreciate any help. Thank you very much for your time!",,"['probability', 'measure-theory']"
13,Prove the scaling property of a Brownian motion.,Prove the scaling property of a Brownian motion.,,"I have to prove that $X_t:=c^{-1/2}W_{ct}$, $t\ge0$, where $c>0$ is a constant is a Wiener process. My attempt: 1) $X_0=c^{-1/2}W_0=0$ 2) We know that $(W_t)$ has continuous trajectories. It implies that $(X_t)$ has also continuous trajectories since continuous function multiplied by a constant and with scaled argument is still continuous. Right? 3) Independence of increments. We know that $W_i-W_j$ and $W_k-W_l$ are independent for all $i, j, k, l\in\mathbb{R}^+$. Now, $X_i-X_j=c^{-1/2}(W_{ci}-W_{cj})$ nad $X_k-X_l=c^{-1/2}(W_{ck}-W_{cl})$ so $(X_t)$ has independent increments. 4) $X_{s+\epsilon}-X_s=c^{-1/2}(W_{c(s+\epsilon)}-W_{cs})$ ~ $N(0,\epsilon)$","I have to prove that $X_t:=c^{-1/2}W_{ct}$, $t\ge0$, where $c>0$ is a constant is a Wiener process. My attempt: 1) $X_0=c^{-1/2}W_0=0$ 2) We know that $(W_t)$ has continuous trajectories. It implies that $(X_t)$ has also continuous trajectories since continuous function multiplied by a constant and with scaled argument is still continuous. Right? 3) Independence of increments. We know that $W_i-W_j$ and $W_k-W_l$ are independent for all $i, j, k, l\in\mathbb{R}^+$. Now, $X_i-X_j=c^{-1/2}(W_{ci}-W_{cj})$ nad $X_k-X_l=c^{-1/2}(W_{ck}-W_{cl})$ so $(X_t)$ has independent increments. 4) $X_{s+\epsilon}-X_s=c^{-1/2}(W_{c(s+\epsilon)}-W_{cs})$ ~ $N(0,\epsilon)$",,"['probability', 'stochastic-processes']"
14,What is the limit behavior of this random sum?,What is the limit behavior of this random sum?,,"Let $(X_n\mid n\in\mathbb{N})$ be an i.i.d. sequence of random variables taking values in $\mathbb{R}$. What can be said about the limit behavior of  \begin{equation} S_n:=\sum_{i=1}^n\frac{X_i}{i} \end{equation} as $n\rightarrow \infty$? In particular, under what conditions does $S_n$ converge (to zero?), and under what conditions does it not converge? Many thanks for any help!","Let $(X_n\mid n\in\mathbb{N})$ be an i.i.d. sequence of random variables taking values in $\mathbb{R}$. What can be said about the limit behavior of  \begin{equation} S_n:=\sum_{i=1}^n\frac{X_i}{i} \end{equation} as $n\rightarrow \infty$? In particular, under what conditions does $S_n$ converge (to zero?), and under what conditions does it not converge? Many thanks for any help!",,"['probability', 'statistics', 'probability-theory']"
15,Interesting but difficult board game question?,Interesting but difficult board game question?,,"I have a 3 by 3 board game. A black marble is randomly place in one of the nine squares. Distance between squares is measured as one if either diagonal or horizontal/vertically next each other, and two otherwise (so max distance of two). With an initial black marble, all squares within distance one of this black marble contains a blue one. The rest of the squares are then filled with a red marble. All 9 squares are covered. How many minimum cups (and which ones) do I need to look at in order to figure out where black marble is? \begin{array}{ccc} a & b & c \\ d & e & f \\ g & h & i \end{array} So my thinking is that the center one will be blue with probability $8/9$ and black with probability $1/9$. If the black marble is one the corners (probability $4/9$), then $e$ must be blue and two of $b, d, f, h$ must be blue. This seems to be a lot of case work, but I get that the minimum is 4? Has to be symmetric?","I have a 3 by 3 board game. A black marble is randomly place in one of the nine squares. Distance between squares is measured as one if either diagonal or horizontal/vertically next each other, and two otherwise (so max distance of two). With an initial black marble, all squares within distance one of this black marble contains a blue one. The rest of the squares are then filled with a red marble. All 9 squares are covered. How many minimum cups (and which ones) do I need to look at in order to figure out where black marble is? \begin{array}{ccc} a & b & c \\ d & e & f \\ g & h & i \end{array} So my thinking is that the center one will be blue with probability $8/9$ and black with probability $1/9$. If the black marble is one the corners (probability $4/9$), then $e$ must be blue and two of $b, d, f, h$ must be blue. This seems to be a lot of case work, but I get that the minimum is 4? Has to be symmetric?",,"['probability', 'probability-theory']"
16,Criterion for independency of random variables,Criterion for independency of random variables,,"I saw in some notes the following ""criterion"" for independency of two random variables. Let $X$ and $Y$ be real-valued random variables defined on the same space. $X$ and $Y$ are statistically independent if and only if for any two functions $g$ and $h$ the following holds true $$ \mathbb{E}\left\{ h(X)g(Y)\right\} = \mathbb{E}\left\{ h(X)\right\}\mathbb{E}\left\{ g(Y)\right\}. $$ (1) Regard the ""any two functions"" requirement, what does it mean? (2) It is helpful to use this criterion to show that two random variables are dependent. I'm curious whether there is an example of showing that two random variables are independent, using this criterion?","I saw in some notes the following ""criterion"" for independency of two random variables. Let $X$ and $Y$ be real-valued random variables defined on the same space. $X$ and $Y$ are statistically independent if and only if for any two functions $g$ and $h$ the following holds true $$ \mathbb{E}\left\{ h(X)g(Y)\right\} = \mathbb{E}\left\{ h(X)\right\}\mathbb{E}\left\{ g(Y)\right\}. $$ (1) Regard the ""any two functions"" requirement, what does it mean? (2) It is helpful to use this criterion to show that two random variables are dependent. I'm curious whether there is an example of showing that two random variables are independent, using this criterion?",,"['probability', 'probability-theory', 'random-variables']"
17,Jensen's inequality and Averaging of Coefficients,Jensen's inequality and Averaging of Coefficients,,"I am using Jensen's inequality and conditional expectation to prove the following inequality: Let $\lambda_i$ be real for $i\in \{1,2,...,M\}$ and $\bar{\lambda}=\frac{\sum_{i=1}^M\lambda_i}{M}$. Let $X_i$, $i\in \{1,2,...,M\}$ be a set of real i.i.d random variables, then we have \begin{align} \mathbb{E}\left(f\left(\sum_{i=1}^M \bar{\lambda} X_i\right)\right) \le \mathbb{E}\left(f\left(\sum_{i=1}^M \lambda_i X_i\right)\right), \end{align} where $f(\cdot)$ is a convex function. The equal sign holds when $\lambda_{i}=\bar{\lambda}$ for all $i\in \{1,2,...,M\}$. And my proof is given as: Let \begin{align} X=\sum_{i=1}^M\bar{\lambda}X_i, \end{align} \begin{align} W=\sum_{i=1}^M\lambda_iX_i, \end{align} and \begin{align} Z=X-W. \end{align} By the symmetric property of $X$, the conditional random variables $X_i|X$, $i\in \{1,...,M\}$, are identically distributed, which implies \begin{align} \mathbb{E}(X_1|X)=\mathbb{E}(X_2|X)= \cdots = \mathbb{E}(X_M|X). \end{align} Therefore \begin{align} \mathbb{E}(Z|X)&=\sum_{i=1}^M \bar{\lambda}\mathbb{E}(X_i|X)-\sum_{i=1}^M \lambda_{i}\mathbb{E}(X_i|X)\\       &=\mathbb{E}(X_1|X)\left(\sum_{i=1}^M \bar{\lambda}-\sum_{i=1}^M \lambda_{i}\right)=0. \end{align} Since $f(\cdot)$ is convex, by Jensen's inequality we have \begin{align} \mathbb{E}(f(X-Z)|X) &\ge f(\mathbb{E}((X-Z)|X)) \\ &=f(X-0)=f(X) \end{align} Therefore \begin{align} \mathbb{E}(f(W))=\mathbb{E}(\mathbb{E}(f(X-Z)|X))\ge \mathbb{E}(f(X)). \end{align} I think the proof is right, but not 100% sure, can someone give me a judgement of this proof? Thanks!","I am using Jensen's inequality and conditional expectation to prove the following inequality: Let $\lambda_i$ be real for $i\in \{1,2,...,M\}$ and $\bar{\lambda}=\frac{\sum_{i=1}^M\lambda_i}{M}$. Let $X_i$, $i\in \{1,2,...,M\}$ be a set of real i.i.d random variables, then we have \begin{align} \mathbb{E}\left(f\left(\sum_{i=1}^M \bar{\lambda} X_i\right)\right) \le \mathbb{E}\left(f\left(\sum_{i=1}^M \lambda_i X_i\right)\right), \end{align} where $f(\cdot)$ is a convex function. The equal sign holds when $\lambda_{i}=\bar{\lambda}$ for all $i\in \{1,2,...,M\}$. And my proof is given as: Let \begin{align} X=\sum_{i=1}^M\bar{\lambda}X_i, \end{align} \begin{align} W=\sum_{i=1}^M\lambda_iX_i, \end{align} and \begin{align} Z=X-W. \end{align} By the symmetric property of $X$, the conditional random variables $X_i|X$, $i\in \{1,...,M\}$, are identically distributed, which implies \begin{align} \mathbb{E}(X_1|X)=\mathbb{E}(X_2|X)= \cdots = \mathbb{E}(X_M|X). \end{align} Therefore \begin{align} \mathbb{E}(Z|X)&=\sum_{i=1}^M \bar{\lambda}\mathbb{E}(X_i|X)-\sum_{i=1}^M \lambda_{i}\mathbb{E}(X_i|X)\\       &=\mathbb{E}(X_1|X)\left(\sum_{i=1}^M \bar{\lambda}-\sum_{i=1}^M \lambda_{i}\right)=0. \end{align} Since $f(\cdot)$ is convex, by Jensen's inequality we have \begin{align} \mathbb{E}(f(X-Z)|X) &\ge f(\mathbb{E}((X-Z)|X)) \\ &=f(X-0)=f(X) \end{align} Therefore \begin{align} \mathbb{E}(f(W))=\mathbb{E}(\mathbb{E}(f(X-Z)|X))\ge \mathbb{E}(f(X)). \end{align} I think the proof is right, but not 100% sure, can someone give me a judgement of this proof? Thanks!",,"['probability', 'statistics', 'inequality']"
18,An interesting puzzle [duplicate],An interesting puzzle [duplicate],,"This question already has answers here : Exercise 1.6.3 from Alon & Spencer's *The Probabilistic Method*: prove that $Pr[|X-Y| \leq 2] \leq 3 Pr[|X-Y| \leq 1]$ for i.i.d. real RVs $X$ and $Y$ (3 answers) Closed 4 years ago . Here is a puzzle challenge for you: Suppose $X,Y$ are independent and identically distributed Random Variables. Show that $$P\{|X-Y|\le2\}\le 3P\{|X-Y|\le 1\}$$","This question already has answers here : Exercise 1.6.3 from Alon & Spencer's *The Probabilistic Method*: prove that $Pr[|X-Y| \leq 2] \leq 3 Pr[|X-Y| \leq 1]$ for i.i.d. real RVs $X$ and $Y$ (3 answers) Closed 4 years ago . Here is a puzzle challenge for you: Suppose $X,Y$ are independent and identically distributed Random Variables. Show that $$P\{|X-Y|\le2\}\le 3P\{|X-Y|\le 1\}$$",,"['probability', 'puzzle']"
19,Probability brainteaser [closed],Probability brainteaser [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question Normal 52 card deck. Cards are dealt one-by-one. You get to say when to stop. After you say stop you win a dollar if the next card is red, lose a dollar if the next is black. Assuming you use the optimal stopping strategy, how much would you be willing to pay to play? Proof?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question Normal 52 card deck. Cards are dealt one-by-one. You get to say when to stop. After you say stop you win a dollar if the next card is red, lose a dollar if the next is black. Assuming you use the optimal stopping strategy, how much would you be willing to pay to play? Proof?",,"['probability', 'statistics', 'stochastic-processes', 'stochastic-calculus', 'card-games']"
20,"Probability that a given Poisson variable samples greater than its mean $\lambda$, provided $\lambda > D$","Probability that a given Poisson variable samples greater than its mean , provided",\lambda \lambda > D,"Given a random variable $X \sim \text{Poisson}(\lambda)$ such that $\lambda > D$, with $\lambda, D \in \mathbb{N}$, what is the probability that a sample obtained from $X$ is greater than $\lambda$? In other words, what is the value of $\mathbb{P}(X > \lambda)$? I think that calculating the probability of $X$ being greater than the said value $D$ analysing this for every possible mean greater than $\lambda$ is a possible path to follow, but I'm not sure of how difficult to calculate this would be. Poisson's CDF states that, for $X \sim \text{Poisson}(\lambda)$ and $k \in \mathbb{N}$, $$ \mathbb{P}(X \leqslant k) = \mathsf{e}^{-\lambda} \sum\limits_{i = 0}^{k} \frac{\lambda^i}{i!} $$ So, the value I'm asking should be the summation of the above probability's complement from $D+1$ to infinity, or even $$ 1 - \sum\limits_{p = 0}^{D} \left( 1 - \mathsf{e}^{-p} \sum\limits_{i = 0}^{D} \frac{p^i}{i!} \right) $$ Is this the correct value? I already know that $\lambda > D$, so maybe there should be some conditional probability involved, but I'm not sure. If this is correct, what I'm asking is if there isn't a more concise way to calculate this, with less summations or none at all. This is because I'll need to calculate this value extensively in a computer program and computational time is very, very precious.","Given a random variable $X \sim \text{Poisson}(\lambda)$ such that $\lambda > D$, with $\lambda, D \in \mathbb{N}$, what is the probability that a sample obtained from $X$ is greater than $\lambda$? In other words, what is the value of $\mathbb{P}(X > \lambda)$? I think that calculating the probability of $X$ being greater than the said value $D$ analysing this for every possible mean greater than $\lambda$ is a possible path to follow, but I'm not sure of how difficult to calculate this would be. Poisson's CDF states that, for $X \sim \text{Poisson}(\lambda)$ and $k \in \mathbb{N}$, $$ \mathbb{P}(X \leqslant k) = \mathsf{e}^{-\lambda} \sum\limits_{i = 0}^{k} \frac{\lambda^i}{i!} $$ So, the value I'm asking should be the summation of the above probability's complement from $D+1$ to infinity, or even $$ 1 - \sum\limits_{p = 0}^{D} \left( 1 - \mathsf{e}^{-p} \sum\limits_{i = 0}^{D} \frac{p^i}{i!} \right) $$ Is this the correct value? I already know that $\lambda > D$, so maybe there should be some conditional probability involved, but I'm not sure. If this is correct, what I'm asking is if there isn't a more concise way to calculate this, with less summations or none at all. This is because I'll need to calculate this value extensively in a computer program and computational time is very, very precious.",,['probability']
21,Sum of stationary process,Sum of stationary process,,"Suppose you have two stationary process $A_{t}$ and $B_{t}$. Suppose $Z_{t} = A_{t} + B_{t}$. Show that $Z_{t}$ is stationary. I am unsure how to solve this without knowing if the processes are independent from each other. $\gamma_{z}(h) = E[Z_{t}Z_{t+h}] = A_{t}A_{t+h} + A_{t}B_{t+h} + B_{t}A_{t+h} + B_{t}B_{t+h} = \gamma_{A}(h) + \gamma_{BA}(h) + \gamma_{AB}(h) + \gamma_{B}(h)$. Now if the series are independent, then the cross-covariance functions are 0 and $\gamma_{z}(h) = \gamma_{A}(h) + \gamma_{B}(h)$. So my question is, do we require that $A_{t}$ and $B_{t}$ are independent from each other in order for $Z_{t}$ to be stationary?","Suppose you have two stationary process $A_{t}$ and $B_{t}$. Suppose $Z_{t} = A_{t} + B_{t}$. Show that $Z_{t}$ is stationary. I am unsure how to solve this without knowing if the processes are independent from each other. $\gamma_{z}(h) = E[Z_{t}Z_{t+h}] = A_{t}A_{t+h} + A_{t}B_{t+h} + B_{t}A_{t+h} + B_{t}B_{t+h} = \gamma_{A}(h) + \gamma_{BA}(h) + \gamma_{AB}(h) + \gamma_{B}(h)$. Now if the series are independent, then the cross-covariance functions are 0 and $\gamma_{z}(h) = \gamma_{A}(h) + \gamma_{B}(h)$. So my question is, do we require that $A_{t}$ and $B_{t}$ are independent from each other in order for $Z_{t}$ to be stationary?",,"['probability', 'stochastic-processes']"
22,"Given that $(X_n)_{n\geq 0}$ is a Markov Chain, prove that $(X_{kn})_{n\geq 0}$ is a Markov Chain","Given that  is a Markov Chain, prove that  is a Markov Chain",(X_n)_{n\geq 0} (X_{kn})_{n\geq 0},"Given that $(X_n)_{n\geq 0}$ is a Markov Chain, prove that $(X_{kn})_{n\geq 0}$ is a Markov Chain. I don't know what this exercise has been so difficult for me, I've been playing around with the definition and its consequences for a while a now without being able to prove it: By definition I know that $P(X_{n+1}\;|\;X_0,...,X_n)=P(X_{n+1}\;|\;X_n)$, and I want to show that $P(X_{k(n+1)}\;|\;X_0,...,X_{kn})=P(X_{k(n+1)}\;|\;X_{kn})$.  Is there an elementary way just using this information and just basic knowledge of how to algebraically manipulate conditional probabilities, to prove what I want to prove?  Or do I need more information about Markov chains to do this problem?","Given that $(X_n)_{n\geq 0}$ is a Markov Chain, prove that $(X_{kn})_{n\geq 0}$ is a Markov Chain. I don't know what this exercise has been so difficult for me, I've been playing around with the definition and its consequences for a while a now without being able to prove it: By definition I know that $P(X_{n+1}\;|\;X_0,...,X_n)=P(X_{n+1}\;|\;X_n)$, and I want to show that $P(X_{k(n+1)}\;|\;X_0,...,X_{kn})=P(X_{k(n+1)}\;|\;X_{kn})$.  Is there an elementary way just using this information and just basic knowledge of how to algebraically manipulate conditional probabilities, to prove what I want to prove?  Or do I need more information about Markov chains to do this problem?",,"['probability', 'markov-chains']"
23,Recursion for Finding Expectation (Somewhat Lengthy),Recursion for Finding Expectation (Somewhat Lengthy),,"Preface: Ever since I read the brilliant answer by Mike Spivey I have been on a mission for re-solving all my probability questions with it when possible. I tried solving the Coupon Collector problem using Recursion which the community assisted on another question of mine. Now, I think I have come close to completely understanding the way of using recursion. But..... Question: This is from Stochastic Processes by Sheldon Ross (Page  49, Question 1.14). The question is: A fair die is continually rolled until an even number has appeared on 10 distinct rolls. Let $X_i$ denote the number of rolls that land on side $i$. Determine : $E[X_1]$ $E[X_2]$ PMF of $X_1$ PMF of $X_2$ My Attempt: Building on my previous question, I begin: Let $N$ denote the total number of throws (Random Variable) and let $Z_{i}$ denote the result of the $i^{th}$ throw. Then: \begin{eqnarray*} E(X_{1}) & = & E\left(\sum_{i=1}^{N}1_{Z_{i}=1}\right)\\  & = & E\left[E\left(\sum_{i=1}^{N}1_{Z_{i}=1}|N\right)\right]\\ E(X_{1}|N) & = & E(1_{Z_{1}=1}+1_{Z_{2}=1}+\cdots+1_{z_{N}=1})\\  & = & \frac{N-10}{3}\\ E(X_{1}) & = & \frac{E(N)-10}{3} \end{eqnarray*} To Find : $E(N)$ Let $W_{i}$ be the waiting time for the $i^{th}$ distinct roll of an even number. Then: $$E(N)=\sum_{i=1}^{10}E(W_{i})$$ Now, \begin{eqnarray*} E(W_{i}) & = & \frac{1}{2}(1)+\frac{1}{2}(1+E(W_{i}))\\ E(W_{i}) & = & 1+\frac{E(W_{i})}{2}\\ \implies E(W_{i}) & = & 2\\ \therefore E(N) & = & \sum_{i=1}^{10}2\\  & = & 20\\ \therefore E(X_{1}) & = & \frac{10}{3}\\  &  & \blacksquare \end{eqnarray*} The exact same procedure can be followed for $E(X_2)$ with the same answer. The answer matches the one given in the book. I am confused how to go from here to get the PMFs. Note : If possible, please provide me an extension to this answer for finding the PMFs rather than a completely different method. The book has the answer at the back using a different method. I am not interested in an answer as much as I am interested in knowing how to continue this attempt to get the PMFs.","Preface: Ever since I read the brilliant answer by Mike Spivey I have been on a mission for re-solving all my probability questions with it when possible. I tried solving the Coupon Collector problem using Recursion which the community assisted on another question of mine. Now, I think I have come close to completely understanding the way of using recursion. But..... Question: This is from Stochastic Processes by Sheldon Ross (Page  49, Question 1.14). The question is: A fair die is continually rolled until an even number has appeared on 10 distinct rolls. Let $X_i$ denote the number of rolls that land on side $i$. Determine : $E[X_1]$ $E[X_2]$ PMF of $X_1$ PMF of $X_2$ My Attempt: Building on my previous question, I begin: Let $N$ denote the total number of throws (Random Variable) and let $Z_{i}$ denote the result of the $i^{th}$ throw. Then: \begin{eqnarray*} E(X_{1}) & = & E\left(\sum_{i=1}^{N}1_{Z_{i}=1}\right)\\  & = & E\left[E\left(\sum_{i=1}^{N}1_{Z_{i}=1}|N\right)\right]\\ E(X_{1}|N) & = & E(1_{Z_{1}=1}+1_{Z_{2}=1}+\cdots+1_{z_{N}=1})\\  & = & \frac{N-10}{3}\\ E(X_{1}) & = & \frac{E(N)-10}{3} \end{eqnarray*} To Find : $E(N)$ Let $W_{i}$ be the waiting time for the $i^{th}$ distinct roll of an even number. Then: $$E(N)=\sum_{i=1}^{10}E(W_{i})$$ Now, \begin{eqnarray*} E(W_{i}) & = & \frac{1}{2}(1)+\frac{1}{2}(1+E(W_{i}))\\ E(W_{i}) & = & 1+\frac{E(W_{i})}{2}\\ \implies E(W_{i}) & = & 2\\ \therefore E(N) & = & \sum_{i=1}^{10}2\\  & = & 20\\ \therefore E(X_{1}) & = & \frac{10}{3}\\  &  & \blacksquare \end{eqnarray*} The exact same procedure can be followed for $E(X_2)$ with the same answer. The answer matches the one given in the book. I am confused how to go from here to get the PMFs. Note : If possible, please provide me an extension to this answer for finding the PMFs rather than a completely different method. The book has the answer at the back using a different method. I am not interested in an answer as much as I am interested in knowing how to continue this attempt to get the PMFs.",,"['probability', 'stochastic-processes']"
24,Bayes' Theorem in Stephen Baxter's Manifold: Time,Bayes' Theorem in Stephen Baxter's Manifold: Time,,"I am currently reading the sci-fi novel Manifold: Time by Stephen Baxter, which contains the following problem. You are given a box which has either 10 marbles or 1000 marbles. By pressing a lever on the box, one marble is randomly taken out and given to you. You know that there is exactly one red marble. After pressing the lever three times, you obtain a red marble. The book claims that this information implies, using Bayes' theorem, that the probability that there are 10 marbles in the box is 2/3. Can anyone explain how this computation actually works out, or at least how one is supposed to set up Bayes' equation to get this answer? Thanks.","I am currently reading the sci-fi novel Manifold: Time by Stephen Baxter, which contains the following problem. You are given a box which has either 10 marbles or 1000 marbles. By pressing a lever on the box, one marble is randomly taken out and given to you. You know that there is exactly one red marble. After pressing the lever three times, you obtain a red marble. The book claims that this information implies, using Bayes' theorem, that the probability that there are 10 marbles in the box is 2/3. Can anyone explain how this computation actually works out, or at least how one is supposed to set up Bayes' equation to get this answer? Thanks.",,['probability']
25,Expectation of joint life span,Expectation of joint life span,,"The life span of a particular mechanical part is a random variable described by the following PDF: If three such parts are put into service independently at t=0, determine a simle expression for the expected value of the time until the majority of the parts will have failed. I can get the PDF: $$ f_L(l) = 0.4 (0 \leq l \leq 2) \\ f_L(l) = -0.4l + 1.2 (2 < l \leq 3) $$ and the expectation: $$ E(l) = \int_0^3 l f_L(l) dl \approx 1.27 $$ I think 'majority' means 2 or more, so we can focus on two parts of the three, and pay no attention to the third. The translation is $E(max(l1, l2))$, how will this be derived I currently have no idea. Sorry about the misleading remark ""$E(max(l_1, l_2))$"", it's wrong to neglect the third part, because if that one fails early, then we only need one of the rest to fail.","The life span of a particular mechanical part is a random variable described by the following PDF: If three such parts are put into service independently at t=0, determine a simle expression for the expected value of the time until the majority of the parts will have failed. I can get the PDF: $$ f_L(l) = 0.4 (0 \leq l \leq 2) \\ f_L(l) = -0.4l + 1.2 (2 < l \leq 3) $$ and the expectation: $$ E(l) = \int_0^3 l f_L(l) dl \approx 1.27 $$ I think 'majority' means 2 or more, so we can focus on two parts of the three, and pay no attention to the third. The translation is $E(max(l1, l2))$, how will this be derived I currently have no idea. Sorry about the misleading remark ""$E(max(l_1, l_2))$"", it's wrong to neglect the third part, because if that one fails early, then we only need one of the rest to fail.",,['probability']
26,Generalization of Hoeffding Inequality,Generalization of Hoeffding Inequality,,"According to Hoeffding Inequality, if $X_1,\ldots,X_n$ are independent random variables with $\mathbb{P}(X_i \in [a_i,b_i]) = 1 \; \forall i = 1,\ldots,n$ then  $$\mathbb{P}(\bar{X_n} - \mathbb{E}[\bar{X_n}] \geq t) \leqslant \exp\left(-\frac{2t^2n^2}{\sum_{i=1}^{n}(b_i - a_i)^2}\right) \quad \forall t > 0.$$ Does there exist any such bound for the probability of the event $\{\bar{X_n} - c\mathbb{E}[\bar{X_n}] \geqslant t\}$ for any constant $c$. Hoeffding Inequality is just a special case of this event when $c = 1$.","According to Hoeffding Inequality, if $X_1,\ldots,X_n$ are independent random variables with $\mathbb{P}(X_i \in [a_i,b_i]) = 1 \; \forall i = 1,\ldots,n$ then  $$\mathbb{P}(\bar{X_n} - \mathbb{E}[\bar{X_n}] \geq t) \leqslant \exp\left(-\frac{2t^2n^2}{\sum_{i=1}^{n}(b_i - a_i)^2}\right) \quad \forall t > 0.$$ Does there exist any such bound for the probability of the event $\{\bar{X_n} - c\mathbb{E}[\bar{X_n}] \geqslant t\}$ for any constant $c$. Hoeffding Inequality is just a special case of this event when $c = 1$.",,['probability']
27,Assessing discreteness of the random variable by its characteristic function,Assessing discreteness of the random variable by its characteristic function,,"It is easy to spot a discrete integer valued random variable by looking at its characteristic function, as that is periodic with period $2 \pi$, i.e. for binomial distribution it is $\phi(t) = (1-p+p \, \mathrm{e}^{i t})^n$. I recently came across Khintchine's result that $\phi(t) = \frac{\zeta(s + i t)}{\zeta(s)}$ is a characteristic function of a random variable for $s > 1$. After some fudging, I determined that it corresponds to $x_k = -\log(k)$, where $k$ follows Zipf distribution with parameter $s-1$. Indeed: $$    \mathbb{E}( \mathrm{e}^{ -i t \log(k)} ) = \mathbb{E}( k^{-i t} )  = \sum_{k \ge 1} k^{-i t} \frac{k^{-s}}{\zeta(s)} = \frac{\zeta(s+i t)}{\zeta(s)} $$ This characteristic function, thus, also corresponds to a discrete random variable. This brings up a question : Can one easily spot a discrete random variable from it's characteristic function ? Or is inverting the characteristic function the only way ? How does one go about doing the inversion ? Ordinary inverse Fourier transform would produce distributions, right ? Thank you.","It is easy to spot a discrete integer valued random variable by looking at its characteristic function, as that is periodic with period $2 \pi$, i.e. for binomial distribution it is $\phi(t) = (1-p+p \, \mathrm{e}^{i t})^n$. I recently came across Khintchine's result that $\phi(t) = \frac{\zeta(s + i t)}{\zeta(s)}$ is a characteristic function of a random variable for $s > 1$. After some fudging, I determined that it corresponds to $x_k = -\log(k)$, where $k$ follows Zipf distribution with parameter $s-1$. Indeed: $$    \mathbb{E}( \mathrm{e}^{ -i t \log(k)} ) = \mathbb{E}( k^{-i t} )  = \sum_{k \ge 1} k^{-i t} \frac{k^{-s}}{\zeta(s)} = \frac{\zeta(s+i t)}{\zeta(s)} $$ This characteristic function, thus, also corresponds to a discrete random variable. This brings up a question : Can one easily spot a discrete random variable from it's characteristic function ? Or is inverting the characteristic function the only way ? How does one go about doing the inversion ? Ordinary inverse Fourier transform would produce distributions, right ? Thank you.",,"['probability', 'probability-distributions']"
28,understanding Log likelihood,understanding Log likelihood,,I have always had a problem of understanding the big picture of probability and get lost every time I am studying it. I am working on implementing a X-means algorithm  for clustering data  and the log likelihood function keeps popping up. Please can anyone give an easy to understand explanation of Likelihood function and the log likelihood function and possible relate  to real life examples . Would mind the equation if they can be broken down .,I have always had a problem of understanding the big picture of probability and get lost every time I am studying it. I am working on implementing a X-means algorithm  for clustering data  and the log likelihood function keeps popping up. Please can anyone give an easy to understand explanation of Likelihood function and the log likelihood function and possible relate  to real life examples . Would mind the equation if they can be broken down .,,['probability']
29,Bounds for submartingale,Bounds for submartingale,,"Let $x$ be a positive number and $X_n$ be real-valued submartingale such that $X_0 = x$. I am interested in upper bounds on probability  $$ \psi(x) = \mathsf{P}_x\left\{\inf\limits_{n\geq 0}X_n \leq 0\right\}. $$ You are welcome to discuss this question with any assumptions you like - maybe, not too strict. One of my ideas was the following. If $Y_n = \frac{1}{X_n}$ is a supermartingale, then Doob's inequality can be used: $$ \mathsf{P}_y\left\{\sup\limits_{n\geq 0}Y_n \geq N\right\}\leq \frac{y}{N}, $$ but here we need to have $Y$ a non-negative process, which is not our case. Edited: The formulation of Doob's inequality [Shiryaev: Probability, p. 492]. If $Z$ is a supermartingale then $$ \mathsf P\{\sup\limits_{n\geq 0} |Z_n|\geq \delta\}\leq\frac{C}{\delta}\sup\limits_{n\geq 0} \mathsf E[|Z_n|] $$ for some $C\leq 3$. If $Z$ is a non-negative supermartingale then $C$ can be taken equal to $1$ and the expectation on the right-hand side attains its maximum at the time moment $n=0$. That leads to the inequality I've formulated in the first version of the question. With regards to random walks: I am not quite sure in you statement since for the Lundberg inequality there exists such a bound. More precisely, if random walk is given by $$ X_n = X_{n-1}+A_n $$ where $A_n$ are i.i.d. such that $\mathsf E A_n >0$ and there exists $r>0$ such that $\mathsf E\mathrm e^{-rA_1} = 1$, then $\psi(x)\leq \mathrm e^{-rx}$. I've just eventually faced such inequality from Financial Mathematics and wonder if there are known bounds for supermartingales (since $X_n$ is a supermartingale in the latter example) - they have not to be exponential of course. P.S. I put this question also here: https://mathoverflow.net/questions/68509/bounds-for-submartingale","Let $x$ be a positive number and $X_n$ be real-valued submartingale such that $X_0 = x$. I am interested in upper bounds on probability  $$ \psi(x) = \mathsf{P}_x\left\{\inf\limits_{n\geq 0}X_n \leq 0\right\}. $$ You are welcome to discuss this question with any assumptions you like - maybe, not too strict. One of my ideas was the following. If $Y_n = \frac{1}{X_n}$ is a supermartingale, then Doob's inequality can be used: $$ \mathsf{P}_y\left\{\sup\limits_{n\geq 0}Y_n \geq N\right\}\leq \frac{y}{N}, $$ but here we need to have $Y$ a non-negative process, which is not our case. Edited: The formulation of Doob's inequality [Shiryaev: Probability, p. 492]. If $Z$ is a supermartingale then $$ \mathsf P\{\sup\limits_{n\geq 0} |Z_n|\geq \delta\}\leq\frac{C}{\delta}\sup\limits_{n\geq 0} \mathsf E[|Z_n|] $$ for some $C\leq 3$. If $Z$ is a non-negative supermartingale then $C$ can be taken equal to $1$ and the expectation on the right-hand side attains its maximum at the time moment $n=0$. That leads to the inequality I've formulated in the first version of the question. With regards to random walks: I am not quite sure in you statement since for the Lundberg inequality there exists such a bound. More precisely, if random walk is given by $$ X_n = X_{n-1}+A_n $$ where $A_n$ are i.i.d. such that $\mathsf E A_n >0$ and there exists $r>0$ such that $\mathsf E\mathrm e^{-rA_1} = 1$, then $\psi(x)\leq \mathrm e^{-rx}$. I've just eventually faced such inequality from Financial Mathematics and wonder if there are known bounds for supermartingales (since $X_n$ is a supermartingale in the latter example) - they have not to be exponential of course. P.S. I put this question also here: https://mathoverflow.net/questions/68509/bounds-for-submartingale",,"['probability', 'stochastic-processes']"
30,Probability on spreading of rumors,Probability on spreading of rumors,,"A little help here. Exercise 21, Ch. 2 from Feller's book reads In a town a $n+1$ inhabitants, a person tells a rumor to a second person, who in turn repeats it to a third person, etc. At each step, the recipient of the rumor is chosen at random from the $n$ people available. Find the probability that the rumor will be told $r$ times without: a) returning to the originator, b) being repeated to any person. Do the same problem when at each step the rumor is told by one person to a gathering of $N$ randomly chosen people. (The first question is the special case N=1). I already did a) and b) for the first description of the problem and a) for the case when the rumor is spreading through a gathering of $N$ people, however, my solution for b) in this second case is not correct. I reasoned in the following way: In a first instance, $n$ people to receive the rumor, however, it's needed to spread such rumor through a group of $N$ people, therefore, there are $\displaystyle n \choose N$ ways to choose those gatherings. Once one of these people is chosen, he/she can choose from another gathering of $N$ people, taking care of not choosing someone who already know the rumor, which is, there are $\displaystyle n-1 \choose N$, and so on, until we reach the $r$ step in this process. Therefore, the probability I get is: $$\frac{\displaystyle {n \choose N} {n-1 \choose N} {n-2 \choose N} ... {n-r+1 \choose N}}{\displaystyle {n \choose N}^{r}}$$ According to the book, the solution must be $\displaystyle \frac{(n)_{Nr}}{(n_{N})^{r}}$ (which is not equivalent to the first expression). I will appreciate any help.","A little help here. Exercise 21, Ch. 2 from Feller's book reads In a town a $n+1$ inhabitants, a person tells a rumor to a second person, who in turn repeats it to a third person, etc. At each step, the recipient of the rumor is chosen at random from the $n$ people available. Find the probability that the rumor will be told $r$ times without: a) returning to the originator, b) being repeated to any person. Do the same problem when at each step the rumor is told by one person to a gathering of $N$ randomly chosen people. (The first question is the special case N=1). I already did a) and b) for the first description of the problem and a) for the case when the rumor is spreading through a gathering of $N$ people, however, my solution for b) in this second case is not correct. I reasoned in the following way: In a first instance, $n$ people to receive the rumor, however, it's needed to spread such rumor through a group of $N$ people, therefore, there are $\displaystyle n \choose N$ ways to choose those gatherings. Once one of these people is chosen, he/she can choose from another gathering of $N$ people, taking care of not choosing someone who already know the rumor, which is, there are $\displaystyle n-1 \choose N$, and so on, until we reach the $r$ step in this process. Therefore, the probability I get is: $$\frac{\displaystyle {n \choose N} {n-1 \choose N} {n-2 \choose N} ... {n-r+1 \choose N}}{\displaystyle {n \choose N}^{r}}$$ According to the book, the solution must be $\displaystyle \frac{(n)_{Nr}}{(n_{N})^{r}}$ (which is not equivalent to the first expression). I will appreciate any help.",,['probability']
31,Proving an upper bound for Prob[X>=E[X]],Proving an upper bound for Prob[X>=E[X]],,"Let random variable $X\sim\text{Binomial}\left(a+b,\frac{a}{a+b}\right)$, where $a$ and $b$ are positive integers. I'm trying to prove that $\mathbb{P}[X\geq\mathbb{E}[X]]\leq\frac{3}{4}$, which appears to be true numerically. Does anyone have a suggestion on how to proceed? The complication in this particular problem is that the condition is not a strict inequality ""$>$"". I've tried the Chernoff bound but it's not tight enough.","Let random variable $X\sim\text{Binomial}\left(a+b,\frac{a}{a+b}\right)$, where $a$ and $b$ are positive integers. I'm trying to prove that $\mathbb{P}[X\geq\mathbb{E}[X]]\leq\frac{3}{4}$, which appears to be true numerically. Does anyone have a suggestion on how to proceed? The complication in this particular problem is that the condition is not a strict inequality ""$>$"". I've tried the Chernoff bound but it's not tight enough.",,['probability']
32,Expected value of a function of a random variable: help!,Expected value of a function of a random variable: help!,,"I am trying to show the following: \begin{equation*} E[e^{-\gamma W}]=e^{-\gamma(E[W]-\frac{\gamma}{2}Var [W])} \end{equation*} but I really can't remember what I am supposed to do to get from the LHS to the RHS. I have tried using integration this way \begin{equation*} \int We^{-\gamma W}dW \end{equation*} and then use integration by parts, but even though what I get resembles it, it can't be correct (because $e^{-\gamma W}$ is not the distribution of W). I have also tried using Taylor series expansion, but I think I am way off, and I don't think an approximation here is what I need, because the equality above is exact. FYI, this is not homework, I am working through a paper (page 10) and I would really like to know how every step was derived. Can anyone at least point me to the right direction? EDIT : This expectation on the RHS is very similar to the moment generating function formula (with a negative exponent). If you check here , you will see that the moment generating function for the normal distribution is like the LHS (but with a positive sign). So in a way I have my answer, but I still would like to know how to derive it, if there is a way. I know little if anything at all about moment generating functions, so maybe I shouldn't try and derive it but rather just use the result? Does it even make sense to try and derive it?","I am trying to show the following: \begin{equation*} E[e^{-\gamma W}]=e^{-\gamma(E[W]-\frac{\gamma}{2}Var [W])} \end{equation*} but I really can't remember what I am supposed to do to get from the LHS to the RHS. I have tried using integration this way \begin{equation*} \int We^{-\gamma W}dW \end{equation*} and then use integration by parts, but even though what I get resembles it, it can't be correct (because $e^{-\gamma W}$ is not the distribution of W). I have also tried using Taylor series expansion, but I think I am way off, and I don't think an approximation here is what I need, because the equality above is exact. FYI, this is not homework, I am working through a paper (page 10) and I would really like to know how every step was derived. Can anyone at least point me to the right direction? EDIT : This expectation on the RHS is very similar to the moment generating function formula (with a negative exponent). If you check here , you will see that the moment generating function for the normal distribution is like the LHS (but with a positive sign). So in a way I have my answer, but I still would like to know how to derive it, if there is a way. I know little if anything at all about moment generating functions, so maybe I shouldn't try and derive it but rather just use the result? Does it even make sense to try and derive it?",,"['probability-theory', 'taylor-expansion', 'probability']"
33,Why is this optimization problem difficult?,Why is this optimization problem difficult?,,"A Normal (Gaussian) Probability Distribution Function can be defined as follows: $$f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{ -\frac{(x-\mu)^2}{2\sigma^2} }$$ Now, we can create an linear weighted (with weights $\pi_i$ ) sum of these Normal Distributions. This is usually referred to as a Mixture Distribution ( https://en.wikipedia.org/wiki/Mixture_distribution ): \begin{align*} p(x|\theta) &= \sum_{i=1}^{k} \pi_i \mathcal{N}(x|\mu_i, \sigma_i^2)  \end{align*} $$\sum_{i=1}^{k} \pi_i = 1$$ Now, suppose we want to estimate all parameters (i.e. $\theta$ = $\pi_i$ , $\mu_i$ , $\sigma_i$ )  of this function (i.e. all $\pi_i$ , $\mu_i$ , $\sigma_i$ ) In Statistics, we usually estimate these parameters using Maximum Likelihood Theory ( https://en.wikipedia.org/wiki/Maximum_likelihood_estimation ): \begin{align*} L(\theta|X) &= \prod_{j=1}^{n} p(x_j|\theta) = \prod_{j=1}^{n} \sum_{i=1}^{k} \pi_i \mathcal{N}(x_j|\mu_i, \sigma_i^2) \end{align*} From here, we would set the following derivatives to 0 and solve for these parameters (for all $i$ ): $$\frac{\partial}{\partial \pi_i} \log L(\theta|X) = 0 $$ $$\frac{\partial}{\partial \mu_i} \log L(\theta|X) = 0 $$ $$\frac{\partial}{\partial \sigma_i} \log L(\theta|X) = 0 $$ However, apparently if we try to solve the above optimization problem using standard Maximum Likelihood Estimation (MLE), we run into a problem of ""Identifiability"". Supposedly, one of the main problem that arises when using MLE is that for a given set of $\mu_i$ , $\sigma_i$ - the values of $\pi_i$ can be interchanged with one another and still produce the same value of the objective function being optimized. But I don't really understand why this is a bad thing. This just means that there are many solutions that satisfy this optimization problem: Nothing good, nothing bad - just a fact. There are many (equally) valid solutions to this optimization problem. To fix this supposed problem, the EM algorithm ( https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm ) is used. My Question: I don't understand why this happens. Suppose if I just proceed anyways and use MLE - how is this a undesirable? And how would using EM fix this supposed problem? Could someone please help me understand why MLE is not well suited for the above optimization problem and why the EM algorithm can be beneficial here? If there are multiple values of $\pi_i$ that can satisfy this optimization problem - won't the EM algorithm run into the same problem? Thanks! Note: I have heard some people say that the EM algorithm is sometimes more computationally efficient/feasible  than MLE - but this is not the point I am interested in. I also heard that the function being optimized  (mixture distribution) is multi-modal and is a difficult function to optimize through MLE - but I am not sure how exactly the EM algorithm has an advantage over MLE in this regard. I heard that the main advantage of the EM algorithm is that it can remedy fundamental mathematical problems (i.e. identifiability) that MLE runs into - but I can't understand what these problems are to begin with and why EM can be useful compared to MLE in this regard. How does the EM algorithm handle identifiability problems better than MLE? References: https://stephens999.github.io/fiveMinuteStats/intro_to_em.html https://link.springer.com/article/10.1007/s42952-022-00180-6 https://stats.stackexchange.com/questions/262538/why-expectation-maximization-is-important-for-mixture-models https://people.csail.mit.edu/rameshvs/content/gmm-em.pdf https://ardianumam.wordpress.com/2017/11/07/how-em-expectation-maximization-method-works-for-clustering/ https://www.youtube.com/watch?v=f2HIW37Ohho","A Normal (Gaussian) Probability Distribution Function can be defined as follows: Now, we can create an linear weighted (with weights ) sum of these Normal Distributions. This is usually referred to as a Mixture Distribution ( https://en.wikipedia.org/wiki/Mixture_distribution ): Now, suppose we want to estimate all parameters (i.e. = , , )  of this function (i.e. all , , ) In Statistics, we usually estimate these parameters using Maximum Likelihood Theory ( https://en.wikipedia.org/wiki/Maximum_likelihood_estimation ): From here, we would set the following derivatives to 0 and solve for these parameters (for all ): However, apparently if we try to solve the above optimization problem using standard Maximum Likelihood Estimation (MLE), we run into a problem of ""Identifiability"". Supposedly, one of the main problem that arises when using MLE is that for a given set of , - the values of can be interchanged with one another and still produce the same value of the objective function being optimized. But I don't really understand why this is a bad thing. This just means that there are many solutions that satisfy this optimization problem: Nothing good, nothing bad - just a fact. There are many (equally) valid solutions to this optimization problem. To fix this supposed problem, the EM algorithm ( https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm ) is used. My Question: I don't understand why this happens. Suppose if I just proceed anyways and use MLE - how is this a undesirable? And how would using EM fix this supposed problem? Could someone please help me understand why MLE is not well suited for the above optimization problem and why the EM algorithm can be beneficial here? If there are multiple values of that can satisfy this optimization problem - won't the EM algorithm run into the same problem? Thanks! Note: I have heard some people say that the EM algorithm is sometimes more computationally efficient/feasible  than MLE - but this is not the point I am interested in. I also heard that the function being optimized  (mixture distribution) is multi-modal and is a difficult function to optimize through MLE - but I am not sure how exactly the EM algorithm has an advantage over MLE in this regard. I heard that the main advantage of the EM algorithm is that it can remedy fundamental mathematical problems (i.e. identifiability) that MLE runs into - but I can't understand what these problems are to begin with and why EM can be useful compared to MLE in this regard. How does the EM algorithm handle identifiability problems better than MLE? References: https://stephens999.github.io/fiveMinuteStats/intro_to_em.html https://link.springer.com/article/10.1007/s42952-022-00180-6 https://stats.stackexchange.com/questions/262538/why-expectation-maximization-is-important-for-mixture-models https://people.csail.mit.edu/rameshvs/content/gmm-em.pdf https://ardianumam.wordpress.com/2017/11/07/how-em-expectation-maximization-method-works-for-clustering/ https://www.youtube.com/watch?v=f2HIW37Ohho","f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{ -\frac{(x-\mu)^2}{2\sigma^2} } \pi_i \begin{align*}
p(x|\theta) &= \sum_{i=1}^{k} \pi_i \mathcal{N}(x|\mu_i, \sigma_i^2) 
\end{align*} \sum_{i=1}^{k} \pi_i = 1 \theta \pi_i \mu_i \sigma_i \pi_i \mu_i \sigma_i \begin{align*}
L(\theta|X) &= \prod_{j=1}^{n} p(x_j|\theta) = \prod_{j=1}^{n} \sum_{i=1}^{k} \pi_i \mathcal{N}(x_j|\mu_i, \sigma_i^2)
\end{align*} i \frac{\partial}{\partial \pi_i} \log L(\theta|X) = 0  \frac{\partial}{\partial \mu_i} \log L(\theta|X) = 0  \frac{\partial}{\partial \sigma_i} \log L(\theta|X) = 0  \mu_i \sigma_i \pi_i \pi_i","['probability', 'optimization']"
34,Find the probability of getting 2 golden coins from a bag of 4 golden coins and 8 iron coins with the following conditions:,Find the probability of getting 2 golden coins from a bag of 4 golden coins and 8 iron coins with the following conditions:,,"There are 4 golden coins and 8 iron coins in a bag. You select one coin from the bag, if it is a golden coin, you keep it; but if it is an iron coin, you put it back in the bag. Find the probability of earning exactly 2 golden coins after three consecutive attempt. My Try: Golden Coins=4 Iron coins = 8 Total coins = 12 We have 3 cases here: (GIG, GGI, IGG) $ (\frac{4}{12}\times \frac{8}{11}\times \frac{3}{11})+(\frac{4}{12}\times \frac{3}{11}\times \frac{8}{10})+(\frac{8}{12}\times \frac{4}{12}\times \frac{3}{11}) = \frac{362}{1815} $ Is this answer correct?","There are 4 golden coins and 8 iron coins in a bag. You select one coin from the bag, if it is a golden coin, you keep it; but if it is an iron coin, you put it back in the bag. Find the probability of earning exactly 2 golden coins after three consecutive attempt. My Try: Golden Coins=4 Iron coins = 8 Total coins = 12 We have 3 cases here: (GIG, GGI, IGG) Is this answer correct?", (\frac{4}{12}\times \frac{8}{11}\times \frac{3}{11})+(\frac{4}{12}\times \frac{3}{11}\times \frac{8}{10})+(\frac{8}{12}\times \frac{4}{12}\times \frac{3}{11}) = \frac{362}{1815} ,"['probability', 'conditional-probability']"
35,Finite entropy and finite moments,Finite entropy and finite moments,,"This question is about when a discrete distribution has finite entropy because it has finite moments. Let $X$ be a discrete random variable (which can be positive, negative, and/or zero unless stated otherwise), and let— $$H(X) = \sum_n -\mathbb{P}[X=n] \ln(\mathbb{P}[X=n]),$$ be the Shannon entropy of $X$ . It is known that: If $X$ is integer-valued and has a finite variance, then $H(X)$ is finite (Massey 1988). If $X$ is integer-valued, is 0 or greater, and has a finite mean, then $H(X)$ is finite (Rioul 2022). It is also known that a discrete distribution can have infinite Shannon entropy even if it's positive-valued (one example is some members of the zeta Dirichlet distribution); see also Devroye and Gravel 2020. However, I believe that this is so because the infinite-entropy zeta Dirichlet distributions (I think) have an infinite $z$ th moment for any real $z>0$ . Moreover, I believe that the Cauchy distribution has a finite entropy even though its mean is infinite because it does have a finite $z$ th moment for some $z$ in $(0, 1)$ , in fact for every $z$ in $(0, 1)$ . However, several questions remain on the relationship between finite entropy and finite moments. Questions If $X$ is supported on the whole set of integers and is such that $\mathbb{E}[X^2]$ is infinite but $\mathbb{E}[X^z]$ is finite for some $z$ in $[1, 2)$ , then is $H(X)$ finite? If not, under what additional conditions is $H(X)$ finite? If $X$ is supported on the integers or some subset thereof, such that $\mathbb{E}[X]$ is infinite but $\mathbb{E}[X^z]$ is finite for every $z$ in $(0, 1)$ , then is $H(X)$ finite? If not, under what additional conditions is $H(X)$ finite? If $X$ is supported on the integers or some subset thereof, such that $\mathbb{E}[X]$ is infinite but $\mathbb{E}[X^z]$ is finite for some $z$ in $(0, 1)$ , then is $H(X)$ finite? If not, under what additional conditions is $H(X)$ finite? If $X$ is supported on the integers or some subset thereof, such that $\mathbb{E}[X^z]$ is infinite for every real $z > 0$ , then is $H(X)$ infinite? Motivation My motivation is to characterize the discrete distributions with finite entropy, since only finite-entropy distributions can be sampled in finite time on average (Knuth and Yao 1976). References O. Rioul, ""Variations on a Theme by Massey,"" in IEEE Transactions on Information Theory, doi: 10.1109/TIT.2022.3141264. Massey, J.L., ""On the entropy of integer-valued random variables"", 1988. Devroye, L., Gravel, C., "" Random variate generation using only finitely many unbiased, independently and identically distributed random bits "", arXiv:1502.02539v6  [cs.IT], 2020. Knuth, Donald E. and Andrew Chi-Chih Yao. ""The complexity of nonuniform random number generation"", in Algorithms and Complexity: New Directions and Recent Results , 1976.","This question is about when a discrete distribution has finite entropy because it has finite moments. Let be a discrete random variable (which can be positive, negative, and/or zero unless stated otherwise), and let— be the Shannon entropy of . It is known that: If is integer-valued and has a finite variance, then is finite (Massey 1988). If is integer-valued, is 0 or greater, and has a finite mean, then is finite (Rioul 2022). It is also known that a discrete distribution can have infinite Shannon entropy even if it's positive-valued (one example is some members of the zeta Dirichlet distribution); see also Devroye and Gravel 2020. However, I believe that this is so because the infinite-entropy zeta Dirichlet distributions (I think) have an infinite th moment for any real . Moreover, I believe that the Cauchy distribution has a finite entropy even though its mean is infinite because it does have a finite th moment for some in , in fact for every in . However, several questions remain on the relationship between finite entropy and finite moments. Questions If is supported on the whole set of integers and is such that is infinite but is finite for some in , then is finite? If not, under what additional conditions is finite? If is supported on the integers or some subset thereof, such that is infinite but is finite for every in , then is finite? If not, under what additional conditions is finite? If is supported on the integers or some subset thereof, such that is infinite but is finite for some in , then is finite? If not, under what additional conditions is finite? If is supported on the integers or some subset thereof, such that is infinite for every real , then is infinite? Motivation My motivation is to characterize the discrete distributions with finite entropy, since only finite-entropy distributions can be sampled in finite time on average (Knuth and Yao 1976). References O. Rioul, ""Variations on a Theme by Massey,"" in IEEE Transactions on Information Theory, doi: 10.1109/TIT.2022.3141264. Massey, J.L., ""On the entropy of integer-valued random variables"", 1988. Devroye, L., Gravel, C., "" Random variate generation using only finitely many unbiased, independently and identically distributed random bits "", arXiv:1502.02539v6  [cs.IT], 2020. Knuth, Donald E. and Andrew Chi-Chih Yao. ""The complexity of nonuniform random number generation"", in Algorithms and Complexity: New Directions and Recent Results , 1976.","X H(X) = \sum_n -\mathbb{P}[X=n] \ln(\mathbb{P}[X=n]), X X H(X) X H(X) z z>0 z z (0, 1) z (0, 1) X \mathbb{E}[X^2] \mathbb{E}[X^z] z [1, 2) H(X) H(X) X \mathbb{E}[X] \mathbb{E}[X^z] z (0, 1) H(X) H(X) X \mathbb{E}[X] \mathbb{E}[X^z] z (0, 1) H(X) H(X) X \mathbb{E}[X^z] z > 0 H(X)","['probability', 'statistics', 'entropy']"
36,Cutting a galette and hitting the fève,Cutting a galette and hitting the fève,,"We have in France a tradition of eating in January countless galettes des rois (*) . Hidden inside is a fève , a small figurine (it was originally a coin). The one who gets the fève without breaking a tooth is crowned queen or king. To give some context, the home-made galette we ate today, together with its fève As I was cutting the galette , my son asked I wonder what the probability to hit the fève when making a cut is? Now I wonder as well. In the tradition of spherical cows in a vacuum, a galette with its fève can be simplified as where $r_g$ and $r_f$ are the radii of, respectively, the galette and the fève . $d_f$ is the distance of the center of the fève from the center of the galette . EDIT: the placement of the fève is random. Asking for a full calculation of the probability would be too much :), so my question is: how should I approach this calculation , especially since it will be dependent on $d_f$ (which will probably have a squared distribution). Any hints and warnings are welcome (**) . (*) We are of course talking about the only proper one - the northern one (in case someone has doubts from Wikipedia). The proper drink for a galette des rois is cidre , of course from Brittany (**) The prize could be a part of the galette but it is already gone.","We have in France a tradition of eating in January countless galettes des rois (*) . Hidden inside is a fève , a small figurine (it was originally a coin). The one who gets the fève without breaking a tooth is crowned queen or king. To give some context, the home-made galette we ate today, together with its fève As I was cutting the galette , my son asked I wonder what the probability to hit the fève when making a cut is? Now I wonder as well. In the tradition of spherical cows in a vacuum, a galette with its fève can be simplified as where and are the radii of, respectively, the galette and the fève . is the distance of the center of the fève from the center of the galette . EDIT: the placement of the fève is random. Asking for a full calculation of the probability would be too much :), so my question is: how should I approach this calculation , especially since it will be dependent on (which will probably have a squared distribution). Any hints and warnings are welcome (**) . (*) We are of course talking about the only proper one - the northern one (in case someone has doubts from Wikipedia). The proper drink for a galette des rois is cidre , of course from Brittany (**) The prize could be a part of the galette but it is already gone.",r_g r_f d_f d_f,"['probability', 'recreational-mathematics']"
37,Explain to a 15 y.o. Byron Schmuland's answer that uses Summation and Product notations to solve the Crazy Lady Airplane Seat probability problem?,Explain to a 15 y.o. Byron Schmuland's answer that uses Summation and Product notations to solve the Crazy Lady Airplane Seat probability problem?,,"Byron Schmuland's answer 1 is too abstruse for a 15 y.o. student who needs details! Please see my enumerated questions below. Let's find the chance that any customer ends up in the wrong seat. For $2\leq k\leq n$ , customer $k$ will get bumped when he finds his seat occupied by someone with a smaller number, who was also bumped by someone with a smaller number, and so on back to customer $1$ . This process can be summarized by the diagram $$1\longrightarrow j_1\longrightarrow j_2\longrightarrow\cdots\longrightarrow j_m\longrightarrow k.$$ Here $j_1<j_2<\cdots <j_m$ is any (possibly empty) increasing sequence of integers strictly between $1$ and $k$ . The probability of this sequence of events is $${1\over n}\times{1\over(n+1)-j_1}\times {1\over(n+1)-j_2}\times\cdots\times{1\over(n+1)-j_m}.$$ What exactly does $1/n$ , the first term, signify? Where do each of the ${1\over(n+1)-j_m}$ terms hail from and signify? How would you divine or forebode to formulate this term? Thus, the probability that customer $k$ gets bumped is $$p(k)={1\over n}{\color{red}{\sum}}\prod_{\ell=1}^m  {1\over(n+1)-j_\ell}$$ where the sum is over all sets of $j$ values $1<j_1<j_2<\cdots <j_m<k$ . That is, Where did the $\color{red}{\sum\limits_{1<j_1<j_2<\cdots <j_m<k}}$ stem from? It appears to come out of left field! \begin{eqnarray*} p(k)&=&{1\over n}\color{limegreen}{\sum_{J\subseteq\{2,\dots,k-1\}}}\ \, \prod_{j\in J}{1\over (n+1)-j}\cr     &=&{1\over n}\ \, \prod_{j=2}^{k-1} \left(1+{1\over (n+1)-j}\right) \cr      &=&\color{sienna}{{1\over n}\ \,\prod_{j=2}^{k-1} {(n+2)-j\over (n+1)-j}}\cr     &=&\color{sienna}{1\over n+2-k}. \end{eqnarray*} How does $\color{red}{\sum\limits_{1<j_1<j_2<\cdots <j_m<k}} \equiv \color{limegreen}{\sum\limits_{J\subseteq\{2,\dots,k-1\}}}$ ? How does $\color{sienna}{{1\over n}\ \,\prod\limits_{j=2}^{k-1} {(n+2)-j\over (n+1)-j}={1\over n+2-k}}$ ? In the case $k=n$ , we get $p(n)=1/2$ as in the other solutions. Maybe there is an intuitive explanation of the general formula; I couldn't think of one. Added reference: Finding your seat versus tossing a coin by Yared Nigussie, American Mathematical Monthly 121, June-July 2014,  545-546 . 1 I currently see no answer on this page by someone called ""Byron Schmuland"" , but this other question refers to the answerer as Byron Schmuland, and there's merely one deleted user. Then I deduced that user940 (the deleted user) was University of Alberta Prof. Byron Schmuland PhD Carleton University 1987.","Byron Schmuland's answer 1 is too abstruse for a 15 y.o. student who needs details! Please see my enumerated questions below. Let's find the chance that any customer ends up in the wrong seat. For , customer will get bumped when he finds his seat occupied by someone with a smaller number, who was also bumped by someone with a smaller number, and so on back to customer . This process can be summarized by the diagram Here is any (possibly empty) increasing sequence of integers strictly between and . The probability of this sequence of events is What exactly does , the first term, signify? Where do each of the terms hail from and signify? How would you divine or forebode to formulate this term? Thus, the probability that customer gets bumped is where the sum is over all sets of values . That is, Where did the stem from? It appears to come out of left field! How does ? How does ? In the case , we get as in the other solutions. Maybe there is an intuitive explanation of the general formula; I couldn't think of one. Added reference: Finding your seat versus tossing a coin by Yared Nigussie, American Mathematical Monthly 121, June-July 2014,  545-546 . 1 I currently see no answer on this page by someone called ""Byron Schmuland"" , but this other question refers to the answerer as Byron Schmuland, and there's merely one deleted user. Then I deduced that user940 (the deleted user) was University of Alberta Prof. Byron Schmuland PhD Carleton University 1987.","2\leq k\leq n k 1 1\longrightarrow j_1\longrightarrow j_2\longrightarrow\cdots\longrightarrow j_m\longrightarrow k. j_1<j_2<\cdots <j_m 1 k {1\over n}\times{1\over(n+1)-j_1}\times {1\over(n+1)-j_2}\times\cdots\times{1\over(n+1)-j_m}. 1/n {1\over(n+1)-j_m} k p(k)={1\over n}{\color{red}{\sum}}\prod_{\ell=1}^m  {1\over(n+1)-j_\ell} j 1<j_1<j_2<\cdots <j_m<k \color{red}{\sum\limits_{1<j_1<j_2<\cdots <j_m<k}} \begin{eqnarray*}
p(k)&=&{1\over n}\color{limegreen}{\sum_{J\subseteq\{2,\dots,k-1\}}}\ \, \prod_{j\in J}{1\over (n+1)-j}\cr
    &=&{1\over n}\ \, \prod_{j=2}^{k-1} \left(1+{1\over (n+1)-j}\right) \cr 
    &=&\color{sienna}{{1\over n}\ \,\prod_{j=2}^{k-1} {(n+2)-j\over (n+1)-j}}\cr
    &=&\color{sienna}{1\over n+2-k}.
\end{eqnarray*} \color{red}{\sum\limits_{1<j_1<j_2<\cdots <j_m<k}} \equiv \color{limegreen}{\sum\limits_{J\subseteq\{2,\dots,k-1\}}} \color{sienna}{{1\over n}\ \,\prod\limits_{j=2}^{k-1} {(n+2)-j\over (n+1)-j}={1\over n+2-k}} k=n p(n)=1/2",[]
38,"A dice puzzle and an ""obvious"" fact I cannot prove [duplicate]","A dice puzzle and an ""obvious"" fact I cannot prove [duplicate]",,"This question already has answers here : Expected outcome for repeated dice rolls with dice fixing (4 answers) Closed 2 years ago . Background This question is inspired by this 538 ""Riddler Classic"" puzzle , and the following puzzle explanation below is copied from there: You have four standard dice, and your goal is simple: Maximize the sum of your rolls. So you roll all four dice at once, hoping to achieve a high score. But wait, there’s more! If you’re not happy with your roll, you can choose to reroll zero, one, two or three of the dice. In other words, you must “freeze” one or more dice and set them aside, never to be rerolled. You repeat this process with the remaining dice — you roll them all and then freeze at least one. You repeat this process until all the dice are frozen. If you play strategically, what score can you expect to achieve on average? Extra credit: Instead of four dice, what if you start with five dice? What if you start with six dice? What if you start with N dice? Question We are interested only in the general $n$ case, and we are interested in a question tangential to the puzzle's solution . Assuming perfect play, consider the expected value (EV) of your sum when starting with $n$ dice, which we'll denote $E_n$ . The question is: Is it possible that, for some $n$ , the following holds? $$E_{n+1} > E_n + 6$$ Alternatively, should you ever choose to re-roll a six when playing optimally? Intuitively, it seems like the answer to both of these equivalent questions should be ""no"".  But I cannot think of a rigorous argument to prove it.","This question already has answers here : Expected outcome for repeated dice rolls with dice fixing (4 answers) Closed 2 years ago . Background This question is inspired by this 538 ""Riddler Classic"" puzzle , and the following puzzle explanation below is copied from there: You have four standard dice, and your goal is simple: Maximize the sum of your rolls. So you roll all four dice at once, hoping to achieve a high score. But wait, there’s more! If you’re not happy with your roll, you can choose to reroll zero, one, two or three of the dice. In other words, you must “freeze” one or more dice and set them aside, never to be rerolled. You repeat this process with the remaining dice — you roll them all and then freeze at least one. You repeat this process until all the dice are frozen. If you play strategically, what score can you expect to achieve on average? Extra credit: Instead of four dice, what if you start with five dice? What if you start with six dice? What if you start with N dice? Question We are interested only in the general case, and we are interested in a question tangential to the puzzle's solution . Assuming perfect play, consider the expected value (EV) of your sum when starting with dice, which we'll denote . The question is: Is it possible that, for some , the following holds? Alternatively, should you ever choose to re-roll a six when playing optimally? Intuitively, it seems like the answer to both of these equivalent questions should be ""no"".  But I cannot think of a rigorous argument to prove it.",n n E_n n E_{n+1} > E_n + 6,"['probability', 'combinatorics', 'dice']"
39,"I throw a coin 10 times, and I get all heads, what is strange here? [duplicate]","I throw a coin 10 times, and I get all heads, what is strange here? [duplicate]",,"This question already has answers here : Why is observing 100 heads for a fair coin flips surprising? (10 answers) Closed 3 years ago . I take a coin and with the assumption that it is a fair coin. I throw it 10 times and I get a sequence of 10 consecutive heads. I feel something is unusual and strange, may be the coin is not fair. But the outcome I got is as likely as any other sequences of heads and tails. However there is something really strange here, the probability of getting no tails in 10 throws of a fair coin is really small. So indeed there is something strange here. And common sense says the coin is probably not fair. But isn't ""containing no tails"" just an arbitrary property of my outcome? May be my outcome is not unusual considering lots of other properties? Or may be you can come by an unusual property for any sequence of heads and tails? My question is why seeing no tails in 10 throws is a good reason to doubt fairness of the coin? P.S.: To abide with the laws of stackexchange my formal question is what I stated above. But my real question is something more general and vague: when we see something strange on what grounds we can say what we saw is just a low probability result of the way I think the world works or I should change my view about how the world works? Are there certain things that I should check before changing my view? I would be grateful if you can help me with my real question too.","This question already has answers here : Why is observing 100 heads for a fair coin flips surprising? (10 answers) Closed 3 years ago . I take a coin and with the assumption that it is a fair coin. I throw it 10 times and I get a sequence of 10 consecutive heads. I feel something is unusual and strange, may be the coin is not fair. But the outcome I got is as likely as any other sequences of heads and tails. However there is something really strange here, the probability of getting no tails in 10 throws of a fair coin is really small. So indeed there is something strange here. And common sense says the coin is probably not fair. But isn't ""containing no tails"" just an arbitrary property of my outcome? May be my outcome is not unusual considering lots of other properties? Or may be you can come by an unusual property for any sequence of heads and tails? My question is why seeing no tails in 10 throws is a good reason to doubt fairness of the coin? P.S.: To abide with the laws of stackexchange my formal question is what I stated above. But my real question is something more general and vague: when we see something strange on what grounds we can say what we saw is just a low probability result of the way I think the world works or I should change my view about how the world works? Are there certain things that I should check before changing my view? I would be grateful if you can help me with my real question too.",,['probability']
40,Probability question is bugging me,Probability question is bugging me,,"I recently saw a question somewhere where I got confused between what exactly I should do about it. Q. Imagine person A speaks truth 9 out of 10 times and another person B speaks truth 8 out of 10 times. A random card is picked from Jack, Queen and Kings (12 cards total). If both A and B say the random card is Jack of Clubs, what is the probability that the Jack of Clubs was not the picked card? A. In the answer the questioner said, the answer is supposed to be 1/144 because both are having 12 possibilities of saying something. I thought it was either 2/100 since then both have lied or 1/37 since if both say same card, then either both are lying or both are truthful and hence 2/(2+72) . Please tell me which is the correct answer and also please explain why. I am getting confused because of the questioners answer ignoring the truthfulness of A and B's claim.","I recently saw a question somewhere where I got confused between what exactly I should do about it. Q. Imagine person A speaks truth 9 out of 10 times and another person B speaks truth 8 out of 10 times. A random card is picked from Jack, Queen and Kings (12 cards total). If both A and B say the random card is Jack of Clubs, what is the probability that the Jack of Clubs was not the picked card? A. In the answer the questioner said, the answer is supposed to be 1/144 because both are having 12 possibilities of saying something. I thought it was either 2/100 since then both have lied or 1/37 since if both say same card, then either both are lying or both are truthful and hence 2/(2+72) . Please tell me which is the correct answer and also please explain why. I am getting confused because of the questioners answer ignoring the truthfulness of A and B's claim.",,['probability']
41,Monte-Carlo integration,Monte-Carlo integration,,"Let a function $f$ to be $x\in \left[a,b\right],\:0\le f\left(x\right)\le c$ . We want to calculate the approximation of the definite integral of the function in the range $[a,b]$ , we can suppose that the exact integral is very difficult to calculate in this range, but we can for all $x$ calculate $f(x)$ easily. We can sample a lot of points randomly $\left\{X_i,Y_i\right\}\:_{i=1}^N$ from the rectangle in the range: $x\in[a,b], y\in[0,c]$ . First of all, we need to find a way to calculate approximately the integral in the range $[a,b]$ My way: I succeeded to calculate and this will be $s'$ . ( $s$ - the original integral, $s'$ - the approximate integral) $s = \int _a^b\:f\left(x\right)dx$ $s'=c\left(b-a\right)\cdot \frac{1}{n}\sum _{j=1}^n\:I_j$ We need to calculate with $a,b,c,s,\epsilon,\delta$ how many points we need to sample for: $p(|s'-s|>\epsilon)<\delta$ and we need also to be helped by Chebyshev's inequality, but I have no idea how to go on with it.","Let a function to be . We want to calculate the approximation of the definite integral of the function in the range , we can suppose that the exact integral is very difficult to calculate in this range, but we can for all calculate easily. We can sample a lot of points randomly from the rectangle in the range: . First of all, we need to find a way to calculate approximately the integral in the range My way: I succeeded to calculate and this will be . ( - the original integral, - the approximate integral) We need to calculate with how many points we need to sample for: and we need also to be helped by Chebyshev's inequality, but I have no idea how to go on with it.","f x\in \left[a,b\right],\:0\le f\left(x\right)\le c [a,b] x f(x) \left\{X_i,Y_i\right\}\:_{i=1}^N x\in[a,b], y\in[0,c] [a,b] s' s s' s = \int _a^b\:f\left(x\right)dx s'=c\left(b-a\right)\cdot \frac{1}{n}\sum _{j=1}^n\:I_j a,b,c,s,\epsilon,\delta p(|s'-s|>\epsilon)<\delta","['probability', 'probability-theory']"
42,Characteristic function of the logistic distribution?,Characteristic function of the logistic distribution?,,"I recently came across a question in my graduate course where we have to calculate the characteristic function for the Logistic distribution. The Logistic distribution we are working with is given by the following PDF: $$ f(x) = \frac{e^{-x}}{(1 + e^{-x})^2}.  $$ The way that I went about doing this is the following: $$E\left[ e^{itX} \right] = E[\cos(tX)] + iE[\sin(tX)]. $$ The $E[\sin(tX)] = 0$ . The real problem for me comes when calculating $E[\cos(tX)]$ . I tried to express $\cos$ in its exponential representation, but I didn't get too far with that. Upon plugging this integral into WolframAlpha, it says that the hypergeometric function is used for it. Any thoughts on how I can analytically compute this? I'd be happy to use the hypergeometric function, but I don't quite see the connection between that and $\text{csch}(x)$ , which is part of the result that WolframAlpha gives (and this result matches the characteristic function listed for the Logistic distribution). Edit: I would like to be able to do this problem without a computer and solely pencil and paper. This is what I mean by an analytic solution.","I recently came across a question in my graduate course where we have to calculate the characteristic function for the Logistic distribution. The Logistic distribution we are working with is given by the following PDF: The way that I went about doing this is the following: The . The real problem for me comes when calculating . I tried to express in its exponential representation, but I didn't get too far with that. Upon plugging this integral into WolframAlpha, it says that the hypergeometric function is used for it. Any thoughts on how I can analytically compute this? I'd be happy to use the hypergeometric function, but I don't quite see the connection between that and , which is part of the result that WolframAlpha gives (and this result matches the characteristic function listed for the Logistic distribution). Edit: I would like to be able to do this problem without a computer and solely pencil and paper. This is what I mean by an analytic solution.","
f(x) = \frac{e^{-x}}{(1 + e^{-x})^2}. 
 E\left[ e^{itX} \right]
= E[\cos(tX)] + iE[\sin(tX)].
 E[\sin(tX)] = 0 E[\cos(tX)] \cos \text{csch}(x)",['probability']
43,Knife inside the square [closed],Knife inside the square [closed],,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 4 years ago . Improve this question A square of side $n$ is drawn in the sand. A knife of length $k$ is thrown into the square. The tip of the knife lands inside the square, and then the knife falls down to the ground in a random direction. The location of the tip of the knife is uniformly distributed within the square, and the direction in which the knife falls to the ground is uniformly distributed in $(0,2\pi]$ . What is the probability that the entire length of the knife now lies inside the square? What happens when we replace the square with a circle of diameter $n$ ?","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 4 years ago . Improve this question A square of side is drawn in the sand. A knife of length is thrown into the square. The tip of the knife lands inside the square, and then the knife falls down to the ground in a random direction. The location of the tip of the knife is uniformly distributed within the square, and the direction in which the knife falls to the ground is uniformly distributed in . What is the probability that the entire length of the knife now lies inside the square? What happens when we replace the square with a circle of diameter ?","n k (0,2\pi] n","['probability', 'geometry']"
44,Probability of a composite number passing the test,Probability of a composite number passing the test,,"Inspired by Theorem 5 in this paper I have created the following algorithm: Let us define polynomials $P_n^{(b)}(x)$ as follows : $$P_n^{(b)}(x)=\left(\frac{1}{2}\right)\cdot\left(\left(x-\sqrt{x^2+b}\right)^n+\left(x+\sqrt{x^2+b}\right)^n\right)$$ Test in pseudocode : Inputs : $n$ : a value to test for primality , $n>3$ ; $k$ : a parameter that determines the number of times to test for primality Output : composite if $n$ is composite, otherwise probably prime Repeat $k$ times : $\phantom{5}$ Pick $b$ randomly in the range $[-100,100]$ $\phantom{5}$ Pick $a$ randomly in the range $[2 , n − 2]$ $\phantom{5}$ If $P_n^{(b)}(a) \not\equiv a \pmod n$ , then return composite If composite is never returned: return probably prime You can run this test here . Unlike in Fermat primality test Carmichael numbers do not always pass this test. As a matter a fact, I don't know if any of them passes this test. Question: What is the probability of an arbitrary composite number  passing this test? Is it possible to estimate its value? EDIT The Android app that implements this test with $k=3$ can be found on Google Play . Python script that implements this test can be found here .","Inspired by Theorem 5 in this paper I have created the following algorithm: Let us define polynomials as follows : Test in pseudocode : Inputs : : a value to test for primality , ; : a parameter that determines the number of times to test for primality Output : composite if is composite, otherwise probably prime Repeat times : Pick randomly in the range Pick randomly in the range If , then return composite If composite is never returned: return probably prime You can run this test here . Unlike in Fermat primality test Carmichael numbers do not always pass this test. As a matter a fact, I don't know if any of them passes this test. Question: What is the probability of an arbitrary composite number  passing this test? Is it possible to estimate its value? EDIT The Android app that implements this test with can be found on Google Play . Python script that implements this test can be found here .","P_n^{(b)}(x) P_n^{(b)}(x)=\left(\frac{1}{2}\right)\cdot\left(\left(x-\sqrt{x^2+b}\right)^n+\left(x+\sqrt{x^2+b}\right)^n\right) n n>3 k n k \phantom{5} b [-100,100] \phantom{5} a [2 , n − 2] \phantom{5} P_n^{(b)}(a) \not\equiv a \pmod n k=3","['probability', 'elementary-number-theory', 'prime-numbers', 'algorithms', 'primality-test']"
45,What is the expected number of coin tosses it would take to get N many heads OR N many tails?,What is the expected number of coin tosses it would take to get N many heads OR N many tails?,,"Where we do NOT require that the heads or tails be consecutive (though they may be!) Obviously, this expectation, $E[T]$ , is bound as follows: $N < E[T] < 2N - 1$ And obviously $E[T] = \sum_{i=N}^{n=2N-1}i*P[Game \ Ends \ On \ i^{th} \ Round]$ , where $\sum_{i=N}^{n=2N-1}P[Game \ Ends \ On \ i^{th} \ Round] = 1$ But how would one find such a probability for an arbitrary $i \in \{N, N+1, ..., 2N-1\}$ ?","Where we do NOT require that the heads or tails be consecutive (though they may be!) Obviously, this expectation, , is bound as follows: And obviously , where But how would one find such a probability for an arbitrary ?","E[T] N < E[T] < 2N - 1 E[T] = \sum_{i=N}^{n=2N-1}i*P[Game \ Ends \ On \ i^{th} \ Round] \sum_{i=N}^{n=2N-1}P[Game \ Ends \ On \ i^{th} \ Round] = 1 i \in \{N, N+1, ..., 2N-1\}",['probability']
46,"Odds of assembling a jigsaw puzzle ""perfectly""","Odds of assembling a jigsaw puzzle ""perfectly""",,"A bunch of my coworkers have gotten way into assembling jigsaw puzzles during the workday, so I got this idea it'd be fun to bore them with random facts. I'm trying to think of the odds of assembling a 1000 piece jigsaw puzzle ""perfectly,"" i.e. choosing each piece such that it fits with one of the pieces already in the puzzle. Assuming that each piece is exactly 1 square inch, that means we probably have a 40x25"" puzzle. Using some basic deduction, I can conclude that this leaves 4 ""corner"" pieces with two possible connections, 122 ""edge"" pieces with three possible connections, and 874 internal pieces with four possible connections. From here, I'm sure that the answer has something to do with this: Jigsaw Probability Using that logic, the probability of our first two pieces matching is: (4/1000) * (2/999) + (122/1000)*(3/999) + (874/1000) * (4/999) The problem is, I can't figure out how to extrapolate this further, since the next iteration seems like it would depend on which type of piece was chosen. I'm not sure if this is the right approach, because I can envision a scenario where the puzzle is assembled from the center outward, after which every edge piece is guaranteed to fit with probability 1, and I'm not sure how the above sequence could be expanded in a way that accounts for that. The other approach I thought of was inspired by this: Proof by Induction: Puzzle Pieces Problem If I could derive the number of possible ""perfect"" sequences of 999 moves, I could just divide that number by 1000! total possible sequences of selections and that would be the probability. I might be missing something here though, as this line of thinking stands in contrast to this post: Probability of a n-piece puzzle being solved on the first try. Is my reasoning sound? ...which makes the assumption that there's only one possible way to select all pieces that solves it. He also gets into piece orientation which I don't really care about. Any thoughts are welcome!","A bunch of my coworkers have gotten way into assembling jigsaw puzzles during the workday, so I got this idea it'd be fun to bore them with random facts. I'm trying to think of the odds of assembling a 1000 piece jigsaw puzzle ""perfectly,"" i.e. choosing each piece such that it fits with one of the pieces already in the puzzle. Assuming that each piece is exactly 1 square inch, that means we probably have a 40x25"" puzzle. Using some basic deduction, I can conclude that this leaves 4 ""corner"" pieces with two possible connections, 122 ""edge"" pieces with three possible connections, and 874 internal pieces with four possible connections. From here, I'm sure that the answer has something to do with this: Jigsaw Probability Using that logic, the probability of our first two pieces matching is: (4/1000) * (2/999) + (122/1000)*(3/999) + (874/1000) * (4/999) The problem is, I can't figure out how to extrapolate this further, since the next iteration seems like it would depend on which type of piece was chosen. I'm not sure if this is the right approach, because I can envision a scenario where the puzzle is assembled from the center outward, after which every edge piece is guaranteed to fit with probability 1, and I'm not sure how the above sequence could be expanded in a way that accounts for that. The other approach I thought of was inspired by this: Proof by Induction: Puzzle Pieces Problem If I could derive the number of possible ""perfect"" sequences of 999 moves, I could just divide that number by 1000! total possible sequences of selections and that would be the probability. I might be missing something here though, as this line of thinking stands in contrast to this post: Probability of a n-piece puzzle being solved on the first try. Is my reasoning sound? ...which makes the assumption that there's only one possible way to select all pieces that solves it. He also gets into piece orientation which I don't really care about. Any thoughts are welcome!",,"['probability', 'combinatorics', 'recreational-mathematics']"
47,What's the precise statement of the continuous-time optional stopping theorem?,What's the precise statement of the continuous-time optional stopping theorem?,,"I searched high and low in a number of probability / financial mathematics textbooks and surprisingly cannot find any precise statement of the continuous time optional stopping theorem. In particular, none of the sources I find tells me what the conditions exactly are which allow us to apply the optional stopping theorem. Let's start with a much-talked-about example. Let $B_t$ be the standard Brownian motion and $\mathcal F_t$ be the filtration it generates. What's the expected time it takes for $B_t$ to hit either $-\alpha<0$ or $\beta>0$ ? Usually, we do as follows: let $\tau$ be a stopping time w.r.t. $\mathcal F_t$ defined as $$\tau = \inf_{t>0}\{t\mid B_t=-\alpha \vee B_t=\beta\}$$ Now, noting that $B_t$ and $B_t^2-t$ are both martingales, and then apply the optional stopping theorem on them with the stopping time $\tau$ . The problem is, why can we apply the optional stopping theorem in this case? I don't think we can apply this theorem without a set of conditions that the martingale and the stopping time must satisfy. So what are this set of conditions? Wikipedia gives the conditions of this theorem in discrete time. What I'm looking for are their continuous time counterparts.","I searched high and low in a number of probability / financial mathematics textbooks and surprisingly cannot find any precise statement of the continuous time optional stopping theorem. In particular, none of the sources I find tells me what the conditions exactly are which allow us to apply the optional stopping theorem. Let's start with a much-talked-about example. Let be the standard Brownian motion and be the filtration it generates. What's the expected time it takes for to hit either or ? Usually, we do as follows: let be a stopping time w.r.t. defined as Now, noting that and are both martingales, and then apply the optional stopping theorem on them with the stopping time . The problem is, why can we apply the optional stopping theorem in this case? I don't think we can apply this theorem without a set of conditions that the martingale and the stopping time must satisfy. So what are this set of conditions? Wikipedia gives the conditions of this theorem in discrete time. What I'm looking for are their continuous time counterparts.",B_t \mathcal F_t B_t -\alpha<0 \beta>0 \tau \mathcal F_t \tau = \inf_{t>0}\{t\mid B_t=-\alpha \vee B_t=\beta\} B_t B_t^2-t \tau,"['probability', 'probability-theory', 'stochastic-processes', 'martingales', 'stopping-times']"
48,Definition of ergodicity and ergodic process,Definition of ergodicity and ergodic process,,"I am confused by the definitions of ergodicity in wikipedia, see formal definition here which says that a measure-preserving transformation $T$ is ergodic if for every event $E$ , $T^{-1}(E) = E$ implies that $P(E)=0$ or $P(E)=1$ . Is this definition in anyway related to the definition of ergodic process here which talks about the statistics of a process being captured by a long trajectory sample? If so, can anyone demonstrate the relation to me? Also, what property must a stochastic process $X(t)$ possess such that a sample of the process with a very long time trajectory can be used to infer statistical properties of $X(t)$ for any time $t$ ? Is it ergodicity and stationary in the strict sense?","I am confused by the definitions of ergodicity in wikipedia, see formal definition here which says that a measure-preserving transformation is ergodic if for every event , implies that or . Is this definition in anyway related to the definition of ergodic process here which talks about the statistics of a process being captured by a long trajectory sample? If so, can anyone demonstrate the relation to me? Also, what property must a stochastic process possess such that a sample of the process with a very long time trajectory can be used to infer statistical properties of for any time ? Is it ergodicity and stationary in the strict sense?",T E T^{-1}(E) = E P(E)=0 P(E)=1 X(t) X(t) t,"['probability', 'probability-theory', 'stochastic-processes', 'dynamical-systems', 'ergodic-theory']"
49,Urn and probabiblity,Urn and probabiblity,,"Two urns contain n balls each, numbered from 1 to n. We pick a ball from the first one and then a ball from the second. What is the probability that the number of the second ball is a) smaller b) equal to the number of the first ball? My humble attempt: b) $\frac{1}{n}$ a) Suppose the number picked is $0< k\leq n$ So we have $k-1$ numbers $< k$ and $n-k$ numbers $> k$ . Isn't the probability $\frac{n-k}{n}$ ?","Two urns contain n balls each, numbered from 1 to n. We pick a ball from the first one and then a ball from the second. What is the probability that the number of the second ball is a) smaller b) equal to the number of the first ball? My humble attempt: b) a) Suppose the number picked is So we have numbers and numbers . Isn't the probability ?",\frac{1}{n} 0< k\leq n k-1 < k n-k > k \frac{n-k}{n},['probability']
50,Does the expected value of the derivative of the log likelihood evaluated at a certain parameter always have negative mean?,Does the expected value of the derivative of the log likelihood evaluated at a certain parameter always have negative mean?,,"It is well known that the derivative of the log likelihood with respect to the parameter of interest (the score) has zero expected value. Assuming $f(z;\theta)$ is a probability density function the quick version of the proof (skipping the technicalities in swapping the derivative and the integral) is $$\int \frac{\partial f(z;\theta)}{\partial \theta}dz=0 \Leftrightarrow \int \frac{f(z;\theta)}{f(z;\theta)} \frac{\partial f(z;\theta)}{\partial \theta}dz=0\Leftrightarrow \int f(z;\theta) \frac{\partial \log f(z;\theta)}{\partial \theta}dz=0$$ I was wondering if taking $\theta_1 > \theta$ the expected value of the score evaluated at $\theta_1$ $$\int f(z;\theta) \frac{\partial \log f(z;\theta_1)}{\partial \theta_1}dz$$ was always negative. I have tried this for the mean and variance of the normal distribution and for the parameter of the exponential distribution and it holds. Here is the derivation for the exponential: Assume $X \sim exp( \lambda)$. Take $\lambda_1 > \lambda$ the derivative of the log likelihood of the exponential density with one observation is $$S(\lambda, x):=\frac{\partial }{ \partial \lambda} (\log(\lambda) - \lambda x) = \frac{1}{\lambda} - x$$ So $$E[S(\lambda_1, X)] = E \left[  \frac{1}{\lambda_1} - X \right] = \frac{1}{\lambda_1} - \frac{1}{\lambda} < 0.$$ does this hold in general? EDIT: I was working on a proof only for the exponential family of distributions but couldn't quite make it, even a subcase like the exponential family would be interesting to me. EDIT2: after thinking about this for a while I think an equivalent way to state the problem is: ""when is the maximum likelihood estimator unbiased"". So, when is it unbiased?","It is well known that the derivative of the log likelihood with respect to the parameter of interest (the score) has zero expected value. Assuming $f(z;\theta)$ is a probability density function the quick version of the proof (skipping the technicalities in swapping the derivative and the integral) is $$\int \frac{\partial f(z;\theta)}{\partial \theta}dz=0 \Leftrightarrow \int \frac{f(z;\theta)}{f(z;\theta)} \frac{\partial f(z;\theta)}{\partial \theta}dz=0\Leftrightarrow \int f(z;\theta) \frac{\partial \log f(z;\theta)}{\partial \theta}dz=0$$ I was wondering if taking $\theta_1 > \theta$ the expected value of the score evaluated at $\theta_1$ $$\int f(z;\theta) \frac{\partial \log f(z;\theta_1)}{\partial \theta_1}dz$$ was always negative. I have tried this for the mean and variance of the normal distribution and for the parameter of the exponential distribution and it holds. Here is the derivation for the exponential: Assume $X \sim exp( \lambda)$. Take $\lambda_1 > \lambda$ the derivative of the log likelihood of the exponential density with one observation is $$S(\lambda, x):=\frac{\partial }{ \partial \lambda} (\log(\lambda) - \lambda x) = \frac{1}{\lambda} - x$$ So $$E[S(\lambda_1, X)] = E \left[  \frac{1}{\lambda_1} - X \right] = \frac{1}{\lambda_1} - \frac{1}{\lambda} < 0.$$ does this hold in general? EDIT: I was working on a proof only for the exponential family of distributions but couldn't quite make it, even a subcase like the exponential family would be interesting to me. EDIT2: after thinking about this for a while I think an equivalent way to state the problem is: ""when is the maximum likelihood estimator unbiased"". So, when is it unbiased?",,"['probability', 'statistics']"
51,Convergence in Distribution of Sums of Random Variable,Convergence in Distribution of Sums of Random Variable,,"Suppose I have $X_1,X_2,...,X_n$ random variables that are independent and identically distributed, from ANY distribution.  Suppose that $E(X_i)=\mu$ and $V(X_i)=\sigma^2$. Suppose I define the following random variable: $$Y=\sum_{i=1}^nX_i$$ What is the limiting distribution of $Y$?  That is, as $n$ goes to infinity, what distribution can $Y$ be approximated by? My intuation tells me that $Y\rightarrow N(n\mu,n\sigma^2)$.  In other words, say $200$ was a sufficiently large number for $n$.  Then I could approximate $Y$ by a normal distribution with mean $200\mu$ and variance $200\sigma^2$.  Is this true, and if so, how can you prove it?  If not, what is the limiting distribution of $Y$?","Suppose I have $X_1,X_2,...,X_n$ random variables that are independent and identically distributed, from ANY distribution.  Suppose that $E(X_i)=\mu$ and $V(X_i)=\sigma^2$. Suppose I define the following random variable: $$Y=\sum_{i=1}^nX_i$$ What is the limiting distribution of $Y$?  That is, as $n$ goes to infinity, what distribution can $Y$ be approximated by? My intuation tells me that $Y\rightarrow N(n\mu,n\sigma^2)$.  In other words, say $200$ was a sufficiently large number for $n$.  Then I could approximate $Y$ by a normal distribution with mean $200\mu$ and variance $200\sigma^2$.  Is this true, and if so, how can you prove it?  If not, what is the limiting distribution of $Y$?",,"['probability', 'probability-theory', 'statistics', 'probability-distributions', 'central-limit-theorem']"
52,Maximizing Winnings - Dice Roll Strategy,Maximizing Winnings - Dice Roll Strategy,,"Let’s consider a simple dice game. Two fair 6-sided dice are rolled. Let $X$ is the sum of the two dice. If $X = 7$, then the game ends and you win nothing (winnings = 0). If $X \neq 7$, then you have the option of either stopping the game and receiving $X$ (what you rolled on your last roll) or starting the whole process over again. Now consider this strategy to play: pick a number $i$, where $2 \leq i \leq 12$, and stop playing the first time that a value greater than or equal to $i$ is rolled (or until you are forced to stop as a result of rolling a 7). Define $Y_i = $  winnings when you use this strategy with chosen value $i$. We are interested in the value $i$ that maximizes the expected winnings $\mathbb{E}[Y_i]$ over the all possible choices of $i$. To make a long story short, it turns out that the value of $i$ that maximizes the expected winnings $\mathbb{E}[Y_i]$ for the game is $i = 8.$ For this problem, what we actually want is for you to explicitly compute the expected winnings $\mathbb{E}[Y_i]$ for $i = 5, 6, 8$ and $9$ to show why the expected winnings is maximized when $i = 8$. You do not need to consider the cases where $i = 2, 3, 4, 10, 11$ or $12$. Attempt:  Tried Expressing $\mathbb{E}[Y_i \mid X = 7] = \text{-Winnings}$ $\mathbb{E}[Y_i \mid X < i, X \neq 7] = X + \mathbb{E}[Y_i]$ $\mathbb{E}[Y_i \mid X \geq i, X \neq 7] = X$ $\mathbb{E}[Y_i \mid X = 7] = -(\mathbb{E}[Y_i | X < i, X \neq 7] + \mathbb{E}[Y_i | X \geq i, X \neq 7])$ But no matter what I do, I'm getting an incorrect answer as $8$ should be the maximum, but it's not... Please help!","Let’s consider a simple dice game. Two fair 6-sided dice are rolled. Let $X$ is the sum of the two dice. If $X = 7$, then the game ends and you win nothing (winnings = 0). If $X \neq 7$, then you have the option of either stopping the game and receiving $X$ (what you rolled on your last roll) or starting the whole process over again. Now consider this strategy to play: pick a number $i$, where $2 \leq i \leq 12$, and stop playing the first time that a value greater than or equal to $i$ is rolled (or until you are forced to stop as a result of rolling a 7). Define $Y_i = $  winnings when you use this strategy with chosen value $i$. We are interested in the value $i$ that maximizes the expected winnings $\mathbb{E}[Y_i]$ over the all possible choices of $i$. To make a long story short, it turns out that the value of $i$ that maximizes the expected winnings $\mathbb{E}[Y_i]$ for the game is $i = 8.$ For this problem, what we actually want is for you to explicitly compute the expected winnings $\mathbb{E}[Y_i]$ for $i = 5, 6, 8$ and $9$ to show why the expected winnings is maximized when $i = 8$. You do not need to consider the cases where $i = 2, 3, 4, 10, 11$ or $12$. Attempt:  Tried Expressing $\mathbb{E}[Y_i \mid X = 7] = \text{-Winnings}$ $\mathbb{E}[Y_i \mid X < i, X \neq 7] = X + \mathbb{E}[Y_i]$ $\mathbb{E}[Y_i \mid X \geq i, X \neq 7] = X$ $\mathbb{E}[Y_i \mid X = 7] = -(\mathbb{E}[Y_i | X < i, X \neq 7] + \mathbb{E}[Y_i | X \geq i, X \neq 7])$ But no matter what I do, I'm getting an incorrect answer as $8$ should be the maximum, but it's not... Please help!",,"['probability', 'expectation', 'dice']"
53,Probability of winning an election while losing the popular vote: electorates of size 3,Probability of winning an election while losing the popular vote: electorates of size 3,,"Suppose we live in a country with an interesting electoral system: each electorate has exactly 3 voters. 2 parties run for office, and each voter has a 50/50 chance of voting for each party. Whoever wins the majority of electorates wins the election overall. Given an arbitrarily large number of electorates, what is the probability that the party that won the election lost the popular vote? (This is a more specific version of my earlier question Probability of winning an election while losing the popular vote ) My 'brute force' computer model yields an answer of very close to 1/6th. Does anyone have ideas for how to solve this problem analytically?","Suppose we live in a country with an interesting electoral system: each electorate has exactly 3 voters. 2 parties run for office, and each voter has a 50/50 chance of voting for each party. Whoever wins the majority of electorates wins the election overall. Given an arbitrarily large number of electorates, what is the probability that the party that won the election lost the popular vote? (This is a more specific version of my earlier question Probability of winning an election while losing the popular vote ) My 'brute force' computer model yields an answer of very close to 1/6th. Does anyone have ideas for how to solve this problem analytically?",,"['probability', 'voting-theory']"
54,Find probability of biased Coins,Find probability of biased Coins,,"There are two biased coins A and B. We have been given the following: P(H|Coin A) = 0.9 P(T|Coin A) = 0.1 P(H|Coin B) = 0.1 P(T|Coin B) = 0.9 We are also given that  Probability of tossing Coin A or Coin B randomly is 0.5 How could we find the probability of ""11th toss = Head"", given that we have 10 heads in a row ?","There are two biased coins A and B. We have been given the following: P(H|Coin A) = 0.9 P(T|Coin A) = 0.1 P(H|Coin B) = 0.1 P(T|Coin B) = 0.9 We are also given that  Probability of tossing Coin A or Coin B randomly is 0.5 How could we find the probability of ""11th toss = Head"", given that we have 10 heads in a row ?",,['probability']
55,"If $X,Y$ are positively correlated, are $f(X),f(Y)$ also positively correlated for a positive increasing $f$?","If  are positively correlated, are  also positively correlated for a positive increasing ?","X,Y f(X),f(Y) f","Suppose that $X$ and $Y$ are positive and square-integrable random variables, such that $X$ and $Y$ are positively correlated, i.e., $\mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y] \geq 0$. Let $f: \mathbb{R} \to \mathbb{R}_+$ be a positive and increasing function. My question is: Are $f(X)$ and $f(Y)$ positively correlated? Thanks very much.","Suppose that $X$ and $Y$ are positive and square-integrable random variables, such that $X$ and $Y$ are positively correlated, i.e., $\mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y] \geq 0$. Let $f: \mathbb{R} \to \mathbb{R}_+$ be a positive and increasing function. My question is: Are $f(X)$ and $f(Y)$ positively correlated? Thanks very much.",,"['probability', 'probability-theory', 'covariance', 'correlation']"
56,Probability that these characters win a game.,Probability that these characters win a game.,,"I have 11 characters, $[2,3,4,5,6,7,8,9,10,11,12]$, and they all play a game. Game Description: All players stand at the start line $n$ spaces away from finish line. Two fair dice are rolled. The two results on each of the dice are summed up, and it gives a result that is equal to the names of one of the characters. The owner of that number moves forward one space. The winner is the character who gets to the finish line first. For example, on a turn, the dice gives a result of $[3,4]$. Since $3+4=7$, the character 7 moves forward. If the dice gives $[6,2]$, the character 8 moves forward. What are the chances that each character wins? Basically I want the chances of winning for all the characters, with respect to other players. By respect to other players , I mean that one can win it quicker than another.  And I want the probabilities to be in terms of $n$, for example, $P(x) = \frac{1}{36^{n}}$ Images of a state midgame: Here, character 6 has won the game, and in this case, $n=9$, since that is the number of spaces a character has to move in order to win. Sources: The game originated from my school The image state example was created by me using Microsoft Excel The question is a challenge set by myself. I need help basically.","I have 11 characters, $[2,3,4,5,6,7,8,9,10,11,12]$, and they all play a game. Game Description: All players stand at the start line $n$ spaces away from finish line. Two fair dice are rolled. The two results on each of the dice are summed up, and it gives a result that is equal to the names of one of the characters. The owner of that number moves forward one space. The winner is the character who gets to the finish line first. For example, on a turn, the dice gives a result of $[3,4]$. Since $3+4=7$, the character 7 moves forward. If the dice gives $[6,2]$, the character 8 moves forward. What are the chances that each character wins? Basically I want the chances of winning for all the characters, with respect to other players. By respect to other players , I mean that one can win it quicker than another.  And I want the probabilities to be in terms of $n$, for example, $P(x) = \frac{1}{36^{n}}$ Images of a state midgame: Here, character 6 has won the game, and in this case, $n=9$, since that is the number of spaces a character has to move in order to win. Sources: The game originated from my school The image state example was created by me using Microsoft Excel The question is a challenge set by myself. I need help basically.",,['probability']
57,Probability that a linear Diophantine equation has a solution,Probability that a linear Diophantine equation has a solution,,"Given a linear Diophantine equation $(\mathcal D)$ with two variable $x$ and $y$ : $$(\mathcal D):\quad ax+by=c$$ I want to calculate the probability $P$ that $(\mathcal D)$ has a solution. We know that $(\mathcal D)$ has a solution if, and only if, $d:=\gcd(a,b)$ divides $c$ . So if $a$ and $b$ are coprime, $(\mathcal D)$ has a solution, so using a result on coprime integers , we have $$p\geqslant \mathbb P(d=1)=\frac 1{\zeta(2)}=\frac 6{\pi^2}.$$ Since $d$ must divides $c$ , for each $d$ there is $\frac 1d$ integers which will work for $c$ , so $$p=\sum_{k=1}^\infty \mathbb P(d=k)\frac 1k.$$ Then we can majorate $\mathbb P(d=k)$ by $\mathbb P(d\geqslant k)=\frac 1k\times \frac 1k$ for $k\geqslant 2$ . So $$p=\mathbb P(d=1)\times 1+\sum_{k=2}^\infty\mathbb P(d=k)\frac 1k\leqslant \frac 1{\zeta(2)}+\sum_{k=2}^\infty\frac 1{k^2}\frac 1k=\frac 1{\zeta(2)}+\zeta(3)-1.$$ What we get so far with our rough analysis is: $$\frac 1{\zeta(2)}\leqslant p\leqslant \frac 1{\zeta(2)}+\zeta(3)-1$$ $$0.6079<p<0.8100.$$ I have read somewhere (if I understand the article correctly) that $$\mathbb P(d=k)=\frac 1{\zeta(2)k^2}.$$ This would mean that $$p=\sum_{k=1}^\infty \mathbb P(d=k)\frac 1k=\frac 1{\zeta(2)}\sum_{k=1}^\infty\frac 1{k^3}=\fbox{$\frac {\zeta(3)}{\zeta(2)}\approx0.7308$}.$$ My questions. Thanks to comments, I have the new following question: can I formalize this approach to make it rigorous, with a well chosen probabilistic distribution? I am really unused to do probabilities, so is the following approach correct? Can we generalize the result / the approach to find $p_n$ the probability that a linear Diophantine equation with $n$ variables has a solution? If we can't get an exact value for $n$ variables, can we get a numerical approximation of $p_n$ ? What is the limit of $p_n$ ?","Given a linear Diophantine equation with two variable and : I want to calculate the probability that has a solution. We know that has a solution if, and only if, divides . So if and are coprime, has a solution, so using a result on coprime integers , we have Since must divides , for each there is integers which will work for , so Then we can majorate by for . So What we get so far with our rough analysis is: I have read somewhere (if I understand the article correctly) that This would mean that My questions. Thanks to comments, I have the new following question: can I formalize this approach to make it rigorous, with a well chosen probabilistic distribution? I am really unused to do probabilities, so is the following approach correct? Can we generalize the result / the approach to find the probability that a linear Diophantine equation with variables has a solution? If we can't get an exact value for variables, can we get a numerical approximation of ? What is the limit of ?","(\mathcal D) x y (\mathcal D):\quad ax+by=c P (\mathcal D) (\mathcal D) d:=\gcd(a,b) c a b (\mathcal D) p\geqslant \mathbb P(d=1)=\frac 1{\zeta(2)}=\frac 6{\pi^2}. d c d \frac 1d c p=\sum_{k=1}^\infty \mathbb P(d=k)\frac 1k. \mathbb P(d=k) \mathbb P(d\geqslant k)=\frac 1k\times \frac 1k k\geqslant 2 p=\mathbb P(d=1)\times 1+\sum_{k=2}^\infty\mathbb P(d=k)\frac 1k\leqslant \frac 1{\zeta(2)}+\sum_{k=2}^\infty\frac 1{k^2}\frac 1k=\frac 1{\zeta(2)}+\zeta(3)-1. \frac 1{\zeta(2)}\leqslant p\leqslant \frac 1{\zeta(2)}+\zeta(3)-1 0.6079<p<0.8100. \mathbb P(d=k)=\frac 1{\zeta(2)k^2}. p=\sum_{k=1}^\infty \mathbb P(d=k)\frac 1k=\frac 1{\zeta(2)}\sum_{k=1}^\infty\frac 1{k^3}=\fbox{\frac {\zeta(3)}{\zeta(2)}\approx0.7308}. p_n n n p_n p_n","['probability', 'arithmetic', 'diophantine-equations', 'riemann-zeta']"
58,"Confused about the definition of a random sample, statistics and estimators/estimates","Confused about the definition of a random sample, statistics and estimators/estimates",,"I'm currently studying basic statistics, and I don't really understand the definition of random sample in the book I'm reading ( Introduction to Probability and Statistics: Principles and Applications for Engineering and the Computing Sciences, 4th Edition ). The book defines random samples as follows: [...] the term ""random sample"" is used in three different but closely related ways in applied statistics. It may refer to the objects selected for study, to the random variables associated with the objects to be selected, or to the numerical values assumed by those variables. A random sample of size $n$ from the distribution of $X$ is a collection of $n$ independent random variables, each with the same distribution as $X$. [...] The objects selected generate $n$ numbers $x_1,x_2,x_3,\ldots,x_n$ which are the observed values of the random variables $X_1,X_2,X_3,\ldots,X_n$. At this point, I don't really see the point of thinking of a random sample as a collection of random variables, considering the data gathered for an experiment is a set of constants. A statistic is defined by the book as ""a random variable whose numerical value can be determined from a random sample"". As an example, the sample mean statistic of a sample (a set of random variables) $\{1, 2, 3\}$ is defined as $$ \bar{X} = \frac{X_1 + \cdots + X_n}{n} $$ where $X_{1..n}$ are random variables (which means that $\bar{X}$ is also a random variable), as opposed to $$ \bar{x} = \frac{x_1 + \cdots + x_n}{n} $$ where $x_{1..n}$ are constant numbers (which means that $\bar{x}$ is a single number). Now, since $X_1, \ldots, X_n$ are random variables but each have a single, known value ($X_i$ assumes the value $x_i$), $\bar{X}$ is also a random variable which assumes a known value -- i.e., $(x_1 + \cdots + x_n)/n$. (Please correct me if I'm wrong here.) Here's my confusion. I think I've understood the concept of statistics and estimators -- to provide an estimation for population parameters using the characteristics of a sample. However, I don't see a reason behind thinking of a random sample as a set of random variables -- as opposed to the (in my eyes) more ""natural"" way of thinking of them as simply a set of numerical constants -- or thinking of a statistic like the sample mean, median, range, etc. as a random variable rather than a single, constant numerical value. This is my first post here, so the question probably has problems with structure, length and clarity, but what I'm essentially asking is for some ""justification"" for thinking of a) a random sample as a set of random variables as opposed to a set of numbers, and b) a statistic as a random variable rather than a constant.","I'm currently studying basic statistics, and I don't really understand the definition of random sample in the book I'm reading ( Introduction to Probability and Statistics: Principles and Applications for Engineering and the Computing Sciences, 4th Edition ). The book defines random samples as follows: [...] the term ""random sample"" is used in three different but closely related ways in applied statistics. It may refer to the objects selected for study, to the random variables associated with the objects to be selected, or to the numerical values assumed by those variables. A random sample of size $n$ from the distribution of $X$ is a collection of $n$ independent random variables, each with the same distribution as $X$. [...] The objects selected generate $n$ numbers $x_1,x_2,x_3,\ldots,x_n$ which are the observed values of the random variables $X_1,X_2,X_3,\ldots,X_n$. At this point, I don't really see the point of thinking of a random sample as a collection of random variables, considering the data gathered for an experiment is a set of constants. A statistic is defined by the book as ""a random variable whose numerical value can be determined from a random sample"". As an example, the sample mean statistic of a sample (a set of random variables) $\{1, 2, 3\}$ is defined as $$ \bar{X} = \frac{X_1 + \cdots + X_n}{n} $$ where $X_{1..n}$ are random variables (which means that $\bar{X}$ is also a random variable), as opposed to $$ \bar{x} = \frac{x_1 + \cdots + x_n}{n} $$ where $x_{1..n}$ are constant numbers (which means that $\bar{x}$ is a single number). Now, since $X_1, \ldots, X_n$ are random variables but each have a single, known value ($X_i$ assumes the value $x_i$), $\bar{X}$ is also a random variable which assumes a known value -- i.e., $(x_1 + \cdots + x_n)/n$. (Please correct me if I'm wrong here.) Here's my confusion. I think I've understood the concept of statistics and estimators -- to provide an estimation for population parameters using the characteristics of a sample. However, I don't see a reason behind thinking of a random sample as a set of random variables -- as opposed to the (in my eyes) more ""natural"" way of thinking of them as simply a set of numerical constants -- or thinking of a statistic like the sample mean, median, range, etc. as a random variable rather than a single, constant numerical value. This is my first post here, so the question probably has problems with structure, length and clarity, but what I'm essentially asking is for some ""justification"" for thinking of a) a random sample as a set of random variables as opposed to a set of numbers, and b) a statistic as a random variable rather than a constant.",,"['probability', 'statistics', 'random-variables', 'statistical-inference', 'random']"
59,Maximum of exponential random variables is bigger than maximum of normal random variables almost surely as $n$ tends to infinity.,Maximum of exponential random variables is bigger than maximum of normal random variables almost surely as  tends to infinity.,n,"Let $X_1,X_2,\cdots,X_n$ be iid Gaussian random variables with mean $0$ and variance $1$. Let $Y_1,Y_2,\cdots,Y_n$ be iid exponential random variables with mean $1$. Prove that $$\lim_{n\to\infty}P\left(\max{(Y_1,Y_2,\cdots,Y_n)}\ge \max{(X_1,X_2,\cdots,X_n)}\right)=1$$ I let $W_n=\max{(X_1,X_2,\cdots,X_n)}$ and $Z_n=\max{(Y_1,Y_2,\cdots,Y_n)}$.As $X_i$'s and $Y_i$'s are iid, it is easy to get the distribution of $W$ and $Z$ as $F_W(w)=\left(F_X(w)\right)^n$ and $F_Z(z)=\left(F_Y(z)\right)^n$. I computed $$P(Z_n\ge W_n)=\int_{-\infty}^{\infty}F_W(w)f_Z(w)dw=\int_0^{\infty}\left(\Phi(w)\right)^nn(1-e^{-w})^{n-1}e^{-w}dw$$ where $\Phi$ is the CDF of standard normal distribution. I am not sure how to estimate the resulting integral as $n\to\infty$, although I could verify that it converges to 1 by using numerical computation. Also, I believe that there should be a more clever and elegant way to deal with this problem, but I don't know how. Thanks for any hint in advance.","Let $X_1,X_2,\cdots,X_n$ be iid Gaussian random variables with mean $0$ and variance $1$. Let $Y_1,Y_2,\cdots,Y_n$ be iid exponential random variables with mean $1$. Prove that $$\lim_{n\to\infty}P\left(\max{(Y_1,Y_2,\cdots,Y_n)}\ge \max{(X_1,X_2,\cdots,X_n)}\right)=1$$ I let $W_n=\max{(X_1,X_2,\cdots,X_n)}$ and $Z_n=\max{(Y_1,Y_2,\cdots,Y_n)}$.As $X_i$'s and $Y_i$'s are iid, it is easy to get the distribution of $W$ and $Z$ as $F_W(w)=\left(F_X(w)\right)^n$ and $F_Z(z)=\left(F_Y(z)\right)^n$. I computed $$P(Z_n\ge W_n)=\int_{-\infty}^{\infty}F_W(w)f_Z(w)dw=\int_0^{\infty}\left(\Phi(w)\right)^nn(1-e^{-w})^{n-1}e^{-w}dw$$ where $\Phi$ is the CDF of standard normal distribution. I am not sure how to estimate the resulting integral as $n\to\infty$, although I could verify that it converges to 1 by using numerical computation. Also, I believe that there should be a more clever and elegant way to deal with this problem, but I don't know how. Thanks for any hint in advance.",,"['probability', 'probability-theory', 'random-variables']"
60,Terminology: Independent Copy of Random Variables,Terminology: Independent Copy of Random Variables,,"Suppose $X_1,\ldots, X_n$ are (independent) RVs. What does it mean to say that $X_1',\ldots, X_n'$ is an independent copy of $X_1,\ldots, X_n$? Does it mean that each $X_i'$ is independent of $X_i$ or does it mean that the joint distribution of $(X_1,\ldots, X_n)$ is the same as the joint distribution of $(X_1',\ldots, X_n')$? Or does it mean something else entirely? I find the term a bit confusing since I am not sure how you can be both independent and a copy (since being a copy would imply being dependent).","Suppose $X_1,\ldots, X_n$ are (independent) RVs. What does it mean to say that $X_1',\ldots, X_n'$ is an independent copy of $X_1,\ldots, X_n$? Does it mean that each $X_i'$ is independent of $X_i$ or does it mean that the joint distribution of $(X_1,\ldots, X_n)$ is the same as the joint distribution of $(X_1',\ldots, X_n')$? Or does it mean something else entirely? I find the term a bit confusing since I am not sure how you can be both independent and a copy (since being a copy would imply being dependent).",,"['probability', 'soft-question', 'terminology']"
61,How long until I get out of bed?,How long until I get out of bed?,,"Suppose I have two independent alarm clocks which I set right before I go to bed. Their ring times are exponentially distributed with rates $\lambda_1$ and $\lambda_2$. Whenever alarm 1 goes off I immediately reset both alarms, but if alarm 2 goes off I actually get up. How long do I stay in bed? Am I right in saying that alarm clock one is redundant with respect to bed staying time?","Suppose I have two independent alarm clocks which I set right before I go to bed. Their ring times are exponentially distributed with rates $\lambda_1$ and $\lambda_2$. Whenever alarm 1 goes off I immediately reset both alarms, but if alarm 2 goes off I actually get up. How long do I stay in bed? Am I right in saying that alarm clock one is redundant with respect to bed staying time?",,"['probability', 'markov-chains']"
62,Is there any probability model for multi-stage motion of an object such as this.,Is there any probability model for multi-stage motion of an object such as this.,,"I have this following case (please refer to attached pic below) where a particle is resting on the ground and it needs a minimum amount of force ( Fmin ) to reach from one level to the next level. But if at any level it receives F which is more than Fmin , then the excess force can be carried forward to the next levels, and hence even a force less than Fmin will be enough for it to go to the next level. The force F follows a two-sided truncated gaussian distribution, and has only positive values. So the probability that the object moves from 0 to 1 is P0=P(F>Fmin) where F = two-sided truncated gaussian distribution with specified mean, and std. dev. Now suppose, when the particle was at level 0, it received force F0>Fmin , reached level 1, and now the probability to go from 1 to 2 P1 = P(F>(Fmin-excess F it received earlier) = P(F>Fmin-(F0-Fmin)) = P(F>2Fmin-F0) At level 1, it had received force F1 , so now probability from level 2 to 3 is P2 = P(F>3Fmin-(F0+F1)) and again from level 3 to target is P3 = P(F>4Fmin-(F0+F1+F2)) So, here the probability at any level is dependent on the F values it received in the previous stages. I guess the final probability can be written as P = P0*P1*P2*P3 Is there any probability model for this type of multi-stage motion? How can I model this problem. Any help or suggestions. If my question is not clear,  please let me know. Thanks. Edit 1 : If it receives more than Fmin at level 0, it will definitely reach level 1, and can even reach level 2, or even the target level depending on the F magnitude. Suppose at level 0 it receives only Fmin , so it reaches level 1, but at level 1 it receives less than Fmin , In this case it falls to the below level that is level 0. Suppose this thing happens at level 2, then the particle falls back to level 1 initially, and if it doesn't get Fmin even at level 1, it finally goes to level 0. So the particle cannot stay in any level for an extended time, except level 0 where it is waiting for Fmin . At other levels, it if doesn't have Fmin it falls back to the below level. Edit 2: The force distribution is updated to a two-sided truncated Gaussian distribution to have only positive forces. and to reflect the possibility that the particle may not even reach the target eventually. The particle receives a force at all levels, irrespective of the force magnitude it received at previous levels. For clarity I must add that if the particle falls down even by 1 level, all the accumulated excesses are now gone, and it will again need at least Fmin to again start over from its present level. Simulated this motion in Matlab. So I am just adding an output image which demonstrates one of the many possible up-and-down movements of the particle until the target level is reached. Hope it will make the question easier to understand. Edit 3: Added the updated image of the particle motion. Hope this helps.","I have this following case (please refer to attached pic below) where a particle is resting on the ground and it needs a minimum amount of force ( Fmin ) to reach from one level to the next level. But if at any level it receives F which is more than Fmin , then the excess force can be carried forward to the next levels, and hence even a force less than Fmin will be enough for it to go to the next level. The force F follows a two-sided truncated gaussian distribution, and has only positive values. So the probability that the object moves from 0 to 1 is P0=P(F>Fmin) where F = two-sided truncated gaussian distribution with specified mean, and std. dev. Now suppose, when the particle was at level 0, it received force F0>Fmin , reached level 1, and now the probability to go from 1 to 2 P1 = P(F>(Fmin-excess F it received earlier) = P(F>Fmin-(F0-Fmin)) = P(F>2Fmin-F0) At level 1, it had received force F1 , so now probability from level 2 to 3 is P2 = P(F>3Fmin-(F0+F1)) and again from level 3 to target is P3 = P(F>4Fmin-(F0+F1+F2)) So, here the probability at any level is dependent on the F values it received in the previous stages. I guess the final probability can be written as P = P0*P1*P2*P3 Is there any probability model for this type of multi-stage motion? How can I model this problem. Any help or suggestions. If my question is not clear,  please let me know. Thanks. Edit 1 : If it receives more than Fmin at level 0, it will definitely reach level 1, and can even reach level 2, or even the target level depending on the F magnitude. Suppose at level 0 it receives only Fmin , so it reaches level 1, but at level 1 it receives less than Fmin , In this case it falls to the below level that is level 0. Suppose this thing happens at level 2, then the particle falls back to level 1 initially, and if it doesn't get Fmin even at level 1, it finally goes to level 0. So the particle cannot stay in any level for an extended time, except level 0 where it is waiting for Fmin . At other levels, it if doesn't have Fmin it falls back to the below level. Edit 2: The force distribution is updated to a two-sided truncated Gaussian distribution to have only positive forces. and to reflect the possibility that the particle may not even reach the target eventually. The particle receives a force at all levels, irrespective of the force magnitude it received at previous levels. For clarity I must add that if the particle falls down even by 1 level, all the accumulated excesses are now gone, and it will again need at least Fmin to again start over from its present level. Simulated this motion in Matlab. So I am just adding an output image which demonstrates one of the many possible up-and-down movements of the particle until the target level is reached. Hope it will make the question easier to understand. Edit 3: Added the updated image of the particle motion. Hope this helps.",,"['probability', 'probability-distributions', 'mathematical-modeling']"
63,weak convergence and unbounded functions with bounded moment,weak convergence and unbounded functions with bounded moment,,"I want to prove the following: Given a topological space (it is a Lusin space, but I think that does not matter) $\Omega$ , a function $f \in C(\Omega,\mathbb{R})$ and a sequence of Radon measures $P^{N}$ defined on it that converges weakly to a measure $P$ , then $$      \mathbb{E}^{P^{N}}\left[ f \right] = \int_{\Omega} f dP^{N} \rightarrow \int_{\Omega} f dP = \mathbb{E}^{P}\left[f\right]. $$ The problem is that the function $f$ is not bounded, i.e. $f \notin C_{b}(\Omega)$ but instead satisfies the condition $$       \sup_{N} \mathbb{E}^{P^{N}}\left[\left|f\right|^{1+\varepsilon}\right] \leq C $$ for an $\varepsilon > 0$ . I read this claim in a paper, but unfortunately with neither a proof nor a reference. A similar question was posed in this thread: weak convergence of probability measures and unbounded functions with bounded expectation but there we had $\varepsilon = 0$ . As far as I can see, the counterexample posted there is not a counterexample here. The result appears to me rather elementary but I haven't found it anywhere yet...","I want to prove the following: Given a topological space (it is a Lusin space, but I think that does not matter) , a function and a sequence of Radon measures defined on it that converges weakly to a measure , then The problem is that the function is not bounded, i.e. but instead satisfies the condition for an . I read this claim in a paper, but unfortunately with neither a proof nor a reference. A similar question was posed in this thread: weak convergence of probability measures and unbounded functions with bounded expectation but there we had . As far as I can see, the counterexample posted there is not a counterexample here. The result appears to me rather elementary but I haven't found it anywhere yet...","\Omega f \in C(\Omega,\mathbb{R}) P^{N} P 
     \mathbb{E}^{P^{N}}\left[ f \right] = \int_{\Omega} f dP^{N} \rightarrow \int_{\Omega} f dP = \mathbb{E}^{P}\left[f\right].
 f f \notin C_{b}(\Omega) 
      \sup_{N} \mathbb{E}^{P^{N}}\left[\left|f\right|^{1+\varepsilon}\right] \leq C
 \varepsilon > 0 \varepsilon = 0","['probability', 'probability-theory', 'convergence-divergence', 'weak-convergence']"
64,"Flip $n$ coins, discard tails, and continue until $k$ heads remain","Flip  coins, discard tails, and continue until  heads remain",n k,"Consider the following game: $n$ participants have a fair coin each, on a given round, the not already discarded participants flip their coins, those who flip a tail are discarded from the game, the remaining ones continue to play until there are at most $k$ of them left. The question is: what's the distribution of the number of rounds in this game? Bonus question: idem, but the $n$ coins are all unfair, with probabilities $p_1$, $p_2$, $\ldots$, $p_n$. Disclaimer: this is not a homework question, it came up when considering distributed routing protocols with a colleague.","Consider the following game: $n$ participants have a fair coin each, on a given round, the not already discarded participants flip their coins, those who flip a tail are discarded from the game, the remaining ones continue to play until there are at most $k$ of them left. The question is: what's the distribution of the number of rounds in this game? Bonus question: idem, but the $n$ coins are all unfair, with probabilities $p_1$, $p_2$, $\ldots$, $p_n$. Disclaimer: this is not a homework question, it came up when considering distributed routing protocols with a colleague.",,"['probability', 'probability-distributions']"
65,Simple application of Donsker's theorem,Simple application of Donsker's theorem,,"I am trying to do exercise 5.15 in Moerter's book ""Brownian Motion"". It seems quite easy, but I can't solve it somehow: Suppose $S(j)_j$ is a SRW on the integers, started at zero. Show that: $$ \frac{1}{n^2}\min_j\{|S(j)|=n\}\to^d\min_t\{|B(t)|=1\}\, . $$ Now, this looks like a standard application of Donsker's theorem (although, perhaps it is not?). Anyway, I tried hitting it with Donskers theorem, but I just keep getting nonesense: $$ P(\min_t\{|B(t)|=1\}\le s)=P(M_s\ge 1)=P(B\in K_s)\, \ $$ where $M$ is the max of the modulus of a Brownian motion and and $K_s$ is the set of continuous functions whos maximum is bigger than one. But: $$ P(S^*(t)\in K_s)=? $$ The first problem is that we don't have the intervall $[0,1]$ as required in Donsker's theorem, and the second one is that I cannot related the interpolated and scaled SRW to the property above (the one with $1/n^2$) Any help welcomed. thank you!","I am trying to do exercise 5.15 in Moerter's book ""Brownian Motion"". It seems quite easy, but I can't solve it somehow: Suppose $S(j)_j$ is a SRW on the integers, started at zero. Show that: $$ \frac{1}{n^2}\min_j\{|S(j)|=n\}\to^d\min_t\{|B(t)|=1\}\, . $$ Now, this looks like a standard application of Donsker's theorem (although, perhaps it is not?). Anyway, I tried hitting it with Donskers theorem, but I just keep getting nonesense: $$ P(\min_t\{|B(t)|=1\}\le s)=P(M_s\ge 1)=P(B\in K_s)\, \ $$ where $M$ is the max of the modulus of a Brownian motion and and $K_s$ is the set of continuous functions whos maximum is bigger than one. But: $$ P(S^*(t)\in K_s)=? $$ The first problem is that we don't have the intervall $[0,1]$ as required in Donsker's theorem, and the second one is that I cannot related the interpolated and scaled SRW to the property above (the one with $1/n^2$) Any help welcomed. thank you!",,"['probability', 'stochastic-processes', 'brownian-motion', 'stochastic-approximation']"
66,Constructing a $4$-state Markov chain model that describes the arrival of customers,Constructing a -state Markov chain model that describes the arrival of customers,4,"The times between successive customer arrivals at a facility are independent and identically distributed random variables with the following PMF: $$p(k) = 0.2(k = 1)$$ $$p(k) = 0.3(k = 3)$$ $$p(k) = 0.5(k = 4)$$ $$p(k) = 0(k \notin \{1,3,4\})$$ Construct a four-state Markov chain model that describes the arrival process. In this model, one of the states should correspond to the times when an arrival occurs. Can you please explain in simple words how to construct this markov chain? Because I am totally lost with given distribution and how can I use this in my problem.","The times between successive customer arrivals at a facility are independent and identically distributed random variables with the following PMF: Construct a four-state Markov chain model that describes the arrival process. In this model, one of the states should correspond to the times when an arrival occurs. Can you please explain in simple words how to construct this markov chain? Because I am totally lost with given distribution and how can I use this in my problem.","p(k) = 0.2(k = 1) p(k) = 0.3(k = 3) p(k) = 0.5(k = 4) p(k) = 0(k \notin \{1,3,4\})","['probability', 'markov-chains']"
67,Distribution of product of bernoulli random variable and poisson random variable,Distribution of product of bernoulli random variable and poisson random variable,,There are random variable $Z=XY$ ($X$ is poisson and $Y$ is bernoulli) $$X(n;\lambda) = \frac{\lambda^n}{n!}e^{-\lambda}$$ $$Y=\begin{cases} & \beta \text{ with probability } \beta \\  & 0 \text{ probability } 1-\beta \end{cases}$$ I likes to know distribution of product of bernoulli random variable and poisson random variable. So i calculate MGF(Moment Generating Function) so i can get below expression. $$M_{z}(t)=\sum_{y}\sum_{x}e^{txy}P(x)P(y)=\sum_{x}(\beta e^{t\beta x}+1-\beta) P(x)  =\beta e^{-\lambda}(e^{\lambda e^{t\beta}})+1-\beta = \beta(e^{\lambda(e^{t\beta -1})}+1-\beta)$$ I can`t obtain pmf(probability mass function) from this calcultaed MGF Is it impossible obtain pmf ? Or is there any technique obtaining probability in this case? Thank you,There are random variable $Z=XY$ ($X$ is poisson and $Y$ is bernoulli) $$X(n;\lambda) = \frac{\lambda^n}{n!}e^{-\lambda}$$ $$Y=\begin{cases} & \beta \text{ with probability } \beta \\  & 0 \text{ probability } 1-\beta \end{cases}$$ I likes to know distribution of product of bernoulli random variable and poisson random variable. So i calculate MGF(Moment Generating Function) so i can get below expression. $$M_{z}(t)=\sum_{y}\sum_{x}e^{txy}P(x)P(y)=\sum_{x}(\beta e^{t\beta x}+1-\beta) P(x)  =\beta e^{-\lambda}(e^{\lambda e^{t\beta}})+1-\beta = \beta(e^{\lambda(e^{t\beta -1})}+1-\beta)$$ I can`t obtain pmf(probability mass function) from this calcultaed MGF Is it impossible obtain pmf ? Or is there any technique obtaining probability in this case? Thank you,,"['probability', 'probability-distributions']"
68,Necessary and Sufficient Conditions for a CDF,Necessary and Sufficient Conditions for a CDF,,"This is an attempt to prove Theorem 1.5.3. in Casella and Berger. Note that the only things that have been proven are really basic set-theory with $\mathbb{P}$ (a probability measure) theorems (e.g., addition rule). Recall for a random variable $X$, we define $$F_X(x) = \mathbb{P}(X \leq x)\text{.}$$ Theorem . $F$ is a CDF iff: $\lim\limits_{x \to -\infty}F(x) = 0$ $\lim\limits_{x \to +\infty}F(x) = 1$ $F$ is nondecreasing. For all $x_0 \in \mathbb{R}$, $\lim\limits_{x \to x_0^{+}} F(x)= F(x_0)$ $\Longrightarrow$ If $F$ is a CDF of $X$, by definition, $$F_{X}(x) = \mathbb{P}(X \leq x) = \mathbb{P}\left(\{s_j \in S: X(s_j) \leq x\} \right) $$ where $S$ denotes the overall sample space. $(3)$ is easy to show. Suppose $x_1 \leq x_2$. Then notice $$\{s_j \in S: X(s_j) \leq x_1\} \subset \{s_j \in S: X(s_j) \leq x_2\}$$ and therefore by a Theorem, $$\mathbb{P}\left(\{s_j \in S: X(s_j) \leq x_1\}\right) \leq \mathbb{P}\left(\{s_j \in S: X(s_j) \leq x_2\}\right)$$ giving $F_{X}(x_1) \leq F_{X}(x_2)$, hence $F$ is nondecreasing. I suppose $(1)$ and $(2)$ aren't consequences of anything more than saying that $\{s_j \in S: X(s_j) \leq -\infty\} = \varnothing$ and $\{s_j \in S: X(s_j) \leq +\infty\} = S$ (unless I'm completely wrong here). But this seems to suggest that $$\lim_{x \to -\infty}\mathbb{P}(\text{blah}(x)) = \mathbb{P}(\lim_{x \to -\infty}\text{blah}(x))$$ where $\text{blah}(x)$ is a set dependent on $x$. At this point of the text, this hasn't been proven (if it's even true). I'm not sure how to show $(4)$. $\Longleftarrow$ I don't know how to prove sufficiency. Casella and Berger state that this is ""much harder"" than necessity, and we have to establish that there is a sample space, a probability measure, and a random variable defined on the sample space such that $F$ is the CDF of this random variable, but this isn't enough detail for me to go on.","This is an attempt to prove Theorem 1.5.3. in Casella and Berger. Note that the only things that have been proven are really basic set-theory with $\mathbb{P}$ (a probability measure) theorems (e.g., addition rule). Recall for a random variable $X$, we define $$F_X(x) = \mathbb{P}(X \leq x)\text{.}$$ Theorem . $F$ is a CDF iff: $\lim\limits_{x \to -\infty}F(x) = 0$ $\lim\limits_{x \to +\infty}F(x) = 1$ $F$ is nondecreasing. For all $x_0 \in \mathbb{R}$, $\lim\limits_{x \to x_0^{+}} F(x)= F(x_0)$ $\Longrightarrow$ If $F$ is a CDF of $X$, by definition, $$F_{X}(x) = \mathbb{P}(X \leq x) = \mathbb{P}\left(\{s_j \in S: X(s_j) \leq x\} \right) $$ where $S$ denotes the overall sample space. $(3)$ is easy to show. Suppose $x_1 \leq x_2$. Then notice $$\{s_j \in S: X(s_j) \leq x_1\} \subset \{s_j \in S: X(s_j) \leq x_2\}$$ and therefore by a Theorem, $$\mathbb{P}\left(\{s_j \in S: X(s_j) \leq x_1\}\right) \leq \mathbb{P}\left(\{s_j \in S: X(s_j) \leq x_2\}\right)$$ giving $F_{X}(x_1) \leq F_{X}(x_2)$, hence $F$ is nondecreasing. I suppose $(1)$ and $(2)$ aren't consequences of anything more than saying that $\{s_j \in S: X(s_j) \leq -\infty\} = \varnothing$ and $\{s_j \in S: X(s_j) \leq +\infty\} = S$ (unless I'm completely wrong here). But this seems to suggest that $$\lim_{x \to -\infty}\mathbb{P}(\text{blah}(x)) = \mathbb{P}(\lim_{x \to -\infty}\text{blah}(x))$$ where $\text{blah}(x)$ is a set dependent on $x$. At this point of the text, this hasn't been proven (if it's even true). I'm not sure how to show $(4)$. $\Longleftarrow$ I don't know how to prove sufficiency. Casella and Berger state that this is ""much harder"" than necessity, and we have to establish that there is a sample space, a probability measure, and a random variable defined on the sample space such that $F$ is the CDF of this random variable, but this isn't enough detail for me to go on.",,"['probability', 'probability-theory', 'probability-distributions']"
69,"Probability of ""L"" shape on chessboard","Probability of ""L"" shape on chessboard",,"If three one by one squares are drawn from the chessboard then the   probability that they form the letter ""L"" is? I was thinking that if we select a 4 sided square is selected it will contain an ""L"".Am i going on right track?Please help!","If three one by one squares are drawn from the chessboard then the   probability that they form the letter ""L"" is? I was thinking that if we select a 4 sided square is selected it will contain an ""L"".Am i going on right track?Please help!",,[]
70,Inclusion Exclusion Principle ( Probability ),Inclusion Exclusion Principle ( Probability ),,"I'm familiar with the Inclusion / Exclusion Principle for general counting but for the life me I'm not able to show the probability version in an exercise I'm trying from a book. I've included a picture below to show my problem. $\mathbf{1.42}$ The inclusion-exclusion identity of Miscellanea $1.8.1$ gets its name from the fact that it is proved by the method of inclusion and exclusion (Feller $1968$, Section IV.$1$). Here we go into the details. The probability $P(\cup_{i=1}^nA_i)$ is the sum of the probabilities of all the sample points that are contained in at least one of the $A_i$s. The method of inclusion and exclusion is a recipe for counting these points. Let $E_k$ denote the set of all sample points that are contained in exactly $k$ of the events $A_1,A_2,\ldots,A_n$. Show that $P(\cup_{i=1}^nA_i)=\sum_{i=1}^nP(E_i)$. If $E_1$ is not empty, show that $P(E_1)=\sum_{i=1}^nP(A_i)$. Without loss of generality, assume that $E_k$ is contained in $A_1,A_2,\ldots,A_k$. Show that $P(E_k)$ appears $k$ times in the sum $P_1$, $\binom{k}2$ times in the sum $P_2$, $\binom{k}3$ times in the sum $P_3$, etc. Show that $$k-\binom{k}2+\binom{k}3-\cdots\pm\binom{k}k=1\;.$$ (See Exercise $1.27$.) Show that parts $(1)-(3)$ imply $\sum_{i=1}^nP(E_i)=P_1-P_2+\cdots\pm P_n$, establishing the inclusion-exclusion identity. Here we define $$\begin{align*}P_1&=\sum_{i=1}^nP(A_i)\\ P_2&=\sum_{1\le i<j\le n}P(A_i\cap A_j)\\ P_3&=\sum_{1\le i<j<k\le n}P(A_i\cap A_j\cap A_k)\\ &\vdots\\ P_n&=P(A_1\cap A_2\cap\cdots\cap A_n)\;. \end{align*}$$ (Original images here and here .) Nearly every proof I've seen of Inclusion / Exclusion has generally been with induction so I'm not entirely sure how to go about it from this direction. To be specific, I'm only interested in part $(5)$ of this question. Could someone have a go at it using the way the book wants you to? Also, ignore part $(2)$, the question is a typo. Part $(5)$ should really be telling you to use $(1),(3)$, and $(4)$ to establish it.","I'm familiar with the Inclusion / Exclusion Principle for general counting but for the life me I'm not able to show the probability version in an exercise I'm trying from a book. I've included a picture below to show my problem. $\mathbf{1.42}$ The inclusion-exclusion identity of Miscellanea $1.8.1$ gets its name from the fact that it is proved by the method of inclusion and exclusion (Feller $1968$, Section IV.$1$). Here we go into the details. The probability $P(\cup_{i=1}^nA_i)$ is the sum of the probabilities of all the sample points that are contained in at least one of the $A_i$s. The method of inclusion and exclusion is a recipe for counting these points. Let $E_k$ denote the set of all sample points that are contained in exactly $k$ of the events $A_1,A_2,\ldots,A_n$. Show that $P(\cup_{i=1}^nA_i)=\sum_{i=1}^nP(E_i)$. If $E_1$ is not empty, show that $P(E_1)=\sum_{i=1}^nP(A_i)$. Without loss of generality, assume that $E_k$ is contained in $A_1,A_2,\ldots,A_k$. Show that $P(E_k)$ appears $k$ times in the sum $P_1$, $\binom{k}2$ times in the sum $P_2$, $\binom{k}3$ times in the sum $P_3$, etc. Show that $$k-\binom{k}2+\binom{k}3-\cdots\pm\binom{k}k=1\;.$$ (See Exercise $1.27$.) Show that parts $(1)-(3)$ imply $\sum_{i=1}^nP(E_i)=P_1-P_2+\cdots\pm P_n$, establishing the inclusion-exclusion identity. Here we define $$\begin{align*}P_1&=\sum_{i=1}^nP(A_i)\\ P_2&=\sum_{1\le i<j\le n}P(A_i\cap A_j)\\ P_3&=\sum_{1\le i<j<k\le n}P(A_i\cap A_j\cap A_k)\\ &\vdots\\ P_n&=P(A_1\cap A_2\cap\cdots\cap A_n)\;. \end{align*}$$ (Original images here and here .) Nearly every proof I've seen of Inclusion / Exclusion has generally been with induction so I'm not entirely sure how to go about it from this direction. To be specific, I'm only interested in part $(5)$ of this question. Could someone have a go at it using the way the book wants you to? Also, ignore part $(2)$, the question is a typo. Part $(5)$ should really be telling you to use $(1),(3)$, and $(4)$ to establish it.",,['probability']
71,Proof that absolute value of a random variable is a random variable,Proof that absolute value of a random variable is a random variable,,"Is this proof correct?: Proof: Suppose that $X$ is a random variable on a probability space $\{\Omega, \mathcal{F}, \mathbb{P}\}$. Suppose $x \in \mathbb{R}$ and $x \geq 0$. Then $\{|X| \leq x\} = \{X \leq x \} \cap \{X < -x \}^c$. Since $X$ is a random variable $\{X \leq x \} \in \mathcal{F}$ and $\{X < -x \}^c \in \mathcal{F}$. Since $\mathcal{F}$ is a $\sigma$-field, the intersection $\{|X| \leq x\} = \{X \leq x \} \cap \{X < -x \}^c$ also belongs to $\mathcal{F}$. Thus $|X|$ is a random variable. $\square$","Is this proof correct?: Proof: Suppose that $X$ is a random variable on a probability space $\{\Omega, \mathcal{F}, \mathbb{P}\}$. Suppose $x \in \mathbb{R}$ and $x \geq 0$. Then $\{|X| \leq x\} = \{X \leq x \} \cap \{X < -x \}^c$. Since $X$ is a random variable $\{X \leq x \} \in \mathcal{F}$ and $\{X < -x \}^c \in \mathcal{F}$. Since $\mathcal{F}$ is a $\sigma$-field, the intersection $\{|X| \leq x\} = \{X \leq x \} \cap \{X < -x \}^c$ also belongs to $\mathcal{F}$. Thus $|X|$ is a random variable. $\square$",,"['probability', 'probability-theory']"
72,How many ways are there to shake hands?,How many ways are there to shake hands?,,"In a group of $9$ people, each person shakes hands with exactly $2$ of the other people from the group. Let $X$ be the number of possible ways to perform these handshakes. Take $2$ handshake patterns (arrangements) distinct if and only if at minimum $2$ people who shake hands under one pattern (arrangement) don't shake hands under the other pattern (arrangement). Find $X$. I think casework is the way to go. $A$ shakes with $B$ & $C$.  $D$ shakes with $E$ & $F$. $G$ shakes with $H$ & $I$. Perhaps I could use a recurrence relation, but I don't see a possible way. In total there are: $$\binom{9}{3} \cdot \binom{6}{3} \cdot \binom{3}{3} = 1680$$ Ways to choose groups of three people. But I dont anything else to this problem, and clearly this is the wrong answer. Hints only please!","In a group of $9$ people, each person shakes hands with exactly $2$ of the other people from the group. Let $X$ be the number of possible ways to perform these handshakes. Take $2$ handshake patterns (arrangements) distinct if and only if at minimum $2$ people who shake hands under one pattern (arrangement) don't shake hands under the other pattern (arrangement). Find $X$. I think casework is the way to go. $A$ shakes with $B$ & $C$.  $D$ shakes with $E$ & $F$. $G$ shakes with $H$ & $I$. Perhaps I could use a recurrence relation, but I don't see a possible way. In total there are: $$\binom{9}{3} \cdot \binom{6}{3} \cdot \binom{3}{3} = 1680$$ Ways to choose groups of three people. But I dont anything else to this problem, and clearly this is the wrong answer. Hints only please!",,"['probability', 'combinatorics', 'algebra-precalculus', 'elementary-number-theory', 'contest-math']"
73,Trying to understand the behaviour of i.i.d.,Trying to understand the behaviour of i.i.d.,,"In a course called introduction to probability theorem we are covering now i.i.d. (independent and identically distributed random variables). I already know when two variables are independent: $X, Y$ are independent if $P(X \le a, Y \le b) = P(X \le a) \cdot P(Y \le b)$ for all possible choices of $a, b$. The following definition comes from Wikipedia: In probability theory and statistics, a sequence or other collection   of random variables is independent and identically distributed   (i.i.d.) if each random variable has the same probability distribution   as the others and all are mutually independent. What does ""the same probability distribution"" mean? That $P(X \le a) = P(y \le a)$ (assuming that we are interested in only two random variables, $X$ and $Y$)? What are (not necessary interesting) examples of i.i.d. variables, that aren't discrete? This exercise has given rise to serious doubts regarding the understanding of random variables: 14. Let $X, Y$ have the same probability distribution. Is it true that $\mathbb E[\frac{X}{X+Y}] = \mathbb E[\frac{Y}{X+Y}]$? At first I thought that the answer if affirmative, but I've heard that there is a counterexample (which I can't find). If it's negative, what is wrong with my intuition behind such equalities?","In a course called introduction to probability theorem we are covering now i.i.d. (independent and identically distributed random variables). I already know when two variables are independent: $X, Y$ are independent if $P(X \le a, Y \le b) = P(X \le a) \cdot P(Y \le b)$ for all possible choices of $a, b$. The following definition comes from Wikipedia: In probability theory and statistics, a sequence or other collection   of random variables is independent and identically distributed   (i.i.d.) if each random variable has the same probability distribution   as the others and all are mutually independent. What does ""the same probability distribution"" mean? That $P(X \le a) = P(y \le a)$ (assuming that we are interested in only two random variables, $X$ and $Y$)? What are (not necessary interesting) examples of i.i.d. variables, that aren't discrete? This exercise has given rise to serious doubts regarding the understanding of random variables: 14. Let $X, Y$ have the same probability distribution. Is it true that $\mathbb E[\frac{X}{X+Y}] = \mathbb E[\frac{Y}{X+Y}]$? At first I thought that the answer if affirmative, but I've heard that there is a counterexample (which I can't find). If it's negative, what is wrong with my intuition behind such equalities?",,"['probability', 'probability-theory']"
74,"A fair die is rolled $N > 6$ times. What is the probability the last 6 rolls were exactly $1,2,3,4,5,6$?",A fair die is rolled  times. What is the probability the last 6 rolls were exactly ?,"N > 6 1,2,3,4,5,6","You rolls a fair die $N > 6$ times and you want to rolls the sequence $1,2,3,4,5,6$ in this order. What is the probability that the last 6 rolls were (in consecutive order) $1,2,3,4,5,6$, (So, on the Nth roll you get a 6, on the $(N-1)$th roll you get a 5, etc.), AND you did not roll this sequence any times before the last 6 rolls. If I state another way, what is the probability that you roll the sequence 1,2,3,4,5,6 starting from the $(N-5)$th roll and you did not roll this sequence starting from any roll before the $(N-5)$th roll? This is not a homework problems, just something I'm thinking about.","You rolls a fair die $N > 6$ times and you want to rolls the sequence $1,2,3,4,5,6$ in this order. What is the probability that the last 6 rolls were (in consecutive order) $1,2,3,4,5,6$, (So, on the Nth roll you get a 6, on the $(N-1)$th roll you get a 5, etc.), AND you did not roll this sequence any times before the last 6 rolls. If I state another way, what is the probability that you roll the sequence 1,2,3,4,5,6 starting from the $(N-5)$th roll and you did not roll this sequence starting from any roll before the $(N-5)$th roll? This is not a homework problems, just something I'm thinking about.",,"['probability', 'dice']"
75,Puzzle: How Many Possibilities Are There Between Connected Points?,Puzzle: How Many Possibilities Are There Between Connected Points?,,"Puzzle Jenny drew on her page six points, as shown below: Jenny wants to build a cool match of her points. In a match , divide the six-point into pairs, so that each point has one partner exactly. Afterwards, each pair will be connected with a line. The condition that the match is called ""cool match"" the connected lines of the pairs must not cut each other, then the match is called a ""cool match"" . Jenny found that for her 6 points, there are five ""cool matches"" possible: Danny Drew on his Page 12 points, as shown below: How many ""Cool matches"" are possible in Danny's 12 points? Bonus: Same Puzzle. What will be the answer (how many possible ""cool matches"") are there for a drawing of 50 points? Another Bonus: How many possible ""cool matches"" are there for a drawing of 1,000,000 (1 Million) Points? This will be a big number, just write its remainder when dividing it by 1,000,000,007 (Billion and seven).","Puzzle Jenny drew on her page six points, as shown below: Jenny wants to build a cool match of her points. In a match , divide the six-point into pairs, so that each point has one partner exactly. Afterwards, each pair will be connected with a line. The condition that the match is called ""cool match"" the connected lines of the pairs must not cut each other, then the match is called a ""cool match"" . Jenny found that for her 6 points, there are five ""cool matches"" possible: Danny Drew on his Page 12 points, as shown below: How many ""Cool matches"" are possible in Danny's 12 points? Bonus: Same Puzzle. What will be the answer (how many possible ""cool matches"") are there for a drawing of 50 points? Another Bonus: How many possible ""cool matches"" are there for a drawing of 1,000,000 (1 Million) Points? This will be a big number, just write its remainder when dividing it by 1,000,000,007 (Billion and seven).",,"['probability', 'combinatorics']"
76,$\{X_n\}$ are iid random variables with symmetric distribution,are iid random variables with symmetric distribution,\{X_n\},"Let $X_1,X_2,\ldots,X_n$ be iid random variables with symmetric distribution. Show that   $$P\left(|X_1+X_2+\cdots+X_n|\ge \max_{1\le i\le n}|X_i|\right)\ge \frac12.$$ I was trying it for $n=2$. Let $I=P(|X+Y|\ge |X| \cap |X+Y| \ge |Y|)$. It is equivalent to $$1-P(|X+Y|<|X| \cup |X+Y|<|Y|)=1-2P(|X+Y|<|X|)=1-2P(Y^2<-2XY)$$ Now $P(Y^2<-2XY)=P(Y^2<-2XY\mid XY\le 0)P(XY\le 0)=\frac12(P(Y^2<-2XY\mid XY\le 0)$. But could not proceed further.","Let $X_1,X_2,\ldots,X_n$ be iid random variables with symmetric distribution. Show that   $$P\left(|X_1+X_2+\cdots+X_n|\ge \max_{1\le i\le n}|X_i|\right)\ge \frac12.$$ I was trying it for $n=2$. Let $I=P(|X+Y|\ge |X| \cap |X+Y| \ge |Y|)$. It is equivalent to $$1-P(|X+Y|<|X| \cup |X+Y|<|Y|)=1-2P(|X+Y|<|X|)=1-2P(Y^2<-2XY)$$ Now $P(Y^2<-2XY)=P(Y^2<-2XY\mid XY\le 0)P(XY\le 0)=\frac12(P(Y^2<-2XY\mid XY\le 0)$. But could not proceed further.",,"['probability', 'probability-theory', 'random-variables']"
77,"I pull $17$ balls out of a bag, and there are $13$ distinct colors in the sample. About how many colors are in the bag?","I pull  balls out of a bag, and there are  distinct colors in the sample. About how many colors are in the bag?",17 13,"I have a bag filled with different colors of balls.  My goal is to determine the number of distinct colors that in the bag, but I am limited to taking a small sample.  From a sample of $N$ balls, I see that there $X$ different colors.  What is the expected number of different colors in the bag? Some assumptions which need to be made: The bag is of sufficiently large size that the probability of drawing a certain color does not depend on the how many balls we have already drawn.  (Effectively, we are drawing with replacement.) There is an equal number of each color in the bag. For an example, let's say that I draw $N=17$ balls out of the bag, and I see $X=13$ distinct colors.  What is a good estimate for the number of colors in the bag? So far, I have made little progress towards answering this on my own. I have tried to reverse the solution to the coupon collector's problem (as to solve for the number of colors as opposed to the number of trials), but I became stuck since it involved the harmonic numbers.","I have a bag filled with different colors of balls.  My goal is to determine the number of distinct colors that in the bag, but I am limited to taking a small sample.  From a sample of $N$ balls, I see that there $X$ different colors.  What is the expected number of different colors in the bag? Some assumptions which need to be made: The bag is of sufficiently large size that the probability of drawing a certain color does not depend on the how many balls we have already drawn.  (Effectively, we are drawing with replacement.) There is an equal number of each color in the bag. For an example, let's say that I draw $N=17$ balls out of the bag, and I see $X=13$ distinct colors.  What is a good estimate for the number of colors in the bag? So far, I have made little progress towards answering this on my own. I have tried to reverse the solution to the coupon collector's problem (as to solve for the number of colors as opposed to the number of trials), but I became stuck since it involved the harmonic numbers.",,['probability']
78,$K$ events that are $(K-1)$-wise Independent but not Mutually/Fully Independent,events that are -wise Independent but not Mutually/Fully Independent,K (K-1),"I had the following question: Construct a probability space $(\Omega,P)$ and $k$ events, each with probability $\frac12$, that are $(k-1)$-wise, but not fully independent. Make the sample space as small as possible. I tried to answer it, but I just couldn't arrive at a correct solution. I solved the cases for $k=3$ and $k=6$, but I didn't arrive at a correct generalization. I made two different attempts (below), in both of which I think I had the general idea, but did not create the Events correctly. Can anyone please guide me to the correct solution? Assume we are given some $k \geq 3$. If $k$ is even, proceed. If $k$ is odd, let $k=k+1$.   First, we define $\Omega = \{A_1,\ldots,A_k\}$, ensuring that $|\Omega|$ is even. We then define the probability distribution $P$ such that $p(A_i) = \frac{1}{k} \forall i$. Because $|\Omega|$ is even and probability is uniformly distributed, we can then select $k$ subsets of $\Omega$, each of size $\frac{k}{2}$ as such:   \begin{align*} E_1 = \{A_1,\ldots,A_{k/2}\}\\ E_2 = \{A_2,\ldots,A_{k/2+1}\}\\ \ldots \\ E_{k/2} = \{A_{k/2},A_{k/2+1},\ldots,A_k\}\\ E_{k/2 + 1} = \{A_{k/2+1},A_{k/2+2}\ldots,A_k,A_1\}\\ \ldots\\ E_k = \{A_k,A_1,\ldots,A_{k/2-1}\} \end{align*}   As each event contains $\frac k2$ outcomes, and each outcome has probability $\frac 1k$, it is clear that $\forall i, P(E_i) = \sum_{a \in E_i} P(a) = \sum_{i=1}^{\frac k2} \frac 1k = \frac k2 \cdot \frac 1k = \frac12$. In other words, each event has probability $\frac 12$. We note that because all events are distinct, have size $\frac k2$, but there are $k$ events, it is the case that $E_1 \cap E_2 \cap \ldots \cap E_k = \emptyset$. (Though this is apparent just from how we have selected our events.) Consequently, $P(E_1 \cap E_2 \cap \ldots \cap E_k) = 0$. However, as each event $E_i$ has probability $\frac 12$, we have that $P(E_1)P(E_2)\ldots P(E_k) = \left(\frac 12 \right)^k \neq 0$. Since $P(E_1)P(E_2)\ldots P(E_k) \neq P(E_1 \cap E_2 \cap \ldots \cap E_k)$, it is the case that our $k$ events are not mutually/fully independent. Now all that remains is to show that the $k$ events are ($k-1$)-wise independent: this is actually not true. Sorry. And another attempt: this one is more hurried as I was running out of time... In order to ensure $(k-1)$-wise independence, we will create events as such:   \begin{align*} E_1 = \{A_1,A_2\}\\ E_2 = \{A_1,A_3\}\\ \ldots \\ E_{l -1} = \{A_1,A_l\}\\ E_{l} = \{A_2,A_3\}\\ E_{l+1} = \{A_2,A_4\}\\ \ldots\\ E_{2l-2} = \{A_2,A_l\}\\ E_{2l-1} = \{A_3,A_4\}\\ \ldots\\ E_k = \{A_{l-1},A_l\}\\ \end{align*}   This means if we want $k$ events, we need some $l$ such that $k = \frac{l^2-l}{2}$. This comes down to a simple summation $\sum_{i=1}^{l-1}l-i$ that equals $\frac{l(l-1)}{2}$, i.e. $\frac{l(l-1)}{2} =k$. So knowing $k$ we find $l$ and define our sample space $\Omega = \{A_1,A_2,\ldots,A_l\}$, again with uniform probability distribution $P$ such that $\forall i, P(A_i) = \frac 1l$. Then for any pair $E_a,E_b$ we will have $P(E_a \cap E_b) = p(A_x)$, where $A_x \in \Omega$. Because of the way we created the events, any two events will share exactly $1$ or $0$ outcomes $\ldots$ and here this attempt fails as well.","I had the following question: Construct a probability space $(\Omega,P)$ and $k$ events, each with probability $\frac12$, that are $(k-1)$-wise, but not fully independent. Make the sample space as small as possible. I tried to answer it, but I just couldn't arrive at a correct solution. I solved the cases for $k=3$ and $k=6$, but I didn't arrive at a correct generalization. I made two different attempts (below), in both of which I think I had the general idea, but did not create the Events correctly. Can anyone please guide me to the correct solution? Assume we are given some $k \geq 3$. If $k$ is even, proceed. If $k$ is odd, let $k=k+1$.   First, we define $\Omega = \{A_1,\ldots,A_k\}$, ensuring that $|\Omega|$ is even. We then define the probability distribution $P$ such that $p(A_i) = \frac{1}{k} \forall i$. Because $|\Omega|$ is even and probability is uniformly distributed, we can then select $k$ subsets of $\Omega$, each of size $\frac{k}{2}$ as such:   \begin{align*} E_1 = \{A_1,\ldots,A_{k/2}\}\\ E_2 = \{A_2,\ldots,A_{k/2+1}\}\\ \ldots \\ E_{k/2} = \{A_{k/2},A_{k/2+1},\ldots,A_k\}\\ E_{k/2 + 1} = \{A_{k/2+1},A_{k/2+2}\ldots,A_k,A_1\}\\ \ldots\\ E_k = \{A_k,A_1,\ldots,A_{k/2-1}\} \end{align*}   As each event contains $\frac k2$ outcomes, and each outcome has probability $\frac 1k$, it is clear that $\forall i, P(E_i) = \sum_{a \in E_i} P(a) = \sum_{i=1}^{\frac k2} \frac 1k = \frac k2 \cdot \frac 1k = \frac12$. In other words, each event has probability $\frac 12$. We note that because all events are distinct, have size $\frac k2$, but there are $k$ events, it is the case that $E_1 \cap E_2 \cap \ldots \cap E_k = \emptyset$. (Though this is apparent just from how we have selected our events.) Consequently, $P(E_1 \cap E_2 \cap \ldots \cap E_k) = 0$. However, as each event $E_i$ has probability $\frac 12$, we have that $P(E_1)P(E_2)\ldots P(E_k) = \left(\frac 12 \right)^k \neq 0$. Since $P(E_1)P(E_2)\ldots P(E_k) \neq P(E_1 \cap E_2 \cap \ldots \cap E_k)$, it is the case that our $k$ events are not mutually/fully independent. Now all that remains is to show that the $k$ events are ($k-1$)-wise independent: this is actually not true. Sorry. And another attempt: this one is more hurried as I was running out of time... In order to ensure $(k-1)$-wise independence, we will create events as such:   \begin{align*} E_1 = \{A_1,A_2\}\\ E_2 = \{A_1,A_3\}\\ \ldots \\ E_{l -1} = \{A_1,A_l\}\\ E_{l} = \{A_2,A_3\}\\ E_{l+1} = \{A_2,A_4\}\\ \ldots\\ E_{2l-2} = \{A_2,A_l\}\\ E_{2l-1} = \{A_3,A_4\}\\ \ldots\\ E_k = \{A_{l-1},A_l\}\\ \end{align*}   This means if we want $k$ events, we need some $l$ such that $k = \frac{l^2-l}{2}$. This comes down to a simple summation $\sum_{i=1}^{l-1}l-i$ that equals $\frac{l(l-1)}{2}$, i.e. $\frac{l(l-1)}{2} =k$. So knowing $k$ we find $l$ and define our sample space $\Omega = \{A_1,A_2,\ldots,A_l\}$, again with uniform probability distribution $P$ such that $\forall i, P(A_i) = \frac 1l$. Then for any pair $E_a,E_b$ we will have $P(E_a \cap E_b) = p(A_x)$, where $A_x \in \Omega$. Because of the way we created the events, any two events will share exactly $1$ or $0$ outcomes $\ldots$ and here this attempt fails as well.",,"['probability', 'combinatorics', 'probability-theory', 'discrete-mathematics']"
79,Spectral gap of mixture of Markov chains,Spectral gap of mixture of Markov chains,,"Context Let $P$ be the transition matrix of an irreducible, aperiodic, discrete-time Markov chain. The spectral gap is given by $$\xi = 1 - \lambda_\max$$ where $\lambda_\max = \max\{\lambda_2, -\lambda_n\}$, i.e. the second largest eigenvalue of the transition matrix $P$. This is related to the mixing time of the Markov chain; the bigger the spectral gap, the faster the convergence to the stationary distribution. Problem Now suppose I have two such chains on the same state space, with transition matrices $P_1$ and $P_2$, and spectral gaps $\xi_1$ and $\xi_2$. Furthermore, let me define a new Markov chain with transition matrix: $$ P' = \alpha P_1 + (1-\alpha) P_2 $$ in other words, each transition is done according to $P_1$ with probability $\alpha$, and according to $P_2$ with probability $1 - \alpha$. Now the question is, what can I say about the spectral gap of $P'$ ? Intuitively, I would guess that the spectral gap is concave, i.e. $$\xi' \ge \alpha \xi_1 + (1-\alpha) \xi_2$$ But I have no idea how to show this... Any help is appreciated!","Context Let $P$ be the transition matrix of an irreducible, aperiodic, discrete-time Markov chain. The spectral gap is given by $$\xi = 1 - \lambda_\max$$ where $\lambda_\max = \max\{\lambda_2, -\lambda_n\}$, i.e. the second largest eigenvalue of the transition matrix $P$. This is related to the mixing time of the Markov chain; the bigger the spectral gap, the faster the convergence to the stationary distribution. Problem Now suppose I have two such chains on the same state space, with transition matrices $P_1$ and $P_2$, and spectral gaps $\xi_1$ and $\xi_2$. Furthermore, let me define a new Markov chain with transition matrix: $$ P' = \alpha P_1 + (1-\alpha) P_2 $$ in other words, each transition is done according to $P_1$ with probability $\alpha$, and according to $P_2$ with probability $1 - \alpha$. Now the question is, what can I say about the spectral gap of $P'$ ? Intuitively, I would guess that the spectral gap is concave, i.e. $$\xi' \ge \alpha \xi_1 + (1-\alpha) \xi_2$$ But I have no idea how to show this... Any help is appreciated!",,"['probability', 'graph-theory', 'markov-chains', 'spectral-graph-theory']"
80,What is the probability of product of two elements is desired element?,What is the probability of product of two elements is desired element?,,"Let $G$ be a group with $n$ element. Fix $x\in G$. If you choose randomly two elements from $G$, what is the probability of $x$ being product of these two elements? At first, I thought answer was $1/n$, because, if $ab=x$ and if I choose $a$, it uniquely determines $b$. I guess it is true answer when $G$ is abelian. But when $G$ is nonabeliean, $ab$ and $ba$ may be different elements; therefore, the probability is higher than $1/n$. I can't say the answer is $2/n$ since some pairs may still commute in nonabelian group. I also noticed that the probability also depends on $x$, because, if $x=e$, you must choose $a,a^{-1}$ as a pair, so answer is $1/n$ regardless of $G$ is abelian or not. If we denote this probability as $P_x(G)$, I think $1/n\leq P_x(G)\leq 2/n$. Any further result will be appreciated. As  Geoff Robinson request let me clarify what I mean, Let $w\in GxG$ i,e, $w=(a,b)$ let  say that $w$ know answer if $ab=x$ or $ba=x$. What is the probability that $w$ know the answer?","Let $G$ be a group with $n$ element. Fix $x\in G$. If you choose randomly two elements from $G$, what is the probability of $x$ being product of these two elements? At first, I thought answer was $1/n$, because, if $ab=x$ and if I choose $a$, it uniquely determines $b$. I guess it is true answer when $G$ is abelian. But when $G$ is nonabeliean, $ab$ and $ba$ may be different elements; therefore, the probability is higher than $1/n$. I can't say the answer is $2/n$ since some pairs may still commute in nonabelian group. I also noticed that the probability also depends on $x$, because, if $x=e$, you must choose $a,a^{-1}$ as a pair, so answer is $1/n$ regardless of $G$ is abelian or not. If we denote this probability as $P_x(G)$, I think $1/n\leq P_x(G)\leq 2/n$. Any further result will be appreciated. As  Geoff Robinson request let me clarify what I mean, Let $w\in GxG$ i,e, $w=(a,b)$ let  say that $w$ know answer if $ab=x$ or $ba=x$. What is the probability that $w$ know the answer?",,"['probability', 'combinatorics', 'group-theory', 'finite-groups']"
81,"What is the sample space, outcomes, event space, random variables in machine learning?","What is the sample space, outcomes, event space, random variables in machine learning?",,"Reading through materials of machine learning problems, I see people treating things like they are doing with probability. Particularly, consider linear regression. I cannot figure out what is the sample space, outcomes, events, random variables. In what manner they are using the word ""probability measure"" in the field of machine learning ? For instance, please take a look at this article. Moreover, in general, why do we need probability for machine learning? I don't see how we are going to calculate any probability of something in machine learning. While the need of linear algebra is obvious, since we are working on lists matrix of numbers. What properties of probability make it essential for machine learning?","Reading through materials of machine learning problems, I see people treating things like they are doing with probability. Particularly, consider linear regression. I cannot figure out what is the sample space, outcomes, events, random variables. In what manner they are using the word ""probability measure"" in the field of machine learning ? For instance, please take a look at this article. Moreover, in general, why do we need probability for machine learning? I don't see how we are going to calculate any probability of something in machine learning. While the need of linear algebra is obvious, since we are working on lists matrix of numbers. What properties of probability make it essential for machine learning?",,"['probability', 'machine-learning']"
82,Surface of a sphere and cube,Surface of a sphere and cube,,"I have a sphere $S^2 \in \mathbb{R}^3$ with radius 1 that is painted red on the surface (90% of it), the rest is painted blue. Now I shall show for every configuration of a  cube that is possible in this sphere, there is always at least one configuration such that a cube with length 2 down each diagonal(that means all corners touch the sphere) has all corners on red painted points. My idea was the following: If this was wrong than at each position of the cube there would be a point that is painted blue. Since we have 8 corners, the minimum likelihood for blue would necessarily be 12.5%, such that this fails, right? But somehow I think my argument is not very strong, as this is one that refers to a discrete configuration. But here we are talking about anuncountable number of points. maybe anybody here knows how to make my argument more rigorous, if it is not even wrong after all? If something is still unclear, please let me know.","I have a sphere $S^2 \in \mathbb{R}^3$ with radius 1 that is painted red on the surface (90% of it), the rest is painted blue. Now I shall show for every configuration of a  cube that is possible in this sphere, there is always at least one configuration such that a cube with length 2 down each diagonal(that means all corners touch the sphere) has all corners on red painted points. My idea was the following: If this was wrong than at each position of the cube there would be a point that is painted blue. Since we have 8 corners, the minimum likelihood for blue would necessarily be 12.5%, such that this fails, right? But somehow I think my argument is not very strong, as this is one that refers to a discrete configuration. But here we are talking about anuncountable number of points. maybe anybody here knows how to make my argument more rigorous, if it is not even wrong after all? If something is still unclear, please let me know.",,['probability']
83,Probability that a vertex in the spanning tree of an $N$ x $N$ grid graph is a leaf,Probability that a vertex in the spanning tree of an  x  grid graph is a leaf,N N,"Suppose we have an $N$ x $N$ grid graph $G(V,E)$ and we construct a spanning tree of this graph in the following way. Start with a set $S$ which contains only the vertex at the top left corner of the grid graph (i.e location $(0,0)$), and an empty set $T$ which will contain our spanning tree. Now pick an edge at random (with equal probability) from the set of  edges $(p,q)\in E(G)$ such that $p \in S$ and $q \not \in S$. Add this edge to our set $T$ and add $q$ to set $S$, repeat this process until we obtain a spanning tree. Now, the problem is this: What is the probability that a given vertex of the grid graph at location $(x,y)$ is a leaf in our spanning tree? In general, what is the expected number of leafs in a spanning tree constructed in this way? If this problem is too difficult, what can we say about a smaller grid? For example if we consider an $N$ x $2$, $N$ x $3$, ... e.t.c grid graph Note: This was inspired by a more general question asked by Nick Wu on Quora","Suppose we have an $N$ x $N$ grid graph $G(V,E)$ and we construct a spanning tree of this graph in the following way. Start with a set $S$ which contains only the vertex at the top left corner of the grid graph (i.e location $(0,0)$), and an empty set $T$ which will contain our spanning tree. Now pick an edge at random (with equal probability) from the set of  edges $(p,q)\in E(G)$ such that $p \in S$ and $q \not \in S$. Add this edge to our set $T$ and add $q$ to set $S$, repeat this process until we obtain a spanning tree. Now, the problem is this: What is the probability that a given vertex of the grid graph at location $(x,y)$ is a leaf in our spanning tree? In general, what is the expected number of leafs in a spanning tree constructed in this way? If this problem is too difficult, what can we say about a smaller grid? For example if we consider an $N$ x $2$, $N$ x $3$, ... e.t.c grid graph Note: This was inspired by a more general question asked by Nick Wu on Quora",,"['probability', 'graph-theory', 'algorithms', 'random-variables', 'algorithmic-randomness']"
84,Joint distribution by independent distributions,Joint distribution by independent distributions,,"We have $N$ independent discrete finite random variables (RVs) $X_1,\dots,X_i,\dots,X_N$ where RV $X_i$ has $M_i$ finite number of elements. We are free to choose any distribution $f_i$ for RV $X_i$ $\forall i=\{1,\dots,N\}$. Then consider the product set $Y = X_1\times\dots\times X_i\times\dots\times X_N$ and say we are interested in a particular distribution $f_Y$, which is in the set of all possible distributions on $Y$. Note $f_Y$ is not the product of $f_i$s. How close can we come to $f_Y$ by manipulating the independent distributions $f_i$ $\forall i=\{1,\dots,N\}$ ? Which I believe (not sure) is same as asking how close is $\prod_i^Nf_i$ to $f_Y$? There is a set of real numbers $\boldsymbol{a}=\{a(y)\}_{y\in Y}$. The objective is to make the expectation of the set $\boldsymbol{a}$ over $f_i$s as close as possible to the expectation of  $\boldsymbol{a}$ over $f_Y$. I tried writing an optimization problem to minimize $\mid \sum_{y\in Y}[\prod_i^Nf_i(y)-f_Y(y)]a(y)\mid$, but it is non convex. And I am not sure if this is the optimization problem I should solve. What does it mean to be ""close"" when we have two distributions? Note that just expectations being close is not sufficient, the probability $\prod_i^Nf_i(y)$ has to be close to the true probability $f_Y(y)$ $\forall y \in Y$. The question Distance between the product of marginal distributions and the joint distribution is bit similar but in there marginals come from the joint but in my question no marginals are compared. Would be very grateful for any clue. Thanks. PS. Not homework. Part of research work.","We have $N$ independent discrete finite random variables (RVs) $X_1,\dots,X_i,\dots,X_N$ where RV $X_i$ has $M_i$ finite number of elements. We are free to choose any distribution $f_i$ for RV $X_i$ $\forall i=\{1,\dots,N\}$. Then consider the product set $Y = X_1\times\dots\times X_i\times\dots\times X_N$ and say we are interested in a particular distribution $f_Y$, which is in the set of all possible distributions on $Y$. Note $f_Y$ is not the product of $f_i$s. How close can we come to $f_Y$ by manipulating the independent distributions $f_i$ $\forall i=\{1,\dots,N\}$ ? Which I believe (not sure) is same as asking how close is $\prod_i^Nf_i$ to $f_Y$? There is a set of real numbers $\boldsymbol{a}=\{a(y)\}_{y\in Y}$. The objective is to make the expectation of the set $\boldsymbol{a}$ over $f_i$s as close as possible to the expectation of  $\boldsymbol{a}$ over $f_Y$. I tried writing an optimization problem to minimize $\mid \sum_{y\in Y}[\prod_i^Nf_i(y)-f_Y(y)]a(y)\mid$, but it is non convex. And I am not sure if this is the optimization problem I should solve. What does it mean to be ""close"" when we have two distributions? Note that just expectations being close is not sufficient, the probability $\prod_i^Nf_i(y)$ has to be close to the true probability $f_Y(y)$ $\forall y \in Y$. The question Distance between the product of marginal distributions and the joint distribution is bit similar but in there marginals come from the joint but in my question no marginals are compared. Would be very grateful for any clue. Thanks. PS. Not homework. Part of research work.",,"['probability', 'measure-theory', 'probability-theory', 'probability-distributions']"
85,Probability of number of equally spaced numbers,Probability of number of equally spaced numbers,,"If you have a uniform sample of size $\ell$ of integers taken from $[1,\dots,n]$ which is then sorted, how do you calculate the probability that there exists an equally spaced subsequence of length  $k$ (or at least $k$)? As an example, if the sample is $\{1,5,10, 15,18,20\}$ so $\ell = 6$ and let us say that $n= 30$, then there is a subsequence of length $4$ which are equally spaced from each other.","If you have a uniform sample of size $\ell$ of integers taken from $[1,\dots,n]$ which is then sorted, how do you calculate the probability that there exists an equally spaced subsequence of length  $k$ (or at least $k$)? As an example, if the sample is $\{1,5,10, 15,18,20\}$ so $\ell = 6$ and let us say that $n= 30$, then there is a subsequence of length $4$ which are equally spaced from each other.",,['probability']
86,How to work with random variables?,How to work with random variables?,,If $X$ and $Y$ are independent random variables described by standard normal distribution could you please explain how to formally evaluate probabilities of occurrences such as $X-Y>0$ (intuitively it's $0.5$ of course) or $X^2-Y^2>0$? Ultimately I'd like to be able to tell how likely is it that parabola $z(t)=X^2+2Yt+t^2$ has a root in positive semi-axis of $t$.,If $X$ and $Y$ are independent random variables described by standard normal distribution could you please explain how to formally evaluate probabilities of occurrences such as $X-Y>0$ (intuitively it's $0.5$ of course) or $X^2-Y^2>0$? Ultimately I'd like to be able to tell how likely is it that parabola $z(t)=X^2+2Yt+t^2$ has a root in positive semi-axis of $t$.,,['probability']
87,Markov chain stochastic process,Markov chain stochastic process,,"Can anyone help me with this question, maybe by giving a hint. Consider a Markov chain with state space $\{0,1,2....\}$. A sequence of positive numbers $p_1,p_2,...$ is given with $\sum p_i=1$. Whenever the chain reaches 0 it chooses a new state according to the $p_i$. Whenever the chain is at a state other than $0$ it proceeds deterministically one step at a time towards $0$. Under what condition on $p_i$ is the chain positive recurrent? Thanks for your help.","Can anyone help me with this question, maybe by giving a hint. Consider a Markov chain with state space $\{0,1,2....\}$. A sequence of positive numbers $p_1,p_2,...$ is given with $\sum p_i=1$. Whenever the chain reaches 0 it chooses a new state according to the $p_i$. Whenever the chain is at a state other than $0$ it proceeds deterministically one step at a time towards $0$. Under what condition on $p_i$ is the chain positive recurrent? Thanks for your help.",,"['probability', 'markov-chains']"
88,"Removing a card while preserving ""shuffled-ness""","Removing a card while preserving ""shuffled-ness""",,"Suppose you have a deck of numbered cards (possibly with multiple copies of each number), perfectly shuffled, and you want to extract certain cards without disturbing the ""shuffled-ness"" of the remaining cards.  This is easy to do in some cases.  For instance, if you want to remove a unique card (the only $10$, say), or all cards with a particular number (all the $5$'s), then you can simply pull out the target cards while keeping the remaining cards in the same order.  If all $n!$ permutations of the full deck were equally likely before, then all $(n-k)!$ permutations of the remaining $n-k$ cards are equally likely now; that is, the remaining cards are still shuffled.  Similarly, if you remove the card in a pre-specified position (the topmost card, say) or set of positions (the bottom ten cards), then the remaining cards (whatever they are) will still be shuffled. My question is about the cases where the simple approach doesn't work.  For example, say you want to remove three of the four $7$'s (you don't care which ones).  If you just pull out the topmost three $7$'s, then the remaining cards will clearly not be shuffled: the fourth $7$ isn't equally likely to be at any position, but rather is much more likely to be near the bottom of the deck.  (The same problem occurs if you remove just one of the four $7$'s, but the bias is less extreme.)  If you instead move cards from the top of the deck to the bottom, removing $7$'s as you find them until only one is left, then you have the opposite problem: the remaining $7$ is now likely to be near the top. Is there any deterministic way to remove a single copy of a particular number without introducing a bias to the remaining cards? (Note: Clearly there can't be one in general .  For instance, if your cards are $\{1,2,2\}$, then there are three equiprobable deck configurations, and any mapping of these to the two target deck configurations $[1,2]$ and $[2,1]$ is going to have a bias.  But are there non-trivial cases where it's possible?)","Suppose you have a deck of numbered cards (possibly with multiple copies of each number), perfectly shuffled, and you want to extract certain cards without disturbing the ""shuffled-ness"" of the remaining cards.  This is easy to do in some cases.  For instance, if you want to remove a unique card (the only $10$, say), or all cards with a particular number (all the $5$'s), then you can simply pull out the target cards while keeping the remaining cards in the same order.  If all $n!$ permutations of the full deck were equally likely before, then all $(n-k)!$ permutations of the remaining $n-k$ cards are equally likely now; that is, the remaining cards are still shuffled.  Similarly, if you remove the card in a pre-specified position (the topmost card, say) or set of positions (the bottom ten cards), then the remaining cards (whatever they are) will still be shuffled. My question is about the cases where the simple approach doesn't work.  For example, say you want to remove three of the four $7$'s (you don't care which ones).  If you just pull out the topmost three $7$'s, then the remaining cards will clearly not be shuffled: the fourth $7$ isn't equally likely to be at any position, but rather is much more likely to be near the bottom of the deck.  (The same problem occurs if you remove just one of the four $7$'s, but the bias is less extreme.)  If you instead move cards from the top of the deck to the bottom, removing $7$'s as you find them until only one is left, then you have the opposite problem: the remaining $7$ is now likely to be near the top. Is there any deterministic way to remove a single copy of a particular number without introducing a bias to the remaining cards? (Note: Clearly there can't be one in general .  For instance, if your cards are $\{1,2,2\}$, then there are three equiprobable deck configurations, and any mapping of these to the two target deck configurations $[1,2]$ and $[2,1]$ is going to have a bias.  But are there non-trivial cases where it's possible?)",,"['probability', 'card-games']"
89,Computing the limit of the expectation of a function of a stochastic process (phew!),Computing the limit of the expectation of a function of a stochastic process (phew!),,"I state my problem in a few lines then describe what I have already done. I have a quite simple stochastic differential equation (SDE): $dx=-2x \, dt+\sqrt{1-x^2} \, dW$ with $W$ a brownian. I want to compute $\displaystyle{\lim_{t\to 0}}~\mathbb{E}\left[B_t\tanh\left(A_t\frac{x(t)-x(0)}{t}\right)|x(0)\right]$ and can't manage to do it. I want to describe a given phenomenon obeying my SDE, thus the factors B_t and A_t will depend on $t$. This is to ensure that as I decrease the time increment $t$ through which I approximate my continuous phenomenom by a ""discrete"" growth rate, as one cannot differentiate a Brownian, I will converge towards a given value. It is equivalent to the normalisation having to be applied to a random walk diffusion coefficient when one wants to converge to the ""underlying"" brownian.   EDIT : $A_t\propto \sqrt{t}$ and $B_t \propto \frac{1}{\sqrt{t}} This is my problem, any suggestions are welcome, below i expand on where I am, and how I approach the problem : Let $\phi(x(t),t)$ be a twice differentiable function, Ito's lemma yields that if $\phi(x(t),t)$ is solution of :  $$(1)~\frac{\partial \phi }{\partial t}-2x\frac{\partial \phi }{\partial x}+\frac{1-x^2}{2}\frac{\partial^2 \phi}{\partial x^2}=0\textrm{, with }\phi(x,0)=\Phi(x(0))\textrm{ as initial condition,}$$ then $\phi(x(t),t)=E[\Phi(x(0))|x(t)=x]$. Noting that the partial differential equation (1) can be rewritten as $\partial_t\phi=M[\phi]$ with $M[.]$ a linear operator, any function of the form : $$\phi(x,t)=f(x)+\displaystyle{\sum_{n=1}^\infty}\frac{t^n}{n!}M^n[f]$$ is a solution to the PDE (1), satisfying the initial condition $\phi(x,0)=f(x)$. Taking $\Phi(x(0))=x(0)$, for example, and with $t>0$, yields $$\mathbb{E}[x(t)|x(0)]=x(0)e^{-2t}$$ thus $\displaystyle{\lim_{t\to 0}}~\mathbb{E}\left[\frac{x(t)-x(0)}{t}|x(0)\right]=-2x(0)$ My problem is that $B_t~tanh\left(A_t\frac{x(t)-x(0)}{t}\right)$ is undefined at $t=0$, thus I cannot use the same approach. I tried to compute the characteristic function as i can compute all the moments, then Fourier-transform it to get the distribution, but it doesn't seem to give any meaningful result, which I guess comes from the fact that the distribution tends towards pathological functions (Dirac distributions) as $t\to 0$ thus I i am not allowed to switch the limit and the expectation operator anymore. I can expand if need be, but i think I'll already be lucky if anyone read this far :). Thanks in advance EDIT I also tried to Taylor expand the $tanh$ (not bothering about radius of convergence to start with) apply the expectation operator, which I can compute when applied to any power of $x$. By $b_{2n}$ I denote the coefficient of the Taylor expansion of $tanh$. \begin{eqnarray}  B_t\tanh\left(A_t\frac{x(t)-x(0)}{t}\right)&=&B_t\tanh\left(A_t\frac{\Delta x}{t}\right)\\ &=&B_t\displaystyle{\sum_{n=1}^\infty}b_{2n}\frac{A_t^{2n-1}}{t^{2n-1}}\Delta x^{2n-1}\\ &=&B_t\displaystyle{\sum_{n=1}^\infty}b_{2n}\frac{A_t^{2n-1}}{t^{2n-1}} \displaystyle{\sum_{k=0}^{2n-1}}(-1)^k\binom{2n-1}{k}x(0)^k x(t)^{2n-1-k} \end{eqnarray} For simplicity I denote $x(0)=x$ and $x(t)=x_t$, and I apply the expectation operator :  $$B_t\displaystyle{\sum_{n=1}^\infty}b_{2n}\frac{A_t^{2n-1}}{t^{2n-1}} \displaystyle{\sum_{k=0}^{2n-1}}(-1)^k\binom{2n-1}{k}x^k \mathbb{E}[x(t)^{2n-1-k}|x]$$ \begin{eqnarray}  =&B\displaystyle{\sum_{n=1}^\infty}b_{2n}\frac{A_t^{2n-1}}{t^{2n-1}}\displaystyle{\sum_{k=0}^{2n-1}}(-1)^k\binom{2n-1}{k}x^k \displaystyle{\sum_{m=0}^{\infty}}\frac{t^m}{m!}M^m[x^{2n-1-k}]\\ \end{eqnarray} Moreover, computations give that  :  $$\forall(n,k)\in\mathbb{N^2},~M^n(x^k)=\displaystyle{\sum_{i=0}^{\lfloor k/2\rfloor}}\alpha_{i,n}(k)x^{2i+\delta_k}$$ with $\delta_k=0$ if $k$ is even, $\delta_k=1$ if not,  $$\alpha_{i,n+1}(k)=\alpha_{i,n}(k)(2i^2+3i+\delta_k(2i+1))-\alpha_{i+1,n}(k)(2i^2+3i+\delta_k(2i+1))$$ and initial condition : $\alpha_{i,0}(k)=\delta_{i,\lfloor k/2 \rfloor}$, with $\delta_{i,j}$ Kronecker's symbol. We can rearrange the sum : \begin{eqnarray} S&=&B\displaystyle{\sum_{n=1}^\infty}b_{2n}\frac{A_t^{2n-1}}{t^{2n-1}}\displaystyle{\sum_{m=0}^{\infty}}\frac{t^m}{m!}\displaystyle{\sum_{k=0}^{2n-1}}(-1)^k\binom{2n-1}{k}x^k \displaystyle{\sum_{i=0}^{\lfloor k/2\rfloor}}\alpha_{i,m}(2n-1-k)x^{2i+\delta_{k+1}}\\ &=&B\displaystyle{\sum_{n=1}^\infty}b_{2n}\frac{A_t^{2n-1}}{t^{2n-1}}\displaystyle{\sum_{m=0}^{\infty}}\frac{t^m}{m!}\displaystyle{\sum_{k=0}^{2n-1}}(-1)^k\binom{2n-1}{k}\displaystyle{\sum_{i=\lfloor k/2\rfloor}^{n-1}} x^{2i+1} \alpha_{i,m}(2n-1-k) \end{eqnarray} In addition $x^{2i+1}\alpha_{i,m}(k)$ is a polynomial in $k$ of degree $d\leq 2m$, therefore its sum over $i$ has a maximal degree of $2m$ too. This implies that $$\forall m< n, ~\displaystyle{\sum_{k=0}^{2n-1}}(-1)^k\binom{2n-1}{k}\displaystyle{\sum_{i=\lfloor k/2\rfloor}^{n-1}} x^{2i+1} \alpha_{i,m}(2n-1-k)=0$$ We are left with : \begin{eqnarray} S&=&B_t\displaystyle{\sum_{n=1}^\infty}b_{2n}\frac{A_t^{2n-1}}{t^{2n-1}}\displaystyle{\sum_{m=n}^{\infty}}\frac{t^m}{m!}\displaystyle{\sum_{i=0}^{n-1}} x^{2i+1}\displaystyle{\sum_{k=0}^{2i+1}}(-1)^k\binom{2n-1}{k} \alpha_{i-\lfloor k/2\rfloor,m}(2n-1-k) \end{eqnarray} If we take $A\propto t$ and $B\propto \frac{1}{t}$, as $t\to0$, only the term $n=1$ is kept, therefore we obtain a result linear in $x$, which is neglecting all the terms in $x$ coming from the term $M^n(x^k)$. If $A\propto \sqrt{t}$ and $B\propto \frac{1}{\sqrt{t}}$ we keep all the terms $m=n$. This expansion reduces to $$S=\displaystyle{\sum_{k=0}^{\infty}}(-1)^{k+1} a_k C^{2k+1}x(1-x^2)^k$$ where $\lfloor n/2 \rfloor ! \leq a_k \leq n!$ numerically (I calculated the first 100 terms...). Thus this expansion diverges. This seems normal to me (now) because $tanh$ has a finite radius of convergence, and as $t\to 0$ when taking the expected value, $\frac{\Delta x}{\sqrt{t}}$ gets larger and larger for some values of $x_t$, those divergences are taken care of by the distribution of $x_t$ which converges to $0$, but the taylor expansion assumes that the argument of $tanh$ is bounded, which is not the case.","I state my problem in a few lines then describe what I have already done. I have a quite simple stochastic differential equation (SDE): $dx=-2x \, dt+\sqrt{1-x^2} \, dW$ with $W$ a brownian. I want to compute $\displaystyle{\lim_{t\to 0}}~\mathbb{E}\left[B_t\tanh\left(A_t\frac{x(t)-x(0)}{t}\right)|x(0)\right]$ and can't manage to do it. I want to describe a given phenomenon obeying my SDE, thus the factors B_t and A_t will depend on $t$. This is to ensure that as I decrease the time increment $t$ through which I approximate my continuous phenomenom by a ""discrete"" growth rate, as one cannot differentiate a Brownian, I will converge towards a given value. It is equivalent to the normalisation having to be applied to a random walk diffusion coefficient when one wants to converge to the ""underlying"" brownian.   EDIT : $A_t\propto \sqrt{t}$ and $B_t \propto \frac{1}{\sqrt{t}} This is my problem, any suggestions are welcome, below i expand on where I am, and how I approach the problem : Let $\phi(x(t),t)$ be a twice differentiable function, Ito's lemma yields that if $\phi(x(t),t)$ is solution of :  $$(1)~\frac{\partial \phi }{\partial t}-2x\frac{\partial \phi }{\partial x}+\frac{1-x^2}{2}\frac{\partial^2 \phi}{\partial x^2}=0\textrm{, with }\phi(x,0)=\Phi(x(0))\textrm{ as initial condition,}$$ then $\phi(x(t),t)=E[\Phi(x(0))|x(t)=x]$. Noting that the partial differential equation (1) can be rewritten as $\partial_t\phi=M[\phi]$ with $M[.]$ a linear operator, any function of the form : $$\phi(x,t)=f(x)+\displaystyle{\sum_{n=1}^\infty}\frac{t^n}{n!}M^n[f]$$ is a solution to the PDE (1), satisfying the initial condition $\phi(x,0)=f(x)$. Taking $\Phi(x(0))=x(0)$, for example, and with $t>0$, yields $$\mathbb{E}[x(t)|x(0)]=x(0)e^{-2t}$$ thus $\displaystyle{\lim_{t\to 0}}~\mathbb{E}\left[\frac{x(t)-x(0)}{t}|x(0)\right]=-2x(0)$ My problem is that $B_t~tanh\left(A_t\frac{x(t)-x(0)}{t}\right)$ is undefined at $t=0$, thus I cannot use the same approach. I tried to compute the characteristic function as i can compute all the moments, then Fourier-transform it to get the distribution, but it doesn't seem to give any meaningful result, which I guess comes from the fact that the distribution tends towards pathological functions (Dirac distributions) as $t\to 0$ thus I i am not allowed to switch the limit and the expectation operator anymore. I can expand if need be, but i think I'll already be lucky if anyone read this far :). Thanks in advance EDIT I also tried to Taylor expand the $tanh$ (not bothering about radius of convergence to start with) apply the expectation operator, which I can compute when applied to any power of $x$. By $b_{2n}$ I denote the coefficient of the Taylor expansion of $tanh$. \begin{eqnarray}  B_t\tanh\left(A_t\frac{x(t)-x(0)}{t}\right)&=&B_t\tanh\left(A_t\frac{\Delta x}{t}\right)\\ &=&B_t\displaystyle{\sum_{n=1}^\infty}b_{2n}\frac{A_t^{2n-1}}{t^{2n-1}}\Delta x^{2n-1}\\ &=&B_t\displaystyle{\sum_{n=1}^\infty}b_{2n}\frac{A_t^{2n-1}}{t^{2n-1}} \displaystyle{\sum_{k=0}^{2n-1}}(-1)^k\binom{2n-1}{k}x(0)^k x(t)^{2n-1-k} \end{eqnarray} For simplicity I denote $x(0)=x$ and $x(t)=x_t$, and I apply the expectation operator :  $$B_t\displaystyle{\sum_{n=1}^\infty}b_{2n}\frac{A_t^{2n-1}}{t^{2n-1}} \displaystyle{\sum_{k=0}^{2n-1}}(-1)^k\binom{2n-1}{k}x^k \mathbb{E}[x(t)^{2n-1-k}|x]$$ \begin{eqnarray}  =&B\displaystyle{\sum_{n=1}^\infty}b_{2n}\frac{A_t^{2n-1}}{t^{2n-1}}\displaystyle{\sum_{k=0}^{2n-1}}(-1)^k\binom{2n-1}{k}x^k \displaystyle{\sum_{m=0}^{\infty}}\frac{t^m}{m!}M^m[x^{2n-1-k}]\\ \end{eqnarray} Moreover, computations give that  :  $$\forall(n,k)\in\mathbb{N^2},~M^n(x^k)=\displaystyle{\sum_{i=0}^{\lfloor k/2\rfloor}}\alpha_{i,n}(k)x^{2i+\delta_k}$$ with $\delta_k=0$ if $k$ is even, $\delta_k=1$ if not,  $$\alpha_{i,n+1}(k)=\alpha_{i,n}(k)(2i^2+3i+\delta_k(2i+1))-\alpha_{i+1,n}(k)(2i^2+3i+\delta_k(2i+1))$$ and initial condition : $\alpha_{i,0}(k)=\delta_{i,\lfloor k/2 \rfloor}$, with $\delta_{i,j}$ Kronecker's symbol. We can rearrange the sum : \begin{eqnarray} S&=&B\displaystyle{\sum_{n=1}^\infty}b_{2n}\frac{A_t^{2n-1}}{t^{2n-1}}\displaystyle{\sum_{m=0}^{\infty}}\frac{t^m}{m!}\displaystyle{\sum_{k=0}^{2n-1}}(-1)^k\binom{2n-1}{k}x^k \displaystyle{\sum_{i=0}^{\lfloor k/2\rfloor}}\alpha_{i,m}(2n-1-k)x^{2i+\delta_{k+1}}\\ &=&B\displaystyle{\sum_{n=1}^\infty}b_{2n}\frac{A_t^{2n-1}}{t^{2n-1}}\displaystyle{\sum_{m=0}^{\infty}}\frac{t^m}{m!}\displaystyle{\sum_{k=0}^{2n-1}}(-1)^k\binom{2n-1}{k}\displaystyle{\sum_{i=\lfloor k/2\rfloor}^{n-1}} x^{2i+1} \alpha_{i,m}(2n-1-k) \end{eqnarray} In addition $x^{2i+1}\alpha_{i,m}(k)$ is a polynomial in $k$ of degree $d\leq 2m$, therefore its sum over $i$ has a maximal degree of $2m$ too. This implies that $$\forall m< n, ~\displaystyle{\sum_{k=0}^{2n-1}}(-1)^k\binom{2n-1}{k}\displaystyle{\sum_{i=\lfloor k/2\rfloor}^{n-1}} x^{2i+1} \alpha_{i,m}(2n-1-k)=0$$ We are left with : \begin{eqnarray} S&=&B_t\displaystyle{\sum_{n=1}^\infty}b_{2n}\frac{A_t^{2n-1}}{t^{2n-1}}\displaystyle{\sum_{m=n}^{\infty}}\frac{t^m}{m!}\displaystyle{\sum_{i=0}^{n-1}} x^{2i+1}\displaystyle{\sum_{k=0}^{2i+1}}(-1)^k\binom{2n-1}{k} \alpha_{i-\lfloor k/2\rfloor,m}(2n-1-k) \end{eqnarray} If we take $A\propto t$ and $B\propto \frac{1}{t}$, as $t\to0$, only the term $n=1$ is kept, therefore we obtain a result linear in $x$, which is neglecting all the terms in $x$ coming from the term $M^n(x^k)$. If $A\propto \sqrt{t}$ and $B\propto \frac{1}{\sqrt{t}}$ we keep all the terms $m=n$. This expansion reduces to $$S=\displaystyle{\sum_{k=0}^{\infty}}(-1)^{k+1} a_k C^{2k+1}x(1-x^2)^k$$ where $\lfloor n/2 \rfloor ! \leq a_k \leq n!$ numerically (I calculated the first 100 terms...). Thus this expansion diverges. This seems normal to me (now) because $tanh$ has a finite radius of convergence, and as $t\to 0$ when taking the expected value, $\frac{\Delta x}{\sqrt{t}}$ gets larger and larger for some values of $x_t$, those divergences are taken care of by the distribution of $x_t$ which converges to $0$, but the taylor expansion assumes that the argument of $tanh$ is bounded, which is not the case.",,"['probability', 'ordinary-differential-equations', 'stochastic-processes', 'brownian-motion', 'stochastic-calculus']"
90,"Why $\frac{X+YZ}{\sqrt{1+Z^2}} \sim \mathcal{N}(0,1)$ if $X,Y,Z$ are i.i.d. $\mathcal{N}(0,1)$?",Why  if  are i.i.d. ?,"\frac{X+YZ}{\sqrt{1+Z^2}} \sim \mathcal{N}(0,1) X,Y,Z \mathcal{N}(0,1)","The original question was: Suppose $X,Y,Z$ are i.i.d. $\mathcal{N}(0,1)$, find a nonnegative continuous function $g$ such that $\frac{X+YZ}{g(Z)} \sim \mathcal{N}(0,1)$. The solution says, since $E(X+YZ\mid Z=z)=0$ and $Var(X+YZ\mid Z=z)=1+z^2$, for all $z \in \mathcal{R}$. So $g=\sqrt{1+Z^2}$. I see the calculation of the expectation and the variance, but not sure why $\frac{X+YZ}{g(Z)}$ follows a normal distribution. The solution says, since $X+YZ\mid Z=z$ follows a normal distribution for all $z$, thus $X+YZ$ is normal.","The original question was: Suppose $X,Y,Z$ are i.i.d. $\mathcal{N}(0,1)$, find a nonnegative continuous function $g$ such that $\frac{X+YZ}{g(Z)} \sim \mathcal{N}(0,1)$. The solution says, since $E(X+YZ\mid Z=z)=0$ and $Var(X+YZ\mid Z=z)=1+z^2$, for all $z \in \mathcal{R}$. So $g=\sqrt{1+Z^2}$. I see the calculation of the expectation and the variance, but not sure why $\frac{X+YZ}{g(Z)}$ follows a normal distribution. The solution says, since $X+YZ\mid Z=z$ follows a normal distribution for all $z$, thus $X+YZ$ is normal.",,['probability']
91,Is the chance of breaking even in this coin toss game $43.75\%$?,Is the chance of breaking even in this coin toss game ?,43.75\%,"I'm trying to do the maths on a coin toss game after 100 games but think I am failing. The rules are as follows. we start with 1000 coins we always bet that heads come up minimum bet is 10 coins maximum bet is 80 coins if tails comes up, we lose our bet if heads comes up, we win twice as much as we bet the coin is fair and so the probability of heads is 50% we start with a bet of 10 if we lose we double our bet if we lose the maximum bet of 80 we start at a bet of 10 again if we win we start at a bet of 10 again So in this set up, 50% of the time we would get our 10 coins back plus 10 additional coins. When we lose we need to lose four times in a row which I've figured out will be 6.25% which will cost 150 coins. 50% (10 coins) * 50% (20 coins) * 50% (40 coins) * (80 coins) 50% = 6.25% (150 coins total) Would I be right in thinking that this means that the chance of breaking even is 43.75%? So after 100 games we would have the win and loss as below and end up with the balance after the wins and losses are applied. Win: (50% win chance * 100 games) * 10 coins won = 500 coins won Loss: (6.25% loss chance * 100 games) * 150 coins = 937.5 coins lost Total: 1000 start + 500 win - 937.5 loss = 562.5 end balance I am not confident that my maths is correct. I'd appreciate it if someone eyed it over and told me if I'm going horribly wrong.","I'm trying to do the maths on a coin toss game after 100 games but think I am failing. The rules are as follows. we start with 1000 coins we always bet that heads come up minimum bet is 10 coins maximum bet is 80 coins if tails comes up, we lose our bet if heads comes up, we win twice as much as we bet the coin is fair and so the probability of heads is 50% we start with a bet of 10 if we lose we double our bet if we lose the maximum bet of 80 we start at a bet of 10 again if we win we start at a bet of 10 again So in this set up, 50% of the time we would get our 10 coins back plus 10 additional coins. When we lose we need to lose four times in a row which I've figured out will be 6.25% which will cost 150 coins. 50% (10 coins) * 50% (20 coins) * 50% (40 coins) * (80 coins) 50% = 6.25% (150 coins total) Would I be right in thinking that this means that the chance of breaking even is 43.75%? So after 100 games we would have the win and loss as below and end up with the balance after the wins and losses are applied. Win: (50% win chance * 100 games) * 10 coins won = 500 coins won Loss: (6.25% loss chance * 100 games) * 150 coins = 937.5 coins lost Total: 1000 start + 500 win - 937.5 loss = 562.5 end balance I am not confident that my maths is correct. I'd appreciate it if someone eyed it over and told me if I'm going horribly wrong.",,['probability']
92,Alternative strategies for the game of number guessing,Alternative strategies for the game of number guessing,,"I took the following game from the Peter Winkler collection (chapter ""Games""): Two numbers are chosen independently at random from the uniform distribution on [0,1]. Player A then looks at the numbers. She must decide which one of them to show to player B, who upon seeing it, guesses whether it's the larger or smaller of the two. If he guesses right, B wins, otherwise A wins. Payoff to a player is his/her winning probability. To be clear, let me define a strategy for B as a (measurable) function ${f}_{B}:[0,1]\longmapsto \{larger, smaller\}$, i.e., a strategy for B specifies ""larger"" or ""smaller"" for every real in [0,1]. Similarly,  A's strategy is a function ${f}_{A}(\{x,y\})$ $=x$ or $y$, i.e., a strategy for A specifies x or y for every $\{x,y\}$ where $x,y\in [0,1]$. A strategy of A (or B) is called a candidate (for equilibrium), if it prevents the opponent from achieving a winning probability strictly greater than 1/2 no matter what strategy he (or she) adopts. By this definition, the following strategy of A is a candidate (check it yourself): "" ${f}_{A}(\{x,y\})=x$ iff $|x-1/2|<|y-1/2|$"" My question: Are there any other candidates for player A, except those that differ only by a measure zero set from the above? (P.S., One can show that the candidate for player B is unique, except by a difference of measure zero set. Hence if candidate for A is unique, too, there's a unique (pure) equilibrium.) Edit: Here's how I prove that candidate for B is unique. I don't know if a simpler argument can be made, or similar reasoning can be used to prove or disprove the case of A. By definition, B's strategy is to choose measurable $B_L\subseteq[0,1]$ such that he reports larger for $x\in B_L$ and smaller for $x\in [0,1]/B_L$. Now if $m(B_L)=a>1/2$, A can adopt the following strategy: ""Show the smaller number if both $x,y\in B_L$, otherwise show the larger number"". which guarantees her winning probability $\geq a^2+(1-a)^2>1/2$. Hence $m(B_L)>1/2$ can't be a candidate for B. Reversing the strategy shows that $m(B_L)<1/2$ can't be candidate, either. Hence $m(B_L)=1/2$. Now let $m(B_L)=1/2$. Define $B_S=[0,1]/B_L$. Consider the following incomplete specification of a strategy for A: ""Show the smaller number if both $x,y\in B_L$; show the larger if both $x,y\in B_S$"" which guarantees her winning probability $=1$ in those situations. What about the remaining situations, i.e., $x\in B_L$ and $y\in B_S$? For any measurable $B\subseteq B_L$ with $m(B)>0$, we can define $C=\{x\in B_S|x>y, \forall y\in B\} $. Suppose there exists such a $B$ such that $m(C)>0$, then A can adopt the following strategy: ""Show the smaller number if both $x,y\in B_L$, otherwise show the larger number"" which will guarantee her winning probability: P(win)$=$ P(both $x,y\in B_L$ or both $x,y\in B_S$ and win)$+$ P($x\in B_L$ and $y\in B_S$ and win) $\ge$ P(both $x,y\in B_L$ or both $x,y\in B_S$ and win)$+$ P($x\in B$ and $y\in C$ and win) $=1/2+2m(B)m(C)>1/2$ Hence if a strategy of B is to be a candidate, we must have $m(C)=0$. Because $B\subseteq B_L$ was arbitrary, it is true that the set $\{x\in B_S|x>y, \forall y\in B_L\} $ has measure zero. Hence, to conclude, necessary conditions for a strategy of B to be candidate: $m(B_L)=m(B_S)=1/2$. $\{x\in B_S|x>y, \forall y\in B_L\} $ has measure zero. There is only one strategy of B satisfying these conditions (up to measure zero difference), i.e., $B_L=[1/2,1]$. For sufficiency it is easy to check this is indeed a candidate. Hence it is the only candidate for B.","I took the following game from the Peter Winkler collection (chapter ""Games""): Two numbers are chosen independently at random from the uniform distribution on [0,1]. Player A then looks at the numbers. She must decide which one of them to show to player B, who upon seeing it, guesses whether it's the larger or smaller of the two. If he guesses right, B wins, otherwise A wins. Payoff to a player is his/her winning probability. To be clear, let me define a strategy for B as a (measurable) function ${f}_{B}:[0,1]\longmapsto \{larger, smaller\}$, i.e., a strategy for B specifies ""larger"" or ""smaller"" for every real in [0,1]. Similarly,  A's strategy is a function ${f}_{A}(\{x,y\})$ $=x$ or $y$, i.e., a strategy for A specifies x or y for every $\{x,y\}$ where $x,y\in [0,1]$. A strategy of A (or B) is called a candidate (for equilibrium), if it prevents the opponent from achieving a winning probability strictly greater than 1/2 no matter what strategy he (or she) adopts. By this definition, the following strategy of A is a candidate (check it yourself): "" ${f}_{A}(\{x,y\})=x$ iff $|x-1/2|<|y-1/2|$"" My question: Are there any other candidates for player A, except those that differ only by a measure zero set from the above? (P.S., One can show that the candidate for player B is unique, except by a difference of measure zero set. Hence if candidate for A is unique, too, there's a unique (pure) equilibrium.) Edit: Here's how I prove that candidate for B is unique. I don't know if a simpler argument can be made, or similar reasoning can be used to prove or disprove the case of A. By definition, B's strategy is to choose measurable $B_L\subseteq[0,1]$ such that he reports larger for $x\in B_L$ and smaller for $x\in [0,1]/B_L$. Now if $m(B_L)=a>1/2$, A can adopt the following strategy: ""Show the smaller number if both $x,y\in B_L$, otherwise show the larger number"". which guarantees her winning probability $\geq a^2+(1-a)^2>1/2$. Hence $m(B_L)>1/2$ can't be a candidate for B. Reversing the strategy shows that $m(B_L)<1/2$ can't be candidate, either. Hence $m(B_L)=1/2$. Now let $m(B_L)=1/2$. Define $B_S=[0,1]/B_L$. Consider the following incomplete specification of a strategy for A: ""Show the smaller number if both $x,y\in B_L$; show the larger if both $x,y\in B_S$"" which guarantees her winning probability $=1$ in those situations. What about the remaining situations, i.e., $x\in B_L$ and $y\in B_S$? For any measurable $B\subseteq B_L$ with $m(B)>0$, we can define $C=\{x\in B_S|x>y, \forall y\in B\} $. Suppose there exists such a $B$ such that $m(C)>0$, then A can adopt the following strategy: ""Show the smaller number if both $x,y\in B_L$, otherwise show the larger number"" which will guarantee her winning probability: P(win)$=$ P(both $x,y\in B_L$ or both $x,y\in B_S$ and win)$+$ P($x\in B_L$ and $y\in B_S$ and win) $\ge$ P(both $x,y\in B_L$ or both $x,y\in B_S$ and win)$+$ P($x\in B$ and $y\in C$ and win) $=1/2+2m(B)m(C)>1/2$ Hence if a strategy of B is to be a candidate, we must have $m(C)=0$. Because $B\subseteq B_L$ was arbitrary, it is true that the set $\{x\in B_S|x>y, \forall y\in B_L\} $ has measure zero. Hence, to conclude, necessary conditions for a strategy of B to be candidate: $m(B_L)=m(B_S)=1/2$. $\{x\in B_S|x>y, \forall y\in B_L\} $ has measure zero. There is only one strategy of B satisfying these conditions (up to measure zero difference), i.e., $B_L=[1/2,1]$. For sufficiency it is easy to check this is indeed a candidate. Hence it is the only candidate for B.",,"['probability', 'measure-theory', 'game-theory']"
93,Mixing two different biased coins,Mixing two different biased coins,,"My problem is as follows: I have two biased coins with probabilities $p_1$ and $p_2$ of landing heads. I start with coin 1 and toss it until it lands heads. Then I swap to coin 2 and toss until it lands heads. I then repeat the procedure until I have tossed $n$ times. $X_n$ is the random variable which counts the number of heads in the process. My objective is to show that with probability $1-e^{-\Omega(n)}$ $X_n$ will be in the interval $[(1-\varepsilon)\mathbb{E}[X_n], (1+\varepsilon)\mathbb{E}[X_n]]$ for a fixed $\varepsilon>0$. I have observed that if $p_1<p_2$ I know that $np_1\leq\mathbb{E}[X_n]\leq np_2$ so if I can somehow use Chernoff's or Azuma's inequality I don't need to calculate the expectation of $\mathbb{E}[X_n]$. I have tried looking at this problem with Markov chains but I haven't been successful there. I have also observed that for each $n$ I can define new random variables $Y_{k,n}$ where I throw coin 1 $k$ times and coin 2 $n-k$ times and let $Y_{k,n}$ denote the number of heads I got. I then know that for each $n$ there exists a unique $k$ such that $\mathbb{E}[Y_{k,n}]\leq \mathbb{E}[X_n] < \mathbb{E}[Y_{k+1,n}]$. I can't take that approach further though because I can't find suitable bounds for the variance of $X_n$ I'm not asking you to spoil the problem for me. I'm merely asking if someone could push me in the right direction.","My problem is as follows: I have two biased coins with probabilities $p_1$ and $p_2$ of landing heads. I start with coin 1 and toss it until it lands heads. Then I swap to coin 2 and toss until it lands heads. I then repeat the procedure until I have tossed $n$ times. $X_n$ is the random variable which counts the number of heads in the process. My objective is to show that with probability $1-e^{-\Omega(n)}$ $X_n$ will be in the interval $[(1-\varepsilon)\mathbb{E}[X_n], (1+\varepsilon)\mathbb{E}[X_n]]$ for a fixed $\varepsilon>0$. I have observed that if $p_1<p_2$ I know that $np_1\leq\mathbb{E}[X_n]\leq np_2$ so if I can somehow use Chernoff's or Azuma's inequality I don't need to calculate the expectation of $\mathbb{E}[X_n]$. I have tried looking at this problem with Markov chains but I haven't been successful there. I have also observed that for each $n$ I can define new random variables $Y_{k,n}$ where I throw coin 1 $k$ times and coin 2 $n-k$ times and let $Y_{k,n}$ denote the number of heads I got. I then know that for each $n$ there exists a unique $k$ such that $\mathbb{E}[Y_{k,n}]\leq \mathbb{E}[X_n] < \mathbb{E}[Y_{k+1,n}]$. I can't take that approach further though because I can't find suitable bounds for the variance of $X_n$ I'm not asking you to spoil the problem for me. I'm merely asking if someone could push me in the right direction.",,"['probability', 'asymptotics']"
94,Relative entropy between singular measures,Relative entropy between singular measures,,"Usually, to define relative entropy between two probability measures, one assumes absolute continuity. Is it possible to extend the usual definition in the non absolutely continuous case?","Usually, to define relative entropy between two probability measures, one assumes absolute continuity. Is it possible to extend the usual definition in the non absolutely continuous case?",,"['probability', 'entropy', 'singular-measures']"
95,Dixon's Theorem to probabilistically bound largest factor of N,Dixon's Theorem to probabilistically bound largest factor of N,,"I have recently decided to read up on the current integer factorization algorithms. When looking into some of the algorithms, I came across the following statement: Say that p is the smallest prime factor of n. By Dixon's Theorem, the probability that the largest factor of n is less than $(p)^\epsilon$ is roughly $\epsilon ^ {- \epsilon}$. But when I hear Dixon's Theorem, I think of this . I currently don't see how Dixon's Theorem yields such a probabilistic bound. Can you help me with that? And the disclaimer - I am not currently in any class, so this was not assigned to me, it is not a homework problem, and I will in no way be receiving any sort of academic credit for this work.","I have recently decided to read up on the current integer factorization algorithms. When looking into some of the algorithms, I came across the following statement: Say that p is the smallest prime factor of n. By Dixon's Theorem, the probability that the largest factor of n is less than $(p)^\epsilon$ is roughly $\epsilon ^ {- \epsilon}$. But when I hear Dixon's Theorem, I think of this . I currently don't see how Dixon's Theorem yields such a probabilistic bound. Can you help me with that? And the disclaimer - I am not currently in any class, so this was not assigned to me, it is not a homework problem, and I will in no way be receiving any sort of academic credit for this work.",,"['probability', 'number-theory', 'reference-request']"
96,"Buckets of Balls, Will one fill if I add another Ball?","Buckets of Balls, Will one fill if I add another Ball?",,"I was refereed here by stackoverflow.com. With some searching I found this: another balls and bins question , but its not quite what I am looking for. Rather the inverse. IE the expected number of buckets that have H-1 balls in them. I realize the title is a bit odd. But this is a statistics/probability problem that I am trying to figure out, but am stumped. (No no, its not homework, see the bottom for the real explanation) The premise is simple. You have N buckets. Each bucket can hold H balls. None of the buckets is full. You have D balls already in the buckets, but you don't know where the balls are (you forgot!) You choose a bucket at random to add 1 ball. What is the probability that that bucket will then be full. Some example possible diagrams, with N = 4, H = 3, D = 4. Each case is just a hypothetical arrangement of the balls. for one of many cases. Scenario 1: 1 bucket could be filled. |   |   |   |   | + - + - + - + - + | B |   |   |   | + - + - + - + - + | B | B |   | B | + - + - + - + - +  Scenario 2: 2 buckets could be filled. |   |   |   |   | + - + - + - + - + |   | B | B |   | + - + - + - + - + |   | B | B |   | + - + - + - + - +  Scenario 3: 0 buckets could be filled. |   |   |   |   | + - + - + - + - + |   |   |   |   | + - + - + - + - + | B | B | B | B | + - + - + - + - + The problem is I need a general purpose equation in the form of P = f(N, H, D) Alright, you've tuned in this far. The reason behind this query on math, is I'm curious in having large battles between units. Each unit could belong to a brigade that contains many units of the same type. however, the battle will progress slowly over time. At each phase of the battle, the state will be saved to the DB. Instead of saving each unit and each health for each unit, I want to save the number of units and the total damage on the brigade. When damage is added to a brigade, the f(N, H, D) is run and returns a % chance that a unit in the brigade is destroyed (all of its HP are used up). This then removes that unit from the brigade decrementing N by 1 and D by H. Before you get too technical, I need to implement this solution to a program. So Integrals are out of the question for now. I'm stuck with algebra and trig functions. I appreciate the help","I was refereed here by stackoverflow.com. With some searching I found this: another balls and bins question , but its not quite what I am looking for. Rather the inverse. IE the expected number of buckets that have H-1 balls in them. I realize the title is a bit odd. But this is a statistics/probability problem that I am trying to figure out, but am stumped. (No no, its not homework, see the bottom for the real explanation) The premise is simple. You have N buckets. Each bucket can hold H balls. None of the buckets is full. You have D balls already in the buckets, but you don't know where the balls are (you forgot!) You choose a bucket at random to add 1 ball. What is the probability that that bucket will then be full. Some example possible diagrams, with N = 4, H = 3, D = 4. Each case is just a hypothetical arrangement of the balls. for one of many cases. Scenario 1: 1 bucket could be filled. |   |   |   |   | + - + - + - + - + | B |   |   |   | + - + - + - + - + | B | B |   | B | + - + - + - + - +  Scenario 2: 2 buckets could be filled. |   |   |   |   | + - + - + - + - + |   | B | B |   | + - + - + - + - + |   | B | B |   | + - + - + - + - +  Scenario 3: 0 buckets could be filled. |   |   |   |   | + - + - + - + - + |   |   |   |   | + - + - + - + - + | B | B | B | B | + - + - + - + - + The problem is I need a general purpose equation in the form of P = f(N, H, D) Alright, you've tuned in this far. The reason behind this query on math, is I'm curious in having large battles between units. Each unit could belong to a brigade that contains many units of the same type. however, the battle will progress slowly over time. At each phase of the battle, the state will be saved to the DB. Instead of saving each unit and each health for each unit, I want to save the number of units and the total damage on the brigade. When damage is added to a brigade, the f(N, H, D) is run and returns a % chance that a unit in the brigade is destroyed (all of its HP are used up). This then removes that unit from the brigade decrementing N by 1 and D by H. Before you get too technical, I need to implement this solution to a program. So Integrals are out of the question for now. I'm stuck with algebra and trig functions. I appreciate the help",,"['probability', 'statistics', 'algorithms']"
97,How many independent random variables can fit in a probability space with equally likely outcomes?,How many independent random variables can fit in a probability space with equally likely outcomes?,,"Suppose we have a finite probability space $S,|S|=n$ with the probability defined as $P(A)=|A|/n$ . There is a set of random variables $V =\{A_1,A_2,\cdots\}$ defined over $S$ , mutually independent. what is the maximum of $|V|$ ? For example, if $|S|=1$ , then $V$ has only the constant. [EDIT: The constant are independent of each other, so we should omit this trivial case.] If $|S|=2$ , then $V$ contains the constant and the variable where the two values are different. What about the $S$ with more elements? My guess is that it's the number of factors of $n$ .","Suppose we have a finite probability space with the probability defined as . There is a set of random variables defined over , mutually independent. what is the maximum of ? For example, if , then has only the constant. [EDIT: The constant are independent of each other, so we should omit this trivial case.] If , then contains the constant and the variable where the two values are different. What about the with more elements? My guess is that it's the number of factors of .","S,|S|=n P(A)=|A|/n V =\{A_1,A_2,\cdots\} S |V| |S|=1 V |S|=2 V S n","['probability', 'combinatorics', 'probability-theory', 'random-variables']"
98,"If random variables $X$ and $Y$ are equal in distribution, then there exists a measurable function $f$ such that $X(\omega)=Y(f(\omega))$ a.s.","If random variables  and  are equal in distribution, then there exists a measurable function  such that  a.s.",X Y f X(\omega)=Y(f(\omega)),"Let $X$ and $Y$ be two identically distributed random vectors in $\mathbb{R}^{d}$ defined on the same underlying probability space $(\Omega,\mathcal{A},P)$ . Suppose that $P$ is non-atomic. Does there exist a measurable function $f:\Omega\to \Omega$ that is measure-preserving such that $X(\omega) = Y(f(\omega))$ a.s.? Note this is similar to a number of questions that have been asked before (links posted below). However, none of the solutions to these questions have been super clear to me. The general conclusion seems to be ""yes,"" at least if $(\Omega,\mathcal{A},P)$ is a standard probability space. But I am looking for a bit of explanation, and then some solid references that will help me see the result clearly for myself. Let's settle the issue once and for all! Anyone up to the challenge? Some previous posts: Link 1 , Link 2 , Link 3 , Link 4 , Link 5 .","Let and be two identically distributed random vectors in defined on the same underlying probability space . Suppose that is non-atomic. Does there exist a measurable function that is measure-preserving such that a.s.? Note this is similar to a number of questions that have been asked before (links posted below). However, none of the solutions to these questions have been super clear to me. The general conclusion seems to be ""yes,"" at least if is a standard probability space. But I am looking for a bit of explanation, and then some solid references that will help me see the result clearly for myself. Let's settle the issue once and for all! Anyone up to the challenge? Some previous posts: Link 1 , Link 2 , Link 3 , Link 4 , Link 5 .","X Y \mathbb{R}^{d} (\Omega,\mathcal{A},P) P f:\Omega\to \Omega X(\omega) = Y(f(\omega)) (\Omega,\mathcal{A},P)","['probability', 'measure-theory', 'probability-distributions', 'random-variables', 'almost-everywhere']"
99,Why are two disjoint events defined to be independent if one has zero probability?,Why are two disjoint events defined to be independent if one has zero probability?,,"Let $A$ and $B$ be two disjoint events in a probability space and suppose that one of the two events has zero probability. According to the standard definition of independence, this means that $A$ and $B$ are independent. Unfortunately this definition seems very counter-intuitive to me: If both events are non-empty, then I would instead define them to be dependent, as the occurrence of one event excludes the occurrence of the other. Why has the math community accepted a different definition? Just out of convenience? What would be the consequences if we changed the definition?","Let and be two disjoint events in a probability space and suppose that one of the two events has zero probability. According to the standard definition of independence, this means that and are independent. Unfortunately this definition seems very counter-intuitive to me: If both events are non-empty, then I would instead define them to be dependent, as the occurrence of one event excludes the occurrence of the other. Why has the math community accepted a different definition? Just out of convenience? What would be the consequences if we changed the definition?",A B A B,"['probability', 'terminology', 'conditional-probability', 'independence']"
