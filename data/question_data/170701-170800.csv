,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Exponential Distribution - Probabilities,Exponential Distribution - Probabilities,,"I'm trying to figure out the answer to the following question on a past exam given for practice. Since there are no solutions, I was hoping I could get the help needed to figure it out. A BS1300 big-screen TV may die due to screen failure or power supply failure. The time until power supply failure is exponential with parameter 0.00001. The time until screen failure is exponential with parameter 0.00002. The time is in hours. a) Which failure is more probable to occur first? For this problem, I simply got the expected values and whichever was lower, it should be more probable as the time expected for it to fail was lower. $E(X) = 100000$ hours for power supply, $E(X) = 50000$ hours for screen failure. Therefore, screen is more likely to fail first. b) What is the probability that a BS1300 will die within 10000 hours? PS = power supply, SF = screen failure $P(PS = 10000) = 0.000016374 P(SF = 10000) = 0.000025422$. If we add it up, we get PS or SF $= 0.000025422$. c) If the BS1300 is known to be dead, what is the probability that its power supply failed? Part c), I'm not sure how to figure this out. I tried drawing a venn diagram but I'm unsure how to do it for exponential. Help would be appreciated on how to solve this problem.","I'm trying to figure out the answer to the following question on a past exam given for practice. Since there are no solutions, I was hoping I could get the help needed to figure it out. A BS1300 big-screen TV may die due to screen failure or power supply failure. The time until power supply failure is exponential with parameter 0.00001. The time until screen failure is exponential with parameter 0.00002. The time is in hours. a) Which failure is more probable to occur first? For this problem, I simply got the expected values and whichever was lower, it should be more probable as the time expected for it to fail was lower. $E(X) = 100000$ hours for power supply, $E(X) = 50000$ hours for screen failure. Therefore, screen is more likely to fail first. b) What is the probability that a BS1300 will die within 10000 hours? PS = power supply, SF = screen failure $P(PS = 10000) = 0.000016374 P(SF = 10000) = 0.000025422$. If we add it up, we get PS or SF $= 0.000025422$. c) If the BS1300 is known to be dead, what is the probability that its power supply failed? Part c), I'm not sure how to figure this out. I tried drawing a venn diagram but I'm unsure how to do it for exponential. Help would be appreciated on how to solve this problem.",,"['probability', 'statistics']"
1,How the step of squaring the deviations in Standard Deviation overcomes the drawback of ignoring the signs of mean deviation.,How the step of squaring the deviations in Standard Deviation overcomes the drawback of ignoring the signs of mean deviation.,,"$$\text{Mean deviation from mean}=\frac1N\sum_{i=1}^nf_i|x_i-\bar x|$$and $$\text{Standard Deviation ($\sigma$)}=\sqrt{\frac1N\sum_{i=1}^nf_i(x_i-\bar x)^2}$$   The step of squaring the deviations in SD overcomes the drawback of ignoring the signs of mean deviation. How is the problem overcome? In the SD too, we ignore the signs by squaring, aren't we?","$$\text{Mean deviation from mean}=\frac1N\sum_{i=1}^nf_i|x_i-\bar x|$$and $$\text{Standard Deviation ($\sigma$)}=\sqrt{\frac1N\sum_{i=1}^nf_i(x_i-\bar x)^2}$$   The step of squaring the deviations in SD overcomes the drawback of ignoring the signs of mean deviation. How is the problem overcome? In the SD too, we ignore the signs by squaring, aren't we?",,['statistics']
2,Solving a Poisson Distribution,Solving a Poisson Distribution,,"The number of contaminants in samples of one cubic centimeter of polluted water has a Poisson distribution with a mean of 1. a. What is the probability a sample will contain some (one or more) contaminants? b. If four samples are independently selected from this water, find the probability that at least one sample will contain some (one or more) contaminants? c.If 100 samples are selected instead, what is approximately the probability of seeing at least 60 samples with a contaminant? (Use the Normal table provided for your calculations) This is a practice final exam, however our teacher does not respond to emails and we do not have the solutions. For a. I got 1 - the probability of zero samples getting contaminated and I am fairly certain that it is correct, however I am stumped on b. and c. Any help would be greatly appreciated.","The number of contaminants in samples of one cubic centimeter of polluted water has a Poisson distribution with a mean of 1. a. What is the probability a sample will contain some (one or more) contaminants? b. If four samples are independently selected from this water, find the probability that at least one sample will contain some (one or more) contaminants? c.If 100 samples are selected instead, what is approximately the probability of seeing at least 60 samples with a contaminant? (Use the Normal table provided for your calculations) This is a practice final exam, however our teacher does not respond to emails and we do not have the solutions. For a. I got 1 - the probability of zero samples getting contaminated and I am fairly certain that it is correct, however I am stumped on b. and c. Any help would be greatly appreciated.",,"['statistics', 'probability-distributions']"
3,Shifted exponential limiting distribution,Shifted exponential limiting distribution,,"Assume we have random variables $X_1,..,X_n$ which have the pdf $f(\theta):=\exp(-(x-\theta))$ for $x>\theta$ and zero elsewhere. Let $X_{(1)} =\min(X_1,...,X_n)$ then I want to find the pdf of $S_n=X_{(1)}$ it's expected value and variances. I calculated the pdf as $n\exp(n(x-\theta))$ and the expected value as $E(X)=\theta+1/n$ and the variance as $Var(X)=1/n^2$. Then I want to find series $S_n,a_n$ and $b_n$ such that the sequence $$\frac{S_n-a_n}{b_n}$$ have limiting distribution with variance that is not just a number. I found that by the central limit theorem we have that $\frac{\sqrt(n)(S_n-(\theta+1/n))}{1/n}$ converges to n(0,1) but how can I find limiting distribution?","Assume we have random variables $X_1,..,X_n$ which have the pdf $f(\theta):=\exp(-(x-\theta))$ for $x>\theta$ and zero elsewhere. Let $X_{(1)} =\min(X_1,...,X_n)$ then I want to find the pdf of $S_n=X_{(1)}$ it's expected value and variances. I calculated the pdf as $n\exp(n(x-\theta))$ and the expected value as $E(X)=\theta+1/n$ and the variance as $Var(X)=1/n^2$. Then I want to find series $S_n,a_n$ and $b_n$ such that the sequence $$\frac{S_n-a_n}{b_n}$$ have limiting distribution with variance that is not just a number. I found that by the central limit theorem we have that $\frac{\sqrt(n)(S_n-(\theta+1/n))}{1/n}$ converges to n(0,1) but how can I find limiting distribution?",,"['statistics', 'convergence-divergence', 'central-limit-theorem']"
4,Why is it answer Stratified random sampling,Why is it answer Stratified random sampling,,"Forty seven math majors, 22 music majors and 31 history majors are randomly selected from 585 math majors, 279 music majors and 393 history majors at the state university. What sampling technique is used? Answer: stratified Why is it stratified and not cluster ?? I am having a hard time figuring out a difference","Forty seven math majors, 22 music majors and 31 history majors are randomly selected from 585 math majors, 279 music majors and 393 history majors at the state university. What sampling technique is used? Answer: stratified Why is it stratified and not cluster ?? I am having a hard time figuring out a difference",,['statistics']
5,Explaining probability theory versus statistics,Explaining probability theory versus statistics,,"I'm not sure whether this question was asked before, but it's hard to search because of lots and lots non-descriptive titles like ""statistics and probability"". The context : There is an anecdote I know on the basic difference between probability and statistics: In an exam in statistics a student would sit near a desk and a professor would begin as follows. ""Let's assume that this is a fair coin."" said he, and then threw it, and when it fell onto the desk, he covered it with a sheet of paper so that neither of them would know whether it was heads or tails. ""What is the probability that it's heads?"" the professor asked. ""It's $\frac{1}{2}$ ."" answered the student. ""No, it's $1$ or $0$ , but we don't know which."" replied the older man and so the exam ended. The question(s): What are your thoughts about this? Does this catches the essential difference? Do you know any other examples that would distinguish in elementary terms between the two (i.e. the probability theory and statistics)? The target audience is math-related fields. It might be hard to distinguish between opinion and an experience, but I'm looking for various perspectives, so I'm fine with non-concrete answers (e.g. the same story wouldn't work for all the students). Thanks for any hints/advice/help!","I'm not sure whether this question was asked before, but it's hard to search because of lots and lots non-descriptive titles like ""statistics and probability"". The context : There is an anecdote I know on the basic difference between probability and statistics: In an exam in statistics a student would sit near a desk and a professor would begin as follows. ""Let's assume that this is a fair coin."" said he, and then threw it, and when it fell onto the desk, he covered it with a sheet of paper so that neither of them would know whether it was heads or tails. ""What is the probability that it's heads?"" the professor asked. ""It's ."" answered the student. ""No, it's or , but we don't know which."" replied the older man and so the exam ended. The question(s): What are your thoughts about this? Does this catches the essential difference? Do you know any other examples that would distinguish in elementary terms between the two (i.e. the probability theory and statistics)? The target audience is math-related fields. It might be hard to distinguish between opinion and an experience, but I'm looking for various perspectives, so I'm fine with non-concrete answers (e.g. the same story wouldn't work for all the students). Thanks for any hints/advice/help!",\frac{1}{2} 1 0,"['probability', 'statistics', 'soft-question', 'education']"
6,Variance of an unbiased estimator of central moments,Variance of an unbiased estimator of central moments,,"Some days ago I asked this question: Unbiased estimators for the moments of 2 non-independent random variables Now, using the same framework employed for the previous question, I'm facing with the problem of estimating the statistical variance of one of these unbiased estimator. Honestly, in this case I'm not sure about how to proceed. Anyway, I started with the following thoughts: I observe random variables $X$ and $Y$ over time. I collect $N$ couples of realizations $(X_i, Y_i)$ , where $i$ denotes the time. Using the notation used by wolfies in its answer, I estimate $E[(X-\mu_X)(Y-\mu_Y)]$ using this formula: $$ h_{1,1} = \frac{N s_{1,1} - s_{1,0}s_{0,1}}{N(N-1)}$$ where $$s_{1,0} = \sum_{i=1}^{N}X_i,~ s_{0,1} = \sum_{i=1}^{N}Y_i,~ s_{1,1} = \sum_{i=1}^{N}X_iY_i $$ This is the crucial point. I pose: $$Z_i = \frac{NX_iY_i - X_iY_i}{N-1 }$$ In practice, I substitute $s_{1,0}$ with $X_i$ , $s_{0,1}$ with $Y_i$ and $s_{1,1}$ with $X_iY_i$ . In this way, I obtain a new sample dataset, formed by $Z_i$ , and I say that: $$h_{1,1} = \frac{1}{N} \sum_{i=1}^N Z_i$$ and the variance is: $$\frac{1}{N-1} \sum_{i=1}^N (Z_i - h_{1,1})^2$$ Is this correct?","Some days ago I asked this question: Unbiased estimators for the moments of 2 non-independent random variables Now, using the same framework employed for the previous question, I'm facing with the problem of estimating the statistical variance of one of these unbiased estimator. Honestly, in this case I'm not sure about how to proceed. Anyway, I started with the following thoughts: I observe random variables and over time. I collect couples of realizations , where denotes the time. Using the notation used by wolfies in its answer, I estimate using this formula: where This is the crucial point. I pose: In practice, I substitute with , with and with . In this way, I obtain a new sample dataset, formed by , and I say that: and the variance is: Is this correct?","X Y N (X_i, Y_i) i E[(X-\mu_X)(Y-\mu_Y)]  h_{1,1} = \frac{N s_{1,1} - s_{1,0}s_{0,1}}{N(N-1)} s_{1,0} = \sum_{i=1}^{N}X_i,~ s_{0,1} = \sum_{i=1}^{N}Y_i,~ s_{1,1} = \sum_{i=1}^{N}X_iY_i  Z_i = \frac{NX_iY_i - X_iY_i}{N-1 } s_{1,0} X_i s_{0,1} Y_i s_{1,1} X_iY_i Z_i h_{1,1} = \frac{1}{N} \sum_{i=1}^N Z_i \frac{1}{N-1} \sum_{i=1}^N (Z_i - h_{1,1})^2","['probability', 'statistics']"
7,Horse Racing Odds in Statistics,Horse Racing Odds in Statistics,,"I have a math project to do in statistics and probability. I am given a table of data related to horse racing biding which contain Odds of a Favorite Winning, if the favorite won etc. So I have 300 races and the bookies odds of the favorite winning right, they are in form of: 4/1 9/4 7/4 11/4 9/4 So now I have the following question: 1) Calculate the average odds of favorite winning. 2) What is the probability of a favorite winning. I can't seem to understand how to calculate the 1st question. My approach was to add up the Odds like so: (in the given 5 samples) 4/1+ 9/4+ 7/4+ 11/4+ 9/4= ............ 40/17 which I divide both side by 5 (since there are 5 races in this sample), thus the average odds of the given samples is 8/3.4 (rounded up gives 8/3). Is that correct? I was told if you convert the odds to probability and then add up the mum of them and then divide by number of races you will get a skewed result. And correct me if I'm wrong but calculating the odds is for example -> given 11/4, the odds of happening are 11/(4+11) = 0.73 == 73% Now for the 2nd question what I simply did is I took the number of wins and divided that by number of races which is basically relative frequency approach.. P(E) = # of desired outcomes / # of repetitions of the experiment.. which in this case relates to P(E) = # of wins / # of races, isn't that correct?","I have a math project to do in statistics and probability. I am given a table of data related to horse racing biding which contain Odds of a Favorite Winning, if the favorite won etc. So I have 300 races and the bookies odds of the favorite winning right, they are in form of: 4/1 9/4 7/4 11/4 9/4 So now I have the following question: 1) Calculate the average odds of favorite winning. 2) What is the probability of a favorite winning. I can't seem to understand how to calculate the 1st question. My approach was to add up the Odds like so: (in the given 5 samples) 4/1+ 9/4+ 7/4+ 11/4+ 9/4= ............ 40/17 which I divide both side by 5 (since there are 5 races in this sample), thus the average odds of the given samples is 8/3.4 (rounded up gives 8/3). Is that correct? I was told if you convert the odds to probability and then add up the mum of them and then divide by number of races you will get a skewed result. And correct me if I'm wrong but calculating the odds is for example -> given 11/4, the odds of happening are 11/(4+11) = 0.73 == 73% Now for the 2nd question what I simply did is I took the number of wins and divided that by number of races which is basically relative frequency approach.. P(E) = # of desired outcomes / # of repetitions of the experiment.. which in this case relates to P(E) = # of wins / # of races, isn't that correct?",,"['probability', 'statistics']"
8,Finding the Maximum Likelihood Estimation (Self learning),Finding the Maximum Likelihood Estimation (Self learning),,"I'm trying to learn the subject of Maximum Likelihood Estimation by myself. I'm facing one of the first questions in the textbook which is: `The waiting time (in minutes) on a queue to the dentist is the random variable $X$ with the following pdf: $$f(x) = \left\{\begin{matrix} 2\theta xe^{-\theta x^2} & x > 0\\  0 & x \leq 0 \end{matrix}\right.$$ Find a maximum likelihood estimator for $\theta$ based on the waiting times of $n$ people that are waiting for the dentist, $X_1, X_2, ... X_n$. (The formula) Find the maximum likelihood estimation in a model of 3 people that were waiting 20, 50 and 30 minutes. (The exact number). Again, I am not interested in the exact answers but a way of looking and thinking of questions like this. I tried sketching the graph of $f(x)$ as $f(\theta, x)$ but I found it difficult.","I'm trying to learn the subject of Maximum Likelihood Estimation by myself. I'm facing one of the first questions in the textbook which is: `The waiting time (in minutes) on a queue to the dentist is the random variable $X$ with the following pdf: $$f(x) = \left\{\begin{matrix} 2\theta xe^{-\theta x^2} & x > 0\\  0 & x \leq 0 \end{matrix}\right.$$ Find a maximum likelihood estimator for $\theta$ based on the waiting times of $n$ people that are waiting for the dentist, $X_1, X_2, ... X_n$. (The formula) Find the maximum likelihood estimation in a model of 3 people that were waiting 20, 50 and 30 minutes. (The exact number). Again, I am not interested in the exact answers but a way of looking and thinking of questions like this. I tried sketching the graph of $f(x)$ as $f(\theta, x)$ but I found it difficult.",,"['statistics', 'statistical-inference']"
9,Working out percentages from other percentages - no raw data,Working out percentages from other percentages - no raw data,,"I'm having some difficulty trying to figure out how to come up with a way to solve my problem. I have the following information: 12% of men do X activity 26% of women do X activity 28% of 25-34 years olds do X activity I want to find out what percentage of women aged 25-34 do the activity, but I'm not sure of how to do this. Can anyone help?","I'm having some difficulty trying to figure out how to come up with a way to solve my problem. I have the following information: 12% of men do X activity 26% of women do X activity 28% of 25-34 years olds do X activity I want to find out what percentage of women aged 25-34 do the activity, but I'm not sure of how to do this. Can anyone help?",,['statistics']
10,Ranking algorthim for views/likes/dislikes,Ranking algorthim for views/likes/dislikes,,"I have a table of data that records the number of views an item receives by users viewing the content and the number of likes and dislikes that the users gave: ID  Views  Like   Dislikes   Rank 1   1000    100      0 2   1000    100     50 3   500     500      0 4   500     300      0 5   300     300     50 I need to come up with an algorithm that calculates a ranking for each row based upon the number of views, likes and dislikes. The higher the rank, the more important the content is. Items that have higher views and likes but with lower dislikes have a higher rank than those with lower views and lots of dislikes. The problem I have is that some items with lower views but higher likes would actually be considered much higher value than items that have higher views but a lot of dislikes and as such the item with the lower views should be ranked higher. How can I accurately calcuate a ranking that takes these three items into account? I am not looking for some solution that would determine a ranking by human subjection but merely an unbiased approach that simply takes imperical values into account.","I have a table of data that records the number of views an item receives by users viewing the content and the number of likes and dislikes that the users gave: ID  Views  Like   Dislikes   Rank 1   1000    100      0 2   1000    100     50 3   500     500      0 4   500     300      0 5   300     300     50 I need to come up with an algorithm that calculates a ranking for each row based upon the number of views, likes and dislikes. The higher the rank, the more important the content is. Items that have higher views and likes but with lower dislikes have a higher rank than those with lower views and lots of dislikes. The problem I have is that some items with lower views but higher likes would actually be considered much higher value than items that have higher views but a lot of dislikes and as such the item with the lower views should be ranked higher. How can I accurately calcuate a ranking that takes these three items into account? I am not looking for some solution that would determine a ranking by human subjection but merely an unbiased approach that simply takes imperical values into account.",,['statistics']
11,Number of ways to form three distinctive items,Number of ways to form three distinctive items,,"Given a 6 by 5 array, Calculate the number of ways to form a set of three distinct items such that no two of the selected items are in the same row or same column. What I did was $C(30,1) \cdot C(20,1) \cdot C(12,1)$ however this is not the answer. They get 1200. How?","Given a 6 by 5 array, Calculate the number of ways to form a set of three distinct items such that no two of the selected items are in the same row or same column. What I did was $C(30,1) \cdot C(20,1) \cdot C(12,1)$ however this is not the answer. They get 1200. How?",,"['combinatorics', 'statistics', 'probability-theory']"
12,Percentile of CDF Function,Percentile of CDF Function,,I am trying to find the 0.5 (mean) percentile of a CDF function. $$ F(X) = 1 - e^{-(x/3)^2} $$ In my book's example it says $$ m = 3[-ln(1-0.5)]^{1/2} = 3\sqrt{ln2}=2.498$$ I am not sure how to get to the above equation even using the definition of a percentile of $$F(x_p) = p $$,I am trying to find the 0.5 (mean) percentile of a CDF function. $$ F(X) = 1 - e^{-(x/3)^2} $$ In my book's example it says $$ m = 3[-ln(1-0.5)]^{1/2} = 3\sqrt{ln2}=2.498$$ I am not sure how to get to the above equation even using the definition of a percentile of $$F(x_p) = p $$,,['statistics']
13,Tail bound sum of unbounded RVs of given mean and variance?,Tail bound sum of unbounded RVs of given mean and variance?,,"Let $X_1,\dots,X_n$ be independent variables, $X_i$ having mean $\mu_i$ and variance $\sigma_i^2$. Let their sum $S = \sum_{i=1}^n X_i$. Of course, $S$ has mean $\lambda = \sum_{i=1}^n \mu_i$ and variance $\epsilon^2 = \sum_{i=1}^n \sigma_i^2$. I was wondering if there are nice/simple tail bounds for $S$ in this case? (Edit: Another way to view this is asking for a rate of convergence to the Central Limit Theorem.) To make things easier feel free to assume all $\mu_i = \mu (= 0)$ and all $\sigma_i = \sigma$, or even that $X_i$ are identically distributed. We can tail bound $S$ using Chebyshev's inequality: $\Pr[|S-\lambda| \geq k\epsilon] \leq \frac{1}{k^2}$. Can we get a better bound at all for $S$? In particular might it be possible to get a Chernoff-type exponential tail bound? If so, does $n$ need to be large (e.g. exponential in $\sigma_i$)? One approach I thought of is to use Chebyshev on each $X_i$ separately, union bound the chance that any of them fail to satisfy this bound, then (with the remaining probability) they are bounded RVs and Chernoff applies. But it looks to me like this shouldn't work out (we need the bound to be on the order of $\sqrt{n}$ in order for the union bound to work, but then it's too big for the Chernoff bound to work). Another is to find a distribution that has the heaviest possible tails for a given mean and variance, assume each $X_i$ is drawn from it, and just figure out a bound for this distribution. Is there any such distribution? Other thoughts/approaches/references? Thanks!","Let $X_1,\dots,X_n$ be independent variables, $X_i$ having mean $\mu_i$ and variance $\sigma_i^2$. Let their sum $S = \sum_{i=1}^n X_i$. Of course, $S$ has mean $\lambda = \sum_{i=1}^n \mu_i$ and variance $\epsilon^2 = \sum_{i=1}^n \sigma_i^2$. I was wondering if there are nice/simple tail bounds for $S$ in this case? (Edit: Another way to view this is asking for a rate of convergence to the Central Limit Theorem.) To make things easier feel free to assume all $\mu_i = \mu (= 0)$ and all $\sigma_i = \sigma$, or even that $X_i$ are identically distributed. We can tail bound $S$ using Chebyshev's inequality: $\Pr[|S-\lambda| \geq k\epsilon] \leq \frac{1}{k^2}$. Can we get a better bound at all for $S$? In particular might it be possible to get a Chernoff-type exponential tail bound? If so, does $n$ need to be large (e.g. exponential in $\sigma_i$)? One approach I thought of is to use Chebyshev on each $X_i$ separately, union bound the chance that any of them fail to satisfy this bound, then (with the remaining probability) they are bounded RVs and Chernoff applies. But it looks to me like this shouldn't work out (we need the bound to be on the order of $\sqrt{n}$ in order for the union bound to work, but then it's too big for the Chernoff bound to work). Another is to find a distribution that has the heaviest possible tails for a given mean and variance, assume each $X_i$ is drawn from it, and just figure out a bound for this distribution. Is there any such distribution? Other thoughts/approaches/references? Thanks!",,"['probability', 'statistics', 'probability-theory', 'distribution-tails', 'concentration-of-measure']"
14,How to prove that $P[X_1=X^{(i)}]=\frac{1}{n}$?,How to prove that ?,P[X_1=X^{(i)}]=\frac{1}{n},"Let $X_1$, $X_2$ be two samples draw from a continuous distribution, then I think there is no reason to say that $X_1\leq X_2$ or $X_1\geq X_2$, so we may have $$P[X_1\leq X_2]=P[X_1>X_2]=\frac{1}{2}$$  more generally, let $X^{(1)},\ldots,X^{(n)}$ denote the $X_1,\ldots,X_n$ ordered, then $$P[X_1=X^{(i)}]=\frac{1}{n}$$ but I don't know how to prove this rigorously.","Let $X_1$, $X_2$ be two samples draw from a continuous distribution, then I think there is no reason to say that $X_1\leq X_2$ or $X_1\geq X_2$, so we may have $$P[X_1\leq X_2]=P[X_1>X_2]=\frac{1}{2}$$  more generally, let $X^{(1)},\ldots,X^{(n)}$ denote the $X_1,\ldots,X_n$ ordered, then $$P[X_1=X^{(i)}]=\frac{1}{n}$$ but I don't know how to prove this rigorously.",,"['probability', 'statistics', 'statistical-inference']"
15,Mean/ Expected Value of $X^4$,Mean/ Expected Value of,X^4,"$\def\Var{\mathop{\rm Var}}$Can anyone help me prove that Expected Value of $X^4$ is $3\Var(X)^4$, if the Expected Value of $X$ is zero and $\Var(X)$ is the Variance of $X$? Thanks!","$\def\Var{\mathop{\rm Var}}$Can anyone help me prove that Expected Value of $X^4$ is $3\Var(X)^4$, if the Expected Value of $X$ is zero and $\Var(X)$ is the Variance of $X$? Thanks!",,"['probability', 'statistics', 'average']"
16,Can I get weighted sample variance from the individual variances?,Can I get weighted sample variance from the individual variances?,,"Suppose I have daily aggregates: mean ($m_i$), variance ($v_i$) and number of samples ($n_i$); and I want to calculate weekly aggregates from these. Weekly mean, $M$, is simple: its just a weighted mean. $$M = \frac{\sum n_i * m_i} {\sum n_i} $$ How about weekly variance, $V$? Can I just calculate $V$ like this? $$ V = \frac{\sum n_i * v_i} {\sum n_i} $$ There are formulas on wikipedia that let you get the weighted sample variance if you have access to each sample, but I don't have access to all the samples over the whole week (well, I do have access, but a lot of data if I want to do monthly aggregates, for example). If needed, I can get any other daily aggregate values though. Thanks!","Suppose I have daily aggregates: mean ($m_i$), variance ($v_i$) and number of samples ($n_i$); and I want to calculate weekly aggregates from these. Weekly mean, $M$, is simple: its just a weighted mean. $$M = \frac{\sum n_i * m_i} {\sum n_i} $$ How about weekly variance, $V$? Can I just calculate $V$ like this? $$ V = \frac{\sum n_i * v_i} {\sum n_i} $$ There are formulas on wikipedia that let you get the weighted sample variance if you have access to each sample, but I don't have access to all the samples over the whole week (well, I do have access, but a lot of data if I want to do monthly aggregates, for example). If needed, I can get any other daily aggregate values though. Thanks!",,['statistics']
17,help find my error in a statistics transformation computation,help find my error in a statistics transformation computation,,"This is a low-priority question, but it has been bugging me so I thought I'd ask.  In my stats homework I have the following exercise: Exercise. Suppose $X_1$ and $X_2$ are iid observations from the pdf $f(x\mid\alpha)=\alpha x^{\alpha-1}e^{-x^\alpha}$, $x>0$, $\alpha>0$.  Show that $(\log X_1)/(\log X_2)$ is an ancillary statistic. This is easy to do by showing that if $X\sim X_i$ then $(\log X)$ is a scale family and then applying the Theorem which states that if $U$ is a scale family and $U_1,\cdots,U_n$ are iid observations from $U$ then each $U_i/U_j$, $i\neq j$, is ancillary.  However I attempted a different method which led to a curious problem.  Observe. Let $U=\frac{\log X_1}{\log X_2}$ and $V=\log X_2$, then consider the transformation $(X_1,X_2)\mapsto(U,V)$.  Then $x_1=e^{uv}$ and $x_2=e^v$, giving us the Jacobian \begin{equation*}J=\left|\begin{array}{cc}\frac{\partial x_1}{\partial u}&\frac{\partial x_1}{\partial v}\\\\\frac{\partial x_2}{\partial u}&\frac{\partial x_2}{\partial v}\end{array}\right|=e^{(u+1)v}.\end{equation*} Thus by independence of $X_1$ and $X_2$ we have \begin{multline*}f_{U,V}(u,v)=f_{X_1,X_2}(e^{uv},e^v)|J|\\\\=[\alpha (e^{uv})^{\alpha-1}e^{-(e^{uv})^\alpha}][\alpha (e^v)^{\alpha-1}e^{-(e^v)^\alpha}]e^{(u+1)v}\\\\=\alpha^2\exp[{(u+1)v\alpha}-e^{uv\alpha}-e^{v\alpha}].\end{multline*} Hence by changing variables $z=v\alpha$ we get $dv=dz/\alpha$, and then \begin{multline*}f_U(u)=\int_{-\infty}^\infty\alpha^2\exp[{(u+1)v\alpha}-e^{uv\alpha}-e^{v\alpha}]\;dv\\\\=\alpha\int_{-\infty}^\infty\exp[{(u+1)z}-e^{uz}-e^z]\;dz=:\alpha g(u),\end{multline*} where $g(u)$ is independent of $\alpha$.  However this is impossible! This leads to my question:  Where is my error?","This is a low-priority question, but it has been bugging me so I thought I'd ask.  In my stats homework I have the following exercise: Exercise. Suppose $X_1$ and $X_2$ are iid observations from the pdf $f(x\mid\alpha)=\alpha x^{\alpha-1}e^{-x^\alpha}$, $x>0$, $\alpha>0$.  Show that $(\log X_1)/(\log X_2)$ is an ancillary statistic. This is easy to do by showing that if $X\sim X_i$ then $(\log X)$ is a scale family and then applying the Theorem which states that if $U$ is a scale family and $U_1,\cdots,U_n$ are iid observations from $U$ then each $U_i/U_j$, $i\neq j$, is ancillary.  However I attempted a different method which led to a curious problem.  Observe. Let $U=\frac{\log X_1}{\log X_2}$ and $V=\log X_2$, then consider the transformation $(X_1,X_2)\mapsto(U,V)$.  Then $x_1=e^{uv}$ and $x_2=e^v$, giving us the Jacobian \begin{equation*}J=\left|\begin{array}{cc}\frac{\partial x_1}{\partial u}&\frac{\partial x_1}{\partial v}\\\\\frac{\partial x_2}{\partial u}&\frac{\partial x_2}{\partial v}\end{array}\right|=e^{(u+1)v}.\end{equation*} Thus by independence of $X_1$ and $X_2$ we have \begin{multline*}f_{U,V}(u,v)=f_{X_1,X_2}(e^{uv},e^v)|J|\\\\=[\alpha (e^{uv})^{\alpha-1}e^{-(e^{uv})^\alpha}][\alpha (e^v)^{\alpha-1}e^{-(e^v)^\alpha}]e^{(u+1)v}\\\\=\alpha^2\exp[{(u+1)v\alpha}-e^{uv\alpha}-e^{v\alpha}].\end{multline*} Hence by changing variables $z=v\alpha$ we get $dv=dz/\alpha$, and then \begin{multline*}f_U(u)=\int_{-\infty}^\infty\alpha^2\exp[{(u+1)v\alpha}-e^{uv\alpha}-e^{v\alpha}]\;dv\\\\=\alpha\int_{-\infty}^\infty\exp[{(u+1)z}-e^{uz}-e^z]\;dz=:\alpha g(u),\end{multline*} where $g(u)$ is independent of $\alpha$.  However this is impossible! This leads to my question:  Where is my error?",,['statistics']
18,Why can't I make this claim about my confidence interval?,Why can't I make this claim about my confidence interval?,,"Say out of a sample of 200 penguins, I find that 192 of them like chocolate. From this sample data, I create the $95\%$ confidence interval for the proportion of penguins that like chocolate: $$ \left(\frac{192}{200} - 1.96 \times \sqrt{\frac{(\frac{192}{200})(\frac{8}{200})}{200}}, \frac{192}{200} + 1.96 \times \sqrt{\frac{(\frac{192}{200})(\frac{8}{200})}{200}}\right) \\ (0.933, 0.987) $$ Why would it be invalid for me to subsequently say that I am $95\%$ confident that the percentage of penguins that like chocolate is above $93.3\%$ and below $98.7\%$?","Say out of a sample of 200 penguins, I find that 192 of them like chocolate. From this sample data, I create the $95\%$ confidence interval for the proportion of penguins that like chocolate: $$ \left(\frac{192}{200} - 1.96 \times \sqrt{\frac{(\frac{192}{200})(\frac{8}{200})}{200}}, \frac{192}{200} + 1.96 \times \sqrt{\frac{(\frac{192}{200})(\frac{8}{200})}{200}}\right) \\ (0.933, 0.987) $$ Why would it be invalid for me to subsequently say that I am $95\%$ confident that the percentage of penguins that like chocolate is above $93.3\%$ and below $98.7\%$?",,['statistics']
19,"Prove that $ \mathsf{E}[g(X)] = \int_{- \infty}^{\infty} G(t) \varphi(t) \, d{t} $.",Prove that .," \mathsf{E}[g(X)] = \int_{- \infty}^{\infty} G(t) \varphi(t) \, d{t} ","Problem Let $ X $ be a real-valued random variable with characteristic function $ \varphi $. Suppose that $ g: \mathbb{R} \to \mathbb{R} $ satisfies   $$ \forall x \in \mathbb{R}: \quad g(x) = \int_{- \infty}^{\infty} G(t) e^{i t x} \, d{t} $$   for some measurable function $ G: \mathbb{R} \to \mathbb{C} $ such that   $$ \| G \|_{{L^{1}}(\mathbb{R})} := \int_{- \infty}^{\infty} \left| G(t) \right| \, d{t}                                < \infty. $$   Then prove that   $$ \mathsf{E}[g(X)] = \int_{- \infty}^{\infty} G(t) \varphi(t) \, d{t}. $$ My attempt: By the definition of expectation, we have $$   \mathsf{E}[g(X)] = \int_{- \infty}^{\infty}   \left[ x \int_{- \infty}^{\infty} G(t) e^{i t x} \, d{t} \right] \, d{x}. $$ However, I am not sure how to proceed from here. Does it have anything to do with expressing $ e^{i t x} $ as $ \cos(t x) + i \sin(t x) $?","Problem Let $ X $ be a real-valued random variable with characteristic function $ \varphi $. Suppose that $ g: \mathbb{R} \to \mathbb{R} $ satisfies   $$ \forall x \in \mathbb{R}: \quad g(x) = \int_{- \infty}^{\infty} G(t) e^{i t x} \, d{t} $$   for some measurable function $ G: \mathbb{R} \to \mathbb{C} $ such that   $$ \| G \|_{{L^{1}}(\mathbb{R})} := \int_{- \infty}^{\infty} \left| G(t) \right| \, d{t}                                < \infty. $$   Then prove that   $$ \mathsf{E}[g(X)] = \int_{- \infty}^{\infty} G(t) \varphi(t) \, d{t}. $$ My attempt: By the definition of expectation, we have $$   \mathsf{E}[g(X)] = \int_{- \infty}^{\infty}   \left[ x \int_{- \infty}^{\infty} G(t) e^{i t x} \, d{t} \right] \, d{x}. $$ However, I am not sure how to proceed from here. Does it have anything to do with expressing $ e^{i t x} $ as $ \cos(t x) + i \sin(t x) $?",,"['real-analysis', 'probability', 'statistics', 'measure-theory', 'characteristic-functions']"
20,Characteristic function,Characteristic function,,"Question: Let $X_1$ and $X_2$ denote independent real-valued random variables with distribution functions $F_1$, $F_2$, and characteristic functions $\varphi_1$, $\varphi_2$, respectively. Let Y denote a random variable such that $X_1$, $X_2$, and Y are independent and $$ Pr(Y = 0)= 1 - Pr(Y = 1) = \alpha$$ for some 0 < $\alpha$ < 1. Define $$ Z =  \begin{cases}  X_1 & \text{if } Y = 0 \\[.2cm] X_2 & \text{if } Y = 1  \end{cases} $$ Find the characteristic function of $Z$ in terms of $\varphi_1$, $\varphi_2$, and $\alpha$. I am not sure how to proceed with this problem.","Question: Let $X_1$ and $X_2$ denote independent real-valued random variables with distribution functions $F_1$, $F_2$, and characteristic functions $\varphi_1$, $\varphi_2$, respectively. Let Y denote a random variable such that $X_1$, $X_2$, and Y are independent and $$ Pr(Y = 0)= 1 - Pr(Y = 1) = \alpha$$ for some 0 < $\alpha$ < 1. Define $$ Z =  \begin{cases}  X_1 & \text{if } Y = 0 \\[.2cm] X_2 & \text{if } Y = 1  \end{cases} $$ Find the characteristic function of $Z$ in terms of $\varphi_1$, $\varphi_2$, and $\alpha$. I am not sure how to proceed with this problem.",,"['real-analysis', 'statistics', 'measure-theory', 'probability-theory', 'characteristic-functions']"
21,Probability on drawing colored balls,Probability on drawing colored balls,,"Here is another question from the book of V. Rohatgi and A. Saleh. I would like to ask help again. Here it goes: An urn contains $r$ red and $g$ green marbles. A marble is drawn at random and its color noted. Then the marble drawn, together with $c > 0$ marbles of the same color, are returned to the urn. Suppose that $n$ such draws are made from the urn. Find the probability of selecting a red marble at any draw. So here is what I have so far: $$\text{P{a red ball in at least one of the $n$ draws}}=1-\text{P{green balls in all $n$ draws}}$$ I noted that $$\text{P{all green balls}}=\text{P{X$_1$$\;$=$\;$green}}\times{...}\times\text{P{X$_n$$\;$=$\;$green$\;$|$\;$X$_1$$\;$=$\;$${...}$$\;$=$\;$X$_{n-1}$=$\;$green}}$$ Thus, I calculated $\text{P{a red ball in at least one of the $n$ draws}}$ as $$1-\left(\frac{g}{r+g}\right)\left(\frac{g+c}{r+g+c}\right){...}\left(\frac{g+(n-1)c}{r+g+(n-1)c}\right)$$ I do not know if my way is correct or not. However, I feel like this is wrong since the text gave an answer of $$\frac{r}{r+g}$$ If that is the actual answer, can anyone help explain why? Thanks.","Here is another question from the book of V. Rohatgi and A. Saleh. I would like to ask help again. Here it goes: An urn contains $r$ red and $g$ green marbles. A marble is drawn at random and its color noted. Then the marble drawn, together with $c > 0$ marbles of the same color, are returned to the urn. Suppose that $n$ such draws are made from the urn. Find the probability of selecting a red marble at any draw. So here is what I have so far: $$\text{P{a red ball in at least one of the $n$ draws}}=1-\text{P{green balls in all $n$ draws}}$$ I noted that $$\text{P{all green balls}}=\text{P{X$_1$$\;$=$\;$green}}\times{...}\times\text{P{X$_n$$\;$=$\;$green$\;$|$\;$X$_1$$\;$=$\;$${...}$$\;$=$\;$X$_{n-1}$=$\;$green}}$$ Thus, I calculated $\text{P{a red ball in at least one of the $n$ draws}}$ as $$1-\left(\frac{g}{r+g}\right)\left(\frac{g+c}{r+g+c}\right){...}\left(\frac{g+(n-1)c}{r+g+(n-1)c}\right)$$ I do not know if my way is correct or not. However, I feel like this is wrong since the text gave an answer of $$\frac{r}{r+g}$$ If that is the actual answer, can anyone help explain why? Thanks.",,"['probability', 'statistics', 'conditional-probability']"
22,Fisher Information and vector differentiation,Fisher Information and vector differentiation,,"I have $\theta = [a,b]^T$ and the loglikelihood function $l(z|\theta)$ The score $V=\frac{\partial l(z|\theta)}{\partial \vec\theta}$ Then the fisher information matrix $Fi$ is the variance of the score. (Definition) Now there are some things I don't understand (and my problems are probably more fundamental than only fisher information) First of all Why is $Fi = E[V^TV]$ and not just $Fi = E[V^2]$ or $Fi = Var[V]$ Secondly, As far as I know $V=\frac{\partial l(z|\theta)}{\partial \vec\theta} = (\frac{\partial l(z|\theta)}{\partial a} , \frac{\partial l(z|\theta)}{\partial b})^T$ So when I follow the definition of Fisher matrix $Fi = E[V^TV] = E[\begin{bmatrix}   (\frac{\partial l(z|\theta)}{\partial a})^2 & \frac{(\partial l(z|\theta))^2}{\partial a \partial b} \\   \frac{(\partial l(z|\theta))^2}{\partial a \partial b}  & (\frac{\partial l(z|\theta)}{\partial b})^2  \end{bmatrix}]$ Which is wrong because my course notes have the hessian so I should have second derivatives. What did I do wrong when working out $V^TV$ ?","I have $\theta = [a,b]^T$ and the loglikelihood function $l(z|\theta)$ The score $V=\frac{\partial l(z|\theta)}{\partial \vec\theta}$ Then the fisher information matrix $Fi$ is the variance of the score. (Definition) Now there are some things I don't understand (and my problems are probably more fundamental than only fisher information) First of all Why is $Fi = E[V^TV]$ and not just $Fi = E[V^2]$ or $Fi = Var[V]$ Secondly, As far as I know $V=\frac{\partial l(z|\theta)}{\partial \vec\theta} = (\frac{\partial l(z|\theta)}{\partial a} , \frac{\partial l(z|\theta)}{\partial b})^T$ So when I follow the definition of Fisher matrix $Fi = E[V^TV] = E[\begin{bmatrix}   (\frac{\partial l(z|\theta)}{\partial a})^2 & \frac{(\partial l(z|\theta))^2}{\partial a \partial b} \\   \frac{(\partial l(z|\theta))^2}{\partial a \partial b}  & (\frac{\partial l(z|\theta)}{\partial b})^2  \end{bmatrix}]$ Which is wrong because my course notes have the hessian so I should have second derivatives. What did I do wrong when working out $V^TV$ ?",,"['calculus', 'statistics', 'multivariable-calculus']"
23,standard deviation calculation using covariance?,standard deviation calculation using covariance?,,"i require a formula to calculate the standard deviation using variances of three or more variables (lets call them a,b,c) and the covariances between them. To complicate matters more i only need a percentage of all three totalling 100%, so for example a = 50%, b = 40% and c = 10%. Can anybody point me to the right direction as to how i can accomplish this please? thanks in advance","i require a formula to calculate the standard deviation using variances of three or more variables (lets call them a,b,c) and the covariances between them. To complicate matters more i only need a percentage of all three totalling 100%, so for example a = 50%, b = 40% and c = 10%. Can anybody point me to the right direction as to how i can accomplish this please? thanks in advance",,"['matrices', 'statistics', 'standard-deviation']"
24,"Building Bayesian Networks, Causality and Cyclic Reasoning","Building Bayesian Networks, Causality and Cyclic Reasoning",,"I am studying Bayesian Statistics and I am trying to get a good understanding on Bayesian Networks, which seems to be vital in order to make something useful in Machine Learning. Most of the texts I am reading just say simply that: ""Vertices are variables and arcs are independence relations"" and then build a whole bunch of complex stuff like d-separation, inference by message passing algorithms, etc. over this very superficial definition of the fundamental logic behind the Bayesian Networks. I still did not gain an intuition about how the ""independence relations"" are built into a Bayesian Network, to begin with. We have a bunch of variables describing a random process, which is OK. Then we build the structure of the graph, which is where I am getting lost. First of all, when we have the variables $V={X_1,X_2,...,X_n}$, according to the Chain Rule of Probability we have $n!$ alternatives to build the factorization of $P(X_1,X_2,...,X_n)$ like $P(X_5)P(X_8|X_5)P(X_{12}|X_8,X_5),...$ So how can one select an ordering among these $n!$ alternatives? People talk about causality, like choosing the ""causes"" of an effect and making them the parents of the ""effect"" variable. But if we are sure that $X_i$ is the cause of $X_j$, still, for some chain rule orderings we can get a term like $P(X_i|X_j,...)$ which implies us that the ""effect"" $X_j$ acts incorrectly as the ""cause"" $X_i$. So, doesn't this conflict with the ""causality"" principle of a Bayesian Network? An another issue is with the notion of the ""Cyclic Reasoning"". Let's think of a hypothetical machine consisting of two parallel plates which rub together. We think of three variables ""Heat"", ""Plate Area Expansion"" and ""Friction"". Plate Area Expansion is the effect of the cause ""Heat"" and ""Friction"" is just the effect of the plate area expansion, in turn. This means a graph with ""Heat"" -> ""Expansion"" -> ""Friction"". But in reality, ""Friction"" is also a cause of the ""Heat"". We cannot draw an arc from ""Friction"" to ""Heat"" since this violates the Directed Acyclic Graph structure by creating a cycle. So, how does the Bayesian Network handle such kinds of ""circularity""? What if our causality assumptions cannot be represented by a Bayesian Network, like in the above sample with ""Friction"" ?","I am studying Bayesian Statistics and I am trying to get a good understanding on Bayesian Networks, which seems to be vital in order to make something useful in Machine Learning. Most of the texts I am reading just say simply that: ""Vertices are variables and arcs are independence relations"" and then build a whole bunch of complex stuff like d-separation, inference by message passing algorithms, etc. over this very superficial definition of the fundamental logic behind the Bayesian Networks. I still did not gain an intuition about how the ""independence relations"" are built into a Bayesian Network, to begin with. We have a bunch of variables describing a random process, which is OK. Then we build the structure of the graph, which is where I am getting lost. First of all, when we have the variables $V={X_1,X_2,...,X_n}$, according to the Chain Rule of Probability we have $n!$ alternatives to build the factorization of $P(X_1,X_2,...,X_n)$ like $P(X_5)P(X_8|X_5)P(X_{12}|X_8,X_5),...$ So how can one select an ordering among these $n!$ alternatives? People talk about causality, like choosing the ""causes"" of an effect and making them the parents of the ""effect"" variable. But if we are sure that $X_i$ is the cause of $X_j$, still, for some chain rule orderings we can get a term like $P(X_i|X_j,...)$ which implies us that the ""effect"" $X_j$ acts incorrectly as the ""cause"" $X_i$. So, doesn't this conflict with the ""causality"" principle of a Bayesian Network? An another issue is with the notion of the ""Cyclic Reasoning"". Let's think of a hypothetical machine consisting of two parallel plates which rub together. We think of three variables ""Heat"", ""Plate Area Expansion"" and ""Friction"". Plate Area Expansion is the effect of the cause ""Heat"" and ""Friction"" is just the effect of the plate area expansion, in turn. This means a graph with ""Heat"" -> ""Expansion"" -> ""Friction"". But in reality, ""Friction"" is also a cause of the ""Heat"". We cannot draw an arc from ""Friction"" to ""Heat"" since this violates the Directed Acyclic Graph structure by creating a cycle. So, how does the Bayesian Network handle such kinds of ""circularity""? What if our causality assumptions cannot be represented by a Bayesian Network, like in the above sample with ""Friction"" ?",,"['probability', 'statistics', 'graph-theory', 'machine-learning', 'bayesian-network']"
25,Using integrals to prove that the mean of the sampling distribution is the population mean,Using integrals to prove that the mean of the sampling distribution is the population mean,,"Let the random variables $X_1, X_2, \dots X_n$ denote a random sample from a population. The sample mean of these random variables is: $\overline{X}=\frac{1}{n}\sum\limits_{i=1}^{n}X_i$ I would like to show that the mean of the sampling distribution of the sample mean is $\mu$, the population mean. Here's what I have done: $$\begin{align}  E(\overline{X}) &= \int\limits_{\overline{X}} \bar{x} f(\bar{x})\,\, d\bar{x} \\ &=\int\limits_{\overline{X}} \left(\frac{1}{n} \sum\limits_{i=1}^n X_i \right) f(\bar{x}) \, d\bar{x} \end{align}$$ From here, I am not sure what to do anymore but anyway I end up with: $$\begin{array} {cc}  &=& \frac{1}{n} \left( \int\limits_{\overline{X}}X_1f(\bar{x}) \, d\bar{x} + \int\limits_{\overline{X}}X_2f(\bar{x}) \, d\bar{x}+ \dots + \int\limits_{\overline{X}}X_nf(\bar{x}) \, d\bar{x}\right) \end{array}$$ Now, I don't know how to complete this as I am unsure how to interpret the last equation. Somehow, the $\int\limits_{\overline{X}}X_if(\bar{x}) \, d\bar{x}$ is suppose to equal to $\mu$ but I don't see how that can be true. I know the answer will be $\mu$ because of here but I would like to arrive at the answer using integrals instead.","Let the random variables $X_1, X_2, \dots X_n$ denote a random sample from a population. The sample mean of these random variables is: $\overline{X}=\frac{1}{n}\sum\limits_{i=1}^{n}X_i$ I would like to show that the mean of the sampling distribution of the sample mean is $\mu$, the population mean. Here's what I have done: $$\begin{align}  E(\overline{X}) &= \int\limits_{\overline{X}} \bar{x} f(\bar{x})\,\, d\bar{x} \\ &=\int\limits_{\overline{X}} \left(\frac{1}{n} \sum\limits_{i=1}^n X_i \right) f(\bar{x}) \, d\bar{x} \end{align}$$ From here, I am not sure what to do anymore but anyway I end up with: $$\begin{array} {cc}  &=& \frac{1}{n} \left( \int\limits_{\overline{X}}X_1f(\bar{x}) \, d\bar{x} + \int\limits_{\overline{X}}X_2f(\bar{x}) \, d\bar{x}+ \dots + \int\limits_{\overline{X}}X_nf(\bar{x}) \, d\bar{x}\right) \end{array}$$ Now, I don't know how to complete this as I am unsure how to interpret the last equation. Somehow, the $\int\limits_{\overline{X}}X_if(\bar{x}) \, d\bar{x}$ is suppose to equal to $\mu$ but I don't see how that can be true. I know the answer will be $\mu$ because of here but I would like to arrive at the answer using integrals instead.",,"['probability', 'algebra-precalculus', 'statistics', 'proof-writing']"
26,"What does ""$.05$ irrespective of the value of $\hat p$"" mean?","What does "" irrespective of the value of "" mean?",.05 \hat p,"I have a statistics question that says ""What sample size would be required for the width of a $99$% confidence interval to be at most $.05$ irrespective of the value of $\hat p$?"". But I'm not sure what irrespective is refering to. I tried to figure it out in my book but I can't find where exactly it explains it.","I have a statistics question that says ""What sample size would be required for the width of a $99$% confidence interval to be at most $.05$ irrespective of the value of $\hat p$?"". But I'm not sure what irrespective is refering to. I tried to figure it out in my book but I can't find where exactly it explains it.",,['statistics']
27,what is the formal mathematical relationship between the variance and the odds that the gambler who has smaller budget here?,what is the formal mathematical relationship between the variance and the odds that the gambler who has smaller budget here?,,"Here is a part of Bob Anderson's answer in my question ( Does variance do any good to gambling game makers? ) Suppose you had two gamblers who were flipping coins against one another with fair odds for 1 dollars a flip. One of the gamblers has 20 dollars and the other has 100 dollars. If someone goes broke the game is over. what is the formal mathematical formula/distribution that link ""the variance and the odds that the gambler who has smaller budget would win"" together and what is its proof?","Here is a part of Bob Anderson's answer in my question ( Does variance do any good to gambling game makers? ) Suppose you had two gamblers who were flipping coins against one another with fair odds for 1 dollars a flip. One of the gamblers has 20 dollars and the other has 100 dollars. If someone goes broke the game is over. what is the formal mathematical formula/distribution that link ""the variance and the odds that the gambler who has smaller budget would win"" together and what is its proof?",,"['probability', 'statistics', 'probability-distributions', 'actuarial-science', 'gambling']"
28,How $\sum_{r=m}^{\infty}\frac{e^{-\lambda}\lambda^r}{r!}=\int_{0}^{\lambda}\frac{e^{-u}u^{m-1}}{(m-1)!}du$,How,\sum_{r=m}^{\infty}\frac{e^{-\lambda}\lambda^r}{r!}=\int_{0}^{\lambda}\frac{e^{-u}u^{m-1}}{(m-1)!}du,"$$P(X\geq m)=\sum_{r=m}^{\infty}\frac{e^{-\lambda}\lambda^r}{r!};m=0,1,...$$ Show that for any $m=1,2,...$ $$P(X\geq m)=\int_{0}^{\lambda}\frac{e^{-u}u^{m-1}}{(m-1)!}du$$ I couldn't derive it also don't understand why $m$ starts from $1$? why not from $0$ ?","$$P(X\geq m)=\sum_{r=m}^{\infty}\frac{e^{-\lambda}\lambda^r}{r!};m=0,1,...$$ Show that for any $m=1,2,...$ $$P(X\geq m)=\int_{0}^{\lambda}\frac{e^{-u}u^{m-1}}{(m-1)!}du$$ I couldn't derive it also don't understand why $m$ starts from $1$? why not from $0$ ?",,"['probability', 'statistics', 'integration', 'definite-integrals', 'summation']"
29,Why the area under the probability density function curve is probability,Why the area under the probability density function curve is probability,,Why is the area under the probability density function(PDF) curve gives probability?,Why is the area under the probability density function(PDF) curve gives probability?,,['statistics']
30,Find an estimator,Find an estimator,,"I'm self studying statistics, and this is one hw problem that I'm getting stuck at. Suppose in two independent trials with prob p of success, we observe X successes. Find an unbiased estimator T(X) of the function $g(p)=p^2$ Is it simply $(X/2)^2$?","I'm self studying statistics, and this is one hw problem that I'm getting stuck at. Suppose in two independent trials with prob p of success, we observe X successes. Find an unbiased estimator T(X) of the function $g(p)=p^2$ Is it simply $(X/2)^2$?",,['statistics']
31,Probability Question (Possibly Bayes Theorem?),Probability Question (Possibly Bayes Theorem?),,"I have a quick probability question. I've solved half of this problem and I'm not sure if this requires Bayes Theorem. Here's the question: ""Suppose that a polygraph can detect 53% of lies, but incorrectly identifies 25% of true statements as lies. A company gives everyone a polygraph test, asking ""Have you ever stolen anything from your place of work? Naturally, all the applicants answer ""No"", but the company has evidence to suggest that 9% of the applicants are lying. When the polygraph indicates that the applicant is lying, that person is ineligible for a job. Here is a probability tree for these relationships. Fill in the probabilities as given. ------> ( C ) Polygraph says ""Lie"" (  A )  Applicant tells truth -------->                                       ------> ( D ) Polygraph Says ""Truth"" -------> ( E ) Polygraph says ""Lie"" ( B  ) Applicant Lies ---------->                                 -------> ( F ) Polygraph says ""Truth"" (C,D are branches of A) (E,F are branches of B)  So far.. the answers that I got are: A = 0.47 B= 0.53 C = 0.25 D = 0.75 E = Not sure F= Not sure I'm thinking that I need Bayes rule to solve for E and F. Could anyone help me with this, and are my answers for A-D correct? Thanks!","I have a quick probability question. I've solved half of this problem and I'm not sure if this requires Bayes Theorem. Here's the question: ""Suppose that a polygraph can detect 53% of lies, but incorrectly identifies 25% of true statements as lies. A company gives everyone a polygraph test, asking ""Have you ever stolen anything from your place of work? Naturally, all the applicants answer ""No"", but the company has evidence to suggest that 9% of the applicants are lying. When the polygraph indicates that the applicant is lying, that person is ineligible for a job. Here is a probability tree for these relationships. Fill in the probabilities as given. ------> ( C ) Polygraph says ""Lie"" (  A )  Applicant tells truth -------->                                       ------> ( D ) Polygraph Says ""Truth"" -------> ( E ) Polygraph says ""Lie"" ( B  ) Applicant Lies ---------->                                 -------> ( F ) Polygraph says ""Truth"" (C,D are branches of A) (E,F are branches of B)  So far.. the answers that I got are: A = 0.47 B= 0.53 C = 0.25 D = 0.75 E = Not sure F= Not sure I'm thinking that I need Bayes rule to solve for E and F. Could anyone help me with this, and are my answers for A-D correct? Thanks!",,"['probability', 'statistics', 'bayes-theorem']"
32,Estimate the true mean: given only measurements above a known threshold,Estimate the true mean: given only measurements above a known threshold,,"I have a computer-generated logfile which shows events which took longer than a particular time threshold (say 1 second for the sake of argument): if I measure the mean of these measurements, it seems this mean value will be 'off' because of the unreported measurements that didn't hit the threshold value. Is there a standard way of re-adjusting the mean to take into account these missing values that failed to meet the threshold : and what assumptions need to be made in order to carry this out ?","I have a computer-generated logfile which shows events which took longer than a particular time threshold (say 1 second for the sake of argument): if I measure the mean of these measurements, it seems this mean value will be 'off' because of the unreported measurements that didn't hit the threshold value. Is there a standard way of re-adjusting the mean to take into account these missing values that failed to meet the threshold : and what assumptions need to be made in order to carry this out ?",,"['statistics', 'average', 'standard-deviation']"
33,Creating Custom Random Number Generator,Creating Custom Random Number Generator,,"My statistics are rusty, but here's what I'm trying to do.  Creating an application around football and have this distribution around rushing yards per attempt. http://farm3.static.flickr.com/2580/3813734078_7801aab534_o.png I'm just looking at the NFL average in the chart.  Basically, I want to create a random number generator that gives me a number along that distribution.  I'll want to do the same for passing too.   Since this is football, I'd like to generate from roughly -10 to 100. I reverse engineered the plot in Excel, but my memory is absolutely failing on how I can create or fit a known distribution to this plot.  I left my math skills behind in college and have just been doing general programming since.","My statistics are rusty, but here's what I'm trying to do.  Creating an application around football and have this distribution around rushing yards per attempt. http://farm3.static.flickr.com/2580/3813734078_7801aab534_o.png I'm just looking at the NFL average in the chart.  Basically, I want to create a random number generator that gives me a number along that distribution.  I'll want to do the same for passing too.   Since this is football, I'd like to generate from roughly -10 to 100. I reverse engineered the plot in Excel, but my memory is absolutely failing on how I can create or fit a known distribution to this plot.  I left my math skills behind in college and have just been doing general programming since.",,"['probability', 'statistics', 'random']"
34,Fisher's information for two independent random variables,Fisher's information for two independent random variables,,"If $X$ and $Y$ are two independent random variables, with regular distributions, how can I prove $I_{x,y}(\theta) = I_x(\theta) + I_y(\theta)$       ? Thanks! I tried: $$ {\rm E}_\theta \left[\left( \frac {\partial}{\partial\theta} \log f_\theta^{xy} (x,y)\right)^2 \right] ={\rm E}_\theta \left[\left( \frac {\partial}{\partial\theta} \log f_\theta^{x} (x)\right)^2 \right]  + {\rm E}_\theta \left[\left( \frac {\partial}{\partial\theta} \log f_\theta^{y} (y)\right)^2 \right]  + {\rm E}_\theta \left[\left( \frac {\partial}{\partial\theta} \log f_\theta^{y} (y)\right) \left( \frac {\partial}{\partial\theta} \log f_\theta^{x} (x)\right) \right]  $$ and the last element should be equal to zero.","If $X$ and $Y$ are two independent random variables, with regular distributions, how can I prove $I_{x,y}(\theta) = I_x(\theta) + I_y(\theta)$       ? Thanks! I tried: $$ {\rm E}_\theta \left[\left( \frac {\partial}{\partial\theta} \log f_\theta^{xy} (x,y)\right)^2 \right] ={\rm E}_\theta \left[\left( \frac {\partial}{\partial\theta} \log f_\theta^{x} (x)\right)^2 \right]  + {\rm E}_\theta \left[\left( \frac {\partial}{\partial\theta} \log f_\theta^{y} (y)\right)^2 \right]  + {\rm E}_\theta \left[\left( \frac {\partial}{\partial\theta} \log f_\theta^{y} (y)\right) \left( \frac {\partial}{\partial\theta} \log f_\theta^{x} (x)\right) \right]  $$ and the last element should be equal to zero.",,"['statistics', 'random-variables', 'statistical-inference']"
35,Unbiased estimate of cross-product for unbiased vector,Unbiased estimate of cross-product for unbiased vector,,"Let $g$ be an unbiased estimate of a vector $G$.  Can $g$ be used to find an unbiased estimate of the cross product $GG'$? I'm stuck because naively using $gg'$ is a biased estimator, with the (upwards) bias depending on the noise in $g$.","Let $g$ be an unbiased estimate of a vector $G$.  Can $g$ be used to find an unbiased estimate of the cross product $GG'$? I'm stuck because naively using $gg'$ is a biased estimator, with the (upwards) bias depending on the noise in $g$.",,"['matrices', 'statistics', 'cross-product']"
36,Does convergence in probability not imply convergence in distribution for Least Squares estimators?,Does convergence in probability not imply convergence in distribution for Least Squares estimators?,,"I have a question relating to convergence in probability and distribution for least squares estimators. Frequently, I see in textbooks that $\hat{\beta} \rightarrow^p b$. Where $b$ is the population parameter, and $\hat{\beta}$ is the Least Squares estimator of that parameter; demonstrating that LS estimators are consistent. I also often see that, $\hat{\beta} \rightarrow^d N(b,\frac{1}{N}(X'X)^{-1})$, showing that $\hat{\beta}$ tends in distribution to a Normal centered around $b$. I just wanted to check that my thinking here is correct. Convergence in probability always implies convergence in distribution as far as I understand it. Hence convergence in probability of LS estimators to a constant $b$ should imply convergence in distribution to the same constant $b$. Hence we should have that $\hat{\beta} \rightarrow^d b$. Is this satisfied above with $N(b,\frac{1}{N}(X'X)^{-1})$ since the variance tends to zero (since $N\rightarrow \infty$), meaning the distribution is itself that of a constant? Or is there some other reason that means that LS estimators converge in distribution to a constant? Many thanks, Ben","I have a question relating to convergence in probability and distribution for least squares estimators. Frequently, I see in textbooks that $\hat{\beta} \rightarrow^p b$. Where $b$ is the population parameter, and $\hat{\beta}$ is the Least Squares estimator of that parameter; demonstrating that LS estimators are consistent. I also often see that, $\hat{\beta} \rightarrow^d N(b,\frac{1}{N}(X'X)^{-1})$, showing that $\hat{\beta}$ tends in distribution to a Normal centered around $b$. I just wanted to check that my thinking here is correct. Convergence in probability always implies convergence in distribution as far as I understand it. Hence convergence in probability of LS estimators to a constant $b$ should imply convergence in distribution to the same constant $b$. Hence we should have that $\hat{\beta} \rightarrow^d b$. Is this satisfied above with $N(b,\frac{1}{N}(X'X)^{-1})$ since the variance tends to zero (since $N\rightarrow \infty$), meaning the distribution is itself that of a constant? Or is there some other reason that means that LS estimators converge in distribution to a constant? Many thanks, Ben",,"['statistics', 'convergence-divergence', 'statistical-inference', 'parameter-estimation']"
37,combining conditional probabilities,combining conditional probabilities,,"I've come across a physics paper in which pdf $$ p(a|b) $$ is desired, but only $$ p(a|c)\\ p(c|b) $$ are known. It is claimed that $$ p(a|b)=\int p(a|c)p(c|b) dc. $$ Is this correct wlog? I can't prove it. I think it is in fact a conditional expectation for $p(a|c)$. Am I right? Is it possible to find $p(a|b)$ exactly?","I've come across a physics paper in which pdf $$ p(a|b) $$ is desired, but only $$ p(a|c)\\ p(c|b) $$ are known. It is claimed that $$ p(a|b)=\int p(a|c)p(c|b) dc. $$ Is this correct wlog? I can't prove it. I think it is in fact a conditional expectation for $p(a|c)$. Am I right? Is it possible to find $p(a|b)$ exactly?",,"['probability', 'statistics', 'bayesian', 'bayes-theorem']"
38,expected value in Poisson distribution [duplicate],expected value in Poisson distribution [duplicate],,"This question already has answers here : Poisson random variable with parameter $\lambda>0$ (2 answers) Closed 10 years ago . E(X) in Poisson Dist, \begin{align} \mathrm{E}(X) &= \sum_{k=0}^{\infty} \frac{k \lambda^k e^{-\lambda}}{k!}= \\ &= e^{-\lambda} \sum_{k=0}^{\infty} \frac{\lambda^{k+1}}{k!}=\\ &= \lambda \end{align} then, $\text{E}(X(X-1)) = \sum_{k=0}^{\infty} \frac{k(k-1) \lambda^{k(k-1)} e^{-\lambda}}{k(k-1)!}$ how can I simplify this ?","This question already has answers here : Poisson random variable with parameter $\lambda>0$ (2 answers) Closed 10 years ago . E(X) in Poisson Dist, \begin{align} \mathrm{E}(X) &= \sum_{k=0}^{\infty} \frac{k \lambda^k e^{-\lambda}}{k!}= \\ &= e^{-\lambda} \sum_{k=0}^{\infty} \frac{\lambda^{k+1}}{k!}=\\ &= \lambda \end{align} then, $\text{E}(X(X-1)) = \sum_{k=0}^{\infty} \frac{k(k-1) \lambda^{k(k-1)} e^{-\lambda}}{k(k-1)!}$ how can I simplify this ?",,['statistics']
39,Probability and Statistics 2,Probability and Statistics 2,,"John invites 12 friends to a dinner party, half of which are men. Exactly one man and one woman are bringing desserts. If one person from this group is selected at random, what is the probability that it is a woman, OR a man who is not bringing a dessert? The above question is a GRE practice question and the answer is 11/12.","John invites 12 friends to a dinner party, half of which are men. Exactly one man and one woman are bringing desserts. If one person from this group is selected at random, what is the probability that it is a woman, OR a man who is not bringing a dessert? The above question is a GRE practice question and the answer is 11/12.",,['probability']
40,Fitting Poisson Distribution to data and calculating probabilities,Fitting Poisson Distribution to data and calculating probabilities,,"Michael, the produce manager at the local Albertson's supermarket store, would like you to estimate the probability that a customer will purchase 5 or more bunches of bananas.  He has collected the following data: $$ \begin{array}{c|c} \text{N=Number of bunches of bananas} & 0 & 1 & 2 & 3 & 4 & \text{5 or more}\\ \hline \text{Frequency} & 49 & 35 & 12 & 3 & 1 & 0\\ \\ \\ \end{array} $$ (a) Calculate the mean and the variance for the aforementioned data. (b)  Which distribution (Poisson, binomial, or negative binomial) would you use to model the data?  Which parameter(s) would you choose? (c)  If 1000 customers visit the store, estimate the probability that someone will purchase 5 or more bunches of bananas. To start, I first calculate the mean and variance: (a)  $E[N] = \frac{1}{100} (1 * 35 + \cdots+4*1) = 0.72$ $Var[N]=E[N^2]-E[N] = 1.26-0.72^2=0.7416$ (b)  Since $E[N] \approx Var[N]$, this can be modeled as a Poisson distribution because  in this distrubtion $E[N]=Var[N]=\lambda$, and here we can assign $\lambda=0.72$. $ \therefore \Pr(N=k)= \cfrac{\lambda^k}{k!} e^{-\lambda} \text{ for } k \ge 0$ (c)  This part I'm a little confused.  The probability of purchasing 5 or more bananas can be written as $\Pr[N \ge 5] = \sum\limits_{k=5}^{\infty} \cfrac{\lambda^k}{k!} e^{-\lambda} = 1-\Pr[N<5]  = 1-\sum\limits_{k=0}^{4} \cfrac{\lambda^k}{k!} e^{-\lambda} =1 - 0.999110 =0.000889$ So is it correct to say if there is 1 person, the probability that he will choose 5 or more bunches of bananas is 0.00089??  Hence, if we have 1000 people the probability they will choose 5 or more is 0.00089^1000.  But this is wrong.  Can someone please explain?  How do I apply this probability to 1000 people?  This is where I am stuck.  Thanks in advance. Oh by the way, does anyone know how to fix my tex code for the horizontal table?  I totally winged it and don't know the proper way.  Thanks again.","Michael, the produce manager at the local Albertson's supermarket store, would like you to estimate the probability that a customer will purchase 5 or more bunches of bananas.  He has collected the following data: $$ \begin{array}{c|c} \text{N=Number of bunches of bananas} & 0 & 1 & 2 & 3 & 4 & \text{5 or more}\\ \hline \text{Frequency} & 49 & 35 & 12 & 3 & 1 & 0\\ \\ \\ \end{array} $$ (a) Calculate the mean and the variance for the aforementioned data. (b)  Which distribution (Poisson, binomial, or negative binomial) would you use to model the data?  Which parameter(s) would you choose? (c)  If 1000 customers visit the store, estimate the probability that someone will purchase 5 or more bunches of bananas. To start, I first calculate the mean and variance: (a)  $E[N] = \frac{1}{100} (1 * 35 + \cdots+4*1) = 0.72$ $Var[N]=E[N^2]-E[N] = 1.26-0.72^2=0.7416$ (b)  Since $E[N] \approx Var[N]$, this can be modeled as a Poisson distribution because  in this distrubtion $E[N]=Var[N]=\lambda$, and here we can assign $\lambda=0.72$. $ \therefore \Pr(N=k)= \cfrac{\lambda^k}{k!} e^{-\lambda} \text{ for } k \ge 0$ (c)  This part I'm a little confused.  The probability of purchasing 5 or more bananas can be written as $\Pr[N \ge 5] = \sum\limits_{k=5}^{\infty} \cfrac{\lambda^k}{k!} e^{-\lambda} = 1-\Pr[N<5]  = 1-\sum\limits_{k=0}^{4} \cfrac{\lambda^k}{k!} e^{-\lambda} =1 - 0.999110 =0.000889$ So is it correct to say if there is 1 person, the probability that he will choose 5 or more bunches of bananas is 0.00089??  Hence, if we have 1000 people the probability they will choose 5 or more is 0.00089^1000.  But this is wrong.  Can someone please explain?  How do I apply this probability to 1000 people?  This is where I am stuck.  Thanks in advance. Oh by the way, does anyone know how to fix my tex code for the horizontal table?  I totally winged it and don't know the proper way.  Thanks again.",,"['probability', 'statistics']"
41,"Confidence interval for Uniform($\theta$, $\theta + a$)","Confidence interval for Uniform(, )",\theta \theta + a,"I am encountering a difficulty with the following task.  Have I made a mistake, or is this an inherent flaw in the notion of confidence intervals?  (Other such flaws exist.) Consider a random sample $X_1,\ldots , X_n$ from a Uniform($\theta$, $\theta + a$) distribution, where $\theta$ is unknown and $a$ is known.  We wish to determine a confidence interval for $\theta$. The reader may verify the following details: The statistics $Y=\text{min}_i X_i$ and $Z=\text{max}_i X_i$ are jointly sufficient for $\theta$.  For  $\theta \le c_1 \le c_2 \le \theta + a$, $P\{c_1 \le Y \le Z \le c_2\} = [(c_2 - c_1)/a]^n$.  For $0 < \gamma < 1$, set $d_1 =(1-\sqrt[n]\gamma)/2$ and $d_2 =(1+\sqrt[n]\gamma)/2$.  Then $\gamma = P\{\theta + ad_1 \le Y \le Z \le \theta + ad_2\} =  P\{Z -ad_2 \le \theta \le Y-ad_1\}$.  Thus, $[Z -ad_2, Y -ad_1]$ is a $\gamma$ confidence interval for $\theta$. Now here's the difficulty:  If we observe $Z - Y > a\sqrt[n]\gamma$, then $Z -ad_2 > Y -ad_1$, so our formula yields a nonsensical answer.  Have I made an error in my calculations?  Or is this one of those problems with confidence intervals? (Homework?  I guess so - but a homework problem that I wrote for my students.  Inspired by another problem in DeGroot & Schervish.)","I am encountering a difficulty with the following task.  Have I made a mistake, or is this an inherent flaw in the notion of confidence intervals?  (Other such flaws exist.) Consider a random sample $X_1,\ldots , X_n$ from a Uniform($\theta$, $\theta + a$) distribution, where $\theta$ is unknown and $a$ is known.  We wish to determine a confidence interval for $\theta$. The reader may verify the following details: The statistics $Y=\text{min}_i X_i$ and $Z=\text{max}_i X_i$ are jointly sufficient for $\theta$.  For  $\theta \le c_1 \le c_2 \le \theta + a$, $P\{c_1 \le Y \le Z \le c_2\} = [(c_2 - c_1)/a]^n$.  For $0 < \gamma < 1$, set $d_1 =(1-\sqrt[n]\gamma)/2$ and $d_2 =(1+\sqrt[n]\gamma)/2$.  Then $\gamma = P\{\theta + ad_1 \le Y \le Z \le \theta + ad_2\} =  P\{Z -ad_2 \le \theta \le Y-ad_1\}$.  Thus, $[Z -ad_2, Y -ad_1]$ is a $\gamma$ confidence interval for $\theta$. Now here's the difficulty:  If we observe $Z - Y > a\sqrt[n]\gamma$, then $Z -ad_2 > Y -ad_1$, so our formula yields a nonsensical answer.  Have I made an error in my calculations?  Or is this one of those problems with confidence intervals? (Homework?  I guess so - but a homework problem that I wrote for my students.  Inspired by another problem in DeGroot & Schervish.)",,['statistics']
42,Distance between the product of marginal distributions and the joint distribution,Distance between the product of marginal distributions and the joint distribution,,"Given a joint distribution $P(A,B,C)$, we can compute various marginal distributions. Now suppose: \begin{align} P1(A,B,C) &= P(A) P(B) P(C)  \\ P2(A,B,C) &= P(A,B) P(C)  \\ P3(A,B,C) &= P(A,B,C) \end{align} Is it true that $d(P1,P3) \geq d(P2,P3)$ where d is the total variation distance? In other words, is it provable that $P(A,B) P(C)$ is a better approximation of $P(A,B,C)$ than $P(A) P(B) P(C)$ in terms of the total variation distance? Intuitively I think it's true but could not find out a proof.","Given a joint distribution $P(A,B,C)$, we can compute various marginal distributions. Now suppose: \begin{align} P1(A,B,C) &= P(A) P(B) P(C)  \\ P2(A,B,C) &= P(A,B) P(C)  \\ P3(A,B,C) &= P(A,B,C) \end{align} Is it true that $d(P1,P3) \geq d(P2,P3)$ where d is the total variation distance? In other words, is it provable that $P(A,B) P(C)$ is a better approximation of $P(A,B,C)$ than $P(A) P(B) P(C)$ in terms of the total variation distance? Intuitively I think it's true but could not find out a proof.",,"['probability', 'statistics', 'inequality', 'probability-distributions']"
43,Notation regarding a value corresponding to a particular rank,Notation regarding a value corresponding to a particular rank,,"I have a hard time trying to come up with a formal way of writing the following problem. Basically I would like to find a value that corresponds to a certain rank (this certain rank might not be an integer). Here's an example: I have a vector of numerical values, $x$. e.g., $x=(1,-5,100,0,1000)$ I can rank the absolute values of $x$. e.g., $rank(|x|)=(2,3,4,1,5)$ Suppose I am given a threshold value $t$ for ranks. e.g., $t=2.2$ I would like to know a value $x_{t}$ which corresponds to $floor(t)$. e.g., $x_{t}=1$ It was easy to write in a code: $x_t = x[rank(abs(x)) == floor(t)]$. But I'm unsure how to write it in more formal notation. I hope it makes sense and let me know if I need to clarify anything. PS: I'm also unsure what tag this should have. Sorry about that!","I have a hard time trying to come up with a formal way of writing the following problem. Basically I would like to find a value that corresponds to a certain rank (this certain rank might not be an integer). Here's an example: I have a vector of numerical values, $x$. e.g., $x=(1,-5,100,0,1000)$ I can rank the absolute values of $x$. e.g., $rank(|x|)=(2,3,4,1,5)$ Suppose I am given a threshold value $t$ for ranks. e.g., $t=2.2$ I would like to know a value $x_{t}$ which corresponds to $floor(t)$. e.g., $x_{t}=1$ It was easy to write in a code: $x_t = x[rank(abs(x)) == floor(t)]$. But I'm unsure how to write it in more formal notation. I hope it makes sense and let me know if I need to clarify anything. PS: I'm also unsure what tag this should have. Sorry about that!",,['statistics']
44,Posterior Distribution of a Prior Variable,Posterior Distribution of a Prior Variable,,"Let $X_{1},\dots,X_{n}$ be a random sample from an exponential distribution with density $f(x;\theta)=\theta e^{-\theta x}$, $x>0$ (having mean $1/\theta$). Assume a prior density for $\theta$ which is also exponential with mean $1/\beta$, where $\beta$ is known. Prove that the posterior distribution of $\beta$ is a gamma distribution. --In general, the posterior distribution would be $\pi(\beta\mid \mathbf{x})=f(\mathbf{x}\mid \beta)\pi(\beta)/m(\mathbf{x})$, where $m(\mathbf{x})$ is the marginal probability of $x$. The part that I am stuck at has to do with being given $f(\mathbf{x}\mid\theta)$, not $f(\mathbf{x}\mid\beta)$. Any help is appreciated!","Let $X_{1},\dots,X_{n}$ be a random sample from an exponential distribution with density $f(x;\theta)=\theta e^{-\theta x}$, $x>0$ (having mean $1/\theta$). Assume a prior density for $\theta$ which is also exponential with mean $1/\beta$, where $\beta$ is known. Prove that the posterior distribution of $\beta$ is a gamma distribution. --In general, the posterior distribution would be $\pi(\beta\mid \mathbf{x})=f(\mathbf{x}\mid \beta)\pi(\beta)/m(\mathbf{x})$, where $m(\mathbf{x})$ is the marginal probability of $x$. The part that I am stuck at has to do with being given $f(\mathbf{x}\mid\theta)$, not $f(\mathbf{x}\mid\beta)$. Any help is appreciated!",,"['statistics', 'bayesian']"
45,"Given that the first child draws $10\$$ from his envelope, what is the probability that the second child has an envelope that contains a 20$ note?",Given that the first child draws  note?,"10\  from his envelope, what is the probability that the second child has an envelope that contains a 20","Three children each receive an envelope from their grandparents. It is known that each envelope contains three banknotes and that in the three envelopes together there are two $5\$$ notes, four $10\$$ notes and three $20\$$ notes. The first child opens his envelope and the first note that he draws from it is a $10\$$ bill. What is now the probability that the second child has a $20$$ note in his envelope? I have difficulty thinking about this question as it seems very complex. My intuition tells me that probably conditional probabilities are involved and maybe even Bayes' rule. However, besides that, I have no clue where to start. Could anyone please help?","Three children each receive an envelope from their grandparents. It is known that each envelope contains three banknotes and that in the three envelopes together there are two $5\$$ notes, four $10\$$ notes and three $20\$$ notes. The first child opens his envelope and the first note that he draws from it is a $10\$$ bill. What is now the probability that the second child has a $20$$ note in his envelope? I have difficulty thinking about this question as it seems very complex. My intuition tells me that probably conditional probabilities are involved and maybe even Bayes' rule. However, besides that, I have no clue where to start. Could anyone please help?",,"['probability', 'statistics']"
46,Confidence Interval for a Binomial,Confidence Interval for a Binomial,,"Having trouble with this question from my textbook. I was wondering if anyone could help me out. The following set of $10$ data points are independent realizations from a Binomial model $X$ ~ $\mathrm{Bin}(36,\pi)$ $$10,12, 7, 6, 6,11, 7,12, 9,10.$$ Compute numerically, showing all your work, the $95$% confidence interval for $\pi$. I know that the confidence interval for a binomial looks like this: $$p\pm z_{1-\frac{\alpha}{2}}\sqrt{\frac{p(1-p)}{n}}$$ But I don't know where to go from there.","Having trouble with this question from my textbook. I was wondering if anyone could help me out. The following set of $10$ data points are independent realizations from a Binomial model $X$ ~ $\mathrm{Bin}(36,\pi)$ $$10,12, 7, 6, 6,11, 7,12, 9,10.$$ Compute numerically, showing all your work, the $95$% confidence interval for $\pi$. I know that the confidence interval for a binomial looks like this: $$p\pm z_{1-\frac{\alpha}{2}}\sqrt{\frac{p(1-p)}{n}}$$ But I don't know where to go from there.",,"['probability', 'statistics']"
47,An Exercise of noncentral $\chi^2$ distribution.,An Exercise of noncentral  distribution.,\chi^2,"Let $Y_1,\ldots,Y_n$ be independent random variables with $Y_k$ distributed as $N\sim(a_k,\sigma^2)$, and $\bar Y=\sum_{k=1}^{n}\frac{Y_k}{n}$ denote the sample mean, $S^2$ denotes the sample variance. Then show that  $$\frac{(n-1)S^2}{\sigma^2}=\frac{1}{\sigma^2}\sum_{k=1}^{n}(Y_k-\bar Y)^2$$ is distributed as noncentral $\chi^2$ with $(n-1)$ degrees of freedom and noncentrality parameter $\lambda=\sum_{k=1}^n\frac{(a_k-\bar a)^2}{\sigma^2}$, where $\bar a=\sum_{k=1}^{n}\frac{a_k}{n}$. I have only the idea of Moment Generating Function(MGF) Technique to prove this type of proof based on transformation. But this is almost impossible to me use the MGF technique for the given exercise. How can i prove it easily?","Let $Y_1,\ldots,Y_n$ be independent random variables with $Y_k$ distributed as $N\sim(a_k,\sigma^2)$, and $\bar Y=\sum_{k=1}^{n}\frac{Y_k}{n}$ denote the sample mean, $S^2$ denotes the sample variance. Then show that  $$\frac{(n-1)S^2}{\sigma^2}=\frac{1}{\sigma^2}\sum_{k=1}^{n}(Y_k-\bar Y)^2$$ is distributed as noncentral $\chi^2$ with $(n-1)$ degrees of freedom and noncentrality parameter $\lambda=\sum_{k=1}^n\frac{(a_k-\bar a)^2}{\sigma^2}$, where $\bar a=\sum_{k=1}^{n}\frac{a_k}{n}$. I have only the idea of Moment Generating Function(MGF) Technique to prove this type of proof based on transformation. But this is almost impossible to me use the MGF technique for the given exercise. How can i prove it easily?",,"['probability', 'statistics', 'probability-distributions', 'statistical-inference', 'sampling']"
48,"How the generating function $P(s)=\mathbb E[s^X]$ uniquely determines probabilities $p_n$, $n=1,2,\ldots$","How the generating function  uniquely determines probabilities ,","P(s)=\mathbb E[s^X] p_n n=1,2,\ldots","for determining the probabilities, it has been written on the book that: $$p_n=\frac{\frac{d^n}{ds^n}P(s)|_{s=0}}{n!};\ldots(A)$$ But if i set $s=0$ then $p_n$ becomes $0$. $$p_1=\frac{\frac{d}{ds}P(s)|_{s=0}}{n!}$$  $$\Rightarrow p_1=\frac{\frac{d}{ds}\mathbb E[s^X]|_{s=0}}{n!}$$ $$\Rightarrow p_1=\frac{\mathbb E[Xs^{(X-1)}]|_{s=0}}{n!}$$ $$\Rightarrow p_1=\frac{\mathbb E[X0^{(X-1)}]}{n!}$$ $$\Rightarrow p_1=\frac{\mathbb E[0]}{n!}$$ $$\Rightarrow p_1=\frac{0}{n!}=0$$ Can you please prove the relation in equation (A) and check it for Binomial Distribution where the generating function of the Binomial Distribution is $P(s)=(1-p+ps)^n$,  where $p$ denotes probability of success.","for determining the probabilities, it has been written on the book that: $$p_n=\frac{\frac{d^n}{ds^n}P(s)|_{s=0}}{n!};\ldots(A)$$ But if i set $s=0$ then $p_n$ becomes $0$. $$p_1=\frac{\frac{d}{ds}P(s)|_{s=0}}{n!}$$  $$\Rightarrow p_1=\frac{\frac{d}{ds}\mathbb E[s^X]|_{s=0}}{n!}$$ $$\Rightarrow p_1=\frac{\mathbb E[Xs^{(X-1)}]|_{s=0}}{n!}$$ $$\Rightarrow p_1=\frac{\mathbb E[X0^{(X-1)}]}{n!}$$ $$\Rightarrow p_1=\frac{\mathbb E[0]}{n!}$$ $$\Rightarrow p_1=\frac{0}{n!}=0$$ Can you please prove the relation in equation (A) and check it for Binomial Distribution where the generating function of the Binomial Distribution is $P(s)=(1-p+ps)^n$,  where $p$ denotes probability of success.",,"['probability', 'statistics', 'probability-theory', 'probability-distributions', 'generating-functions']"
49,"How can I find a villain's hideout given a set of previous locations? (Or, how can I identify the centeroid of a cluster of datapoints?)","How can I find a villain's hideout given a set of previous locations? (Or, how can I identify the centeroid of a cluster of datapoints?)",,"Imagine this... Batman has just retrieved a tracking device he placed on The Joker 150 days ago.  The good news is that it has 150 coordinates one from each day.  The bad news is that all the data is randomly sorted  there's no way to tell when the coordinates were recorded, nor their sequence. Further, all the data was collected at random times during the day so we can't even be sure any of the points were actually taken at the hideout it might very well be in between some of them.  How can we help Batman find the secret hideout? Here's a map of the dataset: http://batchgeo.com/map/c3676fe29985f00e1605cd4f86920179 Here's a pastebin of raw 150 geocodes: http://pastebin.com/grVsbgL9 In math terms, I'm looking for help identifying the centroid of a complex cluster of data. As you'll notice in this data set, there are several clusters (San Francisco, LA, Chicago and NYC) along with lots of noise throughout the rest.  I need to determine which cluster is primary, and identify the centroid of this cluster. Can you recommend a strategy?  Preferably one with some meat I can use to begin analyzing the data for the ""secret hideout""? ;)","Imagine this... Batman has just retrieved a tracking device he placed on The Joker 150 days ago.  The good news is that it has 150 coordinates one from each day.  The bad news is that all the data is randomly sorted  there's no way to tell when the coordinates were recorded, nor their sequence. Further, all the data was collected at random times during the day so we can't even be sure any of the points were actually taken at the hideout it might very well be in between some of them.  How can we help Batman find the secret hideout? Here's a map of the dataset: http://batchgeo.com/map/c3676fe29985f00e1605cd4f86920179 Here's a pastebin of raw 150 geocodes: http://pastebin.com/grVsbgL9 In math terms, I'm looking for help identifying the centroid of a complex cluster of data. As you'll notice in this data set, there are several clusters (San Francisco, LA, Chicago and NYC) along with lots of noise throughout the rest.  I need to determine which cluster is primary, and identify the centroid of this cluster. Can you recommend a strategy?  Preferably one with some meat I can use to begin analyzing the data for the ""secret hideout""? ;)",,"['statistics', 'algorithms', 'clustering']"
50,generalized method of moments and the case when solving linear regression with two error conditions,generalized method of moments and the case when solving linear regression with two error conditions,,"So, I am slowly getting introduced to generalized method of moments (GMM), but I am getting confused over some issues, and this is one of them: I heard that GMM solves the problem that an estimator may not be able to satisfy both conditions, that is $E(x_t\epsilon_t) = 0$ and $E(\epsilon_t) = 0$. But I am having a hard time understanding a GMM estimator function created in this case - so we can create two super-functions (above two) that are zero - and how are they combined to form a single zero function that GMM requires? In other words, in slide 13 of http://homepage.univie.ac.at/robert.kunst/gmm.pdf , there is OLS as GMM, but I am having a hard time understanding how a function is being created. Can anyone explain this? Edit: OK, so it seems that what I am really having a problem is this: in OLS, it is often said that we need to satisfy the above equation and variance conditions (that expectation of variance always the same.). But in GMM usage of OLS, while instrumental variables $k_t$ are used so that $E(k_t \epsilon_t) = 0$, there are no other further conditions imposed. So, what's going on?","So, I am slowly getting introduced to generalized method of moments (GMM), but I am getting confused over some issues, and this is one of them: I heard that GMM solves the problem that an estimator may not be able to satisfy both conditions, that is $E(x_t\epsilon_t) = 0$ and $E(\epsilon_t) = 0$. But I am having a hard time understanding a GMM estimator function created in this case - so we can create two super-functions (above two) that are zero - and how are they combined to form a single zero function that GMM requires? In other words, in slide 13 of http://homepage.univie.ac.at/robert.kunst/gmm.pdf , there is OLS as GMM, but I am having a hard time understanding how a function is being created. Can anyone explain this? Edit: OK, so it seems that what I am really having a problem is this: in OLS, it is often said that we need to satisfy the above equation and variance conditions (that expectation of variance always the same.). But in GMM usage of OLS, while instrumental variables $k_t$ are used so that $E(k_t \epsilon_t) = 0$, there are no other further conditions imposed. So, what's going on?",,"['statistics', 'economics', 'statistical-inference']"
51,Converting Expected value to integrals and differentiating,Converting Expected value to integrals and differentiating,,"Can you suggest me how to convert the following expected value function in to an integral and differentiate it with respect to $a$. \begin{equation*} g \equiv E \left[ \max \left( a + x-b,0 \right) \times 1_{\{c+y \leq b \}} \right] \end{equation*}  where $a,b,c \in \mathbb{R} $ and the term after the multiplication sign is an indicator function. $x$ and $y$ are both random variables with continuous probability distribution functions $f_x$ and $f_y$.","Can you suggest me how to convert the following expected value function in to an integral and differentiate it with respect to $a$. \begin{equation*} g \equiv E \left[ \max \left( a + x-b,0 \right) \times 1_{\{c+y \leq b \}} \right] \end{equation*}  where $a,b,c \in \mathbb{R} $ and the term after the multiplication sign is an indicator function. $x$ and $y$ are both random variables with continuous probability distribution functions $f_x$ and $f_y$.",,"['probability', 'integration', 'statistics', 'probability-distributions']"
52,Finding a PDF given a (strictly) right continuous CDF.,Finding a PDF given a (strictly) right continuous CDF.,,"I have the CDF: $$ F(x)= \begin{cases} 0 & \text{if } x < 1 \\ \frac{x^2-2x+2}{2} & \text{if } 1 \le x < 2 \\ 1 & \text{if } x \ge 2 \end{cases} $$ I want to find the PDF and I noticed that $F$ is not continuous (at $x=1$), but it is right continuous.  Therefore it's still a valid CDF.  And I know, after reading different posts on this site, that the PDF is $$ f(x)= \begin{cases} 0 & \text{if } x < 1 \\ \frac{1}{2} & \text{if } x=1 \\ x-1 & \text{if } 1 < x < 2 \\ 0 & \text{if } x \ge 2 \end{cases} $$ My question is this: Why (according to what definition/fact/theorem) is the probability mass $\frac{1}{2}$ at $x=1$? If you blindly differentiate the CDF, piece-by-piece, you lose that information; at least I did. (Edit) I always thought the probability of a single point from a continuous random variable was $0$. (End edit) Thank you in advance for your help and insights.","I have the CDF: $$ F(x)= \begin{cases} 0 & \text{if } x < 1 \\ \frac{x^2-2x+2}{2} & \text{if } 1 \le x < 2 \\ 1 & \text{if } x \ge 2 \end{cases} $$ I want to find the PDF and I noticed that $F$ is not continuous (at $x=1$), but it is right continuous.  Therefore it's still a valid CDF.  And I know, after reading different posts on this site, that the PDF is $$ f(x)= \begin{cases} 0 & \text{if } x < 1 \\ \frac{1}{2} & \text{if } x=1 \\ x-1 & \text{if } 1 < x < 2 \\ 0 & \text{if } x \ge 2 \end{cases} $$ My question is this: Why (according to what definition/fact/theorem) is the probability mass $\frac{1}{2}$ at $x=1$? If you blindly differentiate the CDF, piece-by-piece, you lose that information; at least I did. (Edit) I always thought the probability of a single point from a continuous random variable was $0$. (End edit) Thank you in advance for your help and insights.",,['statistics']
53,prove that it is a pivotal quantity,prove that it is a pivotal quantity,,"My problem is that I have the distribution $f_{z}(x)=\dfrac{2z^2}{x^3}$, $0<z<x$ and I have to prove that $T(X_1,\ldots,X_n\mid z)= \dfrac{1}{z}\min(X_1,\ldots,X_n)$ is a pivotal quantity. I have calculated the distribution of $\min(X_1,\ldots,X_n)$ and my result is $\dfrac{z^{2n}}{x^{2n+1}}n$ so I dont get the result i have been asked. I have calculate the distribution wrong? thanks","My problem is that I have the distribution $f_{z}(x)=\dfrac{2z^2}{x^3}$, $0<z<x$ and I have to prove that $T(X_1,\ldots,X_n\mid z)= \dfrac{1}{z}\min(X_1,\ldots,X_n)$ is a pivotal quantity. I have calculated the distribution of $\min(X_1,\ldots,X_n)$ and my result is $\dfrac{z^{2n}}{x^{2n+1}}n$ so I dont get the result i have been asked. I have calculate the distribution wrong? thanks",,['statistics']
54,Covariance of sums of random variables,Covariance of sums of random variables,,"I need some help understanding an excercise. Let $X_1, X_2, X_3 \sim N(-2,3).$ (right here there is an ambiguity about the second parameter: is it $\sigma$ or $\sigma^2$ ?) First they calculate the variance $$\sigma^2\left(\sum_{i=1}^3 i  X_i\right) = \sum_{i=1}^3 i^2 \sigma^2 X_i = 14 \cdot 9 = 126$$ So far I think I understand. This result implies that $\sigma = 3$ . Am I correct? Then they give the following line without any explanation: $$\operatorname{cov}\left(\sum_{i=1}^3 i  X_i, \sum_{i=1}^3 X_i\right) = \sum_{i=1}^3 i \cdot \sigma^2 X_i = 54$$ Can you explain what happens here? How did they derive $\sum_{i=1}^3 i \cdot \sigma^2 X_i $?","I need some help understanding an excercise. Let $X_1, X_2, X_3 \sim N(-2,3).$ (right here there is an ambiguity about the second parameter: is it $\sigma$ or $\sigma^2$ ?) First they calculate the variance $$\sigma^2\left(\sum_{i=1}^3 i  X_i\right) = \sum_{i=1}^3 i^2 \sigma^2 X_i = 14 \cdot 9 = 126$$ So far I think I understand. This result implies that $\sigma = 3$ . Am I correct? Then they give the following line without any explanation: $$\operatorname{cov}\left(\sum_{i=1}^3 i  X_i, \sum_{i=1}^3 X_i\right) = \sum_{i=1}^3 i \cdot \sigma^2 X_i = 54$$ Can you explain what happens here? How did they derive $\sum_{i=1}^3 i \cdot \sigma^2 X_i $?",,"['probability', 'statistics']"
55,How can I prove the variance of residuals in simple linear regression?,How can I prove the variance of residuals in simple linear regression?,,"How can I prove the variance of residuals in simple linear regression? Please help me. $ \operatorname{var}(r_i)=\sigma^2\left[1-\frac{1}{n}-\dfrac{(x_i-\bar{x})^2}{\sum_{l=1}^{n}(x_l-\bar{x})}\right]$ I tried.. using $r_i=y_i-\hat{y_i}$ $\operatorname{var}(r_i)=\operatorname{var}(y_i-\hat{y_i})=\operatorname{var}(y_i-\bar{y})+\operatorname{var}(\hat{\beta_1}(x_i-\bar{x}))-2\operatorname{Cov}((y_i-\bar{y}),\hat{\beta_1}(x_i-\bar{x}))$ How can I go further? If there's more information needed, please ask me to provide it.","How can I prove the variance of residuals in simple linear regression? Please help me. $ \operatorname{var}(r_i)=\sigma^2\left[1-\frac{1}{n}-\dfrac{(x_i-\bar{x})^2}{\sum_{l=1}^{n}(x_l-\bar{x})}\right]$ I tried.. using $r_i=y_i-\hat{y_i}$ $\operatorname{var}(r_i)=\operatorname{var}(y_i-\hat{y_i})=\operatorname{var}(y_i-\bar{y})+\operatorname{var}(\hat{\beta_1}(x_i-\bar{x}))-2\operatorname{Cov}((y_i-\bar{y}),\hat{\beta_1}(x_i-\bar{x}))$ How can I go further? If there's more information needed, please ask me to provide it.",,['statistics']
56,Can we find an unbiased estimator for $\theta$ which reaches the Cramer Rao Lower Bound?,Can we find an unbiased estimator for  which reaches the Cramer Rao Lower Bound?,\theta,"Let $f(x)=\theta x^{-(\theta+1)}$ for $x>1$, where $\theta>1$ is an unknown parameter. Can we find an unbiased estimator for $\theta$ in this case which reaches the Cramer Rao Lower Bound? My attempt: To find such an estimator, I think I first need to find a sufficient statistic. After that, I think the Rao-Blackwell theorem should be applied.  Therefore, I tried to use the following theorem : (Screenshot from the Wikipedia website). In my case this would imply $\prod\limits_{i=1}^n \theta x_i^{-\theta-1}=\prod\limits_{i=1}^n x_i^{-1}\theta x_i^{-\theta}$. I am in doubt whether this is in the form required by theorem or not (I would say no). Is it in the required form? If not, would there be an other way to factor the function properly?","Let $f(x)=\theta x^{-(\theta+1)}$ for $x>1$, where $\theta>1$ is an unknown parameter. Can we find an unbiased estimator for $\theta$ in this case which reaches the Cramer Rao Lower Bound? My attempt: To find such an estimator, I think I first need to find a sufficient statistic. After that, I think the Rao-Blackwell theorem should be applied.  Therefore, I tried to use the following theorem : (Screenshot from the Wikipedia website). In my case this would imply $\prod\limits_{i=1}^n \theta x_i^{-\theta-1}=\prod\limits_{i=1}^n x_i^{-1}\theta x_i^{-\theta}$. I am in doubt whether this is in the form required by theorem or not (I would say no). Is it in the required form? If not, would there be an other way to factor the function properly?",,['statistics']
57,"How to calculate R-square from adjusted r-square, n, and p?","How to calculate R-square from adjusted r-square, n, and p?",,Let $\bar{R}^2$ denote the adjusted coefficient of determination. I have $\bar{R}^2 = 0.9199$ with 15 cases. Now I am trying to find $R^2$ given the results below. I found the formula for $R^2$ but did not understand it. How do you calculate $R^2$ from $\bar{R}^2$? $\bar{R}^2 = 1-\dfrac{(n-1)(1- R^2)}{n-p-1}$,Let $\bar{R}^2$ denote the adjusted coefficient of determination. I have $\bar{R}^2 = 0.9199$ with 15 cases. Now I am trying to find $R^2$ given the results below. I found the formula for $R^2$ but did not understand it. How do you calculate $R^2$ from $\bar{R}^2$? $\bar{R}^2 = 1-\dfrac{(n-1)(1- R^2)}{n-p-1}$,,['statistics']
58,Combination calculation with reducing set size,Combination calculation with reducing set size,,"My statistics aren't too great, so I'm struggle to work out the result of the following situation. Say you have 5 sets of 5 possible options (25 options total); and you select 1 option from each set. Each time you select 1 option from a set, that set is removed from the next round of possible options; leaving 4 sets of 5 possible options. Again, pick another option, leaving 3 sets of 5 possible options. So the selection process is to pick 5 options, one from each set; and the total amount of options reduces by 5 on each round of selection. Eg. Set 1 Option 1, Option 2, Option 3, Option 4, Option 5  Set 2 Option 6, Option 7, Option 8, Option 9, Option 10  Set 3 Option 11, Option 12, Option 13, Option 14, Option 15  Set 4 Option 16, Option 17, Option 18, Option 19, Option 20  Set 5 Option 21, Option 22, Option 23, Option 24, Option 25 So you pick Option 1 first, that leaves Sets 2-5 (20 options remaining) Then you pick Option 6 , that leaves Sets 3-5 (15 options remaining) You pick Option 11 , that leaves Sets 4-5 (10 options remaining) You pick Option 16 , that leaves Set 5 (5 options remaining) You pick Option 21 , there are no items left The order that the items are selected in is not important - but how many possible combinations does it mean you could select? The very basic maths of 25 * 20 * 15 * 10 * 5 = 375,000 Wouldn't factor in out-of-order repetition (which we don't care about). So how many combinations could there be?","My statistics aren't too great, so I'm struggle to work out the result of the following situation. Say you have 5 sets of 5 possible options (25 options total); and you select 1 option from each set. Each time you select 1 option from a set, that set is removed from the next round of possible options; leaving 4 sets of 5 possible options. Again, pick another option, leaving 3 sets of 5 possible options. So the selection process is to pick 5 options, one from each set; and the total amount of options reduces by 5 on each round of selection. Eg. Set 1 Option 1, Option 2, Option 3, Option 4, Option 5  Set 2 Option 6, Option 7, Option 8, Option 9, Option 10  Set 3 Option 11, Option 12, Option 13, Option 14, Option 15  Set 4 Option 16, Option 17, Option 18, Option 19, Option 20  Set 5 Option 21, Option 22, Option 23, Option 24, Option 25 So you pick Option 1 first, that leaves Sets 2-5 (20 options remaining) Then you pick Option 6 , that leaves Sets 3-5 (15 options remaining) You pick Option 11 , that leaves Sets 4-5 (10 options remaining) You pick Option 16 , that leaves Set 5 (5 options remaining) You pick Option 21 , there are no items left The order that the items are selected in is not important - but how many possible combinations does it mean you could select? The very basic maths of 25 * 20 * 15 * 10 * 5 = 375,000 Wouldn't factor in out-of-order repetition (which we don't care about). So how many combinations could there be?",,['statistics']
59,Cramer-Rao Lower Bound,Cramer-Rao Lower Bound,,"Assume that $X_1,X_2,\ldots,X_N\sim N(\mu,2^2)$ and $Y_1,Y_2,\ldots,Y_M\sim N(0,\sigma^2)$. a)Find the Cramer-Rao Lower Bound (CRLB) for the variance of the unbiased estimators of $\mu$. b)Find the CRLB for the variances of the unbiased estimators of $\mu^2$. c)Is the MLE, $\hat{\mu}$, a uniformly minimum variance unbiased estimator (UMVUE) of $\mu$? so for part a) I got $\dfrac{4}{n}$  and for part b) I got $\dfrac{\sigma^2}{n}$. Are these answers correct? Just want to know if I'm on the right track. Lastly for part C, can anyone give me some guidance on where to start? Kind of lost haha","Assume that $X_1,X_2,\ldots,X_N\sim N(\mu,2^2)$ and $Y_1,Y_2,\ldots,Y_M\sim N(0,\sigma^2)$. a)Find the Cramer-Rao Lower Bound (CRLB) for the variance of the unbiased estimators of $\mu$. b)Find the CRLB for the variances of the unbiased estimators of $\mu^2$. c)Is the MLE, $\hat{\mu}$, a uniformly minimum variance unbiased estimator (UMVUE) of $\mu$? so for part a) I got $\dfrac{4}{n}$  and for part b) I got $\dfrac{\sigma^2}{n}$. Are these answers correct? Just want to know if I'm on the right track. Lastly for part C, can anyone give me some guidance on where to start? Kind of lost haha",,"['probability', 'statistics']"
60,Difference between a long tail and normal distribution,Difference between a long tail and normal distribution,,"Wikipedia article about ""long tail"" says that: A probability distribution is said to have a long tail, if a larger   share of population rests within its tail than would under a normal   distribution. I am confused about this. Isn't a normal distribution exponential one? Meaning even large values of x will have some corresponding value of y, thus making those values of x ""rest within its tail""? Or does this sentence mean that area of tail in long-tail-distribution is more than are of tails in a normal-distribution? Also what would be the difference between a long-tailed and a fat-tailed distribution?","Wikipedia article about ""long tail"" says that: A probability distribution is said to have a long tail, if a larger   share of population rests within its tail than would under a normal   distribution. I am confused about this. Isn't a normal distribution exponential one? Meaning even large values of x will have some corresponding value of y, thus making those values of x ""rest within its tail""? Or does this sentence mean that area of tail in long-tail-distribution is more than are of tails in a normal-distribution? Also what would be the difference between a long-tailed and a fat-tailed distribution?",,['statistics']
61,Homework Question. Joint Probability Distribution.,Homework Question. Joint Probability Distribution.,,"Here is the question. The joint PDF of X and Y is given by $f_{XY}(x,y) = {\frac 14} e^{-|x|-|y|}$. Find $P(X \le 1 ,and,  Y \le 0)$ Solving the problem I first found the marginal probabilities of X and Y. Can you please explain what I should do next.","Here is the question. The joint PDF of X and Y is given by $f_{XY}(x,y) = {\frac 14} e^{-|x|-|y|}$. Find $P(X \le 1 ,and,  Y \le 0)$ Solving the problem I first found the marginal probabilities of X and Y. Can you please explain what I should do next.",,"['probability', 'statistics', 'random-variables']"
62,How to estimate CTR (click-through rate)?,How to estimate CTR (click-through rate)?,,"How many times banner should be shown to estimate CTR?  For example, a banner was shown x times, and was clicked y times. CTR = y/x; How to evaluate incaccuracy of this value?","How many times banner should be shown to estimate CTR?  For example, a banner was shown x times, and was clicked y times. CTR = y/x; How to evaluate incaccuracy of this value?",,['statistics']
63,Problem with a summation suffix.,Problem with a summation suffix.,,"Please can someone tell me why we drop the summation of i here? $S = \sum_{i,j}(y_{ij}-\mu -\alpha_i)^2$ $\frac{dS}{d\alpha_i} = \sum_{j}(y_{ij}-\mu -\alpha_i)$ It's part of a question which is really bugging me? Thank you for any help I really appreciate it!","Please can someone tell me why we drop the summation of i here? $S = \sum_{i,j}(y_{ij}-\mu -\alpha_i)^2$ $\frac{dS}{d\alpha_i} = \sum_{j}(y_{ij}-\mu -\alpha_i)$ It's part of a question which is really bugging me? Thank you for any help I really appreciate it!",,"['statistics', 'summation', 'notation']"
64,Monte Carlo Rejection Sampling Method,Monte Carlo Rejection Sampling Method,,"I have the following passage from a set of lecture notes I am working on that I would like to understand a little better. $\underline{\text{Algorithm for Rejection Sampling}}$: Given two densities $f$, $g$, with $f(x) < M.g(x)$ for all $x$, and some constant $M$, we can generate a sample from $f$ by Draw $X \sim g$ Accept $X$ as a sample from $f$ with probability $\large \frac{f(X)}{M.g(X)}$ otherwise go back to step 1. Now, I understand why this will generate samples from the distribution with density $f$, but I really do not understand how to use this algorithm in practice. I draw an $X$ from the distribution with density $g$, and then I need to decide whether to keep it, or reject it.  I don't understand the condition to accept it... do I accept it if the probability defined in 2. is greater than e.g. 0.5, or maybe 0.7? Is this probability up to me to decide? Thanks for your insight and thoughts.","I have the following passage from a set of lecture notes I am working on that I would like to understand a little better. $\underline{\text{Algorithm for Rejection Sampling}}$: Given two densities $f$, $g$, with $f(x) < M.g(x)$ for all $x$, and some constant $M$, we can generate a sample from $f$ by Draw $X \sim g$ Accept $X$ as a sample from $f$ with probability $\large \frac{f(X)}{M.g(X)}$ otherwise go back to step 1. Now, I understand why this will generate samples from the distribution with density $f$, but I really do not understand how to use this algorithm in practice. I draw an $X$ from the distribution with density $g$, and then I need to decide whether to keep it, or reject it.  I don't understand the condition to accept it... do I accept it if the probability defined in 2. is greater than e.g. 0.5, or maybe 0.7? Is this probability up to me to decide? Thanks for your insight and thoughts.",,"['statistics', 'monte-carlo', 'sampling']"
65,Statistics and Probabilities- Distributions,Statistics and Probabilities- Distributions,,A quality control engineer tests the quality of produced computers. Suppose that 5% of computers have defects and defects occur independently of each other. I need to find the probability that the engineer has to test at least 5 computers in order to find 2 defective ones. I thought of it and came up with a solution though I'm not sure whether it's correct or not. The probability of finding $2$ defective is $p= 0.05*0.05=0.025$   $P(T\geq 5)= 1- [ P(T=4)+P(T=3)+(T=2)]= \\ 1-[0.025*(0.975)^3+0.025*(0.975)^2+0.025*(0.975)^1]=\\ 1-[0.025*0.975(1+0.975+(0.975)^2)= 0.9300442$,A quality control engineer tests the quality of produced computers. Suppose that 5% of computers have defects and defects occur independently of each other. I need to find the probability that the engineer has to test at least 5 computers in order to find 2 defective ones. I thought of it and came up with a solution though I'm not sure whether it's correct or not. The probability of finding $2$ defective is $p= 0.05*0.05=0.025$   $P(T\geq 5)= 1- [ P(T=4)+P(T=3)+(T=2)]= \\ 1-[0.025*(0.975)^3+0.025*(0.975)^2+0.025*(0.975)^1]=\\ 1-[0.025*0.975(1+0.975+(0.975)^2)= 0.9300442$,,"['probability', 'statistics', 'probability-distributions']"
66,simple explanation of gaussian mixture model,simple explanation of gaussian mixture model,,"I need some help understanding Gaussian mixture models. In particular, I am trying to find the relationship between GMMs and K means. What is the basic algorithm for GMM? I am not sure where the ""clustering"" comes in. Can someone give me a basic example as to how this actually works?","I need some help understanding Gaussian mixture models. In particular, I am trying to find the relationship between GMMs and K means. What is the basic algorithm for GMM? I am not sure where the ""clustering"" comes in. Can someone give me a basic example as to how this actually works?",,"['probability', 'statistics']"
67,Obtain the cumulative distribution function of $X_1+X_2$,Obtain the cumulative distribution function of,X_1+X_2,"Suppose $X_1$ is a standard normal random variable. Define    $$X_2=\begin{cases} -X_1, &\text{if} \,\, |X_1|<1 \\ \,\,\,\,X_1, & \text{otherwise}\end{cases}$$ Obtain the cumulative distribution function of $X_1+X_2$ in terms of the cumulative distribution function of a standard normal random   variable. Trial: Define $Y=X_1+X_2$. So we have $$Y=\begin{cases} \,\,0, &\text{if} \,\, |X_1|<1 \\ 2X_1, & \text{otherwise}\end{cases}$$Then I am stuck. Please help. Thanks in advance.","Suppose $X_1$ is a standard normal random variable. Define    $$X_2=\begin{cases} -X_1, &\text{if} \,\, |X_1|<1 \\ \,\,\,\,X_1, & \text{otherwise}\end{cases}$$ Obtain the cumulative distribution function of $X_1+X_2$ in terms of the cumulative distribution function of a standard normal random   variable. Trial: Define $Y=X_1+X_2$. So we have $$Y=\begin{cases} \,\,0, &\text{if} \,\, |X_1|<1 \\ 2X_1, & \text{otherwise}\end{cases}$$Then I am stuck. Please help. Thanks in advance.",,"['statistics', 'probability-distributions']"
68,Quicksort analysis problem,Quicksort analysis problem,,"This is a problem from a probability textbook, not a CS one, if you are curious. Since I'm too lazy to retype the $\LaTeX$ I will post an ugly stitched screenshot: This seems ridiculously hard to approach, and it doesn't help that all the difficult problems have no solutions in the textbook (the uselessly easy ones do :P ). How would I attack it?","This is a problem from a probability textbook, not a CS one, if you are curious. Since I'm too lazy to retype the $\LaTeX$ I will post an ugly stitched screenshot: This seems ridiculously hard to approach, and it doesn't help that all the difficult problems have no solutions in the textbook (the uselessly easy ones do :P ). How would I attack it?",,"['probability', 'statistics', 'algorithms']"
69,Probability Related Question,Probability Related Question,,"In a large population, people are one of 3 genetic types A, B and C: 30% are type A, 60% type B and 10% type C. The probability a person carries another gene making them susceptible for a disease is .05 for A, .04 for B and .02 for C. If ten unrelated persons are selected, what is the probability at least one is susceptible for the disease? Answer is 0.342 . I know the probability that a random person would be susceptible for the disease will be percentage of each population * their likeliness for the disease. So, 0.3 * 0.05 + 0.6 * 0.04 + 0.1 * 0.02 = 0.041. Nvm, got it. So since the probability for a random person who has the disease is 0.041, the probability that a random person DOESN'T have the disease will be 1 - 0.041 = 0.959. Now since  we have ten people the probability that NONE of them have the disease is (0.959)*10 = 0.658. The original question asked for the probability that atleast one person had it. Which is the same as asking, 1 - P(0 people have it) = 1 - 0.658 = 0.342","In a large population, people are one of 3 genetic types A, B and C: 30% are type A, 60% type B and 10% type C. The probability a person carries another gene making them susceptible for a disease is .05 for A, .04 for B and .02 for C. If ten unrelated persons are selected, what is the probability at least one is susceptible for the disease? Answer is 0.342 . I know the probability that a random person would be susceptible for the disease will be percentage of each population * their likeliness for the disease. So, 0.3 * 0.05 + 0.6 * 0.04 + 0.1 * 0.02 = 0.041. Nvm, got it. So since the probability for a random person who has the disease is 0.041, the probability that a random person DOESN'T have the disease will be 1 - 0.041 = 0.959. Now since  we have ten people the probability that NONE of them have the disease is (0.959)*10 = 0.658. The original question asked for the probability that atleast one person had it. Which is the same as asking, 1 - P(0 people have it) = 1 - 0.658 = 0.342",,"['probability', 'statistics', 'conditional-probability']"
70,Fairly complicated partial derivative,Fairly complicated partial derivative,,"I have a stats assignment, that requires the use of non linear regression - this I am fine with in principle, however I can't get the initial $X$ matrix, because I don't understand partial derivatives. All the examples I have found are either very simple, or don't explain why things are the way they are, or both. So, this is the equation: $$y_i = \frac{r_1^*x_i^2+x_i(1-x_i)}{r_1^*x_i^2+2x_i(1-x_i) + r_2^*(1-x_i)^2}+\mathcal{E}_i$$ I need to know the derivative with relation to $r_1^*$ and $r_2^*$. I would love to know both the equation of how to get it (with an explanation) as well as the actual values where: $r_1^* = 0.4$ $r_2^* = 0.6$ for each of these four $x_i$'s: 0.2145 0.6074 0.7831 0.8976 Like I said above, an explanation, the equation and the answers would be great, as I am totally stumped! Thanks!","I have a stats assignment, that requires the use of non linear regression - this I am fine with in principle, however I can't get the initial $X$ matrix, because I don't understand partial derivatives. All the examples I have found are either very simple, or don't explain why things are the way they are, or both. So, this is the equation: $$y_i = \frac{r_1^*x_i^2+x_i(1-x_i)}{r_1^*x_i^2+2x_i(1-x_i) + r_2^*(1-x_i)^2}+\mathcal{E}_i$$ I need to know the derivative with relation to $r_1^*$ and $r_2^*$. I would love to know both the equation of how to get it (with an explanation) as well as the actual values where: $r_1^* = 0.4$ $r_2^* = 0.6$ for each of these four $x_i$'s: 0.2145 0.6074 0.7831 0.8976 Like I said above, an explanation, the equation and the answers would be great, as I am totally stumped! Thanks!",,"['statistics', 'partial-derivative']"
71,Condition Expectation of Difference between Two Poisson processes,Condition Expectation of Difference between Two Poisson processes,,$P_t$ and $Q_t$ are poisson processes with rates $a$ and $b$. How do I calculate $E[(P_t-Q_t)]^2|Q_t=m-P_t]$?,$P_t$ and $Q_t$ are poisson processes with rates $a$ and $b$. How do I calculate $E[(P_t-Q_t)]^2|Q_t=m-P_t]$?,,"['probability', 'statistics', 'probability-theory', 'probability-distributions', 'stochastic-processes']"
72,Packing radios into cartons - why is my solution wrong?,Packing radios into cartons - why is my solution wrong?,,"A manufacturer of car radios ships them to retailers in cartons of $n$ radios. The profit per radio is $\$59.50$, minus shipping cost of $\$25$ per carton, so the profit is $59.5n-25$ dollars per carton. To promote sales by assuring high quality, the manufacturer promises to pay the retailer $\$200X^2$ if $X$ radios in the carton are defective. Suppose radios are produced independently and that $5\%$ of radios are defective. How many radios should be packed per carton to maximize expected net profit per carton? My solution: Suppose that $n$ radios are packed into a carton. Then, the expected profit is clearly $$\mu = 59.5n-25-200(0.05n)^2$$ Simplifying we get $$\mu = 59.5n-25-0.5n^2$$ We want to find a maximum, so we find a derivative: $$\mu'=59.5-n^2$$ Clearly there is just one maximum, so set $\mu'=0$ we find that $$n^2=59.5$$ and thus $$n\approx 7.7136\dots$$This however seems to be wrong. The textbook gives an answer of exactly $50$ in the solution without any explanation of the steps.","A manufacturer of car radios ships them to retailers in cartons of $n$ radios. The profit per radio is $\$59.50$, minus shipping cost of $\$25$ per carton, so the profit is $59.5n-25$ dollars per carton. To promote sales by assuring high quality, the manufacturer promises to pay the retailer $\$200X^2$ if $X$ radios in the carton are defective. Suppose radios are produced independently and that $5\%$ of radios are defective. How many radios should be packed per carton to maximize expected net profit per carton? My solution: Suppose that $n$ radios are packed into a carton. Then, the expected profit is clearly $$\mu = 59.5n-25-200(0.05n)^2$$ Simplifying we get $$\mu = 59.5n-25-0.5n^2$$ We want to find a maximum, so we find a derivative: $$\mu'=59.5-n^2$$ Clearly there is just one maximum, so set $\mu'=0$ we find that $$n^2=59.5$$ and thus $$n\approx 7.7136\dots$$This however seems to be wrong. The textbook gives an answer of exactly $50$ in the solution without any explanation of the steps.",,"['probability', 'statistics', 'optimization']"
73,Random variable and trying to find $E(N)$,Random variable and trying to find,E(N),Given $N = \text{random variable that counts the fraction of trials that are successful trials} = 12$ $N = S/12$ $S = \text{number of successful trials}$ $E(N)= ?$ i don't know how to find $E(N)$ is there a specific formula? can someone help me out with this problem,Given $N = \text{random variable that counts the fraction of trials that are successful trials} = 12$ $N = S/12$ $S = \text{number of successful trials}$ $E(N)= ?$ i don't know how to find $E(N)$ is there a specific formula? can someone help me out with this problem,,"['statistics', 'discrete-mathematics']"
74,Proof related to Chebychev's inequality,Proof related to Chebychev's inequality,,"I need to prove that in a set of $N$ data $x_1, x_2, \ldots, x_n$, for all $i$ between 1 and $N$, we have $$\mu-\sigma \sqrt N \leq x_i \leq \mu+\sigma \sqrt N$$ where $\mu$ is the average and $\sigma$ the standard deviation. I know I need to use Chebyshev's inequality $$\Pr(\mu-k\sigma  \leq X \leq \mu+k\sigma) > 1-1/k^2.$$ I did notice the $X$ changing to $x_i$, which from my understanding means that what I am trying to prove is about one data ($x_i$) whereas Chebichev's inequality is about a set of data.","I need to prove that in a set of $N$ data $x_1, x_2, \ldots, x_n$, for all $i$ between 1 and $N$, we have $$\mu-\sigma \sqrt N \leq x_i \leq \mu+\sigma \sqrt N$$ where $\mu$ is the average and $\sigma$ the standard deviation. I know I need to use Chebyshev's inequality $$\Pr(\mu-k\sigma  \leq X \leq \mu+k\sigma) > 1-1/k^2.$$ I did notice the $X$ changing to $x_i$, which from my understanding means that what I am trying to prove is about one data ($x_i$) whereas Chebichev's inequality is about a set of data.",,['statistics']
75,Pearson Correlation Coefficient Interpretation,Pearson Correlation Coefficient Interpretation,,"Let $X=(1,2,3,...,20)$. Suppose that $Y=(y_1,y_2,...,y_{20})$ with $y_i=x_i^2$ and $Z=(z_1,z_2,...,z_{20})$ with $z_i=e^{x_i}$. Pearson correlation coefficient is defined by formula  \begin{equation} \rho(X,Y)=\frac{\sum_{i=1}^{20} (x_i-\bar{x})(y_i-\bar{y})}{\sqrt{(\sum_{i=1}^{20}(x_i-\bar{x})^{2})(\sum_{i=1}^{20}(y_i-\bar{y})^{2})}} \end{equation} If $\rho(X,Y)=1$, we can say that $X$ and $Y$ have a linear correlation. If $0.7\leq\rho(X,Y)<1$ then $X$ and $Y$ has a strong linear correlation, if $0.5\leq\rho(X,Y)<0.7$ then $X$ and $Y$ has a modest linear correlation, and if $0\leq\rho(X,Y)<0.5$ then $X$ and $Y$ has a weak linear correlation.  Using this formula, we get $\rho(X,Y)=0.9$ and $\rho(X,Z)=0.5$. However, the relationship between $X$ and $Y$ is actually quadratic but they have the high correlation coefficient that indicate linear correlation. So, my question is what is ""linear correlation"" actually between $X$ and $Y$ ? Since $\rho(X,Z)=0.5$ indicate the modest correlation coefficient, what is another intepretation of this value? What is the difference between $\rho(X,Y)$ and $ \rho(X,Z)$, noting that $Y$ and $Z$ is not a linear function of $X$.","Let $X=(1,2,3,...,20)$. Suppose that $Y=(y_1,y_2,...,y_{20})$ with $y_i=x_i^2$ and $Z=(z_1,z_2,...,z_{20})$ with $z_i=e^{x_i}$. Pearson correlation coefficient is defined by formula  \begin{equation} \rho(X,Y)=\frac{\sum_{i=1}^{20} (x_i-\bar{x})(y_i-\bar{y})}{\sqrt{(\sum_{i=1}^{20}(x_i-\bar{x})^{2})(\sum_{i=1}^{20}(y_i-\bar{y})^{2})}} \end{equation} If $\rho(X,Y)=1$, we can say that $X$ and $Y$ have a linear correlation. If $0.7\leq\rho(X,Y)<1$ then $X$ and $Y$ has a strong linear correlation, if $0.5\leq\rho(X,Y)<0.7$ then $X$ and $Y$ has a modest linear correlation, and if $0\leq\rho(X,Y)<0.5$ then $X$ and $Y$ has a weak linear correlation.  Using this formula, we get $\rho(X,Y)=0.9$ and $\rho(X,Z)=0.5$. However, the relationship between $X$ and $Y$ is actually quadratic but they have the high correlation coefficient that indicate linear correlation. So, my question is what is ""linear correlation"" actually between $X$ and $Y$ ? Since $\rho(X,Z)=0.5$ indicate the modest correlation coefficient, what is another intepretation of this value? What is the difference between $\rho(X,Y)$ and $ \rho(X,Z)$, noting that $Y$ and $Z$ is not a linear function of $X$.",,"['statistics', 'correlation']"
76,How to sample point from triangle where vertex is not in origin,How to sample point from triangle where vertex is not in origin,,"This link http://mathworld.wolfram.com/TrianglePointPicking.html gives an overview of how to sample points from either a quadrilateral or triangle given one vertex is at the origin.  The standard formula is:  $x=a_{1}v_{1}+a_{2}v_{2}$ where $a_1$ and $a_2$ are from the distribution $U(0,1)$. Also, I am not sure of the intuition behind this formula as to why this yields random samples. EDIT: I know you simply add the offset vertex if the vertex is not in the origin. But the point is then contained in the quadrilateral. If I only want the points formed by a particular triangle in the quadrilateral, do I simply reject points that are not inside the vertices of the triangle?","This link http://mathworld.wolfram.com/TrianglePointPicking.html gives an overview of how to sample points from either a quadrilateral or triangle given one vertex is at the origin.  The standard formula is:  $x=a_{1}v_{1}+a_{2}v_{2}$ where $a_1$ and $a_2$ are from the distribution $U(0,1)$. Also, I am not sure of the intuition behind this formula as to why this yields random samples. EDIT: I know you simply add the offset vertex if the vertex is not in the origin. But the point is then contained in the quadrilateral. If I only want the points formed by a particular triangle in the quadrilateral, do I simply reject points that are not inside the vertices of the triangle?",,"['statistics', 'optimization']"
77,Confidence Intervals for an Exponential Distribution,Confidence Intervals for an Exponential Distribution,,"$y_{1}$ is distributed $f_{Y}(y\mid\theta) = \theta e^{-\theta y} I_{(0, \infty)}(y)$, where $\theta > 0$. Analyze the confidence interval for $\frac{1}{\theta}$ given by $[L(Y), U(Y)] = [Y, 2Y]$. 1: Determine the confidence coefficient of this interval. $\bf{Thoughts:}$ I realized that my single data point is distributed $\textit{exponential}(\beta=\frac{1}{\theta}) = \textit{gamma}(\alpha = 1, \beta = \frac{1}{\theta})$. So I did the following steps to get my confidence coefficient: $$\ P_{\theta}(\frac{1}{\theta}\in [Y, 2Y])\\ = P_{\theta}(Y \leq \frac{1}{\theta} \leq 2Y)\\ = P_{\theta}(\frac{1}{2\theta} \leq Y \leq\frac{1}{\theta})\\ = \int\limits_{\frac{1}{2\theta}}^{\frac{1}{\theta}} \! \theta e^{-\theta y} \mathrm{d}y\\ = e^{-.5} - e^{-1}\\ = .23865$$ So I got .23685 as my confidence coefficient, and I'm fairly confident that that is the correct answer. 2: Determine the expected length of this interval. $\bf{Thoughts:}$ So I'm not too sure what to do here, but this is what I have written down. $$\ P_{\theta}(\frac{1}{\theta}\in [Y, 2Y])\\ = P_{\theta}(Y \leq \frac{1}{\theta} \leq 2Y)\\ = P_{\theta}(\frac{1}{2\theta} \leq Y \leq \frac{1}{\theta})\\ = P(e^{-1} \leq Y \leq e^{-.5}) = .23865$$ Since my 'a' and 'b' values were $e^{-1}$ and $e^{-.5}$, respectively. Now I don't know where to go from there to find the expected length. Do I use the gamma probability table? Any help would be greatly appreciated. 3: Find a different confidence interval for $\frac{1}{\theta}$ with the exact same confidence coefficient, but the expected length is smaller than part 2's expected length. Find an interval where the characteristics, same confidence coefficient and expected length is smaller than part 2's, are satisfied. $\bf{Thoughts:}$ I let $W = Y^{\frac{1}{\theta}}$, and I performed the transformation to get the distribution of W, and I ended with: $$\ f_{W}(w\mid \theta) = \theta^{2} e^{-\theta w^{\theta}}w^{\theta -1}$$, which I think looks sorta like the gamma distribution, but then I don't know what to do from there. Any help would be appreciated. Thanks in advance!","$y_{1}$ is distributed $f_{Y}(y\mid\theta) = \theta e^{-\theta y} I_{(0, \infty)}(y)$, where $\theta > 0$. Analyze the confidence interval for $\frac{1}{\theta}$ given by $[L(Y), U(Y)] = [Y, 2Y]$. 1: Determine the confidence coefficient of this interval. $\bf{Thoughts:}$ I realized that my single data point is distributed $\textit{exponential}(\beta=\frac{1}{\theta}) = \textit{gamma}(\alpha = 1, \beta = \frac{1}{\theta})$. So I did the following steps to get my confidence coefficient: $$\ P_{\theta}(\frac{1}{\theta}\in [Y, 2Y])\\ = P_{\theta}(Y \leq \frac{1}{\theta} \leq 2Y)\\ = P_{\theta}(\frac{1}{2\theta} \leq Y \leq\frac{1}{\theta})\\ = \int\limits_{\frac{1}{2\theta}}^{\frac{1}{\theta}} \! \theta e^{-\theta y} \mathrm{d}y\\ = e^{-.5} - e^{-1}\\ = .23865$$ So I got .23685 as my confidence coefficient, and I'm fairly confident that that is the correct answer. 2: Determine the expected length of this interval. $\bf{Thoughts:}$ So I'm not too sure what to do here, but this is what I have written down. $$\ P_{\theta}(\frac{1}{\theta}\in [Y, 2Y])\\ = P_{\theta}(Y \leq \frac{1}{\theta} \leq 2Y)\\ = P_{\theta}(\frac{1}{2\theta} \leq Y \leq \frac{1}{\theta})\\ = P(e^{-1} \leq Y \leq e^{-.5}) = .23865$$ Since my 'a' and 'b' values were $e^{-1}$ and $e^{-.5}$, respectively. Now I don't know where to go from there to find the expected length. Do I use the gamma probability table? Any help would be greatly appreciated. 3: Find a different confidence interval for $\frac{1}{\theta}$ with the exact same confidence coefficient, but the expected length is smaller than part 2's expected length. Find an interval where the characteristics, same confidence coefficient and expected length is smaller than part 2's, are satisfied. $\bf{Thoughts:}$ I let $W = Y^{\frac{1}{\theta}}$, and I performed the transformation to get the distribution of W, and I ended with: $$\ f_{W}(w\mid \theta) = \theta^{2} e^{-\theta w^{\theta}}w^{\theta -1}$$, which I think looks sorta like the gamma distribution, but then I don't know what to do from there. Any help would be appreciated. Thanks in advance!",,['probability']
78,How do I show the equivalence of the two forms of the Anderson-Darling test statistic?,How do I show the equivalence of the two forms of the Anderson-Darling test statistic?,,"It's stated in many places regarding the Anderson-Darling test statistic, which is defined as $$n\int_{-\infty}^\infty \frac{(F_n(x) - F(x))^2}{F(x)(1 - F(x))}dF(x)$$ that this is functionally equivalent to the statistic $$A^2 = -n - S$$ where $$S = \sum_{k=1}^n\frac{2k-1}{n}\left(\ln F(Y_k) + \ln(1 - F(Y_{n+1-k}))\right)$$ Note that $F_n(x)$ is the empirical distribution function and $F(x)$ is the distribution to which we are comparing the sample. $Y_k$ is the $k^{th}$ ranked element in the sample. I even went so far as to read the original 1954 paper by Anderson and Darling and I have yet to discover how this equivalence was computed - these authors merely stated the equivalence too. I've tried writing out the numerator inside the integral and splitting into 3 integrals - I was only able to simplify one of them. I have an inkling that maybe the Probability Integral Transformation should be applied, but I'm not really sure how. I'd really appreciate if anyone could give any pointers.","It's stated in many places regarding the Anderson-Darling test statistic, which is defined as $$n\int_{-\infty}^\infty \frac{(F_n(x) - F(x))^2}{F(x)(1 - F(x))}dF(x)$$ that this is functionally equivalent to the statistic $$A^2 = -n - S$$ where $$S = \sum_{k=1}^n\frac{2k-1}{n}\left(\ln F(Y_k) + \ln(1 - F(Y_{n+1-k}))\right)$$ Note that $F_n(x)$ is the empirical distribution function and $F(x)$ is the distribution to which we are comparing the sample. $Y_k$ is the $k^{th}$ ranked element in the sample. I even went so far as to read the original 1954 paper by Anderson and Darling and I have yet to discover how this equivalence was computed - these authors merely stated the equivalence too. I've tried writing out the numerator inside the integral and splitting into 3 integrals - I was only able to simplify one of them. I have an inkling that maybe the Probability Integral Transformation should be applied, but I'm not really sure how. I'd really appreciate if anyone could give any pointers.",,"['statistics', 'improper-integrals']"
79,Stochastic ordering,Stochastic ordering,,Assume $Y$ is non negative random variable. Prove that $X+Y$ is stochastically greater than $X$ for any random variable $X$. We have to prove there that $\Pr(X+Y > x) \geq \Pr(X>x) $ for all $x$,Assume $Y$ is non negative random variable. Prove that $X+Y$ is stochastically greater than $X$ for any random variable $X$. We have to prove there that $\Pr(X+Y > x) \geq \Pr(X>x) $ for all $x$,,"['probability', 'statistics']"
80,Rayleigh distribution,Rayleigh distribution,,I have this question from my statistical theory course: A sniper shoots at a target. X and Y measure its deviation on the x and y axes. X and Y are independent and are distibuted normally with mean=0 and variance=$\sigma^2$. We are asked to calculate the PDF of R=$\sqrt{x^2+y^2}$. This is the solution to this question: I don't understand why the density that is placed in the integral is like that (in the red box). I'd love an explanation :),I have this question from my statistical theory course: A sniper shoots at a target. X and Y measure its deviation on the x and y axes. X and Y are independent and are distibuted normally with mean=0 and variance=$\sigma^2$. We are asked to calculate the PDF of R=$\sqrt{x^2+y^2}$. This is the solution to this question: I don't understand why the density that is placed in the integral is like that (in the red box). I'd love an explanation :),,"['probability', 'statistics', 'probability-theory', 'probability-distributions', 'normal-distribution']"
81,Would this still hold true if the probabilities of A and B followed a normal distribution?,Would this still hold true if the probabilities of A and B followed a normal distribution?,,"Part a) Let A be the number that the left-hand die shows. Let B be the number that the right-hand die shows. Both dice are fair and therefore the probability that B is greater than A is equal to the probability that B is less than A (i.e. P(B > A) = P(B < A)). Furthermore, all possibilities can be summarized by B > A, B < A, and B = A. Therefore P(B > A) + P(B < A) + P(A = B) = 1 Note that P(A = B) is 1/6 because whatever number B shows, A will show the same number one out of six times. Therefore equation 1 becomes: P(B > A) + P(B < A) + 1/6 = 1 Since P(B > A) = P(B < A), we can arrive at our solution with: 2P(B > A) = 1  1/6 = 5/6 Therefore P(B > A) = 5/12 The probability that the right-hand die shows a larger number than the left-hand die is 5/12 Part b) Given that the left hand die is rolled first and it shows 5 (i.e. A = 5). What is P(B > A|A = 5)? The answer is 1/6 because to be greater than A = 5, die B must show 6 (which occurs one out of six times for a six sided die).","Part a) Let A be the number that the left-hand die shows. Let B be the number that the right-hand die shows. Both dice are fair and therefore the probability that B is greater than A is equal to the probability that B is less than A (i.e. P(B > A) = P(B < A)). Furthermore, all possibilities can be summarized by B > A, B < A, and B = A. Therefore P(B > A) + P(B < A) + P(A = B) = 1 Note that P(A = B) is 1/6 because whatever number B shows, A will show the same number one out of six times. Therefore equation 1 becomes: P(B > A) + P(B < A) + 1/6 = 1 Since P(B > A) = P(B < A), we can arrive at our solution with: 2P(B > A) = 1  1/6 = 5/6 Therefore P(B > A) = 5/12 The probability that the right-hand die shows a larger number than the left-hand die is 5/12 Part b) Given that the left hand die is rolled first and it shows 5 (i.e. A = 5). What is P(B > A|A = 5)? The answer is 1/6 because to be greater than A = 5, die B must show 6 (which occurs one out of six times for a six sided die).",,"['probability', 'statistics']"
82,A Problem about Hypothesis Testing and Decision Making,A Problem about Hypothesis Testing and Decision Making,,"I have a problem about the hypothesis testing and decision making as follows: A botanist wishes to test the null hypothesis that the average diameter of the flowers of a particular plant is 9.6cm. He decides to take a random sample of size $n$=80 and accept the null hypothesis if the mean of the sample falls between 9.3cm and 9.9cm; if the mean of this sample falls outside this interval, he will reject the null hypothesis. What decision will he make and will it be in error if (a) he gets a sample mean of 10.2cm and $\mu$=9.6cm; (b) he gets a sample mean of 10.2cm and $\mu$=9.8cm; (c) he gets a sample mean of 9.2cm and $\mu$=9.6cm; (d) he gets a sample mean of 9.2cm and $\mu$=9.8cm. Does anyone have an idea how to deal with this problem? I just do not so understand what the problem asks for and how to approach it.  Many thanks!","I have a problem about the hypothesis testing and decision making as follows: A botanist wishes to test the null hypothesis that the average diameter of the flowers of a particular plant is 9.6cm. He decides to take a random sample of size $n$=80 and accept the null hypothesis if the mean of the sample falls between 9.3cm and 9.9cm; if the mean of this sample falls outside this interval, he will reject the null hypothesis. What decision will he make and will it be in error if (a) he gets a sample mean of 10.2cm and $\mu$=9.6cm; (b) he gets a sample mean of 10.2cm and $\mu$=9.8cm; (c) he gets a sample mean of 9.2cm and $\mu$=9.6cm; (d) he gets a sample mean of 9.2cm and $\mu$=9.8cm. Does anyone have an idea how to deal with this problem? I just do not so understand what the problem asks for and how to approach it.  Many thanks!",,['statistics']
83,"Basic terms for the elements of an observation, sample?","Basic terms for the elements of an observation, sample?",,"I'm trying to write a database schema for measurements in a variety of categories, and am having trouble coming up with names for some basic elements. Let's assume we are gathering heart rate and blood pressure measurements from a group of people. Each observation is composed of the individual being measured, the time the measurement was taken, and the amount of the measurement.  What are each of these called? What is the name for the type of measurement being taken (heart rate or blood pressure in this example)? The collection of individuals is the sample, correct?  If so, what is the term for the collection of observations? Please feel free to revise the question, as my lack of knowledge makes it hard to ask accurately.  I can come up with names myself, but after failing to find anything in some statistical glossaries, I'm curious if there are canonical names for these basic elements.","I'm trying to write a database schema for measurements in a variety of categories, and am having trouble coming up with names for some basic elements. Let's assume we are gathering heart rate and blood pressure measurements from a group of people. Each observation is composed of the individual being measured, the time the measurement was taken, and the amount of the measurement.  What are each of these called? What is the name for the type of measurement being taken (heart rate or blood pressure in this example)? The collection of individuals is the sample, correct?  If so, what is the term for the collection of observations? Please feel free to revise the question, as my lack of knowledge makes it hard to ask accurately.  I can come up with names myself, but after failing to find anything in some statistical glossaries, I'm curious if there are canonical names for these basic elements.",,"['statistics', 'terminology']"
84,How do you calculate IQR (interquartile range)?,How do you calculate IQR (interquartile range)?,,"I have the following data (ordered): $$0,	1,	1,	2,	3,	4,	4,	7,	9,	23.$$ As far as I know, $Q_1 \text{(median of the upper half)} = 1$; $Q_3 \text{(median of the lower half)} = 7$; Therefore, $\text{IQR} = Q_3-Q_1 = 6$. But when I boxplot this simple data in 'R', the summary says that, $Q_1 = 1.25$, $Q_3 = 6.25$, and consequently, $\text{IQR} = 6.25-1.25 = 5$!! How is that? I don't think we can question the statistical computation by ""R"" anyway ... Can anyone tell me what is wrong?","I have the following data (ordered): $$0,	1,	1,	2,	3,	4,	4,	7,	9,	23.$$ As far as I know, $Q_1 \text{(median of the upper half)} = 1$; $Q_3 \text{(median of the lower half)} = 7$; Therefore, $\text{IQR} = Q_3-Q_1 = 6$. But when I boxplot this simple data in 'R', the summary says that, $Q_1 = 1.25$, $Q_3 = 6.25$, and consequently, $\text{IQR} = 6.25-1.25 = 5$!! How is that? I don't think we can question the statistical computation by ""R"" anyway ... Can anyone tell me what is wrong?",,['statistics']
85,Is this a Permutation or a Combination?,Is this a Permutation or a Combination?,,"To win a lottery, you must pick the winning 3 numbers from the integers 1-9 (no repeat numbers). What is the probability of winning the lottery by choosing the correct 3 numbers? I think its a combination problem. I got 84 for the answer but I'm not sure.","To win a lottery, you must pick the winning 3 numbers from the integers 1-9 (no repeat numbers). What is the probability of winning the lottery by choosing the correct 3 numbers? I think its a combination problem. I got 84 for the answer but I'm not sure.",,"['combinatorics', 'statistics', 'permutations']"
86,What is Kendall Tau's co-efficient and Pearson co-efficient,What is Kendall Tau's co-efficient and Pearson co-efficient,,I came across these two terms in a paper about Natural Language Processing. So I looked both of them up on the net and couldn't understand a thing. So far I think their a method of comparing two quantities that have some kind of relationship with each other.Their used to determine how strong that relationship is but I don't quite understand how they work. Could someone explain this like I'm 5?,I came across these two terms in a paper about Natural Language Processing. So I looked both of them up on the net and couldn't understand a thing. So far I think their a method of comparing two quantities that have some kind of relationship with each other.Their used to determine how strong that relationship is but I don't quite understand how they work. Could someone explain this like I'm 5?,,['statistics']
87,Central limit theorem to approximate a Poisson distribution.,Central limit theorem to approximate a Poisson distribution.,,"Considerable controversy has arisen over the possible aftereffects of a nuclear weapons test conducted in Nevada in $1957$. Included as part of the test were some $3000$ military and civilian ""observers."" Now, more than $50$ years later, eight cases of leukemia have been diagnosed among those $3000$. The expected number of cases, based on the demographic characteristics of the observers, was three. Assess the statistical significance of those findings. Calculate both an exact answer using the Poisson distribution as well as an approximation based on the central limit theorem. I was able to calculate the answer using the Poisson distribution such that I found that $\frac{3}{3000}=p$ so $\lambda = (3000)(0.001)$ and then $P(X=8)=\frac{e^{-3}(3^8)}{8!}=0.008101$. (If I am incorrect please correct me.)  I am now having trouble using the central limit theorem to approximate.","Considerable controversy has arisen over the possible aftereffects of a nuclear weapons test conducted in Nevada in $1957$. Included as part of the test were some $3000$ military and civilian ""observers."" Now, more than $50$ years later, eight cases of leukemia have been diagnosed among those $3000$. The expected number of cases, based on the demographic characteristics of the observers, was three. Assess the statistical significance of those findings. Calculate both an exact answer using the Poisson distribution as well as an approximation based on the central limit theorem. I was able to calculate the answer using the Poisson distribution such that I found that $\frac{3}{3000}=p$ so $\lambda = (3000)(0.001)$ and then $P(X=8)=\frac{e^{-3}(3^8)}{8!}=0.008101$. (If I am incorrect please correct me.)  I am now having trouble using the central limit theorem to approximate.",,"['probability', 'statistics', 'probability-theory', 'probability-distributions']"
88,Probability distribution functions: factorization 3-way implies 2-way?,Probability distribution functions: factorization 3-way implies 2-way?,,"I recently asked a question about pairwise versus mutual independence (also related to this and this q). However, (1) I inadvertently used incorrect terminology: three events, A, B, C are mutually independent when: P[A,B]=P[A]P[B], P[B,C]=P[B]P[C], P[A,C]=P[A]P[C],   P[A,B,C]=P[A]P[B]P[C] Did and others pointed out that ""Mutual independence means the four identities you copied, pairwise   independence means the first three of these identities."" -- Did Note that the term mutual has varying definitions across math. For example, mutual information is a pairwise relation. (2) Going back to probability, GC Rota said the theory can be approached two ways: focusing on random variables (event algebra) or focusing on distributions. Here I am interested in distributions, where independence can be interpreted as factorization of the probability distribution function. The conditions are the same as above, where P is interpreted as the PDF function. The following graphic based on a standard example from Counterexamples in Probability and Statistics of a 3-dimensional binomial PDF that factorizes pairwise (ie, each of the 3 pairs of random variables are independent and the 2-dim joint distributions can all be written as the product of the respective marginals) but not 3-way independent (the joint distribution cannot be written as the product of the individual marginal distributions) My question is whether the opposite can happen, ie if the 3-dim (or perhaps higher) joint distribution factorizes into the 1-dim marginals, does that imply the pairwise factorization of all 2-dim joint distributions into the marginals?","I recently asked a question about pairwise versus mutual independence (also related to this and this q). However, (1) I inadvertently used incorrect terminology: three events, A, B, C are mutually independent when: P[A,B]=P[A]P[B], P[B,C]=P[B]P[C], P[A,C]=P[A]P[C],   P[A,B,C]=P[A]P[B]P[C] Did and others pointed out that ""Mutual independence means the four identities you copied, pairwise   independence means the first three of these identities."" -- Did Note that the term mutual has varying definitions across math. For example, mutual information is a pairwise relation. (2) Going back to probability, GC Rota said the theory can be approached two ways: focusing on random variables (event algebra) or focusing on distributions. Here I am interested in distributions, where independence can be interpreted as factorization of the probability distribution function. The conditions are the same as above, where P is interpreted as the PDF function. The following graphic based on a standard example from Counterexamples in Probability and Statistics of a 3-dimensional binomial PDF that factorizes pairwise (ie, each of the 3 pairs of random variables are independent and the 2-dim joint distributions can all be written as the product of the respective marginals) but not 3-way independent (the joint distribution cannot be written as the product of the individual marginal distributions) My question is whether the opposite can happen, ie if the 3-dim (or perhaps higher) joint distribution factorizes into the 1-dim marginals, does that imply the pairwise factorization of all 2-dim joint distributions into the marginals?",,"['probability', 'statistics', 'probability-distributions', 'terminology']"
89,"estimate population percentage within an interval, given a small sample","estimate population percentage within an interval, given a small sample",,"Given a small sample from a normally-distributed population, how  do I calculate the confidence that a specified percentage of the population is within some bounds [A,B]? To make it concrete, if I get a sample of [50,51,52] from a normally-distributed set, I should be able to calculate a fairly high confidence that 50% of the population lies within the range of 0-100, even with such a small sample. This is certainly related to the ""tolerance interval"", but differs in an important way. In all of the examples I can find for tolerance intervals, the required percentile and confidence is given, and the interval is found. In my problem, the interval and percentile are given, and I need to find the confidence. The relevant equation is this one: (Guttman 1970) $$1 - \gamma = P\left[P(X \geqq t_0) \geqq 1 - p\right] = P\left[T_{n-1}^*(\sqrt n z_p) \leqq \sqrt n K\right]$$ With definitions: $1 - \gamma$ is the confidence $n$ is the number of samples $100p$ is the percentage of the population required to be within the interval, as estimated from the sample mean and sample variance. $t_0 = x - K_{S, z_p}$ is the $(1 - p) 100$th percentile of the standard normal distribution $T_v^*(\delta)$ is the noncentral Students t distribution with $v$ degrees of freedom and noncentrality parameter $\delta$. This solves the one-sided problem, but I'm having trouble extending this to the two-sided problem. In confidence-interval land, I'd use the fact that $P(t_1 \leqq X \leqq t_2) = 1 - P(t_1 \gt X) - P(X \gt t_2)$, to break this into two one-sided problems, but in tolerance-interval land I need to relate these back to the confidence ($1-\gamma$), and I don't see how. $$1 - \gamma = P\left[P(t_1 \geqq X \geqq t_2) \geqq 1 - p\right] = ??? $$ If I attempt to turn this into two one-sided problems: $$1 - \gamma = P\left[1 - P(t_1 \lt X) - P(X \lt t_2) \geqq 1 - p\right] = ??? $$ And I'm utterly stuck there. I don't see how to relate this back to the one-sided tolerance interval solution. I'm not certain this is useful for people to understand the question, but it might, so I'm putting it in this addenda. In scipy, I'm able to pretty easily calculate $K$ given $p$ $\gamma$ and $n$ as: def K(p, gamma, n):     from scipy import stats     return stats.nct.ppf(1-gamma, n-1, sqrt(n) * stats.norm.ppf(1-p)) / sqrt(n) I'm also able to find $\gamma$ given $K$ $p$ and $n$ as: def gamma(p, n, K):     from scipy import stats                                                                                                   z_p = stats.norm.ppf(1-p)     return 1 - stats.nct.cdf(sqrt(n) * K, n-1, sqrt(n) * z_p) Much less important, but is this a valid simplification of the Guttman's formula? $$1 - \gamma = P\left[P(X \geqq t_0) \geqq 1 - p\right] = P\left[T_{n-1}^*(\sqrt n z_p) \leqq \sqrt n K\right]$$ $$\gamma = P\left[P(X \geqq t_0) \lt 1 - p\right] = P\left[T_{n-1}^*(\sqrt n z_p) \gt \sqrt n K\right]$$ $$\gamma = P\left[P(X \lt t_0) \lt p\right] = P\left[T_{n-1}^*(\sqrt n z_p) \gt \sqrt n K\right]$$ If so, this form seems way easier to understand, to me.","Given a small sample from a normally-distributed population, how  do I calculate the confidence that a specified percentage of the population is within some bounds [A,B]? To make it concrete, if I get a sample of [50,51,52] from a normally-distributed set, I should be able to calculate a fairly high confidence that 50% of the population lies within the range of 0-100, even with such a small sample. This is certainly related to the ""tolerance interval"", but differs in an important way. In all of the examples I can find for tolerance intervals, the required percentile and confidence is given, and the interval is found. In my problem, the interval and percentile are given, and I need to find the confidence. The relevant equation is this one: (Guttman 1970) $$1 - \gamma = P\left[P(X \geqq t_0) \geqq 1 - p\right] = P\left[T_{n-1}^*(\sqrt n z_p) \leqq \sqrt n K\right]$$ With definitions: $1 - \gamma$ is the confidence $n$ is the number of samples $100p$ is the percentage of the population required to be within the interval, as estimated from the sample mean and sample variance. $t_0 = x - K_{S, z_p}$ is the $(1 - p) 100$th percentile of the standard normal distribution $T_v^*(\delta)$ is the noncentral Students t distribution with $v$ degrees of freedom and noncentrality parameter $\delta$. This solves the one-sided problem, but I'm having trouble extending this to the two-sided problem. In confidence-interval land, I'd use the fact that $P(t_1 \leqq X \leqq t_2) = 1 - P(t_1 \gt X) - P(X \gt t_2)$, to break this into two one-sided problems, but in tolerance-interval land I need to relate these back to the confidence ($1-\gamma$), and I don't see how. $$1 - \gamma = P\left[P(t_1 \geqq X \geqq t_2) \geqq 1 - p\right] = ??? $$ If I attempt to turn this into two one-sided problems: $$1 - \gamma = P\left[1 - P(t_1 \lt X) - P(X \lt t_2) \geqq 1 - p\right] = ??? $$ And I'm utterly stuck there. I don't see how to relate this back to the one-sided tolerance interval solution. I'm not certain this is useful for people to understand the question, but it might, so I'm putting it in this addenda. In scipy, I'm able to pretty easily calculate $K$ given $p$ $\gamma$ and $n$ as: def K(p, gamma, n):     from scipy import stats     return stats.nct.ppf(1-gamma, n-1, sqrt(n) * stats.norm.ppf(1-p)) / sqrt(n) I'm also able to find $\gamma$ given $K$ $p$ and $n$ as: def gamma(p, n, K):     from scipy import stats                                                                                                   z_p = stats.norm.ppf(1-p)     return 1 - stats.nct.cdf(sqrt(n) * K, n-1, sqrt(n) * z_p) Much less important, but is this a valid simplification of the Guttman's formula? $$1 - \gamma = P\left[P(X \geqq t_0) \geqq 1 - p\right] = P\left[T_{n-1}^*(\sqrt n z_p) \leqq \sqrt n K\right]$$ $$\gamma = P\left[P(X \geqq t_0) \lt 1 - p\right] = P\left[T_{n-1}^*(\sqrt n z_p) \gt \sqrt n K\right]$$ $$\gamma = P\left[P(X \lt t_0) \lt p\right] = P\left[T_{n-1}^*(\sqrt n z_p) \gt \sqrt n K\right]$$ If so, this form seems way easier to understand, to me.",,"['statistics', 'probability-distributions', 'sampling']"
90,"Class Limits, boundaries, midpoint, relative frequency?","Class Limits, boundaries, midpoint, relative frequency?",,"I have a list: 85 45 75 60 90 90 115 30 55 58 78 120 80 65 65 140 65 50 30 125 75 137 80 120 15 45 70 65 50 45 95 70 70 28 40 125 105 75 80 70 90 68 73 75 55 70 95 65 200 75 15 90 46 33 100 65 60 55 85 50 10 68 99 145 45 75 45 95 85 65 65 52 82 Sorry for the poor formatting, but I created a program that would count the frequencies, etc. and I still am getting the problem wrong. The class limits make sense to me since the smallest value is 10 and you just need to add 28 (the class width), and the next should be 48, right? Or am I completely off base? Class boundaries will make sense once I get the right values for the class limits. The midpoint should be the lower limit + the upper, so wouldn't the midpoint of (10 + 38)/2 be 24?? And relative frequency is just the frequency divided by the total frequencies, right? Thanks for your help :/","I have a list: 85 45 75 60 90 90 115 30 55 58 78 120 80 65 65 140 65 50 30 125 75 137 80 120 15 45 70 65 50 45 95 70 70 28 40 125 105 75 80 70 90 68 73 75 55 70 95 65 200 75 15 90 46 33 100 65 60 55 85 50 10 68 99 145 45 75 45 95 85 65 65 52 82 Sorry for the poor formatting, but I created a program that would count the frequencies, etc. and I still am getting the problem wrong. The class limits make sense to me since the smallest value is 10 and you just need to add 28 (the class width), and the next should be 48, right? Or am I completely off base? Class boundaries will make sense once I get the right values for the class limits. The midpoint should be the lower limit + the upper, so wouldn't the midpoint of (10 + 38)/2 be 24?? And relative frequency is just the frequency divided by the total frequencies, right? Thanks for your help :/",,['statistics']
91,Gumbel distribution and expected values,Gumbel distribution and expected values,,"Suppose that $V= max_i${$V_i+\epsilon_i $} where $i=1...N$ and each $epsilon_i$ has a a type 1 extreme value distribution with $F(x)=e^{e^{-x}}$ I need to show that $E[V]=u+ln(\sum_je^{V_j})$ where u is eulers constant, i.e. $E[\epsilon_i]$ I have so far that  P(option i is picked)=$\frac{e^{V_i}}{\sum_je^{V_j}}$ (This seems to be a standard result, so I'm fairly sure of it) Using this formula, I get that: $E[V] = u +\sum_i\frac{e^{V_i}}{\sum_je^{V_j}}V_i $ and then I dont know how to proceed. EDIT: Ok I realize that my formula for E[V] is incorrect, it should be $E[V] = \sum_i\frac{e^{V_i}}{\sum_je^{V_j}}(V_i+E[\epsilon_i|V_i+\epsilon_i>V_j+\epsilon_j ]  $ for all $j\neq i$","Suppose that $V= max_i${$V_i+\epsilon_i $} where $i=1...N$ and each $epsilon_i$ has a a type 1 extreme value distribution with $F(x)=e^{e^{-x}}$ I need to show that $E[V]=u+ln(\sum_je^{V_j})$ where u is eulers constant, i.e. $E[\epsilon_i]$ I have so far that  P(option i is picked)=$\frac{e^{V_i}}{\sum_je^{V_j}}$ (This seems to be a standard result, so I'm fairly sure of it) Using this formula, I get that: $E[V] = u +\sum_i\frac{e^{V_i}}{\sum_je^{V_j}}V_i $ and then I dont know how to proceed. EDIT: Ok I realize that my formula for E[V] is incorrect, it should be $E[V] = \sum_i\frac{e^{V_i}}{\sum_je^{V_j}}(V_i+E[\epsilon_i|V_i+\epsilon_i>V_j+\epsilon_j ]  $ for all $j\neq i$",,"['statistics', 'probability-distributions', 'discrete-mathematics']"
92,Bounding the variance of an unbiased estimator for a uniform-distribution parameter,Bounding the variance of an unbiased estimator for a uniform-distribution parameter,,"$X_1,\ldots,X_6$ is a sample from a uniform distribution $ \left[ 0, \theta \right] $, $\theta$ is $[1,2]$. Find an unbiased estimator for $\theta$ with variance less than $\dfrac{1}{10}$. I thought the M.L.E is  $\max \left( X_i \right) $,and the unbiased estimator without other restriction shoud be $\hat\theta_N=\dfrac{N+1}N\max(X_i)$, (N=6). But, I have no idea how to make the variance be less than $\dfrac{1}{10}$. I know that $$ \mathrm{Var}\left(\hat\theta\right) = \theta^2\dfrac{N}{(N+1)^2(N+2)}\,, $$  so $\mathrm{Var}\left(\hat\theta\dfrac{N+1}{N}\right) = \dfrac{\theta^2}{(N+2)N}$.","$X_1,\ldots,X_6$ is a sample from a uniform distribution $ \left[ 0, \theta \right] $, $\theta$ is $[1,2]$. Find an unbiased estimator for $\theta$ with variance less than $\dfrac{1}{10}$. I thought the M.L.E is  $\max \left( X_i \right) $,and the unbiased estimator without other restriction shoud be $\hat\theta_N=\dfrac{N+1}N\max(X_i)$, (N=6). But, I have no idea how to make the variance be less than $\dfrac{1}{10}$. I know that $$ \mathrm{Var}\left(\hat\theta\right) = \theta^2\dfrac{N}{(N+1)^2(N+2)}\,, $$  so $\mathrm{Var}\left(\hat\theta\dfrac{N+1}{N}\right) = \dfrac{\theta^2}{(N+2)N}$.",,['statistics']
93,gambling probability problem,gambling probability problem,,"We are given a fair coin. We start out with 5 dollars. We keep tossing the coin. If the outcome is different than the previous one, we are awarded another 5 dollars. However, we do not get anything if the outcome is the same as the previous one. Let's say we toss the coin X times in the long run. How much do we expect to have in the end?","We are given a fair coin. We start out with 5 dollars. We keep tossing the coin. If the outcome is different than the previous one, we are awarded another 5 dollars. However, we do not get anything if the outcome is the same as the previous one. Let's say we toss the coin X times in the long run. How much do we expect to have in the end?",,"['probability', 'statistics', 'probability-theory']"
94,Can I use ANOVA when I have negative values in my data?,Can I use ANOVA when I have negative values in my data?,,I was trying to analyze a data with negative values. Is it possible to use ANOVA in this case?,I was trying to analyze a data with negative values. Is it possible to use ANOVA in this case?,,['statistics']
95,What is the difference between two variables being proportional versus being directly proportional? [duplicate],What is the difference between two variables being proportional versus being directly proportional? [duplicate],,"This question already has answers here : What is the difference between proportional and directly proportional in differential equations? (3 answers) Closed 12 months ago . I hear these expressions being thrown around, and realize that ""proportional"" may also incorporate inverse proportion, but are there any other differences?","This question already has answers here : What is the difference between proportional and directly proportional in differential equations? (3 answers) Closed 12 months ago . I hear these expressions being thrown around, and realize that ""proportional"" may also incorporate inverse proportion, but are there any other differences?",,"['statistics', 'terminology']"
96,Regression with arbitrary norm,Regression with arbitrary norm,,"I have a function $f(X,Y,Z)$, which I know is polynomial of degree $3$. I have a set of samples $(X_i,Y_i,Z_i)$ and corresponding values of $f$. My task is to find (the best approximation of) the coefficients of the polynomial with the data in hand. I can do this with least-squares approximation in Matlab but the problem is that least-squares minimizes the Euclidean distance from points to the model (i.e. the error) and I need to use totally different norm for the error than Euclidean distance. How should I approach this? In a high level the polynomial handles input and output in some space $A$ but I should choose the coefficients in such a way that the distance (error) in space $B$ would be minimal. The transformation between $A$ and $B$ is not linear, but it doesn't look too complicated (I haven't figured out all the details yet). If it matters, $X$, $Y$ and $Z$ are in the range $[0,255]$ (in $A$).","I have a function $f(X,Y,Z)$, which I know is polynomial of degree $3$. I have a set of samples $(X_i,Y_i,Z_i)$ and corresponding values of $f$. My task is to find (the best approximation of) the coefficients of the polynomial with the data in hand. I can do this with least-squares approximation in Matlab but the problem is that least-squares minimizes the Euclidean distance from points to the model (i.e. the error) and I need to use totally different norm for the error than Euclidean distance. How should I approach this? In a high level the polynomial handles input and output in some space $A$ but I should choose the coefficients in such a way that the distance (error) in space $B$ would be minimal. The transformation between $A$ and $B$ is not linear, but it doesn't look too complicated (I haven't figured out all the details yet). If it matters, $X$, $Y$ and $Z$ are in the range $[0,255]$ (in $A$).",,['statistics']
97,"Algebraic manipulation of normal, $\chi^2$ and Gamma probability distributions","Algebraic manipulation of normal,  and Gamma probability distributions",\chi^2,"If $X_1, \ldots, X_n \sim N(\mu, \sigma^2)$, then  $$ \frac{n - 1}{\sigma^2}S^2 \sim \chi^2_{n - 1} $$  where $S^2 = \frac{1}{n-1}\sum_{i=1}^n (x_i^2- \bar{x})^2$, and there's a direct relationship between the $\chi^2_p$ and Gamma($\alpha, \beta$) distributions: $$ \chi^2_{n - 1} = \text{Gamma}(\tfrac{n-1}{2}, 2). $$ But then why is $$ S^2 \sim \text{Gamma}(\tfrac{n-1}{2}, \tfrac{2\sigma^2}{n-1}) \,? $$  And why do we multiply the reciprocal with $\beta$ and not $\alpha$?  Is it because $\beta$ is the scale parameter? In general, are there methods for algebraic manipulation around the ""$\sim$"" other than the standard transformation procedures?  Something that uses the properties of location/scale families perhaps?","If $X_1, \ldots, X_n \sim N(\mu, \sigma^2)$, then  $$ \frac{n - 1}{\sigma^2}S^2 \sim \chi^2_{n - 1} $$  where $S^2 = \frac{1}{n-1}\sum_{i=1}^n (x_i^2- \bar{x})^2$, and there's a direct relationship between the $\chi^2_p$ and Gamma($\alpha, \beta$) distributions: $$ \chi^2_{n - 1} = \text{Gamma}(\tfrac{n-1}{2}, 2). $$ But then why is $$ S^2 \sim \text{Gamma}(\tfrac{n-1}{2}, \tfrac{2\sigma^2}{n-1}) \,? $$  And why do we multiply the reciprocal with $\beta$ and not $\alpha$?  Is it because $\beta$ is the scale parameter? In general, are there methods for algebraic manipulation around the ""$\sim$"" other than the standard transformation procedures?  Something that uses the properties of location/scale families perhaps?",,"['statistics', 'probability-distributions', 'gamma-distribution', 'chi-squared']"
98,Are correlated univariate normals equivalent to a bivariate normal?,Are correlated univariate normals equivalent to a bivariate normal?,,"If I know that $A \sim \mathcal N(0,\sigma^2_A)$ and $B \sim \mathcal N(0,\sigma^2_B)$ and that $A$ and $B$ have a correlation coefficient of $\rho$, does this mean that it must be that case that $\begin{bmatrix} A \\ B \end{bmatrix} \sim  \mathcal N\left( \begin{bmatrix} 0 \\ 0 \end{bmatrix}, \begin{bmatrix} \sigma^2_A & \rho \sigma_A \sigma_B \\ \rho \sigma_A \sigma_B & \sigma_B^2    \end{bmatrix} \right) \>?$ Or is that just one possibility? And what about for other distributions?","If I know that $A \sim \mathcal N(0,\sigma^2_A)$ and $B \sim \mathcal N(0,\sigma^2_B)$ and that $A$ and $B$ have a correlation coefficient of $\rho$, does this mean that it must be that case that $\begin{bmatrix} A \\ B \end{bmatrix} \sim  \mathcal N\left( \begin{bmatrix} 0 \\ 0 \end{bmatrix}, \begin{bmatrix} \sigma^2_A & \rho \sigma_A \sigma_B \\ \rho \sigma_A \sigma_B & \sigma_B^2    \end{bmatrix} \right) \>?$ Or is that just one possibility? And what about for other distributions?",,"['probability', 'statistics', 'probability-distributions']"
99,Estimation with method of maximum likelihood,Estimation with method of maximum likelihood,,"Can anybody help me to generate the estimator of equation: $$Y_i = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2}+\cdots+\beta_4X_{i4}+\varepsilon_i$$ using method of maximum likelihood, where $\varepsilon_i$ are independent variables which have normal distribution $N(0,\sigma^2)$","Can anybody help me to generate the estimator of equation: $$Y_i = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2}+\cdots+\beta_4X_{i4}+\varepsilon_i$$ using method of maximum likelihood, where $\varepsilon_i$ are independent variables which have normal distribution $N(0,\sigma^2)$",,['statistics']
