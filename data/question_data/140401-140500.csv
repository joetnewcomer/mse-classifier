,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Verify solution to ODE,Verify solution to ODE,,"I am given the ODE $$\left(f''(x)+\frac{f'(x)}{x} \right) \left(1+f'(x)^2 \right) = f'(x)^2f''(x)$$ and I already know that the solution to this ODE is given by $$f(x)= c  \cdot arcosh \left( \frac{r}{c} \right) + d$$ where $|c|<r$ and $d \in \mathbb{R}.$ The problem is I want to show that this is an actual solution by direct integration (so I want to derive it) and not just verify it by plugging it in. Does anybody know how this can be done? After rearraging as Daniel Fischer proposed, I end up with $$f''(r) = -\frac{(f'(r)^3+f'(r))}{r}$$","I am given the ODE $$\left(f''(x)+\frac{f'(x)}{x} \right) \left(1+f'(x)^2 \right) = f'(x)^2f''(x)$$ and I already know that the solution to this ODE is given by $$f(x)= c  \cdot arcosh \left( \frac{r}{c} \right) + d$$ where $|c|<r$ and $d \in \mathbb{R}.$ The problem is I want to show that this is an actual solution by direct integration (so I want to derive it) and not just verify it by plugging it in. Does anybody know how this can be done? After rearraging as Daniel Fischer proposed, I end up with $$f''(r) = -\frac{(f'(r)^3+f'(r))}{r}$$",,"['real-analysis', 'integration', 'analysis', 'ordinary-differential-equations']"
1,Poincaré lemma and conservative vector fields,Poincaré lemma and conservative vector fields,,"Let $U$ be some contractible neighbourhood of $0\in\mathbb{R}^n$ and let $X=\sum_{i=1}^nX_i\frac{\partial}{\partial x_i}$ be a (smooth) vector field on $U$. This vector field can be thought as a differential 1-form $X=\sum_{i=1}^nX_i\,dx_i$ on $U$. By Poincaré's lemma, ""every closed form is exact"". Does this mean that ""every vector field on $U$ is conservative (due to exactness) and satisfies $\frac{\partial X_i}{\partial x_j}=\frac{\partial X_j}{\partial x_i}$ (due to closeness)""? Then, should I interpret this as ""every v.f. on $U$ is the gradient of a potential function (0-form) $V:U\to\mathbb{R}$"" It seems to me that the word every is too strong. So, is my interpretation incomplete? What about non-conservative vector fields? Are they ""always"" defined in non-contractible domains?","Let $U$ be some contractible neighbourhood of $0\in\mathbb{R}^n$ and let $X=\sum_{i=1}^nX_i\frac{\partial}{\partial x_i}$ be a (smooth) vector field on $U$. This vector field can be thought as a differential 1-form $X=\sum_{i=1}^nX_i\,dx_i$ on $U$. By Poincaré's lemma, ""every closed form is exact"". Does this mean that ""every vector field on $U$ is conservative (due to exactness) and satisfies $\frac{\partial X_i}{\partial x_j}=\frac{\partial X_j}{\partial x_i}$ (due to closeness)""? Then, should I interpret this as ""every v.f. on $U$ is the gradient of a potential function (0-form) $V:U\to\mathbb{R}$"" It seems to me that the word every is too strong. So, is my interpretation incomplete? What about non-conservative vector fields? Are they ""always"" defined in non-contractible domains?",,"['ordinary-differential-equations', 'differential-geometry', 'dynamical-systems']"
2,Is $\exp$ the only function satisfying $f(x)=\displaystyle \int_{-x}^{+\infty} f(-t) dt$?,Is  the only function satisfying ?,\exp f(x)=\displaystyle \int_{-x}^{+\infty} f(-t) dt,"Today in class we first dealt with improper integrals, and as an example we found $ \displaystyle \int_0 ^{+\infty} e^{-x}dx=1$. Soon, I noticed that in fact $$e^x=\int_{-x}^{+\infty}e^{-t}dt. $$ Since I already know $\exp$ can be defined as the unique function such that $f'(x)=f(x)$ and $f(0)=1$, I wondered whether this might be yet another way of defining it. In search of counterexamples, I rewrote the equation $$f(x) = \int_{-x}^{+\infty}f(-t)dt $$ as $$\lim_{x\to-\infty}f(x)=f(x)-f'(x),$$ but still couldn't think of other solutions (besides $c \cdot e^x$ ). If there aren't, how to prove it? (this question arised from a terribly silly oversight)","Today in class we first dealt with improper integrals, and as an example we found $ \displaystyle \int_0 ^{+\infty} e^{-x}dx=1$. Soon, I noticed that in fact $$e^x=\int_{-x}^{+\infty}e^{-t}dt. $$ Since I already know $\exp$ can be defined as the unique function such that $f'(x)=f(x)$ and $f(0)=1$, I wondered whether this might be yet another way of defining it. In search of counterexamples, I rewrote the equation $$f(x) = \int_{-x}^{+\infty}f(-t)dt $$ as $$\lim_{x\to-\infty}f(x)=f(x)-f'(x),$$ but still couldn't think of other solutions (besides $c \cdot e^x$ ). If there aren't, how to prove it? (this question arised from a terribly silly oversight)",,"['integration', 'ordinary-differential-equations', 'improper-integrals', 'exponential-function']"
3,Finding radius of convergence without cauchy ratio test,Finding radius of convergence without cauchy ratio test,,"I am currently taking a differential equations course and we are learning about second order homogeneous equations and finding the general solution using series. I noticed during lecture, that sometimes my professor is able to find the radius of convergence without using cauchy ratio test. For example, I noted down this equation: $$ y''-2ty'+\lambda y=0$$ where $\lambda$ is a fixed constant. My professor said $2t$ and $\lambda$ are both polynomials, so the equation converges everywhere. Keep in mind that my professor speaks a bit fast, so I could have mistaken what he said above. Also, I would ask my professor but he does not like answering questions. There are times where my professor does use the ratio test but my question is: How can we determine the radius of convergence without using the cauchy ratio test? I know there shall be times where you have to use the ratio test but I would like to know the cases in which you can determine the radius of convergence by just looking at the equation.","I am currently taking a differential equations course and we are learning about second order homogeneous equations and finding the general solution using series. I noticed during lecture, that sometimes my professor is able to find the radius of convergence without using cauchy ratio test. For example, I noted down this equation: $$ y''-2ty'+\lambda y=0$$ where $\lambda$ is a fixed constant. My professor said $2t$ and $\lambda$ are both polynomials, so the equation converges everywhere. Keep in mind that my professor speaks a bit fast, so I could have mistaken what he said above. Also, I would ask my professor but he does not like answering questions. There are times where my professor does use the ratio test but my question is: How can we determine the radius of convergence without using the cauchy ratio test? I know there shall be times where you have to use the ratio test but I would like to know the cases in which you can determine the radius of convergence by just looking at the equation.",,['ordinary-differential-equations']
4,Number of Real roots of cubic,Number of Real roots of cubic,,"I just have a quick question about a polynomial and its roots, For example, I was solving the differential equation $$\frac{dy}{dx}=\frac{3x^2+4x+2}{2y-2}$$ I solved it using the basic methods of separation and solved the solution implicitly as $y^2-2y=(x^3+2x^2+2x+D)$,  $D \in \mathbb{R}$ so when I want to solve for $y$ explicitly, I solve the quadratic and get the form, $$y=1\pm\sqrt{1+(x^3+2x^2+2x+D)}$$ Now, my question is in regard to the cubic $\mathbb{P}=x^3+2x^2+2x+1+D$ My book noted without explanation that , $\mathbb{P}$ is such that for all $D \in \mathbb{R}$, only one real root exists, call it $r(D)$. My question is, how was this seen so easily? I think there might be something very simple I am missing potentially. I hope someone can help with the explanation, thanks!","I just have a quick question about a polynomial and its roots, For example, I was solving the differential equation $$\frac{dy}{dx}=\frac{3x^2+4x+2}{2y-2}$$ I solved it using the basic methods of separation and solved the solution implicitly as $y^2-2y=(x^3+2x^2+2x+D)$,  $D \in \mathbb{R}$ so when I want to solve for $y$ explicitly, I solve the quadratic and get the form, $$y=1\pm\sqrt{1+(x^3+2x^2+2x+D)}$$ Now, my question is in regard to the cubic $\mathbb{P}=x^3+2x^2+2x+1+D$ My book noted without explanation that , $\mathbb{P}$ is such that for all $D \in \mathbb{R}$, only one real root exists, call it $r(D)$. My question is, how was this seen so easily? I think there might be something very simple I am missing potentially. I hope someone can help with the explanation, thanks!",,"['ordinary-differential-equations', 'polynomials']"
5,Solving an SDE: $dX=-Xdt+e^{-t}dW$,Solving an SDE:,dX=-Xdt+e^{-t}dW,"I have the following problem which comes with the solution, but I am unable to obtain the solution... Any help would be greatly appreciated - I am preparing for finals :( Thanks a lot! The SDE that I need to solve is $dX=-Xdt+e^{-t}dW$. The solution is $X(t)=(X_0+W(t))e^{-t}$ I have noticed that  $$dW=e^{t}dX+e^{t}Xdt=d(e^tX)$$ So I have tried with $Z=e^tX$ and get $dZ=2e^t(dX+X)$, but I am stuck there now :/ (and I am not sure if that $dZ$ is correct either because we are not shown how the professor got the final result).","I have the following problem which comes with the solution, but I am unable to obtain the solution... Any help would be greatly appreciated - I am preparing for finals :( Thanks a lot! The SDE that I need to solve is $dX=-Xdt+e^{-t}dW$. The solution is $X(t)=(X_0+W(t))e^{-t}$ I have noticed that  $$dW=e^{t}dX+e^{t}Xdt=d(e^tX)$$ So I have tried with $Z=e^tX$ and get $dZ=2e^t(dX+X)$, but I am stuck there now :/ (and I am not sure if that $dZ$ is correct either because we are not shown how the professor got the final result).",,"['ordinary-differential-equations', 'stochastic-differential-equations']"
6,find $\lambda$ such that the integral has a solution.,find  such that the integral has a solution.,\lambda,I have the integral equation: $u(x) = f(x) + \lambda \int_0^{\frac{1}{2}}u(y)dy$ I have to find $\lambda$ such that the integral has a solution. How to approach such problems?,I have the integral equation: $u(x) = f(x) + \lambda \int_0^{\frac{1}{2}}u(y)dy$ I have to find $\lambda$ such that the integral has a solution. How to approach such problems?,,"['calculus', 'linear-algebra', 'integration', 'ordinary-differential-equations', 'operator-theory']"
7,Show that all solutions remain in the interval for all time,Show that all solutions remain in the interval for all time,,"I really have no idea on how to get started with these, there's no similar example in my book. Do I need to compute $\frac{dy}{dx}$? Any help would be greatly appreciated. Maybe there's just some theorem I am not aware of Show that all solutions $x(t),y(t)$ which start inside the given interval ($x>0,y>0$ for part a,$x>0$ for part b, $x^2+y^2=1$ for part c) must remain there for all time: a. $$\frac{dx}{dt}=x^2+ysinx$$ $$\frac{dy}{dt}=-1+xy+cosy$$ $$x>0,y>0$$ 1st attempt: $\frac{dy}{dx}=\frac{-1+xy+cosy}{x^2+ysinx}$ I can't remember how to solve for this since it's not separable b. $$\frac{dx}{dt}=y(e^x-1)$$ $$\frac{dy}{dt}=x+e^y$$ $$x>0$$ 1st attempt: $\frac{dy}{dx}=\frac{x+e^y}{y(e^x-1)}$ c. $$\frac{dx}{dt}=-1-y+x^2$$ $$\frac{dy}{dt}=x+xy$$ $$x^2+y^2=1$$ 1st attempt:  I just substituted...I don't think this is right though $\frac{dx}{dt}=-1-y+x^2$ $\frac{d(x^2+y^2)}{dt}=1-y+(x^2+y^2)^2$ $\frac{dy}{dt}=x+xy$ $\frac{d(x^2+y^2)}{dt}=x+x(x^2+y^2)$ hint: compute $\frac{d(x^2+y^2)}{dt}$","I really have no idea on how to get started with these, there's no similar example in my book. Do I need to compute $\frac{dy}{dx}$? Any help would be greatly appreciated. Maybe there's just some theorem I am not aware of Show that all solutions $x(t),y(t)$ which start inside the given interval ($x>0,y>0$ for part a,$x>0$ for part b, $x^2+y^2=1$ for part c) must remain there for all time: a. $$\frac{dx}{dt}=x^2+ysinx$$ $$\frac{dy}{dt}=-1+xy+cosy$$ $$x>0,y>0$$ 1st attempt: $\frac{dy}{dx}=\frac{-1+xy+cosy}{x^2+ysinx}$ I can't remember how to solve for this since it's not separable b. $$\frac{dx}{dt}=y(e^x-1)$$ $$\frac{dy}{dt}=x+e^y$$ $$x>0$$ 1st attempt: $\frac{dy}{dx}=\frac{x+e^y}{y(e^x-1)}$ c. $$\frac{dx}{dt}=-1-y+x^2$$ $$\frac{dy}{dt}=x+xy$$ $$x^2+y^2=1$$ 1st attempt:  I just substituted...I don't think this is right though $\frac{dx}{dt}=-1-y+x^2$ $\frac{d(x^2+y^2)}{dt}=1-y+(x^2+y^2)^2$ $\frac{dy}{dt}=x+xy$ $\frac{d(x^2+y^2)}{dt}=x+x(x^2+y^2)$ hint: compute $\frac{d(x^2+y^2)}{dt}$",,"['calculus', 'analysis', 'ordinary-differential-equations']"
8,Wave Equation Partial Differential Equation,Wave Equation Partial Differential Equation,,"Basically I got a simple wave equation with an extra twist. The PDE is $$\frac {\partial^2 y}{\partial t^2} = c^2\frac {\partial^2 y}{\partial x^2} + L$$ with homogeneous boundary condition. As usual, I use the ansatz $ Y(x,t) = F(x)G(t) $ and I have $\frac {\partial^2 y}{\partial t^2} =F''G $ and $\frac {\partial^2 y}{\partial x^2} =FG''$ . I substitute this in, and now I have $$FG''= c^2F''G +L$$ I don't know how to make this separable--obviously if I divide both sides with $FG$ as usual, then I will have that annoying constant $\frac {L}{FG} $ and I don't know how to make it separable then. Can anybody help me? I know I have to separate it somehow, but I don't know exactly how to. {The actual problem is an inhomogeneous boundary condition type of problem, but I have found the steady state solution, so I just have to find the transient solution, thus the homogeneous boundary condition} The actual question: PDE: $$\frac {\partial^2 y}{\partial t^2} = c^2\frac {\partial^2 y}{\partial x^2} + L,\quad 0 \leq x \leq J,\quad t>0$$ where $L$ is constant and $c$ is constant wave speed. Boundary Condition: $ u(0,t)=0$ , $t>0$ and $u(J,t)=h$ , $t>0$ Initial Condition: $u(x,0) = 0$ , $0<x<J$ and $\frac {\partial u}{\partial t} (x,0)=0$ , $0<x<J$ I did the steady state solution bit, and got it as $w(x)= \frac{-Lx^2}{c} + \frac {Hx}{L} +\frac{GLx}{c} $ . So I modified the boundary condition and initial condition to get the transient part, and I got the boundary condition homogeneous. However when I try to solve it, the question above arises.","Basically I got a simple wave equation with an extra twist. The PDE is with homogeneous boundary condition. As usual, I use the ansatz and I have and . I substitute this in, and now I have I don't know how to make this separable--obviously if I divide both sides with as usual, then I will have that annoying constant and I don't know how to make it separable then. Can anybody help me? I know I have to separate it somehow, but I don't know exactly how to. {The actual problem is an inhomogeneous boundary condition type of problem, but I have found the steady state solution, so I just have to find the transient solution, thus the homogeneous boundary condition} The actual question: PDE: where is constant and is constant wave speed. Boundary Condition: , and , Initial Condition: , and , I did the steady state solution bit, and got it as . So I modified the boundary condition and initial condition to get the transient part, and I got the boundary condition homogeneous. However when I try to solve it, the question above arises.","\frac {\partial^2 y}{\partial t^2} = c^2\frac {\partial^2 y}{\partial x^2} + L  Y(x,t) = F(x)G(t)  \frac {\partial^2 y}{\partial t^2} =F''G  \frac {\partial^2 y}{\partial x^2} =FG'' FG''= c^2F''G +L FG \frac {L}{FG}  \frac {\partial^2 y}{\partial t^2} = c^2\frac {\partial^2 y}{\partial x^2} + L,\quad 0 \leq x \leq J,\quad t>0 L c  u(0,t)=0 t>0 u(J,t)=h t>0 u(x,0) = 0 0<x<J \frac {\partial u}{\partial t} (x,0)=0 0<x<J w(x)= \frac{-Lx^2}{c} + \frac {Hx}{L} +\frac{GLx}{c} ","['ordinary-differential-equations', 'partial-differential-equations', 'wave-equation']"
9,What kind of differential equation is $(x^2+2y^3)y'=xy$?,What kind of differential equation is ?,(x^2+2y^3)y'=xy,"what kind of differential equation is $(x^2+2y^3)y'=xy$?, I think its an inexact differential first order, its surely not linear, I tried to check if its separable also didn't work, its not also homogeneous. and I thought about a way of solving the equation by saying that $\frac{dx}{dy} = \frac{x}{y} + \frac{2y^2}{x} $ , and I succeeded at finding $u(y)$ at this case but got stuck at calculating: $\int \frac{2y}{x} $ at some point, does it have to be according to  $y$ or $x$?, I would be glad for some clarifications.","what kind of differential equation is $(x^2+2y^3)y'=xy$?, I think its an inexact differential first order, its surely not linear, I tried to check if its separable also didn't work, its not also homogeneous. and I thought about a way of solving the equation by saying that $\frac{dx}{dy} = \frac{x}{y} + \frac{2y^2}{x} $ , and I succeeded at finding $u(y)$ at this case but got stuck at calculating: $\int \frac{2y}{x} $ at some point, does it have to be according to  $y$ or $x$?, I would be glad for some clarifications.",,"['calculus', 'ordinary-differential-equations']"
10,Impulse function and the laplace transform,Impulse function and the laplace transform,,How do I get to the following inverse Laplace transform? $$\mathcal{L}^{-1}\left\{e^{-5s} \cdot \frac 1{s+1}\right\}=u_5(t)e^{-(t-5)} \; ?$$ Here $u_5$ is a step function. I'm using my Laplace Transform table and just can't make any sense of this. Thanks for any help!,How do I get to the following inverse Laplace transform? $$\mathcal{L}^{-1}\left\{e^{-5s} \cdot \frac 1{s+1}\right\}=u_5(t)e^{-(t-5)} \; ?$$ Here $u_5$ is a step function. I'm using my Laplace Transform table and just can't make any sense of this. Thanks for any help!,,['ordinary-differential-equations']
11,The reasoning behind variation of parameters.,The reasoning behind variation of parameters.,,"Let's say you have the second order equation: $y''+p(x)y'+q(x)y=f(x)$ And let's say you have found two solutions ($y_1$ and $y_2$) to the homogeneous equation: $y''+p(x)y'+q(x)y=0$. Then the method using variation of parameters says that we should try a function: $y_p=u_1(x)y_1(x)+u_2(x)y_2(x)$. Now my books says: Since we have two unknown functions, but we have only one requirement(that $y_p$ satisfies our differential equation), we impose one more condition annd that condition is that $u_1'y_1+u_2'y_2=0$. But why this second condition. Is there any theory that says if you are going to find two functions, you need two conditions, and then you are ok? I mean, if I could just impose conditions as I wanted I would much rather choose that either $u_1=0$ or $u_2=0$?","Let's say you have the second order equation: $y''+p(x)y'+q(x)y=f(x)$ And let's say you have found two solutions ($y_1$ and $y_2$) to the homogeneous equation: $y''+p(x)y'+q(x)y=0$. Then the method using variation of parameters says that we should try a function: $y_p=u_1(x)y_1(x)+u_2(x)y_2(x)$. Now my books says: Since we have two unknown functions, but we have only one requirement(that $y_p$ satisfies our differential equation), we impose one more condition annd that condition is that $u_1'y_1+u_2'y_2=0$. But why this second condition. Is there any theory that says if you are going to find two functions, you need two conditions, and then you are ok? I mean, if I could just impose conditions as I wanted I would much rather choose that either $u_1=0$ or $u_2=0$?",,['ordinary-differential-equations']
12,Are the solutions of a Sturm-Liouville equation entire in the spectral parameter?,Are the solutions of a Sturm-Liouville equation entire in the spectral parameter?,,"In $[1]$ the following (paraphrased) claim is made: Let $q\in L^1_{loc}([0,\infty);\mathbb{R})$, and suppose $\varphi$ and $\theta$ solve the one-dimensional Schrödinger equation    \begin{equation}       -u''(x;\lambda)+q(x)u(x;\lambda)=\lambda u(x;\lambda)~~~(x\in[0,\infty)),    \end{equation} subject to    \begin{equation}       \left\{\begin{array}{cc}          \varphi(0;\lambda)=0, &\varphi'(0;\lambda)=1, \\          \theta(0;\lambda)=1,  &\theta'(0;\lambda)=0.       \end{array}\right.    \end{equation} Then, for each fixed $x$, we have that $\varphi(x;\cdot)$ and $\theta(x;\cdot)$ are entire functions of order $1/2$. I understand from a previous question I asked that for entireness one needs the solution to obey a $\lambda$-independent initial condition (since otherwise specifying an i.c. non-analytic in $\lambda$ gives a counter-example), and $\varphi,\theta$ do exactly this. Further than this I am fairly stumped, and in particular have no idea how to approach the claim on the order. Recall that the order of an entire function $f$ is    \begin{equation}       \rho:=\inf\{m\geq0~|~f(z)=O(\exp(|z|^m))\text{ as }|z|\rightarrow\infty\}.    \end{equation} Is it perhaps related to the Gel'fand-Levitan integral transformation , which allows one to write    \begin{align}       \varphi(x;\lambda)=\frac{\sin(\sqrt\lambda x)}{\sqrt\lambda}+\int_0^x K(x,t)\frac{\sin(\sqrt\lambda t)}{\sqrt\lambda}\text{d}t, \\       \theta(x,\lambda)=\cos(\sqrt\lambda x)+\int_0^x K(x,t)\cos(\sqrt\lambda t)\text{d}t?    \end{align} Here $K$ satisfies    \begin{align}       K_{xx}&=K_{tt}+q(x)K(x,t), \\       2\frac{\text d}{\text dx}K(x,x)&=q(x),~K_t(x,0)=0.    \end{align} $1$. Bennewitz, C., A proof of the local Borg-Marchenko theorem , Commun. Math. Phys. $218$, pp. $131$--$132$ ($2001$).","In $[1]$ the following (paraphrased) claim is made: Let $q\in L^1_{loc}([0,\infty);\mathbb{R})$, and suppose $\varphi$ and $\theta$ solve the one-dimensional Schrödinger equation    \begin{equation}       -u''(x;\lambda)+q(x)u(x;\lambda)=\lambda u(x;\lambda)~~~(x\in[0,\infty)),    \end{equation} subject to    \begin{equation}       \left\{\begin{array}{cc}          \varphi(0;\lambda)=0, &\varphi'(0;\lambda)=1, \\          \theta(0;\lambda)=1,  &\theta'(0;\lambda)=0.       \end{array}\right.    \end{equation} Then, for each fixed $x$, we have that $\varphi(x;\cdot)$ and $\theta(x;\cdot)$ are entire functions of order $1/2$. I understand from a previous question I asked that for entireness one needs the solution to obey a $\lambda$-independent initial condition (since otherwise specifying an i.c. non-analytic in $\lambda$ gives a counter-example), and $\varphi,\theta$ do exactly this. Further than this I am fairly stumped, and in particular have no idea how to approach the claim on the order. Recall that the order of an entire function $f$ is    \begin{equation}       \rho:=\inf\{m\geq0~|~f(z)=O(\exp(|z|^m))\text{ as }|z|\rightarrow\infty\}.    \end{equation} Is it perhaps related to the Gel'fand-Levitan integral transformation , which allows one to write    \begin{align}       \varphi(x;\lambda)=\frac{\sin(\sqrt\lambda x)}{\sqrt\lambda}+\int_0^x K(x,t)\frac{\sin(\sqrt\lambda t)}{\sqrt\lambda}\text{d}t, \\       \theta(x,\lambda)=\cos(\sqrt\lambda x)+\int_0^x K(x,t)\cos(\sqrt\lambda t)\text{d}t?    \end{align} Here $K$ satisfies    \begin{align}       K_{xx}&=K_{tt}+q(x)K(x,t), \\       2\frac{\text d}{\text dx}K(x,x)&=q(x),~K_t(x,0)=0.    \end{align} $1$. Bennewitz, C., A proof of the local Borg-Marchenko theorem , Commun. Math. Phys. $218$, pp. $131$--$132$ ($2001$).",,"['functional-analysis', 'ordinary-differential-equations', 'partial-differential-equations', 'spectral-theory']"
13,"Method of successive approximations to solve $y'(t) = ty(t)+2 , y(0)=0$",Method of successive approximations to solve,"y'(t) = ty(t)+2 , y(0)=0","By using the method of successive approximations, solve the problem: $y'(t) = ty(t)+2 , y(0)=0$ I couldn't find what method of successive approximations is, or any examples on how it is used to solve differential equations in my text book. If you can help me, I will be glad. Thanks!","By using the method of successive approximations, solve the problem: $y'(t) = ty(t)+2 , y(0)=0$ I couldn't find what method of successive approximations is, or any examples on how it is used to solve differential equations in my text book. If you can help me, I will be glad. Thanks!",,['ordinary-differential-equations']
14,Differentiating exponential functions - is base e the only situation?,Differentiating exponential functions - is base e the only situation?,,My maths book gives the example of; Where $$ f = e^x $$ $$ f` = e^x $$ It only uses the example of base e in all of the questions so does that mean this is the only situation where the differential of an exponential function is the same as the original?,My maths book gives the example of; Where $$ f = e^x $$ $$ f` = e^x $$ It only uses the example of base e in all of the questions so does that mean this is the only situation where the differential of an exponential function is the same as the original?,,"['calculus', 'ordinary-differential-equations', 'exponential-function']"
15,"Problem with translating ""Kozyklus Eigenschaft"" to English","Problem with translating ""Kozyklus Eigenschaft"" to English",,"I'm struggling with the translation of the book written by Bernd Aulbach about ordinary differential equations. There is one notion that I can't find reference for even on the German webpages about ODEs. Bernd Aulbach wrote: Gegeben sei eine offene Teilmenge $D$ des $\mathbb R^{1+N}$ , eine stetige und bezüglich $x$ Lipschitz-stetige   Funktion $f \colon D \to \mathbb R^N$ und die somit definierte   Differenzialgleichung $\dot x = f(t,x)$ . Die für alle $(t, \tau, \xi)$ aus der Menge $\Omega$ definierte Funktion $\lambda(t, \tau, \xi) = \lambda_{\max}(t, \tau, \xi)$ nennen wir dann die allgemeine Lösung   der Differenzialgleichung. $$\Omega : \{(t, \tau, \xi) \in \mathbb > R^{1+1+N} : (\tau, \xi) \in D, t \in I_{\max}(\tau, \xi)\}$$ Sei $(\tau, \xi)$ ein beliebiger Punkt aus $D$ . Dann gelten für jedes $\sigma \in I_{\max}(\tau, \xi)$ die Beziehungen (...) und $\lambda(t, > \sigma, \lambda(\sigma, \tau, \xi)) = \lambda(t, \tau, \xi)$ für alle $t \in I_{\max}(\tau, \xi)$ , die Identität nennen wir die Kozyklus-Eigenschaft der allgemeinen Here is my (quite accurate, I hope) translation of the second part (which is more important): Let $(\tau, \xi)$ be any point lying in $D$ . The following equalities are true for all $\sigma \in I_{\max}(\tau, \xi)$ : (...) and $\lambda(t, \sigma, \lambda(\sigma, \tau, \xi)) = \lambda(t, \tau, \xi)$ for all $t \in I_{\max}(\tau, \xi)$ , this identity is called the cocycle property of general solution. I can't understand what the ""cocycle property"" actually is and how to translate it to English.","I'm struggling with the translation of the book written by Bernd Aulbach about ordinary differential equations. There is one notion that I can't find reference for even on the German webpages about ODEs. Bernd Aulbach wrote: Gegeben sei eine offene Teilmenge des , eine stetige und bezüglich Lipschitz-stetige   Funktion und die somit definierte   Differenzialgleichung . Die für alle aus der Menge definierte Funktion nennen wir dann die allgemeine Lösung   der Differenzialgleichung. Sei ein beliebiger Punkt aus . Dann gelten für jedes die Beziehungen (...) und für alle , die Identität nennen wir die Kozyklus-Eigenschaft der allgemeinen Here is my (quite accurate, I hope) translation of the second part (which is more important): Let be any point lying in . The following equalities are true for all : (...) and for all , this identity is called the cocycle property of general solution. I can't understand what the ""cocycle property"" actually is and how to translate it to English.","D \mathbb R^{1+N} x f \colon D \to \mathbb R^N \dot x = f(t,x) (t, \tau, \xi) \Omega \lambda(t, \tau, \xi) = \lambda_{\max}(t, \tau, \xi) \Omega : \{(t, \tau, \xi) \in \mathbb
> R^{1+1+N} : (\tau, \xi) \in D, t \in I_{\max}(\tau, \xi)\} (\tau, \xi) D \sigma \in I_{\max}(\tau, \xi) \lambda(t,
> \sigma, \lambda(\sigma, \tau, \xi)) = \lambda(t, \tau, \xi) t \in I_{\max}(\tau, \xi) (\tau, \xi) D \sigma \in I_{\max}(\tau, \xi) \lambda(t, \sigma, \lambda(\sigma, \tau, \xi)) = \lambda(t, \tau, \xi) t \in I_{\max}(\tau, \xi)",['ordinary-differential-equations']
16,How can we show that for $\lambda <0$ we get the trivial solution $X(x)=0$?,How can we show that for  we get the trivial solution ?,\lambda <0 X(x)=0,"Find the solution of the problem $$u_t(x, t)-u_{xx}(x, t)=0, 0<x<1, t>0 \tag {*} \\ u(0, t)=0, t>0 \\ u_x(1,t)+u_t(1,t)=0, t>0$$ I have done the following: We are looking for solutions of the form $$u(x, t)=X(x) \cdot T(t)$$ $$u(0, t)=X(0) \cdot T(t)=0 \Rightarrow X(0)=0 \\ X'(1) \cdot T(t)+X(1) \cdot T'(t)=0 \Rightarrow \frac{X'(1)}{X(1)}=-\frac{T'(t)}{T(t)}$$ $$(*) \Rightarrow X(x) \cdot T'(t)-X''(x) \cdot T(t)=0 \\ \Rightarrow \frac{X(x) \cdot T'(t)}{X(x) \cdot T(t)}-\frac{X''(x) \cdot T(t)}{X(x) \cdot T(t)}=0 \\ \Rightarrow \frac{T'(t)}{T(t)}=\frac{X''(x)}{X(x)}=-\lambda$$ So, we get the following two problems:  $$\left.\begin{matrix} X''(x)+\lambda X(x)=0, 0<x<1\\  X(0)=0 \\ \frac{X'(1)}{X(1)}=\lambda \Rightarrow X'(1)-\lambda X(1)=0 \end{matrix}\right\}(1) $$ $$\left.\begin{matrix} T'(t)+\lambda T(t)=0, t>0 \end{matrix}\right\}(2)$$ For the problem $(1)$ we do the following: The characteristic polynomial is $d^2+\lambda=0$. $\lambda<0$: General solution: $X(x)=c_1 e^{\sqrt{-\lambda }x}+c_2e^{-\sqrt{-\lambda}x}$ $$X(0)=0 \Rightarrow c_1+c_2=0 \Rightarrow c_1=-c_2$$ $$X(1)=c_1e^{\sqrt{-\lambda}}+c_2e^{-\sqrt{-\lambda}}=c_1(e^{\sqrt{-\lambda}}-e^{-\sqrt{-\lambda}})$$ $$X'(x)=\sqrt{-\lambda}c_1e^{\sqrt{-\lambda }x}-\sqrt{-\lambda}c_2 e^{-\sqrt{-\lambda}x} \\ X'(1)=\sqrt{-\lambda}c_1e^{\sqrt{-\lambda }}-\sqrt{-\lambda}c_2 e^{-\sqrt{-\lambda}}=\sqrt{-\lambda}c_1(e^{-\lambda}+e^{-\sqrt{-\lambda}}$$ $$X'(1)-\lambda X(1)=0 \Rightarrow \sqrt{-\lambda}c_1(e^{\sqrt{-\lambda}}+e^{-\sqrt{-\lambda}})-\lambda c_1(e^{\sqrt{-\lambda}}-e^{-\sqrt{-\lambda}})=0 \Rightarrow c_1 [ e^{\sqrt{-\lambda}}(\sqrt{-\lambda}-\lambda)+e^{-\sqrt{-\lambda}}(\sqrt{-\lambda}+\lambda)]=0$$ How could we continue to show that for $\lambda <0$ we get the trivial solution $X(x)=0$ ?? $$$$ EDIT: $\lambda <0$ : $X(x)=c_1 \sinh (\sqrt{-\lambda} x)+c_2 \cosh (\sqrt{-\lambda}x)$ Using the initial values we get that $X(x)=0$, trivial solution. $\lambda=0$ : $X(x)=c_1 x+c_2$ Using the initial values we get that $X(x)=0$, trivial solution. $\lambda >0$ : $X(x)=c_1 cos (\sqrt{\lambda}x)+c_2 \sin (\sqrt{\lambda}x)$ $X(0)=0 \Rightarrow c_1=0 \Rightarrow X(x)=c_2=\sin (\sqrt{\lambda}x)$ $X'(1)-\lambda X(1)=0 \Rightarrow \tan (\sqrt{\lambda})=\frac{1}{\sqrt{\lambda}}$ That means that the eigenvalue problem $(1)$ has only positive eigenvalues $0<\lambda_1 < \lambda_2 < \dots < \lambda_k < \dots $ that are the positive roots of the equation $\tan \sqrt{x}=\frac{1}{\sqrt{x}}$. Is this correct?? Why can we say that the number of the eigenvalues is countable ?? How can we show that $$\lim_{k \rightarrow +\infty} \frac{\sqrt{\lambda_k}}{k \pi}=1$$ ??","Find the solution of the problem $$u_t(x, t)-u_{xx}(x, t)=0, 0<x<1, t>0 \tag {*} \\ u(0, t)=0, t>0 \\ u_x(1,t)+u_t(1,t)=0, t>0$$ I have done the following: We are looking for solutions of the form $$u(x, t)=X(x) \cdot T(t)$$ $$u(0, t)=X(0) \cdot T(t)=0 \Rightarrow X(0)=0 \\ X'(1) \cdot T(t)+X(1) \cdot T'(t)=0 \Rightarrow \frac{X'(1)}{X(1)}=-\frac{T'(t)}{T(t)}$$ $$(*) \Rightarrow X(x) \cdot T'(t)-X''(x) \cdot T(t)=0 \\ \Rightarrow \frac{X(x) \cdot T'(t)}{X(x) \cdot T(t)}-\frac{X''(x) \cdot T(t)}{X(x) \cdot T(t)}=0 \\ \Rightarrow \frac{T'(t)}{T(t)}=\frac{X''(x)}{X(x)}=-\lambda$$ So, we get the following two problems:  $$\left.\begin{matrix} X''(x)+\lambda X(x)=0, 0<x<1\\  X(0)=0 \\ \frac{X'(1)}{X(1)}=\lambda \Rightarrow X'(1)-\lambda X(1)=0 \end{matrix}\right\}(1) $$ $$\left.\begin{matrix} T'(t)+\lambda T(t)=0, t>0 \end{matrix}\right\}(2)$$ For the problem $(1)$ we do the following: The characteristic polynomial is $d^2+\lambda=0$. $\lambda<0$: General solution: $X(x)=c_1 e^{\sqrt{-\lambda }x}+c_2e^{-\sqrt{-\lambda}x}$ $$X(0)=0 \Rightarrow c_1+c_2=0 \Rightarrow c_1=-c_2$$ $$X(1)=c_1e^{\sqrt{-\lambda}}+c_2e^{-\sqrt{-\lambda}}=c_1(e^{\sqrt{-\lambda}}-e^{-\sqrt{-\lambda}})$$ $$X'(x)=\sqrt{-\lambda}c_1e^{\sqrt{-\lambda }x}-\sqrt{-\lambda}c_2 e^{-\sqrt{-\lambda}x} \\ X'(1)=\sqrt{-\lambda}c_1e^{\sqrt{-\lambda }}-\sqrt{-\lambda}c_2 e^{-\sqrt{-\lambda}}=\sqrt{-\lambda}c_1(e^{-\lambda}+e^{-\sqrt{-\lambda}}$$ $$X'(1)-\lambda X(1)=0 \Rightarrow \sqrt{-\lambda}c_1(e^{\sqrt{-\lambda}}+e^{-\sqrt{-\lambda}})-\lambda c_1(e^{\sqrt{-\lambda}}-e^{-\sqrt{-\lambda}})=0 \Rightarrow c_1 [ e^{\sqrt{-\lambda}}(\sqrt{-\lambda}-\lambda)+e^{-\sqrt{-\lambda}}(\sqrt{-\lambda}+\lambda)]=0$$ How could we continue to show that for $\lambda <0$ we get the trivial solution $X(x)=0$ ?? $$$$ EDIT: $\lambda <0$ : $X(x)=c_1 \sinh (\sqrt{-\lambda} x)+c_2 \cosh (\sqrt{-\lambda}x)$ Using the initial values we get that $X(x)=0$, trivial solution. $\lambda=0$ : $X(x)=c_1 x+c_2$ Using the initial values we get that $X(x)=0$, trivial solution. $\lambda >0$ : $X(x)=c_1 cos (\sqrt{\lambda}x)+c_2 \sin (\sqrt{\lambda}x)$ $X(0)=0 \Rightarrow c_1=0 \Rightarrow X(x)=c_2=\sin (\sqrt{\lambda}x)$ $X'(1)-\lambda X(1)=0 \Rightarrow \tan (\sqrt{\lambda})=\frac{1}{\sqrt{\lambda}}$ That means that the eigenvalue problem $(1)$ has only positive eigenvalues $0<\lambda_1 < \lambda_2 < \dots < \lambda_k < \dots $ that are the positive roots of the equation $\tan \sqrt{x}=\frac{1}{\sqrt{x}}$. Is this correct?? Why can we say that the number of the eigenvalues is countable ?? How can we show that $$\lim_{k \rightarrow +\infty} \frac{\sqrt{\lambda_k}}{k \pi}=1$$ ??",,"['ordinary-differential-equations', 'partial-differential-equations']"
17,Prove that the problem has at most one solution,Prove that the problem has at most one solution,,"Prove that the initial and boundary problem $$\\u_t(x,t)-u_{xxx}(x,t)=0,0<x<1,t>0 \\ u(x,0)=\phi(x),0<x<1 \\ u_x(0,t)=h(t),t>0 \\ u_{xx}(0,t)=H(t), t>0 \\ u_x(1,t)=g(t),t>0$$ has at most one solution. Hint: Use the function $$\frac{1}{2} \int_0^1 u_x^2(x,t)dx$$ I have done the following: We consider the function $E(t)=\int_0^1 u_x^2(x,t)dx$. $$E'(t)=\int_0^1 2u_x(x,t)u_{xt}(x,t)dx=\int_0^1 2u_x(x,t)(u_t(x,t))_x dx \\ =\left[2u_x(x,t)u_t(x,t)\right]_0^1-\int_0^1 2u_{xx}(x,t)u_t(x,t)dx \\=2u_x(1,t)u_t(1,t)-2u_x(0,t)u_t(0,t)-\int_0^1 2u_{xx}(x,t)u_{xxx}(x,t)dx\\= -\int_0^1 (u_{xx}^2)_x dx=-u_{xx}^2(1,t) \leq 0$$ So $E(t)$ is decreasing. $E(0)=\int_0^1 u_x^2(x,0)dx=0$ because $u(x,0)=0 \Rightarrow u_x(x,0)=0$. $t \geq 0 \Rightarrow E(t) \leq 0 \Rightarrow E(t) \leq 0$ $$\left\{\begin{matrix} E(t)\leq 0\\  E(t) \geq 0 \end{matrix}\right. \Rightarrow E(t)=0 \Rightarrow u_x^2(x,t)=0 \Rightarrow u_x(x,t)=0 $$ We would know that the solution is unique when $u(x,t)=u(x_0,t), 0<x_0<1$. How could we show this??","Prove that the initial and boundary problem $$\\u_t(x,t)-u_{xxx}(x,t)=0,0<x<1,t>0 \\ u(x,0)=\phi(x),0<x<1 \\ u_x(0,t)=h(t),t>0 \\ u_{xx}(0,t)=H(t), t>0 \\ u_x(1,t)=g(t),t>0$$ has at most one solution. Hint: Use the function $$\frac{1}{2} \int_0^1 u_x^2(x,t)dx$$ I have done the following: We consider the function $E(t)=\int_0^1 u_x^2(x,t)dx$. $$E'(t)=\int_0^1 2u_x(x,t)u_{xt}(x,t)dx=\int_0^1 2u_x(x,t)(u_t(x,t))_x dx \\ =\left[2u_x(x,t)u_t(x,t)\right]_0^1-\int_0^1 2u_{xx}(x,t)u_t(x,t)dx \\=2u_x(1,t)u_t(1,t)-2u_x(0,t)u_t(0,t)-\int_0^1 2u_{xx}(x,t)u_{xxx}(x,t)dx\\= -\int_0^1 (u_{xx}^2)_x dx=-u_{xx}^2(1,t) \leq 0$$ So $E(t)$ is decreasing. $E(0)=\int_0^1 u_x^2(x,0)dx=0$ because $u(x,0)=0 \Rightarrow u_x(x,0)=0$. $t \geq 0 \Rightarrow E(t) \leq 0 \Rightarrow E(t) \leq 0$ $$\left\{\begin{matrix} E(t)\leq 0\\  E(t) \geq 0 \end{matrix}\right. \Rightarrow E(t)=0 \Rightarrow u_x^2(x,t)=0 \Rightarrow u_x(x,t)=0 $$ We would know that the solution is unique when $u(x,t)=u(x_0,t), 0<x_0<1$. How could we show this??",,"['ordinary-differential-equations', 'partial-differential-equations']"
18,discrepancies between diff.equ. for SIR model of infection,discrepancies between diff.equ. for SIR model of infection,,"I'm reading this book as background for thesis research, and I'm confused by the explanation of the SIR model given in the first chapter. On p. 7, I'm given $$\frac{dS}{dt} = −\beta SI$$ and $$\frac{dI}{dt} = \beta SI - \gamma I$$ where S is the number of susceptibles and I is the number of infectious people. $S + I + R = N$ is the total population size. It says ""Here, the transmission rate (per capita) is β and the recovery rate is γ (so the mean infectious period is 1/γ)."" Taking the ratio of the above two, they give $$\frac{dI}{dS} = -1 + \frac{1}{\mathcal{R}_0S}$$ where $\mathcal{R}_0S = \frac{\beta N}{\gamma}$, but I get $$\frac{dI}{dS} = -1 + \frac{N}{\mathcal{R}_0S}$$ which is what is in a later chapter (by a different author). Any ideas what's going on?","I'm reading this book as background for thesis research, and I'm confused by the explanation of the SIR model given in the first chapter. On p. 7, I'm given $$\frac{dS}{dt} = −\beta SI$$ and $$\frac{dI}{dt} = \beta SI - \gamma I$$ where S is the number of susceptibles and I is the number of infectious people. $S + I + R = N$ is the total population size. It says ""Here, the transmission rate (per capita) is β and the recovery rate is γ (so the mean infectious period is 1/γ)."" Taking the ratio of the above two, they give $$\frac{dI}{dS} = -1 + \frac{1}{\mathcal{R}_0S}$$ where $\mathcal{R}_0S = \frac{\beta N}{\gamma}$, but I get $$\frac{dI}{dS} = -1 + \frac{N}{\mathcal{R}_0S}$$ which is what is in a later chapter (by a different author). Any ideas what's going on?",,['ordinary-differential-equations']
19,On the constant solutions to differential equations,On the constant solutions to differential equations,,"when do we have to look for a constant solution to the differential equation? Example : Solve the initial value problem $\frac{dy}{dx} = \frac{2}{\sin y}$ with the condition $y(0)=0$. The first step in the worked solution is as follows: Noting that $\frac{1}{\sin y} ≠ 0$ for any $y$, we see there are no constant solutions. My question is: do we always have to look for constant solutions when solving any differential equations? and what is the significance of the constant solution? thank you","when do we have to look for a constant solution to the differential equation? Example : Solve the initial value problem $\frac{dy}{dx} = \frac{2}{\sin y}$ with the condition $y(0)=0$. The first step in the worked solution is as follows: Noting that $\frac{1}{\sin y} ≠ 0$ for any $y$, we see there are no constant solutions. My question is: do we always have to look for constant solutions when solving any differential equations? and what is the significance of the constant solution? thank you",,['ordinary-differential-equations']
20,Computing the Laplace transform of $\frac {f(x)}{x}$,Computing the Laplace transform of,\frac {f(x)}{x},"I am having trouble computing the following Laplace transform: $\frac {f(x)}{x}$. From Wikipedia it should be equivalent to this: $\int_s^\infty F(\sigma) \,d\sigma$ . What I've done so far is substitute in $-\int_s^\infty e^{-sx} ds$ for $\frac {e^{-sx}}{x}$. After simplification I end up with this: $-\int_s^\infty F(s)\, ds$ where $F(s)$ is the Laplace transform of $f(x)$ I am unsure of how to simplify this further to get the result of Wikipedia.","I am having trouble computing the following Laplace transform: $\frac {f(x)}{x}$. From Wikipedia it should be equivalent to this: $\int_s^\infty F(\sigma) \,d\sigma$ . What I've done so far is substitute in $-\int_s^\infty e^{-sx} ds$ for $\frac {e^{-sx}}{x}$. After simplification I end up with this: $-\int_s^\infty F(s)\, ds$ where $F(s)$ is the Laplace transform of $f(x)$ I am unsure of how to simplify this further to get the result of Wikipedia.",,"['ordinary-differential-equations', 'laplace-transform']"
21,Solving systems of ODE,Solving systems of ODE,,"Consider the following eigenvectors which satisfies the system of a homogenous ODE $$x_1 = \left(   \begin{array}{c}     t \\     1 \\   \end{array} \right)$$ $$x_2 = \left(   \begin{array}{c}     e^t \\     e^t \\   \end{array} \right)$$ . (a) Compute the Wronskian of $x_1$ and $x_2$ (b) Find the original system of equations. What i tried (a)The Wronskain is $te^t(t-2)$ and for it to be lineraly indepedent, the Wronskain not equals to $0$ hence $t$ lies in the interval $0<t<2$. (b) The answer to this part is $$x'=\begin{bmatrix}0&1\\\frac{2-2t}{t^2-2t}&\frac{t^2-2}{t^2-2t}\\\end{bmatrix}x$$. I know how to get the eigenvectors from the system of ODE. But now the question require me to work backwards.Im unsure how to do so  from the eigenvectors though. Could anyone explain. Thanks","Consider the following eigenvectors which satisfies the system of a homogenous ODE $$x_1 = \left(   \begin{array}{c}     t \\     1 \\   \end{array} \right)$$ $$x_2 = \left(   \begin{array}{c}     e^t \\     e^t \\   \end{array} \right)$$ . (a) Compute the Wronskian of $x_1$ and $x_2$ (b) Find the original system of equations. What i tried (a)The Wronskain is $te^t(t-2)$ and for it to be lineraly indepedent, the Wronskain not equals to $0$ hence $t$ lies in the interval $0<t<2$. (b) The answer to this part is $$x'=\begin{bmatrix}0&1\\\frac{2-2t}{t^2-2t}&\frac{t^2-2}{t^2-2t}\\\end{bmatrix}x$$. I know how to get the eigenvectors from the system of ODE. But now the question require me to work backwards.Im unsure how to do so  from the eigenvectors though. Could anyone explain. Thanks",,['ordinary-differential-equations']
22,Existence and Uniqueness of ODEs and form of initial conditions,Existence and Uniqueness of ODEs and form of initial conditions,,"Is there a technical reason as to why the existence and uniqueness theorem for ODEs of the form $$y'(x) = F(x,y(x))$$ is proved for initial conditions of the form $$y(x_0) = y_0$$ and not for $$y'(x_0) = y'_0$$ I understand that in most physical applications, only initial values of the form $y(x_0) = y_0$ are present. But is there any other reason for such a form of initial condition, or can we prove existence and uniqueness even for the IVP of the form $y'(x_0) = y'_0$ ?","Is there a technical reason as to why the existence and uniqueness theorem for ODEs of the form $$y'(x) = F(x,y(x))$$ is proved for initial conditions of the form $$y(x_0) = y_0$$ and not for $$y'(x_0) = y'_0$$ I understand that in most physical applications, only initial values of the form $y(x_0) = y_0$ are present. But is there any other reason for such a form of initial condition, or can we prove existence and uniqueness even for the IVP of the form $y'(x_0) = y'_0$ ?",,['ordinary-differential-equations']
23,Transfer function for double cart system,Transfer function for double cart system,,"System: Define X2 = Y2; I've described the system with the following diff equation:  $$f_{tot} = m_1\ddot{x_1} + k(x_2-x_1)+m_2\ddot{x_2}+B(\dot{x_2}-\dot{x_1})$$ where m1, m2, k and B are Cart mass 1, Cart mass 2, Spring constant and dampner constant respectively. All derivatives are in respect to time, $t$. I get the following after Laplace:  $$F(s) = m_1s^2X_1(s)+k(X_2(s)-X_1(s)) + m2s^2X_2(s)+B(sX_2(s)+sX_1(s))$$ $$G(s) = \frac{Output}{Input} = \frac{X_2(s)}{F(s)} = ???$$ But the problem is that I can't define $X_1(s)$. Should $X_1(s)$ be interpreted as $1$? Is the above formula correct?","System: Define X2 = Y2; I've described the system with the following diff equation:  $$f_{tot} = m_1\ddot{x_1} + k(x_2-x_1)+m_2\ddot{x_2}+B(\dot{x_2}-\dot{x_1})$$ where m1, m2, k and B are Cart mass 1, Cart mass 2, Spring constant and dampner constant respectively. All derivatives are in respect to time, $t$. I get the following after Laplace:  $$F(s) = m_1s^2X_1(s)+k(X_2(s)-X_1(s)) + m2s^2X_2(s)+B(sX_2(s)+sX_1(s))$$ $$G(s) = \frac{Output}{Input} = \frac{X_2(s)}{F(s)} = ???$$ But the problem is that I can't define $X_1(s)$. Should $X_1(s)$ be interpreted as $1$? Is the above formula correct?",,['ordinary-differential-equations']
24,How can you tell that a general solution to a DE is general?,How can you tell that a general solution to a DE is general?,,"At school, or in a first-year course on DEs, we learn (perhaps in less abstract language) that if you have a linear $n$th-order differential equation $$Ly = f$$ then the general solution is something of the form $$y = a_1 y_1 + ... + a_n y_n + g$$ where the $y_i$ are independent and satisfy $Ly_i = 0$, and $g$ satisfies $Lg = f$. Then we receive lots of training in how to find the $y_i$ and $g$. Obviously any choice of the $a_i$ will give us a solution to $Ly=f$. But how do you know that there aren't any more solutions? We justify this by making an analogy with systems of linear equations $Ax=b$, saying something along the lines of 'the space of solutions has the same dimension as the kernel of $A$'. But that works in finite dimensions - how do we know that the same is true with linear operators?","At school, or in a first-year course on DEs, we learn (perhaps in less abstract language) that if you have a linear $n$th-order differential equation $$Ly = f$$ then the general solution is something of the form $$y = a_1 y_1 + ... + a_n y_n + g$$ where the $y_i$ are independent and satisfy $Ly_i = 0$, and $g$ satisfies $Lg = f$. Then we receive lots of training in how to find the $y_i$ and $g$. Obviously any choice of the $a_i$ will give us a solution to $Ly=f$. But how do you know that there aren't any more solutions? We justify this by making an analogy with systems of linear equations $Ax=b$, saying something along the lines of 'the space of solutions has the same dimension as the kernel of $A$'. But that works in finite dimensions - how do we know that the same is true with linear operators?",,"['linear-algebra', 'functional-analysis', 'ordinary-differential-equations']"
25,How to show that all solutions of $ay''+by'+cy=0$ approach $0$ as $t \rightarrow \infty$,How to show that all solutions of  approach  as,ay''+by'+cy=0 0 t \rightarrow \infty,"Given $ay''+by'+cy=0$ and assuming that $a, b, c > 0$ show that all solutions approach $0$ as $t\rightarrow\infty$ I was able to begin by seperating the problem into three cases: Case 1: Repeated roots of the characteristic equation I was able to find the limit as $t\rightarrow\infty$ for the general solution to Case 1 using l'Hospital's Rule: Fairly straightforward. Case 2: Imaginary roots of the characteristic equation I was able to find the limit as $t\rightarrow\infty$ for the general solution to Case 2 using the squeeze theorem: Also fairly straightforward. Case 3: Two distinct real roots of the characteristic equation In this case the two roots are: $$r_1=\frac{-b+\sqrt{b^2-4ac}}{2a}$$ $$r_2=\frac{-b-\sqrt{b^2-4ac}}{2a}$$ and the general solution is: $$y=c_1e^{r_1t}+c_2e^{r_2t}$$ The only way I see that $\lim_{t\to\infty} y=0$ is if $r_1,r_2<0$.  However if the only information I am given is that $a,b,c>0$ how can I prove that $r_1,r_2<0$?","Given $ay''+by'+cy=0$ and assuming that $a, b, c > 0$ show that all solutions approach $0$ as $t\rightarrow\infty$ I was able to begin by seperating the problem into three cases: Case 1: Repeated roots of the characteristic equation I was able to find the limit as $t\rightarrow\infty$ for the general solution to Case 1 using l'Hospital's Rule: Fairly straightforward. Case 2: Imaginary roots of the characteristic equation I was able to find the limit as $t\rightarrow\infty$ for the general solution to Case 2 using the squeeze theorem: Also fairly straightforward. Case 3: Two distinct real roots of the characteristic equation In this case the two roots are: $$r_1=\frac{-b+\sqrt{b^2-4ac}}{2a}$$ $$r_2=\frac{-b-\sqrt{b^2-4ac}}{2a}$$ and the general solution is: $$y=c_1e^{r_1t}+c_2e^{r_2t}$$ The only way I see that $\lim_{t\to\infty} y=0$ is if $r_1,r_2<0$.  However if the only information I am given is that $a,b,c>0$ how can I prove that $r_1,r_2<0$?",,"['ordinary-differential-equations', 'limits']"
26,differential equation; how to find the general solution.,differential equation; how to find the general solution.,,"how do i get the general solution of $$\frac{dy}{dx} = \frac{(10x^2-1)y}{  x(10x^2+7x+1)}$$  please? i have been trying for a while now. I have been doing it as a separable DE and I get $$\frac {dy}y = \frac{(10x^2-1)\, dx}{x(10x^2+7x+1)} $$I can not seem to carry on.","how do i get the general solution of $$\frac{dy}{dx} = \frac{(10x^2-1)y}{  x(10x^2+7x+1)}$$  please? i have been trying for a while now. I have been doing it as a separable DE and I get $$\frac {dy}y = \frac{(10x^2-1)\, dx}{x(10x^2+7x+1)} $$I can not seem to carry on.",,['ordinary-differential-equations']
27,Difficult Reduction of Order with $y_1 = \sin(2t^2)$,Difficult Reduction of Order with,y_1 = \sin(2t^2),"I found this problem in the extra questions section of my text, and I can't figure it out.  It is: Use reduction of order to find a second solution for $ty''-y'+16t^3y=0, \ \ \ \ y_1=\sin(2t^2)$. I tried going about it by reducing a general second order differential to $v''y_1+v'(2y_1'+p(t)y_1)v'=0$ $v''\sin(2t^2)+v'(8t\cos(2t^2)-t^{-1})=0$ But from here I am stuck.  Any and all help is appreciated!  Thanks!","I found this problem in the extra questions section of my text, and I can't figure it out.  It is: Use reduction of order to find a second solution for $ty''-y'+16t^3y=0, \ \ \ \ y_1=\sin(2t^2)$. I tried going about it by reducing a general second order differential to $v''y_1+v'(2y_1'+p(t)y_1)v'=0$ $v''\sin(2t^2)+v'(8t\cos(2t^2)-t^{-1})=0$ But from here I am stuck.  Any and all help is appreciated!  Thanks!",,['ordinary-differential-equations']
28,Let $f$ be a continous function $\mathbb R \to \mathbb R$ and $x(t)$ be of class $C^1$ given by the solution of: >$x' - f(x) = t^2$,Let  be a continous function  and  be of class  given by the solution of: >,f \mathbb R \to \mathbb R x(t) C^1 x' - f(x) = t^2,"I can't find a way to solve this one, although it seems to be quite basic: Let $f$ be a continous function $\mathbb R \to \mathbb R$ and $x(t)$ be of class $C^1$ given by the solution of: $x' - f(x) = t^2$ Knowing that $x(0) = x_0 \geq 0$ and $f(0) > 0$ prove that if $t > 0 $ then $ x(t) > 0$. Is it true if $ t < 0$?","I can't find a way to solve this one, although it seems to be quite basic: Let $f$ be a continous function $\mathbb R \to \mathbb R$ and $x(t)$ be of class $C^1$ given by the solution of: $x' - f(x) = t^2$ Knowing that $x(0) = x_0 \geq 0$ and $f(0) > 0$ prove that if $t > 0 $ then $ x(t) > 0$. Is it true if $ t < 0$?",,"['real-analysis', 'ordinary-differential-equations', 'derivatives']"
29,Solve Laplace's equation inside a semi-infinite strip,Solve Laplace's equation inside a semi-infinite strip,,"Solve $$\frac{\partial^{2} u}{\partial x^{2}} + \frac{\partial^{2} u}{\partial y^{2}} = 0$$ $$0 < x < \infty, \ \ 0 < y < H$$ subject to these boundary conditions: $${\frac {\partial u} {\partial y}} (x,0)=0$$ $${\frac {\partial u} {\partial y}} (x,H)=0$$ $$u(0,y)=f(y)$$ This is what I found and I understand it but is this the answer? You will want to solve this using separation of variables. Then ∇2(XY)=X′′(x)Y(y)+X(x)Y′′(y)=0 so that X′′X+Y′′Y=0.Hence X′′=CX,  Y′′=−CY for some real constant C (known as a separation constant). The values of C we need to take depend on the boundary conditions, which are: X(0)=1, limx→∞X(x)=0 ,Y(0)=Y(h)=0 with Y(y) not identically zero (actually all that's required is X(0)≠0, but it is convenient to specify X(0)=1). The easiest boundary condition to satisfy is that X(x)→0 as x→∞. We must have X(x)=$e^{−kx}$ for some k>0. This means that C=k$^2$ so that Y′′=−$k^2$Y subject to Y(0)=Y(h)=0 but with Y(y) not identically zero. That can be done if we take k=(nπ)/h for some positive integer n with Y(y)=Bsin(nπyh) where the constant B cannot be determined from the boundary conditions on Y. But given the next stage of the solution we may as well take B=1. Putting this together, we have, for each positive integer n, an eigenfunction $X_n$(x)$Y_n$(y)=exp(−nπxh)sin(nπyh) and the natural thing to do is to take a linear combination of these, u(x,y)=∑$a_n$exp(−nπxh)sin(nπyh), and choose the coefficients an to satisfy the boundary condition u(0,y)=f(y). We then have f(y)=u(0,y)=∑$a_n$sin(nπyh) which is the fourier sine series for f(y) on the interval 0≤y≤h. Thus $a_n$= 2/h ∫f(y)sin(nπyh)dy. Reference https://www.physicsforums.com/threads/laplaces-equation-inside-a-semi-infinite-strip.673334/ I understand how to compute Laplace equations but I don't understand where the semi-infinite strip comes to play. So this question is a bit difficult for me to understand. Can someone please help?","Solve $$\frac{\partial^{2} u}{\partial x^{2}} + \frac{\partial^{2} u}{\partial y^{2}} = 0$$ $$0 < x < \infty, \ \ 0 < y < H$$ subject to these boundary conditions: $${\frac {\partial u} {\partial y}} (x,0)=0$$ $${\frac {\partial u} {\partial y}} (x,H)=0$$ $$u(0,y)=f(y)$$ This is what I found and I understand it but is this the answer? You will want to solve this using separation of variables. Then ∇2(XY)=X′′(x)Y(y)+X(x)Y′′(y)=0 so that X′′X+Y′′Y=0.Hence X′′=CX,  Y′′=−CY for some real constant C (known as a separation constant). The values of C we need to take depend on the boundary conditions, which are: X(0)=1, limx→∞X(x)=0 ,Y(0)=Y(h)=0 with Y(y) not identically zero (actually all that's required is X(0)≠0, but it is convenient to specify X(0)=1). The easiest boundary condition to satisfy is that X(x)→0 as x→∞. We must have X(x)=$e^{−kx}$ for some k>0. This means that C=k$^2$ so that Y′′=−$k^2$Y subject to Y(0)=Y(h)=0 but with Y(y) not identically zero. That can be done if we take k=(nπ)/h for some positive integer n with Y(y)=Bsin(nπyh) where the constant B cannot be determined from the boundary conditions on Y. But given the next stage of the solution we may as well take B=1. Putting this together, we have, for each positive integer n, an eigenfunction $X_n$(x)$Y_n$(y)=exp(−nπxh)sin(nπyh) and the natural thing to do is to take a linear combination of these, u(x,y)=∑$a_n$exp(−nπxh)sin(nπyh), and choose the coefficients an to satisfy the boundary condition u(0,y)=f(y). We then have f(y)=u(0,y)=∑$a_n$sin(nπyh) which is the fourier sine series for f(y) on the interval 0≤y≤h. Thus $a_n$= 2/h ∫f(y)sin(nπyh)dy. Reference https://www.physicsforums.com/threads/laplaces-equation-inside-a-semi-infinite-strip.673334/ I understand how to compute Laplace equations but I don't understand where the semi-infinite strip comes to play. So this question is a bit difficult for me to understand. Can someone please help?",,"['ordinary-differential-equations', 'partial-differential-equations']"
30,"$a\frac{d^{3}x}{dt^{3}}+b\frac{d^{2}x}{dt^{2}}+c\frac{dx}{dt}+kx=0$, on what condition of k that we have $\lim_{t\rightarrow\infty}x(t)=0$",", on what condition of k that we have",a\frac{d^{3}x}{dt^{3}}+b\frac{d^{2}x}{dt^{2}}+c\frac{dx}{dt}+kx=0 \lim_{t\rightarrow\infty}x(t)=0,"$a\frac{d^{3}x}{dt^{3}}+b\frac{d^{2}x}{dt^{2}}+c\frac{dx}{dt}+kx=0$, $a,b,c$ are constant, and  on what condition of k that we have $\lim_{t\rightarrow\infty}x(t)=0$ I completely have no idea how to solve this problem, can anyone could help me? Thanks very much! What I did: If we let $a=b=c=1$, then we have $r^{3}+r^{2}+r+k=0,x(t)=C_{1}e^{r_{1}t}+C_{2}e^{r_{2}t}+C_{3}e^{r_{3}t}$, since $\lim_{t\rightarrow\infty}x(t)=0$, we get $r_{1}<0,r_{2}<0,r_{3}<0$. Assume that $r^{3}+r^{2}+r+k=(r+r_{1})(r+r_{2})(r+r_{3})$, then expand it we get $r_{1}+r_{2}+r_{3}=1,r_{3}(r_{1}+r_{2})+r_{1}r_{2}=1,r_{1}r_{2}r_{3}=k$, then how can I get the condition on k? I can just get $k>0$","$a\frac{d^{3}x}{dt^{3}}+b\frac{d^{2}x}{dt^{2}}+c\frac{dx}{dt}+kx=0$, $a,b,c$ are constant, and  on what condition of k that we have $\lim_{t\rightarrow\infty}x(t)=0$ I completely have no idea how to solve this problem, can anyone could help me? Thanks very much! What I did: If we let $a=b=c=1$, then we have $r^{3}+r^{2}+r+k=0,x(t)=C_{1}e^{r_{1}t}+C_{2}e^{r_{2}t}+C_{3}e^{r_{3}t}$, since $\lim_{t\rightarrow\infty}x(t)=0$, we get $r_{1}<0,r_{2}<0,r_{3}<0$. Assume that $r^{3}+r^{2}+r+k=(r+r_{1})(r+r_{2})(r+r_{3})$, then expand it we get $r_{1}+r_{2}+r_{3}=1,r_{3}(r_{1}+r_{2})+r_{1}r_{2}=1,r_{1}r_{2}r_{3}=k$, then how can I get the condition on k? I can just get $k>0$",,['ordinary-differential-equations']
31,Bounding the solution to ODE using initial condition,Bounding the solution to ODE using initial condition,,"I am working on the following problem:  Suppose $f\in C(U,\mathbb{R}^n)$ satisfies $|f(t,x)-f(t,y)|\leq L(t)|x-y|$. Show that the solution $\varphi(t,x_0)$, to the equation $$x'=f(t,x) \ \ \  x(t_0)=x_0$$ satisfies $$|\varphi(t,x_0)-\varphi(t,y_0)|\leq |x_0-y_0|e^{|\int_{t_0}^tL(s)ds}$$ From the textbook, we know that $$|\varphi(t,x_0)-\varphi(t,y_0)|\leq |x_0-y_0|e^{L|t-t_0|}$$ where $$L=\sup_{(t,x)\neq (t,y)}|\frac{f(t,x)-f(t,y)}{x-y}|$$ How would I use the above result to prove the problem","I am working on the following problem:  Suppose $f\in C(U,\mathbb{R}^n)$ satisfies $|f(t,x)-f(t,y)|\leq L(t)|x-y|$. Show that the solution $\varphi(t,x_0)$, to the equation $$x'=f(t,x) \ \ \  x(t_0)=x_0$$ satisfies $$|\varphi(t,x_0)-\varphi(t,y_0)|\leq |x_0-y_0|e^{|\int_{t_0}^tL(s)ds}$$ From the textbook, we know that $$|\varphi(t,x_0)-\varphi(t,y_0)|\leq |x_0-y_0|e^{L|t-t_0|}$$ where $$L=\sup_{(t,x)\neq (t,y)}|\frac{f(t,x)-f(t,y)}{x-y}|$$ How would I use the above result to prove the problem",,"['real-analysis', 'ordinary-differential-equations']"
32,Euler's method - Order of accuracy,Euler's method - Order of accuracy,,"Theorem Let $f \in C([a,b] \times \mathbb{R})$ a function that satisfies the Lipschitz condition and let $y \in C^2[a,b]$ the solution of the ODE $\left\{\begin{matrix} y'=f(t,y(t)) &, a \leq t \leq b \\  y(a)=y_0 &  \end{matrix}\right.$. If $y^0, y^1, \dots, y^N$ are the approximations of Euler's method for uniform partition of $[a,b]$ with step $h=\frac{b-a}{N}$ then $$\max_{0 \leq n \leq N} |y(t^n)-y^n| \leq \frac{M}{2L} (e^{L(b-a)}-1)h$$ where $M=\max_{a \leq t \leq b} |y''(t)|$. From the above theorem, we conclude that the order of accuracy of Euler's method is at least $1$. We will show that the order of accuracy of Euler's method is exactly $1$. We consider the following ODE: $\left\{\begin{matrix} y'=2t &, 0 \leq t \leq 1 \\  y(0)=0 &  \end{matrix}\right.$ Its only solution is $y(t)=t^2,\ \  0 \leq t \leq 1$. Let $N \in \mathbb{N}, \ h=\frac{1}{N}, \ \ t^n=nh, \ n=0,1, \dots, N$ $$y^{n+1}=y^n+hf(t^n, y^n) \Rightarrow y^{n+1}=y^n+h2t^n=y^n+h2nh=y^n+2nh^2$$ $$y^0=y(0)=0$$ $$y^1=y^0+2 \cdot 0 \cdot h^2=0$$ $$y^2=y^1+2h^2=2h^2$$ $$y^3=y^2+2 \cdot 2 \cdot h^2=2h^2+4h^2=2h^2(1+2)$$ $$y^4=y^3+2 \cdot 3 \cdot h^2=2h^2(1+2)+ 2 \cdot 3 \cdot  h^2=2h^2(1+2+3)$$ $$\dots \dots$$ $$y^n=2h^2 \sum_{i=1}^{n-1} i=2h^2 \frac{(n-1)n}{2}=n(n-1)h^2$$ For $n=N$: $y^N=N(N-1)h^2=(Nh-h) Nh=1-h$ $|y(t^N)-y^N|=|1-1+h|=h$ Theorefore, we conclude that the order of accuracy is exactly $1$. According to the theorem: $$\max_{0 \leq n \leq N} |y(t^n)-y^n| \leq \frac{M}{2L} (e^{L(b-a)}-1)h$$ So, why do we conclude that  the order of accuracy of Euler's method is at least $1$ and not at most $1$? Also,  we take into consideration that the order of accuracy of the method is at least $1$ and then we take an example of an ODE and see that the order of accuracy is exactly $1$.  Why do we conclude in this way something for the general case? How do we  deduce that for each ODE the same  holds?","Theorem Let $f \in C([a,b] \times \mathbb{R})$ a function that satisfies the Lipschitz condition and let $y \in C^2[a,b]$ the solution of the ODE $\left\{\begin{matrix} y'=f(t,y(t)) &, a \leq t \leq b \\  y(a)=y_0 &  \end{matrix}\right.$. If $y^0, y^1, \dots, y^N$ are the approximations of Euler's method for uniform partition of $[a,b]$ with step $h=\frac{b-a}{N}$ then $$\max_{0 \leq n \leq N} |y(t^n)-y^n| \leq \frac{M}{2L} (e^{L(b-a)}-1)h$$ where $M=\max_{a \leq t \leq b} |y''(t)|$. From the above theorem, we conclude that the order of accuracy of Euler's method is at least $1$. We will show that the order of accuracy of Euler's method is exactly $1$. We consider the following ODE: $\left\{\begin{matrix} y'=2t &, 0 \leq t \leq 1 \\  y(0)=0 &  \end{matrix}\right.$ Its only solution is $y(t)=t^2,\ \  0 \leq t \leq 1$. Let $N \in \mathbb{N}, \ h=\frac{1}{N}, \ \ t^n=nh, \ n=0,1, \dots, N$ $$y^{n+1}=y^n+hf(t^n, y^n) \Rightarrow y^{n+1}=y^n+h2t^n=y^n+h2nh=y^n+2nh^2$$ $$y^0=y(0)=0$$ $$y^1=y^0+2 \cdot 0 \cdot h^2=0$$ $$y^2=y^1+2h^2=2h^2$$ $$y^3=y^2+2 \cdot 2 \cdot h^2=2h^2+4h^2=2h^2(1+2)$$ $$y^4=y^3+2 \cdot 3 \cdot h^2=2h^2(1+2)+ 2 \cdot 3 \cdot  h^2=2h^2(1+2+3)$$ $$\dots \dots$$ $$y^n=2h^2 \sum_{i=1}^{n-1} i=2h^2 \frac{(n-1)n}{2}=n(n-1)h^2$$ For $n=N$: $y^N=N(N-1)h^2=(Nh-h) Nh=1-h$ $|y(t^N)-y^N|=|1-1+h|=h$ Theorefore, we conclude that the order of accuracy is exactly $1$. According to the theorem: $$\max_{0 \leq n \leq N} |y(t^n)-y^n| \leq \frac{M}{2L} (e^{L(b-a)}-1)h$$ So, why do we conclude that  the order of accuracy of Euler's method is at least $1$ and not at most $1$? Also,  we take into consideration that the order of accuracy of the method is at least $1$ and then we take an example of an ODE and see that the order of accuracy is exactly $1$.  Why do we conclude in this way something for the general case? How do we  deduce that for each ODE the same  holds?",,"['ordinary-differential-equations', 'numerical-methods']"
33,$X'=AX$ with $\det A=0$ - Plane solutions,with  - Plane solutions,X'=AX \det A=0,"Let $A$ be a $3\times3$ real matrix with $\det A=0$, we consider the system of differential equations defined by $X'=AX$. How can I show that each solution is contained in a plane ? What I have so far : $\det A=0\Rightarrow$ there are constant solutions to $AX=0,X\neq0$. Let $\lambda_0$ be the eigenvalue for $0$ and $E_0$ the set of associated eigenvectors. If $A=0$, it's clear. If $A\neq0$: if $\dim E_0=1:E_0=span(X_0)$. If we were in $\Bbb{C}$, we could say that there are two more eigenvalues $\lambda_1,\lambda_2$ and thus the solutions of $X'=AX$ would be $c_0E_0+c_1E_{\lambda_1}e^{\lambda_1t}+c_2E_{\lambda_2}e^{\lambda_2t}$, which defines a plane. However, I doubt that by going back to $\Bbb{R}$ by taking the real part, that is still the equation of a plane. if $\dim E_0=2$ :  ?","Let $A$ be a $3\times3$ real matrix with $\det A=0$, we consider the system of differential equations defined by $X'=AX$. How can I show that each solution is contained in a plane ? What I have so far : $\det A=0\Rightarrow$ there are constant solutions to $AX=0,X\neq0$. Let $\lambda_0$ be the eigenvalue for $0$ and $E_0$ the set of associated eigenvectors. If $A=0$, it's clear. If $A\neq0$: if $\dim E_0=1:E_0=span(X_0)$. If we were in $\Bbb{C}$, we could say that there are two more eigenvalues $\lambda_1,\lambda_2$ and thus the solutions of $X'=AX$ would be $c_0E_0+c_1E_{\lambda_1}e^{\lambda_1t}+c_2E_{\lambda_2}e^{\lambda_2t}$, which defines a plane. However, I doubt that by going back to $\Bbb{R}$ by taking the real part, that is still the equation of a plane. if $\dim E_0=2$ :  ?",,['ordinary-differential-equations']
34,Solving a first order non-linear homogenous ODE,Solving a first order non-linear homogenous ODE,,"Original problem is: $$ \frac{dy}{dx} = \frac{3y + x}{3x + y} $$ After substituting multiplying the top and bottom by $\frac{1}{x}$ and substituting $v = \frac{y}{x}$ I arrived at $$ \frac{1}{x}dx = \frac{3+v}{1-v^2}dv  $$ After integrating initially using partial fractions and getting: $$ \ln |x| = \frac{3}{2}\ln(1+v) - \frac{3}{2}\ln(1-u) - \frac{1}{2}\ln(1-v^2)  $$ I went to wolfram alpha and found the simplified version of the integral was $$ \ln |x| = \ln(1+v) - 2\ln(1-v) $$ Which made my final answer without solving for y $$ \ln|x| = \ln\frac{1+\frac{y}{x}}{\left(1-\frac{y}{x}\right)^2} $$ The answer in the textbook, however, was: $$ \ln|x| = \ln \left(\left(1+\frac{y}{x}\right) \left(1-\frac{y}{x}\right)^2\right) $$ Where did I go wrong?","Original problem is: $$ \frac{dy}{dx} = \frac{3y + x}{3x + y} $$ After substituting multiplying the top and bottom by $\frac{1}{x}$ and substituting $v = \frac{y}{x}$ I arrived at $$ \frac{1}{x}dx = \frac{3+v}{1-v^2}dv  $$ After integrating initially using partial fractions and getting: $$ \ln |x| = \frac{3}{2}\ln(1+v) - \frac{3}{2}\ln(1-u) - \frac{1}{2}\ln(1-v^2)  $$ I went to wolfram alpha and found the simplified version of the integral was $$ \ln |x| = \ln(1+v) - 2\ln(1-v) $$ Which made my final answer without solving for y $$ \ln|x| = \ln\frac{1+\frac{y}{x}}{\left(1-\frac{y}{x}\right)^2} $$ The answer in the textbook, however, was: $$ \ln|x| = \ln \left(\left(1+\frac{y}{x}\right) \left(1-\frac{y}{x}\right)^2\right) $$ Where did I go wrong?",,['ordinary-differential-equations']
35,Detection of Cycles without a Center in an ODE,Detection of Cycles without a Center in an ODE,,"In my classes in dynamical systems theory, we were taught how to detect cycles or cyclic behavior in an ODE (be it dampened, sustained or growing) around a fixed point by looking at the eigenvalues for that point (their imaginary component in particular). But in higher dimensional systems cycles need not have a center, and in practice one often does not even always know the location of a center even if it exists. Are there any non-local techniques for identifying cyclic behavior in an ODE which would allow me to detect a cycle without a center? Would such techniques similarly allow me to identify strange attractors?","In my classes in dynamical systems theory, we were taught how to detect cycles or cyclic behavior in an ODE (be it dampened, sustained or growing) around a fixed point by looking at the eigenvalues for that point (their imaginary component in particular). But in higher dimensional systems cycles need not have a center, and in practice one often does not even always know the location of a center even if it exists. Are there any non-local techniques for identifying cyclic behavior in an ODE which would allow me to detect a cycle without a center? Would such techniques similarly allow me to identify strange attractors?",,"['ordinary-differential-equations', 'dynamical-systems', 'chaos-theory']"
36,Explicit solution to linear stochastic differential equation (in several dimensions),Explicit solution to linear stochastic differential equation (in several dimensions),,"I have found many references where they provide with a ""explicit"" solution of the following SDE: $$dX_t = (a_1(t) X_t + a_2(t) )dt + (b_1(t) X_t + b_2(t))dB_t, \quad X_0=x, \quad (1)$$ where $B$ is a standard Brownian motion. It is namely given by $$X_t = \Phi_t \left(x + \int_0^t (a_2(s) - b_1(s)b_2(s)) \Phi_s^{-1} ds+ \int_0^t b_2(s)\Phi_s^{-1} dB_s \right)$$ where \begin{align} \Phi_t = \exp \left\{\int_0^t \left(a_1(s)-\frac{1}{2}b_1(s)^2 \right)ds + \int_0^t b_1(s)dB_s\right\}. \quad (2) \end{align} Nevertheless, in several dimensions it might not be possible to find a close explicit solution of the homogeneous SDE and hence expression (2) does not make sense. Can we solve Equation (1) when $a_1,a_2,b_1$ and $b_2$ are square matrices? In exactly the same way by just defining $\Phi_t$ the solution to $d\Phi_t = a_1(t)\Phi_1 dt + b_1(t) \Phi_t dB_t$, $\Phi_0=Id$? I found this link http://math.uni-heidelberg.de/studinfo/reiss/sode-lecture.pdf (page 26) for the case $b_1(t)=0$. Does anyone know any references on this? Thanks!","I have found many references where they provide with a ""explicit"" solution of the following SDE: $$dX_t = (a_1(t) X_t + a_2(t) )dt + (b_1(t) X_t + b_2(t))dB_t, \quad X_0=x, \quad (1)$$ where $B$ is a standard Brownian motion. It is namely given by $$X_t = \Phi_t \left(x + \int_0^t (a_2(s) - b_1(s)b_2(s)) \Phi_s^{-1} ds+ \int_0^t b_2(s)\Phi_s^{-1} dB_s \right)$$ where \begin{align} \Phi_t = \exp \left\{\int_0^t \left(a_1(s)-\frac{1}{2}b_1(s)^2 \right)ds + \int_0^t b_1(s)dB_s\right\}. \quad (2) \end{align} Nevertheless, in several dimensions it might not be possible to find a close explicit solution of the homogeneous SDE and hence expression (2) does not make sense. Can we solve Equation (1) when $a_1,a_2,b_1$ and $b_2$ are square matrices? In exactly the same way by just defining $\Phi_t$ the solution to $d\Phi_t = a_1(t)\Phi_1 dt + b_1(t) \Phi_t dB_t$, $\Phi_0=Id$? I found this link http://math.uni-heidelberg.de/studinfo/reiss/sode-lecture.pdf (page 26) for the case $b_1(t)=0$. Does anyone know any references on this? Thanks!",,"['calculus', 'probability', 'functional-analysis', 'ordinary-differential-equations', 'stochastic-processes']"
37,Where did I go wrong in solving this differential equation by substitution?,Where did I go wrong in solving this differential equation by substitution?,,"I have $\frac{dy}{dx}=\frac{x+3y}{x-y}$ and am trying to solve by substituting $y=xv(x)$.  This results in $xv'+v=\frac{1+3v}{1-v}$.  By seperating the variables and integrating I get $\frac{v-1}{v+1}-\ln\left|{v+1}\right|=\ln\left|{x}\right|+C$.  Finally, substituting $v=\frac yx$ back in and rearranging yields $\frac{y-x}{y+x}-\ln\left|y+x\right|=C$. However, the answer key shows $\frac{2x}{x+y}+\ln\left|x+y\right|=C$ and I am having trouble seeing where I went wrong.","I have $\frac{dy}{dx}=\frac{x+3y}{x-y}$ and am trying to solve by substituting $y=xv(x)$.  This results in $xv'+v=\frac{1+3v}{1-v}$.  By seperating the variables and integrating I get $\frac{v-1}{v+1}-\ln\left|{v+1}\right|=\ln\left|{x}\right|+C$.  Finally, substituting $v=\frac yx$ back in and rearranging yields $\frac{y-x}{y+x}-\ln\left|y+x\right|=C$. However, the answer key shows $\frac{2x}{x+y}+\ln\left|x+y\right|=C$ and I am having trouble seeing where I went wrong.",,['ordinary-differential-equations']
38,Solving the differential equation $\frac{y'}{x\sqrt{1+y'^2}}=C$,Solving the differential equation,\frac{y'}{x\sqrt{1+y'^2}}=C,"I have tried solving the differential equation: $$\frac{y'}{x\sqrt{1+y'^2}}=C,$$ where $y=y(x),\;y'=dy/dx$ and $C$ is a constant. The solution should be a circle apparently. My boundary conditions are $y(1)=0,\;y(2)=1$. Here is my attempt (UPDATE I added LaTex and corrected my solution by using Prajakta's hint): $$\frac{y'}{x\sqrt{1+y'^2}}=C$$ $$\frac{y'^2}{1+y'^2}=Dx^2$$ $$y'^2(1-Dx^2)=Dx^2$$ $$y'^2=\frac{Dx^2}{1-Dx^2}$$ $$y'=\pm\frac{\sqrt{D}x}{1-Dx^2}$$ $$\int \;dy=\pm\int \frac{\sqrt{D}x}{1-Dx^2}\;dx$$ $$u=1-Dx^2,\;\;du/dx=-2Dx,\;\rightarrow\;-\frac{1}{2D}\;du=x\;dx$$ $$y=\pm\int \frac{\sqrt{D}x}{1-Dx^2}\;dx=\pm\frac{\sqrt{D}}{2D}\int \frac{du}{\sqrt{u}}=\pm\frac{\sqrt{D}}{2D}\left(2\sqrt{u}+H\right)$$ $$y=\pm \frac{\sqrt{D}}{D}\left(\sqrt{1-Dx^2}+H\right)$$ $$y=\pm \frac{\sqrt{D}}{D}\left(\sqrt{D\left(\frac{1}{D}-x^2\right)}+H\right)$$ $$y=\pm \sqrt{\frac{1}{D}-x^2}+N$$ Thank you =)","I have tried solving the differential equation: $$\frac{y'}{x\sqrt{1+y'^2}}=C,$$ where $y=y(x),\;y'=dy/dx$ and $C$ is a constant. The solution should be a circle apparently. My boundary conditions are $y(1)=0,\;y(2)=1$. Here is my attempt (UPDATE I added LaTex and corrected my solution by using Prajakta's hint): $$\frac{y'}{x\sqrt{1+y'^2}}=C$$ $$\frac{y'^2}{1+y'^2}=Dx^2$$ $$y'^2(1-Dx^2)=Dx^2$$ $$y'^2=\frac{Dx^2}{1-Dx^2}$$ $$y'=\pm\frac{\sqrt{D}x}{1-Dx^2}$$ $$\int \;dy=\pm\int \frac{\sqrt{D}x}{1-Dx^2}\;dx$$ $$u=1-Dx^2,\;\;du/dx=-2Dx,\;\rightarrow\;-\frac{1}{2D}\;du=x\;dx$$ $$y=\pm\int \frac{\sqrt{D}x}{1-Dx^2}\;dx=\pm\frac{\sqrt{D}}{2D}\int \frac{du}{\sqrt{u}}=\pm\frac{\sqrt{D}}{2D}\left(2\sqrt{u}+H\right)$$ $$y=\pm \frac{\sqrt{D}}{D}\left(\sqrt{1-Dx^2}+H\right)$$ $$y=\pm \frac{\sqrt{D}}{D}\left(\sqrt{D\left(\frac{1}{D}-x^2\right)}+H\right)$$ $$y=\pm \sqrt{\frac{1}{D}-x^2}+N$$ Thank you =)",,['ordinary-differential-equations']
39,Finding constants in intial value problems,Finding constants in intial value problems,,"I have some problems in determining constants in initial value problems, especially for linear nonhomogenous differential equations. Use this as an example: $$x'(t) = 2t - 2x(t) + 5, x(0) = -1$$ We are basically allowed to simply solve these by using the formula for nonhomogenous ODEs: $$ y = (\int_0^t b(t)e^{A(t)} dt + c)e^{-A(t)}$$ So in this case I have: $a(t) = 2, b(t) = 2t + 5$ and therefore: $$y = (\int_0^t(2t+5)e^{2t} dt + c)e^{-2t}$$ for which I get: $$y=(\frac{1}{2}e^{2t}(2t-1)+\frac{1}{2}e^0 + \frac{5}{2}e^{2t}-\frac{5}{2}+c)e^{-2t}$$ $$y = t-2e^{-2t}+2+ce^{-2t}$$ Now what value should the constant take? First of all is it correct to simply carry the c ""with me"" in the calculations? I know that ultimately the first c comes from the integration of $a(t)$, then after the term in the braces is integrated, that c changes. So if I get this term in the end: $$y = t-2e^{-2t}+2+ce^{-2t}$$ Can I simply determine c by plugging in $t=0$ and setting the value of that expression equal to -1? $$y(0)=0-2e^0+2+ce^0=-1$$ And then solve for c? Would that be correct?","I have some problems in determining constants in initial value problems, especially for linear nonhomogenous differential equations. Use this as an example: $$x'(t) = 2t - 2x(t) + 5, x(0) = -1$$ We are basically allowed to simply solve these by using the formula for nonhomogenous ODEs: $$ y = (\int_0^t b(t)e^{A(t)} dt + c)e^{-A(t)}$$ So in this case I have: $a(t) = 2, b(t) = 2t + 5$ and therefore: $$y = (\int_0^t(2t+5)e^{2t} dt + c)e^{-2t}$$ for which I get: $$y=(\frac{1}{2}e^{2t}(2t-1)+\frac{1}{2}e^0 + \frac{5}{2}e^{2t}-\frac{5}{2}+c)e^{-2t}$$ $$y = t-2e^{-2t}+2+ce^{-2t}$$ Now what value should the constant take? First of all is it correct to simply carry the c ""with me"" in the calculations? I know that ultimately the first c comes from the integration of $a(t)$, then after the term in the braces is integrated, that c changes. So if I get this term in the end: $$y = t-2e^{-2t}+2+ce^{-2t}$$ Can I simply determine c by plugging in $t=0$ and setting the value of that expression equal to -1? $$y(0)=0-2e^0+2+ce^0=-1$$ And then solve for c? Would that be correct?",,['ordinary-differential-equations']
40,Differential Equation with Substitution,Differential Equation with Substitution,,"I have this Differential Equation: $$(2x^2 + y^2) \frac{dy}{dx} = 2xy$$ and I solved it by substituting $y=vx$ and $\frac{dy}{dx}=\frac{dv}{dx} (x +v)$ I found a result of $y^2(\ln(y) +C)=x^2$, but since it must be an implicit solution for $y$ I don't know whether I should leave it in this form. Also I hope I didn't do anything wrong in the proccess. Any help would be much appreciated","I have this Differential Equation: $$(2x^2 + y^2) \frac{dy}{dx} = 2xy$$ and I solved it by substituting $y=vx$ and $\frac{dy}{dx}=\frac{dv}{dx} (x +v)$ I found a result of $y^2(\ln(y) +C)=x^2$, but since it must be an implicit solution for $y$ I don't know whether I should leave it in this form. Also I hope I didn't do anything wrong in the proccess. Any help would be much appreciated",,['ordinary-differential-equations']
41,"Non-linear ODE $1+y'^2=yy'^2$, $y(0)=b,y(a)=c$","Non-linear ODE ,","1+y'^2=yy'^2 y(0)=b,y(a)=c","How to solve $$1+y'^2=yy'^2?$$ The initial condition is $y(0)=b,y(a)=c$. It is a non-linear ODE. Some hint is also helpful! Thanks for your help!","How to solve $$1+y'^2=yy'^2?$$ The initial condition is $y(0)=b,y(a)=c$. It is a non-linear ODE. Some hint is also helpful! Thanks for your help!",,"['ordinary-differential-equations', 'initial-value-problems']"
42,Solve for $y'' - 4y=0$,Solve for,y'' - 4y=0,I wanted to check my answer. So I have the characteristic equation: $$r^{2}-4=0\Rightarrow r^{2} = 4 \Rightarrow r = \pm 2.$$ So the general solution is: $$y = c_{1}e^{-2t}+c_{2}e^{2t}$$ Is this correct?,I wanted to check my answer. So I have the characteristic equation: $$r^{2}-4=0\Rightarrow r^{2} = 4 \Rightarrow r = \pm 2.$$ So the general solution is: $$y = c_{1}e^{-2t}+c_{2}e^{2t}$$ Is this correct?,,['ordinary-differential-equations']
43,Finding the equilibrium solutions of a logistic equation,Finding the equilibrium solutions of a logistic equation,,"Given a logistic equation  $$dy/dt = r(1 − y/K)y − Ey$$ (a) Show that if $E < r$, then there are two equilibrium points, $y_{1} = 0$ and $y_{2} = K(1 −E/r) > 0$. (b) Show that $y = y_{1}$ is unstable and $y = y_{2}$ is asymptotically stable. What i tried, Equating $dy/dt=0$, the eqution becomes  $$r(1 − y/K)y − Ey=0$$.  Taking the common facter $y$ out, i got $y=0$ which corresponds to $y_{1}$ while the second portion becomes   $$r(1 − y/K) − E=0$$, then solving for $y$, i got $$y=1-(Ek/r)=0$$. But that dosent seem to be simillar to $y_{2}$. I might have missed out something. Could anyone please explain. Thanks","Given a logistic equation  $$dy/dt = r(1 − y/K)y − Ey$$ (a) Show that if $E < r$, then there are two equilibrium points, $y_{1} = 0$ and $y_{2} = K(1 −E/r) > 0$. (b) Show that $y = y_{1}$ is unstable and $y = y_{2}$ is asymptotically stable. What i tried, Equating $dy/dt=0$, the eqution becomes  $$r(1 − y/K)y − Ey=0$$.  Taking the common facter $y$ out, i got $y=0$ which corresponds to $y_{1}$ while the second portion becomes   $$r(1 − y/K) − E=0$$, then solving for $y$, i got $$y=1-(Ek/r)=0$$. But that dosent seem to be simillar to $y_{2}$. I might have missed out something. Could anyone please explain. Thanks",,['ordinary-differential-equations']
44,Why aren't my Laplace transform and Undetermind Coefficients answers matching up?,Why aren't my Laplace transform and Undetermind Coefficients answers matching up?,,"I might be losing my mind this morning (I am, for sure), but I can't these two techniques to give me the same answer to a basic differential equations problem. The problem is $y''-8y'+27y=0$ with the initial conditions $y(0)=-8$ and $y'(0)=5$.  Using Laplace transforms, I get the solution $y=\frac{69}{\sqrt{11}}e^{4t}\sin(\sqrt{11}t)-8e^{4t}\cos(\sqrt{11}t)$, which is wrong. If I use Undetermined Coefficients, I get $y=\frac{37}{\sqrt{11}}e^{4t}\sin(\sqrt{11}t)-8e^{4t}\cos(\sqrt{11}t)$, which is right. The problem is that I can't find my mistake with the LT technique.  The process is straight forward enough: $\mathcal{L}(y''-8y'+27y)=\mathcal{L}(s^2-8s+27)-s(-8)-5+8(-8)=\mathcal{L}(s^2-8s+27)+8s-69=0$ $\mathcal{L}(y)=\frac{69-8s}{s^2-8s+16+11}=\frac{69-8s}{(s-4)^2+11}$ Then I split up the fraction, scale by $\frac{69}{\sqrt{11}}$ for the $\sin$ term and I'm done.  The $\cos$ term is right, but that 69 is the bugaboo for some reason. Please point out the idiotic thing I missed!","I might be losing my mind this morning (I am, for sure), but I can't these two techniques to give me the same answer to a basic differential equations problem. The problem is $y''-8y'+27y=0$ with the initial conditions $y(0)=-8$ and $y'(0)=5$.  Using Laplace transforms, I get the solution $y=\frac{69}{\sqrt{11}}e^{4t}\sin(\sqrt{11}t)-8e^{4t}\cos(\sqrt{11}t)$, which is wrong. If I use Undetermined Coefficients, I get $y=\frac{37}{\sqrt{11}}e^{4t}\sin(\sqrt{11}t)-8e^{4t}\cos(\sqrt{11}t)$, which is right. The problem is that I can't find my mistake with the LT technique.  The process is straight forward enough: $\mathcal{L}(y''-8y'+27y)=\mathcal{L}(s^2-8s+27)-s(-8)-5+8(-8)=\mathcal{L}(s^2-8s+27)+8s-69=0$ $\mathcal{L}(y)=\frac{69-8s}{s^2-8s+16+11}=\frac{69-8s}{(s-4)^2+11}$ Then I split up the fraction, scale by $\frac{69}{\sqrt{11}}$ for the $\sin$ term and I'm done.  The $\cos$ term is right, but that 69 is the bugaboo for some reason. Please point out the idiotic thing I missed!",,"['ordinary-differential-equations', 'laplace-transform']"
45,"the boundary value problem: $u''(x)+\lambda u(x)=0,x\in (0,1),$ $u(0)=u(1); u'(0)=u'(1).$",the boundary value problem:,"u''(x)+\lambda u(x)=0,x\in (0,1), u(0)=u(1); u'(0)=u'(1).","Find all possible $(\lambda,u)$ where $\lambda \in \mathbb R$ and $u\ne0$, to the boundary value problem: $u''(x)+\lambda u(x)=0,x\in (0,1),$ $u(0)=u(1); u'(0)=u'(1).$ My Effort: for $\lambda>0,u(x)=A_n\cos\sqrt\lambda_nx+B_n\sin\sqrt\lambda_nx$, where$\lambda_n=4n\pi^2$(After Calculation), $n=\pm1,\pm2,....$ is it correct? Please verify.What will be the case if $\lambda<0$. I am confused. Please help.","Find all possible $(\lambda,u)$ where $\lambda \in \mathbb R$ and $u\ne0$, to the boundary value problem: $u''(x)+\lambda u(x)=0,x\in (0,1),$ $u(0)=u(1); u'(0)=u'(1).$ My Effort: for $\lambda>0,u(x)=A_n\cos\sqrt\lambda_nx+B_n\sin\sqrt\lambda_nx$, where$\lambda_n=4n\pi^2$(After Calculation), $n=\pm1,\pm2,....$ is it correct? Please verify.What will be the case if $\lambda<0$. I am confused. Please help.",,"['ordinary-differential-equations', 'boundary-value-problem']"
46,"Heat Equation, possible solutions","Heat Equation, possible solutions",,"NOTE: This is a homework problem. Please do not solve. I was given a problem that asked me to find a function of the form $u_n(x,t)=\chi_n(x) \cdot T_n(t) $ that solves the heat equation with the following conditions: $u_t = u_{xx}\\ u(0,t)=0\\ u_x(1,t)=0$ That is all of the information, but I am unsure how to solve such a problem without an initial heat distribution. I considered using the final condition, but all that tells me is that $u(1,t) = f(t)$ for some function $f(t)$, not what that $f(t)$ could be. Is a solution even possible based on this information?","NOTE: This is a homework problem. Please do not solve. I was given a problem that asked me to find a function of the form $u_n(x,t)=\chi_n(x) \cdot T_n(t) $ that solves the heat equation with the following conditions: $u_t = u_{xx}\\ u(0,t)=0\\ u_x(1,t)=0$ That is all of the information, but I am unsure how to solve such a problem without an initial heat distribution. I considered using the final condition, but all that tells me is that $u(1,t) = f(t)$ for some function $f(t)$, not what that $f(t)$ could be. Is a solution even possible based on this information?",,"['ordinary-differential-equations', 'partial-differential-equations']"
47,Using the Existence and uniqueness theorem to find the interval of validity of a unique solution in a Linear ODE,Using the Existence and uniqueness theorem to find the interval of validity of a unique solution in a Linear ODE,,"From the existness and uniqueness Theorem,the initial value problem $$y'=3x(y-1)^{1/3}            ,          y(3)=-7$$ has a unique solution on some open interval that contains $x=3$. Find the solution and determine the largest open interval on which it’s unique. What i tried,              First i tried to solve the equation by the seperable equation method to get,$$y=1+(x^2-5)^{1.5}$$. Then from here i used the existence and uniquness theorem to calculate $f(x,y)$ and $f_{y}$. From the calculations,when $y$ not equals to $1$, $f_{y}$ will be continuous, hence there will be a unique solution when $y$ not equals to $1$, according to the wxistness and uniquness theorem. However im stuck from here onwards as to finding the interval of validaty. Is my working correct. Could anyone explain. Thanks","From the existness and uniqueness Theorem,the initial value problem $$y'=3x(y-1)^{1/3}            ,          y(3)=-7$$ has a unique solution on some open interval that contains $x=3$. Find the solution and determine the largest open interval on which it’s unique. What i tried,              First i tried to solve the equation by the seperable equation method to get,$$y=1+(x^2-5)^{1.5}$$. Then from here i used the existence and uniquness theorem to calculate $f(x,y)$ and $f_{y}$. From the calculations,when $y$ not equals to $1$, $f_{y}$ will be continuous, hence there will be a unique solution when $y$ not equals to $1$, according to the wxistness and uniquness theorem. However im stuck from here onwards as to finding the interval of validaty. Is my working correct. Could anyone explain. Thanks",,['ordinary-differential-equations']
48,Differential Equations Hermitian Matrix Proof,Differential Equations Hermitian Matrix Proof,,"I would like some help with the following proof below. Thanks for any help in advance. Prove that if $u(t) ∈ \mathbb C^N$ is a solution to the initial value problem $iu’ =Au$, $u(0)=u_0$, where $A$ is hermitian, then $||u(t)|| = ||u_0||$.","I would like some help with the following proof below. Thanks for any help in advance. Prove that if $u(t) ∈ \mathbb C^N$ is a solution to the initial value problem $iu’ =Au$, $u(0)=u_0$, where $A$ is hermitian, then $||u(t)|| = ||u_0||$.",,"['matrices', 'ordinary-differential-equations']"
49,Why is it that while taking the inverse matrix a Wronskian pops up in this solution?,Why is it that while taking the inverse matrix a Wronskian pops up in this solution?,,"I was working on an ordinary differential equation solution when I saw another way that could be used to solve using matrices such that \begin{align*} \left(\begin{matrix} y_1\left(x\right) && y_2\left(x\right) \\ y_1'\left(x\right) && y_2'\left(x\right) \end{matrix}\right)\bigg(\begin{matrix} v_1'\left(x\right) \\ v_2'\left(x\right) \end{matrix}\bigg)&=\left(\begin{matrix}0 \\ \frac{G\left(x\right)}{a}\end{matrix}\right)\tag{1} \\ \implies\int\left(\begin{matrix}v_1'\left(x\right) \\ v_2'\left(x\right)\end{matrix}\right) & =\int\frac{\left(\begin{matrix}0 \\ \frac{G\left(x\right)}{a}\end{matrix}\right)}{\left(\begin{matrix} y_1\left(x\right) && y_2\left(x\right) \\ y_1'\left(x\right) && y_2'\left(x\right) \end{matrix}\right)}\:dx\tag{2} \end{align*} But now, I've seen that (2) is re-written as \begin{align} \frac{\left(\begin{matrix}0 \\ \frac{G\left(x\right)}{a}\end{matrix}\right)}{\left(\begin{matrix} y_1\left(x\right) && y_2\left(x\right) \\ y_1'\left(x\right) && y_2'\left(x\right) \end{matrix}\right)}&=\frac{1}{W\left[y_1,y_2\right]\left(x\right)}\left(\begin{matrix} y_1\left(x\right) && y_2\left(x\right) \\ y_1'\left(x\right) && y_2'\left(x\right) \end{matrix}\right)^{-1}\tag{3}\\ &=\frac{1}{W\left[y_1,y_2\right]\left(x\right)}\left(\begin{matrix} y_2'\left(x\right) && -y_2\left(x\right) \\ -y_1'\left(x\right) && y_1\left(x\right) \end{matrix}\right)\tag{4}\end{align} So my question is, why is the left-hand side of (3) the same as (4)? I have not taken linear algebra as of yet, but it seems the inverse of the matrix is taken because its inverse on top is the same as it regularly stands in the denominator. I understood the rest of the problem, it's just this step that I couldn't understand. It seems like introducing a Wronskian, just to ensure that the solutions are linearly independent, is like adding an extra term. But it definitely adds up to give the same results (somehow) since I still end up with \begin{align} v_1\left(x\right) & = -\int\frac{y_2\left(x\right)f\left(x\right)\:dx}{aW\left[y_1,y_2\right]\left(x\right)},\tag{5} \\ v_2\left(x\right) & =\int\frac{y_1\left(x\right)f\left(x\right)\:dx}{aW\left[y_1,y_2\right]\left(x\right)},\tag{6} \end{align} such that $f\left(x\right)$ is the forcing term on the RHS of the original ODE. Thank you for your time,","I was working on an ordinary differential equation solution when I saw another way that could be used to solve using matrices such that \begin{align*} \left(\begin{matrix} y_1\left(x\right) && y_2\left(x\right) \\ y_1'\left(x\right) && y_2'\left(x\right) \end{matrix}\right)\bigg(\begin{matrix} v_1'\left(x\right) \\ v_2'\left(x\right) \end{matrix}\bigg)&=\left(\begin{matrix}0 \\ \frac{G\left(x\right)}{a}\end{matrix}\right)\tag{1} \\ \implies\int\left(\begin{matrix}v_1'\left(x\right) \\ v_2'\left(x\right)\end{matrix}\right) & =\int\frac{\left(\begin{matrix}0 \\ \frac{G\left(x\right)}{a}\end{matrix}\right)}{\left(\begin{matrix} y_1\left(x\right) && y_2\left(x\right) \\ y_1'\left(x\right) && y_2'\left(x\right) \end{matrix}\right)}\:dx\tag{2} \end{align*} But now, I've seen that (2) is re-written as \begin{align} \frac{\left(\begin{matrix}0 \\ \frac{G\left(x\right)}{a}\end{matrix}\right)}{\left(\begin{matrix} y_1\left(x\right) && y_2\left(x\right) \\ y_1'\left(x\right) && y_2'\left(x\right) \end{matrix}\right)}&=\frac{1}{W\left[y_1,y_2\right]\left(x\right)}\left(\begin{matrix} y_1\left(x\right) && y_2\left(x\right) \\ y_1'\left(x\right) && y_2'\left(x\right) \end{matrix}\right)^{-1}\tag{3}\\ &=\frac{1}{W\left[y_1,y_2\right]\left(x\right)}\left(\begin{matrix} y_2'\left(x\right) && -y_2\left(x\right) \\ -y_1'\left(x\right) && y_1\left(x\right) \end{matrix}\right)\tag{4}\end{align} So my question is, why is the left-hand side of (3) the same as (4)? I have not taken linear algebra as of yet, but it seems the inverse of the matrix is taken because its inverse on top is the same as it regularly stands in the denominator. I understood the rest of the problem, it's just this step that I couldn't understand. It seems like introducing a Wronskian, just to ensure that the solutions are linearly independent, is like adding an extra term. But it definitely adds up to give the same results (somehow) since I still end up with \begin{align} v_1\left(x\right) & = -\int\frac{y_2\left(x\right)f\left(x\right)\:dx}{aW\left[y_1,y_2\right]\left(x\right)},\tag{5} \\ v_2\left(x\right) & =\int\frac{y_1\left(x\right)f\left(x\right)\:dx}{aW\left[y_1,y_2\right]\left(x\right)},\tag{6} \end{align} such that $f\left(x\right)$ is the forcing term on the RHS of the original ODE. Thank you for your time,",,"['matrices', 'ordinary-differential-equations']"
50,"Solve $\textbf y'=A\textbf y$ with $\textbf y\in \mathbb R^4$ and $A\in \text{Mat}(4\times 4,\mathbb R)$",Solve  with  and,"\textbf y'=A\textbf y \textbf y\in \mathbb R^4 A\in \text{Mat}(4\times 4,\mathbb R)","We consider   $$\textbf y'(t)=A\textbf y(t)$$   with $\textbf y(0)=\textbf y_0\in \mathbb R^4$ and $A\in \text{Mat}(4\times 4,\mathbb R)$. Let $\textbf y_1,\textbf y_2,\textbf y_3,\textbf y_4\in\mathbb R^4$ linearly independant and $\lambda_1\neq \lambda_2\neq \lambda_3\in\mathbb C$ s.t.   $$A\textbf y_1=\lambda_1\textbf y_1$$   $$A\textbf y_2=\lambda_2\textbf y_2$$   $$A\textbf y_3=\lambda_3\textbf y_3$$   $$(A-\lambda_3)\textbf y_4=\textbf y_3.$$ Write the solution $\textbf y(t)$ in function of $\textbf y_0, \textbf y_i$ and $\lambda_i$. Give a condition such that $$\limsup_{t\to\infty }\textbf y(t)<\infty .$$ I agree that $\textbf y_1,\textbf y_2,\textbf y_3$ are eigenvectors and $\lambda_1,\lambda_2,\lambda_3$ eigenvalues, but what can I do with $\textbf y_4$ ? Because I can not diagonalize this matrix. And even if I could, I would like to calculate the $e^A$, but it doesn't look easy since I don't have the change of basis. It's probably not a complicate exercise, but like that, I can't continue.","We consider   $$\textbf y'(t)=A\textbf y(t)$$   with $\textbf y(0)=\textbf y_0\in \mathbb R^4$ and $A\in \text{Mat}(4\times 4,\mathbb R)$. Let $\textbf y_1,\textbf y_2,\textbf y_3,\textbf y_4\in\mathbb R^4$ linearly independant and $\lambda_1\neq \lambda_2\neq \lambda_3\in\mathbb C$ s.t.   $$A\textbf y_1=\lambda_1\textbf y_1$$   $$A\textbf y_2=\lambda_2\textbf y_2$$   $$A\textbf y_3=\lambda_3\textbf y_3$$   $$(A-\lambda_3)\textbf y_4=\textbf y_3.$$ Write the solution $\textbf y(t)$ in function of $\textbf y_0, \textbf y_i$ and $\lambda_i$. Give a condition such that $$\limsup_{t\to\infty }\textbf y(t)<\infty .$$ I agree that $\textbf y_1,\textbf y_2,\textbf y_3$ are eigenvectors and $\lambda_1,\lambda_2,\lambda_3$ eigenvalues, but what can I do with $\textbf y_4$ ? Because I can not diagonalize this matrix. And even if I could, I would like to calculate the $e^A$, but it doesn't look easy since I don't have the change of basis. It's probably not a complicate exercise, but like that, I can't continue.",,['ordinary-differential-equations']
51,yet another simple Laplace transform,yet another simple Laplace transform,,"what is $ℒ(t^2e^{3t})$ I have got this far so far: $=\int_{0}^\infty (t^2e^{t(3-s)})$ Integration by parts using: $u = t^2$ and $du = 2t$ $v = \frac{e^{t(3-2)}}{3-s}$ and $dv = e^{t(3-s)}$ Which I think yields: $0 - \int_{0}^\infty \frac{2t}{3-s} e^{t(3-2)}$ and now i'm stuck. if someone could do a step by step instructions, that would be so helpful!","what is $ℒ(t^2e^{3t})$ I have got this far so far: $=\int_{0}^\infty (t^2e^{t(3-s)})$ Integration by parts using: $u = t^2$ and $du = 2t$ $v = \frac{e^{t(3-2)}}{3-s}$ and $dv = e^{t(3-s)}$ Which I think yields: $0 - \int_{0}^\infty \frac{2t}{3-s} e^{t(3-2)}$ and now i'm stuck. if someone could do a step by step instructions, that would be so helpful!",,"['integration', 'ordinary-differential-equations', 'exponential-function', 'laplace-transform']"
52,"Solving a differential equation $F-y'F_{y'}=C$, with $F(y,y')= \frac{1+2y'^2}{3y^3\sqrt{1+y'^2}}$","Solving a differential equation , with","F-y'F_{y'}=C F(y,y')= \frac{1+2y'^2}{3y^3\sqrt{1+y'^2}}","If $$F= F(y,y')= \frac{1+2y'^2}{3y^3\sqrt{1+y'^2}},$$ where $y=y(x)$ and $y'= y'(x)=\frac{dy}{dx}$, then how to solve the differential equation: $$F-y'F_{y'}=C, $$ that is: $$F(y,y')-\frac{dy}{dx}\frac{\partial F}{\partial y'}=C,$$ where $C$ is some constant? What are the keys steps? I got a very hairy differential equation when I tried solving this directly by doing the calculations. Is there some substitution trick etc. that I should apply here?","If $$F= F(y,y')= \frac{1+2y'^2}{3y^3\sqrt{1+y'^2}},$$ where $y=y(x)$ and $y'= y'(x)=\frac{dy}{dx}$, then how to solve the differential equation: $$F-y'F_{y'}=C, $$ that is: $$F(y,y')-\frac{dy}{dx}\frac{\partial F}{\partial y'}=C,$$ where $C$ is some constant? What are the keys steps? I got a very hairy differential equation when I tried solving this directly by doing the calculations. Is there some substitution trick etc. that I should apply here?",,"['ordinary-differential-equations', 'calculus-of-variations']"
53,How do you tell where an ODE has singularities?,How do you tell where an ODE has singularities?,,"For a linear differential equation, such as $$y'' + p(x)\ y' + q(x)\ y=0$$ we know that the solutions may be singular at points where $p$ or $q$ have singularities, but must be regular everywhere else. But if you have a nonlinear second-order ODE $$y'' = F(x,y,y')$$ together with initial conditions $y(0)$ and $y'(0)$, can you predict the values of $x$ where the solutions may be singular, by looking at $F$? Here $F$ is some given function that is Lipschitz in $y$ and $y'$, so that a unique solution exists in a neighbourhood of $0$.","For a linear differential equation, such as $$y'' + p(x)\ y' + q(x)\ y=0$$ we know that the solutions may be singular at points where $p$ or $q$ have singularities, but must be regular everywhere else. But if you have a nonlinear second-order ODE $$y'' = F(x,y,y')$$ together with initial conditions $y(0)$ and $y'(0)$, can you predict the values of $x$ where the solutions may be singular, by looking at $F$? Here $F$ is some given function that is Lipschitz in $y$ and $y'$, so that a unique solution exists in a neighbourhood of $0$.",,"['real-analysis', 'complex-analysis', 'ordinary-differential-equations']"
54,Using Linear Transformations to Solve Differential Equation,Using Linear Transformations to Solve Differential Equation,,"Define $T:P_3 -> P_3$ by $T(f)(t) = 2f(t)+(1-t)f'(t)$. a) Show that T is a linear transformation. b) Give the matrix representing T with respect to the ""standard basis"" {$1,t,t^2,t^3$}. c) Determine ker(T) and Image(T). d) Let $g(t) = 1+2t$. Use your answer from (b) to find a solution to the differential eqn $T(f)=g$. e) What are all the solutions of $T(f)=g$? Here is my attempt. Please critique my work and help me with the questions I am stuck on. a) Was pretty easy. Will spare y'all the details. b) Let $f(t) = a+bt+ct^2+dt^3.$ $T(f(t)) = 2a+2b+2c+2d+(1-t)(b+2ct+3dt^2)$ $ =(2a+b)+(b+2c)t+3dt^2+(-dt^3)$ So $T=\begin{bmatrix} 2&1&0&0\\0&1&2&0\\0&0&0&3\\0&0&0&-1\end{bmatrix}$. c) Using (b), we write: $2x_1+x_2=0$ $x_2+2x_3=0$ $x_4=0$ so $ker(T) = \begin{bmatrix} 1\\-2\\1\\0 \end{bmatrix}$ Image(T) = span{$\begin{bmatrix} 2\\0\\0\\0 \end{bmatrix}$, $\begin{bmatrix} 0\\2\\0\\0 \end{bmatrix}$, $\begin{bmatrix} 0\\0\\3\\-1 \end{bmatrix}$}. d) I set: $\begin{bmatrix} 2&1&0&0\\0&1&2&0\\0&0&0&3\\0&0&0&-1\end{bmatrix} x = \begin{bmatrix} 1\\2\\0\\0\end{bmatrix}$. and got $x = \begin{bmatrix} 0.25\\0.5\\0.75\\0\end{bmatrix}$. How do I interpret this? e) Not sure how to do this part without understanding (d).","Define $T:P_3 -> P_3$ by $T(f)(t) = 2f(t)+(1-t)f'(t)$. a) Show that T is a linear transformation. b) Give the matrix representing T with respect to the ""standard basis"" {$1,t,t^2,t^3$}. c) Determine ker(T) and Image(T). d) Let $g(t) = 1+2t$. Use your answer from (b) to find a solution to the differential eqn $T(f)=g$. e) What are all the solutions of $T(f)=g$? Here is my attempt. Please critique my work and help me with the questions I am stuck on. a) Was pretty easy. Will spare y'all the details. b) Let $f(t) = a+bt+ct^2+dt^3.$ $T(f(t)) = 2a+2b+2c+2d+(1-t)(b+2ct+3dt^2)$ $ =(2a+b)+(b+2c)t+3dt^2+(-dt^3)$ So $T=\begin{bmatrix} 2&1&0&0\\0&1&2&0\\0&0&0&3\\0&0&0&-1\end{bmatrix}$. c) Using (b), we write: $2x_1+x_2=0$ $x_2+2x_3=0$ $x_4=0$ so $ker(T) = \begin{bmatrix} 1\\-2\\1\\0 \end{bmatrix}$ Image(T) = span{$\begin{bmatrix} 2\\0\\0\\0 \end{bmatrix}$, $\begin{bmatrix} 0\\2\\0\\0 \end{bmatrix}$, $\begin{bmatrix} 0\\0\\3\\-1 \end{bmatrix}$}. d) I set: $\begin{bmatrix} 2&1&0&0\\0&1&2&0\\0&0&0&3\\0&0&0&-1\end{bmatrix} x = \begin{bmatrix} 1\\2\\0\\0\end{bmatrix}$. and got $x = \begin{bmatrix} 0.25\\0.5\\0.75\\0\end{bmatrix}$. How do I interpret this? e) Not sure how to do this part without understanding (d).",,"['linear-algebra', 'ordinary-differential-equations', 'linear-transformations']"
55,Proving that the solutions of a system of ODE $\frac{dx}{dt}=-\nabla V(x)$ are defined for all positive time.,Proving that the solutions of a system of ODE  are defined for all positive time.,\frac{dx}{dt}=-\nabla V(x),"Let $V:\mathbb{R}^n \to \mathbb{R}$ s.t $V\in C^2$ and $\lim_{x\to \infty} V(x)=\infty$. Prove that the system:  $$\frac{dx}{dt}=-\nabla V(x)$$  has defined solutions for all positive time. My questions: is the hypothesis right? How can I use the limit? Also, thanks for any suggestions.","Let $V:\mathbb{R}^n \to \mathbb{R}$ s.t $V\in C^2$ and $\lim_{x\to \infty} V(x)=\infty$. Prove that the system:  $$\frac{dx}{dt}=-\nabla V(x)$$  has defined solutions for all positive time. My questions: is the hypothesis right? How can I use the limit? Also, thanks for any suggestions.",,"['real-analysis', 'ordinary-differential-equations']"
56,A property of a solution of a differential problem.,A property of a solution of a differential problem.,,"Let $f \in L^2(0,1)$ such that $f(x)=f(-x)$ a.e. in $(-1,1)$ and let $u$ be the solution of the problem $$ -u''(x)+u(x)=f(x) \,\,\,\, x \in (-1,1)$$ with the condition $$ u(-1)=u(1)=0 $$ Can I deduce that $u(x)=u(-x)$?","Let $f \in L^2(0,1)$ such that $f(x)=f(-x)$ a.e. in $(-1,1)$ and let $u$ be the solution of the problem $$ -u''(x)+u(x)=f(x) \,\,\,\, x \in (-1,1)$$ with the condition $$ u(-1)=u(1)=0 $$ Can I deduce that $u(x)=u(-x)$?",,"['analysis', 'ordinary-differential-equations']"
57,Can't find adequate change of variables $y=u^{\alpha}x^{\beta}$ for this equation.,Can't find adequate change of variables  for this equation.,y=u^{\alpha}x^{\beta},"I'm studying for a test on ODEs by running through a bunch of exercises, and this one has me stuck. I'd like to find the general solution to $$xy^{\prime} = \sqrt{y^2+x^6}$$ The hint they give me is to use a substitution $y = u^{\alpha}x^{\beta}$. The first thing I did was (and in this removing $0$ from the domain of $y(x)$, at least for the time being) divide out to get $$y^{\prime} = \sqrt{\frac{y^2+x^6}{x^2}}$$ After some playing around I figured that, for the substitution to be useful, it should change the equation into a separable one, which could happen if, for instance, $y^2/x^2 = u^{2\alpha}x^{2\beta-2}$ were equal to $x^3u^{\gamma}$, so I could take a common factor of $x^{3/2}$, and the other factor would only have terms in $u$. So I tried $y = ux^{5/2}$. This got me to the expression $$u' = \frac{1}{x^{5/2}}\left(\sqrt{u^3 + x^3}-5x^{3/2}u\right) \Rightarrow \frac{u'}{\sqrt{u^2+1} - 5u} = \frac1x$$ Which in theory can be integrated quite easily. However the LHS is clearly not a simple integral (for humans; a CAS solves it in no time). It can be done with some hyperbolic trig. substitutions, but on the whole it becomes quite ugly. What's more, I'd end up having [complicated expression in u] = $\ln x + k$, and that won't help me find $u$ explicitly, let alone $y$. Then I tried doing the same without a determined value for $\alpha$, to see if a certain value would make the LHS have an easy primitive. This is to say, I let $y=u^{\alpha}x^{5/2}$ and I got: $$\frac{\alpha u'u^{\alpha-1}}{\sqrt{u^{2\alpha}+1} - 5u} = \frac1x$$ However I can't see which value of $\alpha$ will help here. A couple of things occur to me as ""the problem"": a) I'm not calculating right and things actually turn out much simpler. b) I'm too set on the idea of achieving a separable equation, when maybe I could get linear, Bernoulli, etc. c) I'm missing something extremely obvious and the only problem is in my head. Can someone help me solve this?","I'm studying for a test on ODEs by running through a bunch of exercises, and this one has me stuck. I'd like to find the general solution to $$xy^{\prime} = \sqrt{y^2+x^6}$$ The hint they give me is to use a substitution $y = u^{\alpha}x^{\beta}$. The first thing I did was (and in this removing $0$ from the domain of $y(x)$, at least for the time being) divide out to get $$y^{\prime} = \sqrt{\frac{y^2+x^6}{x^2}}$$ After some playing around I figured that, for the substitution to be useful, it should change the equation into a separable one, which could happen if, for instance, $y^2/x^2 = u^{2\alpha}x^{2\beta-2}$ were equal to $x^3u^{\gamma}$, so I could take a common factor of $x^{3/2}$, and the other factor would only have terms in $u$. So I tried $y = ux^{5/2}$. This got me to the expression $$u' = \frac{1}{x^{5/2}}\left(\sqrt{u^3 + x^3}-5x^{3/2}u\right) \Rightarrow \frac{u'}{\sqrt{u^2+1} - 5u} = \frac1x$$ Which in theory can be integrated quite easily. However the LHS is clearly not a simple integral (for humans; a CAS solves it in no time). It can be done with some hyperbolic trig. substitutions, but on the whole it becomes quite ugly. What's more, I'd end up having [complicated expression in u] = $\ln x + k$, and that won't help me find $u$ explicitly, let alone $y$. Then I tried doing the same without a determined value for $\alpha$, to see if a certain value would make the LHS have an easy primitive. This is to say, I let $y=u^{\alpha}x^{5/2}$ and I got: $$\frac{\alpha u'u^{\alpha-1}}{\sqrt{u^{2\alpha}+1} - 5u} = \frac1x$$ However I can't see which value of $\alpha$ will help here. A couple of things occur to me as ""the problem"": a) I'm not calculating right and things actually turn out much simpler. b) I'm too set on the idea of achieving a separable equation, when maybe I could get linear, Bernoulli, etc. c) I'm missing something extremely obvious and the only problem is in my head. Can someone help me solve this?",,['ordinary-differential-equations']
58,Online resource for solved ODEs,Online resource for solved ODEs,,"I'm looking for some sort of list of solved ODEs, that is, ODEs with solution included. I was hoping to find something like this on the internet but I haven't been able to. Does someone know of a place? I'm basically looking for standard ""solvable-by-quadrature"" ODEs, simply to gain some mechanical ability and ""pattern recognition"", if you will. I know plenty of books but my school's library is closed these days.","I'm looking for some sort of list of solved ODEs, that is, ODEs with solution included. I was hoping to find something like this on the internet but I haven't been able to. Does someone know of a place? I'm basically looking for standard ""solvable-by-quadrature"" ODEs, simply to gain some mechanical ability and ""pattern recognition"", if you will. I know plenty of books but my school's library is closed these days.",,"['ordinary-differential-equations', 'reference-request']"
59,Third solution to a third order ODE,Third solution to a third order ODE,,"Find the general solution, given two independent solutions: $(x-1)^2y'''+(1-x^2)y''+2xy'-2y =0 ,\,\, y_1=x ,\,\, y_2=e^x.$ I tried $v=\dfrac{x}{e^x}$ and $v=\dfrac{e^x}{x}$ to reduce order of the original ODE by one, but it doesn't work.","Find the general solution, given two independent solutions: $(x-1)^2y'''+(1-x^2)y''+2xy'-2y =0 ,\,\, y_1=x ,\,\, y_2=e^x.$ I tried $v=\dfrac{x}{e^x}$ and $v=\dfrac{e^x}{x}$ to reduce order of the original ODE by one, but it doesn't work.",,[]
60,Can an arbitrary constant in the solution of a differential equation really take on any value?,Can an arbitrary constant in the solution of a differential equation really take on any value?,,"Consider the first order differential equation $y' = -2y^{\frac{3}{2}}$. It has $y = \dfrac{1}{(x+c)^2}$ as the solution. Now, if I divide both the numerator and denominator by $c^2$ (assuming $c \neq 0$), we get $y = \dfrac{C^2}{(Cx+1)^2}$, where $C = \dfrac{1}{c}$. Since $c$ is arbitrary, $C$ is also arbitrary. Can $C$ take on the value of $0$? It seems simple enough to argue that since $C = \dfrac{1}{c}$, $C \neq 0$. But $y = \dfrac{C^2}{(Cx+1)^2}$ satisfies the differential equation, which implies that it's a solution for all possible values of $C$. Moreover, the function obtained when $C = 0$ is $y=0$, which satisfies the differential equation. So where am I going wrong in my reasoning?","Consider the first order differential equation $y' = -2y^{\frac{3}{2}}$. It has $y = \dfrac{1}{(x+c)^2}$ as the solution. Now, if I divide both the numerator and denominator by $c^2$ (assuming $c \neq 0$), we get $y = \dfrac{C^2}{(Cx+1)^2}$, where $C = \dfrac{1}{c}$. Since $c$ is arbitrary, $C$ is also arbitrary. Can $C$ take on the value of $0$? It seems simple enough to argue that since $C = \dfrac{1}{c}$, $C \neq 0$. But $y = \dfrac{C^2}{(Cx+1)^2}$ satisfies the differential equation, which implies that it's a solution for all possible values of $C$. Moreover, the function obtained when $C = 0$ is $y=0$, which satisfies the differential equation. So where am I going wrong in my reasoning?",,['ordinary-differential-equations']
61,Solve $x''(t)-\frac{x^2(t)}{\sin t}=\frac{\sin\left( (t-1)^2\right)}{\sin t}$.,Solve .,x''(t)-\frac{x^2(t)}{\sin t}=\frac{\sin\left( (t-1)^2\right)}{\sin t},Solve the following Cauchy problem: $$x''(t)-\frac{x^2(t)}{\sin t}=\frac{\sin\left( (t-1)^2\right)}{\sin t}$$ with $x'(1)=x(1)=0$. I would appreciate some help with this problem. Thank you very much.,Solve the following Cauchy problem: $$x''(t)-\frac{x^2(t)}{\sin t}=\frac{\sin\left( (t-1)^2\right)}{\sin t}$$ with $x'(1)=x(1)=0$. I would appreciate some help with this problem. Thank you very much.,,['ordinary-differential-equations']
62,"How ""sharp"" does a cusp have to be in order for the equation to be nondifferentiable?","How ""sharp"" does a cusp have to be in order for the equation to be nondifferentiable?",,"From a mathematical standpoint, I understand the concept of cusps: for example, a cusp exists at the origin of $y=|x|$ because one cannot take the limit from both sides, and therefore the derivative does not exist. However, I have always wondered: how ""sharp"" does a cusp have to be in order to make the region nondifferentiable? To extend the previous example, intuitively I would assume that if the slopes from both sides of $y = |x|$ where to decrease (eg $y = |\frac{1}{2}x|$, etc) the point at (0,0) would stay nondifferentiable until the slopes from both sides became 0, at which point $f'(x) = 0$. Is this correct, and if so, why? Does it have something to do with the ""abruptness"" of the change between values, or is there a more basic underlying concept that I am missing?","From a mathematical standpoint, I understand the concept of cusps: for example, a cusp exists at the origin of $y=|x|$ because one cannot take the limit from both sides, and therefore the derivative does not exist. However, I have always wondered: how ""sharp"" does a cusp have to be in order to make the region nondifferentiable? To extend the previous example, intuitively I would assume that if the slopes from both sides of $y = |x|$ where to decrease (eg $y = |\frac{1}{2}x|$, etc) the point at (0,0) would stay nondifferentiable until the slopes from both sides became 0, at which point $f'(x) = 0$. Is this correct, and if so, why? Does it have something to do with the ""abruptness"" of the change between values, or is there a more basic underlying concept that I am missing?",,"['calculus', 'ordinary-differential-equations', 'derivatives']"
63,Solving $\frac{dy(t)}{dt} = y(t)^2-2 y(t)+2$,Solving,\frac{dy(t)}{dt} = y(t)^2-2 y(t)+2,"How can I solve the following ODE $$ \frac{dy(t)}{dt} = y(t)^2-2 y(t)+2 $$ I'm having a tough time because the differential is in terms of $dt$. My gut instinct is to integrate both sides, but to do, I would have to integrate the right hand side by $dt$, and would get a messy solution. How do I do this?","How can I solve the following ODE $$ \frac{dy(t)}{dt} = y(t)^2-2 y(t)+2 $$ I'm having a tough time because the differential is in terms of $dt$. My gut instinct is to integrate both sides, but to do, I would have to integrate the right hand side by $dt$, and would get a messy solution. How do I do this?",,['ordinary-differential-equations']
64,Linear systems of differential equations,Linear systems of differential equations,,I would like to see an example of a real physical situation where one can find a set of variables evolving according to a system of linear differential equations. I wasn't able to find any such example so I'm asking here.,I would like to see an example of a real physical situation where one can find a set of variables evolving according to a system of linear differential equations. I wasn't able to find any such example so I'm asking here.,,"['ordinary-differential-equations', 'motivation']"
65,Problem integrating in problem using the Poincaré Lemma,Problem integrating in problem using the Poincaré Lemma,,"a) It is easy to show that $d\beta=0$. b) $\begin{align}\hat{\mathbb{X}}_t &= \left(\frac{\partial}{\partial t}\hat{\Phi}_t \right) \hat{\Phi}_t^{-1} \\ &= \left(\frac{\partial}{\partial t}\hat{\Phi}_t\right) \left(\frac{x}{t},y,z\right) \\ &=\left(\frac{x}{t},0,0\right) \end{align}$ Now $\Phi_t^* x =tx$, $\Phi_t^* y =y$, $\Phi_t^* z =z$ , so, $$ \hat{\Phi}_t^* \beta = \hat{\Phi}_t^* \left(\frac{(t^2x^2-y^2)dy \wedge dz + 2txyt dz \wedge dx}{(t^2x^2+y^2)^2}\right) \rightarrow \frac{-1}{y^2} dy \wedge dz $$ So chose $\alpha_0$ to be $(t^2x^2+y^2)^2=\frac{1}{y}dz$ c) First want to compute that $i_{\mathbb{X}_t}\beta$. $$\begin{align} i_{\mathbb{X}_t} \beta &= i_{\mathbb{X}_t} \left(\frac{(x^2-y^2)}{\rho^4} dy \wedge dz\right) + i_{\mathbb{X}_t}\left(\frac{2xy }{\rho^4} dz \wedge dx\right) \\ &= \left(\frac{(x^2-y^2)}{\rho^4}\right)\left[dy(\mathbb{X}_t)dz - dz(\mathbb{X}_t)dy\right]+\frac{2xy}{\rho^4}[dz(\mathbb{X}_t)dx-dx(\mathbb{X}_t)dz] \\ &= \left(\frac{2xy}{\rho^4}\right)[\frac{-x}{t}dz] \end{align}$$ Then I have that, $$\begin{align} \Phi_t^*(i_{\mathbb{X}_t}\beta) &= \frac{-2y}{t}\left(\frac{t^2x^2}{(t^2x^2+y^2)^2}\right)dz \\ &= -2yt \left(\frac{x^2}{(t^2x^2+y^2)^2}\right)dz \end{align}$$ But this isnt very integrable. Not sure where I have gone wrong or how I should proceed with this question.","a) It is easy to show that $d\beta=0$. b) $\begin{align}\hat{\mathbb{X}}_t &= \left(\frac{\partial}{\partial t}\hat{\Phi}_t \right) \hat{\Phi}_t^{-1} \\ &= \left(\frac{\partial}{\partial t}\hat{\Phi}_t\right) \left(\frac{x}{t},y,z\right) \\ &=\left(\frac{x}{t},0,0\right) \end{align}$ Now $\Phi_t^* x =tx$, $\Phi_t^* y =y$, $\Phi_t^* z =z$ , so, $$ \hat{\Phi}_t^* \beta = \hat{\Phi}_t^* \left(\frac{(t^2x^2-y^2)dy \wedge dz + 2txyt dz \wedge dx}{(t^2x^2+y^2)^2}\right) \rightarrow \frac{-1}{y^2} dy \wedge dz $$ So chose $\alpha_0$ to be $(t^2x^2+y^2)^2=\frac{1}{y}dz$ c) First want to compute that $i_{\mathbb{X}_t}\beta$. $$\begin{align} i_{\mathbb{X}_t} \beta &= i_{\mathbb{X}_t} \left(\frac{(x^2-y^2)}{\rho^4} dy \wedge dz\right) + i_{\mathbb{X}_t}\left(\frac{2xy }{\rho^4} dz \wedge dx\right) \\ &= \left(\frac{(x^2-y^2)}{\rho^4}\right)\left[dy(\mathbb{X}_t)dz - dz(\mathbb{X}_t)dy\right]+\frac{2xy}{\rho^4}[dz(\mathbb{X}_t)dx-dx(\mathbb{X}_t)dz] \\ &= \left(\frac{2xy}{\rho^4}\right)[\frac{-x}{t}dz] \end{align}$$ Then I have that, $$\begin{align} \Phi_t^*(i_{\mathbb{X}_t}\beta) &= \frac{-2y}{t}\left(\frac{t^2x^2}{(t^2x^2+y^2)^2}\right)dz \\ &= -2yt \left(\frac{x^2}{(t^2x^2+y^2)^2}\right)dz \end{align}$$ But this isnt very integrable. Not sure where I have gone wrong or how I should proceed with this question.",,"['calculus', 'integration', 'ordinary-differential-equations', 'differential-geometry', 'differential-forms']"
66,fixed points for the following system,fixed points for the following system,,"I'm trying to find the fixed point for the system (see document attached) but it seems so hard and I don't know what Im doing wrong . Can somebody help me with this. I need to find the to look for the value where the bifurcation occur (for that process i need to evaluate the jacobian matrix for the system on the fixed points and the looks for the delta, which is really easy) but this is the only part where I'm stuck. Please see document attached","I'm trying to find the fixed point for the system (see document attached) but it seems so hard and I don't know what Im doing wrong . Can somebody help me with this. I need to find the to look for the value where the bifurcation occur (for that process i need to evaluate the jacobian matrix for the system on the fixed points and the looks for the delta, which is really easy) but this is the only part where I'm stuck. Please see document attached",,['ordinary-differential-equations']
67,Solving this second order differential equation (Damping mechanism),Solving this second order differential equation (Damping mechanism),,"I'm trying to self-teach myself differential equations, but I'm having trouble with second order equations! How can I solve this one? $\frac{3}{32}y''+12y=0$, $y(0)=\frac{-1}{12}$, $y'(0)=2$ Thanks! I'd appreciate if you show step by step so that I can understand it for the rest of the problems. Consider this to be a damping mechanism now... What would be the bounds on the damping force coefficient k so that oscillatory motion would remain intact?","I'm trying to self-teach myself differential equations, but I'm having trouble with second order equations! How can I solve this one? $\frac{3}{32}y''+12y=0$, $y(0)=\frac{-1}{12}$, $y'(0)=2$ Thanks! I'd appreciate if you show step by step so that I can understand it for the rest of the problems. Consider this to be a damping mechanism now... What would be the bounds on the damping force coefficient k so that oscillatory motion would remain intact?",,['ordinary-differential-equations']
68,A simple second order differential equation,A simple second order differential equation,,"When I try and solve the equation: \begin{equation} \frac{\partial^2 \psi}{\partial x^2}=-k^2\psi \end{equation} I get a solution in the form of: \begin{equation} \psi=Ae^{-ikx} +Be^{ikx} \end{equation} which I believe is right, but I often see the answer written as: \begin{equation} \psi=Asin(kx)+Bcos(kx) \end{equation} which I believe is just obtained using Euler's formula but I feel like if that's the case, then the sin terms would cancel out. Can someone explain how to go from the first form to the second.","When I try and solve the equation: \begin{equation} \frac{\partial^2 \psi}{\partial x^2}=-k^2\psi \end{equation} I get a solution in the form of: \begin{equation} \psi=Ae^{-ikx} +Be^{ikx} \end{equation} which I believe is right, but I often see the answer written as: \begin{equation} \psi=Asin(kx)+Bcos(kx) \end{equation} which I believe is just obtained using Euler's formula but I feel like if that's the case, then the sin terms would cancel out. Can someone explain how to go from the first form to the second.",,['ordinary-differential-equations']
69,Can we take negative step size in Euler's method?,Can we take negative step size in Euler's method?,,"Thus far we've taken the step size $h$ to be positive, and therefore we've developed solutions to the right of the initial point. Is Euler's method valid if we use a negative step size $h<0$ and develop a solution to the left? I think it is not possible.How we can explain that it is possible or not? Thanks.","Thus far we've taken the step size $h$ to be positive, and therefore we've developed solutions to the right of the initial point. Is Euler's method valid if we use a negative step size $h<0$ and develop a solution to the left? I think it is not possible.How we can explain that it is possible or not? Thanks.",,"['ordinary-differential-equations', 'numerical-methods']"
70,(Another) linear ODE first order - solving by Laplace transformation?,(Another) linear ODE first order - solving by Laplace transformation?,,"I don't know what's up, but I got stuck another time in a first order linear ODE. - The problem can be seen on p. 26 of a Lecture-Script . There's also the solution, which is comprehensible. For my own interest, I've tried to solve this equation by Laplace-transformation but I don't get the same solution and don't know why. Given is the following equation: $\sigma = E_1\cdot\epsilon + \eta\cdot\dot\epsilon$ as well as it's initial conditions: $\epsilon(t = 0) = 0$ and $\sigma(t = 0) = \sigma_0$ The disturbing function $\sigma(t)$ is given by: $\sigma(t) = \sigma_0 = \text{const.}$ By using Laplace, I get: $\mathcal{L}\{\sigma\}(s) = E_1\cdot\mathcal{L}\{\epsilon\}+\eta\cdot\bigl[\mathcal{L}\{\epsilon\}(s)\cdot s - 0\bigr]$ This leads me to: $\mathcal{L}\{\epsilon\} = \frac{1}{E_1+\eta\cdot s}\cdot\mathcal{L}\{\sigma\}(s)$ and with $\sigma(t) = \sigma_0$ finally to: $\epsilon(t) = \frac{\sigma_0}{\eta}\cdot e^{-\frac{E_1}{\eta}t}$ ... but that's actually not the same thing from this Script (p. 27), which says: $\epsilon(t) = \frac{\sigma_0}{E_1}\bigl(1-e^{-\frac{E_1}{\eta}t}\bigr)$ Does anyone know what's wrong or where I make a mistake?! ... Thank you so much in advance!","I don't know what's up, but I got stuck another time in a first order linear ODE. - The problem can be seen on p. 26 of a Lecture-Script . There's also the solution, which is comprehensible. For my own interest, I've tried to solve this equation by Laplace-transformation but I don't get the same solution and don't know why. Given is the following equation: $\sigma = E_1\cdot\epsilon + \eta\cdot\dot\epsilon$ as well as it's initial conditions: $\epsilon(t = 0) = 0$ and $\sigma(t = 0) = \sigma_0$ The disturbing function $\sigma(t)$ is given by: $\sigma(t) = \sigma_0 = \text{const.}$ By using Laplace, I get: $\mathcal{L}\{\sigma\}(s) = E_1\cdot\mathcal{L}\{\epsilon\}+\eta\cdot\bigl[\mathcal{L}\{\epsilon\}(s)\cdot s - 0\bigr]$ This leads me to: $\mathcal{L}\{\epsilon\} = \frac{1}{E_1+\eta\cdot s}\cdot\mathcal{L}\{\sigma\}(s)$ and with $\sigma(t) = \sigma_0$ finally to: $\epsilon(t) = \frac{\sigma_0}{\eta}\cdot e^{-\frac{E_1}{\eta}t}$ ... but that's actually not the same thing from this Script (p. 27), which says: $\epsilon(t) = \frac{\sigma_0}{E_1}\bigl(1-e^{-\frac{E_1}{\eta}t}\bigr)$ Does anyone know what's wrong or where I make a mistake?! ... Thank you so much in advance!",,['ordinary-differential-equations']
71,Solving PDE with modified Bessel?,Solving PDE with modified Bessel?,,I'm trying to find the particular solution to the PDEs below from this paper : I only took basic undergrad ODEs so I tried solving for the particular solution by using initial conditions and couldn't find where to plug those in. I also looked up modified Bessel but I'm completely baffled. All I have right now is a hand-wavy ansatz.,I'm trying to find the particular solution to the PDEs below from this paper : I only took basic undergrad ODEs so I tried solving for the particular solution by using initial conditions and couldn't find where to plug those in. I also looked up modified Bessel but I'm completely baffled. All I have right now is a hand-wavy ansatz.,,"['ordinary-differential-equations', 'partial-differential-equations', 'bessel-functions']"
72,Finding solutions of system of differential equations with eigenvectors,Finding solutions of system of differential equations with eigenvectors,,"I was trying to solve this system of differential equations: $$\frac{dx}{dt}=3x-y-z$$    $$\frac{dy}{dt}=x+y-z$$   $$\frac{dz}{dt}=x-y+z$$ I found the eigenvalues: $\lambda_1=1,\lambda_2=2$. The last one has multiplicity $2$. I found $\boldsymbol K_1 =\left( \begin{array}{c} 1\\ 1\\ 1 \end{array} \right)$ is an eigenvector, so a solution to the differential equation would be $\boldsymbol X_1=\boldsymbol K_1e^t.$ However when I want to find a solution with an eigenvector $\boldsymbol K_2 = \left( \begin{array}{c} k_1\\ k_2\\ k_3 \end{array} \right)$ associated to $\lambda_2$ I have the equation $k_1-k_2-k_3=0$ three times. From this I know that two eigenvectors are $\left( \begin{array}{c} 1\\ 1\\ 0 \end{array} \right)$ and $\left( \begin{array}{c} 1\\ 0\\ 1 \end{array} \right)$. Now if I choose one of them, let's say the first one, then this will be a second eigenvector $\boldsymbol K_2$, such that $\boldsymbol X_2=\boldsymbol K_2e^{2t}$ solves the system. However if I try to find a third solution (which I already found: $\boldsymbol K_3e^{2t}$ from the third eigenvector; but I want to find it from here) that has the form $\boldsymbol X_3=\boldsymbol K_2te^{2t}+\boldsymbol P e^{2t}$, then I have to solve $(\boldsymbol A-2\boldsymbol I)\boldsymbol P = \boldsymbol K_2$. But this system doesn't have a solution: $$(\boldsymbol A-2\boldsymbol I)\boldsymbol P = \boldsymbol K_2 \Longrightarrow \left(\begin{array}{ccc|c}1 & -1 & -1 & 1\\1 & -1 & -1 & 1\\1 & -1 & -1 & 0\\ \end{array}\right)???$$ Is it possible to obtain a third solution from the second one? I'm trying to construct this solution which should end up with an expression that involves the third eigenvector. I appreciate any thoughts on this. Thanks.","I was trying to solve this system of differential equations: $$\frac{dx}{dt}=3x-y-z$$    $$\frac{dy}{dt}=x+y-z$$   $$\frac{dz}{dt}=x-y+z$$ I found the eigenvalues: $\lambda_1=1,\lambda_2=2$. The last one has multiplicity $2$. I found $\boldsymbol K_1 =\left( \begin{array}{c} 1\\ 1\\ 1 \end{array} \right)$ is an eigenvector, so a solution to the differential equation would be $\boldsymbol X_1=\boldsymbol K_1e^t.$ However when I want to find a solution with an eigenvector $\boldsymbol K_2 = \left( \begin{array}{c} k_1\\ k_2\\ k_3 \end{array} \right)$ associated to $\lambda_2$ I have the equation $k_1-k_2-k_3=0$ three times. From this I know that two eigenvectors are $\left( \begin{array}{c} 1\\ 1\\ 0 \end{array} \right)$ and $\left( \begin{array}{c} 1\\ 0\\ 1 \end{array} \right)$. Now if I choose one of them, let's say the first one, then this will be a second eigenvector $\boldsymbol K_2$, such that $\boldsymbol X_2=\boldsymbol K_2e^{2t}$ solves the system. However if I try to find a third solution (which I already found: $\boldsymbol K_3e^{2t}$ from the third eigenvector; but I want to find it from here) that has the form $\boldsymbol X_3=\boldsymbol K_2te^{2t}+\boldsymbol P e^{2t}$, then I have to solve $(\boldsymbol A-2\boldsymbol I)\boldsymbol P = \boldsymbol K_2$. But this system doesn't have a solution: $$(\boldsymbol A-2\boldsymbol I)\boldsymbol P = \boldsymbol K_2 \Longrightarrow \left(\begin{array}{ccc|c}1 & -1 & -1 & 1\\1 & -1 & -1 & 1\\1 & -1 & -1 & 0\\ \end{array}\right)???$$ Is it possible to obtain a third solution from the second one? I'm trying to construct this solution which should end up with an expression that involves the third eigenvector. I appreciate any thoughts on this. Thanks.",,"['ordinary-differential-equations', 'eigenvalues-eigenvectors', 'systems-of-equations']"
73,Why is the Wronskian of these two functions equal to $\frac{2}{\sqrt{\pi}}$,Why is the Wronskian of these two functions equal to,\frac{2}{\sqrt{\pi}},I can't seem to get the right answer $ f = e^{\frac{y^2}{2}}$ and $g =e^{\frac{y^2}{2}}erf(y)$ where $erf(y) = \frac{2}{\sqrt\pi}\int_{0}^{y}e^{-\alpha^2}d\alpha$ I get $W = \frac{2}{\sqrt\pi} - ye^{y^2}erf(y)$ Am I making some kind of stupid mistake that I can't get the answer. Thanks,I can't seem to get the right answer $ f = e^{\frac{y^2}{2}}$ and $g =e^{\frac{y^2}{2}}erf(y)$ where $erf(y) = \frac{2}{\sqrt\pi}\int_{0}^{y}e^{-\alpha^2}d\alpha$ I get $W = \frac{2}{\sqrt\pi} - ye^{y^2}erf(y)$ Am I making some kind of stupid mistake that I can't get the answer. Thanks,,"['ordinary-differential-equations', 'error-function']"
74,"Solution of $ay''+by'+cy=0$ with positive constants $a,b,c$ satisfies $y(x)\to0$ as $x\to\infty$",Solution of  with positive constants  satisfies  as,"ay''+by'+cy=0 a,b,c y(x)\to0 x\to\infty","Given that $a, b, c$ are positive constants and $y(x)$ is a solution to the differential equation $ay''+by'+cy=0$, show that $\lim\limits_{x \to \infty} y(x) = 0$. I've been able to determine that the general solution $y(x)=c_1e^{r_1x} + c_2e^{r_2x}$ should have $r_1$ and $r_2$ be negative (I found this by playing around with the numbers in GeoGebra). As for the proper solution though, I'm at a loss.","Given that $a, b, c$ are positive constants and $y(x)$ is a solution to the differential equation $ay''+by'+cy=0$, show that $\lim\limits_{x \to \infty} y(x) = 0$. I've been able to determine that the general solution $y(x)=c_1e^{r_1x} + c_2e^{r_2x}$ should have $r_1$ and $r_2$ be negative (I found this by playing around with the numbers in GeoGebra). As for the proper solution though, I'm at a loss.",,"['calculus', 'ordinary-differential-equations']"
75,How to prove that solution of ODE is even function?,How to prove that solution of ODE is even function?,,"Could you please give me some hint how to prove this statement: If $f(x)$ is solution of $y'=4x^3e^{-|y|}$ then $f(x)$ is even function. It is obvious that $f(x)$ increasing for all $x>0$ and decreasing for all $x<0$, so there are $a<0,b>0$ such as $f(a)=f(b)$, but how to prove that $a=-b$ ? Thanks.","Could you please give me some hint how to prove this statement: If $f(x)$ is solution of $y'=4x^3e^{-|y|}$ then $f(x)$ is even function. It is obvious that $f(x)$ increasing for all $x>0$ and decreasing for all $x<0$, so there are $a<0,b>0$ such as $f(a)=f(b)$, but how to prove that $a=-b$ ? Thanks.",,"['ordinary-differential-equations', 'functions']"
76,When do you drop the absolute value from ln|x| + C when integrating $\frac{1}{u}du$,When do you drop the absolute value from ln|x| + C when integrating,\frac{1}{u}du,"Given:  p(t) represents the number of cats, when t>=0. Given:  p(t) is increasing at a rate directly proportional to $800-p(t)$ So, I represent this as:  $\frac{dp}{dt}= k(800-P)$ I want p(t), so I separate and integrate:  $\int{\frac{dp}{800-p}} = \int{kdt}$ When I integrate, I am told this yields $-ln(800-p)=kt+c$ Shouldn't it be:  $-ln|800-p|=kt+c$? Why did they drop the absolute value? Is the phrasing of the givens implying that $800-p$ is always positive?","Given:  p(t) represents the number of cats, when t>=0. Given:  p(t) is increasing at a rate directly proportional to $800-p(t)$ So, I represent this as:  $\frac{dp}{dt}= k(800-P)$ I want p(t), so I separate and integrate:  $\int{\frac{dp}{800-p}} = \int{kdt}$ When I integrate, I am told this yields $-ln(800-p)=kt+c$ Shouldn't it be:  $-ln|800-p|=kt+c$? Why did they drop the absolute value? Is the phrasing of the givens implying that $800-p$ is always positive?",,"['calculus', 'integration', 'ordinary-differential-equations']"
77,"Combinations of fruits and their ""nutrients""","Combinations of fruits and their ""nutrients""",,"As a computer scientist and not a mathematician, I know not some of the formal language to describe my problem, so I'll present it in a word problem form. Maybe someone can help me hone my search and vocabulary. Suppose we have a set of fruits, costs, and their associated provisions Apples cost \$1 and provide 1HP, 3MP, 2STA Oranges cost \$2 and provide 2HP, 3STA Pears cost \$3 and provide 2MP, 4STA If I require 10HP, 7MP, and 12STA, how do I optimize for the following: Generate fruit combinations which go as minimally in excess of the requirements (you know, because wasting fruit is bad) Consume as few fruits as possible to fully satisfy the requirements Spend as little money as possible to fully satisfy the requirements The problem illustrated is obviously trivial, but if the number of fruits can approach dozens or even hundreds, and the number of required parameters HP, MP, STA, etc. can also reach dozens, the combinatorial space becomes computationally prohibitive, so I need another approach. What are some possible alternatives that may not be perfect, but give an asymptotically increasingly accurate answer? Some approach thoughts I've had but haven't fully pursued due to intuited problems with the approach: Brute force (the permutations become innumerable and the computation time even worse) Multidimensional hill-climbing algorithm with a distribution of starting points (not sure what ""up"" means in this approach) Linear Programming (The selection process doesn't appear linear? Maybe I don't understand the formal definition of linear here) Network Flow (not sure how to encode edges, and it looks like edges end up being as numerous as in the brute force approach)","As a computer scientist and not a mathematician, I know not some of the formal language to describe my problem, so I'll present it in a word problem form. Maybe someone can help me hone my search and vocabulary. Suppose we have a set of fruits, costs, and their associated provisions Apples cost \$1 and provide 1HP, 3MP, 2STA Oranges cost \$2 and provide 2HP, 3STA Pears cost \$3 and provide 2MP, 4STA If I require 10HP, 7MP, and 12STA, how do I optimize for the following: Generate fruit combinations which go as minimally in excess of the requirements (you know, because wasting fruit is bad) Consume as few fruits as possible to fully satisfy the requirements Spend as little money as possible to fully satisfy the requirements The problem illustrated is obviously trivial, but if the number of fruits can approach dozens or even hundreds, and the number of required parameters HP, MP, STA, etc. can also reach dozens, the combinatorial space becomes computationally prohibitive, so I need another approach. What are some possible alternatives that may not be perfect, but give an asymptotically increasingly accurate answer? Some approach thoughts I've had but haven't fully pursued due to intuited problems with the approach: Brute force (the permutations become innumerable and the computation time even worse) Multidimensional hill-climbing algorithm with a distribution of starting points (not sure what ""up"" means in this approach) Linear Programming (The selection process doesn't appear linear? Maybe I don't understand the formal definition of linear here) Network Flow (not sure how to encode edges, and it looks like edges end up being as numerous as in the brute force approach)",,"['combinatorics', 'ordinary-differential-equations', 'linear-programming', 'integer-programming']"
78,''Differential equation'' with known solution $\sin$ and $\cos$,''Differential equation'' with known solution  and,\sin \cos,"I am given the following two two equations $f,g : \mathbb{R} \to \mathbb{R}$ are differentiable on $\mathbb{R}$ and they satisfy $\forall x,y \in \mathbb{R}$ $$f(x+y) = f(x)g(y)+f(y)g(x)\\g(x+y)=g(x)g(y)-f(x)f(y)  $$ with $f(-x)=-f(x), \forall x \in \mathbb{R}$ and $f'(0)=1$. I want to show that $f= \sin$ and $g = \cos$ My approach : My general idea was to show that $$f'=g, \  g'=-f$$ and $$f(0)=0, \ g(0)=1$$ If I'd manage to do that, then the solution follows because the above differential equation has unique solution $f= \sin, g = \cos$. Since $f$ is an 'uneven' function it follows that $f(0)=-f(0) \implies f(0)=0$ so that's one check out of the above list $\checkmark$ Differentiating the first given expression I'd obtain $$f'(x+y)=f'(x)g(y)+f(y)g'(x) \implies f'(0)=f'(0)g(0)+f(0)g'(0)=g(0)=1 $$  So that takes care of $g(0)=1 \ \checkmark$ Now to the tricky part. I will differentiate both equations and obtain: $$f'(x+y)=f'(x)g(y) + f(y) g'(x) \\ g'(x+y)=g'(x)g(y)-f'(x)f(y) $$ It seems like it is crucial that $g'(0)=0$ in order to show the remaining two properties out of my list above. Which so far I didn't manage to do because all my steps are circular: $$f'(y)= f'(0)g(y)+f(y)g'(0)=g(y)+f(y)g'(0) \tag{*} $$ which would only help me if $g'(0)=0$, similarly $$g'(y)=g'(0)g(y)-f'(0)f(y)=g'(0)g(y)-f(y) $$ Which doesn't help me either, maybe there is a substitution I have to perform but I yet fail to see it.","I am given the following two two equations $f,g : \mathbb{R} \to \mathbb{R}$ are differentiable on $\mathbb{R}$ and they satisfy $\forall x,y \in \mathbb{R}$ $$f(x+y) = f(x)g(y)+f(y)g(x)\\g(x+y)=g(x)g(y)-f(x)f(y)  $$ with $f(-x)=-f(x), \forall x \in \mathbb{R}$ and $f'(0)=1$. I want to show that $f= \sin$ and $g = \cos$ My approach : My general idea was to show that $$f'=g, \  g'=-f$$ and $$f(0)=0, \ g(0)=1$$ If I'd manage to do that, then the solution follows because the above differential equation has unique solution $f= \sin, g = \cos$. Since $f$ is an 'uneven' function it follows that $f(0)=-f(0) \implies f(0)=0$ so that's one check out of the above list $\checkmark$ Differentiating the first given expression I'd obtain $$f'(x+y)=f'(x)g(y)+f(y)g'(x) \implies f'(0)=f'(0)g(0)+f(0)g'(0)=g(0)=1 $$  So that takes care of $g(0)=1 \ \checkmark$ Now to the tricky part. I will differentiate both equations and obtain: $$f'(x+y)=f'(x)g(y) + f(y) g'(x) \\ g'(x+y)=g'(x)g(y)-f'(x)f(y) $$ It seems like it is crucial that $g'(0)=0$ in order to show the remaining two properties out of my list above. Which so far I didn't manage to do because all my steps are circular: $$f'(y)= f'(0)g(y)+f(y)g'(0)=g(y)+f(y)g'(0) \tag{*} $$ which would only help me if $g'(0)=0$, similarly $$g'(y)=g'(0)g(y)-f'(0)f(y)=g'(0)g(y)-f(y) $$ Which doesn't help me either, maybe there is a substitution I have to perform but I yet fail to see it.",,"['calculus', 'real-analysis', 'ordinary-differential-equations']"
79,Need to draw phase portrait near the equilibrium points of differential equation,Need to draw phase portrait near the equilibrium points of differential equation,,"So, this equation  $$\ddot{x}+3\dot{x}-4x+2x^2 = 0.$$ I can write like a system \begin{equation} \left\{ \begin{array}{ll} \dot{x} = v, \\ \dot{v} = 2x^2 - 4x - 3v. \end{array} \right. \end{equation} Finding the equilibrium points: \begin{equation}\nonumber \left\{ \begin{array}{ll} v = 0 \\ 2x^2 - 4x - 3v = 0 \end{array} \right. \ \ \Rightarrow 2x^2 - 4x = 0,  \ \ \Rightarrow  \ \ x_1 = 0, \ x_2 = 2. \ \ \Rightarrow \\ \Rightarrow \boldsymbol{(0,0)}, \ \boldsymbol{(2,0)} - \text{equilibrium  points} \end{equation} The matrix for the system \begin{equation}  A(x,y)=\left(\begin{array}{ccc} 0     &   1 \\ 4-2х  &  -3 \\ \end{array} \right)\end{equation} Now I need to determine what is happening in each equilibrium point. For example, first point, $\boldsymbol{(0,0)}$: $$A(0,0)=\left(\begin{array}{ccc} 0   &  1 \\ 4   &  -3 \\ \end{array} \right)$$ $$\lambda^2 + 3\lambda -4 = 0,$$ $$\lambda_1=1, \ \ \lambda_2=-4$$ Here everything looks fine, the different signs of real roots saying to me it's saddle. But then, let's look at the point $\boldsymbol{(2,0)}$: $$A(2,0)=\left(\begin{array}{ccc} 0   &  1 \\ 0   &  -3 \\ \end{array} \right)$$ $$\lambda^2 - 3\lambda = 0,$$ $$\lambda_1 = 0, \ \ \lambda_2 = -3.$$ It means just one eigen vector. Would it be a sink? On a picture you can see numerical solution: I think, after I write it here, I get it why the picture should be like this. But I wrote so a lot, so I will post it, maybe someone can explain it better or add something. Thank you for attention.","So, this equation  $$\ddot{x}+3\dot{x}-4x+2x^2 = 0.$$ I can write like a system \begin{equation} \left\{ \begin{array}{ll} \dot{x} = v, \\ \dot{v} = 2x^2 - 4x - 3v. \end{array} \right. \end{equation} Finding the equilibrium points: \begin{equation}\nonumber \left\{ \begin{array}{ll} v = 0 \\ 2x^2 - 4x - 3v = 0 \end{array} \right. \ \ \Rightarrow 2x^2 - 4x = 0,  \ \ \Rightarrow  \ \ x_1 = 0, \ x_2 = 2. \ \ \Rightarrow \\ \Rightarrow \boldsymbol{(0,0)}, \ \boldsymbol{(2,0)} - \text{equilibrium  points} \end{equation} The matrix for the system \begin{equation}  A(x,y)=\left(\begin{array}{ccc} 0     &   1 \\ 4-2х  &  -3 \\ \end{array} \right)\end{equation} Now I need to determine what is happening in each equilibrium point. For example, first point, $\boldsymbol{(0,0)}$: $$A(0,0)=\left(\begin{array}{ccc} 0   &  1 \\ 4   &  -3 \\ \end{array} \right)$$ $$\lambda^2 + 3\lambda -4 = 0,$$ $$\lambda_1=1, \ \ \lambda_2=-4$$ Here everything looks fine, the different signs of real roots saying to me it's saddle. But then, let's look at the point $\boldsymbol{(2,0)}$: $$A(2,0)=\left(\begin{array}{ccc} 0   &  1 \\ 0   &  -3 \\ \end{array} \right)$$ $$\lambda^2 - 3\lambda = 0,$$ $$\lambda_1 = 0, \ \ \lambda_2 = -3.$$ It means just one eigen vector. Would it be a sink? On a picture you can see numerical solution: I think, after I write it here, I get it why the picture should be like this. But I wrote so a lot, so I will post it, maybe someone can explain it better or add something. Thank you for attention.",,"['ordinary-differential-equations', 'nonlinear-system']"
80,Solving ODE $F(t)=A(t)F'(t) $,Solving ODE,F(t)=A(t)F'(t) ,"How to solve    $F(t)=A(t)F'(t) ,F(0)= I\tag 1$ All are $3 \times 3$ matrices except variable t A(t) is given and has determinant $0$.  $A(t)=(I-tC_1)^{-1}t^3C_2 \tag 2$ I is a constant unit  rotation matrix means  I is unity matrix $C_1,C_2$ are constant skew symmetric matrices of  $0$ determinent $$C_1=\left( \begin{array}{ccc}     0 & -c_0 & b_0 \\    c_0 & 0 & -a_0 \\   -b_0 & a_0 & 0 \\    \end{array} \right).$$ $$C_2=\left( \begin{array}{ccc}     0 & -(c_1-c_0) & (b_1-b_0) \\    (c_1-c_0) & 0 & -(a_1-a_0) \\    -(b_1-b_0) & (a_1-a_0) & 0 \\    \end{array} \right).$$ NB: All entries of the matrices $C_1$, $C_2$ are constants,can't be altered","How to solve    $F(t)=A(t)F'(t) ,F(0)= I\tag 1$ All are $3 \times 3$ matrices except variable t A(t) is given and has determinant $0$.  $A(t)=(I-tC_1)^{-1}t^3C_2 \tag 2$ I is a constant unit  rotation matrix means  I is unity matrix $C_1,C_2$ are constant skew symmetric matrices of  $0$ determinent $$C_1=\left( \begin{array}{ccc}     0 & -c_0 & b_0 \\    c_0 & 0 & -a_0 \\   -b_0 & a_0 & 0 \\    \end{array} \right).$$ $$C_2=\left( \begin{array}{ccc}     0 & -(c_1-c_0) & (b_1-b_0) \\    (c_1-c_0) & 0 & -(a_1-a_0) \\    -(b_1-b_0) & (a_1-a_0) & 0 \\    \end{array} \right).$$ NB: All entries of the matrices $C_1$, $C_2$ are constants,can't be altered",,"['calculus', 'linear-algebra', 'integration', 'matrices', 'ordinary-differential-equations']"
81,Solving a 1st order homogeneous differential equation,Solving a 1st order homogeneous differential equation,,"Let $(1):y'-a(x)y=0$. This is the way I am used to solving those : $\frac{y'}y=a(x)$ $\ln\left({\frac{y}{|\lambda|}}\right)=\int a(x)=A(x)$ $y=\lambda e^{A(x)}$ Several people have told me that this method was not rigorous enough. (especially when I use $\ln$, as $y$ can be $0$ at times.) It has however always worked with me. What exactly is wrong with this method, and why does it still give good results ? What is a way to 'fix' this method, while keeping it short ?","Let $(1):y'-a(x)y=0$. This is the way I am used to solving those : $\frac{y'}y=a(x)$ $\ln\left({\frac{y}{|\lambda|}}\right)=\int a(x)=A(x)$ $y=\lambda e^{A(x)}$ Several people have told me that this method was not rigorous enough. (especially when I use $\ln$, as $y$ can be $0$ at times.) It has however always worked with me. What exactly is wrong with this method, and why does it still give good results ? What is a way to 'fix' this method, while keeping it short ?",,['ordinary-differential-equations']
82,How to plot a phase potrait of a system of ODEs (in Mathematica),How to plot a phase potrait of a system of ODEs (in Mathematica),,"I am asked to use mathematica to plot the fixed-points of the following system $$ \frac{dN}{dt} = -\gamma N \left( 1 - \left( \beta M + N\right) \right) $$ $$\frac{dM}{dt} = M \left( 1 - \left( \alpha N + M\right) \right) $$ for the case where $\alpha = 2,\ \beta =2,\ \gamma=1$. I assume it will involve using NDSolve, but I'm not sure what to take for initial conditions or really the format of the statement. Any help is appreciated so far I have the following: sol = NDSolve[{x'[t] == x[t] - \[Sigma] (y[t] x[t]) - x[t]^2,     y'[t] == \[Rho] (\[Beta] y[t] x[t]) - \[Rho] y[        t] - \[Rho] y[t]^2, \[Sigma] == 2, \[Beta] == 2, \[Rho] == 1},    x, y, {t, 0, 50}]","I am asked to use mathematica to plot the fixed-points of the following system $$ \frac{dN}{dt} = -\gamma N \left( 1 - \left( \beta M + N\right) \right) $$ $$\frac{dM}{dt} = M \left( 1 - \left( \alpha N + M\right) \right) $$ for the case where $\alpha = 2,\ \beta =2,\ \gamma=1$. I assume it will involve using NDSolve, but I'm not sure what to take for initial conditions or really the format of the statement. Any help is appreciated so far I have the following: sol = NDSolve[{x'[t] == x[t] - \[Sigma] (y[t] x[t]) - x[t]^2,     y'[t] == \[Rho] (\[Beta] y[t] x[t]) - \[Rho] y[        t] - \[Rho] y[t]^2, \[Sigma] == 2, \[Beta] == 2, \[Rho] == 1},    x, y, {t, 0, 50}]",,"['ordinary-differential-equations', 'mathematical-modeling', 'mathematica']"
83,Differential geometry: Conformal map,Differential geometry: Conformal map,,"Let $f:\mathbb{R}_{>0} \times (0,2\pi) \rightarrow \mathbb{R}^3$ $$f(t,\xi) := (r(t) \cos( \xi) , r(t) \sin(\xi),z(t))$$ be a surface of revolution, where we assume that $r>0$ and $r'^2+z'^2>0.$ Then the metric tensor of this immersion can be written as  $(g_{ij})(t):=\begin{pmatrix} r'(t)^2+z'(t)^2 & 0 \\ 0 & r(t)^2 \end{pmatrix}.$ By exploiting the fact that there is an arc-length parametrization ($\psi(t) = \int_0^t r'(s)^2+z'(s)^2 ds$) for the underlying curve of the surface of revolution, the map $f$ can be reparametrized so that we get the metric tensor $\tilde{g_{ij}}(t) = \begin{pmatrix} 1 & 0 \\ 0 & r(\psi(t))^2 \end{pmatrix}.$ Now, I am asked in my exercise, if there is also a reparametrization $\phi: \hat{U} \rightarrow U$, such that we locally(!) (for some open set $U$ in the domain of $f$) get a map $\hat{f}: \hat{U} \rightarrow \mathbb{R}^3$ satisfying $\hat{f} =  f \circ \phi$, where the new metric tensor $(\hat{g}_{ij})(p) = \lambda(p) \ Id$ for all $p \in \hat{U}$. I know that a reparametrization would give rise to the following equation regarding the metric tensor: $$(\hat{g}_{ij}) = D \phi^T ( g_{ij}) D \phi$$. I also got the hint that I shall look for an ODE and use the Picard-Lindelöf theorem to show the local existence, but I just don't see a differential equation showing up here anywhere. If anything is unclear, please let me know.","Let $f:\mathbb{R}_{>0} \times (0,2\pi) \rightarrow \mathbb{R}^3$ $$f(t,\xi) := (r(t) \cos( \xi) , r(t) \sin(\xi),z(t))$$ be a surface of revolution, where we assume that $r>0$ and $r'^2+z'^2>0.$ Then the metric tensor of this immersion can be written as  $(g_{ij})(t):=\begin{pmatrix} r'(t)^2+z'(t)^2 & 0 \\ 0 & r(t)^2 \end{pmatrix}.$ By exploiting the fact that there is an arc-length parametrization ($\psi(t) = \int_0^t r'(s)^2+z'(s)^2 ds$) for the underlying curve of the surface of revolution, the map $f$ can be reparametrized so that we get the metric tensor $\tilde{g_{ij}}(t) = \begin{pmatrix} 1 & 0 \\ 0 & r(\psi(t))^2 \end{pmatrix}.$ Now, I am asked in my exercise, if there is also a reparametrization $\phi: \hat{U} \rightarrow U$, such that we locally(!) (for some open set $U$ in the domain of $f$) get a map $\hat{f}: \hat{U} \rightarrow \mathbb{R}^3$ satisfying $\hat{f} =  f \circ \phi$, where the new metric tensor $(\hat{g}_{ij})(p) = \lambda(p) \ Id$ for all $p \in \hat{U}$. I know that a reparametrization would give rise to the following equation regarding the metric tensor: $$(\hat{g}_{ij}) = D \phi^T ( g_{ij}) D \phi$$. I also got the hint that I shall look for an ODE and use the Picard-Lindelöf theorem to show the local existence, but I just don't see a differential equation showing up here anywhere. If anything is unclear, please let me know.",,"['real-analysis', 'ordinary-differential-equations']"
84,Expectation and Variance of stochastic equation,Expectation and Variance of stochastic equation,,"My questions is related to this question: Stochastic Differential equation, expectation and variance I.e how do you calculate the variance and expectation of $U_t = e^{-\gamma t}U_0 + \int_0^t e^{\gamma (s-t)}\sigma dX_s$?","My questions is related to this question: Stochastic Differential equation, expectation and variance I.e how do you calculate the variance and expectation of $U_t = e^{-\gamma t}U_0 + \int_0^t e^{\gamma (s-t)}\sigma dX_s$?",,"['ordinary-differential-equations', 'stochastic-processes', 'expectation']"
85,$\frac{d^2 y}{dx^2}-2y=2\tan^3\left(x\right)$,,\frac{d^2 y}{dx^2}-2y=2\tan^3\left(x\right),"Problem: \begin{equation} \frac{d^2 y}{dx^2}-2y=2\tan^3\left(x\right). \end{equation} using the method of undetermined coefficients or variation of parameters, with $y_p\left(x\right)=\tan\left(x\right)$. This is what I have so far: \begin{equation} r^2-2=0\implies r=\pm\sqrt{2},\:\:\:\:\:\therefore\:\: C_1 e^{\sqrt{2}t}+C_2 e^{-\sqrt{2}t}, \end{equation} so that is my ""complimentary solution"" but I do not know what to do with it.","Problem: \begin{equation} \frac{d^2 y}{dx^2}-2y=2\tan^3\left(x\right). \end{equation} using the method of undetermined coefficients or variation of parameters, with $y_p\left(x\right)=\tan\left(x\right)$. This is what I have so far: \begin{equation} r^2-2=0\implies r=\pm\sqrt{2},\:\:\:\:\:\therefore\:\: C_1 e^{\sqrt{2}t}+C_2 e^{-\sqrt{2}t}, \end{equation} so that is my ""complimentary solution"" but I do not know what to do with it.",,['ordinary-differential-equations']
86,"Solve $y'= \sin(x+y) ,\ \ y(0) = -\frac{\pi}{2}$",Solve,"y'= \sin(x+y) ,\ \ y(0) = -\frac{\pi}{2}","How can I solve this differential equation $$y'= \sin(x+y) ,\ \  y(0) = -\frac{\pi}{2}, -\infty < x < \infty$$ I tried to denote $z=x+y$ but I got an unfamiliar integral. Please help. Thanks",How can I solve this differential equation I tried to denote but I got an unfamiliar integral. Please help. Thanks,"y'= \sin(x+y) ,\ \  y(0) = -\frac{\pi}{2}, -\infty < x < \infty z=x+y",['ordinary-differential-equations']
87,How to get rid of $(\frac{dw}{dx})^2$ term in a differential equation,How to get rid of  term in a differential equation,(\frac{dw}{dx})^2,"My try: $$y=w^{-1}$$ $$y'=-w^{-2} \frac{dw}{dx}$$ $$y''=\frac{2}{w^3} \frac{dw}{dx} - \frac{1}{w^2} \frac {d^2w}{dx^2}$$ Substituting these to the first expression : $$\frac{2}{w^4}\frac{dw}{dx}-\frac{1}{w^3}\frac{d^2w}{dx^2}-\frac{2}{w^3}\frac{dw}{dx}-\frac{2}{w^4}(\frac{dw}{dx})^2-\frac{5}{w^2}=(5x^2+4x+2)\frac{1}{w^3}$$ I don't know how to get rid of the ""squared"" term: $$-\frac{2}{w^4}(\frac{dw}{dx})^2$$ Since the required expression doesn't have  a squared term. Please help.","My try: Substituting these to the first expression : I don't know how to get rid of the ""squared"" term: Since the required expression doesn't have  a squared term. Please help.",y=w^{-1} y'=-w^{-2} \frac{dw}{dx} y''=\frac{2}{w^3} \frac{dw}{dx} - \frac{1}{w^2} \frac {d^2w}{dx^2} \frac{2}{w^4}\frac{dw}{dx}-\frac{1}{w^3}\frac{d^2w}{dx^2}-\frac{2}{w^3}\frac{dw}{dx}-\frac{2}{w^4}(\frac{dw}{dx})^2-\frac{5}{w^2}=(5x^2+4x+2)\frac{1}{w^3} -\frac{2}{w^4}(\frac{dw}{dx})^2,"['calculus', 'ordinary-differential-equations']"
88,bibliography for weak solutions of ODE's,bibliography for weak solutions of ODE's,,Could someone recommend to me some bibliography about weak solutions of ODE's and solutions of ODE's that are not Lipschitz or discontinuous??,Could someone recommend to me some bibliography about weak solutions of ODE's and solutions of ODE's that are not Lipschitz or discontinuous??,,"['calculus', 'ordinary-differential-equations', 'nonlinear-system']"
89,Converting an ODE in polar form,Converting an ODE in polar form,,Convert the ODE system     $$ \dot{x}=\begin{pmatrix}a(t) & b(t)\\c(t) & d(t)\end{pmatrix}x $$     into polar form. You should get two equations     $$ \frac{d}{dt}\Phi(t)=...\\ \frac{d}{dt}\ln r(t)=.... $$ I set $$ x_1:=r(t)\cos\Phi(t)\\ x_2:=r(t)\sin\Phi(t) $$ and got $$ \frac{d}{dt}\Phi(t)=b(t)+\frac{\frac{d}{dt}r(t)\cos\Phi(t)}{r(t)\sin\Phi(t)}-\frac{a(t)\cos\Phi(t)}{\sin\Phi(t)}\\ \frac{d}{dt}\ln r(t)=d(t)+\frac{c(t)\cos\Phi(t)}{\sin\Phi(t)}-\frac{\cos\Phi(t)\frac{d}{dt}\Phi(t)}{\sin\Phi(t)} $$ Would like to know if this is right. With greetings,Convert the ODE system     $$ \dot{x}=\begin{pmatrix}a(t) & b(t)\\c(t) & d(t)\end{pmatrix}x $$     into polar form. You should get two equations     $$ \frac{d}{dt}\Phi(t)=...\\ \frac{d}{dt}\ln r(t)=.... $$ I set $$ x_1:=r(t)\cos\Phi(t)\\ x_2:=r(t)\sin\Phi(t) $$ and got $$ \frac{d}{dt}\Phi(t)=b(t)+\frac{\frac{d}{dt}r(t)\cos\Phi(t)}{r(t)\sin\Phi(t)}-\frac{a(t)\cos\Phi(t)}{\sin\Phi(t)}\\ \frac{d}{dt}\ln r(t)=d(t)+\frac{c(t)\cos\Phi(t)}{\sin\Phi(t)}-\frac{\cos\Phi(t)\frac{d}{dt}\Phi(t)}{\sin\Phi(t)} $$ Would like to know if this is right. With greetings,,['ordinary-differential-equations']
90,Solving $y''-4y=x^2 e^{2x}$.,Solving .,y''-4y=x^2 e^{2x},"I want to solve the differential equation $$y''-4y = x^2e^{2x}$$ Clearly $y_1 = e^{2x}$ and $y_2 = e^{-2x}$ are linearly independent solutions of the homogeneous equation. I would propose $y = (a+bx)e^{2x}$ as a solution, but since it contains $ae^{2x}$, which is in the span of $y_1$ and $y_2$, it won't do. Then I propose $$y = (ax+bx^2)e^{2x} = axe^{2x} + bx^2e^{2x}$$ as a solution, and we procced to find $a$ and $b$. We have: $$\begin{align} y &= axe^{2x} + bx^2e^{2x}\\ y' &= ae^{2x} + (2a+2b)xe^{2x} + 2bx^2e^{2x}\\ y'' &=(4a+2b)e^{2x} + (4a+8b)xe^{2x} + 4bx^2e^{2x}\end{align}$$ I mean, I could be wrong here... but I don't think so, I triple checked it. Then, forcing it as a solution, I get: $$\left\{\begin{array}{l} 4a + 2b = 0 \\8b = 0 \\0 = 1 \end{array} \right.$$ Can someone explain to me what's going on wrong here? Thanks.","I want to solve the differential equation $$y''-4y = x^2e^{2x}$$ Clearly $y_1 = e^{2x}$ and $y_2 = e^{-2x}$ are linearly independent solutions of the homogeneous equation. I would propose $y = (a+bx)e^{2x}$ as a solution, but since it contains $ae^{2x}$, which is in the span of $y_1$ and $y_2$, it won't do. Then I propose $$y = (ax+bx^2)e^{2x} = axe^{2x} + bx^2e^{2x}$$ as a solution, and we procced to find $a$ and $b$. We have: $$\begin{align} y &= axe^{2x} + bx^2e^{2x}\\ y' &= ae^{2x} + (2a+2b)xe^{2x} + 2bx^2e^{2x}\\ y'' &=(4a+2b)e^{2x} + (4a+8b)xe^{2x} + 4bx^2e^{2x}\end{align}$$ I mean, I could be wrong here... but I don't think so, I triple checked it. Then, forcing it as a solution, I get: $$\left\{\begin{array}{l} 4a + 2b = 0 \\8b = 0 \\0 = 1 \end{array} \right.$$ Can someone explain to me what's going on wrong here? Thanks.",,['ordinary-differential-equations']
91,Is the continuity of a vector field enough for the existence of the solution of a differential equation?,Is the continuity of a vector field enough for the existence of the solution of a differential equation?,,"I've recently seen the existence-uniqueness theorem for ordinary differential equations from Arnold's book. I understand that the theorem as stated guarantees both existence and uniqueness if the corresponding vector field is continuously differentiable. I've also noticed that in most examples where unique solutions don't exist, the differentiability of the vector field is missing. My question is, will any weaker condition suffice to guarantee only the existence (and not necessarily the uniqueness) of an ODE? Is it for example true that if the vector field is just continuous, a solution exists even if not unique?  I went through Existence of Solution to Differential Equations. , but this doesn't seem to answer my doubts completely.","I've recently seen the existence-uniqueness theorem for ordinary differential equations from Arnold's book. I understand that the theorem as stated guarantees both existence and uniqueness if the corresponding vector field is continuously differentiable. I've also noticed that in most examples where unique solutions don't exist, the differentiability of the vector field is missing. My question is, will any weaker condition suffice to guarantee only the existence (and not necessarily the uniqueness) of an ODE? Is it for example true that if the vector field is just continuous, a solution exists even if not unique?  I went through Existence of Solution to Differential Equations. , but this doesn't seem to answer my doubts completely.",,"['ordinary-differential-equations', 'vector-fields']"
92,"Homework Help, Solving particular solution for ODE","Homework Help, Solving particular solution for ODE",,"(a)Find the particular solution to $y''+2y'+y=\frac{5.5e^{-t}}{t^2+1}$ my question is how to find particular solution when right hand side is negative power (b)$x^2y''+19xy'+81y=x^7$ by Euler's equation, let $t=lnx$ we have $\frac{d^2y}{dt^2}+18\frac{dy}{dt}+81y$ but this is only the case when its homogeneous equation. I don't know how the right hand side would be after the substitution","(a)Find the particular solution to $y''+2y'+y=\frac{5.5e^{-t}}{t^2+1}$ my question is how to find particular solution when right hand side is negative power (b)$x^2y''+19xy'+81y=x^7$ by Euler's equation, let $t=lnx$ we have $\frac{d^2y}{dt^2}+18\frac{dy}{dt}+81y$ but this is only the case when its homogeneous equation. I don't know how the right hand side would be after the substitution",,"['calculus', 'ordinary-differential-equations']"
93,Let $F = u \nabla u $. Show that div $F = u \nabla^2 u + \nabla u \cdot \nabla u $,Let . Show that div,F = u \nabla u  F = u \nabla^2 u + \nabla u \cdot \nabla u ,"just a quick question, I'm just a bit off track as to how to derive this. This is what I have: $$F = u \nabla u = u(\frac{\partial u}{\partial x}+\frac{\partial u}{\partial y}+\frac{\partial u}{\partial z})$$ $$ \implies \nabla F = \nabla (u \nabla u) = \nabla(u\frac{\partial u}{\partial x}+u\frac{\partial u}{\partial y}+u\frac{\partial u}{\partial z}) $$ $$ \implies \nabla F = (\frac{\partial u}{\partial x}+\frac{\partial u}{\partial y}+\frac{\partial u}{\partial z})\frac{\partial u}{\partial x}+(\frac{\partial u}{\partial x}+\frac{\partial u}{\partial y}+\frac{\partial u}{\partial z})\frac{\partial u}{\partial y}+(\frac{\partial u}{\partial x}+\frac{\partial u}{\partial y}+\frac{\partial u}{\partial z})\frac{\partial u}{\partial z}$$ Expanding this out, I get the $\nabla u \cdot \nabla u$ term, and I am left with, the terms such as $\frac{\partial u}{\partial y}\frac{\partial u}{\partial x}$, but can't seem to derive $u \nabla^2 u$ from these terms. Any help or guidance would be greatly appreciated!","just a quick question, I'm just a bit off track as to how to derive this. This is what I have: $$F = u \nabla u = u(\frac{\partial u}{\partial x}+\frac{\partial u}{\partial y}+\frac{\partial u}{\partial z})$$ $$ \implies \nabla F = \nabla (u \nabla u) = \nabla(u\frac{\partial u}{\partial x}+u\frac{\partial u}{\partial y}+u\frac{\partial u}{\partial z}) $$ $$ \implies \nabla F = (\frac{\partial u}{\partial x}+\frac{\partial u}{\partial y}+\frac{\partial u}{\partial z})\frac{\partial u}{\partial x}+(\frac{\partial u}{\partial x}+\frac{\partial u}{\partial y}+\frac{\partial u}{\partial z})\frac{\partial u}{\partial y}+(\frac{\partial u}{\partial x}+\frac{\partial u}{\partial y}+\frac{\partial u}{\partial z})\frac{\partial u}{\partial z}$$ Expanding this out, I get the $\nabla u \cdot \nabla u$ term, and I am left with, the terms such as $\frac{\partial u}{\partial y}\frac{\partial u}{\partial x}$, but can't seem to derive $u \nabla^2 u$ from these terms. Any help or guidance would be greatly appreciated!",,['ordinary-differential-equations']
94,"ODEs of the form $a''= -f(b,t) b',\,\,\, b''=f(b,t) a'$",ODEs of the form,"a''= -f(b,t) b',\,\,\, b''=f(b,t) a'","In the course of doing some physics I've encountered serveral systems of the form  $$a''= -f(b,t) b'\\ b''=f(b,t) a'$$ where prime denotes derivative in $t$ and $f$ is maybe a polynomial or e.g. $\cos (a-t)$. I know that it's a non-linear system and I shouldn't expect exact soultions, but I would like to know what sort of methods I might use to obtain analytic approximations (I've successfully used Picard iteration in the linear case $f(t)$) and maybe some indication as to why they are hard to solve. I think but haven't proven that one gets chaotic dynamics even for simple $f$ like $\cos (b)$.","In the course of doing some physics I've encountered serveral systems of the form  $$a''= -f(b,t) b'\\ b''=f(b,t) a'$$ where prime denotes derivative in $t$ and $f$ is maybe a polynomial or e.g. $\cos (a-t)$. I know that it's a non-linear system and I shouldn't expect exact soultions, but I would like to know what sort of methods I might use to obtain analytic approximations (I've successfully used Picard iteration in the linear case $f(t)$) and maybe some indication as to why they are hard to solve. I think but haven't proven that one gets chaotic dynamics even for simple $f$ like $\cos (b)$.",,['ordinary-differential-equations']
95,How to prove a tempered distribution is in $L^p(\mathbb{R}^n)$,How to prove a tempered distribution is in,L^p(\mathbb{R}^n),"Given $g \in L^p(\mathbb{R}^n)$, how can I to prove that the tempered distribution  $$f=\mathcal{F}^{-1}[(z-4\pi^2|x|^2)^{-1}\mathcal{F}g]$$ is in $L^{p}(\mathbb{R}^n)$ where $z \in \{u \in \mathbb{C}-\mathbb{R}_{+};Re(u) \geq 0\}, x \in \mathbb{R}^n$ and $\mathcal{F}$ and $\mathcal{F}^{-1}$ denotes the Fourier Transforms and Inverse Fouries Tranformes respectively? This problems appears in a proof line of theorem 2.3.3, pg 40, from book ""The Theory of Fractional Powers of Operators"", by C. Martínez Carracedo and M. Sanz Alix.","Given $g \in L^p(\mathbb{R}^n)$, how can I to prove that the tempered distribution  $$f=\mathcal{F}^{-1}[(z-4\pi^2|x|^2)^{-1}\mathcal{F}g]$$ is in $L^{p}(\mathbb{R}^n)$ where $z \in \{u \in \mathbb{C}-\mathbb{R}_{+};Re(u) \geq 0\}, x \in \mathbb{R}^n$ and $\mathcal{F}$ and $\mathcal{F}^{-1}$ denotes the Fourier Transforms and Inverse Fouries Tranformes respectively? This problems appears in a proof line of theorem 2.3.3, pg 40, from book ""The Theory of Fractional Powers of Operators"", by C. Martínez Carracedo and M. Sanz Alix.",,"['functional-analysis', 'ordinary-differential-equations', 'lp-spaces', 'distribution-theory']"
96,The function $4x^3y/(x^4+y^2)$ fails the Lipschitz condition near the origin,The function  fails the Lipschitz condition near the origin,4x^3y/(x^4+y^2),"I have to prove that Lipschitz condition is  not satisfied for the function, $$ f(x) = \begin{cases} {4x^3y \over x^4 +y^2},  & \text{if $(x,y) \neq (0,0)$ } \\ 0, & \text{if $(x,y)=(0,0)$ } \end{cases}$$ throughout any domain which includes $(0,0)$. I considered ,the domain $D = \{(x,y) : |x| \le a , |y|\le b,a \gt b\}$, and then considered, $$f(x,y_1)-f(x,y_2) = 4x^3\left[{y_1 \over x^4+y_1^2} - {y_2 \over x^4+y_2^2}\right]$$ How to  proceed further?","I have to prove that Lipschitz condition is  not satisfied for the function, $$ f(x) = \begin{cases} {4x^3y \over x^4 +y^2},  & \text{if $(x,y) \neq (0,0)$ } \\ 0, & \text{if $(x,y)=(0,0)$ } \end{cases}$$ throughout any domain which includes $(0,0)$. I considered ,the domain $D = \{(x,y) : |x| \le a , |y|\le b,a \gt b\}$, and then considered, $$f(x,y_1)-f(x,y_2) = 4x^3\left[{y_1 \over x^4+y_1^2} - {y_2 \over x^4+y_2^2}\right]$$ How to  proceed further?",,"['ordinary-differential-equations', 'lipschitz-functions']"
97,"What to do when regular approaches fail on linear, non-homogeneous ODES.","What to do when regular approaches fail on linear, non-homogeneous ODES.",,"In my research problem, I have come across the following form of a time varying, non-homogeneous ordinary differential equation. $$\dot x + \frac{k_1}{t} x = k_2t^{3n}\sin(bt) + k_3 t^n \sin(bt) - k_4 t^{3n} \cos(bt) + k_5 t^{n} \cos(bt)$$ where the $k_i$'s are constants. I found in a book by Henry D'Angelo that as long as the time-varying coefficient is continuous (which it is in the interval that I'm examining said system), a solution exists for this system. I used Mathematica to compute an analytical solution which it does fairly rapidly but the solution involves incomplete gamma functions. If the second term didn't exist i.e. $\frac{k_1}{t} x$, then I can see where the incomplete gamma integral might come from. I do not, however understand how Mathematica constructed the analytical solution. I have tested the analytical result versus numerical results and they seem to behave identically which tests the validity of the analytical result (am I wrong in assuming this?). If anyone can share some input on how one goes about attempting to generate solutions for such a system, I'd be much in gratitude as one can't use traditional approaches to determine $x(t)$. In fact, any reference/discussion at all would be excellent, too.","In my research problem, I have come across the following form of a time varying, non-homogeneous ordinary differential equation. $$\dot x + \frac{k_1}{t} x = k_2t^{3n}\sin(bt) + k_3 t^n \sin(bt) - k_4 t^{3n} \cos(bt) + k_5 t^{n} \cos(bt)$$ where the $k_i$'s are constants. I found in a book by Henry D'Angelo that as long as the time-varying coefficient is continuous (which it is in the interval that I'm examining said system), a solution exists for this system. I used Mathematica to compute an analytical solution which it does fairly rapidly but the solution involves incomplete gamma functions. If the second term didn't exist i.e. $\frac{k_1}{t} x$, then I can see where the incomplete gamma integral might come from. I do not, however understand how Mathematica constructed the analytical solution. I have tested the analytical result versus numerical results and they seem to behave identically which tests the validity of the analytical result (am I wrong in assuming this?). If anyone can share some input on how one goes about attempting to generate solutions for such a system, I'd be much in gratitude as one can't use traditional approaches to determine $x(t)$. In fact, any reference/discussion at all would be excellent, too.",,"['calculus', 'ordinary-differential-equations', 'dynamical-systems']"
98,Problem in Differential Equations,Problem in Differential Equations,,"Solution curves for the differential equations 1) y' = max{y,y^2} 2) y‘ = min {y,y^2} Please can anybody help me because I am really confused","Solution curves for the differential equations 1) y' = max{y,y^2} 2) y‘ = min {y,y^2} Please can anybody help me because I am really confused",,['ordinary-differential-equations']
99,Why aren't exact differential equations considered PDE?,Why aren't exact differential equations considered PDE?,,"Exact differential equations come from finding the total differential from some multivariable function. In the exact differential equation $M\mathrm{d}x+N\mathrm{d}y=0$ M and N are considered to be partial derivatives of some potential function... So why aren't exact differential equations considered PDEs? After all, you're finding the potential function given it's partial derivatives... Thanks.","Exact differential equations come from finding the total differential from some multivariable function. In the exact differential equation $M\mathrm{d}x+N\mathrm{d}y=0$ M and N are considered to be partial derivatives of some potential function... So why aren't exact differential equations considered PDEs? After all, you're finding the potential function given it's partial derivatives... Thanks.",,['ordinary-differential-equations']
