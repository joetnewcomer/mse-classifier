,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Find limit without using l'Hospital rule $\lim\limits_{x\rightarrow0}\frac{\tan x-\sin x}{x^3}$,Find limit without using l'Hospital rule,\lim\limits_{x\rightarrow0}\frac{\tan x-\sin x}{x^3},How to find the following limit without using l'Hospital rule $$\lim_{x\rightarrow0}\frac{\tan x-\sin x}{x^3}$$ Using l'Hospital I got $1\over2$. Thanks for your help.,How to find the following limit without using l'Hospital rule $$\lim_{x\rightarrow0}\frac{\tan x-\sin x}{x^3}$$ Using l'Hospital I got $1\over2$. Thanks for your help.,,['calculus']
1,"Examples of open ended calculus ""class project"" ideas","Examples of open ended calculus ""class project"" ideas",,"I have instructed calculus I an II, each once, at the college level and would like to emphasize that math is not just about memorizing formulas and concepts for a test and that applied math is not a bunch of contrived word problems. I would like to encourage my students to do a a ""final project"" the next time I teach the course (or at least the next time I have a bit more power to set the curriculum). For those of you who have done this before, what types of questions have you asked? and what were the students solutions like? Did you find your projects were manageable enough with just the calculus they learned and some basic research about whatever topic they chose? The next time you teach calculus will you do it again? I'd like to have at least one or two theory and application choices, plus an option for them to choose their own topic (requiring my approval for this option).","I have instructed calculus I an II, each once, at the college level and would like to emphasize that math is not just about memorizing formulas and concepts for a test and that applied math is not a bunch of contrived word problems. I would like to encourage my students to do a a ""final project"" the next time I teach the course (or at least the next time I have a bit more power to set the curriculum). For those of you who have done this before, what types of questions have you asked? and what were the students solutions like? Did you find your projects were manageable enough with just the calculus they learned and some basic research about whatever topic they chose? The next time you teach calculus will you do it again? I'd like to have at least one or two theory and application choices, plus an option for them to choose their own topic (requiring my approval for this option).",,"['calculus', 'education']"
2,Series of inverses of binomial coefficients,Series of inverses of binomial coefficients,,"Can you think of a simple way of proving that $$ \sum_{n=k+1}^\infty \frac{1}{n \choose k} $$  is rational for any $k \geq 2$? Here's the background. Consider a series: $$ \sum_{n=1}^\infty \frac{1}{n(n+1)} $$ Elementary algebra gives us that: $$ \sum_{n=1}^\infty \frac{1}{n(n+1)} = \sum_{n=1}^\infty \left(\frac{1}{n} - \frac{1}{n+1}\right) = \lim_{k\to\infty} (1 - \frac{1}{k+1}) = 1 $$ so the sum turns out to be rational. Next, consider  $$ \sum_{n=1}^\infty \frac{1}{n(n+1)(n+2)(n+3)(n+4)} $$ With the same method, but much more effort we can show that: $$ \sum_{n=1}^\infty \frac{1}{n(n+1)(n+2)(n+3)(n+4)} = \sum_{n=1}^\infty \left(\frac{1}{24 n}-\frac{1}{6(n+1)}+\frac{1}{4(n+2)}-\frac{1}{6(n+3)}+\frac{1}{24(n+4)}\right) $$ and again we'll see that stuff cancels out, and the sum is again rational. So the obvious conjecture is that this method will work for arbitrary (but $\geq 2$) fixed number of factors in denominator, and the sum will always be rational. Indeed that's the case. I provide a solution as an answer, but I'm not fully satisfied with it (it seems for me to be too brute force), so I'm looking for alternative solutions.","Can you think of a simple way of proving that $$ \sum_{n=k+1}^\infty \frac{1}{n \choose k} $$  is rational for any $k \geq 2$? Here's the background. Consider a series: $$ \sum_{n=1}^\infty \frac{1}{n(n+1)} $$ Elementary algebra gives us that: $$ \sum_{n=1}^\infty \frac{1}{n(n+1)} = \sum_{n=1}^\infty \left(\frac{1}{n} - \frac{1}{n+1}\right) = \lim_{k\to\infty} (1 - \frac{1}{k+1}) = 1 $$ so the sum turns out to be rational. Next, consider  $$ \sum_{n=1}^\infty \frac{1}{n(n+1)(n+2)(n+3)(n+4)} $$ With the same method, but much more effort we can show that: $$ \sum_{n=1}^\infty \frac{1}{n(n+1)(n+2)(n+3)(n+4)} = \sum_{n=1}^\infty \left(\frac{1}{24 n}-\frac{1}{6(n+1)}+\frac{1}{4(n+2)}-\frac{1}{6(n+3)}+\frac{1}{24(n+4)}\right) $$ and again we'll see that stuff cancels out, and the sum is again rational. So the obvious conjecture is that this method will work for arbitrary (but $\geq 2$) fixed number of factors in denominator, and the sum will always be rational. Indeed that's the case. I provide a solution as an answer, but I'm not fully satisfied with it (it seems for me to be too brute force), so I'm looking for alternative solutions.",,"['calculus', 'sequences-and-series', 'binomial-coefficients', 'power-series']"
3,prove that the following function is: $f(x) = 0$,prove that the following function is:,f(x) = 0,"let $f: [0,1] \to \mathbb R$ , $f$ is differentiable $f(0) = 0$ $|f'(x)|\le|f(x)|$ for $x\in [0,1]$ prove that $f(x) =0$ for $x\in [0,1]$ i believe that i need to somehow use the mean value theorem iteratively any hints?","let , is differentiable for prove that for i believe that i need to somehow use the mean value theorem iteratively any hints?","f: [0,1] \to \mathbb R f f(0) = 0 |f'(x)|\le|f(x)| x\in [0,1] f(x) =0 x\in [0,1]","['calculus', 'ordinary-differential-equations']"
4,Prove that $\|a\|+\|b\| + \|c\| + \|a+b+c\| \geq \|a+b\| + \|b+c\| + \|c +a\|$ in the plane.,Prove that  in the plane.,\|a\|+\|b\| + \|c\| + \|a+b+c\| \geq \|a+b\| + \|b+c\| + \|c +a\|,"Prove that $\|a\| + \|b\| + \|c\| + \|a+b+c\| \geq \|a+b\| + \|b+c\| + \|c +a\|$ in the plane. Gentle hints only, please! I know that attempting to decompose R.H.S. into $$\alpha a + \beta b + \gamma c + \delta (a+b+c) = a + b$$ so that $$\alpha \|a\| + \beta \|b\| + \gamma \|c\| + \delta \|a+b+c\| \geq \|a + b\|$$ and sum over all L.H.S. terms does not work. I also know that I can interpret $\|a+b\|+\|b+c\|+\|c+a\|$ as the ""straighter"" path from $0$ to $2a+2b+2c$. However, I haven't been able to translate that intuition into a proof! Hints only please!","Prove that $\|a\| + \|b\| + \|c\| + \|a+b+c\| \geq \|a+b\| + \|b+c\| + \|c +a\|$ in the plane. Gentle hints only, please! I know that attempting to decompose R.H.S. into $$\alpha a + \beta b + \gamma c + \delta (a+b+c) = a + b$$ so that $$\alpha \|a\| + \beta \|b\| + \gamma \|c\| + \delta \|a+b+c\| \geq \|a + b\|$$ and sum over all L.H.S. terms does not work. I also know that I can interpret $\|a+b\|+\|b+c\|+\|c+a\|$ as the ""straighter"" path from $0$ to $2a+2b+2c$. However, I haven't been able to translate that intuition into a proof! Hints only please!",,"['calculus', 'linear-algebra', 'inequality', 'analytic-geometry', 'triangles']"
5,How to find the inverse Mellin transform?,How to find the inverse Mellin transform?,,"On the wikipedia page http://en.wikipedia.org/wiki/Mellin_transform The second formula is an integral transformation for the inverse Mellin transform. Being new to integral transforms, I wonder how that formula was reached. In fact, how do we prove that transform is indeed the inverse of the Mellin transform? I know a bit about contour integration and Fourier series but I'm still confused. Do we need to work with residues or can we just 'plug things in' ?","On the wikipedia page http://en.wikipedia.org/wiki/Mellin_transform The second formula is an integral transformation for the inverse Mellin transform. Being new to integral transforms, I wonder how that formula was reached. In fact, how do we prove that transform is indeed the inverse of the Mellin transform? I know a bit about contour integration and Fourier series but I'm still confused. Do we need to work with residues or can we just 'plug things in' ?",,"['calculus', 'complex-analysis', 'integral-transforms', 'mellin-transform']"
6,Identity concerning complete elliptic integrals,Identity concerning complete elliptic integrals,,"It can be easily checked that both the complete elliptic integrals $K(k), K'(k)$ satisfy the same second order differential equation $$kk'^{2}\frac{d^{2}y}{dk^{2}} + (1 - 3k^{2})\frac{dy}{dk} - ky = 0$$ and hence from the theory of second order differential equations there is a relation of the form $$K'(k) = cK(k)\cdot\log k + f(k)$$ where $c$ is some constant and $f(k)$ is some analytic function of $k$. The exact relation between $K(k)$ and $K'(k)$ is given by $$K'(k) = \frac{2K(k)}{\pi}\log\left(\frac{4}{k}\right) - 2\left[\left(\frac{1}{2}\right)^{2}\left(\frac{1}{1\cdot 2}\right)k^{2} + \left(\frac{1\cdot 3}{2\cdot 4}\right)^{2}\left(\frac{1}{1\cdot 2} + \frac{1}{3\cdot 4}\right)k^{4} + \cdots\right]$$ It can be verified with some patience that the RHS does satisfy the differential equation given above and thereby the relation between $K'(k)$ and $K(k)$ can be established. However is there an alternative proof based on the definition of $K'(k)$ and $K(k)$ as complete elliptic integrals or using the hypergeometric relation $$\frac{2K(k)}{\pi} =\,_{2}F_{1}\left(\frac{1}{2}, \frac{1}{2}; 1; k^{2}\right)$$ which can be presented to someone unaware of the theory of second order differential equations?","It can be easily checked that both the complete elliptic integrals $K(k), K'(k)$ satisfy the same second order differential equation $$kk'^{2}\frac{d^{2}y}{dk^{2}} + (1 - 3k^{2})\frac{dy}{dk} - ky = 0$$ and hence from the theory of second order differential equations there is a relation of the form $$K'(k) = cK(k)\cdot\log k + f(k)$$ where $c$ is some constant and $f(k)$ is some analytic function of $k$. The exact relation between $K(k)$ and $K'(k)$ is given by $$K'(k) = \frac{2K(k)}{\pi}\log\left(\frac{4}{k}\right) - 2\left[\left(\frac{1}{2}\right)^{2}\left(\frac{1}{1\cdot 2}\right)k^{2} + \left(\frac{1\cdot 3}{2\cdot 4}\right)^{2}\left(\frac{1}{1\cdot 2} + \frac{1}{3\cdot 4}\right)k^{4} + \cdots\right]$$ It can be verified with some patience that the RHS does satisfy the differential equation given above and thereby the relation between $K'(k)$ and $K(k)$ can be established. However is there an alternative proof based on the definition of $K'(k)$ and $K(k)$ as complete elliptic integrals or using the hypergeometric relation $$\frac{2K(k)}{\pi} =\,_{2}F_{1}\left(\frac{1}{2}, \frac{1}{2}; 1; k^{2}\right)$$ which can be presented to someone unaware of the theory of second order differential equations?",,"['calculus', 'alternative-proof', 'elliptic-integrals']"
7,Why do I get a negative value for this integral?,Why do I get a negative value for this integral?,,"I am trying to compute the integral: $$\int_{4}^{5} \frac{dx}{\sqrt{x^{2}-16}}$$ The question is related to hyperbolic functions, so I let $x = 4\cosh(u)$ therefore the integral becomes: $$-\int_{0}^{\ln(2)}\frac{4\sinh(u)}{\sqrt{16-16\cosh^{2}(u)}}du = -\int_{0}^{\ln(2)}1du = -\ln(2)$$ The answer is $\ln(2)$ so if someone could point out where I went wrong that would be great, thanks","I am trying to compute the integral: $$\int_{4}^{5} \frac{dx}{\sqrt{x^{2}-16}}$$ The question is related to hyperbolic functions, so I let $x = 4\cosh(u)$ therefore the integral becomes: $$-\int_{0}^{\ln(2)}\frac{4\sinh(u)}{\sqrt{16-16\cosh^{2}(u)}}du = -\int_{0}^{\ln(2)}1du = -\ln(2)$$ The answer is $\ln(2)$ so if someone could point out where I went wrong that would be great, thanks",,"['calculus', 'integration']"
8,infinity times infinitesimal - what happens?,infinity times infinitesimal - what happens?,,"So what happens if we multiply infinite number by. Infinitesimal number? Like $dx \times \infty$ where $dx$ is treated as in one-dimensional integration. Also, can we divide infinite number by infinite number and get a finite number?","So what happens if we multiply infinite number by. Infinitesimal number? Like $dx \times \infty$ where $dx$ is treated as in one-dimensional integration. Also, can we divide infinite number by infinite number and get a finite number?",,"['calculus', 'infinity']"
9,Find the value of $\lim_{n \rightarrow \infty} \sqrt{1+\left(\frac1{2n}\right)^n}$,Find the value of,\lim_{n \rightarrow \infty} \sqrt{1+\left(\frac1{2n}\right)^n},"Find the limit of the sequence as it approches $\infty$   $$\sqrt{1+\left(\frac1{2n}\right)^n}$$ I made a table of the values of the sequence and the values approach 1, so why is the limit $e^{1/4}$? I know that if the answer is $e^{1/4}$ I must have to take the $\ln$ of the sequence but how and where do I do that with the square root? I did some work getting the sequence into an indeterminate form and trying to use L'Hospitals but I'm not sure if it's right and then where to go from there. Here is the work I've done $$\sqrt{1+\left(\frac1{2n}\right)^n} = \frac1{2n} \ln \left(1+\frac1{2n}\right) = \lim_{x\to\infty} \frac1{2n} \ln \left(1+\frac1{2n}\right) \\ = \frac 1{1+\frac1{2n}}\cdot-\frac1{2n^2}\div-\frac1{2n^2}$$ Thank you","Find the limit of the sequence as it approches $\infty$   $$\sqrt{1+\left(\frac1{2n}\right)^n}$$ I made a table of the values of the sequence and the values approach 1, so why is the limit $e^{1/4}$? I know that if the answer is $e^{1/4}$ I must have to take the $\ln$ of the sequence but how and where do I do that with the square root? I did some work getting the sequence into an indeterminate form and trying to use L'Hospitals but I'm not sure if it's right and then where to go from there. Here is the work I've done $$\sqrt{1+\left(\frac1{2n}\right)^n} = \frac1{2n} \ln \left(1+\frac1{2n}\right) = \lim_{x\to\infty} \frac1{2n} \ln \left(1+\frac1{2n}\right) \\ = \frac 1{1+\frac1{2n}}\cdot-\frac1{2n^2}\div-\frac1{2n^2}$$ Thank you",,"['calculus', 'sequences-and-series', 'limits']"
10,"If $f(\frac{x}{y})=\frac{f(x)}{f(y)} \, , f(y),y \neq 0$ and $f'(1)=2$ then $f(x)=$?",If  and  then ?,"f(\frac{x}{y})=\frac{f(x)}{f(y)} \, , f(y),y \neq 0 f'(1)=2 f(x)=","If $f(\frac{x}{y})=\frac{f(x)}{f(y)} \, , f(y),y \neq 0$ and $f'(1)=2$ then $f(x)=$? I am not sure where to begin, any hints on starting and steps is apreciated. Thank you","If $f(\frac{x}{y})=\frac{f(x)}{f(y)} \, , f(y),y \neq 0$ and $f'(1)=2$ then $f(x)=$? I am not sure where to begin, any hints on starting and steps is apreciated. Thank you",,['calculus']
11,"If $\int_0^\infty f\text{d}x$ exists, does $\lim_{x\to\infty}f(x)=0$?","If  exists, does ?",\int_0^\infty f\text{d}x \lim_{x\to\infty}f(x)=0,"Are there examples of functions $f$ such that $\int_0^\infty f\text{d}x$ exists, but $\lim_{x\to\infty}f(x)\neq 0$? I curious because I know for infinite series, if $a_n\not\to 0$, then $\sum a_n$ diverges. I'm wondering if there is something similar for improper integrals.","Are there examples of functions $f$ such that $\int_0^\infty f\text{d}x$ exists, but $\lim_{x\to\infty}f(x)\neq 0$? I curious because I know for infinite series, if $a_n\not\to 0$, then $\sum a_n$ diverges. I'm wondering if there is something similar for improper integrals.",,['calculus']
12,"$(a+b)^\beta \leq a^\beta +b^\beta$ for $a,b\geq0$ and $0\leq\beta\leq1$ [duplicate]",for  and  [duplicate],"(a+b)^\beta \leq a^\beta +b^\beta a,b\geq0 0\leq\beta\leq1","This question already has answers here : Prove that $(p+q)^m \leq p^m+q^m$ (2 answers) Closed 9 years ago . It seems that $(a+b)^\beta \leq a^\beta +b^\beta$ for $a,b\geq0$ and $0\leq\beta\leq1$. However, I could not prove this nor the same result for a general concave and increasing function (for which it might not hold). If the inequality is true, does it follow from some general inequality or is there some other simple proof?","This question already has answers here : Prove that $(p+q)^m \leq p^m+q^m$ (2 answers) Closed 9 years ago . It seems that $(a+b)^\beta \leq a^\beta +b^\beta$ for $a,b\geq0$ and $0\leq\beta\leq1$. However, I could not prove this nor the same result for a general concave and increasing function (for which it might not hold). If the inequality is true, does it follow from some general inequality or is there some other simple proof?",,"['calculus', 'algebra-precalculus', 'inequality']"
13,Always a differentiable path through a convergent sequence of points in $\mathbb{R}^n$?,Always a differentiable path through a convergent sequence of points in ?,\mathbb{R}^n,"I've come up with this question in trying to solve a vaguely related exercise: If $x_n$ is any sequence of points in $\mathbb{R}^n$ with $x_n \longrightarrow 0$, is there a path $\gamma(t)$, $\gamma(0)=0$ that goes through all $x_n$ and which is differentiable at $t=0$ ? Thank you.","I've come up with this question in trying to solve a vaguely related exercise: If $x_n$ is any sequence of points in $\mathbb{R}^n$ with $x_n \longrightarrow 0$, is there a path $\gamma(t)$, $\gamma(0)=0$ that goes through all $x_n$ and which is differentiable at $t=0$ ? Thank you.",,['calculus']
14,Exponential of formal power series and Bell polynomials,Exponential of formal power series and Bell polynomials,,"Wikipedia gives here the following formula for the exponential of a formal power series: $\exp \Big[\  \sum_{n=1}^\infty \frac{a_n}{n!} x^n\ \Big] = \sum_{n=0}^\infty \frac{B_n(a_1,\dots,a_n)}{n!} x^n$ where $B_n$ are (complete) Bell-polynomials. Can anybody give me a (""standard"") reference for this?","Wikipedia gives here the following formula for the exponential of a formal power series: $\exp \Big[\  \sum_{n=1}^\infty \frac{a_n}{n!} x^n\ \Big] = \sum_{n=0}^\infty \frac{B_n(a_1,\dots,a_n)}{n!} x^n$ where $B_n$ are (complete) Bell-polynomials. Can anybody give me a (""standard"") reference for this?",,"['calculus', 'combinatorics', 'analysis', 'reference-request', 'power-series']"
15,Expressions for the second derivative,Expressions for the second derivative,,"Suppose that $f$ has continuous second derivatives. How do I show that $$\frac{f(x+h) + f(x-h) - 2f(x)}{h^2}$$ and $$2\frac{f(x+h) - f(x) - f'(x)h}{h^2}$$ both tend to $f''(x)$  as $h \rightarrow 0$? For the first expression, I can rewrite it as $$\lim_{h \rightarrow 0} \frac{1}{h}(\frac{f(x+h) - f(x)}{h} - \frac{f(x) - f(x-h)}{h})$$ which I can sort of see should tend to $f''(x)$, but I can't seem to show it rigorously. For the second expression, I can rewrite it as $$\lim_{h\rightarrow 0} \frac{(f(x+h)-f(x))/h - f'(x)}{h}$$ I'm not sure where the factor of 2 comes in, but I guess it should have to do with the fact that we're trying to take limits ""simultaneously"" for $f'$ and $f''$. Can anyone help? Thanks.","Suppose that $f$ has continuous second derivatives. How do I show that $$\frac{f(x+h) + f(x-h) - 2f(x)}{h^2}$$ and $$2\frac{f(x+h) - f(x) - f'(x)h}{h^2}$$ both tend to $f''(x)$  as $h \rightarrow 0$? For the first expression, I can rewrite it as $$\lim_{h \rightarrow 0} \frac{1}{h}(\frac{f(x+h) - f(x)}{h} - \frac{f(x) - f(x-h)}{h})$$ which I can sort of see should tend to $f''(x)$, but I can't seem to show it rigorously. For the second expression, I can rewrite it as $$\lim_{h\rightarrow 0} \frac{(f(x+h)-f(x))/h - f'(x)}{h}$$ I'm not sure where the factor of 2 comes in, but I guess it should have to do with the fact that we're trying to take limits ""simultaneously"" for $f'$ and $f''$. Can anyone help? Thanks.",,['calculus']
16,"Finding limit of function with more than one variable: $\lim_{(x,y)\to(0,0)} \frac{xy}{\sqrt{x^2+y^2}}$",Finding limit of function with more than one variable:,"\lim_{(x,y)\to(0,0)} \frac{xy}{\sqrt{x^2+y^2}}","$$\lim_{(x,y)\to(0,0)} \frac{xy}{\sqrt{x^2+y^2}}$$ Approaching (0,0) along x or along y both result in the limit approaching 0, so you want to make sure that the limit exists by doing more tests. My solutions manual uses $x = y^2$ (or $y = x^2$). Why either of those? Why not $y=x$ or $x=y$? Why a parabola?","$$\lim_{(x,y)\to(0,0)} \frac{xy}{\sqrt{x^2+y^2}}$$ Approaching (0,0) along x or along y both result in the limit approaching 0, so you want to make sure that the limit exists by doing more tests. My solutions manual uses $x = y^2$ (or $y = x^2$). Why either of those? Why not $y=x$ or $x=y$? Why a parabola?",,"['calculus', 'limits', 'multivariable-calculus']"
17,"Spivak's Calculus exercise. Chapter 10, Problem 27","Spivak's Calculus exercise. Chapter 10, Problem 27",,"Suppose that $f$ is differentiable at   0, and that $f(0) = 0$. Prove that   $f(x) = xg(x)$ for some function $g$   which is continuous at 0. This is a problem from Spivak's Calculus , namely problem 27 of Chapter 10. (This is not homework, but rather self-study.) I am not sure how to go about this proof. The hint given in the text is to consider that $g(x)$ can be written as $f(x)/x$, but this puzzles me, because then continuity of $g$ at 0 says that $\lim_{x \to 0} g(x) = g(0) = f(0)/0 = 0/0$.","Suppose that $f$ is differentiable at   0, and that $f(0) = 0$. Prove that   $f(x) = xg(x)$ for some function $g$   which is continuous at 0. This is a problem from Spivak's Calculus , namely problem 27 of Chapter 10. (This is not homework, but rather self-study.) I am not sure how to go about this proof. The hint given in the text is to consider that $g(x)$ can be written as $f(x)/x$, but this puzzles me, because then continuity of $g$ at 0 says that $\lim_{x \to 0} g(x) = g(0) = f(0)/0 = 0/0$.",,['calculus']
18,How to prove that a conditionally convergent series can be rearranged to sum to any real number?,How to prove that a conditionally convergent series can be rearranged to sum to any real number?,,"There is a  theorem of Riemann to that effect. How to prove it? Note: This was asked by Kenny in the beta for ""calculus"".","There is a  theorem of Riemann to that effect. How to prove it? Note: This was asked by Kenny in the beta for ""calculus"".",,[]
19,How to evaluate this integral without using numerical method?,How to evaluate this integral without using numerical method?,,$$\int_{-0.5}^{0.5} \frac{x^2\cos(4\pi x^3)}{1+e^x} dx $$ I tried it using Numerical method and got roughly the same with Wolfram. Are there any other methods to evaluate this integral?,I tried it using Numerical method and got roughly the same with Wolfram. Are there any other methods to evaluate this integral?,\int_{-0.5}^{0.5} \frac{x^2\cos(4\pi x^3)}{1+e^x} dx ,"['calculus', 'integration']"
20,Is there a good way to simplify this expression?,Is there a good way to simplify this expression?,,"The Short Version Is there a way to simplify this expression? $$ \left(\left(\left(d × \left(\frac{j}{2}\right)^2\right)^2 − \frac{1}{27} × \left(\frac{j^2}{2 s}\right)^6\right)^\frac{1}{2} + d × \left(\frac{j}{2}\right)^2\right)^\frac{1}{3}  + \frac{1}{3} × \left(\frac{j^{2}}{2 s}\right)^2 × \left(\left(\left(d × \left(\frac{j}{2}\right)^2\right)^2 − \frac{1}{27} × \left(\frac{j^2}{2 s}\right)^6\right)^\frac{1}{2} + d × \left(\frac{j}{2}\right)^2\right)^\frac{−1}{3}  − \frac{j^2}{2 s} $$ Specifically, I'd like to condense the cube root portion and the inverse cube root portion into a single root, if possible. The Longer Version We have five positive-valued variables: $s_{max}$ $j_{max}$ $a_{max}$ $v_{max}$ $d_{max}$ We also have the function $\operatorname{Min}(n_1,\, n_2,\, …,\, n_k)$ , which returns the input with the lowest value. Finally, we have three variables whose values are each based on combinations of the previously-defined variables: $$ j_{limit}\,=\,{\operatorname{Min}\begin{pmatrix} \left(j_{max} × \frac{s_{max}^0}{0!}\right)^\frac{1}{1},\\ \left(a_{max} × \frac{s_{max}^1}{1!}\right)^\frac{1}{2},\\ \left(v_{max} × \frac{s_{max}^2}{2!}\right)^\frac{1}{3},\\ \left(d_{max} × \frac{s_{max}^3}{3!}\right)^\frac{1}{4} \end{pmatrix}} $$ $$ a_{limit}\,=\,{\operatorname{Min}\begin{pmatrix} a_{max},\\ \left(v_{max} × j_{limit} + \left(\frac{j_{limit}^2}{2 s_{max}}\right)^2\right)^\frac{1}{2} - \frac{j_{limit}^2}{2 s_{max}},\\ \left(\left(\left(d_{max} × \left(\frac{j_{limit}}{2}\right)^2\right)^2 − \frac{1}{27} × \left(\frac{j_{limit}^2}{2 s_{max}}\right)^6\right)^\frac{1}{2} + d_{max} × \left(\frac{j_{limit}}{2}\right)^2\right)^\frac{1}{3} + \frac{1}{3} × \left(\frac{j_{limit}^{2}}{2 s_{max}}\right)^2 × \left(\left(\left(d_{max} × \left(\frac{j_{limit}}{2}\right)^2\right)^2 − \frac{1}{27} × \left(\frac{j_{limit}^2}{2 s_{max}}\right)^6\right)^\frac{1}{2} + d_{max} × \left(\frac{j_{limit}}{2}\right)^2\right)^\frac{−1}{3} − \frac{j_{limit}^2}{2 s_{max}} \end{pmatrix}} $$ and $$ v_{limit}\,=\,{\operatorname{Min}\begin{pmatrix} v_{max},\\ \left(d_{max} × a_{limit} + \left(\frac{a_{limit}^2}{2 j_{limit}}\right)^2\right)^\frac{1}{2} - \frac{a_{limit}^2}{2 j_{limit}} \end{pmatrix}} $$ The third option for the value of $a_{limit}$ stands out for being so much longer than all the other expressions in the values of the limit-variables. Can it be condensed any? Is there a better way to write it? As-is, it feels very unsatisfying.","The Short Version Is there a way to simplify this expression? Specifically, I'd like to condense the cube root portion and the inverse cube root portion into a single root, if possible. The Longer Version We have five positive-valued variables: We also have the function , which returns the input with the lowest value. Finally, we have three variables whose values are each based on combinations of the previously-defined variables: and The third option for the value of stands out for being so much longer than all the other expressions in the values of the limit-variables. Can it be condensed any? Is there a better way to write it? As-is, it feels very unsatisfying.","
\left(\left(\left(d × \left(\frac{j}{2}\right)^2\right)^2 − \frac{1}{27} × \left(\frac{j^2}{2 s}\right)^6\right)^\frac{1}{2} + d × \left(\frac{j}{2}\right)^2\right)^\frac{1}{3}
 + \frac{1}{3} × \left(\frac{j^{2}}{2 s}\right)^2 × \left(\left(\left(d × \left(\frac{j}{2}\right)^2\right)^2 − \frac{1}{27} × \left(\frac{j^2}{2 s}\right)^6\right)^\frac{1}{2} + d × \left(\frac{j}{2}\right)^2\right)^\frac{−1}{3}
 − \frac{j^2}{2 s}
 s_{max} j_{max} a_{max} v_{max} d_{max} \operatorname{Min}(n_1,\, n_2,\, …,\, n_k) 
j_{limit}\,=\,{\operatorname{Min}\begin{pmatrix}
\left(j_{max} × \frac{s_{max}^0}{0!}\right)^\frac{1}{1},\\
\left(a_{max} × \frac{s_{max}^1}{1!}\right)^\frac{1}{2},\\
\left(v_{max} × \frac{s_{max}^2}{2!}\right)^\frac{1}{3},\\
\left(d_{max} × \frac{s_{max}^3}{3!}\right)^\frac{1}{4}
\end{pmatrix}}
 
a_{limit}\,=\,{\operatorname{Min}\begin{pmatrix}
a_{max},\\
\left(v_{max} × j_{limit} + \left(\frac{j_{limit}^2}{2 s_{max}}\right)^2\right)^\frac{1}{2} - \frac{j_{limit}^2}{2 s_{max}},\\
\left(\left(\left(d_{max} × \left(\frac{j_{limit}}{2}\right)^2\right)^2 − \frac{1}{27} × \left(\frac{j_{limit}^2}{2 s_{max}}\right)^6\right)^\frac{1}{2} + d_{max} × \left(\frac{j_{limit}}{2}\right)^2\right)^\frac{1}{3} + \frac{1}{3} × \left(\frac{j_{limit}^{2}}{2 s_{max}}\right)^2 × \left(\left(\left(d_{max} × \left(\frac{j_{limit}}{2}\right)^2\right)^2 − \frac{1}{27} × \left(\frac{j_{limit}^2}{2 s_{max}}\right)^6\right)^\frac{1}{2} + d_{max} × \left(\frac{j_{limit}}{2}\right)^2\right)^\frac{−1}{3} − \frac{j_{limit}^2}{2 s_{max}}
\end{pmatrix}}
 
v_{limit}\,=\,{\operatorname{Min}\begin{pmatrix}
v_{max},\\
\left(d_{max} × a_{limit} + \left(\frac{a_{limit}^2}{2 j_{limit}}\right)^2\right)^\frac{1}{2} - \frac{a_{limit}^2}{2 j_{limit}}
\end{pmatrix}}
 a_{limit}","['calculus', 'polynomials', 'continuity', 'piecewise-continuity']"
21,Show that function's inverse is not continuous at a point,Show that function's inverse is not continuous at a point,,"I have the following problem: Show a bijective function $f$ where $f'(0)$ is equal to $1$ and where the $f^{-1}(x)$ is not cont. at $f(1)$ . So far, I know that the derivative of an inverse function $f^{-1}(f(x))$ is $(f^{-1})'(f(x))=\frac{1}{f'(f^{-1}(f(x)))}=\frac{1}{f'(x)}$ , and that $f^{-1}$ not being continuous at $f(0)$ would mean that it is not differentiable there either. Don't know where to go from there however. Any help would be appreciated!","I have the following problem: Show a bijective function where is equal to and where the is not cont. at . So far, I know that the derivative of an inverse function is , and that not being continuous at would mean that it is not differentiable there either. Don't know where to go from there however. Any help would be appreciated!",f f'(0) 1 f^{-1}(x) f(1) f^{-1}(f(x)) (f^{-1})'(f(x))=\frac{1}{f'(f^{-1}(f(x)))}=\frac{1}{f'(x)} f^{-1} f(0),"['calculus', 'derivatives', 'inverse']"
22,"Why is $\frac{d}{dx}\ln(x) = \frac1x$ when the domain of $\frac1x$ includes $(-\infty,0)$?",Why is  when the domain of  includes ?,"\frac{d}{dx}\ln(x) = \frac1x \frac1x (-\infty,0)","On Khan Academy , they factor the derivative: $$ \frac{d}{dx}\frac{x^2 + x - 2}{x-1} = \frac{d}{dx}(x+2) = 1 \text { where $x \ne 1$}  $$ Khan says: Recall the derivative is equal to $1$ for all $x$ values where the function is defined. Since the function is undefined for $x=1$ , so is the derivative. I did some Google searching to see if this was really a rule, and I found this Quora question .  One of the answers provides a seeming counterexample: $$ \ln(x) \text{ has a domain } (0,∞) \\ \frac{d}{dx}\ln(x)=\frac{1}{x} \\ \frac{1}{x} \text { has the domain} (-∞,0) \text{ and } (0,∞) \\ $$ So is Khan stating an incorrect rule?  Obviously it works in the case of his problem, but I'd like to understand what the accurate rule is here.  Maybe the key question is what type of discontinuity we're dealing with?","On Khan Academy , they factor the derivative: Khan says: Recall the derivative is equal to for all values where the function is defined. Since the function is undefined for , so is the derivative. I did some Google searching to see if this was really a rule, and I found this Quora question .  One of the answers provides a seeming counterexample: So is Khan stating an incorrect rule?  Obviously it works in the case of his problem, but I'd like to understand what the accurate rule is here.  Maybe the key question is what type of discontinuity we're dealing with?","
\frac{d}{dx}\frac{x^2 + x - 2}{x-1} = \frac{d}{dx}(x+2) = 1 \text { where x \ne 1} 
 1 x x=1 
\ln(x) \text{ has a domain } (0,∞) \\
\frac{d}{dx}\ln(x)=\frac{1}{x} \\
\frac{1}{x} \text { has the domain} (-∞,0) \text{ and } (0,∞) \\
","['calculus', 'functions', 'derivatives', 'continuity']"
23,Can we compute all digits of the Euler-Mascheroni constant?,Can we compute all digits of the Euler-Mascheroni constant?,,"Let $\gamma$ be the Euler-Mascheroni constant from calculus. Is there an algorithm that computes the $n$ -th digit of the decimal expansion of $\gamma$ given $n$ as input? For all we know $\gamma$ could be a rational number, it could even be something like $k/10^n$ and have a terminating decimal expansion. Do we run into difficulties in this case when trying to compute the $n$ -th or later digits? Clarification : As another example assume we have a computable strictly increasing lower bound series $a_n$ and a computable strictly decreasing upperbound series $b_n$ both converging to an unkown real number $x$ : $a_n < x < b_n$ and $b_n-a_n\rightarrow0$ . Now assume that $x$ is in fact $0.5$ but we do not know this. All we see is that no matter how (finitely) many $a_n$ and $b_n$ we compute that always $a_n < 0.5 < b_n$ . How can we then ever know the first digit of the decimal expansion of $x$ ? Ist could be $4$ or $5$ . My questions is: Could we run into a similar problem with $\gamma$ ? Is there a known computable function $f(n)$ that provably computes the $n$ -th digit of the decimal expansion of $\gamma$ for every $n$ ? Theorertically of course there is such an algorithm in any case. But can we write it down not knowing wether $\gamma$ is irrational?","Let be the Euler-Mascheroni constant from calculus. Is there an algorithm that computes the -th digit of the decimal expansion of given as input? For all we know could be a rational number, it could even be something like and have a terminating decimal expansion. Do we run into difficulties in this case when trying to compute the -th or later digits? Clarification : As another example assume we have a computable strictly increasing lower bound series and a computable strictly decreasing upperbound series both converging to an unkown real number : and . Now assume that is in fact but we do not know this. All we see is that no matter how (finitely) many and we compute that always . How can we then ever know the first digit of the decimal expansion of ? Ist could be or . My questions is: Could we run into a similar problem with ? Is there a known computable function that provably computes the -th digit of the decimal expansion of for every ? Theorertically of course there is such an algorithm in any case. But can we write it down not knowing wether is irrational?",\gamma n \gamma n \gamma k/10^n n a_n b_n x a_n < x < b_n b_n-a_n\rightarrow0 x 0.5 a_n b_n a_n < 0.5 < b_n x 4 5 \gamma f(n) n \gamma n \gamma,"['calculus', 'computability', 'computational-mathematics']"
24,Can we define differentiation without using a norm?,Can we define differentiation without using a norm?,,"Since all norms on $\mathbb R^n$ are equivalent, the following question makes sense: Can we define the notion of ""differentiability"" of a map $\mathbb R^n \to \mathbb R$ without refering to a norm at all? Can we define the derivative itself? (That is, without mentioning any kind of norm, or a distance induced by it). In fact, I guess that one could ask that even for $n=1$ .","Since all norms on are equivalent, the following question makes sense: Can we define the notion of ""differentiability"" of a map without refering to a norm at all? Can we define the derivative itself? (That is, without mentioning any kind of norm, or a distance induced by it). In fact, I guess that one could ask that even for .",\mathbb R^n \mathbb R^n \to \mathbb R n=1,"['calculus', 'multivariable-calculus', 'derivatives', 'differential-geometry', 'differential-topology']"
25,Catenary Cable Problem: Timoshenko (2 solvers since last year only),Catenary Cable Problem: Timoshenko (2 solvers since last year only),,"I was doing this amazing problem Chapter 4, Problem 10 from book Engg Mechanics Revised 4E by Timoshenko,  and here is the link having the modified problem which resembles a lot from book. Timoshenko modified problem Let me attach the image of the question in the above link also for better visual appearance. Last year I couldn't solve this problem, but today i see this question again. so i really wanted to solve it and wanna know how do we get to the solution!  I have the final answer but dont know how to get these. I got some complex term involving $\coth$ function dont remember exactly.. as i solved this one last year. This is an example of catenary cable . So catenary equations are useful, but needs more mathematics than that. It's been more than a year, only 2 persons were able to solve this Let me attach the original problem from the book: which is almost the same as in the link i shared, solve this please. I feel this is one of the most hard problem in the book. Bounty started : 19/06/2019","I was doing this amazing problem Chapter 4, Problem 10 from book Engg Mechanics Revised 4E by Timoshenko,  and here is the link having the modified problem which resembles a lot from book. Timoshenko modified problem Let me attach the image of the question in the above link also for better visual appearance. Last year I couldn't solve this problem, but today i see this question again. so i really wanted to solve it and wanna know how do we get to the solution!  I have the final answer but dont know how to get these. I got some complex term involving function dont remember exactly.. as i solved this one last year. This is an example of catenary cable . So catenary equations are useful, but needs more mathematics than that. It's been more than a year, only 2 persons were able to solve this Let me attach the original problem from the book: which is almost the same as in the link i shared, solve this please. I feel this is one of the most hard problem in the book. Bounty started : 19/06/2019",\coth,"['calculus', 'maxima-minima', 'classical-mechanics']"
26,"How is Wolfram Alpha and the reduction formula arriving at a different result for the integral of $\int \sec^4 x\,dx$ than naive $u$-substitution?",How is Wolfram Alpha and the reduction formula arriving at a different result for the integral of  than naive -substitution?,"\int \sec^4 x\,dx u","I calculated the following on paper for the value of $\int \sec^4 x\,dx$ . $$\int \sec^4 x\,dx=\int \sec^2 x \sec^2 x\,dx=\int (\tan^2 x + 1)(\sec^2 x)\,dx.$$ Let $u = \tan x$ , $du = \sec^2 x\,dx$ so \begin{align}\int \sec^4 x\,dx&=\int u^2 + 1\,du\\&=\frac{1}{3} u^3 + u + C\\&=\frac{1}{3} \tan^3 x + \tan x + C\\&=\frac{1}{3} (\tan x)(\tan^2 x + 1) + C\\&=\frac{1}{3} \tan x \sec^2 x + C\end{align} Wolfram Alpha, however, gives $\int \sec^4(x)\,dx = \frac13(\cos(2 x) + 2) \tan(x) \sec^2(x) + C$ . This is notably not equal to my solution. According to the ""step-by-step solution"" from the Wolfram Alpha app, the reduction formula was used to produce $$\frac{1}{3}\tan x \sec^2 x + \frac{2}{3} \int \sec^2 x \,dx$$ then $$\frac{2}{3} \tan x + \frac{1}{3} \tan x \sec^2 x + C$$ Why does the reduction formula produce this added term compared to naive $u$ -substitution?","I calculated the following on paper for the value of . Let , so Wolfram Alpha, however, gives . This is notably not equal to my solution. According to the ""step-by-step solution"" from the Wolfram Alpha app, the reduction formula was used to produce then Why does the reduction formula produce this added term compared to naive -substitution?","\int \sec^4 x\,dx \int \sec^4 x\,dx=\int \sec^2 x \sec^2 x\,dx=\int (\tan^2 x + 1)(\sec^2 x)\,dx. u = \tan x du = \sec^2 x\,dx \begin{align}\int \sec^4 x\,dx&=\int u^2 + 1\,du\\&=\frac{1}{3} u^3 + u + C\\&=\frac{1}{3} \tan^3 x + \tan x + C\\&=\frac{1}{3} (\tan x)(\tan^2 x + 1) + C\\&=\frac{1}{3} \tan x \sec^2 x + C\end{align} \int \sec^4(x)\,dx = \frac13(\cos(2 x) + 2) \tan(x) \sec^2(x) + C \frac{1}{3}\tan x \sec^2 x + \frac{2}{3} \int \sec^2 x \,dx \frac{2}{3} \tan x + \frac{1}{3} \tan x \sec^2 x + C u","['calculus', 'indefinite-integrals', 'substitution']"
27,"$f \in C[-1,1]$, Prove ${\lim_{h \to 0^+}}{\int_{-1}^1 \frac{h}{h^2+x^2}f(x)\,dx} = \pi f(0)$",", Prove","f \in C[-1,1] {\lim_{h \to 0^+}}{\int_{-1}^1 \frac{h}{h^2+x^2}f(x)\,dx} = \pi f(0)","I took a look at the special situation that $f=1$ , $${\lim_{h \to 0^+}}{\int_{-1}^1 \frac{h}{h^2+x^2}f(x)\,dx} ={\lim_{h \to 0^+}}{\int_{-1}^1 \frac{h}{h^2+x^2}\,dx} = \left.{\lim_{h \to 0^+}} \arctan{\frac{x}{h}} \right|_{-1}^1 = \pi $$ but I don't know how to find the next step.","I took a look at the special situation that , but I don't know how to find the next step.","f=1 {\lim_{h \to 0^+}}{\int_{-1}^1 \frac{h}{h^2+x^2}f(x)\,dx} ={\lim_{h \to 0^+}}{\int_{-1}^1 \frac{h}{h^2+x^2}\,dx} = \left.{\lim_{h \to 0^+}} \arctan{\frac{x}{h}} \right|_{-1}^1 = \pi ","['calculus', 'definite-integrals']"
28,Is $e^{\int \frac {1}{x}dx}$ equal to $x$ or $|x|$?,Is  equal to  or ?,e^{\int \frac {1}{x}dx} x |x|,"I encountered this expression quite a lot of times as a part of the integrating factor while solving linear differential equations. $$e^{\int \frac {1}{x}dx}$$ For sometime, I wrote it as $x$ , and was satisfied as even the answer given in my textbook had $x$ instead of $|x|$ . But after realising the possibility to be $|x|$ , I am confused. What should be the answer, and why? Edit: (My reasoning) $\int \frac {1}{X} dx = log |x|$ and $e^{logt} = t$ if I'm not wrong. So in this case, $t = |x|$ so the answer should be $|x|$ . What is wrong with this reasoning? And could you please provide a mathematical proof if the answer should be $x$ ? Edit 2: Wolfram Alpha evaluates e^(∫(1/x)dx) to $x$","I encountered this expression quite a lot of times as a part of the integrating factor while solving linear differential equations. For sometime, I wrote it as , and was satisfied as even the answer given in my textbook had instead of . But after realising the possibility to be , I am confused. What should be the answer, and why? Edit: (My reasoning) and if I'm not wrong. So in this case, so the answer should be . What is wrong with this reasoning? And could you please provide a mathematical proof if the answer should be ? Edit 2: Wolfram Alpha evaluates e^(∫(1/x)dx) to",e^{\int \frac {1}{x}dx} x x |x| |x| \int \frac {1}{X} dx = log |x| e^{logt} = t t = |x| |x| x x,"['calculus', 'integration', 'exponential-function', 'indefinite-integrals']"
29,When can we use derivative test to identify injective function?,When can we use derivative test to identify injective function?,,"I have doubt regarding first derivative test for identifying whether a function is injective or not: For example: $$f(x)=\ln x$$ has domain $(0, \infty)$ . Now $$f'(x)=\frac{1}{x} \gt 0$$ hence $f(x)=\ln x$ is strictly increasing and hence injective. But if we consider: $$f(x)=\tan x$$ $$f'(x)=\sec^2 x \gt 0$$ But still $\tan x$ is not injective. So can I know the formal conditions to test whether a function is injective or not?",I have doubt regarding first derivative test for identifying whether a function is injective or not: For example: has domain . Now hence is strictly increasing and hence injective. But if we consider: But still is not injective. So can I know the formal conditions to test whether a function is injective or not?,"f(x)=\ln x (0, \infty) f'(x)=\frac{1}{x} \gt 0 f(x)=\ln x f(x)=\tan x f'(x)=\sec^2 x \gt 0 \tan x","['calculus', 'functions', 'derivatives', 'monotone-functions']"
30,Find function $f(x)$ that satisfying differential relation,Find function  that satisfying differential relation,f(x),"Suppose the functions $F(x)$ and $G(x)$ satisfying $$F(x)=f(x)-\frac{1}{f(x)}$$ $$G(x)=f(x)+\frac{1}{f(x)}$$ such that $F'(x)=(G\circ G)(x)$ , with initial condition $f(\frac{\pi}{4})=1$ is given. Find $f(x)$ . I have attempted $F(x)+G(x)=2f(x)$ , and try to relate functions $F(x)$ and $G(x)$ , but stuck in the composite function $(G\circ G)(x)$ . Taking integration for $F'(x)$ and $(G\circ G)(x)$ on both sides with respect to $x$ or $F(x)$ do not help that much. Any clue?","Suppose the functions and satisfying such that , with initial condition is given. Find . I have attempted , and try to relate functions and , but stuck in the composite function . Taking integration for and on both sides with respect to or do not help that much. Any clue?",F(x) G(x) F(x)=f(x)-\frac{1}{f(x)} G(x)=f(x)+\frac{1}{f(x)} F'(x)=(G\circ G)(x) f(\frac{\pi}{4})=1 f(x) F(x)+G(x)=2f(x) F(x) G(x) (G\circ G)(x) F'(x) (G\circ G)(x) x F(x),"['calculus', 'integration', 'ordinary-differential-equations', 'functional-equations', 'function-and-relation-composition']"
31,Computing $2^{2^1}+2^{2^2}+2^{2^3}+\cdots+2^{2^n}$,Computing,2^{2^1}+2^{2^2}+2^{2^3}+\cdots+2^{2^n},"How can I compute the following sum? $$2^{2^1}+2^{2^2}+2^{2^3}+\cdots+2^{2^n}$$ My attempt was to apply the known formula for the sum of an geometric progression, but it seems that the ratio is variable. So there is a formula for this type of sum?","How can I compute the following sum? My attempt was to apply the known formula for the sum of an geometric progression, but it seems that the ratio is variable. So there is a formula for this type of sum?",2^{2^1}+2^{2^2}+2^{2^3}+\cdots+2^{2^n},"['calculus', 'algebra-precalculus']"
32,Evaluating the limit for a point on the curve,Evaluating the limit for a point on the curve,,"For a point $P(a,b)$ is a point lying on the curve satisfying $$2xy^2dx + 2x^2 y dy -  \tan(x^2y^2) dx =0 $$ $\lim_{a\to -\infty}b = ? $ Options are: a) $ 0$ b) $-1 $ c) $1$ d) does not exist. Attempt: If we observe carefully we get: $d(x^2 y^2) = \tan(x^2 y^2) dx$ $\implies \ln(c\sin x^2y^2) = x$ $\implies c \sin (x^2 y^2) = e^x$ Now clearly as $x \to -\infty ~ , e^x \to 0$ ,  so clearly $y \to 0$ But answer given is d. Please let me know my mistake.","For a point is a point lying on the curve satisfying Options are: a) b) c) d) does not exist. Attempt: If we observe carefully we get: Now clearly as ,  so clearly But answer given is d. Please let me know my mistake.","P(a,b) 2xy^2dx + 2x^2 y dy -
 \tan(x^2y^2) dx =0  \lim_{a\to -\infty}b = ?   0 -1  1 d(x^2 y^2) = \tan(x^2 y^2) dx \implies \ln(c\sin x^2y^2) = x \implies c \sin (x^2 y^2) = e^x x \to -\infty ~ , e^x \to 0 y \to 0",['calculus']
33,$\int_{0}^{2008}x|\sin\pi x| dx$,,\int_{0}^{2008}x|\sin\pi x| dx,Evaluate: $$\int_{0}^{2008}x|\sin\pi x| dx$$ That modulus sign is causing problems. How do I handle it? I am trying integration by parts I have even evaluated: $\int_0^1 {|\sin \pi x|}= \frac 2 \pi$. Not sure how to utilise it in the problem. I just need help with the modulus part.,Evaluate: $$\int_{0}^{2008}x|\sin\pi x| dx$$ That modulus sign is causing problems. How do I handle it? I am trying integration by parts I have even evaluated: $\int_0^1 {|\sin \pi x|}= \frac 2 \pi$. Not sure how to utilise it in the problem. I just need help with the modulus part.,,['calculus']
34,Integrate using the method of undetermined coefficients,Integrate using the method of undetermined coefficients,,"I was trying to solve this integral using the method of Undetermined Coefficients . $$\int x^3\cos(3x)\,dx$$ My calculus book says: We try: $y=P(x)\cos(3x)+Q(x)\sin(3x)+C$, where $P(x)$ and $Q(x)$ are polynomials of degrees $m$ and $n$ respectively. $y'=P'(x)\cos(3x)-3P(x)\sin(3x)+Q'(x)\sin(3x)+3Q'(x)\cos(3x)=x^3\cos(3x)$ Equating coefficients of like trigonometric functions, we find: $P'(x)+3Q(x)=x^3$ and $Q'(x)-3P(x)=0$ The second of these equations requires that $m=n-1$. From the first we   conclude that $n=3$, which implies that $m=2$. With this information I can calculate the correct integral, being: $$(-\frac{2}{27}+\frac{x^2}{3})\cos(3x)+(-\frac{2x}{9}+\frac{x^3}{3})\sin(3x)+C$$ QUESTION: My question is about the last sentence. Given these conditions couldn't it also be the case that $m=4$ and $n=5$? As a matter of fact I calculated that this would indeed give the correct answer as well, but I don't fully understand why there are two options and what is going on here. Any insight would be appreciated.","I was trying to solve this integral using the method of Undetermined Coefficients . $$\int x^3\cos(3x)\,dx$$ My calculus book says: We try: $y=P(x)\cos(3x)+Q(x)\sin(3x)+C$, where $P(x)$ and $Q(x)$ are polynomials of degrees $m$ and $n$ respectively. $y'=P'(x)\cos(3x)-3P(x)\sin(3x)+Q'(x)\sin(3x)+3Q'(x)\cos(3x)=x^3\cos(3x)$ Equating coefficients of like trigonometric functions, we find: $P'(x)+3Q(x)=x^3$ and $Q'(x)-3P(x)=0$ The second of these equations requires that $m=n-1$. From the first we   conclude that $n=3$, which implies that $m=2$. With this information I can calculate the correct integral, being: $$(-\frac{2}{27}+\frac{x^2}{3})\cos(3x)+(-\frac{2x}{9}+\frac{x^3}{3})\sin(3x)+C$$ QUESTION: My question is about the last sentence. Given these conditions couldn't it also be the case that $m=4$ and $n=5$? As a matter of fact I calculated that this would indeed give the correct answer as well, but I don't fully understand why there are two options and what is going on here. Any insight would be appreciated.",,"['calculus', 'integration']"
35,Whats the result of double integral $\int_{0}^{1}\int_{0}^{1}\frac{1+x^2}{1+x^2+y^2}dxdy$,Whats the result of double integral,\int_{0}^{1}\int_{0}^{1}\frac{1+x^2}{1+x^2+y^2}dxdy,Whats the result of double integral  $$\int_{0}^{1}\int_{0}^{1}\frac{1+x^2}{1+x^2+y^2}dxdy$$ I was trying to get this $$\int_{0}^{1}\sqrt{1+x^2}\arctan {\frac{1}{\sqrt{1+x^2}}}dx=\int_{0}^{1}(1+x^2)\frac{1}{\sqrt{1+x^2}}\arctan {\frac{1}{\sqrt{1+x^2}}}dx$$ $$=\int_{0}^{1}(1+x^2)\int_{0}^{1}\frac{1}{1+x^2+y^2}dxdy=\int_{0}^{1}\int_{0}^{1}\frac{1+x^2}{1+x^2+y^2}dxdy$$ So far I don't know what to do next.any helps are to be grateful.,Whats the result of double integral  $$\int_{0}^{1}\int_{0}^{1}\frac{1+x^2}{1+x^2+y^2}dxdy$$ I was trying to get this $$\int_{0}^{1}\sqrt{1+x^2}\arctan {\frac{1}{\sqrt{1+x^2}}}dx=\int_{0}^{1}(1+x^2)\frac{1}{\sqrt{1+x^2}}\arctan {\frac{1}{\sqrt{1+x^2}}}dx$$ $$=\int_{0}^{1}(1+x^2)\int_{0}^{1}\frac{1}{1+x^2+y^2}dxdy=\int_{0}^{1}\int_{0}^{1}\frac{1+x^2}{1+x^2+y^2}dxdy$$ So far I don't know what to do next.any helps are to be grateful.,,"['calculus', 'integration', 'analysis', 'definite-integrals']"
36,Why is the intermediate value theorem so important?,Why is the intermediate value theorem so important?,,I would like to know why the intermediate value theorem is so important. So my questions are: Which important theorems do we prove using the intermediate value theorem? Are there direct applications of the intermediate value theorem outside mathematics? Does the intermediate value theorem have a historically importance?,I would like to know why the intermediate value theorem is so important. So my questions are: Which important theorems do we prove using the intermediate value theorem? Are there direct applications of the intermediate value theorem outside mathematics? Does the intermediate value theorem have a historically importance?,,['calculus']
37,How to evaluate the integral $\int_0^{\pi}\frac{a^n\sin^2x+b^n\cos^2x}{a^{2n}\sin^2x+b^{2n}\cos^2x}dx$?,How to evaluate the integral ?,\int_0^{\pi}\frac{a^n\sin^2x+b^n\cos^2x}{a^{2n}\sin^2x+b^{2n}\cos^2x}dx,"Evaluate the integral $$\int_0^{\pi}\frac{a^n\sin^2x+b^n\cos^2x}{a^{2n}\sin^2x+b^{2n}\cos^2x}dx.$$ I have no idea. $$\int_0^{\pi}\dfrac{a^n\sin^2x+b^n\cos^2x}{a^{2n}\sin^2x+b^{2n}\cos^2x}dx=\int_0^{\pi/2}\dfrac{a^n\sin^2x+b^n\cos^2x}{a^{2n}\sin^2x+b^{2n}\cos^2x}dx=\int_0^{\pi/2}\dfrac{a^n\tan^2x+b^n}{a^{2n}\tan^2x+b^{2n}}dx$$ I try the substitution $\tan{x}=t$, but it doesn’t work.","Evaluate the integral $$\int_0^{\pi}\frac{a^n\sin^2x+b^n\cos^2x}{a^{2n}\sin^2x+b^{2n}\cos^2x}dx.$$ I have no idea. $$\int_0^{\pi}\dfrac{a^n\sin^2x+b^n\cos^2x}{a^{2n}\sin^2x+b^{2n}\cos^2x}dx=\int_0^{\pi/2}\dfrac{a^n\sin^2x+b^n\cos^2x}{a^{2n}\sin^2x+b^{2n}\cos^2x}dx=\int_0^{\pi/2}\dfrac{a^n\tan^2x+b^n}{a^{2n}\tan^2x+b^{2n}}dx$$ I try the substitution $\tan{x}=t$, but it doesn’t work.",,"['calculus', 'integration']"
38,Prove: $\int_{0}^{\pi/2}\frac{dx}{\sqrt{(1-m \cos^2 x)(1+ m \sin^2x)}}=\int_{0}^{\pi/2}\frac{dx}{\sqrt{1-m^2 \sin^2 x}}$ for $0\le m<1$,Prove:  for,\int_{0}^{\pi/2}\frac{dx}{\sqrt{(1-m \cos^2 x)(1+ m \sin^2x)}}=\int_{0}^{\pi/2}\frac{dx}{\sqrt{1-m^2 \sin^2 x}} 0\le m<1,"How to prove the following identity? For $0\le m<1$, $$\int_{0}^{\pi/2}\frac{dx}{\sqrt{(1-m \cos^2 x)(1+ m \sin^2x)}}=\int_{0}^{\pi/2}\frac{dx}{\sqrt{1-m^2 \sin^2 x}}$$ Then it will be an elliptic integral. Firstly, the objects in the squre root are not equal, so it cann't be solved just by Trigonometric Identities.","How to prove the following identity? For $0\le m<1$, $$\int_{0}^{\pi/2}\frac{dx}{\sqrt{(1-m \cos^2 x)(1+ m \sin^2x)}}=\int_{0}^{\pi/2}\frac{dx}{\sqrt{1-m^2 \sin^2 x}}$$ Then it will be an elliptic integral. Firstly, the objects in the squre root are not equal, so it cann't be solved just by Trigonometric Identities.",,"['calculus', 'definite-integrals', 'elliptic-integrals']"
39,What is the difference between the Laplacian and second order derivative?,What is the difference between the Laplacian and second order derivative?,,Is the Laplacian of a function the same as second order derivative of the function in 1-D? What about in 2-D?,Is the Laplacian of a function the same as second order derivative of the function in 1-D? What about in 2-D?,,"['calculus', 'multivariable-calculus', 'laplace-transform', 'laplacian']"
40,Why define $|x|$ by a piecewise function?,Why define  by a piecewise function?,|x|,"So I was recently solving a question about the derivative of $|\sin(x)|$ and the answer above me used a piecewise function to solve the problem. I used the definition $|x|=\sqrt{x^2}$, and the problem became incredibly easy to solve. So, I guess what I am wondering, is why people define $|x|$ by a piecewise function like: $|x|= \begin{cases}     x , & \text{if } x \geq 0\\     -x, & \text{otherwise} \end{cases}$ Because it is harder to differentiate and the only possible application is integration. Plus, my definition is easier to prove: Proof: The absolute value of a real number is defined as the magnitude of the real number. The magnitude of a complex number $a+bi$ is $\sqrt{a^2+b^2}$, and if $b$ is $0$ because the number is real, then $|a|=\sqrt{a^2}$. So, in conclusion, I want to know how this piecewise definition came about, and why I never see my definition. If possible, I would like to know the flaws with my definition.","So I was recently solving a question about the derivative of $|\sin(x)|$ and the answer above me used a piecewise function to solve the problem. I used the definition $|x|=\sqrt{x^2}$, and the problem became incredibly easy to solve. So, I guess what I am wondering, is why people define $|x|$ by a piecewise function like: $|x|= \begin{cases}     x , & \text{if } x \geq 0\\     -x, & \text{otherwise} \end{cases}$ Because it is harder to differentiate and the only possible application is integration. Plus, my definition is easier to prove: Proof: The absolute value of a real number is defined as the magnitude of the real number. The magnitude of a complex number $a+bi$ is $\sqrt{a^2+b^2}$, and if $b$ is $0$ because the number is real, then $|a|=\sqrt{a^2}$. So, in conclusion, I want to know how this piecewise definition came about, and why I never see my definition. If possible, I would like to know the flaws with my definition.",,"['calculus', 'definition']"
41,Solving a second-order linear ODE: $\frac{d^2 y}{dx^2}+(x+1)\cdot \frac{dy}{dx}+5x^2\cdot y=0$,Solving a second-order linear ODE:,\frac{d^2 y}{dx^2}+(x+1)\cdot \frac{dy}{dx}+5x^2\cdot y=0,"Recently, a friend challenged me to find the general solution of the following differential equation: $$\frac{d^2 y}{dx^2}+(x+1)\cdot \frac{dy}{dx}+5x^2\cdot y=0 \tag{1}$$ This is a second-order linear ordinary differential equation. I have tried putting this ODE into the form of a Sturm-Liouville Equation by multiplying both sides by $e^{\int (x+1)~dx}$ to obtain: $$e^{\frac{x^2}{2}+x}\cdot\frac{d^2 y}{dx^2}+(x+1)\cdot e^{\frac{x^2}{2}+x}\cdot \frac{dy}{dx}+5x^2\cdot e^{\frac{x^2}{2}+x}\cdot y=0$$ By the reverse product rule: $$\frac{d}{dx}\left(e^{\frac{x^2}{2}+x}\cdot y'(x)\right)+5x^2\cdot e^{\frac{x^2}{2}+x}\cdot y=0 \tag{2}$$ Now, it is in Sturm-Liouville form, however I am unsure how to proceed from here. Therefore, I have instead tried to do some substitution on the differential equation to eliminate the first order term to obtain this form: $$\frac{d^2 y}{dx^2}+q(x)\cdot y=0$$ Therefore, I have tried using the substitution: $$y=e^{-\frac{(1+x)^2}{4}}\cdot z$$ $$\ln y=\ln{z}-\frac{(1+x)^2}{4}$$ Differentiating implicitly both sides w.r.t $x$: $$\frac{y'}{y}=\frac{z'}{z}-\frac{1}{2}(x+1) \tag{3}$$ Differentiating again: $$\frac{y\cdot y''-(y')^2}{y^2}=\frac{z\cdot z''-(z')^2}{z^2}-\frac{1}{2}$$ Thus: $$\frac{y''}{y}-\left(\frac{y'}{y}\right)^2=\frac{z''}{z}-\left(\frac{z'}{z}\right)^2-\frac{1}{2}$$ Substituting $(3)$: $$\frac{y''}{y}=\left(\frac{z'}{z}-\frac{1}{2}(x+1)\right)^2+\frac{z''}{z}-\left(\frac{z'}{z}\right)^2-\frac{1}{2}$$ Expanding gives: $$\frac{y''}{y}=\left(\frac{z'}{z}\right)^2-\left(\frac{z'}{z}\right)(x+1)+\frac{1}{4}(x+1)^2+\left(\frac{z''}{z}\right)-\left(\frac{z'}{z}\right)^2-\frac{1}{2}$$ $$\frac{y''}{y}=-\left(\frac{z'}{z}\right)(x+1)+\frac{1}{4}(x+1)^2+\left(\frac{z''}{z}\right)-\frac{1}{2} \tag{4}$$ Going back to our original ODE $(1)$: $$y''+(x+1)y'+5x^2\cdot y=0$$ $$\frac{y''}{y}+(x+1)\frac{y'}{y}+5x^2=0$$ Substituting $(3)$ and $(4)$ gives: $$-\left(\frac{z'}{z}\right)(x+1)+\frac{1}{4}(x+1)^2+\left(\frac{z''}{z}\right)-\frac{1}{2}+(x+1)\left[\frac{z'}{z}-\frac{1}{2}(x+1)\right]+5x^2=0$$ Cancelling terms gives: $$\left(\frac{z''}{z}\right)-\frac{1}{4}(x+1)^2-\frac{1}{2}+5x^2=0$$ Which gives the ODE: $$z''+\left[5x^2-\frac{1}{2}-\frac{1}{4}(x+1)^2\right]z=0$$ When the $z$ term is expanded, it gives: $$z''+\frac{1}{4}(19x^2-2x-3)z=0 \tag{5}$$ I tried identifying this ODE as a known type, however I could not. Therefore, I am stuck at this point. Note that I am trying to avoid a series solution for this differential equation. I am aware that the result will be in terms of non-elementary functions. Wolfram|Alpha suggests that the solution will be in terms of the Hermite polynomial $H_n(z)$ defined as: $$H_n(z)=\frac{n!}{2\pi i} \oint e^{-t^2+2tz}\cdot t^{-n-1}~dt$$ And the Kummer confluent hypergeometric function $_1F_1(a;b;x)$ defined as: $$_1F_1(a;b;x)=1+\frac{a}{b}x+\frac{a(a+1)}{b(b+1)}\frac{x^2}{2!}+\cdots=\sum_{k=0}^{\infty} \frac{(a)_k}{(b)_k}\frac{x^k}{k!}$$ Where $(a)_k$ and $(b)_k$ are Pochhammer Symbols . In conclusion, I would appreciate some guidance on how to continue solving this ODE analytically. I was thinking that equation $(5)$ seems simpler to solve from what we have, however if $(1)$ seems easier, please feel free to continue from the original ODE. Thanks in advance. Edit: I figured that I could simplify $(5)$ further by completing the square: $$\frac{d^2 z}{dx^2}+\left[\frac{19}{4}\left(x-\frac{1}{19}\right)^2-\frac{29}{38}\right]z=0$$ And then applying the substitution $u=x-\frac{1}{19}$ and $du=dx$. Evaluating $\frac{d^2 z}{dx^2}$: $$\frac{dz}{dx}=\frac{dz}{du}\cdot \frac{du}{dx}=\frac{dz}{du}$$ Thus, differentiating w.r.t $x$ gives: $$\frac{d^2 z}{dx^2}=\frac{d}{dx}\left(\frac{dz}{du}\right)=\frac{d}{du}\left(\frac{dz}{du}\right)\frac{du}{dx}=\frac{d^2 z}{du^2}$$ Therefore, we reduce it to the form: $$\frac{d^2 z}{du^2}+\left[\frac{19}{4}u^2-\frac{29}{38}\right]z=0 \tag{6}$$","Recently, a friend challenged me to find the general solution of the following differential equation: $$\frac{d^2 y}{dx^2}+(x+1)\cdot \frac{dy}{dx}+5x^2\cdot y=0 \tag{1}$$ This is a second-order linear ordinary differential equation. I have tried putting this ODE into the form of a Sturm-Liouville Equation by multiplying both sides by $e^{\int (x+1)~dx}$ to obtain: $$e^{\frac{x^2}{2}+x}\cdot\frac{d^2 y}{dx^2}+(x+1)\cdot e^{\frac{x^2}{2}+x}\cdot \frac{dy}{dx}+5x^2\cdot e^{\frac{x^2}{2}+x}\cdot y=0$$ By the reverse product rule: $$\frac{d}{dx}\left(e^{\frac{x^2}{2}+x}\cdot y'(x)\right)+5x^2\cdot e^{\frac{x^2}{2}+x}\cdot y=0 \tag{2}$$ Now, it is in Sturm-Liouville form, however I am unsure how to proceed from here. Therefore, I have instead tried to do some substitution on the differential equation to eliminate the first order term to obtain this form: $$\frac{d^2 y}{dx^2}+q(x)\cdot y=0$$ Therefore, I have tried using the substitution: $$y=e^{-\frac{(1+x)^2}{4}}\cdot z$$ $$\ln y=\ln{z}-\frac{(1+x)^2}{4}$$ Differentiating implicitly both sides w.r.t $x$: $$\frac{y'}{y}=\frac{z'}{z}-\frac{1}{2}(x+1) \tag{3}$$ Differentiating again: $$\frac{y\cdot y''-(y')^2}{y^2}=\frac{z\cdot z''-(z')^2}{z^2}-\frac{1}{2}$$ Thus: $$\frac{y''}{y}-\left(\frac{y'}{y}\right)^2=\frac{z''}{z}-\left(\frac{z'}{z}\right)^2-\frac{1}{2}$$ Substituting $(3)$: $$\frac{y''}{y}=\left(\frac{z'}{z}-\frac{1}{2}(x+1)\right)^2+\frac{z''}{z}-\left(\frac{z'}{z}\right)^2-\frac{1}{2}$$ Expanding gives: $$\frac{y''}{y}=\left(\frac{z'}{z}\right)^2-\left(\frac{z'}{z}\right)(x+1)+\frac{1}{4}(x+1)^2+\left(\frac{z''}{z}\right)-\left(\frac{z'}{z}\right)^2-\frac{1}{2}$$ $$\frac{y''}{y}=-\left(\frac{z'}{z}\right)(x+1)+\frac{1}{4}(x+1)^2+\left(\frac{z''}{z}\right)-\frac{1}{2} \tag{4}$$ Going back to our original ODE $(1)$: $$y''+(x+1)y'+5x^2\cdot y=0$$ $$\frac{y''}{y}+(x+1)\frac{y'}{y}+5x^2=0$$ Substituting $(3)$ and $(4)$ gives: $$-\left(\frac{z'}{z}\right)(x+1)+\frac{1}{4}(x+1)^2+\left(\frac{z''}{z}\right)-\frac{1}{2}+(x+1)\left[\frac{z'}{z}-\frac{1}{2}(x+1)\right]+5x^2=0$$ Cancelling terms gives: $$\left(\frac{z''}{z}\right)-\frac{1}{4}(x+1)^2-\frac{1}{2}+5x^2=0$$ Which gives the ODE: $$z''+\left[5x^2-\frac{1}{2}-\frac{1}{4}(x+1)^2\right]z=0$$ When the $z$ term is expanded, it gives: $$z''+\frac{1}{4}(19x^2-2x-3)z=0 \tag{5}$$ I tried identifying this ODE as a known type, however I could not. Therefore, I am stuck at this point. Note that I am trying to avoid a series solution for this differential equation. I am aware that the result will be in terms of non-elementary functions. Wolfram|Alpha suggests that the solution will be in terms of the Hermite polynomial $H_n(z)$ defined as: $$H_n(z)=\frac{n!}{2\pi i} \oint e^{-t^2+2tz}\cdot t^{-n-1}~dt$$ And the Kummer confluent hypergeometric function $_1F_1(a;b;x)$ defined as: $$_1F_1(a;b;x)=1+\frac{a}{b}x+\frac{a(a+1)}{b(b+1)}\frac{x^2}{2!}+\cdots=\sum_{k=0}^{\infty} \frac{(a)_k}{(b)_k}\frac{x^k}{k!}$$ Where $(a)_k$ and $(b)_k$ are Pochhammer Symbols . In conclusion, I would appreciate some guidance on how to continue solving this ODE analytically. I was thinking that equation $(5)$ seems simpler to solve from what we have, however if $(1)$ seems easier, please feel free to continue from the original ODE. Thanks in advance. Edit: I figured that I could simplify $(5)$ further by completing the square: $$\frac{d^2 z}{dx^2}+\left[\frac{19}{4}\left(x-\frac{1}{19}\right)^2-\frac{29}{38}\right]z=0$$ And then applying the substitution $u=x-\frac{1}{19}$ and $du=dx$. Evaluating $\frac{d^2 z}{dx^2}$: $$\frac{dz}{dx}=\frac{dz}{du}\cdot \frac{du}{dx}=\frac{dz}{du}$$ Thus, differentiating w.r.t $x$ gives: $$\frac{d^2 z}{dx^2}=\frac{d}{dx}\left(\frac{dz}{du}\right)=\frac{d}{du}\left(\frac{dz}{du}\right)\frac{du}{dx}=\frac{d^2 z}{du^2}$$ Therefore, we reduce it to the form: $$\frac{d^2 z}{du^2}+\left[\frac{19}{4}u^2-\frac{29}{38}\right]z=0 \tag{6}$$",,"['calculus', 'ordinary-differential-equations', 'sturm-liouville']"
42,Find the $\frac{c}{a-b}+\frac{a}{b-c}+\frac{b}{c-a}$,Find the,\frac{c}{a-b}+\frac{a}{b-c}+\frac{b}{c-a},"Let $a,b,c$ such $$a\sin^2{x}+b\cos^2{x}=c,~~~\dfrac{a}{\sin^2{x}}+\dfrac{b}{\cos^2{x}}=c$$ find the value $$\dfrac{c}{a-b}+\dfrac{a}{b-c}+\dfrac{b}{c-a}$$","Let $a,b,c$ such $$a\sin^2{x}+b\cos^2{x}=c,~~~\dfrac{a}{\sin^2{x}}+\dfrac{b}{\cos^2{x}}=c$$ find the value $$\dfrac{c}{a-b}+\dfrac{a}{b-c}+\dfrac{b}{c-a}$$",,"['calculus', 'trigonometry']"
43,Find the integral $\int_0^\infty \frac{\log(1+x^2)}{e^{2\pi x}+1}dx$,Find the integral,\int_0^\infty \frac{\log(1+x^2)}{e^{2\pi x}+1}dx,"With reference to my previous question : Show that $\int_0^\infty \frac{x\log(1+x^2)}{e^{2\pi x}+1}dx=\frac{19}{24} - \frac{23}{24}\log 2 - \frac12\log A$ The following integral should be manageable : $$\int_0^\infty \frac{\log(1+x^2)}{e^{2\pi x}+1}dx = ?$$ But it seams that wolframalpha struggles to find a closed form solution. Maybe the answer should be in terms Barnes G function or the derivative of the Hurwtiz zeta function. Update My attempt so far, consider $$F(a,b) = \int^\infty_0 \frac{\log(a+t^2)}{e^{2\pi (t+b)}-1}\,dt$$ We consider the derivative $$\frac{\partial }{\partial a}F(a,b) = \int^\infty_0 \frac{dt}{(t^2+a)(e^{2\pi (t+b)}-1)}\,dt$$ Use the following expansion $$\frac{2t}{e^{2\pi t}-1} =\frac{1}{\pi}-t+\frac{2t^2}{\pi}\sum_{k=1}^\infty\frac{1}{k^2+t^2} $$ Hence have $$\frac{1}{e^{2\pi (t+b)}-1} =\frac{1}{2(t+b)\pi}-\frac{1}{2}+\frac{t+b}{\pi}\sum_{k=1}^\infty\frac{1}{k^2+(t+b)^2} $$ $$\frac{\partial }{\partial a}F(a,b) = \int^\infty_0 \frac{1}{t^2+a}\left\{ \frac{1}{2(t+b)\pi}-\frac{1}{2}+\frac{t+b}{\pi}\sum_{k=1}^\infty\frac{1}{k^2+(t+b)^2}\right\}dt$$ This reduces to  $$\frac{\partial }{\partial a}F(a,b)= \frac{1}{2\pi}\int^\infty_0 \frac{1}{(t^2+a)(t+b)} dt-\frac{1}{2}\int^\infty_0 \frac{dt}{t^2+a}+\\\frac{1}{\pi}\sum_{k=1}^\infty\int^\infty_0\frac{t+b}{(t^2+a)(k^2+(t+b)^2)}dt$$ The first and second are simple it remains to evaluate $$\sum_{k=1}^\infty\int^\infty_0\frac{t+b}{(t^2+a)(k^2+(t+b)^2)}dt$$ Then integrate with respect to $a$. Or evaluate $$\sum_{k=1}^\infty\int^\infty_0\frac{\log(t^2+1)(t+b)}{k^2+(t+b)^2}dt$$ Where $b = i+1/2$.","With reference to my previous question : Show that $\int_0^\infty \frac{x\log(1+x^2)}{e^{2\pi x}+1}dx=\frac{19}{24} - \frac{23}{24}\log 2 - \frac12\log A$ The following integral should be manageable : $$\int_0^\infty \frac{\log(1+x^2)}{e^{2\pi x}+1}dx = ?$$ But it seams that wolframalpha struggles to find a closed form solution. Maybe the answer should be in terms Barnes G function or the derivative of the Hurwtiz zeta function. Update My attempt so far, consider $$F(a,b) = \int^\infty_0 \frac{\log(a+t^2)}{e^{2\pi (t+b)}-1}\,dt$$ We consider the derivative $$\frac{\partial }{\partial a}F(a,b) = \int^\infty_0 \frac{dt}{(t^2+a)(e^{2\pi (t+b)}-1)}\,dt$$ Use the following expansion $$\frac{2t}{e^{2\pi t}-1} =\frac{1}{\pi}-t+\frac{2t^2}{\pi}\sum_{k=1}^\infty\frac{1}{k^2+t^2} $$ Hence have $$\frac{1}{e^{2\pi (t+b)}-1} =\frac{1}{2(t+b)\pi}-\frac{1}{2}+\frac{t+b}{\pi}\sum_{k=1}^\infty\frac{1}{k^2+(t+b)^2} $$ $$\frac{\partial }{\partial a}F(a,b) = \int^\infty_0 \frac{1}{t^2+a}\left\{ \frac{1}{2(t+b)\pi}-\frac{1}{2}+\frac{t+b}{\pi}\sum_{k=1}^\infty\frac{1}{k^2+(t+b)^2}\right\}dt$$ This reduces to  $$\frac{\partial }{\partial a}F(a,b)= \frac{1}{2\pi}\int^\infty_0 \frac{1}{(t^2+a)(t+b)} dt-\frac{1}{2}\int^\infty_0 \frac{dt}{t^2+a}+\\\frac{1}{\pi}\sum_{k=1}^\infty\int^\infty_0\frac{t+b}{(t^2+a)(k^2+(t+b)^2)}dt$$ The first and second are simple it remains to evaluate $$\sum_{k=1}^\infty\int^\infty_0\frac{t+b}{(t^2+a)(k^2+(t+b)^2)}dt$$ Then integrate with respect to $a$. Or evaluate $$\sum_{k=1}^\infty\int^\infty_0\frac{\log(t^2+1)(t+b)}{k^2+(t+b)^2}dt$$ Where $b = i+1/2$.",,"['calculus', 'integration', 'definite-integrals']"
44,Does the order matter for chain rule?,Does the order matter for chain rule?,,"Given $$\frac{\partial J}{\partial z_2}=\delta_1$$ $$z_2 = hW_2+b_2$$ Derive gradients of $J$ with respect to $h$ and $W_2$, where $J \in \mathbb{R}$, $z_2 \in \mathbb{R}^{D_x \times D_y}$, $\delta_1 \in \mathbb{R}^{D_x \times D_y}$,$W_2 \in \mathbb{R}^{H \times D_y}$, $h \in \mathbb{R}^{D_x \times H}$. Here's the correct solution:  \begin{align*}     &\frac{\partial J}{\partial h}=\frac{\partial J}{\partial z_2} \frac{\partial z_2}{\partial h}=\delta_1 W_2^T\\     &\frac{\partial J}{\partial W_2}=\frac{\partial z_2}{\partial W_2} \frac{\partial J}{\partial z_2}=h^T \delta_1 \end{align*} The results are obtained by applying chain rule, however chaining in different orders. The change of orders reflect a compromise to meet the dimension requirements of $\frac{\partial J}{\partial W_2}$. It's very annoying that you have to examine the dimension every time. Is there any general rule that can be followed knowing which order to arrange in terms of applying chain rule without examining the dimension?","Given $$\frac{\partial J}{\partial z_2}=\delta_1$$ $$z_2 = hW_2+b_2$$ Derive gradients of $J$ with respect to $h$ and $W_2$, where $J \in \mathbb{R}$, $z_2 \in \mathbb{R}^{D_x \times D_y}$, $\delta_1 \in \mathbb{R}^{D_x \times D_y}$,$W_2 \in \mathbb{R}^{H \times D_y}$, $h \in \mathbb{R}^{D_x \times H}$. Here's the correct solution:  \begin{align*}     &\frac{\partial J}{\partial h}=\frac{\partial J}{\partial z_2} \frac{\partial z_2}{\partial h}=\delta_1 W_2^T\\     &\frac{\partial J}{\partial W_2}=\frac{\partial z_2}{\partial W_2} \frac{\partial J}{\partial z_2}=h^T \delta_1 \end{align*} The results are obtained by applying chain rule, however chaining in different orders. The change of orders reflect a compromise to meet the dimension requirements of $\frac{\partial J}{\partial W_2}$. It's very annoying that you have to examine the dimension every time. Is there any general rule that can be followed knowing which order to arrange in terms of applying chain rule without examining the dimension?",,"['calculus', 'matrices', 'derivatives']"
45,Mean value theorem for a gradient of convex function,Mean value theorem for a gradient of convex function,,"This is from an article , page 19. Let $J(u)=\sum \sqrt  {u_i^2+\epsilon}$, and $p^{k+1}=\nabla J(u^{k+1})$, $p^{k}=\nabla  J(u^{k})$. Since $J$ is convex, the mean value theorem tells us that   $$p^{k+1}-p^{k} = D^{k+\frac{1}{2}}(u^{k+1}-u^k) $$ where   $D^{k+\frac{1}{2}}$ is a diagonal matrix such that   $$D^{k+\frac{1}{2}}_{i,i} = \epsilon  ((u_i^{k+\frac{1}{2}})^2+\epsilon)^{-3/2}$$ for some   $u^{k+\frac{1}{2}}$ between $u^k$ and $u^{k+1}$. But I can't understand why there is such $D^{k+\frac{1}{2}}$. Let $f(u)=\sqrt {u^2+\epsilon}$, then $f'(u)=u/\sqrt{u^2+\epsilon}$, $f''(u)=\epsilon/\sqrt{u^2+\epsilon}^3$. I thought 2 implementation of MVT: Method1: If we use MVT for components of $p^{k+1}-p^{k}$, then we obtain separately $u_i^{k+\frac{1}{2}}$, which is on the segment $[u_i^{k},u_i^{k+1}]$, but the whole $u^{k+\frac{1}{2}}$ may not be on the segment $[u^{k},u^{k+1}]$. I'm not sure the author meant this. Method2: If we use MVT for $g(t)=\nabla J (u^{k+1}t + u^k (1-t))\cdot(u^{k+1}-u^k) = \sum \frac{u_i}{\sqrt{u_i^2+\epsilon}} (u_i^{k+1}-u_i^k)$, where $u_i=u_i^{k+1}t + u_i^k (1-t)$ then we have $g(1)-g(0)=g'(c) \Rightarrow (p^{k+1}-p^{k}) \cdot (u^{k+1}-u^k)= \sum \frac{\epsilon}{\sqrt{(u_i^{k+\frac{1}{2}})^2+\epsilon}^3}(u_i^{k+1}-u_i^k)^2$ where $u^{k+\frac 1 2}=u_i^{k+1}c + u_i^k (1-c)$ is on the segment $[u^{k},u^{k+1}]$. But how can we conclude that $p^{k+1}-p^{k} = D^{k+\frac{1}{2}}(u^{k+1}-u^k)$? Can we use convexity? Summary: I want to know where to use MVT, and convexity.","This is from an article , page 19. Let $J(u)=\sum \sqrt  {u_i^2+\epsilon}$, and $p^{k+1}=\nabla J(u^{k+1})$, $p^{k}=\nabla  J(u^{k})$. Since $J$ is convex, the mean value theorem tells us that   $$p^{k+1}-p^{k} = D^{k+\frac{1}{2}}(u^{k+1}-u^k) $$ where   $D^{k+\frac{1}{2}}$ is a diagonal matrix such that   $$D^{k+\frac{1}{2}}_{i,i} = \epsilon  ((u_i^{k+\frac{1}{2}})^2+\epsilon)^{-3/2}$$ for some   $u^{k+\frac{1}{2}}$ between $u^k$ and $u^{k+1}$. But I can't understand why there is such $D^{k+\frac{1}{2}}$. Let $f(u)=\sqrt {u^2+\epsilon}$, then $f'(u)=u/\sqrt{u^2+\epsilon}$, $f''(u)=\epsilon/\sqrt{u^2+\epsilon}^3$. I thought 2 implementation of MVT: Method1: If we use MVT for components of $p^{k+1}-p^{k}$, then we obtain separately $u_i^{k+\frac{1}{2}}$, which is on the segment $[u_i^{k},u_i^{k+1}]$, but the whole $u^{k+\frac{1}{2}}$ may not be on the segment $[u^{k},u^{k+1}]$. I'm not sure the author meant this. Method2: If we use MVT for $g(t)=\nabla J (u^{k+1}t + u^k (1-t))\cdot(u^{k+1}-u^k) = \sum \frac{u_i}{\sqrt{u_i^2+\epsilon}} (u_i^{k+1}-u_i^k)$, where $u_i=u_i^{k+1}t + u_i^k (1-t)$ then we have $g(1)-g(0)=g'(c) \Rightarrow (p^{k+1}-p^{k}) \cdot (u^{k+1}-u^k)= \sum \frac{\epsilon}{\sqrt{(u_i^{k+\frac{1}{2}})^2+\epsilon}^3}(u_i^{k+1}-u_i^k)^2$ where $u^{k+\frac 1 2}=u_i^{k+1}c + u_i^k (1-c)$ is on the segment $[u^{k},u^{k+1}]$. But how can we conclude that $p^{k+1}-p^{k} = D^{k+\frac{1}{2}}(u^{k+1}-u^k)$? Can we use convexity? Summary: I want to know where to use MVT, and convexity.",,"['calculus', 'linear-algebra', 'convex-analysis']"
46,"How did the name ""The Calculus"" come about, was there a reason or just good marketing?","How did the name ""The Calculus"" come about, was there a reason or just good marketing?",,"This is a historical and lighthearted question about etymology. The area of mathematics that deals with limiting processes over real numbers (Real Analysis) or real vector spaces, or even complex vector spaces (I think it depends on who you ask) is variably referred to as ""analysis"" or "" the Calculus"". My question is, why did this particular area of math get the somewhat grandiose name The Calculus , which, if taken literally, means something like ""The way to reason"". It seems like linear algebra, geometry, logic, and many other formal systems are just as worthy of this title in their areas of application, yet the mathematics that was developed by Newton, Leibniz, Weierstrass & Co. has taken this moniker without contest. Any anyone fill in the history of why this particular branch of mathematics got this title? I know calculus is a great achievement, has lots of applications, etc...but I don't think that is the reason. Half-jokingly, maybe it was just good marketing on the developers of Calculus...give it a great name so people pay attention to it ;-)","This is a historical and lighthearted question about etymology. The area of mathematics that deals with limiting processes over real numbers (Real Analysis) or real vector spaces, or even complex vector spaces (I think it depends on who you ask) is variably referred to as ""analysis"" or "" the Calculus"". My question is, why did this particular area of math get the somewhat grandiose name The Calculus , which, if taken literally, means something like ""The way to reason"". It seems like linear algebra, geometry, logic, and many other formal systems are just as worthy of this title in their areas of application, yet the mathematics that was developed by Newton, Leibniz, Weierstrass & Co. has taken this moniker without contest. Any anyone fill in the history of why this particular branch of mathematics got this title? I know calculus is a great achievement, has lots of applications, etc...but I don't think that is the reason. Half-jokingly, maybe it was just good marketing on the developers of Calculus...give it a great name so people pay attention to it ;-)",,['calculus']
47,Summation of a series involving powers of Fibonacci numbers.,Summation of a series involving powers of Fibonacci numbers.,,"I'm interested in this series: $$\mathcal S_p=\sum_{n=1}^\infty\frac{\left(F_n\right)^p}{2^{np}},\quad p\in\mathbb N,\tag1$$ where $F_n$ are the Fibonacci numbers , defined by the recurrence $$F_1=1,\quad F_2=1,\quad F_n=F_{n-1}+F_{n-2}\tag2$$ or by the explicit formula $$F_n=\frac{\phi^n-(1-\phi)^n}{\sqrt5},\quad\phi=\frac{1+\sqrt5}2.\tag3$$ We can find that $$\mathcal S_1=2,\quad\mathcal S_2=\frac{12}{25},\quad\mathcal S_3=\frac{376}{2201},\quad\mathcal S_4=\frac{16048}{221125},\quad\mathcal S_5=\frac{25697312}{765370111}.\tag4$$ Can we prove that $\mathcal S_p$ is always a rational? Can we find a general formula, recurrence or generating function for $\mathcal S_p$? Update: I was able to get the following formula involving finite summation: $$\mathcal S_p=\sum_{m=0}^p\binom{p}{m}\left(\vphantom{\Large|}\left(2\sqrt5\right)^p\,\phi^{p-2 m}+(-1)^{p-m+1}\right)^{-1}.\tag5$$","I'm interested in this series: $$\mathcal S_p=\sum_{n=1}^\infty\frac{\left(F_n\right)^p}{2^{np}},\quad p\in\mathbb N,\tag1$$ where $F_n$ are the Fibonacci numbers , defined by the recurrence $$F_1=1,\quad F_2=1,\quad F_n=F_{n-1}+F_{n-2}\tag2$$ or by the explicit formula $$F_n=\frac{\phi^n-(1-\phi)^n}{\sqrt5},\quad\phi=\frac{1+\sqrt5}2.\tag3$$ We can find that $$\mathcal S_1=2,\quad\mathcal S_2=\frac{12}{25},\quad\mathcal S_3=\frac{376}{2201},\quad\mathcal S_4=\frac{16048}{221125},\quad\mathcal S_5=\frac{25697312}{765370111}.\tag4$$ Can we prove that $\mathcal S_p$ is always a rational? Can we find a general formula, recurrence or generating function for $\mathcal S_p$? Update: I was able to get the following formula involving finite summation: $$\mathcal S_p=\sum_{m=0}^p\binom{p}{m}\left(\vphantom{\Large|}\left(2\sqrt5\right)^p\,\phi^{p-2 m}+(-1)^{p-m+1}\right)^{-1}.\tag5$$",,"['calculus', 'sequences-and-series', 'closed-form', 'fibonacci-numbers', 'golden-ratio']"
48,What does an apostrophe mean in a function?,What does an apostrophe mean in a function?,,"In a workbook, I saw the function $f(x)=x^2$. Then, there was the same function with an apostrophe $f'(x)$. It was stated that $f'(x)=2x$. What is the apostrophe, and why does it change the function?","In a workbook, I saw the function $f(x)=x^2$. Then, there was the same function with an apostrophe $f'(x)$. It was stated that $f'(x)=2x$. What is the apostrophe, and why does it change the function?",,"['calculus', 'derivatives', 'notation']"
49,"What is a ""natural group action""?","What is a ""natural group action""?",,"Eg. The symmetric group on S acts on S in a natural way, for all sets S. Thanks in advance!","Eg. The symmetric group on S acts on S in a natural way, for all sets S. Thanks in advance!",,"['calculus', 'group-theory', 'terminology', 'group-actions']"
50,How to prove this multivariable function is constant?,How to prove this multivariable function is constant?,,"Suppose the multivariable function $z=f(x,y)$ is defined on $\mathbb R^2$, has continuous partial derivatives and always satisfies $$x\frac{\partial f}{\partial x}(x,y)+y\frac{\partial f}{\partial y}(x,y)=0$$ Prove $z=f(x,y)$ is constant. To be honest I don't quite know where to start. I tried rewriting the equation as a matrix multiplication $$[x,y](\nabla f(x,y))=0$$ but this seems to be of little or no help. Can anyone help me on this or give some hint that may push me further? Best regards!","Suppose the multivariable function $z=f(x,y)$ is defined on $\mathbb R^2$, has continuous partial derivatives and always satisfies $$x\frac{\partial f}{\partial x}(x,y)+y\frac{\partial f}{\partial y}(x,y)=0$$ Prove $z=f(x,y)$ is constant. To be honest I don't quite know where to start. I tried rewriting the equation as a matrix multiplication $$[x,y](\nabla f(x,y))=0$$ but this seems to be of little or no help. Can anyone help me on this or give some hint that may push me further? Best regards!",,"['calculus', 'functional-analysis', 'multivariable-calculus', 'partial-differential-equations']"
51,Does $\lim_{x \to 0} \frac{\sin (\left \lfloor x \right \rfloor)}{\left \lfloor x \right \rfloor}$ exist?,Does  exist?,\lim_{x \to 0} \frac{\sin (\left \lfloor x \right \rfloor)}{\left \lfloor x \right \rfloor},"The function is defined $\mathbb{R}-[ 0,1)\to \mathbb{R}$. As $x$ approaches $0$ from the left, $\left \lfloor x \right \rfloor=-1$ hence the left hand limit is $\sin \left ( 1 \right )$. Quite clearly , the right hand limit does not exist. Now does the limit exist? On one hand , since LHL is not equal to RHL , it should not exist. On the other , the definition of limit says for all  $\varepsilon > 0$ , there exists a  $\delta > 0 $ such that for all  $x $ in  $D $ that satisfy $ 0 < | x - c | < \delta $, the inequality  $|f(x) - L| < \varepsilon$  holds. Now since we only consider all $x$ in the domain , I don't see how $x$ not being able to approach from the right creates a problem. I think the definition  is still verified if the limit is $\sin \left ( 1 \right )$. This is what we were told in class (no explanation was given , the definition thing is my idea) but I'm not very sure . Wolfram alpha says the limit does not exist.  This question - Find $\lim_{x\to 0}\frac{\lfloor \sin x\rfloor}{\lfloor x\rfloor}$ implies the same. So please help me. Thank you.","The function is defined $\mathbb{R}-[ 0,1)\to \mathbb{R}$. As $x$ approaches $0$ from the left, $\left \lfloor x \right \rfloor=-1$ hence the left hand limit is $\sin \left ( 1 \right )$. Quite clearly , the right hand limit does not exist. Now does the limit exist? On one hand , since LHL is not equal to RHL , it should not exist. On the other , the definition of limit says for all  $\varepsilon > 0$ , there exists a  $\delta > 0 $ such that for all  $x $ in  $D $ that satisfy $ 0 < | x - c | < \delta $, the inequality  $|f(x) - L| < \varepsilon$  holds. Now since we only consider all $x$ in the domain , I don't see how $x$ not being able to approach from the right creates a problem. I think the definition  is still verified if the limit is $\sin \left ( 1 \right )$. This is what we were told in class (no explanation was given , the definition thing is my idea) but I'm not very sure . Wolfram alpha says the limit does not exist.  This question - Find $\lim_{x\to 0}\frac{\lfloor \sin x\rfloor}{\lfloor x\rfloor}$ implies the same. So please help me. Thank you.",,"['calculus', 'limits', 'functions']"
52,How to determine the curve?,How to determine the curve?,,"In the figure above, segment $PQ$ is determined by two points: $P: (t,0)$ and $Q: (1,t)$, where $t\in [0,1]$ continuously increases and decreases between $0$ and $1$. Then this gives a close region swept by $PQ$, the upper edge of which is a curve. How to determine the equation of the curve (maybe implicit form)? My own method Suppose the curve is $y=y(x)$, then for any fixed $x_0\in (0,1)$, there is a vertical line $x=x_0$, which intersects a bundle of such $PQ(t)$ segments: $$y=\frac{t(x-t)}{1-t}$$ Easy to conclude that, the desired $y_0$ on the curve corresponding to $x_0\left(\in(0,1)\right)$ is the maximum of: $$y_0= \max\limits_{t\in (0,1)}\frac{t(x_0-t)}{1-t}=2-2\sqrt{1-x_0}-x_0$$ How to use the envelope concept? Is there any elementary method since the area is $\frac{1}{6}$?","In the figure above, segment $PQ$ is determined by two points: $P: (t,0)$ and $Q: (1,t)$, where $t\in [0,1]$ continuously increases and decreases between $0$ and $1$. Then this gives a close region swept by $PQ$, the upper edge of which is a curve. How to determine the equation of the curve (maybe implicit form)? My own method Suppose the curve is $y=y(x)$, then for any fixed $x_0\in (0,1)$, there is a vertical line $x=x_0$, which intersects a bundle of such $PQ(t)$ segments: $$y=\frac{t(x-t)}{1-t}$$ Easy to conclude that, the desired $y_0$ on the curve corresponding to $x_0\left(\in(0,1)\right)$ is the maximum of: $$y_0= \max\limits_{t\in (0,1)}\frac{t(x_0-t)}{1-t}=2-2\sqrt{1-x_0}-x_0$$ How to use the envelope concept? Is there any elementary method since the area is $\frac{1}{6}$?",,"['calculus', 'geometry']"
53,Find the value of : $\lim\limits_{n\to \infty} \sqrt [n]{\frac{(3n)!}{n!(2n+1)!}} $,Find the value of :,\lim\limits_{n\to \infty} \sqrt [n]{\frac{(3n)!}{n!(2n+1)!}} ,"First of all, sorry if something similar to this has been posted before (it's my first time in this web). I need to calculate the limit as $n\rightarrow \infty$ for this: $$\lim\limits_{n\to \infty} \sqrt [n]{\dfrac{(3n)!}{n!(2n+1)!}} $$ But I don't know which steps I need to follow in order to do it. Thank you everyone in advance :)","First of all, sorry if something similar to this has been posted before (it's my first time in this web). I need to calculate the limit as $n\rightarrow \infty$ for this: $$\lim\limits_{n\to \infty} \sqrt [n]{\dfrac{(3n)!}{n!(2n+1)!}} $$ But I don't know which steps I need to follow in order to do it. Thank you everyone in advance :)",,"['calculus', 'limits', 'factorial', 'radicals']"
54,Evaluating $\int_0^\infty \sqrt{\frac{x}{e^x-1}}dx$ in terms of special functions,Evaluating  in terms of special functions,\int_0^\infty \sqrt{\frac{x}{e^x-1}}dx,"Introduction: I've been studying integrals of the form $$\int_0^\infty \frac{x^a}{(e^x-1)^b}dx$$ where a and b are real parameters. I've been able to find closed forms for the integral in terms of the Riemann Zeta function, the Gamma function and the Polygamma functions provided the integral converges when at least one of the parameters is a positive integer. I then thought of generalizing this to the case where both a and b are non-integers. As a start I considered the integral $$\int_0^\infty \sqrt\frac{x}{e^x-1}dx$$ Using substitution methods and integration by parts I deduced that $$\int_0^\infty \sqrt\frac{x}{e^x-1}dx=\int_1^\infty \frac{\sqrt{\ln x}}{x\sqrt{x-1}}dx=\int_0^1 \sqrt\frac{-\ln x}{x(1-x)} dx=2\sqrt2\int_0^1 \sqrt\frac{-\ln x}{1-x^2}dx$$ $$=\sqrt2\int_0^1 \frac{\arcsin x}{x\sqrt{-\ln x}}dx=\sqrt2\int_0^\infty \frac{\arcsin(e^{-x})}{\sqrt x} dx=2\sqrt2\int_0^\infty \arcsin(e^{-x^2})dx$$ $$=2\sqrt2\int_0^\frac{\pi}{2} \sqrt{-\ln(\sin x)} dx$$ I tried different methods on each integral but pretty much nothing worked. With each method not shown here the integral got much more complicated. The last integral looks similar to the Clausen's integral but I wasn't able to establish a relation between them. It is of course possible to represent the value of the integral as an infinite series but my question is that does anybody know a method to evaluate this integral in terms of well-known special functions or mathematical constants? Thanks in advance.","Introduction: I've been studying integrals of the form $$\int_0^\infty \frac{x^a}{(e^x-1)^b}dx$$ where a and b are real parameters. I've been able to find closed forms for the integral in terms of the Riemann Zeta function, the Gamma function and the Polygamma functions provided the integral converges when at least one of the parameters is a positive integer. I then thought of generalizing this to the case where both a and b are non-integers. As a start I considered the integral $$\int_0^\infty \sqrt\frac{x}{e^x-1}dx$$ Using substitution methods and integration by parts I deduced that $$\int_0^\infty \sqrt\frac{x}{e^x-1}dx=\int_1^\infty \frac{\sqrt{\ln x}}{x\sqrt{x-1}}dx=\int_0^1 \sqrt\frac{-\ln x}{x(1-x)} dx=2\sqrt2\int_0^1 \sqrt\frac{-\ln x}{1-x^2}dx$$ $$=\sqrt2\int_0^1 \frac{\arcsin x}{x\sqrt{-\ln x}}dx=\sqrt2\int_0^\infty \frac{\arcsin(e^{-x})}{\sqrt x} dx=2\sqrt2\int_0^\infty \arcsin(e^{-x^2})dx$$ $$=2\sqrt2\int_0^\frac{\pi}{2} \sqrt{-\ln(\sin x)} dx$$ I tried different methods on each integral but pretty much nothing worked. With each method not shown here the integral got much more complicated. The last integral looks similar to the Clausen's integral but I wasn't able to establish a relation between them. It is of course possible to represent the value of the integral as an infinite series but my question is that does anybody know a method to evaluate this integral in terms of well-known special functions or mathematical constants? Thanks in advance.",,"['calculus', 'definite-integrals']"
55,How to show that $\lim_{n \to \infty} a_n^{1/n} = l$?,How to show that ?,\lim_{n \to \infty} a_n^{1/n} = l,"Suppose that $a_n > 0$, $n \in \mathbb{N}$. Suppose that  $$\lim_{n \to \infty} a_{n+1}/a_n =l$$. How to show that $$\lim_{n \to \infty} a_n^{1/n} = l \;?$$ My solution: let  $$b_n = a_{n+1}/a_n$$ Then $$b_1 b_2 \cdots b_{n-1} = a_{n}/a_1$$. Do we have $\lim_{n \to \infty} (b_1 \cdots b_{n-1})^{1/n} = l\;?$","Suppose that $a_n > 0$, $n \in \mathbb{N}$. Suppose that  $$\lim_{n \to \infty} a_{n+1}/a_n =l$$. How to show that $$\lim_{n \to \infty} a_n^{1/n} = l \;?$$ My solution: let  $$b_n = a_{n+1}/a_n$$ Then $$b_1 b_2 \cdots b_{n-1} = a_{n}/a_1$$. Do we have $\lim_{n \to \infty} (b_1 \cdots b_{n-1})^{1/n} = l\;?$",,"['calculus', 'limits', 'radicals']"
56,How to solve the differential equation $(2x^3y)\:\text{dy}+(1-y^2)(x^2y^2+y^2-1)\:\text{dx}=0$?,How to solve the differential equation ?,(2x^3y)\:\text{dy}+(1-y^2)(x^2y^2+y^2-1)\:\text{dx}=0,"Solve $$(2x^3y)\:\text{dy}+(1-y^2)(x^2y^2+y^2-1)\:\text{dx}=0$$ I tried the substitution $y^2=t$ ; $2y\:\text{dy}=\text{dt}$ to get $$(x^3)\:\text{dt}+(1-t)[(x^2+1)t-1]\:\text{dx}=0$$ However, I don't know how to proceed further.","Solve $$(2x^3y)\:\text{dy}+(1-y^2)(x^2y^2+y^2-1)\:\text{dx}=0$$ I tried the substitution $y^2=t$ ; $2y\:\text{dy}=\text{dt}$ to get $$(x^3)\:\text{dt}+(1-t)[(x^2+1)t-1]\:\text{dx}=0$$ However, I don't know how to proceed further.",,"['calculus', 'integration', 'algebra-precalculus', 'ordinary-differential-equations', 'derivatives']"
57,Reference request regarding calculus exam,Reference request regarding calculus exam,,"I'm currently a first year computer science student and I'm deeply interested in calculus . That being said, what we studied so far consists of: Cantor sets, sequences and a brief introduction to series and convergence tests. Our final exam is going to be set sometime in February and I took the liberty of looking up exams from past years. Since the structure is mainly the same, I'll post the relevant content of a final: Find the Fourier series for $f(x) = x\left|x\right|$ on $[ - \pi ,\pi ]$ and the value of the series for $x = \pi$. Compute the integral  $\int\limits_0^1 {\frac{x}{{\sqrt[3]{{1 - x^3 }}}}}\,\mathrm dx $ using Beta integrals and study its convergence Compute $\int\int_D xy\,\mathrm dx\,\mathrm dy $ where $D = \left\{ (x,y)\mid 2x^2  + y^2  \le 2,x \ge 0,x \le y \le 3x\right\}$. Find the volume bounded by the following two surfaces: $x^2  + y^2  = 1,\,\,z = \sqrt {x^2  + y^2 } ,z \ge 0$ . Approximate the value of this integral to two decimal places: $\int\limits_0^{\frac{1}{4}}\mathrm e^{-x^2}\,\mathrm dx $. Find the extrema of $f(x,y) = 4(x - y) - x^2  - y^2$. I know this might be a lot to ask for free, but I really like calculus and I want to learn on my own. I'm really eager to study but I need a bit of guidance. So, to get to the question, can anyone suggest some reading materials that could cover these areas? EDIT : I posted this question almost a week ago hoping someone would answer,after seeing that my post didn't get much attention I placed a bounty on it, but still nobody comes forward with answers/comments. Since apparently no one can shed any useful insight on this matter, maybe it is because my post is poorly structured. If so, can you at least suggest an edit ? Thanks.","I'm currently a first year computer science student and I'm deeply interested in calculus . That being said, what we studied so far consists of: Cantor sets, sequences and a brief introduction to series and convergence tests. Our final exam is going to be set sometime in February and I took the liberty of looking up exams from past years. Since the structure is mainly the same, I'll post the relevant content of a final: Find the Fourier series for $f(x) = x\left|x\right|$ on $[ - \pi ,\pi ]$ and the value of the series for $x = \pi$. Compute the integral  $\int\limits_0^1 {\frac{x}{{\sqrt[3]{{1 - x^3 }}}}}\,\mathrm dx $ using Beta integrals and study its convergence Compute $\int\int_D xy\,\mathrm dx\,\mathrm dy $ where $D = \left\{ (x,y)\mid 2x^2  + y^2  \le 2,x \ge 0,x \le y \le 3x\right\}$. Find the volume bounded by the following two surfaces: $x^2  + y^2  = 1,\,\,z = \sqrt {x^2  + y^2 } ,z \ge 0$ . Approximate the value of this integral to two decimal places: $\int\limits_0^{\frac{1}{4}}\mathrm e^{-x^2}\,\mathrm dx $. Find the extrema of $f(x,y) = 4(x - y) - x^2  - y^2$. I know this might be a lot to ask for free, but I really like calculus and I want to learn on my own. I'm really eager to study but I need a bit of guidance. So, to get to the question, can anyone suggest some reading materials that could cover these areas? EDIT : I posted this question almost a week ago hoping someone would answer,after seeing that my post didn't get much attention I placed a bounty on it, but still nobody comes forward with answers/comments. Since apparently no one can shed any useful insight on this matter, maybe it is because my post is poorly structured. If so, can you at least suggest an edit ? Thanks.",,"['calculus', 'reference-request', 'soft-question']"
58,Show there exists a unique $f$ (in $\mathbb R^+$) such that $\frac{d}{dx}f(x)=f^{-1}(x)$,Show there exists a unique  (in ) such that,f \mathbb R^+ \frac{d}{dx}f(x)=f^{-1}(x),"Question: Show there exists a unique bijection $f:\mathbb R^+\to\mathbb R^+$ such that $\frac{d}{dx}f(x)=f^{-1}(x)$, where the right-hand side is the functional inverse. I figured I would start by finding a trivial example of existence, but 1) I can't think of one and 2) I don't know how I'd prove uniqueness.","Question: Show there exists a unique bijection $f:\mathbb R^+\to\mathbb R^+$ such that $\frac{d}{dx}f(x)=f^{-1}(x)$, where the right-hand side is the functional inverse. I figured I would start by finding a trivial example of existence, but 1) I can't think of one and 2) I don't know how I'd prove uniqueness.",,"['calculus', 'analysis']"
59,Epsilon-Delta Confusion,Epsilon-Delta Confusion,,"I don't understand the epsilon delta definition of a limit ""According to the formal definition above, a limit statement is correct if and only if confining  x to d units of c will inevitably confine f(x) to epsilon units of L."" So, if we can confine x to infinitely small delta units of c, we can confine f(x) to infitely small epsilon units of L.  Like, constricting f(x) to L ? Is that a right way of explaining what the general idea of the epsilon-delta method is ?","I don't understand the epsilon delta definition of a limit ""According to the formal definition above, a limit statement is correct if and only if confining  x to d units of c will inevitably confine f(x) to epsilon units of L."" So, if we can confine x to infinitely small delta units of c, we can confine f(x) to infitely small epsilon units of L.  Like, constricting f(x) to L ? Is that a right way of explaining what the general idea of the epsilon-delta method is ?",,['calculus']
60,How to prove a set is open,How to prove a set is open,,"The problem is to prove $\{(x,y) \mid 2 \lt x^2 + y^2 \lt 4\}$ is open. So I have an arbitrary circle in this set, with a radius greater than $2$ and less than 4 (as given in the problem) and an arbitrary point $(a,b)$ in this arbitrary circle. I want to show that this arbitrary point is in the set, so I have that $2 \lt |a - x| \lt 4$ and $2 \lt |b - y| \lt 4$ since $2 \lt |a - x| \lt x^2 + y^2 \lt 4$ and $2 \lt |b - y| \lt x^2 + y^2 \lt 4$ (I think?). But after some time algebraically manipulating these inequalities, I cannot come to the conclusion that $2 \lt a \lt 4$ and $2 \lt b \lt 4$ which is what I think we want.","The problem is to prove $\{(x,y) \mid 2 \lt x^2 + y^2 \lt 4\}$ is open. So I have an arbitrary circle in this set, with a radius greater than $2$ and less than 4 (as given in the problem) and an arbitrary point $(a,b)$ in this arbitrary circle. I want to show that this arbitrary point is in the set, so I have that $2 \lt |a - x| \lt 4$ and $2 \lt |b - y| \lt 4$ since $2 \lt |a - x| \lt x^2 + y^2 \lt 4$ and $2 \lt |b - y| \lt x^2 + y^2 \lt 4$ (I think?). But after some time algebraically manipulating these inequalities, I cannot come to the conclusion that $2 \lt a \lt 4$ and $2 \lt b \lt 4$ which is what I think we want.",,"['calculus', 'general-topology']"
61,Question about indefinite integral $\int{\frac{dx}{x\sqrt{x^2+4x-4}}}$,Question about indefinite integral,\int{\frac{dx}{x\sqrt{x^2+4x-4}}},"I need to integrate the following integral: $$\int{\frac{dx}{x\sqrt{x^2+4x-4}}}.$$ To solve this I used Euler's substitution (i.e. $\sqrt{x^2+4x-4}=x+t$ ). The result I got is: $$\arctan\frac{\sqrt{x^2+4x-4}-x}{2} + C$$ This result seems to be correct because after differentiating it I got the original integral. But for some reason the answer in my book is: $$\frac{1}{2}\arcsin{\frac{x-2}{x\sqrt{2}}} + C$$ which also seems to be correct. My question is how can I go from the form I got to the other form (which seems to be a lot simpler than the form that I got)? Also, I think I need to use Euler's substition here, because it in chapter that explains it. Thank you in advance.","I need to integrate the following integral: To solve this I used Euler's substitution (i.e. ). The result I got is: This result seems to be correct because after differentiating it I got the original integral. But for some reason the answer in my book is: which also seems to be correct. My question is how can I go from the form I got to the other form (which seems to be a lot simpler than the form that I got)? Also, I think I need to use Euler's substition here, because it in chapter that explains it. Thank you in advance.",\int{\frac{dx}{x\sqrt{x^2+4x-4}}}. \sqrt{x^2+4x-4}=x+t \arctan\frac{\sqrt{x^2+4x-4}-x}{2} + C \frac{1}{2}\arcsin{\frac{x-2}{x\sqrt{2}}} + C,"['calculus', 'integration', 'indefinite-integrals']"
62,"Prove that $|f'(x)| \le \frac{A}2 \forall x \in [0,1] $ [duplicate]",Prove that  [duplicate],"|f'(x)| \le \frac{A}2 \forall x \in [0,1] ","This question already has answers here : Proving that if $|f''(x)| \le A$ then $|f'(x)| \le A/2$ (2 answers) Closed 10 years ago . Let $f$ be twice differentiable in $[0,1]$ $f(0) = f(1) = 0$ , $|f''(x)|\le A$ . Prove that $|f'(x)| \le  \frac{A}2, \forall x \in [0,1] $ . Well this is what I came up with, $f'(c_1) = \dfrac{f(x) -f(0)}{x-0} = \dfrac{f(x)}{x}$ $f'(c_2) = \dfrac{f(x) -f(1)}{x-1} = \dfrac{f(x)}{x-1}$ for $0\lt c_1,c_2 \lt 1$ from rolle's theorem we know that there is : $0\lt c_3 \lt 1$ such that $f'(c_3) =0$","This question already has answers here : Proving that if $|f''(x)| \le A$ then $|f'(x)| \le A/2$ (2 answers) Closed 10 years ago . Let be twice differentiable in , . Prove that . Well this is what I came up with, for from rolle's theorem we know that there is : such that","f [0,1] f(0) = f(1) = 0 |f''(x)|\le A |f'(x)| \le  \frac{A}2, \forall x \in [0,1]  f'(c_1) = \dfrac{f(x) -f(0)}{x-0} = \dfrac{f(x)}{x} f'(c_2) = \dfrac{f(x) -f(1)}{x-1} = \dfrac{f(x)}{x-1} 0\lt c_1,c_2 \lt 1 0\lt c_3 \lt 1 f'(c_3) =0",['calculus']
63,Applications of Calculus II to the real world,Applications of Calculus II to the real world,,"A lot of my calc II students are asking me what are the real world applications of what we are studying in Calc II (right now we are studying methods of integrations, so of course one of the applications is in finding areas and volumes, are there any other cool applications? I mean something that can be explained in a simple way to a calc II student). Later we will study series and sequences. I'm just looking for ways to pick up the interest of my students, do you have any ideas?","A lot of my calc II students are asking me what are the real world applications of what we are studying in Calc II (right now we are studying methods of integrations, so of course one of the applications is in finding areas and volumes, are there any other cool applications? I mean something that can be explained in a simple way to a calc II student). Later we will study series and sequences. I'm just looking for ways to pick up the interest of my students, do you have any ideas?",,"['calculus', 'applications']"
64,How do I integrate this natural logarithmic function?,How do I integrate this natural logarithmic function?,,"$$ \int 5 \mathrm{ln}(x^{1/3}) dx$$ Working with natural log is very unfamiliar to me (I think I was cheated in my Calc 1 course). I know that integrating $\frac{1}{x} = \ln x$, but is it the same in reverse? Also would this simply be $u$ - substitution or instead integral by parts? Help is so very appreciated!","$$ \int 5 \mathrm{ln}(x^{1/3}) dx$$ Working with natural log is very unfamiliar to me (I think I was cheated in my Calc 1 course). I know that integrating $\frac{1}{x} = \ln x$, but is it the same in reverse? Also would this simply be $u$ - substitution or instead integral by parts? Help is so very appreciated!",,"['calculus', 'integration']"
65,$S\subseteq T$ implies $\inf T\leq\inf S\leq\sup S\leq \sup T$,implies,S\subseteq T \inf T\leq\inf S\leq\sup S\leq \sup T,Let $S$ and $T$ be nonempty bounded subsets of $\mathbb{R}$ with $S\subseteq T$. How do I prove that $\inf T\leq\inf S\leq\sup S\leq \sup T$?,Let $S$ and $T$ be nonempty bounded subsets of $\mathbb{R}$ with $S\subseteq T$. How do I prove that $\inf T\leq\inf S\leq\sup S\leq \sup T$?,,"['calculus', 'inequality']"
66,Strange Method of Differentiating $x^2$ [duplicate],Strange Method of Differentiating  [duplicate],x^2,"This question already has answers here : Differentiating $y=x^{2}$ (4 answers) Closed 10 years ago . A book by the name of Calculus Super Textbook teaches the following method for differentiating $x^2$ with respect to $x$: Take the equation: $$y=x^2$$ The author reasons that increasing both $x$ and $y$ by a differential amount preserves equality, so: $$y+dy=(x+dx)^2$$ Expanding the right-hand side gives: $$y+dy=x^2+2xdx+dx^2$$ Subtract off the original $y=x^2$: $$dy=2xdx+dx^2$$ The author reasons that an infinitesimal change in $x$ multiplied by an infinitesimal change in $x$ is so small to be effectively zero, so any powers of $dx$ higher than $1$ are dropped: $$dy=2xdx$$ Then, dividing both sides by $dx$: $${dy \over dx}=2x$$ This is nothing like the methods I've been taught for finding a derivative (which I understand to be the limit of the difference quotient), but it at least appears to work for polynomial functions. Is this a valid method that I'm not aware of? If so, how would you use this to differentiate more complex functions like sine and cosine?","This question already has answers here : Differentiating $y=x^{2}$ (4 answers) Closed 10 years ago . A book by the name of Calculus Super Textbook teaches the following method for differentiating $x^2$ with respect to $x$: Take the equation: $$y=x^2$$ The author reasons that increasing both $x$ and $y$ by a differential amount preserves equality, so: $$y+dy=(x+dx)^2$$ Expanding the right-hand side gives: $$y+dy=x^2+2xdx+dx^2$$ Subtract off the original $y=x^2$: $$dy=2xdx+dx^2$$ The author reasons that an infinitesimal change in $x$ multiplied by an infinitesimal change in $x$ is so small to be effectively zero, so any powers of $dx$ higher than $1$ are dropped: $$dy=2xdx$$ Then, dividing both sides by $dx$: $${dy \over dx}=2x$$ This is nothing like the methods I've been taught for finding a derivative (which I understand to be the limit of the difference quotient), but it at least appears to work for polynomial functions. Is this a valid method that I'm not aware of? If so, how would you use this to differentiate more complex functions like sine and cosine?",,"['calculus', 'derivatives']"
67,Behavior of the Nth root of N?,Behavior of the Nth root of N?,,"Taking the Nth root of some real number $N$ (ie: $R(N) = N^{1/N}$), generally $R(X) > R(Y)$ when $X < Y$. This obviously isn't the case though when $ X< Y < 3$. Put another way, starting at $N = 0.1$ and increasing $N$ by $0.1$ if we plot each R(N) we see that the result increases and peaks just before it reaches $3$ and then steadily decreases. Can anyone shed some light on this behavior?","Taking the Nth root of some real number $N$ (ie: $R(N) = N^{1/N}$), generally $R(X) > R(Y)$ when $X < Y$. This obviously isn't the case though when $ X< Y < 3$. Put another way, starting at $N = 0.1$ and increasing $N$ by $0.1$ if we plot each R(N) we see that the result increases and peaks just before it reaches $3$ and then steadily decreases. Can anyone shed some light on this behavior?",,"['calculus', 'radicals', 'monotone-functions']"
68,How to find asymptotics?,How to find asymptotics?,,"The function $\Phi:(0,\infty) \mapsto \mathbb{R}$ is defined as follows. We put $\Phi(x):=1$ if $x \ge 1$. Let the function $\Phi$ satisfy $$\Phi(x)=\int_0^x \Phi\left(\frac t {1-t}\right) \frac {dt} t $$ if $x <1.$ What is the asymptotics of $\Phi(x)$ as $x\downarrow 0$?","The function $\Phi:(0,\infty) \mapsto \mathbb{R}$ is defined as follows. We put $\Phi(x):=1$ if $x \ge 1$. Let the function $\Phi$ satisfy $$\Phi(x)=\int_0^x \Phi\left(\frac t {1-t}\right) \frac {dt} t $$ if $x <1.$ What is the asymptotics of $\Phi(x)$ as $x\downarrow 0$?",,"['calculus', 'integration', 'asymptotics']"
69,Limit with roots,Limit with roots,,"I have to evaluate the following limit: $$ \lim_{x\to 1}\dfrac{\sqrt{x+1}+\sqrt{x^2-1}-\sqrt{x^3+1}}{\sqrt{x-1}+\sqrt{x^2+1}-\sqrt{x^4+1}} . $$ I rationalized both the numerator and the denominator two times, and still got nowhere. Also I tried change of variable and it didn't work. Any help is grateful. Thanks.","I have to evaluate the following limit: $$ \lim_{x\to 1}\dfrac{\sqrt{x+1}+\sqrt{x^2-1}-\sqrt{x^3+1}}{\sqrt{x-1}+\sqrt{x^2+1}-\sqrt{x^4+1}} . $$ I rationalized both the numerator and the denominator two times, and still got nowhere. Also I tried change of variable and it didn't work. Any help is grateful. Thanks.",,['calculus']
70,"Calculus, dx on top of fraction?","Calculus, dx on top of fraction?",,"I'm studying for a final, and I haven't seen any mention of any problem of this form in class or in my homework. I can't figure out how to go about solving this problem: $$\int^{e^6}_{1}{\frac{dx}{x(1+\ln(x))}}$$ What I was thinking is: $$\int{\frac{dx}{x(1+\ln(x))}}+C = \int{\frac{1}{x(1+\ln(x))}dx}+C =ln(ln(x)+1)+C $$ then solve for $$F(e^6) - F(1)$$ But I'm not so sure this is the correct approach. Can someone highlight why the dx is in such an unusual position?","I'm studying for a final, and I haven't seen any mention of any problem of this form in class or in my homework. I can't figure out how to go about solving this problem: $$\int^{e^6}_{1}{\frac{dx}{x(1+\ln(x))}}$$ What I was thinking is: $$\int{\frac{dx}{x(1+\ln(x))}}+C = \int{\frac{1}{x(1+\ln(x))}dx}+C =ln(ln(x)+1)+C $$ then solve for $$F(e^6) - F(1)$$ But I'm not so sure this is the correct approach. Can someone highlight why the dx is in such an unusual position?",,"['calculus', 'integration']"
71,The series $\displaystyle\sum_{p=1}^{\infty}\mathrm{tr}(A^p)z^p$,The series,\displaystyle\sum_{p=1}^{\infty}\mathrm{tr}(A^p)z^p,"Let $A\in\mathcal{M}_n(\mathbb{C})$. For $z\neq0$, how to express the sum of the series  $$f(z)=\sum_{p=1}^{\infty}\mathrm{tr}(A^p)z^p$$ with the characteristic polynomial $P$ of $A$. Thanks.","Let $A\in\mathcal{M}_n(\mathbb{C})$. For $z\neq0$, how to express the sum of the series  $$f(z)=\sum_{p=1}^{\infty}\mathrm{tr}(A^p)z^p$$ with the characteristic polynomial $P$ of $A$. Thanks.",,['calculus']
72,$\sum _{k=1}^{n-1}k^p \lt \frac {n^{p+1}} {p+1} \lt \sum _{k=-1}^{n}k^p$,,\sum _{k=1}^{n-1}k^p \lt \frac {n^{p+1}} {p+1} \lt \sum _{k=-1}^{n}k^p,"I'm going through a proof of the theorem that says $\int_0^bx^pdx = \frac {b^{p+1}}{p+1}$, and it begins with the inequality. $\sum _{k=1}^{n-1}k^p \lt \frac {n^{p+1}} {p+1} \lt \sum _{k=-1}^{n}k^p$ What I'm having trouble understanding where this middle term came from.","I'm going through a proof of the theorem that says $\int_0^bx^pdx = \frac {b^{p+1}}{p+1}$, and it begins with the inequality. $\sum _{k=1}^{n-1}k^p \lt \frac {n^{p+1}} {p+1} \lt \sum _{k=-1}^{n}k^p$ What I'm having trouble understanding where this middle term came from.",,['calculus']
73,How to solve $\int \ln(x)\cos(x)\: \mathrm{d}x$?,How to solve ?,\int \ln(x)\cos(x)\: \mathrm{d}x,"I'm a high school senior and I'm taking Calc II. Last week we went through integration by parts. I encountered this problem which is not on the textbook, and I couldn't solve it. I tried several different approaches but each of them led me to an dead end. Any help is appreciated. Here it is: $$\int \ln(x)\cos(x)\: \mathrm{d}x$$","I'm a high school senior and I'm taking Calc II. Last week we went through integration by parts. I encountered this problem which is not on the textbook, and I couldn't solve it. I tried several different approaches but each of them led me to an dead end. Any help is appreciated. Here it is: $$\int \ln(x)\cos(x)\: \mathrm{d}x$$",,['calculus']
74,Solve $xy=3$ and $4^{x^2}+2^{y^2}=72$,Solve  and,xy=3 4^{x^2}+2^{y^2}=72,"I have a system of equations $xy=3$ and $4^{x^2}+2^{y^2}=72$ whose solution I know is $x=y=\sqrt 3$, but what are the steps to solve it?","I have a system of equations $xy=3$ and $4^{x^2}+2^{y^2}=72$ whose solution I know is $x=y=\sqrt 3$, but what are the steps to solve it?",,"['calculus', 'systems-of-equations']"
75,Can a limit of a function that's defined only in one point exist?,Can a limit of a function that's defined only in one point exist?,,The question is whether by definition there can exist a limit of a function that's defined only in one point ( or in several points but there's no interval in which the function is defined). This came up when thinking about $\lim_{x \to 0}{\sqrt{-|x|}}$,The question is whether by definition there can exist a limit of a function that's defined only in one point ( or in several points but there's no interval in which the function is defined). This came up when thinking about $\lim_{x \to 0}{\sqrt{-|x|}}$,,"['calculus', 'limits']"
76,integral of exponential divided by polynomial,integral of exponential divided by polynomial,,"I would like to solve the integral $$A\int_{-\infty}^\infty\frac{e^{-ipx/h}}{x^2+a^2}dx$$ where h and a are positive constants. Mathematica gives the solution as $\frac\pi{a}e^{-|p|a/h}$, but I have been trying to reduce my reliance on mathematica. I have no idea what methods I would use to solve it. Is there a good (preferably online) resource where I could look up methods for integrals like this fairly easily?","I would like to solve the integral $$A\int_{-\infty}^\infty\frac{e^{-ipx/h}}{x^2+a^2}dx$$ where h and a are positive constants. Mathematica gives the solution as $\frac\pi{a}e^{-|p|a/h}$, but I have been trying to reduce my reliance on mathematica. I have no idea what methods I would use to solve it. Is there a good (preferably online) resource where I could look up methods for integrals like this fairly easily?",,"['calculus', 'definite-integrals', 'improper-integrals']"
77,Verification the integral identity $\int_0^\pi \ln(1+\alpha\cos(x))dx=\pi\ln\left(\frac{1+\sqrt{1-\alpha^2}}{2}\right)$.,Verification the integral identity .,\int_0^\pi \ln(1+\alpha\cos(x))dx=\pi\ln\left(\frac{1+\sqrt{1-\alpha^2}}{2}\right),I have to verify that $$\int_0^\pi \ln(1+\alpha\cos(x))dx=\pi\ln\left(\frac{1+\sqrt{1-\alpha^2}}{2}\right)$$ with $|\alpha|<1$. It is my homework and don't know where to begin.,I have to verify that $$\int_0^\pi \ln(1+\alpha\cos(x))dx=\pi\ln\left(\frac{1+\sqrt{1-\alpha^2}}{2}\right)$$ with $|\alpha|<1$. It is my homework and don't know where to begin.,,"['calculus', 'integration']"
78,why is the derivative of a number 0 while the derivative of $x$ is 1?,why is the derivative of a number 0 while the derivative of  is 1?,x,why is the derivative of a number 0 while the derivative of $x$ is 1? I can't understand why it changes for number and a variable for a number.,why is the derivative of a number 0 while the derivative of $x$ is 1? I can't understand why it changes for number and a variable for a number.,,"['calculus', 'derivatives']"
79,Remainder term of Lagrange Interpolation Polynomial,Remainder term of Lagrange Interpolation Polynomial,,"Suppose  $x_0,x_1,\ldots,x_n$ are $n+1$ distinct numbers in the interval $[a,b]$ and $f\in C^{n+1}[a,b]$. Then for each $x$ in $[a,b]$, there is a number $\xi$ in $(a,b)$ such that $$f(x) = P(x) + \frac{f^{(n+1)}(\xi)}{(n+1)!}(x-x_0)(x-x_1)\cdots(x-x_n)$$ where $P(x)$ is the Lagrange interpolating polynomial of degree at most $n$ with $f(x_k)=P(x_k)$. I am trying to understand geometrically why the remainder term $f(x)-P(x)$ should have the form given above. I am looking for a conceptual argument similar to the following for the lagrange form of the taylor remainder. Let $$R(x) = g(x) - g(0) - g^{(1)}(0)x- \frac{g^{(2)}(0)}{2} x^2 - \cdots - \frac{g^{(k)}(0)}{k!}x^k$$ be the taylor remainder of $g(x)$. For a fixed $h>0$ let $$p(x)= \frac{R(h)}{h^{k+1}}x^{k+1}$$ Then we have $$R(0)=p(0), R^{(1)}(0)=p^{(1)}(0), \ldots, R^{(k)}(0)=p^{(k)}(0)$$ and $p(h) = R(h)$. In addition $p^{(k+1)}(x)$ is constant. If $R^{(k+1)}(x)$ were always strictly greater than the constant $p^{(k+1)}$ then since $R$ and $p$ agree on initial conditions at $x=0$ we would expect geometrically that $R$ to be greater than $p$ after $x=0$ contradicting $p(h) = R(h)$. Likewise if $R^{(k+1)}(x)$ were always strictly less than the $p^{(k+1)}$ we would expect $R$ to be less than $p$ after $x=0$ contradicting $p(h) = R(h)$. Thus we expect that $R^{(k+1)}(x)$ takes on values above and below $p^{(k+1)}$ and as well as $p^{(k+1)}$ itself. And this gives the lagrange form of the taylor remainder. Of course the standard formal argument would use the generalized form of Rolle's theorem, but I didn't need Rolle's theorem to see why the lagrange form of the taylor remainder should be right. There should be a similar ""geometric"" argument to motivate the error term for the lagrange interpolation polynomial.","Suppose  $x_0,x_1,\ldots,x_n$ are $n+1$ distinct numbers in the interval $[a,b]$ and $f\in C^{n+1}[a,b]$. Then for each $x$ in $[a,b]$, there is a number $\xi$ in $(a,b)$ such that $$f(x) = P(x) + \frac{f^{(n+1)}(\xi)}{(n+1)!}(x-x_0)(x-x_1)\cdots(x-x_n)$$ where $P(x)$ is the Lagrange interpolating polynomial of degree at most $n$ with $f(x_k)=P(x_k)$. I am trying to understand geometrically why the remainder term $f(x)-P(x)$ should have the form given above. I am looking for a conceptual argument similar to the following for the lagrange form of the taylor remainder. Let $$R(x) = g(x) - g(0) - g^{(1)}(0)x- \frac{g^{(2)}(0)}{2} x^2 - \cdots - \frac{g^{(k)}(0)}{k!}x^k$$ be the taylor remainder of $g(x)$. For a fixed $h>0$ let $$p(x)= \frac{R(h)}{h^{k+1}}x^{k+1}$$ Then we have $$R(0)=p(0), R^{(1)}(0)=p^{(1)}(0), \ldots, R^{(k)}(0)=p^{(k)}(0)$$ and $p(h) = R(h)$. In addition $p^{(k+1)}(x)$ is constant. If $R^{(k+1)}(x)$ were always strictly greater than the constant $p^{(k+1)}$ then since $R$ and $p$ agree on initial conditions at $x=0$ we would expect geometrically that $R$ to be greater than $p$ after $x=0$ contradicting $p(h) = R(h)$. Likewise if $R^{(k+1)}(x)$ were always strictly less than the $p^{(k+1)}$ we would expect $R$ to be less than $p$ after $x=0$ contradicting $p(h) = R(h)$. Thus we expect that $R^{(k+1)}(x)$ takes on values above and below $p^{(k+1)}$ and as well as $p^{(k+1)}$ itself. And this gives the lagrange form of the taylor remainder. Of course the standard formal argument would use the generalized form of Rolle's theorem, but I didn't need Rolle's theorem to see why the lagrange form of the taylor remainder should be right. There should be a similar ""geometric"" argument to motivate the error term for the lagrange interpolation polynomial.",,"['calculus', 'polynomials', 'taylor-expansion', 'interpolation']"
80,Infinite series over recursive digit-counts,Infinite series over recursive digit-counts,,"Given a positive integer $n,$ let $S(n)$ be the number of digits in the decimal expansion of $n,$ and let $$f(n)=n\cdot S(n)\cdot S(S(n))\cdot\ldots$$ Note that this is well-defined, since repeatedly applying $S$ eventually yields a stable state at $1$ . For example, $$f(99)=99\cdot 2\cdot 1\cdot1\cdot\ldots=198$$ and $$f\left(10^{100}\right)=10^{100}\cdot 101\cdot 3\cdot 1\cdot1\cdot\ldots=303\cdot 10^{100}.$$ Does the sum $\displaystyle\sum_{n=1}^{\infty}\frac{1}{f(n)}$ converge or diverge? Some context: I'm trying to find the limits of what can be done with the integral test. The functions $\frac{1}{x},$ $\frac{1}{x\ln(x)},$ $\frac{1}{x\ln(x)\ln(\ln(x))},$ etc. have integrals of $\ln(x),$ $\ln(\ln(x)),$ $\ln(\ln(\ln(x)))$ etc. respectively, so infinite sums over these functions diverge ever more slowly as you add more terms; this is an attempt to find out what the limiting behavior looks like. Base 10 used for ease of reading, but the answer in any base should be the same since sums are off by at most a constant factor.","Given a positive integer let be the number of digits in the decimal expansion of and let Note that this is well-defined, since repeatedly applying eventually yields a stable state at . For example, and Does the sum converge or diverge? Some context: I'm trying to find the limits of what can be done with the integral test. The functions etc. have integrals of etc. respectively, so infinite sums over these functions diverge ever more slowly as you add more terms; this is an attempt to find out what the limiting behavior looks like. Base 10 used for ease of reading, but the answer in any base should be the same since sums are off by at most a constant factor.","n, S(n) n, f(n)=n\cdot S(n)\cdot S(S(n))\cdot\ldots S 1 f(99)=99\cdot 2\cdot 1\cdot1\cdot\ldots=198 f\left(10^{100}\right)=10^{100}\cdot 101\cdot 3\cdot 1\cdot1\cdot\ldots=303\cdot 10^{100}. \displaystyle\sum_{n=1}^{\infty}\frac{1}{f(n)} \frac{1}{x}, \frac{1}{x\ln(x)}, \frac{1}{x\ln(x)\ln(\ln(x))}, \ln(x), \ln(\ln(x)), \ln(\ln(\ln(x)))","['calculus', 'sequences-and-series', 'improper-integrals', 'recursion']"
81,Solving the differential equation $y'=\frac{3y^2+x}{4y^2+5}$,Solving the differential equation,y'=\frac{3y^2+x}{4y^2+5},Ley $y=ƒ(x)$ be the solution of the differential equation $y'=\frac{3y^2+x}{4y^2+5}$ where $y(\frac{15}{4})=0$ then which of the following are correct (A) $y'\ge\frac{3}{4} \forall x\ge \frac{15}{4}$ (B) $\int\limits_{\frac{{15}}{4}}^{\frac{{27}}{4}} {f\left( x \right)dx}  \ge \frac{{27}}{8}$ (C) $\int\limits_{\frac{{15}}{4}}^{\frac{{27}}{4}} {f\left( x \right)dx}  < \frac{{27}}{8}$ (D) $f'(x)\ge 0 \forall x\ge 0$ My approach is as follow $y' = \frac{{3{y^2} + x}}{{4{y^2} + 5}} \Rightarrow y' = \frac{3}{4}\left( {\frac{{4{y^2} + \frac{{4x}}{3}}}{{4{y^2} + 5}}} \right) \Rightarrow y' = \frac{3}{4}\left( {\frac{{4{y^2} + 5 + \frac{{4x}}{3} - 5}}{{4{y^2} + 5}}} \right)$ $ \Rightarrow y' = \frac{3}{4}\left( {\frac{{4{y^2} + 5}}{{4{y^2} + 5}}} \right) + \frac{3}{4}\left( {\frac{{\frac{{4x}}{3} - 5}}{{4{y^2} + 5}}} \right) \Rightarrow y' = \frac{3}{4} + \frac{1}{4}\left( {\frac{{4x - 15}}{{4{y^2} + 5}}} \right)$,Ley be the solution of the differential equation where then which of the following are correct (A) (B) (C) (D) My approach is as follow,y=ƒ(x) y'=\frac{3y^2+x}{4y^2+5} y(\frac{15}{4})=0 y'\ge\frac{3}{4} \forall x\ge \frac{15}{4} \int\limits_{\frac{{15}}{4}}^{\frac{{27}}{4}} {f\left( x \right)dx}  \ge \frac{{27}}{8} \int\limits_{\frac{{15}}{4}}^{\frac{{27}}{4}} {f\left( x \right)dx}  < \frac{{27}}{8} f'(x)\ge 0 \forall x\ge 0 y' = \frac{{3{y^2} + x}}{{4{y^2} + 5}} \Rightarrow y' = \frac{3}{4}\left( {\frac{{4{y^2} + \frac{{4x}}{3}}}{{4{y^2} + 5}}} \right) \Rightarrow y' = \frac{3}{4}\left( {\frac{{4{y^2} + 5 + \frac{{4x}}{3} - 5}}{{4{y^2} + 5}}} \right)  \Rightarrow y' = \frac{3}{4}\left( {\frac{{4{y^2} + 5}}{{4{y^2} + 5}}} \right) + \frac{3}{4}\left( {\frac{{\frac{{4x}}{3} - 5}}{{4{y^2} + 5}}} \right) \Rightarrow y' = \frac{3}{4} + \frac{1}{4}\left( {\frac{{4x - 15}}{{4{y^2} + 5}}} \right),"['calculus', 'ordinary-differential-equations']"
82,Bringing limits inside functions when limits go to infinity,Bringing limits inside functions when limits go to infinity,,"A standard result says that under suitable conditions to make sure the functions are defined where they need to be, we can write $$\lim_{x \to c} f(g(x)) = f \left( \lim_{x \to c} g(x) \right)$$ as long as $f$ is continuous at $\lim_{x \to c} g(x)$ . But if the inside function is going to infinity, the continuity condition makes no sense, yet it is still common to say something like $$\lim_{x \to c} f(g(x)) = \lim_{t \to \infty} f(t) $$ This is more the ""substitution"" method of evaluating the limit, which I also see used in the standard case instead of formally bringing limits inside of functions. But I've never liked it, because it never felt like substituting was using a limit theorem which I knew to be true in all situations. My question: what is the most general version of the theorem here? In the second formula, does something need to be continuous in some sense?","A standard result says that under suitable conditions to make sure the functions are defined where they need to be, we can write as long as is continuous at . But if the inside function is going to infinity, the continuity condition makes no sense, yet it is still common to say something like This is more the ""substitution"" method of evaluating the limit, which I also see used in the standard case instead of formally bringing limits inside of functions. But I've never liked it, because it never felt like substituting was using a limit theorem which I knew to be true in all situations. My question: what is the most general version of the theorem here? In the second formula, does something need to be continuous in some sense?",\lim_{x \to c} f(g(x)) = f \left( \lim_{x \to c} g(x) \right) f \lim_{x \to c} g(x) \lim_{x \to c} f(g(x)) = \lim_{t \to \infty} f(t) ,['calculus']
83,Variational Calculus - Derivation of Lagrangian Equation,Variational Calculus - Derivation of Lagrangian Equation,,"While learning about the calculus of variations to look at the principle of least action, we arrive at a point where we want to minimise the following functional (or of similar form): $$S(y(x),y'(x),x) = \int_{x_{1}}^{x_{2}} \sqrt{1+\left(\frac{dy}{dx}\right)^2} \, dx$$ We start of by creating a 'corrected' function of the following form: $\Upsilon = y(x) + \eta(x)$ such that $\eta(x_{1}) = \eta(x_{2}) = 0.$ Whilst doing so, we say that $\eta(x)$ is continuous and differentiable . I don't understand why $\eta(x)$ needs to be differentiable. Isn't it just a function that allows us to look at all the possible paths from $x_{1}$ to $x_{2}$ ? Why do these paths need to be differentiable? Why can't these paths have corners?","While learning about the calculus of variations to look at the principle of least action, we arrive at a point where we want to minimise the following functional (or of similar form): We start of by creating a 'corrected' function of the following form: such that Whilst doing so, we say that is continuous and differentiable . I don't understand why needs to be differentiable. Isn't it just a function that allows us to look at all the possible paths from to ? Why do these paths need to be differentiable? Why can't these paths have corners?","S(y(x),y'(x),x) = \int_{x_{1}}^{x_{2}} \sqrt{1+\left(\frac{dy}{dx}\right)^2} \, dx \Upsilon = y(x) + \eta(x) \eta(x_{1}) = \eta(x_{2}) = 0. \eta(x) \eta(x) x_{1} x_{2}","['calculus', 'multivariable-calculus', 'variational-analysis']"
84,"Ways to show $\int_0^{\infty}\frac{\sin^2(\pi x)}{x^2}\Big\lvert x-\Big\lfloor x +\frac12 \Big\rfloor \Big\rvert \, \mathrm{d}x = \frac{\pi^2}{8}$?",Ways to show ?,"\int_0^{\infty}\frac{\sin^2(\pi x)}{x^2}\Big\lvert x-\Big\lfloor x +\frac12 \Big\rfloor \Big\rvert \, \mathrm{d}x = \frac{\pi^2}{8}","Whilst reading about Lobachevsky's integral formula I tried constructing some interesting integrals which could be evaluated with said formula. One result I found was $$\int\limits_{0}^{\infty} \frac{\sin^2(\pi x)}{x^2}\left\lvert x - \left\lfloor x + \frac12 \right\rfloor \right\rvert \, \mathrm{d}x = \frac{\pi^2}{8} $$ which under substitution $ u = \pi x$ can  be evaluated with Lobachevsky's formula since $\int_0^\frac\pi2 \Big\lvert \frac{x}{\pi} - \Big\lfloor \frac{x}{\pi} + \frac12 \Big\rfloor \Big\rvert \, \mathrm{d}x =\int_0^\frac\pi2 \frac{x}{\pi}\, \mathrm{d}x =\frac{\pi}{8}$ . However, once I had shown this result I remembered that $\frac{\pi^2}{8}$ has the very nice series representation $$ \sum_{n\ge0} \frac{1}{(2n+1)^2} =  \frac{\pi^2}{8} \tag{1} $$ So my question is Is there a way to evaluate this integral using $(1)$ ? Or is it just a coincidence that the values match?","Whilst reading about Lobachevsky's integral formula I tried constructing some interesting integrals which could be evaluated with said formula. One result I found was which under substitution can  be evaluated with Lobachevsky's formula since . However, once I had shown this result I remembered that has the very nice series representation So my question is Is there a way to evaluate this integral using ? Or is it just a coincidence that the values match?","\int\limits_{0}^{\infty} \frac{\sin^2(\pi x)}{x^2}\left\lvert x - \left\lfloor x + \frac12 \right\rfloor \right\rvert \, \mathrm{d}x = \frac{\pi^2}{8}   u = \pi x \int_0^\frac\pi2 \Big\lvert \frac{x}{\pi} - \Big\lfloor \frac{x}{\pi} + \frac12 \Big\rfloor \Big\rvert \, \mathrm{d}x =\int_0^\frac\pi2 \frac{x}{\pi}\, \mathrm{d}x =\frac{\pi}{8} \frac{\pi^2}{8} 
\sum_{n\ge0} \frac{1}{(2n+1)^2} =  \frac{\pi^2}{8} \tag{1}
 (1)","['calculus', 'integration', 'sequences-and-series', 'definite-integrals', 'alternative-proof']"
85,A high school problem on derivatives.,A high school problem on derivatives.,,"I came across this problem in an old high school textbook of 1978: Suppose that $f, g$ are polynomials with domain and image all of $\mathbb{R}$ . Prove that if \begin{equation}     \begin{cases}                    f(x)\neq g(x) \\                   f''(x)\neq g''(x)      \end{cases} \forall \hspace{.1cm} x\in \mathbb{R} \end{equation} there is exactly one solution of $f'(x)=g'(x)$ Clarification: By $f'$ and $f''$ , the first and second derivatives are inferred. I'm not sure what I must do, but I have come up with  a few vague ideas: The polynomials can only be of odd order $\geq 3$ and they must have the same coefficient in their largest power. (Otherwise, a new polynomial of odd order would arise, which would oppose the assumption that $f(x)\neq g(x)$ ) I have started by assuming that there is no $x_0$ such that $f'(x_0)=g'(x_0),$ which in turn implies that the function $h(x)=f(x)-g(x)$ is strictly monotone. Can I consider cases? Would that be helpful? Also because I need $f''(x)\neq g''(x)$ , the function $q(x)=f'(x)-g'(x)$ is also strictly monotone. There are also other ideas that are just floating around, but I just can't poke the problem well enough. Do you have any ideas?","I came across this problem in an old high school textbook of 1978: Suppose that are polynomials with domain and image all of . Prove that if there is exactly one solution of Clarification: By and , the first and second derivatives are inferred. I'm not sure what I must do, but I have come up with  a few vague ideas: The polynomials can only be of odd order and they must have the same coefficient in their largest power. (Otherwise, a new polynomial of odd order would arise, which would oppose the assumption that ) I have started by assuming that there is no such that which in turn implies that the function is strictly monotone. Can I consider cases? Would that be helpful? Also because I need , the function is also strictly monotone. There are also other ideas that are just floating around, but I just can't poke the problem well enough. Do you have any ideas?","f, g \mathbb{R} \begin{equation}
    \begin{cases} 
                  f(x)\neq g(x) \\
                  f''(x)\neq g''(x) 
    \end{cases} \forall \hspace{.1cm} x\in \mathbb{R}
\end{equation} f'(x)=g'(x) f' f'' \geq 3 f(x)\neq g(x) x_0 f'(x_0)=g'(x_0), h(x)=f(x)-g(x) f''(x)\neq g''(x) q(x)=f'(x)-g'(x)","['calculus', 'derivatives']"
86,"Computing zillions of digits of the ""derangement constant""","Computing zillions of digits of the ""derangement constant""",,"This is a sort of inspired sequel to the following question: Evaluating $\sum\limits_{x=2}^\infty \frac{1}{!x}$ in exact form. where the question is the discussion of the "" $e$ -like constant"" $$\sum_{n=2}^{\infty} \frac{1}{!n}$$ which is at least visually similar to the series for $e$ , i.e. $$\sum_{n=0}^{\infty} \frac{1}{n!}$$ except there's also an indexing difference. I call the former the ""derangement constant"" ( $ɘ$ ?), for lack of a better name. This number is about 1.638. I want more digits. Millions; billions maybe. Kind of like $\pi$ , but $\pi$ is old stuff; need a new kid on the block. :) Actually the reason is because that comments on that question were talking about how that the series for the derangement constant converges fast, superlinear actually in a digit-for-digit sense, which at first seems like we should be able to use it to compute millions of digits. But in reality, things aren't so simple - ""fast convergence"" alone by no means does a fast algorithm make for a computer. On a computer, it's not just the amount of terms that matters (i.e. the convergence rate), but also how many digits you need to keep for each term, because the computer has to go through the digits and add and multiply them. Addition, of course, is cheap; multiplication is not - in this domain, it's typically done using tricked out and once written, often kept secret, code based on Fast Fourier Transform (FFT) methods, which for numbers of $n$ digits have complexities $$O(n \log n\ a(n))$$ for some slow-grow function $a(n)$ . The ideal FFT has $a(n) = 1$ , but this is unachievable in practice because of precision limits of the computer processor. We can call the complexity above $M(n)$ , or complexity of multiplication of $n$ -digit numbers. Fortunately though, for a naive implementation of $e$ , we don't need this FFT multiplication. Naively, to sum a series of the type above, if we want millions of digits, we better evaluate each term to millions of digits precision. Given the factorial has a recurrence relation $$(n+1)! = (n+1) \cdot n!$$ which gives $$\frac{1}{n!} = \frac{1}{n} \frac{1}{(n-1)!}$$ we can get $n$ digits in time $$O\left(n\ e^{W(\ln n)}\right)$$ which isn't too bad, but is still bad enough for large $n$ (millions or more) that we want something better. So we need to do something more clever. The usual way to overcome this is to use methods based on binary splitting - basically, consider the following. Note that we can write a factorial as $$n! = \left(1 \cdot 2 \cdot 3 \cdot \cdots \cdot m\right) \cdot \left((m+1) \cdot (m+2) \cdot \cdots \cdot (n-1) \cdot n\right)$$ for some $m$ with $1 < m < n$ . Defining $$P(a, b) := (a+1)(a+2)\cdots(b-1)b$$ we have the identity $$P(a, b) = P(a, m) P(m, b)$$ which allows us to recursively compute $$P(0, n) = n!$$ using much smaller multiplications. Something similar can be done by splitting the sum for $e$ . Take $$e = \sum_{n=0}^{\infty} \frac{1}{n!}$$ and now write the equation $$\frac{P(a, b)}{Q(a, b)} = \sum_{n=a+1}^{b} \frac{1}{(a+1)(a+2)\cdots (n-1)n}$$ Then note that $$\frac{P(a, b)}{Q(a, b)} = \frac{P(a, m)}{Q(a, m)} + \frac{1}{Q(a, m)} \frac{P(m, b)}{Q(m, b)}$$ once we take $$Q(a, b) := (a+1)(a+2) \cdots (b-1)b$$ like before. In particular, we have the recurrence equations $$Q(a, b) = Q(a, m) Q(m, b)$$ and $$P(a, b) = P(a, m) + Q(m, b) P(m, b)$$ of which the second can be found by multiplying out. Then, $$e \approx 1 + \frac{P(0, N)}{Q(0, N)}$$ gives $N+1$ terms of the $e$ -series, with, again, smaller multiplies. This saves a lot; and permits efficient computation of $e$ by leveraging fast multiplication - in particular, the complexity to generate $n$ digits is now on the order instead of $O(\log(n)\ M(n \log n))$ ; obviously much better at least if $M(n)$ is suitably good, which it is for FFT-based multiplication methods (giving $O(n \log^2(n)\ a(n))$ ). However, this trick does not appear to generalize so easily to the derangements. In particular, while factorials expand nicely as a product, derangements $!n$ have a ""branching"" recurrence identity $$!(n+1) = n(!n\ +\ !(n-1))$$ which means that, in particular, we cannot simply parcel out the computation of $!n$ into a ""1 to $m$ "" and "" $m+1$ to $n$ "" part as we did above. Worse, because the derangement is in the denominator, we cannot do something analogous to $$\frac{1}{n!} = \frac{1}{n} \frac{1}{(n-1)!}$$ which was key to the naive method for $e$ ! Instead, we must do the full reciprocal, and now have horrible costs on the order of $O(e^{W(\ln n)} \lg(n)\ M(n))$ ! So is there some trick that we can use to compute $ɘ$ efficiently like how we can use binary splitting + FFT multiplication to compute $e$ efficiently? ADD: Not sure if this is useful. But I found on Wikipedia that $$!n = n! \sum_{i=0}^{n} \frac{(-1)^i}{i!}$$ Thus $$\frac{1}{!n} = \frac{1}{n!} \frac{1}{\sum_{i=0}^{n} \frac{(-1)^i}{i!}}$$ and then we can use the series reciprocal formula to get $$\frac{1}{!n} = \frac{1}{n!} \sum_{k=0}^{\infty} (-1)^k d_{k,n}$$ where $d_{0,n} = 1$ and $d_{k,n} = -\sum_{l=\max(0, k-n)}^{k-1} \frac{d_{l,n}}{(k-l)!}$ . See: https://mathoverflow.net/questions/53384/power-series-of-the-reciprocal-does-a-recursive-formula-exist-for-the-coeffic Thus $$ɘ = \sum_{n=2}^{\infty} \sum_{k=0}^{\infty} \frac{(-1)^k d_{k,n}}{n!}$$ however; I'm not sure if this recursion for this $d_{k,n}$ is any better. At least though we're no longer recursing in a denominator! ADD 2: The next thing I wonder about is the possibility that, because this is a nested sum, we should not consider a simple binary splitting, but a splitting into four ""squares"" or ""rectangles"", i.e. a quad splitting. In particular, if we want to sum the double sum $$\sum_{n=0}^{\infty} \sum_{m=0}^{\infty} a_{n,m}$$ as an approximant $$\sum_{n=0}^{N} \sum_{m=0}^{M} a_{n,m}$$ we should break into fours $$a_{0,0} + a_{0,1} + a_{1,0} + \sum_{n=1}^{N} \sum_{m=1}^{M} a_{n,m} = \left(\sum_{n=1}^{n_m} \sum_{m=1}^{m_m} a_{n, m}\right) + \left(\sum_{n=n_m+1}^{N} \sum_{m=1}^{m_m} a_{n, m}\right) + \left(\sum_{n=1}^{n_m} \sum_{m=m_m+1}^{M} a_{n, m}\right) + \left(\sum_{n=n_m+1}^{N} \sum_{m=m_m+1}^{M} a_{n, m}\right)$$ viz. $$a_{0,0} + a_{0,1} + a_{1,0} + \frac{P(0, 0, N, M)}{Q(0, 0, N, M)} = \frac{P(0, 0, n_m, m_m)}{Q(0, 0, n_m, m_m)} + \frac{P(n_m, 0, N, m_m)}{Q(n_m, 0, N, m_m)} + \frac{P(0, m_m, n_m, M)}{Q(0, m_m, n_m, M)} + \frac{P(n_m, m_m, N, M)}{Q(n_m, m_m, N, M)}$$","This is a sort of inspired sequel to the following question: Evaluating $\sum\limits_{x=2}^\infty \frac{1}{!x}$ in exact form. where the question is the discussion of the "" -like constant"" which is at least visually similar to the series for , i.e. except there's also an indexing difference. I call the former the ""derangement constant"" ( ?), for lack of a better name. This number is about 1.638. I want more digits. Millions; billions maybe. Kind of like , but is old stuff; need a new kid on the block. :) Actually the reason is because that comments on that question were talking about how that the series for the derangement constant converges fast, superlinear actually in a digit-for-digit sense, which at first seems like we should be able to use it to compute millions of digits. But in reality, things aren't so simple - ""fast convergence"" alone by no means does a fast algorithm make for a computer. On a computer, it's not just the amount of terms that matters (i.e. the convergence rate), but also how many digits you need to keep for each term, because the computer has to go through the digits and add and multiply them. Addition, of course, is cheap; multiplication is not - in this domain, it's typically done using tricked out and once written, often kept secret, code based on Fast Fourier Transform (FFT) methods, which for numbers of digits have complexities for some slow-grow function . The ideal FFT has , but this is unachievable in practice because of precision limits of the computer processor. We can call the complexity above , or complexity of multiplication of -digit numbers. Fortunately though, for a naive implementation of , we don't need this FFT multiplication. Naively, to sum a series of the type above, if we want millions of digits, we better evaluate each term to millions of digits precision. Given the factorial has a recurrence relation which gives we can get digits in time which isn't too bad, but is still bad enough for large (millions or more) that we want something better. So we need to do something more clever. The usual way to overcome this is to use methods based on binary splitting - basically, consider the following. Note that we can write a factorial as for some with . Defining we have the identity which allows us to recursively compute using much smaller multiplications. Something similar can be done by splitting the sum for . Take and now write the equation Then note that once we take like before. In particular, we have the recurrence equations and of which the second can be found by multiplying out. Then, gives terms of the -series, with, again, smaller multiplies. This saves a lot; and permits efficient computation of by leveraging fast multiplication - in particular, the complexity to generate digits is now on the order instead of ; obviously much better at least if is suitably good, which it is for FFT-based multiplication methods (giving ). However, this trick does not appear to generalize so easily to the derangements. In particular, while factorials expand nicely as a product, derangements have a ""branching"" recurrence identity which means that, in particular, we cannot simply parcel out the computation of into a ""1 to "" and "" to "" part as we did above. Worse, because the derangement is in the denominator, we cannot do something analogous to which was key to the naive method for ! Instead, we must do the full reciprocal, and now have horrible costs on the order of ! So is there some trick that we can use to compute efficiently like how we can use binary splitting + FFT multiplication to compute efficiently? ADD: Not sure if this is useful. But I found on Wikipedia that Thus and then we can use the series reciprocal formula to get where and . See: https://mathoverflow.net/questions/53384/power-series-of-the-reciprocal-does-a-recursive-formula-exist-for-the-coeffic Thus however; I'm not sure if this recursion for this is any better. At least though we're no longer recursing in a denominator! ADD 2: The next thing I wonder about is the possibility that, because this is a nested sum, we should not consider a simple binary splitting, but a splitting into four ""squares"" or ""rectangles"", i.e. a quad splitting. In particular, if we want to sum the double sum as an approximant we should break into fours viz.","e \sum_{n=2}^{\infty} \frac{1}{!n} e \sum_{n=0}^{\infty} \frac{1}{n!} ɘ \pi \pi n O(n \log n\ a(n)) a(n) a(n) = 1 M(n) n e (n+1)! = (n+1) \cdot n! \frac{1}{n!} = \frac{1}{n} \frac{1}{(n-1)!} n O\left(n\ e^{W(\ln n)}\right) n n! = \left(1 \cdot 2 \cdot 3 \cdot \cdots \cdot m\right) \cdot \left((m+1) \cdot (m+2) \cdot \cdots \cdot (n-1) \cdot n\right) m 1 < m < n P(a, b) := (a+1)(a+2)\cdots(b-1)b P(a, b) = P(a, m) P(m, b) P(0, n) = n! e e = \sum_{n=0}^{\infty} \frac{1}{n!} \frac{P(a, b)}{Q(a, b)} = \sum_{n=a+1}^{b} \frac{1}{(a+1)(a+2)\cdots (n-1)n} \frac{P(a, b)}{Q(a, b)} = \frac{P(a, m)}{Q(a, m)} + \frac{1}{Q(a, m)} \frac{P(m, b)}{Q(m, b)} Q(a, b) := (a+1)(a+2) \cdots (b-1)b Q(a, b) = Q(a, m) Q(m, b) P(a, b) = P(a, m) + Q(m, b) P(m, b) e \approx 1 + \frac{P(0, N)}{Q(0, N)} N+1 e e n O(\log(n)\ M(n \log n)) M(n) O(n \log^2(n)\ a(n)) !n !(n+1) = n(!n\ +\ !(n-1)) !n m m+1 n \frac{1}{n!} = \frac{1}{n} \frac{1}{(n-1)!} e O(e^{W(\ln n)} \lg(n)\ M(n)) ɘ e !n = n! \sum_{i=0}^{n} \frac{(-1)^i}{i!} \frac{1}{!n} = \frac{1}{n!} \frac{1}{\sum_{i=0}^{n} \frac{(-1)^i}{i!}} \frac{1}{!n} = \frac{1}{n!} \sum_{k=0}^{\infty} (-1)^k d_{k,n} d_{0,n} = 1 d_{k,n} = -\sum_{l=\max(0, k-n)}^{k-1} \frac{d_{l,n}}{(k-l)!} ɘ = \sum_{n=2}^{\infty} \sum_{k=0}^{\infty} \frac{(-1)^k d_{k,n}}{n!} d_{k,n} \sum_{n=0}^{\infty} \sum_{m=0}^{\infty} a_{n,m} \sum_{n=0}^{N} \sum_{m=0}^{M} a_{n,m} a_{0,0} + a_{0,1} + a_{1,0} + \sum_{n=1}^{N} \sum_{m=1}^{M} a_{n,m} = \left(\sum_{n=1}^{n_m} \sum_{m=1}^{m_m} a_{n, m}\right) + \left(\sum_{n=n_m+1}^{N} \sum_{m=1}^{m_m} a_{n, m}\right) + \left(\sum_{n=1}^{n_m} \sum_{m=m_m+1}^{M} a_{n, m}\right) + \left(\sum_{n=n_m+1}^{N} \sum_{m=m_m+1}^{M} a_{n, m}\right) a_{0,0} + a_{0,1} + a_{1,0} + \frac{P(0, 0, N, M)}{Q(0, 0, N, M)} = \frac{P(0, 0, n_m, m_m)}{Q(0, 0, n_m, m_m)} + \frac{P(n_m, 0, N, m_m)}{Q(n_m, 0, N, m_m)} + \frac{P(0, m_m, n_m, M)}{Q(0, m_m, n_m, M)} + \frac{P(n_m, m_m, N, M)}{Q(n_m, m_m, N, M)}","['calculus', 'algorithms', 'factorial', 'constants']"
87,"Prove that $a_{n+2}=0.5(a_n+a_{n+1})$, $a_1=2, a_2=5$ has a limit, and find it","Prove that ,  has a limit, and find it","a_{n+2}=0.5(a_n+a_{n+1}) a_1=2, a_2=5","I have the following sequence: $a_{n+2}=0.5(a_n+a_{n+1})$ , $a_1=2, a_2=5$ , and I need to prove that $\lim\limits_{n\to \infty}a_n$ exists, and find it. I don't know ho to prove that the limit exists, since the sequence neither decreasing nor increasing. In addition, I don't know how to find it, since the equation I am getting is $L=0.5(L+L)$ , whitch is true for all $L$ .","I have the following sequence: , , and I need to prove that exists, and find it. I don't know ho to prove that the limit exists, since the sequence neither decreasing nor increasing. In addition, I don't know how to find it, since the equation I am getting is , whitch is true for all .","a_{n+2}=0.5(a_n+a_{n+1}) a_1=2, a_2=5 \lim\limits_{n\to \infty}a_n L=0.5(L+L) L","['calculus', 'limits']"
88,Learn how to sketch functions intuitively,Learn how to sketch functions intuitively,,"A professor told us that it is better to have an idea of the graph of a function before starting to apply the techniques of differential calculus in order to sketch it rigorously. He was able to sketch an approximate graph of functions like: $$e^{|x^2-1|+x}$$ $$\sqrt[3]{x^2 (x-1)}$$ $$e^{-x} \sqrt[3]{ (x^2-4)}$$ It is easy to understand the process when guided, however I can't seem to be able to build the same kind of intuition alone. Are there methods/books that help you to have a general idea on the behavior of a function on its domain before using differential calculus? I believe it should be a set of techniques more advanced than the horizontal/vertical shifting/flipping/scaling that is learned in precalculus but less advanced than differential calculus.","A professor told us that it is better to have an idea of the graph of a function before starting to apply the techniques of differential calculus in order to sketch it rigorously. He was able to sketch an approximate graph of functions like: It is easy to understand the process when guided, however I can't seem to be able to build the same kind of intuition alone. Are there methods/books that help you to have a general idea on the behavior of a function on its domain before using differential calculus? I believe it should be a set of techniques more advanced than the horizontal/vertical shifting/flipping/scaling that is learned in precalculus but less advanced than differential calculus.",e^{|x^2-1|+x} \sqrt[3]{x^2 (x-1)} e^{-x} \sqrt[3]{ (x^2-4)},"['calculus', 'graphing-functions']"
89,When can you divide out $dx$ in an integral as if it is a fraction?,When can you divide out  in an integral as if it is a fraction?,dx,"A recent question on Math SE included finding the antiderivative of $$\int y'y''\,dx,$$ where $y'=\frac{dy(x)}{dx},y''=\frac{d^2y(x)}{dx^2}$ as $y=y(x)$ . One approach to solve this problem is by direct substitution. Let $$u=\frac{dy}{dx}=y' \implies du=y''\,dx,$$ then the left hand side becomes $$\int  y'y''\,dx=\int u\,du=\frac{1}{2}u^2+C=\frac{1}{2}\left(y'\right)^2+C.$$ A second approach involves writing out the integral completely and then canceling the $dx$ $$\int  \frac{dy}{dx} \frac{d^2y}{dx^2}\,dx=\int \frac{d^2y}{dx^2}\,dy=\int y''\,dy=\int\frac{dy'}{dx}\,dy=\int\frac{dy'}{dx}y'\,dx=\int y'\,dy'=\frac{1}{2}\left(y'\right)^2+C.$$ In this approach, it is crucial to make the following observation: $$dy=\frac{dy}{\require{cancel} \cancel{dx}}\,\require{cancel} \cancel{dx}=y'\,dx \implies \frac{dy'}{dx}\,dy=\frac{dy'}{\require{cancel} \cancel{dx}}y'\,\require{cancel} \cancel{dx}=y'\,dy'.$$ Through reviewing other questions on Math SE and Math Overflow, it appears that you are always able to ""divide out"" the $dx/dx$ . This is because $dx$ is an infinitesimally small positive change in $x$ . Therefore, as $dx\neq 0$ you can divide out $dx$ with itself to conclude $$\frac{dx}{dx}=1$$ at any point in integration (assuming that what you are integrating is well-defined). This is further represented by the fact that the Riemann integral can be expressed as the limit of Riemann sums $$\int_a^b f(x)\,dx=\lim_{n\to\infty}\sum_{i=1}^nf(x_i)\Delta x,$$ where $\Delta x$ means an infinitesimally small step on the x-axis to correspond with the infinitesimally small change in $x$ associated with the Riemann integral. One can justify canceling the $dx$ terms by the answer shown inside this Math Overflow question . However, they would need to know differential forms which is a topic that I am unfamiliar with. A different answer on Math SE provides a more familiar explanation in which one can write the first fundamental theorem of calculus in Leibnitz notation as: $$\int _a^b \frac{df}{dx}\,dx = f(b) - f(a).$$ Inside this answer, it is shown that you can ""cancel"" the two $dx$ terms even though you are not literally cancelling $dx/dx$ . The fact that these two terms cancel is directly due to notational convenience. I'm curious if this sort of notational convenience will fail. I think that one could write $$\int \frac{dy}{dx}\,dx=\int \frac{dy}{\require{cancel} \cancel{dx}}\,\require{cancel} \cancel{dx}=\int dy,$$ $$\int \frac{dy}{dx}\frac{dy}{dx}\,dx=\int \frac{dy}{\require{cancel} \cancel{dx}}\frac{dy}{dx}\,\require{cancel} \cancel{dx}=\int \frac{dy}{dx}\,dy,$$ $$\int \frac{dy^n}{dx^n}\frac{dy}{dx}\,dx=\int \frac{dy^n}{dx^n}\frac{dy}{\require{cancel} \cancel{dx}}\,\require{cancel} \cancel{dx}=\int \frac{dy^n}{dx^n} \,dy,$$ $$\int \frac{dy}{dx}dy\,dx=\int \frac{dy}{\require{cancel} \cancel{dx}}dy\,\require{cancel} \cancel{dx}=\int (dy)^2.$$ Is there a scenario in which you cannot cancel $\frac{dx}{dx}$ inside an integral as if it were a fraction equal to $1$ ? Can you also cancel these two terms through a substitution made inside the integral?","A recent question on Math SE included finding the antiderivative of where as . One approach to solve this problem is by direct substitution. Let then the left hand side becomes A second approach involves writing out the integral completely and then canceling the In this approach, it is crucial to make the following observation: Through reviewing other questions on Math SE and Math Overflow, it appears that you are always able to ""divide out"" the . This is because is an infinitesimally small positive change in . Therefore, as you can divide out with itself to conclude at any point in integration (assuming that what you are integrating is well-defined). This is further represented by the fact that the Riemann integral can be expressed as the limit of Riemann sums where means an infinitesimally small step on the x-axis to correspond with the infinitesimally small change in associated with the Riemann integral. One can justify canceling the terms by the answer shown inside this Math Overflow question . However, they would need to know differential forms which is a topic that I am unfamiliar with. A different answer on Math SE provides a more familiar explanation in which one can write the first fundamental theorem of calculus in Leibnitz notation as: Inside this answer, it is shown that you can ""cancel"" the two terms even though you are not literally cancelling . The fact that these two terms cancel is directly due to notational convenience. I'm curious if this sort of notational convenience will fail. I think that one could write Is there a scenario in which you cannot cancel inside an integral as if it were a fraction equal to ? Can you also cancel these two terms through a substitution made inside the integral?","\int y'y''\,dx, y'=\frac{dy(x)}{dx},y''=\frac{d^2y(x)}{dx^2} y=y(x) u=\frac{dy}{dx}=y' \implies du=y''\,dx, \int  y'y''\,dx=\int u\,du=\frac{1}{2}u^2+C=\frac{1}{2}\left(y'\right)^2+C. dx \int  \frac{dy}{dx} \frac{d^2y}{dx^2}\,dx=\int \frac{d^2y}{dx^2}\,dy=\int y''\,dy=\int\frac{dy'}{dx}\,dy=\int\frac{dy'}{dx}y'\,dx=\int y'\,dy'=\frac{1}{2}\left(y'\right)^2+C. dy=\frac{dy}{\require{cancel} \cancel{dx}}\,\require{cancel} \cancel{dx}=y'\,dx \implies \frac{dy'}{dx}\,dy=\frac{dy'}{\require{cancel} \cancel{dx}}y'\,\require{cancel} \cancel{dx}=y'\,dy'. dx/dx dx x dx\neq 0 dx \frac{dx}{dx}=1 \int_a^b f(x)\,dx=\lim_{n\to\infty}\sum_{i=1}^nf(x_i)\Delta x, \Delta x x dx \int _a^b \frac{df}{dx}\,dx = f(b) - f(a). dx dx/dx \int \frac{dy}{dx}\,dx=\int \frac{dy}{\require{cancel} \cancel{dx}}\,\require{cancel} \cancel{dx}=\int dy, \int \frac{dy}{dx}\frac{dy}{dx}\,dx=\int \frac{dy}{\require{cancel} \cancel{dx}}\frac{dy}{dx}\,\require{cancel} \cancel{dx}=\int \frac{dy}{dx}\,dy, \int \frac{dy^n}{dx^n}\frac{dy}{dx}\,dx=\int \frac{dy^n}{dx^n}\frac{dy}{\require{cancel} \cancel{dx}}\,\require{cancel} \cancel{dx}=\int \frac{dy^n}{dx^n} \,dy, \int \frac{dy}{dx}dy\,dx=\int \frac{dy}{\require{cancel} \cancel{dx}}dy\,\require{cancel} \cancel{dx}=\int (dy)^2. \frac{dx}{dx} 1","['calculus', 'integration', 'ordinary-differential-equations']"
90,"A CMIMC Integration Bee integral: $\int_0^\infty \left( \sin(1/x) - \frac{\sin(\pi/x)}{\pi} \right) \,dx$",A CMIMC Integration Bee integral:,"\int_0^\infty \left( \sin(1/x) - \frac{\sin(\pi/x)}{\pi} \right) \,dx","At the 2020 CMIMC Integration Bee, the following integral was one of the qualifying problems: $$\int_0^\infty \left( \sin(1/x) - \frac{\sin(\pi/x)}{\pi} \right) \,dx.$$ I attempted to use differentiation under the integral sign: $$f(t) = \int_0^\infty \left( \sin(t/x) - \frac{\sin(\pi t/x)}{\pi} \right) \,dx$$ $$f'(t) = \int_0^\infty \left( \frac{\cos(t/x) - \cos(\pi t/x)}{x}\right) \,dx.$$ We are very close to being able to use the Frullani integral, that is, $$\int_0^\infty \frac{f(ax)-f(bx)}{x} \, dx = \ln \left( \frac{b}{a} \right) \cdot \left( f(0) - \lim_{x \to \infty} f(x) \right)$$ for $f$ continuously differentiable on the nonnegative reals. However, if we try to use it with $f(x)=\cos(1/x)$ , we obtain $$f'(t) = (\ln \pi) \left( \cos(1/0) - \lim_{x \to \infty} \cos(1/x) \right),$$ which is nonsensical. This happens, of course, because $f$ is not continuously differentiable or even defined at $0$ . But if we were able to show that $$\int_0^\infty \left( \frac{\cos(t/x) - \cos(\pi t/x)}{x}\right) \,dx = (\ln \pi) \left(\lim_{x \to \infty} \cos(1/x) \right) = \ln \pi,$$ (which is true according to Wolfram Alpha,) then we would have $$f(1) = f(0) + \int_0^1 f'(x) \, dx = \int_0^1 \ln \pi \, dx = \ln \pi,$$ which is the correct answer. Any hints or solutions for how to complete my solution or for an entirely different solution are appreciated!","At the 2020 CMIMC Integration Bee, the following integral was one of the qualifying problems: I attempted to use differentiation under the integral sign: We are very close to being able to use the Frullani integral, that is, for continuously differentiable on the nonnegative reals. However, if we try to use it with , we obtain which is nonsensical. This happens, of course, because is not continuously differentiable or even defined at . But if we were able to show that (which is true according to Wolfram Alpha,) then we would have which is the correct answer. Any hints or solutions for how to complete my solution or for an entirely different solution are appreciated!","\int_0^\infty \left( \sin(1/x) - \frac{\sin(\pi/x)}{\pi} \right) \,dx. f(t) = \int_0^\infty \left( \sin(t/x) - \frac{\sin(\pi t/x)}{\pi} \right) \,dx f'(t) = \int_0^\infty \left( \frac{\cos(t/x) - \cos(\pi t/x)}{x}\right) \,dx. \int_0^\infty \frac{f(ax)-f(bx)}{x} \, dx = \ln \left( \frac{b}{a} \right) \cdot \left( f(0) - \lim_{x \to \infty} f(x) \right) f f(x)=\cos(1/x) f'(t) = (\ln \pi) \left( \cos(1/0) - \lim_{x \to \infty} \cos(1/x) \right), f 0 \int_0^\infty \left( \frac{\cos(t/x) - \cos(\pi t/x)}{x}\right) \,dx = (\ln \pi) \left(\lim_{x \to \infty} \cos(1/x) \right) = \ln \pi, f(1) = f(0) + \int_0^1 f'(x) \, dx = \int_0^1 \ln \pi \, dx = \ln \pi,","['calculus', 'integration', 'contest-math']"
91,"Evaluate $\lim_{a\rightarrow \infty }\frac{1}{a}\int_{0}^{a}\sin(x)\sin(x^2)\,dx$",Evaluate,"\lim_{a\rightarrow \infty }\frac{1}{a}\int_{0}^{a}\sin(x)\sin(x^2)\,dx","Calculate $$I=\lim_{a\rightarrow \infty }\frac{1}{a}\int_{0}^{a}\sin(x)\cdot \sin(x^2)\,dx$$ I tried to use the fact that $\lim_{x\rightarrow \infty }\frac{1}{x}\int_{0}^{x}(f(t))\,dt=\frac{1}{T}\int_{0}^{T}f(t)\,dt$ where $T$ is period of the function. So in my case $T=2\pi$ , right? So I have $$I=\lim_{a\rightarrow \infty }\frac{1}{2\pi}\int_{0}^{2\pi}\sin(x)\sin(x^2)\,dx=\lim_{a\rightarrow \infty }\frac{1}{\pi}\int_{0}^{\pi}\sin(x)\sin(x^2)\,dx$$ Now I noted $x=\pi-t$ so $$I=\lim_{a\rightarrow \infty }\frac{1}{\pi}\int_{0}^{\pi}\sin(\pi-t)\sin((\pi-t)^2)\,dt=\lim_{a\rightarrow \infty }\frac{1}{\pi}\int_{0}^{\pi}\sin(t)\sin((\pi-t)^2)\,dt$$ How to continue? Is my method correct?","Calculate I tried to use the fact that where is period of the function. So in my case , right? So I have Now I noted so How to continue? Is my method correct?","I=\lim_{a\rightarrow \infty }\frac{1}{a}\int_{0}^{a}\sin(x)\cdot \sin(x^2)\,dx \lim_{x\rightarrow \infty }\frac{1}{x}\int_{0}^{x}(f(t))\,dt=\frac{1}{T}\int_{0}^{T}f(t)\,dt T T=2\pi I=\lim_{a\rightarrow \infty }\frac{1}{2\pi}\int_{0}^{2\pi}\sin(x)\sin(x^2)\,dx=\lim_{a\rightarrow \infty }\frac{1}{\pi}\int_{0}^{\pi}\sin(x)\sin(x^2)\,dx x=\pi-t I=\lim_{a\rightarrow \infty }\frac{1}{\pi}\int_{0}^{\pi}\sin(\pi-t)\sin((\pi-t)^2)\,dt=\lim_{a\rightarrow \infty }\frac{1}{\pi}\int_{0}^{\pi}\sin(t)\sin((\pi-t)^2)\,dt","['calculus', 'integration', 'limits']"
92,$\sum_{k=0}^\infty\frac{ (-1)^{\left\lfloor\frac kn\right\rfloor}}{k+1}$,,\sum_{k=0}^\infty\frac{ (-1)^{\left\lfloor\frac kn\right\rfloor}}{k+1},"While testing a program I came across an interesting equality: $$ \sum_{k=0}^\infty\frac{ (-1)^{\left\lfloor\frac kn\right\rfloor}}{k+1} =\frac1n\left[\log2+\frac\pi2\sum_{k=1}^{n-1}\tan\frac{k\pi}{2n}\right], $$ which generalises the well-known identity for $n=1$ . Is there a simple way to prove it?",While testing a program I came across an interesting equality: which generalises the well-known identity for . Is there a simple way to prove it?,"
\sum_{k=0}^\infty\frac{ (-1)^{\left\lfloor\frac kn\right\rfloor}}{k+1}
=\frac1n\left[\log2+\frac\pi2\sum_{k=1}^{n-1}\tan\frac{k\pi}{2n}\right],
 n=1","['calculus', 'sequences-and-series']"
93,Why is it necessary to split the definite integral of a piecewise function into a sum,Why is it necessary to split the definite integral of a piecewise function into a sum,,"The second fundamental theorem of calculus (Newton-Leibniz) tells us that: If $f$ is a real-valued function on a closed interval $[a, b]$ and $F$ is an antiderivative of $f$ in $[a,b]$ s.t. $F'(x)=f(x)$ , then $$\int_{a}^{b} f(t) dt = F(b)-F(a)$$ Say we have a real-valued piecewise continuous function $f(x)$ defined on $[a,b]$ which is made up of intermediate functions $f_1, f_2, ..., f_n$ for $x \in [x_1, x_2) \cup [x_2, x_3) \cup ...\cup[x_n, x_{n+1}]$ Using the second fundamental theorem of calculus, one may find an appropriate $F(x)$ s.t. $F'(x)=f(x)$ , like $$ F(x) = \begin{cases}        \int f_1(x) dx + C_1 &\quad\text{if} \ x \in [x_1, x_2) \\        \int f_2(x) dx + C_2 &\quad\text{if} \ x \in [x_2, x_3) \\ ... \\        \int f_n(x) dx + C_n &\quad\text{if} \ x \in [x_n, x_{n+1}] \\       \end{cases} $$ My question is the following: why is it that when we evaluate $\int_a^b f(t) dt$ , we must split it into the sum of $$\int_a^{x_i} f(t) dt + \int_{x_i}^{x_{i+1}} f(t) dt + \ ...\ +\int_{x_{i+k}}^{b} f(t) dt $$ instead of simply evaluating $F(b) - F(a)$ by selecting the appropriate antiderivatives from the piecewise function? Example: $$ f(x) = \begin{cases}        x+1 &\quad\text{if} \ x \in [0, 2) \\        1 &\quad\text{if} \ x \geq 2 \\      \end{cases} $$ An antiderivative of $f$ would be $$ F(x) = \begin{cases}        \frac{x^2}{2}+x + 2 &\quad\text{if} \ x \in [0, 2) \\        x+7 &\quad\text{if} \ x \geq 2\\       \end{cases} $$ Note the constants of integration $C_1=2$ and $C_2=7$ Clearly, computing $F(3)-F(1)$ does not give the area under $f(x)$ from $x=1$ to $x=3$ . Even though intuitively it makes sense to break the definite integral into a sum, I am unable to come up with a straightforward reason as to why $F(3)-F(1)$ wouldn't give the correct result as long as I respected the conditions of the Newton-Leibniz axiom (maybe I didn't, but please let me know why ). EDIT: For a discontinuous function $f(x)$ on an interval $[a,b]$ with finitely many discontinuities, why do we need its antiderivative $F(x)$ to be continuous in order to apply $F(b)$ - $F(a)$ ? No matter if it's continuous or not, $F(x)$ still won't be differentiable at $f$ 's discontinuities, and its derivative would still be $f$ . However, if it's not continuous, $F(b)$ - $F(a)$ would no longer give the area under the curve. Why? EDIT 2: Lebesgue Integration asserts that that an antiderivative $F$ differentiable almost everywhere must be absolutely continuous for $F(b)$ - $F(a)$ (FTC 2) to work. So case closed.","The second fundamental theorem of calculus (Newton-Leibniz) tells us that: If is a real-valued function on a closed interval and is an antiderivative of in s.t. , then Say we have a real-valued piecewise continuous function defined on which is made up of intermediate functions for Using the second fundamental theorem of calculus, one may find an appropriate s.t. , like My question is the following: why is it that when we evaluate , we must split it into the sum of instead of simply evaluating by selecting the appropriate antiderivatives from the piecewise function? Example: An antiderivative of would be Note the constants of integration and Clearly, computing does not give the area under from to . Even though intuitively it makes sense to break the definite integral into a sum, I am unable to come up with a straightforward reason as to why wouldn't give the correct result as long as I respected the conditions of the Newton-Leibniz axiom (maybe I didn't, but please let me know why ). EDIT: For a discontinuous function on an interval with finitely many discontinuities, why do we need its antiderivative to be continuous in order to apply - ? No matter if it's continuous or not, still won't be differentiable at 's discontinuities, and its derivative would still be . However, if it's not continuous, - would no longer give the area under the curve. Why? EDIT 2: Lebesgue Integration asserts that that an antiderivative differentiable almost everywhere must be absolutely continuous for - (FTC 2) to work. So case closed.","f [a, b] F f [a,b] F'(x)=f(x) \int_{a}^{b} f(t) dt = F(b)-F(a) f(x) [a,b] f_1, f_2, ..., f_n x \in [x_1, x_2) \cup [x_2, x_3) \cup ...\cup[x_n, x_{n+1}] F(x) F'(x)=f(x)  F(x) = \begin{cases}
       \int f_1(x) dx + C_1 &\quad\text{if} \ x \in [x_1, x_2) \\
       \int f_2(x) dx + C_2 &\quad\text{if} \ x \in [x_2, x_3) \\
... \\
       \int f_n(x) dx + C_n &\quad\text{if} \ x \in [x_n, x_{n+1}] \\ 
     \end{cases}  \int_a^b f(t) dt \int_a^{x_i} f(t) dt + \int_{x_i}^{x_{i+1}} f(t) dt + \ ...\ +\int_{x_{i+k}}^{b} f(t) dt  F(b) - F(a)  f(x) = \begin{cases}
       x+1 &\quad\text{if} \ x \in [0, 2) \\
       1 &\quad\text{if} \ x \geq 2 \\
     \end{cases}  f  F(x) = \begin{cases}
       \frac{x^2}{2}+x + 2 &\quad\text{if} \ x \in [0, 2) \\
       x+7 &\quad\text{if} \ x \geq 2\\ 
     \end{cases}  C_1=2 C_2=7 F(3)-F(1) f(x) x=1 x=3 F(3)-F(1) f(x) [a,b] F(x) F(b) F(a) F(x) f f F(b) F(a) F F(b) F(a)","['calculus', 'integration', 'derivatives', 'riemann-integration', 'piecewise-continuity']"
94,"If a first derivative doesn't exist at a certain point, is it not a critical point?","If a first derivative doesn't exist at a certain point, is it not a critical point?",,"Say $f'(x)$ of a function was $(x+2) \over (x+3)$ . If $x = -2$ then $f'(x)$ = 0, which means that at this point, there would be a local min or max. But what if $x = -3$ ? It doesn't exist for $f'(x)$ , so do we just ignore it? State it DNE at $x = -3$ so it is not a critical point?","Say of a function was . If then = 0, which means that at this point, there would be a local min or max. But what if ? It doesn't exist for , so do we just ignore it? State it DNE at so it is not a critical point?",f'(x) (x+2) \over (x+3) x = -2 f'(x) x = -3 f'(x) x = -3,['calculus']
95,Inflection point for function with fractional exponents,Inflection point for function with fractional exponents,,"Show that $f(x) = 4x^{1/3}-x^{4/3}$ has an inflection point at $x=1$ . I correctly get $$f'(x) = \frac{4(1-x)}{3x^{2/3}}\implies f''(x)=-\frac{4(x+2)}{9x^{5/3}}$$ It is clear to me that there is an inflection point at $x=-2$ since this value of $x$ makes the second derivative zero. The text shows that there is also an inflection point at $x=1$ .  I see that this value makes the first derivative $=0$ , but I don't understand why this causes an inflection point.  Can anyone help clarify this point?","Show that has an inflection point at . I correctly get It is clear to me that there is an inflection point at since this value of makes the second derivative zero. The text shows that there is also an inflection point at .  I see that this value makes the first derivative , but I don't understand why this causes an inflection point.  Can anyone help clarify this point?",f(x) = 4x^{1/3}-x^{4/3} x=1 f'(x) = \frac{4(1-x)}{3x^{2/3}}\implies f''(x)=-\frac{4(x+2)}{9x^{5/3}} x=-2 x x=1 =0,"['calculus', 'derivatives', 'stationary-point']"
96,Confused when changing from Lebesgue Integral to Riemann Integral,Confused when changing from Lebesgue Integral to Riemann Integral,,"I'm currently studying Stochastic Calculus via Shreve II. I have a question about switching back and forth between Lebesgue and Riemann Integral. Suppose we have a non-negative random variable $X$ defined on a probability space $(\Omega, F, P)$ with exponential distribution: $$P(X<x) = 1-e^{-\lambda x}$$ Written in Lebesgue Integral, the expected value of $X$ can be written as: $$E[X] = \int_{\{{\omega \mid X(\omega) \geq 0}\}}^{ }X(\omega)dP(\omega)$$ Question: How exactly do we switch from $\omega$ in the Lebesgue Integral to $x$ in Riemann integral so that we get $$E[X] = \int_{0}^{\infty}x\lambda e^{-\lambda x}dx$$ Does this have to do with the fact that we should define our $\Omega$ to be the Borel $\sigma$ -algebra $B(\mathbb{R})$ and simply define $X(\omega) = \omega$ for non-negative $\omega$ 's? Any help is greatly appreciated!","I'm currently studying Stochastic Calculus via Shreve II. I have a question about switching back and forth between Lebesgue and Riemann Integral. Suppose we have a non-negative random variable defined on a probability space with exponential distribution: Written in Lebesgue Integral, the expected value of can be written as: Question: How exactly do we switch from in the Lebesgue Integral to in Riemann integral so that we get Does this have to do with the fact that we should define our to be the Borel -algebra and simply define for non-negative 's? Any help is greatly appreciated!","X (\Omega, F, P) P(X<x) = 1-e^{-\lambda x} X E[X] = \int_{\{{\omega \mid X(\omega) \geq 0}\}}^{ }X(\omega)dP(\omega) \omega x E[X] = \int_{0}^{\infty}x\lambda e^{-\lambda x}dx \Omega \sigma B(\mathbb{R}) X(\omega) = \omega \omega","['calculus', 'probability', 'probability-theory', 'measure-theory']"
97,Monotonicity of the function $(1+x)^{\frac{1}{x}}\left(1+\frac{1}{x}\right)^x$.,Monotonicity of the function .,(1+x)^{\frac{1}{x}}\left(1+\frac{1}{x}\right)^x,"Let $f(x)=(1+x)^{\frac{1}{x}}\left(1+\frac{1}{x}\right)^x, 0<x\leq 1.$ Prove that $f$ is strictly increasing and $e<f(x)\leq 4.$ In order to study the Monotonicity of $f$, let  $$g(x)=\log f(x)=\frac{1}{x}\log (1+x)+x\log \left(1+\frac{1}{x}\right).$$ And $f$ and $g$ has the same Monotonicity. By computation, $$g'(x)=\frac{1}{x^2}\left(\frac{x}{1+x}-\log (1+x)\right)+\log \left(1+\frac{1}{x}\right)-\frac{1}{1+x}.$$  As we know $\frac{x}{1+x}-\log (1+x)\leq 0$ and $\log \left(1+\frac{1}{x}\right)-\frac{1}{1+x}\geq 0$. So it does not determine the sign of $g'(x)$. If we compute the second derivative $g''(x)$,  you will find it is also difficult to determine the sign of $g''(x)$.  Our main goal is to prove $$\frac{1}{x^2}\left(\frac{x}{1+x}-\log (1+x)\right)+\log \left(1+\frac{1}{x}\right)-\frac{1}{1+x}>0.$$ Is there some tricks to prove this result. Any help and hint will welcome.","Let $f(x)=(1+x)^{\frac{1}{x}}\left(1+\frac{1}{x}\right)^x, 0<x\leq 1.$ Prove that $f$ is strictly increasing and $e<f(x)\leq 4.$ In order to study the Monotonicity of $f$, let  $$g(x)=\log f(x)=\frac{1}{x}\log (1+x)+x\log \left(1+\frac{1}{x}\right).$$ And $f$ and $g$ has the same Monotonicity. By computation, $$g'(x)=\frac{1}{x^2}\left(\frac{x}{1+x}-\log (1+x)\right)+\log \left(1+\frac{1}{x}\right)-\frac{1}{1+x}.$$  As we know $\frac{x}{1+x}-\log (1+x)\leq 0$ and $\log \left(1+\frac{1}{x}\right)-\frac{1}{1+x}\geq 0$. So it does not determine the sign of $g'(x)$. If we compute the second derivative $g''(x)$,  you will find it is also difficult to determine the sign of $g''(x)$.  Our main goal is to prove $$\frac{1}{x^2}\left(\frac{x}{1+x}-\log (1+x)\right)+\log \left(1+\frac{1}{x}\right)-\frac{1}{1+x}>0.$$ Is there some tricks to prove this result. Any help and hint will welcome.",,"['calculus', 'analysis', 'inequality', 'exponential-function', 'tangent-line-method']"
98,Solving improper integral involving product logarithm and exponentials,Solving improper integral involving product logarithm and exponentials,,"It is possible to verify or show numerically that $\displaystyle \int^{\infty}_{-\infty}W(e^{x-e^{x}})dx=\frac{\pi^2}{12}$ $W(x)$ is the Lambert W function. This integral does not seem to have a solution in terms of elementary or standard functions. Is it possible to prove or solve it analytically, or to find a symbolic solution?","It is possible to verify or show numerically that $\displaystyle \int^{\infty}_{-\infty}W(e^{x-e^{x}})dx=\frac{\pi^2}{12}$ $W(x)$ is the Lambert W function. This integral does not seem to have a solution in terms of elementary or standard functions. Is it possible to prove or solve it analytically, or to find a symbolic solution?",,"['calculus', 'integration', 'analysis', 'lambert-w']"
99,Integrate $\sin^{-1}\frac{2x}{1+x^2}$,Integrate,\sin^{-1}\frac{2x}{1+x^2},"Integrate $\sin^{-1}\frac{2x}{1+x^2}$ The solution is given in my reference as: $2x\tan^{-1}x-\log(1+x^2)+C$. But, is it a complete solution ? My Attempt $$ \int 2\tan^{-1}x \, dx=\int \tan^{-1}x \cdot 2\,dx=\tan^{-1}x\int2\,dx-\int\frac{1}{1+x^2}\int2\,dx\cdot dx\\ =\tan^{-1}x \cdot2x-\int\frac{2x}{1+x^2}\,dx=2x\tan^{-1}x-\log(1+x^2)+C $$ $$ 2\tan^{-1}x=\begin{cases}\sin^{-1}\frac{2x}{1+x^2}\text{ if }|x|\leq{1}\\ \pi-\sin^{-1}\frac{2x}{1+x^2}\text{ if }|x|>{1}\text{ and }x>0\\ -\pi-\sin^{-1}\frac{2x}{1+x^2}\text{ if }|x|>{1}\text{ and }x<0 \end{cases}\\ \sin^{-1}\frac{2x}{1+x^2}=\begin{cases}2\tan^{-1}x\text{ if }|x|\leq{1}\\ \pi-2\tan^{-1}x\text{ if }|x|>{1}\text{ and }x>0\\ -\pi-2\tan^{-1}x\text{ if }|x|>{1}\text{ and }x<0 \end{cases} $$ $$ \int\sin^{-1}\frac{2x}{1+x^2}\,dx=\begin{cases}\int2\tan^{-1}x\,dx&\text{ if } |x|\leq{1}\\\int\pi\, dx-\int2\tan^{-1}x\,dx&\text{ if }|x|>{1}&\text{ and } x>0\\-\int\pi \,dx-\int2\tan^{-1}x\,dx&\text{ if }|x|>{1}&\text{ and } x<0\end{cases}=\begin{cases}\color{red}{2x\tan^{-1}x-\log(1+x^2)+C\text{ if } |x|\leq{1}}\\\pi x-2x\tan^{-1}x+\log(1+x^2)+C&\text{ if }|x|>{1}&\text{ and } x>0\\-\pi x-2x\tan^{-1}x+\log(1+x^2)+C&\text{ if }|x|>{1}&\text{ and } x<0\end{cases}$$ So don't we have two more cases for our solution rather than that is given in my reference, right ?","Integrate $\sin^{-1}\frac{2x}{1+x^2}$ The solution is given in my reference as: $2x\tan^{-1}x-\log(1+x^2)+C$. But, is it a complete solution ? My Attempt $$ \int 2\tan^{-1}x \, dx=\int \tan^{-1}x \cdot 2\,dx=\tan^{-1}x\int2\,dx-\int\frac{1}{1+x^2}\int2\,dx\cdot dx\\ =\tan^{-1}x \cdot2x-\int\frac{2x}{1+x^2}\,dx=2x\tan^{-1}x-\log(1+x^2)+C $$ $$ 2\tan^{-1}x=\begin{cases}\sin^{-1}\frac{2x}{1+x^2}\text{ if }|x|\leq{1}\\ \pi-\sin^{-1}\frac{2x}{1+x^2}\text{ if }|x|>{1}\text{ and }x>0\\ -\pi-\sin^{-1}\frac{2x}{1+x^2}\text{ if }|x|>{1}\text{ and }x<0 \end{cases}\\ \sin^{-1}\frac{2x}{1+x^2}=\begin{cases}2\tan^{-1}x\text{ if }|x|\leq{1}\\ \pi-2\tan^{-1}x\text{ if }|x|>{1}\text{ and }x>0\\ -\pi-2\tan^{-1}x\text{ if }|x|>{1}\text{ and }x<0 \end{cases} $$ $$ \int\sin^{-1}\frac{2x}{1+x^2}\,dx=\begin{cases}\int2\tan^{-1}x\,dx&\text{ if } |x|\leq{1}\\\int\pi\, dx-\int2\tan^{-1}x\,dx&\text{ if }|x|>{1}&\text{ and } x>0\\-\int\pi \,dx-\int2\tan^{-1}x\,dx&\text{ if }|x|>{1}&\text{ and } x<0\end{cases}=\begin{cases}\color{red}{2x\tan^{-1}x-\log(1+x^2)+C\text{ if } |x|\leq{1}}\\\pi x-2x\tan^{-1}x+\log(1+x^2)+C&\text{ if }|x|>{1}&\text{ and } x>0\\-\pi x-2x\tan^{-1}x+\log(1+x^2)+C&\text{ if }|x|>{1}&\text{ and } x<0\end{cases}$$ So don't we have two more cases for our solution rather than that is given in my reference, right ?",,"['calculus', 'integration', 'trigonometry', 'indefinite-integrals', 'inverse-function']"
