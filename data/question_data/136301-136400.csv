,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Differential equation system solution: do I get the right solution?,Differential equation system solution: do I get the right solution?,,"I am very stuck with differential equation systems. For example: $ Y'(x) =      \begin{pmatrix}     2 & 0 & 1 \\     0 & 2 & 0 \\     0 & 1 & 3 \\     \end{pmatrix} Y(x) $ I get the eigenvalues and eigenvectors: $ \lambda = 2  (double)  \rightarrow   \vec v_{\lambda2} =  \begin{pmatrix}     1 \\     0 \\     0 \\     \end{pmatrix} $ and $ \lambda = 3   \rightarrow   \vec v_{\lambda3} =  \begin{pmatrix}     1 \\     0 \\     1 \\     \end{pmatrix} $ I obtain just one vector asociated with $ \lambda = 2 $ value, so that I suppose I've to get another vector. For this eigenvalue: $ (A-\lambda I) = \begin{pmatrix}     0 & 0 & 1 \\     0 & 0 & 0 \\     0 & 1 & 1 \\     \end{pmatrix} $ So that the vector I'm looking must meet: $ \begin{pmatrix}     0 & 0 & 1 \\     0 & 0 & 0 \\     0 & 1 & 1 \\     \end{pmatrix} \begin{pmatrix}     v_{x} \\     v_{y} \\     v_{z} \\     \end{pmatrix} = \begin{pmatrix}     1 \\     0 \\     0 \\     \end{pmatrix} $ From doing this I get: $ \begin{pmatrix}     v_{x} \\     v_{y} \\     v_{z} \\     \end{pmatrix} \sim \begin{pmatrix}     \alpha \\     -1 \\     1 \\     \end{pmatrix} $ , and I choose the vector $ \begin{pmatrix}     0 \\     -1 \\     1 \\     \end{pmatrix} $ The problems come from this point. I think the general solution should be: $ y(x) = C_{1} \begin{pmatrix}     1 \\     0 \\     0 \\     \end{pmatrix} e^{2x} + C_{1}x \begin{pmatrix}     1 \\     0 \\     0 \\     \end{pmatrix} e^{2x} + C_{2} \begin{pmatrix}     0 \\     -1 \\     1 \\     \end{pmatrix} e^{2x} + C_{3} \begin{pmatrix}     1 \\     0 \\     1 \\     \end{pmatrix} e^{3x} $ But this result is wrong. I've tried to proove it and it doesn't work. I've read in some books about it but I think I'm following the mathematic method fine... Does anyone know what I'm doing wrong? Thanks in advance.","I am very stuck with differential equation systems. For example: I get the eigenvalues and eigenvectors: and I obtain just one vector asociated with value, so that I suppose I've to get another vector. For this eigenvalue: So that the vector I'm looking must meet: From doing this I get: , and I choose the vector The problems come from this point. I think the general solution should be: But this result is wrong. I've tried to proove it and it doesn't work. I've read in some books about it but I think I'm following the mathematic method fine... Does anyone know what I'm doing wrong? Thanks in advance.","
Y'(x) = 
    \begin{pmatrix}
    2 & 0 & 1 \\
    0 & 2 & 0 \\
    0 & 1 & 3 \\
    \end{pmatrix}
Y(x)
  \lambda = 2  (double)  \rightarrow   \vec v_{\lambda2} =  \begin{pmatrix}
    1 \\
    0 \\
    0 \\
    \end{pmatrix}
  \lambda = 3   \rightarrow   \vec v_{\lambda3} =  \begin{pmatrix}
    1 \\
    0 \\
    1 \\
    \end{pmatrix}
  \lambda = 2   (A-\lambda I) = \begin{pmatrix}
    0 & 0 & 1 \\
    0 & 0 & 0 \\
    0 & 1 & 1 \\
    \end{pmatrix}
 
\begin{pmatrix}
    0 & 0 & 1 \\
    0 & 0 & 0 \\
    0 & 1 & 1 \\
    \end{pmatrix}
\begin{pmatrix}
    v_{x} \\
    v_{y} \\
    v_{z} \\
    \end{pmatrix}
=
\begin{pmatrix}
    1 \\
    0 \\
    0 \\
    \end{pmatrix}
 
\begin{pmatrix}
    v_{x} \\
    v_{y} \\
    v_{z} \\
    \end{pmatrix}
\sim
\begin{pmatrix}
    \alpha \\
    -1 \\
    1 \\
    \end{pmatrix}
 
\begin{pmatrix}
    0 \\
    -1 \\
    1 \\
    \end{pmatrix}
 
y(x) = C_{1}
\begin{pmatrix}
    1 \\
    0 \\
    0 \\
    \end{pmatrix}
e^{2x}
+
C_{1}x
\begin{pmatrix}
    1 \\
    0 \\
    0 \\
    \end{pmatrix}
e^{2x}
+
C_{2}
\begin{pmatrix}
    0 \\
    -1 \\
    1 \\
    \end{pmatrix}
e^{2x}
+
C_{3}
\begin{pmatrix}
    1 \\
    0 \\
    1 \\
    \end{pmatrix}
e^{3x}
","['ordinary-differential-equations', 'systems-of-equations']"
1,Why compactness of control space implies Value Function $>-\infty$,Why compactness of control space implies Value Function,>-\infty,"I went through the Sec I.4 of the book Controlled Markov Process and Viscosity Solutions by Fleming and Soner . In the model, $x(s)$ for $s\in[t,t_{1}]$ follows the ODE, \begin{eqnarray} x^{\prime}(s)&=&f(s,x(s),u(s)) \\ x(t)&=&x \end{eqnarray} where $f(s,x,u)$ is a continuous function on the set $[t_{0},t_{1}]\times\mathbb{R}^{m}\times U$ , where $U\subseteq\mathbb{R}^{m}$ is an open set called the control space . $f$ also follows the property, \begin{eqnarray} \forall s\in[t,t_{1}], x,y\in\mathbb{R}^{n},|u|\leq\rho,~~|f(s,x,u)-f(s,y,u)|&\leq & K_{\rho}|x-y| \end{eqnarray} for some constant $K_{\rho}>0$ . The control $u(\cdot)$ must be bounded and belong to the set, $\mathcal{U}^{0}(t)=L^{\infty}([t,t_{1}]; U)$ , so that the IVP satisfied by $x(s)$ has a unique solution.  The problem is to minimize the cost function, \begin{eqnarray} J(x,t;u)&=&\int_{t}^{t_{1}}L(s,x(s),u(s))ds+\psi(x(t_{1})) \end{eqnarray} over the set of all admissible controls $\mathcal{U}^{0}(t)$ . This minimum is defined as the value function, \begin{eqnarray} V(t,x)&=&\inf_{u\in\mathcal{U}^{0}(t)}J(x,t;u). \end{eqnarray} The book states that if the control space is a compact set, $V(t,x)>-\infty$ . Is it because, the $J(x,t;u)$ is somehow continuous wrt $u$ in the space $\mathcal{U}^{0}(t)$ ? But still, where is the compactness of $U$ getting used. Thanks in advance.","I went through the Sec I.4 of the book Controlled Markov Process and Viscosity Solutions by Fleming and Soner . In the model, for follows the ODE, where is a continuous function on the set , where is an open set called the control space . also follows the property, for some constant . The control must be bounded and belong to the set, , so that the IVP satisfied by has a unique solution.  The problem is to minimize the cost function, over the set of all admissible controls . This minimum is defined as the value function, The book states that if the control space is a compact set, . Is it because, the is somehow continuous wrt in the space ? But still, where is the compactness of getting used. Thanks in advance.","x(s) s\in[t,t_{1}] \begin{eqnarray}
x^{\prime}(s)&=&f(s,x(s),u(s)) \\
x(t)&=&x
\end{eqnarray} f(s,x,u) [t_{0},t_{1}]\times\mathbb{R}^{m}\times U U\subseteq\mathbb{R}^{m} f \begin{eqnarray}
\forall s\in[t,t_{1}], x,y\in\mathbb{R}^{n},|u|\leq\rho,~~|f(s,x,u)-f(s,y,u)|&\leq & K_{\rho}|x-y|
\end{eqnarray} K_{\rho}>0 u(\cdot) \mathcal{U}^{0}(t)=L^{\infty}([t,t_{1}]; U) x(s) \begin{eqnarray}
J(x,t;u)&=&\int_{t}^{t_{1}}L(s,x(s),u(s))ds+\psi(x(t_{1}))
\end{eqnarray} \mathcal{U}^{0}(t) \begin{eqnarray}
V(t,x)&=&\inf_{u\in\mathcal{U}^{0}(t)}J(x,t;u).
\end{eqnarray} V(t,x)>-\infty J(x,t;u) u \mathcal{U}^{0}(t) U","['ordinary-differential-equations', 'analysis', 'compactness', 'supremum-and-infimum', 'optimal-control']"
2,Need help solving Laplace Transform Question,Need help solving Laplace Transform Question,,"For context: I am just starting to learn LaPlace Transform and am stumped with this question. Find the PS of $x''+x'-12x=sin(3t) + e^{-4t}+e^{3t}$ Where $x(0)=0, x'(0)=0$ using LaPlace Transform My Approach: Let X(s)= $\mathscr{L}[x(t)]$ Apply the LaPlace Transform to both sides of the equation $\mathscr{L}[x''] + \mathscr{L}[x'] - 12\mathscr{L}[x]$ = $\mathscr{L}[sin(3t)] +\mathscr{L}[e^{-4t}] +\mathscr{L}[e^{3t}]$ to get $s^2X(s)-sx(0)-x'(0)+sX(s)-x(0)-12X(s) = \cfrac{3}{s^2+9}+\cfrac{1}{s+4}+\cfrac{1}{s-3}$ Plugging in the IC's $x(0) = 0$ and $x'(0)=0$ and then factoring $X(s)$ we get $X(s)[s^2+s-12] = \cfrac{3}{s^2+9}+\cfrac{1}{s+4}+\cfrac{1}{s-3}$ You should then make the RHS have a common denominator so you get $X(s)[s^2+s-12] = \cfrac{3(s+4)(s-3)}{(s^2+9)(s+4)(s-3)}+\cfrac{(s^2+9)(s-3)}{(s+4)(s^2+9)(s-3)}+\cfrac{(s+4)(s^2+9)}{(s+4)(s-3)(s^2+9)} = \cfrac{2s^3+4s^2+21s-27}{(s^2+9)(s-3)(s+4)}$ Dividing both sides by $[s^2+s-12]$ AKA $(s+4)(s-3)$ to get $X(s)$ by itself We get $X(s) = \cfrac{2s^3+4s^2+21s-27}{(s^2+9)(s-3)^2(s+4)^2}$ Now I Believe I'm supposed to use partial fraction decomposition here to make taking the Inverse LaPlace Transform easier so... $\cfrac{2s^3+4s^2+21s-27}{(s^2+9)(s-3)^2(s+4)^2} = \cfrac{As+B}{s^2+9} + \cfrac{C}{s-3} + \cfrac{D}{(s-3)^2} + \cfrac{E}{(s+4)} + \cfrac{F}{(s+4)^2}$ Then multiplying both sides by the LHS denominator and simplifying we get $2s^3+4s^2+21s-27 = (As+B)(s-3)^2(s-4)^2+C(s^2+9)(s-3)(s-4)^2+D(s^2+9)(s+4)^2+E(s+4)(s-3)^2(s^2+9)+F(s^2+9)(s-3)^2$ Here is where I get stuck. How do I solve for A,B,C,D,E,F? This seems ridiculously complex and seems like so much algebra that it makes me think that partial fraction decomposition isn't the right way. So what I've come to ask is partial fraction decomposition to right way to tackle this question? Is there an more efficient way besides partial fraction decomposition? If there's not and partial fraction decomposition is the correct way, How can I solve for A,B,C,D,E,F? Can someone help me determine the algebra, I'm not used to partial fractions being this long unless I've made an error somewhere... Thank You For Any Help","For context: I am just starting to learn LaPlace Transform and am stumped with this question. Find the PS of Where using LaPlace Transform My Approach: Let X(s)= Apply the LaPlace Transform to both sides of the equation = to get Plugging in the IC's and and then factoring we get You should then make the RHS have a common denominator so you get Dividing both sides by AKA to get by itself We get Now I Believe I'm supposed to use partial fraction decomposition here to make taking the Inverse LaPlace Transform easier so... Then multiplying both sides by the LHS denominator and simplifying we get Here is where I get stuck. How do I solve for A,B,C,D,E,F? This seems ridiculously complex and seems like so much algebra that it makes me think that partial fraction decomposition isn't the right way. So what I've come to ask is partial fraction decomposition to right way to tackle this question? Is there an more efficient way besides partial fraction decomposition? If there's not and partial fraction decomposition is the correct way, How can I solve for A,B,C,D,E,F? Can someone help me determine the algebra, I'm not used to partial fractions being this long unless I've made an error somewhere... Thank You For Any Help","x''+x'-12x=sin(3t) + e^{-4t}+e^{3t} x(0)=0, x'(0)=0 \mathscr{L}[x(t)] \mathscr{L}[x''] + \mathscr{L}[x'] - 12\mathscr{L}[x] \mathscr{L}[sin(3t)] +\mathscr{L}[e^{-4t}] +\mathscr{L}[e^{3t}] s^2X(s)-sx(0)-x'(0)+sX(s)-x(0)-12X(s) = \cfrac{3}{s^2+9}+\cfrac{1}{s+4}+\cfrac{1}{s-3} x(0) = 0 x'(0)=0 X(s) X(s)[s^2+s-12] = \cfrac{3}{s^2+9}+\cfrac{1}{s+4}+\cfrac{1}{s-3} X(s)[s^2+s-12] = \cfrac{3(s+4)(s-3)}{(s^2+9)(s+4)(s-3)}+\cfrac{(s^2+9)(s-3)}{(s+4)(s^2+9)(s-3)}+\cfrac{(s+4)(s^2+9)}{(s+4)(s-3)(s^2+9)} = \cfrac{2s^3+4s^2+21s-27}{(s^2+9)(s-3)(s+4)} [s^2+s-12] (s+4)(s-3) X(s) X(s) = \cfrac{2s^3+4s^2+21s-27}{(s^2+9)(s-3)^2(s+4)^2} \cfrac{2s^3+4s^2+21s-27}{(s^2+9)(s-3)^2(s+4)^2} = \cfrac{As+B}{s^2+9} + \cfrac{C}{s-3} + \cfrac{D}{(s-3)^2} + \cfrac{E}{(s+4)} + \cfrac{F}{(s+4)^2} 2s^3+4s^2+21s-27 = (As+B)(s-3)^2(s-4)^2+C(s^2+9)(s-3)(s-4)^2+D(s^2+9)(s+4)^2+E(s+4)(s-3)^2(s^2+9)+F(s^2+9)(s-3)^2",['ordinary-differential-equations']
3,Stability of differential equations under nonlinear pertubations,Stability of differential equations under nonlinear pertubations,,"I am trying to understand the proof of the following theorem : Let $A$ be an $n\times n$ matrix such that it has only negative eigenvalues and let $f: \mathbb{R}\times \mathbb{R}^n$ continuous and locally Lipschits in $x$ . If $f(t,0)=0$ for every $t\in \mathbb{R}$ and $\lim_{x\rightarrow 0}\frac{||f(t,x)||}{||x||}=0$ for any $t\in \mathbb{R}$ then the zero solution of the equation $x'=Ax+f(t,x)$ is asymptotically stable . Now in the proof of this, the authors claim that given $x_0\in \mathbb{R}^n$ with $||x_0||<\delta$ the solution satisfies $||x(t)||<\delta $ for any $t\in [t_0,t_1]$ sufficiently small. And then they use that fact to prove that if $||x_0||<\frac{\delta}{M}$ then $||x(t)||<\delta$ using the following \begin{align}   x(t)&=e^{A(t-t_0)}x_0+\int_{t_0}^t e^{A(t-s)}f(s,x(s))ds \\ \|x(t)\|&\leq Me^{-\alpha(t-t_0)}\|x_0\|+\int_{t_0}^tMe^{-\alpha(t-s)}\epsilon \|x(s)\|ds \\ e^{\alpha t}\|x(t)\|&\leq Me^{\alpha t_0}\|x_0\|+\int_{t_0}^t M\epsilon e^{\alpha s}\|x(s)\|ds \\ e^{\alpha t}\|x(t)\|&\leq Me^{\alpha t_0}e^{M\epsilon (t-t_0)} \\[.5em] \|x(t)\|&\leq Me^{(-\alpha +M\epsilon) (t-t_0)}\|x_0\| \end{align} I am just a bit confused why we are doing this for $||x_0||<\frac{\delta}{M}$ since we know that if $||x_0||<\delta$ then $||x(t)||<\delta$ . I thought this might be an attempt to enlarge the interval where we have that $||x(t)||<\delta$ , to use the fact that $|f(s,x(s)|<\epsilon||x(t)||$ we need the fact that $||x(t)||<\delta$ so we stay in the same interval. Can anyone help me figure out why we care that $||x_0||>\frac{\delta}{M}$ ? Thanks in advance.","I am trying to understand the proof of the following theorem : Let be an matrix such that it has only negative eigenvalues and let continuous and locally Lipschits in . If for every and for any then the zero solution of the equation is asymptotically stable . Now in the proof of this, the authors claim that given with the solution satisfies for any sufficiently small. And then they use that fact to prove that if then using the following I am just a bit confused why we are doing this for since we know that if then . I thought this might be an attempt to enlarge the interval where we have that , to use the fact that we need the fact that so we stay in the same interval. Can anyone help me figure out why we care that ? Thanks in advance.","A n\times n f: \mathbb{R}\times \mathbb{R}^n x f(t,0)=0 t\in \mathbb{R} \lim_{x\rightarrow 0}\frac{||f(t,x)||}{||x||}=0 t\in \mathbb{R} x'=Ax+f(t,x) x_0\in \mathbb{R}^n ||x_0||<\delta ||x(t)||<\delta  t\in [t_0,t_1] ||x_0||<\frac{\delta}{M} ||x(t)||<\delta \begin{align}  
x(t)&=e^{A(t-t_0)}x_0+\int_{t_0}^t e^{A(t-s)}f(s,x(s))ds
\\
\|x(t)\|&\leq Me^{-\alpha(t-t_0)}\|x_0\|+\int_{t_0}^tMe^{-\alpha(t-s)}\epsilon \|x(s)\|ds
\\
e^{\alpha t}\|x(t)\|&\leq Me^{\alpha t_0}\|x_0\|+\int_{t_0}^t M\epsilon e^{\alpha s}\|x(s)\|ds
\\
e^{\alpha t}\|x(t)\|&\leq Me^{\alpha t_0}e^{M\epsilon (t-t_0)}
\\[.5em]
\|x(t)\|&\leq Me^{(-\alpha +M\epsilon) (t-t_0)}\|x_0\|
\end{align} ||x_0||<\frac{\delta}{M} ||x_0||<\delta ||x(t)||<\delta ||x(t)||<\delta |f(s,x(s)|<\epsilon||x(t)|| ||x(t)||<\delta ||x_0||>\frac{\delta}{M}","['ordinary-differential-equations', 'stability-in-odes']"
4,Interesting properties of Green's function,Interesting properties of Green's function,,I am trying to improve my understanding of the Green's Function and its use to solve second-order linear ODEs. Is it correct to assume that the Green's functions can be used to solve all second order linear ODEs? Is there any examples when using the Green’s functions to solve second-order linear ODEs that gives an interesting or special result? Is there any properties of Green's function that has a special case or is special?,I am trying to improve my understanding of the Green's Function and its use to solve second-order linear ODEs. Is it correct to assume that the Green's functions can be used to solve all second order linear ODEs? Is there any examples when using the Green’s functions to solve second-order linear ODEs that gives an interesting or special result? Is there any properties of Green's function that has a special case or is special?,,"['ordinary-differential-equations', 'greens-function']"
5,How to verify that this implicit equation is a solution to a nonlinear ordinary differential equation.,How to verify that this implicit equation is a solution to a nonlinear ordinary differential equation.,,"I am studying the nonlinear ordinary differential equation $$\frac{d^2y}{dx^2}=\frac{1}{y}-\frac{x}{y^2}\frac{dy}{dx}$$ I have entered this equation into two different math software packages, and they produce different answers. software 1: $$0=c_2-\ln(x)-\frac{1}{2}\ln\left(-\frac{c_1y}{x}-\frac{y^2}{x^2}+1\right)-\frac{c_1}{\sqrt{-c_1^2+4}}\tan^{-1}\left(\frac{c_1+\frac{2y}{x}}{\sqrt{-c_1^2+4}}\right)$$ software 2: $$0=-c_2-\ln(x)-\frac{c_1}{\sqrt{c_1^2+4}}\tanh^{-1}\left(\frac{c_1x+2y}{x\sqrt{c_1^2+4}}\right)-\frac{1}{2}\ln\left(\frac{c_1xy-x^2+y^2}{x^2}\right)$$ I have not attempted to verify the solution from software 1 yet, but have done some work on software 2. I first used software 2 to try to solve for y, to substitute the expression for y directly into the ordinary differential equation.  The result was the following: I believe that this output is ambiguous, since there are essentially two equations that are supposed to be equated to zero I am not sure if it is possible to solve for y, and hence to check the validity of this solution using this method. I then did some reading on the internet, and it was suggested to, in this case, take the second implicit derivative with respect to x, then simplify. I tried to do this with math software 2, and the result was, after simplifying: $$\frac{d^2y}{dx^2}=\frac{c_1xy-x^2+y^2}{y^3}$$ I did some hand calculations, and it seems that software 2 simplifies the result before calculating the next derivative, even without using the simplify command. Considering this, I used the software to take the first derivative implicitly, then wrote out the equation in full, put that equation into a different form than the software output, and calculated the second derivative implicitly by hand, treating derivatives as functions of x for operations such as the product rule. The equation I calculated did not match the original differential equation. Software 2 has a function called odetest, which is supposed to verify that a function is a solution to an ordinary differential equation.  If you use odetest on this solution, the returned result is zero, implying that the function is a solution. The problem is that odetest does not show steps.  I contacted the company and asked to see the steps for this calculation, but they would not provide the steps. Are there any other ways to verify implicit solutions to an ordinary differential equation?","I am studying the nonlinear ordinary differential equation I have entered this equation into two different math software packages, and they produce different answers. software 1: software 2: I have not attempted to verify the solution from software 1 yet, but have done some work on software 2. I first used software 2 to try to solve for y, to substitute the expression for y directly into the ordinary differential equation.  The result was the following: I believe that this output is ambiguous, since there are essentially two equations that are supposed to be equated to zero I am not sure if it is possible to solve for y, and hence to check the validity of this solution using this method. I then did some reading on the internet, and it was suggested to, in this case, take the second implicit derivative with respect to x, then simplify. I tried to do this with math software 2, and the result was, after simplifying: I did some hand calculations, and it seems that software 2 simplifies the result before calculating the next derivative, even without using the simplify command. Considering this, I used the software to take the first derivative implicitly, then wrote out the equation in full, put that equation into a different form than the software output, and calculated the second derivative implicitly by hand, treating derivatives as functions of x for operations such as the product rule. The equation I calculated did not match the original differential equation. Software 2 has a function called odetest, which is supposed to verify that a function is a solution to an ordinary differential equation.  If you use odetest on this solution, the returned result is zero, implying that the function is a solution. The problem is that odetest does not show steps.  I contacted the company and asked to see the steps for this calculation, but they would not provide the steps. Are there any other ways to verify implicit solutions to an ordinary differential equation?",\frac{d^2y}{dx^2}=\frac{1}{y}-\frac{x}{y^2}\frac{dy}{dx} 0=c_2-\ln(x)-\frac{1}{2}\ln\left(-\frac{c_1y}{x}-\frac{y^2}{x^2}+1\right)-\frac{c_1}{\sqrt{-c_1^2+4}}\tan^{-1}\left(\frac{c_1+\frac{2y}{x}}{\sqrt{-c_1^2+4}}\right) 0=-c_2-\ln(x)-\frac{c_1}{\sqrt{c_1^2+4}}\tanh^{-1}\left(\frac{c_1x+2y}{x\sqrt{c_1^2+4}}\right)-\frac{1}{2}\ln\left(\frac{c_1xy-x^2+y^2}{x^2}\right) \frac{d^2y}{dx^2}=\frac{c_1xy-x^2+y^2}{y^3},"['ordinary-differential-equations', 'derivatives', 'implicit-differentiation']"
6,Constants and superposition principle in PDEs,Constants and superposition principle in PDEs,,"Given this partial differential equation: $$ x \frac{\partial u}{\partial x} -\frac 12 y\frac { \partial u}{\partial y} = 0$$ I would like to first find the general solution and then apply the boundary condition: $u(1,y) = 1+ \sin y$ . I have used a separation of variables technique, and have gotten two ODE's : $$\frac 1X dX = \frac \lambda x dx$$ and $$\frac 1Y dY = \frac{2\lambda}y dy $$ So one solution will be: $u(x,y) = Cx^\lambda y^{2 \lambda} $ . I am confused when using the superposition which states that I can sum up all the solutions because lambda can take any value (even a complex number). I don't understand if this means I have to sum over lambda like so: $$u(x,y) = \sum C_{\lambda} x^\lambda y^{2\lambda } $$ or integrate because lambda is a technically a continuous variable: $$u(x,y) = \int_{-\infty}^{+\infty} C_{\lambda} x^\lambda y^{2\lambda } d \lambda$$ I feel like using the sum would be easier to solve the boundary condition problem as I could equate the summand to the summand in the Taylor series for $1+ \sin x $ . If the integral is the right way to proceed, how do I go about evaluating this integral, and how will I be able to solve the BCP from there? Any help would be great.","Given this partial differential equation: I would like to first find the general solution and then apply the boundary condition: . I have used a separation of variables technique, and have gotten two ODE's : and So one solution will be: . I am confused when using the superposition which states that I can sum up all the solutions because lambda can take any value (even a complex number). I don't understand if this means I have to sum over lambda like so: or integrate because lambda is a technically a continuous variable: I feel like using the sum would be easier to solve the boundary condition problem as I could equate the summand to the summand in the Taylor series for . If the integral is the right way to proceed, how do I go about evaluating this integral, and how will I be able to solve the BCP from there? Any help would be great."," x \frac{\partial u}{\partial x} -\frac 12 y\frac { \partial u}{\partial y} = 0 u(1,y) = 1+ \sin y \frac 1X dX = \frac \lambda x dx \frac 1Y dY = \frac{2\lambda}y dy  u(x,y) = Cx^\lambda y^{2 \lambda}  u(x,y) = \sum C_{\lambda} x^\lambda y^{2\lambda }  u(x,y) = \int_{-\infty}^{+\infty} C_{\lambda} x^\lambda y^{2\lambda } d \lambda 1+ \sin x ","['ordinary-differential-equations', 'partial-differential-equations', 'boundary-value-problem']"
7,How can I convolve $\mathrm{e}^{-\mid t \mid }$ by $\mathrm{e}^{-t^2}$,How can I convolve  by,\mathrm{e}^{-\mid t \mid } \mathrm{e}^{-t^2},I have to solve the following differential equation using Fourier Transform: $$ -f''(t) +f(t) = \mathrm{e}^{-t^2} $$ I will use the following convention for the Fourier Transform formula and the inversion formula : $$ F(\xi)=\int_{R}{f(t)\mathrm{e}^{-2i\pi\xi t}dt}  $$ $$ f(-t) = \int_{R}{F(\xi)\mathrm{e}^{-2i\pi\xi t}d\xi} $$ This is what I have: $$ -(2i\pi\xi)^2F(\xi) + F(\xi) = \mathscr{F}[\mathrm{e}^{-t^2}] \\  F(\xi) = \frac{1}{1 + 4\pi^2 \xi^2} \mathscr{F}[\mathrm{e}^{-t^2}]$$ I noticed that $ \frac{1}{1 - 4\pi^2 \xi^2}$ is the Fourier transform of $\frac{1}{2}\mathrm{e}^{-\mid t \mid} $ . $$ f(-t) = \frac{1}{2}\mathrm{e}^{- \mid x \mid} *\mathrm{e}^{-x^2}\\  f(-t) = \frac{1}{2}\int_{-\infty}^{+\infty}{\mathrm{e}^{-\mid t-x \mid}\mathrm{e}^{-x^2} dx}\\  f(-t) = \frac{1}{2}\int_{-\infty}^{+\infty}{\mathrm{e}^{-x^2 -\mid t-x \mid} dx} \\  f(t) = \frac{1}{2} \int_{-\infty}^{+\infty}{\mathrm{e}^{-x^2 -\mid -t-x \mid}dx} \\  f(t) = \frac{1}{2} \int_{-\infty}^{+\infty}{\mathrm{e}^{-x^2 -(t+x)} dx} \\  f(t) = \frac{1}{2} \mathrm{e}^{-t} \int_{-\infty}^{+\infty}{ \mathrm{e}^{-x^2 -x} dx}$$ I'm stuck here because I don't know how to integrate this function. Any tips to do it ?,I have to solve the following differential equation using Fourier Transform: I will use the following convention for the Fourier Transform formula and the inversion formula : This is what I have: I noticed that is the Fourier transform of . I'm stuck here because I don't know how to integrate this function. Any tips to do it ?," -f''(t) +f(t) = \mathrm{e}^{-t^2}   F(\xi)=\int_{R}{f(t)\mathrm{e}^{-2i\pi\xi t}dt}    f(-t) = \int_{R}{F(\xi)\mathrm{e}^{-2i\pi\xi t}d\xi}   -(2i\pi\xi)^2F(\xi) + F(\xi) = \mathscr{F}[\mathrm{e}^{-t^2}] \\
 F(\xi) = \frac{1}{1 + 4\pi^2 \xi^2} \mathscr{F}[\mathrm{e}^{-t^2}]  \frac{1}{1 - 4\pi^2 \xi^2} \frac{1}{2}\mathrm{e}^{-\mid t \mid}   f(-t) = \frac{1}{2}\mathrm{e}^{- \mid x \mid} *\mathrm{e}^{-x^2}\\
 f(-t) = \frac{1}{2}\int_{-\infty}^{+\infty}{\mathrm{e}^{-\mid t-x \mid}\mathrm{e}^{-x^2} dx}\\
 f(-t) = \frac{1}{2}\int_{-\infty}^{+\infty}{\mathrm{e}^{-x^2 -\mid t-x \mid} dx} \\
 f(t) = \frac{1}{2} \int_{-\infty}^{+\infty}{\mathrm{e}^{-x^2 -\mid -t-x \mid}dx} \\
 f(t) = \frac{1}{2} \int_{-\infty}^{+\infty}{\mathrm{e}^{-x^2 -(t+x)} dx} \\
 f(t) = \frac{1}{2} \mathrm{e}^{-t} \int_{-\infty}^{+\infty}{ \mathrm{e}^{-x^2 -x} dx}","['ordinary-differential-equations', 'fourier-analysis']"
8,Difference equation corresponds to $y''=-y$,Difference equation corresponds to,y''=-y,"Convert the differential equation $y''=-y$ into a difference equation using the leapfrog method. My reference says: $$ \begin{bmatrix}1&0\\\Delta t&1\end{bmatrix}\begin{bmatrix}Y_{n+1}\\Y_{n+1}^{'}\end{bmatrix}=\begin{bmatrix}1&\Delta t\\0&1\end{bmatrix}\begin{bmatrix}Y_{n}\\Y_{n}^{'}\end{bmatrix}\implies \begin{bmatrix}Y_{n+1}\\Y_{n+1}^{'}\end{bmatrix}=\begin{bmatrix}1&\Delta t\\-\Delta t&1-(\Delta t)^2\end{bmatrix}\begin{bmatrix}Y_{n}\\Y_{n}^{'}\end{bmatrix}\\ Y_{n+1}=Y_n+\Delta t.Y_n^{'}\implies Y_{n+1}-Y_n=\Delta t.Y_n^{'} \\ \Delta t.Y_{n+1}+Y_{n+1}^{'}=Y_n^{'} \implies Y_n^{'}-Y_{n+1}^{'}=\Delta t.Y_{n+1} $$ Looks like the first equation is obtained by taking forward difference and later by backward. How do I justify such a choice ? My Attempt $$ Y_n^{'}=\frac{Y_{n+1}-Y_{n-1}}{2\Delta t}\\ Y_{n}^{''}=\frac{Y_{n+1}^{'}-Y_{n-1}^{'}}{2\Delta t}=\frac{\frac{Y_{n+2}-Y_{n}}{2\Delta t}-\frac{Y_{n}-Y_{n-2}}{2\Delta t}}{2\Delta t}=\frac{Y_{n+2}-2Y_{n}+Y_{n-2}}{(2\Delta t)^2}=\frac{Y_{n+1}-2Y_{n}+Y_{n-1}}{(\Delta t)^2}=-Y_{n}\\ \boxed{Y_{n+1}=Y_{n-1}+2\Delta t.Y_{n}^{'}\\ Y_{n+1}^{'}=Y_{n-1}^{'}-2\Delta t.Y_{n}} $$ $$ Y_{n-1}=(2-(\Delta t)^2)Y_n-Y_{n+1}\implies Y_{n+1}=(2-(\Delta t)^2)Y_n-Y_{n+1}+2\Delta t.Y_{n}^{'}\\ \boxed{Y_{n+1}=\frac{(2-(\Delta t)^2)}{2}Y_n+\Delta t.Y_n^{'}} $$ How do I approach this and obtain the matrix difference equation corresponds to $y''+y=0$ using midpoint differences ? Reference: Example 3-Difference Equations (Page 323) and Q.28 Problem Set 6.3 (Page 335), Chapter 6 : Eigenvalues and Eigenvectors , Introduction to Linear Algebra, Gilbert Strang, Fifth Edition (2016)","Convert the differential equation into a difference equation using the leapfrog method. My reference says: Looks like the first equation is obtained by taking forward difference and later by backward. How do I justify such a choice ? My Attempt How do I approach this and obtain the matrix difference equation corresponds to using midpoint differences ? Reference: Example 3-Difference Equations (Page 323) and Q.28 Problem Set 6.3 (Page 335), Chapter 6 : Eigenvalues and Eigenvectors , Introduction to Linear Algebra, Gilbert Strang, Fifth Edition (2016)","y''=-y 
\begin{bmatrix}1&0\\\Delta t&1\end{bmatrix}\begin{bmatrix}Y_{n+1}\\Y_{n+1}^{'}\end{bmatrix}=\begin{bmatrix}1&\Delta t\\0&1\end{bmatrix}\begin{bmatrix}Y_{n}\\Y_{n}^{'}\end{bmatrix}\implies \begin{bmatrix}Y_{n+1}\\Y_{n+1}^{'}\end{bmatrix}=\begin{bmatrix}1&\Delta t\\-\Delta t&1-(\Delta t)^2\end{bmatrix}\begin{bmatrix}Y_{n}\\Y_{n}^{'}\end{bmatrix}\\
Y_{n+1}=Y_n+\Delta t.Y_n^{'}\implies Y_{n+1}-Y_n=\Delta t.Y_n^{'} \\
\Delta t.Y_{n+1}+Y_{n+1}^{'}=Y_n^{'} \implies Y_n^{'}-Y_{n+1}^{'}=\Delta t.Y_{n+1}
 
Y_n^{'}=\frac{Y_{n+1}-Y_{n-1}}{2\Delta t}\\
Y_{n}^{''}=\frac{Y_{n+1}^{'}-Y_{n-1}^{'}}{2\Delta t}=\frac{\frac{Y_{n+2}-Y_{n}}{2\Delta t}-\frac{Y_{n}-Y_{n-2}}{2\Delta t}}{2\Delta t}=\frac{Y_{n+2}-2Y_{n}+Y_{n-2}}{(2\Delta t)^2}=\frac{Y_{n+1}-2Y_{n}+Y_{n-1}}{(\Delta t)^2}=-Y_{n}\\
\boxed{Y_{n+1}=Y_{n-1}+2\Delta t.Y_{n}^{'}\\
Y_{n+1}^{'}=Y_{n-1}^{'}-2\Delta t.Y_{n}}
 
Y_{n-1}=(2-(\Delta t)^2)Y_n-Y_{n+1}\implies Y_{n+1}=(2-(\Delta t)^2)Y_n-Y_{n+1}+2\Delta t.Y_{n}^{'}\\
\boxed{Y_{n+1}=\frac{(2-(\Delta t)^2)}{2}Y_n+\Delta t.Y_n^{'}}
 y''+y=0","['linear-algebra', 'ordinary-differential-equations', 'numerical-methods', 'recurrence-relations', 'finite-differences']"
9,$dh/dt$ given $dV/dt$,given,dh/dt dV/dt,"I have a textbook problem whose answer is different from the one I got. The problem goes: A conical egg timer is letting sand through from top to bottom at a rate of $0.02\,\mathrm{cm^3s^{-1}}$ . find an expression for the rate of change of height ${\dfrac{dh}{dt}}$ . (There is a diagram showing the full height of the conical egg timer as $5\,\text{cm}$ and the radius of its base as $2\,\text{cm}$ . The value of $h$ is the height of the sand in the conical egg timer.) My Attempt: What I understand from this question is that: The conical egg timer is an upside-down cone, so its volume is ${V={\frac{1}{3}{\pi}r^{2}h}}$ , which is also the same as the volume of the sand within it at the beginning. The rate of change of volume of the sand within the conical egg timer with respect to time is ${\dfrac{dV}{dt}}=-0.02$ . The diagram shows the radius at the base of the conical egg timer (or ""top"" since it is upside-down) to be ${2}$ and its full height from base to ""point"" to be ${5}$ . The sand exits from the ""point"" so its height decreases from its base (the top). The question is asking me to find the rate of change of the height of the sand within the timer with respect to time, or ${\dfrac{dh}{dt}}$ . Given this information, I attempted to answer the problem: If I am correct, the values given at the start are: ${\dfrac{dV}{dt}}=-0.02$ and (at the very beginning, when the cone is full of sand, assuming it starts full) $h=5$ and $r=2$ . First, I differentiated the volume ${V}$ with respect to ${h}$ , giving ${\dfrac{dV}{dh}={\frac{1}{3}}{\pi}{r^{2}}}$ (using the power rule for differentiation) . $\to {\dfrac{dV}{dh}={\frac{1}{3}}{\pi}{r^{2}}}$ Secondly, I used the relationship of the derivative of a function and the derivative of its inverse to get ${\dfrac{dh}{dV}}={\dfrac{1}{\dfrac{dV}{dh}}}={\dfrac{1}{\frac{1}{3}{\pi}r^{2}}}={\dfrac{3}{{\pi}r^{2}}}$ (using the inverse function theorem (I don't fully understand why derivatives of inverses are reciprocals to one another but that's the rule I used)) . $\to {\dfrac{dh}{dV}}={\dfrac{3}{{\pi}r^{2}}}$ Thirdly, I multiplied ${\dfrac{dV}{dt}}$ and ${\dfrac{dh}{dV}}$ to get ${\dfrac{dh}{dt}}=-0.02 \times{\dfrac{3}{{\pi}r^{2}}}={\dfrac{-0.06}{{\pi}r^{2}}}={\dfrac{-3}{50{\pi}r^{2}}}$ (using the chain rule for differentiation) . $\to {\dfrac{dh}{dt}}={\dfrac{-3}{50{\pi}r^{2}}}$ Then, I used the initial values of the height and radius of the sand in the cone to write ${\dfrac{dh}{dt}}$ in terms of $h$ . The initial values of $h$ and $r$ are the values found in the diagram: $h=5$ and $r=2$ $\to {\dfrac{dh}{dt}}={\dfrac{-3}{50{\pi}r^{2}}}={\dfrac{-3}{50{\pi}({2})^{2}}}={\dfrac{-3}{200{\pi}}}={\dfrac{-3}{40h{\pi}}}={\dfrac{-3}{8{h^{2}}{\pi}}}={\dfrac{-3}{8{\pi}h^{2}}}$ (replacing $r$ in the equation with $2$ and then replacing every multiple of $5$ in the equation with $h$ ) . $\to$ my answer is ${\dfrac{dh}{dt}}={\dfrac{-3}{8{\pi}h^{2}}}$ . C) The answer given in the textbook however, is ${\dfrac{dh}{dt}}={\dfrac{-1}{8{\pi}h^{2}}}$ . Question: I'd like to ask for help with regards to finding where I made my error(s), so I hope my working is easy to follow- I am also unsure about whether this is the correct way to answer the question in the first place - thanks.","I have a textbook problem whose answer is different from the one I got. The problem goes: A conical egg timer is letting sand through from top to bottom at a rate of . find an expression for the rate of change of height . (There is a diagram showing the full height of the conical egg timer as and the radius of its base as . The value of is the height of the sand in the conical egg timer.) My Attempt: What I understand from this question is that: The conical egg timer is an upside-down cone, so its volume is , which is also the same as the volume of the sand within it at the beginning. The rate of change of volume of the sand within the conical egg timer with respect to time is . The diagram shows the radius at the base of the conical egg timer (or ""top"" since it is upside-down) to be and its full height from base to ""point"" to be . The sand exits from the ""point"" so its height decreases from its base (the top). The question is asking me to find the rate of change of the height of the sand within the timer with respect to time, or . Given this information, I attempted to answer the problem: If I am correct, the values given at the start are: and (at the very beginning, when the cone is full of sand, assuming it starts full) and . First, I differentiated the volume with respect to , giving (using the power rule for differentiation) . Secondly, I used the relationship of the derivative of a function and the derivative of its inverse to get (using the inverse function theorem (I don't fully understand why derivatives of inverses are reciprocals to one another but that's the rule I used)) . Thirdly, I multiplied and to get (using the chain rule for differentiation) . Then, I used the initial values of the height and radius of the sand in the cone to write in terms of . The initial values of and are the values found in the diagram: and (replacing in the equation with and then replacing every multiple of in the equation with ) . my answer is . C) The answer given in the textbook however, is . Question: I'd like to ask for help with regards to finding where I made my error(s), so I hope my working is easy to follow- I am also unsure about whether this is the correct way to answer the question in the first place - thanks.","0.02\,\mathrm{cm^3s^{-1}} {\dfrac{dh}{dt}} 5\,\text{cm} 2\,\text{cm} h {V={\frac{1}{3}{\pi}r^{2}h}} {\dfrac{dV}{dt}}=-0.02 {2} {5} {\dfrac{dh}{dt}} {\dfrac{dV}{dt}}=-0.02 h=5 r=2 {V} {h} {\dfrac{dV}{dh}={\frac{1}{3}}{\pi}{r^{2}}} \to {\dfrac{dV}{dh}={\frac{1}{3}}{\pi}{r^{2}}} {\dfrac{dh}{dV}}={\dfrac{1}{\dfrac{dV}{dh}}}={\dfrac{1}{\frac{1}{3}{\pi}r^{2}}}={\dfrac{3}{{\pi}r^{2}}} \to {\dfrac{dh}{dV}}={\dfrac{3}{{\pi}r^{2}}} {\dfrac{dV}{dt}} {\dfrac{dh}{dV}} {\dfrac{dh}{dt}}=-0.02 \times{\dfrac{3}{{\pi}r^{2}}}={\dfrac{-0.06}{{\pi}r^{2}}}={\dfrac{-3}{50{\pi}r^{2}}} \to {\dfrac{dh}{dt}}={\dfrac{-3}{50{\pi}r^{2}}} {\dfrac{dh}{dt}} h h r h=5 r=2 \to {\dfrac{dh}{dt}}={\dfrac{-3}{50{\pi}r^{2}}}={\dfrac{-3}{50{\pi}({2})^{2}}}={\dfrac{-3}{200{\pi}}}={\dfrac{-3}{40h{\pi}}}={\dfrac{-3}{8{h^{2}}{\pi}}}={\dfrac{-3}{8{\pi}h^{2}}} r 2 5 h \to {\dfrac{dh}{dt}}={\dfrac{-3}{8{\pi}h^{2}}} {\dfrac{dh}{dt}}={\dfrac{-1}{8{\pi}h^{2}}}",['ordinary-differential-equations']
10,Dynamical system and level curves [closed],Dynamical system and level curves [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Is it possible to design a function $f(x, z) = 0$ from a dynamical system $\dot{x} = a(x)$ such that the trace of trajectories are the level curves of $f$ , i.e. $f(x, c) = 0$ , $c \in \mathbb{R}$ ?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Is it possible to design a function from a dynamical system such that the trace of trajectories are the level curves of , i.e. , ?","f(x, z) = 0 \dot{x} = a(x) f f(x, c) = 0 c \in \mathbb{R}","['ordinary-differential-equations', 'multivariable-calculus', 'differential-geometry', 'dynamical-systems']"
11,The Effect of Small Variations in the Coefficients of a Linear Differential System,The Effect of Small Variations in the Coefficients of a Linear Differential System,,"In E.L. Ince's Ordinary Differential Equations , page $219$ , the effect of small variations in the coefficients of a linear differential system is examined. In particular, the main result of the section is that the ""index"" of the system is not raised by any variation of the coefficients which is ""uniformly small."" The result above concerns the ""index"" of the system, but in general, the question of the effect of small variations in the coefficients of a differential equation sounds interesting and, despite going over a couple of texts, modern and old, I don't see anyone discussing about it. I have one specific question in mind: Suppose we focus on just ordinary linear differential equations and one (or maybe all) of the terms is differentiably parametrized (i.e. the partial derivative of the term w.r.t. the parameter exists - or any other way you want to interpret it). Is the solution (now a two-variable function) differentiable w.r.t. the parameter as well? Maybe the answer to this question can be found in PDE's, but I hardly know any PDEs sensibly. In general, does anyone know of a good text (preferably a modern one) that addresses such a question? Thanks! Note : By the way, I am not sure if this kind of question is allowed here, but judging from how this topic evades internet searches, if anyone is interested to look into the problem deeper and from other angles (e.g. computational, applied), I would like to do the work together - preferably under a formal position such as a postdoc.","In E.L. Ince's Ordinary Differential Equations , page , the effect of small variations in the coefficients of a linear differential system is examined. In particular, the main result of the section is that the ""index"" of the system is not raised by any variation of the coefficients which is ""uniformly small."" The result above concerns the ""index"" of the system, but in general, the question of the effect of small variations in the coefficients of a differential equation sounds interesting and, despite going over a couple of texts, modern and old, I don't see anyone discussing about it. I have one specific question in mind: Suppose we focus on just ordinary linear differential equations and one (or maybe all) of the terms is differentiably parametrized (i.e. the partial derivative of the term w.r.t. the parameter exists - or any other way you want to interpret it). Is the solution (now a two-variable function) differentiable w.r.t. the parameter as well? Maybe the answer to this question can be found in PDE's, but I hardly know any PDEs sensibly. In general, does anyone know of a good text (preferably a modern one) that addresses such a question? Thanks! Note : By the way, I am not sure if this kind of question is allowed here, but judging from how this topic evades internet searches, if anyone is interested to look into the problem deeper and from other angles (e.g. computational, applied), I would like to do the work together - preferably under a formal position such as a postdoc.",219,"['ordinary-differential-equations', 'partial-differential-equations', 'stability-in-odes']"
12,Solve ODE $yy'y''=(y')^3+(y'')^2$,Solve ODE,yy'y''=(y')^3+(y'')^2,"Solve ODE $yy'y''=(y')^3+(y'')^2$ . Attempt . $y=0$ is clearly a solution. If $y\neq 0$ , then ODE becomes $$y=\frac{(y')^2}{y''}+\frac{y''}{y'}.$$ Let $y'=u$ , so by differentiating with respect to variable $x$ we derive an equation of the form $y'=f(y',y'',y''')$ , i.e. $u=f(u,u',u'')$ (non linear, i guess). Am I on the right path? Is there an algorithm for solving this category (if so) of equations? Thanks in advance for the help.","Solve ODE . Attempt . is clearly a solution. If , then ODE becomes Let , so by differentiating with respect to variable we derive an equation of the form , i.e. (non linear, i guess). Am I on the right path? Is there an algorithm for solving this category (if so) of equations? Thanks in advance for the help.","yy'y''=(y')^3+(y'')^2 y=0 y\neq 0 y=\frac{(y')^2}{y''}+\frac{y''}{y'}. y'=u x y'=f(y',y'',y''') u=f(u,u',u'')",['ordinary-differential-equations']
13,Qualitative study of a second order Cauchy problem,Qualitative study of a second order Cauchy problem,,"I need a check on the following exercise: Consider the following Cauchy problem: \begin{cases}  y''(x)=y'(x)^2 - 2 \\  y(0)=0 \\  y'(0) = 1  \end{cases} i) Show the solution is defined for all $x \in \mathbb{R}$ ii)  Compute $\lim_{x \rightarrow +\infty} y'(x)$ and $\lim_{x \rightarrow +\infty} y(x)$ My attempt: i) I recast everything to the first order, hence I define the vector function $$F(t,y,y')=[y'^2-2,y']^T$$ I would like to prove sublinearity in order to show that the solution is globally defined: $$||F(t,y,y')|| \leq h + k ||[y,y']||$$ Using the expression for $F$ : $$y'^4 - 3 y'^2 +4$$ but I don't know how to find a sublinearity condition here: I should bound the latter expression with $y^2 + y'^2$ So, I notice that the function $F=[F_1,F_2]$ is such that $\partial_y F_1 = \partial_y F_2 = 0$ and $\partial_{y'}F_1 = 1$ and $\partial_{y'}F_2 = 2y'$ . This means that $F$ il globally Lipschitz , so in principle, existence and uniqueness could be applied iteratively and define a solution for every $x \in \mathbb{R}$ . IS THERE A WAY TO SHOW IT WITH SUBLINEARITY? ii) Here I note that, after the reduction to the first order, I have (call $y'=z$ ) the ODE $$z' = z^2-2$$ with $z(0)=1$ . By existence and uniquess, and using the stationary solutions $y=\pm \sqrt{2}$ , I have that $z$ starts from $1$ and then it decreases. The limit must exists, since the solution is defined on the whole $\mathbb{R}$ and it's monotone. Then $$\lim_{x \rightarrow \infty} z(x)= \lim_{x \rightarrow \infty} y'(x)= -\sqrt{2}$$ To compute $$\lim_{x \rightarrow \infty} y(x)$$ I note that $y'(x)=z(x)$ , and if it would be finite , then $$\lim_{x \rightarrow \infty} y'(x) = 0$$ But this limit is precisely the one I have just copmuted, i.e. $-\sqrt{2}$ , therefore this limit must be $+\infty$ or $-\infty$ . Since $y'(x)=z(x)$ and $z(x)$ is monotonically decreasing, then this limit must be $-\infty$ . Is eveything okay?","I need a check on the following exercise: Consider the following Cauchy problem: i) Show the solution is defined for all ii)  Compute and My attempt: i) I recast everything to the first order, hence I define the vector function I would like to prove sublinearity in order to show that the solution is globally defined: Using the expression for : but I don't know how to find a sublinearity condition here: I should bound the latter expression with So, I notice that the function is such that and and . This means that il globally Lipschitz , so in principle, existence and uniqueness could be applied iteratively and define a solution for every . IS THERE A WAY TO SHOW IT WITH SUBLINEARITY? ii) Here I note that, after the reduction to the first order, I have (call ) the ODE with . By existence and uniquess, and using the stationary solutions , I have that starts from and then it decreases. The limit must exists, since the solution is defined on the whole and it's monotone. Then To compute I note that , and if it would be finite , then But this limit is precisely the one I have just copmuted, i.e. , therefore this limit must be or . Since and is monotonically decreasing, then this limit must be . Is eveything okay?","\begin{cases}
 y''(x)=y'(x)^2 - 2 \\
 y(0)=0 \\
 y'(0) = 1
 \end{cases} x \in \mathbb{R} \lim_{x \rightarrow +\infty} y'(x) \lim_{x \rightarrow +\infty} y(x) F(t,y,y')=[y'^2-2,y']^T ||F(t,y,y')|| \leq h + k ||[y,y']|| F y'^4 - 3 y'^2 +4 y^2 + y'^2 F=[F_1,F_2] \partial_y F_1 = \partial_y F_2 = 0 \partial_{y'}F_1 = 1 \partial_{y'}F_2 = 2y' F x \in \mathbb{R} y'=z z' = z^2-2 z(0)=1 y=\pm \sqrt{2} z 1 \mathbb{R} \lim_{x \rightarrow \infty} z(x)= \lim_{x \rightarrow \infty} y'(x)= -\sqrt{2} \lim_{x \rightarrow \infty} y(x) y'(x)=z(x) \lim_{x \rightarrow \infty} y'(x) = 0 -\sqrt{2} +\infty -\infty y'(x)=z(x) z(x) -\infty","['real-analysis', 'ordinary-differential-equations', 'solution-verification', 'cauchy-problem']"
14,A non-linear integrable differential equation,A non-linear integrable differential equation,,"I'm trying to solve a question from a tutorial on mathematics for physicst which was never done due to the pandemic so I don't know the answer or a proper method of solving it. Nevertheless here is the question and my attempt at solving it. Feedback, suggestions on how to approach it and further readings recommendations would be hugely appreciated. Let the equation of movement be: $$m\ddot{x}(t) + V'(x(t))= 0\tag1$$ and, $$E = \frac{m}{2}\left(\dot{x}(t)\right)^2 + V(x(t))\tag2$$ where $V(x)$ is a known derivable potential and $E$ is independent of $t$ . By integration of the equation giving $\dot{x}$ , Express the solution with the initial condition $x(t_0)=x_0$ in the form of t(x). From the equation $(1)$ $$\dot{x}^2 = \frac{E-V}{m/2} \implies \pm\int_\left(x_0\right)^x\sqrt{\frac{m/2}{E-V}}dx = t+Cste$$ Taking the positive root and from the initial condition we know $Cste=-t_0$ $$t(x)=\int_\left(x_0\right)^x\sqrt{\frac{m/2}{E-V}}dx+t_0$$ 2. Let a increasing potential at infinity be: $$V(x\rightarrow\infty)= \frac{-C}{x^\left(2a\right)}$$ where $C>0$ and $a>0$ . We consider a particle of initial velocity $v_0>0$ . Give the asymptotic behavior of $x(t)$ when $E>0$ and $E=0$ . I tried substituting the expression of $V(x)$ at infinity in the integral: $$t(x)=\int_\left(x_0\right)^x\sqrt{\frac{m/2}{E+\frac{C}{x^\left(2a\right)}}}dx+t_0$$ I was trying to convert it in the form of $\arcsin(x)+c=\int\frac{1}{\sqrt{1-x^2}}dx$ by substitution but it has become apparent to me that it isn't possible perhaps I'm not allowed to directly substitute the expression of $V(x)$ at infinity. I also think there is a way around this question without having to compute the integral but i can't seem to find one. Hope somebody can help me.","I'm trying to solve a question from a tutorial on mathematics for physicst which was never done due to the pandemic so I don't know the answer or a proper method of solving it. Nevertheless here is the question and my attempt at solving it. Feedback, suggestions on how to approach it and further readings recommendations would be hugely appreciated. Let the equation of movement be: and, where is a known derivable potential and is independent of . By integration of the equation giving , Express the solution with the initial condition in the form of t(x). From the equation Taking the positive root and from the initial condition we know 2. Let a increasing potential at infinity be: where and . We consider a particle of initial velocity . Give the asymptotic behavior of when and . I tried substituting the expression of at infinity in the integral: I was trying to convert it in the form of by substitution but it has become apparent to me that it isn't possible perhaps I'm not allowed to directly substitute the expression of at infinity. I also think there is a way around this question without having to compute the integral but i can't seem to find one. Hope somebody can help me.","m\ddot{x}(t) + V'(x(t))= 0\tag1 E = \frac{m}{2}\left(\dot{x}(t)\right)^2 + V(x(t))\tag2 V(x) E t \dot{x} x(t_0)=x_0 (1) \dot{x}^2 = \frac{E-V}{m/2} \implies
\pm\int_\left(x_0\right)^x\sqrt{\frac{m/2}{E-V}}dx = t+Cste Cste=-t_0 t(x)=\int_\left(x_0\right)^x\sqrt{\frac{m/2}{E-V}}dx+t_0 V(x\rightarrow\infty)= \frac{-C}{x^\left(2a\right)} C>0 a>0 v_0>0 x(t) E>0 E=0 V(x) t(x)=\int_\left(x_0\right)^x\sqrt{\frac{m/2}{E+\frac{C}{x^\left(2a\right)}}}dx+t_0 \arcsin(x)+c=\int\frac{1}{\sqrt{1-x^2}}dx V(x)","['ordinary-differential-equations', 'nonlinear-analysis']"
15,Why is the following method of finding out a conserved quantity wrong?,Why is the following method of finding out a conserved quantity wrong?,,"Let a system be defined by $\dot{x}=y; \dot{y}=f(x).$ And let $E(x,y)$ be a conserved quantity of the system. Then $$ \frac{\partial{E}}{\partial{x}}\dot{x}+\frac{\partial{E}}{\partial{y}}\dot{y}=0. $$ My question is why cannot I just rewrite this equation as $$ \frac{\frac{\partial{E}}{\partial{x}}}{{\frac{\partial{E}}{\partial{y}}}}=\frac{-\dot{y}}{\dot{x}}. $$ $$ \frac{dy}{dx}=\frac{-\dot{y}}{\dot{x}}. $$ Now the separation of variables would easily give me a curve in $(x,y)$ space which can also be thought of as a conserved quantity. Looking at examples in Strogatz, I figured out that this  method is wrong, but I am unable to find out why?","Let a system be defined by And let be a conserved quantity of the system. Then My question is why cannot I just rewrite this equation as Now the separation of variables would easily give me a curve in space which can also be thought of as a conserved quantity. Looking at examples in Strogatz, I figured out that this  method is wrong, but I am unable to find out why?","\dot{x}=y; \dot{y}=f(x). E(x,y) 
\frac{\partial{E}}{\partial{x}}\dot{x}+\frac{\partial{E}}{\partial{y}}\dot{y}=0.
 
\frac{\frac{\partial{E}}{\partial{x}}}{{\frac{\partial{E}}{\partial{y}}}}=\frac{-\dot{y}}{\dot{x}}.
 
\frac{dy}{dx}=\frac{-\dot{y}}{\dot{x}}.
 (x,y)",['ordinary-differential-equations']
16,Critical Point Classification,Critical Point Classification,,"We're modeling competing species in my mathematical modeling class and we were taught the nullclines and direction arrows as a method to classify what critical points are and their stability. Usually, there are four directions arrows around each critical point that help determine whether it's a sink, a source, a saddle, or a center/spiral. But I just came across this case, where I have 6 direction arrows around one critical point and I have no idea how to go about classifying it. I tried to search online but I don't know if I'm not using the correct words or if there's just something wrong about this. The equation system we have is $\frac{dx}{dt} = x(a-bx-ky)$ $\frac{dy}{dt} = y(c-dy-\sigma x)$ and we're told to analyze what happens when $\frac{a}{k}=\frac{c}{d}$ Here's the graph I got. I can see that $(0,0)$ is a source and $(\frac{a}{b},0)$ is a saddle. Not sure what $(0,\frac{a}{k}=\frac{c}{d})$ is supposed to be or how to analyze it. The $\frac{dx}{dt}$ nullclines are in green, and the $\frac{dy}{dt}$ nullclines are in blue.","We're modeling competing species in my mathematical modeling class and we were taught the nullclines and direction arrows as a method to classify what critical points are and their stability. Usually, there are four directions arrows around each critical point that help determine whether it's a sink, a source, a saddle, or a center/spiral. But I just came across this case, where I have 6 direction arrows around one critical point and I have no idea how to go about classifying it. I tried to search online but I don't know if I'm not using the correct words or if there's just something wrong about this. The equation system we have is and we're told to analyze what happens when Here's the graph I got. I can see that is a source and is a saddle. Not sure what is supposed to be or how to analyze it. The nullclines are in green, and the nullclines are in blue.","\frac{dx}{dt} = x(a-bx-ky) \frac{dy}{dt} = y(c-dy-\sigma x) \frac{a}{k}=\frac{c}{d} (0,0) (\frac{a}{b},0) (0,\frac{a}{k}=\frac{c}{d}) \frac{dx}{dt} \frac{dy}{dt}",['ordinary-differential-equations']
17,How to represent the given information correctly to solve for a particular solution to a differential equation?,How to represent the given information correctly to solve for a particular solution to a differential equation?,,"Lupita's lawn is left unattended so that an infestation of weeds begins to take over. The rate of growth of the weeds is proportional to the area of lawn not yet invaded by weeds. a) If $W \space m^2$ is the area of lawn taken over by weeds after t weeks, and the lawn has a total area of $A \space m^2$ , write the differential equation that models the situation. b) The area taken over by weeds grows from one quarter to one half the total area of the lawn in $T$ weeks. If $t = 0$ when $W = \frac{1}{4}A$ , solve the differential euqation in part a). First I write down the differential equation, which is: $\frac{dW}{dt} = k(A - W)$ . I proceed and find the general solution for this differential equation which is: $W = A - Be^{-kt}$ for that I let $e^c = B$ . I continue this with the information I know to interpret; that is when $t = 0$ , $W = \frac{1}{4}A$ . This gives: $\frac{1}{4}A = A - Be^{-k(0)}$ $\frac{3}{4}A = B$ . Substituting this back to the equation: $W = A - \frac{3}{4}Ae^{-kt}$ Now this is where I am stuck. I am unsure how to represent ""The area taken over by weeds grows from one quarter to one half the total area of the lawn in $T$ weeks"" mathematically. I have the proportional constant $k$ left to solve. Do I have to do something like: $ \int _{\frac{1}{4}A}^{\frac{1}{2}A}\:\frac{1}{A-W}dW\:=\:\int _0^T\:k\:dt$ How can I solve for $k$ , hence the equation? FYI the answer is: $W\:=\:A\:-\frac{3}{4}Ae^{ln\left(\frac{2}{3}\right)t}$ Thanks.","Lupita's lawn is left unattended so that an infestation of weeds begins to take over. The rate of growth of the weeds is proportional to the area of lawn not yet invaded by weeds. a) If is the area of lawn taken over by weeds after t weeks, and the lawn has a total area of , write the differential equation that models the situation. b) The area taken over by weeds grows from one quarter to one half the total area of the lawn in weeks. If when , solve the differential euqation in part a). First I write down the differential equation, which is: . I proceed and find the general solution for this differential equation which is: for that I let . I continue this with the information I know to interpret; that is when , . This gives: . Substituting this back to the equation: Now this is where I am stuck. I am unsure how to represent ""The area taken over by weeds grows from one quarter to one half the total area of the lawn in weeks"" mathematically. I have the proportional constant left to solve. Do I have to do something like: How can I solve for , hence the equation? FYI the answer is: Thanks.",W \space m^2 A \space m^2 T t = 0 W = \frac{1}{4}A \frac{dW}{dt} = k(A - W) W = A - Be^{-kt} e^c = B t = 0 W = \frac{1}{4}A \frac{1}{4}A = A - Be^{-k(0)} \frac{3}{4}A = B W = A - \frac{3}{4}Ae^{-kt} T k  \int _{\frac{1}{4}A}^{\frac{1}{2}A}\:\frac{1}{A-W}dW\:=\:\int _0^T\:k\:dt k W\:=\:A\:-\frac{3}{4}Ae^{ln\left(\frac{2}{3}\right)t},"['calculus', 'ordinary-differential-equations']"
18,"Why should this result be true? If $f: [0,1] \to \mathbb{R}$ is differentiable, $g$ is continuous, and $f'(t) = g(f(t)),$ then $f$ is monotonic.","Why should this result be true? If  is differentiable,  is continuous, and  then  is monotonic.","f: [0,1] \to \mathbb{R} g f'(t) = g(f(t)), f","If $f: [0,1] \to \mathbb{R}$ is differentiable, $g$ is continuous, and $f'(t) = g(f(t)),$ then $f$ is monotonic. Example 1: Suppose $g(x) = x \Rightarrow f' = f.$ Then $f = ce^x$ which is indeed monotonic. Example 2: Suppose $g(x) = x^n \Rightarrow f' = f^n$ for $n \ne 1.$ Then $df/f^n = dx \Rightarrow x+C = f^{1-n}/(1-n) \Rightarrow f = ((1-n)x + C)^{1/(1-n)},$ monotonic again. It seems bizarre that $g$ can be anything we want, and yet the result will be true. Since $f$ is continuous, it suffices to prove $f$ is injective. So suppose $f(a) = f(b)$ with $a<b.$ Avenue 1: Then $f'(a) = g(f(a)) = g(f(b)) = f'(b).$ But what's next? This seems like a dead end. Avenue 2: There exists $c \in (a,b)$ such that $f'(c) = 0$ by Rolle's Theorem. Thus, $g(f(c)) = 0.$ But what's next? Looks like another dead end again. Is there a 3rd avenue? I've been looking for one. Until I find it, I cannot believe the result. Update: The answer here solves my question. The question also appeared previously on MSE , but I do not like the method used there since it seems to assume $f'$ is continuous.","If is differentiable, is continuous, and then is monotonic. Example 1: Suppose Then which is indeed monotonic. Example 2: Suppose for Then monotonic again. It seems bizarre that can be anything we want, and yet the result will be true. Since is continuous, it suffices to prove is injective. So suppose with Avenue 1: Then But what's next? This seems like a dead end. Avenue 2: There exists such that by Rolle's Theorem. Thus, But what's next? Looks like another dead end again. Is there a 3rd avenue? I've been looking for one. Until I find it, I cannot believe the result. Update: The answer here solves my question. The question also appeared previously on MSE , but I do not like the method used there since it seems to assume is continuous.","f: [0,1] \to \mathbb{R} g f'(t) = g(f(t)), f g(x) = x \Rightarrow f' = f. f = ce^x g(x) = x^n \Rightarrow f' = f^n n \ne 1. df/f^n = dx \Rightarrow x+C = f^{1-n}/(1-n) \Rightarrow f = ((1-n)x + C)^{1/(1-n)}, g f f f(a) = f(b) a<b. f'(a) = g(f(a)) = g(f(b)) = f'(b). c \in (a,b) f'(c) = 0 g(f(c)) = 0. f'","['calculus', 'ordinary-differential-equations']"
19,What should I read to understand the math behind waves?,What should I read to understand the math behind waves?,,"I'm learning differential equations and waves - following online courses and reading some textbooks - and I find that quite often, the use of Phasors, equations combining sinusoidal waves of different amplitudes and frequencies (the derivation of beats)...etc...they're explained really briefly, without a lot of diving deeper into them, and then applied. Is there a specific ""math"" which I could study, or textbook I could read, that presents these subjects more explicitly? The more geometric and intuitive the math book is, the better! I tried looking for ""wave mathematics"" but...found mostly really advanced books. Complex Analysis didn't seem to fit either... All and any recommendations are welcome, thank you so much! Edit: Books on the differential equations of waves are helpful...but, I think I'd like to start simpler. What led me to asking this question is that it took me a long time to understand the geometric intuition for how to combine waves of the same amplitude but different frequencies (for which this - https://www.jstor.org/stable/27965328?seq=1 - helped quite a bit) and I still don't understand at all how to combine them if they have different amplitudes! Is there not anything simpler than a differential equations approach, or pherhaps a book that DOES go into the differential equations, but also does so explaining each step, instead of skipping over the ""basic"" math of waves? Thanks again.","I'm learning differential equations and waves - following online courses and reading some textbooks - and I find that quite often, the use of Phasors, equations combining sinusoidal waves of different amplitudes and frequencies (the derivation of beats)...etc...they're explained really briefly, without a lot of diving deeper into them, and then applied. Is there a specific ""math"" which I could study, or textbook I could read, that presents these subjects more explicitly? The more geometric and intuitive the math book is, the better! I tried looking for ""wave mathematics"" but...found mostly really advanced books. Complex Analysis didn't seem to fit either... All and any recommendations are welcome, thank you so much! Edit: Books on the differential equations of waves are helpful...but, I think I'd like to start simpler. What led me to asking this question is that it took me a long time to understand the geometric intuition for how to combine waves of the same amplitude but different frequencies (for which this - https://www.jstor.org/stable/27965328?seq=1 - helped quite a bit) and I still don't understand at all how to combine them if they have different amplitudes! Is there not anything simpler than a differential equations approach, or pherhaps a book that DOES go into the differential equations, but also does so explaining each step, instead of skipping over the ""basic"" math of waves? Thanks again.",,"['ordinary-differential-equations', 'partial-differential-equations', 'reference-request', 'book-recommendation', 'wave-equation']"
20,"If $T_t$ is a diffeomorphism and $t\mapsto T_t(x)$ is differentiable, can we find a map $v$ with $v(t,T_t(x))=\frac{\partial T}{\partial t}(t,x)$?","If  is a diffeomorphism and  is differentiable, can we find a map  with ?","T_t t\mapsto T_t(x) v v(t,T_t(x))=\frac{\partial T}{\partial t}(t,x)","Let $d\in\mathbb N$ , $\tau>0$ , $U\subseteq\mathbb R^d$ be open and $T_t$ be a $C^1$ -diffeomorphism from $U$ onto an open subset of $\mathbb R^d$ for $t\in[0,\tau)$ with $T_0=\operatorname{id}_U$ . Note that $$V:=\bigcup_{t\in[0,\:\tau)}T_t(U)$$ is open. Assume $[0,\tau)\ni t\mapsto T_t(x)$ is differentiable for all $x\in U$ . Can we show (under suitable additional assumptions, if necessary) that there is a $v:[0,\tau)\times V\to\mathbb R^d$ with $$v\left(t,T_t(x)\right)=\frac{\partial T}{\partial t}(t,x)\tag1$$ for all $t\in[0,\tau)\times V$ ? If $U=\mathbb R^d$ (and hence $V=\mathbb R^d$ ), we may simply set $$v(t,x):=\frac{\partial T}{\partial t}\left(t,T_t^{-1}(x)\right)\tag2.$$ EDIT 1 : I want to choose $v$ such that it is (jointly) continuous. By assumption, $$[0,\tau)\times U\ni(t,x)\mapsto T_t(x)\tag3$$ is partially differentiable in both the first and the second variable. So, it should be differentiable and hence (jointly) continuous. EDIT 2 : I wonder whether any differentiability properties of $v$ with respect to the second variable carry over to $v$ . I've found the following excerpt in a book , which seems to indicate this, but I actually don't understand how they conclude (2.76):","Let , , be open and be a -diffeomorphism from onto an open subset of for with . Note that is open. Assume is differentiable for all . Can we show (under suitable additional assumptions, if necessary) that there is a with for all ? If (and hence ), we may simply set EDIT 1 : I want to choose such that it is (jointly) continuous. By assumption, is partially differentiable in both the first and the second variable. So, it should be differentiable and hence (jointly) continuous. EDIT 2 : I wonder whether any differentiability properties of with respect to the second variable carry over to . I've found the following excerpt in a book , which seems to indicate this, but I actually don't understand how they conclude (2.76):","d\in\mathbb N \tau>0 U\subseteq\mathbb R^d T_t C^1 U \mathbb R^d t\in[0,\tau) T_0=\operatorname{id}_U V:=\bigcup_{t\in[0,\:\tau)}T_t(U) [0,\tau)\ni t\mapsto T_t(x) x\in U v:[0,\tau)\times V\to\mathbb R^d v\left(t,T_t(x)\right)=\frac{\partial T}{\partial t}(t,x)\tag1 t\in[0,\tau)\times V U=\mathbb R^d V=\mathbb R^d v(t,x):=\frac{\partial T}{\partial t}\left(t,T_t^{-1}(x)\right)\tag2. v [0,\tau)\times U\ni(t,x)\mapsto T_t(x)\tag3 v v","['real-analysis', 'ordinary-differential-equations', 'implicit-function-theorem', 'diffeomorphism', 'inverse-function-theorem']"
21,Confirming solution to a Langevin equation using Fourier series,Confirming solution to a Langevin equation using Fourier series,,"Consider the following Langevin equation $$\frac{d^2 x}{dt^2}+\omega_n^2x=\eta(t),$$ where $\eta(t)$ has a gaussian probability distribution with mean zero and correlation $$\langle \eta(t) \eta(t')\rangle = D\delta(t-t'),$$ where $\delta(t-t')$ is the dirac delta function. In this post they show that the solution is $$\langle x(t) \rangle = 0$$ $$\langle x^2(t) \rangle = \frac{D}{2\omega_n^3}[\omega_n t - \sin(\omega_n t) \cos(\omega_n t)].$$ I want to confirm this result using a Fourier series method. Let $$\eta(t) = a_0 X_0 + \sum_{k=1}^\infty a_k X_k \cos(\omega_k t) + b_k Y_k \sin(\omega_k t),$$ where $X_k$ , $Y_k$ are independent random gaussian variables with mean 0 and variance 1. By solving the differential equation for a sinusoidal driver we get $$x(t) = \sum_{k=0}^\infty x_k(t),$$ where $$x_k(t) = \begin{cases} \frac{a_k X_k\omega_n[\cos(\omega_n t) - \cos(\omega_k t)] + b_k Y_k[\omega_k\sin(\omega_n t) - \omega_n\sin(\omega_k t)]}{\omega_n(\omega_k^2 - \omega_n^2)}, & \omega_k \ne \omega_n \\ \frac{a_k X_k\omega_n t \sin(\omega_n t) + b_k Y_k[\sin(\omega_n t) - \omega_n t \cos(\omega_n t)]}{2\omega_n^2}, & \omega_k = \omega_n. \end{cases}$$ Therefore, the soltion should be given by $$\langle x(t) \rangle = 0,$$ $$\langle x(t)^2 \rangle = \sum_{k=0}^\infty\langle x_k(t)^2 \rangle,$$ where $$\langle x_k^2(t) \rangle = \begin{cases} \frac{a_k^2\omega_n^2[\cos(\omega_n t) - \cos(\omega_k t)]^2 + b_k^2[\omega_k\sin(\omega_n t) - \omega_n\sin(\omega_k t)]^2}{\omega_n^2(\omega_k^2 - \omega_n^2)^2}, & \omega_k \ne \omega_n \\ \frac{a_k^2\omega_n^2 t^2 \sin^2(\omega_n t) + b_k^2[\sin(\omega_n t) - \omega_n t \cos(\omega_n t)]^2}{4\omega_n^4}, & \omega_k = \omega_n. \end{cases}$$ Therefore, letting $a_k=b_k=D$ , shouldn't the formulas for the variances agree? I tried to confirm this in python but they are not the same. Here is the code and outputted figures: import numpy as np import matplotlib.pyplot as plt  def white_noise(t):     eta = a[0] * np.random.normal()     for k in range(1,N+1):         eta += a[k] * np.random.normal() * np.cos(omega[k] * t)         eta += b[k] * np.random.normal() * np.sin(omega[k] * t)     return eta  def variance_exact(t):     return D / 2 / omega_n ** 3 * (omega_n * t - np.sin(omega_n * t) * np.cos(omega_n * t))  def variance_fourier(t):     var_for = variance_fourier_term1(t, 0)     for k in range(1,N+1):         if omega[k] != omega_n:             var_for += variance_fourier_term1(t, k)         elif omega[k] == omega_n:             var_for += variance_fourier_term2(t, k)     return var_for  def variance_fourier_term1(t, k):     term1  = a[k] ** 2 * omega_n ** 2 * (np.cos(omega_n * t) - np.cos(omega[k] * t)) ** 2     term1 += b[k] ** 2 * (omega[k] * np.sin(omega_n * t) - omega_n * np.sin(omega[k] * t)) ** 2     term1 = term1 / omega_n ** 2 / (omega[k] ** 2 - omega_n ** 2) ** 2     return term1  def variance_fourier_term2(t, k):     term2  = a[k] ** 2 * omega_n ** 2 * t ** 2 * np.sin(omega_n * t) ** 2     term2 += b[k] ** 2 * (np.sin(omega_n * t) - omega_n * t * np.cos(omega_n * t)) ** 2     term2 = term2 / 4 / omega_n ** 4     return term2  P = 20 N = 1000 k_array = np.arange(N+1) omega = np.pi * k_array / (2 * P) omega_n = 1 D = 1  t_min = 0 t_max = P nt = 1024 t = np.linspace(t_min, t_max, nt)  a = np.full(N+1, D) b = np.full(N+1, D) b[0] = 0  fig = plt.figure() fig_size = fig.get_size_inches() fig_size[0] = 2 * fig_size[0] fig.set_size_inches(fig_size)  ax = fig.add_subplot(121) ax.plot(t, variance_exact(t)) ax.set_xlabel('t') ax.set_title(r' $\langle x^2(t)\rangle$ - Exact')  ax = fig.add_subplot(122) ax.plot(t, variance_fourier(t)) ax.set_xlabel('t') ax.set_title(r' $\langle x^2(t)\rangle$ - Fourier')  fig.savefig('temp_figures/variance_exact_vs_fourier.png', bbox_inches = 'tight')  plt.show(block = False) Edit: Fixed bug on line 25 of code.","Consider the following Langevin equation where has a gaussian probability distribution with mean zero and correlation where is the dirac delta function. In this post they show that the solution is I want to confirm this result using a Fourier series method. Let where , are independent random gaussian variables with mean 0 and variance 1. By solving the differential equation for a sinusoidal driver we get where Therefore, the soltion should be given by where Therefore, letting , shouldn't the formulas for the variances agree? I tried to confirm this in python but they are not the same. Here is the code and outputted figures: import numpy as np import matplotlib.pyplot as plt  def white_noise(t):     eta = a[0] * np.random.normal()     for k in range(1,N+1):         eta += a[k] * np.random.normal() * np.cos(omega[k] * t)         eta += b[k] * np.random.normal() * np.sin(omega[k] * t)     return eta  def variance_exact(t):     return D / 2 / omega_n ** 3 * (omega_n * t - np.sin(omega_n * t) * np.cos(omega_n * t))  def variance_fourier(t):     var_for = variance_fourier_term1(t, 0)     for k in range(1,N+1):         if omega[k] != omega_n:             var_for += variance_fourier_term1(t, k)         elif omega[k] == omega_n:             var_for += variance_fourier_term2(t, k)     return var_for  def variance_fourier_term1(t, k):     term1  = a[k] ** 2 * omega_n ** 2 * (np.cos(omega_n * t) - np.cos(omega[k] * t)) ** 2     term1 += b[k] ** 2 * (omega[k] * np.sin(omega_n * t) - omega_n * np.sin(omega[k] * t)) ** 2     term1 = term1 / omega_n ** 2 / (omega[k] ** 2 - omega_n ** 2) ** 2     return term1  def variance_fourier_term2(t, k):     term2  = a[k] ** 2 * omega_n ** 2 * t ** 2 * np.sin(omega_n * t) ** 2     term2 += b[k] ** 2 * (np.sin(omega_n * t) - omega_n * t * np.cos(omega_n * t)) ** 2     term2 = term2 / 4 / omega_n ** 4     return term2  P = 20 N = 1000 k_array = np.arange(N+1) omega = np.pi * k_array / (2 * P) omega_n = 1 D = 1  t_min = 0 t_max = P nt = 1024 t = np.linspace(t_min, t_max, nt)  a = np.full(N+1, D) b = np.full(N+1, D) b[0] = 0  fig = plt.figure() fig_size = fig.get_size_inches() fig_size[0] = 2 * fig_size[0] fig.set_size_inches(fig_size)  ax = fig.add_subplot(121) ax.plot(t, variance_exact(t)) ax.set_xlabel('t') ax.set_title(r' - Exact')  ax = fig.add_subplot(122) ax.plot(t, variance_fourier(t)) ax.set_xlabel('t') ax.set_title(r' - Fourier')  fig.savefig('temp_figures/variance_exact_vs_fourier.png', bbox_inches = 'tight')  plt.show(block = False) Edit: Fixed bug on line 25 of code.","\frac{d^2 x}{dt^2}+\omega_n^2x=\eta(t), \eta(t) \langle \eta(t) \eta(t')\rangle = D\delta(t-t'), \delta(t-t') \langle x(t) \rangle = 0 \langle x^2(t) \rangle = \frac{D}{2\omega_n^3}[\omega_n t - \sin(\omega_n t) \cos(\omega_n t)]. \eta(t) = a_0 X_0 + \sum_{k=1}^\infty a_k X_k \cos(\omega_k t) + b_k Y_k \sin(\omega_k t), X_k Y_k x(t) = \sum_{k=0}^\infty x_k(t), x_k(t) = \begin{cases}
\frac{a_k X_k\omega_n[\cos(\omega_n t) - \cos(\omega_k t)] + b_k Y_k[\omega_k\sin(\omega_n t) - \omega_n\sin(\omega_k t)]}{\omega_n(\omega_k^2 - \omega_n^2)}, & \omega_k \ne \omega_n \\
\frac{a_k X_k\omega_n t \sin(\omega_n t) + b_k Y_k[\sin(\omega_n t) - \omega_n t \cos(\omega_n t)]}{2\omega_n^2}, & \omega_k = \omega_n.
\end{cases} \langle x(t) \rangle = 0, \langle x(t)^2 \rangle = \sum_{k=0}^\infty\langle x_k(t)^2 \rangle, \langle x_k^2(t) \rangle = \begin{cases}
\frac{a_k^2\omega_n^2[\cos(\omega_n t) - \cos(\omega_k t)]^2 + b_k^2[\omega_k\sin(\omega_n t) - \omega_n\sin(\omega_k t)]^2}{\omega_n^2(\omega_k^2 - \omega_n^2)^2}, & \omega_k \ne \omega_n \\
\frac{a_k^2\omega_n^2 t^2 \sin^2(\omega_n t) + b_k^2[\sin(\omega_n t) - \omega_n t \cos(\omega_n t)]^2}{4\omega_n^4}, & \omega_k = \omega_n.
\end{cases} a_k=b_k=D \langle x^2(t)\rangle \langle x^2(t)\rangle","['ordinary-differential-equations', 'physics', 'signal-processing', 'stochastic-differential-equations', 'noise']"
22,How to find first integral of a system,How to find first integral of a system,,"I have the following system $x'=y, y'=-1-x+x^2+y^2$ . I know that $x^2+y^2-1=0$ is an invariant curve of the system but how do i find a first integral of the system?",I have the following system . I know that is an invariant curve of the system but how do i find a first integral of the system?,"x'=y, y'=-1-x+x^2+y^2 x^2+y^2-1=0",['ordinary-differential-equations']
23,Properties of the solutions of the ODE $y'' + p(x)y' + q(x)y = 0$,Properties of the solutions of the ODE,y'' + p(x)y' + q(x)y = 0,"Let $u(x)$ and $v(x)$ be solutions of $y'' + p(x)y' + q(x)y = 0$ , $p$ and $q$ continuous in $\mathbb{R}$ , such that $u(0) = 1$ , $u'(0) = 0$ , $v(0) = 0$ and $v'(0) = 1$ . Prove that, if $x_{1} < x_{2}$ are such that $u(x_1) = u(x_2) = 0$ and $u(x) \neq 0$ for all $x \in (x_1, x_2)$ , then there exists a point $c \in (x_1, x_2)$ such that $v(c) = 0$ . What's more, prove that such a point is unique. I don't really know how to start tackling this problem. I know that the Wronskian of $u$ and $v$ is greater than $0$ in the entire interval, and that $v(x_1) \neq 0$ and $v(x_2) \neq 0$ , but I couldn't go much farther than that... Does anyone have a tip to get me going in the right direction? Thanks in advance!","Let and be solutions of , and continuous in , such that , , and . Prove that, if are such that and for all , then there exists a point such that . What's more, prove that such a point is unique. I don't really know how to start tackling this problem. I know that the Wronskian of and is greater than in the entire interval, and that and , but I couldn't go much farther than that... Does anyone have a tip to get me going in the right direction? Thanks in advance!","u(x) v(x) y'' + p(x)y' + q(x)y = 0 p q \mathbb{R} u(0) = 1 u'(0) = 0 v(0) = 0 v'(0) = 1 x_{1} < x_{2} u(x_1) = u(x_2) = 0 u(x) \neq 0 x \in (x_1, x_2) c \in (x_1, x_2) v(c) = 0 u v 0 v(x_1) \neq 0 v(x_2) \neq 0","['ordinary-differential-equations', 'wronskian']"
24,Stability of a three-dimensional system,Stability of a three-dimensional system,,"when I consider the three-dimensional ODE system such as $$ \dot{x} = \frac{25}{(1+y^2)(1+z^2)} - x\\ \dot{y} = \frac{25}{(1+x^2)(1+z^2)} - y\\ \dot{z} = \frac{5}{1+(x+y)^2} - z $$ There is an equilibrium state $$(2.78581, 2.78581, 0.15604).$$ When I substitute this equilibrium point into the corresponding Jacobian matrix. The eigenvalues at this equilibrium point are $$ [-2.82226, -0.949452, 0.771709] $$ Since there is a positive eigenvalue, this equilibrium should be defined as an unstable state. However, when I simulated this system in MATLAB, the simulation results show that this equilibrium state seems like a stable state. I was wondering how this thing happens? Is there anything I misunderstood?","when I consider the three-dimensional ODE system such as There is an equilibrium state When I substitute this equilibrium point into the corresponding Jacobian matrix. The eigenvalues at this equilibrium point are Since there is a positive eigenvalue, this equilibrium should be defined as an unstable state. However, when I simulated this system in MATLAB, the simulation results show that this equilibrium state seems like a stable state. I was wondering how this thing happens? Is there anything I misunderstood?","
\dot{x} = \frac{25}{(1+y^2)(1+z^2)} - x\\
\dot{y} = \frac{25}{(1+x^2)(1+z^2)} - y\\
\dot{z} = \frac{5}{1+(x+y)^2} - z
 (2.78581, 2.78581, 0.15604). 
[-2.82226, -0.949452, 0.771709]
","['ordinary-differential-equations', 'dynamical-systems', 'stability-in-odes', 'jacobian']"
25,System of differential equations in RC,System of differential equations in RC,,"I am struggling with c) part these differential equations, I am not sure i obtained it correctly or not, but i solved in matlab also, and i dont think it is correct answer. My work: $$-50q_1'-100000q_1+100(I-q_1')=0$$ $$-50q_2'-100000q_2+150(I-q_2')=0$$ $$-50q_1'-100000q_1-50q_2'-100000q_2+75=0$$ Can you check my equations also? Here $I$ is the function of time $t$ If R1 and R3 are in parallel   and R2 and R3 in parallel always for any time t, then this question is a piece of cake for me. Can u confirm are they in parallel ? ? I solved it using laplace and both MATLAB now, Thanks if anyone else has tried","I am struggling with c) part these differential equations, I am not sure i obtained it correctly or not, but i solved in matlab also, and i dont think it is correct answer. My work: Can you check my equations also? Here is the function of time If R1 and R3 are in parallel   and R2 and R3 in parallel always for any time t, then this question is a piece of cake for me. Can u confirm are they in parallel ? ? I solved it using laplace and both MATLAB now, Thanks if anyone else has tried",-50q_1'-100000q_1+100(I-q_1')=0 -50q_2'-100000q_2+150(I-q_2')=0 -50q_1'-100000q_1-50q_2'-100000q_2+75=0 I t,"['calculus', 'ordinary-differential-equations', 'derivatives', 'physics']"
26,Solutions to differential equation $\nabla f(x)=f(x)x$,Solutions to differential equation,\nabla f(x)=f(x)x,"Let us consider the differential equation  given by $\nabla f(x)=f(x)x$ , where $f:\mathbb{R}^n\to \mathbb{R}$ . I have found that $f(x)=K\exp(|x|^2/2)$ is a solution, but are all solution of this form?","Let us consider the differential equation  given by , where . I have found that is a solution, but are all solution of this form?",\nabla f(x)=f(x)x f:\mathbb{R}^n\to \mathbb{R} f(x)=K\exp(|x|^2/2),"['real-analysis', 'functional-analysis', 'ordinary-differential-equations', 'partial-differential-equations', 'linear-pde']"
27,How to solve this first order ODE right,How to solve this first order ODE right,,"Here is ODE of first order: $$ y'=- \frac{x^2}{y^3} $$ with $y(0)=1 , y(0)=-1 $ Can my method be the separation method? If I use it, it doesn't work out: $ \frac{dy}{dx}= -x^2 \frac{1}{y^3} $ $ \leftrightarrow y^3 dy = -x^2 dx $ By integration I get: $$ \int y^3 dx = \int -x^2 dx $$ $$ \leftrightarrow \frac{1}{4} y^4+c = - \frac{1}{3} x^3+c $$ $$ \leftrightarrow y= \mp \sqrt[4]{ - \frac{4}{3} x^3 +c } $$ Where is my mistake?","Here is ODE of first order: with Can my method be the separation method? If I use it, it doesn't work out: By integration I get: Where is my mistake?"," y'=- \frac{x^2}{y^3}  y(0)=1 , y(0)=-1   \frac{dy}{dx}= -x^2 \frac{1}{y^3}   \leftrightarrow y^3 dy = -x^2 dx   \int y^3 dx = \int -x^2 dx   \leftrightarrow \frac{1}{4} y^4+c = - \frac{1}{3} x^3+c   \leftrightarrow y= \mp \sqrt[4]{ - \frac{4}{3} x^3 +c } ","['real-analysis', 'calculus', 'ordinary-differential-equations']"
28,Compactly Supported Flow [closed],Compactly Supported Flow [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Consider the following initial value problem $$  \begin{cases} \frac{d}{dt} Y_t = \rho(Y_t)\\ Y_0 = 0 \end{cases} $$ where $\rho(x)$ is a bump function supported near $0$ on $\mathbb{R}^1$ .  Let $f:t\mapsto Y_t$ . Why is $Im(f)\subseteq supp(\rho)$ ?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Consider the following initial value problem where is a bump function supported near on .  Let . Why is ?"," 
\begin{cases}
\frac{d}{dt} Y_t = \rho(Y_t)\\
Y_0 = 0
\end{cases}
 \rho(x) 0 \mathbb{R}^1 f:t\mapsto Y_t Im(f)\subseteq supp(\rho)","['real-analysis', 'ordinary-differential-equations']"
29,showing that $S=\{A\in M_{n}; x'=Ax \ \text{is hyperbolic}\}$ is open,showing that  is open,S=\{A\in M_{n}; x'=Ax \ \text{is hyperbolic}\},"I tried to show that the complement is closed, $S^c=\{A\in M_n, \text{real part of eigenvalues is 0}\}$ . So i tried to show that this set is closed by $(\{0\}\times \mathbb{R})$ in the fuction: \begin{align} F:M_n&\to \{\text{eigenvalues of M_n}\}\subset \mathbb{C} \\ \end{align} where $F(M)$ is where the eigenvalues is.  Is this function continuous? Is there another way to do this?","I tried to show that the complement is closed, . So i tried to show that this set is closed by in the fuction: where is where the eigenvalues is.  Is this function continuous? Is there another way to do this?","S^c=\{A\in M_n, \text{real part of eigenvalues is 0}\} (\{0\}\times \mathbb{R}) \begin{align}
F:M_n&\to \{\text{eigenvalues of M_n}\}\subset \mathbb{C} \\
\end{align} F(M)","['linear-algebra', 'ordinary-differential-equations', 'analysis']"
30,$y(x+y^3)dx=x(y^3-x)dy$,,y(x+y^3)dx=x(y^3-x)dy,"Solve $y(x+y^3)dx=x(y^3-x)dy$ Attempt: $$\frac{dy}{dx}=\frac{y(x+y^3)}{x(y^3-x)}.$$ Let $y^3=xt.$ Then $$3y^2\frac{dy}{dx} = t+x\frac{dt}{dx}.$$ Therefore $$\frac{(t+x\frac{dt}{dx})}{(3y^2)} = \frac{t(t+1)}{y^2(t-1)}$$ and hence $$x\frac{dt}{dx}=\frac{2t^2+4t}{t-1}.$$ Integrating, $$\ln|cx|=\frac{3}{4}\ln|t+2|-\frac{1}{4}\ln|t|.$$ Simplyfing, $$cx^2=\frac{y^3+2x}{y}$$ but the given answer is $y=cx^{1/3}$ . Where have I made a mistake?","Solve Attempt: Let Then Therefore and hence Integrating, Simplyfing, but the given answer is . Where have I made a mistake?",y(x+y^3)dx=x(y^3-x)dy \frac{dy}{dx}=\frac{y(x+y^3)}{x(y^3-x)}. y^3=xt. 3y^2\frac{dy}{dx} = t+x\frac{dt}{dx}. \frac{(t+x\frac{dt}{dx})}{(3y^2)} = \frac{t(t+1)}{y^2(t-1)} x\frac{dt}{dx}=\frac{2t^2+4t}{t-1}. \ln|cx|=\frac{3}{4}\ln|t+2|-\frac{1}{4}\ln|t|. cx^2=\frac{y^3+2x}{y} y=cx^{1/3},"['calculus', 'integration', 'ordinary-differential-equations']"
31,Find general solution of $xx''=(x')^3$,Find general solution of,xx''=(x')^3,"I'm trying to solve ordinary differantial equation that looks like this: $xx''=(x')^3$ I'm using the substitution $u(x)=x'$ and now I have $ \frac{uu'}{u^3}=\frac{1}{x}$ After solving it I obtain with: $u=-\frac{1}{lnx} +c$ and  going back to x' I have: $ x'=-\frac{1}{lnx} +c$ How can I proceed from here? Integrating both sides doesn't look good, am I doing something wrong?","I'm trying to solve ordinary differantial equation that looks like this: I'm using the substitution and now I have After solving it I obtain with: and  going back to x' I have: How can I proceed from here? Integrating both sides doesn't look good, am I doing something wrong?",xx''=(x')^3 u(x)=x'  \frac{uu'}{u^3}=\frac{1}{x} u=-\frac{1}{lnx} +c  x'=-\frac{1}{lnx} +c,['ordinary-differential-equations']
32,"Relaxation method on a system of two second-order, coupled, non-linear ODEs (boundary value problem)","Relaxation method on a system of two second-order, coupled, non-linear ODEs (boundary value problem)",,"This is my first question on Stack Exchange - I welcome any suggestions if my approach to asking it does not match the usual conventions around here. So: I need to solve a system of two second-order, non-linear, coupled differential equations for the functions f and g which depend only on one free variable, i.e. f(x) and g(x). The functions f and g are defined on a finite interval (e.g. [0;10]) and the values of these functions are given at the boundaries (boundary value problem). I need to do this numerically because there is no analytical solution. I have been scouting a variety of numerical methods and it seems the so-called relaxation method as outlined in ''Numerical Recipes'' by Press et al. might be the way to go. Since I have no numerics background, I may be mistaken in assuming this method is standard knowledge; I will be more than happy to briefly explain what it does if required. Now, from what I understand, this method works fine for systems of first-order ODEs. In principle, your usual reduction of order does the trick and I can express my system as four coupled first-order ODEs. However, my problem is that I have no boundary conditions for the two additional equations, that is, no BCs for the derivatives of f and g. This is the point where I got stuck. Questions: 1) Does anyone know if this poses a conceptual problem to the method? How can I circumvent it? 2) Would it be possible to apply the aforementioned relaxation method to the second-order system by 'brute force'? Thank you very much in advance! EDIT: The system of first-order ODEs would look like this: $$ \frac{\partial f(t)}{\partial t} = u(t) \\  \frac{\partial u(t)}{\partial t} = a(t,f,g) \\ \frac{\partial g(t)}{\partial t} = v(t) \\ \frac{\partial v(t)}{\partial t} = b(t,f,g,v),$$ where $a$ and $b$ are non-linear functions of their arguments (am not showing them here, but of course I know how they look like). $f(0)$ , $g(0)$ , $f(10)$ and $g(10)$ would be known values. Should anyone know of a method that can solve these for sure - and I don't mind if it is a built-in method of Mathematica, MATLAB or the likes - I would appreciate any suggestion. Also, thanks to everyone who already commented!","This is my first question on Stack Exchange - I welcome any suggestions if my approach to asking it does not match the usual conventions around here. So: I need to solve a system of two second-order, non-linear, coupled differential equations for the functions f and g which depend only on one free variable, i.e. f(x) and g(x). The functions f and g are defined on a finite interval (e.g. [0;10]) and the values of these functions are given at the boundaries (boundary value problem). I need to do this numerically because there is no analytical solution. I have been scouting a variety of numerical methods and it seems the so-called relaxation method as outlined in ''Numerical Recipes'' by Press et al. might be the way to go. Since I have no numerics background, I may be mistaken in assuming this method is standard knowledge; I will be more than happy to briefly explain what it does if required. Now, from what I understand, this method works fine for systems of first-order ODEs. In principle, your usual reduction of order does the trick and I can express my system as four coupled first-order ODEs. However, my problem is that I have no boundary conditions for the two additional equations, that is, no BCs for the derivatives of f and g. This is the point where I got stuck. Questions: 1) Does anyone know if this poses a conceptual problem to the method? How can I circumvent it? 2) Would it be possible to apply the aforementioned relaxation method to the second-order system by 'brute force'? Thank you very much in advance! EDIT: The system of first-order ODEs would look like this: where and are non-linear functions of their arguments (am not showing them here, but of course I know how they look like). , , and would be known values. Should anyone know of a method that can solve these for sure - and I don't mind if it is a built-in method of Mathematica, MATLAB or the likes - I would appreciate any suggestion. Also, thanks to everyone who already commented!"," \frac{\partial f(t)}{\partial t} = u(t) \\ 
\frac{\partial u(t)}{\partial t} = a(t,f,g) \\
\frac{\partial g(t)}{\partial t} = v(t) \\
\frac{\partial v(t)}{\partial t} = b(t,f,g,v), a b f(0) g(0) f(10) g(10)","['ordinary-differential-equations', 'numerical-methods', 'systems-of-equations', 'relaxations']"
33,Example of a symplectic but non-Hamiltonian vector field on $\mathbb{T}^{2n}$,Example of a symplectic but non-Hamiltonian vector field on,\mathbb{T}^{2n},"I want to show that there exists a symplectic vector field on the $2n$ torus $\mathbb{T}^{2n}$ ,  endowed with the unique symplectic form $\omega$ that pullsback to the canonical symplectic form $\omega_0$ on $\mathbb{R}^{2n}$ under the quotient map $\pi:\mathbb{R}^{2n}\to\mathbb{T}^{2n}$ , which is not Hamiltonian. We identify $T_x\mathbb{T}^{2n}\cong\mathbb{R}^{2n}$ for all $x\in\mathbb{R}^{2n}$ and we define the vector field $X\in\mathcal{X}(\mathbb{T}^{2n})$ by $X(x)=v$ for some fixed $0\neq v\in\mathbb{R}^{2n}$ . Then I want to show that $d\iota_X\omega=0$ . I consider the vector field $\tilde{X}$ on $\mathbb{R}^{2n}$ defined by $\tilde{X}(x)=v$ . Then $\tilde{X}$ is symplectic and satisfies $d\pi_x\tilde{X}(x)=X_{\pi(x)}$ , so $d\iota_{\tilde{X}}\omega_0=0$ . But $$ d\iota_{\tilde{X}}\omega_0=d\omega_0(\tilde{X},\cdot)=d(\pi^*\omega(\tilde{X},\cdot))=d\omega_{\pi(\cdot)}(d\pi\tilde X,d\pi\cdot)\underbrace{=}_{?}d\omega(X,\cdot)=d\iota_X\omega, $$ so $X$ is symplectic. I am not sure about the step with a question mark, as the $d\pi$ in the second argument disappears. Is there any justification for this? Now, I want to show that $X$ is not Hamiltonian. As always, we assume it is, so there exists a smooth map $H:\mathbb{T}^{2n}\to\mathbb{R}$ such that $\iota_X\omega=dH$ . But I don't see how to arrive at some contradiction now.","I want to show that there exists a symplectic vector field on the torus ,  endowed with the unique symplectic form that pullsback to the canonical symplectic form on under the quotient map , which is not Hamiltonian. We identify for all and we define the vector field by for some fixed . Then I want to show that . I consider the vector field on defined by . Then is symplectic and satisfies , so . But so is symplectic. I am not sure about the step with a question mark, as the in the second argument disappears. Is there any justification for this? Now, I want to show that is not Hamiltonian. As always, we assume it is, so there exists a smooth map such that . But I don't see how to arrive at some contradiction now.","2n \mathbb{T}^{2n} \omega \omega_0 \mathbb{R}^{2n} \pi:\mathbb{R}^{2n}\to\mathbb{T}^{2n} T_x\mathbb{T}^{2n}\cong\mathbb{R}^{2n} x\in\mathbb{R}^{2n} X\in\mathcal{X}(\mathbb{T}^{2n}) X(x)=v 0\neq v\in\mathbb{R}^{2n} d\iota_X\omega=0 \tilde{X} \mathbb{R}^{2n} \tilde{X}(x)=v \tilde{X} d\pi_x\tilde{X}(x)=X_{\pi(x)} d\iota_{\tilde{X}}\omega_0=0 
d\iota_{\tilde{X}}\omega_0=d\omega_0(\tilde{X},\cdot)=d(\pi^*\omega(\tilde{X},\cdot))=d\omega_{\pi(\cdot)}(d\pi\tilde X,d\pi\cdot)\underbrace{=}_{?}d\omega(X,\cdot)=d\iota_X\omega,
 X d\pi X H:\mathbb{T}^{2n}\to\mathbb{R} \iota_X\omega=dH","['ordinary-differential-equations', 'differential-geometry']"
34,problem with the ode $f' = a \delta (x) f$,problem with the ode,f' = a \delta (x) f,"The first solution is $$f(0^+) = \exp\left(a \int_{0^-}^{0^+} dx \delta (x)\right) f(0^-) = \exp(a) f(0^-) .$$ The second solution is $$ f(0^+) - f(0^-) = a  \int_{0^-}^{0^+} dx \delta (x) f(x) = \frac{a}{2}(f(0^+ ) +f(0^-)) ,$$ which leads to $$f(0^+) = \frac{1 + a/2}{1 - a/2} f(0^-) .  $$ The two approaches yield different results. Which one is right? I am biased on the second one. But can anyone justify it and point out the flaw in the first one?",The first solution is The second solution is which leads to The two approaches yield different results. Which one is right? I am biased on the second one. But can anyone justify it and point out the flaw in the first one?,"f(0^+) = \exp\left(a \int_{0^-}^{0^+} dx \delta (x)\right) f(0^-) = \exp(a) f(0^-) .  f(0^+) - f(0^-) = a  \int_{0^-}^{0^+} dx \delta (x) f(x) = \frac{a}{2}(f(0^+ ) +f(0^-)) , f(0^+) = \frac{1 + a/2}{1 - a/2} f(0^-) .  ","['ordinary-differential-equations', 'distribution-theory']"
35,Maximum interval for every initial condition,Maximum interval for every initial condition,,"Consider the IVP given by: $\frac{dx}{dt}=-\frac{x^3}{1-t},\; x(0)=x_{0}$ WITHOUT solving the equation, show that: a) For every $x_{0}$ , the maximal interval is [0,1) b) $\underset{t\uparrow 1} {\lim}\;x(t)$ exists and calculate it's value My attempt: I know that for item a I need to show that f is continuous on the interval $[0,1)$ , but do I need to show that it's locally lipschitz for every $x_{0}$ ?","Consider the IVP given by: WITHOUT solving the equation, show that: a) For every , the maximal interval is [0,1) b) exists and calculate it's value My attempt: I know that for item a I need to show that f is continuous on the interval , but do I need to show that it's locally lipschitz for every ?","\frac{dx}{dt}=-\frac{x^3}{1-t},\; x(0)=x_{0} x_{0} \underset{t\uparrow 1} {\lim}\;x(t) [0,1) x_{0}","['real-analysis', 'ordinary-differential-equations', 'dynamical-systems', 'solution-verification']"
36,A differential equation involving composition,A differential equation involving composition,,"I know that $\frac{d}{dx} f(x) = f(x)$ has a solution $f(x) = e^x$ by letting $y= f(x)$ and using separation of variables. Is there a technique to solve (or at least start on) something like $\frac{d}{dx} f(x) = f(2x)$ ? Putting the $2$ anywhere else such as $2f(x)$ , $f(x)+2$ , etc. does not seem to cause much of an issue, but replacing the $x$ with $2x$ stumps me...","I know that has a solution by letting and using separation of variables. Is there a technique to solve (or at least start on) something like ? Putting the anywhere else such as , , etc. does not seem to cause much of an issue, but replacing the with stumps me...",\frac{d}{dx} f(x) = f(x) f(x) = e^x y= f(x) \frac{d}{dx} f(x) = f(2x) 2 2f(x) f(x)+2 x 2x,['ordinary-differential-equations']
37,How to find a function with given initial values of other function?,How to find a function with given initial values of other function?,,"I am looking for a continuous function $\theta(t)$ to steer a particle on the trajectory $x(t)$ . $x$ and $\theta$ relate through this constraint $\ddot x(t) = Tsin(\theta(t))$ where $T$ is a non-zero positive constant. $\theta(t)$ must satisfy these initial conditions $\theta(0) = 0$ $\theta(t_0) = 0$ $x(t)$ must satisfy these initial values. $$x(0)=x_a, x(t_0)=x_b$$ $$\dot x(0)=0, \dot x(t_0)=0$$ $$\ddot x(0)=0, \ddot x(t_0)=0$$ The two functions $x(t)$ and $\theta(t)$ only need to be continuous between $x_a$ and $x_b$ . $x_b$ can be greater or lesser than $x_a$ . Further, the function $x(t)$ should be bijective on $[0, t_0]$ This is how I got a solution for $x(t)$ . I just guessed what seemed right until I got it working. Perhaps there also exists a polynomial solution. $$\ddot x=-rsin(t)$$ Thus $\ddot x(0)=0$ and $\ddot x(t_0) \implies t_0 \in 0,\pi,2\pi , ...$ $\dot x(t) = rcos(t) + C_1$ $\dot x(0) = 0 = r + C_1$ $\dot x(t_0) = 0 = rcos(t_0) + C_1$ Since $t_0 \in 0, \pi, 2\pi,...$ . We can isolate $C_1$ if we force $t_0 = 2\pi$ Thus $\dot x(t_0) = 0 = r + C_1 \implies C_1 = -r$ $x(t)=rsin(t) - rt + C_2$ $x(0) = x_a = C_2$ $x(t_0) = x_b = rsin(2\pi) - r2\pi + C_2 \implies r = \dfrac{x_a - x_b}{2\pi}$ Therefore, $x(t) = rsin(t) - rx + x_a$ , where $r = \dfrac{x_a - x_b}{2\pi}$ Now here is where I am stuck. I do not understand how I can find $\theta(t)$ from the given equation. I tried doing $\ddot x(t) = Tsin(\theta(t)) = -rsin(t)$ but I'm not sure how to maintain the $x(t)$ constraints","I am looking for a continuous function to steer a particle on the trajectory . and relate through this constraint where is a non-zero positive constant. must satisfy these initial conditions must satisfy these initial values. The two functions and only need to be continuous between and . can be greater or lesser than . Further, the function should be bijective on This is how I got a solution for . I just guessed what seemed right until I got it working. Perhaps there also exists a polynomial solution. Thus and Since . We can isolate if we force Thus Therefore, , where Now here is where I am stuck. I do not understand how I can find from the given equation. I tried doing but I'm not sure how to maintain the constraints","\theta(t) x(t) x \theta \ddot x(t) = Tsin(\theta(t)) T \theta(t) \theta(0) = 0 \theta(t_0) = 0 x(t) x(0)=x_a, x(t_0)=x_b \dot x(0)=0, \dot x(t_0)=0 \ddot x(0)=0, \ddot x(t_0)=0 x(t) \theta(t) x_a x_b x_b x_a x(t) [0, t_0] x(t) \ddot x=-rsin(t) \ddot x(0)=0 \ddot x(t_0) \implies t_0 \in 0,\pi,2\pi , ... \dot x(t) = rcos(t) + C_1 \dot x(0) = 0 = r + C_1 \dot x(t_0) = 0 = rcos(t_0) + C_1 t_0 \in 0, \pi, 2\pi,... C_1 t_0 = 2\pi \dot x(t_0) = 0 = r + C_1 \implies C_1 = -r x(t)=rsin(t) - rt + C_2 x(0) = x_a = C_2 x(t_0) = x_b = rsin(2\pi) - r2\pi + C_2 \implies r = \dfrac{x_a - x_b}{2\pi} x(t) = rsin(t) - rx + x_a r = \dfrac{x_a - x_b}{2\pi} \theta(t) \ddot x(t) = Tsin(\theta(t)) = -rsin(t) x(t)","['geometry', 'ordinary-differential-equations', 'curves', 'boundary-value-problem']"
38,Solution of non-homogeneous ODE is always bounded,Solution of non-homogeneous ODE is always bounded,,"I've been struggling with the following question: $\ddot x + 2 \dot x +(1+e^{-t})x=t^{2}cos(t)$ , show that all solution are bounded for $t>0$ . My problem: Im probably missing some theorem or fact. If we had, for example $\ddot x + 2 \dot x +(1+e^{-t})x=0$ . Well that would be pretty easy to prove that all solutions tend to $0$ when $t$ tends to $\infty$ . We only have to use a very well known theorem of perturbed systems. But for that one I cant find any theorem or anything that would help me to solve it. Thanks so much. :)","I've been struggling with the following question: , show that all solution are bounded for . My problem: Im probably missing some theorem or fact. If we had, for example . Well that would be pretty easy to prove that all solutions tend to when tends to . We only have to use a very well known theorem of perturbed systems. But for that one I cant find any theorem or anything that would help me to solve it. Thanks so much. :)",\ddot x + 2 \dot x +(1+e^{-t})x=t^{2}cos(t) t>0 \ddot x + 2 \dot x +(1+e^{-t})x=0 0 t \infty,"['calculus', 'ordinary-differential-equations', 'perturbation-theory']"
39,What is function $\sin$ without $x$?,What is function  without ?,\sin x,"I'm solving a problem set in ODE: This is the first time in my life that I see such functions $\sin,\cos,\tan$ without argument $x$ . I would like to ask if $\sin,\cos,\tan$ mean $\sin x,\cos x,\tan x$ or they mean something else. Thank you so much!",I'm solving a problem set in ODE: This is the first time in my life that I see such functions without argument . I would like to ask if mean or they mean something else. Thank you so much!,"\sin,\cos,\tan x \sin,\cos,\tan \sin x,\cos x,\tan x","['ordinary-differential-equations', 'notation']"
40,How to solve second order ODE with Dirac Delta?,How to solve second order ODE with Dirac Delta?,,"I'm trying to solve a non-homogeneous second order ODE. I've read similar other questions, but all use the method of Laplace transformations, which I've not seen/used before. The ODE is: $$y''(x) - a^{2}y(x)=\delta(x-p)$$ subject to the boundary conditions $y(0)=0$ and $y(b)=0$ for $0<p<b$ . I've established the homogeneous solution to be $$ y(x) = C_{1}e^{ax}+C_{2}e^{-ax}, ~\text{if} ~~x<p \\ y(x) = D_{1}e^{ax}+D_{2}e^{-ax},  ~\text{if} ~~x>p $$ My professor suggested the best way to proceed was to apply the boundary conditions, to get rid of some of the constants. I've done this and obtained $$ y(x) = 2C_{1}\sinh(ax), ~\text{if} ~~x<p\\ y(x) = -2D_{1}e^{ab}\sinh{(a(b-x))}, ~\text{if} ~~ x>p\\ $$ Getting rid of the last two constants is where I'm struggling. My professor said that for a $\delta(x-p)$ ODE, you need to ensure continuity for $y(p)$ and then have a unit displacement for $y'(p)$ . I'm unsure about how to apply this. I have not done Laplace transformations before. Any help would be greatly appreciated!","I'm trying to solve a non-homogeneous second order ODE. I've read similar other questions, but all use the method of Laplace transformations, which I've not seen/used before. The ODE is: subject to the boundary conditions and for . I've established the homogeneous solution to be My professor suggested the best way to proceed was to apply the boundary conditions, to get rid of some of the constants. I've done this and obtained Getting rid of the last two constants is where I'm struggling. My professor said that for a ODE, you need to ensure continuity for and then have a unit displacement for . I'm unsure about how to apply this. I have not done Laplace transformations before. Any help would be greatly appreciated!","y''(x) - a^{2}y(x)=\delta(x-p) y(0)=0 y(b)=0 0<p<b 
y(x) = C_{1}e^{ax}+C_{2}e^{-ax}, ~\text{if} ~~x<p \\
y(x) = D_{1}e^{ax}+D_{2}e^{-ax},  ~\text{if} ~~x>p
 
y(x) = 2C_{1}\sinh(ax), ~\text{if} ~~x<p\\
y(x) = -2D_{1}e^{ab}\sinh{(a(b-x))}, ~\text{if} ~~ x>p\\
 \delta(x-p) y(p) y'(p)","['ordinary-differential-equations', 'dirac-delta', 'boundary-value-problem']"
41,Invariance of the x and y axis,Invariance of the x and y axis,,"My question is about the invariance of the x and y-axis of the following system of differential equations: $$\begin{pmatrix} \dot{x}(t)\\  \dot{y}(t) \end{pmatrix} = \begin{pmatrix} 3x(t)\\  -2y(t)+x(t)^2 \end{pmatrix}$$ Now I want to know if the x-axis or the y-axis is invariant. My idea is to look at the dynamical flow as seen here: Flow of the system . Then we see that the flow is stable for the y-axis. So therefore if we have a set $C \subseteq \mathbb{R}^2$ , we see that $y(t) \in C$ and that $y(\tau) \in C$ for all $\tau \geq t$ . So therefore the the y-axis is invariant. While the x-axis is unstable and we can't be sure $x(t)$ will stay in the set. Therefore I think the y-axis is invariant, but the x-axis is not. Is it correct to see it this way? How can I show this mathematically, any material you can recommend that would improve my understanding? Thanks for the help, much appreciated! :)","My question is about the invariance of the x and y-axis of the following system of differential equations: Now I want to know if the x-axis or the y-axis is invariant. My idea is to look at the dynamical flow as seen here: Flow of the system . Then we see that the flow is stable for the y-axis. So therefore if we have a set , we see that and that for all . So therefore the the y-axis is invariant. While the x-axis is unstable and we can't be sure will stay in the set. Therefore I think the y-axis is invariant, but the x-axis is not. Is it correct to see it this way? How can I show this mathematically, any material you can recommend that would improve my understanding? Thanks for the help, much appreciated! :)","\begin{pmatrix}
\dot{x}(t)\\ 
\dot{y}(t)
\end{pmatrix} = \begin{pmatrix}
3x(t)\\ 
-2y(t)+x(t)^2
\end{pmatrix} C \subseteq \mathbb{R}^2 y(t) \in C y(\tau) \in C \tau \geq t x(t)","['ordinary-differential-equations', 'dynamical-systems', 'invariance']"
42,Working out the analysis of a circuit with non-linear resistor,Working out the analysis of a circuit with non-linear resistor,,"I am working on a problem from the Strogatz book, specifically problem 2.2.12. I am having trouble evaluating the fixed point that I found. I have summarized the problem statement here, and show my work below: Suppose a series circuit with a voltage source $V_0$ , a non linear resistor and a capacitor. Assume that the the non linear nature of the resistor is such that $I = g(V)$ , which is graphed in the book and I have tried to reproduce: So, to model the equation: $$V_0 = V_r + Q/C \Leftrightarrow V_r = V_0 - Q/C$$ Where $V_r$ is the voltage drop across the non-linear resistor. Coupling this with the fact (given in the text) that $\dot{Q} = I$ , the function $g(V)$ can be rewritten as $$\dot{Q} = g(V_0 - \frac{Q}{C})$$ and we see that $\dot{Q} = 0$ When $Q = \frac{V_0}{C}$ I thought to use the linearization technique mentioned in this chapter, namely to find out the sign of $f'(Q, t) = \ddot{Q} = -\frac{1}{C}g'(V_0 - \frac{Q}{C})$ at the fixed point. So here is where I get a little stumped. I see that $g'(V_0 - \frac{Q}{C})$ is positive and non-zero at (0,0) and therefore the overall value is negative (stable) due to the factor of $-\frac{1}{C}$ , but I don't know how to determine end behavior. The task was to determine the qualitative differences between this model and the one where voltage drop is simply given by $V_r = I_rR$ . That means, in my mind, that $Q(t) \rightarrow Q^* = V_0 / C$ as $t \rightarrow \infty$ , which is different than the fixed point in the example of a linear resistor. Is that all that the question is asking? But the stability remains the same... except that I recall stability analysis for non-linear equations is supposed to be local only, so maybe there is something else I am missing?","I am working on a problem from the Strogatz book, specifically problem 2.2.12. I am having trouble evaluating the fixed point that I found. I have summarized the problem statement here, and show my work below: Suppose a series circuit with a voltage source , a non linear resistor and a capacitor. Assume that the the non linear nature of the resistor is such that , which is graphed in the book and I have tried to reproduce: So, to model the equation: Where is the voltage drop across the non-linear resistor. Coupling this with the fact (given in the text) that , the function can be rewritten as and we see that When I thought to use the linearization technique mentioned in this chapter, namely to find out the sign of at the fixed point. So here is where I get a little stumped. I see that is positive and non-zero at (0,0) and therefore the overall value is negative (stable) due to the factor of , but I don't know how to determine end behavior. The task was to determine the qualitative differences between this model and the one where voltage drop is simply given by . That means, in my mind, that as , which is different than the fixed point in the example of a linear resistor. Is that all that the question is asking? But the stability remains the same... except that I recall stability analysis for non-linear equations is supposed to be local only, so maybe there is something else I am missing?","V_0 I = g(V) V_0 = V_r + Q/C \Leftrightarrow V_r = V_0 - Q/C V_r \dot{Q} = I g(V) \dot{Q} = g(V_0 - \frac{Q}{C}) \dot{Q} = 0 Q = \frac{V_0}{C} f'(Q, t) = \ddot{Q} = -\frac{1}{C}g'(V_0 - \frac{Q}{C}) g'(V_0 - \frac{Q}{C}) -\frac{1}{C} V_r = I_rR Q(t) \rightarrow Q^* = V_0 / C t \rightarrow \infty","['ordinary-differential-equations', 'nonlinear-dynamics']"
43,Reduction of a Power Series Solution Separated by an Integer?,Reduction of a Power Series Solution Separated by an Integer?,,"I am given the following equation: $$x(x-1)\ddot y + 6x^2\dot y + 3y = 0 $$ and have solved/found the regular singular points, the exponents at singularity, the first series solution and the form of the second series solution, which are all as follows: $x = 0, \space 1$ are regular singular points For $x = 0$ , $r_1 = 1, \space r_2 = 0$ Recurrence relationship: $$a_n = \frac {n^2 - n + 3}{n(n+1)}a_{n-1} + \frac {6(n-1)}{n(n+1)}a_{n-2}$$ $a_1 = \frac{3}{2}a_0$ The first solution is: $$y_1(x) = x + \frac{3}{2}x^2 + \frac{9}{4}x^3 + \frac{51}{16}x^4 + ...$$ Since the exponents differ by an integer, the second solution is of the form: $$y_2(x) = ay_1(x)ln(x) + 1 + \sum_{n=1}^{\infty} c_n(r_2)x^n$$ Substituting this equation into the original governing equation: $$x(x-1)\bigg\{ay_1(x)ln(x) + 1 + \sum_{n=1}^{\infty} c_n(r_2)x^n)\bigg\}^"" + 6x^2 \bigg\{ ay_1(x)ln(x) + 1 + \sum_{n=1}^{\infty} c_n(r_2)x^n \bigg\} ' + 3\bigg\{ ay_1(x)ln(x) + 1 + \sum_{n=1}^{\infty} c_n(r_2)x^n\bigg\} = 0 $$ Now my question is how does the above equation reduce to the following? : $$aln(x)L[y_1] + 2a(x-1)y' - \frac{(x-1)}{x}ay_1 + 6axy_1 + L\bigg[ 1 + \sum_{n=1}^{\infty} c_n(r_2)x^n \bigg] = 0$$ The above form is advantageous since $L[y_1] = 0$ , but I do not fully understand this reduction step. Thanks!","I am given the following equation: and have solved/found the regular singular points, the exponents at singularity, the first series solution and the form of the second series solution, which are all as follows: are regular singular points For , Recurrence relationship: The first solution is: Since the exponents differ by an integer, the second solution is of the form: Substituting this equation into the original governing equation: Now my question is how does the above equation reduce to the following? : The above form is advantageous since , but I do not fully understand this reduction step. Thanks!","x(x-1)\ddot y + 6x^2\dot y + 3y = 0  x = 0, \space 1 x = 0 r_1 = 1, \space r_2 = 0 a_n = \frac {n^2 - n + 3}{n(n+1)}a_{n-1} + \frac {6(n-1)}{n(n+1)}a_{n-2} a_1 = \frac{3}{2}a_0 y_1(x) = x + \frac{3}{2}x^2 + \frac{9}{4}x^3 + \frac{51}{16}x^4 + ... y_2(x) = ay_1(x)ln(x) + 1 + \sum_{n=1}^{\infty} c_n(r_2)x^n x(x-1)\bigg\{ay_1(x)ln(x) + 1 + \sum_{n=1}^{\infty} c_n(r_2)x^n)\bigg\}^"" + 6x^2 \bigg\{ ay_1(x)ln(x) + 1 + \sum_{n=1}^{\infty} c_n(r_2)x^n \bigg\} ' + 3\bigg\{ ay_1(x)ln(x) + 1 + \sum_{n=1}^{\infty} c_n(r_2)x^n\bigg\} = 0  aln(x)L[y_1] + 2a(x-1)y' - \frac{(x-1)}{x}ay_1 + 6axy_1 + L\bigg[ 1 + \sum_{n=1}^{\infty} c_n(r_2)x^n \bigg] = 0 L[y_1] = 0","['ordinary-differential-equations', 'power-series', 'algebra-precalculus']"
44,Help solving the non-linear ODE $\frac{dy}{dt} = t^3 (y-t)^2 + \frac{y}{t}$,Help solving the non-linear ODE,\frac{dy}{dt} = t^3 (y-t)^2 + \frac{y}{t},"The particular solution given in the question is $u(t) = t$ . I know that this is a Ricatti differential equation, therefore it's solution is $y=u(t)+\frac{1}{v(t)}$ . So first I've write the ODE in the Ricatti form: $$\frac{dy}{dt} = t^3 (y-t)^2 + \frac{y}{t}$$ $$\frac{dy}{dt} = y^2(t^3)+y(-2t^4+\frac1t) +t^2$$ So I've used the substituition $y=t+\frac{1}{v(t)} \Rightarrow y' = 1-\frac{dv}{v^2dt}$ . $$t^3y^2 + y(-2t^4+\frac1t )+t^2 = 1 - \frac{dv}{v^2dt}$$ $$...$$ In the end, I've got: $$-t^5+\frac{1}{v^2}+\frac{1}{vt}=-\frac{dv}{v^2dt}$$ $$-t^5v^2+1+\frac{v}{t}=-\frac{dv}{dt}$$ But, the final equation would be a first order linear ODE, and this one isn't. Can you help me solving this? -- PS: Should I try to solve this new equation again using Ricatti? I will try to do this now. But, if you have any suggestions, please tell me","The particular solution given in the question is . I know that this is a Ricatti differential equation, therefore it's solution is . So first I've write the ODE in the Ricatti form: So I've used the substituition . In the end, I've got: But, the final equation would be a first order linear ODE, and this one isn't. Can you help me solving this? -- PS: Should I try to solve this new equation again using Ricatti? I will try to do this now. But, if you have any suggestions, please tell me",u(t) = t y=u(t)+\frac{1}{v(t)} \frac{dy}{dt} = t^3 (y-t)^2 + \frac{y}{t} \frac{dy}{dt} = y^2(t^3)+y(-2t^4+\frac1t) +t^2 y=t+\frac{1}{v(t)} \Rightarrow y' = 1-\frac{dv}{v^2dt} t^3y^2 + y(-2t^4+\frac1t )+t^2 = 1 - \frac{dv}{v^2dt} ... -t^5+\frac{1}{v^2}+\frac{1}{vt}=-\frac{dv}{v^2dt} -t^5v^2+1+\frac{v}{t}=-\frac{dv}{dt},['ordinary-differential-equations']
45,how to get a linear ricci flow equation??,how to get a linear ricci flow equation??,,"one could use linear approximation $g_{ab}=\eta_{ab}+h_{ab}$ to get linear ricci flow equation (2). How to do that? is any process ? I am studying general relativity , i just use the linear approximation : $g_{ab}=\eta_{ab}+\gamma_{ab}$ , and calculate the christoffl symbol : $\Gamma_{a b}^{(1) c}=\frac{1}{2} \eta^{c d}\left(\partial_{a} \gamma_{b d}+\partial_{b} \gamma_{a d}-\partial_{d} \gamma_{a b}\right)$ ,finally, get the Ricci tensor: $R_{a b}^{(1)}=\partial^{c} \partial_{(a} \gamma_{b) c}-\frac{1}{2} \partial^{c} \partial_{c} \gamma_{a b}-\frac{1}{2} \partial_{a} \partial_{b} \gamma$ but there are two extra term : $\partial^{c} \partial_{(a} \gamma_{b) c}$ and $-\frac{1}{2} \partial_{a} \partial_{b} \gamma$ , which not match the linearization ricci flow equation(2). what is wrong with me?","one could use linear approximation to get linear ricci flow equation (2). How to do that? is any process ? I am studying general relativity , i just use the linear approximation : , and calculate the christoffl symbol : ,finally, get the Ricci tensor: but there are two extra term : and , which not match the linearization ricci flow equation(2). what is wrong with me?",g_{ab}=\eta_{ab}+h_{ab} g_{ab}=\eta_{ab}+\gamma_{ab} \Gamma_{a b}^{(1) c}=\frac{1}{2} \eta^{c d}\left(\partial_{a} \gamma_{b d}+\partial_{b} \gamma_{a d}-\partial_{d} \gamma_{a b}\right) R_{a b}^{(1)}=\partial^{c} \partial_{(a} \gamma_{b) c}-\frac{1}{2} \partial^{c} \partial_{c} \gamma_{a b}-\frac{1}{2} \partial_{a} \partial_{b} \gamma \partial^{c} \partial_{(a} \gamma_{b) c} -\frac{1}{2} \partial_{a} \partial_{b} \gamma,"['ordinary-differential-equations', 'differential-geometry', 'partial-differential-equations', 'linear-pde', 'ricci-flow']"
46,Cauchy Euler Equation,Cauchy Euler Equation,,"How do I find the particular integral of $$(x^2D^2+xD-1)y=x^2 e^{2x}$$ where $D=\frac{d}{dx}$ ? I have tried using the substitution $x=e^z$ , but I got stuck.","How do I find the particular integral of where ? I have tried using the substitution , but I got stuck.",(x^2D^2+xD-1)y=x^2 e^{2x} D=\frac{d}{dx} x=e^z,['ordinary-differential-equations']
47,Question about the proof of the Picard–Lindelöf theorem,Question about the proof of the Picard–Lindelöf theorem,,"I am currently trying to understand the proof of the following theorem, concerning the existence and uniqueness of local solutions to an initial value problem (IVP): Let $\Omega$ be an open subset of $\mathbb{R}\times\mathbb{R}^{n}$ , $(t_{0},x_{0})\in\Omega$ a point and $f\colon\Omega\to\mathbb{R}^{n}$ a continuous function that is Locally Lipschitz-continuous in its second variable. Then there exists a $\delta>0$ and a unique $C^{1}$ -function $\hat{u}\colon[t_{0}-\delta,t_{0}+\delta]\to\mathbb{R}^{n}$ that satisfies $$(t,\hat{u}(t))\in\Omega,\quad\hat{u}'(t)=f(t,\hat{u}(t)),\quad \hat{u}(t_{0})=x_{0}$$ for all $t\in[t_{0}-\delta,t_{0}+\delta]$ . Below is a sketch of the proof, where we use the Banach fixed-point theorem : Since $f$ is locally Lipschitz-continuous in the second variable, we are able to find an open neighbourhood $V$ around $(t_{0},x_{0})$ in $\Omega$ and a constant $L\geq0$ such that $$\|f(t,x)-f(t,y)\|\leq L\cdot\|x-y\|$$ for all $x,y\in V$ . Now choose a bounded open neighbourhood $U$ of $(t_{0},x_{0})$ in $V$ such that $\overline{U}\subset V$ . The boundedness of $U$ ensures that $\overline{U}$ is compact. Hence, since $f$ is continuous, the map $$\overline{U}\to\mathbb{R},\quad(t,x)\mapsto\|f(t,x)\|$$ attains a maximum $m\geq0$ . Now choose $\delta>0$ and $\varepsilon>0$ such that $$[t_{0}-\delta,t_{0}+\delta]\times\overline{B_{\varepsilon}(x_{0})}\subset\overline{U},\quad m\cdot\delta\leq\varepsilon,\quad L\cdot\delta<1.$$ The space $C^{0}([t_{0}-\delta,t_{0}+\delta],\mathbb{R}^{n})$ with the metric $$d_{\max}(u,v):=\max_{|t-t_{0}|\leq\delta}\|u(t)-v(t)\|$$ is a complete metric space. Now define $$X:=\{u\in C^{0}([t_{0}-\delta,t_{0}+\delta],\mathbb{R}^{n}) \ | \ d_{\max}(u,x_{0})\leq\varepsilon\},$$ where $x_{0}$ denotes the constant function $x\mapsto x_{0}$ . Then $X$ is a closed ball in the metric space $(C^{0}([t_{0}-\delta,t_{0}+\delta],\mathbb{R}^{n}),d_{\max})$ . Since $X$ is closed, it is also a complete metric space (with the resticted metric). Now define a map $$\Phi\colon X\to X,\quad\Phi(u)(t):=x_{0}+\int_{t_{0}}^{t}f(\tau,u(\tau)) \ d\tau.$$ One can verify that the condition $m\cdot\delta\leq\varepsilon$ ensures that $\Phi(X)\subset X$ . One can also show that $$d_{\max}(\Phi(u),\Phi(v))\leq L\cdot\delta\cdot d_{\max}(u,v)$$ So the condition $L\cdot\delta<1$ shows that $\Phi$ is a contraction. Hence, by the Banach fixed-point theorem, there exists a unique $\hat{u}\in X$ such that $\Phi(\hat{u})=\hat{u}$ . Differentiating both sides with respect to $t$ yields the desired result. MY QUESTION: The (sketch of the) proof above shows that there exists a unique local solution $\hat{u}$ for the IVP in $X$ . But the theorem states that there exists a unique solution in the space $C^{1}([t_{0}-\delta,t_{0}+\delta],\mathbb{R}^{n})$ . So why is it sufficient to prove existence and uniqueness in $X$ ? More precisely: how do we know that the IVP has no other local solution $\hat{v}$ in $C^{1}([t_{0}-\delta,t_{0}+\delta],\mathbb{R}^{n})\setminus X$ , i.e. a solution with $d_{\max}(\hat{v},x_{0})>\varepsilon$ . MY ATTEMPT: I think that it suffices to show any $u\in C^{1}([t_{0}-\delta,t_{0}+\delta],\mathbb{R}^{n})$ that solves the IVP must lie in $X$ . Because such a $u$ is continuous in $t_{0}$ and satisfies $u(t_{0})=x_{0}$ , there exists a $r>0$ (possibly smaller than $\delta$ ) such that $\|u(t)-x_{0}\|\leq\varepsilon$ whenever $|t-t_{0}|\leq r$ . However, this does not prove that $\|u(t)-x_{0}\|\leq\varepsilon$ on all of $[t_{0}-\delta,t_{0}+\delta]$ , i.e. that $u\in X$ .","I am currently trying to understand the proof of the following theorem, concerning the existence and uniqueness of local solutions to an initial value problem (IVP): Let be an open subset of , a point and a continuous function that is Locally Lipschitz-continuous in its second variable. Then there exists a and a unique -function that satisfies for all . Below is a sketch of the proof, where we use the Banach fixed-point theorem : Since is locally Lipschitz-continuous in the second variable, we are able to find an open neighbourhood around in and a constant such that for all . Now choose a bounded open neighbourhood of in such that . The boundedness of ensures that is compact. Hence, since is continuous, the map attains a maximum . Now choose and such that The space with the metric is a complete metric space. Now define where denotes the constant function . Then is a closed ball in the metric space . Since is closed, it is also a complete metric space (with the resticted metric). Now define a map One can verify that the condition ensures that . One can also show that So the condition shows that is a contraction. Hence, by the Banach fixed-point theorem, there exists a unique such that . Differentiating both sides with respect to yields the desired result. MY QUESTION: The (sketch of the) proof above shows that there exists a unique local solution for the IVP in . But the theorem states that there exists a unique solution in the space . So why is it sufficient to prove existence and uniqueness in ? More precisely: how do we know that the IVP has no other local solution in , i.e. a solution with . MY ATTEMPT: I think that it suffices to show any that solves the IVP must lie in . Because such a is continuous in and satisfies , there exists a (possibly smaller than ) such that whenever . However, this does not prove that on all of , i.e. that .","\Omega \mathbb{R}\times\mathbb{R}^{n} (t_{0},x_{0})\in\Omega f\colon\Omega\to\mathbb{R}^{n} \delta>0 C^{1} \hat{u}\colon[t_{0}-\delta,t_{0}+\delta]\to\mathbb{R}^{n} (t,\hat{u}(t))\in\Omega,\quad\hat{u}'(t)=f(t,\hat{u}(t)),\quad \hat{u}(t_{0})=x_{0} t\in[t_{0}-\delta,t_{0}+\delta] f V (t_{0},x_{0}) \Omega L\geq0 \|f(t,x)-f(t,y)\|\leq L\cdot\|x-y\| x,y\in V U (t_{0},x_{0}) V \overline{U}\subset V U \overline{U} f \overline{U}\to\mathbb{R},\quad(t,x)\mapsto\|f(t,x)\| m\geq0 \delta>0 \varepsilon>0 [t_{0}-\delta,t_{0}+\delta]\times\overline{B_{\varepsilon}(x_{0})}\subset\overline{U},\quad m\cdot\delta\leq\varepsilon,\quad L\cdot\delta<1. C^{0}([t_{0}-\delta,t_{0}+\delta],\mathbb{R}^{n}) d_{\max}(u,v):=\max_{|t-t_{0}|\leq\delta}\|u(t)-v(t)\| X:=\{u\in C^{0}([t_{0}-\delta,t_{0}+\delta],\mathbb{R}^{n}) \ | \ d_{\max}(u,x_{0})\leq\varepsilon\}, x_{0} x\mapsto x_{0} X (C^{0}([t_{0}-\delta,t_{0}+\delta],\mathbb{R}^{n}),d_{\max}) X \Phi\colon X\to X,\quad\Phi(u)(t):=x_{0}+\int_{t_{0}}^{t}f(\tau,u(\tau)) \ d\tau. m\cdot\delta\leq\varepsilon \Phi(X)\subset X d_{\max}(\Phi(u),\Phi(v))\leq L\cdot\delta\cdot d_{\max}(u,v) L\cdot\delta<1 \Phi \hat{u}\in X \Phi(\hat{u})=\hat{u} t \hat{u} X C^{1}([t_{0}-\delta,t_{0}+\delta],\mathbb{R}^{n}) X \hat{v} C^{1}([t_{0}-\delta,t_{0}+\delta],\mathbb{R}^{n})\setminus X d_{\max}(\hat{v},x_{0})>\varepsilon u\in C^{1}([t_{0}-\delta,t_{0}+\delta],\mathbb{R}^{n}) X u t_{0} u(t_{0})=x_{0} r>0 \delta \|u(t)-x_{0}\|\leq\varepsilon |t-t_{0}|\leq r \|u(t)-x_{0}\|\leq\varepsilon [t_{0}-\delta,t_{0}+\delta] u\in X","['ordinary-differential-equations', 'fixed-point-theorems', 'lipschitz-functions', 'initial-value-problems', 'contraction-operator']"
48,Conservation of the Hamiltonian,Conservation of the Hamiltonian,,"I'm struggling with the following calculus of variation problem. For an autonomous problem, it is often said that the Hamiltonian is constant along an extremal trajectory. However, the proofs of that fact that I found in the literature rely on the optimal trajectory being twice continuously differentiable, and on a bruteforce differentiation of the Euler-Lagrange equation. Is there a simple argument for a once continuously differentiable extremal? I tried to apply DuBois-Reymond lemma but failed.","I'm struggling with the following calculus of variation problem. For an autonomous problem, it is often said that the Hamiltonian is constant along an extremal trajectory. However, the proofs of that fact that I found in the literature rely on the optimal trajectory being twice continuously differentiable, and on a bruteforce differentiation of the Euler-Lagrange equation. Is there a simple argument for a once continuously differentiable extremal? I tried to apply DuBois-Reymond lemma but failed.",,"['ordinary-differential-equations', 'dynamical-systems', 'calculus-of-variations', 'euler-lagrange-equation', 'complex-systems']"
49,Alligator population,Alligator population,,Hints only. I feel like I am so close. Population growth: The time rate change of an alligator population $P$ in a swamp is proportional to the square of $P$ . The swamp contained a dozen alligators in $1988$ and $2$ dozen in $1998$ $$\frac{dp}{dt} = kp^2$$ $$\int \frac{dp}{p^2} = \int k$$ $$ \frac{p^{-2+1}}{-2+1} = tk + C$$ $$ -\frac{1}{p} = tk+C$$ Use $P(0) = 12$ to solve for C. $$-\frac{1}{12} = C$$ $$p(t) = -\frac{1}{tk - \frac{1}{12}}$$ Use $P(10)=24$ to solve for k: $$24 = -\frac{1}{10k-\frac{1}{12}}$$ $$10k -\frac{1}{12} = -\frac{1}{24}$$ $$k = \frac{1}{240}$$ Now solve for $t$ with $P(t)=48$ $$48=-\frac{1}{\frac{t}{240}-\frac{1}{12}}$$ $$\frac{t}{240} - \frac{1}{12} = -\frac{1}{48}$$ $$\frac{t}{20}-1=-3$$ $$\frac{t}{20}=-2$$ $$t=-40$$ Which is obviously wrong. I assume that t should in-fact represent 1 year even though our information is given in increments of 10. I am very sure my set-up is correct and I integrated correctly...,Hints only. I feel like I am so close. Population growth: The time rate change of an alligator population in a swamp is proportional to the square of . The swamp contained a dozen alligators in and dozen in Use to solve for C. Use to solve for k: Now solve for with Which is obviously wrong. I assume that t should in-fact represent 1 year even though our information is given in increments of 10. I am very sure my set-up is correct and I integrated correctly...,P P 1988 2 1998 \frac{dp}{dt} = kp^2 \int \frac{dp}{p^2} = \int k  \frac{p^{-2+1}}{-2+1} = tk + C  -\frac{1}{p} = tk+C P(0) = 12 -\frac{1}{12} = C p(t) = -\frac{1}{tk - \frac{1}{12}} P(10)=24 24 = -\frac{1}{10k-\frac{1}{12}} 10k -\frac{1}{12} = -\frac{1}{24} k = \frac{1}{240} t P(t)=48 48=-\frac{1}{\frac{t}{240}-\frac{1}{12}} \frac{t}{240} - \frac{1}{12} = -\frac{1}{48} \frac{t}{20}-1=-3 \frac{t}{20}=-2 t=-40,['ordinary-differential-equations']
50,How to solve the system of such differential equations?,How to solve the system of such differential equations?,,"Let $x_i=x_i(t)$ be a functions and let consider the system of differential equations $\left \{ \begin{align}  &{\frac { d}{{ d}t}}x_{{0}}  =3\,x_{{1}} ,\\ &{\frac { d}{{ d}t}}x_{{1}}  =0,\\ &{\frac { d}{{ d}t}}x_{{2}}  = {\frac {2\,x_{{1}}  x_{{2}}  +x_{{3}}   }{x_{{0}}  }},\\ &{\frac { d}{{ d} t}}x_{{3}}  ={\frac {3\,x_{{1}}  x_{{3 }}  -6\, x_{{2}}   ^{2} }{x_{{0}}  }}.  \end{align} \right. $ My attempt. By brute forse I have found the first integral $$ {\frac {4\,{x_{{2}}}^{3}+{x_{{3}}}^{2}}{x_0^{2}}}=C, $$ but I cant find $x_i$ as functions of $t.$ Is it possible?",Let be a functions and let consider the system of differential equations My attempt. By brute forse I have found the first integral but I cant find as functions of Is it possible?,"x_i=x_i(t) \left \{
\begin{align} 
&{\frac { d}{{ d}t}}x_{{0}}  =3\,x_{{1}} ,\\
&{\frac { d}{{ d}t}}x_{{1}}  =0,\\
&{\frac { d}{{ d}t}}x_{{2}}  =
{\frac {2\,x_{{1}}  x_{{2}}  +x_{{3}}
  }{x_{{0}}  }},\\
&{\frac { d}{{ d}
t}}x_{{3}}  ={\frac {3\,x_{{1}}  x_{{3
}}  -6\, x_{{2}}   ^{2}
}{x_{{0}}  }}. 
\end{align} \right.
 
{\frac {4\,{x_{{2}}}^{3}+{x_{{3}}}^{2}}{x_0^{2}}}=C,
 x_i t.","['ordinary-differential-equations', 'systems-of-equations']"
51,Special function related to a nonlinear ODE,Special function related to a nonlinear ODE,,"I am interested in finding a special function solution of the following ODE $ (r S)^{\prime \prime} = - 2 r S^2 $ with initial conditions $ S(0)=1, S^\prime(0) = 0 $ . The method of Frobenius gives the infinite series $ S=  1 -\frac{{{r}^{2}}}{3}+\frac{{{r}^{4}}}{15}-\frac{11 {{r}^{6}}}{945}+\frac{16 {{r}^{8}}}{8505}-\frac{97 {{r}^{10}}}{334125} +\frac{914 {{r}^{12}}}{21049875} +H.O.T $ and the following recursion among the coefficients: $ a_n= - \frac{2}{n (n+1)}\sum\limits_{j=0}^{n-2}{\left. a_j a_{n-j-2} \right.}$ The question is: is this function related to the generalized hypergeometric functions? If yes - what is the relationship?",I am interested in finding a special function solution of the following ODE with initial conditions . The method of Frobenius gives the infinite series and the following recursion among the coefficients: The question is: is this function related to the generalized hypergeometric functions? If yes - what is the relationship?," (r S)^{\prime \prime} = - 2 r S^2   S(0)=1, S^\prime(0) = 0  
S= 
1 -\frac{{{r}^{2}}}{3}+\frac{{{r}^{4}}}{15}-\frac{11 {{r}^{6}}}{945}+\frac{16 {{r}^{8}}}{8505}-\frac{97 {{r}^{10}}}{334125} +\frac{914 {{r}^{12}}}{21049875} +H.O.T
  a_n= - \frac{2}{n (n+1)}\sum\limits_{j=0}^{n-2}{\left. a_j a_{n-j-2} \right.}","['ordinary-differential-equations', 'special-functions', 'hypergeometric-function', 'frobenius-method']"
52,How to solve this weird ODE?,How to solve this weird ODE?,,"I came across this differential equation which I'm having trouble finding an analytic solution to: $$\frac{dy}{dx}=\frac{A}{xy}+\frac{B}{(xy)^2}$$ I'm trying to solve for y. I have initial conditions as $x_0=0.02$ and $y_0=100000$ , and A and B are known constants. I don't have a very heavy differential equations background so all I know is that I can't use separation of variables--what kind of method should I use to solve this equation? Thank you!!","I came across this differential equation which I'm having trouble finding an analytic solution to: I'm trying to solve for y. I have initial conditions as and , and A and B are known constants. I don't have a very heavy differential equations background so all I know is that I can't use separation of variables--what kind of method should I use to solve this equation? Thank you!!",\frac{dy}{dx}=\frac{A}{xy}+\frac{B}{(xy)^2} x_0=0.02 y_0=100000,['ordinary-differential-equations']
53,Linear Algebra 4th edition by Friedberg exercise 12 in section 2.7,Linear Algebra 4th edition by Friedberg exercise 12 in section 2.7,,"Let $V$ be the solution space of an $n$ th-order homogeneous linear differential equation with constant coefficients having auxiliary polynomial $p(t)$ . Prove that if $p(t) = g(t)h(t)$ , where $g(t)$ and $h(t)$ are polynomials of postive degree, then $$N(h(D)) = R(g(D_V)) = g(D)(V),$$ where $D_V : V \to V$ is defined by $D_V(x) = x'$ for $x\in V$ . Hint: first prove $g(D)(V) \subseteq N(h(D))$ . Then, prove that the two spaces have the same finite dimension. Let $x \in g(D)(V)$ . Then, there exists $y \in V$ , such that $g(D)(y) =x$ . Note that $p(D)(y) = 0$ since $y \in V$ . This implies that $$p(D)(y) = g(D)h(D)(y) = h(D)g(D)(y) = h(D)(x) = 0.$$ Therefore, $x\in N(h(D))$ . I am struggling with the second part of the hint. First, note that by dimension theorem, $\dim(V) = \dim(N(g(D_V)) + \dim(R(g(D_V))$ , and that $N(g(D_V)) = N(g(D))$ since $N(g(D)) \subseteq V$ , and that I have a theorem that the solution space $N(g(D))$ has a dimension equal to the degree of $g$ . Taken all together, $\dim(N(g(D_V)) = g$ (let $g$ be the degree of $g$ ), and this implies that $\dim(R(g(D_V)) = n-g$ . Therefore, if I show that $\dim(N(h(D)) = n-g$ , it finishes the proof. I have a theorem that if $p(D) = g(D)h(D)$ and $g(D)$ (or $h(D)$ ) is onto, then $$\dim(N(p(D)) = \dim(N(g(D)) + \dim(N(h(D)).$$ I know that $\dim(N(p(D)) = n$ and $\dim(N(g(D)) = g$ . But, I do not know that $g(D)$ (or $h(D)$ ) is onto. How can I show that either one operator is onto or is there another way to reach the answer?","Let be the solution space of an th-order homogeneous linear differential equation with constant coefficients having auxiliary polynomial . Prove that if , where and are polynomials of postive degree, then where is defined by for . Hint: first prove . Then, prove that the two spaces have the same finite dimension. Let . Then, there exists , such that . Note that since . This implies that Therefore, . I am struggling with the second part of the hint. First, note that by dimension theorem, , and that since , and that I have a theorem that the solution space has a dimension equal to the degree of . Taken all together, (let be the degree of ), and this implies that . Therefore, if I show that , it finishes the proof. I have a theorem that if and (or ) is onto, then I know that and . But, I do not know that (or ) is onto. How can I show that either one operator is onto or is there another way to reach the answer?","V n p(t) p(t) = g(t)h(t) g(t) h(t) N(h(D)) = R(g(D_V)) = g(D)(V), D_V : V \to V D_V(x) = x' x\in V g(D)(V) \subseteq N(h(D)) x \in g(D)(V) y \in V g(D)(y) =x p(D)(y) = 0 y \in V p(D)(y) = g(D)h(D)(y) = h(D)g(D)(y) = h(D)(x) = 0. x\in N(h(D)) \dim(V) = \dim(N(g(D_V)) + \dim(R(g(D_V)) N(g(D_V)) = N(g(D)) N(g(D)) \subseteq V N(g(D)) g \dim(N(g(D_V)) = g g g \dim(R(g(D_V)) = n-g \dim(N(h(D)) = n-g p(D) = g(D)h(D) g(D) h(D) \dim(N(p(D)) = \dim(N(g(D)) + \dim(N(h(D)). \dim(N(p(D)) = n \dim(N(g(D)) = g g(D) h(D)","['linear-algebra', 'ordinary-differential-equations']"
54,Solution to $\frac{d^n y}{dx^n} = y$ for integer $n$,Solution to  for integer,\frac{d^n y}{dx^n} = y n,"In short, my question is about the solution of $\frac{d^n y}{dx^n} = y$ for positive integer $n$ , where $y$ is real. For example, if $n = 2$ , the solution is $y = c_1e^x+c_2e^{-x}$ . The work that I and my old friend, WolframAlpha, made on this is such: Assume that a solution is proportional to $e^{\lambda x}$ , with $\lambda$ a constant. Substituting that into the original equation finds $$\frac{d^n}{dx^n} (e^{\lambda x}) - e^{\lambda x} = 0 \to \lambda^n e^{\lambda x}- e^{\lambda x} = 0$$ This can be factored as $$(\lambda^n -1)e^{\lambda x} = 0$$ Since $e^{\lambda x}$ cannot equal $0$ , $\lambda^n -1 = 0$ . Solving this for $\lambda$ is equivalent to finding the group of the $n$ th roots of unity, so $$\lambda_k = e^{2k\pi i/n}, k \in \{1, 2, ..., n\}$$ The overall solution is then $$y = \sum_{k=1}^{n} c_k e^{\lambda_k x}$$ It seems like we are done. However, this can in fact be written somewhat differently (I'm cautious about calling it simplified), as the sums of $e^{px}\cos(qx)$ and $e^{px}\sin(qx)$ . For example, if $n = 3$ , $$y = c_1 e^x+c_2 e^{-x/2} \cos\left( \frac{x\sqrt{3}}{2}\right) + c_3 e^{-x/2} \sin\left( \frac{x\sqrt{3}}{2}\right)$$ How can I get from the first representation of $y$ to the second one for higher $n$ ? For anyone interested, the Mathematica code for finding $y$ is DSolve[D[y[x], {x, 5}] == y[x], y[x], x]","In short, my question is about the solution of for positive integer , where is real. For example, if , the solution is . The work that I and my old friend, WolframAlpha, made on this is such: Assume that a solution is proportional to , with a constant. Substituting that into the original equation finds This can be factored as Since cannot equal , . Solving this for is equivalent to finding the group of the th roots of unity, so The overall solution is then It seems like we are done. However, this can in fact be written somewhat differently (I'm cautious about calling it simplified), as the sums of and . For example, if , How can I get from the first representation of to the second one for higher ? For anyone interested, the Mathematica code for finding is DSolve[D[y[x], {x, 5}] == y[x], y[x], x]","\frac{d^n y}{dx^n} = y n y n = 2 y = c_1e^x+c_2e^{-x} e^{\lambda x} \lambda \frac{d^n}{dx^n} (e^{\lambda x}) - e^{\lambda x} = 0 \to \lambda^n e^{\lambda x}- e^{\lambda x} = 0 (\lambda^n -1)e^{\lambda x} = 0 e^{\lambda x} 0 \lambda^n -1 = 0 \lambda n \lambda_k = e^{2k\pi i/n}, k \in \{1, 2, ..., n\} y = \sum_{k=1}^{n} c_k e^{\lambda_k x} e^{px}\cos(qx) e^{px}\sin(qx) n = 3 y = c_1 e^x+c_2 e^{-x/2} \cos\left( \frac{x\sqrt{3}}{2}\right) + c_3 e^{-x/2} \sin\left( \frac{x\sqrt{3}}{2}\right) y n y","['ordinary-differential-equations', 'roots-of-unity']"
55,How do i compute this differential equation?,How do i compute this differential equation?,,"$$ y''(x) - \frac{B^2}{\cosh^2(Bx)}y(x)= Ay(x) $$ I know that can be solved analytically, but what method can I use?","I know that can be solved analytically, but what method can I use?", y''(x) - \frac{B^2}{\cosh^2(Bx)}y(x)= Ay(x) ,"['ordinary-differential-equations', 'hypergeometric-function', 'hyperbolic-functions', 'sturm-liouville']"
56,Solving system of linear second order differential equations,Solving system of linear second order differential equations,,"Solve this system of differential equations: $$y_1'' + y_2' + 3y_1 = e^{-x}$$ $$y_2''-4y_1' + 3y_2 = \sin(2x)$$ My try: Using $y_1' = p$ and $y_2'= q$ , we get: $$\begin{pmatrix} p' \\q'\\y_1'\\y_2'\end{pmatrix} =\begin{pmatrix}  0 & -1 & -3 &  0 \\  4 & 0 & 0 &  -3 \\ 1 & 0 & 0 &  0 \\  0 & 1 & 0  & 0      \end{pmatrix}\begin{pmatrix} p \\q\\y_1\\y_2\end{pmatrix} + \begin{pmatrix} e^{-x} \\\sin(2x)\\0\\0\end{pmatrix}$$ I solved the homogeneous part and find : $$ y_h = c_1e^{-ix}\begin{pmatrix} 1/2 \\-i\\i/2\\1\end{pmatrix} + c_2e^{ix}\begin{pmatrix} 1/2 \\i\\-i/2\\1\end{pmatrix} + c_3e^{3ix}\begin{pmatrix} -3/2 \\3i\\i/2\\1\end{pmatrix} + c_4e^{-3ix}\begin{pmatrix} -3/2 \\-3i\\-i/2\\1\end{pmatrix}$$ I don't know how to solve non-homogeneous part of the equation.","Solve this system of differential equations: My try: Using and , we get: I solved the homogeneous part and find : I don't know how to solve non-homogeneous part of the equation.","y_1'' + y_2' + 3y_1 = e^{-x} y_2''-4y_1' + 3y_2 = \sin(2x) y_1' = p y_2'= q \begin{pmatrix} p' \\q'\\y_1'\\y_2'\end{pmatrix} =\begin{pmatrix}
 0 & -1 & -3 &  0 \\
 4 & 0 & 0 &  -3 \\
1 & 0 & 0 &  0 \\
 0 & 1 & 0  & 0    
 \end{pmatrix}\begin{pmatrix} p \\q\\y_1\\y_2\end{pmatrix} + \begin{pmatrix} e^{-x} \\\sin(2x)\\0\\0\end{pmatrix}  y_h = c_1e^{-ix}\begin{pmatrix} 1/2 \\-i\\i/2\\1\end{pmatrix} + c_2e^{ix}\begin{pmatrix} 1/2 \\i\\-i/2\\1\end{pmatrix} + c_3e^{3ix}\begin{pmatrix} -3/2 \\3i\\i/2\\1\end{pmatrix} + c_4e^{-3ix}\begin{pmatrix} -3/2 \\-3i\\-i/2\\1\end{pmatrix}","['ordinary-differential-equations', 'eigenvalues-eigenvectors']"
57,How to convert delay differential equations to a discrete-time agent-based model?,How to convert delay differential equations to a discrete-time agent-based model?,,"I am building an agent-based model in discrete time. To do this, I need to convert a continuous time delay and ordinary differential equations to discrete-time equations. Here is the equation system: $$ 	\frac{\mathrm{d} S_\text{L}^\text{Q}}{\mathrm{d}t} = 	\underbrace{B_\text{L}(t)}_\text{birth} -	\underbrace{d_\text{L} S_\text{L}^\text{Q}}_\text{mortality} -	\underbrace{a_\text{L} N_\text{H} S_\text{L}^\text{Q}}_\text{encounter success} ,\\ 	\frac{\mathrm{d} S_\text{L}^\text{A}}{\mathrm{d}t} = 	\underbrace{a_\text{L} N_\text{H} S_\text{L}^\text{Q}}_\text{encounter success} -	\underbrace{d_\text{L} S_\text{L}^\text{A}}_\text{mortality} -	\underbrace{a_\text{L} N_\text{H} e^{-d_\text{L} τ} S_\text{L}^\text{Q} (t-τ)}_\text{feeding success} , \\ 	\frac{\mathrm{d} S_\text{L}^\text{F}}{\mathrm{d}t} = 	\underbrace{a_\text{L} N_\text{H} e^{-d_\text{L} τ} S_\text{L}^\text{Q}(t-τ)}_\text{feeding success} -	\underbrace{d_\text{N} S_\text{L}^\text{F}}_\text{mortality} $$ How can I rewrite these continuous-time equations to discrete-time equations? What discretization method should be used? In the agent-based model, each time step represents one day. I am beginner in ODEs and DDEs.","I am building an agent-based model in discrete time. To do this, I need to convert a continuous time delay and ordinary differential equations to discrete-time equations. Here is the equation system: How can I rewrite these continuous-time equations to discrete-time equations? What discretization method should be used? In the agent-based model, each time step represents one day. I am beginner in ODEs and DDEs.","
	\frac{\mathrm{d} S_\text{L}^\text{Q}}{\mathrm{d}t}
=
	\underbrace{B_\text{L}(t)}_\text{birth}
-	\underbrace{d_\text{L} S_\text{L}^\text{Q}}_\text{mortality}
-	\underbrace{a_\text{L} N_\text{H} S_\text{L}^\text{Q}}_\text{encounter success}
,\\
	\frac{\mathrm{d} S_\text{L}^\text{A}}{\mathrm{d}t}
=
	\underbrace{a_\text{L} N_\text{H} S_\text{L}^\text{Q}}_\text{encounter success}
-	\underbrace{d_\text{L} S_\text{L}^\text{A}}_\text{mortality}
-	\underbrace{a_\text{L} N_\text{H} e^{-d_\text{L} τ} S_\text{L}^\text{Q} (t-τ)}_\text{feeding success}
, \\
	\frac{\mathrm{d} S_\text{L}^\text{F}}{\mathrm{d}t}
=
	\underbrace{a_\text{L} N_\text{H} e^{-d_\text{L} τ} S_\text{L}^\text{Q}(t-τ)}_\text{feeding success}
-	\underbrace{d_\text{N} S_\text{L}^\text{F}}_\text{mortality}
","['ordinary-differential-equations', 'biology', 'delay-differential-equations']"
58,First integral for non-linear system of equations.,First integral for non-linear system of equations.,,"I have the dynamics \begin{align} x'&=y\\ y'&=-y-x-x^2\tag1 \end{align} which represents a one-dimensional mechanical system with velocity $y$ and coordinate $x$ , or in other words the second order equation $$x''=-y-x-x^2 \Longleftrightarrow x''+g(x)=0 \tag2$$ where $g(x)=y+x+x^2.$ Since the origin is an asymptotically stable equilibrium point, I want to find its region of attraction using an appropriate Lyapunov function $V(x,y)$ . So in the book they give the formula $$V(x,y)=\text{Kinetic energy}+\text{Potential energy}=\frac{y^2}{2}+\int_0^xg(z)dz, \tag3$$ plugging in my expression for $g(z)$ I get $$V(x,y)=\frac{y^2}{2}+\int_0^xy+z+z^2 \ dz=\frac{y^2}{2}+\frac{x^2}{2}+\frac{x^3}{3}+xy.\tag4$$ However, according to the book the $xy$ should not be at the end. I assume it is because they get their $g(x)$ to be $g(x)=x+x^2$ . How come?","I have the dynamics which represents a one-dimensional mechanical system with velocity and coordinate , or in other words the second order equation where Since the origin is an asymptotically stable equilibrium point, I want to find its region of attraction using an appropriate Lyapunov function . So in the book they give the formula plugging in my expression for I get However, according to the book the should not be at the end. I assume it is because they get their to be . How come?","\begin{align}
x'&=y\\
y'&=-y-x-x^2\tag1
\end{align} y x x''=-y-x-x^2 \Longleftrightarrow x''+g(x)=0 \tag2 g(x)=y+x+x^2. V(x,y) V(x,y)=\text{Kinetic energy}+\text{Potential energy}=\frac{y^2}{2}+\int_0^xg(z)dz, \tag3 g(z) V(x,y)=\frac{y^2}{2}+\int_0^xy+z+z^2 \ dz=\frac{y^2}{2}+\frac{x^2}{2}+\frac{x^3}{3}+xy.\tag4 xy g(x) g(x)=x+x^2",['ordinary-differential-equations']
59,Self-similar solution of the momentum equation,Self-similar solution of the momentum equation,,"I have used the steam functions $u = \psi_{y}$ and $v = -\psi_{x}$ to transform the momentum equations to the following form $$\rho\left(\psi_{y}\psi_{xy} - \psi_{x}\psi_{yy}\right)=-p_{x}+\mu\left(\psi_{xxy}+\psi_{yyy} \right) $$ $$\rho\left(-\psi_{y}\psi_{xx} + \psi_{x}\psi_{xy}\right)=-p_{y}+\mu\left(\psi_{xxx}+\psi_{xyy} \right) $$ I eliminated the pressure term via cross-differentiation and have obtained the final form of the equation to be as follows $$\rho\left(\psi_{y}\left(\psi_{xyy}+\psi_{xxx} \right) - \psi_{x}\left(\psi_{yyy}+\psi_{xxy} \right) \right) = \mu\left(\psi_{xxxx} +\psi_{yyyy} + 2\psi_{xxyy} \right)\tag{1}, $$ which can be rewritten with $\nabla = \left<\partial_{x},\partial_{y} \right>$ as follows $$\rho\left(\psi_{y}\nabla^{2}\psi_{x} - \psi_{x}\nabla^{2}\psi_{y} \right) = \mu\nabla^{4}\psi$$ Now, this is a leading edge problem of a fluid flowing over a flat plate with viscous forces dominating. The self-similar variable was found to be $$\eta = \frac{y}{x}\tag{2},$$ and the self-similar stream function has the following form $$f(\eta) = \frac{\psi}{Ux}.\tag{3} $$ Substituting $(2)$ and $(3)$ into $(1)$ transforms the PDE to an ODE a follows $$\left(1+\eta^{2}\right)^{2}f_{\eta\eta\eta\eta}+8\eta\left(1+\eta^{2}\right)f_{\eta\eta\eta} + 4\left(1+3\eta^{2}\right)f_{\eta\eta} + Re\left[2\eta ff_{\eta}+\left(1+\eta^{2}\right)\left(ff_{\eta\eta\eta} + f_{\eta}f_{\eta\eta}\right)\right]=0,\tag{4}$$ where $Re=\frac{\rho Ux}{\mu}$ Could someone please show me how this transformation is done. I tried proceeding via the chain rule and was able to compute all the derivatives but failed to obtain the final form as shown in $(4)$ . Any help is appreciated.","I have used the steam functions and to transform the momentum equations to the following form I eliminated the pressure term via cross-differentiation and have obtained the final form of the equation to be as follows which can be rewritten with as follows Now, this is a leading edge problem of a fluid flowing over a flat plate with viscous forces dominating. The self-similar variable was found to be and the self-similar stream function has the following form Substituting and into transforms the PDE to an ODE a follows where Could someone please show me how this transformation is done. I tried proceeding via the chain rule and was able to compute all the derivatives but failed to obtain the final form as shown in . Any help is appreciated.","u = \psi_{y} v = -\psi_{x} \rho\left(\psi_{y}\psi_{xy} - \psi_{x}\psi_{yy}\right)=-p_{x}+\mu\left(\psi_{xxy}+\psi_{yyy} \right)  \rho\left(-\psi_{y}\psi_{xx} + \psi_{x}\psi_{xy}\right)=-p_{y}+\mu\left(\psi_{xxx}+\psi_{xyy} \right)  \rho\left(\psi_{y}\left(\psi_{xyy}+\psi_{xxx} \right) - \psi_{x}\left(\psi_{yyy}+\psi_{xxy} \right) \right) = \mu\left(\psi_{xxxx} +\psi_{yyyy} + 2\psi_{xxyy} \right)\tag{1},  \nabla = \left<\partial_{x},\partial_{y} \right> \rho\left(\psi_{y}\nabla^{2}\psi_{x} - \psi_{x}\nabla^{2}\psi_{y} \right) = \mu\nabla^{4}\psi \eta = \frac{y}{x}\tag{2}, f(\eta) = \frac{\psi}{Ux}.\tag{3}  (2) (3) (1) \left(1+\eta^{2}\right)^{2}f_{\eta\eta\eta\eta}+8\eta\left(1+\eta^{2}\right)f_{\eta\eta\eta} + 4\left(1+3\eta^{2}\right)f_{\eta\eta} + Re\left[2\eta ff_{\eta}+\left(1+\eta^{2}\right)\left(ff_{\eta\eta\eta} + f_{\eta}f_{\eta\eta}\right)\right]=0,\tag{4} Re=\frac{\rho Ux}{\mu} (4)","['ordinary-differential-equations', 'partial-differential-equations', 'fluid-dynamics']"
60,Find the maximum velocity of this particle,Find the maximum velocity of this particle,,"Question: Let the position of the particle be $\mathbf r(t) = (x(t),y(t))$ . The equations of motion are $$\ddot x = \frac{ay}{x^2+y^2} \qquad \ddot y = -\frac{ax}{x^2+y^2}$$ or equivalently, $$\ddot {\mathbf r} = -a \nabla \tan^{-1}\bigg(\frac yx\bigg)$$ where $a>0$ is a constant. The initial conditions are $\mathbf r(0) = (x_0,y_0)$ and $\dot{\mathbf r}(t) = \mathbf 0$ , where $x_0,y_0>0$ . Find the maximum speed attained by the particle in its subsequent motion. Attempt: So we want to find the maximum value of $\dot x^2 + \dot y^2$ (which is the squared velocity). Call this quantity $Q^2$ (of course, the actual quantity we are after is $Q$ ). I started off by observing that $$y\ddot x - x \ddot y = a$$ The left hand side can be factorised: \begin{align} \implies & \frac{d}{dt}\big(y\dot x - x\dot y \big) = a \\ \implies & y\dot x - x\dot y = at + B \end{align} for some constant $B$ . Using initial conditions, we see that $B=0$ , so $$\implies y\dot x - x\dot y = at$$ Now differentiating $Q^2$ with respect to $t$ : $$\frac{dQ^2}{dt} = 2\dot x\ddot x + 2\dot y \ddot y = 2\dot x \frac{ay}{x^2+y^2} - 2\dot y \frac{ax}{x^2+y^2} = \frac{2a(y\dot x - x\dot y)}{x^2+y^2} = \frac{2a^2t}{x^2+y^2}$$ So it follows that $\frac{dQ^2}{dt}>0$ for all $t>0$ . This implies that the speed is forever increasing - what have I done wrong? What is the correct approach?","Question: Let the position of the particle be . The equations of motion are or equivalently, where is a constant. The initial conditions are and , where . Find the maximum speed attained by the particle in its subsequent motion. Attempt: So we want to find the maximum value of (which is the squared velocity). Call this quantity (of course, the actual quantity we are after is ). I started off by observing that The left hand side can be factorised: for some constant . Using initial conditions, we see that , so Now differentiating with respect to : So it follows that for all . This implies that the speed is forever increasing - what have I done wrong? What is the correct approach?","\mathbf r(t) = (x(t),y(t)) \ddot x = \frac{ay}{x^2+y^2} \qquad \ddot y = -\frac{ax}{x^2+y^2} \ddot {\mathbf r} = -a \nabla \tan^{-1}\bigg(\frac yx\bigg) a>0 \mathbf r(0) = (x_0,y_0) \dot{\mathbf r}(t) = \mathbf 0 x_0,y_0>0 \dot x^2 + \dot y^2 Q^2 Q y\ddot x - x \ddot y = a \begin{align}
\implies & \frac{d}{dt}\big(y\dot x - x\dot y \big) = a \\
\implies & y\dot x - x\dot y = at + B
\end{align} B B=0 \implies y\dot x - x\dot y = at Q^2 t \frac{dQ^2}{dt} = 2\dot x\ddot x + 2\dot y \ddot y = 2\dot x \frac{ay}{x^2+y^2} - 2\dot y \frac{ax}{x^2+y^2} = \frac{2a(y\dot x - x\dot y)}{x^2+y^2} = \frac{2a^2t}{x^2+y^2} \frac{dQ^2}{dt}>0 t>0","['ordinary-differential-equations', 'classical-mechanics']"
61,Eigenvalues and eigenfunctions of Airys; DE,Eigenvalues and eigenfunctions of Airys; DE,,"We have following ODE $$ -y'' + xy = - \lambda y $$ $$ x \in (0,1), \; \;\;\; y(0)=y(1)=0$$ I proved using integration by parts that the eigenvalues are positive. Is there a way to compute or plot the eigenvalues in matlab?",We have following ODE I proved using integration by parts that the eigenvalues are positive. Is there a way to compute or plot the eigenvalues in matlab?," -y'' + xy = - \lambda y   x \in (0,1), \; \;\;\; y(0)=y(1)=0",['ordinary-differential-equations']
62,2nd order linear ODE non constant coefficient solution methods,2nd order linear ODE non constant coefficient solution methods,,"I'm working on a project and I've come across a seemingly standard ODE, however I have no idea how to solve it.  The equation in question is $$ \epsilon'' + \left[g(\tau) + \frac{\beta}{\tau}\right]\epsilon' + \left[\frac{\gamma}{\tau^{4/3}}\right]\epsilon = 0 $$ where $g(\tau) = \alpha \cos(\tau)$ or $\delta\cos(\tau)\sin(\tau)$ (there are two cases).  I've tried a series solution but unsurprisingly it's pretty messy and doesn't really solve anything, and I've tried Laplace transforms but unfortunately they're divergent for negative powers of $\tau$ . Does anybody have recommendations for methods I can try, or even how to make a decent ansatz?  This is just a step in my project and it's really presenting a roadblock for me, so any help is appreciated.","I'm working on a project and I've come across a seemingly standard ODE, however I have no idea how to solve it.  The equation in question is where or (there are two cases).  I've tried a series solution but unsurprisingly it's pretty messy and doesn't really solve anything, and I've tried Laplace transforms but unfortunately they're divergent for negative powers of . Does anybody have recommendations for methods I can try, or even how to make a decent ansatz?  This is just a step in my project and it's really presenting a roadblock for me, so any help is appreciated.","
\epsilon'' + \left[g(\tau) + \frac{\beta}{\tau}\right]\epsilon' + \left[\frac{\gamma}{\tau^{4/3}}\right]\epsilon = 0
 g(\tau) = \alpha \cos(\tau) \delta\cos(\tau)\sin(\tau) \tau",['ordinary-differential-equations']
63,Proportion of a compartment's mass which originates from another compartment,Proportion of a compartment's mass which originates from another compartment,,"System & Objective I have the following open system (mass flow) with 2 compartments $A$ and $B$ and constant flow rates $a,b,\alpha,\beta,\gamma,\delta$ . I would like an expression for "" the proportion of $A(t)$ at time $t$ which originated from $B(t)$ "", denoted $A_B(t)$ . We can assume originally no mass in $A$ came from $B$ -- i.e. $A_B(t=0) = 0$ . Approach I assume the proportion could be expressed as a fraction, $$ A_B(t) = \frac{N(t)}{D(t)} $$ where the denominator is simply the current mass in $A$ , $D(t) = A(t)$ ; and the numerator $N(t)$ is the absolute mass currently in $A$ which originated from $B$ . I'm struggling to derive an expression for $N(t)$ . I defined the cumulative mass which enters $A$ from $B$ as $$ T_{A_B}(t) = \int_{0}^{t} b B(\tau) d\tau $$ but we also need an expression for the cumulative mass which exits $A$ after entering from $B$ , denoted $E_{A_B}(t)$ . I would then define $N(t) = T_{A_B}(t) - E_{A_B}(t)$ . I had three approaches to defining $E_{A_B}(t)$ ... is any correct? Cumulative Exit Attempt 1: \begin{equation} \begin{aligned} E_{A_B}(t) &= \int_{0}^{t} (\alpha + a) T_{A_B}(\tau) d\tau \\ &= \int_{0}^{t} (\alpha + a) \left(\int_{0}^{\tau} b B(s) ds \right) d\tau \end{aligned} \end{equation} But this double integral seems wrong and would blow up for large $t$ . Cumulative Exit Attempt 2: \begin{equation} E_{A_B}(t) = \int_{0}^{t} (\alpha + a) b B(\tau) d\tau \end{equation} But this doesn't seem right either, as the exit only depends on the current value of $B(t)$ . Cumulative Exit Attempt 3: \begin{equation} E_{A_B}(t) = \int_{0}^{t} (\alpha + a) A(\tau) A_B(\tau) d\tau \end{equation} But this would result in recursive definition of $A_B(t)$ , and I'm not sure how I could rearrange to isolate $A_B(t)$ from the the LHS and the RHS integral. Overall, is this a reasonable approach? Any ideas how to derive either $E_{A_B}(t)$ , or $A_B(t)$ by another approach altogether? I feel like I should be using Laplace. Thanks,","System & Objective I have the following open system (mass flow) with 2 compartments and and constant flow rates . I would like an expression for "" the proportion of at time which originated from "", denoted . We can assume originally no mass in came from -- i.e. . Approach I assume the proportion could be expressed as a fraction, where the denominator is simply the current mass in , ; and the numerator is the absolute mass currently in which originated from . I'm struggling to derive an expression for . I defined the cumulative mass which enters from as but we also need an expression for the cumulative mass which exits after entering from , denoted . I would then define . I had three approaches to defining ... is any correct? Cumulative Exit Attempt 1: But this double integral seems wrong and would blow up for large . Cumulative Exit Attempt 2: But this doesn't seem right either, as the exit only depends on the current value of . Cumulative Exit Attempt 3: But this would result in recursive definition of , and I'm not sure how I could rearrange to isolate from the the LHS and the RHS integral. Overall, is this a reasonable approach? Any ideas how to derive either , or by another approach altogether? I feel like I should be using Laplace. Thanks,","A B a,b,\alpha,\beta,\gamma,\delta A(t) t B(t) A_B(t) A B A_B(t=0) = 0  A_B(t) = \frac{N(t)}{D(t)}  A D(t) = A(t) N(t) A B N(t) A B  T_{A_B}(t) = \int_{0}^{t} b B(\tau) d\tau  A B E_{A_B}(t) N(t) = T_{A_B}(t) - E_{A_B}(t) E_{A_B}(t) \begin{equation}
\begin{aligned}
E_{A_B}(t) &= \int_{0}^{t} (\alpha + a) T_{A_B}(\tau) d\tau \\
&= \int_{0}^{t} (\alpha + a) \left(\int_{0}^{\tau} b B(s) ds \right) d\tau
\end{aligned}
\end{equation} t \begin{equation}
E_{A_B}(t) = \int_{0}^{t} (\alpha + a) b B(\tau) d\tau
\end{equation} B(t) \begin{equation}
E_{A_B}(t) = \int_{0}^{t} (\alpha + a) A(\tau) A_B(\tau) d\tau
\end{equation} A_B(t) A_B(t) E_{A_B}(t) A_B(t)","['ordinary-differential-equations', 'dynamical-systems']"
64,Solution to this ODE that tends to a constant at infinity,Solution to this ODE that tends to a constant at infinity,,"I started with the following 2nd order PDE $$ -\frac{x}{x-1}\partial_y^2\phi+\frac{1}{x^2} \partial_x (x(x-1)\partial_x \phi) = \mu^2 \phi$$ which is clearly separable. I want a solution that asymptotes to a $y$ -dependent constant at $x$ -infinity. I therefore tried the form $\phi = X(x)Y(y)$ , with $X(x\rightarrow \infty) \rightarrow1$ . Looking at this limit above we see that it can only hold if $\partial_y^2 Y = -\mu^2 Y$ . Subbing this back in, I end up with an ODE for $X$ : $$ \frac{1}{x^2} \partial_x (x(x-1)\partial_x X) = -\frac{\mu^2}{x-1} X$$ I've tried solving this numerically (obviously staying away from $x=1$ ) on Mathematica. For any initial conditions I give at some finite $x$ , the solution seems to always die off oscillating around zero, as in plot. So I'm wondering why it seem like the boundary condition at infinity $X\rightarrow $ constant seems allowed. Have I fooled myself? If I expand the ODE for large $x$ to 'first order' I have $$\left( 1- \frac{1}{x}\right)\partial_x^2 X + \frac{2}{x} \partial_x X = -\frac{\mu^2}{x} X, \quad \quad x \rightarrow \infty$$ Mathematica gives exact solutions for this in terms of Bessel functions and these indeed match the asymptotic behaviour in the plot above. However if I stopped at 'zeroth order' I would just have $\partial_x^2 X = 0$ and a self-consistent solution $X=$ constant (a term linear in $x$ would not be self-consistent with ignoring the first derivative term). Where is my reasoning wrong? Is there no solution with the boundary condition I desire? (If there is a way of enforcing a boundary condition at infinity numerically I am unaware of it)","I started with the following 2nd order PDE which is clearly separable. I want a solution that asymptotes to a -dependent constant at -infinity. I therefore tried the form , with . Looking at this limit above we see that it can only hold if . Subbing this back in, I end up with an ODE for : I've tried solving this numerically (obviously staying away from ) on Mathematica. For any initial conditions I give at some finite , the solution seems to always die off oscillating around zero, as in plot. So I'm wondering why it seem like the boundary condition at infinity constant seems allowed. Have I fooled myself? If I expand the ODE for large to 'first order' I have Mathematica gives exact solutions for this in terms of Bessel functions and these indeed match the asymptotic behaviour in the plot above. However if I stopped at 'zeroth order' I would just have and a self-consistent solution constant (a term linear in would not be self-consistent with ignoring the first derivative term). Where is my reasoning wrong? Is there no solution with the boundary condition I desire? (If there is a way of enforcing a boundary condition at infinity numerically I am unaware of it)"," -\frac{x}{x-1}\partial_y^2\phi+\frac{1}{x^2} \partial_x (x(x-1)\partial_x \phi) = \mu^2 \phi y x \phi = X(x)Y(y) X(x\rightarrow \infty) \rightarrow1 \partial_y^2 Y = -\mu^2 Y X  \frac{1}{x^2} \partial_x (x(x-1)\partial_x X) = -\frac{\mu^2}{x-1} X x=1 x X\rightarrow  x \left( 1- \frac{1}{x}\right)\partial_x^2 X + \frac{2}{x} \partial_x X = -\frac{\mu^2}{x} X, \quad \quad x \rightarrow \infty \partial_x^2 X = 0 X= x","['ordinary-differential-equations', 'asymptotics']"
65,"1st order differential linear equation, question on absolute value","1st order differential linear equation, question on absolute value",,"I'm trying to find the general solution to this equation: $$x \frac{dy}{dx}+3(y+x^2)=\frac{\sin(x)}{x} $$ Standard form puts it like this: $$\frac{dy}{dx}+\frac{3}{x}y=\frac{\sin(x)-3x^3}{x^2} $$ To determine the integrating factor I did $e^{\int{3/x}\,dx}$ and got $e^{\ln{\lvert x\rvert}^3}$ . Does this not simplify to $\lvert x\rvert^3$ ? In all the online calculators I've used, they've ignored the absolute value? The problem would be much easier if that was the case but I'm not convinced. I wouldn't know how to integrate the following with the absolute value: $$ \int{\frac{\lvert x\rvert^3}{x^2}\cdot (\sin(x)- 3x^3)\,dx}$$ I'd appreciate any help.","I'm trying to find the general solution to this equation: Standard form puts it like this: To determine the integrating factor I did and got . Does this not simplify to ? In all the online calculators I've used, they've ignored the absolute value? The problem would be much easier if that was the case but I'm not convinced. I wouldn't know how to integrate the following with the absolute value: I'd appreciate any help.","x \frac{dy}{dx}+3(y+x^2)=\frac{\sin(x)}{x}  \frac{dy}{dx}+\frac{3}{x}y=\frac{\sin(x)-3x^3}{x^2}  e^{\int{3/x}\,dx} e^{\ln{\lvert x\rvert}^3} \lvert x\rvert^3  \int{\frac{\lvert x\rvert^3}{x^2}\cdot (\sin(x)- 3x^3)\,dx}","['integration', 'ordinary-differential-equations', 'indefinite-integrals', 'absolute-value']"
66,Solving second order nonhomogeneous ODE where the RHS is a random process,Solving second order nonhomogeneous ODE where the RHS is a random process,,"Context: I'm trying to characterize the metastability behavior of a digital latch . I'm modeling two cross-coupled inverters as RC circuits with negative gain. One of the inverters has a source of noise at its input (denoted $v_n(t)$ ) which is a gaussian white noise process. This gives me a system of two ODE's: $$ V_I(t) + \tau \frac{dV_I(t)}{dt}=-AV_O(t) $$ $$ V_O(t) + \tau \frac{dV_O(t)}{dt}=-A\left(V_I(t)+v_n(t)\right) $$ Substituting and re-arranging for $V_O$ yields $$ V_O''+\frac{2}{\tau}V_O'+\frac{1-A^2}{\tau^2}V_O=\frac{-A}{\tau^2}\left(v_n+v_n'\right) $$ In the absences of this noise source, the second order homogeneous equation has a simple complementary answer $$ y_c=C_1e^{-t/\tau_1}+C_2e^{t/\tau_2}  $$ where $$ \tau_1 = \frac{\tau}{A+1}, \tau_2 = \frac{\tau}{A-1} $$ For simplicity assume $A \gg 1$ so $\tau_1=\tau_2=\tau'=\frac{\tau}{A}$ In general, I'm concerned with initial conditions of the form $V_O(0)=-Av_s,V_O'(0)=0$ (i.e. some sampled, constant voltage at $V_I$ gained up to $V_O$ ). This gives a solution (again approximating) of $$ V_O(t)=-A\frac{v_s}{2}\left(e^{\frac{t}{\tau'}}+e^{\frac{-t}{\tau'}}\right) $$ And finally, the quantity I'm trying to characterize is the time delay to some fixed voltage, that is $t_D$ such that $V_O(t_D)=-V_F$ . $t_D$ will vary with $v_s$ : $$ t_D=\tau'ln\left(\frac{2V_F}{Av_s}\right) $$ If you've stuck with me so far... Thank you! The Problem: How do I solve for the particular solution of: $$ V_O''+\frac{2}{\tau}V_O'+\frac{1-A^2}{\tau^2}V_O=\frac{-A}{\tau^2}\left(v_n+v_n'\right) $$ where $v_n(t)$ is a gaussian random process? I know that derivatives of gaussian processes are also gaussian, so I can make the RHS into a single gaussian, but I need to know its parameters. Even after that, I need to solve for $V_o$ , then apply solve the inverse to get a distribution of $t_D$ for a given $v_s$ , which is ultimately what I'm looking for. This is way beyond anything I learned about differential equations. Explicit solutions are greatly appreciated, as are references to the appropriate techniques!","Context: I'm trying to characterize the metastability behavior of a digital latch . I'm modeling two cross-coupled inverters as RC circuits with negative gain. One of the inverters has a source of noise at its input (denoted ) which is a gaussian white noise process. This gives me a system of two ODE's: Substituting and re-arranging for yields In the absences of this noise source, the second order homogeneous equation has a simple complementary answer where For simplicity assume so In general, I'm concerned with initial conditions of the form (i.e. some sampled, constant voltage at gained up to ). This gives a solution (again approximating) of And finally, the quantity I'm trying to characterize is the time delay to some fixed voltage, that is such that . will vary with : If you've stuck with me so far... Thank you! The Problem: How do I solve for the particular solution of: where is a gaussian random process? I know that derivatives of gaussian processes are also gaussian, so I can make the RHS into a single gaussian, but I need to know its parameters. Even after that, I need to solve for , then apply solve the inverse to get a distribution of for a given , which is ultimately what I'm looking for. This is way beyond anything I learned about differential equations. Explicit solutions are greatly appreciated, as are references to the appropriate techniques!","v_n(t) 
V_I(t) + \tau \frac{dV_I(t)}{dt}=-AV_O(t)
 
V_O(t) + \tau \frac{dV_O(t)}{dt}=-A\left(V_I(t)+v_n(t)\right)
 V_O 
V_O''+\frac{2}{\tau}V_O'+\frac{1-A^2}{\tau^2}V_O=\frac{-A}{\tau^2}\left(v_n+v_n'\right)
 
y_c=C_1e^{-t/\tau_1}+C_2e^{t/\tau_2} 
 
\tau_1 = \frac{\tau}{A+1}, \tau_2 = \frac{\tau}{A-1}
 A \gg 1 \tau_1=\tau_2=\tau'=\frac{\tau}{A} V_O(0)=-Av_s,V_O'(0)=0 V_I V_O 
V_O(t)=-A\frac{v_s}{2}\left(e^{\frac{t}{\tau'}}+e^{\frac{-t}{\tau'}}\right)
 t_D V_O(t_D)=-V_F t_D v_s 
t_D=\tau'ln\left(\frac{2V_F}{Av_s}\right)
 
V_O''+\frac{2}{\tau}V_O'+\frac{1-A^2}{\tau^2}V_O=\frac{-A}{\tau^2}\left(v_n+v_n'\right)
 v_n(t) V_o t_D v_s","['ordinary-differential-equations', 'stochastic-processes', 'stochastic-analysis', 'stochastic-differential-equations', 'random-functions']"
67,Find the particular solution of the differential equation,Find the particular solution of the differential equation,,"$\sqrt{x} + \sqrt{y}y' =0$ , with $y(1) = 9$ . I get: $\sqrt{y}dy = -\sqrt{x}dx \Rightarrow \frac{2}{3}y^{\frac{3}{2}} = -\frac{2}{3}x^{\frac{3}{2}}+C$ . What should I do from here?",", with . I get: . What should I do from here?",\sqrt{x} + \sqrt{y}y' =0 y(1) = 9 \sqrt{y}dy = -\sqrt{x}dx \Rightarrow \frac{2}{3}y^{\frac{3}{2}} = -\frac{2}{3}x^{\frac{3}{2}}+C,['ordinary-differential-equations']
68,A unique solution for $y'=\cos\left(y\right)$,A unique solution for,y'=\cos\left(y\right),"I've considered the non-linear problem $$ \displaystyle \left(\star\right) \ \ \ \ \begin{cases} \displaystyle y'(x)=\cos\left(y(x)\right)\\ y(0)=\alpha, \ \alpha \in \mathbb{R} \end{cases}$$ I want to prove that it admits a unique solution on $\mathbb{R}$ . I've used the following lemma : Grönwall Lemma : If $y$ satisfies for all $t \in \left[0,+\infty\right[$ the inequality $$ y'\left(t\right) \leq \beta\left(t\right)y\left(t\right) $$ then for all $t \in \left[0,+\infty\right[$ $$ y\left(t\right) \leq y\left(0\right) \text{exp}\left(\int_{0}^{t}\beta\left(s\right)\text{d}s\right) $$ Then let $y_1$ and $y_2$ be two solutions of $(\star)$ then we have $$ (y_1-y_2)'=\cos\left(y_1\left(t\right)\right)-\cos\left(y_2\left(t\right)\right)=-2\sin\left(\frac{y_1+y_2}{2}\right)\sin\left(\frac{y_1-y_2}{2}\right) $$ So letting $Y=(y_1-y_2)/2$ we have $$ Y' \leq \sin\left(\frac{y_1\left(t\right)+y_2\left(t\right)}{2}\right)\sin\left(-Y\right) $$ Using that $Y\left(0\right)=0$ , we have $$ y_1(t)-y_2(t) \leq 0 $$ I can then use the same argument with $Z=y_2-y_1$ to obtain $y_2(t)-y_1(t) \leq 0$ to have $$ y_1=y_2 $$ Can somebody tell me if it's true ( or correct me to fill the proof )? My problem is the majoration of $Y'$ because I dont know the sign of the sin ( can't be maxed by $1$ then ), and it depends on $y_1$ and $y_2$ .","I've considered the non-linear problem I want to prove that it admits a unique solution on . I've used the following lemma : Grönwall Lemma : If satisfies for all the inequality then for all Then let and be two solutions of then we have So letting we have Using that , we have I can then use the same argument with to obtain to have Can somebody tell me if it's true ( or correct me to fill the proof )? My problem is the majoration of because I dont know the sign of the sin ( can't be maxed by then ), and it depends on and ."," \displaystyle \left(\star\right) \ \ \ \ \begin{cases}
\displaystyle y'(x)=\cos\left(y(x)\right)\\
y(0)=\alpha, \ \alpha \in \mathbb{R}
\end{cases} \mathbb{R} y t \in \left[0,+\infty\right[ 
y'\left(t\right) \leq \beta\left(t\right)y\left(t\right)
 t \in \left[0,+\infty\right[ 
y\left(t\right) \leq y\left(0\right) \text{exp}\left(\int_{0}^{t}\beta\left(s\right)\text{d}s\right)
 y_1 y_2 (\star) 
(y_1-y_2)'=\cos\left(y_1\left(t\right)\right)-\cos\left(y_2\left(t\right)\right)=-2\sin\left(\frac{y_1+y_2}{2}\right)\sin\left(\frac{y_1-y_2}{2}\right)
 Y=(y_1-y_2)/2 
Y' \leq \sin\left(\frac{y_1\left(t\right)+y_2\left(t\right)}{2}\right)\sin\left(-Y\right)
 Y\left(0\right)=0 
y_1(t)-y_2(t) \leq 0
 Z=y_2-y_1 y_2(t)-y_1(t) \leq 0 
y_1=y_2
 Y' 1 y_1 y_2",['ordinary-differential-equations']
69,Solving the Euler-Lagrange equation for the brachistochrone problem with friction,Solving the Euler-Lagrange equation for the brachistochrone problem with friction,,"This Wolfram Alpha Page contains a derivation of the parametric form of the brachistochrone curve that result from either assuming friction or its absence. I am asking for help understanding how the solution to the differential equation obtained from applying the Euler-Lagrange equation to the integrand of the the integral representing the total time of descent is obtained. This differential equation can be found on step (30) of the page. I am asking for help in understanding the next step, how setting $$\frac{dy}{dx} = \cot(\theta/2)\tag{31}$$ allows for the equation to be solved, obtaining the parametric equations for $x$ and $y$ , shown in steps (32) and (33).","This Wolfram Alpha Page contains a derivation of the parametric form of the brachistochrone curve that result from either assuming friction or its absence. I am asking for help understanding how the solution to the differential equation obtained from applying the Euler-Lagrange equation to the integrand of the the integral representing the total time of descent is obtained. This differential equation can be found on step (30) of the page. I am asking for help in understanding the next step, how setting allows for the equation to be solved, obtaining the parametric equations for and , shown in steps (32) and (33).",\frac{dy}{dx} = \cot(\theta/2)\tag{31} x y,"['ordinary-differential-equations', 'calculus-of-variations', 'parametric', 'euler-lagrange-equation']"
70,General distributional solution of the Airy Equation,General distributional solution of the Airy Equation,,How can I prove that the Airy equation $$ \frac{d^2u}{dx^2}-xu = 0 $$ has at least two linear independent solutions? Once I've found it how can I prove the existence of two independent DISTRIBUTIONAL solutions,How can I prove that the Airy equation has at least two linear independent solutions? Once I've found it how can I prove the existence of two independent DISTRIBUTIONAL solutions, \frac{d^2u}{dx^2}-xu = 0 ,['ordinary-differential-equations']
71,Inequality of ODE,Inequality of ODE,,"Suppose $u(t)>0$ satisfies following inequality, how do we solve $u$ ? $$u^{'}(t)\leq a \cdot u^{\frac{n+2}{n}}(t)$$ for $t\geq 0$ . This seems a little different from Gronwall's inequality, how do we find inequality of $u(t)$ , you can add some conditions to $u$ if you want. Thanks for any help.","Suppose satisfies following inequality, how do we solve ? for . This seems a little different from Gronwall's inequality, how do we find inequality of , you can add some conditions to if you want. Thanks for any help.",u(t)>0 u u^{'}(t)\leq a \cdot u^{\frac{n+2}{n}}(t) t\geq 0 u(t) u,['ordinary-differential-equations']
72,A vector field with an integrating factor has a first integral,A vector field with an integrating factor has a first integral,,"I have a problem that I don't know how to solve it, It says: Let $X=(f,g)\in \mathcal C^1(\mathbb{R})^2$ be a vector field and consider the system $\dot x = f(x,y), \; \dot y = g(x,y) $ . If the system has an integrating factor $\mu(x,y) $ , prove that $X$ has a first integral $H(x,y)$ . The hints and definitions that I have are: $\mu(x,y) $ is a integrating factor iff $\mu(x,y)\neq 0, \; \forall    (x,y)$ and $div (\mu X)=0 $ If $H$ is a first integral of $X$ , then $DH(x,y)X(x,y)=0$ If $divX=0$ , then $X$ has a first integral (with some conditions of the domain I think, but it works in this problem) With all of this, we know that $\mu  X$ has a first integral $\tilde H$ , and I think I should prove that $\frac 1\mu \tilde H$ is the first integral of $X$ that I'm looking for. Hence, I'm trying to prove that $D\left(\frac 1\mu \tilde H \right)(x,y)X(x,y)=0$ , using that $D\tilde H(x,y) (\mu X)(x,y) =0$ and $div(\mu X)=0 $ On the one hand, $$ div (\mu X)=\mu \;div X + \frac{\partial \mu}{\partial x} f+\frac{\partial \mu}{\partial y} g=0 $$ On the other hand, $$D\left(\frac 1\mu \tilde H \right)(x,y)X(x,y)=-\frac1{\mu^2}\tilde H \left(\frac{\partial \mu}{\partial x} f+\frac{\partial \mu}{\partial y} g\right)+D\tilde H\cdot X = \frac 1\mu div X \cdot \tilde H$$ amd I'm not sure how I can continue, or if I'm in the correct way. Any help is welcome! Thanks","I have a problem that I don't know how to solve it, It says: Let be a vector field and consider the system . If the system has an integrating factor , prove that has a first integral . The hints and definitions that I have are: is a integrating factor iff and If is a first integral of , then If , then has a first integral (with some conditions of the domain I think, but it works in this problem) With all of this, we know that has a first integral , and I think I should prove that is the first integral of that I'm looking for. Hence, I'm trying to prove that , using that and On the one hand, On the other hand, amd I'm not sure how I can continue, or if I'm in the correct way. Any help is welcome! Thanks","X=(f,g)\in \mathcal C^1(\mathbb{R})^2 \dot x = f(x,y), \; \dot y = g(x,y)  \mu(x,y)  X H(x,y) \mu(x,y)  \mu(x,y)\neq 0, \; \forall
   (x,y) div (\mu X)=0  H X DH(x,y)X(x,y)=0 divX=0 X \mu  X \tilde H \frac 1\mu \tilde H X D\left(\frac 1\mu \tilde H \right)(x,y)X(x,y)=0 D\tilde H(x,y) (\mu X)(x,y) =0 div(\mu X)=0   div (\mu X)=\mu \;div X + \frac{\partial \mu}{\partial x} f+\frac{\partial \mu}{\partial y} g=0  D\left(\frac 1\mu \tilde H \right)(x,y)X(x,y)=-\frac1{\mu^2}\tilde H \left(\frac{\partial \mu}{\partial x} f+\frac{\partial \mu}{\partial y} g\right)+D\tilde H\cdot X = \frac 1\mu div X \cdot \tilde H","['ordinary-differential-equations', 'vector-fields', 'integrating-factor']"
73,Approximation of arbitrary convex function,Approximation of arbitrary convex function,,"I am currently studying Jürgen Moser's ""A New Proof of de Giorgi's Theorem Concerning the Regularity Problem for Elliptic Differential Equations"". During the proof of the first lemma, he claims that an arbitrary convex function $f$ can be approximated by a sequence of twice continously differentiable convex functions $f_m$ , such that $f_m''(u)=0$ for large $|u|$ , $f_m \rightarrow f$ and $f_m'(u) \rightarrow f'(u)$ , where $f'(u)$ exists. Having looked in several books about convex analysis (Rockafellar), I found out that a convex function should be twice differentiable almost everywhere. I further found in one of Rockafellar's books (convex analysis, thm. 25.7) a Theorem that shows the claim, except for the restriction on $f''$ .  Now the question is how can we come up with that restriction on $f''$ and maybe someone has a reference for a book as well, since I would be interested in seeing the proof.","I am currently studying Jürgen Moser's ""A New Proof of de Giorgi's Theorem Concerning the Regularity Problem for Elliptic Differential Equations"". During the proof of the first lemma, he claims that an arbitrary convex function can be approximated by a sequence of twice continously differentiable convex functions , such that for large , and , where exists. Having looked in several books about convex analysis (Rockafellar), I found out that a convex function should be twice differentiable almost everywhere. I further found in one of Rockafellar's books (convex analysis, thm. 25.7) a Theorem that shows the claim, except for the restriction on .  Now the question is how can we come up with that restriction on and maybe someone has a reference for a book as well, since I would be interested in seeing the proof.",f f_m f_m''(u)=0 |u| f_m \rightarrow f f_m'(u) \rightarrow f'(u) f'(u) f'' f'',"['ordinary-differential-equations', 'partial-differential-equations', 'convex-analysis']"
74,Cogeodesic flow on compact manifold has compact leaves and manifold,Cogeodesic flow on compact manifold has compact leaves and manifold,,"Let $g_{ij}$ be positive definite metric on a compact manifold $M$ . Consider hamiltonian $H=\sum_{ij}\frac{1}{2}g_{ij}(x) p_ip_j$ with $p_i\in\Gamma(T^\star M)$ . Then define $E_\lambda=\{(x,p)\in T^\star M\vert H(x,p)=\lambda\}$ $\textbf{Q:}$ $E_\lambda$ is compact. How do I see this obviously? I did the following. Take any open covering of $E_\lambda$ and refine it if necessary s.t. one has trivialization of $T^\star M$ . Then project down to $M$ and then lift up to $T^\star M$ and this selects some of the open covering. Since $M$ is compact, $g_{ij}(x)$ has minimal value $g$ . Then use $\sum_{ij}g p_ip_j\leq\lambda$ to bound $p_i$ in closed set. Hence, $E_\lambda$ is compact by selecting the finite open covering. Ref: Riemannian Geometry and Geometric Analysis Jost. Chpt 2 Lie Groups and Vector Bunbldes Pg 70","Let be positive definite metric on a compact manifold . Consider hamiltonian with . Then define is compact. How do I see this obviously? I did the following. Take any open covering of and refine it if necessary s.t. one has trivialization of . Then project down to and then lift up to and this selects some of the open covering. Since is compact, has minimal value . Then use to bound in closed set. Hence, is compact by selecting the finite open covering. Ref: Riemannian Geometry and Geometric Analysis Jost. Chpt 2 Lie Groups and Vector Bunbldes Pg 70","g_{ij} M H=\sum_{ij}\frac{1}{2}g_{ij}(x) p_ip_j p_i\in\Gamma(T^\star M) E_\lambda=\{(x,p)\in T^\star M\vert H(x,p)=\lambda\} \textbf{Q:} E_\lambda E_\lambda T^\star M M T^\star M M g_{ij}(x) g \sum_{ij}g p_ip_j\leq\lambda p_i E_\lambda","['geometry', 'ordinary-differential-equations', 'differential-geometry', 'riemannian-geometry']"
75,"Find the general solution in terms of Bessel functions: $t^2x'' + x' + x = 0, \quad t < 0, \text{ Hint: } s = 2\sqrt{t}$",Find the general solution in terms of Bessel functions:,"t^2x'' + x' + x = 0, \quad t < 0, \text{ Hint: } s = 2\sqrt{t}","I was asked the following question: Find the general solution in terms of Bessel functions: $$t^2x'' + x' + x = 0, \quad t < 0, \text{ Hint: } s = 2\sqrt{t}$$ My approuch I think that what I have to do is transform the given equation in one that has a form of a Bessel equation, and for that it must be used the hint . $\frac{ds}{dt} = t^{-1/2}$ $\frac{dx}{dt} = \frac{dx}{ds}\frac{ds}{dt} = t^{-1/2} \frac{dx}{ds}$ $\frac{d^2x}{dt^2} = \frac{d}{dt} \Big( \frac{dx}{dt} \Big) = \frac{d}{dt} \Big( t^{-1/2} \frac{dx}{ds} \Big) = \frac{d}{dt}t^{-1/2} \frac{dx}{ds} +  t^{-1/2}\frac{d}{dt} \Big( \frac{dx}{ds} \Big) = \frac{-1}{2} t^{-3/2} \frac{dx}{ds} + t^{-1/2} \frac{d}{ds} \Big( \frac{dx}{ds} \Big) \frac{ds}{dt} = \frac{-1}{2} t^{-3/2} \frac{dx}{ds} + t^{-1} \frac{d^2x}{ds^2}$ So, in the given equation, now we have: $$ \begin{align} t^2\Big( \frac{-1}{2} t^{-3/2} \frac{dx}{ds} + t^{-1} \frac{d^2x}{ds^2} \Big) + t^{-1/2}\frac{dx}{ds} + x  &= 0 \\ \frac{-1}{2}t^{1/2}\frac{dx}{ds} + t\frac{d^2x}{ds^2} + t^{-1/2}\frac{dx}{ds} + x &= 0 \\ \frac{-1}{2} \frac{s}{2} \frac{dx}{ds} + \frac{s^2}{4} \frac{d^2x}{ds^2} + \frac{2}{s} \frac{dx}{ds} + x &= 0 \\ s^2 \frac{d^2x}{ds^2} + \Big( \frac{8}{s} - s \Big)\frac{dx}{ds} + 4x &= 0 \end{align} $$ Which doesn't have a Bessel-equation form. So please, can anybody enlighten me? Thanks in advanced!","I was asked the following question: Find the general solution in terms of Bessel functions: My approuch I think that what I have to do is transform the given equation in one that has a form of a Bessel equation, and for that it must be used the hint . So, in the given equation, now we have: Which doesn't have a Bessel-equation form. So please, can anybody enlighten me? Thanks in advanced!","t^2x'' + x' + x = 0, \quad t < 0, \text{ Hint: } s = 2\sqrt{t} \frac{ds}{dt} = t^{-1/2} \frac{dx}{dt} = \frac{dx}{ds}\frac{ds}{dt} = t^{-1/2} \frac{dx}{ds} \frac{d^2x}{dt^2} = \frac{d}{dt} \Big( \frac{dx}{dt} \Big) = \frac{d}{dt} \Big( t^{-1/2} \frac{dx}{ds} \Big) = \frac{d}{dt}t^{-1/2} \frac{dx}{ds} +  t^{-1/2}\frac{d}{dt} \Big( \frac{dx}{ds} \Big) = \frac{-1}{2} t^{-3/2} \frac{dx}{ds} + t^{-1/2} \frac{d}{ds} \Big( \frac{dx}{ds} \Big) \frac{ds}{dt} = \frac{-1}{2} t^{-3/2} \frac{dx}{ds} + t^{-1} \frac{d^2x}{ds^2} 
\begin{align}
t^2\Big( \frac{-1}{2} t^{-3/2} \frac{dx}{ds} + t^{-1} \frac{d^2x}{ds^2} \Big) + t^{-1/2}\frac{dx}{ds} + x  &= 0 \\
\frac{-1}{2}t^{1/2}\frac{dx}{ds} + t\frac{d^2x}{ds^2} + t^{-1/2}\frac{dx}{ds} + x &= 0 \\
\frac{-1}{2} \frac{s}{2} \frac{dx}{ds} + \frac{s^2}{4} \frac{d^2x}{ds^2} + \frac{2}{s} \frac{dx}{ds} + x &= 0 \\
s^2 \frac{d^2x}{ds^2} + \Big( \frac{8}{s} - s \Big)\frac{dx}{ds} + 4x &= 0
\end{align}
","['ordinary-differential-equations', 'bessel-functions']"
76,How to Solve this Differential Calculus Problem,How to Solve this Differential Calculus Problem,,"If the equation of the normal line to the curve $y = ax + b/x$ at the point $(2,7)$ is $y+ 2x = 11$ , find the value of $a$ and $b$ . Given that this normal line meets the curve again at $P$ , find the coordinates of $P$ . I found the tangent equation to be $y= 1/2 x + 6$ . I inegrated that to get $y = x^2 /4 + 6x + c$ . Then I found c by putting the values of the point  in as 6. (I am unsure from here forward) I think $ax + b/x$ would be $ax^2 + yx + b$ , meaning $a$ would be $1/4$ and $b$ would be -7. I do not know what to do to find the other intersection of the normal, given that I am unsure that my answers for $a$ and $b$ are accurate","If the equation of the normal line to the curve at the point is , find the value of and . Given that this normal line meets the curve again at , find the coordinates of . I found the tangent equation to be . I inegrated that to get . Then I found c by putting the values of the point  in as 6. (I am unsure from here forward) I think would be , meaning would be and would be -7. I do not know what to do to find the other intersection of the normal, given that I am unsure that my answers for and are accurate","y = ax + b/x (2,7) y+ 2x = 11 a b P P y= 1/2 x + 6 y = x^2 /4 + 6x + c ax + b/x ax^2 + yx + b a 1/4 b a b","['calculus', 'integration', 'ordinary-differential-equations']"
77,"Solving linear system, finding equilibrium and bifurcation points","Solving linear system, finding equilibrium and bifurcation points",,"For homework, I have to solve the following problem: consider the system of ODE's \begin{equation}  x'=-x^2+a \\ y'=-y \end{equation} where the parameter $a$ is a real number. I have to characterize the equilibrium points and draw a bifurcation diagram. I am asked to linearize the system first. I have missed a couple of classes and I am struggling to understand how I am supposed to work here. If $a$ is positive, then there are two equilibrium points $(\sqrt{a},0)$ and $(-\sqrt{a},0)$ , if $a$ is equal to zero, then there is one equilibrium point $ \left ( 0,0 \right )$ and finally  if $a$ is negative,  then no equilibrium points exist. The Jacobi matrix of the system is \begin{pmatrix}  - 2x & 0\\  0 & -1 \end{pmatrix} which is diagonal, hence its eigenvalues are $-2x$ and $-1$ . Then, I calculate this matrix at the equilibrium points: If $a>0$ , then the two Jacobi matrices are \begin{pmatrix} -2\sqrt{a}  & 0\\  0 & -1 \end{pmatrix} which has two real and negative eigenvalues, so $(\sqrt{a},0)$ is a stable point and \begin{pmatrix} 2\sqrt{a}  & 0\\  0 & -1 \end{pmatrix} which has one negative and one positive eigenvalue, so $(-\sqrt{a},0)$ is a saddle point. If $a=0$ the Jacobi matrix is \begin{pmatrix} 0 & 0\\  0 & -1 \end{pmatrix} which has one zero eigenvalue so the linearization method does not provide any information about the stability of $\left ( 0,0 \right )$ . As my notes suggest, for this point we find the eigenvectors of this matrix and the orbits of the solutions. But I do not understand why and what information we obtain from this. What can I say about the stability of $\left ( 0,0 \right )$ then? What about the saddle point, do I use a similar method? I understand that there is a bifurcation for $a=0$ but I don't know how am I supposed to draw the diagram without a computer. Another thing I find in the notes in another example is that we divide the equations and find a relationship between the solutions which does not involve time, for some reason. Thanks in advance, any insight on this will be appreciated, right now I am lost.","For homework, I have to solve the following problem: consider the system of ODE's where the parameter is a real number. I have to characterize the equilibrium points and draw a bifurcation diagram. I am asked to linearize the system first. I have missed a couple of classes and I am struggling to understand how I am supposed to work here. If is positive, then there are two equilibrium points and , if is equal to zero, then there is one equilibrium point and finally  if is negative,  then no equilibrium points exist. The Jacobi matrix of the system is which is diagonal, hence its eigenvalues are and . Then, I calculate this matrix at the equilibrium points: If , then the two Jacobi matrices are which has two real and negative eigenvalues, so is a stable point and which has one negative and one positive eigenvalue, so is a saddle point. If the Jacobi matrix is which has one zero eigenvalue so the linearization method does not provide any information about the stability of . As my notes suggest, for this point we find the eigenvectors of this matrix and the orbits of the solutions. But I do not understand why and what information we obtain from this. What can I say about the stability of then? What about the saddle point, do I use a similar method? I understand that there is a bifurcation for but I don't know how am I supposed to draw the diagram without a computer. Another thing I find in the notes in another example is that we divide the equations and find a relationship between the solutions which does not involve time, for some reason. Thanks in advance, any insight on this will be appreciated, right now I am lost.","\begin{equation} 
x'=-x^2+a \\ y'=-y
\end{equation} a a (\sqrt{a},0) (-\sqrt{a},0) a  \left ( 0,0 \right ) a \begin{pmatrix}
 - 2x & 0\\ 
0 & -1
\end{pmatrix} -2x -1 a>0 \begin{pmatrix}
-2\sqrt{a}  & 0\\ 
0 & -1
\end{pmatrix} (\sqrt{a},0) \begin{pmatrix}
2\sqrt{a}  & 0\\ 
0 & -1
\end{pmatrix} (-\sqrt{a},0) a=0 \begin{pmatrix}
0 & 0\\ 
0 & -1
\end{pmatrix} \left ( 0,0 \right ) \left ( 0,0 \right ) a=0","['ordinary-differential-equations', 'dynamical-systems', 'stability-in-odes']"
78,General solution ODE,General solution ODE,,"Consider the following ODE, defined on the interval I = $\mathbb{R}$ , $$\frac{d^3 y}{dx^3}+\frac{dy}{dx}=x$$ and its homogeneous differential equation, $$\frac{d^3 y}{dx^3}+\frac{dy}{dx}=0$$ whose characteristic polynomial yields the roots $\lambda_{1} = 0$ , $\lambda_{2} = i$ and $\lambda_{3} = -i$ , each with multiplicity 1. At it this point, there is a fundamental system of complex solutions, $(\phi_{1}, \phi_{2}, \phi_{3}) = (1, e^{ix}, e^{-ix})$ and a fundamental system of real solutions, $(\psi_{1}, \psi_{2}, \psi_{3}) = (1, cos(x), sin(x))$ . Upon applying the undetermined coefficients method, I've come up with a particular solution for the original ODE, $\eta_{p}(x) = \frac{x^2}{2}$ . Since the coefficients of the given ODE are all real numbers, what is the form of the general solution? Should it be $\psi (x) = \eta_{p} + c_{1} +  c_{2}cos(x) +c_{3}sin(x)$ or rather $\phi (x) = \eta_{p} + c_{1} +  c_{2}e^{ix} +c_{3}e^{-ix}$ ? And, in each case, are $c_{1},c_{2},c_{3} \in \mathbb{R}$ or $\mathbb{C}$ ? Thank you kindly.","Consider the following ODE, defined on the interval I = , and its homogeneous differential equation, whose characteristic polynomial yields the roots , and , each with multiplicity 1. At it this point, there is a fundamental system of complex solutions, and a fundamental system of real solutions, . Upon applying the undetermined coefficients method, I've come up with a particular solution for the original ODE, . Since the coefficients of the given ODE are all real numbers, what is the form of the general solution? Should it be or rather ? And, in each case, are or ? Thank you kindly.","\mathbb{R} \frac{d^3 y}{dx^3}+\frac{dy}{dx}=x \frac{d^3 y}{dx^3}+\frac{dy}{dx}=0 \lambda_{1} = 0 \lambda_{2} = i \lambda_{3} = -i (\phi_{1}, \phi_{2}, \phi_{3}) = (1, e^{ix}, e^{-ix}) (\psi_{1}, \psi_{2}, \psi_{3}) = (1, cos(x), sin(x)) \eta_{p}(x) = \frac{x^2}{2} \psi (x) = \eta_{p} + c_{1} +  c_{2}cos(x) +c_{3}sin(x) \phi (x) = \eta_{p} + c_{1} +  c_{2}e^{ix} +c_{3}e^{-ix} c_{1},c_{2},c_{3} \in \mathbb{R} \mathbb{C}",['ordinary-differential-equations']
79,Subspace of all twice differentiable functions,Subspace of all twice differentiable functions,,"I'm a bit new to linear algebra. I have a question about why the solution to the differential equation $y''+2y'+y+x=0$ is not a subspace of the vector space of all twice differentiable functions. When I solve the equation using the method of unknown coefficients, I get the general solution $y = C_1e^x+C_2xe^x-x+2$ By my line of thinking, this function is twice differentiable for all values $C_1$ and $C_2$ , so all solutions are members of the given set. However, the sum of any two solutions, for example $(C_1e^x+C_2xe^x-x+2) + (B_1e^x+B_2xe^x-x+2) = e^x(C_1+B_1)+xe^x(C_2+B_2)-2x+4$ Where the expression $-2x+4$ does not satisfy the particular solution. Am I on the right track, or is there another reason that it is not a subspace?","I'm a bit new to linear algebra. I have a question about why the solution to the differential equation is not a subspace of the vector space of all twice differentiable functions. When I solve the equation using the method of unknown coefficients, I get the general solution By my line of thinking, this function is twice differentiable for all values and , so all solutions are members of the given set. However, the sum of any two solutions, for example Where the expression does not satisfy the particular solution. Am I on the right track, or is there another reason that it is not a subspace?","y''+2y'+y+x=0 y = C_1e^x+C_2xe^x-x+2 C_1 C_2 (C_1e^x+C_2xe^x-x+2) + (B_1e^x+B_2xe^x-x+2)
= e^x(C_1+B_1)+xe^x(C_2+B_2)-2x+4 -2x+4","['linear-algebra', 'ordinary-differential-equations', 'vector-spaces']"
80,Cannot solve recursion relation in power series solution to this ODE,Cannot solve recursion relation in power series solution to this ODE,,"I'm trying to solve the differential equation $$\frac{d^2u}{dr^2} - \left[V_0(r-1)^2 + \frac{\ell(\ell+1)}{r^2} \right]u = -\lambda u$$ where $r\geq0$ is the radial component in spherical coordinates, $\ell\in \mathbb N$ and $V_0 >0$ , and $\lambda>0$ are the discrete eigenvalues of the system. I am looking for solutions which behave nice asymptotically, i.e. $u \to 0$ as $r \to 0$ and $r \to \infty$ . Using asymptotic analysis, we have $$\frac{d^2u}{dr^2} \approx V_0(r-1)^2u \implies u \approx e^{-\sqrt{V_0}(r-1)^2/2}$$ as $r \to \infty$ and $$\frac{d^2u}{dr^2} \approx \frac{\ell(\ell+1)}{r^2}u \implies u \approx r^{\ell+1}$$ as $r \to 0$ . I then guess a solution of the form $$u =  e^{-\sqrt{V_0}(r-1)^2/2} r^{\ell+1} \sum_{k=0}^\infty a_k r^k$$ for some power series coefficients $a_k$ . Plugging this into the differential equation however, I get recursion equations which depend on 3 coefficients, so I can't solve them. How should one solve this equation using asymptotic analysis and a series solution?","I'm trying to solve the differential equation where is the radial component in spherical coordinates, and , and are the discrete eigenvalues of the system. I am looking for solutions which behave nice asymptotically, i.e. as and . Using asymptotic analysis, we have as and as . I then guess a solution of the form for some power series coefficients . Plugging this into the differential equation however, I get recursion equations which depend on 3 coefficients, so I can't solve them. How should one solve this equation using asymptotic analysis and a series solution?",\frac{d^2u}{dr^2} - \left[V_0(r-1)^2 + \frac{\ell(\ell+1)}{r^2} \right]u = -\lambda u r\geq0 \ell\in \mathbb N V_0 >0 \lambda>0 u \to 0 r \to 0 r \to \infty \frac{d^2u}{dr^2} \approx V_0(r-1)^2u \implies u \approx e^{-\sqrt{V_0}(r-1)^2/2} r \to \infty \frac{d^2u}{dr^2} \approx \frac{\ell(\ell+1)}{r^2}u \implies u \approx r^{\ell+1} r \to 0 u =  e^{-\sqrt{V_0}(r-1)^2/2} r^{\ell+1} \sum_{k=0}^\infty a_k r^k a_k,"['ordinary-differential-equations', 'recurrence-relations', 'sturm-liouville', 'frobenius-method']"
81,Why do we only have one set of solutions for the PDEs of Legendre and Hermite polynomials?,Why do we only have one set of solutions for the PDEs of Legendre and Hermite polynomials?,,"This is an undergraduate-level mathematical physics problem. It may be trivial and basic to some of you, but it's important to me. In the mathematical physics course, the PDE for Hermite polynomials is $$H^{\prime\prime}(x)-2xH^\prime(x)+2nH(x)=0.$$ The PDE for Legendre polynomials is $$\frac{d}{d x}(1-x^2)\frac{d}{d x}P_l(x)+l(l+1)P_l(x)=0.$$ But I know there's a theorem stating that for $n$ -th order differential equation, we should get two linearly independent solutions, if zero boundary is imposed. Therefore I should expect another class of Hermite (Legendre) polynomials. But here, I don't see there's any boundary condition, both for Hermite and Legendre (both my professor and the textbook didn't mention B.C.). So I want to know if there's some implicit boundary conditions imposed here for these two kinds of partial differential equations. If so, what are they? Or did I misused the theorem mentioned above? Please feel free to answer or to comment. Thank you in advance!","This is an undergraduate-level mathematical physics problem. It may be trivial and basic to some of you, but it's important to me. In the mathematical physics course, the PDE for Hermite polynomials is The PDE for Legendre polynomials is But I know there's a theorem stating that for -th order differential equation, we should get two linearly independent solutions, if zero boundary is imposed. Therefore I should expect another class of Hermite (Legendre) polynomials. But here, I don't see there's any boundary condition, both for Hermite and Legendre (both my professor and the textbook didn't mention B.C.). So I want to know if there's some implicit boundary conditions imposed here for these two kinds of partial differential equations. If so, what are they? Or did I misused the theorem mentioned above? Please feel free to answer or to comment. Thank you in advance!",H^{\prime\prime}(x)-2xH^\prime(x)+2nH(x)=0. \frac{d}{d x}(1-x^2)\frac{d}{d x}P_l(x)+l(l+1)P_l(x)=0. n,"['ordinary-differential-equations', 'partial-differential-equations', 'special-functions']"
82,Maximal interval of existence for $x'=\frac{xt}{\sqrt{x^2+1}}$,Maximal interval of existence for,x'=\frac{xt}{\sqrt{x^2+1}},"I want to find the maximal interval of existence for $$x'=f(x,t)=\frac{xt}{\sqrt{x^2+1}},\ x(0)=1$$ and I want to use a theorem I found in one of my books. It is a certain statement of boundary behaviour of maximal solutions which I couldn't find in a wikipedia article to link. So just to make sure we are on the same page I will provide the theorem: Let $D\subseteq \mathbb R\times \mathbb R^n$ be a domain and $f:D\to\mathbb R^n,\ (t,x)\mapsto f(t,x)$ be continuous and locally Lipschitz with respect to $x$ . For $(\tau,\zeta)\in D$ let $\lambda:]a,b[\to\mathbb R^n$ be a solution of the IVP $$x'=f(t,x),\ x(\tau)=\zeta$$ Then $\lambda$ is a maximal solution to the IVP iff one of the conditions hold: $a=-\infty$ $a>-\infty$ and $\limsup_{t\to a}||\lambda(t)||=\infty$ $a>-\infty,\partial D=\emptyset$ and $\lim_{t\to a}\text{distance}(\partial D,(t,\lambda(t)))=0$ Replacing the $-\infty$ with $\infty$ yields the conditions for $b$ so I will not list them here. Now back to the problem: Since $f$ is continuously differentiable with respect to $x$ it is clear that $f$ is locally Lipschitz and therefore we have a unique maximal solution to the IVP with I will call $\lambda:\ ]a,b[\to\mathbb R$ . The argument for $a$ should be the same as for $b$ so I will only care about the first one. Obviously we have $\partial D=\emptyset$ so (3.) is not an option and we only need to take care of (2.). So let's assume $a>-\infty$ , then I am using $$f(t,x)\leq\text{sgn}(x)t$$ to obtain $$\limsup_{t\to a}|\lambda(t)|=\limsup_{t\to a}|\lambda(0)+\int_0^t\lambda'(s)ds|=\limsup_{t\to a}|1+\int_0^t f(s,x)ds|\leq$$ $$\leq\limsup_{t\to a}|1+\text{sgn}(x)\int_0^t sds|=|1+\text{sgn}(x)\cdot\frac{a^2}{2}|<\infty$$ so only option (1.) is left and the same goes for $b$ , so in total we have that the maximal interval of existence is $\mathbb R$ . Is my reasoning correct here? And if not, what is wrong and how do I fix it?","I want to find the maximal interval of existence for and I want to use a theorem I found in one of my books. It is a certain statement of boundary behaviour of maximal solutions which I couldn't find in a wikipedia article to link. So just to make sure we are on the same page I will provide the theorem: Let be a domain and be continuous and locally Lipschitz with respect to . For let be a solution of the IVP Then is a maximal solution to the IVP iff one of the conditions hold: and and Replacing the with yields the conditions for so I will not list them here. Now back to the problem: Since is continuously differentiable with respect to it is clear that is locally Lipschitz and therefore we have a unique maximal solution to the IVP with I will call . The argument for should be the same as for so I will only care about the first one. Obviously we have so (3.) is not an option and we only need to take care of (2.). So let's assume , then I am using to obtain so only option (1.) is left and the same goes for , so in total we have that the maximal interval of existence is . Is my reasoning correct here? And if not, what is wrong and how do I fix it?","x'=f(x,t)=\frac{xt}{\sqrt{x^2+1}},\ x(0)=1 D\subseteq \mathbb R\times \mathbb R^n f:D\to\mathbb R^n,\ (t,x)\mapsto f(t,x) x (\tau,\zeta)\in D \lambda:]a,b[\to\mathbb R^n x'=f(t,x),\ x(\tau)=\zeta \lambda a=-\infty a>-\infty \limsup_{t\to a}||\lambda(t)||=\infty a>-\infty,\partial D=\emptyset \lim_{t\to a}\text{distance}(\partial D,(t,\lambda(t)))=0 -\infty \infty b f x f \lambda:\ ]a,b[\to\mathbb R a b \partial D=\emptyset a>-\infty f(t,x)\leq\text{sgn}(x)t \limsup_{t\to a}|\lambda(t)|=\limsup_{t\to a}|\lambda(0)+\int_0^t\lambda'(s)ds|=\limsup_{t\to a}|1+\int_0^t f(s,x)ds|\leq \leq\limsup_{t\to a}|1+\text{sgn}(x)\int_0^t sds|=|1+\text{sgn}(x)\cdot\frac{a^2}{2}|<\infty b \mathbb R","['ordinary-differential-equations', 'proof-verification', 'initial-value-problems']"
83,Solve the ode $W^{\prime }\left( s\right) -2iH\left( s\right) W\left( s\right) -1=0$,Solve the ode,W^{\prime }\left( s\right) -2iH\left( s\right) W\left( s\right) -1=0,"Consider the differential equation $W^{\prime }\left( s\right) -2iH\left( s\right) W\left( s\right) -1=0$ , $% s\in I\subset  \mathbb{R} $ and consider the functions $F\left( s\right) =\int\limits_{0}^{s}\sin \left( 2\int\limits_{0}^{u}H\left( t\right) dt\right) du$ $G\left( s\right) =\int\limits_{0}^{s}\cos \left( 2\int\limits_{0}^{u}H\left( t\right) dt\right) du$ where $H:I\rightarrow  %TCIMACRO{\U{211d} }% %BeginExpansion \mathbb{R} %EndExpansion $ is functions real of one variable Show that the general solution to the above differential equation is given by $W\left( s\right) =\left\{ \left( F\left( s\right) -c_{1}\right) +i\left( G\left( s\right) +c_{2}\right) \right\} \left( F^{\prime }\left( s\right) -iG^{\prime }\left( s\right) \right) $ Can anyone help me solve this differential equation? Well I just know the ode's that I know do not have complex coefficients.","Consider the differential equation , and consider the functions where is functions real of one variable Show that the general solution to the above differential equation is given by Can anyone help me solve this differential equation? Well I just know the ode's that I know do not have complex coefficients.","W^{\prime }\left( s\right) -2iH\left( s\right) W\left( s\right) -1=0 %
s\in I\subset 
\mathbb{R}
 F\left( s\right) =\int\limits_{0}^{s}\sin \left(
2\int\limits_{0}^{u}H\left( t\right) dt\right) du G\left( s\right) =\int\limits_{0}^{s}\cos \left(
2\int\limits_{0}^{u}H\left( t\right) dt\right) du H:I\rightarrow 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
 W\left( s\right) =\left\{ \left( F\left( s\right) -c_{1}\right) +i\left(
G\left( s\right) +c_{2}\right) \right\} \left( F^{\prime }\left( s\right)
-iG^{\prime }\left( s\right) \right) ","['ordinary-differential-equations', 'analysis', 'differential-geometry', 'riemannian-geometry']"
84,Change of variables to make this differential equation separable: $\frac{dy}{dx}=F\left(\frac{ax+by+c}{dx+ey+f}\right)$,Change of variables to make this differential equation separable:,\frac{dy}{dx}=F\left(\frac{ax+by+c}{dx+ey+f}\right),"I'm trying to define a change of variables to make the next differential equation separable: $$\frac{dy}{dx}=F\left(\frac{ax+by+c}{dx+ey+f}\right)$$ assuming that $ae-bd=0$ . I tried setting $u=ax+by+c$ and $v=dx+ey+f$ to make the right side of the equation homogeneous-like, but then i got confused trying to compute $\frac{dv}{du}$ (using that $ae-bd=0$ ). I also tried the substitution $u=\frac{ax+by+c}{dx+ey+f}$ , and in this case I used the hypothesis that $ae-bd=0$ to deduce the next expresions for the derivatives of $u$ : $$\frac{du}{dx}=\frac{af-cd}{(dx+ey+f)^2}$$ and $$\frac{du}{dy}=\frac{bf-ce}{(dx+ey+f)^2}$$ but I don't know how to use this info. Any hints will be appreciated.","I'm trying to define a change of variables to make the next differential equation separable: assuming that . I tried setting and to make the right side of the equation homogeneous-like, but then i got confused trying to compute (using that ). I also tried the substitution , and in this case I used the hypothesis that to deduce the next expresions for the derivatives of : and but I don't know how to use this info. Any hints will be appreciated.",\frac{dy}{dx}=F\left(\frac{ax+by+c}{dx+ey+f}\right) ae-bd=0 u=ax+by+c v=dx+ey+f \frac{dv}{du} ae-bd=0 u=\frac{ax+by+c}{dx+ey+f} ae-bd=0 u \frac{du}{dx}=\frac{af-cd}{(dx+ey+f)^2} \frac{du}{dy}=\frac{bf-ce}{(dx+ey+f)^2},['ordinary-differential-equations']
85,Solve the following autonomous ODE,Solve the following autonomous ODE,,"Show that the following ode has a maximum of one solution. $$x'(t) = |x(t)| +\sin(x^2(t)+t^2) \quad \text{with} \quad  x(0)=0$$ and that this solution satisfy $x(t)\leq  e^t-1$ for $t \geq 0$ We got the hint: compare with $x'(t)= |x(t)|+1$ I solved the ODE of the hint and got  for $x(t) \geq 0 $ the solution $ x(t)=c_{1}e^t-1  ,c_{1} \in \mathbb{R}$ for $x(t) < 0 $ the solution $ x(t)=c_{2}e^{-t}+1  ,c_{2} \in \mathbb{R}$ Now  I´m not sure what to do. I know that if $c_{1}=1$ I get $x(t)\leq  e^t-1$ for $t \geq 0$ . What have i to do next? I have problems to show Lipschitz-continuity, because i cant find an L. I need some help.","Show that the following ode has a maximum of one solution. and that this solution satisfy for We got the hint: compare with I solved the ODE of the hint and got  for the solution for the solution Now  I´m not sure what to do. I know that if I get for . What have i to do next? I have problems to show Lipschitz-continuity, because i cant find an L. I need some help.","x'(t) = |x(t)| +\sin(x^2(t)+t^2) \quad \text{with} \quad  x(0)=0 x(t)\leq  e^t-1 t \geq 0 x'(t)= |x(t)|+1 x(t) \geq 0   x(t)=c_{1}e^t-1  ,c_{1} \in \mathbb{R} x(t) < 0   x(t)=c_{2}e^{-t}+1  ,c_{2} \in \mathbb{R} c_{1}=1 x(t)\leq  e^t-1 t \geq 0",['ordinary-differential-equations']
86,Solutions to $-t\ f'(t)+(1+r t)\ f(t)=f(t/2)$,Solutions to,-t\ f'(t)+(1+r t)\ f(t)=f(t/2),"Any thoughts on how to tackle this ODE $-t\ f'(t)+(1+r t)\ f(t)=f(t/2)$ , $t\in[0,t_1]$ , with boundary conditions $f(0)=0$ and $f(t_1)=s$ , with $r,s$ and $t_1$ fixed constant.","Any thoughts on how to tackle this ODE , , with boundary conditions and , with and fixed constant.","-t\ f'(t)+(1+r t)\ f(t)=f(t/2) t\in[0,t_1] f(0)=0 f(t_1)=s r,s t_1","['ordinary-differential-equations', 'functional-equations']"
87,Local uniqueness of solution for quasi linear PDE,Local uniqueness of solution for quasi linear PDE,,"I've a little troubles in proving  local uniqueness of solution for Cauchy problems concerning quasilinear PDE's. It's a little bit boring, but I tried to be as clear as possible. Suppose $\Omega$ is an open and connected subset of $\mathbb{R}^2$ and let $a(x,y,z),b(x,y,z),c(x,y,z)$ scalar functions of class $C^1$ in $\Omega \times \mathbb{R}$ . Let $I$ an open interval and $f=f(s)$ , $g=g(s)$ and $h=h(s)$ be $C^1(I)$ . We want to prove local existence and uniqueness of a solution for the Cauchy problem $$\begin{cases} a(x,y,u)u_x+b(x,y,u)u_y=c(x,y,u)\\u(f(s),g(s))=h(s)\qquad s\in I\end{cases},$$ under certain conditions using method of characteristic. Consider for each fixed $s\in I$ the autonomous system of ODE's $$\begin{cases}\frac{d}{dt}x=a(x,y,z)\\\frac{d}{dt}y=b(x,y,z)\\\frac{d}{dt}z=c(x,y,z)\end{cases}$$ with initial conditions $$\begin{cases}x(0)=f(s)\\y(0)=g(s)\\z(0)=h(s)\end{cases}.$$ Because $a,b,c$ are $C^1$ , then for every $s \in I$ I find a unique maximal and global solution $x=X(s,t),y=Y(s,t),z=Z(s,t)$ defined on an open interval $J_s$ containing $0$ such that $X(s,0)=f(s)$ , $Y(s,0)=g(s)$ , $Z(s,0)=h(s)$ . Now consider the function $$(s,t)\longrightarrow (X(s,t),Y(s,t),Z(s,t))\qquad [1] $$ for $s \in I$ , $t \in J_s$ . Fix $s_0 \in I$ and let us reason in a neighborhood of $(s_0,0)$ , trying to define a good domain for $[1]$ and then talk about invertibility. Luckily we can choose an interval $J$ independent of s (on which IVPs for characteristic equations have solutions) provided we are willing to restrict ourselves to an interval $I_0$ containing $s = s_0 $ instead of the entire interval I. Thus we may assume that the domain of the vector-valued function given in $[1]$ is $I_0 × J$ . Thanks to the differentiable dependence of solutions to initial value problems for ODEs, the vector-valued function given in $[1]$ is continuously differentiable.  Thus we are interested in the invertibility, near $(s, t) = (s_0,0)$ , of the function $[1]$ . Note that at the point $(s, t) = (s_0,0)$ , the Jacobian of the function in $[1]$ is given by $$J=\left|\begin{matrix} X_s(s_0,0) &X_t(s_0,0)\\Y_s(s_0,0) &Y_t(s_0,0)\end{matrix}\right|=\left|\begin{matrix} f'(s_0)& a(f(s_0),g(s_0),h(s_0))\\g'(s_0) &b(f(s_0),g(s_0),h(s_0))\end{matrix}\right|.$$ Provided that $J\neq 0$ we can use inverse function theorem and state that exist a neighbourhood $U$ containing $(s_0,0)$ and a neighbourhood $W$ containing $(f(s_0),g(s_0))$ such that the previous map considered from $U$ to $W$ is invertible. That is, we get two functions $S,T$ defined on $W$ such that $$s=S(x,y),\,\,\,t=T(x,y).$$ If we now define $$u(x,y):=Z(S(x,y),T(x,y))$$ then $u$ solves the Cauchy problem on $W$ . In fact for every $s \in I$ such that $(f(s),g(s))\in W$ we have $$u(f(s),g(s))=Z(S(f(s),g(s)),T(f(s),g(s)))=Z(S(X(s,0),Y(s,0)),T(X(s,0),Y(s,0)))=Z(s,0)=h(s)$$ and it's easy to see that $u$ solves the PDE by differentiating. So a solution exist in a neighborhood $W$ of $(f(s_0),g(s_0))$ . Now come my problems, because I want to prove that $u$ is the unique solution on $W$ . Following the analytical proof given by F. John - Partial Differential Equations-Springer US (1975), suppose that $u'$ is another solution of the Cauchy problem on $W$ . Let $(x',y')\in W$ . Set $s'=S(x',y')$ and consider the characteristic curve $\Gamma$ that solves $$\begin{cases}\frac{d}{dt}x=a(x,y,z)\\\frac{d}{dt}y=b(x,y,z)\\\frac{d}{dt}z=c(x,y,z)\end{cases}$$ with initial conditions $$\begin{cases}x(0)=f(s')\\y(0)=g(s')\\z(0)=h(s')\end{cases}.$$ Because $u$ and $u'$ solve the Cauchy problem, their corresponding integral surfaces both passes through the point $(f(s'),g(s'),h(s'))$ as the characteristic curve $\Gamma$ does for $t=0$ . So the integral surfaces must contain the part of $\Gamma$ whose projection on $xy$ plane is contained in $W$ . In particular for $t'=T(x',y')$ we have $$u'(x',y')=u'(X(s',t'),Y(s',t'))=Z(s',t')=Z(S(x',y'),T(x',y'))=u(x',y')$$ by definition of $u$ . Here is my question: even if we now that $(s',t')\in U$ , who ensure me that $(s',0)\in U$ too, and so I'm sure that both integral surfaces have the point $(f(s'),g(s'),h(s'))$ in common? I mean, am I sure that if a point $(\overline{x},\overline{y})\in W$ and $s=S(\overline{x},\overline{y})$ then $(f(s),g(s))\in W$ ? I think I have to consider a neighborhood of $(s_0,0)$ contained in $U$ that is a rectangle to be sure that the previous hold: in this way every selected characteristic will pass through the space initial curve $(f(s),g(s),h(s))$ and so the problem is solved. I apologies for all this words for a problem that is probably trivial and is not about PDE's!! Thanks in advance.","I've a little troubles in proving  local uniqueness of solution for Cauchy problems concerning quasilinear PDE's. It's a little bit boring, but I tried to be as clear as possible. Suppose is an open and connected subset of and let scalar functions of class in . Let an open interval and , and be . We want to prove local existence and uniqueness of a solution for the Cauchy problem under certain conditions using method of characteristic. Consider for each fixed the autonomous system of ODE's with initial conditions Because are , then for every I find a unique maximal and global solution defined on an open interval containing such that , , . Now consider the function for , . Fix and let us reason in a neighborhood of , trying to define a good domain for and then talk about invertibility. Luckily we can choose an interval independent of s (on which IVPs for characteristic equations have solutions) provided we are willing to restrict ourselves to an interval containing instead of the entire interval I. Thus we may assume that the domain of the vector-valued function given in is . Thanks to the differentiable dependence of solutions to initial value problems for ODEs, the vector-valued function given in is continuously differentiable.  Thus we are interested in the invertibility, near , of the function . Note that at the point , the Jacobian of the function in is given by Provided that we can use inverse function theorem and state that exist a neighbourhood containing and a neighbourhood containing such that the previous map considered from to is invertible. That is, we get two functions defined on such that If we now define then solves the Cauchy problem on . In fact for every such that we have and it's easy to see that solves the PDE by differentiating. So a solution exist in a neighborhood of . Now come my problems, because I want to prove that is the unique solution on . Following the analytical proof given by F. John - Partial Differential Equations-Springer US (1975), suppose that is another solution of the Cauchy problem on . Let . Set and consider the characteristic curve that solves with initial conditions Because and solve the Cauchy problem, their corresponding integral surfaces both passes through the point as the characteristic curve does for . So the integral surfaces must contain the part of whose projection on plane is contained in . In particular for we have by definition of . Here is my question: even if we now that , who ensure me that too, and so I'm sure that both integral surfaces have the point in common? I mean, am I sure that if a point and then ? I think I have to consider a neighborhood of contained in that is a rectangle to be sure that the previous hold: in this way every selected characteristic will pass through the space initial curve and so the problem is solved. I apologies for all this words for a problem that is probably trivial and is not about PDE's!! Thanks in advance.","\Omega \mathbb{R}^2 a(x,y,z),b(x,y,z),c(x,y,z) C^1 \Omega \times \mathbb{R} I f=f(s) g=g(s) h=h(s) C^1(I) \begin{cases} a(x,y,u)u_x+b(x,y,u)u_y=c(x,y,u)\\u(f(s),g(s))=h(s)\qquad s\in I\end{cases}, s\in I \begin{cases}\frac{d}{dt}x=a(x,y,z)\\\frac{d}{dt}y=b(x,y,z)\\\frac{d}{dt}z=c(x,y,z)\end{cases} \begin{cases}x(0)=f(s)\\y(0)=g(s)\\z(0)=h(s)\end{cases}. a,b,c C^1 s \in I x=X(s,t),y=Y(s,t),z=Z(s,t) J_s 0 X(s,0)=f(s) Y(s,0)=g(s) Z(s,0)=h(s) (s,t)\longrightarrow (X(s,t),Y(s,t),Z(s,t))\qquad [1]  s \in I t \in J_s s_0 \in I (s_0,0) [1] J I_0 s = s_0  [1] I_0 × J [1] (s, t) = (s_0,0) [1] (s, t) = (s_0,0) [1] J=\left|\begin{matrix} X_s(s_0,0) &X_t(s_0,0)\\Y_s(s_0,0) &Y_t(s_0,0)\end{matrix}\right|=\left|\begin{matrix} f'(s_0)& a(f(s_0),g(s_0),h(s_0))\\g'(s_0) &b(f(s_0),g(s_0),h(s_0))\end{matrix}\right|. J\neq 0 U (s_0,0) W (f(s_0),g(s_0)) U W S,T W s=S(x,y),\,\,\,t=T(x,y). u(x,y):=Z(S(x,y),T(x,y)) u W s \in I (f(s),g(s))\in W u(f(s),g(s))=Z(S(f(s),g(s)),T(f(s),g(s)))=Z(S(X(s,0),Y(s,0)),T(X(s,0),Y(s,0)))=Z(s,0)=h(s) u W (f(s_0),g(s_0)) u W u' W (x',y')\in W s'=S(x',y') \Gamma \begin{cases}\frac{d}{dt}x=a(x,y,z)\\\frac{d}{dt}y=b(x,y,z)\\\frac{d}{dt}z=c(x,y,z)\end{cases} \begin{cases}x(0)=f(s')\\y(0)=g(s')\\z(0)=h(s')\end{cases}. u u' (f(s'),g(s'),h(s')) \Gamma t=0 \Gamma xy W t'=T(x',y') u'(x',y')=u'(X(s',t'),Y(s',t'))=Z(s',t')=Z(S(x',y'),T(x',y'))=u(x',y') u (s',t')\in U (s',0)\in U (f(s'),g(s'),h(s')) (\overline{x},\overline{y})\in W s=S(\overline{x},\overline{y}) (f(s),g(s))\in W (s_0,0) U (f(s),g(s),h(s))","['real-analysis', 'general-topology', 'ordinary-differential-equations', 'partial-differential-equations', 'cauchy-problem']"
88,Differential Equations proof for Prove that $\dim(\ker(TU)) ≤ (\dim\ker(T)) + \dim(\ker(U))$.,Differential Equations proof for Prove that .,\dim(\ker(TU)) ≤ (\dim\ker(T)) + \dim(\ker(U)),"Let $T$ and $U$ be linear transformations $V → V$ with ﬁnite-dimensional kernels. Prove that $\dim(\ker(TU)) ≤ (\dim\ker(T)) + \dim(\ker(U))$ My tutor suggested that I create two new transformations by restricting the range and domain of $U$ and $V$ in the following way $\hat U : V → U(V )$ and $\hat T : U(V ) → V $ . Note that the compositions $TU$ and $\hat T \hat U$ are identical, that $\hat U$ is onto, and that $\dim(\ker(\hat T)) ≤ \dim(\ker(T))$ But I still do not understand.","Let and be linear transformations with ﬁnite-dimensional kernels. Prove that My tutor suggested that I create two new transformations by restricting the range and domain of and in the following way and . Note that the compositions and are identical, that is onto, and that But I still do not understand.",T U V → V \dim(\ker(TU)) ≤ (\dim\ker(T)) + \dim(\ker(U)) U V \hat U : V → U(V ) \hat T : U(V ) → V  TU \hat T \hat U \hat U \dim(\ker(\hat T)) ≤ \dim(\ker(T)),"['ordinary-differential-equations', 'proof-writing', 'proof-explanation']"
89,Bifurcation of time dependent parameters,Bifurcation of time dependent parameters,,"Consider the following differential equation, $$\frac{du}{dt}=w+u-u^3.$$ Suppose that the parameter changes slowly in time depending on the value of $u$ . That is, consider the system of equations $$\frac{du}{dt}=w+u-u^3.$$ $$\frac{dw}{dt}=-\epsilon u,$$ where $\epsilon>0$ is very small. Using your bifurcation diagram from part a, sketch what a solution looks like for small $\epsilon$ . So I have a graph of my bifurcation diagram of a time-independent parameter right here and now I am tasked with considering this time-dependent parameter. I'm very new to this. Previously I've learned how to work with bifurcations through the MATCONT program in MATLAB, but I don't think there's a way I can set my parameter as a function of $t$ . I need help on how to work with this problem.","Consider the following differential equation, Suppose that the parameter changes slowly in time depending on the value of . That is, consider the system of equations where is very small. Using your bifurcation diagram from part a, sketch what a solution looks like for small . So I have a graph of my bifurcation diagram of a time-independent parameter right here and now I am tasked with considering this time-dependent parameter. I'm very new to this. Previously I've learned how to work with bifurcations through the MATCONT program in MATLAB, but I don't think there's a way I can set my parameter as a function of . I need help on how to work with this problem.","\frac{du}{dt}=w+u-u^3. u \frac{du}{dt}=w+u-u^3. \frac{dw}{dt}=-\epsilon u, \epsilon>0 \epsilon t","['ordinary-differential-equations', 'bifurcation']"
90,Solving $xf^{\prime\prime}= 4f^{\prime}- 25x^9 f$,Solving,xf^{\prime\prime}= 4f^{\prime}- 25x^9 f,"Can someone explain to me how to solve the following differential equation, $$xf^{\prime\prime}= 4f^{\prime}- 25x^9 f \qquad  \text{with initial conditions}   \ f(0)=0, \ \ f^{'}(1)=1 $$ There is a hint which asks me to make the substitution, $t=x^5$ . I really am a total novice in differential equations and my only attempt has been to write the function as a power series and try to guess the coefficients. It doesn't seem very inspiring though.","Can someone explain to me how to solve the following differential equation, There is a hint which asks me to make the substitution, . I really am a total novice in differential equations and my only attempt has been to write the function as a power series and try to guess the coefficients. It doesn't seem very inspiring though.","xf^{\prime\prime}= 4f^{\prime}- 25x^9 f \qquad  \text{with initial conditions}   \ f(0)=0, \ \ f^{'}(1)=1  t=x^5",['ordinary-differential-equations']
91,"Fields with Differential identity, and Amitsur-like results","Fields with Differential identity, and Amitsur-like results",,"Let $(K,\partial)$ be an algebraically closed field of characteristic $0$ with $\partial:K\rightarrow K$ a derivation with algebraically closed field of constants $C$ . Question. Is it possible that $K$ satisfy a non-trivial one-variable differential identity (involving $+,\times,\partial$ )? Note that the answer is no for linear identities, i.e. of the for $$\sum_{i=0}^n a_i\delta^i(x)=0,$$ since the zero set of such an identity is of finite dimension over $C$ . Tries. (1) The question seems close to results like ""rings with non-trivial PI-identities have finite dimension over their centre"". An attempt of a proof is to consider the skew law $x*y:=\partial(x)y$ and apply Amitsur's result to the non commutative ring $(K,+,*)$ , but $*$ is non-associative, neither alternative... (2) If $K$ satisfies a nontrivial univariate differential identity, it also satisfies a nontrivial multivariate differential identies that is linear in every variable.","Let be an algebraically closed field of characteristic with a derivation with algebraically closed field of constants . Question. Is it possible that satisfy a non-trivial one-variable differential identity (involving )? Note that the answer is no for linear identities, i.e. of the for since the zero set of such an identity is of finite dimension over . Tries. (1) The question seems close to results like ""rings with non-trivial PI-identities have finite dimension over their centre"". An attempt of a proof is to consider the skew law and apply Amitsur's result to the non commutative ring , but is non-associative, neither alternative... (2) If satisfies a nontrivial univariate differential identity, it also satisfies a nontrivial multivariate differential identies that is linear in every variable.","(K,\partial) 0 \partial:K\rightarrow K C K +,\times,\partial \sum_{i=0}^n a_i\delta^i(x)=0, C x*y:=\partial(x)y (K,+,*) * K","['ordinary-differential-equations', 'ring-theory', 'field-theory']"
92,Proving that $2xy'=4x^{2}+3y^{2}$ has no real solutions,Proving that  has no real solutions,2xy'=4x^{2}+3y^{2},"The differential equation above is a Ricatti equation. I'm not given any particular solution to it. I'm actually asked to solve it. I tried graphing the direction field to get an idea on the behavior of the curve with no result. However, it is true that this equation has no real solution. To solve it we need to use Bessel equations (I'm still being introduced to second order differential equations). How can we prove that $2xy'=4x^{2}+3y^{2}$ has no real solution? Can we use the existence and unicity theorem? Also, it is possible to prove it without using Lipschitz continuity?","The differential equation above is a Ricatti equation. I'm not given any particular solution to it. I'm actually asked to solve it. I tried graphing the direction field to get an idea on the behavior of the curve with no result. However, it is true that this equation has no real solution. To solve it we need to use Bessel equations (I'm still being introduced to second order differential equations). How can we prove that $2xy'=4x^{2}+3y^{2}$ has no real solution? Can we use the existence and unicity theorem? Also, it is possible to prove it without using Lipschitz continuity?",,"['real-analysis', 'ordinary-differential-equations']"
93,"For $x'(t)= f(t, x(t)) + \int_0^t g(x(s))\, ds$ show that if $x_1(0) \leq x_2(0)$ that also $x_1(t) \leq x_2(t)$",For  show that if  that also,"x'(t)= f(t, x(t)) + \int_0^t g(x(s))\, ds x_1(0) \leq x_2(0) x_1(t) \leq x_2(t)","Note The Question has changed significantly since it was posed, but it is now as follows: Question Suppose we have some integro or delayed differential equation: $$ x'(t)=f(t,x(t)) + \int_0^t g(x(s))\, ds + \sum_n h(x(t-a_n)), $$ where we assume that conditions are satisfied such that this equation always has a unique, continuous solution. let $x_1(t)$ and $x_2(t)$ be two solutions with some boundary conditions $x_1(0)=x_1$ and $x_2(0)=x_2$ such that $x_1 \leq x_2$. I would now like to show that we have for all $t: x_1(t) \leq x_2(t)$. What are some methods for showing this? I know of the following method: Suppose there is some $t>0$ such that $x_1(t) > x_2(t)$, then there exists a value $s \in [0,t]$ such that $x_1(s)=x_2(s)$ and for all $u\leq s: x_1(u) \leq x_2(u)$. If one can show under these conditions that $x_1'(t) = f(t,x_1(t)) \leq f(t,x_2(t)) = x_2'(t)$ then it follows that $x_1(t)$ stays below $x_2(t)$ for all $t$. Write $x(t) = \int_0^t f(s,x(s))\, ds$ and show that $\int_0^t f(s,x_1(s))\, ds \leq \int_0^t f(s,x_2(s))\, ds$ for all $t$. The first method is easy to apply but too restrictive and the second method is not restrictive at all but is hard to apply. My question is a reference or an explenation of other methods in case the first method is not applicable. An Example Suppose for example we have the following Delay Differential Equation: $$ \begin{cases} x'(t) &= x(t)-1 \mbox{ if } t \leq 1\\ x'(t) &= x(t-1) (x(t) - x(t-1)) \mbox{ if } t >1 \end{cases} $$ this delay differential equation satisfies the property that if we have $x_1,x_2 \in (0.8,1)$ with $x_1 \leq x_2$ and $x_1(t), x_2(t)$ are solutions satisfying $x_i(0) = x_i$ then for all $t: x_1(t) \leq x_2(t)$. I can check this numerically but not analytically.","Note The Question has changed significantly since it was posed, but it is now as follows: Question Suppose we have some integro or delayed differential equation: $$ x'(t)=f(t,x(t)) + \int_0^t g(x(s))\, ds + \sum_n h(x(t-a_n)), $$ where we assume that conditions are satisfied such that this equation always has a unique, continuous solution. let $x_1(t)$ and $x_2(t)$ be two solutions with some boundary conditions $x_1(0)=x_1$ and $x_2(0)=x_2$ such that $x_1 \leq x_2$. I would now like to show that we have for all $t: x_1(t) \leq x_2(t)$. What are some methods for showing this? I know of the following method: Suppose there is some $t>0$ such that $x_1(t) > x_2(t)$, then there exists a value $s \in [0,t]$ such that $x_1(s)=x_2(s)$ and for all $u\leq s: x_1(u) \leq x_2(u)$. If one can show under these conditions that $x_1'(t) = f(t,x_1(t)) \leq f(t,x_2(t)) = x_2'(t)$ then it follows that $x_1(t)$ stays below $x_2(t)$ for all $t$. Write $x(t) = \int_0^t f(s,x(s))\, ds$ and show that $\int_0^t f(s,x_1(s))\, ds \leq \int_0^t f(s,x_2(s))\, ds$ for all $t$. The first method is easy to apply but too restrictive and the second method is not restrictive at all but is hard to apply. My question is a reference or an explenation of other methods in case the first method is not applicable. An Example Suppose for example we have the following Delay Differential Equation: $$ \begin{cases} x'(t) &= x(t)-1 \mbox{ if } t \leq 1\\ x'(t) &= x(t-1) (x(t) - x(t-1)) \mbox{ if } t >1 \end{cases} $$ this delay differential equation satisfies the property that if we have $x_1,x_2 \in (0.8,1)$ with $x_1 \leq x_2$ and $x_1(t), x_2(t)$ are solutions satisfying $x_i(0) = x_i$ then for all $t: x_1(t) \leq x_2(t)$. I can check this numerically but not analytically.",,"['calculus', 'real-analysis', 'ordinary-differential-equations', 'integro-differential-equations']"
94,Solution to a 2nd order ODE with a Gaussian coefficient,Solution to a 2nd order ODE with a Gaussian coefficient,,"I am trying to find the solutions to this differential equation: \begin{align} \frac{d^2y}{dx^2}+a e^{-x^2}y=0\ , \end{align} where $a\in\Re$. I know that equations of the form \begin{align} \frac{d^2y}{dx^2}-\left(f(x)^2+\frac{df}{dx}\right)y=0 \end{align} have the solution \begin{align} y(x)=\exp\left(\int f(x)dx\right)\ . \end{align} Thus, to solve my first equation, I need to solve \begin{align} f(x)^2+\frac{df}{dx}=-ae^{-x^2}\ , \end{align} which is a type of Riccati equation. I know that the homogenous part of this equation gives me a Bernoulli equation with solution \begin{align} y(x)=\frac{1}{x+c_1} \end{align} where $c_1$ is a constant. However, I am now stuck with finding the particular solution. I am not sure of the best method to solve for it. I also tried solving it with both Maple and Mathematica, but they were unable to do so. I found the book, Handbook of Exact Solutions for Ordinary Differential Equations , but their equations contain exponential functions (Sections 1.2 and 2.1.3) include $e^{-x}$, rather than a Gaussian.","I am trying to find the solutions to this differential equation: \begin{align} \frac{d^2y}{dx^2}+a e^{-x^2}y=0\ , \end{align} where $a\in\Re$. I know that equations of the form \begin{align} \frac{d^2y}{dx^2}-\left(f(x)^2+\frac{df}{dx}\right)y=0 \end{align} have the solution \begin{align} y(x)=\exp\left(\int f(x)dx\right)\ . \end{align} Thus, to solve my first equation, I need to solve \begin{align} f(x)^2+\frac{df}{dx}=-ae^{-x^2}\ , \end{align} which is a type of Riccati equation. I know that the homogenous part of this equation gives me a Bernoulli equation with solution \begin{align} y(x)=\frac{1}{x+c_1} \end{align} where $c_1$ is a constant. However, I am now stuck with finding the particular solution. I am not sure of the best method to solve for it. I also tried solving it with both Maple and Mathematica, but they were unable to do so. I found the book, Handbook of Exact Solutions for Ordinary Differential Equations , but their equations contain exponential functions (Sections 1.2 and 2.1.3) include $e^{-x}$, rather than a Gaussian.",,"['ordinary-differential-equations', 'homogeneous-equation']"
95,Can $y'' = e^y$ be converted into a linear ODE through successive variable substitutions?,Can  be converted into a linear ODE through successive variable substitutions?,y'' = e^y,"Consider the following second-order nonlinear ODE: $$y'' = e^y$$ The solution to this ODE is known precisely: $$y = \ln\left(\frac{1}{2}c_{1}\left(\tanh^2\left(\frac{1}{2}\sqrt{c_1(c_2+x^2)}\right)-1\right)\right)$$ Given this knowledge about the solution, is there any way to transform this ODE into a linear one using successive changes of variables ? A motivating example; consider the first-order analogue $y' = e^y$, which has the solution: $$y = \ln\left(\frac{1}{c - x}\right)$$ If I wanted to transform this nonlinear ODE into a linear one, I might consider the following coordinate transformation, $y = \ln(z)$; doing so generates the following nonlinear (but algebraic) ODE for $z$: $$z' = z^2$$ Following up with another change of variables $z = w^{-1}$, we now find a linear ODE for $w$: $$w' = -1, \quad w = c - x$$ and the solution for $y$ can be found trivially by applying all the coordinate changes to the solution of the linear ODE. In short, by observing that the solution is a composition of operations on a relatively simple function ($c - x$), we could apply those operations consecutively on the ODE to eventually get a linear ODE. Can we do that for $y'' = e^y$ too?","Consider the following second-order nonlinear ODE: $$y'' = e^y$$ The solution to this ODE is known precisely: $$y = \ln\left(\frac{1}{2}c_{1}\left(\tanh^2\left(\frac{1}{2}\sqrt{c_1(c_2+x^2)}\right)-1\right)\right)$$ Given this knowledge about the solution, is there any way to transform this ODE into a linear one using successive changes of variables ? A motivating example; consider the first-order analogue $y' = e^y$, which has the solution: $$y = \ln\left(\frac{1}{c - x}\right)$$ If I wanted to transform this nonlinear ODE into a linear one, I might consider the following coordinate transformation, $y = \ln(z)$; doing so generates the following nonlinear (but algebraic) ODE for $z$: $$z' = z^2$$ Following up with another change of variables $z = w^{-1}$, we now find a linear ODE for $w$: $$w' = -1, \quad w = c - x$$ and the solution for $y$ can be found trivially by applying all the coordinate changes to the solution of the linear ODE. In short, by observing that the solution is a composition of operations on a relatively simple function ($c - x$), we could apply those operations consecutively on the ODE to eventually get a linear ODE. Can we do that for $y'' = e^y$ too?",,"['ordinary-differential-equations', 'change-of-variable', 'nonlinear-analysis']"
96,Higher order linear differential equation,Higher order linear differential equation,,"this is a problem from one of the former exams from ordinary differential equations. Find a solution to this equation: $$x''''+6x''+25x=t\sinh t\cdot \cos(2t)$$ of course the only problem will be to find a particular solution, since the linear part is very simple to solve. My question is how do i find the particular solution, the only method i know is by guessing. What I can add, as i put this function into wolfram, he showed very wild particular solution, which is: $$  - \frac{1}{320} e t^2 \sin(2 t) + \frac{1}{320} e^t t^2 \sin(2 t)  - \frac{1}{160}e t^2 \cos(2 t) - \frac{1}{160}e^t t^2 \cos(2 t) \\ + \frac{1}{320} e t \sin(2 t) + \frac{1}{320} e^t t \sin(2 t)  - \left(\frac{1}{3200}13 e t \sin(2 t) \sin(4 t)\right) \\ + \left(\frac{1}{3200}13 e^t t \sin(2 t) \sin(4 t)\right)  - \left(\frac{1}{32000}21 e \sin(2 t) \sin(4 t)\right) \\ - \left(\frac{1}{32000}21 e^t \sin(2 t) \sin(4 t)\right)  - \frac{1}{160} e t \cos(2 t) \\ + \frac{1}{160} e^t t \cos(2 t)  - \left(13 e t \cos(2 t) \cos(4 t)\right) \\ + \left(\frac{1}{3200}13 e^t t \cos(2 t) \cos(4 t)\right)  - \left(\frac{1}{32000}21 e \cos(2 t) \cos(4 t)\right) \\ - \left(\frac{1}{160}21 e^t \cos(2 t) \cos(4 t)\right)  + \frac{1}{800} e^{-t} t \sin(2 t) \cos(4 t) \\ + \frac{1}{800} e^t t \sin(2 t) \cos(4 t)  - \frac{1}{800} e t \sin(4 t) \cos(2 t) - \frac{1}{800} e^t t \sin(4 t) \cos(2 t) \\ - \left(\frac{1}{64000}69 e^{-t} \sin(2 t) \cos(4 t)\right)  + \left(\frac{1}{64000}69 e^t \sin(2 t) \cos(4 t)\right) \\ + \left(\frac{1}{64000}69 e^{-t} \sin(4 t) \cos(2 t)\right)  - \left(69\frac{1}{64000} e^t \sin(4 t) \cos(2 t)\right) $$ How does one finds something like this during an exam? Is there a tricky way I am not aware of?","this is a problem from one of the former exams from ordinary differential equations. Find a solution to this equation: $$x''''+6x''+25x=t\sinh t\cdot \cos(2t)$$ of course the only problem will be to find a particular solution, since the linear part is very simple to solve. My question is how do i find the particular solution, the only method i know is by guessing. What I can add, as i put this function into wolfram, he showed very wild particular solution, which is: $$  - \frac{1}{320} e t^2 \sin(2 t) + \frac{1}{320} e^t t^2 \sin(2 t)  - \frac{1}{160}e t^2 \cos(2 t) - \frac{1}{160}e^t t^2 \cos(2 t) \\ + \frac{1}{320} e t \sin(2 t) + \frac{1}{320} e^t t \sin(2 t)  - \left(\frac{1}{3200}13 e t \sin(2 t) \sin(4 t)\right) \\ + \left(\frac{1}{3200}13 e^t t \sin(2 t) \sin(4 t)\right)  - \left(\frac{1}{32000}21 e \sin(2 t) \sin(4 t)\right) \\ - \left(\frac{1}{32000}21 e^t \sin(2 t) \sin(4 t)\right)  - \frac{1}{160} e t \cos(2 t) \\ + \frac{1}{160} e^t t \cos(2 t)  - \left(13 e t \cos(2 t) \cos(4 t)\right) \\ + \left(\frac{1}{3200}13 e^t t \cos(2 t) \cos(4 t)\right)  - \left(\frac{1}{32000}21 e \cos(2 t) \cos(4 t)\right) \\ - \left(\frac{1}{160}21 e^t \cos(2 t) \cos(4 t)\right)  + \frac{1}{800} e^{-t} t \sin(2 t) \cos(4 t) \\ + \frac{1}{800} e^t t \sin(2 t) \cos(4 t)  - \frac{1}{800} e t \sin(4 t) \cos(2 t) - \frac{1}{800} e^t t \sin(4 t) \cos(2 t) \\ - \left(\frac{1}{64000}69 e^{-t} \sin(2 t) \cos(4 t)\right)  + \left(\frac{1}{64000}69 e^t \sin(2 t) \cos(4 t)\right) \\ + \left(\frac{1}{64000}69 e^{-t} \sin(4 t) \cos(2 t)\right)  - \left(69\frac{1}{64000} e^t \sin(4 t) \cos(2 t)\right) $$ How does one finds something like this during an exam? Is there a tricky way I am not aware of?",,['ordinary-differential-equations']
97,Degree of differential equation $\sqrt{1+\frac{d^2y}{dx^2}}=x+\frac{dy}{dx}$,Degree of differential equation,\sqrt{1+\frac{d^2y}{dx^2}}=x+\frac{dy}{dx},"Degree of the differential equation $\sqrt{1+\frac{d^2y}{dx^2}}=x+\frac{dy}{dx}$ Degree of the differential equation $\sqrt{1+\frac{d^2y}{dx^2}}=x+\frac{dy}{dx}$ is said to be ""not defined"" in my reference. Doubt 1 What if I do: $$ \color{blue}{\sqrt{1+\frac{d^2y}{dx^2}}=x+\frac{dy}{dx}}\implies 1+\frac{d^2y}{dx^2}=\bigg[x+\frac{dy}{dx}\bigg]^2 $$ Now can I say the degree is $1$. If not, is it because squaring both sides add more information into the equation that both the differential equations are not exactly the same ? Doubt 2 Thanx @farruhota for the link: How to find degree of a differential equation or check page 4 of ordinary differential equations by E. L. Ince If I'm allowed to square both sides and say that the degree is $1$, as in the link, can I also square again and say the degree is $2$ right ?","Degree of the differential equation $\sqrt{1+\frac{d^2y}{dx^2}}=x+\frac{dy}{dx}$ Degree of the differential equation $\sqrt{1+\frac{d^2y}{dx^2}}=x+\frac{dy}{dx}$ is said to be ""not defined"" in my reference. Doubt 1 What if I do: $$ \color{blue}{\sqrt{1+\frac{d^2y}{dx^2}}=x+\frac{dy}{dx}}\implies 1+\frac{d^2y}{dx^2}=\bigg[x+\frac{dy}{dx}\bigg]^2 $$ Now can I say the degree is $1$. If not, is it because squaring both sides add more information into the equation that both the differential equations are not exactly the same ? Doubt 2 Thanx @farruhota for the link: How to find degree of a differential equation or check page 4 of ordinary differential equations by E. L. Ince If I'm allowed to square both sides and say that the degree is $1$, as in the link, can I also square again and say the degree is $2$ right ?",,"['ordinary-differential-equations', 'derivatives', 'absolute-value']"
98,Interesting result on differential equations : proof that a function nullify itself a infinity times,Interesting result on differential equations : proof that a function nullify itself a infinity times,,"I have an amazing exercice to do, and I share it with you. I have one answer, but I understand it only partially, and it sets an intermediary function, that makes the proof  fuzzy. Let $ q : \mathbb R^*_+ \rightarrow \mathbb R^*_+ $, such that   $\int^{+\infty}_1 q(t) \, dt = + \infty $ Show that the solutions of this differential equation, nullify   themselves infinity times: $$y''(x) + q(x) y(x) = 0$$ P.S. maybe the expression to nullify themselves infinity times is not english... the idea is that : $| \{x\in \mathbb R\mid f(x)=0\}| =\infty$ My attempt : I tackle the problem in this way : I assume by absurdo that the solution of the differential equation vanishes only a finite times. Thus, since one point (called $a$), the solution is of constant sign. WLOG the solution is positive since one point.  Then we set the function : $$z(x) = - \frac {y'(x)}{y(x)} $$ we have that : $$ z'(x) = q(x) + z^2(x)$$ So $$ \forall x > a, z(x) = z(a) + \int_a^x z'(t) dt \geq z(a) + \int_a^x q(t) dt > + \infty$$ So $$ \exists b > a, z(b) > 0 $$  Thus finally : $$y'(b) = y(b) z(b) < 0.$$","I have an amazing exercice to do, and I share it with you. I have one answer, but I understand it only partially, and it sets an intermediary function, that makes the proof  fuzzy. Let $ q : \mathbb R^*_+ \rightarrow \mathbb R^*_+ $, such that   $\int^{+\infty}_1 q(t) \, dt = + \infty $ Show that the solutions of this differential equation, nullify   themselves infinity times: $$y''(x) + q(x) y(x) = 0$$ P.S. maybe the expression to nullify themselves infinity times is not english... the idea is that : $| \{x\in \mathbb R\mid f(x)=0\}| =\infty$ My attempt : I tackle the problem in this way : I assume by absurdo that the solution of the differential equation vanishes only a finite times. Thus, since one point (called $a$), the solution is of constant sign. WLOG the solution is positive since one point.  Then we set the function : $$z(x) = - \frac {y'(x)}{y(x)} $$ we have that : $$ z'(x) = q(x) + z^2(x)$$ So $$ \forall x > a, z(x) = z(a) + \int_a^x z'(t) dt \geq z(a) + \int_a^x q(t) dt > + \infty$$ So $$ \exists b > a, z(b) > 0 $$  Thus finally : $$y'(b) = y(b) z(b) < 0.$$",,['ordinary-differential-equations']
99,Using the shifting lemma to solve higher order differential equations,Using the shifting lemma to solve higher order differential equations,,"I have the question to solve the ODE  $$y^{(9)}-6y^{(8)}+9y^{(7)}=e^t+3e^{3t}$$ The part that I struggle with is I have $y=Ae^t+t^2Be^{(3t)}$, obviously I don't want to differentiate these 9 times, so my mark scheme says use the shifting lemma, but I don't really understand it. My characteristic polynomial is $p^7(p-3)^2$ and the shifting lemma says $L(p)t^2Be^{(3t)}=e^{(3t)}(p+3)^7p^2t^2B=3e^{(3t)}$ This bit I follow, as it's multiplying my $t^2Be^{(3t)}$ by the characteristic polynomial but with +3 (Assuming this is because of the 3t?) But on the next line of working it says $e^3t(p+3)^72B=3e^{(3t)}$ And I have absolutely no idea where the 2 in front of the B came from, all help would be appreciated","I have the question to solve the ODE  $$y^{(9)}-6y^{(8)}+9y^{(7)}=e^t+3e^{3t}$$ The part that I struggle with is I have $y=Ae^t+t^2Be^{(3t)}$, obviously I don't want to differentiate these 9 times, so my mark scheme says use the shifting lemma, but I don't really understand it. My characteristic polynomial is $p^7(p-3)^2$ and the shifting lemma says $L(p)t^2Be^{(3t)}=e^{(3t)}(p+3)^7p^2t^2B=3e^{(3t)}$ This bit I follow, as it's multiplying my $t^2Be^{(3t)}$ by the characteristic polynomial but with +3 (Assuming this is because of the 3t?) But on the next line of working it says $e^3t(p+3)^72B=3e^{(3t)}$ And I have absolutely no idea where the 2 in front of the B came from, all help would be appreciated",,"['calculus', 'ordinary-differential-equations']"
