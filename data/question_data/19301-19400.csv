,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Can a basis for a vector space $V$ can be restricted to a basis for any subspace $W$?,Can a basis for a vector space  can be restricted to a basis for any subspace ?,V W,"I don't understand why this statement is wrong: $V$ is a vector space, and $W$ is a subspace of $V$. $K$ is a basis of $V$. We can manage to find a subset of $K$ that will be a basis of $W$. Sorry if my English is bad... and if you can show me an example of something that contradicts it, it'd be great.","I don't understand why this statement is wrong: $V$ is a vector space, and $W$ is a subspace of $V$. $K$ is a basis of $V$. We can manage to find a subset of $K$ that will be a basis of $W$. Sorry if my English is bad... and if you can show me an example of something that contradicts it, it'd be great.",,"['linear-algebra', 'vector-spaces']"
1,Eigenvalues of symmetric orthogonal matrix,Eigenvalues of symmetric orthogonal matrix,,Can we say that Eigenvalues of symmetric orthogonal matrix must be $+1$ and $-1$ ? Since eigenvalues of symmetric matrices are real and eigenvalues of orthogonal matrix have  unit modulus. Combining both result eigenvalues of symmetric orthogonal matrices must be $+1$ and $-1$ . Please clarify whether I am correct? Is there any other approach to solve this problem? Thanks,Can we say that Eigenvalues of symmetric orthogonal matrix must be and ? Since eigenvalues of symmetric matrices are real and eigenvalues of orthogonal matrix have  unit modulus. Combining both result eigenvalues of symmetric orthogonal matrices must be and . Please clarify whether I am correct? Is there any other approach to solve this problem? Thanks,+1 -1 +1 -1,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'symmetric-matrices', 'orthogonal-matrices']"
2,Clarifying the definition of the Dual Map,Clarifying the definition of the Dual Map,,"I would appreciate help understanding the definition of the dual map $T'$ as presented in Axler's ""Linear Algebra Done Right"" 3rd ed. on page 103. If $T \in \cal {L}$$(V,W)$ then the dual map of $T$ is the linear map $T' \in \cal {L}$$(W',V')$ defined by $T'(\phi)= \phi \circ T$ for $\phi \in W'$ . With $\phi \in W'$ , on the LHS, $T'(\phi)$ maps to $V'$ , where elements of $V'$ are linear functionals on $V$ . This is my question: When looking at the RHS, $\phi \circ T$ , $T$ maps to $W$ but since $\phi \in W'$ , $\phi$ is a linear functional on $W$ and I would think should map to a scalar, not a linear functional on $V$ , i.e., not an element of $V'$ . So things don't seen to line up. Thanks.","I would appreciate help understanding the definition of the dual map as presented in Axler's ""Linear Algebra Done Right"" 3rd ed. on page 103. If then the dual map of is the linear map defined by for . With , on the LHS, maps to , where elements of are linear functionals on . This is my question: When looking at the RHS, , maps to but since , is a linear functional on and I would think should map to a scalar, not a linear functional on , i.e., not an element of . So things don't seen to line up. Thanks.","T' T \in \cal {L}(V,W) T T' \in \cal {L}(W',V') T'(\phi)= \phi \circ T \phi \in W' \phi \in W' T'(\phi) V' V' V \phi \circ T T W \phi \in W' \phi W V V'",['linear-algebra']
3,Eigenvalues of $A^TA$,Eigenvalues of,A^TA,"Suppose $A$ is a $n\times n$ matrix in $M(\mathbb{R})$ . I'd like to know if there is an exact formula for the eigenvalues of $A^TA$ . Clearly, it's false that $\lambda^2$ is an eigenvalue of $A^TA$ if $\lambda$ is an eigenvalue of $A$ .","Suppose is a matrix in . I'd like to know if there is an exact formula for the eigenvalues of . Clearly, it's false that is an eigenvalue of if is an eigenvalue of .",A n\times n M(\mathbb{R}) A^TA \lambda^2 A^TA \lambda A,"['linear-algebra', 'matrices']"
4,Why does a diagonalization of a matrix B with the basis of a commuting matrix A give a block diagonal matrix?,Why does a diagonalization of a matrix B with the basis of a commuting matrix A give a block diagonal matrix?,,"I am trying to understand a proof concerning commuting matrices and simultaneous diagonalization of these. It seems to be a well known result that when you take the eigenvectors of $A$ as a basis and diagonalize $B$ with it then you get a block diagonal matrix: $$B=  \begin{pmatrix}   B_{1} & 0 & \cdots & 0 \\   0 & B_{2} & \cdots & 0 \\   \vdots  & \vdots  & \ddots & \vdots  \\   0 & 0 & \cdots & B_{m}  \end{pmatrix},$$ where each $B_{i}$ is an $m_{g}(\lambda_{i}) \times m_{g}(\lambda_{i})$ block ($m_{g}(\lambda_{i})$ being the geometric multiplicity of $\lambda_{i}$). My question Why is this so? I calculated an example and, lo and behold, it really works :-) But I don't understand how it works out so neatly. Can you please explain this result to me in an intuitive and step-by-step manner - Thank you!","I am trying to understand a proof concerning commuting matrices and simultaneous diagonalization of these. It seems to be a well known result that when you take the eigenvectors of $A$ as a basis and diagonalize $B$ with it then you get a block diagonal matrix: $$B=  \begin{pmatrix}   B_{1} & 0 & \cdots & 0 \\   0 & B_{2} & \cdots & 0 \\   \vdots  & \vdots  & \ddots & \vdots  \\   0 & 0 & \cdots & B_{m}  \end{pmatrix},$$ where each $B_{i}$ is an $m_{g}(\lambda_{i}) \times m_{g}(\lambda_{i})$ block ($m_{g}(\lambda_{i})$ being the geometric multiplicity of $\lambda_{i}$). My question Why is this so? I calculated an example and, lo and behold, it really works :-) But I don't understand how it works out so neatly. Can you please explain this result to me in an intuitive and step-by-step manner - Thank you!",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'diagonalization', 'block-matrices']"
5,"If $A+A^T$ is negative definite, then the eigenvalues of $A$ have negative real parts?","If  is negative definite, then the eigenvalues of  have negative real parts?",A+A^T A,"Assume that $A$ is non-symmetric. ""If $A+A^T$ is positive definite, the eigenvalues of $A$ have positive real parts"" (originally, it was with negative definite/negative real parts, but I suspect it doesn't matter). This was claimed in a comment of an answer to if eigenvalues are positive, is the matrix positive definite? . Could anyone provide a proof for this? Is the other direction true? If yes can anyone present a proof, if not a counterexample. If $A$ is diagonalizable is the other direction true? If yes can anyone present a proof, if not a counterexample. Thank you. (In case anyone wonders what the motivation behind this is, I'm looking into it with the hope of getting some simple geometric intuition for the conditions of asymptotic stability of the origin of a continuous-time LTI system $\dot{x}=Ax$ (i.e. that the eigenvalues have negative real parts). If the other direction of the claim is true, it implies that the angle between the velocity vector $\dot{x}$ and the position vector $x$ is always greater than $\pi/2$, hence the state is always 'being pushed roughly in the direction of the origin'.)","Assume that $A$ is non-symmetric. ""If $A+A^T$ is positive definite, the eigenvalues of $A$ have positive real parts"" (originally, it was with negative definite/negative real parts, but I suspect it doesn't matter). This was claimed in a comment of an answer to if eigenvalues are positive, is the matrix positive definite? . Could anyone provide a proof for this? Is the other direction true? If yes can anyone present a proof, if not a counterexample. If $A$ is diagonalizable is the other direction true? If yes can anyone present a proof, if not a counterexample. Thank you. (In case anyone wonders what the motivation behind this is, I'm looking into it with the hope of getting some simple geometric intuition for the conditions of asymptotic stability of the origin of a continuous-time LTI system $\dot{x}=Ax$ (i.e. that the eigenvalues have negative real parts). If the other direction of the claim is true, it implies that the angle between the velocity vector $\dot{x}$ and the position vector $x$ is always greater than $\pi/2$, hence the state is always 'being pushed roughly in the direction of the origin'.)",,['linear-algebra']
6,What is the difference between linear space and a subspace?,What is the difference between linear space and a subspace?,,"If W is a subspace, is it also a linear space? If V is a linear space, is it also a subspace? I am having trouble wrapping my head around the difference between the two, as it seems that the way the book defines them is the following: both have to have a zero (neutral) element, both are closed under addition and scalar multiplication. Thanks!","If W is a subspace, is it also a linear space? If V is a linear space, is it also a subspace? I am having trouble wrapping my head around the difference between the two, as it seems that the way the book defines them is the following: both have to have a zero (neutral) element, both are closed under addition and scalar multiplication. Thanks!",,['linear-algebra']
7,The difference between applying a rotation matrix to a vector (points) and to a matrix (transformation),The difference between applying a rotation matrix to a vector (points) and to a matrix (transformation),,"Suppose that the rotation matrix is defined as $\mathbf{R}$ . Then in order to rotate a vector and a matrix, the following expressions are, respectively, used $\mathbf{u'}=\mathbf{R} \mathbf{u}$ and $\mathbf{U'}=\mathbf{R} \mathbf{U} \mathbf{R}^T$ , where $\mathbf{u}$ and $\mathbf{U}$ are, respectively, an arbitrary vector and an arbitrary matrix. For me, the first one is obvious since you simply multiply the rotation matrix by the vector (for example a point coordinate in 3D) and obtain the rotated vector (rotated point coordinate in 3D). However, the second one is not clear for me and why the rotation should be multiplied from both sides and how this expression is derived. P.S. The matrix $\mathbf{U}$ can be interpreted as a stretch matrix in 3D.","Suppose that the rotation matrix is defined as . Then in order to rotate a vector and a matrix, the following expressions are, respectively, used and , where and are, respectively, an arbitrary vector and an arbitrary matrix. For me, the first one is obvious since you simply multiply the rotation matrix by the vector (for example a point coordinate in 3D) and obtain the rotated vector (rotated point coordinate in 3D). However, the second one is not clear for me and why the rotation should be multiplied from both sides and how this expression is derived. P.S. The matrix can be interpreted as a stretch matrix in 3D.",\mathbf{R} \mathbf{u'}=\mathbf{R} \mathbf{u} \mathbf{U'}=\mathbf{R} \mathbf{U} \mathbf{R}^T \mathbf{u} \mathbf{U} \mathbf{U},"['linear-algebra', 'matrices', 'vectors', 'rotations']"
8,RQ decomposition,RQ decomposition,,"Can someone explain me how we can compute RQ decomposition for a given matrix (say, $3 \times 4$). I know how to compute QR decomposition. I know the function in MATLAB which computes this RQ decomposition. But, I want to know how we can do that on paper. PS: The practical use of RQ decomposition is in extracting the intrinsic and extrinsic parameters of the camera when the camera matrix $P(3 \times 4$) is given thanks!","Can someone explain me how we can compute RQ decomposition for a given matrix (say, $3 \times 4$). I know how to compute QR decomposition. I know the function in MATLAB which computes this RQ decomposition. But, I want to know how we can do that on paper. PS: The practical use of RQ decomposition is in extracting the intrinsic and extrinsic parameters of the camera when the camera matrix $P(3 \times 4$) is given thanks!",,"['linear-algebra', 'matrices', 'matrix-decomposition']"
9,Why doesn't the dot product give you the coefficients of the linear combination?,Why doesn't the dot product give you the coefficients of the linear combination?,,"So the setting is $\Bbb R^{2}$. Let's pick two unit vectors that are linearly independent.  Say: $v_{1}= \begin{bmatrix}  \frac{1}{2} \\ \frac{\sqrt{3}}{2}\end{bmatrix}$ and $v_{2} = \begin{bmatrix}  \frac{\sqrt{3}}{2} \\ \frac{1}{2}\end{bmatrix}$. Now, let's pick another vector with length smaller than $1$, say, $a = \begin{bmatrix}  \frac{1}{2} \\ 0\end{bmatrix}$. I've been trying to understand the dot product geometrically, and what I've read online has led me to believe that $a \cdot v_{1}$ is the scalar $c$ so that $cv_{1}$ is the ""shadow"" of $a$ on $v_{1}$.  Similarly, $a \cdot v_{2}$ is the scalar $d$ so that $dv_{2}$ is the ""shadow"" of $a$ on $v_{2}$. If this is true, then it should be that $cv_{1} + dv_{2} = a$, right?  But this isn't the case. We have $a \cdot v_{1} = \frac{1}{4}$ and $a \cdot v_{2} = \frac{\sqrt{3}}{4}$.  So $$cv_{1} + d v_{2} = \frac{1}{4}\begin{bmatrix}  \frac{1}{2} \\ \frac{\sqrt{3}}{2}\end{bmatrix} + \frac{\sqrt{3}}{4}\begin{bmatrix}  \frac{\sqrt{3}}{2} \\ \frac{1}{2}\end{bmatrix} =  \begin{bmatrix}  \frac{1}{2} \\ \frac{\sqrt{3}}{4}\end{bmatrix} \neq a.$$ This means something is wrong with my understanding about the intuition of the dot product.  I'm not sure what's wrong with it, though.  Any help would be appreciated.","So the setting is $\Bbb R^{2}$. Let's pick two unit vectors that are linearly independent.  Say: $v_{1}= \begin{bmatrix}  \frac{1}{2} \\ \frac{\sqrt{3}}{2}\end{bmatrix}$ and $v_{2} = \begin{bmatrix}  \frac{\sqrt{3}}{2} \\ \frac{1}{2}\end{bmatrix}$. Now, let's pick another vector with length smaller than $1$, say, $a = \begin{bmatrix}  \frac{1}{2} \\ 0\end{bmatrix}$. I've been trying to understand the dot product geometrically, and what I've read online has led me to believe that $a \cdot v_{1}$ is the scalar $c$ so that $cv_{1}$ is the ""shadow"" of $a$ on $v_{1}$.  Similarly, $a \cdot v_{2}$ is the scalar $d$ so that $dv_{2}$ is the ""shadow"" of $a$ on $v_{2}$. If this is true, then it should be that $cv_{1} + dv_{2} = a$, right?  But this isn't the case. We have $a \cdot v_{1} = \frac{1}{4}$ and $a \cdot v_{2} = \frac{\sqrt{3}}{4}$.  So $$cv_{1} + d v_{2} = \frac{1}{4}\begin{bmatrix}  \frac{1}{2} \\ \frac{\sqrt{3}}{2}\end{bmatrix} + \frac{\sqrt{3}}{4}\begin{bmatrix}  \frac{\sqrt{3}}{2} \\ \frac{1}{2}\end{bmatrix} =  \begin{bmatrix}  \frac{1}{2} \\ \frac{\sqrt{3}}{4}\end{bmatrix} \neq a.$$ This means something is wrong with my understanding about the intuition of the dot product.  I'm not sure what's wrong with it, though.  Any help would be appreciated.",,"['linear-algebra', 'vector-spaces', 'vectors']"
10,$\operatorname{adj}(AB) = \operatorname{adj} B \operatorname{adj} A$,,\operatorname{adj}(AB) = \operatorname{adj} B \operatorname{adj} A,"How can I prove that $\operatorname{adj}(AB) = \operatorname{adj} B \operatorname{adj} A$, if $A$ and $B$ are any two $n\times n$-matrices. Here, $\operatorname{adj} A$ means the adjugate of the matrix $A$. I know how to prove it for non singular matrices, but I have no idea what to do in this case.","How can I prove that $\operatorname{adj}(AB) = \operatorname{adj} B \operatorname{adj} A$, if $A$ and $B$ are any two $n\times n$-matrices. Here, $\operatorname{adj} A$ means the adjugate of the matrix $A$. I know how to prove it for non singular matrices, but I have no idea what to do in this case.",,"['linear-algebra', 'matrices', 'determinant']"
11,Is $AB+BA$ positive definite too if $A$ and $B$ are positive definite?,Is  positive definite too if  and  are positive definite?,AB+BA A B,I have a question: Is $AB+BA$ positive definite too if $A$ and $B$ are positive definite matrices?,I have a question: Is $AB+BA$ positive definite too if $A$ and $B$ are positive definite matrices?,,"['linear-algebra', 'matrices']"
12,Family of sets with $|F_i| \equiv 2\pmod 3$ and $|F_i \cap F_j| \equiv 0 \pmod 3$,Family of sets with  and,|F_i| \equiv 2\pmod 3 |F_i \cap F_j| \equiv 0 \pmod 3,"Let $p$ be a prime. By considering the incidence vectors of subsets $F_1,\ldots,F_m$ of $\{1,2,\ldots,n\}$ , such that $|F_i| = a \not\equiv 0 \pmod p$ and $|F_i \cap F_j| \equiv 0 \pmod p$ for all $1\leq i<j \leq m$ , we can show $m\leq n$ (the vectors are linearly independent). For $p=2$ this bound is achievable by the singletons $\{\{1\},\{2\},\ldots,\{n\}\}$ , same for $p=3$ with $a=1$ . But what about $p=3$ with $a=2$ ? Is there an example of size $n$ (or perhaps $n - c$ for some ""small"" constant $c$ )?. Observing such examples could help towards ideas of how to improve the bound $m\leq n$ where possible.","Let be a prime. By considering the incidence vectors of subsets of , such that and for all , we can show (the vectors are linearly independent). For this bound is achievable by the singletons , same for with . But what about with ? Is there an example of size (or perhaps for some ""small"" constant )?. Observing such examples could help towards ideas of how to improve the bound where possible.","p F_1,\ldots,F_m \{1,2,\ldots,n\} |F_i| = a \not\equiv 0 \pmod p |F_i \cap F_j| \equiv 0 \pmod p 1\leq i<j \leq m m\leq n p=2 \{\{1\},\{2\},\ldots,\{n\}\} p=3 a=1 p=3 a=2 n n - c c m\leq n","['linear-algebra', 'combinatorics', 'finite-fields', 'extremal-combinatorics', 'algebraic-combinatorics']"
13,Checking connectivity of adjacency matrix,Checking connectivity of adjacency matrix,,"What do you think is the most efficient algorithm for checking whether a graph represented by an adjacency matrix is connected? In my case I'm also given the weights of each edge. There is another question very similar to mine: How to test if a graph is fully connected and finding isolated graphs from an adjacency matrix That answer seems to be good, except I don't really understand it. How does repeatedly squaring the matrix give information about its connectivity? There is another an answer that claims eigenvectors also give information about the connectivity of the graph, could anyone explain that as well? I'm asking this because I don't have the background to understand the answers given, I'm just solving a problem that has to do with these topics. Searching around on google didn't give me an answer either, so hopefully someone can clear it up.","What do you think is the most efficient algorithm for checking whether a graph represented by an adjacency matrix is connected? In my case I'm also given the weights of each edge. There is another question very similar to mine: How to test if a graph is fully connected and finding isolated graphs from an adjacency matrix That answer seems to be good, except I don't really understand it. How does repeatedly squaring the matrix give information about its connectivity? There is another an answer that claims eigenvectors also give information about the connectivity of the graph, could anyone explain that as well? I'm asking this because I don't have the background to understand the answers given, I'm just solving a problem that has to do with these topics. Searching around on google didn't give me an answer either, so hopefully someone can clear it up.",,"['linear-algebra', 'matrices', 'graph-theory', 'algorithms']"
14,What are the eigenvalues of $\operatorname{ad}x$?,What are the eigenvalues of ?,\operatorname{ad}x,"Let $x\in \operatorname{gl}(n,F)$ have $n$ distinct eigenvalues $a_1,\ldots,a_n$ in $F$. Prove that the eigenvalues of $\text{ad }x$ are precisely the $n^2$ scalars $a_i-a_j$ ($1\leq i,j\leq n$), which of course need not be distinct. So we can represent $x$ by an $n\times n$ matrix $X$. We have $Xv_1=a_1v_1,\ldots, Xv_n=a_nv_n$ for eigenvectors $v_1,\ldots,v_n$. Now, $\operatorname{ad}x$ takes $y\in \operatorname{gl}(n,F)$ to $xy-yx$. I need to show that some $y$ is taken to a scalar multiple of itself, where that scalar is $a_i-a_j$. What could be that $y$?","Let $x\in \operatorname{gl}(n,F)$ have $n$ distinct eigenvalues $a_1,\ldots,a_n$ in $F$. Prove that the eigenvalues of $\text{ad }x$ are precisely the $n^2$ scalars $a_i-a_j$ ($1\leq i,j\leq n$), which of course need not be distinct. So we can represent $x$ by an $n\times n$ matrix $X$. We have $Xv_1=a_1v_1,\ldots, Xv_n=a_nv_n$ for eigenvectors $v_1,\ldots,v_n$. Now, $\operatorname{ad}x$ takes $y\in \operatorname{gl}(n,F)$ to $xy-yx$. I need to show that some $y$ is taken to a scalar multiple of itself, where that scalar is $a_i-a_j$. What could be that $y$?",,"['linear-algebra', 'eigenvalues-eigenvectors', 'lie-algebras']"
15,matrix similarity upper triangular matrix [closed],matrix similarity upper triangular matrix [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question How to show: Any matrix A with real or complex entries is similar to an upper triangular matrix M whose diagonal entries are the eigenvalue of A. Thank you!","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question How to show: Any matrix A with real or complex entries is similar to an upper triangular matrix M whose diagonal entries are the eigenvalue of A. Thank you!",,"['linear-algebra', 'numerical-methods', 'numerical-linear-algebra']"
16,Simple proof that if $A^n=I$ then $\mathrm{tr}(A^{-1})=\overline{\mathrm{tr}(A)}$,Simple proof that if  then,A^n=I \mathrm{tr}(A^{-1})=\overline{\mathrm{tr}(A)},"Let $A$ be a linear map from a finite dimensional complex vector space to itself. If $A$ has finite order then the trace of its inverse is the conjugate of its trace. I know two proofs of this fact, but they both require linear algebra facts whose proofs are themselves quite involved. Since $A^n=I$, the eigenvalues of $A$ are roots of unity. Hence they have unit norm, and so their reciprocals are their conjugates. Then the result follows from following facts: (a) The eigenvalues of $A^{-1}$ are the reciprocals of the eigenvalues of $A$, (b) the dimensions of the eigenspaces of $A^{-1}$ are equal to the dimensions of the corresponding eigenspaces of $A$, (c) the trace is equal to the sum of the (generalised) eigenvalues. The proof of (a) is relatively easy, but (b) and (c) seem to require the existence of Jordan Normal Form, which requires a lot of work. By Weyl's Unitary Trick , there's a inner product for which $A$ is unitary (this proof is itself a fair amount of work). So in an orthonormal basis (which we must construct with the Gram-Schmidt procedure) the inverse of $A$ is given by its conjugate transpose (one must also prove this). So the trace of the inverse is the conjugate of the trace. Since  the condition $A^n=I$ and the consequence $\mathrm{tr}(A^{-1})=\overline{\mathrm{tr}(A)}$ are both elementary statements, I'm wondering if there's a short proof from first principles (ideally without quoting any big linear algebra Theorems). Can anyone find one?","Let $A$ be a linear map from a finite dimensional complex vector space to itself. If $A$ has finite order then the trace of its inverse is the conjugate of its trace. I know two proofs of this fact, but they both require linear algebra facts whose proofs are themselves quite involved. Since $A^n=I$, the eigenvalues of $A$ are roots of unity. Hence they have unit norm, and so their reciprocals are their conjugates. Then the result follows from following facts: (a) The eigenvalues of $A^{-1}$ are the reciprocals of the eigenvalues of $A$, (b) the dimensions of the eigenspaces of $A^{-1}$ are equal to the dimensions of the corresponding eigenspaces of $A$, (c) the trace is equal to the sum of the (generalised) eigenvalues. The proof of (a) is relatively easy, but (b) and (c) seem to require the existence of Jordan Normal Form, which requires a lot of work. By Weyl's Unitary Trick , there's a inner product for which $A$ is unitary (this proof is itself a fair amount of work). So in an orthonormal basis (which we must construct with the Gram-Schmidt procedure) the inverse of $A$ is given by its conjugate transpose (one must also prove this). So the trace of the inverse is the conjugate of the trace. Since  the condition $A^n=I$ and the consequence $\mathrm{tr}(A^{-1})=\overline{\mathrm{tr}(A)}$ are both elementary statements, I'm wondering if there's a short proof from first principles (ideally without quoting any big linear algebra Theorems). Can anyone find one?",,"['linear-algebra', 'matrices', 'representation-theory', 'linear-transformations']"
17,Why are we allowed to multiply a 1x1 matrix by any matrix? [duplicate],Why are we allowed to multiply a 1x1 matrix by any matrix? [duplicate],,"This question already has answers here : Multiplying by a $1\times 1$ matrix? (3 answers) Closed 7 years ago . So, in order to multiply 2 matrices, there must be the same number of columns in the left matrix as there are rows in the right matrix. So if $A$ is an $m \times n$ matrix and $B$ is a $p \times q$ matrix, must not $n = p$ in order for $AB$ to exist and mustn't $q = m$ in order for $BA$ to exist. By this logic, we should only be allowed to multiply a $1 \times 1$ matrix by either a $1 \times n$ matrix on the right or a $n \times 1$ matrix on the left. However, if $C$ is a $1 \times 1$ matrix and $D$ is a $m \times n$ matrix, where neither $m$ nor $n = 1$, we're allowed to multiply the 2 matrices simply by multiplying each entry in $D$ by the entry in $C$. Why? Shouldn't the rules for a $1 \times 1$ matrix be the same as for all the other matrices?","This question already has answers here : Multiplying by a $1\times 1$ matrix? (3 answers) Closed 7 years ago . So, in order to multiply 2 matrices, there must be the same number of columns in the left matrix as there are rows in the right matrix. So if $A$ is an $m \times n$ matrix and $B$ is a $p \times q$ matrix, must not $n = p$ in order for $AB$ to exist and mustn't $q = m$ in order for $BA$ to exist. By this logic, we should only be allowed to multiply a $1 \times 1$ matrix by either a $1 \times n$ matrix on the right or a $n \times 1$ matrix on the left. However, if $C$ is a $1 \times 1$ matrix and $D$ is a $m \times n$ matrix, where neither $m$ nor $n = 1$, we're allowed to multiply the 2 matrices simply by multiplying each entry in $D$ by the entry in $C$. Why? Shouldn't the rules for a $1 \times 1$ matrix be the same as for all the other matrices?",,['linear-algebra']
18,Calculate center of circle tangent to two lines in space,Calculate center of circle tangent to two lines in space,,"Good afternoon everyone! I am facing a problem which is straining my memory of linear algebra. I have: Three points with known coordinates, forming a triangle in space. Let the coordinates be R(top), P(left bottom) and Q(right bottom) (only rough positions) I'm not interested in the triangle as such, but in its two lines QP and QR These lines are tangent to a circle of known radius (basically I'm trying to smooth the angle via a radius, like in CAD) I need the equation of the circle, so I can pick any point I want between P and R to smooth out the angle. The angle is <180°, so there should exist one solution (correct me if I'm wrong) I found an image which illustrates my problem: You can see my points R,P,Q, aswell as my circle which is tangent to both rays originating in Q. Please note, that PQ does not necessarily have to be horizontal and that the angle $\alpha$ is not always 50°. My goal is to calculate the origin O and thus the complete equation of my circle in the form $\vec{r}(t)=\vec{c}+r\cdot\cos{\varphi}\cdot\vec{a}+r\cdot\sin{\varphi}\cdot\vec{b}$ Plan I have made so far: Calculate $\vec{PR}$ Calculate $a=\arccos{\frac{\vec{QP}\bullet\vec{QR}}{\left|\vec{QP}\right|\cdot\left|\vec{QR}\right|}}$ Calculate $b=\frac{\pi}{2}-a$ From here on it gets tricky. I know, that the origin is on the ray seperating the angle in Q in exact half. If I project that ray on my line $\vec{PQ}$, will I end up in the exact middle? Couldn't I just do something like ""rotate $\frac{\vec{PR}}{2}$ around an axis through P by b degrees ccw, where the axis is perpendicular to the triangles plane"" I start to get lost here. The perpendicular vector would be $\vec{QP}\times\vec{QR}$, wouldn't it? The German Wikipedia suggests for rotating via an rotation-matrix $R_{\hat{n}}(\alpha)\vec{x}=\hat{n}(\hat{n}\cdot\vec{x})+\cos\left(\alpha\right)(\hat{n}\times\vec{x})\times\hat{n}+\sin\left(\alpha\right)(\hat{n}\times\vec{x})$ where $\vec{n}$ is the unity-normal-vector around which to rotate. Can I use this formula? How do I finally compile my circle-equation? Edit: And yes, I have seen this , but it didn't help :-)","Good afternoon everyone! I am facing a problem which is straining my memory of linear algebra. I have: Three points with known coordinates, forming a triangle in space. Let the coordinates be R(top), P(left bottom) and Q(right bottom) (only rough positions) I'm not interested in the triangle as such, but in its two lines QP and QR These lines are tangent to a circle of known radius (basically I'm trying to smooth the angle via a radius, like in CAD) I need the equation of the circle, so I can pick any point I want between P and R to smooth out the angle. The angle is <180°, so there should exist one solution (correct me if I'm wrong) I found an image which illustrates my problem: You can see my points R,P,Q, aswell as my circle which is tangent to both rays originating in Q. Please note, that PQ does not necessarily have to be horizontal and that the angle $\alpha$ is not always 50°. My goal is to calculate the origin O and thus the complete equation of my circle in the form $\vec{r}(t)=\vec{c}+r\cdot\cos{\varphi}\cdot\vec{a}+r\cdot\sin{\varphi}\cdot\vec{b}$ Plan I have made so far: Calculate $\vec{PR}$ Calculate $a=\arccos{\frac{\vec{QP}\bullet\vec{QR}}{\left|\vec{QP}\right|\cdot\left|\vec{QR}\right|}}$ Calculate $b=\frac{\pi}{2}-a$ From here on it gets tricky. I know, that the origin is on the ray seperating the angle in Q in exact half. If I project that ray on my line $\vec{PQ}$, will I end up in the exact middle? Couldn't I just do something like ""rotate $\frac{\vec{PR}}{2}$ around an axis through P by b degrees ccw, where the axis is perpendicular to the triangles plane"" I start to get lost here. The perpendicular vector would be $\vec{QP}\times\vec{QR}$, wouldn't it? The German Wikipedia suggests for rotating via an rotation-matrix $R_{\hat{n}}(\alpha)\vec{x}=\hat{n}(\hat{n}\cdot\vec{x})+\cos\left(\alpha\right)(\hat{n}\times\vec{x})\times\hat{n}+\sin\left(\alpha\right)(\hat{n}\times\vec{x})$ where $\vec{n}$ is the unity-normal-vector around which to rotate. Can I use this formula? How do I finally compile my circle-equation? Edit: And yes, I have seen this , but it didn't help :-)",,"['linear-algebra', 'circles', 'vectors']"
19,How to determine if a linear system is solvable,How to determine if a linear system is solvable,,"I have this problem $$\begin{array}{rcccccl} 3x &-&y &+& 2z &=& 2\\ 2x &+& y &+& z &=& -1\\ x &+& 3y & & &=&-1 \end{array}$$ This gives me the matrix: $$\left(\begin{array}{rrr|r} 3 & -1 & 2 & 2\\ 2 & 1 & 1 & -1\\ 1 & 3 & 0 & -1 \end{array}\right)$$ I remember something about if you can show that a system in inconsistent, you know it's not solvable. I set Row1->Row1 - 2*Row2 + Row3 The result is: $$\left(\begin{array}{rrr|r} 0 & 0 & 0 & 3\\ 2 & 1 & 1 & -1\\ 1 & 3 & 0 & -1 \end{array}\right)$$ So 0 = 3 which is inconsistent. Two things, did I do this right and is my assumption that all linear systems that can be shown as inconsistent are unsolvable? Are there less painful ways to show it's not solvable? It seems like this could go on for a while with trial and error to either find an inconsistency or an answer for x y and z.","I have this problem $$\begin{array}{rcccccl} 3x &-&y &+& 2z &=& 2\\ 2x &+& y &+& z &=& -1\\ x &+& 3y & & &=&-1 \end{array}$$ This gives me the matrix: $$\left(\begin{array}{rrr|r} 3 & -1 & 2 & 2\\ 2 & 1 & 1 & -1\\ 1 & 3 & 0 & -1 \end{array}\right)$$ I remember something about if you can show that a system in inconsistent, you know it's not solvable. I set Row1->Row1 - 2*Row2 + Row3 The result is: $$\left(\begin{array}{rrr|r} 0 & 0 & 0 & 3\\ 2 & 1 & 1 & -1\\ 1 & 3 & 0 & -1 \end{array}\right)$$ So 0 = 3 which is inconsistent. Two things, did I do this right and is my assumption that all linear systems that can be shown as inconsistent are unsolvable? Are there less painful ways to show it's not solvable? It seems like this could go on for a while with trial and error to either find an inconsistency or an answer for x y and z.",,['linear-algebra']
20,Understanding the Gram-Schmidt process,Understanding the Gram-Schmidt process,,"I would like to better understand the gram-schmidt process. The statement of the theorem in my textbook is the following: The Gram-Schmidt sequence $[u_1, u_2,\ldots]$ has the property that $\{u_1, u_2,\ldots, u_n\}$ is an orthonormal base for the linear span of $\{x_1, x_2, \ldots, x_k\}$ for $k\geq 1$. The formula for $\{u_1, u_2,\ldots, u_n\}$ is:   \begin{equation} x_k   = \left|\left| x_k - \sum\limits_{i<k}\langle x_k, u_i\rangle u_i \right|\right|_2^{-1} \left(x_k - \sum\limits_{i<k}\langle x_k, u_i\rangle u_i\right) \end{equation} Note that I am primarily interested in how all of the vectors are orthogonal. The norm term in the above equation tells me that all the vectors will be unit vectors and hence we get an orthonormal set. Anyway, I see how this works algebraically; Let $v = x_k - \sum\limits_{i<k}\langle x_k, u_i\rangle u_i$. Now, take the dot product of $\langle v, u_j\rangle$ for some $j<k$: \begin{equation}   \langle v, u_j\rangle = \langle x_k, u_j\rangle - \sum\limits_{i<k}\langle x_k, u_i\rangle\langle u_i, u_j\rangle \end{equation} When we assume in the induction hypothesis that we have an orthonormal basis for $i<k$ then the sum is zero except when $i=j$. This leaves us with: \begin{equation}   \langle v, u_j\rangle = \langle x_k, u_j\rangle - \langle x_k, u_j\rangle = 0 \end{equation} OK, I can logically follow algebra, but how can I see this geometrically? Can someone provide both 2D and 3D examples/plots? Since I am specifically interested in seeing how all the vectors meet at 90 degrees.","I would like to better understand the gram-schmidt process. The statement of the theorem in my textbook is the following: The Gram-Schmidt sequence $[u_1, u_2,\ldots]$ has the property that $\{u_1, u_2,\ldots, u_n\}$ is an orthonormal base for the linear span of $\{x_1, x_2, \ldots, x_k\}$ for $k\geq 1$. The formula for $\{u_1, u_2,\ldots, u_n\}$ is:   \begin{equation} x_k   = \left|\left| x_k - \sum\limits_{i<k}\langle x_k, u_i\rangle u_i \right|\right|_2^{-1} \left(x_k - \sum\limits_{i<k}\langle x_k, u_i\rangle u_i\right) \end{equation} Note that I am primarily interested in how all of the vectors are orthogonal. The norm term in the above equation tells me that all the vectors will be unit vectors and hence we get an orthonormal set. Anyway, I see how this works algebraically; Let $v = x_k - \sum\limits_{i<k}\langle x_k, u_i\rangle u_i$. Now, take the dot product of $\langle v, u_j\rangle$ for some $j<k$: \begin{equation}   \langle v, u_j\rangle = \langle x_k, u_j\rangle - \sum\limits_{i<k}\langle x_k, u_i\rangle\langle u_i, u_j\rangle \end{equation} When we assume in the induction hypothesis that we have an orthonormal basis for $i<k$ then the sum is zero except when $i=j$. This leaves us with: \begin{equation}   \langle v, u_j\rangle = \langle x_k, u_j\rangle - \langle x_k, u_j\rangle = 0 \end{equation} OK, I can logically follow algebra, but how can I see this geometrically? Can someone provide both 2D and 3D examples/plots? Since I am specifically interested in seeing how all the vectors meet at 90 degrees.",,"['linear-algebra', 'geometry', 'orthonormal', 'visualization', 'gram-schmidt']"
21,Dual Space Isomorphism,Dual Space Isomorphism,,"If $V$ is a finite dimensional real vector space. Let  $$ V^* = \{f: V \to \mathbb{R} : f ~\text{is linear}\} $$ (Note $V^*$ is called the dual space of $V$.) Prove the vector spaces $V$ and $V^*$ are isomorphic. I have attempted this question by trying to prove the dimensions of both vector spaces are equal.  $$ V = Span(v_1,\ldots,v_n) $$ $f_i$ is an element of $V^*$ and $f_i(v_j) = 1$ when $f_i = v_i$ and $0$ otherwise and now I want to show $V^*$ is spanned by $\{f_1,\ldots,f_n\}$. Can somebody please help me out? Is this even possible or am I way off?","If $V$ is a finite dimensional real vector space. Let  $$ V^* = \{f: V \to \mathbb{R} : f ~\text{is linear}\} $$ (Note $V^*$ is called the dual space of $V$.) Prove the vector spaces $V$ and $V^*$ are isomorphic. I have attempted this question by trying to prove the dimensions of both vector spaces are equal.  $$ V = Span(v_1,\ldots,v_n) $$ $f_i$ is an element of $V^*$ and $f_i(v_j) = 1$ when $f_i = v_i$ and $0$ otherwise and now I want to show $V^*$ is spanned by $\{f_1,\ldots,f_n\}$. Can somebody please help me out? Is this even possible or am I way off?",,"['linear-algebra', 'vector-spaces', 'vector-space-isomorphism']"
22,Fractional power of matrix,Fractional power of matrix,,"If I am given a matrix, for example $A = \begin{bmatrix}        0.7 & 0.2 & 0.1           \\[0.3em]        0.2 & 0.5           & 0.3 \\[0.3em]        0           & 0 & 1      \end{bmatrix}$, how do I calculate the fractional matrix like $A^{\frac{1}{2}}, A^{\frac{3}{2}}$?","If I am given a matrix, for example $A = \begin{bmatrix}        0.7 & 0.2 & 0.1           \\[0.3em]        0.2 & 0.5           & 0.3 \\[0.3em]        0           & 0 & 1      \end{bmatrix}$, how do I calculate the fractional matrix like $A^{\frac{1}{2}}, A^{\frac{3}{2}}$?",,"['linear-algebra', 'matrices']"
23,Sum of Singular Values of (A+B),Sum of Singular Values of (A+B),,How we can prove that: $$\sum_{i=1}^n\sigma_i(A+B)\leq\sum_{i=1}^n\sigma_i(A)+\sum_{i=1}^n\sigma_i(B)$$ Where $\sigma_i$s are singular values  $\sigma_1\geq\sigma_2\geq\cdots\geq\sigma_n\geq0$ .,How we can prove that: $$\sum_{i=1}^n\sigma_i(A+B)\leq\sum_{i=1}^n\sigma_i(A)+\sum_{i=1}^n\sigma_i(B)$$ Where $\sigma_i$s are singular values  $\sigma_1\geq\sigma_2\geq\cdots\geq\sigma_n\geq0$ .,,"['linear-algebra', 'matrices', 'svd']"
24,Kostrikin's Definition of Tensor Product,Kostrikin's Definition of Tensor Product,,"I'm having serious trouble to understand the definition of tensor products from Kostrikin's Linear Algebra and Geometry. Until now I've understood a tensor as a multilinear map from the cartesian product of $k$ copies of a vector space $V$ to the underlying field of scalars. However I was instructed to study the multilinear construction from Kostrikin's book, and I'm really not understanding what he's trying to do. Well, i'll post how he constructs the tensor product, and then post my doubts. Any help or reference is very good. He constructs the tensor product in three steps: Let $L_1,L_2\cdots L_p$ a family of vector spaces over the same field $\mathbb{K}$. Define the set $M$ of all functions with finite support on $L_1\times L_2\times\cdots\times L_p$ and with values in $\mathbb{K}$, in other words, all functions from this cartesian product to $\mathbb{K}$ which vanish at all except a finite number of points of the domain. One basis consists of the functions $\delta(l_1, l_2 \dots l_p)$ which is $1$ at the point $(l_1, l_2 ...l_p)$ and zero everywhere else. He also omits the $\delta$ symbol, so that we have $$M=\left\{\sum a_{l_1\cdots l_p}(l_1,l_2\cdots l_p) \mid a_{l_1\cdots l_p} \in \mathbb{K} \right\}$$ Consider the subspace $M_0$ generated by the vectors on $M$ of the form: $$(l_1\cdots l'_j+l''_j\cdots l_p) - (l_1\cdots l'_j\cdots l_p) - (l_1\cdots l''_j\cdots l_p)$$ $$(l_1\cdots al_j\cdots l_p) - a(l_1\cdots l_j\cdots l_p) \quad \quad a\in \mathbb{K}$$ The tensor product is then defined as  $$L_1\otimes L_2 \otimes \cdots \otimes L_p = M / M_0$$ $$l_1\otimes l_2 \otimes \cdots \otimes l_p = (l_1, l_2\cdots l_p) + M_0$$ $$t: L_1 \times L_2 \times \cdots \times L_p \to L_1\otimes L_2 \otimes \cdots \otimes L_p, \quad t(l_1, l_2 \cdots l_p) = l_1 \otimes l_2 \otimes \cdots \otimes l_p$$ Well, I already did my best to understand this definition, but it just can't get it. First, he says that this definition is being made to be able to construct some universal multilienear application. That's fine, but why this definition allows us this? What's the intuition behind this definition? Second, on step 1 he consider functions that vanishes at all points except a finite number. Well, why these delta functions form a basis ? I don't understand why, since for me it seems that for using those delta, we would need to know on which points the functions is not zero. Third, why considering that subspace $M_0$ ? I just can't grasp what's the reason for this. And finally fourth: why setting the tensor product as that quotient space ? I really thought a lot already on this definition, trying to came out with something, however I couldn't have any idea. Can someone give some help? Thanks in advance, and really sorry if this question is too long, too specific or too silly. I'm just very confused with this definition.","I'm having serious trouble to understand the definition of tensor products from Kostrikin's Linear Algebra and Geometry. Until now I've understood a tensor as a multilinear map from the cartesian product of $k$ copies of a vector space $V$ to the underlying field of scalars. However I was instructed to study the multilinear construction from Kostrikin's book, and I'm really not understanding what he's trying to do. Well, i'll post how he constructs the tensor product, and then post my doubts. Any help or reference is very good. He constructs the tensor product in three steps: Let $L_1,L_2\cdots L_p$ a family of vector spaces over the same field $\mathbb{K}$. Define the set $M$ of all functions with finite support on $L_1\times L_2\times\cdots\times L_p$ and with values in $\mathbb{K}$, in other words, all functions from this cartesian product to $\mathbb{K}$ which vanish at all except a finite number of points of the domain. One basis consists of the functions $\delta(l_1, l_2 \dots l_p)$ which is $1$ at the point $(l_1, l_2 ...l_p)$ and zero everywhere else. He also omits the $\delta$ symbol, so that we have $$M=\left\{\sum a_{l_1\cdots l_p}(l_1,l_2\cdots l_p) \mid a_{l_1\cdots l_p} \in \mathbb{K} \right\}$$ Consider the subspace $M_0$ generated by the vectors on $M$ of the form: $$(l_1\cdots l'_j+l''_j\cdots l_p) - (l_1\cdots l'_j\cdots l_p) - (l_1\cdots l''_j\cdots l_p)$$ $$(l_1\cdots al_j\cdots l_p) - a(l_1\cdots l_j\cdots l_p) \quad \quad a\in \mathbb{K}$$ The tensor product is then defined as  $$L_1\otimes L_2 \otimes \cdots \otimes L_p = M / M_0$$ $$l_1\otimes l_2 \otimes \cdots \otimes l_p = (l_1, l_2\cdots l_p) + M_0$$ $$t: L_1 \times L_2 \times \cdots \times L_p \to L_1\otimes L_2 \otimes \cdots \otimes L_p, \quad t(l_1, l_2 \cdots l_p) = l_1 \otimes l_2 \otimes \cdots \otimes l_p$$ Well, I already did my best to understand this definition, but it just can't get it. First, he says that this definition is being made to be able to construct some universal multilienear application. That's fine, but why this definition allows us this? What's the intuition behind this definition? Second, on step 1 he consider functions that vanishes at all points except a finite number. Well, why these delta functions form a basis ? I don't understand why, since for me it seems that for using those delta, we would need to know on which points the functions is not zero. Third, why considering that subspace $M_0$ ? I just can't grasp what's the reason for this. And finally fourth: why setting the tensor product as that quotient space ? I really thought a lot already on this definition, trying to came out with something, however I couldn't have any idea. Can someone give some help? Thanks in advance, and really sorry if this question is too long, too specific or too silly. I'm just very confused with this definition.",,"['linear-algebra', 'multilinear-algebra']"
25,What is linear programming?,What is linear programming?,,"I asked this question on Stack Overflow but it was closed as ""not programming related"". So I think this is probably the best place for it... I read over the wikipedia article , but it seems to be beyond my comprehension. It says it's for optimization, but how is it different than any other method for optimizing things? An answer that introduces me to linear programming so I can begin diving into some less beginner-accessible material would be most helpful.","I asked this question on Stack Overflow but it was closed as ""not programming related"". So I think this is probably the best place for it... I read over the wikipedia article , but it seems to be beyond my comprehension. It says it's for optimization, but how is it different than any other method for optimizing things? An answer that introduces me to linear programming so I can begin diving into some less beginner-accessible material would be most helpful.",,"['linear-algebra', 'algorithms', 'computer-science', 'optimization', 'linear-programming']"
26,Can a symmetric matrix become non-symmetric by changing the basis?,Can a symmetric matrix become non-symmetric by changing the basis?,,"We know that a hermitian matrix is a matrix which satisfies $A=A^*$ , where $A^*$ is the conjugate transpose. A symmetric matrix (special case of hermitian - with real entries) is one for which $A=A^T$ . Observation: this property is dependent on choice of basis. We know that we can even choose a basis where these matrices are diagonal (spectral theorem). So, my question is: Is this observation correct? Can we choose a basis where such matrices are not hermitian  or symmetric? If so, is there a characterization of operators whose matrices can be hermitian or symmetric in some basis? The following is a paragraph from wiki page which I'm unable to understand. Can someone shed light on this? ... Denote by $ \langle \cdot,\cdot \rangle $ the standard inner product on $R^n$ . The real $n-by-n$ matrix $A$ is symmetric if and only if $\langle Ax,y \rangle = \langle x, Ay\rangle \quad \forall x,y\in\Bbb{R}^n$ . Since this definition is independent of the choice of basis, symmetry is a property that depends only on the linear operator A and a choice of inner product. This characterization of symmetry is useful, for example, in differential geometry, for each tangent space to a manifold may be endowed with an inner product, giving rise to what is called a Riemannian manifold. Another area where this formulation is used is in Hilbert spaces...","We know that a hermitian matrix is a matrix which satisfies , where is the conjugate transpose. A symmetric matrix (special case of hermitian - with real entries) is one for which . Observation: this property is dependent on choice of basis. We know that we can even choose a basis where these matrices are diagonal (spectral theorem). So, my question is: Is this observation correct? Can we choose a basis where such matrices are not hermitian  or symmetric? If so, is there a characterization of operators whose matrices can be hermitian or symmetric in some basis? The following is a paragraph from wiki page which I'm unable to understand. Can someone shed light on this? ... Denote by the standard inner product on . The real matrix is symmetric if and only if . Since this definition is independent of the choice of basis, symmetry is a property that depends only on the linear operator A and a choice of inner product. This characterization of symmetry is useful, for example, in differential geometry, for each tangent space to a manifold may be endowed with an inner product, giving rise to what is called a Riemannian manifold. Another area where this formulation is used is in Hilbert spaces...","A=A^* A^* A=A^T  \langle \cdot,\cdot \rangle  R^n n-by-n A \langle Ax,y \rangle = \langle x, Ay\rangle \quad \forall x,y\in\Bbb{R}^n","['linear-algebra', 'matrices', 'symmetric-matrices', 'hermitian-matrices']"
27,Is the inverse of a symmetric positive semidefinite matrix also a symmetric positive semidefinite matrix?,Is the inverse of a symmetric positive semidefinite matrix also a symmetric positive semidefinite matrix?,,"If we let $$S_{++}^n(\mathbb{R})$$ denote the set of all square symmetric positive definite matrix over the real numbers, then is it true if $A\in S_{++}(\mathbb{R}) \implies A^{-1} \in S_{++}(\mathbb{R})$? EDIT Never mind, if the eigenvalues of $A^{-1}$ are the inverse of $A$ So $A \in S_{++}  \iff A ^{-1} \in S_{++}$. This answers my own question.","If we let $$S_{++}^n(\mathbb{R})$$ denote the set of all square symmetric positive definite matrix over the real numbers, then is it true if $A\in S_{++}(\mathbb{R}) \implies A^{-1} \in S_{++}(\mathbb{R})$? EDIT Never mind, if the eigenvalues of $A^{-1}$ are the inverse of $A$ So $A \in S_{++}  \iff A ^{-1} \in S_{++}$. This answers my own question.",,"['linear-algebra', 'analysis']"
28,Squared norm of matrix equal to squared norm of its transpose,Squared norm of matrix equal to squared norm of its transpose,,"With the definition of matrix norm as $$\|M \|=\sup_x \{  |Mx|: |x|=1 \},$$ where $M$ is square and $|\cdot|$ denotes the standard euclidean 2-norm. I'm trying to prove that $$\|M\|^2=\|M^T\|^2 = \mathrm{largest \; eigenvalue \; of \;} M^TM?$$","With the definition of matrix norm as $$\|M \|=\sup_x \{  |Mx|: |x|=1 \},$$ where $M$ is square and $|\cdot|$ denotes the standard euclidean 2-norm. I'm trying to prove that $$\|M\|^2=\|M^T\|^2 = \mathrm{largest \; eigenvalue \; of \;} M^TM?$$",,"['linear-algebra', 'matrices']"
29,Solve a linear system with more variables than equations,Solve a linear system with more variables than equations,,"Suppose that, after a series of elementary row operations the augmented matrix of a linear system with variables $x_1$, $x_2$, $x_3$, $x_4$ is transformed into reduced row echelon form as follows: $$\left(\begin{array}{cccc|c}1 & 0 & 0 & 1 & 0\\0 & 1 & 0 & 2 & 1 \\0 & 0 & 1 & 3 & 0 \end{array}\right)$$. Can I solve the linear system as below? Let $t$ be an arbitrary real number. Then solving each linear equation corresponding to the augmented matrix for leading variable and setting $x_4=t$, we get $x_1=-t, x_2=1-2t$, and $x_3=-3t$. Thus the general solution of the linear system is \begin{align} x_1=-t\\ x_2=1-2t\\ x_3=-3t\\ \end{align} where t is an arbitrary real number.","Suppose that, after a series of elementary row operations the augmented matrix of a linear system with variables $x_1$, $x_2$, $x_3$, $x_4$ is transformed into reduced row echelon form as follows: $$\left(\begin{array}{cccc|c}1 & 0 & 0 & 1 & 0\\0 & 1 & 0 & 2 & 1 \\0 & 0 & 1 & 3 & 0 \end{array}\right)$$. Can I solve the linear system as below? Let $t$ be an arbitrary real number. Then solving each linear equation corresponding to the augmented matrix for leading variable and setting $x_4=t$, we get $x_1=-t, x_2=1-2t$, and $x_3=-3t$. Thus the general solution of the linear system is \begin{align} x_1=-t\\ x_2=1-2t\\ x_3=-3t\\ \end{align} where t is an arbitrary real number.",,"['linear-algebra', 'systems-of-equations']"
30,To what extent is the Singular Value Decomposition unique? [duplicate],To what extent is the Singular Value Decomposition unique? [duplicate],,"This question already has answers here : How unique are $U$ and $V$ in the singular value decomposition $A=UDV^\dagger$? (4 answers) Closed 2 months ago . In Adam Koranyi's article "" Around the finite dimensioal spectral theorem "", in Theorem 1 he says that there exist unique orthogonal decompositions. What is meant here by unique? We know that the Polar Decomposition and the SVD are equivalent, but the polar decomposition is not unique unless the operator is invertible, therefore the SVD is not unique. What is the difference between these uniquenesses?","This question already has answers here : How unique are $U$ and $V$ in the singular value decomposition $A=UDV^\dagger$? (4 answers) Closed 2 months ago . In Adam Koranyi's article "" Around the finite dimensioal spectral theorem "", in Theorem 1 he says that there exist unique orthogonal decompositions. What is meant here by unique? We know that the Polar Decomposition and the SVD are equivalent, but the polar decomposition is not unique unless the operator is invertible, therefore the SVD is not unique. What is the difference between these uniquenesses?",,"['linear-algebra', 'matrix-decomposition', 'svd', 'singular-values']"
31,natural isomorphism in linear algebra,natural isomorphism in linear algebra,,"Let $\mathsf{C}$ and $\mathsf{D}$ two categories and $\mathcal  F,\mathcal G$ two functors $\mathsf{C}\rightarrow\mathsf{D}$. A   natural isomorphism from $\mathcal F$ to $\mathcal G$ is the datum of   a isomorphism $\nu_X:\mathcal F(X)\rightarrow \mathcal G (X)$ for every   $X\in Obj(\mathsf{C})$ such that for every $\alpha\in \operatorname{Hom}(X,Y)$ in   $\mathsf{C}$ we have that $$\mathcal G(\alpha)\circ\nu_X=\nu_Y\circ\mathcal F(\alpha)$$ Now, many books say that a linear isomorphism $f$ between vector spaces is a natural isomorphism if ""$f$ doesn't depend from the choice of the basis"". I have two questions: 1) What does formally mean the phrase ""$f$ doesn't depend from the choice of the basis""? 2) How can I match the two definitions of natural isomorphism?","Let $\mathsf{C}$ and $\mathsf{D}$ two categories and $\mathcal  F,\mathcal G$ two functors $\mathsf{C}\rightarrow\mathsf{D}$. A   natural isomorphism from $\mathcal F$ to $\mathcal G$ is the datum of   a isomorphism $\nu_X:\mathcal F(X)\rightarrow \mathcal G (X)$ for every   $X\in Obj(\mathsf{C})$ such that for every $\alpha\in \operatorname{Hom}(X,Y)$ in   $\mathsf{C}$ we have that $$\mathcal G(\alpha)\circ\nu_X=\nu_Y\circ\mathcal F(\alpha)$$ Now, many books say that a linear isomorphism $f$ between vector spaces is a natural isomorphism if ""$f$ doesn't depend from the choice of the basis"". I have two questions: 1) What does formally mean the phrase ""$f$ doesn't depend from the choice of the basis""? 2) How can I match the two definitions of natural isomorphism?",,"['linear-algebra', 'category-theory']"
32,Matrices with real entries such that $(I -(AB-BA))^{n}=0$,Matrices with real entries such that,(I -(AB-BA))^{n}=0,"I was just trying out some problems, when i couldn't solve this question: Does there exist $n \times n$ matrices $A$ and $B$ with real entries such that $$ \Bigl(I - (AB-BA)\Bigr)^{n}=0?$$ I really don't know how to proceed for this question. What i did was to find some examples, but that didn't quite work. Any ideas on how one goes about thinking on such problems and how to solve them would be of great help.","I was just trying out some problems, when i couldn't solve this question: Does there exist $n \times n$ matrices $A$ and $B$ with real entries such that $$ \Bigl(I - (AB-BA)\Bigr)^{n}=0?$$ I really don't know how to proceed for this question. What i did was to find some examples, but that didn't quite work. Any ideas on how one goes about thinking on such problems and how to solve them would be of great help.",,[]
33,"$A$ is an invertible $n\times n$ matrix, where $n$ is an even number. Given that $A^3+A=0$, calculate $\det(A^4)$. Is there too much information?","is an invertible  matrix, where  is an even number. Given that , calculate . Is there too much information?",A n\times n n A^3+A=0 \det(A^4),"$A$ is an invertible matrix with $n$ columns and $n$ rows, where $n$ is an even number. We are given that $A^3+A=0$ and we need to calculate $\det(A^4)$ . Here is my solution: $$A^3+A=0 \implies A^{-1}(A^3+A)=0 \implies A^2=-I \implies A^4=I \implies \det(A^4)=1.$$ But I did not use the fact that n is even. Am I wrong, or this is not needed? If I'm wrong, please don't tell me the solution yet. Just tell me where I'm wrong. Thanks!","is an invertible matrix with columns and rows, where is an even number. We are given that and we need to calculate . Here is my solution: But I did not use the fact that n is even. Am I wrong, or this is not needed? If I'm wrong, please don't tell me the solution yet. Just tell me where I'm wrong. Thanks!",A n n n A^3+A=0 \det(A^4) A^3+A=0 \implies A^{-1}(A^3+A)=0 \implies A^2=-I \implies A^4=I \implies \det(A^4)=1.,['linear-algebra']
34,Khatri-Rao product example,Khatri-Rao product example,,"I am trying to understand the following definition of the Khatri-Rao product taken from Kolda, Tamara G., and Brett W. Bader. ""Tensor decompositions and applications.""(2009): ""The Khatri-Rao product is the ""matching columnwise"" Kronecker product. Given matrices $\mathrm{A} \in \mathbb{R}^{I \times K}$ and $\mathrm{B} \in \mathbb{R}^{J \times K}$, their Khatri-Rao product is denoted by $\mathrm{A} \odot \mathrm{B}$. The result is a matrix of size $(IJ) \times K$ and defined by $$\mathrm{A} \odot \mathrm{B} = [\mathrm{a}_1 \otimes \mathrm{b}_1 \mathrm{a}_2 \otimes \mathrm{b}_2 \ldots \mathrm{a}_k \otimes \mathrm{b}_k] $$."" I do understand the result of a matrix Kronecker product, but not of elementwise Kronecker products and so I'm having a hard time understanding the result of such a multiplication. So far I just can't find good online examples. Can someone give me a simple numerical example of what the result of such a multiplication should be?","I am trying to understand the following definition of the Khatri-Rao product taken from Kolda, Tamara G., and Brett W. Bader. ""Tensor decompositions and applications.""(2009): ""The Khatri-Rao product is the ""matching columnwise"" Kronecker product. Given matrices $\mathrm{A} \in \mathbb{R}^{I \times K}$ and $\mathrm{B} \in \mathbb{R}^{J \times K}$, their Khatri-Rao product is denoted by $\mathrm{A} \odot \mathrm{B}$. The result is a matrix of size $(IJ) \times K$ and defined by $$\mathrm{A} \odot \mathrm{B} = [\mathrm{a}_1 \otimes \mathrm{b}_1 \mathrm{a}_2 \otimes \mathrm{b}_2 \ldots \mathrm{a}_k \otimes \mathrm{b}_k] $$."" I do understand the result of a matrix Kronecker product, but not of elementwise Kronecker products and so I'm having a hard time understanding the result of such a multiplication. So far I just can't find good online examples. Can someone give me a simple numerical example of what the result of such a multiplication should be?",,"['linear-algebra', 'tensor-products', 'matrix-decomposition']"
35,Raising $e$ to the power of a matrix,Raising  to the power of a matrix,e,"Does there exist a definition for matrix exponentiation? If we have, say, an integer, one can define $A^B$ as follows: $$\prod_{n = 1}^B A$$ We can define exponentials of fractions as a power of a radical, and we even have the following definition of the exponential: $$e^z = \sum_{n = 0}^\infty \frac{z^n}{n!}$$ which comes from a Taylor Series for the function $\exp(z)$ . Now, a problem seems to arise when we attempt to calculate $\exp(A)$ , where $A$ is an $n$ x $n$ (square) matrix. We cannot define it as multiplication a ""matrix number of times"" as this makes no sense. The only reasonable definition that could work is the latter definition (the infinite series): $$e^A = 1 + A + \frac{AA}{2!} + \frac{AAA}{3!} + \frac{AAAA}{4!} + \cdots$$ where we can define matrix exponentiation to the power of an integer, which is all that is required here. We know that $e^x$ will converge absolutely for all complex numbers, but do we know that this is true for matrices? Can this ""matrix sum"" diverge, and are there ways to test divergence/convergence when a matrix is applied? Or is this concept of ""matrix divergence"" not well defined?","Does there exist a definition for matrix exponentiation? If we have, say, an integer, one can define as follows: We can define exponentials of fractions as a power of a radical, and we even have the following definition of the exponential: which comes from a Taylor Series for the function . Now, a problem seems to arise when we attempt to calculate , where is an x (square) matrix. We cannot define it as multiplication a ""matrix number of times"" as this makes no sense. The only reasonable definition that could work is the latter definition (the infinite series): where we can define matrix exponentiation to the power of an integer, which is all that is required here. We know that will converge absolutely for all complex numbers, but do we know that this is true for matrices? Can this ""matrix sum"" diverge, and are there ways to test divergence/convergence when a matrix is applied? Or is this concept of ""matrix divergence"" not well defined?",A^B \prod_{n = 1}^B A e^z = \sum_{n = 0}^\infty \frac{z^n}{n!} \exp(z) \exp(A) A n n e^A = 1 + A + \frac{AA}{2!} + \frac{AAA}{3!} + \frac{AAAA}{4!} + \cdots e^x,"['linear-algebra', 'matrices', 'summation']"
36,Understanding the Leibniz formula for determinants [closed],Understanding the Leibniz formula for determinants [closed],,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 4 months ago . The community reviewed whether to reopen this question 7 days ago and left it closed: Original close reason(s) were not resolved Improve this question I am a programmer and trying to write the Leibniz formula for determinants into C++ code, but I am unable to fully understand it. Can someone walk me through it? $$\det(A)=\sum_{\sigma \in S_n}\text{sgn}(\sigma)\prod_{i=1}^{n} a_{\sigma(i),i}$$","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 4 months ago . The community reviewed whether to reopen this question 7 days ago and left it closed: Original close reason(s) were not resolved Improve this question I am a programmer and trying to write the Leibniz formula for determinants into C++ code, but I am unable to fully understand it. Can someone walk me through it? $$\det(A)=\sum_{\sigma \in S_n}\text{sgn}(\sigma)\prod_{i=1}^{n} a_{\sigma(i),i}$$",,"['linear-algebra', 'matrices', 'determinant']"
37,Upper Bound on Determinant of Matrix in terms of Trace,Upper Bound on Determinant of Matrix in terms of Trace,,"For an $n\times n$ positive definite matrix $A$, I wish to prove that $$\det(A) \leq \bigg(\frac{Trace(A)}{n}\bigg)^n$$ To me this seems some form of AM-GM Inequality (Arithmatic Mean-Geometric Mean Inequality). Therefore If I can show the following, above inequality follows : $$\det(A) \leq \prod_{i=1}^{i=n} A_{ii}$$ Any idea how to prove the above. Thanks","For an $n\times n$ positive definite matrix $A$, I wish to prove that $$\det(A) \leq \bigg(\frac{Trace(A)}{n}\bigg)^n$$ To me this seems some form of AM-GM Inequality (Arithmatic Mean-Geometric Mean Inequality). Therefore If I can show the following, above inequality follows : $$\det(A) \leq \prod_{i=1}^{i=n} A_{ii}$$ Any idea how to prove the above. Thanks",,"['linear-algebra', 'matrices', 'determinant', 'trace']"
38,Adjoint of a linear transformation in an infinite dimension inner product space,Adjoint of a linear transformation in an infinite dimension inner product space,,"We learned that if $V$ is a finite inner product space then for every linear transformation $T:V\to V$, there exists a unique linear transformation $T^*:V\to V$ such that $\forall u, v \in V: (Tv, u)=(v, T^*u)$. The construction of $T^*$ used the fact that $V$ is finite and therefore has an orthonormal basis, which is not the case had it been infinite. Are there infinite dimension inner product spaces such that not all linear transformations have an adjoint? Or is it somehow possible to extend this definition to infinite spaces as well?","We learned that if $V$ is a finite inner product space then for every linear transformation $T:V\to V$, there exists a unique linear transformation $T^*:V\to V$ such that $\forall u, v \in V: (Tv, u)=(v, T^*u)$. The construction of $T^*$ used the fact that $V$ is finite and therefore has an orthonormal basis, which is not the case had it been infinite. Are there infinite dimension inner product spaces such that not all linear transformations have an adjoint? Or is it somehow possible to extend this definition to infinite spaces as well?",,['linear-algebra']
39,Sign of determinant when using $det A^\top A$,Sign of determinant when using,det A^\top A,"We have been given matrix: $$A =          \begin{pmatrix}         a& b& c &d \\         b &−a& d& −c\\         c& −d &−a& b \\         d &c& −b& −a\\         \end{pmatrix}     $$ ...and have been asked to calculate $\det(A)$ using $AA^T$. We see that: $$AA^T=\begin{pmatrix}         a^2+ b^2+ c^2+ d^2& 0& 0&0\\         0 &a^2+ b^2+ c^2+ d^2& 0& 0\\         0& 0 &a^2+ b^2+ c^2+ d^2& b \\        0 &0& 0& a^2+ b^2+ c^2+ d^2\\         \end{pmatrix}     $$ So, $\det(AA^T)= (a^2+ b^2+ c^2+ d^2)^4$ Now which should I choose: $(a^2+ b^2+ c^2+ d^2)^2 $ or $ -(a^2+ b^2+ c^2+ d^2)^2$ ? Please explain me which one and why.","We have been given matrix: $$A =          \begin{pmatrix}         a& b& c &d \\         b &−a& d& −c\\         c& −d &−a& b \\         d &c& −b& −a\\         \end{pmatrix}     $$ ...and have been asked to calculate $\det(A)$ using $AA^T$. We see that: $$AA^T=\begin{pmatrix}         a^2+ b^2+ c^2+ d^2& 0& 0&0\\         0 &a^2+ b^2+ c^2+ d^2& 0& 0\\         0& 0 &a^2+ b^2+ c^2+ d^2& b \\        0 &0& 0& a^2+ b^2+ c^2+ d^2\\         \end{pmatrix}     $$ So, $\det(AA^T)= (a^2+ b^2+ c^2+ d^2)^4$ Now which should I choose: $(a^2+ b^2+ c^2+ d^2)^2 $ or $ -(a^2+ b^2+ c^2+ d^2)^2$ ? Please explain me which one and why.",,"['linear-algebra', 'determinant']"
40,Why Is $\sqrt{\det(A^TA)}$ A Volume / Volume Factor?,Why Is  A Volume / Volume Factor?,\sqrt{\det(A^TA)},"The determinant of an $n\times n$ matrix is a volume / volume factor. So far, I'm good in my understanding. You take a linear map, encode it as a matrix, compute the volume of the parallelepiped (or whatever the proper name is) spanned by the column vectors, and look at the factor by which this transformation scaled the unit $n$-dimensional volume from before the transformation to the new one. That scaling is the determinant. There are many ways to view the determinant, but this is the most interesting to me, because I can visualize it. Now, what if I have a transformation from $\mathbb{R}^n$ to $\mathbb{R^m}$, encode it by an $m\times n$ matrix, and want the $n$-dimensional volume of the parallelepiped spanned by the column vectors of my matrix? This is a well-grounded question (think of a 2-d parallellogram embedded arbitrarily in 3-space: what is it's area?), but pretty much never addressed in linear algebra courses / books. Apparently (check e.g. the Wikipedia entry for determinants) I'm supposed to compute $\sqrt{\det(A^TA)}$ now. This makes sense in the $m=n$ scenario (except that orientation changes might be lost due to the square root?), since $|\det(A)|=\sqrt{\det(A)^2}=\sqrt{\det(A^T)\det(A)}=\sqrt{\det(A^TA)}$, but I just can't visualize it in the case $n\neq m$. I see that the end result of the product $A^TA$ is an $n\times n$ matrix, so clearly the determinant is then an n-dimensional volume / volume factor, but I can't see why I get the correct volume. Any help?","The determinant of an $n\times n$ matrix is a volume / volume factor. So far, I'm good in my understanding. You take a linear map, encode it as a matrix, compute the volume of the parallelepiped (or whatever the proper name is) spanned by the column vectors, and look at the factor by which this transformation scaled the unit $n$-dimensional volume from before the transformation to the new one. That scaling is the determinant. There are many ways to view the determinant, but this is the most interesting to me, because I can visualize it. Now, what if I have a transformation from $\mathbb{R}^n$ to $\mathbb{R^m}$, encode it by an $m\times n$ matrix, and want the $n$-dimensional volume of the parallelepiped spanned by the column vectors of my matrix? This is a well-grounded question (think of a 2-d parallellogram embedded arbitrarily in 3-space: what is it's area?), but pretty much never addressed in linear algebra courses / books. Apparently (check e.g. the Wikipedia entry for determinants) I'm supposed to compute $\sqrt{\det(A^TA)}$ now. This makes sense in the $m=n$ scenario (except that orientation changes might be lost due to the square root?), since $|\det(A)|=\sqrt{\det(A)^2}=\sqrt{\det(A^T)\det(A)}=\sqrt{\det(A^TA)}$, but I just can't visualize it in the case $n\neq m$. I see that the end result of the product $A^TA$ is an $n\times n$ matrix, so clearly the determinant is then an n-dimensional volume / volume factor, but I can't see why I get the correct volume. Any help?",,"['linear-algebra', 'geometry', 'determinant', 'linear-transformations', 'geometric-algebras']"
41,"Why an eigenspace is a linear subspace, if the zero vector is not an eigenvector?","Why an eigenspace is a linear subspace, if the zero vector is not an eigenvector?",,"I've started studying Eigenvector and Eigenvalue. It says in my book that 0 is excluded from being an eigenvector because it breaks the uniqueness of eigenvalue associated with each eigenvector. But, there is a proof in my book showing that Eigenspace is a subspace. In order for it to be subspace, does that mean that there must be a zero element? But, Eigenvector can not be zero... Am I misunderstanding something?","I've started studying Eigenvector and Eigenvalue. It says in my book that 0 is excluded from being an eigenvector because it breaks the uniqueness of eigenvalue associated with each eigenvector. But, there is a proof in my book showing that Eigenspace is a subspace. In order for it to be subspace, does that mean that there must be a zero element? But, Eigenvector can not be zero... Am I misunderstanding something?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
42,Eigenvalue test faster than $O\left(n^3\right)$?,Eigenvalue test faster than ?,O\left(n^3\right),"Given a real $n\times n$ matrix $A$, one can find the eigenvalues in $O\left(n^3\right)$ by using say, the $QR$ algorithm. Now, what if we guess an eigenvalue $\lambda_0$, and we want to know if it's actually an eigenvalue of the matrix $A$? Intuitively, this should be significantly faster than actually finding all of the eigenvalues. We can of course check if $\lambda_0$ is actually an eigenvalue by calculating $\det(A-\lambda_0 I)$, but calculating the determinant is also $O\left(n^3\right)$. So: Is there a faster than $O\left(n^3\right)$ method for testing the ""eigenvaluedness"" of a specific number $\lambda_0$, without solving for the rest of the $\lambda_i$'s? Can one prove there isn't? Does it help if $A$ is orthogonal, symmetric or has other special (non triangular) form? Bounty update: Bounty goes to whoever shows either of the following: A method of checking a possible eigenvalue in less than $O(n^3)$. A proof or sufficiently convincing heuristic argument that no such method exists.","Given a real $n\times n$ matrix $A$, one can find the eigenvalues in $O\left(n^3\right)$ by using say, the $QR$ algorithm. Now, what if we guess an eigenvalue $\lambda_0$, and we want to know if it's actually an eigenvalue of the matrix $A$? Intuitively, this should be significantly faster than actually finding all of the eigenvalues. We can of course check if $\lambda_0$ is actually an eigenvalue by calculating $\det(A-\lambda_0 I)$, but calculating the determinant is also $O\left(n^3\right)$. So: Is there a faster than $O\left(n^3\right)$ method for testing the ""eigenvaluedness"" of a specific number $\lambda_0$, without solving for the rest of the $\lambda_i$'s? Can one prove there isn't? Does it help if $A$ is orthogonal, symmetric or has other special (non triangular) form? Bounty update: Bounty goes to whoever shows either of the following: A method of checking a possible eigenvalue in less than $O(n^3)$. A proof or sufficiently convincing heuristic argument that no such method exists.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'computational-complexity']"
43,Isomorphisms of inner-product spaces,Isomorphisms of inner-product spaces,,"I think I understand why all finite-dimensional vector spaces over a field $\mathbb{K}$ are isomorphic to $\mathbb{K}^n$. Any linear map $T: V \rightarrow W$ between finite-dimensional vector spaces taking a basis to a basis is automatically an isomorphism, by linearity. (c.f. this nice post .) But I'm puzzled about the following. On the one hand, there's $\mathbb{R}^n$ with standard basis $\{e_i\}_{i=1}^n$ and the natural Euclidean inner-product $$\langle \bar{x},\bar{y}\rangle = \sum_{i=1}^n x_i y_i$$ On the other hand, there's $P_n([-1,1])$, the space of real polynomials on $[-1,1]$ of degree less than $n$, with the obvious basis $\{1,x,x^2,...,x^{n-1}\}$ and the $L^2$ inner-product $$\langle p,q\rangle = \int_{-1}^{-1} p(x)q(x)dx$$ They're both Hilbert spaces. The basis given for the former is orthonormal; the latter is not (but we can apply Gram-Schmidt to build the Legendre polynomials, which are.) This seems somehow strange to me: the $L^2$ inner-product looks like the most straightforward generalization of the Euclidean inner-product to function spaces, and the basis of monomials seems like the most natural basis of $P_n$ corresponding to the standard basis on $\mathbb{R}^n$. The Legendre polynomials, by contrast, appear bizarre and complicated. The vector spaces are obviously isomorphic: given any basis of each, we can easily construct an isomorphism $T$ mapping each basis to the other. But in the above example, orthonormality isn't preserved. If I want to keep orthonormality, it seems I have to choose: if I want the $L^2$ inner-product on $P_n([-1,1])$, I have to map $\{e_i\}_{i=1}^n$ to the Legendre polynomials. If I want the monomial basis $\{1,x,x^2,...,x^{n-1}\}$, I have to pick a different inner-product. I can't have my cake and eat it, too. (And I don't even know if an inner-product on $P_n$ exists for which the basis of monomials is orthonormal.) This leads me to several questions. For isomorphic, finite-dimensional vector spaces V and W, just how many isomorphisms are there? How many distinct inner-products can there be? Is there some sort of 'natural' correspondence here between isomorphisms and pairs of inner-products? (This was just confusion on my part.) Suppose I specify an inner-product and an orthonormal basis for $V$, and I map that to a basis for $W$. Is there an inner-product on $W$ such that this latter basis is orthonormal in $W$? More generally, is there an inner-product on $W$ that acts the same on $W$ as the inner-product on $V$ acts on $V$? I have a feeling that I'm confused about some pretty fundamental things here.","I think I understand why all finite-dimensional vector spaces over a field $\mathbb{K}$ are isomorphic to $\mathbb{K}^n$. Any linear map $T: V \rightarrow W$ between finite-dimensional vector spaces taking a basis to a basis is automatically an isomorphism, by linearity. (c.f. this nice post .) But I'm puzzled about the following. On the one hand, there's $\mathbb{R}^n$ with standard basis $\{e_i\}_{i=1}^n$ and the natural Euclidean inner-product $$\langle \bar{x},\bar{y}\rangle = \sum_{i=1}^n x_i y_i$$ On the other hand, there's $P_n([-1,1])$, the space of real polynomials on $[-1,1]$ of degree less than $n$, with the obvious basis $\{1,x,x^2,...,x^{n-1}\}$ and the $L^2$ inner-product $$\langle p,q\rangle = \int_{-1}^{-1} p(x)q(x)dx$$ They're both Hilbert spaces. The basis given for the former is orthonormal; the latter is not (but we can apply Gram-Schmidt to build the Legendre polynomials, which are.) This seems somehow strange to me: the $L^2$ inner-product looks like the most straightforward generalization of the Euclidean inner-product to function spaces, and the basis of monomials seems like the most natural basis of $P_n$ corresponding to the standard basis on $\mathbb{R}^n$. The Legendre polynomials, by contrast, appear bizarre and complicated. The vector spaces are obviously isomorphic: given any basis of each, we can easily construct an isomorphism $T$ mapping each basis to the other. But in the above example, orthonormality isn't preserved. If I want to keep orthonormality, it seems I have to choose: if I want the $L^2$ inner-product on $P_n([-1,1])$, I have to map $\{e_i\}_{i=1}^n$ to the Legendre polynomials. If I want the monomial basis $\{1,x,x^2,...,x^{n-1}\}$, I have to pick a different inner-product. I can't have my cake and eat it, too. (And I don't even know if an inner-product on $P_n$ exists for which the basis of monomials is orthonormal.) This leads me to several questions. For isomorphic, finite-dimensional vector spaces V and W, just how many isomorphisms are there? How many distinct inner-products can there be? Is there some sort of 'natural' correspondence here between isomorphisms and pairs of inner-products? (This was just confusion on my part.) Suppose I specify an inner-product and an orthonormal basis for $V$, and I map that to a basis for $W$. Is there an inner-product on $W$ such that this latter basis is orthonormal in $W$? More generally, is there an inner-product on $W$ that acts the same on $W$ as the inner-product on $V$ acts on $V$? I have a feeling that I'm confused about some pretty fundamental things here.",,"['linear-algebra', 'hilbert-spaces', 'inner-products', 'orthogonal-polynomials', 'orthonormal']"
44,"Jordan Canonical Form determined by characteristic and minimal polynomials in dimension $3$, but not beyond","Jordan Canonical Form determined by characteristic and minimal polynomials in dimension , but not beyond",3,Why and how is the Jordan Canonical form of a matrix in $M_3(\mathbb C)$ fully determined by its characteristic and minimal polynomials? And why does it fail for $n >3$? Thanks.,Why and how is the Jordan Canonical form of a matrix in $M_3(\mathbb C)$ fully determined by its characteristic and minimal polynomials? And why does it fail for $n >3$? Thanks.,,"['linear-algebra', 'matrices']"
45,How to understand the exponential operator geometrically?,How to understand the exponential operator geometrically?,,"Consider the geometric interpretation of an orthogonal matrix, a projection matrix, a (Householder) reflector, or even just matrix-vector multiply in general. A matrix takes a vector from a vector space (after a basis has been fixed) and performs a scaling, rotation, reflection, shear, projection, or a combination of these. This can include affine transformations as well. An orthogonal matrix represents a rotation. A projection matrix represents the projection of a vector onto a subspace. A Householder reflector reflects a vector onto an axis where a coordinate can become zero. A Givens matrix is a rotation with the same effect. My question is, what is a geometric interpretation of the exponential operator ? For example, if $P$ is a projection matrix, then it represents the shadow of a vector onto a subspace. The property $P^2=P$ becomes obvious. But what is $e^P$? What does this represent geometrically? Similarly, thinking of an orthogonal matrix as a rotation, some of its properties become very obvious. It is always invertible because rotations can always be undone. It always preserves the Euclidean norm of a vector because a rotation cannot change the length of a vector, and so on. But what does the exponential of an orthogonal matrix represent? Addendum After looking at some comments and counter-comments posted below, let me clarify. After the initial setup (fields, building a vector space on top of it, selecting a basis, etc.) a matrix represents a linear transformation. If you imagine all possible linear transformations in $\mathbb{R}^3$ or $\mathbb{R}^2$, it turns out that they represent only a handful of geometric transformations such as dilation, reflection, rotation, shear, and so on. All linear transformations are some combination of these. In fact, the singular value decomposition's existence for any matrix tells us that every single linear transformation can be thought of as a rotation, dilation, followed by a rotation. For some matrices, the geometric transformation is easy to see and easy to explain. For example, an orthogonal matrix represents a rotation. Thinking about orthogonal matrices like this, it is ""obvious"" that it must always be invertible, it must always preserve the Euclidean norm, the determinant must be $\pm1$ because rotations will preserve volume, etc. A projection matrix collapses a vector onto a subspace and it becomes apparent that $P^2$ must always be equal to $P$ for any projection matrix $P$. My question is that for a given matrix $A$, what does $e^A$ represent? Does it represent something? Is there a general statement that can be made, like ""the exponential always maps a matrix to an orthogonal matrix"" or ""the exponential of a matrix is always a projection followed by a dilation"" or something? If not, then when can we say something and what can we say? For example, the exponential of any matrix is always invertible, but why? This is easy to see algebraically by what happens geometrically? Another example, the exponential of a dilation is another dilation where the new dilation factor is the exponential of the old dilation factor. Well, what if I take the exponential of a rotation? It must be a dilation/rotation/shear/reflection or some combination, but which is it in general? What about the exponential of a projection? Why and how does the exponential turn a projection into an invertible transformation even when the projection itself is not invertible? And why does the determinant of the exponential equal the exponential of the trace?","Consider the geometric interpretation of an orthogonal matrix, a projection matrix, a (Householder) reflector, or even just matrix-vector multiply in general. A matrix takes a vector from a vector space (after a basis has been fixed) and performs a scaling, rotation, reflection, shear, projection, or a combination of these. This can include affine transformations as well. An orthogonal matrix represents a rotation. A projection matrix represents the projection of a vector onto a subspace. A Householder reflector reflects a vector onto an axis where a coordinate can become zero. A Givens matrix is a rotation with the same effect. My question is, what is a geometric interpretation of the exponential operator ? For example, if $P$ is a projection matrix, then it represents the shadow of a vector onto a subspace. The property $P^2=P$ becomes obvious. But what is $e^P$? What does this represent geometrically? Similarly, thinking of an orthogonal matrix as a rotation, some of its properties become very obvious. It is always invertible because rotations can always be undone. It always preserves the Euclidean norm of a vector because a rotation cannot change the length of a vector, and so on. But what does the exponential of an orthogonal matrix represent? Addendum After looking at some comments and counter-comments posted below, let me clarify. After the initial setup (fields, building a vector space on top of it, selecting a basis, etc.) a matrix represents a linear transformation. If you imagine all possible linear transformations in $\mathbb{R}^3$ or $\mathbb{R}^2$, it turns out that they represent only a handful of geometric transformations such as dilation, reflection, rotation, shear, and so on. All linear transformations are some combination of these. In fact, the singular value decomposition's existence for any matrix tells us that every single linear transformation can be thought of as a rotation, dilation, followed by a rotation. For some matrices, the geometric transformation is easy to see and easy to explain. For example, an orthogonal matrix represents a rotation. Thinking about orthogonal matrices like this, it is ""obvious"" that it must always be invertible, it must always preserve the Euclidean norm, the determinant must be $\pm1$ because rotations will preserve volume, etc. A projection matrix collapses a vector onto a subspace and it becomes apparent that $P^2$ must always be equal to $P$ for any projection matrix $P$. My question is that for a given matrix $A$, what does $e^A$ represent? Does it represent something? Is there a general statement that can be made, like ""the exponential always maps a matrix to an orthogonal matrix"" or ""the exponential of a matrix is always a projection followed by a dilation"" or something? If not, then when can we say something and what can we say? For example, the exponential of any matrix is always invertible, but why? This is easy to see algebraically by what happens geometrically? Another example, the exponential of a dilation is another dilation where the new dilation factor is the exponential of the old dilation factor. Well, what if I take the exponential of a rotation? It must be a dilation/rotation/shear/reflection or some combination, but which is it in general? What about the exponential of a projection? Why and how does the exponential turn a projection into an invertible transformation even when the projection itself is not invertible? And why does the determinant of the exponential equal the exponential of the trace?",,"['linear-algebra', 'geometry', 'linear-transformations', 'matrix-exponential', 'geometric-interpretation']"
46,"About Hahn Banach, can I get an example problem outside of functional analysis?","About Hahn Banach, can I get an example problem outside of functional analysis?",,"I just started reading functional analysis and came across the Hahn-Banach theorem . Everywhere I looked stated this was very important and it is because it allows us to enlarge a dual space $X^+.$ I keep wondering to myself, so what ? (sorry for sounding rude). If we can enlarge the dual space of $X^+$ , then what? Why are the extensions of linear functionals so important? I mean for a subspace $Z \subset X$ , is there something important we need to know about $X-Z$ that we need to extend a linear functional $f: Z \to \mathbb{F}$ on? Is there an application or simple example of this outside of functional analysis that motivates the idea? Even something simple in Differential Equations would be insightful or a simple problem in Linear Algebra (actually this is probably dumb since I would imagine you would just extend the basis, so we have to give an example for a infinite dim space; maybe showing an analogue of Hahn-Banach in this setting would be neat and somewhat convincing ) Right now I just can't be bothered reading the full proof of something that I don't see the importance in and only to forget about it later... Thank you.","I just started reading functional analysis and came across the Hahn-Banach theorem . Everywhere I looked stated this was very important and it is because it allows us to enlarge a dual space I keep wondering to myself, so what ? (sorry for sounding rude). If we can enlarge the dual space of , then what? Why are the extensions of linear functionals so important? I mean for a subspace , is there something important we need to know about that we need to extend a linear functional on? Is there an application or simple example of this outside of functional analysis that motivates the idea? Even something simple in Differential Equations would be insightful or a simple problem in Linear Algebra (actually this is probably dumb since I would imagine you would just extend the basis, so we have to give an example for a infinite dim space; maybe showing an analogue of Hahn-Banach in this setting would be neat and somewhat convincing ) Right now I just can't be bothered reading the full proof of something that I don't see the importance in and only to forget about it later... Thank you.",X^+. X^+ Z \subset X X-Z f: Z \to \mathbb{F},"['linear-algebra', 'functional-analysis', 'intuition']"
47,If $\mathrm{Tr}(A)=0$ then $T=R^{-1}AR$ has all entries on its main diagonal equal to $0$,If  then  has all entries on its main diagonal equal to,\mathrm{Tr}(A)=0 T=R^{-1}AR 0,"Prove that if $A$ is a square matrix and $\mathrm{Tr}(A)=0$ , then there exists an invertible matrix $R$ such that the matrix $T=R^{-1}AR$ has all entries on its main diagonal equal to $0$ . It seems like the formula $A=S\Lambda S^{-1}$ , but maybe it does not help. Thanks so much.","Prove that if is a square matrix and , then there exists an invertible matrix such that the matrix has all entries on its main diagonal equal to . It seems like the formula , but maybe it does not help. Thanks so much.",A \mathrm{Tr}(A)=0 R T=R^{-1}AR 0 A=S\Lambda S^{-1},"['linear-algebra', 'matrices', 'trace', 'matrix-decomposition']"
48,What is Cramer's rule used for?,What is Cramer's rule used for?,,"Cramer's rule appears in introductory linear algebra courses without comments on its utility.  It is a flaw in our system of pedagogy that one learns answers to questions of this kind in courses only if one takes a course on something in which the topic is used. On the discussion page to Wikipedia's article on Cramer's rule, we find this detailed indictment on charges of uselessness , posted in December 2009. But in the present day, we find in the article itself the assertion that it is useful for solving problems in differential geometry; proving a theorem in integer programming; deriving the general solution to an inhomogeneous linear differential equation by the method of variation of parameters; (a surprise) solving small systems of linear equations.  This one is what it superficially purports to be in linear algebra texts, but then elementary row operations turn out to be what is actually used. At some point in its history, the Wikipedia article asserted that it's used in proving the Cayley–Hamilton theorem , but that's not there now.  To me the Cayley–Hamilton theorem has always been a very memorable statement, but at this moment I can't recall anything about the proof. What enlightening expansions on these partial answers to this question can the present company offer?","Cramer's rule appears in introductory linear algebra courses without comments on its utility.  It is a flaw in our system of pedagogy that one learns answers to questions of this kind in courses only if one takes a course on something in which the topic is used. On the discussion page to Wikipedia's article on Cramer's rule, we find this detailed indictment on charges of uselessness , posted in December 2009. But in the present day, we find in the article itself the assertion that it is useful for solving problems in differential geometry; proving a theorem in integer programming; deriving the general solution to an inhomogeneous linear differential equation by the method of variation of parameters; (a surprise) solving small systems of linear equations.  This one is what it superficially purports to be in linear algebra texts, but then elementary row operations turn out to be what is actually used. At some point in its history, the Wikipedia article asserted that it's used in proving the Cayley–Hamilton theorem , but that's not there now.  To me the Cayley–Hamilton theorem has always been a very memorable statement, but at this moment I can't recall anything about the proof. What enlightening expansions on these partial answers to this question can the present company offer?",,"['linear-algebra', 'determinant', 'applications']"
49,Inverse of the $n$-by-$n$ matrix $(a_{jk})$ where $a_{jk} = \binom{j-1}{k-1}$,Inverse of the -by- matrix  where,n n (a_{jk}) a_{jk} = \binom{j-1}{k-1},"I have an interesting problem which can be solved by Induction and Gaussian Elimination, but due to the nice structure of the matrix, I think there can be many more approaches. Here's the problem : Let $$ A = (a_{jk})_{n \times n} $$ where $\displaystyle a_{jk} = \dbinom{j-1}{k-1}$ (by convention, this is $0$ for $k>j$ ). Prove that $$ A^{-1} = ((-1)^{j+k} a_{jk})_{n \times n} $$ The small alphabets along with subscripts denote the element of the $j^{\text{th}}$ row and $k^{\text{th}}$ column of the matrix. I think this problem might have deep links to theorems of linear algebra due to the nice structure of inverse. I'm specifically seeking an answer that makes such a connection. Thanks in advance.","I have an interesting problem which can be solved by Induction and Gaussian Elimination, but due to the nice structure of the matrix, I think there can be many more approaches. Here's the problem : Let where (by convention, this is for ). Prove that The small alphabets along with subscripts denote the element of the row and column of the matrix. I think this problem might have deep links to theorems of linear algebra due to the nice structure of inverse. I'm specifically seeking an answer that makes such a connection. Thanks in advance.", A = (a_{jk})_{n \times n}  \displaystyle a_{jk} = \dbinom{j-1}{k-1} 0 k>j  A^{-1} = ((-1)^{j+k} a_{jk})_{n \times n}  j^{\text{th}} k^{\text{th}},"['linear-algebra', 'matrices', 'binomial-coefficients']"
50,What kind of transformation an upper triangular matrix represents,What kind of transformation an upper triangular matrix represents,,"Every matrix represents a linear transformation, but depending on characteristics of the matrix, the linear transformation it represents can be limited to a specific type. For example, an orthogonal matrix represents a rotation (and possibly a reflection ). Is it something similar about triangular matrices ? Do they represent any specific type of transformation? (Actually a reference describing different geometric transformations and their corresponding transformation matrices would be great)","Every matrix represents a linear transformation, but depending on characteristics of the matrix, the linear transformation it represents can be limited to a specific type. For example, an orthogonal matrix represents a rotation (and possibly a reflection ). Is it something similar about triangular matrices ? Do they represent any specific type of transformation? (Actually a reference describing different geometric transformations and their corresponding transformation matrices would be great)",,"['linear-algebra', 'transformation', 'rotations', 'linear-transformations']"
51,"Prove that $v_0, v_1,...,v_k$ are affinely independent if and only if $v_1 - v_0,...,v_k - v_0$ are linearly independent",Prove that  are affinely independent if and only if  are linearly independent,"v_0, v_1,...,v_k v_1 - v_0,...,v_k - v_0","Definition: Let $v_0, v_1.. v_k$ be points in $\mathbb{R}^d$.  These points are called affinely independent if there do not exist real numbers $\alpha_0, \alpha_1...\alpha_k$ that are not all zero such that $\sum_{i=0}^k \alpha_i v_i = 0$ and $\sum_{i=0}^k \alpha_i = 0$. We need to prove the following: The points $v_0, v_1... v_k$ are affinely independent if and only if the vectors $v_1 - v_0, v_2 -v_0... v_k - v_0$ are linearly independent. Thank you so much.","Definition: Let $v_0, v_1.. v_k$ be points in $\mathbb{R}^d$.  These points are called affinely independent if there do not exist real numbers $\alpha_0, \alpha_1...\alpha_k$ that are not all zero such that $\sum_{i=0}^k \alpha_i v_i = 0$ and $\sum_{i=0}^k \alpha_i = 0$. We need to prove the following: The points $v_0, v_1... v_k$ are affinely independent if and only if the vectors $v_1 - v_0, v_2 -v_0... v_k - v_0$ are linearly independent. Thank you so much.",,"['linear-algebra', 'affine-geometry']"
52,Proof that $\dim(U \times V) = \dim U + \dim V$.,Proof that .,\dim(U \times V) = \dim U + \dim V,"The following theorem in Serge Lang's Linear Algebra is left as an exercise, namely, Let $U$ and $V$ be finite dimensional vector spaces over a field $K$, where $\dim U = n$ and $\dim V = m$. Then  $\dim W = \dim U + \dim V$, where $W = U \times V$, the direct product of the two vector spaces $U$ and $V$. Namely, $W$ contains the set of all ordered pairs $(u,w)$ such that $u \in U$ and $v \in V$. The usual axioms for such a direct product are: 1) Addition is defined component wise, namely if $(u_1,v_1),(u_2,v_2) \in W$, then $(u_1,v_1)+(u_2,v_2) = (u_1 + u_2, v_1 + v_2)$; 2)If $c \in K$, then $c(u_1,w_1) = (cu_1,cw_1)$. To prove it, let $(u_1, u_2 \ldots u_n)$ be a basis for $U$ and $(v_1,v_2 , \ldots v_m)$ a basis for $V$. So by definition, every element of $W$ can be written in the form $(a_1u_1 + \ldots a_nu_n, b_1v_1 + \ldots b_mv_m)$, where the $a_i's$ and $b_j's$ belong to the field $K$. Using the above axioms this can be rewritten as: $a_1(u_1,0) + a_2(u_2,0) + \ldots a_n(u_n,0) + b_1(0,v_1) + \ldots b_m(0,v_m)$. Doubt: If we view all the $(u_i,0)$ ordered pairs as being the ""basis"" vectors of $U$ and similarly for the $(0,v_j)$ ordered pairs of $V$, then there are $n+m$ number of them and so proving the linear independence of these objects should suffice. But I'm confused because I know that $u_i's$ by themselves are the basis vectors of $U$, but now we are talking about ordered pairs $(u_i,0)$. How can I get out of such a situation? Perhaps one can define some linear map between say a $u_i$ and the ordered pair $(u_i,0)$. $\textbf{Edit}:$ First it is easy to see that $(U \times \{0\}) \cap (\{0\} \times V)$ is the ordered pair $(0,0)$. The linear independence of the basis vectors as stated above then follows.","The following theorem in Serge Lang's Linear Algebra is left as an exercise, namely, Let $U$ and $V$ be finite dimensional vector spaces over a field $K$, where $\dim U = n$ and $\dim V = m$. Then  $\dim W = \dim U + \dim V$, where $W = U \times V$, the direct product of the two vector spaces $U$ and $V$. Namely, $W$ contains the set of all ordered pairs $(u,w)$ such that $u \in U$ and $v \in V$. The usual axioms for such a direct product are: 1) Addition is defined component wise, namely if $(u_1,v_1),(u_2,v_2) \in W$, then $(u_1,v_1)+(u_2,v_2) = (u_1 + u_2, v_1 + v_2)$; 2)If $c \in K$, then $c(u_1,w_1) = (cu_1,cw_1)$. To prove it, let $(u_1, u_2 \ldots u_n)$ be a basis for $U$ and $(v_1,v_2 , \ldots v_m)$ a basis for $V$. So by definition, every element of $W$ can be written in the form $(a_1u_1 + \ldots a_nu_n, b_1v_1 + \ldots b_mv_m)$, where the $a_i's$ and $b_j's$ belong to the field $K$. Using the above axioms this can be rewritten as: $a_1(u_1,0) + a_2(u_2,0) + \ldots a_n(u_n,0) + b_1(0,v_1) + \ldots b_m(0,v_m)$. Doubt: If we view all the $(u_i,0)$ ordered pairs as being the ""basis"" vectors of $U$ and similarly for the $(0,v_j)$ ordered pairs of $V$, then there are $n+m$ number of them and so proving the linear independence of these objects should suffice. But I'm confused because I know that $u_i's$ by themselves are the basis vectors of $U$, but now we are talking about ordered pairs $(u_i,0)$. How can I get out of such a situation? Perhaps one can define some linear map between say a $u_i$ and the ordered pair $(u_i,0)$. $\textbf{Edit}:$ First it is easy to see that $(U \times \{0\}) \cap (\{0\} \times V)$ is the ordered pair $(0,0)$. The linear independence of the basis vectors as stated above then follows.",,[]
53,Maximising determinant problem,Maximising determinant problem,,The problem is to maximize the determinant of a $3 \times 3$ matrix with elements from $1$ to $9$. Is there a method to do this without resorting to brute force?,The problem is to maximize the determinant of a $3 \times 3$ matrix with elements from $1$ to $9$. Is there a method to do this without resorting to brute force?,,"['linear-algebra', 'matrices', 'optimization', 'discrete-optimization']"
54,Uniqueness of determinant,Uniqueness of determinant,,"In Artin Algebra 2nd edition page 22, the author proved the uniqueness of determinant by saying that any matrix $A$ can be written in reduced row-echelon form $A'$: $A'=E_1\cdots E_kA$ where $E_i$ are the elementary matrix. Then $A'$ is either $I$ or has a zero row. If $A'=I$, then $\delta(A')=1$. Otherwise, $\delta(A')=0$. In both cases, $\delta(A')$ is determined, and hence by $$\delta(A')=\delta(E_1)\cdots\delta(E_k)\delta(A)$$ $\delta(A)$ is determined uniquely. However, as he himself pointed out immediately in the following paragraph, the sequence $E_1\cdots E_k$ is not unique. Then why is $\delta(A)$ uniquely determined? Edit: The author defined determinant as a function $\delta(A)=d\in \mathbb{R}$ satisfying the following 3 conditions: (i) $\delta(I)=1$ (ii) $\delta$ is linear in the rows of the matrix $A$ (iii) If two adjacent rows of $A$ are equal, then $\delta(A)=0$ He then proved that the above conditions imply some properties that all of us know, e.g., (a) Interchanging two rows reverses the sign (b) If $A$ has a zero row, then $\delta(A)=0$ (c) Multiplying one row by a number and adding it to another row doesn't change the determinant (d) $\delta(E)=\pm1$ or $c$ (e) $\delta(AB)=\delta(A)\delta(B)$ Then he proved that the function $\delta$ so defined is unique, as shown in the beginning of my post, which I don't understand","In Artin Algebra 2nd edition page 22, the author proved the uniqueness of determinant by saying that any matrix $A$ can be written in reduced row-echelon form $A'$: $A'=E_1\cdots E_kA$ where $E_i$ are the elementary matrix. Then $A'$ is either $I$ or has a zero row. If $A'=I$, then $\delta(A')=1$. Otherwise, $\delta(A')=0$. In both cases, $\delta(A')$ is determined, and hence by $$\delta(A')=\delta(E_1)\cdots\delta(E_k)\delta(A)$$ $\delta(A)$ is determined uniquely. However, as he himself pointed out immediately in the following paragraph, the sequence $E_1\cdots E_k$ is not unique. Then why is $\delta(A)$ uniquely determined? Edit: The author defined determinant as a function $\delta(A)=d\in \mathbb{R}$ satisfying the following 3 conditions: (i) $\delta(I)=1$ (ii) $\delta$ is linear in the rows of the matrix $A$ (iii) If two adjacent rows of $A$ are equal, then $\delta(A)=0$ He then proved that the above conditions imply some properties that all of us know, e.g., (a) Interchanging two rows reverses the sign (b) If $A$ has a zero row, then $\delta(A)=0$ (c) Multiplying one row by a number and adding it to another row doesn't change the determinant (d) $\delta(E)=\pm1$ or $c$ (e) $\delta(AB)=\delta(A)\delta(B)$ Then he proved that the function $\delta$ so defined is unique, as shown in the beginning of my post, which I don't understand",,"['linear-algebra', 'determinant']"
55,"If $A^3=A+I$, then $\det A>0$","If , then",A^3=A+I \det A>0,"If $A$ is a $n\times n$ matrix such that $A^3=A+I$, then $\det A>0$. I don't know how to solve this problem. It's easy to see that $\det A\neq 0$. Suppose $\det A<0$. These are ways that I tried: $\bullet$ $\det A^3=(\det A)^3<0$, thus $\det(A+I)<0$; $A^2+A+I=A^3+A^2=A^2(A+I)$, thus $\det (A^2+A+I)<0$. $\bullet$ Let $\lambda_1,\lambda_2,\ldots,\lambda_n$ are all eigenvalues of $A$. Then $\lambda_i^3-\lambda_i$ is eigenvalue of $A^3-A$ for all $i=\overline{1,n}$. But $A^3-A=I$, then $\lambda_i^3-\lambda_i=1$ for all $i$. $\bullet$ $\lambda_i^2+\lambda_i+1$ are eigenvalues of $A^2+A+I$. Since $\det (A^2+A+I)<0$, $\displaystyle \prod_{i=1}^n (\lambda_i^2+\lambda_i+1)<0$. The problem is I don't know how to deal with complex eigenvalues, so I don't know how to connect all the above things. Thanks in advance.","If $A$ is a $n\times n$ matrix such that $A^3=A+I$, then $\det A>0$. I don't know how to solve this problem. It's easy to see that $\det A\neq 0$. Suppose $\det A<0$. These are ways that I tried: $\bullet$ $\det A^3=(\det A)^3<0$, thus $\det(A+I)<0$; $A^2+A+I=A^3+A^2=A^2(A+I)$, thus $\det (A^2+A+I)<0$. $\bullet$ Let $\lambda_1,\lambda_2,\ldots,\lambda_n$ are all eigenvalues of $A$. Then $\lambda_i^3-\lambda_i$ is eigenvalue of $A^3-A$ for all $i=\overline{1,n}$. But $A^3-A=I$, then $\lambda_i^3-\lambda_i=1$ for all $i$. $\bullet$ $\lambda_i^2+\lambda_i+1$ are eigenvalues of $A^2+A+I$. Since $\det (A^2+A+I)<0$, $\displaystyle \prod_{i=1}^n (\lambda_i^2+\lambda_i+1)<0$. The problem is I don't know how to deal with complex eigenvalues, so I don't know how to connect all the above things. Thanks in advance.",,['linear-algebra']
56,Every endomorphims is a linear combination of how many idempotents in infinite dimensions?,Every endomorphims is a linear combination of how many idempotents in infinite dimensions?,,"Every endomorphism of a finite-dimensional vector space is a linear combination of at most three idempotents, and the constant three is best possible, as Clément de Seguins has shown in this paper . On the other hand, Georges Lowther has shown in a recent MSE answer that every endomorphism of an infinite-dimensional vector space is a linear combination of at most nine idempotents. (a) Can the constant 9 be improved? (b) Multiplication by the indeterminate $X$ defines an endomorphism of $k[X]$ (where $k$ is a field). How many idempotents are needed to write this endomorphism as a linear combination of idempotents? [Edit (Aug 8,14): The first comment below, of George Lowther answers Question (b).]","Every endomorphism of a finite-dimensional vector space is a linear combination of at most three idempotents, and the constant three is best possible, as Clément de Seguins has shown in this paper . On the other hand, Georges Lowther has shown in a recent MSE answer that every endomorphism of an infinite-dimensional vector space is a linear combination of at most nine idempotents. (a) Can the constant 9 be improved? (b) Multiplication by the indeterminate $X$ defines an endomorphism of $k[X]$ (where $k$ is a field). How many idempotents are needed to write this endomorphism as a linear combination of idempotents? [Edit (Aug 8,14): The first comment below, of George Lowther answers Question (b).]",,['linear-algebra']
57,"Strength of the statement ""$\mathbb R$ has a Hamel basis over $\mathbb Q$""","Strength of the statement "" has a Hamel basis over """,\mathbb R \mathbb Q,"I would like to know if there are ""interesting"" equivalences to the statement ""$\mathbb R$ has a Hamel basis over $\mathbb Q$"". I am not interested in more general statements, like ""every vector space has a Hamel basis"" or ""every vector space over (your favorite field) has a Hamel basis"". On the other hand, according to this post , such Hamel basis of $\mathbb R$ over $\mathbb Q$ may not exist, assuming the negation of the Axiom of Choice. Where can I find a proof of this fact?","I would like to know if there are ""interesting"" equivalences to the statement ""$\mathbb R$ has a Hamel basis over $\mathbb Q$"". I am not interested in more general statements, like ""every vector space has a Hamel basis"" or ""every vector space over (your favorite field) has a Hamel basis"". On the other hand, according to this post , such Hamel basis of $\mathbb R$ over $\mathbb Q$ may not exist, assuming the negation of the Axiom of Choice. Where can I find a proof of this fact?",,"['linear-algebra', 'reference-request', 'set-theory', 'axiom-of-choice']"
58,"My proof of ""the set of diagonalizable matrices is Zariski-dense in $M_n(\mathbb F)$"".","My proof of ""the set of diagonalizable matrices is Zariski-dense in "".",M_n(\mathbb F),"The following is my proof of the assertion that the set of diagonalizable matrices  is Zariski-dense in $M_n(\mathbb F)$. Is this right ? Let $\mathbb F$ be an infinite field (not necessarily algebraically closed)and $M_n(\mathbb F)$ the set of all $n \times n$ matrices with entries in $\mathbb F$. We denote by $D_n(\mathbb F)$ the set of $n \times n$ diagonalizable matrices with entries in $\mathbb F$. For each $A \in M_n(\mathbb F)$, we denote by $d(A)$ the discriminant of the characteristic polynomial of $A$. Since $d(A)$ is a polynomial in the entries of $A$ with coefficients in $\mathbb F$, the set $U :=  \{ X \in M_n(\mathbb F) : d(X) \not = 0 \}$ is Zariski-open. (Here, we are identifying $M_n(\mathbb F)$ with ${\mathbb A}^{n^2}$.) It follows from the fact that ${\mathbb A}^{n^2}$ is irreducible that $U$ is Zariski-dense in ${\mathbb A}^{n^2}$. As $U$ is contained in $D_n(\mathbb F)$, $D_n(\mathbb F)$ is also Zariski-dense in ${\mathbb A}^{n^2}$. Thanks in advance.","The following is my proof of the assertion that the set of diagonalizable matrices  is Zariski-dense in $M_n(\mathbb F)$. Is this right ? Let $\mathbb F$ be an infinite field (not necessarily algebraically closed)and $M_n(\mathbb F)$ the set of all $n \times n$ matrices with entries in $\mathbb F$. We denote by $D_n(\mathbb F)$ the set of $n \times n$ diagonalizable matrices with entries in $\mathbb F$. For each $A \in M_n(\mathbb F)$, we denote by $d(A)$ the discriminant of the characteristic polynomial of $A$. Since $d(A)$ is a polynomial in the entries of $A$ with coefficients in $\mathbb F$, the set $U :=  \{ X \in M_n(\mathbb F) : d(X) \not = 0 \}$ is Zariski-open. (Here, we are identifying $M_n(\mathbb F)$ with ${\mathbb A}^{n^2}$.) It follows from the fact that ${\mathbb A}^{n^2}$ is irreducible that $U$ is Zariski-dense in ${\mathbb A}^{n^2}$. As $U$ is contained in $D_n(\mathbb F)$, $D_n(\mathbb F)$ is also Zariski-dense in ${\mathbb A}^{n^2}$. Thanks in advance.",,"['linear-algebra', 'general-topology']"
59,Prove that $A^3\equiv I\mod p$.,Prove that .,A^3\equiv I\mod p,"Let $p$ be a prime. Let $A$ be a $p\times p$ matrix whose $(i,j)$ th-coordinate is ${i+j-2\choose i-1}$ . Prove that $A^3\equiv I\mod p$ . Source: problem 10 from this problem set . We need to show that in $\mathbb{Z}_p, A^3 -I $ is the zero matrix, or equivalently that the minimal polynomial of $A$ in $M_p(\mathbb{Z}_p)$ divides $x^3-1=(x-1)(x^2+x+1).$ Clearly $A$ is not the identity matrix (e.g. $A_{1,2} = 1\neq 0$ ). I'm not sure if it's necessary to determine the eigenvalues or the characteristic polynomial of $A$ (in $\mathbb{Z}_p$ of course, since it doesn't seem necessary to consider other fields). I know the formula for the Vandermonde determinant, but I'm not sure if it's useful for this problem. Clearly modular arithmetic properties would be useful (e.g. $a^x\equiv b^x\mod n$ whenever $a\equiv b\mod n$ and if $f$ is a polynomial with integer coefficients, then $f(n)\equiv f(m)\mod a$ whenever $n\equiv m\mod a$ ), but they're not enough to make some progress on this problem. I'm not sure if it's useful to find the inverse of $A$ .","Let be a prime. Let be a matrix whose th-coordinate is . Prove that . Source: problem 10 from this problem set . We need to show that in is the zero matrix, or equivalently that the minimal polynomial of in divides Clearly is not the identity matrix (e.g. ). I'm not sure if it's necessary to determine the eigenvalues or the characteristic polynomial of (in of course, since it doesn't seem necessary to consider other fields). I know the formula for the Vandermonde determinant, but I'm not sure if it's useful for this problem. Clearly modular arithmetic properties would be useful (e.g. whenever and if is a polynomial with integer coefficients, then whenever ), but they're not enough to make some progress on this problem. I'm not sure if it's useful to find the inverse of .","p A p\times p (i,j) {i+j-2\choose i-1} A^3\equiv I\mod p \mathbb{Z}_p, A^3 -I  A M_p(\mathbb{Z}_p) x^3-1=(x-1)(x^2+x+1). A A_{1,2} = 1\neq 0 A \mathbb{Z}_p a^x\equiv b^x\mod n a\equiv b\mod n f f(n)\equiv f(m)\mod a n\equiv m\mod a A","['linear-algebra', 'matrices', 'elementary-number-theory', 'modular-arithmetic', 'contest-math']"
60,Diagonalization: Can you spot a trick to avoid tedious computation?,Diagonalization: Can you spot a trick to avoid tedious computation?,,"I am studying for my graduate qualifying exam and unfortunately for me I have spent the last two years studying commutative algebra and algebraic geometry, and the qualifying exam is entirely 'fundamental / core' material - tricky multivariable calculus and linear algebra questions, eek! Here is the question from an old exam I am working on. Please note, how to solve the problem is not my specific question. After I introduce the problem, I will ask my specific questions about the problem below. No calculators. Let $$M = \begin{bmatrix} 2 & 0 & 0 \\ 1 & 4 & 2 \\ 0 & -2 & -1\\ \end{bmatrix}$$ Find the determinant of $M$ , Find the eigenvalues and associated eigenvectors of $M$ , Calculate $$M^{2013} \cdot \begin{bmatrix}1\\1\\1\\\end{bmatrix}.$$ My issue is that with computational problems on an exam where calculators aren't allowed I always expect that either: there will be a trick to sidestep nasty calculations by hand, the problem will be contrived in such a way that the computation goes very easily. This seems to be the case in part 1 and part 2 of the problem since: The determinant of $M$ can very easily be found by cofactor across row 1 to get $\mathrm{det}(M) = 2(-4+4) = 0$ , or by inspection we see column 2 is quite visibly a scalar multiple of column 3 so that $\mathrm{det}(M) = 0$ . Since $\mathrm{det}(M) = 0$ we know $0$ is an eigenvalue. Noting the dependence relation between column 2 and column 3 allows us to easily read off an eigenvector for $\lambda = 0$ . Further, manually computing $\mathrm{det}(M - \lambda I)$ is again computationally easy because of the 0's across row 1. We get $p_{M}(t) = \lambda(2-\lambda)(\lambda - 3)$ . Solving for the corresponding eigenvectors is also fairly fast. Now - part 3 starts off fine. Considering part 2 it is practically implied from context clues that we are intended to diagonalize this matrix $M$ , as the only thing needed at this point is the inverse of the matrix of eigenvectors. The computation is when I go into a whirlwind because it does not flow as easily as the previous computations. In part 2 we had a degree three polynomial we wanted roots of, and of course it split into linear factors. Now I am inverting a 3x3 matrix by hand and getting all entries as ratios? On the exam, this will panic me. Time is definitely an issue and I need to learn how to not waste it. I immediately start re-studying the problem trying to see if there is some way around computing a 3 x 3 inverse by hand. One other approach I took, since I am just studying right now and not worried about time, was trying to express the vector $(1,1,1)^T$ as a linear combination of eigenvectors, say $$(1,1,1)^T = a_1v_1 + a_2v_2 + a_3v_3$$ with suitably chosen eigenvectors $v_1, v_2, v_3$ , since then $$M^{2013}(a_1 v_1 + a_2 v_2 + a_3 v_3) = a_2 \lambda_2^{2013}v_2 + a_3 \lambda_3^{2013} v_3.$$ Finding the linear combinations of eigenvectors seems to be no more or less easy than inverting the matrix of eigenvectors. Although I took a graduate abstract linear algebra course, I also worked in a tutoring center for years where I tutored problems like this without advanced methods - thus when I see questions like this, the method that immediately comes to mind is the classic one - diagonalize. Does anyone see any tricks to avoid nasty computation by hand in the problem above? More generally (I am sure lots of other users have taken graduate qual exams, and might have feedback here) does anyone have exam advice, perhaps a systematic way to decide if I should simply commit to doing the computation, and try to do it carefully yet as fast as possible, or halt myself in my tracks and say ""they wouldn't expect me to do this computation by hand, I should study the problem and see if there is a way around this."" Thank you. Edit: I suppose I may slightly be misusing this site since I know how to solve my problem, and my question is more geared towards exam skills? Part of my question even borders psychology... This is a bit of a philosophical conundrum whether my question is appropriate for the site. But, my exam is tomorrow so I will risk it! If it gets closed, so be it :)","I am studying for my graduate qualifying exam and unfortunately for me I have spent the last two years studying commutative algebra and algebraic geometry, and the qualifying exam is entirely 'fundamental / core' material - tricky multivariable calculus and linear algebra questions, eek! Here is the question from an old exam I am working on. Please note, how to solve the problem is not my specific question. After I introduce the problem, I will ask my specific questions about the problem below. No calculators. Let Find the determinant of , Find the eigenvalues and associated eigenvectors of , Calculate My issue is that with computational problems on an exam where calculators aren't allowed I always expect that either: there will be a trick to sidestep nasty calculations by hand, the problem will be contrived in such a way that the computation goes very easily. This seems to be the case in part 1 and part 2 of the problem since: The determinant of can very easily be found by cofactor across row 1 to get , or by inspection we see column 2 is quite visibly a scalar multiple of column 3 so that . Since we know is an eigenvalue. Noting the dependence relation between column 2 and column 3 allows us to easily read off an eigenvector for . Further, manually computing is again computationally easy because of the 0's across row 1. We get . Solving for the corresponding eigenvectors is also fairly fast. Now - part 3 starts off fine. Considering part 2 it is practically implied from context clues that we are intended to diagonalize this matrix , as the only thing needed at this point is the inverse of the matrix of eigenvectors. The computation is when I go into a whirlwind because it does not flow as easily as the previous computations. In part 2 we had a degree three polynomial we wanted roots of, and of course it split into linear factors. Now I am inverting a 3x3 matrix by hand and getting all entries as ratios? On the exam, this will panic me. Time is definitely an issue and I need to learn how to not waste it. I immediately start re-studying the problem trying to see if there is some way around computing a 3 x 3 inverse by hand. One other approach I took, since I am just studying right now and not worried about time, was trying to express the vector as a linear combination of eigenvectors, say with suitably chosen eigenvectors , since then Finding the linear combinations of eigenvectors seems to be no more or less easy than inverting the matrix of eigenvectors. Although I took a graduate abstract linear algebra course, I also worked in a tutoring center for years where I tutored problems like this without advanced methods - thus when I see questions like this, the method that immediately comes to mind is the classic one - diagonalize. Does anyone see any tricks to avoid nasty computation by hand in the problem above? More generally (I am sure lots of other users have taken graduate qual exams, and might have feedback here) does anyone have exam advice, perhaps a systematic way to decide if I should simply commit to doing the computation, and try to do it carefully yet as fast as possible, or halt myself in my tracks and say ""they wouldn't expect me to do this computation by hand, I should study the problem and see if there is a way around this."" Thank you. Edit: I suppose I may slightly be misusing this site since I know how to solve my problem, and my question is more geared towards exam skills? Part of my question even borders psychology... This is a bit of a philosophical conundrum whether my question is appropriate for the site. But, my exam is tomorrow so I will risk it! If it gets closed, so be it :)","M = \begin{bmatrix} 2 & 0 & 0 \\ 1 & 4 & 2 \\ 0 & -2 & -1\\ \end{bmatrix} M M M^{2013} \cdot \begin{bmatrix}1\\1\\1\\\end{bmatrix}. M \mathrm{det}(M) = 2(-4+4) = 0 \mathrm{det}(M) = 0 \mathrm{det}(M) = 0 0 \lambda = 0 \mathrm{det}(M - \lambda I) p_{M}(t) = \lambda(2-\lambda)(\lambda - 3) M (1,1,1)^T (1,1,1)^T = a_1v_1 + a_2v_2 + a_3v_3 v_1, v_2, v_3 M^{2013}(a_1 v_1 + a_2 v_2 + a_3 v_3) = a_2 \lambda_2^{2013}v_2 + a_3 \lambda_3^{2013} v_3.","['linear-algebra', 'matrices', 'diagonalization']"
61,Scaling of Lebesgue measure under a linear transformation and the volume of a parallelepiped.,Scaling of Lebesgue measure under a linear transformation and the volume of a parallelepiped.,,"$\def\vect{\mathbf} \def\diag{{\rm{diag}}} \def\R{\mathbb R} \def\vol{{\rm vol}} \def\sign{{\rm sign}}$ This post intentionally duplicates two other threads of questions from MSE.  The intention is to point out how they are connected and to give a succinct explanation. One thread asks why the Lebesgue measure of the parallelepiped $P$  in $\R^n$ determined by vectors $v_1, \dots, v_n$ is  $$ \lambda(P) = |\det(v_1, \dots, v_n)|$$ and hence the signed volume is  $$\vol(v_1, \dots, v_n) = \det(v_1, \dots, v_n).$$ The second thread asks why Lebesgue measure scales as it does under a linear transformation, namely, given a linear transformation $T$ of $\R^n$,  for all Borel sets $S$, $$ \lambda(T(S)) = |\det(T)|  \lambda(S). $$ For the first question, see here and here . For the second question, see here and here .","$\def\vect{\mathbf} \def\diag{{\rm{diag}}} \def\R{\mathbb R} \def\vol{{\rm vol}} \def\sign{{\rm sign}}$ This post intentionally duplicates two other threads of questions from MSE.  The intention is to point out how they are connected and to give a succinct explanation. One thread asks why the Lebesgue measure of the parallelepiped $P$  in $\R^n$ determined by vectors $v_1, \dots, v_n$ is  $$ \lambda(P) = |\det(v_1, \dots, v_n)|$$ and hence the signed volume is  $$\vol(v_1, \dots, v_n) = \det(v_1, \dots, v_n).$$ The second thread asks why Lebesgue measure scales as it does under a linear transformation, namely, given a linear transformation $T$ of $\R^n$,  for all Borel sets $S$, $$ \lambda(T(S)) = |\det(T)|  \lambda(S). $$ For the first question, see here and here . For the second question, see here and here .",,"['linear-algebra', 'measure-theory', 'determinant', 'volume']"
62,How should I study The Matrix Cookbook?,How should I study The Matrix Cookbook?,,"I use The Matrix Cookbook by Kaare Brandt Petersen and Michael Syskind Pedersen to solve many problems (mostly matrix derivatives). In most cases, I just map the problem to one of the formula and solve it but I cannot derive the formula by myself easily (I may prove the given formula is correct). Since I do not have access to the book when I am taking test, I am wondering how others perform these kinds of calculation without a reference book. Is this book just considered as a reference or I should study the book and try to drive the formula by myself? Does anyone have an insight on how to get better in matrix calculus (specially derivative with respect to vector or matrix) or how should I study such books? Thanks in advance.","I use The Matrix Cookbook by Kaare Brandt Petersen and Michael Syskind Pedersen to solve many problems (mostly matrix derivatives). In most cases, I just map the problem to one of the formula and solve it but I cannot derive the formula by myself easily (I may prove the given formula is correct). Since I do not have access to the book when I am taking test, I am wondering how others perform these kinds of calculation without a reference book. Is this book just considered as a reference or I should study the book and try to drive the formula by myself? Does anyone have an insight on how to get better in matrix calculus (specially derivative with respect to vector or matrix) or how should I study such books? Thanks in advance.",,"['linear-algebra', 'multivariable-calculus', 'self-learning', 'matrix-equations', 'matrix-calculus']"
63,Efficient diagonal update of matrix inverse,Efficient diagonal update of matrix inverse,,"I am computing $(kI + A)^{-1}$ in an iterative algorithm where $k$ changes in each iteration. $I$ is an $n$-by-$n$ identity matrix, $A$ is an $n$-by-$n$ precomputed symmetric positive-definite matrix. Since $A$ is precomputed I may invert, factor, decompose, or do anything to $A$ before the algorithm starts. $k$ will converge (not monotonically) to the sought output. Now, my question is if there is an efficient way to compute the inverse that does not involve computing the inverse of a full $n$-by-$n$ matrix?","I am computing $(kI + A)^{-1}$ in an iterative algorithm where $k$ changes in each iteration. $I$ is an $n$-by-$n$ identity matrix, $A$ is an $n$-by-$n$ precomputed symmetric positive-definite matrix. Since $A$ is precomputed I may invert, factor, decompose, or do anything to $A$ before the algorithm starts. $k$ will converge (not monotonically) to the sought output. Now, my question is if there is an efficient way to compute the inverse that does not involve computing the inverse of a full $n$-by-$n$ matrix?",,"['linear-algebra', 'matrices', 'inverse']"
64,Books for linear algebra over commutative rings,Books for linear algebra over commutative rings,,"I was thinking about reviewing linear algebra to recover many theorems that I can use over commutative rings with unity. But it seems very tedious and I did not want to make any mistakes on these theorems, as I often need to use them. I am wondering if there are good books out there for this purpose and want to know why they are good. More specifically, I want a good book that discusses (finite size) matrices over ring and their relationships with $R$-module homomorphisms, where $R$ is a ring or commutative ring (with $1$, of course).","I was thinking about reviewing linear algebra to recover many theorems that I can use over commutative rings with unity. But it seems very tedious and I did not want to make any mistakes on these theorems, as I often need to use them. I am wondering if there are good books out there for this purpose and want to know why they are good. More specifically, I want a good book that discusses (finite size) matrices over ring and their relationships with $R$-module homomorphisms, where $R$ is a ring or commutative ring (with $1$, of course).",,"['linear-algebra', 'reference-request', 'soft-question']"
65,Why invent the definition of pseudovector?,Why invent the definition of pseudovector?,,"According to Wikipedia: a pseudovector (or axial vector) is a quantity that transforms like a vector under a proper rotation, but in three dimensions gains an additional sign flip under an improper rotation such as a reflection. It seems that pseudovectors ""are not real vectors"". But if you think about it, every vector in $\mathbb{R}^3$ can be written as a cross product of two vectors. Let $\vec{v}_1 = (a,b,c)$, then $\vec{v}_2 = \frac{1}{\sqrt{a}}(-c,0,a), \vec{v}_3 = \frac{1}{\sqrt{a}}(-b,a,0)$ satisfy  $$ \vec{v}_3\times\vec{v}_2 = (a,b,c) $$ So we get that $\vec{v}_1$ is a pseudovector. But this means every vector is a pseudovector, so this definition seems empty to me in 3D.","According to Wikipedia: a pseudovector (or axial vector) is a quantity that transforms like a vector under a proper rotation, but in three dimensions gains an additional sign flip under an improper rotation such as a reflection. It seems that pseudovectors ""are not real vectors"". But if you think about it, every vector in $\mathbb{R}^3$ can be written as a cross product of two vectors. Let $\vec{v}_1 = (a,b,c)$, then $\vec{v}_2 = \frac{1}{\sqrt{a}}(-c,0,a), \vec{v}_3 = \frac{1}{\sqrt{a}}(-b,a,0)$ satisfy  $$ \vec{v}_3\times\vec{v}_2 = (a,b,c) $$ So we get that $\vec{v}_1$ is a pseudovector. But this means every vector is a pseudovector, so this definition seems empty to me in 3D.",,"['linear-algebra', 'cross-product']"
66,Degrees of freedom for a matrix,Degrees of freedom for a matrix,,"What does it mean for a matrix to have degrees of freedom? How does the degrees of freedom relate to constraints on what those values could be in the context of an optimization problem? I'm specifically confused about the last paragraph in this screenshot, but a more general explanation would be much appreciated.","What does it mean for a matrix to have degrees of freedom? How does the degrees of freedom relate to constraints on what those values could be in the context of an optimization problem? I'm specifically confused about the last paragraph in this screenshot, but a more general explanation would be much appreciated.",,['linear-algebra']
67,Does Cauchy-Schwarz Inequality depend on positive definiteness?,Does Cauchy-Schwarz Inequality depend on positive definiteness?,,"Let $V$ be a vector space over $\mathbb{R}$. Suppose we have a product $\langle \cdot,\cdot\rangle:V^2\to \mathbb{R}$ that satisfies all the inner product axioms except the second part of positive-definiteness: $$\langle x,x\rangle=0\iff x=0\tag{1}$$ So far, every proof I've seen that the Cauchy-Schwarz Inequality holds for all inner product spaces uses $(1)$. But does a proof of the Cauchy-Schwarz Inequality necessarily depend on $(1)$? Specifically, I'm looking for one of the following: A proof that the Cauchy-Schwarz Inequality holds for all ""inner product spaces"" where $(1)$ does not necessarily hold. A counterexample of a product $\langle \cdot,\cdot\rangle$ that follows symmetry, linearity in the first parameter, and $\langle u,u\rangle\ge 0$ for all $u\in V$, but where $\lvert \langle u,v\rangle\rvert\le \lvert\lvert u\rvert\rvert\ \lvert\lvert v\rvert\rvert$ does not always hold. I suspect that there is a counterexample, but it's hard for me to come up with one.","Let $V$ be a vector space over $\mathbb{R}$. Suppose we have a product $\langle \cdot,\cdot\rangle:V^2\to \mathbb{R}$ that satisfies all the inner product axioms except the second part of positive-definiteness: $$\langle x,x\rangle=0\iff x=0\tag{1}$$ So far, every proof I've seen that the Cauchy-Schwarz Inequality holds for all inner product spaces uses $(1)$. But does a proof of the Cauchy-Schwarz Inequality necessarily depend on $(1)$? Specifically, I'm looking for one of the following: A proof that the Cauchy-Schwarz Inequality holds for all ""inner product spaces"" where $(1)$ does not necessarily hold. A counterexample of a product $\langle \cdot,\cdot\rangle$ that follows symmetry, linearity in the first parameter, and $\langle u,u\rangle\ge 0$ for all $u\in V$, but where $\lvert \langle u,v\rangle\rvert\le \lvert\lvert u\rvert\rvert\ \lvert\lvert v\rvert\rvert$ does not always hold. I suspect that there is a counterexample, but it's hard for me to come up with one.",,"['linear-algebra', 'vector-spaces', 'inner-products', 'cauchy-schwarz-inequality']"
68,Are greedy methods such as orthogonal matching pursuit considered obsolete for finding sparse solutions?,Are greedy methods such as orthogonal matching pursuit considered obsolete for finding sparse solutions?,,"When researchers first began seeking sparse solutions to $Ax = b$ , they used greedy methods such as orthogonal matching pursuit (OMP).  In OMP, we activate components of $x$ one by one, and at each stage we select the component $i$ such that the $i$ th column of $A$ is most correlated with the residual $Ax - b$ . Researchers then developed methods such as Basis Pursuit and Lasso, which are based on solving optimization problems with sparsity-inducing regularizers. The Basis Pursuit problem is \begin{align} \underset{x}{\text{minimize}} & \quad \| x \|_1 \\ \text{subject to} & \quad Ax = b. \end{align} The Lasso problem is $$ \underset{x}{\text{minimize}} \quad \frac12 \| Ax - b \|_2^2 + \lambda \| x \|_1. $$ This new strategy was made possible by new optimization algorithms (interior point methods) which were able to solve these large scale optimization problems efficiently. Question: Are greedy methods such as orthogonal matching pursuit and its variants now considered to be obsolete?  Is there a consensus that they do not work as well as the approaches based on optimization with sparsity-inducing regularizers? Has OMP been abandoned? Here is a 1994 paper by Chen and Donoho that gives a brief overview of early attempts to find sparse solutions to $Ax = b$ , leading up to Basis Pursuit and Lasso: Atomic Decomposition by Basis Pursuit","When researchers first began seeking sparse solutions to , they used greedy methods such as orthogonal matching pursuit (OMP).  In OMP, we activate components of one by one, and at each stage we select the component such that the th column of is most correlated with the residual . Researchers then developed methods such as Basis Pursuit and Lasso, which are based on solving optimization problems with sparsity-inducing regularizers. The Basis Pursuit problem is The Lasso problem is This new strategy was made possible by new optimization algorithms (interior point methods) which were able to solve these large scale optimization problems efficiently. Question: Are greedy methods such as orthogonal matching pursuit and its variants now considered to be obsolete?  Is there a consensus that they do not work as well as the approaches based on optimization with sparsity-inducing regularizers? Has OMP been abandoned? Here is a 1994 paper by Chen and Donoho that gives a brief overview of early attempts to find sparse solutions to , leading up to Basis Pursuit and Lasso: Atomic Decomposition by Basis Pursuit","Ax = b x i i A Ax - b \begin{align}
\underset{x}{\text{minimize}} & \quad \| x \|_1 \\
\text{subject to} & \quad Ax = b.
\end{align} 
\underset{x}{\text{minimize}} \quad \frac12 \| Ax - b \|_2^2 + \lambda \| x \|_1.
 Ax = b","['linear-algebra', 'convex-optimization', 'sparsity']"
69,Orthogonal projection onto an affine subspace,Orthogonal projection onto an affine subspace,,"If we want to find the distance from a vector $x$ to a subspace $S$, we take $\| (I-P_S) x\|$, where $P_S$ is the orthogonal projection onto the subspace $S$. Obviously we could do the same thing for an affine subspace $A$, although $P_A$ would now not be a linear operator. But how can we find $P_A$? Or perhaps we need not go to the trouble of finding $P_A$ in order to calculate the distance from a point $x$ to $A$? Once we find $(I - P_A)(0)$, whose norm is the distance from the affine subspace to the origin, we're good, because then if $v = (I - P_A)(0)$, we have $\{a - v \mid a\in A\}$ is a subspace, and the distance from $x$ to $A$ is the distance from $x-v$ to $\{a - v \mid a\in A\}$. But is there an easier way? What is the easiest way to describe a projection onto an affine subspace? What is the easiest way to find the distance from a point to an affine subspace? I ask because I am afraid this will come up on some exams in the fall, so I am biased toward ""calculation"" type answers... (I apologize if this is a repeat... I didn't find this on the site)","If we want to find the distance from a vector $x$ to a subspace $S$, we take $\| (I-P_S) x\|$, where $P_S$ is the orthogonal projection onto the subspace $S$. Obviously we could do the same thing for an affine subspace $A$, although $P_A$ would now not be a linear operator. But how can we find $P_A$? Or perhaps we need not go to the trouble of finding $P_A$ in order to calculate the distance from a point $x$ to $A$? Once we find $(I - P_A)(0)$, whose norm is the distance from the affine subspace to the origin, we're good, because then if $v = (I - P_A)(0)$, we have $\{a - v \mid a\in A\}$ is a subspace, and the distance from $x$ to $A$ is the distance from $x-v$ to $\{a - v \mid a\in A\}$. But is there an easier way? What is the easiest way to describe a projection onto an affine subspace? What is the easiest way to find the distance from a point to an affine subspace? I ask because I am afraid this will come up on some exams in the fall, so I am biased toward ""calculation"" type answers... (I apologize if this is a repeat... I didn't find this on the site)",,"['linear-algebra', 'matrices']"
70,tensor product with dual space,tensor product with dual space,,"I will explain what I know, and then I will ask my question. Let $V$ and $W$ be vector spaces such that at least one is finite dimensional. In class, we showed that if either $V$ or $W$ is finite dimensional, then $W \otimes V^* \cong \operatorname{Hom}(V,W)$. We set up $\hat{e} : W \times V^* \to \operatorname{Hom}(V,W)$ with $\hat{e}(w,f)(v) = f(v)w$. This induced the linear map $e : W \otimes V^* \to \operatorname{Hom}(V,W)$ where $\hat{e} = e \otimes$. I understand why $e$ is injective, but I do not understand why it is surjective. I understand that any linear map $T: V \to W$ has finite rank (given that at least one of $V$ or $W$ has finite dimension), which gives me a finite basis of $im(T)$, but I do not know how to proceed. Any help would be great.","I will explain what I know, and then I will ask my question. Let $V$ and $W$ be vector spaces such that at least one is finite dimensional. In class, we showed that if either $V$ or $W$ is finite dimensional, then $W \otimes V^* \cong \operatorname{Hom}(V,W)$. We set up $\hat{e} : W \times V^* \to \operatorname{Hom}(V,W)$ with $\hat{e}(w,f)(v) = f(v)w$. This induced the linear map $e : W \otimes V^* \to \operatorname{Hom}(V,W)$ where $\hat{e} = e \otimes$. I understand why $e$ is injective, but I do not understand why it is surjective. I understand that any linear map $T: V \to W$ has finite rank (given that at least one of $V$ or $W$ has finite dimension), which gives me a finite basis of $im(T)$, but I do not know how to proceed. Any help would be great.",,"['linear-algebra', 'multilinear-algebra']"
71,Find the determinant of $A$ satisfying $A^{-1}=I-2A.$,Find the determinant of  satisfying,A A^{-1}=I-2A.,I am stuck with the following problem: Let $A$ be a $3\times 3$ matrix over real numbers satisfying $A^{-1}=I-2A.$ Then find the value of det$(A).$ I do not know how to proceed. Can someone point me in the right direction? Thanks in advance for your time.,I am stuck with the following problem: Let $A$ be a $3\times 3$ matrix over real numbers satisfying $A^{-1}=I-2A.$ Then find the value of det$(A).$ I do not know how to proceed. Can someone point me in the right direction? Thanks in advance for your time.,,['linear-algebra']
72,"When are two elements conjugated in GL(2), but not in SL(2)","When are two elements conjugated in GL(2), but not in SL(2)",,"Let $F$ be an arbitrary field. How can we describe the set of elements in $SL(2,F)$ which are conjugated in $GL(2,F)$ but not in $SL(2,F)$? I would be happy already with a partial solution as given in the comments.","Let $F$ be an arbitrary field. How can we describe the set of elements in $SL(2,F)$ which are conjugated in $GL(2,F)$ but not in $SL(2,F)$? I would be happy already with a partial solution as given in the comments.",,"['linear-algebra', 'abstract-algebra']"
73,Path for learning linear algebra and multivariable calculus,Path for learning linear algebra and multivariable calculus,,"I'll be finishing Calculus by Spivak somewhat soon, and want to continue into linear algebra and multivariable calculus afterwards. My current plan for learning the two subjects is just to read and work through Apostol volume II; is this a good idea, or would it be better to get a dedicated Linear Algebra book? Are there better books for multivariable calculus? (I don't think I want to jump right into Calculus on manifolds .) EDIT: I'd like to add, as part of my question, something I mentioned in a comment below. Namely, is it useful to learn multivariable calculus without differential forms and the general results on manifolds before reading something like Calculus on Manifolds or Analysis on Manifolds ? That is, do I need to learn vector calculus as it is taught in a second semester undergraduate course before approaching differential forms?","I'll be finishing Calculus by Spivak somewhat soon, and want to continue into linear algebra and multivariable calculus afterwards. My current plan for learning the two subjects is just to read and work through Apostol volume II; is this a good idea, or would it be better to get a dedicated Linear Algebra book? Are there better books for multivariable calculus? (I don't think I want to jump right into Calculus on manifolds .) EDIT: I'd like to add, as part of my question, something I mentioned in a comment below. Namely, is it useful to learn multivariable calculus without differential forms and the general results on manifolds before reading something like Calculus on Manifolds or Analysis on Manifolds ? That is, do I need to learn vector calculus as it is taught in a second semester undergraduate course before approaching differential forms?",,"['linear-algebra', 'reference-request', 'multivariable-calculus']"
74,Intersection of kernels and linear dependence of linear maps,Intersection of kernels and linear dependence of linear maps,,"Let $f_1,...,f_n,f: V \to W$ be linear maps of $K$-vector spaces. If $W=K$ it's known that $f$ is linear dependent from $f_1,...,f_n$ iff $\;\;\bigcap_{i=1}^n \ker(f_i) \subseteq \ker(f)$. Question: Is this statement true for general $W$ ? Remark: The direction $(\Rightarrow)$ is obviously true and  if it helps $W$ can be assumed to be finite dimensional. Edit: You can also assume $\dim V \ge \dim W$ (and if necessary the dimension of $V$ can be assumed to be much larger than that of $W$).","Let $f_1,...,f_n,f: V \to W$ be linear maps of $K$-vector spaces. If $W=K$ it's known that $f$ is linear dependent from $f_1,...,f_n$ iff $\;\;\bigcap_{i=1}^n \ker(f_i) \subseteq \ker(f)$. Question: Is this statement true for general $W$ ? Remark: The direction $(\Rightarrow)$ is obviously true and  if it helps $W$ can be assumed to be finite dimensional. Edit: You can also assume $\dim V \ge \dim W$ (and if necessary the dimension of $V$ can be assumed to be much larger than that of $W$).",,['linear-algebra']
75,Diagonalizable upper triangular matrices,Diagonalizable upper triangular matrices,,"It is true that if an upper triangular matrix $A$ with complex entries has distinct elements on the diagonal, then $A$ is diagonalizable. However, I don't think the converse is true. Is there a complete characterization of all diagonalizable upper triangular matrices?","It is true that if an upper triangular matrix $A$ with complex entries has distinct elements on the diagonal, then $A$ is diagonalizable. However, I don't think the converse is true. Is there a complete characterization of all diagonalizable upper triangular matrices?",,['linear-algebra']
76,Deducing results in linear algebra from results in commutative algebra,Deducing results in linear algebra from results in commutative algebra,,"Here are two examples of results which can be deduced from commutative algebra: Any $n\times n$ complex matrix is conjugate to a Jordan canonical matrix (can be proven using the structure theorem for modules over a PID, in this case $\mathbf{C}[T]$ - see for example these course notes ). Commuting matrices have a common eigenvector (this can be seen as a consequence of Hilbert's Nullstellensatz, according to Wikipedia ). My question is, does anyone know of other examples of results in linear algebra which can be deduced (non-trivially*) from results in commutative algebra? More precisely : Which results about modules over fields can be optained via modules over more general commutative rings? (Thanks to Martin's comment below for suggesting this precision of the question). .* by ""non-trivially,"" I mean you have to go deeper than simply applying module theory to modules over a field.","Here are two examples of results which can be deduced from commutative algebra: Any $n\times n$ complex matrix is conjugate to a Jordan canonical matrix (can be proven using the structure theorem for modules over a PID, in this case $\mathbf{C}[T]$ - see for example these course notes ). Commuting matrices have a common eigenvector (this can be seen as a consequence of Hilbert's Nullstellensatz, according to Wikipedia ). My question is, does anyone know of other examples of results in linear algebra which can be deduced (non-trivially*) from results in commutative algebra? More precisely : Which results about modules over fields can be optained via modules over more general commutative rings? (Thanks to Martin's comment below for suggesting this precision of the question). .* by ""non-trivially,"" I mean you have to go deeper than simply applying module theory to modules over a field.",,"['linear-algebra', 'commutative-algebra', 'modules', 'big-list', 'applications']"
77,Determine the matrix relative to a given basis,Determine the matrix relative to a given basis,,"Question: (a) Let $f: V \rightarrow W$ with $ V,W \simeq \mathbb{R}^{3}$ given by: $$f(x_1, x_2, x_3) = (x_1 - x_3, 2x_1 -5x_2 -x_3, x_2 + x_3).$$ Determine the matrix of $f$ relative to the basis $\{(0,2,1),(-1,1,1),(2,-1,1)\}$ of $V$ and $\{(-1,-1,0),(1,-1,2),(0,2,0)\}$ of $W$. (b) Let $n \in \mathbb{N}$ and $U_n$ the vector space of real polynomials of degree $\leq n$. The linear map $f: U_n \rightarrow U_n$ is given by $f(p) = p'$. Determine the matrix of $f$ relative to the basis $\{1,t,t^{2},...,t^{n}\}$ of $U_n$. My attempt so far:  (a): First relative to the bases of $W$ I found the coordinates of an arbitrary vector: $\left( \begin{array}{r} a \\ b \\ c \end{array} \right) = x \left( \begin{array}{r} -1 \\ -1 \\ 0 \end{array} \right) + y \left( \begin{array}{r} 1 \\ -1 \\ 2 \end{array} \right) + z \left( \begin{array}{c} 0 \\ 2 \\ 0 \end{array} \right)$ $\begin{array}{l} a = -x + y \\ b = - x - y + 2z \\ c = 2y \end{array}$ or $\begin{array}{l} x = -a + \frac{1}{2}c \\ z = -\frac{1}{2}a + \frac{1}{2}b + \frac{1}{2}c \\ y = \frac{1}{2}c \end{array}$ At this point I believe I have the linear combinations of the given basis in $W$ for an arbitrary vector, so next I take the vectors from $V$ and send them to $W$ using the given function: $\begin{array}{l} f(v_1) = f(0,2,1) = (-1,-11,3) = (1 + \frac{3}{2})w_1 + \frac{3}{2}w_2 + (\frac{1}{2} - \frac{11}{2} + \frac{3}{2})w_3 \\ f(v_2) = f(-1,1,1) = (-2,-8,2) = (2+1)w_1 + w_2 + (1 - 4 +1)w_3 \\ f(v_3) = f(2,-1,1) = (1,8,0) = w_1 + (-\frac{1}{2} + 4)w_3 \end{array}$ or $\left( \begin{array}{rrc} \frac{5}{2} & 3 & 1 \\ \frac{3}{2} & 1 & 0 \\ -\frac{7}{2} & -2 & \frac{7}{2}\end{array} \right)$ Was I taking the correct steps? I didn't really do anything differently based on the fact that $V,W$ were isometric... Is there a particular significance or interpretation for the resulting matrix? (b): Not really sure here... $f(p) = p'$ would it make sense to write something like: $f(1,t,t^{2},\dots, t^{n}) = (0,1,2t, \dots, nt^{n-1})$? and if a basis for $(1,t,t^{2},\dots, t^{n})$ would be $A = \left( \begin{array}{ccccc} 1 & 0 & 0 & \cdots & 0 \\ 0 & 1 & 0 & \cdots & 0 \\ 0 & 0 & 1 & \cdots & 0 \\ \vdots & \vdots & \vdots & \ddots & \vdots \\0 & \cdots & \cdots & 0 & 1 \end{array} \right)$ could i write: $A' =  \left( \begin{array}{ccccc} 0 & 0 & 0 & \cdots & 0 \\ 1 & 0 & 0 & \cdots & 0 \\ 0 & 1 & 0 & \cdots & 0 \\ \vdots & \vdots & \vdots & \ddots & \vdots \\0 & 0 & 0 & 1 & 0 \end{array} \right)$?","Question: (a) Let $f: V \rightarrow W$ with $ V,W \simeq \mathbb{R}^{3}$ given by: $$f(x_1, x_2, x_3) = (x_1 - x_3, 2x_1 -5x_2 -x_3, x_2 + x_3).$$ Determine the matrix of $f$ relative to the basis $\{(0,2,1),(-1,1,1),(2,-1,1)\}$ of $V$ and $\{(-1,-1,0),(1,-1,2),(0,2,0)\}$ of $W$. (b) Let $n \in \mathbb{N}$ and $U_n$ the vector space of real polynomials of degree $\leq n$. The linear map $f: U_n \rightarrow U_n$ is given by $f(p) = p'$. Determine the matrix of $f$ relative to the basis $\{1,t,t^{2},...,t^{n}\}$ of $U_n$. My attempt so far:  (a): First relative to the bases of $W$ I found the coordinates of an arbitrary vector: $\left( \begin{array}{r} a \\ b \\ c \end{array} \right) = x \left( \begin{array}{r} -1 \\ -1 \\ 0 \end{array} \right) + y \left( \begin{array}{r} 1 \\ -1 \\ 2 \end{array} \right) + z \left( \begin{array}{c} 0 \\ 2 \\ 0 \end{array} \right)$ $\begin{array}{l} a = -x + y \\ b = - x - y + 2z \\ c = 2y \end{array}$ or $\begin{array}{l} x = -a + \frac{1}{2}c \\ z = -\frac{1}{2}a + \frac{1}{2}b + \frac{1}{2}c \\ y = \frac{1}{2}c \end{array}$ At this point I believe I have the linear combinations of the given basis in $W$ for an arbitrary vector, so next I take the vectors from $V$ and send them to $W$ using the given function: $\begin{array}{l} f(v_1) = f(0,2,1) = (-1,-11,3) = (1 + \frac{3}{2})w_1 + \frac{3}{2}w_2 + (\frac{1}{2} - \frac{11}{2} + \frac{3}{2})w_3 \\ f(v_2) = f(-1,1,1) = (-2,-8,2) = (2+1)w_1 + w_2 + (1 - 4 +1)w_3 \\ f(v_3) = f(2,-1,1) = (1,8,0) = w_1 + (-\frac{1}{2} + 4)w_3 \end{array}$ or $\left( \begin{array}{rrc} \frac{5}{2} & 3 & 1 \\ \frac{3}{2} & 1 & 0 \\ -\frac{7}{2} & -2 & \frac{7}{2}\end{array} \right)$ Was I taking the correct steps? I didn't really do anything differently based on the fact that $V,W$ were isometric... Is there a particular significance or interpretation for the resulting matrix? (b): Not really sure here... $f(p) = p'$ would it make sense to write something like: $f(1,t,t^{2},\dots, t^{n}) = (0,1,2t, \dots, nt^{n-1})$? and if a basis for $(1,t,t^{2},\dots, t^{n})$ would be $A = \left( \begin{array}{ccccc} 1 & 0 & 0 & \cdots & 0 \\ 0 & 1 & 0 & \cdots & 0 \\ 0 & 0 & 1 & \cdots & 0 \\ \vdots & \vdots & \vdots & \ddots & \vdots \\0 & \cdots & \cdots & 0 & 1 \end{array} \right)$ could i write: $A' =  \left( \begin{array}{ccccc} 0 & 0 & 0 & \cdots & 0 \\ 1 & 0 & 0 & \cdots & 0 \\ 0 & 1 & 0 & \cdots & 0 \\ \vdots & \vdots & \vdots & \ddots & \vdots \\0 & 0 & 0 & 1 & 0 \end{array} \right)$?",,"['linear-algebra', 'matrices']"
78,Proving the following inequality (positive def. matrix),Proving the following inequality (positive def. matrix),,"I'm trying to prove (or disprove) the following: $$ \sum_{i=1}^{N} \sum_{j=1}^{N} c_i c_j K_{ij}  \geq  0$$ where $c \in \mathbb{R}^N$ , and $K_{ij}$ is referring to a kernel matrix : $$K_{ij} = K(x_i,x_j) = \frac{\sum_{k=1}^{N} \min(x_{ik}, x_{jk})}{\sum_{k=1}^{N} \max(x_{ik}, x_{jk})}$$ Here, $x \in \mathbb{R}^N \geq 0$ . I'm basically trying to prove that $K_{ij}$ is a positive definite matrix, so I can use it as a Kernel, but I'm really stuck trying to work with $\max$ Edit: the function I'm refering to is: $$K(u,v) = \frac{\sum_{k=1}^{N} \min(u_{k}, v_{k})}{\sum_{k=1}^{N} \max(u_{k}, v_{k})}$$ where $u, v \in \mathbb{R}^N \geq 0$","I'm trying to prove (or disprove) the following: where , and is referring to a kernel matrix : Here, . I'm basically trying to prove that is a positive definite matrix, so I can use it as a Kernel, but I'm really stuck trying to work with Edit: the function I'm refering to is: where"," \sum_{i=1}^{N} \sum_{j=1}^{N} c_i c_j K_{ij}  \geq  0 c \in \mathbb{R}^N K_{ij} K_{ij} = K(x_i,x_j) = \frac{\sum_{k=1}^{N} \min(x_{ik}, x_{jk})}{\sum_{k=1}^{N} \max(x_{ik}, x_{jk})} x \in \mathbb{R}^N \geq 0 K_{ij} \max K(u,v) = \frac{\sum_{k=1}^{N} \min(u_{k}, v_{k})}{\sum_{k=1}^{N} \max(u_{k}, v_{k})} u, v \in \mathbb{R}^N \geq 0","['linear-algebra', 'positive-definite', 'reproducing-kernel-hilbert-spaces']"
79,How can I determine the number of wedge products of $1$-forms needed to express a $k$-form as a sum of such?,How can I determine the number of wedge products of -forms needed to express a -form as a sum of such?,1 k,"This question was motivated by this related one: How ""far"" a differential form is from an exterior product . Let $\mathbb{V}$ be a vector space of dimension $n$ with underlying field $\mathbb{F}$, and say (for lack of a better term) that the wedge rank of a $k$-form $$\phi \in \Lambda^k \mathbb{V}^*$$ is the minimum number $r$ for which there are exist wedge products $v_a^1 \wedge \cdots \wedge v_a^k$, $a = 1, \ldots, r$, of $1$-forms $v_a^b \in \mathbb{V}^*$, $b = 1, \ldots, k$, such that $$\phi = \sum_{a = 1}^r v_a^1 \wedge \cdots \wedge v_a^k.$$ (For convenience, we can declare the empty sum to have value the $0$ k-form, so that the wedge rank of $0$ is $0$.) In general, given  $\phi$, what is an effective way to determine its wedge rank $r$? We can make a few obvious remarks: First, $r \leq \dim \Lambda^k \mathbb{V}^* = {{n}\choose{k}}$, but in general it is much smaller, and anyway for nonzero $0$-, $1$- and $n$-forms, $r = 1$, and exploiting the natural isomorphism $\Lambda^{n - 1} \mathbb{V}^* \cong \mathbb{V} \otimes \Lambda^n \mathbb{V}^*$ gives that the same applies to nonzero $(n - 1)$-forms. For $k = 2$, a $2$-form $\phi$ has wedge rank $1$ (that is, it is decomposable) iff $\phi \wedge \phi = 0$, and we can exploit the isomorphism $\Lambda^{n - 2} \mathbb{V}^* \cong \Lambda^2 \mathbb{V} \otimes \Lambda^n \mathbb{V}^*$ to make an analogous statement about the $k = n - 2$ case. Furthermore, if $n$ is even, the wedge rank of $\phi$ is exactly $r$ iff $$\underbrace{\phi \wedge \cdots \wedge \phi}_r \neq 0 \qquad \text{but} \qquad \underbrace{\phi \wedge \cdots \wedge \phi}_{r + 1} = 0.$$ (Perhaps something similar holds for odd $n$?) In higher tensor ranks, the story quickly becomes more complicated. For example, if $\dim \mathbb{V} = 7$ (and $\mathbb{F}$ perfect and $\text{char } \mathbb{F} \neq 2$), the tensor rank of a $3$-form $\phi$ is at most $5$ (this already seems nonobvious). It turns out (at least over $\mathbb{R}$ and $\mathbb{C}$) that $r = 5$ iff the $\Lambda^7 \mathbb{V}^*$-valued bilinear form $$(X, Y) \mapsto (i_X \phi) \wedge (i_Y \phi) \wedge \phi$$ is nondegenerate, but the rank of the bilinear form does not determine $r$ for all smaller values of $r$. Anyway, this particular property seems essentially unique to this $(n, k)$. There's a further complication, namely that the wedge rank of a $k$-form need not remain the same under extension of the base field. This phenomenon already shows up in the smallest-dimensional case not covered by the above considerations: If $\mathbb{F} = \mathbb{R}$ and $\dim \Bbb V = 6$, there is a $3$-form whose stabilizer under the pullback action of $GL(\mathbb{V}) \cong GL(6, \mathbb{R})$ on $\Lambda^3 \mathbb{V}^*$ is exactly $SU(3)$, and any such $3$-form has wedge rank $4$ (in fact, there is a single $GL(\Bbb V)$-orbit of such $3$-forms, and it is open). When viewed as an element of the complex vector space $\mathbb{V} \otimes_{\mathbb{R}} \mathbb{C}$, however, any such $3$-form has wedge rank $2$. So, the structure of the underlying field $\mathbb{F}$ plays a (to me) subtle role, and quite possibly it turns out this question is easier to answer over algebraically closed fields.","This question was motivated by this related one: How ""far"" a differential form is from an exterior product . Let $\mathbb{V}$ be a vector space of dimension $n$ with underlying field $\mathbb{F}$, and say (for lack of a better term) that the wedge rank of a $k$-form $$\phi \in \Lambda^k \mathbb{V}^*$$ is the minimum number $r$ for which there are exist wedge products $v_a^1 \wedge \cdots \wedge v_a^k$, $a = 1, \ldots, r$, of $1$-forms $v_a^b \in \mathbb{V}^*$, $b = 1, \ldots, k$, such that $$\phi = \sum_{a = 1}^r v_a^1 \wedge \cdots \wedge v_a^k.$$ (For convenience, we can declare the empty sum to have value the $0$ k-form, so that the wedge rank of $0$ is $0$.) In general, given  $\phi$, what is an effective way to determine its wedge rank $r$? We can make a few obvious remarks: First, $r \leq \dim \Lambda^k \mathbb{V}^* = {{n}\choose{k}}$, but in general it is much smaller, and anyway for nonzero $0$-, $1$- and $n$-forms, $r = 1$, and exploiting the natural isomorphism $\Lambda^{n - 1} \mathbb{V}^* \cong \mathbb{V} \otimes \Lambda^n \mathbb{V}^*$ gives that the same applies to nonzero $(n - 1)$-forms. For $k = 2$, a $2$-form $\phi$ has wedge rank $1$ (that is, it is decomposable) iff $\phi \wedge \phi = 0$, and we can exploit the isomorphism $\Lambda^{n - 2} \mathbb{V}^* \cong \Lambda^2 \mathbb{V} \otimes \Lambda^n \mathbb{V}^*$ to make an analogous statement about the $k = n - 2$ case. Furthermore, if $n$ is even, the wedge rank of $\phi$ is exactly $r$ iff $$\underbrace{\phi \wedge \cdots \wedge \phi}_r \neq 0 \qquad \text{but} \qquad \underbrace{\phi \wedge \cdots \wedge \phi}_{r + 1} = 0.$$ (Perhaps something similar holds for odd $n$?) In higher tensor ranks, the story quickly becomes more complicated. For example, if $\dim \mathbb{V} = 7$ (and $\mathbb{F}$ perfect and $\text{char } \mathbb{F} \neq 2$), the tensor rank of a $3$-form $\phi$ is at most $5$ (this already seems nonobvious). It turns out (at least over $\mathbb{R}$ and $\mathbb{C}$) that $r = 5$ iff the $\Lambda^7 \mathbb{V}^*$-valued bilinear form $$(X, Y) \mapsto (i_X \phi) \wedge (i_Y \phi) \wedge \phi$$ is nondegenerate, but the rank of the bilinear form does not determine $r$ for all smaller values of $r$. Anyway, this particular property seems essentially unique to this $(n, k)$. There's a further complication, namely that the wedge rank of a $k$-form need not remain the same under extension of the base field. This phenomenon already shows up in the smallest-dimensional case not covered by the above considerations: If $\mathbb{F} = \mathbb{R}$ and $\dim \Bbb V = 6$, there is a $3$-form whose stabilizer under the pullback action of $GL(\mathbb{V}) \cong GL(6, \mathbb{R})$ on $\Lambda^3 \mathbb{V}^*$ is exactly $SU(3)$, and any such $3$-form has wedge rank $4$ (in fact, there is a single $GL(\Bbb V)$-orbit of such $3$-forms, and it is open). When viewed as an element of the complex vector space $\mathbb{V} \otimes_{\mathbb{R}} \mathbb{C}$, however, any such $3$-form has wedge rank $2$. So, the structure of the underlying field $\mathbb{F}$ plays a (to me) subtle role, and quite possibly it turns out this question is easier to answer over algebraically closed fields.",,"['linear-algebra', 'tensors', 'multilinear-algebra']"
80,Solving the quadratic equation for matrices,Solving the quadratic equation for matrices,,"Suppose that $A,\;B,\;C,\;$and $X$ are all real commuting matrices. I am curious how to solve $$AX^2+BX+C=0$$ for $X$. In addition what properties do we need on $A,\;B,$ and $C$ for the solution to exist? Last is this possible for non-commuting matrices?","Suppose that $A,\;B,\;C,\;$and $X$ are all real commuting matrices. I am curious how to solve $$AX^2+BX+C=0$$ for $X$. In addition what properties do we need on $A,\;B,$ and $C$ for the solution to exist? Last is this possible for non-commuting matrices?",,"['linear-algebra', 'matrices', 'quadratics', 'matrix-equations']"
81,What does abstract algebra have to say about the determinant?,What does abstract algebra have to say about the determinant?,,"The determinant is a homomorphism from the multiplicative monoid of matrices to the multiplicative monoid of a field (right?). I find this to be the most intuitive way to interpret some of the determinant's properties (notably the invertibility condition: obviously a matrix is only invertible if a homomorphism maps it to an invertible element of the field). So from an algebraic point of view, can anything interesting be said about the determinant? Some fairly specific technical questions: How does it fit into the big picture of all homomorphisms $M_n(\Bbb K)\to\Bbb K^\star$? Is it the only one? How are the others related to it? The invertibility condition is one of the most useful things about the determinant, but any homomorphism would give the same condition, so why use the determinant? Are there any ring homomorphisms $M_n(\Bbb K)\to\Bbb K^\star$? How does the determinant relate to them? Some vaguer, softer questions: Are there any intuitive proofs for the formulae for calculating determinants based on the fact that it's a homomorphism? This one's pretty out there. The characteristic polynomial is a polynomial with coefficients in a field which is the homomorphic image of a polynomial with coefficients in the ring of matrices. Is there any way of explaining the various relationships between a matrix and its characteristic polynomial based on the structure-preserving properties of the determinant?","The determinant is a homomorphism from the multiplicative monoid of matrices to the multiplicative monoid of a field (right?). I find this to be the most intuitive way to interpret some of the determinant's properties (notably the invertibility condition: obviously a matrix is only invertible if a homomorphism maps it to an invertible element of the field). So from an algebraic point of view, can anything interesting be said about the determinant? Some fairly specific technical questions: How does it fit into the big picture of all homomorphisms $M_n(\Bbb K)\to\Bbb K^\star$? Is it the only one? How are the others related to it? The invertibility condition is one of the most useful things about the determinant, but any homomorphism would give the same condition, so why use the determinant? Are there any ring homomorphisms $M_n(\Bbb K)\to\Bbb K^\star$? How does the determinant relate to them? Some vaguer, softer questions: Are there any intuitive proofs for the formulae for calculating determinants based on the fact that it's a homomorphism? This one's pretty out there. The characteristic polynomial is a polynomial with coefficients in a field which is the homomorphic image of a polynomial with coefficients in the ring of matrices. Is there any way of explaining the various relationships between a matrix and its characteristic polynomial based on the structure-preserving properties of the determinant?",,"['linear-algebra', 'abstract-algebra', 'determinant']"
82,Understanding the Leontief inverse,Understanding the Leontief inverse,,"What I remember from economics about input/output analysis is that it basically analyses the interdependencies between business sectors and demand. If we use matrices we have $A$ as the input-output matrix, $I$ as an identity matrix and $d$ as final demand. In order to find the final input $x$ we may solve the Leontief Inverse : $$ x = (I-A)^{-1}\cdot d $$ So here's my question: Is there a simple rationale behind this inverse? Especially when considering the form: $$    (I-A)^{-1} = I+A + A^2 + A^3\ldots $$ What happens if we change an element $a_{i,j}$ in $A$? How is this transmitted within the system? And is there decent literature about this behaviour around? Thank you very much for your help!","What I remember from economics about input/output analysis is that it basically analyses the interdependencies between business sectors and demand. If we use matrices we have $A$ as the input-output matrix, $I$ as an identity matrix and $d$ as final demand. In order to find the final input $x$ we may solve the Leontief Inverse : $$ x = (I-A)^{-1}\cdot d $$ So here's my question: Is there a simple rationale behind this inverse? Especially when considering the form: $$    (I-A)^{-1} = I+A + A^2 + A^3\ldots $$ What happens if we change an element $a_{i,j}$ in $A$? How is this transmitted within the system? And is there decent literature about this behaviour around? Thank you very much for your help!",,"['linear-algebra', 'matrices', 'economics']"
83,Spielman's proof of graph connectivity,Spielman's proof of graph connectivity,,"I use Spielman's lectures on course Spectral Graph Theory I have few question regarding Lecture 2. The Laplacian , especially Lemma 2.3.1 (Graph connectivity). Please, help me to make it a little bit clearer. Lemma 2.3.1 . Let $G=(V,E)$ be a graph, and let $0=\lambda_1 \leq \lambda_2 \leq \cdots\leq \lambda_n $ be the eigenvalues of its Laplacian matrix. Then, $\lambda_2 >0 $ if and only if $G$ is connected. Proof: We first show that $\lambda_{2}=0$  if $G$ is disconnected. If $G$ is disconnected, then it can be described as the union of two graphs, $G_{1}$ and $G_{2}$. After suitably re-numbering the vertices, we can write $$L_{G}=\begin{bmatrix} L_{G_{1}}  & 0\\ 0  &  L_{G_{2}} \end{bmatrix}.$$ So, $L_{G}$ has at least two orthogonal eigenvectors of eigenvalue zero: $$\begin{bmatrix} 0\\  1 \end{bmatrix} \mbox{ and }  \begin{bmatrix} 1\\  0 \end{bmatrix}$$ where we have partitioned the vectors as we did the matrix $L_{G}$. What does it mean ""orthogonal eigenvectors of eigenvalue zero""? Are those eigenvalues really equal 0? How do we know that these eigenvalues correspond to $\lambda_{2}$? I suspect that all $\lambda$'s are ordered by multiplicity. But how can I prove that every $n$ by $n$ symmetric matrix has $n$ eigenvalues, counted with multiplicity. On the other hand, assume that $G$ is connected and that $x$ is an eigenvector of $L_{G}$ of eigenvalue $0$. As $L_{G}x=0$, we have $$x^{T}L_{G}x=\sum_{(u,v)\in E}^{} (x(u)-x(v))^{2} = 0.$$ Thus, for each pair of vertices $(u,v)$ connected by an edge, we have $x(u)=x(v)$. Why actually it's true if $u$ and $v$ are connected by an edge, we have $x(u)=x(v)$, it's not obvious for me, even if $x$ is eigenvector, and what happens in the general case? As every pair of vertices $u$ and $v$  are connected by a path, we may inductively apply this fact to show that $x(u)=x(v)$ for all vertices $u$ and $v$. Thus, $x$ must be a constant vector. We conclude that the eigenspace of eigenvalue $0$ has dimension $1$. OK, by now we prove that $\lambda_{1}=0$, but what about $\lambda_{2}$?","I use Spielman's lectures on course Spectral Graph Theory I have few question regarding Lecture 2. The Laplacian , especially Lemma 2.3.1 (Graph connectivity). Please, help me to make it a little bit clearer. Lemma 2.3.1 . Let $G=(V,E)$ be a graph, and let $0=\lambda_1 \leq \lambda_2 \leq \cdots\leq \lambda_n $ be the eigenvalues of its Laplacian matrix. Then, $\lambda_2 >0 $ if and only if $G$ is connected. Proof: We first show that $\lambda_{2}=0$  if $G$ is disconnected. If $G$ is disconnected, then it can be described as the union of two graphs, $G_{1}$ and $G_{2}$. After suitably re-numbering the vertices, we can write $$L_{G}=\begin{bmatrix} L_{G_{1}}  & 0\\ 0  &  L_{G_{2}} \end{bmatrix}.$$ So, $L_{G}$ has at least two orthogonal eigenvectors of eigenvalue zero: $$\begin{bmatrix} 0\\  1 \end{bmatrix} \mbox{ and }  \begin{bmatrix} 1\\  0 \end{bmatrix}$$ where we have partitioned the vectors as we did the matrix $L_{G}$. What does it mean ""orthogonal eigenvectors of eigenvalue zero""? Are those eigenvalues really equal 0? How do we know that these eigenvalues correspond to $\lambda_{2}$? I suspect that all $\lambda$'s are ordered by multiplicity. But how can I prove that every $n$ by $n$ symmetric matrix has $n$ eigenvalues, counted with multiplicity. On the other hand, assume that $G$ is connected and that $x$ is an eigenvector of $L_{G}$ of eigenvalue $0$. As $L_{G}x=0$, we have $$x^{T}L_{G}x=\sum_{(u,v)\in E}^{} (x(u)-x(v))^{2} = 0.$$ Thus, for each pair of vertices $(u,v)$ connected by an edge, we have $x(u)=x(v)$. Why actually it's true if $u$ and $v$ are connected by an edge, we have $x(u)=x(v)$, it's not obvious for me, even if $x$ is eigenvector, and what happens in the general case? As every pair of vertices $u$ and $v$  are connected by a path, we may inductively apply this fact to show that $x(u)=x(v)$ for all vertices $u$ and $v$. Thus, $x$ must be a constant vector. We conclude that the eigenspace of eigenvalue $0$ has dimension $1$. OK, by now we prove that $\lambda_{1}=0$, but what about $\lambda_{2}$?",,"['linear-algebra', 'graph-theory', 'eigenvalues-eigenvectors', 'spectral-graph-theory']"
84,when does $\det(AB^T+BA^T)\le \det(AA^T+BB^T)$ hold?,when does  hold?,\det(AB^T+BA^T)\le \det(AA^T+BB^T),When does the following matrix inequality hold? $$\det(AB+B^TA^T)\le \det(AA^T+BB^T)$$ $A$ and $B$ are any real matrices. My reply gives a counter example. The question is under what condition does that hold? [Update] The inequality actually I would like to prove is not the above one. It should be $$\det(AB^T+BA^T)\le \det(AA^T+BB^T)$$ I also changed the title. Thanks julien.,When does the following matrix inequality hold? $$\det(AB+B^TA^T)\le \det(AA^T+BB^T)$$ $A$ and $B$ are any real matrices. My reply gives a counter example. The question is under what condition does that hold? [Update] The inequality actually I would like to prove is not the above one. It should be $$\det(AB^T+BA^T)\le \det(AA^T+BB^T)$$ I also changed the title. Thanks julien.,,"['linear-algebra', 'matrices', 'inequality', 'determinant']"
85,A matrix involving distances of $n$ points in $\mathbb{R}^3$,A matrix involving distances of  points in,n \mathbb{R}^3,"Let $x_1,\ldots ,x_n$ be $n$ distinct points in $\mathbb{R}^3$ . Consider the $n\times n$ real symmetric matrix $A$ defined by $A_{ij}:=|x_i-x_j|$ . I would like to show that $$Ker\,A\;\cap\,\{v\in\mathbb{R}^n\,:\, v_1+v_2 +\ldots +v_n=0\}=\{0\}$$ Thank you for any suggestions.",Let be distinct points in . Consider the real symmetric matrix defined by . I would like to show that Thank you for any suggestions.,"x_1,\ldots ,x_n n \mathbb{R}^3 n\times n A A_{ij}:=|x_i-x_j| Ker\,A\;\cap\,\{v\in\mathbb{R}^n\,:\, v_1+v_2 +\ldots +v_n=0\}=\{0\}","['linear-algebra', 'matrices']"
86,Proving associativity of matrix multiplication,Proving associativity of matrix multiplication,,"I'm trying to prove that matrix multiplication is associative, but seem to be making mistakes in each of my past write-ups, so hopefully someone can check over my work. Theorem. Let $A$ be $\alpha \times \beta$, $B$ be $\beta \times \gamma$, and $C$ be $\gamma \times \delta$. Prove that $(AB)C = A(BC)$. Proof. Define general entries of the matrices $A$, $B$, and $C$ by $a_{g,h}$, $b_{i,j}$, and $c_{k,m}$, respectively. Then, for the LHS: \begin{align*} & (AB)_{\alpha, \gamma} = \sum\limits_{p=1}^{\beta} a_{\alpha,p} b_{p,\gamma} \\ & \left((AB)C\right)_{\alpha, \delta} = \sum\limits_{n=1}^{\gamma} \left(AB\right)_{\alpha, n} c_{n, \delta} = \sum\limits_{n=1}^{\gamma} \left(\sum\limits_{p=1}^{\beta} a_{\alpha,p} b_{p,n} \right) c_{n, \delta} = \sum\limits_{n=1}^{\gamma} \sum\limits_{p=1}^{\beta} \left(a_{\alpha,p} b_{p,n}\right) c_{n, \delta}. \end{align*} For the RHS:  \begin{align*} & \left(BC\right)_{\beta, \delta} = \sum\limits_{n=1}^{\gamma} b_{\beta, n} c_{n, \delta} \\ & \left(A\left(BC\right)\right)_{\alpha,\delta} = \sum\limits_{p=1}^{\beta} a_{\alpha,p} (BC)_{p, \delta} = \sum\limits_{p=1}^{\beta} a_{\alpha,p} \left(\sum\limits_{n=1}^{\gamma} b_{p, n} c_{n, \delta} \right) = \sum\limits_{p=1}^{\beta} \sum\limits_{n=1}^{\gamma} a_{\alpha,p} \left(b_{p, n} c_{n, \delta} \right). \end{align*} Assuming I have written these correctly, we can make two observations: first, the summands are equivalent, as multiplication is associative. Second, the order of the summations doesn't matter when we're summing a finite number of entries. Thus, $(AB)C = A(BC)$. How does this look?","I'm trying to prove that matrix multiplication is associative, but seem to be making mistakes in each of my past write-ups, so hopefully someone can check over my work. Theorem. Let $A$ be $\alpha \times \beta$, $B$ be $\beta \times \gamma$, and $C$ be $\gamma \times \delta$. Prove that $(AB)C = A(BC)$. Proof. Define general entries of the matrices $A$, $B$, and $C$ by $a_{g,h}$, $b_{i,j}$, and $c_{k,m}$, respectively. Then, for the LHS: \begin{align*} & (AB)_{\alpha, \gamma} = \sum\limits_{p=1}^{\beta} a_{\alpha,p} b_{p,\gamma} \\ & \left((AB)C\right)_{\alpha, \delta} = \sum\limits_{n=1}^{\gamma} \left(AB\right)_{\alpha, n} c_{n, \delta} = \sum\limits_{n=1}^{\gamma} \left(\sum\limits_{p=1}^{\beta} a_{\alpha,p} b_{p,n} \right) c_{n, \delta} = \sum\limits_{n=1}^{\gamma} \sum\limits_{p=1}^{\beta} \left(a_{\alpha,p} b_{p,n}\right) c_{n, \delta}. \end{align*} For the RHS:  \begin{align*} & \left(BC\right)_{\beta, \delta} = \sum\limits_{n=1}^{\gamma} b_{\beta, n} c_{n, \delta} \\ & \left(A\left(BC\right)\right)_{\alpha,\delta} = \sum\limits_{p=1}^{\beta} a_{\alpha,p} (BC)_{p, \delta} = \sum\limits_{p=1}^{\beta} a_{\alpha,p} \left(\sum\limits_{n=1}^{\gamma} b_{p, n} c_{n, \delta} \right) = \sum\limits_{p=1}^{\beta} \sum\limits_{n=1}^{\gamma} a_{\alpha,p} \left(b_{p, n} c_{n, \delta} \right). \end{align*} Assuming I have written these correctly, we can make two observations: first, the summands are equivalent, as multiplication is associative. Second, the order of the summations doesn't matter when we're summing a finite number of entries. Thus, $(AB)C = A(BC)$. How does this look?",,['linear-algebra']
87,"Topology of ""degenerate spectrum submanifold"" in the space of hermitian matrices","Topology of ""degenerate spectrum submanifold"" in the space of hermitian matrices",,"Consider the space $H_n$ of hermitian matrices acting on $\mathbb C^n$ . It contains a subset $LC_n$ of matrices with degenerate spectrum. I want to know as much as possible about topology and geometry of this set and its complement. In particular is $LC_n$ a submanifold? I suspect that it could, and its codimension is 3.  Can we calculate the cohomology ring and some homotopy groups of $LC_n$ and $H_n \setminus LC_n$ ? $H_n \setminus LC_n$ is an open subset of $H_n$ , hence submanifold. Moreover it is dense and connected. All these properties are easily seen by considering decomposition $T=U^{\dagger}DU$ with $D$ - diagonal and $U$ unitary. Let $\mathcal E_n$ be the space of increasing $n$ -tuples of real numbers and let $\mathcal F_n$ be the complete flag variety, $\mathcal F_n = \frac{U(n)}{U(1)^n}$ . Then $H_n \setminus LC_n$ is diffeomorphic to the cartesian product $\mathcal E_n \times \mathcal F_n$ . Once again, this is is easy to see using the $U^{\dagger}DU$ decomposition. Elements of $\mathcal E_n$ are the eigenvalues, while elements of $\mathcal F_n$ are the eigenspaces. Since $\mathcal E_n \cong \mathbb R^n$ , we see that $H^{\bullet}(H_n \setminus LC_n) \cong H^{\bullet} (\mathcal F_n)$ . This cohomology group is computed on Wikipedia in the article on generalized flag varieties. I think this description of the topology of $H_n \setminus LC_n$ is pretty complete and satisfying. However I am also interested in $LC_n$ itself. Let $\chi_T$ be the characteristic polynomial of $T$ . Operator $T$ is in $LC_n$ if and only if $\chi_T$ has a double zero. This is equivalent to vanishing of the discriminant $\Delta$ of $\chi_T$ , which is easily seen to be a polynomial in the matrix elements of $T$ . Thus $LC_n$ is an algebraic variety in $H_n \cong \mathbb R^{n^2}$ . For any $n$ there exist points of $LC_n$ on which the first derivative of $\Delta$ vanishes. Thus it's impossible to conclude that $LC_n$ is a submanifold using implicit function theorem. My conjecture about codimension $3$ is based on the analysis of oribts of $U(n)$ acting on $H_n$ . Namely we need to tune one real number parametrizing $T$ to make it degenerate, but then dimension of the stabilizer of $T$ in $U(n)$ (acting by conjugation) becomes larger at lest by $2$ . More precisely, if $T$ has $k$ distinct eigenvalues with dimensions of eigenspaces $g_1,...,g_k$ , then $\mathrm{Stab}(T) \cong U(g_1) \times ... \times U(g_k)$ . $0$ is in $LC_n$ and $\lambda T \in LC_n$ whenever $T \in LC_n$ and $\lambda \in \mathbb R$ . In particular $LC_n$ is contractible to a point. Clearly the ""correct"" way of studying the geometry of $LC_n$ would be to consider it as a projective variety in $\mathbb P \mathbb R^{n^2-1}$ . In fact $\Delta$ is a homogeneous polynomial of degree $n(n-1)$ .","Consider the space of hermitian matrices acting on . It contains a subset of matrices with degenerate spectrum. I want to know as much as possible about topology and geometry of this set and its complement. In particular is a submanifold? I suspect that it could, and its codimension is 3.  Can we calculate the cohomology ring and some homotopy groups of and ? is an open subset of , hence submanifold. Moreover it is dense and connected. All these properties are easily seen by considering decomposition with - diagonal and unitary. Let be the space of increasing -tuples of real numbers and let be the complete flag variety, . Then is diffeomorphic to the cartesian product . Once again, this is is easy to see using the decomposition. Elements of are the eigenvalues, while elements of are the eigenspaces. Since , we see that . This cohomology group is computed on Wikipedia in the article on generalized flag varieties. I think this description of the topology of is pretty complete and satisfying. However I am also interested in itself. Let be the characteristic polynomial of . Operator is in if and only if has a double zero. This is equivalent to vanishing of the discriminant of , which is easily seen to be a polynomial in the matrix elements of . Thus is an algebraic variety in . For any there exist points of on which the first derivative of vanishes. Thus it's impossible to conclude that is a submanifold using implicit function theorem. My conjecture about codimension is based on the analysis of oribts of acting on . Namely we need to tune one real number parametrizing to make it degenerate, but then dimension of the stabilizer of in (acting by conjugation) becomes larger at lest by . More precisely, if has distinct eigenvalues with dimensions of eigenspaces , then . is in and whenever and . In particular is contractible to a point. Clearly the ""correct"" way of studying the geometry of would be to consider it as a projective variety in . In fact is a homogeneous polynomial of degree .","H_n \mathbb C^n LC_n LC_n LC_n H_n \setminus LC_n H_n \setminus LC_n H_n T=U^{\dagger}DU D U \mathcal E_n n \mathcal F_n \mathcal F_n = \frac{U(n)}{U(1)^n} H_n \setminus LC_n \mathcal E_n \times \mathcal F_n U^{\dagger}DU \mathcal E_n \mathcal F_n \mathcal E_n \cong \mathbb R^n H^{\bullet}(H_n \setminus LC_n) \cong H^{\bullet} (\mathcal F_n) H_n \setminus LC_n LC_n \chi_T T T LC_n \chi_T \Delta \chi_T T LC_n H_n \cong \mathbb R^{n^2} n LC_n \Delta LC_n 3 U(n) H_n T T U(n) 2 T k g_1,...,g_k \mathrm{Stab}(T) \cong U(g_1) \times ... \times U(g_k) 0 LC_n \lambda T \in LC_n T \in LC_n \lambda \in \mathbb R LC_n LC_n \mathbb P \mathbb R^{n^2-1} \Delta n(n-1)","['linear-algebra', 'differential-geometry', 'algebraic-geometry', 'algebraic-topology', 'algebraic-groups']"
88,Symmetric tensor decomposition in higher tensor powers,Symmetric tensor decomposition in higher tensor powers,,"Let $V$ be a vector space over a field $k$ of characteristic $0$ (not assumed to be algebraically closed, if it makes any difference). Consider the sequence of tensor powers  $$V^{\otimes 2} = V\otimes V, \quad V^{\otimes 3} = V\otimes V\otimes V,\dots $$ It's well-known that there is a ""braiding"" representation of $S_n$ on $V^{\otimes n}$ given by permuting the tensor factors; i.e., $\sigma\in S_n$ acts linearly and on basis vectors via $$\sigma(v_{i_1}\otimes\dots\otimes v_{i_n}) = v_{i_{\sigma(1)}}\otimes\dots\otimes v_{i_{\sigma(n)}}.$$ The symmetric tensors are defined as the subspace $\text{Sym}^n (V)\subseteq V^{\otimes n}$ on which $S_n$ acts trivially. Now for $n=2$ there is a very nice decomposition: namely $S_2=C_2$ is cyclic of order $2$, and the nontrivial automorphism $\epsilon$ of $V\otimes V$ has order $2$. Therefore its eigenvalues are $\pm 1$; $\text{Sym}^2 (V)$ is the $+1$ eigenspace and $\Lambda^2 (V)$ (the alternating square) is the $-1$ eigenspace. This decomposes $V\otimes V$ as a direct sum: $$V\otimes V = \text{Sym}^2 (V)\oplus \Lambda^2 (V).$$ My questions are: Does a generalisation of this decomposition hold in the higher tensor powers? i.e. Can we write $V^{\otimes n} = \text{Sym}^n (V)\oplus W$ for some explicit direct sum $W$? As a specialisation, assume that $\Lambda^2 (V) \cong k$ is one-dimensional, so that $\dim V = 2$, say with basis vectors $v_1$ and $v_2$. In this case I have tried working out the decomposition of the higher tensor powers into symmetric tensors plus ""alternating"" parts. For example, $V^{\otimes 3} = \text{Sym}^3 (V)\oplus W$ for a $4$-dimensional space $W$ on which $S_3$ acts nontrivially. I calculated that $$W = (V\otimes\Lambda^2 (V)) \oplus (\Lambda^2 (V)\otimes V)\cong V\oplus V.$$ I want to convince myself (and make precise) that in the decomposition $V^{\otimes n} = \text{Sym}^n (V)\oplus W$ for larger $n$, the direct summands appearing in $W$ are ""all things we've seen before"" (e.g. for $n=3$ there were just two copies of $V$, rather than some horrible ""higher-alternating-square"" thing), and so the only genuinely new things (representations of the symmetric group?) occurring in this sequence are the symmetric tensors $\text{Sym}^n (V)$. Can anyone explain how this holds in general?","Let $V$ be a vector space over a field $k$ of characteristic $0$ (not assumed to be algebraically closed, if it makes any difference). Consider the sequence of tensor powers  $$V^{\otimes 2} = V\otimes V, \quad V^{\otimes 3} = V\otimes V\otimes V,\dots $$ It's well-known that there is a ""braiding"" representation of $S_n$ on $V^{\otimes n}$ given by permuting the tensor factors; i.e., $\sigma\in S_n$ acts linearly and on basis vectors via $$\sigma(v_{i_1}\otimes\dots\otimes v_{i_n}) = v_{i_{\sigma(1)}}\otimes\dots\otimes v_{i_{\sigma(n)}}.$$ The symmetric tensors are defined as the subspace $\text{Sym}^n (V)\subseteq V^{\otimes n}$ on which $S_n$ acts trivially. Now for $n=2$ there is a very nice decomposition: namely $S_2=C_2$ is cyclic of order $2$, and the nontrivial automorphism $\epsilon$ of $V\otimes V$ has order $2$. Therefore its eigenvalues are $\pm 1$; $\text{Sym}^2 (V)$ is the $+1$ eigenspace and $\Lambda^2 (V)$ (the alternating square) is the $-1$ eigenspace. This decomposes $V\otimes V$ as a direct sum: $$V\otimes V = \text{Sym}^2 (V)\oplus \Lambda^2 (V).$$ My questions are: Does a generalisation of this decomposition hold in the higher tensor powers? i.e. Can we write $V^{\otimes n} = \text{Sym}^n (V)\oplus W$ for some explicit direct sum $W$? As a specialisation, assume that $\Lambda^2 (V) \cong k$ is one-dimensional, so that $\dim V = 2$, say with basis vectors $v_1$ and $v_2$. In this case I have tried working out the decomposition of the higher tensor powers into symmetric tensors plus ""alternating"" parts. For example, $V^{\otimes 3} = \text{Sym}^3 (V)\oplus W$ for a $4$-dimensional space $W$ on which $S_3$ acts nontrivially. I calculated that $$W = (V\otimes\Lambda^2 (V)) \oplus (\Lambda^2 (V)\otimes V)\cong V\oplus V.$$ I want to convince myself (and make precise) that in the decomposition $V^{\otimes n} = \text{Sym}^n (V)\oplus W$ for larger $n$, the direct summands appearing in $W$ are ""all things we've seen before"" (e.g. for $n=3$ there were just two copies of $V$, rather than some horrible ""higher-alternating-square"" thing), and so the only genuinely new things (representations of the symmetric group?) occurring in this sequence are the symmetric tensors $\text{Sym}^n (V)$. Can anyone explain how this holds in general?",,"['linear-algebra', 'representation-theory', 'tensor-products', 'symmetric-groups']"
89,Properties of the A-transpose-A matrix,Properties of the A-transpose-A matrix,,"I believe that $A^TA$ is a key matrix structure because of its connection to variance-covariance matrices. In Professor Strang's linear algebra lectures, "" A-transpose-A "" - with this nomenclature, as opposed to $X'X$ , for example - is the revolving axis. Yet, it is not easy to find on a quick Google search a list of its properties. I presume that part of the reason may be that they are shared by variance-covariance matrices. But I'd like to confirm this (does it have identical properties to a var-cov matrix?), and have the list easily available from now on here at SE-Mathematics. Just to not shy away from the initial effort, here is what I think I have so far: Symmetry Positive semidefinite- ness Real and positive eigenvalues The trace is positive (the trace is the sum of eigenvalues) The determinant is positive (the determinant is the product of the eigenvalues) The diagonal entries are all positive Orthogonal eigenvectors (**) Diagonalizable as $Q\Lambda Q^T$ It is possible to obtain a Cholesky decomposition . Rank of $A^TA$ is the same as rank of $A$ . $\text{ker}(A^TA)=\text{ker}(A)$ (**) The eigenvectors of A-transpose-A form the matrix $V$ in singular value decomposition (SVD) of $A,$ while the square root of the eigenvalues of A-transpose-A are the singular values of the SVD. Similarly, the eigenvectors of A-A-transpose $AA^\top$ include the columns in the matrix $U$ of the SVD of $A.$ The importance of this is exemplified in the fact that SVD can be used to solve least squares regression by computing the Penrose-Moore pseudo-inverse $A^\dagger = V\Sigma^\dagger U^*,$ although the QR decomposition is a more expedient computational method. There is a nice post on the topic here .","I believe that is a key matrix structure because of its connection to variance-covariance matrices. In Professor Strang's linear algebra lectures, "" A-transpose-A "" - with this nomenclature, as opposed to , for example - is the revolving axis. Yet, it is not easy to find on a quick Google search a list of its properties. I presume that part of the reason may be that they are shared by variance-covariance matrices. But I'd like to confirm this (does it have identical properties to a var-cov matrix?), and have the list easily available from now on here at SE-Mathematics. Just to not shy away from the initial effort, here is what I think I have so far: Symmetry Positive semidefinite- ness Real and positive eigenvalues The trace is positive (the trace is the sum of eigenvalues) The determinant is positive (the determinant is the product of the eigenvalues) The diagonal entries are all positive Orthogonal eigenvectors (**) Diagonalizable as It is possible to obtain a Cholesky decomposition . Rank of is the same as rank of . (**) The eigenvectors of A-transpose-A form the matrix in singular value decomposition (SVD) of while the square root of the eigenvalues of A-transpose-A are the singular values of the SVD. Similarly, the eigenvectors of A-A-transpose include the columns in the matrix of the SVD of The importance of this is exemplified in the fact that SVD can be used to solve least squares regression by computing the Penrose-Moore pseudo-inverse although the QR decomposition is a more expedient computational method. There is a nice post on the topic here .","A^TA X'X Q\Lambda Q^T A^TA A \text{ker}(A^TA)=\text{ker}(A) V A, AA^\top U A. A^\dagger = V\Sigma^\dagger U^*,",['linear-algebra']
90,Difference Between Tensoring and Wedging.,Difference Between Tensoring and Wedging.,,"Let $V$ be a vector space and $\omega\in \otimes^k V$. There are $2$ ways (at least) of thinking about $\omega\otimes \omega$. 1) We may think of $\otimes^k V$ as a vector space $W$, and $\omega\otimes \omega$ as a member of $W\otimes W$. 2) We may think of $\omega\otimes \omega$ as a member of $\otimes^{2k}V$. The two interpretations are ""same"" because $W\otimes W$ is naturally isomorphic to $\otimes^{2k} V$. However, the situation is a bit different when talking about ""wedging"". Let $\eta\in \Lambda^k V$. We want to wonder about $\eta\wedge \eta$. 1) Let $W=\Lambda^k V$ and think of $\eta\wedge \eta$ as a member of $\Lambda^2 W$. Then $\eta\wedge \eta=0$ by super-commutativity of the wedge-product. 2) Think of $\eta\wedge\eta$ as a member of $\Lambda^{2k}V$. Then $\eta\wedge \eta$ may not be $0$. Perhaps this confusion would not arise if we write $\wedge_V$ rather that $\wedge$, for when wedging we must remember the base space. Moreover, there is no such thing as taking the wedge product of two vector spaces, thought we can talk about tensor product of two vector spaces. Admittedly, my mind is not completely clear here. Can somebody throw some more light on the different behaviours of tensoring and wedging.","Let $V$ be a vector space and $\omega\in \otimes^k V$. There are $2$ ways (at least) of thinking about $\omega\otimes \omega$. 1) We may think of $\otimes^k V$ as a vector space $W$, and $\omega\otimes \omega$ as a member of $W\otimes W$. 2) We may think of $\omega\otimes \omega$ as a member of $\otimes^{2k}V$. The two interpretations are ""same"" because $W\otimes W$ is naturally isomorphic to $\otimes^{2k} V$. However, the situation is a bit different when talking about ""wedging"". Let $\eta\in \Lambda^k V$. We want to wonder about $\eta\wedge \eta$. 1) Let $W=\Lambda^k V$ and think of $\eta\wedge \eta$ as a member of $\Lambda^2 W$. Then $\eta\wedge \eta=0$ by super-commutativity of the wedge-product. 2) Think of $\eta\wedge\eta$ as a member of $\Lambda^{2k}V$. Then $\eta\wedge \eta$ may not be $0$. Perhaps this confusion would not arise if we write $\wedge_V$ rather that $\wedge$, for when wedging we must remember the base space. Moreover, there is no such thing as taking the wedge product of two vector spaces, thought we can talk about tensor product of two vector spaces. Admittedly, my mind is not completely clear here. Can somebody throw some more light on the different behaviours of tensoring and wedging.",,"['linear-algebra', 'tensor-products', 'exterior-algebra']"
91,Evaluating eigenvalues of a product of two positive definite matrices,Evaluating eigenvalues of a product of two positive definite matrices,,"Let $A,B\in M_n(\mathbb{R})$ be two symmetric positive definite matrices, i.e.: $$\forall x\in\mathbb{R}^n, x\neq 0, (Ax,x)>0, (Bx,x)>0,$$ where $(\cdot,\cdot)$ is the usual scalar product in $\mathbb{R}^n$. It is equivalent to saying that eigenvalues of A and B are strictly positive (and there exists an orthonormal eigenbasis for both matrices). We order these eigenvalues which are not necessarily distinct: $$\lambda_1(A)\leq ...\leq\lambda_n(A)$$ and $$\lambda_1(B)\leq ...\leq\lambda_n(B).$$ It is not hard to prove the minimax principle for these eigenvalues: $$\lambda_k(A)=\min_{\substack{F\subset \mathbb{R}^n \\ \dim(F)=k}} \left( \max_{x\in F\backslash \{0\}} \frac{(Ax,x)}{(x,x)}\right).$$ For $\lambda_1(A)$ and $\lambda_n(A)$ we have simpler expressions: $$\lambda_1(A)=\min_{x\in\mathbb{R}^n \backslash \{0\}} \frac{(Ax,x)}{(x,x)}\hspace{1cm}\text{and}\hspace{1cm}\lambda_n(A)=\max_{x\in\mathbb{R}^n \backslash \{0\}} \frac{(Ax,x)}{(x,x)}.$$ We now consider the matrix $AB$. It is possible to prove that the eigenvalues of $AB$ are the same as for the matrix $\sqrt{B}\cdot A\cdot \sqrt{B}$, where $\sqrt{A}$ denotes the unique square root matrix of $A$: $\sqrt{A}$ is real, symmetric, positive definite and $\sqrt{A}\cdot \sqrt{A}=A$. The eigenvalues of $AB$ are hence real and strictly positive. We order them similarly like we did for A and B. Prove that, $\forall 1\leq k\leq n$: $$\lambda_k(A)\lambda_1(B)\leq \lambda_k(AB)\leq \lambda_k(A)\lambda_n(B).$$ Note that we cannot use the minimax principle for the $\lambda_k(AB)$ since $AB$ is not necessarily symmetric. However, the hint of the exercise suggests we use the the minimax principle at some point. References to some books? Any thoughts how I should attack the problem? Maybe I am missing some obvious observations?","Let $A,B\in M_n(\mathbb{R})$ be two symmetric positive definite matrices, i.e.: $$\forall x\in\mathbb{R}^n, x\neq 0, (Ax,x)>0, (Bx,x)>0,$$ where $(\cdot,\cdot)$ is the usual scalar product in $\mathbb{R}^n$. It is equivalent to saying that eigenvalues of A and B are strictly positive (and there exists an orthonormal eigenbasis for both matrices). We order these eigenvalues which are not necessarily distinct: $$\lambda_1(A)\leq ...\leq\lambda_n(A)$$ and $$\lambda_1(B)\leq ...\leq\lambda_n(B).$$ It is not hard to prove the minimax principle for these eigenvalues: $$\lambda_k(A)=\min_{\substack{F\subset \mathbb{R}^n \\ \dim(F)=k}} \left( \max_{x\in F\backslash \{0\}} \frac{(Ax,x)}{(x,x)}\right).$$ For $\lambda_1(A)$ and $\lambda_n(A)$ we have simpler expressions: $$\lambda_1(A)=\min_{x\in\mathbb{R}^n \backslash \{0\}} \frac{(Ax,x)}{(x,x)}\hspace{1cm}\text{and}\hspace{1cm}\lambda_n(A)=\max_{x\in\mathbb{R}^n \backslash \{0\}} \frac{(Ax,x)}{(x,x)}.$$ We now consider the matrix $AB$. It is possible to prove that the eigenvalues of $AB$ are the same as for the matrix $\sqrt{B}\cdot A\cdot \sqrt{B}$, where $\sqrt{A}$ denotes the unique square root matrix of $A$: $\sqrt{A}$ is real, symmetric, positive definite and $\sqrt{A}\cdot \sqrt{A}=A$. The eigenvalues of $AB$ are hence real and strictly positive. We order them similarly like we did for A and B. Prove that, $\forall 1\leq k\leq n$: $$\lambda_k(A)\lambda_1(B)\leq \lambda_k(AB)\leq \lambda_k(A)\lambda_n(B).$$ Note that we cannot use the minimax principle for the $\lambda_k(AB)$ since $AB$ is not necessarily symmetric. However, the hint of the exercise suggests we use the the minimax principle at some point. References to some books? Any thoughts how I should attack the problem? Maybe I am missing some obvious observations?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
92,Why are matrices indexed by row first?,Why are matrices indexed by row first?,,"I am wondering is there some greater thought behind indexing matrices in form (row, column) , or is it just something that someone did, and everybody else followed? When I'm looking at the matrix, I find it easiest to imagine it as numbers laid out in 2D grid. From my previous education I usually follow (x,y) order, and it seems like in many other areas we usually deal with x before y . When I'm thinking ${A_{3 1}}$ , I'd think x = 3, y = 1, and therefore get 3rd column and 1st row. This would be wrong - I need to do otherwise(y's first). Why so?","I am wondering is there some greater thought behind indexing matrices in form (row, column) , or is it just something that someone did, and everybody else followed? When I'm looking at the matrix, I find it easiest to imagine it as numbers laid out in 2D grid. From my previous education I usually follow (x,y) order, and it seems like in many other areas we usually deal with x before y . When I'm thinking , I'd think x = 3, y = 1, and therefore get 3rd column and 1st row. This would be wrong - I need to do otherwise(y's first). Why so?",{A_{3 1}},"['linear-algebra', 'matrices']"
93,$C(M)=\{A\in M_n(\mathbb{C}) \mid AM=MA\}$ is a subspace of dimension at least $n$.,is a subspace of dimension at least .,C(M)=\{A\in M_n(\mathbb{C}) \mid AM=MA\} n,"Let $M_n(\mathbb{C})$ denote the vector space over $\mathbb{C}$ of all $n\times n$ complex matrices. Prove that if $M$ is a complex $n\times n$ matrix then $C(M)=\{A\in M_n(\mathbb{C}) \mid AM=MA\}$ is a subspace of dimension at least $n$. My Try: I proved that $C(M)$ is a subspace. But how can I show that it is of dimension at least $n$. No idea how to do it. I found similar questions posted in MSE but could not find a clear answer. So, please do not mark this as duplicate. Can somebody please help me how to find this? EDIT: Non of the given answers were clear to me. I would appreciate if somebody check my try below: If $J$ is a Jordan Canonical form of $A$, then they are similar. Similar matrices have same rank. $J$ has dimension at least $n$. So does $A$. Am I correct?","Let $M_n(\mathbb{C})$ denote the vector space over $\mathbb{C}$ of all $n\times n$ complex matrices. Prove that if $M$ is a complex $n\times n$ matrix then $C(M)=\{A\in M_n(\mathbb{C}) \mid AM=MA\}$ is a subspace of dimension at least $n$. My Try: I proved that $C(M)$ is a subspace. But how can I show that it is of dimension at least $n$. No idea how to do it. I found similar questions posted in MSE but could not find a clear answer. So, please do not mark this as duplicate. Can somebody please help me how to find this? EDIT: Non of the given answers were clear to me. I would appreciate if somebody check my try below: If $J$ is a Jordan Canonical form of $A$, then they are similar. Similar matrices have same rank. $J$ has dimension at least $n$. So does $A$. Am I correct?",,"['linear-algebra', 'matrices', 'vector-spaces']"
94,A non-square matrix with orthonormal columns,A non-square matrix with orthonormal columns,,"I know these 2 statements to be true: 1) An $n$ x $n$ matrix U has orthonormal columns iff. $U^TU=I=UU^T$. 2) An $m$ x $n$ matrix U has orthonormal columns iff. $U^TU=I$. But can (2) be generalised to become ""An $m$ x $n$ matrix U has orthonormal columns iff. $U^TU=I=UU^T$"" ? Why or why not? Thanks!","I know these 2 statements to be true: 1) An $n$ x $n$ matrix U has orthonormal columns iff. $U^TU=I=UU^T$. 2) An $m$ x $n$ matrix U has orthonormal columns iff. $U^TU=I$. But can (2) be generalised to become ""An $m$ x $n$ matrix U has orthonormal columns iff. $U^TU=I=UU^T$"" ? Why or why not? Thanks!",,"['linear-algebra', 'matrices']"
95,Maximum determinant of Latin squares,Maximum determinant of Latin squares,,"I strongly conjecture that the maximum absolute determinant of a Latin square can be attained by a circulant matrix. For example, $$\pmatrix {5&4&2&3&1 \\ 1&5&4&2&3 \\ 3&1&5&4&2 \\ 2&3&1&5&4 \\ 4&2&3&1&5}$$ has determinant $2325$, which is indeed the maximum absolute value of the determinant of a Latin square of size $5 \times 5$. The sign of the determinant is not important because changing two rows always gives a Latin square with positive determinant, if the given has a negative determinant. Is this conjecture true ? If the conjecture is true, there would be a suitable way to find the maximum absolute value of the determinant of a Latin square of size $n \times n$. I tried to find out how an arbitrary Latin square can be transformed in the circulant form without changing its determinant, but without success. If the conjecture is true, the maximal values are (on the left side, the top row of the matrix is given, the sign is not considered): $$ 1\ 2\ \ \ \ \ \ \ 3$$ $$ 3\ 1\ 2\ \ \ \ \ \ 18$$ $$ 4\ 1\ 2\ 3\ \ \ \ \ 160$$ $$ 5\ 4\ 2\ 3\ 1\ \ \ \ \ \ 2 \ 325$$ $$ 6\ 5\ 3\ 2\ 4\ 1\ \ \ \ \ 41 \ 895$$ The next $4$ values are $961\ 772,\ 26\ 978\ 400,\ 929\ 587\ 995\ and \ 36\ 843\ 728\ 625$. So, an upper bound for the determinant of a $9 \times 9$-Sudoku-matrix would be $929\ 587\ 995$.","I strongly conjecture that the maximum absolute determinant of a Latin square can be attained by a circulant matrix. For example, $$\pmatrix {5&4&2&3&1 \\ 1&5&4&2&3 \\ 3&1&5&4&2 \\ 2&3&1&5&4 \\ 4&2&3&1&5}$$ has determinant $2325$, which is indeed the maximum absolute value of the determinant of a Latin square of size $5 \times 5$. The sign of the determinant is not important because changing two rows always gives a Latin square with positive determinant, if the given has a negative determinant. Is this conjecture true ? If the conjecture is true, there would be a suitable way to find the maximum absolute value of the determinant of a Latin square of size $n \times n$. I tried to find out how an arbitrary Latin square can be transformed in the circulant form without changing its determinant, but without success. If the conjecture is true, the maximal values are (on the left side, the top row of the matrix is given, the sign is not considered): $$ 1\ 2\ \ \ \ \ \ \ 3$$ $$ 3\ 1\ 2\ \ \ \ \ \ 18$$ $$ 4\ 1\ 2\ 3\ \ \ \ \ 160$$ $$ 5\ 4\ 2\ 3\ 1\ \ \ \ \ \ 2 \ 325$$ $$ 6\ 5\ 3\ 2\ 4\ 1\ \ \ \ \ 41 \ 895$$ The next $4$ values are $961\ 772,\ 26\ 978\ 400,\ 929\ 587\ 995\ and \ 36\ 843\ 728\ 625$. So, an upper bound for the determinant of a $9 \times 9$-Sudoku-matrix would be $929\ 587\ 995$.",,"['linear-algebra', 'matrices', 'determinant', 'latin-square']"
96,What is an intuitive way to understand the dot product in the context of matrix multiplication?,What is an intuitive way to understand the dot product in the context of matrix multiplication?,,"I was trying to understand where it came from that each row in a matrix multiplication is a dot product, as in: $$ Ax = \left(  \begin{array}{ccc} a_{1}^T \\ \vdots \\ a_m^T  \end{array}  \right)x = \left(  \begin{array}{ccc} a_{1}^Tx \\ \vdots \\ a_m^T x  \end{array}  \right) $$ what is an intuitive explanation or interpretation that each row is a dot product of the vector x? What I do understand is that $Ax$ encodes a linear transformation $T$. Consider a super simple example in 2 dimensions to explain what I do understand. I understand that $Ax = A [x_1 x_2] = T(v) = T(x_1 \hat i + x_2 \hat j) = x_1 T(\hat i) + x_2 T( \hat j)$. This makes me interpret intuitively that a multiplication by a matrix gives me a new vector that is composed of the same linear combination of the transformed basis vectors (or whatever vectors v is composed of) [source] . Furthermore one can easily see from this view where the multiplication of a matrix comes from: $$Ax = \left[  \begin{array}{ccc} a_{11} & a_{12} \\ a_{21} & a_{22} \\   \end{array}  \right]  x = \left[  \begin{array}{ccc} T(\hat i)_1 & T(\hat j)_1\\ T(\hat i)_2 & T(\hat j)_2\\ \end{array} \right] \left[ \begin{array}{ccc} x_{1} \\ x_{2} \\   \end{array}  \right] =  x_1\left[  \begin{array}{ccc} T(\hat i)_1 \\ T(\hat i)_2 \\ \end{array} \right] +  x_2 \left[ \begin{array}{ccc} T(\hat j)_1\\ T(\hat j)_2\\ \end{array} \right] = \left[  \begin{array}{ccc} T(\hat i)_1 x_1 + T(\hat j)_1 x_2\\ T(\hat i)_2x_2 + T(\hat j)_2 x_2\\ \end{array} \right] $$ where now its obvious why matrix multiplication is defined the way it is (because of linear transformations). Notice that the nice thing about this view is that one can interpret that each column of the matrix tells us how each basis vector changes . i.e. each column specifies how $\hat i$, $\hat j$ are transformed. Furthermore, the amount it used to be in the old vector is retained but now its in the new direction $T(\hat i)$ for the first coordinate. This for me is really intuitive and explains a lot of where matrix multiplication comes from. However, if you notice this view reveals that each row $(Ax)_i = a_1^T x$ is a dot product of the initial array representation of the vector. This seems to me to not be a coincidence and that something deeper has to be going on. Usually dot products are related with projections so I was trying to understand if each coordinate of $(Ax)_i$ might actually be encoding how much the original $x$ is being projected into each row vector of $A$ (or possible something to do with the row space of $A$ i.e. $C(A^T)$ ). In an attempt to understand this I considered what each row means: $$ \left[ a_{i,1} \dots a_{i,m} \right] \left[ \begin{array}{ccc} x_1\\ \vdots\\ x_n \\ \end{array} \right] = \sum^n_{j=1} a_{ij} x_j$$ in the old interpretation I had of what a column of a matrix is (this time the matrix is 1 by m), it seems that the columns $a_{i,j}$ specifies how much some basis vector $e_i$ is transformed. However, I've had difficulties understanding beyond that what the significance of the dot product of $x$ with the rows of $A$ means. Does someone know how to interpret this or how to understand it at a conceptual level, similar to the way the interpretation I gave of what the columns of a matrix mean? Are we doing some transformation to the row space of $A$ or something like that?","I was trying to understand where it came from that each row in a matrix multiplication is a dot product, as in: $$ Ax = \left(  \begin{array}{ccc} a_{1}^T \\ \vdots \\ a_m^T  \end{array}  \right)x = \left(  \begin{array}{ccc} a_{1}^Tx \\ \vdots \\ a_m^T x  \end{array}  \right) $$ what is an intuitive explanation or interpretation that each row is a dot product of the vector x? What I do understand is that $Ax$ encodes a linear transformation $T$. Consider a super simple example in 2 dimensions to explain what I do understand. I understand that $Ax = A [x_1 x_2] = T(v) = T(x_1 \hat i + x_2 \hat j) = x_1 T(\hat i) + x_2 T( \hat j)$. This makes me interpret intuitively that a multiplication by a matrix gives me a new vector that is composed of the same linear combination of the transformed basis vectors (or whatever vectors v is composed of) [source] . Furthermore one can easily see from this view where the multiplication of a matrix comes from: $$Ax = \left[  \begin{array}{ccc} a_{11} & a_{12} \\ a_{21} & a_{22} \\   \end{array}  \right]  x = \left[  \begin{array}{ccc} T(\hat i)_1 & T(\hat j)_1\\ T(\hat i)_2 & T(\hat j)_2\\ \end{array} \right] \left[ \begin{array}{ccc} x_{1} \\ x_{2} \\   \end{array}  \right] =  x_1\left[  \begin{array}{ccc} T(\hat i)_1 \\ T(\hat i)_2 \\ \end{array} \right] +  x_2 \left[ \begin{array}{ccc} T(\hat j)_1\\ T(\hat j)_2\\ \end{array} \right] = \left[  \begin{array}{ccc} T(\hat i)_1 x_1 + T(\hat j)_1 x_2\\ T(\hat i)_2x_2 + T(\hat j)_2 x_2\\ \end{array} \right] $$ where now its obvious why matrix multiplication is defined the way it is (because of linear transformations). Notice that the nice thing about this view is that one can interpret that each column of the matrix tells us how each basis vector changes . i.e. each column specifies how $\hat i$, $\hat j$ are transformed. Furthermore, the amount it used to be in the old vector is retained but now its in the new direction $T(\hat i)$ for the first coordinate. This for me is really intuitive and explains a lot of where matrix multiplication comes from. However, if you notice this view reveals that each row $(Ax)_i = a_1^T x$ is a dot product of the initial array representation of the vector. This seems to me to not be a coincidence and that something deeper has to be going on. Usually dot products are related with projections so I was trying to understand if each coordinate of $(Ax)_i$ might actually be encoding how much the original $x$ is being projected into each row vector of $A$ (or possible something to do with the row space of $A$ i.e. $C(A^T)$ ). In an attempt to understand this I considered what each row means: $$ \left[ a_{i,1} \dots a_{i,m} \right] \left[ \begin{array}{ccc} x_1\\ \vdots\\ x_n \\ \end{array} \right] = \sum^n_{j=1} a_{ij} x_j$$ in the old interpretation I had of what a column of a matrix is (this time the matrix is 1 by m), it seems that the columns $a_{i,j}$ specifies how much some basis vector $e_i$ is transformed. However, I've had difficulties understanding beyond that what the significance of the dot product of $x$ with the rows of $A$ means. Does someone know how to interpret this or how to understand it at a conceptual level, similar to the way the interpretation I gave of what the columns of a matrix mean? Are we doing some transformation to the row space of $A$ or something like that?",,"['linear-algebra', 'linear-transformations', 'intuition']"
97,Largest $n\times m$ matrix with no $n\times (n+1)$ submatrix having $n$-wise independent columns,Largest  matrix with no  submatrix having -wise independent columns,n\times m n\times (n+1) n,"What is the largest $m$ for which an $n \times m$ matrix with the given property that any subset of $n+1$ columns have rank $n$, may contain no subset of $n+1$ columns in which the columns are $n$-wise independent? In general one can see that the assumptions imply that $m \leq 2n$, but I feel it should be possible to determine $m$ exactly. If $n = 3$ I can verify by hand that the maximum $m$ is $5$. What should it be in general? Edit: As some people have pointed to, the question can be generalized to matroids. However in the interest of keeping things concrete, I am willing at this point to accept any answer that exhibits a matrix with entries over any field that improves the general bound $n+2 \leq m \leq 2n$. If the example of Heptagon can be extended to arbitrary $n$, this would for instance show that $2n-1 \leq m \leq 2n$ over the set of realizable matroids.","What is the largest $m$ for which an $n \times m$ matrix with the given property that any subset of $n+1$ columns have rank $n$, may contain no subset of $n+1$ columns in which the columns are $n$-wise independent? In general one can see that the assumptions imply that $m \leq 2n$, but I feel it should be possible to determine $m$ exactly. If $n = 3$ I can verify by hand that the maximum $m$ is $5$. What should it be in general? Edit: As some people have pointed to, the question can be generalized to matroids. However in the interest of keeping things concrete, I am willing at this point to accept any answer that exhibits a matrix with entries over any field that improves the general bound $n+2 \leq m \leq 2n$. If the example of Heptagon can be extended to arbitrary $n$, this would for instance show that $2n-1 \leq m \leq 2n$ over the set of realizable matroids.",,"['linear-algebra', 'combinatorics']"
98,A criteria for Jordan decomposition of a matrix over a general ring,A criteria for Jordan decomposition of a matrix over a general ring,,"When I look at different proofs of the Jordan-Chevalley decomposition of a matrix, the minimal hypothesis I usually found is about the perfection of the field over which such decomposition occurs (e.g. Wikipedia article). But it seems to me that the gist of the proofs is not about the field but about the type of characteristic polynomial of the matrix. More precisely, by using the two following definitions for a general ring $A$ Definition 1 A $n\times n$ -matrix $M$ with coefficients in the ring A is semisimple if $A^n$ is a semi-simple $A[M]$ -module (here I am using the definition provided in the answer of @ಠ_ಠ below) Definition 2 A separable polynomial $P$ of $A[X]$ is a polynomial whose discriminant is invertible in $A$ Then is the following statement true? A matrix $M$ over $A$ have a unique Jordan-Chevalley decomposition if its characteristic polynomial divides a power of a separable polynomial. Edit By Jordan decomposition I mean here the additive one, i.e. there exist $M_s$ and $M_n$ , two $n\times n$ $A$ -matrices respectively semi-simple and nilpotent, such that $$M=M_s+M_n$$ If it is not, what kind of general property should the ring $A$ have to make it true (like $A$ is a domain, etc.) Edit 2 I believe my definition 1 is equivalent in a domain to the fact that the matrix is diagonalizable in an algebraic extension of the fraction field which is equivalent to the fact that the matrix is cancelled by a separable polynomial (definition 2) in $A[X]$","When I look at different proofs of the Jordan-Chevalley decomposition of a matrix, the minimal hypothesis I usually found is about the perfection of the field over which such decomposition occurs (e.g. Wikipedia article). But it seems to me that the gist of the proofs is not about the field but about the type of characteristic polynomial of the matrix. More precisely, by using the two following definitions for a general ring Definition 1 A -matrix with coefficients in the ring A is semisimple if is a semi-simple -module (here I am using the definition provided in the answer of @ಠ_ಠ below) Definition 2 A separable polynomial of is a polynomial whose discriminant is invertible in Then is the following statement true? A matrix over have a unique Jordan-Chevalley decomposition if its characteristic polynomial divides a power of a separable polynomial. Edit By Jordan decomposition I mean here the additive one, i.e. there exist and , two -matrices respectively semi-simple and nilpotent, such that If it is not, what kind of general property should the ring have to make it true (like is a domain, etc.) Edit 2 I believe my definition 1 is equivalent in a domain to the fact that the matrix is diagonalizable in an algebraic extension of the fraction field which is equivalent to the fact that the matrix is cancelled by a separable polynomial (definition 2) in",A n\times n M A^n A[M] P A[X] A M A M_s M_n n\times n A M=M_s+M_n A A A[X],"['linear-algebra', 'commutative-algebra', 'matrix-decomposition']"
99,mortality problem,mortality problem,,"The mortality problem is the question if some product of a given set of matrices  yields the 0-matrix. In general, the mortality problem is undecidable. To have a  feeling for the difficulty of the problem, I only considered two 3x3-matrices with  entries -1..1. The hardest problem I found so far is $$ A = \begin{bmatrix} 0 & 1 & -1\\ 0 & 1 & -1\\ 1 & 0 & 1 \end{bmatrix} $$ $$ B = \begin{bmatrix} -1 & 0 & 1\\ -1 & -1 & 0\\ -1 & 1 & 1 \end{bmatrix} $$ Solution : [0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0] 0 stands for A, 1 for B, so $AB^2A^3B^4A^2BAB^2A=0$. There is no shorter  product, the optimal length is 17. An obvious condition for the matrices A and B is $\det(A)\det(B)=0$ Now my questions Is the mortality problem decidable for two 3x3-matrices ? What is the maximal possible length of an optimal solution depending on the size of the entries ? How can it be proven that the general mortality problem is undecidacle, and where are the limits for decidability ? My new record is : 18 [1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1] a $$ \begin{bmatrix} 1 & -1 & 0\\ -1 & 0 & -1\\ 0 & 0 & 1 \end{bmatrix} $$ b $$ \begin{bmatrix} 1 & -1 & -1\\ 1 & 0 & 0\\ -1 & -1 & -1 \end{bmatrix} $$","The mortality problem is the question if some product of a given set of matrices  yields the 0-matrix. In general, the mortality problem is undecidable. To have a  feeling for the difficulty of the problem, I only considered two 3x3-matrices with  entries -1..1. The hardest problem I found so far is $$ A = \begin{bmatrix} 0 & 1 & -1\\ 0 & 1 & -1\\ 1 & 0 & 1 \end{bmatrix} $$ $$ B = \begin{bmatrix} -1 & 0 & 1\\ -1 & -1 & 0\\ -1 & 1 & 1 \end{bmatrix} $$ Solution : [0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0] 0 stands for A, 1 for B, so $AB^2A^3B^4A^2BAB^2A=0$. There is no shorter  product, the optimal length is 17. An obvious condition for the matrices A and B is $\det(A)\det(B)=0$ Now my questions Is the mortality problem decidable for two 3x3-matrices ? What is the maximal possible length of an optimal solution depending on the size of the entries ? How can it be proven that the general mortality problem is undecidacle, and where are the limits for decidability ? My new record is : 18 [1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1] a $$ \begin{bmatrix} 1 & -1 & 0\\ -1 & 0 & -1\\ 0 & 0 & 1 \end{bmatrix} $$ b $$ \begin{bmatrix} 1 & -1 & -1\\ 1 & 0 & 0\\ -1 & -1 & -1 \end{bmatrix} $$",,"['linear-algebra', 'matrices']"
