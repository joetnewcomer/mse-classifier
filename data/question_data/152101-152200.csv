,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Laurent Series of $\frac{1}{(z-2)(z+1)}$,Laurent Series of,\frac{1}{(z-2)(z+1)},$$\frac{1}{(z-2)(z+1)}=\frac{A}{z-2}+\frac{B}{z+1}$$ \begin{cases}A+B=0 \\ A-2B=1 \end{cases} $$A=\frac{1}{3} \\ B=-\frac{1}{3}$$ $$\frac{1}{(z-2)(z+1)}=\frac{\frac{1}{3}}{z-2}+\frac{-\frac{1}{3}}{z+1}$$ Laurent Series $$\rvert z \rvert <1$$ $$\frac{1}{(z-2)(z+1)}=-\frac{1}{6} \ \frac{1}{-\frac{z}{2}+1}-\frac{1}{3} \ \frac{1}{1-(-z)}= \\=-\frac{1}{6} \sum_{n=0}^{+\infty} (\frac{z}{2})^n-\frac{1}{3} \sum_{n=0}^{+\infty} (-z)^n  $$ $$1 < \rvert z \rvert <2$$ $$\frac{1}{(z-2)(z+1)}=-\frac{1}{6} \ \frac{1}{-\frac{z}{2}+1}-\frac{1}{3} \ \frac{1}{z(1-(-\frac{1}{z}) )}= \\ \\ = -\frac{1}{6} \sum_{n=0}^{+\infty} (\frac{z}{2})^n-\frac{1}{3} \sum_{n=0}^{+\infty} (-\frac{1}{z})^{n+1} $$ $$ \rvert z \rvert >2$$ $$\frac{1}{(z-2)(z+1)}=\frac{1}{3} \ \frac{1}{z(1-\frac{2}{z})}-\frac{1}{3} \ \frac{1}{z(1-(-\frac{1}{z}) )}= \\ \\ = \frac{1}{3} \sum_{n=0}^{+\infty} (\frac{2}{z})^{n+1}-\frac{1}{3} \sum_{n=0}^{+\infty} (-\frac{1}{z})^{n+1} $$ Is it correct? Thanks!,$$\frac{1}{(z-2)(z+1)}=\frac{A}{z-2}+\frac{B}{z+1}$$ \begin{cases}A+B=0 \\ A-2B=1 \end{cases} $$A=\frac{1}{3} \\ B=-\frac{1}{3}$$ $$\frac{1}{(z-2)(z+1)}=\frac{\frac{1}{3}}{z-2}+\frac{-\frac{1}{3}}{z+1}$$ Laurent Series $$\rvert z \rvert <1$$ $$\frac{1}{(z-2)(z+1)}=-\frac{1}{6} \ \frac{1}{-\frac{z}{2}+1}-\frac{1}{3} \ \frac{1}{1-(-z)}= \\=-\frac{1}{6} \sum_{n=0}^{+\infty} (\frac{z}{2})^n-\frac{1}{3} \sum_{n=0}^{+\infty} (-z)^n  $$ $$1 < \rvert z \rvert <2$$ $$\frac{1}{(z-2)(z+1)}=-\frac{1}{6} \ \frac{1}{-\frac{z}{2}+1}-\frac{1}{3} \ \frac{1}{z(1-(-\frac{1}{z}) )}= \\ \\ = -\frac{1}{6} \sum_{n=0}^{+\infty} (\frac{z}{2})^n-\frac{1}{3} \sum_{n=0}^{+\infty} (-\frac{1}{z})^{n+1} $$ $$ \rvert z \rvert >2$$ $$\frac{1}{(z-2)(z+1)}=\frac{1}{3} \ \frac{1}{z(1-\frac{2}{z})}-\frac{1}{3} \ \frac{1}{z(1-(-\frac{1}{z}) )}= \\ \\ = \frac{1}{3} \sum_{n=0}^{+\infty} (\frac{2}{z})^{n+1}-\frac{1}{3} \sum_{n=0}^{+\infty} (-\frac{1}{z})^{n+1} $$ Is it correct? Thanks!,,"['sequences-and-series', 'complex-analysis', 'analysis', 'laurent-series']"
1,Function in a Fractional Sobolev Space,Function in a Fractional Sobolev Space,,"Suppose I have $f \in W^{s, p}(B_R)$ for some ball $B_R\subset \mathbb{R}^d$, and for some $s > 0$ and $p > 1$. Is it true that the function ${\rm sgn}(x_1)f$ belongs to some $W^{s', p'}(B_R)$ for some $s' > 0$ and $p' > 1$? (note that ${\rm sgn}(x_1)$ represents the sign function, a one dimensional step function). I already know that the result is true if $f \in W^{s, p}(\mathbb{R}^d)$ and $g \in W^{1, 1}(\mathbb{R}^d)\cap L^\infty(\mathbb{R}^d)$, but I am not sure about this local case.","Suppose I have $f \in W^{s, p}(B_R)$ for some ball $B_R\subset \mathbb{R}^d$, and for some $s > 0$ and $p > 1$. Is it true that the function ${\rm sgn}(x_1)f$ belongs to some $W^{s', p'}(B_R)$ for some $s' > 0$ and $p' > 1$? (note that ${\rm sgn}(x_1)$ represents the sign function, a one dimensional step function). I already know that the result is true if $f \in W^{s, p}(\mathbb{R}^d)$ and $g \in W^{1, 1}(\mathbb{R}^d)\cap L^\infty(\mathbb{R}^d)$, but I am not sure about this local case.",,"['analysis', 'sobolev-spaces']"
2,Equivalence of Rudin's Definition 4.33 (infinite limits and limits at infinity)?,Equivalence of Rudin's Definition 4.33 (infinite limits and limits at infinity)?,,"In Rudin's ""Principles of Mathematical Analysis"", Definition 4.33 (p98) states: Definition 4.33 $\quad$ Let $f$ be a real function defined on $E.$  We say that $f(t)\to A$ as $t\to x$, where $A$ and $x$ are in the extended real number system, if for every neighborhood $U$ of $A$ there is a neighborhood $V$ of $x$ such that $V \cap E$ is not empty, and such that $f(t)\in U$ for all $t\in V\cap E, t\ne x.$ I'm a little confused however by the remark right beneath this definition: A moment's consideration will show that this coincides with Definition 4.1 when $A$ and $x$ are real. My question is actually three-fold: 1) In Definition 4.1 (quoted below), it is explicitly stated that $x$ ($p$) has to be a limit point of $E$.  Can this be inferred from Definition 4.33?  It appears to me that according to Definition 4.33 the limit of a function can also be defined even if $x$ is an isolated point of $E.$  For example, let $E=\{0\}\cup \{1\} \cup [3,4]$, and $f(0)=f(1)=0, f(t)=t, t\in [3,4]$.  Then with $V$ being a neighborhood of radius $2$ around $0$, we have $f(t)\to 0$, as $t\to 0,$ don't we? 2) Similarly, for limits at infinity, i.e. $x=\infty$, Definition 4.33 doesn't require the domain $E$ of $f$ to be unbounded, does it?  For example, if $f(x)=1$ for $x\in[0,1]$, then we may also say $f(x)\to 1$ as $x\to \infty$? (e.g. with $V=(0.5, \infty)$ we clearly satisfy Definition 4.33.) 3) Should we or should we not add these assumptions (or implicitly assume them) when using Definition 4.33?  (i.e. $x$ has to be limit point of $E$, or $E$ is unbounded above or below.) Thanks a lot! Definition 4.1 $\quad$ Let $X$ and $Y$ be metric space; suppose $E \subset X$, $f$ maps $E$ into $Y$, and $p$ is a limit point of $E$.  We write $f(x)\to q$ as $x\to p$, or $\lim_{x\to p}f(x)=q$ if there is a point $q \in Y$ with the following property: For every $\epsilon > 0$ there exists a $\delta > 0$ such that $d_Y(f(x),q)<\epsilon$ for all points $x\in E$ for which $0<d_X(x,p)<\delta$.","In Rudin's ""Principles of Mathematical Analysis"", Definition 4.33 (p98) states: Definition 4.33 $\quad$ Let $f$ be a real function defined on $E.$  We say that $f(t)\to A$ as $t\to x$, where $A$ and $x$ are in the extended real number system, if for every neighborhood $U$ of $A$ there is a neighborhood $V$ of $x$ such that $V \cap E$ is not empty, and such that $f(t)\in U$ for all $t\in V\cap E, t\ne x.$ I'm a little confused however by the remark right beneath this definition: A moment's consideration will show that this coincides with Definition 4.1 when $A$ and $x$ are real. My question is actually three-fold: 1) In Definition 4.1 (quoted below), it is explicitly stated that $x$ ($p$) has to be a limit point of $E$.  Can this be inferred from Definition 4.33?  It appears to me that according to Definition 4.33 the limit of a function can also be defined even if $x$ is an isolated point of $E.$  For example, let $E=\{0\}\cup \{1\} \cup [3,4]$, and $f(0)=f(1)=0, f(t)=t, t\in [3,4]$.  Then with $V$ being a neighborhood of radius $2$ around $0$, we have $f(t)\to 0$, as $t\to 0,$ don't we? 2) Similarly, for limits at infinity, i.e. $x=\infty$, Definition 4.33 doesn't require the domain $E$ of $f$ to be unbounded, does it?  For example, if $f(x)=1$ for $x\in[0,1]$, then we may also say $f(x)\to 1$ as $x\to \infty$? (e.g. with $V=(0.5, \infty)$ we clearly satisfy Definition 4.33.) 3) Should we or should we not add these assumptions (or implicitly assume them) when using Definition 4.33?  (i.e. $x$ has to be limit point of $E$, or $E$ is unbounded above or below.) Thanks a lot! Definition 4.1 $\quad$ Let $X$ and $Y$ be metric space; suppose $E \subset X$, $f$ maps $E$ into $Y$, and $p$ is a limit point of $E$.  We write $f(x)\to q$ as $x\to p$, or $\lim_{x\to p}f(x)=q$ if there is a point $q \in Y$ with the following property: For every $\epsilon > 0$ there exists a $\delta > 0$ such that $d_Y(f(x),q)<\epsilon$ for all points $x\in E$ for which $0<d_X(x,p)<\delta$.",,"['real-analysis', 'analysis']"
3,Show that $\prod\limits_{i=1}^\infty\left(1-\frac{\alpha}{i}\right)=0$ if $\alpha > 0$,Show that  if,\prod\limits_{i=1}^\infty\left(1-\frac{\alpha}{i}\right)=0 \alpha > 0,Show that $\prod\limits_{i=1}^\infty\left(1-\frac{\alpha}{i}\right)=0$ if $\alpha > 0$. Hint: Look at the logarithm of the absolute value of the product. My attempt If $\alpha\in\mathbb{N}$ then $\exists i=\alpha$ so one factor will equal $0$ and thus the desired result will be attained. I now consider the case when $\alpha\notin\mathbb{N}$.  I don't know how to go about this so I attempt to use the hint. $$\prod_{i=1}^\infty\left\vert1-\frac{\alpha}{i}\right\vert=\operatorname{exp}\left(\ln\left(\prod_{i=1}^\infty\left\vert1-\frac{\alpha}{i}\right\vert\right)\right) = \operatorname{exp}\left(\sum_{i=1}^\infty\ln\left(\left\vert1-\frac{\alpha}{i}\right\vert\right)\right)$$ I don't know where to go from there. I assume that I am to show that the series diverges to $-\infty$. I have briefly had a look at series but in the book I'm currently studying I haven't gotten to the series part yet so I don't think the author expects me to use a bunch of fancy methods to deduce the divergence of the series.,Show that $\prod\limits_{i=1}^\infty\left(1-\frac{\alpha}{i}\right)=0$ if $\alpha > 0$. Hint: Look at the logarithm of the absolute value of the product. My attempt If $\alpha\in\mathbb{N}$ then $\exists i=\alpha$ so one factor will equal $0$ and thus the desired result will be attained. I now consider the case when $\alpha\notin\mathbb{N}$.  I don't know how to go about this so I attempt to use the hint. $$\prod_{i=1}^\infty\left\vert1-\frac{\alpha}{i}\right\vert=\operatorname{exp}\left(\ln\left(\prod_{i=1}^\infty\left\vert1-\frac{\alpha}{i}\right\vert\right)\right) = \operatorname{exp}\left(\sum_{i=1}^\infty\ln\left(\left\vert1-\frac{\alpha}{i}\right\vert\right)\right)$$ I don't know where to go from there. I assume that I am to show that the series diverges to $-\infty$. I have briefly had a look at series but in the book I'm currently studying I haven't gotten to the series part yet so I don't think the author expects me to use a bunch of fancy methods to deduce the divergence of the series.,,"['calculus', 'real-analysis', 'analysis', 'limits', 'infinite-product']"
4,Line integrals along space filling curves,Line integrals along space filling curves,,"Let ${C_n}$ be a family of curves (on the unit square $M$) such that $C_\infty$ is a space filling curve and $f(x,y)$ a function of two variables. Does the following identity hold? $$lim_{n\to\infty}\int_{C_n}f(x,y)\,ds=\iint_Mf(x,y)\,dx\,dy$$ Can it even be computed? The above identity is surely incorrect but the idea is that if we interpret the line integral {double integral} as the area {volume} between the curve {unit square} and the function ant take the limit, when $C_n$ ""approaches"" the area of the unit square, the line integral should approach the volume  over it Thanks in advance","Let ${C_n}$ be a family of curves (on the unit square $M$) such that $C_\infty$ is a space filling curve and $f(x,y)$ a function of two variables. Does the following identity hold? $$lim_{n\to\infty}\int_{C_n}f(x,y)\,ds=\iint_Mf(x,y)\,dx\,dy$$ Can it even be computed? The above identity is surely incorrect but the idea is that if we interpret the line integral {double integral} as the area {volume} between the curve {unit square} and the function ant take the limit, when $C_n$ ""approaches"" the area of the unit square, the line integral should approach the volume  over it Thanks in advance",,"['calculus', 'integration', 'analysis', 'curves']"
5,What is the geometric difference between Gamma regularization and convex conjugate?,What is the geometric difference between Gamma regularization and convex conjugate?,,"I am studying convex analysis and of course, one needs the Fenchel conjugate (or polar function) and also the $\Gamma$-regularization of a function (I'm going to write it as $F^\Gamma$). In this question, $V$ is a topological vector space, $V'$ its dual (topological dual). If $x^*\in V'$, I denote $x^*(x)$ by $\left\langle x,x^*\right\rangle$. In order to understand my question, I attach an image of the book Nonsmooth analysis written by Winfried Schirotzek : $\Gamma$ regularization of $F$ in two points: Let $z_0$ be the point of the image given by the intersection of $F$ and $a_2$. That is, $z_0=F^\Gamma \left(x_0\right)$ for some $x_0$. My question is , I think that $$\exists x^{*}\in V':\ F^{*}\left(x^{*}\right)=a_{2}\left(x_{0}\right)=z_{0}\,.$$ In other words, If you have $F^\Gamma \left(x_0\right)$ then you have $F^*$ since $F^{\Gamma}\left(x_{0}\right)=a_2\left(x_{0}\right)$ and thus, you have the affine function $a_2$. I.e., both functions return the same result, $z_0$, but under different points of view. Is this correct? If it is, what is the difference between $F^*$ and $F^{**}$ which is always $F^\Gamma$? Otherwise, how can I understand the geometric difference between $F^*$ and $F^\Gamma$? Thank you so much in advance! Definitions: The definition of the polar function is: Let $F:V\to \overline{\mathbb{R}}$. Then, the functional, $F^{*}:V'\to \overline{\mathbb{R}}$ is defined by,   $$F^*\left(x^*\right)={\displaystyle \sup_{x\in V}\left\{ \left\langle x,x^*\right\rangle -F\left(x\right)\right\} }.$$ On the other hand, the definition of the $F^\Gamma$ is: Let    $$\mathcal{A}\left(F\right)=\sup\left\{ a:E\to\mathbb{R\ }|\ a\text{ is continuous and affine},\ a\leq F\right\}$$   So, the functional $F^\Gamma$ defined by,   $$F^{\Gamma}\left(x\right)=\sup\left\{ a\left(x\right)\ |\ a\in\mathcal{A}\right\}$$   is called the $\Gamma$ regularization of $F$.","I am studying convex analysis and of course, one needs the Fenchel conjugate (or polar function) and also the $\Gamma$-regularization of a function (I'm going to write it as $F^\Gamma$). In this question, $V$ is a topological vector space, $V'$ its dual (topological dual). If $x^*\in V'$, I denote $x^*(x)$ by $\left\langle x,x^*\right\rangle$. In order to understand my question, I attach an image of the book Nonsmooth analysis written by Winfried Schirotzek : $\Gamma$ regularization of $F$ in two points: Let $z_0$ be the point of the image given by the intersection of $F$ and $a_2$. That is, $z_0=F^\Gamma \left(x_0\right)$ for some $x_0$. My question is , I think that $$\exists x^{*}\in V':\ F^{*}\left(x^{*}\right)=a_{2}\left(x_{0}\right)=z_{0}\,.$$ In other words, If you have $F^\Gamma \left(x_0\right)$ then you have $F^*$ since $F^{\Gamma}\left(x_{0}\right)=a_2\left(x_{0}\right)$ and thus, you have the affine function $a_2$. I.e., both functions return the same result, $z_0$, but under different points of view. Is this correct? If it is, what is the difference between $F^*$ and $F^{**}$ which is always $F^\Gamma$? Otherwise, how can I understand the geometric difference between $F^*$ and $F^\Gamma$? Thank you so much in advance! Definitions: The definition of the polar function is: Let $F:V\to \overline{\mathbb{R}}$. Then, the functional, $F^{*}:V'\to \overline{\mathbb{R}}$ is defined by,   $$F^*\left(x^*\right)={\displaystyle \sup_{x\in V}\left\{ \left\langle x,x^*\right\rangle -F\left(x\right)\right\} }.$$ On the other hand, the definition of the $F^\Gamma$ is: Let    $$\mathcal{A}\left(F\right)=\sup\left\{ a:E\to\mathbb{R\ }|\ a\text{ is continuous and affine},\ a\leq F\right\}$$   So, the functional $F^\Gamma$ defined by,   $$F^{\Gamma}\left(x\right)=\sup\left\{ a\left(x\right)\ |\ a\in\mathcal{A}\right\}$$   is called the $\Gamma$ regularization of $F$.",,"['functional-analysis', 'analysis', 'convex-analysis', 'convex-optimization']"
6,What are appropriate techniques to aggregate rankings from experts?,What are appropriate techniques to aggregate rankings from experts?,,"I am not sure if this would be considered a programming or a math or another problem.  I've come across a situation in my day to day work and am looking for an optimal solution. If you have a list of rankings from experts (let's say players in a gaming league).  Not every list has the same players (but there's some common overlap) and not every list is the same length.  What would be an appropriate technique to join the lists to a single one that reflect the generate group consensus? Small example: List 1: 1. A 2. B 3. C List 2: 1. A, 2. C, 3, B, 4. D The final list may look something like: 1. A. 2. B. 3. C. 4. D A few solutions I've looked at are: Elo or Glicko - using the matches (from the lists above as as an example): A beats B, A beats C, B beats C, A beats C, A beats B, A beats D, C beats B, C beats D, B beats D. Swarm Analysis / Particle Swarm Optimization - find a weighting that minimizes discrepancies between master list and each expert's opinion. Simple weighted average - problems run with not all lists are same length nor do all lists include all players Ranked Average Voting (such as in STV voting systems).. What are others, or what would be the best technique for this problem?  By best I mean what produces a final list that best reflects group consensus?","I am not sure if this would be considered a programming or a math or another problem.  I've come across a situation in my day to day work and am looking for an optimal solution. If you have a list of rankings from experts (let's say players in a gaming league).  Not every list has the same players (but there's some common overlap) and not every list is the same length.  What would be an appropriate technique to join the lists to a single one that reflect the generate group consensus? Small example: List 1: 1. A 2. B 3. C List 2: 1. A, 2. C, 3, B, 4. D The final list may look something like: 1. A. 2. B. 3. C. 4. D A few solutions I've looked at are: Elo or Glicko - using the matches (from the lists above as as an example): A beats B, A beats C, B beats C, A beats C, A beats B, A beats D, C beats B, C beats D, B beats D. Swarm Analysis / Particle Swarm Optimization - find a weighting that minimizes discrepancies between master list and each expert's opinion. Simple weighted average - problems run with not all lists are same length nor do all lists include all players Ranked Average Voting (such as in STV voting systems).. What are others, or what would be the best technique for this problem?  By best I mean what produces a final list that best reflects group consensus?",,['analysis']
7,"If $\mu $ is mutually singular with $\nu$ and absolutely continuous with respect to $\nu$, show that $\mu=0$","If  is mutually singular with  and absolutely continuous with respect to , show that",\mu  \nu \nu \mu=0,"Here is a problem I want to confirm my answer to. Let $\mu $ and $\nu$ be two non-negative measures on a $\sigma$ - algebra $M$ . If $\mu$ is mutually singular with $\nu$ and absolutely continuous with respect to $\nu$ , show that $\mu=0$ My proof: Since $\mu$ is mutually singular with $\nu$ , that implies the support of $\mu$ and the support of $\nu$ have an empty intersection. Now, if $\mu$ is not identically $0$ , there exists an $A \in M$ such that $\mu(A)>0.$ But then $\nu(A)>0$ as well since, if $\nu(A)=0$ , then $\mu(A)=0$ . But then $A$ is a set for which both measures are positive, i.e. $A$ is in both the support of $\mu$ and the support of $\nu$ . But that's a contradiction since support of $\mu$ and support of $\nu$ have empty intersection. Thanks!","Here is a problem I want to confirm my answer to. Let and be two non-negative measures on a - algebra . If is mutually singular with and absolutely continuous with respect to , show that My proof: Since is mutually singular with , that implies the support of and the support of have an empty intersection. Now, if is not identically , there exists an such that But then as well since, if , then . But then is a set for which both measures are positive, i.e. is in both the support of and the support of . But that's a contradiction since support of and support of have empty intersection. Thanks!",\mu  \nu \sigma M \mu \nu \nu \mu=0 \mu \nu \mu \nu \mu 0 A \in M \mu(A)>0. \nu(A)>0 \nu(A)=0 \mu(A)=0 A A \mu \nu \mu \nu,['analysis']
8,A question regarding integration over $\varnothing$,A question regarding integration over,\varnothing,Does $\int_\varnothing f(x)dx = 0$ make any sense? Is one allowed to use $\varnothing$ this way?,Does $\int_\varnothing f(x)dx = 0$ make any sense? Is one allowed to use $\varnothing$ this way?,,"['calculus', 'real-analysis', 'integration', 'analysis']"
9,Evaluating $\lim_{n \to \infty}\frac{1}{n}(n!)^{\frac{1}{n}}$ [duplicate],Evaluating  [duplicate],\lim_{n \to \infty}\frac{1}{n}(n!)^{\frac{1}{n}},"This question already has answers here : Finding the limit of $\frac {n}{\sqrt[n]{n!}}$ (11 answers) How to find $\lim _{ n\to \infty } \frac { ({ n!) }^{ 1\over n } }{ n } $? [duplicate] (3 answers) Closed 7 years ago . I'm trying to find and prove the value of$$\lim_{n \to \infty}\frac{1}{n}(n!)^{\frac{1}{n}}$$ I was thinking that since $$\frac{1}{n}(n!)^{\frac{1}{n}} = \frac{1}{n} \left[ (1)^{\frac{1}{n}}(2)^{\frac{1}{n}}...(n)^{\frac{1}{n}}  \right]$$ and we know that  $$\lim_{n\to \infty}n^{\frac{1}{n}} = 1$$ and  $$k^{\frac{1}{n}} \leq  \ n^{\frac{1}{n}} \ \ \ \ \forall k \leq n$$ So $(n!)^{\frac{1}{n}} \leq 1 \ \forall n \ $* Then it is bounded. We also know that $\lim \frac{1}{n} =0$, therefore  $$\lim_{n \to \infty}\frac{1}{n}(n!)^{\frac{1}{n}} = 0 **$$ I'm pretty sure this line of reasoning is ok. Now proving it is another thing. Any suggestions? *I realize now that this statement is not true, but that did not get me any closer to solving the problem. I do believe that it is bounded though. **LOL, I don't believe this to be true anymore either, a wild guess tells me that the solution may be $\frac{1}{e}$","This question already has answers here : Finding the limit of $\frac {n}{\sqrt[n]{n!}}$ (11 answers) How to find $\lim _{ n\to \infty } \frac { ({ n!) }^{ 1\over n } }{ n } $? [duplicate] (3 answers) Closed 7 years ago . I'm trying to find and prove the value of$$\lim_{n \to \infty}\frac{1}{n}(n!)^{\frac{1}{n}}$$ I was thinking that since $$\frac{1}{n}(n!)^{\frac{1}{n}} = \frac{1}{n} \left[ (1)^{\frac{1}{n}}(2)^{\frac{1}{n}}...(n)^{\frac{1}{n}}  \right]$$ and we know that  $$\lim_{n\to \infty}n^{\frac{1}{n}} = 1$$ and  $$k^{\frac{1}{n}} \leq  \ n^{\frac{1}{n}} \ \ \ \ \forall k \leq n$$ So $(n!)^{\frac{1}{n}} \leq 1 \ \forall n \ $* Then it is bounded. We also know that $\lim \frac{1}{n} =0$, therefore  $$\lim_{n \to \infty}\frac{1}{n}(n!)^{\frac{1}{n}} = 0 **$$ I'm pretty sure this line of reasoning is ok. Now proving it is another thing. Any suggestions? *I realize now that this statement is not true, but that did not get me any closer to solving the problem. I do believe that it is bounded though. **LOL, I don't believe this to be true anymore either, a wild guess tells me that the solution may be $\frac{1}{e}$",,['analysis']
10,Is the image f(A) of a n-Lebesgue measurable function m-Lebesgue measurable when f is Lipschitz?,Is the image f(A) of a n-Lebesgue measurable function m-Lebesgue measurable when f is Lipschitz?,,"Suppose $f:\mathbb{R}^n \rightarrow \mathbb{R}^m$ is Lipschitz, where $n\geq m$ . Let $\lambda_n$ and $\lambda_m$ denote the Lebesgue measure on $\mathbb{R}^n$ and $\mathbb{R}^m$ respectively. If $A\subset\mathbb{R}^n$ is $\lambda_n$ --measurable,  is $f(A)$ $\lambda_m$ --measurable? This statement appears in Evans-Gariepi's book on Measure Theory and fine properties of functions (Lemma 2 (i) in section 3.4.1) Using the machinery of Analytic sets I think the statement is true if $A$ is analytic, but without that machinery a prove for any $\lambda_n$ -measurable set that uses the fact that $f$ is Lipschitz escapes me at the moment. Just found out that the statement is actually incorrect. Evans-Gariepi already listed the statement above as errata and removed the statement in their second edition.","Suppose is Lipschitz, where . Let and denote the Lebesgue measure on and respectively. If is --measurable,  is --measurable? This statement appears in Evans-Gariepi's book on Measure Theory and fine properties of functions (Lemma 2 (i) in section 3.4.1) Using the machinery of Analytic sets I think the statement is true if is analytic, but without that machinery a prove for any -measurable set that uses the fact that is Lipschitz escapes me at the moment. Just found out that the statement is actually incorrect. Evans-Gariepi already listed the statement above as errata and removed the statement in their second edition.",f:\mathbb{R}^n \rightarrow \mathbb{R}^m n\geq m \lambda_n \lambda_m \mathbb{R}^n \mathbb{R}^m A\subset\mathbb{R}^n \lambda_n f(A) \lambda_m A \lambda_n f,['analysis']
11,Apparent (minor) error in Cauchy's article on pressure or tension in a solid body,Apparent (minor) error in Cauchy's article on pressure or tension in a solid body,,"In his article De la pression ou tension dans un corps solide [On the pressure or tension in a solid body] , Cauchy introduces a theory that allows to define Cauchy stress tensor .  It looks like he makes a mistake in his formula for the ""surface element"" in his surface integrals, unless I am missing something.  Can someone check and confirm this, please?  I didn't find any errata or comments about this apparent error. The ""surface element"" in his surface integrals (in Cartesian coordinates) is written as $\cos\gamma\,dy\,dx$, where $\gamma$ is the angle between the normal vector to the surface and the positive direction of the $z$-axis.  On page 62 , formula (4), he seems to claim that the surface area is given by the integral $$ \int\!\!\!\int\cos\gamma\,dy\,dx = s. $$ Of course this is not true: the surface area is given by the integral $$ \int\!\!\!\int\frac{dy\,dx}{\cos\gamma} = s, $$ and the ""surface element"" is $\frac{dy\,dx}{\cos\gamma}$. Since one should be free to use any symbol for the ""surface element"" as long as one does not try to interpret it, such an error would normally not lead to any other errors, so it seems plausible that it might pass unnoticed...","In his article De la pression ou tension dans un corps solide [On the pressure or tension in a solid body] , Cauchy introduces a theory that allows to define Cauchy stress tensor .  It looks like he makes a mistake in his formula for the ""surface element"" in his surface integrals, unless I am missing something.  Can someone check and confirm this, please?  I didn't find any errata or comments about this apparent error. The ""surface element"" in his surface integrals (in Cartesian coordinates) is written as $\cos\gamma\,dy\,dx$, where $\gamma$ is the angle between the normal vector to the surface and the positive direction of the $z$-axis.  On page 62 , formula (4), he seems to claim that the surface area is given by the integral $$ \int\!\!\!\int\cos\gamma\,dy\,dx = s. $$ Of course this is not true: the surface area is given by the integral $$ \int\!\!\!\int\frac{dy\,dx}{\cos\gamma} = s, $$ and the ""surface element"" is $\frac{dy\,dx}{\cos\gamma}$. Since one should be free to use any symbol for the ""surface element"" as long as one does not try to interpret it, such an error would normally not lead to any other errors, so it seems plausible that it might pass unnoticed...",,"['analysis', 'math-history', 'tensors', 'surface-integrals']"
12,Hadamard's inequality - alternative proof(the use of Lagrange multiplier method),Hadamard's inequality - alternative proof(the use of Lagrange multiplier method),,"Heard that Hadamard's inequality: $$\left|\det(A)\right| \leq {\prod_{i=1}^{n}\sqrt{\sum_{j=1}^{n} |a_{ij}|^2} } $$ can be proved by the use of Lagrange multiplier methods. I saw and understand the proof by the use of methods from linear algebra. However, I do not see how to apply such tool from mathematical analysis to detive the aforementioned proof. If anyone would be able to provide proof, which uses Lagrange multiplier method, I would be very thankful!","Heard that Hadamard's inequality: $$\left|\det(A)\right| \leq {\prod_{i=1}^{n}\sqrt{\sum_{j=1}^{n} |a_{ij}|^2} } $$ can be proved by the use of Lagrange multiplier methods. I saw and understand the proof by the use of methods from linear algebra. However, I do not see how to apply such tool from mathematical analysis to detive the aforementioned proof. If anyone would be able to provide proof, which uses Lagrange multiplier method, I would be very thankful!",,"['real-analysis', 'analysis']"
13,"How to show that $\int [Y(x)\frac{dZ(x)}{dx} + Z(x)\frac{dY(x)}{dx}]\, dx = Y(x)Z(x) + C$?",How to show that ?,"\int [Y(x)\frac{dZ(x)}{dx} + Z(x)\frac{dY(x)}{dx}]\, dx = Y(x)Z(x) + C","How can I show that  $$\int\left[Y(x)\frac{dZ(x)}{dx} + Z(x)\frac{dY(x)}{dx}\right]\, dx = Y(x)Z(x) + C$$  when it is known that  $$\frac{d}{dt}[f(x)] = \frac{df}{dx}\cdot\frac{dx}{dt} \tag{1.}$$ and  $$\frac{df(x)}{dx} = \left[\frac{dx(f)}{df}\right]^{-1} \tag{2.}$$  I have (of course) divided the two expressions like below to individually solve for the two integrals, but I can't continue from that (supposing that my first step is correct) $$\int\left[Y(x)\frac{dZ(x)}{dx}\right]dx + \int\left[Z(x)\frac{dY(x)}{dx}\right] \, dx$$ This problem is the problem number 12. from the first Project PHYSNET module.","How can I show that  $$\int\left[Y(x)\frac{dZ(x)}{dx} + Z(x)\frac{dY(x)}{dx}\right]\, dx = Y(x)Z(x) + C$$  when it is known that  $$\frac{d}{dt}[f(x)] = \frac{df}{dx}\cdot\frac{dx}{dt} \tag{1.}$$ and  $$\frac{df(x)}{dx} = \left[\frac{dx(f)}{df}\right]^{-1} \tag{2.}$$  I have (of course) divided the two expressions like below to individually solve for the two integrals, but I can't continue from that (supposing that my first step is correct) $$\int\left[Y(x)\frac{dZ(x)}{dx}\right]dx + \int\left[Z(x)\frac{dY(x)}{dx}\right] \, dx$$ This problem is the problem number 12. from the first Project PHYSNET module.",,"['calculus', 'integration', 'analysis']"
14,Use Duhamel principle to find inhomogenous solution to $u_t-u_{xx}=sin^2(x)$,Use Duhamel principle to find inhomogenous solution to,u_t-u_{xx}=sin^2(x),"Question as it appeared in the book: $$u_t-u_{xx}=sin^2(x) \quad 0<x<\pi, t>0$$ $$u(x,0)=\frac{\pi x}{2}\quad 0<x<\pi$$ $$u_x(0,t)=u_x(\pi,t)=0$$ The homogeneous solution of the above problem is given by $$u_h(x,t)=\sum_{n=0}^\infty C_n e^{-n^2t}cos(nx)$$ with $C_n=-2/n^2$ for odd n and $C_n=0$ otherwise. Use Duhamel principle to find the inhomogeneous solution. Theory stated in book: For: $$u_t-ku_{xx}=f(x,t) \quad 0<x<l, t>0$$ $$u(x,0)=g(x)\quad 0<x<l$$ $$u_x(0,t)=u_x(l,t)=0$$ Duhamel principle: $$u(x,t)= S(t)g(x)-\int_0^tS(t-s)f(x,s)ds=u_h(x,t)+w(x,t) $$ $$u_h(x,t)+w(x,t)=\sum_{n=0}^{\infty}A_nX_n(x)e^{-kt\lambda_nt}+\int_0^t\sum_{n=1}^{\infty}B_n(s)X_n(x)e^{-kt\lambda_n(t-s)}$$ Where: $$A_n=\frac{<X_n,g(x)>}{<X_n,X_n>}$$ $$B_n=\frac{<X_n,f(s)>}{<X_n,X_n>}$$ My attempt: Taking n is odd,$\lambda_n=n^2$ and $k=1$ $$u(x,t)= \sum_{n=0}^\infty \frac{-2}{n^2}e^{-n^2t}cos(nx) +\int_0^{t}\sum_{n=0}^\infty B_n(s)e^{-n^2(t-s)}cos(nx) $$ $$B_n(s)=\frac{<cos(nx),sin^2(x)>}{<cos(nx),cos(nx)>}$$ $$<cos(nx),cos(nx)>=\frac{\pi}{2}$$ as n is odd and n>0 and $cos(x)$ is repeating $cos(nx)=cos(x)$ $$<cos(nx),sin^2(x)>=\int_0^{\pi}cos(x)sin^2(x)dx=[\frac{sin^3(x)}{3}]_0^{\pi}=0-0=0$$ $$B_n=0$$ $$u(x,t)= \sum_{n=0}^\infty \frac{-2}{n^2}e^{-n^2t}cos(nx) $$ I don't think this can be right as I can't see how you can get $u(x,0)=\frac{\pi x}{2}$. Any help would be greatly appreciated thank you.","Question as it appeared in the book: $$u_t-u_{xx}=sin^2(x) \quad 0<x<\pi, t>0$$ $$u(x,0)=\frac{\pi x}{2}\quad 0<x<\pi$$ $$u_x(0,t)=u_x(\pi,t)=0$$ The homogeneous solution of the above problem is given by $$u_h(x,t)=\sum_{n=0}^\infty C_n e^{-n^2t}cos(nx)$$ with $C_n=-2/n^2$ for odd n and $C_n=0$ otherwise. Use Duhamel principle to find the inhomogeneous solution. Theory stated in book: For: $$u_t-ku_{xx}=f(x,t) \quad 0<x<l, t>0$$ $$u(x,0)=g(x)\quad 0<x<l$$ $$u_x(0,t)=u_x(l,t)=0$$ Duhamel principle: $$u(x,t)= S(t)g(x)-\int_0^tS(t-s)f(x,s)ds=u_h(x,t)+w(x,t) $$ $$u_h(x,t)+w(x,t)=\sum_{n=0}^{\infty}A_nX_n(x)e^{-kt\lambda_nt}+\int_0^t\sum_{n=1}^{\infty}B_n(s)X_n(x)e^{-kt\lambda_n(t-s)}$$ Where: $$A_n=\frac{<X_n,g(x)>}{<X_n,X_n>}$$ $$B_n=\frac{<X_n,f(s)>}{<X_n,X_n>}$$ My attempt: Taking n is odd,$\lambda_n=n^2$ and $k=1$ $$u(x,t)= \sum_{n=0}^\infty \frac{-2}{n^2}e^{-n^2t}cos(nx) +\int_0^{t}\sum_{n=0}^\infty B_n(s)e^{-n^2(t-s)}cos(nx) $$ $$B_n(s)=\frac{<cos(nx),sin^2(x)>}{<cos(nx),cos(nx)>}$$ $$<cos(nx),cos(nx)>=\frac{\pi}{2}$$ as n is odd and n>0 and $cos(x)$ is repeating $cos(nx)=cos(x)$ $$<cos(nx),sin^2(x)>=\int_0^{\pi}cos(x)sin^2(x)dx=[\frac{sin^3(x)}{3}]_0^{\pi}=0-0=0$$ $$B_n=0$$ $$u(x,t)= \sum_{n=0}^\infty \frac{-2}{n^2}e^{-n^2t}cos(nx) $$ I don't think this can be right as I can't see how you can get $u(x,0)=\frac{\pi x}{2}$. Any help would be greatly appreciated thank you.",,"['analysis', 'partial-differential-equations', 'heat-equation']"
15,Limits - generalizing formula,Limits - generalizing formula,,I'm confused with this: I need to generalize the formula $\lim_{n\to\infty }\frac{a_n}{b_n}$ = $ \frac{\lim_{n\to\infty} a_n}{\lim_{n\to\infty } b_n} $  so that it can be applied even in exceptional cases. I see that the exceptional case is when $b_n=0$. Shall I use the formula $\lim_{n\to\infty } (1+\frac {a}{n})^n=e^a$ ?,I'm confused with this: I need to generalize the formula $\lim_{n\to\infty }\frac{a_n}{b_n}$ = $ \frac{\lim_{n\to\infty} a_n}{\lim_{n\to\infty } b_n} $  so that it can be applied even in exceptional cases. I see that the exceptional case is when $b_n=0$. Shall I use the formula $\lim_{n\to\infty } (1+\frac {a}{n})^n=e^a$ ?,,"['analysis', 'limits']"
16,Relation between function integral and sum of function values.,Relation between function integral and sum of function values.,,"Let $f:\mathbb{R} \to \mathbb{R}$ be a continuous function with period $T$,where $T$ is an irrational number,and the integral of $f$ over one period is $0$. Can we conclude that $\sum_{k=0}^N f(k)$ is bounded?","Let $f:\mathbb{R} \to \mathbb{R}$ be a continuous function with period $T$,where $T$ is an irrational number,and the integral of $f$ over one period is $0$. Can we conclude that $\sum_{k=0}^N f(k)$ is bounded?",,"['calculus', 'analysis', 'functions', 'ergodic-theory']"
17,"completeness in $C^\infty [a,b]$ [closed]",completeness in  [closed],"C^\infty [a,b]","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Good day, how show that $C^\infty [a,b]=\bigcap_{k=1}^\infty C^k[a,b]$ is complete with norm  $$\sum_{n=0}^\infty \frac{1}{2^n}\frac{\| f^{(n)}-g^{(n)}\|_\infty}{1+\| f^{(n)}-g^{(n)}\|_\infty} \text{?}$$ Thanks.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Good day, how show that $C^\infty [a,b]=\bigcap_{k=1}^\infty C^k[a,b]$ is complete with norm  $$\sum_{n=0}^\infty \frac{1}{2^n}\frac{\| f^{(n)}-g^{(n)}\|_\infty}{1+\| f^{(n)}-g^{(n)}\|_\infty} \text{?}$$ Thanks.",,"['real-analysis', 'functional-analysis', 'analysis']"
18,Antenna adjustment: A real-life problem -- solving for a factor when only a change is known,Antenna adjustment: A real-life problem -- solving for a factor when only a change is known,,"I'm an amateur radio operator, and part of this involves making antennas. One of the simplest is a dipole, which requires two pieces of wire. The total length of these is given by the formula: Total length (in metres) = $143$ / Frequency (in MHz) You then divide this number by two (and usually add a bit for safety as it's easier to cut wire off the end than add more) and cut your wires. $143$ is a ""standard constant"". In the real world, this number varies due to a variety of factors and has to be measured by experimentation. Essentially by raising and lowering the antenna, cutting a bit off each time. It's really quite time-consuming. Here's the problem I'd like to solve: My measuring instruments are limited to a ruler and a metre stick. So I can measure long distances (metres) with poor accuracy, and short distances (up to $60$ cm) with good accuracy. In other words, if I've cut a wire to $14.2$m, it's probably $15$ or $14.7$ or something (but both wires will be equal because I've used one to cut the other). Conversely, if I've cut $15$cm off the end of a wire, I know that I've cut $15$cm off, and that it's accurate, because it's a single measurement. Here's the problem I'd like to solve: Let's say I have a magical instrument which tells me the frequency of my antenna (this is a real thing: it's called an antenna analyser). This tells my that my antenna works at $F1$. I don't know what the length truly is, so $L1$ is unknown. Now I cut a known length of wire off the end of each leg of my antenna (it has to be symmetrical). Let's say $15$cm. Now I have a new, higher, resonant frequency measurement, $F2$. Knowing $F1$, $F2$ and the change in length, is it possible to calculate the value of the ""constant"" $143$, to get a better idea what my next change in length should be? In actuality, the length starts out as (L + n), then changes to (L + n - c); where L = the 'wanted' length, n is an unknown offset, and c is the amount we cut off (which we know because we measured it accurately). My goal here is to go from time-consuming ""successive approximation"" to a method which allows the antenna to be raised and lowered an absolute minimum number of times. Is this possible? I've had a go with dividing the deltas between the lengths and frequencies, but the numbers I get don't make sense to me.","I'm an amateur radio operator, and part of this involves making antennas. One of the simplest is a dipole, which requires two pieces of wire. The total length of these is given by the formula: Total length (in metres) = $143$ / Frequency (in MHz) You then divide this number by two (and usually add a bit for safety as it's easier to cut wire off the end than add more) and cut your wires. $143$ is a ""standard constant"". In the real world, this number varies due to a variety of factors and has to be measured by experimentation. Essentially by raising and lowering the antenna, cutting a bit off each time. It's really quite time-consuming. Here's the problem I'd like to solve: My measuring instruments are limited to a ruler and a metre stick. So I can measure long distances (metres) with poor accuracy, and short distances (up to $60$ cm) with good accuracy. In other words, if I've cut a wire to $14.2$m, it's probably $15$ or $14.7$ or something (but both wires will be equal because I've used one to cut the other). Conversely, if I've cut $15$cm off the end of a wire, I know that I've cut $15$cm off, and that it's accurate, because it's a single measurement. Here's the problem I'd like to solve: Let's say I have a magical instrument which tells me the frequency of my antenna (this is a real thing: it's called an antenna analyser). This tells my that my antenna works at $F1$. I don't know what the length truly is, so $L1$ is unknown. Now I cut a known length of wire off the end of each leg of my antenna (it has to be symmetrical). Let's say $15$cm. Now I have a new, higher, resonant frequency measurement, $F2$. Knowing $F1$, $F2$ and the change in length, is it possible to calculate the value of the ""constant"" $143$, to get a better idea what my next change in length should be? In actuality, the length starts out as (L + n), then changes to (L + n - c); where L = the 'wanted' length, n is an unknown offset, and c is the amount we cut off (which we know because we measured it accurately). My goal here is to go from time-consuming ""successive approximation"" to a method which allows the antenna to be raised and lowered an absolute minimum number of times. Is this possible? I've had a go with dividing the deltas between the lengths and frequencies, but the numbers I get don't make sense to me.",,['analysis']
19,Do we use compact for $\textbf{sets}$ or for $\textbf{spaces}$?,Do we use compact for  or for ?,\textbf{sets} \textbf{spaces},"Before stating my questions, let me first state my understandings. Note this post is talking on metric spaces case. For the definition of a set $E$ being open in a metric space $(X,d)$, we mean that for every $a\in E$, there exists a ball $B(a,r)=\{x\in X|d(x,a)<r\}$ such that $B(a,r)\subseteq E$. Notice that the ball in the definition is with respect to $(X,d)$ (the ball centered at point $a$ and consist of all point in $X$ such that $d(x,a)<r$), it is quite important. Since every subset of a metric space is always a metric subspace; and every metric subspace is open in itself, by trivial reason. If we consider $E\subseteq X$ to be a metric space, then $E$ is always open in $(E,d)$. For example, $[0,1]$ is not open in $(\mathbb{R},d)$; however, $[0,1]$ is open in $([0,1],d)$. So what metric space the set $E$ is with respect to play an important role when talking whether the set $E$ is open. Now go to the definition of compactness , and also sequentially compact . It has two variations the definition would make: (For $\textbf{compactness}$) Let $(X,d)$ be a metric space, let $E\subseteq X$, if every open covering $\mathcal{A}$ of $E$ contains a finite subcollection that also covers $E$, then $E$ is called a $\textbf{compact set}$. Let $(X,d)$ be a metric space. If every open covering $\mathcal{A}$ of $X$ contains a finite subcollection that also covers $X$, then $X$ is called a $\textbf{compact metric space}$. (For $\textbf{sequentially compactess}$) Let $(X,d)$ be a metric space, let $E\subseteq X$, if every sequence of points of $E$ has a convergent subsequence, then $E$ is called a $\textbf{sequentially compact set}$. Let $(X,d)$ be a metric space. If every sequence of points of $X$ has a convergent subsequence, then $X$ is called a $\textbf{sequentially compact metric space}$. I don't know which is the regular definition people use. And also, I'm not sure whether this two versions of definition differs in some tricky circumstance, just like the ""open set"" mentioned above.","Before stating my questions, let me first state my understandings. Note this post is talking on metric spaces case. For the definition of a set $E$ being open in a metric space $(X,d)$, we mean that for every $a\in E$, there exists a ball $B(a,r)=\{x\in X|d(x,a)<r\}$ such that $B(a,r)\subseteq E$. Notice that the ball in the definition is with respect to $(X,d)$ (the ball centered at point $a$ and consist of all point in $X$ such that $d(x,a)<r$), it is quite important. Since every subset of a metric space is always a metric subspace; and every metric subspace is open in itself, by trivial reason. If we consider $E\subseteq X$ to be a metric space, then $E$ is always open in $(E,d)$. For example, $[0,1]$ is not open in $(\mathbb{R},d)$; however, $[0,1]$ is open in $([0,1],d)$. So what metric space the set $E$ is with respect to play an important role when talking whether the set $E$ is open. Now go to the definition of compactness , and also sequentially compact . It has two variations the definition would make: (For $\textbf{compactness}$) Let $(X,d)$ be a metric space, let $E\subseteq X$, if every open covering $\mathcal{A}$ of $E$ contains a finite subcollection that also covers $E$, then $E$ is called a $\textbf{compact set}$. Let $(X,d)$ be a metric space. If every open covering $\mathcal{A}$ of $X$ contains a finite subcollection that also covers $X$, then $X$ is called a $\textbf{compact metric space}$. (For $\textbf{sequentially compactess}$) Let $(X,d)$ be a metric space, let $E\subseteq X$, if every sequence of points of $E$ has a convergent subsequence, then $E$ is called a $\textbf{sequentially compact set}$. Let $(X,d)$ be a metric space. If every sequence of points of $X$ has a convergent subsequence, then $X$ is called a $\textbf{sequentially compact metric space}$. I don't know which is the regular definition people use. And also, I'm not sure whether this two versions of definition differs in some tricky circumstance, just like the ""open set"" mentioned above.",,"['general-topology', 'analysis', 'metric-spaces', 'definition']"
20,Multiplication theorem for modified Bessel Function of the Second Kind,Multiplication theorem for modified Bessel Function of the Second Kind,,"The modified Bessel Function of the Second Kind $K_v(z)$ can be expressed by  $$ K_v(z)=\frac{\pi\csc(\pi v)}{2}(I_{-v}(z)-I_v(z)), $$ if $v$ is not an integer. $K_v(z)$ satisfies the following multiplication theorem.  $$ \lambda^{v}K_v(\lambda z)=\sum_{l=0}^{\infty}{\frac{(-1)^l}{l!}(\frac{(\lambda^2-1)z}{2})^jK_{v-j}(z)}, ~~\text{if}~~ |\lambda^2-1|<1. $$ My question is that whether the condition $|\lambda^2-1|<1$ here is necessary at least when $\lambda$ is real. I saw this formula  in the book 'Handbook of Mathematical Functions' edited by Abramowitz and Stegun. However, when I checked in Wikipedia, it seems that this condition is not necessary and the formula is valid for any $\lambda$. Can anyone show me which is correct?","The modified Bessel Function of the Second Kind $K_v(z)$ can be expressed by  $$ K_v(z)=\frac{\pi\csc(\pi v)}{2}(I_{-v}(z)-I_v(z)), $$ if $v$ is not an integer. $K_v(z)$ satisfies the following multiplication theorem.  $$ \lambda^{v}K_v(\lambda z)=\sum_{l=0}^{\infty}{\frac{(-1)^l}{l!}(\frac{(\lambda^2-1)z}{2})^jK_{v-j}(z)}, ~~\text{if}~~ |\lambda^2-1|<1. $$ My question is that whether the condition $|\lambda^2-1|<1$ here is necessary at least when $\lambda$ is real. I saw this formula  in the book 'Handbook of Mathematical Functions' edited by Abramowitz and Stegun. However, when I checked in Wikipedia, it seems that this condition is not necessary and the formula is valid for any $\lambda$. Can anyone show me which is correct?",,"['complex-analysis', 'analysis', 'special-functions']"
21,Fundamental theorem of calculus in higher dimensions?,Fundamental theorem of calculus in higher dimensions?,,"I would like to make this statement rigorous: I have a smooth function $E:(a,b)^3\rightarrow \mathbb{R}$ and I would like to prove that $\int_{\{(a,b); E(a,b)<k\}}1 dx$ is differentiable as a function of $k$ if $E^{-1}(k)$ contains only regular points, then: $\frac{d}{dk}\left(\int_{\{(a,b); E(a,b)<k\}}1 dx\right) = \int_{E^{-1}(k)} \frac{1}{|\nabla E(x)|} dS(x).$ If you can only show it under weaker /additional assumptions, then this is also perfectly fine. This is a sort of higher-dimensional variant of $\frac{d}{dx} \int_a^x F(s)ds= F(x).$","I would like to make this statement rigorous: I have a smooth function $E:(a,b)^3\rightarrow \mathbb{R}$ and I would like to prove that $\int_{\{(a,b); E(a,b)<k\}}1 dx$ is differentiable as a function of $k$ if $E^{-1}(k)$ contains only regular points, then: $\frac{d}{dk}\left(\int_{\{(a,b); E(a,b)<k\}}1 dx\right) = \int_{E^{-1}(k)} \frac{1}{|\nabla E(x)|} dS(x).$ If you can only show it under weaker /additional assumptions, then this is also perfectly fine. This is a sort of higher-dimensional variant of $\frac{d}{dx} \int_a^x F(s)ds= F(x).$",,"['real-analysis', 'integration', 'analysis', 'manifolds', 'lebesgue-integral']"
22,Find the gradient of unimodal function,Find the gradient of unimodal function,,"I am doing the following exercise:  given the level lines of the unimodal function $f$ with minimum $x^*$, a point $x_0$, and vectors $v_1$,$v_2$, $v_3$, $v_4$, $v_5$, one of which is equal to $\nabla f(x_0)$, which vector $v_i$ is equal to $\nabla f(x_0)$? My answer is $v_5$, because I know that the gradient of a function in a point $x_0$ must be perpendicular to level line in that point and the only two perpendicular vectors are $v_1$ and $v_5$. Furthermore, the gradient points towards the direction of maximum increase and $v_1$ is going towards the minimum $x^*$, therefore the correct answer should be $v_5$. Am I correct or am I missing something?","I am doing the following exercise:  given the level lines of the unimodal function $f$ with minimum $x^*$, a point $x_0$, and vectors $v_1$,$v_2$, $v_3$, $v_4$, $v_5$, one of which is equal to $\nabla f(x_0)$, which vector $v_i$ is equal to $\nabla f(x_0)$? My answer is $v_5$, because I know that the gradient of a function in a point $x_0$ must be perpendicular to level line in that point and the only two perpendicular vectors are $v_1$ and $v_5$. Furthermore, the gradient points towards the direction of maximum increase and $v_1$ is going towards the minimum $x^*$, therefore the correct answer should be $v_5$. Am I correct or am I missing something?",,['analysis']
23,Attempt to prove that separately convexity implies locally Lipschitz,Attempt to prove that separately convexity implies locally Lipschitz,,"I want to prove this fact: I consider a function $g:\mathbb{R}^m \rightarrow \mathbb{R}$ separatelly convex, i.e. convex in each variable. Then $g\in {\rm Lip}_{\rm loc}(\mathbb{R}^m)$ . My attempt: I make this preliminar observation. I consider $k\in\{0,...,m \}$ and let $\xi^0:=z$ and $\xi^k:=(w_1,...,w_k,z_{k+1},...,z_m)$ , with $z,w\in \mathbb{R}^m$ . I have that $$|g(z)-g(w)|=|g(\xi^0)-g(\xi^m)|\leq \sum_{i=0}^{m-1}|g(\xi^{i+1})-g(\xi^i)|$$ I observe that $\xi^{i+1}-\xi^i=(0,\dots,o,w_{i+1}-z_{i+1}, \dots, 0)$ . Now let be $R>0$ . I consider $w,z\in \mathbb{R}^m$ such that $|\xi^{i+1}-\xi^i|=|w_{i+1}-z_{i+1}|\leq R$ for every $i\in \{0,\dots, m-1\}$ . Let be $g_{i+1} :(0,2R) \rightarrow \mathbb{R}$ defined as follows: $$g_{i+1}(t)=g(w_1,\dots, w_i,t,z_{i+2},\dots, z_m).$$ I observe that $g_{i+1}(w_{i+1})=g(\xi^{i+1})$ and $g_{i+1}(z_{i+1})=g(\xi^i)$ . Now, using the convexity of $g_{i+1}$ in $(0,2R)$ , I obtain: $$\dfrac{g_{i+1}(w_{i+1})-g_{i+1}(z_{i+1})}{w_{i+1}-z_{i+1}}\leq \dfrac{g_{i+1}(2R)-g_{i+1}(z_{i+1})}{2R-z_{i+1}}\leq \dfrac{{\rm osc}(g_{i+1}, (0,2R))}{R}:=c_i(R),$$ where ${\rm osc}(f,S)={\rm sup}\{|f(t)-f(s)|\ {\rm with}\: s,t\in S\}$ . From this follows that $|g(z)-g(w)|\leq(\sum_{i=0}^{m-1}c_i(R) )|z-w|$ . I call $c(R)=\sum_{i=0}^{m-1}c_i(R)$ and I have finished. My problem: Is my proof correct? It seems to me that in this way I have proved something more than the locally lipschitz condition, since $R$ is generic. Is this the Lipschitz condition on every compact set? Edit I have proved that $\forall R$ if $z,w\in\mathbb{R}^m$ are such that $|z_i-w_i|\leq R$ $\forall i\in\{1,...,m\}$ then $|g(z)-g(w)|\leq c(R)|z-w|$ . So I have proved the Lipschitz condition on every m-cube and so in every compact set. Is it right? Thanks a lot for the help!","I want to prove this fact: I consider a function separatelly convex, i.e. convex in each variable. Then . My attempt: I make this preliminar observation. I consider and let and , with . I have that I observe that . Now let be . I consider such that for every . Let be defined as follows: I observe that and . Now, using the convexity of in , I obtain: where . From this follows that . I call and I have finished. My problem: Is my proof correct? It seems to me that in this way I have proved something more than the locally lipschitz condition, since is generic. Is this the Lipschitz condition on every compact set? Edit I have proved that if are such that then . So I have proved the Lipschitz condition on every m-cube and so in every compact set. Is it right? Thanks a lot for the help!","g:\mathbb{R}^m \rightarrow \mathbb{R} g\in {\rm Lip}_{\rm loc}(\mathbb{R}^m) k\in\{0,...,m \} \xi^0:=z \xi^k:=(w_1,...,w_k,z_{k+1},...,z_m) z,w\in \mathbb{R}^m |g(z)-g(w)|=|g(\xi^0)-g(\xi^m)|\leq \sum_{i=0}^{m-1}|g(\xi^{i+1})-g(\xi^i)| \xi^{i+1}-\xi^i=(0,\dots,o,w_{i+1}-z_{i+1}, \dots, 0) R>0 w,z\in \mathbb{R}^m |\xi^{i+1}-\xi^i|=|w_{i+1}-z_{i+1}|\leq R i\in \{0,\dots, m-1\} g_{i+1} :(0,2R) \rightarrow \mathbb{R} g_{i+1}(t)=g(w_1,\dots, w_i,t,z_{i+2},\dots, z_m). g_{i+1}(w_{i+1})=g(\xi^{i+1}) g_{i+1}(z_{i+1})=g(\xi^i) g_{i+1} (0,2R) \dfrac{g_{i+1}(w_{i+1})-g_{i+1}(z_{i+1})}{w_{i+1}-z_{i+1}}\leq \dfrac{g_{i+1}(2R)-g_{i+1}(z_{i+1})}{2R-z_{i+1}}\leq \dfrac{{\rm osc}(g_{i+1}, (0,2R))}{R}:=c_i(R), {\rm osc}(f,S)={\rm sup}\{|f(t)-f(s)|\ {\rm with}\: s,t\in S\} |g(z)-g(w)|\leq(\sum_{i=0}^{m-1}c_i(R) )|z-w| c(R)=\sum_{i=0}^{m-1}c_i(R) R \forall R z,w\in\mathbb{R}^m |z_i-w_i|\leq R \forall i\in\{1,...,m\} |g(z)-g(w)|\leq c(R)|z-w|","['real-analysis', 'analysis', 'convex-analysis', 'lipschitz-functions']"
24,Show that $f(z)$ has at least two zeros (complex analysis),Show that  has at least two zeros (complex analysis),f(z),"Let $f$ be a non-constant analytic function in $D=1<|z|<2$ with $|f|\equiv5$ on the boundary (assume $f$ is continuous on $\overline D$). Show that $f$ has at least two zeros. My opinion: Based on maximum module principle, we have $|f(z)|\leq 5$ for any $z \in D$. Suppose $f$ doesn't have zero in that region $D$, then we have $1/|f(z)|=1/5$ on the boundary and $1/|f(z)|<1/5$ (based on maximum module principle theorem and the fact that $f(z)$ is non-constant). However, it contracts to $|f(z)|\leq 5$, which implies that $|f(z)|\geq 1/5$ for any $z \in D$. That's why $f(z)$ has to have at least one zero in $D$. But how can I prove $f$ has at least two zeroes?","Let $f$ be a non-constant analytic function in $D=1<|z|<2$ with $|f|\equiv5$ on the boundary (assume $f$ is continuous on $\overline D$). Show that $f$ has at least two zeros. My opinion: Based on maximum module principle, we have $|f(z)|\leq 5$ for any $z \in D$. Suppose $f$ doesn't have zero in that region $D$, then we have $1/|f(z)|=1/5$ on the boundary and $1/|f(z)|<1/5$ (based on maximum module principle theorem and the fact that $f(z)$ is non-constant). However, it contracts to $|f(z)|\leq 5$, which implies that $|f(z)|\geq 1/5$ for any $z \in D$. That's why $f(z)$ has to have at least one zero in $D$. But how can I prove $f$ has at least two zeroes?",,"['real-analysis', 'complex-analysis', 'functional-analysis', 'analysis', 'maximum-principle']"
25,What else do we gain from best linear approximations?,What else do we gain from best linear approximations?,,"In single-variable calculus, we're introduced to the idea of best linear approximation, that is: $$f(x) \approx f(a) + f'(a)(x - a)\tag{1}$$ And this is the best linear approximation for the point $a$. In multivariable calculus, the same concept is presented again, and for two variables, it is: $$f\left(x,y\right)\approx f\left(a,b\right)+\frac{\partial f}{\partial x}\left(a,b\right)\left(x-a\right)+\frac{\partial f}{\partial y}\left(a,b\right)\left(y-b\right)$$ I've been thinking for a while but couldn't guess why this is important. We could say that in the $(1)$, it gives us the line tangent to the function $f$ at point $f(a)$, this would reveal us how much the function is increasing at that point - but the same can be said using only the derivative. For multivariable calculus, it's even worse. They tell us that there is a best linear approximation, which is a plane tangent to a surface, but in this case, we are not told what they are important for: The subject is given and then we jump to other subjects. For a while, I thought that for some functions, there is some points in which it is easy to calculate the value and other points in which it's hard to do it, ex: $\sqrt{4}=2,\sqrt{5}=2.23607\dots$ and then we could anchor to these points $a$, vary a little bit the $x$ and find an approximation, perhaps with an error function that allows an easier computation some way? But then I thought that this kind of problem could be easily bypassed with the use of modern computers. So after all, why are best linear approximations useful? I'm mostly interested in applications to mathematics, but whatever comes in mind, just say it.","In single-variable calculus, we're introduced to the idea of best linear approximation, that is: $$f(x) \approx f(a) + f'(a)(x - a)\tag{1}$$ And this is the best linear approximation for the point $a$. In multivariable calculus, the same concept is presented again, and for two variables, it is: $$f\left(x,y\right)\approx f\left(a,b\right)+\frac{\partial f}{\partial x}\left(a,b\right)\left(x-a\right)+\frac{\partial f}{\partial y}\left(a,b\right)\left(y-b\right)$$ I've been thinking for a while but couldn't guess why this is important. We could say that in the $(1)$, it gives us the line tangent to the function $f$ at point $f(a)$, this would reveal us how much the function is increasing at that point - but the same can be said using only the derivative. For multivariable calculus, it's even worse. They tell us that there is a best linear approximation, which is a plane tangent to a surface, but in this case, we are not told what they are important for: The subject is given and then we jump to other subjects. For a while, I thought that for some functions, there is some points in which it is easy to calculate the value and other points in which it's hard to do it, ex: $\sqrt{4}=2,\sqrt{5}=2.23607\dots$ and then we could anchor to these points $a$, vary a little bit the $x$ and find an approximation, perhaps with an error function that allows an easier computation some way? But then I thought that this kind of problem could be easily bypassed with the use of modern computers. So after all, why are best linear approximations useful? I'm mostly interested in applications to mathematics, but whatever comes in mind, just say it.",,"['analysis', 'approximation', 'linear-approximation']"
26,explain why one can write $\hat{f}(\xi)=\lim_{n\to\infty}\frac{1}{\sqrt{2\pi}}\int_{-n}^{n}e^{-i\xi x}f(x)dx$ when $f\in L^2(\mathbb{R})$,explain why one can write  when,\hat{f}(\xi)=\lim_{n\to\infty}\frac{1}{\sqrt{2\pi}}\int_{-n}^{n}e^{-i\xi x}f(x)dx f\in L^2(\mathbb{R}),"Let $f\in L^1(\mathbb{R})$ where the measure is taken to be the Lebesgue measure. The Fourier transform of $f$ is the function $\hat{f}$ defined as  $$\hat{f}(\xi)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{-i\xi x}f(x)dx \qquad ,\xi\in \mathbb{R}$$ (Plancherel Forumla) If $f\in L^1(\mathbb{R})\cap L^2(\mathbb{R})$ then $\hat{f}\in L^2(\mathbb{R})$ and $\|f\|_2=\|\hat{f}\|_2$ (*) Assuming this result, we can extend the Fourier transform to an isometric operator $L^2(\mathbb{R})\to L^2(\mathbb{R})$ Let $f\in L^2(\mathbb{R})$ be arbitrary. Using the idea (*), explain why one can write $$\hat{f}(\xi)=\lim_{n\to\infty}\frac{1}{\sqrt{2\pi}}\int_{-n}^{n}e^{-i\xi x}f(x)dx$$ In this question, Is the equality $$\lim_{n\to\infty}\frac{1}{\sqrt{2\pi}}\int_{-n}^{n}e^{-i\xi x}f(x)dx=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{-i\xi x}f(x)dx$$ correct? If the above equality is correct, do we need to show that $\int_{-\infty}^{\infty}e^{-i\xi x}f(x)dx<\infty$  i.e.  $\int_{-\infty}^{\infty}|f(x)|dx<\infty$? How can we solve the question? Thanks! Update Let $f_n=\chi_{[-n,n]}f$. Is it enough to show the following steps? $f_n\in L^1(\mathbb{R})\cap L^2(\mathbb{R})$ $f_n \to f$ in $L^2$ and $L^1$ as $n\to\infty$ $\hat{f_n}=\frac{1}{\sqrt{2\pi}}\int_{-n}^{n} e^{-i\xi x}f(x)dx$ If the answer yes, my attempt is the following: Note that $|f_n|<|f|$ implies $\int_{\mathbb{R}}|f_n|^2<\int_{\mathbb{R}}|f|^2<\infty$ . So $f_n\in L^2(\mathbb{R})$. On the other hand we have  $$\int_{\mathbb{R}}|f_n|=\int_{[-n,n]}|f_n|\leq \sqrt{\int_{[-n,n]}|f_n|^2}\sqrt{\int_{[-n,n]}\textbf{1}^2}=\sqrt{\int_{[-n,n]}|f_n|^2}.\mathcal{L}([-n,n])<\infty$$ So $f_n\in L^1(\mathbb{R})$ as well. $|f_n-f|^2= \left\{   \begin{array}{lr}     0 & \text{on $[-n,n]$}\\     |f|^2 &  \text{otherwise}   \end{array} \right. $ So $$\|f_n-f\|_2^2=\int |f_n-f|^2=\int_{[-n,n]}|f_n-f|^2+\int_{\mathbb{R}-[-n,n]}|f_n-f|^2=\int_{\mathbb{R}-[-n,n]}|f|^2$$ So $\|f_n-f\|_2\to 0$ as $n\to \infty$. Similarly $\|f_n-f\|_1 \to 0$ as $n\to\infty$ By definition, it is obvious.","Let $f\in L^1(\mathbb{R})$ where the measure is taken to be the Lebesgue measure. The Fourier transform of $f$ is the function $\hat{f}$ defined as  $$\hat{f}(\xi)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{-i\xi x}f(x)dx \qquad ,\xi\in \mathbb{R}$$ (Plancherel Forumla) If $f\in L^1(\mathbb{R})\cap L^2(\mathbb{R})$ then $\hat{f}\in L^2(\mathbb{R})$ and $\|f\|_2=\|\hat{f}\|_2$ (*) Assuming this result, we can extend the Fourier transform to an isometric operator $L^2(\mathbb{R})\to L^2(\mathbb{R})$ Let $f\in L^2(\mathbb{R})$ be arbitrary. Using the idea (*), explain why one can write $$\hat{f}(\xi)=\lim_{n\to\infty}\frac{1}{\sqrt{2\pi}}\int_{-n}^{n}e^{-i\xi x}f(x)dx$$ In this question, Is the equality $$\lim_{n\to\infty}\frac{1}{\sqrt{2\pi}}\int_{-n}^{n}e^{-i\xi x}f(x)dx=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{-i\xi x}f(x)dx$$ correct? If the above equality is correct, do we need to show that $\int_{-\infty}^{\infty}e^{-i\xi x}f(x)dx<\infty$  i.e.  $\int_{-\infty}^{\infty}|f(x)|dx<\infty$? How can we solve the question? Thanks! Update Let $f_n=\chi_{[-n,n]}f$. Is it enough to show the following steps? $f_n\in L^1(\mathbb{R})\cap L^2(\mathbb{R})$ $f_n \to f$ in $L^2$ and $L^1$ as $n\to\infty$ $\hat{f_n}=\frac{1}{\sqrt{2\pi}}\int_{-n}^{n} e^{-i\xi x}f(x)dx$ If the answer yes, my attempt is the following: Note that $|f_n|<|f|$ implies $\int_{\mathbb{R}}|f_n|^2<\int_{\mathbb{R}}|f|^2<\infty$ . So $f_n\in L^2(\mathbb{R})$. On the other hand we have  $$\int_{\mathbb{R}}|f_n|=\int_{[-n,n]}|f_n|\leq \sqrt{\int_{[-n,n]}|f_n|^2}\sqrt{\int_{[-n,n]}\textbf{1}^2}=\sqrt{\int_{[-n,n]}|f_n|^2}.\mathcal{L}([-n,n])<\infty$$ So $f_n\in L^1(\mathbb{R})$ as well. $|f_n-f|^2= \left\{   \begin{array}{lr}     0 & \text{on $[-n,n]$}\\     |f|^2 &  \text{otherwise}   \end{array} \right. $ So $$\|f_n-f\|_2^2=\int |f_n-f|^2=\int_{[-n,n]}|f_n-f|^2+\int_{\mathbb{R}-[-n,n]}|f_n-f|^2=\int_{\mathbb{R}-[-n,n]}|f|^2$$ So $\|f_n-f\|_2\to 0$ as $n\to \infty$. Similarly $\|f_n-f\|_1 \to 0$ as $n\to\infty$ By definition, it is obvious.",,"['real-analysis', 'functional-analysis', 'analysis']"
27,Spivak's proof of change of variable,Spivak's proof of change of variable,,"I'm reading the proof of change of variable in Spivak's Calculus on manifolds. In the last inequality of page 68, I think this proof assumed that $ f\circ g |\det g'| $ is integrable on $g^{-1}(V)$ . But Cannot understand why this is true. Can anyone explain why this function is integrable?","I'm reading the proof of change of variable in Spivak's Calculus on manifolds. In the last inequality of page 68, I think this proof assumed that $ f\circ g |\det g'| $ is integrable on $g^{-1}(V)$ . But Cannot understand why this is true. Can anyone explain why this function is integrable?",,"['analysis', 'multivariable-calculus']"
28,"Differentiable, nonnegative derivative ae, nondecreasing?","Differentiable, nonnegative derivative ae, nondecreasing?",,"I have been trying to prove or disprove: Let $g$ be differentiable on the reals, have $g'(x)\geq 0$ except countably many values, then $g$ is non-decreasing. The main problem I face is that I don't have that $g'$ is continuous. Otherwise fundamental theorem of calculus $\int_a^b g'=g(b)-g(a)\geq 0$ will do the job nicely. I can't find counter examples either. Thanks for help!","I have been trying to prove or disprove: Let $g$ be differentiable on the reals, have $g'(x)\geq 0$ except countably many values, then $g$ is non-decreasing. The main problem I face is that I don't have that $g'$ is continuous. Otherwise fundamental theorem of calculus $\int_a^b g'=g(b)-g(a)\geq 0$ will do the job nicely. I can't find counter examples either. Thanks for help!",,"['calculus', 'real-analysis', 'analysis']"
29,Integral of determinant of Jacobian of a $C^{1}$ application.,Integral of determinant of Jacobian of a  application.,C^{1},"Let $f \in C^{1}(\mathbb{R}^{n}; \mathbb{R}^{n})$, suppose that exists $r>0$ such that $f(x)=0$ if $|x|>r$. Prove that exists $k>0$ such that $\int_{\overline{B_{k}(0)}} det Jf(x) dx=0$. My work: Let$g:(0,\infty)\to \mathbb{R}$ defined as  $g(t)=\int_{\overline{B_{t}(0)}} det Jf(x) dx$. I want to prove that $g$ is a continuous function e to prove that there $p,q\in (0,\infty)$ such that $g(p)<0<g(q)$ then the intermediate value theorem ensures that there exists the number $k>0$ s.t $g(k)=0$. I'm in the right way?","Let $f \in C^{1}(\mathbb{R}^{n}; \mathbb{R}^{n})$, suppose that exists $r>0$ such that $f(x)=0$ if $|x|>r$. Prove that exists $k>0$ such that $\int_{\overline{B_{k}(0)}} det Jf(x) dx=0$. My work: Let$g:(0,\infty)\to \mathbb{R}$ defined as  $g(t)=\int_{\overline{B_{t}(0)}} det Jf(x) dx$. I want to prove that $g$ is a continuous function e to prove that there $p,q\in (0,\infty)$ such that $g(p)<0<g(q)$ then the intermediate value theorem ensures that there exists the number $k>0$ s.t $g(k)=0$. I'm in the right way?",,"['calculus', 'integration', 'analysis', 'multivariable-calculus']"
30,"Prob. 4, Sec. 4.3, in Kreyszig's Functional Analysis textbook: Application of the Hahn Banach Theorem","Prob. 4, Sec. 4.3, in Kreyszig's Functional Analysis textbook: Application of the Hahn Banach Theorem",,"Here is Prob. 4, Sec. 4.3, in the book Introductory Functional Analysis With Applications by Erwine Kreyszig: Let $X$ be a (real or complex) vector space, and let $p \colon X \longrightarrow \mathbb{R}$ be a real-valued function satisfying $$p(x+y) \leq p(x) + p(y) \ \mbox{ for all } \ x, y \in X$$ and $$p(\alpha x) = \lvert \alpha \rvert p(x) \ \mbox{ for all } x \in X \mbox{ and for all scalars } \alpha.$$ Let $x_0$ be a point of $X$ . Then (how to show that?) there is a linear functional $\tilde{f}$ on $X$ such that $\tilde{f}(x_0) = p(x_0)$ and $\left\lvert \tilde{f}(x) \right\rvert \leq p(x)$ for all $x \in X$ . First of all, here is the (generalized) Hahn Banach Theorem for (real or complex) vector spaces. Let $X$ be a real or complex vector space; let $Z$ be a subspace of $X$ ; let $p \colon X \longrightarrow \mathbb{R}$ satisfy $$p(x+y) \leq p(x) + p(y) \ \mbox{ for all } x, y \in X$$ and $$p(\alpha x ) = \lvert \alpha \rvert p(x) \ \mbox{ for all } x \in X \mbox{ and for all scalars } \alpha;$$ and let $f$ be a linear functional defined on $Z$ such that $$\lvert f(x) \rvert \leq p(x) \ \mbox{ for all }  x \in Z.$$ Then there is a linear functional $\tilde{f}$ defined on all of $X$ such that $$\tilde{f}(x) = f(x) \ \mbox{ for all } x \in Z$$ and $$\left\lvert \tilde{f}(x) \right\rvert \leq p(x) \ \mbox{ for all } x \in X.$$ This is Theorem 4.3-1 in the book Introductory Functional Analysis With Applications by Erwine Kreyszig. My effort: Let $Z$ be the subspace of $X$ spanned by $x_0$ , and let the functional $f$ be defined on $Z$ by $$f(\alpha x_0) = \alpha p(x_0) \mbox{ for all scalars } \alpha .$$ We can show that $p(x) \geq 0$ for all $x \in X$ , and $f$ satisfies all the conditions in the hypothesis of the above theorem. So there is a linear functional $\tilde{f}$ on $X$ such that $\tilde{f}(x) = f(x)$ for all $x \in Z$ and $\left\lvert \tilde{f}(x) \right\rvert \leq p(x)$ for all $x \in X$ . Thus, $\tilde{f}(\alpha x_0) = \alpha p(x_0)$ for all scalars $\alpha$ and hence $\tilde{f}(x_0) = p(x_0)$ . Is this proof correct and clear enough in each and every detail? Or, are there any issues therein of accuracy or clarity?","Here is Prob. 4, Sec. 4.3, in the book Introductory Functional Analysis With Applications by Erwine Kreyszig: Let be a (real or complex) vector space, and let be a real-valued function satisfying and Let be a point of . Then (how to show that?) there is a linear functional on such that and for all . First of all, here is the (generalized) Hahn Banach Theorem for (real or complex) vector spaces. Let be a real or complex vector space; let be a subspace of ; let satisfy and and let be a linear functional defined on such that Then there is a linear functional defined on all of such that and This is Theorem 4.3-1 in the book Introductory Functional Analysis With Applications by Erwine Kreyszig. My effort: Let be the subspace of spanned by , and let the functional be defined on by We can show that for all , and satisfies all the conditions in the hypothesis of the above theorem. So there is a linear functional on such that for all and for all . Thus, for all scalars and hence . Is this proof correct and clear enough in each and every detail? Or, are there any issues therein of accuracy or clarity?","X p \colon X \longrightarrow \mathbb{R} p(x+y) \leq p(x) + p(y) \ \mbox{ for all } \ x, y \in X p(\alpha x) = \lvert \alpha \rvert p(x) \ \mbox{ for all } x \in X \mbox{ and for all scalars } \alpha. x_0 X \tilde{f} X \tilde{f}(x_0) = p(x_0) \left\lvert \tilde{f}(x) \right\rvert \leq p(x) x \in X X Z X p \colon X \longrightarrow \mathbb{R} p(x+y) \leq p(x) + p(y) \ \mbox{ for all } x, y \in X p(\alpha x ) = \lvert \alpha \rvert p(x) \ \mbox{ for all } x \in X \mbox{ and for all scalars } \alpha; f Z \lvert f(x) \rvert \leq p(x) \ \mbox{ for all }  x \in Z. \tilde{f} X \tilde{f}(x) = f(x) \ \mbox{ for all } x \in Z \left\lvert \tilde{f}(x) \right\rvert \leq p(x) \ \mbox{ for all } x \in X. Z X x_0 f Z f(\alpha x_0) = \alpha p(x_0) \mbox{ for all scalars } \alpha . p(x) \geq 0 x \in X f \tilde{f} X \tilde{f}(x) = f(x) x \in Z \left\lvert \tilde{f}(x) \right\rvert \leq p(x) x \in X \tilde{f}(\alpha x_0) = \alpha p(x_0) \alpha \tilde{f}(x_0) = p(x_0)","['real-analysis', 'functional-analysis', 'analysis', 'solution-verification', 'normed-spaces']"
31,Spherical Harmonics and $L_+$ and $L_-$ operators,Spherical Harmonics and  and  operators,L_+ L_-,"I have the spherical harmonics $Y_{m}^{l}\left(\theta,\varphi\right)$ and I want to show that the operators $L^{\pm}$ act as ""creation"" and ""annihilation"" operators such that $$L^{\pm}Y_{m}^{l}=\sqrt{l\left(l+1\right)-m\left(m\pm1\right)}Y_{m\pm1}^{l}$$ I'm quite sure that if I substitute all the definitions and proceed analitically I will end up after 3 or 4 pages of calculations with the right solution, but I'm wondering if there's a smarter and algebric way to demonstrate this equation. Does anybody have at least an hint to follow? Here's the definitions I used: Spherical Harmonics $$Y_{m}^{l}\left(\theta,\varphi\right)=\left(-1\right)^{m}\sqrt{\frac{\left(2l+1\right)}{4\pi}\frac{\left(l-m\right)!}{\left(l+m\right)!}}P_{l}^{m}(\cos\theta)e^{\imath m\varphi}$$ Legendre Functions $$P_{l}^{m}(x)=\frac{1}{2^{l}l!}\left(1-x^{2}\right)^{m/2}\left(\frac{d}{dx}\right)^{l+m}\left(x^{2}-1\right)^{l},$$ Operators $$ L^{+}	=L_{1}+\imath L_{2}=e^{\imath\varphi}\left(-\imath\frac{\partial}{\partial\theta}+\cot\theta\frac{\partial}{\partial\varphi}\right),$$ $$ L^{-}	=L_{1}-\imath L_{2}=e^{-\imath\varphi}\left(\imath\frac{\partial}{\partial\theta}+\cot\theta\frac{\partial}{\partial\varphi}\right), $$ $$ L_{z}	=\imath L_{3}=-\imath\frac{\partial}{\partial\varphi}. $$ Commutation relations $$\left[L_{z},\,L^{+}\right]=L^{+},$$ $$\left[L_{z},\,L^{-}\right]=-L^{-},$$ $$\left[L^{+},L^{-}\right]=2L_{z}.$$ Laplacian or Casimir Operator $$L^{-}L^{+}	=L_{1}^{2}+L_{2}^{2}+\imath\left[L_{1},\,L_{2}\right],$$ $$ L^{+}L^{-}	=L_{1}^{2}+L_{2}^{2}-\imath\left[L_{1},\,L_{2}\right], $$  $$\mathbf{L}^{2}=L_{z}^{2}+L_{z}+L^{-}L^{+}.$$","I have the spherical harmonics $Y_{m}^{l}\left(\theta,\varphi\right)$ and I want to show that the operators $L^{\pm}$ act as ""creation"" and ""annihilation"" operators such that $$L^{\pm}Y_{m}^{l}=\sqrt{l\left(l+1\right)-m\left(m\pm1\right)}Y_{m\pm1}^{l}$$ I'm quite sure that if I substitute all the definitions and proceed analitically I will end up after 3 or 4 pages of calculations with the right solution, but I'm wondering if there's a smarter and algebric way to demonstrate this equation. Does anybody have at least an hint to follow? Here's the definitions I used: Spherical Harmonics $$Y_{m}^{l}\left(\theta,\varphi\right)=\left(-1\right)^{m}\sqrt{\frac{\left(2l+1\right)}{4\pi}\frac{\left(l-m\right)!}{\left(l+m\right)!}}P_{l}^{m}(\cos\theta)e^{\imath m\varphi}$$ Legendre Functions $$P_{l}^{m}(x)=\frac{1}{2^{l}l!}\left(1-x^{2}\right)^{m/2}\left(\frac{d}{dx}\right)^{l+m}\left(x^{2}-1\right)^{l},$$ Operators $$ L^{+}	=L_{1}+\imath L_{2}=e^{\imath\varphi}\left(-\imath\frac{\partial}{\partial\theta}+\cot\theta\frac{\partial}{\partial\varphi}\right),$$ $$ L^{-}	=L_{1}-\imath L_{2}=e^{-\imath\varphi}\left(\imath\frac{\partial}{\partial\theta}+\cot\theta\frac{\partial}{\partial\varphi}\right), $$ $$ L_{z}	=\imath L_{3}=-\imath\frac{\partial}{\partial\varphi}. $$ Commutation relations $$\left[L_{z},\,L^{+}\right]=L^{+},$$ $$\left[L_{z},\,L^{-}\right]=-L^{-},$$ $$\left[L^{+},L^{-}\right]=2L_{z}.$$ Laplacian or Casimir Operator $$L^{-}L^{+}	=L_{1}^{2}+L_{2}^{2}+\imath\left[L_{1},\,L_{2}\right],$$ $$ L^{+}L^{-}	=L_{1}^{2}+L_{2}^{2}-\imath\left[L_{1},\,L_{2}\right], $$  $$\mathbf{L}^{2}=L_{z}^{2}+L_{z}+L^{-}L^{+}.$$",,"['complex-analysis', 'functional-analysis', 'analysis', 'lie-groups', 'lie-algebras']"
32,Rank theorem implies inverse function theorem,Rank theorem implies inverse function theorem,,"I am studying analysis on $\mathbb{R}^n$ and there is this question I cannot solve. Indeed it was not asked to me in any sense, but is usual to hear people saying that rank theorem is also one of the equivalents theorem to the inverse function theorem. I have the following version of the rank theorem, and I am searching a way to prove that indeed, it implies the inverse function theorem. Let $f : U \subset \mathbb{R}^{n+m} \to \mathbb{R}^{n+k}$ a $C^r(U)$ function with constant rank $n$, $U$ open set. Then  for each $z = (x,y) \in U$ there are diffeomorphisms $\alpha$ defined on one open neighborhood of $f(z)$ to an open set containing $(x,0)$ and $\beta$ defined on one open set of $\mathbb{R}^{n+m}$ to another containing $z$ such that $\alpha \circ f\circ \beta(x,y) = (x,0).$ I know that if I can argue that if $f : \mathbb{R}^n \to \mathbb{R}^n$ and $f$ has constant rank $n$ then $\alpha = id$ the identity, the function $\beta$ will work as a candidate for the inverse and then I can finish showing that the rank theorem implies the inverse function theorem, but I don't know how to proceed. I do appreciate any suggestions, answers, comments, etc. Thanks a lot!","I am studying analysis on $\mathbb{R}^n$ and there is this question I cannot solve. Indeed it was not asked to me in any sense, but is usual to hear people saying that rank theorem is also one of the equivalents theorem to the inverse function theorem. I have the following version of the rank theorem, and I am searching a way to prove that indeed, it implies the inverse function theorem. Let $f : U \subset \mathbb{R}^{n+m} \to \mathbb{R}^{n+k}$ a $C^r(U)$ function with constant rank $n$, $U$ open set. Then  for each $z = (x,y) \in U$ there are diffeomorphisms $\alpha$ defined on one open neighborhood of $f(z)$ to an open set containing $(x,0)$ and $\beta$ defined on one open set of $\mathbb{R}^{n+m}$ to another containing $z$ such that $\alpha \circ f\circ \beta(x,y) = (x,0).$ I know that if I can argue that if $f : \mathbb{R}^n \to \mathbb{R}^n$ and $f$ has constant rank $n$ then $\alpha = id$ the identity, the function $\beta$ will work as a candidate for the inverse and then I can finish showing that the rank theorem implies the inverse function theorem, but I don't know how to proceed. I do appreciate any suggestions, answers, comments, etc. Thanks a lot!",,['analysis']
33,Theorem 3.55 in Baby Rudin: Every re-arrangement of an absolutely convergent series converges to the same sum in every normed space?,Theorem 3.55 in Baby Rudin: Every re-arrangement of an absolutely convergent series converges to the same sum in every normed space?,,"Here's Theorem 3.55 in the book Principles of Mathematical Analysis by Walter Rudin, third edition. If $\sum a_n$ is a series of complex numbers which converges absolutely, then every rearrangement of $\sum a_n$ converges, and they all converge to the same sum. And, here's Rudin's proof. Let $\sum a_n^\prime$ be a rearrangement, with partial sums $s_n^\prime$. Given $\varepsilon > 0$, there exists an integer $N$ such that $m \geq n \geq N$ implies $$\mbox{ (26) } \ \ \ \ \sum_{i=n}^m \vert a_i \vert \leq  \varepsilon.$$ Now choose $p$ such that the integers $1, 2, \ldots, N$ are all contained in the set $k_1, k_2, \ldots, k_p$. [Here $\{k_n\}$ is a sequence of positive integers in which every positive integer appears as a term once and only once, and $a_n^\prime = a_{k_n}$ for each $n = 1, 2, 3, \ldots$; moreover, $s_n^\prime = a_1^\prime + \cdots + a_n^\prime$. This is the notation of  Definition 3.52 in Rudin. ] Then if $n > p$, the $a_1, \ldots, a_N$ will cancel in the difference $s_n - s_n^\prime$, so that $\vert s_n - s_n^\prime \vert \leq \varepsilon$ by (26). Hence $\{s_n^\prime \}$ converges to the same sum as $\{s_n \}$. Now here is my reading of Rudin's proof. As $\sum a_n$ converges absolutely, the series $\sum \vert a_n \vert$ converges, which means that the sequence $\{ \sum_{i =1}^n \vert a_i \vert \}_{n \in \mathbb{N}}$ is convergent and therefore Cauchy. Thus, given a real number $\varepsilon > 0$, we can find a natural number $N$ such that $$ \left\vert \sum_{i=1}^n \vert a_i \vert - \sum_{i=1}^m \vert a_i \vert \right\vert < \varepsilon \ \mbox{ for all } \ m, n \in \mathbb{N} \ \mbox{ such that } \ n \geq m >  N.$$ That is, $$ \sum_{i = m+1}^n \vert a_i \vert < \varepsilon \  \mbox{ for all } \ m, n \in \mathbb{N} \ \mbox{ such that } \ n \geq m >  N.$$ Now let $\{k_n \}$ be a sequence of natural numbers in which every natural number appears exactly once, and let $a_n^\prime = a_{k_n}$, and then let $s_n^\prime = \sum_{i=1}^n a_i^\prime$ for each $n = 1, 2, 3, \ldots$. We first need to show that the series $\sum a_n^\prime$ converges.   let $p$ be a natural number such that $$\{1, \ldots, N \} \subset \{ k_1, \ldots, k_p \}.$$ Then, for all $n \in \mathbb{N}$ such that $n > p$, we have    $$ \vert s_n^\prime - s_n \vert = \left\vert \sum_{i \in \{ k_1, \ldots, k_n \} \setminus \{1, \ldots, n \} } a_i  \right\vert \leq \sum_{i \in \{ k_1, \ldots, k_n \} \setminus \{1, \ldots, n \} } \vert a_i \vert \leq \sum_{i = N+1}^{\max ( \{ k_1, \ldots, k_n \} \setminus \{1, \ldots, N \} )} \vert a_i \vert < \varepsilon. $$ Now let's suppose that $$\lim_{n \to \infty} s_n = s.$$ Then for $n \in \mathbb{N}$ such that $n > p$, we have $$0 \leq \vert s_n^\prime - s \vert \leq \vert s_n^\prime - s_n \vert + \vert s_n - s \vert < \epsilon + \vert s_n - s \vert \to \epsilon + 0 \ \mbox{ as } \ n \to \infty.$$ So, $$ 0 \leq \lim_{n \to \infty} \vert s_n^\prime - s \vert \leq \epsilon,$$ provided that the limit exists (i.e. provided that the sequence $\{s_n^\prime \}$ converges). We now show that the sequence $\{s_n^\prime \}$ is Cauchy. For all $m, n \in \mathbb{N}$ such that $n \geq m > p$, we have $$ \vert s_n^\prime - s_m^\prime \vert \leq \sum_{i = m+1}^n \vert a_i^\prime \vert \leq  \sum_{i = N+1}^{\max( \{ k_{m+1}, \ldots, k_n \} )} \vert a_i \vert < \varepsilon,$$ showing that the sequence $\{s_n^\prime \}$ is indeed Cauchy. So for all $m, n \in \mathbb{N}$ such that $n \geq m > p$, we have $$ \left\vert \vert s_n^\prime - s \vert - \vert s_m^\prime - s \vert \right\vert \leq \left\vert (s_n^\prime - s) - (s_m^\prime - s) \right\vert = \vert s_n^\prime - s_m^\prime \vert < \varepsilon,$$ showing that the sequence $\{ \vert s_n^\prime - s \vert \}$ is Cauchy. Therefore $ \lim_{n \to \infty} \vert s_n^\prime - s \vert $ exists and we also  have $$0 \leq \lim_{n \to \infty} \vert s_n^\prime - s \vert \leq \varepsilon$$ for every $\varepsilon >0$, which shows that the last limit is $0$ and therefore $$ \lim_{n \to \infty} s_n^\prime = s.$$ Is this presentation correct? Have I understood Rudin's logic correctly? If there are any errors or issues in my presentation, please do point those out!! In proving this result, we have only used the axioms of a Banach space; so this result holds in any Banach space Am I right? Does this result hold in every normed space?","Here's Theorem 3.55 in the book Principles of Mathematical Analysis by Walter Rudin, third edition. If $\sum a_n$ is a series of complex numbers which converges absolutely, then every rearrangement of $\sum a_n$ converges, and they all converge to the same sum. And, here's Rudin's proof. Let $\sum a_n^\prime$ be a rearrangement, with partial sums $s_n^\prime$. Given $\varepsilon > 0$, there exists an integer $N$ such that $m \geq n \geq N$ implies $$\mbox{ (26) } \ \ \ \ \sum_{i=n}^m \vert a_i \vert \leq  \varepsilon.$$ Now choose $p$ such that the integers $1, 2, \ldots, N$ are all contained in the set $k_1, k_2, \ldots, k_p$. [Here $\{k_n\}$ is a sequence of positive integers in which every positive integer appears as a term once and only once, and $a_n^\prime = a_{k_n}$ for each $n = 1, 2, 3, \ldots$; moreover, $s_n^\prime = a_1^\prime + \cdots + a_n^\prime$. This is the notation of  Definition 3.52 in Rudin. ] Then if $n > p$, the $a_1, \ldots, a_N$ will cancel in the difference $s_n - s_n^\prime$, so that $\vert s_n - s_n^\prime \vert \leq \varepsilon$ by (26). Hence $\{s_n^\prime \}$ converges to the same sum as $\{s_n \}$. Now here is my reading of Rudin's proof. As $\sum a_n$ converges absolutely, the series $\sum \vert a_n \vert$ converges, which means that the sequence $\{ \sum_{i =1}^n \vert a_i \vert \}_{n \in \mathbb{N}}$ is convergent and therefore Cauchy. Thus, given a real number $\varepsilon > 0$, we can find a natural number $N$ such that $$ \left\vert \sum_{i=1}^n \vert a_i \vert - \sum_{i=1}^m \vert a_i \vert \right\vert < \varepsilon \ \mbox{ for all } \ m, n \in \mathbb{N} \ \mbox{ such that } \ n \geq m >  N.$$ That is, $$ \sum_{i = m+1}^n \vert a_i \vert < \varepsilon \  \mbox{ for all } \ m, n \in \mathbb{N} \ \mbox{ such that } \ n \geq m >  N.$$ Now let $\{k_n \}$ be a sequence of natural numbers in which every natural number appears exactly once, and let $a_n^\prime = a_{k_n}$, and then let $s_n^\prime = \sum_{i=1}^n a_i^\prime$ for each $n = 1, 2, 3, \ldots$. We first need to show that the series $\sum a_n^\prime$ converges.   let $p$ be a natural number such that $$\{1, \ldots, N \} \subset \{ k_1, \ldots, k_p \}.$$ Then, for all $n \in \mathbb{N}$ such that $n > p$, we have    $$ \vert s_n^\prime - s_n \vert = \left\vert \sum_{i \in \{ k_1, \ldots, k_n \} \setminus \{1, \ldots, n \} } a_i  \right\vert \leq \sum_{i \in \{ k_1, \ldots, k_n \} \setminus \{1, \ldots, n \} } \vert a_i \vert \leq \sum_{i = N+1}^{\max ( \{ k_1, \ldots, k_n \} \setminus \{1, \ldots, N \} )} \vert a_i \vert < \varepsilon. $$ Now let's suppose that $$\lim_{n \to \infty} s_n = s.$$ Then for $n \in \mathbb{N}$ such that $n > p$, we have $$0 \leq \vert s_n^\prime - s \vert \leq \vert s_n^\prime - s_n \vert + \vert s_n - s \vert < \epsilon + \vert s_n - s \vert \to \epsilon + 0 \ \mbox{ as } \ n \to \infty.$$ So, $$ 0 \leq \lim_{n \to \infty} \vert s_n^\prime - s \vert \leq \epsilon,$$ provided that the limit exists (i.e. provided that the sequence $\{s_n^\prime \}$ converges). We now show that the sequence $\{s_n^\prime \}$ is Cauchy. For all $m, n \in \mathbb{N}$ such that $n \geq m > p$, we have $$ \vert s_n^\prime - s_m^\prime \vert \leq \sum_{i = m+1}^n \vert a_i^\prime \vert \leq  \sum_{i = N+1}^{\max( \{ k_{m+1}, \ldots, k_n \} )} \vert a_i \vert < \varepsilon,$$ showing that the sequence $\{s_n^\prime \}$ is indeed Cauchy. So for all $m, n \in \mathbb{N}$ such that $n \geq m > p$, we have $$ \left\vert \vert s_n^\prime - s \vert - \vert s_m^\prime - s \vert \right\vert \leq \left\vert (s_n^\prime - s) - (s_m^\prime - s) \right\vert = \vert s_n^\prime - s_m^\prime \vert < \varepsilon,$$ showing that the sequence $\{ \vert s_n^\prime - s \vert \}$ is Cauchy. Therefore $ \lim_{n \to \infty} \vert s_n^\prime - s \vert $ exists and we also  have $$0 \leq \lim_{n \to \infty} \vert s_n^\prime - s \vert \leq \varepsilon$$ for every $\varepsilon >0$, which shows that the last limit is $0$ and therefore $$ \lim_{n \to \infty} s_n^\prime = s.$$ Is this presentation correct? Have I understood Rudin's logic correctly? If there are any errors or issues in my presentation, please do point those out!! In proving this result, we have only used the axioms of a Banach space; so this result holds in any Banach space Am I right? Does this result hold in every normed space?",,"['real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence', 'absolute-convergence']"
34,Integral involving minimum,Integral involving minimum,,"How can I show that:  $\int_0^t min(s,u)\phi_u du +\int_0^s min(t,u) \phi_u du = \int_0^t \int_0^s min(u,v) \phi_u \phi_v du dv =0$ when I know: $s \phi_s=\phi_s \int_0^s \phi_u u du$ for all $s \geq 0$","How can I show that:  $\int_0^t min(s,u)\phi_u du +\int_0^s min(t,u) \phi_u du = \int_0^t \int_0^s min(u,v) \phi_u \phi_v du dv =0$ when I know: $s \phi_s=\phi_s \int_0^s \phi_u u du$ for all $s \geq 0$",,"['real-analysis', 'integration', 'analysis', 'definite-integrals']"
35,"Show that for the triples $V \subset H \subset V^{*}$, the following are true","Show that for the triples , the following are true",V \subset H \subset V^{*},"Let $H$ be a Hilbert space equipped with scalar product $(,)$ and the corresponding norm $|\cdot|$. Let $V \subset H$ be a linear subspace that is dense in $H$. Assume that $V$ is a Banach space for $\|\cdot\|$. Assume also that the injection from $V$ to $H$ is continuous i.e $|v| \le C\|v\|,\forall v \in V $. Consider the operator $$T: H \to V^*$$ defined by $$ (Tu,v)_{V^*,V}=(u,v), \forall u \in H, \forall v \in V$$ Prove that $\|Tu\|_{V^*} \le C|u|, \forall u \in H$ Ans: $\|Tu\|_{V^*}=\sup_{\|v\| \le 1}|(Tu,v)|_{V^*,V}=\sup_{\|v\| \le 1}|(u,v)| \le |u||v| \le \sup_{\|v\| \le 1}C|u|\, \|v\| \le C|u|$ Prove that $T$ is injective Ans: Suppose that $Tu=0$ for some $u \in H$. Then $(Tu,v)=0, \forall v \in V \implies (u,v)=0, \forall v \in V$. Since $V$ is dense in $H$, there exists a sequence $\{v_n\}\in V$ which converges to $u$. Then $(u,v_n) \to (u,u)$ and hence $u=0$. Prove that $R(T)$ is dense in $V^*$ if $V$ is reflexive. This I am unable to show. Any hints for this? I know that the map $J(V)=V^{**}$. But How do I construct a sequence? Given $f \in V^*$, prove that $f \in R(T)$ iff there is a constant $a \ge 0$ such that $|(f,v)_{V^{*},V}| \le a|v|, \forall v \in V$ Ans: Suppose there is  constant $a \ge 0$ such that $|(f,v)_{V^{*},V}| \le a|v|, \forall v \in V$. Then Define a map $\phi: V \to K$ by $\phi(v)=(f,v)_{V^*,V}$. Then $\phi$ is continuous  and since $V$ is dense in $H$ , it can be extended to $\tilde{\phi}:H \to K$ . Then since $H$ is a Hilbert space, by the Riesz Representation Theorem, there exists $v_f \in H$ such that $\tilde{\phi}(u)=(v_f,u), \forall u \in H $. Then $(Tv_f,v)=(v_f,v)=(f,v), \forall v\in V$. Hence $f \in R(T)$. The other side is trivial. The only thing that remains to show is $3$ and I have not used that $V$ is Banach with respect to $\|\cdot\|$. How do I show it? I need a hint. Thanks for the help!","Let $H$ be a Hilbert space equipped with scalar product $(,)$ and the corresponding norm $|\cdot|$. Let $V \subset H$ be a linear subspace that is dense in $H$. Assume that $V$ is a Banach space for $\|\cdot\|$. Assume also that the injection from $V$ to $H$ is continuous i.e $|v| \le C\|v\|,\forall v \in V $. Consider the operator $$T: H \to V^*$$ defined by $$ (Tu,v)_{V^*,V}=(u,v), \forall u \in H, \forall v \in V$$ Prove that $\|Tu\|_{V^*} \le C|u|, \forall u \in H$ Ans: $\|Tu\|_{V^*}=\sup_{\|v\| \le 1}|(Tu,v)|_{V^*,V}=\sup_{\|v\| \le 1}|(u,v)| \le |u||v| \le \sup_{\|v\| \le 1}C|u|\, \|v\| \le C|u|$ Prove that $T$ is injective Ans: Suppose that $Tu=0$ for some $u \in H$. Then $(Tu,v)=0, \forall v \in V \implies (u,v)=0, \forall v \in V$. Since $V$ is dense in $H$, there exists a sequence $\{v_n\}\in V$ which converges to $u$. Then $(u,v_n) \to (u,u)$ and hence $u=0$. Prove that $R(T)$ is dense in $V^*$ if $V$ is reflexive. This I am unable to show. Any hints for this? I know that the map $J(V)=V^{**}$. But How do I construct a sequence? Given $f \in V^*$, prove that $f \in R(T)$ iff there is a constant $a \ge 0$ such that $|(f,v)_{V^{*},V}| \le a|v|, \forall v \in V$ Ans: Suppose there is  constant $a \ge 0$ such that $|(f,v)_{V^{*},V}| \le a|v|, \forall v \in V$. Then Define a map $\phi: V \to K$ by $\phi(v)=(f,v)_{V^*,V}$. Then $\phi$ is continuous  and since $V$ is dense in $H$ , it can be extended to $\tilde{\phi}:H \to K$ . Then since $H$ is a Hilbert space, by the Riesz Representation Theorem, there exists $v_f \in H$ such that $\tilde{\phi}(u)=(v_f,u), \forall u \in H $. Then $(Tv_f,v)=(v_f,v)=(f,v), \forall v\in V$. Hence $f \in R(T)$. The other side is trivial. The only thing that remains to show is $3$ and I have not used that $V$ is Banach with respect to $\|\cdot\|$. How do I show it? I need a hint. Thanks for the help!",,"['functional-analysis', 'analysis', 'hilbert-spaces']"
36,How to prove $\lim_{a \to + \infty}a^q \int_{a}^{+\infty}\frac{\sin(x)dx}{x^p}=0$ when $p>q>0$,How to prove  when,\lim_{a \to + \infty}a^q \int_{a}^{+\infty}\frac{\sin(x)dx}{x^p}=0 p>q>0,"I know a similar problem in demidovich's problem set #2357 about proving $$\lim_{x \to 0^+}x^a\int_{x}^1 \frac{f(t)}{t^{a+1}}dt$$it proves by dividing the integral into two parts and used two inequality to prove it less than some $\epsilon$. But the method does not apply. My other attempts discovered $\sin(x)$ is not arbitrary and cannot be casually changed to some constant even on some intervals. After Taylor expansion of $sin(x)$ is applied, the limit is still hard to calculate since it is a sum of an infinite series of improper integrals.","I know a similar problem in demidovich's problem set #2357 about proving $$\lim_{x \to 0^+}x^a\int_{x}^1 \frac{f(t)}{t^{a+1}}dt$$it proves by dividing the integral into two parts and used two inequality to prove it less than some $\epsilon$. But the method does not apply. My other attempts discovered $\sin(x)$ is not arbitrary and cannot be casually changed to some constant even on some intervals. After Taylor expansion of $sin(x)$ is applied, the limit is still hard to calculate since it is a sum of an infinite series of improper integrals.",,"['analysis', 'improper-integrals']"
37,Does there exist a continuous function whose composition with itself is the exponential map?,Does there exist a continuous function whose composition with itself is the exponential map?,,"All of the maps $$ F(x) = x^4 \\ G(x) = \exp (\exp x) \\ H(x) = \sin (\sin x) $$ can be expressed as the self-compositions of the functions $$ f(x) = x^2 \\ g(x) = \exp x \\ h(x) = \sin x $$ So this led me naturally to the question whether other functions can be expressed as the self composition of another function. So this led me naturally to the question: Does there exist a continous function $f : \mathbb{R} \rightarrow \mathbb{R}$ such that, for all $x \in \mathbb{R}$, $f(f(x)) = \exp (x)$?","All of the maps $$ F(x) = x^4 \\ G(x) = \exp (\exp x) \\ H(x) = \sin (\sin x) $$ can be expressed as the self-compositions of the functions $$ f(x) = x^2 \\ g(x) = \exp x \\ h(x) = \sin x $$ So this led me naturally to the question whether other functions can be expressed as the self composition of another function. So this led me naturally to the question: Does there exist a continous function $f : \mathbb{R} \rightarrow \mathbb{R}$ such that, for all $x \in \mathbb{R}$, $f(f(x)) = \exp (x)$?",,['analysis']
38,Continuous strictly increasing function with derivative infinity at a measure 0 set,Continuous strictly increasing function with derivative infinity at a measure 0 set,,"Let $E\subset [0,1]$ with $\mu(E)=0$. Does there exist a continuous, strictly increasing function $f$ on $[0,1]$ so that $f'(x)=\infty$ for all $x\in E$ (in Lebesgue sense)? I think there exist such a function, but I don't know how to construct.","Let $E\subset [0,1]$ with $\mu(E)=0$. Does there exist a continuous, strictly increasing function $f$ on $[0,1]$ so that $f'(x)=\infty$ for all $x\in E$ (in Lebesgue sense)? I think there exist such a function, but I don't know how to construct.",,"['real-analysis', 'analysis', 'measure-theory', 'derivatives', 'lebesgue-measure']"
39,Exercise #9 in chapter 11 of Rudin's Principles of Mathematical Analysis.,Exercise #9 in chapter 11 of Rudin's Principles of Mathematical Analysis.,,"Suppose $f$ is Lebesgue integrable on $[a,b]$. Let $F(x)$=$\int_{a}^x fdt$. Then prove that $F$ is continuous on $[a,b]$. I know that $F$ is continuous almost everywhere, because $F'(x)=f(x)$ almost everywhere on $[a,b]$. But does this imply that $F$ is continuous on $[a,b]$?","Suppose $f$ is Lebesgue integrable on $[a,b]$. Let $F(x)$=$\int_{a}^x fdt$. Then prove that $F$ is continuous on $[a,b]$. I know that $F$ is continuous almost everywhere, because $F'(x)=f(x)$ almost everywhere on $[a,b]$. But does this imply that $F$ is continuous on $[a,b]$?",,['analysis']
40,Bound for a certain integration,Bound for a certain integration,,"Let $\psi$ be a smooth function with compact support, and $\phi$ is smooth and $\phi'(x) \neq 0$ for any $x$ in support of $\psi$. Define $$I(\lambda) = \int_\mathbb{R} e^{i\lambda \phi(x)}\psi(x) dx$$ for $\lambda > 0.$ Then there exists a constant $c$ such that $$|I(\lambda)| \leq c\lambda^{-a}$$ for any $a > 0$ as $\lambda \rightarrow \infty.$ I guess I should apply integration by parts to have some term related to $\lambda$, but it does not work. Any suggestion or guidance what to try ? --------------------------------- update ------------------------------ First set $$F(\psi)(\lambda) = I(\lambda),$$ then $$F(\psi')(\lambda) = \int_\mathbb{R} e^{i\lambda \phi(x)}\psi'(x) dx = (-i\lambda)F(\phi'\psi).$$ Generally, $$F(\psi^p)(\lambda) = (-i\lambda)^pF((\phi')^p\psi)$$ for $p \in \mathbb{N}.$ Then, by compactness of $\psi$, $$|\int_\mathbb{R} e^{i\lambda \phi(x)}\psi(x)(\phi')^p dx| \leq C.$$ I want to show that, somehow, $|I(\lambda)| \leq k|\int_\mathbb{R} e^{i\lambda \phi(x)}\psi(x)(\phi')^p dx| $ for some constant $k$, but I get stuck.","Let $\psi$ be a smooth function with compact support, and $\phi$ is smooth and $\phi'(x) \neq 0$ for any $x$ in support of $\psi$. Define $$I(\lambda) = \int_\mathbb{R} e^{i\lambda \phi(x)}\psi(x) dx$$ for $\lambda > 0.$ Then there exists a constant $c$ such that $$|I(\lambda)| \leq c\lambda^{-a}$$ for any $a > 0$ as $\lambda \rightarrow \infty.$ I guess I should apply integration by parts to have some term related to $\lambda$, but it does not work. Any suggestion or guidance what to try ? --------------------------------- update ------------------------------ First set $$F(\psi)(\lambda) = I(\lambda),$$ then $$F(\psi')(\lambda) = \int_\mathbb{R} e^{i\lambda \phi(x)}\psi'(x) dx = (-i\lambda)F(\phi'\psi).$$ Generally, $$F(\psi^p)(\lambda) = (-i\lambda)^pF((\phi')^p\psi)$$ for $p \in \mathbb{N}.$ Then, by compactness of $\psi$, $$|\int_\mathbb{R} e^{i\lambda \phi(x)}\psi(x)(\phi')^p dx| \leq C.$$ I want to show that, somehow, $|I(\lambda)| \leq k|\int_\mathbb{R} e^{i\lambda \phi(x)}\psi(x)(\phi')^p dx| $ for some constant $k$, but I get stuck.",,"['analysis', 'partial-differential-equations']"
41,"If $n\in N$ and $f(x)=\ln(1+x^{2n})$, then derivative $f^{(2n)}(-1)=0$.","If  and , then derivative .",n\in N f(x)=\ln(1+x^{2n}) f^{(2n)}(-1)=0,"If $n\in N$ and  $f(x)=\ln(1+x^{2n})$, then derivative  $f^{(2n)}(-1)=0$. I try: $e^{f(x)}=1+x^{2n}$ ,$(f'e^f)'=f''e^f+(f')^2e^f$ but I don't know what next.","If $n\in N$ and  $f(x)=\ln(1+x^{2n})$, then derivative  $f^{(2n)}(-1)=0$. I try: $e^{f(x)}=1+x^{2n}$ ,$(f'e^f)'=f''e^f+(f')^2e^f$ but I don't know what next.",,['analysis']
42,How to prove that Riemann-Stieltjes integral exist on subinterval,How to prove that Riemann-Stieltjes integral exist on subinterval,,"I have to prove that if $\int_{a}^{b}fdg$ exist then for every subinterval $[c,d] \subset [a,b]$ also $\int_{c}^{d}fdg$ exist.  I was thinking about using theorem that says, if we add some points to our partition of interval then we get fine inequalities for lower and upper sums. But I don't know if it's right and also I do not know how to write it properly.","I have to prove that if $\int_{a}^{b}fdg$ exist then for every subinterval $[c,d] \subset [a,b]$ also $\int_{c}^{d}fdg$ exist.  I was thinking about using theorem that says, if we add some points to our partition of interval then we get fine inequalities for lower and upper sums. But I don't know if it's right and also I do not know how to write it properly.",,"['real-analysis', 'analysis', 'definite-integrals']"
43,"Unnecessary assumption in exercise (from Spivak, Calculus on Manifolds)","Unnecessary assumption in exercise (from Spivak, Calculus on Manifolds)",,"I have a question on the following exercise (which is taken from Spivak, Calculus on Manifolds , page 105). If $\omega$ is a $1$-form $f dx$ on $[0,1]$ with $f(0) = f(1)$, show that there is a unique number $\lambda$ such that $\omega - \lambda dx = dg$ for some function $g$ with $g(0) = g(1)$. Why the assumption $f(0) = f(1)$ in the exercise, I guess it is superfluous; as I guess I have a solution without using it. So why the assumption $f(0) = f(1)$? My solution : Set $\lambda := \int_0^1 f$, then with $g(x) := \int_0^x (f - \lambda)$ everything works out as it should, and integrating $f - \lambda = g' \Leftrightarrow \omega - \lambda dx = dg$ on $[0,1]$ gives uniqueness of $\lambda$.","I have a question on the following exercise (which is taken from Spivak, Calculus on Manifolds , page 105). If $\omega$ is a $1$-form $f dx$ on $[0,1]$ with $f(0) = f(1)$, show that there is a unique number $\lambda$ such that $\omega - \lambda dx = dg$ for some function $g$ with $g(0) = g(1)$. Why the assumption $f(0) = f(1)$ in the exercise, I guess it is superfluous; as I guess I have a solution without using it. So why the assumption $f(0) = f(1)$? My solution : Set $\lambda := \int_0^1 f$, then with $g(x) := \int_0^x (f - \lambda)$ everything works out as it should, and integrating $f - \lambda = g' \Leftrightarrow \omega - \lambda dx = dg$ on $[0,1]$ gives uniqueness of $\lambda$.",,"['calculus', 'integration', 'analysis', 'differential-geometry', 'differential-forms']"
44,Is testing on $L^2 \cap L^{\infty}$ sufficient?,Is testing on  sufficient?,L^2 \cap L^{\infty},"I want to show that some operator $T:L^2  \rightarrow L^2$ is a $L^{1}$ contraction, i.e. I want to show that for all $f \in L^2 \cap L^{1}$ we have $$\|(Tf)\|_1 \le \|f\|_1.$$ To do so, I used $g \in L^2 \cap L^\infty$ and proved $$|\langle Tf ,g \rangle |\le \|f\|_1 \|g\|_\infty.$$ If $L^2 \cap L^\infty$ was dense in $L^\infty$, then this would show me by Hahn-Banach that $\|Tf\|_1\le \|f\|_1.$ But unfortunately this is not true. Does anybody know whether this conclusion is still valid?","I want to show that some operator $T:L^2  \rightarrow L^2$ is a $L^{1}$ contraction, i.e. I want to show that for all $f \in L^2 \cap L^{1}$ we have $$\|(Tf)\|_1 \le \|f\|_1.$$ To do so, I used $g \in L^2 \cap L^\infty$ and proved $$|\langle Tf ,g \rangle |\le \|f\|_1 \|g\|_\infty.$$ If $L^2 \cap L^\infty$ was dense in $L^\infty$, then this would show me by Hahn-Banach that $\|Tf\|_1\le \|f\|_1.$ But unfortunately this is not true. Does anybody know whether this conclusion is still valid?",,"['real-analysis', 'analysis', 'functional-analysis']"
45,"Equicontinuous, bounded, and closed implies pointwise compact","Equicontinuous, bounded, and closed implies pointwise compact",,"I'm trying to prove a particular version of the Arzela-Ascoli theorem. I have already gone through the general version: Let $A \subset M$ where $A$ is compact and $M$ is a metric space. Let $B \subset C_{b}(A,N)$ where $N$ is a metric space and $C_{b}(A,N)$ is the set of all bounded continuous functions from $A$ to $N$. Then $B$ is compact if and only if $B$ is equicontinuous, closed, and pointwise compact. Now I'm trying to prove the specific version. In particular I'm trying to prove the reverse direction of the following: Let $A \subset M$ where $A$ is compact and $M$ is a metric space. Let $B \subset C_{b}(A,\mathbb{R}^{m})$ where $\mathbb{R}^{m}$ is a metric space and $C_{b}(A,\mathbb{R}^{m})$ is the set of all bounded continuous functions from $A$ to $\mathbb{R}^{m}$. Then $B$ is compact if and only if $B$ is equicontinuous, closed, and bounded. I'm trying to use the general version to prove the specific version. I've managed to show the forward direction (this is trivial using the general version of the theorem). I'm trying to prove the reverse direction. So I assume equicontinuous, closed and bounded. I will be done if I can show $B$ is pointwise compact, and then apply the general version of the theorem. How can this be done (showing pointwise compactness)? Notes: $B$ is pointwise compact if for each $x \in A$, the set $B_{x} = \{ f(x) | f  \in B \} $ is compact.","I'm trying to prove a particular version of the Arzela-Ascoli theorem. I have already gone through the general version: Let $A \subset M$ where $A$ is compact and $M$ is a metric space. Let $B \subset C_{b}(A,N)$ where $N$ is a metric space and $C_{b}(A,N)$ is the set of all bounded continuous functions from $A$ to $N$. Then $B$ is compact if and only if $B$ is equicontinuous, closed, and pointwise compact. Now I'm trying to prove the specific version. In particular I'm trying to prove the reverse direction of the following: Let $A \subset M$ where $A$ is compact and $M$ is a metric space. Let $B \subset C_{b}(A,\mathbb{R}^{m})$ where $\mathbb{R}^{m}$ is a metric space and $C_{b}(A,\mathbb{R}^{m})$ is the set of all bounded continuous functions from $A$ to $\mathbb{R}^{m}$. Then $B$ is compact if and only if $B$ is equicontinuous, closed, and bounded. I'm trying to use the general version to prove the specific version. I've managed to show the forward direction (this is trivial using the general version of the theorem). I'm trying to prove the reverse direction. So I assume equicontinuous, closed and bounded. I will be done if I can show $B$ is pointwise compact, and then apply the general version of the theorem. How can this be done (showing pointwise compactness)? Notes: $B$ is pointwise compact if for each $x \in A$, the set $B_{x} = \{ f(x) | f  \in B \} $ is compact.",,['analysis']
46,a.e. continuous to left continuous,a.e. continuous to left continuous,,"Let $f:\mathbb{R} \to \mathbb{C}$ be a $\lambda$-a.e. continuous function. Is the following statement true? There exists another function $g:\mathbb{R} \to \mathbb{C}$ such that $g$ is left continuous and $f = g$ $\lambda$-a.e. If it is not, what are some conditions on $f$ for this to hold? I came up with this while comparing Riemann-Stieltjes integral to Lebesgue-Stieltjes integral. Edit: What if $f$ is BV or NBV?","Let $f:\mathbb{R} \to \mathbb{C}$ be a $\lambda$-a.e. continuous function. Is the following statement true? There exists another function $g:\mathbb{R} \to \mathbb{C}$ such that $g$ is left continuous and $f = g$ $\lambda$-a.e. If it is not, what are some conditions on $f$ for this to hold? I came up with this while comparing Riemann-Stieltjes integral to Lebesgue-Stieltjes integral. Edit: What if $f$ is BV or NBV?",,"['real-analysis', 'analysis', 'continuity', 'lebesgue-measure']"
47,"If a function is Gteaux differentiable and the Gteaux derivative is linear everywhere, is this function Frchet differentiable?","If a function is Gteaux differentiable and the Gteaux derivative is linear everywhere, is this function Frchet differentiable?",,"In our analysis 2 lecture, we learned that if a function is Gteaux differentiable, the function might not be Frchet differentiable. One example we saw was that $f: \mathbb{R}^2 \rightarrow \mathbb{R}$ defined by $$ f(x, y) = \begin{cases} \frac{x^2y}{x^4 + y^2}, x, y \ne 0 \\ 0, x = y = 0 \end{cases}$$ We know that $G(a, b) = a^2/b$ if $b \ne 0$ and $G$ is not linear everywhere so $f$ is not Frchet differentiable. But does this generally mean if $G$ is linear everywhere then $f$ is Frchet differentiable? Thanks!!!","In our analysis 2 lecture, we learned that if a function is Gteaux differentiable, the function might not be Frchet differentiable. One example we saw was that $f: \mathbb{R}^2 \rightarrow \mathbb{R}$ defined by $$ f(x, y) = \begin{cases} \frac{x^2y}{x^4 + y^2}, x, y \ne 0 \\ 0, x = y = 0 \end{cases}$$ We know that $G(a, b) = a^2/b$ if $b \ne 0$ and $G$ is not linear everywhere so $f$ is not Frchet differentiable. But does this generally mean if $G$ is linear everywhere then $f$ is Frchet differentiable? Thanks!!!",,"['analysis', 'derivatives']"
48,"If $f,g$ are analytic in the unit disk, and $|f|^2+|g|^2=1$, then $f,g$ constant.","If  are analytic in the unit disk, and , then  constant.","f,g |f|^2+|g|^2=1 f,g","I need to prove that if $f,g$ are analytic in the unit disk, and $|f|^2+|g|^2=1$ for all $z$ in the unit disk, then $f,g$ are constant. This is an exercise question so it should not be very hard, but I don't know where to start. Any hint is appreciated.","I need to prove that if $f,g$ are analytic in the unit disk, and $|f|^2+|g|^2=1$ for all $z$ in the unit disk, then $f,g$ are constant. This is an exercise question so it should not be very hard, but I don't know where to start. Any hint is appreciated.",,['complex-analysis']
49,Set Function in semi algebra countably subadditive,Set Function in semi algebra countably subadditive,,"Let $\mu: C\rightarrow [0,+\infty]$ be a set function where $C$ is a semi algebra in $X$. If $\mu$ is subadditive i.e. (If $B=\cup_{i\geq 1}A_i$ with $A_i, B$ in $C$ then $\mu(B)\leq \sum \mu(A_i)$) Then $\mu$ is monotone?","Let $\mu: C\rightarrow [0,+\infty]$ be a set function where $C$ is a semi algebra in $X$. If $\mu$ is subadditive i.e. (If $B=\cup_{i\geq 1}A_i$ with $A_i, B$ in $C$ then $\mu(B)\leq \sum \mu(A_i)$) Then $\mu$ is monotone?",,"['real-analysis', 'analysis', 'measure-theory', 'borel-cantelli-lemmas']"
50,"If $\vec{x}_k \to \vec{0}$, and $\vec{y}_k$ is bounded then $\vec{x}_k \cdot \vec{y}_k\to 0$","If , and  is bounded then",\vec{x}_k \to \vec{0} \vec{y}_k \vec{x}_k \cdot \vec{y}_k\to 0,"I would appreciate if you could please look at my proof and let me know if it's OK. Proof: $\vec{x}_k\to\vec{0}$ implies that $\forall\varepsilon>0$ , $\exists N\in \mathbb{N}$ such that $\lVert \vec{x}_k\lVert <\varepsilon$ for $k>N$ . $\vec{y}_k$ is bounded implies that $\exists C>0$ such that $\lVert \vec{y}_k\lVert \le C$ . Let $\tilde\varepsilon = \varepsilon C.$ Now, $\lvert\vec{x}_k \cdot \vec{y}_k\lvert \leq \lVert \vec{x}_k\lVert \lVert \vec{y}_k\lVert < \varepsilon C = \tilde\varepsilon $ (by Cauchy-Schwartz) for $k >N$ . Thus $\vec{x}_k \cdot \vec{y}_k\to 0$ as $k\to \infty$ .","I would appreciate if you could please look at my proof and let me know if it's OK. Proof: implies that , such that for . is bounded implies that such that . Let Now, (by Cauchy-Schwartz) for . Thus as .",\vec{x}_k\to\vec{0} \forall\varepsilon>0 \exists N\in \mathbb{N} \lVert \vec{x}_k\lVert <\varepsilon k>N \vec{y}_k \exists C>0 \lVert \vec{y}_k\lVert \le C \tilde\varepsilon = \varepsilon C. \lvert\vec{x}_k \cdot \vec{y}_k\lvert \leq \lVert \vec{x}_k\lVert \lVert \vec{y}_k\lVert < \varepsilon C = \tilde\varepsilon  k >N \vec{x}_k \cdot \vec{y}_k\to 0 k\to \infty,"['real-analysis', 'analysis']"
51,Entire function approaching zero along upper half plane,Entire function approaching zero along upper half plane,,"Suppose $f$ is entire, i.e, $\;f: \Bbb C \to \Bbb C$ is analytic. Let $\Bbb H:= \{ z: Im(z)>0\}$ be the upper half plane. Suppose that $$\lim_{\substack{z \to \infty \\ z \in \Bbb H}} f(z)=0$$ Then is it true that $\;f \equiv 0$? I can prove it in the case that $f$ sends reals to reals, because in that case $f(\bar{z}) = \overline{f(z)}$, and it is easy to proceed knowing this, together with the fact that bounded entire functions are constant. For the general case, I tried all kinds of counterexamples involving exponentials. The closest thing I can find is $f(z)=e^{iz}$. In this case $f(z) \to 0$ as $z \to \infty$ along all directions with $\arg(z) \in (0, \pi)$. But $f(2\pi+\frac{i}{n}) \to 1$ even though $2\pi+\frac{i}{n} \to \infty$ in $\Bbb H$. I'm wondering, more generally, if $A \subset [0, 2\pi)$ is a set of Lebesgue measure greater than or equal to $\pi$ such that $f(z) \to 0$ within $\{ z: \arg(z) \in A\}$, then $f \equiv 0$? Maybe there's an obvious counterexample...","Suppose $f$ is entire, i.e, $\;f: \Bbb C \to \Bbb C$ is analytic. Let $\Bbb H:= \{ z: Im(z)>0\}$ be the upper half plane. Suppose that $$\lim_{\substack{z \to \infty \\ z \in \Bbb H}} f(z)=0$$ Then is it true that $\;f \equiv 0$? I can prove it in the case that $f$ sends reals to reals, because in that case $f(\bar{z}) = \overline{f(z)}$, and it is easy to proceed knowing this, together with the fact that bounded entire functions are constant. For the general case, I tried all kinds of counterexamples involving exponentials. The closest thing I can find is $f(z)=e^{iz}$. In this case $f(z) \to 0$ as $z \to \infty$ along all directions with $\arg(z) \in (0, \pi)$. But $f(2\pi+\frac{i}{n}) \to 1$ even though $2\pi+\frac{i}{n} \to \infty$ in $\Bbb H$. I'm wondering, more generally, if $A \subset [0, 2\pi)$ is a set of Lebesgue measure greater than or equal to $\pi$ such that $f(z) \to 0$ within $\{ z: \arg(z) \in A\}$, then $f \equiv 0$? Maybe there's an obvious counterexample...",,"['complex-analysis', 'analysis', 'complex-numbers', 'cauchy-integral-formula']"
52,"Given a linear Hilbert-Schmidt embedding $$ between Hilbert spaces, prove that $^*$ is a bounded, linear operator with finite trace","Given a linear Hilbert-Schmidt embedding  between Hilbert spaces, prove that  is a bounded, linear operator with finite trace", ^*,"Let $(U,\langle\;\cdot\;,\;\cdot\;\rangle)$ be a separable Hilbert space $Q$ be a bounded, linear, nonnegative and symmetric operator on $U$ $U_0:=Q^{\frac 12}(U)$, $$\langle u,v\rangle_0:=\langle Q^{-\frac 12}u,Q^{-\frac 12}v\rangle\;\;\;\text{for }u,v\in U_0$$ where $Q^{-\frac 12}$ is the pseudo inverse of $Q^{\frac 12}$ and $(e_n)_{n\in\mathbb N}$ be an orthonormal basis of $U_0$ $(U_1,\langle\;\cdot\;,\;\cdot\;\rangle_1)$ be a separable Hilbert space and $$\iota:(U_0,\langle\;\cdot\;,\;\cdot\;\rangle_0)\to(U,\langle\;\cdot\;,\;\cdot\;\rangle)$$ be a linear Hilbert-Schmidt embedding How can we show that $$Q_1:=\iota\iota^\ast$$ is a bounded, linear, symmetric and nonnegative operator on $U_1$ with finite trace? Clearly, $$\langle Q_1u,v\rangle_1\stackrel{\text{def}}=\langle\iota\color{blue}(\iota^\ast u\color{blue}),v\rangle_1\stackrel{\text{def}}=\langle\iota^\ast u,\iota^\ast v\rangle_0\;\;\;\text{for all }u,v\in U_1\tag 1\;.$$ Thus, $\iota$ is nonnegative (since $\langle\iota^\ast u,\iota^\ast u\rangle_0\ge 0$ for all $u\in U_1$) and symmetric, since $$\langle u,Q_1v\rangle_1\stackrel{\text{Hermitian symmetry}}=\overline{\langle Q_1v,u\rangle_1}\stackrel{\text{(1)}}=\overline{\langle\iota^\ast v,\iota^\ast u\rangle_0}\stackrel{\text{Hermitian symmetry}+(1)}=\langle Q_1u,v\rangle_1\;.$$ However, I fail to prove that $Q_1$ has finite trace, i.e. $$\operatorname{tr}Q_1:=\sum_{n\in\mathbb N}\langle Q_1e_n,e_n\rangle_1<\infty\tag 2$$ for any orthonormal basis $(e_n)_{n\in\mathbb N}$ of $U_1$. How can we show $(2)$ and that $\iota$ is bounded and linear?","Let $(U,\langle\;\cdot\;,\;\cdot\;\rangle)$ be a separable Hilbert space $Q$ be a bounded, linear, nonnegative and symmetric operator on $U$ $U_0:=Q^{\frac 12}(U)$, $$\langle u,v\rangle_0:=\langle Q^{-\frac 12}u,Q^{-\frac 12}v\rangle\;\;\;\text{for }u,v\in U_0$$ where $Q^{-\frac 12}$ is the pseudo inverse of $Q^{\frac 12}$ and $(e_n)_{n\in\mathbb N}$ be an orthonormal basis of $U_0$ $(U_1,\langle\;\cdot\;,\;\cdot\;\rangle_1)$ be a separable Hilbert space and $$\iota:(U_0,\langle\;\cdot\;,\;\cdot\;\rangle_0)\to(U,\langle\;\cdot\;,\;\cdot\;\rangle)$$ be a linear Hilbert-Schmidt embedding How can we show that $$Q_1:=\iota\iota^\ast$$ is a bounded, linear, symmetric and nonnegative operator on $U_1$ with finite trace? Clearly, $$\langle Q_1u,v\rangle_1\stackrel{\text{def}}=\langle\iota\color{blue}(\iota^\ast u\color{blue}),v\rangle_1\stackrel{\text{def}}=\langle\iota^\ast u,\iota^\ast v\rangle_0\;\;\;\text{for all }u,v\in U_1\tag 1\;.$$ Thus, $\iota$ is nonnegative (since $\langle\iota^\ast u,\iota^\ast u\rangle_0\ge 0$ for all $u\in U_1$) and symmetric, since $$\langle u,Q_1v\rangle_1\stackrel{\text{Hermitian symmetry}}=\overline{\langle Q_1v,u\rangle_1}\stackrel{\text{(1)}}=\overline{\langle\iota^\ast v,\iota^\ast u\rangle_0}\stackrel{\text{Hermitian symmetry}+(1)}=\langle Q_1u,v\rangle_1\;.$$ However, I fail to prove that $Q_1$ has finite trace, i.e. $$\operatorname{tr}Q_1:=\sum_{n\in\mathbb N}\langle Q_1e_n,e_n\rangle_1<\infty\tag 2$$ for any orthonormal basis $(e_n)_{n\in\mathbb N}$ of $U_1$. How can we show $(2)$ and that $\iota$ is bounded and linear?",,"['analysis', 'functional-analysis', 'operator-theory', 'hilbert-spaces']"
53,Estimating the sum $\sum_{y \in \Bbb{Z}^d} (|y|+1)^{-\alpha}(|x-y|+1)^{-\beta}$ as $|x| \to \infty$,Estimating the sum  as,\sum_{y \in \Bbb{Z}^d} (|y|+1)^{-\alpha}(|x-y|+1)^{-\beta} |x| \to \infty,"I would like to know a rather precise asymptotics of the sum $$ S(x) = S_{\alpha,\beta}(x) :=  \sum_{y \in \Bbb{Z}^d} \frac{1}{(|y| + 1)^{\alpha}(|x-y| + 1)^{\beta}}$$ as $|x| \to \infty$. Here, $\alpha, \beta > 0$. I suspect that $$ S(x) = \Theta_{\alpha,\beta}( |x|^{-(\alpha+\beta-d)} ) $$ for each fixed $\alpha, \beta$ with $\alpha+\beta > d$. However, I would like to know about this asymptotics in a better precision, such as an expansion of the form $$ S(x) = \frac{a_0}{|x|^{\alpha+\beta-d}} + \frac{a_1}{|x|^{\alpha+\beta+1-d}} + \cdots $$ as $|x| \to \infty$, with constants $a_0, a_1, \cdots$ depending only on $d$, $\alpha$ and $\beta$. Even a keyword would be appreciated. Remark. It is easy to see that for some constants $c, C > 0$ depending only on the dimension $d$, we have $$ c^{\alpha+\beta} I(x) \leq S(x) \leq C^{\alpha+\beta} I(x), $$ where $I$ is defined as $$ I(x) = I_{\alpha,\beta}(x) := \int_{\Bbb{R}^d} \frac{1}{(|y|^2 + 1)^{\alpha/2}(|x-y|^2 + 1)^{\beta/2}} \, dy $$ This one is easier to deal with, and in fact we have $$ I(x) = \frac{2\pi^{d/2}\Gamma(\frac{\alpha+\beta-d}{2})}{\Gamma(\frac{\alpha}{2})\Gamma(\frac{\beta}{2})} \int_{0}^{\pi/2} \frac{\cos^{\alpha-1}\theta\sin^{\beta-1}\theta}{(|x|^2 \cos^2\theta\sin^2\theta + 1)^{(\alpha+\beta-d)/2}} \, d\theta. $$ This might be used to give a good guess on the leading coefficient of the expansion, but I am not confident about this. Or, recognizing $S(x)$ as a convolution might be helpful in conjunction with Fourier analysis technique, but I am even less confident about this.","I would like to know a rather precise asymptotics of the sum $$ S(x) = S_{\alpha,\beta}(x) :=  \sum_{y \in \Bbb{Z}^d} \frac{1}{(|y| + 1)^{\alpha}(|x-y| + 1)^{\beta}}$$ as $|x| \to \infty$. Here, $\alpha, \beta > 0$. I suspect that $$ S(x) = \Theta_{\alpha,\beta}( |x|^{-(\alpha+\beta-d)} ) $$ for each fixed $\alpha, \beta$ with $\alpha+\beta > d$. However, I would like to know about this asymptotics in a better precision, such as an expansion of the form $$ S(x) = \frac{a_0}{|x|^{\alpha+\beta-d}} + \frac{a_1}{|x|^{\alpha+\beta+1-d}} + \cdots $$ as $|x| \to \infty$, with constants $a_0, a_1, \cdots$ depending only on $d$, $\alpha$ and $\beta$. Even a keyword would be appreciated. Remark. It is easy to see that for some constants $c, C > 0$ depending only on the dimension $d$, we have $$ c^{\alpha+\beta} I(x) \leq S(x) \leq C^{\alpha+\beta} I(x), $$ where $I$ is defined as $$ I(x) = I_{\alpha,\beta}(x) := \int_{\Bbb{R}^d} \frac{1}{(|y|^2 + 1)^{\alpha/2}(|x-y|^2 + 1)^{\beta/2}} \, dy $$ This one is easier to deal with, and in fact we have $$ I(x) = \frac{2\pi^{d/2}\Gamma(\frac{\alpha+\beta-d}{2})}{\Gamma(\frac{\alpha}{2})\Gamma(\frac{\beta}{2})} \int_{0}^{\pi/2} \frac{\cos^{\alpha-1}\theta\sin^{\beta-1}\theta}{(|x|^2 \cos^2\theta\sin^2\theta + 1)^{(\alpha+\beta-d)/2}} \, d\theta. $$ This might be used to give a good guess on the leading coefficient of the expansion, but I am not confident about this. Or, recognizing $S(x)$ as a convolution might be helpful in conjunction with Fourier analysis technique, but I am even less confident about this.",,"['real-analysis', 'analysis', 'asymptotics']"
54,Fourier distribution $\frac{e^{i|x|}}{|x|}$,Fourier distribution,\frac{e^{i|x|}}{|x|},"I need help to calculate Fourier transform in distribution sense of $\frac{e^{i|x|}}{|x|}$ in $D'(\mathbb{R}^3)$ we have $ \frac{e^{i|x|}}{|x|} \in L^1_{loc}(\mathbb{R}^3)$ edit, Let $E(x)=\frac{e^{i|x|}}{|x|}$, i have $\langle\hat E,\phi\rangle=\langle E,\hat \phi\rangle=\int_{R^3}\int_{R^3}\frac{e^{i|\xi|}}{|\xi|}e^{-i\xi.x}\phi(x)dx\,d\xi$ Thanks","I need help to calculate Fourier transform in distribution sense of $\frac{e^{i|x|}}{|x|}$ in $D'(\mathbb{R}^3)$ we have $ \frac{e^{i|x|}}{|x|} \in L^1_{loc}(\mathbb{R}^3)$ edit, Let $E(x)=\frac{e^{i|x|}}{|x|}$, i have $\langle\hat E,\phi\rangle=\langle E,\hat \phi\rangle=\int_{R^3}\int_{R^3}\frac{e^{i|\xi|}}{|\xi|}e^{-i\xi.x}\phi(x)dx\,d\xi$ Thanks",,"['analysis', 'functional-analysis', 'fourier-analysis', 'distribution-theory']"
55,A question about general Marcinkiewicz interpolation theorem,A question about general Marcinkiewicz interpolation theorem,,"The general Marcinkiewicz interpolation theorem states as following: If $T$ is a linear operator of weak type $(p_0,q_0)$ and of weak type   $(p_1,q_1)$ where $q_0\neq q_1$, then for each $\theta\in(0,1)$, $T$   is of type $(p,q)$, for $p$ and $q$ with $p\le q$ of the form   $$\frac{1}{p} = \frac{1-\theta}{p_0}+\frac{\theta}{p_1},\quad \frac{1}{q} = \frac{1-\theta}{q_0} + \frac{\theta}{q_1}.$$ When we say an operator $T$ is of type $(p,q)$ , it means $||Tf||_q \leq C||f||_p$ for some $C$. When we say an operator $T$ is of weak type $(p,q)$ , it means $||Tf||_{q,w} \le C||f||_p$ for some $C$. Here $||||_{q,w}$ means the     best constant $C$ such that $\lambda_f(t)\le \frac{C^q}{t^q}$ holds for all $t$ and $f$, where     $\lambda$ is the distribution function. On wikipedia it says this follows from the former (the restricted from where $p_0=p_1$ and $q_0 = q_1$) through an application of Hlder's inequality and a duality argument. But I do not understand it. Can someone say something more clear?","The general Marcinkiewicz interpolation theorem states as following: If $T$ is a linear operator of weak type $(p_0,q_0)$ and of weak type   $(p_1,q_1)$ where $q_0\neq q_1$, then for each $\theta\in(0,1)$, $T$   is of type $(p,q)$, for $p$ and $q$ with $p\le q$ of the form   $$\frac{1}{p} = \frac{1-\theta}{p_0}+\frac{\theta}{p_1},\quad \frac{1}{q} = \frac{1-\theta}{q_0} + \frac{\theta}{q_1}.$$ When we say an operator $T$ is of type $(p,q)$ , it means $||Tf||_q \leq C||f||_p$ for some $C$. When we say an operator $T$ is of weak type $(p,q)$ , it means $||Tf||_{q,w} \le C||f||_p$ for some $C$. Here $||||_{q,w}$ means the     best constant $C$ such that $\lambda_f(t)\le \frac{C^q}{t^q}$ holds for all $t$ and $f$, where     $\lambda$ is the distribution function. On wikipedia it says this follows from the former (the restricted from where $p_0=p_1$ and $q_0 = q_1$) through an application of Hlder's inequality and a duality argument. But I do not understand it. Can someone say something more clear?",,"['real-analysis', 'analysis', 'lp-spaces', 'harmonic-analysis']"
56,Is there only one way to prove the Riemann Mapping Theorem?,Is there only one way to prove the Riemann Mapping Theorem?,,"After looking through several complex analysis books and online resources (e.g. see here , here , and here ), it seems that there is basically one well-known proof of the Riemann Mapping Theorem, which goes through an argument involving a certain extremal problem in a normal family of holomorphic bijections. Is this the only proof known to the mathematical community, or is there some other reason that (apparently) everyone chooses to teach the theorem this way?","After looking through several complex analysis books and online resources (e.g. see here , here , and here ), it seems that there is basically one well-known proof of the Riemann Mapping Theorem, which goes through an argument involving a certain extremal problem in a normal family of holomorphic bijections. Is this the only proof known to the mathematical community, or is there some other reason that (apparently) everyone chooses to teach the theorem this way?",,"['complex-analysis', 'analysis']"
57,How do I find a meagre dense subset of $\mathbb{R}^2$?,How do I find a meagre dense subset of ?,\mathbb{R}^2,"How do I find $M \subset \mathbb{R}^2$ which is meagre and dense? If I understand the definitions correctly, I need  $$M=\bigcup_{k \in \mathbb{N}} A_k, \quad {\overline M}=\mathbb{R}^2.$$ Trying $M=\mathbb{Q}^2$ seems to work, $A_k$ being a singleton set is rare in $\mathbb{R}^2$ for each $k$. Is this correct?","How do I find $M \subset \mathbb{R}^2$ which is meagre and dense? If I understand the definitions correctly, I need  $$M=\bigcup_{k \in \mathbb{N}} A_k, \quad {\overline M}=\mathbb{R}^2.$$ Trying $M=\mathbb{Q}^2$ seems to work, $A_k$ being a singleton set is rare in $\mathbb{R}^2$ for each $k$. Is this correct?",,"['real-analysis', 'analysis', 'proof-verification']"
58,Examples of functions which maintain the ordering of an ordered set in an interesting way.,Examples of functions which maintain the ordering of an ordered set in an interesting way.,,"I'm looking for functions $f(x, m)$ with the following property. Let $(x, y, z, ...)$ be an ordered set of positive real numbers such that $(x < y < z < \cdots)$. I'm looking for a function which for sufficiently large $N$, $f(x, N) < f(y, N) < f(z, N) < \cdots$, and for sufficiently small $n$, $f(x, n) > f(y, n) > f(z, N) > \cdots$. And for which there is no $m$, such that $n < m < N$, for which $f(x, m) = f(y, m) = f(z, m) = \cdots$. I'm looking for any functions which when given a very large parameter will maintain the sortedness of a set of numbers, and when given a very small parameter, will cause the set to be in reverse order, but which ""jumbles"" the ordering somewhere in the middle rather than simply condensing everything to a single point before the ordering is reversed. An example of a function which fails is $f(x, m) = mx$. For $m>0$ this will monotonically increase (maintaining the ordered property), and for $m<0$ it will monotonically decrease, but it fails on the last point since $m=0$ takes everything to the same point. This is just for my own interest, so anything remotely related is appreciated.","I'm looking for functions $f(x, m)$ with the following property. Let $(x, y, z, ...)$ be an ordered set of positive real numbers such that $(x < y < z < \cdots)$. I'm looking for a function which for sufficiently large $N$, $f(x, N) < f(y, N) < f(z, N) < \cdots$, and for sufficiently small $n$, $f(x, n) > f(y, n) > f(z, N) > \cdots$. And for which there is no $m$, such that $n < m < N$, for which $f(x, m) = f(y, m) = f(z, m) = \cdots$. I'm looking for any functions which when given a very large parameter will maintain the sortedness of a set of numbers, and when given a very small parameter, will cause the set to be in reverse order, but which ""jumbles"" the ordering somewhere in the middle rather than simply condensing everything to a single point before the ordering is reversed. An example of a function which fails is $f(x, m) = mx$. For $m>0$ this will monotonically increase (maintaining the ordered property), and for $m<0$ it will monotonically decrease, but it fails on the last point since $m=0$ takes everything to the same point. This is just for my own interest, so anything remotely related is appreciated.",,"['analysis', 'functions']"
59,Mapping open intervals to dense sets,Mapping open intervals to dense sets,,"Let $f$: $\mathbb{R}\longrightarrow\mathbb{R}$ be badly behaved to the extent that the image of every $(a, b)$ is dense in $\mathbb{R}$. Do such functions have a name? How much work on such functions has been done? What is a good (or not so good) reference? Thanks.","Let $f$: $\mathbb{R}\longrightarrow\mathbb{R}$ be badly behaved to the extent that the image of every $(a, b)$ is dense in $\mathbb{R}$. Do such functions have a name? How much work on such functions has been done? What is a good (or not so good) reference? Thanks.",,"['real-analysis', 'analysis', 'reference-request']"
60,Definitions of the extended real number system: Baby Rudin vs Tom M. Apostol's _Mathematical Analysis_ 2nd edition,Definitions of the extended real number system: Baby Rudin vs Tom M. Apostol's _Mathematical Analysis_ 2nd edition,,"I have of late had the chance to go through Chapter 1 of each of the following two books: Principles of Mathematical Analysis by Walter Rudin, 3rd edition Mathematical Analysis by Tom M. Apostol, 2nd edition The first chapters of both of these books is about the real and complex number systems, the extended real and complex number systems, and $\mathbb{R}^n$. In Definition 1.23, Rudin defined the extended real number system as the real field $\mathbb{R}$ and two symbols, $+\infty$ and $-\infty$, with the definitions that, for every $x \in \mathbb{R}$, $$-\infty < x < +\infty, \ \ \ x+\infty= +\infty, \ \ \ x-\infty = - \infty, \ \ \ {x \over +\infty} = {x \over -\infty} = 0, \ \ \ .$$ If $x \in \mathbb{R}$ and $x > 0$, then  $$x \cdot (+\infty) = + \infty, \ \ \ \mbox{ and } \ \ \ x \cdot (-\infty) = - \infty.$$ On the other hand, if  $x \in \mathbb{R}$ and $x < 0$, then  $$x \cdot (+\infty) = - \infty, \ \ \ \mbox{ and } \ \ \ x \cdot (-\infty) = + \infty.$$ That's all that Rudin defines. Now in Definition 1.24 in his book, Tom M. Apostol defines the extended real number system as the set of real numbers together with two symbols $+\infty$ and $-\infty$ which satisfy the following properties. (a) If $x \in \mathbb{R}$, then we have  $$x+(+\infty) = +\infty, \ \ x+(-\infty)= -\infty, \ \ x-(+\infty) = - \infty, \ \  x-(-\infty)=+\infty, \ \ {x \over +\infty}= {x \over -\infty}=0.$$ (b) If $x > 0$, then we have  $$x(+\infty)=+\infty, \ \ \ x(-\infty)=-\infty.$$ (c) If $x < 0$, then we have  $$x(+\infty)=-\infty, \ \ \ x(-\infty)=+\infty.$$ (d) $$(+\infty)+(+\infty)=(+\infty)(+\infty)=(-\infty)(-\infty)=+\infty.$$ $$(-\infty)+(-\infty)=(+\infty)(-\infty)=-\infty.$$ (e) If $x \in \mathbb{R}$, then we have $-\infty<x< +\infty$. Although both the books are about mathematical analysis, Apostol assumes much more than Rudin does about $\pm\infty$. Why? What is the justification for these discrepencies? Which definition is the more standard one? How does Rudin manage to do without what Apostol defines in part (d) above?","I have of late had the chance to go through Chapter 1 of each of the following two books: Principles of Mathematical Analysis by Walter Rudin, 3rd edition Mathematical Analysis by Tom M. Apostol, 2nd edition The first chapters of both of these books is about the real and complex number systems, the extended real and complex number systems, and $\mathbb{R}^n$. In Definition 1.23, Rudin defined the extended real number system as the real field $\mathbb{R}$ and two symbols, $+\infty$ and $-\infty$, with the definitions that, for every $x \in \mathbb{R}$, $$-\infty < x < +\infty, \ \ \ x+\infty= +\infty, \ \ \ x-\infty = - \infty, \ \ \ {x \over +\infty} = {x \over -\infty} = 0, \ \ \ .$$ If $x \in \mathbb{R}$ and $x > 0$, then  $$x \cdot (+\infty) = + \infty, \ \ \ \mbox{ and } \ \ \ x \cdot (-\infty) = - \infty.$$ On the other hand, if  $x \in \mathbb{R}$ and $x < 0$, then  $$x \cdot (+\infty) = - \infty, \ \ \ \mbox{ and } \ \ \ x \cdot (-\infty) = + \infty.$$ That's all that Rudin defines. Now in Definition 1.24 in his book, Tom M. Apostol defines the extended real number system as the set of real numbers together with two symbols $+\infty$ and $-\infty$ which satisfy the following properties. (a) If $x \in \mathbb{R}$, then we have  $$x+(+\infty) = +\infty, \ \ x+(-\infty)= -\infty, \ \ x-(+\infty) = - \infty, \ \  x-(-\infty)=+\infty, \ \ {x \over +\infty}= {x \over -\infty}=0.$$ (b) If $x > 0$, then we have  $$x(+\infty)=+\infty, \ \ \ x(-\infty)=-\infty.$$ (c) If $x < 0$, then we have  $$x(+\infty)=-\infty, \ \ \ x(-\infty)=+\infty.$$ (d) $$(+\infty)+(+\infty)=(+\infty)(+\infty)=(-\infty)(-\infty)=+\infty.$$ $$(-\infty)+(-\infty)=(+\infty)(-\infty)=-\infty.$$ (e) If $x \in \mathbb{R}$, then we have $-\infty<x< +\infty$. Although both the books are about mathematical analysis, Apostol assumes much more than Rudin does about $\pm\infty$. Why? What is the justification for these discrepencies? Which definition is the more standard one? How does Rudin manage to do without what Apostol defines in part (d) above?",,['analysis']
61,"If $0\lt y \le 1$, prove that there exists a unique positive real number $x$ such that $x^2=y$","If , prove that there exists a unique positive real number  such that",0\lt y \le 1 x x^2=y,"I'm stumped. I don't want an entire solution, just a hint. If $0\lt y \le 1$, prove that there exists a unique positive real number $x$ such that $x^2=y$ The section in the book I'm on is the least upper bound property","I'm stumped. I don't want an entire solution, just a hint. If $0\lt y \le 1$, prove that there exists a unique positive real number $x$ such that $x^2=y$ The section in the book I'm on is the least upper bound property",,"['real-analysis', 'analysis', 'proof-writing']"
62,Kuratowski measure of non-compactness of unit ball and unit sphere.,Kuratowski measure of non-compactness of unit ball and unit sphere.,,"Let X be any metric space. Let $\mathcal{M}_X$ denote the class of all bounded subsets of a metric space $X$. Definition: Let $(X,d)$ be a complete metric space. The function $\alpha:\mathcal{M}_X\rightarrow[0,\infty)$ with $(k = 1, 2, . . . , n\in\mathbb{N})$, s.t. \begin{eqnarray*} % \nonumber to remove numbering (before each equation)   \alpha(Q) &=& \inf\{\epsilon > 0: Q\subset \bigcup^n_{k=1}S_k, S_k\subseteq X, diam(S_k) <\epsilon\}\\   &=& \inf\{\epsilon>0: \text{$Q$ may be covered by finitely many sets of diameter }\leq\epsilon\} \end{eqnarray*} is called the Kuratowski measure of non-compactness. Question: If X is infinite dimensional, how to show that $$\alpha(B_1(X))=\alpha(S_1(X))=2$$ where $B_1(X)$ and $S_1(X)$ are the unit ball and unit sphere in X , respectively.","Let X be any metric space. Let $\mathcal{M}_X$ denote the class of all bounded subsets of a metric space $X$. Definition: Let $(X,d)$ be a complete metric space. The function $\alpha:\mathcal{M}_X\rightarrow[0,\infty)$ with $(k = 1, 2, . . . , n\in\mathbb{N})$, s.t. \begin{eqnarray*} % \nonumber to remove numbering (before each equation)   \alpha(Q) &=& \inf\{\epsilon > 0: Q\subset \bigcup^n_{k=1}S_k, S_k\subseteq X, diam(S_k) <\epsilon\}\\   &=& \inf\{\epsilon>0: \text{$Q$ may be covered by finitely many sets of diameter }\leq\epsilon\} \end{eqnarray*} is called the Kuratowski measure of non-compactness. Question: If X is infinite dimensional, how to show that $$\alpha(B_1(X))=\alpha(S_1(X))=2$$ where $B_1(X)$ and $S_1(X)$ are the unit ball and unit sphere in X , respectively.",,"['analysis', 'functional-analysis', 'measure-theory', 'compactness']"
63,Integral exists over the closure but not the set itself,Integral exists over the closure but not the set itself,,Is there an open set $S$ with a bounded function $f$ such that $\int_\overline{S}f$ exists but $\int_Sf$ does not?,Is there an open set $S$ with a bounded function $f$ such that $\int_\overline{S}f$ exists but $\int_Sf$ does not?,,"['analysis', 'multivariable-calculus']"
64,Upper and lower derivatives at local minimum,Upper and lower derivatives at local minimum,,"I've come across an exercise in Royden & Fitzpatrick that's got me a bit confused. It claims that if $c$ is the local minimizer for $f$ in $(a,b)$, then  $$\underline{D} f(c) \leq 0 \leq \overline{D} f(c).$$ Here $\overline{D}f(x)=\lim\limits_{h \to 0}\left[\sup\limits_{0<|t|\leq h}\frac{f(x+t)-f(x)}{t}\right]$ is the upper derivative, and $\underline{D}f(x)=\lim\limits_{h \to 0}\left[\inf\limits_{0<|t|\leq h}\frac{f(x+t)-f(x)}{t}\right]$ is the lower derivative of f. Since $c$ is the local minimizer, for a small neighborhood around $c$, $f(c+t)-f(c)>0$ for any $t\in(0,h]$, so I do not see how I can ever get the lower derivative to be negative. No matter what value $t$ takes, it seems that $\frac{f(x+t)-f(x)}{t}>0$.","I've come across an exercise in Royden & Fitzpatrick that's got me a bit confused. It claims that if $c$ is the local minimizer for $f$ in $(a,b)$, then  $$\underline{D} f(c) \leq 0 \leq \overline{D} f(c).$$ Here $\overline{D}f(x)=\lim\limits_{h \to 0}\left[\sup\limits_{0<|t|\leq h}\frac{f(x+t)-f(x)}{t}\right]$ is the upper derivative, and $\underline{D}f(x)=\lim\limits_{h \to 0}\left[\inf\limits_{0<|t|\leq h}\frac{f(x+t)-f(x)}{t}\right]$ is the lower derivative of f. Since $c$ is the local minimizer, for a small neighborhood around $c$, $f(c+t)-f(c)>0$ for any $t\in(0,h]$, so I do not see how I can ever get the lower derivative to be negative. No matter what value $t$ takes, it seems that $\frac{f(x+t)-f(x)}{t}>0$.",,"['real-analysis', 'analysis', 'measure-theory', 'derivatives', 'lebesgue-measure']"
65,Showing the outward unit normal does not depend on choice of coordinate system,Showing the outward unit normal does not depend on choice of coordinate system,,"I am doing an independent study at my school and have been studying the book Calculus on Manifolds by Spivak. There are a couple of details Spivak states are ""easy to check"" but even with the help of our professor we were not able to do so. Let $M\subset \mathbb{R}^n$ be a $k$-dimensional manifold with boundary. Then $\partial M$ is a $(k-1)$-dimensional manifold. Let $x\in \partial M$, so $(\partial M)_{x} \subset M_{x}$ is a $(k-1)$-dimensional subspace, and thus there are exactly two unit vectors in $M_{x}$ that are orthogonal to $(\partial M)_{x}$. Let $f: W \to \mathbb{R}^n$ be a coordinate system with $W \subset H^k$ and $f(0)=x$. $f$ induces a linear transformation $f_{*}: \mathbb{R}_{0}^{k}\to \mathbb{R}^n_{x}$ defined by $f_{*}(v_0)=(f^{\prime}(0)v)_{x}$. Then only one of these unit vectors can be written as $f_{*}(v_{0})$ for some $v\in \mathbb{R}^k$ with $v_k < 0$. Spivak states it can be checked that this definition does not depend on choice of coordinate system $f$, but how can I show this? Any help is much appreciated.","I am doing an independent study at my school and have been studying the book Calculus on Manifolds by Spivak. There are a couple of details Spivak states are ""easy to check"" but even with the help of our professor we were not able to do so. Let $M\subset \mathbb{R}^n$ be a $k$-dimensional manifold with boundary. Then $\partial M$ is a $(k-1)$-dimensional manifold. Let $x\in \partial M$, so $(\partial M)_{x} \subset M_{x}$ is a $(k-1)$-dimensional subspace, and thus there are exactly two unit vectors in $M_{x}$ that are orthogonal to $(\partial M)_{x}$. Let $f: W \to \mathbb{R}^n$ be a coordinate system with $W \subset H^k$ and $f(0)=x$. $f$ induces a linear transformation $f_{*}: \mathbb{R}_{0}^{k}\to \mathbb{R}^n_{x}$ defined by $f_{*}(v_0)=(f^{\prime}(0)v)_{x}$. Then only one of these unit vectors can be written as $f_{*}(v_{0})$ for some $v\in \mathbb{R}^k$ with $v_k < 0$. Spivak states it can be checked that this definition does not depend on choice of coordinate system $f$, but how can I show this? Any help is much appreciated.",,"['real-analysis', 'analysis', 'differential-geometry']"
66,"Is there a name for the ""famous"" inequality $1+x \leq e^x$?","Is there a name for the ""famous"" inequality ?",1+x \leq e^x,"Is there a name for the ""famous"" inequality $1+x \leq e^x$? It has many variants depending on how you arrange the terms: $$1 + x \leq e^x$$ $$e^{-x} -x - 1 \geq 0 $$ $$\ln(1+x) \leq x$$ Et cetera. Perhaps the simplest mnemonic device is, ""$e^x$ lies above its tangent line at the origin."" This is at least a geometric instead of arbitrary algebraic expression of the fact. It comes up in computer science and probability proofs quite frequently. In particular it is a lemma to Chernoff's bounds, and some results regarding the perceptron algorithm and Occam's razor in the PAC learning model. It is very easy to prove by drawing a graph or taking a derivative. Does it have a name? More generally I want to ask ""why"" it's so important, but this is an extremely soft question and I only hope to get used to it in time.","Is there a name for the ""famous"" inequality $1+x \leq e^x$? It has many variants depending on how you arrange the terms: $$1 + x \leq e^x$$ $$e^{-x} -x - 1 \geq 0 $$ $$\ln(1+x) \leq x$$ Et cetera. Perhaps the simplest mnemonic device is, ""$e^x$ lies above its tangent line at the origin."" This is at least a geometric instead of arbitrary algebraic expression of the fact. It comes up in computer science and probability proofs quite frequently. In particular it is a lemma to Chernoff's bounds, and some results regarding the perceptron algorithm and Occam's razor in the PAC learning model. It is very easy to prove by drawing a graph or taking a derivative. Does it have a name? More generally I want to ask ""why"" it's so important, but this is an extremely soft question and I only hope to get used to it in time.",,"['probability', 'analysis', 'inequality']"
67,Bounded sequence with repelling terms,Bounded sequence with repelling terms,,"I was wondering: Is it possible to construct a bounded sequence $\{a_n\}$ of real numbers satisfying $|a_n-a_{n+k}| > 1/k \, \forall k \geq 1$? I've tried to come up with such a sequence, to no avail.  But I have no idea how to disprove this, either.","I was wondering: Is it possible to construct a bounded sequence $\{a_n\}$ of real numbers satisfying $|a_n-a_{n+k}| > 1/k \, \forall k \geq 1$? I've tried to come up with such a sequence, to no avail.  But I have no idea how to disprove this, either.",,"['real-analysis', 'sequences-and-series', 'analysis']"
68,$L^p(\mathbb{R})$ separable.,separable.,L^p(\mathbb{R}),"I'm trying to prove $L^p(\mathbb{R})$, $p \in [1,\infty)$ is separable by showing the collection$$ S:= \{\sum_{i=1}^nr\chi_{(a_i,b_i)}\}_{(a_i,b_i,r) \in \mathbb{Q}^3}$$ is dense in $L^p$. So, since simple functions are dense in $L^p$ given $f \in L^p$, Let $\epsilon >0$ and choose $s$ such that $\|s-f\|_{L^p} < \frac{\epsilon}{2}$. Now, let $$s = \sum_{i=1}^{n} c_i \chi_{E_i}$$ with $E_i$ pairwise disjoint, wlog. Let $r \in S$ with $$r = \sum_{i=1}^{n} r_i \chi_{A_i}$$  be such that $|r_i - c_i| <1$ and $\mu((E_i\setminus A_i) \cup (A_i\setminus E_i)) < \frac{\epsilon^p}{2n}$ with the $A_i$ pairwise disjoint, wlog. Then, \begin{eqnarray*} \left( \int_\mathbb{R} |s-r|^p \, d\mu \right)^\frac{1}{p} &\leq& \left( \int_\mathbb{R} \left(\sum_{i=1}^{n}|c_i\chi_{E_i}-r\chi_{A_i}| \right)^p \, d\mu  \right)^\frac{1}{p} \\ & \stackrel{Minkowski}{\leq}& \sum_{i=1}^{n} \left( \int_\mathbb{R} |c_i\chi_{E_i}-r_i\chi_{A_i}|^p \, d\mu \right)^\frac{1}{p} \\ &<& \sum_{i=1}^{n} \left( \int_\mathbb{R} |\chi_{(E_i\setminus A_i) \cup (A_i\setminus E_i)}|^p \, d\mu \right)^\frac{1}{p} \\ &=& \sum_{i=1}^{n} \left( \int_{(E_i\setminus A_i) \cup (A_i\setminus E_i)} 1 \, d\mu \right)^\frac{1}{p} \\ &<& \sum_{i=1}^{n} (\frac{\epsilon^p}{n})^\frac{1}{p} = \frac{\epsilon}{2} \end{eqnarray*} since $$|c_i\chi_{E_i} - r_i\chi_{A_i}| = \begin{cases}   0 \quad \,:  x \notin A_i \cup E_i, x \in A_i \cap E_i\\ |c_i-r_i| \quad \, : x \in  (E_i \setminus A_i) \cup (A_i \setminus E_i). \\ \end{cases} < \chi_{(E_i\setminus A_i) \cup (A_i\setminus E_i)} $$ and therefore $\|f-r\|_{L^p} < \epsilon$, as desired. Is this correct? Do I need fix anything? Are the assumptions I've made about disjointness okay?","I'm trying to prove $L^p(\mathbb{R})$, $p \in [1,\infty)$ is separable by showing the collection$$ S:= \{\sum_{i=1}^nr\chi_{(a_i,b_i)}\}_{(a_i,b_i,r) \in \mathbb{Q}^3}$$ is dense in $L^p$. So, since simple functions are dense in $L^p$ given $f \in L^p$, Let $\epsilon >0$ and choose $s$ such that $\|s-f\|_{L^p} < \frac{\epsilon}{2}$. Now, let $$s = \sum_{i=1}^{n} c_i \chi_{E_i}$$ with $E_i$ pairwise disjoint, wlog. Let $r \in S$ with $$r = \sum_{i=1}^{n} r_i \chi_{A_i}$$  be such that $|r_i - c_i| <1$ and $\mu((E_i\setminus A_i) \cup (A_i\setminus E_i)) < \frac{\epsilon^p}{2n}$ with the $A_i$ pairwise disjoint, wlog. Then, \begin{eqnarray*} \left( \int_\mathbb{R} |s-r|^p \, d\mu \right)^\frac{1}{p} &\leq& \left( \int_\mathbb{R} \left(\sum_{i=1}^{n}|c_i\chi_{E_i}-r\chi_{A_i}| \right)^p \, d\mu  \right)^\frac{1}{p} \\ & \stackrel{Minkowski}{\leq}& \sum_{i=1}^{n} \left( \int_\mathbb{R} |c_i\chi_{E_i}-r_i\chi_{A_i}|^p \, d\mu \right)^\frac{1}{p} \\ &<& \sum_{i=1}^{n} \left( \int_\mathbb{R} |\chi_{(E_i\setminus A_i) \cup (A_i\setminus E_i)}|^p \, d\mu \right)^\frac{1}{p} \\ &=& \sum_{i=1}^{n} \left( \int_{(E_i\setminus A_i) \cup (A_i\setminus E_i)} 1 \, d\mu \right)^\frac{1}{p} \\ &<& \sum_{i=1}^{n} (\frac{\epsilon^p}{n})^\frac{1}{p} = \frac{\epsilon}{2} \end{eqnarray*} since $$|c_i\chi_{E_i} - r_i\chi_{A_i}| = \begin{cases}   0 \quad \,:  x \notin A_i \cup E_i, x \in A_i \cap E_i\\ |c_i-r_i| \quad \, : x \in  (E_i \setminus A_i) \cup (A_i \setminus E_i). \\ \end{cases} < \chi_{(E_i\setminus A_i) \cup (A_i\setminus E_i)} $$ and therefore $\|f-r\|_{L^p} < \epsilon$, as desired. Is this correct? Do I need fix anything? Are the assumptions I've made about disjointness okay?",,"['real-analysis', 'integration', 'analysis', 'lp-spaces', 'separable-spaces']"
69,"Measure Theory: Prove that $\mu( \cap_{k \in \mathbb{N}}B_k)=1$ given $\mu(B_k)=\mu(B)=1$, $B_k \subset B$ for all $k \in \mathbb{N}$","Measure Theory: Prove that  given ,  for all",\mu( \cap_{k \in \mathbb{N}}B_k)=1 \mu(B_k)=\mu(B)=1 B_k \subset B k \in \mathbb{N},"Problem: For a measurable space $(\Omega, \mathcal{A}, \mu)$ with measure $\mu: \mathcal{A} \to [0, \infty]$ Let $B \in \mathcal{A}$ and $(B_k)_{k \in \mathbb{N}} \subset \mathcal{A}$ be a sequence such that $B_k \subset B$ for all $k \in \mathbb{N}$. Prove that if $\mu(B)=\mu(B_k)=1$ for all $k$ then $\mu( \cap_{k \in \mathbb{N}}B_k)=1$ My approach : It doesn't take much effort to show that $$ \mu \left(  \bigcap_{k \in \mathbb{N}} B_k\right) \leq 1 $$ Because certainly: $$\bigcap_{k \in \mathbb{N}} B_k \subset B_k \text{ for every } k \in \mathbb{N} \implies \mu \left(  \bigcap_{k \in \mathbb{N}} B_k\right) \leq \mu(B_k)=1 $$ because the measure $\mu$ is monotone. My problem is the less obvious inequality, namely $$  \mu \left(\bigcap_{k \in \mathbb{N}} B_k \right) \geq 1 $$ I haven't made use of the fact that $\mu(B)=1$ but I also don't believe that $$\bigcup_{k \in \mathbb{N}} B_k = B $$ is true. Any hints for the missing inequality? The problem at hand clearly being that $\cap_{k \in \mathbb{N}} B_k$ cannot easily be approximated from below, thus the only possible way might be to bring the strict inequality $$ \mu \left( \bigcap_{k \in \mathbb{N}} B_k \right)< 1 $$ to a contradiction.","Problem: For a measurable space $(\Omega, \mathcal{A}, \mu)$ with measure $\mu: \mathcal{A} \to [0, \infty]$ Let $B \in \mathcal{A}$ and $(B_k)_{k \in \mathbb{N}} \subset \mathcal{A}$ be a sequence such that $B_k \subset B$ for all $k \in \mathbb{N}$. Prove that if $\mu(B)=\mu(B_k)=1$ for all $k$ then $\mu( \cap_{k \in \mathbb{N}}B_k)=1$ My approach : It doesn't take much effort to show that $$ \mu \left(  \bigcap_{k \in \mathbb{N}} B_k\right) \leq 1 $$ Because certainly: $$\bigcap_{k \in \mathbb{N}} B_k \subset B_k \text{ for every } k \in \mathbb{N} \implies \mu \left(  \bigcap_{k \in \mathbb{N}} B_k\right) \leq \mu(B_k)=1 $$ because the measure $\mu$ is monotone. My problem is the less obvious inequality, namely $$  \mu \left(\bigcap_{k \in \mathbb{N}} B_k \right) \geq 1 $$ I haven't made use of the fact that $\mu(B)=1$ but I also don't believe that $$\bigcup_{k \in \mathbb{N}} B_k = B $$ is true. Any hints for the missing inequality? The problem at hand clearly being that $\cap_{k \in \mathbb{N}} B_k$ cannot easily be approximated from below, thus the only possible way might be to bring the strict inequality $$ \mu \left( \bigcap_{k \in \mathbb{N}} B_k \right)< 1 $$ to a contradiction.",,"['analysis', 'measure-theory']"
70,Fundamental theorem of algebra in different functional form,Fundamental theorem of algebra in different functional form,,"Consider the polynomial function: $f(x)=c_0+c_1x+c_2 x^2+\cdots+c_{n-1}x^{n-1}+x^n$, with $x$ and $c_0,c_1,c_2,\ldots,c_{n-1}$ are complex numbers. $|f(x)|$ is continuous and there exists closed and compact disc $Q:=\{ x: |x|\le P_0\}$, with $\inf_{x\in Q} |f(x)|$ exists at some $x_0$ in the interior of $Q$. Each circle $x_0+we^{i\theta}$ of radius $w$ centered at $x_0$ lies interior of $Q$ for all small $w>0$, $0\le\theta <2\pi$. Let the new polynomial be $h(v):=f(x_0+v)=b_0+b_1v+b_2 v^2+\cdots+b_{n-1}v^{n-1}+v^n$, where $v:=we^{i\theta}$, with $b_0=f(x_0)$ and other coefficients in the form of $c_0...c_{n-1},x_0$. Let $b_t \neq0,t\ge1$ be the first coefficient not equal to zero (since it may occur that $b_1=0$). Show that we have $|b_{t+1}v^{t+1}+b_{t+2}v^{t+2}+\cdots+ b_{n-1}v^{n-1}+v^n|<|b_t||v|^t$ if we pick $w=|v|$ small enough. Provide the magnitude of $w$. I used the Euler's formula of $e^{i\theta}=\cos(\theta)+i\sin(\theta)$ and the fact that $e^{i(\theta+\gamma)}=e^{i\theta}e^{i\gamma}$ and plug in $v:=we^{i\theta}$ to rewrite the inequality as: $|(b_{t+1}w^{t+1}e^{i\theta}+b_{t+2}w^{t+2}e^{2i\theta}+\cdots+b_{n-1}w^{n-1}e^{(n-j-1) i\theta}+w^ne^{(n-j)i\theta})||e^{i\theta t}|<|b_t||w^te^{i\theta t}|$ but don't know what to do next or if the steps are wrong.","Consider the polynomial function: $f(x)=c_0+c_1x+c_2 x^2+\cdots+c_{n-1}x^{n-1}+x^n$, with $x$ and $c_0,c_1,c_2,\ldots,c_{n-1}$ are complex numbers. $|f(x)|$ is continuous and there exists closed and compact disc $Q:=\{ x: |x|\le P_0\}$, with $\inf_{x\in Q} |f(x)|$ exists at some $x_0$ in the interior of $Q$. Each circle $x_0+we^{i\theta}$ of radius $w$ centered at $x_0$ lies interior of $Q$ for all small $w>0$, $0\le\theta <2\pi$. Let the new polynomial be $h(v):=f(x_0+v)=b_0+b_1v+b_2 v^2+\cdots+b_{n-1}v^{n-1}+v^n$, where $v:=we^{i\theta}$, with $b_0=f(x_0)$ and other coefficients in the form of $c_0...c_{n-1},x_0$. Let $b_t \neq0,t\ge1$ be the first coefficient not equal to zero (since it may occur that $b_1=0$). Show that we have $|b_{t+1}v^{t+1}+b_{t+2}v^{t+2}+\cdots+ b_{n-1}v^{n-1}+v^n|<|b_t||v|^t$ if we pick $w=|v|$ small enough. Provide the magnitude of $w$. I used the Euler's formula of $e^{i\theta}=\cos(\theta)+i\sin(\theta)$ and the fact that $e^{i(\theta+\gamma)}=e^{i\theta}e^{i\gamma}$ and plug in $v:=we^{i\theta}$ to rewrite the inequality as: $|(b_{t+1}w^{t+1}e^{i\theta}+b_{t+2}w^{t+2}e^{2i\theta}+\cdots+b_{n-1}w^{n-1}e^{(n-j-1) i\theta}+w^ne^{(n-j)i\theta})||e^{i\theta t}|<|b_t||w^te^{i\theta t}|$ but don't know what to do next or if the steps are wrong.",,"['real-analysis', 'complex-analysis', 'analysis', 'inequality', 'complex-numbers']"
71,"Compute the limit of $\int_\mathbb{R} f(x)\sin (nx)$ when $n\to\infty$, for $f \in L^1$","Compute the limit of  when , for",\int_\mathbb{R} f(x)\sin (nx) n\to\infty f \in L^1,"Let $f \in L^1(\mathbb{R})$. Find   $$ \lim_{n \rightarrow \infty} \int_{-\infty}^\infty f(x)\sin(nx) dx \,. $$ LDCT is a no go, as well as MCT and FL, which are really the only integration techniques we've developed thus far in the course. Integration by parts isn't applicable either since we just have $f \in L^1$. I've tried splitting both $f$ and $\sin(nx)$ into unsigned parts but that didn't seem to help. I don't think using the density of $C_c(\mathbb{R}) \subset L^1$ will help either. I've tried approximating $f$ by simple functions, but that didn't seem to do the trick either.. To give you a gauge in regards to where we are at, we just finished basic properties of $L^p$ spaces and will begin Hilbert Space Theory as in Big Rudin. Any hints are greatly appreciated. Edit: Let $h \in C^1_c(\mathbb{R})$ be such that $\sup \limits_{x \in X}|f(x)-h(x)| < \epsilon.$ Let $\operatorname{supp}(h) = [a,b]$. Then, \begin{eqnarray*} \int_{\mathbb{R}} h(x)\sin(nx) \, dx &=& \int_{[a,b]} h(x)\sin(nx) \, dx \\ &=& \left[h(x) \frac{\cos(nx)}{n} \right] + \int_{[a,b]} \frac{1}{n} f'(x) \cos(nx) \, dx \\ \end{eqnarray*} note that since $f'$ is continuous on $[a,b]$, it is bounded by some $M$. Thus, $\frac{1}{n}f'(x)\cos(nx) \leq \frac{M}{n}$, which converges to $0$ uniformly. Taking limits yields that $$\lim_{n \rightarrow \infty} \int_{-\infty}^\infty f(x)\sin(nx) dx = 0.$$ Alternatively, if we use a simple function approximation, we end up with something of the form $$\int_\mathbb{R} \sum_{i=1}^{k}c_i \chi_{E_i}(x)\sin(nx)dx = \sum_{i=1}^k c_i \int_{E_i} \sin(nx)dx$$ which goes to $0$ as $n \to \infty$ (The $E_i$'s can be chosen to be disjoint), but the below question still remains. How does this follow from the fact that we've only used a dense subset?","Let $f \in L^1(\mathbb{R})$. Find   $$ \lim_{n \rightarrow \infty} \int_{-\infty}^\infty f(x)\sin(nx) dx \,. $$ LDCT is a no go, as well as MCT and FL, which are really the only integration techniques we've developed thus far in the course. Integration by parts isn't applicable either since we just have $f \in L^1$. I've tried splitting both $f$ and $\sin(nx)$ into unsigned parts but that didn't seem to help. I don't think using the density of $C_c(\mathbb{R}) \subset L^1$ will help either. I've tried approximating $f$ by simple functions, but that didn't seem to do the trick either.. To give you a gauge in regards to where we are at, we just finished basic properties of $L^p$ spaces and will begin Hilbert Space Theory as in Big Rudin. Any hints are greatly appreciated. Edit: Let $h \in C^1_c(\mathbb{R})$ be such that $\sup \limits_{x \in X}|f(x)-h(x)| < \epsilon.$ Let $\operatorname{supp}(h) = [a,b]$. Then, \begin{eqnarray*} \int_{\mathbb{R}} h(x)\sin(nx) \, dx &=& \int_{[a,b]} h(x)\sin(nx) \, dx \\ &=& \left[h(x) \frac{\cos(nx)}{n} \right] + \int_{[a,b]} \frac{1}{n} f'(x) \cos(nx) \, dx \\ \end{eqnarray*} note that since $f'$ is continuous on $[a,b]$, it is bounded by some $M$. Thus, $\frac{1}{n}f'(x)\cos(nx) \leq \frac{M}{n}$, which converges to $0$ uniformly. Taking limits yields that $$\lim_{n \rightarrow \infty} \int_{-\infty}^\infty f(x)\sin(nx) dx = 0.$$ Alternatively, if we use a simple function approximation, we end up with something of the form $$\int_\mathbb{R} \sum_{i=1}^{k}c_i \chi_{E_i}(x)\sin(nx)dx = \sum_{i=1}^k c_i \int_{E_i} \sin(nx)dx$$ which goes to $0$ as $n \to \infty$ (The $E_i$'s can be chosen to be disjoint), but the below question still remains. How does this follow from the fact that we've only used a dense subset?",,"['real-analysis', 'integration', 'analysis', 'measure-theory']"
72,"$Q=A\times B$. if $\int_Q f$ exists, then $\int_{y\in B}f(x,y)$ exists for $x\in A-D$, where $D$ is a set of measure zero in $\mathbb{R^k}$.",". if  exists, then  exists for , where  is a set of measure zero in .","Q=A\times B \int_Q f \int_{y\in B}f(x,y) x\in A-D D \mathbb{R^k}","Let $A$ be a rectangle in $\mathbb{R^k}$; let $B$ be a rectangle in $\mathbb{R^n}$; let $Q=A\times B$. Let $f: Q\to \mathbb{R}$ be a bounded function. Show that if $\int_Q f$ exists, then $$\int_{y\in B}f(x,y)$$ exists for $x\in A-D$, where $D$ is a set of measure zero in $\mathbb{R^k}$. I'm having difficulty proving this. I think I might have to use these theorems in the proof. My work: Let $D$ be a set of measure zero in $\mathbb{R^k}$, and $x\in A-D$. Since $f$ is integrable on $Q$, $f$ is continuous except in a measure zero set, say $E$. Claim: Let $B$ be a rectangle in $\mathbb{R^n}$. If $D$ is a set of measure zero in $\mathbb{R^k}$, then $D\times B$ is a set of measure zero in $\mathbb{R^{k+n}}$. Proof of Claim: Let $\epsilon\gt 0$ be given.Since $D$ is a set of measure zero in $\mathbb{R^k}$, there exists a countable set of rectangles $\{Q_i\}$ whose union covers $D$ and the sum of the volumes is less than $\epsilon/v(B)$, where $v(B)$ is the volume of $B$. Now consider the countable set of rectangles $Q_i \times B$. Then the union of all of these rectangles cover $D\times B$, and $\sum v(Q_i\times B)=\sum v(Q_i)\times v(B)\lt \epsilon$. QED. Hence by the claim above, $E \cup (D\times B)$ is a set of measure zero in $\mathbb{R^{k+n}}$. Hence, $f$ is continuous almost everywhere when $x\in A-D$. Now for a fixed $x\in A-D$, $f$ is continuous almost everywhere in $\mathbb{R^{k+n}}$. However, to complete the proof of this problem, I need to show that $f(x,y)$, for a fixed $x\in A-D$ is continuous almost everywhere in $\mathbb{R^n}$, but I don't know how to show this part. I would greatly appreciate any hints, suggestions or solutions.","Let $A$ be a rectangle in $\mathbb{R^k}$; let $B$ be a rectangle in $\mathbb{R^n}$; let $Q=A\times B$. Let $f: Q\to \mathbb{R}$ be a bounded function. Show that if $\int_Q f$ exists, then $$\int_{y\in B}f(x,y)$$ exists for $x\in A-D$, where $D$ is a set of measure zero in $\mathbb{R^k}$. I'm having difficulty proving this. I think I might have to use these theorems in the proof. My work: Let $D$ be a set of measure zero in $\mathbb{R^k}$, and $x\in A-D$. Since $f$ is integrable on $Q$, $f$ is continuous except in a measure zero set, say $E$. Claim: Let $B$ be a rectangle in $\mathbb{R^n}$. If $D$ is a set of measure zero in $\mathbb{R^k}$, then $D\times B$ is a set of measure zero in $\mathbb{R^{k+n}}$. Proof of Claim: Let $\epsilon\gt 0$ be given.Since $D$ is a set of measure zero in $\mathbb{R^k}$, there exists a countable set of rectangles $\{Q_i\}$ whose union covers $D$ and the sum of the volumes is less than $\epsilon/v(B)$, where $v(B)$ is the volume of $B$. Now consider the countable set of rectangles $Q_i \times B$. Then the union of all of these rectangles cover $D\times B$, and $\sum v(Q_i\times B)=\sum v(Q_i)\times v(B)\lt \epsilon$. QED. Hence by the claim above, $E \cup (D\times B)$ is a set of measure zero in $\mathbb{R^{k+n}}$. Hence, $f$ is continuous almost everywhere when $x\in A-D$. Now for a fixed $x\in A-D$, $f$ is continuous almost everywhere in $\mathbb{R^{k+n}}$. However, to complete the proof of this problem, I need to show that $f(x,y)$, for a fixed $x\in A-D$ is continuous almost everywhere in $\mathbb{R^n}$, but I don't know how to show this part. I would greatly appreciate any hints, suggestions or solutions.",,"['calculus', 'real-analysis', 'integration', 'analysis', 'multivariable-calculus']"
73,Equilateral Triangle from three complex points,Equilateral Triangle from three complex points,,"I need some help proving this, I've seen it proven in the other direction (prove the formula if it is an equilateral) but cant figure out how to prove it this way around. Given three complex numbers $z_1, z_2, z_3$ prove that the points $z_1, z_2, z_3$ are vertices of an equilateral triangle in $\Bbb C$, if $$z_1^2 + z_2^2 + z_3^2 = z_1z_2 + z_1z_3 + z_2z_3$$","I need some help proving this, I've seen it proven in the other direction (prove the formula if it is an equilateral) but cant figure out how to prove it this way around. Given three complex numbers $z_1, z_2, z_3$ prove that the points $z_1, z_2, z_3$ are vertices of an equilateral triangle in $\Bbb C$, if $$z_1^2 + z_2^2 + z_3^2 = z_1z_2 + z_1z_3 + z_2z_3$$",,['complex-analysis']
74,"$f$, $g$ is measurable, then $fg$ is measurable.",",  is measurable, then  is measurable.",f g fg,"I'm trying to show that, given $f$ and $g$ measurable, $fg$ is also measurable. Is it true that $$\{x : fg(x) > c\} = \left( \bigcup_{\substack{a,b \in \mathbb{Q}^+\\ ab \geq c}} \{x: f(x) > a \} \cap \{x : g(x) > b\} \right) \cup \left( \bigcup_{\substack{a \in \mathbb{Q}^-\\ b \in \mathbb{Q}^+ \\ ab \geq c}} \{x: f(x) < a \} \cap \{x : g(x) > b\} \right) \cup \left( \bigcup_{\substack{a \in \mathbb{Q}^+\\ b \in \mathbb{Q}^- \\ ab \geq c}} \{x: f(x) > a \} \cap \{x : g(x) < b\} \right) \cup \left( \bigcup_{\substack{a,b \in \mathbb{Q}^-\\ ab \geq c}} \{x: f(x) < a \} \cap \{x : g(x) < b\} \right)?$$ As of now I'm still trying to work through this problem, so hints, not answers would be appreciated, as well as if the above is correct. If the above is correct and there is a more compact way to write this, then I'd be happy to hear that as well.","I'm trying to show that, given $f$ and $g$ measurable, $fg$ is also measurable. Is it true that $$\{x : fg(x) > c\} = \left( \bigcup_{\substack{a,b \in \mathbb{Q}^+\\ ab \geq c}} \{x: f(x) > a \} \cap \{x : g(x) > b\} \right) \cup \left( \bigcup_{\substack{a \in \mathbb{Q}^-\\ b \in \mathbb{Q}^+ \\ ab \geq c}} \{x: f(x) < a \} \cap \{x : g(x) > b\} \right) \cup \left( \bigcup_{\substack{a \in \mathbb{Q}^+\\ b \in \mathbb{Q}^- \\ ab \geq c}} \{x: f(x) > a \} \cap \{x : g(x) < b\} \right) \cup \left( \bigcup_{\substack{a,b \in \mathbb{Q}^-\\ ab \geq c}} \{x: f(x) < a \} \cap \{x : g(x) < b\} \right)?$$ As of now I'm still trying to work through this problem, so hints, not answers would be appreciated, as well as if the above is correct. If the above is correct and there is a more compact way to write this, then I'd be happy to hear that as well.",,"['real-analysis', 'analysis', 'measure-theory']"
75,Cauchy sequence that satisfies $\|x_{k+2}-x_{k+1}\|\le\theta\|x_{k+1}-x_k\|$,Cauchy sequence that satisfies,\|x_{k+2}-x_{k+1}\|\le\theta\|x_{k+1}-x_k\|,"Suppose the sequence $\{x_k\}_{k=1}^\infty\subset\mathbb{R}^n$ satisfies $\|x_{k+2}-x_{k+1}\|\le\theta\|x_{k+1}-x_k\|$ for all $k\ge1$, with $0<\theta<1$. Show that $\{x_k\}$ is a Cauchy sequence and hence converges. So I did some rearranging to try and get something that worked and got this: For $m,n>1$ such that $m<n$ $$\|x_n-x_m\|=\|x_n+\left(\sum_{i=m+1}^{n-1}(x_i-x_i)\right)-x_m\|$$ $$\le \sum_{i=m+1}^n\|x_i-x_{i-1}\|\text{ (triangle inequality)}$$ $$\le\left(\sum_{i=0}^{n-m}\theta^i\right)\|x_{m+1}-x_m\|$$ Is it sufficient to choose $N$ for any $\epsilon$ such that $\|x_{N+1}-x_N\|<\epsilon$ and since $\left(\sum_{i=0}^{n-m}\theta^i\right)<1$ I can use the above derivation?","Suppose the sequence $\{x_k\}_{k=1}^\infty\subset\mathbb{R}^n$ satisfies $\|x_{k+2}-x_{k+1}\|\le\theta\|x_{k+1}-x_k\|$ for all $k\ge1$, with $0<\theta<1$. Show that $\{x_k\}$ is a Cauchy sequence and hence converges. So I did some rearranging to try and get something that worked and got this: For $m,n>1$ such that $m<n$ $$\|x_n-x_m\|=\|x_n+\left(\sum_{i=m+1}^{n-1}(x_i-x_i)\right)-x_m\|$$ $$\le \sum_{i=m+1}^n\|x_i-x_{i-1}\|\text{ (triangle inequality)}$$ $$\le\left(\sum_{i=0}^{n-m}\theta^i\right)\|x_{m+1}-x_m\|$$ Is it sufficient to choose $N$ for any $\epsilon$ such that $\|x_{N+1}-x_N\|<\epsilon$ and since $\left(\sum_{i=0}^{n-m}\theta^i\right)<1$ I can use the above derivation?",,"['sequences-and-series', 'analysis', 'cauchy-sequences']"
76,"Show that $\{x \in \mathbb{Q}:x \geq 0, x^2 \leq 2\}$ has no rational least upper bound.",Show that  has no rational least upper bound.,"\{x \in \mathbb{Q}:x \geq 0, x^2 \leq 2\}",Lets denote the least upper bound by  $\alpha \in \mathbb{Q}$ and $\delta > 0$ be a small number. Now $\alpha^2 \neq 2$ because there is no such rational $\alpha$. If $\alpha^2 > 2$ then $(\alpha +\delta)^2 >2$ and so $\alpha$ is not a least upper bound. I can't obtain a valid reason why $\alpha^2 < 2$ can not be the case.,Lets denote the least upper bound by  $\alpha \in \mathbb{Q}$ and $\delta > 0$ be a small number. Now $\alpha^2 \neq 2$ because there is no such rational $\alpha$. If $\alpha^2 > 2$ then $(\alpha +\delta)^2 >2$ and so $\alpha$ is not a least upper bound. I can't obtain a valid reason why $\alpha^2 < 2$ can not be the case.,,[]
77,Upper bound for the Dirichlet kernel,Upper bound for the Dirichlet kernel,,"I'd like to to prove the following statement: For every $N \geq 1$, there exists $C>0$ such that $|D_N(t)| \leq C|t|^{-1}$, for $|t|<\frac{1}{2}$, where $D_N(t) = \sum_{k=-N}^N e^{2\pi ikt} = \frac{\sin{((2N+1) \pi x)}}{\sin(\pi x)}$ and $\{ D_N \}$ is the Dirichlet kernel. Not sure how to approach this, but here is my work: Since $\frac{\pi t}{\sin(\pi t)} \to 1$, there exists some $\delta > 0$ such that  $\left| \frac{\pi t}{\sin (\pi t)} - 1 \right| < 1$ whenever $|t| < \delta$. From here we get $0 < \frac{\pi t}{\sin (\pi t)} < 2$, i.e., $\frac{1}{|\sin(\pi t)|} \leq \frac{2}{\pi} \frac{1}{|t|}$. It follows that $|D_N(t)| \leq \frac{2}{\pi}\frac{1}{|t|}$. This is almost what I want, except that if $\delta < \frac{1}{2}$, I don't know how to cover the $\delta < |t| < \frac{1}{2}$ case. Thanks in advance for any help.","I'd like to to prove the following statement: For every $N \geq 1$, there exists $C>0$ such that $|D_N(t)| \leq C|t|^{-1}$, for $|t|<\frac{1}{2}$, where $D_N(t) = \sum_{k=-N}^N e^{2\pi ikt} = \frac{\sin{((2N+1) \pi x)}}{\sin(\pi x)}$ and $\{ D_N \}$ is the Dirichlet kernel. Not sure how to approach this, but here is my work: Since $\frac{\pi t}{\sin(\pi t)} \to 1$, there exists some $\delta > 0$ such that  $\left| \frac{\pi t}{\sin (\pi t)} - 1 \right| < 1$ whenever $|t| < \delta$. From here we get $0 < \frac{\pi t}{\sin (\pi t)} < 2$, i.e., $\frac{1}{|\sin(\pi t)|} \leq \frac{2}{\pi} \frac{1}{|t|}$. It follows that $|D_N(t)| \leq \frac{2}{\pi}\frac{1}{|t|}$. This is almost what I want, except that if $\delta < \frac{1}{2}$, I don't know how to cover the $\delta < |t| < \frac{1}{2}$ case. Thanks in advance for any help.",,['analysis']
78,Doubt about inequalities,Doubt about inequalities,,"My original question is if $f\in \mathcal{S}_{\alpha_1}^{\beta_1},\: g \in \mathcal{S}_{\alpha_2}^{\beta_2}$, where does $(f\cdot g) (x)=f(x)g(x)$ belong ? where $\mathcal{S}_\alpha^\beta$ is defined here . To address this we proceed as follows: $f\in \mathcal{S}_{\alpha_1}^{\beta_1},\: g \in \mathcal{S}_{\alpha_2}^{\beta_2}\implies$ $$\sup_x|f(x)|\exp(k_1|x|^{1/\alpha_1}) < \infty \\ \sup_x|g(x)|\exp(k_2|x|^{1/\alpha_2}) < \infty$$ Multiplying both the inequalities we have $$\sup_x|f(x)g(x)|\exp(k_1|x|^{1/\alpha_1}+k_2|x|^{1/\alpha_2}) < \infty -(1)$$ Now, $k_1|x|^{1/\alpha_1} \le k_1|x|^{\max\{1/\alpha_1,1/\alpha_2\}}$. Similarly  $k_2|x|^{1/\alpha_2} \le k_2|x|^{\max\{1/\alpha_1,1/\alpha_2\}}$. hence $$\sup_x|f(x)g(x)|\exp(k_1|x|^{1/\alpha_1}+k_2|x|^{1/\alpha_2})\le\sup_x|f(x)g(x)|\exp(k|x|^{\max\{1/\alpha_1,1/\alpha_2\}}\:)-(2)$$ where $k=k_1+k_2$. My question: How do I show that $$\sup_x|f(x)g(x)|\exp(k|x|^{\max\{1/\alpha_1,1/\alpha_2\}}\:) < \infty$$ given $(1),\: (2)$. But before that is it right what I have done so far ?. Any help will be welcome. Intuitively I feel that $(f \cdot g)(x)$ will belong to $\mathcal{S}_{\max\{1/\alpha_1,1/\alpha_2\}}^{\max\{1/\beta_1,1/\beta_2\}}$  $\:$and thats why I was proceeding in that direction. Thanks in advance.","My original question is if $f\in \mathcal{S}_{\alpha_1}^{\beta_1},\: g \in \mathcal{S}_{\alpha_2}^{\beta_2}$, where does $(f\cdot g) (x)=f(x)g(x)$ belong ? where $\mathcal{S}_\alpha^\beta$ is defined here . To address this we proceed as follows: $f\in \mathcal{S}_{\alpha_1}^{\beta_1},\: g \in \mathcal{S}_{\alpha_2}^{\beta_2}\implies$ $$\sup_x|f(x)|\exp(k_1|x|^{1/\alpha_1}) < \infty \\ \sup_x|g(x)|\exp(k_2|x|^{1/\alpha_2}) < \infty$$ Multiplying both the inequalities we have $$\sup_x|f(x)g(x)|\exp(k_1|x|^{1/\alpha_1}+k_2|x|^{1/\alpha_2}) < \infty -(1)$$ Now, $k_1|x|^{1/\alpha_1} \le k_1|x|^{\max\{1/\alpha_1,1/\alpha_2\}}$. Similarly  $k_2|x|^{1/\alpha_2} \le k_2|x|^{\max\{1/\alpha_1,1/\alpha_2\}}$. hence $$\sup_x|f(x)g(x)|\exp(k_1|x|^{1/\alpha_1}+k_2|x|^{1/\alpha_2})\le\sup_x|f(x)g(x)|\exp(k|x|^{\max\{1/\alpha_1,1/\alpha_2\}}\:)-(2)$$ where $k=k_1+k_2$. My question: How do I show that $$\sup_x|f(x)g(x)|\exp(k|x|^{\max\{1/\alpha_1,1/\alpha_2\}}\:) < \infty$$ given $(1),\: (2)$. But before that is it right what I have done so far ?. Any help will be welcome. Intuitively I feel that $(f \cdot g)(x)$ will belong to $\mathcal{S}_{\max\{1/\alpha_1,1/\alpha_2\}}^{\max\{1/\beta_1,1/\beta_2\}}$  $\:$and thats why I was proceeding in that direction. Thanks in advance.",,"['analysis', 'inequality', 'gelfand-shilov-spaces']"
79,"infimum of a functional in $W^{1,p}((0,1))$",infimum of a functional in,"W^{1,p}((0,1))","Consider the functional  $$\mathcal{F}(u)=\int_{0}^{1}x^{\alpha}|u'(x)|^pdx,\ \ u\in W^{1,p}((0,1)),$$ where $\alpha\ge 0$ and $1<p<\infty$. Given $a<b$, find the value of  $$\inf\{\mathcal{F}(u): u\in W^{1,p}((0,1)), u(0)=a, u(1)=b\}.$$ For the case $p>\alpha +1$, my proof look like this: Since $u\in W^{1,p}((0,1))$, we have  $$u(1)-u(0)=\int_{0}^{1}u'(x)dx.$$ Base on the above equality, we have \begin{align*} (b-a)&=u(1)-u(0)\\ &=\int_{0}^{1}u'(x)dx\\ &=\int_{0}^{1}x^{\frac{\alpha}{p}}u'(x)x^{-\frac{\alpha}{p}}dx\\ &\le (\int_{0}^{1}x^{\alpha}|u(x)|^pdx)^{1/p}(\int_{0}^1x^{-\frac{\alpha q}{p}})^{1/q}\\ &=(\int_{0}^{1}x^{\alpha}|u(x)|^pdx)^{1/p}(\int_{0}^1x^{-\frac{\alpha}{p-1}})^{1/q} \end{align*} where we have $\frac{1}{p}+\frac{1}{q}=1$. Since $-\frac{\alpha}{p-1}>-1$, we have  \begin{align*} \int_{0}^1x^{-\frac{\alpha}{p-1}}&=\frac{1}{1-\frac{\alpha}{p-1}}\\ &=\frac{p-1}{p-\alpha-1}. \end{align*} Thus, we imply that  $$(b-a)\le (\int_{0}^{1}x^{\alpha}|u(x)|^pdx)^{1/p}(\frac{p-1}{p-\alpha-1})^{1/q}.$$ We deduce that  $$\int_{0}^{1}x^{\alpha}|u(x)|^pdx \ge (b-a)^p(\frac{p-\alpha-1}{p-1})^{p/q}$$ I am trying to prove that  $$(b-a)^p(\frac{p-\alpha-1}{p-1})^{p/q}=\inf\{\mathcal{F}(u): u\in W^{1,p}((0,1)), u(0)=a, u(1)=b\}$$ by finding a function $u\in W^{1,p}((0,1)), u(0)=a, u(1)=b$ such that $\mathcal{F}(u)=(b-a)^p(\frac{p-\alpha-1}{p-1})^{p/q}$. However, I have not succeeded. Can any one help me with that ?","Consider the functional  $$\mathcal{F}(u)=\int_{0}^{1}x^{\alpha}|u'(x)|^pdx,\ \ u\in W^{1,p}((0,1)),$$ where $\alpha\ge 0$ and $1<p<\infty$. Given $a<b$, find the value of  $$\inf\{\mathcal{F}(u): u\in W^{1,p}((0,1)), u(0)=a, u(1)=b\}.$$ For the case $p>\alpha +1$, my proof look like this: Since $u\in W^{1,p}((0,1))$, we have  $$u(1)-u(0)=\int_{0}^{1}u'(x)dx.$$ Base on the above equality, we have \begin{align*} (b-a)&=u(1)-u(0)\\ &=\int_{0}^{1}u'(x)dx\\ &=\int_{0}^{1}x^{\frac{\alpha}{p}}u'(x)x^{-\frac{\alpha}{p}}dx\\ &\le (\int_{0}^{1}x^{\alpha}|u(x)|^pdx)^{1/p}(\int_{0}^1x^{-\frac{\alpha q}{p}})^{1/q}\\ &=(\int_{0}^{1}x^{\alpha}|u(x)|^pdx)^{1/p}(\int_{0}^1x^{-\frac{\alpha}{p-1}})^{1/q} \end{align*} where we have $\frac{1}{p}+\frac{1}{q}=1$. Since $-\frac{\alpha}{p-1}>-1$, we have  \begin{align*} \int_{0}^1x^{-\frac{\alpha}{p-1}}&=\frac{1}{1-\frac{\alpha}{p-1}}\\ &=\frac{p-1}{p-\alpha-1}. \end{align*} Thus, we imply that  $$(b-a)\le (\int_{0}^{1}x^{\alpha}|u(x)|^pdx)^{1/p}(\frac{p-1}{p-\alpha-1})^{1/q}.$$ We deduce that  $$\int_{0}^{1}x^{\alpha}|u(x)|^pdx \ge (b-a)^p(\frac{p-\alpha-1}{p-1})^{p/q}$$ I am trying to prove that  $$(b-a)^p(\frac{p-\alpha-1}{p-1})^{p/q}=\inf\{\mathcal{F}(u): u\in W^{1,p}((0,1)), u(0)=a, u(1)=b\}$$ by finding a function $u\in W^{1,p}((0,1)), u(0)=a, u(1)=b$ such that $\mathcal{F}(u)=(b-a)^p(\frac{p-\alpha-1}{p-1})^{p/q}$. However, I have not succeeded. Can any one help me with that ?",,"['analysis', 'partial-differential-equations', 'sobolev-spaces', 'calculus-of-variations']"
80,Define a particular function,Define a particular function,,"Does anybody know how you can define a function $\eta \in C_c^1(B_R(0))$ such that $\eta = 1$ on $B_{\frac{R}{2}}(0)$ cause I need such a function in a particular proof, so I would really like to know if something like this is possible. (I don't know how you get the transitions smooth). Thank you very much in advance.","Does anybody know how you can define a function $\eta \in C_c^1(B_R(0))$ such that $\eta = 1$ on $B_{\frac{R}{2}}(0)$ cause I need such a function in a particular proof, so I would really like to know if something like this is possible. (I don't know how you get the transitions smooth). Thank you very much in advance.",,"['calculus', 'real-analysis', 'analysis', 'partial-differential-equations', 'special-functions']"
81,The $p=\infty$ case for an $L^2$ convolution operator on $\mathbb{R}^n$,The  case for an  convolution operator on,p=\infty L^2 \mathbb{R}^n,"Let $T$ be a convolution operator on $L^2(\mathbb{R}^n)$, suppose $K$ is a tempered distribution in $\mathbb{R}^n$ that coincides with a locally integrable function on $\mathbb{R}^n\setminus \{0\}$. Suppose that $K$ satisfies $$ \int_{|x|>2|y|}|K(x-y)-K(x)|dx\leq B,~y\in\mathbb{R}^n $$  for a constant $B$ (this is called the Hrmander condition and is a smoothness condition). Then for $1<r<\infty$ and $1<p<\infty$, we have the boundedness estimate $$ \left|\left|  \left(\sum_{j}|Tf_j|^r\right)^{\frac{1}{r}}  \right|\right|_p \leq C_{p,r} \left|\left|  \left(\sum_j|f_j|^r\right)^{\frac{1}{r}}    \right|\right|_p. $$ For $p=1$, we have the estimate $$ \left| \{ x\in\mathbb{R}^n:\left(\sum_j|Tf_j(x)|^r\right)^{\frac{1}{r}}>\lambda  \} \right| \leq \frac{C_r}{\lambda} \left|\left|  \left( \sum_j|f_j|^r \right)^{\frac{1}{r}}  \right|\right|_1. $$ (This is a weak-type $(1,1)$ inequality). My question is why do we not have a similar inequality for $p=\infty?$ A similar example is that we do not have $L^{\infty}-$boundedness for the Hilbert transform and a counterexample can be provided via step functions. Does anyone know of a counterexample why $L^{\infty}-$boundedness does not exist? What is the intuitive reason behind it?","Let $T$ be a convolution operator on $L^2(\mathbb{R}^n)$, suppose $K$ is a tempered distribution in $\mathbb{R}^n$ that coincides with a locally integrable function on $\mathbb{R}^n\setminus \{0\}$. Suppose that $K$ satisfies $$ \int_{|x|>2|y|}|K(x-y)-K(x)|dx\leq B,~y\in\mathbb{R}^n $$  for a constant $B$ (this is called the Hrmander condition and is a smoothness condition). Then for $1<r<\infty$ and $1<p<\infty$, we have the boundedness estimate $$ \left|\left|  \left(\sum_{j}|Tf_j|^r\right)^{\frac{1}{r}}  \right|\right|_p \leq C_{p,r} \left|\left|  \left(\sum_j|f_j|^r\right)^{\frac{1}{r}}    \right|\right|_p. $$ For $p=1$, we have the estimate $$ \left| \{ x\in\mathbb{R}^n:\left(\sum_j|Tf_j(x)|^r\right)^{\frac{1}{r}}>\lambda  \} \right| \leq \frac{C_r}{\lambda} \left|\left|  \left( \sum_j|f_j|^r \right)^{\frac{1}{r}}  \right|\right|_1. $$ (This is a weak-type $(1,1)$ inequality). My question is why do we not have a similar inequality for $p=\infty?$ A similar example is that we do not have $L^{\infty}-$boundedness for the Hilbert transform and a counterexample can be provided via step functions. Does anyone know of a counterexample why $L^{\infty}-$boundedness does not exist? What is the intuitive reason behind it?",,"['analysis', 'fourier-analysis']"
82,Integral evaluation for the Riesz means special case $s_n=1$,Integral evaluation for the Riesz means special case,s_n=1,"At the moment, I am  investigating the Riesz means defined as the series $$ s^{\delta}(\lambda)=\sum_{n\leq\lambda}\left( 1-\frac{n}{\lambda} \right)^{\delta}s_n. $$ Consider the special case $s_n=1$. Then we have the representation $$ \sum_{n\leq\lambda}\left( 1-\frac{n}{\lambda} \right)^{\delta}=\frac{1}{2\pi i}\int^{c+i\infty}_{c-i\infty}\frac{\Gamma(1+\delta)\Gamma(s)}{\Gamma(1+\delta+s)}\zeta(s)\lambda^sds=\frac{\lambda}{1+\delta}+\sum_n b_n\lambda^{-n} $$ (according to Wikipedia) where $\Gamma$ is the gamma function $$ \Gamma(t)=\int^{\infty}_{0}x^{t-1}e^{-x}dx $$ and $\zeta$ is the Riemann zeta function $$ \zeta(s)=\sum^{\infty}_{n=1}\frac{1}{n^s}. $$ A recommendation to solve this integral is to use Perron's formula $$ A(x)\sum_{n\leq x}a(n)=\frac{1}{2\pi i}\int^{c+i\infty}_{c-i\infty}g(z)\frac{x^z}{z}dz $$ where we have the uniformly convergent Dirichlet series $$ g(s)=\sum^{\infty}_{n=1}\frac{a(n)}{n^s},~\Re(s)>\sigma,~s\in\mathbb{C} $$ for an arithmetic function $a(n)$. Applying the Perron formula to the Riesz means gives $$ \sum_{n\leq \lambda}\left( 1-\frac{n}{\lambda} \right)^{\delta}=\frac{1}{2\pi i}\int^{c+i\infty}_{c-i\infty}\sum^{\infty}_{n=1}\frac{\left( 1-\frac{n}{\lambda} \right)^{\delta}}{n^s}\frac{x^z}{z}dz. $$ The key next step appears to be evaluating the infinite sum, but I am not sure how to do it. Any ideas? Thanks in advance for your comments and help.","At the moment, I am  investigating the Riesz means defined as the series $$ s^{\delta}(\lambda)=\sum_{n\leq\lambda}\left( 1-\frac{n}{\lambda} \right)^{\delta}s_n. $$ Consider the special case $s_n=1$. Then we have the representation $$ \sum_{n\leq\lambda}\left( 1-\frac{n}{\lambda} \right)^{\delta}=\frac{1}{2\pi i}\int^{c+i\infty}_{c-i\infty}\frac{\Gamma(1+\delta)\Gamma(s)}{\Gamma(1+\delta+s)}\zeta(s)\lambda^sds=\frac{\lambda}{1+\delta}+\sum_n b_n\lambda^{-n} $$ (according to Wikipedia) where $\Gamma$ is the gamma function $$ \Gamma(t)=\int^{\infty}_{0}x^{t-1}e^{-x}dx $$ and $\zeta$ is the Riemann zeta function $$ \zeta(s)=\sum^{\infty}_{n=1}\frac{1}{n^s}. $$ A recommendation to solve this integral is to use Perron's formula $$ A(x)\sum_{n\leq x}a(n)=\frac{1}{2\pi i}\int^{c+i\infty}_{c-i\infty}g(z)\frac{x^z}{z}dz $$ where we have the uniformly convergent Dirichlet series $$ g(s)=\sum^{\infty}_{n=1}\frac{a(n)}{n^s},~\Re(s)>\sigma,~s\in\mathbb{C} $$ for an arithmetic function $a(n)$. Applying the Perron formula to the Riesz means gives $$ \sum_{n\leq \lambda}\left( 1-\frac{n}{\lambda} \right)^{\delta}=\frac{1}{2\pi i}\int^{c+i\infty}_{c-i\infty}\sum^{\infty}_{n=1}\frac{\left( 1-\frac{n}{\lambda} \right)^{\delta}}{n^s}\frac{x^z}{z}dz. $$ The key next step appears to be evaluating the infinite sum, but I am not sure how to do it. Any ideas? Thanks in advance for your comments and help.",,"['real-analysis', 'sequences-and-series']"
83,Prove that $|f''(\xi)|\geqslant\frac{4|f(a)-f(b)|}{(b-a)^2}$,Prove that,|f''(\xi)|\geqslant\frac{4|f(a)-f(b)|}{(b-a)^2},"Let ${\rm f}:\left[a, b\right]\to\mathbb{R}$ be twice differentiable, and suppose $$\lim_{x\to a^{+}} \frac{{\rm f}\left(x\right) - {\rm f}\left(a\right)}{x - a} = \lim_{x\to b^{-}}\frac{{\rm f}\left(x\right) - {\rm f}\left(b\right)}{x - b} =0 $$ Show that there exists $\xi \in \left(a, b\right)$ such that $\displaystyle{% \left\vert\vphantom{\Large A}\,{\rm f}''\left(\xi\right)\right\vert \geq \frac{4\left\vert\vphantom{\Large A}% \,{\rm f}\left(a\right) - {\rm f}\left(b\right)\right\vert} {\left(b - a\right)^{2}}}$. I don't know how to start. Any hints ?.","Let ${\rm f}:\left[a, b\right]\to\mathbb{R}$ be twice differentiable, and suppose $$\lim_{x\to a^{+}} \frac{{\rm f}\left(x\right) - {\rm f}\left(a\right)}{x - a} = \lim_{x\to b^{-}}\frac{{\rm f}\left(x\right) - {\rm f}\left(b\right)}{x - b} =0 $$ Show that there exists $\xi \in \left(a, b\right)$ such that $\displaystyle{% \left\vert\vphantom{\Large A}\,{\rm f}''\left(\xi\right)\right\vert \geq \frac{4\left\vert\vphantom{\Large A}% \,{\rm f}\left(a\right) - {\rm f}\left(b\right)\right\vert} {\left(b - a\right)^{2}}}$. I don't know how to start. Any hints ?.",,"['calculus', 'real-analysis', 'analysis']"
84,Measure-preserving map between a function and its symmetric rearrangement,Measure-preserving map between a function and its symmetric rearrangement,,"Let $f \, \colon \mathbb{R}^d \rightarrow[0, \infty)$ be a function such that the sets $ \{ y \: \colon f(y) > \lambda \}$ are of finite Lebesgue-measure, for every $\lambda \geq 0$. Then, we can define the spherically symmetric and decreasing rearrangement $f^* \, \colon \mathbb{R}^d \rightarrow \mathbb{R}$ of $f$ by  \begin{equation*} f^* (x) = \int_{0}^{\infty} \chi_{ \{f > t \}^*} (x) \,  \textrm{d}t. \end{equation*}  Due to a theorem of John V. Ryff , in the $1$-dimensional case, there is a measure-preserving map $\sigma$ such that  \begin{equation*} f=  f^* \circ \sigma  \quad \textrm{ a.e.} \end{equation*} (In fact, this is a corollary of the cited theorem and can be found in [1, Corollary 7.6 in chapter 2].) My question is now: Is there a similar statement when $d \geq 2$? I expect that the answer is yes, but, unfortunately, I haven't found a reference. I would be grateful for any suggestions. Thanks! [1] Bennett, C., Sharpley, R. Interpolation of operators","Let $f \, \colon \mathbb{R}^d \rightarrow[0, \infty)$ be a function such that the sets $ \{ y \: \colon f(y) > \lambda \}$ are of finite Lebesgue-measure, for every $\lambda \geq 0$. Then, we can define the spherically symmetric and decreasing rearrangement $f^* \, \colon \mathbb{R}^d \rightarrow \mathbb{R}$ of $f$ by  \begin{equation*} f^* (x) = \int_{0}^{\infty} \chi_{ \{f > t \}^*} (x) \,  \textrm{d}t. \end{equation*}  Due to a theorem of John V. Ryff , in the $1$-dimensional case, there is a measure-preserving map $\sigma$ such that  \begin{equation*} f=  f^* \circ \sigma  \quad \textrm{ a.e.} \end{equation*} (In fact, this is a corollary of the cited theorem and can be found in [1, Corollary 7.6 in chapter 2].) My question is now: Is there a similar statement when $d \geq 2$? I expect that the answer is yes, but, unfortunately, I haven't found a reference. I would be grateful for any suggestions. Thanks! [1] Bennett, C., Sharpley, R. Interpolation of operators",,"['functional-analysis', 'analysis', 'measure-theory', 'reference-request', 'decreasing-rearrangements']"
85,Question on complete metric spaces and whether the following is a complete metric space:,Question on complete metric spaces and whether the following is a complete metric space:,,"Let   $ S \subset C^2([0,1])$(set of all two-times differentiable functions on $[0,1]$), which satisfy $$f(0)+f(\frac{1}{2})+f(1)=0.$$ Question :Is $ (S,d)$ is a complete metric space, where $d$ is defined as: $$d(f,g)=\sup_{x \in [0,1]}{|f(x)-g(x)|}+3\sup_{x \in [0,1]}{|f'(x)-g'(x)|}+2\sup_{x \in [0,1]}{|f''(x)-g''(x)|}$$ My thoughts: I cant definitely prove that $d$ is a metric, and was thinking if I could prove that $(C^2([0,1]),d)$ is a complete metric space as well (which I don't know how) and also proving that the set satisfying  $f(0)+f(\frac{1}{2})+f(1)=0.$ is a closed subset, then the job would be done. I don't know how to do these things.","Let   $ S \subset C^2([0,1])$(set of all two-times differentiable functions on $[0,1]$), which satisfy $$f(0)+f(\frac{1}{2})+f(1)=0.$$ Question :Is $ (S,d)$ is a complete metric space, where $d$ is defined as: $$d(f,g)=\sup_{x \in [0,1]}{|f(x)-g(x)|}+3\sup_{x \in [0,1]}{|f'(x)-g'(x)|}+2\sup_{x \in [0,1]}{|f''(x)-g''(x)|}$$ My thoughts: I cant definitely prove that $d$ is a metric, and was thinking if I could prove that $(C^2([0,1]),d)$ is a complete metric space as well (which I don't know how) and also proving that the set satisfying  $f(0)+f(\frac{1}{2})+f(1)=0.$ is a closed subset, then the job would be done. I don't know how to do these things.",,"['calculus', 'general-topology', 'analysis', 'metric-spaces']"
86,A integral inequality,A integral inequality,,"Let $g\in C_0^\infty((-1,1))$.Prove $\forall t\in (-1,1)$,$${g^4}\left( t \right) \le 16\int_{ - 1}^1 {\left( {{{\left| {g'\left( s \right)} \right|}^2} - \frac{{{g^2}\left( s \right)}}{{4{{\left( {1 - \left| s \right|} \right)}^2}}}} \right)ds}  \cdot \int_{ - 1}^1 {{{\left| {g\left( s \right)} \right|}^2}ds} .$$ This is a unusual integral inequality, I have tyied Cauchy-Schwarz inequality, but have no use.","Let $g\in C_0^\infty((-1,1))$.Prove $\forall t\in (-1,1)$,$${g^4}\left( t \right) \le 16\int_{ - 1}^1 {\left( {{{\left| {g'\left( s \right)} \right|}^2} - \frac{{{g^2}\left( s \right)}}{{4{{\left( {1 - \left| s \right|} \right)}^2}}}} \right)ds}  \cdot \int_{ - 1}^1 {{{\left| {g\left( s \right)} \right|}^2}ds} .$$ This is a unusual integral inequality, I have tyied Cauchy-Schwarz inequality, but have no use.",,"['calculus', 'real-analysis', 'integration', 'analysis']"
87,"Question about equivalent norms on $W^{2,2}(\Omega) \cap W^{1,2}_0(\Omega)$.",Question about equivalent norms on .,"W^{2,2}(\Omega) \cap W^{1,2}_0(\Omega)","Assume $\mathbb{H}=W^{2,2}(\Omega) \cap W^{1,2}_0(\Omega)$ with the norm induced from inner product $$\langle u,v\rangle_{\mathbb{H}}=\int_\Omega \Delta u \,\Delta v\, dx$$ for any $u \in W^{2,2}(\Omega) \cap W^{1,2}_0(\Omega)$ it is well-known that we have inequality (Rellich inequality) $$ \Lambda_N \int_{\Omega}\frac{u^2}{|x|^4}\mathrm{d}x \leq \int_\Omega |\Delta u|^2 \, \mathrm{d}x $$  where $\Lambda_N=(\frac{N^2(N-4)^2}{16})$ is optimal constant and also it is known that $$\|u\|^2=\int_\Omega \Big(|\Delta u|^2-\Lambda_N \frac{u^2}{|x|^4}\Big) \, \mathrm{d}x $$  Defines an other norm on $W^{2,2}(\Omega) \cap W^{1,2}_0(\Omega)$. My question is this that is these two norm equivalent? To see an improved case of this inequality with reminder term see Corollary 1 of Paper . I know if I could show that $W^{2,2}(\Omega) \cap W^{1,2}_0(\Omega)$ is a hilbert space with the new inner product then this two norms must be equivalent due to Open mapping theorem.","Assume $\mathbb{H}=W^{2,2}(\Omega) \cap W^{1,2}_0(\Omega)$ with the norm induced from inner product $$\langle u,v\rangle_{\mathbb{H}}=\int_\Omega \Delta u \,\Delta v\, dx$$ for any $u \in W^{2,2}(\Omega) \cap W^{1,2}_0(\Omega)$ it is well-known that we have inequality (Rellich inequality) $$ \Lambda_N \int_{\Omega}\frac{u^2}{|x|^4}\mathrm{d}x \leq \int_\Omega |\Delta u|^2 \, \mathrm{d}x $$  where $\Lambda_N=(\frac{N^2(N-4)^2}{16})$ is optimal constant and also it is known that $$\|u\|^2=\int_\Omega \Big(|\Delta u|^2-\Lambda_N \frac{u^2}{|x|^4}\Big) \, \mathrm{d}x $$  Defines an other norm on $W^{2,2}(\Omega) \cap W^{1,2}_0(\Omega)$. My question is this that is these two norm equivalent? To see an improved case of this inequality with reminder term see Corollary 1 of Paper . I know if I could show that $W^{2,2}(\Omega) \cap W^{1,2}_0(\Omega)$ is a hilbert space with the new inner product then this two norms must be equivalent due to Open mapping theorem.",,"['analysis', 'partial-differential-equations', 'sobolev-spaces', 'functional-inequalities', 'regularity-theory-of-pdes']"
88,"Connect two points, given their angles, with a maximum radius","Connect two points, given their angles, with a maximum radius",,"I want to connect two points on a plane. Both of the points have their angles given. The angles are representing the direction vector in which the curve should pass through this point. One of this point lies on the center of the coordinate system. The other one can lie in any other location of the space. For example:  $$ \begin{aligned} P_0 &= (0,0) & \vec P_0 &= \begin{bmatrix} 1 \\ 0 \end{bmatrix} & \gamma &= 90 \\ P_1 &= (3,3) & \vec P_1 &= \begin{bmatrix} 0 \\ 1 \end{bmatrix} & \gamma &= 0 \end{aligned}  $$ The resulting minimum radius of the curve should be as big as possible between the two points. In addition the length of the path should also be as short as possible. There should be no addition of extra straight paths or very big loops to stretch the radius. For now I tried to access this problem with Bezir curves, Hermite curves and ellipses but for none I got the sufficient solution.","I want to connect two points on a plane. Both of the points have their angles given. The angles are representing the direction vector in which the curve should pass through this point. One of this point lies on the center of the coordinate system. The other one can lie in any other location of the space. For example:  $$ \begin{aligned} P_0 &= (0,0) & \vec P_0 &= \begin{bmatrix} 1 \\ 0 \end{bmatrix} & \gamma &= 90 \\ P_1 &= (3,3) & \vec P_1 &= \begin{bmatrix} 0 \\ 1 \end{bmatrix} & \gamma &= 0 \end{aligned}  $$ The resulting minimum radius of the curve should be as big as possible between the two points. In addition the length of the path should also be as short as possible. There should be no addition of extra straight paths or very big loops to stretch the radius. For now I tried to access this problem with Bezir curves, Hermite curves and ellipses but for none I got the sufficient solution.",,"['analysis', 'geometry']"
89,"""composition"" of ""pointwise convergent sequences of functions""","""composition"" of ""pointwise convergent sequences of functions""",,"Intuitively, if $f_n\to f$ as $n\to\infty$ and $g^{(n)}_i\to f_n$ as $i\to\infty$, can we get $g_j\to f$ as $j\to\infty$? Formally, Let $\{f_n\}_n$ be a sequence of functions from $\mathbb{R}^d$ to   $\overline{\mathbb{R}}$, the extended real line. Let $f$ be its   pointwise limit, i.e. for each $x\in\mathbb{R}^d$ and each   $\varepsilon>0$, there exists $N\in\mathbb{Z}^+$ such that   $|f_n(x)-f(x)|<\varepsilon$ for all $n\geq N$. Each $f_n$ is the   pointwise limit of sequence $\{g^{(n)}_i\}_i$. Can we construct a   sequence of $g$'s converging pointwise to $f$? If not, what additional   conditions do we need? As usual, any help is appreciated:)","Intuitively, if $f_n\to f$ as $n\to\infty$ and $g^{(n)}_i\to f_n$ as $i\to\infty$, can we get $g_j\to f$ as $j\to\infty$? Formally, Let $\{f_n\}_n$ be a sequence of functions from $\mathbb{R}^d$ to   $\overline{\mathbb{R}}$, the extended real line. Let $f$ be its   pointwise limit, i.e. for each $x\in\mathbb{R}^d$ and each   $\varepsilon>0$, there exists $N\in\mathbb{Z}^+$ such that   $|f_n(x)-f(x)|<\varepsilon$ for all $n\geq N$. Each $f_n$ is the   pointwise limit of sequence $\{g^{(n)}_i\}_i$. Can we construct a   sequence of $g$'s converging pointwise to $f$? If not, what additional   conditions do we need? As usual, any help is appreciated:)",,"['real-analysis', 'analysis']"
90,Locally constant property,Locally constant property,,"Suppose f is positive and Schwartz function. Fix $N>0$ and $A>0$. Suppose that for any $x \in [-N,N]$,   $$A \leq \int_{-N}^{N}f(x-z)dz$$   Then do the inequality   $$A \leq C_{r} \frac{1}{N^{1/r}}\int_{-N}^{N}(\int_{-N}^{N}|f(x-z)|^rdx)^{1/r}dz$$   hold for some $C_{r}$ and for any $r>0$? I solved the case $r \geq 1$. By integration on $[-N,N]$, $$2NA \leq \int_{-N}^{N}\int_{-N}^{N}f(x-z)dxdz$$ Now, just apply Holder inequality. Then we get the desired result. If $f$ is a monotone function, then the case $0<r<1$ also true but I don't know how to prove general case. Thanks in advance. EDIT : (Scaling argument) Put $g(z)=\frac{1}{N}f(\frac{z}{N})$. Then we may assume that $N=1$.","Suppose f is positive and Schwartz function. Fix $N>0$ and $A>0$. Suppose that for any $x \in [-N,N]$,   $$A \leq \int_{-N}^{N}f(x-z)dz$$   Then do the inequality   $$A \leq C_{r} \frac{1}{N^{1/r}}\int_{-N}^{N}(\int_{-N}^{N}|f(x-z)|^rdx)^{1/r}dz$$   hold for some $C_{r}$ and for any $r>0$? I solved the case $r \geq 1$. By integration on $[-N,N]$, $$2NA \leq \int_{-N}^{N}\int_{-N}^{N}f(x-z)dxdz$$ Now, just apply Holder inequality. Then we get the desired result. If $f$ is a monotone function, then the case $0<r<1$ also true but I don't know how to prove general case. Thanks in advance. EDIT : (Scaling argument) Put $g(z)=\frac{1}{N}f(\frac{z}{N})$. Then we may assume that $N=1$.",,"['real-analysis', 'analysis', 'measure-theory', 'harmonic-analysis']"
91,Operator Norm and Submultiplicativity against the Spectral Norm,Operator Norm and Submultiplicativity against the Spectral Norm,,"Consider $\mathcal{A}:\mathbb{R}^{n\times m}\to \mathbb{R}^{p\times q}$ to be a linear operator. I know that by considering the trace norm and using the submultiplicativity of the operator norm we have $\|\mathcal{A}(X)\|_F \leq \|\mathcal{A}\|_{op} \|X\|_F$ where $\|.\|_{op}$ denotes the operator norm and $\|.\|_F$ is the Frobenius norm. The question is whether we can also claim that $\|\mathcal{A}(X)\| \leq \|\mathcal{A}\|_{op} \|X\|  $ where $\|.\|$ is the matrix spectral norm (the largest singular value of $X$)? It is certainly true that $\|\mathcal{A}(X)\| \leq \sqrt{r}\|\mathcal{A}\|_{op} \|X\| $ where $r = rank(X)$, but I am not sure about the one without $\sqrt{r}$. I would appreciate any thoughts or even references.","Consider $\mathcal{A}:\mathbb{R}^{n\times m}\to \mathbb{R}^{p\times q}$ to be a linear operator. I know that by considering the trace norm and using the submultiplicativity of the operator norm we have $\|\mathcal{A}(X)\|_F \leq \|\mathcal{A}\|_{op} \|X\|_F$ where $\|.\|_{op}$ denotes the operator norm and $\|.\|_F$ is the Frobenius norm. The question is whether we can also claim that $\|\mathcal{A}(X)\| \leq \|\mathcal{A}\|_{op} \|X\|  $ where $\|.\|$ is the matrix spectral norm (the largest singular value of $X$)? It is certainly true that $\|\mathcal{A}(X)\| \leq \sqrt{r}\|\mathcal{A}\|_{op} \|X\| $ where $r = rank(X)$, but I am not sure about the one without $\sqrt{r}$. I would appreciate any thoughts or even references.",,"['linear-algebra', 'analysis', 'normed-spaces', 'spectral-theory']"
92,The question about the support of Fourier transform of $|f|^p$,The question about the support of Fourier transform of,|f|^p,"Suppose $f$ is a smooth function with $\mathbb{supp}{(\mathcal{F}{f})} \subset B(0,1)$. In addition, assume $f$ is non-negative. We can observe that the function $|f|^2$ has a nice property : $$\mathbb{supp{(\mathcal{F}(|f|^2))}}=\mathbb{supp{(\mathcal{F}(f)\ast\mathcal{F}(f)})} \subset B(0,2)$$ Let $1 < p < 2$. Does there exist a constant $C$ independent of $p$ such that $$\mathbb{supp(\mathcal{F}(|f|^p))} \subset B(0,C)$$ for all $f$ satisfying the above conditions? This is not homework or exercise in some book but my pure question. What I did : Let $p$ be a rational number. Put $p=\frac{n}{m}$, $n$, $m$ $\in \mathbb{N}$. First, we have $$\mathbb{supp{(\mathcal{F}(|f|^n))}} \subset B(0,n)$$ Put $g(x)=|f(x)|^{\frac{n}{m}}$. Then  $$\mathcal{F}(g) \; \ast \; ... \; \ast \; \mathcal{F}(g) (m-times) = \mathcal{F}(|f|^n)$$ If $\mathcal{F}(g)$ is always non-negative, then $$\mathcal{F}(g) \subset B(0,\frac{n}{m})$$ But in general we cannot deduce that because $\mathcal{F}g$ can be negative. I don't believe that my claim is correct, but finding counterexample is extremely hard. Any comments or hints are welcomed.","Suppose $f$ is a smooth function with $\mathbb{supp}{(\mathcal{F}{f})} \subset B(0,1)$. In addition, assume $f$ is non-negative. We can observe that the function $|f|^2$ has a nice property : $$\mathbb{supp{(\mathcal{F}(|f|^2))}}=\mathbb{supp{(\mathcal{F}(f)\ast\mathcal{F}(f)})} \subset B(0,2)$$ Let $1 < p < 2$. Does there exist a constant $C$ independent of $p$ such that $$\mathbb{supp(\mathcal{F}(|f|^p))} \subset B(0,C)$$ for all $f$ satisfying the above conditions? This is not homework or exercise in some book but my pure question. What I did : Let $p$ be a rational number. Put $p=\frac{n}{m}$, $n$, $m$ $\in \mathbb{N}$. First, we have $$\mathbb{supp{(\mathcal{F}(|f|^n))}} \subset B(0,n)$$ Put $g(x)=|f(x)|^{\frac{n}{m}}$. Then  $$\mathcal{F}(g) \; \ast \; ... \; \ast \; \mathcal{F}(g) (m-times) = \mathcal{F}(|f|^n)$$ If $\mathcal{F}(g)$ is always non-negative, then $$\mathcal{F}(g) \subset B(0,\frac{n}{m})$$ But in general we cannot deduce that because $\mathcal{F}g$ can be negative. I don't believe that my claim is correct, but finding counterexample is extremely hard. Any comments or hints are welcomed.",,"['real-analysis', 'analysis', 'fourier-analysis', 'harmonic-analysis']"
93,Fundamental Lemma of the Calculus of Variations with higher derivatives,Fundamental Lemma of the Calculus of Variations with higher derivatives,,"The fundamental lemma of the calculus of variations is often presented as: If $M(x) \in C[a,b]$ such that $\int_{a}^{b}{M(x)\eta(x)} = 0 ~~\forall\eta\in C^1[a,b],\eta(a)=\eta(b)=0$, then $M(x)=0$ for all $x\in[a,b]$. I am trying to prove the specific higher order case when $M$ is multiplied not by $\eta$ but by $\eta''$. The case for $\eta'$ is easy enough to understand (Lemma of du Bois-Reymond). So, the problem is this: If $M(x) \in C[a,b]$ such that $\int_{a}^{b}{M(x)\eta''(x)} = 0 ~~\forall\eta\in C^2[a,b],\eta(a)=\eta(b)=\eta'(a)=\eta'(b)=0$, then $M(x)=px+q$ for all $x\in[a,b]$. I have tried to work backward from what I think will be the last statement in the proof : $\int_{a}^{b}{[M(x) - (px+q)]^2}=0$. But I can't quite get it to work.","The fundamental lemma of the calculus of variations is often presented as: If $M(x) \in C[a,b]$ such that $\int_{a}^{b}{M(x)\eta(x)} = 0 ~~\forall\eta\in C^1[a,b],\eta(a)=\eta(b)=0$, then $M(x)=0$ for all $x\in[a,b]$. I am trying to prove the specific higher order case when $M$ is multiplied not by $\eta$ but by $\eta''$. The case for $\eta'$ is easy enough to understand (Lemma of du Bois-Reymond). So, the problem is this: If $M(x) \in C[a,b]$ such that $\int_{a}^{b}{M(x)\eta''(x)} = 0 ~~\forall\eta\in C^2[a,b],\eta(a)=\eta(b)=\eta'(a)=\eta'(b)=0$, then $M(x)=px+q$ for all $x\in[a,b]$. I have tried to work backward from what I think will be the last statement in the proof : $\int_{a}^{b}{[M(x) - (px+q)]^2}=0$. But I can't quite get it to work.",,"['analysis', 'calculus-of-variations']"
94,finding polynomials to approximate a multivariable function,finding polynomials to approximate a multivariable function,,"Let $U := B_1(0) \subseteq \mathbb{R}^2$, with $B_1(0) := \{(x, y) \in \mathbb{R}^2,\space \|(x, y)\| _1 < 1\}$. Now consider the function: $$g: U \to \mathbb{R}^2, (x, y) \mapsto \frac{1}{1-x-y}$$ For each $n \in \mathbb{N}_0$, I now want to find a polynomial function $p_n$, so that $g(x, y) = p_n(x, y) + o(\|(x, y)\|^n)$ for $(x, y) \to 0$. (Where $o(\cdots)$ is the ""Small O"" Landau symbol.) I must admit that I don't really know how to approach this. What's the technique or formula to determine these polynomials?","Let $U := B_1(0) \subseteq \mathbb{R}^2$, with $B_1(0) := \{(x, y) \in \mathbb{R}^2,\space \|(x, y)\| _1 < 1\}$. Now consider the function: $$g: U \to \mathbb{R}^2, (x, y) \mapsto \frac{1}{1-x-y}$$ For each $n \in \mathbb{N}_0$, I now want to find a polynomial function $p_n$, so that $g(x, y) = p_n(x, y) + o(\|(x, y)\|^n)$ for $(x, y) \to 0$. (Where $o(\cdots)$ is the ""Small O"" Landau symbol.) I must admit that I don't really know how to approach this. What's the technique or formula to determine these polynomials?",,"['analysis', 'multivariable-calculus', 'polynomials']"
95,Convolution with standard mollifier,Convolution with standard mollifier,,"Let $\Omega \subset \mathbb{R}$ open and $f \in L^p(\Omega).$ Now, we define $$\eta(x):=\chi_{[-1,1]}(x) e^{\frac{-1}{1-x^2}}.$$ Then we define $$\eta_h(x):=\frac{1}{h} \eta\left( \frac{x}{h}\right).$$ This means that $\mathrm{supp}(\eta_h) \subset [-h,h].$ Now, we clearly have $f * \eta_h \in L^p(\Omega).$ Does anybody know if we have $$||f * \eta_h ||_p \le ||f||$$ and $$||f - f* \eta_h || \rightarrow 0$$ for $h \rightarrow \infty, $ too?","Let $\Omega \subset \mathbb{R}$ open and $f \in L^p(\Omega).$ Now, we define $$\eta(x):=\chi_{[-1,1]}(x) e^{\frac{-1}{1-x^2}}.$$ Then we define $$\eta_h(x):=\frac{1}{h} \eta\left( \frac{x}{h}\right).$$ This means that $\mathrm{supp}(\eta_h) \subset [-h,h].$ Now, we clearly have $f * \eta_h \in L^p(\Omega).$ Does anybody know if we have $$||f * \eta_h ||_p \le ||f||$$ and $$||f - f* \eta_h || \rightarrow 0$$ for $h \rightarrow \infty, $ too?",,"['real-analysis', 'analysis', 'functional-analysis', 'partial-differential-equations', 'lebesgue-integral']"
96,Derivative at the point of inflection,Derivative at the point of inflection,,"Is it always true that when a function changes concavity from concave-up to concave-down, its derivative at the point of inflection is undefined? And in the reverse order the derivative is Zero?","Is it always true that when a function changes concavity from concave-up to concave-down, its derivative at the point of inflection is undefined? And in the reverse order the derivative is Zero?",,"['calculus', 'algebra-precalculus', 'analysis']"
97,"Prob. 10, Sec. 3.10 in Kreyszig's functional analysis book: Every isometric linear operator on a finite-dimensional inner product space is unitary? [duplicate]","Prob. 10, Sec. 3.10 in Kreyszig's functional analysis book: Every isometric linear operator on a finite-dimensional inner product space is unitary? [duplicate]",,"This question already has answers here : Is linear surjective isometry always unitary? (2 answers) Closed 9 years ago . Let $X$ be an inner product space such that $\dim X < \infty$, and let $T \colon X \to X$ be an isometric linear operator. Since $\dim X < \infty$, $X$ is complete and thus a Hilbert space; since $T$ is isometric, $T$ is also injective and hence also surjective and thus bijective, because $\dim X < \infty$. So $T^{-1}$ exists. How to show that $T$ is unitary. That is, how to show that the Hilbert adjoint operator $T^*$ of $T$ equals $T^{-1}$? Since $X$ is finite-dimensional, we can choose an orthonormal basis for $X$; let $n \colon= \dim X$, and let $\{e_1, \ldots, e_n \}$ be an orthonormal basis for $X$. Then, for each $i, j = 1, \ldots, n$, we have  $$\langle Te_i , e_j \rangle = \langle e_i, T^* e_j \rangle,$$ and,  $$\langle T^* T e_i, e_j \rangle =  \langle T e_i , T e_j \rangle = \langle e_i , e_j \rangle = \begin{cases} 1 \ & \mbox{ if  } \ i = j \\ 0 \ & \mbox{ if } \ i \neq j. \end{cases} $$ What next?","This question already has answers here : Is linear surjective isometry always unitary? (2 answers) Closed 9 years ago . Let $X$ be an inner product space such that $\dim X < \infty$, and let $T \colon X \to X$ be an isometric linear operator. Since $\dim X < \infty$, $X$ is complete and thus a Hilbert space; since $T$ is isometric, $T$ is also injective and hence also surjective and thus bijective, because $\dim X < \infty$. So $T^{-1}$ exists. How to show that $T$ is unitary. That is, how to show that the Hilbert adjoint operator $T^*$ of $T$ equals $T^{-1}$? Since $X$ is finite-dimensional, we can choose an orthonormal basis for $X$; let $n \colon= \dim X$, and let $\{e_1, \ldots, e_n \}$ be an orthonormal basis for $X$. Then, for each $i, j = 1, \ldots, n$, we have  $$\langle Te_i , e_j \rangle = \langle e_i, T^* e_j \rangle,$$ and,  $$\langle T^* T e_i, e_j \rangle =  \langle T e_i , T e_j \rangle = \langle e_i , e_j \rangle = \begin{cases} 1 \ & \mbox{ if  } \ i = j \\ 0 \ & \mbox{ if } \ i \neq j. \end{cases} $$ What next?",,"['real-analysis', 'analysis', 'functional-analysis', 'operator-theory', 'hilbert-spaces']"
98,"Prob. 9, Sec. 3.9 in Erwin Kreyszig's INTRODUCTORY FUNCTIONAL ANALYSIS WITH APPLICATIONS: Finite-dimensional range and the form of images","Prob. 9, Sec. 3.9 in Erwin Kreyszig's INTRODUCTORY FUNCTIONAL ANALYSIS WITH APPLICATIONS: Finite-dimensional range and the form of images",,"Let $H$ be a Hilbert space, and let $T \colon H \to H$ be a bounded linear operator. Then how to show the following? The range of $T$ is finite-dimensional if and only if $T$ can be represented in the form  $$Tx = \sum_{j=1}^n \langle x, v_j \rangle w_j \ \ \ [ v_j, w_j \in H].$$ My effort: Supppose that $\dim \mathscr{R}(T) = n$, and let $\{ e_1, \ldots, e_n \}$ be an orthonormal basis for $\mathscr{R}(T)$. Then, for every $x \in H$, we have  $$Tx = \sum_{j=1}^n \langle Tx, e_j \rangle e_j = \sum_{j=1}^n \langle x, T^* e_j \rangle e_j,$$ where $T^*$ denotes the Hilbert adjoint operator of $T$. So we can take $w_j \colon= e_j$ and $v_j \colon= T^* e_j$. Is this reasoning correct? If so, then how to show the converse? Doest the above representation mean that the same $v_j$s and $w_j$s will be used for every $x \in H$? If so, then range of $T$ is spanned by the finitely many elements $w_1, \ldots, w_n$ and is clearly finite-dimensional.","Let $H$ be a Hilbert space, and let $T \colon H \to H$ be a bounded linear operator. Then how to show the following? The range of $T$ is finite-dimensional if and only if $T$ can be represented in the form  $$Tx = \sum_{j=1}^n \langle x, v_j \rangle w_j \ \ \ [ v_j, w_j \in H].$$ My effort: Supppose that $\dim \mathscr{R}(T) = n$, and let $\{ e_1, \ldots, e_n \}$ be an orthonormal basis for $\mathscr{R}(T)$. Then, for every $x \in H$, we have  $$Tx = \sum_{j=1}^n \langle Tx, e_j \rangle e_j = \sum_{j=1}^n \langle x, T^* e_j \rangle e_j,$$ where $T^*$ denotes the Hilbert adjoint operator of $T$. So we can take $w_j \colon= e_j$ and $v_j \colon= T^* e_j$. Is this reasoning correct? If so, then how to show the converse? Doest the above representation mean that the same $v_j$s and $w_j$s will be used for every $x \in H$? If so, then range of $T$ is spanned by the finitely many elements $w_1, \ldots, w_n$ and is clearly finite-dimensional.",,"['real-analysis', 'linear-algebra', 'analysis', 'functional-analysis', 'inner-products']"
99,"How to evalute: $\int_0^1 \frac{e^{-ax}}{ax} -\frac{e^{-abx}}{1- e^{-ax}}((1-x)\cos (\pi x) + \frac{3}{\pi} \sin(\pi x)) dx$ and $a, b >0$",How to evalute:  and,"\int_0^1 \frac{e^{-ax}}{ax} -\frac{e^{-abx}}{1- e^{-ax}}((1-x)\cos (\pi x) + \frac{3}{\pi} \sin(\pi x)) dx a, b >0","How to evalute: $$\int_0^1 \left[ \frac{e^{-ax}}{ax} -\frac{e^{-abx}}{1- e^{-ax}}\left((1-x)\cos (\pi x) + \frac{3}{\pi} \sin(\pi x)\right) \right] dx$$ and $a, b >0$","How to evalute: $$\int_0^1 \left[ \frac{e^{-ax}}{ax} -\frac{e^{-abx}}{1- e^{-ax}}\left((1-x)\cos (\pi x) + \frac{3}{\pi} \sin(\pi x)\right) \right] dx$$ and $a, b >0$",,"['calculus', 'analysis']"
