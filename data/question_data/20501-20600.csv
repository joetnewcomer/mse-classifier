,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Matrix function which is multiplicative and homogeneous of degree n is the determinant,Matrix function which is multiplicative and homogeneous of degree n is the determinant,,"I'm solving the following problem: Let $\Delta : M_n(\mathbb{F}) \to \mathbb{F}$ satisfy the following properties: $\Delta(AB) = \Delta(A)\Delta(B), \quad\forall A, B \in M_n(\mathbb{F})$ $\Delta(\lambda I) = \lambda^n, \quad\forall \lambda \in \mathbb{F}$ Prove that $\Delta = \det$. $M_n(\mathbb{F})$ is the space of $n\times n$ matrices with entries from the field $\mathbb{F}$, and $I \in M_n(\mathbb{F})$ is the identity matrix. $\mathbb{F}$ can pretty much be assumed to be $\mathbb{R}$ or $\mathbb{C}$, since it was the general setting where the problem was given. $\DeclareMathOperator{\sgn}{sgn}$ The definition of the determinant I'm using is $\det A = \sum_{p\in S_n} \sgn p\cdot a_{1p(1)}a_{2p(2)}\cdots a_{np(n)}$, for $A = (a_{ij})$. I'm also aware of certain characterizations, such as: Let $f : M_n(\mathbb{F}) \to \mathbb{F}$ be a multilinear and alternating function in the columns of the matrix such that $f(I) = 1$. Then $f = \det$. My attempt: My idea was to prove that $\Delta = \det$ on all elementary matrices and then use the fact that every invertible matrix can be written as a product of elementary matrices. I would separately prove that $\Delta$ of all singular matrices is $0$. Some results I obtained along the way (using the notation for elementary matrices from the linked Wikipedia article): By plugging in $\lambda = 1$ in $(2)$ we get $\Delta(I) = 1$. $\lambda = 0$ gives $\Delta(0) = 0$. $\Delta$ is homogenous of degree $n$: By pluggin in $\lambda = 1$ in $(2)$ we get $\Delta(I) = 1$. Then for any matrix $A$ we have $\Delta(\lambda A) = \Delta(\lambda I\cdot A) = \Delta(\lambda I)\Delta(A) = \lambda^n \Delta(A)$. Let $A$ be invertible. Then $1 = \Delta(I) = \Delta(AA^{-1}) = \Delta(A)\Delta(A^{-1})$. Thus, $\Delta(A) \ne 0$. ${T_{i,j}}^2 = I$ so $|\Delta(T_{i,j})| = 1$ For $\lambda \notin \{0, 1\}$ we have $$\Delta(D_1(\lambda))\,\Delta(D_2(\lambda))\cdots \Delta(D_n(\lambda))) = \Delta(D_1(\lambda)\,D_2(\lambda)\cdots D_n(\lambda)) = \Delta(\lambda I) = \lambda^n$$ so there surely exists $i_0 \in \{1, \ldots, n\}$ such that $|\Delta(D_{i_0}(\lambda))|\ne 1$. $\Delta$ of a matrix with a zero row is $0$. Let $A$ be such that its $i$-th row is $0$. Then there exists a sequence of row transpositions $T_1, T_2, \ldots, T_k$ of the type $T_{r,s}$ such that $A$ is transformed to a matrix with its $i_0$-th row is a zero row, where $|\Delta(D_{i_0}(\lambda))|\ne 1$. Now, since multiplying that row with $\lambda \notin \{0, 1\}$ does not change the matrix, by doing the row transpositions again in reverse order we return to the matrix $A$. Then we have $$\Delta(A) = \Delta(T_1\ldots T_{n-1}T_nD_{i_0}(\lambda)T_n\ldots T_2T_1A) = \Delta(T_1)^2\Delta(T_2)^2\cdots\Delta(T_n)^2\Delta(D_{i_0}(\lambda))\Delta(A)$$ By taking the absolute value of both sides, since $|\Delta(T_{r,s})| = 1$ and $|\Delta(D_{i_0}(\lambda))|\ne 1$, we obtain $\Delta(A) = 0$. $\Delta$ of a singular matrix is $0$. Let $A$ be singular. Then $A$ can be transformed by row and column operations to a matrix $$E_r = \begin{bmatrix} 1 &  &  & & \\ & \ddots & & &  \\ & & 1 & & &  \\ & & & 0 & &  \\ & & & & \ddots \\ & & & & & 0 \\ \end{bmatrix}$$ with $r$ nonzero rows, where $r$ is the rank of $A$. Since $\Delta$ of elementary matrices is nonzero because they are invertible, and $\Delta(E_r) = 0$ because there is always a zero row in it, we can conclude that $\Delta(A) = 0$. $L_{i,j}(\lambda)$ can be diagonalized to $I$, and since $\Delta$ is obviously a similarity invariant we have $\Delta(L_{i,j}(\lambda)) = 1$. This is where I'm stuck, we still have to prove $\Delta(T_{i,j}) = -1$ and $\Delta(D_i(\lambda)) = \lambda$. I have had some further progress if we assume $\mathbb{F} = \mathbb{R}$: From $|\Delta(T_{i,j})| = 1$ we can conclude $\Delta(T_{i,j})$ = $\pm 1$. Notice that $D_i(\lambda) = T_{i,j}D_j(\lambda)$. Applying $\Delta$ we conclude that $\Delta(D_i(\lambda))$ and $\Delta(D_j(\lambda))$ can differ only by sign. Since we know $\Delta(D_1(\lambda))\,\Delta(D_2(\lambda))\cdots \Delta(D_n(\lambda))) = \lambda^n$, we get $|\Delta(D_i(\lambda))| = |\lambda|$. If we further assume $\lambda > 0$, we have $\Delta(D_i(\lambda)) = \Delta(D_i(\sqrt{\lambda})D_i(\sqrt{\lambda})) = \Delta\left(D_i(\sqrt{\lambda})\right)^2 > 0$. Finally we obtain $\Delta(D_i(\lambda)) = \lambda$.","I'm solving the following problem: Let $\Delta : M_n(\mathbb{F}) \to \mathbb{F}$ satisfy the following properties: $\Delta(AB) = \Delta(A)\Delta(B), \quad\forall A, B \in M_n(\mathbb{F})$ $\Delta(\lambda I) = \lambda^n, \quad\forall \lambda \in \mathbb{F}$ Prove that $\Delta = \det$. $M_n(\mathbb{F})$ is the space of $n\times n$ matrices with entries from the field $\mathbb{F}$, and $I \in M_n(\mathbb{F})$ is the identity matrix. $\mathbb{F}$ can pretty much be assumed to be $\mathbb{R}$ or $\mathbb{C}$, since it was the general setting where the problem was given. $\DeclareMathOperator{\sgn}{sgn}$ The definition of the determinant I'm using is $\det A = \sum_{p\in S_n} \sgn p\cdot a_{1p(1)}a_{2p(2)}\cdots a_{np(n)}$, for $A = (a_{ij})$. I'm also aware of certain characterizations, such as: Let $f : M_n(\mathbb{F}) \to \mathbb{F}$ be a multilinear and alternating function in the columns of the matrix such that $f(I) = 1$. Then $f = \det$. My attempt: My idea was to prove that $\Delta = \det$ on all elementary matrices and then use the fact that every invertible matrix can be written as a product of elementary matrices. I would separately prove that $\Delta$ of all singular matrices is $0$. Some results I obtained along the way (using the notation for elementary matrices from the linked Wikipedia article): By plugging in $\lambda = 1$ in $(2)$ we get $\Delta(I) = 1$. $\lambda = 0$ gives $\Delta(0) = 0$. $\Delta$ is homogenous of degree $n$: By pluggin in $\lambda = 1$ in $(2)$ we get $\Delta(I) = 1$. Then for any matrix $A$ we have $\Delta(\lambda A) = \Delta(\lambda I\cdot A) = \Delta(\lambda I)\Delta(A) = \lambda^n \Delta(A)$. Let $A$ be invertible. Then $1 = \Delta(I) = \Delta(AA^{-1}) = \Delta(A)\Delta(A^{-1})$. Thus, $\Delta(A) \ne 0$. ${T_{i,j}}^2 = I$ so $|\Delta(T_{i,j})| = 1$ For $\lambda \notin \{0, 1\}$ we have $$\Delta(D_1(\lambda))\,\Delta(D_2(\lambda))\cdots \Delta(D_n(\lambda))) = \Delta(D_1(\lambda)\,D_2(\lambda)\cdots D_n(\lambda)) = \Delta(\lambda I) = \lambda^n$$ so there surely exists $i_0 \in \{1, \ldots, n\}$ such that $|\Delta(D_{i_0}(\lambda))|\ne 1$. $\Delta$ of a matrix with a zero row is $0$. Let $A$ be such that its $i$-th row is $0$. Then there exists a sequence of row transpositions $T_1, T_2, \ldots, T_k$ of the type $T_{r,s}$ such that $A$ is transformed to a matrix with its $i_0$-th row is a zero row, where $|\Delta(D_{i_0}(\lambda))|\ne 1$. Now, since multiplying that row with $\lambda \notin \{0, 1\}$ does not change the matrix, by doing the row transpositions again in reverse order we return to the matrix $A$. Then we have $$\Delta(A) = \Delta(T_1\ldots T_{n-1}T_nD_{i_0}(\lambda)T_n\ldots T_2T_1A) = \Delta(T_1)^2\Delta(T_2)^2\cdots\Delta(T_n)^2\Delta(D_{i_0}(\lambda))\Delta(A)$$ By taking the absolute value of both sides, since $|\Delta(T_{r,s})| = 1$ and $|\Delta(D_{i_0}(\lambda))|\ne 1$, we obtain $\Delta(A) = 0$. $\Delta$ of a singular matrix is $0$. Let $A$ be singular. Then $A$ can be transformed by row and column operations to a matrix $$E_r = \begin{bmatrix} 1 &  &  & & \\ & \ddots & & &  \\ & & 1 & & &  \\ & & & 0 & &  \\ & & & & \ddots \\ & & & & & 0 \\ \end{bmatrix}$$ with $r$ nonzero rows, where $r$ is the rank of $A$. Since $\Delta$ of elementary matrices is nonzero because they are invertible, and $\Delta(E_r) = 0$ because there is always a zero row in it, we can conclude that $\Delta(A) = 0$. $L_{i,j}(\lambda)$ can be diagonalized to $I$, and since $\Delta$ is obviously a similarity invariant we have $\Delta(L_{i,j}(\lambda)) = 1$. This is where I'm stuck, we still have to prove $\Delta(T_{i,j}) = -1$ and $\Delta(D_i(\lambda)) = \lambda$. I have had some further progress if we assume $\mathbb{F} = \mathbb{R}$: From $|\Delta(T_{i,j})| = 1$ we can conclude $\Delta(T_{i,j})$ = $\pm 1$. Notice that $D_i(\lambda) = T_{i,j}D_j(\lambda)$. Applying $\Delta$ we conclude that $\Delta(D_i(\lambda))$ and $\Delta(D_j(\lambda))$ can differ only by sign. Since we know $\Delta(D_1(\lambda))\,\Delta(D_2(\lambda))\cdots \Delta(D_n(\lambda))) = \lambda^n$, we get $|\Delta(D_i(\lambda))| = |\lambda|$. If we further assume $\lambda > 0$, we have $\Delta(D_i(\lambda)) = \Delta(D_i(\sqrt{\lambda})D_i(\sqrt{\lambda})) = \Delta\left(D_i(\sqrt{\lambda})\right)^2 > 0$. Finally we obtain $\Delta(D_i(\lambda)) = \lambda$.",,"['linear-algebra', 'determinant']"
1,Coordinate free proof that det is algebraic,Coordinate free proof that det is algebraic,,"If V is a vector space of dimension N, and $f\colon V \to V$ is an endomorphism, one defines its determinant to be the scalar corresponding to the induced map $\wedge^N V \to \wedge^N V$. I like this definition, but I can't really say I know how to work with it. For example, how do you prove that det is an algebraic map? I'd like to show that GL(V) is a variety, without picking a basis for V. Edit: As pointed out in the comments, det is a map End(V) $\to$ End($\wedge^N V$) = k, where k is the base field. This map is far from being linear, in fact by picking a basis for V one can look up in any textbook that it is a polynomial of degree N in the entries of the corresponding matrix, i.e. an element of Sym$^N$End(V)$^*$. So the question boils down to identifying det as an element of SymEnd(V)$^*$ = SymEnd(V$^*$).","If V is a vector space of dimension N, and $f\colon V \to V$ is an endomorphism, one defines its determinant to be the scalar corresponding to the induced map $\wedge^N V \to \wedge^N V$. I like this definition, but I can't really say I know how to work with it. For example, how do you prove that det is an algebraic map? I'd like to show that GL(V) is a variety, without picking a basis for V. Edit: As pointed out in the comments, det is a map End(V) $\to$ End($\wedge^N V$) = k, where k is the base field. This map is far from being linear, in fact by picking a basis for V one can look up in any textbook that it is a polynomial of degree N in the entries of the corresponding matrix, i.e. an element of Sym$^N$End(V)$^*$. So the question boils down to identifying det as an element of SymEnd(V)$^*$ = SymEnd(V$^*$).",,"['linear-algebra', 'algebraic-geometry', 'category-theory', 'lie-groups']"
2,Generalizing Determinants Through Multilinear Algebra and Immanants,Generalizing Determinants Through Multilinear Algebra and Immanants,,"Let $V$ and $W$ be $n$-dimensional vector spaces (over $\mathbb{C}$ say) and $A: V\to W$ a linear map. We define $\det(A):\bigwedge^nV\to \bigwedge^nW$ to be the map induced by $A$ on the exterior algebras. In the case $V=W$ then we have that $\det(A)$ is a scalar matching the traditional definition of a determinant. Naively, for a partition $\lambda$ of $n$ one is tempted to define the map $\det_{\lambda}(A): \mathbb{S}_\lambda(V)\to\mathbb{S}_\lambda(V)$ between the Schur functors of $V$ to be the map induced by $A$, which is not necessarily one-dimensional. However, we do have that $\det(A)=\det_{(1, \dots ,1)}(A)$. My first question is, is there any insight into these maps? Do they show up at all in the literature, or are they needless or ""incorrect"" generalizations? Since $\dim(\text{Sym}^n(V))\neq 1$ one immediate problem with this generalization is that it does not turn into the permanent by replacing the exterior powers with symmetric powers, regardless if we go so far as to use Schur functors or not. (Edit: But it is one-dimensional as a representation of $\mathfrak{S}_n$) Alternatively, there is the generalization of the determinant called the immanant given by \begin{equation*} \text{Imm}_{\lambda}(A)=\sum\limits_{\sigma\in\mathfrak{S}_n}\chi_{\lambda}(\sigma)a_{1\sigma(1)}\cdots a_{n\sigma(n)} \end{equation*} where $\chi_\lambda$ is the character of the irreducible representation of $\mathfrak{S}_n$ corresponding to the (conjugacy class corresponding to the) partition $\lambda$, which of course turns out to be $\mathbb{S}_\lambda(V)$. This definition is nice since for $\lambda=(1,\dots,1)$ it lines up with the determinant, and for $\lambda=(n)$ it lines up with the permanent. So my second question is, do these two generalizations line up somehow? Am I missing something where $\det_\lambda(A)$ turns into $\text{Imm}_\lambda(A)$ via the trace or something like this?","Let $V$ and $W$ be $n$-dimensional vector spaces (over $\mathbb{C}$ say) and $A: V\to W$ a linear map. We define $\det(A):\bigwedge^nV\to \bigwedge^nW$ to be the map induced by $A$ on the exterior algebras. In the case $V=W$ then we have that $\det(A)$ is a scalar matching the traditional definition of a determinant. Naively, for a partition $\lambda$ of $n$ one is tempted to define the map $\det_{\lambda}(A): \mathbb{S}_\lambda(V)\to\mathbb{S}_\lambda(V)$ between the Schur functors of $V$ to be the map induced by $A$, which is not necessarily one-dimensional. However, we do have that $\det(A)=\det_{(1, \dots ,1)}(A)$. My first question is, is there any insight into these maps? Do they show up at all in the literature, or are they needless or ""incorrect"" generalizations? Since $\dim(\text{Sym}^n(V))\neq 1$ one immediate problem with this generalization is that it does not turn into the permanent by replacing the exterior powers with symmetric powers, regardless if we go so far as to use Schur functors or not. (Edit: But it is one-dimensional as a representation of $\mathfrak{S}_n$) Alternatively, there is the generalization of the determinant called the immanant given by \begin{equation*} \text{Imm}_{\lambda}(A)=\sum\limits_{\sigma\in\mathfrak{S}_n}\chi_{\lambda}(\sigma)a_{1\sigma(1)}\cdots a_{n\sigma(n)} \end{equation*} where $\chi_\lambda$ is the character of the irreducible representation of $\mathfrak{S}_n$ corresponding to the (conjugacy class corresponding to the) partition $\lambda$, which of course turns out to be $\mathbb{S}_\lambda(V)$. This definition is nice since for $\lambda=(1,\dots,1)$ it lines up with the determinant, and for $\lambda=(n)$ it lines up with the permanent. So my second question is, do these two generalizations line up somehow? Am I missing something where $\det_\lambda(A)$ turns into $\text{Imm}_\lambda(A)$ via the trace or something like this?",,"['linear-algebra', 'abstract-algebra', 'soft-question', 'representation-theory', 'multilinear-algebra']"
3,Prove that $\text{rank } T = \operatorname{rank} T^2 \iff \operatorname{Im}T \cap \ker T = \{ \vec 0\}$,Prove that,\text{rank } T = \operatorname{rank} T^2 \iff \operatorname{Im}T \cap \ker T = \{ \vec 0\},"$\newcommand{\r}{ \operatorname{rank} } $ Let $T: V\to V$ be a linear transformation with $\dim V< \infty$ . Prove that: $$ \r T = \r T^2 \iff \operatorname{Im} T \cap \ker T = \{ \vec 0 \}.$$ $""\Rightarrow""$ Let $\r T = \r T^2$ . Then, by rank - nullity theorem we have that $$\dim \ker T  =\dim \ker T^2 \tag 1.$$ But it is always true that: $\ker T \subseteq \ker T^2 .\tag 2$ By $(1),(2)$ we have that $\ker  T = \ker T^2.$ So, instead of $\r T = \r T^2$ we can say that $\ker T = \ker T^2$ and we need to prove that $\operatorname{Im} T \cap \ker T = \{ \vec 0\}$ . Proof: Suppose that there is a $z \in \operatorname{Im}T \cap \ker T$ with $z \neq 0$ . Since $z \in \ker T \implies T(z) = 0$ . Also, since $z \in \operatorname{Im}T \implies \exists y\in V$ such that $T(y) = z \implies T^2(y) = T(z) = 0.$ But this implies that $y \in \ker T^2 $ and by our hypothesis we have that $y \in \ker T \implies T(y) = 0 = z, $ which is absurd, because we assumed that $z \neq 0$ . $""\Leftarrow""$ We need to prove that $\ker T = \ker T^2$ or $\ker T^2  \subseteq \ker T.$ Proof: Let $x \in \ker T^2$ , which implies $T^2(x) = T\left(T(x)\right) = 0$ . It is implied $T(x) \in \ker T,$ but also $T(x) \in \operatorname{Im}T.$ Thus, $T(x) \in \operatorname{Im}T \cap \ker T = \{0\}$ . Thus, $T(x) = 0 \implies x \in \ker T.$ I would like to know if my reasoning is correct and if all the points are clear. Also, I would like to know if there is any shorter proof.","Let be a linear transformation with . Prove that: Let . Then, by rank - nullity theorem we have that But it is always true that: By we have that So, instead of we can say that and we need to prove that . Proof: Suppose that there is a with . Since . Also, since such that But this implies that and by our hypothesis we have that which is absurd, because we assumed that . We need to prove that or Proof: Let , which implies . It is implied but also Thus, . Thus, I would like to know if my reasoning is correct and if all the points are clear. Also, I would like to know if there is any shorter proof.","\newcommand{\r}{ \operatorname{rank} }  T: V\to V \dim V< \infty  \r T = \r T^2 \iff \operatorname{Im} T \cap \ker T = \{ \vec 0 \}. ""\Rightarrow"" \r T = \r T^2 \dim \ker T  =\dim \ker T^2 \tag 1. \ker T \subseteq \ker T^2 .\tag 2 (1),(2) \ker  T = \ker T^2. \r T = \r T^2 \ker T = \ker T^2 \operatorname{Im} T \cap \ker T = \{ \vec 0\} z \in \operatorname{Im}T \cap \ker T z \neq 0 z \in \ker T \implies T(z) = 0 z \in \operatorname{Im}T \implies \exists y\in V T(y) = z \implies T^2(y) = T(z) = 0. y \in \ker T^2  y \in \ker T \implies T(y) = 0 = z,  z \neq 0 ""\Leftarrow"" \ker T = \ker T^2 \ker T^2  \subseteq \ker T. x \in \ker T^2 T^2(x) = T\left(T(x)\right) = 0 T(x) \in \ker T, T(x) \in \operatorname{Im}T. T(x) \in \operatorname{Im}T \cap \ker T = \{0\} T(x) = 0 \implies x \in \ker T.","['linear-algebra', 'proof-verification', 'linear-transformations']"
4,Generators for a matrix group,Generators for a matrix group,,"Let's denote $\Gamma_0(4)$ the subgroup of $\mathrm{SL}_2(\mathbb Z)$: $$\Gamma_0(4):=\left\{\begin{pmatrix} a &b\\ 4c&d \end{pmatrix}\in \mathrm{SL}_2(\mathbb Z)\right\}.$$ We also define $A$ and $B$ in $\Gamma_0(4)$ as follow: $$\gamma_1:=\begin{pmatrix} 1 &1\\ 0&1 \end{pmatrix},$$ $$\gamma_2:=\begin{pmatrix} 1 &0\\ 4&1 \end{pmatrix}.$$ We already know (with sophisticated arguments using group actions) that $\Gamma_0(4)/\{\pm 1\}$ is generated by $\gamma_1$ and $\gamma_2$: $$\Gamma_0(4)/\{\pm 1\}=\langle \gamma_1,\gamma_2\rangle.$$ I would like to prove it with elementary arguments if possible. Do you have any leads/solutions to this problem?","Let's denote $\Gamma_0(4)$ the subgroup of $\mathrm{SL}_2(\mathbb Z)$: $$\Gamma_0(4):=\left\{\begin{pmatrix} a &b\\ 4c&d \end{pmatrix}\in \mathrm{SL}_2(\mathbb Z)\right\}.$$ We also define $A$ and $B$ in $\Gamma_0(4)$ as follow: $$\gamma_1:=\begin{pmatrix} 1 &1\\ 0&1 \end{pmatrix},$$ $$\gamma_2:=\begin{pmatrix} 1 &0\\ 4&1 \end{pmatrix}.$$ We already know (with sophisticated arguments using group actions) that $\Gamma_0(4)/\{\pm 1\}$ is generated by $\gamma_1$ and $\gamma_2$: $$\Gamma_0(4)/\{\pm 1\}=\langle \gamma_1,\gamma_2\rangle.$$ I would like to prove it with elementary arguments if possible. Do you have any leads/solutions to this problem?",,"['linear-algebra', 'arithmetic']"
5,Necessary and sufficient conditions for left and right eigenvectors to be equal,Necessary and sufficient conditions for left and right eigenvectors to be equal,,"Suppose I have a matrix $A$ such that for some eigenvalue $\lambda$, the left eigenvector corresponding to $\lambda$ is equal to the conjugate transpose of the right eigenvector corresponding to $\lambda$. That is, I have $\lambda$, $\mathbf u$ and $\mathbf v$ such that $$ A\mathbf u = \lambda \mathbf u, \qquad \mathbf v A = \mathbf v \lambda\quad\text{and}\quad \mathbf u = \mathbf v^*. $$ Obviously this will be the case for all eigenvalues if $A$ is Hermitian (or symmetric in the real case). However, I'm interested in the case where $A$ is not symmetric, and the relation holds only for some particular eigenvalue, not for all of them. I am interested in what properties the matrix $A$ must have in order for this to be the case. Are there any simple necessary and sufficient conditions in terms of the elements of $A$? Although I have stated the question more generally, I am actually interested in the case where $A$ has real, non-negative elements and is irreducible, and where $\lambda$ is the Perron-Frobenius eigenvalue. So if it helps to assume that $\lambda$ is real or that the elements of $\mathbf{u}$ and $\mathbf{v}$ are positive then please do so.","Suppose I have a matrix $A$ such that for some eigenvalue $\lambda$, the left eigenvector corresponding to $\lambda$ is equal to the conjugate transpose of the right eigenvector corresponding to $\lambda$. That is, I have $\lambda$, $\mathbf u$ and $\mathbf v$ such that $$ A\mathbf u = \lambda \mathbf u, \qquad \mathbf v A = \mathbf v \lambda\quad\text{and}\quad \mathbf u = \mathbf v^*. $$ Obviously this will be the case for all eigenvalues if $A$ is Hermitian (or symmetric in the real case). However, I'm interested in the case where $A$ is not symmetric, and the relation holds only for some particular eigenvalue, not for all of them. I am interested in what properties the matrix $A$ must have in order for this to be the case. Are there any simple necessary and sufficient conditions in terms of the elements of $A$? Although I have stated the question more generally, I am actually interested in the case where $A$ has real, non-negative elements and is irreducible, and where $\lambda$ is the Perron-Frobenius eigenvalue. So if it helps to assume that $\lambda$ is real or that the elements of $\mathbf{u}$ and $\mathbf{v}$ are positive then please do so.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
6,Dot product with which polynomial gives evaluation at $x_0$?,Dot product with which polynomial gives evaluation at ?,x_0,"I thought of following situation/problem, and was surprised that the solution did not jump out at me. Fix a positive integer $n$ and let $V_n$ be the finite-dimensional vector space of all polynomials of degree $\leq n$ with real coefficents. Make $V_n$ into a inner product space with respect to $$ \langle p,q \rangle = \int_{-1}^1 p(x) q(x) \ dx.$$ Now, fix some point $x_0 \in \mathbb{R}$ and consider the linear functional  \begin{align*} \varphi_{x_0} : V_n \to \mathbb{R} && \varphi_{x_0}(p) = p(x_0). \end{align*} Because we are in an inner product space, there is a unique polynomial $p_{x_0,n}$ such that \begin{align*} \varphi_{x_0}(p) = \langle p_{x_0,n},p \rangle =\int_{-1}^1 p_{x_0,n}(x) p(x) \ dx && \forall p \in V_n. \end{align*} Question: What is this polynomial!? One approach would be to apply the Gramm-Schmidt procedure to $1,x,\ldots,x^n$ to get an orthonormal basis $p_0,p_1,\ldots,p_n$ for $V_n$ and then calculate $\varphi_{x_0}$ on this basis. The required vector should be then $\sum_{i=0}^n \varphi_{x_0}(p_i) p_i$, but I am hoping for a more enlightening representation.","I thought of following situation/problem, and was surprised that the solution did not jump out at me. Fix a positive integer $n$ and let $V_n$ be the finite-dimensional vector space of all polynomials of degree $\leq n$ with real coefficents. Make $V_n$ into a inner product space with respect to $$ \langle p,q \rangle = \int_{-1}^1 p(x) q(x) \ dx.$$ Now, fix some point $x_0 \in \mathbb{R}$ and consider the linear functional  \begin{align*} \varphi_{x_0} : V_n \to \mathbb{R} && \varphi_{x_0}(p) = p(x_0). \end{align*} Because we are in an inner product space, there is a unique polynomial $p_{x_0,n}$ such that \begin{align*} \varphi_{x_0}(p) = \langle p_{x_0,n},p \rangle =\int_{-1}^1 p_{x_0,n}(x) p(x) \ dx && \forall p \in V_n. \end{align*} Question: What is this polynomial!? One approach would be to apply the Gramm-Schmidt procedure to $1,x,\ldots,x^n$ to get an orthonormal basis $p_0,p_1,\ldots,p_n$ for $V_n$ and then calculate $\varphi_{x_0}$ on this basis. The required vector should be then $\sum_{i=0}^n \varphi_{x_0}(p_i) p_i$, but I am hoping for a more enlightening representation.",,"['linear-algebra', 'polynomials', 'inner-products']"
7,Continuously extending a set of independent vectors to a basis.,Continuously extending a set of independent vectors to a basis.,,"Question: Let $I=(a,b)$ be an interval and let   $$v_i:I\to\mathbb{R}^n,\quad i=1,\ldots,k$$   be continuous curves such that $v_1(t),\ldots,v_k(t)$ are linearly independent in $\mathbb{R}^n$ for every $t\in I$. Can we always find continuous curves   $$v_{k+1},\ldots,v_n:I\to\mathbb{R}^n$$   such that $v_1(t),\ldots,v_n(t)$ forms a basis for $\mathbb{R}^n$ for each $t\in I$? I know that if $v_1(t),\ldots,v_k(t)$ is a set of linearly independent vectors, then we can extend it to a basis $v_1(t),\ldots,v_n(t)$ for $\mathbb{R}^n$. But how do we ensure that $v_{k+1},\ldots,v_n$ will be continuous? It is intuitively very clear that there must exist such continuous curves, because there are a lot of possible choices for extending the set to a basis. Surely, there must be no problem of doing it continuously, but I cannot find a way to prove it rigorously. It is easy to do this in $\mathbb{R}^3$ when we have two curves $v_1,v_2$. In that case, we can just take the cross product $$v_3(t)=v_1(t)\times v_2(t)\in\mathbb{R}^3.$$ But in general, I don't know any explicit expression for producing linearly independent vectors $v_{k+1},\ldots,v_n$ in terms of the previous ones. Is there any?","Question: Let $I=(a,b)$ be an interval and let   $$v_i:I\to\mathbb{R}^n,\quad i=1,\ldots,k$$   be continuous curves such that $v_1(t),\ldots,v_k(t)$ are linearly independent in $\mathbb{R}^n$ for every $t\in I$. Can we always find continuous curves   $$v_{k+1},\ldots,v_n:I\to\mathbb{R}^n$$   such that $v_1(t),\ldots,v_n(t)$ forms a basis for $\mathbb{R}^n$ for each $t\in I$? I know that if $v_1(t),\ldots,v_k(t)$ is a set of linearly independent vectors, then we can extend it to a basis $v_1(t),\ldots,v_n(t)$ for $\mathbb{R}^n$. But how do we ensure that $v_{k+1},\ldots,v_n$ will be continuous? It is intuitively very clear that there must exist such continuous curves, because there are a lot of possible choices for extending the set to a basis. Surely, there must be no problem of doing it continuously, but I cannot find a way to prove it rigorously. It is easy to do this in $\mathbb{R}^3$ when we have two curves $v_1,v_2$. In that case, we can just take the cross product $$v_3(t)=v_1(t)\times v_2(t)\in\mathbb{R}^3.$$ But in general, I don't know any explicit expression for producing linearly independent vectors $v_{k+1},\ldots,v_n$ in terms of the previous ones. Is there any?",,"['linear-algebra', 'general-topology', 'continuity']"
8,Range of A and null space of the transpose of A,Range of A and null space of the transpose of A,,"So I'm complete stuck with something. I know it the following statements are true (or at least the seem to be from the results that I got from messing around with it a bit on MATLAB), but I don't understand why they are true or how to show so. Let $A$ be and $m$X$n$ matrix. Show that: a) if $x \in N(A^TA)$ then $Ax$ is in both $R(A)$ and $N(A^T)$. For this one I messed around with it with my own examples and I got $Ax=0$, therefore satisfing the statement, but I don't understand what's actually going on. b) $N(A^TA)=N(A)$ again, makes sense when I see the results in MATLAB, but don't undestand why it works. c) $A$ and $A^TA$ have the same rank d) If $A$ has linearly independent columns, the $A^TA$ is nonsingular. For the last two I have no idea on how to even start showing the relationship. I feel like I'm missing some crucial relationship between $A$ and $A^TA$, but I'm just not seeing it. I would greatly appreciate any help of sugestions on how to show that these statements are true. Thank you very much.","So I'm complete stuck with something. I know it the following statements are true (or at least the seem to be from the results that I got from messing around with it a bit on MATLAB), but I don't understand why they are true or how to show so. Let $A$ be and $m$X$n$ matrix. Show that: a) if $x \in N(A^TA)$ then $Ax$ is in both $R(A)$ and $N(A^T)$. For this one I messed around with it with my own examples and I got $Ax=0$, therefore satisfing the statement, but I don't understand what's actually going on. b) $N(A^TA)=N(A)$ again, makes sense when I see the results in MATLAB, but don't undestand why it works. c) $A$ and $A^TA$ have the same rank d) If $A$ has linearly independent columns, the $A^TA$ is nonsingular. For the last two I have no idea on how to even start showing the relationship. I feel like I'm missing some crucial relationship between $A$ and $A^TA$, but I'm just not seeing it. I would greatly appreciate any help of sugestions on how to show that these statements are true. Thank you very much.",,"['linear-algebra', 'orthogonality']"
9,Can the cross product be a matrix?,Can the cross product be a matrix?,,"On the Wikipedia article on the cross product is says that a vector $a$ which is itself a cross product (that is $a=c\times d$), can be represented in the expression $a \times b$ for some other vector $b$ by the matrix $$[a]_{\times}=\begin{bmatrix} 0 & -a_3 & a_2 \\ a_3 & 0 & -a_1 \\ -a_2 & a_1 & 0 \end{bmatrix}$$ where $a\times b = [a]_{\times} b = (dc^T - cd^T)b = (c\times d)\times b$. I'm wondering if this matrix representation of the cross product of a vector encodes any of the other properties of the cross product. I don't see a way of extracting a scalar from it to reproduce the triple scalar product. It's determinant and trace are always $0$. $[a]_{\times}^T=-a$ It's Frobenius norm is $\sqrt{a_2^2+a_3^2+a_1^2+a_3^2+a_1^2+a_2^2} = \sqrt{2}\sqrt{a_1^2+a_2^2+a_3^2}=\sqrt{2}\|a\|$.  So up to a factor of $\sqrt{2}$, the Frobenius norm is the same as the norm of $a$. A vector orthogonal to $c$ and $d$ (finding such a vector is one of the main purposes of the cross product) will be a member of the kernel of this matrix.  $\operatorname{Ker}[a]_{\times}=t\begin{bmatrix} a_1 \\ a_2 \\ a_3\end{bmatrix}$ This means that the plane spanned by $c$ and $d$ is the row space of $[a]_{\times}$ because $\operatorname{Ker}(A)^{\bot} = \operatorname{Row}(A)$ for all matrices $A$.  And because this matrix equals its negative transpose, the plane is also given by the column space of $[a]_{\times}$. The mapping $[a]_{\times} \mapsto a$ is linear, so if someone could come up with an explicit expression for this mapping, then we could expression the triple scalar product in terms of that mapping.  As in, let $T$ be the name of that mapping, then $T(a)\cdot b = (c\times d) \cdot b$, though I admit this is cheating.  Even so, it'd be useful to have an explicit expression for $T$. Are there any other useful properties of this matrix?","On the Wikipedia article on the cross product is says that a vector $a$ which is itself a cross product (that is $a=c\times d$), can be represented in the expression $a \times b$ for some other vector $b$ by the matrix $$[a]_{\times}=\begin{bmatrix} 0 & -a_3 & a_2 \\ a_3 & 0 & -a_1 \\ -a_2 & a_1 & 0 \end{bmatrix}$$ where $a\times b = [a]_{\times} b = (dc^T - cd^T)b = (c\times d)\times b$. I'm wondering if this matrix representation of the cross product of a vector encodes any of the other properties of the cross product. I don't see a way of extracting a scalar from it to reproduce the triple scalar product. It's determinant and trace are always $0$. $[a]_{\times}^T=-a$ It's Frobenius norm is $\sqrt{a_2^2+a_3^2+a_1^2+a_3^2+a_1^2+a_2^2} = \sqrt{2}\sqrt{a_1^2+a_2^2+a_3^2}=\sqrt{2}\|a\|$.  So up to a factor of $\sqrt{2}$, the Frobenius norm is the same as the norm of $a$. A vector orthogonal to $c$ and $d$ (finding such a vector is one of the main purposes of the cross product) will be a member of the kernel of this matrix.  $\operatorname{Ker}[a]_{\times}=t\begin{bmatrix} a_1 \\ a_2 \\ a_3\end{bmatrix}$ This means that the plane spanned by $c$ and $d$ is the row space of $[a]_{\times}$ because $\operatorname{Ker}(A)^{\bot} = \operatorname{Row}(A)$ for all matrices $A$.  And because this matrix equals its negative transpose, the plane is also given by the column space of $[a]_{\times}$. The mapping $[a]_{\times} \mapsto a$ is linear, so if someone could come up with an explicit expression for this mapping, then we could expression the triple scalar product in terms of that mapping.  As in, let $T$ be the name of that mapping, then $T(a)\cdot b = (c\times d) \cdot b$, though I admit this is cheating.  Even so, it'd be useful to have an explicit expression for $T$. Are there any other useful properties of this matrix?",,"['linear-algebra', 'matrices', 'cross-product']"
10,"If $A \in M_{n,n}(\mathbb F)$ is invertible then $A = UPB$, $U$ is unipotent upper triangular, $B$ is upper triangular and $P$ is a permutation.","If  is invertible then ,  is unipotent upper triangular,  is upper triangular and  is a permutation.","A \in M_{n,n}(\mathbb F) A = UPB U B P","Let $\mathbb{F}$ be a field. If $A \in M_{n,n}(\mathbb F)$ is invertible, then $A$ can be written as $A = UPB$ , where $U$ is unipotent upper triangular, $B$ is upper triangular and $P$ a permutation matrix. A hint is given that one could relate the problem to elementary row and column operations. I know that if $A$ is invertible then we can row reduce $A$ to $I_n$ and this corresponds to a sequence of elementary matrices $E_1, \ldots, E_n$ . Also, $A=UPB$ would imply $\det(A) = \det(U) \det(P) \det(B)$ , which in turn implies that the diagonal entries of $B$ must be nonzero. I guess that the permutation matrix corresponds to column exchange.","Let be a field. If is invertible, then can be written as , where is unipotent upper triangular, is upper triangular and a permutation matrix. A hint is given that one could relate the problem to elementary row and column operations. I know that if is invertible then we can row reduce to and this corresponds to a sequence of elementary matrices . Also, would imply , which in turn implies that the diagonal entries of must be nonzero. I guess that the permutation matrix corresponds to column exchange.","\mathbb{F} A \in M_{n,n}(\mathbb F) A A = UPB U B P A A I_n E_1, \ldots, E_n A=UPB \det(A) = \det(U) \det(P) \det(B) B","['linear-algebra', 'abstract-algebra', 'matrices', 'algebraic-groups']"
11,Prove the matrix Holder inequality $|{\rm Tr}(DAB)|\le {\rm Tr}(D|A|^p)^{1/p} {\rm Tr}(D|B|^q)^{1/q}$,Prove the matrix Holder inequality,|{\rm Tr}(DAB)|\le {\rm Tr}(D|A|^p)^{1/p} {\rm Tr}(D|B|^q)^{1/q},"I am interested in the following version of the Holder inequality. Let $D \in M_n(\mathbb{C})$ be a positive semi-definite matrix of trace $1$ and $A, B \in M_n(\mathbb{C}).$ Does it follow that  $$ |\mbox{Tr } DAB| \leq (\mbox{Tr } D|A|^p)^{1/p} (\mbox{Tr } D|B|^q)^{1/q}, $$ where $1/p + 1/q = 1$? I found the case $D = I$ with proof (for the tracial state) in the literature but nothing more.","I am interested in the following version of the Holder inequality. Let $D \in M_n(\mathbb{C})$ be a positive semi-definite matrix of trace $1$ and $A, B \in M_n(\mathbb{C}).$ Does it follow that  $$ |\mbox{Tr } DAB| \leq (\mbox{Tr } D|A|^p)^{1/p} (\mbox{Tr } D|B|^q)^{1/q}, $$ where $1/p + 1/q = 1$? I found the case $D = I$ with proof (for the tracial state) in the literature but nothing more.",,"['linear-algebra', 'matrices', 'trace']"
12,Finding the generators of a subgroup of $\mathrm{SL}_2(\mathbb Z)$,Finding the generators of a subgroup of,\mathrm{SL}_2(\mathbb Z),"I am trying to solve the following problem: Let $T_{ij}(c)\in\mathrm{SL}_2(\mathbb Z)\ (i\neq j)$ be the elementary matrix which represents the elementary row operation of adding the $j$-th row multiplied by $c$ to the $i$-th row, $A:=T_{12}(2)$ and $B:=T_{21}(2)$.  Let $G$ be a subgroup $\left\{ \begin{pmatrix}a&b\\c&d\end{pmatrix}\in\mathrm{SL}_2(\mathbb Z):a\equiv d\equiv 1 \pmod 4, b\equiv c\equiv 0 \pmod 2 \right\}$. Show that $G = \langle A, B\rangle$. That $\langle A,B\rangle\subset G$ can be shown easily by induction.  The problem is the converse inclusion.  I tried to show it by using an analogy from the fact that $\mathrm{SL}_2$ is generated by $T_{ij}(c)$.  I tried to show that if $C\in G$ is written as a product $C_1\dots C_n$, where each $C_k$ is of the form $T_{i_kj_k}(c_k)$, then each $c_k$ is even, from which what I want to show follows.  However I cannot prove this (it may simply be wrong). I would be grateful if you could provide a clue.","I am trying to solve the following problem: Let $T_{ij}(c)\in\mathrm{SL}_2(\mathbb Z)\ (i\neq j)$ be the elementary matrix which represents the elementary row operation of adding the $j$-th row multiplied by $c$ to the $i$-th row, $A:=T_{12}(2)$ and $B:=T_{21}(2)$.  Let $G$ be a subgroup $\left\{ \begin{pmatrix}a&b\\c&d\end{pmatrix}\in\mathrm{SL}_2(\mathbb Z):a\equiv d\equiv 1 \pmod 4, b\equiv c\equiv 0 \pmod 2 \right\}$. Show that $G = \langle A, B\rangle$. That $\langle A,B\rangle\subset G$ can be shown easily by induction.  The problem is the converse inclusion.  I tried to show it by using an analogy from the fact that $\mathrm{SL}_2$ is generated by $T_{ij}(c)$.  I tried to show that if $C\in G$ is written as a product $C_1\dots C_n$, where each $C_k$ is of the form $T_{i_kj_k}(c_k)$, then each $c_k$ is even, from which what I want to show follows.  However I cannot prove this (it may simply be wrong). I would be grateful if you could provide a clue.",,"['linear-algebra', 'group-theory', 'matrices', 'modular-arithmetic']"
13,Proving differentiable mapping is onto,Proving differentiable mapping is onto,,"Let $f: \mathbb{R}^n \rightarrow \mathbb{R}^n$ be a differentiable mapping such that there is a $C > 0$ satisfying $C|x-y| \leq |f(x) - f(y)|$ for all $x,y \in \mathbb{R}^n$.  Prove that $\det f'(x) \neq 0$ for all $x \in \mathbb{R}^n$ and then deduce that $f(\mathbb{R}^n) = \mathbb{R}^n$. I am really stuck on this, but it's a problem I've been working on for a while.  I have used the first inequality to show that $f$ is one-to-one.  Am I on the right track, or is there something additional I can determine from this inequality?","Let $f: \mathbb{R}^n \rightarrow \mathbb{R}^n$ be a differentiable mapping such that there is a $C > 0$ satisfying $C|x-y| \leq |f(x) - f(y)|$ for all $x,y \in \mathbb{R}^n$.  Prove that $\det f'(x) \neq 0$ for all $x \in \mathbb{R}^n$ and then deduce that $f(\mathbb{R}^n) = \mathbb{R}^n$. I am really stuck on this, but it's a problem I've been working on for a while.  I have used the first inequality to show that $f$ is one-to-one.  Am I on the right track, or is there something additional I can determine from this inequality?",,"['linear-algebra', 'analysis', 'multivariable-calculus']"
14,Pedagogy of Teaching the Inverse Matrix Method,Pedagogy of Teaching the Inverse Matrix Method,,"I am teaching a group of (ordinary rather than honours) second-year engineers and we are studying matrices. I told the class today that as far as I could see we were only studying matrices and, particularly, the inverse matrix method as an introduction to more advanced matrix methods that would be studied in future. However, the maths modules that they take in their next, final, third year are differential equations (no linear systems of differential equations) and, well, probability and statistics. The only use that I can see that this group have for matrices is for solving linear systems. I know that there are plenty of more reasons to study matrices and in particular matrix inverses but this cohort will not see them. It obviously strikes me as odd that the syllabus would recommend that we use the Inverse Matrix Method rather than the full Gaussian elimination theory. Therefore my question is: Assuming that we want to solve a linear system $A\mathbf{x}=\mathbf{b}$, what advantages, if any, does the inverse matrix method have over the full Gaussian elimination theory. Thank you in advance for any answers; I am struggling to find one!","I am teaching a group of (ordinary rather than honours) second-year engineers and we are studying matrices. I told the class today that as far as I could see we were only studying matrices and, particularly, the inverse matrix method as an introduction to more advanced matrix methods that would be studied in future. However, the maths modules that they take in their next, final, third year are differential equations (no linear systems of differential equations) and, well, probability and statistics. The only use that I can see that this group have for matrices is for solving linear systems. I know that there are plenty of more reasons to study matrices and in particular matrix inverses but this cohort will not see them. It obviously strikes me as odd that the syllabus would recommend that we use the Inverse Matrix Method rather than the full Gaussian elimination theory. Therefore my question is: Assuming that we want to solve a linear system $A\mathbf{x}=\mathbf{b}$, what advantages, if any, does the inverse matrix method have over the full Gaussian elimination theory. Thank you in advance for any answers; I am struggling to find one!",,"['linear-algebra', 'matrices', 'education']"
15,Matrices whose Linear Combinations are All Singular,Matrices whose Linear Combinations are All Singular,,"I'd like to know if the following problem of elementary linear algebra is already solved / solvable. For two (singular) $n\times n$ matrices $P$ and $Q$, if $\det(\lambda P+\mu Q)=0$ for any $\lambda,\mu\in\mathbb{R}$, what are conditions on $P$ and $Q$?","I'd like to know if the following problem of elementary linear algebra is already solved / solvable. For two (singular) $n\times n$ matrices $P$ and $Q$, if $\det(\lambda P+\mu Q)=0$ for any $\lambda,\mu\in\mathbb{R}$, what are conditions on $P$ and $Q$?",,['linear-algebra']
16,Inequality involving matrix inverse elements,Inequality involving matrix inverse elements,,"Let $A$ be an $N \times N$ matrix with all nonnegative entries and row sums strictly less than one, let $v$ be an $N$ dimensional vector with all nonnegative entries and weakly lower than one, let $B\equiv\left(I-A\mathrm{diag}\left(v\right)\right)^{-1}$ and let $B^*\equiv\left(I-A\right)^{-1}$ , where $\mathrm{diag}\left(v\right)$ is the diagonal matrix formed from vector $v$ . I want to show that for any $i,j=1,...,N$ the following inequality holds: $$ v_{j}b_{ji}^2+\sum_{k}\left(1-v_{k}\right) b_{jk}b_{ki}^{2}\leq b_{ji} b_{ii}^{*}. $$ Simulations suggest that this is true. The case in which $v$ is the vector of all ones follows from the fact that $b_{ji} \leq b_{ii}^{*}$ , which is shown here . The case with $A$ diagonal is trivial for $i \neq j$ , whereas for $i=j$ it boils down to showing $$v_{i}b_{ii}+(1-v_{i})b_{ii}^{2}\leq b_{ii}^{*},$$ which can be shown by plugging in for $$b_{ii}=\frac{1}{1-v_{i}a_{ii}},\quad b_{ii}^{*}=\frac{1}{1-a_{ii}},$$ and basic algebra. Apart from these simple cases, I have been able to show the result for the case in which $j=i$ , but it is an arduous induction proof that does not extend to the case in which $i \neq j$ . We would appreciate hints for approaches that could be useful to prove the claim. The problem above comes from a more general problem in matrix algebra, which is to show that $ii$ of the following matrix is less than $b_{ii}^*$ , $$J \equiv \left(\mathrm{diag}\left\{ B^{T}x\right\} \right)^{-1}B^{T}\left[ \mathrm{diag}\left\{ v\right\}\mathrm{diag}\left\{ x\right\} +\mathrm{diag}\left\{ 1-v\right\} \mathrm{diag}\left\{ B^{T}x\right\} \right]B,$$ with $x$ being an $N$ dimensional vector in the simplex, i.e., $x_j \geq 0,\sum_j x_j=1$ . It can be shown that the diagonal elements will be maximized with respect to $x$ when $x$ is at a corner of the simplex, and that if $x_j = 1$ then $$ J_{ii} = v_{j}b_{ji}+\sum_{k}\left(1-v_{k}\right) \frac{b_{jk}b_{ki}^{2}}{b_{ji}}\leq b_{ii}^{*}. $$ Multiplying by $b_{ji}$ on both sides leads to the inequality postulated above. A closely related problem is to show that the spectral radius of $J\mathrm{diag}(\iota-A \iota)$ is lower than one (where $\iota$ is the vector of all ones), see here . (Note that if $v=\iota$ then $J\mathrm{diag}(\iota-A \iota)\iota = J(I-A)\iota = \iota$ and so the spectral radius of $J\mathrm{diag}(\iota-A \iota)$ is one.)","Let be an matrix with all nonnegative entries and row sums strictly less than one, let be an dimensional vector with all nonnegative entries and weakly lower than one, let and let , where is the diagonal matrix formed from vector . I want to show that for any the following inequality holds: Simulations suggest that this is true. The case in which is the vector of all ones follows from the fact that , which is shown here . The case with diagonal is trivial for , whereas for it boils down to showing which can be shown by plugging in for and basic algebra. Apart from these simple cases, I have been able to show the result for the case in which , but it is an arduous induction proof that does not extend to the case in which . We would appreciate hints for approaches that could be useful to prove the claim. The problem above comes from a more general problem in matrix algebra, which is to show that of the following matrix is less than , with being an dimensional vector in the simplex, i.e., . It can be shown that the diagonal elements will be maximized with respect to when is at a corner of the simplex, and that if then Multiplying by on both sides leads to the inequality postulated above. A closely related problem is to show that the spectral radius of is lower than one (where is the vector of all ones), see here . (Note that if then and so the spectral radius of is one.)","A N \times N v N B\equiv\left(I-A\mathrm{diag}\left(v\right)\right)^{-1} B^*\equiv\left(I-A\right)^{-1} \mathrm{diag}\left(v\right) v i,j=1,...,N 
v_{j}b_{ji}^2+\sum_{k}\left(1-v_{k}\right) b_{jk}b_{ki}^{2}\leq b_{ji} b_{ii}^{*}.
 v b_{ji} \leq b_{ii}^{*} A i \neq j i=j v_{i}b_{ii}+(1-v_{i})b_{ii}^{2}\leq b_{ii}^{*}, b_{ii}=\frac{1}{1-v_{i}a_{ii}},\quad b_{ii}^{*}=\frac{1}{1-a_{ii}}, j=i i \neq j ii b_{ii}^* J \equiv \left(\mathrm{diag}\left\{ B^{T}x\right\} \right)^{-1}B^{T}\left[ \mathrm{diag}\left\{ v\right\}\mathrm{diag}\left\{ x\right\} +\mathrm{diag}\left\{ 1-v\right\} \mathrm{diag}\left\{ B^{T}x\right\} \right]B, x N x_j \geq 0,\sum_j x_j=1 x x x_j = 1 
J_{ii} = v_{j}b_{ji}+\sum_{k}\left(1-v_{k}\right) \frac{b_{jk}b_{ki}^{2}}{b_{ji}}\leq b_{ii}^{*}.
 b_{ji} J\mathrm{diag}(\iota-A \iota) \iota v=\iota J\mathrm{diag}(\iota-A \iota)\iota = J(I-A)\iota = \iota J\mathrm{diag}(\iota-A \iota)","['linear-algebra', 'matrices', 'inequality']"
17,How do you find (continuous) bounds on the matrix exponential,How do you find (continuous) bounds on the matrix exponential,,"Let $A$ be a $n \times n$ real or complex matrix. I am interested in bounds on the matrix exponential $e^{A t}$ , for $t \geq 0$ . In particular: is there a continuous function $C: M_{n\times n} \rightarrow \mathbb{R}_+$ such that $$\|e^{At}\| \leq C(A) e^{\Lambda(A) t} (1 + t^{n-1})$$ As some background: for any individual $A$ , I know how to find the big- $\mathcal{O}$ behavior of $e^{A t}$ . Namely, re-write $A$ as $T B T^{-1}$ where $B$ is in Jordan canonical form, compute the matrix exponential of $B$ , then translate back to the matrix exponential of $A$ . This gives the upper bound: $$\|e^{A t} \| = \mathcal{O}(e^{\Lambda(A) t} (1+t^{n-1}))$$ where $\Lambda(A)$ is the largest real part of any eigenvalue of $A$ . Now, the eigenvalues of $A$ depend continuously on $A$ , so $\Lambda(A)$ is continuous. However the big- $\mathcal{O}$ constant in this expression depends on the matrix $T$ used above. However, $T$ will not vary continuously with $A$ , and may even be arbitrarily large when $\|A\|$ is bounded. So, this doesn't give a continuous bound on the matrix exponential. Hence my question: is there a continuous function $C(A)$ such that: $$\|e^{At}\| \leq C(A) e^{\Lambda(A) t} (1 + t^{n-1})$$","Let be a real or complex matrix. I am interested in bounds on the matrix exponential , for . In particular: is there a continuous function such that As some background: for any individual , I know how to find the big- behavior of . Namely, re-write as where is in Jordan canonical form, compute the matrix exponential of , then translate back to the matrix exponential of . This gives the upper bound: where is the largest real part of any eigenvalue of . Now, the eigenvalues of depend continuously on , so is continuous. However the big- constant in this expression depends on the matrix used above. However, will not vary continuously with , and may even be arbitrarily large when is bounded. So, this doesn't give a continuous bound on the matrix exponential. Hence my question: is there a continuous function such that:",A n \times n e^{A t} t \geq 0 C: M_{n\times n} \rightarrow \mathbb{R}_+ \|e^{At}\| \leq C(A) e^{\Lambda(A) t} (1 + t^{n-1}) A \mathcal{O} e^{A t} A T B T^{-1} B B A \|e^{A t} \| = \mathcal{O}(e^{\Lambda(A) t} (1+t^{n-1})) \Lambda(A) A A A \Lambda(A) \mathcal{O} T T A \|A\| C(A) \|e^{At}\| \leq C(A) e^{\Lambda(A) t} (1 + t^{n-1}),"['linear-algebra', 'matrices', 'inequality', 'matrix-exponential']"
18,Rank of a module over an integral domain,Rank of a module over an integral domain,,"Suppose $A$ be an integral domain. The rank of an $A$-module $M$ is defined to be the maximal number of $A$-linear independent elements of $M$. Let $S=A-\{0\}$. Then $S^{-1}A:=k$ is a field and it can be proved that rank $M=$ the $k$-vector space dimension of $M\otimes_{A}k$. Hence rank is well defined. Now suppose $B\supset A$, where $B$ is also a domain. Let $T=B-\{0\}$ and $L=T^{-1}B$. It is also given that $L/k$ is finite algebraic extension. Let $[L:k]=r$. I want to prove that $\operatorname{rank}B$ as an $A$-module is equals to $r$. Clearly $k\subset S^{-1}B\subset L$. Hence $\operatorname{rank}B\leq r$. How to prove that rank is exactly equals to $r$? Thank you in advance.","Suppose $A$ be an integral domain. The rank of an $A$-module $M$ is defined to be the maximal number of $A$-linear independent elements of $M$. Let $S=A-\{0\}$. Then $S^{-1}A:=k$ is a field and it can be proved that rank $M=$ the $k$-vector space dimension of $M\otimes_{A}k$. Hence rank is well defined. Now suppose $B\supset A$, where $B$ is also a domain. Let $T=B-\{0\}$ and $L=T^{-1}B$. It is also given that $L/k$ is finite algebraic extension. Let $[L:k]=r$. I want to prove that $\operatorname{rank}B$ as an $A$-module is equals to $r$. Clearly $k\subset S^{-1}B\subset L$. Hence $\operatorname{rank}B\leq r$. How to prove that rank is exactly equals to $r$? Thank you in advance.",,"['linear-algebra', 'vector-spaces']"
19,Intersection of $n+k$ subspaces of $\mathbb{R}^n$,Intersection of  subspaces of,n+k \mathbb{R}^n,"In $\mathbb{R}^n$, $n\ge 3$ consider $n+k$ vectors $v_1,\dots, v_{n+k}$, $k\ge 1$ such that subset of cardinality $n$ of $\{v_1,\dots, v_{n+k}\}$ is composed of linearly independent vectors of $\mathbb{R}^n$. Define the subsets $W_i$ of $\mathbb{R}^n$ as $W_i:=\{w\in \mathbb{R}^n|w\cdot v_i\le 0\}$, $i=1,\dots, n+k$. What is a sufficient condition on $v_1,\dots,v_{n+k}$ to guarantee $\cap_{i=1}^{n+k}W_i=\{0\}$? In the meantime I had an idea: A sufficient condition for $\cap_{i=1}^{n+k}W_i=\{0\}$ could be, up to renumbering the indexes, $v_{n+1}=\alpha_1v_1+\dots \alpha_nv_n$ with $\alpha_i<0$ for all $i=1,\dots,n$. In this way, if $w\in \cap_{i=1}^{n}W_i,w\neq 0$ it's clear that $w\notin W_{n+1}$. But this is a rather unpleasant condition: for $n$ big it will be very complicated to compute the $\alpha_i$ (as one should compute the inverse of the Gram matrix associated to the basis). Do you know a better condition?","In $\mathbb{R}^n$, $n\ge 3$ consider $n+k$ vectors $v_1,\dots, v_{n+k}$, $k\ge 1$ such that subset of cardinality $n$ of $\{v_1,\dots, v_{n+k}\}$ is composed of linearly independent vectors of $\mathbb{R}^n$. Define the subsets $W_i$ of $\mathbb{R}^n$ as $W_i:=\{w\in \mathbb{R}^n|w\cdot v_i\le 0\}$, $i=1,\dots, n+k$. What is a sufficient condition on $v_1,\dots,v_{n+k}$ to guarantee $\cap_{i=1}^{n+k}W_i=\{0\}$? In the meantime I had an idea: A sufficient condition for $\cap_{i=1}^{n+k}W_i=\{0\}$ could be, up to renumbering the indexes, $v_{n+1}=\alpha_1v_1+\dots \alpha_nv_n$ with $\alpha_i<0$ for all $i=1,\dots,n$. In this way, if $w\in \cap_{i=1}^{n}W_i,w\neq 0$ it's clear that $w\notin W_{n+1}$. But this is a rather unpleasant condition: for $n$ big it will be very complicated to compute the $\alpha_i$ (as one should compute the inverse of the Gram matrix associated to the basis). Do you know a better condition?",,"['linear-algebra', 'inequality', 'convex-analysis', 'euclidean-geometry']"
20,Number of subsets when each pair of distinct elements is contained in exactly one subset,Number of subsets when each pair of distinct elements is contained in exactly one subset,,"Let $E$ be a set of cardinality $n$. Suppose $M_1, M_2, .. , M_m$ are    distinct proper subsets of $E$ such that for each pair of distinct elements $x_1, x_2\in E$, there is exactly one $M_i\supseteq\{x_1,x_2\}$. Prove that $m \ge n$. It's obvious $n$ has to be greater or equal to $3$. Also, for $n=3$, it's easy to prove it, but I have no idea how to extend it. I think a proof using induction by $n$ is possible.","Let $E$ be a set of cardinality $n$. Suppose $M_1, M_2, .. , M_m$ are    distinct proper subsets of $E$ such that for each pair of distinct elements $x_1, x_2\in E$, there is exactly one $M_i\supseteq\{x_1,x_2\}$. Prove that $m \ge n$. It's obvious $n$ has to be greater or equal to $3$. Also, for $n=3$, it's easy to prove it, but I have no idea how to extend it. I think a proof using induction by $n$ is possible.",,['linear-algebra']
21,Can one factor matrices?,Can one factor matrices?,,I know that one can factor integers as a product of prime numbers. Is there an analog of it to matrices? Can we define prime matrices such that every matrix is a product of prime matrices? Is there any applications of factorization of matrices?,I know that one can factor integers as a product of prime numbers. Is there an analog of it to matrices? Can we define prime matrices such that every matrix is a product of prime matrices? Is there any applications of factorization of matrices?,,"['linear-algebra', 'matrices', 'prime-factorization']"
22,Approximate spectral decomposition,Approximate spectral decomposition,,"I am interested in effective and computations for finding approximate spectral decompositions in some suitable format. Namely, let $A: H \rightarrow H$ be a Hermitian operator on an $n-$dimensional Hilbert space $H$ with the spectrum $\{\lambda_1, \ldots \lambda_m\}, m \leq n$. Then, $A$ can be decomposed as: $$ A = \sum_{i=1}^{m}\lambda_i P_i,$$ where $P_i, i=1,\ldots m$ are orthogonal projections with $P_i, P_j = 0, i \neq j$ onto the eigenspaces $H_i = \ker \{ \lambda_i I - A \}$ such that: $$  H= \displaystyle \underset{i=1}{\overset{m}{\oplus}} H_i.$$ In an approximate format, the theorem can be stated as follows (p. 380): for any $\varepsilon > 0$, there exist projections $P_i, i=1, \ldots n$ with $P_i, P_j = 0, i \neq j$, and real numbers $\alpha_1, \ldots \alpha_n$ such that $\big|\big| A - \displaystyle \sum_{i=1}^{n} \alpha_i P_i \big|\big| \leq \varepsilon$. What about the approximate eigenspaces? A particular example is this article , but it addresses exact spectral decomposition at the cost of additional input (cardinality of spectrum).","I am interested in effective and computations for finding approximate spectral decompositions in some suitable format. Namely, let $A: H \rightarrow H$ be a Hermitian operator on an $n-$dimensional Hilbert space $H$ with the spectrum $\{\lambda_1, \ldots \lambda_m\}, m \leq n$. Then, $A$ can be decomposed as: $$ A = \sum_{i=1}^{m}\lambda_i P_i,$$ where $P_i, i=1,\ldots m$ are orthogonal projections with $P_i, P_j = 0, i \neq j$ onto the eigenspaces $H_i = \ker \{ \lambda_i I - A \}$ such that: $$  H= \displaystyle \underset{i=1}{\overset{m}{\oplus}} H_i.$$ In an approximate format, the theorem can be stated as follows (p. 380): for any $\varepsilon > 0$, there exist projections $P_i, i=1, \ldots n$ with $P_i, P_j = 0, i \neq j$, and real numbers $\alpha_1, \ldots \alpha_n$ such that $\big|\big| A - \displaystyle \sum_{i=1}^{n} \alpha_i P_i \big|\big| \leq \varepsilon$. What about the approximate eigenspaces? A particular example is this article , but it addresses exact spectral decomposition at the cost of additional input (cardinality of spectrum).",,"['linear-algebra', 'complex-analysis', 'functional-analysis', 'computability', 'constructive-mathematics']"
23,Eigenvalues of large tridiagonal matrix,Eigenvalues of large tridiagonal matrix,,"Given $a, b \in \Bbb R$ , consider the following large tridiagonal matrix $$M := \begin{pmatrix}         a^2 & b & 0 & 0 & \cdots \\         b & (a+1)^2 & b & 0 & \cdots & \\         0 & b & (a+2)^2 & b & \cdots  \\         \vdots  & \vdots  & \vdots & \vdots & \ddots         \end{pmatrix}$$ What can be said about its eigenvalues? Are analytic expressions known? Or, at least, properties of the eigenvalues?","Given , consider the following large tridiagonal matrix What can be said about its eigenvalues? Are analytic expressions known? Or, at least, properties of the eigenvalues?","a, b \in \Bbb R M := \begin{pmatrix}
        a^2 & b & 0 & 0 & \cdots \\
        b & (a+1)^2 & b & 0 & \cdots & \\
        0 & b & (a+2)^2 & b & \cdots  \\
        \vdots  & \vdots  & \vdots & \vdots & \ddots
        \end{pmatrix}","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'tridiagonal-matrices']"
24,Linear Algebra Book Recommendation like Tao's Analysis,Linear Algebra Book Recommendation like Tao's Analysis,,"I am looking for a book that explains Linear Algebra, where it is build from axioms to higher level of Linear Algebra. It does not have to be a book on elementary level. As example from other fields, Tao's Analysis would be perfect example. Do you know if there is a book which is in similar manner as Tao's Analysis? Thanks","I am looking for a book that explains Linear Algebra, where it is build from axioms to higher level of Linear Algebra. It does not have to be a book on elementary level. As example from other fields, Tao's Analysis would be perfect example. Do you know if there is a book which is in similar manner as Tao's Analysis? Thanks",,"['linear-algebra', 'book-recommendation']"
25,Why are 1 and -1 eigenvalues of this matrix?,Why are 1 and -1 eigenvalues of this matrix?,,"This is a subject I've been working on for a very long time now, but still did not manage to fully understand the interesting properties of this matrix. I have already asked a (viewed but unanswered) question about the same matrices ( cf. here ), but the question here is different. First, let's define two matrices: $\mathbf{N}$ is the following matrix: \begin{equation} \mathbf{N}=\begin{bmatrix} \mathbf{I}_n & \mathbf{0}_n \\  \mathbf{0}_n & \mathbf{P}^{-1}\begin{bmatrix}1 & && \\ & \ddots && \\ & & 1& \\ &&& -1 \end{bmatrix}\mathbf{P}  \end{bmatrix}\in\mathbb{R}^{2n\times2n} \end{equation} where $\mathbf{P}\in\mathbb{R}^{n\times n}$ is any invertible matrix. with $\omega_i>0$ and $t>0$, the block-diagonal matrix: \begin{equation} \mathbf{S}(t)=\begin{bmatrix} \begin{bmatrix} \cos(\omega_1t) & \\& \ddots & \\ & & \cos(\omega_n t) \end{bmatrix} & \begin{bmatrix} \dfrac{\sin(\omega_1t)}{\omega_1} & \\& \ddots & \\ & & \dfrac{\sin(\omega_nt)}{\omega_n} \end{bmatrix} \\ \begin{bmatrix}  -\omega_1 \sin(\omega_1t) & \\& \ddots & \\ & & -\omega_n\sin(\omega_n t) \end{bmatrix} &  \begin{bmatrix} \cos(\omega_1t) & \\& \ddots & \\ & & \cos(\omega_n t) \end{bmatrix}\end{bmatrix}\in\mathbb{R}^{2n\times2n} \end{equation} The eigenvalues of $\mathbf N$ are of course 1 (multiplicity $2n-1$) and $-1$ (multiplicity $1$). The eigenvalues of $\mathbf{S}(t)$, which is an exponential matrix, are the $n$ couples of the complex conjugates $(\exp(i\omega_jt),\overline{\exp(i\omega_jt)})$. Now, we can define $\forall t>0$, $\mathbf{A}(t)=\mathbf N\mathbf S(t)$. We know that the product of the eigenvalues of $\mathbf{A}(t)$ is the product of those of $\mathbf{N}$ and $\mathbf S(t)$, i.e. $-1$. I observe an interesting property but can't prove where it stems from: $1$ and $-1$ are eigenvalues of  $\mathbf{A}(t)$ ($\forall t$); $1$ and $-1$ are $\color{red}{\text{not}}$  eigenvalues of  $\mathbf{A}(t_2)\mathbf{A}(t)$ ($\forall t,t_2$, except maybe for specific values of $\mathbf P$ and $\omega_k$); $1$ and $-1$ are eigenvalues of  $\mathbf{A}(t_3)\mathbf{A}(t_2)\mathbf{A}(t)$ ($\forall t,t_2,t_3$); $1$ and $-1$ are $\color{red}{\text{not}}$  eigenvalues of  $\mathbf{A}(t_4)\mathbf{A}(t_3)\mathbf{A}(t_2)\mathbf{A}(t)$ ($\forall t,t_2,t_3,t_4$, except maybe for specific values of $\mathbf P$ and $\omega_k$); $\dots$ I managed to prove $1$ and $-1$ are eigenvalues of  $\mathbf{A}(t)$ by considering $\mathbf{S}(t)\pm\operatorname{diag}(1,\dots,1,-1,\dots,1)$, calculating its kernel, and building the appropriate vectors (without having to calculate them explicitly ). Also, I understand that the product of the eigenvalues of $\mathbf{A}(t_2)\mathbf{A}(t)$ is 1, while that of $\mathbf{A}(t_3)\mathbf{A}(t_2)\mathbf{A}(t)$ is -1, but that does not prove anything. Questions 1) Any suggestion to prove the framed observation would be very welcome: why are apparently 1 and -1 eigenvalues of $\prod_{i=1}^m A(t_i)$ if and only if $m$ is odd? 2) Also, I have the impression that there exists a powerful mathematical framework to study these matrices, but I can't figure out which one, as not being a mathematician; Lie algebra because $\mathbf S(t)$ is an exponential? Galois groups because the eigenvalues are complex conjugate? Zariksi topology because @loup blanc mentioned it (see end of answer)? Edit A simple Mathematica file to reproduce the results is available here . Just play with the arguments of calculateEigenvals to change the dimension $n$ or/and the exponent $m$ (to prove: 1,-1 eigenvalues iff $m$ is odd).","This is a subject I've been working on for a very long time now, but still did not manage to fully understand the interesting properties of this matrix. I have already asked a (viewed but unanswered) question about the same matrices ( cf. here ), but the question here is different. First, let's define two matrices: $\mathbf{N}$ is the following matrix: \begin{equation} \mathbf{N}=\begin{bmatrix} \mathbf{I}_n & \mathbf{0}_n \\  \mathbf{0}_n & \mathbf{P}^{-1}\begin{bmatrix}1 & && \\ & \ddots && \\ & & 1& \\ &&& -1 \end{bmatrix}\mathbf{P}  \end{bmatrix}\in\mathbb{R}^{2n\times2n} \end{equation} where $\mathbf{P}\in\mathbb{R}^{n\times n}$ is any invertible matrix. with $\omega_i>0$ and $t>0$, the block-diagonal matrix: \begin{equation} \mathbf{S}(t)=\begin{bmatrix} \begin{bmatrix} \cos(\omega_1t) & \\& \ddots & \\ & & \cos(\omega_n t) \end{bmatrix} & \begin{bmatrix} \dfrac{\sin(\omega_1t)}{\omega_1} & \\& \ddots & \\ & & \dfrac{\sin(\omega_nt)}{\omega_n} \end{bmatrix} \\ \begin{bmatrix}  -\omega_1 \sin(\omega_1t) & \\& \ddots & \\ & & -\omega_n\sin(\omega_n t) \end{bmatrix} &  \begin{bmatrix} \cos(\omega_1t) & \\& \ddots & \\ & & \cos(\omega_n t) \end{bmatrix}\end{bmatrix}\in\mathbb{R}^{2n\times2n} \end{equation} The eigenvalues of $\mathbf N$ are of course 1 (multiplicity $2n-1$) and $-1$ (multiplicity $1$). The eigenvalues of $\mathbf{S}(t)$, which is an exponential matrix, are the $n$ couples of the complex conjugates $(\exp(i\omega_jt),\overline{\exp(i\omega_jt)})$. Now, we can define $\forall t>0$, $\mathbf{A}(t)=\mathbf N\mathbf S(t)$. We know that the product of the eigenvalues of $\mathbf{A}(t)$ is the product of those of $\mathbf{N}$ and $\mathbf S(t)$, i.e. $-1$. I observe an interesting property but can't prove where it stems from: $1$ and $-1$ are eigenvalues of  $\mathbf{A}(t)$ ($\forall t$); $1$ and $-1$ are $\color{red}{\text{not}}$  eigenvalues of  $\mathbf{A}(t_2)\mathbf{A}(t)$ ($\forall t,t_2$, except maybe for specific values of $\mathbf P$ and $\omega_k$); $1$ and $-1$ are eigenvalues of  $\mathbf{A}(t_3)\mathbf{A}(t_2)\mathbf{A}(t)$ ($\forall t,t_2,t_3$); $1$ and $-1$ are $\color{red}{\text{not}}$  eigenvalues of  $\mathbf{A}(t_4)\mathbf{A}(t_3)\mathbf{A}(t_2)\mathbf{A}(t)$ ($\forall t,t_2,t_3,t_4$, except maybe for specific values of $\mathbf P$ and $\omega_k$); $\dots$ I managed to prove $1$ and $-1$ are eigenvalues of  $\mathbf{A}(t)$ by considering $\mathbf{S}(t)\pm\operatorname{diag}(1,\dots,1,-1,\dots,1)$, calculating its kernel, and building the appropriate vectors (without having to calculate them explicitly ). Also, I understand that the product of the eigenvalues of $\mathbf{A}(t_2)\mathbf{A}(t)$ is 1, while that of $\mathbf{A}(t_3)\mathbf{A}(t_2)\mathbf{A}(t)$ is -1, but that does not prove anything. Questions 1) Any suggestion to prove the framed observation would be very welcome: why are apparently 1 and -1 eigenvalues of $\prod_{i=1}^m A(t_i)$ if and only if $m$ is odd? 2) Also, I have the impression that there exists a powerful mathematical framework to study these matrices, but I can't figure out which one, as not being a mathematician; Lie algebra because $\mathbf S(t)$ is an exponential? Galois groups because the eigenvalues are complex conjugate? Zariksi topology because @loup blanc mentioned it (see end of answer)? Edit A simple Mathematica file to reproduce the results is available here . Just play with the arguments of calculateEigenvals to change the dimension $n$ or/and the exponent $m$ (to prove: 1,-1 eigenvalues iff $m$ is odd).",,"['linear-algebra', 'matrices', 'determinant', 'zariski-topology']"
26,"nilpotent elements of $M_2(\mathbb{R})$, $M_2(\mathbb{Z}/4\mathbb{Z})$","nilpotent elements of ,",M_2(\mathbb{R}) M_2(\mathbb{Z}/4\mathbb{Z}),Let $R$ be a ring. We define $\mathfrak{N}(R)$ to be the set of nilpotent elements in $R$ . Find $\mathfrak{N}(R)$ for: $R = M_2(\mathbb{R})$ $R = M_2(\mathbb{Z}/4\mathbb{Z})$,Let be a ring. We define to be the set of nilpotent elements in . Find for:,R \mathfrak{N}(R) R \mathfrak{N}(R) R = M_2(\mathbb{R}) R = M_2(\mathbb{Z}/4\mathbb{Z}),"['linear-algebra', 'abstract-algebra']"
27,Determining consistency of a general overdetermined linear system,Determining consistency of a general overdetermined linear system,,"For $m > 2$ , consider the $m \times 2$ (overdetermined) linear system $$A \mathbf{x} = \mathbf{b}$$ with (general) coefficients in a field $\mathbb{F}$ ; in components we write the system as $$\left(\begin{array}{cc}a_{11} & a_{12} \\ \vdots & \vdots \\ a_{m1} & a_{m2} \end{array}\right) \left(\begin{array}{c}x_1 \\ x_2\end{array}\right) = \left(\begin{array}{c}b_1 \\ \vdots \\ b_m\end{array}\right),$$ where $m > 2$ , so that the system is overdetermined. If $m = 3$ and the system is consistent, (equivalently, $\mathbf{b}$ is in the column space of $A$ ), then the columns of the augmented matrix $\pmatrix{ A \mid {\bf b}}$ are linearly dependent, and so $$\det \pmatrix{ A \mid {\bf b}} = 0.$$ In particular, we have produced a polynomial in the components $(a_{ij}, b_j)$ of the linear system for which vanishing is a necessary condition for system's consistency. I'll call such polynomials polynomial obstructions for the system. If $m > 3$ , then we can produce ${m}\choose{3}$ such polynomials by considering the determinants of the $3 \times 3$ minors of $\pmatrix{ A \mid {\bf b}}$ . Are essentially all polynomials obstructions to the system essentially given by these, or are there others? Put more precisely: By definition the polynomial obstructions comprise an ideal in the polynomial ring $\mathbb{F}[a_{11}, \ldots, a_{m2}, b_1, \ldots b_m]$ ---do the determinants of the $3 \times 3$ minors generate this ideal? If not, how does one produce a complete set of generators? More generally, for an $m \times n$ overdetermined linear system (so that $m > n$ ) $$A \mathbf{x} = \mathbf{b},$$ we can produce polynomial obstructions by taking the determinants of the ${m}\choose{n+1}$ minors (of size $(n + 1) \times (n + 1)$ ). What are the answers to the obvious analogues to the above questions in the $n = 2$ case?","For , consider the (overdetermined) linear system with (general) coefficients in a field ; in components we write the system as where , so that the system is overdetermined. If and the system is consistent, (equivalently, is in the column space of ), then the columns of the augmented matrix are linearly dependent, and so In particular, we have produced a polynomial in the components of the linear system for which vanishing is a necessary condition for system's consistency. I'll call such polynomials polynomial obstructions for the system. If , then we can produce such polynomials by considering the determinants of the minors of . Are essentially all polynomials obstructions to the system essentially given by these, or are there others? Put more precisely: By definition the polynomial obstructions comprise an ideal in the polynomial ring ---do the determinants of the minors generate this ideal? If not, how does one produce a complete set of generators? More generally, for an overdetermined linear system (so that ) we can produce polynomial obstructions by taking the determinants of the minors (of size ). What are the answers to the obvious analogues to the above questions in the case?","m > 2 m \times 2 A \mathbf{x} = \mathbf{b} \mathbb{F} \left(\begin{array}{cc}a_{11} & a_{12} \\ \vdots & \vdots \\ a_{m1} & a_{m2} \end{array}\right)
\left(\begin{array}{c}x_1 \\ x_2\end{array}\right)
=
\left(\begin{array}{c}b_1 \\ \vdots \\ b_m\end{array}\right), m > 2 m = 3 \mathbf{b} A \pmatrix{ A \mid {\bf b}} \det \pmatrix{ A \mid {\bf b}} = 0. (a_{ij}, b_j) m > 3 {m}\choose{3} 3 \times 3 \pmatrix{ A \mid {\bf b}} \mathbb{F}[a_{11}, \ldots, a_{m2}, b_1, \ldots b_m] 3 \times 3 m \times n m > n A \mathbf{x} = \mathbf{b}, {m}\choose{n+1} (n + 1) \times (n + 1) n = 2","['linear-algebra', 'abstract-algebra', 'matrices', 'ideals']"
28,Invariant tensors in adjoint representation,Invariant tensors in adjoint representation,,"Suppose we have a simple Lie group $G$ with algebra $\mathfrak{g}=\{X_a\}$, where the generators $X_a$ are in some matrix representation. Is it true that the only invariant rank $n$ tensor in the adjoint representation is $$ \hspace{80pt} T_{a_1\cdots a_n}={\rm Tr} (X_{a_1}\cdots X_{a_n}) \ ?  \hspace{80pt} (*) $$ Hint: $T$ is clearly invariant, under $g\in G$ it transforms as $$ \begin{split} T_{a_1\cdots a_n}\xrightarrow{\ g\ } & \ \sum_{b_1\ldots b_n}D_{\rm adj}(g)_{a_1}{}^{b_1}\cdots D_{\rm adj}(g)_{a_n}{}^{b_n}T_{b_1\cdots b_n} \\ &={\rm Tr}(g^{-1}X_{a_1}g\cdots g^{-1}X_{a_n}g)\\ &= T_{a_1\cdots a_n}\, .  \end{split} $$ ($D_{\rm adj}(g)$ is the group element $g$ in the the adjoint representation, and in the second line $g$ is in the same matrix representation as the generators.) However I do not know whether all invariant tensors can be written in the form $(*)$. Example: The statement $(*)$ is true for $G=SO(3)$, where it is well known that the only tensors invariant under rotations are $\delta_{ij}$ and $\epsilon_{ijk}$ and tensor products of these two (which allows one to build the rotational-invariant scalar and cross products in $\mathbb{R}^3$). Using a matrix representation of the algebra (e.g. Pauli matrices), it is easy to see that $$ \delta_{ij} \propto {\rm Tr} (\sigma_i\sigma_j) \hspace{20pt}\hbox{and}\hspace{20pt} \epsilon_{ijk}\propto {\rm Tr} (\sigma_i\sigma_j\sigma_k)\, , $$ showing that tensors of the form $(*)$ exhaust all possible invariant tensors.","Suppose we have a simple Lie group $G$ with algebra $\mathfrak{g}=\{X_a\}$, where the generators $X_a$ are in some matrix representation. Is it true that the only invariant rank $n$ tensor in the adjoint representation is $$ \hspace{80pt} T_{a_1\cdots a_n}={\rm Tr} (X_{a_1}\cdots X_{a_n}) \ ?  \hspace{80pt} (*) $$ Hint: $T$ is clearly invariant, under $g\in G$ it transforms as $$ \begin{split} T_{a_1\cdots a_n}\xrightarrow{\ g\ } & \ \sum_{b_1\ldots b_n}D_{\rm adj}(g)_{a_1}{}^{b_1}\cdots D_{\rm adj}(g)_{a_n}{}^{b_n}T_{b_1\cdots b_n} \\ &={\rm Tr}(g^{-1}X_{a_1}g\cdots g^{-1}X_{a_n}g)\\ &= T_{a_1\cdots a_n}\, .  \end{split} $$ ($D_{\rm adj}(g)$ is the group element $g$ in the the adjoint representation, and in the second line $g$ is in the same matrix representation as the generators.) However I do not know whether all invariant tensors can be written in the form $(*)$. Example: The statement $(*)$ is true for $G=SO(3)$, where it is well known that the only tensors invariant under rotations are $\delta_{ij}$ and $\epsilon_{ijk}$ and tensor products of these two (which allows one to build the rotational-invariant scalar and cross products in $\mathbb{R}^3$). Using a matrix representation of the algebra (e.g. Pauli matrices), it is easy to see that $$ \delta_{ij} \propto {\rm Tr} (\sigma_i\sigma_j) \hspace{20pt}\hbox{and}\hspace{20pt} \epsilon_{ijk}\propto {\rm Tr} (\sigma_i\sigma_j\sigma_k)\, , $$ showing that tensors of the form $(*)$ exhaust all possible invariant tensors.",,"['linear-algebra', 'lie-algebras', 'tensors']"
29,Is the finite dimension required in this proof?,Is the finite dimension required in this proof?,,"Let $V$ and $W$ be vector spaces over a field $K$. If a linear map $L:V \rightarrow W$ is surjective then its dual is injective. If $V$ and $W$ are finite dimensional then the converse holds, i.e.  $L^*:W^* \rightarrow V^*$ injective implies $L$ surjective. I have proved both statements but I don't see where I used the finite dimensional requirement for the second. Here is my proof: Assume $L$ is not surjective, say the element $e_i$ of the basis of $W$ is not in the image of $L$.  Take its corresponding dual $\alpha_i \in W^*$, then $L^*(\alpha_i)=\alpha_i \circ L =0$ so the kernel of $L^*$ is not 0 and therefore $L^*$ is not injective.","Let $V$ and $W$ be vector spaces over a field $K$. If a linear map $L:V \rightarrow W$ is surjective then its dual is injective. If $V$ and $W$ are finite dimensional then the converse holds, i.e.  $L^*:W^* \rightarrow V^*$ injective implies $L$ surjective. I have proved both statements but I don't see where I used the finite dimensional requirement for the second. Here is my proof: Assume $L$ is not surjective, say the element $e_i$ of the basis of $W$ is not in the image of $L$.  Take its corresponding dual $\alpha_i \in W^*$, then $L^*(\alpha_i)=\alpha_i \circ L =0$ so the kernel of $L^*$ is not 0 and therefore $L^*$ is not injective.",,['linear-algebra']
30,Matrix factorization,Matrix factorization,,"I'd like to factorize matrices as follows: $$ \left(\begin{array}{cc}X_1&X_2\\X_3&X_4\end{array}\right) = \left(\begin{array}{cc}D_1&D_2\\D_3&D_4\end{array}\right)\left(\begin{array}{cc}P_1&\\&P_2\end{array}\right)\left(\begin{array}{cc}D_5&D_6\\D_7&D_8\end{array}\right) $$ provided such a factorization exists. The left-hand side is known and the right-hand side isn't. The $X_i$ are full $2^t\times 2^t$ ($t\ge 1$), the other blocks have same dimension with the $D_i$ being diagonal and the $P_i$ full. ( edit : the previous notations I used for the diagonal blocks $D_i$ might have been unclear, these blocks are not necessarily scalar multiples of the identity) Assuming a simple case where the diagonal matrices are invertible, the above problem amounts to solving the following nonlinear system of matrix equations (I use the same notations to simplify notations but the matrices are not the same as the previous one. The reason for this is to keep notations simple esp. given that what matters here is the structure of the blocks.) $$ \left\{ \begin{array}{lcl} X_1 &=& P_1 + P_2 \\ X_2 &=& P_1 D_1 + P_2 D_2\\ X_3 &=& D_3 P_1 + D_4 P_2\\ X_4 &=& D_3 P_1 D_1 + D_4 P_2 D_2\end{array} \right. $$ I've been looking for a way to solve this system but so far without much success. One thing I tried is to fix $D_1$ and $D_2$ (say), obtain the corresponding $P_1$ and $P_2$ with the first two equations and then find the best $D_3$ and $D_4$ in the Frobenius sense using the last two equations. Then start the other way around using $D_3$ and $D_4$. However this does not seem to converge (looks like projections on non-convex sets.). Also, given that there is $4n^2$ ($n:=2^t)$ equations with $2n^2+4n$ unknowns, maybe that this system can be further simplified. Any insight is most welcome, thanks! Edit: any result or idea on the potential infeasibility of finding an efficient/elegant way of solving this is also welcome. I've also been looking into the simplest case of this problem where each block is exactly $2\times 2$. In order not to use notations which would make this question even more confusing that it already is, I'll just use bullets to denote potentially non-zero entries since what really matters here is structure. $$ \left(\begin{array}{cc}  \left(\begin{array}{cc}\bullet&\bullet\\\bullet&\bullet\end{array}\right)   & \left(\begin{array}{cc}\bullet&\bullet\\\bullet&\bullet\end{array}\right)\\\left(\begin{array}{cc}\bullet&\bullet\\\bullet&\bullet\end{array}\right)&\left(\begin{array}{cc}\bullet&\bullet\\\bullet&\bullet\end{array}\right)\end{array}\right)  =  \left(\begin{array}{cc}\left(\begin{array}{cc}\bullet&\\&\bullet\end{array}\right)&\left(\begin{array}{cc}\bullet&\\&\bullet\end{array}\right)\\ \left(\begin{array}{cc}\bullet&\\&\bullet\end{array}\right)&\left(\begin{array}{cc}\bullet&\\&\bullet\end{array}\right)\end{array}\right) \left(\begin{array}{cc}  \left(\begin{array}{cc}\bullet&\bullet\\\bullet&\bullet\end{array}\right) &\\&\left(\begin{array}{cc}\bullet&\bullet\\\bullet&\bullet\end{array}\right)\end{array}\right) \left(\begin{array}{cc}\left(\begin{array}{cc}\bullet&\\&\bullet\end{array}\right)&\left(\begin{array}{cc}\bullet&\\&\bullet\end{array}\right)\\ \left(\begin{array}{cc}\bullet&\\&\bullet\end{array}\right)&\left(\begin{array}{cc}\bullet&\\&\bullet\end{array}\right)\end{array}\right) $$ (lots of bullets...) when applying a perfect shuffle on this (aka bit-reversal) on this system, we get an equivalent system with the following form: $$ \left(\begin{array}{cc}  \left(\begin{array}{cc}\bullet&\bullet\\\bullet&\bullet\end{array}\right)   & \left(\begin{array}{cc}\bullet&\bullet\\\bullet&\bullet\end{array}\right)\\\left(\begin{array}{cc}\bullet&\bullet\\\bullet&\bullet\end{array}\right)&\left(\begin{array}{cc}\bullet&\bullet\\\bullet&\bullet\end{array}\right)\end{array}\right)  =  \left(\begin{array}{cc}  \left(\begin{array}{cc}\bullet&\bullet\\\bullet&\bullet\end{array}\right) &\\&\left(\begin{array}{cc}\bullet&\bullet\\\bullet&\bullet\end{array}\right)\end{array}\right) \left(\begin{array}{cc}\left(\begin{array}{cc}\bullet&\\&\bullet\end{array}\right)&\left(\begin{array}{cc}\bullet&\\&\bullet\end{array}\right)\\ \left(\begin{array}{cc}\bullet&\\&\bullet\end{array}\right)&\left(\begin{array}{cc}\bullet&\\&\bullet\end{array}\right)\end{array}\right) \left(\begin{array}{cc}  \left(\begin{array}{cc}\bullet&\bullet\\\bullet&\bullet\end{array}\right) &\\&\left(\begin{array}{cc}\bullet&\bullet\\\bullet&\bullet\end{array}\right)\end{array}\right) $$ which maybe can be considered in the context of simultaneous diagonalization of matrices? If solving this particular system can be done, I'm hoping it could give hints for the more general case. Thanks!","I'd like to factorize matrices as follows: $$ \left(\begin{array}{cc}X_1&X_2\\X_3&X_4\end{array}\right) = \left(\begin{array}{cc}D_1&D_2\\D_3&D_4\end{array}\right)\left(\begin{array}{cc}P_1&\\&P_2\end{array}\right)\left(\begin{array}{cc}D_5&D_6\\D_7&D_8\end{array}\right) $$ provided such a factorization exists. The left-hand side is known and the right-hand side isn't. The $X_i$ are full $2^t\times 2^t$ ($t\ge 1$), the other blocks have same dimension with the $D_i$ being diagonal and the $P_i$ full. ( edit : the previous notations I used for the diagonal blocks $D_i$ might have been unclear, these blocks are not necessarily scalar multiples of the identity) Assuming a simple case where the diagonal matrices are invertible, the above problem amounts to solving the following nonlinear system of matrix equations (I use the same notations to simplify notations but the matrices are not the same as the previous one. The reason for this is to keep notations simple esp. given that what matters here is the structure of the blocks.) $$ \left\{ \begin{array}{lcl} X_1 &=& P_1 + P_2 \\ X_2 &=& P_1 D_1 + P_2 D_2\\ X_3 &=& D_3 P_1 + D_4 P_2\\ X_4 &=& D_3 P_1 D_1 + D_4 P_2 D_2\end{array} \right. $$ I've been looking for a way to solve this system but so far without much success. One thing I tried is to fix $D_1$ and $D_2$ (say), obtain the corresponding $P_1$ and $P_2$ with the first two equations and then find the best $D_3$ and $D_4$ in the Frobenius sense using the last two equations. Then start the other way around using $D_3$ and $D_4$. However this does not seem to converge (looks like projections on non-convex sets.). Also, given that there is $4n^2$ ($n:=2^t)$ equations with $2n^2+4n$ unknowns, maybe that this system can be further simplified. Any insight is most welcome, thanks! Edit: any result or idea on the potential infeasibility of finding an efficient/elegant way of solving this is also welcome. I've also been looking into the simplest case of this problem where each block is exactly $2\times 2$. In order not to use notations which would make this question even more confusing that it already is, I'll just use bullets to denote potentially non-zero entries since what really matters here is structure. $$ \left(\begin{array}{cc}  \left(\begin{array}{cc}\bullet&\bullet\\\bullet&\bullet\end{array}\right)   & \left(\begin{array}{cc}\bullet&\bullet\\\bullet&\bullet\end{array}\right)\\\left(\begin{array}{cc}\bullet&\bullet\\\bullet&\bullet\end{array}\right)&\left(\begin{array}{cc}\bullet&\bullet\\\bullet&\bullet\end{array}\right)\end{array}\right)  =  \left(\begin{array}{cc}\left(\begin{array}{cc}\bullet&\\&\bullet\end{array}\right)&\left(\begin{array}{cc}\bullet&\\&\bullet\end{array}\right)\\ \left(\begin{array}{cc}\bullet&\\&\bullet\end{array}\right)&\left(\begin{array}{cc}\bullet&\\&\bullet\end{array}\right)\end{array}\right) \left(\begin{array}{cc}  \left(\begin{array}{cc}\bullet&\bullet\\\bullet&\bullet\end{array}\right) &\\&\left(\begin{array}{cc}\bullet&\bullet\\\bullet&\bullet\end{array}\right)\end{array}\right) \left(\begin{array}{cc}\left(\begin{array}{cc}\bullet&\\&\bullet\end{array}\right)&\left(\begin{array}{cc}\bullet&\\&\bullet\end{array}\right)\\ \left(\begin{array}{cc}\bullet&\\&\bullet\end{array}\right)&\left(\begin{array}{cc}\bullet&\\&\bullet\end{array}\right)\end{array}\right) $$ (lots of bullets...) when applying a perfect shuffle on this (aka bit-reversal) on this system, we get an equivalent system with the following form: $$ \left(\begin{array}{cc}  \left(\begin{array}{cc}\bullet&\bullet\\\bullet&\bullet\end{array}\right)   & \left(\begin{array}{cc}\bullet&\bullet\\\bullet&\bullet\end{array}\right)\\\left(\begin{array}{cc}\bullet&\bullet\\\bullet&\bullet\end{array}\right)&\left(\begin{array}{cc}\bullet&\bullet\\\bullet&\bullet\end{array}\right)\end{array}\right)  =  \left(\begin{array}{cc}  \left(\begin{array}{cc}\bullet&\bullet\\\bullet&\bullet\end{array}\right) &\\&\left(\begin{array}{cc}\bullet&\bullet\\\bullet&\bullet\end{array}\right)\end{array}\right) \left(\begin{array}{cc}\left(\begin{array}{cc}\bullet&\\&\bullet\end{array}\right)&\left(\begin{array}{cc}\bullet&\\&\bullet\end{array}\right)\\ \left(\begin{array}{cc}\bullet&\\&\bullet\end{array}\right)&\left(\begin{array}{cc}\bullet&\\&\bullet\end{array}\right)\end{array}\right) \left(\begin{array}{cc}  \left(\begin{array}{cc}\bullet&\bullet\\\bullet&\bullet\end{array}\right) &\\&\left(\begin{array}{cc}\bullet&\bullet\\\bullet&\bullet\end{array}\right)\end{array}\right) $$ which maybe can be considered in the context of simultaneous diagonalization of matrices? If solving this particular system can be done, I'm hoping it could give hints for the more general case. Thanks!",,"['linear-algebra', 'matrices', 'matrix-decomposition']"
31,Augmented Reality Transformation Matrix Optimization,Augmented Reality Transformation Matrix Optimization,,"i am a software developer, i'm working on an Augmented Reality system. I'd like to receive some advice in order to optimize my math model. My program has to be slim and fast. Here's the situation : This is a photo of my marker: I want to find the transformation matrix that ""normalize"" the photo in order to get this: I have 4 angles coordinates of the marker: $$P{1} = (352; 90)$$ $$P{2} = (93; 384)$$ $$P{3} = (852; 283)$$ $$P{4} = (663; 677)$$ Equation to find a generic projective transformation of a point P(x,y): $$\left[\begin{array}{c}  x' \\  y' \\  z' \\  \end{array}\right]= \left[\begin{array}{ccc}  h_{1,1} & h_{1,2} & h_{1,3} \\  h_{2,1} & h_{2,2} & h_{2,3} \\  h_{3,1} & h_{3,2} & h_{3,3}  \end{array}\right]* \left[\begin{array}{c}  x \\  y \\  1 \\  \end{array}\right]$$ In algebric form (we don't need z' and it is divided in x' and y'): $$x' = \frac{h_{11}x + h_{12}y + h_{13}}{h_{31}x + h_{32}y + h_{33}}$$ $$y' = \frac{h_{21}x + h_{22}y + h_{23}}{h_{31}x + h_{32}y + h_{33}}$$ Supposing that H(3,3) is 1, I can find H by solving this 8 equation by 8 variables system: $$\left[\begin{array}{cccccccc}  x_{1} & y_{1} & 1 & 0 & 0 & 0 & -x_{1}*x'_{1} & -y_{1}*x'_{1} \\  x_{2} & y_{2} & 1 & 0 & 0 & 0 & -x_{2}*x'_{2} & -y_{2}*x'_{2} \\  x_{3} & y_{3} & 1 & 0 & 0 & 0 & -x_{3}*x'_{3} & -y_{3}*x'_{3} \\  x_{4} & y_{4} & 1 & 0 & 0 & 0 & -x_{4}*x'_{4} & -y_{4}*x'_{4} \\  0 & 0 & 0 & x_{1} & y_{1} & 1 & -x_{1}*y'_{1} & -y_{1}*y'_{1} \\  0 & 0 & 0 & x_{2} & y_{2} & 1 & -x_{2}*y'_{2} & -y_{2}*y'_{2} \\  0 & 0 & 0 & x_{3} & y_{3} & 1 & -x_{3}*y'_{3} & -y_{3}*y'_{3} \\  0 & 0 & 0 & x_{4} & y_{4} & 1 & -x_{4}*y'_{4} & -y_{4}*y'_{4}  \end{array}\right]* \left[\begin{array}{c}  h_{1,1} \\  h_{1,2} \\  h_{1,3} \\  h_{2,1} \\  h_{2,2} \\  h_{2,3} \\  h_{3,1} \\  h_{3,2}  \end{array}\right]= \left[\begin{array}{c}  x'_{1} \\  x'_{2} \\  x'_{3} \\  x'_{4} \\  y'_{1} \\  y'_{2} \\  y'_{3} \\  y'_{4}  \end{array}\right]$$ Considering that the final marker will have mx as width and my as height, the transformations of my 4 initial points will be: $$P'{1} = (0; 0)$$ $$P'{2} = (0; my)$$ $$P'{3} = (mx; 0)$$ $$P'{4} = (mx; my)$$ So the system, in my case, become: $$\left[\begin{array}{cccccccc}  x_{1} & y_{1} & 1 & 0 & 0 & 0 & 0 & 0 \\  x_{2} & y_{2} & 1 & 0 & 0 & 0 & 0 & 0 \\  x_{3} & y_{3} & 1 & 0 & 0 & 0 & -x_{3}*mx & -y_{3}*mx \\  x_{4} & y_{4} & 1 & 0 & 0 & 0 & -x_{4}*mx & -y_{4}*mx \\  0 & 0 & 0 & x_{1} & y_{1} & 1 & 0 & 0 \\  0 & 0 & 0 & x_{2} & y_{2} & 1 & -x_{2}*my & -y_{2}*my \\  0 & 0 & 0 & x_{3} & y_{3} & 1 & 0 & 0 \\  0 & 0 & 0 & x_{4} & y_{4} & 1 & -x_{4}*my & -y_{4}*my  \end{array}\right]* \left[\begin{array}{c}  h_{1,1} \\  h_{1,2} \\  h_{1,3} \\  h_{2,1} \\  h_{2,2} \\  h_{2,3} \\  h_{3,1} \\  h_{3,2}  \end{array}\right]= \left[\begin{array}{c}  0 \\  0 \\  mx \\  mx \\  0 \\  my \\  0 \\  my  \end{array}\right]$$ This model works: i tried in Matlab and Java. My question is: could it be simplified or optimized? This system has a lot of zeros, and that means that it has little ""information""... Should I change something? (I could consider mx and my equals to 1, in order to semplify the system even more. but i'd like to reduce the number of equations if it's possible)","i am a software developer, i'm working on an Augmented Reality system. I'd like to receive some advice in order to optimize my math model. My program has to be slim and fast. Here's the situation : This is a photo of my marker: I want to find the transformation matrix that ""normalize"" the photo in order to get this: I have 4 angles coordinates of the marker: $$P{1} = (352; 90)$$ $$P{2} = (93; 384)$$ $$P{3} = (852; 283)$$ $$P{4} = (663; 677)$$ Equation to find a generic projective transformation of a point P(x,y): $$\left[\begin{array}{c}  x' \\  y' \\  z' \\  \end{array}\right]= \left[\begin{array}{ccc}  h_{1,1} & h_{1,2} & h_{1,3} \\  h_{2,1} & h_{2,2} & h_{2,3} \\  h_{3,1} & h_{3,2} & h_{3,3}  \end{array}\right]* \left[\begin{array}{c}  x \\  y \\  1 \\  \end{array}\right]$$ In algebric form (we don't need z' and it is divided in x' and y'): $$x' = \frac{h_{11}x + h_{12}y + h_{13}}{h_{31}x + h_{32}y + h_{33}}$$ $$y' = \frac{h_{21}x + h_{22}y + h_{23}}{h_{31}x + h_{32}y + h_{33}}$$ Supposing that H(3,3) is 1, I can find H by solving this 8 equation by 8 variables system: $$\left[\begin{array}{cccccccc}  x_{1} & y_{1} & 1 & 0 & 0 & 0 & -x_{1}*x'_{1} & -y_{1}*x'_{1} \\  x_{2} & y_{2} & 1 & 0 & 0 & 0 & -x_{2}*x'_{2} & -y_{2}*x'_{2} \\  x_{3} & y_{3} & 1 & 0 & 0 & 0 & -x_{3}*x'_{3} & -y_{3}*x'_{3} \\  x_{4} & y_{4} & 1 & 0 & 0 & 0 & -x_{4}*x'_{4} & -y_{4}*x'_{4} \\  0 & 0 & 0 & x_{1} & y_{1} & 1 & -x_{1}*y'_{1} & -y_{1}*y'_{1} \\  0 & 0 & 0 & x_{2} & y_{2} & 1 & -x_{2}*y'_{2} & -y_{2}*y'_{2} \\  0 & 0 & 0 & x_{3} & y_{3} & 1 & -x_{3}*y'_{3} & -y_{3}*y'_{3} \\  0 & 0 & 0 & x_{4} & y_{4} & 1 & -x_{4}*y'_{4} & -y_{4}*y'_{4}  \end{array}\right]* \left[\begin{array}{c}  h_{1,1} \\  h_{1,2} \\  h_{1,3} \\  h_{2,1} \\  h_{2,2} \\  h_{2,3} \\  h_{3,1} \\  h_{3,2}  \end{array}\right]= \left[\begin{array}{c}  x'_{1} \\  x'_{2} \\  x'_{3} \\  x'_{4} \\  y'_{1} \\  y'_{2} \\  y'_{3} \\  y'_{4}  \end{array}\right]$$ Considering that the final marker will have mx as width and my as height, the transformations of my 4 initial points will be: $$P'{1} = (0; 0)$$ $$P'{2} = (0; my)$$ $$P'{3} = (mx; 0)$$ $$P'{4} = (mx; my)$$ So the system, in my case, become: $$\left[\begin{array}{cccccccc}  x_{1} & y_{1} & 1 & 0 & 0 & 0 & 0 & 0 \\  x_{2} & y_{2} & 1 & 0 & 0 & 0 & 0 & 0 \\  x_{3} & y_{3} & 1 & 0 & 0 & 0 & -x_{3}*mx & -y_{3}*mx \\  x_{4} & y_{4} & 1 & 0 & 0 & 0 & -x_{4}*mx & -y_{4}*mx \\  0 & 0 & 0 & x_{1} & y_{1} & 1 & 0 & 0 \\  0 & 0 & 0 & x_{2} & y_{2} & 1 & -x_{2}*my & -y_{2}*my \\  0 & 0 & 0 & x_{3} & y_{3} & 1 & 0 & 0 \\  0 & 0 & 0 & x_{4} & y_{4} & 1 & -x_{4}*my & -y_{4}*my  \end{array}\right]* \left[\begin{array}{c}  h_{1,1} \\  h_{1,2} \\  h_{1,3} \\  h_{2,1} \\  h_{2,2} \\  h_{2,3} \\  h_{3,1} \\  h_{3,2}  \end{array}\right]= \left[\begin{array}{c}  0 \\  0 \\  mx \\  mx \\  0 \\  my \\  0 \\  my  \end{array}\right]$$ This model works: i tried in Matlab and Java. My question is: could it be simplified or optimized? This system has a lot of zeros, and that means that it has little ""information""... Should I change something? (I could consider mx and my equals to 1, in order to semplify the system even more. but i'd like to reduce the number of equations if it's possible)",,"['linear-algebra', 'matrices', 'optimization', 'numerical-linear-algebra', 'projective-geometry']"
32,Matrix involving reciprocal factorials,Matrix involving reciprocal factorials,,"Let $m$ and $n$ be two integers and $m \le n$ . There are a matrix $A$ of $m$ -by- $m$ with $A(i,j) = 1/(2n+2j-2i)!$ and a vector $r$ of $m$ entries with $r(i) = 2/(2n+2i)!$ . Is there a formula for the inner product of $r$ and the first column of the inverse of $A$ ? Actually, I do not even know whether $A$ is invertible (a proof of this is helpful). Or, can it be shown that the inner product aforementioned is bounded from above by a constant strictly less than one? The background of this question is as follows. There is a linear system $$    \begin{pmatrix}1 & \frac{2}{(2n+2)!} & \frac{2}{(2n+4)!} & \ldots & \frac{2}{(2n+2m)!} \\1 & \frac{1}{(2n)!} & \frac{1}{(2n+2)!} & \ldots & \frac{1}{(2n+2m-2)!}\\ 0 & \frac{1}{(2n-2)!} & \frac{1}{(2n)!} & \ldots & \frac{1}{(2n+2m-4)!}\\ 0 & \frac{1}{(2n-4)!} & \frac{1}{(2n-2)!} & \ldots & \frac{1}{(2n+2m-6)!}\\ \vdots & \vdots & \vdots & \vdots & \vdots \\ 0 & \frac{1}{(2n+2-2m)!} & \frac{1}{(2n+4-2m)!} & \ldots & \frac{1}{(2n)!}\end{pmatrix} \begin{pmatrix}\vphantom{\frac{1}{(2n+2)!}}v_0\\ \vphantom{\frac{1}{(2n+2)!}}v_1\\ \vphantom{\frac{1}{(2n+2)!}}v_2\\ \vphantom{\frac{1}{(2n+2)!}}v_3\\ \vdots\\ \vphantom{\frac{1}{(2n+2)!}}v_m\end{pmatrix} =  \begin{pmatrix}\vphantom{\frac{1}{(2n+2)!}}O(\epsilon)\\ \vphantom{\frac{1}{(2n+2)!}}O(\epsilon)\\ \vphantom{\frac{1}{(2n+2)!}}O(\epsilon)\\ \vphantom{\frac{1}{(2n+2)!}} O(\epsilon) \\ \vdots \\ \vphantom{\frac{1}{(2n+2)!}} O(\epsilon)\end{pmatrix}, $$ where $\epsilon>0$ , and $O(\epsilon)$ is the big $O$ notation for any quantity $Q$ for which there exists a constant $C>0$ such that $|Q|< C \epsilon$ . So $O(\epsilon)$ can be different values at different places. It can be found the previous matrix $A$ is just the the bottom right part of the above coefficient matrix, from the 2nd column to the last and from the 2nd row to the last. The previous vector $r$ is just the top right part of the above coefficient matrix, from the 2nd column to the last in the 1st row. It is wanted to have an estimate of $v_0$ . Can it be shown that $v_0=O(\epsilon)$ independent of $m$ and $n$ ? A less but still wanted goal is just to show the above matrix is invertible.","Let and be two integers and . There are a matrix of -by- with and a vector of entries with . Is there a formula for the inner product of and the first column of the inverse of ? Actually, I do not even know whether is invertible (a proof of this is helpful). Or, can it be shown that the inner product aforementioned is bounded from above by a constant strictly less than one? The background of this question is as follows. There is a linear system where , and is the big notation for any quantity for which there exists a constant such that . So can be different values at different places. It can be found the previous matrix is just the the bottom right part of the above coefficient matrix, from the 2nd column to the last and from the 2nd row to the last. The previous vector is just the top right part of the above coefficient matrix, from the 2nd column to the last in the 1st row. It is wanted to have an estimate of . Can it be shown that independent of and ? A less but still wanted goal is just to show the above matrix is invertible.","m n m \le n A m m A(i,j) = 1/(2n+2j-2i)! r m r(i) = 2/(2n+2i)! r A A 
   \begin{pmatrix}1 & \frac{2}{(2n+2)!} & \frac{2}{(2n+4)!} & \ldots & \frac{2}{(2n+2m)!} \\1 & \frac{1}{(2n)!} & \frac{1}{(2n+2)!} & \ldots & \frac{1}{(2n+2m-2)!}\\ 0 & \frac{1}{(2n-2)!} & \frac{1}{(2n)!} & \ldots & \frac{1}{(2n+2m-4)!}\\ 0 & \frac{1}{(2n-4)!} & \frac{1}{(2n-2)!} & \ldots & \frac{1}{(2n+2m-6)!}\\ \vdots & \vdots & \vdots & \vdots & \vdots \\ 0 & \frac{1}{(2n+2-2m)!} & \frac{1}{(2n+4-2m)!} & \ldots & \frac{1}{(2n)!}\end{pmatrix}
\begin{pmatrix}\vphantom{\frac{1}{(2n+2)!}}v_0\\ \vphantom{\frac{1}{(2n+2)!}}v_1\\ \vphantom{\frac{1}{(2n+2)!}}v_2\\ \vphantom{\frac{1}{(2n+2)!}}v_3\\ \vdots\\ \vphantom{\frac{1}{(2n+2)!}}v_m\end{pmatrix} = 
\begin{pmatrix}\vphantom{\frac{1}{(2n+2)!}}O(\epsilon)\\ \vphantom{\frac{1}{(2n+2)!}}O(\epsilon)\\ \vphantom{\frac{1}{(2n+2)!}}O(\epsilon)\\ \vphantom{\frac{1}{(2n+2)!}} O(\epsilon) \\ \vdots \\ \vphantom{\frac{1}{(2n+2)!}} O(\epsilon)\end{pmatrix},
 \epsilon>0 O(\epsilon) O Q C>0 |Q|< C \epsilon O(\epsilon) A r v_0 v_0=O(\epsilon) m n","['linear-algebra', 'matrices', 'factorial', 'toeplitz-matrices', 'schur-complement']"
33,Finding a minimal set of equations that determine a variable.,Finding a minimal set of equations that determine a variable.,,"I have a system of $m$ linear equations on $n$ variables, which I'm representing as $Ax=b$ , with $A$ an $m\times n$ matrix representing the equations and $b$ an $\mathbb R^m$ vector representing the constants of the equations. I'm given that there exists a solution $s\in\mathbb R^n$ (i.e., $As=b$ ) and that $m<<n$ . My goal is the following: Find which variables are determined (i.e., for which there are no solutions with values different than the one in $s$ ). For each variable in 1., find a minimal subsystem of equations which determines that variable. By minimal, I mean the following: the subsystem determines the variable as in 1., and no proper subset of the subsystem does this. I want to find any such minimal subsystem: which one I pick is irrelevant to me. Progress : I've solved 1. by computing the nullspace of $A$ , and looking for those variables for which none of the basis vectors of the null-space have a non-zero coefficient. I've attempted to solve 2. by computing the pseudo-inverse of $A$ and looking at which coefficients in the row corresponding to the chosen variable are non-zero. This approach doesn't work, though, since the pseudo-inverse $P$ minimizes the Frobenius norm among all matrices $X$ such that $X\cdot b=s$ . This norm is an $L_2$ norm, but what we really want is an $L_0$ norm, since we want to minimize the number of non-zero elements in the matrix. Of course, finding the matrix minimizing the $L_0$ norm is an NP problem, so that's not a valid approach for me. Are there any other approaches I'm missing? Perhaps some heuristic methods? I'm struggling to find anything on this subject.","I have a system of linear equations on variables, which I'm representing as , with an matrix representing the equations and an vector representing the constants of the equations. I'm given that there exists a solution (i.e., ) and that . My goal is the following: Find which variables are determined (i.e., for which there are no solutions with values different than the one in ). For each variable in 1., find a minimal subsystem of equations which determines that variable. By minimal, I mean the following: the subsystem determines the variable as in 1., and no proper subset of the subsystem does this. I want to find any such minimal subsystem: which one I pick is irrelevant to me. Progress : I've solved 1. by computing the nullspace of , and looking for those variables for which none of the basis vectors of the null-space have a non-zero coefficient. I've attempted to solve 2. by computing the pseudo-inverse of and looking at which coefficients in the row corresponding to the chosen variable are non-zero. This approach doesn't work, though, since the pseudo-inverse minimizes the Frobenius norm among all matrices such that . This norm is an norm, but what we really want is an norm, since we want to minimize the number of non-zero elements in the matrix. Of course, finding the matrix minimizing the norm is an NP problem, so that's not a valid approach for me. Are there any other approaches I'm missing? Perhaps some heuristic methods? I'm struggling to find anything on this subject.",m n Ax=b A m\times n b \mathbb R^m s\in\mathbb R^n As=b m<<n s A A P X X\cdot b=s L_2 L_0 L_0,"['linear-algebra', 'systems-of-equations', 'pseudoinverse']"
34,How to calculate 3d rotation from 3 points?,How to calculate 3d rotation from 3 points?,,"If you have 3 labeled points on a surface of a paper. Like 1   2       3 This makes a perfect equilateral triangle. From this perspective I can say that the camera is on top of the paper looking down. We can say the camera is at coordinate $(0,0,100)$. Which is 0 degree rotation in Z axis and 90 degree rotation in Y axis. Then, I move the camera to some arbitrary spot. Now, the points are like 1  2                 3 This looks like the camera is farther back from the paper and was lowered, say at location $(0, -100, 50)$. Which is about -90 degree rotation in Z-axis, and 45 degree rotation in Y axis. So my question is basically, given the $(x_1,y_1), (x_2,y_2), (x_3,y_3)$. Is there some formula that can take these arbitrary points, compare it with the original 3, to know how much of a X,Y,Z rotation it is of the camera? I can also rotate on angles like this. For example, I can take the second example from above, then I can rotate my head clockwise, making the numbers flip like 2     1 3 I think getting a normal vector from the center might be better to find.","If you have 3 labeled points on a surface of a paper. Like 1   2       3 This makes a perfect equilateral triangle. From this perspective I can say that the camera is on top of the paper looking down. We can say the camera is at coordinate $(0,0,100)$. Which is 0 degree rotation in Z axis and 90 degree rotation in Y axis. Then, I move the camera to some arbitrary spot. Now, the points are like 1  2                 3 This looks like the camera is farther back from the paper and was lowered, say at location $(0, -100, 50)$. Which is about -90 degree rotation in Z-axis, and 45 degree rotation in Y axis. So my question is basically, given the $(x_1,y_1), (x_2,y_2), (x_3,y_3)$. Is there some formula that can take these arbitrary points, compare it with the original 3, to know how much of a X,Y,Z rotation it is of the camera? I can also rotate on angles like this. For example, I can take the second example from above, then I can rotate my head clockwise, making the numbers flip like 2     1 3 I think getting a normal vector from the center might be better to find.",,"['linear-algebra', 'geometry', 'rotations']"
35,What does it mean for a polar coordinate system to have basis vectors?,What does it mean for a polar coordinate system to have basis vectors?,,"So I understand that every element of a vector space can be represented uniquely by a linear combination of the basis vectors: $v=\alpha_1v_1+\cdots+\alpha_nv_n$ Then coordinates to those basis vectors are the coefficients of that linear combination. A vector v in R^2 has the coordinates (x,y) in Cartesian means that: $v=x\hat{i} + y\hat{j}$ That same vector can also be described uniquely in polar coordinate by $(x,\Theta )$. The basis vectors are $\hat{r}$ and $\hat{\theta}$ But a vector in polar coordinate written in basis vectors is: $v=x\hat{r}(\theta)$. So the basis vectors are essentially not the same but depend on some $\theta$, and the coordinate $(r,\theta)$ are not the coefficients of those basis. Can anyone explain to me how we can even have non-constant basis vectors and what is the role of $\hat{\theta}$ in polar coordinate?","So I understand that every element of a vector space can be represented uniquely by a linear combination of the basis vectors: $v=\alpha_1v_1+\cdots+\alpha_nv_n$ Then coordinates to those basis vectors are the coefficients of that linear combination. A vector v in R^2 has the coordinates (x,y) in Cartesian means that: $v=x\hat{i} + y\hat{j}$ That same vector can also be described uniquely in polar coordinate by $(x,\Theta )$. The basis vectors are $\hat{r}$ and $\hat{\theta}$ But a vector in polar coordinate written in basis vectors is: $v=x\hat{r}(\theta)$. So the basis vectors are essentially not the same but depend on some $\theta$, and the coordinate $(r,\theta)$ are not the coefficients of those basis. Can anyone explain to me how we can even have non-constant basis vectors and what is the role of $\hat{\theta}$ in polar coordinate?",,"['linear-algebra', 'vectors', 'coordinate-systems', 'polar-coordinates']"
36,Is an infinite system of (linear) equations solvable if all finite subsystems are?,Is an infinite system of (linear) equations solvable if all finite subsystems are?,,"I was wondering about the following. Let $A$ be an abelian group, $a_i$ variables indexed with some arbitrary set $I$ and assume we have an infinite set $E$ of linear equations in finitely many variables of the form $$n_1a_1 + \ldots + n_ka_k = b$$ with $n_i \in \mathbb Z, b \in A$. If this system has no solution, then is there already a finite subset of equations in $E$ which are inconsistent? So if any finite subset of equations is solvable, is $E$ solvable? This is reminiscent of the compactness theorem in 1st order logic, but of course you can't apply this for a concrete group. Am I missing something? I assume that this has an easy proof or a counterexample. What happens if we presume particularly nice groups (divisible, free) or vector spaces? Then this question is just about linear systems of equations. Thanks for giving some insight","I was wondering about the following. Let $A$ be an abelian group, $a_i$ variables indexed with some arbitrary set $I$ and assume we have an infinite set $E$ of linear equations in finitely many variables of the form $$n_1a_1 + \ldots + n_ka_k = b$$ with $n_i \in \mathbb Z, b \in A$. If this system has no solution, then is there already a finite subset of equations in $E$ which are inconsistent? So if any finite subset of equations is solvable, is $E$ solvable? This is reminiscent of the compactness theorem in 1st order logic, but of course you can't apply this for a concrete group. Am I missing something? I assume that this has an easy proof or a counterexample. What happens if we presume particularly nice groups (divisible, free) or vector spaces? Then this question is just about linear systems of equations. Thanks for giving some insight",,"['linear-algebra', 'group-theory', 'logic']"
37,Does the inverse of a polynomial matrix have polynomial growth?,Does the inverse of a polynomial matrix have polynomial growth?,,"Let $M : \mathbb{R}^n \to \mathbb{R}^{n \times n}$ be a matrix-valued function whose entries $m_{ij}(x_1, \dots, x_n)$ are all multivariate polynomials with real coefficients.  Suppose that $M(\mathbf{x})$ is invertible for every $\mathbf{x} \in \mathbb{R}^n$.  I would like to show that $M^{-1}$ has at most polynomial growth. That is, let $\|\cdot\|$ be your favorite matrix norm on $\mathbb{R}^{n \times n}$.  I want to show there are constants $C,N$ such that $\|M^{-1}(\mathbf{x})\| \le C (1 + |\mathbf{x}|)^N$. One possible approach starts as follows.  Let $p(\lambda, \mathbf{x}) = \det(M(\mathbf{x}) - I \lambda)$ be the characteristic polynomial of $M$.  Then, if we consider $M$ as a matrix over the commutative ring $\mathbb{R}[\mathbf{x}] = \mathbb{R}[x_1, \dots, x_n]$, we can apply the Cayley-Hamilton theorem to conclude that $p(M(\mathbf{x}), \mathbf{x}) = 0$.  Since $M(\mathbf{x})$ is invertible for every $\mathbf{x}$, we have that $p(0, \mathbf{x}) = \det(M(\mathbf{x}))$ is a polynomial with no zeros; let us suppose $p(0, \mathbf{x}) > 0$.  Then by writing $p(\lambda, \mathbf{x}) = \lambda r(\lambda, \mathbf{x}) + p(0, \mathbf{x})$ for some other polynomial $r$, we get that $$M(\mathbf{x})^{-1} = -\frac{r(M(\mathbf{x}), \mathbf{x})}{p(0,\mathbf{x})}.$$  The numerator certainly has polynomial growth, so it would remain to estimate $1/p(0,\mathbf{x}) = 1/\det(M(\mathbf{x}))$.  I am not sure of an easy way to do that; I considered applying Hilbert's 17th problem (as solved by Artin) to write $p(0,\mathbf{x})$ as a sum of squares of rational functions, but that seems much too complicated. I feel like there must be something much simpler that I am missing.","Let $M : \mathbb{R}^n \to \mathbb{R}^{n \times n}$ be a matrix-valued function whose entries $m_{ij}(x_1, \dots, x_n)$ are all multivariate polynomials with real coefficients.  Suppose that $M(\mathbf{x})$ is invertible for every $\mathbf{x} \in \mathbb{R}^n$.  I would like to show that $M^{-1}$ has at most polynomial growth. That is, let $\|\cdot\|$ be your favorite matrix norm on $\mathbb{R}^{n \times n}$.  I want to show there are constants $C,N$ such that $\|M^{-1}(\mathbf{x})\| \le C (1 + |\mathbf{x}|)^N$. One possible approach starts as follows.  Let $p(\lambda, \mathbf{x}) = \det(M(\mathbf{x}) - I \lambda)$ be the characteristic polynomial of $M$.  Then, if we consider $M$ as a matrix over the commutative ring $\mathbb{R}[\mathbf{x}] = \mathbb{R}[x_1, \dots, x_n]$, we can apply the Cayley-Hamilton theorem to conclude that $p(M(\mathbf{x}), \mathbf{x}) = 0$.  Since $M(\mathbf{x})$ is invertible for every $\mathbf{x}$, we have that $p(0, \mathbf{x}) = \det(M(\mathbf{x}))$ is a polynomial with no zeros; let us suppose $p(0, \mathbf{x}) > 0$.  Then by writing $p(\lambda, \mathbf{x}) = \lambda r(\lambda, \mathbf{x}) + p(0, \mathbf{x})$ for some other polynomial $r$, we get that $$M(\mathbf{x})^{-1} = -\frac{r(M(\mathbf{x}), \mathbf{x})}{p(0,\mathbf{x})}.$$  The numerator certainly has polynomial growth, so it would remain to estimate $1/p(0,\mathbf{x}) = 1/\det(M(\mathbf{x}))$.  I am not sure of an easy way to do that; I considered applying Hilbert's 17th problem (as solved by Artin) to write $p(0,\mathbf{x})$ as a sum of squares of rational functions, but that seems much too complicated. I feel like there must be something much simpler that I am missing.",,"['linear-algebra', 'matrices', 'polynomials']"
38,On the maximum number of polynomials in a certain subspace,On the maximum number of polynomials in a certain subspace,,"I've already asked this question on mathoverflow, but no one answered. So I put this problem also here. Sorry for that. Let $\mathbb F_q$ be a finite field  and let $e, k$ be positive integers with $k \leq e < q-1$.  Define $T_e$ to be the set  $$T_e:=\left\{p(x) \in \mathbb F_q[x] \;|\; \deg p =e \;, lc(p)=1, \; p(x)|x^{q-1}-1 \right\}. $$ I would like to find the value $$ M(e,k,q):=\max\left\{ | W \cap T_e| \;\; | \; W \mbox{ is a subspace of } \mathbb F_q[x]_{\leq e}, \; \dim W=k+1\right\}. $$ I could determine exactly the value $ M(e,k,q) $ in three simple cases. When $e=k$ then we have  $$M(e,e,q)= \binom{q-1}{e}.$$ In fact, since $e=k$, we can take $W=\mathbb F_q[x]_{\leq e}$ and so we obtain $W\cap T_e=T_e$. When $k=1$, we have $$M(e,1,q)=q-e.$$ If $e=q-2$ then  $$M(q-2, k, q) =k+1.$$ For the general case I conjecture that the maximum $M(e,k,q)$ is obtained by taking a polynomial $p(x)$ of degree $e-k$ that divides $x^{q-1}-1$ and choosing the subspace $$ W= \langle p(x), xp(x), \ldots, x^kp(x) \rangle.$$ From this I would obtain the following conjecture. Conjecture: Let $q$ be a power of a prime, and let $e,k$ positive integers such that $ 0<k\leq e < q-1$. Then $$ M(e,k,q) = \dbinom{q-1-e+k}{k}.$$ Could anyone help me to prove or disprove it? Thanks in advance.","I've already asked this question on mathoverflow, but no one answered. So I put this problem also here. Sorry for that. Let $\mathbb F_q$ be a finite field  and let $e, k$ be positive integers with $k \leq e < q-1$.  Define $T_e$ to be the set  $$T_e:=\left\{p(x) \in \mathbb F_q[x] \;|\; \deg p =e \;, lc(p)=1, \; p(x)|x^{q-1}-1 \right\}. $$ I would like to find the value $$ M(e,k,q):=\max\left\{ | W \cap T_e| \;\; | \; W \mbox{ is a subspace of } \mathbb F_q[x]_{\leq e}, \; \dim W=k+1\right\}. $$ I could determine exactly the value $ M(e,k,q) $ in three simple cases. When $e=k$ then we have  $$M(e,e,q)= \binom{q-1}{e}.$$ In fact, since $e=k$, we can take $W=\mathbb F_q[x]_{\leq e}$ and so we obtain $W\cap T_e=T_e$. When $k=1$, we have $$M(e,1,q)=q-e.$$ If $e=q-2$ then  $$M(q-2, k, q) =k+1.$$ For the general case I conjecture that the maximum $M(e,k,q)$ is obtained by taking a polynomial $p(x)$ of degree $e-k$ that divides $x^{q-1}-1$ and choosing the subspace $$ W= \langle p(x), xp(x), \ldots, x^kp(x) \rangle.$$ From this I would obtain the following conjecture. Conjecture: Let $q$ be a power of a prime, and let $e,k$ positive integers such that $ 0<k\leq e < q-1$. Then $$ M(e,k,q) = \dbinom{q-1-e+k}{k}.$$ Could anyone help me to prove or disprove it? Thanks in advance.",,"['linear-algebra', 'combinatorics', 'polynomials', 'finite-fields']"
39,Counting the number of elements in a double coset,Counting the number of elements in a double coset,,"Let $G$ denote the groups of $n\times n$ invertible matrices and $H$ be the subgroup of invertible upper triangular matrices. For $n=2$, by row reduction, or equivalently LU decomposition, it is straightforward to see that  $$H\backslash G/H=\{H, HpH\}$$ where $p$ is the $2\times 2$ permutation matrix different from $I$. A similar argument shows that for $n>2$ the double coset $H\backslash G/H$ is a finite set, but I am curious to know if there is an algebraic/combinatorial way to count the number of its elements. It is also quite probable that this has already been worked out because of Bruhat decomposition. Any thoughts are appreciated!","Let $G$ denote the groups of $n\times n$ invertible matrices and $H$ be the subgroup of invertible upper triangular matrices. For $n=2$, by row reduction, or equivalently LU decomposition, it is straightforward to see that  $$H\backslash G/H=\{H, HpH\}$$ where $p$ is the $2\times 2$ permutation matrix different from $I$. A similar argument shows that for $n>2$ the double coset $H\backslash G/H$ is a finite set, but I am curious to know if there is an algebraic/combinatorial way to count the number of its elements. It is also quite probable that this has already been worked out because of Bruhat decomposition. Any thoughts are appreciated!",,"['linear-algebra', 'combinatorics', 'group-theory', 'representation-theory']"
40,"$\mathcal{O}(n,\mathbb R)$ spans $\mathcal{M}(n,\mathbb R)$",spans,"\mathcal{O}(n,\mathbb R) \mathcal{M}(n,\mathbb R)","Let $n\geq 3$. One can show that the orthogonal group of degree $n$ over the real field, $\mathcal{O}(n,\mathbb R)$, spans the entire vector space of real $n\times n$ matrices, $\mathcal{M}(n,\mathbb  R)$. More precisely if $k(n)$ denotes the smallest integer such that each $M\in \mathcal{M}(n,\mathbb  R)$ can be written as $$M=\sum_{i=1}^{k(n)} \lambda_i O_i, \quad \text{with}\quad (\lambda_i, O_i)\in (\mathbb R, \mathcal{O}(n,\mathbb R)).$$ After showing that $\forall n>2, k(n)\leq 4$, can one find the integers such that $k(n)=4$?","Let $n\geq 3$. One can show that the orthogonal group of degree $n$ over the real field, $\mathcal{O}(n,\mathbb R)$, spans the entire vector space of real $n\times n$ matrices, $\mathcal{M}(n,\mathbb  R)$. More precisely if $k(n)$ denotes the smallest integer such that each $M\in \mathcal{M}(n,\mathbb  R)$ can be written as $$M=\sum_{i=1}^{k(n)} \lambda_i O_i, \quad \text{with}\quad (\lambda_i, O_i)\in (\mathbb R, \mathcal{O}(n,\mathbb R)).$$ After showing that $\forall n>2, k(n)\leq 4$, can one find the integers such that $k(n)=4$?",,"['linear-algebra', 'matrices']"
41,Matrix powers of product of diagonalizable and orthogonal matrix,Matrix powers of product of diagonalizable and orthogonal matrix,,"Suppose I have the following matrix constructed from some orthogonal matrix $O$ and a $\pm 1$ diagonal matrix $D=diag(\pm1,\dots,\pm1)$ $$ A = O D O^{-1} D. $$ Is there a simple way to evaluate $A^n$ for positive integer $n$ in a similar way to e.g. a diagonalizable matrix $B$ ? $$ B^n = (P L P^{-1})^n = P L^n P^{-1} . $$ I feel this ought to be the case given how $A$ is decomposed into a product of orthogonal and diagonal matrices but alas it is not clear to me if this is always so. Of course $A$ is also an orthogonal matrix and can itself be diagonalized $$ A = O D O^{-1} D = RTR^{-1} $$ but do $R,T$ have clear expressions in terms of $O,D$ ? EDIT: Some further work on my end indicates that the same question when $$ A = D^{1/2} O D O^{-1} D^{1/2}, $$ is also acceptable. Answers in this direction would be helpful, though the original question is still my primary goal.","Suppose I have the following matrix constructed from some orthogonal matrix and a diagonal matrix Is there a simple way to evaluate for positive integer in a similar way to e.g. a diagonalizable matrix ? I feel this ought to be the case given how is decomposed into a product of orthogonal and diagonal matrices but alas it is not clear to me if this is always so. Of course is also an orthogonal matrix and can itself be diagonalized but do have clear expressions in terms of ? EDIT: Some further work on my end indicates that the same question when is also acceptable. Answers in this direction would be helpful, though the original question is still my primary goal.","O \pm 1 D=diag(\pm1,\dots,\pm1) 
A = O D O^{-1} D.
 A^n n B 
B^n = (P L P^{-1})^n = P L^n P^{-1} .
 A A 
A = O D O^{-1} D = RTR^{-1}
 R,T O,D 
A = D^{1/2} O D O^{-1} D^{1/2},
","['linear-algebra', 'matrices', 'matrix-decomposition']"
42,The definition of simple eigenvalue,The definition of simple eigenvalue,,"There seem to be two accepted definitions for simple eigenvalues. The definitions involve algebraic multiplicity and geometric multiplicity. When space has a finite dimension, the most used is algebraic multiplicity. As I am interested in the general case (where the dimension of space can be infinite) I will not speak of the characteristic polynomial. Let $E$ be a Banach space (possible infinite) and $A:E\to E$ a linear operator, then the eigenvalue $\lambda$ is simple if The dimension of $\mathcal{N}_{\lambda}=\cup_{k\in\mathbb{N}}\mathcal{N}((\lambda I-A)^k)$ is $1$ -- algebraic multiplicity $m_a(\lambda)=1$ ; or The dimension of $\mathcal{N}(\lambda I - A)$ is $1$ -- geometric multiplicity $m_g(\lambda)=1$ . Note that the definitions are not equivalent as we have $m_a \ge m_g$ . We have that, if an eigenvalue is simple in definition 1, it will be simple in definition 2, but the opposite is not true. As mentioned in this answer , in ergodic theory definition 2 is used. In this case, why is this definition used? Is there a range of useful operators they want to include using this definition? More precisely, I am interested in the difference when we characterize ergodicity . A system is ergodic iff 1 is a simple eigenvalue of the Koopman operator . See question 4 or this post . Another moment where the simplicity of the eigenvalue appears is in the Perron-Frobenius theorem ( finite version ). In this context of ergodic theory (mainly, speaking of the Koopman operator), I was told that the definitions are equivalent, but I was not convinced by the explanation. $\mathcal{N}(X)$ is the kernel or nullpace of $X$ .","There seem to be two accepted definitions for simple eigenvalues. The definitions involve algebraic multiplicity and geometric multiplicity. When space has a finite dimension, the most used is algebraic multiplicity. As I am interested in the general case (where the dimension of space can be infinite) I will not speak of the characteristic polynomial. Let be a Banach space (possible infinite) and a linear operator, then the eigenvalue is simple if The dimension of is -- algebraic multiplicity ; or The dimension of is -- geometric multiplicity . Note that the definitions are not equivalent as we have . We have that, if an eigenvalue is simple in definition 1, it will be simple in definition 2, but the opposite is not true. As mentioned in this answer , in ergodic theory definition 2 is used. In this case, why is this definition used? Is there a range of useful operators they want to include using this definition? More precisely, I am interested in the difference when we characterize ergodicity . A system is ergodic iff 1 is a simple eigenvalue of the Koopman operator . See question 4 or this post . Another moment where the simplicity of the eigenvalue appears is in the Perron-Frobenius theorem ( finite version ). In this context of ergodic theory (mainly, speaking of the Koopman operator), I was told that the definitions are equivalent, but I was not convinced by the explanation. is the kernel or nullpace of .",E A:E\to E \lambda \mathcal{N}_{\lambda}=\cup_{k\in\mathbb{N}}\mathcal{N}((\lambda I-A)^k) 1 m_a(\lambda)=1 \mathcal{N}(\lambda I - A) 1 m_g(\lambda)=1 m_a \ge m_g \mathcal{N}(X) X,"['linear-algebra', 'functional-analysis', 'dynamical-systems', 'spectral-theory', 'ergodic-theory']"
43,Linear independence of tensor product basis $\{ v_i \otimes w_j\}$ for $\{v_i\}$ and $\{w_j\}$ linearly independent.,Linear independence of tensor product basis  for  and  linearly independent.,\{ v_i \otimes w_j\} \{v_i\} \{w_j\},"Show that the set $\{v_i \otimes w_j\}$ is a linear independent subset of $V\otimes W$ when $\{v_i\}$ and $\{w_j\}$ are independent subsets of V and W respectively. I want to find an error in a proof of this statement. I've found great proofs of this in the literature, for example these notes and this book . They all have the same core of the proof: take a linear form $\varphi_i: V\to \mathbb{F}$ such that $\varphi_i(v_j)=\delta_{ij}$ , which exists thanks to independence of $\{v_i\}$ . Then we have a linear function $\phi_i : V\otimes W \to W$ given by $\phi_i(v\otimes w)= \varphi_i(v)w$ which is as close to projection $V\otimes W \to W$ as one can hope to get. We can transport any linear relation from $V\otimes W$ to $W$ and use linear independence of $W$ . Details are in the links above. Okay, so here is the argument I am skeptical of: $\DeclareMathOperator{span}{span}$ We can assume that $\{v_i\}$ and $\{w_j\}$ are bases, instead we can consider $V'=\operatorname{span}\{v_i\}$ and $W'=\operatorname{span}\{w_j\}$ . Let $F=F(\{v_i\}\times\{w_j\})$ be a free vector space with a basis $\{(v_i,w_j)\}$ . We define a bilinear function $\Phi:V\times W \to F$ by $$ \Phi(v,w) = \Phi\left(\sum_i a_iv_i,\sum_j b_jw_j\right) = \sum_{i,j}  a_ib_j(v_i,w_j)$$ As $\{v_i\}$ and $\{w_j\}$ are bases, therefore the coefficients $a_i,b_j$ are unique, so this function is well defined. By the universal property, there is a linear function $\varphi: V\otimes W \to F$ such that $\varphi(v_i\otimes w_j)=\Phi(v_i,w_j)=(v_i,w_j)$ . We see that it is an isomorphism, because we can define $\phi^{-1}$ on a basis: $$\phi^{-1}((v_i,w_j))=v_i\otimes w_j$$ which is clearly a linear inverse. Therefore $F$ is isomorphic to $V\otimes W$ , so if $\{(v_i,w_j)\}$ is a basis for $F$ , $\phi[\{(v_i,w_j)\}] = \{v_i\otimes w_j\}$ must also be a basis. It uses no projection or a variant thereof, and proves the statement almost tautologically. I've tried to find the error, but I couldn't. I have a really hard time accepting this proof, because I haven't seen it anywhere (and there are plenty of places) and it really feels logically less demanding as there are no linear functionals used, the isomorphism $F\simeq V\otimes W$ is constructed directly. Actually, there are a few faulty proofs similar to the above that were already corrected on this site, namely here and here (one of the answers) . If the proof above is correct, can you direct me to the the sources or, ideally, explain why this proof don't appear in the sources I linked to?","Show that the set is a linear independent subset of when and are independent subsets of V and W respectively. I want to find an error in a proof of this statement. I've found great proofs of this in the literature, for example these notes and this book . They all have the same core of the proof: take a linear form such that , which exists thanks to independence of . Then we have a linear function given by which is as close to projection as one can hope to get. We can transport any linear relation from to and use linear independence of . Details are in the links above. Okay, so here is the argument I am skeptical of: We can assume that and are bases, instead we can consider and . Let be a free vector space with a basis . We define a bilinear function by As and are bases, therefore the coefficients are unique, so this function is well defined. By the universal property, there is a linear function such that . We see that it is an isomorphism, because we can define on a basis: which is clearly a linear inverse. Therefore is isomorphic to , so if is a basis for , must also be a basis. It uses no projection or a variant thereof, and proves the statement almost tautologically. I've tried to find the error, but I couldn't. I have a really hard time accepting this proof, because I haven't seen it anywhere (and there are plenty of places) and it really feels logically less demanding as there are no linear functionals used, the isomorphism is constructed directly. Actually, there are a few faulty proofs similar to the above that were already corrected on this site, namely here and here (one of the answers) . If the proof above is correct, can you direct me to the the sources or, ideally, explain why this proof don't appear in the sources I linked to?","\{v_i \otimes w_j\} V\otimes W \{v_i\} \{w_j\} \varphi_i: V\to \mathbb{F} \varphi_i(v_j)=\delta_{ij} \{v_i\} \phi_i : V\otimes W \to W \phi_i(v\otimes w)= \varphi_i(v)w V\otimes W \to W V\otimes W W W \DeclareMathOperator{span}{span} \{v_i\} \{w_j\} V'=\operatorname{span}\{v_i\} W'=\operatorname{span}\{w_j\} F=F(\{v_i\}\times\{w_j\}) \{(v_i,w_j)\} \Phi:V\times W \to F  \Phi(v,w) = \Phi\left(\sum_i a_iv_i,\sum_j b_jw_j\right) = \sum_{i,j}  a_ib_j(v_i,w_j) \{v_i\} \{w_j\} a_i,b_j \varphi: V\otimes W \to F \varphi(v_i\otimes w_j)=\Phi(v_i,w_j)=(v_i,w_j) \phi^{-1} \phi^{-1}((v_i,w_j))=v_i\otimes w_j F V\otimes W \{(v_i,w_j)\} F \phi[\{(v_i,w_j)\}] = \{v_i\otimes w_j\} F\simeq V\otimes W","['linear-algebra', 'solution-verification', 'tensor-products', 'tensors']"
44,"Exemples of applications of ""groupoidification"" to linear algebra","Exemples of applications of ""groupoidification"" to linear algebra",,"I just read Baez's very nice blog notes about groupoidification, and around the beginning, he states : ""From all this, you should begin to vaguely see that starting from any sort of incidence geometry, we should be able to get a bunch of matrices. Facts about incidence geometry will give facts about linear algebra! 'Groupoidification' is an attempt to reverse-engineer this process. We will discover that lots of famous facts about linear algebra are secretly facts about incidence geometry!"" There are hints of how this could happen in said notes but (as far as I can see) no concrete example. What would be some nice (easy to begin with) examples of such a phenomenon ? I'm not looking for particularly hard or even new facts about linear algebra that a groupoidification could unravel (although if there are, I would be very glad to see them); but just some result(s) that appear more naturally, more clearly, or on which another point of view is given via this ""groupoidification"" method. So the perfect story would be:  ""I have this theorem of linear algebra, I know how to prove it but if I were to prove it the usual way there would be a whole lot of annoying mess and non-motivated computations; but there is a nice way to prove it by viewing the situation as some degroupoidification of a problem I know how to handle"". Something that would also be interesting (which is not so different) would be : ""Oh I have this fact about incidence geometry/spans of groupoids that's quite nice to prove; and if I translate it into linear algebraic terms I get something interesting, that I might have known but the proof of which is not very enlightening"". If there are a lot of details for your answer and you don't want to write them all down, it's not a problem, I'm fine with just ideas, as long as there's a concrete result in the end.","I just read Baez's very nice blog notes about groupoidification, and around the beginning, he states : ""From all this, you should begin to vaguely see that starting from any sort of incidence geometry, we should be able to get a bunch of matrices. Facts about incidence geometry will give facts about linear algebra! 'Groupoidification' is an attempt to reverse-engineer this process. We will discover that lots of famous facts about linear algebra are secretly facts about incidence geometry!"" There are hints of how this could happen in said notes but (as far as I can see) no concrete example. What would be some nice (easy to begin with) examples of such a phenomenon ? I'm not looking for particularly hard or even new facts about linear algebra that a groupoidification could unravel (although if there are, I would be very glad to see them); but just some result(s) that appear more naturally, more clearly, or on which another point of view is given via this ""groupoidification"" method. So the perfect story would be:  ""I have this theorem of linear algebra, I know how to prove it but if I were to prove it the usual way there would be a whole lot of annoying mess and non-motivated computations; but there is a nice way to prove it by viewing the situation as some degroupoidification of a problem I know how to handle"". Something that would also be interesting (which is not so different) would be : ""Oh I have this fact about incidence geometry/spans of groupoids that's quite nice to prove; and if I translate it into linear algebraic terms I get something interesting, that I might have known but the proof of which is not very enlightening"". If there are a lot of details for your answer and you don't want to write them all down, it's not a problem, I'm fine with just ideas, as long as there's a concrete result in the end.",,"['linear-algebra', 'geometry', 'soft-question', 'groupoids', 'categorification']"
45,"If nilpotent matrix $A$ and $ABBA$ commute, show that $AB$ is nilpotent.","If nilpotent matrix  and  commute, show that  is nilpotent.",A ABBA AB,"Let $A$ and $B$ be $nn$ complex matrices. If $A$ is a nilpotent matrix, and $A$ commute with $ABBA$ , show that $AB$ is nilpotent. Equivalently, the question can be expressed as following description. Let $A$ and $B$ be $nn$ complex matrices. Define the linear transformation $T$ as $T(B)=AB-BA$ . If $A$ is a nilpotent matrix, and $T^2(B)=0$ , show that $AB$ is nilpotent. I've known that $AB-BA$ is nilpotent. Furtherly, if $A^m=0$ , by considering $T^n(B)=\sum_{i=0}^n(-1)^iA^{n-i}BA^i$ , I found that $A^kBA^l=0$ when $k+l\geqslant m$ . But I don't know how to continue, thanks for any help.","Let and be complex matrices. If is a nilpotent matrix, and commute with , show that is nilpotent. Equivalently, the question can be expressed as following description. Let and be complex matrices. Define the linear transformation as . If is a nilpotent matrix, and , show that is nilpotent. I've known that is nilpotent. Furtherly, if , by considering , I found that when . But I don't know how to continue, thanks for any help.",A B nn A A ABBA AB A B nn T T(B)=AB-BA A T^2(B)=0 AB AB-BA A^m=0 T^n(B)=\sum_{i=0}^n(-1)^iA^{n-i}BA^i A^kBA^l=0 k+l\geqslant m,"['linear-algebra', 'matrices', 'nilpotence']"
46,Determinant of a symmetric matrix with entries on diagonals,Determinant of a symmetric matrix with entries on diagonals,,"I am interested in the calculation of the determinant of the $N\times N$ symmetric matrix \begin{equation*} \mathbf B = \left(\begin{array}{*{20}c} 2	&	& -1&	&-1& &\\ 	& 2	&	& -1& 	& \ddots&    \\  -1& & \ddots&  &\ddots & & -1 \\  & -1& & & &-1&  \\  -1& & \ddots&  &2 &&-1 \\  &\ddots & &-1  & &2& \\   & &-1 &  &-1 & &2   \\ \end{array}\right) \end{equation*} where the first $-1$ on the left in the first line from left to right is at position n, and the second $-1$ at position $m>n$ . The blank parts of the matrix are implicitly filled with zeros. Are there results or properties for such class of matrices that would allow to compute the determinant for any dimension $N$ , without its explicit calculation (that could be accomplished e.g. via Leibniz formula for determinants)?","I am interested in the calculation of the determinant of the symmetric matrix where the first on the left in the first line from left to right is at position n, and the second at position . The blank parts of the matrix are implicitly filled with zeros. Are there results or properties for such class of matrices that would allow to compute the determinant for any dimension , without its explicit calculation (that could be accomplished e.g. via Leibniz formula for determinants)?","N\times N \begin{equation*}
\mathbf B = \left(\begin{array}{*{20}c}
2	&	& -1&	&-1& &\\
	& 2	&	& -1& 	& \ddots&    \\
 -1& & \ddots&  &\ddots & & -1 \\
 & -1& & & &-1&  \\
 -1& & \ddots&  &2 &&-1 \\
 &\ddots & &-1  & &2& \\
  & &-1 &  &-1 & &2   \\
\end{array}\right)
\end{equation*} -1 -1 m>n N","['linear-algebra', 'matrices', 'determinant', 'symmetric-matrices']"
47,A conjecture on the Lyapunov equation,A conjecture on the Lyapunov equation,,"Let $A\in\mathbb{R}^{n\times n}$ be a Hurwitz stable matrix (i.e., all the eigenvalues of $A$ have strictly negative real part). Let $X\in\mathbb{R}^{n\times n}$ be a positive semi-definite matrix of unit trace, that is $X\succeq 0$ s.t. $\mathrm{tr}(X)=1$ , and let $P$ be the positive semidefinite solution of the following Lyapunov equation $$ AP+PA^\top = -X. $$ My question. Does there always exist a matrix $X\succeq 0$ with $\mathrm{tr}(X)=1$ such that the equality $$ \|P\|_2 = \frac{1}{-2\,\mathrm{tr}(A)} $$ holds true, where $\|P\|_2$ denotes the 2-norm of matrix $P$ ? If $A+A^\top$ is negative semi-definite ( $A+A^\top\preceq0$ ), then it is easy to see that the answer is in the affirmative. In fact, in this case, picking $X=\frac{1}{2\mathrm{tr}(A)}(A+A^\top)$ , yields $P=\frac{1}{-2\mathrm{tr}(A)}I$ , which in turn clearly implies $\|P\|_2=\frac{1}{-2\,\mathrm{tr}(A)}$ . After running an extensive amount of numerical simulations, it seems that the answer should be in the affirmative also for the case $A+A^\top\not\preceq 0$ . However, proving the latter fact seems to be a daunting task. So, any help or suggestions to tackle this conjecture is very appreciated. Thanks a lot! Remark. (Condition $A+A^\top\preceq 0$ is not necessary) Consider the following $2\times 2$ matrix $$ A = \begin{bmatrix}-1 & \frac{\sqrt{3}+2}{2} \\ \frac{\sqrt{3}-2}{2} & 0 \end{bmatrix}. $$ Matrix $A$ has two eigenvalues at $-0.5$ , whereas the eigenvalues of $A+A^\top$ are $-3$ and $1$ . Let us define $$ X = \begin{bmatrix}1 & 0 \\ 0 & 0 \end{bmatrix}, \quad P = \begin{bmatrix}\frac{1}{2} & 0 \\ 0 & -\frac{\sqrt{3}-2}{2(\sqrt{3}+2)} \end{bmatrix}\succ 0. $$ It holds that $$ AP+PA^\top =-X, $$ and $$\|P\|_2 = \frac{1}{2}=-\frac{1}{2\mathrm{tr}(A)}.$$","Let be a Hurwitz stable matrix (i.e., all the eigenvalues of have strictly negative real part). Let be a positive semi-definite matrix of unit trace, that is s.t. , and let be the positive semidefinite solution of the following Lyapunov equation My question. Does there always exist a matrix with such that the equality holds true, where denotes the 2-norm of matrix ? If is negative semi-definite ( ), then it is easy to see that the answer is in the affirmative. In fact, in this case, picking , yields , which in turn clearly implies . After running an extensive amount of numerical simulations, it seems that the answer should be in the affirmative also for the case . However, proving the latter fact seems to be a daunting task. So, any help or suggestions to tackle this conjecture is very appreciated. Thanks a lot! Remark. (Condition is not necessary) Consider the following matrix Matrix has two eigenvalues at , whereas the eigenvalues of are and . Let us define It holds that and","A\in\mathbb{R}^{n\times n} A X\in\mathbb{R}^{n\times n} X\succeq 0 \mathrm{tr}(X)=1 P 
AP+PA^\top = -X.
 X\succeq 0 \mathrm{tr}(X)=1 
\|P\|_2 = \frac{1}{-2\,\mathrm{tr}(A)}
 \|P\|_2 P A+A^\top A+A^\top\preceq0 X=\frac{1}{2\mathrm{tr}(A)}(A+A^\top) P=\frac{1}{-2\mathrm{tr}(A)}I \|P\|_2=\frac{1}{-2\,\mathrm{tr}(A)} A+A^\top\not\preceq 0 A+A^\top\preceq 0 2\times 2 
A = \begin{bmatrix}-1 & \frac{\sqrt{3}+2}{2} \\ \frac{\sqrt{3}-2}{2} & 0 \end{bmatrix}.
 A -0.5 A+A^\top -3 1 
X = \begin{bmatrix}1 & 0 \\ 0 & 0 \end{bmatrix}, \quad P = \begin{bmatrix}\frac{1}{2} & 0 \\ 0 & -\frac{\sqrt{3}-2}{2(\sqrt{3}+2)} \end{bmatrix}\succ 0.
 
AP+PA^\top =-X,
 \|P\|_2 = \frac{1}{2}=-\frac{1}{2\mathrm{tr}(A)}.","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'matrix-equations', 'positive-definite']"
48,Box-constrained orthogonal matrix,Box-constrained orthogonal matrix,,"Given constants $\ell, u \in \mathbb{R}^{3 \times 3}$ and the following system of constraints in $P \in \mathbb{R}^{3 \times 3}$ $$ P^T P = I_{3 \times 3},\quad \ell_{ij} \leq P_{ij} \leq u_{ij}, $$ I would like to find a matrix $P$ which satisfies this system, or determine that it is infeasible. Is there a computationally efficient way to perform this task? The solution doesn't have to be closed form, but it should be an algorithm implementable on a computer which runs quickly, exploiting the fact that it is an $9$ dimensional problem. A numerical algorithm which converges to a solution is also a very good option, but there should be a proof that indeed it converges to a solution of the problem.","Given constants $\ell, u \in \mathbb{R}^{3 \times 3}$ and the following system of constraints in $P \in \mathbb{R}^{3 \times 3}$ $$ P^T P = I_{3 \times 3},\quad \ell_{ij} \leq P_{ij} \leq u_{ij}, $$ I would like to find a matrix $P$ which satisfies this system, or determine that it is infeasible. Is there a computationally efficient way to perform this task? The solution doesn't have to be closed form, but it should be an algorithm implementable on a computer which runs quickly, exploiting the fact that it is an $9$ dimensional problem. A numerical algorithm which converges to a solution is also a very good option, but there should be a proof that indeed it converges to a solution of the problem.",,"['linear-algebra', 'orthogonal-matrices', 'non-convex-optimization', 'real-algebraic-geometry', 'semialgebraic-geometry']"
49,Does the space of matrices above rank $k$ admit a transitive Lie group action?,Does the space of matrices above rank  admit a transitive Lie group action?,k,"$\newcommand{\End}{\operatorname{End}}$ $\newcommand{\GL}{\operatorname{GL}}$ Let $V$ be a real $d$-dimensional vector space ($d \ge 4$). Let $2 \le k \le d-2$. Define $H_{>k}=\{ A \in \text{End}(V) \mid \operatorname{rank}(A) > k  \}$. $H_{>k}$ is an open connected submanifold of $ \text{End}(V)$. Question: Does $H_{>k}$ admit a transitive Lie group action ? That is, does there exist a (finite dimeniosnal) Lie group $G$, and a smooth action of $G$ on $H_{>k}$, such that $H_{>k}$ itself is the single orbit? There is a natural action on $H_{>k}$ by $\GL(V) \times \GL(V)$, given by $$ (A,B) \cdot C=ACB^{-1}. $$ This action has a finite number of orbits; each orbit is the space of endomorphisms of a certain rank. So, it is not transitive. (but is ""close"" to being one). Even if there is a transitive action, I don't expect it to be as ""natural"" as the action considered above. Still, I am interested to know whether or not such group action exist. I know there are obstructions for a manifold to admit a transitive Lie group action in the case where it's compact. Here, however, our manifold $H_{>k}$ is non-compact.","$\newcommand{\End}{\operatorname{End}}$ $\newcommand{\GL}{\operatorname{GL}}$ Let $V$ be a real $d$-dimensional vector space ($d \ge 4$). Let $2 \le k \le d-2$. Define $H_{>k}=\{ A \in \text{End}(V) \mid \operatorname{rank}(A) > k  \}$. $H_{>k}$ is an open connected submanifold of $ \text{End}(V)$. Question: Does $H_{>k}$ admit a transitive Lie group action ? That is, does there exist a (finite dimeniosnal) Lie group $G$, and a smooth action of $G$ on $H_{>k}$, such that $H_{>k}$ itself is the single orbit? There is a natural action on $H_{>k}$ by $\GL(V) \times \GL(V)$, given by $$ (A,B) \cdot C=ACB^{-1}. $$ This action has a finite number of orbits; each orbit is the space of endomorphisms of a certain rank. So, it is not transitive. (but is ""close"" to being one). Even if there is a transitive action, I don't expect it to be as ""natural"" as the action considered above. Still, I am interested to know whether or not such group action exist. I know there are obstructions for a manifold to admit a transitive Lie group action in the case where it's compact. Here, however, our manifold $H_{>k}$ is non-compact.",,"['linear-algebra', 'lie-groups', 'group-actions', 'matrix-rank', 'homogeneous-spaces']"
50,"What is $\ \overline{\bigcup_{p 1}\ \{A\in M_n(\mathbb C), \ A^p = I_n\}} \ $?",What is ?,"\ \overline{\bigcup_{p 1}\ \{A\in M_n(\mathbb C), \ A^p = I_n\}} \ ","Let $\Gamma_p = \{A\in M_n(\mathbb C), A^p = I_n\}$ and let $\Gamma = \bigcup_{p 1}\ \Gamma_p$. What is the closure of $\Gamma$ ? (This is from an oral exam). Let $B \in M_n(\mathbb C)$ such that there exists a sequence $(A_k)_k$ of elements of $\Gamma$ such that $A_k \to B$. $\chi_{A_k} \to_{k\to\infty} \chi_B$ ($\chi$ is the characteristic polynomial). We will prove that each eigenvalue $\lambda$ of $B$ satisfies $|\lambda | =1$. Writing $\chi_{A_k} = \prod_{1  i  n}(X-a_{i,k})$, with $|a_{i,k}|=1$ for all $i$ and $k$ ; all $(a_{i,k})_k, 1in,$ are bounded so we can apply Bolzano-Weierstrass theorem in $\mathbb C^n$ to prove there exists $\phi$ an extraction such that $a_{i, \phi(k)} \to_{k\to\infty} l_i$ for all $i$, $1in$. $\chi_{A_{\phi(k)}} \to_{k \to \infty} P$ and the roots of $P$ are the limits $l_i$ of $a_{i, \phi(k)}$, hence $|l_i|=1$ for all $i$. Otherwise $\chi_{A_{\phi(k)}} \to \chi_B$, hence $ \chi_B = P$. Reciprocally, suppose $B$ is such that for all eigenvalue $\lambda,$ $|\lambda| =1$. $B = P^{-1}TP$ with $T$ being upper triangular. For every diagonal coefficient $t_i$ of $T$, $t_i$ is one of the eigenvalue of $B$, hence $|t_i|=1$. For all $t_i$, we consider a sequence of roots of $1$ $(a_{i,k})_k$ such that $a_{i,k} \to_{k\to\infty} t_i$ and we choose these sequences such that for all $k$ and for all $(i,j), \ 1  i < j n$, $a_{i,k} \neq a_{j,k}$ (using the density of the roots of $1$ in the unit circle). Let $A_k$ be the upper triangular matrix with diagonal coefficient $i$ equal to $a_{i,k}$ for  $1  i  n$ and the other coefficients equal to the ones of $T$. We will prove that for all $k$, there exists $p_k  1$ such that $A_k^{p_k} = I_n$ : For all $i$ there exists $p_{i,k}$ such that $a_{i,k}^{p_{i,k}} = 1$. Let $p_k$ be a common multiple of the $p_{i,k}, 1  i  n$. $\chi_{A_k} = \prod_{1  i  n}(X-a_{i,k})$ vanishes $A_k$ and the $a_{i,k},  \ 1  i  n$ are distinct, hence $X^{p_k} - 1$ vanishes $A_k$ i.e. $A_k^{p_k} = I_n$. Finally, $(P^{-1}A_kP)_k$ is a sequence of $\Gamma$ (because $(P^{-1}A_kP)^{p_k} = I_n$)  that converges towards $B$ since $A_k \to T$. Is that correct?","Let $\Gamma_p = \{A\in M_n(\mathbb C), A^p = I_n\}$ and let $\Gamma = \bigcup_{p 1}\ \Gamma_p$. What is the closure of $\Gamma$ ? (This is from an oral exam). Let $B \in M_n(\mathbb C)$ such that there exists a sequence $(A_k)_k$ of elements of $\Gamma$ such that $A_k \to B$. $\chi_{A_k} \to_{k\to\infty} \chi_B$ ($\chi$ is the characteristic polynomial). We will prove that each eigenvalue $\lambda$ of $B$ satisfies $|\lambda | =1$. Writing $\chi_{A_k} = \prod_{1  i  n}(X-a_{i,k})$, with $|a_{i,k}|=1$ for all $i$ and $k$ ; all $(a_{i,k})_k, 1in,$ are bounded so we can apply Bolzano-Weierstrass theorem in $\mathbb C^n$ to prove there exists $\phi$ an extraction such that $a_{i, \phi(k)} \to_{k\to\infty} l_i$ for all $i$, $1in$. $\chi_{A_{\phi(k)}} \to_{k \to \infty} P$ and the roots of $P$ are the limits $l_i$ of $a_{i, \phi(k)}$, hence $|l_i|=1$ for all $i$. Otherwise $\chi_{A_{\phi(k)}} \to \chi_B$, hence $ \chi_B = P$. Reciprocally, suppose $B$ is such that for all eigenvalue $\lambda,$ $|\lambda| =1$. $B = P^{-1}TP$ with $T$ being upper triangular. For every diagonal coefficient $t_i$ of $T$, $t_i$ is one of the eigenvalue of $B$, hence $|t_i|=1$. For all $t_i$, we consider a sequence of roots of $1$ $(a_{i,k})_k$ such that $a_{i,k} \to_{k\to\infty} t_i$ and we choose these sequences such that for all $k$ and for all $(i,j), \ 1  i < j n$, $a_{i,k} \neq a_{j,k}$ (using the density of the roots of $1$ in the unit circle). Let $A_k$ be the upper triangular matrix with diagonal coefficient $i$ equal to $a_{i,k}$ for  $1  i  n$ and the other coefficients equal to the ones of $T$. We will prove that for all $k$, there exists $p_k  1$ such that $A_k^{p_k} = I_n$ : For all $i$ there exists $p_{i,k}$ such that $a_{i,k}^{p_{i,k}} = 1$. Let $p_k$ be a common multiple of the $p_{i,k}, 1  i  n$. $\chi_{A_k} = \prod_{1  i  n}(X-a_{i,k})$ vanishes $A_k$ and the $a_{i,k},  \ 1  i  n$ are distinct, hence $X^{p_k} - 1$ vanishes $A_k$ i.e. $A_k^{p_k} = I_n$. Finally, $(P^{-1}A_kP)_k$ is a sequence of $\Gamma$ (because $(P^{-1}A_kP)^{p_k} = I_n$)  that converges towards $B$ since $A_k \to T$. Is that correct?",,"['linear-algebra', 'general-topology']"
51,"Matrix exponential, containing a thermal state","Matrix exponential, containing a thermal state",,"Define an infinite matrix $$ M =  \begin{bmatrix} 0 & -1 & 0 & 0 & \cdots \\ 1 & 0 & -2 & 0 & \cdots \\ 0 & 2 & 0 & -3 & \cdots \\ 0 & 0 & 3 & 0 & \cdots \\ \vdots & \vdots & \vdots & \vdots & \ddots \\ \end{bmatrix}$$ Numerically, I've found that the first column of $\exp(M)$ is given by $\alpha(1,e^{-\lambda},e^{-2\lambda},e^{-3\lambda},\dots)^T$, where $\lambda \approx 0.27$ and $\alpha = \sqrt{1-e^{-2\lambda}} \approx 0.65$. Question: How to prove analytically that the first column of $\exp(M)$ has the stated form? [Context: This question comes from a quantum mechanical model of two harmonic oscillators coupled by a Hamiltonian of the form $\hat{H} \propto \hat{a}_1^\dagger \hat{a}_2^\dagger - \hat{a}_1 \hat{a}_2$, where $\hat{a}_i$ is the lowering operator for oscillator $i$.  The matrix $M$ is precisely this Hamiltonian on the subspace spanned by $\{\left|nn\right>\}$.  I've tried using a BCH approach, but was discouraged by the fact that the commutators are not very cooperative.  The exponential coefficients in the first column indicates (roughly) a thermal state.] Edit: I cross-posted this on mathoverflow , and got a beautiful solution by Jochen Glueck.  The exact value of $\lambda$ is $-\ln(\tanh(1))$.","Define an infinite matrix $$ M =  \begin{bmatrix} 0 & -1 & 0 & 0 & \cdots \\ 1 & 0 & -2 & 0 & \cdots \\ 0 & 2 & 0 & -3 & \cdots \\ 0 & 0 & 3 & 0 & \cdots \\ \vdots & \vdots & \vdots & \vdots & \ddots \\ \end{bmatrix}$$ Numerically, I've found that the first column of $\exp(M)$ is given by $\alpha(1,e^{-\lambda},e^{-2\lambda},e^{-3\lambda},\dots)^T$, where $\lambda \approx 0.27$ and $\alpha = \sqrt{1-e^{-2\lambda}} \approx 0.65$. Question: How to prove analytically that the first column of $\exp(M)$ has the stated form? [Context: This question comes from a quantum mechanical model of two harmonic oscillators coupled by a Hamiltonian of the form $\hat{H} \propto \hat{a}_1^\dagger \hat{a}_2^\dagger - \hat{a}_1 \hat{a}_2$, where $\hat{a}_i$ is the lowering operator for oscillator $i$.  The matrix $M$ is precisely this Hamiltonian on the subspace spanned by $\{\left|nn\right>\}$.  I've tried using a BCH approach, but was discouraged by the fact that the commutators are not very cooperative.  The exponential coefficients in the first column indicates (roughly) a thermal state.] Edit: I cross-posted this on mathoverflow , and got a beautiful solution by Jochen Glueck.  The exact value of $\lambda$ is $-\ln(\tanh(1))$.",,"['linear-algebra', 'matrices', 'matrix-exponential', 'infinite-matrices']"
52,"Finite group of ""linear substitutions""","Finite group of ""linear substitutions""",,"From what I can tell, a linear substitution is an operation on a set of variables $x_1,\ldots,x_n$ which sends them to a new set of variables $y_1,\ldots, y_n$ via a linear transformation $$\vec{y} = \mathbf{A}\vec{x}$$ Provided the matrix $\mathbf{A}$ is invertible, the transformation $\vec{x}\rightarrow \vec{y}$ is invertible. Because these linear substitutions are composable, invertible, and have unit, it is possible to form group structures out of them. In a paper, Painlev considers an arbitrary group $\alpha$ of linear substitutions for two variables. From what I understand, Painlev asks us to consider a pair of functions $\varphi$ and $\psi$ which constitute an invariant with respect to the group operation; that is to say, $$\langle x, y\rangle =\langle \varphi(t,u), \psi(t,u)\rangle$$ and whenever $\langle T, U\rangle$ are a solution to this equation, the complete set of solutions is exactly obtained by applying all the transformations in the group $\alpha$ to $T$ and $U$:  $\{ f(T,U) : f\in \alpha\}$. I am mystified, however, when Painlev subsequently treats $t$ and $u$ as functions of $x$ and $y$, forming all second-order partial derivatives of $$ \begin{align*} t(aT + bU + c) = a^\prime T + b^\prime U + c^\prime\\ u(aT + bU + c) = a^{\prime\prime} T + b^{\prime\prime}U + c^{\prime\prime}\\ \end{align*} $$ My questions are: Where did these equations come from? How is it that we treat $t$ and $u$ as functions of $x$ and $y$? Why are there constant terms $c$? (Surely a ""linear substitution"" cannot include constant terms or else the transformation wouldn't be invertible?) What does it mean to call $\langle \phi, \psi\rangle$ invariants? They look like a description of an arbitrary member $f\in \alpha$. Perhaps something was lost in translation? The relevant excerpt, in French (which I don't speak), is included below: I surmise that it says something like: Consider a finite group $\alpha$ of linear substitutions involving two variables and two fundamental invariant functions which correspond to them: $$x=\phi(t,u),\qquad y=\psi(t,u);$$   If, for a system $(x,y)$, the values $(T,U)$ satisfy the equations (1), all the other solutions of these equations are obtained by applying to the values $(T,U)$ all of the substitutions in the group $\alpha$. We differentiate the equations $$ \begin{align*} t(aT + bU + c) = a^\prime T + b^\prime U + c^\prime\\ u(aT + bU + c) = a^{\prime\prime} T + b^{\prime\prime}U + c^{\prime\prime}\\ \end{align*} $$ with respect to $x$ and $y$ up to second order, inclusive. In this way, we form twelve equations [i.e. $\partial_x, \partial_y, \partial_{xx}, \partial_{xy}, \partial_{yx}, \partial_{yy}$ for the two equations?], homogeneous and linear in $a$, $b$, $c$, $a^\prime$, ...., and, if we eliminate those constants, the remaining four equations involve partial derivatives (first and second) of $t$ and of $u$. I discovered that in Transformations of the fundamental equations of thermodynamics , Buckley describes a similar kind of (reversible differentiable but not necessarily linear?) substitution which might shed light on this one. To paraphrase: Suppose the state of a physical system can be described by variables $x_1,\ldots x_n$. Then there exists a characteristic function $E$, which is the solution of a particular exact differential equation $$\partial E - \sum_i x_i\,dx_i = 0,$$ such that the behavior of the system in any state can be conveniently described in terms of $E$ and its partial derivatives with respect to the variables $x_1,\ldots, x_n$.  Specifically, in thermodynamics, we can use the state variables volume $V$ and entropy $S$; all general thermodynamic formulas are derivable from $E$ and the basic equations     $$\begin{align*}H &\equiv E - V\, \partial_V E\\ F &\equiv E - S\partial_S E\\G &\equiv E - V\,\partial_V E - S\partial_S E = H-F+E\end{align*}$$ ""Legitimate operations"" (?) on these equations generate a collection $C$ of thermodynamic formulas which are valid with respect to the state variables $V, S$. If we change variables $\mathbf{\mathsf t}:(V,S)\mapsto (V^\prime, S^\prime)$, the new variables \begin{align*}V^\prime = \varphi(V)\\S^\prime = \psi(S)\\\end{align*} are functionally independent and hence soluble for the originals. Under this change of variables, the functions $EHFG$ and formulas $C$ will, generally speaking, change form, but they must remain identically true. Suppose, however, that the functions $EHFG$ retain the same form under a particular change of variables. Then the formulas $C$ must also retain the same form. If the collection of all such transformations forms a group $\mathsf{G}$, then the fundamental formulas will remain unchanged under $\mathsf{G}$, and hence the group characterizes a fundamental arbitrariness in the thermodynamic description of all systems. This is analogous to the fact that, in relativity, the description of the laws of electromagnetic radiation remain unchanged under a Lorentz(-Einstein) transform. But I guess I still don't understand formally what it means to say that an equation has ""the same form"" under a transformation $T$, e.g. expressed as a composition of functions. Perhaps it means that unlike in the general case where of course a change of variables doesn't affect the equation $$(f\circ T^{-1})[T(x)] = (g\circ T^{-1})[T(x)]\;\iff\; f(x) = g(x),$$ a change of variables has the same functional form if not only that, but also (?): $$f(T(x)) = g(T(x)) \iff f(x) = g(x)$$","From what I can tell, a linear substitution is an operation on a set of variables $x_1,\ldots,x_n$ which sends them to a new set of variables $y_1,\ldots, y_n$ via a linear transformation $$\vec{y} = \mathbf{A}\vec{x}$$ Provided the matrix $\mathbf{A}$ is invertible, the transformation $\vec{x}\rightarrow \vec{y}$ is invertible. Because these linear substitutions are composable, invertible, and have unit, it is possible to form group structures out of them. In a paper, Painlev considers an arbitrary group $\alpha$ of linear substitutions for two variables. From what I understand, Painlev asks us to consider a pair of functions $\varphi$ and $\psi$ which constitute an invariant with respect to the group operation; that is to say, $$\langle x, y\rangle =\langle \varphi(t,u), \psi(t,u)\rangle$$ and whenever $\langle T, U\rangle$ are a solution to this equation, the complete set of solutions is exactly obtained by applying all the transformations in the group $\alpha$ to $T$ and $U$:  $\{ f(T,U) : f\in \alpha\}$. I am mystified, however, when Painlev subsequently treats $t$ and $u$ as functions of $x$ and $y$, forming all second-order partial derivatives of $$ \begin{align*} t(aT + bU + c) = a^\prime T + b^\prime U + c^\prime\\ u(aT + bU + c) = a^{\prime\prime} T + b^{\prime\prime}U + c^{\prime\prime}\\ \end{align*} $$ My questions are: Where did these equations come from? How is it that we treat $t$ and $u$ as functions of $x$ and $y$? Why are there constant terms $c$? (Surely a ""linear substitution"" cannot include constant terms or else the transformation wouldn't be invertible?) What does it mean to call $\langle \phi, \psi\rangle$ invariants? They look like a description of an arbitrary member $f\in \alpha$. Perhaps something was lost in translation? The relevant excerpt, in French (which I don't speak), is included below: I surmise that it says something like: Consider a finite group $\alpha$ of linear substitutions involving two variables and two fundamental invariant functions which correspond to them: $$x=\phi(t,u),\qquad y=\psi(t,u);$$   If, for a system $(x,y)$, the values $(T,U)$ satisfy the equations (1), all the other solutions of these equations are obtained by applying to the values $(T,U)$ all of the substitutions in the group $\alpha$. We differentiate the equations $$ \begin{align*} t(aT + bU + c) = a^\prime T + b^\prime U + c^\prime\\ u(aT + bU + c) = a^{\prime\prime} T + b^{\prime\prime}U + c^{\prime\prime}\\ \end{align*} $$ with respect to $x$ and $y$ up to second order, inclusive. In this way, we form twelve equations [i.e. $\partial_x, \partial_y, \partial_{xx}, \partial_{xy}, \partial_{yx}, \partial_{yy}$ for the two equations?], homogeneous and linear in $a$, $b$, $c$, $a^\prime$, ...., and, if we eliminate those constants, the remaining four equations involve partial derivatives (first and second) of $t$ and of $u$. I discovered that in Transformations of the fundamental equations of thermodynamics , Buckley describes a similar kind of (reversible differentiable but not necessarily linear?) substitution which might shed light on this one. To paraphrase: Suppose the state of a physical system can be described by variables $x_1,\ldots x_n$. Then there exists a characteristic function $E$, which is the solution of a particular exact differential equation $$\partial E - \sum_i x_i\,dx_i = 0,$$ such that the behavior of the system in any state can be conveniently described in terms of $E$ and its partial derivatives with respect to the variables $x_1,\ldots, x_n$.  Specifically, in thermodynamics, we can use the state variables volume $V$ and entropy $S$; all general thermodynamic formulas are derivable from $E$ and the basic equations     $$\begin{align*}H &\equiv E - V\, \partial_V E\\ F &\equiv E - S\partial_S E\\G &\equiv E - V\,\partial_V E - S\partial_S E = H-F+E\end{align*}$$ ""Legitimate operations"" (?) on these equations generate a collection $C$ of thermodynamic formulas which are valid with respect to the state variables $V, S$. If we change variables $\mathbf{\mathsf t}:(V,S)\mapsto (V^\prime, S^\prime)$, the new variables \begin{align*}V^\prime = \varphi(V)\\S^\prime = \psi(S)\\\end{align*} are functionally independent and hence soluble for the originals. Under this change of variables, the functions $EHFG$ and formulas $C$ will, generally speaking, change form, but they must remain identically true. Suppose, however, that the functions $EHFG$ retain the same form under a particular change of variables. Then the formulas $C$ must also retain the same form. If the collection of all such transformations forms a group $\mathsf{G}$, then the fundamental formulas will remain unchanged under $\mathsf{G}$, and hence the group characterizes a fundamental arbitrariness in the thermodynamic description of all systems. This is analogous to the fact that, in relativity, the description of the laws of electromagnetic radiation remain unchanged under a Lorentz(-Einstein) transform. But I guess I still don't understand formally what it means to say that an equation has ""the same form"" under a transformation $T$, e.g. expressed as a composition of functions. Perhaps it means that unlike in the general case where of course a change of variables doesn't affect the equation $$(f\circ T^{-1})[T(x)] = (g\circ T^{-1})[T(x)]\;\iff\; f(x) = g(x),$$ a change of variables has the same functional form if not only that, but also (?): $$f(T(x)) = g(T(x)) \iff f(x) = g(x)$$",,"['linear-algebra', 'group-theory', 'ordinary-differential-equations', 'linear-transformations']"
53,Is there a name for the group of real matrices whose determinant is an element of $\pm 1$?,Is there a name for the group of real matrices whose determinant is an element of ?,\pm 1,"The group of matrices whose determinant is non-zero is called the ""general linear group"", and the group of matrices whose determinant is $1$ is called the ""special linear group"". In between these two extremes is the group of matrices whose determinant is an element of $\pm 1$. It would be nice if this group had a name, for pedagogical reasons, because its elements can be thought of as volume-preserving transformations that fix the origin (whereas the elements of the special linear group preserve signed volume). Does it? Related.","The group of matrices whose determinant is non-zero is called the ""general linear group"", and the group of matrices whose determinant is $1$ is called the ""special linear group"". In between these two extremes is the group of matrices whose determinant is an element of $\pm 1$. It would be nice if this group had a name, for pedagogical reasons, because its elements can be thought of as volume-preserving transformations that fix the origin (whereas the elements of the special linear group preserve signed volume). Does it? Related.",,"['linear-algebra', 'matrices', 'group-theory', 'terminology', 'representation-theory']"
54,Flipping Summation of Kronecker Products,Flipping Summation of Kronecker Products,,"Question Suppose $\mathbf A$ is an $n\times n$ matrix, and that $\mathbf B_i$ is an $m\times m$ matrix, for all $i\in\{1,\dots, n\}$. Is it possible to find $n\times n$ matrices $\mathbf U$ and $\mathbf V$, and $m\times m$ matrices $\mathbf X_i$, for all $i\in\{1,\dots, n\}$, such that $$\left[\sum_{i}(\mathbf E_{ii}^n\otimes \mathbf B_{i})\right](\mathbf A\otimes \mathbf I_{m})\left[\sum_{i}(\mathbf E_{ii}^n\otimes \mathbf B_{i})\right]=(\mathbf U \otimes \mathbf I_{m})\left[\sum_{i}(\mathbf E_{ii}^n\otimes \mathbf X_{i})\right](\mathbf V\otimes \mathbf I_{m}), $$ where $\{\mathbf E_{ij}^n\}$ are the $n\times n$ matrix units? Perhaps this is not possible under these general conditions, so basically I'm trying to find the minimal set of assumptions (i.e. necessary and sufficient) on the $\{\mathbf B_i\}$ matrices that would allow such a transformation. Obs$_1$: The answers to this question can be helpful to generalize Assumption 2 bellow in the step that leads to equation $(2.2)$. Obs$_2$: If a complete answer is not possible, I would be happy with an answer that simply relaxes one of the assumptions bellow. $$$$ $$$$ How far I've been able to go, i.e. strong assumptions that work: Assumption 1 Suppose that $$ \mathbf B_i \mathbf B_j = d_{ij} \mathbf D. \tag 1$$ To see how it works in this case, notice that $$\left[\sum_{i}(\mathbf E_{ii}^n\otimes \mathbf B_{i})\right](\mathbf A\otimes \mathbf I_{m})\left[\sum_{i}(\mathbf E_{ii}^n\otimes \mathbf B_{i})\right]= \sum_{i,j}(\mathbf E_{ii}^n\mathbf A\mathbf E_{jj}^n\otimes \mathbf B_{i}\mathbf B_{j}), $$ and that under $(1)$, $$\sum_{i,j}(\mathbf E_{ii}^n\mathbf A\mathbf E_{jj}^n\otimes \mathbf B_{i}\mathbf B_{j})=\sum_{i,j}(a_{ij}d_{ij}\mathbf E_{ij}^n\otimes \mathbf D), $$ so it is enough to set $$ \mathbf U = \sum_{i,j}a_{ij}d_{ij}\mathbf E_{ij}^n, \quad \mathbf X_i = \mathbf D, \quad \text{and} \quad \mathbf V = \mathbf I_n.$$ Assumption 2 For all $i\in\{1,\dots, n\}$, let $$	\mathbf B_{i} = \operatorname{diag}(b_{i1},...,b_{im}),\tag{2.1}$$ and notice that \begin{align} \left[\sum_{i=1}^n(\mathbf E_{ii}^n\otimes \mathbf B_{i})\right](\mathbf A\otimes I_{m})\left[\sum_{j=1}^n(\mathbf E_{jj}^n \otimes \mathbf B_{j})\right] &=\sum_{i=1}^n\sum_{j=1}^n(\mathbf E_{ii}^n \mathbf A \mathbf E_{jj}^n\otimes \mathbf B_{i}\mathbf B_{j})\\[1.5ex] &=\sum_{i=1}^n\sum_{j=1}^n(a_{ij}\mathbf E_{ij}^n \otimes\sum_{k=1}^{m}b_{ik}b_{jk}\mathbf E_{kk}^{m})\\[1.5ex] &=\sum_{i=1}^n\sum_{j=1}^n\sum_{k=1}^{m}(a_{ij}b_{ik}b_{jk}\mathbf E_{ij}^n\otimes \mathbf E_{kk}^{m})\\[1.5ex] &=\sum_{k=1}^{m}\left(\left(\sum_{i=1}^n\sum_{j=1}^n a_{ij}b_{ik} b_{jk}\mathbf E_{ij}^n\right)\otimes \mathbf E_{kk}^{m}\right) \end{align} Next, suppose $ \sum_{i=1}^n\sum_{j=1}^n a_{ij}b_{ik}b_{jk} \mathbf E_{ij}^n$ has eigenvectors independent of $k$, so that its eigendecomposition can be written as $$ \sum_{i=1}^n\sum_{j=1}^n a_{ij}b_{ik}b_{jk} \mathbf E_{ij}^n = \mathbf Q \mathbf \Omega_{k}\mathbf Q^{-1} \tag{2.2} $$ with $$	\mathbf \Omega_{k} = \operatorname{diag}(\omega_{k1},...,\omega_{kn}).$$ Then \begin{align} \sum_{k=1}^{m}\left(\left(\sum_{i=1}^n\sum_{j=1}^n a_{ij}b_{ik} b_{jk}\mathbf E_{ij}^n\right)\otimes \mathbf E_{kk}^{m}\right) &=\sum_{k=1}^{m}\left(\sum_{i=1}^n\omega_{ki}\mathbf Q\mathbf E_{ii}^n \mathbf Q^{-1}\otimes \mathbf E_{kk}^{m}\right)\\[1.5ex] &=(\mathbf Q\otimes \mathbf I_{m})\left[\sum_{k=1}^{m}\sum_{i=1}^n(\mathbf E_{ii}^n\otimes \omega_{ki}\mathbf E_{kk}^{m})\right](\mathbf Q^{-1}\otimes \mathbf I_{m})\\[1.5ex] &=(\mathbf Q\otimes \mathbf I_{m})\left[\sum_{i=1}^n(\mathbf E_{ii}^n\otimes \sum_{k=1}^{m}\omega_{ki}\mathbf E_{kk}^{m})\right](\mathbf Q^{-1}\otimes \mathbf I_{m}) \end{align} so, we can set $$ \mathbf U = \mathbf Q,\quad \mathbf X_{i} =\sum_{k=1}^{m}\omega_{ki}\mathbf E_{kk}^{m},\quad\text{and}\quad \mathbf V = \mathbf Q^{-1}.$$","Question Suppose $\mathbf A$ is an $n\times n$ matrix, and that $\mathbf B_i$ is an $m\times m$ matrix, for all $i\in\{1,\dots, n\}$. Is it possible to find $n\times n$ matrices $\mathbf U$ and $\mathbf V$, and $m\times m$ matrices $\mathbf X_i$, for all $i\in\{1,\dots, n\}$, such that $$\left[\sum_{i}(\mathbf E_{ii}^n\otimes \mathbf B_{i})\right](\mathbf A\otimes \mathbf I_{m})\left[\sum_{i}(\mathbf E_{ii}^n\otimes \mathbf B_{i})\right]=(\mathbf U \otimes \mathbf I_{m})\left[\sum_{i}(\mathbf E_{ii}^n\otimes \mathbf X_{i})\right](\mathbf V\otimes \mathbf I_{m}), $$ where $\{\mathbf E_{ij}^n\}$ are the $n\times n$ matrix units? Perhaps this is not possible under these general conditions, so basically I'm trying to find the minimal set of assumptions (i.e. necessary and sufficient) on the $\{\mathbf B_i\}$ matrices that would allow such a transformation. Obs$_1$: The answers to this question can be helpful to generalize Assumption 2 bellow in the step that leads to equation $(2.2)$. Obs$_2$: If a complete answer is not possible, I would be happy with an answer that simply relaxes one of the assumptions bellow. $$$$ $$$$ How far I've been able to go, i.e. strong assumptions that work: Assumption 1 Suppose that $$ \mathbf B_i \mathbf B_j = d_{ij} \mathbf D. \tag 1$$ To see how it works in this case, notice that $$\left[\sum_{i}(\mathbf E_{ii}^n\otimes \mathbf B_{i})\right](\mathbf A\otimes \mathbf I_{m})\left[\sum_{i}(\mathbf E_{ii}^n\otimes \mathbf B_{i})\right]= \sum_{i,j}(\mathbf E_{ii}^n\mathbf A\mathbf E_{jj}^n\otimes \mathbf B_{i}\mathbf B_{j}), $$ and that under $(1)$, $$\sum_{i,j}(\mathbf E_{ii}^n\mathbf A\mathbf E_{jj}^n\otimes \mathbf B_{i}\mathbf B_{j})=\sum_{i,j}(a_{ij}d_{ij}\mathbf E_{ij}^n\otimes \mathbf D), $$ so it is enough to set $$ \mathbf U = \sum_{i,j}a_{ij}d_{ij}\mathbf E_{ij}^n, \quad \mathbf X_i = \mathbf D, \quad \text{and} \quad \mathbf V = \mathbf I_n.$$ Assumption 2 For all $i\in\{1,\dots, n\}$, let $$	\mathbf B_{i} = \operatorname{diag}(b_{i1},...,b_{im}),\tag{2.1}$$ and notice that \begin{align} \left[\sum_{i=1}^n(\mathbf E_{ii}^n\otimes \mathbf B_{i})\right](\mathbf A\otimes I_{m})\left[\sum_{j=1}^n(\mathbf E_{jj}^n \otimes \mathbf B_{j})\right] &=\sum_{i=1}^n\sum_{j=1}^n(\mathbf E_{ii}^n \mathbf A \mathbf E_{jj}^n\otimes \mathbf B_{i}\mathbf B_{j})\\[1.5ex] &=\sum_{i=1}^n\sum_{j=1}^n(a_{ij}\mathbf E_{ij}^n \otimes\sum_{k=1}^{m}b_{ik}b_{jk}\mathbf E_{kk}^{m})\\[1.5ex] &=\sum_{i=1}^n\sum_{j=1}^n\sum_{k=1}^{m}(a_{ij}b_{ik}b_{jk}\mathbf E_{ij}^n\otimes \mathbf E_{kk}^{m})\\[1.5ex] &=\sum_{k=1}^{m}\left(\left(\sum_{i=1}^n\sum_{j=1}^n a_{ij}b_{ik} b_{jk}\mathbf E_{ij}^n\right)\otimes \mathbf E_{kk}^{m}\right) \end{align} Next, suppose $ \sum_{i=1}^n\sum_{j=1}^n a_{ij}b_{ik}b_{jk} \mathbf E_{ij}^n$ has eigenvectors independent of $k$, so that its eigendecomposition can be written as $$ \sum_{i=1}^n\sum_{j=1}^n a_{ij}b_{ik}b_{jk} \mathbf E_{ij}^n = \mathbf Q \mathbf \Omega_{k}\mathbf Q^{-1} \tag{2.2} $$ with $$	\mathbf \Omega_{k} = \operatorname{diag}(\omega_{k1},...,\omega_{kn}).$$ Then \begin{align} \sum_{k=1}^{m}\left(\left(\sum_{i=1}^n\sum_{j=1}^n a_{ij}b_{ik} b_{jk}\mathbf E_{ij}^n\right)\otimes \mathbf E_{kk}^{m}\right) &=\sum_{k=1}^{m}\left(\sum_{i=1}^n\omega_{ki}\mathbf Q\mathbf E_{ii}^n \mathbf Q^{-1}\otimes \mathbf E_{kk}^{m}\right)\\[1.5ex] &=(\mathbf Q\otimes \mathbf I_{m})\left[\sum_{k=1}^{m}\sum_{i=1}^n(\mathbf E_{ii}^n\otimes \omega_{ki}\mathbf E_{kk}^{m})\right](\mathbf Q^{-1}\otimes \mathbf I_{m})\\[1.5ex] &=(\mathbf Q\otimes \mathbf I_{m})\left[\sum_{i=1}^n(\mathbf E_{ii}^n\otimes \sum_{k=1}^{m}\omega_{ki}\mathbf E_{kk}^{m})\right](\mathbf Q^{-1}\otimes \mathbf I_{m}) \end{align} so, we can set $$ \mathbf U = \mathbf Q,\quad \mathbf X_{i} =\sum_{k=1}^{m}\omega_{ki}\mathbf E_{kk}^{m},\quad\text{and}\quad \mathbf V = \mathbf Q^{-1}.$$",,"['linear-algebra', 'matrices', 'kronecker-product']"
55,"In a finite dimensional inner product space with $T  L(V)$, show that $\langle u,v\rangle = \langle T(u),T(v)\rangle$ implies $T$ is invertible.","In a finite dimensional inner product space with , show that  implies  is invertible.","T  L(V) \langle u,v\rangle = \langle T(u),T(v)\rangle T","Here is how I've tried to go about it, and I'm curious if it's true or if I'm way off base. T is invertible iff null$(T)=\{0\}$. Let $vV$ and suppose $T(v)=0$. If we can show that $v=0$, then $T$ is invertible. Consider $\langle v,v\rangle = \langle T(v),T(v)\rangle = \langle 0,0 \rangle = 0$, and $\langle v,v\rangle = 0$ iff $v=0$. Does this prove the statement? Or are there any other thoughts or methods? Thanks in advance!","Here is how I've tried to go about it, and I'm curious if it's true or if I'm way off base. T is invertible iff null$(T)=\{0\}$. Let $vV$ and suppose $T(v)=0$. If we can show that $v=0$, then $T$ is invertible. Consider $\langle v,v\rangle = \langle T(v),T(v)\rangle = \langle 0,0 \rangle = 0$, and $\langle v,v\rangle = 0$ iff $v=0$. Does this prove the statement? Or are there any other thoughts or methods? Thanks in advance!",,"['linear-algebra', 'inner-products']"
56,Is kernel density estimation a GMM with uniform mixture weight?,Is kernel density estimation a GMM with uniform mixture weight?,,"recall that for a Gaussian Mixture Model, the density of p(x) (multivariate) is  $$P(x) = \Sigma_{i=1}^{C}\pi(c_i)\mathcal{N}(\mu_i,\Sigma_i)$$ On the other hand, non-parametric density estimation using kernel can be stated as follows, $$P(x)=\frac{1}{N}\Sigma_{i=1}^{N}\frac{1}{(2\pi h^2)^\frac{D}{2}}exp(-\frac{||x-x_n||^2}{2h^2})$$ where N is total number of data we have and D is the dimension we're in, and h is window size. It seems to me that non-parametric estimation is just a GMM with uniformly distributed prior. But I can't find any justification.","recall that for a Gaussian Mixture Model, the density of p(x) (multivariate) is  $$P(x) = \Sigma_{i=1}^{C}\pi(c_i)\mathcal{N}(\mu_i,\Sigma_i)$$ On the other hand, non-parametric density estimation using kernel can be stated as follows, $$P(x)=\frac{1}{N}\Sigma_{i=1}^{N}\frac{1}{(2\pi h^2)^\frac{D}{2}}exp(-\frac{||x-x_n||^2}{2h^2})$$ where N is total number of data we have and D is the dimension we're in, and h is window size. It seems to me that non-parametric estimation is just a GMM with uniformly distributed prior. But I can't find any justification.",,"['linear-algebra', 'probability', 'pattern-recognition']"
57,Why is this set of polynomials linearly dependent?,Why is this set of polynomials linearly dependent?,,"$$1 + 2t+ t^2, 3-9t^2,1 + 4t + 5t^2$$ (A) Linearly dependent       or        (B) Linearly independent The answer is A from the answer key. This is a test review. I don't see that either polynomial is a scalar multiple of any of the other polynomials.  How can I test for linear dep|indep with polynomials?  If these were vectors I would just put them in a matrix and row reduce (or if a square matrix, check the determinant), but I'm not sure what to do here.","$$1 + 2t+ t^2, 3-9t^2,1 + 4t + 5t^2$$ (A) Linearly dependent       or        (B) Linearly independent The answer is A from the answer key. This is a test review. I don't see that either polynomial is a scalar multiple of any of the other polynomials.  How can I test for linear dep|indep with polynomials?  If these were vectors I would just put them in a matrix and row reduce (or if a square matrix, check the determinant), but I'm not sure what to do here.",,['linear-algebra']
58,How can I find all the matrices that commute with this matrix?,How can I find all the matrices that commute with this matrix?,,"I would like to find all the matrices that commute with the following matrix $$A = \begin{pmatrix}2&0&0\\ \:0&2&0\\ \:0&0&3\end{pmatrix}$$ I set $AX = XA$, but still can't find the solutions from the equations.","I would like to find all the matrices that commute with the following matrix $$A = \begin{pmatrix}2&0&0\\ \:0&2&0\\ \:0&0&3\end{pmatrix}$$ I set $AX = XA$, but still can't find the solutions from the equations.",,"['linear-algebra', 'matrices', 'matrix-equations']"
59,What is $\mathbb R^{\mathbb R}$ as a vector space?,What is  as a vector space?,\mathbb R^{\mathbb R},"In Sheldon Axler's Linear Algebra Done Right third edition the following is given as an example of a subspace: The set of differentiable real-valued functions on $\mathbb R$ is a subspace of $\mathbb R^{\mathbb R}$ I'm looking for an intuitive explanation of the statement? Letting $S$ be the set of all differentiable real-valued functions, in order for the statement to be true, $S$ must be a subset of $\mathbb R^{\mathbb R}$(a subspace needs to be a subset). - How can $S \subset \mathbb R^{\mathbb R}$ when $S$ is a set containing functions and  $\mathbb R^{\mathbb R}$ is a set containing real numbers? - What are the elements in $\mathbb R^{\mathbb R}$? How can we think of $  \mathbb R^{\mathbb R}$ as a tuple?","In Sheldon Axler's Linear Algebra Done Right third edition the following is given as an example of a subspace: The set of differentiable real-valued functions on $\mathbb R$ is a subspace of $\mathbb R^{\mathbb R}$ I'm looking for an intuitive explanation of the statement? Letting $S$ be the set of all differentiable real-valued functions, in order for the statement to be true, $S$ must be a subset of $\mathbb R^{\mathbb R}$(a subspace needs to be a subset). - How can $S \subset \mathbb R^{\mathbb R}$ when $S$ is a set containing functions and  $\mathbb R^{\mathbb R}$ is a set containing real numbers? - What are the elements in $\mathbb R^{\mathbb R}$? How can we think of $  \mathbb R^{\mathbb R}$ as a tuple?",,"['linear-algebra', 'vector-spaces']"
60,Does the CayleyHamilton theorem work in the opposite direction?,Does the CayleyHamilton theorem work in the opposite direction?,,"The CayleyHamilton theorem states that every square matrix satisfies its own characteristic equation. But does it work in the opposite direction? If for example for a certain matrix $A$ we know that $ A^2-6A+9I=0, $ does that mean that the characteristic equation of $A$ is $ \lambda^2-6\lambda+9=0 $ ?",The CayleyHamilton theorem states that every square matrix satisfies its own characteristic equation. But does it work in the opposite direction? If for example for a certain matrix we know that does that mean that the characteristic equation of is ?,"A  A^2-6A+9I=0,  A 
\lambda^2-6\lambda+9=0
","['linear-algebra', 'cayley-hamilton']"
61,Can the product of two nonsymmetric matrices be symmetric?,Can the product of two nonsymmetric matrices be symmetric?,,I was wondering if the product of two nonsymmetric matrices can ever be a symmetric matrix. Honestly I would not know how to tackle this problem.,I was wondering if the product of two nonsymmetric matrices can ever be a symmetric matrix. Honestly I would not know how to tackle this problem.,,['linear-algebra']
62,I get a wrong determinant - why?,I get a wrong determinant - why?,,"I'm trying to calculate the following determinant: $$\begin{vmatrix} a_0 & a_1 & a_2 & \dots & a_n \\ a_0 & x & a_2 & \dots & a_n \\ a_0 & a_1 & x & \dots & a_n \\ \dots & \dots & \dots & \dots & \dots \\ a_0 & a_1 & a_2 & \dots & x \end{vmatrix} = $$ $$ = \begin{vmatrix} a_0 & a_1 & a_2 & \dots & a_n \\ a_0 & a_1 & a_2 & \dots & a_n \\ a_0 & a_1 & a_2 & \dots & a_n \\ \dots & \dots & \dots & \dots & \dots \\ a_0 & a_1 & a_2 & \dots & a_n \end{vmatrix} + \begin{vmatrix} 0 & 0 & 0 & \dots & 0 \\ 0 & x - a_1 & 0 & \dots & 0 \\ 0 & 0 & x - a_2 & \dots & 0 \\ \dots & \dots & \dots & \dots & \dots \\ 0 & 0 & 0 & \dots & x-a_n \end{vmatrix} = 0 + 0 = 0 $$ Still, experimental results contradict, since for one example I get a non-zero determinant. What am I doing wrong?","I'm trying to calculate the following determinant: $$\begin{vmatrix} a_0 & a_1 & a_2 & \dots & a_n \\ a_0 & x & a_2 & \dots & a_n \\ a_0 & a_1 & x & \dots & a_n \\ \dots & \dots & \dots & \dots & \dots \\ a_0 & a_1 & a_2 & \dots & x \end{vmatrix} = $$ $$ = \begin{vmatrix} a_0 & a_1 & a_2 & \dots & a_n \\ a_0 & a_1 & a_2 & \dots & a_n \\ a_0 & a_1 & a_2 & \dots & a_n \\ \dots & \dots & \dots & \dots & \dots \\ a_0 & a_1 & a_2 & \dots & a_n \end{vmatrix} + \begin{vmatrix} 0 & 0 & 0 & \dots & 0 \\ 0 & x - a_1 & 0 & \dots & 0 \\ 0 & 0 & x - a_2 & \dots & 0 \\ \dots & \dots & \dots & \dots & \dots \\ 0 & 0 & 0 & \dots & x-a_n \end{vmatrix} = 0 + 0 = 0 $$ Still, experimental results contradict, since for one example I get a non-zero determinant. What am I doing wrong?",,"['linear-algebra', 'determinant', 'numerical-linear-algebra']"
63,Confused about the definition of subspace,Confused about the definition of subspace,,"The definition from my textbook is: A subspace of a vector space is a set of vectors that satisfies   two requirements: If $v$ and $w$ are vectors in the subspace and $c$ is any   scalar, then (1) $v + w$ is in the subspace. (2) $cv$ is in the subspace. And my textbook says vector space $\mathbb{R}^2$ is not a subspace of $\mathbb{R}^3$ but let say $V=\begin{pmatrix}a\\b\end{pmatrix}$ and $W=\begin{pmatrix}c\\d\end{pmatrix}$, so $V$ and $W$ are all in $\mathbb{R}^2$, and clearly $V+W$ is in $\mathbb{R}^2$, $cV$ or $cW$ is in $\mathbb{R}^2$ too, so the first two requirements are met, why we say $\mathbb{R}^2$ is not a subspace of $\mathbb{R}^3$? It looks like we have to add $V$ and $W$ should be in $\mathbb{R}^3$ as well, but why we don't have this in the definition?","The definition from my textbook is: A subspace of a vector space is a set of vectors that satisfies   two requirements: If $v$ and $w$ are vectors in the subspace and $c$ is any   scalar, then (1) $v + w$ is in the subspace. (2) $cv$ is in the subspace. And my textbook says vector space $\mathbb{R}^2$ is not a subspace of $\mathbb{R}^3$ but let say $V=\begin{pmatrix}a\\b\end{pmatrix}$ and $W=\begin{pmatrix}c\\d\end{pmatrix}$, so $V$ and $W$ are all in $\mathbb{R}^2$, and clearly $V+W$ is in $\mathbb{R}^2$, $cV$ or $cW$ is in $\mathbb{R}^2$ too, so the first two requirements are met, why we say $\mathbb{R}^2$ is not a subspace of $\mathbb{R}^3$? It looks like we have to add $V$ and $W$ should be in $\mathbb{R}^3$ as well, but why we don't have this in the definition?",,"['linear-algebra', 'vector-spaces']"
64,$A=\pmatrix{1&2&3&4&5\\2&3&4&5&6}.$ Find $\det(A^TA)$. [duplicate],Find . [duplicate],A=\pmatrix{1&2&3&4&5\\2&3&4&5&6}. \det(A^TA),"This question already has answers here : When A and B are of different order given the $\det(AB)$,then calculate $\det(BA)$ (3 answers) Closed 5 years ago . Suppose $$A=\pmatrix{1&2&3&4&5\\2&3&4&5&6}$$ Find $\det(A^TA)$. I know exactly how to calculate it by writing it as a $5\times5$ matrix. But how to calculate it smartly?","This question already has answers here : When A and B are of different order given the $\det(AB)$,then calculate $\det(BA)$ (3 answers) Closed 5 years ago . Suppose $$A=\pmatrix{1&2&3&4&5\\2&3&4&5&6}$$ Find $\det(A^TA)$. I know exactly how to calculate it by writing it as a $5\times5$ matrix. But how to calculate it smartly?",,"['linear-algebra', 'matrices', 'determinant']"
65,$A+A^2B+B=0$ implies $A^2+I$ invertible?,implies  invertible?,A+A^2B+B=0 A^2+I,Let $A$ and $B$ be two square matrices over a field such that $A+A^2B+B=0$. Is it true that $A^2+I$ is always invertible ?,Let $A$ and $B$ be two square matrices over a field such that $A+A^2B+B=0$. Is it true that $A^2+I$ is always invertible ?,,"['linear-algebra', 'matrices', 'matrix-equations', 'matrix-rank']"
66,Proving functions are linearly independent,Proving functions are linearly independent,,"I'm currently going through Harvard's Abstract Algebra using Michael Artin's book, and have no real way of verifying my proofs, and was hoping to make sure that my proof was right. The question reads: Let $V$ be the vector space of functions on the interval $[0, 1]$. Prove that the functions $x^{3}$, $\sin(x)$, and $\cos(x)$ are linearly independent. My proof goes as follows: For these to be linearly dependent there must exist an $a_{i} \neq0$, where $ i = 1, 2, 3$ such that $$a_{1}x^{3} + a_{2}\sin(x) + a_{3}\cos(x) = 0. $$ So, we'll do this in 3 cases: Case 1: $x = 0$ In this case, $x^{3} = 0$, $\sin(x) = 0$ but $\cos(x) = 1$. So, we have  $$0\times a_{1} + 0\times a_{2} + 1\times a_{3} = 0.$$ So, $a_{1}$ and $a_{2}$ could be anything but $a_{3}$ must be 0. Case 2: $x \in (0,1)$ In this case, $x^{3} \neq 0$, $\sin(x) \neq 0$ and $\cos(x) \neq 0$. So, for this to be true, $a_{1}$, $a_{2}$ and $a_{3}$ all must be $0$. Case 3: $x = 1$ In this case, $x^{3} = 1$, $\sin(x) = .8...$ and $\cos(x) = .5...$. So, we have  $$1\times a_{1} +.8\times a_{2} + .5\times a_{3} = 0.$$ So, $a_{3}$ could be any value, while $a_{1}$ and $a_{2}$ must be $0$. So, if $a_{1} \neq 0$ then we have a problem in Case 3. If $a_{2} \neq 0$ we have a problem in Case 3. If $a_{3} \neq 0$ we have a problem in Case 1. So, we know that all of the $a$ values must be $0$ and we complete the proof.","I'm currently going through Harvard's Abstract Algebra using Michael Artin's book, and have no real way of verifying my proofs, and was hoping to make sure that my proof was right. The question reads: Let $V$ be the vector space of functions on the interval $[0, 1]$. Prove that the functions $x^{3}$, $\sin(x)$, and $\cos(x)$ are linearly independent. My proof goes as follows: For these to be linearly dependent there must exist an $a_{i} \neq0$, where $ i = 1, 2, 3$ such that $$a_{1}x^{3} + a_{2}\sin(x) + a_{3}\cos(x) = 0. $$ So, we'll do this in 3 cases: Case 1: $x = 0$ In this case, $x^{3} = 0$, $\sin(x) = 0$ but $\cos(x) = 1$. So, we have  $$0\times a_{1} + 0\times a_{2} + 1\times a_{3} = 0.$$ So, $a_{1}$ and $a_{2}$ could be anything but $a_{3}$ must be 0. Case 2: $x \in (0,1)$ In this case, $x^{3} \neq 0$, $\sin(x) \neq 0$ and $\cos(x) \neq 0$. So, for this to be true, $a_{1}$, $a_{2}$ and $a_{3}$ all must be $0$. Case 3: $x = 1$ In this case, $x^{3} = 1$, $\sin(x) = .8...$ and $\cos(x) = .5...$. So, we have  $$1\times a_{1} +.8\times a_{2} + .5\times a_{3} = 0.$$ So, $a_{3}$ could be any value, while $a_{1}$ and $a_{2}$ must be $0$. So, if $a_{1} \neq 0$ then we have a problem in Case 3. If $a_{2} \neq 0$ we have a problem in Case 3. If $a_{3} \neq 0$ we have a problem in Case 1. So, we know that all of the $a$ values must be $0$ and we complete the proof.",,"['linear-algebra', 'abstract-algebra', 'proof-verification']"
67,Is the rank of $AB$ always equal to the rank of $BA$?,Is the rank of  always equal to the rank of ?,AB BA,"I know that matrix multiplication is not commutative: $ AB \neq BA$ . Still, it may be possible that $\operatorname {rank} (AB) = \operatorname {rank}  (BA)$ . But it seems to be not true . How can it be proved that $ \operatorname {rank} (AB) \neq \operatorname {rank} (BA)$ ? (Usually it's not a big deal to prove a negative statement since it only take to find one example when the equality is not satisfied.)","I know that matrix multiplication is not commutative: . Still, it may be possible that . But it seems to be not true . How can it be proved that ? (Usually it's not a big deal to prove a negative statement since it only take to find one example when the equality is not satisfied.)", AB \neq BA \operatorname {rank} (AB) = \operatorname {rank}  (BA)  \operatorname {rank} (AB) \neq \operatorname {rank} (BA),"['linear-algebra', 'matrices', 'examples-counterexamples', 'matrix-rank']"
68,Why does the column space of a linear transformation equal its image?,Why does the column space of a linear transformation equal its image?,,I'm having trouble understanding this. Why does the column space of the matrix of a linear transformation equal the image of the linear transformation?,I'm having trouble understanding this. Why does the column space of the matrix of a linear transformation equal the image of the linear transformation?,,[]
69,Constructing two matrices that do not commute,Constructing two matrices that do not commute,,"I need to construct square matrices $A$ and $B$ such that $AB=0$ but $BA \neq 0$. I know matrix multiplication is not commutative, but I don't know how to construct such matrices. Thanks in advance. Edit: looking for some simple way","I need to construct square matrices $A$ and $B$ such that $AB=0$ but $BA \neq 0$. I know matrix multiplication is not commutative, but I don't know how to construct such matrices. Thanks in advance. Edit: looking for some simple way",,"['linear-algebra', 'matrices', 'examples-counterexamples']"
70,Assuming $AB=I$ prove $BA=I$ [duplicate],Assuming  prove  [duplicate],AB=I BA=I,"This question already has answers here : Closed 11 years ago . Possible Duplicate: If $AB = I$ then $BA = I$ Most introductory linear algebra texts define the inverse of a square matrix $A$ as such: Inverse of $A$, if it exists, is a matrix $B$ such that $AB=BA=I$. That definition, in my opinion, is problematic. A few books (in my sample less than 20%) give a different definition: Inverse of $A$, if it exists, is a matrix $B$ such that $AB=I$. Then they go and prove that $BA=I$. Do you know of a proof other than defining inverse through determinants or through using rref ? Is there a general setting in algebra under which $ab=e$  leads to $ba=e$ where $e$ is the identity?","This question already has answers here : Closed 11 years ago . Possible Duplicate: If $AB = I$ then $BA = I$ Most introductory linear algebra texts define the inverse of a square matrix $A$ as such: Inverse of $A$, if it exists, is a matrix $B$ such that $AB=BA=I$. That definition, in my opinion, is problematic. A few books (in my sample less than 20%) give a different definition: Inverse of $A$, if it exists, is a matrix $B$ such that $AB=I$. Then they go and prove that $BA=I$. Do you know of a proof other than defining inverse through determinants or through using rref ? Is there a general setting in algebra under which $ab=e$  leads to $ba=e$ where $e$ is the identity?",,"['linear-algebra', 'abstract-algebra', 'matrices']"
71,"How to prove that $GL(n,\mathbb R)$ is not connected subset and open subset of$M_n (\mathbb{R})$",How to prove that  is not connected subset and open subset of,"GL(n,\mathbb R) M_n (\mathbb{R})","let $n\gt1$  be a fixed natural number, $S:=${$A $: $M_n (\mathbb{R})$ be all real matrix,define this meter for all $A=[a_{ij}]$ $B=[b_{ij}]$ $d(A,B)$:=max{|$a_{ij}-b_{ij}$|:i,j=1,2,2...,n} and $GL(n,\mathbb R)$ is set of all $n\times n$ non singular matrix how to prove that $GL(n,\mathbb R)$ is  not connected and open subset of $S$ ? thanks in advance","let $n\gt1$  be a fixed natural number, $S:=${$A $: $M_n (\mathbb{R})$ be all real matrix,define this meter for all $A=[a_{ij}]$ $B=[b_{ij}]$ $d(A,B)$:=max{|$a_{ij}-b_{ij}$|:i,j=1,2,2...,n} and $GL(n,\mathbb R)$ is set of all $n\times n$ non singular matrix how to prove that $GL(n,\mathbb R)$ is  not connected and open subset of $S$ ? thanks in advance",,"['linear-algebra', 'analysis']"
72,This theorem about matrices of linear maps doesn't look correct.,This theorem about matrices of linear maps doesn't look correct.,,"Consider the following theorem: Theorem. Let $f\colon L\to M$ be a linear mapping of finite-dimensional vector spaces. Then there exist bases in $L$ and $M$ and a natural number $r$ such that the matrix of $f$ in these bases has the form $(a_{ij})$, where $a_{ii}=1$ for $1\leq i\leq r$ and $a_{ij}=0$ for the other values of $i,j$. Furthermore, $r$ is the rank of $f$. This theorem doesn't make much sense to me. Doesn't it imply that, for example, if $L$ and $M$ have the same dimension, every injective linear map can be represented by the identity matrix in some basis? This looks weird. Can you comment on this? It is a theorem in Section 8, Chapter 1 of Kostrikin and Manin's book ""Linear Algebra and Geometry"". Actually, it is not copied word-by-word, but I think that I wrote exactly what they meant.","Consider the following theorem: Theorem. Let $f\colon L\to M$ be a linear mapping of finite-dimensional vector spaces. Then there exist bases in $L$ and $M$ and a natural number $r$ such that the matrix of $f$ in these bases has the form $(a_{ij})$, where $a_{ii}=1$ for $1\leq i\leq r$ and $a_{ij}=0$ for the other values of $i,j$. Furthermore, $r$ is the rank of $f$. This theorem doesn't make much sense to me. Doesn't it imply that, for example, if $L$ and $M$ have the same dimension, every injective linear map can be represented by the identity matrix in some basis? This looks weird. Can you comment on this? It is a theorem in Section 8, Chapter 1 of Kostrikin and Manin's book ""Linear Algebra and Geometry"". Actually, it is not copied word-by-word, but I think that I wrote exactly what they meant.",,"['linear-algebra', 'matrices', 'linear-transformations']"
73,Finding Intersection of an ellipse with another ellipse when both are rotated,Finding Intersection of an ellipse with another ellipse when both are rotated,,Equation of first ellipse=> $$\dfrac {((x-xFirstEllipseCenterPoint)\cdot \cos(A)+(y-yFirstEllipseCenterPoint)\cdot \sin(A))^2}{(a_1^2)}+\dfrac{((x-xFirstEllipseCenterPoint)\cdot \sin(A)-(y-yFirstEllipseCenterPoint)\cdot \cos(A))^2}{(b_1^2)}=1$$ Equation of the second ellipse=> $$\dfrac {((x-xSecondEllipseCenterPoint)\cdot \cos(B)+(y-ySecondEllipseCenterPoint)\cdot \sin(B))^2}{(a_2^2)}+\dfrac{((x-xSecondEllipseCenterPoint)\cdot \sin(B)-(y-ySecondEllipseCenterPoint)\cdot \cos(B))^2}{(b_2^2)}=1$$ I know that the ellipse will intersect at One Point Two Point Three Point Four Point No intersection at all Is there a general set of equation to solve the same.,Equation of first ellipse=> $$\dfrac {((x-xFirstEllipseCenterPoint)\cdot \cos(A)+(y-yFirstEllipseCenterPoint)\cdot \sin(A))^2}{(a_1^2)}+\dfrac{((x-xFirstEllipseCenterPoint)\cdot \sin(A)-(y-yFirstEllipseCenterPoint)\cdot \cos(A))^2}{(b_1^2)}=1$$ Equation of the second ellipse=> $$\dfrac {((x-xSecondEllipseCenterPoint)\cdot \cos(B)+(y-ySecondEllipseCenterPoint)\cdot \sin(B))^2}{(a_2^2)}+\dfrac{((x-xSecondEllipseCenterPoint)\cdot \sin(B)-(y-ySecondEllipseCenterPoint)\cdot \cos(B))^2}{(b_2^2)}=1$$ I know that the ellipse will intersect at One Point Two Point Three Point Four Point No intersection at all Is there a general set of equation to solve the same.,,"['linear-algebra', 'geometry', 'conic-sections']"
74,What is a short exact sequence?,What is a short exact sequence?,,"I'll just quote my book here so you can see the definitions I have: Suppose that you are given a sequence of vector spaces $V_i$ and linear maps $\varphi_i: V_i\to V_{i+1}$ connecting them, as illustrated below: $$\cdots \longrightarrow V_{i-1} \stackrel{\varphi_{i-1}}{\longrightarrow} V_i \stackrel{\varphi_{i}}{\longrightarrow} V_{i+1} \stackrel{\varphi_{i+1}}{\longrightarrow} \cdots$$ The maps are said to be exact at V_i if $\operatorname{im} \varphi_{i-1} = \operatorname{ker}\varphi_i$.  The sequence is called an exact sequence if the maps are exact at $V_i$ for all $i$.  $\dots$ If $V_1, V_2$ and $V_3$ are three vector spaces, and if the sequence $$0 \stackrel{\varphi_0}{\longrightarrow} V_{1} \stackrel{\varphi_{1}}{\longrightarrow} V_2 \stackrel{\varphi_{2}}{\longrightarrow} V_{3} \stackrel{\varphi_{3}}{\longrightarrow} 0 \tag{1.7}$$ is exact, it is called a short exact sequence .  In this diagram ""$0$"" represents the zero-dimensional vector space. OK, here's what I'm not understanding.  If the image of any function in this sequence is the kernel of the next function, doesn't every step of this just map to $0$?  And even if it didn't, because we're starting with the $0$ vector space, everything has to map to $0$ because linear transformations always map $0$ to $0$.  So I'm not understanding this definition at all.  The first exercise right below these definitions is to show that equation $(1.7)$ implies that $\varphi_1$ is injective and $\varphi_2$ is surjective.  But all I'm seeing here is a chain of functions mapping zero to zero.  Can someone explain what I'm missing here?","I'll just quote my book here so you can see the definitions I have: Suppose that you are given a sequence of vector spaces $V_i$ and linear maps $\varphi_i: V_i\to V_{i+1}$ connecting them, as illustrated below: $$\cdots \longrightarrow V_{i-1} \stackrel{\varphi_{i-1}}{\longrightarrow} V_i \stackrel{\varphi_{i}}{\longrightarrow} V_{i+1} \stackrel{\varphi_{i+1}}{\longrightarrow} \cdots$$ The maps are said to be exact at V_i if $\operatorname{im} \varphi_{i-1} = \operatorname{ker}\varphi_i$.  The sequence is called an exact sequence if the maps are exact at $V_i$ for all $i$.  $\dots$ If $V_1, V_2$ and $V_3$ are three vector spaces, and if the sequence $$0 \stackrel{\varphi_0}{\longrightarrow} V_{1} \stackrel{\varphi_{1}}{\longrightarrow} V_2 \stackrel{\varphi_{2}}{\longrightarrow} V_{3} \stackrel{\varphi_{3}}{\longrightarrow} 0 \tag{1.7}$$ is exact, it is called a short exact sequence .  In this diagram ""$0$"" represents the zero-dimensional vector space. OK, here's what I'm not understanding.  If the image of any function in this sequence is the kernel of the next function, doesn't every step of this just map to $0$?  And even if it didn't, because we're starting with the $0$ vector space, everything has to map to $0$ because linear transformations always map $0$ to $0$.  So I'm not understanding this definition at all.  The first exercise right below these definitions is to show that equation $(1.7)$ implies that $\varphi_1$ is injective and $\varphi_2$ is surjective.  But all I'm seeing here is a chain of functions mapping zero to zero.  Can someone explain what I'm missing here?",,"['linear-algebra', 'exact-sequence']"
75,how to solve system of linear equations of XOR operation?,how to solve system of linear equations of XOR operation?,,"how can i solve this set of equations ? to get values of $x,y,z,w$ ? $$\begin{aligned} 1=x \oplus y \oplus z \end{aligned}$$ $$\begin{aligned}1=x \oplus y \oplus w \end{aligned}$$ $$\begin{aligned}0=x \oplus w \oplus z \end{aligned}$$ $$\begin{aligned}1=w \oplus y \oplus z \end{aligned}$$ this is not a real example, the variables don't have to make sense, i just want to know the method.","how can i solve this set of equations ? to get values of $x,y,z,w$ ? $$\begin{aligned} 1=x \oplus y \oplus z \end{aligned}$$ $$\begin{aligned}1=x \oplus y \oplus w \end{aligned}$$ $$\begin{aligned}0=x \oplus w \oplus z \end{aligned}$$ $$\begin{aligned}1=w \oplus y \oplus z \end{aligned}$$ this is not a real example, the variables don't have to make sense, i just want to know the method.",,"['linear-algebra', 'boolean-algebra']"
76,Is every self-inverse matrix diagonalizable?,Is every self-inverse matrix diagonalizable?,,"If $A=A^{-1}$, is there always a matrix C such that $C^{-1}AC$ is a diagonal  matrix (containing only -1 and 1 in the main diagonal) ? How can I check with PARI/GP, if a given matrix is diagonalizable ? I only found out that $A=A^{-1}$ implies $C^{-1}AC=(C^{-1}AC)^{-1}$ for any  invertible C, but this does not answer my question. Additional question : If A has integer entries, is there always a matrix C with  INTEGER values such that $C^{-1}AC$ is a diagoal matrix and $C^{-1}$ has also integer values ? For example, I did not find such a matrix for   A = [ [11,-16,-4] [9,-13,-3] [-6,8,1] ] yet.","If $A=A^{-1}$, is there always a matrix C such that $C^{-1}AC$ is a diagonal  matrix (containing only -1 and 1 in the main diagonal) ? How can I check with PARI/GP, if a given matrix is diagonalizable ? I only found out that $A=A^{-1}$ implies $C^{-1}AC=(C^{-1}AC)^{-1}$ for any  invertible C, but this does not answer my question. Additional question : If A has integer entries, is there always a matrix C with  INTEGER values such that $C^{-1}AC$ is a diagoal matrix and $C^{-1}$ has also integer values ? For example, I did not find such a matrix for   A = [ [11,-16,-4] [9,-13,-3] [-6,8,1] ] yet.",,"['linear-algebra', 'matrices', 'inverse', 'diagonalization']"
77,Why the transpose of a singular matrix is singular? [closed],Why the transpose of a singular matrix is singular? [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question I have to prove this lemma without using the concept of rank neither the concept of determinant: $A$ is a singular matrix iff $A^T$ is singular Unfortunately i've only found proofs that contains rank and determinant. Can you help me ?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question I have to prove this lemma without using the concept of rank neither the concept of determinant: $A$ is a singular matrix iff $A^T$ is singular Unfortunately i've only found proofs that contains rank and determinant. Can you help me ?",,['linear-algebra']
78,Short proof for the determinant of a $4$ by $4$ matrix,Short proof for the determinant of a  by  matrix,4 4,"Prove that $\det \begin{bmatrix}x&y&z&t\\-y&x&-t&z\\-z&t&x&-y\\-t&-z&y&x\end{bmatrix} = (x^2+y^2+z^2+t^2)^2$ I'm looking for an elegant proof that doesn't involve bruteforce. Since the answer is given, I'm thinking we can argue that the determinant here is a homogeneous polynomial $P(x,y,z,t)$ with degree $4$, that is invariant under $x\to -x$ and permutations of $x,y,z,t$. As a result, $P(x,y,z,t) = \lambda (x^4+y^4+z^4+t^4) + \delta (x^2y^2+x^2z^2 + x^2t^2+y^2z^2 + y^2t^2 + z^2t^2)$ $\lambda$ and $\delta$ can be found by computing $P(0,0,0,1)$ or some such. The problem is, it doesn't look easy to prove that $P$ doesn't change under permutation of $x,y,z,t$, neither that it's invariant when the variables are negated. Can you suggest another short proof, or prove the two claims above ?","Prove that $\det \begin{bmatrix}x&y&z&t\\-y&x&-t&z\\-z&t&x&-y\\-t&-z&y&x\end{bmatrix} = (x^2+y^2+z^2+t^2)^2$ I'm looking for an elegant proof that doesn't involve bruteforce. Since the answer is given, I'm thinking we can argue that the determinant here is a homogeneous polynomial $P(x,y,z,t)$ with degree $4$, that is invariant under $x\to -x$ and permutations of $x,y,z,t$. As a result, $P(x,y,z,t) = \lambda (x^4+y^4+z^4+t^4) + \delta (x^2y^2+x^2z^2 + x^2t^2+y^2z^2 + y^2t^2 + z^2t^2)$ $\lambda$ and $\delta$ can be found by computing $P(0,0,0,1)$ or some such. The problem is, it doesn't look easy to prove that $P$ doesn't change under permutation of $x,y,z,t$, neither that it's invariant when the variables are negated. Can you suggest another short proof, or prove the two claims above ?",,"['linear-algebra', 'polynomials', 'proof-writing', 'determinant', 'alternative-proof']"
79,How to prove that two non-zero linear functionals defined on the same vector space and having the same null-space are proportional?,How to prove that two non-zero linear functionals defined on the same vector space and having the same null-space are proportional?,,Let $f$ and $g$ be two non-zero linear functionals defined on a vector space $X$ such that the null-space of $f$ is equal to that of $g$. How to prove that $f$ and $g$ are proportional (i.e. one is a scalar multiple of the other)?,Let $f$ and $g$ be two non-zero linear functionals defined on a vector space $X$ such that the null-space of $f$ is equal to that of $g$. How to prove that $f$ and $g$ are proportional (i.e. one is a scalar multiple of the other)?,,"['linear-algebra', 'functional-analysis', 'vector-spaces', 'operator-theory']"
80,Cayley-Hamilton theorem on square matrices,Cayley-Hamilton theorem on square matrices,,"Can anyone help me by giving the proof of the Cayley-Hamilton theorem? It states that every square matrix $A$ satisfies its own characteristic equation: $$p_{A}(A) = 0$$ I could prove it when $A$ has distinct eigenvalues, because then it will have $n$ linearly independent eigenvectors, but I couldn't prove the general case.","Can anyone help me by giving the proof of the Cayley-Hamilton theorem? It states that every square matrix satisfies its own characteristic equation: I could prove it when has distinct eigenvalues, because then it will have linearly independent eigenvectors, but I couldn't prove the general case.",A p_{A}(A) = 0 A n,"['linear-algebra', 'matrices', 'cayley-hamilton']"
81,The Importance of Minors,The Importance of Minors,,"Let $A \in K^{m \times n}$ be a matrix. A $r \times r$-minor is defined as the determinant of the $r \times r$-matrix formed by $r$ rows and $r$ columns of the original matrix. In a Linear Algebra lecture, my instructor told us that the notion of a minor is very vast and could alone be the subject of a whole course. We've then only introduced a possibility to find $\text{rank}(A)$ using minors. What else is there that makes minors cool/interesting?","Let $A \in K^{m \times n}$ be a matrix. A $r \times r$-minor is defined as the determinant of the $r \times r$-matrix formed by $r$ rows and $r$ columns of the original matrix. In a Linear Algebra lecture, my instructor told us that the notion of a minor is very vast and could alone be the subject of a whole course. We've then only introduced a possibility to find $\text{rank}(A)$ using minors. What else is there that makes minors cool/interesting?",,"['linear-algebra', 'determinant']"
82,How can I visualize independent and dependent set of vectors? [closed],How can I visualize independent and dependent set of vectors? [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Can someone help me visualize those concepts? It will also help me understand it better. Thanks :)","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Can someone help me visualize those concepts? It will also help me understand it better. Thanks :)",,['linear-algebra']
83,Why do complex eigenvalues correspond to a rotation of the vector?,Why do complex eigenvalues correspond to a rotation of the vector?,,"We have a linear transformation $T: \Bbb R^m \to \Bbb R^n$ defined by $T(x)=Ax$ for $x \in \Bbb R^m$ and $A \in M_{n \times m}(\Bbb R)$.  I understand why real-valued eigenvalues of $A$ correspond to scaling the length of the associated eigenvectors, but why is it that complex eigenvalues are said to rotate the eigenvector? If you have an eigenvector $x = (x_1, x_2, x_3)$ whose eigenvalue is $\lambda = a+bi$, how is $\lambda x = ((a+bi)x_1, (a+bi)x_2, (a+bi)x_3)$ a rotation of $x$?  Shouldn't a rotation just be something like $\sin$'s and $\cos$'s multiplied by each component?  Where do the imaginary parts fit in? Thanks.","We have a linear transformation $T: \Bbb R^m \to \Bbb R^n$ defined by $T(x)=Ax$ for $x \in \Bbb R^m$ and $A \in M_{n \times m}(\Bbb R)$.  I understand why real-valued eigenvalues of $A$ correspond to scaling the length of the associated eigenvectors, but why is it that complex eigenvalues are said to rotate the eigenvector? If you have an eigenvector $x = (x_1, x_2, x_3)$ whose eigenvalue is $\lambda = a+bi$, how is $\lambda x = ((a+bi)x_1, (a+bi)x_2, (a+bi)x_3)$ a rotation of $x$?  Shouldn't a rotation just be something like $\sin$'s and $\cos$'s multiplied by each component?  Where do the imaginary parts fit in? Thanks.",,['linear-algebra']
84,A faster way of calculating this determinant?,A faster way of calculating this determinant?,,"I'm doing a problem involving Cramer's rule, and one of the determinants I have to work with is as follows: \begin{vmatrix} 1&1&1\\ a&b&c\\ a^3&b^3&c^3 \end{vmatrix} So I started off by getting the matrix to a triangular matrix so I can just take the product of the diagonal cells by doing this: \begin{equation} R_1 \times-c + R_2 \end{equation} \begin{equation} R_3 \times -\frac{1}{c^3} + R_1 \end{equation} \begin{equation} R_2 \times(\frac{1-\frac{b^3}{c^3}}{b-c})+R_1 \end{equation} I then got this matrix \begin{vmatrix} ((1-\frac{a^3}{c^3})-(a-c)(\frac{1-\frac{b^3}{c^3}}{b-c}))&0&0\\ a-c&b-c&0\\ a^3&b^3&c^3\\ \end{vmatrix} By summing the diagonal cells, I got this as a final answer: \begin{equation} (c-a)(a-b)(b-c)(a+b+c) \end{equation} However, it was a lengthy process, and I can't help but thinking this is not the type of calculations I can afford the time in a written exam, especially if I have to repeat this four times solving three linear equations with Cramer's rule. I'd really appreciate it if you have any thoughts as to speed this process up. Just as an added detail, the original question was (translated from Chinese): With regards to the following set of equations \begin{equation} x + y + z = 1\\ ax + by + cz = d\\ a^3x + b^3y + c^3z = d^3 \end{equation} Under what conditions can Cramer's rule be used? Please solve the set of equations with Cramer's rule. For the first question, the immediate thought that pops up is that \begin{equation} det(A)  0 \end{equation} For the second question, it's simply \begin{equation} \frac{\begin{vmatrix}A_1\end{vmatrix}}{\begin{vmatrix}A\end{vmatrix}}, \frac{\begin{vmatrix}A_2\end{vmatrix}}{\begin{vmatrix}A\end{vmatrix}}, \frac{\begin{vmatrix}A_3\end{vmatrix}}{\begin{vmatrix}A\end{vmatrix}}  \end{equation} which is the part I'm having trouble solving quickly. Again, appreciate any hints or thoughts on this. Thanks.","I'm doing a problem involving Cramer's rule, and one of the determinants I have to work with is as follows: \begin{vmatrix} 1&1&1\\ a&b&c\\ a^3&b^3&c^3 \end{vmatrix} So I started off by getting the matrix to a triangular matrix so I can just take the product of the diagonal cells by doing this: \begin{equation} R_1 \times-c + R_2 \end{equation} \begin{equation} R_3 \times -\frac{1}{c^3} + R_1 \end{equation} \begin{equation} R_2 \times(\frac{1-\frac{b^3}{c^3}}{b-c})+R_1 \end{equation} I then got this matrix \begin{vmatrix} ((1-\frac{a^3}{c^3})-(a-c)(\frac{1-\frac{b^3}{c^3}}{b-c}))&0&0\\ a-c&b-c&0\\ a^3&b^3&c^3\\ \end{vmatrix} By summing the diagonal cells, I got this as a final answer: \begin{equation} (c-a)(a-b)(b-c)(a+b+c) \end{equation} However, it was a lengthy process, and I can't help but thinking this is not the type of calculations I can afford the time in a written exam, especially if I have to repeat this four times solving three linear equations with Cramer's rule. I'd really appreciate it if you have any thoughts as to speed this process up. Just as an added detail, the original question was (translated from Chinese): With regards to the following set of equations \begin{equation} x + y + z = 1\\ ax + by + cz = d\\ a^3x + b^3y + c^3z = d^3 \end{equation} Under what conditions can Cramer's rule be used? Please solve the set of equations with Cramer's rule. For the first question, the immediate thought that pops up is that \begin{equation} det(A)  0 \end{equation} For the second question, it's simply \begin{equation} \frac{\begin{vmatrix}A_1\end{vmatrix}}{\begin{vmatrix}A\end{vmatrix}}, \frac{\begin{vmatrix}A_2\end{vmatrix}}{\begin{vmatrix}A\end{vmatrix}}, \frac{\begin{vmatrix}A_3\end{vmatrix}}{\begin{vmatrix}A\end{vmatrix}}  \end{equation} which is the part I'm having trouble solving quickly. Again, appreciate any hints or thoughts on this. Thanks.",,"['linear-algebra', 'determinant']"
85,Determinant of a Special Symmetric Matrix,Determinant of a Special Symmetric Matrix,,"If $A$ is a symmatric matrix of odd order with integer entries and the diagonal entries $0$ then $A$ has determinant value even. I can prove the result if I can show that the eigenvalues of $A$ are integers,but I am unable to do that. Thanks for any help.","If $A$ is a symmatric matrix of odd order with integer entries and the diagonal entries $0$ then $A$ has determinant value even. I can prove the result if I can show that the eigenvalues of $A$ are integers,but I am unable to do that. Thanks for any help.",,"['linear-algebra', 'matrices', 'determinant']"
86,"What are the technical reasons that we must define vectors as ""arrows"" and carefully distinguish them from a point?","What are the technical reasons that we must define vectors as ""arrows"" and carefully distinguish them from a point?",,"I thought I'd bring this question to math.SE, as it could spark some interesting discussion. When I first learned vectors - a long time ago and in high school - the textbook and teachers would always introduce them as a ""quantity with both a magnitude and direction"" This definition always seemed to irk me. It seems to favor the ""polar"" definition of a vector and then teach ""but it has Cartesian components [x_0, x_1 ... x_n] like so"" I felt that I reached a personal breakthrough when I realized that ""A vector is a higher dimensional generalization of a 'number' or 'value' or 'quantity' "" This has been how I've always thought of them. For example, the number five can be thought of as a one dimensional vector - <5>. < . . . . . 0---------> . . .>             0         5 Just like the vector <5,5> is a two dimensional analog of this notion. It just so happens that the circumstances change in subtle ways - (the ability to have a 'distance' or 'absolute value' different from either of the components and the notion of a direction when graphed). Also, it seems that every teacher is very very careful to distinguish a vector from a point. However, this seems trivial in concept, as we wouldn't distinguish ""the position '5' on a number line"" from ""the value 5"" or ""distance from zero to 5"" Why is the 'magnitude and direction' definition favored?","I thought I'd bring this question to math.SE, as it could spark some interesting discussion. When I first learned vectors - a long time ago and in high school - the textbook and teachers would always introduce them as a ""quantity with both a magnitude and direction"" This definition always seemed to irk me. It seems to favor the ""polar"" definition of a vector and then teach ""but it has Cartesian components [x_0, x_1 ... x_n] like so"" I felt that I reached a personal breakthrough when I realized that ""A vector is a higher dimensional generalization of a 'number' or 'value' or 'quantity' "" This has been how I've always thought of them. For example, the number five can be thought of as a one dimensional vector - <5>. < . . . . . 0---------> . . .>             0         5 Just like the vector <5,5> is a two dimensional analog of this notion. It just so happens that the circumstances change in subtle ways - (the ability to have a 'distance' or 'absolute value' different from either of the components and the notion of a direction when graphed). Also, it seems that every teacher is very very careful to distinguish a vector from a point. However, this seems trivial in concept, as we wouldn't distinguish ""the position '5' on a number line"" from ""the value 5"" or ""distance from zero to 5"" Why is the 'magnitude and direction' definition favored?",,"['linear-algebra', 'definition', 'education']"
87,linear transformation $T$ such that $TS = ST$,linear transformation  such that,T TS = ST,Let $V$ be a finite-dimensional vector space over $F$. Let $T:V \rightarrow V$ be a linear transformation such that $ST=TS$ for all linear transformations $S:V \rightarrow V$. Show that $T = cI_v$ for some $c \in F$.,Let $V$ be a finite-dimensional vector space over $F$. Let $T:V \rightarrow V$ be a linear transformation such that $ST=TS$ for all linear transformations $S:V \rightarrow V$. Show that $T = cI_v$ for some $c \in F$.,,['linear-algebra']
88,What does it mean to say that a linear transformation *is* the change of basis matrix?,What does it mean to say that a linear transformation *is* the change of basis matrix?,,"I wish to check my understanding on part of the proof of Proposition 5.3 in Lee's Introduction to Smooth Manifold . It reads as follows: $\def\tE {\widetilde{E}}$ Let $(E_i)$ and $(\tE_i)$ be two ordered bases for a finite-dimensional vector space $V$ and let $B:V\to V$ be the linear map sending $E_j$ to $\tE_j$ . This means that $$\tE_j = BE_j = \sum_iB_i^j E_i,$$ so $B$ is the transition matrix between the two bases. To me it does not even make sense to write $B$ in matrix form before stating on which basis such matrix form is being written. What is basis independent is the transition matrix $A^j_i$ between the two bases. So, I think, what the author means is that, in the basis $(E_i)$ , the map $B$ has matrix matrix form $A_i^j$ . Right?","I wish to check my understanding on part of the proof of Proposition 5.3 in Lee's Introduction to Smooth Manifold . It reads as follows: Let and be two ordered bases for a finite-dimensional vector space and let be the linear map sending to . This means that so is the transition matrix between the two bases. To me it does not even make sense to write in matrix form before stating on which basis such matrix form is being written. What is basis independent is the transition matrix between the two bases. So, I think, what the author means is that, in the basis , the map has matrix matrix form . Right?","\def\tE {\widetilde{E}} (E_i) (\tE_i) V B:V\to V E_j \tE_j \tE_j = BE_j = \sum_iB_i^j E_i, B B A^j_i (E_i) B A_i^j","['linear-algebra', 'linear-transformations', 'change-of-basis']"
89,"Proof explanation of result 5.11 in Linear Algebra Done Right, Sheldon Axler","Proof explanation of result 5.11 in Linear Algebra Done Right, Sheldon Axler",,"The below result is from Linear Algebra Done Right, 4th edition, Sheldon Axler: 5.11 linearly independent eigenvectors Suppose $T \in \mathcal{L}(V)$ . Then every list of eigenvectors of $T$ corresponding to distinct eigenvalues of $T$ is linearly independent. Firstly, cool result. Secondly, I'm having trouble understanding one part of Axler's proof. He does a proof by contradiction. Since he assumes the result is false, he states: ... there exists a smallest positive integer $m$ such that there exists a linearly dependent list $v_1, \ldots, v_m$ of eigenvectors of $T$ corresponding to distinct eigenvalues $\lambda_1, \ldots, \lambda_m$ of T (note that $m \geq 2$ because an eigenvector is, by definition, nonzero). My question is why must there specifically exist a smallest positive integer $m$ ? Is that referring to the note in the parenthesis, i.e., $m$ cannot be smaller than $2$ ? This part is integral to the proof, so I must know.","The below result is from Linear Algebra Done Right, 4th edition, Sheldon Axler: 5.11 linearly independent eigenvectors Suppose . Then every list of eigenvectors of corresponding to distinct eigenvalues of is linearly independent. Firstly, cool result. Secondly, I'm having trouble understanding one part of Axler's proof. He does a proof by contradiction. Since he assumes the result is false, he states: ... there exists a smallest positive integer such that there exists a linearly dependent list of eigenvectors of corresponding to distinct eigenvalues of T (note that because an eigenvector is, by definition, nonzero). My question is why must there specifically exist a smallest positive integer ? Is that referring to the note in the parenthesis, i.e., cannot be smaller than ? This part is integral to the proof, so I must know.","T \in \mathcal{L}(V) T T m v_1, \ldots, v_m T \lambda_1, \ldots,
\lambda_m m \geq 2 m m 2","['linear-algebra', 'eigenvalues-eigenvectors', 'proof-explanation', 'linear-transformations']"
90,Prove that matrix can be square of matrix with real entries,Prove that matrix can be square of matrix with real entries,,Prove that matrix \begin{bmatrix}1&0&0\\0&-1&0\\0&0&-1\end{bmatrix} can be square of matrix with all real entries. I have found one such matrix to be \begin{bmatrix}1&0&0\\0&1&-1\\0&2&-1\end{bmatrix} but is there an elegant way to do it without any trial and error?,Prove that matrix \begin{bmatrix}1&0&0\\0&-1&0\\0&0&-1\end{bmatrix} can be square of matrix with all real entries. I have found one such matrix to be \begin{bmatrix}1&0&0\\0&1&-1\\0&2&-1\end{bmatrix} but is there an elegant way to do it without any trial and error?,,"['linear-algebra', 'matrices', 'matrix-equations']"
91,Let $A$ and $B$ be $n\times n$ matrices. Suppose $A$ is similar to $B$. Prove trace($A$) = trace($B$).,Let  and  be  matrices. Suppose  is similar to . Prove trace() = trace().,A B n\times n A B A B,"Let $A$ and $B$ be $n\times n$ matrices. Suppose $A$ is similar to $B$. Prove $\operatorname{trace}(A) = \operatorname{trace}(B)$. I'm not sure where to go on this. So far I have this: If $A$ is similar to $B$, then $B=P^{-1}AP$ and $A=PBP^{-1}$ This implies that: $\operatorname{trace}(B) = \operatorname{trace}(P^{-1}AP)$ and $\operatorname{trace}(A) = \operatorname{trace}(PBP^{-1})$ Not sure where to go from here","Let $A$ and $B$ be $n\times n$ matrices. Suppose $A$ is similar to $B$. Prove $\operatorname{trace}(A) = \operatorname{trace}(B)$. I'm not sure where to go on this. So far I have this: If $A$ is similar to $B$, then $B=P^{-1}AP$ and $A=PBP^{-1}$ This implies that: $\operatorname{trace}(B) = \operatorname{trace}(P^{-1}AP)$ and $\operatorname{trace}(A) = \operatorname{trace}(PBP^{-1})$ Not sure where to go from here",,"['linear-algebra', 'matrices']"
92,Geometric meaning of the rising and the lowering of indices,Geometric meaning of the rising and the lowering of indices,,"I'm an undergraduate student and currently I'm approaching tensorial calculus. I was wondering: is there some geometric meaning to the operation of rising/lowering indices (and then if there was any geometric difference between vectors and covectors), or are they only mere formal operations? In case, why don't we simply use only vectors in the definition of tensors?","I'm an undergraduate student and currently I'm approaching tensorial calculus. I was wondering: is there some geometric meaning to the operation of rising/lowering indices (and then if there was any geometric difference between vectors and covectors), or are they only mere formal operations? In case, why don't we simply use only vectors in the definition of tensors?",,"['linear-algebra', 'vectors', 'manifolds', 'tensors', 'differential-forms']"
93,Inequality for determinant of sum of two orthogonal matrices,Inequality for determinant of sum of two orthogonal matrices,,"My goal is to show that : $$\forall A,B\in O_n(\mathbb{R}), |{\rm det}(A+B)| \le 2^n.$$ How can I access the determinant of the sum of two matrices ? I dont know many ways to establish inequalities in Algebra, except with Bessels inequality and Cauchy Schwarz inequality, but they dont seem to be very helpful here.","My goal is to show that : How can I access the determinant of the sum of two matrices ? I dont know many ways to establish inequalities in Algebra, except with Bessels inequality and Cauchy Schwarz inequality, but they dont seem to be very helpful here.","\forall A,B\in O_n(\mathbb{R}), |{\rm det}(A+B)| \le 2^n.","['linear-algebra', 'inequality', 'determinant', 'orthogonal-matrices']"
94,Dimension of a subspace defined by multiple restrictions.,Dimension of a subspace defined by multiple restrictions.,,"Let $$V=\{(x_1,x_2,x_3,\dots,x_{100})\in\mathbb{R}^{100}\,|\, x_1=x_2=x_3 \text{ and } x_{51}=x_{52}=x_{53}= \dots=x_{100}\}$$ What is $\dim V$? If W is a subspace of vector space $V$ then $$\dim W = \dim V - \text{number of linearly independent restrictions}$$ In our case $\dim V= 100$ and the number of linearly independent restriction is $2$. Therefore $\dim W = 100-2=98$. Am I correct?? I think not. Please help.","Let $$V=\{(x_1,x_2,x_3,\dots,x_{100})\in\mathbb{R}^{100}\,|\, x_1=x_2=x_3 \text{ and } x_{51}=x_{52}=x_{53}= \dots=x_{100}\}$$ What is $\dim V$? If W is a subspace of vector space $V$ then $$\dim W = \dim V - \text{number of linearly independent restrictions}$$ In our case $\dim V= 100$ and the number of linearly independent restriction is $2$. Therefore $\dim W = 100-2=98$. Am I correct?? I think not. Please help.",,"['linear-algebra', 'vector-spaces']"
95,New proof about normal matrix is diagonalizable.,New proof about normal matrix is diagonalizable.,,I try to prove normal matrix is diagonalizable. I found that $A^*A$ is hermitian matrix. I know that hermitian matrix is diagonalizable. I can not go more. I want to prove statement use only this fact. I need you help. (professor said that we can prove only use this fact.,I try to prove normal matrix is diagonalizable. I found that $A^*A$ is hermitian matrix. I know that hermitian matrix is diagonalizable. I can not go more. I want to prove statement use only this fact. I need you help. (professor said that we can prove only use this fact.,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'diagonalization', 'matrix-decomposition']"
96,A square matrix has the same minimal polynomial over its base field as it has over an extension field,A square matrix has the same minimal polynomial over its base field as it has over an extension field,,"I think I have heard that the following is true before, but I don't know how to prove it: Let $A$ be a matrix with real entries. Then the minimal polynomial of $A$ over $\mathbb{C}$ is the same as the minimal polynomial of $A$ over $\mathbb{R}$. Is this true? Would anyone be willing to provide a proof? Attempt at a proof: Let $M(t)$ be the minimal polynomial over the reals, and $P(t)$ over the complex numbers. We can look at $M$ as a polynomial over $\Bbb C$, in which case it will fulfil $M(A)=0$, and therefore $P(t)$ divides it. In addition, we can look at $P(t)$ as the sum of two polynomials: $R(t)+iK(t)$. Plugging $A$ we get that $R(A)+iK(A)=P(A)=0$, but this forces both $R(A)=0$ and $K(A)=0$. Looking at both $K$ and $R$ as real polynomials, we get that $M(t)$ divides them both, and therefore divides $R+iK=P$. Now $M$ and $P$ are monic polynomials, and they divide each other, therefore $M=P$. Does this look to be correct? More generally, one might prove the following Let $A$ be any square matrix with entries in a field$~K$, and let $F$ be an extension field of$~K$. Then the minimal polynomial of$~A$ over$~F$ is the same as the minimal polynomial of $A$ over$~K$.","I think I have heard that the following is true before, but I don't know how to prove it: Let $A$ be a matrix with real entries. Then the minimal polynomial of $A$ over $\mathbb{C}$ is the same as the minimal polynomial of $A$ over $\mathbb{R}$. Is this true? Would anyone be willing to provide a proof? Attempt at a proof: Let $M(t)$ be the minimal polynomial over the reals, and $P(t)$ over the complex numbers. We can look at $M$ as a polynomial over $\Bbb C$, in which case it will fulfil $M(A)=0$, and therefore $P(t)$ divides it. In addition, we can look at $P(t)$ as the sum of two polynomials: $R(t)+iK(t)$. Plugging $A$ we get that $R(A)+iK(A)=P(A)=0$, but this forces both $R(A)=0$ and $K(A)=0$. Looking at both $K$ and $R$ as real polynomials, we get that $M(t)$ divides them both, and therefore divides $R+iK=P$. Now $M$ and $P$ are monic polynomials, and they divide each other, therefore $M=P$. Does this look to be correct? More generally, one might prove the following Let $A$ be any square matrix with entries in a field$~K$, and let $F$ be an extension field of$~K$. Then the minimal polynomial of$~A$ over$~F$ is the same as the minimal polynomial of $A$ over$~K$.",,['linear-algebra']
97,Can you transpose a matrix using matrix multiplication?,Can you transpose a matrix using matrix multiplication?,,Say you have a matrix A = \begin{bmatrix}a&b\\c&d\end{bmatrix} and I want it to look like $A^T$ = \begin{bmatrix}c&a\\d&b\end{bmatrix} Can this be done via matrix multiplication? Something like a matrix T such that $T*A = A^T$.,Say you have a matrix A = \begin{bmatrix}a&b\\c&d\end{bmatrix} and I want it to look like $A^T$ = \begin{bmatrix}c&a\\d&b\end{bmatrix} Can this be done via matrix multiplication? Something like a matrix T such that $T*A = A^T$.,,"['linear-algebra', 'matrices', 'transpose']"
98,"If $A$ is a square matrix and $A^2 = 0$ then $A=0$. Is this true? If not, provide a counter-example.","If  is a square matrix and  then . Is this true? If not, provide a counter-example.",A A^2 = 0 A=0,"This is a proof question and I am not sure how to prove it. It is obviously true if you start with $A = 0$ and square it. I was thinking: If $ A^2 = 0 $ then $ A A = 0 $ $ A A A^{-1} = 0 A^{-1}$ $I\,A = 0 $ but the zero matrix is not invertible and that it was not among the given conditions. Where's a good place to start?","This is a proof question and I am not sure how to prove it. It is obviously true if you start with $A = 0$ and square it. I was thinking: If $ A^2 = 0 $ then $ A A = 0 $ $ A A A^{-1} = 0 A^{-1}$ $I\,A = 0 $ but the zero matrix is not invertible and that it was not among the given conditions. Where's a good place to start?",,"['linear-algebra', 'matrices', 'examples-counterexamples', 'nilpotence']"
99,Characteristic polynomial of a matrix is monic?,Characteristic polynomial of a matrix is monic?,,"Given a $n \times n$ matrix A, I need to show that its characteristic polynomial, defined as $P_A (x) = det (xI-A)$ is monic . I am trying induction. But no clue after induction hypothesis.","Given a $n \times n$ matrix A, I need to show that its characteristic polynomial, defined as $P_A (x) = det (xI-A)$ is monic . I am trying induction. But no clue after induction hypothesis.",,['linear-algebra']
