,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"if this limit $\displaystyle\lim_{n\to\infty}a_{n}$ exists, then find this $w$","if this limit  exists, then find this",\displaystyle\lim_{n\to\infty}a_{n} w,"Let the sequence $\{a_{n}\}$ be such that $$a_{0}=w,\quad a_{n+1}=w^{a_{n}},\quad w>0.$$ If $\displaystyle\lim_{n\to\infty}a_{n}$ exists, then find the possible values for $w$. My try: assume that  $\displaystyle\lim_{n\to\infty}a_{n}$ exists, and let  $\displaystyle\lim_{n\to\infty}a_{n}=a$.  Then $$a=w^a$$@  can prove when $w\in(\dfrac{1}{e},e^{\frac{1}{e}})$ But $w\in(0,\dfrac{1}{e})$.I can't then I don't know how to proceed.","Let the sequence $\{a_{n}\}$ be such that $$a_{0}=w,\quad a_{n+1}=w^{a_{n}},\quad w>0.$$ If $\displaystyle\lim_{n\to\infty}a_{n}$ exists, then find the possible values for $w$. My try: assume that  $\displaystyle\lim_{n\to\infty}a_{n}$ exists, and let  $\displaystyle\lim_{n\to\infty}a_{n}=a$.  Then $$a=w^a$$@  can prove when $w\in(\dfrac{1}{e},e^{\frac{1}{e}})$ But $w\in(0,\dfrac{1}{e})$.I can't then I don't know how to proceed.",,['sequences-and-series']
1,Limit of a sequence with ln.,Limit of a sequence with ln.,,"even though it's not actually a homework rather than some training for myself, I'm posting it with tag ""Homework"": $ \ln(x)^4 $ means: $ \left( \ln (x) \right)^4 $ What is $\lim_{x\to 0} (x^a \cdot \ln(x)^4)$ ? I am not allowed to use L-Hospital. I tried writing it like: $\lim_{x\to 0} (x^a \cdot ln(x)^4) = \lim_{x \to 0} (e^{a\cdot \ln(x)} * e^{\ln(\ln(x)^4)}) = \lim_{x \to 0} (e^{a\cdot \ln(x) + \ln(\ln(x)^4})$ ... Well, I'm really not sure whether I made the right choice(s) ;-(","even though it's not actually a homework rather than some training for myself, I'm posting it with tag ""Homework"": $ \ln(x)^4 $ means: $ \left( \ln (x) \right)^4 $ What is $\lim_{x\to 0} (x^a \cdot \ln(x)^4)$ ? I am not allowed to use L-Hospital. I tried writing it like: $\lim_{x\to 0} (x^a \cdot ln(x)^4) = \lim_{x \to 0} (e^{a\cdot \ln(x)} * e^{\ln(\ln(x)^4)}) = \lim_{x \to 0} (e^{a\cdot \ln(x) + \ln(\ln(x)^4})$ ... Well, I'm really not sure whether I made the right choice(s) ;-(",,['limits']
2,"Continuity, removable and essential discontinuity","Continuity, removable and essential discontinuity",,"I want to know if the following 2 functions are continuous or not. 1. $$ f(x) = \begin{cases} 1/x{^2}  & \text{if } x \ne 0 \\ 2 & \text{ if } x=0  \\ \end{cases}. $$  Then I want to check the continuity at $0$, $$ \lim_{x\to 0}f(x)=\infty, \quad f(0)=2. $$ Since $f(0)$ is not equal to $\lim_{x\to 0}f(x)$,  $f$ is not continuous at $0$. Since $\lim_{x\to 0}f(x)$ exists (going to infinity doesn't mean that limit doesn't exist right?) and $f$ is not continuous at $0$, isn't this a removable discontinuity? Because if limit at that point exists and f is not continuous at that point it is a removable discontinuity and otherwise it is essential discontinuity. 2)  $$ f(x) = \begin{cases} x^2  & \text{if } x \ne 2 \\ 4 & \text{ if } x=2  \\ \end{cases}. $$ Clearly this is continuous at $x=2$. Isn't it? But the lecturer said this is a removable discontinuity. Isn't he making a mistake here?","I want to know if the following 2 functions are continuous or not. 1. $$ f(x) = \begin{cases} 1/x{^2}  & \text{if } x \ne 0 \\ 2 & \text{ if } x=0  \\ \end{cases}. $$  Then I want to check the continuity at $0$, $$ \lim_{x\to 0}f(x)=\infty, \quad f(0)=2. $$ Since $f(0)$ is not equal to $\lim_{x\to 0}f(x)$,  $f$ is not continuous at $0$. Since $\lim_{x\to 0}f(x)$ exists (going to infinity doesn't mean that limit doesn't exist right?) and $f$ is not continuous at $0$, isn't this a removable discontinuity? Because if limit at that point exists and f is not continuous at that point it is a removable discontinuity and otherwise it is essential discontinuity. 2)  $$ f(x) = \begin{cases} x^2  & \text{if } x \ne 2 \\ 4 & \text{ if } x=2  \\ \end{cases}. $$ Clearly this is continuous at $x=2$. Isn't it? But the lecturer said this is a removable discontinuity. Isn't he making a mistake here?",,"['calculus', 'limits', 'continuity']"
3,How find this limit $\lim_{x\to 0}\frac{1}{x^4}\left(\frac{1}{x}\left(\frac{1}{\tanh{x}}-\frac{1}{\tan{x}}\right)-\frac{2}{3}\right)=？$,How find this limit,\lim_{x\to 0}\frac{1}{x^4}\left(\frac{1}{x}\left(\frac{1}{\tanh{x}}-\frac{1}{\tan{x}}\right)-\frac{2}{3}\right)=？,"Find this following limit $$\displaystyle \lim_{x\to 0}\dfrac{1}{x^4}\left(\dfrac{1}{x}\left(\dfrac{1}{\tanh{x}}-\dfrac{1}{\tan{x}}\right)-\dfrac{2}{3}\right)=？$$ My try: since $$\tanh{x}=\dfrac{e^{x}-e^{-x}}{e^x+e^{-x}}$$ then $$\dfrac{1}{\tanh{x}}-\dfrac{1}{\tan{x}}=\dfrac{e^{x}-e^{-x}}{e^x+e^{-x}}-\dfrac{1}{\tan{x}}$$ then I can't,Thank you","Find this following limit $$\displaystyle \lim_{x\to 0}\dfrac{1}{x^4}\left(\dfrac{1}{x}\left(\dfrac{1}{\tanh{x}}-\dfrac{1}{\tan{x}}\right)-\dfrac{2}{3}\right)=？$$ My try: since $$\tanh{x}=\dfrac{e^{x}-e^{-x}}{e^x+e^{-x}}$$ then $$\dfrac{1}{\tanh{x}}-\dfrac{1}{\tan{x}}=\dfrac{e^{x}-e^{-x}}{e^x+e^{-x}}-\dfrac{1}{\tan{x}}$$ then I can't,Thank you",,"['calculus', 'limits']"
4,Prove that $\lim_{x\to\infty}\sin(x-\lfloor x\rfloor) $ doesn't exist,Prove that  doesn't exist,\lim_{x\to\infty}\sin(x-\lfloor x\rfloor) ,"I need to prove that $\lim_{x \to \infty} {\sin(x-\lfloor x\rfloor)}$ doesn't exist. I wanted to prove it using the definition only so I assumed the limit exists and is equal $L$. I showed that for $L<0$ and $L=0$, $L$ doesn't exist but I can't find the right epsilon in order to prove that for $L>0$. The logic is to find an $x_0$ so that $f(x_0)$ will be greater then L+epsilon or lower then L-epsilon and then You get the limit can't be $L$. Please use my logic and help me find the right epsilon for $L>0$. (The epsilon for $L<0$ was $-L + 0.25$)","I need to prove that $\lim_{x \to \infty} {\sin(x-\lfloor x\rfloor)}$ doesn't exist. I wanted to prove it using the definition only so I assumed the limit exists and is equal $L$. I showed that for $L<0$ and $L=0$, $L$ doesn't exist but I can't find the right epsilon in order to prove that for $L>0$. The logic is to find an $x_0$ so that $f(x_0)$ will be greater then L+epsilon or lower then L-epsilon and then You get the limit can't be $L$. Please use my logic and help me find the right epsilon for $L>0$. (The epsilon for $L<0$ was $-L + 0.25$)",,"['calculus', 'limits', 'functions']"
5,What am I doing wrong? Integration and limits,What am I doing wrong? Integration and limits,,"I need some help identifying what I'm doing wrong here.. What is the limit of $y(x)$ when $x→∞$ if $y$ is given by: $$y(x) = 10 + \int_0^x \frac{22(y(t))^2}{1 + t^2}\,dt$$ What i've done: 1) Integrating on both sides(and using the Fundamental Theorem of Calculus): $$\frac{dy}{dx} = 0 + \frac{22(y(x))^2}{1 + x^2}$$ 2) $$\frac{-1}{22y} = \arctan x$$ And after moving around and stuff I end up with the answer: $\quad\dfrac{-1}{11 \pi}.$ What's wrong?",I need some help identifying what I'm doing wrong here.. What is the limit of when if is given by: What i've done: 1) Integrating on both sides(and using the Fundamental Theorem of Calculus): 2) And after moving around and stuff I end up with the answer: What's wrong?,"y(x) x→∞ y y(x) = 10 + \int_0^x \frac{22(y(t))^2}{1 + t^2}\,dt \frac{dy}{dx} = 0 + \frac{22(y(x))^2}{1 + x^2} \frac{-1}{22y} = \arctan x \quad\dfrac{-1}{11 \pi}.","['calculus', 'integration', 'limits']"
6,Proving the limits of the sum of two functions is equal to the sum of the limits,Proving the limits of the sum of two functions is equal to the sum of the limits,,"I am new to proving in math so I want to know if this informal proof of limits is possible: Theorem: If $\lim_{x \to a}f(x)=A$ and $\lim_{x \to a}g(x) = B$ , then $$\lim_{x \to a}[f(x)+g(x)]=A+B$$ $\lim_{x \to a}[f(x) + g(x)] = A + B$ is the same as $\lim_{x\to a}[(f(x)-A) + (g(x) - B)]=0$ . Also, $$0 \le |(f(x)-A) + (g(x) - B)| \le |f(x)-A| + |g(x) - B|.$$ Since $\lim_{x\to a} f(x)-A=0$ and $\lim_{x\to a}g(x)-B=0$ , we know $$0 \le \lim_{x\to a}|(f(x)-A) + (g(x) - B)| \le 0.$$ So, $\lim_{x\to a}[(f(x)-A) + (g(x) - B)] = 0.\ \square$ If my proof is wrong what is wrong with it and how can I correct it? Also, why is the proof in Apostol II on page 248 of limits correct? Why didn't he use limits ( $\epsilon,\delta$ ) as he did in Apostol I on page 132? Also, why did he assume that the limits A and B are 0 then say that proves it for all cases? This is Apostol's proof(Theorem 8.1) in vector valued functions of vector variables. Is this proof also valid for real valued functions of vector variables(scalar fields) ? Why didn't he use $\epsilon,\delta$ ?","I am new to proving in math so I want to know if this informal proof of limits is possible: Theorem: If and , then is the same as . Also, Since and , we know So, If my proof is wrong what is wrong with it and how can I correct it? Also, why is the proof in Apostol II on page 248 of limits correct? Why didn't he use limits ( ) as he did in Apostol I on page 132? Also, why did he assume that the limits A and B are 0 then say that proves it for all cases? This is Apostol's proof(Theorem 8.1) in vector valued functions of vector variables. Is this proof also valid for real valued functions of vector variables(scalar fields) ? Why didn't he use ?","\lim_{x \to a}f(x)=A \lim_{x \to a}g(x) = B \lim_{x \to a}[f(x)+g(x)]=A+B \lim_{x \to a}[f(x) + g(x)] = A + B \lim_{x\to a}[(f(x)-A) + (g(x) - B)]=0 0 \le |(f(x)-A) + (g(x) - B)| \le |f(x)-A| + |g(x) - B|. \lim_{x\to a} f(x)-A=0 \lim_{x\to a}g(x)-B=0 0 \le \lim_{x\to a}|(f(x)-A) + (g(x) - B)| \le 0. \lim_{x\to a}[(f(x)-A) + (g(x) - B)] = 0.\ \square \epsilon,\delta \epsilon,\delta","['calculus', 'limits', 'multivariable-calculus', 'proof-verification', 'alternative-proof']"
7,Proof of divergence of two complex series,Proof of divergence of two complex series,,"My task is to check, when the following series are convergent: $$ $$ a) $\sum_{n=1}^{\infty} \frac{(2n)!}{(n!)^2} z^n$ $$ $$ B) $\sum_{n=1}^{\infty} \frac{n!}{n^n} z^n$ $$ $$ where $z$ is a complex number. Ok, in both cases I computed the radius of convegence. In a) I got $R=\frac{1}{4}$ and in b) I got $R=e$. Those answers are correct, as my book says. But I don't know what happens on the boundary, I mean in first case for $z=\frac{1}{4}$ and in the second for $z=e$. Wolfram says that in both cases it diverges, first one from the comparison test (but compare to what?) and the second one because $\lim \frac{n!}{n^n} e^n = \infty$ (Why?). How to prove it? I really tried, got so frustrated, because it should be easy, but I can't prove divergence of those series... Can you help? Edit: after hint with Stirling formula, I've got the second one (b) ). Help still needed in a). Thanks in advance.","My task is to check, when the following series are convergent: $$ $$ a) $\sum_{n=1}^{\infty} \frac{(2n)!}{(n!)^2} z^n$ $$ $$ B) $\sum_{n=1}^{\infty} \frac{n!}{n^n} z^n$ $$ $$ where $z$ is a complex number. Ok, in both cases I computed the radius of convegence. In a) I got $R=\frac{1}{4}$ and in b) I got $R=e$. Those answers are correct, as my book says. But I don't know what happens on the boundary, I mean in first case for $z=\frac{1}{4}$ and in the second for $z=e$. Wolfram says that in both cases it diverges, first one from the comparison test (but compare to what?) and the second one because $\lim \frac{n!}{n^n} e^n = \infty$ (Why?). How to prove it? I really tried, got so frustrated, because it should be easy, but I can't prove divergence of those series... Can you help? Edit: after hint with Stirling formula, I've got the second one (b) ). Help still needed in a). Thanks in advance.",,"['limits', 'divergent-series']"
8,Use the epsilon-delta definition of limits to evaluate the limit $\lim_{x\to1}\frac{x^2-x-2}{2x-3}$,Use the epsilon-delta definition of limits to evaluate the limit,\lim_{x\to1}\frac{x^2-x-2}{2x-3},"The Problem: Evaluate the given limit and prove your conclusion using only definitions from the epsilon-delta limit definition: $$\lim_{x \rightarrow1}\frac{x^2-x-2}{2x-3}$$ First I evaluated the limit and found the limit of the function to be 2.  For every $\varepsilon > 0$, there exists a $\delta$ so that $|x-1|< \delta \Rightarrow \frac{x^2-x-2}{2x-3}-2 < \varepsilon$.  After doing some algebra on the epsilon inequality, I end up with $|\frac{x^2-5x+4}{2x-3}| < \varepsilon$.  Since I can't prove the limit by defining x in terms of $\varepsilon$ and $x$, my professor has told my class that I should choose a specific value for delta for the purpose of substituting it into the inequality to find an x in terms of epsilon alone. I let $\delta=1$, and it follows from $|x-1|<1$ that $|x|<2$, and that $|\frac{1}{2x-3}|>1$ (for eliminating the denominator.  I think that $|\frac{1}{2x-3}|>1$ has set me back in solving this rigorously, since the expression is greater than, rather than less than 1. How do I proceed?","The Problem: Evaluate the given limit and prove your conclusion using only definitions from the epsilon-delta limit definition: $$\lim_{x \rightarrow1}\frac{x^2-x-2}{2x-3}$$ First I evaluated the limit and found the limit of the function to be 2.  For every $\varepsilon > 0$, there exists a $\delta$ so that $|x-1|< \delta \Rightarrow \frac{x^2-x-2}{2x-3}-2 < \varepsilon$.  After doing some algebra on the epsilon inequality, I end up with $|\frac{x^2-5x+4}{2x-3}| < \varepsilon$.  Since I can't prove the limit by defining x in terms of $\varepsilon$ and $x$, my professor has told my class that I should choose a specific value for delta for the purpose of substituting it into the inequality to find an x in terms of epsilon alone. I let $\delta=1$, and it follows from $|x-1|<1$ that $|x|<2$, and that $|\frac{1}{2x-3}|>1$ (for eliminating the denominator.  I think that $|\frac{1}{2x-3}|>1$ has set me back in solving this rigorously, since the expression is greater than, rather than less than 1. How do I proceed?",,"['real-analysis', 'limits', 'epsilon-delta']"
9,Limits of an integrable function over increasing sequence of sets,Limits of an integrable function over increasing sequence of sets,,Let $(E_n)_{n \geq 1}$ be an increasing sequence of sets such that $\bigcup_{n \geq 1} E_n = \Omega$. Then for every integrable function $f$ we have $$\lim_{n \rightarrow \infty} \int_{E_n} f d\mu = \int_{\Omega} f d\mu.$$ I think we have to start by showing that the result is true for a simple function. But I don't know how to complete the proof.,Let $(E_n)_{n \geq 1}$ be an increasing sequence of sets such that $\bigcup_{n \geq 1} E_n = \Omega$. Then for every integrable function $f$ we have $$\lim_{n \rightarrow \infty} \int_{E_n} f d\mu = \int_{\Omega} f d\mu.$$ I think we have to start by showing that the result is true for a simple function. But I don't know how to complete the proof.,,"['integration', 'limits', 'measure-theory', 'lebesgue-integral']"
10,"Evaluating limits by subsituting special sequences, justification for that","Evaluating limits by subsituting special sequences, justification for that",,Sometimes I saw people using transformations like $$  \lim_{x\to 0} f(x) = \lim_{n\to \infty} f(\frac{1}{n}) $$ or $$  \lim_{x \to p} f(x) = \lim_{n \to \infty} f(x + \frac{1}{n}). \quad (*) $$ I know that $$   \lim_{x \to p} f(x) = q \Leftrightarrow \lim_{n\to \infty} f(p_n) = q $$ for all sequences $p_n \to p$. So how is it justified to use such a special sequence like $p_n = \frac{1}{n}$ for example in (*)?,Sometimes I saw people using transformations like $$  \lim_{x\to 0} f(x) = \lim_{n\to \infty} f(\frac{1}{n}) $$ or $$  \lim_{x \to p} f(x) = \lim_{n \to \infty} f(x + \frac{1}{n}). \quad (*) $$ I know that $$   \lim_{x \to p} f(x) = q \Leftrightarrow \lim_{n\to \infty} f(p_n) = q $$ for all sequences $p_n \to p$. So how is it justified to use such a special sequence like $p_n = \frac{1}{n}$ for example in (*)?,,"['real-analysis', 'analysis', 'limits', 'continuity']"
11,What is the precise mathematical definition of $\lim_{x\to \infty} f(x) = −∞$?,What is the precise mathematical definition of ?,\lim_{x\to \infty} f(x) = −∞,"What is the precise mathematical definition of $\lim_{x\to \infty} f(x) = −∞$? As x approaches infinity. I know what the general meaning is, but I heard my teacher talk about some precise mathematical definition.","What is the precise mathematical definition of $\lim_{x\to \infty} f(x) = −∞$? As x approaches infinity. I know what the general meaning is, but I heard my teacher talk about some precise mathematical definition.",,"['calculus', 'limits']"
12,Prove $\lim_{n\to\infty}{\frac{3n+1}{2n+5}=\frac{3}{2}}$,Prove,\lim_{n\to\infty}{\frac{3n+1}{2n+5}=\frac{3}{2}},Problem Prove $$\lim_{n\to\infty}{\frac{3n+1}{2n+5}=\frac{3}{2}}$$ using a epsilon proof Scratchwork We see that $\mid{\frac{3n+1}{2n+5}-\frac{3}{2}}\mid=\mid{\frac{6n+2}{4n+10}-\frac{6n+15}{4n+10}}\mid= \mid\frac{-13}{4n+10}\mid=\frac{13}{4n+10}<\frac{13}{4n}<\frac{13}{n}<\epsilon$. (I like to go all the way to $\frac{13}{n}$ which im not sure i can do). If we let $\epsilon >0$ then $\frac{1}{\epsilon}>0\implies \frac{13}{\epsilon}$ By the Archimdean property we can choose a $k\in\mathbb{N}$ such that $\frac{13}{\epsilon}<k\implies \frac{13}{k}<\epsilon$. Proof: Let let $\epsilon >0$ then $\frac{1}{\epsilon}>0\implies \frac{13}{\epsilon}>0$ Then by the Archimedean property there exists a $k\in\mathbb{N}$ such that $k>\frac{13}{\epsilon}\implies \frac{13}{k}<\epsilon$. If $n \geq k$ then $\frac{1}{n} \leq \frac{1}{k}\implies \frac{13}{n}\leq \frac{13}{k} <\epsilon$. Thus$\mid{\frac{3n+1}{2n+5}-\frac{3}{2}}\mid<\frac{13}{n}<\epsilon$. Thus $\lim{\frac{3n+1}{2n+5}=\frac{3}{2}}$. Would this suffice?,Problem Prove $$\lim_{n\to\infty}{\frac{3n+1}{2n+5}=\frac{3}{2}}$$ using a epsilon proof Scratchwork We see that $\mid{\frac{3n+1}{2n+5}-\frac{3}{2}}\mid=\mid{\frac{6n+2}{4n+10}-\frac{6n+15}{4n+10}}\mid= \mid\frac{-13}{4n+10}\mid=\frac{13}{4n+10}<\frac{13}{4n}<\frac{13}{n}<\epsilon$. (I like to go all the way to $\frac{13}{n}$ which im not sure i can do). If we let $\epsilon >0$ then $\frac{1}{\epsilon}>0\implies \frac{13}{\epsilon}$ By the Archimdean property we can choose a $k\in\mathbb{N}$ such that $\frac{13}{\epsilon}<k\implies \frac{13}{k}<\epsilon$. Proof: Let let $\epsilon >0$ then $\frac{1}{\epsilon}>0\implies \frac{13}{\epsilon}>0$ Then by the Archimedean property there exists a $k\in\mathbb{N}$ such that $k>\frac{13}{\epsilon}\implies \frac{13}{k}<\epsilon$. If $n \geq k$ then $\frac{1}{n} \leq \frac{1}{k}\implies \frac{13}{n}\leq \frac{13}{k} <\epsilon$. Thus$\mid{\frac{3n+1}{2n+5}-\frac{3}{2}}\mid<\frac{13}{n}<\epsilon$. Thus $\lim{\frac{3n+1}{2n+5}=\frac{3}{2}}$. Would this suffice?,,"['real-analysis', 'limits']"
13,Math Differentiation Limits,Math Differentiation Limits,,$$\lim_{x\to 3} \frac{2x^2 + 7x-15}{x-3}$$ What I Simplified Step 1 : $\frac{2x^2 + 10x -3x -15}{x-3}$ Step 2 : $\frac{2x(x + 5)-3(x + 5)}{x-3}$ Step 3 : $\frac{(2x - 3)(x + 5)}{x-3}$ but unable to move further.. im thinking either the question is wrong or there is some trick which im unable to understand thanx in advance,$$\lim_{x\to 3} \frac{2x^2 + 7x-15}{x-3}$$ What I Simplified Step 1 : $\frac{2x^2 + 10x -3x -15}{x-3}$ Step 2 : $\frac{2x(x + 5)-3(x + 5)}{x-3}$ Step 3 : $\frac{(2x - 3)(x + 5)}{x-3}$ but unable to move further.. im thinking either the question is wrong or there is some trick which im unable to understand thanx in advance,,"['limits', 'derivatives']"
14,Limit $\lim \limits_{z\rightarrow 0^+} z \int_1^\infty x^2 e^{-zx^2-zx} \mathrm dx $,Limit,\lim \limits_{z\rightarrow 0^+} z \int_1^\infty x^2 e^{-zx^2-zx} \mathrm dx ,Consider the following limit: $$\lim \limits_{z\rightarrow 0^+} z \int_1^\infty x^2 e^{-zx^2-zx} dx $$ Can we find the answer to this limit without  calculating the integral? I'll be thankful if you help me on this issue?,Consider the following limit: $$\lim \limits_{z\rightarrow 0^+} z \int_1^\infty x^2 e^{-zx^2-zx} dx $$ Can we find the answer to this limit without  calculating the integral? I'll be thankful if you help me on this issue?,,"['calculus', 'limits', 'improper-integrals']"
15,Limit of a Sum Involving Binomial Coefficients,Limit of a Sum Involving Binomial Coefficients,,I would like to prove that $$\dfrac{\sqrt{n}}{{{2n \choose n}^2}} \cdot \sum_{j=0}^n {n \choose j}^4$$ converges to $\sqrt{\dfrac{2}{\pi}}$ as $n \to \infty$. Evaluating the sum in Matematica for large values of $n$ suggests that $\sqrt{\dfrac{2}{\pi}}$ is indeed the correct limit.,I would like to prove that $$\dfrac{\sqrt{n}}{{{2n \choose n}^2}} \cdot \sum_{j=0}^n {n \choose j}^4$$ converges to $\sqrt{\dfrac{2}{\pi}}$ as $n \to \infty$. Evaluating the sum in Matematica for large values of $n$ suggests that $\sqrt{\dfrac{2}{\pi}}$ is indeed the correct limit.,,"['limits', 'binomial-coefficients']"
16,How find this $\sum_{n=1}^{\infty}a_{n}$,How find this,\sum_{n=1}^{\infty}a_{n},"let $$a_{1}=1,a_{n+1}=\dfrac{1}{a_{1}+a_{2}+\cdots+a_{n}}-\sqrt{2}$$ find the value $$\sum_{n=1}^{\infty}a_{n}$$ my try:let $S_{n}=a_{1}+a_{2}+\cdots+a_{n}$,then we have $$S_{n+1}=S_{n}+a_{n+1}=S_{n}+\dfrac{1}{S_{n}}-\sqrt{2}$$ and I find $S_{1}=1,S_{2}=2-\sqrt{2},S_{3}=3-\dfrac{3}{2}\sqrt{2}$ I guess $$S_{1}>S_{3}>\cdots>S_{2n-1},S_{2}>S_{4}>\cdots>S_{2n}$$ so I try prove $$S_{2n+1}-S_{2n-1}<0$$ if $n$ is odd. then $$S_{n+2}=S_{n+1}+\dfrac{1}{S_{n+1}}=S_{n}+\dfrac{1}{S_{n}}-\sqrt{2}+\dfrac{1}{S_{n}+\dfrac{1}{S_{n}}-\sqrt{2}}-\sqrt{2}$$ let $$f(x)=x+\dfrac{1}{x}-\sqrt{2}\Longleftrightarrow S_{n+2}=f(S_{n+1})=f(f(S_{n}))$$ where $x=S_{n}$ and $$S_{n+2}-S_{n}=\dfrac{1}{x}-\sqrt{2}+\dfrac{1}{x+\dfrac{1}{x}-\sqrt{2}}-\sqrt{2}=\dfrac{(1-\sqrt{2}x)^3}{x(x^2-\sqrt{2}x+1)}$$ so if $x\in \left(\dfrac{1}{\sqrt{2}},1\right)$, then we have $$S_{n+2}<S_{n}$$ where $n$ is odd numbers. my question: How can I  determine  $x=S_{n}$(n is odd) in $(\dfrac{1}{\sqrt{2}},1)$? and I think this problem have other nice methods.Thank you","let $$a_{1}=1,a_{n+1}=\dfrac{1}{a_{1}+a_{2}+\cdots+a_{n}}-\sqrt{2}$$ find the value $$\sum_{n=1}^{\infty}a_{n}$$ my try:let $S_{n}=a_{1}+a_{2}+\cdots+a_{n}$,then we have $$S_{n+1}=S_{n}+a_{n+1}=S_{n}+\dfrac{1}{S_{n}}-\sqrt{2}$$ and I find $S_{1}=1,S_{2}=2-\sqrt{2},S_{3}=3-\dfrac{3}{2}\sqrt{2}$ I guess $$S_{1}>S_{3}>\cdots>S_{2n-1},S_{2}>S_{4}>\cdots>S_{2n}$$ so I try prove $$S_{2n+1}-S_{2n-1}<0$$ if $n$ is odd. then $$S_{n+2}=S_{n+1}+\dfrac{1}{S_{n+1}}=S_{n}+\dfrac{1}{S_{n}}-\sqrt{2}+\dfrac{1}{S_{n}+\dfrac{1}{S_{n}}-\sqrt{2}}-\sqrt{2}$$ let $$f(x)=x+\dfrac{1}{x}-\sqrt{2}\Longleftrightarrow S_{n+2}=f(S_{n+1})=f(f(S_{n}))$$ where $x=S_{n}$ and $$S_{n+2}-S_{n}=\dfrac{1}{x}-\sqrt{2}+\dfrac{1}{x+\dfrac{1}{x}-\sqrt{2}}-\sqrt{2}=\dfrac{(1-\sqrt{2}x)^3}{x(x^2-\sqrt{2}x+1)}$$ so if $x\in \left(\dfrac{1}{\sqrt{2}},1\right)$, then we have $$S_{n+2}<S_{n}$$ where $n$ is odd numbers. my question: How can I  determine  $x=S_{n}$(n is odd) in $(\dfrac{1}{\sqrt{2}},1)$? and I think this problem have other nice methods.Thank you",,"['sequences-and-series', 'limits']"
17,Mathematical Analysis help,Mathematical Analysis help,,"Let $f$ be differentiable on $(0,\infty)$ and suppose $\lim_{x\rightarrow\ \infty}(f(x)+f'(x))=L$. Then, $\lim_{x\rightarrow\ \infty}f(x)= L$ and $\lim_{x\rightarrow\ \infty}f'(x)= 0$ My approach: $\lim_{x\rightarrow\ \infty}f(x)= \lim_{x\rightarrow\ \infty}\frac{f(x)e^x}{e^x} =\lim_{x\rightarrow\ \infty}\frac{f(x)e^x+f'(x)e^x}{e^x }=L$. But this is only valid if we can show $\lim_{x\rightarrow\ \infty}f(x)e^x=\infty$. Could anyone advise please?","Let $f$ be differentiable on $(0,\infty)$ and suppose $\lim_{x\rightarrow\ \infty}(f(x)+f'(x))=L$. Then, $\lim_{x\rightarrow\ \infty}f(x)= L$ and $\lim_{x\rightarrow\ \infty}f'(x)= 0$ My approach: $\lim_{x\rightarrow\ \infty}f(x)= \lim_{x\rightarrow\ \infty}\frac{f(x)e^x}{e^x} =\lim_{x\rightarrow\ \infty}\frac{f(x)e^x+f'(x)e^x}{e^x }=L$. But this is only valid if we can show $\lim_{x\rightarrow\ \infty}f(x)e^x=\infty$. Could anyone advise please?",,"['real-analysis', 'limits', 'functions', 'convergence-divergence']"
18,"Prove that $\lim_{t\to 0}f(g(t))$ exists, where $g$ is differentiable and $f(x,y)=x^2y/(x^2+y^2)$.","Prove that  exists, where  is differentiable and .","\lim_{t\to 0}f(g(t)) g f(x,y)=x^2y/(x^2+y^2)","Let $f:\mathbb{R}^2\to\mathbb{R}$ be a function defined by $$f(x,y)=\left\{\begin{matrix} \frac{x^2y}{x^2+y^2}&\text{if }(x,y)\neq(0,0) \\   0&\text{if }x=y=0  \end{matrix}\right.$$ Let $\varepsilon>0$ and $g:(-\varepsilon,\varepsilon)\to\mathbb{R}^2$ be a function differentiable at point $0$ such that $g(0)=(0,0)$. Prove that the following limit exists. $$\lim_{t\to 0}\frac{f\left(g(t)\right)}{t}$$ Thanks.","Let $f:\mathbb{R}^2\to\mathbb{R}$ be a function defined by $$f(x,y)=\left\{\begin{matrix} \frac{x^2y}{x^2+y^2}&\text{if }(x,y)\neq(0,0) \\   0&\text{if }x=y=0  \end{matrix}\right.$$ Let $\varepsilon>0$ and $g:(-\varepsilon,\varepsilon)\to\mathbb{R}^2$ be a function differentiable at point $0$ such that $g(0)=(0,0)$. Prove that the following limit exists. $$\lim_{t\to 0}\frac{f\left(g(t)\right)}{t}$$ Thanks.",,"['limits', 'multivariable-calculus']"
19,What is the limit $( 1 + 1/\tan(n) )^{\tan(n)}$,What is the limit,( 1 + 1/\tan(n) )^{\tan(n)},What is the result of  $\displaystyle \lim_{n\to \infty}\left(1+\frac{1}{\tan(n)}\right)^{\tan(n)}$ = ? The limit does not exist as stated by Adam Rubinson. Looks like the requested answer is e but then the question is wrong. What is the correct question to give the answer e ? Did answered correctly. But I think this is some kind of typographical error. This question appeared on a high school test and the intended question should have a small error. What is the correct question to give the answer e changing only one value?,What is the result of  $\displaystyle \lim_{n\to \infty}\left(1+\frac{1}{\tan(n)}\right)^{\tan(n)}$ = ? The limit does not exist as stated by Adam Rubinson. Looks like the requested answer is e but then the question is wrong. What is the correct question to give the answer e ? Did answered correctly. But I think this is some kind of typographical error. This question appeared on a high school test and the intended question should have a small error. What is the correct question to give the answer e changing only one value?,,"['limits', 'recreational-mathematics']"
20,"How to compute the limit $\lim\limits_{q \rightarrow 1} (1-q)^3 \sum_{n>0} n^{1+\epsilon} q^n$ for $\epsilon \in (0,1)$",How to compute the limit  for,"\lim\limits_{q \rightarrow 1} (1-q)^3 \sum_{n>0} n^{1+\epsilon} q^n \epsilon \in (0,1)","I am able to show that $\lim\limits_{q \rightarrow 1} (1-q)^3 \sum_{n>0} n q^n  = 0 $ and $ \lim\limits_{q \rightarrow 1} (1-q)^3 \sum_{n>0} n^2 q^n  = 2$ and now I want to prove that $$ \lim\limits_{q \rightarrow 1} (1-q)^3 \sum_{n>0} n^{1+\epsilon} q^n = 0 $$ for $\epsilon \in (0,1)$. Do you guys have any hints for me doing so? I can't use the methods which I used to proof the first two statements because they are just valid for integral exponents of $n$. Thanks in advance!","I am able to show that $\lim\limits_{q \rightarrow 1} (1-q)^3 \sum_{n>0} n q^n  = 0 $ and $ \lim\limits_{q \rightarrow 1} (1-q)^3 \sum_{n>0} n^2 q^n  = 2$ and now I want to prove that $$ \lim\limits_{q \rightarrow 1} (1-q)^3 \sum_{n>0} n^{1+\epsilon} q^n = 0 $$ for $\epsilon \in (0,1)$. Do you guys have any hints for me doing so? I can't use the methods which I used to proof the first two statements because they are just valid for integral exponents of $n$. Thanks in advance!",,"['limits', 'special-functions']"
21,Finding limits with substitution,Finding limits with substitution,,$\lim_{x\to0+}(\sinh(x))^{1/x}$ I started by setting $y=\frac{\sinh(x)}{x}$ and taking the natural logarithm of both sides and trying to solve the limit for $ln(y)$ but I got stuck trying to solve $\lim_{x\to0+}\frac{\ln(\sinh(x))}{x}$. Any ideas? Thanks in advanced.,$\lim_{x\to0+}(\sinh(x))^{1/x}$ I started by setting $y=\frac{\sinh(x)}{x}$ and taking the natural logarithm of both sides and trying to solve the limit for $ln(y)$ but I got stuck trying to solve $\lim_{x\to0+}\frac{\ln(\sinh(x))}{x}$. Any ideas? Thanks in advanced.,,"['calculus', 'limits', 'probability-limit-theorems']"
22,multivariable limit question,multivariable limit question,,"Is this an acceptable solution? $$\lim_{(x,y)\rightarrow(0,0)}\frac{\sin(2(x^2+y^2))}{x^2+y^2}$$ $$t=x^2+y^2$$ So $t\rightarrow0$. Now I change the limit to: $$\lim_{t\rightarrow0}\frac{\sin(2t)}{t}=2$$ This solution makes sense to me, but I am still having doubts.","Is this an acceptable solution? $$\lim_{(x,y)\rightarrow(0,0)}\frac{\sin(2(x^2+y^2))}{x^2+y^2}$$ $$t=x^2+y^2$$ So $t\rightarrow0$. Now I change the limit to: $$\lim_{t\rightarrow0}\frac{\sin(2t)}{t}=2$$ This solution makes sense to me, but I am still having doubts.",,"['limits', 'multivariable-calculus']"
23,On the existence of a non-negative function on a Banach space whose limit at every point is infinity.,On the existence of a non-negative function on a Banach space whose limit at every point is infinity.,,"Does there exist a Banach space $ X $ (possibly non-separable) and a mapping $ F: X \to X $ such that $$ \forall a \in X: \quad \lim_{\substack{x \in X \setminus \{ a \} \\ x \to a}} \| F(x) \|_{X} = \infty? $$ Note: If the Banach space $ X $ is trivial, then the answer is a vacuous affirmative as the zero element, $ 0_{X} $, is the sole element of $ X $ and hence not a limit point. We may thus restrict our attention to only non-trivial Banach spaces.","Does there exist a Banach space $ X $ (possibly non-separable) and a mapping $ F: X \to X $ such that $$ \forall a \in X: \quad \lim_{\substack{x \in X \setminus \{ a \} \\ x \to a}} \| F(x) \|_{X} = \infty? $$ Note: If the Banach space $ X $ is trivial, then the answer is a vacuous affirmative as the zero element, $ 0_{X} $, is the sole element of $ X $ and hence not a limit point. We may thus restrict our attention to only non-trivial Banach spaces.",,"['general-topology', 'functional-analysis', 'limits', 'banach-spaces']"
24,Evaluate a limit of an integral,Evaluate a limit of an integral,,"Here is another Prelim Question. Evaluate: $$ I = \lim_{\epsilon\to 0} \frac{1}{2\pi\epsilon}\int_{-\infty}^\infty\int_{-\infty}^\infty f(x,y)\frac{y}{\epsilon}\exp{\frac{-(x^2+y^2)}{2\epsilon}}dxdy $$ where $f$ and $\nabla f$ are continuous and bounded. So I have tried to change the integration order and used integration by parts and then changing to polar coordinates to obtain $$ I = \frac{\partial f}{\partial y}(0,0) + \lim_{\epsilon \to 0}\frac{1}{2 \pi}\int_0^{2\pi}\int_0^\infty\frac{\partial}{\partial r}\left(\frac{\partial f}{\partial y}(r\cos(\theta),r\sin(\theta) )\right) \exp\left(\frac{-r^2}{2\epsilon}\right) dr d\theta $$ I hope the final answer is the first term and we can show the second term goes to zero.  Can we just apply DCT?  I am getting second order partials in the second term and we only know information for the first order derivatives.  Note that  $$ \frac{\partial }{\partial r} = \cos(\theta)\frac{\partial}{\partial x}+\sin(\theta)\frac{\partial}{\partial y} $$","Here is another Prelim Question. Evaluate: $$ I = \lim_{\epsilon\to 0} \frac{1}{2\pi\epsilon}\int_{-\infty}^\infty\int_{-\infty}^\infty f(x,y)\frac{y}{\epsilon}\exp{\frac{-(x^2+y^2)}{2\epsilon}}dxdy $$ where $f$ and $\nabla f$ are continuous and bounded. So I have tried to change the integration order and used integration by parts and then changing to polar coordinates to obtain $$ I = \frac{\partial f}{\partial y}(0,0) + \lim_{\epsilon \to 0}\frac{1}{2 \pi}\int_0^{2\pi}\int_0^\infty\frac{\partial}{\partial r}\left(\frac{\partial f}{\partial y}(r\cos(\theta),r\sin(\theta) )\right) \exp\left(\frac{-r^2}{2\epsilon}\right) dr d\theta $$ I hope the final answer is the first term and we can show the second term goes to zero.  Can we just apply DCT?  I am getting second order partials in the second term and we only know information for the first order derivatives.  Note that  $$ \frac{\partial }{\partial r} = \cos(\theta)\frac{\partial}{\partial x}+\sin(\theta)\frac{\partial}{\partial y} $$",,"['integration', 'limits']"
25,When does $\lim\limits_{n\to\infty}\int_{b}^{a_n}f_n(x)dx=\lim\limits_{n\to\infty}\int_b^\infty f_n(x)dx$ hold?,When does  hold?,\lim\limits_{n\to\infty}\int_{b}^{a_n}f_n(x)dx=\lim\limits_{n\to\infty}\int_b^\infty f_n(x)dx,"Let $\{a_n\}\subset \mathbb{R}$ be sequence and $$f_n:[b,\infty)\longrightarrow \mathbb{R}, \qquad n=1,2,\dots .$$ Assume that $$\lim_{n\longrightarrow\infty}a_n=+\infty.$$ Obviously, from the definition $\lim_{n\longrightarrow\infty}\int_{b}^{a_n}g(x)dx=\int_{b}^{\infty}g(x)dx.$ I am interested about conditions to set of functions $\{f_n\},$ when the following is true: $$\lim_{n\longrightarrow\infty}\int_{b}^{a_n}f_n(x)dx=\lim_{n\longrightarrow\infty}\int_b^\infty f_n(x)dx.$$","Let $\{a_n\}\subset \mathbb{R}$ be sequence and $$f_n:[b,\infty)\longrightarrow \mathbb{R}, \qquad n=1,2,\dots .$$ Assume that $$\lim_{n\longrightarrow\infty}a_n=+\infty.$$ Obviously, from the definition $\lim_{n\longrightarrow\infty}\int_{b}^{a_n}g(x)dx=\int_{b}^{\infty}g(x)dx.$ I am interested about conditions to set of functions $\{f_n\},$ when the following is true: $$\lim_{n\longrightarrow\infty}\int_{b}^{a_n}f_n(x)dx=\lim_{n\longrightarrow\infty}\int_b^\infty f_n(x)dx.$$",,"['calculus', 'limits', 'integration', 'convergence-divergence']"
26,Limit of Multivariable Fourier Series,Limit of Multivariable Fourier Series,,"If I have some Fourier Series representation of a function with $x$ period of $2L$ $$G(x,y) = \sum_{n = 1}^{\infty} \left[a_n \sin\left(\frac{n \pi x}{L}\right) + b_n \cos\left(\frac{n \pi x}{L}\right)\right]f_n(y)$$ Where each of the $f_n(y) \to 0 $ as $y \to \infty$ do we necessarily have that $$\lim_{y \to \infty} G(x,y) = 0$$ I do not know the relevant analysis -- maybe we are not always allowed to take termwise limits of an infinite sum? Is there a general result that applies here? How might I go about calculating limits of sums like this, is there a particular method that is usually applied? Thank You","If I have some Fourier Series representation of a function with $x$ period of $2L$ $$G(x,y) = \sum_{n = 1}^{\infty} \left[a_n \sin\left(\frac{n \pi x}{L}\right) + b_n \cos\left(\frac{n \pi x}{L}\right)\right]f_n(y)$$ Where each of the $f_n(y) \to 0 $ as $y \to \infty$ do we necessarily have that $$\lim_{y \to \infty} G(x,y) = 0$$ I do not know the relevant analysis -- maybe we are not always allowed to take termwise limits of an infinite sum? Is there a general result that applies here? How might I go about calculating limits of sums like this, is there a particular method that is usually applied? Thank You",,"['real-analysis', 'sequences-and-series', 'limits', 'fourier-series']"
27,Limit and integral,Limit and integral,,"Let $\Omega = \mathopen]0,1\mathclose[$ and let a function $A_n: \Omega \to \mathbb R$ defined as: $$A_n(x) = \begin{cases}\alpha &\text{if } k \epsilon \leq x < (k+\tfrac{1}{2}) \epsilon \\ \beta &\text{if } \big(k+\tfrac{1}{2}\big) \epsilon \leq x < (k+1) \epsilon \end{cases} $$  for $k=0$, $1,\ldots,n-1$ where $\alpha$, $\beta > 0$ and $\epsilon=1/n$. Let $f: [0,1] \rightarrow \mathbb{R}$ be a continuous function. How can we prove that  $$\lim_{n \to \infty} \int_0^1 A_n(x) f(x) dx = \lim_{n \to\infty} \sum_{k=0}^{n-1} \left(\alpha \int_{k \epsilon}^{(k+1/2)\epsilon} f(x) dx + \beta \int_{(k+1/2)\epsilon}^{(k+1)\epsilon} f(x) dx\right) \\= \dfrac{\alpha + \beta}{2} \int_0^1 f(x) dx$$ Thanks for your help.","Let $\Omega = \mathopen]0,1\mathclose[$ and let a function $A_n: \Omega \to \mathbb R$ defined as: $$A_n(x) = \begin{cases}\alpha &\text{if } k \epsilon \leq x < (k+\tfrac{1}{2}) \epsilon \\ \beta &\text{if } \big(k+\tfrac{1}{2}\big) \epsilon \leq x < (k+1) \epsilon \end{cases} $$  for $k=0$, $1,\ldots,n-1$ where $\alpha$, $\beta > 0$ and $\epsilon=1/n$. Let $f: [0,1] \rightarrow \mathbb{R}$ be a continuous function. How can we prove that  $$\lim_{n \to \infty} \int_0^1 A_n(x) f(x) dx = \lim_{n \to\infty} \sum_{k=0}^{n-1} \left(\alpha \int_{k \epsilon}^{(k+1/2)\epsilon} f(x) dx + \beta \int_{(k+1/2)\epsilon}^{(k+1)\epsilon} f(x) dx\right) \\= \dfrac{\alpha + \beta}{2} \int_0^1 f(x) dx$$ Thanks for your help.",,"['calculus', 'integration', 'limits']"
28,Is this convergent or diverges to infinity?,Is this convergent or diverges to infinity?,,"Solve or give some hints. $\lim_{n\to\infty}\dfrac {C_n^{F_n}}{F_n^{C_n}}$, where $C_n=\dfrac {(2n)!}{n!(n+1)!}$ is the n-th Catalan number and $F_n=2^{2^n}+1$ is the n-th Fermat number.","Solve or give some hints. $\lim_{n\to\infty}\dfrac {C_n^{F_n}}{F_n^{C_n}}$, where $C_n=\dfrac {(2n)!}{n!(n+1)!}$ is the n-th Catalan number and $F_n=2^{2^n}+1$ is the n-th Fermat number.",,[]
29,computing a limit of a ratio of derangements,computing a limit of a ratio of derangements,,"Fix $m$. Consider $\lbrace 1,\ldots ,n\rbrace$. Let $a_1\dots a_n$ be a permutation of this set. How many permutations are there such that $a_i\not=i$ for all $i$ and each $i$ travels at most $m$ spots? small valued examples below. Suppose $m=2$. If $n=2$, the only permutation we like is 2,1. If $n=3$, then the permutations we like are of the form $\_\, \_\,\_$ where 2 or 3 can go in slot 1; 1 or 3 can go in slot 2, and 2 or 3 can go in slot 3. This gives a total of 2 permutations. In this simplified setting we cannot resort to the fundamental counting principle due to the constraint-the number of times you can choose each number. Is there a way to generalize this to the set of $n$ elements for any integer $m$? EDIT: I am trying to compute -for a fixed $m$-the limit of the number of such permutations described above, denoted $D_{n,.m}$, divided by $D_n$  as $n\to \infty$. I  wonder if we can still estimate the ratio for large $n$.","Fix $m$. Consider $\lbrace 1,\ldots ,n\rbrace$. Let $a_1\dots a_n$ be a permutation of this set. How many permutations are there such that $a_i\not=i$ for all $i$ and each $i$ travels at most $m$ spots? small valued examples below. Suppose $m=2$. If $n=2$, the only permutation we like is 2,1. If $n=3$, then the permutations we like are of the form $\_\, \_\,\_$ where 2 or 3 can go in slot 1; 1 or 3 can go in slot 2, and 2 or 3 can go in slot 3. This gives a total of 2 permutations. In this simplified setting we cannot resort to the fundamental counting principle due to the constraint-the number of times you can choose each number. Is there a way to generalize this to the set of $n$ elements for any integer $m$? EDIT: I am trying to compute -for a fixed $m$-the limit of the number of such permutations described above, denoted $D_{n,.m}$, divided by $D_n$  as $n\to \infty$. I  wonder if we can still estimate the ratio for large $n$.",,['combinatorics']
30,Using L hopital theorem,Using L hopital theorem,,"I have just begun to read limits . In the second section of the first chapter,L hospital theorem is being used to find the limit of the function  . I have been trying to find its limiting value. I am stuck, I need help on solving this problem. $$\lim_{x\to 0}   \sec{x}^{\sin{2x}}$$","I have just begun to read limits . In the second section of the first chapter,L hospital theorem is being used to find the limit of the function  . I have been trying to find its limiting value. I am stuck, I need help on solving this problem. $$\lim_{x\to 0}   \sec{x}^{\sin{2x}}$$",,"['calculus', 'limits']"
31,Accumulation Points of a Complex Sequence,Accumulation Points of a Complex Sequence,,"Let $z$ be a complex number of absolute value 1: $ z=e^{i\theta},  0 \le \theta \lt 2\pi$.  What are the accumulation points of the sequence $\lbrace z^n \rbrace$?  Distinguish between the case where $\theta$ is a rational multiple of $2\pi$ and the case when it is not.  Be sure to justify all of your assertions. So far I have the following: By DeMoivre's theorem, $z_n=\cos n\theta + i\sin n\theta$. Also, $|z|=1$, so $z_n$ is on the unit circle for all $n$, and any value of $\theta$.  I suppose the thing that is tripping me up is the part about$\theta$ being a rational multiple of $2\pi$.  Is this the case when $\theta = 2\pi p/q$? If so, what difference does it make?","Let $z$ be a complex number of absolute value 1: $ z=e^{i\theta},  0 \le \theta \lt 2\pi$.  What are the accumulation points of the sequence $\lbrace z^n \rbrace$?  Distinguish between the case where $\theta$ is a rational multiple of $2\pi$ and the case when it is not.  Be sure to justify all of your assertions. So far I have the following: By DeMoivre's theorem, $z_n=\cos n\theta + i\sin n\theta$. Also, $|z|=1$, so $z_n$ is on the unit circle for all $n$, and any value of $\theta$.  I suppose the thing that is tripping me up is the part about$\theta$ being a rational multiple of $2\pi$.  Is this the case when $\theta = 2\pi p/q$? If so, what difference does it make?",,"['real-analysis', 'sequences-and-series', 'limits', 'complex-numbers']"
32,$ \lim_{x\rightarrow 0^-} \frac{\operatorname{arccot}(x) - \frac{\pi}{2}}{x}$,, \lim_{x\rightarrow 0^-} \frac{\operatorname{arccot}(x) - \frac{\pi}{2}}{x},"$$\lim_{x\rightarrow 0^-} \frac{\operatorname{arccot}(x) - \frac{\pi}{2}}{x}$$ The title says everything. I already know the limit is $+\infty$, I just want to see how it can be calculated. (Please don't use L'Hôpital's rule, I haven't covered it yet at school)","$$\lim_{x\rightarrow 0^-} \frac{\operatorname{arccot}(x) - \frac{\pi}{2}}{x}$$ The title says everything. I already know the limit is $+\infty$, I just want to see how it can be calculated. (Please don't use L'Hôpital's rule, I haven't covered it yet at school)",,['limits']
33,Finding the asymptote of the function $f(x) = \frac{x^3 -x^2 +2}{x-1}$,Finding the asymptote of the function,f(x) = \frac{x^3 -x^2 +2}{x-1},"Find the asymptote of $f(x)$ as $x \rightarrow \infty$ where $$f(x) = \frac{x^3 -x^2 +2}{x-1}$$ The answer key mentioned that the answer is $h(x)=x^2$ where $h(x)$ is the equation of the asymptote of $f(x)$. I beg to differ with this solution: $$ \begin{align*} f(x) = \frac{x^3 -x^2 +2}{x-1}  &= \frac{x^2 (x-1) +2}{x-1}\\ &=x^2+\frac{2}{x-1} \end{align*} $$ With arrangement, we attempt to fit it in the form $f(x) + h(x) =\frac{2}{x-1}$ and so we have $h(x) = -x^2$. So the asymptote of $f(x)$ as $x \rightarrow \infty$ is $y=-x^2$ Any ideas to solve this?","Find the asymptote of $f(x)$ as $x \rightarrow \infty$ where $$f(x) = \frac{x^3 -x^2 +2}{x-1}$$ The answer key mentioned that the answer is $h(x)=x^2$ where $h(x)$ is the equation of the asymptote of $f(x)$. I beg to differ with this solution: $$ \begin{align*} f(x) = \frac{x^3 -x^2 +2}{x-1}  &= \frac{x^2 (x-1) +2}{x-1}\\ &=x^2+\frac{2}{x-1} \end{align*} $$ With arrangement, we attempt to fit it in the form $f(x) + h(x) =\frac{2}{x-1}$ and so we have $h(x) = -x^2$. So the asymptote of $f(x)$ as $x \rightarrow \infty$ is $y=-x^2$ Any ideas to solve this?",,"['calculus', 'limits']"
34,"Piecewise function $f(x,y)$ - Limits and Continuity",Piecewise function  - Limits and Continuity,"f(x,y)","Consider the function $f : \Bbb R^2 \to \Bbb R$ given by: $$f(x, y) = \begin{cases} 0 &\text{if } x < 0,\\ x\cdot y &\text{if } x \geq 0 \text{ and }y \geq 0,\\-x &\text{if } x \geq 0 \text{ and }y < 0.\end{cases}$$ Describe the properties of this function in terms of limits and continuity. I am struggling to see which limits I should be taking and how to check if $f(x,y)$ is continuous. Any help would be appreciated, thank you.","Consider the function $f : \Bbb R^2 \to \Bbb R$ given by: $$f(x, y) = \begin{cases} 0 &\text{if } x < 0,\\ x\cdot y &\text{if } x \geq 0 \text{ and }y \geq 0,\\-x &\text{if } x \geq 0 \text{ and }y < 0.\end{cases}$$ Describe the properties of this function in terms of limits and continuity. I am struggling to see which limits I should be taking and how to check if $f(x,y)$ is continuous. Any help would be appreciated, thank you.",,"['calculus', 'limits', 'continuity']"
35,Distributional limit of $\phi/n$,Distributional limit of,\phi/n,I have the following problem: Let $\phi(x) \in C_0^\infty(\mathbb{R}^n)$ satisfy $\phi \geq 0$ and $\phi(0) = 1$. Show that $\phi_n = \phi/n$ converge to $0$ in $\mathcal{D}'(\mathbb{R}^n)$. My solution sketch is the following. $\lim_{n->\infty} \int_{\mathbb{R}^n} \frac{\phi(x)}{n} f(x)dx = \lim_{n->\infty} \frac{1}{n}\int_{\mathbb{R}^n} \phi(x) f(x)dx \leq \lim_{n->\infty} \frac{C}{n} = 0$ where I in the last step have used that $\phi$ has compact support so that the integral will be finite. I'm a little confused about this solution since I have not used the addional properties of $\phi$ and therefore I think I have cheated a little. So my question is then what I have missed in my solution? Thanks in advice!,I have the following problem: Let $\phi(x) \in C_0^\infty(\mathbb{R}^n)$ satisfy $\phi \geq 0$ and $\phi(0) = 1$. Show that $\phi_n = \phi/n$ converge to $0$ in $\mathcal{D}'(\mathbb{R}^n)$. My solution sketch is the following. $\lim_{n->\infty} \int_{\mathbb{R}^n} \frac{\phi(x)}{n} f(x)dx = \lim_{n->\infty} \frac{1}{n}\int_{\mathbb{R}^n} \phi(x) f(x)dx \leq \lim_{n->\infty} \frac{C}{n} = 0$ where I in the last step have used that $\phi$ has compact support so that the integral will be finite. I'm a little confused about this solution since I have not used the addional properties of $\phi$ and therefore I think I have cheated a little. So my question is then what I have missed in my solution? Thanks in advice!,,"['limits', 'distribution-theory']"
36,"How to prove $f(x,y) = \frac{2 \log(y) \sqrt{y ( x-y)}}{x \log(x)}$ is bounded by $1$ for $(x,y) \to (0,0) $?",How to prove  is bounded by  for ?,"f(x,y) = \frac{2 \log(y) \sqrt{y ( x-y)}}{x \log(x)} 1 (x,y) \to (0,0) ","Let $$f(x,y) = \frac{2 \log(y) \sqrt{y ( x-y)}}{x \log(x)} $$ with $(x,y) \in D = \{(x,y)\mid 0 < y \leq x \le 1 \}$. How would one show (or disprove)  that $$ \forall \epsilon > 0\ \ \exists \delta>0 \ \ \forall (x,y) \in D \cap B_\delta(0) \quad f(x,y) < 1 + \epsilon $$ where $B_\delta(0)$ is the disk with radius $\delta$ and center $(0,0)$? Please note that this is not just a bound on the limit of the function, as it is not well-defined. Choosing another coordinate $0 \le \theta \le 1$ with $y = \theta x$ we get $$      f(x, \theta x) = \frac{2 (\log(\theta) + \log(x)) \sqrt{\theta (1-\theta)}}{\log(x)}\\ $$ such that $$ \lim_{x \to 0} f(x, \theta x) = 2 \sqrt{\theta (1-\theta)}. $$ So the limit is not well-defined as it depends on how the origin is approached. Also, although the above limit $2 \sqrt{\theta (1-\theta)}$ is indeed bound by 1, it does not directly prove the original question, because $\theta$ is assumed constant with respect to $x$. More specifically, this limit shows (variable ranges omitted for clarity) $$ \forall \epsilon \ \ \forall \theta\ \ \exists \delta \ \ \forall x < \delta \quad f(x,\theta x) < 1 + \epsilon $$ while the original problem corresponds to the stronger (note the different quantifier ordering): $$ \forall \epsilon \ \ \exists \delta \ \ \forall \theta\ \ \forall x < \delta \quad f(x,\theta x) < 1 + \epsilon $$","Let $$f(x,y) = \frac{2 \log(y) \sqrt{y ( x-y)}}{x \log(x)} $$ with $(x,y) \in D = \{(x,y)\mid 0 < y \leq x \le 1 \}$. How would one show (or disprove)  that $$ \forall \epsilon > 0\ \ \exists \delta>0 \ \ \forall (x,y) \in D \cap B_\delta(0) \quad f(x,y) < 1 + \epsilon $$ where $B_\delta(0)$ is the disk with radius $\delta$ and center $(0,0)$? Please note that this is not just a bound on the limit of the function, as it is not well-defined. Choosing another coordinate $0 \le \theta \le 1$ with $y = \theta x$ we get $$      f(x, \theta x) = \frac{2 (\log(\theta) + \log(x)) \sqrt{\theta (1-\theta)}}{\log(x)}\\ $$ such that $$ \lim_{x \to 0} f(x, \theta x) = 2 \sqrt{\theta (1-\theta)}. $$ So the limit is not well-defined as it depends on how the origin is approached. Also, although the above limit $2 \sqrt{\theta (1-\theta)}$ is indeed bound by 1, it does not directly prove the original question, because $\theta$ is assumed constant with respect to $x$. More specifically, this limit shows (variable ranges omitted for clarity) $$ \forall \epsilon \ \ \forall \theta\ \ \exists \delta \ \ \forall x < \delta \quad f(x,\theta x) < 1 + \epsilon $$ while the original problem corresponds to the stronger (note the different quantifier ordering): $$ \forall \epsilon \ \ \exists \delta \ \ \forall \theta\ \ \forall x < \delta \quad f(x,\theta x) < 1 + \epsilon $$",,"['calculus', 'real-analysis', 'limits', 'inequality', 'multivariable-calculus']"
37,"Prove that if $f$ is Riemann-Integrable on $ [0,1]$ then $\lim_{c\rightarrow 0} \int_c^1 f(x)dx$ exists and is equal to $\int_0^1 f(x)dx$",Prove that if  is Riemann-Integrable on  then  exists and is equal to,"f  [0,1] \lim_{c\rightarrow 0} \int_c^1 f(x)dx \int_0^1 f(x)dx","The definition of Riemann-integrability states that if $f$ is Riemann integrable on $[0,1]$ then for any $\epsilon$ there exists a partition $P_\epsilon$ of $[0,1] $ such that for any $P\supset P_\epsilon$ and corresponding choice $T=\{t_i,\dots,t_N\}$, $$\left|\sum_{i=1}^Nf(t_i)(x_i-x_{i-1})-I\right|<\epsilon$$ for some real number $I$. (We denote the above sum by $S(P,T,f,\alpha)$.) A sketch of how I think I should prove this would be to fix $\epsilon>0$ so that we have a partition $P_\epsilon=\{x_0=0,x_1,\dots,x_N=1\}$ and then put $\delta<x_1$, and show that for any $c<\delta$, the Riemann sum with the same mesh size as $P_\epsilon$ is very close to $I$, but I am having a hard time formalizing this and maybe that's because I'm not right. Does this seem like a reasonable approach?","The definition of Riemann-integrability states that if $f$ is Riemann integrable on $[0,1]$ then for any $\epsilon$ there exists a partition $P_\epsilon$ of $[0,1] $ such that for any $P\supset P_\epsilon$ and corresponding choice $T=\{t_i,\dots,t_N\}$, $$\left|\sum_{i=1}^Nf(t_i)(x_i-x_{i-1})-I\right|<\epsilon$$ for some real number $I$. (We denote the above sum by $S(P,T,f,\alpha)$.) A sketch of how I think I should prove this would be to fix $\epsilon>0$ so that we have a partition $P_\epsilon=\{x_0=0,x_1,\dots,x_N=1\}$ and then put $\delta<x_1$, and show that for any $c<\delta$, the Riemann sum with the same mesh size as $P_\epsilon$ is very close to $I$, but I am having a hard time formalizing this and maybe that's because I'm not right. Does this seem like a reasonable approach?",,"['real-analysis', 'integration', 'limits']"
38,$f$ is continuous at $c$ $\implies$ $f$ has a limit at $c$. True?,is continuous at    has a limit at . True?,f c \implies f c,"Further to Another simple/conceptual limit question where I was questioning David Brannan's assertion in his A First Course in Mathematical Analysis that $f(x)=\sqrt x,x\geq 0$ has no limit at $0$ (Example 2c if you type in page 184 in the box on http://www.scribd.com/doc/74564079/Mathematical-Analysis ), I just noticed that in an earlier section he asserted that the same function is continuous on its domain i.e. including at $0$ (Example 3 if you type in page 148 in the box). Does this not violate the well-known theorem (Thm 2 if you type in page 185 in the box) implying that if $f$ is continuous at $c$, then $f$ has a limit at $c$ ? Is David Brannan contradicting himself (by setting up his definition of limits badly)? EDIT:  Thanks, Wisefool. It turns out that there is no contradiction in Brannan's assertions (that the square root function is continuous at 0 yet has no limit at 0) after all, but only because of his peculiar definition of ""limit"" and his analogously peculiar statement of the theorem relating continuity at a point to the limit at that point.","Further to Another simple/conceptual limit question where I was questioning David Brannan's assertion in his A First Course in Mathematical Analysis that $f(x)=\sqrt x,x\geq 0$ has no limit at $0$ (Example 2c if you type in page 184 in the box on http://www.scribd.com/doc/74564079/Mathematical-Analysis ), I just noticed that in an earlier section he asserted that the same function is continuous on its domain i.e. including at $0$ (Example 3 if you type in page 148 in the box). Does this not violate the well-known theorem (Thm 2 if you type in page 185 in the box) implying that if $f$ is continuous at $c$, then $f$ has a limit at $c$ ? Is David Brannan contradicting himself (by setting up his definition of limits badly)? EDIT:  Thanks, Wisefool. It turns out that there is no contradiction in Brannan's assertions (that the square root function is continuous at 0 yet has no limit at 0) after all, but only because of his peculiar definition of ""limit"" and his analogously peculiar statement of the theorem relating continuity at a point to the limit at that point.",,"['real-analysis', 'limits', 'functions', 'continuity']"
39,Limiting distribution,Limiting distribution,,"Let $(q_n)_{n>0}$ be a real sequence such that $0<q_n<1$ for all $n>0$ and $\lim_{n\to \infty} q_n = 0$. For each $n > 0$, let $X_n$ be a random variable, such that $P[X_n =k]=q_n(1−q_n)^{k−1}, (k=1,2,...)$. Prove that the limit distribution of $\frac{X_n}{\mathbb{E}[X_n]}$ ￼is exponential with parameter 1. I see that $\mathbb{E}[X_n] = \frac{1}{q_n}$ but after that I don't really know where to go from there. Are there any tips please?","Let $(q_n)_{n>0}$ be a real sequence such that $0<q_n<1$ for all $n>0$ and $\lim_{n\to \infty} q_n = 0$. For each $n > 0$, let $X_n$ be a random variable, such that $P[X_n =k]=q_n(1−q_n)^{k−1}, (k=1,2,...)$. Prove that the limit distribution of $\frac{X_n}{\mathbb{E}[X_n]}$ ￼is exponential with parameter 1. I see that $\mathbb{E}[X_n] = \frac{1}{q_n}$ but after that I don't really know where to go from there. Are there any tips please?",,"['probability', 'limits', 'probability-distributions']"
40,Two Real Analysis Questions,Two Real Analysis Questions,,"If I have $ A = \{a \in \ell_2 : |a(n)| \leqslant c(n)\}$ for $c(n)\geqslant 0$ where $ n \in N $, and I want to show that is $A$ compact in $\ell_2$ iff $\sum{c(n)^2}<\infty$. How do I go about showing both directions? If $f \in C(T)$ is the $1$-periodic continuous functions in $\Bbb R$, how to show $\lim \limits_{|n|\to\infty}\int_0^1 e^{-2\pi inx}f(x)=0 ?$ Also is this true if $f$ were in the closure of the set of 1-periodic step functions in $R$ Intuitively, I think the later is false since boundedness does not imply continuity.","If I have $ A = \{a \in \ell_2 : |a(n)| \leqslant c(n)\}$ for $c(n)\geqslant 0$ where $ n \in N $, and I want to show that is $A$ compact in $\ell_2$ iff $\sum{c(n)^2}<\infty$. How do I go about showing both directions? If $f \in C(T)$ is the $1$-periodic continuous functions in $\Bbb R$, how to show $\lim \limits_{|n|\to\infty}\int_0^1 e^{-2\pi inx}f(x)=0 ?$ Also is this true if $f$ were in the closure of the set of 1-periodic step functions in $R$ Intuitively, I think the later is false since boundedness does not imply continuity.",,"['real-analysis', 'functional-analysis', 'limits', 'compactness', 'lp-spaces']"
41,A Question on Using a Half-Divergent Sequence.,A Question on Using a Half-Divergent Sequence.,,"$\theta$ is an irrational in $[0,1]$ with continued fraction representation $[0;a_1,a_2,\dots]$, and the sequences $(a_k), (n_k)$ are related by the recurrence relation $n_{k+1}=a_{k+1}n_k+n_{k-1},  n_0=1, n_{-1}=0$. They are also related by the fact that there is a $\delta>0$ for which $n_k^\delta<a_k<2n_k^\delta$. Suppose $(a_k)$ is half-divergent (see $(*)$ below for my definition). Suppose that for any $i$ I have $$ s\ge \frac{\log_{a_{k_i+1}}}{n_{k_{i+1}}-n_{k_i}},$$ where $n_{k_{i+1}}-n_{k_i}\to \infty$ and $(a_{k_i})$ is a subsequence of $(a_k)$ Hence $$s\ge\limsup_{i\to \infty} \frac{\log(a_{k_i+1})}{n_{k_{i+1}}-n_{k_i}}$$ Since $(a_k)$ is half-divergent, it diverges on any subsequence where it is unbounded.  My question: Does this imply that for any $\varepsilon \le s$, I can choose a subsequence of $(a_k)$ for which $$\varepsilon \le \limsup_{i\to \infty} \frac{\log(a_{k_i+1})}{ n_{k_{i+1}}- n_{k_i}}\le s?$$ If so, is there an effective way to choose the subsequence? $(*)$ A sequence $(a_k)$ is half-divergent if $\exists M\in \mathbb{R}$ such that $\forall N>M, \exists k_0$ such that $k>k_0$ implies that either $a_{k+1}\le M$ or $a_{k+1}>N$","$\theta$ is an irrational in $[0,1]$ with continued fraction representation $[0;a_1,a_2,\dots]$, and the sequences $(a_k), (n_k)$ are related by the recurrence relation $n_{k+1}=a_{k+1}n_k+n_{k-1},  n_0=1, n_{-1}=0$. They are also related by the fact that there is a $\delta>0$ for which $n_k^\delta<a_k<2n_k^\delta$. Suppose $(a_k)$ is half-divergent (see $(*)$ below for my definition). Suppose that for any $i$ I have $$ s\ge \frac{\log_{a_{k_i+1}}}{n_{k_{i+1}}-n_{k_i}},$$ where $n_{k_{i+1}}-n_{k_i}\to \infty$ and $(a_{k_i})$ is a subsequence of $(a_k)$ Hence $$s\ge\limsup_{i\to \infty} \frac{\log(a_{k_i+1})}{n_{k_{i+1}}-n_{k_i}}$$ Since $(a_k)$ is half-divergent, it diverges on any subsequence where it is unbounded.  My question: Does this imply that for any $\varepsilon \le s$, I can choose a subsequence of $(a_k)$ for which $$\varepsilon \le \limsup_{i\to \infty} \frac{\log(a_{k_i+1})}{ n_{k_{i+1}}- n_{k_i}}\le s?$$ If so, is there an effective way to choose the subsequence? $(*)$ A sequence $(a_k)$ is half-divergent if $\exists M\in \mathbb{R}$ such that $\forall N>M, \exists k_0$ such that $k>k_0$ implies that either $a_{k+1}\le M$ or $a_{k+1}>N$",,['limits']
42,Proving $\lim_{x\to x_0}f(x)$ with epsilon delta definition,Proving  with epsilon delta definition,\lim_{x\to x_0}f(x),"I've asked the question below before with no answer, but I would like to stress that this time it is not a homework question (and also that I've spent hours trying to come up with a solution). This is the question: Let f be a function defined around $x_o$.    For every $\epsilon>0$ there's some $\delta>0$ such that if $0<|x-x_0|<\delta$ and $0<|y-x_0|<\delta$ then $|f(x)-f(y)|<\epsilon$. And what's needed to be proven is that $\lim_{x\to x_0}f(x)$ exists. I've been told that there are two ways to do so: One is quite easy and requires Cauchy sequences (I haven't learned sequences yet, but I think I'll look it up sometime soon and try to solve it this way). The second way is a direct way, which I've been told is cumbersome and unrecommended, but since this is the way I tried solving it so far, I am really curious as to how the proof goes and this is the way I'm asking about. I tried applying all kinds of inequalities but with no success. Even a little hint/direction would be swell. Thank you in advance.","I've asked the question below before with no answer, but I would like to stress that this time it is not a homework question (and also that I've spent hours trying to come up with a solution). This is the question: Let f be a function defined around $x_o$.    For every $\epsilon>0$ there's some $\delta>0$ such that if $0<|x-x_0|<\delta$ and $0<|y-x_0|<\delta$ then $|f(x)-f(y)|<\epsilon$. And what's needed to be proven is that $\lim_{x\to x_0}f(x)$ exists. I've been told that there are two ways to do so: One is quite easy and requires Cauchy sequences (I haven't learned sequences yet, but I think I'll look it up sometime soon and try to solve it this way). The second way is a direct way, which I've been told is cumbersome and unrecommended, but since this is the way I tried solving it so far, I am really curious as to how the proof goes and this is the way I'm asking about. I tried applying all kinds of inequalities but with no success. Even a little hint/direction would be swell. Thank you in advance.",,['limits']
43,"Limits with trig, log functions and variable exponents","Limits with trig, log functions and variable exponents",,Would someone mind verifying this? $$ \lim_{x\to \infty} \frac{2 \cdot 3^{5x} + 5}{3^{5x} + 2^{5x}} = \lim_{x\to \infty} \frac{3^{5x}(2 + \frac{5}{3^{5x}})}{3^{5x}(1 + (\frac{2}{3})^{5x})} = \lim_{x\to \infty} \frac{2 + \frac{5}{3^{5x}}}{1 + (\frac{2}{3})^{5x}} = \frac{2 + \frac{5}{3^{5(\infty)}}}{1 + (\frac{2}{3})^{5(\infty)}} = \frac{2 + 0}{1 + 0} = 2 $$ $$ \lim_{x\to 0} \frac{e^{2x}-\pi^{x}}{sin(3x)} = \lim_{x\to 0} \frac{2e^{2x}-\pi^{x} ln(\pi)}{3cos(3x)} = \frac{2e^{2(0)}-\pi^{(0)} ln(\pi)}{3cos(3(0))} = \frac{2\cdot 1 - 1\cdot ln(\pi)}{3\cdot1} = \frac{2-ln(\pi)}{3} $$,Would someone mind verifying this? $$ \lim_{x\to \infty} \frac{2 \cdot 3^{5x} + 5}{3^{5x} + 2^{5x}} = \lim_{x\to \infty} \frac{3^{5x}(2 + \frac{5}{3^{5x}})}{3^{5x}(1 + (\frac{2}{3})^{5x})} = \lim_{x\to \infty} \frac{2 + \frac{5}{3^{5x}}}{1 + (\frac{2}{3})^{5x}} = \frac{2 + \frac{5}{3^{5(\infty)}}}{1 + (\frac{2}{3})^{5(\infty)}} = \frac{2 + 0}{1 + 0} = 2 $$ $$ \lim_{x\to 0} \frac{e^{2x}-\pi^{x}}{sin(3x)} = \lim_{x\to 0} \frac{2e^{2x}-\pi^{x} ln(\pi)}{3cos(3x)} = \frac{2e^{2(0)}-\pi^{(0)} ln(\pi)}{3cos(3(0))} = \frac{2\cdot 1 - 1\cdot ln(\pi)}{3\cdot1} = \frac{2-ln(\pi)}{3} $$,,"['calculus', 'limits']"
44,Proving that $\limsup a_n$ and $\liminf a_n$ are subsequential limits of $\{a_n\}$,Proving that  and  are subsequential limits of,\limsup a_n \liminf a_n \{a_n\},"Prove that if $\{a_n\}$ is a sequence, then $\limsup a_n$ and $\liminf a_n$ are subsequential limits of $\{a_n\}$. I don't know the case where $\limsup a_n = \infty$.","Prove that if $\{a_n\}$ is a sequence, then $\limsup a_n$ and $\liminf a_n$ are subsequential limits of $\{a_n\}$. I don't know the case where $\limsup a_n = \infty$.",,"['sequences-and-series', 'analysis', 'limits', 'limsup-and-liminf']"
45,$\lim _{x\rightarrow\infty} \left(\sqrt{(1+ab)(1+ab+(1-a)cx^{-d})}-\sqrt{ab(ab+(1-a)cx^{-d})}\right)^{-x}$,,\lim _{x\rightarrow\infty} \left(\sqrt{(1+ab)(1+ab+(1-a)cx^{-d})}-\sqrt{ab(ab+(1-a)cx^{-d})}\right)^{-x},"I am interested in the limit as $x\rightarrow\infty$ of the following function: $$f(x)=\left(\sqrt{(1+ab)(1+ab+(1-a)cx^{-d})}-\sqrt{ab(ab+(1-a)cx^{-d})}\right)^{-x}$$ Here $0<a<1$, $0<b\ll 1$ and $0<c\ll 1$ are small positive constants, and $d$ is a positive variable. I did some numerical evaluations of this expression, and it seems to me that for $0<d<1/2$, $\lim_{x\rightarrow\infty}f(x)=0$; for $d=1/2$, $\lim_{x\rightarrow\infty}f(x)=e^{g(a,b,c)}$; and for $d>1/2$, $\lim_{x\rightarrow\infty}f(x)=1$.  But I am stuck trying to prove that. I tried putting the expression into the following form: $$f(x)=\left(\sqrt{(1+ab)(1+ab+(1-a)cx^{-d})}\left(1-\sqrt{\frac{ab(ab+(1-a)cx^{-d})}{(1+ab)(1+ab+(1-a)cx^{-d})}}\right)\right)^{-x}$$ and then applying the usual method of taking its log to obtain the following: $$\begin{array}{rcl}\log f(x)&=&-\frac{x}{2}\left(\log(1+ab)+\log(1+ab+(1-a)cx^{-d})\right)\\ & &-x\log\left(1-\sqrt{\frac{ab(ab+(1-a)cx^{-d})}{(1+ab)(1+ab+(1-a)cx^{-d})}}\right)\\ &\approx&-\frac{x}{2}\left(2ab+(1-a)cx^{-d}\right)+x\sqrt{\frac{ab(ab+(1-a)cx^{-d})}{(1+ab)(1+ab+(1-a)cx^{-d})}}\end{array}$$ but I don't know what to do with the square root... Any hints would be appreciated.","I am interested in the limit as $x\rightarrow\infty$ of the following function: $$f(x)=\left(\sqrt{(1+ab)(1+ab+(1-a)cx^{-d})}-\sqrt{ab(ab+(1-a)cx^{-d})}\right)^{-x}$$ Here $0<a<1$, $0<b\ll 1$ and $0<c\ll 1$ are small positive constants, and $d$ is a positive variable. I did some numerical evaluations of this expression, and it seems to me that for $0<d<1/2$, $\lim_{x\rightarrow\infty}f(x)=0$; for $d=1/2$, $\lim_{x\rightarrow\infty}f(x)=e^{g(a,b,c)}$; and for $d>1/2$, $\lim_{x\rightarrow\infty}f(x)=1$.  But I am stuck trying to prove that. I tried putting the expression into the following form: $$f(x)=\left(\sqrt{(1+ab)(1+ab+(1-a)cx^{-d})}\left(1-\sqrt{\frac{ab(ab+(1-a)cx^{-d})}{(1+ab)(1+ab+(1-a)cx^{-d})}}\right)\right)^{-x}$$ and then applying the usual method of taking its log to obtain the following: $$\begin{array}{rcl}\log f(x)&=&-\frac{x}{2}\left(\log(1+ab)+\log(1+ab+(1-a)cx^{-d})\right)\\ & &-x\log\left(1-\sqrt{\frac{ab(ab+(1-a)cx^{-d})}{(1+ab)(1+ab+(1-a)cx^{-d})}}\right)\\ &\approx&-\frac{x}{2}\left(2ab+(1-a)cx^{-d}\right)+x\sqrt{\frac{ab(ab+(1-a)cx^{-d})}{(1+ab)(1+ab+(1-a)cx^{-d})}}\end{array}$$ but I don't know what to do with the square root... Any hints would be appreciated.",,['limits']
46,Need a tip/hint evaluating a limit,Need a tip/hint evaluating a limit,,"I have the following limit: $$\lim_{x\rightarrow\infty}\left(1+\frac{a}{x^{1/2+\epsilon}}\left(1-\exp\left(-\frac{b}{x^{1/2+\epsilon}}\right)\right)\ln\left(\frac{a}{x^{1/2+\epsilon}}\right)\right)^x$$ where $0<b<a$. I care for the case where $\epsilon>-1/2$. I suspect that for $\epsilon>0$ this limit evaluates to 1, and for $-1/2<\epsilon\leq0$ it evaluates to 0.  However, I am having hard time evaluating this.  I have tried taking the log of the expression (moving the $x$ in the exponent down), substituting $y=1/x$ and then Taylor-expanding the log, but didn't get anywhere. Does anyone have any tips/hints that might help me evaluate this?","I have the following limit: $$\lim_{x\rightarrow\infty}\left(1+\frac{a}{x^{1/2+\epsilon}}\left(1-\exp\left(-\frac{b}{x^{1/2+\epsilon}}\right)\right)\ln\left(\frac{a}{x^{1/2+\epsilon}}\right)\right)^x$$ where $0<b<a$. I care for the case where $\epsilon>-1/2$. I suspect that for $\epsilon>0$ this limit evaluates to 1, and for $-1/2<\epsilon\leq0$ it evaluates to 0.  However, I am having hard time evaluating this.  I have tried taking the log of the expression (moving the $x$ in the exponent down), substituting $y=1/x$ and then Taylor-expanding the log, but didn't get anywhere. Does anyone have any tips/hints that might help me evaluate this?",,['limits']
47,escape velocity using limits,escape velocity using limits,,"I have the formula for a rocket's escape velocity from earth, $V$ being velocity, $v$ being initial velocity, and $r$ being the distance between the rocket and the center of the earth. $$V = \sqrt{\frac{192000}{r}+v^2-48}$$ I am trying to find the value of $v$ for which an infinite limit for $r$ is obtained as $V$ approaches zero, this value of $v$ being the escape velocity for earth. I have solved for $v$ (with $V$ being $0$), as $v = \sqrt{48-\frac{192000}{r}}$, but do not know how to continue solving the problem. I thought setting it up as the limit of the square root of $48-\frac{192000}{r}$ as $r$ approaches infinity (to give $v$), but that doesn't seem right.","I have the formula for a rocket's escape velocity from earth, $V$ being velocity, $v$ being initial velocity, and $r$ being the distance between the rocket and the center of the earth. $$V = \sqrt{\frac{192000}{r}+v^2-48}$$ I am trying to find the value of $v$ for which an infinite limit for $r$ is obtained as $V$ approaches zero, this value of $v$ being the escape velocity for earth. I have solved for $v$ (with $V$ being $0$), as $v = \sqrt{48-\frac{192000}{r}}$, but do not know how to continue solving the problem. I thought setting it up as the limit of the square root of $48-\frac{192000}{r}$ as $r$ approaches infinity (to give $v$), but that doesn't seem right.",,"['calculus', 'limits', 'physics']"
48,Limit with functionals,Limit with functionals,,"I need to evaluate the following limit which intuitively I know its equal to 0 but I can't really prove it, so I need some help: $$\lim_{\epsilon \to 0}{\frac{F[\rho + \epsilon\rho' + \epsilon^2\rho'']-F[\rho + \epsilon\rho']}{\epsilon}}$$ where $\epsilon$ is a real number, $F$ is a functional and $\rho$, $\rho'$ and $\rho''$ are functions in some function space.","I need to evaluate the following limit which intuitively I know its equal to 0 but I can't really prove it, so I need some help: $$\lim_{\epsilon \to 0}{\frac{F[\rho + \epsilon\rho' + \epsilon^2\rho'']-F[\rho + \epsilon\rho']}{\epsilon}}$$ where $\epsilon$ is a real number, $F$ is a functional and $\rho$, $\rho'$ and $\rho''$ are functions in some function space.",,"['limits', 'calculus-of-variations']"
49,On the limit of a sequence,On the limit of a sequence,,"My homework asks me to calculate (if it exists) the following limit: $$\lim_{n\to\infty}{\frac{(1+(-1)^n)^n}{n}}$$ My thinking is: $(-1)^n$ would, as we all know, oscillate between 1 and -1, meaning that $(1+(-1)^n)$ would be either $0$ or $2$. Thus, for all odd cases: $$\lim_{n\to\infty}{\frac{0^n}{n}}=0$$ And then, for all even cases: $$\lim_{n\to\infty}{\frac{2^n}{n}}$$ Using Cauchy: $$\lim_{n\to\infty}{^n\sqrt{\frac{2^n}{n}}}$$ $$\lim_{n\to\infty}{\frac{^n\sqrt{2^n}}{^n\sqrt{n}}}$$ $$\lim_{n\to\infty}{\frac{2}{1}} = 2$$ And then, it follows that $$\lim_{n\to\infty}{\frac{2^n}{n}} = \infty$$ Which means that our original expression... has no limit?","My homework asks me to calculate (if it exists) the following limit: $$\lim_{n\to\infty}{\frac{(1+(-1)^n)^n}{n}}$$ My thinking is: $(-1)^n$ would, as we all know, oscillate between 1 and -1, meaning that $(1+(-1)^n)$ would be either $0$ or $2$. Thus, for all odd cases: $$\lim_{n\to\infty}{\frac{0^n}{n}}=0$$ And then, for all even cases: $$\lim_{n\to\infty}{\frac{2^n}{n}}$$ Using Cauchy: $$\lim_{n\to\infty}{^n\sqrt{\frac{2^n}{n}}}$$ $$\lim_{n\to\infty}{\frac{^n\sqrt{2^n}}{^n\sqrt{n}}}$$ $$\lim_{n\to\infty}{\frac{2}{1}} = 2$$ And then, it follows that $$\lim_{n\to\infty}{\frac{2^n}{n}} = \infty$$ Which means that our original expression... has no limit?",,"['sequences-and-series', 'limits']"
50,Recursive sequence and a quadratic equation related inequality proof,Recursive sequence and a quadratic equation related inequality proof,,"I am trying to show that if a sequence of number $x_{n}$ is defined by $x_1 = h$, $x_{n+1}=x_n^2 + k$, where $0<k<\frac{1}{4}$ and $h$ lies between the roots $a$ and $b$ of the equation $$x^2 -x +k = 0$$ Then show that $$a < x_{n+1}<x_n<b$$ and i am also interested in evaluating the limit of $x_n$. Analysis towards a solution I suspect that geometrically this sequence may have tendencies to converge or intersect  this quadratic equation's parabola although i am unsure how to exploit this hunch. What else do I know $$x^2 -x +k = 0 = (x-a)(x-b)$$ hence $a + b =1$ and $0 < k = ab < \frac{1}{4}$ Although i am unsure how to proceed from here any help would be much appreciated.","I am trying to show that if a sequence of number $x_{n}$ is defined by $x_1 = h$, $x_{n+1}=x_n^2 + k$, where $0<k<\frac{1}{4}$ and $h$ lies between the roots $a$ and $b$ of the equation $$x^2 -x +k = 0$$ Then show that $$a < x_{n+1}<x_n<b$$ and i am also interested in evaluating the limit of $x_n$. Analysis towards a solution I suspect that geometrically this sequence may have tendencies to converge or intersect  this quadratic equation's parabola although i am unsure how to exploit this hunch. What else do I know $$x^2 -x +k = 0 = (x-a)(x-b)$$ hence $a + b =1$ and $0 < k = ab < \frac{1}{4}$ Although i am unsure how to proceed from here any help would be much appreciated.",,"['real-analysis', 'geometry', 'sequences-and-series', 'limits', 'quadratics']"
51,Why doesn't integrating infinitesimally small likelihoods work in this sense?,Why doesn't integrating infinitesimally small likelihoods work in this sense?,,"We know that something is going to happen after $x$ amount of time, but the exact time at which the event occurs is random within $x$ time. (Like, say we did it a bunch of times where it happened in $x-y$ time, and all the $y$s were uniformly distributed reals $0$ to $x$.) It is easy to see that you can compute the likelihood that the event has occurred by using $t/x$ with $t$ being the time that has elapsed and $x$ being the total amount of time. It's also easy to see the likelihood of it occurring in the next $n$ amount of time by computing $n/(x-t)$ (With $t$ again being the amount of time elapsed) As $n$ gets very small, the likelihood of it happening in the next $n$ amount of time also clearly also becomes very small. This can be expressed by realizing that: $$\lim_{n \to 0}\frac{n}{x-t}=0$$ But now, if you take that limit and integrate it over some period of $t$ s.t. $0<t<x$, you should get $t/x$ again, since we're essentially adding up all the likelihoods of it happening at each instant in the interval: $$\int_{0}^{x-a} \lim_{n \to 0} \frac{n}{x-t} dt$$ Resolving the limit first clearly doesn't yield. Does that mean that the integral I have constructed does not represent what I think it does? -- I considered using arclength in the integral but stopped myself in realizing that we are concerned with the values the function outputs, not the length of the curve.","We know that something is going to happen after $x$ amount of time, but the exact time at which the event occurs is random within $x$ time. (Like, say we did it a bunch of times where it happened in $x-y$ time, and all the $y$s were uniformly distributed reals $0$ to $x$.) It is easy to see that you can compute the likelihood that the event has occurred by using $t/x$ with $t$ being the time that has elapsed and $x$ being the total amount of time. It's also easy to see the likelihood of it occurring in the next $n$ amount of time by computing $n/(x-t)$ (With $t$ again being the amount of time elapsed) As $n$ gets very small, the likelihood of it happening in the next $n$ amount of time also clearly also becomes very small. This can be expressed by realizing that: $$\lim_{n \to 0}\frac{n}{x-t}=0$$ But now, if you take that limit and integrate it over some period of $t$ s.t. $0<t<x$, you should get $t/x$ again, since we're essentially adding up all the likelihoods of it happening at each instant in the interval: $$\int_{0}^{x-a} \lim_{n \to 0} \frac{n}{x-t} dt$$ Resolving the limit first clearly doesn't yield. Does that mean that the integral I have constructed does not represent what I think it does? -- I considered using arclength in the integral but stopped myself in realizing that we are concerned with the values the function outputs, not the length of the curve.",,"['calculus', 'probability', 'statistics', 'limits']"
52,"Proving divergence of a series, $\sum a_n/(z+n)$, $a_n=\pm1$, alternating $p+q$ negative terms with $p$ positive terms","Proving divergence of a series, , , alternating  negative terms with  positive terms",\sum a_n/(z+n) a_n=\pm1 p+q p,"We observe that the series $$\dfrac {1} {z} -\dfrac {1} {z+1}+\dfrac {1} {z+2}- \dfrac {1} {z+3}+\ldots $$ is conditionally convergent, except for certain exceptional values of $z$ ($z\in\mathbb{C}\setminus{{-\infty,\infty}}$ interpreted via ratio test), but the series $$\dfrac {1} {z}+\dfrac {1} {z+1}+\ldots +\dfrac {1} {z+p-1}-\dfrac {1} {z+p}-\dfrac {1} {z+p+1}-.\ldots -\dfrac {1} {z+2p+q-1}+\dfrac {1} {z+2p+ q} +\ldots $$ in which $(p + q)$ negative terms always follow $p$ positive terms, is divergent. The second series i think can be rewritten as $$\sum _{t=0}^{t=\infty }\left(\sum _{n=t\left( 2p+q\right)}^{n=t\left( 2p+q\right) + \left( p-1\right) }\dfrac {1} {z+n}-\sum _{n=t\left( 2p+q\right) + p}^{n=t\left( 2p+q\right) + \left(p+q-1\right) }\dfrac {1} {z+n}\right)$$ but i am not sure how to proceed forward to prove this statement from here. Any help would be much appreciated.","We observe that the series $$\dfrac {1} {z} -\dfrac {1} {z+1}+\dfrac {1} {z+2}- \dfrac {1} {z+3}+\ldots $$ is conditionally convergent, except for certain exceptional values of $z$ ($z\in\mathbb{C}\setminus{{-\infty,\infty}}$ interpreted via ratio test), but the series $$\dfrac {1} {z}+\dfrac {1} {z+1}+\ldots +\dfrac {1} {z+p-1}-\dfrac {1} {z+p}-\dfrac {1} {z+p+1}-.\ldots -\dfrac {1} {z+2p+q-1}+\dfrac {1} {z+2p+ q} +\ldots $$ in which $(p + q)$ negative terms always follow $p$ positive terms, is divergent. The second series i think can be rewritten as $$\sum _{t=0}^{t=\infty }\left(\sum _{n=t\left( 2p+q\right)}^{n=t\left( 2p+q\right) + \left( p-1\right) }\dfrac {1} {z+n}-\sum _{n=t\left( 2p+q\right) + p}^{n=t\left( 2p+q\right) + \left(p+q-1\right) }\dfrac {1} {z+n}\right)$$ but i am not sure how to proceed forward to prove this statement from here. Any help would be much appreciated.",,"['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
53,What's wrong with this limit?,What's wrong with this limit?,,"Let's say we have this limit: $$\lim\limits_{x\to \infty} \frac{1}{x}$$ which is clearly  $$\lim\limits_{x\to \infty} \frac{1}{x} = 0.$$ From there, to prove it we should: $$\left\lvert \frac{1}{x} - 0 \right\rvert < \epsilon$$  (with $\epsilon > 0$ and small). To solve that inequality we should deal with a system of: $$\begin{align*} \frac{1}{x} &\lt \epsilon&&\text{(for }\frac{1}{x} \gt 0\text{)}\\ \frac{1}{x} &\gt -\epsilon&&\text{(for }\frac{1}{x}\lt 0\text{)} \end{align*}$$ Then from the () we have that the first inequality is for $x < 0$ and the second is for $x > 0$. Is this right?","Let's say we have this limit: $$\lim\limits_{x\to \infty} \frac{1}{x}$$ which is clearly  $$\lim\limits_{x\to \infty} \frac{1}{x} = 0.$$ From there, to prove it we should: $$\left\lvert \frac{1}{x} - 0 \right\rvert < \epsilon$$  (with $\epsilon > 0$ and small). To solve that inequality we should deal with a system of: $$\begin{align*} \frac{1}{x} &\lt \epsilon&&\text{(for }\frac{1}{x} \gt 0\text{)}\\ \frac{1}{x} &\gt -\epsilon&&\text{(for }\frac{1}{x}\lt 0\text{)} \end{align*}$$ Then from the () we have that the first inequality is for $x < 0$ and the second is for $x > 0$. Is this right?",,[]
54,Use the definition of a limit to show that $\lim_{z \to z_0} (az + b) = az_0 + b.$,Use the definition of a limit to show that,\lim_{z \to z_0} (az + b) = az_0 + b.,"Let $a,b, z_0$ denote complex constants. Use the definition of a limit to show that    $$\lim_{z \to z_0} (az + b) = az_0 + b.$$ Here is what I have done: \begin{align*} |az + b - (az_0 + b)| &= |az - az_0 + b - b|\\ &= |a(z - z_0)|\\ &= |a||z - z_0|. \end{align*} So for a positive number $\epsilon$, $$|az + b - (az_0 + b)| < \epsilon \text{ whenever } |a||z - z_0| < \epsilon$$ or in other words $|az + b - (az_0 + b)| < \epsilon$ whenever $|z - z_0| < \delta$ where $\delta = \epsilon/|a|$. Have I proved the statement correctly?","Let $a,b, z_0$ denote complex constants. Use the definition of a limit to show that    $$\lim_{z \to z_0} (az + b) = az_0 + b.$$ Here is what I have done: \begin{align*} |az + b - (az_0 + b)| &= |az - az_0 + b - b|\\ &= |a(z - z_0)|\\ &= |a||z - z_0|. \end{align*} So for a positive number $\epsilon$, $$|az + b - (az_0 + b)| < \epsilon \text{ whenever } |a||z - z_0| < \epsilon$$ or in other words $|az + b - (az_0 + b)| < \epsilon$ whenever $|z - z_0| < \delta$ where $\delta = \epsilon/|a|$. Have I proved the statement correctly?",,"['complex-analysis', 'limits', 'proof-verification']"
55,Differentiablility of a function of two variables,Differentiablility of a function of two variables,,"Here is a problem from an old comprehensive exam that I am trying to solve Problem: let $f:\mathbb{R}^{2} \to \mathbb{R}$ be a function defined as follows: $f\left ( x,y \right )=\frac{\left ( x^{2}-y \right ).y^{2}}{x^{4}+y^{2}}$ if  $\left ( x,y \right )\neq \left ( 0,0 \right )$ $f\left ( x,y \right )=0$ if $\left ( x,y \right )=\left ( 0,0 \right )$ The question is : Investigate the differentiability of $f$ at the point $\left ( 0,0 \right )$ Here is what I did so far: $\frac{\partial f}{\partial x}\left ( 0,0 \right )=0  $ and $\frac{\partial f}{\partial y}\left ( 0,0 \right )=-1  $ Now I applied the condition for differentiability for $f$ at the point $\left ( 0,0 \right )$: $lim_{\left ( x,y \right ) \to \left ( 0,0 \right )}\frac{\left \| f\left ( x,y \right )-f\left ( 0,0 \right )-\bigtriangledown f\left ( 0,0 \right ).\left ( x,y \right ) \right \|}{\sqrt{x^{2}+y^{2}}}$ has to be $0$ if $f$ is differentiable at the desired point. After simplifying the above limit, I got the following limit: $lim_{\left ( x,y \right ) \to \left ( 0,0 \right ) }\frac{x^{2}y^{2}+x^{4}y}{\left ( x^{2}+y^{4} \right ).\sqrt{x^{2}+y^{2}}}$ . Here is where I am stuck. I cannot evaluate this limit. I tried everything, like evaluating the limit through different paths... but nothing seems to work out for me. Any help please on how to finish my proof?","Here is a problem from an old comprehensive exam that I am trying to solve Problem: let $f:\mathbb{R}^{2} \to \mathbb{R}$ be a function defined as follows: $f\left ( x,y \right )=\frac{\left ( x^{2}-y \right ).y^{2}}{x^{4}+y^{2}}$ if  $\left ( x,y \right )\neq \left ( 0,0 \right )$ $f\left ( x,y \right )=0$ if $\left ( x,y \right )=\left ( 0,0 \right )$ The question is : Investigate the differentiability of $f$ at the point $\left ( 0,0 \right )$ Here is what I did so far: $\frac{\partial f}{\partial x}\left ( 0,0 \right )=0  $ and $\frac{\partial f}{\partial y}\left ( 0,0 \right )=-1  $ Now I applied the condition for differentiability for $f$ at the point $\left ( 0,0 \right )$: $lim_{\left ( x,y \right ) \to \left ( 0,0 \right )}\frac{\left \| f\left ( x,y \right )-f\left ( 0,0 \right )-\bigtriangledown f\left ( 0,0 \right ).\left ( x,y \right ) \right \|}{\sqrt{x^{2}+y^{2}}}$ has to be $0$ if $f$ is differentiable at the desired point. After simplifying the above limit, I got the following limit: $lim_{\left ( x,y \right ) \to \left ( 0,0 \right ) }\frac{x^{2}y^{2}+x^{4}y}{\left ( x^{2}+y^{4} \right ).\sqrt{x^{2}+y^{2}}}$ . Here is where I am stuck. I cannot evaluate this limit. I tried everything, like evaluating the limit through different paths... but nothing seems to work out for me. Any help please on how to finish my proof?",,"['real-analysis', 'analysis', 'limits', 'multivariable-calculus']"
56,liminf in terms of the point-to-set distance,liminf in terms of the point-to-set distance,,"Let $\mathcal{X}$ be a normed space and $C\subseteq \mathcal{X}$. We define the point-to-set distance for the set $C$ to be: $$ d_C:\mathcal{X}\ni x \mapsto d_c(x):= \inf_{y\in C}\|x-y\| \in [0,\infty] $$ Additionally, we define the inner limit of a sequence of sets $C_n$ in $\mathcal{X}$ to be: $$ \liminf_n C_n = \bigcup_{n=1}^\infty \bigcap_{m=n}^\infty C_m $$ This definition is equivalent to: $$ \liminf_n C_n = \left\{x \in \mathcal{X} | x\in C_k \text{ ultimately for all } k \right\} $$ My initial question was: I need to prove that: $$ \liminf_n C_n = \left\{ x \in \mathcal{X} | \liminf_{n\to\infty} \left(d_{C_n}(x)\right)=0\right\} $$ But this is not true! The following facts hold true: $\liminf_n C_n \subseteq \left\{ x \in \mathcal{X} | \liminf_{n\to\infty} \left(d_{C_n}(x)\right)=0\right\} = \limsup_n C_n$ $\liminf_n C_n \subseteq \left\{ x \in \mathcal{X} | \lim_{n\to\infty} \left(d_{C_n}(x)\right)=0\right\}$ Edit 1. The meaning of ""ultimately for all $k$"" should be interpreted as follows: $$ \liminf_{n\to\infty} C_n = \{x | \forall k\in\mathbb{N} \exists x_k\in C_{n_k}:\ x_k\to x\} $$ where $n_k\in\mathbb{N}$ is a strictly increasing sequence of indices (i.e. $\{C_{n_k}\}_{k\in\mathbb{N}}$ is some subsequence of $\{C_n\}_{n\in\mathbb{N}}$). Edit 2. I changed the formula with the closure which was wrong according to Tim (thanks to the answer to Tim's question by Matthias Klupsch). I'll post a new question for what I read in Rockafellar's book and gave me the confusion.","Let $\mathcal{X}$ be a normed space and $C\subseteq \mathcal{X}$. We define the point-to-set distance for the set $C$ to be: $$ d_C:\mathcal{X}\ni x \mapsto d_c(x):= \inf_{y\in C}\|x-y\| \in [0,\infty] $$ Additionally, we define the inner limit of a sequence of sets $C_n$ in $\mathcal{X}$ to be: $$ \liminf_n C_n = \bigcup_{n=1}^\infty \bigcap_{m=n}^\infty C_m $$ This definition is equivalent to: $$ \liminf_n C_n = \left\{x \in \mathcal{X} | x\in C_k \text{ ultimately for all } k \right\} $$ My initial question was: I need to prove that: $$ \liminf_n C_n = \left\{ x \in \mathcal{X} | \liminf_{n\to\infty} \left(d_{C_n}(x)\right)=0\right\} $$ But this is not true! The following facts hold true: $\liminf_n C_n \subseteq \left\{ x \in \mathcal{X} | \liminf_{n\to\infty} \left(d_{C_n}(x)\right)=0\right\} = \limsup_n C_n$ $\liminf_n C_n \subseteq \left\{ x \in \mathcal{X} | \lim_{n\to\infty} \left(d_{C_n}(x)\right)=0\right\}$ Edit 1. The meaning of ""ultimately for all $k$"" should be interpreted as follows: $$ \liminf_{n\to\infty} C_n = \{x | \forall k\in\mathbb{N} \exists x_k\in C_{n_k}:\ x_k\to x\} $$ where $n_k\in\mathbb{N}$ is a strictly increasing sequence of indices (i.e. $\{C_{n_k}\}_{k\in\mathbb{N}}$ is some subsequence of $\{C_n\}_{n\in\mathbb{N}}$). Edit 2. I changed the formula with the closure which was wrong according to Tim (thanks to the answer to Tim's question by Matthias Klupsch). I'll post a new question for what I read in Rockafellar's book and gave me the confusion.",,"['real-analysis', 'general-topology', 'analysis', 'limits', 'limsup-and-liminf']"
57,Proof of a Limit related to Gauss' Convergence test,Proof of a Limit related to Gauss' Convergence test,,"So this is the question: if the series $\sum_{n=1}^{\infty} a_n$ is such that $$\frac{a_n}{a_{n+1}} = 1 + \frac pn  + \alpha_n$$ and the series $\sum_{n=1}^{\infty} \alpha_n$ converges absolutely, then $a_n\sim \frac{c}{n^p}$ and that $\sum_{n=1}^{\infty} a_n$ and it converges absolutely for $p>1$ and diverges for $p \leq 1$ Here is my failed attempt $$\text{let} \:c_n=\ln(\frac{b_n}{b_{n+1}})=\ln(\alpha_n+\frac{p}{n}+1)\le \alpha_n+\frac{p}{n} $$ $$\implies \sum_{i=1}^{n}c_i={\ln(b_1)}-{\ln(b_{n+1})}=\ln\prod_{i=1}^{n}(\alpha_i+\frac{p}{i}+1)$$ $$\le\sum_{i=1}^n\alpha_i+p\sum_{i=1}^n \frac 1i$$ Although I applied the inequality, it seems that it doesn't work. And I have absolutely no idea how to continue this? Could you help me? Edit: I also noticed that a similar question has already been asked here Gauss' test for Convergence And nobody answered.","So this is the question: if the series is such that and the series converges absolutely, then and that and it converges absolutely for and diverges for Here is my failed attempt Although I applied the inequality, it seems that it doesn't work. And I have absolutely no idea how to continue this? Could you help me? Edit: I also noticed that a similar question has already been asked here Gauss' test for Convergence And nobody answered.",\sum_{n=1}^{\infty} a_n \frac{a_n}{a_{n+1}} = 1 + \frac pn  + \alpha_n \sum_{n=1}^{\infty} \alpha_n a_n\sim \frac{c}{n^p} \sum_{n=1}^{\infty} a_n p>1 p \leq 1 \text{let} \:c_n=\ln(\frac{b_n}{b_{n+1}})=\ln(\alpha_n+\frac{p}{n}+1)\le \alpha_n+\frac{p}{n}  \implies \sum_{i=1}^{n}c_i={\ln(b_1)}-{\ln(b_{n+1})}=\ln\prod_{i=1}^{n}(\alpha_i+\frac{p}{i}+1) \le\sum_{i=1}^n\alpha_i+p\sum_{i=1}^n \frac 1i,"['real-analysis', 'calculus', 'limits', 'convergence-divergence', 'absolute-convergence']"
58,A difficult example to show that limits and integrals can't always be switched,A difficult example to show that limits and integrals can't always be switched,,"I wanted an interesting one, so I started with $$\lim\limits_{N\to\infty}\int\limits_{-\infty}^{\infty} \operatorname{sech}\left(x-\sum\limits_{n=1}^{N}\frac{1}{x+n^{2}}\right)dx$$ The idea was that, by Glasser's master theorem, this direction should be easy to evaluate, as it should just become $$\lim\limits_{N\to\infty}\int\limits_{-\infty}^{\infty} \operatorname{sech}(x)dx=\lim\limits_{N\to\infty}\pi=\pi$$ Then, the other direction will be $$\int\limits_{-\infty}^{\infty}\lim\limits_{N\to\infty}\operatorname{sech}\left(x-\sum\limits_{n=1}^{N}\frac{1}{x+n^{2}}\right)dx=\int\limits_{-\infty}^{\infty}\operatorname{sech}\left(x-\sum\limits_{n=1}^{\infty}\frac{1}{x+n^{2}}\right)dx$$ Using the result from complex analysis that states $$\sum\limits_{k=1}^{\infty}\frac{1}{k^{2}+a^{2}}=-\frac{1}{2a^2}+\frac{\pi}{2a}\coth(\pi a)$$ the integral should transform into $$\int\limits_{-\infty}^{0}\operatorname{sech}(x+\frac{1}{2x}+\frac{\pi}{2\sqrt{-x}}\cot(\pi\sqrt{-x}))dx+\int\limits_{0}^{\infty}\operatorname{sech}(x+\frac{1}{2x}-\frac{\pi}{2\sqrt{x}}\coth(\pi\sqrt{x}))dx$$ A substitution $-x\mapsto x$ on the first makes it $$ \int\limits_{0}^{\infty} \operatorname{sech}\left(x + \frac{1}{2x} - \frac{\pi}{2\sqrt{x}} \cot{\left(\pi\sqrt{x}\right)}\right) + \operatorname{sech}\left(x + \frac{1}{2x} - \frac{\pi}{2\sqrt{x}} \coth{\left(\pi\sqrt{x}\right)}\right)dx$$ Now I need to either evaluate it exactly, or show it's not equal to $\pi$ . I initially thought Feynman's technique might help. If I consider $$I(p)=\int\limits_{0}^{\infty} \operatorname{sech}\left(x + \frac{1}{2x} - \frac{\pi}{2\sqrt{px}} \cot{\left(\pi\sqrt{px}\right)}\right) + \operatorname{sech}\left(x + \frac{1}{2x} - \frac{\pi}{2\sqrt{px}} \coth{\left(\pi\sqrt{px}\right)}\right)dx$$ then I want $I(1)$ . The idea was, the terms with $p$ should both vanish almost everywhere as $p\to\infty$ , so I would end up with $$\lim\limits_{p\to\infty} I(p)=\int\limits_{0}^{\infty}2\operatorname{sech}(x+\frac{1}{2x})dx$$ I would then apply Glasser's master theorem again to get $\lim\limits_{p\to\infty}I(p)=\pi$ , and use sign analysis on $I'(p)$ to show it wouldn't take this value for a finite $p$ . The issue is that Glasser's master theorem would only apply if I was subtracting the $\frac{1}{2x}$ , not adding it, so $\lim\limits_{p\to\infty}I(p)$ may be different from $\pi$ . So, I don't know what technique would allow this to progress further, and how we'd find the closed form, if there somehow is one.","I wanted an interesting one, so I started with The idea was that, by Glasser's master theorem, this direction should be easy to evaluate, as it should just become Then, the other direction will be Using the result from complex analysis that states the integral should transform into A substitution on the first makes it Now I need to either evaluate it exactly, or show it's not equal to . I initially thought Feynman's technique might help. If I consider then I want . The idea was, the terms with should both vanish almost everywhere as , so I would end up with I would then apply Glasser's master theorem again to get , and use sign analysis on to show it wouldn't take this value for a finite . The issue is that Glasser's master theorem would only apply if I was subtracting the , not adding it, so may be different from . So, I don't know what technique would allow this to progress further, and how we'd find the closed form, if there somehow is one.","\lim\limits_{N\to\infty}\int\limits_{-\infty}^{\infty} \operatorname{sech}\left(x-\sum\limits_{n=1}^{N}\frac{1}{x+n^{2}}\right)dx \lim\limits_{N\to\infty}\int\limits_{-\infty}^{\infty} \operatorname{sech}(x)dx=\lim\limits_{N\to\infty}\pi=\pi \int\limits_{-\infty}^{\infty}\lim\limits_{N\to\infty}\operatorname{sech}\left(x-\sum\limits_{n=1}^{N}\frac{1}{x+n^{2}}\right)dx=\int\limits_{-\infty}^{\infty}\operatorname{sech}\left(x-\sum\limits_{n=1}^{\infty}\frac{1}{x+n^{2}}\right)dx \sum\limits_{k=1}^{\infty}\frac{1}{k^{2}+a^{2}}=-\frac{1}{2a^2}+\frac{\pi}{2a}\coth(\pi a) \int\limits_{-\infty}^{0}\operatorname{sech}(x+\frac{1}{2x}+\frac{\pi}{2\sqrt{-x}}\cot(\pi\sqrt{-x}))dx+\int\limits_{0}^{\infty}\operatorname{sech}(x+\frac{1}{2x}-\frac{\pi}{2\sqrt{x}}\coth(\pi\sqrt{x}))dx -x\mapsto x 
\int\limits_{0}^{\infty} \operatorname{sech}\left(x + \frac{1}{2x} - \frac{\pi}{2\sqrt{x}} \cot{\left(\pi\sqrt{x}\right)}\right) + \operatorname{sech}\left(x + \frac{1}{2x} - \frac{\pi}{2\sqrt{x}} \coth{\left(\pi\sqrt{x}\right)}\right)dx \pi I(p)=\int\limits_{0}^{\infty} \operatorname{sech}\left(x + \frac{1}{2x} - \frac{\pi}{2\sqrt{px}} \cot{\left(\pi\sqrt{px}\right)}\right) + \operatorname{sech}\left(x + \frac{1}{2x} - \frac{\pi}{2\sqrt{px}} \coth{\left(\pi\sqrt{px}\right)}\right)dx I(1) p p\to\infty \lim\limits_{p\to\infty} I(p)=\int\limits_{0}^{\infty}2\operatorname{sech}(x+\frac{1}{2x})dx \lim\limits_{p\to\infty}I(p)=\pi I'(p) p \frac{1}{2x} \lim\limits_{p\to\infty}I(p) \pi","['real-analysis', 'integration', 'limits']"
59,Proof of Beppo Levi's Theorem [closed],Proof of Beppo Levi's Theorem [closed],,"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed last month . Improve this question I am self-studying measure theory using Measure Theory by Donald Cohn. The text presented the following result but lack of detailed proof. I tried to write up the proof, and I would really appreciate it if someone could help me check if it is correct and rigorous. Here is the result: Beppo Levi's Theorem $\quad$ Let $(X,\mathscr{A},\mu)$ be a measure space, and let $\sum_{k=1}^{\infty}$ be an infinite series whose terms are $[0,+\infty]$ -valued $\mathscr{A}$ -measurable functions on $X$ . Then \begin{align*}     \int\sum_{k=1}^{\infty}f_kd\mu = \sum_{k=1}^{\infty}\int f_kd\mu. \end{align*} Here is my attempt: Proof $\quad$ Let $g_n=\sum_{k=1}^nf_k(x)$ and $g=\sum_{k=1}^{\infty}f_k(x)$ . By Proposition 2.1.5 and 2.1.6 below, $g_n$ 's and $g$ are $\mathscr{A}$ -measurable. Since each $f_k$ is $[0,+\infty]$ -valued $\mathscr{A}$ -measurable, it follows that the sequence $\{g_n\}$ is an increasing sequence, so that $\sum_{k=1}^nf_k(x)\to\alpha\in[0,+\infty]$ for all $x$ in $X$ . Denote $\alpha = \sum_{k=1}^{\infty}f_k(x)$ , so that $\lim_{n\to\infty}\sum_{k=1}^nf_k(x) = \alpha = \sum_{k=1}^{\infty}f_k(x)$ for all $x$ in $X$ . Hence, \begin{align*} g_1(x) \leq g_2(x) \leq \dots \end{align*} and \begin{align*} g(x) = \sum_{k=1}^{\infty}f_k(x) = \lim_{n\to\infty}\sum_{k=1}^{n}f_k(x) = \lim_{n\to\infty}g_n(x) \end{align*} hold at every $x$ in $X$ . Therefore, by the Monotone Convergence Theorem (see below), we have \begin{align*} \int\sum_{k=1}^{\infty}f_kd\mu = \int gd\mu = \lim_{n\to\infty}\int g_nd\mu = \lim_{n\to\infty}\int\sum_{k=1}^{n}f_kd\mu = \lim_{n\to\infty}\sum_{k=1}^n\int f_kd\mu,\tag1 \end{align*} where the last equality holds because of the linearity of the integral. Since $f_k$ is $[0,+\infty]$ -valued, then $\int f_kd\mu = \sup\left\{\int gd\mu:g\in\mathscr{S}_+\ \text{and}\ g\leq f_k\right\}$ must be $[0,+\infty]$ -valued, and so $\sum_{k=1}^n\int f_kd\mu$ is $[0,+\infty]$ -valued as well. Thus, $\sum_{k=1}^n\int f_kd\mu \to \lambda \in [0,+\infty]$ . Denote $\lambda=\sum_{k=1}^{\infty}\int f_kd\mu$ , so that $\lim_{n\to\infty}\sum_{k=1}^n\int f_kd\mu = \lambda = \sum_{k=1}^{\infty}\int f_kd\mu$ . Therefore, equality (1) implies \begin{align*} \int\sum_{k=1}^{\infty}f_kd\mu = \sum_{k=1}^{\infty}\int f_kd\mu. \end{align*} Thanks a lot in advance! Results used in my proof: Proposition 2.1.5 $\quad$ Let $(X,\mathscr{A})$ be a measurable space, let $A$ be a subset of $X$ that belongs to $\mathcal{A}$ , and let $\{f_n\}$ be a sequence of $[-\infty,+\infty]$ -valued measurable functions on $A$ . Then the function $\lim_{n\to\infty}f_n$ (whose domain is $\{x\in A:\limsup_{n\to\infty}f_n(x)=\liminf_{n\to\infty}f_n(x)\}$ ) is measurable. Proposition 2.1.6 $\quad$ Let $(X,\mathscr{A})$ be a measurable space, let $A$ be a subset of $X$ that belongs to $\mathcal{A}$ , let $f$ and $g$ be $[0,+\infty]$ -valued measurable functions on $A$ , and let $\alpha$ be a nonnegative real number. Then $\alpha f$ and $f+g$ are measurable. Theorem 2.4.1 $\quad$ The Monotone Convergence Theorem $\quad$ Let $(X,\mathscr{A},\mu)$ be a measure space, and let $f$ and $f_1,f_2,\dots,$ be $[0,\infty]$ -valued $\mathscr{A}$ -measurable functions on $X$ . Suppose that \begin{align}     f_1(x) \leq f_2(x) \leq \dots \end{align} and \begin{align}     f(x) = \lim_{n\to\infty}f_n(x) \end{align} hold at $\mu$ -almost every $x$ in $X$ . Then $\int fd\mu = \lim_{n\to\infty}\int f_nd\mu$ .","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed last month . Improve this question I am self-studying measure theory using Measure Theory by Donald Cohn. The text presented the following result but lack of detailed proof. I tried to write up the proof, and I would really appreciate it if someone could help me check if it is correct and rigorous. Here is the result: Beppo Levi's Theorem Let be a measure space, and let be an infinite series whose terms are -valued -measurable functions on . Then Here is my attempt: Proof Let and . By Proposition 2.1.5 and 2.1.6 below, 's and are -measurable. Since each is -valued -measurable, it follows that the sequence is an increasing sequence, so that for all in . Denote , so that for all in . Hence, and hold at every in . Therefore, by the Monotone Convergence Theorem (see below), we have where the last equality holds because of the linearity of the integral. Since is -valued, then must be -valued, and so is -valued as well. Thus, . Denote , so that . Therefore, equality (1) implies Thanks a lot in advance! Results used in my proof: Proposition 2.1.5 Let be a measurable space, let be a subset of that belongs to , and let be a sequence of -valued measurable functions on . Then the function (whose domain is ) is measurable. Proposition 2.1.6 Let be a measurable space, let be a subset of that belongs to , let and be -valued measurable functions on , and let be a nonnegative real number. Then and are measurable. Theorem 2.4.1 The Monotone Convergence Theorem Let be a measure space, and let and be -valued -measurable functions on . Suppose that and hold at -almost every in . Then .","\quad (X,\mathscr{A},\mu) \sum_{k=1}^{\infty} [0,+\infty] \mathscr{A} X \begin{align*}
    \int\sum_{k=1}^{\infty}f_kd\mu = \sum_{k=1}^{\infty}\int f_kd\mu.
\end{align*} \quad g_n=\sum_{k=1}^nf_k(x) g=\sum_{k=1}^{\infty}f_k(x) g_n g \mathscr{A} f_k [0,+\infty] \mathscr{A} \{g_n\} \sum_{k=1}^nf_k(x)\to\alpha\in[0,+\infty] x X \alpha = \sum_{k=1}^{\infty}f_k(x) \lim_{n\to\infty}\sum_{k=1}^nf_k(x) = \alpha = \sum_{k=1}^{\infty}f_k(x) x X \begin{align*}
g_1(x) \leq g_2(x) \leq \dots
\end{align*} \begin{align*}
g(x) = \sum_{k=1}^{\infty}f_k(x) = \lim_{n\to\infty}\sum_{k=1}^{n}f_k(x) = \lim_{n\to\infty}g_n(x)
\end{align*} x X \begin{align*}
\int\sum_{k=1}^{\infty}f_kd\mu = \int gd\mu = \lim_{n\to\infty}\int g_nd\mu = \lim_{n\to\infty}\int\sum_{k=1}^{n}f_kd\mu = \lim_{n\to\infty}\sum_{k=1}^n\int f_kd\mu,\tag1
\end{align*} f_k [0,+\infty] \int f_kd\mu = \sup\left\{\int gd\mu:g\in\mathscr{S}_+\ \text{and}\ g\leq f_k\right\} [0,+\infty] \sum_{k=1}^n\int f_kd\mu [0,+\infty] \sum_{k=1}^n\int f_kd\mu \to \lambda \in [0,+\infty] \lambda=\sum_{k=1}^{\infty}\int f_kd\mu \lim_{n\to\infty}\sum_{k=1}^n\int f_kd\mu = \lambda = \sum_{k=1}^{\infty}\int f_kd\mu \begin{align*}
\int\sum_{k=1}^{\infty}f_kd\mu = \sum_{k=1}^{\infty}\int f_kd\mu.
\end{align*} \quad (X,\mathscr{A}) A X \mathcal{A} \{f_n\} [-\infty,+\infty] A \lim_{n\to\infty}f_n \{x\in A:\limsup_{n\to\infty}f_n(x)=\liminf_{n\to\infty}f_n(x)\} \quad (X,\mathscr{A}) A X \mathcal{A} f g [0,+\infty] A \alpha \alpha f f+g \quad \quad (X,\mathscr{A},\mu) f f_1,f_2,\dots, [0,\infty] \mathscr{A} X \begin{align}
    f_1(x) \leq f_2(x) \leq \dots
\end{align} \begin{align}
    f(x) = \lim_{n\to\infty}f_n(x)
\end{align} \mu x X \int fd\mu = \lim_{n\to\infty}\int f_nd\mu","['real-analysis', 'integration', 'limits', 'analysis', 'measure-theory']"
60,Is there a kind of L'hopital's Rule for multivariable calculus?,Is there a kind of L'hopital's Rule for multivariable calculus?,,"Question: Is there a kind of L'hopital's Rule for multivariable calculus? It is widely known that L'hopital's can be applied for single-variable functions also multivariable functions when one reduces the variable. For example, $$\lim\limits_{(x,y) \to (0,0)} \frac{x^2y + \sin(y)}{y} = \lim\limits_{(x) \to (0)}x^2 + \lim\limits_{(y) \to (0)} \frac{\sin(y)}{y}=1 $$ So, converting a multivariable limit to a single-variable limit provides us to use the rules we have for single-variable limit i.e., L'Hopital's Rule is allowed. Do we have such a theorem or rule to take limit easily for multivariable functions?","Question: Is there a kind of L'hopital's Rule for multivariable calculus? It is widely known that L'hopital's can be applied for single-variable functions also multivariable functions when one reduces the variable. For example, So, converting a multivariable limit to a single-variable limit provides us to use the rules we have for single-variable limit i.e., L'Hopital's Rule is allowed. Do we have such a theorem or rule to take limit easily for multivariable functions?","\lim\limits_{(x,y) \to (0,0)} \frac{x^2y + \sin(y)}{y} = \lim\limits_{(x) \to (0)}x^2 + \lim\limits_{(y) \to (0)} \frac{\sin(y)}{y}=1 ","['calculus', 'limits']"
61,When does a sparse failing subsequence of $(a_n)_n\subset\mathbb{R}_{>0}$ exist when $\lim_{n\to\infty}\frac{1}{n}\sum_{j=1}^na_j = c$ exists,When does a sparse failing subsequence of  exist when  exists,(a_n)_n\subset\mathbb{R}_{>0} \lim_{n\to\infty}\frac{1}{n}\sum_{j=1}^na_j = c,"My question is a bit longer than the actual title, so here is the full question: Let $(a_n)_n\subset\mathbb{R}_{>0}$ be a sequence of positive real numbers such that the limit $$\lim_{n\to\infty}\frac{1}{n}\sum_{j=1}^na_j = c$$ exist for some $c\in\mathbb{R}$ . Then, it isn't too difficult to show that there exist a dense subsequence $(a_{n_k})_k$ of $(a_n)$ such that $$\lim_{k\to\infty}a_{n_k} = c$$ where by a ""dense"" subsequence we mean that $$\lim_{N\to\infty}\frac{\#(\{n_k: k\in\mathbb{N}\}\cap\{1,2,\dots,N\})}{N} = 1$$ Consequently, by a ""sparse"" subsequence of indices $(n_k)_k$ we mean such a subsequence of indices that $$\lim_{N\to\infty}\frac{\#(\{n_k: k\in\mathbb{N}\}\cap\{1,2,\dots,N\})}{N} = 0$$ Knowing that such a dense subsequence always exist when the limit of the mean exist, what do we know about the existence of sparse sequences $(a_{n_k})_k$ such that $$\lim_{k\to\infty}a_{n_k}\neq c$$ I'm interested in learning about any facts and/or references that discuss this question!","My question is a bit longer than the actual title, so here is the full question: Let be a sequence of positive real numbers such that the limit exist for some . Then, it isn't too difficult to show that there exist a dense subsequence of such that where by a ""dense"" subsequence we mean that Consequently, by a ""sparse"" subsequence of indices we mean such a subsequence of indices that Knowing that such a dense subsequence always exist when the limit of the mean exist, what do we know about the existence of sparse sequences such that I'm interested in learning about any facts and/or references that discuss this question!","(a_n)_n\subset\mathbb{R}_{>0} \lim_{n\to\infty}\frac{1}{n}\sum_{j=1}^na_j = c c\in\mathbb{R} (a_{n_k})_k (a_n) \lim_{k\to\infty}a_{n_k} = c \lim_{N\to\infty}\frac{\#(\{n_k: k\in\mathbb{N}\}\cap\{1,2,\dots,N\})}{N} = 1 (n_k)_k \lim_{N\to\infty}\frac{\#(\{n_k: k\in\mathbb{N}\}\cap\{1,2,\dots,N\})}{N} = 0 (a_{n_k})_k \lim_{k\to\infty}a_{n_k}\neq c","['real-analysis', 'sequences-and-series', 'limits', 'reference-request']"
62,Limit Piecewise Function Rational and Irrational [closed],Limit Piecewise Function Rational and Irrational [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 months ago . Improve this question I have \begin{align} f(x) = \begin{cases}  x^2,  & \text{if $x$ is rational} \\ -x^2, & \text{if $x$ is irrational} \end{cases} \end{align} I must compute the limit of $f(x)$ as $x$ tends to $0$ . I did $|f(x)| \leq |x|$ for $|x|<1$ , so by the sandwich theorem the limit of $f(x)$ as $x$ tends to $0$ is $0$ . Is this correct?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 months ago . Improve this question I have I must compute the limit of as tends to . I did for , so by the sandwich theorem the limit of as tends to is . Is this correct?","\begin{align} f(x) =
\begin{cases} 
x^2,  & \text{if x is rational} \\
-x^2, & \text{if x is irrational}
\end{cases}
\end{align} f(x) x 0 |f(x)| \leq |x| |x|<1 f(x) x 0 0","['real-analysis', 'calculus', 'limits', 'analysis', 'solution-verification']"
63,Compute $F(x)$ and $\displaystyle{\lim_{x \to \frac{\pi}{2}}} F(x)$,Compute  and,F(x) \displaystyle{\lim_{x \to \frac{\pi}{2}}} F(x),Consider the function F defined on $]-\frac{\pi}{2}; \frac{\pi}{2}[$ by $$F(x) = \int_0^x t \tan^2{t} dt$$ Compute $F(x)$ and $\displaystyle{\lim_{x \to \frac{\pi}{2}}} F(x)$ Determine the sign of $F(x)$ My attempt: \begin{align}F(x) &= \int_0^x t \tan^2{t} dt \\ &= [t \tan{t}-t^2]_0^x - \int_0^x \tan{t} dt + \int_0^x t dt \\ &= x \tan{x}-x^2 + \ln{|\cos{x}|} + \frac{x^2}{2} \\ &= x \tan{x} + \ln{|\cos{x}|} - \frac{x^2}{2}\end{align} I don't see how to compute the limit knowing that $\displaystyle{\lim_{x \to \frac{\pi}{2}}}\tan{x}$ does not exist. EDIT: \begin{align}\displaystyle{\lim_{x \to \frac{\pi}{2}}} F(x) &= \displaystyle{\lim_{x \to \frac{\pi}{2}^-}} F(x) \\ &= \displaystyle{\lim_{x \to \frac{\pi}{2}^-}} (x \tan{x} + \ln{|\cos{x}|}) - \displaystyle{\lim_{x \to \frac{\pi}{2}^-}} \frac{x^2}{2} \\ &= \displaystyle{\lim_{x \to \frac{\pi}{2}^-}} \tan{x}(x + \frac{\ln{|\cos{x}|}}{\tan{x}}) - \displaystyle{\lim_{x \to \frac{\pi}{2}^-}} \frac{x^2}{2} \end{align} and $\displaystyle{\lim_{x \to \frac{\pi}{2}^-}}  \frac{\ln{|\cos{x}|}}{\tan{x}} = \displaystyle{\lim_{x \to \frac{\pi}{2}^-}}  \frac{\ln{\cos{x}}}{\tan{x}} = 0$ Thus \begin{align}\displaystyle{\lim_{x \to \frac{\pi}{2}}} F(x)  &= + \infty\end{align} Is it like this?,Consider the function F defined on by Compute and Determine the sign of My attempt: I don't see how to compute the limit knowing that does not exist. EDIT: and Thus Is it like this?,"]-\frac{\pi}{2}; \frac{\pi}{2}[ F(x) = \int_0^x t \tan^2{t} dt F(x) \displaystyle{\lim_{x \to \frac{\pi}{2}}} F(x) F(x) \begin{align}F(x) &= \int_0^x t \tan^2{t} dt \\
&= [t \tan{t}-t^2]_0^x - \int_0^x \tan{t} dt + \int_0^x t dt \\
&= x \tan{x}-x^2 + \ln{|\cos{x}|} + \frac{x^2}{2} \\
&= x \tan{x} + \ln{|\cos{x}|} - \frac{x^2}{2}\end{align} \displaystyle{\lim_{x \to \frac{\pi}{2}}}\tan{x} \begin{align}\displaystyle{\lim_{x \to \frac{\pi}{2}}} F(x) &= \displaystyle{\lim_{x \to \frac{\pi}{2}^-}} F(x) \\
&= \displaystyle{\lim_{x \to \frac{\pi}{2}^-}} (x \tan{x} + \ln{|\cos{x}|}) - \displaystyle{\lim_{x \to \frac{\pi}{2}^-}} \frac{x^2}{2} \\
&= \displaystyle{\lim_{x \to \frac{\pi}{2}^-}} \tan{x}(x + \frac{\ln{|\cos{x}|}}{\tan{x}}) - \displaystyle{\lim_{x \to \frac{\pi}{2}^-}} \frac{x^2}{2} \end{align} \displaystyle{\lim_{x \to \frac{\pi}{2}^-}}  \frac{\ln{|\cos{x}|}}{\tan{x}} = \displaystyle{\lim_{x \to \frac{\pi}{2}^-}}  \frac{\ln{\cos{x}}}{\tan{x}} = 0 \begin{align}\displaystyle{\lim_{x \to \frac{\pi}{2}}} F(x)  &= + \infty\end{align}","['integration', 'limits', 'analysis', 'improper-integrals']"
64,Limit of measurable functions in a general metric space is measurable?,Limit of measurable functions in a general metric space is measurable?,,"Let $(f_n)_{n \geq 1}$ be a sequence of measurable functions from $(\Omega, \mathcal F)$ to a metric space $(X, d)$ . Suppose that $f$ is such that $$ \lim_{n \rightarrow +\infty} d(f_n(\omega),f(\omega)) = 0, \ \forall \omega \in \Omega. $$ Is the function $f$ $\mathcal F$ -measurable? What I know: if the space $X$ is separable (not necessarily complete) then this is true. I wonder why separability is needed here because I found an attempt that does not use separability. However, I still have a little bit of doubt so I would like to ask where did I go wrong... My attempt: Let $C$ be a closed set then the map $x \mapsto d(x,C)$ is continuous. Call this map $\Psi_C$ then the composition maps $ \Psi_C \circ f_n$ is measurable. $\Psi_C \circ f_n \rightarrow \Psi_C \circ f$ pointwise so $\Psi_C \circ f$ is measurable too. Now I observe $$ f^{-1}(C)= \{ \omega \in \Omega: d(f(\omega),C) = 0 \} = (\Psi_C \circ f)^{-1} \{ 0 \} \in \mathcal F $$ for every closed set $C$ , which generates the Borel sets $\mathcal B(X)$ induced by $d$ , and so $f$ is measurable. A similar argument is given here . However, I don't understand why in most probability/ analysis books, they usually assume the metric space is separable to have the limit measurable. Thanks in advance!","Let be a sequence of measurable functions from to a metric space . Suppose that is such that Is the function -measurable? What I know: if the space is separable (not necessarily complete) then this is true. I wonder why separability is needed here because I found an attempt that does not use separability. However, I still have a little bit of doubt so I would like to ask where did I go wrong... My attempt: Let be a closed set then the map is continuous. Call this map then the composition maps is measurable. pointwise so is measurable too. Now I observe for every closed set , which generates the Borel sets induced by , and so is measurable. A similar argument is given here . However, I don't understand why in most probability/ analysis books, they usually assume the metric space is separable to have the limit measurable. Thanks in advance!","(f_n)_{n \geq 1} (\Omega, \mathcal F) (X, d) f  \lim_{n \rightarrow +\infty} d(f_n(\omega),f(\omega)) = 0, \ \forall \omega \in \Omega.  f \mathcal F X C x \mapsto d(x,C) \Psi_C  \Psi_C \circ f_n \Psi_C \circ f_n \rightarrow \Psi_C \circ f \Psi_C \circ f  f^{-1}(C)= \{ \omega \in \Omega: d(f(\omega),C) = 0 \} = (\Psi_C \circ f)^{-1} \{ 0 \} \in \mathcal F  C \mathcal B(X) d f","['real-analysis', 'limits', 'measure-theory', 'measurable-functions']"
65,Limit of the ratio of two matrix expansions,Limit of the ratio of two matrix expansions,,"With $M$ , $N$ and $W_r$ non-zero (invertible) square matrices, how can one calculate the following limit \begin{equation} \lim_{\epsilon \to 0}\left[ I -  \frac{M \left(\sum_{r=0}^{\infty} \epsilon^{-r} W_{r} \right) M}{I - M \left( \sum_{r=0}^{\infty} \epsilon^{-r} W_{r} \right)  N} \right], \end{equation} where $0 < \epsilon \ll 1$ and $I$ is the identity matrix.","With , and non-zero (invertible) square matrices, how can one calculate the following limit where and is the identity matrix.","M N W_r \begin{equation}
\lim_{\epsilon \to 0}\left[ I -  \frac{M \left(\sum_{r=0}^{\infty} \epsilon^{-r} W_{r} \right) M}{I - M \left( \sum_{r=0}^{\infty} \epsilon^{-r} W_{r} \right)  N} \right],
\end{equation} 0 < \epsilon \ll 1 I","['matrices', 'limits']"
66,Prove Verification: show by definition that $\lim_{x \to 9} \sqrt{x-8}=1$,Prove Verification: show by definition that,\lim_{x \to 9} \sqrt{x-8}=1,"Question: Prove by definition that $\lim_{x \to 9} \sqrt{x-8}=1$ Answer: $\forall \epsilon > 0, \exists \delta = (1+ \epsilon)^2-1 > 0 \; s.t. \; | \sqrt{x-8} -1 | < \epsilon$ indeed: 1- We can note that $\forall \epsilon>0 , (1+ \epsilon)^2-1>(1- \epsilon)^2-1$ and that $(1- \epsilon)^2-1>-(1+ \epsilon)^2+1$ (trivial to prove). 2- Hence from ""1-"" we have that if $ |x-9| < (1+\epsilon)^2-1 \Leftrightarrow -(1+\epsilon)^2+1< x-9 < (1+\epsilon)^2-1 $ we can restrict to the case of $x$ verifying $ (1- \epsilon)^2-1 < x-9 < (1+\epsilon)^2-1$ 3- So we continue and we get: $(1- \epsilon)^2 < x-8 < (1+\epsilon)^2 \Leftrightarrow 1-\epsilon < \sqrt{x-8} < 1+\epsilon \Leftrightarrow -\epsilon < \sqrt{x-8} -1 < \epsilon$ and the last expression is equivalent to write $|\sqrt{x-8} -1|<\epsilon$ . Q.E.D. Is it correct? Thank you.","Question: Prove by definition that Answer: indeed: 1- We can note that and that (trivial to prove). 2- Hence from ""1-"" we have that if we can restrict to the case of verifying 3- So we continue and we get: and the last expression is equivalent to write . Q.E.D. Is it correct? Thank you.","\lim_{x \to 9} \sqrt{x-8}=1 \forall \epsilon > 0, \exists \delta = (1+ \epsilon)^2-1 > 0 \; s.t. \; | \sqrt{x-8} -1 | < \epsilon \forall \epsilon>0 , (1+ \epsilon)^2-1>(1- \epsilon)^2-1 (1- \epsilon)^2-1>-(1+ \epsilon)^2+1  |x-9| < (1+\epsilon)^2-1 \Leftrightarrow -(1+\epsilon)^2+1< x-9 < (1+\epsilon)^2-1  x  (1- \epsilon)^2-1 < x-9 < (1+\epsilon)^2-1 (1- \epsilon)^2 < x-8 < (1+\epsilon)^2 \Leftrightarrow 1-\epsilon < \sqrt{x-8} < 1+\epsilon \Leftrightarrow -\epsilon < \sqrt{x-8} -1 < \epsilon |\sqrt{x-8} -1|<\epsilon","['calculus', 'limits', 'solution-verification', 'continuity']"
67,Asymptotic calculations for Bell numbers with little-o notation,Asymptotic calculations for Bell numbers with little-o notation,,"I'm having trouble doing calculations with the little-o notation. Let's say that the following asymptotic formula holds for $n\to\infty$ : $$ B_n = \frac{1}{\sqrt{r+1}} \exp\left( n \left(r+\frac{1}{r}-1\right)-1 \right)(1+o(1)) $$ where $r$ is the (unique and positive) solution to $re^r=n$ . In the following you can assume that you can use the same $r$ for $B_n$ , $B_{n+1}$ and $B_{n+2}$ (you can show that with some more asymptotics). Somehow you can then prove the identity $$\frac{B_{n+1}}{B_n} -1 = \frac{n}{r} (1+o(1))$$ and $$ \frac{B_{n+2}}{B_n} - \left( \frac{B_{n+1}}{B_n} \right)^2 -1 = \frac{n}{r^2} (1+o(1)). $$ How do I go about proving these formulas? I attempted to derive the first identity by just plugging in the asymptotic formula and got stuck: $$ \begin{align*} \frac{B_{n+1}}{B_n} - 1 &= \frac{ \frac{1}{\sqrt{r+1}} \exp\left((n+1)\left(r+\frac{1}{r}-1\right)-1\right) (1+o(1)) }{ \frac{1}{\sqrt{r+1}} \exp\left(n\left(r+\frac{1}{r}-1\right)-1\right) (1+o(1)) } -1 \\\\ &= \exp\left(r+\frac{1}{r}-1\right)(1+o(1)) - 1 \\\\ &= e^r e^{1/r} e^{-1} (1+o(1)) -1 \\\\ &= \frac{n}{r} e^{-1} (1+o(1)) -1 . \end{align*} $$ In the last equality I used $e^r = \frac{n}{r}$ and that $e^{1/r}=1+o(1)$ (at least I think that's true b/c $n\to\infty$ implies $r\to\infty$ and $e^{1/r}\xrightarrow{r\to\infty} 1$ and afaik that's precisely what $1+o(1)$ means for $r\to\infty$ ). But now there's still the term $e^{-1}$ left, of which I can't say it's $1+o(1)$ , b/c $e^{-1}(1+o(1)) = e^{-1} + o(1)$ . So somewhere I made a mistake but I don't know where... (The $B_n$ are Bell numbers btw.)","I'm having trouble doing calculations with the little-o notation. Let's say that the following asymptotic formula holds for : where is the (unique and positive) solution to . In the following you can assume that you can use the same for , and (you can show that with some more asymptotics). Somehow you can then prove the identity and How do I go about proving these formulas? I attempted to derive the first identity by just plugging in the asymptotic formula and got stuck: In the last equality I used and that (at least I think that's true b/c implies and and afaik that's precisely what means for ). But now there's still the term left, of which I can't say it's , b/c . So somewhere I made a mistake but I don't know where... (The are Bell numbers btw.)","n\to\infty  B_n = \frac{1}{\sqrt{r+1}} \exp\left( n \left(r+\frac{1}{r}-1\right)-1 \right)(1+o(1))  r re^r=n r B_n B_{n+1} B_{n+2} \frac{B_{n+1}}{B_n} -1 = \frac{n}{r} (1+o(1))  \frac{B_{n+2}}{B_n} - \left( \frac{B_{n+1}}{B_n} \right)^2 -1 = \frac{n}{r^2} (1+o(1)).   \begin{align*} \frac{B_{n+1}}{B_n} - 1 &= \frac{ \frac{1}{\sqrt{r+1}} \exp\left((n+1)\left(r+\frac{1}{r}-1\right)-1\right) (1+o(1)) }{ \frac{1}{\sqrt{r+1}} \exp\left(n\left(r+\frac{1}{r}-1\right)-1\right) (1+o(1)) } -1 \\\\
&= \exp\left(r+\frac{1}{r}-1\right)(1+o(1)) - 1 \\\\
&= e^r e^{1/r} e^{-1} (1+o(1)) -1 \\\\
&= \frac{n}{r} e^{-1} (1+o(1)) -1 .
\end{align*}  e^r = \frac{n}{r} e^{1/r}=1+o(1) n\to\infty r\to\infty e^{1/r}\xrightarrow{r\to\infty} 1 1+o(1) r\to\infty e^{-1} 1+o(1) e^{-1}(1+o(1)) = e^{-1} + o(1) B_n","['combinatorics', 'limits', 'discrete-mathematics', 'notation', 'asymptotics']"
68,Substitution of Variable in Limits,Substitution of Variable in Limits,,So Here's the Question $$F(x)=\frac{x^2\sin(\frac1x)+2x}{(1+x)^\frac1x -e} \quad \text{then}\quad \lim_{x \to 0}{F(x)=}$$ I tried this Question by Substituting $x = \frac1y$ Where $\lim_{y \to \infty}$ But still in the outcome there are $2$ forms coming out which are $\frac00$ and $\frac \infty\infty$ . I also came across an alternate solution which is by substituting $x$ as $\frac1x$ and no limits specified for $x$ . Can anyone explain the logic behind the alternate solution and even how to proceed further with my approach. Thanks :),So Here's the Question I tried this Question by Substituting Where But still in the outcome there are forms coming out which are and . I also came across an alternate solution which is by substituting as and no limits specified for . Can anyone explain the logic behind the alternate solution and even how to proceed further with my approach. Thanks :),F(x)=\frac{x^2\sin(\frac1x)+2x}{(1+x)^\frac1x -e} \quad \text{then}\quad \lim_{x \to 0}{F(x)=} x = \frac1y \lim_{y \to \infty} 2 \frac00 \frac \infty\infty x \frac1x x,"['calculus', 'limits']"
69,Limit involving a recursively-defined sequence,Limit involving a recursively-defined sequence,,"$$A_{n+1}=A_n+\frac{1}{\sum_{i=1}^n A_i}$$ with $$A_1=1$$ Find out the value of $$\lim_{n→∞}A_n/\sqrt{\log(n)}$$ I used Stolz Theorem, but it seems to be useless.","$$A_{n+1}=A_n+\frac{1}{\sum_{i=1}^n A_i}$$ with $$A_1=1$$ Find out the value of $$\lim_{n→∞}A_n/\sqrt{\log(n)}$$ I used Stolz Theorem, but it seems to be useless.",,"['calculus', 'sequences-and-series', 'analysis']"
70,exchanging limit and integral of a function which tends to infinity in some points,exchanging limit and integral of a function which tends to infinity in some points,,"I am studying Young's Introduction to nonharmonic Fourier series and I am currently stucked in the proof of Theorem 4 from Chapter 3 (after Levinson). The question is: why can we state that, for $n \in \mathbb{N} $ and $\alpha < 0$ , $$\int_{-\pi}^{\pi} (1+e^{it})^{\alpha}e^{int} \,dt = \lim_{r \to 1^-} \int_{-\pi}^{\pi} (1+re^{it})^{\alpha}e^{int} \,dt \,?$$ If $\alpha \geq 0$ , it is pretty clear that it is possible to do so, as the function would be bounded. In this case I am struggling to find a proper bound for the function in order to apply the dominated convergence theorem , since $$\lvert 1 + re^{it} \rvert^2 = \left( 1 + r\cos t \right)^2 + \left( r\sin t \right)^2 = 1 + 2r\cos t + r^2$$ can be arbitrarily close to $0$ when $t = -\pi,\pi$ . Note: in the original text $\alpha = 2(\frac{1}{2p} + \varepsilon) - 1$ , where $\varepsilon >0$ and $p \in (1,\infty)$ .","I am studying Young's Introduction to nonharmonic Fourier series and I am currently stucked in the proof of Theorem 4 from Chapter 3 (after Levinson). The question is: why can we state that, for and , If , it is pretty clear that it is possible to do so, as the function would be bounded. In this case I am struggling to find a proper bound for the function in order to apply the dominated convergence theorem , since can be arbitrarily close to when . Note: in the original text , where and .","n \in \mathbb{N}  \alpha < 0 \int_{-\pi}^{\pi} (1+e^{it})^{\alpha}e^{int} \,dt = \lim_{r \to 1^-} \int_{-\pi}^{\pi} (1+re^{it})^{\alpha}e^{int} \,dt \,? \alpha \geq 0 \lvert 1 + re^{it} \rvert^2 = \left( 1 + r\cos t \right)^2 + \left( r\sin t \right)^2 = 1 + 2r\cos t + r^2 0 t = -\pi,\pi \alpha = 2(\frac{1}{2p} + \varepsilon) - 1 \varepsilon >0 p \in (1,\infty)","['calculus', 'integration', 'limits', 'lebesgue-integral', 'binomial-theorem']"
71,Residue involving Lambert function: $\underset{z=\frac{2}{\pi}W(\frac{\pi}{2})}{\text{Res}}\dfrac{1}{(e^{-\pi x}-x^2)^n}$,Residue involving Lambert function:,\underset{z=\frac{2}{\pi}W(\frac{\pi}{2})}{\text{Res}}\dfrac{1}{(e^{-\pi x}-x^2)^n},"Context I was tring to find a way to evaluate the infinite tetration of $i$ defined as: $${}^{\infty}i:=i^{i^{i^{.^{.^{.}}}}}$$ 1° attempt Considering $z=i^{i^{i^{.^{.^{.}}}}}$ we can notice that $z=i^z$ , solving this equation we have that: $$z=\dfrac{2i}{\pi}W\left(-\dfrac{\pi i }{2}\right)$$ Where $W(x)$ is the Lambert function. 2° attempt I tried to solve it by separating the absolute value and the argument: $z=\rho e^{i\theta}=x+iy$ $$x+iy=i^{x+iy}\\ \rho e^{i\theta}=e^{\frac{\pi}{2} i(x+iy)}\\ \rho e^{i\theta}=e^{\frac{\pi}{2} ix-\frac{\pi}{2} y}\\ \rho=e^{-\frac{\pi}{2} y}\qquad \theta=\dfrac{\pi}{2} x $$ I tried to graphically look at their representation considering $\rho=\sqrt{x^2+y^2}$ and $\theta=\arctan\left(\dfrac{y}{x}\right)$ Now I have this system: $$\begin{cases}\sqrt{x^2+y^2}=e^{-\frac{\pi}{2} y}\\\arctan\left(\dfrac{y}{x}\right)=\dfrac{\pi}{2} x\end{cases}\quad\Rightarrow\quad\begin{cases}x^2+y^2=e^{-\pi y}\\y=\tan\left(\dfrac{\pi x}{2}\right)x\end{cases}$$ I tried to calculate the inverse function that appears in the first equation: $x=\sqrt{e^{-\pi y}-y^2}$ and after several steps I found that I had to calculate the following residue: $$\underset{x=\frac{2}{\pi}W\left(\frac{\pi}{2}\right)}{\text{Res}}\dfrac{1}{\left(e^{-\pi x}-x^2\right)^n}\qquad\text{for }n\in\mathbb{N}^{+}$$ Aspect of interest Is that both functions are: In addition to both being even functions, it turns out that although these two functions are not periodic, they intersect infinitely with tangents that are always perpendicular: Question How can I calculate the closed form of this residue? $$\color{blue}{w_n:=\underset{x=\frac{2}{\pi}W\left(\frac{\pi}{2}\right)}{\text{Res}}\dfrac{1}{\left(e^{-\pi x}-x^2\right)^n}\qquad\text{for }n\in\mathbb{N}^{+}}$$ Where $W(z)$ is the Lambert function. Considerig $\color{red}{k:=W\left(\frac{\pi}{2}\right)}$ for brevity, the first cases are: $w_1=-\left(\dfrac{\pi}{k(k+1)}\right)^1\cdot\dfrac{1}{4}$ $w_2=+\left(\dfrac{\pi}{k(k+1)}\right)^3\cdot\dfrac{2k^2-1}{32}$ $w_3=-\left(\dfrac{\pi}{k(k+1)}\right)^5\cdot\dfrac{8k^4-4k^3-12k^2+3}{512}$ $w_4=+\left(\dfrac{\pi}{k(k+1)}\right)^7\cdot\dfrac{48k^6-64k^5-132k^4+40k^3+90k^2-15}{12288}$ $w_5=-\left(\dfrac{\pi}{k(k+1)}\right)^9\cdot\dfrac{384k^8-928k^7-1448k^6+1424k^5+1980k^4-420k^2-840k^2+105}{393216}$ So in general I think the general solution has this form: $$\color{blue}{w_n=(-1)^n\left(\dfrac{\pi}{k(k+1)}\right)^{2n-1}\cdot\dfrac{P_{n}(k)}{2^{3n-1}\cdot(n-1)!}}$$ Where $P_{n}(x)$ is a degree $(2n-2)$ polynomial with the following properties: $P_n(-1)=(2n-3)!!\qquad$ ( $x!!$ is the double factorial) The term of degree $0$ is $(-1)^{n-1}\cdot (2n-3)!!$ The term of degree $1$ is zero. The term of degree $n$ is $(2n-2)!!=2^{n-1}(n-1)!$ The first polynomials I was able to compute are: $P_{1}(x)=1$ $P_{2}(x)=2x^2-1$ $P_{3}(x)=8x^4-4x^3-12x^2+3$ $P_{4}(x)=48x^6-64x^5-132x^4+40x^3+90x^2-15$ $P_{5}(x)=384x^8-928x^7-1448x^6+1424 x^5+1980x^4-420x^3-840x^2+105$ $P_{6}(x)=3840x^{10}-14208x^9-15488 x^8+37424 x^7+36880x^6-26544 x^5-31080x^4+5040x^3+9450x^2-945$ $P_{7}(x)=46080 x^{12}-237312 x^{11}-138624 x^{10}+903424x^9+567408 x^8-1116480x^7-901040x^6+497952 x^5+529200 x^4-69300 x^3-124740x^2+ 10395$ $P_{8}(x)=645120x^{14}-4349952x^{13}-118016x^{12}+21436160 x^{11}+4877280x^{10}-39785600 x^9-20113072x^8+30796704 x^7 + 22251600x^6-9868320x^5-9854460x^4+1081080x^3+1891890 x^2 - 135135$ $P_{9}(x)=10321920 x^{16}- 87330816 x^{15}+ 50932224 x^{14}+ 512583680 x^{13}- 135690880 x^{12}- 1302740160 x^{11}- 221455936 x^{10}+ 1534016384 x^9+ 699496560 x^8- 845079840 x^7 - 569451960 x^6+ 209729520 x^5+200540340 x^4- 18918900 x^3- 32432400 x^2+2027025$ Etc... I put the coefficients from $P_1(x)$ to $P_{13}(x)$ in this triangular pattern to better visualize the values: $$\begin{array}{rrrrrrrrrrrrcllllllllllll} &&&&&&&&&&&&_1\\ \hline &&&&&&&&&&&_2&_0&_{\color{red}{-1}}\\ \hline &&&&&&&&&&_8&_{\color{red}{-4}}&_{\color{red}{-12}}&_0&_3\\ \hline &&&&&&&&&_{48}&_{\color{red}{-64}}&_{\color{red}{-132}}&_{40}&_{90}&_0&_{\color{red}{-15}}\\ \hline &&&&&&&&_{384}&_{\color{red}{-928}}&_{\color{red}{-1448}}&_{1424}&_{1980}&_{\color{red}{-420}}&_{\color{red}{-840}}&_0&_{105}\\ \hline &&&&&&&_{3840}&_{\color{red}{-14208}}&_{\color{red}{-15488}}&_{37424}&_{36880}&_{\color{red}{-26544}}&_{\color{red}{-31080}}&_{5040}&_{9450}&_0&_{\color{red}{-945}}\\ \hline &&&&&&_{46080}&_{\color{red}{-237312}}&_{\color{red}{-138624}}&_{903424}&_{567408}&_{\color{red}{-1116480}}&_{\color{red}{-901040}}&_{497952}&_{529200}&_{\color{red}{-69300}}&_{\color{red}{-124740}}&_0&_{10395}\\ \hline &&&&&_{645120}&_{\color{red}{-4349952}}&_{\color{red}{-118016}}&_{21436160}&_{4877280}&_{\color{red}{-39785600}}&_{\color{red}{-20113072}}&_{30796704}&_{22251600}&_{\color{red}{-9868320}}&_{\color{red}{-9854460}}&_{1081080}&_{1891890}&_0&_{\color{red}{-135135}}\\ \hline &&&&_{10321920}&_{\color{red}{-87330816}}&_{50932224}&_{512583680}&_{\color{red}{-135690880}}&_{\color{red}{-1302740160}}&_{\color{red}{-221455936}}&_{1534016384}&_{699496560}&_{\color{red}{-845079840}}&_{\color{red}{-569451960}}&_{209729520}&_{200540340}&_{\color{red}{-18918900}}&_{\color{red}{-32432400}}&_0&_{2027025}\\ \hline &&&_{185794560}&_{\color{red}{-1911767040}}&_{2380548096}&_{12435495936}&_{\color{red}{-11230101760}}&_{\color{red}{-40444523520}}&_{10605697600}&_{67185673856}&_{11246571936}&_{\color{red}{-56339106560}}&_{\color{red}{-24293530800}}&_{23789010960}&_{15281426160}&_{\color{red}{-4797833040}}&_{\color{red}{-4443238800}}&_{367567200}&_{620269650}&_0&_{\color{red}{-34459425}}\\ \hline &&_{3715891200}&_{\color{red}{-45403176960}}&_{88809394176}&_{304618168320}&_{\color{red}{-542481679360}}&_{\color{red}{-1196513206784}}&_{1056393994880}&_{2677512110336}&_{\color{red}{-555243289280}}&_{\color{red}{-3204955762240}}&_{\color{red}{-568295633664}}&_{2050613136000}&_{855124189920}&_{\color{red}{-696743087040}}&_{\color{red}{-432280648800}}&_{118062584640}&_{106686379800}&_{\color{red}{-7856748900}}&_{\color{red}{-13094581500}}&_0&_{654729075}\\ \hline &_{81749606400}&_{\color{red}{-1163994071040}}&_{3157404549120}&_{7404607168512}&_{\color{red}{-22961450293248}}&_{\color{red}{-33019002361856}}&_{63005932451840}&_{97343743979776}&_{\color{red}{-75013307410688}}&_{\color{red}{-161351897989888}}&_{23949914036224}&_{147655256436480}&_{27950317851456}&_{\color{red}{-75531748131840}}&_{\color{red}{-30823056744480}}&_{21371092142400}&_{12914289788400}&_{\color{red}{-3117557963520}}&_{\color{red}{-2762956696500}}&_{183324141000}&_{302484832650}&_0&_{\color{red}{-13749310575}}\\ \hline _{1961990553600}&_{\color{red}{-32062337187840}}&_{112598807740416}& _{171315761872896}&_{\color{red}{-929295126183936}}&_{\color{red}{-785427471704064}}&_{3224048625086464}&_{3108838134571008}&_{\color{red}{-5607501784821504}}&_{\color{red}{-7203781137012736}}&_{4632255496628736}&_{9229708405761024}&_{\color{red}{-892152626613248}}&_{\color{red}{-6733346574864000}}&_{\color{red}{-1350185114678400}}&_{2848465773753600}&_{1145257280687520}&_{\color{red}{-688389482420640}}&_{\color{red}{-407477187117000}}&_{88068917336400}&_{76831147493100}&_{\color{red}{-4638100767300}}&_{\color{red}{-7589619437400}}&_0&_{316234143225} \end{array}$$ I highlighted the negative numbers with red, at the beginning I thought it changed sign every $2$ but going beyond $P_9(x)$ you see that this is not the case","Context I was tring to find a way to evaluate the infinite tetration of defined as: 1° attempt Considering we can notice that , solving this equation we have that: Where is the Lambert function. 2° attempt I tried to solve it by separating the absolute value and the argument: I tried to graphically look at their representation considering and Now I have this system: I tried to calculate the inverse function that appears in the first equation: and after several steps I found that I had to calculate the following residue: Aspect of interest Is that both functions are: In addition to both being even functions, it turns out that although these two functions are not periodic, they intersect infinitely with tangents that are always perpendicular: Question How can I calculate the closed form of this residue? Where is the Lambert function. Considerig for brevity, the first cases are: So in general I think the general solution has this form: Where is a degree polynomial with the following properties: ( is the double factorial) The term of degree is The term of degree is zero. The term of degree is The first polynomials I was able to compute are: Etc... I put the coefficients from to in this triangular pattern to better visualize the values: I highlighted the negative numbers with red, at the beginning I thought it changed sign every but going beyond you see that this is not the case","i {}^{\infty}i:=i^{i^{i^{.^{.^{.}}}}} z=i^{i^{i^{.^{.^{.}}}}} z=i^z z=\dfrac{2i}{\pi}W\left(-\dfrac{\pi i }{2}\right) W(x) z=\rho e^{i\theta}=x+iy x+iy=i^{x+iy}\\
\rho e^{i\theta}=e^{\frac{\pi}{2} i(x+iy)}\\
\rho e^{i\theta}=e^{\frac{\pi}{2} ix-\frac{\pi}{2} y}\\
\rho=e^{-\frac{\pi}{2} y}\qquad \theta=\dfrac{\pi}{2} x
 \rho=\sqrt{x^2+y^2} \theta=\arctan\left(\dfrac{y}{x}\right) \begin{cases}\sqrt{x^2+y^2}=e^{-\frac{\pi}{2} y}\\\arctan\left(\dfrac{y}{x}\right)=\dfrac{\pi}{2} x\end{cases}\quad\Rightarrow\quad\begin{cases}x^2+y^2=e^{-\pi y}\\y=\tan\left(\dfrac{\pi x}{2}\right)x\end{cases} x=\sqrt{e^{-\pi y}-y^2} \underset{x=\frac{2}{\pi}W\left(\frac{\pi}{2}\right)}{\text{Res}}\dfrac{1}{\left(e^{-\pi x}-x^2\right)^n}\qquad\text{for }n\in\mathbb{N}^{+} \color{blue}{w_n:=\underset{x=\frac{2}{\pi}W\left(\frac{\pi}{2}\right)}{\text{Res}}\dfrac{1}{\left(e^{-\pi x}-x^2\right)^n}\qquad\text{for }n\in\mathbb{N}^{+}} W(z) \color{red}{k:=W\left(\frac{\pi}{2}\right)} w_1=-\left(\dfrac{\pi}{k(k+1)}\right)^1\cdot\dfrac{1}{4} w_2=+\left(\dfrac{\pi}{k(k+1)}\right)^3\cdot\dfrac{2k^2-1}{32} w_3=-\left(\dfrac{\pi}{k(k+1)}\right)^5\cdot\dfrac{8k^4-4k^3-12k^2+3}{512} w_4=+\left(\dfrac{\pi}{k(k+1)}\right)^7\cdot\dfrac{48k^6-64k^5-132k^4+40k^3+90k^2-15}{12288} w_5=-\left(\dfrac{\pi}{k(k+1)}\right)^9\cdot\dfrac{384k^8-928k^7-1448k^6+1424k^5+1980k^4-420k^2-840k^2+105}{393216} \color{blue}{w_n=(-1)^n\left(\dfrac{\pi}{k(k+1)}\right)^{2n-1}\cdot\dfrac{P_{n}(k)}{2^{3n-1}\cdot(n-1)!}} P_{n}(x) (2n-2) P_n(-1)=(2n-3)!!\qquad x!! 0 (-1)^{n-1}\cdot (2n-3)!! 1 n (2n-2)!!=2^{n-1}(n-1)! P_{1}(x)=1 P_{2}(x)=2x^2-1 P_{3}(x)=8x^4-4x^3-12x^2+3 P_{4}(x)=48x^6-64x^5-132x^4+40x^3+90x^2-15 P_{5}(x)=384x^8-928x^7-1448x^6+1424 x^5+1980x^4-420x^3-840x^2+105 P_{6}(x)=3840x^{10}-14208x^9-15488 x^8+37424 x^7+36880x^6-26544 x^5-31080x^4+5040x^3+9450x^2-945 P_{7}(x)=46080 x^{12}-237312 x^{11}-138624 x^{10}+903424x^9+567408 x^8-1116480x^7-901040x^6+497952 x^5+529200 x^4-69300 x^3-124740x^2+ 10395 P_{8}(x)=645120x^{14}-4349952x^{13}-118016x^{12}+21436160 x^{11}+4877280x^{10}-39785600 x^9-20113072x^8+30796704 x^7 + 22251600x^6-9868320x^5-9854460x^4+1081080x^3+1891890 x^2 - 135135 P_{9}(x)=10321920 x^{16}- 87330816 x^{15}+ 50932224 x^{14}+ 512583680 x^{13}- 135690880 x^{12}- 1302740160 x^{11}- 221455936 x^{10}+ 1534016384 x^9+ 699496560 x^8- 845079840 x^7 - 569451960 x^6+ 209729520 x^5+200540340 x^4- 18918900 x^3- 32432400 x^2+2027025 P_1(x) P_{13}(x) \begin{array}{rrrrrrrrrrrrcllllllllllll}
&&&&&&&&&&&&_1\\ \hline
&&&&&&&&&&&_2&_0&_{\color{red}{-1}}\\ \hline
&&&&&&&&&&_8&_{\color{red}{-4}}&_{\color{red}{-12}}&_0&_3\\ \hline
&&&&&&&&&_{48}&_{\color{red}{-64}}&_{\color{red}{-132}}&_{40}&_{90}&_0&_{\color{red}{-15}}\\ \hline
&&&&&&&&_{384}&_{\color{red}{-928}}&_{\color{red}{-1448}}&_{1424}&_{1980}&_{\color{red}{-420}}&_{\color{red}{-840}}&_0&_{105}\\ \hline
&&&&&&&_{3840}&_{\color{red}{-14208}}&_{\color{red}{-15488}}&_{37424}&_{36880}&_{\color{red}{-26544}}&_{\color{red}{-31080}}&_{5040}&_{9450}&_0&_{\color{red}{-945}}\\ \hline
&&&&&&_{46080}&_{\color{red}{-237312}}&_{\color{red}{-138624}}&_{903424}&_{567408}&_{\color{red}{-1116480}}&_{\color{red}{-901040}}&_{497952}&_{529200}&_{\color{red}{-69300}}&_{\color{red}{-124740}}&_0&_{10395}\\ \hline
&&&&&_{645120}&_{\color{red}{-4349952}}&_{\color{red}{-118016}}&_{21436160}&_{4877280}&_{\color{red}{-39785600}}&_{\color{red}{-20113072}}&_{30796704}&_{22251600}&_{\color{red}{-9868320}}&_{\color{red}{-9854460}}&_{1081080}&_{1891890}&_0&_{\color{red}{-135135}}\\ \hline
&&&&_{10321920}&_{\color{red}{-87330816}}&_{50932224}&_{512583680}&_{\color{red}{-135690880}}&_{\color{red}{-1302740160}}&_{\color{red}{-221455936}}&_{1534016384}&_{699496560}&_{\color{red}{-845079840}}&_{\color{red}{-569451960}}&_{209729520}&_{200540340}&_{\color{red}{-18918900}}&_{\color{red}{-32432400}}&_0&_{2027025}\\ \hline
&&&_{185794560}&_{\color{red}{-1911767040}}&_{2380548096}&_{12435495936}&_{\color{red}{-11230101760}}&_{\color{red}{-40444523520}}&_{10605697600}&_{67185673856}&_{11246571936}&_{\color{red}{-56339106560}}&_{\color{red}{-24293530800}}&_{23789010960}&_{15281426160}&_{\color{red}{-4797833040}}&_{\color{red}{-4443238800}}&_{367567200}&_{620269650}&_0&_{\color{red}{-34459425}}\\ \hline
&&_{3715891200}&_{\color{red}{-45403176960}}&_{88809394176}&_{304618168320}&_{\color{red}{-542481679360}}&_{\color{red}{-1196513206784}}&_{1056393994880}&_{2677512110336}&_{\color{red}{-555243289280}}&_{\color{red}{-3204955762240}}&_{\color{red}{-568295633664}}&_{2050613136000}&_{855124189920}&_{\color{red}{-696743087040}}&_{\color{red}{-432280648800}}&_{118062584640}&_{106686379800}&_{\color{red}{-7856748900}}&_{\color{red}{-13094581500}}&_0&_{654729075}\\ \hline
&_{81749606400}&_{\color{red}{-1163994071040}}&_{3157404549120}&_{7404607168512}&_{\color{red}{-22961450293248}}&_{\color{red}{-33019002361856}}&_{63005932451840}&_{97343743979776}&_{\color{red}{-75013307410688}}&_{\color{red}{-161351897989888}}&_{23949914036224}&_{147655256436480}&_{27950317851456}&_{\color{red}{-75531748131840}}&_{\color{red}{-30823056744480}}&_{21371092142400}&_{12914289788400}&_{\color{red}{-3117557963520}}&_{\color{red}{-2762956696500}}&_{183324141000}&_{302484832650}&_0&_{\color{red}{-13749310575}}\\ \hline
_{1961990553600}&_{\color{red}{-32062337187840}}&_{112598807740416}& _{171315761872896}&_{\color{red}{-929295126183936}}&_{\color{red}{-785427471704064}}&_{3224048625086464}&_{3108838134571008}&_{\color{red}{-5607501784821504}}&_{\color{red}{-7203781137012736}}&_{4632255496628736}&_{9229708405761024}&_{\color{red}{-892152626613248}}&_{\color{red}{-6733346574864000}}&_{\color{red}{-1350185114678400}}&_{2848465773753600}&_{1145257280687520}&_{\color{red}{-688389482420640}}&_{\color{red}{-407477187117000}}&_{88068917336400}&_{76831147493100}&_{\color{red}{-4638100767300}}&_{\color{red}{-7589619437400}}&_0&_{316234143225}
\end{array} 2 P_9(x)","['calculus', 'limits', 'polynomials', 'residue-calculus', 'lambert-w']"
72,How to find the limit of a composite function?,How to find the limit of a composite function?,,"I think there's a law something like this $\lim_{x\to a}f(g(x))= f(\lim_{x\to a} g(x))$ but it has two conditions: Limit of g(x) at $a$ should exist. $f(x)$ should be continuous at $\lim_{x\to a} g(x)$ . In the picture, I don't think $f(x)$ is continuous so how would I find this limit? There's an answer given in the book but I don't understand it. Any help would be appreciated. Edit : The answers already given are understood by me, but I am not sure if they are correct, so if you could give a specific credible answer, I'd appreciate that. Thanks!","I think there's a law something like this but it has two conditions: Limit of g(x) at should exist. should be continuous at . In the picture, I don't think is continuous so how would I find this limit? There's an answer given in the book but I don't understand it. Any help would be appreciated. Edit : The answers already given are understood by me, but I am not sure if they are correct, so if you could give a specific credible answer, I'd appreciate that. Thanks!",\lim_{x\to a}f(g(x))= f(\lim_{x\to a} g(x)) a f(x) \lim_{x\to a} g(x) f(x),"['limits', 'limits-without-lhopital']"
73,"If $\,\lim_{n\to\infty}f(nx)\,$ exists, for all $x\in\mathbb R$, then so does $\,\lim_{x\to\infty}f(x)\,$","If  exists, for all , then so does","\,\lim_{n\to\infty}f(nx)\, x\in\mathbb R \,\lim_{x\to\infty}f(x)\,","Let $\,f:\mathbb{R}\to\mathbb{R}$. The following limit exists for all $x \in \mathbb{R}$:  $$\lim_{n\to ∞} f(nx) ,$$ where $n \in \mathbb N$. Is it correct that: $$\lim_{x\to ∞} f(x) ,$$  exists if: a) $f$ any function, b) $f$ continuous on $\mathbb{R}$.","Let $\,f:\mathbb{R}\to\mathbb{R}$. The following limit exists for all $x \in \mathbb{R}$:  $$\lim_{n\to ∞} f(nx) ,$$ where $n \in \mathbb N$. Is it correct that: $$\lim_{x\to ∞} f(x) ,$$  exists if: a) $f$ any function, b) $f$ continuous on $\mathbb{R}$.",,"['calculus', 'real-analysis', 'limits', 'continuity']"
74,Behaviour and limits of $f(n+1) = \frac{f^5(n)}{2} - f(n-1)$,Behaviour and limits of,f(n+1) = \frac{f^5(n)}{2} - f(n-1),"Let $f(0) = 0,f(1) = \frac{1}{2}$ and $$f(n+1) = \frac{f^5(n)}{2} - f(n-1)$$ where $*^5$ is a power. Then it seems $$ \sup f(n) = \lim \sup f(n) = \frac{1}{2}$$ and $$ \inf f(n) = \lim \inf f(n) = \frac{-1}{2}$$ Is this true ? How to prove it ? background Why I wonder about this ? Well for most values $0<q<1$ I found that $f(0) = 0,f(1) = q$ and $$f(n+1) = \frac{f^5(n)}{2} - f(n-1)$$ Then $$ \sup f(n) = \lim \sup f(n) \neq q$$ and $$ \inf f(n) = \lim \inf f(n) \neq -q$$ So it seems remarkable that $f(1) = 1/2$ does satisfy it.",Let and where is a power. Then it seems and Is this true ? How to prove it ? background Why I wonder about this ? Well for most values I found that and Then and So it seems remarkable that does satisfy it.,"f(0) = 0,f(1) = \frac{1}{2} f(n+1) = \frac{f^5(n)}{2} - f(n-1) *^5  \sup f(n) = \lim \sup f(n) = \frac{1}{2}  \inf f(n) = \lim \inf f(n) = \frac{-1}{2} 0<q<1 f(0) = 0,f(1) = q f(n+1) = \frac{f^5(n)}{2} - f(n-1)  \sup f(n) = \lim \sup f(n) \neq q  \inf f(n) = \lim \inf f(n) \neq -q f(1) = 1/2","['limits', 'polynomials', 'dynamical-systems', 'fractions', 'limsup-and-liminf']"
75,How to find $\lim_{n\to\infty}6^n(x_n-3)$ if $x_{n+1}=\sqrt{x_n+6}$?,How to find  if ?,\lim_{n\to\infty}6^n(x_n-3) x_{n+1}=\sqrt{x_n+6},"I was dealing with the following problem: Define the sequence $x_{n+1}=\sqrt{x_n+6}$ with $x_1=1$ . Find the value of $\lim_{n\to\infty}6^n(x_n-3)$ . It was easy to see that $x_n\to 3$ . However, I find it hard to solve the limit $\lim_{n\to\infty}6^n(x_n-3)$ . I try to use the Taylor series of $\sqrt{6+x}$ at point $x=3$ , but I cannot go further. Indeed, I was also curious about whether we can find the asymptotic $$x_n\sim3+a_1A^n+a_2A^{n+1}+\cdots+a_mA^{n+m-1}+O(A^{n+m}).$$ Can someone help me? Thanks in advance.","I was dealing with the following problem: Define the sequence with . Find the value of . It was easy to see that . However, I find it hard to solve the limit . I try to use the Taylor series of at point , but I cannot go further. Indeed, I was also curious about whether we can find the asymptotic Can someone help me? Thanks in advance.",x_{n+1}=\sqrt{x_n+6} x_1=1 \lim_{n\to\infty}6^n(x_n-3) x_n\to 3 \lim_{n\to\infty}6^n(x_n-3) \sqrt{6+x} x=3 x_n\sim3+a_1A^n+a_2A^{n+1}+\cdots+a_mA^{n+m-1}+O(A^{n+m}).,"['real-analysis', 'calculus', 'limits']"
76,"Prove that $f(x) =1/x^2$ is continuous on its domain $(0, \infty)$ [duplicate]",Prove that  is continuous on its domain  [duplicate],"f(x) =1/x^2 (0, \infty)","This question already has an answer here : Using delta epsilon to prove the continuity of a function (1 answer) Closed 8 months ago . Prove that the function $\smash{\dfrac{1}{x^2}}$ is continuous on its domain $(0,∞)$ . Proof: $$ \lim_{x \to a} \frac{1}{x^2} = \frac{1}{a^2}  $$ After manipulating I would get $$  \frac{(x-a)(x+a)}{x^2 a^2}  $$ where $x+a < 1 + 2\lvert a \rvert$ What do I do to the $\smash{\dfrac{1}{x^2 a^2}}$ ?",This question already has an answer here : Using delta epsilon to prove the continuity of a function (1 answer) Closed 8 months ago . Prove that the function is continuous on its domain . Proof: After manipulating I would get where What do I do to the ?,"\smash{\dfrac{1}{x^2}} (0,∞) 
\lim_{x \to a} \frac{1}{x^2} = \frac{1}{a^2} 
  
\frac{(x-a)(x+a)}{x^2 a^2} 
 x+a < 1 + 2\lvert a \rvert \smash{\dfrac{1}{x^2 a^2}}","['real-analysis', 'calculus', 'limits', 'continuity', 'epsilon-delta']"
77,Show that $\lim_{n \to \infty} x_n = a \implies \lim_{n \to \infty} |x_n| = |a|$,Show that,\lim_{n \to \infty} x_n = a \implies \lim_{n \to \infty} |x_n| = |a|,"I know this is a basic question, but I could not find a question on the site about exactly this question, even by using https://approach0.xyz/search/ Maybe I searched wrongly ? Show that $\lim_{n \to \infty} x_n = a  \;\implies\;  \lim_{n \to \infty} |x_n| = |a|$ . Let $(x_n)_{n \in \mathbb{N}}$ be a real sequence and $a \in \mathbb{R}$ . Show that $$ \lim_{n \to \infty} x_n = a  \;\implies\;  \lim_{n \to \infty} |x_n| = |a| $$ $\lim_{n \to \infty} x_n = a$ means that for each $\epsilon > 0$ there exists $N \in \mathbb{N}$ such that $\forall n > N$ there is $|x_n - a| < \epsilon$ Using the triangle inequality: $$ \epsilon > |x_n - a| \geq \bigl| |x_n| - |a| \bigr|  $$ But $\epsilon > \bigl| |x_n| - |a| \bigr|$ is the definition of $\lim_{n \to \infty} |x_n| = |a|$ This completes the proof. Note that the reverse claim $$ \lim_{n \to \infty} x_n = a  \;\Longleftarrow\;  \lim_{n \to \infty} |x_n| = |a| $$ does not hold in general. For example if $x_n = (-1)^n$ then $$\lim_{n \to \infty} |x_n| = 1$$ but $$\lim_{n \to \infty} x_n \neq 1$$ The exception being for $a = 0$ , because in this case \begin{align} \bigl| |x_n| - 0 \bigr| < \epsilon  \; &\Longleftrightarrow \;  |x_n - 0| < \epsilon \\[4pt] \lim_{n \to \infty} |x_n| = |0|  \; &\Longleftrightarrow \;  \lim_{n \to \infty} x_n  = 0  \end{align} Is my attempt correct?","I know this is a basic question, but I could not find a question on the site about exactly this question, even by using https://approach0.xyz/search/ Maybe I searched wrongly ? Show that . Let be a real sequence and . Show that means that for each there exists such that there is Using the triangle inequality: But is the definition of This completes the proof. Note that the reverse claim does not hold in general. For example if then but The exception being for , because in this case Is my attempt correct?","\lim_{n \to \infty} x_n = a 
\;\implies\; 
\lim_{n \to \infty} |x_n| = |a| (x_n)_{n \in \mathbb{N}} a \in \mathbb{R} 
\lim_{n \to \infty} x_n = a 
\;\implies\; 
\lim_{n \to \infty} |x_n| = |a|
 \lim_{n \to \infty} x_n = a \epsilon > 0 N \in \mathbb{N} \forall n > N |x_n - a| < \epsilon 
\epsilon > |x_n - a| \geq \bigl| |x_n| - |a| \bigr| 
 \epsilon > \bigl| |x_n| - |a| \bigr| \lim_{n \to \infty} |x_n| = |a| 
\lim_{n \to \infty} x_n = a 
\;\Longleftarrow\; 
\lim_{n \to \infty} |x_n| = |a|
 x_n = (-1)^n \lim_{n \to \infty} |x_n| = 1 \lim_{n \to \infty} x_n \neq 1 a = 0 \begin{align}
\bigl| |x_n| - 0 \bigr| < \epsilon 
\; &\Longleftrightarrow \; 
|x_n - 0| < \epsilon \\[4pt]
\lim_{n \to \infty} |x_n| = |0| 
\; &\Longleftrightarrow \; 
\lim_{n \to \infty} x_n  = 0 
\end{align}","['real-analysis', 'calculus', 'algebra-precalculus', 'limits', 'analysis']"
78,Does $\displaystyle \lim_{t\to 0} \frac{g(\gamma(t))}{t}=0$ imply $\displaystyle\lim_{v\to 0} \frac{g(v)}{\|v\|}=0$?,Does  imply ?,\displaystyle \lim_{t\to 0} \frac{g(\gamma(t))}{t}=0 \displaystyle\lim_{v\to 0} \frac{g(v)}{\|v\|}=0,"Let $g:U\subset \mathbb R^2 \to\mathbb R$ be a function defined in a neighborhood $U$ of $(0,0)$ . I'd like to understand the relationship between the following three limits. $\displaystyle \lim_{v\to 0} \frac{g(v)}{\|v\|}=0$ $\displaystyle \lim_{v\to 0} \frac{g(tv_0)}{t}=0$ , for all $v_0\in\mathbb R^2$ . $\displaystyle\lim_{t\to 0} \frac{g(\gamma(t))}{t}=0$ , for all differentiable path $\gamma:(-\varepsilon,\varepsilon)\to U$ such that $\gamma(0)=(0,0)$ . Remarks: Limits having the form 1 appear in the definition of differentiability. It is clear that 1 implies 2 (what is used, for example, to prove existence of the directional derivatives of differentiable functions). In general, 2 does not imply 1 (what can be seen, for example, by defining $g(0,0)=0$ and $g(x,y)=\frac{x^3y}{x^6+y^2}$ otherwise). Of course, 2 is a much weaker version of 3 (with $\gamma$ restrict to linear paths of the form $\gamma(t)=tv_0$ ). Question: Does 3 imply 1? I couldn't prove it (for example, I'm not able to connect $|v|$ with $t$ in a $\delta$ - $\epsilon$ proof; I also cannot see how could we find a $\delta$ that works for all $v$ inside a disk since each $\delta=\delta_\gamma$ works only for $v$ in the image of $\gamma$ ). Also, I couldn't construct a counterexample. In fact, in every case that I have seen in which 1 fails, there are differentiable paths that do not satisfy the limit in 3.","Let be a function defined in a neighborhood of . I'd like to understand the relationship between the following three limits. , for all . , for all differentiable path such that . Remarks: Limits having the form 1 appear in the definition of differentiability. It is clear that 1 implies 2 (what is used, for example, to prove existence of the directional derivatives of differentiable functions). In general, 2 does not imply 1 (what can be seen, for example, by defining and otherwise). Of course, 2 is a much weaker version of 3 (with restrict to linear paths of the form ). Question: Does 3 imply 1? I couldn't prove it (for example, I'm not able to connect with in a - proof; I also cannot see how could we find a that works for all inside a disk since each works only for in the image of ). Also, I couldn't construct a counterexample. In fact, in every case that I have seen in which 1 fails, there are differentiable paths that do not satisfy the limit in 3.","g:U\subset \mathbb R^2 \to\mathbb R U (0,0) \displaystyle \lim_{v\to 0} \frac{g(v)}{\|v\|}=0 \displaystyle \lim_{v\to 0} \frac{g(tv_0)}{t}=0 v_0\in\mathbb R^2 \displaystyle\lim_{t\to 0} \frac{g(\gamma(t))}{t}=0 \gamma:(-\varepsilon,\varepsilon)\to U \gamma(0)=(0,0) g(0,0)=0 g(x,y)=\frac{x^3y}{x^6+y^2} \gamma \gamma(t)=tv_0 |v| t \delta \epsilon \delta v \delta=\delta_\gamma v \gamma","['limits', 'multivariable-calculus', 'vector-analysis']"
79,Find $\lim\limits_{x\to \infty}\frac{(2+x)\sin^2 x}{x^2 (2+\sin x)}$,Find,\lim\limits_{x\to \infty}\frac{(2+x)\sin^2 x}{x^2 (2+\sin x)},"Self studying and would like some reassurance, all criticism/corrections are welcome. We show $\lim_{x\to \infty}\frac{(2+x)\sin^2 x}{x^2 (2+\sin x)} = 0$ First we note that $-1 \le \sin x \iff 1 \le 2 + \sin x \iff \frac{1}{2+\sin x} \le 1$ and $0 \le \sin^2 x \le 1$ . So we can write $\frac{(2+x)\sin^2 x}{x^2 (2+\sin x)} \le \frac{(2+x)\sin^2 x}{x^2} = \frac{\sin^2 x}{x} + \frac{2 \sin^2 x}{x^2} \le \frac{1}{x} + \frac{2}{x^2}$ and $\lim_{x\to \infty} \frac{1}{x} + \frac{2}{x^2} = 0$ . From the previous inequalities we can see $\lim\limits_{x\to \infty}\frac{(2+x)\sin^2 x}{x^2 (2+\sin x)}$ is nonnegative for $x > 0$ . We have $2+x > 0$ , $\sin^2(x) \ge 0 $ and $x^2 >0$ because they are squares, and $2 + \sin x \ge 1 > 0$ as shown above. Since these are all nonnegative, the main expression is nonnegative. Therefore for large $x$ , we have $ 0 \le \frac{(2+x)\sin^2 x}{x^2 (2+\sin x)} \le (\frac{1}{x} + \frac{2}{x^2})$ and by the squeeze theorem $\lim\limits_{x\to \infty}\frac{(2+x)\sin^2 x}{x^2 (2+\sin x)} = 0$ . (from Velleman's Calculus: A Rigorous First Course )","Self studying and would like some reassurance, all criticism/corrections are welcome. We show First we note that and . So we can write and . From the previous inequalities we can see is nonnegative for . We have , and because they are squares, and as shown above. Since these are all nonnegative, the main expression is nonnegative. Therefore for large , we have and by the squeeze theorem . (from Velleman's Calculus: A Rigorous First Course )",\lim_{x\to \infty}\frac{(2+x)\sin^2 x}{x^2 (2+\sin x)} = 0 -1 \le \sin x \iff 1 \le 2 + \sin x \iff \frac{1}{2+\sin x} \le 1 0 \le \sin^2 x \le 1 \frac{(2+x)\sin^2 x}{x^2 (2+\sin x)} \le \frac{(2+x)\sin^2 x}{x^2} = \frac{\sin^2 x}{x} + \frac{2 \sin^2 x}{x^2} \le \frac{1}{x} + \frac{2}{x^2} \lim_{x\to \infty} \frac{1}{x} + \frac{2}{x^2} = 0 \lim\limits_{x\to \infty}\frac{(2+x)\sin^2 x}{x^2 (2+\sin x)} x > 0 2+x > 0 \sin^2(x) \ge 0  x^2 >0 2 + \sin x \ge 1 > 0 x  0 \le \frac{(2+x)\sin^2 x}{x^2 (2+\sin x)} \le (\frac{1}{x} + \frac{2}{x^2}) \lim\limits_{x\to \infty}\frac{(2+x)\sin^2 x}{x^2 (2+\sin x)} = 0,"['real-analysis', 'calculus', 'limits', 'trigonometry']"
80,"Given the definition of $a_0$ and $a_n$, what is $\lim_{x\to\infty}\frac{a_n}n$?","Given the definition of  and , what is ?",a_0 a_n \lim_{x\to\infty}\frac{a_n}n,"So I was looking through Youtube to see if there were, yet again, any math equations that I thought that I might like to solve when I came across this video by Michael Penn that was apparently ""Erdős but simpler"" [ $1$ ] with this as the question: Given $$\begin{cases}a_0=&1\\a_n=&a_\left\lfloor\frac n2\right\rfloor+a_\left\lfloor \frac n3\right\rfloor\end{cases}$$ $$\lim_{n\to\infty}\dfrac{a_n}n=\,?$$ which I thought that I might be able to solve. Here is my attempt at doing so: $$\begin{align}a_1&=a_0+a_0=1+1=2\\a_2&=a_1+a_0=2+1=3\\a_3&=a_1+a_1=2+2=4\\a_4&=a_2+a_1=3+2=5\\a_5&=a_2+a_1=3+2=5\\a_6&=a_3+a_2=4+3=7\\a_7&=a_3+a_2=4+3=7\\a_8&=a_4+a_2=5+3=8\\a_9&=a_4+a_3=5+4=9\\a_{10}&=a_5+a_3=5+4=9\\a_{11}&=a_5+a_3=5+4=9\\a_{12}&=a_6+a_4=7+5=12\end{align}$$ Now I'll let $g_n=\dfrac{a_n}n$ and see what happens when we calculate $g_n$ for each of these terms that I have already calculated: $$\begin{align}g_1&=2\\g_2&=1.5\\g_3&=\dfrac43\\g_4&=1.25\\g_5&=1\\g_6&=\dfrac76\\g_7&=1\\g_8&=1\end{align}$$ Now here's where I'm stuck. I tried plugging the points that  I got into Desmos [ $2$ ] to get an equation of some sort so I could approximate an equation that I then could use to get an answer that I could use to determine the limit of $\dfrac{a_n}n$ , but it doesn't work. Here is a table of the strategies that I used and why they don't work (explanations that are too long can be found the ""Notes"" section of this question): What I have tried What I get Why it doesn't work Logarithmic regression: $y_1\sim a+b\ln(cx_1+d)$ $R^2=0.4439$ with $a=9.18093$ , $b=-2.05547$ , $c=1.3042$ , $d=42.1349$ [ $3$ ] Sinusoidal regression (with $\sin$ ): $y_1\sim a+b\sin(cx_1+d)$ $R^2=0.5272$ with $a=1.16481$ , $b=0.287905$ , $c=0.389035$ , $d=1.0123$ With $\cos$ , as we add more terms, $R^2\to0$ Sinusoidal regression (with $\cos$ ): $y_1\sim a+b\cos(cx_1+d)$ $R^2=0.5272$ with $a=1.16481$ , $b=0.287905$ , $c=0.389035$ , $d=-0.5588498$ See above Exponential regression: $y_1\sim ab^{x_1}+c$ $R^2=0.4447$ with $a=1.787828$ , $b=0.963084$ , $c=-0.288955$ Again, as more terms are added, $R^2\to0$ Now, if that's the case, what can we do? I actually tried just plotting terms of $a_n$ , but that didn't really seem to work. So, at this point, I decided to just give up. My question Is there any way that I might be able to solve this problem in any way? [ $4$ ] Notes [ $1$ ] The original question (since Michael Penn had modified it) was: Given the exact same system of equations defining the recursive formula (as noted in the beginning), show that $\lim_{n\to\infty}\frac{a_n}n=\frac{12}{\ln(432)}$ ."" I had to go into the video to find this, but I wouldn't say it's cheating, since all I'm doing here is getting the original question for reference. [ $2$ ] [ $3$ ] The reason logarithmic regression seems to not be accurate here because the terms are constantly expanding, while this just eventually drops to $0$ [ $4$ ] As stated in Note $1$ , I'm supposed to get $\frac{12}{\ln(432)}$ , however I guess my question is more ""How would I achieve said answer if I didn't know that the solution was said answer ?"" now that I think about it.","So I was looking through Youtube to see if there were, yet again, any math equations that I thought that I might like to solve when I came across this video by Michael Penn that was apparently ""Erdős but simpler"" [ ] with this as the question: Given which I thought that I might be able to solve. Here is my attempt at doing so: Now I'll let and see what happens when we calculate for each of these terms that I have already calculated: Now here's where I'm stuck. I tried plugging the points that  I got into Desmos [ ] to get an equation of some sort so I could approximate an equation that I then could use to get an answer that I could use to determine the limit of , but it doesn't work. Here is a table of the strategies that I used and why they don't work (explanations that are too long can be found the ""Notes"" section of this question): What I have tried What I get Why it doesn't work Logarithmic regression: with , , , [ ] Sinusoidal regression (with ): with , , , With , as we add more terms, Sinusoidal regression (with ): with , , , See above Exponential regression: with , , Again, as more terms are added, Now, if that's the case, what can we do? I actually tried just plotting terms of , but that didn't really seem to work. So, at this point, I decided to just give up. My question Is there any way that I might be able to solve this problem in any way? [ ] Notes [ ] The original question (since Michael Penn had modified it) was: Given the exact same system of equations defining the recursive formula (as noted in the beginning), show that ."" I had to go into the video to find this, but I wouldn't say it's cheating, since all I'm doing here is getting the original question for reference. [ ] [ ] The reason logarithmic regression seems to not be accurate here because the terms are constantly expanding, while this just eventually drops to [ ] As stated in Note , I'm supposed to get , however I guess my question is more ""How would I achieve said answer if I didn't know that the solution was said answer ?"" now that I think about it.","1 \begin{cases}a_0=&1\\a_n=&a_\left\lfloor\frac n2\right\rfloor+a_\left\lfloor
\frac n3\right\rfloor\end{cases} \lim_{n\to\infty}\dfrac{a_n}n=\,? \begin{align}a_1&=a_0+a_0=1+1=2\\a_2&=a_1+a_0=2+1=3\\a_3&=a_1+a_1=2+2=4\\a_4&=a_2+a_1=3+2=5\\a_5&=a_2+a_1=3+2=5\\a_6&=a_3+a_2=4+3=7\\a_7&=a_3+a_2=4+3=7\\a_8&=a_4+a_2=5+3=8\\a_9&=a_4+a_3=5+4=9\\a_{10}&=a_5+a_3=5+4=9\\a_{11}&=a_5+a_3=5+4=9\\a_{12}&=a_6+a_4=7+5=12\end{align} g_n=\dfrac{a_n}n g_n \begin{align}g_1&=2\\g_2&=1.5\\g_3&=\dfrac43\\g_4&=1.25\\g_5&=1\\g_6&=\dfrac76\\g_7&=1\\g_8&=1\end{align} 2 \dfrac{a_n}n y_1\sim a+b\ln(cx_1+d) R^2=0.4439 a=9.18093 b=-2.05547 c=1.3042 d=42.1349 3 \sin y_1\sim a+b\sin(cx_1+d) R^2=0.5272 a=1.16481 b=0.287905 c=0.389035 d=1.0123 \cos R^2\to0 \cos y_1\sim a+b\cos(cx_1+d) R^2=0.5272 a=1.16481 b=0.287905 c=0.389035 d=-0.5588498 y_1\sim ab^{x_1}+c R^2=0.4447 a=1.787828 b=0.963084 c=-0.288955 R^2\to0 a_n 4 1 \lim_{n\to\infty}\frac{a_n}n=\frac{12}{\ln(432)} 2 3 0 4 1 \frac{12}{\ln(432)}","['limits', 'recursion']"
81,Where was the formal definition of a limit first published?,Where was the formal definition of a limit first published?,,"I've read that Bolzano was one of the first to formalize a limit with the epsilon-delta definition, but I can't find a text of his that shows his definition. Does anyone know what publication the first epsilon-delta definition of a limit can be found in?","I've read that Bolzano was one of the first to formalize a limit with the epsilon-delta definition, but I can't find a text of his that shows his definition. Does anyone know what publication the first epsilon-delta definition of a limit can be found in?",,"['real-analysis', 'limits', 'epsilon-delta', 'math-history', 'mathematicians']"
82,On the ratio test for power series; Hunter,On the ratio test for power series; Hunter,,"This is a small detail, but I'm stuck nevertheless. In these lecture notes , there is the following theorem and proof on the ratio test for power series: Theorem 10.5. Suppose that $a_n\neq0$ for all sufficiently large $n$ and the limit $$R=\lim _{n\to \infty }\left|\frac{a_n}{a_{n+1}}\right|\tag1$$ exists or diverges to infinity. Then the power series $\sum_{n=0}^\infty a_n (x-c)^n$ has radius of convergence $R$ . Proof. Let $$r=\lim_{n\to\infty}\left|\frac{a_{n+1}(x-c)^{n+1}}{a_n(x-c)^n}\right|=|x-c|\lim_{n\to\infty}\left|\frac{a_{n+1}}{a_{n}}\right|.\tag2$$ By the ratio test, the power series converges if $0\leq r<1$ , or $|x-c|<R$ , and diverges if $1<r\leq\infty$ , or $|x-c|>R$ , which proves the result. I'm confused about factoring out $|x-c|$ in $(2)$ . According to the same lecture notes, this is only possible if $\left|\frac{a_{n+1}}{a_{n}}\right|$ converges. The author has specifically written that $\lim_{n\to\infty}\left|\frac{a_n}{a_{n+1}}\right|$ exists or diverges to infinity. This makes me wonder what $\left|\frac{a_{n+1}}{a_{n}}\right|$ converges to in those cases. To elaborate, consider the case $0<R<\infty$ . Then $$\lim_{n\to\infty}\left|\frac{a_{n+1}}{a_{n}}\right|=\lim_{n\to\infty}\frac1{\left|\frac{a_{n}}{a_{n+1}}\right|}=\frac1{R}.$$ Here I've used the rule that if $\lim_{n\to\infty} a_n$ and $\lim_{n\to\infty} b_n$ converge, and $\lim_{n\to\infty} b_n\neq0$ (which implies that there is some $N$ such that $b_n\neq0$ for $n>N$ ), then $\lim_{n\to\infty} (a_n/b_n)=\lim_{n\to\infty} a_n/\lim_{n\to\infty} b_n$ . But what about $R=0$ and $R=\infty$ ? Then what is $\lim_{n\to\infty}\left|\frac{a_{n+1}}{a_{n}}\right|$ ? Can we pull out $|x-c|$ nevertheless? Is the statement of the theorem complete as it is or does it lack anything?","This is a small detail, but I'm stuck nevertheless. In these lecture notes , there is the following theorem and proof on the ratio test for power series: Theorem 10.5. Suppose that for all sufficiently large and the limit exists or diverges to infinity. Then the power series has radius of convergence . Proof. Let By the ratio test, the power series converges if , or , and diverges if , or , which proves the result. I'm confused about factoring out in . According to the same lecture notes, this is only possible if converges. The author has specifically written that exists or diverges to infinity. This makes me wonder what converges to in those cases. To elaborate, consider the case . Then Here I've used the rule that if and converge, and (which implies that there is some such that for ), then . But what about and ? Then what is ? Can we pull out nevertheless? Is the statement of the theorem complete as it is or does it lack anything?",a_n\neq0 n R=\lim _{n\to \infty }\left|\frac{a_n}{a_{n+1}}\right|\tag1 \sum_{n=0}^\infty a_n (x-c)^n R r=\lim_{n\to\infty}\left|\frac{a_{n+1}(x-c)^{n+1}}{a_n(x-c)^n}\right|=|x-c|\lim_{n\to\infty}\left|\frac{a_{n+1}}{a_{n}}\right|.\tag2 0\leq r<1 |x-c|<R 1<r\leq\infty |x-c|>R |x-c| (2) \left|\frac{a_{n+1}}{a_{n}}\right| \lim_{n\to\infty}\left|\frac{a_n}{a_{n+1}}\right| \left|\frac{a_{n+1}}{a_{n}}\right| 0<R<\infty \lim_{n\to\infty}\left|\frac{a_{n+1}}{a_{n}}\right|=\lim_{n\to\infty}\frac1{\left|\frac{a_{n}}{a_{n+1}}\right|}=\frac1{R}. \lim_{n\to\infty} a_n \lim_{n\to\infty} b_n \lim_{n\to\infty} b_n\neq0 N b_n\neq0 n>N \lim_{n\to\infty} (a_n/b_n)=\lim_{n\to\infty} a_n/\lim_{n\to\infty} b_n R=0 R=\infty \lim_{n\to\infty}\left|\frac{a_{n+1}}{a_{n}}\right| |x-c|,"['real-analysis', 'calculus', 'sequences-and-series', 'limits', 'power-series']"
83,Is the average distance of a prime from its nearest square $1/3$ the average distance from the next nearest square?,Is the average distance of a prime from its nearest square  the average distance from the next nearest square?,1/3,"Let $p_n$ be the $n$ -th prime. Let $l_n = p_n - \lfloor \sqrt{p_n} \rfloor^{2}$ be the gap between a prime and the nearest square before the prime and $u_n= (\lfloor \sqrt{p_n} \rfloor + 1)^{2} - p_n$ be the gap between a prime and the nearest square after the prime. Is it true that $$ \lim_{m \to \infty}  \frac{\sum_{n=1}^{m}\max(l_n, u_n)}{\sum_{n=1}^{m}\min(l_n, u_n)} \stackrel{?}{=} 3 $$ My experimental data shows that: $m=1000000$ , ratio $=2.999936$ $m= 100000000$ , ratio $= 3.000107$ $m= 1000000000$ , ratio $= 3.000083$","Let be the -th prime. Let be the gap between a prime and the nearest square before the prime and be the gap between a prime and the nearest square after the prime. Is it true that My experimental data shows that: , ratio , ratio , ratio","p_n n l_n = p_n - \lfloor \sqrt{p_n} \rfloor^{2} u_n= (\lfloor \sqrt{p_n} \rfloor + 1)^{2} - p_n 
\lim_{m \to \infty}  \frac{\sum_{n=1}^{m}\max(l_n, u_n)}{\sum_{n=1}^{m}\min(l_n, u_n)} \stackrel{?}{=} 3
 m=1000000 =2.999936 m= 100000000 = 3.000107 m= 1000000000 = 3.000083","['limits', 'number-theory', 'elementary-number-theory', 'prime-numbers', 'asymptotics']"
84,Can the limit at an essential singularity be infinite from all directions?,Can the limit at an essential singularity be infinite from all directions?,,"Suppose a function $f$ is holomorphic in a punctured neighborhood of a point $z_0 \in \mathbb{C}$ . Suppose that for all fixed $\theta \in \mathbb{R}$ , $\lim_{r \to 0} |f(re^{i\theta})|=\infty$ . Does it necessarily follow that $f$ has a pole at $z_0$ ? Or is it possible for the singularity to be essential? (Recall that if $\lim_{z \to z_0} |f(z)|=\infty$ , then $f$ must have a pole. But I'm requiring only the limit from any fixed direction to be infinite.) If the latter, this should be one of those funny counterexamples in analysis. On the other hand, the compactness of $S^1$ vaguely suggests to me that it has to be a pole.","Suppose a function is holomorphic in a punctured neighborhood of a point . Suppose that for all fixed , . Does it necessarily follow that has a pole at ? Or is it possible for the singularity to be essential? (Recall that if , then must have a pole. But I'm requiring only the limit from any fixed direction to be infinite.) If the latter, this should be one of those funny counterexamples in analysis. On the other hand, the compactness of vaguely suggests to me that it has to be a pole.",f z_0 \in \mathbb{C} \theta \in \mathbb{R} \lim_{r \to 0} |f(re^{i\theta})|=\infty f z_0 \lim_{z \to z_0} |f(z)|=\infty f S^1,"['complex-analysis', 'limits', 'singularity']"
85,Proving that: $\lim\limits_{n\to\infty} \left(\frac{a^{\frac{1}{n}}+b^{\frac{1}{n}}}{2}\right)^n =\sqrt{ab}$,Proving that:,\lim\limits_{n\to\infty} \left(\frac{a^{\frac{1}{n}}+b^{\frac{1}{n}}}{2}\right)^n =\sqrt{ab},Let $a$ and $b$ be positive reals. Show that $$\lim\limits_{n\to\infty} \left(\frac{a^{\frac{1}{n}}+b^{\frac{1}{n}}}{2}\right)^n =\sqrt{ab}$$,Let $a$ and $b$ be positive reals. Show that $$\lim\limits_{n\to\infty} \left(\frac{a^{\frac{1}{n}}+b^{\frac{1}{n}}}{2}\right)^n =\sqrt{ab}$$,,"['real-analysis', 'limits', 'real-numbers', 'radicals', 'means']"
86,A proof for $\left ( \lim_{x\rightarrow 0} {sin(x) \over x} \right )$ using Euler reflection formula.,A proof for  using Euler reflection formula.,\left ( \lim_{x\rightarrow 0} {sin(x) \over x} \right ),"I found a proof for $\left( \lim\limits_{x\rightarrow 0} {\sin(x) \over x} \right)$ using Euler reflection formula I would like to ask if this proof can be strong mathematical proof? so let me start by Euler reflection formula. $${\pi \over {\sin(\pi a)} } = \Gamma (a)\Gamma (1-a)$$ where $a\notin \mathbb{Z}$ let $x = \pi a \Rightarrow a = {x \over \pi}$ So the formula can be written as: $$\sin(x) = {\pi \over {\Gamma\left({x \over \pi}\right) \Gamma\left(1 - {x \over \pi}\right)}}$$ where ${x \over \pi}\notin \mathbb{Z}$ Thus, we have. $$ \lim_{x \rightarrow 0}{\sin(x) \over x} = \lim_{x \rightarrow 0}{1 \over {{x \over \pi}\Gamma\left({x \over \pi}\right) \Gamma\left(1 - {x \over \pi}\right)}}$$ $$= \lim_{x \rightarrow 0}{1 \over {\Gamma\left({{x \over \pi} + 1}\right) \Gamma\left(1 - {x \over \pi}\right)}}$$ $$ = {1 \over {\Gamma({0 + 1}) \Gamma(1 - 0)}}  = {1 \over {\Gamma({1})^2}} = 1$$","I found a proof for using Euler reflection formula I would like to ask if this proof can be strong mathematical proof? so let me start by Euler reflection formula. where let So the formula can be written as: where Thus, we have.",\left( \lim\limits_{x\rightarrow 0} {\sin(x) \over x} \right) {\pi \over {\sin(\pi a)} } = \Gamma (a)\Gamma (1-a) a\notin \mathbb{Z} x = \pi a \Rightarrow a = {x \over \pi} \sin(x) = {\pi \over {\Gamma\left({x \over \pi}\right) \Gamma\left(1 - {x \over \pi}\right)}} {x \over \pi}\notin \mathbb{Z}  \lim_{x \rightarrow 0}{\sin(x) \over x} = \lim_{x \rightarrow 0}{1 \over {{x \over \pi}\Gamma\left({x \over \pi}\right) \Gamma\left(1 - {x \over \pi}\right)}} = \lim_{x \rightarrow 0}{1 \over {\Gamma\left({{x \over \pi} + 1}\right) \Gamma\left(1 - {x \over \pi}\right)}}  = {1 \over {\Gamma({0 + 1}) \Gamma(1 - 0)}}  = {1 \over {\Gamma({1})^2}} = 1,['limits']
87,$\lim_{h\to 0}\int_{\mathbb{R}}|f_h(x)-g(x)|^2dx=0$,,\lim_{h\to 0}\int_{\mathbb{R}}|f_h(x)-g(x)|^2dx=0,"I can do (a) and (b), but I cannot do (c). I have attempted to use $$f_n(x)=\frac{f(x+\frac{1}{n})-f(x)}{\frac{1}{n}},$$ but I do not know how to proceed. (a) If $f(x) \in L^2(X)$ and $\mu(X)<\infty$ , show that $f(x) \in L^1(X)$ . (b) If $f(x)$ , $x f(x) \in L^2(\mathbb{R})$ , show that $f(x) \in L^1(\mathbb{R})$ . (c) If $f(x)$ , $g(x) \in L^2(\mathbb{R})$ and $$\lim _{h \rightarrow 0} \int_{\mathbb{R}}\left|f_h(x)-g(x)\right|^2 d x=0,$$ where $$f_h(x):=\frac{f(x+h)-f(x)}{h} \quad \text { for any } h \in \mathbb{R} \setminus \{0\},$$ show that $$f(x)=\int_{[0, x]} g(t) d t+C $$ for some constant $C$ .","I can do (a) and (b), but I cannot do (c). I have attempted to use but I do not know how to proceed. (a) If and , show that . (b) If , , show that . (c) If , and where show that for some constant .","f_n(x)=\frac{f(x+\frac{1}{n})-f(x)}{\frac{1}{n}}, f(x) \in L^2(X) \mu(X)<\infty f(x) \in L^1(X) f(x) x f(x) \in L^2(\mathbb{R}) f(x) \in L^1(\mathbb{R}) f(x) g(x) \in L^2(\mathbb{R}) \lim _{h \rightarrow 0} \int_{\mathbb{R}}\left|f_h(x)-g(x)\right|^2 d x=0, f_h(x):=\frac{f(x+h)-f(x)}{h} \quad \text { for any } h \in \mathbb{R} \setminus \{0\}, f(x)=\int_{[0, x]} g(t) d t+C
 C","['real-analysis', 'integration', 'limits', 'sequence-of-function']"
88,Laurent series of the inverse of a perturbed singular matrix,Laurent series of the inverse of a perturbed singular matrix,,"Let $M(\epsilon)$ be the square matrix $M(\epsilon) = A + \epsilon B$ where $A,B$ are square matrices. Throughout this post, suppose that $M(\epsilon)$ is invertible for arbitrarily small but positive $\epsilon$ , but $M(0) = A$ is not invertible (note that my question becomes very easy when $A$ is invertible by just using the Neumann series of the inverse). I'm interested in finding the limiting behavior of the inverse $M(\epsilon)^{-1}$ as $\epsilon\to 0^+$ . Specifically, I'd like to expand it as $$M(\epsilon)^{-1} = \sum_{n=-\infty}^\infty D_n \epsilon^n,$$ where $D_n$ are some matrices. How can I find the matrices $D_n$ ? Ultimately, I am interested in the limiting behavior of the solution $x$ to the equation $M(\epsilon)x = c$ , where $x,c$ are vectors. Specifically, suppose that $x(\epsilon)$ is the solution to $M(\epsilon)x(\epsilon) = c$ . I'm interested in knowing $$\lim_{\epsilon\to 0^+} \frac{x(\epsilon)}{\lVert x(\epsilon) \rVert_{\infty}} = \lim_{\epsilon\to 0^+} \frac{M(\epsilon)^{-1}c}{\lVert M(\epsilon)^{-1}c \rVert_{\infty}}.$$ If $M(\epsilon)^{-1} = \sum_{n=s}^\infty D_n \epsilon^n$ for some finite $s\in\mathbb Z$ , then I think it follows that $\lim_{\epsilon\to 0^+} \frac{x(\epsilon)}{\lVert x(\epsilon) \rVert_{\infty}} = \frac{D_s c}{\lVert D_s c \rVert_\infty}$ . Hence, I want to determine $s$ and $D_s$ .","Let be the square matrix where are square matrices. Throughout this post, suppose that is invertible for arbitrarily small but positive , but is not invertible (note that my question becomes very easy when is invertible by just using the Neumann series of the inverse). I'm interested in finding the limiting behavior of the inverse as . Specifically, I'd like to expand it as where are some matrices. How can I find the matrices ? Ultimately, I am interested in the limiting behavior of the solution to the equation , where are vectors. Specifically, suppose that is the solution to . I'm interested in knowing If for some finite , then I think it follows that . Hence, I want to determine and .","M(\epsilon) M(\epsilon) = A + \epsilon B A,B M(\epsilon) \epsilon M(0) = A A M(\epsilon)^{-1} \epsilon\to 0^+ M(\epsilon)^{-1} = \sum_{n=-\infty}^\infty D_n \epsilon^n, D_n D_n x M(\epsilon)x = c x,c x(\epsilon) M(\epsilon)x(\epsilon) = c \lim_{\epsilon\to 0^+} \frac{x(\epsilon)}{\lVert x(\epsilon) \rVert_{\infty}} = \lim_{\epsilon\to 0^+} \frac{M(\epsilon)^{-1}c}{\lVert M(\epsilon)^{-1}c \rVert_{\infty}}. M(\epsilon)^{-1} = \sum_{n=s}^\infty D_n \epsilon^n s\in\mathbb Z \lim_{\epsilon\to 0^+} \frac{x(\epsilon)}{\lVert x(\epsilon) \rVert_{\infty}} = \frac{D_s c}{\lVert D_s c \rVert_\infty} s D_s","['linear-algebra', 'limits', 'numerical-linear-algebra', 'laurent-series']"
89,"For finding limit of $ \displaystyle\lim_{x\to0} \dfrac{e^{1/x} - 1}{e^{1/x} +1}$ , why L'Hospital rule is not working?","For finding limit of  , why L'Hospital rule is not working?", \displaystyle\lim_{x\to0} \dfrac{e^{1/x} - 1}{e^{1/x} +1},"So for evaluating limit of $\displaystyle\lim_{x\to0} \frac{e^{1/x} - 1}{e^{1/x} +1 } $ I used De l'Hospital rule, as conditions are being satisfied, a) $f(x),g(x)\to \infty $ b) both are differential And upon using L'Hospital rule I get $\displaystyle\lim_{x\to0} \frac{e^{1/x} (-1/x^2)}{e^{1/x} (-1/x^2)} $ , cancelling denominator with numerator i get 1. But evaluating left hand and right hand limit separately shows that this function does not have limiting value at $x\to 0$ I don't understand what I am doing wrong ?","So for evaluating limit of I used De l'Hospital rule, as conditions are being satisfied, a) b) both are differential And upon using L'Hospital rule I get , cancelling denominator with numerator i get 1. But evaluating left hand and right hand limit separately shows that this function does not have limiting value at I don't understand what I am doing wrong ?","\displaystyle\lim_{x\to0} \frac{e^{1/x} - 1}{e^{1/x} +1 }  f(x),g(x)\to \infty  \displaystyle\lim_{x\to0} \frac{e^{1/x} (-1/x^2)}{e^{1/x} (-1/x^2)}  x\to 0",['limits']
90,Limit of a sequence of functions $f_n$ defined implicitly from the sequence of functions $F_n$ by the Implicit Function Theorem,Limit of a sequence of functions  defined implicitly from the sequence of functions  by the Implicit Function Theorem,f_n F_n,"I have a sequence of functions $f_n(x,y)$ where each $f_n$ is defined by the Implicit Function Theorem from a particular $C^1$ function $F_n(x,y,z)$ , that is, $F_n(x,y,f_n(x,y))=0$ . The sequence $F_n$ is converging pointwise to the function $F$ which is also $C^1$ . I wonder if we can conclude that $f_n$ converges pointwise to the function $f$ defined implicitly from $F$ by the Implicit Function Theorem? I haven't been able to find such a result. Thank you for any insight.","I have a sequence of functions where each is defined by the Implicit Function Theorem from a particular function , that is, . The sequence is converging pointwise to the function which is also . I wonder if we can conclude that converges pointwise to the function defined implicitly from by the Implicit Function Theorem? I haven't been able to find such a result. Thank you for any insight.","f_n(x,y) f_n C^1 F_n(x,y,z) F_n(x,y,f_n(x,y))=0 F_n F C^1 f_n f F","['limits', 'multivariable-calculus', 'implicit-function-theorem']"
91,Apostol: Why exactly is $\frac{1}{2}\sqrt{\frac{y}{x}}$ not defined at origin if the partial derivative of $\sqrt{|xy|}$ exists at origin?,Apostol: Why exactly is  not defined at origin if the partial derivative of  exists at origin?,\frac{1}{2}\sqrt{\frac{y}{x}} \sqrt{|xy|},"Consider the scalar field $f(x,y)=\sqrt{|xy|}$ . Here is a $3D$ plot In Apostol's Calculus we are asked to verify that the partial derivatives $\frac{\partial f}{\partial x}$ and $\frac{\partial f}{\partial y}$ are zero at the origin. I know how to show this by using the definition of partial derivatives. For example $$\frac{\partial f(0,0)}{\partial x}=\lim\limits_{h\to 0} \frac{f(\langle 0,0\rangle+h\langle 1,0\rangle)-f(0,0)}{h}\tag{1}$$ $$\lim\limits_{h\to 0} \frac{f(h,0)-f(0,0)}{h}\tag{2}$$ $$=0$$ But my question regards the expression $$\frac{\partial f(x,y)}{\partial x}=\frac{1}{2}\sqrt{\frac{y}{x}}\tag{3}$$ which is the partial derivative when $x>0$ and $y>0$ (alternatively, when both are negative). What are the precise reasons that this expression isn't true at the origin, and yet the partial derivative is defined at the origin? Note that this expression isn't defined at the origin. But why? It is defined for all points except the origin on the line of points of form $(x,0)$ . To obtain (3), we took the limit of $$\frac{f(x+h,y)-f(x,y)}{h}\tag{4}$$ and the resulting expression isn't defined at the origin. On the other hand to show that the partial derivative does exist at the origin we took the limit of $$\frac{f(0+h,0)-f(0,0)}{h}\tag{5}$$ It is not clear to me why the limit of this expression is defined, but the limit of (4) evaluated at the origin isn't defined.","Consider the scalar field . Here is a plot In Apostol's Calculus we are asked to verify that the partial derivatives and are zero at the origin. I know how to show this by using the definition of partial derivatives. For example But my question regards the expression which is the partial derivative when and (alternatively, when both are negative). What are the precise reasons that this expression isn't true at the origin, and yet the partial derivative is defined at the origin? Note that this expression isn't defined at the origin. But why? It is defined for all points except the origin on the line of points of form . To obtain (3), we took the limit of and the resulting expression isn't defined at the origin. On the other hand to show that the partial derivative does exist at the origin we took the limit of It is not clear to me why the limit of this expression is defined, but the limit of (4) evaluated at the origin isn't defined.","f(x,y)=\sqrt{|xy|} 3D \frac{\partial f}{\partial x} \frac{\partial f}{\partial y} \frac{\partial f(0,0)}{\partial x}=\lim\limits_{h\to 0} \frac{f(\langle 0,0\rangle+h\langle 1,0\rangle)-f(0,0)}{h}\tag{1} \lim\limits_{h\to 0} \frac{f(h,0)-f(0,0)}{h}\tag{2} =0 \frac{\partial f(x,y)}{\partial x}=\frac{1}{2}\sqrt{\frac{y}{x}}\tag{3} x>0 y>0 (x,0) \frac{f(x+h,y)-f(x,y)}{h}\tag{4} \frac{f(0+h,0)-f(0,0)}{h}\tag{5}","['limits', 'multivariable-calculus', 'partial-derivative']"
92,Solution to $\lim_{n\rightarrow\infty}\int_{-\infty}^\infty\frac{\sin^{\circ n}x}{x}dx$,Solution to,\lim_{n\rightarrow\infty}\int_{-\infty}^\infty\frac{\sin^{\circ n}x}{x}dx,"This was a question that I thought about as I was playing around with the cardinal sine function: Find $$\lim_{n\rightarrow\infty}\int_{-\infty}^\infty\frac{\sin^{\circ n}x}{x}dx$$ with ${}^\circ$ denoting functional composition. I don't really know how to approach this question, but intuition tells me that this approaches $\pi$ . Maybe it doesn't even converge. I have no idea. WA says that for $n=3$ we get something around $-5$ .","This was a question that I thought about as I was playing around with the cardinal sine function: Find with denoting functional composition. I don't really know how to approach this question, but intuition tells me that this approaches . Maybe it doesn't even converge. I have no idea. WA says that for we get something around .",\lim_{n\rightarrow\infty}\int_{-\infty}^\infty\frac{\sin^{\circ n}x}{x}dx {}^\circ \pi n=3 -5,"['calculus', 'integration', 'limits', 'definite-integrals', 'trigonometric-integrals']"
93,How far can one get in analysis without leaving $\mathbb{Q}$?,How far can one get in analysis without leaving ?,\mathbb{Q},"Suppose you're trying to teach analysis to a stubborn algebraist who refuses to acknowledge the existence of any characteristic $0$ field other than $\mathbb{Q}$.  How ugly are things going to get for him? The algebraist argues that the real numbers are a silly construction because any real number can be approximated to arbitrarily high precision by the rational numbers - i.e., given any real number $r$ and any $\epsilon>0$, the set $\left\{x\in\mathbb{Q}:\left|x-r\right|<\epsilon\right\}$ is nonempty, thus sating the mad gods of algebra. As @J.M. and @75064 pointed out to me in chat, we do start having some topology problems, for example that $f(x)=x^2$ and $g(x)=2$ are nonintersecting functions in $\mathbb{Q}$.  They do, however, come arbitrarily close to intersecting, i.e. given any $\epsilon>0$ there exist rational solutions to $\left|2-x^2\right|<\epsilon$.  The algebraist doesn't find this totally unsatisfying. Where is this guy really going to start running into trouble?  Are there definitions in analysis which simply can't be reasonably formulated without leaving the rational numbers?  Which concepts would be particularly difficult to understand without the rest of the reals?","Suppose you're trying to teach analysis to a stubborn algebraist who refuses to acknowledge the existence of any characteristic $0$ field other than $\mathbb{Q}$.  How ugly are things going to get for him? The algebraist argues that the real numbers are a silly construction because any real number can be approximated to arbitrarily high precision by the rational numbers - i.e., given any real number $r$ and any $\epsilon>0$, the set $\left\{x\in\mathbb{Q}:\left|x-r\right|<\epsilon\right\}$ is nonempty, thus sating the mad gods of algebra. As @J.M. and @75064 pointed out to me in chat, we do start having some topology problems, for example that $f(x)=x^2$ and $g(x)=2$ are nonintersecting functions in $\mathbb{Q}$.  They do, however, come arbitrarily close to intersecting, i.e. given any $\epsilon>0$ there exist rational solutions to $\left|2-x^2\right|<\epsilon$.  The algebraist doesn't find this totally unsatisfying. Where is this guy really going to start running into trouble?  Are there definitions in analysis which simply can't be reasonably formulated without leaving the rational numbers?  Which concepts would be particularly difficult to understand without the rest of the reals?",,"['analysis', 'number-theory', 'philosophy', 'constructive-mathematics', 'finitism']"
94,Is non-standard analysis worth learning?,Is non-standard analysis worth learning?,,"As a former physics major, I did a lot of (seemingly sloppy) calculus using the notion of infinitesimals. Recently I heard that there is a branch of math called non-standard analysis that provides some formalism to this type of calculus. So, do you guys think it is a subject worth learning? Is it a branch that is growing and becoming interconnected with other branches of math? Does it make calculus any easier?","As a former physics major, I did a lot of (seemingly sloppy) calculus using the notion of infinitesimals. Recently I heard that there is a branch of math called non-standard analysis that provides some formalism to this type of calculus. So, do you guys think it is a subject worth learning? Is it a branch that is growing and becoming interconnected with other branches of math? Does it make calculus any easier?",,"['analysis', 'soft-question']"
95,Continuity of the roots of a polynomial in terms of its coefficients,Continuity of the roots of a polynomial in terms of its coefficients,,"It's commonly stated that the roots of a polynomial are a continuous function of the coefficients.  How is this statement formalized?  I would assume it's by restricting to polynomials of a fixed degree n (maybe monic? seems like that shouldn't matter), and considering the collection of roots as a a point in $F^n/\sim$ where F is the field and $\sim$ is permutation of coordinates, but is there something I'm missing?  More to the point, where would I find a proof? At least, I've seen this stated for C (and hence R ); is this even true in general -- say, for an algebraically closed valued field (and hence complete non-Archimedean field because those extend uniquely)?  I've seen it implied that it's not always true in the non-Archimedean case; is this correct?  What's a counterexample?  (If this is wrong and it is true in this generality, is it true in any greater generality?)","It's commonly stated that the roots of a polynomial are a continuous function of the coefficients.  How is this statement formalized?  I would assume it's by restricting to polynomials of a fixed degree n (maybe monic? seems like that shouldn't matter), and considering the collection of roots as a a point in $F^n/\sim$ where F is the field and $\sim$ is permutation of coordinates, but is there something I'm missing?  More to the point, where would I find a proof? At least, I've seen this stated for C (and hence R ); is this even true in general -- say, for an algebraically closed valued field (and hence complete non-Archimedean field because those extend uniquely)?  I've seen it implied that it's not always true in the non-Archimedean case; is this correct?  What's a counterexample?  (If this is wrong and it is true in this generality, is it true in any greater generality?)",,"['analysis', 'polynomials', 'field-theory', 'roots']"
96,Pointwise vs. Uniform Convergence,Pointwise vs. Uniform Convergence,,"This is a pretty basic question. I just don't understand the definition of uniform convergence. Here are my given definitions for pointwise and uniform convergence: Pointwise convergence: Let $X$ be a set, and let $F$ be the real or complex numbers. Consider a sequence of functions $f_n$ where $f_n:X\to F$ is a bounded function for each $n\in \mathbb N$. $f:X\to F$ is the pointwise limit of $f_n$ if for every $x \in X$, $$\lim_{n\to \infty}f_n(x)=f(x).$$ Uniform convergence: Let $f_n$ be a sequence of functions in the set of all bounded functions from $X$ to $F$ where $F$ is the real or complex numbers. The sequence is said to converge uniformly to a bounded function $f:X \to F$ if, given $\epsilon>0$, there exists an $N\in \mathbb N$ s.t. $\sup\{|f_n(x)-f(x)| : x \in X \}<\epsilon$ for $n\ge N$ I'm sorry I don't have a more specific question. I just don't see the exact relation/difference between the two definitions. I've asked two different professors to explain this to me but neither of their explanations helped. Edit: Attempting to show that uniform convergence implies pointwise convergence if $f_n$ converges uniformly to f, then $\sup\{|f_n(x)-f(x)| : x\in X \}$ for $n\ge N$. Thus, $|f_n(x)-f(x)|<\epsilon$ for $n\ge N$, which is the definition of pointwise convergence.","This is a pretty basic question. I just don't understand the definition of uniform convergence. Here are my given definitions for pointwise and uniform convergence: Pointwise convergence: Let $X$ be a set, and let $F$ be the real or complex numbers. Consider a sequence of functions $f_n$ where $f_n:X\to F$ is a bounded function for each $n\in \mathbb N$. $f:X\to F$ is the pointwise limit of $f_n$ if for every $x \in X$, $$\lim_{n\to \infty}f_n(x)=f(x).$$ Uniform convergence: Let $f_n$ be a sequence of functions in the set of all bounded functions from $X$ to $F$ where $F$ is the real or complex numbers. The sequence is said to converge uniformly to a bounded function $f:X \to F$ if, given $\epsilon>0$, there exists an $N\in \mathbb N$ s.t. $\sup\{|f_n(x)-f(x)| : x \in X \}<\epsilon$ for $n\ge N$ I'm sorry I don't have a more specific question. I just don't see the exact relation/difference between the two definitions. I've asked two different professors to explain this to me but neither of their explanations helped. Edit: Attempting to show that uniform convergence implies pointwise convergence if $f_n$ converges uniformly to f, then $\sup\{|f_n(x)-f(x)| : x\in X \}$ for $n\ge N$. Thus, $|f_n(x)-f(x)|<\epsilon$ for $n\ge N$, which is the definition of pointwise convergence.",,"['analysis', 'uniform-convergence']"
97,Factorial and exponential dual identities,Factorial and exponential dual identities,,There are two identities that have a seemingly dual correspondence: $$e^x = \sum_{n\ge0} {x^n\over n!}$$ and $$n! = \int_0^{\infty} {x^n\over e^x}\ dx.$$ Is there anything to this comparison? (I vaguely remember a generating function/integration correspondence) Are there similar sum/integration pairs for other well-known (or not-so-well-known) functions?,There are two identities that have a seemingly dual correspondence: $$e^x = \sum_{n\ge0} {x^n\over n!}$$ and $$n! = \int_0^{\infty} {x^n\over e^x}\ dx.$$ Is there anything to this comparison? (I vaguely remember a generating function/integration correspondence) Are there similar sum/integration pairs for other well-known (or not-so-well-known) functions?,,"['analysis', 'power-series', 'generating-functions']"
98,Has Prof. Otelbaev shown existence of strong solutions for Navier-Stokes equations? [closed],Has Prof. Otelbaev shown existence of strong solutions for Navier-Stokes equations? [closed],,"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 10 years ago . Improve this question Moderator Notice : I am unilaterally closing this question for three reasons. The discussion here has turned too chatty and not suitable for the MSE framework. Given the recent pre-print of T. Tao (see also the blog-post here ), the continued usefulness of this question is diminished. The final update on this answer is probably as close to an ""answer"" an we can expect. Eminent Kazakh mathematician Mukhtarbay Otelbaev, Prof. Dr. has published a full proof of the Clay Navier-Stokes Millennium Problem. Is it correct? See http://bnews.kz/en/news/post/180213/ A link to the paper (in Russian): http://www.math.kz/images/journal/2013-4/Otelbaev_N-S_21_12_2013.pdf Mukhtarbay Otelbaev has published over 200 papers, had over 70 PhD students, and he is a member of the Kazak Academy of Sciences. He has published papers on Navier-Stokes and Functional Analysis. please confine answers to any actual mathematical error found! thanks","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 10 years ago . Improve this question Moderator Notice : I am unilaterally closing this question for three reasons. The discussion here has turned too chatty and not suitable for the MSE framework. Given the recent pre-print of T. Tao (see also the blog-post here ), the continued usefulness of this question is diminished. The final update on this answer is probably as close to an ""answer"" an we can expect. Eminent Kazakh mathematician Mukhtarbay Otelbaev, Prof. Dr. has published a full proof of the Clay Navier-Stokes Millennium Problem. Is it correct? See http://bnews.kz/en/news/post/180213/ A link to the paper (in Russian): http://www.math.kz/images/journal/2013-4/Otelbaev_N-S_21_12_2013.pdf Mukhtarbay Otelbaev has published over 200 papers, had over 70 PhD students, and he is a member of the Kazak Academy of Sciences. He has published papers on Navier-Stokes and Functional Analysis. please confine answers to any actual mathematical error found! thanks",,"['analysis', 'partial-differential-equations']"
99,"A continuous, nowhere differentiable but invertible function?","A continuous, nowhere differentiable but invertible function?",,"I am aware of a few example of continuous, nowhere differentiable functions. The most famous is perhaps the Weierstrass functions $$W(t)=\sum_k^{\infty} a^k\cos\left(b^k t\right)$$ but there are other examples, like the van der Waerden functions, or the Faber functions. Most of these ""look like"" some variation of: (Weierstrass functions from Wolfram) Specifically, they are clearly not invertible. Since these functions are generally self-similar at many scales, this non-invertability would seem to hold essentially everywhere. I'm wondering if it's possible to construct such a function which is invertible . Intuitively, maybe this would be ""jittery"" in the same way as the Weierstrass function, but if it had a slope which always increased, it would be invertible. Or perhaps there is at least an example in which the function is invertible over some segment of the range.","I am aware of a few example of continuous, nowhere differentiable functions. The most famous is perhaps the Weierstrass functions $$W(t)=\sum_k^{\infty} a^k\cos\left(b^k t\right)$$ but there are other examples, like the van der Waerden functions, or the Faber functions. Most of these ""look like"" some variation of: (Weierstrass functions from Wolfram) Specifically, they are clearly not invertible. Since these functions are generally self-similar at many scales, this non-invertability would seem to hold essentially everywhere. I'm wondering if it's possible to construct such a function which is invertible . Intuitively, maybe this would be ""jittery"" in the same way as the Weierstrass function, but if it had a slope which always increased, it would be invertible. Or perhaps there is at least an example in which the function is invertible over some segment of the range.",,"['analysis', 'continuity', 'differential-topology']"
