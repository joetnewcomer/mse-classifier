,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,To show $\mathrm{ker} f=\{0\}$ for linear mapping $f$.,To show  for linear mapping .,\mathrm{ker} f=\{0\} f,"Let $V$ be a vector space over $F$ with basis $\{e_1,e_2,...e_n\}$. Let $F$ be a linear mapping from $V$ to $V$ such that $F(e_1) =e_2,...F(e_n)=e_1$. Show that  $\mathrm{ker} f=\{0\}$. Also find $f^{-1}$. I just know that $\mathrm{ker} f=\{0\}$ iff $f$ is 1-1. So is it enough to show that basic definition for 1-1? The inverse mapping will be defined as $f^{-1}(e_1) =e_n, f^{-1}(e_2)=e_1,...$ Am I right?","Let $V$ be a vector space over $F$ with basis $\{e_1,e_2,...e_n\}$. Let $F$ be a linear mapping from $V$ to $V$ such that $F(e_1) =e_2,...F(e_n)=e_1$. Show that  $\mathrm{ker} f=\{0\}$. Also find $f^{-1}$. I just know that $\mathrm{ker} f=\{0\}$ iff $f$ is 1-1. So is it enough to show that basic definition for 1-1? The inverse mapping will be defined as $f^{-1}(e_1) =e_n, f^{-1}(e_2)=e_1,...$ Am I right?",,['linear-algebra']
1,"Show that $(1,1,1)$, $(a,b,c)$, $(a^2, b^2, c^2)$ are linearly indepdenent for distinct $a,b,c$","Show that , ,  are linearly indepdenent for distinct","(1,1,1) (a,b,c) (a^2, b^2, c^2) a,b,c","Show that $(1,1,1)$, $(a,b,c)$, $(a^2, b^2, c^2)$ are linearly indepdenent, where $a,b,$ and $c$ are distinct real numbers. I will show my attempt and then state where I get stuck. Suppose $c_1(1,1,1) + c_2(a,b,c) + c_3(a^2,b^2,c^2) = 0$ This leads to the three equations: $c_1+c_2a+c_3a^2 = c_1+c_2b+c_3b^2 = c_1+c_2c+c_3c^2 = 0$. Now I am not sure how to sure from here that each $c_i$ must be 0. Hints appreciated.","Show that $(1,1,1)$, $(a,b,c)$, $(a^2, b^2, c^2)$ are linearly indepdenent, where $a,b,$ and $c$ are distinct real numbers. I will show my attempt and then state where I get stuck. Suppose $c_1(1,1,1) + c_2(a,b,c) + c_3(a^2,b^2,c^2) = 0$ This leads to the three equations: $c_1+c_2a+c_3a^2 = c_1+c_2b+c_3b^2 = c_1+c_2c+c_3c^2 = 0$. Now I am not sure how to sure from here that each $c_i$ must be 0. Hints appreciated.",,['linear-algebra']
2,Solve an overdetermined system of linear equations,Solve an overdetermined system of linear equations,,I have doubt to solve this system of equations \begin{cases} x+y=r_1\\ x+z=c_1\\ x+w=d_1\\ y+z=d_2\\ y+w=c_2\\ z+w=r_2 \end{cases} Is it an overdetermined system because I see there are more equations than unknowns. Can we just solve this system in a simple way?,I have doubt to solve this system of equations \begin{cases} x+y=r_1\\ x+z=c_1\\ x+w=d_1\\ y+z=d_2\\ y+w=c_2\\ z+w=r_2 \end{cases} Is it an overdetermined system because I see there are more equations than unknowns. Can we just solve this system in a simple way?,,"['linear-algebra', 'systems-of-equations', 'least-squares']"
3,Prove $x^TAx = 0$ $\implies$ A is skew-symmetric,Prove   A is skew-symmetric,x^TAx = 0 \implies,"We know for a skew-symmetric matrix A, $x^TAx = 0$. But is the converse statement true, i.e. does $x^TAx = 0$ imply A is skew-symmetric? If yes, then how to prove it?","We know for a skew-symmetric matrix A, $x^TAx = 0$. But is the converse statement true, i.e. does $x^TAx = 0$ imply A is skew-symmetric? If yes, then how to prove it?",,"['linear-algebra', 'matrices', 'transpose']"
4,Show that $2^{105} + 3^{105}$ is divisible by $7$,Show that  is divisible by,2^{105} + 3^{105} 7,"I know that $$\frac{(ak \pm 1)^n}{a}$$ gives remainder $a - 1$ is n is odd or $1$ is n is even. So, I wrote $ 2^{105} + 3^{105}$ as $8^{35} + 27^{35}$ and then as $(7\cdot 1+1)^{35} + (7\cdot 4-1)^{35}$ , which on division should give remainder of $6$ for each term and total remainder of 5 (12/7). But, as per question, it should be divisible by 7, so remainder should be zero not 5. Where did I go wrong? [note: i don't know binomial theorem or number theory.]","I know that gives remainder is n is odd or is n is even. So, I wrote as and then as , which on division should give remainder of for each term and total remainder of 5 (12/7). But, as per question, it should be divisible by 7, so remainder should be zero not 5. Where did I go wrong? [note: i don't know binomial theorem or number theory.]",\frac{(ak \pm 1)^n}{a} a - 1 1  2^{105} + 3^{105} 8^{35} + 27^{35} (7\cdot 1+1)^{35} + (7\cdot 4-1)^{35} 6,"['linear-algebra', 'arithmetic']"
5,Decompose this matrix as a sum of unit and nilpotent matrix.,Decompose this matrix as a sum of unit and nilpotent matrix.,,"Show that the matrix $A=\begin{bmatrix}          1 & 0  \\         2 & 1  \\         \end{bmatrix}$ can be decomposed as a sum of a unit and nilpotent matrix. Hence evaluate the matrix $A^{2007}$. I read about nilpotent matrices the other day. It said ""A square matrix such that is the zero matrix for some positive integer matrix power"" is nilpotent. But how to use this in this problem is beyond me.","Show that the matrix $A=\begin{bmatrix}          1 & 0  \\         2 & 1  \\         \end{bmatrix}$ can be decomposed as a sum of a unit and nilpotent matrix. Hence evaluate the matrix $A^{2007}$. I read about nilpotent matrices the other day. It said ""A square matrix such that is the zero matrix for some positive integer matrix power"" is nilpotent. But how to use this in this problem is beyond me.",,"['linear-algebra', 'matrices', 'determinant', 'nilpotence']"
6,Vectors spanning a plane,Vectors spanning a plane,,"I am having a problem with this question, I just can't seem to get it. Consider the plane $\mathbb{R}^3$ defined by the equation $x+2y-z=0$ Find any two vectors $\mathbf{v},\mathbf{w}$ such that their span may be identified with this plane.","I am having a problem with this question, I just can't seem to get it. Consider the plane $\mathbb{R}^3$ defined by the equation $x+2y-z=0$ Find any two vectors $\mathbf{v},\mathbf{w}$ such that their span may be identified with this plane.",,['linear-algebra']
7,Proving $(A+B)^2=A^2+2AB+B^2$ wrong using examples,Proving  wrong using examples,(A+B)^2=A^2+2AB+B^2,Give a counter example to show the following statements are false? $$(A+B)^2=A^2+2AB+B^2.$$ Would it be possible to use the $2\times 2$ matrices to show the statement the above?,Give a counter example to show the following statements are false? $$(A+B)^2=A^2+2AB+B^2.$$ Would it be possible to use the $2\times 2$ matrices to show the statement the above?,,['linear-algebra']
8,"The row rank of an $m\times n$ matrix $A$ is at most $\min\{m,n\}$. Why?",The row rank of an  matrix  is at most . Why?,"m\times n A \min\{m,n\}","Ok, so let $A$ be an $m\times n$ matrix. I understand by intuition that the row rank has to be $\le m$, but why also $n$? Is this because there can be no more leading ones than $m$ or $n$?","Ok, so let $A$ be an $m\times n$ matrix. I understand by intuition that the row rank has to be $\le m$, but why also $n$? Is this because there can be no more leading ones than $m$ or $n$?",,"['linear-algebra', 'matrix-rank']"
9,How to find a matrix $X$ such that $X+X^2+X^3 = \begin{bmatrix} 1&2005\\ 2006&1 \end{bmatrix}$?,How to find a matrix  such that ?,X X+X^2+X^3 = \begin{bmatrix} 1&2005\\ 2006&1 \end{bmatrix},"Find a matrix $X \in M_{2}(\mathbb Z)$ such that $$X+X^2+X^3=\begin{bmatrix} 1&2005\\ 2006&1\end{bmatrix}$$ My try: Let  $$X=\begin{bmatrix} a&b\\ c&d \end{bmatrix}$$ where $a,b,c,d\in Z$ then $$X^2=\begin{bmatrix} a^2+bc&ab+bd\\ ac+cd&bc+d^2 \end{bmatrix}$$ then $$X^3=\begin{bmatrix} a^3+abc+abc+bcd&a^2b+b^2c+abd+bd^2\\ a^2c+acd+bc^2+cd^2&abc+bcd+bcd+d^3 \end{bmatrix}$$ so $$X+X^2+X^3=\begin{bmatrix} a^3+2abc+bcd+a^2+bc+a&a^2b+b^2c+abd+bd^2+ab+bd+b\\ a^2c+acd+bc^2+cd^2+ac+cd+c&abc+2bcd+d^3+bc+d^2+d \end{bmatrix}$$ then we have $$\begin{cases} a^3+2abc+bcd+a^2+bc+a=1\\ a^2b+b^2c+abd+bd^2+ab+bd+b=2005\\ a^2c+acd+bc^2+cd^2+ac+cd+c=2006\\ abc+2bcd+d^3+bc+d^2+d=1 \end{cases}$$ Note $2005=5\cdot 401 $ is prime number,so $$b(a^2+bc+ad+d^2+a+d+1)=2005$$ we can $b=\pm 1$ ,or $b=\pm 2005$ ,or $b=\pm 5$ or $b=\pm 401$ and note $2006=2\times 1003$ then $c=\pm 2$ ,or $c=\pm 1003$or $c=\pm 1$,or $c=\pm 2006$ so following is very ugly.  But I can't.Thank you ,maybe this problem have other nice methods,Thank you","Find a matrix $X \in M_{2}(\mathbb Z)$ such that $$X+X^2+X^3=\begin{bmatrix} 1&2005\\ 2006&1\end{bmatrix}$$ My try: Let  $$X=\begin{bmatrix} a&b\\ c&d \end{bmatrix}$$ where $a,b,c,d\in Z$ then $$X^2=\begin{bmatrix} a^2+bc&ab+bd\\ ac+cd&bc+d^2 \end{bmatrix}$$ then $$X^3=\begin{bmatrix} a^3+abc+abc+bcd&a^2b+b^2c+abd+bd^2\\ a^2c+acd+bc^2+cd^2&abc+bcd+bcd+d^3 \end{bmatrix}$$ so $$X+X^2+X^3=\begin{bmatrix} a^3+2abc+bcd+a^2+bc+a&a^2b+b^2c+abd+bd^2+ab+bd+b\\ a^2c+acd+bc^2+cd^2+ac+cd+c&abc+2bcd+d^3+bc+d^2+d \end{bmatrix}$$ then we have $$\begin{cases} a^3+2abc+bcd+a^2+bc+a=1\\ a^2b+b^2c+abd+bd^2+ab+bd+b=2005\\ a^2c+acd+bc^2+cd^2+ac+cd+c=2006\\ abc+2bcd+d^3+bc+d^2+d=1 \end{cases}$$ Note $2005=5\cdot 401 $ is prime number,so $$b(a^2+bc+ad+d^2+a+d+1)=2005$$ we can $b=\pm 1$ ,or $b=\pm 2005$ ,or $b=\pm 5$ or $b=\pm 401$ and note $2006=2\times 1003$ then $c=\pm 2$ ,or $c=\pm 1003$or $c=\pm 1$,or $c=\pm 2006$ so following is very ugly.  But I can't.Thank you ,maybe this problem have other nice methods,Thank you",,['linear-algebra']
10,"If $\langle Ta, a\rangle \in \mathbb{R}$ for all $a$ then $T$ is self-adjoint",If  for all  then  is self-adjoint,"\langle Ta, a\rangle \in \mathbb{R} a T","I have having trouble with the following question: Let $V$ be a finite-dimensional complex inner product space, and let $T$ be a linear operator on $V$. Prove that if $\langle T\alpha, \alpha\rangle$ is real for every $\alpha$ in $V$ then $T$ is self-adjoint. I have been focusing on trying to show that if $\langle T\alpha, \alpha\rangle = \langle T^*\alpha, \alpha\rangle$ for all $\alpha$ then  $T = T^*$, but have been unable to do so. I also feel like I'm missing something really obvious. How should I proceed.","I have having trouble with the following question: Let $V$ be a finite-dimensional complex inner product space, and let $T$ be a linear operator on $V$. Prove that if $\langle T\alpha, \alpha\rangle$ is real for every $\alpha$ in $V$ then $T$ is self-adjoint. I have been focusing on trying to show that if $\langle T\alpha, \alpha\rangle = \langle T^*\alpha, \alpha\rangle$ for all $\alpha$ then  $T = T^*$, but have been unable to do so. I also feel like I'm missing something really obvious. How should I proceed.",,['linear-algebra']
11,Geometric interpretation of linear transformation,Geometric interpretation of linear transformation,,"I have a linear transformation, given by the following matrix $$ \begin{pmatrix} x_1\\ x_2 \end{pmatrix} \mapsto \begin{pmatrix} 2 & 2\\ -1 & -1\\ \end{pmatrix} \begin{pmatrix} x_1\\ x_2 \end{pmatrix} $$ How can I determine what this corresponds to geometrically, when I apply it to the $x_1,x_2$-plane. I have tried to visualise the transformation by hand, and using a fieldplot in Maple, to get an idea of what is happening. My idea was then to decompose it into scaling, rotation, reflection or some other simple transformations. My question is: Which geometric transformation does the above linear map correspond to, and in general, what is a good strategy for solving this kind of problem?","I have a linear transformation, given by the following matrix $$ \begin{pmatrix} x_1\\ x_2 \end{pmatrix} \mapsto \begin{pmatrix} 2 & 2\\ -1 & -1\\ \end{pmatrix} \begin{pmatrix} x_1\\ x_2 \end{pmatrix} $$ How can I determine what this corresponds to geometrically, when I apply it to the $x_1,x_2$-plane. I have tried to visualise the transformation by hand, and using a fieldplot in Maple, to get an idea of what is happening. My idea was then to decompose it into scaling, rotation, reflection or some other simple transformations. My question is: Which geometric transformation does the above linear map correspond to, and in general, what is a good strategy for solving this kind of problem?",,"['linear-algebra', 'geometry', 'transformation']"
12,Irreducible minimal polynomial implies every invariant subspace has an invariant complement,Irreducible minimal polynomial implies every invariant subspace has an invariant complement,,"Full version of the problem is following: Let T be a linear transformation on a finite dimensional vector space $V$ over a field $\mathbb{F}$. If the minimal polynomial $p_t$ of T is irreducible, then every T invariant subspace $W$ has a T-invariant complement $W'$ I used Cyclic decomposition Theorem which states that ""The finite dimensional vector space V can be expressed a s a decomposition of T-cyclic subspaces $Z(\alpha_1;T)\oplus Z(\alpha_2;T)\oplus...\oplus Z(\alpha_k;T)$ and their annihilators $p_1,...p_k$ have properties;  (1) $p_k|...|p_1$, (2)$p_T=p_1, f_T=p_1\cdot p_2\cdot ...\cdot p_k$ where $f_T$ is the characteristic polynomial of T."" Can I say this? Since $p_T$ is irreducible, there is a cyclic vector $\alpha$ such that $V=Z(\alpha;T)$ and V=W and $W'=\{0\}$.  Therefore $W'$ for each W is T-invariant. Is my way correct? Thank you in advance.","Full version of the problem is following: Let T be a linear transformation on a finite dimensional vector space $V$ over a field $\mathbb{F}$. If the minimal polynomial $p_t$ of T is irreducible, then every T invariant subspace $W$ has a T-invariant complement $W'$ I used Cyclic decomposition Theorem which states that ""The finite dimensional vector space V can be expressed a s a decomposition of T-cyclic subspaces $Z(\alpha_1;T)\oplus Z(\alpha_2;T)\oplus...\oplus Z(\alpha_k;T)$ and their annihilators $p_1,...p_k$ have properties;  (1) $p_k|...|p_1$, (2)$p_T=p_1, f_T=p_1\cdot p_2\cdot ...\cdot p_k$ where $f_T$ is the characteristic polynomial of T."" Can I say this? Since $p_T$ is irreducible, there is a cyclic vector $\alpha$ such that $V=Z(\alpha;T)$ and V=W and $W'=\{0\}$.  Therefore $W'$ for each W is T-invariant. Is my way correct? Thank you in advance.",,['linear-algebra']
13,The contributions of James Sylvester to linear algebra.,The contributions of James Sylvester to linear algebra.,,"The claim is James Sylvester and Arthur Cayley are the fathers of Linear Algebra. I can find the various parts that Cayley contributed to Linear Algebra, but there is not much on the contributions made by Sylvester. Does anyone have this information and direct me to resources on this? I only want his contribution to LA.","The claim is James Sylvester and Arthur Cayley are the fathers of Linear Algebra. I can find the various parts that Cayley contributed to Linear Algebra, but there is not much on the contributions made by Sylvester. Does anyone have this information and direct me to resources on this? I only want his contribution to LA.",,"['linear-algebra', 'math-history']"
14,"Demonstration: If all vectors of $V$ are eigenvectors of $T$, then there is one $\lambda$ such that $T(v) = \lambda v$ for all $v \in V$.","Demonstration: If all vectors of  are eigenvectors of , then there is one  such that  for all .",V T \lambda T(v) = \lambda v v \in V,"Let $T: V \rightarrow V$ be a linear operator. I need to demonstrate that if all nonzero vectors of $V$ are eigenvectors of $T$, then there is one specific $\lambda \in K$ such that $T(v) = \lambda v$, for all $v \in V$. I understand that, if all nonzero vectors of $V$ are eigenvectors of $T$, then $T$ must be a scaling transformation. It just stretch or shrinks vectors, but doesn't change their  directions. So, the statement says that if it happens, then, there is a single $\lambda$ such that $T(v) = \lambda v$. In other words, if there is such transformation, then it scales all vectors by the same scalar $\lambda$. Applying the transformation to our standard basis vectors, we have: $$ T(e_1) = \lambda_1 e_1 \\ T(e_2) = \lambda_2 e_2 \\ \vdots \\ T(e_n) = \lambda_n e_n $$ I understand I need to prove that $\lambda_1 = \lambda_2 = \dots = \lambda_n$, but I can't see how! EDIT $$ v = c_1e_1 + c_2e_2 + \dots + c_ne_n \\ T(v) = \mu v = \lambda_1c_1e_1 + \lambda_2c_2e_2 + \dots + \lambda_nc_ne_n \\ $$ Since what's multiplying $v$ coordinates is $\lambda_i$, then all of them must be $\mu$. I'm not sure how to 'mathematize' this. Is this idea correct? EDIT 2 Extending the left hand side of EDIT 1 , we have: $$ \mu v = \lambda_1c_1e_1 + \dots + \lambda_nc_ne_n \\ \mu(c_1e_1 + \dots + c_ne_n) = \lambda_1c_1e_1 + \dots + \lambda_nc_ne_n \\ \mu c_1e_1 + \dots + \mu c_ne_n = \lambda_1c_1e_1 + \dots + \lambda_nc_ne_n \\ $$ And since $e_i$ are linearly independent, $\mu = \lambda_1 = \lambda_2 = \dots = \lambda_n$. Is this proof correct?","Let $T: V \rightarrow V$ be a linear operator. I need to demonstrate that if all nonzero vectors of $V$ are eigenvectors of $T$, then there is one specific $\lambda \in K$ such that $T(v) = \lambda v$, for all $v \in V$. I understand that, if all nonzero vectors of $V$ are eigenvectors of $T$, then $T$ must be a scaling transformation. It just stretch or shrinks vectors, but doesn't change their  directions. So, the statement says that if it happens, then, there is a single $\lambda$ such that $T(v) = \lambda v$. In other words, if there is such transformation, then it scales all vectors by the same scalar $\lambda$. Applying the transformation to our standard basis vectors, we have: $$ T(e_1) = \lambda_1 e_1 \\ T(e_2) = \lambda_2 e_2 \\ \vdots \\ T(e_n) = \lambda_n e_n $$ I understand I need to prove that $\lambda_1 = \lambda_2 = \dots = \lambda_n$, but I can't see how! EDIT $$ v = c_1e_1 + c_2e_2 + \dots + c_ne_n \\ T(v) = \mu v = \lambda_1c_1e_1 + \lambda_2c_2e_2 + \dots + \lambda_nc_ne_n \\ $$ Since what's multiplying $v$ coordinates is $\lambda_i$, then all of them must be $\mu$. I'm not sure how to 'mathematize' this. Is this idea correct? EDIT 2 Extending the left hand side of EDIT 1 , we have: $$ \mu v = \lambda_1c_1e_1 + \dots + \lambda_nc_ne_n \\ \mu(c_1e_1 + \dots + c_ne_n) = \lambda_1c_1e_1 + \dots + \lambda_nc_ne_n \\ \mu c_1e_1 + \dots + \mu c_ne_n = \lambda_1c_1e_1 + \dots + \lambda_nc_ne_n \\ $$ And since $e_i$ are linearly independent, $\mu = \lambda_1 = \lambda_2 = \dots = \lambda_n$. Is this proof correct?",,['linear-algebra']
15,Can QR decomposition be used for matrix inversion?,Can QR decomposition be used for matrix inversion?,,Is there any simple algorithm for matrix inversion (that can be implemented using C/C++)? Can QR decomposition be used for matrix inversion? How?,Is there any simple algorithm for matrix inversion (that can be implemented using C/C++)? Can QR decomposition be used for matrix inversion? How?,,"['linear-algebra', 'matrices', 'inverse', 'numerical-linear-algebra', 'matrix-decomposition']"
16,Is every subspace of a infinite-dimensional vector space an intersection of hyperspaces?,Is every subspace of a infinite-dimensional vector space an intersection of hyperspaces?,,"The corollary below is from Hoffman and Kunze's book, Linear Algebra . Corollary. If $W$ is a $k$-dimensional subspace of an $n$-dimensional vector space $V$, then $W$ is the intersection of   $(n-k)$ hyperspaces in $V$. In the proof, they find $n-k$ linear functionals $f_{i}$ such that $W=\cap_{i=1}^{n-k}\ker (f_{i})$. I want to know if the following is true: For all proper subspace $W$ of a infinite-dimensional vector space $V$ there are a set of linear functionals $\{f_{i}|i\in I\}$, where $I$ is a set of index such that $W=\cap_{i\in I}\ker (f_{i})$. Thanks for your kindly help.","The corollary below is from Hoffman and Kunze's book, Linear Algebra . Corollary. If $W$ is a $k$-dimensional subspace of an $n$-dimensional vector space $V$, then $W$ is the intersection of   $(n-k)$ hyperspaces in $V$. In the proof, they find $n-k$ linear functionals $f_{i}$ such that $W=\cap_{i=1}^{n-k}\ker (f_{i})$. I want to know if the following is true: For all proper subspace $W$ of a infinite-dimensional vector space $V$ there are a set of linear functionals $\{f_{i}|i\in I\}$, where $I$ is a set of index such that $W=\cap_{i\in I}\ker (f_{i})$. Thanks for your kindly help.",,['linear-algebra']
17,Determinant of a special $0$-$1$ matrix,Determinant of a special - matrix,0 1,I have a matrix which is of odd order and has exactly two ones in each row and column. The rest of the entries in each row/column are all zero. What will be the determinant of this matrix? I believe the answer is $\pm 2$. My question is as to how is this derived? Any help will be appreciated. Thanks.,I have a matrix which is of odd order and has exactly two ones in each row and column. The rest of the entries in each row/column are all zero. What will be the determinant of this matrix? I believe the answer is $\pm 2$. My question is as to how is this derived? Any help will be appreciated. Thanks.,,['linear-algebra']
18,Is there an infinite dimensional inner product space without an orthogonal Hamel basis?,Is there an infinite dimensional inner product space without an orthogonal Hamel basis?,,"I want to know if there exists an (real or complex) vector space $X$ with infinite dimension and an inner product $\langle\cdot,\cdot\rangle$ such that there is no orthogonal Hamel (algebraic) basis of $X$ . I do not seek conditions on to the topological aspects of completeness nor an example Schauder Basis or a Maximal Orthogonal System in Hilbert Spaces. I am aware that this is not usually discussed in the scenario of infinite dimensional vector spaces, but this is exactly what I'm curious about. I know how to prove that every inner product space of Hamel dimension $\aleph_0$ has an orthonormal Hamel basis using the Gram-Schimidt process but this proof does not work for the uncountable case. I also know that a maximal orthogonal set of non-zero vectors is not always a Hamel basis, thus a simple application of Zorn's Lemma does not seem to solve the problem. Finally, I have seen many other similar questions here but none of them provide an answer for the question posed here.","I want to know if there exists an (real or complex) vector space with infinite dimension and an inner product such that there is no orthogonal Hamel (algebraic) basis of . I do not seek conditions on to the topological aspects of completeness nor an example Schauder Basis or a Maximal Orthogonal System in Hilbert Spaces. I am aware that this is not usually discussed in the scenario of infinite dimensional vector spaces, but this is exactly what I'm curious about. I know how to prove that every inner product space of Hamel dimension has an orthonormal Hamel basis using the Gram-Schimidt process but this proof does not work for the uncountable case. I also know that a maximal orthogonal set of non-zero vectors is not always a Hamel basis, thus a simple application of Zorn's Lemma does not seem to solve the problem. Finally, I have seen many other similar questions here but none of them provide an answer for the question posed here.","X \langle\cdot,\cdot\rangle X \aleph_0","['linear-algebra', 'inner-products', 'hamel-basis']"
19,"Prove that with some conditions,$\sum\limits_{i=1}^mA_iBA_i^T=B$ iff $A_iB=BA_i(1\leq i\leq m)$.","Prove that with some conditions, iff .",\sum\limits_{i=1}^mA_iBA_i^T=B A_iB=BA_i(1\leq i\leq m),"$A_i\in\mathbb{R}^{n\times n}(1\leq i\leq m)$ , $\sum\limits_{i=1}^mA_iA_i^T=I$ , $B\in\mathbb{R}^{n\times n}$ and $\text{tr}(B)=1$ , $B$ is positive. Prove that $\sum\limits_{i=1}^mA_iBA_i^T=B$ iff $A_iB=BA_i(1\leq i\leq m)$ . One direction is simple:if $A_iB=BA_i(1\leq i\leq m)$ ,then $\sum\limits_{i=1}^mA_iBA_i^T=\sum\limits_{i=1}^mBA_iA_i^T=B$ ,but what about the other direction? Update:Thank @mechanodroid for answer and I realize that the condition: $\sum\limits_{i=1}^mA_i^TA_i\preceq I$ should be added after reviewing the background of this problem.The form $\sum A_iBA_i^T$ is called the Kraus representation and the property $\sum\limits_{i=1}^mA_iBA_i^T=B$ is called ""unital"".And for Kraus representation there is a default rule $\sum\limits_{i=1}^mA_i^TA_i\preceq I$ (which is called trace-preserving) as mentioned by @mechanodroid, but I omitted that,my fault.",", , and , is positive. Prove that iff . One direction is simple:if ,then ,but what about the other direction? Update:Thank @mechanodroid for answer and I realize that the condition: should be added after reviewing the background of this problem.The form is called the Kraus representation and the property is called ""unital"".And for Kraus representation there is a default rule (which is called trace-preserving) as mentioned by @mechanodroid, but I omitted that,my fault.",A_i\in\mathbb{R}^{n\times n}(1\leq i\leq m) \sum\limits_{i=1}^mA_iA_i^T=I B\in\mathbb{R}^{n\times n} \text{tr}(B)=1 B \sum\limits_{i=1}^mA_iBA_i^T=B A_iB=BA_i(1\leq i\leq m) A_iB=BA_i(1\leq i\leq m) \sum\limits_{i=1}^mA_iBA_i^T=\sum\limits_{i=1}^mBA_iA_i^T=B \sum\limits_{i=1}^mA_i^TA_i\preceq I \sum A_iBA_i^T \sum\limits_{i=1}^mA_iBA_i^T=B \sum\limits_{i=1}^mA_i^TA_i\preceq I,"['linear-algebra', 'matrices']"
20,Difference between Modules and Vector Spaces,Difference between Modules and Vector Spaces,,"I have a curiosity question on a fundamental difference between vector spaces and general modules over rings. Some of the fundamental facts of linear algebra: (1) A finitely generated vector space has a basis. (2) Minimal generating (spanning) sets of a vector space are linearly independent and therefore form a basis. I recently took a course on modules. One basic example discussed: Let $R = K[x,y]$ , where $K$ is a field, and let $I = \langle x,y \rangle $ . We consider $I$ as a module over $R$ . $I$ is a finitely generated module, however it is not free (does not contain a basis). This is because the smallest generating set has size $2$ , and no matter what generating set you choose, you can write a non-trivial $R-$ linear combination of the elements of that set that equals $0$ . If $S = \{ f(x,y), g(x,y) \} $ so that $I = \langle S \rangle $ , then $g(x,y)f(x,y) + (-f(x,y))g(x,y) = 0 $ . A similar argument can be made for any finite generating set for $I$ . This example shows that those fundamental facts of vector spaces are not necessarily true for modules over general rings. My question is what is it about the scalars coming from a field that makes these facts true but not so when the scalars come from a general ring? I never got a chance to ask my professor during the class. I tried reading proofs from linear algebra texts but I cannot see where the underlying scalar FIELD makes the difference. Any clarification would be very helpful.","I have a curiosity question on a fundamental difference between vector spaces and general modules over rings. Some of the fundamental facts of linear algebra: (1) A finitely generated vector space has a basis. (2) Minimal generating (spanning) sets of a vector space are linearly independent and therefore form a basis. I recently took a course on modules. One basic example discussed: Let , where is a field, and let . We consider as a module over . is a finitely generated module, however it is not free (does not contain a basis). This is because the smallest generating set has size , and no matter what generating set you choose, you can write a non-trivial linear combination of the elements of that set that equals . If so that , then . A similar argument can be made for any finite generating set for . This example shows that those fundamental facts of vector spaces are not necessarily true for modules over general rings. My question is what is it about the scalars coming from a field that makes these facts true but not so when the scalars come from a general ring? I never got a chance to ask my professor during the class. I tried reading proofs from linear algebra texts but I cannot see where the underlying scalar FIELD makes the difference. Any clarification would be very helpful.","R = K[x,y] K I = \langle x,y \rangle  I R I 2 R- 0 S = \{ f(x,y), g(x,y) \}  I = \langle S \rangle  g(x,y)f(x,y) + (-f(x,y))g(x,y) = 0  I","['linear-algebra', 'modules', 'finitely-generated', 'free-modules']"
21,How to prove this equality of the determinant of matrix?,How to prove this equality of the determinant of matrix?,,Prove that \begin{equation*} \det\begin{pmatrix} a^2 & b^2 & c^2 \\ ab & bc & ca \\ b^2 & c^2 & a^2 \end{pmatrix} =(a^2-bc)(b^2-ca)(c^2-ab)\end{equation*} My attempt: \begin{equation*} \det\begin{pmatrix} a^2 & b^2 & c^2 \\ ab & bc & ca \\ b^2 & c^2 & a^2 \end{pmatrix} =\det\begin{pmatrix} a^2 & b^2 & c^2 \\ a(b-a) & b(c-b) & c(a-c) \\ (b+a)(b-a) & (c+b)(c-b) & (a+c)(a-c) \end{pmatrix} \end{equation*} But I think my direction is incorrect. Can anyone give me some hints or the solution of this question?,Prove that My attempt: But I think my direction is incorrect. Can anyone give me some hints or the solution of this question?,"\begin{equation*}
\det\begin{pmatrix}
a^2 & b^2 & c^2 \\
ab & bc & ca \\
b^2 & c^2 & a^2
\end{pmatrix}
=(a^2-bc)(b^2-ca)(c^2-ab)\end{equation*} \begin{equation*}
\det\begin{pmatrix}
a^2 & b^2 & c^2 \\
ab & bc & ca \\
b^2 & c^2 & a^2
\end{pmatrix}
=\det\begin{pmatrix}
a^2 & b^2 & c^2 \\
a(b-a) & b(c-b) & c(a-c) \\
(b+a)(b-a) & (c+b)(c-b) & (a+c)(a-c)
\end{pmatrix}
\end{equation*}","['linear-algebra', 'matrices', 'determinant']"
22,Prove that $T$ is invertible if and only if $0$ is not an eigenvalue of $T$,Prove that  is invertible if and only if  is not an eigenvalue of,T 0 T,"Prove that a linear operator $T$ on a finite-dimensional vector space is invertible if and only if zero is not an eigenvalue of $T$ . Definition: Let $T$ be a linear operator on a vector space $V$ . A nonzero vector $v \in V$ in an eigenvector of $T$ if there exists a scalar $\lambda$ such that $T(v)= \lambda v$ . The scalar $\lambda$ is called an eigenvalue . Let $A$ be in $M_{n,n}(F)$ . A nonzero vector $v\in F^n$ is an eigenvector of $A$ if $v$ is an eigenvector of $L_{A}$ . The scalar $\lambda$ is called the eigenvalue of $A$ . Proof: $\Rightarrow$ Let $T$ be a finite linear operator, and $T(v)=Av=\lambda v$ for $A$ to be a $M_{n,n}(F)$ matrix. If $T(v)$ is invertible, then $T(T^{-1})=(Av)(Av)^{-1}=(\lambda v)(\lambda v)^{-1}=I_n$ . That means $\lambda v$ is nonzero. $\Leftarrow$ If zero is not an eigenvalue of $T$ , that means $\lambda v=Av \neq0$ , then $det(Av)$ $\neq$ $0$ . Hence $T$ is invertible. I know this is a crappy work, but this is all I can think about.","Prove that a linear operator on a finite-dimensional vector space is invertible if and only if zero is not an eigenvalue of . Definition: Let be a linear operator on a vector space . A nonzero vector in an eigenvector of if there exists a scalar such that . The scalar is called an eigenvalue . Let be in . A nonzero vector is an eigenvector of if is an eigenvector of . The scalar is called the eigenvalue of . Proof: Let be a finite linear operator, and for to be a matrix. If is invertible, then . That means is nonzero. If zero is not an eigenvalue of , that means , then . Hence is invertible. I know this is a crappy work, but this is all I can think about.","T T T V v \in V T \lambda T(v)= \lambda v \lambda A M_{n,n}(F) v\in F^n A v L_{A} \lambda A \Rightarrow T T(v)=Av=\lambda v A M_{n,n}(F) T(v) T(T^{-1})=(Av)(Av)^{-1}=(\lambda v)(\lambda v)^{-1}=I_n \lambda v \Leftarrow T \lambda v=Av \neq0 det(Av) \neq 0 T","['linear-algebra', 'eigenvalues-eigenvectors', 'determinant']"
23,Why do real positive eigenvalues result in an unstable system? What about eigenvalues between 0 and 1? or 1?,Why do real positive eigenvalues result in an unstable system? What about eigenvalues between 0 and 1? or 1?,,"I'm confused why, if all eigenvalues of a linear system are real and positive, this entails an unstable system. For example, if eigenvalues are between 0 and 1, surely this means the system is gradually shrinking? And even more so, doesn't an eigenvalue of 1 mean the system is ""staying put""? Why is a negative eigenvalue instead related to stability? I'm confused because in the case of eigenvalues of a Markov matrix, it seemed an eigenvalue of 1 meant stability, and 0 < λ < 1 meant it was shrinking. But in both those cases the eigenvalue is positive.","I'm confused why, if all eigenvalues of a linear system are real and positive, this entails an unstable system. For example, if eigenvalues are between 0 and 1, surely this means the system is gradually shrinking? And even more so, doesn't an eigenvalue of 1 mean the system is ""staying put""? Why is a negative eigenvalue instead related to stability? I'm confused because in the case of eigenvalues of a Markov matrix, it seemed an eigenvalue of 1 meant stability, and 0 < λ < 1 meant it was shrinking. But in both those cases the eigenvalue is positive.",,"['linear-algebra', 'eigenvalues-eigenvectors', 'dynamical-systems', 'stability-theory']"
24,Why is the Change of Basis map unique?,Why is the Change of Basis map unique?,,"I've been looking all over, but I haven't found anything satisfactory. We've been shown in class by a commutative diagram that, given an $n$ -dimensional vector space $V$ over a field, $\mathbb{F}$ , and bases, $\mathcal{B}=\{v_1,...,v_n\}$ and $\mathcal{C}=\{u_1,...,u_n\}$ , the coordinate maps $[]_{\mathcal{B}}:V\rightarrow \mathbb{F}^n$ and $[]_{\mathcal{C}}:V\rightarrow \mathbb{F}^n$ give rise to a unique map $P=[]_{\mathcal{B}}\circ []^{-1}_{\mathcal{C}}:\mathbb{F}^n\rightarrow \mathbb{F}^n$ , which is our change of basis matrix. But I am having a lot of trouble proving that $P$ is unique. Can anyone enlighten me as to why this is necessarily true?","I've been looking all over, but I haven't found anything satisfactory. We've been shown in class by a commutative diagram that, given an -dimensional vector space over a field, , and bases, and , the coordinate maps and give rise to a unique map , which is our change of basis matrix. But I am having a lot of trouble proving that is unique. Can anyone enlighten me as to why this is necessarily true?","n V \mathbb{F} \mathcal{B}=\{v_1,...,v_n\} \mathcal{C}=\{u_1,...,u_n\} []_{\mathcal{B}}:V\rightarrow \mathbb{F}^n []_{\mathcal{C}}:V\rightarrow \mathbb{F}^n P=[]_{\mathcal{B}}\circ []^{-1}_{\mathcal{C}}:\mathbb{F}^n\rightarrow \mathbb{F}^n P","['linear-algebra', 'linear-transformations']"
25,"If rank$(T)\le$rank$(T^3)$, then intersection of range and null space of $T$ is zero","If rankrank, then intersection of range and null space of  is zero",(T)\le (T^3) T,"If $T:V\to V$ be linear trannsformation on a vector space $V$ with rank $(T)\le$ rank $(T^3)$ . Then, what can be said about the null space and range of $T$ . Typically, do they have empty intersection, or, is one the subset of another, or does there exist a subspace $W$ of $V$ which equals their intersection. I know that range of $T^3$ is a subspace of $T$ and null space of $T$ is a subspace of $T^3$ . But, am unable to proceed further. Any hints. Thanks beforehand.","If be linear trannsformation on a vector space with rank rank . Then, what can be said about the null space and range of . Typically, do they have empty intersection, or, is one the subset of another, or does there exist a subspace of which equals their intersection. I know that range of is a subspace of and null space of is a subspace of . But, am unable to proceed further. Any hints. Thanks beforehand.",T:V\to V V (T)\le (T^3) T W V T^3 T T T^3,"['linear-algebra', 'linear-transformations']"
26,Set of symmetric positive semidefinite matrices is closed,Set of symmetric positive semidefinite matrices is closed,,"I am self-studying Boyd & Vandenberghe's Convex Optimization . Example 2.15 (page 43) states that the symmetric positive semi-definite cone $S^n_+$ is a proper cone. This necessitates, amongst other things, that it is closed. I am not sure how to show that $S^n_+$ is closed, particularly because this set consists of matrices, which I am less comfortable working with. The most relevant question I have found that may have some relation to this one is here ; I am not sure how to act on the answer of this question for I am not sure of whether the functions $f_1$ and $f_2$ as defined in the answer are relevant to my task.","I am self-studying Boyd & Vandenberghe's Convex Optimization . Example 2.15 (page 43) states that the symmetric positive semi-definite cone is a proper cone. This necessitates, amongst other things, that it is closed. I am not sure how to show that is closed, particularly because this set consists of matrices, which I am less comfortable working with. The most relevant question I have found that may have some relation to this one is here ; I am not sure how to act on the answer of this question for I am not sure of whether the functions and as defined in the answer are relevant to my task.",S^n_+ S^n_+ f_1 f_2,"['linear-algebra', 'general-topology', 'matrices', 'symmetric-matrices', 'positive-semidefinite']"
27,Can I quickly determine the eigenvalues of this matrix?,Can I quickly determine the eigenvalues of this matrix?,,"I am working on observability and detectability in controls and I ran across this example that I didn't understand. The author deliberately sought the form of this matrix, because of its ""block-form"" in order to quickly find the eigenvalues \begin{bmatrix}     l_{11}  & -1  & -1 & 0 & 0 \\     0 & -1  & 0 & 0 & 0 \\     0 & -1  & -1 & 0 & 0 \\     0 & -.1 & -2 & l_{42} & -.1 \\     0 &  1  &  2 & 0 & -.2 \end{bmatrix} The author was then able to state the eigenvalues were $\{l_{11}, -1, l_{42}, -.2\}$ I was under the impression that I could only determine the eigenvalues via  a matrix diagonal if the matrix was upper/lower triangular?","I am working on observability and detectability in controls and I ran across this example that I didn't understand. The author deliberately sought the form of this matrix, because of its ""block-form"" in order to quickly find the eigenvalues \begin{bmatrix}     l_{11}  & -1  & -1 & 0 & 0 \\     0 & -1  & 0 & 0 & 0 \\     0 & -1  & -1 & 0 & 0 \\     0 & -.1 & -2 & l_{42} & -.1 \\     0 &  1  &  2 & 0 & -.2 \end{bmatrix} The author was then able to state the eigenvalues were $\{l_{11}, -1, l_{42}, -.2\}$ I was under the impression that I could only determine the eigenvalues via  a matrix diagonal if the matrix was upper/lower triangular?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'control-theory']"
28,Linear Transformation,Linear Transformation,,"Alright so I have this Transformation that I know isn't one to one transformation, but I'm not sure why. A Transformation is defined as $f(x,y)=(x+y, 2x+2y)$. Now my knowledge is that you need to fulfill the 2 conditions: Additivity and the scalar multiplication one. I tried both of them and somehow both of them are met perfectly. However, the transformation is NOT linear. This is because the column vectors of the transformation are linearly dependent. So how am I supposed to relate these 2 seemingly unrelated conjectures to check the one-one transformation ?","Alright so I have this Transformation that I know isn't one to one transformation, but I'm not sure why. A Transformation is defined as $f(x,y)=(x+y, 2x+2y)$. Now my knowledge is that you need to fulfill the 2 conditions: Additivity and the scalar multiplication one. I tried both of them and somehow both of them are met perfectly. However, the transformation is NOT linear. This is because the column vectors of the transformation are linearly dependent. So how am I supposed to relate these 2 seemingly unrelated conjectures to check the one-one transformation ?",,['linear-algebra']
29,Difference between vectors and span,Difference between vectors and span,,"Let $A =      \begin{bmatrix}     	1       & 0 & -4 \\     	0       & 3 & -2 \\     	-2       & 6 & 3      \end{bmatrix} $ and $b=\begin{bmatrix}     	4   \\     	1    \\     	4        \end{bmatrix}$. Denote the columns of $A$ by $a_1,a_2,a_3$ and let $W=Span\{a_1,a_2,a_3\}$ How many vectors are in $\{a_1, a_2, a_3\}$? Correct answer: 3 How many vectors are in $W$? Correct answer: infinitely many I'm confused on the difference between the two answers, and why the answer to both isn't infinitely many. I thought that the vector $a_1$ and $2*a_1$ are equivalently the same.","Let $A =      \begin{bmatrix}     	1       & 0 & -4 \\     	0       & 3 & -2 \\     	-2       & 6 & 3      \end{bmatrix} $ and $b=\begin{bmatrix}     	4   \\     	1    \\     	4        \end{bmatrix}$. Denote the columns of $A$ by $a_1,a_2,a_3$ and let $W=Span\{a_1,a_2,a_3\}$ How many vectors are in $\{a_1, a_2, a_3\}$? Correct answer: 3 How many vectors are in $W$? Correct answer: infinitely many I'm confused on the difference between the two answers, and why the answer to both isn't infinitely many. I thought that the vector $a_1$ and $2*a_1$ are equivalently the same.",,"['linear-algebra', 'matrices', 'vectors']"
30,Linear Algebra Proof $AB=0 \Longrightarrow \det(A)=0$,Linear Algebra Proof,AB=0 \Longrightarrow \det(A)=0,"Two squared matrices $A$ and $B$, with $B\neq0$, give $AB=0$. Prove that $\det(A)=0$. After trying with some examples, I believe that $A$ needs to have lines that are equal or can be made equal by scalar multiplication, B needs to have columns that are equal or can be made equal by scalar multiplication, like $$ A=         \begin{bmatrix}         1 & 2 \\         2 & 4 \\         \end{bmatrix} $$ and $$ B=         \begin{bmatrix}         2 & 4 \\         -1 & -2 \\         \end{bmatrix} $$ which would mean that $\det(A)=0$ and $\det(B)=0$. But this is still far from being a proof of anything. Am I on the right track? What would be my next step?","Two squared matrices $A$ and $B$, with $B\neq0$, give $AB=0$. Prove that $\det(A)=0$. After trying with some examples, I believe that $A$ needs to have lines that are equal or can be made equal by scalar multiplication, B needs to have columns that are equal or can be made equal by scalar multiplication, like $$ A=         \begin{bmatrix}         1 & 2 \\         2 & 4 \\         \end{bmatrix} $$ and $$ B=         \begin{bmatrix}         2 & 4 \\         -1 & -2 \\         \end{bmatrix} $$ which would mean that $\det(A)=0$ and $\det(B)=0$. But this is still far from being a proof of anything. Am I on the right track? What would be my next step?",,['linear-algebra']
31,Three angles are linearly independent over $\mathbb{Q}$?,Three angles are linearly independent over ?,\mathbb{Q},"If$$\tan \alpha = 1, \text{ }\tan \beta = {3\over 2}, \text{ }\tan \gamma = 2,$$then does it follow that $\alpha$, $\beta$, $\gamma$ are linearly independent over $\mathbb{Q}$? It is possible to test combinations $m\alpha+n\beta+\ell\gamma$ with some small integer coefficients $m,n,\ell$. The tool for doing that is the sum formula for tangents of two angles with known tangents: $$ \tan(x\pm y)=\frac{\tan x\pm\tan y}{1\mp\tan x\tan y}. $$ For example, judging from a picture $\beta+2\gamma$ is relatively close to $\pi=4\alpha$, but the calculations: $$ \tan 2\gamma=\frac{2+2}{1-2\cdot2}=-\frac43, $$ $$ \tan(\beta+2\gamma)=\frac{3/2-4/3}{1+(3/2)(4/3)}=\frac1{18} $$ show that it is not a match.","If$$\tan \alpha = 1, \text{ }\tan \beta = {3\over 2}, \text{ }\tan \gamma = 2,$$then does it follow that $\alpha$, $\beta$, $\gamma$ are linearly independent over $\mathbb{Q}$? It is possible to test combinations $m\alpha+n\beta+\ell\gamma$ with some small integer coefficients $m,n,\ell$. The tool for doing that is the sum formula for tangents of two angles with known tangents: $$ \tan(x\pm y)=\frac{\tan x\pm\tan y}{1\mp\tan x\tan y}. $$ For example, judging from a picture $\beta+2\gamma$ is relatively close to $\pi=4\alpha$, but the calculations: $$ \tan 2\gamma=\frac{2+2}{1-2\cdot2}=-\frac43, $$ $$ \tan(\beta+2\gamma)=\frac{3/2-4/3}{1+(3/2)(4/3)}=\frac1{18} $$ show that it is not a match.",,"['linear-algebra', 'abstract-algebra']"
32,Proof: The identity matrix is invertible and the inverse of the identity is the identity,Proof: The identity matrix is invertible and the inverse of the identity is the identity,,"How can i show that: $II^{-1} = I = I^{-1}I$ (the identity matrix is invertible) for all cases. And then proof that: $I^{-1} = I$ (The inverse of the identity is the identity). I don't know how start both proof, any suggestion?","How can i show that: $II^{-1} = I = I^{-1}I$ (the identity matrix is invertible) for all cases. And then proof that: $I^{-1} = I$ (The inverse of the identity is the identity). I don't know how start both proof, any suggestion?",,"['linear-algebra', 'matrices', 'inverse']"
33,Inequality between AM-GM,Inequality between AM-GM,,"Prove that for $x>y>0$ $$\sqrt {xy} <\frac {x-y}{\ln x-\ln y}<\frac {x+y}2$$ Using $x=y+k$, we can turn the inequality into $$\sqrt{y^2+ky}<\frac k{\ln(1+\frac ky)}<y+\frac k2$$ Now what do i do to the $\ln\left(1+\frac ky\right) $ term? We can't approximate it using the expansion of $\ln(1+x)$ as we have no information on $k$, i.e., how big $x$ is in comparison to $y$.  Please give a hint.","Prove that for $x>y>0$ $$\sqrt {xy} <\frac {x-y}{\ln x-\ln y}<\frac {x+y}2$$ Using $x=y+k$, we can turn the inequality into $$\sqrt{y^2+ky}<\frac k{\ln(1+\frac ky)}<y+\frac k2$$ Now what do i do to the $\ln\left(1+\frac ky\right) $ term? We can't approximate it using the expansion of $\ln(1+x)$ as we have no information on $k$, i.e., how big $x$ is in comparison to $y$.  Please give a hint.",,"['linear-algebra', 'inequality', 'a.m.-g.m.-inequality']"
34,Can a matrix A with the property $A=A^{-1}$ only have the eigenvalues -1 and 1?,Can a matrix A with the property  only have the eigenvalues -1 and 1?,A=A^{-1},"If a matrix A has the property $A=A^{-1}$, are the only possible eigenvalues   1 and -1 ? How can the matrices with integer values and the property $A=A^{-1}$ be  characterized ? I found out that if A has the property $A=A^{-1}$, then $-A$, $A^T$ and  $B^{-1}AB$ for any invertible B also have the property. I also think that the theorem of caley-hamilton is useful for my  problem.","If a matrix A has the property $A=A^{-1}$, are the only possible eigenvalues   1 and -1 ? How can the matrices with integer values and the property $A=A^{-1}$ be  characterized ? I found out that if A has the property $A=A^{-1}$, then $-A$, $A^T$ and  $B^{-1}AB$ for any invertible B also have the property. I also think that the theorem of caley-hamilton is useful for my  problem.",,"['linear-algebra', 'matrices', 'inverse']"
35,Eigenvalues of a Permutation?,Eigenvalues of a Permutation?,,"I'm stuck on the following problem for my Linear Algebra class: Let $\pi:\{1, \ldots, n \} \rightarrow \{1, \ldots, n\}$ be a bijective map (permutation). Let $f_{\pi}:\mathbb{R}^n \rightarrow \mathbb{R}^n$ be defined by $f_{\pi}(x_1, \ldots, x_n) := (x_{\pi(1)},\ldots,x_{\pi(n)})$. Determine the set of eigenvalues of $f_{\pi}$. I know that in general for a square matrix $A$, the set of real eigenvalues of $A$ is given by $\{\lambda \in \mathbb{R}\mid \det(A- \lambda I) = 0\}.$ I've figured out that the matrix representation of $f_{\pi}$ with respect to the standard ordered basis will have rows that contain all $0$s except for a $1$ in one column; this column will be different for each row. However, I can't figure out how to get the eigenvalues from there. Please advise.","I'm stuck on the following problem for my Linear Algebra class: Let $\pi:\{1, \ldots, n \} \rightarrow \{1, \ldots, n\}$ be a bijective map (permutation). Let $f_{\pi}:\mathbb{R}^n \rightarrow \mathbb{R}^n$ be defined by $f_{\pi}(x_1, \ldots, x_n) := (x_{\pi(1)},\ldots,x_{\pi(n)})$. Determine the set of eigenvalues of $f_{\pi}$. I know that in general for a square matrix $A$, the set of real eigenvalues of $A$ is given by $\{\lambda \in \mathbb{R}\mid \det(A- \lambda I) = 0\}.$ I've figured out that the matrix representation of $f_{\pi}$ with respect to the standard ordered basis will have rows that contain all $0$s except for a $1$ in one column; this column will be different for each row. However, I can't figure out how to get the eigenvalues from there. Please advise.",,"['linear-algebra', 'eigenvalues-eigenvectors']"
36,$x^TAx=0$ for all $x$ when $A$ is a skew symmetric matrix,for all  when  is a skew symmetric matrix,x^TAx=0 x A,Let $A$ be an $n\times n$ skew symmetric matrix. Show that $x^TAx =0 \ \forall x \in \mathbb R^n$ . How to prove this?,Let be an skew symmetric matrix. Show that . How to prove this?,A n\times n x^TAx =0 \ \forall x \in \mathbb R^n,"['linear-algebra', 'matrices']"
37,"Proof that the set $\{ x \in R^n | Ax \leq b, Cx = d \}$ is convex",Proof that the set  is convex,"\{ x \in R^n | Ax \leq b, Cx = d \}","I need a help with prooving that a given set is a convex set: $\{ x \in R^n | Ax \leq b, Cx = d \}$ I know the definition of convexity: $X \in R^n$ is a convex set if $\forall \alpha \in R, 0 \leq\alpha \leq 1$ and $\forall x,y \in X$ holds: $\alpha x + (1 - \alpha)y \in X$. I tried to apply this for my set but I dont know how to prove that it works... Thanks in advance for any tips.","I need a help with prooving that a given set is a convex set: $\{ x \in R^n | Ax \leq b, Cx = d \}$ I know the definition of convexity: $X \in R^n$ is a convex set if $\forall \alpha \in R, 0 \leq\alpha \leq 1$ and $\forall x,y \in X$ holds: $\alpha x + (1 - \alpha)y \in X$. I tried to apply this for my set but I dont know how to prove that it works... Thanks in advance for any tips.",,"['linear-algebra', 'convex-analysis', 'convex-geometry']"
38,Condition for commuting matrices,Condition for commuting matrices,,"Let $A,B$ be $n \times n$ matrices over the complex numbers. If $B=p(A)$ where $p(x) \in \mathbb{C}[x]$ then certainly $A,B$ commute. Under which conditions the converse is  true? Thanks :-)","Let $A,B$ be $n \times n$ matrices over the complex numbers. If $B=p(A)$ where $p(x) \in \mathbb{C}[x]$ then certainly $A,B$ commute. Under which conditions the converse is  true? Thanks :-)",,"['linear-algebra', 'matrices']"
39,What's the Difference Between a Vector and an Hypercomplex Number?,What's the Difference Between a Vector and an Hypercomplex Number?,,What's the difference between a vector and an hypercomplex number? For instance a 4-vector and a quaternion. They seem to share many properties. Perhaps this question could be put more generally as: what's the difference between a vector space and a field?,What's the difference between a vector and an hypercomplex number? For instance a 4-vector and a quaternion. They seem to share many properties. Perhaps this question could be put more generally as: what's the difference between a vector space and a field?,,"['linear-algebra', 'field-theory', 'vector-spaces', 'complex-numbers']"
40,Does the ratio of consecutive terms converge for all linear recursions?,Does the ratio of consecutive terms converge for all linear recursions?,,"Does $f(n+1)/f(n)$ converge as $n\rightarrow\infty$ for $f(n)$ defined by a linear recursion, for all linear recursions?","Does $f(n+1)/f(n)$ converge as $n\rightarrow\infty$ for $f(n)$ defined by a linear recursion, for all linear recursions?",,"['linear-algebra', 'convergence-divergence', 'recurrence-relations']"
41,Rolling an elliptical disc on the $x$ axis,Rolling an elliptical disc on the  axis,x,"You're given the elliptical disc bounded by $ \dfrac{x^2}{a^2} + \dfrac{(y - b)^2}{b^2} = 1 $ where $a = 5, b = 2 $ .  You roll this ellipse to the right along the positive $x$ axis, such that it is always tangent to the $x$ axis.  You stop rolling when the elliptical boundary of the disc becomes tangent to the $x$ axis at $(5, 0)$ .  If a point $P_0 = (-2, 3) $ is on the disc before rolling, find its position after rolling. My attempt: The distance travelled on the $x$ axis, is the same traversed on the boundary of the ellipse.  From the parametric equation of the ellipse, one can determine the eccentric angle of the point of tangency on the rolled ellipse using the arc length (which is equal to $5$ ).  Then, the initial and final normal vectors to the circumference of the ellipse can be computed, and the angle between them determines the rotation of the ellipse.  Having the tangency point and the rotation determines the center of the rolled ellipse.  The affine transformation between the initial points on the disc and rolled images is given by $ p' = C + R (p - C_0) $ where $R$ is the rotation matrix resulting from rolling, $C_0$ is the initial center (equal to $(0, b)$ ) and $C$ is the final center.  Using this equation with $p = (-2, 3)$ , one can compute its image $p'$ .","You're given the elliptical disc bounded by where .  You roll this ellipse to the right along the positive axis, such that it is always tangent to the axis.  You stop rolling when the elliptical boundary of the disc becomes tangent to the axis at .  If a point is on the disc before rolling, find its position after rolling. My attempt: The distance travelled on the axis, is the same traversed on the boundary of the ellipse.  From the parametric equation of the ellipse, one can determine the eccentric angle of the point of tangency on the rolled ellipse using the arc length (which is equal to ).  Then, the initial and final normal vectors to the circumference of the ellipse can be computed, and the angle between them determines the rotation of the ellipse.  Having the tangency point and the rotation determines the center of the rolled ellipse.  The affine transformation between the initial points on the disc and rolled images is given by where is the rotation matrix resulting from rolling, is the initial center (equal to ) and is the final center.  Using this equation with , one can compute its image ."," \dfrac{x^2}{a^2} + \dfrac{(y - b)^2}{b^2} = 1  a = 5, b = 2  x x x (5, 0) P_0 = (-2, 3)  x 5  p' = C + R (p - C_0)  R C_0 (0, b) C p = (-2, 3) p'","['linear-algebra', 'geometry', 'analytic-geometry', 'conic-sections', 'rotations']"
42,Dual space isomorphism non-canonical choice example,Dual space isomorphism non-canonical choice example,,"In a lot of resources that I have read it is mentioned that the isomorphism between $V$ and $V^*$ is non-canonical, but I was never sure that I properly understood precisely what this means. I haven't studied category theory so I couldn't really understand some other answers on math.stack based on category theory arguments. I will try to explain how I understand things and maybe someone could comment on that. Setting Let $V$ be a finite-dimensional vector space. Its dual $V^*$ is the space of linear maps from $V$ to $\mathbb{F}$ : $V^* = L(V,\mathbb{F})$ . Now let $v$ be an arbitrary vector from $V$ . If I fix a basis $F = (f_1,\ldots,f_n)$ for $V$ then I can write $v = \sum_{i=1}^n [v]_F^i f_i$ , for some unique coefficient vector $[v]_F\in\mathbb{F}^n$ . Then as far as I understand the ""non-canonical isomorphism"" that depends on this basis is $v\mapsto\alpha = \sum_{i=1}^n [\alpha]^{F^*}_if^i = \sum_{i=1}^n [v]^i_F f^i$ where $F^* = (f^1,\ldots,f^n)$ is the dual basis w.r.t. $F$ such that $f^i(f_j) = \delta^i_j$ . Now let me choose a different basis $G = (g_1,\ldots,g_n)$ for $V$ , then the dual basis w.r.t. it is $G^*=(g^1,\ldots,g^n)$ such that $g^i(g_j) = \delta^i_j$ . Now I can write the vector $v$ in terms of the basis $G$ : $v = \sum_{i=1}^n [v]_G^i g_i$ , and I can form a functional $\beta = \sum_{i=1}^n [\beta]^{G^*}_ig^i = \sum_{i=1}^n [v]^i_G g^i$ . For $\alpha$ and $\beta$ to be well-defined (i.e. so that the values $\alpha(u)$ and $\beta(u)$ are fixed for every $u\in V$ irrespective of the coordinate representation), their coordinates must transform in an appropriate manner under a change of basis. Let $P$ be the change of basis matrix such that $g_i = \sum_{j=1}^n f_j P^j_i$ then in order to have $f^i(f_j) =\delta^i_k \implies g^i(g_j) = \delta^i_j$ I need that $g^i = \sum_{j=1}^n (P^{-1})^i_j f^j$ since: $$g^i(g_j) = \sum_{k=1}^n (P^{-1})^i_k f^k \left(\sum_{l=1}^n P^l_j f_l\right) = \sum_{k=1}^{n}\sum_{l=1}^n(P^{-1})^i_kP^l_jf^k(f_l) = \sum_{k=1}^n(P^{-1})^i_kP^k_j = \delta^i_j.$$ If I make it so that I transform the coordinates as follows: $[\alpha]^{G^*} = [\alpha]^{F^*}P$ and $[v]_G = P^{-1}[v]_F$ , then I should get the same result for $\alpha(v)$ regardless of the chosen basis. Of course now $[\alpha]^{G^*}\ne[v]_G^T=[\beta]^{G^*}$ in general showing that $\alpha\ne \beta$ . The Question Is the ""non-canonical isomorphism"" referring to the fact that $\alpha\ne \beta$ in the general case? That is, the construction of $\alpha$ was a function of my choice of $f_1,\ldots,f_n$ , while the construction of $\beta$ was a function of my choice of $g_1,\ldots,g_n$ . I assume that $\alpha$ could miraculously be equal to $\beta$ in some cases (e.g. if $f_1=g_1$ and $[v]^i_F=0=[v]^i_G$ for $i>1$ ), but in general it is not, so the non-canonicity is referring to the dependence on the choice of basis? If I had a canonical basis, for example $(e_1,\ldots,e_n)$ in $\mathbb{R}^n$ , would this then be considered a canonical choice instead? On the other hand, if I have a non-degenerate bilinear form $B:V\times V\to\mathbb{F}$ , I can construct an isomorphism $\gamma_v = B(v,\_)$ , that is independent of the basis. So is the word canonic referring to independence of the choice of basis in the initial definition of the functional? Example If I take a basis for the space of polynomials of at most degree $1$ then I could choose the monomial basis $(f_1,f_2) = (1,x)$ or the Lagrange basis $(g_1,g_2) = \left(\frac{x-x_0}{x_1-x_0}, \frac{x-x_1}{x_0-x_1}\right)$ . The dual bases are given as $(f^1,f^2) = \left(\delta_0, \delta_0 \circ \frac{d}{dx}\right)$ and $(g^1,g^2) = \left(\delta_{x_1},\delta_{x_0}\right)$ , where $\delta_p(f) = f(p)$ . Now if I have the vector $v = a\frac{x-x_0}{x_1-x_0} + b\frac{x-x_1}{x_0-x_1}$ then $v = \frac{bx_1-ax_0}{x_1-x_0}+\frac{a-b}{x_1-x_0}x = p\cdot 1 + qx $ . This induces the two functionals $\alpha = p\delta_0 + q\delta_0 \circ \frac{d}{dx}$ and $\beta = a\delta_{x_1}+b\delta_{x_0}$ . Now $\beta(v) = a^2+b^2$ while $\alpha(v) = p^2+q^2$ , and in general those are unequal, thus $\alpha\ne\beta$ . However if I were to take for example the inner product $\langle v, w\rangle = \int_{0}^{1} v(t) w(t)\,dt$ then I could define the ""canonical isomorphism"" $\alpha_v = \langle v, \_\rangle$ . Now a subsequent question is - didn't I just swap the choice of the initial basis for a choice of an inner product? Because I could of course take a number of other inner products that would result in different functionals. In what sense is the choice of inner product more canonical than the choice of basis? Or did I misunderstand this, and a choice of inner product doesn't result in a canonical isomorphism either. But if I go with that and I presuppose that the choice is canonical only when a canonical inner product exists, wouldn't this be the same argument as presupposing that a canonical basis exists? Basically I have trouble making sense of what ""canonical"" is referring to in the first place. Canonical Isomorphism to the Double Dual $V^{**}$ The isomorphism $v\mapsto v^{**}(\phi) = \phi(v)$ is considered ""canonical"" , I assume because there is no choice whatsoever in the above definition - I neither choose a basis nor an inner product. So is the above the only thing that ""canonical"" can refer to? Or is a vector space with a bilinear form isomorphism to $V^*$ also considered ""canonical"" if I assume that there is some distinguished canonical bilinear form? And similarly a choice $v\mapsto \alpha = \sum_{i=1}[\alpha]^{E^*}_ie^i = \sum_{i=1}^n[v]^i_E e_i$ would be considered ""canonical"" if there is some distinguished ""canonical"" basis $E$ ? Essentially, what do I need for something to be considered canonic? Edit: Attempt at a solution of the exercise from coiso For $(V,\mathbb{F}_2)$ with $\dim = 1$ , with $V=\{0,x_1\}$ and $V^*=\{0^*,y^1\}$ I can construct the table: \begin{equation} \begin{array}{c|c} & 0^* & y^1 \\ \hline 0 & 0 & 0 \\ \hline x_1 & 0 & 1  \end{array} \end{equation} If I assign $0\ne x_1\mapsto 0^*$ and $0\mapsto y^1$ then $0^* = (x_1)^* = (x_1+0)^* = 0^* + y^1 = y^1$ , and the assignment is not injective hence not an isomorphism. So there is only one valid choice and then $V\cong V^*$ . For $(V,\mathbb{F}_2)$ with $\dim = 2$ , with $V=\{0,x_1,x_2,x_{12}=x_1+x_2\}$ and $V^* = \{0^*, y^1, y^2, y^{12}\}$ I can construct the table: \begin{equation} \begin{array}{c|c|c|c|c} & 0^* & y^1 & y^2 & y^{12} \\ \hline 0 & 0 & 0 & 0 & 0 \\ \hline x_1 & 0 & 1 & 0 & 1 \\ \hline x_2 & 0 & 0 & 1 & 1 \\ \hline x_{12} & 0 & 1 & 1 & 0 \end{array} \end{equation} I cannot have any other columns in the table as otherwise the functionals won't be linear. One possible assignment, when taking $x_1,x_2$ as basis vectors for $V$ is $0\mapsto 0^*$ and $x_J\mapsto y^J$ . I could of course take $x_1,x_{12}$ instead as a basis, then the dual basis must be $x_1\mapsto y^{12}$ and $x_{12}\mapsto y^2$ . Then the only option left is $x_2\mapsto y^1$ . I don't get why any of the two assignments should be more canonical, unless I am missing something and the second assignment breaks linearity, which doesn't seem obvious from the table. Edit 2: Other Isomorphisms to the Double Dual Thinking about this I realized that I could define the map $I_{\lambda} : V\to V^{**}$ , $v\mapsto I_{\lambda}(V) = \lambda v^{**}$ such that $I_{\lambda}(v)(\phi) = \lambda v^{**}(\phi) = \lambda \phi(v)$ for $\lambda \ne 0$ . This map looks like it is bijective, and it is linear since $I_{\lambda}(au + bv)(\phi) = \lambda \phi(au+bv) = a\lambda\phi(u)+b\lambda\phi(v) = (aI(u)+bI(v))(\phi)$ , so it must be an isomorphism. Yet I suppose this would not be termed canonical, or will it? To my understanding it is the map $I_{1}(v)(\phi) = \phi(v)$ that is the canonical isomorphism, i.e., where $\lambda=1$ . Clearly the above family of isomorphisms $\{I_{\lambda}\,:\,\lambda \ne 0\}$ do not depend on a choice of basis or inner product, and yet I do not think that they would be termed canonical. So I am even more confused now, as clearly ""canonical"" is not referring to a choice of basis or inner product in this case. Or is it that all $I_{\lambda}$ are considered canonical isomorphisms? I also found this video , I don't know how relevant it is to the above setting though. From the video it becomes clear that canonical isomorphism is a kind of equality that is true for a certain subset of statements, but not all. So the question is what is canonical referring to in the above cases. Are all $I_{\lambda}$ canonical isomorphisms or is $I_1$ the only canonical isomorphism? Is $v\mapsto B(v, \_)$ canonical if there is only a single non-degenerate bilinear form $B$ given, i.e. $(V,B)\stackrel{canon}{\cong} (V^*, B^*) = (V^*, B(B^{-1}, B^{-1})$ but $V\stackrel{non-canon}{\cong} V^*$ , and also $(V,(B_1, B_2)) \stackrel{non-canon}{\cong} (V^*, (B^*_1, B^*_2))$ . Is the isomorphism canonical if a single basis is specified but no more, i.e. $(V, E) \stackrel{canon}{\cong} (V^*, E^*)$ but $(V, (E_1, E_2)) \stackrel{non-canon}{\cong} (V, (E_1^*, E_2^*))$ ? Also in the answers Mikhail Katz mentioned that in differential geometry the map may be considered non-canonical if it is not continuous, but I assume this is slightly different, as I don't need a notion of continuity to talk about the algebraic dual and double dual as far as I know. Edit 4: What I Gathered so Far There are many isomorphisms $V\to V^{**}$ but the canonical one is given by the evaluation functional $\Lambda:V\to V^{**}$ : $$\Lambda(v)(\phi) := \phi(v), \quad \forall \phi\in V^*.$$ According to coiso also the isomorphisms $I_{\lambda}(v)(\phi) = \lambda\phi(v)$ for $\lambda\ne 0$ can be termed canonical, but they are not the canonical isomorphism, which is instead given by the evaluation functional $\Lambda(v)$ . As far as I understood the motivation for calling those canonical is because they do not require making a choice of basis, and we care greatly about results being coordinate invariant in linear algebra, differential geometry, physics, etc. The isomorphism $J : (V,B) \to (V^*,B^*)$ where $J(v) = B(v,\_)$ is also canonical if $B$ is a non-degenerate bilinear form (conjugate-linear in one of the slots in the complex-valued case). If $B$ is degenerate then the latter is not bijective so not an isomorphism, but I guess that restricted to the subspace where it is non-degenerate it will be a bijection and thus a canonical isomorphism on that subspace. Arturo Magidin made it clear that the isomorphism $J$ is canonical because it is between the vector space along with its inner product/bilinear form $(V,B)$ and its counterpart $(V^*,B^*)$ . As far as I understood a map $J_B : V\to V^*$ where $J_B(v) = B(v,\_)$ would not provide a canonical isomorphism between $V$ and $V^*$ by themselves, since one could choose a different bilinear form, which would result in a different identification. The isomorphism $K_F:V\to V^*$ : $$K_F(v) = \sum_{i=1}^n [v]^i_F f^i \in V^*,$$ where $F = (f_1,\ldots,f_n)$ is a basis for $V$ and $F^*=(f^1,\ldots,f^n)$ is the dual basis for $V^*$ such that $f^i(f_j) = \delta^i_j$ , is not a canonical isomorphism between $V$ and $V^*$ in general. This is because it depend on a choice of basis $F$ , and as illustrated by my above examples a different basis can result in a different result. A special case when there is a canonical isomorphism is when $\dim V\leq 2$ and the underlying field is $\mathbb{F}_2$ as given in the solution by coiso. I believe that also a one-dimensional space is canonically isomorphic to its dual (even if it is not over $\mathbb{F}_2$ ) since the map $K_F$ and $K_G$ actually produce the same functionals irrespective of the choice of $F$ and $G$ (the scalar difference gets canceled out by the change of basis). Finally, supposedly also $K : (V,F) \to (V^*,F^*)$ where $K(v) = K_F(v)$ should also be a canonical isomorphism since I am now identifying a vector space along with a basis. The latter is more restrictive than providing a non-degenerate bilinear form however.","In a lot of resources that I have read it is mentioned that the isomorphism between and is non-canonical, but I was never sure that I properly understood precisely what this means. I haven't studied category theory so I couldn't really understand some other answers on math.stack based on category theory arguments. I will try to explain how I understand things and maybe someone could comment on that. Setting Let be a finite-dimensional vector space. Its dual is the space of linear maps from to : . Now let be an arbitrary vector from . If I fix a basis for then I can write , for some unique coefficient vector . Then as far as I understand the ""non-canonical isomorphism"" that depends on this basis is where is the dual basis w.r.t. such that . Now let me choose a different basis for , then the dual basis w.r.t. it is such that . Now I can write the vector in terms of the basis : , and I can form a functional . For and to be well-defined (i.e. so that the values and are fixed for every irrespective of the coordinate representation), their coordinates must transform in an appropriate manner under a change of basis. Let be the change of basis matrix such that then in order to have I need that since: If I make it so that I transform the coordinates as follows: and , then I should get the same result for regardless of the chosen basis. Of course now in general showing that . The Question Is the ""non-canonical isomorphism"" referring to the fact that in the general case? That is, the construction of was a function of my choice of , while the construction of was a function of my choice of . I assume that could miraculously be equal to in some cases (e.g. if and for ), but in general it is not, so the non-canonicity is referring to the dependence on the choice of basis? If I had a canonical basis, for example in , would this then be considered a canonical choice instead? On the other hand, if I have a non-degenerate bilinear form , I can construct an isomorphism , that is independent of the basis. So is the word canonic referring to independence of the choice of basis in the initial definition of the functional? Example If I take a basis for the space of polynomials of at most degree then I could choose the monomial basis or the Lagrange basis . The dual bases are given as and , where . Now if I have the vector then . This induces the two functionals and . Now while , and in general those are unequal, thus . However if I were to take for example the inner product then I could define the ""canonical isomorphism"" . Now a subsequent question is - didn't I just swap the choice of the initial basis for a choice of an inner product? Because I could of course take a number of other inner products that would result in different functionals. In what sense is the choice of inner product more canonical than the choice of basis? Or did I misunderstand this, and a choice of inner product doesn't result in a canonical isomorphism either. But if I go with that and I presuppose that the choice is canonical only when a canonical inner product exists, wouldn't this be the same argument as presupposing that a canonical basis exists? Basically I have trouble making sense of what ""canonical"" is referring to in the first place. Canonical Isomorphism to the Double Dual The isomorphism is considered ""canonical"" , I assume because there is no choice whatsoever in the above definition - I neither choose a basis nor an inner product. So is the above the only thing that ""canonical"" can refer to? Or is a vector space with a bilinear form isomorphism to also considered ""canonical"" if I assume that there is some distinguished canonical bilinear form? And similarly a choice would be considered ""canonical"" if there is some distinguished ""canonical"" basis ? Essentially, what do I need for something to be considered canonic? Edit: Attempt at a solution of the exercise from coiso For with , with and I can construct the table: If I assign and then , and the assignment is not injective hence not an isomorphism. So there is only one valid choice and then . For with , with and I can construct the table: I cannot have any other columns in the table as otherwise the functionals won't be linear. One possible assignment, when taking as basis vectors for is and . I could of course take instead as a basis, then the dual basis must be and . Then the only option left is . I don't get why any of the two assignments should be more canonical, unless I am missing something and the second assignment breaks linearity, which doesn't seem obvious from the table. Edit 2: Other Isomorphisms to the Double Dual Thinking about this I realized that I could define the map , such that for . This map looks like it is bijective, and it is linear since , so it must be an isomorphism. Yet I suppose this would not be termed canonical, or will it? To my understanding it is the map that is the canonical isomorphism, i.e., where . Clearly the above family of isomorphisms do not depend on a choice of basis or inner product, and yet I do not think that they would be termed canonical. So I am even more confused now, as clearly ""canonical"" is not referring to a choice of basis or inner product in this case. Or is it that all are considered canonical isomorphisms? I also found this video , I don't know how relevant it is to the above setting though. From the video it becomes clear that canonical isomorphism is a kind of equality that is true for a certain subset of statements, but not all. So the question is what is canonical referring to in the above cases. Are all canonical isomorphisms or is the only canonical isomorphism? Is canonical if there is only a single non-degenerate bilinear form given, i.e. but , and also . Is the isomorphism canonical if a single basis is specified but no more, i.e. but ? Also in the answers Mikhail Katz mentioned that in differential geometry the map may be considered non-canonical if it is not continuous, but I assume this is slightly different, as I don't need a notion of continuity to talk about the algebraic dual and double dual as far as I know. Edit 4: What I Gathered so Far There are many isomorphisms but the canonical one is given by the evaluation functional : According to coiso also the isomorphisms for can be termed canonical, but they are not the canonical isomorphism, which is instead given by the evaluation functional . As far as I understood the motivation for calling those canonical is because they do not require making a choice of basis, and we care greatly about results being coordinate invariant in linear algebra, differential geometry, physics, etc. The isomorphism where is also canonical if is a non-degenerate bilinear form (conjugate-linear in one of the slots in the complex-valued case). If is degenerate then the latter is not bijective so not an isomorphism, but I guess that restricted to the subspace where it is non-degenerate it will be a bijection and thus a canonical isomorphism on that subspace. Arturo Magidin made it clear that the isomorphism is canonical because it is between the vector space along with its inner product/bilinear form and its counterpart . As far as I understood a map where would not provide a canonical isomorphism between and by themselves, since one could choose a different bilinear form, which would result in a different identification. The isomorphism : where is a basis for and is the dual basis for such that , is not a canonical isomorphism between and in general. This is because it depend on a choice of basis , and as illustrated by my above examples a different basis can result in a different result. A special case when there is a canonical isomorphism is when and the underlying field is as given in the solution by coiso. I believe that also a one-dimensional space is canonically isomorphic to its dual (even if it is not over ) since the map and actually produce the same functionals irrespective of the choice of and (the scalar difference gets canceled out by the change of basis). Finally, supposedly also where should also be a canonical isomorphism since I am now identifying a vector space along with a basis. The latter is more restrictive than providing a non-degenerate bilinear form however.","V V^* V V^* V \mathbb{F} V^* = L(V,\mathbb{F}) v V F = (f_1,\ldots,f_n) V v = \sum_{i=1}^n [v]_F^i f_i [v]_F\in\mathbb{F}^n v\mapsto\alpha = \sum_{i=1}^n [\alpha]^{F^*}_if^i = \sum_{i=1}^n [v]^i_F f^i F^* = (f^1,\ldots,f^n) F f^i(f_j) = \delta^i_j G = (g_1,\ldots,g_n) V G^*=(g^1,\ldots,g^n) g^i(g_j) = \delta^i_j v G v = \sum_{i=1}^n [v]_G^i g_i \beta = \sum_{i=1}^n [\beta]^{G^*}_ig^i = \sum_{i=1}^n [v]^i_G g^i \alpha \beta \alpha(u) \beta(u) u\in V P g_i = \sum_{j=1}^n f_j P^j_i f^i(f_j) =\delta^i_k \implies g^i(g_j) = \delta^i_j g^i = \sum_{j=1}^n (P^{-1})^i_j f^j g^i(g_j) = \sum_{k=1}^n (P^{-1})^i_k f^k \left(\sum_{l=1}^n P^l_j f_l\right) = \sum_{k=1}^{n}\sum_{l=1}^n(P^{-1})^i_kP^l_jf^k(f_l) = \sum_{k=1}^n(P^{-1})^i_kP^k_j = \delta^i_j. [\alpha]^{G^*} = [\alpha]^{F^*}P [v]_G = P^{-1}[v]_F \alpha(v) [\alpha]^{G^*}\ne[v]_G^T=[\beta]^{G^*} \alpha\ne \beta \alpha\ne \beta \alpha f_1,\ldots,f_n \beta g_1,\ldots,g_n \alpha \beta f_1=g_1 [v]^i_F=0=[v]^i_G i>1 (e_1,\ldots,e_n) \mathbb{R}^n B:V\times V\to\mathbb{F} \gamma_v = B(v,\_) 1 (f_1,f_2) = (1,x) (g_1,g_2) = \left(\frac{x-x_0}{x_1-x_0}, \frac{x-x_1}{x_0-x_1}\right) (f^1,f^2) = \left(\delta_0, \delta_0 \circ \frac{d}{dx}\right) (g^1,g^2) = \left(\delta_{x_1},\delta_{x_0}\right) \delta_p(f) = f(p) v = a\frac{x-x_0}{x_1-x_0} + b\frac{x-x_1}{x_0-x_1} v = \frac{bx_1-ax_0}{x_1-x_0}+\frac{a-b}{x_1-x_0}x = p\cdot 1 + qx  \alpha = p\delta_0 + q\delta_0 \circ \frac{d}{dx} \beta = a\delta_{x_1}+b\delta_{x_0} \beta(v) = a^2+b^2 \alpha(v) = p^2+q^2 \alpha\ne\beta \langle v, w\rangle = \int_{0}^{1} v(t) w(t)\,dt \alpha_v = \langle v, \_\rangle V^{**} v\mapsto v^{**}(\phi) = \phi(v) V^* v\mapsto \alpha = \sum_{i=1}[\alpha]^{E^*}_ie^i = \sum_{i=1}^n[v]^i_E e_i E (V,\mathbb{F}_2) \dim = 1 V=\{0,x_1\} V^*=\{0^*,y^1\} \begin{equation}
\begin{array}{c|c}
& 0^* & y^1 \\
\hline
0 & 0 & 0 \\
\hline
x_1 & 0 & 1 
\end{array}
\end{equation} 0\ne x_1\mapsto 0^* 0\mapsto y^1 0^* = (x_1)^* = (x_1+0)^* = 0^* + y^1 = y^1 V\cong V^* (V,\mathbb{F}_2) \dim = 2 V=\{0,x_1,x_2,x_{12}=x_1+x_2\} V^* = \{0^*, y^1, y^2, y^{12}\} \begin{equation}
\begin{array}{c|c|c|c|c}
& 0^* & y^1 & y^2 & y^{12} \\
\hline
0 & 0 & 0 & 0 & 0 \\
\hline
x_1 & 0 & 1 & 0 & 1 \\
\hline
x_2 & 0 & 0 & 1 & 1 \\
\hline
x_{12} & 0 & 1 & 1 & 0
\end{array}
\end{equation} x_1,x_2 V 0\mapsto 0^* x_J\mapsto y^J x_1,x_{12} x_1\mapsto y^{12} x_{12}\mapsto y^2 x_2\mapsto y^1 I_{\lambda} : V\to V^{**} v\mapsto I_{\lambda}(V) = \lambda v^{**} I_{\lambda}(v)(\phi) = \lambda v^{**}(\phi) = \lambda \phi(v) \lambda \ne 0 I_{\lambda}(au + bv)(\phi) = \lambda \phi(au+bv) = a\lambda\phi(u)+b\lambda\phi(v) = (aI(u)+bI(v))(\phi) I_{1}(v)(\phi) = \phi(v) \lambda=1 \{I_{\lambda}\,:\,\lambda \ne 0\} I_{\lambda} I_{\lambda} I_1 v\mapsto B(v, \_) B (V,B)\stackrel{canon}{\cong} (V^*, B^*) = (V^*, B(B^{-1}, B^{-1}) V\stackrel{non-canon}{\cong} V^* (V,(B_1, B_2)) \stackrel{non-canon}{\cong} (V^*, (B^*_1, B^*_2)) (V, E) \stackrel{canon}{\cong} (V^*, E^*) (V, (E_1, E_2)) \stackrel{non-canon}{\cong} (V, (E_1^*, E_2^*)) V\to V^{**} \Lambda:V\to V^{**} \Lambda(v)(\phi) := \phi(v), \quad \forall \phi\in V^*. I_{\lambda}(v)(\phi) = \lambda\phi(v) \lambda\ne 0 \Lambda(v) J : (V,B) \to (V^*,B^*) J(v) = B(v,\_) B B J (V,B) (V^*,B^*) J_B : V\to V^* J_B(v) = B(v,\_) V V^* K_F:V\to V^* K_F(v) = \sum_{i=1}^n [v]^i_F f^i \in V^*, F = (f_1,\ldots,f_n) V F^*=(f^1,\ldots,f^n) V^* f^i(f_j) = \delta^i_j V V^* F \dim V\leq 2 \mathbb{F}_2 \mathbb{F}_2 K_F K_G F G K : (V,F) \to (V^*,F^*) K(v) = K_F(v)","['linear-algebra', 'dual-spaces', 'change-of-basis', 'vector-space-isomorphism']"
43,"$A+B$ and $AB$ are nilpotent matrices, are $A$ and $B$ nilpotent?","and  are nilpotent matrices, are  and  nilpotent?",A+B AB A B,"Suppose $A+B$ and $AB$ are nilpotent matrices, I'm thinking whether $A$ and $B$ are nilpotent. If $A$ and $B$ transform $A$ and $B$ into Jordan form at the same time, then we can just calculate the Jordan block to see that $A$ and $B$ aren't nilpotent. But the general case is $A$ and $B$ cannot be turned into Jordan form at the same time, what can we do? Any help would be appreciated!","Suppose and are nilpotent matrices, I'm thinking whether and are nilpotent. If and transform and into Jordan form at the same time, then we can just calculate the Jordan block to see that and aren't nilpotent. But the general case is and cannot be turned into Jordan form at the same time, what can we do? Any help would be appreciated!",A+B AB A B A B A B A B A B,"['linear-algebra', 'matrices']"
44,Eigenvectors of different eigenvalues are linearly independent (without matrices),Eigenvectors of different eigenvalues are linearly independent (without matrices),,"Usually the independence of eigenvalues are shown for matrices, I did a proof without considering matrices. Let $T:V\to V$ be a linear operator and let $X=$ { $v_1,v_2,...,v_m$ } be eigenvectors each corresponding to a different eigenvalue then $X$ is linearly independent. Proof: Suppose this is wrong and $X$ is linearly dependent. Then there are atleast two of which (if there were only one, that would make an eigenvector to be $0$ ) are nonzero $a_i \in F$ such that: $$ a_1v_1+a_2v_2+⋯+a_mv_m=0 \tag{*} $$ Let us pick only the vectors whose coefficients are nonzero and write: $$α_1 u_1+α_2 u_2+⋯+α_k u_k=0$$ where for each $i$ , $u_i=v_j$ for some $j$ . (We don't pick the same vector twice here). Now all $\alpha$ 's are different than $0$ in this context. Now let us multiply each side of this equation with $a_1^{-1}$ (since $a_1\neq 0$ , $a_1^{-1}\in F$ ) to get: $$ u_1+α_1^{-1}α_2u_2+⋯+α_1^{-1}α_ku_k=0 \tag{**} $$ Since each $u_i$ is an eigenvector we may write $Tu_i=\lambda_iu_i$ and we apply $T$ to the both sides of this equation to get: $$\lambda_1u_1+\lambda_2α_1^{-1}α_2u_2+⋯+\lambda_kα_1^{-1}α_ku_k=0$$ and since $\lambda_1 \neq 0$ we can multiply each side with $\lambda_1^{-1}$ to get: $$u_1+λ_1^{-1} λ_2 α_1^{-1} α_2 u_2+⋯+λ_1^{-1} λ_k.α^{-1} α_k u_k=0$$ From this equation and the $(**)$ equation we get: $$ α_1^{-1} α_2 u_2+⋯+α_1^{-1} α_k u_k=λ_1^{-1} λ_2 α_1^{-1} α_2 u_2+⋯+λ_1^{-1} λ_k.α_1^{-1} α_k u_k$$ and this equation can be multiplied by $\alpha_1$ to get: $$  α_2 u_2+⋯+α_k u_k=λ_1^{-1} λ_2 α_2 u_2+⋯+λ_1^{-1} λ_k.α_1^{-1} α_k u_k $$ which is $$ (1-λ_1^{-1} λ_2 ) α_2 u_2+⋯+(1-λ_1^{-1} λ_k ) α_k u_k=0 $$ since all the $a_i$ are nonzero, none of these coefficients can be $0$ , had it been the case we would have for some $i\neq 1$ , $1-λ_1^{-1} λ_i=0$ which is $λ_1=λ_i$ and this can't be since we picked $\lambda$ 's to be distinct eigenvalues. Now if we rename each coefficient as $(1-λ_1^{-1} λ_2 ) α_2=β_2$ , $(1-λ_1^{-1} λ_3 ) α_3=β_3$ , $\ldots$ , $(1-λ_1^{-1} λ_k ) α_k=β_k$ we get: $$  β_2 u_2+β_3 u_3+⋯+β_k u_k=0 $$ where all $\beta_i$ are nonzero. This is precisely the same case within the equation $(*)$ (except we have $k-1$ vectors now) so we may apply the same procedure to this equation to get: $$ (1-λ_2^{-1} λ_3 ) β_3 u_3+⋯+(1-λ_2^{-1} λ_k ) β_k u_k=0 $$ and for the same reasoning each coefficient is nonzero and we rename coefficients once more as $$γ_3=(1-λ_2^{-1} λ_3 ) β_3$$ and so on to get another equation but this time without $u_1$ and $u_2$ . Since we have finite vectors, we keep doing this process, after $(k-1)$ th iteration we get the equation: $$ (1-λ_{k-1}^{-1} λ_k ) A_k u_k=0 $$ and since $A_k \neq 0$ (due to the process) and $u_k \neq 0$ we must have $$ 1-λ_{k-1}^{-1} λ_k=0 $$ which leads to the contradiction $λ_{k-1}=λ_{k}$ . What do you think about this proof? Is it valid?","Usually the independence of eigenvalues are shown for matrices, I did a proof without considering matrices. Let be a linear operator and let { } be eigenvectors each corresponding to a different eigenvalue then is linearly independent. Proof: Suppose this is wrong and is linearly dependent. Then there are atleast two of which (if there were only one, that would make an eigenvector to be ) are nonzero such that: Let us pick only the vectors whose coefficients are nonzero and write: where for each , for some . (We don't pick the same vector twice here). Now all 's are different than in this context. Now let us multiply each side of this equation with (since , ) to get: Since each is an eigenvector we may write and we apply to the both sides of this equation to get: and since we can multiply each side with to get: From this equation and the equation we get: and this equation can be multiplied by to get: which is since all the are nonzero, none of these coefficients can be , had it been the case we would have for some , which is and this can't be since we picked 's to be distinct eigenvalues. Now if we rename each coefficient as , , , we get: where all are nonzero. This is precisely the same case within the equation (except we have vectors now) so we may apply the same procedure to this equation to get: and for the same reasoning each coefficient is nonzero and we rename coefficients once more as and so on to get another equation but this time without and . Since we have finite vectors, we keep doing this process, after th iteration we get the equation: and since (due to the process) and we must have which leads to the contradiction . What do you think about this proof? Is it valid?","T:V\to V X= v_1,v_2,...,v_m X X 0 a_i \in F 
a_1v_1+a_2v_2+⋯+a_mv_m=0 \tag{*}
 α_1 u_1+α_2 u_2+⋯+α_k u_k=0 i u_i=v_j j \alpha 0 a_1^{-1} a_1\neq 0 a_1^{-1}\in F 
u_1+α_1^{-1}α_2u_2+⋯+α_1^{-1}α_ku_k=0 \tag{**}
 u_i Tu_i=\lambda_iu_i T \lambda_1u_1+\lambda_2α_1^{-1}α_2u_2+⋯+\lambda_kα_1^{-1}α_ku_k=0 \lambda_1 \neq 0 \lambda_1^{-1} u_1+λ_1^{-1} λ_2 α_1^{-1} α_2 u_2+⋯+λ_1^{-1} λ_k.α^{-1} α_k u_k=0 (**)  α_1^{-1} α_2 u_2+⋯+α_1^{-1} α_k u_k=λ_1^{-1} λ_2 α_1^{-1} α_2 u_2+⋯+λ_1^{-1} λ_k.α_1^{-1} α_k u_k \alpha_1  
α_2 u_2+⋯+α_k u_k=λ_1^{-1} λ_2 α_2 u_2+⋯+λ_1^{-1} λ_k.α_1^{-1} α_k u_k
 
(1-λ_1^{-1} λ_2 ) α_2 u_2+⋯+(1-λ_1^{-1} λ_k ) α_k u_k=0
 a_i 0 i\neq 1 1-λ_1^{-1} λ_i=0 λ_1=λ_i \lambda (1-λ_1^{-1} λ_2 ) α_2=β_2 (1-λ_1^{-1} λ_3 ) α_3=β_3 \ldots (1-λ_1^{-1} λ_k ) α_k=β_k  
β_2 u_2+β_3 u_3+⋯+β_k u_k=0
 \beta_i (*) k-1 
(1-λ_2^{-1} λ_3 ) β_3 u_3+⋯+(1-λ_2^{-1} λ_k ) β_k u_k=0
 γ_3=(1-λ_2^{-1} λ_3 ) β_3 u_1 u_2 (k-1) 
(1-λ_{k-1}^{-1} λ_k ) A_k u_k=0
 A_k \neq 0 u_k \neq 0 
1-λ_{k-1}^{-1} λ_k=0  λ_{k-1}=λ_{k}","['linear-algebra', 'eigenvalues-eigenvectors', 'linear-transformations', 'linear-independence']"
45,Difficulty understanding the inverse of a homogeneous transformation matrix,Difficulty understanding the inverse of a homogeneous transformation matrix,,"I am having difficulty understand how to get from a homogeneous transformation matrix: $ M = \begin{pmatrix} R&t\\0&1 \end{pmatrix} $ Where $R$ is a rotation matrix (or it could be any linear transformation, I just chose rotation for this example) and $t$ is a translation vector. To it's inverse definition: $ M^{-1} = \begin{pmatrix} R^{-1}&-R^{-1}t\\0&1 \end{pmatrix} $ The part that I don't understand is the $-R^{-1}t$ term in the $(0,1)$ entry of $M^{-1}$ . My incorrect expectation was: $ M^{-1} = \begin{pmatrix} R^{-1}&-t\\0&1 \end{pmatrix} $ So where does the extra multiplication by $R^{-1}$ come from? Thank you! (:","I am having difficulty understand how to get from a homogeneous transformation matrix: Where is a rotation matrix (or it could be any linear transformation, I just chose rotation for this example) and is a translation vector. To it's inverse definition: The part that I don't understand is the term in the entry of . My incorrect expectation was: So where does the extra multiplication by come from? Thank you! (:","
M = \begin{pmatrix} R&t\\0&1 \end{pmatrix}
 R t 
M^{-1} = \begin{pmatrix} R^{-1}&-R^{-1}t\\0&1 \end{pmatrix}
 -R^{-1}t (0,1) M^{-1} 
M^{-1} = \begin{pmatrix} R^{-1}&-t\\0&1 \end{pmatrix}
 R^{-1}","['linear-algebra', 'matrices', 'inverse']"
46,Why two formulas for a in a linear function are equal,Why two formulas for a in a linear function are equal,,"Sorry if this is an easy question for some, but I have tried and failed for a while now, so I need someone else to help me. I want to figure out why $$a=\frac{f(x)-b}{x_1}$$ is equal to $$a=\frac{y_2-y_1}{x_2-x_1}$$ For example in a task where you know to points in a linear graph, you use the second formula to find $a$ . $$A: (2, 10)\\ B: (8, 130)$$ Then $a=20$ . But why is the flipped version of $f(x)=ax+b$ equal to the direct formula $a=\frac{y_2-y_1}{x_2-x_1}$ ? Are they even the same? What I mean is that can the two formulas be flipped/multiplied and stuff like that to be the same?","Sorry if this is an easy question for some, but I have tried and failed for a while now, so I need someone else to help me. I want to figure out why is equal to For example in a task where you know to points in a linear graph, you use the second formula to find . Then . But why is the flipped version of equal to the direct formula ? Are they even the same? What I mean is that can the two formulas be flipped/multiplied and stuff like that to be the same?","a=\frac{f(x)-b}{x_1} a=\frac{y_2-y_1}{x_2-x_1} a A: (2, 10)\\
B: (8, 130) a=20 f(x)=ax+b a=\frac{y_2-y_1}{x_2-x_1}",['linear-algebra']
47,Find $2^A$ with $A$ is a matrix,Find  with  is a matrix,2^A A,"Let $$A=\begin{bmatrix} -1 &-2 &-2 \\1&2&1  \\-1&-1&0  \end{bmatrix}$$ How to find $2^A$ ? I find out that $A^2=I$ so it would be simple if they ask me how to find a power of $A$, but not. So could you help me?","Let $$A=\begin{bmatrix} -1 &-2 &-2 \\1&2&1  \\-1&-1&0  \end{bmatrix}$$ How to find $2^A$ ? I find out that $A^2=I$ so it would be simple if they ask me how to find a power of $A$, but not. So could you help me?",,"['linear-algebra', 'matrices', 'matrix-exponential']"
48,Show that the Gram Matrix G(B) is Positive Definite,Show that the Gram Matrix G(B) is Positive Definite,,"Suppose we have $\boldsymbol P_{\leqslant 1}=\operatorname{span}\{1,x\}$ that is an inner product space with respect to $\int^1_0p(x)q(x)dx$ . Consider the basis $B=\left\{b_1 = 1, b_2 = x\right\}$ . Finding the Gram matrix $G(B)$ I would have $G(B)=\begin{bmatrix}\int^1_01dx & \int^1_0xdx\\ \int^1_0xdx & \int^1_0x^2dx \end{bmatrix} = \begin{bmatrix}1&\frac12\\\frac12&\frac13\end{bmatrix}$ I want  to show that $G(B)$ is positive definite. It is obvious that $G(B)$ is symmetric since $G_{12} = G_{21}$ . Now, to calculate the eigenvalues, I have found that $\lambda_1 = \frac{4+\sqrt{13}}{6}$ $\lambda_2 = \frac{4-\sqrt{13}}{6}$ Since $4 > \sqrt{13}$ , then $\lambda_2 > 0$ . Is this enough to show that this is positive definite? The reason that I am skeptical is because I had a homework question similar to the one above, but with a longer basis and with $\textbf{P}_{\leq 2}=\operatorname{span}\{1,x,x^2\}$ : consider the basis $B = \{b_1 = 1, b_2 = x, b_3 = x^2\}$ The Gram matrix is then $G(B)=\begin{bmatrix}1&\frac12&\frac13\\\frac12&\frac13&\frac14\\\frac13&\frac14&\frac15\end{bmatrix}$ I attempted to show that $G(B)$ is positive definite by first showing that the matrix is symmetric, and that its eigenvalues $\lambda$ are positive. However, it seems to be extremely unfeasible to find the eigenvalues of $G(B)$ . Is there another way I could go about and prove this?","Suppose we have that is an inner product space with respect to . Consider the basis . Finding the Gram matrix I would have I want  to show that is positive definite. It is obvious that is symmetric since . Now, to calculate the eigenvalues, I have found that Since , then . Is this enough to show that this is positive definite? The reason that I am skeptical is because I had a homework question similar to the one above, but with a longer basis and with : consider the basis The Gram matrix is then I attempted to show that is positive definite by first showing that the matrix is symmetric, and that its eigenvalues are positive. However, it seems to be extremely unfeasible to find the eigenvalues of . Is there another way I could go about and prove this?","\boldsymbol P_{\leqslant 1}=\operatorname{span}\{1,x\} \int^1_0p(x)q(x)dx B=\left\{b_1 = 1, b_2 = x\right\} G(B) G(B)=\begin{bmatrix}\int^1_01dx & \int^1_0xdx\\ \int^1_0xdx & \int^1_0x^2dx \end{bmatrix} = \begin{bmatrix}1&\frac12\\\frac12&\frac13\end{bmatrix} G(B) G(B) G_{12} = G_{21} \lambda_1 = \frac{4+\sqrt{13}}{6} \lambda_2 = \frac{4-\sqrt{13}}{6} 4 > \sqrt{13} \lambda_2 > 0 \textbf{P}_{\leq 2}=\operatorname{span}\{1,x,x^2\} B = \{b_1 = 1, b_2 = x, b_3 = x^2\} G(B)=\begin{bmatrix}1&\frac12&\frac13\\\frac12&\frac13&\frac14\\\frac13&\frac14&\frac15\end{bmatrix} G(B) \lambda G(B)","['linear-algebra', 'matrices', 'inner-products', 'positive-definite']"
49,Deriving the inverse of a 2x2 matrix,Deriving the inverse of a 2x2 matrix,,I am looking for a derivation for the inverse of a 2x2 matrix. I am also wondering why the determinant is involved in the expression. I am familiar with high school maths and linear algebra. If there is an intuitive reason for expression i would also be interested in that.,I am looking for a derivation for the inverse of a 2x2 matrix. I am also wondering why the determinant is involved in the expression. I am familiar with high school maths and linear algebra. If there is an intuitive reason for expression i would also be interested in that.,,"['linear-algebra', 'matrices', 'inverse']"
50,Relation between Frobenius norm and trace,Relation between Frobenius norm and trace,,"Is the following inequality true? $$\mbox{Tr} \left( \mathrm P \, \mathrm M^T \mathrm M \right) \leq \lambda_{\max}(\mathrm P) \, \|\mathrm M\|_F^2$$ where $\mathrm P$ is a positive definite matrix with appropriate dimension. How about the following? $$\mbox{Tr}(\mathrm A \mathrm B)\leq \|\mathrm A\|_F \|\mathrm B\|_F$$",Is the following inequality true? where is a positive definite matrix with appropriate dimension. How about the following?,"\mbox{Tr} \left( \mathrm P \, \mathrm M^T \mathrm M \right) \leq \lambda_{\max}(\mathrm P) \, \|\mathrm M\|_F^2 \mathrm P \mbox{Tr}(\mathrm A \mathrm B)\leq \|\mathrm A\|_F \|\mathrm B\|_F","['linear-algebra', 'matrices', 'normed-spaces', 'trace', 'matrix-norms']"
51,How to gain an intuition of the affine function's definition?,How to gain an intuition of the affine function's definition?,,"Here is the definition of Affine Functions according to Stephan Boyd (EE263 Stanford)  : 1- I believe linearity is more restrictive property of a function than being affine since it requires $f(0) = 0.$ I wonder, how is it the case that by imposing a constraint, namely $\alpha + \beta = 1$ , we achieve a less restrictive property? 2- How can we get an intuitive feeling of the concept of affinity via this definition? Thanks","Here is the definition of Affine Functions according to Stephan Boyd (EE263 Stanford)  : 1- I believe linearity is more restrictive property of a function than being affine since it requires I wonder, how is it the case that by imposing a constraint, namely , we achieve a less restrictive property? 2- How can we get an intuitive feeling of the concept of affinity via this definition? Thanks",f(0) = 0. \alpha + \beta = 1,"['linear-algebra', 'affine-geometry']"
52,Matrix sequence convergence vs. matrix power series convergence:,Matrix sequence convergence vs. matrix power series convergence:,,"Is my thinking correct? The sequence $A^n$ converges if each entry converges to a finite number. But for a matrix power series, $ I + A + \cdots + A^n + \cdots $ can never converge if it has, for example a ""1"" in the upper left corner, in entry $a_{11}$.  Take, for simplicty, $A$ to be diagonal, so that the other diagonal entry is $.5$.  This entry will eventually go to zero, but the ""$1$"" entry will accumulate to infinity. So we really need, just as in the nth-term test for series convergence of real / complex numbers, for the matrices to tend to the zero-matrix, which I am guessing is a necessary but not sufficient condition for convergence. What do you think? Thanks,","Is my thinking correct? The sequence $A^n$ converges if each entry converges to a finite number. But for a matrix power series, $ I + A + \cdots + A^n + \cdots $ can never converge if it has, for example a ""1"" in the upper left corner, in entry $a_{11}$.  Take, for simplicty, $A$ to be diagonal, so that the other diagonal entry is $.5$.  This entry will eventually go to zero, but the ""$1$"" entry will accumulate to infinity. So we really need, just as in the nth-term test for series convergence of real / complex numbers, for the matrices to tend to the zero-matrix, which I am guessing is a necessary but not sufficient condition for convergence. What do you think? Thanks,",,"['linear-algebra', 'sequences-and-series', 'matrices', 'convergence-divergence']"
53,How to find max and min of this quadratic form?,How to find max and min of this quadratic form?,,"So I'm a little confused about how to finish of this homework question and even unsure if my attempt is correct so far. . . "" Find an orthonormal basis of matrix A"" = $$\begin{pmatrix} 2 & 1 & 1 \\ 1 & 1 & 0 \\ 1 & 0 & 1\\ \end{pmatrix}$$ so I found the eigenvalues to be $\mathbb c=0, 1, 3 $ and the respective normalized eigenvectors to be $$\mathbb 1/\sqrt3 \begin{pmatrix} -1\\ 1 \\ 1\\ \end{pmatrix} \mathbb 1/\sqrt2 \begin{pmatrix} 0\\ -1 \\ 1\\ \end{pmatrix} \mathbb 1/\sqrt6 \begin{pmatrix} 2 \\ 1 \\ 1 \\ \end{pmatrix}$$ I think this ok, they seem to be orthogonal. For the quadratic form $\mathbb q(x)=(Ax, x)$ find the maximal and minimal value of $\mathbb q(x)$ on the unit spher $\mathbb S = \{x| (x,x) = 1\}$ For this I'm stumped, I know it clearly must involve what I've done and I've read questions about quadratic forms which say there is diagonalizable matrix with eigenvectors on the diagonal such that max and min are the max an min eigenvalues but how I apply that to my question I'm really unsure. Is it just the clearly can't be the eigenvectors I found and obviously not just expected to do it through partial derivatives, can anyone just how I may proceed ? How does the definition of unit sphere come into it? are the eigenvalues and vectors of A I found correct AND involved in the 2nd part?","So I'm a little confused about how to finish of this homework question and even unsure if my attempt is correct so far. . . "" Find an orthonormal basis of matrix A"" = $$\begin{pmatrix} 2 & 1 & 1 \\ 1 & 1 & 0 \\ 1 & 0 & 1\\ \end{pmatrix}$$ so I found the eigenvalues to be $\mathbb c=0, 1, 3 $ and the respective normalized eigenvectors to be $$\mathbb 1/\sqrt3 \begin{pmatrix} -1\\ 1 \\ 1\\ \end{pmatrix} \mathbb 1/\sqrt2 \begin{pmatrix} 0\\ -1 \\ 1\\ \end{pmatrix} \mathbb 1/\sqrt6 \begin{pmatrix} 2 \\ 1 \\ 1 \\ \end{pmatrix}$$ I think this ok, they seem to be orthogonal. For the quadratic form $\mathbb q(x)=(Ax, x)$ find the maximal and minimal value of $\mathbb q(x)$ on the unit spher $\mathbb S = \{x| (x,x) = 1\}$ For this I'm stumped, I know it clearly must involve what I've done and I've read questions about quadratic forms which say there is diagonalizable matrix with eigenvectors on the diagonal such that max and min are the max an min eigenvalues but how I apply that to my question I'm really unsure. Is it just the clearly can't be the eigenvectors I found and obviously not just expected to do it through partial derivatives, can anyone just how I may proceed ? How does the definition of unit sphere come into it? are the eigenvalues and vectors of A I found correct AND involved in the 2nd part?",,"['linear-algebra', 'matrices', 'optimization']"
54,What's the explicit categorical relation between a linear transformation and its matrix representation?,What's the explicit categorical relation between a linear transformation and its matrix representation?,,"There several questions about linear transformations and its respective matrices in some basis, but I'm particularly interested in the explicit definition of this relation in the category $Vect$ (of vector spaces and linear transformations.) Of course, a inevitable question is if there is always an matrix (or tensor, perhaps) representation (representable Functor?) for every morphism in an arbitrary category. Sorry if it is a silly question, but I'm learning CT by myself and it is difficult for me to understand this relation in categorical terms.","There several questions about linear transformations and its respective matrices in some basis, but I'm particularly interested in the explicit definition of this relation in the category $Vect$ (of vector spaces and linear transformations.) Of course, a inevitable question is if there is always an matrix (or tensor, perhaps) representation (representable Functor?) for every morphism in an arbitrary category. Sorry if it is a silly question, but I'm learning CT by myself and it is difficult for me to understand this relation in categorical terms.",,"['linear-algebra', 'abstract-algebra', 'category-theory']"
55,A matrix $M$ that commutes with any matrix is of the form $M=\alpha I$,A matrix  that commutes with any matrix is of the form,M M=\alpha I,I feel like this is probably a simple proof but I can't quite come up with it in an elegant way nor could I find it here. Prove that if a matrix $M$ commutes with any matrix then $M$ is of the form $M=\alpha I$. Proving the contrapositive seems like the natural way to go where we can logically transform $\lnot \forall A(MA = AM)$ into $\exists A (MA \neq AM)$ but assuming that $M \neq \alpha I$ immediately becomes messy. Is there a nice way out of this or is it inevitably going to get messy?,I feel like this is probably a simple proof but I can't quite come up with it in an elegant way nor could I find it here. Prove that if a matrix $M$ commutes with any matrix then $M$ is of the form $M=\alpha I$. Proving the contrapositive seems like the natural way to go where we can logically transform $\lnot \forall A(MA = AM)$ into $\exists A (MA \neq AM)$ but assuming that $M \neq \alpha I$ immediately becomes messy. Is there a nice way out of this or is it inevitably going to get messy?,,['linear-algebra']
56,Interchanging Rows Of Matrix Changes Sign Of Determinants!,Interchanging Rows Of Matrix Changes Sign Of Determinants!,,Now a days I am learning about matrix and determinants and I confused on one properties of determinants which is: interchanging two rows/Columns of a determinant changes the sign of the determinant. My question is what is the logic(reason) that -ve sign is places outside the determinants while interchanging rows/Columns but no sign is places outsides in gaussian elimination (OR more specific in matrix) I don't understand the logic behind this. I Google it a lot but found no answer. Can anybody please explain why we do this.,Now a days I am learning about matrix and determinants and I confused on one properties of determinants which is: interchanging two rows/Columns of a determinant changes the sign of the determinant. My question is what is the logic(reason) that -ve sign is places outside the determinants while interchanging rows/Columns but no sign is places outsides in gaussian elimination (OR more specific in matrix) I don't understand the logic behind this. I Google it a lot but found no answer. Can anybody please explain why we do this.,,"['linear-algebra', 'determinant']"
57,Can all real/complex vector spaces be equipped with a Hilbert space structure?,Can all real/complex vector spaces be equipped with a Hilbert space structure?,,"Let $X$ be a vector space over $\mathbb K \in \{\mathbb R, \mathbb C\}$.  Does there exists a pairing $X \times X \rightarrow \mathbb K$ that induces a Hilbert space structure on $X$? I have been thinking that one may choose an arbitrary basis of $X$, which exists by the axiom of choice, and declare it as the orthonormal basis of the Hilbert space, but then limits of series might become unhandy.","Let $X$ be a vector space over $\mathbb K \in \{\mathbb R, \mathbb C\}$.  Does there exists a pairing $X \times X \rightarrow \mathbb K$ that induces a Hilbert space structure on $X$? I have been thinking that one may choose an arbitrary basis of $X$, which exists by the axiom of choice, and declare it as the orthonormal basis of the Hilbert space, but then limits of series might become unhandy.",,"['linear-algebra', 'functional-analysis', 'set-theory']"
58,"Is thi set of vectors, $\{(2, 1), (3, 2), (1, 2)\}$, is linearly dependent or independent?","Is thi set of vectors, , is linearly dependent or independent?","\{(2, 1), (3, 2), (1, 2)\}","Given a set of vectors  S = $\left\{ \begin{bmatrix} 2 \\ 1 \end{bmatrix}, \begin{bmatrix} 3 \\ 2 \end{bmatrix}, \begin{bmatrix} 1 \\ 2 \end{bmatrix} \right\}  $ Find out if the vectors are linearly dependent or independent I know that for a set of vectors to be linearly dependent, they must satisfy the below equation: $$c_1v_1 + c_2v_2 ... c_nv_n = \mathbf 0 $$ such that not all $c_i$ are zero. So, I decided to apply Gauss Elimination and I got the following equation: $ c_1\begin{bmatrix} 2 \\ 1 \end{bmatrix} + c_2 \begin{bmatrix} 3 \\ 2 \end{bmatrix} + c_3 \begin{bmatrix} 1 \\2 \end{bmatrix} = \begin{bmatrix} 0 \\ 0\end{bmatrix} $ And, needless to say, I get an under-determined system of equations below: $$2c_1 + 3c_2 + c3 = 0$$ $$c_1 + 2c_2 + 2c_3 = 0$$ And after solving, I get this: $$c_1 = 4c_3$$ $$c_2 = -3c_3$$ So, $c_3$ is the free variable. Assuming $c_3$ is non zero, the vectors are linearly dependent. If it is zero, vectors are linearly dependent. How can it be that a free variable decides whether vectors are linearly dependent or not ? Shouldn't it a 100% yes or no answer that does not fluctuate depending on values of constants?","Given a set of vectors  S = $\left\{ \begin{bmatrix} 2 \\ 1 \end{bmatrix}, \begin{bmatrix} 3 \\ 2 \end{bmatrix}, \begin{bmatrix} 1 \\ 2 \end{bmatrix} \right\}  $ Find out if the vectors are linearly dependent or independent I know that for a set of vectors to be linearly dependent, they must satisfy the below equation: $$c_1v_1 + c_2v_2 ... c_nv_n = \mathbf 0 $$ such that not all $c_i$ are zero. So, I decided to apply Gauss Elimination and I got the following equation: $ c_1\begin{bmatrix} 2 \\ 1 \end{bmatrix} + c_2 \begin{bmatrix} 3 \\ 2 \end{bmatrix} + c_3 \begin{bmatrix} 1 \\2 \end{bmatrix} = \begin{bmatrix} 0 \\ 0\end{bmatrix} $ And, needless to say, I get an under-determined system of equations below: $$2c_1 + 3c_2 + c3 = 0$$ $$c_1 + 2c_2 + 2c_3 = 0$$ And after solving, I get this: $$c_1 = 4c_3$$ $$c_2 = -3c_3$$ So, $c_3$ is the free variable. Assuming $c_3$ is non zero, the vectors are linearly dependent. If it is zero, vectors are linearly dependent. How can it be that a free variable decides whether vectors are linearly dependent or not ? Shouldn't it a 100% yes or no answer that does not fluctuate depending on values of constants?",,"['linear-algebra', 'matrices']"
59,Proof that the method of Gauss/Jordan yields the inverse of a matrix,Proof that the method of Gauss/Jordan yields the inverse of a matrix,,I have trouble in solving the following exercise: let A be an invertible matrix. Consider the matrix A|I where I is the identity matrix. Prove that the matrix obtained by transforming A|I in it's row-echelon form is equal to I|B where B is the inverse matrix of A. Any help would be appreciated.,I have trouble in solving the following exercise: let A be an invertible matrix. Consider the matrix A|I where I is the identity matrix. Prove that the matrix obtained by transforming A|I in it's row-echelon form is equal to I|B where B is the inverse matrix of A. Any help would be appreciated.,,"['linear-algebra', 'matrices', 'gaussian-elimination']"
60,Adding two subspaces,Adding two subspaces,,"I have two subspaces: $$W_1 = \{(x, 3x) : x\in \Bbb R \}$$ and $$W_2 = \{(2x, 0): x\in \Bbb R \}$$ How do I get $W_1 + W_2$?  I tried simply adding a sample vector from each, i.e. $$ (1, 3) + (2, 0) = (3, 3)$$ but I don't think this makes sense since this new vector doesn't fit it $W_1$ nor $W_2$....","I have two subspaces: $$W_1 = \{(x, 3x) : x\in \Bbb R \}$$ and $$W_2 = \{(2x, 0): x\in \Bbb R \}$$ How do I get $W_1 + W_2$?  I tried simply adding a sample vector from each, i.e. $$ (1, 3) + (2, 0) = (3, 3)$$ but I don't think this makes sense since this new vector doesn't fit it $W_1$ nor $W_2$....",,"['linear-algebra', 'vector-spaces']"
61,Can $ABA-BAB=I$?,Can ?,ABA-BAB=I,"Let $A,B\in M_{n\times n} (\mathbb{C})$. Is it possible that $ABA-BAB=I$? I came across this interesting problem as I was studying for an exam.  I guess in the case when $A$ and $B$ commute we have $A(A-B)B=I$ and I am not sure if it can happen. Any ideas?","Let $A,B\in M_{n\times n} (\mathbb{C})$. Is it possible that $ABA-BAB=I$? I came across this interesting problem as I was studying for an exam.  I guess in the case when $A$ and $B$ commute we have $A(A-B)B=I$ and I am not sure if it can happen. Any ideas?",,['linear-algebra']
62,Why aren't all real self-adjoint operators diagonal?,Why aren't all real self-adjoint operators diagonal?,,"I'm experiencing some confusion regarding self-adjoint operators.  As background for my question, I give the following 3 results (all from Linear Algebra, 3rd ed. by Friedberg, Insel, and Spence): Theorem 6.17: ""Let $T$ be a linear operator on a finite-dimensional real inner product space $V$.  Then $T$ is self-adjoint if and only if there exists an orthonormal basis $\beta$ consisting of eigenvectors of $T$."" Theorem 6.14 (Schur's Theorem): ""Let $T$ be a linear operator on a finite-dimensional inner product space $V$.  Suppose that the characteristic polynomial of $T$ splits.  Then there exists an orthonormal basis $\beta$ for $V$ such that the matrix $[T]_\beta$ is upper triangular."" Lemma: ""Let $T$ be a self-adjoint operator on a finite-dimensional inner product space $V$.  Then (a) Every eigenvalue of $T$ is real.  (b) Suppose that $V$ is a real inner product space.  Then the characteristic polynomial of $T$ splits."" From these results I want to say this: Let $T$ be a self-adjoint linear operator on a finite-dimensional real vector space $V$.  Then by the lemma, the characteristic polynomial splits, and by Schur's theorem there exists an orthonormal basis $\beta$ for $V$ so that $[T]_\beta$ is upper triangular.  Let $A=[T]_\beta$.  Then since $T$ is self-adjoint we have $A^*=[T]_\beta^*=[T^*]_\beta=[T]_\beta=A$ (this comes from the proof of Theorem 6.17) so that $A$ must be a diagonal matrix.  So this seems to imply that every self-adjoint linear operator on a finite-dimensional real vector space is diagonal, but this is not the case since the self-adjoint matrices clearly include any real symmetric matrix.  What am I not seeing?","I'm experiencing some confusion regarding self-adjoint operators.  As background for my question, I give the following 3 results (all from Linear Algebra, 3rd ed. by Friedberg, Insel, and Spence): Theorem 6.17: ""Let $T$ be a linear operator on a finite-dimensional real inner product space $V$.  Then $T$ is self-adjoint if and only if there exists an orthonormal basis $\beta$ consisting of eigenvectors of $T$."" Theorem 6.14 (Schur's Theorem): ""Let $T$ be a linear operator on a finite-dimensional inner product space $V$.  Suppose that the characteristic polynomial of $T$ splits.  Then there exists an orthonormal basis $\beta$ for $V$ such that the matrix $[T]_\beta$ is upper triangular."" Lemma: ""Let $T$ be a self-adjoint operator on a finite-dimensional inner product space $V$.  Then (a) Every eigenvalue of $T$ is real.  (b) Suppose that $V$ is a real inner product space.  Then the characteristic polynomial of $T$ splits."" From these results I want to say this: Let $T$ be a self-adjoint linear operator on a finite-dimensional real vector space $V$.  Then by the lemma, the characteristic polynomial splits, and by Schur's theorem there exists an orthonormal basis $\beta$ for $V$ so that $[T]_\beta$ is upper triangular.  Let $A=[T]_\beta$.  Then since $T$ is self-adjoint we have $A^*=[T]_\beta^*=[T^*]_\beta=[T]_\beta=A$ (this comes from the proof of Theorem 6.17) so that $A$ must be a diagonal matrix.  So this seems to imply that every self-adjoint linear operator on a finite-dimensional real vector space is diagonal, but this is not the case since the self-adjoint matrices clearly include any real symmetric matrix.  What am I not seeing?",,['linear-algebra']
63,"Is there a polynomial of degree at most 99 whose values at 1, 2,... , 1000 alternate between 0 and 1?","Is there a polynomial of degree at most 99 whose values at 1, 2,... , 1000 alternate between 0 and 1?",,"This is a question from Problem set 2 of MIT’s 18.700 Linear algebra course . We have the following setup. $V$ is a vector space of real polynomials with degree at most $99$ . Also, $T:V\to \mathbb{R}^{1000}$ is a linear map such that $$ T(p) = (p(1), \dots, p(1000)) $$ Part (a) asks you what $\dim\ker T$ is. I thought the only possible polynomial $p$ with $\deg p \leq 99$ such that $p(1)=\dots=p(1000)=0$ is the zero polynomial (i.e. $p=0$ ). The reason is because the factor theorem tells you if $p$ is a polynomial for which $p(1)=\dots=p(1000)$ , it must have $(x-1), \dots, (x-1000)$ as its factors. So, $\deg p \geq1000$ , which is contradictory to the assumption that $\deg p \leq 99$ . Therefore, $\dim\ker T = 0$ . Part (b) asks you what $\dim \text{range }T$ is. I used the theorem $\dim V = \dim\ker T + \dim \text{range }T$ for linear maps, and concluded $\dim \text{range }T = 100$ ( $\because \dim V = 100)$ . Part (c) is where I got stuck. It asks whether the vector $(0,1,0,1,\dots,0,1)$ is in the range of $T$ . I have one solution in mind, but it seems like it’s completely irrelevant to linear algebra, which makes me think this is not the solution that the instructor intended. My argument went similarly as in part (a). If there is a polynomial $p$ with $\deg p \leq 99$ such that $T(p) = (0, 1, 0, 1, \dots, 0, 1)$ , it means that $p$ has at least $500$ roots ( $x\in\{1, 3, 5, \dots, 999\}$ ), which implies $\deg p \geq 500$ (by the factor theorem), which again is a contradiction. I think my question can be phrased as: Can you deduce $(0,1,0,1,\dots,0,1)\notin \text{range }T$ just from the fact that (without referring to the factor theorem) $T:V\to \mathbb{R}^{1000}$ is a linear map $\dim V = 100$ $\dim \ker T = 0$ $\dim \text{range } T = 100$ I think there should be some more linear algebra-style of proof. I’d appreciate any comment or help! Thank you.","This is a question from Problem set 2 of MIT’s 18.700 Linear algebra course . We have the following setup. is a vector space of real polynomials with degree at most . Also, is a linear map such that Part (a) asks you what is. I thought the only possible polynomial with such that is the zero polynomial (i.e. ). The reason is because the factor theorem tells you if is a polynomial for which , it must have as its factors. So, , which is contradictory to the assumption that . Therefore, . Part (b) asks you what is. I used the theorem for linear maps, and concluded ( . Part (c) is where I got stuck. It asks whether the vector is in the range of . I have one solution in mind, but it seems like it’s completely irrelevant to linear algebra, which makes me think this is not the solution that the instructor intended. My argument went similarly as in part (a). If there is a polynomial with such that , it means that has at least roots ( ), which implies (by the factor theorem), which again is a contradiction. I think my question can be phrased as: Can you deduce just from the fact that (without referring to the factor theorem) is a linear map I think there should be some more linear algebra-style of proof. I’d appreciate any comment or help! Thank you.","V 99 T:V\to \mathbb{R}^{1000}  T(p) = (p(1), \dots, p(1000))  \dim\ker T p \deg p \leq 99 p(1)=\dots=p(1000)=0 p=0 p p(1)=\dots=p(1000) (x-1), \dots, (x-1000) \deg p \geq1000 \deg p \leq 99 \dim\ker T = 0 \dim \text{range }T \dim V = \dim\ker T + \dim \text{range }T \dim \text{range }T = 100 \because \dim V = 100) (0,1,0,1,\dots,0,1) T p \deg p \leq 99 T(p) = (0, 1, 0, 1, \dots, 0, 1) p 500 x\in\{1, 3, 5, \dots, 999\} \deg p \geq 500 (0,1,0,1,\dots,0,1)\notin \text{range }T T:V\to \mathbb{R}^{1000} \dim V = 100 \dim \ker T = 0 \dim \text{range } T = 100","['linear-algebra', 'polynomials', 'solution-verification', 'vector-spaces', 'linear-transformations']"
64,"Let $T\colon \mathbb{R}^2 \to \mathbb{R}^2$ be linear transformation. Show that there are $a,b\in \mathbb{R}$ such that $T^2+aT+bI=0$.",Let  be linear transformation. Show that there are  such that .,"T\colon \mathbb{R}^2 \to \mathbb{R}^2 a,b\in \mathbb{R} T^2+aT+bI=0","This is a problem from the book Linear Algebra by Larry Smith and the author has so far introduced Vector Spaces. This problem shows up in the introductory chapter of Linear Transformations. Let $T\colon \mathbb{R}^2 \to \mathbb{R}^2$ be linear transformation.   Show that there are $a,b\in \mathbb{R}$ such that $T^2+aT+bI=0$ . I think I have worked out the proof but I was looking for a simpler way. Here's how I did it: I defined $T\colon \mathbb{R}^2 \to \mathbb{R}^2$ by $T(x,y)=(t_1 (x,y) , t_2 (x,y))$ where $t_1$ and $t_2$ are functions from $\mathbb{R}^2$ to $\mathbb{R}$ . I showed that $T$ is a linear transformation iff $t_1$ and $t_2$ are linear transformations. Since every linear transformation from $\mathbb{R}^2$ to $\mathbb{R}$ is of the form $ax+by$ for all $(x,y)\in \mathbb{R}^2$ , I completed it by comparing the components. Is there any far far better way of doing this?","This is a problem from the book Linear Algebra by Larry Smith and the author has so far introduced Vector Spaces. This problem shows up in the introductory chapter of Linear Transformations. Let be linear transformation.   Show that there are such that . I think I have worked out the proof but I was looking for a simpler way. Here's how I did it: I defined by where and are functions from to . I showed that is a linear transformation iff and are linear transformations. Since every linear transformation from to is of the form for all , I completed it by comparing the components. Is there any far far better way of doing this?","T\colon \mathbb{R}^2 \to \mathbb{R}^2 a,b\in \mathbb{R} T^2+aT+bI=0 T\colon \mathbb{R}^2 \to \mathbb{R}^2 T(x,y)=(t_1 (x,y) , t_2 (x,y)) t_1 t_2 \mathbb{R}^2 \mathbb{R} T t_1 t_2 \mathbb{R}^2 \mathbb{R} ax+by (x,y)\in \mathbb{R}^2","['linear-algebra', 'linear-transformations']"
65,Finding a certain entry in a matrix,Finding a certain entry in a matrix,,Can we find the entry $s_{23}$ of $S=H^3$ where $H=\pmatrix{2&-1&0\\3&1&2\\-1&1&1} $ without finding $S$. I know that $s_{23}$ is given by multiplying the second row of $H^2$ and the third coloumn of $H$ but it is dull. Thank you for any hints!,Can we find the entry $s_{23}$ of $S=H^3$ where $H=\pmatrix{2&-1&0\\3&1&2\\-1&1&1} $ without finding $S$. I know that $s_{23}$ is given by multiplying the second row of $H^2$ and the third coloumn of $H$ but it is dull. Thank you for any hints!,,['linear-algebra']
66,Find all matrices that commute with $A$,Find all matrices that commute with,A,Given $$A = \begin{bmatrix} 3 & 1 &0 \\  0 &3  & 1\\  0 &0  & 3 \end{bmatrix}$$ find matrices $B$ such that $AB=BA$. Trivially $B=A^{-1}$ and $B=kI$ are the solutions Also we have Characteristic Polynomial as $$A^3-9A^2+27A-27I=0$$ $\implies$ $$(A-3I)^3=0$$ Is it possible to find other $B's$ using above Nilpotency of $A-3I$?,Given $$A = \begin{bmatrix} 3 & 1 &0 \\  0 &3  & 1\\  0 &0  & 3 \end{bmatrix}$$ find matrices $B$ such that $AB=BA$. Trivially $B=A^{-1}$ and $B=kI$ are the solutions Also we have Characteristic Polynomial as $$A^3-9A^2+27A-27I=0$$ $\implies$ $$(A-3I)^3=0$$ Is it possible to find other $B's$ using above Nilpotency of $A-3I$?,,"['linear-algebra', 'matrices', 'polynomials', 'determinant', 'matrix-equations']"
67,Find a real matrix $B$ such that $B^3 = A$,Find a real matrix  such that,B B^3 = A,"Given $$A = \begin{bmatrix}-5 & 3\\6 & -2\end{bmatrix}$$ find a real, invertible matrix $B$ such that $B^3 = A$ I think I am doing something wrong here, so let me describe my attempt: 1) So I started off with diagonalizing the matrix $A$ with finding the eigenvalues $\lambda_1 = -8$ and $\lambda_2 = 1$ and the corresponding eigenvectors $ \vec v_1 = \begin{bmatrix}1 & 1\\0 & 0\end{bmatrix} = x + y = 0 \Rightarrow -x = y \Rightarrow \begin{bmatrix}1\\-1\end{bmatrix}$ and $ \vec v_2 = \begin{bmatrix}1 & -\frac{1}{2}\\0 & 0\end{bmatrix} = x - \frac{1}{2}y = 0 \Rightarrow 2x = y \Rightarrow \begin{bmatrix}1\\2\end{bmatrix}$ 2) With that being done I proceeded with computing $D = \begin{bmatrix}-8 & 0\\0 & 1\end{bmatrix}$ and $P = \begin{bmatrix}1 & 1\\-1 & 2\end{bmatrix}$ and check everything with $D = PAP^{-1}$ 3) Now I thought I will simple find a diagonal matrix $M = PBP^{-1}$ and $M^3 = D$ and the easiest solution I came up with was $M = \begin{bmatrix}\sqrt[3]{-8} & 0\\0& 1\end{bmatrix}$ so basically $M = D^{\frac{1}{3}}.$ So that $B = PMP^{-1}$. But now come the tricky part, if I compute $B$ it results in a complex matrix not a real. //It is real! Have I perhaps overlooked something here or miscalculated the solution for $B$? Edit : As Cameron pointed out my calculator and I totally failed as it was in complex mode and computed one of the non-real cube roots instead of -2. So $M = \begin{bmatrix}-2 & 0\\0 & 1\end{bmatrix}$ and consequentially $B = \begin{bmatrix}-1 & 1\\2 & 0\end{bmatrix}$","Given $$A = \begin{bmatrix}-5 & 3\\6 & -2\end{bmatrix}$$ find a real, invertible matrix $B$ such that $B^3 = A$ I think I am doing something wrong here, so let me describe my attempt: 1) So I started off with diagonalizing the matrix $A$ with finding the eigenvalues $\lambda_1 = -8$ and $\lambda_2 = 1$ and the corresponding eigenvectors $ \vec v_1 = \begin{bmatrix}1 & 1\\0 & 0\end{bmatrix} = x + y = 0 \Rightarrow -x = y \Rightarrow \begin{bmatrix}1\\-1\end{bmatrix}$ and $ \vec v_2 = \begin{bmatrix}1 & -\frac{1}{2}\\0 & 0\end{bmatrix} = x - \frac{1}{2}y = 0 \Rightarrow 2x = y \Rightarrow \begin{bmatrix}1\\2\end{bmatrix}$ 2) With that being done I proceeded with computing $D = \begin{bmatrix}-8 & 0\\0 & 1\end{bmatrix}$ and $P = \begin{bmatrix}1 & 1\\-1 & 2\end{bmatrix}$ and check everything with $D = PAP^{-1}$ 3) Now I thought I will simple find a diagonal matrix $M = PBP^{-1}$ and $M^3 = D$ and the easiest solution I came up with was $M = \begin{bmatrix}\sqrt[3]{-8} & 0\\0& 1\end{bmatrix}$ so basically $M = D^{\frac{1}{3}}.$ So that $B = PMP^{-1}$. But now come the tricky part, if I compute $B$ it results in a complex matrix not a real. //It is real! Have I perhaps overlooked something here or miscalculated the solution for $B$? Edit : As Cameron pointed out my calculator and I totally failed as it was in complex mode and computed one of the non-real cube roots instead of -2. So $M = \begin{bmatrix}-2 & 0\\0 & 1\end{bmatrix}$ and consequentially $B = \begin{bmatrix}-1 & 1\\2 & 0\end{bmatrix}$",,"['linear-algebra', 'matrices', 'matrix-equations']"
68,Geometric Interpretation of Eigendecomposition,Geometric Interpretation of Eigendecomposition,,"If we have an orthogonal matrix $U$ , then $U^Tx$ is essentially a rotation of the vector $x$ . If we have a diagonal matrix $\Lambda$ , then $\Lambda x$ is scaling the vector $x$ in each direction by the corresponding diagonal value. Since any symmetric matrix $S=U\Lambda U^T$ , $Sx=U\Lambda U^Tx$ which is rotation of $x$ by some angle $\vartheta$ , scaling it by $\Lambda$ and then re-rotating by angle $-\vartheta$ . Does this imply that multiplying by any symmetric matrix $S$ is just a scaling by its eigenvalues since the net rotation is $0$ ?","If we have an orthogonal matrix , then is essentially a rotation of the vector . If we have a diagonal matrix , then is scaling the vector in each direction by the corresponding diagonal value. Since any symmetric matrix , which is rotation of by some angle , scaling it by and then re-rotating by angle . Does this imply that multiplying by any symmetric matrix is just a scaling by its eigenvalues since the net rotation is ?",U U^Tx x \Lambda \Lambda x x S=U\Lambda U^T Sx=U\Lambda U^Tx x \vartheta \Lambda -\vartheta S 0,"['linear-algebra', 'vector-spaces', 'eigenvalues-eigenvectors', 'topological-vector-spaces']"
69,What does it mean for a ring to have an involution? Are there any examples?,What does it mean for a ring to have an involution? Are there any examples?,,"A ring $R$ is said to be a ring with an involution if there exists a mapping  $*\colon R  \to  R$ such that for every $a, b \in R$: $a^{**} = a$, $(a + b)^* = b^* +  a^*$, $(ab)^* = b^*a^*$. Can anyone please explain this definition with an example?","A ring $R$ is said to be a ring with an involution if there exists a mapping  $*\colon R  \to  R$ such that for every $a, b \in R$: $a^{**} = a$, $(a + b)^* = b^* +  a^*$, $(ab)^* = b^*a^*$. Can anyone please explain this definition with an example?",,"['linear-algebra', 'ring-theory', 'commutative-algebra', 'definition', 'involutions']"
70,Characterization of the matrix inverse,Characterization of the matrix inverse,,Suppose that $A$ is an invertible matrix and let $X$ be such that $AX+XA=2I$. Does this imply that $X=A^{-1}$? I have tried simple algebra manipulations but I have not been able to conclude. For a simple example of 2x2 matrices I found it was true.,Suppose that $A$ is an invertible matrix and let $X$ be such that $AX+XA=2I$. Does this imply that $X=A^{-1}$? I have tried simple algebra manipulations but I have not been able to conclude. For a simple example of 2x2 matrices I found it was true.,,"['linear-algebra', 'matrices']"
71,Showing Bernstein polynomial is a basis,Showing Bernstein polynomial is a basis,,"Hello I want to show that the Bernstein polynomial $$B_{n,k}=\binom{n}{k}x^k(1-x)^{n-k}\,$$ is a basis. For linear independence I got a hint from my teacher to expand the binom $(1-x)^{n-k}$ This way I get: $$B_{n,k}=\binom{n}{k}x^k\sum_{j=0}^{n-k}\binom{n-k}{j}(-1)^jx^j$$ And changing the index of summation gives: $$B_{n,k}=\sum_{j=k}^{n}\binom{n-k}{j-k}\binom{n}{k}(-1)^{j-k}x^{j-k+k}=\sum_{j=k}^{n}\binom{n}{j}\binom{j}{k}(-1)^{j-k}x^j$$ Now I have to show  that $\alpha_i$ are $0$ in the relation $\sum_{i=0}^{n}\alpha_iB_{i,n}=0\,$ or $$\alpha_0\sum_{j=0}^n(-1)^j\binom{n}{j}\binom{j}{0}x^j+\alpha_1\sum_{j=1}^n(-1)^{j-1}\binom{n}{j}\binom{j}{1}x^j+...+\alpha_{n}\sum_{j=n}^n(-1)^{j-n}\binom{n}{j}\binom{j}{n}x^j=0$$ Now what can I  do and how  can I finish this problem? Thanks in advance!",Hello I want to show that the Bernstein polynomial is a basis. For linear independence I got a hint from my teacher to expand the binom This way I get: And changing the index of summation gives: Now I have to show  that are in the relation or Now what can I  do and how  can I finish this problem? Thanks in advance!,"B_{n,k}=\binom{n}{k}x^k(1-x)^{n-k}\, (1-x)^{n-k} B_{n,k}=\binom{n}{k}x^k\sum_{j=0}^{n-k}\binom{n-k}{j}(-1)^jx^j B_{n,k}=\sum_{j=k}^{n}\binom{n-k}{j-k}\binom{n}{k}(-1)^{j-k}x^{j-k+k}=\sum_{j=k}^{n}\binom{n}{j}\binom{j}{k}(-1)^{j-k}x^j \alpha_i 0 \sum_{i=0}^{n}\alpha_iB_{i,n}=0\, \alpha_0\sum_{j=0}^n(-1)^j\binom{n}{j}\binom{j}{0}x^j+\alpha_1\sum_{j=1}^n(-1)^{j-1}\binom{n}{j}\binom{j}{1}x^j+...+\alpha_{n}\sum_{j=n}^n(-1)^{j-n}\binom{n}{j}\binom{j}{n}x^j=0","['linear-algebra', 'polynomials', 'hamel-basis']"
72,"Image of a basis forms a basis, if and only if matrix is invertible","Image of a basis forms a basis, if and only if matrix is invertible",,"Suppose $B_1=\{v_1,v_2,...,v_n\}$ is a basis of $\mathbb{R}^n$, and $M$ is an $n*n$ matrix. Prove that $B_2=\{Mv_1,Mv_2,...,Mv_n\}$ is also a basis of $\mathbb{R}^n$ if and only if $M$ is invertible. Following is what I have so far: Assume $B_2$ is basis of $\mathbb{R}^n$. Then, $B_2$ is a set of linearly independent vectors, and $B_2$ spans $\mathbb{R}^n$. Since $B_1$ is also a basis of $\mathbb{R}^n$, then any element(vector) of $B_2$ is a linear combination of elements(vectors) of $B_1$ and vice-versa. $Mv_1= a_{11}v_1+a_{21}v_2+...+a_{n1}v_n$ , where $a_{11},a_{21},...,a_{n1}\in \mathbb{R}$ Likewise, $Mv_2= a_{12}v_1+a_{22}v_2+...+a_{n2}v_n$ , where $a_{12},a_{22},...,a_{n2}\in \mathbb{R}$ $\begin{bmatrix}Mv_1&Mv_2&...&Mv_n\end{bmatrix}=\begin{bmatrix}v_1&v_2&...&v_n\end{bmatrix}\begin{bmatrix}a_{11}&a_{12}&...&a_{1n}\\a_{21}&a_{22}&...&a_{2n}\\ \vdots&\vdots&\vdots&\vdots\\a_{n1}&a_{n2}&...&a_{nn}\end{bmatrix}$ Not sure what to do next ...","Suppose $B_1=\{v_1,v_2,...,v_n\}$ is a basis of $\mathbb{R}^n$, and $M$ is an $n*n$ matrix. Prove that $B_2=\{Mv_1,Mv_2,...,Mv_n\}$ is also a basis of $\mathbb{R}^n$ if and only if $M$ is invertible. Following is what I have so far: Assume $B_2$ is basis of $\mathbb{R}^n$. Then, $B_2$ is a set of linearly independent vectors, and $B_2$ spans $\mathbb{R}^n$. Since $B_1$ is also a basis of $\mathbb{R}^n$, then any element(vector) of $B_2$ is a linear combination of elements(vectors) of $B_1$ and vice-versa. $Mv_1= a_{11}v_1+a_{21}v_2+...+a_{n1}v_n$ , where $a_{11},a_{21},...,a_{n1}\in \mathbb{R}$ Likewise, $Mv_2= a_{12}v_1+a_{22}v_2+...+a_{n2}v_n$ , where $a_{12},a_{22},...,a_{n2}\in \mathbb{R}$ $\begin{bmatrix}Mv_1&Mv_2&...&Mv_n\end{bmatrix}=\begin{bmatrix}v_1&v_2&...&v_n\end{bmatrix}\begin{bmatrix}a_{11}&a_{12}&...&a_{1n}\\a_{21}&a_{22}&...&a_{2n}\\ \vdots&\vdots&\vdots&\vdots\\a_{n1}&a_{n2}&...&a_{nn}\end{bmatrix}$ Not sure what to do next ...",,"['linear-algebra', 'matrices', 'proof-writing', 'hamel-basis']"
73,Uniqueness of characteristic polynomial of linear transformation in finite fields,Uniqueness of characteristic polynomial of linear transformation in finite fields,,"Let $T : V \to V$ be a linear transformation of the $F$-vector space $V$. Then using the (abstract) determinant function $\det : \operatorname{Hom}(V, V) \to K$ we can define a function $$   \lambda \mapsto \det(\lambda \cdot \operatorname{id} - T) $$ from $F$ to $F$. Now if we represent $T$ by a matrix $A$ we then have $\det(\lambda \cdot \operatorname{id} - T) = \det(\lambda \cdot I - A)$ where on the RHS the determinant function is an expression in the entries of the matrix. Now if we change the basis, i.e. represent $T$ by a different matrix $B = S^{-1}AS$, then a simple calculation shows that $\det(\lambda \cdot I - B) = \det(S^{-1}(\lambda I - A)S) = \det(\lambda \cdot I - A)$ and hence the value of of the matrix expression does not depend on the basis. But this is often used as a justification that the characteristic polynomial (i.e. the polynomial $\det(\lambda \cdot I - A)$) is independent of the of the choosen basis. But all I can derive from the above arguments is that the values of the determinant are the same, i.e. if $p(x) := \det(x\cdot I - A)$ and $q(x) := \det(x \cdot I - A)$, then $q(x) = p(x)$ for all $x \in F$ if $B = S^{-1}AS$. If $F$ is infinite, this gives that the polynomials are equal (i.e. have the same sequence of coefficients, and hence the coefficients represent also invariants of the transformation). But the equality that for $p(x) = a_n x^n + \ldots + a_1 x + a_0$, $q(x) = b_m x^m + \ldots + b_1 x + b_0$ we have $$  p(x) = q(x) \quad \mbox{ iff } \quad m = n, ~ a_i = b_i, ~ i = 0,\ldots, n $$ does not need to hold in finite fields, for example $p(x) = x^2 + x$ and $q(x) = 0$ in $\mathbb Z/2\mathbb Z$. So then, is the characteristic polynomial (as a formal polynomial, i.e. determined by its coefficients) still unique in the case of finite fields? And if not, do you know an example?","Let $T : V \to V$ be a linear transformation of the $F$-vector space $V$. Then using the (abstract) determinant function $\det : \operatorname{Hom}(V, V) \to K$ we can define a function $$   \lambda \mapsto \det(\lambda \cdot \operatorname{id} - T) $$ from $F$ to $F$. Now if we represent $T$ by a matrix $A$ we then have $\det(\lambda \cdot \operatorname{id} - T) = \det(\lambda \cdot I - A)$ where on the RHS the determinant function is an expression in the entries of the matrix. Now if we change the basis, i.e. represent $T$ by a different matrix $B = S^{-1}AS$, then a simple calculation shows that $\det(\lambda \cdot I - B) = \det(S^{-1}(\lambda I - A)S) = \det(\lambda \cdot I - A)$ and hence the value of of the matrix expression does not depend on the basis. But this is often used as a justification that the characteristic polynomial (i.e. the polynomial $\det(\lambda \cdot I - A)$) is independent of the of the choosen basis. But all I can derive from the above arguments is that the values of the determinant are the same, i.e. if $p(x) := \det(x\cdot I - A)$ and $q(x) := \det(x \cdot I - A)$, then $q(x) = p(x)$ for all $x \in F$ if $B = S^{-1}AS$. If $F$ is infinite, this gives that the polynomials are equal (i.e. have the same sequence of coefficients, and hence the coefficients represent also invariants of the transformation). But the equality that for $p(x) = a_n x^n + \ldots + a_1 x + a_0$, $q(x) = b_m x^m + \ldots + b_1 x + b_0$ we have $$  p(x) = q(x) \quad \mbox{ iff } \quad m = n, ~ a_i = b_i, ~ i = 0,\ldots, n $$ does not need to hold in finite fields, for example $p(x) = x^2 + x$ and $q(x) = 0$ in $\mathbb Z/2\mathbb Z$. So then, is the characteristic polynomial (as a formal polynomial, i.e. determined by its coefficients) still unique in the case of finite fields? And if not, do you know an example?",,"['linear-algebra', 'abstract-algebra', 'matrices', 'finite-fields']"
74,How to prove that $\lVert x + y\rVert = \lVert x\rVert + \lVert y \rVert \implies \lVert tx + (1-t)y\rVert = t\lVert x\rVert + (1-t)\lVert y \rVert$?,How to prove that ?,\lVert x + y\rVert = \lVert x\rVert + \lVert y \rVert \implies \lVert tx + (1-t)y\rVert = t\lVert x\rVert + (1-t)\lVert y \rVert,"If a norm on a vector space is given by an inner product then we have $\lVert x + y\rVert = \lVert x \rVert + \lVert y \rVert$ only when $x=\lambda y$ for some constant $\lambda$ (this follows from the Cauchy-Schwarz inequality). This does not hold for general (unitary invariant) norms. E.g. when considering generalised Ky Fan norms: $$ \lVert x\rVert_s = \sum_i s_i \lvert x\rvert_i^\downarrow$$ where $s_i\geq 0$ and $\lvert x\rvert^\downarrow$ is $\lvert x\rvert$ rearranged with the components from biggest to smallest. So for example the operator (sup) norm has $s_1 = 1$ and $s_i=0$ for $i>1$, while the trace norm has $s_i=1$ for all $i$. For these norms it is in general possible to find $x$ and $y$ so that the norm is additive for these vectors, while $x$ and $y$ are not multiples of each other. Now my question is the following. Given a unitarily invariant norm and vectors $x$ and $y$ such that $\lVert x + y\rVert = \lVert x\rVert + \lVert y \rVert$, can we then conclude that $\lVert tx + (1-t)y\rVert = t\lVert x\rVert + (1-t)\lVert y \rVert$ for $0\leq t \leq 1$? This is true for these generalised Ky Fan norms and for all norms that come from inner products. It is also true for all $p$-norms as this additivity still only holds for $x$ and $y$ that are multiples of each other (even though $p$-norms don't come from an inner product). My intuition behind this is that this additivity property (when $x$ and $y$ aren't multiples of each other) forces the norm to be ""linear"" in a certain way, such that the norm has to be very similar to a generalised Ky Fan norm. So as a bonus question: If there are $x$ and $y$ that aren't multiples of each other, such that the norm is additive for these $x$ and $y$, show that the norm is a generalised Ky Fan norm. Note that this can only be true when we assume the norm to be unitary invariant since otherwise we can construct a counter-example of this (take $(X_1,\lVert\cdot\rVert_1)\oplus (X_2,\lVert\cdot\rvert_2)$ with the norm $\sup\{\lVert\cdot\rVert_1,\lVert\cdot\rVert_2\}$ where $\lVert\cdot\rVert_1$ is a Ky Fan norm and $\lVert\cdot\rVert_2$ is some other norm).","If a norm on a vector space is given by an inner product then we have $\lVert x + y\rVert = \lVert x \rVert + \lVert y \rVert$ only when $x=\lambda y$ for some constant $\lambda$ (this follows from the Cauchy-Schwarz inequality). This does not hold for general (unitary invariant) norms. E.g. when considering generalised Ky Fan norms: $$ \lVert x\rVert_s = \sum_i s_i \lvert x\rvert_i^\downarrow$$ where $s_i\geq 0$ and $\lvert x\rvert^\downarrow$ is $\lvert x\rvert$ rearranged with the components from biggest to smallest. So for example the operator (sup) norm has $s_1 = 1$ and $s_i=0$ for $i>1$, while the trace norm has $s_i=1$ for all $i$. For these norms it is in general possible to find $x$ and $y$ so that the norm is additive for these vectors, while $x$ and $y$ are not multiples of each other. Now my question is the following. Given a unitarily invariant norm and vectors $x$ and $y$ such that $\lVert x + y\rVert = \lVert x\rVert + \lVert y \rVert$, can we then conclude that $\lVert tx + (1-t)y\rVert = t\lVert x\rVert + (1-t)\lVert y \rVert$ for $0\leq t \leq 1$? This is true for these generalised Ky Fan norms and for all norms that come from inner products. It is also true for all $p$-norms as this additivity still only holds for $x$ and $y$ that are multiples of each other (even though $p$-norms don't come from an inner product). My intuition behind this is that this additivity property (when $x$ and $y$ aren't multiples of each other) forces the norm to be ""linear"" in a certain way, such that the norm has to be very similar to a generalised Ky Fan norm. So as a bonus question: If there are $x$ and $y$ that aren't multiples of each other, such that the norm is additive for these $x$ and $y$, show that the norm is a generalised Ky Fan norm. Note that this can only be true when we assume the norm to be unitary invariant since otherwise we can construct a counter-example of this (take $(X_1,\lVert\cdot\rVert_1)\oplus (X_2,\lVert\cdot\rvert_2)$ with the norm $\sup\{\lVert\cdot\rVert_1,\lVert\cdot\rVert_2\}$ where $\lVert\cdot\rVert_1$ is a Ky Fan norm and $\lVert\cdot\rVert_2$ is some other norm).",,"['linear-algebra', 'normed-spaces']"
75,"$AB − BA = A$, $A$ is a nonzero matrix then $B$ is not nilpotent.",",  is a nonzero matrix then  is not nilpotent.",AB − BA = A A B,"Let $A,B$ $n\times n$ matrices such that $$AB-BA=A$$ and $A$ is a nonzero matrix. Prove that $B$ is not nilpotent. I know why $A$ is nilpotent, but how can I prove $B$ is not nilpotent?","Let $A,B$ $n\times n$ matrices such that $$AB-BA=A$$ and $A$ is a nonzero matrix. Prove that $B$ is not nilpotent. I know why $A$ is nilpotent, but how can I prove $B$ is not nilpotent?",,"['linear-algebra', 'matrices', 'matrix-equations']"
76,"Finding the dimension of $S = \{B \in M_n \,|\, AB = BA\}$, where $A$ is a diagonalizable matrix","Finding the dimension of , where  is a diagonalizable matrix","S = \{B \in M_n \,|\, AB = BA\} A","I need some help proving the following: Let $A \in M_{n \times n}$ be a diagonalizable matrix with distinct eigenvalues $\lambda_1, \ldots , \lambda_k$ and corresponding multiplicities $d_1, \ldots, d_k$. Given that $S = \{B \in M_n \,|\, AB = BA\}$, prove that $\dim (S) = d_1^2 + \cdots + d_k^2$. Here's what I've done: Since $A$ is diagonalizable, we have that $D = P^{-1}AP$, where $D$ is a diagonal matrix. We may then write, $$P^{-1}AP = \begin{bmatrix}\Lambda_1 & & &  \\ & \Lambda_2 & & \\ & & \ddots & \\ & & & \Lambda_k\end{bmatrix} = D,$$ where $$\Lambda_i = \underbrace{\begin{bmatrix}\lambda_i & & & \\ & \lambda_i & & \\ & & \ddots & \\ & & & \lambda_i\end{bmatrix}}_{d_i \, \text {columns}}.$$ Obviously the size of each $\Lambda_i$ is $d_i \times d_i = d_i^2$, which I'm thinking we need to show that each block contributes this to the total dimension as we work along the diagonal, but I'm having a little trouble coming to this conclusion via the why and how . Clearly each column vector of $D$ and consequently each column vector of each $\Lambda_i$ is linearly independent, but I am unable to see what comes next to receive the squared part. Can someone provide a hint as to how I can make this conclusion?","I need some help proving the following: Let $A \in M_{n \times n}$ be a diagonalizable matrix with distinct eigenvalues $\lambda_1, \ldots , \lambda_k$ and corresponding multiplicities $d_1, \ldots, d_k$. Given that $S = \{B \in M_n \,|\, AB = BA\}$, prove that $\dim (S) = d_1^2 + \cdots + d_k^2$. Here's what I've done: Since $A$ is diagonalizable, we have that $D = P^{-1}AP$, where $D$ is a diagonal matrix. We may then write, $$P^{-1}AP = \begin{bmatrix}\Lambda_1 & & &  \\ & \Lambda_2 & & \\ & & \ddots & \\ & & & \Lambda_k\end{bmatrix} = D,$$ where $$\Lambda_i = \underbrace{\begin{bmatrix}\lambda_i & & & \\ & \lambda_i & & \\ & & \ddots & \\ & & & \lambda_i\end{bmatrix}}_{d_i \, \text {columns}}.$$ Obviously the size of each $\Lambda_i$ is $d_i \times d_i = d_i^2$, which I'm thinking we need to show that each block contributes this to the total dimension as we work along the diagonal, but I'm having a little trouble coming to this conclusion via the why and how . Clearly each column vector of $D$ and consequently each column vector of each $\Lambda_i$ is linearly independent, but I am unable to see what comes next to receive the squared part. Can someone provide a hint as to how I can make this conclusion?",,"['linear-algebra', 'matrices', 'vector-spaces', 'proof-writing', 'eigenvalues-eigenvectors']"
77,Algebraic multiplicity = geometric multiplicity?,Algebraic multiplicity = geometric multiplicity?,,"I was wondering if algebraic multiplicity was equal to the geometric multiplicity. If the matrix (of size $n\times n$) is diagonalisable, i.e. the characteristic polynomial is of the form $$p(x)=(x-\lambda_1)^{m_1}\cdot ...\cdot (x-\lambda_k)^{m_k}$$ with $m_1+...+m_k=n$, I think that indeed algebraic multiplicity of $\lambda_i$ (i.e. $m_i$) and geometric multiplicity are the same. But is there cases where it doesn't hold ?","I was wondering if algebraic multiplicity was equal to the geometric multiplicity. If the matrix (of size $n\times n$) is diagonalisable, i.e. the characteristic polynomial is of the form $$p(x)=(x-\lambda_1)^{m_1}\cdot ...\cdot (x-\lambda_k)^{m_k}$$ with $m_1+...+m_k=n$, I think that indeed algebraic multiplicity of $\lambda_i$ (i.e. $m_i$) and geometric multiplicity are the same. But is there cases where it doesn't hold ?",,['linear-algebra']
78,What is the meaning of subtracting from the identity matrix?,What is the meaning of subtracting from the identity matrix?,,"If I subtract the matrix $A$ from the identity matrix $I$, $I - A$, is there a meaning to the resulting matrix perhaps given some conditions like invertibility or symmetry? For example, $$ \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} - \begin{bmatrix} a & b \\ c & d \end{bmatrix} = \begin{bmatrix} 1-a & -b \\ -c & 1-d \end{bmatrix} $$ For future reference, is there a good reference to lookup answers to such questions when I can't derive the answer on my own?","If I subtract the matrix $A$ from the identity matrix $I$, $I - A$, is there a meaning to the resulting matrix perhaps given some conditions like invertibility or symmetry? For example, $$ \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} - \begin{bmatrix} a & b \\ c & d \end{bmatrix} = \begin{bmatrix} 1-a & -b \\ -c & 1-d \end{bmatrix} $$ For future reference, is there a good reference to lookup answers to such questions when I can't derive the answer on my own?",,['linear-algebra']
79,If $N$ is nilpotent then there exists $A$ such that $A^2=I+N$,If  is nilpotent then there exists  such that,N A A^2=I+N,"Suppose $N\in M_{3\times 3}^{\mathbb{C}}$ is a nilpotent matrix. Prove that there exists $A\in M_{3\times 3}^{\mathbb{C}}$ such that $A^2=I+N$. Hint: find $A$ in the form $A=P(N)$ where $P$ is a polynomial in $\mathbb{C}[x]$. I really can't think of a way to prove this. I know that $N^3=0$, so I tried combining this fact with $A^2=I+N$. I deduced that $(A^2-I)^3=0$, and thus the minimal polynomial of $A$ must include the terms $(x-1)(x+1)$. Any suggestions?","Suppose $N\in M_{3\times 3}^{\mathbb{C}}$ is a nilpotent matrix. Prove that there exists $A\in M_{3\times 3}^{\mathbb{C}}$ such that $A^2=I+N$. Hint: find $A$ in the form $A=P(N)$ where $P$ is a polynomial in $\mathbb{C}[x]$. I really can't think of a way to prove this. I know that $N^3=0$, so I tried combining this fact with $A^2=I+N$. I deduced that $(A^2-I)^3=0$, and thus the minimal polynomial of $A$ must include the terms $(x-1)(x+1)$. Any suggestions?",,['linear-algebra']
80,Can we prove that matrix multiplication by its inverse is commutative? [duplicate],Can we prove that matrix multiplication by its inverse is commutative? [duplicate],,"This question already has answers here : If $AB = I$ then $BA = I$ (34 answers) Closed 8 years ago . We know that $AA^{-1} = I$ and $A^{-1}A = I$, but is there a proof for the commutative property here? Or is this just the definition of invertibility?","This question already has answers here : If $AB = I$ then $BA = I$ (34 answers) Closed 8 years ago . We know that $AA^{-1} = I$ and $A^{-1}A = I$, but is there a proof for the commutative property here? Or is this just the definition of invertibility?",,"['linear-algebra', 'matrices']"
81,How can I intuitively interpret this vector operation?,How can I intuitively interpret this vector operation?,,"In reading through some very old source code that I inherited and came across a three-dimensional Euclidean vector operation that I can't seem to gain an intuition for. Transcribing the program code into mathematics, the operation (which has three inputs, vectors $a$, $b$, and $c$) is as follows: $$ a - \frac{a \cdot b}{b \cdot c}c $$ The comment alongside the source code advertises that the function ""projects a vector into a plane along a third vector"" and that the result is ""vector a projected into the plane defined by b along c"" . My geometric intuition is a bit rusty, but I've been unable to put together any combination of vector and plane projections that would yield the expression above. The comments are a bit ambiguous and can be interpreted in multiple ways, so I suppose that I'm not looking at it in the right way. Could someone elaborate on how this sort of operation might be described qualitatively?","In reading through some very old source code that I inherited and came across a three-dimensional Euclidean vector operation that I can't seem to gain an intuition for. Transcribing the program code into mathematics, the operation (which has three inputs, vectors $a$, $b$, and $c$) is as follows: $$ a - \frac{a \cdot b}{b \cdot c}c $$ The comment alongside the source code advertises that the function ""projects a vector into a plane along a third vector"" and that the result is ""vector a projected into the plane defined by b along c"" . My geometric intuition is a bit rusty, but I've been unable to put together any combination of vector and plane projections that would yield the expression above. The comments are a bit ambiguous and can be interpreted in multiple ways, so I suppose that I'm not looking at it in the right way. Could someone elaborate on how this sort of operation might be described qualitatively?",,"['linear-algebra', 'vector-spaces']"
82,Diagonalize a symmetric matrix,Diagonalize a symmetric matrix,,"let $$A = \left(\begin{array}{cccc} 1&2&3\\2&3&4\\3&4&5 \end{array}\right)$$ I need to find an invertible matrix $P$ such that $P^tAP$ is a diagonal matrix and it's main diagonal may have only the terms from the set $\{ 1,-1,0 \}$ I'd be glad if you could explain to me how to solve this. I haven't found the right theorem/algorithm. Thanks.","let $$A = \left(\begin{array}{cccc} 1&2&3\\2&3&4\\3&4&5 \end{array}\right)$$ I need to find an invertible matrix $P$ such that $P^tAP$ is a diagonal matrix and it's main diagonal may have only the terms from the set $\{ 1,-1,0 \}$ I'd be glad if you could explain to me how to solve this. I haven't found the right theorem/algorithm. Thanks.",,"['linear-algebra', 'matrices', 'diagonalization']"
83,Verifying the examples of Dual Space,Verifying the examples of Dual Space,,"I am new to the concept of Dual space. Can someone please explain which of these are dual spaces and why? Also if $y$ is a polynomial on $x$, then what does $x(0)$ means? • For $x \in P$, $y(x) = x(0)$ • Any vector space V, $y(x) = 0$; for every $x \in V $","I am new to the concept of Dual space. Can someone please explain which of these are dual spaces and why? Also if $y$ is a polynomial on $x$, then what does $x(0)$ means? • For $x \in P$, $y(x) = x(0)$ • Any vector space V, $y(x) = 0$; for every $x \in V $",,['linear-algebra']
84,$A$ is diagonalizable and $A^3 = A^2$,is diagonalizable and,A A^3 = A^2,If $A$ is diagonalizable and $A^3 = A^2$. Is it necessary true that $A^2 = A$?,If $A$ is diagonalizable and $A^3 = A^2$. Is it necessary true that $A^2 = A$?,,"['linear-algebra', 'matrices', 'diagonalization']"
85,Generate random symmetric positive-definite matrix,Generate random symmetric positive-definite matrix,,"Is there a simple way to generate a random matrix that is symmetric and positive-definite? The symmetry seems like it could be achieved by generating a matrix $M$ with independent random entries and using $M + M^T$, but is there a way I can ensure positive-definiteness?","Is there a simple way to generate a random matrix that is symmetric and positive-definite? The symmetry seems like it could be achieved by generating a matrix $M$ with independent random entries and using $M + M^T$, but is there a way I can ensure positive-definiteness?",,"['linear-algebra', 'matrices', 'random']"
86,if rank(A)=trace(A)=1 than A is a projection,if rank(A)=trace(A)=1 than A is a projection,,"Let A be a complex square matrix. Suppose that rank(A)=trace(A)=1. Prove that A is a projection. I have no idea how to prove it, so will be thankful for any help.","Let A be a complex square matrix. Suppose that rank(A)=trace(A)=1. Prove that A is a projection. I have no idea how to prove it, so will be thankful for any help.",,"['linear-algebra', 'matrices', 'trace']"
87,Why is $\det (A-\lambda I)=0$?,Why is ?,\det (A-\lambda I)=0,I'm not sure I understand the logic behind why $\det (A-\lambda I)=0$ for any non-trivial solution to $(A-\lambda I)x=0$.,I'm not sure I understand the logic behind why $\det (A-\lambda I)=0$ for any non-trivial solution to $(A-\lambda I)x=0$.,,"['linear-algebra', 'eigenvalues-eigenvectors']"
88,"For subspaces, if $N\subseteq M_1\cup\cdots\cup M_k$, then $N\subseteq M_i$ for some $i$?","For subspaces, if , then  for some ?",N\subseteq M_1\cup\cdots\cup M_k N\subseteq M_i i,"I have a vector space $V$ over a field of characteristic $0$. If $M_1,\dots,M_k$ are proper subspaces of $V$, and $N$ is a subspace of $V$ such that $N\subseteq M_1\cup\cdots\cup M_k$, how can you tell $N\subseteq M_i$ for some $i$? I was first trying to show it just in the case $N\subseteq M_1\cup M_2$, and hoping to extend it to finitely many $M_i$. If either of the $M_i$ contains the other, the claim follows, so I suppose neither $M_i$ contains the other. In hopes of a contradiction, I suppose $N\not\subseteq M_1$ and $N\not\subseteq M_2$, Picking $x_1\in N\setminus M_1$ and $x_2\in N\setminus M_2$, I'd have $x_1+x_2\in N\subseteq M_1\cup M_2$. The only thing I could conclude was that actually $x_1,x_2\notin M_1$ and $x_1,x_2\notin M_2$, which seems like a dead end. What's the right way to do this?","I have a vector space $V$ over a field of characteristic $0$. If $M_1,\dots,M_k$ are proper subspaces of $V$, and $N$ is a subspace of $V$ such that $N\subseteq M_1\cup\cdots\cup M_k$, how can you tell $N\subseteq M_i$ for some $i$? I was first trying to show it just in the case $N\subseteq M_1\cup M_2$, and hoping to extend it to finitely many $M_i$. If either of the $M_i$ contains the other, the claim follows, so I suppose neither $M_i$ contains the other. In hopes of a contradiction, I suppose $N\not\subseteq M_1$ and $N\not\subseteq M_2$, Picking $x_1\in N\setminus M_1$ and $x_2\in N\setminus M_2$, I'd have $x_1+x_2\in N\subseteq M_1\cup M_2$. The only thing I could conclude was that actually $x_1,x_2\notin M_1$ and $x_1,x_2\notin M_2$, which seems like a dead end. What's the right way to do this?",,"['linear-algebra', 'vector-spaces']"
89,How to show this tridiagonal matrix has eigenvalues $\lambda_{j}=4\sin^2{\frac{j\pi}{2(n+1)}}$?,How to show this tridiagonal matrix has eigenvalues ?,\lambda_{j}=4\sin^2{\frac{j\pi}{2(n+1)}},"Show that the $n\times n$ tridiagonal matrix  $$A=\begin{bmatrix} 2&-1&0&0&0\\ -1&2&-1&0&0\\ \vdots&\ddots&\ddots&\ddots&\vdots\\ 0&0&-1&2&-1\\ 0&0&0&-1&2 \end{bmatrix} $$ has the eigenvalues $$\lambda_{j}=4\sin^2{\dfrac{j\pi}{2(n+1)}},j=1,2,\cdots,n$$","Show that the $n\times n$ tridiagonal matrix  $$A=\begin{bmatrix} 2&-1&0&0&0\\ -1&2&-1&0&0\\ \vdots&\ddots&\ddots&\ddots&\vdots\\ 0&0&-1&2&-1\\ 0&0&0&-1&2 \end{bmatrix} $$ has the eigenvalues $$\lambda_{j}=4\sin^2{\dfrac{j\pi}{2(n+1)}},j=1,2,\cdots,n$$",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'tridiagonal-matrices', 'toeplitz-matrices']"
90,form of symmetric matrix of rank one,form of symmetric matrix of rank one,,"The question is: Let $C$ be a symmetric matrix of rank one. Prove that $C$ must have the form $C=aww^T$, where $a$ is a scalar and $w$ is a vector of norm one. I think we can easily prove that if $C$ has the form $C=aww^T$, then $C$ is symmetric and of rank one. Why we need $w$ is of norm one?","The question is: Let $C$ be a symmetric matrix of rank one. Prove that $C$ must have the form $C=aww^T$, where $a$ is a scalar and $w$ is a vector of norm one. I think we can easily prove that if $C$ has the form $C=aww^T$, then $C$ is symmetric and of rank one. Why we need $w$ is of norm one?",,"['linear-algebra', 'matrices']"
91,Understanding matrices as linear transformations & Relationship with Gaussian Elimination and Bézout's Identity,Understanding matrices as linear transformations & Relationship with Gaussian Elimination and Bézout's Identity,,"I am currently taking a intro course to abstract algebra and am revisiting ideas from linear algebra so that I can better understand examples. When i was in undergraduate learning L.A., I thought of matrix manipulations as ways of solving $n \times n$ systems of equations. Recently i was exposed to the idea of a matrix being a linear transformation, and matrix multiplication being composition of linear transformations. Im trying to understand this in a more intuitive way and was hoping for some insight... I was thinking of a basic $2\times2$ example and how it affects a point $(x,y)$.  We could have a matrix : \begin{bmatrix} a & b \\ c & d \end{bmatrix} When we 'apply' or multiply this to a point $(x,y)$ using matrix multiplication we get new $x' = ax + by$ and $y' = cx + dy$. So if $b,c = 0$, then I can see that what we are doing is 'scaling' both $x \;\& \;y$. I'm guessing that if $b,c \neq 0$, then this becomes some sort of rotation or reflection, but how do you understand this on a fundamental level? How do these operations relate to Gaussian elimination when we are trying to solve systems of equations? Or are are these two seperate applications of matrices? Another observation is that when multiplying a matrix such as this one with a point, we get two equations which remind me of Bézout's identity. Am I overanalyzing this or can I draw connections between these two concepts? Thanks for any input!","I am currently taking a intro course to abstract algebra and am revisiting ideas from linear algebra so that I can better understand examples. When i was in undergraduate learning L.A., I thought of matrix manipulations as ways of solving $n \times n$ systems of equations. Recently i was exposed to the idea of a matrix being a linear transformation, and matrix multiplication being composition of linear transformations. Im trying to understand this in a more intuitive way and was hoping for some insight... I was thinking of a basic $2\times2$ example and how it affects a point $(x,y)$.  We could have a matrix : \begin{bmatrix} a & b \\ c & d \end{bmatrix} When we 'apply' or multiply this to a point $(x,y)$ using matrix multiplication we get new $x' = ax + by$ and $y' = cx + dy$. So if $b,c = 0$, then I can see that what we are doing is 'scaling' both $x \;\& \;y$. I'm guessing that if $b,c \neq 0$, then this becomes some sort of rotation or reflection, but how do you understand this on a fundamental level? How do these operations relate to Gaussian elimination when we are trying to solve systems of equations? Or are are these two seperate applications of matrices? Another observation is that when multiplying a matrix such as this one with a point, we get two equations which remind me of Bézout's identity. Am I overanalyzing this or can I draw connections between these two concepts? Thanks for any input!",,"['linear-algebra', 'matrices', 'gaussian-elimination']"
92,How to build a orthogonal basis from a vector?,How to build a orthogonal basis from a vector?,,"Anybody know how I can build a orthogonal base using only a vector? I have a vector in the form $v_1 = [a, b, -a, -b]$, where $a$ and $b$ are real numbers. I did try build in the ""adhoc way"" but, nothing, I only got two orthogonal vectors: $$v_1 = [a, b, -a, -b], \text{    } v_2 = [a, -b, a, -b]$$ I need more two vectors to complete the orthogonal basis $\{v_1, v_2, v_3, v_4\}$. Anybody can help me? Thanks...","Anybody know how I can build a orthogonal base using only a vector? I have a vector in the form $v_1 = [a, b, -a, -b]$, where $a$ and $b$ are real numbers. I did try build in the ""adhoc way"" but, nothing, I only got two orthogonal vectors: $$v_1 = [a, b, -a, -b], \text{    } v_2 = [a, -b, a, -b]$$ I need more two vectors to complete the orthogonal basis $\{v_1, v_2, v_3, v_4\}$. Anybody can help me? Thanks...",,"['linear-algebra', 'vector-spaces']"
93,"The ring $\{a+b\sqrt{2}\mid a,b\in\mathbb{Z}\}$",The ring,"\{a+b\sqrt{2}\mid a,b\in\mathbb{Z}\}","The set $\{a+b\sqrt{2}\mid a,b\in\mathbb{Z}\}$ spans a ring under real addition and multiplication. Which elements have multiplicative inverses? This is part of an exercise from an introductory text to algebraic structures. The answer is that an element has a multiplicative inverse if and only if $a^2 - 2b^2 = \pm 1$. It is evident that elements verifying the condition are units but I fail to see that it is the only possible solution. Any one can shed some light?","The set $\{a+b\sqrt{2}\mid a,b\in\mathbb{Z}\}$ spans a ring under real addition and multiplication. Which elements have multiplicative inverses? This is part of an exercise from an introductory text to algebraic structures. The answer is that an element has a multiplicative inverse if and only if $a^2 - 2b^2 = \pm 1$. It is evident that elements verifying the condition are units but I fail to see that it is the only possible solution. Any one can shed some light?",,"['linear-algebra', 'abstract-algebra', 'ring-theory']"
94,Books on module theory for linear algebra,Books on module theory for linear algebra,,"I'm looking for a reference to learn the basics of module theory for linear algebra. The purpose is to understand linear algebra in the general setting. I was reading Artin when I came across such topic, but his explanation is too brief. Can anyone give me some reference? Thanks in advance.","I'm looking for a reference to learn the basics of module theory for linear algebra. The purpose is to understand linear algebra in the general setting. I was reading Artin when I came across such topic, but his explanation is too brief. Can anyone give me some reference? Thanks in advance.",,"['linear-algebra', 'reference-request', 'modules', 'book-recommendation']"
95,When do addition and inversion of matrices commute?,When do addition and inversion of matrices commute?,,"Can you guys help me out with the following problem: Problem: Find conditions that the matrices $A$ and $B$ have to satisfy in order for the following to be valid: $(A+B)^{-1} = A^{-1} + B^{-1}$. My solution. The first condition is that $A$ and $B$ have to have the same dimensions. Then by expanding the two equalities: $(A+B) \cdot (A^{-1} + B^{-1})=I $ and $(A^{-1} + B^{-1}) \cdot (A+B)=I$, I deduced the second condition: $A$ and $B$ have both to be invertible, and third condition is: $A^{-1} \cdot B=B \cdot A^{-1}$. Is my answer correct?","Can you guys help me out with the following problem: Problem: Find conditions that the matrices $A$ and $B$ have to satisfy in order for the following to be valid: $(A+B)^{-1} = A^{-1} + B^{-1}$. My solution. The first condition is that $A$ and $B$ have to have the same dimensions. Then by expanding the two equalities: $(A+B) \cdot (A^{-1} + B^{-1})=I $ and $(A^{-1} + B^{-1}) \cdot (A+B)=I$, I deduced the second condition: $A$ and $B$ have both to be invertible, and third condition is: $A^{-1} \cdot B=B \cdot A^{-1}$. Is my answer correct?",,"['linear-algebra', 'matrices']"
96,"$A$ is skew hermitian, prove $e^A$ is unitary","is skew hermitian, prove  is unitary",A e^A,"Given $A$ is a skew-hermitian, ( i.e $A^H = -A$ ) then how do you prove the matrix exponential $e^A$ is unitary. To prove the unitary property of the matrix, I need to show $(e^A)^{*}(e^A) = (e^A)(e^A)^{*}= I$. Can any one help me how to proceed and prove the result?","Given $A$ is a skew-hermitian, ( i.e $A^H = -A$ ) then how do you prove the matrix exponential $e^A$ is unitary. To prove the unitary property of the matrix, I need to show $(e^A)^{*}(e^A) = (e^A)(e^A)^{*}= I$. Can any one help me how to proceed and prove the result?",,['linear-algebra']
97,How do I rotate a matrix transformation with a centered origin?,How do I rotate a matrix transformation with a centered origin?,,"This is actually something I'm doing in Objective-C programming, but since it's very math-oriented I thought I'd post it here. I was reading up on linear transformations: http://en.wikipedia.org/wiki/Matrix_(mathematics) Basically I need to rotate a shape around its center. With my current implementation, the rotation is done using the top left as its origin. Here's a screenshot: Here's the code I'm using to create each x,y coordinate: CGFloat angle = 0.261799; // (15 degrees) CGFloat xr = x1 * cosf(angle) + y1 * -sinf(angle) + tx; CGFloat yr = x1 * sinf(angle) + y1 * cosf(angle) + ty; I realize there are functions (CGAffineTransformRotate, etc) in Objective-C that do this for me, but since I'm not using a UIView, I need to do it manually. Plus, it would be nice to know. :) So when I plug in 35.0 for tx and -35.0 for ty, (numbers I just found that seemed to work, through trial and error) here is what I get: That's what I want! Now I just need some way to figure out these tx and ty translation values to center the shape's origin, based on the given angle, width or height values. (I'm also seeing different results when setting tx and ty if the original x,y coordinates of the shape are different.) Any help would be much appreciated. Thanks!","This is actually something I'm doing in Objective-C programming, but since it's very math-oriented I thought I'd post it here. I was reading up on linear transformations: http://en.wikipedia.org/wiki/Matrix_(mathematics) Basically I need to rotate a shape around its center. With my current implementation, the rotation is done using the top left as its origin. Here's a screenshot: Here's the code I'm using to create each x,y coordinate: CGFloat angle = 0.261799; // (15 degrees) CGFloat xr = x1 * cosf(angle) + y1 * -sinf(angle) + tx; CGFloat yr = x1 * sinf(angle) + y1 * cosf(angle) + ty; I realize there are functions (CGAffineTransformRotate, etc) in Objective-C that do this for me, but since I'm not using a UIView, I need to do it manually. Plus, it would be nice to know. :) So when I plug in 35.0 for tx and -35.0 for ty, (numbers I just found that seemed to work, through trial and error) here is what I get: That's what I want! Now I just need some way to figure out these tx and ty translation values to center the shape's origin, based on the given angle, width or height values. (I'm also seeing different results when setting tx and ty if the original x,y coordinates of the shape are different.) Any help would be much appreciated. Thanks!",,"['linear-algebra', 'geometry', 'transformation', 'rotations']"
98,if $A^2 \in M_{3}(\mathbb{R})$ is diagonalizable then so is $A$,if  is diagonalizable then so is,A^2 \in M_{3}(\mathbb{R}) A,"Prove or disprove: if $A^2 \in M_{3}(\mathbb{R})$ is diagonalizable then so is $A$. I'm pretty confident this is not true, but I've tried and tried to find a counter example without success. If someone contradicts this, I'd appreciate if you can outline your thought process when constructing the matrix $A$. Also, another question in the same batch, asks: Let $p(t)=t(t-0.25)(t-1)$ be the characteristic polynomial of $A^2$, is $A$ diagonalizable? I'm thinking this is suppose to answer the previous question, assuming the answer here is false. Here I know $A^2$ is diagonalizable, but I haven't made any substantial progress other than that. I know I can go from the eigenvalues of $A$ to the eigenvalues of $A^k$, but not the other way around.","Prove or disprove: if $A^2 \in M_{3}(\mathbb{R})$ is diagonalizable then so is $A$. I'm pretty confident this is not true, but I've tried and tried to find a counter example without success. If someone contradicts this, I'd appreciate if you can outline your thought process when constructing the matrix $A$. Also, another question in the same batch, asks: Let $p(t)=t(t-0.25)(t-1)$ be the characteristic polynomial of $A^2$, is $A$ diagonalizable? I'm thinking this is suppose to answer the previous question, assuming the answer here is false. Here I know $A^2$ is diagonalizable, but I haven't made any substantial progress other than that. I know I can go from the eigenvalues of $A$ to the eigenvalues of $A^k$, but not the other way around.",,['linear-algebra']
99,Find the rotation matrix for +15° out of the rotation matrix +60° without using trigonometric functions,Find the rotation matrix for +15° out of the rotation matrix +60° without using trigonometric functions,,"Find the rotation matrix for +15° out of the rotation matrix +60° without using trigonometric functions. And as much as I would love to tell you what I did so far. I don't even know where to start with this exercise.. The matrix for +60° is this. $$\left[ \begin{matrix}  1/2 &  -\sqrt 3/2\\ \sqrt 3/2 & 1/2 \\ \end{matrix} \right] $$ How do I even start?","Find the rotation matrix for +15° out of the rotation matrix +60° without using trigonometric functions. And as much as I would love to tell you what I did so far. I don't even know where to start with this exercise.. The matrix for +60° is this. $$\left[ \begin{matrix}  1/2 &  -\sqrt 3/2\\ \sqrt 3/2 & 1/2 \\ \end{matrix} \right] $$ How do I even start?",,"['linear-algebra', 'matrices', 'rotations']"
