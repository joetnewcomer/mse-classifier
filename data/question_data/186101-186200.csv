,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,what is the graph of $y = x^x$?,what is the graph of ?,y = x^x,Obviously on the RHS this graph is just a really steep exponential graph however problems arise on the LHS and I cannot find any graph sketching programs that can do. Some will give a graph but then simply say that the LHS is undefined which must be incorrect since negative values with odd powers must still work like $-3^{-3} = -1/27$ but then of course values like $(-1/2)^{-1/2}$ do not. I asked my maths teacher about this and my tutor and both didn't seem to have answers.,Obviously on the RHS this graph is just a really steep exponential graph however problems arise on the LHS and I cannot find any graph sketching programs that can do. Some will give a graph but then simply say that the LHS is undefined which must be incorrect since negative values with odd powers must still work like $-3^{-3} = -1/27$ but then of course values like $(-1/2)^{-1/2}$ do not. I asked my maths teacher about this and my tutor and both didn't seem to have answers.,,['functions']
1,Solving a functional equation $2 f(2x)=f(x)(1+\cos(x))+f(x+\pi)(1-\cos(x))$,Solving a functional equation,2 f(2x)=f(x)(1+\cos(x))+f(x+\pi)(1-\cos(x)),"I am trying to solve the following functional equation, which appears in some of my physics calculations :  $f(x)=\frac{1}{2}\left(f(\frac{x}{2})(1+\cos(\frac{x}{2}))+f(\frac{x}{2}+\pi)(1-\cos(\frac{x}{2}))\right)$, for a function $f$ defined on $(0,2\pi)$. I am interested in all functions satisfying this equation, but let us start by finding all functions which are defined on $[0,2\pi]$, which are continuous and differentiable. Constant functions are solutions. Let us note $f(0)=a$ and $f(2\pi)=b$.  My numerics, starting from some function and iterating until I get to a fixed point seem to point towards a unique solution depending only on $a$ and $b$. If $a=b$, this would be the constant function. If $a\neq b$, the solution seems to have the following properties (conjectures) : Monotonous $f(\pi)=(f(0)+f(2\pi))/2$ $f(\pi-x)+f(\pi+x)=2*f(\pi)$, so $f(\pi)$ is a symmetric point of the curve First derivative is $0$ in $0$ and $2\pi$ The last point can be proven easily by deriving the equation, which gives $f'(x)=\frac{1}{4}\left(f'(\frac{x}{2})(1+\cos(\frac{x}{2}))+f'(\frac{x}{2}+\pi)(1-\cos(\frac{x}{2}))+\sin(\frac{x}{2})(f(\frac{x}{2}+\pi)-f(\frac{x}{2}+)\right)$ Evaluating at $0$ and $2\pi$ yields $0$. Evaluating the original equation at $0$ or $2\pi$ yields consistent results, at $\pi$ gives $f(\pi)=(f(\pi/2)+f(3\pi/2))/2$, which is consistent with the second conjecture at this particular point. So far I haven't managed to prove the other conjectures on the solution. Note that all these properties are shared by $Cos(x/2)+C$, but this is not a solution, so there are other things needed to characterize a solution. Here is a plot of an approximate numerical solution. Any ideas how to solve this? Thanks!","I am trying to solve the following functional equation, which appears in some of my physics calculations :  $f(x)=\frac{1}{2}\left(f(\frac{x}{2})(1+\cos(\frac{x}{2}))+f(\frac{x}{2}+\pi)(1-\cos(\frac{x}{2}))\right)$, for a function $f$ defined on $(0,2\pi)$. I am interested in all functions satisfying this equation, but let us start by finding all functions which are defined on $[0,2\pi]$, which are continuous and differentiable. Constant functions are solutions. Let us note $f(0)=a$ and $f(2\pi)=b$.  My numerics, starting from some function and iterating until I get to a fixed point seem to point towards a unique solution depending only on $a$ and $b$. If $a=b$, this would be the constant function. If $a\neq b$, the solution seems to have the following properties (conjectures) : Monotonous $f(\pi)=(f(0)+f(2\pi))/2$ $f(\pi-x)+f(\pi+x)=2*f(\pi)$, so $f(\pi)$ is a symmetric point of the curve First derivative is $0$ in $0$ and $2\pi$ The last point can be proven easily by deriving the equation, which gives $f'(x)=\frac{1}{4}\left(f'(\frac{x}{2})(1+\cos(\frac{x}{2}))+f'(\frac{x}{2}+\pi)(1-\cos(\frac{x}{2}))+\sin(\frac{x}{2})(f(\frac{x}{2}+\pi)-f(\frac{x}{2}+)\right)$ Evaluating at $0$ and $2\pi$ yields $0$. Evaluating the original equation at $0$ or $2\pi$ yields consistent results, at $\pi$ gives $f(\pi)=(f(\pi/2)+f(3\pi/2))/2$, which is consistent with the second conjecture at this particular point. So far I haven't managed to prove the other conjectures on the solution. Note that all these properties are shared by $Cos(x/2)+C$, but this is not a solution, so there are other things needed to characterize a solution. Here is a plot of an approximate numerical solution. Any ideas how to solve this? Thanks!",,"['functional-analysis', 'functions', 'recurrence-relations', 'special-functions', 'functional-equations']"
2,"$f(f(...f(x)...))$ $a$ times, where $a\in\mathbb{R}$","times, where",f(f(...f(x)...)) a a\in\mathbb{R},"Take $f(x)$ and do a ""double-call"": $f^2(x)=f(f(x))$ I use this notation here to explain my problem. This can be easy calculated for any function. Also $f^{100}(x)$ is not really a problem. This means this operation works well for natural numbers. Then it is already possible to define some easy rules: $f^1(x)=f(x)$ $f^{n+1}(x)=f(f^n(x))$ $f^a(f^b(x))=f^{a+b}(x)$ The next question is: Can this be extended to all integers? Yes. For this we only require the inverse function $f^{-1}(x)$, because we know $f^{-1}(f(x))=x$. Or more detailed: $f^{-1}(f^n(x))=f^{n-1}(x)$. This results in some more rules: $f^0(x)=x$ $f^{-1}(f(x))=x$ $f^{n-1}(x)=f^{-1}(f^n(x))$ Of course it is not that easy to calculate $f^{-1}(x)$ for many functions, but this is ok. One may ask if this can be extended to rational numbers? E.g. $f^{0.5}(x)$ would mean that $f^{0.5}(f^{0.5}(x))=f(x)$. It sounds reasonable, but this seems to be very hard to calculate for many function and the result does not seem to be unique. Take $f(x)=x$ then $f^{0.5}(x)$ could be $x$ or also $-x$. Maybe there are much more other solutions? Or is it also possible to extend this thing to the real numbers? Or even complex numbers? If it can be done: how? And if not: why? Thank you very much Kevin","Take $f(x)$ and do a ""double-call"": $f^2(x)=f(f(x))$ I use this notation here to explain my problem. This can be easy calculated for any function. Also $f^{100}(x)$ is not really a problem. This means this operation works well for natural numbers. Then it is already possible to define some easy rules: $f^1(x)=f(x)$ $f^{n+1}(x)=f(f^n(x))$ $f^a(f^b(x))=f^{a+b}(x)$ The next question is: Can this be extended to all integers? Yes. For this we only require the inverse function $f^{-1}(x)$, because we know $f^{-1}(f(x))=x$. Or more detailed: $f^{-1}(f^n(x))=f^{n-1}(x)$. This results in some more rules: $f^0(x)=x$ $f^{-1}(f(x))=x$ $f^{n-1}(x)=f^{-1}(f^n(x))$ Of course it is not that easy to calculate $f^{-1}(x)$ for many functions, but this is ok. One may ask if this can be extended to rational numbers? E.g. $f^{0.5}(x)$ would mean that $f^{0.5}(f^{0.5}(x))=f(x)$. It sounds reasonable, but this seems to be very hard to calculate for many function and the result does not seem to be unique. Take $f(x)=x$ then $f^{0.5}(x)$ could be $x$ or also $-x$. Maybe there are much more other solutions? Or is it also possible to extend this thing to the real numbers? Or even complex numbers? If it can be done: how? And if not: why? Thank you very much Kevin",,"['functions', 'real-numbers']"
3,Will this function be odd?,Will this function be odd?,,"Question: If $f:R\to R$ is an invertible function such that $f(x)$ and $f^{-1}(x)$ are symmetric about the line $y = -x$, then: A) $f(x)$ is odd B) $f(x)$ and $f^{-1}(x)$ may not be symmetric about $y = x$ C) $f(x)$ may not be odd D) None of these Purely by intuition, I'd say that $f(x)$ will be an odd function. Is this right? If it is, how can I prove this?","Question: If $f:R\to R$ is an invertible function such that $f(x)$ and $f^{-1}(x)$ are symmetric about the line $y = -x$, then: A) $f(x)$ is odd B) $f(x)$ and $f^{-1}(x)$ may not be symmetric about $y = x$ C) $f(x)$ may not be odd D) None of these Purely by intuition, I'd say that $f(x)$ will be an odd function. Is this right? If it is, how can I prove this?",,"['functions', 'inverse']"
4,Find how many such complex numbers exist,Find how many such complex numbers exist,,"Let $f:\mathbb{C}\to\mathbb{C}$ be defined by $f(z)=z^2+iz+1$ . How many complex numbers $z$ are there such that $\text{Im}(z)>0$ and both the real and the imaginary parts of $f(z)$ are integers with absolute value at most $10$ ? A. $\, 399 \quad$ B. $\, 401 \quad$ C. $\, 413 \quad$ D. $\, 431 \quad$ I cannot make much progress here. I tried putting $z=a+ib$ . I also tried putting $z=e^{i\theta}$ . But, none of them worked. Any kind of help would be appreciated.","Let be defined by . How many complex numbers are there such that and both the real and the imaginary parts of are integers with absolute value at most ? A. B. C. D. I cannot make much progress here. I tried putting . I also tried putting . But, none of them worked. Any kind of help would be appreciated.","f:\mathbb{C}\to\mathbb{C} f(z)=z^2+iz+1 z \text{Im}(z)>0 f(z) 10 \, 399 \quad \, 401 \quad \, 413 \quad \, 431 \quad z=a+ib z=e^{i\theta}","['functions', 'complex-numbers', 'functional-inequalities']"
5,Sufficient conditions to have the supremum of a continuous function continuous?,Sufficient conditions to have the supremum of a continuous function continuous?,,"Consider a function $f:\mathcal{X}\times \mathcal{Y}\rightarrow \mathbb{R}$ with $\mathcal{X}\subseteq \mathbb{R}^k$ and $\mathcal{Y}\subseteq \mathbb{R}^p$. Under which sets of conditions is $\sup_{x\in \mathcal{X}} f(x,y)$ continuous? Similar questions are asked here and here (among the others) but I can't summarise the main findings. In particular, is having $f(x,y)$ jointly continuous in $x$ and $y$ plus $\mathcal{X}$ compact sufficient?","Consider a function $f:\mathcal{X}\times \mathcal{Y}\rightarrow \mathbb{R}$ with $\mathcal{X}\subseteq \mathbb{R}^k$ and $\mathcal{Y}\subseteq \mathbb{R}^p$. Under which sets of conditions is $\sup_{x\in \mathcal{X}} f(x,y)$ continuous? Similar questions are asked here and here (among the others) but I can't summarise the main findings. In particular, is having $f(x,y)$ jointly continuous in $x$ and $y$ plus $\mathcal{X}$ compact sufficient?",,"['functional-analysis', 'functions', 'continuity', 'supremum-and-infimum']"
6,holomorphic function on unit disk $D$,holomorphic function on unit disk,D,Suppose $f$ is holomorphic on $D= (\ z:|z|<1)$ and $f$ is an even function  (i.e. $f(z)=f(-z)$). Show that there is a holomorphic function $g$ on $D$ such that $g(x) = f(\sqrt{x})$ for all positive real numbers $x<1$. Here is my attempt: $f$ even $\implies f'$ odd $\implies f''$ even $\implies\cdots \implies f(z) = a_0 + a_2 z^2 + a_4 z^4 + \cdots$ Now let $g(z) = b_0 + b_1 z + b_2 z^2 + \cdots$ Is it simple to put $x$ and $\sqrt{x}$ to $f$ and $g$ respectively such that $g(x) = f(\sqrt{x})$? How about $a_n$ and $b_n$? Are they equal?,Suppose $f$ is holomorphic on $D= (\ z:|z|<1)$ and $f$ is an even function  (i.e. $f(z)=f(-z)$). Show that there is a holomorphic function $g$ on $D$ such that $g(x) = f(\sqrt{x})$ for all positive real numbers $x<1$. Here is my attempt: $f$ even $\implies f'$ odd $\implies f''$ even $\implies\cdots \implies f(z) = a_0 + a_2 z^2 + a_4 z^4 + \cdots$ Now let $g(z) = b_0 + b_1 z + b_2 z^2 + \cdots$ Is it simple to put $x$ and $\sqrt{x}$ to $f$ and $g$ respectively such that $g(x) = f(\sqrt{x})$? How about $a_n$ and $b_n$? Are they equal?,,"['complex-analysis', 'functions', 'derivatives']"
7,Looking analytically if one formula is better than another one,Looking analytically if one formula is better than another one,,"I'm studying errors in functions on numerical methods. On my notes , I've written the Heron's Formula: Let $a\geq b\geq c$:$$A=\sqrt{p(p-a)(p-b)(p-c)}\ \ ,$$ where $$p=\frac{a+b+c}{2}.$$ This formula suffers cancellation if $a \sim b+c$ on the factor $(p-a)$. Looking for a reformulation, we find that $$A=\frac{1}{4}\sqrt{(a+(b+c))(c-(a-b))(c+(a-b))(a+(b-c))}.$$ It's easy to show that this formula is better analytically than the other one if we're looking to avoid cancellation errors. How we show this? How can we see that the formula below is better than the above one? Thanks!","I'm studying errors in functions on numerical methods. On my notes , I've written the Heron's Formula: Let $a\geq b\geq c$:$$A=\sqrt{p(p-a)(p-b)(p-c)}\ \ ,$$ where $$p=\frac{a+b+c}{2}.$$ This formula suffers cancellation if $a \sim b+c$ on the factor $(p-a)$. Looking for a reformulation, we find that $$A=\frac{1}{4}\sqrt{(a+(b+c))(c-(a-b))(c+(a-b))(a+(b-c))}.$$ It's easy to show that this formula is better analytically than the other one if we're looking to avoid cancellation errors. How we show this? How can we see that the formula below is better than the above one? Thanks!",,"['calculus', 'real-analysis', 'functions', 'numerical-methods', 'error-propagation']"
8,Function $s(x)=1+\sum_{k=1}^{\infty} \frac{x^k}{k^k}$ - is there any other way to define it?,Function  - is there any other way to define it?,s(x)=1+\sum_{k=1}^{\infty} \frac{x^k}{k^k},"This series converges for all $x \in (-\infty, \infty)$, thus the function is analytic on the real line and defined by its Taylor series. However, unlike the exponential function, this one is very elusive - I can't find any other representation for it - no integral, no differential equation, nothing. Its derivatives exist (an infinite number of them), but it seems they can't be connected to the function itself in any other way. I know that $s(1)-1$ is called 'Sophomore's Dream', because it has integral representation: $$\sum_{k=1}^{\infty} \frac{1}{k^k}=\int_0^1 \frac{1}{t^t}dt$$ I don't know if the same method can be used to find a general integral representation for $s(x)$, but I have some hope. The most interesting (in my opinion) property of this function - $s(x)$ and its derivatives all have exactly one zero (for $x_0<0$) and one minimum ($x_m<0$ and $s(x_m)<0$). They are connected in a sense, since obviously if $s(x)$ has a minimum, then $s'(x)$ has a zero. Here is a plot of $s(x)$ for $x<0$: I added $1$ to the sum in the definition of $s(x)$ for consistency - it can be seen from the form of its derivatives: $$s(x)=1+x+\frac{x^2}{2^2}+\frac{x^3}{3^3}+\dots=1+\sum_{k=1}^{\infty} \frac{x^k}{k^k}$$ $$s'(x)=1+\frac{x}{2}+\frac{x^2}{9}+\frac{x^3}{64}+\dots=\sum_{k=1}^{\infty} \frac{k~x^{k-1}}{k^k}$$ $$s''(x)=\frac{1}{2}+\frac{2x}{9}+\frac{3x^2}{64}+\frac{4x^3}{625}+\dots=\sum_{k=1}^{\infty} \frac{k(k-1)~x^{k-2}}{k^k}$$ $$s'''(x)=\frac{2}{9}+\frac{3x}{32}+\frac{12x^2}{625}+\frac{5x^3}{1944}+\dots=\sum_{k=1}^{\infty} \frac{k(k-1)(k-2)~x^{k-3}}{k^k}$$ It is apparent, that $\lim_{n \to \infty} s^{(n)}(0) = 0$, however, the zero on the negative line actually moves to the left with each differentiation (see the position of the minimum). So I'm not sure, how the 'infinite derivative' of $s(x)$ would look. I'd like to know if someone studied this function, and get a reference for it. But the main question is - what other definitions are possible for $s(x)$, except for the series? Thanks to this great answer , I'm able to write the integral definition: $$s(x)=1+\int_0^1 x ~u^{-u~x} du$$ So I consider my question answered. By the way, if we define the function: $$p(x)=\frac{s(x)-1}{x}=\int_0^1 u^{-u~x} du=\sum_{k=1}^{\infty} \frac{x^{k-1}}{k^k}$$ We get a very monotone (exponential-like) behavior, without any zeroes or minima. The function and several of its derivatives are plotted below.","This series converges for all $x \in (-\infty, \infty)$, thus the function is analytic on the real line and defined by its Taylor series. However, unlike the exponential function, this one is very elusive - I can't find any other representation for it - no integral, no differential equation, nothing. Its derivatives exist (an infinite number of them), but it seems they can't be connected to the function itself in any other way. I know that $s(1)-1$ is called 'Sophomore's Dream', because it has integral representation: $$\sum_{k=1}^{\infty} \frac{1}{k^k}=\int_0^1 \frac{1}{t^t}dt$$ I don't know if the same method can be used to find a general integral representation for $s(x)$, but I have some hope. The most interesting (in my opinion) property of this function - $s(x)$ and its derivatives all have exactly one zero (for $x_0<0$) and one minimum ($x_m<0$ and $s(x_m)<0$). They are connected in a sense, since obviously if $s(x)$ has a minimum, then $s'(x)$ has a zero. Here is a plot of $s(x)$ for $x<0$: I added $1$ to the sum in the definition of $s(x)$ for consistency - it can be seen from the form of its derivatives: $$s(x)=1+x+\frac{x^2}{2^2}+\frac{x^3}{3^3}+\dots=1+\sum_{k=1}^{\infty} \frac{x^k}{k^k}$$ $$s'(x)=1+\frac{x}{2}+\frac{x^2}{9}+\frac{x^3}{64}+\dots=\sum_{k=1}^{\infty} \frac{k~x^{k-1}}{k^k}$$ $$s''(x)=\frac{1}{2}+\frac{2x}{9}+\frac{3x^2}{64}+\frac{4x^3}{625}+\dots=\sum_{k=1}^{\infty} \frac{k(k-1)~x^{k-2}}{k^k}$$ $$s'''(x)=\frac{2}{9}+\frac{3x}{32}+\frac{12x^2}{625}+\frac{5x^3}{1944}+\dots=\sum_{k=1}^{\infty} \frac{k(k-1)(k-2)~x^{k-3}}{k^k}$$ It is apparent, that $\lim_{n \to \infty} s^{(n)}(0) = 0$, however, the zero on the negative line actually moves to the left with each differentiation (see the position of the minimum). So I'm not sure, how the 'infinite derivative' of $s(x)$ would look. I'd like to know if someone studied this function, and get a reference for it. But the main question is - what other definitions are possible for $s(x)$, except for the series? Thanks to this great answer , I'm able to write the integral definition: $$s(x)=1+\int_0^1 x ~u^{-u~x} du$$ So I consider my question answered. By the way, if we define the function: $$p(x)=\frac{s(x)-1}{x}=\int_0^1 u^{-u~x} du=\sum_{k=1}^{\infty} \frac{x^{k-1}}{k^k}$$ We get a very monotone (exponential-like) behavior, without any zeroes or minima. The function and several of its derivatives are plotted below.",,"['calculus', 'functions', 'reference-request', 'taylor-expansion']"
9,What is the Name of the Function $f(x) = \frac{x + |x|}{2}$,What is the Name of the Function,f(x) = \frac{x + |x|}{2},"I'm dealing with a function that can be written in all of the following forms: $$f(x) = \frac{x + |x|}{2} \\ = x\ \Theta(x) \\ = \int_{-\infty}^x \Theta(y) \operatorname{d}y,$$ where $\Theta(x)$ is the standard unit step (Heaviside) function. Does the function $f(x)$ have a special name?","I'm dealing with a function that can be written in all of the following forms: $$f(x) = \frac{x + |x|}{2} \\ = x\ \Theta(x) \\ = \int_{-\infty}^x \Theta(y) \operatorname{d}y,$$ where $\Theta(x)$ is the standard unit step (Heaviside) function. Does the function $f(x)$ have a special name?",,"['functions', 'terminology', 'absolute-value']"
10,$f'$ decreasing everywhere but not defined in one point. Is $f$ concave?,decreasing everywhere but not defined in one point. Is  concave?,f' f,"Small issue: Suppose that $f:[a,b] \rightarrow \mathbb{R}$ is a continuous function, differentiable except on a finite set of points, let say in one point $y$. For $x<y$ and $x>y$ we have $f''<0$. Moreover, $f'$ is larger on $[0,y[$ than on $]y,1]$. Overall: $f'$ is decreasing everywhere but not defined on $y$. Can I say that $f$ is concave? Why? Thank you","Small issue: Suppose that $f:[a,b] \rightarrow \mathbb{R}$ is a continuous function, differentiable except on a finite set of points, let say in one point $y$. For $x<y$ and $x>y$ we have $f''<0$. Moreover, $f'$ is larger on $[0,y[$ than on $]y,1]$. Overall: $f'$ is decreasing everywhere but not defined on $y$. Can I say that $f$ is concave? Why? Thank you",,"['real-analysis', 'functional-analysis', 'functions', 'convex-analysis']"
11,"bijective function between $(0,1)$ and $(0,1)\times(0,1)$",bijective function between  and,"(0,1) (0,1)\times(0,1)","I know there exists one, but I want to find the explicit form of some bijective function between $(0,1)$ and $(0,1)\times(0,1)$.","I know there exists one, but I want to find the explicit form of some bijective function between $(0,1)$ and $(0,1)\times(0,1)$.",,['functions']
12,A possible f(x) and g(x) having these characteristics?,A possible f(x) and g(x) having these characteristics?,,"Do you think that it is possible to find a $f$ and a $g$ such that $\forall x \gt 1, \forall y \gt 1$  then $$f(x) \gt 0,$$ $$g(x) \gt 0,$$ $$f(x) \neq x,$$ $$g(x) \neq x,$$ and $$f\left({1\over 1-{1\over x}-{1\over y}}\right) = {1\over 1-{1\over g(x)}-{1\over g(y)}}$$ and $$g\left({1\over 1-{1\over x}-{1\over y}}\right) = {1\over 1-{1\over f(x)}-{1\over f(y)}}$$ Like suggested to avoid division by 0, I restrict the domain of the function at the cases where : $${1\over f(x)}+{1\over f(y)} \lt 1$$ $${1\over g(x)}+{1\over g(y)} \lt 1$$ (I know that ${1\over x}+{1\over y} \lt 1$ because I know (given the characteritics) that one function will give an output greater than its parameter and the other will give an output lower) This is a more restrictive version of my other question : Find a possible f(x) and g(x)","Do you think that it is possible to find a $f$ and a $g$ such that $\forall x \gt 1, \forall y \gt 1$  then $$f(x) \gt 0,$$ $$g(x) \gt 0,$$ $$f(x) \neq x,$$ $$g(x) \neq x,$$ and $$f\left({1\over 1-{1\over x}-{1\over y}}\right) = {1\over 1-{1\over g(x)}-{1\over g(y)}}$$ and $$g\left({1\over 1-{1\over x}-{1\over y}}\right) = {1\over 1-{1\over f(x)}-{1\over f(y)}}$$ Like suggested to avoid division by 0, I restrict the domain of the function at the cases where : $${1\over f(x)}+{1\over f(y)} \lt 1$$ $${1\over g(x)}+{1\over g(y)} \lt 1$$ (I know that ${1\over x}+{1\over y} \lt 1$ because I know (given the characteritics) that one function will give an output greater than its parameter and the other will give an output lower) This is a more restrictive version of my other question : Find a possible f(x) and g(x)",,['functions']
13,Find null space (kernel) and image of these linear transformations/maps of functions,Find null space (kernel) and image of these linear transformations/maps of functions,,"let $V$ be a vector space of functions $f : \mathbb{R} \to \mathbb{R}$. Consider the linear map: \begin{equation} \mathcal{A} : V \to V \mbox{ where } (\mathcal{A}f)(x) = f(x^2) \end{equation} and the linear map: \begin{equation} \mathcal{A} : V \to V \mbox{ where } (\mathcal{A}f)(x) = (x+1)f(x) \end{equation} I have proven they are in fact linear. I have no trouble determining the null space and image of other maps of functions, however, I couldn't quite get the hang of these. What I have (in a nutshell): (Kernel of the first map): Exists of all functions whose value is $0$ whenever $x \geq 0$. (Image of the first map): All functions which are symmetric in the $y$-axis. Because if we take $x = -a$, then $(\mathcal{A}f)(x) = f(x^2) = f((-a)^2) = f(a^2)$. (Kernel of second map): All funtions whose value is 0 whenever $x \neq -1$. What value those functions have whenever $x = -1$ does not matter as long as they are defined for this value of $x$. (Image of second map): In any case the set of functions $\{f(-1) = 0 \mid f \in V \}$ will be in the image space. If the function is not defined in $x = -1$, then the function will still be in the image space whenever the function is defined for some other value of $x$. The image space is therefore the union of the the set $\{f(-1) = 0 \mid f \in V \}$ and all functions that are not defined on $x = -1$ but are defined for some other value of $x$. Are these foundings correct?","let $V$ be a vector space of functions $f : \mathbb{R} \to \mathbb{R}$. Consider the linear map: \begin{equation} \mathcal{A} : V \to V \mbox{ where } (\mathcal{A}f)(x) = f(x^2) \end{equation} and the linear map: \begin{equation} \mathcal{A} : V \to V \mbox{ where } (\mathcal{A}f)(x) = (x+1)f(x) \end{equation} I have proven they are in fact linear. I have no trouble determining the null space and image of other maps of functions, however, I couldn't quite get the hang of these. What I have (in a nutshell): (Kernel of the first map): Exists of all functions whose value is $0$ whenever $x \geq 0$. (Image of the first map): All functions which are symmetric in the $y$-axis. Because if we take $x = -a$, then $(\mathcal{A}f)(x) = f(x^2) = f((-a)^2) = f(a^2)$. (Kernel of second map): All funtions whose value is 0 whenever $x \neq -1$. What value those functions have whenever $x = -1$ does not matter as long as they are defined for this value of $x$. (Image of second map): In any case the set of functions $\{f(-1) = 0 \mid f \in V \}$ will be in the image space. If the function is not defined in $x = -1$, then the function will still be in the image space whenever the function is defined for some other value of $x$. The image space is therefore the union of the the set $\{f(-1) = 0 \mid f \in V \}$ and all functions that are not defined on $x = -1$ but are defined for some other value of $x$. Are these foundings correct?",,"['linear-algebra', 'functions', 'linear-transformations']"
14,Same values for Gamma Function,Same values for Gamma Function,,"I was thinking about the Gamma function, which for an integer positive argument is nothing but the factorial function. Using the integral representation, namely $$\Gamma[x] = \int_0^{+\infty}\ t^{x-1}e^{-t}\ \text{d} t$$ one can extend the ""factorial"" also to the whole set or Real numbers, except for the negative integers which are all poles. Observing the plot of the function, and precisely with respect to the negative $X$ axis, I noticed that there are different values of $x$ in which $\Gamma[x]$ returns the same value: In my example, there are different values of $x$ for which $\Gamma[x] = 7$. Now the main question: assuming there are two values, $x$ and $x + h$ (for $h \neq 0$) such that $$\Gamma[x] = \Gamma[x + h]$$ how could I solve such an equation? And more generally: how could I find all the different values $x_{\alpha}$ for which $\Gamma[x_{\alpha}] = n$ for a fixed real $n$ ($n\neq \{-1; -2; -3; \ldots \}$)","I was thinking about the Gamma function, which for an integer positive argument is nothing but the factorial function. Using the integral representation, namely $$\Gamma[x] = \int_0^{+\infty}\ t^{x-1}e^{-t}\ \text{d} t$$ one can extend the ""factorial"" also to the whole set or Real numbers, except for the negative integers which are all poles. Observing the plot of the function, and precisely with respect to the negative $X$ axis, I noticed that there are different values of $x$ in which $\Gamma[x]$ returns the same value: In my example, there are different values of $x$ for which $\Gamma[x] = 7$. Now the main question: assuming there are two values, $x$ and $x + h$ (for $h \neq 0$) such that $$\Gamma[x] = \Gamma[x + h]$$ how could I solve such an equation? And more generally: how could I find all the different values $x_{\alpha}$ for which $\Gamma[x_{\alpha}] = n$ for a fixed real $n$ ($n\neq \{-1; -2; -3; \ldots \}$)",,"['functions', 'special-functions', 'factorial', 'gamma-function', 'integral-equations']"
15,System of equations for operations,System of equations for operations,,"Given a system with multiple equations, where we know the values and the result, but not the operations between the values: \begin{cases} 3 ⊕ 5 ⊙ 2 = 13 \\ 7 ⊕ 2 ⊙ 4 = 10 \\ 4 ⊕ 3 ⊙ 3 = 9 \end{cases} Is there an algorithmic way to deduce $⊕$ and $⊙$ (which in this case would be multiplication and subtraction, respectively)? Like a system of equations where the unknowns are the operations themselves? Does this have a name? Are there ways of calculating it?","Given a system with multiple equations, where we know the values and the result, but not the operations between the values: \begin{cases} 3 ⊕ 5 ⊙ 2 = 13 \\ 7 ⊕ 2 ⊙ 4 = 10 \\ 4 ⊕ 3 ⊙ 3 = 9 \end{cases} Is there an algorithmic way to deduce $⊕$ and $⊙$ (which in this case would be multiplication and subtraction, respectively)? Like a system of equations where the unknowns are the operations themselves? Does this have a name? Are there ways of calculating it?",,"['functions', 'systems-of-equations']"
16,"How to write this function in a ""well-formed"" way","How to write this function in a ""well-formed"" way",,"Given an input $0 \lt x \lt 1$, find $x$'s Nearest Integer Continued Fraction with structure $$x = a_0 \pm \cfrac{1}{a_1 \pm \cfrac{1}{a_2 \pm \cdots}}.$$ Then $$f(c) = a_0 + 1 \mp \cfrac{1}{a_1 + 1 \mp \cfrac{1}{a_2 + 1 \mp \cdots}}.$$ That is, replace each instance of ""$a_i +$"" with ""$(a_i + 1) -$"" and of ""$a_i -$"" with ""$(a_i + 1) +$"".  See here for the motivation behind the following questions. Questions I know that $f$ is a function, but I can't type this definition into a graphing calculator directly (that I know of). How can this function be written in a ""well-formed"" way, without english and with the ability to type it directly into a graphing calculator? Do there exist functions that can't be written in a ""well-formed"" way?  What are they called?","Given an input $0 \lt x \lt 1$, find $x$'s Nearest Integer Continued Fraction with structure $$x = a_0 \pm \cfrac{1}{a_1 \pm \cfrac{1}{a_2 \pm \cdots}}.$$ Then $$f(c) = a_0 + 1 \mp \cfrac{1}{a_1 + 1 \mp \cfrac{1}{a_2 + 1 \mp \cdots}}.$$ That is, replace each instance of ""$a_i +$"" with ""$(a_i + 1) -$"" and of ""$a_i -$"" with ""$(a_i + 1) +$"".  See here for the motivation behind the following questions. Questions I know that $f$ is a function, but I can't type this definition into a graphing calculator directly (that I know of). How can this function be written in a ""well-formed"" way, without english and with the ability to type it directly into a graphing calculator? Do there exist functions that can't be written in a ""well-formed"" way?  What are they called?",,"['functions', 'definition', 'continued-fractions']"
17,Sum of periodic functions,Sum of periodic functions,,"Let $f,g :\Bbb{R}\to\Bbb{R}$ be two periodic functions with periods $T$ and $T'$. What can be said, in general about the periodic behaviour of their sum $f+g$? If $T/T'$ is rational then a common multiple of $T$ and $T'$ should be a period for $f+g$. But what can we say about the smallest positive period of $f+g?$ And if, in the problem is too general,how about when $f,g$ are trigonometric functions of the form $\sin{(ax)}, \cos{(ax)}$, where $a$ is rational? For example, what is the smallest positive period for $f(x)=\sin {(35x)}+\cos{(42x)}$? , It is $\frac{2\pi}7$, where $7$ is $\gcd(35,42)$, i think. But how do we prove it is the smallest? Do you know some online materials specifically treating this? Especially on functions like $\sin{(mx)}+\sin{(nx)}$, $\sin{(mx)}+\cos{(nx)}$ where $m,n$ are integers, for the beginning.","Let $f,g :\Bbb{R}\to\Bbb{R}$ be two periodic functions with periods $T$ and $T'$. What can be said, in general about the periodic behaviour of their sum $f+g$? If $T/T'$ is rational then a common multiple of $T$ and $T'$ should be a period for $f+g$. But what can we say about the smallest positive period of $f+g?$ And if, in the problem is too general,how about when $f,g$ are trigonometric functions of the form $\sin{(ax)}, \cos{(ax)}$, where $a$ is rational? For example, what is the smallest positive period for $f(x)=\sin {(35x)}+\cos{(42x)}$? , It is $\frac{2\pi}7$, where $7$ is $\gcd(35,42)$, i think. But how do we prove it is the smallest? Do you know some online materials specifically treating this? Especially on functions like $\sin{(mx)}+\sin{(nx)}$, $\sin{(mx)}+\cos{(nx)}$ where $m,n$ are integers, for the beginning.",,['functions']
18,Question regarding Odd and Even function $g(x)=f(x)(f(x)+f(-x))$,Question regarding Odd and Even function,g(x)=f(x)(f(x)+f(-x)),Let $f\colon\mathbb{R} \to \mathbb{R}$. Define $g: \mathbb{R}\to \mathbb{R}$ by $g(x)=f(x)(f(x)+f(-x))$ Then which of following is/are correct? A. $g$ is even for all $f$ B. $g$ is odd for all $f$ C. $g$ is even if $f$ is even D. $g$ is even if $f$ is odd Taking $f(x)=\sin x$ eliminated option B. What if I take $f$ to be odd function then my $g=0$. Is $0$ a even function or odd function? Thanks,Let $f\colon\mathbb{R} \to \mathbb{R}$. Define $g: \mathbb{R}\to \mathbb{R}$ by $g(x)=f(x)(f(x)+f(-x))$ Then which of following is/are correct? A. $g$ is even for all $f$ B. $g$ is odd for all $f$ C. $g$ is even if $f$ is even D. $g$ is even if $f$ is odd Taking $f(x)=\sin x$ eliminated option B. What if I take $f$ to be odd function then my $g=0$. Is $0$ a even function or odd function? Thanks,,['functions']
19,"Given $f(x+y)=f(x)+f(y)$ and $f(0)=0$, what can we say about f(x)","Given  and , what can we say about f(x)",f(x+y)=f(x)+f(y) f(0)=0,"Given $f(x+y)=f(x)+f(y)$ and $f(0)=0$, what can we deduce about $f(x)$? I intend to say that $f(x)=x$, but find difficult to prove it. Is my guess correct, or wrong?","Given $f(x+y)=f(x)+f(y)$ and $f(0)=0$, what can we deduce about $f(x)$? I intend to say that $f(x)=x$, but find difficult to prove it. Is my guess correct, or wrong?",,"['functions', 'functional-equations']"
20,Computation of a sum using Stirling's approximation and Watson's lemma,Computation of a sum using Stirling's approximation and Watson's lemma,,$$Ω=\sum_{n=0}^{N-\frac{E}{\epsilon}}  \frac{Ν!}{\left(\frac{N-n-\frac{E}{\epsilon}}{2}\right)!\left(\frac{N-n+\frac{E}{\epsilon}}{2}\right)!n!}$$ I am supposed to calculate the above sum using first Stirling's approximation and then using Watson's lemma integrate for $x=n/N$. The point is to find the natural logarithm of Ω and the above are hints.,$$Ω=\sum_{n=0}^{N-\frac{E}{\epsilon}}  \frac{Ν!}{\left(\frac{N-n-\frac{E}{\epsilon}}{2}\right)!\left(\frac{N-n+\frac{E}{\epsilon}}{2}\right)!n!}$$ I am supposed to calculate the above sum using first Stirling's approximation and then using Watson's lemma integrate for $x=n/N$. The point is to find the natural logarithm of Ω and the above are hints.,,"['functions', 'approximation', 'entropy']"
21,Existence of a holomorphic function in the open disc $D=\{z\in \mathbb{C}: |z|<1\}$.,Existence of a holomorphic function in the open disc .,D=\{z\in \mathbb{C}: |z|<1\},"Let $D=\{z\in \mathbb{C}: |z|<1\}$. Then there exists a holomorphic function $$f:D\to \bar{D}$$ with $f(0)=0$ with the property $f'(0)=1/2$ $|f(1/3)|=1/4$ $f(1/3)=1/2$ $|f'(0)|=\sec{\pi/6}$ I have no idea how to come up with such a fuction, which satisfies atmost two of the above conditions as there are contradictory statements.  I tried to guess some but it does not work. Is there any general rule to solve this kind of problems? Which options are true? Please help me out. Any hint is sufficient. edit - Answer says $1$ and $2$ are correct.","Let $D=\{z\in \mathbb{C}: |z|<1\}$. Then there exists a holomorphic function $$f:D\to \bar{D}$$ with $f(0)=0$ with the property $f'(0)=1/2$ $|f(1/3)|=1/4$ $f(1/3)=1/2$ $|f'(0)|=\sec{\pi/6}$ I have no idea how to come up with such a fuction, which satisfies atmost two of the above conditions as there are contradictory statements.  I tried to guess some but it does not work. Is there any general rule to solve this kind of problems? Which options are true? Please help me out. Any hint is sufficient. edit - Answer says $1$ and $2$ are correct.",,"['complex-analysis', 'functions']"
22,"optimal hill ""rank"" cannot be solved?","optimal hill ""rank"" cannot be solved?",,"Okay so I was thinking about the following problem today: We have a guy who is h tall stand upon a paraboloid shaped hill of the form $z=-ar^n$ How far away (in r) does his friend who is also h tall have to stand so that they just barely can't see each other (assuming they have eyes at h and nothing visible above it). This yields the equation for the line between their eyes (we can move to 2D cartesian): $z = -ar_0^{n-1}r+h$ Where $r_0$ is the location of his friend. Solving these for the case where they have only one intersection yields: $r_0=n^{\frac{1}{n-1}}\left(\frac{h}{a(n-1)}\right)^{1/n}$ Now an interesting question is whether there is some special hill ""rank"" such as a hill which minimizes the distance $r_0$. It is clear that $r_0$ tends to infinity as $n\rightarrow1$ so that is not interesting. Plotting $r_0$ as a function of n reveals that it indeed has a minimum but only if $a\gt h$. Trying to to solve for this minimum seems impossible. The derivative with respect to n is really messy but finding it's root boils down to the equation: $(n-1)^2\log\left(\frac{h}{a(n-1)}\right)+n^2\log(n) = 0$ Which I cant solve and Mathematica (at least with me using it) can't seem to solve it either. Anything on how this might be solved?","Okay so I was thinking about the following problem today: We have a guy who is h tall stand upon a paraboloid shaped hill of the form $z=-ar^n$ How far away (in r) does his friend who is also h tall have to stand so that they just barely can't see each other (assuming they have eyes at h and nothing visible above it). This yields the equation for the line between their eyes (we can move to 2D cartesian): $z = -ar_0^{n-1}r+h$ Where $r_0$ is the location of his friend. Solving these for the case where they have only one intersection yields: $r_0=n^{\frac{1}{n-1}}\left(\frac{h}{a(n-1)}\right)^{1/n}$ Now an interesting question is whether there is some special hill ""rank"" such as a hill which minimizes the distance $r_0$. It is clear that $r_0$ tends to infinity as $n\rightarrow1$ so that is not interesting. Plotting $r_0$ as a function of n reveals that it indeed has a minimum but only if $a\gt h$. Trying to to solve for this minimum seems impossible. The derivative with respect to n is really messy but finding it's root boils down to the equation: $(n-1)^2\log\left(\frac{h}{a(n-1)}\right)+n^2\log(n) = 0$ Which I cant solve and Mathematica (at least with me using it) can't seem to solve it either. Anything on how this might be solved?",,"['functions', 'logarithms']"
23,Order of Operations for Horizontal Transformations,Order of Operations for Horizontal Transformations,,"We know that when we want to combine two horizontal transformations, specifically that of translating and stretching a function, we have to translate $f(x)$ first, and then afterwards stretch it. Following the normal order of operations, you would expect to resolve ‘$qx$’ (the stretching) before ‘$+ d$’ (the translation) but you resolve the transformation in the opposite order. My question is, what if we explicitly try to do the two in reverse order? If the graph $y = f(x)$ is transformed by applying first a horizontal stretch factor of  $q$ relative to the y-axis, then a horizontal translation $d$ to the left, what is the equation of the resulting graph? I would go with: $$y = f\left(\frac{1}{q}\big(x + d\big)\right)$$ ...as you are stretching $f(x)$ first, meaning that you replace $x$ with $x/q$, and then translating $f(x)$, replacing $x$ with $(x + d)$ to give the above equation. But according to the rule, one would expect it to be: $$y = f\left(\frac{x}{q} + d\right)$$ Which equation is correct?","We know that when we want to combine two horizontal transformations, specifically that of translating and stretching a function, we have to translate $f(x)$ first, and then afterwards stretch it. Following the normal order of operations, you would expect to resolve ‘$qx$’ (the stretching) before ‘$+ d$’ (the translation) but you resolve the transformation in the opposite order. My question is, what if we explicitly try to do the two in reverse order? If the graph $y = f(x)$ is transformed by applying first a horizontal stretch factor of  $q$ relative to the y-axis, then a horizontal translation $d$ to the left, what is the equation of the resulting graph? I would go with: $$y = f\left(\frac{1}{q}\big(x + d\big)\right)$$ ...as you are stretching $f(x)$ first, meaning that you replace $x$ with $x/q$, and then translating $f(x)$, replacing $x$ with $(x + d)$ to give the above equation. But according to the rule, one would expect it to be: $$y = f\left(\frac{x}{q} + d\right)$$ Which equation is correct?",,"['functions', 'graphing-functions', 'transformation']"
24,Multiple composition of function notation,Multiple composition of function notation,,"Let's say I have a set of $n$ functions of $m$ variables: $$f_1(x_1,x_2,\ldots,x_m),f_2(x_1,x_2,\ldots,x_m),\ldots,f_n(x_1,x_2,\ldots,x_m)$$ Is there a notation that represents $(f_1 \circ f_2 \circ \cdots \circ f_n)(x_1,x_2,\ldots,x_m)$? Basically, is there an equivalent of $\sum$, $\prod$, $\bigcup$, etc. for function composition? I have looked at several other threads on this site regarding this, but I have not found one containing a definite answer that satisfies me.","Let's say I have a set of $n$ functions of $m$ variables: $$f_1(x_1,x_2,\ldots,x_m),f_2(x_1,x_2,\ldots,x_m),\ldots,f_n(x_1,x_2,\ldots,x_m)$$ Is there a notation that represents $(f_1 \circ f_2 \circ \cdots \circ f_n)(x_1,x_2,\ldots,x_m)$? Basically, is there an equivalent of $\sum$, $\prod$, $\bigcup$, etc. for function composition? I have looked at several other threads on this site regarding this, but I have not found one containing a definite answer that satisfies me.",,"['functions', 'notation']"
25,Translating Equations to Algorithms,Translating Equations to Algorithms,,"I can't understand equations. But I'm a software engineer. I think the brevity of the equation is confusing to me where a program spells it all out. Trying to translate the equation for a bezier curve into javascript. The equation on wikipedia appears as- Translating this to javascript looks like this- function B(t, p0, p1, p2, p3) {   return Math.pow((1 - t), 3) * p0 + 3 * Math.pow((1 - t), 2) * p1 + 3 * (1 - t) * Math.pow(t, 2) * p2 + Math.pow(t, 3) * p3; } But this doesn't make sense. A point is made up of an x and y coordinate. How can multiple values be represented by a single value in a meaningful way? How is this equation usable?","I can't understand equations. But I'm a software engineer. I think the brevity of the equation is confusing to me where a program spells it all out. Trying to translate the equation for a bezier curve into javascript. The equation on wikipedia appears as- Translating this to javascript looks like this- function B(t, p0, p1, p2, p3) {   return Math.pow((1 - t), 3) * p0 + 3 * Math.pow((1 - t), 2) * p1 + 3 * (1 - t) * Math.pow(t, 2) * p2 + Math.pow(t, 3) * p3; } But this doesn't make sense. A point is made up of an x and y coordinate. How can multiple values be represented by a single value in a meaningful way? How is this equation usable?",,"['functions', 'bezier-curve']"
26,How do we address a function whose values are again functions?,How do we address a function whose values are again functions?,,"One may call a function whose values are functions simply a function-valued function. But is there a canonical name for such an object? A $k$-form on some open $A \subset \mathbb{R}^{n}$ is an example of such an object, for it assigns to every $x \in A$ exactly one alternating $k$-tensor on the tangent space $T_{x}(\mathbb{R}^{n})$, which is a function $\prod_{1}^{k}T_{x}(\mathbb{R}^{n}) \to \mathbb{R}$.","One may call a function whose values are functions simply a function-valued function. But is there a canonical name for such an object? A $k$-form on some open $A \subset \mathbb{R}^{n}$ is an example of such an object, for it assigns to every $x \in A$ exactly one alternating $k$-tensor on the tangent space $T_{x}(\mathbb{R}^{n})$, which is a function $\prod_{1}^{k}T_{x}(\mathbb{R}^{n}) \to \mathbb{R}$.",,"['functions', 'differential-geometry', 'terminology']"
27,Domain values of inverse funtion,Domain values of inverse funtion,,If I'm plotting $$y=3e^{{x\over3}+1}$$ from $x=0$ to $x=1$ and on the same axes I want to plot its inverse $$y=3\ln\left({x\over3}\right)-3$$ but only for the domain values of $x$ given by the range of $f$ Would the inverse domain range be $x=8.15$ to $x=11.38$?,If I'm plotting $$y=3e^{{x\over3}+1}$$ from $x=0$ to $x=1$ and on the same axes I want to plot its inverse $$y=3\ln\left({x\over3}\right)-3$$ but only for the domain values of $x$ given by the range of $f$ Would the inverse domain range be $x=8.15$ to $x=11.38$?,,['functions']
28,Is there a name for this property of multiplication (and other functions)?,Is there a name for this property of multiplication (and other functions)?,,"Suppose $x,y \in \mathbb{R_+}, x<y$, and $ 0 < \varepsilon \leq (y-x)/2$. It seems to me that $xy < (x+\varepsilon)(y-\varepsilon)$ and equivalently that  $(x+\varepsilon)(y-\varepsilon)$ is strictly increasing in epsilon for $ 0 \leq \varepsilon \leq (y-x)/2$. Is there a name for this property? More generally, is there a name for this kind of property of a function where moving the inputs closer together while preserving their sum (or perhaps preserving some other analogous property) increases the function? **Edited from previous error:**As example of a similar (but opposite) property, it looks like the euclidean norm of a two dimensional vector (this probably could be extended to higher dimensions)  is decreasing as the coordinates are brought closer together while keeping their sum (i.e. the taxicab norm) constant. It seems like this has something to do with convexity but I'm not sure what the relationship is and if it is directly related or more of an analogy.","Suppose $x,y \in \mathbb{R_+}, x<y$, and $ 0 < \varepsilon \leq (y-x)/2$. It seems to me that $xy < (x+\varepsilon)(y-\varepsilon)$ and equivalently that  $(x+\varepsilon)(y-\varepsilon)$ is strictly increasing in epsilon for $ 0 \leq \varepsilon \leq (y-x)/2$. Is there a name for this property? More generally, is there a name for this kind of property of a function where moving the inputs closer together while preserving their sum (or perhaps preserving some other analogous property) increases the function? **Edited from previous error:**As example of a similar (but opposite) property, it looks like the euclidean norm of a two dimensional vector (this probably could be extended to higher dimensions)  is decreasing as the coordinates are brought closer together while keeping their sum (i.e. the taxicab norm) constant. It seems like this has something to do with convexity but I'm not sure what the relationship is and if it is directly related or more of an analogy.",,"['functions', 'arithmetic']"
29,Characterize in terms of fibre,Characterize in terms of fibre,,"I am not familiar with the notion ""characterize"" in the following context. Does this mean to redefine or?.... Any help would be appreciated. Thank you. For a function $f:X\to Y$, and y an element of Y, let $f^{-1}(y) := \{ x \in X| f(x) = y\}$. This is sometimes called the ""fibre"" of f over y. Problem: characterize 1 to 1 and onto functions in terrm of the fibres","I am not familiar with the notion ""characterize"" in the following context. Does this mean to redefine or?.... Any help would be appreciated. Thank you. For a function $f:X\to Y$, and y an element of Y, let $f^{-1}(y) := \{ x \in X| f(x) = y\}$. This is sometimes called the ""fibre"" of f over y. Problem: characterize 1 to 1 and onto functions in terrm of the fibres",,"['functions', 'notation', 'definition']"
30,Find two homeomorphic topological spaces and a bijective continuous map between them which is not homeomorphism.,Find two homeomorphic topological spaces and a bijective continuous map between them which is not homeomorphism.,,"I'm aware that it is duplicate, but I'd like to know whether my example is appropriate or not. Let our function $f$ be on the set $\mathbb{Q}\cap\mathbb{Z}$ induced by standard topology of a line. I'm going to construct a bijection from $\mathbb{Q}\cap\left((0,1)\cup(1,2)\right)$ to $\mathbb{Q}\cap(0,1)$. The rest from $(2n,2n+1)\cup(2n+1,2n+2)$ to $(n,n+1)$ is similar. My idea is that if the restriction of $f$ on $(n,n+1)$ is monotonous then we are done, because $f$ becomes continuous but its inverse does not. Let's denote elements from $(0,1)$ in domain as $\dfrac{a'}{b}$ and elements from $(1,2)$ in domain decrease by $1$ and denote as $\dfrac{a''}{b}$ . $$f(\dfrac{1'}{2})=\dfrac{1}{2}\text{and}f(\dfrac{1''}{2})=\dfrac{1}{3}$$ $$f(\dfrac{1'}{3})=\dfrac{1}{4}\text{and}f(\dfrac{1''}{3})=\dfrac{1}{5}$$ $$f(\dfrac{2'}{3})=\dfrac{2}{3}\text{and}f(\dfrac{2''}{3})=\dfrac{3}{4}$$ $$f(\dfrac{1'}{4})=\dfrac{1}{6}\text{and}f(\dfrac{1''}{4})=\dfrac{1}{7}$$ $$...\text{so.on}...$$ So $f(x'')=y$ means $f(1+x)=y$ and $f(x')=f(x)$ The idea is next: We start from the elements with smallest denominator and map them on element which satisfies the condition of monotone on the one hand, and has the smallest possible denominator on the other hand. I think that this idea will lead us to surjectivity, but still I doubt. P.S. There is an assumption that for $\dfrac{p}{q}$ we have $\gcd(p,q)=1$","I'm aware that it is duplicate, but I'd like to know whether my example is appropriate or not. Let our function $f$ be on the set $\mathbb{Q}\cap\mathbb{Z}$ induced by standard topology of a line. I'm going to construct a bijection from $\mathbb{Q}\cap\left((0,1)\cup(1,2)\right)$ to $\mathbb{Q}\cap(0,1)$. The rest from $(2n,2n+1)\cup(2n+1,2n+2)$ to $(n,n+1)$ is similar. My idea is that if the restriction of $f$ on $(n,n+1)$ is monotonous then we are done, because $f$ becomes continuous but its inverse does not. Let's denote elements from $(0,1)$ in domain as $\dfrac{a'}{b}$ and elements from $(1,2)$ in domain decrease by $1$ and denote as $\dfrac{a''}{b}$ . $$f(\dfrac{1'}{2})=\dfrac{1}{2}\text{and}f(\dfrac{1''}{2})=\dfrac{1}{3}$$ $$f(\dfrac{1'}{3})=\dfrac{1}{4}\text{and}f(\dfrac{1''}{3})=\dfrac{1}{5}$$ $$f(\dfrac{2'}{3})=\dfrac{2}{3}\text{and}f(\dfrac{2''}{3})=\dfrac{3}{4}$$ $$f(\dfrac{1'}{4})=\dfrac{1}{6}\text{and}f(\dfrac{1''}{4})=\dfrac{1}{7}$$ $$...\text{so.on}...$$ So $f(x'')=y$ means $f(1+x)=y$ and $f(x')=f(x)$ The idea is next: We start from the elements with smallest denominator and map them on element which satisfies the condition of monotone on the one hand, and has the smallest possible denominator on the other hand. I think that this idea will lead us to surjectivity, but still I doubt. P.S. There is an assumption that for $\dfrac{p}{q}$ we have $\gcd(p,q)=1$",,"['general-topology', 'functions', 'proof-verification', 'examples-counterexamples']"
31,Injective function on the domain of natural numbers,Injective function on the domain of natural numbers,,"Find all injective functions $f:N \rightarrow N$ such that $$f(f(m)+f(n))=f(f(m))+f(n)$$ Where $m,n$ are natural numbers.","Find all injective functions $f:N \rightarrow N$ such that $$f(f(m)+f(n))=f(f(m))+f(n)$$ Where $m,n$ are natural numbers.",,['functions']
32,an integral equation in a function with two arguments,an integral equation in a function with two arguments,,"Say we are given $C(s,t)=\min(s,t)+\zeta st$. How can we solve $$g(s,t)=C(s,t)+\lambda \int_0^1g(s,u)C(u,t)du.$$ Looking into some text books on integral equations I see that most of the kernels, $C$, that they exemplify are degenerate (separable) hence solving such an equation becomes easy. But how could one proceed in this case where the kernel is not separable?","Say we are given $C(s,t)=\min(s,t)+\zeta st$. How can we solve $$g(s,t)=C(s,t)+\lambda \int_0^1g(s,u)C(u,t)du.$$ Looking into some text books on integral equations I see that most of the kernels, $C$, that they exemplify are degenerate (separable) hence solving such an equation becomes easy. But how could one proceed in this case where the kernel is not separable?",,"['calculus', 'functional-analysis', 'functions']"
33,Technical name for an almost-monotonic function,Technical name for an almost-monotonic function,,"I'm wondering if there’s a technical name or short phrase that describes a function that’s monotonic, subject to some uniformly bounded amount of backtracking. $\exists \epsilon \forall x , y : y \gt x \Rightarrow f(y) \gt f(x) - \epsilon $ A simple example of this would be if you start with any monotonic function and add a fixed amount of error, jitter or oscillation. Concretely, if our epsilon is 20, you’d have a function that, once it reaches 100, we know it will never drop to 80 (or less) again. Extra credit: There's a similar form of near-monotonicity where the limit is on the function’s domain rather than the range. If this also has a name, I’d be curious to hear it—though the case I really care about is the one I described above. This other form looks like this: $\exists \delta \forall x , y : y \gt x + \delta \Rightarrow f(y) \gt f(x)$","I'm wondering if there’s a technical name or short phrase that describes a function that’s monotonic, subject to some uniformly bounded amount of backtracking. $\exists \epsilon \forall x , y : y \gt x \Rightarrow f(y) \gt f(x) - \epsilon $ A simple example of this would be if you start with any monotonic function and add a fixed amount of error, jitter or oscillation. Concretely, if our epsilon is 20, you’d have a function that, once it reaches 100, we know it will never drop to 80 (or less) again. Extra credit: There's a similar form of near-monotonicity where the limit is on the function’s domain rather than the range. If this also has a name, I’d be curious to hear it—though the case I really care about is the one I described above. This other form looks like this: $\exists \delta \forall x , y : y \gt x + \delta \Rightarrow f(y) \gt f(x)$",,"['functions', 'terminology']"
34,Looking for a bound on a function involving $\sinh$,Looking for a bound on a function involving,\sinh,"Fix $T > 0$ and let $t \in (0,T)$ let $c > 1$ be a constant (which may be bigger than $T$). Consider the function $$f(c,t,T) = \frac{\sinh ((T-t)c)}{\sinh (Tc)}.$$ I am looking for a bound of the form $$f(c,t,T) \leq {K(t,T)}{c^{-\frac 12}} \qquad \text{for $t \in (0,T)$}$$ where $K(t,T)$ is a constant that depends on $t$ and $T$ (preferably in a continuous or a nice way). Can such a bound be possible? By using half-angle formula, we can write  $$f(c,t,T)=\cosh(tc)-\coth(Tc)\sinh(tc)$$ but I just don't see how to estimate it.","Fix $T > 0$ and let $t \in (0,T)$ let $c > 1$ be a constant (which may be bigger than $T$). Consider the function $$f(c,t,T) = \frac{\sinh ((T-t)c)}{\sinh (Tc)}.$$ I am looking for a bound of the form $$f(c,t,T) \leq {K(t,T)}{c^{-\frac 12}} \qquad \text{for $t \in (0,T)$}$$ where $K(t,T)$ is a constant that depends on $t$ and $T$ (preferably in a continuous or a nice way). Can such a bound be possible? By using half-angle formula, we can write  $$f(c,t,T)=\cosh(tc)-\coth(Tc)\sinh(tc)$$ but I just don't see how to estimate it.",,"['functional-analysis', 'functions', 'hyperbolic-functions']"
35,What are some simple (or elegant) functions that satisfy these conditions? [closed],What are some simple (or elegant) functions that satisfy these conditions? [closed],,"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 9 years ago . Improve this question I wish to have a function that maps [0,1] to [0,1].  I also require that f(0) = 0 and f(1) = 1. Also, I would like the function to be of sigmoidal shape, such as this: The above function is ok, but I would like the function to be flatter at the bottom and the top, and have a steeper ascent. Ideally I would be able to adjust the parameters of the function to control how ""flat"" it is. Optional:  If I could also adjust the parameters so that I can choose where the inflection occurs that would be great.  (e.g. in the above function the inflection seems to occur at x = 0.5, but ideally I would be able to shift this to say x = 0.1, or x = 0.05 etc.)","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 9 years ago . Improve this question I wish to have a function that maps [0,1] to [0,1].  I also require that f(0) = 0 and f(1) = 1. Also, I would like the function to be of sigmoidal shape, such as this: The above function is ok, but I would like the function to be flatter at the bottom and the top, and have a steeper ascent. Ideally I would be able to adjust the parameters of the function to control how ""flat"" it is. Optional:  If I could also adjust the parameters so that I can choose where the inflection occurs that would be great.  (e.g. in the above function the inflection seems to occur at x = 0.5, but ideally I would be able to shift this to say x = 0.1, or x = 0.05 etc.)",,"['functions', 'recreational-mathematics']"
36,Area & order-preserving function transformations,Area & order-preserving function transformations,,"Consider a bounded function $f(x \in \mathbf{R}) \rightarrow \mathbf{R}$ with bounded support $\left[0,L\right]$ (illustration below). What type of transformations $g(f(x))$ guarantee that: Area under the curve between $x=0$ and $x=L$ of $g\left(f(x)\right)$ is equal to that of $f(x)$ For any pair of $(x_1,x_2)$ points, the ordering of $g\left(f(x_1)\right)$ and $g\left(f(x_2)\right)$ is that of  $f(x_1)$ and $f(x_2)$ $g\left(f(x)\right)\ge0 \, \forall x$, noting that $f(x)\ge 0$ as well. For example, what subset of linear transformations $g(f(x)) = A f(x) + B$ satisfy this property? What about non-linear transformations? Any good (algebraically simple) examples of the latter?","Consider a bounded function $f(x \in \mathbf{R}) \rightarrow \mathbf{R}$ with bounded support $\left[0,L\right]$ (illustration below). What type of transformations $g(f(x))$ guarantee that: Area under the curve between $x=0$ and $x=L$ of $g\left(f(x)\right)$ is equal to that of $f(x)$ For any pair of $(x_1,x_2)$ points, the ordering of $g\left(f(x_1)\right)$ and $g\left(f(x_2)\right)$ is that of  $f(x_1)$ and $f(x_2)$ $g\left(f(x)\right)\ge0 \, \forall x$, noting that $f(x)\ge 0$ as well. For example, what subset of linear transformations $g(f(x)) = A f(x) + B$ satisfy this property? What about non-linear transformations? Any good (algebraically simple) examples of the latter?",,"['calculus', 'linear-algebra', 'geometry', 'functions', 'polynomials']"
37,"given the following two conditions, find $f(x,y)$","given the following two conditions, find","f(x,y)","Suppose that a function $f$ defined on $\mathbb R^2$ satisfies the following conditions: $f(x+t,y)=f(x,y)+ty$; $f(x,t+y)=f(x,y)+tx$; $f(0,0)=k$; then for all $x,y  \in\mathbb R$,  $f(x,y)=$ a) $k(x+y)$ b) $k-xy$ c) $k+xy$ I tried in the following way : $$ f(x+t,y+t)=f(x,y+t)+t(y+t)=f(x,y)+tx+t(y+t)$$ so, $$f(x+t,y+t)-f(x,y)=(x+t)(y+t)-xy$$ this implies $f(x,y)=k+xy$. But could anyone solve this in a more methodical way, so that i can solve similar type of problems?","Suppose that a function $f$ defined on $\mathbb R^2$ satisfies the following conditions: $f(x+t,y)=f(x,y)+ty$; $f(x,t+y)=f(x,y)+tx$; $f(0,0)=k$; then for all $x,y  \in\mathbb R$,  $f(x,y)=$ a) $k(x+y)$ b) $k-xy$ c) $k+xy$ I tried in the following way : $$ f(x+t,y+t)=f(x,y+t)+t(y+t)=f(x,y)+tx+t(y+t)$$ so, $$f(x+t,y+t)-f(x,y)=(x+t)(y+t)-xy$$ this implies $f(x,y)=k+xy$. But could anyone solve this in a more methodical way, so that i can solve similar type of problems?",,"['real-analysis', 'algebra-precalculus', 'functional-analysis', 'functions']"
38,How is $\cos^3{x}$ an odd function while $\sin^3{x}$ an even function?,How is  an odd function while  an even function?,\cos^3{x} \sin^3{x},"We know that for odd function $f(-x) = -f(x)$ and for even function $f(-x) = f(x)$. Therefore, $\cos^3(-x) = \cos(-x)\cos(-x)\cos(-x) = \cos{x}\cos{x}\cos{x} = \cos^3{x}$ (i.e. $\cos^3{x}$ must be even function). And similarly, since $\sin(-x) = - \sin{x}$, $\sin^3{x}$ must be odd function. But in my text book they claimed that $\cos^3{x}$ is odd function while $\sin^3{x}$ is even function. Maybe I have done something wrong, but I am unable to understand how $\cos^3{x}$ is an odd function while $\sin^3{x}$ an even function. Please help. Thanks.","We know that for odd function $f(-x) = -f(x)$ and for even function $f(-x) = f(x)$. Therefore, $\cos^3(-x) = \cos(-x)\cos(-x)\cos(-x) = \cos{x}\cos{x}\cos{x} = \cos^3{x}$ (i.e. $\cos^3{x}$ must be even function). And similarly, since $\sin(-x) = - \sin{x}$, $\sin^3{x}$ must be odd function. But in my text book they claimed that $\cos^3{x}$ is odd function while $\sin^3{x}$ is even function. Maybe I have done something wrong, but I am unable to understand how $\cos^3{x}$ is an odd function while $\sin^3{x}$ an even function. Please help. Thanks.",,['functions']
39,How would I simplify this function,How would I simplify this function,,"I'm learning about the function transformations, and was wondering how I would simplify this function: $$f(x)=7\left(\frac{-1}{6 (x-1)}\right)^2+1 $$ It includes both the base function and the transformation. Should I square everything in brackets, or multiply by brackets and then add?","I'm learning about the function transformations, and was wondering how I would simplify this function: $$f(x)=7\left(\frac{-1}{6 (x-1)}\right)^2+1 $$ It includes both the base function and the transformation. Should I square everything in brackets, or multiply by brackets and then add?",,"['algebra-precalculus', 'functions']"
40,A discrepancy in the total number of conjunctive normal forms and the number of distinct boolean functions.,A discrepancy in the total number of conjunctive normal forms and the number of distinct boolean functions.,,"Consider a set of truth literals $C$. The set $\{\text T, \text F\}^{\mathcal{P}(C)}$ is the set of all boolean functions over all subsets of $C$. This comes from the notation $\mathcal{Y}^\mathcal{X}$ which is the set of all functions $f : \mathcal{X} \to \mathcal{Y}$. The set of all conjugations will be a subset of $\{\text T, \text F\}^{\mathcal{P}(C)}$ which I'd like to call $\mathcal{H}_C$. $$ \mathcal{H}_C = \left\{\ \bigwedge_c^X c \ \middle| \ X \in \mathcal{P}(C) \ \right\} $$ I still can't see a simpler notation to describe the set. This still feels somewhat burdensome, but it's all I've got for now. Some things to note about $\mathcal{H}_C$ if we allow $C$ to contain all of its complimentary elements. $$ \mathcal{L}_C = \left\{\ \bigvee_c^X c \ \middle| \ X \in \mathcal{P}(C) \ \right\} $$ $$ \mathcal{L}_{\mathcal{H}_C} \equiv \mathcal{H}_{\mathcal{L}_C} $$ Any boolean function over $C$ will have at least one corresponding element in each of those two sets and there are no boolean functions over $C$ that are unrepresented by either of them. This follows from the fact that every boolean function has a conjunctive normal form and a disjunctive normal form. The set of all disjunctive normal forms is $\mathcal{L}_{\mathcal{H}_C}$ and the set of all conjunctive normal forms is $\mathcal{H}_{\mathcal{L}_C}$. Now the fruit of the question: For any set of literals $C$, $\mathcal{H}_{\mathcal{L}_C} \subseteq \{\text T, \text F\}^{\mathcal{P}(C)}$. $$ \left|\mathcal{H}_C\right| = \left|\mathcal{L}_C\right| = \sqrt{3}^{|C|} \\ \left|\mathcal{H}_{\mathcal{L}_C}\right| = \left|\mathcal{L}_{\mathcal{H}_C}\right| = \sqrt{3^{\sqrt{3}^{|C|}}} \\ \left|\{\text T, \text F\}^{\mathcal{P}(C)}\right| = \sqrt{2^{\sqrt{2}^{|C|}}} $$ Any conjunction can be described as a tuple of states for every non-complementary literal. Each literal can be either absent, present, or its complement can be present. Half of the literals are compliments of other literals, so they're excluded. Therefore, there are $3^{\frac{|C|}{2}}$ conjugations. The same applies to disjunctions. Combining this logic, we have a total number of conjunctive normal forms over $C$. The same applies for disjunctive normal forms. You can easily see that the number of distinct functions is smaller than the number of conjunctive normal forms. However, shouldn't there be an equal number of distinct functions as there are normal forms? I've read before that every distinct function has exactly one conjunctive normal form. Is this wrong or am I wrong in something I've presented above? As seen on Wikipedia right now Any particular Boolean function can be represented by one and only one full disjunctive normal form... Above, I showed that the number of disjunctive normal forms and the number of conjunctive normal forms are the same, so either set can be used for this argument. Edit: The total number of boolean functions was accounting for literals and their complements both being independent variables. This is not the case so the number was rooted. The question still stands: Why are there are more forms than functions?","Consider a set of truth literals $C$. The set $\{\text T, \text F\}^{\mathcal{P}(C)}$ is the set of all boolean functions over all subsets of $C$. This comes from the notation $\mathcal{Y}^\mathcal{X}$ which is the set of all functions $f : \mathcal{X} \to \mathcal{Y}$. The set of all conjugations will be a subset of $\{\text T, \text F\}^{\mathcal{P}(C)}$ which I'd like to call $\mathcal{H}_C$. $$ \mathcal{H}_C = \left\{\ \bigwedge_c^X c \ \middle| \ X \in \mathcal{P}(C) \ \right\} $$ I still can't see a simpler notation to describe the set. This still feels somewhat burdensome, but it's all I've got for now. Some things to note about $\mathcal{H}_C$ if we allow $C$ to contain all of its complimentary elements. $$ \mathcal{L}_C = \left\{\ \bigvee_c^X c \ \middle| \ X \in \mathcal{P}(C) \ \right\} $$ $$ \mathcal{L}_{\mathcal{H}_C} \equiv \mathcal{H}_{\mathcal{L}_C} $$ Any boolean function over $C$ will have at least one corresponding element in each of those two sets and there are no boolean functions over $C$ that are unrepresented by either of them. This follows from the fact that every boolean function has a conjunctive normal form and a disjunctive normal form. The set of all disjunctive normal forms is $\mathcal{L}_{\mathcal{H}_C}$ and the set of all conjunctive normal forms is $\mathcal{H}_{\mathcal{L}_C}$. Now the fruit of the question: For any set of literals $C$, $\mathcal{H}_{\mathcal{L}_C} \subseteq \{\text T, \text F\}^{\mathcal{P}(C)}$. $$ \left|\mathcal{H}_C\right| = \left|\mathcal{L}_C\right| = \sqrt{3}^{|C|} \\ \left|\mathcal{H}_{\mathcal{L}_C}\right| = \left|\mathcal{L}_{\mathcal{H}_C}\right| = \sqrt{3^{\sqrt{3}^{|C|}}} \\ \left|\{\text T, \text F\}^{\mathcal{P}(C)}\right| = \sqrt{2^{\sqrt{2}^{|C|}}} $$ Any conjunction can be described as a tuple of states for every non-complementary literal. Each literal can be either absent, present, or its complement can be present. Half of the literals are compliments of other literals, so they're excluded. Therefore, there are $3^{\frac{|C|}{2}}$ conjugations. The same applies to disjunctions. Combining this logic, we have a total number of conjunctive normal forms over $C$. The same applies for disjunctive normal forms. You can easily see that the number of distinct functions is smaller than the number of conjunctive normal forms. However, shouldn't there be an equal number of distinct functions as there are normal forms? I've read before that every distinct function has exactly one conjunctive normal form. Is this wrong or am I wrong in something I've presented above? As seen on Wikipedia right now Any particular Boolean function can be represented by one and only one full disjunctive normal form... Above, I showed that the number of disjunctive normal forms and the number of conjunctive normal forms are the same, so either set can be used for this argument. Edit: The total number of boolean functions was accounting for literals and their complements both being independent variables. This is not the case so the number was rooted. The question still stands: Why are there are more forms than functions?",,"['functions', 'boolean-algebra']"
41,Using the Intermediate Value Theorem to prove a statement about an equation true,Using the Intermediate Value Theorem to prove a statement about an equation true,,"I want to prove this statement true by using the IVF: For any real number $b > 2$, the equation $2^x = bx$ has a solution. Here are some questions I need help with answering: Define a function $f(x) = 2^x-bx$. How does finding a zero for $f(x)$ correspond to finding a solution to $2^x=bx$? (I know that $f(x)$ is continuous, but don't understand what the correlation is). Find a value $x$, when plugging it into $f$ gives a positive output, regardless of the value of the variable $b$, and Find a value $x$ when plugged into $f$, gives a negative output, as long as $b > 2$.","I want to prove this statement true by using the IVF: For any real number $b > 2$, the equation $2^x = bx$ has a solution. Here are some questions I need help with answering: Define a function $f(x) = 2^x-bx$. How does finding a zero for $f(x)$ correspond to finding a solution to $2^x=bx$? (I know that $f(x)$ is continuous, but don't understand what the correlation is). Find a value $x$, when plugging it into $f$ gives a positive output, regardless of the value of the variable $b$, and Find a value $x$ when plugged into $f$, gives a negative output, as long as $b > 2$.",,"['calculus', 'functions', 'proof-writing']"
42,Application of Stone Weierstrass theorem,Application of Stone Weierstrass theorem,,"Let $K$ be the unit circle in the complex plane and let $A$ be the algebra of all functions of the form $f(e^{i\theta})=\Sigma_{n=0}^{N}c_{n}e^{in\theta}$ ($\theta$ real). Prove that $A$ is uniformly dense in the space of continuous functions from unit circle to complex plane. This is a problem from Rudin. The things I have in mind for this is that I need to prove the  following: 1) $A$ separates points in $K$. 2) $A$ vanishes at no point of $K$. The solution manual says that : Note that $f(e^{i\theta})=e^{i\theta}\in A$ and for every $f\in A$, $\int_{0}^{2\pi}f(e^{i\theta})e^{i\theta}d\theta=0$. How do we prove these two things? the solution manual is not that obvious to me. I am reading Rudin on my own. please explain.","Let $K$ be the unit circle in the complex plane and let $A$ be the algebra of all functions of the form $f(e^{i\theta})=\Sigma_{n=0}^{N}c_{n}e^{in\theta}$ ($\theta$ real). Prove that $A$ is uniformly dense in the space of continuous functions from unit circle to complex plane. This is a problem from Rudin. The things I have in mind for this is that I need to prove the  following: 1) $A$ separates points in $K$. 2) $A$ vanishes at no point of $K$. The solution manual says that : Note that $f(e^{i\theta})=e^{i\theta}\in A$ and for every $f\in A$, $\int_{0}^{2\pi}f(e^{i\theta})e^{i\theta}d\theta=0$. How do we prove these two things? the solution manual is not that obvious to me. I am reading Rudin on my own. please explain.",,"['real-analysis', 'sequences-and-series', 'functions', 'uniform-convergence']"
43,A question about a notation used in the Folland Real Analysis,A question about a notation used in the Folland Real Analysis,,"This is the exercise 11 in the Folland Real Analysis. Could anyone tell me what it means by f(x,・) and f(・,y)? I have never seen such notations before...","This is the exercise 11 in the Folland Real Analysis. Could anyone tell me what it means by f(x,・) and f(・,y)? I have never seen such notations before...",,"['real-analysis', 'functions', 'notation']"
44,Number of functions from domain to codomain,Number of functions from domain to codomain,,"Let A and B be finite sets. Let a be the size of A. Let b be the size of B. Assume 0 < a < b. (a) How many functions are there with domain A and co-domain B? (b) How many one-to-one functions are there with domain A and co-domain B? (c) How many one-to-one functions are there with domain B and co-domain A? (NOTE- domain is B, co-domain is A.) (d) How many onto functions are there with domain A and co-domain B? How are we supposed to figure out how many functions there are? Couldn't it be pretty much infinite since each function can do things differently and still get the same value? How would we solve a)? Would both b) be a or !b/!a? and would c) be 0? I understand that d) should be 0, but what if the domain was B and co-domain A? how would we solve that then? -thanks and any explanation would be appreciated","Let A and B be finite sets. Let a be the size of A. Let b be the size of B. Assume 0 < a < b. (a) How many functions are there with domain A and co-domain B? (b) How many one-to-one functions are there with domain A and co-domain B? (c) How many one-to-one functions are there with domain B and co-domain A? (NOTE- domain is B, co-domain is A.) (d) How many onto functions are there with domain A and co-domain B? How are we supposed to figure out how many functions there are? Couldn't it be pretty much infinite since each function can do things differently and still get the same value? How would we solve a)? Would both b) be a or !b/!a? and would c) be 0? I understand that d) should be 0, but what if the domain was B and co-domain A? how would we solve that then? -thanks and any explanation would be appreciated",,"['functions', 'discrete-mathematics']"
45,How many such functions are possible?,How many such functions are possible?,,"Let $f$ be a function from $\{1,2,3,\dots,10\}$ to $\Bbb{R}$ such that $$\left(\sum\limits_{i=1}^{10}{\frac{|f(i)|}{2^i}}\right)^2=\left(\sum\limits_{i=1}^{10}{|f(i)|^2}\right)\left(\sum\limits_{i=1}^{10}{\frac{1}{4^i}}\right)$$ How many such $f$ are possible? I used the Cauchy-Schwarz inequality to conclude that this condition would imply $2|f(1)|=2^2|f(2)|=\dots=2^{10}|f(10)|$, and hence there are uncountably many such functions possible. However, I am not sure of this. Any help solving this question would be great.","Let $f$ be a function from $\{1,2,3,\dots,10\}$ to $\Bbb{R}$ such that $$\left(\sum\limits_{i=1}^{10}{\frac{|f(i)|}{2^i}}\right)^2=\left(\sum\limits_{i=1}^{10}{|f(i)|^2}\right)\left(\sum\limits_{i=1}^{10}{\frac{1}{4^i}}\right)$$ How many such $f$ are possible? I used the Cauchy-Schwarz inequality to conclude that this condition would imply $2|f(1)|=2^2|f(2)|=\dots=2^{10}|f(10)|$, and hence there are uncountably many such functions possible. However, I am not sure of this. Any help solving this question would be great.",,['functions']
46,A question about surjective functions.,A question about surjective functions.,,"I am looking for a sample of surjective function $f:X \to Y$ and a set $A \subseteq X$ such that $f^{-1}(f(A))\neq A$. Is the  sample $f(x)=x^2, f^{-1}(x)=\sqrt{x}, X=\mathbb{R}, Y=[0, +\infty), A=[-1,1]$  a correct one?","I am looking for a sample of surjective function $f:X \to Y$ and a set $A \subseteq X$ such that $f^{-1}(f(A))\neq A$. Is the  sample $f(x)=x^2, f^{-1}(x)=\sqrt{x}, X=\mathbb{R}, Y=[0, +\infty), A=[-1,1]$  a correct one?",,"['functions', 'elementary-set-theory']"
47,Condition on vector-valued function,Condition on vector-valued function,,"Does anyone have any ideas on how to show that the following is true: Let $\Omega \subset \mathbb{R}^{n}$ be open and bounded. Consider vector-valued function $$f: \Omega \times \mathbb{R} \times \mathbb{R}^{n} \rightarrow \mathbb{R}^{n}$$  which has growth conditions $$|f_{i}(x,y,z)| \leq \beta\big(k(x) + |s|^{p_{0}}+\sum_{j=1}^{n}|z_{j}|^{p_{j}}\big)^{1-\frac{1}{p_{i}}}$$ where $\alpha > 0$, $\beta > 0$, $p_{0} > 1$ and $k \in L^{1}(\Omega)$. Assume $$f(x,y,z)\cdot z \geq \alpha\sum_{i=1}^{n}|z_{i}|^{p_{i}}$$ for a.e. $x \in \Omega$ and every $(y,z)\in \mathbb{R} \times \mathbb{R}^{n}$, where $p=(p_{1},...,p_{n})$ and $p_{i} > 1$. Does the previous condition on $f$ imply the following condition or does the following condition on $f$ imply the previous condition? $$\forall z_{0} \in \mathbb{R}^{n}: \lim\limits_{|z|\rightarrow \infty}\frac{f(x,y,z)\cdot(z-z_{0})}{|z|} = \infty ~~~ for~y~bounded. $$ Thanks for any assistance.","Does anyone have any ideas on how to show that the following is true: Let $\Omega \subset \mathbb{R}^{n}$ be open and bounded. Consider vector-valued function $$f: \Omega \times \mathbb{R} \times \mathbb{R}^{n} \rightarrow \mathbb{R}^{n}$$  which has growth conditions $$|f_{i}(x,y,z)| \leq \beta\big(k(x) + |s|^{p_{0}}+\sum_{j=1}^{n}|z_{j}|^{p_{j}}\big)^{1-\frac{1}{p_{i}}}$$ where $\alpha > 0$, $\beta > 0$, $p_{0} > 1$ and $k \in L^{1}(\Omega)$. Assume $$f(x,y,z)\cdot z \geq \alpha\sum_{i=1}^{n}|z_{i}|^{p_{i}}$$ for a.e. $x \in \Omega$ and every $(y,z)\in \mathbb{R} \times \mathbb{R}^{n}$, where $p=(p_{1},...,p_{n})$ and $p_{i} > 1$. Does the previous condition on $f$ imply the following condition or does the following condition on $f$ imply the previous condition? $$\forall z_{0} \in \mathbb{R}^{n}: \lim\limits_{|z|\rightarrow \infty}\frac{f(x,y,z)\cdot(z-z_{0})}{|z|} = \infty ~~~ for~y~bounded. $$ Thanks for any assistance.",,['real-analysis']
48,How many functions? [duplicate],How many functions? [duplicate],,"This question already has answers here : find the number of injections/surjections (2 answers) Closed 9 years ago . How many into and onto functions are there from [k] to [n]? a) If k > n? Thus far I have S(k,n)n! However, I don't know how to calculate that. b) If k <= n?","This question already has answers here : find the number of injections/surjections (2 answers) Closed 9 years ago . How many into and onto functions are there from [k] to [n]? a) If k > n? Thus far I have S(k,n)n! However, I don't know how to calculate that. b) If k <= n?",,"['combinatorics', 'functions']"
49,Extension theorem of twice continuously differentiable functions?,Extension theorem of twice continuously differentiable functions?,,Is there a theorem which guarantees me that any function $f$ with bounded first and second order derivatives defined over a compact interval of $\mathbb{R}^2$ can be extended to a twice continuously differentiable function $F$ on the whole space $\mathbb{R}^3$ ?,Is there a theorem which guarantees me that any function with bounded first and second order derivatives defined over a compact interval of can be extended to a twice continuously differentiable function on the whole space ?,f \mathbb{R}^2 F \mathbb{R}^3,['functions']
50,Functional inequalities involving cubing and incrementing,Functional inequalities involving cubing and incrementing,,"Consider the set $S$ of positive increasing invertible functions $f$ satisfying: $$f((x+1)^3-1)≤(f(x)+1)^3-1$$ $$f(x^3)≥(f(x))^³$$ $$f(x)+1≤f(x+1)$$ for all positive real $x$. Clearly the identity function is in $S$, since then each inequality holds with equality. My question is: Show that the set $S$ is infinite. In other words, show that there exist infinitely many functions $f$ for which the above inequalities holds true.","Consider the set $S$ of positive increasing invertible functions $f$ satisfying: $$f((x+1)^3-1)≤(f(x)+1)^3-1$$ $$f(x^3)≥(f(x))^³$$ $$f(x)+1≤f(x+1)$$ for all positive real $x$. Clearly the identity function is in $S$, since then each inequality holds with equality. My question is: Show that the set $S$ is infinite. In other words, show that there exist infinitely many functions $f$ for which the above inequalities holds true.",,"['real-analysis', 'sequences-and-series', 'functional-analysis', 'functions']"
51,Triplets of distinct integers > 1 that return integer values.,Triplets of distinct integers > 1 that return integer values.,,"If $(A, B, C)$ are distinct integers $> 1$, and $$f(A, B, C) = \frac{\frac{A^2-1}{A} + \frac{B^2-1}{B}}{\frac{C^2-1}{C}},$$ then for what (if any) triplets $(A, B, C)$ is $f(A, B, C)$ an integer? UPDATE: I've done some more work and have come up with the following: Based on the equations I combined to arrive at $f(A,B,C)$, it follows that: $$f(A,B,C)>2\Rightarrow A>B>C>1$$ and $$f(A,B,C)=2\Rightarrow A>C>B>1$$ My further work: Let $f(A,B,C)=k$ where $k>1$ is an integer. Rewrite $f(A,B,C)$ as $$k=\frac{A-\frac{1}{A}+B-\frac{1}{B}}{C-\frac{1}{C}}$$ Thus $$A-\frac{1}{A}+B-\frac{1}{B}-k(C-\frac{1}{C})=A-\frac{1}{A}+B-\frac{1}{B}-kC+\frac{k}{C}=0$$ Note that $\frac{1}{A}+\frac{1}{B}<1$ for all integer $A,B$ such that $A>B>1$. Now we consider three cases: $1)$ $k<C$, $2)$ $k=C$, $3)$ $k>C$. Case $1$: $k<C$ Since $\frac{k}{C}<1$ it follows that these equations hold:$$A+B=kC$$ $$\frac{1}{A}+\frac{1}{B}=\frac{A+B}{AB}=\frac{k}{C}$$ By substitution $$\frac{kC}{AB}=\frac{k}{C}\Rightarrow AB=C^2$$ This is only true if $A=np^2$, $B=nq^2$, and $C=npq$ for integers $n,p,q\geq 1$ and $p>q$. From this we have $$p^2>pq; pq>q^2\Rightarrow p^2>pq>q^2\Rightarrow np^2>npq>nq^2\Rightarrow A>C>B>1\Rightarrow k\leq2$$ This last condition, $k\leq2$, follows from the conditions stated above and leaves us with two possibilities: $k=2$ or $k=1$. If $k=1$ we have $A+B=C$ which violates the condition $A>C$. If $k=2$ we have $A+B=2C\Rightarrow \frac{A+B}{2}=C$. By substitution we have $$\frac{A+B}{AB}=\frac{2}{\frac{A+B}{2}}$$ $$4AB=(A+B)^2\Rightarrow A^2-2AB+B^2=0\Rightarrow (A-B)^2=0$$ However, this leads to the result that $A=B$ which violates that stated conditions. Case $1$, $k<C$, fails. Case $2$: $k=C$ Since $\frac{k}{C}=1$ it follows that $$A+B+1=C^2$$ $$\frac{1}{A}+\frac{1}{B}=0$$ This later equation is clearly false and thus Case $2$, $k=C$ fails. Case $3$: $k>C$ Let $p,q$ be integers with $C>q\geq0$ and $p>1$. From this we can write $k$ as $k=pC+q$. Note that $q=0 \Rightarrow A+B+p=C^2$ and $\frac{1}{A}+\frac{1}{B}=0$ (as above) which is clearly a contradiction, so $q\geq1$. Substituting in $k$ above we have $$A-\frac{1}{A}+B-\frac{1}{B}-pC^2-qC+p+\frac{q}{C}=0$$ Applying the same logic as above we have the following equations $$\frac{A+B}{AB}=\frac{q}{C}$$ $$A+B=pC^2+qC-p$$ By substitution we have the following system of equations (one cubic and one quadratic) $$pC^3+qC^2-pC=qAB\Rightarrow pC^3+qC^2-pC-qAB=0$$ $$pC^2+qC-p-A-B=0$$ Since at least one $C$ that works must solve both equations we denote the roots of the cubic as $x_1,x_2,x_3$ and the roots of the quadratic as $x_1,y_2$. By Vieta's formulas we have $$x_1+x_2+x_3=-\frac{q}{p}$$ $$x_1 x_2+x_1 x_3+x_2 x_3=-1$$ $$x_1 x_2 x_3=\frac{qAB}{p}$$ and $$x_1+y_2=-\frac{q}{p}$$ $$x_1 y_2= -\frac{p+A+B}{p}$$ From the first equations (setting $x_1+x_2+x_3=x_1+y_2$) we have $y_2=x_2+x_3$. Thus $$x_1 y_2=x_1 x_2+x_1 x_3=-\frac{p+A+B}{p}\Rightarrow x_2 x_3-\frac{p+A+B}{p}=-1\Rightarrow x_2 x_3=\frac{A+B}{p}$$ $$x_2 x_3=\frac{A+B}{p}\Rightarrow x_1\frac{A+B}{p}=\frac{qAB}{p}\Rightarrow x_1=\frac{qAB}{A+B}$$ Since $x_1=C$ and $B>C$ we have $$B>\frac{qAB}{A+B}\Rightarrow B^2>(q-1)AB$$ Since $A>B$ it follows $AB>B^2$ and so $q-1<1\Rightarrow q<2$. Since $q\neq0$ (see above) we have $q=1$ and so $C=\frac{AB}{A+B}$. Substituting for $C$ in the quadratic (although the cubic has the same result) we have $$0=p(\frac{AB}{A+B})^2+\frac{AB}{A+B}-p-A-B$$ Which, after a fair bit of rearranging, becomes $$p=\frac{(A+B)^3-AB(A+B)}{(AB)^2-(A+B)^2}$$ Thus $f(A,B,C)$ is an integer (with $A,B,C$ constrained as above) iff $$p=\frac{(A+B)^3-AB(A+B)}{(AB)^2-(A+B)^2}$$ has integer solutions $p>0$ and $A>B>2$. Any help with solving this last bit would be greatly appreciated. So far guesswork and Wolfram Alpha have failed to produce results.","If $(A, B, C)$ are distinct integers $> 1$, and $$f(A, B, C) = \frac{\frac{A^2-1}{A} + \frac{B^2-1}{B}}{\frac{C^2-1}{C}},$$ then for what (if any) triplets $(A, B, C)$ is $f(A, B, C)$ an integer? UPDATE: I've done some more work and have come up with the following: Based on the equations I combined to arrive at $f(A,B,C)$, it follows that: $$f(A,B,C)>2\Rightarrow A>B>C>1$$ and $$f(A,B,C)=2\Rightarrow A>C>B>1$$ My further work: Let $f(A,B,C)=k$ where $k>1$ is an integer. Rewrite $f(A,B,C)$ as $$k=\frac{A-\frac{1}{A}+B-\frac{1}{B}}{C-\frac{1}{C}}$$ Thus $$A-\frac{1}{A}+B-\frac{1}{B}-k(C-\frac{1}{C})=A-\frac{1}{A}+B-\frac{1}{B}-kC+\frac{k}{C}=0$$ Note that $\frac{1}{A}+\frac{1}{B}<1$ for all integer $A,B$ such that $A>B>1$. Now we consider three cases: $1)$ $k<C$, $2)$ $k=C$, $3)$ $k>C$. Case $1$: $k<C$ Since $\frac{k}{C}<1$ it follows that these equations hold:$$A+B=kC$$ $$\frac{1}{A}+\frac{1}{B}=\frac{A+B}{AB}=\frac{k}{C}$$ By substitution $$\frac{kC}{AB}=\frac{k}{C}\Rightarrow AB=C^2$$ This is only true if $A=np^2$, $B=nq^2$, and $C=npq$ for integers $n,p,q\geq 1$ and $p>q$. From this we have $$p^2>pq; pq>q^2\Rightarrow p^2>pq>q^2\Rightarrow np^2>npq>nq^2\Rightarrow A>C>B>1\Rightarrow k\leq2$$ This last condition, $k\leq2$, follows from the conditions stated above and leaves us with two possibilities: $k=2$ or $k=1$. If $k=1$ we have $A+B=C$ which violates the condition $A>C$. If $k=2$ we have $A+B=2C\Rightarrow \frac{A+B}{2}=C$. By substitution we have $$\frac{A+B}{AB}=\frac{2}{\frac{A+B}{2}}$$ $$4AB=(A+B)^2\Rightarrow A^2-2AB+B^2=0\Rightarrow (A-B)^2=0$$ However, this leads to the result that $A=B$ which violates that stated conditions. Case $1$, $k<C$, fails. Case $2$: $k=C$ Since $\frac{k}{C}=1$ it follows that $$A+B+1=C^2$$ $$\frac{1}{A}+\frac{1}{B}=0$$ This later equation is clearly false and thus Case $2$, $k=C$ fails. Case $3$: $k>C$ Let $p,q$ be integers with $C>q\geq0$ and $p>1$. From this we can write $k$ as $k=pC+q$. Note that $q=0 \Rightarrow A+B+p=C^2$ and $\frac{1}{A}+\frac{1}{B}=0$ (as above) which is clearly a contradiction, so $q\geq1$. Substituting in $k$ above we have $$A-\frac{1}{A}+B-\frac{1}{B}-pC^2-qC+p+\frac{q}{C}=0$$ Applying the same logic as above we have the following equations $$\frac{A+B}{AB}=\frac{q}{C}$$ $$A+B=pC^2+qC-p$$ By substitution we have the following system of equations (one cubic and one quadratic) $$pC^3+qC^2-pC=qAB\Rightarrow pC^3+qC^2-pC-qAB=0$$ $$pC^2+qC-p-A-B=0$$ Since at least one $C$ that works must solve both equations we denote the roots of the cubic as $x_1,x_2,x_3$ and the roots of the quadratic as $x_1,y_2$. By Vieta's formulas we have $$x_1+x_2+x_3=-\frac{q}{p}$$ $$x_1 x_2+x_1 x_3+x_2 x_3=-1$$ $$x_1 x_2 x_3=\frac{qAB}{p}$$ and $$x_1+y_2=-\frac{q}{p}$$ $$x_1 y_2= -\frac{p+A+B}{p}$$ From the first equations (setting $x_1+x_2+x_3=x_1+y_2$) we have $y_2=x_2+x_3$. Thus $$x_1 y_2=x_1 x_2+x_1 x_3=-\frac{p+A+B}{p}\Rightarrow x_2 x_3-\frac{p+A+B}{p}=-1\Rightarrow x_2 x_3=\frac{A+B}{p}$$ $$x_2 x_3=\frac{A+B}{p}\Rightarrow x_1\frac{A+B}{p}=\frac{qAB}{p}\Rightarrow x_1=\frac{qAB}{A+B}$$ Since $x_1=C$ and $B>C$ we have $$B>\frac{qAB}{A+B}\Rightarrow B^2>(q-1)AB$$ Since $A>B$ it follows $AB>B^2$ and so $q-1<1\Rightarrow q<2$. Since $q\neq0$ (see above) we have $q=1$ and so $C=\frac{AB}{A+B}$. Substituting for $C$ in the quadratic (although the cubic has the same result) we have $$0=p(\frac{AB}{A+B})^2+\frac{AB}{A+B}-p-A-B$$ Which, after a fair bit of rearranging, becomes $$p=\frac{(A+B)^3-AB(A+B)}{(AB)^2-(A+B)^2}$$ Thus $f(A,B,C)$ is an integer (with $A,B,C$ constrained as above) iff $$p=\frac{(A+B)^3-AB(A+B)}{(AB)^2-(A+B)^2}$$ has integer solutions $p>0$ and $A>B>2$. Any help with solving this last bit would be greatly appreciated. So far guesswork and Wolfram Alpha have failed to produce results.",,"['algebra-precalculus', 'elementary-number-theory', 'functions', 'diophantine-equations']"
52,Homomorphisms of the integers under addition,Homomorphisms of the integers under addition,,I am solving the following textbook question: Consider the group $\Bbb Z$ given by the integers under addition. Determine which of the following functions $f : \Bbb Z \to  \Bbb Z$ are homomorphisms (justify your answer). (1) $f(z) = 2z$. (2) $f(z) = 5z + 2$. (3) $f(z) = -z$. Is it correct to proceed like $f(x+y)=2(x+y)=2x+2y=f(x)+f(y)$ therefore (1) is a homomorphism?,I am solving the following textbook question: Consider the group $\Bbb Z$ given by the integers under addition. Determine which of the following functions $f : \Bbb Z \to  \Bbb Z$ are homomorphisms (justify your answer). (1) $f(z) = 2z$. (2) $f(z) = 5z + 2$. (3) $f(z) = -z$. Is it correct to proceed like $f(x+y)=2(x+y)=2x+2y=f(x)+f(y)$ therefore (1) is a homomorphism?,,"['group-theory', 'functions']"
53,Show that a set is uncountable.,Show that a set is uncountable.,,"Let $B$ be the set of all sequences cosisting of digits $7$, $8$, and $9$. Show that $B$ is uncountable. Here is my attempt. Assume to the contrary that $B$ is countable. Then there exists a bijection from $\mathbb{N}$ to $B$. Let $\psi : \mathbb{N} \to B$ be a bijective function, so that it is defined by $$\psi(\mathbb{N})=\{\psi(1),\psi(2),\psi(3),\ldots\}=: \{a_1,a_2,a_3,\ldots\}.$$ We want to define a sequence not in the set $\{a_1,a_2,a_3,\ldots\}$. Define a sequence $g_n : \mathbb{N} \to \{7,8,9\}$ by $$g_n=\begin{cases} 7 & \text{if the $n$th term of $a_n$ is $8$} \\ 8 & \text{if the $n$th term of $a_n$ is not $8$} \end{cases}.$$ Thus, $g_1 \not= a_1, g_2 \not=a_2, ... ,g_n \not= a_n$. Thus, $(g_1,g_2,\ldots,g_n) \not\in \{a_1,a_2,a_3,\ldots\}=\psi(\mathbb{N})$, which is a contradiction.","Let $B$ be the set of all sequences cosisting of digits $7$, $8$, and $9$. Show that $B$ is uncountable. Here is my attempt. Assume to the contrary that $B$ is countable. Then there exists a bijection from $\mathbb{N}$ to $B$. Let $\psi : \mathbb{N} \to B$ be a bijective function, so that it is defined by $$\psi(\mathbb{N})=\{\psi(1),\psi(2),\psi(3),\ldots\}=: \{a_1,a_2,a_3,\ldots\}.$$ We want to define a sequence not in the set $\{a_1,a_2,a_3,\ldots\}$. Define a sequence $g_n : \mathbb{N} \to \{7,8,9\}$ by $$g_n=\begin{cases} 7 & \text{if the $n$th term of $a_n$ is $8$} \\ 8 & \text{if the $n$th term of $a_n$ is not $8$} \end{cases}.$$ Thus, $g_1 \not= a_1, g_2 \not=a_2, ... ,g_n \not= a_n$. Thus, $(g_1,g_2,\ldots,g_n) \not\in \{a_1,a_2,a_3,\ldots\}=\psi(\mathbb{N})$, which is a contradiction.",,"['functions', 'elementary-set-theory']"
54,how to prove that an entire function is positive on the real axis,how to prove that an entire function is positive on the real axis,,"The error function $\mathrm{erf}(x)$ is defined as: $$\mathrm{erf}(x):=\frac {2}{\sqrt{\pi}} \int_{0}^{x} e^{-t^{2}}dt\tag{1}$$ Let us define the following 3 functions: $$h(y)=\frac{1}{2}\left(\mathrm{erf}(1+i y)+\mathrm{erf}(1-i y)\right)\tag{2}$$ $$f(y)=e\sqrt{\pi}(2y^2+1)e^{-y^2}h(y)+2\left(\cos(2y) - y\sin(2y)\right) \tag{3}$$ $$g(y)=e\sqrt{\pi}y(2y^2-1)e^{-y^2}h(y)+2\left(y\cos(2y)+(y^2-1)\sin(2y)\right) \tag{4}$$ We also have: $$\frac{df}{dy}(y)=-2 g(y)$$ Here is a plot of $\color{red}{\log(f^2(y))}$ and $\color{blue}{\log(g^2(y))}$ vs. $y$: Question: How to prove that the following sum-of-square function $\phi(y)$ is always positive: $$\phi(y):=f^2(y)+g^2(y)>0,\qquad y\in \mathbb{R}\tag{5}$$ This problem showed up when I was looking for some approximation to Riemann $\Xi(z)$ function. I will soon describe several unsuccessful attempts that I tried. Best regards- mike","The error function $\mathrm{erf}(x)$ is defined as: $$\mathrm{erf}(x):=\frac {2}{\sqrt{\pi}} \int_{0}^{x} e^{-t^{2}}dt\tag{1}$$ Let us define the following 3 functions: $$h(y)=\frac{1}{2}\left(\mathrm{erf}(1+i y)+\mathrm{erf}(1-i y)\right)\tag{2}$$ $$f(y)=e\sqrt{\pi}(2y^2+1)e^{-y^2}h(y)+2\left(\cos(2y) - y\sin(2y)\right) \tag{3}$$ $$g(y)=e\sqrt{\pi}y(2y^2-1)e^{-y^2}h(y)+2\left(y\cos(2y)+(y^2-1)\sin(2y)\right) \tag{4}$$ We also have: $$\frac{df}{dy}(y)=-2 g(y)$$ Here is a plot of $\color{red}{\log(f^2(y))}$ and $\color{blue}{\log(g^2(y))}$ vs. $y$: Question: How to prove that the following sum-of-square function $\phi(y)$ is always positive: $$\phi(y):=f^2(y)+g^2(y)>0,\qquad y\in \mathbb{R}\tag{5}$$ This problem showed up when I was looking for some approximation to Riemann $\Xi(z)$ function. I will soon describe several unsuccessful attempts that I tried. Best regards- mike",,"['functions', 'riemann-zeta']"
55,A game involving Even and Odd functions,A game involving Even and Odd functions,,"I was doodling around today and thought of this fun game. Two players take alternate turns playing this game. A function from now on refers to real valued functions with domain $\mathbb R$, and Odd and Even functions have the usual meaning. A function $F$ is said to be complimentary to a function $H$, if one of the following is true:- $F$ is Odd when $H$ is Even. $F$ is Even when $H$ is Odd. The first player starts with a function $h_n$, which is either Odd or Even. The second player is said to make a move when he/she provides another function $g$ along with an operation $*$, (where $*$ is one of $+,-,\times,\circ$), so as to create a function $h_{n+1} = h_n*g$ or $h_{n+1}=g*h_n$ such that $g$ is complimentary to $h_n$ and in accordance with the rules (stated below). Moves are made alternately between the two players, and the player who is unable to make a move loses. Rules $h_n$ is added to a set $V$ for each $n$. $\forall g_1,g_2,.... \in V$ and $\forall a_1,a_2,.... \in \mathbb R$, $(a_1g_1^n+a_2g_2^n+....)$ is added to $V$, where $n \in \mathbb N$ and $g_k^n = {g_k} \times {g_k} \times  \cdots g_k$ $n$ times. If $g \in V,$ $g$ cannot be used in a move. At the end of each move, the function used by the player is added to $V$. The $*$ provided by the player cannot be used in consecutive moves. Example Player 1: $h_1(x)=x$, which is Odd. Player 2: $g(x)=|x|$ and $* = \circ$ so that $h_2(x) = (h_1 \circ g)(x) = |x|$, which is Even. Player 1: $g(x)=sin(x)$ and $*=\times$ so that $h_3(x)=h_2(x)\times g(x) = |x|sin(x),$ which is Odd. Player 2: $g(x) = cos(x)$ and $* = \circ$ so that $h_4(x) = (g \circ h_3)(x) = cos(|x|sin(x))$, which is Even. And it goes on... I don't know much of higher mathematics to analyze this game, so I would appreciate it if someone could answer the following questions which I had in mind:- Are the rules well defined? I don't think my statement of Rule #2 is correctly written, since $V$ turns out to be an infinite set. Are the rules strong enough to ensure that both players don't have a ""trick"" to play for an indefinitely long time? If not, how do I make them stronger? Does the game terminate, in theory? What is $V$ eventually? Can it ever contain all Even and Odd functions?","I was doodling around today and thought of this fun game. Two players take alternate turns playing this game. A function from now on refers to real valued functions with domain $\mathbb R$, and Odd and Even functions have the usual meaning. A function $F$ is said to be complimentary to a function $H$, if one of the following is true:- $F$ is Odd when $H$ is Even. $F$ is Even when $H$ is Odd. The first player starts with a function $h_n$, which is either Odd or Even. The second player is said to make a move when he/she provides another function $g$ along with an operation $*$, (where $*$ is one of $+,-,\times,\circ$), so as to create a function $h_{n+1} = h_n*g$ or $h_{n+1}=g*h_n$ such that $g$ is complimentary to $h_n$ and in accordance with the rules (stated below). Moves are made alternately between the two players, and the player who is unable to make a move loses. Rules $h_n$ is added to a set $V$ for each $n$. $\forall g_1,g_2,.... \in V$ and $\forall a_1,a_2,.... \in \mathbb R$, $(a_1g_1^n+a_2g_2^n+....)$ is added to $V$, where $n \in \mathbb N$ and $g_k^n = {g_k} \times {g_k} \times  \cdots g_k$ $n$ times. If $g \in V,$ $g$ cannot be used in a move. At the end of each move, the function used by the player is added to $V$. The $*$ provided by the player cannot be used in consecutive moves. Example Player 1: $h_1(x)=x$, which is Odd. Player 2: $g(x)=|x|$ and $* = \circ$ so that $h_2(x) = (h_1 \circ g)(x) = |x|$, which is Even. Player 1: $g(x)=sin(x)$ and $*=\times$ so that $h_3(x)=h_2(x)\times g(x) = |x|sin(x),$ which is Odd. Player 2: $g(x) = cos(x)$ and $* = \circ$ so that $h_4(x) = (g \circ h_3)(x) = cos(|x|sin(x))$, which is Even. And it goes on... I don't know much of higher mathematics to analyze this game, so I would appreciate it if someone could answer the following questions which I had in mind:- Are the rules well defined? I don't think my statement of Rule #2 is correctly written, since $V$ turns out to be an infinite set. Are the rules strong enough to ensure that both players don't have a ""trick"" to play for an indefinitely long time? If not, how do I make them stronger? Does the game terminate, in theory? What is $V$ eventually? Can it ever contain all Even and Odd functions?",,['functions']
56,How to express $e^{yS^2}(f(x))$ in closed form where $\frac{d}{dx}=S$,How to express  in closed form where,e^{yS^2}(f(x)) \frac{d}{dx}=S,"$$ f(x)+\frac{y.f'(x)}{1!}+\frac{y^2 f^{''}(x)}{2!}+\cdots=e^{yS}(f(x))=f(x+y) \text{ where }\frac{d}{dx}=S$$ is a operator $$ f(x)+\frac{y.f''(x)}{1!}+\frac{y^2 f^{(4)}(x)}{2!}+\cdots=e^{yS^2}(f(x))=?$$ how can it be expressed  as f(x)? The most nearly expression I had: $$ f(x)+\frac{y.f''(x)}{2!}+\frac{y^2 f^{(4)}(x)}{4!}+\cdots=\frac{1}{2} (e^{\sqrt{y}S}(f(x))+e^{-\sqrt{y}S}(f(x)))=\frac{1}{2}(f(x+\sqrt{y})+f(x-\sqrt{y}))$$ Is there any way to express $e^{yS^2}(f(x))$ as closed form of $f(x)$? Thanks for answers EDIT: I thought fourier transform  may help to get progress, What I tried: Fourier transform of $f(x)$ can be written $$F(x)= \int_{-\infty}^{+\infty} f(t)e^{-2πixt} \mathrm{d}t$$ inverse Fourier transform of $f(x)$ can be written $$f(x)= \int_{-\infty}^{+\infty} F(t)e^{2πixt} \mathrm{d}t$$ Thus $$f^{(2n)}(x)= \int_{-\infty}^{+\infty} F(t) (2πit)^{(2n)}e^{2πixt} \mathrm{d}t$$ $$\frac{y^nf^{(2n)}(x)}{n!}= \int_{-\infty}^{+\infty} F(t) \frac{y^n(2πit)^{(2n)}}{n!}e^{2πixt} \mathrm{d}t$$ $$\sum_{n=0}^\infty\frac{y^nf^{(2n)}(x)}{n!}= \int_{-\infty}^{+\infty} F(t) (\sum_{n=0}^\infty\frac{y^n(2πit)^{(2n)}}{n!})e^{2πixt} \mathrm{d}t$$ $$\sum_{n=0}^\infty\frac{y^nf^{(2n)}(x)}{n!}= \int_{-\infty}^{+\infty} F(t) (\sum_{n=0}^\infty\frac{((2πit)^2y)^{(n)}}{n!})e^{2πixt} \mathrm{d}t$$ $$\sum_{n=0}^\infty\frac{y^nf^{(2n)}(x)}{n!}= \int_{-\infty}^{+\infty} F(t) e^{(2πit)^2y} e^{2πixt} \mathrm{d}t$$ $$\sum_{n=0}^\infty\frac{y^nf^{(2n)}(x)}{n!}=e^{yS^2}(f(x))= \int_{-\infty}^{+\infty} F(t) e^{-4π^2yt^2+2πixt}  \mathrm{d}t$$ $$e^{yS^2}(f(x))= \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} f(u)e^{-2πiut}  e^{-4π^2yt^2+2πixt} \mathrm{d}u \mathrm{d}t$$ $$e^{yS^2}(f(x))= \int_{-\infty}^{+\infty} f(u) \int_{-\infty}^{+\infty} e^{-2πiut}  e^{-4π^2yt^2+2πixt}  \mathrm{d}t \mathrm{d}u$$ Now Need to find $$\int_{-\infty}^{+\infty} e^{-2πiut}  e^{-4π^2yt^2+2πixt}  \mathrm{d}t=\int_{-\infty}^{+\infty}  e^{-4π^2yt^2+2πi(x-u)t}  \mathrm{d}t $$ EDIT:(after @Winther comment) $$\int_{-\infty}^{+\infty}  e^{-4π^2yt^2+2πi(x-u)t}  \mathrm{d}t =e^{4π^2y\alpha^2}\int_{-\infty}^{+\infty}  e^{-4π^2y(t-\alpha)^2}  \mathrm{d}t$$ Where  $$2\alpha.(-4\pi^2y)=2\pi i(x-u)$$ $$\alpha=\frac{-i(x-u)}{4\pi y}$$ $$\int_{-\infty}^{+\infty}  e^{-4π^2yt^2+2πi(x-u)t}  \mathrm{d}t =e^{\frac{-(x-u)^2}{4y}}\int_{-\infty}^{+\infty}  e^{-4π^2y(t-\alpha)^2}  \mathrm{d}t$$ $$z=t-\alpha$$ $$\int_{-\infty}^{+\infty}  e^{-4π^2yt^2+2πi(x-u)t}  \mathrm{d}t =e^{\frac{-(x-u)^2}{4y}}\int_{-\infty}^{+\infty}  e^{-4π^2yz^2}  \mathrm{d}z=\frac{e^{\frac{-(x-u)^2}{4y}}}{2\sqrt{\pi y}}$$ $$e^{yS^2}(f(x))= \int_{-\infty}^{+\infty} f(u) \int_{-\infty}^{+\infty} e^{-2πiut}  e^{-4π^2yt^2+2πixt}  \mathrm{d}t \mathrm{d}u=\int_{-\infty}^{+\infty} f(u) \frac{e^{\frac{-(x-u)^2}{4y}}}{2\sqrt{\pi y}} \mathrm{d}u$$ Finally the result can be written as $$e^{yS^2}(f(x))=\frac{1}{2\sqrt{\pi y}}\int_{-\infty}^{+\infty} f(u) e^{\frac{-(x-u)^2}{4y}} \mathrm{d}u$$ Please check my result if I made a mistake during calculations. Thanks for comments and advice.","$$ f(x)+\frac{y.f'(x)}{1!}+\frac{y^2 f^{''}(x)}{2!}+\cdots=e^{yS}(f(x))=f(x+y) \text{ where }\frac{d}{dx}=S$$ is a operator $$ f(x)+\frac{y.f''(x)}{1!}+\frac{y^2 f^{(4)}(x)}{2!}+\cdots=e^{yS^2}(f(x))=?$$ how can it be expressed  as f(x)? The most nearly expression I had: $$ f(x)+\frac{y.f''(x)}{2!}+\frac{y^2 f^{(4)}(x)}{4!}+\cdots=\frac{1}{2} (e^{\sqrt{y}S}(f(x))+e^{-\sqrt{y}S}(f(x)))=\frac{1}{2}(f(x+\sqrt{y})+f(x-\sqrt{y}))$$ Is there any way to express $e^{yS^2}(f(x))$ as closed form of $f(x)$? Thanks for answers EDIT: I thought fourier transform  may help to get progress, What I tried: Fourier transform of $f(x)$ can be written $$F(x)= \int_{-\infty}^{+\infty} f(t)e^{-2πixt} \mathrm{d}t$$ inverse Fourier transform of $f(x)$ can be written $$f(x)= \int_{-\infty}^{+\infty} F(t)e^{2πixt} \mathrm{d}t$$ Thus $$f^{(2n)}(x)= \int_{-\infty}^{+\infty} F(t) (2πit)^{(2n)}e^{2πixt} \mathrm{d}t$$ $$\frac{y^nf^{(2n)}(x)}{n!}= \int_{-\infty}^{+\infty} F(t) \frac{y^n(2πit)^{(2n)}}{n!}e^{2πixt} \mathrm{d}t$$ $$\sum_{n=0}^\infty\frac{y^nf^{(2n)}(x)}{n!}= \int_{-\infty}^{+\infty} F(t) (\sum_{n=0}^\infty\frac{y^n(2πit)^{(2n)}}{n!})e^{2πixt} \mathrm{d}t$$ $$\sum_{n=0}^\infty\frac{y^nf^{(2n)}(x)}{n!}= \int_{-\infty}^{+\infty} F(t) (\sum_{n=0}^\infty\frac{((2πit)^2y)^{(n)}}{n!})e^{2πixt} \mathrm{d}t$$ $$\sum_{n=0}^\infty\frac{y^nf^{(2n)}(x)}{n!}= \int_{-\infty}^{+\infty} F(t) e^{(2πit)^2y} e^{2πixt} \mathrm{d}t$$ $$\sum_{n=0}^\infty\frac{y^nf^{(2n)}(x)}{n!}=e^{yS^2}(f(x))= \int_{-\infty}^{+\infty} F(t) e^{-4π^2yt^2+2πixt}  \mathrm{d}t$$ $$e^{yS^2}(f(x))= \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} f(u)e^{-2πiut}  e^{-4π^2yt^2+2πixt} \mathrm{d}u \mathrm{d}t$$ $$e^{yS^2}(f(x))= \int_{-\infty}^{+\infty} f(u) \int_{-\infty}^{+\infty} e^{-2πiut}  e^{-4π^2yt^2+2πixt}  \mathrm{d}t \mathrm{d}u$$ Now Need to find $$\int_{-\infty}^{+\infty} e^{-2πiut}  e^{-4π^2yt^2+2πixt}  \mathrm{d}t=\int_{-\infty}^{+\infty}  e^{-4π^2yt^2+2πi(x-u)t}  \mathrm{d}t $$ EDIT:(after @Winther comment) $$\int_{-\infty}^{+\infty}  e^{-4π^2yt^2+2πi(x-u)t}  \mathrm{d}t =e^{4π^2y\alpha^2}\int_{-\infty}^{+\infty}  e^{-4π^2y(t-\alpha)^2}  \mathrm{d}t$$ Where  $$2\alpha.(-4\pi^2y)=2\pi i(x-u)$$ $$\alpha=\frac{-i(x-u)}{4\pi y}$$ $$\int_{-\infty}^{+\infty}  e^{-4π^2yt^2+2πi(x-u)t}  \mathrm{d}t =e^{\frac{-(x-u)^2}{4y}}\int_{-\infty}^{+\infty}  e^{-4π^2y(t-\alpha)^2}  \mathrm{d}t$$ $$z=t-\alpha$$ $$\int_{-\infty}^{+\infty}  e^{-4π^2yt^2+2πi(x-u)t}  \mathrm{d}t =e^{\frac{-(x-u)^2}{4y}}\int_{-\infty}^{+\infty}  e^{-4π^2yz^2}  \mathrm{d}z=\frac{e^{\frac{-(x-u)^2}{4y}}}{2\sqrt{\pi y}}$$ $$e^{yS^2}(f(x))= \int_{-\infty}^{+\infty} f(u) \int_{-\infty}^{+\infty} e^{-2πiut}  e^{-4π^2yt^2+2πixt}  \mathrm{d}t \mathrm{d}u=\int_{-\infty}^{+\infty} f(u) \frac{e^{\frac{-(x-u)^2}{4y}}}{2\sqrt{\pi y}} \mathrm{d}u$$ Finally the result can be written as $$e^{yS^2}(f(x))=\frac{1}{2\sqrt{\pi y}}\int_{-\infty}^{+\infty} f(u) e^{\frac{-(x-u)^2}{4y}} \mathrm{d}u$$ Please check my result if I made a mistake during calculations. Thanks for comments and advice.",,"['functions', 'operator-theory', 'taylor-expansion']"
57,Math Formula For A Loyalty System,Math Formula For A Loyalty System,,"Basically, I help someone manage a stream on twitch.tv.  She uses a program that rewards the viewers with a virtual currency.  For every $30$ minutes they watch they get $1$ point.  Also, they get a $1$ time increase of $8$ points if they follow her on twitch.  Now the question arising here is what is the most fair way to reward donators.  She wants to reward the people who support the stream in any way including donations, and coming up with a fair formula seems like the best option, but coming up with the formula is the problem. The formula should be able to prevent exploitation.  She would like to have a flat value for donating in general as support, but increase the amount based on the size of the donation.  An example of an exploitation to be prevented would be donating $\$1$ ten times rather than just $\$10$ one time in order to receive more points. What is the most fair way to handle this situation, and what type of formula am I looking?  Please help","Basically, I help someone manage a stream on twitch.tv.  She uses a program that rewards the viewers with a virtual currency.  For every $30$ minutes they watch they get $1$ point.  Also, they get a $1$ time increase of $8$ points if they follow her on twitch.  Now the question arising here is what is the most fair way to reward donators.  She wants to reward the people who support the stream in any way including donations, and coming up with a fair formula seems like the best option, but coming up with the formula is the problem. The formula should be able to prevent exploitation.  She would like to have a flat value for donating in general as support, but increase the amount based on the size of the donation.  An example of an exploitation to be prevented would be donating $\$1$ ten times rather than just $\$10$ one time in order to receive more points. What is the most fair way to handle this situation, and what type of formula am I looking?  Please help",,"['algebra-precalculus', 'functions']"
58,Prove that a function is symmetric in its three variables.,Prove that a function is symmetric in its three variables.,,"Let $U=\{1, 2,\ldots, 2014\}$. For positive integers $a$, $b$, and $c$, we denote by $f(a, b, c)$ the number of ordered $6$-tuples of sets $(X_1,X_2,X_3,Y_1,Y_2,Y_3)$ satisfying the following conditions: (i) $Y_1 \subseteq X_1 \subseteq U$ and $|X_1|=a$; (ii) $Y_2    \subseteq X_2 \subseteq U\setminus Y_1$ and $|X_2|=b$; (iii) $Y_3    \subseteq X_3 \subseteq U\setminus (Y_1\cup Y_2)$ and $|X_3|=c$. Prove that $f(a,b,c)$ does not change when $a$, $b$, and $c$ are rearranged. I tried drawing the venn diagram of the sets but I couldn't go anywhere from there. This is a problem from International Zhautykov Olympiad 2014. I hope someone could help me.","Let $U=\{1, 2,\ldots, 2014\}$. For positive integers $a$, $b$, and $c$, we denote by $f(a, b, c)$ the number of ordered $6$-tuples of sets $(X_1,X_2,X_3,Y_1,Y_2,Y_3)$ satisfying the following conditions: (i) $Y_1 \subseteq X_1 \subseteq U$ and $|X_1|=a$; (ii) $Y_2    \subseteq X_2 \subseteq U\setminus Y_1$ and $|X_2|=b$; (iii) $Y_3    \subseteq X_3 \subseteq U\setminus (Y_1\cup Y_2)$ and $|X_3|=c$. Prove that $f(a,b,c)$ does not change when $a$, $b$, and $c$ are rearranged. I tried drawing the venn diagram of the sets but I couldn't go anywhere from there. This is a problem from International Zhautykov Olympiad 2014. I hope someone could help me.",,"['combinatorics', 'functions', 'elementary-set-theory', 'contest-math', 'symmetry']"
59,Finding the maximum value of $\displaystyle \frac{x^n-a\sin x}{x^{n+1}}\ (x\gt 0)$,Finding the maximum value of,\displaystyle \frac{x^n-a\sin x}{x^{n+1}}\ (x\gt 0),"For $a\ge 1\in\mathbb R,n\ge 1\in\mathbb N$, let us define $f(x)$ as $$f(x)=\frac{x^n-a\sin x}{x^{n+1}}\ \ \ (x\gt 0).$$ Also, let $M(a,n)$ be the maximum value of $f(x).$ We may be able to know the approximate value of $M(a,n)$ using some tools, but... Question 1 : Can we represent $M(a,n)$ by $a,n$? Since Question 1 seems very difficult for me, I've been trying to find the explicit value of $M(a,n)$ for smaller $(a,n)$. However, even proving that $M(1,1)=1/\pi$ for $x=\pi$ was not very easy for me. So... Question 2 : For what $(a,n)\not=(1,1)$ can we find the explicit value of $M(a,n)$? Edit : I noticed that $a\ge 1\in\mathbb R$ is needed for the existence of $M(a,1)$.","For $a\ge 1\in\mathbb R,n\ge 1\in\mathbb N$, let us define $f(x)$ as $$f(x)=\frac{x^n-a\sin x}{x^{n+1}}\ \ \ (x\gt 0).$$ Also, let $M(a,n)$ be the maximum value of $f(x).$ We may be able to know the approximate value of $M(a,n)$ using some tools, but... Question 1 : Can we represent $M(a,n)$ by $a,n$? Since Question 1 seems very difficult for me, I've been trying to find the explicit value of $M(a,n)$ for smaller $(a,n)$. However, even proving that $M(1,1)=1/\pi$ for $x=\pi$ was not very easy for me. So... Question 2 : For what $(a,n)\not=(1,1)$ can we find the explicit value of $M(a,n)$? Edit : I noticed that $a\ge 1\in\mathbb R$ is needed for the existence of $M(a,1)$.",,"['calculus', 'real-analysis', 'functions']"
60,Motivation for defining a functions codomain,Motivation for defining a functions codomain,,"Why not always refer to a functions image, what is the point in specifying some super set of that image and then naming that super set the functions ""codomain""? What does explicitly defining a set that contains more values for which your function does not even output add to the definition of a function.","Why not always refer to a functions image, what is the point in specifying some super set of that image and then naming that super set the functions ""codomain""? What does explicitly defining a set that contains more values for which your function does not even output add to the definition of a function.",,"['functions', 'definition']"
61,$\beta_a(n)=(a_1*\cdots(a_n*b))\setminus_* b$ and Iterations in right divisible magmas e representability by left translations.),and Iterations in right divisible magmas e representability by left translations.),\beta_a(n)=(a_1*\cdots(a_n*b))\setminus_* b,"Let's consider the magma $(G,*)$ with infinite elements. Now I define $\operatorname{left}(G)$ the set of all the left translations $$\operatorname{left}(G):\{L_a:a \in G ,L_a(b)=a*b\}$$ And $iter(a)$ as the set formed by the left traslation by $a\in G$ and closed under function composition or in other words the iterations of the left translations. $$\operatorname{iter}(a):\{L_a^{n}:n \in \Bbb N\setminus\{0\}\}$$ What are the weakest conditions that $(G,*)$ must satisfies if we want   that exist an collection injective functions $\mathcal F_a:\operatorname{iter}(a)\rightarrow > \operatorname{left}(G)$? $\mathcal F_a$ are defined in this way $$\mathcal F_a(L_a^1)=L_a$$ $$\mathcal F_a(L_a^n)=L_{a'}$$ $$\mathcal (\mathcal F_a(L_a^n))(x)=L_a^n(x)$$ I think that this is equivalent to the this statement $$\operatorname{iter}(a)\subseteq \operatorname{left}(G)$$. If $(G,*)$ is associative  then this always holds because $L_a(L_a(x))=L_{a*a}(x)$ and in general $\mathcal F_a(L_a^n)=L_{a^n}$ But this assumption is too stroong for my needs A weaker condition is that if $*$ is right invertible (exists a $R_a^{-1}$ such that $R_b^{-1}(a*b)=(a*b)\setminus_* b=a$)  then we can define the functions $\beta_a$ $$\beta_a(n)*b=a_1*\cdots(a_n*b)$$  $$\beta_a(n)=(a_1*\cdots(a_n*b))\setminus_* b$$ $$\beta_a(n)=R_b(L_a^n(b))$$ and these functions MUST be always constants (for every $b$) so we can define the injections $\mathcal F_a$ : $$L^n_a=L_{\beta_a(n)}$$ and thus $ \mathcal F_a$ exists $$\mathcal F_a(L_a^n)=L_{\beta_a(n)}$$ $1$-$\beta_a$ not depend from $b$ and is always costant, $\mathcal F_a$   exists and $\operatorname{iter}(a)\subseteq \operatorname{left}(G)$ are three equivalent statements? $3$-When $\beta_a$ satisfies that weak condition? $3$-If $(G,*)$ is not commutative, not associative and not left   invertible but is right invertible is possible that this bijection   from $\operatorname{iter}(a)\rightarrow \operatorname{left}(g)$ exists? Or maybe there can be a surjection? Note: the image of $\operatorname{iter}(a)$ by$\mathcal F_a$, $\mathcal F_a[\operatorname{iter}(a)]$ should be always a commutative subsemigroup of $(\operatorname{left}(G),\circ)$, commutative submonoid if $*$ has a left unit and a commutative  subgroup if $*$ is right invertible. Update: here a related question. Existence of an operation $\cdot$ such that $(a*(b*c))=(a\cdot b)*c$ Seems to me that the conditiont that the User Goos found is really similar to inclusion condition. Anyways I think that the questions here are still meaningfull.","Let's consider the magma $(G,*)$ with infinite elements. Now I define $\operatorname{left}(G)$ the set of all the left translations $$\operatorname{left}(G):\{L_a:a \in G ,L_a(b)=a*b\}$$ And $iter(a)$ as the set formed by the left traslation by $a\in G$ and closed under function composition or in other words the iterations of the left translations. $$\operatorname{iter}(a):\{L_a^{n}:n \in \Bbb N\setminus\{0\}\}$$ What are the weakest conditions that $(G,*)$ must satisfies if we want   that exist an collection injective functions $\mathcal F_a:\operatorname{iter}(a)\rightarrow > \operatorname{left}(G)$? $\mathcal F_a$ are defined in this way $$\mathcal F_a(L_a^1)=L_a$$ $$\mathcal F_a(L_a^n)=L_{a'}$$ $$\mathcal (\mathcal F_a(L_a^n))(x)=L_a^n(x)$$ I think that this is equivalent to the this statement $$\operatorname{iter}(a)\subseteq \operatorname{left}(G)$$. If $(G,*)$ is associative  then this always holds because $L_a(L_a(x))=L_{a*a}(x)$ and in general $\mathcal F_a(L_a^n)=L_{a^n}$ But this assumption is too stroong for my needs A weaker condition is that if $*$ is right invertible (exists a $R_a^{-1}$ such that $R_b^{-1}(a*b)=(a*b)\setminus_* b=a$)  then we can define the functions $\beta_a$ $$\beta_a(n)*b=a_1*\cdots(a_n*b)$$  $$\beta_a(n)=(a_1*\cdots(a_n*b))\setminus_* b$$ $$\beta_a(n)=R_b(L_a^n(b))$$ and these functions MUST be always constants (for every $b$) so we can define the injections $\mathcal F_a$ : $$L^n_a=L_{\beta_a(n)}$$ and thus $ \mathcal F_a$ exists $$\mathcal F_a(L_a^n)=L_{\beta_a(n)}$$ $1$-$\beta_a$ not depend from $b$ and is always costant, $\mathcal F_a$   exists and $\operatorname{iter}(a)\subseteq \operatorname{left}(G)$ are three equivalent statements? $3$-When $\beta_a$ satisfies that weak condition? $3$-If $(G,*)$ is not commutative, not associative and not left   invertible but is right invertible is possible that this bijection   from $\operatorname{iter}(a)\rightarrow \operatorname{left}(g)$ exists? Or maybe there can be a surjection? Note: the image of $\operatorname{iter}(a)$ by$\mathcal F_a$, $\mathcal F_a[\operatorname{iter}(a)]$ should be always a commutative subsemigroup of $(\operatorname{left}(G),\circ)$, commutative submonoid if $*$ has a left unit and a commutative  subgroup if $*$ is right invertible. Update: here a related question. Existence of an operation $\cdot$ such that $(a*(b*c))=(a\cdot b)*c$ Seems to me that the conditiont that the User Goos found is really similar to inclusion condition. Anyways I think that the questions here are still meaningfull.",,"['abstract-algebra', 'functions', 'magma']"
62,Proving a function is surjective given the composition is surjective [duplicate],Proving a function is surjective given the composition is surjective [duplicate],,"This question already has answers here : Show that if $g \circ f$ is injective, then so is $f$. (5 answers) Closed 8 years ago . Here is the question $f:X \rightarrow Y$ and $g: Y \rightarrow Z$ are functions and $g \circ f$ is surjective, is $g$ surjective? My proof: If $g \circ f$ is surjective than $\forall z \in Z \; \exists x \in X \; \mid (g \circ f)(x)=z  $ Suppose $f$ is surjective, than $\forall y \in Y \; \exists x \in X\; \mid f(x)=y$. By def. of $(g\circ f)(x)=z$ we have $g(f(x))=z$ and since $f(x)=y$ than we have $g(y)=z$ which implies surjectivity therefore $g$ is surjecive. Is this the correct way to prove that $g$ is surjective? Or can I not assume surjectivity on $f$","This question already has answers here : Show that if $g \circ f$ is injective, then so is $f$. (5 answers) Closed 8 years ago . Here is the question $f:X \rightarrow Y$ and $g: Y \rightarrow Z$ are functions and $g \circ f$ is surjective, is $g$ surjective? My proof: If $g \circ f$ is surjective than $\forall z \in Z \; \exists x \in X \; \mid (g \circ f)(x)=z  $ Suppose $f$ is surjective, than $\forall y \in Y \; \exists x \in X\; \mid f(x)=y$. By def. of $(g\circ f)(x)=z$ we have $g(f(x))=z$ and since $f(x)=y$ than we have $g(y)=z$ which implies surjectivity therefore $g$ is surjecive. Is this the correct way to prove that $g$ is surjective? Or can I not assume surjectivity on $f$",,"['functions', 'elementary-set-theory', 'discrete-mathematics', 'proof-verification', 'function-and-relation-composition']"
63,Is this function monotonically non-decreasing?,Is this function monotonically non-decreasing?,,"I am wondering if the function $L[n]$ defined on $n=0,1,2,\ldots,N$ below is ""monotonically"" non-decreasing in $n$.  I put monotonically in quotes because the function is not continuous and I am not sure if I can safely use the usual definition of monotone function here. $$\begin{array}{rcl}L[n]&=&\frac{1}{\binom{N}{n}}\sum_{t=0}^{\min(n,M)}\binom{M}{t}p^{t-n}(1-p)^{M-t-N+n}\binom{N-M}{n-t}q^{n-t}(1-q)^{N-M-n+t}\\ &=&\frac{(f_A\ast f_B)[n]}{f_X[n]}\end{array}$$ where $M$, $N$ are integers satisfying $0\leq M\leq N$, and $p$ and $q$ satisfy $0<p<q<1$. The second line gives the definition of $L[n]$ as the likelihood ratio between likelihoods of an observation of a random variable $X\sim\text{Binomial}(N,p)$ and $Y=A+B$, where $A\sim\text{Binomial}(M,p)$ and $B\sim\text{Binomial}(N-M,q)$ (hence the convolution).  This is related to a question I posted on stats.SE .  Any ideas? WHAT I'VE DONE I took the difference $L[n+1]-L[n]$ and came up with the following form: $$L[n+1]-L[n]=\left[\frac{1}{\binom{N}{n}}\sum_{t=0}^{\min(n,M)}l[t]\left(\frac{(N+1)(N-n-M+t)}{(N-n)(n+t+1)}\frac{q(1-p)}{p(1-q)}-1\right)\right]+\frac{\binom{M}{n+1}}{\binom{N}{n+1}}\left(\frac{1-q}{1-p}\right)^{N-M}$$ where $l[t]=\binom{M}{t}p^{t-n}(1-p)^{M-t-N+n}\binom{N-M}{n-t}q^{n-t}(1-q)^{N-M-n+t}$ is the summand in $L[n]$.  It's easy to show that $\frac{q(1-p)}{p(1-q)}>1$ when $q>p$, and if I can show that $\frac{(N+1)(N-n-M+t)}{(N-n)(n+t+1)}>1$ then I am done.  However, I am not sure if that is always true.  Perhaps there is a different way, or I am overlooking something?","I am wondering if the function $L[n]$ defined on $n=0,1,2,\ldots,N$ below is ""monotonically"" non-decreasing in $n$.  I put monotonically in quotes because the function is not continuous and I am not sure if I can safely use the usual definition of monotone function here. $$\begin{array}{rcl}L[n]&=&\frac{1}{\binom{N}{n}}\sum_{t=0}^{\min(n,M)}\binom{M}{t}p^{t-n}(1-p)^{M-t-N+n}\binom{N-M}{n-t}q^{n-t}(1-q)^{N-M-n+t}\\ &=&\frac{(f_A\ast f_B)[n]}{f_X[n]}\end{array}$$ where $M$, $N$ are integers satisfying $0\leq M\leq N$, and $p$ and $q$ satisfy $0<p<q<1$. The second line gives the definition of $L[n]$ as the likelihood ratio between likelihoods of an observation of a random variable $X\sim\text{Binomial}(N,p)$ and $Y=A+B$, where $A\sim\text{Binomial}(M,p)$ and $B\sim\text{Binomial}(N-M,q)$ (hence the convolution).  This is related to a question I posted on stats.SE .  Any ideas? WHAT I'VE DONE I took the difference $L[n+1]-L[n]$ and came up with the following form: $$L[n+1]-L[n]=\left[\frac{1}{\binom{N}{n}}\sum_{t=0}^{\min(n,M)}l[t]\left(\frac{(N+1)(N-n-M+t)}{(N-n)(n+t+1)}\frac{q(1-p)}{p(1-q)}-1\right)\right]+\frac{\binom{M}{n+1}}{\binom{N}{n+1}}\left(\frac{1-q}{1-p}\right)^{N-M}$$ where $l[t]=\binom{M}{t}p^{t-n}(1-p)^{M-t-N+n}\binom{N-M}{n-t}q^{n-t}(1-q)^{N-M-n+t}$ is the summand in $L[n]$.  It's easy to show that $\frac{q(1-p)}{p(1-q)}>1$ when $q>p$, and if I can show that $\frac{(N+1)(N-n-M+t)}{(N-n)(n+t+1)}>1$ then I am done.  However, I am not sure if that is always true.  Perhaps there is a different way, or I am overlooking something?",,"['probability', 'functions', 'summation', 'convolution']"
64,Is there any interesting interpretation of the set of all functions between two sets?,Is there any interesting interpretation of the set of all functions between two sets?,,"Is there any way to interpret the set of all functions from a set $X$ to a set $Y$? There is an interpretation of it as the cartesian product of $X$-many copies of $Y$, but I am asking for a more fun, if you want, interpretation. Maybe something of combinatorial flavour?","Is there any way to interpret the set of all functions from a set $X$ to a set $Y$? There is an interpretation of it as the cartesian product of $X$-many copies of $Y$, but I am asking for a more fun, if you want, interpretation. Maybe something of combinatorial flavour?",,"['elementary-set-theory', 'functions', 'soft-question']"
65,Edges and Vertices,Edges and Vertices,,"Three missionaries and three cannibals start on the left bank of a river. They have a rowboat with them that holds at most two people that can be used to transport people across the river (assume the rowboat can be rowed by either a missionary or a cannibal). Assuming that at no time the number of cannibals can outnumber the missionaries on either bank of the river how can all three missionaries and all three cannibals be transported safely across the river to the right bank. My answer would be to take a cannibal over first, then take a missionary, then another, and then you have 2 missionaries and 1 cannibal on the right, and 1 missionary and 1 cannibal still on the left with a cannibal crossing back but then that wouldn't work because now you will have a cannibal crossing back over to the left to pick up another person which will lead to 2 cannibals and 1 missionary.","Three missionaries and three cannibals start on the left bank of a river. They have a rowboat with them that holds at most two people that can be used to transport people across the river (assume the rowboat can be rowed by either a missionary or a cannibal). Assuming that at no time the number of cannibals can outnumber the missionaries on either bank of the river how can all three missionaries and all three cannibals be transported safely across the river to the right bank. My answer would be to take a cannibal over first, then take a missionary, then another, and then you have 2 missionaries and 1 cannibal on the right, and 1 missionary and 1 cannibal still on the left with a cannibal crossing back but then that wouldn't work because now you will have a cannibal crossing back over to the left to pick up another person which will lead to 2 cannibals and 1 missionary.",,['functions']
66,Wanted: simple invertible function with specified derivative properties,Wanted: simple invertible function with specified derivative properties,,"I'm looking for a positive function $F(x)$, defined for positive real numbers, with the following properties. $F(x)$ is expressible with the standard computer math library routines; $F(x)$ is invertible and its inversion expressible with the standard computer math library routines; $F'(x)>0$ and $F''\le0$. at $x\to\infty$: $F'\propto x^{-\gamma}$ with $1<\gamma<2$ a parameter; at $x\to0$: $F'\to\mathrm{const}$ and $F''\to0$ (ideally $F''\propto x$ in this limit). I couldn't find anything (despite some extensive search). The first condition really makes it hard. Of course, I could use numerical inversion, but this will be the second choice. Why do I need this? I want to sample $N$ points at positive $x$ with number density $n\propto x^{-\gamma}$ at large $x$ and continuously differentiable $n(r)=F'(|r|)$ (in particular at $r=0$). I don't want to sample these positions randomly, but equidistantly in the cumulated number $\propto F(x)$, hence the requirement to invert $F(x)$. Also, I don't want to suffer from round-off error more than absolutely necessary (hence preferentially no numerical inversion). edit So what did I try? $F(x)=x(1+x^{\gamma-1})^{1/(1-\gamma)}$ meets  criteria 1-4, but $F''(x)\to\infty$ at $x\to0$","I'm looking for a positive function $F(x)$, defined for positive real numbers, with the following properties. $F(x)$ is expressible with the standard computer math library routines; $F(x)$ is invertible and its inversion expressible with the standard computer math library routines; $F'(x)>0$ and $F''\le0$. at $x\to\infty$: $F'\propto x^{-\gamma}$ with $1<\gamma<2$ a parameter; at $x\to0$: $F'\to\mathrm{const}$ and $F''\to0$ (ideally $F''\propto x$ in this limit). I couldn't find anything (despite some extensive search). The first condition really makes it hard. Of course, I could use numerical inversion, but this will be the second choice. Why do I need this? I want to sample $N$ points at positive $x$ with number density $n\propto x^{-\gamma}$ at large $x$ and continuously differentiable $n(r)=F'(|r|)$ (in particular at $r=0$). I don't want to sample these positions randomly, but equidistantly in the cumulated number $\propto F(x)$, hence the requirement to invert $F(x)$. Also, I don't want to suffer from round-off error more than absolutely necessary (hence preferentially no numerical inversion). edit So what did I try? $F(x)=x(1+x^{\gamma-1})^{1/(1-\gamma)}$ meets  criteria 1-4, but $F''(x)\to\infty$ at $x\to0$",,"['functions', 'derivatives']"
67,Find the unit normal,Find the unit normal,,"find the unit normal $\bf \hat{N}$ of $${\bf r}=6 \mathrm{e}^{-14 t}\cos(t){\bf i}+6 \mathrm{e}^{-14 t}\sin(t){\bf j}$$ The answer should be in vector form. Use t as parameter. Write $e^x$ for exponentials. Have been working with this a long time now but cant get the right answer. My answer is  $$(-((e^{-14t})(\cos(t)-14\sin(t)))/((\sqrt{12})\sqrt{e^{-28t}}), (-((e^{-14t})(\sin(t)+14\cos(t)))/((\sqrt{12})\sqrt{(e^{-28t}})),0)$$ but it aint right. Thx for help!","find the unit normal $\bf \hat{N}$ of $${\bf r}=6 \mathrm{e}^{-14 t}\cos(t){\bf i}+6 \mathrm{e}^{-14 t}\sin(t){\bf j}$$ The answer should be in vector form. Use t as parameter. Write $e^x$ for exponentials. Have been working with this a long time now but cant get the right answer. My answer is  $$(-((e^{-14t})(\cos(t)-14\sin(t)))/((\sqrt{12})\sqrt{e^{-28t}}), (-((e^{-14t})(\sin(t)+14\cos(t)))/((\sqrt{12})\sqrt{(e^{-28t}})),0)$$ but it aint right. Thx for help!",,"['functions', 'vectors']"
68,Terminology Regarding Basic Properties of Functions,Terminology Regarding Basic Properties of Functions,,"Is there a cultural difference between saying that a function is 1-to-1 or injective, onto or surjective and a 1-to-1 correspondence or bijective?","Is there a cultural difference between saying that a function is 1-to-1 or injective, onto or surjective and a 1-to-1 correspondence or bijective?",,"['functions', 'terminology', 'math-history']"
69,"Associative, commutative properties and identity elements of non-binary functions","Associative, commutative properties and identity elements of non-binary functions",,"I'm making a compiler (for a new language) wich supports AC unification via pattern matching. The matching algorithms already works but i'm having trouble with the logical and mathematical aspects of functions and it's properties, wich will define in great ways the design of the language. I have several questions about function properties: Associative and commutative properties apply only on binary functions? For example, if I declare a function max(a,b), this will be commutative because max(a,b) = max(b,a) and associative because max(a,max(b,c)) = max(max(a,b), c), but I can't think of any function with more than two arguments that satisfy this axioms. For example I can define max(a,b,c) = max(a,max(b,c)) which will be a ternary function but the language will be able to unify it with the binary operations that conforms it. The unification works by reducing associative functions such as max(a,max(b,c)) onto a varydic and canonical form max(a,b,c) and then performing the pattern matching onto this canonical forms, so all (I think) possible functions that have this properties with more than 2 arguments are in fact composites of the same binary function Do identity elements apply only on binary functions? Explanaition: There can be a varydic function f(a,b,...) (more than 2 arguments) such that exist an element e that satisfies f(a,b,c,e) = f(a,b,c) for functions that doesn't have an immediate binary parent (such as addition is binary but the compiler manages addition as a varydic function for internal representantion) Unitiy elements such as zero in addition are managed in the language only by removing its aparences on functions for example add(1,2,x,0) wich represents the expression 1+2+x+0 reduces to 1+2+x This questions are decisive for the design of algorithms that automatically identify this properties on functions when rules are defined such as a+b = b+a and for the language design and the constraints that will impose to the declaration of functions, which can be, if false for any of this questions, illogical","I'm making a compiler (for a new language) wich supports AC unification via pattern matching. The matching algorithms already works but i'm having trouble with the logical and mathematical aspects of functions and it's properties, wich will define in great ways the design of the language. I have several questions about function properties: Associative and commutative properties apply only on binary functions? For example, if I declare a function max(a,b), this will be commutative because max(a,b) = max(b,a) and associative because max(a,max(b,c)) = max(max(a,b), c), but I can't think of any function with more than two arguments that satisfy this axioms. For example I can define max(a,b,c) = max(a,max(b,c)) which will be a ternary function but the language will be able to unify it with the binary operations that conforms it. The unification works by reducing associative functions such as max(a,max(b,c)) onto a varydic and canonical form max(a,b,c) and then performing the pattern matching onto this canonical forms, so all (I think) possible functions that have this properties with more than 2 arguments are in fact composites of the same binary function Do identity elements apply only on binary functions? Explanaition: There can be a varydic function f(a,b,...) (more than 2 arguments) such that exist an element e that satisfies f(a,b,c,e) = f(a,b,c) for functions that doesn't have an immediate binary parent (such as addition is binary but the compiler manages addition as a varydic function for internal representantion) Unitiy elements such as zero in addition are managed in the language only by removing its aparences on functions for example add(1,2,x,0) wich represents the expression 1+2+x+0 reduces to 1+2+x This questions are decisive for the design of algorithms that automatically identify this properties on functions when rules are defined such as a+b = b+a and for the language design and the constraints that will impose to the declaration of functions, which can be, if false for any of this questions, illogical",,"['abstract-algebra', 'functions', 'lambda-calculus', 'unification']"
70,Bounded Function limit proof,Bounded Function limit proof,,Let  $ f:\mathbb{R}\rightarrow \mathbb{R} $ be a bounded function and suppose  $$g(x) =\sup_{t>x}\hspace{.2cm} f(t)\ .$$ Show that $\lim_{x \rightarrow a^{+}} g(x)=g(a) $ for all real $a$. My firsts thoughts are if it is bounded it should converge to its supremum (I would have to prove that),Let  $ f:\mathbb{R}\rightarrow \mathbb{R} $ be a bounded function and suppose  $$g(x) =\sup_{t>x}\hspace{.2cm} f(t)\ .$$ Show that $\lim_{x \rightarrow a^{+}} g(x)=g(a) $ for all real $a$. My firsts thoughts are if it is bounded it should converge to its supremum (I would have to prove that),,"['real-analysis', 'limits', 'functions']"
71,Lower semicontinuous and discontinuous everywhere real bounded function?,Lower semicontinuous and discontinuous everywhere real bounded function?,,"Does there exist an $f:\mathbb{R}\rightarrow\mathbb{R}$ that is bounded such that for any $a$ then $f^{-1}(a,+\infty)$ is open but $f$ is discontinuous everywhere? Such a function seems too likely to exist, because it seems like you can just grab a nice continuous function, distort it slightly so that it won't ruin lower semicontinuous property but enough to make it lose continuity everywhere. Yet it seems much harder than I thought at first. I have not been able to construct one. Though I figure out that there cannot exist an onto and monotone function from the natural number into the range of $f$. This basically ruled out construction using series of scalar multiple of characteristic functions. I am also investigating a non-constructive method: trying to prove that discontinuous everywhere function are dense in the uniform metric space of function, and that lower semicontinuous function subspace is open in that space. So anyone have any other ideas? Or can help me with this current ideas?","Does there exist an $f:\mathbb{R}\rightarrow\mathbb{R}$ that is bounded such that for any $a$ then $f^{-1}(a,+\infty)$ is open but $f$ is discontinuous everywhere? Such a function seems too likely to exist, because it seems like you can just grab a nice continuous function, distort it slightly so that it won't ruin lower semicontinuous property but enough to make it lose continuity everywhere. Yet it seems much harder than I thought at first. I have not been able to construct one. Though I figure out that there cannot exist an onto and monotone function from the natural number into the range of $f$. This basically ruled out construction using series of scalar multiple of characteristic functions. I am also investigating a non-constructive method: trying to prove that discontinuous everywhere function are dense in the uniform metric space of function, and that lower semicontinuous function subspace is open in that space. So anyone have any other ideas? Or can help me with this current ideas?",,"['real-analysis', 'general-topology', 'functions']"
72,"Representation for a function that, when added/multiplied/composed with another function of the same form, yields a new function of the same form","Representation for a function that, when added/multiplied/composed with another function of the same form, yields a new function of the same form",,"I apologize for the possibly unclear wording of the title. I'm not well versed in math terminology. I'm after a concrete representation of a function, eg $y(x) = Ax^p$ (where $A$ and $p$ are constant), where multiplying $y_1(x)$ by $y_2(x)$, $(A_1x^{p_1})(A_2x^{p_2})$, results in a function of the same form*. In this case, $y_3(x) = y_1(x)*y_2(x) = A_1A_2x^{p_1+p_2}$, which, after combining constants, could be represented in the original form: $y_3(x) = y_1(x)*y_2(x) = A_3x^{p_3}$ However, I want this to also be true when adding two such functions, and composing them as in $y_3(x) = y_1(y_2(x))$. The example function can be composed, but does not appear to be addable. The constraints upon $y(x)$ is that the function must be able to pass through 3 provided points, which are guaranteed to either be all increasing or all decreasing (thus $y(x)$ will be either purely increasing or purely decreasing on its domain, though an operation involving $y(x)$ may result in a function that is not purely increasing/decreasing). *any operation involving two functions $y_1(x)$ and $y_2(x)$ must return a piecewise function composed solely of pieces in the form of $y(x)$. In the examples, the resulting functions can be considered to be piecewise functions with only one piece. If no such functional representation exists, I'd like to find a suitable alternative that can approximate the addition/multiplication/composition operations, perhaps through the use of many pieces. Additional context for those who may find use in it: Initially, I have a set of points in 2-dimensional space (numbered 1, 2, 3, 4, 5, ...) which I am to connect in a specific way - form a function that connects points 1, 2 and 3 smoothly and with no points of inflection or local extrema between point 1 and 3, then connect points 3, 4, and 5 smoothly & with no inflection/extrema between point 3 and 5 (though the curve could easily have a cusp at point 3), then connect points 5, 6, 7 smoothly, and so on. In general, a polynomial satisfies the majority of my requirements, but as I perform many operations on these polynomials, their degree becomes very high and it becomes too computationally expensive to evaluate the polynomial accurately. So, I've decided that the most obvious thing to do is represent my curve as a piecewise function of pieces that connect 3 points in the defined way. Thus my initial function of pieces in the form of $y(x)$ is exact and not an approximation. The difficulty comes in that I have many of these piecewise functions that I need to multiply, add, or compose with eachother. Addition/multiplication operations can be reduced to first splitting pieces such that each critical value of x where the functions change from one piece to the other is aligned, then performing the operation on the individual corresponding pieces of each function and concatenating the resulting pieces to form a new piecewise function. Composition can be achieved in a similar piece-by-piece way. In order to preserve the same form, each operation on each piece, $y(x)$, must result in a sequence of pieces each in the form $y_n(x)$. I mentioned approximation earlier because I am not sure that an exact solution is possible.","I apologize for the possibly unclear wording of the title. I'm not well versed in math terminology. I'm after a concrete representation of a function, eg $y(x) = Ax^p$ (where $A$ and $p$ are constant), where multiplying $y_1(x)$ by $y_2(x)$, $(A_1x^{p_1})(A_2x^{p_2})$, results in a function of the same form*. In this case, $y_3(x) = y_1(x)*y_2(x) = A_1A_2x^{p_1+p_2}$, which, after combining constants, could be represented in the original form: $y_3(x) = y_1(x)*y_2(x) = A_3x^{p_3}$ However, I want this to also be true when adding two such functions, and composing them as in $y_3(x) = y_1(y_2(x))$. The example function can be composed, but does not appear to be addable. The constraints upon $y(x)$ is that the function must be able to pass through 3 provided points, which are guaranteed to either be all increasing or all decreasing (thus $y(x)$ will be either purely increasing or purely decreasing on its domain, though an operation involving $y(x)$ may result in a function that is not purely increasing/decreasing). *any operation involving two functions $y_1(x)$ and $y_2(x)$ must return a piecewise function composed solely of pieces in the form of $y(x)$. In the examples, the resulting functions can be considered to be piecewise functions with only one piece. If no such functional representation exists, I'd like to find a suitable alternative that can approximate the addition/multiplication/composition operations, perhaps through the use of many pieces. Additional context for those who may find use in it: Initially, I have a set of points in 2-dimensional space (numbered 1, 2, 3, 4, 5, ...) which I am to connect in a specific way - form a function that connects points 1, 2 and 3 smoothly and with no points of inflection or local extrema between point 1 and 3, then connect points 3, 4, and 5 smoothly & with no inflection/extrema between point 3 and 5 (though the curve could easily have a cusp at point 3), then connect points 5, 6, 7 smoothly, and so on. In general, a polynomial satisfies the majority of my requirements, but as I perform many operations on these polynomials, their degree becomes very high and it becomes too computationally expensive to evaluate the polynomial accurately. So, I've decided that the most obvious thing to do is represent my curve as a piecewise function of pieces that connect 3 points in the defined way. Thus my initial function of pieces in the form of $y(x)$ is exact and not an approximation. The difficulty comes in that I have many of these piecewise functions that I need to multiply, add, or compose with eachother. Addition/multiplication operations can be reduced to first splitting pieces such that each critical value of x where the functions change from one piece to the other is aligned, then performing the operation on the individual corresponding pieces of each function and concatenating the resulting pieces to form a new piecewise function. Composition can be achieved in a similar piece-by-piece way. In order to preserve the same form, each operation on each piece, $y(x)$, must result in a sequence of pieces each in the form $y_n(x)$. I mentioned approximation earlier because I am not sure that an exact solution is possible.",,"['functions', 'approximation']"
73,The domain of fractional exponents,The domain of fractional exponents,,Take the following: $$f(x) = x^{6/4}$$ The domain of this function is all real numbers. This function can be simplified to: $$f(x) = x^{3/2}$$ The domain of this function is all real numbers greater than or equal to 0. Why is this true? Why does simplifying the function change its domain?,Take the following: $$f(x) = x^{6/4}$$ The domain of this function is all real numbers. This function can be simplified to: $$f(x) = x^{3/2}$$ The domain of this function is all real numbers greater than or equal to 0. Why is this true? Why does simplifying the function change its domain?,,"['algebra-precalculus', 'functions', 'exponentiation']"
74,How do I group unique pairs of sequential numbers in a grid?,How do I group unique pairs of sequential numbers in a grid?,,"Not sure if this SE site the best place to help find a solution for this problem. I am open to suggestions! It basically boils down to this: Given a grid of numbers where the numbers are ordered sequentially across and down (typewriter-style), how do I get unique vertical pairs of numbers using a formula, so that applying the formula to any given number in the grid will provide it's paired number? Some rules.... There is always either an even number of columns or an even number of rows The minimum number of columns is 1. The minimum number of rows is 2. A simple example would be a 6x2 grid: The pairs here are simple- 1-7, 2-8, 3-9, 4-10, 5-11, and 6-12.  If the function I am looking for was named myFunc, then in the simple 6x2 grid example above, myFunc(1) would return 7, and myFunc(7) would return 1. myFunc(4) would return 10, etc. I'd like to have good distribution of the pairs across the rows, too. So if we add 2 more rows to the example above, the pairs might look like this (color-coded here): The reds are pairs, like 1-13 and 2-20,  and the greens are pairs, like 7-19 and 8-14. This is just a suggestion of course; in a different version of pairing, 1-7 and 13-19 could be pairs, and 2-8 and 14-20 could be pairs, etc. I have come up with a formula in Excel which works for a 4-row grid of any number of columns. Unfortunately it is woefully inelegant and I wouldn’t even know how to best post it here. I can say it is a function of the MOD of the position number and row count, multiplied and added by the column count. And although it scales down to work with 2 rows, it does not scale to odd numbers of rows, or even 6 rows. ------ EDIT, 11 Hours Later ------ It would seem the ""magic"" of this lies in the calculated offset from the source number. Using the equation 1 + (ColNum mod (RowCount - 1)) it is easy to obtain the offset for the 1st row. Looking at the 1-13 pair, the offset is 2 (that is, the match is found 2 rows away from row 1). Looking at the 2-20 pair, the offset is 3. Looking at the 3-9 pair, the offset 1. The pattern continues for as many columns that reside in the grid. So this 2,3,1 sequence also works for row 3 in a 4-row grid. We only need to do a little subtraction if the values exceed the maximum position in the grid. For me, it's easy to visualize if I drop a ""phantom"" grid below the first to show that match: So here if we look at the 6-12 match, it’s easy to see the 12 is one row away from the 6. But we can also see that the 6 is 3 rows away from the 12 (count down into the phantom grid). For the 2nd and 4th rows of a 4-row grid, the pattern of offsets is 2,1,3... It is this 2,3,1 and 2,1,3 sequence that is the secret! I will refer to this 1,2, or 3 value as the ""offset"".  So, I need to calculate the offset based on the following variables: RowCount (in this example, 4) ColCount (in this example, 6) RowNum (in this example, 1,2,3 or 4) ColNum (in this example, 1,2,3,4,5 or 6) Pos (in this example, 1 through 24) I have found other sequences that work, but have been unable to calculate them. For a 6-row grid, the sequences are Row1 - 2,3,4,5,1... Row2 - 2,3,4,1,5... Row3 - 4,3,1,5,2... Row4 - 4,3,5,1,2... Row5 - 5,3,2,1,4... And as if this all weren't hairy enough (at least for me!) if we are dealing with an odd number of rows, a position's match may not be in the same column!","Not sure if this SE site the best place to help find a solution for this problem. I am open to suggestions! It basically boils down to this: Given a grid of numbers where the numbers are ordered sequentially across and down (typewriter-style), how do I get unique vertical pairs of numbers using a formula, so that applying the formula to any given number in the grid will provide it's paired number? Some rules.... There is always either an even number of columns or an even number of rows The minimum number of columns is 1. The minimum number of rows is 2. A simple example would be a 6x2 grid: The pairs here are simple- 1-7, 2-8, 3-9, 4-10, 5-11, and 6-12.  If the function I am looking for was named myFunc, then in the simple 6x2 grid example above, myFunc(1) would return 7, and myFunc(7) would return 1. myFunc(4) would return 10, etc. I'd like to have good distribution of the pairs across the rows, too. So if we add 2 more rows to the example above, the pairs might look like this (color-coded here): The reds are pairs, like 1-13 and 2-20,  and the greens are pairs, like 7-19 and 8-14. This is just a suggestion of course; in a different version of pairing, 1-7 and 13-19 could be pairs, and 2-8 and 14-20 could be pairs, etc. I have come up with a formula in Excel which works for a 4-row grid of any number of columns. Unfortunately it is woefully inelegant and I wouldn’t even know how to best post it here. I can say it is a function of the MOD of the position number and row count, multiplied and added by the column count. And although it scales down to work with 2 rows, it does not scale to odd numbers of rows, or even 6 rows. ------ EDIT, 11 Hours Later ------ It would seem the ""magic"" of this lies in the calculated offset from the source number. Using the equation 1 + (ColNum mod (RowCount - 1)) it is easy to obtain the offset for the 1st row. Looking at the 1-13 pair, the offset is 2 (that is, the match is found 2 rows away from row 1). Looking at the 2-20 pair, the offset is 3. Looking at the 3-9 pair, the offset 1. The pattern continues for as many columns that reside in the grid. So this 2,3,1 sequence also works for row 3 in a 4-row grid. We only need to do a little subtraction if the values exceed the maximum position in the grid. For me, it's easy to visualize if I drop a ""phantom"" grid below the first to show that match: So here if we look at the 6-12 match, it’s easy to see the 12 is one row away from the 6. But we can also see that the 6 is 3 rows away from the 12 (count down into the phantom grid). For the 2nd and 4th rows of a 4-row grid, the pattern of offsets is 2,1,3... It is this 2,3,1 and 2,1,3 sequence that is the secret! I will refer to this 1,2, or 3 value as the ""offset"".  So, I need to calculate the offset based on the following variables: RowCount (in this example, 4) ColCount (in this example, 6) RowNum (in this example, 1,2,3 or 4) ColNum (in this example, 1,2,3,4,5 or 6) Pos (in this example, 1 through 24) I have found other sequences that work, but have been unable to calculate them. For a 6-row grid, the sequences are Row1 - 2,3,4,5,1... Row2 - 2,3,4,1,5... Row3 - 4,3,1,5,2... Row4 - 4,3,5,1,2... Row5 - 5,3,2,1,4... And as if this all weren't hairy enough (at least for me!) if we are dealing with an odd number of rows, a position's match may not be in the same column!",,['functions']
75,Is there any intuition why this relation holds?,Is there any intuition why this relation holds?,,If $$D=\frac{d}{dx}$$ then we have $$\exp(D)f(x)=f(x+1)$$ What does it mean? Is there any intuition behind it?,If $$D=\frac{d}{dx}$$ then we have $$\exp(D)f(x)=f(x+1)$$ What does it mean? Is there any intuition behind it?,,"['functions', 'derivatives']"
76,Binary sequences and ${2}^{\mathbb{N}}$ have the same cardinality,Binary sequences and  have the same cardinality,{2}^{\mathbb{N}},"I recently got the book ""selected problems in real analysis"", and I'm stuck solving the very first problem $(u_n)$ is a binary sequence iff it only contains $0$ and $1$ in the sequence Let $A$ be the set of all binary sequences I have to prove that $A$ and ${2}^{\mathbb{N}}$ have the same cardinality, that is to say there exists a 1-1 function from one set to another I've thought about maybe considering integers as base-2 numbers Thanks for your help","I recently got the book ""selected problems in real analysis"", and I'm stuck solving the very first problem $(u_n)$ is a binary sequence iff it only contains $0$ and $1$ in the sequence Let $A$ be the set of all binary sequences I have to prove that $A$ and ${2}^{\mathbb{N}}$ have the same cardinality, that is to say there exists a 1-1 function from one set to another I've thought about maybe considering integers as base-2 numbers Thanks for your help",,"['elementary-set-theory', 'functions', 'cardinals']"
77,What's the Period of This Function?,What's the Period of This Function?,,"Consider the following function defined on the interval $[0,\pi]$ $$f(t)=\left\{\begin{array}{ll}t^2,\quad&\mbox{for } -\pi\leq t<\pi, \\ f(t-2k\pi),\quad&\mbox{for } -\pi+2k\pi\leq t<\pi+2k\pi.\end{array}\right.$$ What is the period for this function?  I'm trying to use the fourier series for this, but the formula requires knowing the period.  My first guess is that the period is $2\pi$ because it's going from $-\pi$ to $\pi$, but the second part of the function throws me off.  Please help?","Consider the following function defined on the interval $[0,\pi]$ $$f(t)=\left\{\begin{array}{ll}t^2,\quad&\mbox{for } -\pi\leq t<\pi, \\ f(t-2k\pi),\quad&\mbox{for } -\pi+2k\pi\leq t<\pi+2k\pi.\end{array}\right.$$ What is the period for this function?  I'm trying to use the fourier series for this, but the formula requires knowing the period.  My first guess is that the period is $2\pi$ because it's going from $-\pi$ to $\pi$, but the second part of the function throws me off.  Please help?",,"['algebra-precalculus', 'functions', 'periodic-functions']"
78,What is $\int^{\frac{\pi}{2}}_0 (x -[\sin x])dx$ equal to ( where [.] denotes the greatest integer function),What is  equal to ( where [.] denotes the greatest integer function),\int^{\frac{\pi}{2}}_0 (x -[\sin x])dx,Problem : $\int^{\frac{\pi}{2}}_0(x-[\sin x])~dx$ is equal to (where [.] denotes the greatest integer function) I have solved it the following way by separating two functions: i.e. $x$ and $[\sin x]$ by integrating first function I got $\dfrac{\pi^2}{8}$ however I am not getting the second part please help. Thanks a lot...,Problem : $\int^{\frac{\pi}{2}}_0(x-[\sin x])~dx$ is equal to (where [.] denotes the greatest integer function) I have solved it the following way by separating two functions: i.e. $x$ and $[\sin x]$ by integrating first function I got $\dfrac{\pi^2}{8}$ however I am not getting the second part please help. Thanks a lot...,,"['calculus', 'functions', 'integration']"
79,What is the double bracket notation used here?,What is the double bracket notation used here?,,Its kind of a bracket but I'm not sure what it means. I have two ideas about it: It means the number of times the expression in satisfied or it changes for $1$ or $0$ depending on the result every time the $i$ value changes.,Its kind of a bracket but I'm not sure what it means. I have two ideas about it: It means the number of times the expression in satisfied or it changes for $1$ or $0$ depending on the result every time the $i$ value changes.,,['notation']
80,What solutions are there to $R(1/x) = x^kR(x)$?,What solutions are there to ?,R(1/x) = x^kR(x),"What solutions are there to the functional equation $R(1/x) = x^kR(x)$, where $k$ is a non-negative integer? This is a generalization of the earlier question Functional equation: $R(1/x)/x^2 = R(x) $ which had $k=2$. A more general question is this: What solutions are there to the functional equation $p(x)R(1/x) = q(x)R(x)$, where $p(x)$ and $q(x)$ are pre-specified polynomials? I have only looked at the first equation, though my technique below might be a starting point for the second question. Here is what I have done so far. I will look for solutions  to $R(1/x) = x^kR(x)$ of the form $R(x) = A(x)/B(x)$, where $A$ and $B$ are  relatively prime polynomials of respective degrees $n$ and $m$, Some definitions: If $C(x)$ is a polynomial of degree $n$ (so $C(x) = \sum_{i=0}^n c_i x^i$), $[x^i]C(x)$ is the coefficient of $x^i$ in $C(x)$ (i.e., $[x^i]C(x) = c_i$), $deg(C(x)$ is the largest $i$ such that $[x^i]C(x)$ is non-zero, $codeg(C(x))$ is the smallest $i$ such that $[x^i]C(x)$ is non-zero, $\rho(C(x))$ is the reciprocal polynomial of $C(x)$ so that $\rho(C(x)) = x^n C(1/x) = \sum_{i=0}^n [x^i]C(x) x^{n-i} = \sum_{i=0}^n c_i x^{n-i}$. A polynomial $C(x)$ is $symmetric$ if $C(x) = \rho(C(x))$. Suppose $R(x) = A(x)/B(x)$ where $A(x)$ and $B(x)$ are  relatively prime polynomials of degrees $n$ and $m$, respectively. Let $\rho(A(x)) = a(x)$ and $\rho(B(x)) = b(x)$, so that $a(x) = x^nA(1/x)$ and $b(x) = x^mB(1/x)$. Let $u = deg(a(x))$ and $v = deg(b(x))$. We have $R(1/x)= A(1/x)/B(1/x)= (a(x)x^{-n})/(b(x)x^{-m})= x^{m-n}a(x)/b(x)$, so that $x^{m-n}a(x)/b(x) = x^k A(x)/B(x)$ or $x^ma(x)B(x) = x^{n+k}A(x)b(x)$. The degree of the left side is $m+u+m = 2m+u$ and the degree of the right side is $n+k+n+v = 2n+k+v$, so $2m+u=2n+k+v$. I now look at various possibilities for $A(x)$ and $B(x)$. If the degrees of $a(x)$ and $b(x)$ are the same as $A$ and $B$, respectively, (i.e., the constant terms of $A$ and $B$ are non-zero), then $u=n$ and $v=m$, so that $2m+n=2n+k+m$ or $m = n+k$. This means that $a(x)B(x) = A(x)b(x)$. Since $A(x)$ and $B(x)$ are relatively prime, so are $a(x)$ and $b(x)$, so that $a(x) = A(x)$ and $b(x) = B(x)$ which means that $A(x)$ and $B(x)$ are symmetric. Conversely, if $A(x)$ and $B(x)$ are symmetric and $deg(B(x)) = k+deg(A(x))$, it is easy to verify that $R(x)=A(x)/B(x)$ satisfies  $R(1/x) = x^kR(x)$. As a special case, if $B(x)$ is symmetric with degree $k$, then $1/B(x)$ satisfies the equation (i.e., $A(x) = 1$). Suppose $A(x) = x^n$, so that $a(x) = 1$ and $u = 0$. The condition $2m+u=2n+k+v$ becomes $2m=2n+k+v$ (or $v = 2m-2n-k$) and $x^ma(x)B(x) = x^{n+k}A(x)b(x)$ becomes $x^mB(x) = x^{2n+k}b(x)$. Since $v \le m$,  $2m=2n+k+v \le 2n+k+m$ or $m \le 2n+k$. so $B(x) = x^{2n+k-m}b(x)$. Therefore the part of $B(x)$  with the low-order zero coefficients removed is symmetric. The converse also holds. If $B(x) = x^m$, so $b(x) = 1$ and $v=0$, $2m+u=2n+k$  and $x^ma(x)B(x) = x^{n+k}A(x)b(x)$  becomes $x^{2m}a(x) = x^{2n+k}A(x)$ or $x^{2m-2n-k}a(x) = A(x)$.  Therefore, as with $B(x)$, above,  the part of $A(x)$  with the low-order zero coefficients removed  is symmetric.  The converse also holds.","What solutions are there to the functional equation $R(1/x) = x^kR(x)$, where $k$ is a non-negative integer? This is a generalization of the earlier question Functional equation: $R(1/x)/x^2 = R(x) $ which had $k=2$. A more general question is this: What solutions are there to the functional equation $p(x)R(1/x) = q(x)R(x)$, where $p(x)$ and $q(x)$ are pre-specified polynomials? I have only looked at the first equation, though my technique below might be a starting point for the second question. Here is what I have done so far. I will look for solutions  to $R(1/x) = x^kR(x)$ of the form $R(x) = A(x)/B(x)$, where $A$ and $B$ are  relatively prime polynomials of respective degrees $n$ and $m$, Some definitions: If $C(x)$ is a polynomial of degree $n$ (so $C(x) = \sum_{i=0}^n c_i x^i$), $[x^i]C(x)$ is the coefficient of $x^i$ in $C(x)$ (i.e., $[x^i]C(x) = c_i$), $deg(C(x)$ is the largest $i$ such that $[x^i]C(x)$ is non-zero, $codeg(C(x))$ is the smallest $i$ such that $[x^i]C(x)$ is non-zero, $\rho(C(x))$ is the reciprocal polynomial of $C(x)$ so that $\rho(C(x)) = x^n C(1/x) = \sum_{i=0}^n [x^i]C(x) x^{n-i} = \sum_{i=0}^n c_i x^{n-i}$. A polynomial $C(x)$ is $symmetric$ if $C(x) = \rho(C(x))$. Suppose $R(x) = A(x)/B(x)$ where $A(x)$ and $B(x)$ are  relatively prime polynomials of degrees $n$ and $m$, respectively. Let $\rho(A(x)) = a(x)$ and $\rho(B(x)) = b(x)$, so that $a(x) = x^nA(1/x)$ and $b(x) = x^mB(1/x)$. Let $u = deg(a(x))$ and $v = deg(b(x))$. We have $R(1/x)= A(1/x)/B(1/x)= (a(x)x^{-n})/(b(x)x^{-m})= x^{m-n}a(x)/b(x)$, so that $x^{m-n}a(x)/b(x) = x^k A(x)/B(x)$ or $x^ma(x)B(x) = x^{n+k}A(x)b(x)$. The degree of the left side is $m+u+m = 2m+u$ and the degree of the right side is $n+k+n+v = 2n+k+v$, so $2m+u=2n+k+v$. I now look at various possibilities for $A(x)$ and $B(x)$. If the degrees of $a(x)$ and $b(x)$ are the same as $A$ and $B$, respectively, (i.e., the constant terms of $A$ and $B$ are non-zero), then $u=n$ and $v=m$, so that $2m+n=2n+k+m$ or $m = n+k$. This means that $a(x)B(x) = A(x)b(x)$. Since $A(x)$ and $B(x)$ are relatively prime, so are $a(x)$ and $b(x)$, so that $a(x) = A(x)$ and $b(x) = B(x)$ which means that $A(x)$ and $B(x)$ are symmetric. Conversely, if $A(x)$ and $B(x)$ are symmetric and $deg(B(x)) = k+deg(A(x))$, it is easy to verify that $R(x)=A(x)/B(x)$ satisfies  $R(1/x) = x^kR(x)$. As a special case, if $B(x)$ is symmetric with degree $k$, then $1/B(x)$ satisfies the equation (i.e., $A(x) = 1$). Suppose $A(x) = x^n$, so that $a(x) = 1$ and $u = 0$. The condition $2m+u=2n+k+v$ becomes $2m=2n+k+v$ (or $v = 2m-2n-k$) and $x^ma(x)B(x) = x^{n+k}A(x)b(x)$ becomes $x^mB(x) = x^{2n+k}b(x)$. Since $v \le m$,  $2m=2n+k+v \le 2n+k+m$ or $m \le 2n+k$. so $B(x) = x^{2n+k-m}b(x)$. Therefore the part of $B(x)$  with the low-order zero coefficients removed is symmetric. The converse also holds. If $B(x) = x^m$, so $b(x) = 1$ and $v=0$, $2m+u=2n+k$  and $x^ma(x)B(x) = x^{n+k}A(x)b(x)$  becomes $x^{2m}a(x) = x^{2n+k}A(x)$ or $x^{2m-2n-k}a(x) = A(x)$.  Therefore, as with $B(x)$, above,  the part of $A(x)$  with the low-order zero coefficients removed  is symmetric.  The converse also holds.",,"['functions', 'polynomials']"
81,problem on monotonicitiy,problem on monotonicitiy,,"I encountered a quite intuitive statement (economic theory research; no homework etc.) but I cannot find a way to prove it: Let $f:[0,1]\rightarrow[0,1]$ be a cadlag function (i.e. right continuous with left side limit existing in each point) that has no downward jumps (i.e. right limit is at least as high as left limit). Assume $f(0)>f(1)$. Then there exists a point $x_0$ and a $\epsilon>0$ such that either (i) $f(x)\geq f(x_0)$ for all  $x\in(x_0-\epsilon,x_0)$ and $f(x)<f(x_0)$ for all $x\in(x_0,x_0+\epsilon)$ or (ii) $f(x)> f(x_0)$ for all  $x\in(x_0-\epsilon,x_0)$ and $f(x)\leq f(x_0)$ for all $x\in(x_0,x_0+\epsilon)$. Any ideas? EDIT: (Roughly speaking, the statement above says: A function that decreases globally has to decrease locally somewhere.)","I encountered a quite intuitive statement (economic theory research; no homework etc.) but I cannot find a way to prove it: Let $f:[0,1]\rightarrow[0,1]$ be a cadlag function (i.e. right continuous with left side limit existing in each point) that has no downward jumps (i.e. right limit is at least as high as left limit). Assume $f(0)>f(1)$. Then there exists a point $x_0$ and a $\epsilon>0$ such that either (i) $f(x)\geq f(x_0)$ for all  $x\in(x_0-\epsilon,x_0)$ and $f(x)<f(x_0)$ for all $x\in(x_0,x_0+\epsilon)$ or (ii) $f(x)> f(x_0)$ for all  $x\in(x_0-\epsilon,x_0)$ and $f(x)\leq f(x_0)$ for all $x\in(x_0,x_0+\epsilon)$. Any ideas? EDIT: (Roughly speaking, the statement above says: A function that decreases globally has to decrease locally somewhere.)",,"['real-analysis', 'functions']"
82,What's the mathematical field called where functions create and delete functions?,What's the mathematical field called where functions create and delete functions?,,"Motivation In the field of modular, reconfigurable robotics there are some groups which use term rewriting, or specifically graph rewriting to describe the reconfiguration process of the modular robots. Grammar rules reflect state changes: $$ a   b \rightarrow  a - c $$ would correspond to the real-world event in which a robot in state $a$ meets a robot in state $b$. The ""$-$""-token reflects a physical connection. And the rule states that the robot in state $b$ transitions into state $c$. This one rule is valid for the entire population of robots. The word ""stochastic"" is used in this context to reflect that robots in state $a$ and $b$ are picked at random from a large population of robots. General case Suppose we write this as: $$ f(P_{t}(x)) = P_{t+1}(x) $$ with $P_{t+1}(x)$ the distribution of states over the population, and $f$ our set of rewriting rules. It just does not seem to describe the same system. For example, suppose I would generalize a bit from rewriting states to rewriting functions: $$ f(a)   g(a) \rightarrow  g(a) h(a) $$ which would reflect the situation that in a set of functions (with unique names $f,g,h$), we would take a function $f$ and a function $g$, would replace $f$ with $g$ and would replace $g$ with $h$. The function $h$ can be taken out of a set of predefined functions, or be composed out of $f$ and $g$. The function $f$ here is ""deleted"". So, there is some kind of quantity of ""occurrence"" attached to each function. There must be a mathematical discipline that handles this type of function annihilation and creation. Or birth-death processes for functions. How is it called?","Motivation In the field of modular, reconfigurable robotics there are some groups which use term rewriting, or specifically graph rewriting to describe the reconfiguration process of the modular robots. Grammar rules reflect state changes: $$ a   b \rightarrow  a - c $$ would correspond to the real-world event in which a robot in state $a$ meets a robot in state $b$. The ""$-$""-token reflects a physical connection. And the rule states that the robot in state $b$ transitions into state $c$. This one rule is valid for the entire population of robots. The word ""stochastic"" is used in this context to reflect that robots in state $a$ and $b$ are picked at random from a large population of robots. General case Suppose we write this as: $$ f(P_{t}(x)) = P_{t+1}(x) $$ with $P_{t+1}(x)$ the distribution of states over the population, and $f$ our set of rewriting rules. It just does not seem to describe the same system. For example, suppose I would generalize a bit from rewriting states to rewriting functions: $$ f(a)   g(a) \rightarrow  g(a) h(a) $$ which would reflect the situation that in a set of functions (with unique names $f,g,h$), we would take a function $f$ and a function $g$, would replace $f$ with $g$ and would replace $g$ with $h$. The function $h$ can be taken out of a set of predefined functions, or be composed out of $f$ and $g$. The function $f$ here is ""deleted"". So, there is some kind of quantity of ""occurrence"" attached to each function. There must be a mathematical discipline that handles this type of function annihilation and creation. Or birth-death processes for functions. How is it called?",,"['functions', 'terminology']"
83,Showing a bijection with a contraction,Showing a bijection with a contraction,,"I have the function $F(x) = x + f(x)$ where $f(x)$ is a contraction: $|f(x)-f(y)| \leq \alpha|x-y|$ for some $0 < \alpha < 1$ and all $x, y \in \mathbb{R}$ I want to show that $F$ is a bijection: Proof of injection: By contradiction suppose $F$ is not injective then $F(x_1) = F(x_2)$ but $x_1 \neq x_2$. \begin{align*}     x_1 + f(x_1) &= x_2+f(x_2) \\     |x_1 - x_2| &= |f(x_2) - f(x_1)| \\ \end{align*} But we have that $|f(x_1)-f(x_2)| \leq \alpha|x-y|$ as $x_1 \neq x_2, |x_1 - x_2| \neq 0 \implies |f(x_1)-f(x_2)| \neq 0$ as $0<\alpha < 1$ then $|f(x_1)-f(x_2)| < |x-y|$ Thus we have a contradiction. Proof of surjection : Let $y\in\mathbb{R}$. We must show that there exists $x$ such that $F(x)=y$. Let $y = F(x)$ \begin{align} y = x + f(x)\\ \iff x = y - f(x) \end{align} We must show that for any $y$ and $f(x)$ we can find an $x$. I am not quite sure how to proceed from here. I think I need to use some properties of $f$ being a contraction, which will tell me that I can find this $x$ for all $f(x)$. Also, is my injective proof correct?","I have the function $F(x) = x + f(x)$ where $f(x)$ is a contraction: $|f(x)-f(y)| \leq \alpha|x-y|$ for some $0 < \alpha < 1$ and all $x, y \in \mathbb{R}$ I want to show that $F$ is a bijection: Proof of injection: By contradiction suppose $F$ is not injective then $F(x_1) = F(x_2)$ but $x_1 \neq x_2$. \begin{align*}     x_1 + f(x_1) &= x_2+f(x_2) \\     |x_1 - x_2| &= |f(x_2) - f(x_1)| \\ \end{align*} But we have that $|f(x_1)-f(x_2)| \leq \alpha|x-y|$ as $x_1 \neq x_2, |x_1 - x_2| \neq 0 \implies |f(x_1)-f(x_2)| \neq 0$ as $0<\alpha < 1$ then $|f(x_1)-f(x_2)| < |x-y|$ Thus we have a contradiction. Proof of surjection : Let $y\in\mathbb{R}$. We must show that there exists $x$ such that $F(x)=y$. Let $y = F(x)$ \begin{align} y = x + f(x)\\ \iff x = y - f(x) \end{align} We must show that for any $y$ and $f(x)$ we can find an $x$. I am not quite sure how to proceed from here. I think I need to use some properties of $f$ being a contraction, which will tell me that I can find this $x$ for all $f(x)$. Also, is my injective proof correct?",,"['functions', 'discrete-mathematics']"
84,How can I Create an integral that can only be evaluated via complex contour integration?,How can I Create an integral that can only be evaluated via complex contour integration?,,"In Richard Feynman's book, Surely You're Joking Mr. Feynman! , he says: One time I boasted, ""I can do by other methods any integral anybody else needs contour integration to do."" So Paul [Olum] puts up this tremendous damn integral he had obtained by starting out with a complex function that he knew the answer to, taking out the real part of it and leaving only the complex part. He had unwrapped it so it was only possible by contour integration! He was always deflating me like that. He was a very smart fellow. There is a similar question on the site about this, but I'd like to know how one can create an integral that can only be evaluated using contour integration. Can someone elaborate on what Paul Olum did to ""unwrap"" the integral and leave only the complex part?","In Richard Feynman's book, Surely You're Joking Mr. Feynman! , he says: One time I boasted, ""I can do by other methods any integral anybody else needs contour integration to do."" So Paul [Olum] puts up this tremendous damn integral he had obtained by starting out with a complex function that he knew the answer to, taking out the real part of it and leaving only the complex part. He had unwrapped it so it was only possible by contour integration! He was always deflating me like that. He was a very smart fellow. There is a similar question on the site about this, but I'd like to know how one can create an integral that can only be evaluated using contour integration. Can someone elaborate on what Paul Olum did to ""unwrap"" the integral and leave only the complex part?",,"['functions', 'integration']"
85,What's the payoff associated with the definition of a relation as an ordered triple?,What's the payoff associated with the definition of a relation as an ordered triple?,,"We can define a binary relation as a set of ordered pairs. Alternatively, we can call the set of ordered pairs the ""graph"" of the relation, and define the relation itself as a triple $(X,Y,f)$, where $f$ is the graph, $X$ is its domain and $Y$ is its codomain. The same goes for functions; they can be defined as sets of ordered pairs, or as triples. So my question is, given that its more lightweight to define a binary relation simply as its graph, why is the ordered-triple approach much more common? Here's a list of reasons and possible objections that I've come up with. Reason 0: The ordered-triple approach allows us to distinguish between surjections and non-surjections. Possible Objection: This could be read another way, as suggesting that surjectivity is a contrived concept. Maybe we should only every say ""$f \in \mathrm{Surj}(X,Y)$"" but never simply say ""$f$ is a surjection."" Reason 1: Category theory works that way. Possible Objection: Perhaps this is a ""hint"" that maybe there's room for improvement in the basic definitions of category theory. Reason 2: The ordered triple approach allows us to define the complement of a relation by $f^c = X \times Y \setminus f$. Thus, the set of all relations with domain $X$ and codomain $Y$ form a Boolean algebra. Possible Objection: This is a pretty minor advantage, given that we can just write ""Defining $A^c = X\times Y \setminus A$, it follows that..."" whenever we need to. So my question is, what's the payoff of the ordered triple approach?","We can define a binary relation as a set of ordered pairs. Alternatively, we can call the set of ordered pairs the ""graph"" of the relation, and define the relation itself as a triple $(X,Y,f)$, where $f$ is the graph, $X$ is its domain and $Y$ is its codomain. The same goes for functions; they can be defined as sets of ordered pairs, or as triples. So my question is, given that its more lightweight to define a binary relation simply as its graph, why is the ordered-triple approach much more common? Here's a list of reasons and possible objections that I've come up with. Reason 0: The ordered-triple approach allows us to distinguish between surjections and non-surjections. Possible Objection: This could be read another way, as suggesting that surjectivity is a contrived concept. Maybe we should only every say ""$f \in \mathrm{Surj}(X,Y)$"" but never simply say ""$f$ is a surjection."" Reason 1: Category theory works that way. Possible Objection: Perhaps this is a ""hint"" that maybe there's room for improvement in the basic definitions of category theory. Reason 2: The ordered triple approach allows us to define the complement of a relation by $f^c = X \times Y \setminus f$. Thus, the set of all relations with domain $X$ and codomain $Y$ form a Boolean algebra. Possible Objection: This is a pretty minor advantage, given that we can just write ""Defining $A^c = X\times Y \setminus A$, it follows that..."" whenever we need to. So my question is, what's the payoff of the ordered triple approach?",,"['functions', 'definition', 'relations']"
86,Monotonicity of a discrete function,Monotonicity of a discrete function,,"Let $k_1+k_2=k$, where $k, k_1$ are all positive integers with $k_1 \ge 1$. Also let $K=\min\{k_1, \lfloor k_2/9 \rfloor +1\}$. Define $g(x)=\max\{1 \le i \le k: \lfloor i/9 \rfloor +1=x\}, x=1, \ldots, K$.  Define for $x=1, \ldots, K$ that  \begin{align*} f(x)=\dfrac{\lfloor 0.1(x+g(x)) \rfloor +1}{k-x+\lfloor 0.1(x+g(x)) \rfloor +1} \end{align*} I would like to show that $f(x)/x$ is nondecreasing for $x=1,\ldots, K$. Can anyone give some hint?","Let $k_1+k_2=k$, where $k, k_1$ are all positive integers with $k_1 \ge 1$. Also let $K=\min\{k_1, \lfloor k_2/9 \rfloor +1\}$. Define $g(x)=\max\{1 \le i \le k: \lfloor i/9 \rfloor +1=x\}, x=1, \ldots, K$.  Define for $x=1, \ldots, K$ that  \begin{align*} f(x)=\dfrac{\lfloor 0.1(x+g(x)) \rfloor +1}{k-x+\lfloor 0.1(x+g(x)) \rfloor +1} \end{align*} I would like to show that $f(x)/x$ is nondecreasing for $x=1,\ldots, K$. Can anyone give some hint?",,"['calculus', 'functions']"
87,Does a continuous function preserve measurability?,Does a continuous function preserve measurability?,,"If $f$ is a continuous function on $X$ and E a Lebesgue measurable set, can we conclude that $f^{-1}(E)$ is measurable?","If $f$ is a continuous function on $X$ and E a Lebesgue measurable set, can we conclude that $f^{-1}(E)$ is measurable?",,"['measure-theory', 'functions']"
88,On which of the following spaces is every continuous (real-valued) function bounded,On which of the following spaces is every continuous (real-valued) function bounded,,"On which of the following spaces is every continuous (real-valued) function bounded?   i) $X_1 = (0, 1)$;   ii) $X_2 = [0,1]$;   iii) $X_3 = [0, 1)$;   iv) $X_4 =\{t \in [0, 1] : t \mbox{ irrational}\}$. (i) is not true: example $f(x)=\frac 1x$ . (ii) I think this is true as the interval is closed and bounded. (iii) is not true. Example: $f(x)=\frac 1{1-x}$. (iv) I think this is true as this is a subset of (ii). Am I right?","On which of the following spaces is every continuous (real-valued) function bounded?   i) $X_1 = (0, 1)$;   ii) $X_2 = [0,1]$;   iii) $X_3 = [0, 1)$;   iv) $X_4 =\{t \in [0, 1] : t \mbox{ irrational}\}$. (i) is not true: example $f(x)=\frac 1x$ . (ii) I think this is true as the interval is closed and bounded. (iii) is not true. Example: $f(x)=\frac 1{1-x}$. (iv) I think this is true as this is a subset of (ii). Am I right?",,"['real-analysis', 'functions', 'continuity']"
89,Minimizing a function over two variables,Minimizing a function over two variables,,"Given two natural numbers $i$ and $p$ such that $0 < i \leqslant 2^p$, let $$ \psi(p,i) := p - \alpha + 1 - \frac{1}{2^p}\left((2^p+i)\lg(2^p+i) - i\lg i - i + \alpha - \frac{2^p}{i+1} -     \frac{i}{2^p+1}\right), $$ where $\alpha \simeq 1.264499$ and $\lg n$ is the binary logarithm of $n$ . I am looking for $\min_{p,i}\psi(p,i)$. It seems that $\min_{i}\psi(p,i) = \psi(p,2^p)$ or $\psi(p,2^p-1)$. Anyway, I can then minimize over $p$. We have $$ \frac{\partial\psi}{\partial i}(p,i) = - \frac{1}{(i+1)^2} - \frac{1}{2^p}\left(\lg(2^p+i) - \lg i - \frac{1}{2^p+1} - 1\right). $$ (The derivative with respect to $p$ is much worse.) Also $$ \lim_{i \rightarrow 0^{+}}\frac{\partial\psi}{\partial i}(p,i) = -\infty,\quad \text{and}\quad \left.\frac{\partial\psi}{\partial i}(p,i)\right|_{i=2^p} \!\!= \frac{1}{2^p(2^p+1)^2} > 0. $$ The derivative is strictly increasing (the second derivative is positive). How can I prove that the root of $\partial\psi/\partial i = 0$ is between $i=2^p-1$ and $i=2^p$? And which of these values is the minimum?","Given two natural numbers $i$ and $p$ such that $0 < i \leqslant 2^p$, let $$ \psi(p,i) := p - \alpha + 1 - \frac{1}{2^p}\left((2^p+i)\lg(2^p+i) - i\lg i - i + \alpha - \frac{2^p}{i+1} -     \frac{i}{2^p+1}\right), $$ where $\alpha \simeq 1.264499$ and $\lg n$ is the binary logarithm of $n$ . I am looking for $\min_{p,i}\psi(p,i)$. It seems that $\min_{i}\psi(p,i) = \psi(p,2^p)$ or $\psi(p,2^p-1)$. Anyway, I can then minimize over $p$. We have $$ \frac{\partial\psi}{\partial i}(p,i) = - \frac{1}{(i+1)^2} - \frac{1}{2^p}\left(\lg(2^p+i) - \lg i - \frac{1}{2^p+1} - 1\right). $$ (The derivative with respect to $p$ is much worse.) Also $$ \lim_{i \rightarrow 0^{+}}\frac{\partial\psi}{\partial i}(p,i) = -\infty,\quad \text{and}\quad \left.\frac{\partial\psi}{\partial i}(p,i)\right|_{i=2^p} \!\!= \frac{1}{2^p(2^p+1)^2} > 0. $$ The derivative is strictly increasing (the second derivative is positive). How can I prove that the root of $\partial\psi/\partial i = 0$ is between $i=2^p-1$ and $i=2^p$? And which of these values is the minimum?",,"['real-analysis', 'functions', 'inequality', 'optimization']"
90,"Prove an image of function $f:[a,b]\to\mathbb R^n:t\mapsto(f^1(t),f^2(t),\ldots,f^n(t))$ where $f^i\in C^\infty$ doesn't contain open ball",Prove an image of function  where  doesn't contain open ball,"f:[a,b]\to\mathbb R^n:t\mapsto(f^1(t),f^2(t),\ldots,f^n(t)) f^i\in C^\infty","$\mathbb R^n\supset[a,b]$ is domain of definition $f:[a,b]\to\mathbb R^n:t\mapsto(f^1(t),f^2(t),\ldots,f^n(t))$ where $f^i\in C^\infty$ I need to prove that image of $f$, that means $f[a,b]$, doesn't contain any open ball in it. Intuitively, if image contains an open ball, then $f$ has a lot of singular points in which it has no derivative. But I have difficulties with the proof of the statement. Can you help me?","$\mathbb R^n\supset[a,b]$ is domain of definition $f:[a,b]\to\mathbb R^n:t\mapsto(f^1(t),f^2(t),\ldots,f^n(t))$ where $f^i\in C^\infty$ I need to prove that image of $f$, that means $f[a,b]$, doesn't contain any open ball in it. Intuitively, if image contains an open ball, then $f$ has a lot of singular points in which it has no derivative. But I have difficulties with the proof of the statement. Can you help me?",,"['calculus', 'real-analysis', 'functions']"
91,"What can we say about functions satisfying $f(a + b) = f(a)f(b) $ for all $a,b\in \mathbb{R}$? [duplicate]",What can we say about functions satisfying  for all ? [duplicate],"f(a + b) = f(a)f(b)  a,b\in \mathbb{R}","This question already has answers here : Closed 12 years ago . Possible Duplicate: Is there a name for such kind of function? I am investigating functions satisfying the exponentiation identity $f(a + b) = f(a)f(b)$ for all $a,b\in \Bbb R$. This is satisfied by $f(x) = \exp(A x)$ but what other functions are possible, if any? What properties must such functions have? This is what I have figured out so far - I am interested in any other properties that are required. $f(0) \in \{0,1\}$ Proof: $f(0) = f(0 + 0) = f(0)f(0) \implies$ either $f(0) = 0$ or dividing by $f(0)$ gives $f(0) = 1$ $f(x)$ is strictly positive everywhere or zero everywhere Proof: If $f(0) = 0$, then $f(x) = f(x+0) = f(x)f(0) = f(x) \cdot 0 = 0$ for all $x$ If $f(0) = 1$ then first we show that $f(x)$ is non zero for all $x$: $$1 = f(0) = f(x - x) = f(x)f(-x)$$ which implies that both $f(x)$ and $f(-x)$ can not be zero and this is true for all $x$. $$f(x) = f(x/2 + x/2) = f(x/2)^2$$  but we already know that $f(x/2)$ is non zero, hence its square is strictly positive. Corollary: $f(-x) = 1/f(x)$ if $f(x)$ is non-zero. For the rest of this question lets assume the non-trivial case where $f(x)$ is strictly positive. $f(p/q) = f(1)^{p/q}$ for all $p/q \in \Bbb Q$ Proof: We have $$f(1/q) ^ q = \underbrace{f(1/q)\cdots f(1/q)}_{q \text{ times }} = f(q/q) = f(1)$$ so taking the $q^{\text{th}}$ root, $f(1/q) = f(1)^{1/q}$. $$f(px) = \underbrace{f(x)\cdots f(x)}_{p \text{ times }} = f(x)^p$$ Combining these we get the result. Setting $A = \log(f(1))$ we see that $f(x) = \exp(A_t x)$ where $x = p/q \in \Bbb Q$. But what about $x \in \Bbb R - \Bbb Q$ (irrational values of $x$)? If $f$ is required to be continuous, then by the density of $\Bbb Q$ in $\Bbb R$, $f(x) = \exp(A_t x)$ everywhere. But if $f$ is not required to be continuous then I think I can define $f(x) = \exp(A_t x)$ where $x$ in $\Bbb Q$ where $t$ in some coset $\Bbb R / \Bbb Q$ and $t \Bbb Q = {t + q \text{ where } q\in \Bbb Q}$ and $A_t$ is different for each coset. This makes for quite an interesting function.  (FYI The cosets of $\Bbb R / \Bbb Q$ are discussed in this question What do the cosets of $\mathbb{R} / \mathbb{Q}$ look like? ) I am pretty sure in this case that $f(x)$ is not Lesbesque integrable. I am not sure what else we can tell about $f(x)$...","This question already has answers here : Closed 12 years ago . Possible Duplicate: Is there a name for such kind of function? I am investigating functions satisfying the exponentiation identity $f(a + b) = f(a)f(b)$ for all $a,b\in \Bbb R$. This is satisfied by $f(x) = \exp(A x)$ but what other functions are possible, if any? What properties must such functions have? This is what I have figured out so far - I am interested in any other properties that are required. $f(0) \in \{0,1\}$ Proof: $f(0) = f(0 + 0) = f(0)f(0) \implies$ either $f(0) = 0$ or dividing by $f(0)$ gives $f(0) = 1$ $f(x)$ is strictly positive everywhere or zero everywhere Proof: If $f(0) = 0$, then $f(x) = f(x+0) = f(x)f(0) = f(x) \cdot 0 = 0$ for all $x$ If $f(0) = 1$ then first we show that $f(x)$ is non zero for all $x$: $$1 = f(0) = f(x - x) = f(x)f(-x)$$ which implies that both $f(x)$ and $f(-x)$ can not be zero and this is true for all $x$. $$f(x) = f(x/2 + x/2) = f(x/2)^2$$  but we already know that $f(x/2)$ is non zero, hence its square is strictly positive. Corollary: $f(-x) = 1/f(x)$ if $f(x)$ is non-zero. For the rest of this question lets assume the non-trivial case where $f(x)$ is strictly positive. $f(p/q) = f(1)^{p/q}$ for all $p/q \in \Bbb Q$ Proof: We have $$f(1/q) ^ q = \underbrace{f(1/q)\cdots f(1/q)}_{q \text{ times }} = f(q/q) = f(1)$$ so taking the $q^{\text{th}}$ root, $f(1/q) = f(1)^{1/q}$. $$f(px) = \underbrace{f(x)\cdots f(x)}_{p \text{ times }} = f(x)^p$$ Combining these we get the result. Setting $A = \log(f(1))$ we see that $f(x) = \exp(A_t x)$ where $x = p/q \in \Bbb Q$. But what about $x \in \Bbb R - \Bbb Q$ (irrational values of $x$)? If $f$ is required to be continuous, then by the density of $\Bbb Q$ in $\Bbb R$, $f(x) = \exp(A_t x)$ everywhere. But if $f$ is not required to be continuous then I think I can define $f(x) = \exp(A_t x)$ where $x$ in $\Bbb Q$ where $t$ in some coset $\Bbb R / \Bbb Q$ and $t \Bbb Q = {t + q \text{ where } q\in \Bbb Q}$ and $A_t$ is different for each coset. This makes for quite an interesting function.  (FYI The cosets of $\Bbb R / \Bbb Q$ are discussed in this question What do the cosets of $\mathbb{R} / \mathbb{Q}$ look like? ) I am pretty sure in this case that $f(x)$ is not Lesbesque integrable. I am not sure what else we can tell about $f(x)$...",,"['real-analysis', 'group-theory', 'functions', 'continuity', 'functional-equations']"
92,Getting an upper bound for a function of two variables,Getting an upper bound for a function of two variables,,"I have a complicated looking expression involving two naturals numbers $n,k$ where $n\ge k\ge 3$: $$f(n,k)=2^{k-2}(2n-k+1)\big(\big\lfloor\tfrac{n}{k-1}\big\rfloor-1\big)+(1-k)2^{k-3}\Big(\big\lfloor\tfrac{n}{k-1}\big\rfloor\big(\big\lfloor\tfrac{n}{k-1}\big\rfloor+1\big)-2\Big)-(n-k+1)\Big(\big\lfloor\tfrac{n}{k-1}\big\rfloor-2\Big).$$ I wish to find a good upper bound $f(n,k)\le g(n,k)$ such that $g(n,k)=1$ can be solved for $n$. The problem is the floor of the first function on the right hand side can be easily eliminated but because the second/third term may be negative the same cannot be done for the whole $f(n,k)$. Basically my question is how do I get an upper bound? Thanks","I have a complicated looking expression involving two naturals numbers $n,k$ where $n\ge k\ge 3$: $$f(n,k)=2^{k-2}(2n-k+1)\big(\big\lfloor\tfrac{n}{k-1}\big\rfloor-1\big)+(1-k)2^{k-3}\Big(\big\lfloor\tfrac{n}{k-1}\big\rfloor\big(\big\lfloor\tfrac{n}{k-1}\big\rfloor+1\big)-2\Big)-(n-k+1)\Big(\big\lfloor\tfrac{n}{k-1}\big\rfloor-2\Big).$$ I wish to find a good upper bound $f(n,k)\le g(n,k)$ such that $g(n,k)=1$ can be solved for $n$. The problem is the floor of the first function on the right hand side can be easily eliminated but because the second/third term may be negative the same cannot be done for the whole $f(n,k)$. Basically my question is how do I get an upper bound? Thanks",,"['algebra-precalculus', 'functions']"
93,Does $\sqrt{\delta^2}$ make more sense than $\delta^2$?,Does  make more sense than ?,\sqrt{\delta^2} \delta^2,"What is the Product of $\delta$ functions with itself? was already asked some time ago. In a comment the OP states: I want to create $\delta(t)$ such that the product with itself is also $\delta(t)$ and the innerproduct $\int_{-\infty}^\infty \delta(t)\cdot\delta(t) dt = 1$ and retain all other properties of the usual $\delta(t)$. I feel that I'm in a comparable situation: I'd like to have a function $\Delta(x)$, such that $\displaystyle\int_{-\infty}^\infty\Delta(x)^2 f(x) dx=\int_{-\infty}^\infty\delta(x)f(x)dx$ and $\displaystyle\int_{-\infty}^\infty\Delta(x-t)\Delta(x+t)f(x)dx=0$ for $t\neq 0$. Combining these gives: $$ \int_{-\infty}^\infty\Delta(x-t)\Delta(x+t)f(x)dx=\delta_{t0}\int_{-\infty}^\infty\delta(x)f(x)dx, $$ with $\delta_{xy}$ being the Kronecker Delta . I think that following could work: Let $\Delta_a(x\pm t)=\frac{1}{a \sqrt{\pi}} \mathrm{e}^{-(x\pm t)^2/\color{red}{2}a^2}$ (little related to the so called nascent delta function ). The $\color{red}{2}$ in the denominator inspired the title and I hope it was not misleading you, sorry if so. When I now, for example, set $f(x)=1$ and calculate $$ \lim_{a\to 0}\int_{-\infty}^\infty\Delta_a(x-t)\Delta_a(x+t)dx, $$ I get exactly what I need. So my question is: Is this OK, and if not can I save it somehow?","What is the Product of $\delta$ functions with itself? was already asked some time ago. In a comment the OP states: I want to create $\delta(t)$ such that the product with itself is also $\delta(t)$ and the innerproduct $\int_{-\infty}^\infty \delta(t)\cdot\delta(t) dt = 1$ and retain all other properties of the usual $\delta(t)$. I feel that I'm in a comparable situation: I'd like to have a function $\Delta(x)$, such that $\displaystyle\int_{-\infty}^\infty\Delta(x)^2 f(x) dx=\int_{-\infty}^\infty\delta(x)f(x)dx$ and $\displaystyle\int_{-\infty}^\infty\Delta(x-t)\Delta(x+t)f(x)dx=0$ for $t\neq 0$. Combining these gives: $$ \int_{-\infty}^\infty\Delta(x-t)\Delta(x+t)f(x)dx=\delta_{t0}\int_{-\infty}^\infty\delta(x)f(x)dx, $$ with $\delta_{xy}$ being the Kronecker Delta . I think that following could work: Let $\Delta_a(x\pm t)=\frac{1}{a \sqrt{\pi}} \mathrm{e}^{-(x\pm t)^2/\color{red}{2}a^2}$ (little related to the so called nascent delta function ). The $\color{red}{2}$ in the denominator inspired the title and I hope it was not misleading you, sorry if so. When I now, for example, set $f(x)=1$ and calculate $$ \lim_{a\to 0}\int_{-\infty}^\infty\Delta_a(x-t)\Delta_a(x+t)dx, $$ I get exactly what I need. So my question is: Is this OK, and if not can I save it somehow?",,"['functions', 'distribution-theory']"
94,Introduction to Elementary Functions,Introduction to Elementary Functions,,"I'm looking for an introductory text on algebraic treatment of elementary functions. Really short and easy-going. Video lectures are even better. I want to learn basic ideas (i.e. definitions) behind integration and differentiation of elementary functions. I'm also interested in how computer algebra systems like REDUCE or AXIOM treat them as rational functions with some ""kernels"" (exponents and logarithm's and so on). Though ideally I'd be happy to start with just polynomials and exponents (sin, cos). So to write symbolic integration and differentiation for a simpler class of functions (no logarithms, division and roots).","I'm looking for an introductory text on algebraic treatment of elementary functions. Really short and easy-going. Video lectures are even better. I want to learn basic ideas (i.e. definitions) behind integration and differentiation of elementary functions. I'm also interested in how computer algebra systems like REDUCE or AXIOM treat them as rational functions with some ""kernels"" (exponents and logarithm's and so on). Though ideally I'd be happy to start with just polynomials and exponents (sin, cos). So to write symbolic integration and differentiation for a simpler class of functions (no logarithms, division and roots).",,"['abstract-algebra', 'reference-request', 'functions', 'computer-algebra-systems', 'differential-algebra']"
95,"Find a bijection between $[1,2)$ and $(1,2)$ [duplicate]",Find a bijection between  and  [duplicate],"[1,2) (1,2)","This question already has answers here : How to define a bijection between $(0,1)$ and $(0,1]$? (9 answers) Closed 2 years ago . I want to find a bijection between $[1,2)$ and $(1,2)$ and prove it. My attempt: $[1,2) = \{x \in \mathbb R | 1 \leq x <2\}$ $(1,2) = \{x \in \mathbb R | 1 < x < 2\}$ $f(x) = x$ if $x \ne 1\frac{1}{n}$ for $n = 1,2,3,...$ and $f(x) = 1\frac{1}{x+1}$ , if $x =1\frac{1}{n}$ for $n = 1,2,3,...$ Proof - Injective - Prove $x_1 = x_2$ for $f(x) = x$ \begin{align*}   f(x_1) = f(x_2) &\implies x_1 = x_2.\\ \end{align*} Proof - Injective - Prove $x_1 = x_2$ for $f(x) = 1\frac{1}{x+1}$ \begin{align*}   f(x_1) = f(x_2) &\implies 1\frac{1}{x_1+1} = 1\frac{1}{x_2+1}\\ 	&\implies \frac{1}{x_1+1} = \frac{1}{x_2+1}\\ 	&\implies x_2+1 = x_1+1\\ 	&\implies x_2 = x_1.\\ \end{align*} Therefore $f$ is injective. Any help will be appreciated. Thanks.","This question already has answers here : How to define a bijection between $(0,1)$ and $(0,1]$? (9 answers) Closed 2 years ago . I want to find a bijection between and and prove it. My attempt: if for and , if for Proof - Injective - Prove for Proof - Injective - Prove for Therefore is injective. Any help will be appreciated. Thanks.","[1,2) (1,2) [1,2) = \{x \in \mathbb R | 1 \leq x <2\} (1,2) = \{x \in \mathbb R | 1 < x < 2\} f(x) = x x \ne 1\frac{1}{n} n = 1,2,3,... f(x) = 1\frac{1}{x+1} x =1\frac{1}{n} n = 1,2,3,... x_1 = x_2 f(x) = x \begin{align*}
  f(x_1) = f(x_2) &\implies x_1 = x_2.\\
\end{align*} x_1 = x_2 f(x) = 1\frac{1}{x+1} \begin{align*}
  f(x_1) = f(x_2) &\implies 1\frac{1}{x_1+1} = 1\frac{1}{x_2+1}\\
	&\implies \frac{1}{x_1+1} = \frac{1}{x_2+1}\\
	&\implies x_2+1 = x_1+1\\
	&\implies x_2 = x_1.\\
\end{align*} f",['functions']
96,Can a self-inverse function $y=f(x)$ always be expressed as an equation that is symmetric in $x$ and $y$?,Can a self-inverse function  always be expressed as an equation that is symmetric in  and ?,y=f(x) x y,"For example, the simple self-inverse function $y = 6 - x$ can be written as $x+y=6$ , which is symmetric in $x$ and $y$ .  Less apparent (to me at least), $y = (x+1)/(1-x)$ can be expanded and written as $x + y = xy - 1$ , which is also symmetric in $x$ and $y$ , and must therefore be self-inverse. Can all self-inverse functions be expressed this way?","For example, the simple self-inverse function can be written as , which is symmetric in and .  Less apparent (to me at least), can be expanded and written as , which is also symmetric in and , and must therefore be self-inverse. Can all self-inverse functions be expressed this way?",y = 6 - x x+y=6 x y y = (x+1)/(1-x) x + y = xy - 1 x y,['functions']
97,"periodic function, find g(6)","periodic function, find g(6)",,"If $f$ is periodic, $g$ is polynomial function, $f(g(x))$ is periodic, $g(2)=3$ , and $g(4)=7$ , then $g(6)$ is A) 13 B) 15 C) 11 D) none of these The answer is c) 11, but I did not understand how $g(x)$ was considered linear polynomial (because i got answer when $g(x)$ is $2n -1$ ), isn't there any $g(x)$ with degree greater than 1 make $f(g(x))$ periodic? Why? How will you solve the problem?","If is periodic, is polynomial function, is periodic, , and , then is A) 13 B) 15 C) 11 D) none of these The answer is c) 11, but I did not understand how was considered linear polynomial (because i got answer when is ), isn't there any with degree greater than 1 make periodic? Why? How will you solve the problem?",f g f(g(x)) g(2)=3 g(4)=7 g(6) g(x) g(x) 2n -1 g(x) f(g(x)),"['functions', 'polynomials']"
98,Inconsequent use of coordinates in function evaluation and differentiation.,Inconsequent use of coordinates in function evaluation and differentiation.,,"Assume $X = (0,\infty)$ , $Y = \mathbb{R}$ and that $f : X \to Y$ is a function with some rule, for example $$ f(x) = ax; \qquad a\in \mathbb{R} := A. $$ These symbols provide the following inference: ( $i$ ) the point $x$ in the domain of $f$ is associated to the point $ax \in Y$ by the relation $f$ . ( $ii$ ) the point $x$ in the domain of $f$ is associated to the point $$ \lim_{t \to 0} \frac{1}{t} \left[ a(x + t) - a(x) \right] \in Y  $$ by the relation $\partial f/\partial x$ . Two concerns: I have seen authors switch between the notation $f(x)$ and $f(b)$ carelessly. That is no problem for inference provided above, because in the first instance I know that I should associate $x \mapsto ax$ and in the second $b \mapsto ab$ .  However, some authors seem to switch between « $f(x)$ » and « $f(a)$ » as to claim that «the symbol $x$ appears in the rule $f$ » and «the symbol $a$ appears in the rule $f$ » respectively. Every time this happens, I wonder if the picture in ( $i$ ) and ( $ii$ ) is indeed correct and/or if it is reconcilable with that meaning. However, this dilemma becomes vivid if you consider the derivative: Suppose (to my horror) that an author wrote $\partial f/ \partial b = 0$ and I wanted to give meaning to this expression. It seems to me that the formula for the derivative no longer applies because we were never told how to define the limit (the derivative) for any other coordinate than $x$ (the coordinate on the domain of $f$ ). At best, I can give meaning to $\partial f/ \partial a$ if I redefine the function $f : A \to Y$ with a different domain by $f(a) = ax$ because the symbol $a$ appears in the rule $f$ above. However, I would not know how to define the limit with respect to the symbol $b$ , because it does not appear in the product $ax$ . If the view in ( $i$ ) and ( $ii$ ) is indeed correct, can you explain what happens when authors write $f(a)$ «seemingly to say that $f$ contains the symbol $a$ » and $\partial f/\partial b = 0$ «seemingly to say that $f$ does not contain the symbol $b$ » such that these notions can be reconciled? If ( $i$ ) or ( $ii$ ) is false, please let me know what is correct.","Assume , and that is a function with some rule, for example These symbols provide the following inference: ( ) the point in the domain of is associated to the point by the relation . ( ) the point in the domain of is associated to the point by the relation . Two concerns: I have seen authors switch between the notation and carelessly. That is no problem for inference provided above, because in the first instance I know that I should associate and in the second .  However, some authors seem to switch between « » and « » as to claim that «the symbol appears in the rule » and «the symbol appears in the rule » respectively. Every time this happens, I wonder if the picture in ( ) and ( ) is indeed correct and/or if it is reconcilable with that meaning. However, this dilemma becomes vivid if you consider the derivative: Suppose (to my horror) that an author wrote and I wanted to give meaning to this expression. It seems to me that the formula for the derivative no longer applies because we were never told how to define the limit (the derivative) for any other coordinate than (the coordinate on the domain of ). At best, I can give meaning to if I redefine the function with a different domain by because the symbol appears in the rule above. However, I would not know how to define the limit with respect to the symbol , because it does not appear in the product . If the view in ( ) and ( ) is indeed correct, can you explain what happens when authors write «seemingly to say that contains the symbol » and «seemingly to say that does not contain the symbol » such that these notions can be reconciled? If ( ) or ( ) is false, please let me know what is correct.","X = (0,\infty) Y = \mathbb{R} f : X \to Y 
f(x) = ax; \qquad a\in \mathbb{R} := A.
 i x f ax \in Y f ii x f 
\lim_{t \to 0} \frac{1}{t} \left[ a(x + t) - a(x) \right] \in Y 
 \partial f/\partial x f(x) f(b) x \mapsto ax b \mapsto ab f(x) f(a) x f a f i ii \partial f/ \partial b = 0 x f \partial f/ \partial a f : A \to Y f(a) = ax a f b ax i ii f(a) f a \partial f/\partial b = 0 f b i ii","['functions', 'derivatives', 'differential-geometry', 'manifolds', 'smooth-manifolds']"
99,Correct Notation of Mean Value Theorem for Vector-Valued Function,Correct Notation of Mean Value Theorem for Vector-Valued Function,,"Let ${\bf f}({\bf x})$ be a function ${\bf f}: \mathbb{R}^n \to \mathbb{R}^m$ with continuous derivatives ${\bf H}({\bf x})$ . We wish to approximate ${\bf f}({\bf x}_0)$ by ${\bf f}({\bf x})$ . It is well known that for $m > 1$ , we cannot guarantee the existence of a vector $\bf \tilde{x}$ between ${\bf x}_0$ and ${\bf x}$ such that $${\bf f}({\bf x}) = {\bf f}({\bf x}_0) + {\bf H}({\bf \tilde{x}})({\bf x}-{\bf x}_0) $$ Hence, the MVT cannot be directly applied to vector-valued function. However, a straightforward modification to the MVT can yield fruitful results, but seems ignored by many sources (including this very site). We can apply the MVT to each of the $m$ components of ${\bf f}$ separately, and thus we can write $${f_k}({\bf x}) = f_k({\bf x}_0) + {\bf h}_k({\bf \tilde{x}}_k)({\bf x}-{\bf x}_0) , k = 1,...,m$$ I have seen some authors combine the $m$ results and write $${\bf f}({\bf x}) = {\bf f}({\bf x}_0) + {\bf H}({\bf {x}_*})({\bf x}-{\bf x}_0) $$ where ${\bf {x}_*} =[{\bf \tilde{x}}_1, ...,{\bf \tilde{x}}_m]^T$ is now a $m \times n$ matrix, and ${\bf H}({\bf {x}_*}) = [{\bf h}_1({\bf \tilde{x}}_1),...,{\bf h}_m({\bf \tilde{x}}_m)]^T$ . Although I remember seeing this notation multiple times, I can't remember how exactly how it was expressed, or in which article I viewed it. Could someone kindly reminds me how to properly use this notation for vector-valued functions?","Let be a function with continuous derivatives . We wish to approximate by . It is well known that for , we cannot guarantee the existence of a vector between and such that Hence, the MVT cannot be directly applied to vector-valued function. However, a straightforward modification to the MVT can yield fruitful results, but seems ignored by many sources (including this very site). We can apply the MVT to each of the components of separately, and thus we can write I have seen some authors combine the results and write where is now a matrix, and . Although I remember seeing this notation multiple times, I can't remember how exactly how it was expressed, or in which article I viewed it. Could someone kindly reminds me how to properly use this notation for vector-valued functions?","{\bf f}({\bf x}) {\bf f}: \mathbb{R}^n \to \mathbb{R}^m {\bf H}({\bf x}) {\bf f}({\bf x}_0) {\bf f}({\bf x}) m > 1 \bf \tilde{x} {\bf x}_0 {\bf x} {\bf f}({\bf x}) = {\bf f}({\bf x}_0) + {\bf H}({\bf \tilde{x}})({\bf x}-{\bf x}_0)  m {\bf f} {f_k}({\bf x}) = f_k({\bf x}_0) + {\bf h}_k({\bf \tilde{x}}_k)({\bf x}-{\bf x}_0) , k = 1,...,m m {\bf f}({\bf x}) = {\bf f}({\bf x}_0) + {\bf H}({\bf {x}_*})({\bf x}-{\bf x}_0)  {\bf {x}_*} =[{\bf \tilde{x}}_1, ...,{\bf \tilde{x}}_m]^T m \times n {\bf H}({\bf {x}_*}) = [{\bf h}_1({\bf \tilde{x}}_1),...,{\bf h}_m({\bf \tilde{x}}_m)]^T","['functions', 'derivatives', 'vectors', 'partial-derivative']"
