,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"In search of a ""perfect"" test on (positive) series convergence","In search of a ""perfect"" test on (positive) series convergence",,"Thus far mathematicians have developed many powerful tests on the convergence of a positive series (I mean $\displaystyle\sum_{i=1}^{\infty}a_i$ specifically), such as : Cauchy's Test which deals with the upper limit of $\lambda_n=\sqrt[n]{a_n}$, but comes to nothing when the upper limit is $1$. D'Alembert's Test which deals with the upper or lower limit of $\lambda_n=\frac{a_{n+1}}{a_n}$, but comes to nothing when the upper limit $\ge1$ or the lower limit $\le1$. Raabe's Test which deals with $\lambda_n= n\Big(\frac{x_n}{x_{n+1}}-1\Big)$, but comes to nothing when $\lim{\lambda_n}=1$. Bertrand's Test which deals with  $\lambda_n= (\ln n)\Big[n\Big(\frac{x_n}{x_{n+1}}-1\Big)-1\Big]$, but still comes to nothing when $\lim\lambda_n=1$. $$\vdots$$ And in my books it says, this progress never ends, ""...We can always go on and establish a even more powerful test, with more complicated proof, though..."" I don't really know how, but since this is not the point of my question, you may just skip it. Well, anyhow I hope you could take a close look at these tests. They are really brilliant in that they can tell you whether the series converges or otherwise based directly on the information of $a_n$, which won't be too obscure. But they are still NOT perfect. They are all ""if"" type but not ""iff"" type. I mean, they are all of such pattern: The series converges if $\lim \lambda_n$ blah blah blah, and diverges if $\lim \lambda_n$ blah blah blah. The worst is, if they both fail us, WE KNOW NOTHING! How I hope that I could replace ""if""s with ""iff""s, and get rid of the ""we know nothing"" case! So I'm quite wondering whether there is a perfect test that: (1) is based directly on $\lambda_n$ which $a_n$ gives rise to (2) is of such pattern as: The series converges iff $\lim \lambda_n$ blah blah blah, and diverges iff $\lim \lambda_n$ blah blah blah. (Say, we know everything!) I know there might be only dim hope, but I'm still curious. Any help will be specially appreciated. Best regards! Further Note In the first place, I'm sincerely grateful to all the help you guys provide for me. However, I'm afraid I have to make a note here because many answers posted here are not what I'm looking for. Well I'm far from criticizing, but I think I need to perhaps make my question clearer so that I'm not misleading your answers. The problem is that some answers here do not really meet the requirement (1) mentioned in my question. Please read (1) closely, I want the test to be based directly on $\lambda_n$ to which $a_n$ gives rise, just like the $\lambda_n$s in the tests listed above. In other words, $\lambda_n$ should be immediately accessible via $a_n$, or, $a_n$ gives all the immediate information needed to write out $\lambda_n$. Therefore, $\lambda_n$ is an expression that contains $a_n, a_{n+1}$ etc etc. I don't want to involve the partial sum in my test, nor am I looking for something like a powerful comparison test, because the knowledge of $a_n$ usually cannot enable us to gain the knowledge of the partial sum, or to find another $b_n$ to compare to. In short, I desire something that is based only and immediately on $a_n$. Thanks. (And, apologies if my post should look too wordy. I'm not a skillful English user)","Thus far mathematicians have developed many powerful tests on the convergence of a positive series (I mean $\displaystyle\sum_{i=1}^{\infty}a_i$ specifically), such as : Cauchy's Test which deals with the upper limit of $\lambda_n=\sqrt[n]{a_n}$, but comes to nothing when the upper limit is $1$. D'Alembert's Test which deals with the upper or lower limit of $\lambda_n=\frac{a_{n+1}}{a_n}$, but comes to nothing when the upper limit $\ge1$ or the lower limit $\le1$. Raabe's Test which deals with $\lambda_n= n\Big(\frac{x_n}{x_{n+1}}-1\Big)$, but comes to nothing when $\lim{\lambda_n}=1$. Bertrand's Test which deals with  $\lambda_n= (\ln n)\Big[n\Big(\frac{x_n}{x_{n+1}}-1\Big)-1\Big]$, but still comes to nothing when $\lim\lambda_n=1$. $$\vdots$$ And in my books it says, this progress never ends, ""...We can always go on and establish a even more powerful test, with more complicated proof, though..."" I don't really know how, but since this is not the point of my question, you may just skip it. Well, anyhow I hope you could take a close look at these tests. They are really brilliant in that they can tell you whether the series converges or otherwise based directly on the information of $a_n$, which won't be too obscure. But they are still NOT perfect. They are all ""if"" type but not ""iff"" type. I mean, they are all of such pattern: The series converges if $\lim \lambda_n$ blah blah blah, and diverges if $\lim \lambda_n$ blah blah blah. The worst is, if they both fail us, WE KNOW NOTHING! How I hope that I could replace ""if""s with ""iff""s, and get rid of the ""we know nothing"" case! So I'm quite wondering whether there is a perfect test that: (1) is based directly on $\lambda_n$ which $a_n$ gives rise to (2) is of such pattern as: The series converges iff $\lim \lambda_n$ blah blah blah, and diverges iff $\lim \lambda_n$ blah blah blah. (Say, we know everything!) I know there might be only dim hope, but I'm still curious. Any help will be specially appreciated. Best regards! Further Note In the first place, I'm sincerely grateful to all the help you guys provide for me. However, I'm afraid I have to make a note here because many answers posted here are not what I'm looking for. Well I'm far from criticizing, but I think I need to perhaps make my question clearer so that I'm not misleading your answers. The problem is that some answers here do not really meet the requirement (1) mentioned in my question. Please read (1) closely, I want the test to be based directly on $\lambda_n$ to which $a_n$ gives rise, just like the $\lambda_n$s in the tests listed above. In other words, $\lambda_n$ should be immediately accessible via $a_n$, or, $a_n$ gives all the immediate information needed to write out $\lambda_n$. Therefore, $\lambda_n$ is an expression that contains $a_n, a_{n+1}$ etc etc. I don't want to involve the partial sum in my test, nor am I looking for something like a powerful comparison test, because the knowledge of $a_n$ usually cannot enable us to gain the knowledge of the partial sum, or to find another $b_n$ to compare to. In short, I desire something that is based only and immediately on $a_n$. Thanks. (And, apologies if my post should look too wordy. I'm not a skillful English user)",,"['calculus', 'real-analysis', 'sequences-and-series', 'analysis']"
1,Definition of $L^0$ space,Definition of  space,L^0,"From Wikipedia : The vector space of (equivalence classes of) measurable functions on $(S, Σ, μ)$ is denoted $L^0(S, Σ, μ)$. This doesn't seem connected to the definition of $L^p(S, Σ, μ), \forall p \in (0, \infty)$ as being the set of measurable functions $f$ such that $\int_S |f|^p d\mu <\infty$. So I wonder if I miss any connection, and why use the notation $L^0$ if there is no connection? Thanks and regards!","From Wikipedia : The vector space of (equivalence classes of) measurable functions on $(S, Σ, μ)$ is denoted $L^0(S, Σ, μ)$. This doesn't seem connected to the definition of $L^p(S, Σ, μ), \forall p \in (0, \infty)$ as being the set of measurable functions $f$ such that $\int_S |f|^p d\mu <\infty$. So I wonder if I miss any connection, and why use the notation $L^0$ if there is no connection? Thanks and regards!",,['real-analysis']
2,"Show that the sequence $\sqrt{2},\sqrt{2\sqrt{2}},\sqrt{2\sqrt{2\sqrt{2}}},...$ converges and find its limit. [duplicate]",Show that the sequence  converges and find its limit. [duplicate],"\sqrt{2},\sqrt{2\sqrt{2}},\sqrt{2\sqrt{2\sqrt{2}}},...","This question already has answers here : How I can prove that the sequence $\sqrt{2} , \sqrt{2\sqrt{2}}, \sqrt{2\sqrt{2\sqrt{2}}}$ converges to 2? (8 answers) Closed 7 years ago . Show that the sequence $$\sqrt{2},\sqrt{2\sqrt{2}},\sqrt{2\sqrt{2\sqrt{2}}},...$$ converges and find its limit. I put the sequence in this form , $(x_n)$ where $$\large x_n=2^{\Large\sum_{k=1}^{n}\left(\frac{1}{2^k}\right)}$$ I want to use Monotonic Convergence Theorem to show. But I stuck at proving the sequence is bounded above by 2. I manage to prove the sequence is an increasing sequence. Anyone can guide me ? I try to use induction to prove but i stuck at inductive step.","This question already has answers here : How I can prove that the sequence $\sqrt{2} , \sqrt{2\sqrt{2}}, \sqrt{2\sqrt{2\sqrt{2}}}$ converges to 2? (8 answers) Closed 7 years ago . Show that the sequence $$\sqrt{2},\sqrt{2\sqrt{2}},\sqrt{2\sqrt{2\sqrt{2}}},...$$ converges and find its limit. I put the sequence in this form , $(x_n)$ where $$\large x_n=2^{\Large\sum_{k=1}^{n}\left(\frac{1}{2^k}\right)}$$ I want to use Monotonic Convergence Theorem to show. But I stuck at proving the sequence is bounded above by 2. I manage to prove the sequence is an increasing sequence. Anyone can guide me ? I try to use induction to prove but i stuck at inductive step.",,['real-analysis']
3,Partial sums of exponential series,Partial sums of exponential series,,"What is known about $f(k)=\sum_{n=0}^{k-1} \frac{k^n}{n!}$ for large $k$? Obviously it is is a partial sum of the series for $e^k$ -- but this partial sum doesn't reach close to $e^k$ itself because we're cutting off the series right at the largest terms. In the full series, the $(k+i-1)$th term is always at least as large as the $(k-i)$th term for $1\le i\le k$, so $f(k)< e^k/2$. Can we estimate more precisely how much smaller than $e^k$ the function is? It would look very nice and pleasing if, say, $f(k)\sim e^{k-1}$ for large $k$, but I have no real evidence for that hypothesis. (Inspired by this question and my answer thereto).","What is known about $f(k)=\sum_{n=0}^{k-1} \frac{k^n}{n!}$ for large $k$? Obviously it is is a partial sum of the series for $e^k$ -- but this partial sum doesn't reach close to $e^k$ itself because we're cutting off the series right at the largest terms. In the full series, the $(k+i-1)$th term is always at least as large as the $(k-i)$th term for $1\le i\le k$, so $f(k)< e^k/2$. Can we estimate more precisely how much smaller than $e^k$ the function is? It would look very nice and pleasing if, say, $f(k)\sim e^{k-1}$ for large $k$, but I have no real evidence for that hypothesis. (Inspired by this question and my answer thereto).",,['real-analysis']
4,Derivative of a linear transformation.,Derivative of a linear transformation.,,"We define derivatives of functions as linear transformations of $R^n \to R^m$. Now talking about the derivative of such linear transformation ,  as we know if $x \in R^n$ , then $A(x+h)-A(x)=A(h)$, because of linearity of $A$, which implies that $A'(x)=A$ where , $A'$ is derivative of $A$ .  What does this mean? I am not getting the point I think.","We define derivatives of functions as linear transformations of $R^n \to R^m$. Now talking about the derivative of such linear transformation ,  as we know if $x \in R^n$ , then $A(x+h)-A(x)=A(h)$, because of linearity of $A$, which implies that $A'(x)=A$ where , $A'$ is derivative of $A$ .  What does this mean? I am not getting the point I think.",,['real-analysis']
5,What is the Atiyah-Singer index theorem about?,What is the Atiyah-Singer index theorem about?,,"I was just a little bit curious about the general statement of this theorem. Honestly, I am not at all interested in fully understanding this, so it is not that I am too lazy to read plenty of books about it, but I would like to know a little bit more, what this means. Therefore I would like to go with an example: Let $(Tf)(x):= \frac{df}{dx}(x)+\sin(x)f(x)$ be a differential operator on $[0,L]$ for some $L \>>0$ . Apparently, the first question would be: Is this operator Fredholm? I do understand what  it means for an operator to be Fredholm and I understand the definition of the Fredholm index, but I don't see whether this one actually is such an example of a Fredholm operator. In case that this is true. Where does topology come into play? I know the definition of an index for a path, but this topological index seems to be different. Maybe this example is not that good, as we are not studying something on any abstract manifolds, but still, could anybody elaborate on this?","I was just a little bit curious about the general statement of this theorem. Honestly, I am not at all interested in fully understanding this, so it is not that I am too lazy to read plenty of books about it, but I would like to know a little bit more, what this means. Therefore I would like to go with an example: Let be a differential operator on for some . Apparently, the first question would be: Is this operator Fredholm? I do understand what  it means for an operator to be Fredholm and I understand the definition of the Fredholm index, but I don't see whether this one actually is such an example of a Fredholm operator. In case that this is true. Where does topology come into play? I know the definition of an index for a path, but this topological index seems to be different. Maybe this example is not that good, as we are not studying something on any abstract manifolds, but still, could anybody elaborate on this?","(Tf)(x):= \frac{df}{dx}(x)+\sin(x)f(x) [0,L] L \>>0","['real-analysis', 'general-topology']"
6,"If every convergent subsequence converges to $a$, then so does the original bounded sequence (Abbott p 58 q2.5.4 and q2.5.3b)","If every convergent subsequence converges to , then so does the original bounded sequence (Abbott p 58 q2.5.4 and q2.5.3b)",a,"Assume $(a_{n})$ is a bounded sequence such that every convergent subsequence of $(a_{n})$ converges to the same limit $a\in \mathbb{R}$. Show $(a_{n})$ must converge to $a$. Prove by contradiction . Prove the contrapositive: If $(a_{n})$ is a bounded sequence that doesn't converge to the number $a\in \mathbb{R}$, then there is some subsequence that converges to $b \neq a$ (videlicet, not every convergent subsequence converges to a). 1. How to presage proof by contrapositive? Why not a direct proof? Suppose $(a_{n})$ is a bounded sequence that does not converge to $a$. In other words, negate [ for every $\epsilon >0$, there is an $N\in \mathbb{N}$ such that, for all $n\geq N,\ |a_{n}-a|<\epsilon$ ]. Pass the negation through the various quantifiers. Then this means that there is some $\epsilon_{0}>0$ such that, for all $N\in \mathbb{N}$, there is an $n \geq N$ such that $|a_{n}-a|\geq\epsilon_{0}$. Stated in plainer language, we're assuming that there is some $\epsilon_{0}>0$ such that infinitely many of the $a_{n}$'s are at a distance at least $\epsilon_{0}$ away from $a$. Now, these infinitely many $a_{n}$'s form a subsequence; call it $(a_{n_{k}})$ . Since $(a_{n})$ is bounded, the subsequence $(a_{n_{k}})$ is also bounded. Thus, by the Bolzano-Weierstrass Theorem, $(a_{n_{k}})$ contains a convergent subsequence $(a_{n_{k_{\ell}}})\rightarrow b$. Since each of the $a_{n_{k_{\ell}}}$ is at a distance at least $\epsilon_{0}$ from $a$, so $b\neq a$. Ergo, not every convergent subsequence of $(a_{n})$ converges to $a. \square $ 2. I understand the steps of the proof, but not the modus operandi of the proof.   Question posits a convergent subsequence, so we work with $\{a_{n_k}\}.$ How to presage we need a convergent sub sub sequence $\{a_{\Large{{n_{k_l}}}}\} $? 3. Intuition please? Figure please? I forgot I asked about this earlier, but this question betters the old one .","Assume $(a_{n})$ is a bounded sequence such that every convergent subsequence of $(a_{n})$ converges to the same limit $a\in \mathbb{R}$. Show $(a_{n})$ must converge to $a$. Prove by contradiction . Prove the contrapositive: If $(a_{n})$ is a bounded sequence that doesn't converge to the number $a\in \mathbb{R}$, then there is some subsequence that converges to $b \neq a$ (videlicet, not every convergent subsequence converges to a). 1. How to presage proof by contrapositive? Why not a direct proof? Suppose $(a_{n})$ is a bounded sequence that does not converge to $a$. In other words, negate [ for every $\epsilon >0$, there is an $N\in \mathbb{N}$ such that, for all $n\geq N,\ |a_{n}-a|<\epsilon$ ]. Pass the negation through the various quantifiers. Then this means that there is some $\epsilon_{0}>0$ such that, for all $N\in \mathbb{N}$, there is an $n \geq N$ such that $|a_{n}-a|\geq\epsilon_{0}$. Stated in plainer language, we're assuming that there is some $\epsilon_{0}>0$ such that infinitely many of the $a_{n}$'s are at a distance at least $\epsilon_{0}$ away from $a$. Now, these infinitely many $a_{n}$'s form a subsequence; call it $(a_{n_{k}})$ . Since $(a_{n})$ is bounded, the subsequence $(a_{n_{k}})$ is also bounded. Thus, by the Bolzano-Weierstrass Theorem, $(a_{n_{k}})$ contains a convergent subsequence $(a_{n_{k_{\ell}}})\rightarrow b$. Since each of the $a_{n_{k_{\ell}}}$ is at a distance at least $\epsilon_{0}$ from $a$, so $b\neq a$. Ergo, not every convergent subsequence of $(a_{n})$ converges to $a. \square $ 2. I understand the steps of the proof, but not the modus operandi of the proof.   Question posits a convergent subsequence, so we work with $\{a_{n_k}\}.$ How to presage we need a convergent sub sub sequence $\{a_{\Large{{n_{k_l}}}}\} $? 3. Intuition please? Figure please? I forgot I asked about this earlier, but this question betters the old one .",,['real-analysis']
7,"The Cantor set is homeomorphic to infinite product of $\{0,1\}$ with itself - cylinder basis - and it topology",The Cantor set is homeomorphic to infinite product of  with itself - cylinder basis - and it topology,"\{0,1\}","I know the Cantor set probably comes up in homework questions all the time but I didn't find many clues - that I understood at least. I am, for a homework problem, supposed to show that the Cantor set is homeomorphic to the infinite product (I am assuming countably infinite?) of $\{0,1\}$ with itself. So members of this two-point space(?) are things like $(0,0,0,1)$ and $(0,1,1,1,1,1,1)$ , etc. Firstly, I think that a homeomorphism (the 'topological isomorhism') is a mapping between two topologies (for the Cantor sets which topology is this? discrete?) that have continuous, bijective functions. So I am pretty lost and don't even know what more to say! :( I have seen something like this in reading some texts, something about $$f: \sum_{i=1}^{+\infty}\,\frac{a_i}{3^i} \mapsto \sum_{i=1}^{+\infty}\,\frac{a_i}{2^{i+1}} ,$$ for $a_i = 0,2$ . But in some ways this seems to be a 'complement' of what I need.... Apparently I am to use ternary numbers represented using only $0$ 's and $1$ 's in; for example, $0.a_1\,a_2\,\ldots = 0.01011101$ ? Thanks much for any help starting out! Here is the verbatim homework question: The standard measure on the Cantor set is given by the Cantor $\phi$ function which is constant on missing thirds and dyadic on ternary rationals. Show the Cantor set is homeomorphic to the infinite product of $\{0,1\}$ with itself. How should we topologize this product? (Hint: this product is the same as the set of all infinite binary sequences) Fix a binary $n$ -tuple $(a_1,\ldots, a_n)$ (for e.g., $(0,1,1,0,0,0)$ if $n = 6$ ). Show that the Cantor measure of points ( $b_k$ ) with $b_k=a_k$ for $k \leq n$ and $b_k \in \{0,1\}$ arbitrary for $k>n$ , is exactly $1/2^n$ . These are called cylinders. (They are the open sets, but also closed!)","I know the Cantor set probably comes up in homework questions all the time but I didn't find many clues - that I understood at least. I am, for a homework problem, supposed to show that the Cantor set is homeomorphic to the infinite product (I am assuming countably infinite?) of with itself. So members of this two-point space(?) are things like and , etc. Firstly, I think that a homeomorphism (the 'topological isomorhism') is a mapping between two topologies (for the Cantor sets which topology is this? discrete?) that have continuous, bijective functions. So I am pretty lost and don't even know what more to say! :( I have seen something like this in reading some texts, something about for . But in some ways this seems to be a 'complement' of what I need.... Apparently I am to use ternary numbers represented using only 's and 's in; for example, ? Thanks much for any help starting out! Here is the verbatim homework question: The standard measure on the Cantor set is given by the Cantor function which is constant on missing thirds and dyadic on ternary rationals. Show the Cantor set is homeomorphic to the infinite product of with itself. How should we topologize this product? (Hint: this product is the same as the set of all infinite binary sequences) Fix a binary -tuple (for e.g., if ). Show that the Cantor measure of points ( ) with for and arbitrary for , is exactly . These are called cylinders. (They are the open sets, but also closed!)","\{0,1\} (0,0,0,1) (0,1,1,1,1,1,1) f: \sum_{i=1}^{+\infty}\,\frac{a_i}{3^i} \mapsto \sum_{i=1}^{+\infty}\,\frac{a_i}{2^{i+1}} , a_i = 0,2 0 1 0.a_1\,a_2\,\ldots = 0.01011101 \phi \{0,1\} n (a_1,\ldots, a_n) (0,1,1,0,0,0) n = 6 b_k b_k=a_k k \leq n b_k \in \{0,1\} k>n 1/2^n","['real-analysis', 'general-topology', 'elementary-set-theory', 'cantor-set', 'product-space']"
8,Can we make $\tan(x)$ arbitrarily close to an integer when $x\in \mathbb{Z}$?,Can we make  arbitrarily close to an integer when ?,\tan(x) x\in \mathbb{Z},"My 7-year-old son was staring at the graph of tan() and its endlessly-repeating serpentine strokes on the number line between multiples of $\pi$ and he asked me the question in the title.  More precisely, is the following true or false? For any $\epsilon > 0$, there exists some $N \in \mathbb{Z}^+$ such that  $|\tan(N)-\lfloor \tan(N) \rceil| < \epsilon$.","My 7-year-old son was staring at the graph of tan() and its endlessly-repeating serpentine strokes on the number line between multiples of $\pi$ and he asked me the question in the title.  More precisely, is the following true or false? For any $\epsilon > 0$, there exists some $N \in \mathbb{Z}^+$ such that  $|\tan(N)-\lfloor \tan(N) \rceil| < \epsilon$.",,"['calculus', 'real-analysis', 'diophantine-approximation']"
9,"What has flipping a coin to do with $\int_0^{2 \pi}\sin^{2n}(x)\,dx$?",What has flipping a coin to do with ?,"\int_0^{2 \pi}\sin^{2n}(x)\,dx","If you toss a coin $2n$ times the chance of getting heads $n$ times is $$\frac{1}{4^n} \binom{2n}{n} $$ (simple combinatorics problem). Now I've made a seemingly obscure observation: $$\int_0^{2 \pi}\sin^{2n}(x)\,dx = \frac{2 \pi}{4^n} \binom{2n}{n} $$ The identities only differ by a factor of $2 \pi$. Is there an explanation for this phenomenon or just a cool coincidence? UPDATE: As Aloizio Macedo pointed out, a fairly related question has been answered with a combinatorial proof by Qiaochu Yuan here .","If you toss a coin $2n$ times the chance of getting heads $n$ times is $$\frac{1}{4^n} \binom{2n}{n} $$ (simple combinatorics problem). Now I've made a seemingly obscure observation: $$\int_0^{2 \pi}\sin^{2n}(x)\,dx = \frac{2 \pi}{4^n} \binom{2n}{n} $$ The identities only differ by a factor of $2 \pi$. Is there an explanation for this phenomenon or just a cool coincidence? UPDATE: As Aloizio Macedo pointed out, a fairly related question has been answered with a combinatorial proof by Qiaochu Yuan here .",,"['real-analysis', 'integration', 'combinatorics', 'analysis']"
10,Pointwise but not Uniformly Convergent,Pointwise but not Uniformly Convergent,,"The Question: Prove that the sequence of functions $f_n(x)=\frac{x^2+nx}{n}$ converges pointwise on $\mathbb{R}$, but does not converge uniformly on $\mathbb{R}$. My Work: Prove Pointwise: First, $\lim\limits_{n\to\infty} \frac{x^2+nx}{n}=\lim\limits_{n\to\infty} \frac{x^2}{n}+x=x$. My Problem: I am not sure where this fails to be uniformly convergent. Any help is appreciated.  Thanks","The Question: Prove that the sequence of functions $f_n(x)=\frac{x^2+nx}{n}$ converges pointwise on $\mathbb{R}$, but does not converge uniformly on $\mathbb{R}$. My Work: Prove Pointwise: First, $\lim\limits_{n\to\infty} \frac{x^2+nx}{n}=\lim\limits_{n\to\infty} \frac{x^2}{n}+x=x$. My Problem: I am not sure where this fails to be uniformly convergent. Any help is appreciated.  Thanks",,"['real-analysis', 'convergence-divergence', 'uniform-convergence']"
11,Convergence of an alternating series : $ \sum_{n\geq 1} \frac{(-1)^n|\sin n|}{n}$,Convergence of an alternating series :, \sum_{n\geq 1} \frac{(-1)^n|\sin n|}{n},"Study the convergence of $$\displaystyle \sum_{n\geq 1} \frac{(-1)^n|\sin n|}{n}.$$ I am stuck with this series, we need probably some measure of irrationally of $\pi$, unfortunately I am unfamiliar with this. So here is my attempt : Let $f(x) = \sum \frac{|\sin{n}|}{n} x^n, |x| < 1$ It's not difficult to compute the Fourier series of $|\sin(x)|$ : $$ \displaystyle|\sin(x)|=\frac{2}{\pi}-\frac{4}{\pi}\sum_{n=1}^{+\infty}\frac{\cos(2nx)}{4n^2-1} $$ Then Fubini's theorem ( Series Version ) works very well (because the previous series converges absolutely at $x$ fixed ) and all calculations made, we find that for all $x\in( -1,1)$: $$ \displaystyle f(x)=\frac{2}{\pi}\sum_{n=1}^{+\infty}\frac{x^n}{n}-\frac{4}{\pi}\sum_{p=1}^{+\infty}\frac{x^2-2x\cos(p)}{(4p^2-1)(x^2-2x\cos(p)+1)} $$ However, the second sum I have not been able to show the convergence. I feel the series diverge because the following series $$ \displaystyle\sum\frac{1}{p^2\sin^2\left(\frac{p}{2}\right)} $$ diverge because  $0$ is an accumulation point of $\displaystyle (n\sin(n))$ sequence. Any ideas (for the original series) ?","Study the convergence of $$\displaystyle \sum_{n\geq 1} \frac{(-1)^n|\sin n|}{n}.$$ I am stuck with this series, we need probably some measure of irrationally of $\pi$, unfortunately I am unfamiliar with this. So here is my attempt : Let $f(x) = \sum \frac{|\sin{n}|}{n} x^n, |x| < 1$ It's not difficult to compute the Fourier series of $|\sin(x)|$ : $$ \displaystyle|\sin(x)|=\frac{2}{\pi}-\frac{4}{\pi}\sum_{n=1}^{+\infty}\frac{\cos(2nx)}{4n^2-1} $$ Then Fubini's theorem ( Series Version ) works very well (because the previous series converges absolutely at $x$ fixed ) and all calculations made, we find that for all $x\in( -1,1)$: $$ \displaystyle f(x)=\frac{2}{\pi}\sum_{n=1}^{+\infty}\frac{x^n}{n}-\frac{4}{\pi}\sum_{p=1}^{+\infty}\frac{x^2-2x\cos(p)}{(4p^2-1)(x^2-2x\cos(p)+1)} $$ However, the second sum I have not been able to show the convergence. I feel the series diverge because the following series $$ \displaystyle\sum\frac{1}{p^2\sin^2\left(\frac{p}{2}\right)} $$ diverge because  $0$ is an accumulation point of $\displaystyle (n\sin(n))$ sequence. Any ideas (for the original series) ?",,['real-analysis']
12,The series $\sum\limits_{n=1}^\infty \frac n{\frac1{a_1}+\frac1{a_2}+\dotsb+\frac1{a_n}}$ is convergent,The series  is convergent,\sum\limits_{n=1}^\infty \frac n{\frac1{a_1}+\frac1{a_2}+\dotsb+\frac1{a_n}},"If a series $\sum\limits_{n=1}^\infty a_n$ is convergent, and $a_n\gt0$... Do not refer to Carleman's inequality or Hardy's inequality , show that the series $$\sum_{n=1}^\infty \frac n{\frac1{a_1}+\frac1{a_2}+\dotsb+\frac1{a_n}} $$ is  also convergent. What is the minimum positive real number $k$ such that the following inequality holds for all convergent series $a_n\gt0$? $$\sum_{n=1}^\infty \frac n{\frac1{a_1}+\frac1{a_2}+\dotsb+\frac1{a_n}}\le k\sum_{n=1}^\infty a_n$$ Does it exist a positive real number $l$ such that $$\sum_{n=1}^\infty a_n \le l\sum_{n=1}^\infty \frac n{\frac1{a_1}+\frac1{a_2}+\dotsb+\frac1{a_n}}$$ holds?","If a series $\sum\limits_{n=1}^\infty a_n$ is convergent, and $a_n\gt0$... Do not refer to Carleman's inequality or Hardy's inequality , show that the series $$\sum_{n=1}^\infty \frac n{\frac1{a_1}+\frac1{a_2}+\dotsb+\frac1{a_n}} $$ is  also convergent. What is the minimum positive real number $k$ such that the following inequality holds for all convergent series $a_n\gt0$? $$\sum_{n=1}^\infty \frac n{\frac1{a_1}+\frac1{a_2}+\dotsb+\frac1{a_n}}\le k\sum_{n=1}^\infty a_n$$ Does it exist a positive real number $l$ such that $$\sum_{n=1}^\infty a_n \le l\sum_{n=1}^\infty \frac n{\frac1{a_1}+\frac1{a_2}+\dotsb+\frac1{a_n}}$$ holds?",,"['calculus', 'real-analysis', 'sequences-and-series', 'analysis', 'inequality']"
13,Sup Norm and Uniform Convergence,Sup Norm and Uniform Convergence,,"My book says Convergence in sup norm $||f_n-f||\to 0$ is equivalent to uniform convergence and this follows immediately from definitions. but I just want to check: $\Rightarrow$ If lim$_{n\to\infty}||f_n-f||=0$, then sup$\{|f_n(x)-f(x)|:x\in[a,b]\}\to 0\Rightarrow |f_n(x)-f(x)|\to 0 \forall x\in[a,b]\Rightarrow (f_n)\to f$ uniformly. And then running in reverse: $\Leftarrow$ If $f_n\to f$ uniformly, then $|f_n(x)-f(x)|\to 0 \forall x\in[a,b]\Rightarrow$sup$\{|f_n(x)-f(x)|:x\in[a,b]\}\to 0\Rightarrow ||f_n-f||\Rightarrow 0$. My question is, why $|f_n(x)-f(x)|\to 0 \forall x\in[a,b]\Rightarrow$sup$\{|f_n(x)-f(x)|:x\in[a,b]\}\to 0$. I think it's because sup is really max here because functions are continuous and $[a,b]$ is compact. Is this right? Does it hold in general if the functions aren't continuous?","My book says Convergence in sup norm $||f_n-f||\to 0$ is equivalent to uniform convergence and this follows immediately from definitions. but I just want to check: $\Rightarrow$ If lim$_{n\to\infty}||f_n-f||=0$, then sup$\{|f_n(x)-f(x)|:x\in[a,b]\}\to 0\Rightarrow |f_n(x)-f(x)|\to 0 \forall x\in[a,b]\Rightarrow (f_n)\to f$ uniformly. And then running in reverse: $\Leftarrow$ If $f_n\to f$ uniformly, then $|f_n(x)-f(x)|\to 0 \forall x\in[a,b]\Rightarrow$sup$\{|f_n(x)-f(x)|:x\in[a,b]\}\to 0\Rightarrow ||f_n-f||\Rightarrow 0$. My question is, why $|f_n(x)-f(x)|\to 0 \forall x\in[a,b]\Rightarrow$sup$\{|f_n(x)-f(x)|:x\in[a,b]\}\to 0$. I think it's because sup is really max here because functions are continuous and $[a,b]$ is compact. Is this right? Does it hold in general if the functions aren't continuous?",,"['real-analysis', 'metric-spaces', 'normed-spaces', 'uniform-convergence']"
14,Infinitely differentiable functions: how to prove that $e^\frac{1}{x^2-1}$ has derivative of any order?,Infinitely differentiable functions: how to prove that  has derivative of any order?,e^\frac{1}{x^2-1},"Let $f:\Bbb{R}\to\Bbb{R}$ be a function given by $$f(x)=\begin{cases} \exp\left(\frac{1}{x^2-1}\right) & \text{if }\vert x\vert\lt 1\\  0 & \text{if }\vert x\vert\geqslant 1  \end{cases}$$ I would like to prove that $f\in C^\infty$ , that is, $f\in C^k$ for all $k\in \mathbb{N}$ . I think that it can be done by induction on $k$ . If $\vert x\vert\gt1$ , the problem is trivial. On other points, the base case is the simplest and the only that I'm be able to do. Can someone help me? Thanks.","Let be a function given by I would like to prove that , that is, for all . I think that it can be done by induction on . If , the problem is trivial. On other points, the base case is the simplest and the only that I'm be able to do. Can someone help me? Thanks.","f:\Bbb{R}\to\Bbb{R} f(x)=\begin{cases}
\exp\left(\frac{1}{x^2-1}\right) & \text{if }\vert x\vert\lt 1\\ 
0 & \text{if }\vert x\vert\geqslant 1 
\end{cases} f\in C^\infty f\in C^k k\in \mathbb{N} k \vert x\vert\gt1","['real-analysis', 'derivatives', 'induction']"
15,What is the derivative of the expected value of a continuous random variable?,What is the derivative of the expected value of a continuous random variable?,,"Say we have a random variable $X$ , with density function $f(x)$ , and moment generating function $M(t) = E[e^{tX}]$ , and we take the derivative of $M(t)$ $$\frac{d}{dt} M(t) = \frac{d}{dt}E\left[e^{tX} \right] = E\left[\frac{d}{dt}e^{tX} \right]$$ I can see how this works in the discrete case, as we are bringing the derivative inside a summation and the derivative of sums is the sum of the derivatives. But why are we allowed to bring the derivative inside the integral in the continuous case? $$\frac{d}{dt}\int_X e^{tX}f(x)dx = \int_X \frac{d}{dt}[e^{tX}f(x)]dx$$ ?","Say we have a random variable , with density function , and moment generating function , and we take the derivative of I can see how this works in the discrete case, as we are bringing the derivative inside a summation and the derivative of sums is the sum of the derivatives. But why are we allowed to bring the derivative inside the integral in the continuous case? ?",X f(x) M(t) = E[e^{tX}] M(t) \frac{d}{dt} M(t) = \frac{d}{dt}E\left[e^{tX} \right] = E\left[\frac{d}{dt}e^{tX} \right] \frac{d}{dt}\int_X e^{tX}f(x)dx = \int_X \frac{d}{dt}[e^{tX}f(x)]dx,"['real-analysis', 'probability', 'probability-theory']"
16,Show that an orthogonal group is a $ \frac{n(n−1)}2 $-dim. $ C^{\infty} $-Manifold and find its tangent space,Show that an orthogonal group is a -dim. -Manifold and find its tangent space, \frac{n(n−1)}2   C^{\infty} ,The orthogonal group is defined as (with group structure inherited from $n\times n$ matrices) $$O(n) := \{X\in \mathbb{R}^{n\times n} : X^\text{t}X=I_n\}.$$ (i) Show that $O(n)$ is an $\frac{n(n-1)}{2}$-dimensional $C^\infty$-manifold in the space of $n\times n$ matrices. Hints. Exhibit $O(n)$ as the preimage of $0$ of the function $\phi$ from $n\times n$ matrices to the symmetric $n\times n$ matrices given by $X\mapsto X^\text{t}X − I_n$. (Note that the target space of $\phi$ is very important in order to satisfy the maximal rank condition.) Then show that the equation $\phi'(A)H=S$ has a solution $H$ for each $A\in O(n)$ and each symmetric $n\times n$ matrix $S$. You will also need to compute the dimension of the space of symmetric $n\times n$ matrices. (ii) Show that the tangent space $T_{I_n}O(n)$ at the identity is the space of antisymmetric matrices.,The orthogonal group is defined as (with group structure inherited from $n\times n$ matrices) $$O(n) := \{X\in \mathbb{R}^{n\times n} : X^\text{t}X=I_n\}.$$ (i) Show that $O(n)$ is an $\frac{n(n-1)}{2}$-dimensional $C^\infty$-manifold in the space of $n\times n$ matrices. Hints. Exhibit $O(n)$ as the preimage of $0$ of the function $\phi$ from $n\times n$ matrices to the symmetric $n\times n$ matrices given by $X\mapsto X^\text{t}X − I_n$. (Note that the target space of $\phi$ is very important in order to satisfy the maximal rank condition.) Then show that the equation $\phi'(A)H=S$ has a solution $H$ for each $A\in O(n)$ and each symmetric $n\times n$ matrix $S$. You will also need to compute the dimension of the space of symmetric $n\times n$ matrices. (ii) Show that the tangent space $T_{I_n}O(n)$ at the identity is the space of antisymmetric matrices.,,"['real-analysis', 'lie-groups']"
17,"Cantor set + Cantor set =$[0,2]$",Cantor set + Cantor set =,"[0,2]","I am trying to prove that $C+C =[0,2]$ ,where $C$ is the Cantor set. My attempt: If $x\in C,$  then  $x= \sum_{n=1}^{\infty}\frac{a_n}{3^n}$ where $a_n=0,2$ so any element of $C+C  $ is of the form  $$\sum_{n=1}^{\infty}\frac{a_n}{3^n} +\sum_{n=1}^{\infty}\frac{b_n}{3^n}= \sum_{n=1}^{\infty}\frac{a_n+b_n}{3^n}=2\sum_{n=1}^{\infty}\frac{(a_n+b_n)/2}{3^n}=2\sum_{n=1}^{\infty}\frac{x_n}{3^n}$$ where $x_n=0,1,2, \ \forall n\geq 1$. Is this correct?","I am trying to prove that $C+C =[0,2]$ ,where $C$ is the Cantor set. My attempt: If $x\in C,$  then  $x= \sum_{n=1}^{\infty}\frac{a_n}{3^n}$ where $a_n=0,2$ so any element of $C+C  $ is of the form  $$\sum_{n=1}^{\infty}\frac{a_n}{3^n} +\sum_{n=1}^{\infty}\frac{b_n}{3^n}= \sum_{n=1}^{\infty}\frac{a_n+b_n}{3^n}=2\sum_{n=1}^{\infty}\frac{(a_n+b_n)/2}{3^n}=2\sum_{n=1}^{\infty}\frac{x_n}{3^n}$$ where $x_n=0,1,2, \ \forall n\geq 1$. Is this correct?",,"['real-analysis', 'general-topology', 'analysis', 'cantor-set']"
18,Omitting the hypotheses of finiteness of the measure in Egorov theorem,Omitting the hypotheses of finiteness of the measure in Egorov theorem,,"I want to prove that if I omit the fact that $\mu (X) < \infty$ in Egorov theorem and place instead that our functions $|f_n| <g$ and $g$ is integrable, we still get the result of Egorov's theorem. Fix $m$ a natural number. I took $ w_{n} = |f_n-f|$ and thus by DCT $\int |f_{n} - f|$ goes to zero. Then I took $\bigcup_n {( w_{n} \geq 1/m)}$. I need its measure to be finite. Its measure is less than the sum of the measures of each $ w_n\geq 1/m)$ varying $n$, and by Tchebychev, this is less than $m\int|f_{n} - f|$.  But I got stuck here. Any help is appreciated. Thanks!","I want to prove that if I omit the fact that $\mu (X) < \infty$ in Egorov theorem and place instead that our functions $|f_n| <g$ and $g$ is integrable, we still get the result of Egorov's theorem. Fix $m$ a natural number. I took $ w_{n} = |f_n-f|$ and thus by DCT $\int |f_{n} - f|$ goes to zero. Then I took $\bigcup_n {( w_{n} \geq 1/m)}$. I need its measure to be finite. Its measure is less than the sum of the measures of each $ w_n\geq 1/m)$ varying $n$, and by Tchebychev, this is less than $m\int|f_{n} - f|$.  But I got stuck here. Any help is appreciated. Thanks!",,"['real-analysis', 'measure-theory', 'uniform-convergence']"
19,Is there always an equivalent metric which is not complete?,Is there always an equivalent metric which is not complete?,,"I have seen that completeness is not a topological property like compactness or connectedness. I have seen some examples also showing that there are two equivalent metrics one of which is complete and the other one is incomplete. I want to know some general result. Consider any metric space $(X,d)$. Let $d$ be complete. Does there exist an equivalent metric $d'$ in $X$ which is incomplete? When $d$ is compact, such $d'$ does not exist (Since compactness implies completeness). So answer me when $d$ is noncompact.","I have seen that completeness is not a topological property like compactness or connectedness. I have seen some examples also showing that there are two equivalent metrics one of which is complete and the other one is incomplete. I want to know some general result. Consider any metric space $(X,d)$. Let $d$ be complete. Does there exist an equivalent metric $d'$ in $X$ which is incomplete? When $d$ is compact, such $d'$ does not exist (Since compactness implies completeness). So answer me when $d$ is noncompact.",,"['real-analysis', 'general-topology', 'analysis', 'metric-spaces']"
20,Real number $x$ such that $\{ x^n\}$ is constant for all $n\in S$,Real number  such that  is constant for all,x \{ x^n\} n\in S,"The golden ratio satisfies the property that $$\{\phi^{-1}\}=\{\phi\}=\{\phi^2\} = 0.618\cdots$$ where $\{x\}$ is the fractional part of $x$ , equal to $x-\lfloor x\rfloor$ . Inspired by that, I was wondering for what subsets $S$ of $\mathbb{Z}\setminus\{0\}$ (e.g. $S=\{-1,1,2\}$ as with the golden ratio), there exist $x\in\mathbb{R}$ such that for all $n\in S$ , $\{x^n\}$ is equal (and not equal to $0$ , because otherwise, there would just be trivial integer/integer root solutions). If $|S|=2$ , then I think there must exist a solution. Write $S=\{m,n\}$ . If both elements are positive, on $[0,2^{mn})$ , $\{x^m\}$ and $\{x^n\}$ have differing number of discontinuities (I think $2^n-1$ and $2^m-1$ ), but they're both increasing from $0$ to $1$ except at those discontinuities. So there must exist some intersection point. If both elements are negative, instead of a solution to $\{x^m\}=\{x^n\}$ , you can just consider the solution to $\{x^{-m}\}=\{x^{-n}\}$ and just take the inverse of that. Finally, if one element's positive and one's negative, the graph for the negative one would be monotonically decreasing to $0$ after $x=1$ , while the graph for the positive one would be increasing (except at those discontinuities) from $0$ to $1$ , so there would be some intersection in the graphs. Obviously, what's a lot more tricky is when $|S|\ge 3$ . I'm not even sure if there's any solution with $|S|=3$ other than when $S=\{-1k,1k,2k\}$ with $k\in\mathbb{Z}$ . I did get that if $S=\{1,2\}$ , the set of solutions for $x$ is given by $$\left\{-\sqrt{m+\frac{3-\sqrt{5+4m}}{2}}:m\in\mathbb{Z}_{\ge 0}\right\}\bigcup\left\{\sqrt{m+\frac{1+\sqrt{1+4m}}{2}}:m\in\mathbb{Z}_{\ge 0}\right\}\bigcup\{0\}$$ Also, if $S$ works, then $kS=\{ks:s\in S\}$ works, where $k$ is an integer. Is there a simple way to characterize the sets $S$ that work? More specifically, is there any way to find out what sets with only three elements work? Edit: This is a small comment but might motivate looking at it through the lens of algebra. If $\{x^a\}=\{x^b\}=\{x^c\}$ with $a>b>c\ge 1$ , then there should exist an integer $m$ such that $x^a-x^b-m$ is reducible over $\mathbb{Z}[x]$ . In fact, we would need integers $m,n$ such that $\deg(\gcd(x^a-x^b-m,x^b-x^c-n))\ge 1$ .","The golden ratio satisfies the property that where is the fractional part of , equal to . Inspired by that, I was wondering for what subsets of (e.g. as with the golden ratio), there exist such that for all , is equal (and not equal to , because otherwise, there would just be trivial integer/integer root solutions). If , then I think there must exist a solution. Write . If both elements are positive, on , and have differing number of discontinuities (I think and ), but they're both increasing from to except at those discontinuities. So there must exist some intersection point. If both elements are negative, instead of a solution to , you can just consider the solution to and just take the inverse of that. Finally, if one element's positive and one's negative, the graph for the negative one would be monotonically decreasing to after , while the graph for the positive one would be increasing (except at those discontinuities) from to , so there would be some intersection in the graphs. Obviously, what's a lot more tricky is when . I'm not even sure if there's any solution with other than when with . I did get that if , the set of solutions for is given by Also, if works, then works, where is an integer. Is there a simple way to characterize the sets that work? More specifically, is there any way to find out what sets with only three elements work? Edit: This is a small comment but might motivate looking at it through the lens of algebra. If with , then there should exist an integer such that is reducible over . In fact, we would need integers such that .","\{\phi^{-1}\}=\{\phi\}=\{\phi^2\} = 0.618\cdots \{x\} x x-\lfloor x\rfloor S \mathbb{Z}\setminus\{0\} S=\{-1,1,2\} x\in\mathbb{R} n\in S \{x^n\} 0 |S|=2 S=\{m,n\} [0,2^{mn}) \{x^m\} \{x^n\} 2^n-1 2^m-1 0 1 \{x^m\}=\{x^n\} \{x^{-m}\}=\{x^{-n}\} 0 x=1 0 1 |S|\ge 3 |S|=3 S=\{-1k,1k,2k\} k\in\mathbb{Z} S=\{1,2\} x \left\{-\sqrt{m+\frac{3-\sqrt{5+4m}}{2}}:m\in\mathbb{Z}_{\ge 0}\right\}\bigcup\left\{\sqrt{m+\frac{1+\sqrt{1+4m}}{2}}:m\in\mathbb{Z}_{\ge 0}\right\}\bigcup\{0\} S kS=\{ks:s\in S\} k S \{x^a\}=\{x^b\}=\{x^c\} a>b>c\ge 1 m x^a-x^b-m \mathbb{Z}[x] m,n \deg(\gcd(x^a-x^b-m,x^b-x^c-n))\ge 1","['real-analysis', 'fractional-part']"
21,Delightful integral: $\int _0^{\frac{\pi }{2}}x\ln \left(\sin \left(x\right)\right)\ln ^2\left(\cos \left(x\right)\right)\:dx.$,Delightful integral:,\int _0^{\frac{\pi }{2}}x\ln \left(\sin \left(x\right)\right)\ln ^2\left(\cos \left(x\right)\right)\:dx.,"A friend of mine proposed me this integral which I find to be very interesting. I managed to find with the help of software that: $$\int _0^{\frac{\pi }{2}}x\ln \left(\sin \left(x\right)\right)\ln ^2\left(\cos \left(x\right)\right)\:dx=\frac{155}{128}\zeta \left(5\right)+\frac{13}{32}\zeta \left(2\right)\zeta \left(3\right)-\operatorname{Li}_5\left(\frac{1}{2}\right)$$ $$-\frac{49}{32}\ln \left(2\right)\zeta \left(4\right)-\frac{5}{6}\ln ^3\left(2\right)\zeta \left(2\right)+\frac{1}{120}\ln ^5\left(2\right).$$ Where $\zeta \left(z\right)$ denotes the Riemann zeta function and $\operatorname{Li}_n\left(z\right)$ denotes the Polylogarithm function. If $x=\tan\left(t\right)$ is used it yields: $$\frac{1}{4}\int _0^{\infty }\frac{\arctan \left(x\right)\ln \left(x\right)\ln ^2\left(1+x^2\right)}{1+x^2}\:dx-\frac{1}{8}\int _0^{\infty }\frac{\arctan \left(x\right)\ln ^3\left(1+x^2\right)}{1+x^2}\:dx.$$ I know of ways to evaluate the latter integral but the former is very difficult and the techniques that work for the $2$ nd do not work for the $1$ st. Integrating by parts gives: $$-\frac{1}{2}\int _0^{\frac{\pi }{2}}x^2\cot \left(x\right)\ln ^2\left(\cos \left(x\right)\right)\:dx-\int _0^{\frac{\pi }{2}}x^2\tan \left(x\right)\ln \left(\sin \left(x\right)\right)\ln \left(\cos \left(x\right)\right)\:dx.$$ And I find myself in the same situation where I can evaluate the $1$ st integral but the techniques that work for it aren't as effective for the $2$ nd, are there any better approaches for the main integral? Please do not post results without proving them since that is not what I'm looking for, thanks.","A friend of mine proposed me this integral which I find to be very interesting. I managed to find with the help of software that: Where denotes the Riemann zeta function and denotes the Polylogarithm function. If is used it yields: I know of ways to evaluate the latter integral but the former is very difficult and the techniques that work for the nd do not work for the st. Integrating by parts gives: And I find myself in the same situation where I can evaluate the st integral but the techniques that work for it aren't as effective for the nd, are there any better approaches for the main integral? Please do not post results without proving them since that is not what I'm looking for, thanks.",\int _0^{\frac{\pi }{2}}x\ln \left(\sin \left(x\right)\right)\ln ^2\left(\cos \left(x\right)\right)\:dx=\frac{155}{128}\zeta \left(5\right)+\frac{13}{32}\zeta \left(2\right)\zeta \left(3\right)-\operatorname{Li}_5\left(\frac{1}{2}\right) -\frac{49}{32}\ln \left(2\right)\zeta \left(4\right)-\frac{5}{6}\ln ^3\left(2\right)\zeta \left(2\right)+\frac{1}{120}\ln ^5\left(2\right). \zeta \left(z\right) \operatorname{Li}_n\left(z\right) x=\tan\left(t\right) \frac{1}{4}\int _0^{\infty }\frac{\arctan \left(x\right)\ln \left(x\right)\ln ^2\left(1+x^2\right)}{1+x^2}\:dx-\frac{1}{8}\int _0^{\infty }\frac{\arctan \left(x\right)\ln ^3\left(1+x^2\right)}{1+x^2}\:dx. 2 1 -\frac{1}{2}\int _0^{\frac{\pi }{2}}x^2\cot \left(x\right)\ln ^2\left(\cos \left(x\right)\right)\:dx-\int _0^{\frac{\pi }{2}}x^2\tan \left(x\right)\ln \left(\sin \left(x\right)\right)\ln \left(\cos \left(x\right)\right)\:dx. 1 2,"['real-analysis', 'calculus', 'integration', 'complex-analysis', 'definite-integrals']"
22,Challenging problem: Calculate $\int_0^{2\pi}x^2 \cos(x)\operatorname{Li}_2(\cos(x))dx$,Challenging problem: Calculate,\int_0^{2\pi}x^2 \cos(x)\operatorname{Li}_2(\cos(x))dx,"The following problem is proposed by a friend: $$\int_0^{2\pi}x^2 \cos(x)\operatorname{Li}_2(\cos(x))dx$$ $$=\frac{9}{8}\pi^4-2\pi^3-2\pi^2-8\ln(2)\pi-\frac12\ln^2(2)\pi^2+8\ln(2)\pi G+16\pi\Im\left\{\operatorname{Li}_3\left(\frac{1+i}{2}\right)\right\}$$ My only try is writing $$\operatorname{Li}_2(\cos(x))=-\int_0^1\frac{\cos(x)\ln(y)}{1-\cos(x)y} \, dy$$ and have no idea how to continue with the double integral. I also tried $\cos(x)=u$ , didn't do much. Any help would be much appreciated.","The following problem is proposed by a friend: My only try is writing and have no idea how to continue with the double integral. I also tried , didn't do much. Any help would be much appreciated.","\int_0^{2\pi}x^2 \cos(x)\operatorname{Li}_2(\cos(x))dx =\frac{9}{8}\pi^4-2\pi^3-2\pi^2-8\ln(2)\pi-\frac12\ln^2(2)\pi^2+8\ln(2)\pi G+16\pi\Im\left\{\operatorname{Li}_3\left(\frac{1+i}{2}\right)\right\} \operatorname{Li}_2(\cos(x))=-\int_0^1\frac{\cos(x)\ln(y)}{1-\cos(x)y} \, dy \cos(x)=u","['real-analysis', 'calculus', 'integration', 'definite-integrals', 'polylogarithm']"
23,Show that $\mathbb{Q}$ is dense in the real numbers. (Using Supremum),Show that  is dense in the real numbers. (Using Supremum),\mathbb{Q},"I am stuck on a homework the teacher gave us to hand in. It is stated as following: The set of rational numbers $\mathbb{Q}$ given by all $q = \frac{m}{n}$ for some $m, n \in \mathbb{Z}$ with $n \neq 0$ is dense in the real numbers in the following sense:   For each real $\epsilon > 0$ and $x \in \mathbb{R} \exists q_{\epsilon}: |x- q_{\epsilon}| < \epsilon$ With the problem, I am given a hint: Prove the statement first for $x > 0$ Let $\mathbb{Q}_{x}$ be the set of rational numbers, such that $a \in \mathbb{Q}_{x} \Rightarrow a \leq x$. Why is $\mathbb{Q}_{x}$ nonempty? Apply the supremum property to $\mathbb{Q}_{x}$ and prove that the supremum of $\mathbb{Q}_{x}$ is $x$ Here my attempts: Suppose $x > 0$. Let $\mathbb{Q}_{x}$ the set of $q \in \mathbb{Q}$ s.th. $q \leq x$. (i) Show that $\mathbb{Q}_{x}$ is nonempty: $ q \leq x \Rightarrow \frac{m}{n} \leq x \Rightarrow n \geq \frac{m}{x}$. Then by the Archimedian property and the fact that $\mathbb{N}$ is a subset of $\mathbb{Z}$ there exists an $n$ for which this is the case, thus $\mathbb{Q}_{x}$ is nonempty. (ii) Supremum By defintion, $\mathbb{Q}_{x}$ is a bounded set. Thus, by the supremum property, $\mathbb{Q}_{x}$ has a supremum. (iii) Show that $x = sup(\mathbb{Q}_{x})$ (I am not too certain how to show this, but I think I will get it in time, however, hints are appreciated) But from then on I am stuck. For any x > 0, I can show that there is a set of rational numbers for which x is the supremum. Thus, I can find a rational number arbitratly close to x, making $x-q < \epsilon$? And what about $x\neq0$? How does my result help me there? Thanks for the time and advice! First, thanks for your help. However, I am still in need of some more. I find myself unable to show that if $\mathbb{Q}_{x}$ is bounded by $x$, $x$ is indeed the supremum of that set. If I suppose that there is a $y < x$ and that that $y$ is an upper bound (in order to prove by contradiction), how can I find a $q \in \mathbb{Q}_{x}$ which is larger than $y$ without using that between any two real numbers, there lies a rational number, which is what I am supposed to show in the first place?","I am stuck on a homework the teacher gave us to hand in. It is stated as following: The set of rational numbers $\mathbb{Q}$ given by all $q = \frac{m}{n}$ for some $m, n \in \mathbb{Z}$ with $n \neq 0$ is dense in the real numbers in the following sense:   For each real $\epsilon > 0$ and $x \in \mathbb{R} \exists q_{\epsilon}: |x- q_{\epsilon}| < \epsilon$ With the problem, I am given a hint: Prove the statement first for $x > 0$ Let $\mathbb{Q}_{x}$ be the set of rational numbers, such that $a \in \mathbb{Q}_{x} \Rightarrow a \leq x$. Why is $\mathbb{Q}_{x}$ nonempty? Apply the supremum property to $\mathbb{Q}_{x}$ and prove that the supremum of $\mathbb{Q}_{x}$ is $x$ Here my attempts: Suppose $x > 0$. Let $\mathbb{Q}_{x}$ the set of $q \in \mathbb{Q}$ s.th. $q \leq x$. (i) Show that $\mathbb{Q}_{x}$ is nonempty: $ q \leq x \Rightarrow \frac{m}{n} \leq x \Rightarrow n \geq \frac{m}{x}$. Then by the Archimedian property and the fact that $\mathbb{N}$ is a subset of $\mathbb{Z}$ there exists an $n$ for which this is the case, thus $\mathbb{Q}_{x}$ is nonempty. (ii) Supremum By defintion, $\mathbb{Q}_{x}$ is a bounded set. Thus, by the supremum property, $\mathbb{Q}_{x}$ has a supremum. (iii) Show that $x = sup(\mathbb{Q}_{x})$ (I am not too certain how to show this, but I think I will get it in time, however, hints are appreciated) But from then on I am stuck. For any x > 0, I can show that there is a set of rational numbers for which x is the supremum. Thus, I can find a rational number arbitratly close to x, making $x-q < \epsilon$? And what about $x\neq0$? How does my result help me there? Thanks for the time and advice! First, thanks for your help. However, I am still in need of some more. I find myself unable to show that if $\mathbb{Q}_{x}$ is bounded by $x$, $x$ is indeed the supremum of that set. If I suppose that there is a $y < x$ and that that $y$ is an upper bound (in order to prove by contradiction), how can I find a $q \in \mathbb{Q}_{x}$ which is larger than $y$ without using that between any two real numbers, there lies a rational number, which is what I am supposed to show in the first place?",,['real-analysis']
24,Find $f$ where $f'(x) = f(1+x)$,Find  where,f f'(x) = f(1+x),"Let $f \colon \mathbb{R} \rightarrow  \mathbb{R}$ be a smooth function such that $$f'(x) = f(1+x)$$ How can we find the general form of $f$? I thought of some differential equations, but not sure how to use them here. Thanks.","Let $f \colon \mathbb{R} \rightarrow  \mathbb{R}$ be a smooth function such that $$f'(x) = f(1+x)$$ How can we find the general form of $f$? I thought of some differential equations, but not sure how to use them here. Thanks.",,"['calculus', 'real-analysis', 'ordinary-differential-equations', 'delay-differential-equations']"
25,Second derivative positive $\implies$ convex,Second derivative positive  convex,\implies,"In proof of the following theorem; If $f$ has a second derivative that is non-negative (positive) over an interval then $f$ is convex (strictly convex). $f$ is in real number space., the book I refer, uses Taylor series expansion but disregards terms of order 3 and above. So I'm not convinced of the correctness of the proof, which I paste below. Is there a way to bound the terms of order 3 and above in the follow proof? I think bounding the error of higher order terms is important in many cases. So would really appreciate a clear answer. Thanks a lot.","In proof of the following theorem; If $f$ has a second derivative that is non-negative (positive) over an interval then $f$ is convex (strictly convex). $f$ is in real number space., the book I refer, uses Taylor series expansion but disregards terms of order 3 and above. So I'm not convinced of the correctness of the proof, which I paste below. Is there a way to bound the terms of order 3 and above in the follow proof? I think bounding the error of higher order terms is important in many cases. So would really appreciate a clear answer. Thanks a lot.",,"['real-analysis', 'analysis', 'convex-analysis', 'convex-optimization']"
26,Existence of an antiderivative function on an arbitrary subset of $\mathbb{R}$,Existence of an antiderivative function on an arbitrary subset of,\mathbb{R},"Let $f:\mathbb{R}\to \mathbb{R}$ be  continuous at $x$ for every $x\in I$  where $I\subset \mathbb R$ could be arbitrary. Does there always exist a function $F:\mathbb{R}\to \mathbb{R}$ differentiable on $I$ and $F'(x) = f(x)$ for every $x \in I$? The definition of a primitive is naturally defined on an interval. A mathematical curiosity is to understand the difficulties that can be encountered when trying to define this notion on any part of $\mathbb{R}$. A first difficulty is to try to find a good definition of the notion of a primitive on any part of $\mathbb{R}$. That was the purpose of this thread "" Correct definition of antiderivative function ."" If we ask $F$ to be differentiable on an open set $J$ containing $I$, the thread "" Existence of an antiderivative for a continuous function on an arbitrary subset of $\mathbb{R}$ "" gives a counterexample to the question. If $I$ is an interval, the answer to the question is positive.  If $I$ is an open set, the answer to the question is also positive. (see comment)","Let $f:\mathbb{R}\to \mathbb{R}$ be  continuous at $x$ for every $x\in I$  where $I\subset \mathbb R$ could be arbitrary. Does there always exist a function $F:\mathbb{R}\to \mathbb{R}$ differentiable on $I$ and $F'(x) = f(x)$ for every $x \in I$? The definition of a primitive is naturally defined on an interval. A mathematical curiosity is to understand the difficulties that can be encountered when trying to define this notion on any part of $\mathbb{R}$. A first difficulty is to try to find a good definition of the notion of a primitive on any part of $\mathbb{R}$. That was the purpose of this thread "" Correct definition of antiderivative function ."" If we ask $F$ to be differentiable on an open set $J$ containing $I$, the thread "" Existence of an antiderivative for a continuous function on an arbitrary subset of $\mathbb{R}$ "" gives a counterexample to the question. If $I$ is an interval, the answer to the question is positive.  If $I$ is an open set, the answer to the question is also positive. (see comment)",,"['real-analysis', 'integration']"
27,A continuous bijection $f:\mathbb{R}\to \mathbb{R}$ is an homeomorphism?,A continuous bijection  is an homeomorphism?,f:\mathbb{R}\to \mathbb{R},"A continuous bijection $f:\mathbb{R}\to \mathbb{R}$ is an homeomorphism. With the usual metric structure. I always heard that this fact is true, but anyone shows to me a proof, and I can't prove it. I was tried using the definition of continuity but it is impossible to me conclude that. I tried using the fact that $f$ bijective has an inverse $f^{-1}$ and the fact that $ff^{-1}$ and $f^{-1}f$ are continuous identity, but I can't follow.","A continuous bijection $f:\mathbb{R}\to \mathbb{R}$ is an homeomorphism. With the usual metric structure. I always heard that this fact is true, but anyone shows to me a proof, and I can't prove it. I was tried using the definition of continuity but it is impossible to me conclude that. I tried using the fact that $f$ bijective has an inverse $f^{-1}$ and the fact that $ff^{-1}$ and $f^{-1}f$ are continuous identity, but I can't follow.",,"['real-analysis', 'general-topology', 'continuity', 'inverse']"
28,Bibliography for Singular Functions [closed],Bibliography for Singular Functions [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question does not appear to be about math within the scope defined in the help center . Closed 10 years ago . Improve this question I wound up assembling a rather lengthy and partially annotated bibliography for my answer to the math StackExchange question Singular continuous functions , but it seems I got a little too carried away and found myself with a post well over the maximum of 30,000 characters. Thus, I'm using this ""question"" to post the full bibliography, which I think could be a useful reference to others. I am including a large portion of the bibliography in the question because I still have too many characters otherwise. SOME HISTORY It is easy to construct non-decreasing continuous functions with a zero derivative almost everywhere, and such functions have numerous appearances in the literature beginning around 1883/1884 in papers by Cantor and Scheeffer (separately authored). See Fleron (1994) [27] and Maurey/Tacchi (2002) [54] . On the other hand, the first proof of the existence of strictly increasing continuous functions with a zero derivative almost everywhere seems to be in Ernst Hellinger's 1907 Ph.D. Dissertation, but I haven't looked into this to see exactly what he proved and where in the Dissertation it can be found. (Minkowski's function came earlier, but it wasn't proved to have this property until Denjoy proved it in 1934/1938 [17] [18] .) Likely unaware of Hellinger's result, Denjoy gave such a function in 1915 [15] . Other early examples of such functions are given in Sierpinski (1916; cites Denjoy) [73] , Hahn (1921; cites no one) [32] , Rajchman (1921; cites Denjoy and Sierpinski) [64] , Vitali (1922; I have not seen this paper) [77] , and Blumberg (1926; cites no one) [2] . Blumberg's example is the first such function I know of that appeared in English. Curiously, I found this seemingly straightforward historical issue of priority especially difficult to uncover. Other than items published recently (last 10 years or so), the only explicit statement of priority I've seen is Salem (1943, p. 427, footnote) [68] , who credits Denjoy with giving the first example. BIBLIOGRAPHY FOR SINGULAR FUNCTIONS [1] Giedrius Alkauskas, Extensive bibliography on the Minkowski Question Mark Function and allied topics , internet web page, 63 entries. Accessed 15 February 2014. [2] Henry Blumberg, Non-measurable functions connected with certain functional equations , Annals of Mathematics (2) 27 #3 (March 1926), 199-208. In a Note (pp. 206-208) at the end of the paper Blumberg constructs a function that he then proves is strictly increasing, continuous, and has a zero derivative almost everywhere. This particular appearance (in English!) of such a function seems to have been missed by authors who cite early appearances of such functions. A footnote on the first page says that the paper was read ""in part"" on 1 December 1917, so it is possible that Blumberg was in possession of this example back in 1917. However, the footnote does explicitly say in part , and the abstract for this paper in Bull. Amer. Math. Soc. 24 #5 (February 1918) [ see abstract #8 on p. 220 ] does not mention the example, so in my opinion the evidence is not very strong that Blumberg was in possession of the example in 1917. [3] Ralph Philip Boas, Oscillating functions , Duke Mathematical Journal 5 #2 (June 1939), 394-400. Various Baire category results (in various function spaces) relating to the properties of bounded variation, absolute continuity, and having a bounded derivative. [4] Ralph Philip Boas, Differentiability of jump functions , Colloquium Mathematicum 8 #1 (1961), 81-82. This paper gives an elementary proof (no use of differentiating monotonic functions; no use of integration; no use of Lebesgue density) that the derivative of a non-decreasing jump function is zero almost everywhere. See Lipinski (1957, 1961), Marczewski (1955), Pu/Pu (1983), Pu (1980-81), Piranian (1966), and Rubel (1963). The relevance of these papers here is that any non-decreasing function $f$ can be written as $f = A + S + J,$ where $A$ is non-decreasing and absolutely continuous, $S$ is non-decreasing and continuous and has a zero derivative almost everywhere, and $J$ is a non-decreasing jump function (also called a saltus function). This decomposition is unique up to the addition or subtraction of constant functions to $A,$ $S,$ and $J.$ Roughly speaking, this is a decomposition of the function $f$ into a continuous function $A$ that is ""maximally nice"" for certain integration purposes, a continuous function $S$ that is ""maximally bad"" for certain integration purposes, and a function $J$ that is the ""discontinuous part"" of the original function $f.$ An absolutely continuous function is continuous and also nice in many other ways. For example, an absolutely continuous function is finitely differentiable almost everywhere. Of course, any non-decreasing function is also finitely differentiable almost everywhere, but this property is true for absolutely continuous functions even when no additional monotonicity condition is assumed. By way of contrast, recall that a continuous function can be nowhere differentiable. An absolutely continuous function $A$ also satisfies the following version of the Fundamental Theorem of Calculus: $\int_{a}^{b}A'(x)dx = A(b) - A(a)$ (Lebesgue integration). A singular function $S$ fails to satisfy this Fundamental Theorem of Calculus version in the worst way: For any interval $[a,b]$ on which $S$ is defined, we have $\int_{a}^{b}S'(x)dx = 0$ (Lebesgue integration). A jump function is the sum of a constant function and a function that can be defined in the following way. Let $Z$ be a nonempty countable (finite or infinite) set of real numbers and let $B = \{b_{z}: \; z \in Z\}$ be a set of positive real numbers such that $Z$ and $B$ have the same number of elements (same cardinality) and the sum of all the numbers in $B$ is finite (of course, this is automatic if $B$ is a finite set). For each $z \in Z$ define the function $f_z$ by $f_{z}(x) = 0$ if $x < z$ and $f_{z}(x) = b_z$ if $x \geq z.$ Finally, a jump function is (any constant function added to) the zero function or a function that equals $\sum_{z \in Z}f_{z}$ for some choice of the sets $Z$ and $B.$ It can be shown that such a function $J$ is continuous at each point not belonging to $Z,$ and $J$ is discontinuous at each $z \in Z$ with $\limsup_{x \rightarrow z}J(x) - \liminf_{x \rightarrow z}J(x) = b_{z}.$ [5] Andrew Michael Bruckner and John Lander Leonard, On differentiable functions having an everywhere dense set of intervals of constancy , Canadian Mathematical Bulletin 8 #1 (February 1963), 73-76. Let $P$ be a perfect nowhere dense subset of $[0,1]$ (i.e. $P$ is a Cantor set). They prove the following result. There exists a function $f:[0,1] \rightarrow \mathbb R$ that (i) $f$ is constant on each open interval contiguous to $P$ $(f$ can have different constant values on different contiguous intervals) AND (ii) $f$ is not constant on each open interval containing a point of $P$ AND (iii) $f$ is finitely differentiable on $(0,1)$ IF AND ONLY IF $P$ has the property that the intersection of $P$ with every open subinterval of $[0,1]$ has positive Lebesgue measure (i.e. $P$ is ""measure dense"", also called ""metrically dense""). This has the following implication for the Cantor staircase function $F$ (defined by making use of the usual Cantor middle thirds set that has measure zero). No matter how we redefine the values of $F$ at points in the Cantor set, it is not possible to change (or to ""smooth out"") the continuous function $F$ in such a way that we will get a finitely differentiable function, even though $F$ has a zero derivative almost everywhere. [6] Frank Sydney Cater, Most monotone functions are not singular , American Mathematical Monthly 89 #7 (Aug.-Sept. 1982), 466-469. Complements Zamfirescu (1981). See MR 92g:26015 and the references there for more elaborate and generalized versions. [7] Frank Sydney Cater, Mappings into sets of measure zero , Rocky Mountain Journal of Mathematics 16 #1 (Winter 1986), 163-171. [8] Lamberto Cesari, Variation, multiplicity, and semicontinuity , American Mathematical Monthly 65 #5 (May 1958), 317-332. A lengthy and detailed expository survey of classical results involving variation and absolute continuity of functions. [9] Donald Richard Chalice, A characterization of the Cantor function , American Mathematical Monthly 98 #3 (March 1991), 255-258. The following theorem is proved (italics not in original): "" Any real-valued function $F(x)$ on $[0,1]$ that is monotone increasing and satisfies (a) $F(0)=0,$ (b) $F(x/3)=F(x)/2,$ and (c) $F(1-x) = 1 - F(x),$ is the Cantor function. [10] Sandra Lynn Cousins, Singular functions , Pi Mu Epsilon Journal 7 #6 (Spring 1982), 374-381. An elementary expository survey written by an undergraduate for the national (U.S.) Pi Mu Epsilon Student Paper competition (won ""Second Prize""). Includes detailed discussions of the Cantor staircase function and Hellinger's function. [11] Richard Brian Darst, Some Cantor sets and Cantor functions , Mathematics Magazine 45 #1 (January 1972), 2-7. This is an expository paper. Darst begins with a discussion of arc length (defined as the least upper bound of the lengths of polygonal paths with vertices on the graph), then the Cantor middle thirds set is described, then the Cantor staircase function from $[0,1]$ onto $[0,1]$ is described and shown to have length $2$ and shown to have no finite two-sided derivative at any point of the Cantor middle thirds set. In the last section of the paper (pp. 5-7), Darst considers Cantor sets defined by removing (from the various closed intervals at each construction stage) centrally located open intervals whose lengths are a fixed proportion $0 < \lambda \leq 1$ of those removed when constructing the Cantor middle thirds set. When $\lambda = 1$ we obtain the Cantor middle thirds set, and when $0 < \lambda < 1$ we obtain a Cantor set with Lebesgue measure $1 - \lambda.$ Darst then shows that the length of the corresponding Cantor function from $[0,1]$ onto $[0,1]$ is $\lambda + \sqrt{1 + (1-\lambda)^{2}}$ and, when $\lambda < 1,$ Darst shows that the two-sided derivative of the corresponding Cantor function is $\frac{1}{1 - \lambda}$ at almost every point of the Cantor set associated with $\lambda.$ (Here, ""almost every point"" means the complement of the set of points has Lebesgue measure zero.) [12] Richard Brian Darst, The Hausdorff dimension of the nondifferentiability set of the Cantor function is $[\ln(2)/\ln(3)]^2$ , Proceedings of the American Mathematical Society 119 #1 (September 1993), 105-108. See Falconer (2004). [13] Richard Brian Darst, Hausdorff dimension of sets of non-differentiability points of Cantor functions , Mathematical Proceedings of the Cambridge Philosophical Society 117 #1 (January 1995), 185-191. [14] Frederik Michel Dekking and Wenxia Li, How smooth is a Devil's staircase? , Fractals 11 #1 (March 2003), 101-107. [15] Arnaud Denjoy, Mémoire sur les nombres dérivés des fonctions continues [Memoir on the derived numbers of continuous functions], Journal de Mathématiques Pures et Appliquées (7) 1 (1915) , 105-240. In Article 63 ( Exemple VI , pp. 204-209) Denjoy defines a function that he then proves is continuous (see footnote on p. 208), strictly increasing, and has a zero derivative almost everywhere. However, Denjoy's proof makes use of some of the specialized results that he had previously developed in the paper. Sierpinski (1916) was written in order to give an example of such a function, and a verification of these properties, that only makes use of elementary results in real analysis. [16] Arnaud Denjoy, Sur quelques points de la théorie des fonctions [On some points in the theory of functions], Comptes Rendus Hebdomadaires des Séances de l'Académie des Sciences (Paris) 194 (1932), 44-46. [17] Arnaud Denjoy, Sur une fonction de Minkowski [On the function of Minkowski], Comptes Rendus Hebdomadaires des Séances de l'Académie des Sciences (Paris) 198 (1934), 44-47. [18] Arnaud Denjoy, Sur une fonction réelle de Minkowski [On the real function of Minkowski], Journal de Mathématiques Pures et Appliquées (9) 17 (1938), 105-151. [19] Arnaud Denjoy, Propriétés différentielles de la fonction Minkowskienne réelle. Statistique des fractions continues [Differential properties of the real Minkowski function. Statistics of continued fractions], Comptes Rendus Hebdomadaires des Séances de l'Académie des Sciences (Paris) 242 (1956), 2075-2079. [20] Oleksiy [Aleksei] Alfredovich Dovgoshey, Olli Tapani Martio, Vladimir Illich Ryazanov, and Matti Vuorinen, The Cantor function , Expositiones Mathematicae 24 #1 (2006), 1-37. A very highly recommended survey paper. [21] Thomas Paul Dence, Differentiable points of the generalized Cantor function , Rocky Mountain Journal of Mathematics 9 #2 (Spring 1979), 214-217. [22] Harry Dym, On a class of monotone functions generated by ergodic sequences , American Mathematical Monthly 75 #6 (June-July 1968), 594-601. [23] John [Jack] Alan Eidswick, A characterization of the nondifferentiability set of the Cantor function , **Proceedings of the American Mathematical Society 42 #1 (January 1974), 214-217. [24] Griffith Conrad Evans, Calculation of moments for a Cantor-Vitali function , American Mathematical Monthly 64 #8 (Part II) (October 1957), 22-27. See Dovgoshey/Martio/Ryazanov/Vuorinen (2006, Section 5) and Wall (1961). [25] Kenneth John Falconer, One-sided multifractal analysis and points of non-differentiability of devil's staircases , Mathematical Proceedings of the Cambridge Philosophical Society 136 #1 (January 2004), 167-174. Author's Abstract (italics not in original): We examine the multifractal spectra of one-sided local dimensions of Ahlfors regular measures on R . This brings into a natural context a curious property that has been observed in a number of instances, namely that the Hausdorff dimension of the set of points of non-differentiability of a self-affine 'devil's staircase' function is the square of the dimension of the set of points of increase. See Darst (1993). [26] Julian Frederic Fleron, Pointwise Derivates of the Cantor Function , Master of Arts Thesis (under Max August Jodeit), University of Minnesota, June 1990, iv + 106 pages. This Thesis gives a very thorough expository analysis of Eidswick (1974) and the appropriate background material. TABLE OF CONTENTS: Chapter 1. Introduction §1. Introduction (pp. 1-3); §2. History of the Cantor Set and Cantor Function (pp. 3-8); §3. Representations of the Cantor Set and Cantor Function (pp. 9-17). Chapter 2. Dini Derivates of the Cantor Function §1. Upper and Lower Right Derivates [of the Cantor Function] (pp. 18-33); §2. Bounding the Upper and Lower Left Derivates [of the Cantor Function] (pp. 33-39); §3. Upper and Lower Left Derivates [of the Cantor Function] (pp. 39-43); §4. Finite Derivates [of the Cantor Function] (pp. 43-51). Chapter 3. Sets of Derivates §1. Preliminary Investigation of the Ternary Structure (pp. 52-56); §2. Hausdorff Measure [of Some Previously Studied Sets] (pp. 56-63); §3. Hausdorff Dimension and Category [and Density of Some Previously Studied Sets] (pp. 63-66). Chapter 4. The Ternary Structure and Sets of Derivates §1. Further Investigation of the Ternary Structure (pp. 67-80). §2. Approximating a Special Essential Secant (pp. 80-85); §3. Constructing Points with Arbitrary Lower Derivates (pp. 85-95). Appendix §1. Construction of a Point Where $f_{+}f(x) = a$ (pp. 96-98); §2. Hausdorff Dimension of the Cantor Set (pp. 98-102). [27] Julian Frederic Fleron, A note on the history of the Cantor set and Cantor function , Mathematics Magazine 67 #2 (April 1994), 136-140. [28] Gerald Freilich, Increasing continuous singular functions , American Mathematical Monthly 80 #8 (October 1973), 918-919. Let $C(x)$ be the Cantor staircase function and let $\{a_{1},\,a_{2},\, \ldots\}$ be a countable set that is dense in ${\mathbb R}.$ Freilich gives a short proof that $\sum_{n=1}^{\infty}2^{-n}C\left(2^{n}(x-a_{n})\right)$ is continuous, strictly increasing, and has a zero derivative almost everywhere. Fubini's theorem on differentiation of series is used to establish ""zero derivative almost everywhere"". [29] Krishna Murari Garg, On singular functions , Revue Roumaine de Mathématiques Pures et Appliquées 14 #10 (1969), 1441-1452. Zbl 191.34601 review [30] Krishna Murari Garg, Construction of absolutely continuous and singular functions that are nowhere of monotonic type , pp. 61-79 in Daniel Waterman (editor), Classical Real Analysis , Contemporary Mathematics 42 (1985), x + 216 pages. [30] Krishna Murari Garg, Construction of absolutely continuous and singular functions that are nowhere of monotonic type , pp. 61-79 in Daniel Waterman (editor), Classical Real Analysis , Contemporary Mathematics 42 (1985), x + 216 pages. [31] Ray Edwin Gilman, A class of functions continuous but not absolutely continuous , Annals of Mathematics (2) 33 #3 (July 1932), 433-442. This is a sequel to Hille/Tamarkin (1929) in which the base 3 representation into a base 2 representation is generalized to base $\alpha$ representation into a base $\beta$ representation, where $\alpha$ and $\beta$ are integers such that $1 < \beta < \alpha$ and $\beta - 1$ divides $\alpha - 1.$ Also, there is more focus in Gilman's paper on the Dini derivate behavior of the corresponding functions than there is in Hille/Tamarkin's paper. See Dovgoshey/Martio/Ryazanov/Vuorinen (2006, p. 32, Remark 10.4) for an erroneous claim in Gilman's paper. [32] Hans Hahn, Theorie der Reellen Funktionen [Theory of Real Functions], Verlag von Julius Springer (Berlin), 1921, viii + 600 pages. Hahn gives an example of a strictly increasing continuous function with a zero derivative almost everywhere on pp. 538-539. [33] Philip Hartman and Richard Brandon Kershner, The structure of monotone functions , American Journal of Mathematics 59 #4 (October 1937), 809-822. This is a study of the absolutely continuous and singular behavior of continuous non-decreasing functions $f: [0,1] \rightarrow [0,1]$ ""in terms of the asymptotic or qualitative properties of the two dense sequences of numbers which are mapped on to each other by $y = f(x).$"" Section 5 (pp. 818-819) proves a general result involving modulus of continuity that implies (as a special case) for each $0 < \alpha < 1$ there exists a strictly increasing continuous function $f$ with derivative zero almost everywhere such that $f$ has Lipschitz order exactly $\alpha$.","Closed. This question is off-topic . It is not currently accepting answers. This question does not appear to be about math within the scope defined in the help center . Closed 10 years ago . Improve this question I wound up assembling a rather lengthy and partially annotated bibliography for my answer to the math StackExchange question Singular continuous functions , but it seems I got a little too carried away and found myself with a post well over the maximum of 30,000 characters. Thus, I'm using this ""question"" to post the full bibliography, which I think could be a useful reference to others. I am including a large portion of the bibliography in the question because I still have too many characters otherwise. SOME HISTORY It is easy to construct non-decreasing continuous functions with a zero derivative almost everywhere, and such functions have numerous appearances in the literature beginning around 1883/1884 in papers by Cantor and Scheeffer (separately authored). See Fleron (1994) [27] and Maurey/Tacchi (2002) [54] . On the other hand, the first proof of the existence of strictly increasing continuous functions with a zero derivative almost everywhere seems to be in Ernst Hellinger's 1907 Ph.D. Dissertation, but I haven't looked into this to see exactly what he proved and where in the Dissertation it can be found. (Minkowski's function came earlier, but it wasn't proved to have this property until Denjoy proved it in 1934/1938 [17] [18] .) Likely unaware of Hellinger's result, Denjoy gave such a function in 1915 [15] . Other early examples of such functions are given in Sierpinski (1916; cites Denjoy) [73] , Hahn (1921; cites no one) [32] , Rajchman (1921; cites Denjoy and Sierpinski) [64] , Vitali (1922; I have not seen this paper) [77] , and Blumberg (1926; cites no one) [2] . Blumberg's example is the first such function I know of that appeared in English. Curiously, I found this seemingly straightforward historical issue of priority especially difficult to uncover. Other than items published recently (last 10 years or so), the only explicit statement of priority I've seen is Salem (1943, p. 427, footnote) [68] , who credits Denjoy with giving the first example. BIBLIOGRAPHY FOR SINGULAR FUNCTIONS [1] Giedrius Alkauskas, Extensive bibliography on the Minkowski Question Mark Function and allied topics , internet web page, 63 entries. Accessed 15 February 2014. [2] Henry Blumberg, Non-measurable functions connected with certain functional equations , Annals of Mathematics (2) 27 #3 (March 1926), 199-208. In a Note (pp. 206-208) at the end of the paper Blumberg constructs a function that he then proves is strictly increasing, continuous, and has a zero derivative almost everywhere. This particular appearance (in English!) of such a function seems to have been missed by authors who cite early appearances of such functions. A footnote on the first page says that the paper was read ""in part"" on 1 December 1917, so it is possible that Blumberg was in possession of this example back in 1917. However, the footnote does explicitly say in part , and the abstract for this paper in Bull. Amer. Math. Soc. 24 #5 (February 1918) [ see abstract #8 on p. 220 ] does not mention the example, so in my opinion the evidence is not very strong that Blumberg was in possession of the example in 1917. [3] Ralph Philip Boas, Oscillating functions , Duke Mathematical Journal 5 #2 (June 1939), 394-400. Various Baire category results (in various function spaces) relating to the properties of bounded variation, absolute continuity, and having a bounded derivative. [4] Ralph Philip Boas, Differentiability of jump functions , Colloquium Mathematicum 8 #1 (1961), 81-82. This paper gives an elementary proof (no use of differentiating monotonic functions; no use of integration; no use of Lebesgue density) that the derivative of a non-decreasing jump function is zero almost everywhere. See Lipinski (1957, 1961), Marczewski (1955), Pu/Pu (1983), Pu (1980-81), Piranian (1966), and Rubel (1963). The relevance of these papers here is that any non-decreasing function $f$ can be written as $f = A + S + J,$ where $A$ is non-decreasing and absolutely continuous, $S$ is non-decreasing and continuous and has a zero derivative almost everywhere, and $J$ is a non-decreasing jump function (also called a saltus function). This decomposition is unique up to the addition or subtraction of constant functions to $A,$ $S,$ and $J.$ Roughly speaking, this is a decomposition of the function $f$ into a continuous function $A$ that is ""maximally nice"" for certain integration purposes, a continuous function $S$ that is ""maximally bad"" for certain integration purposes, and a function $J$ that is the ""discontinuous part"" of the original function $f.$ An absolutely continuous function is continuous and also nice in many other ways. For example, an absolutely continuous function is finitely differentiable almost everywhere. Of course, any non-decreasing function is also finitely differentiable almost everywhere, but this property is true for absolutely continuous functions even when no additional monotonicity condition is assumed. By way of contrast, recall that a continuous function can be nowhere differentiable. An absolutely continuous function $A$ also satisfies the following version of the Fundamental Theorem of Calculus: $\int_{a}^{b}A'(x)dx = A(b) - A(a)$ (Lebesgue integration). A singular function $S$ fails to satisfy this Fundamental Theorem of Calculus version in the worst way: For any interval $[a,b]$ on which $S$ is defined, we have $\int_{a}^{b}S'(x)dx = 0$ (Lebesgue integration). A jump function is the sum of a constant function and a function that can be defined in the following way. Let $Z$ be a nonempty countable (finite or infinite) set of real numbers and let $B = \{b_{z}: \; z \in Z\}$ be a set of positive real numbers such that $Z$ and $B$ have the same number of elements (same cardinality) and the sum of all the numbers in $B$ is finite (of course, this is automatic if $B$ is a finite set). For each $z \in Z$ define the function $f_z$ by $f_{z}(x) = 0$ if $x < z$ and $f_{z}(x) = b_z$ if $x \geq z.$ Finally, a jump function is (any constant function added to) the zero function or a function that equals $\sum_{z \in Z}f_{z}$ for some choice of the sets $Z$ and $B.$ It can be shown that such a function $J$ is continuous at each point not belonging to $Z,$ and $J$ is discontinuous at each $z \in Z$ with $\limsup_{x \rightarrow z}J(x) - \liminf_{x \rightarrow z}J(x) = b_{z}.$ [5] Andrew Michael Bruckner and John Lander Leonard, On differentiable functions having an everywhere dense set of intervals of constancy , Canadian Mathematical Bulletin 8 #1 (February 1963), 73-76. Let $P$ be a perfect nowhere dense subset of $[0,1]$ (i.e. $P$ is a Cantor set). They prove the following result. There exists a function $f:[0,1] \rightarrow \mathbb R$ that (i) $f$ is constant on each open interval contiguous to $P$ $(f$ can have different constant values on different contiguous intervals) AND (ii) $f$ is not constant on each open interval containing a point of $P$ AND (iii) $f$ is finitely differentiable on $(0,1)$ IF AND ONLY IF $P$ has the property that the intersection of $P$ with every open subinterval of $[0,1]$ has positive Lebesgue measure (i.e. $P$ is ""measure dense"", also called ""metrically dense""). This has the following implication for the Cantor staircase function $F$ (defined by making use of the usual Cantor middle thirds set that has measure zero). No matter how we redefine the values of $F$ at points in the Cantor set, it is not possible to change (or to ""smooth out"") the continuous function $F$ in such a way that we will get a finitely differentiable function, even though $F$ has a zero derivative almost everywhere. [6] Frank Sydney Cater, Most monotone functions are not singular , American Mathematical Monthly 89 #7 (Aug.-Sept. 1982), 466-469. Complements Zamfirescu (1981). See MR 92g:26015 and the references there for more elaborate and generalized versions. [7] Frank Sydney Cater, Mappings into sets of measure zero , Rocky Mountain Journal of Mathematics 16 #1 (Winter 1986), 163-171. [8] Lamberto Cesari, Variation, multiplicity, and semicontinuity , American Mathematical Monthly 65 #5 (May 1958), 317-332. A lengthy and detailed expository survey of classical results involving variation and absolute continuity of functions. [9] Donald Richard Chalice, A characterization of the Cantor function , American Mathematical Monthly 98 #3 (March 1991), 255-258. The following theorem is proved (italics not in original): "" Any real-valued function $F(x)$ on $[0,1]$ that is monotone increasing and satisfies (a) $F(0)=0,$ (b) $F(x/3)=F(x)/2,$ and (c) $F(1-x) = 1 - F(x),$ is the Cantor function. [10] Sandra Lynn Cousins, Singular functions , Pi Mu Epsilon Journal 7 #6 (Spring 1982), 374-381. An elementary expository survey written by an undergraduate for the national (U.S.) Pi Mu Epsilon Student Paper competition (won ""Second Prize""). Includes detailed discussions of the Cantor staircase function and Hellinger's function. [11] Richard Brian Darst, Some Cantor sets and Cantor functions , Mathematics Magazine 45 #1 (January 1972), 2-7. This is an expository paper. Darst begins with a discussion of arc length (defined as the least upper bound of the lengths of polygonal paths with vertices on the graph), then the Cantor middle thirds set is described, then the Cantor staircase function from $[0,1]$ onto $[0,1]$ is described and shown to have length $2$ and shown to have no finite two-sided derivative at any point of the Cantor middle thirds set. In the last section of the paper (pp. 5-7), Darst considers Cantor sets defined by removing (from the various closed intervals at each construction stage) centrally located open intervals whose lengths are a fixed proportion $0 < \lambda \leq 1$ of those removed when constructing the Cantor middle thirds set. When $\lambda = 1$ we obtain the Cantor middle thirds set, and when $0 < \lambda < 1$ we obtain a Cantor set with Lebesgue measure $1 - \lambda.$ Darst then shows that the length of the corresponding Cantor function from $[0,1]$ onto $[0,1]$ is $\lambda + \sqrt{1 + (1-\lambda)^{2}}$ and, when $\lambda < 1,$ Darst shows that the two-sided derivative of the corresponding Cantor function is $\frac{1}{1 - \lambda}$ at almost every point of the Cantor set associated with $\lambda.$ (Here, ""almost every point"" means the complement of the set of points has Lebesgue measure zero.) [12] Richard Brian Darst, The Hausdorff dimension of the nondifferentiability set of the Cantor function is $[\ln(2)/\ln(3)]^2$ , Proceedings of the American Mathematical Society 119 #1 (September 1993), 105-108. See Falconer (2004). [13] Richard Brian Darst, Hausdorff dimension of sets of non-differentiability points of Cantor functions , Mathematical Proceedings of the Cambridge Philosophical Society 117 #1 (January 1995), 185-191. [14] Frederik Michel Dekking and Wenxia Li, How smooth is a Devil's staircase? , Fractals 11 #1 (March 2003), 101-107. [15] Arnaud Denjoy, Mémoire sur les nombres dérivés des fonctions continues [Memoir on the derived numbers of continuous functions], Journal de Mathématiques Pures et Appliquées (7) 1 (1915) , 105-240. In Article 63 ( Exemple VI , pp. 204-209) Denjoy defines a function that he then proves is continuous (see footnote on p. 208), strictly increasing, and has a zero derivative almost everywhere. However, Denjoy's proof makes use of some of the specialized results that he had previously developed in the paper. Sierpinski (1916) was written in order to give an example of such a function, and a verification of these properties, that only makes use of elementary results in real analysis. [16] Arnaud Denjoy, Sur quelques points de la théorie des fonctions [On some points in the theory of functions], Comptes Rendus Hebdomadaires des Séances de l'Académie des Sciences (Paris) 194 (1932), 44-46. [17] Arnaud Denjoy, Sur une fonction de Minkowski [On the function of Minkowski], Comptes Rendus Hebdomadaires des Séances de l'Académie des Sciences (Paris) 198 (1934), 44-47. [18] Arnaud Denjoy, Sur une fonction réelle de Minkowski [On the real function of Minkowski], Journal de Mathématiques Pures et Appliquées (9) 17 (1938), 105-151. [19] Arnaud Denjoy, Propriétés différentielles de la fonction Minkowskienne réelle. Statistique des fractions continues [Differential properties of the real Minkowski function. Statistics of continued fractions], Comptes Rendus Hebdomadaires des Séances de l'Académie des Sciences (Paris) 242 (1956), 2075-2079. [20] Oleksiy [Aleksei] Alfredovich Dovgoshey, Olli Tapani Martio, Vladimir Illich Ryazanov, and Matti Vuorinen, The Cantor function , Expositiones Mathematicae 24 #1 (2006), 1-37. A very highly recommended survey paper. [21] Thomas Paul Dence, Differentiable points of the generalized Cantor function , Rocky Mountain Journal of Mathematics 9 #2 (Spring 1979), 214-217. [22] Harry Dym, On a class of monotone functions generated by ergodic sequences , American Mathematical Monthly 75 #6 (June-July 1968), 594-601. [23] John [Jack] Alan Eidswick, A characterization of the nondifferentiability set of the Cantor function , **Proceedings of the American Mathematical Society 42 #1 (January 1974), 214-217. [24] Griffith Conrad Evans, Calculation of moments for a Cantor-Vitali function , American Mathematical Monthly 64 #8 (Part II) (October 1957), 22-27. See Dovgoshey/Martio/Ryazanov/Vuorinen (2006, Section 5) and Wall (1961). [25] Kenneth John Falconer, One-sided multifractal analysis and points of non-differentiability of devil's staircases , Mathematical Proceedings of the Cambridge Philosophical Society 136 #1 (January 2004), 167-174. Author's Abstract (italics not in original): We examine the multifractal spectra of one-sided local dimensions of Ahlfors regular measures on R . This brings into a natural context a curious property that has been observed in a number of instances, namely that the Hausdorff dimension of the set of points of non-differentiability of a self-affine 'devil's staircase' function is the square of the dimension of the set of points of increase. See Darst (1993). [26] Julian Frederic Fleron, Pointwise Derivates of the Cantor Function , Master of Arts Thesis (under Max August Jodeit), University of Minnesota, June 1990, iv + 106 pages. This Thesis gives a very thorough expository analysis of Eidswick (1974) and the appropriate background material. TABLE OF CONTENTS: Chapter 1. Introduction §1. Introduction (pp. 1-3); §2. History of the Cantor Set and Cantor Function (pp. 3-8); §3. Representations of the Cantor Set and Cantor Function (pp. 9-17). Chapter 2. Dini Derivates of the Cantor Function §1. Upper and Lower Right Derivates [of the Cantor Function] (pp. 18-33); §2. Bounding the Upper and Lower Left Derivates [of the Cantor Function] (pp. 33-39); §3. Upper and Lower Left Derivates [of the Cantor Function] (pp. 39-43); §4. Finite Derivates [of the Cantor Function] (pp. 43-51). Chapter 3. Sets of Derivates §1. Preliminary Investigation of the Ternary Structure (pp. 52-56); §2. Hausdorff Measure [of Some Previously Studied Sets] (pp. 56-63); §3. Hausdorff Dimension and Category [and Density of Some Previously Studied Sets] (pp. 63-66). Chapter 4. The Ternary Structure and Sets of Derivates §1. Further Investigation of the Ternary Structure (pp. 67-80). §2. Approximating a Special Essential Secant (pp. 80-85); §3. Constructing Points with Arbitrary Lower Derivates (pp. 85-95). Appendix §1. Construction of a Point Where $f_{+}f(x) = a$ (pp. 96-98); §2. Hausdorff Dimension of the Cantor Set (pp. 98-102). [27] Julian Frederic Fleron, A note on the history of the Cantor set and Cantor function , Mathematics Magazine 67 #2 (April 1994), 136-140. [28] Gerald Freilich, Increasing continuous singular functions , American Mathematical Monthly 80 #8 (October 1973), 918-919. Let $C(x)$ be the Cantor staircase function and let $\{a_{1},\,a_{2},\, \ldots\}$ be a countable set that is dense in ${\mathbb R}.$ Freilich gives a short proof that $\sum_{n=1}^{\infty}2^{-n}C\left(2^{n}(x-a_{n})\right)$ is continuous, strictly increasing, and has a zero derivative almost everywhere. Fubini's theorem on differentiation of series is used to establish ""zero derivative almost everywhere"". [29] Krishna Murari Garg, On singular functions , Revue Roumaine de Mathématiques Pures et Appliquées 14 #10 (1969), 1441-1452. Zbl 191.34601 review [30] Krishna Murari Garg, Construction of absolutely continuous and singular functions that are nowhere of monotonic type , pp. 61-79 in Daniel Waterman (editor), Classical Real Analysis , Contemporary Mathematics 42 (1985), x + 216 pages. [30] Krishna Murari Garg, Construction of absolutely continuous and singular functions that are nowhere of monotonic type , pp. 61-79 in Daniel Waterman (editor), Classical Real Analysis , Contemporary Mathematics 42 (1985), x + 216 pages. [31] Ray Edwin Gilman, A class of functions continuous but not absolutely continuous , Annals of Mathematics (2) 33 #3 (July 1932), 433-442. This is a sequel to Hille/Tamarkin (1929) in which the base 3 representation into a base 2 representation is generalized to base $\alpha$ representation into a base $\beta$ representation, where $\alpha$ and $\beta$ are integers such that $1 < \beta < \alpha$ and $\beta - 1$ divides $\alpha - 1.$ Also, there is more focus in Gilman's paper on the Dini derivate behavior of the corresponding functions than there is in Hille/Tamarkin's paper. See Dovgoshey/Martio/Ryazanov/Vuorinen (2006, p. 32, Remark 10.4) for an erroneous claim in Gilman's paper. [32] Hans Hahn, Theorie der Reellen Funktionen [Theory of Real Functions], Verlag von Julius Springer (Berlin), 1921, viii + 600 pages. Hahn gives an example of a strictly increasing continuous function with a zero derivative almost everywhere on pp. 538-539. [33] Philip Hartman and Richard Brandon Kershner, The structure of monotone functions , American Journal of Mathematics 59 #4 (October 1937), 809-822. This is a study of the absolutely continuous and singular behavior of continuous non-decreasing functions $f: [0,1] \rightarrow [0,1]$ ""in terms of the asymptotic or qualitative properties of the two dense sequences of numbers which are mapped on to each other by $y = f(x).$"" Section 5 (pp. 818-819) proves a general result involving modulus of continuity that implies (as a special case) for each $0 < \alpha < 1$ there exists a strictly increasing continuous function $f$ with derivative zero almost everywhere such that $f$ has Lipschitz order exactly $\alpha$.",,['real-analysis']
29,Covering $\mathbb R^2$ with function graphs,Covering  with function graphs,\mathbb R^2,"Suppose we have a countable family of function graphs (each function is $\mathbb R\to\mathbb R$, not necessary continuous). Obviously, they cannot cover the whole plane $\mathbb R^2$, because they cannot even cover every of uncountably many points on a single vertical line. But suppose now we are allowed to rotate each graph from the family by arbitrary angle around an arbitrary point of the plane (the total number of graphs is still countable). Is it possible to cover the whole plane $\mathbb R^2$ in this case?","Suppose we have a countable family of function graphs (each function is $\mathbb R\to\mathbb R$, not necessary continuous). Obviously, they cannot cover the whole plane $\mathbb R^2$, because they cannot even cover every of uncountably many points on a single vertical line. But suppose now we are allowed to rotate each graph from the family by arbitrary angle around an arbitrary point of the plane (the total number of graphs is still countable). Is it possible to cover the whole plane $\mathbb R^2$ in this case?",,"['real-analysis', 'geometry', 'functions', 'set-theory', 'cardinals']"
30,A tricky integral inequality,A tricky integral inequality,,"A friend has submitted this problem to me: Let $0<a<b<1$ and $f:[0,1]\to \mathbb R$ be a differentiable function such that $$\displaystyle \frac{\int_0^a f(x) dx}{a(1-a)}+\frac{\int_b^1 f(x) dx}{b(1-b)}=0$$ Prove that $\displaystyle \left| \int_0^1 f(x)dx\right|\leq \left(\frac{b-a}2\right)\sup_{x\in [0,1]}\left|f'(x)+2\normalsize{\frac{\int_0^a f(t) dt}{a(1-a)}}\right|$ I haven't solved this yet, but I've made some progress: Since $\frac{1}{1-a}\left(\frac{1}{a}\int_0^a f(x) dx\right)+ \frac{1}{b}\left(\frac{1}{1-b}\int_b^1 f(x) dx\right)=0$, the Mean Value Theorem yields the existence of $\xi_a\in[0,a]$ and $\xi_b\in[b,1]$ such that $$bf(\xi_a)+(1-a)f(\xi_b)=0$$ This implies $f(\xi_a)$ and $f(\xi_b)$ have opposite signs, thus there is some $c\in [\xi_a,\xi_b]$ such that $f(c)=0$. We may suppose WLOG that $\sup_{x\in [0,1]} |f'(x)|<\infty$ (there's nothing to prove otherwise). Let $M=\sup_{x\in [0,1]} |f'(x)|$. One can write $$\displaystyle  \begin{align}\int_0^1 f(x)dx &= \int_0^a f(x)dx + \int_a^b f(x)dx + \int_b^1 f(x)dx \\ &= \frac{\int_0^a f(x) dx}{a(1-a)} \left( a(1-a) -b(1-b)\right) +  \color{red}{\int_a^b f(x)dx} \end{align} $$ Note that $\displaystyle \color{red}{\int_a^b f(x)dx} = \color{green}{\int_a^b (f(x)-f(a))dx} + \color{blue}{(b-a)f(a)}$. Since $f(a) = f(a)-f(c) = f'(\xi_c) (a-c)$, we have $|f(a)|\leq M |a-c|\leq M$. The Mean Value Theorem also gives $\displaystyle \left| \int_a^b (f(x)-f(a))dx \right| \leq M \int_a^b (x-a) dx = M \frac{(b-a)^2}2$. Putting everything together we have the estimate $$ \left| \int_0^1 f(x)dx\right|\leq \left(\frac{b-a}2\right)  \left[2 \left|\frac{\int_0^a f(x) dx}{a(1-a)}\right| \underbrace{\frac{ \left( a(1-a) -b(1-b)\right)}{b-a}}_{\leq 1} + \color{green}{M (b-a)} +\color{blue}{2M }   \right]$$ and we get the bound $$ \left| \int_0^1 f(x)dx\right|\leq \left(\frac{b-a}2\right)\left(2\normalsize{\frac{\int_0^a f(t) dt}{a(1-a)}} + 3M\right) $$ which is not as sharp as what's required... Note: this problem is similar to this Prove an integral inequality $|\int\limits_0^1f(x)dx|\leq\frac{1-a+b}{4}M$ I've tried to apply similar techniques, to no avail.","A friend has submitted this problem to me: Let $0<a<b<1$ and $f:[0,1]\to \mathbb R$ be a differentiable function such that $$\displaystyle \frac{\int_0^a f(x) dx}{a(1-a)}+\frac{\int_b^1 f(x) dx}{b(1-b)}=0$$ Prove that $\displaystyle \left| \int_0^1 f(x)dx\right|\leq \left(\frac{b-a}2\right)\sup_{x\in [0,1]}\left|f'(x)+2\normalsize{\frac{\int_0^a f(t) dt}{a(1-a)}}\right|$ I haven't solved this yet, but I've made some progress: Since $\frac{1}{1-a}\left(\frac{1}{a}\int_0^a f(x) dx\right)+ \frac{1}{b}\left(\frac{1}{1-b}\int_b^1 f(x) dx\right)=0$, the Mean Value Theorem yields the existence of $\xi_a\in[0,a]$ and $\xi_b\in[b,1]$ such that $$bf(\xi_a)+(1-a)f(\xi_b)=0$$ This implies $f(\xi_a)$ and $f(\xi_b)$ have opposite signs, thus there is some $c\in [\xi_a,\xi_b]$ such that $f(c)=0$. We may suppose WLOG that $\sup_{x\in [0,1]} |f'(x)|<\infty$ (there's nothing to prove otherwise). Let $M=\sup_{x\in [0,1]} |f'(x)|$. One can write $$\displaystyle  \begin{align}\int_0^1 f(x)dx &= \int_0^a f(x)dx + \int_a^b f(x)dx + \int_b^1 f(x)dx \\ &= \frac{\int_0^a f(x) dx}{a(1-a)} \left( a(1-a) -b(1-b)\right) +  \color{red}{\int_a^b f(x)dx} \end{align} $$ Note that $\displaystyle \color{red}{\int_a^b f(x)dx} = \color{green}{\int_a^b (f(x)-f(a))dx} + \color{blue}{(b-a)f(a)}$. Since $f(a) = f(a)-f(c) = f'(\xi_c) (a-c)$, we have $|f(a)|\leq M |a-c|\leq M$. The Mean Value Theorem also gives $\displaystyle \left| \int_a^b (f(x)-f(a))dx \right| \leq M \int_a^b (x-a) dx = M \frac{(b-a)^2}2$. Putting everything together we have the estimate $$ \left| \int_0^1 f(x)dx\right|\leq \left(\frac{b-a}2\right)  \left[2 \left|\frac{\int_0^a f(x) dx}{a(1-a)}\right| \underbrace{\frac{ \left( a(1-a) -b(1-b)\right)}{b-a}}_{\leq 1} + \color{green}{M (b-a)} +\color{blue}{2M }   \right]$$ and we get the bound $$ \left| \int_0^1 f(x)dx\right|\leq \left(\frac{b-a}2\right)\left(2\normalsize{\frac{\int_0^a f(t) dt}{a(1-a)}} + 3M\right) $$ which is not as sharp as what's required... Note: this problem is similar to this Prove an integral inequality $|\int\limits_0^1f(x)dx|\leq\frac{1-a+b}{4}M$ I've tried to apply similar techniques, to no avail.",,"['real-analysis', 'integration', 'inequality', 'integral-inequality']"
31,$\phi(\pi)$ and other irrationals (Euler's totient function),and other irrationals (Euler's totient function),\phi(\pi),"Over the natural numbers, Euler's totient function $\phi(n)$ has the nice property that $\phi(n^m)=n^{m-1}\phi(n)$ . I've found that this can naively extend the totient function over the rationals via: $$\phi(b)=\phi\left(\left(\frac{1}{b}\right)^{-1}\right)=\left(\frac{1}{b}\right)^{-2}\phi\left(\frac{1}{b}\right)=b^2\phi\left(\frac{1}{b}\right)$$ $$\implies\phi\left(\frac{1}{b}\right)=\frac{\phi(b)}{b^2}$$ Thus, with another property being that $\gcd(a,b)=1\implies \phi(ab)=\phi(a)\phi(b)$ , then under the assumption that $\gcd(a,b)=1\implies\gcd\left(a,\frac{1}{b}\right)=1$ , we can define $$\phi\left(\frac{a}{b}\right):=\frac{\phi(a)\phi(b)}{b^2}$$ Note that this still preserves consistency over the natural numbers: $$\phi\left(\frac{a}{1}\right):=\frac{\phi(a)\phi(1)}{1^2}=\phi(a)$$ With this, I was immediately curious as to if a sequence of rational numbers $q_n$ converged to an irrational, would $\phi(q_n)$ also converge, and if so, to what? As an initial test, I used the sequence $\pi_n=\sum_{k=0}^n\frac{4(-1)^k}{2k+1}$ . Which, as you know, converges to $\pi$ . I also tested the sequence $e_n=\sum_{k=0}^n\frac{1}{k!}$ (which converges to $e$ , respectively). To my surprise, I found that with this definition of $\phi$ , it seemed that both $\pi_n$ and $e_n$ converge. The graph of $\phi(e_n)$ is in blue, and $\phi(\pi_n)$ in red. Curiously, (perhaps due to the closeness of $\pi\approx e$ ), they both seemingly approach a value of about $0.4$ . That being said, my computer and I only had the patience to calculate the first $40$ -ish terms, so I would very much like to know what the long-term behavior of the graph is. Any insight would be very much appreciated. Addendum: May 13th With the feedback I've gathered in the comments, I've done some more analysis, which may be interesting to some of you. Specifically, it does not appear that the totient of all converging rational sequences converge. For example, with suggestions from Conifold, I tested the sequence of rationals defined by the continued fraction for the golden ratio. Which can be simplified to $\varphi_n=\frac{F_{n+1}}{F_n}$ , where $F_n$ is the $n$ 'th Fibonacci number. It seemed evident from computational analysis that $\phi(\varphi_n)$ did not converge, however it seemed evident that $\limsup_{n\to\infty}\phi(\varphi_n)=\varphi=\frac{1+\sqrt{5}}{2}.$ . The average and $\liminf$ also seemed to converge, however the $\limsup$ seemed to give more canonical results; on other sequences as well. For the case of $\sqrt{2}$ , it may be defined by $\phi(\sqrt{2})=2^{\frac{1}{2}-1}\phi(2)=\frac{\sqrt{2}}{2}$ , which seemed to be exactly what the $\limsup$ converged to when taking the continued fraction for $\sqrt{2}$ . Lastly, and perhaps most curious, if we define $e_n:=\left(\frac{n+1}{n}\right)^n$ (as $\lim_{n\to\infty}e_n=e$ ), then, assuming my math is correct, we can deduce $$L=\limsup{\phi(e_n)}=\limsup_{n\to\infty}{\phi\left(\left(\frac{n+1}{n}\right)^n\right)}=\limsup\left(\frac{n+1}{n}\right)^{n-1}\frac{\phi(n)\phi(n+1)}{n^2}$$ $$\implies \ln L = \limsup(n-1)\ln\left(\frac{n+1}{n}\right)+\ln\left(\frac{\phi(n)}{n}\right)+\ln\left(\frac{\phi(n+1)}{n}\right)$$ $$=1+\ln 1+\ln 1$$ $$\implies L=e$$ However in my computational analyses, It seemed that $\limsup \phi(e_n)\approx \frac{e}{2}$ I'm not sure what can be taken from this, however I do find these results interesting, so perhaps you will too.","Over the natural numbers, Euler's totient function has the nice property that . I've found that this can naively extend the totient function over the rationals via: Thus, with another property being that , then under the assumption that , we can define Note that this still preserves consistency over the natural numbers: With this, I was immediately curious as to if a sequence of rational numbers converged to an irrational, would also converge, and if so, to what? As an initial test, I used the sequence . Which, as you know, converges to . I also tested the sequence (which converges to , respectively). To my surprise, I found that with this definition of , it seemed that both and converge. The graph of is in blue, and in red. Curiously, (perhaps due to the closeness of ), they both seemingly approach a value of about . That being said, my computer and I only had the patience to calculate the first -ish terms, so I would very much like to know what the long-term behavior of the graph is. Any insight would be very much appreciated. Addendum: May 13th With the feedback I've gathered in the comments, I've done some more analysis, which may be interesting to some of you. Specifically, it does not appear that the totient of all converging rational sequences converge. For example, with suggestions from Conifold, I tested the sequence of rationals defined by the continued fraction for the golden ratio. Which can be simplified to , where is the 'th Fibonacci number. It seemed evident from computational analysis that did not converge, however it seemed evident that . The average and also seemed to converge, however the seemed to give more canonical results; on other sequences as well. For the case of , it may be defined by , which seemed to be exactly what the converged to when taking the continued fraction for . Lastly, and perhaps most curious, if we define (as ), then, assuming my math is correct, we can deduce However in my computational analyses, It seemed that I'm not sure what can be taken from this, however I do find these results interesting, so perhaps you will too.","\phi(n) \phi(n^m)=n^{m-1}\phi(n) \phi(b)=\phi\left(\left(\frac{1}{b}\right)^{-1}\right)=\left(\frac{1}{b}\right)^{-2}\phi\left(\frac{1}{b}\right)=b^2\phi\left(\frac{1}{b}\right) \implies\phi\left(\frac{1}{b}\right)=\frac{\phi(b)}{b^2} \gcd(a,b)=1\implies \phi(ab)=\phi(a)\phi(b) \gcd(a,b)=1\implies\gcd\left(a,\frac{1}{b}\right)=1 \phi\left(\frac{a}{b}\right):=\frac{\phi(a)\phi(b)}{b^2} \phi\left(\frac{a}{1}\right):=\frac{\phi(a)\phi(1)}{1^2}=\phi(a) q_n \phi(q_n) \pi_n=\sum_{k=0}^n\frac{4(-1)^k}{2k+1} \pi e_n=\sum_{k=0}^n\frac{1}{k!} e \phi \pi_n e_n \phi(e_n) \phi(\pi_n) \pi\approx e 0.4 40 \varphi_n=\frac{F_{n+1}}{F_n} F_n n \phi(\varphi_n) \limsup_{n\to\infty}\phi(\varphi_n)=\varphi=\frac{1+\sqrt{5}}{2}. \liminf \limsup \sqrt{2} \phi(\sqrt{2})=2^{\frac{1}{2}-1}\phi(2)=\frac{\sqrt{2}}{2} \limsup \sqrt{2} e_n:=\left(\frac{n+1}{n}\right)^n \lim_{n\to\infty}e_n=e L=\limsup{\phi(e_n)}=\limsup_{n\to\infty}{\phi\left(\left(\frac{n+1}{n}\right)^n\right)}=\limsup\left(\frac{n+1}{n}\right)^{n-1}\frac{\phi(n)\phi(n+1)}{n^2} \implies \ln L = \limsup(n-1)\ln\left(\frac{n+1}{n}\right)+\ln\left(\frac{\phi(n)}{n}\right)+\ln\left(\frac{\phi(n+1)}{n}\right) =1+\ln 1+\ln 1 \implies L=e \limsup \phi(e_n)\approx \frac{e}{2}","['real-analysis', 'irrational-numbers', 'totient-function']"
32,Finding an example of nonhomeomorphic closed connected sets,Finding an example of nonhomeomorphic closed connected sets,,"Question: Find two closed, connected subsets in $\mathbb{R}^2$, $A$ and $B$, such that $A$ is not homeomorphic to $B$, but there is a continuous bijection $f:A \rightarrow B$ and a continuous bijection $g:B \rightarrow A$. This is a homework question, so please only very small hints. I realize that both $A$ and $B$ must not be compact. Since they both must be closed, then they must be unbounded. However, I am having a hard time getting started on this. It is very easy to find two closed, unbounded, connected subsets of the plane that are not homeomorphic to each other, but it is hard to find the continuous bijections required. I know the classic example of a continuous bijection with a discontinuous inverse is a map $f: [0,2\pi) \rightarrow \mathbb{S}^1$ given by $f(x) = (\cos x, \sin x)$. I am trying to use this map as a template to come up with my sets but I am having no success.","Question: Find two closed, connected subsets in $\mathbb{R}^2$, $A$ and $B$, such that $A$ is not homeomorphic to $B$, but there is a continuous bijection $f:A \rightarrow B$ and a continuous bijection $g:B \rightarrow A$. This is a homework question, so please only very small hints. I realize that both $A$ and $B$ must not be compact. Since they both must be closed, then they must be unbounded. However, I am having a hard time getting started on this. It is very easy to find two closed, unbounded, connected subsets of the plane that are not homeomorphic to each other, but it is hard to find the continuous bijections required. I know the classic example of a continuous bijection with a discontinuous inverse is a map $f: [0,2\pi) \rightarrow \mathbb{S}^1$ given by $f(x) = (\cos x, \sin x)$. I am trying to use this map as a template to come up with my sets but I am having no success.",,"['real-analysis', 'general-topology', 'analysis', 'examples-counterexamples']"
33,Closed form of $\sum_{n=1}^{\infty} \left(\frac{H_n}{n}\right)^4$,Closed form of,\sum_{n=1}^{\infty} \left(\frac{H_n}{n}\right)^4,"Find the closed form of $$\sum_{n=1}^{\infty} \left(\frac{H_n}{n}\right)^4$$ I know the closed form for smaller powers like $2, 3$ exists, but I'm not sure if there is a closed form  for this variant. Is it possible to tackle the question in an elementary way and find the answer, without using integrals at all? Then, if this exists, I'd also propose the alternating variant $$\sum_{n=1}^{\infty} (-1)^{n+1} \left(\frac{H_n}{n}\right)^4$$ Update (by editor): The last sum by OP is: $$\small \sum _{n=1}^{\infty } (-1)^{n-1} \left(\frac{H_n}{n}\right){}^4=\frac{633}{128} \zeta(6,2)-\frac{1}{6} \pi ^2\zeta(\bar5,1)+6 \zeta(\bar7,1)-4 \zeta(\bar5,1,\bar1,1)+6 \log ^2(2) \zeta(\bar5,1)+12 \log (2) \zeta(\bar5,1,1)-16 \text{Li}_5\left(\frac{1}{2}\right) \zeta (3)-\frac{1}{9} \pi ^4 \text{Li}_4\left(\frac{1}{2}\right)+\frac{\pi ^2 \zeta (3)^2}{8}-\frac{63 \zeta (3) \zeta (5)}{64}+\frac{2}{15} \zeta (3) \log ^5(2)-\frac{2}{9} \pi ^2 \zeta (3) \log ^3(2)-\frac{31}{12} \zeta (5) \log ^3(2)+3 \zeta (3)^2 \log ^2(2)-\frac{16}{45} \pi ^4 \zeta (3) \log (2)-\frac{65}{48} \pi ^2 \zeta (5) \log (2)+\frac{155}{8} \zeta (7) \log (2)+\frac{149141 \pi ^8}{43545600}-\frac{1}{216} \pi ^4 \log ^4(2)+\frac{25 \pi ^6 \log ^2(2)}{1512}$$","Find the closed form of I know the closed form for smaller powers like exists, but I'm not sure if there is a closed form  for this variant. Is it possible to tackle the question in an elementary way and find the answer, without using integrals at all? Then, if this exists, I'd also propose the alternating variant Update (by editor): The last sum by OP is:","\sum_{n=1}^{\infty} \left(\frac{H_n}{n}\right)^4 2, 3 \sum_{n=1}^{\infty} (-1)^{n+1} \left(\frac{H_n}{n}\right)^4 \small \sum _{n=1}^{\infty } (-1)^{n-1} \left(\frac{H_n}{n}\right){}^4=\frac{633}{128} \zeta(6,2)-\frac{1}{6} \pi ^2\zeta(\bar5,1)+6 \zeta(\bar7,1)-4 \zeta(\bar5,1,\bar1,1)+6 \log ^2(2) \zeta(\bar5,1)+12 \log (2) \zeta(\bar5,1,1)-16 \text{Li}_5\left(\frac{1}{2}\right) \zeta (3)-\frac{1}{9} \pi ^4 \text{Li}_4\left(\frac{1}{2}\right)+\frac{\pi ^2 \zeta (3)^2}{8}-\frac{63 \zeta (3) \zeta (5)}{64}+\frac{2}{15} \zeta (3) \log ^5(2)-\frac{2}{9} \pi ^2 \zeta (3) \log ^3(2)-\frac{31}{12} \zeta (5) \log ^3(2)+3 \zeta (3)^2 \log ^2(2)-\frac{16}{45} \pi ^4 \zeta (3) \log (2)-\frac{65}{48} \pi ^2 \zeta (5) \log (2)+\frac{155}{8} \zeta (7) \log (2)+\frac{149141 \pi ^8}{43545600}-\frac{1}{216} \pi ^4 \log ^4(2)+\frac{25 \pi ^6 \log ^2(2)}{1512}","['calculus', 'real-analysis', 'sequences-and-series', 'harmonic-numbers']"
34,Proof of that every interval is connected,Proof of that every interval is connected,,"I don't understand the following proof of that every interval is connected in $\mathbb{R}$. Let $Y$ be an interval in $\mathbb{R}$ and suppose that $Y$ is not connected. Then $Y=A\cup B$, where $A,B\subseteq Y$ are open in $Y$, $A,B\neq\emptyset$ and $A\cap B=\emptyset$. Let $a\in A$ and $b\in B$. Without loss of generality, $a<b$. Let $\alpha=\sup\{x\in\mathbb{R}:[a,x)\cap Y\subseteq A\}$. Then $\alpha\le b$ and $\alpha\in Y$. It is clear that $\alpha\in Cl_{Y}(A)$, and $A$ is closed in $Y$, then $\alpha\in A$. Since $A$ is open in $Y$, $Y$ is an interval, $b\in Y\setminus A$ and $\alpha <b$, then there exists $r>0$ such that $(\alpha-r,\alpha+r)\cap Y\subseteq A$. We can conclude that $[a,\alpha+r)\cap Y\subseteq A$, which is a contradiction. I don't understand why $\alpha\le b$ and $\alpha\in Y$. Can anyone explain this to me? Thanks.","I don't understand the following proof of that every interval is connected in $\mathbb{R}$. Let $Y$ be an interval in $\mathbb{R}$ and suppose that $Y$ is not connected. Then $Y=A\cup B$, where $A,B\subseteq Y$ are open in $Y$, $A,B\neq\emptyset$ and $A\cap B=\emptyset$. Let $a\in A$ and $b\in B$. Without loss of generality, $a<b$. Let $\alpha=\sup\{x\in\mathbb{R}:[a,x)\cap Y\subseteq A\}$. Then $\alpha\le b$ and $\alpha\in Y$. It is clear that $\alpha\in Cl_{Y}(A)$, and $A$ is closed in $Y$, then $\alpha\in A$. Since $A$ is open in $Y$, $Y$ is an interval, $b\in Y\setminus A$ and $\alpha <b$, then there exists $r>0$ such that $(\alpha-r,\alpha+r)\cap Y\subseteq A$. We can conclude that $[a,\alpha+r)\cap Y\subseteq A$, which is a contradiction. I don't understand why $\alpha\le b$ and $\alpha\in Y$. Can anyone explain this to me? Thanks.",,"['real-analysis', 'general-topology']"
35,Proof verification: $\sum_{n=1}^\infty \frac{1}{n^2}=\frac{\pi^2}{6}$,Proof verification:,\sum_{n=1}^\infty \frac{1}{n^2}=\frac{\pi^2}{6},"While milking the integral $\int_0^\pi\sin^{-1}\left(\sin x\right)dx$ , I believe I may have conceived of a nice proof of $$\sum_{n=1}^\infty\frac{1}{n^2}=\frac{\pi^2}{6}$$ It makes use of the established fact that an equivalent problem is to prove that $$\sum_{n=0}^\infty\frac{1}{(2n+1)^2}=\frac{\pi^2}{8}$$ Proof : since $\sum 1/n^2$ and $\sum 1/(2n+1)^2$ both converge, the chain of equalities \begin{align} \sum_{n=1}^{2k}\frac{1}{n^2} &= 1+\frac{1}{2^2}+\frac{1}{3^2}+\cdots+\frac{1}{(2k-1)^2}+\frac{1}{(2k)^2}\\ &= 1+\frac{1}{3^2}+\cdots+\frac{1}{(2(k-1)+1)^2}+\frac{1}{2^2}+\frac{1}{4^2}+\cdots+\frac{1}{(2k)^2}\\ &= \sum_{n=0}^{k-1}\frac{1}{(2n+1)^2}+\sum_{n=1}^k\frac{1}{(2n)^2}\\ &= \sum_{n=0}^{k-1}\frac{1}{(2n+1)^2}+\frac{1}{4}\sum_{n=1}^k\frac{1}{n^2} \end{align} implies that $\sum_{n=1}^\infty 1/n^2=\frac{4}{3}\sum_{n=0}^\infty 1/(2n+1)^2$ , so $$\sum_{n=1}^\infty\frac{1}{n^2}=\frac{\pi^2}{6}\iff\sum_{n=0}^\infty\frac{1}{(2n+1)^2}=\frac{\pi^2}{8}$$ I'd greatly appreciate it if someone took the time to review it! Proof : consider the integral $$\int_0^\pi\sin^{-1}\left(\sin x\right)dx$$ On one hand, \begin{align} \int_0^\pi\sin^{-1}\left(\sin x\right)dx &= \int_0^{\frac{\pi}{2}}\sin^{-1}\left(\sin x\right)dx+\int_{\frac{\pi}{2}}^\pi\sin^{-1}\left(\sin x\right)dx\\ &= \int_0^{\frac{\pi}{2}}xdx+\int_{\frac{\pi}{2}}^\pi(\pi-x)dx\\ &= \frac{1}{2}\left(\frac{\pi}{2}\right)^2+\pi\left(\pi-\frac{\pi}{2}\right)-\left[\frac{1}{2}\pi^2-\frac{1}{2}\left(\frac{\pi}{2}\right)^2\right]\\ &= \frac{\pi^2}{8}+\pi^2-\frac{\pi^2}{2}-\frac{\pi^2}{2}+\frac{\pi^2}{8}\\ &= \frac{\pi^2}{4} \end{align} On the other, the fact that $-1\leq\sin(x)\leq 1$ and $\sum_{n=0}^k\frac{(2n)!}{(2^n n!)^2(2n+1)}t^{2n+1}$ converges uniformly to $\sin^{-1}(t)$ over $[-1,1]$ as $k\to\infty$ justifies us writing \begin{align} \int_0^\pi\sin^{-1}\left(\sin x\right)dx &= \lim_{k\to\infty}\int_0^\pi\left(\sum_{n=0}^k\frac{(2n)!}{(2^n n!)^2(2n+1)}\sin^{2n+1}(x)\right)dx\\ &= \lim_{k\to\infty}\sum_{n=0}^k\frac{(2n)!}{(2^n n!)^2(2n+1)}\int_0^\pi\sin^{2n+1}(x)dx\\ &= \lim_{k\to\infty}\sum_{n=0}^k\frac{(2n)!}{(2^n n!)^2(2n+1)}\left(\int_0^{\frac{\pi}{2}}\sin^{2n+1}(x)dx+\int_{\frac{\pi}{2}}^\pi\sin^{2n+1}(x)dx\right)\\ &= \lim_{k\to\infty}\sum_{n=0}^k\frac{(2n)!}{(2^n n!)^2(2n+1)}\left(\int_0^{\frac{\pi}{2}}\sin^{2n+1}(x)dx+\int_{\frac{\pi}{2}-\frac{\pi}{2}}^{\pi-\frac{\pi}{2}}\sin^{2n+1}\left(x+\frac{\pi}{2}\right)dx\right)\\ &= \lim_{k\to\infty}\sum_{n=0}^k\frac{(2n)!}{(2^n n!)^2(2n+1)}\left(\int_0^{\frac{\pi}{2}}\sin^{2n+1}(x)dx+\int_0^{\frac{\pi}{2}}\cos^{2n+1}(x)dx\right)\\ &= \lim_{k\to\infty}\sum_{n=0}^k\frac{(2n)!}{(2^n n!)^2(2n+1)}\left(\int_0^{\frac{\pi}{2}}\sin^{2n+1}(x)dx+\int_{\frac{\pi}{2}-0}^{\frac{\pi}{2}-\frac{\pi}{2}}\cos^{2n+1}\left(\frac{\pi}{2}-x\right)dx\right)\\ &= \lim_{k\to\infty}\sum_{n=0}^k\frac{(2n)!}{(2^n n!)^2(2n+1)}\left(\int_0^{\frac{\pi}{2}}\sin^{2n+1}(x)dx+\int_0^{\frac{\pi}{2}}\sin^{2n+1}(x)dx\right)\\ &= \lim_{k\to\infty}2\sum_{n=0}^k\frac{(2n)!}{(2^n n!)^2(2n+1)}\int_0^{\frac{\pi}{2}}\sin^{2n+1}(x)dx\\ \end{align} All that remains is to evaluate $\int_0^{\frac{\pi}{2}}\sin^{2n+1}(x)dx$ . Using the reduction formula for $\int\sin^n (x)dx$ , it can be shown that for every integer $n\geq 1$ , $$\int_0^{\frac{\pi}{2}}\sin^{2n+1}(x)dx=\frac{2n}{2n+1}\int_0^{\frac{\pi}{2}}\sin^{2n-1}(x)dx$$ and, after inducting on $n$ , \begin{align} \int_0^{\frac{\pi}{2}}\sin^{2n+1}(x)dx &= \int_0^{\frac{\pi}{2}}\sin(x)dx\cdot\frac{2(1)}{2(1)+1}\cdot\frac{2(2)}{2(2)+1}\cdots\frac{2n}{2n+1}\\ &= \frac{2\cdot 4\cdots (2n)}{1\cdot 3\cdots (2n+1)}\\ &= \frac{2^n(1\cdot 2\cdots n)}{1\cdot 3\cdots(2n+1)}\\ &= \frac{2^n(1\cdot 2\cdots n)\cdot(2\cdot 4\cdots(2n))}{1\cdot 2\cdot 3\cdot 4\cdots(2n)\cdot(2n+1)}\\ &= \frac{2^n(1\cdot 2\cdots n)\cdot 2^n(1\cdot 2\cdots n)}{(2n+1)!}\\ &= \frac{\left(2^n n!\right)^2}{(2n+1)!}\\ \end{align} Substituting this expression into our limit, we get \begin{align} \int_0^\pi\sin^{-1}\left(\sin x\right)dx &= \lim_{k\to\infty}2\sum_{n=0}^k\frac{(2n)!}{(2^n n!)^2(2n+1)}\cdot\frac{\left(2^n n!\right)^2}{(2n+1)!}\\ &= \lim_{k\to\infty}2\sum_{n=0}^k\frac{(2n)!}{(2n+1)}\cdot\frac{1}{(2n)!(2n+1)}\\ &= \lim_{k\to\infty}2\sum_{n=0}^k\frac{1}{(2n+1)^2}\\ &= 2\sum_{n=0}^\infty\frac{1}{(2n+1)^2}\\ \end{align} which, together with the original result $\int_0^\pi\sin^{-1}\left(\sin x\right)dx=\frac{\pi^2}{4}$ , immediately yields $$\sum_{n=0}^\infty\frac{1}{(2n+1)^2}=\frac{1}{2}\cdot\frac{\pi^2}{4}=\frac{\pi^2}{8}$$ Q.E.D. Let me know what you think! If you identify any errors or optimizations, please don't hesitate to share them with me.","While milking the integral , I believe I may have conceived of a nice proof of It makes use of the established fact that an equivalent problem is to prove that Proof : since and both converge, the chain of equalities implies that , so I'd greatly appreciate it if someone took the time to review it! Proof : consider the integral On one hand, On the other, the fact that and converges uniformly to over as justifies us writing All that remains is to evaluate . Using the reduction formula for , it can be shown that for every integer , and, after inducting on , Substituting this expression into our limit, we get which, together with the original result , immediately yields Q.E.D. Let me know what you think! If you identify any errors or optimizations, please don't hesitate to share them with me.","\int_0^\pi\sin^{-1}\left(\sin x\right)dx \sum_{n=1}^\infty\frac{1}{n^2}=\frac{\pi^2}{6} \sum_{n=0}^\infty\frac{1}{(2n+1)^2}=\frac{\pi^2}{8} \sum 1/n^2 \sum 1/(2n+1)^2 \begin{align} \sum_{n=1}^{2k}\frac{1}{n^2} &= 1+\frac{1}{2^2}+\frac{1}{3^2}+\cdots+\frac{1}{(2k-1)^2}+\frac{1}{(2k)^2}\\
&= 1+\frac{1}{3^2}+\cdots+\frac{1}{(2(k-1)+1)^2}+\frac{1}{2^2}+\frac{1}{4^2}+\cdots+\frac{1}{(2k)^2}\\
&= \sum_{n=0}^{k-1}\frac{1}{(2n+1)^2}+\sum_{n=1}^k\frac{1}{(2n)^2}\\
&= \sum_{n=0}^{k-1}\frac{1}{(2n+1)^2}+\frac{1}{4}\sum_{n=1}^k\frac{1}{n^2}
\end{align} \sum_{n=1}^\infty 1/n^2=\frac{4}{3}\sum_{n=0}^\infty 1/(2n+1)^2 \sum_{n=1}^\infty\frac{1}{n^2}=\frac{\pi^2}{6}\iff\sum_{n=0}^\infty\frac{1}{(2n+1)^2}=\frac{\pi^2}{8} \int_0^\pi\sin^{-1}\left(\sin x\right)dx \begin{align}
\int_0^\pi\sin^{-1}\left(\sin x\right)dx &= \int_0^{\frac{\pi}{2}}\sin^{-1}\left(\sin x\right)dx+\int_{\frac{\pi}{2}}^\pi\sin^{-1}\left(\sin x\right)dx\\
&= \int_0^{\frac{\pi}{2}}xdx+\int_{\frac{\pi}{2}}^\pi(\pi-x)dx\\
&= \frac{1}{2}\left(\frac{\pi}{2}\right)^2+\pi\left(\pi-\frac{\pi}{2}\right)-\left[\frac{1}{2}\pi^2-\frac{1}{2}\left(\frac{\pi}{2}\right)^2\right]\\
&= \frac{\pi^2}{8}+\pi^2-\frac{\pi^2}{2}-\frac{\pi^2}{2}+\frac{\pi^2}{8}\\
&= \frac{\pi^2}{4}
\end{align} -1\leq\sin(x)\leq 1 \sum_{n=0}^k\frac{(2n)!}{(2^n n!)^2(2n+1)}t^{2n+1} \sin^{-1}(t) [-1,1] k\to\infty \begin{align}
\int_0^\pi\sin^{-1}\left(\sin x\right)dx &= \lim_{k\to\infty}\int_0^\pi\left(\sum_{n=0}^k\frac{(2n)!}{(2^n n!)^2(2n+1)}\sin^{2n+1}(x)\right)dx\\
&= \lim_{k\to\infty}\sum_{n=0}^k\frac{(2n)!}{(2^n n!)^2(2n+1)}\int_0^\pi\sin^{2n+1}(x)dx\\
&= \lim_{k\to\infty}\sum_{n=0}^k\frac{(2n)!}{(2^n n!)^2(2n+1)}\left(\int_0^{\frac{\pi}{2}}\sin^{2n+1}(x)dx+\int_{\frac{\pi}{2}}^\pi\sin^{2n+1}(x)dx\right)\\
&= \lim_{k\to\infty}\sum_{n=0}^k\frac{(2n)!}{(2^n n!)^2(2n+1)}\left(\int_0^{\frac{\pi}{2}}\sin^{2n+1}(x)dx+\int_{\frac{\pi}{2}-\frac{\pi}{2}}^{\pi-\frac{\pi}{2}}\sin^{2n+1}\left(x+\frac{\pi}{2}\right)dx\right)\\
&= \lim_{k\to\infty}\sum_{n=0}^k\frac{(2n)!}{(2^n n!)^2(2n+1)}\left(\int_0^{\frac{\pi}{2}}\sin^{2n+1}(x)dx+\int_0^{\frac{\pi}{2}}\cos^{2n+1}(x)dx\right)\\
&= \lim_{k\to\infty}\sum_{n=0}^k\frac{(2n)!}{(2^n n!)^2(2n+1)}\left(\int_0^{\frac{\pi}{2}}\sin^{2n+1}(x)dx+\int_{\frac{\pi}{2}-0}^{\frac{\pi}{2}-\frac{\pi}{2}}\cos^{2n+1}\left(\frac{\pi}{2}-x\right)dx\right)\\
&= \lim_{k\to\infty}\sum_{n=0}^k\frac{(2n)!}{(2^n n!)^2(2n+1)}\left(\int_0^{\frac{\pi}{2}}\sin^{2n+1}(x)dx+\int_0^{\frac{\pi}{2}}\sin^{2n+1}(x)dx\right)\\
&= \lim_{k\to\infty}2\sum_{n=0}^k\frac{(2n)!}{(2^n n!)^2(2n+1)}\int_0^{\frac{\pi}{2}}\sin^{2n+1}(x)dx\\
\end{align} \int_0^{\frac{\pi}{2}}\sin^{2n+1}(x)dx \int\sin^n (x)dx n\geq 1 \int_0^{\frac{\pi}{2}}\sin^{2n+1}(x)dx=\frac{2n}{2n+1}\int_0^{\frac{\pi}{2}}\sin^{2n-1}(x)dx n \begin{align}
\int_0^{\frac{\pi}{2}}\sin^{2n+1}(x)dx &= \int_0^{\frac{\pi}{2}}\sin(x)dx\cdot\frac{2(1)}{2(1)+1}\cdot\frac{2(2)}{2(2)+1}\cdots\frac{2n}{2n+1}\\
&= \frac{2\cdot 4\cdots (2n)}{1\cdot 3\cdots (2n+1)}\\
&= \frac{2^n(1\cdot 2\cdots n)}{1\cdot 3\cdots(2n+1)}\\
&= \frac{2^n(1\cdot 2\cdots n)\cdot(2\cdot 4\cdots(2n))}{1\cdot 2\cdot 3\cdot 4\cdots(2n)\cdot(2n+1)}\\
&= \frac{2^n(1\cdot 2\cdots n)\cdot 2^n(1\cdot 2\cdots n)}{(2n+1)!}\\
&= \frac{\left(2^n n!\right)^2}{(2n+1)!}\\
\end{align} \begin{align}
\int_0^\pi\sin^{-1}\left(\sin x\right)dx &= \lim_{k\to\infty}2\sum_{n=0}^k\frac{(2n)!}{(2^n n!)^2(2n+1)}\cdot\frac{\left(2^n n!\right)^2}{(2n+1)!}\\
&= \lim_{k\to\infty}2\sum_{n=0}^k\frac{(2n)!}{(2n+1)}\cdot\frac{1}{(2n)!(2n+1)}\\
&= \lim_{k\to\infty}2\sum_{n=0}^k\frac{1}{(2n+1)^2}\\
&= 2\sum_{n=0}^\infty\frac{1}{(2n+1)^2}\\
\end{align} \int_0^\pi\sin^{-1}\left(\sin x\right)dx=\frac{\pi^2}{4} \sum_{n=0}^\infty\frac{1}{(2n+1)^2}=\frac{1}{2}\cdot\frac{\pi^2}{4}=\frac{\pi^2}{8}","['real-analysis', 'calculus', 'sequences-and-series', 'solution-verification']"
36,Calculate $\lim_{n \to\infty}\sqrt[n]{\{\sqrt{2}\}\{2\sqrt{2}\}\{3\sqrt{2}\}\cdots\ \{n\sqrt{2}\} }$,Calculate,\lim_{n \to\infty}\sqrt[n]{\{\sqrt{2}\}\{2\sqrt{2}\}\{3\sqrt{2}\}\cdots\ \{n\sqrt{2}\} },"$$\text{Calculate :}\lim_{n \to\infty}\sqrt[n]{\{\sqrt{2}\}\{2\sqrt{2}\}\{3\sqrt{2}\}\cdots\{n\sqrt{2}\} } . $$ Note: Weyl's equidistributed criterion. The following are equivalent: $$x_n\quad\text{is equidistributed modulo 1}$$ $$\forall~ \text{continuous & 1-peridic} f: \quad\frac{1}{N}\sum_{n=1}^Nf(x_n)\rightarrow\int_0^1f $$ $$\forall~ k\in \mathbb Z^*:\quad \frac{1}{N}\sum_{n=1}^Ne^{2πikx_n}\rightarrow 0 $$ Background: Im trying to approach this problem by weyl's criterion, so my thoughts so far are: \begin{align} &\sqrt[n]{\{\sqrt{2}\}\{2\sqrt{2}\}\{3\sqrt{2}\}\cdots\{n\sqrt{2}\} }\\ &=\big(\{\sqrt{2}\}\{2\sqrt{2}\}\{3\sqrt{2}\}\cdots\{n\sqrt{2}\}\big)^{1/n}=\\ &=\exp\left(\frac{1}{n}\log\big(\{\sqrt{2}\}\{2\sqrt{2}\}\{3\sqrt{2}\}\cdots\{n\sqrt{2}\}\big) \right)=\\ &=\exp\left(\frac{1}{n}\sum_{k=1}^n \log\big(\{k\sqrt{2}\}\big)\right)\\ \end{align} So, since $\sqrt{2}$ is irrational  then it's simple to prove that the sequence $x_n=\{ n\cdot \sqrt{2}\}$ is equidistributed $\text{mod}\ 1$ . Let us define the continuous & $1$ -periodic function $f(x):=\log(x-[x])$ by the weyl's criterion we get: $$\begin{align*} \frac{1}{n}\sum_{k=1}^n \log(\{k\sqrt{2}\})&\longrightarrow\int_0^1\log(\color{black}{\underbrace{\{x\}}_{=x-[x]}})dx\\[5pt] &=\int_0^1\log(x)dx\\[5pt] &=\bigg[x\log(x)\bigg]_0^1-\int_0^1dx\\[5pt] &=-1\\[5pt] \end{align*}$$ Hence $$\lim\limits_{n \to\infty}\sqrt[n]{\{\sqrt{2}\}\{2\sqrt{2}\}\{3\sqrt{2}\}\cdots\{n\sqrt{2}\} }=e^{-1}  $$ Is there something wrong? Also, can we find this limit with some other way? Let me know, thank you.","Note: Weyl's equidistributed criterion. The following are equivalent: Background: Im trying to approach this problem by weyl's criterion, so my thoughts so far are: So, since is irrational  then it's simple to prove that the sequence is equidistributed . Let us define the continuous & -periodic function by the weyl's criterion we get: Hence Is there something wrong? Also, can we find this limit with some other way? Let me know, thank you.","\text{Calculate :}\lim_{n \to\infty}\sqrt[n]{\{\sqrt{2}\}\{2\sqrt{2}\}\{3\sqrt{2}\}\cdots\{n\sqrt{2}\} } .  x_n\quad\text{is equidistributed modulo 1} \forall~ \text{continuous & 1-peridic} f: \quad\frac{1}{N}\sum_{n=1}^Nf(x_n)\rightarrow\int_0^1f  \forall~ k\in \mathbb Z^*:\quad \frac{1}{N}\sum_{n=1}^Ne^{2πikx_n}\rightarrow 0  \begin{align}
&\sqrt[n]{\{\sqrt{2}\}\{2\sqrt{2}\}\{3\sqrt{2}\}\cdots\{n\sqrt{2}\} }\\
&=\big(\{\sqrt{2}\}\{2\sqrt{2}\}\{3\sqrt{2}\}\cdots\{n\sqrt{2}\}\big)^{1/n}=\\
&=\exp\left(\frac{1}{n}\log\big(\{\sqrt{2}\}\{2\sqrt{2}\}\{3\sqrt{2}\}\cdots\{n\sqrt{2}\}\big) \right)=\\
&=\exp\left(\frac{1}{n}\sum_{k=1}^n \log\big(\{k\sqrt{2}\}\big)\right)\\
\end{align} \sqrt{2} x_n=\{ n\cdot \sqrt{2}\} \text{mod}\ 1 1 f(x):=\log(x-[x]) \begin{align*}
\frac{1}{n}\sum_{k=1}^n \log(\{k\sqrt{2}\})&\longrightarrow\int_0^1\log(\color{black}{\underbrace{\{x\}}_{=x-[x]}})dx\\[5pt]
&=\int_0^1\log(x)dx\\[5pt]
&=\bigg[x\log(x)\bigg]_0^1-\int_0^1dx\\[5pt]
&=-1\\[5pt]
\end{align*} \lim\limits_{n \to\infty}\sqrt[n]{\{\sqrt{2}\}\{2\sqrt{2}\}\{3\sqrt{2}\}\cdots\{n\sqrt{2}\} }=e^{-1}  ","['real-analysis', 'calculus', 'limits', 'uniform-distribution', 'ergodic-theory']"
37,Function defined as a limit,Function defined as a limit,,"Q. If $f(x)=\lim\limits_{n\to\infty}\dfrac{\log(2+x)-x^{2n}\sin(x)}{1+x^{2n}}$, then explain why the function does not vanish anywhere in the interval $[0,\pi/2]$, although $f(0)$ and $f(\pi/2)$ differ in sign. My solution: First, we simplify the limit a bit. $$f(x)=\lim_{n\to\infty}\frac{\log(2+x)-x^{2n}\sin(x)}{1+x^{2n}}=\lim_{n\to\infty}\frac{\log(2+x)-(x^{2n}+1-1)\sin(x)}{1+x^{2n}}\\ \implies f(x)=\lim_{n\to\infty}\frac{\log(2+x)+\sin(x)}{1+x^{2n}}-\sin(x)$$ Now, we divide the interval $[0,\pi/2]$ into three parts, viz ., $x\in [0,1)$, $x=1$ and $x\in (1,\pi/2]$. For the first part, i.e., when $x\in [0,1)$, we have $|x|\lt 1$ and hence $1+x^{2n}\to 1+0=1$ as $n\to\infty$, hence, by the quotient rule of limits, it reduces to, $$f(x)=\log(2+x)+\sin(x)-\sin(x)=\log(2+x)~\forall~x\in [0,1)$$ At $x=1$, we have $f(x)=\lim\limits_{n\to\infty}\frac{\log3+\sin(1)}{1+1^{2n}}-\sin(1)=\frac 12(\log3-\sin1)$ For $x\in (1,\pi/2]$, we have $|x|\gt 1$, hence $1+x^{2n}\to\infty$ as $n\to\infty$. But $\log(2+x)$ and $\sin(x)$ are finite constants for a particular $x$. Hence, the limit goes to $0$ as $n\to\infty$ and we get $f(x)=0-\sin(x)=-\sin(x)~\forall~x\in (1,\pi/2]$. So, we write our results collectively as, $$f(x)=\begin{cases}\begin{align}\log(2+x)&&\forall~x\in [0,1)\\ \frac 12(\log3-\sin1)&&\textrm{at }x=1\\ -\sin(x)&&\forall~x\in (1,\pi/2]\end{align}\end{cases}$$ It's easy to verify now that $f(x)$ doesn't vanish in $[0,\pi/2]$ since, for $x\in [0,1)$, we have $f(x)=\log(2+x)\gt \log1=0$ since logarithm is strictly increasing. At $x=1$, the function value is obviously not $0$ since $\log3\neq\sin1$ and for $x\in (1,\pi/2]$, we have $f(x)=-\sin(x)\in [-1,-\sin 1)$ since sine is strictly increasing in $[0,\pi/2]$. Now, the explanation behind why $f(x)$ doesn't vanish even when $f(0)$ and $f(\pi/2)$ differ in sign would be that $f(x)$ isn't continuous on $[0,\pi/2]$, more specifically it's discontinuous at $x=1$ with left hand limit being $\log3$ and right hand limit being $-\sin1$ and hence Bolzano's theorem isn't applicable for $f(x)$ on $[0,\pi/2]$. Comments about my solution and improvements are welcome. Thanks! :)","Q. If $f(x)=\lim\limits_{n\to\infty}\dfrac{\log(2+x)-x^{2n}\sin(x)}{1+x^{2n}}$, then explain why the function does not vanish anywhere in the interval $[0,\pi/2]$, although $f(0)$ and $f(\pi/2)$ differ in sign. My solution: First, we simplify the limit a bit. $$f(x)=\lim_{n\to\infty}\frac{\log(2+x)-x^{2n}\sin(x)}{1+x^{2n}}=\lim_{n\to\infty}\frac{\log(2+x)-(x^{2n}+1-1)\sin(x)}{1+x^{2n}}\\ \implies f(x)=\lim_{n\to\infty}\frac{\log(2+x)+\sin(x)}{1+x^{2n}}-\sin(x)$$ Now, we divide the interval $[0,\pi/2]$ into three parts, viz ., $x\in [0,1)$, $x=1$ and $x\in (1,\pi/2]$. For the first part, i.e., when $x\in [0,1)$, we have $|x|\lt 1$ and hence $1+x^{2n}\to 1+0=1$ as $n\to\infty$, hence, by the quotient rule of limits, it reduces to, $$f(x)=\log(2+x)+\sin(x)-\sin(x)=\log(2+x)~\forall~x\in [0,1)$$ At $x=1$, we have $f(x)=\lim\limits_{n\to\infty}\frac{\log3+\sin(1)}{1+1^{2n}}-\sin(1)=\frac 12(\log3-\sin1)$ For $x\in (1,\pi/2]$, we have $|x|\gt 1$, hence $1+x^{2n}\to\infty$ as $n\to\infty$. But $\log(2+x)$ and $\sin(x)$ are finite constants for a particular $x$. Hence, the limit goes to $0$ as $n\to\infty$ and we get $f(x)=0-\sin(x)=-\sin(x)~\forall~x\in (1,\pi/2]$. So, we write our results collectively as, $$f(x)=\begin{cases}\begin{align}\log(2+x)&&\forall~x\in [0,1)\\ \frac 12(\log3-\sin1)&&\textrm{at }x=1\\ -\sin(x)&&\forall~x\in (1,\pi/2]\end{align}\end{cases}$$ It's easy to verify now that $f(x)$ doesn't vanish in $[0,\pi/2]$ since, for $x\in [0,1)$, we have $f(x)=\log(2+x)\gt \log1=0$ since logarithm is strictly increasing. At $x=1$, the function value is obviously not $0$ since $\log3\neq\sin1$ and for $x\in (1,\pi/2]$, we have $f(x)=-\sin(x)\in [-1,-\sin 1)$ since sine is strictly increasing in $[0,\pi/2]$. Now, the explanation behind why $f(x)$ doesn't vanish even when $f(0)$ and $f(\pi/2)$ differ in sign would be that $f(x)$ isn't continuous on $[0,\pi/2]$, more specifically it's discontinuous at $x=1$ with left hand limit being $\log3$ and right hand limit being $-\sin1$ and hence Bolzano's theorem isn't applicable for $f(x)$ on $[0,\pi/2]$. Comments about my solution and improvements are welcome. Thanks! :)",,"['real-analysis', 'limits', 'proof-verification']"
38,Can you please check my Cesaro means proof [duplicate],Can you please check my Cesaro means proof [duplicate],,This question already has answers here : On Cesàro convergence: If $x_n \to x$ then $z_n = \frac{x_1 + \dots +x_n}{n} \to x$ (3 answers) Closed 6 years ago . I wanted to prove the following: if $x_n \to x$ then $y_n \to x$ where $$ y_n = {x_1 + \dots + x_n \over n}$$ Please can you tell me if my proof is correct? My proof is this: Let $\varepsilon > 0$. Fix $N$ such that $n > N$ implies $|x_n - x| < {\varepsilon \over 2}$. Then $\left |{1 \over n} \sum_{k=N+1}^{N + n} x_k - x \right | < {\varepsilon \over 2}$. Now let $M$ be such that ${|x_1 + \dots + x_N | \over M} < {\varepsilon \over 2}$ and $M > N$. Then  $$ \left | \sum_{k=1}^M {x_k \over M} - x   \right | \le  \left | \sum_{k=1}^N {x_k \over M}   \right | +  \left | \sum_{k=N+1}^M {x_k \over |M-N|}  - x   \right | < \varepsilon $$ Here the proof is finished. But one can observe: It is possible that $y_n$ converges even if $x_n$ doesn't: If $x_{2n} = 0$ and $x_{2n + 1} = 1$ then $x_n$ does not converge but $y_n \to {1 \over 2}$.,This question already has answers here : On Cesàro convergence: If $x_n \to x$ then $z_n = \frac{x_1 + \dots +x_n}{n} \to x$ (3 answers) Closed 6 years ago . I wanted to prove the following: if $x_n \to x$ then $y_n \to x$ where $$ y_n = {x_1 + \dots + x_n \over n}$$ Please can you tell me if my proof is correct? My proof is this: Let $\varepsilon > 0$. Fix $N$ such that $n > N$ implies $|x_n - x| < {\varepsilon \over 2}$. Then $\left |{1 \over n} \sum_{k=N+1}^{N + n} x_k - x \right | < {\varepsilon \over 2}$. Now let $M$ be such that ${|x_1 + \dots + x_N | \over M} < {\varepsilon \over 2}$ and $M > N$. Then  $$ \left | \sum_{k=1}^M {x_k \over M} - x   \right | \le  \left | \sum_{k=1}^N {x_k \over M}   \right | +  \left | \sum_{k=N+1}^M {x_k \over |M-N|}  - x   \right | < \varepsilon $$ Here the proof is finished. But one can observe: It is possible that $y_n$ converges even if $x_n$ doesn't: If $x_{2n} = 0$ and $x_{2n + 1} = 1$ then $x_n$ does not converge but $y_n \to {1 \over 2}$.,,"['real-analysis', 'sequences-and-series', 'proof-verification']"
39,Can the chain rule be relaxed to allow one of the functions to not be defined on an open set?,Can the chain rule be relaxed to allow one of the functions to not be defined on an open set?,,"I've written the question first, then the motivation behind it and lastly some background. Note that the question makes references to definitions and theorems written in the background bit at the end. The short description of why I'm asking this question is in there in case anyone is interested/has something to say about it (e.g., that I'm going wrong somewhere). Thank you very much in advanced. Is it possible to modify Theorem 3 such that the premise reads ""Suppose $E$ is an open set in $\mathbb{R}^n$, $f$ maps $E$ into $\mathbb{R}^m$, $f$ is differentiable at $x\in E$, $g$ maps (1) containing $f(E)$ into $\mathbb{R}^k$, and $g$ is (2) at $f(x)$"" and the conclusion reads ""$F(x)=g(f(x))$ is differentiable at $x$ and $F'(x)=(3)f'(x).$"" where (1) = the non-negative orthant of $\mathbb{R}^n$, that is $\{x\in\mathbb{R}^n:x_i\geq 0,\quad i=1,2,\dots,n\}$ (or some type of set that encompasses the non-negative orthant, for example, closed, convex, etc.). (2) = some criterion analogous to differentiable but that can be defined functions that are not defined on open subsets of $\mathbb{R}^n$ (see Definition 1). (3) = some function evaluated at $f(x)$ analogous to the derivative $g'$ (see Definition 1) of $g$ evaluated at the point $f(x)$. If so, What are (1), (2), and (3)? Does there exists some theorems analogous to Theorem 1 and 2 that provide us with an easy way to test for (2) and compute (3)? Otherwise how does one check for (2) and computes (3)? Alternatively, if there is a reason one cannot do the above, what is it? Motivation I'm looking at initial value problems of the type $$\dot{x}=f(x),\quad\quad x(0)=x_0$$ where $x_0$ is in the non-negative orthant of $\mathbb{R}^n$, $\{x\in\mathbb{R}^n:x_i\geq 0,\quad i=1,2,\dots,n\}$ and $f$ maps from the non-negative orthant to $\mathbb{R}^n$. I'm looking at the above because I'm would like to be able to evaluate the time derivative of a Lyapunov function $V$ which is only defined on the non-negative orthant much in the same fashion one usually does, by using the chain rule: $$\dot{V}(x(t))=\frac{\partial V}{\partial x}(x(t))\dot{x}(t).$$ Background If $x\in\mathbb{R}^n$, let $|x|$ denote the euclidean norm on $\mathbb{R}^n$ evaluated at $x$. Definition 1 (Differentiable): Suppose $E$ is an open set in $\mathbb{R}^n$, $f$ is a function that maps $E$ into $\mathbb{R}^m$, and $x\in E$. If there exists a linear transformation $A$ from $\mathbb{R}^n$ to $\mathbb{R}^m$ such that $$\lim_{h\rightarrow 0}\frac{|f(x+h)-f(x)-Ah|}{|h|}=0$$ then we say that $f$ is differentiable at $x$, and we write $$f'(x)=A.$$ If $f$ is differentiable at every $x\in E$ we say that $f$ is differentiable in $E$. In the above, it was implicitly assumed that $h\in\mathbb{R}^n$. Since $E$ is open, if $|h|$ is small enough then $x+h\in E$, thus the above limit makes sense. If we know a priori that $f$ is differentiable at a point $x$, then we can use the following handy theorem to find $f'(x)$. Theorem 1: Suppose $f$ maps an open set $E\subset\mathbb{R}^n$ into $\mathbb{R}^m$ and $f$ is differentiable at a point $x\in E$. Then the partial derivatives $$\frac{\partial f_i}{\partial x_j}(x) := \lim_{t\rightarrow0}\frac{f_i(x+te_j)-f_i(x)}{t},$$ where $e_j$ denotes the $j^{th}$ vector of the standard basis of $\mathbb{R}^n$, exist. Furthermore, using the standard bases of $\mathbb{R}^n$ and $\mathbb{R}^m$, $$f'(x)=\left[\frac{\partial f}{\partial x}(x)\right],$$ where the RHS denotes the matrix whose $i,j$ entry is given by the partial derivative defined above. However, to be able to use the above we first need to be able to conclude that $f$ is differentiable. If $f$ is continuously differentiable we can also use the partial derivatives to this end. Definition 2 (Continuously differentiable): We say that a differentiable function $f$ that maps from an open subset $E\subset \mathbb{R}^n$ into $\mathbb{R}^m$ is continuously differentiable if $f'$ is a continuous mapping of $E$ into the set of linear functions that map from $\mathbb{R}^n$ to $\mathbb{R}^m$, $L(\mathbb{R}^n,\mathbb{R}^m)$ (for example, defining continuity using some induced matrix norm to define a metric on $L(\mathbb{R}^n,\mathbb{R}^m))$. Theorem 2: Suppose that $f$ maps an open set $E\subset \mathbb{R}^n$ into $\mathbb{R}^m$. If $f$ is continuously differentiable if and only if all the partial derivatives of $f$ exist and are on continuous on $E$. One final result, the chain rule: Theorem 3: Suppose $E$ is an open set in $\mathbb{R}^n$, $f$ maps $E$ into $\mathbb{R}^m$, $f$ is differentiable at $x\in E$, $g$ maps an open set containing $f(E)$ into $\mathbb{R}^k$, and $g$ is differentiable at $f(x)$. Then the mapping of $E$ into $\mathbb{R}^k$ defined by $$F(x)=g(f(x))$$ is differentiable at $x$ and $$F'(x)=g'(f(x))f'(x).$$","I've written the question first, then the motivation behind it and lastly some background. Note that the question makes references to definitions and theorems written in the background bit at the end. The short description of why I'm asking this question is in there in case anyone is interested/has something to say about it (e.g., that I'm going wrong somewhere). Thank you very much in advanced. Is it possible to modify Theorem 3 such that the premise reads ""Suppose $E$ is an open set in $\mathbb{R}^n$, $f$ maps $E$ into $\mathbb{R}^m$, $f$ is differentiable at $x\in E$, $g$ maps (1) containing $f(E)$ into $\mathbb{R}^k$, and $g$ is (2) at $f(x)$"" and the conclusion reads ""$F(x)=g(f(x))$ is differentiable at $x$ and $F'(x)=(3)f'(x).$"" where (1) = the non-negative orthant of $\mathbb{R}^n$, that is $\{x\in\mathbb{R}^n:x_i\geq 0,\quad i=1,2,\dots,n\}$ (or some type of set that encompasses the non-negative orthant, for example, closed, convex, etc.). (2) = some criterion analogous to differentiable but that can be defined functions that are not defined on open subsets of $\mathbb{R}^n$ (see Definition 1). (3) = some function evaluated at $f(x)$ analogous to the derivative $g'$ (see Definition 1) of $g$ evaluated at the point $f(x)$. If so, What are (1), (2), and (3)? Does there exists some theorems analogous to Theorem 1 and 2 that provide us with an easy way to test for (2) and compute (3)? Otherwise how does one check for (2) and computes (3)? Alternatively, if there is a reason one cannot do the above, what is it? Motivation I'm looking at initial value problems of the type $$\dot{x}=f(x),\quad\quad x(0)=x_0$$ where $x_0$ is in the non-negative orthant of $\mathbb{R}^n$, $\{x\in\mathbb{R}^n:x_i\geq 0,\quad i=1,2,\dots,n\}$ and $f$ maps from the non-negative orthant to $\mathbb{R}^n$. I'm looking at the above because I'm would like to be able to evaluate the time derivative of a Lyapunov function $V$ which is only defined on the non-negative orthant much in the same fashion one usually does, by using the chain rule: $$\dot{V}(x(t))=\frac{\partial V}{\partial x}(x(t))\dot{x}(t).$$ Background If $x\in\mathbb{R}^n$, let $|x|$ denote the euclidean norm on $\mathbb{R}^n$ evaluated at $x$. Definition 1 (Differentiable): Suppose $E$ is an open set in $\mathbb{R}^n$, $f$ is a function that maps $E$ into $\mathbb{R}^m$, and $x\in E$. If there exists a linear transformation $A$ from $\mathbb{R}^n$ to $\mathbb{R}^m$ such that $$\lim_{h\rightarrow 0}\frac{|f(x+h)-f(x)-Ah|}{|h|}=0$$ then we say that $f$ is differentiable at $x$, and we write $$f'(x)=A.$$ If $f$ is differentiable at every $x\in E$ we say that $f$ is differentiable in $E$. In the above, it was implicitly assumed that $h\in\mathbb{R}^n$. Since $E$ is open, if $|h|$ is small enough then $x+h\in E$, thus the above limit makes sense. If we know a priori that $f$ is differentiable at a point $x$, then we can use the following handy theorem to find $f'(x)$. Theorem 1: Suppose $f$ maps an open set $E\subset\mathbb{R}^n$ into $\mathbb{R}^m$ and $f$ is differentiable at a point $x\in E$. Then the partial derivatives $$\frac{\partial f_i}{\partial x_j}(x) := \lim_{t\rightarrow0}\frac{f_i(x+te_j)-f_i(x)}{t},$$ where $e_j$ denotes the $j^{th}$ vector of the standard basis of $\mathbb{R}^n$, exist. Furthermore, using the standard bases of $\mathbb{R}^n$ and $\mathbb{R}^m$, $$f'(x)=\left[\frac{\partial f}{\partial x}(x)\right],$$ where the RHS denotes the matrix whose $i,j$ entry is given by the partial derivative defined above. However, to be able to use the above we first need to be able to conclude that $f$ is differentiable. If $f$ is continuously differentiable we can also use the partial derivatives to this end. Definition 2 (Continuously differentiable): We say that a differentiable function $f$ that maps from an open subset $E\subset \mathbb{R}^n$ into $\mathbb{R}^m$ is continuously differentiable if $f'$ is a continuous mapping of $E$ into the set of linear functions that map from $\mathbb{R}^n$ to $\mathbb{R}^m$, $L(\mathbb{R}^n,\mathbb{R}^m)$ (for example, defining continuity using some induced matrix norm to define a metric on $L(\mathbb{R}^n,\mathbb{R}^m))$. Theorem 2: Suppose that $f$ maps an open set $E\subset \mathbb{R}^n$ into $\mathbb{R}^m$. If $f$ is continuously differentiable if and only if all the partial derivatives of $f$ exist and are on continuous on $E$. One final result, the chain rule: Theorem 3: Suppose $E$ is an open set in $\mathbb{R}^n$, $f$ maps $E$ into $\mathbb{R}^m$, $f$ is differentiable at $x\in E$, $g$ maps an open set containing $f(E)$ into $\mathbb{R}^k$, and $g$ is differentiable at $f(x)$. Then the mapping of $E$ into $\mathbb{R}^k$ defined by $$F(x)=g(f(x))$$ is differentiable at $x$ and $$F'(x)=g'(f(x))f'(x).$$",,"['real-analysis', 'analysis']"
40,Analytic form of: $ \int \frac{\bigl[\cos^{-1}(x)\sqrt{1-x^2}\bigr]^{-1}}{\ln\bigl( 1+\sin(2x\sqrt{1-x^2})/\pi\bigr)} dx $,Analytic form of:, \int \frac{\bigl[\cos^{-1}(x)\sqrt{1-x^2}\bigr]^{-1}}{\ln\bigl( 1+\sin(2x\sqrt{1-x^2})/\pi\bigr)} dx ,"Background: On my quest to solve difficult integrals, I chanced upon this site: http://www.durofy.com/5-most-beautiful-questions-from-integral-calculus/ Good problems for me, (novice), although I believe these integrals maybe easy for others. I'm having problems with this integral :- $$ \int \frac{\left[\cos^{-1}(x)\left(\sqrt{1-x^2}\right)\right]^{-1}}{\ln\left( 1+\frac{\sin(2x\sqrt{1-x^2})}{\pi}\right)} dx $$ My effort : I tried to convert $\sqrt{1-x^2}$ into '$ \cos(\arcsin x) $' and then simplify, but I couldn't do it. Then I tried to convert it to '$\sin(\arccos x)$', but it just made it worse. Edit : As Lucian suggested in the comments, I came this far - $$ \int \frac{-1}{t\ln\left( 1+\frac{\sin(\sin(2t))}{\pi}\right)} dx $$ I would've thought of doing this, if the 'primary sine function' in the denominator was an 'arcsin function'. Problem is, I'm not able to proceed. What do I do? Question: What is this integral's analytic form? What is the underlying trick/substitution/concept needed to solve this integral? Note: A non-closed form solution may also exist; I don't know about that. If you do manage to evaluate this integral in terms of even special functions including Bessel, Gamma or Faddeeva, it's okay; you can post it.","Background: On my quest to solve difficult integrals, I chanced upon this site: http://www.durofy.com/5-most-beautiful-questions-from-integral-calculus/ Good problems for me, (novice), although I believe these integrals maybe easy for others. I'm having problems with this integral :- $$ \int \frac{\left[\cos^{-1}(x)\left(\sqrt{1-x^2}\right)\right]^{-1}}{\ln\left( 1+\frac{\sin(2x\sqrt{1-x^2})}{\pi}\right)} dx $$ My effort : I tried to convert $\sqrt{1-x^2}$ into '$ \cos(\arcsin x) $' and then simplify, but I couldn't do it. Then I tried to convert it to '$\sin(\arccos x)$', but it just made it worse. Edit : As Lucian suggested in the comments, I came this far - $$ \int \frac{-1}{t\ln\left( 1+\frac{\sin(\sin(2t))}{\pi}\right)} dx $$ I would've thought of doing this, if the 'primary sine function' in the denominator was an 'arcsin function'. Problem is, I'm not able to proceed. What do I do? Question: What is this integral's analytic form? What is the underlying trick/substitution/concept needed to solve this integral? Note: A non-closed form solution may also exist; I don't know about that. If you do manage to evaluate this integral in terms of even special functions including Bessel, Gamma or Faddeeva, it's okay; you can post it.",,"['calculus', 'real-analysis', 'integration', 'indefinite-integrals', 'closed-form']"
41,Show that there are no real solutions of $1 + \sum_{n = 1}^{\infty} \frac{x^n}{\prod_{k=1}^{n} H_k} = 0$ where $H_k = \sum_{i=1}^{k} \frac{1}{i}.$,Show that there are no real solutions of  where,1 + \sum_{n = 1}^{\infty} \frac{x^n}{\prod_{k=1}^{n} H_k} = 0 H_k = \sum_{i=1}^{k} \frac{1}{i}.,"EDIT : I found an alternative proof which seems correct. You do not have to read until bold letters since they are wrong. I recommend reading from ""New Solution"". Show that there are no real solutions of $$1 + \sum_{n = 1}^{\infty} \frac{x^n}{\prod_{k=1}^{n} H_k} = 0$$ where $$H_k = \sum_{i=1}^{k} \frac{1}{i}.$$ I managed to prove this, and I want to know if my proof is correct, and if there are any other (better) ways of proving it. (This is my first question and my English may be incorrect since English is not my first language.) The overall proof is proving by contradiction : let $$f(x) = 1 + \sum_{n = 1}^{\infty} \frac{x^n}{\prod_{k=1}^{n} H_k}$$ and suppose a real number $a$ such that $f(a) = 0$ . Since $f(x) \geq 1$ for $x \geq 0$ , it is trivial that $a \lt 0$ . Just a little distribution : $$\begin{align} f(x) &= 1 + \sum_{n = 1}^{\infty} \frac{x^n}{\prod_{k=1}^{n} H_k} = 1 + \sum_{n = 1}^{\infty} \left\{ \frac{x^{2n-1}}{\prod_{k=1}^{2n-1} H_k} + \frac{x^{2n}}{\prod_{k=1}^{2n} H_k}\right\} \\ \\ &= 1 + \sum_{n=1}^{\infty} \frac{x^{2n} + H_{2n} x^{2n-1}}{\prod_{k=1}^{2n} H_k} \end{align}$$ and $$e^x = \sum_{n=0}^{\infty} \frac{x^n}{n!} = 1 + \sum_{n=1}^{\infty} \left\{ \frac{x^{2n-1}}{(2n-1)!} + \frac{x^{2n}}{(2n)!} \right\} = 1 + \sum_{n=1}^{\infty} \frac{x^{2n} + 2nx^{2n-1}}{(2n)!} .$$ Meanwhile, $$H_k = \frac{1}{1} + \frac{1}{2} + \frac{1}{3} + \cdots + \frac{1}{k} \lt k$$ and $$\prod_{k=1}^{2n} H_k \lt (2n)!.$$ Since $a \in \mathbb{R^-}$ , $$a^{2n} + H_{2n} a^{2n-1} \gt a^{2n} + 2na^{2n-1}.$$ Therefore, $$0 = f(a) = 1 + \sum_{n=1}^{\infty} \frac{a^{2n} + H_{2n} a^{2n-1}}{\prod_{k=1}^{2n} H_k}$$ $$\gt 1 + \sum_{n=1}^{\infty} \frac{a^{2n} + 2na^{2n-1}}{(2n)!} = e^a \gt 0$$ and it is a contradiction. Edit : According to the comments, I found out that the following identity can not be concluded right away since $a^{2n} + H_{2n} a^{2n-1}$ may be negative for some $a$ . So now we have to show the whole identity for all $n \in \mathbb{N}$ and $a \in \mathbb{R^-}$ such that $f(a) = 0$ : $$ \frac{a^{2n} + H_{2n} a^{2n-1}}{\prod_{k=1}^{2n} H_k} \gt \frac{a^{2n} + 2na^{2n-1}}{(2n)!}$$ or prove a weaker statement : $$ \frac{a^{2n} + H_{2n} a^{2n-1}}{\prod_{k=1}^{2n} H_k} \gt \frac{(a \ln t)^{2n} + 2n(a \ln t)^{2n-1}}{(2n)!}$$ where $t \gt 0$ since the contradiction still holds if $f(a) \gt t^a.$ Since $a^{2n-1} < 0$ , the inequality we want to show is equivalent to the following inequalities : $$\frac{a + H_{2n}}{\prod_{k=1}^{2n} H_k} < \frac{a (\ln t)^{2n} + 2n (\ln t)^{2n-1}}{(2n)!},$$ $$a < \frac{2n \cdot \prod_{k=1}^{2n} H_k \cdot (\ln t)^{2n-1} - (2n)! \cdot H_{2n}}{(2n)! - (\ln t)^{2n} \cdot \prod_{k=1}^{2n} H_k}$$ for all $a \in \mathbb{R^-}$ such that $f(a) = 0.$ My further attempt was trying to show that there exists $t \in \mathbb{R^+}$ such that $$\frac{2n \cdot \prod_{k=1}^{2n} H_k \cdot (\ln t)^{2n-1} - (2n)! \cdot H_{2n}}{(2n)! - (\ln t)^{2n} \cdot \prod_{k=1}^{2n} H_k} \geq 0$$ for all $n \in \mathbb{N}.$ Suppose $t < e$ for a weaker statement, then $\ln t < 1$ and $$(2n)! - (\ln t)^{2n} \cdot \prod_{k=1}^{2n} H_k > (2n)! - \prod_{k=1}^{2n} H_k > 0,$$ and we get to prove: $$2n \cdot \prod_{k=1}^{2n} H_k \cdot (\ln t)^{2n-1} - (2n)! \cdot H_{2n} \geq 0$$ or $$\ln t \geq \left\{ \frac{(2n-1)!}{\prod_{k=1}^{2n-1} H_k} \right\}^{\frac{1}{2n-1}} =: F(n).$$ If there exists $M \in \mathbb{R}$ such that $F(n) < M$ for all $n \in \mathbb{N}$ , such $t \in \mathbb{R^+}$ will exist and the inequality above will hold. I am now struggling to prove this with the identities : $$\ln n + \frac{1}{n} < H_n < \ln n + 1$$ for $n \in \mathbb{N}$ , and $$\ln k = \sum_{n=1}^{\infty} \frac{1}{n} \cdot \left( \frac{k - 1}{k} \right)^n$$ for $\frac{1}{2} < k \in \mathbb{R}.$ Bad News : The function $F(n)$ seems to diverge when $n \to \infty$ , according to here . New Solution Note that $f$ is defined for all $x \in \mathbb{R}$ and if $f(a) = 0$ for $a \in \mathbb{R}$ then $a < 0$ . Assume $a < 0$ exists. For $n \in \mathbb{N}$ , $$\begin{align} \int_{a}^{0} \frac{x^{n}-a^{n}}{x-a} dx &= \int_{a}^{0} (x^{n-1} + ax^{x-2} + \cdots + a^{n-1}) dx \\ \\ &= \left[ \sum_{k=0}^{n-1} \frac{a^{k}}{n-k} x^{n-k} \right]_{a}^{0} = - a^{n} \sum_{k=1}^{n} \frac{1}{k} = - a^{n} H_{n} \end{align}$$ Using this result, $$\begin{align} \int_{a}^{0} \frac{f(x)}{x-a} dx &= \int_{a}^{0} \frac{f(x) - f(a)}{x-a} dx \\ \\ &= \int_{a}^{0} \frac{1}{x-a} \left[ \sum_{n=1}^{\infty} \frac{x^{n}}{H_{1} H_{2} \cdots H_{n}} - \sum_{n=1}^{\infty} \frac{a^{n}}{H_{1} H_{2} \cdots H_{n}} \right] dx \\ \\ &= \int_{a}^{0} \frac{1}{x-a} \left[ \sum_{n=1}^{\infty} \frac{x^{n} - a^{n}}{H_{1} H_{2} \cdots H_{n}} \right] dx \\ \\ &= \sum_{n=1}^{\infty} \frac{1}{H_{1} H_{2} \cdots H_{n}} \int_{a}^{0} \frac{x^{n} - a^{n}}{x-a} dx \\ \\ &= - \sum_{n=1}^{\infty} \frac{a^{n} H_{n}}{H_{1} H_{2} \cdots H_{n}} \\ \\ &= -a - \sum_{n=2}^{\infty} \frac{a^{n} H_{n}}{H_{1} H_{2} \cdots H_{n}} \\ \\ &= -a - a \sum_{n=2}^{\infty} \frac{a^{n-1}}{H_{1} H_{2} \cdots H_{n-1}} \\ \\ &= -a - a \sum_{n=1}^{\infty} \frac{a^{n}}{H_{1} H_{2} \cdots H_{n}} \\ \\ &= - a \cdot \left[ 1 + \sum_{n=1}^{\infty} \frac{a^{n}}{H_{1} H_{2} \cdots H_{n}} \right] \\ \\ &= - a f(a) \\ \\ &= 0 \end{align}$$ Since $f(0) = 1$ , $f(a) = 0$ and $\int_{a}^{0} \frac{f(x)}{x-a} dx = 0$ , there exists real number $a_{1} \in (a, 0)$ such that $f(a_{1}) = 0$ . (If not, then it is a contradiction since $f \ge 0$ on $(a, 0)$ , not $f = 0$ everywhere since $f(0) = 1$ , but $\int_{a}^{0} \frac{f(x)}{x-a} dx = 0$ .) Similarly, we can find $a_{n+1} \in (a_{n}, 0)$ such that $f(a_{n+1}) = 0$ . Now there are two ways to prove that this is impossible. First, since $f$ is an entire function, its zero-set should not have accumulation points. However, we can find infinite zeros in the interval $(a, 0)$ and it is a contradiction. Second, let the (partial) zero-set of $f$ be $S$ such that $S \subseteq (a, 0)$ . Since $S$ is bounded above, there exists $s := \sup S$ . Now we can find a sequence $\{ s_{n} \}_{n \in \mathbb{N}}$ in $S$ , that converges to $s$ . Since $f$ is continuous, $0 = \lim_{n \to \infty} f(s_{n}) = f(s)$ . Since $s$ is a zero of $f$ , there exists $s' \in (s, 0)$ such that $f(s') = 0$ . Since $s' \in S$ , if $s \ne 0$ then it contradicts the fact that $s = \sup S$ . If $s = 0$ , $f(s) = f(0) = 0$ because $s$ is a zero of $f$ . However, it is trivial that $f(0) = 1$ and it is a contradiction. Therefore, if $f(z) = 0$ then $z \in \mathbb{C} \, \backslash \, \mathbb{R}$ .","EDIT : I found an alternative proof which seems correct. You do not have to read until bold letters since they are wrong. I recommend reading from ""New Solution"". Show that there are no real solutions of where I managed to prove this, and I want to know if my proof is correct, and if there are any other (better) ways of proving it. (This is my first question and my English may be incorrect since English is not my first language.) The overall proof is proving by contradiction : let and suppose a real number such that . Since for , it is trivial that . Just a little distribution : and Meanwhile, and Since , Therefore, and it is a contradiction. Edit : According to the comments, I found out that the following identity can not be concluded right away since may be negative for some . So now we have to show the whole identity for all and such that : or prove a weaker statement : where since the contradiction still holds if Since , the inequality we want to show is equivalent to the following inequalities : for all such that My further attempt was trying to show that there exists such that for all Suppose for a weaker statement, then and and we get to prove: or If there exists such that for all , such will exist and the inequality above will hold. I am now struggling to prove this with the identities : for , and for Bad News : The function seems to diverge when , according to here . New Solution Note that is defined for all and if for then . Assume exists. For , Using this result, Since , and , there exists real number such that . (If not, then it is a contradiction since on , not everywhere since , but .) Similarly, we can find such that . Now there are two ways to prove that this is impossible. First, since is an entire function, its zero-set should not have accumulation points. However, we can find infinite zeros in the interval and it is a contradiction. Second, let the (partial) zero-set of be such that . Since is bounded above, there exists . Now we can find a sequence in , that converges to . Since is continuous, . Since is a zero of , there exists such that . Since , if then it contradicts the fact that . If , because is a zero of . However, it is trivial that and it is a contradiction. Therefore, if then .","1 + \sum_{n = 1}^{\infty} \frac{x^n}{\prod_{k=1}^{n} H_k} = 0 H_k = \sum_{i=1}^{k} \frac{1}{i}. f(x) = 1 + \sum_{n = 1}^{\infty} \frac{x^n}{\prod_{k=1}^{n} H_k} a f(a) = 0 f(x) \geq 1 x \geq 0 a \lt 0 \begin{align} f(x) &= 1 + \sum_{n = 1}^{\infty} \frac{x^n}{\prod_{k=1}^{n} H_k} = 1 + \sum_{n = 1}^{\infty} \left\{ \frac{x^{2n-1}}{\prod_{k=1}^{2n-1} H_k} + \frac{x^{2n}}{\prod_{k=1}^{2n} H_k}\right\} \\ \\ &= 1 + \sum_{n=1}^{\infty} \frac{x^{2n} + H_{2n} x^{2n-1}}{\prod_{k=1}^{2n} H_k} \end{align} e^x = \sum_{n=0}^{\infty} \frac{x^n}{n!} = 1 + \sum_{n=1}^{\infty} \left\{ \frac{x^{2n-1}}{(2n-1)!} + \frac{x^{2n}}{(2n)!} \right\} = 1 + \sum_{n=1}^{\infty} \frac{x^{2n} + 2nx^{2n-1}}{(2n)!} . H_k = \frac{1}{1} + \frac{1}{2} + \frac{1}{3} + \cdots + \frac{1}{k} \lt k \prod_{k=1}^{2n} H_k \lt (2n)!. a \in \mathbb{R^-} a^{2n} + H_{2n} a^{2n-1} \gt a^{2n} + 2na^{2n-1}. 0 = f(a) = 1 + \sum_{n=1}^{\infty} \frac{a^{2n} + H_{2n} a^{2n-1}}{\prod_{k=1}^{2n} H_k} \gt 1 + \sum_{n=1}^{\infty} \frac{a^{2n} + 2na^{2n-1}}{(2n)!} = e^a \gt 0 a^{2n} + H_{2n} a^{2n-1} a n \in \mathbb{N} a \in \mathbb{R^-} f(a) = 0  \frac{a^{2n} + H_{2n} a^{2n-1}}{\prod_{k=1}^{2n} H_k} \gt \frac{a^{2n} + 2na^{2n-1}}{(2n)!}  \frac{a^{2n} + H_{2n} a^{2n-1}}{\prod_{k=1}^{2n} H_k} \gt \frac{(a \ln t)^{2n} + 2n(a \ln t)^{2n-1}}{(2n)!} t \gt 0 f(a) \gt t^a. a^{2n-1} < 0 \frac{a + H_{2n}}{\prod_{k=1}^{2n} H_k} < \frac{a (\ln t)^{2n} + 2n (\ln t)^{2n-1}}{(2n)!}, a < \frac{2n \cdot \prod_{k=1}^{2n} H_k \cdot (\ln t)^{2n-1} - (2n)! \cdot H_{2n}}{(2n)! - (\ln t)^{2n} \cdot \prod_{k=1}^{2n} H_k} a \in \mathbb{R^-} f(a) = 0. t \in \mathbb{R^+} \frac{2n \cdot \prod_{k=1}^{2n} H_k \cdot (\ln t)^{2n-1} - (2n)! \cdot H_{2n}}{(2n)! - (\ln t)^{2n} \cdot \prod_{k=1}^{2n} H_k} \geq 0 n \in \mathbb{N}. t < e \ln t < 1 (2n)! - (\ln t)^{2n} \cdot \prod_{k=1}^{2n} H_k > (2n)! - \prod_{k=1}^{2n} H_k > 0, 2n \cdot \prod_{k=1}^{2n} H_k \cdot (\ln t)^{2n-1} - (2n)! \cdot H_{2n} \geq 0 \ln t \geq \left\{ \frac{(2n-1)!}{\prod_{k=1}^{2n-1} H_k} \right\}^{\frac{1}{2n-1}} =: F(n). M \in \mathbb{R} F(n) < M n \in \mathbb{N} t \in \mathbb{R^+} \ln n + \frac{1}{n} < H_n < \ln n + 1 n \in \mathbb{N} \ln k = \sum_{n=1}^{\infty} \frac{1}{n} \cdot \left( \frac{k - 1}{k} \right)^n \frac{1}{2} < k \in \mathbb{R}. F(n) n \to \infty f x \in \mathbb{R} f(a) = 0 a \in \mathbb{R} a < 0 a < 0 n \in \mathbb{N} \begin{align} \int_{a}^{0} \frac{x^{n}-a^{n}}{x-a} dx &= \int_{a}^{0} (x^{n-1} + ax^{x-2} + \cdots + a^{n-1}) dx \\ \\ &= \left[ \sum_{k=0}^{n-1} \frac{a^{k}}{n-k} x^{n-k} \right]_{a}^{0} = - a^{n} \sum_{k=1}^{n} \frac{1}{k} = - a^{n} H_{n} \end{align} \begin{align} \int_{a}^{0} \frac{f(x)}{x-a} dx &= \int_{a}^{0} \frac{f(x) - f(a)}{x-a} dx \\ \\ &= \int_{a}^{0} \frac{1}{x-a} \left[ \sum_{n=1}^{\infty} \frac{x^{n}}{H_{1} H_{2} \cdots H_{n}} - \sum_{n=1}^{\infty} \frac{a^{n}}{H_{1} H_{2} \cdots H_{n}} \right] dx \\ \\ &= \int_{a}^{0} \frac{1}{x-a} \left[ \sum_{n=1}^{\infty} \frac{x^{n} - a^{n}}{H_{1} H_{2} \cdots H_{n}} \right] dx \\ \\ &= \sum_{n=1}^{\infty} \frac{1}{H_{1} H_{2} \cdots H_{n}} \int_{a}^{0} \frac{x^{n} - a^{n}}{x-a} dx \\ \\ &= - \sum_{n=1}^{\infty} \frac{a^{n} H_{n}}{H_{1} H_{2} \cdots H_{n}} \\ \\ &= -a - \sum_{n=2}^{\infty} \frac{a^{n} H_{n}}{H_{1} H_{2} \cdots H_{n}} \\ \\ &= -a - a \sum_{n=2}^{\infty} \frac{a^{n-1}}{H_{1} H_{2} \cdots H_{n-1}} \\ \\ &= -a - a \sum_{n=1}^{\infty} \frac{a^{n}}{H_{1} H_{2} \cdots H_{n}} \\ \\ &= - a \cdot \left[ 1 + \sum_{n=1}^{\infty} \frac{a^{n}}{H_{1} H_{2} \cdots H_{n}} \right] \\ \\ &= - a f(a) \\ \\ &= 0 \end{align} f(0) = 1 f(a) = 0 \int_{a}^{0} \frac{f(x)}{x-a} dx = 0 a_{1} \in (a, 0) f(a_{1}) = 0 f \ge 0 (a, 0) f = 0 f(0) = 1 \int_{a}^{0} \frac{f(x)}{x-a} dx = 0 a_{n+1} \in (a_{n}, 0) f(a_{n+1}) = 0 f (a, 0) f S S \subseteq (a, 0) S s := \sup S \{ s_{n} \}_{n \in \mathbb{N}} S s f 0 = \lim_{n \to \infty} f(s_{n}) = f(s) s f s' \in (s, 0) f(s') = 0 s' \in S s \ne 0 s = \sup S s = 0 f(s) = f(0) = 0 s f f(0) = 1 f(z) = 0 z \in \mathbb{C} \, \backslash \, \mathbb{R}","['real-analysis', 'complex-analysis', 'harmonic-numbers']"
42,Norms and pointwise convergence,Norms and pointwise convergence,,"It is known that There is no norm $\| \cdot\|$ on the space $E$ of continuous real-valued functions on an interval, say $[0,1]$ such that $f_n \to f$ for $\|\cdot\|$ if and only if $f_n$ converges pointwise to $f$ , cf Norm for pointwise convergence For any finite dimensional subspace $F$ of $E$ there is such a norm (for polynomials, on can use the Lagrange interpolating polynomials). Hence my question: is there an infinite dimensional subspace of $E$ having such a norm. Note that the problem concerns only sequences: it is not about inducing the pointwise convergence topology Thanks","It is known that There is no norm on the space of continuous real-valued functions on an interval, say such that for if and only if converges pointwise to , cf Norm for pointwise convergence For any finite dimensional subspace of there is such a norm (for polynomials, on can use the Lagrange interpolating polynomials). Hence my question: is there an infinite dimensional subspace of having such a norm. Note that the problem concerns only sequences: it is not about inducing the pointwise convergence topology Thanks","\| \cdot\| E [0,1] f_n \to f \|\cdot\| f_n f F E E","['real-analysis', 'general-topology']"
43,"Regularity of PDEs, general question (Example: Kolmogorov equation)","Regularity of PDEs, general question (Example: Kolmogorov equation)",,"Let us consider a differential operator $L$ and the PDE $$Lu=0$$ with some boundary conditions (assume Neumann conditions). For example, Kolmogorov's equation $$(1) \quad Lu(t,x) = \partial_t u(t,x) + b(t,x) \partial_x u(t,x)+ \frac{1}{2}\partial_x^2 u(t,x) =0, \quad u(T,x) = g(x), \quad t\in [0,T].$$ Then one can consider the adjoint equation, namely $$L^\ast u =0$$ which in the example above is the Fokker-Planck equation $$(2) \quad L^\ast u(t,x) = -\partial_t u(t,x) -\partial_x [b(t,x)u(t,x)]+\frac{1}{2} \partial_x^2 u(t,x), \quad u(0,x)=g(x), \quad t\in [0,T].$$ My question is: Whatever regularity properties you obtain for $u$ from (1), are these properties transferable to (2)?. In other words, studying (1) is equivalent to studying (2) when it comes to well-posedness and regularity of the solution? I am specially interested in this matter for this particular PDE (Kolmogorov) Thanks for any feedback or ideas!","Let us consider a differential operator $L$ and the PDE $$Lu=0$$ with some boundary conditions (assume Neumann conditions). For example, Kolmogorov's equation $$(1) \quad Lu(t,x) = \partial_t u(t,x) + b(t,x) \partial_x u(t,x)+ \frac{1}{2}\partial_x^2 u(t,x) =0, \quad u(T,x) = g(x), \quad t\in [0,T].$$ Then one can consider the adjoint equation, namely $$L^\ast u =0$$ which in the example above is the Fokker-Planck equation $$(2) \quad L^\ast u(t,x) = -\partial_t u(t,x) -\partial_x [b(t,x)u(t,x)]+\frac{1}{2} \partial_x^2 u(t,x), \quad u(0,x)=g(x), \quad t\in [0,T].$$ My question is: Whatever regularity properties you obtain for $u$ from (1), are these properties transferable to (2)?. In other words, studying (1) is equivalent to studying (2) when it comes to well-posedness and regularity of the solution? I am specially interested in this matter for this particular PDE (Kolmogorov) Thanks for any feedback or ideas!",,"['real-analysis', 'functional-analysis', 'partial-differential-equations']"
44,"Supremum of $\int_{-\infty}^{\infty}\frac{p'(x)^2}{p(x)^2+p'(x)^2}\,\mathrm{d}x$",Supremum of,"\int_{-\infty}^{\infty}\frac{p'(x)^2}{p(x)^2+p'(x)^2}\,\mathrm{d}x","Question. Let $P_d = \{ p \in \mathbb{R}[x] : \deg p = d\}$ denote the set of all degree $d$ polynomials with real coefficients. Also, for $p \in \mathbb{R}[x]$ , define $$ I(p) = \frac{1}{\pi} \int_{-\infty}^{\infty}\frac{p'(x)^2}{p(x)^2+p'(x)^2}\,\mathrm{d}x. $$ Is it possible to identify the supremum of $I(\cdot)$ over $P_d$ ? In other words, what is $$ C_d := \sup_{p \in P_d} I(p) = \ ? $$ This question is motivated by this posting . Here are some observations: If $p \in P_d$ has only real zeros, then $I(p) = d$ holds. (See this and this .) $I(p) \leq d^{3/2}$ for any $p \in P_d$ . Indeed, write $p(x) = a (x - \alpha_1) \dots (x - \alpha_d)$ . Then by the Cauchy-Schwarz inequality, $$ \left| \frac{p'(x)}{p(x)} \right|^2 = \left| \sum_{k=1}^{d} \frac{1}{x - \alpha_k} \right|^2 \leq d \sum_{k=1}^{d} \frac{1}{\left| x - \alpha_k \right|^2}. $$ Now by noting that the map $f(t) = \frac{t}{t+1}$ is increasing and subadditive for $t \geq 0$ , \begin{align*} I(p) &= \frac{1}{\pi} \int_{-\infty}^{\infty} f\biggl( \left| \frac{p'(x)}{p(x)} \right|^2 \biggr) \, \mathrm{d}x \\ &\leq \sum_{k=1}^{d} \frac{1}{\pi} \int_{-\infty}^{\infty} f\biggl( \frac{d}{\left| x - \alpha_k \right|^2} \biggr) \,\mathrm{d}x \\ &\leq \sum_{k=1}^{d} \frac{1}{\pi} \int_{-\infty}^{\infty} f\biggl( \frac{d}{(x - \operatorname{Re}(\alpha_k))^2} \biggr) \,\mathrm{d}x \\ &= d^{3/2}. \end{align*} In particular, we learn that $d \leq C_d \leq d^{3/2}$ . When $d = 2$ , we can show that $C_2 = 2$ by using the first part and a direct computation. For $d \geq 4$ , we seem to have $C_d > d$ . Indeed, numerical experiments reveal that we can find $a, b > 0$ satisfying $$I((x^2+a^2)(x - b)^{d-2}) > d.$$ However, $C_d$ seems much smaller than $d^{3/2}$ , differing from $d$ only by a tiny fraction. A simple computation shows that $$ I(p) = d - 2 \sum_{\substack{\alpha : \operatorname{Im}(\alpha) < 0 \\ p(\alpha) = ip'(\alpha) }} \operatorname{Re} \biggl( \frac{1}{1+p''(\alpha)/p(\alpha)} \biggr). $$ This provides an alternative proof of part 1. Indeed, if $p$ has only real zeros, then $\operatorname{Im}\bigl(\frac{p'(z)}{p(z)}\bigr)$ and $\operatorname{Im}(z)$ always have the opposite signs, and so, the summation part in the above formula vanishes.","Question. Let denote the set of all degree polynomials with real coefficients. Also, for , define Is it possible to identify the supremum of over ? In other words, what is This question is motivated by this posting . Here are some observations: If has only real zeros, then holds. (See this and this .) for any . Indeed, write . Then by the Cauchy-Schwarz inequality, Now by noting that the map is increasing and subadditive for , In particular, we learn that . When , we can show that by using the first part and a direct computation. For , we seem to have . Indeed, numerical experiments reveal that we can find satisfying However, seems much smaller than , differing from only by a tiny fraction. A simple computation shows that This provides an alternative proof of part 1. Indeed, if has only real zeros, then and always have the opposite signs, and so, the summation part in the above formula vanishes.","P_d = \{ p \in \mathbb{R}[x] : \deg p = d\} d p \in \mathbb{R}[x]  I(p) = \frac{1}{\pi} \int_{-\infty}^{\infty}\frac{p'(x)^2}{p(x)^2+p'(x)^2}\,\mathrm{d}x.  I(\cdot) P_d  C_d := \sup_{p \in P_d} I(p) = \ ?  p \in P_d I(p) = d I(p) \leq d^{3/2} p \in P_d p(x) = a (x - \alpha_1) \dots (x - \alpha_d)  \left| \frac{p'(x)}{p(x)} \right|^2
= \left| \sum_{k=1}^{d} \frac{1}{x - \alpha_k} \right|^2
\leq d \sum_{k=1}^{d} \frac{1}{\left| x - \alpha_k \right|^2}.  f(t) = \frac{t}{t+1} t \geq 0 \begin{align*}
I(p)
&= \frac{1}{\pi} \int_{-\infty}^{\infty} f\biggl( \left| \frac{p'(x)}{p(x)} \right|^2 \biggr) \, \mathrm{d}x \\
&\leq \sum_{k=1}^{d} \frac{1}{\pi} \int_{-\infty}^{\infty} f\biggl( \frac{d}{\left| x - \alpha_k \right|^2} \biggr) \,\mathrm{d}x \\
&\leq \sum_{k=1}^{d} \frac{1}{\pi} \int_{-\infty}^{\infty} f\biggl( \frac{d}{(x - \operatorname{Re}(\alpha_k))^2} \biggr) \,\mathrm{d}x \\
&= d^{3/2}.
\end{align*} d \leq C_d \leq d^{3/2} d = 2 C_2 = 2 d \geq 4 C_d > d a, b > 0 I((x^2+a^2)(x - b)^{d-2}) > d. C_d d^{3/2} d  I(p) = d - 2 \sum_{\substack{\alpha : \operatorname{Im}(\alpha) < 0 \\ p(\alpha) = ip'(\alpha) }} \operatorname{Re} \biggl( \frac{1}{1+p''(\alpha)/p(\alpha)} \biggr).  p \operatorname{Im}\bigl(\frac{p'(z)}{p(z)}\bigr) \operatorname{Im}(z)","['real-analysis', 'integration', 'optimization', 'definite-integrals']"
45,Extension of Vector Field in the $\mathcal{C}^r$ topology.,Extension of Vector Field in the  topology.,\mathcal{C}^r,"Let $M\subset \mathbb{R}^n$ be a compact smooth manifold embedded in $\mathbb{R}^n$ , we define $$\mathfrak{X}(M) := \{X: M \to \mathbb{R}^n;\ X\mbox{ is smooth and }\ X(p) \in T_p M \subset \mathbb{R}^n,\ \forall\ p \in M \}.$$ Choosing an atlas $\{(\varphi_i,U_i)\}_{i=1}^{n}$ , and compacts $K_i \subset U_i$ , such that $$\bigcup_{i=1}^n K_i = M,$$ we define the $\|\cdot \|_r$ norm as \begin{align*}\|\cdot\|_r :  \mathfrak{X}(M)&\to \mathbb{R}\\ X &\to \max_{\substack{i\in\{1,...,n\} \\ j\in \{0,...,r\}}}\left\{\sup_{x \in \varphi^{-1}_i(K_i)}\left\| \text{d}^{j}\left( X\circ\varphi_i \right)  \right\|\right\}, \end{align*} then we named $\mathfrak{X}^r(M)$ as the complete Banach space $(\mathfrak{X}(M),\|\cdot\|_r)$ (it is possible to prove that the topology of $\mathfrak{X}^r(M)$ does not depend on the selected atlas). My Question: Let $X \in \mathfrak{X}(M)$ and $Y$ be a smooth vector field on $M$ defined just in a compact $K \subset M$ such that $$\max_{\substack{i\in\{1,...,n\} \\ j\in \{0,...,r\}}}\left\{\sup_{x \in \varphi^{-1}_i(K_i\cap K)}\left\| \text{d}^{j}\left( X\circ\varphi_i \right) - \text{d}^{j}\left( Y\circ\varphi_i \right) \right\|\right\}<\varepsilon,$$ is it possible extend $Y$ to a vector field $\tilde{Y}$ such that 1) $\left.\tilde{Y}\right|_{K} = Y$ , 2) $\|X-\tilde Y\|_r < A\cdot\varepsilon$ , where $A $ is a constant that depends only on the manifold $K$ ? The compact $K$ is a connected submanifold with boundary of $M$ , such that $\dim K = \dim M$ . Edit: I changed $\|X-\tilde Y\|_r < \varepsilon$ to $\|X-\tilde Y\|_r < A\cdot\varepsilon$ after Moishe Kohan's comment. My ideas First, I extend $Y$ by a smooth vector field $Z$ $\in \mathfrak{X}(M)$ , by the continuity of $Z$ , so there exists a neighborhood $U$ of $K$ , such that $$\max_{\substack{i\in\{1,...,n\} \\ j\in \{0,...,r\}}}\left\{\sup_{x \in \varphi^{-1}_i(K_i\cap U)}\left\| \text{d}^{j}\left( X\circ\varphi_i \right) - \text{d}^{j}\left( Z\circ\varphi_i \right) \right\|\right\}<\varepsilon,$$ and then choosing a partition of unity $ \{\phi_1, \phi_2\}$ subordinate to the cover $\{U,M\setminus K\}$ we can define $$\tilde{Y} = \phi_1 Z + \phi_2 X, $$ however I could not guarantee that $\|X - \tilde{Y}\|_r < \varepsilon$ , because I can not control de derivatives of $\phi_1$ and $\phi_2$ . Does anyone know how  should I proceed?","Let be a compact smooth manifold embedded in , we define Choosing an atlas , and compacts , such that we define the norm as then we named as the complete Banach space (it is possible to prove that the topology of does not depend on the selected atlas). My Question: Let and be a smooth vector field on defined just in a compact such that is it possible extend to a vector field such that 1) , 2) , where is a constant that depends only on the manifold ? The compact is a connected submanifold with boundary of , such that . Edit: I changed to after Moishe Kohan's comment. My ideas First, I extend by a smooth vector field , by the continuity of , so there exists a neighborhood of , such that and then choosing a partition of unity subordinate to the cover we can define however I could not guarantee that , because I can not control de derivatives of and . Does anyone know how  should I proceed?","M\subset \mathbb{R}^n \mathbb{R}^n \mathfrak{X}(M) := \{X: M \to \mathbb{R}^n;\ X\mbox{ is smooth and }\ X(p) \in T_p M \subset \mathbb{R}^n,\ \forall\ p \in M \}. \{(\varphi_i,U_i)\}_{i=1}^{n} K_i \subset U_i \bigcup_{i=1}^n K_i = M, \|\cdot \|_r \begin{align*}\|\cdot\|_r :  \mathfrak{X}(M)&\to \mathbb{R}\\
X &\to \max_{\substack{i\in\{1,...,n\} \\ j\in \{0,...,r\}}}\left\{\sup_{x \in \varphi^{-1}_i(K_i)}\left\| \text{d}^{j}\left( X\circ\varphi_i \right)  \right\|\right\},
\end{align*} \mathfrak{X}^r(M) (\mathfrak{X}(M),\|\cdot\|_r) \mathfrak{X}^r(M) X \in \mathfrak{X}(M) Y M K \subset M \max_{\substack{i\in\{1,...,n\} \\ j\in \{0,...,r\}}}\left\{\sup_{x \in \varphi^{-1}_i(K_i\cap K)}\left\| \text{d}^{j}\left( X\circ\varphi_i \right) - \text{d}^{j}\left( Y\circ\varphi_i \right) \right\|\right\}<\varepsilon, Y \tilde{Y} \left.\tilde{Y}\right|_{K} = Y \|X-\tilde Y\|_r < A\cdot\varepsilon A  K K M \dim K = \dim M \|X-\tilde Y\|_r < \varepsilon \|X-\tilde Y\|_r < A\cdot\varepsilon Y Z \in \mathfrak{X}(M) Z U K \max_{\substack{i\in\{1,...,n\} \\ j\in \{0,...,r\}}}\left\{\sup_{x \in \varphi^{-1}_i(K_i\cap U)}\left\| \text{d}^{j}\left( X\circ\varphi_i \right) - \text{d}^{j}\left( Z\circ\varphi_i \right) \right\|\right\}<\varepsilon,  \{\phi_1, \phi_2\} \{U,M\setminus K\} \tilde{Y} = \phi_1 Z + \phi_2 X,  \|X - \tilde{Y}\|_r < \varepsilon \phi_1 \phi_2","['real-analysis', 'analysis', 'differential-geometry', 'manifolds', 'differential-topology']"
46,"How ""messy"" can a multivariable function be?","How ""messy"" can a multivariable function be?",,"In calculus lectures, we are told that approaching a limit in a multivariable function through some paths does not guarantee the existence of the limit, for example: here . And one can see that, in the following example its easy to see that there are $2$ paths that seem to point, in one case, that the limit is $0$ and in another case, that the limit is $1$ and hence it does not exist. $$f(x,y)=\frac{2xy}{x^2+y^2} \quad f(0,0)=0  $$ But the objects we have in question are continuous functions with the exception of perhaps, the point $(0,0)$ or some other strategically placed point . It seems intuitively reasonable that one can approach $(0,0)$ though $2$ or some ""low"" number of different paths and have different possible limits but it doesn't seems reasonable that one could have a function such that the possible limit is different for $278$ paths, for example. I'm not sure if these lectures try to take into account the full generality of multivariable functions, but  it seems absurd that a  $2-$variable function can have such a messy structure that actually allows it to have an arbitrary number of paths that suggest (each one) different possible limits. But I don't know what to say/think when the number of variables is greater than $3$. EDIT: There are interesting related sub-questions I forgot to add: Is it possible that we test the existence of a limit with all the lines and polar coordinates, obtain a possible limit $L$ for these tests and yet, the function have another path such that the possible limit for this path is different of $L$? The question above glooms to this: Isn't there a minimum number of paths given by families of curves in which, the possible limit is $L$ for all of them and this actually guarantees that the limit is $L$? I make this question because it seems extremely counter-intuitive that given - for example - tests with all the lines, all the parabolas and polar coordinates all with possible limit $L$, there could still be one path that gives a possible limit different of $L$.","In calculus lectures, we are told that approaching a limit in a multivariable function through some paths does not guarantee the existence of the limit, for example: here . And one can see that, in the following example its easy to see that there are $2$ paths that seem to point, in one case, that the limit is $0$ and in another case, that the limit is $1$ and hence it does not exist. $$f(x,y)=\frac{2xy}{x^2+y^2} \quad f(0,0)=0  $$ But the objects we have in question are continuous functions with the exception of perhaps, the point $(0,0)$ or some other strategically placed point . It seems intuitively reasonable that one can approach $(0,0)$ though $2$ or some ""low"" number of different paths and have different possible limits but it doesn't seems reasonable that one could have a function such that the possible limit is different for $278$ paths, for example. I'm not sure if these lectures try to take into account the full generality of multivariable functions, but  it seems absurd that a  $2-$variable function can have such a messy structure that actually allows it to have an arbitrary number of paths that suggest (each one) different possible limits. But I don't know what to say/think when the number of variables is greater than $3$. EDIT: There are interesting related sub-questions I forgot to add: Is it possible that we test the existence of a limit with all the lines and polar coordinates, obtain a possible limit $L$ for these tests and yet, the function have another path such that the possible limit for this path is different of $L$? The question above glooms to this: Isn't there a minimum number of paths given by families of curves in which, the possible limit is $L$ for all of them and this actually guarantees that the limit is $L$? I make this question because it seems extremely counter-intuitive that given - for example - tests with all the lines, all the parabolas and polar coordinates all with possible limit $L$, there could still be one path that gives a possible limit different of $L$.",,"['real-analysis', 'limits', 'multivariable-calculus', 'soft-question']"
47,Does $\frac{x}{x}=1$ when $x=\infty$?,Does  when ?,\frac{x}{x}=1 x=\infty,"This may be a dumb question: Does $\frac{x}{x}=1$ when $x=\infty$? I understand why $\frac{x}{x}$ is undefined when $x=0$:  This can cause errors if an equation is divided by $x$ without restrictions. Also, $\frac{\infty}{\infty}$ is undefined.  So when I use $\frac{x}{x}=1$ to simplify an equation, can it also lead to errors because $x$ can equal infinity? Or is $x=\infty$ meaningless?","This may be a dumb question: Does $\frac{x}{x}=1$ when $x=\infty$? I understand why $\frac{x}{x}$ is undefined when $x=0$:  This can cause errors if an equation is divided by $x$ without restrictions. Also, $\frac{\infty}{\infty}$ is undefined.  So when I use $\frac{x}{x}=1$ to simplify an equation, can it also lead to errors because $x$ can equal infinity? Or is $x=\infty$ meaningless?",,"['calculus', 'real-analysis']"
48,Prove that $\sqrt 2 +\sqrt 3$ is irrational. [duplicate],Prove that  is irrational. [duplicate],\sqrt 2 +\sqrt 3,"This question already has answers here : Can $\sqrt{n} + \sqrt{m}$ be rational if neither $n,m$ are perfect squares? [duplicate] (5 answers) Closed 10 years ago . Please prove that $\sqrt 2 + \sqrt 3$ is irrational. One of the proofs I've seen goes: If $\sqrt 2 +\sqrt 3$ is rational, then consider $(\sqrt 3 +\sqrt 2)(\sqrt 3 -\sqrt 2)=1$, which implies that $\sqrt 3 − \sqrt 2$ is rational. Hence, $\sqrt 3$ would be rational. It is impossible. So $\sqrt 2 +\sqrt 3$ is irrational. Now how do we know that if $\sqrt 3 -\sqrt 2$ is rational, then $\sqrt 3$ should be rational? Thank you.","This question already has answers here : Can $\sqrt{n} + \sqrt{m}$ be rational if neither $n,m$ are perfect squares? [duplicate] (5 answers) Closed 10 years ago . Please prove that $\sqrt 2 + \sqrt 3$ is irrational. One of the proofs I've seen goes: If $\sqrt 2 +\sqrt 3$ is rational, then consider $(\sqrt 3 +\sqrt 2)(\sqrt 3 -\sqrt 2)=1$, which implies that $\sqrt 3 − \sqrt 2$ is rational. Hence, $\sqrt 3$ would be rational. It is impossible. So $\sqrt 2 +\sqrt 3$ is irrational. Now how do we know that if $\sqrt 3 -\sqrt 2$ is rational, then $\sqrt 3$ should be rational? Thank you.",,"['real-analysis', 'radicals', 'irrational-numbers', 'rationality-testing']"
49,"If $f$ is continuous at $a$, is it continuous in some open interval around $a$?","If  is continuous at , is it continuous in some open interval around ?",f a a,"If $f: \mathbb{R} \to \mathbb{R}$ is continuous at $a$, is it continuous in some open interval around $a$?","If $f: \mathbb{R} \to \mathbb{R}$ is continuous at $a$, is it continuous in some open interval around $a$?",,"['calculus', 'real-analysis', 'continuity', 'examples-counterexamples']"
50,Is differentiation as a map discontinuous?,Is differentiation as a map discontinuous?,,"I came across the statement below: Let $C([0,1])$ be the space of all continuous functions over the interval $[0,1]$ equipped with the Supremum norm. Assume $A$ is a map on the space of all differentiable functions whose derivative is continuous into $C([0,1])$ . Also, $A$ is differentiation in the sense that it maps a functions to its derivative. The map $A$ (differentiation) is discontinuous. It's written that the last sentence is well-known but I can't make any sense of it. How can I arrive at such a conclusion? Actually, I am looking for an explicit counterexample. Any help would be highly appreciated.","I came across the statement below: Let be the space of all continuous functions over the interval equipped with the Supremum norm. Assume is a map on the space of all differentiable functions whose derivative is continuous into . Also, is differentiation in the sense that it maps a functions to its derivative. The map (differentiation) is discontinuous. It's written that the last sentence is well-known but I can't make any sense of it. How can I arrive at such a conclusion? Actually, I am looking for an explicit counterexample. Any help would be highly appreciated.","C([0,1]) [0,1] A C([0,1]) A A","['real-analysis', 'calculus', 'general-topology', 'functional-analysis']"
51,"If a compact set is covered by a finite union of open balls of same radii, can we always get a lesser radius?","If a compact set is covered by a finite union of open balls of same radii, can we always get a lesser radius?",,"This question seems obvious, but I'm not secure of my proof. If a compact set $V\subset \mathbb{R^n}$ is covered by a finite union of open balls of common radii $C(r):=\bigcup_{i=1}^m B(c_i,r)$, then is it true that there exists $0<s<r$ such that $V\subseteq C(s)$ as well? The centers are fixed. I believe this statement is true and this is my attempt to prove it: Each point of $v\in V$ is an interior point of least one ball (suppose its index is $j_v$), that is, there exists $\varepsilon_v>0$ such that $B(v,\varepsilon_v)\subseteq B(c_{j_v},r)$, so $v\in B(c_{j_v},r-\varepsilon_v)$. Lets consider only the greatest $\varepsilon_v$ such that this holds. Then defining $\varepsilon:=\inf\{\varepsilon_v\mid v\in V\}$ and $s=r-\varepsilon$ we get $V\subseteq C(s)$. But why is $\varepsilon$ not zero? I thought that considering the greatest $\varepsilon_v$ was important, but still couldn't convince myself. I would appreciate any help.","This question seems obvious, but I'm not secure of my proof. If a compact set $V\subset \mathbb{R^n}$ is covered by a finite union of open balls of common radii $C(r):=\bigcup_{i=1}^m B(c_i,r)$, then is it true that there exists $0<s<r$ such that $V\subseteq C(s)$ as well? The centers are fixed. I believe this statement is true and this is my attempt to prove it: Each point of $v\in V$ is an interior point of least one ball (suppose its index is $j_v$), that is, there exists $\varepsilon_v>0$ such that $B(v,\varepsilon_v)\subseteq B(c_{j_v},r)$, so $v\in B(c_{j_v},r-\varepsilon_v)$. Lets consider only the greatest $\varepsilon_v$ such that this holds. Then defining $\varepsilon:=\inf\{\varepsilon_v\mid v\in V\}$ and $s=r-\varepsilon$ we get $V\subseteq C(s)$. But why is $\varepsilon$ not zero? I thought that considering the greatest $\varepsilon_v$ was important, but still couldn't convince myself. I would appreciate any help.",,"['real-analysis', 'general-topology', 'metric-spaces']"
52,"Could someone explain the concept of a set being ""open relative"" to another set?","Could someone explain the concept of a set being ""open relative"" to another set?",,"My professor sort of skimmed through this concept, giving only the definition and one example (i.e. $(0,1) \subset \mathbb{R}$ vs. $(0,1) \subset \mathbb{R}^2$, where $(0,1)$ is open relative to $\mathbb{R}$ but not $\mathbb{R}^2$), but no real explanation. I understand, more or less, this particular example, but I'm having trouble understanding this more generally. Can somebody please intuitively explain this concept in the context of a metric space? (If it matters, we are using Rudin's Principles Of Mathematical Analysis ).","My professor sort of skimmed through this concept, giving only the definition and one example (i.e. $(0,1) \subset \mathbb{R}$ vs. $(0,1) \subset \mathbb{R}^2$, where $(0,1)$ is open relative to $\mathbb{R}$ but not $\mathbb{R}^2$), but no real explanation. I understand, more or less, this particular example, but I'm having trouble understanding this more generally. Can somebody please intuitively explain this concept in the context of a metric space? (If it matters, we are using Rudin's Principles Of Mathematical Analysis ).",,"['real-analysis', 'analysis']"
53,Does $0 < x < 0$ imply $x =0$?,Does  imply ?,0 < x < 0 x =0,"In Real Analysis class, a professor told us 4 claims: let x be a real number, then: 1) $0\leq x \leq 0$ implies $x = 0$ 2) $0 \leq x < 0$ implies $x = 0$ 3) $0 < x \leq 0$ implies $x = 0$ 4) $0 < x < 0$ implies $x = 0$ Of course, claim #1 comes from the fact that the reals are totally ordered by the $\leq$ relation, and when you think about it from the perspective of the trichotomy property, it makes sense, because claim #1 says: $0 \leq x$, and, $x \leq 0$, and then, the only number which satisfies both propositions, is $x = 0$. But I am not sure I understand the reason behind claims #2, #3 and #4. Let's analyze claim #2: It starts saying $0 \leq x$, that means that x is a number greater than or equal to $0$, but then it says $x < 0$, therefore x is less than zero. I think no number can satisfy both propositions, as it would contradict the trichotomy property, because x is less than $0$ AND greater than or equal to $0$. Same thing with claim #4, since $0 < x < 0$ means: $x > 0$ AND $x < 0$, and no real number satisfy both propositions at the same time. Therefore, saying $x = 0$ is the same as saying $x = 1$, or $2$, or $42$ (this is because the antecedent if always false). Am I missing something? My professor then told us that these were axioms, but I think that axioms should not contradict well established properties (like the trichotomy property) or, at the very least, make some sense. Are claims #2 to #4  well accepted and used ""axioms"" in real analysis?","In Real Analysis class, a professor told us 4 claims: let x be a real number, then: 1) $0\leq x \leq 0$ implies $x = 0$ 2) $0 \leq x < 0$ implies $x = 0$ 3) $0 < x \leq 0$ implies $x = 0$ 4) $0 < x < 0$ implies $x = 0$ Of course, claim #1 comes from the fact that the reals are totally ordered by the $\leq$ relation, and when you think about it from the perspective of the trichotomy property, it makes sense, because claim #1 says: $0 \leq x$, and, $x \leq 0$, and then, the only number which satisfies both propositions, is $x = 0$. But I am not sure I understand the reason behind claims #2, #3 and #4. Let's analyze claim #2: It starts saying $0 \leq x$, that means that x is a number greater than or equal to $0$, but then it says $x < 0$, therefore x is less than zero. I think no number can satisfy both propositions, as it would contradict the trichotomy property, because x is less than $0$ AND greater than or equal to $0$. Same thing with claim #4, since $0 < x < 0$ means: $x > 0$ AND $x < 0$, and no real number satisfy both propositions at the same time. Therefore, saying $x = 0$ is the same as saying $x = 1$, or $2$, or $42$ (this is because the antecedent if always false). Am I missing something? My professor then told us that these were axioms, but I think that axioms should not contradict well established properties (like the trichotomy property) or, at the very least, make some sense. Are claims #2 to #4  well accepted and used ""axioms"" in real analysis?",,"['real-analysis', 'logic', 'real-numbers']"
54,What is the limit of $\frac{\prod\mathrm{Odd}}{\prod\mathrm{Even}}?$ Does $\pi$ show up here (again)?,What is the limit of  Does  show up here (again)?,\frac{\prod\mathrm{Odd}}{\prod\mathrm{Even}}? \pi,"What is the result of the following limit? $$   \frac{1\times3\times5\times\cdots}{2\times4\times6\times8\times\cdots} = \lim_{n \rightarrow \infty}\prod_{i=1}^{n}\frac{(2i-1)}{2i} $$ If I remember correctly, it is something related to $\pi$ . How can I compute it? And moreover,  how can I compute the summation of the series?","What is the result of the following limit? If I remember correctly, it is something related to . How can I compute it? And moreover,  how can I compute the summation of the series?","
  \frac{1\times3\times5\times\cdots}{2\times4\times6\times8\times\cdots} = \lim_{n \rightarrow \infty}\prod_{i=1}^{n}\frac{(2i-1)}{2i}
 \pi","['real-analysis', 'calculus', 'probability', 'limits', 'pi']"
55,Why do we take a derivative?,Why do we take a derivative?,,I don't have a fundamental  understanding of what taking a derivative means and I don't understand why would we do it. I just know how to mechanically do it.,I don't have a fundamental  understanding of what taking a derivative means and I don't understand why would we do it. I just know how to mechanically do it.,,"['calculus', 'real-analysis']"
56,Is showing that $x_n \rightarrow x_0\Rightarrow f(x_n) \rightarrow f(x_0)$ for a single sequence enough to prove continuity?,Is showing that  for a single sequence enough to prove continuity?,x_n \rightarrow x_0\Rightarrow f(x_n) \rightarrow f(x_0),"For all my homework in real analysis, when I've been asked to show that a function is continuous, I just found a single $x_n \in D$ and showed that when $x_n \rightarrow x_0$ , $f(x_n) \rightarrow f(x_0)$ . Apparently, the sequence definition (as opposed to the epsilon delta definition) is (basically) only used to prove a function is not continuous, and I can't prove a function is continuous because then I'd have to show this is true for all possible sequences? Am I doing the math wrongly? Should I always use the epsilon delta definition when trying to prove that a function is continuous?","For all my homework in real analysis, when I've been asked to show that a function is continuous, I just found a single and showed that when , . Apparently, the sequence definition (as opposed to the epsilon delta definition) is (basically) only used to prove a function is not continuous, and I can't prove a function is continuous because then I'd have to show this is true for all possible sequences? Am I doing the math wrongly? Should I always use the epsilon delta definition when trying to prove that a function is continuous?",x_n \in D x_n \rightarrow x_0 f(x_n) \rightarrow f(x_0),"['real-analysis', 'continuity']"
57,How to answer mathematical questions in the proper way,How to answer mathematical questions in the proper way,,"In my linear algebra class, I get the answer and everything right but lose points on the mathematical writing, symbols, notations etc ... Is there a comprehensive book or document that provides extensive explanation and examples on how to answer questions in the proper mathematical way?","In my linear algebra class, I get the answer and everything right but lose points on the mathematical writing, symbols, notations etc ... Is there a comprehensive book or document that provides extensive explanation and examples on how to answer questions in the proper mathematical way?",,"['calculus', 'real-analysis']"
58,"Evaluating $\int_0^1\int_0^1\cdots\int_0^1\frac{n\max\{x_1,x_2,\cdots,x_n\}}{x_1+x_2+\cdots+x_n}dx_1dx_2\cdots dx_n$",Evaluating,"\int_0^1\int_0^1\cdots\int_0^1\frac{n\max\{x_1,x_2,\cdots,x_n\}}{x_1+x_2+\cdots+x_n}dx_1dx_2\cdots dx_n","I was trying to compute the following integral: $$I=\int_0^1\int_0^1\cdots \int_0^1 \frac{n \max\{x_1,x_2,\ldots,x_n\}}{x_1+x_2+\ldots+x_n}dx_1dx_2\ldots dx_n.$$ My attempt was: Let $X_1,X_2, \ldots, X_n$ be a collection of i.i.d. uniform random variables on $(0,1)$ . Then, $$I= n\mathbb{E}\left[\frac{\max\{X_1,X_2,\ldots,X_n\}}{X_1+X_2+\ldots+X_n} \right] = n\mathbb{E}\left[\max_{1\leqslant i \leqslant n}\frac{X_i}{X_1+X_2+\ldots+X_n} \right].$$ Now, I understand that $$\left\{\frac{X_i}{X_1+X_2+\ldots+X_n}\right\}_{1\leqslant i \leqslant n}$$ is an identically distributed sequence of random variables but I don't know how to proceed further. Any further help using either probability or general integral calculus is much appreciated. Thank you very much for your time and attention.","I was trying to compute the following integral: My attempt was: Let be a collection of i.i.d. uniform random variables on . Then, Now, I understand that is an identically distributed sequence of random variables but I don't know how to proceed further. Any further help using either probability or general integral calculus is much appreciated. Thank you very much for your time and attention.","I=\int_0^1\int_0^1\cdots \int_0^1 \frac{n \max\{x_1,x_2,\ldots,x_n\}}{x_1+x_2+\ldots+x_n}dx_1dx_2\ldots dx_n. X_1,X_2, \ldots, X_n (0,1) I= n\mathbb{E}\left[\frac{\max\{X_1,X_2,\ldots,X_n\}}{X_1+X_2+\ldots+X_n} \right] = n\mathbb{E}\left[\max_{1\leqslant i \leqslant n}\frac{X_i}{X_1+X_2+\ldots+X_n} \right]. \left\{\frac{X_i}{X_1+X_2+\ldots+X_n}\right\}_{1\leqslant i \leqslant n}","['real-analysis', 'integration', 'probability-theory', 'multivariable-calculus', 'definite-integrals']"
59,"Is ""integrability"" equivalent to ""having antiderivative""?","Is ""integrability"" equivalent to ""having antiderivative""?",,"I am wondering if ""a function $f(x)$ is integrable on a domain $D$"" this proposition is equivalent to ""$f(x)$ has antiderivative on domain $D$"". If it is not the case, give me a counter example. Thank you.","I am wondering if ""a function $f(x)$ is integrable on a domain $D$"" this proposition is equivalent to ""$f(x)$ has antiderivative on domain $D$"". If it is not the case, give me a counter example. Thank you.",,"['real-analysis', 'integration']"
60,Why doesn't the Stone-Weierstrass theorem imply that every function has a power series expansion?,Why doesn't the Stone-Weierstrass theorem imply that every function has a power series expansion?,,"I know that not every function has a power series expansion.  Yet what I don't understand is that for every $C^{\infty}$ functions there is a sequence of polynomial $(P_n)$ such that $P_n$ converges uniformly to $f$ . That's to say : $$\forall x \in [a,b], f(x) = \lim_{n \to \infty}  \sum_{k = 0}^{\infty} a_{k,n}x^k$$ But then because it converges uniformly why can't I say that : $$\forall x \in [a,b], f(x) =   \sum_{k = 0}^{\infty} \lim_{n \to \infty} a_{k,n}x^k$$ And so $f$ has a power series expansion with coefficients: $\lim_{n \to \infty} a_{k,n}x^k$ .",I know that not every function has a power series expansion.  Yet what I don't understand is that for every functions there is a sequence of polynomial such that converges uniformly to . That's to say : But then because it converges uniformly why can't I say that : And so has a power series expansion with coefficients: .,"C^{\infty} (P_n) P_n f \forall x \in [a,b], f(x) = \lim_{n \to \infty}  \sum_{k = 0}^{\infty} a_{k,n}x^k \forall x \in [a,b], f(x) =   \sum_{k = 0}^{\infty} \lim_{n \to \infty} a_{k,n}x^k f \lim_{n \to \infty} a_{k,n}x^k","['calculus', 'real-analysis', 'power-series']"
61,Multivariate Taylor Expansion,Multivariate Taylor Expansion,,"I am in confidence with Taylor expansion of function $f\colon R \to R$, but I when my professor started to use higher order derivatives and multivariate Taylor expansion of $f\colon R^n \to R$ and $f\colon R^n \to R^m$ I felt lost. Can somean explain to me from scratch multivariate Taylor? In particular I don't understand the notation $$ f(x+h) = \sum_{k=0}^p \frac{1}{k!} f^{(k)}(x)[h,...,h] + O(h^{p+1}) $$ Why we need the k-linear form $ \frac{1}{k!} f^{(k)}(x)[h,...,h]$? This k-linear form is the derivative or the derivative is only $f^{(k)}(x)$? I'm quite lost. Thank you.","I am in confidence with Taylor expansion of function $f\colon R \to R$, but I when my professor started to use higher order derivatives and multivariate Taylor expansion of $f\colon R^n \to R$ and $f\colon R^n \to R^m$ I felt lost. Can somean explain to me from scratch multivariate Taylor? In particular I don't understand the notation $$ f(x+h) = \sum_{k=0}^p \frac{1}{k!} f^{(k)}(x)[h,...,h] + O(h^{p+1}) $$ Why we need the k-linear form $ \frac{1}{k!} f^{(k)}(x)[h,...,h]$? This k-linear form is the derivative or the derivative is only $f^{(k)}(x)$? I'm quite lost. Thank you.",,"['real-analysis', 'taylor-expansion', 'multilinear-algebra']"
62,The series $\sum_{n=1}^{+\infty}\frac{1}{1^2+2^2+\cdots+n^2}.$,The series,\sum_{n=1}^{+\infty}\frac{1}{1^2+2^2+\cdots+n^2}.,How to justify the convergence and calculate the sum of the series: $$\sum_{n=1}^{+\infty}\frac{1}{1^2+2^2+\cdots+n^2}.$$,How to justify the convergence and calculate the sum of the series: $$\sum_{n=1}^{+\infty}\frac{1}{1^2+2^2+\cdots+n^2}.$$,,['calculus']
63,$L^1$ and $L^{\infty}$ are not reflexive,and  are not reflexive,L^1 L^{\infty},"I want some proof for the following statement : $L^1$ and $L^{\infty}$ are not reflexive. Can anyone help me, please? or reference me?","I want some proof for the following statement : $L^1$ and $L^{\infty}$ are not reflexive. Can anyone help me, please? or reference me?",,"['real-analysis', 'functional-analysis']"
64,When does equality hold in the Minkowski's inequality $\|f+g\|_p\leq\|f\|_p+\|g\|_p$?,When does equality hold in the Minkowski's inequality ?,\|f+g\|_p\leq\|f\|_p+\|g\|_p,"I would like to see a proof of when equality holds in Minkowski's inequality . Minkowski's inequality. If $1\le p<\infty$ and $f,g\in L^p$ , then $$\|f+g\|_p \le \|f\|_p + \|g\|_p.$$ The proof is quite different for when $p=1$ and when $1<p<\infty$ . Could someone provide a reference? Thanks!","I would like to see a proof of when equality holds in Minkowski's inequality . Minkowski's inequality. If and , then The proof is quite different for when and when . Could someone provide a reference? Thanks!","1\le p<\infty f,g\in L^p \|f+g\|_p \le \|f\|_p + \|g\|_p. p=1 1<p<\infty","['real-analysis', 'inequality', 'reference-request', 'lp-spaces', 'triangle-inequality']"
65,A curiosity: how do we prove $\mathbb{R}$ is closed under addition and multiplication?,A curiosity: how do we prove  is closed under addition and multiplication?,\mathbb{R},"So I tried looking around for this question, but I didn't find much of anything - mostly unrelated-but-similarly-worded stuff. So either I suck at Googling or whatever but I'll get to the point. So far in my coursework, it seems like we've mostly taken for granted that $(\mathbb{R},+,\cdot)$ is a field. I'm not doubting that much, that would seem silly. However, my question is: how would one prove this? In particular, how would one prove that $(\mathbb{R},+)$ and $(\mathbb{R}\setminus \{0\}, \cdot)$ are closed under their respective operations? I understand the definition of closure, but to say ""a real number plus/times a real number is a real number"" seems oddly circular since, without demonstrating that, it essentially invokes the assumptions we're trying to prove. Obviously, there's something ""more"" to the definition of ""real number"" that would make proving this possible. Though I'm not sure what property would be used for this. One thought I dwelled on for a while was instead looking at what the real numbers are not . For example, they are numbers lacking those ""imaginary"" components you see in their higher-dimensional generalizations - the complex numbers ( $i$ ), quaternions ( $i,j,k$ ), and so on. But that didn't seem quite ""right"" to me? Like I'm not sure if it's really wrong, it just irked me in some way. Like it's simple enough to say ""a real number is any complex number with a zero imaginary component,"" take two real numbers, show their imaginary parts sum/multiply to zero, and thus we have a real number. Maybe it's just a personal issue? Like I said - I'm not saying it's inherently wrong (it might be, though, I don't know - if it is, I would like to know why). Maybe it's just the whole idea of ""defining a number by what's it's not "" that bugs me. Like I said, I'm not really sure, and I think I'm rambling/unclear enough as it is, so I'll get straight to the point. In short, how does one properly demonstrate, if not in the above way, $$a,b \in \mathbb{R} \Rightarrow (a+b)\in \mathbb{R}$$ $$a,b\in \mathbb{R \setminus \{0\}} \Rightarrow (a\cdot b) \in \mathbb{R \setminus \{0\}}$$ (And again, I'm not at all doubting that these are true. I'm just curious as to how one would demonstrate these facts in the most appropriate manner since I don't believe it's come up in my coursework and I've been curious on how one would prove it for a few days now.)","So I tried looking around for this question, but I didn't find much of anything - mostly unrelated-but-similarly-worded stuff. So either I suck at Googling or whatever but I'll get to the point. So far in my coursework, it seems like we've mostly taken for granted that is a field. I'm not doubting that much, that would seem silly. However, my question is: how would one prove this? In particular, how would one prove that and are closed under their respective operations? I understand the definition of closure, but to say ""a real number plus/times a real number is a real number"" seems oddly circular since, without demonstrating that, it essentially invokes the assumptions we're trying to prove. Obviously, there's something ""more"" to the definition of ""real number"" that would make proving this possible. Though I'm not sure what property would be used for this. One thought I dwelled on for a while was instead looking at what the real numbers are not . For example, they are numbers lacking those ""imaginary"" components you see in their higher-dimensional generalizations - the complex numbers ( ), quaternions ( ), and so on. But that didn't seem quite ""right"" to me? Like I'm not sure if it's really wrong, it just irked me in some way. Like it's simple enough to say ""a real number is any complex number with a zero imaginary component,"" take two real numbers, show their imaginary parts sum/multiply to zero, and thus we have a real number. Maybe it's just a personal issue? Like I said - I'm not saying it's inherently wrong (it might be, though, I don't know - if it is, I would like to know why). Maybe it's just the whole idea of ""defining a number by what's it's not "" that bugs me. Like I said, I'm not really sure, and I think I'm rambling/unclear enough as it is, so I'll get straight to the point. In short, how does one properly demonstrate, if not in the above way, (And again, I'm not at all doubting that these are true. I'm just curious as to how one would demonstrate these facts in the most appropriate manner since I don't believe it's come up in my coursework and I've been curious on how one would prove it for a few days now.)","(\mathbb{R},+,\cdot) (\mathbb{R},+) (\mathbb{R}\setminus \{0\}, \cdot) i i,j,k a,b \in \mathbb{R} \Rightarrow (a+b)\in \mathbb{R} a,b\in \mathbb{R \setminus \{0\}} \Rightarrow (a\cdot b) \in \mathbb{R \setminus \{0\}}","['real-analysis', 'group-theory', 'field-theory', 'real-numbers', 'abelian-groups']"
66,Proof of Convergence: Babylonian Method $x_{n+1}=\frac{1}{2}(x_n + \frac{a}{x_n})$,Proof of Convergence: Babylonian Method,x_{n+1}=\frac{1}{2}(x_n + \frac{a}{x_n}),"a) Let $a>0$ and the sequence $x_n$ fulfills $x_1>0$ and $x_{n+1}=\frac{1}{2}(x_n + \frac{a}{x_n})$ for $n \in \mathbb N$ . Show that $x_n \rightarrow \sqrt a$ when $n\rightarrow \infty$ . I have done it in two ways, but I guess I'm not allowed to use the first one and the second one is incomplete. Can someone please help me? We already know $x_n \rightarrow \sqrt a$ , so we do another step of the iteration and see that $x_{n+1} = \sqrt a$ . Using limit, $x_n \rightarrow x, x_{n+1} \rightarrow x$ (this is the part I think it's incomplete, don't I have to show $x_{n+1} \rightarrow x$ , how?), we have that $$x = \frac x 2 (1 + \frac a {x^2}) \Rightarrow 1 = a/x^2 \Rightarrow x = \sqrt a$$ b) Let the sequence $x_n$ be defined as $x_{n+1}= 1 + \frac 1 {x_n} (n \in \mathbb N), x_1=1$ . Show that it converges and calculate its limit. ""Tip: Show that sequences $x_{2n}$ and $x_{2n+1}$ monotone convergent to the limit."" I didn't understand the tip, how can this help me? Does it make a difference if the number is odd or even? Thanks in advance!","a) Let and the sequence fulfills and for . Show that when . I have done it in two ways, but I guess I'm not allowed to use the first one and the second one is incomplete. Can someone please help me? We already know , so we do another step of the iteration and see that . Using limit, (this is the part I think it's incomplete, don't I have to show , how?), we have that b) Let the sequence be defined as . Show that it converges and calculate its limit. ""Tip: Show that sequences and monotone convergent to the limit."" I didn't understand the tip, how can this help me? Does it make a difference if the number is odd or even? Thanks in advance!","a>0 x_n x_1>0 x_{n+1}=\frac{1}{2}(x_n + \frac{a}{x_n}) n \in \mathbb N x_n \rightarrow \sqrt a n\rightarrow \infty x_n \rightarrow \sqrt a x_{n+1} = \sqrt a x_n \rightarrow x, x_{n+1} \rightarrow x x_{n+1} \rightarrow x x = \frac x 2 (1 + \frac a {x^2}) \Rightarrow 1 = a/x^2 \Rightarrow x = \sqrt a x_n x_{n+1}= 1 + \frac 1 {x_n} (n \in \mathbb N), x_1=1 x_{2n} x_{2n+1}","['real-analysis', 'limits', 'recurrence-relations', 'radicals']"
67,"Prove $\int_{0}^{\pi/2} x\csc^2(x)\arctan \left(\alpha \tan x\right)\, dx = \frac{\pi}{2}\left[\ln\frac{(1+\alpha)^{1+\alpha}}{\alpha^\alpha}\right]$",Prove,"\int_{0}^{\pi/2} x\csc^2(x)\arctan \left(\alpha \tan x\right)\, dx = \frac{\pi}{2}\left[\ln\frac{(1+\alpha)^{1+\alpha}}{\alpha^\alpha}\right]","When I showed to my brother how I proved \begin{equation} \int_{0}^{\!\Large \frac{\pi}{2}} \ln \left(x^{2} + \ln^2\cos x\right) \, \mathrm{d}x=\pi\ln\ln2 \end{equation} using the following theorem by Mr. Olivier Oloa \begin{equation}{\large\int_{0}^{\!\Large \frac{\pi}{2}}} \frac{\cos \left( s \arctan \left(-\frac{x}{\ln \cos x}\right)\right)}{(x^2+\ln^2\! \cos x)^{\Large\frac{s}{2}}}\, \mathrm{d}x = \frac{\pi}{2}\frac{1}{\ln^{\Large s}\!2}\qquad,\;\text{for }-1<s<1.\end{equation} He showed me the following interesting formula \begin{equation} \int_{0}^{\!\Large \frac{\pi}{2}} x\csc^2(x)\arctan \left(\alpha \tan x\right)\, \mathrm{d}x =\frac{\pi}{2}\, \ln\left(\left[1 + \alpha\right]^{1 + \alpha} \over \alpha^\alpha\right)\,,\qquad \mbox{for}\ \alpha > 0\tag{✪}. \end{equation} I tried several values of $\alpha$ to check its validity ( since he always messes around with me ) and the numerical results match the output of Mathematica $9$ . The problem is how to prove this formula since he didn't tell me ( as always ). I tried Feynman's integration trick and I arrived to the following result: \begin{equation}\partial_\alpha\int_{0}^{\!\Large \frac{\pi}{2}} x\csc^2(x)\arctan \left(\alpha \tan x\right)\, \mathrm{d}x = \int_{0}^{\!\Large \frac{\pi}{2}} \frac{x\cot x}{\cos^2x+\alpha^2 \sin^2 x}\, \mathrm{d}x\end{equation} but I am having difficulty to crack the very last integral. Could anyone here please help me to prove the formula $(✪)$ preferably with elementary ways (high school methods)? Any help would be greatly appreciated. Thank you.",When I showed to my brother how I proved using the following theorem by Mr. Olivier Oloa He showed me the following interesting formula I tried several values of to check its validity ( since he always messes around with me ) and the numerical results match the output of Mathematica . The problem is how to prove this formula since he didn't tell me ( as always ). I tried Feynman's integration trick and I arrived to the following result: but I am having difficulty to crack the very last integral. Could anyone here please help me to prove the formula preferably with elementary ways (high school methods)? Any help would be greatly appreciated. Thank you.,"\begin{equation}
\int_{0}^{\!\Large \frac{\pi}{2}} \ln \left(x^{2} + \ln^2\cos x\right) \, \mathrm{d}x=\pi\ln\ln2
\end{equation} \begin{equation}{\large\int_{0}^{\!\Large \frac{\pi}{2}}} \frac{\cos \left( s \arctan \left(-\frac{x}{\ln \cos x}\right)\right)}{(x^2+\ln^2\! \cos x)^{\Large\frac{s}{2}}}\, \mathrm{d}x = \frac{\pi}{2}\frac{1}{\ln^{\Large s}\!2}\qquad,\;\text{for }-1<s<1.\end{equation} \begin{equation}
\int_{0}^{\!\Large \frac{\pi}{2}} x\csc^2(x)\arctan \left(\alpha \tan x\right)\, \mathrm{d}x
=\frac{\pi}{2}\,
\ln\left(\left[1 + \alpha\right]^{1 + \alpha}
\over \alpha^\alpha\right)\,,\qquad
\mbox{for}\ \alpha > 0\tag{✪}.
\end{equation} \alpha 9 \begin{equation}\partial_\alpha\int_{0}^{\!\Large \frac{\pi}{2}} x\csc^2(x)\arctan \left(\alpha \tan x\right)\, \mathrm{d}x = \int_{0}^{\!\Large \frac{\pi}{2}} \frac{x\cot x}{\cos^2x+\alpha^2 \sin^2 x}\, \mathrm{d}x\end{equation} (✪)","['calculus', 'real-analysis', 'integration', 'definite-integrals', 'closed-form']"
68,Number of elements in a finite $\sigma$-algebra,Number of elements in a finite -algebra,\sigma,"I have been asked to prove that the number of elements in a finite sigma algebra over a set $X$ is $2^n$ for some integer $n$. How do I go about this problem? I have no idea where to start. Thanks in advance for any ideas. Do I need to prove that given a set $F$, $\sigma(F)$ is actually a power set of some set say $S$?","I have been asked to prove that the number of elements in a finite sigma algebra over a set $X$ is $2^n$ for some integer $n$. How do I go about this problem? I have no idea where to start. Thanks in advance for any ideas. Do I need to prove that given a set $F$, $\sigma(F)$ is actually a power set of some set say $S$?",,"['real-analysis', 'measure-theory']"
69,Prove $\lim_{x \rightarrow 0} \frac {\sin(x)}{x} = 1$ with the epsilon-delta definition of limit.,Prove  with the epsilon-delta definition of limit.,\lim_{x \rightarrow 0} \frac {\sin(x)}{x} = 1,"It is well known that $$\lim_{x \rightarrow 0} \frac {\sin(x)}{x} = 1$$ I know several proofs of this: the geometric proof shows that $\cos(\theta)\leq\frac {\sin(\theta)}{\theta}\leq1$ and using the Squeeze Theorem I conclude that $\lim_{x \rightarrow 0} \frac {\sin(x)}{x} = 1$, other proof uses the Maclaurin series of $\sin(x)$. My question is: is there a demonstration of this limit using the epsilon-delta definition of limit?","It is well known that $$\lim_{x \rightarrow 0} \frac {\sin(x)}{x} = 1$$ I know several proofs of this: the geometric proof shows that $\cos(\theta)\leq\frac {\sin(\theta)}{\theta}\leq1$ and using the Squeeze Theorem I conclude that $\lim_{x \rightarrow 0} \frac {\sin(x)}{x} = 1$, other proof uses the Maclaurin series of $\sin(x)$. My question is: is there a demonstration of this limit using the epsilon-delta definition of limit?",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
70,Is every open subset of $ \mathbb{R} $ uncountable?,Is every open subset of  uncountable?, \mathbb{R} ,"Is every open subset of $\mathbb{R}$ uncountable?  I was crafting a proof for the theorem that states every open subset of $\mathbb{R}$ can be written as the union of a countable number of disjoint intervals when this question came up.  I feel like the answer is yes, but I'm not sure how to go about proving it or whether there is a crazy construction (like the Cantor Set)  that creates a countable, open subset of $\mathbb{R}$. Any ideas?","Is every open subset of $\mathbb{R}$ uncountable?  I was crafting a proof for the theorem that states every open subset of $\mathbb{R}$ can be written as the union of a countable number of disjoint intervals when this question came up.  I feel like the answer is yes, but I'm not sure how to go about proving it or whether there is a crazy construction (like the Cantor Set)  that creates a countable, open subset of $\mathbb{R}$. Any ideas?",,"['real-analysis', 'general-topology']"
71,“Pseudo-Cauchy” sequences: are they also Cauchy?,“Pseudo-Cauchy” sequences: are they also Cauchy?,,"I tried to prove something but I could not, I don't know if it's true or not, but I did not found a counterexample. Let $(a_n)$ be a sequence in a general metric space such that for any fixed $k \in \mathbb N$, we have $|a_{kn} - a_n| \to 0$.  Is $a_n$ necessarily a Cauchy sequence?","I tried to prove something but I could not, I don't know if it's true or not, but I did not found a counterexample. Let $(a_n)$ be a sequence in a general metric space such that for any fixed $k \in \mathbb N$, we have $|a_{kn} - a_n| \to 0$.  Is $a_n$ necessarily a Cauchy sequence?",,"['calculus', 'real-analysis', 'sequences-and-series', 'convergence-divergence', 'examples-counterexamples']"
72,How can construction of the gamma function be motivated?,How can construction of the gamma function be motivated?,,"The gamma function extends the factorial function. This can be proved inductively using integration-by-parts. But if you didn't already know that the gamma function had this property, and you wanted to construct a continuation of factorial a priori, how could you construct the gamma function? How could you motivate an exploration which would lead to this answer?","The gamma function extends the factorial function. This can be proved inductively using integration-by-parts. But if you didn't already know that the gamma function had this property, and you wanted to construct a continuation of factorial a priori, how could you construct the gamma function? How could you motivate an exploration which would lead to this answer?",,"['real-analysis', 'factorial', 'gamma-function', 'motivation']"
73,Proof that the continuous image of a compact set is compact [duplicate],Proof that the continuous image of a compact set is compact [duplicate],,"This question already has answers here : Continuous image of compact sets are compact (3 answers) Closed 9 years ago . Let $X\subset \mathbb R^{n}$ be a compact set, and $f :\mathbb R^{n}\to \mathbb R $ a continuous function. Then, $F(X)$ is a compact set. I know that this question may be a duplicate, but the problem is that I have to prove this using real analysis instead of topology. I'm struggling with proving that $F(X)$ is bounded. I know that the image of a continuous function is bounded, but I'm having trouble when it comes to prove this for vectorial functions. If somebody could help me with a step-to-step proof, that would be great.","This question already has answers here : Continuous image of compact sets are compact (3 answers) Closed 9 years ago . Let be a compact set, and a continuous function. Then, is a compact set. I know that this question may be a duplicate, but the problem is that I have to prove this using real analysis instead of topology. I'm struggling with proving that is bounded. I know that the image of a continuous function is bounded, but I'm having trouble when it comes to prove this for vectorial functions. If somebody could help me with a step-to-step proof, that would be great.",X\subset \mathbb R^{n} f :\mathbb R^{n}\to \mathbb R  F(X) F(X),"['real-analysis', 'analysis', 'proof-explanation', 'compactness']"
74,"Coming up with an example, a function that is continuous but not uniformly continuous","Coming up with an example, a function that is continuous but not uniformly continuous",,"What would be an example of a function that is continuous, but not uniformly continuous? Will $f(x)=\frac{1}{x}$ on the domain $(0,2)$ be an example? And why is it an example? Please explain strictly using relevant definitions.","What would be an example of a function that is continuous, but not uniformly continuous? Will $f(x)=\frac{1}{x}$ on the domain $(0,2)$ be an example? And why is it an example? Please explain strictly using relevant definitions.",,"['real-analysis', 'functions']"
75,True Definition of the Real Numbers,True Definition of the Real Numbers,,"I've found lots of resources that say this is a real number if it's not rational, but what is a real number, really?  I mean what is the definition of a real number?  If nothing else, anyone know of a resource where I could find out myself? Thanks!","I've found lots of resources that say this is a real number if it's not rational, but what is a real number, really?  I mean what is the definition of a real number?  If nothing else, anyone know of a resource where I could find out myself? Thanks!",,"['real-analysis', 'definition']"
76,"Sequence of monotone functions converging to a continuous limit, is the convergence uniform?","Sequence of monotone functions converging to a continuous limit, is the convergence uniform?",,"I'm reading some extreme value theory and in particular regular variation in Resnick's 1987 book Extreme Values, Regular Variation, and Point Processes , and several times he has claimed uniform convergence of a sequence of functions because ""monotone functions are converging pointwise to a continuous limit"". I am finding this reasoning a little dubious. EDIT: Thanks to some comments below I realized I was confused, I really want each $f_n$ to be a monotone function on $[a,b]$, so it isn't quite like Dini's theorem. Formally I am wondering: Let $f_n:[a,b]\rightarrow\mathbb{R}$ be a sequence of non-decreasing functions converging pointwise to the continuous function $f$. Is the convergence uniform? My thoughts: I have been thinking that since the set of discontinuities of each function $f_n$ is at most countable, the set of discontinuities of the entire sequence is at most countable...something something? Any thoughts would be greatly appreciated!","I'm reading some extreme value theory and in particular regular variation in Resnick's 1987 book Extreme Values, Regular Variation, and Point Processes , and several times he has claimed uniform convergence of a sequence of functions because ""monotone functions are converging pointwise to a continuous limit"". I am finding this reasoning a little dubious. EDIT: Thanks to some comments below I realized I was confused, I really want each $f_n$ to be a monotone function on $[a,b]$, so it isn't quite like Dini's theorem. Formally I am wondering: Let $f_n:[a,b]\rightarrow\mathbb{R}$ be a sequence of non-decreasing functions converging pointwise to the continuous function $f$. Is the convergence uniform? My thoughts: I have been thinking that since the set of discontinuities of each function $f_n$ is at most countable, the set of discontinuities of the entire sequence is at most countable...something something? Any thoughts would be greatly appreciated!",,['real-analysis']
77,If $\int_{\mathbb R^2} \frac{\vert f(x)-f(y)\vert}{\vert x-y\vert^2}dxdy<+\infty$ then $f$ is a.e. constant,If  then  is a.e. constant,\int_{\mathbb R^2} \frac{\vert f(x)-f(y)\vert}{\vert x-y\vert^2}dxdy<+\infty f,"Let $f \in L^1(\mathbb R)$. If    $$ \int_\mathbb R \int_\mathbb R \frac{\vert f(x)-f(y)\vert}{\vert x-y\vert^2}dxdy<+\infty $$   then $f$ is a.e. constant. I do not know how to begin. I thought that we are set if we show that the integrand is bounded a.e. (the function $f$ would be 2-holderian, hence constant)  but this is not true in general. I mean $\phi \in L^1(\mathbb R)$ does not imply $\phi \in L^{\infty}(\mathbb R)$. Would you please give me some useful hints in order to start? Thanks.","Let $f \in L^1(\mathbb R)$. If    $$ \int_\mathbb R \int_\mathbb R \frac{\vert f(x)-f(y)\vert}{\vert x-y\vert^2}dxdy<+\infty $$   then $f$ is a.e. constant. I do not know how to begin. I thought that we are set if we show that the integrand is bounded a.e. (the function $f$ would be 2-holderian, hence constant)  but this is not true in general. I mean $\phi \in L^1(\mathbb R)$ does not imply $\phi \in L^{\infty}(\mathbb R)$. Would you please give me some useful hints in order to start? Thanks.",,"['real-analysis', 'lebesgue-integral']"
78,$f\colon\mathbb{R}\to\mathbb{R}$ such that $f(x)+f(f(x))=x^2$ for all $x$?,such that  for all ?,f\colon\mathbb{R}\to\mathbb{R} f(x)+f(f(x))=x^2 x,"A friend came up with this problem, and we and a few others tried to solve it.  It turned out to be really hard, so one of us asked his professor.  I came with him, and it took me, him and the professor about an hour to say anything interesting about it. We figured out that for positive $x$, assuming $f$ exists and is differentiable, $f$ is monotonically increasing.  (Differentiating both sides gives $f'(x)*[\text{positive stuff}]=2x$).  So $f$ is invertible there.  We also figured out that f becomes arbitrarily large, and we guessed that it grows faster than any linear function.  Plugging in $f{-1}(x)$ for $x$ gives $x+f(x)=[f^{-1}(x)]^2$.  Since $f(x)$ grows faster than $x$, $f^{-1}$ grows slower and therefore $f(x)=[f^{-1}(x)]^2-x\le x^2$. Unfortunately, that's about all we know...  No one knew how to deal with the $f(f(x))$ term.  We don't even know if the equation has a solution.  How can you solve this problem, and how do you deal with repeated function applications in general?","A friend came up with this problem, and we and a few others tried to solve it.  It turned out to be really hard, so one of us asked his professor.  I came with him, and it took me, him and the professor about an hour to say anything interesting about it. We figured out that for positive $x$, assuming $f$ exists and is differentiable, $f$ is monotonically increasing.  (Differentiating both sides gives $f'(x)*[\text{positive stuff}]=2x$).  So $f$ is invertible there.  We also figured out that f becomes arbitrarily large, and we guessed that it grows faster than any linear function.  Plugging in $f{-1}(x)$ for $x$ gives $x+f(x)=[f^{-1}(x)]^2$.  Since $f(x)$ grows faster than $x$, $f^{-1}$ grows slower and therefore $f(x)=[f^{-1}(x)]^2-x\le x^2$. Unfortunately, that's about all we know...  No one knew how to deal with the $f(f(x))$ term.  We don't even know if the equation has a solution.  How can you solve this problem, and how do you deal with repeated function applications in general?",,"['real-analysis', 'functional-equations']"
79,Necessary and Sufficient Conditions for Riemann Integrability,Necessary and Sufficient Conditions for Riemann Integrability,,"A function is called Riemann integrable if and only if it is bounded and continuous almost everywhere on its domain.  However, I have read that the following two statements are also true: a) If $f$ is continuous then $f$ is Riemann integrable b) If $f$ is bounded then $f$ is Riemann integrable How exactly do these conditions fit together to give the necessary and sufficient condition first stated here?","A function is called Riemann integrable if and only if it is bounded and continuous almost everywhere on its domain.  However, I have read that the following two statements are also true: a) If $f$ is continuous then $f$ is Riemann integrable b) If $f$ is bounded then $f$ is Riemann integrable How exactly do these conditions fit together to give the necessary and sufficient condition first stated here?",,['real-analysis']
80,Does Bounded Covergence Theorem hold for Riemann integral?,Does Bounded Covergence Theorem hold for Riemann integral?,,"Just after studying the Bounded Convergence Theorem BCT for Lebesgue integral, I asked myself a question. Does the BCT hold for Riemann? I answered YES since the function is bounded according to the hypothesis of the BCT. But some Lebesgue integral are not Riemann, this is where I got confused, please I need a guide from experts in the field. Thanks. Statement of the BCT: Let $\{f_{n}\}$ be a sequence of measurable functions defined on a set $E$ of finite measure. Assume  $\{f_{n}\}$ converges to $f$ pointwise and also  $\{f_{n}\}$ is bounded for all $n$. Then $$\int_{E}f=\lim_{n \to \infty}\int_{E}f_{n}.$$","Just after studying the Bounded Convergence Theorem BCT for Lebesgue integral, I asked myself a question. Does the BCT hold for Riemann? I answered YES since the function is bounded according to the hypothesis of the BCT. But some Lebesgue integral are not Riemann, this is where I got confused, please I need a guide from experts in the field. Thanks. Statement of the BCT: Let $\{f_{n}\}$ be a sequence of measurable functions defined on a set $E$ of finite measure. Assume  $\{f_{n}\}$ converges to $f$ pointwise and also  $\{f_{n}\}$ is bounded for all $n$. Then $$\int_{E}f=\lim_{n \to \infty}\int_{E}f_{n}.$$",,"['real-analysis', 'measure-theory']"
81,Is equality inherently defined?,Is equality inherently defined?,,"I recently pulled out my old Real Analysis textbook and noticed something that didn't stand out when I was taking the class all those years ago.  When the book is listing out the axioms it seems to assume we understand what equality is. Consider the first axiom $(A)$ , which defines commutivity over addition and multiplication.  It states $x+y=y+x, \forall x,y \in \mathbb{R}$ and similiarly for multiplication. I understand that we're in process of defining how $+$ operates on $\mathbb{R}$ here, but we never defined what $=$ means.  Equality seems like a crucial concept, but I never see it defined anywhere. Is equality inherently defined or an understood concept, or does it have a more formal definition? EDIT It was commented that $x+y$ is not the same as $y+x$ , but rather for the case above they could be considered equal if they evaluate to the same real number.  My question has to do with the general sense, though.  It seems this axiom is defining a property of $+$ to say that these two terms can be treated the same, an idea which could be applied in other situations outside of the example of the $+$ operation and $\mathbb{R}$ .","I recently pulled out my old Real Analysis textbook and noticed something that didn't stand out when I was taking the class all those years ago.  When the book is listing out the axioms it seems to assume we understand what equality is. Consider the first axiom , which defines commutivity over addition and multiplication.  It states and similiarly for multiplication. I understand that we're in process of defining how operates on here, but we never defined what means.  Equality seems like a crucial concept, but I never see it defined anywhere. Is equality inherently defined or an understood concept, or does it have a more formal definition? EDIT It was commented that is not the same as , but rather for the case above they could be considered equal if they evaluate to the same real number.  My question has to do with the general sense, though.  It seems this axiom is defining a property of to say that these two terms can be treated the same, an idea which could be applied in other situations outside of the example of the operation and .","(A) x+y=y+x, \forall x,y \in \mathbb{R} + \mathbb{R} = x+y y+x + + \mathbb{R}","['real-analysis', 'logic']"
82,$f^2+(1+f')^2\leq 1 \implies f=0$,,f^2+(1+f')^2\leq 1 \implies f=0,"Find all $f\in C^1(\mathbb R,\mathbb R)$ such that $f^2+(1+f')^2\leq 1$ It's quite likely the answer is $f=0$. Note that $|f|\leq 1$ and $-2\leq f'\leq 0$. Therefore $f$ is decreasing and bounded. What then ? I tried contradiction, without success.","Find all $f\in C^1(\mathbb R,\mathbb R)$ such that $f^2+(1+f')^2\leq 1$ It's quite likely the answer is $f=0$. Note that $|f|\leq 1$ and $-2\leq f'\leq 0$. Therefore $f$ is decreasing and bounded. What then ? I tried contradiction, without success.",,['real-analysis']
83,"How to show that $\frac{\pi}{5}\leq\int_0^1 x^x\,dx\leq\frac{\pi}{4}$",How to show that,"\frac{\pi}{5}\leq\int_0^1 x^x\,dx\leq\frac{\pi}{4}","Show that: $$\frac{\pi}{5}\leq\int_0^1 x^x\,dx\leq\frac{\pi}{4}$$ All I've got so far is that the minimum of $x^x$ is $e^{-1/e}$. At this point I could  compare $\pi/5$ to  $e^{-1/e}$ but I'm required to prove both sides without using the calculator. This is all I've got at the moment.","Show that: $$\frac{\pi}{5}\leq\int_0^1 x^x\,dx\leq\frac{\pi}{4}$$ All I've got so far is that the minimum of $x^x$ is $e^{-1/e}$. At this point I could  compare $\pi/5$ to  $e^{-1/e}$ but I'm required to prove both sides without using the calculator. This is all I've got at the moment.",,"['calculus', 'real-analysis', 'inequality']"
84,Does every non empty open set has measure greater than zero?,Does every non empty open set has measure greater than zero?,,"Is it true that every non-empty open set has Lebesgue measure greater than zero? I could think of a proof along the following lines but not sure if that would be right: Since every non-empty open set is a finite or countable union of open intervals where at least one open interval is nonempty, and since every nonempty open interval has measure greater than zero implies every nonempty open set has measure greater than zero. Is my reasoning sound?","Is it true that every non-empty open set has Lebesgue measure greater than zero? I could think of a proof along the following lines but not sure if that would be right: Since every non-empty open set is a finite or countable union of open intervals where at least one open interval is nonempty, and since every nonempty open interval has measure greater than zero implies every nonempty open set has measure greater than zero. Is my reasoning sound?",,"['real-analysis', 'measure-theory']"
85,Dedekind cuts for $\pi$ and $e$,Dedekind cuts for  and,\pi e,I tried to search in the internet about this but did not get any exciting answers. So my question is: How is construction of transcendental numbers like  $\pi$ and $e$ explained via Dedekind cuts?,I tried to search in the internet about this but did not get any exciting answers. So my question is: How is construction of transcendental numbers like  $\pi$ and $e$ explained via Dedekind cuts?,,"['real-analysis', 'real-numbers']"
86,Distance from $x^n$ to lesser polynomials,Distance from  to lesser polynomials,x^n,"I am interested in the $L_1$ distance of $x^n$ to the $\mathbb R$-span of $\{1,x,\ldots,x^{n-1}\}$ over some interval. We can WLOG consider the interval $[0,1]$ (say) because scaling and shifting only affects the distance by a constant factor. So far we have these basic results: Theorem $x^n$ is not in the $\mathbb R$-span of $\{1,x,\ldots,x^{n-1}\}$ Proof Suppose it was: $x^n = \sum_{i=0}^{n-1} a_i x^i$. Differentiate $n$ times to get the contradiction $n! = 0$. We can rephrase this theorem as saying that $\| x^n - p(x) \| > 0$ for all polynomials of degree $< n$. Theorem There is no sequence of polynomials in the span, with limit $x^n$. Proof We would have a sequence $p_n(x) = \sum_{i=0}^{n-1} a_{i,n} x^i$ where $x^n = \lim_{n \to \infty} p_n(x) = \sum_{i=0}^{n-1} (\lim_{n \to \infty} a_{i,n}) x^i$, but that is a contradiction. Together with the previous result this implies there should be some positive $h(n)$ such that $\| x^n - p(x) \| > h$. My question is about finding some $h$. How could we go about doing this? Are there relevant theorems? My first idea was that we might use induction and find some way of saying: Since we can only get within $r$ of the derivative we can only get within $h(r)$ of the function itself.. but finding $h(r)$ seems just as hard as finding $h$ at all.","I am interested in the $L_1$ distance of $x^n$ to the $\mathbb R$-span of $\{1,x,\ldots,x^{n-1}\}$ over some interval. We can WLOG consider the interval $[0,1]$ (say) because scaling and shifting only affects the distance by a constant factor. So far we have these basic results: Theorem $x^n$ is not in the $\mathbb R$-span of $\{1,x,\ldots,x^{n-1}\}$ Proof Suppose it was: $x^n = \sum_{i=0}^{n-1} a_i x^i$. Differentiate $n$ times to get the contradiction $n! = 0$. We can rephrase this theorem as saying that $\| x^n - p(x) \| > 0$ for all polynomials of degree $< n$. Theorem There is no sequence of polynomials in the span, with limit $x^n$. Proof We would have a sequence $p_n(x) = \sum_{i=0}^{n-1} a_{i,n} x^i$ where $x^n = \lim_{n \to \infty} p_n(x) = \sum_{i=0}^{n-1} (\lim_{n \to \infty} a_{i,n}) x^i$, but that is a contradiction. Together with the previous result this implies there should be some positive $h(n)$ such that $\| x^n - p(x) \| > h$. My question is about finding some $h$. How could we go about doing this? Are there relevant theorems? My first idea was that we might use induction and find some way of saying: Since we can only get within $r$ of the derivative we can only get within $h(r)$ of the function itself.. but finding $h(r)$ seems just as hard as finding $h$ at all.",,['real-analysis']
87,Monotone functions and non-vanishing of derivative,Monotone functions and non-vanishing of derivative,,"The following result is well known: If $f$ is continuous on $[a, b]$, differentiable on $(a, b)$ and $f'$ is non-zero on $(a, b)$ then $f$ is strictly monotone on $[a, b]$. However if the derivative vanishes at a finite number of points in $(a, b)$ and apart from these derivative maintains a constant sign in $(a, b)$ then also the function is strictly monotone on $[a, b]$ (just split the interval into finite number of intervals using these points where derivative vanishes and $f$ is strictly monotone in same direction in each of these intervals). Let's suppose now that $f$ is strictly monotone and continuous in $[a, b]$ and differentiable in $(a, b)$. What can we say about set of points $$A = \{x \mid x \in (a, b), f'(x) = 0\}$$ Can it be infinite? Can it be uncountable? How large the set $A$ can be?","The following result is well known: If $f$ is continuous on $[a, b]$, differentiable on $(a, b)$ and $f'$ is non-zero on $(a, b)$ then $f$ is strictly monotone on $[a, b]$. However if the derivative vanishes at a finite number of points in $(a, b)$ and apart from these derivative maintains a constant sign in $(a, b)$ then also the function is strictly monotone on $[a, b]$ (just split the interval into finite number of intervals using these points where derivative vanishes and $f$ is strictly monotone in same direction in each of these intervals). Let's suppose now that $f$ is strictly monotone and continuous in $[a, b]$ and differentiable in $(a, b)$. What can we say about set of points $$A = \{x \mid x \in (a, b), f'(x) = 0\}$$ Can it be infinite? Can it be uncountable? How large the set $A$ can be?",,"['calculus', 'real-analysis']"
88,"In what sense is the derivative the ""best"" linear approximation?","In what sense is the derivative the ""best"" linear approximation?",,"I am familiar with the definition of the Frechet derivative and it's uniqueness if it exists. I would however like to know, how the derivative is the ""best"" linear approximation. What does this mean formally? The ""best"" on the entire domain is surely wrong, so it must mean the ""best"" on a small neighborhood of the point we are differentiating at, where this neighborhood becomes arbitrarily small? Why does the definition of the derivative formalize precisely this? Thank you in advance.","I am familiar with the definition of the Frechet derivative and it's uniqueness if it exists. I would however like to know, how the derivative is the ""best"" linear approximation. What does this mean formally? The ""best"" on the entire domain is surely wrong, so it must mean the ""best"" on a small neighborhood of the point we are differentiating at, where this neighborhood becomes arbitrarily small? Why does the definition of the derivative formalize precisely this? Thank you in advance.",,"['real-analysis', 'numerical-methods', 'derivatives']"
89,Limit of ${a_n}^{1/n}$ is equal to $\lim_{n\to\infty} a_{n+1}/a_n$,Limit of  is equal to,{a_n}^{1/n} \lim_{n\to\infty} a_{n+1}/a_n,"Today my lecturer put up on the board that: If $\lim\limits_{n\to\infty}\frac{a_{n+1}}{a_n}$ exists and $a_n>0$ then $\displaystyle \limsup\limits_{n\to\infty}\left(a_n^{\frac{1}{n}}\right)=\lim_{n\to\infty}\frac{a_{n+1}}{a_n}$ however I am not sure why this is true, can somebody give me a hint or something as to how to go about proving this. thanks for any help","Today my lecturer put up on the board that: If $\lim\limits_{n\to\infty}\frac{a_{n+1}}{a_n}$ exists and $a_n>0$ then $\displaystyle \limsup\limits_{n\to\infty}\left(a_n^{\frac{1}{n}}\right)=\lim_{n\to\infty}\frac{a_{n+1}}{a_n}$ however I am not sure why this is true, can somebody give me a hint or something as to how to go about proving this. thanks for any help",,"['real-analysis', 'limits', 'radicals']"
90,Who came up with the $\varepsilon$-$\delta$ definitions and the axioms in Real Analysis?,Who came up with the - definitions and the axioms in Real Analysis?,\varepsilon \delta,"I've seen a lot of definitions of notions like boundary points, accumulation points, continuity, etc, and axioms for the set of the real numbers. But I have a hard time accepting these as ""true"" definitions or acceptable axioms and because of this it's awfully hard to believe that I can ""prove"" anything from them. It feels like I can create a close approximation to things found in calculus, but it feels like I'm constructing a forgery rather than proving. What I'm looking for is a way to discover these things on my own rather than have someone tell them to me. For instance, if I want to derive the area of a circle and I know the definition of $\pi$ and an integral, I can figure it out.","I've seen a lot of definitions of notions like boundary points, accumulation points, continuity, etc, and axioms for the set of the real numbers. But I have a hard time accepting these as ""true"" definitions or acceptable axioms and because of this it's awfully hard to believe that I can ""prove"" anything from them. It feels like I can create a close approximation to things found in calculus, but it feels like I'm constructing a forgery rather than proving. What I'm looking for is a way to discover these things on my own rather than have someone tell them to me. For instance, if I want to derive the area of a circle and I know the definition of $\pi$ and an integral, I can figure it out.",,"['calculus', 'real-analysis', 'analysis', 'soft-question', 'math-history']"
91,Locally Lipschitz implies continuity. Does the converse implication hold?,Locally Lipschitz implies continuity. Does the converse implication hold?,,"Let $A$ be open in $\mathbb{R}^m$ ; let $g:A\rightarrow\mathbb{R}^n$ . If $S\subseteq A$ , we say that $S$ satisfies the Lipschitz condition on $S$ if the function $\lambda(x,y)=|g(x)-g(y)|/|x-y|$ is bounded for $x\neq y\in S$ . We say that $g$ is locally Lipschitz if each point of $A$ has a neighborhood on which $g$ satisfies the Lipschitz condition. Show that if $g$ is locally Lipschitz, then $g$ is continuous. Does the converse hold? For the first part, suppose $g$ is locally Lipschitz. So for each point $r\in A$ , there exists a neighborhood for which $|g(x)-g(y)|/|x-y|$ is bounded. Suppose $|g(x)-g(y)|/|x-y|<M$ in that neighborhood. Then $|g(x)-g(r)|<M|x-r|$ in that neighborhood of $r$ . Therefore $g(x)\rightarrow g(r)$ as $x\rightarrow r$ , and so $g$ is continuous at $r$ . This means $g$ is continuous everywhere in $A$ . What about the converse? I don't think it holds, but can't come up with a counterexample.","Let be open in ; let . If , we say that satisfies the Lipschitz condition on if the function is bounded for . We say that is locally Lipschitz if each point of has a neighborhood on which satisfies the Lipschitz condition. Show that if is locally Lipschitz, then is continuous. Does the converse hold? For the first part, suppose is locally Lipschitz. So for each point , there exists a neighborhood for which is bounded. Suppose in that neighborhood. Then in that neighborhood of . Therefore as , and so is continuous at . This means is continuous everywhere in . What about the converse? I don't think it holds, but can't come up with a counterexample.","A \mathbb{R}^m g:A\rightarrow\mathbb{R}^n S\subseteq A S S \lambda(x,y)=|g(x)-g(y)|/|x-y| x\neq y\in S g A g g g g r\in A |g(x)-g(y)|/|x-y| |g(x)-g(y)|/|x-y|<M |g(x)-g(r)|<M|x-r| r g(x)\rightarrow g(r) x\rightarrow r g r g A",['real-analysis']
92,Measurability of the inverse of a measurable function,Measurability of the inverse of a measurable function,,"Let $m$ denote Lebesgue measure, and let $f:[0,1] \to [0,1]$ be a (Lebesgue) measurable and bijective function. In general, it is not true that $f^{-1}$ is measurable. However, suppose that we now have the condition that $\forall A \subset [0,1]$, $m(A) = 0 \Rightarrow m(f(A)) = 0$. Why does this condition guarantee the measurability of $f^{-1}$?","Let $m$ denote Lebesgue measure, and let $f:[0,1] \to [0,1]$ be a (Lebesgue) measurable and bijective function. In general, it is not true that $f^{-1}$ is measurable. However, suppose that we now have the condition that $\forall A \subset [0,1]$, $m(A) = 0 \Rightarrow m(f(A)) = 0$. Why does this condition guarantee the measurability of $f^{-1}$?",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
93,Books in the spirit of Problems and Theorems in Analysis by George Pólya and Gábor Szegő,Books in the spirit of Problems and Theorems in Analysis by George Pólya and Gábor Szegő,,"In the Preface of the first German Edition of the book Problems and Theorems in Analysis by George Pólya and Gábor Szegő, one can read [emphasis mine] : The chief aim of this book, which we trust is not unrealistic, is to    accustom advanced students of mathematics, through systematically    arranged problems in some important fields of analysis, to the ways    and means of independent thought and research . It is intended to serve   the need for individual active study on the part of both the student   and  the teacher. The book may be used by the student to extend his   own  reading or lecture material, or he may work quite independently   through  selected portions of the book in detail. The instructor may   use it as an  aid in organizing tutorials or seminars. This book is no mere collection of problems. Its most important feature is the systematic arrangement of the material which aims to  stimulate the   reader to independent work and to suggest to him useful  lines of   thought .   [...] The origin of the material is highly varied . We have made selections    from the classical body of knowledge of mathematics and also from    treatises of more recent date . We collected problems which had in part   already been published in various periodicals and in part communicated   to us verbally by their authors. We have adapted the material to our    purpose, completed, reformulated and substantially expanded it. In    addition we have published here for the first time in the form of   problems  a number of our own original results. We thus hope to be   able to offer something new even to the expert . To put it briefly, I think that the description of the key features that distinguish this book which is given in the preface is perfectly appropriate. With respect to these aspects, I regard this book as a sort of introduction to the methods, the ideas, and the results of mathematical research . It is actually the first book of this kind which I have ever come across, and I've really enjoyed going through it. So, the natural question that I pose is: $\color{#c00}{\text{Question:}}$ Are there other similar books (in analysis or also different topics )     that are in the spirit of Pólya and Szegő's work? That is, are there     other books that are actually introductions to mathematical research in the sense which is implied by the emphasized parts of the passage     quoted above ?","In the Preface of the first German Edition of the book Problems and Theorems in Analysis by George Pólya and Gábor Szegő, one can read [emphasis mine] : The chief aim of this book, which we trust is not unrealistic, is to    accustom advanced students of mathematics, through systematically    arranged problems in some important fields of analysis, to the ways    and means of independent thought and research . It is intended to serve   the need for individual active study on the part of both the student   and  the teacher. The book may be used by the student to extend his   own  reading or lecture material, or he may work quite independently   through  selected portions of the book in detail. The instructor may   use it as an  aid in organizing tutorials or seminars. This book is no mere collection of problems. Its most important feature is the systematic arrangement of the material which aims to  stimulate the   reader to independent work and to suggest to him useful  lines of   thought .   [...] The origin of the material is highly varied . We have made selections    from the classical body of knowledge of mathematics and also from    treatises of more recent date . We collected problems which had in part   already been published in various periodicals and in part communicated   to us verbally by their authors. We have adapted the material to our    purpose, completed, reformulated and substantially expanded it. In    addition we have published here for the first time in the form of   problems  a number of our own original results. We thus hope to be   able to offer something new even to the expert . To put it briefly, I think that the description of the key features that distinguish this book which is given in the preface is perfectly appropriate. With respect to these aspects, I regard this book as a sort of introduction to the methods, the ideas, and the results of mathematical research . It is actually the first book of this kind which I have ever come across, and I've really enjoyed going through it. So, the natural question that I pose is: $\color{#c00}{\text{Question:}}$ Are there other similar books (in analysis or also different topics )     that are in the spirit of Pólya and Szegő's work? That is, are there     other books that are actually introductions to mathematical research in the sense which is implied by the emphasized parts of the passage     quoted above ?",,"['real-analysis', 'reference-request', 'soft-question', 'big-list', 'book-recommendation']"
94,Dual space of $H^1$,Dual space of,H^1,"It holds that $W^{1,2}=H^1 \subset L^2 \subset H^{-1}$. This is clear since for every $v \in H^1(U)$, $u \mapsto (u,v)_{H^1}$ is an element of $H^{-1}$. Moreover for every $v \in L^2(U)$, $u \mapsto (u,v)_{L^2}$ is an element of $H^{-1}$. But I also know that $H^1$ is a Hilbert space and therefore it is isomorphic to its dual by the Riesz theorem. My question is: how can there be $H^1(U) \subset L^2(U) \subset H^{-1}$ as well as $H^{-1}$ can be identified with $H^1(U)$?","It holds that $W^{1,2}=H^1 \subset L^2 \subset H^{-1}$. This is clear since for every $v \in H^1(U)$, $u \mapsto (u,v)_{H^1}$ is an element of $H^{-1}$. Moreover for every $v \in L^2(U)$, $u \mapsto (u,v)_{L^2}$ is an element of $H^{-1}$. But I also know that $H^1$ is a Hilbert space and therefore it is isomorphic to its dual by the Riesz theorem. My question is: how can there be $H^1(U) \subset L^2(U) \subset H^{-1}$ as well as $H^{-1}$ can be identified with $H^1(U)$?",,"['real-analysis', 'functional-analysis', 'sobolev-spaces']"
95,Concave implies subadditive [duplicate],Concave implies subadditive [duplicate],,"This question already has answers here : If $f$ is a concave function and $f(0) \geq 0$ then $f(a+b) \leq f(a) + f(b)$ for all $a,b >0$ (3 answers) Closed 3 years ago . Let $f: [0, \infty) \to [0,\infty)$ be concave, meaning;  $$ f(tx + (1-t)y) \ge tf(x) + (1-t)f(y)$$ for $t \in [0,1]$. Also, assume $f(0) = 0$. I trying to show $f(x+y) \le f(x) + f(y)$ but I fail. Is there any advise you can give me for help? I appreciate.","This question already has answers here : If $f$ is a concave function and $f(0) \geq 0$ then $f(a+b) \leq f(a) + f(b)$ for all $a,b >0$ (3 answers) Closed 3 years ago . Let $f: [0, \infty) \to [0,\infty)$ be concave, meaning;  $$ f(tx + (1-t)y) \ge tf(x) + (1-t)f(y)$$ for $t \in [0,1]$. Also, assume $f(0) = 0$. I trying to show $f(x+y) \le f(x) + f(y)$ but I fail. Is there any advise you can give me for help? I appreciate.",,"['real-analysis', 'convex-analysis']"
96,Real Analysis. Suggestions.,Real Analysis. Suggestions.,,"Let $f,g: [a,b] \longrightarrow \mathbb{R}$ continuous in $[a,b]$ and differentiable in $(a,b)$ such that $f(a) = f(b) = 0$, then, theres exist $c \in (a,b)$ such that:   $$g'(c)f(c) + f'(c) = 0.$$ I tried to use the Rolle's Theorem and Means Value Theorem, but I couldn't define an auxiliary function $\varphi$ to help me. I didn't want a solution of exercise, just a suggestion.","Let $f,g: [a,b] \longrightarrow \mathbb{R}$ continuous in $[a,b]$ and differentiable in $(a,b)$ such that $f(a) = f(b) = 0$, then, theres exist $c \in (a,b)$ such that:   $$g'(c)f(c) + f'(c) = 0.$$ I tried to use the Rolle's Theorem and Means Value Theorem, but I couldn't define an auxiliary function $\varphi$ to help me. I didn't want a solution of exercise, just a suggestion.",,"['real-analysis', 'derivatives']"
97,Intuition for the Universal Chord Theorem,Intuition for the Universal Chord Theorem,,"So the Universal Chord theorem is a statement and proof that; The numbers of the form $r = \displaystyle \frac{1}{n} \ \ n \ge 1$ are the only numbers such that for any continuous function $\displaystyle f:[0,1] \to \mathbb{R}$ such that $\displaystyle f(0) = f(1)$, there is some point $\displaystyle c \in [0,1]$ such that $\displaystyle f(c) = f(c+r)$. The proof is straightforward to understand. I don't have difficulty with any step in it or anything. As well, I understand the counterexamples for any non rational, and why they fail. But even still it seems absurd that this is how it is. I've been drawing a bunch of graphs here on a page and I just can't see any sort of insight as to why rationals should work but not non rationals. I heard once that there was intuition to be found with topology, but I can't find it anywhere online. Could someone please enlighten me as to why this theorem makes sense intuitively? (without just giving a proof)","So the Universal Chord theorem is a statement and proof that; The numbers of the form $r = \displaystyle \frac{1}{n} \ \ n \ge 1$ are the only numbers such that for any continuous function $\displaystyle f:[0,1] \to \mathbb{R}$ such that $\displaystyle f(0) = f(1)$, there is some point $\displaystyle c \in [0,1]$ such that $\displaystyle f(c) = f(c+r)$. The proof is straightforward to understand. I don't have difficulty with any step in it or anything. As well, I understand the counterexamples for any non rational, and why they fail. But even still it seems absurd that this is how it is. I've been drawing a bunch of graphs here on a page and I just can't see any sort of insight as to why rationals should work but not non rationals. I heard once that there was intuition to be found with topology, but I can't find it anywhere online. Could someone please enlighten me as to why this theorem makes sense intuitively? (without just giving a proof)",,['real-analysis']
98,"Is there a continuous $f(x,y)$ which is not of the form $f(x,y) = g_1(x) h_1(y) + \dots + g_n(x) h_n(y)$",Is there a continuous  which is not of the form,"f(x,y) f(x,y) = g_1(x) h_1(y) + \dots + g_n(x) h_n(y)","For a continuous function $f : [0,1]^2 \to \mathbb{R}$, let us say $f$ is a sum of products (SOP) if there exist an integer $n > 0$ and continuous functions $g_1, \dots, g_n, h_1, \dots, h_n : [0,1] \to \mathbb{R}$ such that $$f(x,y) = g_1(x) h_1(y) + \dots + g_n(x) h_n(y)$$ for all $x,y \in [0,1]$. How can I show there exists a continuous $f$ which is not an SOP? I think this should be easy, but for some reason I don't see how to proceed. Note that the Stone-Weierstrass theorem says that every continuous function on $[0,1]^2$ is a uniform limit of SOPs.  I want to see that you can't drop the limit. Is the same true if we replace $[0,1]$ by an arbitrary infinite compact Hausdorff space?","For a continuous function $f : [0,1]^2 \to \mathbb{R}$, let us say $f$ is a sum of products (SOP) if there exist an integer $n > 0$ and continuous functions $g_1, \dots, g_n, h_1, \dots, h_n : [0,1] \to \mathbb{R}$ such that $$f(x,y) = g_1(x) h_1(y) + \dots + g_n(x) h_n(y)$$ for all $x,y \in [0,1]$. How can I show there exists a continuous $f$ which is not an SOP? I think this should be easy, but for some reason I don't see how to proceed. Note that the Stone-Weierstrass theorem says that every continuous function on $[0,1]^2$ is a uniform limit of SOPs.  I want to see that you can't drop the limit. Is the same true if we replace $[0,1]$ by an arbitrary infinite compact Hausdorff space?",,"['real-analysis', 'functional-analysis']"
99,Proving $\text{Li}_3\left(-\frac{1}{3}\right)-2 \text{Li}_3\left(\frac{1}{3}\right)= -\frac{\log^33}{6}+\frac{\pi^2}{6}\log 3-\frac{13\zeta(3)}{6}$?,Proving ?,\text{Li}_3\left(-\frac{1}{3}\right)-2 \text{Li}_3\left(\frac{1}{3}\right)= -\frac{\log^33}{6}+\frac{\pi^2}{6}\log 3-\frac{13\zeta(3)}{6},"Ramanujan gave the following identities for the Dilogarithm function : $$ \begin{align*} \operatorname{Li}_2\left(\frac{1}{3}\right)-\frac{1}{6}\operatorname{Li}_2\left(\frac{1}{9}\right) &=\frac{{\pi}^2}{18}-\frac{\log^23}{6} \\ \operatorname{Li}_2\left(-\frac{1}{3}\right)-\frac{1}{3}\operatorname{Li}_2\left(\frac{1}{9}\right) &=-\frac{{\pi}^2}{18}+\frac{1}{6}\log^23  \end{align*} $$ Now, I was wondering if there are similar identities for the trilogarithm ? I found numerically that $$\text{Li}_3\left(-\frac{1}{3}\right)-2 \text{Li}_3\left(\frac{1}{3}\right)\stackrel?= -\frac{\log^3 3}{6}+\frac{\pi^2}{6}\log 3-\frac{13\zeta(3)}{6} \tag{1}$$ I was not able to find equation $(1)$ anywhere in literature. Is it a new result? How can we prove $(1)$? I believe that it must be true since it agrees to a lot of decimal places.","Ramanujan gave the following identities for the Dilogarithm function : $$ \begin{align*} \operatorname{Li}_2\left(\frac{1}{3}\right)-\frac{1}{6}\operatorname{Li}_2\left(\frac{1}{9}\right) &=\frac{{\pi}^2}{18}-\frac{\log^23}{6} \\ \operatorname{Li}_2\left(-\frac{1}{3}\right)-\frac{1}{3}\operatorname{Li}_2\left(\frac{1}{9}\right) &=-\frac{{\pi}^2}{18}+\frac{1}{6}\log^23  \end{align*} $$ Now, I was wondering if there are similar identities for the trilogarithm ? I found numerically that $$\text{Li}_3\left(-\frac{1}{3}\right)-2 \text{Li}_3\left(\frac{1}{3}\right)\stackrel?= -\frac{\log^3 3}{6}+\frac{\pi^2}{6}\log 3-\frac{13\zeta(3)}{6} \tag{1}$$ I was not able to find equation $(1)$ anywhere in literature. Is it a new result? How can we prove $(1)$? I believe that it must be true since it agrees to a lot of decimal places.",,"['real-analysis', 'sequences-and-series', 'analysis', 'special-functions', 'polylogarithm']"
