,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Between any two continuous functions $f>g$, can we find a real-analytic function?","Between any two continuous functions , can we find a real-analytic function?",f>g,"I asked myself a question which I thought was interesting, but I'm not sure how to approach it. The Question The question is, given two continuous functions $f,g:\mathbb{R}\rightarrow\mathbb{R}$ such that $f > g$, is there a real analytic function $h$ between $f$ and $g$ (so that $f > h > g$)? An easier version of the question can be asked over closed intervals. Given two continuous functions $f,g:[a,b]\rightarrow\mathbb{R}$ such that $f > g$, is there a real analytic function $h$ between $f$ and $g$? In this case, the answer is yes, and the result is actually stronger than expected. Essentially, this is an application of the Weierstrass Approximation Theorem. By the extreme value theorem, the function $H(x) = (f(x) - g(x))/2$ has a   minimum $M > 0$. You can define $k(x) = (f(x) + g(x))/2$ and then apply the Weierstrass Approximation Theorem to get a polynomial $p:[a,b]\rightarrow\mathbb{R}$ such that $\lVert p - k\rVert < M$.   Then $p$ is a polynomial such that $p(x)\in (k(x) - M, k(x) + M)\subseteq (g(x), f(x))$ for all $x\in[a,b]$. However, you run into issues when the domain is the real line. Here are some thoughts I had so far. For I am aware that there is a generalized Stone-Weierstrass Theorem on locally compact Hausdorff spaces $X$ concerning functions in $C_{0}(X)$. There are two issues though. The first is that our continuous functions $f,g$ don't necessarily tend to zero as $x\rightarrow\pm\infty$. The second is that the difference $f(x) - g(x)$ is allowed to get arbitrarily small as $x\rightarrow\pm\infty$. The first issue isn't major to me, because I think I have a way to evade it. For all intents and purposes, we can assume that $f,g$ tend to zero if it suits our needs. The second issue, though, seems to be fatal, and it seems like there is no way to modify the Stone-Weierstrass Theorem appropriately. This makes me think I need a completely different approach. Against If I want to make a counterexample, maybe I can use pathological functions like the Weierstrass function $w:\mathbb{R}\rightarrow\mathbb{R}$. I'm thinking that if we take $f(x) = w(x) + e^{-x^{2}}$ and $g(x) = w(x)$, we can force any intermediate function $h$ to become ""more and more detailed"" as it gets sandwiched closer and closer between $f,g$ for large $x$. Then this function might become ""too detailed"" to be analytic, but I have no rigorous way to express this idea. Even worse, I'm having doubts that this would go anywhere because there is always ""wiggle room"" between $f$ and $g$, so maybe we can fit an analytic function between them after all. Are there any suggestions on how to make progress on this question?","I asked myself a question which I thought was interesting, but I'm not sure how to approach it. The Question The question is, given two continuous functions $f,g:\mathbb{R}\rightarrow\mathbb{R}$ such that $f > g$, is there a real analytic function $h$ between $f$ and $g$ (so that $f > h > g$)? An easier version of the question can be asked over closed intervals. Given two continuous functions $f,g:[a,b]\rightarrow\mathbb{R}$ such that $f > g$, is there a real analytic function $h$ between $f$ and $g$? In this case, the answer is yes, and the result is actually stronger than expected. Essentially, this is an application of the Weierstrass Approximation Theorem. By the extreme value theorem, the function $H(x) = (f(x) - g(x))/2$ has a   minimum $M > 0$. You can define $k(x) = (f(x) + g(x))/2$ and then apply the Weierstrass Approximation Theorem to get a polynomial $p:[a,b]\rightarrow\mathbb{R}$ such that $\lVert p - k\rVert < M$.   Then $p$ is a polynomial such that $p(x)\in (k(x) - M, k(x) + M)\subseteq (g(x), f(x))$ for all $x\in[a,b]$. However, you run into issues when the domain is the real line. Here are some thoughts I had so far. For I am aware that there is a generalized Stone-Weierstrass Theorem on locally compact Hausdorff spaces $X$ concerning functions in $C_{0}(X)$. There are two issues though. The first is that our continuous functions $f,g$ don't necessarily tend to zero as $x\rightarrow\pm\infty$. The second is that the difference $f(x) - g(x)$ is allowed to get arbitrarily small as $x\rightarrow\pm\infty$. The first issue isn't major to me, because I think I have a way to evade it. For all intents and purposes, we can assume that $f,g$ tend to zero if it suits our needs. The second issue, though, seems to be fatal, and it seems like there is no way to modify the Stone-Weierstrass Theorem appropriately. This makes me think I need a completely different approach. Against If I want to make a counterexample, maybe I can use pathological functions like the Weierstrass function $w:\mathbb{R}\rightarrow\mathbb{R}$. I'm thinking that if we take $f(x) = w(x) + e^{-x^{2}}$ and $g(x) = w(x)$, we can force any intermediate function $h$ to become ""more and more detailed"" as it gets sandwiched closer and closer between $f,g$ for large $x$. Then this function might become ""too detailed"" to be analytic, but I have no rigorous way to express this idea. Even worse, I'm having doubts that this would go anywhere because there is always ""wiggle room"" between $f$ and $g$, so maybe we can fit an analytic function between them after all. Are there any suggestions on how to make progress on this question?",,"['real-analysis', 'analysis', 'power-series']"
1,Relax Egoroff's Theorem to pointwise convergence a.e. and bounded a.e. pointwise limit,Relax Egoroff's Theorem to pointwise convergence a.e. and bounded a.e. pointwise limit,,"The following question is taken from Royden's Real Analysis $4$th edition, Chapter $3,$ question $28,$ page $67:$ Question: Show that Egoroff's Theorem continues to hold if the convergence is pointwise a.e. and $f$ is finite a.e., that is. Assume $E$ has finite measure. Let $\{f_n\}$ be a sequence of measurable functions on $E$ that converges pointwise almost everywhere on $E$ to the real-valued function $f$ which is finite almost everywhere. Then for each $\varepsilon>0,$ there is a closed set $F$ contained in $E$ for which    $$\{f_n\}\to f \text{ uniformly on }F \text{ and }m(E\setminus F)<\varepsilon.$$ My attempt: Let $A = \{ x\in E: f_n\not\to f \text{ pointwise} \}$ and $B=\{ x\in E: |f|=\infty \}.$  By assumption,  $$m(A)=m(B)=0.$$ So $A$ and $B$ are measurable.  Note that  $$m[E\setminus (A\cup B)] = m(E) - m(A\cup B) = m(E).$$ Fix $\varepsilon>0.$ Since $\{f_n\}$ converges to $f$ pointwise on $E\setminus (A\cup B)$ to the real-valued function $f,$ by Egoroff's Theorem, there exists a closed set $F$ contained in $(E\setminus (A\cup B))\subseteq E$ for which  $$f_n\to f \text{ uniformly on } F \text{ and }m[(E\setminus(A\cup B)) \setminus F] < \varepsilon.$$ Since $E$ has finite measure and $F\subseteq E,$ by monotonicity,  $F$ has finite measure. As $F$ is closed, it is also measurable.  Therefore, by Excision property, we have $$m(E\setminus F) = m(E) - m(F) = m[E\setminus(A\cup B)] - m(F) = m[(E\setminus(A\cup B)) \setminus F]<\varepsilon.$$ Is my proof correct? I ask for verification because in this post Kenny's comment about countable union of null sets. I do not use this anywhere in my proof. So I wonder whether my proof miss out something.","The following question is taken from Royden's Real Analysis $4$th edition, Chapter $3,$ question $28,$ page $67:$ Question: Show that Egoroff's Theorem continues to hold if the convergence is pointwise a.e. and $f$ is finite a.e., that is. Assume $E$ has finite measure. Let $\{f_n\}$ be a sequence of measurable functions on $E$ that converges pointwise almost everywhere on $E$ to the real-valued function $f$ which is finite almost everywhere. Then for each $\varepsilon>0,$ there is a closed set $F$ contained in $E$ for which    $$\{f_n\}\to f \text{ uniformly on }F \text{ and }m(E\setminus F)<\varepsilon.$$ My attempt: Let $A = \{ x\in E: f_n\not\to f \text{ pointwise} \}$ and $B=\{ x\in E: |f|=\infty \}.$  By assumption,  $$m(A)=m(B)=0.$$ So $A$ and $B$ are measurable.  Note that  $$m[E\setminus (A\cup B)] = m(E) - m(A\cup B) = m(E).$$ Fix $\varepsilon>0.$ Since $\{f_n\}$ converges to $f$ pointwise on $E\setminus (A\cup B)$ to the real-valued function $f,$ by Egoroff's Theorem, there exists a closed set $F$ contained in $(E\setminus (A\cup B))\subseteq E$ for which  $$f_n\to f \text{ uniformly on } F \text{ and }m[(E\setminus(A\cup B)) \setminus F] < \varepsilon.$$ Since $E$ has finite measure and $F\subseteq E,$ by monotonicity,  $F$ has finite measure. As $F$ is closed, it is also measurable.  Therefore, by Excision property, we have $$m(E\setminus F) = m(E) - m(F) = m[E\setminus(A\cup B)] - m(F) = m[(E\setminus(A\cup B)) \setminus F]<\varepsilon.$$ Is my proof correct? I ask for verification because in this post Kenny's comment about countable union of null sets. I do not use this anywhere in my proof. So I wonder whether my proof miss out something.",,"['real-analysis', 'measure-theory', 'proof-verification']"
2,Proving convergent sequences are Cauchy sequences,Proving convergent sequences are Cauchy sequences,,"Prove that if $x_n \rightarrow a, n \rightarrow \infty$ then $\{x_n\}$ is a Cauchy sequence. I believe I have found the proof as follows, wondering if there are any simpler methods or added intuition. For me, it makes sense that if a sequence has a limit, then distances between elements in the sequence must be getting smaller, in order for it to converge. Given $\epsilon > 0, \exists N_1 \ s.t. \ \forall n \geq N_1:$ $|x_n - a| < \frac{\epsilon}{2} < \epsilon$ and for $m > n \geq N_1$ we also have: $|x_m -a| < \frac{\epsilon}{2} < \epsilon$ Let $N \geq N_1$, then $\forall n,m \geq N$ we have: $|x_n-x_m| = |x_n - a -x_m+a| < |x_n - a| + |-(x_m -a)| < \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon$ Therefore $\{x_n\}$ is a Cauchy sequence. Also, if a sequence is Cauchy does it always converge ? In other words, is it sufficient to check if a sequence is Cauchy to check for convergence.","Prove that if $x_n \rightarrow a, n \rightarrow \infty$ then $\{x_n\}$ is a Cauchy sequence. I believe I have found the proof as follows, wondering if there are any simpler methods or added intuition. For me, it makes sense that if a sequence has a limit, then distances between elements in the sequence must be getting smaller, in order for it to converge. Given $\epsilon > 0, \exists N_1 \ s.t. \ \forall n \geq N_1:$ $|x_n - a| < \frac{\epsilon}{2} < \epsilon$ and for $m > n \geq N_1$ we also have: $|x_m -a| < \frac{\epsilon}{2} < \epsilon$ Let $N \geq N_1$, then $\forall n,m \geq N$ we have: $|x_n-x_m| = |x_n - a -x_m+a| < |x_n - a| + |-(x_m -a)| < \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon$ Therefore $\{x_n\}$ is a Cauchy sequence. Also, if a sequence is Cauchy does it always converge ? In other words, is it sufficient to check if a sequence is Cauchy to check for convergence.",,"['real-analysis', 'limits', 'proof-verification', 'cauchy-sequences']"
3,Does a glass of water sing because of the SO(2) symmetry?,Does a glass of water sing because of the SO(2) symmetry?,,"This might very well be a crucially flawed reasoning. But I think has to have something true behind. I was trying to explain basic ideas of representation of Lie Groups to an 11 years old girl who asked what I was studying. What I wanted to explain was the relation between special functions and symmetries. The mathematical thing I wanted to explain was that if we have a group $G$ acting on a space $X$, and we look at the space of infinitely derivable functions on $X$, i.e. $\mathcal C^\infty(X)$, then there is a natural representation of $G$ on $C^\infty(X)$. So I thought about the simplest example I had in mind which was the case of of the circle $G = S^1$ which has ($SO(2)$ symmetry). Then the representation are given by harmonic analysis of Fourier series. To explain this I said to consider a glass of water which has cylindrical symmetry (""if I rotate the glass you cannot say how much I did rotate the glass, so it has a symmetry"") then the vibration and deformations of the edge of the glass are the functions on $S^1$ that can be classified in harmonics... That's why - I concluded - glasses are used to sing with water... I said that but really I'm not sure at all if it's effectively the case. I mean that I'm note sure until what extension my reasoning was correct. Is the role of the water just the one to annihilate every representation but one (or some) which get excited and that's why the glass emits only one definite sound? Do you think the reasoning has a tragic flaw somewhere? To what extension is valid?","This might very well be a crucially flawed reasoning. But I think has to have something true behind. I was trying to explain basic ideas of representation of Lie Groups to an 11 years old girl who asked what I was studying. What I wanted to explain was the relation between special functions and symmetries. The mathematical thing I wanted to explain was that if we have a group $G$ acting on a space $X$, and we look at the space of infinitely derivable functions on $X$, i.e. $\mathcal C^\infty(X)$, then there is a natural representation of $G$ on $C^\infty(X)$. So I thought about the simplest example I had in mind which was the case of of the circle $G = S^1$ which has ($SO(2)$ symmetry). Then the representation are given by harmonic analysis of Fourier series. To explain this I said to consider a glass of water which has cylindrical symmetry (""if I rotate the glass you cannot say how much I did rotate the glass, so it has a symmetry"") then the vibration and deformations of the edge of the glass are the functions on $S^1$ that can be classified in harmonics... That's why - I concluded - glasses are used to sing with water... I said that but really I'm not sure at all if it's effectively the case. I mean that I'm note sure until what extension my reasoning was correct. Is the role of the water just the one to annihilate every representation but one (or some) which get excited and that's why the glass emits only one definite sound? Do you think the reasoning has a tragic flaw somewhere? To what extension is valid?",,"['real-analysis', 'functional-analysis', 'soft-question', 'representation-theory', 'lie-groups']"
4,Relationship between l'Hospital's rule and the least upper bound property.,Relationship between l'Hospital's rule and the least upper bound property.,,"Statement of L'Hospital's Rule Let $F$ be an ordered field. L'Hospital's Rule. Let $f$ and $g$ be $F$-valued functions defined on an open interval $I$ in $F$. Let $c$ be an endpoint of $I$. Note $c$ may be a finite number or one of the symbols $-\infty$,$+\infty$. Suppose $f'(x)$ and $g'(x)$ are defined everywhere on $I$. Suppose $g(x)$ and $g'(x)$ are never zero and never change sign on $I.$ Suppose one of the following two hypothesis is satisfied: $\displaystyle \lim_{x \rightarrow c} f(x) = \lim_{x \rightarrow c} g(x) = 0$ $\displaystyle \lim_{x \rightarrow c} g(x) = \pm \infty$ Suppose $$\displaystyle \lim_{x\rightarrow c} \frac{f'(x)}{g'(x)} = L$$  where $L$ is a finite number or one of the symbols $-\infty$, $+\infty$. Then  $$\displaystyle \lim_{x\rightarrow c} \frac{f(x)}{g(x)} = L.$$ Motivation The standard proof of l'Hospital's rule (when $F=\mathbb{R}$) uses Cauchy's mean value theorem. See: Taylor, A. E. (1952), ""L'Hospital's rule"", Amer. Math. Monthly, 59: 20–24 https://en.wikipedia.org/wiki/L%27H%C3%B4pital%27s_rule#General_proof Apostol, ""Mathematical Analysis"" For an ordered field, the mean value theorem is equivalent to the least upper bound property. See: Equivalence of Rolle's theorem, the mean value theorem, and the least upper bound property? ) https://arxiv.org/abs/1204.4483 Another proof of l'Hospital's rule can be based on the convergence of bounded monotone sequences and the statement that $f' > 0$ on an interval $I$ implies $f$ strictly increasing on $I$. See Taylor, A. E. (1952), ""L'Hospital's rule"", Amer. Math. Monthly, 59: 20–24. The convergence of bounded monotone sequences is equivalent to the least upper bound property. The statement that $f' > 0$ on $I$ implies $f$ strictly increasing on $I$ is also equivalent to the least upper bound property. L'Hospital's rule is false for $F=\mathbb{Q}$. The following proof outline is adapted from Exercise 7.8 of Korner's book ""A Companion to Analysis."" Choose a sequence $a_n \in \mathbb{R} \setminus \mathbb{Q}$ with $4^{-n-1} < a_n < 4^{-n}$ for $n=1,2,\ldots$. Define $I_0 = \{x \in \mathbb{Q} : a_0 < x \}$ and $I_n = \{x \in \mathbb{Q} : a_n < x < a_{n-1}\}$. Notice $4^{-n-1} < x< 4^{-n}$ whenever $x \in I_n$. Define $f:\mathbb{Q} \rightarrow \mathbb{Q}$ by $f(0)=0$ and $f(x) = 8^{-n}$ if $|x| \in I_n$. Remember we work in the ordered field $F=\mathbb{Q}$. We have $f'(x)=0$ for all $x \in \mathbb{Q}$, and  $$ \lim_{x \to 0}\frac{f(x)}{x^2} = \infty \quad \text{and} \quad \lim_{x \to 0}\frac{f'(x)}{2x} = 0. $$ Question What is the logical relationship between l'Hospital's rule and the least upper bound property? Does l'Hospital's rule imply the least upper bound property? Or is there an ordered field with l'Hospital's rule but not the least upper bound property? Related Here is a related question. Limit of the derivative and LUB It asks (in my notation) whether the following differentiability criteria implies the least upper bound property. Differentiability Criteria. Let $f$ be an $F$-valued function defined on open interval $I$. Suppose $f$ is continuous on $I$. Suppose $f$ is differentiable on $I$ except at one point $c$ in $I$. If $\lim_{x \rightarrow c} f'(x)$ exists, then $f'(c)$ exists and equals this limit. L'Hospital's rule implies this differentiability criteria. See for example https://en.wikipedia.org/wiki/L%27H%C3%B4pital%27s_rule#Corollary","Statement of L'Hospital's Rule Let $F$ be an ordered field. L'Hospital's Rule. Let $f$ and $g$ be $F$-valued functions defined on an open interval $I$ in $F$. Let $c$ be an endpoint of $I$. Note $c$ may be a finite number or one of the symbols $-\infty$,$+\infty$. Suppose $f'(x)$ and $g'(x)$ are defined everywhere on $I$. Suppose $g(x)$ and $g'(x)$ are never zero and never change sign on $I.$ Suppose one of the following two hypothesis is satisfied: $\displaystyle \lim_{x \rightarrow c} f(x) = \lim_{x \rightarrow c} g(x) = 0$ $\displaystyle \lim_{x \rightarrow c} g(x) = \pm \infty$ Suppose $$\displaystyle \lim_{x\rightarrow c} \frac{f'(x)}{g'(x)} = L$$  where $L$ is a finite number or one of the symbols $-\infty$, $+\infty$. Then  $$\displaystyle \lim_{x\rightarrow c} \frac{f(x)}{g(x)} = L.$$ Motivation The standard proof of l'Hospital's rule (when $F=\mathbb{R}$) uses Cauchy's mean value theorem. See: Taylor, A. E. (1952), ""L'Hospital's rule"", Amer. Math. Monthly, 59: 20–24 https://en.wikipedia.org/wiki/L%27H%C3%B4pital%27s_rule#General_proof Apostol, ""Mathematical Analysis"" For an ordered field, the mean value theorem is equivalent to the least upper bound property. See: Equivalence of Rolle's theorem, the mean value theorem, and the least upper bound property? ) https://arxiv.org/abs/1204.4483 Another proof of l'Hospital's rule can be based on the convergence of bounded monotone sequences and the statement that $f' > 0$ on an interval $I$ implies $f$ strictly increasing on $I$. See Taylor, A. E. (1952), ""L'Hospital's rule"", Amer. Math. Monthly, 59: 20–24. The convergence of bounded monotone sequences is equivalent to the least upper bound property. The statement that $f' > 0$ on $I$ implies $f$ strictly increasing on $I$ is also equivalent to the least upper bound property. L'Hospital's rule is false for $F=\mathbb{Q}$. The following proof outline is adapted from Exercise 7.8 of Korner's book ""A Companion to Analysis."" Choose a sequence $a_n \in \mathbb{R} \setminus \mathbb{Q}$ with $4^{-n-1} < a_n < 4^{-n}$ for $n=1,2,\ldots$. Define $I_0 = \{x \in \mathbb{Q} : a_0 < x \}$ and $I_n = \{x \in \mathbb{Q} : a_n < x < a_{n-1}\}$. Notice $4^{-n-1} < x< 4^{-n}$ whenever $x \in I_n$. Define $f:\mathbb{Q} \rightarrow \mathbb{Q}$ by $f(0)=0$ and $f(x) = 8^{-n}$ if $|x| \in I_n$. Remember we work in the ordered field $F=\mathbb{Q}$. We have $f'(x)=0$ for all $x \in \mathbb{Q}$, and  $$ \lim_{x \to 0}\frac{f(x)}{x^2} = \infty \quad \text{and} \quad \lim_{x \to 0}\frac{f'(x)}{2x} = 0. $$ Question What is the logical relationship between l'Hospital's rule and the least upper bound property? Does l'Hospital's rule imply the least upper bound property? Or is there an ordered field with l'Hospital's rule but not the least upper bound property? Related Here is a related question. Limit of the derivative and LUB It asks (in my notation) whether the following differentiability criteria implies the least upper bound property. Differentiability Criteria. Let $f$ be an $F$-valued function defined on open interval $I$. Suppose $f$ is continuous on $I$. Suppose $f$ is differentiable on $I$ except at one point $c$ in $I$. If $\lim_{x \rightarrow c} f'(x)$ exists, then $f'(c)$ exists and equals this limit. L'Hospital's rule implies this differentiability criteria. See for example https://en.wikipedia.org/wiki/L%27H%C3%B4pital%27s_rule#Corollary",,"['real-analysis', 'limits-without-lhopital', 'reverse-math']"
5,Diffeomorphism-invariant spaces of smooth functions,Diffeomorphism-invariant spaces of smooth functions,,"Let's start with an interesting story. In his celebrated Partial Differential Relations (p. 146), the great Misha Gromov gives a nice exercise of which the following is a (strict) part. Exercise. Consider the action of $\mathrm{Diff}(\mathbb R^n)$  on the space $C^\infty(\mathbb R^n)$ of smooth functions $\mathbb R^n \to \mathbb R$. Then if $n\geq 2$, then there are exactly six (non-trivial) invariant subspaces; if $n = 1$, then there are ten of them. It's quite easy to find the invariant subspaces Gromov is thinking about (I will give no spoiler...). The problem is that, according to these two messages of C. McMullen, the exercise is actually false. McMullen's counterexample is the following: take a sequence space $S \subset \mathbb R^\mathbb N$ and define the function subspace $$\mathscr E[S] = \left\{ f \in C^\infty(\mathbb R) \,\middle|\, \forall (x_n)_n \in \mathbb R^\mathbb N, |x_n| \xrightarrow[n\to\infty]{} \infty \Longrightarrow (f(x_n))_n \in S\right\}.$$ This is quite clearly a $\mathrm{Diff}(\mathbb R)$-invariant subspace of $C^\infty(\mathbb R)$. McMullen claims that the sequence space $$\mathrm{bv} = \left\{(a_n)_{n\in\mathbb N}\,\middle |\, \sum_{k} |a_{k+1} - a_k| < +\infty \right\}$$ already gives an example of an ""exotic"" invariant subspace $\mathscr E[\mathrm{bv}]$ (i.e. one different from Gromov's ten examples) and that it is easy to modify it to give uncountably many such invariant subspaces. I have two questions: Can you prove McMullen's claims? If $n\geq 2$, is there an invariant subspace which isn't of the form $\mathscr E[S]$? (if $n = 1$, the space $\mathscr E[S]$ doesn't make any difference between the two ends $\pm \infty$ of $\mathbb R$ so only six of Gromov's ten subspaces are of this form).","Let's start with an interesting story. In his celebrated Partial Differential Relations (p. 146), the great Misha Gromov gives a nice exercise of which the following is a (strict) part. Exercise. Consider the action of $\mathrm{Diff}(\mathbb R^n)$  on the space $C^\infty(\mathbb R^n)$ of smooth functions $\mathbb R^n \to \mathbb R$. Then if $n\geq 2$, then there are exactly six (non-trivial) invariant subspaces; if $n = 1$, then there are ten of them. It's quite easy to find the invariant subspaces Gromov is thinking about (I will give no spoiler...). The problem is that, according to these two messages of C. McMullen, the exercise is actually false. McMullen's counterexample is the following: take a sequence space $S \subset \mathbb R^\mathbb N$ and define the function subspace $$\mathscr E[S] = \left\{ f \in C^\infty(\mathbb R) \,\middle|\, \forall (x_n)_n \in \mathbb R^\mathbb N, |x_n| \xrightarrow[n\to\infty]{} \infty \Longrightarrow (f(x_n))_n \in S\right\}.$$ This is quite clearly a $\mathrm{Diff}(\mathbb R)$-invariant subspace of $C^\infty(\mathbb R)$. McMullen claims that the sequence space $$\mathrm{bv} = \left\{(a_n)_{n\in\mathbb N}\,\middle |\, \sum_{k} |a_{k+1} - a_k| < +\infty \right\}$$ already gives an example of an ""exotic"" invariant subspace $\mathscr E[\mathrm{bv}]$ (i.e. one different from Gromov's ten examples) and that it is easy to modify it to give uncountably many such invariant subspaces. I have two questions: Can you prove McMullen's claims? If $n\geq 2$, is there an invariant subspace which isn't of the form $\mathscr E[S]$? (if $n = 1$, the space $\mathscr E[S]$ doesn't make any difference between the two ends $\pm \infty$ of $\mathbb R$ so only six of Gromov's ten subspaces are of this form).",,"['real-analysis', 'invariant-theory']"
6,Exercise about continuous functions,Exercise about continuous functions,,"Consider a continuous function $f \, : \, [0,1] \, \longrightarrow \, [0,+\infty)$ such that $f(0)=f(1)=0$ and : $\forall x \in (0,1), \; f(x) > 0$. I would like to prove that there exist $x_{1},x_{2} \in [0,1]$ with $x_{1} < x_{2}$ and such that $f(x_{1}) = f(x_{2}) = x_{2}-x_{1}$. If we assume that $x_{1}$ and $x_{2}$ exist, then let $a = x_{2}-x_{1} > 0$ and we see that the equation $f(x_{1})=f(x_{2})=a$ becomes : $f(x_{1}) = f(x_{1}+a) = a$. Therefore, $f(x_{1}) = f\big( x_{1}+f(x_{1}) \big)=a$. But I do not see how this will help me to find $x_{1}$.","Consider a continuous function $f \, : \, [0,1] \, \longrightarrow \, [0,+\infty)$ such that $f(0)=f(1)=0$ and : $\forall x \in (0,1), \; f(x) > 0$. I would like to prove that there exist $x_{1},x_{2} \in [0,1]$ with $x_{1} < x_{2}$ and such that $f(x_{1}) = f(x_{2}) = x_{2}-x_{1}$. If we assume that $x_{1}$ and $x_{2}$ exist, then let $a = x_{2}-x_{1} > 0$ and we see that the equation $f(x_{1})=f(x_{2})=a$ becomes : $f(x_{1}) = f(x_{1}+a) = a$. Therefore, $f(x_{1}) = f\big( x_{1}+f(x_{1}) \big)=a$. But I do not see how this will help me to find $x_{1}$.",,"['calculus', 'real-analysis', 'functions', 'continuity']"
7,Showing that $\Omega$ is of class $C^1$,Showing that  is of class,\Omega C^1,"I have done a lot in this problem, but unfortunately it is not enough to solve it, answers or hints are very welcome. Let $B$ be a rectangle in $\mathbb R^2$ and consider $\varphi\colon B\to\mathbb{R}^3$ of class $C^1$. For any $y\in\mathbb{R}^3\setminus\varphi(B)$, define $$\Omega(y) = \int_\varphi\frac{1}{|y-x|^3}(y-x)\,dS_x.$$ Show that $\Omega$ is of class $C^1$ as a function in variable $y$. At this point I could find that $$\frac{\partial\Omega}{\partial y_1}(y)=\int_{\partial\varphi}\frac{1}{|y-x|^3}(0,x_3-y_3,x_2-y_2)$$  $$\frac{\partial\Omega}{\partial y_2}(y)=\int_{\partial\varphi}\frac{1}{|y-x|^3}(x_3-y_3,0,x_1-y_1)$$ and $$\frac{\partial\Omega}{\partial y_3}(y)=\int_{\partial\varphi}\frac{1}{|y-x|^3}(x_2-y_2,x_1-y_1,0).$$ Now, all I have to do is show that any one of these is continuous as a function of $y$, and that is where things are getting hard. Thanks.","I have done a lot in this problem, but unfortunately it is not enough to solve it, answers or hints are very welcome. Let $B$ be a rectangle in $\mathbb R^2$ and consider $\varphi\colon B\to\mathbb{R}^3$ of class $C^1$. For any $y\in\mathbb{R}^3\setminus\varphi(B)$, define $$\Omega(y) = \int_\varphi\frac{1}{|y-x|^3}(y-x)\,dS_x.$$ Show that $\Omega$ is of class $C^1$ as a function in variable $y$. At this point I could find that $$\frac{\partial\Omega}{\partial y_1}(y)=\int_{\partial\varphi}\frac{1}{|y-x|^3}(0,x_3-y_3,x_2-y_2)$$  $$\frac{\partial\Omega}{\partial y_2}(y)=\int_{\partial\varphi}\frac{1}{|y-x|^3}(x_3-y_3,0,x_1-y_1)$$ and $$\frac{\partial\Omega}{\partial y_3}(y)=\int_{\partial\varphi}\frac{1}{|y-x|^3}(x_2-y_2,x_1-y_1,0).$$ Now, all I have to do is show that any one of these is continuous as a function of $y$, and that is where things are getting hard. Thanks.",,"['real-analysis', 'analysis', 'integration', 'continuity']"
8,$\int_{\mathbb{R}}|f(t)|^2dt=\int_{\mathbb{R}}|f'(t)|^2dt$ implies $f(t)=\mathbb{x}_{i}|f(t)|$,implies,\int_{\mathbb{R}}|f(t)|^2dt=\int_{\mathbb{R}}|f'(t)|^2dt f(t)=\mathbb{x}_{i}|f(t)|,"Let $f \in C^{1}(\mathbb{R},\mathbb{R}^m)$ be such that $f$ and $f'$ are square integrable and $$\{t:f(t)=0\} \subset \{t:f'(t)=0\}$$ $$ |\{t:f(t)=0\}|=n\in \mathbb{N}$$ Prove that if $\int_{\mathbb{R}}|f'(t)|^2\, dt=\int_{\mathbb{R}}|f|'^2(t)\,dt$, then there are $n+1$ points $$\mathbb{x}_{1},\mathbb{x}_{2},...,\mathbb{x}_{n+1}\in S^{m-1}=\{\mathbb{x}\in \mathbb{R}^m:|\mathbb{x}|=1\} $$ such that for each $t \in \mathbb{R}$, $f(t)=\mathbb{x}_{i}|f(t)|$ for some $i$. I got this problem from Postech Math Competition. Thanks in advance.","Let $f \in C^{1}(\mathbb{R},\mathbb{R}^m)$ be such that $f$ and $f'$ are square integrable and $$\{t:f(t)=0\} \subset \{t:f'(t)=0\}$$ $$ |\{t:f(t)=0\}|=n\in \mathbb{N}$$ Prove that if $\int_{\mathbb{R}}|f'(t)|^2\, dt=\int_{\mathbb{R}}|f|'^2(t)\,dt$, then there are $n+1$ points $$\mathbb{x}_{1},\mathbb{x}_{2},...,\mathbb{x}_{n+1}\in S^{m-1}=\{\mathbb{x}\in \mathbb{R}^m:|\mathbb{x}|=1\} $$ such that for each $t \in \mathbb{R}$, $f(t)=\mathbb{x}_{i}|f(t)|$ for some $i$. I got this problem from Postech Math Competition. Thanks in advance.",,"['real-analysis', 'analysis', 'multivariable-calculus']"
9,Show that f is a polynomial,Show that f is a polynomial,,"Suppose $f$ is an entire function on $\mathbb{C}^n$ that satisfies for every $\epsilon>0$ a growth-condition $$|f(z)|\leq C_{\epsilon}(1+|z|)^{N_{\epsilon}}e^{\epsilon |  \text{Im}\,z|}$$ Show that $f$ is a polynomial. (Hint: study $\hat{f} = \mathcal{F}(f)$ the Fourier-transform). I know I'm supposed to apply the Paley-Wiener-Schwartz Theorem, but not sure how.;. (See http://en.wikipedia.org/wiki/Paley%E2%80%93Wiener_theorem below). Any suggestions and/or tips are greatly appreciated. Thnx.","Suppose $f$ is an entire function on $\mathbb{C}^n$ that satisfies for every $\epsilon>0$ a growth-condition $$|f(z)|\leq C_{\epsilon}(1+|z|)^{N_{\epsilon}}e^{\epsilon |  \text{Im}\,z|}$$ Show that $f$ is a polynomial. (Hint: study $\hat{f} = \mathcal{F}(f)$ the Fourier-transform). I know I'm supposed to apply the Paley-Wiener-Schwartz Theorem, but not sure how.;. (See http://en.wikipedia.org/wiki/Paley%E2%80%93Wiener_theorem below). Any suggestions and/or tips are greatly appreciated. Thnx.",,"['real-analysis', 'complex-analysis', 'polynomials', 'fourier-analysis', 'several-complex-variables']"
10,Asymptotic behavior of $|f'(x)|^n e^{-f(x)}$,Asymptotic behavior of,|f'(x)|^n e^{-f(x)},"Let $f$ be a strictly convex function on $\mathbb R$, $f'' \geq C > 0$. Let $n$ be a positive integer. What can we say about the growth rate of $|f'(x)|^n e^{-f(x)}$ as $x\rightarrow \infty$? Must it go to $0$? If so, how rapidly? Intuitively, it seems like it should at least tend to $0$, as $e^{-f(x)}$ will decay at a very rapid rate, and $f'$ cannot grow too much more rapidly than $f$ does. Does anyone have any suggestions as to how to prove this, or any counterexamples?","Let $f$ be a strictly convex function on $\mathbb R$, $f'' \geq C > 0$. Let $n$ be a positive integer. What can we say about the growth rate of $|f'(x)|^n e^{-f(x)}$ as $x\rightarrow \infty$? Must it go to $0$? If so, how rapidly? Intuitively, it seems like it should at least tend to $0$, as $e^{-f(x)}$ will decay at a very rapid rate, and $f'$ cannot grow too much more rapidly than $f$ does. Does anyone have any suggestions as to how to prove this, or any counterexamples?",,"['real-analysis', 'asymptotics']"
11,Are these two Abel's criteria for uniform convergence different?,Are these two Abel's criteria for uniform convergence different?,,"I wonder what differences are between the folowwing two versions of Abel's criteria for uniform convergence: From Elementary classical analysis by Marsden and Hoffman : Abel's Test . Let $A \subset R^n$ and $\phi_n: A  \rightarrow R$ be a sequence of functions which are decreasing; that   is, $\phi_{n+1}(x) \leq \phi_n(x)$ for each $x \in A$. Suppose there   is a constant $M$ such that$|\phi_n(x)| \leq M$ for all $x \in A$ and   all $n$. If $\displaystyle\sum_{n=1}^\infty f_n(x)$ converges uniformly on $A$, then   so does $\displaystyle \sum_{n=1}^\infty \phi_n(x)f_n(x)$. From Wikipedia : Abel's uniform convergence test . Let $\{g_n\}$ be a uniformly   bounded sequence of real-valued continuous functions on a set $E$ such   that $g_{n+1}(x) \leq  g_n(x)$ for all $x ∈ E$ and positive integers $n$,   and let $\{f_n\}$ be a sequence of real-valued functions such that the   series $\displaystyle\sum f_n(x)$ converges uniformly on $E$. Then $\displaystyle\sum f_n(x)g_n(x)   $converges uniformly on $E$. Is the additional requirement of continuity for a sequence of functions in Wikipedia the only difference? If not, what else? Is this continuity unnecessary and can be ignored as in Marsden's? If yes, is Marsden's a more general version? Or do you have a different one? Thanks and regards!","I wonder what differences are between the folowwing two versions of Abel's criteria for uniform convergence: From Elementary classical analysis by Marsden and Hoffman : Abel's Test . Let $A \subset R^n$ and $\phi_n: A  \rightarrow R$ be a sequence of functions which are decreasing; that   is, $\phi_{n+1}(x) \leq \phi_n(x)$ for each $x \in A$. Suppose there   is a constant $M$ such that$|\phi_n(x)| \leq M$ for all $x \in A$ and   all $n$. If $\displaystyle\sum_{n=1}^\infty f_n(x)$ converges uniformly on $A$, then   so does $\displaystyle \sum_{n=1}^\infty \phi_n(x)f_n(x)$. From Wikipedia : Abel's uniform convergence test . Let $\{g_n\}$ be a uniformly   bounded sequence of real-valued continuous functions on a set $E$ such   that $g_{n+1}(x) \leq  g_n(x)$ for all $x ∈ E$ and positive integers $n$,   and let $\{f_n\}$ be a sequence of real-valued functions such that the   series $\displaystyle\sum f_n(x)$ converges uniformly on $E$. Then $\displaystyle\sum f_n(x)g_n(x)   $converges uniformly on $E$. Is the additional requirement of continuity for a sequence of functions in Wikipedia the only difference? If not, what else? Is this continuity unnecessary and can be ignored as in Marsden's? If yes, is Marsden's a more general version? Or do you have a different one? Thanks and regards!",,['real-analysis']
12,An intriguing integral representation of $\zeta^2(3)$,An intriguing integral representation of,\zeta^2(3),"In a short presentation on RG, that is A Mesmerizing Integral Representation of $ζ^2(3)$ by C. I. Valean , we have, if I'm allowed, the following intriguing integral representation of $\zeta^2(3)$ , $$\zeta^2(3)$$ $$\small =\frac{32}{5} \int_0^1\left(2 \Re\left\{\text{Li}_3\left(\frac{1-x^2}{1+x^2}+i\frac{ 2 x}{1+x^2}\right)\right\}-\text{Li}_3\left(\frac{1}{1+x^2}\right)\right)\frac{\log (1-x) \log (1+x)}{x} \textrm{d}x.$$ The proof flows as follows: using that $\displaystyle  \int_0^1 \frac{y\log(1-y)\log(1+y)}{x^2+y^2}\textrm{d}y=-\frac{3}{8}\zeta(3)+\frac{1}{4}\operatorname{Li}_3\left(\frac{1}{1+x^2}\right)-\frac{1}{2}\Re\biggr\{\operatorname{Li}_3\left(\frac{1-x^2}{1+x^2}+i\frac{2x}{1+x^2}\right)\biggr\}=-\frac{3}{8}\zeta(3)+\frac{1}{4}\operatorname{Li}_3(\cos^2(\theta))-\frac{1}{2}\sum_{n=1}^{\infty} \frac{\cos(2n\theta)}{n^3}, \ x=\tan(\theta), \theta \in \left(-\frac{\pi}{2},\frac{\pi}{2}\right)$ , given in More (Almost) Impossible Integrals, Sums, and Series (2023) , page $4$ , together with the fact that $\displaystyle \int_0^1 \frac{\log (1-x) \log (1+x)}{x}\textrm{d}x=-\frac{5}{8}\zeta(3)$ , also found (Almost) Impossible Integrals, Sums, and Series (2019) , we have $$\frac{32}{5} \int_0^1\left(2 \Re\left\{\text{Li}_3\left(\frac{1-x^2}{1+x^2}+i\frac{ 2 x}{1+x^2}\right)\right\}-\text{Li}_3\left(\frac{1}{1+x^2}\right)\right)\frac{\log (1-x) \log (1+x)}{x} \textrm{d}x$$ $$=-\frac{32}{5} \int_0^1\left(\frac{3}{2}\zeta(3)+4\int_0^1 \frac{y\log(1-y)\log(1+y)}{x^2+y^2}\textrm{d}y\right)\frac{\log (1-x) \log (1+x)}{x} \textrm{d}x$$ $$=6\zeta^2(3)-\frac{128}{5} \int_0^1 \left( \int_0^1 \frac{y \overbrace{\log (1-x) \log (1+x)}^{\displaystyle  f(x)} \overbrace{\log(1-y)\log(1+y)}^{\displaystyle  f(y)}}{x(x^2+y^2)}\textrm{d}y\right)\textrm{d}x$$ $$\small=6\zeta^2(3)-\frac{128}{5} \int_0^1 \left( \int_0^1 \frac{y f(x) f(y)}{x(x^2+y^2)}\textrm{d}y\right)\textrm{d}x\overset{\substack{\text{use  } \\ \text{the symmetry}}}{=}6\zeta^2(3)-\frac{128}{5} \int_0^1 \left( \int_0^1 \frac{x f(x) f(y)}{y(x^2+y^2)}\textrm{d}y\right)\textrm{d}x$$ $$\small=6\zeta^2(3)-\frac{64}{5} \int_0^1 \left( \int_0^1 \left(\frac{y}{x}+\frac{x}{y}\right) \frac{ f(x) f(y)}{x^2+y^2}\textrm{d}y\right)\textrm{d}x=6\zeta^2(3)-\frac{64}{5}\int_0^1 \frac{f(x)}{x}\left( \int_0^1 \frac{f(y) }{y}\textrm{d}y\right)\textrm{d}x$$ $$\small=6\zeta^2(3)-\frac{64}{5}\left(\int_0^1 \frac{\log(1-x)\log(1+x)}{x} \textrm{d}x\right)^2=\zeta^2(3),$$ which gives an end to the present proof. Question $1$ : Do we have in the mathematical literature challenging integrals with a similar structure of the integrand involving parts like $\displaystyle \text{Li}_n\left(\frac{1-x^2}{1+x^2}+i\frac{ 2 x}{1+x^2}\right), n\ge 2$ ? Any links or references would be highly appreciated. Question $2$ : I would love to see different approaches to this integral, this time not for the sake of seeing more proofs, but out of the curiosity of possible interesting and subtle connections with integrals and series that are out of my sight at the moment. Therefore, even some unfinished solutions could be very interesting. Update $1$ : Another very nice version presented by the same author is $$\zeta^2(3)$$ $$=\frac{32}{19} \int_0^1 \biggr(\Re\left\{\operatorname{Li}_3\left(\frac{1-x^2}{1+x^2}+i\frac{ 2 x}{1+x^2}\right)\right\}\left(\log ^2(1-x)+\log ^2(1+x)\right)$$ $$-\operatorname{Li}_3\left(\frac{1}{1+x^2}\right) \log (1-x) \log (1+x)\biggr)\frac{1}{x} \textrm{d}x,$$ which combines the previous result and the particular case of a result given in More (Almost) Impossible Integrals, Sums, and Series (2023) .","In a short presentation on RG, that is A Mesmerizing Integral Representation of by C. I. Valean , we have, if I'm allowed, the following intriguing integral representation of , The proof flows as follows: using that , given in More (Almost) Impossible Integrals, Sums, and Series (2023) , page , together with the fact that , also found (Almost) Impossible Integrals, Sums, and Series (2019) , we have which gives an end to the present proof. Question : Do we have in the mathematical literature challenging integrals with a similar structure of the integrand involving parts like ? Any links or references would be highly appreciated. Question : I would love to see different approaches to this integral, this time not for the sake of seeing more proofs, but out of the curiosity of possible interesting and subtle connections with integrals and series that are out of my sight at the moment. Therefore, even some unfinished solutions could be very interesting. Update : Another very nice version presented by the same author is which combines the previous result and the particular case of a result given in More (Almost) Impossible Integrals, Sums, and Series (2023) .","ζ^2(3) \zeta^2(3) \zeta^2(3) \small =\frac{32}{5} \int_0^1\left(2 \Re\left\{\text{Li}_3\left(\frac{1-x^2}{1+x^2}+i\frac{ 2 x}{1+x^2}\right)\right\}-\text{Li}_3\left(\frac{1}{1+x^2}\right)\right)\frac{\log (1-x) \log (1+x)}{x} \textrm{d}x. \displaystyle  \int_0^1 \frac{y\log(1-y)\log(1+y)}{x^2+y^2}\textrm{d}y=-\frac{3}{8}\zeta(3)+\frac{1}{4}\operatorname{Li}_3\left(\frac{1}{1+x^2}\right)-\frac{1}{2}\Re\biggr\{\operatorname{Li}_3\left(\frac{1-x^2}{1+x^2}+i\frac{2x}{1+x^2}\right)\biggr\}=-\frac{3}{8}\zeta(3)+\frac{1}{4}\operatorname{Li}_3(\cos^2(\theta))-\frac{1}{2}\sum_{n=1}^{\infty} \frac{\cos(2n\theta)}{n^3}, \ x=\tan(\theta), \theta \in \left(-\frac{\pi}{2},\frac{\pi}{2}\right) 4 \displaystyle \int_0^1 \frac{\log (1-x) \log (1+x)}{x}\textrm{d}x=-\frac{5}{8}\zeta(3) \frac{32}{5} \int_0^1\left(2 \Re\left\{\text{Li}_3\left(\frac{1-x^2}{1+x^2}+i\frac{ 2 x}{1+x^2}\right)\right\}-\text{Li}_3\left(\frac{1}{1+x^2}\right)\right)\frac{\log (1-x) \log (1+x)}{x} \textrm{d}x =-\frac{32}{5} \int_0^1\left(\frac{3}{2}\zeta(3)+4\int_0^1 \frac{y\log(1-y)\log(1+y)}{x^2+y^2}\textrm{d}y\right)\frac{\log (1-x) \log (1+x)}{x} \textrm{d}x =6\zeta^2(3)-\frac{128}{5} \int_0^1 \left( \int_0^1 \frac{y \overbrace{\log (1-x) \log (1+x)}^{\displaystyle  f(x)} \overbrace{\log(1-y)\log(1+y)}^{\displaystyle  f(y)}}{x(x^2+y^2)}\textrm{d}y\right)\textrm{d}x \small=6\zeta^2(3)-\frac{128}{5} \int_0^1 \left( \int_0^1 \frac{y f(x) f(y)}{x(x^2+y^2)}\textrm{d}y\right)\textrm{d}x\overset{\substack{\text{use  } \\ \text{the symmetry}}}{=}6\zeta^2(3)-\frac{128}{5} \int_0^1 \left( \int_0^1 \frac{x f(x) f(y)}{y(x^2+y^2)}\textrm{d}y\right)\textrm{d}x \small=6\zeta^2(3)-\frac{64}{5} \int_0^1 \left( \int_0^1 \left(\frac{y}{x}+\frac{x}{y}\right) \frac{ f(x) f(y)}{x^2+y^2}\textrm{d}y\right)\textrm{d}x=6\zeta^2(3)-\frac{64}{5}\int_0^1 \frac{f(x)}{x}\left( \int_0^1 \frac{f(y) }{y}\textrm{d}y\right)\textrm{d}x \small=6\zeta^2(3)-\frac{64}{5}\left(\int_0^1 \frac{\log(1-x)\log(1+x)}{x} \textrm{d}x\right)^2=\zeta^2(3), 1 \displaystyle \text{Li}_n\left(\frac{1-x^2}{1+x^2}+i\frac{ 2 x}{1+x^2}\right), n\ge 2 2 1 \zeta^2(3) =\frac{32}{19} \int_0^1 \biggr(\Re\left\{\operatorname{Li}_3\left(\frac{1-x^2}{1+x^2}+i\frac{ 2 x}{1+x^2}\right)\right\}\left(\log ^2(1-x)+\log ^2(1+x)\right) -\operatorname{Li}_3\left(\frac{1}{1+x^2}\right) \log (1-x) \log (1+x)\biggr)\frac{1}{x} \textrm{d}x,","['real-analysis', 'calculus', 'integration', 'sequences-and-series', 'definite-integrals']"
13,"Are there functions $f,g:\mathbb{R} \to \mathbb{R}$ such that they differentiate each other?",Are there functions  such that they differentiate each other?,"f,g:\mathbb{R} \to \mathbb{R}","i.e. $$f'(x) = (g \circ f)(x)$$ $$g'(x) = (f\circ g)(x)$$ I came up with this question a few years ago. A friend found the only example I know: for $c\in\mathbb{R}$ $$f(x) = c$$ $$g(x) = cx - c^2 $$ After trying with some particular cases (with no success), I used the formula for the derivative of the inverse function and got that if $f$ and $g$ are bijective then, $$f^{-1} = \int{}{dt \over g(t)}  $$ $$g^{-1} = \int {dt \over f(t)}$$ Assuming all conditions neccesary for this to be possible. I tried using this fact to construct the functions, again with no luck. I would really appreciate any insight on how to tackle this problem.","i.e. I came up with this question a few years ago. A friend found the only example I know: for After trying with some particular cases (with no success), I used the formula for the derivative of the inverse function and got that if and are bijective then, Assuming all conditions neccesary for this to be possible. I tried using this fact to construct the functions, again with no luck. I would really appreciate any insight on how to tackle this problem.",f'(x) = (g \circ f)(x) g'(x) = (f\circ g)(x) c\in\mathbb{R} f(x) = c g(x) = cx - c^2  f g f^{-1} = \int{}{dt \over g(t)}   g^{-1} = \int {dt \over f(t)},"['real-analysis', 'functions', 'derivatives']"
14,What's the sum of the reciprocals of the numbers that can be written as the sum of two positive cubes?,What's the sum of the reciprocals of the numbers that can be written as the sum of two positive cubes?,,"A very specific question: What's the sum of the reciprocals of the numbers that can be written as the sum of two positive cubes ( Oeis: A003325 )? $$\begin{align} &\sum_{n=1}^\infty \frac{1}{A003325(n)} \\ & =\frac{1}{1^3+1^2}+\frac{1}{1^3+2^3}+\frac{1}{2^3+2^3}+\frac{1}{1^3+3^3}+\frac{1}{2^3+3^3}+\frac{1}{3^3+3^3}+\frac{1}{1^3+4^3}+\dots \\ & = \frac{1}{2}+\frac{1}{9}+\frac{1}{16}+\frac{1}{28}+\frac{1}{35}+\frac{1}{54}+\frac{1}{65}+\frac{1}{72}+\frac{1}{91}+\dots \end{align} $$ Some notation to organize my thinking: Let $S\subset \mathbb{N}$ and let $1_{S}(x)=\cases{1 \mbox{ when }{x\in S }\\0 \mbox{ when } x\notin S}$ And define $m(S)=\sum_{n=1}^\infty \frac{1_S(n)}{n}$ . This would be the sum of the reciprocals of $S$ . $S_{a,b}=\{n\in \mathbb{N}: \exists \vec{x} \in\mathbb{N^{a}}, \sum_{i=1}^a |x_i|^b=n \}$ .  This is the set of numbers that can be written as sum of $a$ positive numbers raised to the fixed power $b$ . Some immediate results of this notation. And some examples. $m(S_{1,k})=\zeta(k)$ and $m(S_{4,2})$ is a divergent sum because $S_{4,2}=\mathbb{N}$ . My question above is asking for $m(S_{2,3})$ . And somewhat more broadly I am asking: Can we get a handle on the density of this set? Why should we have any chances of answering this? As we can read here , here , here and here . The first two links are MSE questions and the latter two are publications by Kevin A. Broughan. $$n\in S_{2,3} \iff \exists m \mid n ,\quad n^{1/3} \leq m \leq 4^{1/3} n^{1/3} \mbox{ s.t. }\\ ( m^{2} - \frac{n}{m})=3l \mbox{ and }(m^{2} - 4l) \mbox{ is a perfect square. }$$ So because we have this nice characterization I was curious if this number could possibly be expressed in relationship to other well-known constants like value of the zeta function. Note that the related question:   What's the sum of the reciprocals of the numbers that can be written as the sum of two non-negative cubes? But we can see that this question is just $\zeta(3)$ away from the title question. That is, $$m(\{n:(x,y)\in \mathbb{N_0^2}: n=x^3+y^3 \})=m(S_{2,3})+\zeta(3)$$ Here is what I know so far $m(S_{2,3}) \approx 	0.9777693455 =\sum_{n=1}^{20000} \frac{1}{A003325(n)}$ . Can anyone provide any more insight into what this number is? Apologies Note that in the original version of this I made a typo and wrote $S_{a,b}=\{n\in \mathbb{N}: \exists \vec{x} \in\mathbb{Z^{a}}, \sum_{i=1}^a |x_i|^b=n \}$ this isn't quite what I wanted to type here.","A very specific question: What's the sum of the reciprocals of the numbers that can be written as the sum of two positive cubes ( Oeis: A003325 )? Some notation to organize my thinking: Let and let And define . This would be the sum of the reciprocals of . .  This is the set of numbers that can be written as sum of positive numbers raised to the fixed power . Some immediate results of this notation. And some examples. and is a divergent sum because . My question above is asking for . And somewhat more broadly I am asking: Can we get a handle on the density of this set? Why should we have any chances of answering this? As we can read here , here , here and here . The first two links are MSE questions and the latter two are publications by Kevin A. Broughan. So because we have this nice characterization I was curious if this number could possibly be expressed in relationship to other well-known constants like value of the zeta function. Note that the related question:   What's the sum of the reciprocals of the numbers that can be written as the sum of two non-negative cubes? But we can see that this question is just away from the title question. That is, Here is what I know so far . Can anyone provide any more insight into what this number is? Apologies Note that in the original version of this I made a typo and wrote this isn't quite what I wanted to type here.","\begin{align}
&\sum_{n=1}^\infty \frac{1}{A003325(n)} \\
& =\frac{1}{1^3+1^2}+\frac{1}{1^3+2^3}+\frac{1}{2^3+2^3}+\frac{1}{1^3+3^3}+\frac{1}{2^3+3^3}+\frac{1}{3^3+3^3}+\frac{1}{1^3+4^3}+\dots \\
& = \frac{1}{2}+\frac{1}{9}+\frac{1}{16}+\frac{1}{28}+\frac{1}{35}+\frac{1}{54}+\frac{1}{65}+\frac{1}{72}+\frac{1}{91}+\dots
\end{align}  S\subset \mathbb{N} 1_{S}(x)=\cases{1 \mbox{ when }{x\in S }\\0 \mbox{ when } x\notin S} m(S)=\sum_{n=1}^\infty \frac{1_S(n)}{n} S S_{a,b}=\{n\in \mathbb{N}: \exists \vec{x} \in\mathbb{N^{a}}, \sum_{i=1}^a |x_i|^b=n \} a b m(S_{1,k})=\zeta(k) m(S_{4,2}) S_{4,2}=\mathbb{N} m(S_{2,3}) n\in S_{2,3} \iff \exists m \mid n ,\quad n^{1/3} \leq m \leq 4^{1/3} n^{1/3} \mbox{ s.t. }\\ ( m^{2} - \frac{n}{m})=3l \mbox{ and }(m^{2} - 4l) \mbox{ is a perfect square. } \zeta(3) m(\{n:(x,y)\in \mathbb{N_0^2}: n=x^3+y^3 \})=m(S_{2,3})+\zeta(3) m(S_{2,3}) \approx 	0.9777693455 =\sum_{n=1}^{20000} \frac{1}{A003325(n)} S_{a,b}=\{n\in \mathbb{N}: \exists \vec{x} \in\mathbb{Z^{a}}, \sum_{i=1}^a |x_i|^b=n \}","['real-analysis', 'sequences-and-series', 'combinatorics', 'number-theory', 'zeta-functions']"
15,Trying to compute limit of singular integrals : $L= \lim_{s\to 1}(1-s)\int_{\Omega}\frac{(u(x)-u(y))}{|x-y|^{d+2s}} d y.$,Trying to compute limit of singular integrals :,L= \lim_{s\to 1}(1-s)\int_{\Omega}\frac{(u(x)-u(y))}{|x-y|^{d+2s}} d y.,"Let $\Omega\subset \Bbb R^d$ be a bounded $C^1$ domain. Let $u:\Bbb R^d\to \Bbb R$ be a function in $C^2_b(\Bbb R^d)$ . I would like to compute the following limit: for $x\in \partial \Omega$ $$L= \lim_{s\to 1}(1-s)\int_{\Omega}\frac{(u(x)-u(y))}{|x-y|^{d+2s}} d y. $$ Here is what I did so far: Let $r>0$ be arbitrarily small enough. Then as $u$ is bounded, we have $$\begin{align}&\lim_{s\to 1}(1-s)\int_{\Omega\cap \{|x-y|\geq r\}}\frac{|u(x)-u(y)|}{|x-y|^{d+2s}}dy\\&\leq C \lim_{s\to 1}(1-s)\int_{|x-y|\geq r}\frac{dy}{|x-y|^{d+2s}} \\&= Cc_d\lim_{s\to 1}(1-s) \int_r^\infty t^{-2s-1} dt= 0. \end{align}$$ so that $$L= \lim_{s\to 1}(1-s)\int_{\Omega \cap B_r(x)}\frac{(u(x)-u(y))}{|x-y|^{d+2s}} d y. $$ Using the fundamental theorem of calculus and  the relation $\nabla [|x|^\alpha]= \alpha x|x|^{\alpha-2}$ , $$\begin{align}&(1-s)\int_{\Omega \cap B_r(x)}\frac{(u(x)-u(y))}{|x-y|^{d+2s}} d y\\ & = -\int_0^1 dt (1-s)\int_{\Omega \cap B_r(x)}\frac{\nabla u(x+ t(y-x))\cdot (y-x)}{|x-y|^{d+2s}}dy\\&=  \int_0^1 dt\frac{ (1-s)}{d-2(1-s))}\int_{\Omega \cap B_r(x)}  \nabla u(x+ t(y-x))\cdot \nabla_y [|x-y|^{-d+2(1-s)}]dy \end{align}$$ Therefore my initial question could be resumed to computaion of $$\lim_{s\to 1} (1-s)\int_{\Omega \cap B_r(x)} \nabla u(x+ t(y-x))\cdot \nabla_y [|x-y|^{-d+2(1-s)}]dy.$$ Any idea on to move further? My feeling is the should be  a multiple factor of $\frac{\partial u}{\partial n}(x)= \nabla u(x).n(x)$ where $n(x)$ is the normal derivative on $\partial \Omega$ at the point $x$ . Don't be mind corrupted, I may have wrong expectation.","Let be a bounded domain. Let be a function in . I would like to compute the following limit: for Here is what I did so far: Let be arbitrarily small enough. Then as is bounded, we have so that Using the fundamental theorem of calculus and  the relation , Therefore my initial question could be resumed to computaion of Any idea on to move further? My feeling is the should be  a multiple factor of where is the normal derivative on at the point . Don't be mind corrupted, I may have wrong expectation.",\Omega\subset \Bbb R^d C^1 u:\Bbb R^d\to \Bbb R C^2_b(\Bbb R^d) x\in \partial \Omega L= \lim_{s\to 1}(1-s)\int_{\Omega}\frac{(u(x)-u(y))}{|x-y|^{d+2s}} d y.  r>0 u \begin{align}&\lim_{s\to 1}(1-s)\int_{\Omega\cap \{|x-y|\geq r\}}\frac{|u(x)-u(y)|}{|x-y|^{d+2s}}dy\\&\leq C \lim_{s\to 1}(1-s)\int_{|x-y|\geq r}\frac{dy}{|x-y|^{d+2s}} \\&= Cc_d\lim_{s\to 1}(1-s) \int_r^\infty t^{-2s-1} dt= 0. \end{align} L= \lim_{s\to 1}(1-s)\int_{\Omega \cap B_r(x)}\frac{(u(x)-u(y))}{|x-y|^{d+2s}} d y.  \nabla [|x|^\alpha]= \alpha x|x|^{\alpha-2} \begin{align}&(1-s)\int_{\Omega \cap B_r(x)}\frac{(u(x)-u(y))}{|x-y|^{d+2s}} d y\\ & = -\int_0^1 dt (1-s)\int_{\Omega \cap B_r(x)}\frac{\nabla u(x+ t(y-x))\cdot (y-x)}{|x-y|^{d+2s}}dy\\&=  \int_0^1 dt\frac{ (1-s)}{d-2(1-s))}\int_{\Omega \cap B_r(x)}  \nabla u(x+ t(y-x))\cdot \nabla_y [|x-y|^{-d+2(1-s)}]dy \end{align} \lim_{s\to 1} (1-s)\int_{\Omega \cap B_r(x)} \nabla u(x+ t(y-x))\cdot \nabla_y [|x-y|^{-d+2(1-s)}]dy. \frac{\partial u}{\partial n}(x)= \nabla u(x).n(x) n(x) \partial \Omega x,"['real-analysis', 'partial-differential-equations', 'contour-integration', 'greens-theorem', 'singular-integrals']"
16,"$I_n(t,a) = \int_0^\infty \frac{\cos(xt)}{\left(x^2 + a^2\right)^n}\:dx$",,"I_n(t,a) = \int_0^\infty \frac{\cos(xt)}{\left(x^2 + a^2\right)^n}\:dx","Spurred on by this , here I'm hoping to resolve the following integral: \begin{equation} I_n(a,t) = \int_0^\infty \frac{\cos(xt)}{\left(x^2 + a^2\right)^n}\:dx \end{equation} Where $a,t \in \mathbb{R}^+$ and $n \in \mathbb{N}$ . To begin with we observe that: \begin{equation} I_n(a,t) =  \int_0^\infty \frac{\cos(xt)}{\left(a^2\left(\frac{x^2}{a^2} + 1\right)\right)^n}\:dx = \frac{1}{a^{2n}}  \int_0^\infty \frac{\cos(xt)}{\left(\left(\frac{x}{a}\right)^2 + 1\right)^n}\:dx \end{equation} Let $u = \frac{x}{a}$ : \begin{align} I_n(a,t) &= \frac{1}{a^{2n}} \int_0^\infty \frac{\cos(uat)}{\left(u^2 + 1\right)^n}\cdot a\:du = a^{1 - 2n}\int_0^\infty \frac{\cos(uat)}{\left(u^2 + 1\right)^n}\:du  \\ &=a^{1 - 2n}I_n(1, at) \end{align} Thus, we need only resolve the following integral to solve $I_n(a,t)$ : \begin{equation} J_n(s) = \int_0^\infty \frac{\cos(su)}{\left(u^2 + 1\right)^n}\:du \end{equation} Noting $I_n(a,t) = J_n(at)$ . Here we will proceed by forming a differential equation for $J_n(s)$ . To do so, we employ Leibniz's Integral Rule and differentiate under the curve twice w.r.t $s$ : \begin{align} \frac{d^2J_n}{ds^2} &= \int_0^\infty \frac{-u^2\cos(su)}{\left(u^2 + 1\right)^n}\:du = -\int_0^\infty \frac{\left(u^2 + 1 - 1\right)\cos(su)}{\left(u^2 + 1\right)^n}\:du \nonumber \\ &=-\left[\int_0^\infty \frac{\cos(su)}{\left(u^2 + 1\right)^{n - 1}}\:du -  \int_0^\infty \frac{\cos(su)}{\left(u^2 + 1\right)^n}\:du\right] \nonumber \\ &=-\left[J_{n - 1}(s) - J_n(s) \right] = J_n(s) - J_{n - 1}(s) \end{align} Thus we form the recursive differential equation: \begin{equation} \frac{d^2J_n}{ds^2}- J_n(s) = -J_{n - 1}(s) \end{equation} In order for a solution to be obtained, the following is required: $I_1(s)$ , $I_n(0)$ , and $I_n'(0)$ . Thankfully these are all easy to obtain. Starting with $I_1(s)$ we find: \begin{equation} I_n(s) = \frac{\pi}{2}e^{-s} \end{equation} For $I_n(0)$ we have: \begin{equation} I_n(0) = \int_0^\infty \frac{1}{\left(u^2 + 1\right)^n}\:du \end{equation} Using the subsitution $u = \tan(w)$ we obtain a solution in terms of the Beta (and by extension Gamma) function: \begin{align} I_n(0) &= \int_0^\frac{\pi}{2} \frac{1}{\left(\tan^2(w) + 1\right)^n}\cdot \sec^2(w)\:dw = \int_0^\frac{\pi}{2} \cos^{2n - 2}(w)\:dw \nonumber \\ &= \frac{1}{2}B\left( \frac{2n - 1}{2}, \frac{1}{2} \right) = \frac{1}{2}\frac{\Gamma\left(\frac{2n - 1}{2}\right)\Gamma\left( \frac{1}{2} \right)}{\Gamma\left(\frac{2n - 1}{2} + \frac{1}{2} \right)} = \frac{\sqrt{\pi}}{2}\frac{\Gamma\left(\frac{2n - 1}{2}\right)}{\Gamma(n)} \end{align} For $I_n'(0)$ we have: \begin{equation} I_n'(0) = \int_0^\infty \frac{-x\sin(x \cdot 0)}{\left(x^2 + 1\right)^n} = 0 \end{equation} Now, and here is where I'm unsure about my process - for our recursive differential equation we take the Laplace Transform: \begin{align} \mathscr{L}_{s \rightarrow p}\left[ \frac{d^2J_n}{ds^2} \right] - \mathscr{L}_{s \rightarrow p}\left[J_n(s) \right] &= -\mathscr{L}_{s \rightarrow p}\left[ J_{n - 1}(s) \right] \nonumber \\ p^2 \overline{J}_n(p) - pJ_n(0) - J_n'(0) - \overline{J}_{n}(p) &= -\overline{J}_{n - 1}(p) \nonumber \\  \left(p^2 - 1\right)\overline{J}_n(p) &= pJ_n(0) -\overline{J}_{n - 1}(p) \end{align} Thus, \begin{equation} \overline{J}_n(p) = \frac{p}{p^2 - 1} J_n(0) - \frac{1}{p^2 - 1}\overline{J}_{n - 1}(p) \end{equation} We now take the Inverse Laplace Transform: \begin{align} \mathscr{L}_{p \rightarrow s}^{-1} \left[\overline{J}_n(p)\right] &= \mathscr{L}_{p \rightarrow s}^{-1} \left[\frac{p}{p^2 - 1}\right]J_n(0)  - \mathscr{L}_{p \rightarrow s}^{-1} \left[\frac{1}{p^2 - 1}\overline{J}_{n - 1}(p)\right] \nonumber \\ J_n(s) &= J_n(0)\cosh(s) - \int_0^s \sinh(s - a)J_{n - 1}(a)\:da \nonumber \\  &= J_n(0)\cosh(s) - \int_0^s \left[\sinh(s)\cosh(a) - \sinh(a)\cosh(s)\right]J_{n - 1}(a)\:da \nonumber \\ &= J_n(0)\cosh(s) - \sinh(s)\int_0^s\cosh(a) J_{n - 1}(a)\:da \nonumber \\ &\quad+ \cosh(s)\int_0^2 \sinh(a)J_{n - 1}(a)\:da \end{align} Now whilst we have a recursive integral form that governs $J_n(s)$ I am unsure how to solve it!. Does anyone have any pointers about how to move forward? Another approach (I believe) is to employ the linear D-operator. Here if we define $D = \frac{d}{ds}$ then our governing differential equation is given by: \begin{equation} \left(D - 1\right)\left(D + 1\right)\left[ J_{n}(s)\right] = -J_{n - 1}(s) \end{equation} Thus, \begin{equation} J_n(s) = -\left(\left(D - 1\right)\left(D + 1\right)\right)^{-1}\left[ J_{n-1}(s)\right] \end{equation} Which is my reasoning is correct implies that \begin{align} J_n(s) &= (-1)^n \left(\left(D - 1\right)\left(D + 1\right)\right)^{-(n - 1)}\left[ J_1(s)\right] = (-1)^n \left(\left(D - 1\right)\left(D + 1\right)\right)^{-(n - 1)}\left[ \frac{\pi}{2}e^{-s}\right] \nonumber \\ &= (-1)^n \frac{\pi}{2} \left(\left(D - 1\right)\left(D + 1\right)\right)^{-(n - 1)}\left[ e^{-s}\right] \end{align}","Spurred on by this , here I'm hoping to resolve the following integral: Where and . To begin with we observe that: Let : Thus, we need only resolve the following integral to solve : Noting . Here we will proceed by forming a differential equation for . To do so, we employ Leibniz's Integral Rule and differentiate under the curve twice w.r.t : Thus we form the recursive differential equation: In order for a solution to be obtained, the following is required: , , and . Thankfully these are all easy to obtain. Starting with we find: For we have: Using the subsitution we obtain a solution in terms of the Beta (and by extension Gamma) function: For we have: Now, and here is where I'm unsure about my process - for our recursive differential equation we take the Laplace Transform: Thus, We now take the Inverse Laplace Transform: Now whilst we have a recursive integral form that governs I am unsure how to solve it!. Does anyone have any pointers about how to move forward? Another approach (I believe) is to employ the linear D-operator. Here if we define then our governing differential equation is given by: Thus, Which is my reasoning is correct implies that","\begin{equation}
I_n(a,t) = \int_0^\infty \frac{\cos(xt)}{\left(x^2 + a^2\right)^n}\:dx
\end{equation} a,t \in \mathbb{R}^+ n \in \mathbb{N} \begin{equation}
I_n(a,t) =  \int_0^\infty \frac{\cos(xt)}{\left(a^2\left(\frac{x^2}{a^2} + 1\right)\right)^n}\:dx = \frac{1}{a^{2n}}  \int_0^\infty \frac{\cos(xt)}{\left(\left(\frac{x}{a}\right)^2 + 1\right)^n}\:dx
\end{equation} u = \frac{x}{a} \begin{align}
I_n(a,t) &= \frac{1}{a^{2n}} \int_0^\infty \frac{\cos(uat)}{\left(u^2 + 1\right)^n}\cdot a\:du = a^{1 - 2n}\int_0^\infty \frac{\cos(uat)}{\left(u^2 + 1\right)^n}\:du  \\
&=a^{1 - 2n}I_n(1, at)
\end{align} I_n(a,t) \begin{equation}
J_n(s) = \int_0^\infty \frac{\cos(su)}{\left(u^2 + 1\right)^n}\:du
\end{equation} I_n(a,t) = J_n(at) J_n(s) s \begin{align}
\frac{d^2J_n}{ds^2} &= \int_0^\infty \frac{-u^2\cos(su)}{\left(u^2 + 1\right)^n}\:du = -\int_0^\infty \frac{\left(u^2 + 1 - 1\right)\cos(su)}{\left(u^2 + 1\right)^n}\:du \nonumber \\
&=-\left[\int_0^\infty \frac{\cos(su)}{\left(u^2 + 1\right)^{n - 1}}\:du -  \int_0^\infty \frac{\cos(su)}{\left(u^2 + 1\right)^n}\:du\right] \nonumber \\
&=-\left[J_{n - 1}(s) - J_n(s) \right] = J_n(s) - J_{n - 1}(s)
\end{align} \begin{equation}
\frac{d^2J_n}{ds^2}- J_n(s) = -J_{n - 1}(s)
\end{equation} I_1(s) I_n(0) I_n'(0) I_1(s) \begin{equation}
I_n(s) = \frac{\pi}{2}e^{-s}
\end{equation} I_n(0) \begin{equation}
I_n(0) = \int_0^\infty \frac{1}{\left(u^2 + 1\right)^n}\:du
\end{equation} u = \tan(w) \begin{align}
I_n(0) &= \int_0^\frac{\pi}{2} \frac{1}{\left(\tan^2(w) + 1\right)^n}\cdot \sec^2(w)\:dw = \int_0^\frac{\pi}{2} \cos^{2n - 2}(w)\:dw \nonumber \\
&= \frac{1}{2}B\left( \frac{2n - 1}{2}, \frac{1}{2} \right) = \frac{1}{2}\frac{\Gamma\left(\frac{2n - 1}{2}\right)\Gamma\left( \frac{1}{2} \right)}{\Gamma\left(\frac{2n - 1}{2} + \frac{1}{2} \right)} = \frac{\sqrt{\pi}}{2}\frac{\Gamma\left(\frac{2n - 1}{2}\right)}{\Gamma(n)}
\end{align} I_n'(0) \begin{equation}
I_n'(0) = \int_0^\infty \frac{-x\sin(x \cdot 0)}{\left(x^2 + 1\right)^n} = 0
\end{equation} \begin{align}
\mathscr{L}_{s \rightarrow p}\left[ \frac{d^2J_n}{ds^2} \right] - \mathscr{L}_{s \rightarrow p}\left[J_n(s) \right] &= -\mathscr{L}_{s \rightarrow p}\left[ J_{n - 1}(s) \right] \nonumber \\
p^2 \overline{J}_n(p) - pJ_n(0) - J_n'(0) - \overline{J}_{n}(p) &= -\overline{J}_{n - 1}(p) \nonumber \\ 
\left(p^2 - 1\right)\overline{J}_n(p) &= pJ_n(0) -\overline{J}_{n - 1}(p)
\end{align} \begin{equation}
\overline{J}_n(p) = \frac{p}{p^2 - 1} J_n(0) - \frac{1}{p^2 - 1}\overline{J}_{n - 1}(p)
\end{equation} \begin{align}
\mathscr{L}_{p \rightarrow s}^{-1} \left[\overline{J}_n(p)\right] &= \mathscr{L}_{p \rightarrow s}^{-1} \left[\frac{p}{p^2 - 1}\right]J_n(0)  - \mathscr{L}_{p \rightarrow s}^{-1} \left[\frac{1}{p^2 - 1}\overline{J}_{n - 1}(p)\right] \nonumber \\
J_n(s) &= J_n(0)\cosh(s) - \int_0^s \sinh(s - a)J_{n - 1}(a)\:da \nonumber \\
 &= J_n(0)\cosh(s) - \int_0^s \left[\sinh(s)\cosh(a) - \sinh(a)\cosh(s)\right]J_{n - 1}(a)\:da \nonumber \\
&= J_n(0)\cosh(s) - \sinh(s)\int_0^s\cosh(a) J_{n - 1}(a)\:da \nonumber \\
&\quad+ \cosh(s)\int_0^2 \sinh(a)J_{n - 1}(a)\:da
\end{align} J_n(s) D = \frac{d}{ds} \begin{equation}
\left(D - 1\right)\left(D + 1\right)\left[ J_{n}(s)\right] = -J_{n - 1}(s)
\end{equation} \begin{equation}
J_n(s) = -\left(\left(D - 1\right)\left(D + 1\right)\right)^{-1}\left[ J_{n-1}(s)\right]
\end{equation} \begin{align}
J_n(s) &= (-1)^n \left(\left(D - 1\right)\left(D + 1\right)\right)^{-(n - 1)}\left[ J_1(s)\right] = (-1)^n \left(\left(D - 1\right)\left(D + 1\right)\right)^{-(n - 1)}\left[ \frac{\pi}{2}e^{-s}\right] \nonumber \\
&= (-1)^n \frac{\pi}{2} \left(\left(D - 1\right)\left(D + 1\right)\right)^{-(n - 1)}\left[ e^{-s}\right]
\end{align}","['real-analysis', 'integration']"
17,What are the functions for which ${f f''\over f'^2} < 2$?,What are the functions for which ?,{f f''\over f'^2} < 2,"What are the functions $f$ on $[0,1]$ (with continuous first and second derivatives) that satisfy the following conditions: Monotonically increasing in $[0,1]$, with $f(0)\geq 0$ and $f(1)=1$; for all $x\in[0,1]$: $$ {f(x) f''(x) \over f'(x)^2} < 2 $$ ? Some simple examples are: $f(x) = x^k$ for some $k> 0$. Then: $f(x)f''(x)=k(k-1)x^{2k-2}$ and $f'(x)^2 = k^2 x^{2k-2}$ so the quotient is $(k-1)/k < 1 < 2$. $f(x) = e^{k (x-1)}$ for some $k\geq 0$. Then: $f(x)f''(x) = k^2 f(x) = f'(x)$ so the quotient is $1 < 2$. Is there a general form of functions that satisfy these conditions?","What are the functions $f$ on $[0,1]$ (with continuous first and second derivatives) that satisfy the following conditions: Monotonically increasing in $[0,1]$, with $f(0)\geq 0$ and $f(1)=1$; for all $x\in[0,1]$: $$ {f(x) f''(x) \over f'(x)^2} < 2 $$ ? Some simple examples are: $f(x) = x^k$ for some $k> 0$. Then: $f(x)f''(x)=k(k-1)x^{2k-2}$ and $f'(x)^2 = k^2 x^{2k-2}$ so the quotient is $(k-1)/k < 1 < 2$. $f(x) = e^{k (x-1)}$ for some $k\geq 0$. Then: $f(x)f''(x) = k^2 f(x) = f'(x)$ so the quotient is $1 < 2$. Is there a general form of functions that satisfy these conditions?",,"['real-analysis', 'functional-analysis', 'ordinary-differential-equations']"
18,"Prove that $f_n(x)=\frac{x}{n}$, $n=1,2,\ldots$ does not converge uniformly on $\mathbb{R}$","Prove that ,  does not converge uniformly on","f_n(x)=\frac{x}{n} n=1,2,\ldots \mathbb{R}","Prove that $f_n(x)=\frac{x}{n}$, $n=1,2,\ldots$ does not converge uniformly on   $\mathbb{R}$. It's clear this function converges pointwise to $0$ function. We have to show that there is $\epsilon>0$ such that for any $N\in \mathbb{N}$ there are $n>N$ and $x$ such that $|\frac{x}{n}|\ge \epsilon$. Choose $\epsilon=1$. However big $N$ is, we can choose $x>N+1$ so we have $|x|\ge n$ if $n=N+1$. Is this correct?","Prove that $f_n(x)=\frac{x}{n}$, $n=1,2,\ldots$ does not converge uniformly on   $\mathbb{R}$. It's clear this function converges pointwise to $0$ function. We have to show that there is $\epsilon>0$ such that for any $N\in \mathbb{N}$ there are $n>N$ and $x$ such that $|\frac{x}{n}|\ge \epsilon$. Choose $\epsilon=1$. However big $N$ is, we can choose $x>N+1$ so we have $|x|\ge n$ if $n=N+1$. Is this correct?",,"['calculus', 'real-analysis', 'proof-verification', 'continuity']"
19,Comparison of integrals,Comparison of integrals,,"Under what conditions on $f$ can we conclude the following inequality: $$\left(\int_a^b f \, \mathrm{d}x\right)^2 \leq \int_a^b f^2 \, \mathrm{d}x.$$ Cauchy-Schwarz looked appealing at first: $$\left(\int_a^b fg \, \mathrm{d}x \right)^2 \leq \left(\int_a^b f^2 \, \mathrm{d}x\right)\left(\int_a^b g^2 \, \mathrm{d}x\right).$$ Setting $g \equiv 1$, we get $$\left(\int_a^b f \, \mathrm{d}x \right)^2 \leq (b-a)\left(\int_a^b f^2 \, \mathrm{d}x\right),$$ so if $b - a \leq 1$, we are done. But what about for more general intervals? I think the answer lies in a more clever substitution/manipulation. Feel free to cite as many powerful inequalities as you want. All integration is done in the Riemann sense, but answers with Lebesgue are welcome.","Under what conditions on $f$ can we conclude the following inequality: $$\left(\int_a^b f \, \mathrm{d}x\right)^2 \leq \int_a^b f^2 \, \mathrm{d}x.$$ Cauchy-Schwarz looked appealing at first: $$\left(\int_a^b fg \, \mathrm{d}x \right)^2 \leq \left(\int_a^b f^2 \, \mathrm{d}x\right)\left(\int_a^b g^2 \, \mathrm{d}x\right).$$ Setting $g \equiv 1$, we get $$\left(\int_a^b f \, \mathrm{d}x \right)^2 \leq (b-a)\left(\int_a^b f^2 \, \mathrm{d}x\right),$$ so if $b - a \leq 1$, we are done. But what about for more general intervals? I think the answer lies in a more clever substitution/manipulation. Feel free to cite as many powerful inequalities as you want. All integration is done in the Riemann sense, but answers with Lebesgue are welcome.",,"['real-analysis', 'integration']"
20,Understanding what people mean in PDEs,Understanding what people mean in PDEs,,"I have noticed that two omissions are frequent in PDE theory, to the point that it has even cropped up in textbooks and very much discouraged my study of the field.  This is my 5th attempt at learning the subject seriously, and I would like some advice, say in the form of answers to the following specific questions, thanks: Why is the solution space rarely specified?  Even in ODE theory, we need to say ""We are looking for solutions in the space of continuous functions."" (Or more usually $C^k$ where $k$ is the order of the DEQ, although vacuously no function with not enough smoothness could solve an ODE in the traditional sense.) Speaking of the traditional sense as opposed to distributional or whatever else exists in PDE theory (I've heard terms like ""weak solution""), how come none of this stuff showed up in ODE theory?  In $d=1$ are all distributions functions?  I am aware, without proof, of a result that says that all linear systems have a distributional solution if and only if it is, in some appropriate sense, a function, for $d=1$.  What about other systems?  Couldn't richer collections of solutions be obtained if we allowed distributions? In both PDE and ODE theory, I have noticed only half the problem is usually solved, in the sense that the authors usually take a given equation, and perform operations on it and end up at some ""solution.""  But this solution need not actually work, especially if noninvertible operations are used, such as the Fourier transform as it's defined on $L^1$.  In ODE theory, without allowing for the generalities in 2., it has usually been easy to verify a guessed solution indeed works.  Although it may not be as trivial, is this what authors are assuming in 3.?  Or is there some agreement among those in PDE theory that not all guessed solutions may actually work?","I have noticed that two omissions are frequent in PDE theory, to the point that it has even cropped up in textbooks and very much discouraged my study of the field.  This is my 5th attempt at learning the subject seriously, and I would like some advice, say in the form of answers to the following specific questions, thanks: Why is the solution space rarely specified?  Even in ODE theory, we need to say ""We are looking for solutions in the space of continuous functions."" (Or more usually $C^k$ where $k$ is the order of the DEQ, although vacuously no function with not enough smoothness could solve an ODE in the traditional sense.) Speaking of the traditional sense as opposed to distributional or whatever else exists in PDE theory (I've heard terms like ""weak solution""), how come none of this stuff showed up in ODE theory?  In $d=1$ are all distributions functions?  I am aware, without proof, of a result that says that all linear systems have a distributional solution if and only if it is, in some appropriate sense, a function, for $d=1$.  What about other systems?  Couldn't richer collections of solutions be obtained if we allowed distributions? In both PDE and ODE theory, I have noticed only half the problem is usually solved, in the sense that the authors usually take a given equation, and perform operations on it and end up at some ""solution.""  But this solution need not actually work, especially if noninvertible operations are used, such as the Fourier transform as it's defined on $L^1$.  In ODE theory, without allowing for the generalities in 2., it has usually been easy to verify a guessed solution indeed works.  Although it may not be as trivial, is this what authors are assuming in 3.?  Or is there some agreement among those in PDE theory that not all guessed solutions may actually work?",,"['real-analysis', 'partial-differential-equations']"
21,Is $e = \sum_n 1/n!$ the most efficient sequence of denominators for rational series for $e$?,Is  the most efficient sequence of denominators for rational series for ?,e = \sum_n 1/n! e,"The classical series $e = \lim_{n \to \infty} X_n$ where $X_n = \sum_{k=0}^n 1/k!$ is incredibly efficient. But is it known to be the most efficient series in terms of denominators for using fractions in the sum? In other words, is it known whether there is another series of fractions $e = \lim_{n \to \infty} Y_n$ where $Y_n = \sum_{k=0}^n a_k/b_k$ ($b_k > 0$, $a_k,b_k$ integers) where $\lim_{k \to \infty} b_k/k! \leq 1$ and $\lim_{n \to \infty} |(Y_n - e)/(X_n -e)| < 1$?","The classical series $e = \lim_{n \to \infty} X_n$ where $X_n = \sum_{k=0}^n 1/k!$ is incredibly efficient. But is it known to be the most efficient series in terms of denominators for using fractions in the sum? In other words, is it known whether there is another series of fractions $e = \lim_{n \to \infty} Y_n$ where $Y_n = \sum_{k=0}^n a_k/b_k$ ($b_k > 0$, $a_k,b_k$ integers) where $\lim_{k \to \infty} b_k/k! \leq 1$ and $\lim_{n \to \infty} |(Y_n - e)/(X_n -e)| < 1$?",,"['real-analysis', 'sequences-and-series', 'approximation']"
22,Motivation behind Dedekind's cut set,Motivation behind Dedekind's cut set,,"I want to know the motivation behind Dedekind's real number construction. The motivation of such properties of the cut sets is not clear to me.  BTW, I am new to real analysis and just have started reading the first chapter of Rudin. what made Dedekind to thought of sets with some nice properties (what are the motivations behind such properties ? For example, why cut set should not have any greatest element ?). I read somewhere that initially Dedekind thought that any real number can be uniquely identified by rational numbers less than that. So, this was his starting point. From that the first and second property of cut set is understandable, but the third property (no greatest element) is not clear to me.","I want to know the motivation behind Dedekind's real number construction. The motivation of such properties of the cut sets is not clear to me.  BTW, I am new to real analysis and just have started reading the first chapter of Rudin. what made Dedekind to thought of sets with some nice properties (what are the motivations behind such properties ? For example, why cut set should not have any greatest element ?). I read somewhere that initially Dedekind thought that any real number can be uniquely identified by rational numbers less than that. So, this was his starting point. From that the first and second property of cut set is understandable, but the third property (no greatest element) is not clear to me.",,['real-analysis']
23,Inverse of a differentiable function equal to its derivative then f is analytic,Inverse of a differentiable function equal to its derivative then f is analytic,,"I've found a nice problem concerning analytic functions. Here it is: Let $f: (0, \infty) \rightarrow \mathbb{R}$ be a  function differentiable on $(0, \infty)$ and such that $f^{-1} = f'$. Prove that $f$ is analytic on $(0, \infty)$. I'm not sure if it's relevant, but I know that $f$ cannot be a bijection :) Could you help me?","I've found a nice problem concerning analytic functions. Here it is: Let $f: (0, \infty) \rightarrow \mathbb{R}$ be a  function differentiable on $(0, \infty)$ and such that $f^{-1} = f'$. Prove that $f$ is analytic on $(0, \infty)$. I'm not sure if it's relevant, but I know that $f$ cannot be a bijection :) Could you help me?",,"['real-analysis', 'derivatives', 'analyticity']"
24,Existence of a specific reordering bijection,Existence of a specific reordering bijection,,"Please consider a bijection $g:\mathbb{N}\rightarrow\mathbb{N}$ with following properties: For all real series $(a_n)_{n\geq1}$, convergence of $\sum_{n=1}^{\infty}a_n$ implies convergence of $\sum_{n=1}^{\infty}a_{g(n)}$ Exist at least one real series $(c_n)_{c\geq1}$, that $\sum_{n=1}^{\infty} c_n$ diverge, but $\sum_{n=1}^{\infty}c_{g(n)}$ converge. If such bijection exist?","Please consider a bijection $g:\mathbb{N}\rightarrow\mathbb{N}$ with following properties: For all real series $(a_n)_{n\geq1}$, convergence of $\sum_{n=1}^{\infty}a_n$ implies convergence of $\sum_{n=1}^{\infty}a_{g(n)}$ Exist at least one real series $(c_n)_{c\geq1}$, that $\sum_{n=1}^{\infty} c_n$ diverge, but $\sum_{n=1}^{\infty}c_{g(n)}$ converge. If such bijection exist?",,"['real-analysis', 'analysis', 'convergence-divergence']"
25,An infinite series expansion in terms of the polylogarithm function,An infinite series expansion in terms of the polylogarithm function,,"We have the complex valued function: $$f(z)=\sum_{n=0}^{\infty}a_{n}\text{Li}_{-n}(z)\;\;\;\;\;\;\;(\left | z\right |<1)$$ We wish to recover the coefficients $a_{n}$. The only thing I though would work is to try and come up with a function $\phi(n,x)$, such that: $$\int f(z)\phi(n,z)dz=a_{n}$$ or: $$\int\text{Li}_{-n}(z)\phi(m,z)dz=\delta_{nm}$$ but that's about as far as I've gotten. Any help is appreciated.  The question is motivated by the following: Suppose that for some analytic function $g(x)$ we have the values of the function at positive integers, so we can write a Taylor development : $$g(m)=\sum_{n=0}^{\infty}a_{n}m^{n}$$ Now suppose that the following summation is convergent in the open unit disk: $$\sum_{m=1}^{\infty}g(m)z^{m}$$ Using the above Taylor expansion, and the definition of the Polylogarithm function, we have: $$\sum_{m=1}^{\infty}g(m)z^{m}=:f(z)=\sum_{n=0}^{\infty}a_{n}\text{Li}_{-n}(z)\;\;\left | z\right |<1$$ The plan is to recover the coefficients $a_{n}$, and the thus the Taylor expansion of $g(z)$. EDIT: By Ramanujan's master theorem , $g(z)$ is given by: $$\frac{\pi}{\sin(\pi s)}g(-s)=\int_{0}^{\infty}\left(f(-x)+g(0)\right)x^{s-1}dx\;\;\;\;(0<\Re(s)<1)$$ However, the function $f(x)$ is not always convergent along the real line, hence the quest for an alternative.","We have the complex valued function: $$f(z)=\sum_{n=0}^{\infty}a_{n}\text{Li}_{-n}(z)\;\;\;\;\;\;\;(\left | z\right |<1)$$ We wish to recover the coefficients $a_{n}$. The only thing I though would work is to try and come up with a function $\phi(n,x)$, such that: $$\int f(z)\phi(n,z)dz=a_{n}$$ or: $$\int\text{Li}_{-n}(z)\phi(m,z)dz=\delta_{nm}$$ but that's about as far as I've gotten. Any help is appreciated.  The question is motivated by the following: Suppose that for some analytic function $g(x)$ we have the values of the function at positive integers, so we can write a Taylor development : $$g(m)=\sum_{n=0}^{\infty}a_{n}m^{n}$$ Now suppose that the following summation is convergent in the open unit disk: $$\sum_{m=1}^{\infty}g(m)z^{m}$$ Using the above Taylor expansion, and the definition of the Polylogarithm function, we have: $$\sum_{m=1}^{\infty}g(m)z^{m}=:f(z)=\sum_{n=0}^{\infty}a_{n}\text{Li}_{-n}(z)\;\;\left | z\right |<1$$ The plan is to recover the coefficients $a_{n}$, and the thus the Taylor expansion of $g(z)$. EDIT: By Ramanujan's master theorem , $g(z)$ is given by: $$\frac{\pi}{\sin(\pi s)}g(-s)=\int_{0}^{\infty}\left(f(-x)+g(0)\right)x^{s-1}dx\;\;\;\;(0<\Re(s)<1)$$ However, the function $f(x)$ is not always convergent along the real line, hence the quest for an alternative.",,"['real-analysis', 'complex-analysis', 'special-functions', 'hilbert-spaces']"
26,$L^{2}(\mathbb R)$- norm of entire function,- norm of entire function,L^{2}(\mathbb R),"Let $f(z)$ be an entire function defined by $$f(z)=\prod_{n=1}^{\infty}\bigg(1-\frac{z^{2}}{a_{n}^{2}}\bigg),\qquad z\in \mathbb C$$ where $\{a_{n}\}_{n=1}^{\infty}$ is a sequence of positive real numbers, determined so that the  infinite product above defines an entire function. How can we compute the integral $$\int_{-\infty}^{\infty}|f(x)|^{2}dx$$ where $x$ is real. Or at least finding an upper bound for it (if it is finite)?","Let $f(z)$ be an entire function defined by $$f(z)=\prod_{n=1}^{\infty}\bigg(1-\frac{z^{2}}{a_{n}^{2}}\bigg),\qquad z\in \mathbb C$$ where $\{a_{n}\}_{n=1}^{\infty}$ is a sequence of positive real numbers, determined so that the  infinite product above defines an entire function. How can we compute the integral $$\int_{-\infty}^{\infty}|f(x)|^{2}dx$$ where $x$ is real. Or at least finding an upper bound for it (if it is finite)?",,"['real-analysis', 'complex-analysis', 'convergence-divergence']"
27,"Evaluate $\sum \limits_{n=1}^\infty \frac1{L_n}$ where $L_n$ is least common multiple of $1, 2, 3,\ldots,n$",Evaluate  where  is least common multiple of,"\sum \limits_{n=1}^\infty \frac1{L_n} L_n 1, 2, 3,\ldots,n","How to evaluate the sum $\displaystyle\sum \limits_{n=1}^\infty \frac1{L_n}$? Where $L_n$ is the least common multiple of $1, 2, 3,\ldots, n$, e.g. least common multiple of $(6,3,4)$ is $12$.","How to evaluate the sum $\displaystyle\sum \limits_{n=1}^\infty \frac1{L_n}$? Where $L_n$ is the least common multiple of $1, 2, 3,\ldots, n$, e.g. least common multiple of $(6,3,4)$ is $12$.",,"['real-analysis', 'sequences-and-series']"
28,The Fourier transform of $e^{-i/x}$,The Fourier transform of,e^{-i/x},"$\def\R{\mathbb R}$ Question. Does anyone know what is the Fourier transform of $$ f(x)=e^{-i/x} $$ on the real line? I would like to compute it explicitly, or to establish some properties to have a good feeling of “how it looks like”. What I know I know that $\widehat f$ is real-valued by the symmetries of the Fourier transform, since $f(-x)=\overline{f(x)}$ . It is natural to consider $h(x)=f(x)-1$ . It is immediate that $h$ decays like $-i/x$ for large $x$ , so $\widehat h$ belongs to $L^p(\mathbb R)$ for any $p\in [2,\infty)$ by Hausdorff-Young inequality. Then $\widehat f$ is recovered just by adding a Dirac delta. Motivation. (not necessary to understand the problem) For some reason, I was looking at the “anti-transport” equation $$ \left\{\begin{aligned}&u_t-\partial_x^{-1}u=0,\\&u|_{t=0}=u_0.\end{aligned}\right. $$ Even if it does not entirely make sense, one can consider the unitary group $e^{t\partial_x^{-1}}$ , which is well-defined on $L^2$ , that acts on the Fourier side as a multiplication by $e^{-it/\xi}$ . The kernel of this PDE coincides with a rescaled version of the anti-Fourier transform of $e^{-i/\xi}$ , in particular it holds $$ u(t)=u_0+H_t*u_0, \quad\text{with}\quad H_t(x):=t\hat h(tx) $$ for any $u_0\in L^1(\R)\cap L^2(\R)$ (up to multiplicative constants). This is why I considered the above problem. I was wondering what is the time-regularity of solutions of such PDEs, even for smooth initial data: I find a bit funny that the operator $\partial_x^{-1}$ should in principle “smoothen things out” if well-defined, but it seems that the time-derivative of a solution of the PDE is not necessarily smooth for smooth data, unless one imposes some low-frequency condition on the initial datum (see below). A similar PDE with dispersion relation that is singular at low frequencies pops up in the linear water wave theory for the deep water case: the PDE would look something like $$ u_t-|\partial_x|^{-1/2}u=0. $$ Edit : as pointed out by Matthew Cassell, one can write the PDE as a system: $$ \left\{\begin{aligned}u_t&=v\\v_x&=u\end{aligned}\right. $$ In principle, $v$ is defined from $u$ only up to an additive constant, which could depend on time in a non-trivial way. This system gives the following information for solutions defined via the $L^2$ -group, $u(t)=e^{t\partial_x^{-1}}u_0$ : if the initial datum $u_0$ lies in $\partial_x L^2(\R)$ , then one can see that $u\in C(\mathbb R;\partial_x L^2(\R))$ , in particular the anti-derivative $v$ is well-defined and $v\in C(\mathbb R; L^2(\R))$ , so since $u_t=v$ , we obtain that $u-u_0\in C^1(\R; L^2(\R))$ . This procedure can be iterated: Lemma Let $u_0\in \partial_x^j L^2(\R)\cap H^k(\R)$ , and $u(t):=e^{t\partial_x^{-1}}u_0$ . One has $u-u_0\in C^j(\R;H^{j+k}(\R))$ . This is a way of seeing how the time regularity of the solution is related to the low frequencies of the initial datum $u_0$ vanishing. More precise information can be derived (like fractional regularity in time) by looking at the Fourier transform of the solution in space-time.","Question. Does anyone know what is the Fourier transform of on the real line? I would like to compute it explicitly, or to establish some properties to have a good feeling of “how it looks like”. What I know I know that is real-valued by the symmetries of the Fourier transform, since . It is natural to consider . It is immediate that decays like for large , so belongs to for any by Hausdorff-Young inequality. Then is recovered just by adding a Dirac delta. Motivation. (not necessary to understand the problem) For some reason, I was looking at the “anti-transport” equation Even if it does not entirely make sense, one can consider the unitary group , which is well-defined on , that acts on the Fourier side as a multiplication by . The kernel of this PDE coincides with a rescaled version of the anti-Fourier transform of , in particular it holds for any (up to multiplicative constants). This is why I considered the above problem. I was wondering what is the time-regularity of solutions of such PDEs, even for smooth initial data: I find a bit funny that the operator should in principle “smoothen things out” if well-defined, but it seems that the time-derivative of a solution of the PDE is not necessarily smooth for smooth data, unless one imposes some low-frequency condition on the initial datum (see below). A similar PDE with dispersion relation that is singular at low frequencies pops up in the linear water wave theory for the deep water case: the PDE would look something like Edit : as pointed out by Matthew Cassell, one can write the PDE as a system: In principle, is defined from only up to an additive constant, which could depend on time in a non-trivial way. This system gives the following information for solutions defined via the -group, : if the initial datum lies in , then one can see that , in particular the anti-derivative is well-defined and , so since , we obtain that . This procedure can be iterated: Lemma Let , and . One has . This is a way of seeing how the time regularity of the solution is related to the low frequencies of the initial datum vanishing. More precise information can be derived (like fractional regularity in time) by looking at the Fourier transform of the solution in space-time.","\def\R{\mathbb R}  f(x)=e^{-i/x}  \widehat f f(-x)=\overline{f(x)} h(x)=f(x)-1 h -i/x x \widehat h L^p(\mathbb R) p\in [2,\infty) \widehat f  \left\{\begin{aligned}&u_t-\partial_x^{-1}u=0,\\&u|_{t=0}=u_0.\end{aligned}\right.  e^{t\partial_x^{-1}} L^2 e^{-it/\xi} e^{-i/\xi}  u(t)=u_0+H_t*u_0, \quad\text{with}\quad H_t(x):=t\hat h(tx)  u_0\in L^1(\R)\cap L^2(\R) \partial_x^{-1}  u_t-|\partial_x|^{-1/2}u=0.   \left\{\begin{aligned}u_t&=v\\v_x&=u\end{aligned}\right.  v u L^2 u(t)=e^{t\partial_x^{-1}}u_0 u_0 \partial_x L^2(\R) u\in C(\mathbb R;\partial_x L^2(\R)) v v\in C(\mathbb R; L^2(\R)) u_t=v u-u_0\in C^1(\R; L^2(\R)) u_0\in \partial_x^j L^2(\R)\cap H^k(\R) u(t):=e^{t\partial_x^{-1}}u_0 u-u_0\in C^j(\R;H^{j+k}(\R)) u_0","['real-analysis', 'functional-analysis', 'partial-differential-equations', 'fourier-analysis', 'fourier-transform']"
29,"Is there a path-connected, ""anti-convex"" subset of $\mathbb R^2$ containing $(\mathbb R\smallsetminus \mathbb Q)^2$?","Is there a path-connected, ""anti-convex"" subset of  containing ?",\mathbb R^2 (\mathbb R\smallsetminus \mathbb Q)^2,"This question was firstly asked in Mathematics Stack Exchange. Getting no answer, I copied it to Math Overflow, as Moishe Kohan commented. For a vector space $V$ over $\mathbb R$ , I say a subset $S$ of $V$ is ""anti-convex"" if $\forall a,b\in S (a\ne b)$ , $\exists t\in \mathopen]0,1\mathclose[$ , $b+t(a-b)\not\in S$ . For example, all hollow circles $(x-a)^2+(y-b)^2=r^2$ on $\mathbb R^2$ are anti-convex. Now I want to know if there is a path-connected, ""anti-convex"" subset of $\mathbb R^2$ containing $(\mathbb R\smallsetminus \mathbb Q)^2$ . My approach I've proven the case when $(\mathbb R\smallsetminus \mathbb Q)^2$ were replaced by a countable subset of $\mathbb R^2$ - Put them in order, and using hollow circles to connect the adjacent elements will work. Further question (edited thanks to Sam Hopkins) Actually, I originally guessed that, if $\dim V\ge 2$ , then for all anti-convex subset $S$ , there's a path-connected anti-convex set containing $S$ , but I'm already stuck at this special case. Edit Since the original question was solved in Math Overflow, the one who answered my further question will receive the bounty.","This question was firstly asked in Mathematics Stack Exchange. Getting no answer, I copied it to Math Overflow, as Moishe Kohan commented. For a vector space over , I say a subset of is ""anti-convex"" if , , . For example, all hollow circles on are anti-convex. Now I want to know if there is a path-connected, ""anti-convex"" subset of containing . My approach I've proven the case when were replaced by a countable subset of - Put them in order, and using hollow circles to connect the adjacent elements will work. Further question (edited thanks to Sam Hopkins) Actually, I originally guessed that, if , then for all anti-convex subset , there's a path-connected anti-convex set containing , but I'm already stuck at this special case. Edit Since the original question was solved in Math Overflow, the one who answered my further question will receive the bounty.","V \mathbb R S V \forall a,b\in S (a\ne b) \exists t\in \mathopen]0,1\mathclose[ b+t(a-b)\not\in S (x-a)^2+(y-b)^2=r^2 \mathbb R^2 \mathbb R^2 (\mathbb R\smallsetminus \mathbb Q)^2 (\mathbb R\smallsetminus \mathbb Q)^2 \mathbb R^2 \dim V\ge 2 S S","['real-analysis', 'general-topology', 'geometry', 'convex-analysis', 'dense-subspaces']"
30,The fabulous Wenger's Summation,The fabulous Wenger's Summation,,"The user Avery Wenger said he came up with this problem randomly and I think it's very interesting! Let $s:\mathbb N\to\mathbb N$ be the sum of digits function. Then, Wenger defines $w:\mathbb N\to\mathbb N$ given by $$w(n):=\min\{k\in\mathbb N: s^k(n)\;\mbox{has a single digit}\}\;\forall n\in\mathbb N$$ Now comes the really interesting part: the Wenger's Summation . $$W_m:=\sum_{n=1}^m (-1)^{n+1}w(n)$$ What is so interesting about it? Its amazingly weird graph! As you can see, it behaves very poorly at first, looking kinda bumpy (jumps seem to occur on multiples of powers of ten). Soon, however, the graph starts to look very much like a line! Is this really Wenger's Summation asymptotic behavior? Is there a real constant $\omega$ (the Wenger's Constant) such that $$\lim_{m\to\infty} \frac{W_m}{m} = \omega\;?$$ If not, then what is Wenger's Summation asymptotic behavior? Can we at least show $(W_m)_m$ to be unbounded? But of course, this is only the decimal Wenger's Summation , since we are taking number in decimal notation! What is so special about $10$ , anyway? What about the binary Wenger's Summation ? Does it behave just as oddly? It actually does worse! So, for every possible basis $b$ we have a different Wenger's Summation $(W_m^b)_m$ and a different Wenger's Constant $\omega^b$ . Ain't that wonderful? Is it true that, if $a<b$ , then $\omega^a>\omega^b$ ? The alternating signals makes proving all this stuff so awkward. I'm just amazed about this very innocent looking procedure generating such a beautiful cacophony. It all looks so useless and disconnected to... anything else. Do mathematics even has the tools to answer these questions? If you can find anything interesting about Wenger's Summations, please let me know it! Attention: Be aware Avery Wenger did not call all these mathematical objects by his own name in the original post. I'm the only one responsible for that (Avery, if you are seeing this, thank you for sharing the formidable problem that randomly came to you). Values for the Wenger's Summation were calculated through this rudimentary python script. import matplotlib.pyplot as plt import numpy as np  b = 4 # basis  def w(n):     ap = 0 # additive persistence     while n>=b:         aux = n         sum = 0         while aux>0:             sum+=(aux%b)             aux//=b         n = sum         ap+=1     return ap  S = 0  # Wenger's summation W = [] for n in range(100000000):     S+=w(n)*(-1)**(n+1)     W.append(S) Update: A comment pointed out the value $w(n)$ is called the additive persistence of $n$ . The sequence $(w(n))_n$ (OEIS A031286 for base $10$ ) has some interesting properties. For example, every positive integer appears in it infinitely many times. Furthermore, the first occurrence of $N$ in it (OEIS A006050 for base $10$ ) is given by the recurrence formula $a(N)=2\cdot b^{\frac{a(N-1)-1}{b-1}}-1$ , so its always odd , i.e., $N$ appears for the first time in the Wenger's Summation with a positive sign! This might be a first step in proving its boundlessness. Also, inspired by the same comment, I decided to add the graph for the Quartenary Wenger's Summation , which twists our expectations by trending downwards (until it doesn't anymore)!","The user Avery Wenger said he came up with this problem randomly and I think it's very interesting! Let be the sum of digits function. Then, Wenger defines given by Now comes the really interesting part: the Wenger's Summation . What is so interesting about it? Its amazingly weird graph! As you can see, it behaves very poorly at first, looking kinda bumpy (jumps seem to occur on multiples of powers of ten). Soon, however, the graph starts to look very much like a line! Is this really Wenger's Summation asymptotic behavior? Is there a real constant (the Wenger's Constant) such that If not, then what is Wenger's Summation asymptotic behavior? Can we at least show to be unbounded? But of course, this is only the decimal Wenger's Summation , since we are taking number in decimal notation! What is so special about , anyway? What about the binary Wenger's Summation ? Does it behave just as oddly? It actually does worse! So, for every possible basis we have a different Wenger's Summation and a different Wenger's Constant . Ain't that wonderful? Is it true that, if , then ? The alternating signals makes proving all this stuff so awkward. I'm just amazed about this very innocent looking procedure generating such a beautiful cacophony. It all looks so useless and disconnected to... anything else. Do mathematics even has the tools to answer these questions? If you can find anything interesting about Wenger's Summations, please let me know it! Attention: Be aware Avery Wenger did not call all these mathematical objects by his own name in the original post. I'm the only one responsible for that (Avery, if you are seeing this, thank you for sharing the formidable problem that randomly came to you). Values for the Wenger's Summation were calculated through this rudimentary python script. import matplotlib.pyplot as plt import numpy as np  b = 4 # basis  def w(n):     ap = 0 # additive persistence     while n>=b:         aux = n         sum = 0         while aux>0:             sum+=(aux%b)             aux//=b         n = sum         ap+=1     return ap  S = 0  # Wenger's summation W = [] for n in range(100000000):     S+=w(n)*(-1)**(n+1)     W.append(S) Update: A comment pointed out the value is called the additive persistence of . The sequence (OEIS A031286 for base ) has some interesting properties. For example, every positive integer appears in it infinitely many times. Furthermore, the first occurrence of in it (OEIS A006050 for base ) is given by the recurrence formula , so its always odd , i.e., appears for the first time in the Wenger's Summation with a positive sign! This might be a first step in proving its boundlessness. Also, inspired by the same comment, I decided to add the graph for the Quartenary Wenger's Summation , which twists our expectations by trending downwards (until it doesn't anymore)!",s:\mathbb N\to\mathbb N w:\mathbb N\to\mathbb N w(n):=\min\{k\in\mathbb N: s^k(n)\;\mbox{has a single digit}\}\;\forall n\in\mathbb N W_m:=\sum_{n=1}^m (-1)^{n+1}w(n) \omega \lim_{m\to\infty} \frac{W_m}{m} = \omega\;? (W_m)_m 10 b (W_m^b)_m \omega^b a<b \omega^a>\omega^b w(n) n (w(n))_n 10 N 10 a(N)=2\cdot b^{\frac{a(N-1)-1}{b-1}}-1 N,"['real-analysis', 'sequences-and-series', 'number-theory', 'asymptotics', 'recreational-mathematics']"
31,What projections of convex bodies can tell us about their volumes,What projections of convex bodies can tell us about their volumes,,"Let $K,L$ be two origin-symmetric, convex bodies in $\mathbb{R}^3$ . Let $\xi\in\mathbb{S}^2$ be a vector on the unit sphere in $\mathbb{R}^3$ . Denote $\xi^{\perp}$ the subspace in $\mathbb{R}^3$ orthogonal to $\xi$ . Denote $K\mid_{\xi^{\perp}}$ as the projection of the body $K$ onto the subspace $\xi^{\perp}$ . Same for $L\mid_{\xi^{\perp}}$ . Suppose that for any vector $\xi$ , there exists a rotation $\phi_{\xi}$ of $K\mid_{\xi^{\perp}}$ such that $\phi_{\xi}(K\mid_{\xi^{\perp}})\subset L\mid_{\xi^{\perp}}$ . This rotation is taken in the $2$ -dimensional sense on the plane $\xi^{\perp}$ . Does it follow that the volume of $K$ is less than or equal to the volume of $L$ ?","Let be two origin-symmetric, convex bodies in . Let be a vector on the unit sphere in . Denote the subspace in orthogonal to . Denote as the projection of the body onto the subspace . Same for . Suppose that for any vector , there exists a rotation of such that . This rotation is taken in the -dimensional sense on the plane . Does it follow that the volume of is less than or equal to the volume of ?","K,L \mathbb{R}^3 \xi\in\mathbb{S}^2 \mathbb{R}^3 \xi^{\perp} \mathbb{R}^3 \xi K\mid_{\xi^{\perp}} K \xi^{\perp} L\mid_{\xi^{\perp}} \xi \phi_{\xi} K\mid_{\xi^{\perp}} \phi_{\xi}(K\mid_{\xi^{\perp}})\subset L\mid_{\xi^{\perp}} 2 \xi^{\perp} K L","['real-analysis', 'geometry', 'convex-analysis']"
32,Extremely rigorous (research level) treatment of the laplace transform,Extremely rigorous (research level) treatment of the laplace transform,,"Edit: I have found what I needed in Schwartz's Mathematics for the Physical Sciences . Will type up a reply when I have time. Bourbaki does not explain the justifications behind operational calculus The quantum book below does not justify the LT of the dirac distribution, nor the unbounded exponentials. Schwartz's original: Theorie des distributions has partial explanations. I am looking for an extremely rigorous treatment of the laplace transform . Stating the usual formulas for convenience: $$ \mathcal{L}: ?\to?\quad\mathcal{L}(f)(s) = \int_{\mathbb{R}^n}f(x)\operatorname{exp}(-\langle s,x\rangle)dx $$ Or as a map on the space of tempered distributions (is it even well defined?), $$ \mathcal{L}: ? \to ?\quad\langle \mathcal{L}(F), \phi\rangle_{(\mathcal{S}', \mathcal{S})} = \langle F, \mathcal{L}(\phi)\rangle_{(\mathcal{S}', \mathcal{S})} $$ Starting from a measure-theoretic, functional-analytic standpoint, I would like to know the following: Can we restrict the LT to some kind of isomorphism? What are the convergence/continuity properties? If its domain/codomain differs from that of the Fourier Transform (which is a toplinear isomorphism on the space of tempered distributions) - what topology are we using? If the LT is defined on all tempered distributions, can we extend the domain from $\mathcal{S}'$ to a bigger space? Is the LT injective/surjective? Is there a duality like in the Fourier Transform, regularity as a distribution gets converted to integrability? With regards to solving differential equations, how is the recovery of a regular solution possible? Is viewing the LT from a purely algebraic perspective , the way researchers today understand the LT? The following is a common scenario in the applied sciences that I would want the proof (in the positive or negative) of. Let $X$ and $Y$ be 'function spaces in a real variable' (left imprecise), and $H: X\to Y$ be a linear map. If $H$ is shift-invariant , meaning $$ H(\tau_a f) = \tau_a H(f)\quad \tau_af(x) = f(x-a)\quad\forall a,x\in\mathbb{R} $$ then we can apply the Laplace Transform. Somehow in this definition we have left out the usual a.e identifications/regularity assumptions that were present if the domain of $H$ were to include the point mass at the origin $\delta_0$ : a tempered distribution defined by the duality pairing: $$ \langle \delta_0, f\rangle_{(\mathcal{S}', \mathcal{S})} = f(0) $$ for every $f\in \mathcal{S}$ in the Schwartz space. It is often claimed (but not proven), that every linear, shift invariant 'operator' $H$ admits a rational 'transfer function' in the 'Frequency domain' $$ \mathcal{L}(h)\mathcal{L}(f) = \mathcal{L}(H(f)) $$ Setting where $h = H(\delta_0)$ is commonly called the 'impulse response' of the 'system' $H$ . It is also often claimed that the LT does not work for functions that are not of 'exponential order', $$ E_{order} = \biggl\{f: \mathbb{R}\to\mathbb{R},\: \exists C,k\in\mathbb{R},\: \vert f(x)\vert\leq C\operatorname{exp}(kx)\: \forall x\in\mathbb{R} \biggr\} $$ but somehow is compatible with extreme irregularity like $\delta_0$ and linear combinations of its derivatives. Smooth functions like $f(x) = \operatorname{exp}(\vert x\vert^2)$ are locally integrable, thus defines a distribution. It is well known the space of test functions (we refer to $C_c^\infty$ as test functions , and $C^\infty$ as smooth functions ), is dense in $\mathcal{S}'$ . It is bizarre to rule out the functions that are above exponential order, if we were to take the 'distributional' approach to the LT. Moreover, the domain of the LT has to extend beyond the Schwartz functions, because we allow for the 'causal' (or even 'eternal' exponentials, so $g(x)=e^{kx}$ ) exponentials $f(x)=e^{kx}$ for $x\geq 0$ and $f(x)=0$ for $x<0$ , where $k\in\mathbb{R}$ to be 'transformed' into reciprocals $$ \mathcal{L}(f)(s) = \dfrac{1}{s-k}\forall \operatorname{Re}(s)>k $$ In sum: The LT clearly converges for every test function, The point mass $\delta_0$ is in the domain of the LT, The domain of the LT has to extend beyond the Schwartz functions, No additonal regularity is imposed, because we also allow for the unit step $u(x)=0$ for $x<0$ and $u(x)=1$ for $x\geq 0$ to be LT-able but we somehow exclude a subset of $L_{loc}^1$ from the domain? Next, to my knowledge there is no agreed upon definition for the mathematical object $\mathcal{L}(f)(s)$ in the last equation. Should we extend $\mathcal{L}(f)$ for $\operatorname{Re}(s)<k$ by zero? Then we can identify $\mathcal{L}(f)$ with a distribution. I have looked at the following posts but the answers (and the texts provided) are not satisfying. Looking for a rigorous treatment about Laplace transform. Why does the mathematics of Laplace transform and fourier transform look so dodgy and non rigouruos and not well formulated? Compare Fourier and Laplace transform Relation Fourier/Laplace Transform Is the Laplace transform essentially a generalized version of the Fourier transform? (seems promising but the link is broken, edit: found the link , but is just a magic transform table derived using integration by parts) along with countless (with respect) pseudo-explanations such as the LT is 'just like' the FT, (no proof, no concrete definitions) the LT converts differentiation, integration into pointwise multiplication (ok, but what function space? a.e class? duality pairing identification? what about $\delta_0$ ? weak derivatives?, we cannot integrate distributions and how about inversion?) the fourier transform does the same. Some more oddities: function which is of exponential order, whose LT does not converge function which is above exponential order, whose LT converges On a related note, what is the unilateral/one-sided/causal Laplace Transform? What is the rigour behind it? Thank you for reading this long post.","Edit: I have found what I needed in Schwartz's Mathematics for the Physical Sciences . Will type up a reply when I have time. Bourbaki does not explain the justifications behind operational calculus The quantum book below does not justify the LT of the dirac distribution, nor the unbounded exponentials. Schwartz's original: Theorie des distributions has partial explanations. I am looking for an extremely rigorous treatment of the laplace transform . Stating the usual formulas for convenience: Or as a map on the space of tempered distributions (is it even well defined?), Starting from a measure-theoretic, functional-analytic standpoint, I would like to know the following: Can we restrict the LT to some kind of isomorphism? What are the convergence/continuity properties? If its domain/codomain differs from that of the Fourier Transform (which is a toplinear isomorphism on the space of tempered distributions) - what topology are we using? If the LT is defined on all tempered distributions, can we extend the domain from to a bigger space? Is the LT injective/surjective? Is there a duality like in the Fourier Transform, regularity as a distribution gets converted to integrability? With regards to solving differential equations, how is the recovery of a regular solution possible? Is viewing the LT from a purely algebraic perspective , the way researchers today understand the LT? The following is a common scenario in the applied sciences that I would want the proof (in the positive or negative) of. Let and be 'function spaces in a real variable' (left imprecise), and be a linear map. If is shift-invariant , meaning then we can apply the Laplace Transform. Somehow in this definition we have left out the usual a.e identifications/regularity assumptions that were present if the domain of were to include the point mass at the origin : a tempered distribution defined by the duality pairing: for every in the Schwartz space. It is often claimed (but not proven), that every linear, shift invariant 'operator' admits a rational 'transfer function' in the 'Frequency domain' Setting where is commonly called the 'impulse response' of the 'system' . It is also often claimed that the LT does not work for functions that are not of 'exponential order', but somehow is compatible with extreme irregularity like and linear combinations of its derivatives. Smooth functions like are locally integrable, thus defines a distribution. It is well known the space of test functions (we refer to as test functions , and as smooth functions ), is dense in . It is bizarre to rule out the functions that are above exponential order, if we were to take the 'distributional' approach to the LT. Moreover, the domain of the LT has to extend beyond the Schwartz functions, because we allow for the 'causal' (or even 'eternal' exponentials, so ) exponentials for and for , where to be 'transformed' into reciprocals In sum: The LT clearly converges for every test function, The point mass is in the domain of the LT, The domain of the LT has to extend beyond the Schwartz functions, No additonal regularity is imposed, because we also allow for the unit step for and for to be LT-able but we somehow exclude a subset of from the domain? Next, to my knowledge there is no agreed upon definition for the mathematical object in the last equation. Should we extend for by zero? Then we can identify with a distribution. I have looked at the following posts but the answers (and the texts provided) are not satisfying. Looking for a rigorous treatment about Laplace transform. Why does the mathematics of Laplace transform and fourier transform look so dodgy and non rigouruos and not well formulated? Compare Fourier and Laplace transform Relation Fourier/Laplace Transform Is the Laplace transform essentially a generalized version of the Fourier transform? (seems promising but the link is broken, edit: found the link , but is just a magic transform table derived using integration by parts) along with countless (with respect) pseudo-explanations such as the LT is 'just like' the FT, (no proof, no concrete definitions) the LT converts differentiation, integration into pointwise multiplication (ok, but what function space? a.e class? duality pairing identification? what about ? weak derivatives?, we cannot integrate distributions and how about inversion?) the fourier transform does the same. Some more oddities: function which is of exponential order, whose LT does not converge function which is above exponential order, whose LT converges On a related note, what is the unilateral/one-sided/causal Laplace Transform? What is the rigour behind it? Thank you for reading this long post.","
\mathcal{L}: ?\to?\quad\mathcal{L}(f)(s) = \int_{\mathbb{R}^n}f(x)\operatorname{exp}(-\langle s,x\rangle)dx
 
\mathcal{L}: ? \to ?\quad\langle \mathcal{L}(F), \phi\rangle_{(\mathcal{S}', \mathcal{S})} = \langle F, \mathcal{L}(\phi)\rangle_{(\mathcal{S}', \mathcal{S})}
 \mathcal{S}' X Y H: X\to Y H 
H(\tau_a f) = \tau_a H(f)\quad \tau_af(x) = f(x-a)\quad\forall a,x\in\mathbb{R}
 H \delta_0 
\langle \delta_0, f\rangle_{(\mathcal{S}', \mathcal{S})} = f(0)
 f\in \mathcal{S} H 
\mathcal{L}(h)\mathcal{L}(f) = \mathcal{L}(H(f))
 h = H(\delta_0) H 
E_{order} = \biggl\{f: \mathbb{R}\to\mathbb{R},\: \exists C,k\in\mathbb{R},\: \vert f(x)\vert\leq C\operatorname{exp}(kx)\: \forall x\in\mathbb{R} \biggr\}
 \delta_0 f(x) = \operatorname{exp}(\vert x\vert^2) C_c^\infty C^\infty \mathcal{S}' g(x)=e^{kx} f(x)=e^{kx} x\geq 0 f(x)=0 x<0 k\in\mathbb{R} 
\mathcal{L}(f)(s) = \dfrac{1}{s-k}\forall \operatorname{Re}(s)>k
 \delta_0 u(x)=0 x<0 u(x)=1 x\geq 0 L_{loc}^1 \mathcal{L}(f)(s) \mathcal{L}(f) \operatorname{Re}(s)<k \mathcal{L}(f) \delta_0","['real-analysis', 'functional-analysis', 'reference-request', 'fourier-analysis', 'laplace-transform']"
33,A (possible) generic spectral property in one dimensional dynamics,A (possible) generic spectral property in one dimensional dynamics,,"This question was previously posted on MathOverflow . Context and Definitions Consider the interval $I=[0,1]$ . We say that $T:I\to I$ satisfies the axiom A (I am following [MvS]) if: $T$ has a finite number of hyperbolic periodic attractors; and defining $B(T)$ as the union of the basins of the hyperbolic attractors of $T$ , the set $\Lambda = I \setminus B(T)$ is a hyperbolic set, i.e. there exists $C >0$ and $\lambda>1$ such that $$ |(T^n)'(x)| > C  \lambda^n\ \text{for every }x\in \Lambda.$$ It is well known that the set $\mathcal A = \{T: I\to I\in\mathcal C^1; \ T\ \text{satisfies the axiom A}\}$ is open and dense in the $\mathcal C^1$ -topology (see [Chapter IV,1]). Defining $R = \overline{\{p\in\Lambda;\ p\ \text{is a periodic orbit of }T\}}$ we know from the dynamical decomposition [Theorem 11.2.15, OV] that there exists a (finite) partition of $R$ in non-empty compact sets $R^{1},R^2,\ldots, R^k$ such that: $R^{i}$ is a $T$ -invariant set for every $i$ ; $T:R^i\to R^i$ is uniformly hyperbolic and topologically transitive. Also, from the thermodynamic formalism for expanding maps, we obtain that the operators \begin{align*}\mathcal L_i : \mathcal C^0(R^i) &\to \mathcal C^0(R^i)\\ f&\mapsto \left(x\mapsto\sum_{T(y)=x}\frac{f(y)}{|T'(y)|}\right), \end{align*} satisfy $\mathrm{dim}\,\mathrm{ker}(\mathcal L_i - r(\mathcal L_i)) = 1,$ where $r(\mathcal L_i)$ is the spectral radius of $\mathcal L_i$ . My Question: Let us now consider the operator \begin{align*}\mathcal L : \mathcal C^0(R) &\to \mathcal C^0(R)\\ f&\mapsto \left(x\mapsto\sum_{T(y)=x}\frac{f(y)}{|T'(y)|}\right), \end{align*} we have that $\mathcal L(f) = \sum_{i=1}^k \mathcal L_i (\mathbf 1_{R^i} f)$ and therefore $r(\mathcal L) = \max\{r(\mathcal L_1),\dots, r(\mathcal L_k)\}.$ It seems to me that (generically) we have that $\mathrm{dim}\, \mathrm{ker}(\mathcal L - r(\mathcal L)) = 1.$ Equivalently, the set $\{r(\mathcal L_1), r(\mathcal L_2), \ldots, r(\mathcal L_k))\}\subset \mathbb R$ (generically) has a unique maximal element. In this way, I would like to prove the following theorem: Possible Theorem: The set $$\mathcal B = \{T:I\to I \in \mathcal C^1;\ T \text{ satisfies the axiom A} \ \text{and }\mathrm{dim}\,\mathrm{ker}(\mathcal L - r(\mathcal L)) =1 \},$$ is open and dense in open and dense (or at least residual) in the $\mathcal C^1$ topology. I feel that this result is known in the literature however I am neither able to find nor prove it. Can anyone please help me? [MvS] One-dimensional Dynamics - W. de Melo and S. van Strien ( https://link.springer.com/book/10.1007/978-3-642-78043-1 ) [OV] Foundations of ergodic theory - M. Viana and K. Oliveira ( https://www.cambridge.org/core/books/foundations-of-ergodic-theory/F5AE11B50B16D9FF32909EA4BBA1E7CE )","This question was previously posted on MathOverflow . Context and Definitions Consider the interval . We say that satisfies the axiom A (I am following [MvS]) if: has a finite number of hyperbolic periodic attractors; and defining as the union of the basins of the hyperbolic attractors of , the set is a hyperbolic set, i.e. there exists and such that It is well known that the set is open and dense in the -topology (see [Chapter IV,1]). Defining we know from the dynamical decomposition [Theorem 11.2.15, OV] that there exists a (finite) partition of in non-empty compact sets such that: is a -invariant set for every ; is uniformly hyperbolic and topologically transitive. Also, from the thermodynamic formalism for expanding maps, we obtain that the operators satisfy where is the spectral radius of . My Question: Let us now consider the operator we have that and therefore It seems to me that (generically) we have that Equivalently, the set (generically) has a unique maximal element. In this way, I would like to prove the following theorem: Possible Theorem: The set is open and dense in open and dense (or at least residual) in the topology. I feel that this result is known in the literature however I am neither able to find nor prove it. Can anyone please help me? [MvS] One-dimensional Dynamics - W. de Melo and S. van Strien ( https://link.springer.com/book/10.1007/978-3-642-78043-1 ) [OV] Foundations of ergodic theory - M. Viana and K. Oliveira ( https://www.cambridge.org/core/books/foundations-of-ergodic-theory/F5AE11B50B16D9FF32909EA4BBA1E7CE )","I=[0,1] T:I\to I T B(T) T \Lambda = I \setminus B(T) C >0 \lambda>1  |(T^n)'(x)| > C  \lambda^n\ \text{for every }x\in \Lambda. \mathcal A = \{T: I\to I\in\mathcal C^1; \ T\ \text{satisfies the axiom A}\} \mathcal C^1 R = \overline{\{p\in\Lambda;\ p\ \text{is a periodic orbit of }T\}} R R^{1},R^2,\ldots, R^k R^{i} T i T:R^i\to R^i \begin{align*}\mathcal L_i : \mathcal C^0(R^i) &\to \mathcal C^0(R^i)\\
f&\mapsto \left(x\mapsto\sum_{T(y)=x}\frac{f(y)}{|T'(y)|}\right),
\end{align*} \mathrm{dim}\,\mathrm{ker}(\mathcal L_i - r(\mathcal L_i)) = 1, r(\mathcal L_i) \mathcal L_i \begin{align*}\mathcal L : \mathcal C^0(R) &\to \mathcal C^0(R)\\
f&\mapsto \left(x\mapsto\sum_{T(y)=x}\frac{f(y)}{|T'(y)|}\right),
\end{align*} \mathcal L(f) = \sum_{i=1}^k \mathcal L_i (\mathbf 1_{R^i} f) r(\mathcal L) = \max\{r(\mathcal L_1),\dots, r(\mathcal L_k)\}. \mathrm{dim}\, \mathrm{ker}(\mathcal L - r(\mathcal L)) = 1. \{r(\mathcal L_1), r(\mathcal L_2), \ldots, r(\mathcal L_k))\}\subset \mathbb R \mathcal B = \{T:I\to I \in \mathcal C^1;\ T \text{ satisfies the axiom A} \ \text{and }\mathrm{dim}\,\mathrm{ker}(\mathcal L - r(\mathcal L)) =1 \}, \mathcal C^1","['real-analysis', 'functional-analysis', 'dynamical-systems', 'ergodic-theory']"
34,Understanding Lang's Proof of Fubini's Theorem,Understanding Lang's Proof of Fubini's Theorem,,"This question concerns the proof of Theorem 8.4 (Fubini's Theorem part 1) on page 162 in Lang's real and functional analysis book. To understand the proof I need to give following background from the book: Let $f:X\to E$ be a function defined on a measure space $(X,\mathcal{M},\mu)$ and taking values in a Banach space $E$ . First we consider step maps of the form $f=\sum_{k=1}^n v_k1_{A_k}$ with $v_k\in E$ , $A_k\in \mathcal{M}$ and $\mu(A_k)<\infty$ , and define the integral of such maps to be $\int_X f d\mu := \sum_{k=1}^n v_k\mu(A_k)\in E$ (we check that this definition does not depend on the step map representation of $f$ ). The vector space of step maps is denoted by $S(\mu,E)$ and we check that the integral is linear on this space. $S(\mu,E)$ is made into a seminormed  space by introducing the $L^1$ -seminorm $\|f\|_1:=\int_X |f|d\mu$ .  Next we define the space $\mathcal{L}(\mu,E)$ of maps $f:X\to E$ such that there exist a sequence $(f_n)\subset S(\mu,E)$ such that $(f_n)$ is $L^1$ -Cauchy $f_n\to f$ for almost every $x\in X$ and we define the integral of such $f$ to be $\int_X fd\mu:=\lim_{n\to \infty}\int_X f_n d\mu$ (we check that this limit exists and is independent of the chosen sequence $(f_n)$ ). The integral is shown to be a linear map on the vector space $\mathcal{L}(\mu,E)$ . $\mathcal{L}(\mu,E)$ is made into a seminormed space by extending the $L^1$ -seminorm $\|f\|_1:=\int_X |f|d\mu$ for $f\in \mathcal{L}(\mu,E)$ . Ok now I can give the statement and proof of Fubini's Theorem on page 162. The set-up is two $\sigma$ -finite measure spaces $(X,\mathcal{M},\mu)$ and $(Y,\mathcal{N},\nu)$ with product space $(X\times Y,\mathcal{M}\otimes\mathcal{N},\mu\otimes\nu)$ . Theorem 8.4 (Fubini's Theorem, part 1). Let $f\in \mathcal{L}(\mu\otimes\nu,E)$ . Then the map $f_x$ ( $x$ section of $f$ ) is in $\mathcal{L}(\nu,E)$ for almost all $x\in X$ , the map given by $x\mapsto \int_Y f_x d\nu$ for almost all $x$ (and defined arbitrarily for other $x$ ) is in $\mathcal{L}(\mu,E)$ , and we have $\int_{X\times Y} f d\mu\otimes\nu=\int_X \int_Y f_x d\nu d\mu$ . Proof (with added details of mine). Let $\mathcal{A}$ , $\mathcal{B}$ denote the rings of sets of finite measure in $\mathcal{M},\mathcal{N}$ respectively. Then $\mathcal{M}\otimes\mathcal{N}=\sigma(\mathcal{A}\times \mathcal{B})$ where $\mathcal{A}\times \mathcal{B}$ is the ring of finite disjoint unions of rectangles $A\times B$ with $A\in\mathcal{A}$ and $B\in\mathcal{B}$ . Moreover $X\times Y$ is $\sigma$ -finite with repect to $\mathcal{A}\times \mathcal{B}$ . By Theorem 6.3 on page 150 of the book, it follows that the space $S(\mathcal{A}\times \mathcal{B},E)$ is $L^1$ -dense in $\mathcal{L}(\mu\otimes\nu,E)$ . Hence given $f\in \mathcal{L}(\mu\otimes\nu,E)$ there exist a sequence $(f_n)\subset S(\mathcal{A}\times \mathcal{B},E)$ such that $(f_n)$ is $L^1$ -convergent to $f$ , and is in particular $L^1$ -Cauchy. Using Theorem 5.2 on page 138 we may also assume that $f_n\to f$ almost everywhere (by considering a subsequence). Let $Z\in \mathcal{M}\otimes\mathcal{N}$ be a $\mu\otimes\nu$ -null set outside of which $f_n\to f$ holds pointwise. Then from Lemma 8.3 on page 161 we have that $\nu(Z_x)=0$ for almost all $x$ , where $Z_x$ denotes the $x$ section of $Z$ . Let $S\in\mathcal{M}$ be a $\mu$ -null set outside of which $\nu(Z_x)=0$ . Note that we have $(f_n)_x\to f_x$ $\nu$ -almost everywhere for all $x\in X\setminus S$ , where $f_x$ denotes the $x$ section of $f$ . Now, for each $n$ consider the map $\Phi_n:X\to S(\mathcal{B},E)\subset \mathcal{L}(\nu,E)$ defined by $\Phi_n(x)=(f_n)_x$ . We note that $\Phi_n \in S(X,\mathcal{L}(\nu,E))$ , i.e. $\Phi_n$ is a step map taking values in the seminormed space $\mathcal{L}(\nu,E)$ . Indeed if $f=\sum_{k=1}^n v_k1_{A_k\times B_k}$ is a step map in $S(\mathcal{A}\times \mathcal{B},E)$ then $$f_x=\sum_{k=1}^n v_k 1_{B_k} 1_{A_k}(x) \hspace{0.5cm} \text{with } \hspace{0.5cm}   v_k 1_{B_k}\in S(\mathcal{B},E) $$ We note also that $(\Phi_n)$ is $L^1$ -Cauchy since $$\|\Phi_n-\Phi_m\|_1=\int_X |\Phi_n-\Phi_m| d\mu=\int_X\int_Y|(f_n)_x-(f_m)_x|d\nu d\mu=\int_{X\times Y} |f_n-f_m|d(\mu\otimes\nu) = \|f_n-f_m\|_1\to 0$$ as $n,m\to\infty$ , where the third equality is because $|f_n-f_m|\in S(\mathcal{A}\times \mathcal{B},\mathbb{R})$ and Fubini's Theorem is easily verified to hold in this case. By the ""fundamental lemma 3.1"" on page 129 we may assume that $\Phi_n$ converges for almost every $x$ (using a subsequence if necessary). Let $T\in\mathcal{M}$ be a $\mu$ -null set outside of which $(\Phi_n)$ converges. Then in particular we have that $x\in X\setminus T$ implies $\Phi_n(x)$ Cauchy, that is $(f_n)_x$ is $L^1$ -Cauchy with respect to $\nu$ . Fundamental lemma 3.1 .(page 129) Let $(f_n)\subset S(\mu,E)$ by an $L^1$ -Cauchy sequence of step maps. Then there exist a subsequence converging almost everywhere. Question 1. Here we apply the fundamental lemma to a sequence of step maps $(\Phi_n)$ taking values in a seminormed space (rather than a normed space). Is the fundamental lemma still valid in this case? If $x\in X\setminus (S\cup T)$ , then combining both previous parts we get that $(f_n)_x\to f_x$ $\nu$ -almost everywhere with $(f_n)_x$ $L^1$ -Cauchy. Lang's then write: ""By Corollary 5.10 we conclude that $f_x \in \mathcal{L}(\nu,E)$ and that $(f_n)_x$ is $L^1$ convergent to $f_x$ for all $x\in X \setminus (S\cup T)$ , so that $\int_Y(f_n)_xd\nu$ converges to $\int_Y f_x d\nu$ "". Corollary 5.10. (page 142) Let $(f_n)\subset  \mathcal{L}(\mu,E)$ with $f_n\to f$ almost everywhere for some map $f:X\to E$ . If there exist $C\geq0$ such that $\|f_n\|_1\leq C$ for all $n$ , then $f\in \mathcal{L}(\mu,E)$ and $\|f\|_1\leq C$ . Question 2. I don't understand why this corollary is needed. To me it seems that $f_x \in \mathcal{L}(\nu,E)$ for all $x\in X \setminus (S\cup T)$ by definition of the integral, since for such $x$ we have $(f_n)_x \subset S(\mathcal{B},E)$ $L^1$ -Cauchy and $(f_n)_x\to f_x$ $\nu$ -almost everywhere. Hence by definition $\int_Y f_x d\nu=\lim_{n\to\infty}\int_Y (f_n)_x d\nu$ for all $x\in X \setminus (S\cup T)$ . Now note that the map $\Psi_n:X\to E$ defined by $x\mapsto \int_Y(f_n)_x$ is a step map in $S(\mathcal{A},E)$ , since it is the composition of $\Phi_n$ and the integral $\int_Y d\nu$ . Lang's writes: ""One sees by repeating the argument above that $(\Psi_n)$ is $L^1$ -Cauchy"" Question 3. Is the following calculation showing that $(\Psi_n)$ is $L^1$ -Cauchy correct? $$\|\Psi_n-\Psi_m\|_1=\int_X |\Psi_n-\Psi_m| d\mu=\int_X\bigg|\int_Y(f_n)_x-(f_m)_xd\nu\bigg| d\mu\leq \int_X\int_Y |(f_n)_x-(f_m)_x| d\nu d\mu$$ $$= \int_{X\times Y} |f_n-f_m|d(\mu\otimes\nu) = \|f_n-f_m\|_1\to 0 \text{ as } n,m\to\infty$$ where I used the inequality $|\int_Xfd\mu|\leq \|f\|_1$ valid for $f\in\mathcal{L}(\mu,E)$ . Hence if we define $\Psi(x)$ to be $\int_Y f_x d\nu$ for $x\in X\setminus(S\cup T)$ and arbitrarily elsewhere we get $\Psi_n\to\Psi$ for almost every $x$ with $(\Psi_n)$ $L^1$ _Cauchy. It follows (by definition of the integral) that $\Psi \in \mathcal{L}(\mu,E)$ and $\int_X \Psi d\mu=\lim_{n\to\infty}\int_X \Psi_n d\mu$ , that is $$\int_X \int_Y f_x d\nu d\mu=\lim_{n\to\infty}\int_X \int_Y (f_n)_x d\nu d\mu=\lim_{n\to\infty}\int_{X\times Y} f_n d(\mu\otimes \nu)=\int_{X\times Y} f d(\mu\otimes \nu)$$ where the last equality is because $(f_n)$ was initially chosen to be $L^1$ -convergent to $f$ . Is anybody familiar with this Fubini's proof in Lang's book? Any help on this is very appreciated.","This question concerns the proof of Theorem 8.4 (Fubini's Theorem part 1) on page 162 in Lang's real and functional analysis book. To understand the proof I need to give following background from the book: Let be a function defined on a measure space and taking values in a Banach space . First we consider step maps of the form with , and , and define the integral of such maps to be (we check that this definition does not depend on the step map representation of ). The vector space of step maps is denoted by and we check that the integral is linear on this space. is made into a seminormed  space by introducing the -seminorm .  Next we define the space of maps such that there exist a sequence such that is -Cauchy for almost every and we define the integral of such to be (we check that this limit exists and is independent of the chosen sequence ). The integral is shown to be a linear map on the vector space . is made into a seminormed space by extending the -seminorm for . Ok now I can give the statement and proof of Fubini's Theorem on page 162. The set-up is two -finite measure spaces and with product space . Theorem 8.4 (Fubini's Theorem, part 1). Let . Then the map ( section of ) is in for almost all , the map given by for almost all (and defined arbitrarily for other ) is in , and we have . Proof (with added details of mine). Let , denote the rings of sets of finite measure in respectively. Then where is the ring of finite disjoint unions of rectangles with and . Moreover is -finite with repect to . By Theorem 6.3 on page 150 of the book, it follows that the space is -dense in . Hence given there exist a sequence such that is -convergent to , and is in particular -Cauchy. Using Theorem 5.2 on page 138 we may also assume that almost everywhere (by considering a subsequence). Let be a -null set outside of which holds pointwise. Then from Lemma 8.3 on page 161 we have that for almost all , where denotes the section of . Let be a -null set outside of which . Note that we have -almost everywhere for all , where denotes the section of . Now, for each consider the map defined by . We note that , i.e. is a step map taking values in the seminormed space . Indeed if is a step map in then We note also that is -Cauchy since as , where the third equality is because and Fubini's Theorem is easily verified to hold in this case. By the ""fundamental lemma 3.1"" on page 129 we may assume that converges for almost every (using a subsequence if necessary). Let be a -null set outside of which converges. Then in particular we have that implies Cauchy, that is is -Cauchy with respect to . Fundamental lemma 3.1 .(page 129) Let by an -Cauchy sequence of step maps. Then there exist a subsequence converging almost everywhere. Question 1. Here we apply the fundamental lemma to a sequence of step maps taking values in a seminormed space (rather than a normed space). Is the fundamental lemma still valid in this case? If , then combining both previous parts we get that -almost everywhere with -Cauchy. Lang's then write: ""By Corollary 5.10 we conclude that and that is convergent to for all , so that converges to "". Corollary 5.10. (page 142) Let with almost everywhere for some map . If there exist such that for all , then and . Question 2. I don't understand why this corollary is needed. To me it seems that for all by definition of the integral, since for such we have -Cauchy and -almost everywhere. Hence by definition for all . Now note that the map defined by is a step map in , since it is the composition of and the integral . Lang's writes: ""One sees by repeating the argument above that is -Cauchy"" Question 3. Is the following calculation showing that is -Cauchy correct? where I used the inequality valid for . Hence if we define to be for and arbitrarily elsewhere we get for almost every with _Cauchy. It follows (by definition of the integral) that and , that is where the last equality is because was initially chosen to be -convergent to . Is anybody familiar with this Fubini's proof in Lang's book? Any help on this is very appreciated.","f:X\to E (X,\mathcal{M},\mu) E f=\sum_{k=1}^n v_k1_{A_k} v_k\in E A_k\in \mathcal{M} \mu(A_k)<\infty \int_X f d\mu := \sum_{k=1}^n v_k\mu(A_k)\in E f S(\mu,E) S(\mu,E) L^1 \|f\|_1:=\int_X |f|d\mu \mathcal{L}(\mu,E) f:X\to E (f_n)\subset S(\mu,E) (f_n) L^1 f_n\to f x\in X f \int_X fd\mu:=\lim_{n\to \infty}\int_X f_n d\mu (f_n) \mathcal{L}(\mu,E) \mathcal{L}(\mu,E) L^1 \|f\|_1:=\int_X |f|d\mu f\in \mathcal{L}(\mu,E) \sigma (X,\mathcal{M},\mu) (Y,\mathcal{N},\nu) (X\times Y,\mathcal{M}\otimes\mathcal{N},\mu\otimes\nu) f\in \mathcal{L}(\mu\otimes\nu,E) f_x x f \mathcal{L}(\nu,E) x\in X x\mapsto \int_Y f_x d\nu x x \mathcal{L}(\mu,E) \int_{X\times Y} f d\mu\otimes\nu=\int_X \int_Y f_x d\nu d\mu \mathcal{A} \mathcal{B} \mathcal{M},\mathcal{N} \mathcal{M}\otimes\mathcal{N}=\sigma(\mathcal{A}\times \mathcal{B}) \mathcal{A}\times \mathcal{B} A\times B A\in\mathcal{A} B\in\mathcal{B} X\times Y \sigma \mathcal{A}\times \mathcal{B} S(\mathcal{A}\times \mathcal{B},E) L^1 \mathcal{L}(\mu\otimes\nu,E) f\in \mathcal{L}(\mu\otimes\nu,E) (f_n)\subset S(\mathcal{A}\times \mathcal{B},E) (f_n) L^1 f L^1 f_n\to f Z\in \mathcal{M}\otimes\mathcal{N} \mu\otimes\nu f_n\to f \nu(Z_x)=0 x Z_x x Z S\in\mathcal{M} \mu \nu(Z_x)=0 (f_n)_x\to f_x \nu x\in X\setminus S f_x x f n \Phi_n:X\to S(\mathcal{B},E)\subset \mathcal{L}(\nu,E) \Phi_n(x)=(f_n)_x \Phi_n \in S(X,\mathcal{L}(\nu,E)) \Phi_n \mathcal{L}(\nu,E) f=\sum_{k=1}^n v_k1_{A_k\times B_k} S(\mathcal{A}\times \mathcal{B},E) f_x=\sum_{k=1}^n v_k 1_{B_k} 1_{A_k}(x) \hspace{0.5cm} \text{with } \hspace{0.5cm}   v_k 1_{B_k}\in S(\mathcal{B},E)  (\Phi_n) L^1 \|\Phi_n-\Phi_m\|_1=\int_X |\Phi_n-\Phi_m| d\mu=\int_X\int_Y|(f_n)_x-(f_m)_x|d\nu d\mu=\int_{X\times Y} |f_n-f_m|d(\mu\otimes\nu) = \|f_n-f_m\|_1\to 0 n,m\to\infty |f_n-f_m|\in S(\mathcal{A}\times \mathcal{B},\mathbb{R}) \Phi_n x T\in\mathcal{M} \mu (\Phi_n) x\in X\setminus T \Phi_n(x) (f_n)_x L^1 \nu (f_n)\subset S(\mu,E) L^1 (\Phi_n) x\in X\setminus (S\cup T) (f_n)_x\to f_x \nu (f_n)_x L^1 f_x \in \mathcal{L}(\nu,E) (f_n)_x L^1 f_x x\in X \setminus (S\cup T) \int_Y(f_n)_xd\nu \int_Y f_x d\nu (f_n)\subset  \mathcal{L}(\mu,E) f_n\to f f:X\to E C\geq0 \|f_n\|_1\leq C n f\in \mathcal{L}(\mu,E) \|f\|_1\leq C f_x \in \mathcal{L}(\nu,E) x\in X \setminus (S\cup T) x (f_n)_x \subset S(\mathcal{B},E) L^1 (f_n)_x\to f_x \nu \int_Y f_x d\nu=\lim_{n\to\infty}\int_Y (f_n)_x d\nu x\in X \setminus (S\cup T) \Psi_n:X\to E x\mapsto \int_Y(f_n)_x S(\mathcal{A},E) \Phi_n \int_Y d\nu (\Psi_n) L^1 (\Psi_n) L^1 \|\Psi_n-\Psi_m\|_1=\int_X |\Psi_n-\Psi_m| d\mu=\int_X\bigg|\int_Y(f_n)_x-(f_m)_xd\nu\bigg| d\mu\leq \int_X\int_Y |(f_n)_x-(f_m)_x| d\nu d\mu = \int_{X\times Y} |f_n-f_m|d(\mu\otimes\nu) = \|f_n-f_m\|_1\to 0 \text{ as } n,m\to\infty |\int_Xfd\mu|\leq \|f\|_1 f\in\mathcal{L}(\mu,E) \Psi(x) \int_Y f_x d\nu x\in X\setminus(S\cup T) \Psi_n\to\Psi x (\Psi_n) L^1 \Psi \in \mathcal{L}(\mu,E) \int_X \Psi d\mu=\lim_{n\to\infty}\int_X \Psi_n d\mu \int_X \int_Y f_x d\nu d\mu=\lim_{n\to\infty}\int_X \int_Y (f_n)_x d\nu d\mu=\lim_{n\to\infty}\int_{X\times Y} f_n d(\mu\otimes \nu)=\int_{X\times Y} f d(\mu\otimes \nu) (f_n) L^1 f","['real-analysis', 'functional-analysis', 'measure-theory', 'banach-spaces', 'fubini-tonelli-theorems']"
35,If $a_n \rightarrow a$ Then $\frac{1}{n} \sum_{k=1}^{n} a_k \rightarrow a$,If  Then,a_n \rightarrow a \frac{1}{n} \sum_{k=1}^{n} a_k \rightarrow a,"If $a_n \rightarrow a$ as $n \rightarrow \infty$ , then can we say that $\frac{1}{n} \sum_{k=1}^{n} a_k \rightarrow a$ as $n \rightarrow \infty$ ? If we consider $\frac{1}{n} \sum_{k=1}^{n} a_k$ as average of terms, it seems obvious intuitively. But how can I prove it rigorously? My idea was followings: Since $a_n \rightarrow a$ , given arbitary $\epsilon>0$ , there is some $N$ s.t. $n \geq N \implies \lvert a_n-a\rvert<\epsilon$ \begin{align} \left\lvert\frac{1}{n} \sum_{k=1}^{n} a_k-a\right\rvert & \leq \frac{1}{n} \sum_{k=1}^{N} \lvert a_k-a\rvert + \frac{1}{n} \sum_{k=N+1}^{n} \lvert a_k-a\rvert \\ &\leq \frac{1}{n} \sum_{k=1}^{N} \lvert a_k-a\rvert + \frac{n-N-1}{n}\epsilon \\& \leq \frac{2}{n}A + \frac{n-N+1}{n}\epsilon \end{align} where $A = \sup a_n <\infty$ as $a_n$ converges. Then by letting $n \rightarrow \infty$ gives \begin{equation} \left\lvert \lim_{n \rightarrow \infty} \frac{1}{n} \sum_{k=1}^{n} a_k-a\right\rvert \leq \epsilon \end{equation} for arbitary $\epsilon$ , so our claim is proved. Is there any non-rigorous part or wrong in my proof? (I'm a beginner at analysis.) Thanks","If as , then can we say that as ? If we consider as average of terms, it seems obvious intuitively. But how can I prove it rigorously? My idea was followings: Since , given arbitary , there is some s.t. where as converges. Then by letting gives for arbitary , so our claim is proved. Is there any non-rigorous part or wrong in my proof? (I'm a beginner at analysis.) Thanks","a_n \rightarrow a n \rightarrow \infty \frac{1}{n} \sum_{k=1}^{n} a_k \rightarrow a n \rightarrow \infty \frac{1}{n} \sum_{k=1}^{n} a_k a_n \rightarrow a \epsilon>0 N n \geq N \implies \lvert a_n-a\rvert<\epsilon \begin{align}
\left\lvert\frac{1}{n} \sum_{k=1}^{n} a_k-a\right\rvert & \leq \frac{1}{n} \sum_{k=1}^{N} \lvert a_k-a\rvert + \frac{1}{n} \sum_{k=N+1}^{n} \lvert a_k-a\rvert \\ &\leq \frac{1}{n} \sum_{k=1}^{N} \lvert a_k-a\rvert + \frac{n-N-1}{n}\epsilon \\& \leq \frac{2}{n}A + \frac{n-N+1}{n}\epsilon
\end{align} A = \sup a_n <\infty a_n n \rightarrow \infty \begin{equation}
\left\lvert \lim_{n \rightarrow \infty} \frac{1}{n} \sum_{k=1}^{n} a_k-a\right\rvert \leq \epsilon
\end{equation} \epsilon","['real-analysis', 'sequences-and-series', 'solution-verification']"
36,Can we further refine $\int_{0}^{1}x^{x^x} \ dx=\frac 1 2+\sum_{n=1}^{\infty}(-1)^n\sum_{k=1}^{\infty}\frac {(n-k)^k}{(k+1)^{n+1}} \binom {n}{k}$,Can we further refine,\int_{0}^{1}x^{x^x} \ dx=\frac 1 2+\sum_{n=1}^{\infty}(-1)^n\sum_{k=1}^{\infty}\frac {(n-k)^k}{(k+1)^{n+1}} \binom {n}{k},"Question Can we further refine the integral $$\int_{0}^{1}x^{x^x}\ dx=\frac 1  2+\sum_{n=1}^{\infty}(-1)^n\sum_{k=1}^{n}\frac {(n-k)^k}{(k+1)^{n+1}}  \binom {n}{k}$$ ? To compute the result, first note that $0<x<x^{x^x}<1$ when $0<x<1$ . Hence by dominated convergence, supposing that $a>0$ , we have $$\lim_{a \to 0^+}  \int_{0}^{1} x^a x^{x^x}\ dx = \int_{0}^{1}x^{x^x}\ dx $$ and therefore \begin{align}  \int_{0}^{1} x^a x^{x^x}\ dx &= \int_{0}^{1} x^a e^{x^x \ln x}\ dx \\ &= \sum_{n=0}^{\infty}\frac 1 {n!}\int_{0}^{1}x^a(x^x \ln x)^n\ dx \\ &=\sum_{n=0}^{\infty}\frac 1 {n!}\sum_{k=0}^{\infty}\int_{0}^{1}x^a (\log x)^n \frac {(nx \log x)^k}{k!}\ dx \\ &= \sum_{n=0}^{\infty}\sum_{k=0}^{\infty}\frac {n^k}{n!k!}\int_{0}^{1}x^{k+a}{(\log x)}^{n+k}\ dx \\ &= \sum_{n=0}^{\infty}\sum_{k=0}^{\infty}\frac {n^k}{n!k!}\int_{1}^{\infty}t^{-k-a-2}{(\log t)}^{n+k}\ dt \\ &= \sum_{n=0}^{\infty}\sum_{k=0}^{\infty}\frac {(-1)^{n+k}n^k}{n!k!(k+1+a)^{n+k+1}}\int_{0}^{\infty} e^{-u} u ^{n+k}\ du \end{align} where in the last line we made the transformation $(k+1+a) \log t = -u$ . Taking the limit $a \to 0^+$ and on further calculations, we get $$\int_{0}^{1}x^{x^x}\ dx=\frac 1 2+\sum_{n=1}^{\infty}(-1)^n\sum_{k=1}^{n}\frac {(n-k)^k}{(k+1)^{n+1}} \binom {n}{k}$$ The result has two summations in the answer. I want to know can we further improve the answer? Thank you for your help!","Question Can we further refine the integral ? To compute the result, first note that when . Hence by dominated convergence, supposing that , we have and therefore where in the last line we made the transformation . Taking the limit and on further calculations, we get The result has two summations in the answer. I want to know can we further improve the answer? Thank you for your help!","\int_{0}^{1}x^{x^x}\ dx=\frac 1
 2+\sum_{n=1}^{\infty}(-1)^n\sum_{k=1}^{n}\frac {(n-k)^k}{(k+1)^{n+1}}
 \binom {n}{k} 0<x<x^{x^x}<1 0<x<1 a>0 \lim_{a \to 0^+}  \int_{0}^{1} x^a x^{x^x}\ dx = \int_{0}^{1}x^{x^x}\ dx  \begin{align} 
\int_{0}^{1} x^a x^{x^x}\ dx &= \int_{0}^{1} x^a e^{x^x \ln x}\ dx \\
&= \sum_{n=0}^{\infty}\frac 1 {n!}\int_{0}^{1}x^a(x^x \ln x)^n\ dx \\
&=\sum_{n=0}^{\infty}\frac 1 {n!}\sum_{k=0}^{\infty}\int_{0}^{1}x^a (\log x)^n \frac {(nx \log x)^k}{k!}\ dx \\
&= \sum_{n=0}^{\infty}\sum_{k=0}^{\infty}\frac {n^k}{n!k!}\int_{0}^{1}x^{k+a}{(\log x)}^{n+k}\ dx \\
&= \sum_{n=0}^{\infty}\sum_{k=0}^{\infty}\frac {n^k}{n!k!}\int_{1}^{\infty}t^{-k-a-2}{(\log t)}^{n+k}\ dt \\
&= \sum_{n=0}^{\infty}\sum_{k=0}^{\infty}\frac {(-1)^{n+k}n^k}{n!k!(k+1+a)^{n+k+1}}\int_{0}^{\infty} e^{-u} u ^{n+k}\ du
\end{align} (k+1+a) \log t = -u a \to 0^+ \int_{0}^{1}x^{x^x}\ dx=\frac 1 2+\sum_{n=1}^{\infty}(-1)^n\sum_{k=1}^{n}\frac {(n-k)^k}{(k+1)^{n+1}} \binom {n}{k}","['real-analysis', 'calculus', 'integration', 'definite-integrals']"
37,"Show that there exist $[a,b]\subset [0,1]$, such that $\int_{a}^{b}f(x)dx$ = $\int_{a}^{b}g(x)dx$ = $\frac{1}{2}$","Show that there exist , such that  =  =","[a,b]\subset [0,1] \int_{a}^{b}f(x)dx \int_{a}^{b}g(x)dx \frac{1}{2}","Let $f(x)$ and $g(x)$ be two continuous functions on $[0,1]$ and $$\int_{0}^{1}f(x) dx= \int_{0}^{1}g(x)dx = 1$$ Show that there exist $[a,b]\subset [0,1]$ , such that $$\int_{a}^{b}f(x) dx= \int_{a}^{b}g(x)dx = \frac{1}{2} $$ The question can be solved by considering the fundamental group of $S^1$ , now I am wondering if we can solve it by real-analysis. Here is the topological solution: Assume that for any $[a,b]\subset[0,1]$ , we always have $$(\int_{a}^{b}f(x) dx\neq \frac{1}{2}) \quad \vee \quad(\int_{a}^{b}g(x)dx \neq \frac{1}{2}) $$ Consider mapping $$\phi:D\rightarrow\mathbb{E}^2\backslash\{(\frac{1}{2},\frac{1}{2})\},\,\,(x,y)\mapsto(\int_{y}^{x}f(t) dt,\int_{y}^{x}g(t) dt)$$ where $D=\{(x,y)|0\leq x\leq y\leq 1\}$ . Let $a$ be path from $(0,0)$ to $(0,1) $ in $D$ and $b$ be path from $(0,1)$ to $(1,1) $ in $D$ , then $ab$ is a path from $(0,0)$ to $(1,1) $ in $D$ and $$\phi\circ (ab):[0,1]\rightarrow\mathbb{E}^2\backslash\{(\frac{1}{2},\frac{1}{2})\} $$ Notice that $$(\phi\circ (ab))(0)=(\phi\circ (ab))(1)=(0,0)$$ so $\phi\circ (ab)$ is a loop based on $(0,0)$ in $\mathbb{E}^2\backslash\{(\frac{1}{2},\frac{1}{2})\}$ . For any $t\in [0,\frac{1}{2}]$ , it's not hard to get that $$(\phi\circ (ab))(t+\frac{1}{2})=(1,1)-(\phi\circ (ab))(t)$$ is equivalent to $$(\phi\circ (ab))(t+\frac{1}{2})-(\frac{1}{2},\frac{1}{2})=-((\phi\circ (ab))(t)-(\frac{1}{2},\frac{1}{2}))$$ Define retraction $$r:\mathbb{E}^2\backslash\{(\frac{1}{2},\frac{1}{2})\}\rightarrow S^1,\quad (x,y)\rightarrow\ \frac{(x,y)-(\frac{1}{2},\frac{1}{2})}{||(x,y)-(\frac{1}{2},\frac{1}{2})||}$$ Then $r\circ \phi\circ (ab)$ is a loop on $S^1$ , such that $$(r\circ \phi\circ (ab))(t+\frac{1}{2})=-(r\circ \phi\circ (ab))(t),\quad\forall t\in [0,\frac{1}{2}]$$ So $<r\circ \phi\circ (ab)>$ is not trivial in $\pi_1(S^1)$ . However, $ab$ is path homotopic to $c$ in $D$ , where $c$ is the path between $(0,0)$ and $(1,1)$ in $D$ . In this case, $r\circ \phi\circ (ab)$ is a point-path in $S^1$ by $$(\phi\circ c)(t)=\phi(t,t)\equiv (0,0)$$ which leads to contradiction.","Let and be two continuous functions on and Show that there exist , such that The question can be solved by considering the fundamental group of , now I am wondering if we can solve it by real-analysis. Here is the topological solution: Assume that for any , we always have Consider mapping where . Let be path from to in and be path from to in , then is a path from to in and Notice that so is a loop based on in . For any , it's not hard to get that is equivalent to Define retraction Then is a loop on , such that So is not trivial in . However, is path homotopic to in , where is the path between and in . In this case, is a point-path in by which leads to contradiction.","f(x) g(x) [0,1] \int_{0}^{1}f(x) dx= \int_{0}^{1}g(x)dx = 1 [a,b]\subset [0,1] \int_{a}^{b}f(x) dx= \int_{a}^{b}g(x)dx = \frac{1}{2}  S^1 [a,b]\subset[0,1] (\int_{a}^{b}f(x) dx\neq \frac{1}{2}) \quad \vee \quad(\int_{a}^{b}g(x)dx \neq \frac{1}{2})  \phi:D\rightarrow\mathbb{E}^2\backslash\{(\frac{1}{2},\frac{1}{2})\},\,\,(x,y)\mapsto(\int_{y}^{x}f(t) dt,\int_{y}^{x}g(t) dt) D=\{(x,y)|0\leq x\leq y\leq 1\} a (0,0) (0,1)  D b (0,1) (1,1)  D ab (0,0) (1,1)  D \phi\circ (ab):[0,1]\rightarrow\mathbb{E}^2\backslash\{(\frac{1}{2},\frac{1}{2})\}  (\phi\circ (ab))(0)=(\phi\circ (ab))(1)=(0,0) \phi\circ (ab) (0,0) \mathbb{E}^2\backslash\{(\frac{1}{2},\frac{1}{2})\} t\in [0,\frac{1}{2}] (\phi\circ (ab))(t+\frac{1}{2})=(1,1)-(\phi\circ (ab))(t) (\phi\circ (ab))(t+\frac{1}{2})-(\frac{1}{2},\frac{1}{2})=-((\phi\circ (ab))(t)-(\frac{1}{2},\frac{1}{2})) r:\mathbb{E}^2\backslash\{(\frac{1}{2},\frac{1}{2})\}\rightarrow S^1,\quad (x,y)\rightarrow\ \frac{(x,y)-(\frac{1}{2},\frac{1}{2})}{||(x,y)-(\frac{1}{2},\frac{1}{2})||} r\circ \phi\circ (ab) S^1 (r\circ \phi\circ (ab))(t+\frac{1}{2})=-(r\circ \phi\circ (ab))(t),\quad\forall t\in [0,\frac{1}{2}] <r\circ \phi\circ (ab)> \pi_1(S^1) ab c D c (0,0) (1,1) D r\circ \phi\circ (ab) S^1 (\phi\circ c)(t)=\phi(t,t)\equiv (0,0)",['real-analysis']
38,"For $n\in\mathbb{N}^+$, $\Re(s)>0$, evaluate $\int_{0}^{\infty}\sin\left(2\pi ne^{x}\right)\left[\frac{s}{e^{sx}-1}-\frac{1}{x}\right]dx$","For , , evaluate",n\in\mathbb{N}^+ \Re(s)>0 \int_{0}^{\infty}\sin\left(2\pi ne^{x}\right)\left[\frac{s}{e^{sx}-1}-\frac{1}{x}\right]dx,"Let $n$ be a positive integer, and $s\in \mathbb{C}\;,\Re(s)>0$ . I want to compute the integral : $$\int_{0}^{\infty}\sin\left(2\pi ne^{x}\right)\left[\frac{s}{e^{sx}-1}-\frac{1}{x}\right]dx$$ I tried using the integral presentation found here : $$\sin(2\pi ne^{x})=\frac{\sqrt{\pi}}{2\pi i }\int_{\gamma-i\infty}^{\gamma+i\infty}\frac{\Gamma(z)}{\Gamma\left(\frac{3}{2}-z \right )}\left(\pi n \right )^{-2z+1}e^{-(2z-1)x}dz\;\;\;\;\;0<\gamma<1$$ in conjunction with : $$r+\log(r)+\psi\left(\frac{1}{r}\right)=-\int_{0}^{\infty}e^{-y}\left(\frac{r}{e^{ry}-1}-\frac{1}{y}\right)dy$$ Where $\psi\left(\cdot\right)$ is the digamma function. But that made the problem even more difficult. Any help is highly appreciated. EDIT : Using the Fourier reciprocity : $$\frac{s}{e^{sx}-1}-\frac{1}{x}=2\int_{0}^{\infty}\left(\frac{1}{e^{2\pi \theta/s}-1}-\frac{s}{2\pi \theta} \right )\sin(x\theta)d\theta$$ Our integral reads : $$2\int_{0}^{\infty} g(\theta)\left(\frac{1}{e^{2\pi \theta/s}-1}-\frac{s}{2\pi \theta} \right )d\theta$$ Where : $$g(\theta)=\int_{0}^{\infty}\sin(2\pi ne^{x})\sin(x\theta)dx$$ $$=\frac{\sqrt{\pi}}{2\pi i }\int_{\gamma-i\infty}^{\gamma+i\infty}\frac{\Gamma(z)}{\Gamma\left(\frac{3}{2}-z \right )}\left(\pi n \right )^{-2z+1}\frac{\theta}{\theta^{2}+(2z-1)^{2}}dz$$","Let be a positive integer, and . I want to compute the integral : I tried using the integral presentation found here : in conjunction with : Where is the digamma function. But that made the problem even more difficult. Any help is highly appreciated. EDIT : Using the Fourier reciprocity : Our integral reads : Where :","n s\in \mathbb{C}\;,\Re(s)>0 \int_{0}^{\infty}\sin\left(2\pi ne^{x}\right)\left[\frac{s}{e^{sx}-1}-\frac{1}{x}\right]dx \sin(2\pi ne^{x})=\frac{\sqrt{\pi}}{2\pi i }\int_{\gamma-i\infty}^{\gamma+i\infty}\frac{\Gamma(z)}{\Gamma\left(\frac{3}{2}-z \right )}\left(\pi n \right )^{-2z+1}e^{-(2z-1)x}dz\;\;\;\;\;0<\gamma<1 r+\log(r)+\psi\left(\frac{1}{r}\right)=-\int_{0}^{\infty}e^{-y}\left(\frac{r}{e^{ry}-1}-\frac{1}{y}\right)dy \psi\left(\cdot\right) \frac{s}{e^{sx}-1}-\frac{1}{x}=2\int_{0}^{\infty}\left(\frac{1}{e^{2\pi \theta/s}-1}-\frac{s}{2\pi \theta} \right )\sin(x\theta)d\theta 2\int_{0}^{\infty} g(\theta)\left(\frac{1}{e^{2\pi \theta/s}-1}-\frac{s}{2\pi \theta} \right )d\theta g(\theta)=\int_{0}^{\infty}\sin(2\pi ne^{x})\sin(x\theta)dx =\frac{\sqrt{\pi}}{2\pi i }\int_{\gamma-i\infty}^{\gamma+i\infty}\frac{\Gamma(z)}{\Gamma\left(\frac{3}{2}-z \right )}\left(\pi n \right )^{-2z+1}\frac{\theta}{\theta^{2}+(2z-1)^{2}}dz","['real-analysis', 'complex-analysis', 'definite-integrals', 'residue-calculus']"
39,"Construct a Borel set on R such that it intersect every open interval with non-zero non-""full"" measure","Construct a Borel set on R such that it intersect every open interval with non-zero non-""full"" measure",,"This is from problem $8$, Chapter II of Rudin's Real and Complex Analysis. The problem asks for a Borel set $M$ on $R$, such that for any interval $I$, $M \cap I$ has measure greater than $0$ and less than $m(I)$. I was thinking of taking the Cantor approach: taking $R$ to be the union of $[a,b]$ with $a$ and $b$ rationals, and for each $[a,b]$ we construct Cantor sets inside it. During theconstruction of each Cantor set, in order to have positive measure on it, we need to take off smaller and smaller intervals from it, namely the proportion goes to $0$. As a result, these Cantor sets are extremely ""dense"" on their ends. If for an interval $I$ it intersects with the Cantor set on $[a,b]$ while $b-a>>m(I)$, we shall expect the measure of intersection to be rather close to $m(I)$ and then we lose control on these cases. Is there any way to fix this or shall I consider other approaches? Thank you","This is from problem $8$, Chapter II of Rudin's Real and Complex Analysis. The problem asks for a Borel set $M$ on $R$, such that for any interval $I$, $M \cap I$ has measure greater than $0$ and less than $m(I)$. I was thinking of taking the Cantor approach: taking $R$ to be the union of $[a,b]$ with $a$ and $b$ rationals, and for each $[a,b]$ we construct Cantor sets inside it. During theconstruction of each Cantor set, in order to have positive measure on it, we need to take off smaller and smaller intervals from it, namely the proportion goes to $0$. As a result, these Cantor sets are extremely ""dense"" on their ends. If for an interval $I$ it intersects with the Cantor set on $[a,b]$ while $b-a>>m(I)$, we shall expect the measure of intersection to be rather close to $m(I)$ and then we lose control on these cases. Is there any way to fix this or shall I consider other approaches? Thank you",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
40,Potential for Monotone Operator,Potential for Monotone Operator,,"I have a question about understanding the proof of Theorem 4.11 in the paper A Potential Theory for Monotone Multivalued Operators (accessible here ).  The authors claim to construct a convex functional and I'm not sure I follow their argument. My specific question is at the end, but I provide some background from the paper below before. Background : The paper shows, how for a pair of dual locally convex topological vector spaces $(X,X')$ and a monotone set-valued operator $M:X \to X'$ , one can define a notion of path integral along polygonal paths (as the restriction of $M$ to any straight line in one's is monotone and hence Riemann-integrable). The authors call $M$ conservative if its path integral around any closed polygonal path in its domain (the set of points of $X$ where it is non-empty valued) is zero.  The authors define the integral of $M$ along any line segment $[x,y] \subseteq \textrm{dom}(M)$ via: $$ \int_{0}^1 \langle M(x + t(y-x)), y-x \rangle \, dt = \sup \bigg\{\sum_{i=0}^{n-1}\langle x_i^*, x_{i+1} - x_i\rangle\bigg\} = \inf\bigg\{\sum_{i=0}^{n-1}\langle x_{i+1}^*, x_{i+1}- x_i\rangle \bigg\} $$ where $x_i^* \in M(x_i)$ , and the sup/inf are over all refinements of the line segment, and just follow from their respective arguments being the left/right Riemann sums of monotone increasing functions. The authors then state the following theorem (4.11, p. 623), which I reproduce below. Theorem 4.11 :  To any conservative monotone multivalued map $M:X \to X'$ with a polygonally path connected domain, there corresponds, to within an arbitrary additive constant, a convex potential $f: X \to \mathbb{R} \cup \{+\infty\}$ , which is the restriction on $\textrm{dom}(M)$ of a lower semicontinuous proper convex functional.  The potential $f$ is assumed to be $+\infty$ outside $\textrm{dom}(M)$ and is defined on $\textrm{dom}(M)$ by: $$ \begin{aligned} f(x) - f(x_0) & = \int_\pi \langle M(z), dz\rangle = \\ & =\sup\bigg\{\sum_{i=0}^{n-1}\langle x_i^*, x_{i+1}- x_i\rangle + \langle x_n^*, x- x_n\rangle \bigg\}\\ & = \inf\bigg\{\sum_{i=0}^{n-1} \langle x_{i+1}^*, x_{i+1} -x_i\rangle + \langle x^*, x-x_n\rangle\bigg\} \end{aligned} $$ where the sup/inf are again over all refinements of the poylgonal path $\pi$ . Source of confusion : The proof argues that, by definition, on $\textrm{dom}(M)$ , $f$ is equal to the lower semicontinuous proper convex function defined as the pointwise supremum of a family of continuous affine functions, the Riemann sums.  I don't follow this step.  Normally, when I have seen that results about the supremum of a family of affine functionals is convex, the family of functionals does not vary point to point, whereas here it seems to, as long as $\textrm{dom}(M)$ is not convex (which the authors are explicitly allowing for).  For example, if I have two points $x,y \in \textrm{dom}(M)$ but for which the line segment connecting them is not, it is not clear to me that how the suprema of the set of affine functionals given by refining a path $\pi_x$ from $x_0$ to $x$ relates to the suprema over refinements for a given path $\pi_y$ . I'd be happy to provide my attempt to verify too and where I get stuck, but as this question is already fairly long I'll leave it for now, and I suspect the answer is probably something simple. Question : Why is $f$ lower semicontinuous/convex?","I have a question about understanding the proof of Theorem 4.11 in the paper A Potential Theory for Monotone Multivalued Operators (accessible here ).  The authors claim to construct a convex functional and I'm not sure I follow their argument. My specific question is at the end, but I provide some background from the paper below before. Background : The paper shows, how for a pair of dual locally convex topological vector spaces and a monotone set-valued operator , one can define a notion of path integral along polygonal paths (as the restriction of to any straight line in one's is monotone and hence Riemann-integrable). The authors call conservative if its path integral around any closed polygonal path in its domain (the set of points of where it is non-empty valued) is zero.  The authors define the integral of along any line segment via: where , and the sup/inf are over all refinements of the line segment, and just follow from their respective arguments being the left/right Riemann sums of monotone increasing functions. The authors then state the following theorem (4.11, p. 623), which I reproduce below. Theorem 4.11 :  To any conservative monotone multivalued map with a polygonally path connected domain, there corresponds, to within an arbitrary additive constant, a convex potential , which is the restriction on of a lower semicontinuous proper convex functional.  The potential is assumed to be outside and is defined on by: where the sup/inf are again over all refinements of the poylgonal path . Source of confusion : The proof argues that, by definition, on , is equal to the lower semicontinuous proper convex function defined as the pointwise supremum of a family of continuous affine functions, the Riemann sums.  I don't follow this step.  Normally, when I have seen that results about the supremum of a family of affine functionals is convex, the family of functionals does not vary point to point, whereas here it seems to, as long as is not convex (which the authors are explicitly allowing for).  For example, if I have two points but for which the line segment connecting them is not, it is not clear to me that how the suprema of the set of affine functionals given by refining a path from to relates to the suprema over refinements for a given path . I'd be happy to provide my attempt to verify too and where I get stuck, but as this question is already fairly long I'll leave it for now, and I suspect the answer is probably something simple. Question : Why is lower semicontinuous/convex?","(X,X') M:X \to X' M M X M [x,y] \subseteq \textrm{dom}(M) 
\int_{0}^1 \langle M(x + t(y-x)), y-x \rangle \, dt = \sup \bigg\{\sum_{i=0}^{n-1}\langle x_i^*, x_{i+1} - x_i\rangle\bigg\} = \inf\bigg\{\sum_{i=0}^{n-1}\langle x_{i+1}^*, x_{i+1}- x_i\rangle \bigg\}
 x_i^* \in M(x_i) M:X \to X' f: X \to \mathbb{R} \cup \{+\infty\} \textrm{dom}(M) f +\infty \textrm{dom}(M) \textrm{dom}(M) 
\begin{aligned}
f(x) - f(x_0) & = \int_\pi \langle M(z), dz\rangle = \\
& =\sup\bigg\{\sum_{i=0}^{n-1}\langle x_i^*, x_{i+1}- x_i\rangle + \langle x_n^*, x- x_n\rangle \bigg\}\\
& = \inf\bigg\{\sum_{i=0}^{n-1} \langle x_{i+1}^*, x_{i+1} -x_i\rangle + \langle x^*, x-x_n\rangle\bigg\}
\end{aligned}
 \pi \textrm{dom}(M) f \textrm{dom}(M) x,y \in \textrm{dom}(M) \pi_x x_0 x \pi_y f","['real-analysis', 'functional-analysis', 'convex-analysis', 'vector-analysis', 'potential-theory']"
41,Spectacular failure of Lebesgue differentiation for rectangles,Spectacular failure of Lebesgue differentiation for rectangles,,"Let $\mathcal{R}$ be the set of rectangles in the plane and, given $f \in L^1$ let $$ f^*(x) = \sup_{x \in R \in \mathcal{R}} \frac{1}{ \lvert R \rvert} \int_R \lvert \, f  \,\rvert $$ as defined in this question . You can show that the weak-type inequality fails for this operator, that is, there is no constant $A$ such that $$ m(\{ f^* > \alpha \}) < \frac{A}{\alpha} \lVert \, f \, \rVert_1 $$ for each integrable $f$. From Stein and Shakarchi's book on real analysis I'd like to show the following, much stronger claim: there is an integrable $f$ such that $$ \limsup_{\text{diam}(R) \to 0} \frac{1}{|\,R\,|} \int_R \lvert \, f \, \rvert = \infty \text{ a.e.} $$ where diam, of course, is the diameter. According to the book this ""should"" follow from the failure of the weak type inequality, but I don't really know how to do it. I was thinking something like the following, but it failed. It suffices to find and $f$ such that the desired conclusion holds only on a set of positive measure, for we can translate around and get the desired effect. So I wanted to show that if the $\limsup$ expression is finite everywhere, the weak-type inequality holds, for that specific function. That would get me what I want. However I got nowhere with finding a function for which the weak-type inequality fails, sadly. Edit: Whoops, this is actually very hard. In its most general form it relies on a result found on page 441 of Stein's book Harmonic Analysis: Real-Variable Methods, Orthogonality, and Oscillatory Integrals. Hopefully this specific case can be had more easily, though?","Let $\mathcal{R}$ be the set of rectangles in the plane and, given $f \in L^1$ let $$ f^*(x) = \sup_{x \in R \in \mathcal{R}} \frac{1}{ \lvert R \rvert} \int_R \lvert \, f  \,\rvert $$ as defined in this question . You can show that the weak-type inequality fails for this operator, that is, there is no constant $A$ such that $$ m(\{ f^* > \alpha \}) < \frac{A}{\alpha} \lVert \, f \, \rVert_1 $$ for each integrable $f$. From Stein and Shakarchi's book on real analysis I'd like to show the following, much stronger claim: there is an integrable $f$ such that $$ \limsup_{\text{diam}(R) \to 0} \frac{1}{|\,R\,|} \int_R \lvert \, f \, \rvert = \infty \text{ a.e.} $$ where diam, of course, is the diameter. According to the book this ""should"" follow from the failure of the weak type inequality, but I don't really know how to do it. I was thinking something like the following, but it failed. It suffices to find and $f$ such that the desired conclusion holds only on a set of positive measure, for we can translate around and get the desired effect. So I wanted to show that if the $\limsup$ expression is finite everywhere, the weak-type inequality holds, for that specific function. That would get me what I want. However I got nowhere with finding a function for which the weak-type inequality fails, sadly. Edit: Whoops, this is actually very hard. In its most general form it relies on a result found on page 441 of Stein's book Harmonic Analysis: Real-Variable Methods, Orthogonality, and Oscillatory Integrals. Hopefully this specific case can be had more easily, though?",,"['real-analysis', 'functional-analysis', 'lebesgue-integral']"
42,Evaluating Elliptic Integrals in terms of Gamma Function,Evaluating Elliptic Integrals in terms of Gamma Function,,"Some complete elliptic integral of first and second kind $E(k)$ and $K(k)$ can be evaluated for some particular values of $k$ in terms of Euler Gamma function. For example, for $k = \sqrt{2}/2$, $E(k)$ and $K(k)$ can be evaluated in terms of $\Gamma(1/4)$. Regarding this topic, I raise the following two questions: 1 - I am curious, about the mathematical procedure which is implemented to transform the elliptic integral to an integral solvable using the Gamma function. I was able to figure out the transformation for the case $k = \sqrt{2}/2$, but couldn't do it for other values of $k$ for which I know that their corresponding elliptic integrals are indeed expressed in terms of Gamma function (e.g. $k = \frac{\sqrt{6} - \sqrt{2}}{4}$). What is the sequence of transformations applied to the elliptic integral to generate the resulting integral solvable in terms of Gamma function (Hint: The Euler Beta function $B(x,y)$ is indeed involved in this evaluation)? 2 - Is there a mathematical formula through which we can tell the values of $k$ whose corresponding elliptic integrals $K(k)$ and $E(k)$ are solvable by Euler Gamma function? and if so, is there a direct solution to the integral in terms of $k$? Please support your answers with necessary references whenever possible. Thanks in advance for your help.","Some complete elliptic integral of first and second kind $E(k)$ and $K(k)$ can be evaluated for some particular values of $k$ in terms of Euler Gamma function. For example, for $k = \sqrt{2}/2$, $E(k)$ and $K(k)$ can be evaluated in terms of $\Gamma(1/4)$. Regarding this topic, I raise the following two questions: 1 - I am curious, about the mathematical procedure which is implemented to transform the elliptic integral to an integral solvable using the Gamma function. I was able to figure out the transformation for the case $k = \sqrt{2}/2$, but couldn't do it for other values of $k$ for which I know that their corresponding elliptic integrals are indeed expressed in terms of Gamma function (e.g. $k = \frac{\sqrt{6} - \sqrt{2}}{4}$). What is the sequence of transformations applied to the elliptic integral to generate the resulting integral solvable in terms of Gamma function (Hint: The Euler Beta function $B(x,y)$ is indeed involved in this evaluation)? 2 - Is there a mathematical formula through which we can tell the values of $k$ whose corresponding elliptic integrals $K(k)$ and $E(k)$ are solvable by Euler Gamma function? and if so, is there a direct solution to the integral in terms of $k$? Please support your answers with necessary references whenever possible. Thanks in advance for your help.",,"['calculus', 'real-analysis', 'gamma-function', 'elliptic-integrals']"
43,Is $L^p$ separable?,Is  separable?,L^p,"Whether a $L^p(X,\mu)$ space is separable? I understand that the answer depends on $p$ and $X$. It seems to me that it is separable when $1\leq p < \infty, X=\mathbb{R}^n$ or  $X=\mathbb{N}$. Also, $L^\infty(X,\mu) $ is seperable iff $X$ has only finite points. Is there any general results? For example, does the separability of $(X,\mu)$ implies the separability of $L^p(X,\mu)$ when $p<\infty$?","Whether a $L^p(X,\mu)$ space is separable? I understand that the answer depends on $p$ and $X$. It seems to me that it is separable when $1\leq p < \infty, X=\mathbb{R}^n$ or  $X=\mathbb{N}$. Also, $L^\infty(X,\mu) $ is seperable iff $X$ has only finite points. Is there any general results? For example, does the separability of $(X,\mu)$ implies the separability of $L^p(X,\mu)$ when $p<\infty$?",,"['real-analysis', 'functional-analysis', 'measure-theory', 'lp-spaces']"
44,"Topologists glueing, cutting and so on. When is this rigorous?","Topologists glueing, cutting and so on. When is this rigorous?",,"I often see that things in topology are explained very non-rigorously recently. Thereby I mean that it is said that we can cut something and glue something together and so on in order to identify two objects. Sure, in general one would need to write down a map, but this is often very very cumbersome. Therefore, I understand that people tend to illustrate things. Despite, we never talked about actual does and dont's and I would like to understand the following. How do I know that I am allowed to cut something or glue something together? Where do I need to pay attention?","I often see that things in topology are explained very non-rigorously recently. Thereby I mean that it is said that we can cut something and glue something together and so on in order to identify two objects. Sure, in general one would need to write down a map, but this is often very very cumbersome. Therefore, I understand that people tend to illustrate things. Despite, we never talked about actual does and dont's and I would like to understand the following. How do I know that I am allowed to cut something or glue something together? Where do I need to pay attention?",,"['calculus', 'real-analysis']"
45,Existence of non-constant continuous functions with infinitely many zeros [duplicate],Existence of non-constant continuous functions with infinitely many zeros [duplicate],,"This question already has answers here : Closed 12 years ago . Possible Duplicate: A nontrivial everywhere continuous function with uncountably many roots? Does there exist a continuous non-constant real-valued function on $[a,b]$ that has infinitely many zeros?   If one does exist, please give me an example.","This question already has answers here : Closed 12 years ago . Possible Duplicate: A nontrivial everywhere continuous function with uncountably many roots? Does there exist a continuous non-constant real-valued function on $[a,b]$ that has infinitely many zeros?   If one does exist, please give me an example.",,['real-analysis']
46,Prove that a degree-$6$ polynomial has exactly $2$ real roots,Prove that a degree- polynomial has exactly  real roots,6 2,"I have the function $f(x)={7x^6+8x+2}$ and I'm trying to prove that $f$ has exactly 2 real roots. What I've done: The only kind of solution I have come up with is essentially guessing pairs of values for $x$ that give $f$ a different sign and then make use of Bolzano's Theorem. More specifically: $f(-1)=1>0$ and $f(-{1\over 2})=-{121\over 64}<0$, so according to Bolzano's Theorem, there is some $x \in (-1, -{1 \over 2})$ such that $f(x)=0$. $f(-{1\over 2})=-{121\over 64}<0$ and $f(0)=2$, so according to Bolzano's Theorem, there is some $x \in (-{1 \over 2}, 0)$ such that $f(x)=0$. Question: The above solution looks kind of meh to me and I don't think it proves there are exactly 2 real roots, but rather that only 2 were found. Is there a better, more convincing way to prove the existence of exactly 2 roots?","I have the function $f(x)={7x^6+8x+2}$ and I'm trying to prove that $f$ has exactly 2 real roots. What I've done: The only kind of solution I have come up with is essentially guessing pairs of values for $x$ that give $f$ a different sign and then make use of Bolzano's Theorem. More specifically: $f(-1)=1>0$ and $f(-{1\over 2})=-{121\over 64}<0$, so according to Bolzano's Theorem, there is some $x \in (-1, -{1 \over 2})$ such that $f(x)=0$. $f(-{1\over 2})=-{121\over 64}<0$ and $f(0)=2$, so according to Bolzano's Theorem, there is some $x \in (-{1 \over 2}, 0)$ such that $f(x)=0$. Question: The above solution looks kind of meh to me and I don't think it proves there are exactly 2 real roots, but rather that only 2 were found. Is there a better, more convincing way to prove the existence of exactly 2 roots?",,"['calculus', 'real-analysis', 'polynomials', 'roots', 'rolles-theorem']"
47,Evaluate the integral $\int_{-\infty}^{\infty}\binom{n}{x}dx$.,Evaluate the integral .,\int_{-\infty}^{\infty}\binom{n}{x}dx,Evaluate the integral $\int_{-\infty}^{\infty}\binom{n}{x}dx$ . This question came in Cambridge Integration Bee and I have no clue what to do in this. I rewrote $\binom{n}{x}$ as $\frac{n!}{{x!}{(n-x)!}}$ but I don't know how to integrate those factorials. Also what to do if $n$ isn't an integer as no information regarding it is given?,Evaluate the integral . This question came in Cambridge Integration Bee and I have no clue what to do in this. I rewrote as but I don't know how to integrate those factorials. Also what to do if isn't an integer as no information regarding it is given?,\int_{-\infty}^{\infty}\binom{n}{x}dx \binom{n}{x} \frac{n!}{{x!}{(n-x)!}} n,"['real-analysis', 'calculus', 'integration']"
48,Limit of f(x) knowing limit of f(x)/x,Limit of f(x) knowing limit of f(x)/x,,Given that: $$\lim_{x\to0} \frac{f(x)}{x}=0$$ How can I prove that: $$\lim_{x\to0} f(x)=0$$ ? Would the L'Hospital's Rule be applicable?,Given that: $$\lim_{x\to0} \frac{f(x)}{x}=0$$ How can I prove that: $$\lim_{x\to0} f(x)=0$$ ? Would the L'Hospital's Rule be applicable?,,"['calculus', 'real-analysis', 'limits']"
49,How to show $\sqrt{4+2\sqrt{3}}-\sqrt{3} = 1$,How to show,\sqrt{4+2\sqrt{3}}-\sqrt{3} = 1,"I start with $x=\sqrt{4+2\sqrt{3}}-\sqrt{3}$, then $\begin{align*} x +\sqrt{3} &= \sqrt{4+2\sqrt{3}}\\ (x +\sqrt{3})^2 &= (\sqrt{4+2\sqrt{3}})^2\\ x^2 + (2\sqrt{3})x + 3 &= 4+ 2\sqrt{3}\\ x^2 + (2\sqrt{3})x - 1 - 2\sqrt{3} &= 0 \end{align*}$ So I have shown that there is some polynomial whose solution is $x=\sqrt{4+2\sqrt{3}}-\sqrt{3}$, but I have not shown it to be 1.","I start with $x=\sqrt{4+2\sqrt{3}}-\sqrt{3}$, then $\begin{align*} x +\sqrt{3} &= \sqrt{4+2\sqrt{3}}\\ (x +\sqrt{3})^2 &= (\sqrt{4+2\sqrt{3}})^2\\ x^2 + (2\sqrt{3})x + 3 &= 4+ 2\sqrt{3}\\ x^2 + (2\sqrt{3})x - 1 - 2\sqrt{3} &= 0 \end{align*}$ So I have shown that there is some polynomial whose solution is $x=\sqrt{4+2\sqrt{3}}-\sqrt{3}$, but I have not shown it to be 1.",,"['calculus', 'real-analysis', 'irrational-numbers']"
50,Prove that $\sum_{k=0}^n a_k x^k = 0$ has at least $1$ real root if $\sum_{k=0}^n \frac{a_k}{k+1} = 0$,Prove that  has at least  real root if,\sum_{k=0}^n a_k x^k = 0 1 \sum_{k=0}^n \frac{a_k}{k+1} = 0,Knowing that $$ \frac{a_0}{1} + \frac{a_1}{2} + \frac{a_2}{3} +\cdots + \frac{a_n}{n+1} =0$$ Prove that  $$ a_0 + a_1x + a_2x^2 + \cdots + a_nx^n = 0$$ has at least one real solution. I suspect that it's proven with the intermediate value theorem. But can't find two numbers that satisfy it.,Knowing that $$ \frac{a_0}{1} + \frac{a_1}{2} + \frac{a_2}{3} +\cdots + \frac{a_n}{n+1} =0$$ Prove that  $$ a_0 + a_1x + a_2x^2 + \cdots + a_nx^n = 0$$ has at least one real solution. I suspect that it's proven with the intermediate value theorem. But can't find two numbers that satisfy it.,,"['calculus', 'real-analysis', 'derivatives', 'polynomials']"
51,Show that $f$ is the zero function if $f''(x)=f(x)$ and $f(0)=f'(0)=0$,Show that  is the zero function if  and,f f''(x)=f(x) f(0)=f'(0)=0,"Suppose that $f''(x)=f(x)$ for all real numbers $x$, and that $f(0)=f'(0)=0$. Show that $f$ is the zero function. I know that $f''(0)=0$ from the assumptions listed. I want to consider the Taylor series for $f$ centered at $x=0$, which is $\sum$ $[f^{(n)}(0)/n!]x^n$. I am not sure where to go from here.","Suppose that $f''(x)=f(x)$ for all real numbers $x$, and that $f(0)=f'(0)=0$. Show that $f$ is the zero function. I know that $f''(0)=0$ from the assumptions listed. I want to consider the Taylor series for $f$ centered at $x=0$, which is $\sum$ $[f^{(n)}(0)/n!]x^n$. I am not sure where to go from here.",,"['real-analysis', 'ordinary-differential-equations']"
52,"If $f(x) = h(x)g(x)$, is $h$ differentiable if $f$ and $g$ are?","If , is  differentiable if  and  are?",f(x) = h(x)g(x) h f g,"I know that if I have two differentiable functions $f, g$ then the functions $(f + g)$ and $fg$ are also differentiable.  I would like to find a way how to argue about the function $h$ where  \begin{equation} f(x) = (hg)(x) := h(x)g(x) \quad \text{and } f,g \text{ are differentiable} \end{equation} For a start I can conclude $h$ is differentiable at all points where $g(x) \neq 0$ since there I can express $h$ as \begin{equation} h = \frac{f}{g} \end{equation} But for the remaining points I am not sure, my guess is that $h$ is differentiable, any hints how I can make this into a formal argument ? Or am I probably wrong ? In that case, would it help to impose further smoothness on $f$ and $g$, say both are $C^\infty$ ? Many thanks!","I know that if I have two differentiable functions $f, g$ then the functions $(f + g)$ and $fg$ are also differentiable.  I would like to find a way how to argue about the function $h$ where  \begin{equation} f(x) = (hg)(x) := h(x)g(x) \quad \text{and } f,g \text{ are differentiable} \end{equation} For a start I can conclude $h$ is differentiable at all points where $g(x) \neq 0$ since there I can express $h$ as \begin{equation} h = \frac{f}{g} \end{equation} But for the remaining points I am not sure, my guess is that $h$ is differentiable, any hints how I can make this into a formal argument ? Or am I probably wrong ? In that case, would it help to impose further smoothness on $f$ and $g$, say both are $C^\infty$ ? Many thanks!",,['real-analysis']
53,How come rearrangement of a convergent series may not converge to the same value / or does not converge at all,How come rearrangement of a convergent series may not converge to the same value / or does not converge at all,,"By looking at Riemann's Rearrangement theorem I wonder, How come a convergent series particular re-arrengement may not converge to the same value of the original series. Isn't the below true? Let $\phi : N \to N$ be a bijection. Where $N$ is the natural number set A re-arrengement of a sequence $\sum a_n$ would be $\sum a_{\phi(n)}$ Then Give me an example of a series that have a rearrangement which does not converge or does not converge to the same value. I mean... 4+1+2+2+9+6 = 1+2+2+6+4+9 = 24 ??","By looking at Riemann's Rearrangement theorem I wonder, How come a convergent series particular re-arrengement may not converge to the same value of the original series. Isn't the below true? Let $\phi : N \to N$ be a bijection. Where $N$ is the natural number set A re-arrengement of a sequence $\sum a_n$ would be $\sum a_{\phi(n)}$ Then Give me an example of a series that have a rearrangement which does not converge or does not converge to the same value. I mean... 4+1+2+2+9+6 = 1+2+2+6+4+9 = 24 ??",,['real-analysis']
54,What is the easy way to calculate the roots of $z^4+4z^3+6z^2+4z$?,What is the easy way to calculate the roots of ?,z^4+4z^3+6z^2+4z,"What is the easy way to calculate the roots of $z^4+4z^3+6z^2+4z$? I know its answer: 0, -2, -1+i, -1-i. But I dont know how to find? Please show me this. I know this is so trivial, but important for me. Thank you.","What is the easy way to calculate the roots of $z^4+4z^3+6z^2+4z$? I know its answer: 0, -2, -1+i, -1-i. But I dont know how to find? Please show me this. I know this is so trivial, but important for me. Thank you.",,"['calculus', 'real-analysis', 'complex-analysis', 'self-learning']"
55,$\sum \limits_{n=1}^{\infty}n(\frac{2}{3})^n$ Evalute Sum [duplicate],Evalute Sum [duplicate],\sum \limits_{n=1}^{\infty}n(\frac{2}{3})^n,This question already has answers here : Closed 12 years ago . Possible Duplicate: How can I evaluate $\sum_{n=1}^\infty \frac{2n}{3^{n+1}}$ How can you compute the limit of  $\sum \limits_{n=1}^{\infty} n(2/3)^n$ Evidently it is equal to 6 by wolfram alpha but how could you compute such a sum analytically?,This question already has answers here : Closed 12 years ago . Possible Duplicate: How can I evaluate $\sum_{n=1}^\infty \frac{2n}{3^{n+1}}$ How can you compute the limit of  $\sum \limits_{n=1}^{\infty} n(2/3)^n$ Evidently it is equal to 6 by wolfram alpha but how could you compute such a sum analytically?,,"['real-analysis', 'sequences-and-series', 'limits']"
56,Measure Theory - Why doesn't empty interior imply zero measure?,Measure Theory - Why doesn't empty interior imply zero measure?,,"I'm studying for a qualifying exam in measure theory and ended up ""proving"" a set with empty interior implies zero measure. I know this isn't true (irrational numbers provide a counterexample in $\mathbb{R}$ ) but can't find my error: Let $E \subset \mathbb{R}^n$ be a set with empty interior. Note that the closure, $\overline{E} = \text{int}(E) \cup \text{Bd} (E)$ , where $\text{int}(E)$ is the interior of $E$ and $\text{Bd}(E)$ is the set of boundary points of $E$ . By monotonicity and subadditivity of outer measure, $|E|_e \le |\overline{E}_e| \le |\text{int}(E)|_e + |\text{Bd}(E)|_e=0$ by assumption and because the boundary of $E$ is $(n-1)-$ dimensional. Can anyone find my mistake?","I'm studying for a qualifying exam in measure theory and ended up ""proving"" a set with empty interior implies zero measure. I know this isn't true (irrational numbers provide a counterexample in ) but can't find my error: Let be a set with empty interior. Note that the closure, , where is the interior of and is the set of boundary points of . By monotonicity and subadditivity of outer measure, by assumption and because the boundary of is dimensional. Can anyone find my mistake?",\mathbb{R} E \subset \mathbb{R}^n \overline{E} = \text{int}(E) \cup \text{Bd} (E) \text{int}(E) E \text{Bd}(E) E |E|_e \le |\overline{E}_e| \le |\text{int}(E)|_e + |\text{Bd}(E)|_e=0 E (n-1)-,"['real-analysis', 'general-topology', 'measure-theory', 'lebesgue-measure']"
57,Confusion in fundamental theorem of calculus,Confusion in fundamental theorem of calculus,,"In the wikipedia article shown below, it says $A(x)$ represents the area beneath the curve $y=f(x)$ between $0$ and $x$ . Then it says $f(x)=A'(x)$ , i.e. $A(x)=\int f(x)dx$ which means $\int f(x)dx$ represents the area beneath the curve $y=f(x)$ between $0$ and $x$ . I think there is nothing wrong up to here. Now if we put $f(x)=e^{x}$ then $A(x)=e^{x}$ and at $x=0$ , $A(x)=e^{0}=1$ That is, the area $A(x)$ beneath the curve $y=f(x)$ between ( $0$ and $x=0$ ) is $1$ . Now how can we get non-zero area when we find the area between same points (i.e. $0$ and $0$ )","In the wikipedia article shown below, it says represents the area beneath the curve between and . Then it says , i.e. which means represents the area beneath the curve between and . I think there is nothing wrong up to here. Now if we put then and at , That is, the area beneath the curve between ( and ) is . Now how can we get non-zero area when we find the area between same points (i.e. and )",A(x) y=f(x) 0 x f(x)=A'(x) A(x)=\int f(x)dx \int f(x)dx y=f(x) 0 x f(x)=e^{x} A(x)=e^{x} x=0 A(x)=e^{0}=1 A(x) y=f(x) 0 x=0 1 0 0,"['calculus', 'real-analysis', 'integration']"
58,"An example of an infinite open cover of the interval (0,1) that has no finite subcover","An example of an infinite open cover of the interval (0,1) that has no finite subcover",,"I've been having a hard time solving this problem that I was given in class. The problem states "" Give an example of an infinite open cover of the interval (0,1) that has no finite subcover."" I know that the set has to be compact and that both 0 and 1 are limit points. Aside from those two known factors I'm at a complete loss as to how to go about solving this problem.","I've been having a hard time solving this problem that I was given in class. The problem states "" Give an example of an infinite open cover of the interval (0,1) that has no finite subcover."" I know that the set has to be compact and that both 0 and 1 are limit points. Aside from those two known factors I'm at a complete loss as to how to go about solving this problem.",,"['real-analysis', 'general-topology', 'compactness']"
59,Prove that $\sqrt{n} > \ln n$,Prove that,\sqrt{n} > \ln n,"Prove that $\sqrt{n} > \ln n$ for all $n \in \mathbb{N}$. I need to use this fact for one of the proofs that I am working on. However, I am having trouble proving this. I tried induction but don't really know how to do it. Can someone help me with this?","Prove that $\sqrt{n} > \ln n$ for all $n \in \mathbb{N}$. I need to use this fact for one of the proofs that I am working on. However, I am having trouble proving this. I tried induction but don't really know how to do it. Can someone help me with this?",,"['real-analysis', 'sequences-and-series', 'inequality']"
60,Prove that if $f: \mathbb{R} \to \mathbb{Q}$ is continuous then $f$ is constant,Prove that if  is continuous then  is constant,f: \mathbb{R} \to \mathbb{Q} f,"Suppose that  $f: \mathbb{R} \to \mathbb{R}$ is continuous and that $f(x) \in \mathbb{Q}$ for all $x \in \mathbb{R}$. Prove that f is constant. I have the idea that because there is an irrational number between any two rational number then if the function is continuous, the function must be constant. But I don't know how to write out a proper proof for it. Any help is appreciated. Thanks in advance.","Suppose that  $f: \mathbb{R} \to \mathbb{R}$ is continuous and that $f(x) \in \mathbb{Q}$ for all $x \in \mathbb{R}$. Prove that f is constant. I have the idea that because there is an irrational number between any two rational number then if the function is continuous, the function must be constant. But I don't know how to write out a proper proof for it. Any help is appreciated. Thanks in advance.",,"['real-analysis', 'continuity']"
61,Compare $\arcsin (1)$ and $\tan (1)$,Compare  and,\arcsin (1) \tan (1),"Which one is greater: $\arcsin (1)$ or $\tan (1)$ ? How to find without using graph or calculator? I tried using $\sin(\tan1)\leq1$ , but how do I eliminate the possibility of an equality without using the calculator?","Which one is greater: or ? How to find without using graph or calculator? I tried using , but how do I eliminate the possibility of an equality without using the calculator?",\arcsin (1) \tan (1) \sin(\tan1)\leq1,"['real-analysis', 'trigonometry', 'inequality', 'inverse-function', 'number-comparison']"
62,"Prove that $\sqrt{x}$ is continuous on its domain $[0, \infty).$",Prove that  is continuous on its domain,"\sqrt{x} [0, \infty).","Prove that the function $\sqrt{x}$ is continuous on its domain $[0,\infty)$ . Proof. Since $\sqrt{0} = 0, $ we consider the function $\sqrt{x}$ on $[a,\infty)$ where $a$ is real number and $a \neq 0.$ Let $\delta=2\sqrt{a}\varepsilon.$ Then, $\forall x \in \mathit{dom},$ and $\left | x-x_0\right | < \delta \Rightarrow \left| \sqrt{x}-\sqrt{x_0}\right| = \left| \frac{x-x_0}{ \sqrt{x}+\sqrt{x_0}} \right| < \left|\frac{\delta}{2\sqrt{a}}\right|=\varepsilon.$ Can I do this?","Prove that the function is continuous on its domain . Proof. Since we consider the function on where is real number and Let Then, and Can I do this?","\sqrt{x} [0,\infty) \sqrt{0} = 0,  \sqrt{x} [a,\infty) a a \neq 0. \delta=2\sqrt{a}\varepsilon. \forall x \in \mathit{dom}, \left | x-x_0\right | < \delta \Rightarrow \left| \sqrt{x}-\sqrt{x_0}\right| = \left| \frac{x-x_0}{ \sqrt{x}+\sqrt{x_0}} \right| < \left|\frac{\delta}{2\sqrt{a}}\right|=\varepsilon.",['real-analysis']
63,Continuity of $\max$ function,Continuity of  function,\max,"Given continuous functions $f,g: \mathbb{R} \to \mathbb{R}$, in order to prove that $ \max(f(x),g(x))$ is continuous, a standard trick is to rewrite it as a linear combination of continuous functions: $$ \max(f(x),g(x)) = \frac{1}{2} (f(x) + g(x) + |f(x) - g(x)|   ) $$ Is there any sort of motivation for why one might come up with this particular combination of continuous functions? I can see that it works, but without knowing this fact beforehand , what might lead you to consider writing $ \max(f(x),g(x)  ) $ in the above manner?","Given continuous functions $f,g: \mathbb{R} \to \mathbb{R}$, in order to prove that $ \max(f(x),g(x))$ is continuous, a standard trick is to rewrite it as a linear combination of continuous functions: $$ \max(f(x),g(x)) = \frac{1}{2} (f(x) + g(x) + |f(x) - g(x)|   ) $$ Is there any sort of motivation for why one might come up with this particular combination of continuous functions? I can see that it works, but without knowing this fact beforehand , what might lead you to consider writing $ \max(f(x),g(x)  ) $ in the above manner?",,['real-analysis']
64,What are BesselJ functions?,What are BesselJ functions?,,I solved an integration on mathematica which gives BesselJ functions and some other terms. I explored mathematica help and google but could not understand the difference between different types of bessel functions. Specially BesselJ function and its explicit form is my target.,I solved an integration on mathematica which gives BesselJ functions and some other terms. I explored mathematica help and google but could not understand the difference between different types of bessel functions. Specially BesselJ function and its explicit form is my target.,,"['real-analysis', 'integration', 'analysis', 'special-functions', 'bessel-functions']"
65,Infinite Series $\sum\limits_{n=1}^\infty\frac{x^{3n}}{(3n-1)!}$,Infinite Series,\sum\limits_{n=1}^\infty\frac{x^{3n}}{(3n-1)!},"How can we prove that? $$\sum_{n=1}^\infty\frac{x^{3n}}{(3n-1)!}=\frac{1}{3}e^{\frac{-x}{2}}x\left(e^{\frac{3x}{2}}-2\sin\left(\frac{\pi+3\sqrt{3}x}{6}\right)\right).$$ I think if we write the taylor expansion of $\sin(u)$ and $e^u$, we can arrive from RHS to LHS, but I am looking for a way to prove it from LHS.","How can we prove that? $$\sum_{n=1}^\infty\frac{x^{3n}}{(3n-1)!}=\frac{1}{3}e^{\frac{-x}{2}}x\left(e^{\frac{3x}{2}}-2\sin\left(\frac{\pi+3\sqrt{3}x}{6}\right)\right).$$ I think if we write the taylor expansion of $\sin(u)$ and $e^u$, we can arrive from RHS to LHS, but I am looking for a way to prove it from LHS.",,"['real-analysis', 'sequences-and-series', 'complex-analysis', 'ordinary-differential-equations', 'closed-form']"
66,Does the series $\sum\limits_{n=1}^\infty \frac{1}{n\sqrt[n]{n}}$ converge?,Does the series  converge?,\sum\limits_{n=1}^\infty \frac{1}{n\sqrt[n]{n}},"Does the following series converge? $$\sum_{n=1}^\infty \frac{1}{n\sqrt[n]{n}}$$ As $$\frac{1}{n\sqrt[n]{n}}=\frac{1}{n^{1+\frac{1}{n}}},$$ I was thinking that you may consider this as a p-series with $p>1$. But I'm not sure if this is correct, as with p-series, p is a fixed number, right ? On the other hand, $1+\frac{1}{n}>1$ for all $n$. Any hints ?","Does the following series converge? $$\sum_{n=1}^\infty \frac{1}{n\sqrt[n]{n}}$$ As $$\frac{1}{n\sqrt[n]{n}}=\frac{1}{n^{1+\frac{1}{n}}},$$ I was thinking that you may consider this as a p-series with $p>1$. But I'm not sure if this is correct, as with p-series, p is a fixed number, right ? On the other hand, $1+\frac{1}{n}>1$ for all $n$. Any hints ?",,"['real-analysis', 'sequences-and-series']"
67,Weaker Condition than Differentiability that Implies Continuity,Weaker Condition than Differentiability that Implies Continuity,,"It is a well-known fact that differentiability implies continuity.  My question is this: is there some condition for a function that is both weaker than differentiability and stronger than continuity?  I.e., is there a condition that ""guarantees"" continuity that does not also guarantee differentiability? Edit: I realized immediately after posting this that I did not give enough thought to asking this question.  The question that I meant to ask (which, I think, is more interesting) is given here .","It is a well-known fact that differentiability implies continuity.  My question is this: is there some condition for a function that is both weaker than differentiability and stronger than continuity?  I.e., is there a condition that ""guarantees"" continuity that does not also guarantee differentiability? Edit: I realized immediately after posting this that I did not give enough thought to asking this question.  The question that I meant to ask (which, I think, is more interesting) is given here .",,"['real-analysis', 'continuity']"
68,Solving $x^x=\frac{1}{\sqrt 2}$,Solving,x^x=\frac{1}{\sqrt 2},"The equation $$x^x=\frac{1}{\sqrt 2},x\in \mathbb R$$ has two obvious solutions $0.5$ and $0.25$ One can easily prove they are the only ones using differential calculus. Is there any natural algebraic manipulation that would lead to finding these solutions ?","The equation $$x^x=\frac{1}{\sqrt 2},x\in \mathbb R$$ has two obvious solutions $0.5$ and $0.25$ One can easily prove they are the only ones using differential calculus. Is there any natural algebraic manipulation that would lead to finding these solutions ?",,['real-analysis']
69,Open cover rationals proper subset of R?,Open cover rationals proper subset of R?,,"If I were to cover each rational number by a non-empty open interval, would their union always be R? It seems correct to me intuitively, but I am quite certain it is wrong. Thanks","If I were to cover each rational number by a non-empty open interval, would their union always be R? It seems correct to me intuitively, but I am quite certain it is wrong. Thanks",,['real-analysis']
70,Evaluating $ \sum\limits_{n=1}^\infty \frac{1}{n^2 2^n} $,Evaluating, \sum\limits_{n=1}^\infty \frac{1}{n^2 2^n} ,"Evaluate $$ \sum_{n=1}^\infty \dfrac{1}{n^2 2^n}. $$ I have tried using the Maclaurin series of $2^{-n}$ but it further complicated the question. Moreover, I have also tried taking help from another question when the $n^2$ is in the numerator, but no significant progress so far. Any help will be appreciated. Thanks!","Evaluate I have tried using the Maclaurin series of but it further complicated the question. Moreover, I have also tried taking help from another question when the is in the numerator, but no significant progress so far. Any help will be appreciated. Thanks!", \sum_{n=1}^\infty \dfrac{1}{n^2 2^n}.  2^{-n} n^2,"['calculus', 'real-analysis']"
71,Evaluate $\int_0^1(x\ln(x))^{50}dx$,Evaluate,\int_0^1(x\ln(x))^{50}dx,Evaluate $$\int_0^1(x\ln(x))^{50} dx.$$ Here are my steps so far using differentiation under the integral sign: $$I(t) = \int_0^1(x\ln(x))^t dx$$ $$I'(t) = \frac d{dt}\int_0^1(x\ln(x))^t dx = \int_0^1\frac \partial{\partial t}(x\ln(x))^t dx = \int_0^1(x\ln(x))^t\ln(x\ln(x)) dx$$ I can't find a way to continue so hints are appreciated.,Evaluate $$\int_0^1(x\ln(x))^{50} dx.$$ Here are my steps so far using differentiation under the integral sign: $$I(t) = \int_0^1(x\ln(x))^t dx$$ $$I'(t) = \frac d{dt}\int_0^1(x\ln(x))^t dx = \int_0^1\frac \partial{\partial t}(x\ln(x))^t dx = \int_0^1(x\ln(x))^t\ln(x\ln(x)) dx$$ I can't find a way to continue so hints are appreciated.,,"['calculus', 'real-analysis', 'definite-integrals']"
72,Is $\prod \limits_{n=2}^{\infty}(1-\frac{1}{n^2})=1$ [duplicate],Is  [duplicate],\prod \limits_{n=2}^{\infty}(1-\frac{1}{n^2})=1,"This question already has answers here : Finding Value of the Infinite Product $\prod \Bigl(1-\frac{1}{n^{2}}\Bigr)$ (6 answers) Closed 9 years ago . Question is to check if  $\prod \limits_{n=2}^{\infty}(1-\frac{1}{n^2})=1$ we have $\prod \limits_{n=2}^{\infty}(1-\frac{1}{n^2})=\prod \limits_{n=2}^{\infty}(\frac{n^2-1}{n^2})=\prod \limits_{n=2}^{\infty}\frac{n+1}{n}\frac{n-1}{n}=(\frac{3}{2}.\frac{1}{2})(\frac{4}{3}.\frac{2}{3})(\frac{5}{4}.\frac{3}{4})...$ In above product we have for each term $\frac{a}{b}$ a term $\frac{b}{a}$ except for $\frac{1}{2}$.. So, all  other terms gets cancelled and we left with $\frac{1}{2}$. So, $\prod \limits_{n=2}^{\infty}(1-\frac{1}{n^2})=\frac{1}{2}$. I would be thankful if some one can assure that this explanation is correct/wrong?? I am solving this kind of problems for the first time so, it would be helpful if some one can tell if there are any other ways to do this.. Thank you","This question already has answers here : Finding Value of the Infinite Product $\prod \Bigl(1-\frac{1}{n^{2}}\Bigr)$ (6 answers) Closed 9 years ago . Question is to check if  $\prod \limits_{n=2}^{\infty}(1-\frac{1}{n^2})=1$ we have $\prod \limits_{n=2}^{\infty}(1-\frac{1}{n^2})=\prod \limits_{n=2}^{\infty}(\frac{n^2-1}{n^2})=\prod \limits_{n=2}^{\infty}\frac{n+1}{n}\frac{n-1}{n}=(\frac{3}{2}.\frac{1}{2})(\frac{4}{3}.\frac{2}{3})(\frac{5}{4}.\frac{3}{4})...$ In above product we have for each term $\frac{a}{b}$ a term $\frac{b}{a}$ except for $\frac{1}{2}$.. So, all  other terms gets cancelled and we left with $\frac{1}{2}$. So, $\prod \limits_{n=2}^{\infty}(1-\frac{1}{n^2})=\frac{1}{2}$. I would be thankful if some one can assure that this explanation is correct/wrong?? I am solving this kind of problems for the first time so, it would be helpful if some one can tell if there are any other ways to do this.. Thank you",,['real-analysis']
73,How to show $(1/n!)^{1/n}$ goes to $0$ as $n$ goes to infinity? [duplicate],How to show  goes to  as  goes to infinity? [duplicate],(1/n!)^{1/n} 0 n,This question already has answers here : $\lim\limits_{n \to{+}\infty}{\sqrt[n]{n!}}$ is infinite (12 answers) Closed 6 years ago . How do I show $(1/n!)^{1/n}$ goes to $0$ as $n$ goes to infinity? I need this to use the spectral radius theorem to show an operator has spectrum {0}.,This question already has answers here : $\lim\limits_{n \to{+}\infty}{\sqrt[n]{n!}}$ is infinite (12 answers) Closed 6 years ago . How do I show $(1/n!)^{1/n}$ goes to $0$ as $n$ goes to infinity? I need this to use the spectral radius theorem to show an operator has spectrum {0}.,,"['real-analysis', 'limits', 'factorial', 'radicals']"
74,Continuous function satisfying $ f\left(\dfrac{x+t}{2}\right) \le f(x) + f(t)$ inequality must be $0$,Continuous function satisfying  inequality must be, f\left(\dfrac{x+t}{2}\right) \le f(x) + f(t) 0,"Let $f$ a real function defined and continuous on $[0,1]$ such that $$f(0)=f(1)=0$$ $$ f\left(\dfrac{x+t}{2}\right) \le f(x) + f(t)$$ for all $x,t$ prove that $f$ is zero. My try was proving first that f is nonnegative (no problem) then using the fact that $f([0,1]) = [0,M]$ try to prove that $M$ must be zero. by contradiction if I assume that $M=f(\alpha)>0$ then continuity of $f$ must be positive on a whole neighbourhood of $\alpha$ . but then I was stuck, trying to draw from here a contradiction. Any advice would be greatly appreciated.","Let a real function defined and continuous on such that for all prove that is zero. My try was proving first that f is nonnegative (no problem) then using the fact that try to prove that must be zero. by contradiction if I assume that then continuity of must be positive on a whole neighbourhood of . but then I was stuck, trying to draw from here a contradiction. Any advice would be greatly appreciated.","f [0,1] f(0)=f(1)=0  f\left(\dfrac{x+t}{2}\right) \le f(x) + f(t) x,t f f([0,1]) = [0,M] M M=f(\alpha)>0 f \alpha","['real-analysis', 'calculus', 'continuity', 'functional-equations', 'functional-inequalities']"
75,Recommended Problem books for undergraduate Real Analysis,Recommended Problem books for undergraduate Real Analysis,,"So I am taking an analysis class in my university and I want a problem book for it. The topics included in the teaching plan are Real Numbers: Introduction to the real number field, supremum, infimum, completeness axiom, basic properties of real numbers, decimal expansion, construction of real numbers. Sequences and Series: Convergence of a sequence, Cauchy sequences and subsequences, absolute and conditional convergence of an infinite series, Riemann's theorem, various tests of convergence. Point-set Topology of: : Open and closed sets; interior, boundary and closure of a set; Bolzano-Weierstrass theorem; sequential definition of compactness and the Heine-Borel theorem. Limit of a Function: Limit of a function, elementary properties of limits. Continuity: Continuous functions, elementary properties of continuous functions, intermediate value theorem, uniform continuity, properties of continuous functions defined on compact sets, set of discontinuities. I am already following up Michael J. Schramm's Introduction to Real Analysis for my theory But a problem book with varied questions on the concepts would help me a lot. Please recommend some problem books. Thanks P.S : I have already asked my professor to recommend some books but he always recommends baby Rudin and also doesn't provide a lot of assignments. I am not compatible with Rudin's book. Also his tests are very tough as he wants us to cook up counter examples and I am very poor in that. So I need a good problem book to master real analysis.","So I am taking an analysis class in my university and I want a problem book for it. The topics included in the teaching plan are Real Numbers: Introduction to the real number field, supremum, infimum, completeness axiom, basic properties of real numbers, decimal expansion, construction of real numbers. Sequences and Series: Convergence of a sequence, Cauchy sequences and subsequences, absolute and conditional convergence of an infinite series, Riemann's theorem, various tests of convergence. Point-set Topology of: : Open and closed sets; interior, boundary and closure of a set; Bolzano-Weierstrass theorem; sequential definition of compactness and the Heine-Borel theorem. Limit of a Function: Limit of a function, elementary properties of limits. Continuity: Continuous functions, elementary properties of continuous functions, intermediate value theorem, uniform continuity, properties of continuous functions defined on compact sets, set of discontinuities. I am already following up Michael J. Schramm's Introduction to Real Analysis for my theory But a problem book with varied questions on the concepts would help me a lot. Please recommend some problem books. Thanks P.S : I have already asked my professor to recommend some books but he always recommends baby Rudin and also doesn't provide a lot of assignments. I am not compatible with Rudin's book. Also his tests are very tough as he wants us to cook up counter examples and I am very poor in that. So I need a good problem book to master real analysis.",,"['real-analysis', 'reference-request', 'book-recommendation']"
76,Are all bijective functions continuous?,Are all bijective functions continuous?,,"I'm teaching my self topology with a book, and I'm trying to understand continuity better. Using the following definition, If X and Y are topological spaces, a map $f$: $X \rightarrow Y$ is said to be continuous if for every open subset $U \subseteq Y$, its preimage $f^{-1}(U)$ is open in $X$. I know the following function is not continuous: $f: \mathbb{R} \rightarrow \mathbb{R} $ $$ f(x) = \left\{         \begin{array}{ll}             2x + \frac{|x|}{x} & \quad x \neq 0 \\             0 & \quad x = 0         \end{array}     \right. $$ If we set $U=(-1/2,+\infty)$. Then $U$ are open in $\mathbb R$. But $f^{-1}(U)=[0,+\infty)$, which is not open. Thus, $f$ is not continuous. However, this function is not a bijection. If I redefine the range to make it a bijection as follows: $f: \mathbb{R} \rightarrow (-\infty, 1)\cap\{0\}\cap(1,\infty) $ Now $U$ cannot be defined the same way. Is the bijective version of $f$ now continuous? If it is, can someone please give an example of a simple 1-dimensional bijective function that is not continuous?","I'm teaching my self topology with a book, and I'm trying to understand continuity better. Using the following definition, If X and Y are topological spaces, a map $f$: $X \rightarrow Y$ is said to be continuous if for every open subset $U \subseteq Y$, its preimage $f^{-1}(U)$ is open in $X$. I know the following function is not continuous: $f: \mathbb{R} \rightarrow \mathbb{R} $ $$ f(x) = \left\{         \begin{array}{ll}             2x + \frac{|x|}{x} & \quad x \neq 0 \\             0 & \quad x = 0         \end{array}     \right. $$ If we set $U=(-1/2,+\infty)$. Then $U$ are open in $\mathbb R$. But $f^{-1}(U)=[0,+\infty)$, which is not open. Thus, $f$ is not continuous. However, this function is not a bijection. If I redefine the range to make it a bijection as follows: $f: \mathbb{R} \rightarrow (-\infty, 1)\cap\{0\}\cap(1,\infty) $ Now $U$ cannot be defined the same way. Is the bijective version of $f$ now continuous? If it is, can someone please give an example of a simple 1-dimensional bijective function that is not continuous?",,"['real-analysis', 'general-topology']"
77,"$\sum_{n=1}^\infty \frac{(-1)^n}{n}$ fails the p-series test, but passes the alternating series test?","fails the p-series test, but passes the alternating series test?",\sum_{n=1}^\infty \frac{(-1)^n}{n},"P-Series Reference Alternating Series Test Reference $$ \sum_{n=1}^\infty \frac{(-1)^n}{n} $$ This alternating series fails the p-series test because the exponent of $n$ is equal to $1$ . Yet it seems to pass the alternating series test. 1 - $a_n$ must be positive. True. 2 - Terms must be decreasing. $\frac{d}{dn} 1/n = -n^{-2}$ , which is < 1. True. 3 - $ \lim_{n\rightarrow\infty} 1/n = 0  $ True. $\frac{(-1)^n}{n}$ is clearly a divergent series, so why does it pass the AST?","P-Series Reference Alternating Series Test Reference This alternating series fails the p-series test because the exponent of is equal to . Yet it seems to pass the alternating series test. 1 - must be positive. True. 2 - Terms must be decreasing. , which is < 1. True. 3 - True. is clearly a divergent series, so why does it pass the AST?", \sum_{n=1}^\infty \frac{(-1)^n}{n}  n 1 a_n \frac{d}{dn} 1/n = -n^{-2}  \lim_{n\rightarrow\infty} 1/n = 0   \frac{(-1)^n}{n},"['real-analysis', 'sequences-and-series', 'power-series']"
78,Completeness can be omitted from Banach Fixed Point Theorem?,Completeness can be omitted from Banach Fixed Point Theorem?,,"In Kreyszig's Functional Analysis, page no. 303, exercise no. 3 says that completeness cannot be omitted from Banach's Fixed Point Theorem. But if we take $f(x)=x^2$ from an incomplete metric space $(-1/3,1/3)$ to $(-1/3,1/3)$, here $f$ is a contraction and this $f$ has a unique fixed point $0$. Here I have omitted completeness but I am still getting a unique fixed point - where am I wrong? Banach's Fixed Point Theorem: Consider a non-empty metric space $X = (X, d)$. Suppose that $X$ is complete and let $T: X \to X$ be a contraction on $X$. Then $T$ has precisely one fixed point.","In Kreyszig's Functional Analysis, page no. 303, exercise no. 3 says that completeness cannot be omitted from Banach's Fixed Point Theorem. But if we take $f(x)=x^2$ from an incomplete metric space $(-1/3,1/3)$ to $(-1/3,1/3)$, here $f$ is a contraction and this $f$ has a unique fixed point $0$. Here I have omitted completeness but I am still getting a unique fixed point - where am I wrong? Banach's Fixed Point Theorem: Consider a non-empty metric space $X = (X, d)$. Suppose that $X$ is complete and let $T: X \to X$ be a contraction on $X$. Then $T$ has precisely one fixed point.",,"['real-analysis', 'functional-analysis', 'metric-spaces', 'banach-fixed-point']"
79,Finding derivative of $\sqrt[3]{\sin(2x)}$ using only definition of derivative,Finding derivative of  using only definition of derivative,\sqrt[3]{\sin(2x)},"First post here, so hello everyone. Here's the problem: Find the first derivative of: $$\sqrt[3]{\sin(2x)}$$ But, you can only use the difference quotient...  (i.e. the limit of $\frac{f(x+h)-f(x)}{h}$ as $h \rightarrow 0$ ) This is a particular problem given by my prof. It's not for credit or even an assignment, she gave it as an interestingly difficult problem with some twist and turns. I've worked on it for several hours now and would like to see how others approach solving it as I'm wrapped around an axel after using double-angle ID and clearing the cubes in the numerator. Any advice is appreciated. Thanks.","First post here, so hello everyone. Here's the problem: Find the first derivative of: But, you can only use the difference quotient...  (i.e. the limit of as ) This is a particular problem given by my prof. It's not for credit or even an assignment, she gave it as an interestingly difficult problem with some twist and turns. I've worked on it for several hours now and would like to see how others approach solving it as I'm wrapped around an axel after using double-angle ID and clearing the cubes in the numerator. Any advice is appreciated. Thanks.",\sqrt[3]{\sin(2x)} \frac{f(x+h)-f(x)}{h} h \rightarrow 0,"['real-analysis', 'calculus', 'derivatives', 'trigonometry']"
80,When does the integral preserve strict inequalities?,When does the integral preserve strict inequalities?,,"Hi everyone: Suppose that $f(t)$ is a continuous function on $[a,b]$, where we also have  $$\alpha<f<\beta.$$  Under which condition(s) do the above strict inequalities are preserved by taking the integral:  $$\alpha<\frac{1}{b-a}\int_{a}^{b}f(t)dt<\beta?$$ Thanks for your reply.","Hi everyone: Suppose that $f(t)$ is a continuous function on $[a,b]$, where we also have  $$\alpha<f<\beta.$$  Under which condition(s) do the above strict inequalities are preserved by taking the integral:  $$\alpha<\frac{1}{b-a}\int_{a}^{b}f(t)dt<\beta?$$ Thanks for your reply.",,['real-analysis']
81,Evaluation of $\lim\limits_{x\rightarrow0} \frac{\tan(x)-x}{x^3}$,Evaluation of,\lim\limits_{x\rightarrow0} \frac{\tan(x)-x}{x^3},One of the previous posts made me think of the following question: Is it possible to evaluate this limit without L'Hopital and Taylor? $$\lim_{x\rightarrow0} \frac{\tan(x)-x}{x^3}$$,One of the previous posts made me think of the following question: Is it possible to evaluate this limit without L'Hopital and Taylor? $$\lim_{x\rightarrow0} \frac{\tan(x)-x}{x^3}$$,,"['real-analysis', 'limits', 'limits-without-lhopital']"
82,An Example of a Nested Decreasing Sequence of Bounded Closed Sets with Empty Intersection,An Example of a Nested Decreasing Sequence of Bounded Closed Sets with Empty Intersection,,Could someone provide me with an example of a metric space having a nested decreasing sequence of bounded closed sets with empty intersection? I first thought of Cantor set but the intersection is not empty!,Could someone provide me with an example of a metric space having a nested decreasing sequence of bounded closed sets with empty intersection? I first thought of Cantor set but the intersection is not empty!,,"['real-analysis', 'metric-spaces', 'examples-counterexamples']"
83,Evaluate of $\lim_{n\rightarrow \infty}\left(\frac{n+1}{n}\right)^{n^2}\cdot \frac{1}{e^n}$,Evaluate of,\lim_{n\rightarrow \infty}\left(\frac{n+1}{n}\right)^{n^2}\cdot \frac{1}{e^n},Evaluate the limit $$ \lim_{n\rightarrow \infty}\left(\frac{n+1}{n}\right)^{n^2}\cdot \frac{1}{e^n} $$ My Attempt: $$ \lim_{n\rightarrow \infty}\left(\frac{n+1}{n}\right)^{n^2}\cdot \frac{1}{e^n} = \lim_{n\rightarrow \infty}\left(1+\frac{1}{n}\right)^{n^2}\cdot \frac{1}{e^n} $$ How can I solve the problem from this point?,Evaluate the limit $$ \lim_{n\rightarrow \infty}\left(\frac{n+1}{n}\right)^{n^2}\cdot \frac{1}{e^n} $$ My Attempt: $$ \lim_{n\rightarrow \infty}\left(\frac{n+1}{n}\right)^{n^2}\cdot \frac{1}{e^n} = \lim_{n\rightarrow \infty}\left(1+\frac{1}{n}\right)^{n^2}\cdot \frac{1}{e^n} $$ How can I solve the problem from this point?,,"['calculus', 'real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
84,Is there a bijection between $\mathbb{Q}$ and $\mathbb{Q}_{>0}$?,Is there a bijection between  and ?,\mathbb{Q} \mathbb{Q}_{>0},"Is there a bijection between $\mathbb{Q}$ and $\mathbb{Q}_{>0}$? For $\mathbb{R}$, we have the exponential function. Is there also a bijection $f: \mathbb{Q} \to \mathbb{Q}_{>0}$ or to $\mathbb{Q}_{\geq 0}$?","Is there a bijection between $\mathbb{Q}$ and $\mathbb{Q}_{>0}$? For $\mathbb{R}$, we have the exponential function. Is there also a bijection $f: \mathbb{Q} \to \mathbb{Q}_{>0}$ or to $\mathbb{Q}_{\geq 0}$?",,"['real-analysis', 'elementary-set-theory', 'functions', 'rational-numbers']"
85,Lebesgue Integral but not a Riemann integral,Lebesgue Integral but not a Riemann integral,,"Is it possible for a function to be a Lebesgue integral, but not a Riemann integral? After the comments below I realize my question was not a good one. Thank you. This is my edited version: Let $f$ be an integrable function on $[a,b]$. Suppose $F(x)=F(a)+\int_a^x{f(t)}dt$. and suppose $f$ is Lebesgue integrable but not Riemann integrable. Is the following true: There is always a function $g$ which is Riemann integrable on $[a,b]$ and for which $F(x)=F(a)+\int_a^x{g(t)}dt$","Is it possible for a function to be a Lebesgue integral, but not a Riemann integral? After the comments below I realize my question was not a good one. Thank you. This is my edited version: Let $f$ be an integrable function on $[a,b]$. Suppose $F(x)=F(a)+\int_a^x{f(t)}dt$. and suppose $f$ is Lebesgue integrable but not Riemann integrable. Is the following true: There is always a function $g$ which is Riemann integrable on $[a,b]$ and for which $F(x)=F(a)+\int_a^x{g(t)}dt$",,"['real-analysis', 'measure-theory', 'lebesgue-integral']"
86,Should one  imagine diagrams/figures when working?,Should one  imagine diagrams/figures when working?,,"I'm working through Baby Rudin and find it exceedingly difficult to understand what's happening without drawing a small figure. For instance when proving properties of compactness, I would often draw figures like : (Black is the set, red are the finite sub-covers) My question is: Am I handicapping myself by continuously drawing figures (limited to $\mathbb{R}^2$ )? I am tuned to imagine things which align neatly. For instance, if someone says imagine a triangle, I imagine an equilateral one and this usually prevents me from understanding subtle points.","I'm working through Baby Rudin and find it exceedingly difficult to understand what's happening without drawing a small figure. For instance when proving properties of compactness, I would often draw figures like : (Black is the set, red are the finite sub-covers) My question is: Am I handicapping myself by continuously drawing figures (limited to )? I am tuned to imagine things which align neatly. For instance, if someone says imagine a triangle, I imagine an equilateral one and this usually prevents me from understanding subtle points.",\mathbb{R}^2,"['real-analysis', 'general-topology', 'soft-question', 'proof-writing']"
87,Compute: $\sum_{k=1}^{\infty}\sum_{n=1}^{\infty} \frac{1}{k^2n+2nk+n^2k}$,Compute:,\sum_{k=1}^{\infty}\sum_{n=1}^{\infty} \frac{1}{k^2n+2nk+n^2k},"I try to solve the following sum: $$\sum_{k=1}^{\infty}\sum_{n=1}^{\infty} \frac{1}{k^2n+2nk+n^2k}$$ I'm very curious about the possible approaching ways that lead us to solve it. I'm not experienced with these sums, and any hint, suggestion is very welcome. Thanks.","I try to solve the following sum: $$\sum_{k=1}^{\infty}\sum_{n=1}^{\infty} \frac{1}{k^2n+2nk+n^2k}$$ I'm very curious about the possible approaching ways that lead us to solve it. I'm not experienced with these sums, and any hint, suggestion is very welcome. Thanks.",,"['real-analysis', 'sequences-and-series', 'limits']"
88,Existence of an injective continuous function $\Bbb R^2\to\Bbb R$?,Existence of an injective continuous function ?,\Bbb R^2\to\Bbb R,"Let's say $f(x,y)$ is a continuous function. $x$ and $y$ can be any real numbers. Can this function have one unique value for any two different pairs of variables? In other words can $f(a,b) \neq f(c,d)$ for any $a$, $b$, $c$, and $d$ such that $a \neq c$ or $b \neq d$? I don't think there can at least not if the range of $f$ is within the real numbers. Could someone please offer a more formal proof of this or at least start me off in the right direction.","Let's say $f(x,y)$ is a continuous function. $x$ and $y$ can be any real numbers. Can this function have one unique value for any two different pairs of variables? In other words can $f(a,b) \neq f(c,d)$ for any $a$, $b$, $c$, and $d$ such that $a \neq c$ or $b \neq d$? I don't think there can at least not if the range of $f$ is within the real numbers. Could someone please offer a more formal proof of this or at least start me off in the right direction.",,"['real-analysis', 'functions']"
89,"Holder Continuous Functions on $[0,1]$ are complete + Banach space",Holder Continuous Functions on  are complete + Banach space,"[0,1]","I am studying for an Analysis prelim and I am stuck on how to show that the following space of Holder continuous functions is complete: $$\Lambda_{\alpha}([0,1]) = \{f: [0,1] \rightarrow \mathbb{R} \mid \sup_{x,y \in [0,1], x\neq y} \frac{|f(x) - f(y)|}{|x-y|^{\alpha}} < \infty \text{ and } 0< \alpha \leq 1\}$$ with the norm $\| \cdot \|_{\Lambda_{\alpha}}$ defined by  $$\|f\|_{\Lambda_{\alpha}} = |f(0)| + \sup_{x,y \in [0,1], x\neq y} \frac{|f(x) - f(y)|}{|x-y|^{\alpha}}$$ I think its easy to see that $\Lambda_{\alpha}([0,1]) \subset B([0,1])$, i.e., it is a subset of bounded functions on $[0,1]$ which is complete.  My thought is that if I let $\{f_n\}$ be a Cauchy sequence in $\Lambda_{\alpha}([0,1])$ and let $f \in B([0,1])$, such that $f_n \rightarrow f$ in the uniform norm, then we can show $f \in \Lambda_{\alpha}([0,1])$ and that $f_n \rightarrow f$ in the Holder norm defined above.  Not sure how to show this though.  Perhaps I could also somehow show that any absolutely convergent series converges in $\Lambda_\alpha$, but I'm not sure how to do this either.  Any help would be appreciated.","I am studying for an Analysis prelim and I am stuck on how to show that the following space of Holder continuous functions is complete: $$\Lambda_{\alpha}([0,1]) = \{f: [0,1] \rightarrow \mathbb{R} \mid \sup_{x,y \in [0,1], x\neq y} \frac{|f(x) - f(y)|}{|x-y|^{\alpha}} < \infty \text{ and } 0< \alpha \leq 1\}$$ with the norm $\| \cdot \|_{\Lambda_{\alpha}}$ defined by  $$\|f\|_{\Lambda_{\alpha}} = |f(0)| + \sup_{x,y \in [0,1], x\neq y} \frac{|f(x) - f(y)|}{|x-y|^{\alpha}}$$ I think its easy to see that $\Lambda_{\alpha}([0,1]) \subset B([0,1])$, i.e., it is a subset of bounded functions on $[0,1]$ which is complete.  My thought is that if I let $\{f_n\}$ be a Cauchy sequence in $\Lambda_{\alpha}([0,1])$ and let $f \in B([0,1])$, such that $f_n \rightarrow f$ in the uniform norm, then we can show $f \in \Lambda_{\alpha}([0,1])$ and that $f_n \rightarrow f$ in the Holder norm defined above.  Not sure how to show this though.  Perhaps I could also somehow show that any absolutely convergent series converges in $\Lambda_\alpha$, but I'm not sure how to do this either.  Any help would be appreciated.",,"['real-analysis', 'functional-analysis']"
90,Closure of union of two sets,Closure of union of two sets,,"Show that $$\overline{A\cup B} = \overline A\cup\overline B$$ $\overline A=A\cup A'$ where $A'$ are the limit points My attempt: Since $\overline A$ is closed and $\overline B$, the union of two closed sets is closed. Hence $\overline A\cup \overline B$ can be rewritten as $\overline{A\cup B}$ My Second attempt: $\overline{A\cup B}=(A\cup B)\cup (A\cup B)'=(A\cup B)\cup (A'\cup B')=A\cup A' \cup B \cup B' = \overline{A}\cup \overline{B}$","Show that $$\overline{A\cup B} = \overline A\cup\overline B$$ $\overline A=A\cup A'$ where $A'$ are the limit points My attempt: Since $\overline A$ is closed and $\overline B$, the union of two closed sets is closed. Hence $\overline A\cup \overline B$ can be rewritten as $\overline{A\cup B}$ My Second attempt: $\overline{A\cup B}=(A\cup B)\cup (A\cup B)'=(A\cup B)\cup (A'\cup B')=A\cup A' \cup B \cup B' = \overline{A}\cup \overline{B}$",,"['real-analysis', 'general-topology']"
91,"For set of positive measure $E$, $\alpha \in (0, 1)$, there is interval $I$ such that $m(E \cap I) > \alpha \, m(I)$","For set of positive measure , , there is interval  such that","E \alpha \in (0, 1) I m(E \cap I) > \alpha \, m(I)","I am a graduate student at Iowa State University attempting to return after a five-year hiatus and take the Real/Complex Analysis qualifier on January 8 for potential reinstatement. Since the professors here are on hiatus, I hope you guys don't mind if I bombard the board with a few past questions from our quals here, like this one from Spring 2013: Let $m$ denote Lebesgue measure on $\mathbb R$ and $M$ the $\sigma$-algebra of Lebesgue measurable subsets. Given $E \in M$ with $m(E) > 0$ and $\alpha \in (0, 1)$, prove that there is an interval $I$ such that $m(E \cap I) > \alpha \, m(I)$. I've attempted to force the issue using the definition of outer measure but it doesn't seem to work. I've also attempted using the definition of Lebesgue measurability but that doesn't seem to work either - so I'm at a loss! Thanks in advance! -Darrin Rasberry","I am a graduate student at Iowa State University attempting to return after a five-year hiatus and take the Real/Complex Analysis qualifier on January 8 for potential reinstatement. Since the professors here are on hiatus, I hope you guys don't mind if I bombard the board with a few past questions from our quals here, like this one from Spring 2013: Let $m$ denote Lebesgue measure on $\mathbb R$ and $M$ the $\sigma$-algebra of Lebesgue measurable subsets. Given $E \in M$ with $m(E) > 0$ and $\alpha \in (0, 1)$, prove that there is an interval $I$ such that $m(E \cap I) > \alpha \, m(I)$. I've attempted to force the issue using the definition of outer measure but it doesn't seem to work. I've also attempted using the definition of Lebesgue measurability but that doesn't seem to work either - so I'm at a loss! Thanks in advance! -Darrin Rasberry",,"['real-analysis', 'measure-theory']"
92,Stuck at proving whether the sequence is convergent or not,Stuck at proving whether the sequence is convergent or not,,"I have been trying to determine whether the following sequence is convergent or not. This is what I got: Exercise 1 : Find the $\min,\max,\sup,\inf, \liminf,\limsup$ and determine whether the sequence is convergent or not: $X_n=\sin\frac{n\pi}{3}-4\cos\frac{n\pi}{3}$ I wrote down a few cases: $X_1 = \sin\frac{\pi}{3}-4\cos\frac{\pi}{3}= \frac{\sqrt3-4}{2} $ $X_2 = \sin\frac{2\pi}{3}-4\cos\frac{2\pi}{3}= \frac{\sqrt3+4}{2} $ $X_3 = \sin\frac{3\pi}{3}-4\cos\frac{3\pi}{3}=4 $ $X_4 = \sin\frac{4\pi}{3}-4\cos\frac{4\pi}{3}= \frac{4-\sqrt3}{2} $ $X_5 = \sin\frac{5\pi}{3}-4\cos\frac{5\pi}{3}= \frac{4-\sqrt3}{2} $ $X_6 = \sin\frac{6\pi}{3}-4\cos\frac{6\pi}{3}= -4 $ So as found above, $\min = -4$ , $\max = 4$ , $\inf = -4$ , $\sup = 4$ , $\liminf = -4$ , $\limsup = 4$ . Let's check whether its convergent or not: The sequence is bounded as stated above so lets check if its decreasing or increasing. $X_n \geq X_{n+1}$ $\sin\frac{n\pi}{3}-4\cos\frac{n\pi}{3} \geq\sin\frac{(n+1)\pi}{3}-4\cos\frac{(n+1)\pi}{3}$ $\sin\frac{n\pi}{3} - \sin\frac{(n+1)\pi}{3} \geq  -4\cos\frac{(n+1)\pi}{3} + 4\cos\frac{n\pi}{3}$ I used trigonometrical identity for $\sin\alpha+\sin\beta$ and $\cos\alpha-\cos\beta$ : $-\cos\frac{\pi(2n+1)}{6} \geq 4\sin\frac{\pi(2n+1)}{6}$ What should I do next? I am stuck here. Thanks, and sorry if I made mistakes.","I have been trying to determine whether the following sequence is convergent or not. This is what I got: Exercise 1 : Find the and determine whether the sequence is convergent or not: I wrote down a few cases: So as found above, , , , , , . Let's check whether its convergent or not: The sequence is bounded as stated above so lets check if its decreasing or increasing. I used trigonometrical identity for and : What should I do next? I am stuck here. Thanks, and sorry if I made mistakes.","\min,\max,\sup,\inf, \liminf,\limsup X_n=\sin\frac{n\pi}{3}-4\cos\frac{n\pi}{3} X_1 = \sin\frac{\pi}{3}-4\cos\frac{\pi}{3}= \frac{\sqrt3-4}{2}  X_2 = \sin\frac{2\pi}{3}-4\cos\frac{2\pi}{3}= \frac{\sqrt3+4}{2}  X_3 = \sin\frac{3\pi}{3}-4\cos\frac{3\pi}{3}=4  X_4 = \sin\frac{4\pi}{3}-4\cos\frac{4\pi}{3}= \frac{4-\sqrt3}{2}  X_5 = \sin\frac{5\pi}{3}-4\cos\frac{5\pi}{3}= \frac{4-\sqrt3}{2}  X_6 = \sin\frac{6\pi}{3}-4\cos\frac{6\pi}{3}= -4  \min = -4 \max = 4 \inf = -4 \sup = 4 \liminf = -4 \limsup = 4 X_n \geq X_{n+1} \sin\frac{n\pi}{3}-4\cos\frac{n\pi}{3} \geq\sin\frac{(n+1)\pi}{3}-4\cos\frac{(n+1)\pi}{3} \sin\frac{n\pi}{3} - \sin\frac{(n+1)\pi}{3} \geq  -4\cos\frac{(n+1)\pi}{3} + 4\cos\frac{n\pi}{3} \sin\alpha+\sin\beta \cos\alpha-\cos\beta -\cos\frac{\pi(2n+1)}{6} \geq 4\sin\frac{\pi(2n+1)}{6}","['real-analysis', 'sequences-and-series', 'trigonometry', 'convergence-divergence', 'limsup-and-liminf']"
93,Is the set of rational number discrete or continuous?,Is the set of rational number discrete or continuous?,,"If the set of real numbers $\Bbb{R}$ is continuous, and the set of integer $\Bbb{Z}$ is a discrete set, then is the set of rational number $\Bbb{Q}$ continuous or discrete? My question is stated in the context of analysis. Sorry if I can’t state my problem clear, this question just passed my mind. If it is just a nonsense question, please tell me right away. Thank you very much.","If the set of real numbers $\Bbb{R}$ is continuous, and the set of integer $\Bbb{Z}$ is a discrete set, then is the set of rational number $\Bbb{Q}$ continuous or discrete? My question is stated in the context of analysis. Sorry if I can’t state my problem clear, this question just passed my mind. If it is just a nonsense question, please tell me right away. Thank you very much.",,"['real-analysis', 'general-topology']"
94,How does a mathematician create a new zeta function?,How does a mathematician create a new zeta function?,,"Imagine a mathematician wants to create this function: $$F(s)=\sum_{n=0}^{\infty} \frac{1}{(n^2+1)^s}$$ Where would he start? More precisely, how would he know the exact values of $F(s)$ for $s> 0$? And how would he extend this result for every natural $s$? And for real $s$?","Imagine a mathematician wants to create this function: $$F(s)=\sum_{n=0}^{\infty} \frac{1}{(n^2+1)^s}$$ Where would he start? More precisely, how would he know the exact values of $F(s)$ for $s> 0$? And how would he extend this result for every natural $s$? And for real $s$?",,"['real-analysis', 'sequences-and-series', 'complex-analysis', 'zeta-functions']"
95,Why Zariski topology is not Hausdorff,Why Zariski topology is not Hausdorff,,"I am reading the book about Algebraic geometry. I am confused about the following  two things the book mentioned: Zariski topology is 1. different from the topology studied in real and complex analysis. 2. not Hausdorff. Well, I roughly now about Hausdorff and some def. in real and complex analysis. However, I cannot know why still. A hint or direction for thinking is ok. Thanks,","I am reading the book about Algebraic geometry. I am confused about the following  two things the book mentioned: Zariski topology is 1. different from the topology studied in real and complex analysis. 2. not Hausdorff. Well, I roughly now about Hausdorff and some def. in real and complex analysis. However, I cannot know why still. A hint or direction for thinking is ok. Thanks,",,"['real-analysis', 'algebraic-geometry']"
96,"Prove $\int_0^1\frac{\ln2-\ln\left(1+x^2\right)}{1-x}\,dx=\frac{5\pi^2}{48}-\frac{\ln^22}{4}$",Prove,"\int_0^1\frac{\ln2-\ln\left(1+x^2\right)}{1-x}\,dx=\frac{5\pi^2}{48}-\frac{\ln^22}{4}","How does one prove the following integral \begin{equation} \int_0^1\frac{\ln2-\ln\left(1+x^2\right)}{1-x}\,dx=\frac{5\pi^2}{48}-\frac{\ln^22}{4}  \end{equation} Wolfram Alpha and Mathematica can easily evaluate this integral. This integral came up in the process of finding the solution this question: Evaluating $\displaystyle\sum_{n=1}^{\infty} (-1)^{n-1}\frac{H_{2n}}{n}$ . There are some good answers there but avoiding this approach. I have been waiting for a day hoping an answers would be posted using this approach, but nothing shows up. The integral cannot be evaluated separately since each terms doesn't converge. I tried integration by parts but the problem arises when substituting the bounds of integration. I would appreciate if anyone here could provide an answer where its approach using integral only preferably with elementary ways .","How does one prove the following integral \begin{equation} \int_0^1\frac{\ln2-\ln\left(1+x^2\right)}{1-x}\,dx=\frac{5\pi^2}{48}-\frac{\ln^22}{4}  \end{equation} Wolfram Alpha and Mathematica can easily evaluate this integral. This integral came up in the process of finding the solution this question: Evaluating $\displaystyle\sum_{n=1}^{\infty} (-1)^{n-1}\frac{H_{2n}}{n}$ . There are some good answers there but avoiding this approach. I have been waiting for a day hoping an answers would be posted using this approach, but nothing shows up. The integral cannot be evaluated separately since each terms doesn't converge. I tried integration by parts but the problem arises when substituting the bounds of integration. I would appreciate if anyone here could provide an answer where its approach using integral only preferably with elementary ways .",,"['real-analysis', 'calculus', 'integration', 'definite-integrals', 'improper-integrals']"
97,"Why is the outer measure of the set of irrational numbers in the interval [0,1] equal to 1?","Why is the outer measure of the set of irrational numbers in the interval [0,1] equal to 1?",,"Just learned Lebesgue outer measure from Royden's Real Analysis. Let me give my proof. First, let $A$ be the set of irrational numbers in [0,1]. So $A\subset [0,1]\Rightarrow m^*(A)\le m^*([0,1])=1$. Then I want to show $m^*(A)\ge 1$ by using $\sum_{k=1}^\infty l(I_k)\le m^*(A)+\epsilon$. $\{I_k\}_k$ covers $A$, then add $I_0$ to this collection. $[0,1]\subset I_0$. So $l(I_0)+\sum_{k=1}^\infty l(I_k)\le m^*(A)+\epsilon\Rightarrow m^*(A)\ge l(I_0)+\sum_{k=1}^\infty l(I_k)-\epsilon\ge 1+\sum_{k=1}^\infty l(I_k)-\epsilon$ We can always choose a small enough $\epsilon>0$ such that $\sum_{k=1}^\infty l(I_k)-\epsilon>0$. Therefore, $m^*(A)=1$.","Just learned Lebesgue outer measure from Royden's Real Analysis. Let me give my proof. First, let $A$ be the set of irrational numbers in [0,1]. So $A\subset [0,1]\Rightarrow m^*(A)\le m^*([0,1])=1$. Then I want to show $m^*(A)\ge 1$ by using $\sum_{k=1}^\infty l(I_k)\le m^*(A)+\epsilon$. $\{I_k\}_k$ covers $A$, then add $I_0$ to this collection. $[0,1]\subset I_0$. So $l(I_0)+\sum_{k=1}^\infty l(I_k)\le m^*(A)+\epsilon\Rightarrow m^*(A)\ge l(I_0)+\sum_{k=1}^\infty l(I_k)-\epsilon\ge 1+\sum_{k=1}^\infty l(I_k)-\epsilon$ We can always choose a small enough $\epsilon>0$ such that $\sum_{k=1}^\infty l(I_k)-\epsilon>0$. Therefore, $m^*(A)=1$.",,"['real-analysis', 'analysis', 'measure-theory', 'proof-verification', 'lebesgue-measure']"
98,Cantor Set and ternary expansions.,Cantor Set and ternary expansions.,,"Does there exists any systematic way to represent a number in ternary expansion? Also, I have hard time trying to figure it out this: Let $x = \sum \frac{a_k}{3^k} = 0.a_1a_2.... $ why is it that  $x \in [0,1]$ belongs to the cantor set $C$ iff all its $a_k$ equal $0$ or $2$? I would really appreciate if someone can explain this to me. Thanks","Does there exists any systematic way to represent a number in ternary expansion? Also, I have hard time trying to figure it out this: Let $x = \sum \frac{a_k}{3^k} = 0.a_1a_2.... $ why is it that  $x \in [0,1]$ belongs to the cantor set $C$ iff all its $a_k$ equal $0$ or $2$? I would really appreciate if someone can explain this to me. Thanks",,"['calculus', 'real-analysis', 'elementary-set-theory']"
99,"If $\{f_n\}$ is a measurable sequence of functions, then $\{x : \lim f_n(x) $ exists $\}$ is measurable","If  is a measurable sequence of functions, then  exists  is measurable",\{f_n\} \{x : \lim f_n(x)  \},"Was hoping someone could help me out on this problem (self-study not hw). If $\{f_n\}$ is a sequence of measurable functions on $X$, then $\{x : \lim f_n(x) $ exists $\}$ is a measurable function. Thank you","Was hoping someone could help me out on this problem (self-study not hw). If $\{f_n\}$ is a sequence of measurable functions on $X$, then $\{x : \lim f_n(x) $ exists $\}$ is a measurable function. Thank you",,"['real-analysis', 'measure-theory']"
