,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Find $\mathbb P(\sqrt{V} \cos(\pi U)\leq c)$,Find,\mathbb P(\sqrt{V} \cos(\pi U)\leq c),"Find $$IC=\mathbb P(\sqrt{V} \cos(\pi U)\leq c),$$ where $V\sim \Gamma(\alpha , \lambda)$ and $U\sim Beta(a,b)$ , that is, $f_V(v)=\frac{1}{\Gamma(\alpha) \lambda^\alpha} v^{\alpha-1} e^{-\frac{v}{\lambda}}1_{v>0}$ and $f_U(u)=\frac{1}{Beta(a,b)}u^{a-1}(1-u)^{b-1}1_{(0,1)}(u)$ . $V$ and $U$ are independent. 1) For a special case $a=b=1$ , $\alpha=1$ and $\lambda=2$ the distribution of $\sqrt{V} \cos (\pi U)$ is standard normal. 2) For a special case $a=b=1$ .  Let $c>0$ \begin{eqnarray} IC &=& \mathbb P(\sqrt{V} \cos(\pi U)\leq c)  \\ &=&  \int_0^{1} \mathbb P(\sqrt{V} \cos(\pi u)\leq c) f_U(u) du \\ &=&  \int_0^{0.5} \mathbb P(\sqrt{V} \cos(\pi u)\leq c)  du +\int_{0.5}^{1} \mathbb P(\sqrt{V} \cos(\pi u)\leq c)  du \\ &=&  \int_0^{0.5} \mathbb P(\sqrt{V} \cos(\pi u)\leq c) du +\int_{0.5}^{1} \mathbb 1 \quad  du \\ &=&  \int_0^{0.5} \mathbb P(\sqrt{V} \leq \frac{c}{\cos(\pi u)}) du +\frac{1}{2} \\ &=&  \int_0^{0.5} \frac{\gamma \left(\alpha , \frac{c^2}{\lambda^2 \cos^2(\pi u)} \right)}{\Gamma(\alpha)} du +\frac{1}{2} \tag{2} \end{eqnarray} Where $\gamma$ is Incomplete_gamma_function . For $c<0$ the solution is similar. Is it possible to simplify (2) or have a better solution ? According to square-root-of-a-gamma-distribution , the distribution of $\sqrt{V}$ is Nakagami_distribution Any  special case is also useful. Thanks in advance for any help you are able to provide.","Find where and , that is, and . and are independent. 1) For a special case , and the distribution of is standard normal. 2) For a special case .  Let Where is Incomplete_gamma_function . For the solution is similar. Is it possible to simplify (2) or have a better solution ? According to square-root-of-a-gamma-distribution , the distribution of is Nakagami_distribution Any  special case is also useful. Thanks in advance for any help you are able to provide.","IC=\mathbb P(\sqrt{V} \cos(\pi U)\leq c), V\sim \Gamma(\alpha , \lambda) U\sim Beta(a,b) f_V(v)=\frac{1}{\Gamma(\alpha) \lambda^\alpha} v^{\alpha-1} e^{-\frac{v}{\lambda}}1_{v>0} f_U(u)=\frac{1}{Beta(a,b)}u^{a-1}(1-u)^{b-1}1_{(0,1)}(u) V U a=b=1 \alpha=1 \lambda=2 \sqrt{V} \cos (\pi U) a=b=1 c>0 \begin{eqnarray}
IC &=& \mathbb P(\sqrt{V} \cos(\pi U)\leq c) 
\\ &=&  \int_0^{1} \mathbb P(\sqrt{V} \cos(\pi u)\leq c) f_U(u) du
\\ &=&  \int_0^{0.5} \mathbb P(\sqrt{V} \cos(\pi u)\leq c)  du
+\int_{0.5}^{1} \mathbb P(\sqrt{V} \cos(\pi u)\leq c)  du
\\ &=&  \int_0^{0.5} \mathbb P(\sqrt{V} \cos(\pi u)\leq c) du
+\int_{0.5}^{1} \mathbb 1 \quad  du
\\ &=&  \int_0^{0.5} \mathbb P(\sqrt{V} \leq \frac{c}{\cos(\pi u)}) du +\frac{1}{2}
\\ &=&  \int_0^{0.5} \frac{\gamma \left(\alpha , \frac{c^2}{\lambda^2 \cos^2(\pi u)} \right)}{\Gamma(\alpha)} du +\frac{1}{2} \tag{2}
\end{eqnarray} \gamma c<0 \sqrt{V}","['probability', 'definite-integrals']"
1,Drunk man at the cliff problem: expected reward while staying alive? & unequal step size,Drunk man at the cliff problem: expected reward while staying alive? & unequal step size,,"There are several variations of this Drunk man (or a monkey) at the cliff problem. I wonder what will be the expected sum of rewards if the drunk man gets paid while he stays alive. The setup is like this. At period zero, a man is standing at $n$ step from the edge of a cliff. In each period from period $1$ , he takes independent steps; $k_1$ steps forward with probability $p$ , $k_2$ steps backward with probability $q$ , and he doesn't move with probability $1-p-q$ . We assume $k_1,k_2>0$ . So, in each period, the random walk takes value of $-k_1$ w/p $p$ , $k_2$ w/p $q$ and $0$ w/p $1-p-q$ . At the end of each period, if he's still alive, he gets paid \$1 as a reward. The future payoff is discounted by $\delta$ . So, if he is expected to fall off the cliff after 3 periods, his sum of expected rewards is $$1+\delta+\delta^2.$$ My question is what is the sum of the expected rewards? There are some relevant questions and answers. Here Q1 , Q2 , Q3 ,... but because of the discount factor and the unequal step sizes, I can't figure out how to apply the identity... Any other approach or an answer?","There are several variations of this Drunk man (or a monkey) at the cliff problem. I wonder what will be the expected sum of rewards if the drunk man gets paid while he stays alive. The setup is like this. At period zero, a man is standing at step from the edge of a cliff. In each period from period , he takes independent steps; steps forward with probability , steps backward with probability , and he doesn't move with probability . We assume . So, in each period, the random walk takes value of w/p , w/p and w/p . At the end of each period, if he's still alive, he gets paid \$1 as a reward. The future payoff is discounted by . So, if he is expected to fall off the cliff after 3 periods, his sum of expected rewards is My question is what is the sum of the expected rewards? There are some relevant questions and answers. Here Q1 , Q2 , Q3 ,... but because of the discount factor and the unequal step sizes, I can't figure out how to apply the identity... Any other approach or an answer?","n 1 k_1 p k_2 q 1-p-q k_1,k_2>0 -k_1 p k_2 q 0 1-p-q \delta 1+\delta+\delta^2.","['probability', 'probability-distributions', 'markov-chains', 'markov-process', 'random-walk']"
2,"In a queue for £1 tickets, there are $m$ people with a £1 coin and $n$ people with a £2 coin. What is the probability that everyone receives change?","In a queue for £1 tickets, there are  people with a £1 coin and  people with a £2 coin. What is the probability that everyone receives change?",m n,"I am selling raffle tickets for £1 per ticket. In the queue for tickets, there are $m$ people each with a single £1 coin and $n$ people each with a single £2 coin. Each person in the queue wants to buy a single raffle ticket and each arrangement of people in the queue is equally likely to occur. Initially, I have no coins and a large supply of tickets. I stop selling tickets if I cannot give the required change. Show that the probability that I am able to serve everyone in the queue is $\frac{m+1-n}{m+1}$ This problem comes from a STEP question ( see Q3 here ) where the solution is shown in the cases $n=1,2$ or $3$ . However they involve conditioning on permutations of the first couple of people in a way that I don't see how to generalise.","I am selling raffle tickets for £1 per ticket. In the queue for tickets, there are people each with a single £1 coin and people each with a single £2 coin. Each person in the queue wants to buy a single raffle ticket and each arrangement of people in the queue is equally likely to occur. Initially, I have no coins and a large supply of tickets. I stop selling tickets if I cannot give the required change. Show that the probability that I am able to serve everyone in the queue is This problem comes from a STEP question ( see Q3 here ) where the solution is shown in the cases or . However they involve conditioning on permutations of the first couple of people in a way that I don't see how to generalise.","m n \frac{m+1-n}{m+1} n=1,2 3",['probability']
3,"Expectation of $\frac{\langle Gx, Gy\rangle}{\|Gx\|_2\|G y\|_2}$ where $\langle x, y\rangle = \rho$",Expectation of  where,"\frac{\langle Gx, Gy\rangle}{\|Gx\|_2\|G y\|_2} \langle x, y\rangle = \rho","Let $x$ and $y$ by unitvectors $\in\mathbb R^d$ , such that $\|x\|_2=\|y\|_2=1$ and $\langle x,y\rangle=\rho\in[-1,1]$ . Let $G\in\mathbb R^{k\times d}$ be a matrix with independent Gaussian entries. I'm interested in the mean and variance of $$\rho'=\frac{\langle Gx, Gy\rangle}{\|Gx\|_2\|G y\|_2}.$$ Another way to state this is that $Gx,Gy\in\mathbb R^k$ are Gaussian vectors with covariance matrix $\big(\begin{smallmatrix}1&\rho\\\rho&1\end{smallmatrix}\big)$ . Thus $E[\langle Gx, Gy\rangle]=k \rho$ and $E[\|Gx\|_2^2]=E[\|Gy\|_2^2]=k$ . From this we would expect $E[\rho']\approx\rho$ . I can make this more precise using the Gaussian Johnson Lindenstrauss lemma, which says that if $k=\varepsilon^{-2}\log1/\delta$ , then $\langle Gx, Gy\rangle=\rho(1\pm\varepsilon)\|x\|_2\|y\|_2$ with probability at least $1-\delta$ . We can get similar bounds for $\|Gx\|_2^2=\|x\|_2^2(1\pm\varepsilon)$ and for $y$ . By Cauchy Schwartz we always have $\rho'\in[-1,1]$ , so we get roughly $$ E[\rho'] = \rho\,(1\pm O(1/\sqrt k)). $$ However, if I series expand $\rho'$ at $\rho=0$ I get $$ \begin{align} E[\rho'] &= E\left[\frac{g}{\sqrt{\chi^2_{k-1} + g^2}} + \frac{\chi_{k} \chi^2_{k-1} }{(\chi^2_{k-1} +g^2)^{3/2}}\rho +O(\rho^2)\right] \\&= \frac{\Gamma(k/2+1/2)^2}{\Gamma(k/2+1)\Gamma(k/2)}\rho +O(\rho^2) \\&= \rho(1 + 1/(2k) + O(1/k^2)) +O(\rho^2), \end{align} $$ where $\chi_k^2$ and $\chi_{k-1}^2$ are independent Chi-Square distributed random variables, and $g$ is a standard Gaussian. At the same time it seems $f(\rho)=E[\rho']$ for $\rho\ge 0$ is a convex function, that $f(\rho)\le\rho$ , and that $f(1)=1$ . Hence we must have $f(\rho)\ge \rho(1-1/(2k))$ and $$E[\rho'] = \rho(1+O(1/k)).$$ This is a quadratic improvement over the previous bound. I would love to have an intuition and a  more rigorous proof. I also wonder what the variance of $\rho'$ is.","Let and by unitvectors , such that and . Let be a matrix with independent Gaussian entries. I'm interested in the mean and variance of Another way to state this is that are Gaussian vectors with covariance matrix . Thus and . From this we would expect . I can make this more precise using the Gaussian Johnson Lindenstrauss lemma, which says that if , then with probability at least . We can get similar bounds for and for . By Cauchy Schwartz we always have , so we get roughly However, if I series expand at I get where and are independent Chi-Square distributed random variables, and is a standard Gaussian. At the same time it seems for is a convex function, that , and that . Hence we must have and This is a quadratic improvement over the previous bound. I would love to have an intuition and a  more rigorous proof. I also wonder what the variance of is.","x y \in\mathbb R^d \|x\|_2=\|y\|_2=1 \langle x,y\rangle=\rho\in[-1,1] G\in\mathbb R^{k\times d} \rho'=\frac{\langle Gx, Gy\rangle}{\|Gx\|_2\|G y\|_2}. Gx,Gy\in\mathbb R^k \big(\begin{smallmatrix}1&\rho\\\rho&1\end{smallmatrix}\big) E[\langle Gx, Gy\rangle]=k \rho E[\|Gx\|_2^2]=E[\|Gy\|_2^2]=k E[\rho']\approx\rho k=\varepsilon^{-2}\log1/\delta \langle Gx, Gy\rangle=\rho(1\pm\varepsilon)\|x\|_2\|y\|_2 1-\delta \|Gx\|_2^2=\|x\|_2^2(1\pm\varepsilon) y \rho'\in[-1,1] 
E[\rho'] = \rho\,(1\pm O(1/\sqrt k)).
 \rho' \rho=0 
\begin{align}
E[\rho']
&= E\left[\frac{g}{\sqrt{\chi^2_{k-1} + g^2}} + \frac{\chi_{k} \chi^2_{k-1} }{(\chi^2_{k-1} +g^2)^{3/2}}\rho
+O(\rho^2)\right]
\\&= \frac{\Gamma(k/2+1/2)^2}{\Gamma(k/2+1)\Gamma(k/2)}\rho
+O(\rho^2)
\\&= \rho(1 + 1/(2k) + O(1/k^2)) +O(\rho^2),
\end{align}
 \chi_k^2 \chi_{k-1}^2 g f(\rho)=E[\rho'] \rho\ge 0 f(\rho)\le\rho f(1)=1 f(\rho)\ge \rho(1-1/(2k)) E[\rho'] = \rho(1+O(1/k)). \rho'","['linear-algebra', 'probability', 'geometry', 'random-matrices']"
4,"Throw a coin $10$ times without knowing the mass distribution and get $10$ heads and $0$ tail, what is the probability of head in the $11$th time?","Throw a coin  times without knowing the mass distribution and get  heads and  tail, what is the probability of head in the th time?",10 10 0 11,"Throw coin A $10$ times without knowing the mass distribution and get $10$ heads and $0$ tails, what is the probability of facing up for the $11$ th time? Throw coin B $100$ times without knowing the mass distribution and get $99$ heads and $1$ tails, what is the probability of facing up for the $101$ th time? Which coin is more likely to face up in the next toss? I already know that parameter estimation methods such as maximum likelihood estimation can be used to estimate the most likely mass distribution of this coin; I already know that Laplace smoothing can help me better compare the difference between Coin A and Coin B; But how should we calculate their probability of heading up in the next toss? I just want a percentage. Thanks for your help.","Throw coin A times without knowing the mass distribution and get heads and tails, what is the probability of facing up for the th time? Throw coin B times without knowing the mass distribution and get heads and tails, what is the probability of facing up for the th time? Which coin is more likely to face up in the next toss? I already know that parameter estimation methods such as maximum likelihood estimation can be used to estimate the most likely mass distribution of this coin; I already know that Laplace smoothing can help me better compare the difference between Coin A and Coin B; But how should we calculate their probability of heading up in the next toss? I just want a percentage. Thanks for your help.",10 10 0 11 100 99 1 101,"['probability', 'statistics', 'probability-distributions']"
5,Model Epidemic Random Graph,Model Epidemic Random Graph,,"I'm reading on random graphs and understand that they can be used to model disease spread (seems particularly germane at the moment). The papers I've found so far are focused on quite complex models. I'm wondering if someone could point me to explanation of or explain how a random graph could be used to model a disease with a simple set of assumptions. For example, Uniform infection transfer from an infected individual to others. People can only be infected once. No countermeasures. I'm trying to understand what exactly it means to model an epidemic with a random graph but am struggling with the complexity of the examples I've found so far. Thank you.","I'm reading on random graphs and understand that they can be used to model disease spread (seems particularly germane at the moment). The papers I've found so far are focused on quite complex models. I'm wondering if someone could point me to explanation of or explain how a random graph could be used to model a disease with a simple set of assumptions. For example, Uniform infection transfer from an infected individual to others. People can only be infected once. No countermeasures. I'm trying to understand what exactly it means to model an epidemic with a random graph but am struggling with the complexity of the examples I've found so far. Thank you.",,"['probability', 'graph-theory', 'mathematical-modeling', 'random-graphs']"
6,"Expectation of a binomial random variable raised to a non-integer power? (""non-integer moment"" of a probability distribution)","Expectation of a binomial random variable raised to a non-integer power? (""non-integer moment"" of a probability distribution)",,"Does anyone know if there's a closed-form expression of the expectation of the maximum likelihood estimator of a binomial variable raised to a non-integer power? Concretely, setting $$ P(m\mid N,p) = \binom{N}{m} p^m (1-p)^{N-m},\\ \hat{p} = \frac{m}{N}. $$ What is $E[\hat{p}^w]$ , where $0<w<1$ ? Specifically $$ E[\hat{p}^w]  = \sum_{m=0}^N \binom{N}{m} p^m (1-p)^{N-m} \left(\frac{m}{N}\right)^w =~? $$ If $w$ was integer we could use the moment-generating function. Is there an analogous function that generates the ""non-integer moments"" of probability distributions?","Does anyone know if there's a closed-form expression of the expectation of the maximum likelihood estimator of a binomial variable raised to a non-integer power? Concretely, setting What is , where ? Specifically If was integer we could use the moment-generating function. Is there an analogous function that generates the ""non-integer moments"" of probability distributions?","
P(m\mid N,p) = \binom{N}{m} p^m (1-p)^{N-m},\\
\hat{p} = \frac{m}{N}.
 E[\hat{p}^w] 0<w<1 
E[\hat{p}^w]
 = \sum_{m=0}^N \binom{N}{m} p^m (1-p)^{N-m} \left(\frac{m}{N}\right)^w =~?
 w","['probability', 'probability-distributions', 'expected-value', 'binomial-distribution', 'moment-generating-functions']"
7,Random walk- minimizing expected distance to the origin,Random walk- minimizing expected distance to the origin,,"Given $\delta\in [0,1]$ and $n\in \mathbb{N}$ , consider a (biased) random walk $S_n(\delta) = \sum_{i = 1}^n X_i$ where $\{X_i:1\le i\le n\}$ are i.i.d. and $X_i = 1$ with probabiltiy $(1+\delta)/2$ and $-1$ otherwise.  I am wondering whether the expected distance to the origin increases as the bias $\delta$ increases.  Formally, if $0\le\delta\le\delta'\le 1$ , for all $n\in \mathbb{N}$ $$\mathbb{E}[|S_n(\delta)|]\le \mathbb{E}[|S_n(\delta')|].$$ where $|\cdot|$ is the one norm. Note that the second moment is increasing as $\delta$ increases, because $\mathbb{E}[S_n(\delta)^2] = n^2\delta^2+n(1-\delta^2)$ .  Additionally, by Chebyshev's inequality, we can prove the above inequality for large enough $n$ .  I am wondering if the inequality holds for all $n\ge 1$ .  Maybe it can be proved by a coupling argument.","Given and , consider a (biased) random walk where are i.i.d. and with probabiltiy and otherwise.  I am wondering whether the expected distance to the origin increases as the bias increases.  Formally, if , for all where is the one norm. Note that the second moment is increasing as increases, because .  Additionally, by Chebyshev's inequality, we can prove the above inequality for large enough .  I am wondering if the inequality holds for all .  Maybe it can be proved by a coupling argument.","\delta\in [0,1] n\in \mathbb{N} S_n(\delta) = \sum_{i = 1}^n X_i \{X_i:1\le i\le n\} X_i = 1 (1+\delta)/2 -1 \delta 0\le\delta\le\delta'\le 1 n\in \mathbb{N} \mathbb{E}[|S_n(\delta)|]\le \mathbb{E}[|S_n(\delta')|]. |\cdot| \delta \mathbb{E}[S_n(\delta)^2] = n^2\delta^2+n(1-\delta^2) n n\ge 1","['probability', 'inequality', 'random-walk', 'coupling']"
8,Wald meets Weitzman/Gittins,Wald meets Weitzman/Gittins,,"Suppose a searcher is faced with $n$ objects. Each object's quality is an i.i.d. Bernoulli random variable $\Theta_{i}$ , $i = 1, \dots, n$ , where $\mathbb{P}\left(\Theta_{i} = 1\right) = \mu_{0}$ . Time is continuous, and each instant the searcher may sample from an object: she chooses object $i$ and observes a process $(Z^{i}_{t})_{t\geq 0}$ . For all $i$ , when object $i$ is being sampled, the change in the process $Z^{i}_{t}$ is the sum of the state and a noise term, which is the increment of a Brownian motion $(W_{t})_{t \geq 0}$ : $$dZ^{i}_{t} = \theta_{i} dt + \sigma dW^{i}_t$$ For simplicity, I've assumed the volatility for each object $i$ is identical, $\sigma$ (more generally, each would have volatility $\sigma^{i}$ ). Let $\mu^{i}_{s}$ denote the agent's posterior belief that object $i$ has quality $1$ : $$\mu_{i}^{s} := \mathbb{P}\left(\Theta_{i} = 1|\left(Z^{i}_{s}\right)_{s \leq t}\right)$$ If object $i$ is not sampled in the interval $[s,s']$ , $\mu^{i}_{s} = \mu^{i}_{s'}$ i.e. she does not learn about its quality. Thus, she can only learn about one object per instant. Each instant the agent must decide whether to sample some object $i$ or to stop and select one of the objects. Selecting an object ends the scenario. When sampling an object $i$ the agent incurs a bounded (positive) flow cost $c\left(\cdot\right)$ , which depends on her posterior belief i.e. if she samples from box $i$ between times $s$ and $s'$ , she pays cost $$\int_{s}^{s'}c\left(\mu^{i}_{t}\right)dt$$ More generally, each object could have a different flow cost, $c^{i}\left(\cdot\right)$ , but for now I suppose that they're all the same. The agent's payoff from stopping and selecting object $i$ at time $\tau$ is $$\mu^{i}_{\tau} - \sum_{j=1}^{n}\int_{0}^{\tau}c\left(\mu_{t}^{j}\right)dt$$ The classic Wald (1945) problem has just one object and a constant flow cost. This set-up is also clearly related to the multi-arm bandit problem, in which each instant/period an arm is selected and a flow payoff is received. Here, each instant and arm is selected and ""flow learning"" occurs. Also closely related is the sequential search problem of Weitzman (1979). Here are my questions: Has this problem been studied? This seems likely, since it seems like a natural question and it is closely related to two rich literatures: the multi-arm bandit literature, and the literature on Wald problems. Is an index policy optimal? I'd appreciate any references or suggestions.","Suppose a searcher is faced with objects. Each object's quality is an i.i.d. Bernoulli random variable , , where . Time is continuous, and each instant the searcher may sample from an object: she chooses object and observes a process . For all , when object is being sampled, the change in the process is the sum of the state and a noise term, which is the increment of a Brownian motion : For simplicity, I've assumed the volatility for each object is identical, (more generally, each would have volatility ). Let denote the agent's posterior belief that object has quality : If object is not sampled in the interval , i.e. she does not learn about its quality. Thus, she can only learn about one object per instant. Each instant the agent must decide whether to sample some object or to stop and select one of the objects. Selecting an object ends the scenario. When sampling an object the agent incurs a bounded (positive) flow cost , which depends on her posterior belief i.e. if she samples from box between times and , she pays cost More generally, each object could have a different flow cost, , but for now I suppose that they're all the same. The agent's payoff from stopping and selecting object at time is The classic Wald (1945) problem has just one object and a constant flow cost. This set-up is also clearly related to the multi-arm bandit problem, in which each instant/period an arm is selected and a flow payoff is received. Here, each instant and arm is selected and ""flow learning"" occurs. Also closely related is the sequential search problem of Weitzman (1979). Here are my questions: Has this problem been studied? This seems likely, since it seems like a natural question and it is closely related to two rich literatures: the multi-arm bandit literature, and the literature on Wald problems. Is an index policy optimal? I'd appreciate any references or suggestions.","n \Theta_{i} i = 1, \dots, n \mathbb{P}\left(\Theta_{i} = 1\right) = \mu_{0} i (Z^{i}_{t})_{t\geq 0} i i Z^{i}_{t} (W_{t})_{t \geq 0} dZ^{i}_{t} = \theta_{i} dt + \sigma dW^{i}_t i \sigma \sigma^{i} \mu^{i}_{s} i 1 \mu_{i}^{s} := \mathbb{P}\left(\Theta_{i} = 1|\left(Z^{i}_{s}\right)_{s \leq t}\right) i [s,s'] \mu^{i}_{s} = \mu^{i}_{s'} i i c\left(\cdot\right) i s s' \int_{s}^{s'}c\left(\mu^{i}_{t}\right)dt c^{i}\left(\cdot\right) i \tau \mu^{i}_{\tau} - \sum_{j=1}^{n}\int_{0}^{\tau}c\left(\mu_{t}^{j}\right)dt","['probability', 'reference-request']"
9,"Concentration (or two sided tail bounds around expectations) of maximum and minimum of $n$ iid, subgaussian random variables","Concentration (or two sided tail bounds around expectations) of maximum and minimum of  iid, subgaussian random variables",n,"Having no answer so far, I asked this on MO now. My question is motivated by this question and this question , where the first was aimed for giving a one sided tail bound for maximum of subgaussians, and the second one was for two sided tail bounds for gaussians. I'm also motivated by questions like this one . Let $\{X_1 \dots X_n\}$ be $n$ iid, subgaussian random variables so that $P[|X_i| \ge t] \le 2 exp (- \frac{ct^2}{ ||X_1||_{\psi_2}^2 }) \forall i, ||*||_{\psi_2}$ denoting the Orlicz norm. I'm looking for concentration inequalities for : $$  X_{max} := max_{1 \le i \le n} X_i, \hspace{1mm}    X_{min} := min_{1 \le i \le n} X_i   $$ So to be more precise, I'm looking for tail bound functions $\alpha(t), \beta(t)$ of the form: $$ P[  | X_{max} - \mathbb{E}X_{max} | \ge t ] \le  \alpha(t), P[  | X_{min} - \mathbb{E}X_{min} | \ge t ] \le  \beta(t)  $$ where the following are decreasing functions of $t$ : $$ 0 \le \alpha(t), \beta(t) \to 0, t \to \infty.$$","Having no answer so far, I asked this on MO now. My question is motivated by this question and this question , where the first was aimed for giving a one sided tail bound for maximum of subgaussians, and the second one was for two sided tail bounds for gaussians. I'm also motivated by questions like this one . Let be iid, subgaussian random variables so that denoting the Orlicz norm. I'm looking for concentration inequalities for : So to be more precise, I'm looking for tail bound functions of the form: where the following are decreasing functions of :","\{X_1 \dots X_n\} n P[|X_i| \ge t] \le 2 exp (- \frac{ct^2}{ ||X_1||_{\psi_2}^2 }) \forall i, ||*||_{\psi_2}   X_{max} := max_{1 \le i \le n} X_i, \hspace{1mm}    X_{min} := min_{1 \le i \le n} X_i    \alpha(t), \beta(t)  P[  | X_{max} - \mathbb{E}X_{max} | \ge t ] \le  \alpha(t), P[  | X_{min} - \mathbb{E}X_{min} | \ge t ] \le  \beta(t)   t  0 \le \alpha(t), \beta(t) \to 0, t \to \infty.","['probability', 'probability-theory', 'probability-distributions', 'random-variables', 'random-matrices']"
10,"If $X_1,X_2\dots$ are i.i.d. Bernoulli$(1/2)$ then $\sum_{k=1}^n\frac{X_k}{2^k}$ converges in distribution to $Y\thicksim\text{Unif}(0,1)$",If  are i.i.d. Bernoulli then  converges in distribution to,"X_1,X_2\dots (1/2) \sum_{k=1}^n\frac{X_k}{2^k} Y\thicksim\text{Unif}(0,1)","Problem: Let $X_1,X_2,\dots$ be i.i.d. $\text{Bernoulli}(1/2)$ random variables. $\textbf{(a)}$ Show that the sequence $Y_n=\displaystyle\sum_{k=1}^n\frac{X_k}{2^k}$ converges with probability one. $\textbf{(b)}$ Let $Y=\lim\limits_{n\to\infty}Y_n.$ Show that $Y\thicksim\text{Unif}(0,1)$ by computing $P\left(Y\leq\frac{k}{2^n}\right)$ for $0\leq k\leq2^n$ , and then using the density of the dyadic rationals. Attempt: We begin with part (a). Fix $\omega\in\Omega$ . Then $X_k(\omega)=0$ or $X_k(\omega)=1$ for any $k\in\mathbb N$ . Therefore, $$Y_n(\omega)=\sum_{k=1}^n\frac{X_k(\omega)}{2^k}\leq\sum_{k=1}^\infty\frac{1}{2^k}<\infty.$$ Since $Y_n(\omega)$ is a uniformly bounded nondecreasing sum of positive real numbers, it must converge to a limit. Since $\omega\in\Omega$ was arbitrary it follows that $Y_n$ converges with probability one. For part (b), I found an approach using moment generating functions, in the question https://math.stackexchange.com/a/1269084/595519 , but I am having problems coming to grips with the approach indicated above for finding the limiting CDF. Could someone give me a heads-up on how to approach the problem in part (b)? Any thoughts on part (a) are also much welcomed. Thank you very much for your time.","Problem: Let be i.i.d. random variables. Show that the sequence converges with probability one. Let Show that by computing for , and then using the density of the dyadic rationals. Attempt: We begin with part (a). Fix . Then or for any . Therefore, Since is a uniformly bounded nondecreasing sum of positive real numbers, it must converge to a limit. Since was arbitrary it follows that converges with probability one. For part (b), I found an approach using moment generating functions, in the question https://math.stackexchange.com/a/1269084/595519 , but I am having problems coming to grips with the approach indicated above for finding the limiting CDF. Could someone give me a heads-up on how to approach the problem in part (b)? Any thoughts on part (a) are also much welcomed. Thank you very much for your time.","X_1,X_2,\dots \text{Bernoulli}(1/2) \textbf{(a)} Y_n=\displaystyle\sum_{k=1}^n\frac{X_k}{2^k} \textbf{(b)} Y=\lim\limits_{n\to\infty}Y_n. Y\thicksim\text{Unif}(0,1) P\left(Y\leq\frac{k}{2^n}\right) 0\leq k\leq2^n \omega\in\Omega X_k(\omega)=0 X_k(\omega)=1 k\in\mathbb N Y_n(\omega)=\sum_{k=1}^n\frac{X_k(\omega)}{2^k}\leq\sum_{k=1}^\infty\frac{1}{2^k}<\infty. Y_n(\omega) \omega\in\Omega Y_n","['probability', 'probability-theory']"
11,Challenging probability problem,Challenging probability problem,,"Hy, I hope everyone is doing fine. Lately I have been studying the topic of probability, I am aiming to improve on it, recently i came across this hard problem. Let $(u_{n})$ a sequence of random independent variables identically distributed following Rademacher distribution. And let $f(x)=\sum_{n \geq 0} u_{n} x^{n}$ . Prove that : $$ f(x)~~\text{almost surely has infinitely many zeros  in}~~ [0,1].$$ I have been stuck on it for weeks now, but I did find out a hint to solve it on a forum by searching about it, Here is the hint:  Construct an increasing sequence $(x_{k})$ such that the event $A_{k}=\{f(x_{0}),...,f(x_{k}) \text{are not zero and have the same sign} \}$ are such that $p(A_{k+1})\leq \frac{6}{7} p(A_{k})$ . I find it difficult to construct by induction such sequence in a way that may permit us to solve the problem. Any proposition is welcome.","Hy, I hope everyone is doing fine. Lately I have been studying the topic of probability, I am aiming to improve on it, recently i came across this hard problem. Let a sequence of random independent variables identically distributed following Rademacher distribution. And let . Prove that : I have been stuck on it for weeks now, but I did find out a hint to solve it on a forum by searching about it, Here is the hint:  Construct an increasing sequence such that the event are such that . I find it difficult to construct by induction such sequence in a way that may permit us to solve the problem. Any proposition is welcome.","(u_{n}) f(x)=\sum_{n \geq 0} u_{n} x^{n}  f(x)~~\text{almost surely has infinitely many zeros  in}~~ [0,1]. (x_{k}) A_{k}=\{f(x_{0}),...,f(x_{k}) \text{are not zero and have the same sign} \} p(A_{k+1})\leq \frac{6}{7} p(A_{k})","['probability', 'sequences-and-series', 'power-series', 'roots']"
12,Expected value of the length of max consecutive sequence of $1$'s in a random binary number,Expected value of the length of max consecutive sequence of 's in a random binary number,1,"Let $X$ be a random integer number from $0$ to $2^d − 1$ . Denote by $M(X)$ the length of the maximum consecutive sequence of 1’s in the binary representation of $X$ . Find expected value $E[M(X)]$ up to a constant multiplicative factor. For example, for the binary number “ $1101110$ ”, we have $M(1101110) = 3$ ; and for the binary number “ $11110110011$ ”, we have $M(11110110011) = 4$ .","Let be a random integer number from to . Denote by the length of the maximum consecutive sequence of 1’s in the binary representation of . Find expected value up to a constant multiplicative factor. For example, for the binary number “ ”, we have ; and for the binary number “ ”, we have .",X 0 2^d − 1 M(X) X E[M(X)] 1101110 M(1101110) = 3 11110110011 M(11110110011) = 4,"['probability', 'computer-science']"
13,How to solve the random walk problem on the complete graph?,How to solve the random walk problem on the complete graph?,,"In a complete graph of order $n$ , a moving point starts from a certain vertex and moves along the edges. At each vertex, the unpassed edges are selected with equal probability to continue the movement. What is the expectation of the number of passed edges until it stops its movement? The moving point probably does not pass all the edges. It just needs to move into a vertex and cannot move out.","In a complete graph of order , a moving point starts from a certain vertex and moves along the edges. At each vertex, the unpassed edges are selected with equal probability to continue the movement. What is the expectation of the number of passed edges until it stops its movement? The moving point probably does not pass all the edges. It just needs to move into a vertex and cannot move out.",n,"['probability', 'random-walk']"
14,Almost sure convergence of average of random variables,Almost sure convergence of average of random variables,,"In my statistical inference course exercise guide, I am confronted with the following problem: Let $0<\theta<1/2$ , and define the sequence $\{X_n\}_{n\in\mathbb{N}}$ of discrete independent random variables as follows: $X_n$ takes the values $n^{\theta}$ and $-n^{\theta}$ with probabilities $P(X_n=n^{\theta})=1/2=P(X_n=-n^{\theta})$ . Show that $$\frac{1}{n}\sum_{k=1}^{n}X_k \to 0$$ almost surely. My attempt: I try to use the Borel-Cantelli lemma. If $\overline{X}_n$ denotes the average of the first $n$ variables, it would suffice to show that for every $\epsilon >0$ it holds that $$\sum_{n=1}^{\infty}P(|{\overline{X}_n}| \geq \epsilon) < \infty \tag{1}$$ A quick computation tells us that for each $n$ , we have $E[X_n] = 0$ and $\operatorname{Var}(X_n) = n^{2\theta}$ , which implies $E[\overline{X}_n]=0$ and $$\operatorname{Var}(\overline{X}_n) = \frac{1}{n^2}\sum_{k=1}^{n}\operatorname{Var}(X_k) = \frac{1}{n^2}\sum_{k=1}^{n}k^{2\theta} \tag{2}$$ If we use Chebyshev's inequality, plugging $(2)$ into $(1)$ would yield $$\sum_{n=1}^{\infty}P(|{\overline{X}_n}| \geq \epsilon) \leq \sum_{n=1}^{\infty}\frac{1}{\epsilon^2}\operatorname{Var}(\overline{X}_n) \leq \sum_{n=1}^{\infty}\frac{1}{\epsilon^2}\frac{1}{n^2}\sum_{k=1}^{n}k^{2\theta}$$ The last term of the previous inequality seems quite divergent. Any suggestions?","In my statistical inference course exercise guide, I am confronted with the following problem: Let , and define the sequence of discrete independent random variables as follows: takes the values and with probabilities . Show that almost surely. My attempt: I try to use the Borel-Cantelli lemma. If denotes the average of the first variables, it would suffice to show that for every it holds that A quick computation tells us that for each , we have and , which implies and If we use Chebyshev's inequality, plugging into would yield The last term of the previous inequality seems quite divergent. Any suggestions?",0<\theta<1/2 \{X_n\}_{n\in\mathbb{N}} X_n n^{\theta} -n^{\theta} P(X_n=n^{\theta})=1/2=P(X_n=-n^{\theta}) \frac{1}{n}\sum_{k=1}^{n}X_k \to 0 \overline{X}_n n \epsilon >0 \sum_{n=1}^{\infty}P(|{\overline{X}_n}| \geq \epsilon) < \infty \tag{1} n E[X_n] = 0 \operatorname{Var}(X_n) = n^{2\theta} E[\overline{X}_n]=0 \operatorname{Var}(\overline{X}_n) = \frac{1}{n^2}\sum_{k=1}^{n}\operatorname{Var}(X_k) = \frac{1}{n^2}\sum_{k=1}^{n}k^{2\theta} \tag{2} (2) (1) \sum_{n=1}^{\infty}P(|{\overline{X}_n}| \geq \epsilon) \leq \sum_{n=1}^{\infty}\frac{1}{\epsilon^2}\operatorname{Var}(\overline{X}_n) \leq \sum_{n=1}^{\infty}\frac{1}{\epsilon^2}\frac{1}{n^2}\sum_{k=1}^{n}k^{2\theta},"['probability', 'probability-theory', 'borel-cantelli-lemmas']"
15,Expected length of longest common substring,Expected length of longest common substring,,"For two strings, a longest common substring is a substring of maximal length which is common to both strings. For example for $babba$ and $cdddabbd$ a longest common substring is $abb$ which has length $3$ . Consider two uniformly sampled binary strings, each of length $n$ .   What is the expected length of a longest common substring? If we wanted to know the expected longest common prefix length would be $1$ .  The length follows the geometric distribution . So we could look at all $1 \leq i, j \leq n$ and try to compute the maximum expected value of all longest common prefixes starting at index $i$ in the first string and index $j$ in the second. I don't know how to complete this analysis however.","For two strings, a longest common substring is a substring of maximal length which is common to both strings. For example for and a longest common substring is which has length . Consider two uniformly sampled binary strings, each of length .   What is the expected length of a longest common substring? If we wanted to know the expected longest common prefix length would be .  The length follows the geometric distribution . So we could look at all and try to compute the maximum expected value of all longest common prefixes starting at index in the first string and index in the second. I don't know how to complete this analysis however.","babba cdddabbd abb 3 n 1 1 \leq i, j \leq n i j",['probability']
16,"Two players alternatively shoot a target, first person who hits two consecutive shots win.","Two players alternatively shoot a target, first person who hits two consecutive shots win.",,"Two players take turns shooting at a target, with each shot by player $i$ hitting the target with probability $p_i$ , $i=1,2$ . Shooting ends when two consecutive shots hit the target. What is the probability that the player who shoots first will win? I understand this problem on a more simple level, when the win condition is only one hit, although I do not know how to solve it given two consecutive hits.","Two players take turns shooting at a target, with each shot by player hitting the target with probability , . Shooting ends when two consecutive shots hit the target. What is the probability that the player who shoots first will win? I understand this problem on a more simple level, when the win condition is only one hit, although I do not know how to solve it given two consecutive hits.","i p_i i=1,2",['probability']
17,Are probability measures always sigma-finite measures?,Are probability measures always sigma-finite measures?,,"$$ \newcommand{\sX}{\mathsf{X}} \newcommand{\cX}{\mathcal{X}} $$ The definition of a measure is A measure on a measurable space $\sX\times\cX$ is a function $$ \mu: \cX\to [0, +\infty) $$ satisfying \begin{align*}     \mu(\emptyset) &= 0  && \text{Null Empty Set}\\     \mu\left(\bigcup_i A_i\right) &= \sum_{i} \mu(A_i) && \text{Countably Additive} \end{align*} The definition of a sigma-finite measure is Let $\sX \times \cX$ be a measurable space and let $\mu:\cX\to [0, +\infty)$ be a measure on it. We say $\mu$ is a sigma-finite measure if the set $\sX$ is a countable union of measurable sets with finite measure $$ \sX = \bigcup_{n\in\mathbb{N}} A_n \qquad A_n\in \cX \qquad \text{and} \qquad \mu(A_n) < \infty  $$ while the definition of a probability measure is A probability measure $\mu$ on the measurable space $\sX\times \cX$ is a measure $\mu:\cX\to[0, 1]$ with $\mu(X) = 1$ Basically it's a probability measure with total measure $1$ . I was wondering if all probability measures are also sigma-finite. In other words, is sigma-finiteness more general than probability measures?","The definition of a measure is A measure on a measurable space is a function satisfying The definition of a sigma-finite measure is Let be a measurable space and let be a measure on it. We say is a sigma-finite measure if the set is a countable union of measurable sets with finite measure while the definition of a probability measure is A probability measure on the measurable space is a measure with Basically it's a probability measure with total measure . I was wondering if all probability measures are also sigma-finite. In other words, is sigma-finiteness more general than probability measures?","
\newcommand{\sX}{\mathsf{X}}
\newcommand{\cX}{\mathcal{X}}
 \sX\times\cX 
\mu: \cX\to [0, +\infty)
 \begin{align*}
    \mu(\emptyset) &= 0  && \text{Null Empty Set}\\
    \mu\left(\bigcup_i A_i\right) &= \sum_{i} \mu(A_i) && \text{Countably Additive}
\end{align*} \sX \times \cX \mu:\cX\to [0, +\infty) \mu \sX 
\sX = \bigcup_{n\in\mathbb{N}} A_n \qquad A_n\in \cX \qquad \text{and} \qquad \mu(A_n) < \infty 
 \mu \sX\times \cX \mu:\cX\to[0, 1] \mu(X) = 1 1","['probability', 'probability-theory', 'measure-theory', 'lebesgue-measure']"
18,Inequality involving stochastic dominance in Likelihood Ratio Order,Inequality involving stochastic dominance in Likelihood Ratio Order,,"Problem : I am struggling for some time to show that a particular inequality holds or to find a counterexample to disprove it: Suppose you have two continuous random variables $X_1, X_2$ with densities $f_1$ and $f_2$ , respectively, which are both distributed on the same compact interval $[\underline{x},\overline{x}]\subset \mathbb{R}_+$ and have the following properties: 1) Both densities are log-concave. 2) $f_i(\underline{x})\cdot\underline{x} \le 1$ $\hspace{2ex}$ $i \in \{1,2\}$ 3) $X_1$ stochastically dominates $X_2$ in the Likelihood Ratio Order, i.e. $\forall x <x'$ , $x,x' \in [\underline{x},\overline{x}]$ it holds that: \begin{equation} \frac{f_1(x)}{f_2(x)} \le \frac{f_1(x')}{f_2(x')} \end{equation} Note that the third property implies that $X_1$ stochastically dominates $X_2$ also in the Hazard Rate-, Reverse Hazard Rate- and First Order. Next, consider two particular realizations $\hat{x}_1$ and $\hat{x}_2$ which are implicitly defined in the following way: \begin{align*} \hat{x}_1 = \frac{1-2\cdot F_1(\hat{x}_1)}{f(\hat{x}_1)} \hspace{4ex} \hat{x}_2 = \frac{1-2\cdot F_2(\hat{x}_2)}{f(\hat{x}_2)} \end{align*} Properties 1) and 2) are sufficient for existence of such values in $[\underline{x},\overline{x}]$ and property 3) implies that $\hat{x}_1 > \hat{x}_2$ . With all this information, I wanted to show that $F_1(\hat{x}_1) \le F_2(\hat{x}_2)$ . Since I miserably failed in doing it so far, I also tried to find counterexamples and haven't found any. Approach : Start with the implicit equations defining $\hat{x}_1$ and $\hat{x}_2$ . Rearranging them yields: \begin{align} &F_1(\hat{x}_1) = \frac{1-\hat{x}_1 \cdot f_1(\hat{x}_1)}{2}\\ &F_2(\hat{x}_2) = \frac{1-\hat{x}_2 \cdot f_2(\hat{x}_2)}{2} \end{align} Now, in order for the desired inequality to hold, namely $F_1(\hat{x}_1) \le F_2(\hat{x}_2)$ , it suffices to show that: \begin{align} \frac{1-\hat{x}_1 \cdot f_1(\hat{x}_1)}{2} \le \frac{1-\hat{x}_2 \cdot f_2(\hat{x}_2)}{2} \Leftrightarrow \hat{x}_1 \cdot f_1(\hat{x}_1) \ge \hat{x}_2 \cdot f_2(\hat{x}_2) \end{align} Now, since $\hat{x}_1 > \hat{x}_2$ , by property 3) we know that: \begin{align} \frac{f_1(\hat{x}_1)}{f_2(\hat{x}_1)} \ge \frac{f_1(\hat{x}_2)}{f_2(\hat{x}_2)} \Leftrightarrow  f_1(\hat{x}_1) \ge \frac{f_2(\hat{x}_1)\cdot f_1(\hat{x}_2)}{f_2(\hat{x}_2)} \end{align} Using this for the original inequality, it now sufficies to show that: \begin{align} \hat{x}_1 \cdot \frac{f_2(\hat{x}_1)\cdot f_1(\hat{x}_2)}{f_2(\hat{x}_2)} \ge \hat{x}_2 \cdot f_2(\hat{x}_2) \Leftrightarrow \hat{x}_1 \cdot f_2(\hat{x}_1)\cdot f_1(\hat{x}_2) \ge \hat{x}_2 \cdot f_2(\hat{x}_2)^2 \end{align} And this is the step in this approach where I am stuck... I would really appreciate some help. Thanks!","Problem : I am struggling for some time to show that a particular inequality holds or to find a counterexample to disprove it: Suppose you have two continuous random variables with densities and , respectively, which are both distributed on the same compact interval and have the following properties: 1) Both densities are log-concave. 2) 3) stochastically dominates in the Likelihood Ratio Order, i.e. , it holds that: Note that the third property implies that stochastically dominates also in the Hazard Rate-, Reverse Hazard Rate- and First Order. Next, consider two particular realizations and which are implicitly defined in the following way: Properties 1) and 2) are sufficient for existence of such values in and property 3) implies that . With all this information, I wanted to show that . Since I miserably failed in doing it so far, I also tried to find counterexamples and haven't found any. Approach : Start with the implicit equations defining and . Rearranging them yields: Now, in order for the desired inequality to hold, namely , it suffices to show that: Now, since , by property 3) we know that: Using this for the original inequality, it now sufficies to show that: And this is the step in this approach where I am stuck... I would really appreciate some help. Thanks!","X_1, X_2 f_1 f_2 [\underline{x},\overline{x}]\subset \mathbb{R}_+ f_i(\underline{x})\cdot\underline{x} \le 1 \hspace{2ex} i \in \{1,2\} X_1 X_2 \forall x <x' x,x' \in [\underline{x},\overline{x}] \begin{equation}
\frac{f_1(x)}{f_2(x)} \le \frac{f_1(x')}{f_2(x')}
\end{equation} X_1 X_2 \hat{x}_1 \hat{x}_2 \begin{align*}
\hat{x}_1 = \frac{1-2\cdot F_1(\hat{x}_1)}{f(\hat{x}_1)} \hspace{4ex} \hat{x}_2 = \frac{1-2\cdot F_2(\hat{x}_2)}{f(\hat{x}_2)}
\end{align*} [\underline{x},\overline{x}] \hat{x}_1 > \hat{x}_2 F_1(\hat{x}_1) \le F_2(\hat{x}_2) \hat{x}_1 \hat{x}_2 \begin{align}
&F_1(\hat{x}_1) = \frac{1-\hat{x}_1 \cdot f_1(\hat{x}_1)}{2}\\
&F_2(\hat{x}_2) = \frac{1-\hat{x}_2 \cdot f_2(\hat{x}_2)}{2}
\end{align} F_1(\hat{x}_1) \le F_2(\hat{x}_2) \begin{align}
\frac{1-\hat{x}_1 \cdot f_1(\hat{x}_1)}{2} \le \frac{1-\hat{x}_2 \cdot f_2(\hat{x}_2)}{2} \Leftrightarrow \hat{x}_1 \cdot f_1(\hat{x}_1) \ge \hat{x}_2 \cdot f_2(\hat{x}_2)
\end{align} \hat{x}_1 > \hat{x}_2 \begin{align}
\frac{f_1(\hat{x}_1)}{f_2(\hat{x}_1)} \ge \frac{f_1(\hat{x}_2)}{f_2(\hat{x}_2)} \Leftrightarrow 
f_1(\hat{x}_1) \ge \frac{f_2(\hat{x}_1)\cdot f_1(\hat{x}_2)}{f_2(\hat{x}_2)}
\end{align} \begin{align}
\hat{x}_1 \cdot \frac{f_2(\hat{x}_1)\cdot f_1(\hat{x}_2)}{f_2(\hat{x}_2)} \ge \hat{x}_2 \cdot f_2(\hat{x}_2) \Leftrightarrow \hat{x}_1 \cdot f_2(\hat{x}_1)\cdot f_1(\hat{x}_2) \ge \hat{x}_2 \cdot f_2(\hat{x}_2)^2
\end{align}",['probability']
19,Local times in higher dimensions,Local times in higher dimensions,,"It is known (see e.g. Revuz/Yor 1991, Chapter VI Corollary 1.6) that for a one-dim. Brownian motion $B$ and a nonegative function $\phi$ it holds $$\int_0^t\phi(B_s)\text{d}s=\int \phi(a)L_t^a(B)\text{d}a$$ where $L_t^a$ is the local time of $B$ at the point $a$ up to time $t$ . I am wondering, whether there is an analogous result in the multi-dimensional case. So $B=(B^1,B^2, \dots , B^n)$ and $\phi: \mathbb{R}^n \to\mathbb{R}$ . I know that the concept of local times must be ""reinvented"" for higher dimensions  but I was hoping that one can break the problem down to $n$ independent  one-dimensional Brownian motions, so that maybe the right hand side of the formula becomes a multidimensional integral?! I couldn't come up with a result by myself and didn't find anything in the literature. More general, I am asking myself whether there exists some concept of local times which measure the time $d$ -dim. Brownian motion has spent in a set $A \subset \mathbb{R}^d$ . Does anybody know?","It is known (see e.g. Revuz/Yor 1991, Chapter VI Corollary 1.6) that for a one-dim. Brownian motion and a nonegative function it holds where is the local time of at the point up to time . I am wondering, whether there is an analogous result in the multi-dimensional case. So and . I know that the concept of local times must be ""reinvented"" for higher dimensions  but I was hoping that one can break the problem down to independent  one-dimensional Brownian motions, so that maybe the right hand side of the formula becomes a multidimensional integral?! I couldn't come up with a result by myself and didn't find anything in the literature. More general, I am asking myself whether there exists some concept of local times which measure the time -dim. Brownian motion has spent in a set . Does anybody know?","B \phi \int_0^t\phi(B_s)\text{d}s=\int \phi(a)L_t^a(B)\text{d}a L_t^a B a t B=(B^1,B^2, \dots , B^n) \phi: \mathbb{R}^n \to\mathbb{R} n d A \subset \mathbb{R}^d","['probability', 'stochastic-processes', 'stochastic-calculus', 'brownian-motion', 'stochastic-analysis']"
20,Conditional Expectation($E[f(X)\mid X \in S]$) is uniformly continous in its argument($S$),Conditional Expectation() is uniformly continous in its argument(),E[f(X)\mid X \in S] S,"I basically want to prove that under some conditions $E[f(X)\mid X \in S]$ is uniformly continuous in $S$ To be more specific, suppose $f:K \subset \mathbb{R}^n \rightarrow \mathbb{R}$ is a bounded uniformly continuous function where $K$ is compact. Also suppose $X: \Omega \subset \mathbb{R}^M \rightarrow K$ is a random variable with a uniformly continuous bounded PDF and $\Omega$ is compact (You can assume the mapping for $X$ is one-to-one. Also you can assume $n=2$ if you want) Suppose $h(v)=g(S)=E[f(X)\mid X \in S]$ where $v=(x_1,y_1,\ldots,x_n,y_n)$ and $S=[x_1,y_1]\times[x_2,y_2]\times\cdots\times [x_n,y_n]$ . I want to prove that $h$ is uniformly continuous in its arguments (or $g$ is uniformly continuous wrt Hausdorff distance or Symmetric difference pseudometric or  Fréchet-Nikodym metric) Is there a general theorem which can encompasses this?","I basically want to prove that under some conditions is uniformly continuous in To be more specific, suppose is a bounded uniformly continuous function where is compact. Also suppose is a random variable with a uniformly continuous bounded PDF and is compact (You can assume the mapping for is one-to-one. Also you can assume if you want) Suppose where and . I want to prove that is uniformly continuous in its arguments (or is uniformly continuous wrt Hausdorff distance or Symmetric difference pseudometric or  Fréchet-Nikodym metric) Is there a general theorem which can encompasses this?","E[f(X)\mid X \in S] S f:K \subset \mathbb{R}^n \rightarrow \mathbb{R} K X: \Omega \subset \mathbb{R}^M \rightarrow K \Omega X n=2 h(v)=g(S)=E[f(X)\mid X \in S] v=(x_1,y_1,\ldots,x_n,y_n) S=[x_1,y_1]\times[x_2,y_2]\times\cdots\times [x_n,y_n] h g","['probability', 'conditional-expectation', 'uniform-continuity', 'hausdorff-distance']"
21,Are $U=\frac{X}{X+Y}$ and $V=X+Y$ independent if $X$ and $Y$ are?,Are  and  independent if  and  are?,U=\frac{X}{X+Y} V=X+Y X Y,"Suppose $X$ and $Y$ are independent random variables. Under what conditions, $ U=\frac{X}{X+Y}$ and $V=X+Y$ are independent? Notes: 1- Of course if you have independent $X,Y\sim \text{Uniform}(0,1)$ , $0\leq U\leq 1$ and $0\leq V\leq 2$ , but $U$ and $V$ are not independent, e.g. $V=2$ implies $U=\frac{1}{2}$ . So we may restrict our attention to independent unbounded random variables $X, Y$ . (Maybe even positive r.v.s so that $X+Y$ will be non-zero. 2- I think when $X$ and $Y$ are unbounded, then scaling both of them with same constant keeps $U$ unchanged, but we can make $V$ as large or small as possible by varying that constant. So it seems given $U$ , we don't have any information about $V$ . 3- Today in our stat class we showed that for independent $X\sim \text{Gamma}\left(\alpha_1,\beta\right)$ and $Y\sim \text{Gamma}\left(\alpha_2,\beta\right)$ , joint pdf $f_{_{U,V}}\left(u,v\right)$ decomposes into product of marginal pdfs of $U\sim \text{Beta}\left(\alpha_1,\alpha_2\right)$ and $V\sim \text{Gamma}\left(\alpha_1+\alpha_2,\beta\right)$ , hence they are independent. That's where I started to think if there is a general result.","Suppose and are independent random variables. Under what conditions, and are independent? Notes: 1- Of course if you have independent , and , but and are not independent, e.g. implies . So we may restrict our attention to independent unbounded random variables . (Maybe even positive r.v.s so that will be non-zero. 2- I think when and are unbounded, then scaling both of them with same constant keeps unchanged, but we can make as large or small as possible by varying that constant. So it seems given , we don't have any information about . 3- Today in our stat class we showed that for independent and , joint pdf decomposes into product of marginal pdfs of and , hence they are independent. That's where I started to think if there is a general result.","X Y  U=\frac{X}{X+Y} V=X+Y X,Y\sim \text{Uniform}(0,1) 0\leq U\leq 1 0\leq V\leq 2 U V V=2 U=\frac{1}{2} X, Y X+Y X Y U V U V X\sim \text{Gamma}\left(\alpha_1,\beta\right) Y\sim \text{Gamma}\left(\alpha_2,\beta\right) f_{_{U,V}}\left(u,v\right) U\sim \text{Beta}\left(\alpha_1,\alpha_2\right) V\sim \text{Gamma}\left(\alpha_1+\alpha_2,\beta\right)","['probability', 'probability-distributions', 'independence']"
22,Distributing balls into bins randomly,Distributing balls into bins randomly,,"Problem: If $n$ balls are distributed at random into $r$ boxes (where $r \geq 3$ ), what is the probability that box $1$ at exactly $j$ balls for $0 \leq j  \leq n$ and box $2$ contains exactly $k$ balls for $0 \leq k  \leq n$ ? Answer: Let $p$ be the probability that we seek. First we consider a special case. If $j + k > n$ then $p = 0$ . Let $p_1$ be the probability that a ball is placed in box $1$ .  Let $p_2$ be the probability that a ball is placed in box $2$ . Let $p_3$ be the probability that a ball is placed in box other than box $1$ and box $2$ . \begin{align*} p_1 &= \frac{1}{r} \\ p_2 &= \frac{1}{r} \\ p_3 &= \frac{r-2}{r} \end{align*} Now we have a multinomial distribution. \begin{align*} P &= \left( \frac{n!}{j!k!(n-j-k)!} \right) \left( \frac{1}{r}\right) ^j \left( \frac{1}{r}\right) ^k \left( \frac{r-2}{r} \right)^{n - j - k} \\ P &= \left( \frac{n!}{j!k!(n-j-k)!} \right) \left( \frac{1}{r}\right) ^{j+k}  \left( \frac{r-2}{r} \right)^{n - j - k} \\ P &= \left( \frac{n!}{j!k!(n-j-k)!} \right) \frac{ (r-2)^{n-j-k} } {r^n} \end{align*} So for example, if we have: $j = 2$ , $k = 2$ , $n = 8$ and $r = 8$ then \begin{align*} P &= \left( \frac{8!}{2!2!(8-2-2)!} \right) \frac{ (8-2)^{8-2-2} } {8^8} = \left( \frac{8!}{4(8-2-2)!} \right) \frac{ (6)^{4} } {8^8} \\ P &= \left( \frac{8!}{4(4)!} \right) \frac{ (6)^{4} } {8^8} =  \left( \frac{8(7)(6)(5)}{4} \right) \frac{ 2^4(3^4) } {8^8} \\ P &= \frac{ 8(7)(6)(5)( 2^4)(3^4) } { 4(8^8) } = \frac{ 2(7)(6)(5)( 2^4)(3^4) } { 4(8^7) } \\ P &= \frac{ 7(6)(5)( 2^4)(3^4) } { 2(8^7) } =  \frac{ 7(6)(5)( 2^3 )(3^4) } { (8^7) }\\ P &= \frac{ 7(6)(5)(3^4) } { 8^6 } = \frac{ 17010 } { 262144 } \\ P &= \frac{8505 } { 131072 } \\ P &\doteq 0.064888 \end{align*} I think I have it right now. Do I?","Problem: If balls are distributed at random into boxes (where ), what is the probability that box at exactly balls for and box contains exactly balls for ? Answer: Let be the probability that we seek. First we consider a special case. If then . Let be the probability that a ball is placed in box .  Let be the probability that a ball is placed in box . Let be the probability that a ball is placed in box other than box and box . Now we have a multinomial distribution. So for example, if we have: , , and then I think I have it right now. Do I?","n r r \geq 3 1 j 0 \leq j  \leq n 2 k 0 \leq k  \leq n p j + k > n p = 0 p_1 1 p_2 2 p_3 1 2 \begin{align*}
p_1 &= \frac{1}{r} \\
p_2 &= \frac{1}{r} \\
p_3 &= \frac{r-2}{r}
\end{align*} \begin{align*}
P &= \left( \frac{n!}{j!k!(n-j-k)!} \right) \left( \frac{1}{r}\right) ^j \left( \frac{1}{r}\right) ^k \left( \frac{r-2}{r} \right)^{n - j - k} \\
P &= \left( \frac{n!}{j!k!(n-j-k)!} \right) \left( \frac{1}{r}\right) ^{j+k}  \left( \frac{r-2}{r} \right)^{n - j - k} \\
P &= \left( \frac{n!}{j!k!(n-j-k)!} \right) \frac{ (r-2)^{n-j-k} } {r^n}
\end{align*} j = 2 k = 2 n = 8 r = 8 \begin{align*}
P &= \left( \frac{8!}{2!2!(8-2-2)!} \right) \frac{ (8-2)^{8-2-2} } {8^8} = \left( \frac{8!}{4(8-2-2)!} \right) \frac{ (6)^{4} } {8^8} \\
P &= \left( \frac{8!}{4(4)!} \right) \frac{ (6)^{4} } {8^8} =  \left( \frac{8(7)(6)(5)}{4} \right) \frac{ 2^4(3^4) } {8^8} \\
P &= \frac{ 8(7)(6)(5)( 2^4)(3^4) } { 4(8^8) } = \frac{ 2(7)(6)(5)( 2^4)(3^4) } { 4(8^7) } \\
P &= \frac{ 7(6)(5)( 2^4)(3^4) } { 2(8^7) } =  \frac{ 7(6)(5)( 2^3 )(3^4) } { (8^7) }\\
P &= \frac{ 7(6)(5)(3^4) } { 8^6 } = \frac{ 17010 } { 262144 } \\
P &= \frac{8505 } { 131072 } \\
P &\doteq 0.064888
\end{align*}","['probability', 'multinomial-distribution']"
23,"Show that : $(X_n,\mathcal{F}_n)_{n \in \mathbb{N}} $ converge in $L^1$",Show that :  converge in,"(X_n,\mathcal{F}_n)_{n \in \mathbb{N}}  L^1","Theorem (Doob decomposition): Let $(X_n,\mathcal{F}_n)_{n \in \mathbb{N}}$ be a submartingale integrable. Then the Doob decomposition of $X_n$ is given by $$X_n = M_n+A_n$$ where $(M_n)_{n \geq 0}$ a martingale, and $(A_n)_{n \geq 0}$ be an increasing process such that $A_{0}=0$ is $\mathcal{F}_0$ -measurable. $A_{n+1}$ is $\mathcal{F}_n$ -measurable. and even more : $$ sup_{n} \mathbb {E}|M_n|<+\infty ~and ~ A_{\infty}\in L^1 \Leftrightarrow sup_{n} \mathbb {E}X_n^+<+\infty$$ Show that : If $(X_n,\mathcal{F}_n)_{n \in \mathbb{N}} $ converge in $L^1 $ if and only if : $\exists M\in L^1 $ such as: $\forall n\in\mathbb {N}~M_n= \mathbb {E}^{\mathcal{F}_n }M$ and $A_\infty\in L^1$ My effort : $(\Rightarrow ) $ $(X_n,\mathcal{F}_n)_{n \in \mathbb{N}} $ converge in $L^1 $ then, according to the definition of the limit, we have : $$ sup_{n} \mathbb {E}|X_n|<+\infty $$ According to what is framed above we have: $$ sup_{n} \mathbb {E}|M_n|<+\infty ~~and~~A_{\infty}\in L^1 $$ Now, $(M_n)_n $ is a martingale, then $M_n\to M_{\infty}$ almoust sure and $ M_{\infty}\in L^{1} $ . Let $n\in \mathbb {N}$ , $(M_n)_n $ is a martingale, then $\forall p>n~:~~M_n=\mathbb {E}^{\mathcal{F}_n }M_p $ by passage to the limit, we have : $$ M_n=\mathbb {E}^{\mathcal{F}_n }\lim_p M_p =\mathbb {E}^{\mathcal{F}_n }M_{\infty} $$ $(\Leftarrow ) $ we have : $X_n^+=(M_n+A_n)^+\leq M_n^+ +A_n\leq M_n^+ +A_{\infty} $ because $A_n \geq 0$ a.s. Then : $$ \mathbb {E}X_n^+\leq \mathbb {E}M_n+\mathbb {E}A_{\infty}=\mathbb {E}M+\mathbb {E}A_{\infty} $$ Because $M_n= \mathbb {E}^{\mathcal{F}_n }M$ . Then : $$ sup_n \mathbb {E}X_n^+\leq \infty ~~(*) $$ According to $(X_n)_n $ is sub-martingale and $(*) $ , we have : $(X_n)_n\to X_\infty $ with $X_\infty\in L^1$ . My problem is to show that : $$ \mathbb {E}|X_n- X_\infty|\to 0 $$ And thank you in advance. I have another idea but I can't get applied, this idea is to show that: $\{X_n~:~n\in \mathbb {N}\}$ is uniformly integrable. We have $\{M_n~:~n \in\mathbb {N}\}$ is uniformly integrable Because $M_n= \mathbb {E}^{\mathcal{F}_n }M$ . My problem is to show that : $\{A_n~:~n \in\mathbb {N}\}$ is uniformly integrable","Theorem (Doob decomposition): Let be a submartingale integrable. Then the Doob decomposition of is given by where a martingale, and be an increasing process such that is -measurable. is -measurable. and even more : Show that : If converge in if and only if : such as: and My effort : converge in then, according to the definition of the limit, we have : According to what is framed above we have: Now, is a martingale, then almoust sure and . Let , is a martingale, then by passage to the limit, we have : we have : because a.s. Then : Because . Then : According to is sub-martingale and , we have : with . My problem is to show that : And thank you in advance. I have another idea but I can't get applied, this idea is to show that: is uniformly integrable. We have is uniformly integrable Because . My problem is to show that : is uniformly integrable","(X_n,\mathcal{F}_n)_{n \in \mathbb{N}} X_n X_n = M_n+A_n (M_n)_{n \geq 0} (A_n)_{n \geq 0} A_{0}=0 \mathcal{F}_0 A_{n+1} \mathcal{F}_n  sup_{n} \mathbb {E}|M_n|<+\infty ~and ~ A_{\infty}\in L^1 \Leftrightarrow sup_{n} \mathbb {E}X_n^+<+\infty (X_n,\mathcal{F}_n)_{n \in \mathbb{N}}  L^1  \exists M\in L^1  \forall n\in\mathbb {N}~M_n= \mathbb {E}^{\mathcal{F}_n }M A_\infty\in L^1 (\Rightarrow )  (X_n,\mathcal{F}_n)_{n \in \mathbb{N}}  L^1  
sup_{n} \mathbb {E}|X_n|<+\infty
 
sup_{n} \mathbb {E}|M_n|<+\infty ~~and~~A_{\infty}\in L^1
 (M_n)_n  M_n\to M_{\infty}  M_{\infty}\in L^{1}  n\in \mathbb {N} (M_n)_n  \forall p>n~:~~M_n=\mathbb {E}^{\mathcal{F}_n }M_p  
M_n=\mathbb {E}^{\mathcal{F}_n }\lim_p M_p =\mathbb {E}^{\mathcal{F}_n }M_{\infty}
 (\Leftarrow )  X_n^+=(M_n+A_n)^+\leq M_n^+ +A_n\leq M_n^+ +A_{\infty}  A_n \geq 0 
\mathbb {E}X_n^+\leq \mathbb {E}M_n+\mathbb {E}A_{\infty}=\mathbb {E}M+\mathbb {E}A_{\infty}
 M_n= \mathbb {E}^{\mathcal{F}_n }M 
sup_n \mathbb {E}X_n^+\leq \infty ~~(*)
 (X_n)_n  (*)  (X_n)_n\to X_\infty  X_\infty\in L^1 
\mathbb {E}|X_n- X_\infty|\to 0
 \{X_n~:~n\in \mathbb {N}\} \{M_n~:~n \in\mathbb {N}\} M_n= \mathbb {E}^{\mathcal{F}_n }M \{A_n~:~n \in\mathbb {N}\}","['probability', 'integration', 'measure-theory', 'conditional-expectation', 'martingales']"
24,Find $\operatorname{Var}$ of the number of isolated vertices,Find  of the number of isolated vertices,\operatorname{Var},"From the full connected fraph with $n$ vertices choose the subgraph $G = G (n, p)$ in such a way that each edge is independently selected with a probability of $p$ or not selected with a probability of $1 - p$ . For the resulting graph G find $\operatorname{Var}$ of the number of isolated vertices. My attempt: Let $X_j$ be random variable for which if $X_j = 1$ then $ v_j $ is isolated. Otherwise $X_j=0$ $$\operatorname{Var}(X) = \operatorname{Var}(\sum_{v}X_v) = E\left(\left(\sum_vX_v\right)^2\right)  - E^2 (\sum_v X_v)$$ Ok, so if it  comes to $$E^2 (\sum_v X_v)) = n^2 \cdot E^2(X_1)  = n^2(1-p)^{2n-2}$$ Now $E\left(\left(\sum_vX_v\right)^2\right)$ : $$ E\left(\left(\sum_vX_v\right)^2\right) = \left(E(X_1+ \cdots +X_n)^2\right) = E\sum_{u,v}X_uX_v = \\ \underbrace{E\sum_{u \neq v}X_uX_v}_{(*)} + \underbrace{E\sum_{u = v}X_uX_v}_{(**)}$$ and now by combinatoric interpretation $(*)$ is $n(n-1)(1-p)^{2n-3}$ . If it comes to $(**)$ there is $n EX_1^2 = \color{red}{n(1-p)^{2n-2}}$ So gathering all together: $$-n^2 (1-p)^{2 n-2}+(n-1) n (1-p)^{2 n-3}+n (1-p)^{\color{red}{2n-2}} $$ but the correct answer is $$-n^2 (1-p)^{2 n-2}+(n-1) n (1-p)^{2 n-3}+n (1-p)^{\color{red}{n-1}} $$ Where did I fail?","From the full connected fraph with vertices choose the subgraph in such a way that each edge is independently selected with a probability of or not selected with a probability of . For the resulting graph G find of the number of isolated vertices. My attempt: Let be random variable for which if then is isolated. Otherwise Ok, so if it  comes to Now : and now by combinatoric interpretation is . If it comes to there is So gathering all together: but the correct answer is Where did I fail?","n G =
G (n, p) p 1 - p \operatorname{Var} X_j X_j = 1  v_j  X_j=0 \operatorname{Var}(X) = \operatorname{Var}(\sum_{v}X_v) = E\left(\left(\sum_vX_v\right)^2\right)  - E^2 (\sum_v X_v) E^2 (\sum_v X_v)) = n^2 \cdot E^2(X_1)  = n^2(1-p)^{2n-2} E\left(\left(\sum_vX_v\right)^2\right)  E\left(\left(\sum_vX_v\right)^2\right) = \left(E(X_1+ \cdots +X_n)^2\right) = E\sum_{u,v}X_uX_v = \\ \underbrace{E\sum_{u \neq v}X_uX_v}_{(*)} + \underbrace{E\sum_{u = v}X_uX_v}_{(**)} (*) n(n-1)(1-p)^{2n-3} (**) n EX_1^2 = \color{red}{n(1-p)^{2n-2}} -n^2 (1-p)^{2 n-2}+(n-1) n (1-p)^{2 n-3}+n (1-p)^{\color{red}{2n-2}}  -n^2 (1-p)^{2 n-2}+(n-1) n (1-p)^{2 n-3}+n (1-p)^{\color{red}{n-1}} ","['probability', 'probability-theory']"
25,Probability to identify highest margin product.,Probability to identify highest margin product.,,"Assume the following scenario. I can      Sell P1 for a profit of  14%      or sell it at a Loss of -7%      Sell P2 for a profit of 11%     Or sell it at a loss of -6%      Sell P3 for a profit of 7%     or sell for a loss of -1% Considering the profit margins provided above and max loss rate, at which they need to be cleared by the end of month. As a seller, stocking which of the above is more profitable to the business.  Assuming all products will be sold at the mentioned P/L levels. If I consider PL ratio, For P1, it would be 14:7 ~ 2   : 1 For P2, it would be 11:6 ~ 1.9 : 1 For P3, it would be 7:1  ~ 7   : 1 Clearly the higher ratios isnt going to aid in determining the ideal product. How could I identify the right product?","Assume the following scenario. I can      Sell P1 for a profit of  14%      or sell it at a Loss of -7%      Sell P2 for a profit of 11%     Or sell it at a loss of -6%      Sell P3 for a profit of 7%     or sell for a loss of -1% Considering the profit margins provided above and max loss rate, at which they need to be cleared by the end of month. As a seller, stocking which of the above is more profitable to the business.  Assuming all products will be sold at the mentioned P/L levels. If I consider PL ratio, For P1, it would be 14:7 ~ 2   : 1 For P2, it would be 11:6 ~ 1.9 : 1 For P3, it would be 7:1  ~ 7   : 1 Clearly the higher ratios isnt going to aid in determining the ideal product. How could I identify the right product?",,[]
26,Confusion about a probability question.,Confusion about a probability question.,,"$90$ students, including Joe and Jane, are to be split into three classes of equal size, and this is to be done at random. What is the probability that Joe and Jane end up in the same class? The answer given is $\frac{29}{89}$ . One explanation is ""put Joe in one class, then, Jane has a $\frac{29}{89}$ chance of choosing to be in the same class"". I also found the combinatorics answer $$\frac{3\binom{88}{28}}{\binom{90}{30}}.$$ I don't understand why it wouldn't just be $\frac 13$ . $9$ ways to split Jane and Joe up, and in $3$ of those $9$ ways, they are together. I don't see why the other $88$ students matter.","students, including Joe and Jane, are to be split into three classes of equal size, and this is to be done at random. What is the probability that Joe and Jane end up in the same class? The answer given is . One explanation is ""put Joe in one class, then, Jane has a chance of choosing to be in the same class"". I also found the combinatorics answer I don't understand why it wouldn't just be . ways to split Jane and Joe up, and in of those ways, they are together. I don't see why the other students matter.",90 \frac{29}{89} \frac{29}{89} \frac{3\binom{88}{28}}{\binom{90}{30}}. \frac 13 9 3 9 88,"['probability', 'combinatorics']"
27,"If a random process $\{Y_t\}_{t\ge0}$ whose moment of first order is uniformly bounded, does $Y_t/t$ converges to $0$ a.s.?","If a random process  whose moment of first order is uniformly bounded, does  converges to  a.s.?",\{Y_t\}_{t\ge0} Y_t/t 0,"I have problem in reading the paper in the proof Theorem 3.6 (in Page 14): http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.386.7227&rep=rep1&type=pdf I am puzzled here: it says $X(t)/t$ converges a.s. to $v$ , and $X(t)-R(t)$ is tight, then concluding that $R(t)/t$ converges a.s. to $v$ . I think the author want to say that he concluded $(X(t)-R(t))/t \rightarrow 0$ a.s. by the tightness, but I failed. There is one counterexample for uniformly bounded first order moment process $Y_n$ whose time is discrete and $Y_n/n$ doesn't converges to $0$ a.s.: $Y_n$ are mutually independent, and $P(Y_n=n)=1/n, P(Y_n=0)=1-1/n$ . Therefore, I think I need some property of the trajectory to have it, but the proof of the paper is really not clear to me. However, it is easy to see uniformly finite first moment obtains the convergence to $0$ in $L^1$ . It might cost you some time to read the paper, and it is very grateful for your help!","I have problem in reading the paper in the proof Theorem 3.6 (in Page 14): http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.386.7227&rep=rep1&type=pdf I am puzzled here: it says converges a.s. to , and is tight, then concluding that converges a.s. to . I think the author want to say that he concluded a.s. by the tightness, but I failed. There is one counterexample for uniformly bounded first order moment process whose time is discrete and doesn't converges to a.s.: are mutually independent, and . Therefore, I think I need some property of the trajectory to have it, but the proof of the paper is really not clear to me. However, it is easy to see uniformly finite first moment obtains the convergence to in . It might cost you some time to read the paper, and it is very grateful for your help!","X(t)/t v X(t)-R(t) R(t)/t v (X(t)-R(t))/t \rightarrow 0 Y_n Y_n/n 0 Y_n P(Y_n=n)=1/n, P(Y_n=0)=1-1/n 0 L^1","['probability', 'probability-theory', 'stochastic-processes']"
28,Strategy for this auction on 100 coin flips question?,Strategy for this auction on 100 coin flips question?,,"Got an interesting interview/game question today: You have the opportunity to participate in an auction for a treasure chest. The seller anonymously flips 200 coins, and for each head he adds \$1 to the treasure chest, and nobody else knows how much is in this chest. Everyone bids a price simultaneously, and the person with the highest bid wins. (If more than one person chooses the same highest number, the winner is chosen randomly.) What should your optimal strategy be? Now, I’ve seen a few auction questions on this site, but they still boggle me as to how an optimal strategy would work. I only know that: The expected value of the chest is \$100. This auction is symmetric, and everyone should have the same strategy for Nash equilibrium. The probability of having the winning bid is $\frac1{n}$ where $n$ is the number of people. (I imagine it as arranging $n$ numbers on a line and the probability of being assigned the first one.) How should one piece these together? Should I be aiming to maximise profit or minimise losses? Would bidding \$50 be better than \$100, and should I even bid at all? Edit: the comments say bidding \$99 is the best bid, why is this so? Is this the case for any number of people? Mathematically speaking is it correct to say would if I bid \$ $b$ , my expected value is $(100-b)/n$ ? Still it feels odd that the strategy is just bidding \$100 — not sure if this is too risky... All help is greatly appreciated!","Got an interesting interview/game question today: You have the opportunity to participate in an auction for a treasure chest. The seller anonymously flips 200 coins, and for each head he adds \$1 to the treasure chest, and nobody else knows how much is in this chest. Everyone bids a price simultaneously, and the person with the highest bid wins. (If more than one person chooses the same highest number, the winner is chosen randomly.) What should your optimal strategy be? Now, I’ve seen a few auction questions on this site, but they still boggle me as to how an optimal strategy would work. I only know that: The expected value of the chest is \$100. This auction is symmetric, and everyone should have the same strategy for Nash equilibrium. The probability of having the winning bid is where is the number of people. (I imagine it as arranging numbers on a line and the probability of being assigned the first one.) How should one piece these together? Should I be aiming to maximise profit or minimise losses? Would bidding \$50 be better than \$100, and should I even bid at all? Edit: the comments say bidding \$99 is the best bid, why is this so? Is this the case for any number of people? Mathematically speaking is it correct to say would if I bid \$ , my expected value is ? Still it feels odd that the strategy is just bidding \$100 — not sure if this is too risky... All help is greatly appreciated!",\frac1{n} n n b (100-b)/n,"['probability', 'game-theory', 'expected-value', 'nash-equilibrium', 'auction-theory']"
29,Sequence convergence almost surely,Sequence convergence almost surely,,"If we consider $(Xn)_{n \in \mathbb{N}}$ , a sequence of independent random variables satisfying: $$ P(X_n = 1) = p_n \;\;\;\;\;\;\;\;\;\; P(X_n = 0) = 1 − p_n$$ then : $$X_n \;{\overset{a.s}{\longrightarrow}} \;0 ⇔\sum_{n \in \mathbb{N}} p_n < ∞$$ I couldn't show the first implication from convergence a.s to the sum of $p_n$ is finite!","If we consider , a sequence of independent random variables satisfying: then : I couldn't show the first implication from convergence a.s to the sum of is finite!",(Xn)_{n \in \mathbb{N}}  P(X_n = 1) = p_n \;\;\;\;\;\;\;\;\;\; P(X_n = 0) = 1 − p_n X_n \;{\overset{a.s}{\longrightarrow}} \;0 ⇔\sum_{n \in \mathbb{N}} p_n < ∞ p_n,"['probability', 'probability-distributions', 'stochastic-processes', 'stochastic-analysis', 'probability-limit-theorems']"
30,Binomial Probability Distribution Question,Binomial Probability Distribution Question,,"Tom has $10$ crazy children. In honor of Christmas, he bought $7$ new toys for each one of them. The children destroy a new toy within one day with a $0.7$ probability. What is the chance that after one day at least 8 children destroyed at least 6 toys each ? Final Answer : $0.82$ I think the question is asking about Binomial Probability Distribution + little combinatorics. I don't know if that's right as well. Answer Attempt : I tried first calculating the probability that a child destroy at least $6$ toys , Let $X$ be random variable indicating that so : $P(6\leq X) = P(X=6)+P(X=7)=\binom{7}{6}(0.7)^6(0.3)^1 + \binom{7}{7}(0.7)^7(0.3)^0 = 0.3294$ $ P(X<6) = 1-P(6\leq X) = 0.6706$ Now we need at least $8$ children to destroy at least $6$ , Let $Y$ be the number of children that destroy at least $6$ toys so : $P(8 \leq Y) = P(Y=8)+P(Y=9)+P(Y=10) = $ $= \binom{10}{8}(0.3294)^8(0.6706)^2 + \binom{10}{9}(0.3294)^9(0.6706)^1 + \binom{10}{10}(0.3294)^{10}(0.6706)^0$ but I get really really small probability $(0.003)$","Tom has crazy children. In honor of Christmas, he bought new toys for each one of them. The children destroy a new toy within one day with a probability. What is the chance that after one day at least 8 children destroyed at least 6 toys each ? Final Answer : I think the question is asking about Binomial Probability Distribution + little combinatorics. I don't know if that's right as well. Answer Attempt : I tried first calculating the probability that a child destroy at least toys , Let be random variable indicating that so : Now we need at least children to destroy at least , Let be the number of children that destroy at least toys so : but I get really really small probability",10 7 0.7 0.82 6 X P(6\leq X) = P(X=6)+P(X=7)=\binom{7}{6}(0.7)^6(0.3)^1 + \binom{7}{7}(0.7)^7(0.3)^0 = 0.3294  P(X<6) = 1-P(6\leq X) = 0.6706 8 6 Y 6 P(8 \leq Y) = P(Y=8)+P(Y=9)+P(Y=10) =  = \binom{10}{8}(0.3294)^8(0.6706)^2 + \binom{10}{9}(0.3294)^9(0.6706)^1 + \binom{10}{10}(0.3294)^{10}(0.6706)^0 (0.003),"['probability', 'combinatorics', 'probability-distributions', 'combinations']"
31,Why is the Rademacher complexity/distribution named after Hans Rademacher?,Why is the Rademacher complexity/distribution named after Hans Rademacher?,,"In machine learning theory, the Rademacher complexity of a function class $\newcommand{\cF}{\mathcal{F}}\newcommand{\E}{\mathbb{E}}\newcommand{\R}{\mathbb{R}} \cF: X \mapsto \R$ over a particular set of inputs $x_{1:n} \in X^n$ is defined as $$ \operatorname{R}(F, x_{1:n})  =  \frac{1}{n}    \E \left[    \sup_{f \in F}    \sum_{i=1}^m \sigma_i f(x_i)  \right], $$ where $\sigma_i$ is a random variable distributed uniformly over $\{-1, +1\}$ . In machine learning literature [1], the Rademacher complexity is defined without etymology. The variable $\sigma_i$ is called a Rademacher-distributed random variable . However, in a biography of Rademacher [2], the words ""distribution"" and ""random variable"" do not occur. It seems that Rademacher was mainly a number theorist. Why do these objects bear his name? From what I can tell, Rademacher did not introduce the definition $R$ above, and it seems strange to name $R$ after the distribution of $\sigma_i$ when $R$ itself is such a rich construct. [1] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning . 2nd edition, MIT Press, 2018. https://www.dropbox.com/s/7voitv0vt24c88s/10290.pdf?dl=1 [2] Bruce C. Berndt. ""Hans Rademacher (1892–1969)."" Acta Arithmetica LXI.3 , 1992. http://matwbn.icm.edu.pl/ksiazki/aa/aa61/aa6131.pdf .","In machine learning theory, the Rademacher complexity of a function class over a particular set of inputs is defined as where is a random variable distributed uniformly over . In machine learning literature [1], the Rademacher complexity is defined without etymology. The variable is called a Rademacher-distributed random variable . However, in a biography of Rademacher [2], the words ""distribution"" and ""random variable"" do not occur. It seems that Rademacher was mainly a number theorist. Why do these objects bear his name? From what I can tell, Rademacher did not introduce the definition above, and it seems strange to name after the distribution of when itself is such a rich construct. [1] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning . 2nd edition, MIT Press, 2018. https://www.dropbox.com/s/7voitv0vt24c88s/10290.pdf?dl=1 [2] Bruce C. Berndt. ""Hans Rademacher (1892–1969)."" Acta Arithmetica LXI.3 , 1992. http://matwbn.icm.edu.pl/ksiazki/aa/aa61/aa6131.pdf .","\newcommand{\cF}{\mathcal{F}}\newcommand{\E}{\mathbb{E}}\newcommand{\R}{\mathbb{R}} \cF: X \mapsto \R x_{1:n} \in X^n 
\operatorname{R}(F, x_{1:n}) 
= 
\frac{1}{n}
   \E \left[
   \sup_{f \in F}
   \sum_{i=1}^m \sigma_i f(x_i) 
\right],
 \sigma_i \{-1, +1\} \sigma_i R R \sigma_i R","['probability', 'terminology', 'math-history', 'rademacher-distribution']"
32,Maximizing chance that randomly drawn numbers fill a row or column,Maximizing chance that randomly drawn numbers fill a row or column,,"This is just a problem I have thought of. Let us say we have a square matrix of dimension $n$ with possible entries $1, 2, 3,..., n^2$ and we can freely distribute the entries in the matrix, each one has to be used exactly once. The numbers then are drawn without replacement. The probability that number $k$ is drawn is $0 \leq P(k) \leq 1$ with $\sum_{k=1}^{n^2}P(k)=1$ Now, we know the probabilities of the draws before we fill in our matrix. Let us say we sorted them such that $0 \leq p_1 \leq p_2 \leq ... \leq p_{n^2}$ . We want to cheat and optimally distribute the numbers in order to minimize the expected number of draws until we cross out a whole row or column. Is this a known problem? Is there an optimal strategy on how to distribute the probabilities $p_i$ in our matrix to minimize the expected number of draws until we cross out a whole row or column, and what is one such strategy? I was thinking of determining the expected value of number of draws to get a certain number (then determine expected draws to get whole rows and columns and minimize those), but I do not think I can define it only given the $P(k)$ , I would need to define how they change after each draw. I was thinking that it is reasonable to remodel the probabilities after each draw for nonzero $P(k)$ .  Let us call $D$ the set of already drawn numbers. Then for $k \notin D$ with $P(k)\neq 0$ we do $$P'(k)=\frac{P(k)}{1-\sum_{ d \in D}P(d)}$$ But it sounds horribly complex! Then again, it shouldn't matter, because if we have some remaining nonzero probabilities, their new probabilities are in the same order, that is if $P(k) \geq P(l)$ and both were not drawn, then $P'(k) \geq P'(l)$ .","This is just a problem I have thought of. Let us say we have a square matrix of dimension with possible entries and we can freely distribute the entries in the matrix, each one has to be used exactly once. The numbers then are drawn without replacement. The probability that number is drawn is with Now, we know the probabilities of the draws before we fill in our matrix. Let us say we sorted them such that . We want to cheat and optimally distribute the numbers in order to minimize the expected number of draws until we cross out a whole row or column. Is this a known problem? Is there an optimal strategy on how to distribute the probabilities in our matrix to minimize the expected number of draws until we cross out a whole row or column, and what is one such strategy? I was thinking of determining the expected value of number of draws to get a certain number (then determine expected draws to get whole rows and columns and minimize those), but I do not think I can define it only given the , I would need to define how they change after each draw. I was thinking that it is reasonable to remodel the probabilities after each draw for nonzero .  Let us call the set of already drawn numbers. Then for with we do But it sounds horribly complex! Then again, it shouldn't matter, because if we have some remaining nonzero probabilities, their new probabilities are in the same order, that is if and both were not drawn, then .","n 1, 2, 3,..., n^2 k 0 \leq P(k) \leq 1 \sum_{k=1}^{n^2}P(k)=1 0 \leq p_1 \leq p_2 \leq ... \leq p_{n^2} p_i P(k) P(k) D k \notin D P(k)\neq 0 P'(k)=\frac{P(k)}{1-\sum_{ d \in D}P(d)} P(k) \geq P(l) P'(k) \geq P'(l)","['probability', 'puzzle', 'maximum-likelihood']"
33,Expected Time Until Absorption for Discrete Finite State Markov Chain,Expected Time Until Absorption for Discrete Finite State Markov Chain,,"The Problem: Consider a discrete time Markov chain with states $0,1,...,N$ whose matrix has elements $$ P_{ij} =   \left\{ \begin{array}{ll}       \mu_i,                 & j = i - 1; \\       \lambda_i,             & j = i + 1, \hspace{5mm} i,j = 0,1,...,N;  \\       1 - \lambda_i - \mu_i, & j = i;             \\       0						 & |j - i| > 1.      \\ \end{array}  \right. $$ Suppose that $\mu_0 = \lambda_0 = \mu_N = 0$ , and all other $\mu_i$ 's and $\lambda_i$ 's are positive, and that the initial state of the process is $k$ . Determine the expected time until absorption. My Progress: So, I know that if we define $\rho_0 = 1$ and $$ \rho_i = \frac{\mu_1 \mu_2 \cdots \mu_i}{\lambda_1 \lambda_2 \cdots \lambda_i}, $$ then $$ \Pr\{\text{absorption at $0$} \} = 1 - \Pr\{\text{absorption at $N$} \} = \frac{\sum_{i=k}^{N-1}\rho_i}{\sum_{i=0}^{N-1}\rho_i} .  $$ However, my thinking is as follows: If we collect the transient states into the set $T = \{1, 2, ..., N-1\}$ , then the expected time until absorption is simply the mean time the process spends in $T$ . Now, I know that, for $i,j \in T$ , if $s_{ij}$ denotes the number of time periods that the process is in $j$ given that it starts in state $i$ , we have the following formula: $$ s_{ij} = \delta_{ij} + \sum_{k=1}^{N-1}P_{ik}s_{kj}. $$ But I assume that I am supposed to use my knowledge of the above probabilities of absorption at $0$ or $N$ (perhaps in some kind of conditioning argument) to determine the desired time. Whatever the case, both approaches have sent me down different rabbit holes, neither of which has proven very fruitful.","The Problem: Consider a discrete time Markov chain with states whose matrix has elements Suppose that , and all other 's and 's are positive, and that the initial state of the process is . Determine the expected time until absorption. My Progress: So, I know that if we define and then However, my thinking is as follows: If we collect the transient states into the set , then the expected time until absorption is simply the mean time the process spends in . Now, I know that, for , if denotes the number of time periods that the process is in given that it starts in state , we have the following formula: But I assume that I am supposed to use my knowledge of the above probabilities of absorption at or (perhaps in some kind of conditioning argument) to determine the desired time. Whatever the case, both approaches have sent me down different rabbit holes, neither of which has proven very fruitful.","0,1,...,N  P_{ij} =   \left\{
\begin{array}{ll}
      \mu_i,                 & j = i - 1; \\
      \lambda_i,             & j = i + 1, \hspace{5mm} i,j = 0,1,...,N;  \\
      1 - \lambda_i - \mu_i, & j = i;             \\
      0						 & |j - i| > 1.      \\
\end{array} 
\right.  \mu_0 = \lambda_0 = \mu_N = 0 \mu_i \lambda_i k \rho_0 = 1  \rho_i = \frac{\mu_1 \mu_2 \cdots \mu_i}{\lambda_1 \lambda_2 \cdots \lambda_i},   \Pr\{\text{absorption at 0} \} = 1 - \Pr\{\text{absorption at N} \} = \frac{\sum_{i=k}^{N-1}\rho_i}{\sum_{i=0}^{N-1}\rho_i} .   T = \{1, 2, ..., N-1\} T i,j \in T s_{ij} j i  s_{ij} = \delta_{ij} + \sum_{k=1}^{N-1}P_{ik}s_{kj}.  0 N","['probability', 'stochastic-processes', 'markov-chains']"
34,How can one make a periodic Markov chain aperiodic with the smallest change in the main structure of the chain?,How can one make a periodic Markov chain aperiodic with the smallest change in the main structure of the chain?,,"Or, how can one ""deform"" a stochastic, irreducible, periodic matrix into a stochastic, irreducible, aperiodic matrix with the smallest change? If there are several possible procedures, then please mention them. If the procedure is complex, then please provide sufficient explanation or link of blogs/books etc. I want to analyze them and check which makes the smallest change in the original graph/chain/matrix.","Or, how can one ""deform"" a stochastic, irreducible, periodic matrix into a stochastic, irreducible, aperiodic matrix with the smallest change? If there are several possible procedures, then please mention them. If the procedure is complex, then please provide sufficient explanation or link of blogs/books etc. I want to analyze them and check which makes the smallest change in the original graph/chain/matrix.",,"['linear-algebra', 'probability', 'matrices', 'markov-chains', 'stochastic-matrices']"
35,What is the meaning of a space-time white,What is the meaning of a space-time white,,"For reference this is from page 27 here Consider the stochastic heat equation $$ \begin{cases} \dfrac{\partial u}{\partial t}(x,t)=\dfrac{\partial ^2 u}{\partial x^2}(x,t) +f(u(x,t))\dot{W}(x,t) & t >0, x\in[0,L] \\ \dfrac{\partial u}{\partial x}(0,t)=\dfrac{\partial u}{\partial x}(L,t)=0 &t>0 \\ u(x,0)=u_0(x) &x \in[0,L] \end{cases} $$ $\dot{W}(x,s$ ) is refered to as the space-time white noise. But this doesn't make sense to the defintion of white noise given in the same text. $\textbf{White Noise}$ here is defined as follows: The gaussian process $\{\dot{W}(A)\}_{A \in \mathscr B(\mathbb R^n)}$ with $E(\dot{W}(A))=0$ and $E(\dot{W}(A)\dot{W}(B))=\lambda^n(A\cap B)$ where $\lambda ^n$ is the n-dimensianal Lesbegue measure is called White Noise. It then goes on to say that we can consider for $A \in \mathscr B(\mathbb R^n)$ that $\dot{W}(A) = \int 1_A dW$ and for $h\in L^2(\mathbb R^n)$ we define $\dot{W}(h) = \int h(t)dW(t)$ which is the standard Wiener Integral. This is also the same definition given in the Walsh notes So my question is, what is $\dot{W}(x,t)$ ? Could it be $\dot{W}(0\times t) \times (0,x])$ ? Also how does this definition relate to the definitions given in this question ? In particular to notion that White noise, $\dot{W}$ , is the weak derivitive of Brownian motion. Ie: If $W_t$ is a standard Brownian motion then $\dot{W}$ is a distribution on the set of test functions such that $$ -\int W_t\dfrac{\partial \phi}{\partial t}(t)dt=\int\dot{W}_t\phi(t)dt $$","For reference this is from page 27 here Consider the stochastic heat equation ) is refered to as the space-time white noise. But this doesn't make sense to the defintion of white noise given in the same text. here is defined as follows: The gaussian process with and where is the n-dimensianal Lesbegue measure is called White Noise. It then goes on to say that we can consider for that and for we define which is the standard Wiener Integral. This is also the same definition given in the Walsh notes So my question is, what is ? Could it be ? Also how does this definition relate to the definitions given in this question ? In particular to notion that White noise, , is the weak derivitive of Brownian motion. Ie: If is a standard Brownian motion then is a distribution on the set of test functions such that"," \begin{cases}
\dfrac{\partial u}{\partial t}(x,t)=\dfrac{\partial ^2 u}{\partial x^2}(x,t) +f(u(x,t))\dot{W}(x,t) & t >0, x\in[0,L]
\\ \dfrac{\partial u}{\partial x}(0,t)=\dfrac{\partial u}{\partial x}(L,t)=0 &t>0
\\ u(x,0)=u_0(x) &x \in[0,L]
\end{cases}  \dot{W}(x,s \textbf{White Noise} \{\dot{W}(A)\}_{A \in \mathscr B(\mathbb R^n)} E(\dot{W}(A))=0 E(\dot{W}(A)\dot{W}(B))=\lambda^n(A\cap B) \lambda ^n A \in \mathscr B(\mathbb R^n) \dot{W}(A) = \int 1_A dW h\in L^2(\mathbb R^n) \dot{W}(h) = \int h(t)dW(t) \dot{W}(x,t) \dot{W}(0\times t) \times (0,x]) \dot{W} W_t \dot{W}  -\int W_t\dfrac{\partial \phi}{\partial t}(t)dt=\int\dot{W}_t\phi(t)dt ","['probability', 'probability-theory', 'stochastic-processes', 'brownian-motion']"
36,Netflix profile and user probability.,Netflix profile and user probability.,,"So we have 4 netflix profiles in a single subscription, say 5 People have access to the account such that 1 of the person is actually piggybacking on other people's profiles. Assuming that a single show lasts about 2 hours and nobody watches more than 3 shows a day at a maximum. Also everyone sleeps at the same time for 8 hours a day. What is the probability that all 5 users will log into their account so that the timing of people watching shows clash. how does this probability be affected as more people get access to the account. Edit : I am pursuing my undergrad in Microbiology. With almost no clue of mathematics and applied mathematics whatsoever.  These days I've been inclined to learn a little more about math because of my master's preparation.  This question came to me when me and some of my friends pooled in money to get the premium Netflix account. My thoughts on this problem: initially I thought that 24-8 = 16 so there are just 8 timings for one person to open the account (assuming they watch one show) so if we multipled it (1/8)^4 would be the probability. Now I know that this answer is wrong because  Firstly it only accounts timing in segments. It takes log in at 2 pm and then straight up at 4 pm while completely ignoring 2:01 log in. Secondly it only takes into account 1 person watching 1 show. Lastly that is outrageously low probability for the number of times the timings clash for us  irl :p","So we have 4 netflix profiles in a single subscription, say 5 People have access to the account such that 1 of the person is actually piggybacking on other people's profiles. Assuming that a single show lasts about 2 hours and nobody watches more than 3 shows a day at a maximum. Also everyone sleeps at the same time for 8 hours a day. What is the probability that all 5 users will log into their account so that the timing of people watching shows clash. how does this probability be affected as more people get access to the account. Edit : I am pursuing my undergrad in Microbiology. With almost no clue of mathematics and applied mathematics whatsoever.  These days I've been inclined to learn a little more about math because of my master's preparation.  This question came to me when me and some of my friends pooled in money to get the premium Netflix account. My thoughts on this problem: initially I thought that 24-8 = 16 so there are just 8 timings for one person to open the account (assuming they watch one show) so if we multipled it (1/8)^4 would be the probability. Now I know that this answer is wrong because  Firstly it only accounts timing in segments. It takes log in at 2 pm and then straight up at 4 pm while completely ignoring 2:01 log in. Secondly it only takes into account 1 person watching 1 show. Lastly that is outrageously low probability for the number of times the timings clash for us  irl :p",,['probability']
37,Inequality between two random walks if comparison of moment of step is known,Inequality between two random walks if comparison of moment of step is known,,"Let $B_i =\pm 1$ be i.i.d. Bernoulli r.v. with probability $1/2$ each. Let $X_i \geq 0$ be i.i.d. r.v.'s, and independent to $B_i$ 's. Let $Y_i \geq 0$ be i.i.d. r.v.'s, and independent to $B_i$ 's. Suppose, for $q=1,2,3,\dotsc$ , $$\mathbb{E} (X_i^q) \leq \mathbb{E} (Y_i^q) <\infty$$ Define $$\tilde{X}_N :=\sum_{i=1}^N X_i B_i$$ $$\tilde{Y}_N :=\sum_{i=1}^N Y_i B_i$$ Prove or disprove: for $q=1,2,3,\dotsc$ , $$\mathbb{E} (|\tilde{X}_N|^q) \leq \mathbb{E} (|\tilde{Y}_N|^q)?$$ Instinct: It seems very plausible to be true. Since each step is symmetric, the larger the step is, the larger the total distance is, and $x^q$ (which is convex) seems even to make the difference larger. However, I do not know how to break the absolute value of $|\tilde{X}_N|^q$ . We can't use Jensen inequality ""on both sides"" (which does not make sense).","Let be i.i.d. Bernoulli r.v. with probability each. Let be i.i.d. r.v.'s, and independent to 's. Let be i.i.d. r.v.'s, and independent to 's. Suppose, for , Define Prove or disprove: for , Instinct: It seems very plausible to be true. Since each step is symmetric, the larger the step is, the larger the total distance is, and (which is convex) seems even to make the difference larger. However, I do not know how to break the absolute value of . We can't use Jensen inequality ""on both sides"" (which does not make sense).","B_i =\pm 1 1/2 X_i \geq 0 B_i Y_i \geq 0 B_i q=1,2,3,\dotsc \mathbb{E} (X_i^q) \leq \mathbb{E} (Y_i^q) <\infty \tilde{X}_N :=\sum_{i=1}^N X_i B_i \tilde{Y}_N :=\sum_{i=1}^N Y_i B_i q=1,2,3,\dotsc \mathbb{E} (|\tilde{X}_N|^q) \leq \mathbb{E} (|\tilde{Y}_N|^q)? x^q |\tilde{X}_N|^q","['probability', 'inequality', 'random-walk', 'expected-value']"
38,"Calculate the probability that in a city of some country with 400000 people, eight or more murders take place.","Calculate the probability that in a city of some country with 400000 people, eight or more murders take place.",,"A country's monthly homicide rate is of 1 per 100000 people. Calculate the probability that in a given month, in a city of such country with 400000 people, eight or more murders take place. I'm currently having a hard time with this problem, my answer differs from the book where I found it so I'm unsure if I'm missing something. I suspect that the random variable X=""Number of murders in a month"" is distributed by a Poisson distribution with parameter $\lambda= 4= 400000 \times  \frac{1}{100000}$ . So I've made the calculations for: $P(X \ge 8)=1-P(X<8)=1-(P(X=0)+P(X=1)+P(X=2)+...+P(X=7))$ with $P(X=k)=\frac{e^{-4} 4^{k}}{k!}$ Both by myself and using RStudio but the result is around 0.051 while book says that the answer is 0.1874. Any help would be greatly appreciated!","A country's monthly homicide rate is of 1 per 100000 people. Calculate the probability that in a given month, in a city of such country with 400000 people, eight or more murders take place. I'm currently having a hard time with this problem, my answer differs from the book where I found it so I'm unsure if I'm missing something. I suspect that the random variable X=""Number of murders in a month"" is distributed by a Poisson distribution with parameter . So I've made the calculations for: with Both by myself and using RStudio but the result is around 0.051 while book says that the answer is 0.1874. Any help would be greatly appreciated!",\lambda= 4= 400000 \times  \frac{1}{100000} P(X \ge 8)=1-P(X<8)=1-(P(X=0)+P(X=1)+P(X=2)+...+P(X=7)) P(X=k)=\frac{e^{-4} 4^{k}}{k!},"['probability', 'probability-theory', 'probability-distributions']"
39,A riddle about guessing hat colours (which is not among the commonly known ones),A riddle about guessing hat colours (which is not among the commonly known ones),,"This is a riddle I heard recently, and my question is if someone happens to know the solution. I'm asking this out of curiosity more than anything else. So here it is. The riddle is one of the countless variations of the ""prisoners have to guess their hat colour"" puzzle. $n$ prisoners are put a hat on top of their head, which can be red or blue. The colours are chosen at random by $n$ independent fair coin tosses. Then each prisoner can guess their own hat colour (red or blue) or pass. The prisoners can see each other, but not hear each other's calls and of course they have no other means of communication. This means that each call can only depend on the other prisoners' hat colours. However, before the distributing of hats begins, the prisoners are told the rules and can agree on a strategy. The prisoners win iff no prisoner guesses wrong and at least one prisoner guesses right. Which strategy should the prisoners use so that the winning probability becomes maximal? Some remarks: A simple strategy is that one player just guesses and all other players pass, so that the maximal probabilty is at least 1/2. For $n=2$ this strategy is optimal. For $n=3$, there is a strategy that wins in 6 out of 8 cases: When a player sees (red,red) he guesses blue, for (blue,blue) he guesses red, and otherwise he passes. More generally this shows that the maximal probability is at least 3/4 for $n\ge 3$. It's possible to show that any strategy fails for at least 2 hat colour configurations (unless $n=1$), which shows that the above strategy is optimal for $n=3$. For $n=4$ there are more than $10^{15}$ strategies, and for $n=5$ it's about $10^{38}$ strategies, making it quite infeasible to just use a brute-force computer program (maybe for $n=4$ it's possible when exploiting the obvious symmetries). When changing the rules slightly by forbidding players to pass, then the maximal winning probability is always 1/2. This is a nice little exercise. Actually I heard the riddle only for $n=3$ and then thought about the general $n$. So it's entirely possible that there is no nice solution.","This is a riddle I heard recently, and my question is if someone happens to know the solution. I'm asking this out of curiosity more than anything else. So here it is. The riddle is one of the countless variations of the ""prisoners have to guess their hat colour"" puzzle. $n$ prisoners are put a hat on top of their head, which can be red or blue. The colours are chosen at random by $n$ independent fair coin tosses. Then each prisoner can guess their own hat colour (red or blue) or pass. The prisoners can see each other, but not hear each other's calls and of course they have no other means of communication. This means that each call can only depend on the other prisoners' hat colours. However, before the distributing of hats begins, the prisoners are told the rules and can agree on a strategy. The prisoners win iff no prisoner guesses wrong and at least one prisoner guesses right. Which strategy should the prisoners use so that the winning probability becomes maximal? Some remarks: A simple strategy is that one player just guesses and all other players pass, so that the maximal probabilty is at least 1/2. For $n=2$ this strategy is optimal. For $n=3$, there is a strategy that wins in 6 out of 8 cases: When a player sees (red,red) he guesses blue, for (blue,blue) he guesses red, and otherwise he passes. More generally this shows that the maximal probability is at least 3/4 for $n\ge 3$. It's possible to show that any strategy fails for at least 2 hat colour configurations (unless $n=1$), which shows that the above strategy is optimal for $n=3$. For $n=4$ there are more than $10^{15}$ strategies, and for $n=5$ it's about $10^{38}$ strategies, making it quite infeasible to just use a brute-force computer program (maybe for $n=4$ it's possible when exploiting the obvious symmetries). When changing the rules slightly by forbidding players to pass, then the maximal winning probability is always 1/2. This is a nice little exercise. Actually I heard the riddle only for $n=3$ and then thought about the general $n$. So it's entirely possible that there is no nice solution.",,"['probability', 'puzzle']"
40,Balls and Bins Variant Unique Maximum Number of Balls in a Bin,Balls and Bins Variant Unique Maximum Number of Balls in a Bin,,"Suppose there are $m$ balls and $n$ bins where $m \geq n$ . The $m$ balls are thrown into the $n$ bins uniformly at random. At the end of each round, all bins with less than the maximum number of balls is removed. Let the remaining number of bins be $n_1$ . The process is repeated with $m$ balls and $n_1$ bins, and so on. The question is, what is the expected number of rounds necessary until one bin remains? One can get a weak bound of $\lg n$ rounds by using the well-known result for expected number of empty bins: $n/e$ . But I'm hoping for a tighter bound. Has anyone ever seen this question before perhaps named differently? Intuitively, this is the opposite of what we want with balls and bins (because that's used most often in load balancing), but I don't know a better name to ask this question under.","Suppose there are balls and bins where . The balls are thrown into the bins uniformly at random. At the end of each round, all bins with less than the maximum number of balls is removed. Let the remaining number of bins be . The process is repeated with balls and bins, and so on. The question is, what is the expected number of rounds necessary until one bin remains? One can get a weak bound of rounds by using the well-known result for expected number of empty bins: . But I'm hoping for a tighter bound. Has anyone ever seen this question before perhaps named differently? Intuitively, this is the opposite of what we want with balls and bins (because that's used most often in load balancing), but I don't know a better name to ask this question under.",m n m \geq n m n n_1 m n_1 \lg n n/e,"['probability', 'discrete-mathematics', 'balls-in-bins']"
41,Proving these inequalities for a symmetric random walk,Proving these inequalities for a symmetric random walk,,"I would like to prove the following inequalities. Here $S_n = \sum_{i=1}^n X_i$ where $x_i$ are i.i.d and symmetric. $$P(|S_n|>x) \geq \frac{1}{2}P(\max_{k\leq n}|X_k|>x)\geq \frac{1}{2}(1-e^{-nP(|X_i|>x)})$$ $$P(\max_{k\leq n} |S_i|>x)\leq 2P(|S_n|>x)$$ My attempt: For the first one $$P(S_n>x) = \sum_{k\leq n} P(S_j<x \text{ for } j<k, S_k>x, S_n-S_k > 0) $$ $$= \frac{1}{2}\sum_{k\leq n} P(S_j<x \text{ for } j<k, S_k>x) $$ $$=\frac{1}{2} P(\max_{k<n} S_k > x)$$ I think by symmetry of the random walk, that then gives $$P(|S_n|>x) \geq \frac{1}{2}P(\max_{k\leq n}|S_k|>x)$$ I am completely stuck on getting from $S_k$ to $X_k$ . Further, I don't know how to get to $$\frac{1}{2}(1-e^{-nP(|X_i|>x)})$$ For the second inequality I believe I need to use the reflection principle in someway. But I have also made no progress towards that one. Edit: It just occured to me that: $$P(\max_{k\leq n}|X_k|>x)=1-P(\max_{k\leq n}|X_k| \leq x)$$ $$=1-P(|X_1| \leq x)^n=1-(1-P(|X_1| > x))^n$$ $$\geq 1-e^{-nP(|X_i|>x)}$$ So I got that part sorted I think I got the first inequality. Basically it goes $$P(S_n>x) \geq \sum_k P(X_k > x, \sum_{i\neq k}X_i>0, \max_{j<k}|X_j|<x)$$ $$=\frac{1}{2}\sum_k P(X_k > x,\max_{j<k}|X_j|<x)$$ Doing the same for $P(S_n<-x)$ and then adding gives the result. Can someone confirm this is correct? For the second inequality Let $A_k = \{ |S_k| > x; S_j < x, j<k\}$ $P(|S_n|>x) =\sum_{k=1}^n P(A_k; sgn(S_n-S_k)=sgn(S_k))$ But here I'm unsure how to get $\geq$ . It seems the latter should be $=\frac{1}{2}...$ ? Where the last inequality follows from the fact that","I would like to prove the following inequalities. Here where are i.i.d and symmetric. My attempt: For the first one I think by symmetry of the random walk, that then gives I am completely stuck on getting from to . Further, I don't know how to get to For the second inequality I believe I need to use the reflection principle in someway. But I have also made no progress towards that one. Edit: It just occured to me that: So I got that part sorted I think I got the first inequality. Basically it goes Doing the same for and then adding gives the result. Can someone confirm this is correct? For the second inequality Let But here I'm unsure how to get . It seems the latter should be ? Where the last inequality follows from the fact that","S_n = \sum_{i=1}^n X_i x_i P(|S_n|>x) \geq \frac{1}{2}P(\max_{k\leq n}|X_k|>x)\geq \frac{1}{2}(1-e^{-nP(|X_i|>x)}) P(\max_{k\leq n} |S_i|>x)\leq 2P(|S_n|>x) P(S_n>x) = \sum_{k\leq n} P(S_j<x \text{ for } j<k, S_k>x, S_n-S_k > 0)  = \frac{1}{2}\sum_{k\leq n} P(S_j<x \text{ for } j<k, S_k>x)  =\frac{1}{2} P(\max_{k<n} S_k > x) P(|S_n|>x) \geq \frac{1}{2}P(\max_{k\leq n}|S_k|>x) S_k X_k \frac{1}{2}(1-e^{-nP(|X_i|>x)}) P(\max_{k\leq n}|X_k|>x)=1-P(\max_{k\leq n}|X_k| \leq x) =1-P(|X_1| \leq x)^n=1-(1-P(|X_1| > x))^n \geq 1-e^{-nP(|X_i|>x)} P(S_n>x) \geq \sum_k P(X_k > x, \sum_{i\neq k}X_i>0, \max_{j<k}|X_j|<x) =\frac{1}{2}\sum_k P(X_k > x,\max_{j<k}|X_j|<x) P(S_n<-x) A_k = \{ |S_k| > x; S_j < x, j<k\} P(|S_n|>x) =\sum_{k=1}^n P(A_k; sgn(S_n-S_k)=sgn(S_k)) \geq =\frac{1}{2}...","['probability', 'inequality']"
42,Showing $E(X) = \sum_{i}E(X\mid A_i)P(A_i)$,Showing,E(X) = \sum_{i}E(X\mid A_i)P(A_i),"Following is my proof. Suppose $X$ is a discrete-type random variable ranging in the set $S$ and $\{A_i : i=1,2,3,\dots\}$ is a finite or countably infinite partition of a sample space $\Omega$ . We have $$P(X=x) = \sum_i{P(X=x\mid A_i)P(A_i)}$$ So $$xP(X=x) = \sum_i{xP(X=x\mid A_i)P(A_i)}$$ Thus $$ \begin{aligned} E(X) &= \sum_{x\in{S}}{xP(X=x)} \\ &= \sum_{x\in{S}}{\sum_i{xP(X=x\mid A_i)P(A_i)}} \\ &= \sum_i{\sum_{x\in{S}}{xP(X=x\mid A_i)P(A_i)}} \\ &= \sum_i{P(A_i)E(X\mid A_i)} \end{aligned} $$ Is this right?",Following is my proof. Suppose is a discrete-type random variable ranging in the set and is a finite or countably infinite partition of a sample space . We have So Thus Is this right?,"X S \{A_i : i=1,2,3,\dots\} \Omega P(X=x) = \sum_i{P(X=x\mid A_i)P(A_i)} xP(X=x) = \sum_i{xP(X=x\mid A_i)P(A_i)} 
\begin{aligned}
E(X) &= \sum_{x\in{S}}{xP(X=x)} \\
&= \sum_{x\in{S}}{\sum_i{xP(X=x\mid A_i)P(A_i)}} \\
&= \sum_i{\sum_{x\in{S}}{xP(X=x\mid A_i)P(A_i)}} \\
&= \sum_i{P(A_i)E(X\mid A_i)}
\end{aligned}
","['probability', 'probability-theory', 'expected-value']"
43,Is the rabbit constant the first natural math constant found to be non-normal?,Is the rabbit constant the first natural math constant found to be non-normal?,,"The rabbit constant (related to Fibonacci numbers), as well as proof of its non-normalcy, is discussed in one of my recent articles, here . One might argue that it is a semi-artificial number. I was wondering if besides that number, and excluding artificial numbers such as $0.100111100000000111\ldots$ , other irrational math constants are known to be non-normal (that is, with a digit distribution that is not uniform.)","The rabbit constant (related to Fibonacci numbers), as well as proof of its non-normalcy, is discussed in one of my recent articles, here . One might argue that it is a semi-artificial number. I was wondering if besides that number, and excluding artificial numbers such as , other irrational math constants are known to be non-normal (that is, with a digit distribution that is not uniform.)",0.100111100000000111\ldots,"['probability', 'number-theory', 'statistics']"
44,Two boys pick a subset of $40$ toys that they like. They can pick the same ones. What is the probability that they picked three same toys or more?,Two boys pick a subset of  toys that they like. They can pick the same ones. What is the probability that they picked three same toys or more?,40,"Two boys pick a subset of $40$ toys that they like. They can pick the same ones. What is the probability that they picked three same toys or more? My answer would be $$\frac{ \sum_{ i =3}^{40} \binom{40}{i} 3^{40-i}} { 2^{40} 2^{40}}.$$ Is that right? I would first pick the same toys that they picked, then for each of the remaining toys, I would either give to the first boy, second boy or nobody.","Two boys pick a subset of toys that they like. They can pick the same ones. What is the probability that they picked three same toys or more? My answer would be Is that right? I would first pick the same toys that they picked, then for each of the remaining toys, I would either give to the first boy, second boy or nobody.",40 \frac{ \sum_{ i =3}^{40} \binom{40}{i} 3^{40-i}} { 2^{40} 2^{40}}.,['probability']
45,"What is the probability that $A,B,C$ are independent",What is the probability that  are independent,"A,B,C","My lecturer posed a question in class which I have tried to solve with no success, any help would be much appreciated. Let $\Omega = {1,2,3,...,n}$ be the sample space. Choose 3 events randomly: A,B,C such that $A,B,C \subseteq P(\Omega)$ where $P(\Omega)$ denotes the power set. What is the probability that A,B and B,C and A,C are all independent? We are taking the uniform distribution on $\Omega$ and on $P(\Omega)$","My lecturer posed a question in class which I have tried to solve with no success, any help would be much appreciated. Let be the sample space. Choose 3 events randomly: A,B,C such that where denotes the power set. What is the probability that A,B and B,C and A,C are all independent? We are taking the uniform distribution on and on","\Omega = {1,2,3,...,n} A,B,C \subseteq P(\Omega) P(\Omega) \Omega P(\Omega)","['probability', 'statistics']"
46,Calculating Probability of a given X range if X is a continuous random variable with a given PDF.,Calculating Probability of a given X range if X is a continuous random variable with a given PDF.,,"This question was given to me as a review for an upcoming exam. Find $P(2 \leq X \leq 3)$ if $X$ is a continous random variable with pdf: $$f_X(x) =  \begin{cases}        xe^{-x} & x\geq 0 \\       0 & x \lt 0    \end{cases} $$ My work: $$P(2 \leq X \leq 3) = \int_{2}^{3}xe^{-x}dx$$ Integration by parts: $u=x,u'=1,v'=e^{-x},v=-e^{-x}$ $$\int_{2}^{3}xe^{-x} dx = -xe^{-x}\Big|^3_2-\int_{2}^{3}-e^{-x}dx\\ =\frac{-4}{e^3} + \frac{3}{e^2} \approx 0.2069$$ Did I miss anything or do something incorrectly? Also, I had a quick question regarding a different case, let's say problem was asking for $P(-1 \leq X \leq 3)$ instead, would we do this instead?: $$P(-1 \leq X \leq 3)= \int_{-1}^{0}0 dx + \int_0^3xe^{-x}dx$$ Obviously the left integral will be 0 in this case, I'm just wondering if we split up the integrals with addition if the range we are looking for is in two different domains of the PDF.","This question was given to me as a review for an upcoming exam. Find if is a continous random variable with pdf: My work: Integration by parts: Did I miss anything or do something incorrectly? Also, I had a quick question regarding a different case, let's say problem was asking for instead, would we do this instead?: Obviously the left integral will be 0 in this case, I'm just wondering if we split up the integrals with addition if the range we are looking for is in two different domains of the PDF.","P(2 \leq X \leq 3) X f_X(x) =
 \begin{cases} 
      xe^{-x} & x\geq 0 \\
      0 & x \lt 0
   \end{cases}
 P(2 \leq X \leq 3) = \int_{2}^{3}xe^{-x}dx u=x,u'=1,v'=e^{-x},v=-e^{-x} \int_{2}^{3}xe^{-x} dx = -xe^{-x}\Big|^3_2-\int_{2}^{3}-e^{-x}dx\\
=\frac{-4}{e^3} + \frac{3}{e^2} \approx 0.2069 P(-1 \leq X \leq 3) P(-1 \leq X \leq 3)= \int_{-1}^{0}0 dx + \int_0^3xe^{-x}dx","['probability', 'statistics']"
47,Probability of third ball drawn,Probability of third ball drawn,,Having a box containing 15 red and 7 blue balls. We want to draw 3 balls at random by these conditions on each draw: If the ball is red you set it aside If the ball is blue put it back in What is probability that the third draw is blue? (If you get a blue ball it counts as a draw even though you put it back in the box.) Can I say by these rules? We have three balls drawn the total possibility is the sum of: Both the first two balls are red Both the first two balls are blue The first ball is red and the second is blue The first ball is blue and the second is red. Then for each case I can calculate the probability of blue ball at third draw. $ P=\frac{15}{22}*\frac{14}{21}*\frac{7}{20}+\frac{7}{22}*\frac{7}{22}*\frac{7}{22}+\frac{7}{22}*\frac{15}{22}*\frac{7}{21}+\frac{15}{22}*\frac{7}{21}*\frac{7}{21} $,Having a box containing 15 red and 7 blue balls. We want to draw 3 balls at random by these conditions on each draw: If the ball is red you set it aside If the ball is blue put it back in What is probability that the third draw is blue? (If you get a blue ball it counts as a draw even though you put it back in the box.) Can I say by these rules? We have three balls drawn the total possibility is the sum of: Both the first two balls are red Both the first two balls are blue The first ball is red and the second is blue The first ball is blue and the second is red. Then for each case I can calculate the probability of blue ball at third draw.,"
P=\frac{15}{22}*\frac{14}{21}*\frac{7}{20}+\frac{7}{22}*\frac{7}{22}*\frac{7}{22}+\frac{7}{22}*\frac{15}{22}*\frac{7}{21}+\frac{15}{22}*\frac{7}{21}*\frac{7}{21}
","['probability', 'balls-in-bins']"
48,Product of Normal and independent log-Normal. What is the density?,Product of Normal and independent log-Normal. What is the density?,,"Let $X$ and $Y$ be independent standard Gaussian random variabless -- i.e., $N(0,1)$ . Let $\sigma \in \mathbb{R}$ . Define $$Z=X \cdot e^{\sigma \cdot Y}.$$ Is there a closed form expression for the density of $Z$ ? The density can be expressed as either of the following two integrals. $$\mathsf{pdf}_Z(z) = \frac{1}{2\pi}\int_{-\infty}^\infty \exp\left(\frac{-z^2 \cdot e^{-2\sigma y} - y^2 - 2\sigma y}{2}\right) \mathrm{d}y$$ $$~~~~~~~~~~~~~~~~~~~=\frac{1}{2\pi|\sigma z|} \int_0^\infty \exp\left(\frac{-\sigma^2 x^2 - (\log(x/|z|))^2}{2\sigma^2}\right) \mathrm{d}x.$$ I think of $Z$ as being a symmetric version of the log-Normal distribution or, alternatively, a heavy-tailed version of the Normal distribution. I expect the density to resemble that of the log-Normal far from the origin, namely something like $e^{-(\log z)^2}$ . I would like an exact expression in order to be able to compute things about this distribution. Alas, computing this is beyond my powers of integration. Any help would be appreciated.","Let and be independent standard Gaussian random variabless -- i.e., . Let . Define Is there a closed form expression for the density of ? The density can be expressed as either of the following two integrals. I think of as being a symmetric version of the log-Normal distribution or, alternatively, a heavy-tailed version of the Normal distribution. I expect the density to resemble that of the log-Normal far from the origin, namely something like . I would like an exact expression in order to be able to compute things about this distribution. Alas, computing this is beyond my powers of integration. Any help would be appreciated.","X Y N(0,1) \sigma \in \mathbb{R} Z=X \cdot e^{\sigma \cdot Y}. Z \mathsf{pdf}_Z(z) = \frac{1}{2\pi}\int_{-\infty}^\infty \exp\left(\frac{-z^2 \cdot e^{-2\sigma y} - y^2 - 2\sigma y}{2}\right) \mathrm{d}y ~~~~~~~~~~~~~~~~~~~=\frac{1}{2\pi|\sigma z|} \int_0^\infty \exp\left(\frac{-\sigma^2 x^2 - (\log(x/|z|))^2}{2\sigma^2}\right) \mathrm{d}x. Z e^{-(\log z)^2}","['probability', 'integration', 'probability-distributions', 'normal-distribution', 'density-function']"
49,Filtration vs Natural Filtration,Filtration vs Natural Filtration,,"I have been reading about natural filtrations, and I think I have a good grasp of them. However, I also found out that natural filtrations are just one type of filtration in general. Now I am not able to imagine a filtration other than a natural filtration which would serve a purpose. So the question is, how does a filtration generalize the idea of a natural filtration, and in particular, where would I use a filtration which is not a natural filtration. (For context, I am studying valuation of derivative contracts, and they are obviously riddled with natural filtrations.)","I have been reading about natural filtrations, and I think I have a good grasp of them. However, I also found out that natural filtrations are just one type of filtration in general. Now I am not able to imagine a filtration other than a natural filtration which would serve a purpose. So the question is, how does a filtration generalize the idea of a natural filtration, and in particular, where would I use a filtration which is not a natural filtration. (For context, I am studying valuation of derivative contracts, and they are obviously riddled with natural filtrations.)",,"['probability', 'measure-theory']"
50,Expected value of falling factorials from axioms of Poisson process,Expected value of falling factorials from axioms of Poisson process,,"Falling factorial, $(x)_n$ , is the product of biggest $n$ terms in factorial, $(x)_n = x(x-1)(x-2)\cdot \ldots \cdot (x-n+1)$ . Or the number of ways to color the set of $n$ objects into different colors if you have $x$ possible colors. Using formula for probabilities of Poisson random variable $X$ one can find that $E((X)_n)=\lambda^n$ . Derivation with $\lambda=1$ . One may also use probability generating function. But I guess the answer is too beautiful to obtain it by boring summation :) Starting from three axioms one may find $E(X)=\lambda$ without calculating probabilities. Just divide the interval $[0;1]$ into $n$ parts and make $n$ goes to infinity. I wonder whether it is possible to find all the expected values $E((X)_n)$ directly from the defining axioms of Poisson process (without derivation of probabilities)??? not a homework — just curiosity :)","Falling factorial, , is the product of biggest terms in factorial, . Or the number of ways to color the set of objects into different colors if you have possible colors. Using formula for probabilities of Poisson random variable one can find that . Derivation with . One may also use probability generating function. But I guess the answer is too beautiful to obtain it by boring summation :) Starting from three axioms one may find without calculating probabilities. Just divide the interval into parts and make goes to infinity. I wonder whether it is possible to find all the expected values directly from the defining axioms of Poisson process (without derivation of probabilities)??? not a homework — just curiosity :)",(x)_n n (x)_n = x(x-1)(x-2)\cdot \ldots \cdot (x-n+1) n x X E((X)_n)=\lambda^n \lambda=1 E(X)=\lambda [0;1] n n E((X)_n),"['probability', 'poisson-distribution']"
51,Probability distribution to maximize the expected distance between two points,Probability distribution to maximize the expected distance between two points,,"Let $M$ be some metric space. We will also say that $M$ is a measurable space . How do you find a probability measure $P$ on $M$ that maximizes the expected distance between two points? (That is, $P$ maximizes $E[d(X,Y)]$ , where $X$ and $Y$ are independent random variables with distribution $P$ .) If $M$ is a discrete metric, then $P$ will the uniform distribution, for example. Note that we can think of this as a game, and $P$ as a mixed strategy . You would have two players, which are both trying to maximize the distance. This is not a purely game-theoretical situation though, since we need to require the players to pick the same strategy. There is something called superrationality that might apply, but it is not well studied though. Maybe techniques similar to those used in game theory could be used though.","Let be some metric space. We will also say that is a measurable space . How do you find a probability measure on that maximizes the expected distance between two points? (That is, maximizes , where and are independent random variables with distribution .) If is a discrete metric, then will the uniform distribution, for example. Note that we can think of this as a game, and as a mixed strategy . You would have two players, which are both trying to maximize the distance. This is not a purely game-theoretical situation though, since we need to require the players to pick the same strategy. There is something called superrationality that might apply, but it is not well studied though. Maybe techniques similar to those used in game theory could be used though.","M M P M P E[d(X,Y)] X Y P M P P","['probability', 'probability-theory', 'optimization', 'metric-spaces', 'expected-value']"
52,Probability for a boolean matrix from a certain class to have full rank,Probability for a boolean matrix from a certain class to have full rank,,"While considering a certain type of computational problems, I have encountered the following probabilistic problem over the two element field $GF_{2}$ . For $c\in \mathbb{Q}_{>0}$ , let $p_{c}(n)$ be the probability for a matrix over $GF_{2}$ with $n$ columns and $c\cdot n$ rows, such that each row contains precisely $3$ non-zero entries, to have a full rank. Is there a $c\geq 1$ with $\lim_{n\rightarrow \infty} p_{c}(n)>0 $ ? Im only interested in the case $c>1$ ; there are results of Calkin from 1997 that solve the case $c\leq 1$ , see Dependent Sets of Constant Weight Binary Vectors . Calkin proved that there exists a sharp treshold $0<\beta<1$ with $\lim_{n\rightarrow \infty} p_{c}(n)=1$ for $c\in (0,\beta)$ and $\lim_{n\rightarrow \infty} p_{c}(n)=0$ for $c\in (\beta,1]$ . Now if I am not mistaken Calkin's results do not have any implications for the case $c>1$ . I would be very happy to see $\lim_{n\rightarrow \infty} p_{c}(n)>0 $ for some $c>1$ and thought that some of you guys might know these kind of problems or simply see an obvious answer. I have been googling for a while without any success. Ps.: There is a positive answer already for $c=1$ if one considers arbitrary $(c\cdot n) \times n$ matrices over $GF_{2}$ . A simple counting exercise shows $p_{1}(n)=(1-1/2)\cdot ... \cdot (1-1/2^{n})$ whose limit lies somewhere in $(0,1)$ (my source is https://arxiv.org/pdf/math/0102059.pdf , p. 38). However, these behave very differently compared to the $3$ -nonzero entries case.","While considering a certain type of computational problems, I have encountered the following probabilistic problem over the two element field . For , let be the probability for a matrix over with columns and rows, such that each row contains precisely non-zero entries, to have a full rank. Is there a with ? Im only interested in the case ; there are results of Calkin from 1997 that solve the case , see Dependent Sets of Constant Weight Binary Vectors . Calkin proved that there exists a sharp treshold with for and for . Now if I am not mistaken Calkin's results do not have any implications for the case . I would be very happy to see for some and thought that some of you guys might know these kind of problems or simply see an obvious answer. I have been googling for a while without any success. Ps.: There is a positive answer already for if one considers arbitrary matrices over . A simple counting exercise shows whose limit lies somewhere in (my source is https://arxiv.org/pdf/math/0102059.pdf , p. 38). However, these behave very differently compared to the -nonzero entries case.","GF_{2} c\in \mathbb{Q}_{>0} p_{c}(n) GF_{2} n c\cdot n 3 c\geq 1 \lim_{n\rightarrow \infty} p_{c}(n)>0  c>1 c\leq 1 0<\beta<1 \lim_{n\rightarrow \infty} p_{c}(n)=1 c\in (0,\beta) \lim_{n\rightarrow \infty} p_{c}(n)=0 c\in (\beta,1] c>1 \lim_{n\rightarrow \infty} p_{c}(n)>0  c>1 c=1 (c\cdot n) \times n GF_{2} p_{1}(n)=(1-1/2)\cdot ... \cdot (1-1/2^{n}) (0,1) 3","['probability', 'combinatorics', 'matrices', 'finite-fields', 'matrix-rank']"
53,Probability of more than n machines down any hour?,Probability of more than n machines down any hour?,,"Suppose we have $N$ identical machines, at any given hour, there's a chance $P$ that any given machine went down. A down machine takes $T$ hours to recover. How do I calculate the chances that in a given longer interval $Y$ (assume $Y >> T$ ), what are the probability that there exists hour $t$ , $0 < t < Y$ , such that at $t$ there are more than $R$ machines that are down?","Suppose we have identical machines, at any given hour, there's a chance that any given machine went down. A down machine takes hours to recover. How do I calculate the chances that in a given longer interval (assume ), what are the probability that there exists hour , , such that at there are more than machines that are down?",N P T Y Y >> T t 0 < t < Y t R,"['probability', 'probability-distributions']"
54,Maximum estimator in upper Chernoff bound,Maximum estimator in upper Chernoff bound,,"I have the following exercise about Chernoff bounds: Let $X_{1}, X_{2}, \dots, X_{n}$ be independent, identically distributed (i.i.d) random variables with distribution $N(0,\sigma^{2})$ . Show that for every $\varepsilon > 0$ : $$P(\overline{X_{n}} > \varepsilon) \le e^{\frac{-n\varepsilon^{2}}{2\sigma^{2}}}$$ A random variable $X$ with $E(X) = 0$ is said to be Subgaussian for the parameter $\sigma > 0$ if its moment generating function $M_{x}(t)$ is such that $M_{x}(t) \le e^{\frac{t^{2}\sigma^{2}}{2}}$ for all $t \in \mathbb{R} $ . Show that the inequality in the previous point holds if $X_{1}, X_{2}, \dots, X_{n}$ are i.i.d random variables, and Subgaussian for the parameter $\sigma > 0$ In the first point I know I am working with Chernoff bounds since the exercise already gives its upper tail, with $\mu = 0$ , so I have something like: $$P(\overline{X_{n}} > \varepsilon +  0) \le e^{-ng(t)}$$ Where $$g(t) = \varepsilon t - \log M_{X_{1}}(t)$$ Recalling that, in this case, $$M_{X_{1}}(t) = e^{\frac{t^{2}\sigma^{2}}{2}}$$ We have $$g(t) = \varepsilon t - \frac{t^{2}\sigma^{2}}{2}$$ Now here comes the problem. In order to discover my upper tail, I have to find a maximum estimator $t^{\star}$ so I can compute $g(t^{*})$ . The solution of the exercise says $t^{\star} = \frac{\varepsilon}{\sigma^{2}}$ , but I don't know why. So, what I ask is how I can find maximum estimators when it comes to this type of exercises. About the second point, since I already have $g(t^{\star})$ , how can this result will be useful to prove the inequality for all the Subgaussian. Wasn't this implicitly proved in the previous point?","I have the following exercise about Chernoff bounds: Let be independent, identically distributed (i.i.d) random variables with distribution . Show that for every : A random variable with is said to be Subgaussian for the parameter if its moment generating function is such that for all . Show that the inequality in the previous point holds if are i.i.d random variables, and Subgaussian for the parameter In the first point I know I am working with Chernoff bounds since the exercise already gives its upper tail, with , so I have something like: Where Recalling that, in this case, We have Now here comes the problem. In order to discover my upper tail, I have to find a maximum estimator so I can compute . The solution of the exercise says , but I don't know why. So, what I ask is how I can find maximum estimators when it comes to this type of exercises. About the second point, since I already have , how can this result will be useful to prove the inequality for all the Subgaussian. Wasn't this implicitly proved in the previous point?","X_{1}, X_{2}, \dots, X_{n} N(0,\sigma^{2}) \varepsilon > 0 P(\overline{X_{n}} > \varepsilon) \le e^{\frac{-n\varepsilon^{2}}{2\sigma^{2}}} X E(X) = 0 \sigma > 0 M_{x}(t) M_{x}(t) \le e^{\frac{t^{2}\sigma^{2}}{2}} t \in \mathbb{R}  X_{1}, X_{2}, \dots, X_{n} \sigma > 0 \mu = 0 P(\overline{X_{n}} > \varepsilon +  0) \le e^{-ng(t)} g(t) = \varepsilon t - \log M_{X_{1}}(t) M_{X_{1}}(t) = e^{\frac{t^{2}\sigma^{2}}{2}} g(t) = \varepsilon t - \frac{t^{2}\sigma^{2}}{2} t^{\star} g(t^{*}) t^{\star} = \frac{\varepsilon}{\sigma^{2}} g(t^{\star})","['probability', 'statistics', 'inequality', 'random-variables', 'parameter-estimation']"
55,Is the sigma algebra generated by $X$ random variable and its square equal to the sigma algebra generated by $X$ alone?,Is the sigma algebra generated by  random variable and its square equal to the sigma algebra generated by  alone?,X X,"I would like to understand which relationships hold among the sigma algebras $\sigma(X, X^2)$ , $\sigma(X)$ and $\sigma(X^2)$ , where X is a random variable. I would expect that $\sigma(X, X^2)=\sigma(X)$ . If this is true, then is $X^2$ $\sigma(X)$ -measurable? Moreover, I would like to know if there exist more sigma algebras w.r.t. a process is adapted. I am thinking that if $X_i$ are i.i.d. than $S_n=\sum_{i=1}^n X_i$ is $\mathcal{F}_n:=\sigma(S_n)$ -measurable but also $\mathcal{G}_n:=\sigma(X_1, X_2, \dots, X_n)$ -measurable. What changes if I take $\{\mathcal{F}_n\}$ instead of $\{\mathcal{G}_n\}$ as a filtration for the process $\{S_n\}$ ? Because for both the process is a martingale!","I would like to understand which relationships hold among the sigma algebras , and , where X is a random variable. I would expect that . If this is true, then is -measurable? Moreover, I would like to know if there exist more sigma algebras w.r.t. a process is adapted. I am thinking that if are i.i.d. than is -measurable but also -measurable. What changes if I take instead of as a filtration for the process ? Because for both the process is a martingale!","\sigma(X, X^2) \sigma(X) \sigma(X^2) \sigma(X, X^2)=\sigma(X) X^2 \sigma(X) X_i S_n=\sum_{i=1}^n X_i \mathcal{F}_n:=\sigma(S_n) \mathcal{G}_n:=\sigma(X_1, X_2, \dots, X_n) \{\mathcal{F}_n\} \{\mathcal{G}_n\} \{S_n\}","['probability', 'measure-theory', 'stochastic-processes', 'martingales']"
56,spaghetti hoops combinatorics variation,spaghetti hoops combinatorics variation,,"You may have heard about the classic spaghetti hoops combinatorics problem, which has been stated like this: ""You have N pieces of rope in a bucket. You reach in and grab one end-piece, then reach in and grab another end-piece, and tie those two together. What is the expected value of the number of loops in the bucket?"" The solution is straight forward and involves defining two events: event that you close the loop, vs. event that you just extend the path you have without forming a loop. My question is what happens when we generalize this a bit so that the objects no longer need to have 2 endpoints? For simplicty lets say the objects all still have the same number $E$ of endpoints. But some interesting things happen: if $E$ is odd, the objects can no longer close up on themselves, e.g. for the $E$ =2 case a loop could form to connect a single object to itself, but now that cannot happen. When $E$ =even, now this can happen again. So specifically, let's define a ""tangle"" as the generalization of a loop, i.e. a ""tangle"" is a group of connected objects, so each object in the tangle is connected by at least 1 edge to at least one other object in that tangle. Let's call the ""size"" of the tangle the number of objects that are in the tangle. My question is: given $N$ objects, for number of endpoints per object $E\geq3$ , what is the expected number of ""tangles""? Bonus points if you can give the expected number of tangles of a given size $S$ ?","You may have heard about the classic spaghetti hoops combinatorics problem, which has been stated like this: ""You have N pieces of rope in a bucket. You reach in and grab one end-piece, then reach in and grab another end-piece, and tie those two together. What is the expected value of the number of loops in the bucket?"" The solution is straight forward and involves defining two events: event that you close the loop, vs. event that you just extend the path you have without forming a loop. My question is what happens when we generalize this a bit so that the objects no longer need to have 2 endpoints? For simplicty lets say the objects all still have the same number of endpoints. But some interesting things happen: if is odd, the objects can no longer close up on themselves, e.g. for the =2 case a loop could form to connect a single object to itself, but now that cannot happen. When =even, now this can happen again. So specifically, let's define a ""tangle"" as the generalization of a loop, i.e. a ""tangle"" is a group of connected objects, so each object in the tangle is connected by at least 1 edge to at least one other object in that tangle. Let's call the ""size"" of the tangle the number of objects that are in the tangle. My question is: given objects, for number of endpoints per object , what is the expected number of ""tangles""? Bonus points if you can give the expected number of tangles of a given size ?",E E E E N E\geq3 S,"['probability', 'combinatorics', 'recreational-mathematics', 'puzzle', 'recursion']"
57,Find the probability that the roots of the quadratic $U_1x^2+U_2x+U_3$ are real,Find the probability that the roots of the quadratic  are real,U_1x^2+U_2x+U_3,"The question, from the textbook: Mathematical Statistics and Data Analysis Let $U_1, U_2, U_3$ be independent random variables uniform on $[0,1]$ . Find the probability that the roots of the quadratic $U_1x^2+U_2x+U_3$ are real. I know this question has already been asked on StackExchange but I'd like to present my incorrect attempt at it in the hopes that someone could tell me where I went wrong. So this question boils down to finding $P(U_2^2-4U_1U_3\ge 0)$ which is the discriminant. Which is equivalent to $1-P(-\sqrt{4U_1U_3} \lt U_2\lt \sqrt{4U_1U_3})$ Since all three random variables are uniform on $[0,1]$ , their density function would be just be $1$ . Putting it all together, I get the triple integral of $\int_{0}^{1}\int_{0}^{1}\int_{-\sqrt{4u_1u_3}}^{\sqrt{4u_1u_3}}du_2du_1du_3$ Which is what I think should equal to $P(-\sqrt{4U_1U_3} \lt U_2\lt \sqrt{4U_1U_3})$ The triple integral turns out to be a number greater than 1 which is obviously wrong. Where did I go wrong? Can anything be salvaged here, perhaps a triple integral with different bounds? I saw the solution to this question done by someone else: Probability that a quadratic polynomial with random coefficients has real roots but I don't think I would be able to think of something like that in a test setting. Any pointers would be much appreciated!","The question, from the textbook: Mathematical Statistics and Data Analysis Let be independent random variables uniform on . Find the probability that the roots of the quadratic are real. I know this question has already been asked on StackExchange but I'd like to present my incorrect attempt at it in the hopes that someone could tell me where I went wrong. So this question boils down to finding which is the discriminant. Which is equivalent to Since all three random variables are uniform on , their density function would be just be . Putting it all together, I get the triple integral of Which is what I think should equal to The triple integral turns out to be a number greater than 1 which is obviously wrong. Where did I go wrong? Can anything be salvaged here, perhaps a triple integral with different bounds? I saw the solution to this question done by someone else: Probability that a quadratic polynomial with random coefficients has real roots but I don't think I would be able to think of something like that in a test setting. Any pointers would be much appreciated!","U_1, U_2, U_3 [0,1] U_1x^2+U_2x+U_3 P(U_2^2-4U_1U_3\ge 0) 1-P(-\sqrt{4U_1U_3} \lt U_2\lt \sqrt{4U_1U_3}) [0,1] 1 \int_{0}^{1}\int_{0}^{1}\int_{-\sqrt{4u_1u_3}}^{\sqrt{4u_1u_3}}du_2du_1du_3 P(-\sqrt{4U_1U_3} \lt U_2\lt \sqrt{4U_1U_3})","['probability', 'integration', 'proof-verification', 'probability-distributions', 'uniform-distribution']"
58,Distribution of no. of siblings of a random child if the no. of children of a family is Poisson distributed,Distribution of no. of siblings of a random child if the no. of children of a family is Poisson distributed,,"Consider a large population of families, and suppose that the number of children in the different families are independent Poisson random variables with mean $\lambda$ . Show that the number of siblings of a randomly chosen child is also Poisson distributed with mean $\lambda$ . My approach: For any random child, if it has $k$ siblings, it implies that its parent had $k+1$ children. Hence, if $S =$ no. of siblings and if $C =$ no. of children I'm not sure how to proceed after this. I tried evaluating the mean of S, by computing I'm not sure where I'm going wrong and how to proceed. EDIT: Found an answer in one of the solution manuals. Basically, The probability of choosing a child that has $j$ siblings is the fraction of total children that have $j$ siblings. Now, if $Z$ is the total number of families, hence the total no. of children would be $\lambda Z$ . Also, if $P(j+1)$ is the probability that a family has $j+1$ children, then the no. of families with $j+1$ children is $Z \cdot P(j+1)$ . Also, each of this family has $(j+1)$ children, each of whom have $j$ siblings. Hence, there are in total children each having $j$ siblings. This divided by total number of children gives the fraction of children with $j$ siblings, i.e. Clearly my answer is wrong in the first step itself. However I'm not able to articulate why the initial step is incorrect.","Consider a large population of families, and suppose that the number of children in the different families are independent Poisson random variables with mean . Show that the number of siblings of a randomly chosen child is also Poisson distributed with mean . My approach: For any random child, if it has siblings, it implies that its parent had children. Hence, if no. of siblings and if no. of children I'm not sure how to proceed after this. I tried evaluating the mean of S, by computing I'm not sure where I'm going wrong and how to proceed. EDIT: Found an answer in one of the solution manuals. Basically, The probability of choosing a child that has siblings is the fraction of total children that have siblings. Now, if is the total number of families, hence the total no. of children would be . Also, if is the probability that a family has children, then the no. of families with children is . Also, each of this family has children, each of whom have siblings. Hence, there are in total children each having siblings. This divided by total number of children gives the fraction of children with siblings, i.e. Clearly my answer is wrong in the first step itself. However I'm not able to articulate why the initial step is incorrect.",\lambda \lambda k k+1 S = C = j j Z \lambda Z P(j+1) j+1 j+1 Z \cdot P(j+1) (j+1) j j j,"['probability', 'probability-distributions', 'poisson-distribution']"
59,"Total probability of a match, where first to $n$ games wins.","Total probability of a match, where first to  games wins.",n,"In a match between two players A and B, the first to win $n$ games wins the match. Player A wins a game with probability $p$ , the outcomes of each game are independent. (The probability of player B winning a game is $q=1-p$ .) I've found that the probability that A wins the match with a scoreline of $n:k$ is $${n+k-1\choose k}p^nq^k\ .$$ Here is my problem: how to prove that the total probability is $1$ , i.e. $$p^n\bigg[1+{n\choose 1}q+{n+1\choose2}q^2+\cdots+{2n-2\choose n-1}q^{n-1}\bigg]+q^n\bigg[1+{n\choose 1}p+{n+1\choose2}p^2+\cdots+{2n-2\choose n-1}p^{n-1}\bigg]=1?$$ I've tried induction but it seems impenetrable.","In a match between two players A and B, the first to win games wins the match. Player A wins a game with probability , the outcomes of each game are independent. (The probability of player B winning a game is .) I've found that the probability that A wins the match with a scoreline of is Here is my problem: how to prove that the total probability is , i.e. I've tried induction but it seems impenetrable.",n p q=1-p n:k {n+k-1\choose k}p^nq^k\ . 1 p^n\bigg[1+{n\choose 1}q+{n+1\choose2}q^2+\cdots+{2n-2\choose n-1}q^{n-1}\bigg]+q^n\bigg[1+{n\choose 1}p+{n+1\choose2}p^2+\cdots+{2n-2\choose n-1}p^{n-1}\bigg]=1?,['probability']
60,Find the probability that the pedestrian has to wait for exactly $4$ seconds before starting to cross.,Find the probability that the pedestrian has to wait for exactly  seconds before starting to cross.,4,"From Statistical Inference (2nd edition) by Casella Berger The flow of traffic at certain street corner can sometimes be modelled as a sequence of Bernoulli trails by assuming that the probability of a car passing during any given second is a constant $p$ and that there is no interaction between the passing of cars at different seconds. If we treat seconds as indivisible time units (trials), the Bernoulli model applies. Suppose a pedestrian can cross the street only if no car is to pass during the next $3$ seconds. Find the probability that the pedestrian has to wait for exactly $4$ seconds before starting to cross. The suggested answer is: $$(1-p(1-p)^3)(1-p)^3$$ And the reasoning is that: the last three seconds must have no car passing (explains the last $(1-p)^3$ ). In considering what happened to the first four seconds, we have to exclude the situation in which the first-second having car passing and the last three seconds having no car passing (explains the term $1-(1-p)^3$ ). Assuming independence between two parts, we multiply these two terms together to get the probability required. However, I find answer $(1-(1-p)^3)p(1-p)^3$ makes better sense. The reasoning is that if we want the pedestrian to wait for exactly $4$ seconds, the fourth second must have car passing, otherwise the pedestrian would cross the street one second earlier. Could someone please explain to me why the first answer is the correct one and what are the points that I have missed when I provides the alternative explanation?","From Statistical Inference (2nd edition) by Casella Berger The flow of traffic at certain street corner can sometimes be modelled as a sequence of Bernoulli trails by assuming that the probability of a car passing during any given second is a constant and that there is no interaction between the passing of cars at different seconds. If we treat seconds as indivisible time units (trials), the Bernoulli model applies. Suppose a pedestrian can cross the street only if no car is to pass during the next seconds. Find the probability that the pedestrian has to wait for exactly seconds before starting to cross. The suggested answer is: And the reasoning is that: the last three seconds must have no car passing (explains the last ). In considering what happened to the first four seconds, we have to exclude the situation in which the first-second having car passing and the last three seconds having no car passing (explains the term ). Assuming independence between two parts, we multiply these two terms together to get the probability required. However, I find answer makes better sense. The reasoning is that if we want the pedestrian to wait for exactly seconds, the fourth second must have car passing, otherwise the pedestrian would cross the street one second earlier. Could someone please explain to me why the first answer is the correct one and what are the points that I have missed when I provides the alternative explanation?",p 3 4 (1-p(1-p)^3)(1-p)^3 (1-p)^3 1-(1-p)^3 (1-(1-p)^3)p(1-p)^3 4,['probability']
61,Max min normalization on random variables,Max min normalization on random variables,,"Suppose I have three i.i.d. random variables $X_1, X_2, X_3$, and I do ""max-min normalization"" on them. $$X_i \mapsto \frac{X_i - \min\limits_i X_i}{\max\limits_i X_i - \min\limits_i X_i} $$ Let $Y$ be the location of the ""middle"" point after normalization. (It lies in $[0,1]$.) What is the distribution of $Y$? How can I extend to the case of $n$ i.i.d. variables $X_1, \dotsc, X_n$, asking about the distribution on the $n-2$ middle points? We could give $X_i$ specific distributions such as $\mathcal{N}(0,1)$ for concreteness.","Suppose I have three i.i.d. random variables $X_1, X_2, X_3$, and I do ""max-min normalization"" on them. $$X_i \mapsto \frac{X_i - \min\limits_i X_i}{\max\limits_i X_i - \min\limits_i X_i} $$ Let $Y$ be the location of the ""middle"" point after normalization. (It lies in $[0,1]$.) What is the distribution of $Y$? How can I extend to the case of $n$ i.i.d. variables $X_1, \dotsc, X_n$, asking about the distribution on the $n-2$ middle points? We could give $X_i$ specific distributions such as $\mathcal{N}(0,1)$ for concreteness.",,"['probability', 'probability-distributions']"
62,CLT for triangular array of globally bounded random variables,CLT for triangular array of globally bounded random variables,,"I am interested in the following statement: A triangular array of globally bounded real non-constant random variables converges in distribution towards the standard Gaussian distribution if and only if its variance goes towards infinity. In technical terms, this is Fix $C>0$ and for each $n \in \mathbb{N}_+$ and $1 \leq i \leq n$ let $X_{i,n}$ be a random variable with support contained in $[-C,C]\subset \mathbb{R}$ , zero mean and nonzero variance $s_{i,n}^{2} > 0$ , and set $X_{n} = \sum_{i=1}^{n} X_{i,n}$ and $s_n^2 = \mathbb{V}(X_n) = \sum_i s_{i,n}^2$ . Then $$X_{n} / s_n \rightarrow N(0,1) \Longleftrightarrow s_n \rightarrow \infty.$$ The reverse implication follows from the observation that the Lindeberg condition is satisfied (because each individual $X_{i,n}$ is bounded). But what about the forward implication? I would like to argue as follows: If $s_n \rightarrow S < \infty$ then the limit distribution $\lim_{n \rightarrow \infty} X_n/s_n$ exists and is not equal to $N(0,1)$ because well, I don't know , maybe an explicit computation or because of Cramer's decomposition theorem ??? Now if $0 < s_n \not\rightarrow \infty$ then it has a bounded and thus a converging subsequence $s_{m_n} \rightarrow S < \infty$ . By 1., the corresponding subsequence $X_{m_n}$ of $X_n$ does not converge towards $N(0,1)$ so $X_n$ itself cannot converge towards $N(0,1)$ either. The first part is unclear to me, and I used in the second part that, if $X_n$ converges in distribution to $X$ , then also every subsequence of $X_n$ converges towards $X$ . This seems a direct consequence of the definition of convergence in distribution, but I have not found this statement anywhere written down in a text book. I would very much appreciate pointers to references where these situations are treated and also some words on the correctness of my arguments. Also, the situation I am actually needing is the above with the additional property that all var's are boolean.","I am interested in the following statement: A triangular array of globally bounded real non-constant random variables converges in distribution towards the standard Gaussian distribution if and only if its variance goes towards infinity. In technical terms, this is Fix and for each and let be a random variable with support contained in , zero mean and nonzero variance , and set and . Then The reverse implication follows from the observation that the Lindeberg condition is satisfied (because each individual is bounded). But what about the forward implication? I would like to argue as follows: If then the limit distribution exists and is not equal to because well, I don't know , maybe an explicit computation or because of Cramer's decomposition theorem ??? Now if then it has a bounded and thus a converging subsequence . By 1., the corresponding subsequence of does not converge towards so itself cannot converge towards either. The first part is unclear to me, and I used in the second part that, if converges in distribution to , then also every subsequence of converges towards . This seems a direct consequence of the definition of convergence in distribution, but I have not found this statement anywhere written down in a text book. I would very much appreciate pointers to references where these situations are treated and also some words on the correctness of my arguments. Also, the situation I am actually needing is the above with the additional property that all var's are boolean.","C>0 n \in \mathbb{N}_+ 1 \leq i \leq n X_{i,n} [-C,C]\subset \mathbb{R} s_{i,n}^{2} > 0 X_{n} = \sum_{i=1}^{n} X_{i,n} s_n^2 = \mathbb{V}(X_n) = \sum_i s_{i,n}^2 X_{n} / s_n \rightarrow N(0,1) \Longleftrightarrow s_n \rightarrow \infty. X_{i,n} s_n \rightarrow S < \infty \lim_{n \rightarrow \infty} X_n/s_n N(0,1) 0 < s_n \not\rightarrow \infty s_{m_n} \rightarrow S < \infty X_{m_n} X_n N(0,1) X_n N(0,1) X_n X X_n X","['probability', 'reference-request', 'central-limit-theorem']"
63,Anti-concentration of Chi squared random variable,Anti-concentration of Chi squared random variable,,"Most of the concentration bounds for chi squared are centered at it's mean.  I am wondering if there are known exponential bounds on the probability of being near zero, meaning $ \mathbb{P}(\chi^2_k \leq \varepsilon \mathbb{E} \chi^2_k ) \leq ?? $ I have tried using the Payley-Zygmund inequality, but this only gives an inverse polynomial bound (in terms of k).","Most of the concentration bounds for chi squared are centered at it's mean.  I am wondering if there are known exponential bounds on the probability of being near zero, meaning $ \mathbb{P}(\chi^2_k \leq \varepsilon \mathbb{E} \chi^2_k ) \leq ?? $ I have tried using the Payley-Zygmund inequality, but this only gives an inverse polynomial bound (in terms of k).",,"['probability', 'inequality', 'probability-distributions', 'chi-squared']"
64,Does infinite mikado exist?,Does infinite mikado exist?,,"Let's define a mikado configuration $m$ as a countable collection $\{T_j\}_{j \in \mathbb{N}}$ of disjoint subsets of Euclidean 3-space $(\mathbb{R}^3,\cdot)$ . Each $T_j$ is a ""tube of radius $R>0$ "" (independent of $j$ ). That is, for every $T_j$ there is $x_j \in \mathbb{R}^3$ (the tube center) and $v_j \in \mathbb{R}^3$ with $\|v_j\|=1$ (the tube direction) so that $$T_j=\{y\in \mathbb{R}^3|\, \|y-x_j\|^2<|v_j\cdot(y-x_j)|^2+R^2\}$$ Q1 : Does there exist a mikado configuration $m$ where the tube directions $v_j$ are distributed randomly(*) and the tube density is the same everywhere(**)? Q2 : Does there exist a mikado configuration $m$ where the tube density is the same everywhere and the tube directions are locally distributed randomly(***)? (*) With ""the direction vectors being distributed randomly"", one could first demand that the empirical measure $\frac{1}{N}\sum_{j=1}^N \delta_{v_j}$ converges weakly to the uniform measure on the unit sphere $S^1 \subset \mathbb{R}^3$ (for the induced topology from the Euclidean topology in $\mathbb{R}^3$ ) (**) With ""the tube density being the same everywhere"", one could intend that the fraction $\frac{\text{Vol}\left((\cup_{j}T_j)\bigcap B(x,r)\right)}{\text{Vol }B(x,r)}$ converges to an $x$ -independent constant $c\in (0,1)$ as $r \to \infty$ . (***) This could mean that the empirical measure $\frac{1}{N(x,r)}\sum_{j\in \mathbb{N},\,T_j\cap B(x,r)\neq \emptyset} \delta_{v_j}$ converges weakly to the uniform measure on the unit sphere $S^1$ as $r \to \infty$ . About my own efforts to solve this problem: I thought about using a variant of Olbers paradox to negatively answer those questions. The reasoning is that if we place ourselves in the middle of one of the tubes of one of those supposedly-existing configurations and we look around us, we would see a sky filled with tubes everywhere. If we remove the tube from where we're looking, that situation doesn't change. If we then want to reinsert that tube, we fail because our tube hits other tubes in every angle of the sky. Contradiction","Let's define a mikado configuration as a countable collection of disjoint subsets of Euclidean 3-space . Each is a ""tube of radius "" (independent of ). That is, for every there is (the tube center) and with (the tube direction) so that Q1 : Does there exist a mikado configuration where the tube directions are distributed randomly(*) and the tube density is the same everywhere(**)? Q2 : Does there exist a mikado configuration where the tube density is the same everywhere and the tube directions are locally distributed randomly(***)? (*) With ""the direction vectors being distributed randomly"", one could first demand that the empirical measure converges weakly to the uniform measure on the unit sphere (for the induced topology from the Euclidean topology in ) (**) With ""the tube density being the same everywhere"", one could intend that the fraction converges to an -independent constant as . (***) This could mean that the empirical measure converges weakly to the uniform measure on the unit sphere as . About my own efforts to solve this problem: I thought about using a variant of Olbers paradox to negatively answer those questions. The reasoning is that if we place ourselves in the middle of one of the tubes of one of those supposedly-existing configurations and we look around us, we would see a sky filled with tubes everywhere. If we remove the tube from where we're looking, that situation doesn't change. If we then want to reinsert that tube, we fail because our tube hits other tubes in every angle of the sky. Contradiction","m \{T_j\}_{j \in \mathbb{N}} (\mathbb{R}^3,\cdot) T_j R>0 j T_j x_j \in \mathbb{R}^3 v_j \in \mathbb{R}^3 \|v_j\|=1 T_j=\{y\in \mathbb{R}^3|\, \|y-x_j\|^2<|v_j\cdot(y-x_j)|^2+R^2\} m v_j m \frac{1}{N}\sum_{j=1}^N \delta_{v_j} S^1 \subset \mathbb{R}^3 \mathbb{R}^3 \frac{\text{Vol}\left((\cup_{j}T_j)\bigcap B(x,r)\right)}{\text{Vol }B(x,r)} x c\in (0,1) r \to \infty \frac{1}{N(x,r)}\sum_{j\in \mathbb{N},\,T_j\cap B(x,r)\neq \emptyset} \delta_{v_j} S^1 r \to \infty","['probability', 'geometry', 'euclidean-geometry', 'recreational-mathematics', 'packing-problem']"
65,Deriving covariance of sample mean and sample variance,Deriving covariance of sample mean and sample variance,,"$\newcommand{\cov}{\operatorname{cov}}$I want to find the covariance between the sample mean and the sample variance, I think I am along the right track but am not sure. Suppose we have i.i.d. random variables $X_1, X_2, \ldots, X_n$. Define $$\overline{X} = \frac{1}{n} \sum_{i=1}^n X_i \text{ and } S^2 = \frac{1}{2n(n-1)} \sum_{i=1}^n \sum_{j=1}^n (X_i - X_j)^2 $$ I want to find their covariance $\cov(\overline{X}, S^2)$. Here is what I have done: \begin{align} & \cov(\overline{X}, S^2) = \cov\left(\frac{1}{n}\sum_{i=1}^n X_i, \frac{1}{2n(n-1)} \sum_{i=1}^n\sum_{j=1}^n (X_i - X_j)^2 \right) \\ = {} & \frac{1}{2n^2(n-1)} \sum_{i=1}^n \cov(X_i,\sum_{k=1}^{n}\sum_{j=1}^{n} (X_k - X_j)^2) \\ = {} & \frac{1}{2n^2(n-1)} \sum_{i=1}^n \cov(X_i,2\sum_{k=1}^{n} (X_i - X_k)^2) \end{align} (because $\cov(X_i, (X_a - X_b)^2)$ is zero if $i$ isn't $a$ or $b$) $$= \frac{1}{n^2(n-1)} \sum_{i=1}^{n} \sum_{k=1}^n \cov(X_i, (X_i - X_k)^2).$$ Now, I compute $\cov(X_i, (X_i - X_k)^2)$: for $ i = k, \cov(X_i, (X_i - X_k)^2) = 0$ for $ i \neq k,$ \begin{align} & \cov(X_i, (X_i - X_k)^2) = \cov(X_i, X_i^2 - 2X_iX_k + X_k^2) = \cov(X_i, X_i^2) - 2\cov(X_i, X_iX_k) \\[8pt] = {} & E(X^3) - E(X)E(X^2) -2E(X)E(X^2) + 2(E(X))^3 \\[8pt] = {} & E(X^3) -3E(X)E(X^2) + 2(E(X))^3. \end{align} When I plug this in, I do not get the answer that I want which is $E(X-E(X))^3$, can anyone please tell me where I went wrong? UPDATE: I plugged it in again and it is actually correct! Sorry for being so careless. Thanks to anyone who has read the question. Should I delete this post?","$\newcommand{\cov}{\operatorname{cov}}$I want to find the covariance between the sample mean and the sample variance, I think I am along the right track but am not sure. Suppose we have i.i.d. random variables $X_1, X_2, \ldots, X_n$. Define $$\overline{X} = \frac{1}{n} \sum_{i=1}^n X_i \text{ and } S^2 = \frac{1}{2n(n-1)} \sum_{i=1}^n \sum_{j=1}^n (X_i - X_j)^2 $$ I want to find their covariance $\cov(\overline{X}, S^2)$. Here is what I have done: \begin{align} & \cov(\overline{X}, S^2) = \cov\left(\frac{1}{n}\sum_{i=1}^n X_i, \frac{1}{2n(n-1)} \sum_{i=1}^n\sum_{j=1}^n (X_i - X_j)^2 \right) \\ = {} & \frac{1}{2n^2(n-1)} \sum_{i=1}^n \cov(X_i,\sum_{k=1}^{n}\sum_{j=1}^{n} (X_k - X_j)^2) \\ = {} & \frac{1}{2n^2(n-1)} \sum_{i=1}^n \cov(X_i,2\sum_{k=1}^{n} (X_i - X_k)^2) \end{align} (because $\cov(X_i, (X_a - X_b)^2)$ is zero if $i$ isn't $a$ or $b$) $$= \frac{1}{n^2(n-1)} \sum_{i=1}^{n} \sum_{k=1}^n \cov(X_i, (X_i - X_k)^2).$$ Now, I compute $\cov(X_i, (X_i - X_k)^2)$: for $ i = k, \cov(X_i, (X_i - X_k)^2) = 0$ for $ i \neq k,$ \begin{align} & \cov(X_i, (X_i - X_k)^2) = \cov(X_i, X_i^2 - 2X_iX_k + X_k^2) = \cov(X_i, X_i^2) - 2\cov(X_i, X_iX_k) \\[8pt] = {} & E(X^3) - E(X)E(X^2) -2E(X)E(X^2) + 2(E(X))^3 \\[8pt] = {} & E(X^3) -3E(X)E(X^2) + 2(E(X))^3. \end{align} When I plug this in, I do not get the answer that I want which is $E(X-E(X))^3$, can anyone please tell me where I went wrong? UPDATE: I plugged it in again and it is actually correct! Sorry for being so careless. Thanks to anyone who has read the question. Should I delete this post?",,"['probability', 'statistics']"
66,Prove that $\mathbb P(\|\nabla f(x)\|< Bn) < KB^2$ for a random spherical harmonic $f$,Prove that  for a random spherical harmonic,\mathbb P(\|\nabla f(x)\|< Bn) < KB^2 f,"I have a random spherical harmonic of degree $n$ on the sphere $S^2$, i.e. $$ f = \sum_{k=-n}^{n} \xi_k Y_k$$ with $\xi_k \sim N\left(0, \dfrac{1}{2n+1}\right)$ being independent Gaussians and $\{Y_k\}$ a $L_2$-orthonormal basis of spherical harmonics of degree $n$. (The variances were chosen so to have $\mathbb{E}\| f \|_{L^2} = 1$.) I want to prove that, given a positive constant $B$ and a point $x \in S^2$, then $$\mathbb P(\|\nabla f(x)\| < Bn) < KB^2$$ for some positive constant $K$. I thought about showing that the density of the random vector $\nabla f(x)$ is bounded, but I don't know how to compute it. This inequality is stated without proof in the second to last inequality in page 9 of this article.","I have a random spherical harmonic of degree $n$ on the sphere $S^2$, i.e. $$ f = \sum_{k=-n}^{n} \xi_k Y_k$$ with $\xi_k \sim N\left(0, \dfrac{1}{2n+1}\right)$ being independent Gaussians and $\{Y_k\}$ a $L_2$-orthonormal basis of spherical harmonics of degree $n$. (The variances were chosen so to have $\mathbb{E}\| f \|_{L^2} = 1$.) I want to prove that, given a positive constant $B$ and a point $x \in S^2$, then $$\mathbb P(\|\nabla f(x)\| < Bn) < KB^2$$ for some positive constant $K$. I thought about showing that the density of the random vector $\nabla f(x)$ is bounded, but I don't know how to compute it. This inequality is stated without proof in the second to last inequality in page 9 of this article.",,"['probability', 'spherical-harmonics']"
67,A Characterization of the Mode of a Distribution,A Characterization of the Mode of a Distribution,,"Let $f:\mathbb{R} \rightarrow [0,\infty)$ be a continuous probability density function on $\mathbb{R}$ such that    \begin{equation} \int_{\mathbb{R}} |x| f(x)\, dx < \infty, \end{equation}   and assume that $f$ has a strict global maximum $x_0$, that is $f(x) < f(x_0)$ for all $x \neq x_0$.   For any fixed $q \in (0,1]$ consider the problem   \begin{equation} \min_{y \in \mathbb{R}} \int_{\mathbb{R}}|y-x|^{q} f(x) \,dx. \end{equation} (I) Can we find simple conditions on $f$ such that for some $\epsilon > 0$ and each given $q \in (0,\epsilon)$ there exists a unique solution to this problem? Assume that for all $q$ small enough the problem has a unique solution, and put   \begin{equation} x_q= \arg \min_{y \in \mathbb{R}} \int_{\mathbb{R}}|y-x|^{q} f(x) \,dx. \end{equation} (II) Can we conclude that $x_q \rightarrow x_0$ for $q \rightarrow 0$? I found this last property stated in a monograph about applied statistics without any proof. Thank you very much in advance for your kind attention. NOTE (1). For $q=1$ the solutions of our minimization problem are all the medians of the distributions defined by $f$: see Why does the median minimize $E[|X-c|]$ . So a simple condition assuring that for $q=1$ our problem has a unique solution is that $f > 0$. NOTE (2). Analogous questions can be asked about the midrange. Make the additional assumption that $f$ has compact support $S$ and put $a= \min S$ and $b = \max S$. If we define the probability measure  \begin{equation} \mu(A)=\int_{A} f(x)dx \end{equation} for every set $A$ in the Borel $\sigma$-algebra $\mathcal{B}(\mathbb{R})$, and we consider for every measurable function $g:\mathbb{R} \rightarrow \mathbb{R}$ the norm $||g||_{\infty}= \operatorname{ess} \sup |g|$ with respect to measure space $(\mathbb{R}, \mathcal{B}(\mathbb{R}), \mu)$, then the problem \begin{equation} \min_{y \in \mathbb{R}} || \mathbb{1} -y||_{\infty}, \end{equation} where $\mathbb{1}(x)=x$ for all $x \in \mathbb{R}$, has as unique solution the midrange $x_{\infty}=(a+b)/2$. Now we can ask: (I') Are there simple conditions on $f$ such that for every $q$ greater then some fixed $M > 0$, there exists a unique solution to the problem \begin{equation} \min_{y \in \mathbb{R}} \int_{\mathbb{R}}|y-x|^{q} f(x) \,dx? \end{equation} (II') Assume that these conditions are verified, and put \begin{equation} x_q= \arg \min_{y \in \mathbb{R}} \int_{\mathbb{R}}|y-x|^{q} f(x) \,dx. \end{equation} Do we have $x_q \rightarrow x_{\infty}$ as $q \rightarrow \infty$?","Let $f:\mathbb{R} \rightarrow [0,\infty)$ be a continuous probability density function on $\mathbb{R}$ such that    \begin{equation} \int_{\mathbb{R}} |x| f(x)\, dx < \infty, \end{equation}   and assume that $f$ has a strict global maximum $x_0$, that is $f(x) < f(x_0)$ for all $x \neq x_0$.   For any fixed $q \in (0,1]$ consider the problem   \begin{equation} \min_{y \in \mathbb{R}} \int_{\mathbb{R}}|y-x|^{q} f(x) \,dx. \end{equation} (I) Can we find simple conditions on $f$ such that for some $\epsilon > 0$ and each given $q \in (0,\epsilon)$ there exists a unique solution to this problem? Assume that for all $q$ small enough the problem has a unique solution, and put   \begin{equation} x_q= \arg \min_{y \in \mathbb{R}} \int_{\mathbb{R}}|y-x|^{q} f(x) \,dx. \end{equation} (II) Can we conclude that $x_q \rightarrow x_0$ for $q \rightarrow 0$? I found this last property stated in a monograph about applied statistics without any proof. Thank you very much in advance for your kind attention. NOTE (1). For $q=1$ the solutions of our minimization problem are all the medians of the distributions defined by $f$: see Why does the median minimize $E[|X-c|]$ . So a simple condition assuring that for $q=1$ our problem has a unique solution is that $f > 0$. NOTE (2). Analogous questions can be asked about the midrange. Make the additional assumption that $f$ has compact support $S$ and put $a= \min S$ and $b = \max S$. If we define the probability measure  \begin{equation} \mu(A)=\int_{A} f(x)dx \end{equation} for every set $A$ in the Borel $\sigma$-algebra $\mathcal{B}(\mathbb{R})$, and we consider for every measurable function $g:\mathbb{R} \rightarrow \mathbb{R}$ the norm $||g||_{\infty}= \operatorname{ess} \sup |g|$ with respect to measure space $(\mathbb{R}, \mathcal{B}(\mathbb{R}), \mu)$, then the problem \begin{equation} \min_{y \in \mathbb{R}} || \mathbb{1} -y||_{\infty}, \end{equation} where $\mathbb{1}(x)=x$ for all $x \in \mathbb{R}$, has as unique solution the midrange $x_{\infty}=(a+b)/2$. Now we can ask: (I') Are there simple conditions on $f$ such that for every $q$ greater then some fixed $M > 0$, there exists a unique solution to the problem \begin{equation} \min_{y \in \mathbb{R}} \int_{\mathbb{R}}|y-x|^{q} f(x) \,dx? \end{equation} (II') Assume that these conditions are verified, and put \begin{equation} x_q= \arg \min_{y \in \mathbb{R}} \int_{\mathbb{R}}|y-x|^{q} f(x) \,dx. \end{equation} Do we have $x_q \rightarrow x_{\infty}$ as $q \rightarrow \infty$?",,"['real-analysis', 'probability', 'probability-theory', 'statistics', 'optimization']"
68,Compound Poisson Process Expected Exit Time,Compound Poisson Process Expected Exit Time,,"It is very well known that expected time for a standard Brownian motion to exit from interval $[a,b]$ (where $a<0$ and $b>0$) is $-ab$. In one of my projects, I wanted to calculate the similar quantity for a compound poisson process. I am not an expert in handling point processes and would be grateful for any help. Question : $X$ is a compound poisson process starting at position $0$ with arrival rate $\lambda$ and the distribution of jumps as $F(dz)$. What would be the expected time $\tau$ for $X$ to exit $[-a,a]$? Also, what would be the distribution of $X_\tau$?","It is very well known that expected time for a standard Brownian motion to exit from interval $[a,b]$ (where $a<0$ and $b>0$) is $-ab$. In one of my projects, I wanted to calculate the similar quantity for a compound poisson process. I am not an expert in handling point processes and would be grateful for any help. Question : $X$ is a compound poisson process starting at position $0$ with arrival rate $\lambda$ and the distribution of jumps as $F(dz)$. What would be the expected time $\tau$ for $X$ to exit $[-a,a]$? Also, what would be the distribution of $X_\tau$?",,"['probability', 'probability-distributions', 'reference-request', 'stochastic-processes', 'markov-chains']"
69,Controlling this function in $L^2$ norm,Controlling this function in  norm,L^2,"We have $(X_i)_{i \in \mathbb Z}$ iid random variables with $1\le X_i \le2$ almost surely. We define $X(x,\omega) \equiv X_i (\omega)$ if $x\in [i,i+1[$ (so it is bounded almost surely) and, for $\epsilon >0$, $X_\epsilon (x, \omega) \equiv X(x/\epsilon, \omega)$. Define for $x\in [0,1]$ $$err_\epsilon(x,\omega)=\frac {\int_0^1 \frac {F(y)}{X_\epsilon(y,\omega)} dy}{X_\epsilon(x,\omega)\int_0^1 \frac {1}{X_\epsilon(y,\omega)} dy}$$ where $F$ is an $L^1(]0,1[)$ function (we can add continuity if it helps). We would like to find functions $g$ and $f$ such that $$\Vert err_\epsilon - g\Vert_{L^2(]0,1[ \times \Omega)} \le f(\epsilon) \to 0$$ when $\epsilon \to 0^+$ Related question: How to use the Lindeberg CLT in this scenario ? (analysis problem)","We have $(X_i)_{i \in \mathbb Z}$ iid random variables with $1\le X_i \le2$ almost surely. We define $X(x,\omega) \equiv X_i (\omega)$ if $x\in [i,i+1[$ (so it is bounded almost surely) and, for $\epsilon >0$, $X_\epsilon (x, \omega) \equiv X(x/\epsilon, \omega)$. Define for $x\in [0,1]$ $$err_\epsilon(x,\omega)=\frac {\int_0^1 \frac {F(y)}{X_\epsilon(y,\omega)} dy}{X_\epsilon(x,\omega)\int_0^1 \frac {1}{X_\epsilon(y,\omega)} dy}$$ where $F$ is an $L^1(]0,1[)$ function (we can add continuity if it helps). We would like to find functions $g$ and $f$ such that $$\Vert err_\epsilon - g\Vert_{L^2(]0,1[ \times \Omega)} \le f(\epsilon) \to 0$$ when $\epsilon \to 0^+$ Related question: How to use the Lindeberg CLT in this scenario ? (analysis problem)",,"['real-analysis', 'probability', 'functional-analysis', 'probability-theory', 'measure-theory']"
70,Showing an $L^2$ convergence (with convergence rate),Showing an  convergence (with convergence rate),L^2,"We have $(X_i)_{i \in \mathbb Z}$ iid random variables with $1\le X_i \le2$ almost surely. We define $X(x,\omega) \equiv X_i (\omega)$ if $x\in [i,i+1[$ (so it is bounded almost surely) and, for $\epsilon >0$, $X_\epsilon (x, \omega) \equiv X(x/\epsilon, \omega)$. Define for $x\in [0,1]$ $$u_\epsilon(x,\omega)= \int_0^x \frac {c_\epsilon(\omega) - F(x)}{X_\epsilon(x,\omega)}\, dx$$ where $F$ is an $L^1(]0,1[)$ function (we can add continuity of $F$ on $[0,1]$ if it helps solving the problem) and $c_\epsilon(\omega)$ is defined by $$c_\epsilon(\omega)\equiv \frac{\int_0^1 \frac{F(y)}{X_\epsilon(y,\omega)} \, dy}{\int_0^1\frac 1 {X_\epsilon(y,\omega)}\, dy}$$ It can be shown that for each $x\in [0,1]$, we have almost surely (we use law of large numbers and the density of the staircase functions in the $L^1$ functions) $$u_\epsilon(x,\omega) \to u_0(x) \equiv -E[\frac 1 {X_1}](\int_0^x F(y) dy - x \int_0^1 F(y) dy) $$ when $\epsilon \to 0^+$ We would like to find a function $f$ such that $$\Vert u_\epsilon - u_0\Vert_{L^2(]0,1[ \times \Omega)} \le f(\epsilon) \to 0$$ when $\epsilon \to 0^+$ Related questions: Show that those random quantities converge in distribution to a normal variable (hard analysis problem) How to use the Lindeberg CLT in this scenario ? (analysis problem) Controlling this function in $L^2$ norm","We have $(X_i)_{i \in \mathbb Z}$ iid random variables with $1\le X_i \le2$ almost surely. We define $X(x,\omega) \equiv X_i (\omega)$ if $x\in [i,i+1[$ (so it is bounded almost surely) and, for $\epsilon >0$, $X_\epsilon (x, \omega) \equiv X(x/\epsilon, \omega)$. Define for $x\in [0,1]$ $$u_\epsilon(x,\omega)= \int_0^x \frac {c_\epsilon(\omega) - F(x)}{X_\epsilon(x,\omega)}\, dx$$ where $F$ is an $L^1(]0,1[)$ function (we can add continuity of $F$ on $[0,1]$ if it helps solving the problem) and $c_\epsilon(\omega)$ is defined by $$c_\epsilon(\omega)\equiv \frac{\int_0^1 \frac{F(y)}{X_\epsilon(y,\omega)} \, dy}{\int_0^1\frac 1 {X_\epsilon(y,\omega)}\, dy}$$ It can be shown that for each $x\in [0,1]$, we have almost surely (we use law of large numbers and the density of the staircase functions in the $L^1$ functions) $$u_\epsilon(x,\omega) \to u_0(x) \equiv -E[\frac 1 {X_1}](\int_0^x F(y) dy - x \int_0^1 F(y) dy) $$ when $\epsilon \to 0^+$ We would like to find a function $f$ such that $$\Vert u_\epsilon - u_0\Vert_{L^2(]0,1[ \times \Omega)} \le f(\epsilon) \to 0$$ when $\epsilon \to 0^+$ Related questions: Show that those random quantities converge in distribution to a normal variable (hard analysis problem) How to use the Lindeberg CLT in this scenario ? (analysis problem) Controlling this function in $L^2$ norm",,"['real-analysis', 'probability', 'functional-analysis', 'probability-theory', 'measure-theory']"
71,Birth-death-process: probability for at least one birth up to $t$,Birth-death-process: probability for at least one birth up to,t,"Consider a Birth-and-Death Process with individual birth rates $\lambda(t)$ and individual death rates $\mu(t)$, starting at $n_0$.  My question is if there is a formula for something like $$\mathbb P_{n_0} (\text{more than one birth happens up to time $t\ge0)$}$$ or—in a similar spirit—the expected time until the first birth happens.  I know that the time spend at a position $n$ is distributed exponentially with parameter $n(\lambda(t)+\mu(t))$ but I couldn't find anything concerning probabilities where we look only at e.g. births.","Consider a Birth-and-Death Process with individual birth rates $\lambda(t)$ and individual death rates $\mu(t)$, starting at $n_0$.  My question is if there is a formula for something like $$\mathbb P_{n_0} (\text{more than one birth happens up to time $t\ge0)$}$$ or—in a similar spirit—the expected time until the first birth happens.  I know that the time spend at a position $n$ is distributed exponentially with parameter $n(\lambda(t)+\mu(t))$ but I couldn't find anything concerning probabilities where we look only at e.g. births.",,"['probability', 'probability-theory', 'stochastic-processes', 'birth-death-process']"
72,ELO Rating as a sorting algorithm,ELO Rating as a sorting algorithm,,"Let $N$ be the number of players each with a true rating $R_i$, where $i \in \{1,2,...,N\}$. Two players, $A$ and $B$, are selected at random and made to play a game. Let their current ratings be $R_A$ and $R_B$, respectively. We use the ELO rating system to calculate the expectation of a player to win the game as $$E_A = \frac{1}{1 + 10^{(R_B - R_A)/400}}, \\ E_B = \frac{1}{1 + 10^{(R_A - R_B)/400}}.$$ Their new ratings are given by the expressions $$R'_A = R_A + K(S_A - E_A),\\ R'_B = R_B + K(S_B - E_B).$$ If $A/B$ wins, $S_{A/B} = 1$. If $A/B$ loses, $S_{A/B} = 0$. $K$ is a constant (say 32). If each of these $N$ players start with a base rating of 1400, what order of games are required for the players to arrive at an accurate ranking? More generally, how does ELO rating serve as a sorting algorithm?","Let $N$ be the number of players each with a true rating $R_i$, where $i \in \{1,2,...,N\}$. Two players, $A$ and $B$, are selected at random and made to play a game. Let their current ratings be $R_A$ and $R_B$, respectively. We use the ELO rating system to calculate the expectation of a player to win the game as $$E_A = \frac{1}{1 + 10^{(R_B - R_A)/400}}, \\ E_B = \frac{1}{1 + 10^{(R_A - R_B)/400}}.$$ Their new ratings are given by the expressions $$R'_A = R_A + K(S_A - E_A),\\ R'_B = R_B + K(S_B - E_B).$$ If $A/B$ wins, $S_{A/B} = 1$. If $A/B$ loses, $S_{A/B} = 0$. $K$ is a constant (say 32). If each of these $N$ players start with a base rating of 1400, what order of games are required for the players to arrive at an accurate ranking? More generally, how does ELO rating serve as a sorting algorithm?",,"['probability', 'statistics', 'sorting']"
73,Show measure zero: application of Theorem 1 in Lang (1986)?,Show measure zero: application of Theorem 1 in Lang (1986)?,,"Suppose I have a random variable $Y$ with support $\{1,2,..., M\}$. Consider a random vector $V\equiv (V_1, V_2,..., V_M)$ with support $\mathcal{V}\subseteq \mathbb{R}^M$ with positive Lebesgue measure. By definition of support we know that $\mathcal{V}$ is a closed set (see here for example). All random variables/vectors are defined on the probability space $(\Omega, \mathcal{F}, \mathbb{P})$. For any $j\in \{1,...,M\}$, let $$\mathcal{V}_j\equiv \{v\equiv (v_1,..., v_M)\in \mathcal{V} \text{ s.t. } v_j\geq v_k \text{ }\forall k\in \{1,...,M\} \text{ with } k\neq j\}$$ Hence, $\{\mathcal{V}_1, ..., \mathcal{V}_M\}$ constitutes a partition of $\mathcal{V}$. Assumption A1: $V$ has a distribution absolutely continuous with respect to Lebesgue measure on $\mathcal{V}$ Assumption A2: $Y\in argmax_{k\in \{1,...,M\}} V_k$ with probability 1. Question 1: Is it true that $\mathcal{V}_j$ is closed and convex? Question 2: Under which sufficient conditions I can claim $$\mathbb{P}(Y=j)=\mathbb{P}(V\in \mathcal{V}_j)$$ $\forall j\in \{1,..., M\}$. Attempted answer 1: we know that if $\mathcal{V}=\mathbb{R}^M$, then $\mathcal{V}_j$ is closed and convex (see here , for example). When $\mathcal{V}\subset \mathbb{R}^M$, I think (can you confirm?) we can generalise the same results for any closed $\mathcal{V}\subset \mathbb{R}^M$. Attempted answer 2: by A1 , I am tempted to naively set up the following proof: wlog take $M=2$; for any $v\in \mathcal{V}$, suppose there exists $\{y,y'\}\subseteq argmax_{k\in \{1,2\}} v_k$ with $y\neq y'$. This can be the case if and only if  $$ v_{y '}-v_y=0 $$ which is an event happening with probability measure zero, by A1, i.e., $$ \mathbb{P}(V \in \{v\in \mathcal{V} \text{ s.t. } v_1=v_2\} )=0 $$ Therefore, $argmax_{k} V_k$ is a singleton set almost surely. This implies (not sure!) $$\mathbb{P}(Y=j)=\mathbb{P}(V\in \mathcal{V}_j)$$ $\forall j\in \{1,2\}$. Doubts on the attempted answer 2: my attempted proof does not use the fact that $\mathcal{V}$ is closed or the fact that $\mathcal{V}_j$ is closed-convex. However, I am confused by Theorem 1 in Lang (1986) , which claims that under A1 and if $\mathcal{A}$ is a convex subset of $\mathcal{V}$ , then $\mathbb{P}(V \in\partial \mathcal{A})=0$, where $\partial \mathcal{A}$ denotes the boundary of $\mathcal{A}$. Any help to clarify? Is this Theorem relevant for my case? Referring to the case $M=2$, is $\partial \mathcal{V}_1\neq  \{v\in \mathcal{V} \text{ s.t. } v_1=v_2\}$?","Suppose I have a random variable $Y$ with support $\{1,2,..., M\}$. Consider a random vector $V\equiv (V_1, V_2,..., V_M)$ with support $\mathcal{V}\subseteq \mathbb{R}^M$ with positive Lebesgue measure. By definition of support we know that $\mathcal{V}$ is a closed set (see here for example). All random variables/vectors are defined on the probability space $(\Omega, \mathcal{F}, \mathbb{P})$. For any $j\in \{1,...,M\}$, let $$\mathcal{V}_j\equiv \{v\equiv (v_1,..., v_M)\in \mathcal{V} \text{ s.t. } v_j\geq v_k \text{ }\forall k\in \{1,...,M\} \text{ with } k\neq j\}$$ Hence, $\{\mathcal{V}_1, ..., \mathcal{V}_M\}$ constitutes a partition of $\mathcal{V}$. Assumption A1: $V$ has a distribution absolutely continuous with respect to Lebesgue measure on $\mathcal{V}$ Assumption A2: $Y\in argmax_{k\in \{1,...,M\}} V_k$ with probability 1. Question 1: Is it true that $\mathcal{V}_j$ is closed and convex? Question 2: Under which sufficient conditions I can claim $$\mathbb{P}(Y=j)=\mathbb{P}(V\in \mathcal{V}_j)$$ $\forall j\in \{1,..., M\}$. Attempted answer 1: we know that if $\mathcal{V}=\mathbb{R}^M$, then $\mathcal{V}_j$ is closed and convex (see here , for example). When $\mathcal{V}\subset \mathbb{R}^M$, I think (can you confirm?) we can generalise the same results for any closed $\mathcal{V}\subset \mathbb{R}^M$. Attempted answer 2: by A1 , I am tempted to naively set up the following proof: wlog take $M=2$; for any $v\in \mathcal{V}$, suppose there exists $\{y,y'\}\subseteq argmax_{k\in \{1,2\}} v_k$ with $y\neq y'$. This can be the case if and only if  $$ v_{y '}-v_y=0 $$ which is an event happening with probability measure zero, by A1, i.e., $$ \mathbb{P}(V \in \{v\in \mathcal{V} \text{ s.t. } v_1=v_2\} )=0 $$ Therefore, $argmax_{k} V_k$ is a singleton set almost surely. This implies (not sure!) $$\mathbb{P}(Y=j)=\mathbb{P}(V\in \mathcal{V}_j)$$ $\forall j\in \{1,2\}$. Doubts on the attempted answer 2: my attempted proof does not use the fact that $\mathcal{V}$ is closed or the fact that $\mathcal{V}_j$ is closed-convex. However, I am confused by Theorem 1 in Lang (1986) , which claims that under A1 and if $\mathcal{A}$ is a convex subset of $\mathcal{V}$ , then $\mathbb{P}(V \in\partial \mathcal{A})=0$, where $\partial \mathcal{A}$ denotes the boundary of $\mathcal{A}$. Any help to clarify? Is this Theorem relevant for my case? Referring to the case $M=2$, is $\partial \mathcal{V}_1\neq  \{v\in \mathcal{V} \text{ s.t. } v_1=v_2\}$?",,"['probability', 'general-topology', 'measure-theory', 'convex-analysis', 'absolute-continuity']"
74,How to construct a Poisson process not based on Lebesgue measure?,How to construct a Poisson process not based on Lebesgue measure?,,"It is clear to me that I can build a suitable underlying probability space for a homogeneous Poisson point process . It is enough to have a pobability space $(\Omega,\mathcal A,P)$ with on it iid random variables $X_1,X_2,\dots$ having exponential distribution. So I could do already with $\mathbb R^{\mathbb N}$ applied with product measure. Then $N_t$ can be defined as the cardinality of the set $\{n\mid S_n\leq t\}$ where $S_n:=X_1+\dots+X_n$. If $A$ is a measurable subset of $[0,\infty)$ then I can define random variable $\hat A$ as the (random) cardinality of $\{n\mid S_n\in A\}$ and then $\hat A$ has Poisson-distribution with a (multiple of) $\lambda(A)$ as parameter, where $\lambda$ denotes the Lebesgue measure. In that sense it can be called a Poisson process on base of the Lebesgue measure . Now my question: How to build up a probability space allowing me to construct a Poisson process based on an arbitrary chosen measure $\nu$ on $[0,\infty)$ with the property that $\nu([0,t])<\infty$ for each $t$? So this means that for a measurable $A\subseteq[0,\infty)$ the random cardinality of $\{n\mid S_n\in A\}$ has Poisson-distribution with parameter $\nu(A)$. Further if two such sets $A,B$ are disjoint then $\{n\mid S_n\in A\}$ and $\{n\mid S_n\in B\}$ must be independent (as is also the case described above).","It is clear to me that I can build a suitable underlying probability space for a homogeneous Poisson point process . It is enough to have a pobability space $(\Omega,\mathcal A,P)$ with on it iid random variables $X_1,X_2,\dots$ having exponential distribution. So I could do already with $\mathbb R^{\mathbb N}$ applied with product measure. Then $N_t$ can be defined as the cardinality of the set $\{n\mid S_n\leq t\}$ where $S_n:=X_1+\dots+X_n$. If $A$ is a measurable subset of $[0,\infty)$ then I can define random variable $\hat A$ as the (random) cardinality of $\{n\mid S_n\in A\}$ and then $\hat A$ has Poisson-distribution with a (multiple of) $\lambda(A)$ as parameter, where $\lambda$ denotes the Lebesgue measure. In that sense it can be called a Poisson process on base of the Lebesgue measure . Now my question: How to build up a probability space allowing me to construct a Poisson process based on an arbitrary chosen measure $\nu$ on $[0,\infty)$ with the property that $\nu([0,t])<\infty$ for each $t$? So this means that for a measurable $A\subseteq[0,\infty)$ the random cardinality of $\{n\mid S_n\in A\}$ has Poisson-distribution with parameter $\nu(A)$. Further if two such sets $A,B$ are disjoint then $\{n\mid S_n\in A\}$ and $\{n\mid S_n\in B\}$ must be independent (as is also the case described above).",,"['probability', 'probability-theory', 'stochastic-processes']"
75,Estimating the volume of a spherical cap,Estimating the volume of a spherical cap,,"If $d$ is very large, there is a nice way to estimate the surface area of a ""spherical cap,"" that is, the part of a unit $(d-1)$-sphere on one side of a hyperplane: each coordinate of a random point on the surface of the sphere is distributed almost like a Gaussian with variance $1/d$, and we can use this to determine the probability that a random point lies in the cap. This trick is useful for other questions related to the spherical cap, and so it would be nice to know how good this Gaussian approximation is. Is there a nice bound one can prove in terms of $d$? The particular quantity I'm trying to estimate is the following: given a spherical cap whose surface area takes up some fraction $\alpha$ of the total surface area of the sphere, what is the average Euclidean distance from a random point on the sphere to the closest point on the cap?","If $d$ is very large, there is a nice way to estimate the surface area of a ""spherical cap,"" that is, the part of a unit $(d-1)$-sphere on one side of a hyperplane: each coordinate of a random point on the surface of the sphere is distributed almost like a Gaussian with variance $1/d$, and we can use this to determine the probability that a random point lies in the cap. This trick is useful for other questions related to the spherical cap, and so it would be nice to know how good this Gaussian approximation is. Is there a nice bound one can prove in terms of $d$? The particular quantity I'm trying to estimate is the following: given a spherical cap whose surface area takes up some fraction $\alpha$ of the total surface area of the sphere, what is the average Euclidean distance from a random point on the sphere to the closest point on the cap?",,"['probability', 'geometry']"
76,Probability that at least $2$ of a group of $4$ people were born on the same day of the week,Probability that at least  of a group of  people were born on the same day of the week,2 4,What is the probability that at least $2$ of a group of $4$ people were born on the same day of the week? My attempt: Probability that at least $2$ of a group of $4$ people were born on the same day of the week=1-probability that at most $1$ of a group of $4$ people were born on the same day of the week (NONE ARE BORN ON SAME DAY). =$1-\frac{\text{Possibility none born on same day}}{\text{No. Of total possibilites}}$ =$1-\frac{4.5.6.7}{7.7.7.7} =$0.659$,What is the probability that at least $2$ of a group of $4$ people were born on the same day of the week? My attempt: Probability that at least $2$ of a group of $4$ people were born on the same day of the week=1-probability that at most $1$ of a group of $4$ people were born on the same day of the week (NONE ARE BORN ON SAME DAY). =$1-\frac{\text{Possibility none born on same day}}{\text{No. Of total possibilites}}$ =$1-\frac{4.5.6.7}{7.7.7.7} =$0.659$,,['probability']
77,Probability a random permutation has no consecutive numbers next to each other,Probability a random permutation has no consecutive numbers next to each other,,"What is the probability that a random permutation of the integers $\{1,\dots,n\}$ has no two consecutive numbers next to each other? For example, for $n=4$ the permutation $2,4,1,3$ has no two consecutive numbers next to each other. However, $1,2,3,4$ and $4,3,2,1$ do.  For $n = 3$ the probability is $0$. If $n=4$ the probability is $1/12$. If $n=5$ the probability is $7/60$. If $n=6$ the probability is $1/8$.","What is the probability that a random permutation of the integers $\{1,\dots,n\}$ has no two consecutive numbers next to each other? For example, for $n=4$ the permutation $2,4,1,3$ has no two consecutive numbers next to each other. However, $1,2,3,4$ and $4,3,2,1$ do.  For $n = 3$ the probability is $0$. If $n=4$ the probability is $1/12$. If $n=5$ the probability is $7/60$. If $n=6$ the probability is $1/8$.",,[]
78,Deriving the posterior distribution given arbitrary measures,Deriving the posterior distribution given arbitrary measures,,"I am given the following facts: $$ \forall A \in \mathcal{B}:  \; \mu(A) = \int_{A} f(x) \alpha(\text{d} x), \quad \text{  and } \quad \nu_x(A) = \int_{A} g_x(y) \beta(\text{d} y), \; \forall x \in \mathbb{R} $$ where $\alpha, \beta$ are $\sigma$-finite measures on $(\mathbb{R}, \mathcal{B})$. Let $$ p_y(x) = \frac{f(x) g_x(y)}{ \int_{\mathbb{R}} f(z)g_z(y)\alpha(\text{d}z)} $$ and asked to prove that $p_y$ is the density of a version of the conditinal distribution of $X$ given $Y$, with respect to $\alpha$. Previously, I have proven that $$ \pi(S) = \int_{\mathbb{R}} \nu_x(S_x) \mu(\text{d} x) = \int_S f(x) g_x(y) \text{d}(\alpha \times \beta), \; S \in \mathcal{B} \times \mathcal{B} $$ is a probability measure on $(\mathbb{R}^2, \mathcal{B} \times \mathcal{B})$, where $S_x$ is defined as $$ S_x = \left\{y \in \mathbb{R} \ \middle|\ (x, y) \in S\right\} $$ My attempt : By the definition of the Radon-Nikodym derivative, we have $f = \frac{\text{d} \mu}{\text{d} \alpha}, g_x = \frac{\text{d} \nu}{\text{d} \beta}$. If we replace the former in $p_y$, we obtain $$ p_y(x) = \frac{f(x) g_x(y)}{ \int_{\mathbb{R}} g_z(y)\mu(\text{d}z)} $$ In order for $p_y(x)$ to be the density of the conditional distribution $\mathbb{P}(X \ |\ Y)$, we need to show that $$ \int_A p_y(x) \alpha(\text{d} x) = \frac{\int_{A} g_z(y) \mu(\text{d} z)}{\int_{\mathbb{R}} g_z(y) \mu(\text{d} z)} $$ is a version of the conditional probability $\mathbb{P}(X \in A \ |\ Y)$, which in turn should satisfy the integral criterion: $$ \int_{B} \left[ \frac{\int_{A} g_z(y) \mu(\text{d} z)}{\int_{\mathbb{R}} g_z(y) \mu(\text{d} z)} \right] \beta(\text{d} y) = \mathbb{P}([X \in A] \cap [Y \in B]) = \pi(A \times B) $$ However, I'm having trouble simplifying the double integral. My only approach has been to try and substitute $g_x = \frac{\text{d} \nu_x}{\text{d} \beta}$ which doesn't give me any useful form to work with, and I'm stuck at this point. Any hints? Edit : I was also able to show that, since $$ \int_{B} \int_{\mathbb{R}} f(z) g_z(y) \alpha(\text{d} z) \beta(\text{d} y) = \pi(\mathbb{R} \times B) $$ we can write $$ \int_{\mathbb{R}} f(z) g_z(y) \alpha(\text{d} z) = \frac{\text{d} \pi(\mathbb{R} \times \cdot)}{\text{d} \beta} $$ but still can't figure out if I can use that somehow.","I am given the following facts: $$ \forall A \in \mathcal{B}:  \; \mu(A) = \int_{A} f(x) \alpha(\text{d} x), \quad \text{  and } \quad \nu_x(A) = \int_{A} g_x(y) \beta(\text{d} y), \; \forall x \in \mathbb{R} $$ where $\alpha, \beta$ are $\sigma$-finite measures on $(\mathbb{R}, \mathcal{B})$. Let $$ p_y(x) = \frac{f(x) g_x(y)}{ \int_{\mathbb{R}} f(z)g_z(y)\alpha(\text{d}z)} $$ and asked to prove that $p_y$ is the density of a version of the conditinal distribution of $X$ given $Y$, with respect to $\alpha$. Previously, I have proven that $$ \pi(S) = \int_{\mathbb{R}} \nu_x(S_x) \mu(\text{d} x) = \int_S f(x) g_x(y) \text{d}(\alpha \times \beta), \; S \in \mathcal{B} \times \mathcal{B} $$ is a probability measure on $(\mathbb{R}^2, \mathcal{B} \times \mathcal{B})$, where $S_x$ is defined as $$ S_x = \left\{y \in \mathbb{R} \ \middle|\ (x, y) \in S\right\} $$ My attempt : By the definition of the Radon-Nikodym derivative, we have $f = \frac{\text{d} \mu}{\text{d} \alpha}, g_x = \frac{\text{d} \nu}{\text{d} \beta}$. If we replace the former in $p_y$, we obtain $$ p_y(x) = \frac{f(x) g_x(y)}{ \int_{\mathbb{R}} g_z(y)\mu(\text{d}z)} $$ In order for $p_y(x)$ to be the density of the conditional distribution $\mathbb{P}(X \ |\ Y)$, we need to show that $$ \int_A p_y(x) \alpha(\text{d} x) = \frac{\int_{A} g_z(y) \mu(\text{d} z)}{\int_{\mathbb{R}} g_z(y) \mu(\text{d} z)} $$ is a version of the conditional probability $\mathbb{P}(X \in A \ |\ Y)$, which in turn should satisfy the integral criterion: $$ \int_{B} \left[ \frac{\int_{A} g_z(y) \mu(\text{d} z)}{\int_{\mathbb{R}} g_z(y) \mu(\text{d} z)} \right] \beta(\text{d} y) = \mathbb{P}([X \in A] \cap [Y \in B]) = \pi(A \times B) $$ However, I'm having trouble simplifying the double integral. My only approach has been to try and substitute $g_x = \frac{\text{d} \nu_x}{\text{d} \beta}$ which doesn't give me any useful form to work with, and I'm stuck at this point. Any hints? Edit : I was also able to show that, since $$ \int_{B} \int_{\mathbb{R}} f(z) g_z(y) \alpha(\text{d} z) \beta(\text{d} y) = \pi(\mathbb{R} \times B) $$ we can write $$ \int_{\mathbb{R}} f(z) g_z(y) \alpha(\text{d} z) = \frac{\text{d} \pi(\mathbb{R} \times \cdot)}{\text{d} \beta} $$ but still can't figure out if I can use that somehow.",,"['real-analysis', 'probability', 'probability-theory', 'measure-theory', 'radon-nikodym']"
79,Limiting Poisson Process,Limiting Poisson Process,,"I'm struggling with the following problem: Suppose there are $n$ bins and balls arrive as a Poisson Process with rate 1. On arrival each ball falls into a bin uniformly likely. Let $M_n$ be the maximum number of balls in any bin at time $n$. Show that $$P\left(M_n\ge(1+\epsilon)\frac{\log{n}}{\log \log{n}}\right) \to0$$ with $n$. And we are given the 'hint' that if $X$ a Poisson Process with rate $1$, then $$P(X\ge x)\le \exp(x-x\log{x})$$ My idea was to use the fact that since #balls in each bin is independent; $P(M_n<k) = P($all bins have $\le k$ members) = $P$(bin 1 has $\le k$ members)$^n$. And as arrivals to bin $1$ are Poisson with rate $1/n$, this is equal to $$\left(e^{-1}(1+1+1/{2!}+...+1/{n!})\right)^n$$ which we want to show converges to $1$. But this is really turning the problem into an analysis question rather than a probability question which is not what I want to do. Also it doesn't use the hint.","I'm struggling with the following problem: Suppose there are $n$ bins and balls arrive as a Poisson Process with rate 1. On arrival each ball falls into a bin uniformly likely. Let $M_n$ be the maximum number of balls in any bin at time $n$. Show that $$P\left(M_n\ge(1+\epsilon)\frac{\log{n}}{\log \log{n}}\right) \to0$$ with $n$. And we are given the 'hint' that if $X$ a Poisson Process with rate $1$, then $$P(X\ge x)\le \exp(x-x\log{x})$$ My idea was to use the fact that since #balls in each bin is independent; $P(M_n<k) = P($all bins have $\le k$ members) = $P$(bin 1 has $\le k$ members)$^n$. And as arrivals to bin $1$ are Poisson with rate $1/n$, this is equal to $$\left(e^{-1}(1+1+1/{2!}+...+1/{n!})\right)^n$$ which we want to show converges to $1$. But this is really turning the problem into an analysis question rather than a probability question which is not what I want to do. Also it doesn't use the hint.",,"['probability', 'probability-limit-theorems', 'poisson-process']"
80,First hitting locations of Brownian motion gets arbitrarily close,First hitting locations of Brownian motion gets arbitrarily close,,"Let $\Omega$ be an open topological disk. Let $\delta > 0$. I want to show that there exists $\epsilon > 0$ and $K_\epsilon \subseteq_c \Omega$, where $d(y,\partial \Omega) < \epsilon$ for all $y \in \partial K_\epsilon$, such that  $\mathbb P(|B_T - B_{T_\epsilon}| \geq \delta) < \delta$, where $T$ and $T_\epsilon$ are the first hitting times of $\partial \Omega$ and $\partial K_\epsilon$, respectively. This argument looks intuitive. If $\epsilon$ decreases sufficiently small, $B_{T_\epsilon}$ gets very close to the boundary of $\Omega$, and thus $B_T$ is expected to be very close to $B_{T_\epsilon}$. But how do I put together a proof for this? I think I may be missing some property of Brownian motions so that I have a hard time doing it.","Let $\Omega$ be an open topological disk. Let $\delta > 0$. I want to show that there exists $\epsilon > 0$ and $K_\epsilon \subseteq_c \Omega$, where $d(y,\partial \Omega) < \epsilon$ for all $y \in \partial K_\epsilon$, such that  $\mathbb P(|B_T - B_{T_\epsilon}| \geq \delta) < \delta$, where $T$ and $T_\epsilon$ are the first hitting times of $\partial \Omega$ and $\partial K_\epsilon$, respectively. This argument looks intuitive. If $\epsilon$ decreases sufficiently small, $B_{T_\epsilon}$ gets very close to the boundary of $\Omega$, and thus $B_T$ is expected to be very close to $B_{T_\epsilon}$. But how do I put together a proof for this? I think I may be missing some property of Brownian motions so that I have a hard time doing it.",,"['probability', 'stochastic-processes', 'brownian-motion']"
81,A case where the weak law of large number holds while SLLN dose not,A case where the weak law of large number holds while SLLN dose not,,"$(X_i)_{i \geq 2}$ is a sequence of independent random variables. Their probability measure is defined as $P(X_i=i)=\frac{1}{i\log i}$ and $P(X_i=0)=1-\frac{1}{i \log i}$. How can we show that $\frac{1}{n}\sum\limits_{i=2}^n(X_i -E(X_i))$ converges in probability to zero but not almost surely. To prove the weak law of large number, I can just use Chebyshev inequality. But I don't know how to show that this doesn't converge to zero almost surely.","$(X_i)_{i \geq 2}$ is a sequence of independent random variables. Their probability measure is defined as $P(X_i=i)=\frac{1}{i\log i}$ and $P(X_i=0)=1-\frac{1}{i \log i}$. How can we show that $\frac{1}{n}\sum\limits_{i=2}^n(X_i -E(X_i))$ converges in probability to zero but not almost surely. To prove the weak law of large number, I can just use Chebyshev inequality. But I don't know how to show that this doesn't converge to zero almost surely.",,['probability']
82,Distribution of Sum of Independent Log-Gamma Random Variables,Distribution of Sum of Independent Log-Gamma Random Variables,,"Suppose $X_i\sim GAM(\alpha,\beta)$ with PDF $$f_{X_i}(x)=\frac{x^{\alpha-1}e^{-x/\beta}}{\Gamma(\alpha)\beta^\alpha}$$  Then, from univariate transformation, the random variable $Y_i:=ln(X_i)\sim LOGGAM(\alpha,\beta)$ with PDF $$f_{Y_i}(y)=\frac{e^{\alpha y}\cdot e^{-e^{y}/\beta}}{\Gamma(\alpha)\beta^\alpha}$$ Suppose I have a random sample of $Y_i$ that are independent and identically distributed from this log-gamma distribution, $i\in\{1,2,...,n\}$. What is the distribution of $\sum_{i=1}^n Y_i$?  That is, what is the distribution of the sum of independent and identically distributed log-gamma random variables? From the MGF technique, we can find that $$M_{\sum Y_i}(t)=\left(\frac{\beta^t\Gamma(\alpha+t)}{\Gamma(\alpha)}\right)^n$$ But I have no idea how to derive the PDF of a distribution that has this particular MGF.  We know that $\alpha>0, \beta>0,$ and $Y_i\in(-\infty, \infty)$, but even with those assumptions I'm still lost.  How can we get this PDF?","Suppose $X_i\sim GAM(\alpha,\beta)$ with PDF $$f_{X_i}(x)=\frac{x^{\alpha-1}e^{-x/\beta}}{\Gamma(\alpha)\beta^\alpha}$$  Then, from univariate transformation, the random variable $Y_i:=ln(X_i)\sim LOGGAM(\alpha,\beta)$ with PDF $$f_{Y_i}(y)=\frac{e^{\alpha y}\cdot e^{-e^{y}/\beta}}{\Gamma(\alpha)\beta^\alpha}$$ Suppose I have a random sample of $Y_i$ that are independent and identically distributed from this log-gamma distribution, $i\in\{1,2,...,n\}$. What is the distribution of $\sum_{i=1}^n Y_i$?  That is, what is the distribution of the sum of independent and identically distributed log-gamma random variables? From the MGF technique, we can find that $$M_{\sum Y_i}(t)=\left(\frac{\beta^t\Gamma(\alpha+t)}{\Gamma(\alpha)}\right)^n$$ But I have no idea how to derive the PDF of a distribution that has this particular MGF.  We know that $\alpha>0, \beta>0,$ and $Y_i\in(-\infty, \infty)$, but even with those assumptions I'm still lost.  How can we get this PDF?",,"['probability', 'probability-theory', 'statistics', 'probability-distributions', 'gamma-function']"
83,"What is a Random Variable in the formulation of McCullagh's ""What is a Statistical Model?""","What is a Random Variable in the formulation of McCullagh's ""What is a Statistical Model?""",,"I'm relatively new to posting here so am not sure if this is the type of question I can ask, but I have been trying to understand for some time this paper by Peter McCullagh (it's not as long as it looks): https://pdfs.semanticscholar.org/4a01/7dd1ace17979828bb9f57e26ccf9c91f0b3b.pdf I have read some basic category theory so am able to follow along with most of the definitions. However, I am confused about how one would model something with no covariates, say, parametric estimation for a sequence of i.i.d. random variables, in this formulation, since it seems to be necessary to have a nonempty covariate space in order to construct the parameter space. Furthermore, we couldn't even have arrows from $U$ to $\Omega$ if $\Omega$ were empty (""a design is a map associating with each unit $u \in U$ a point $x_u \in \Omega$""p.1234). This is where I begin to question whether I understand this formulation at all. Would love some clarification on these definitions if anyone has read this paper (or feels inclined to skim through it). The construction of the definition of a statistical model is on p.1235. edit: This is the basic setup: There are three main building blocks with three different categories $cat_U$ the category with sets of statistical units as objects, injective maps as morphisms $cat_\Omega$ the category of covaraite spaces $cat_V$ the category of response scales Then we define a design to be a map $x:U \rightarrow \Omega$ and the set of all such designs to be a category $cat_D$. This is all on p.1234-1236. A choice of response scale $V$ determines an object $V^\Omega$. In the linear model our parameter space $\Theta_\Omega$ is a subspace $\subset V^\Omega$. A linear model is determined by a subrepresentation $\Theta_\Omega \subset V^\Omega$, together with a design pullback $\psi^*$: $$ U \overset{\psi}{\rightarrow} \Omega \ \ \ \ \ \mathcal{S} = V^U \overset{\psi^*}{\leftarrow}\Theta_\Omega $$ When we add a dispersion parameter to the parameter space, $$ U \overset{\psi}{\rightarrow} \Omega \ \ \ \ \ \mathcal{P}(V^U) \overset{P_\psi}{\leftarrow}\Theta_\Omega $$ we get the probability distributions on the sample space.","I'm relatively new to posting here so am not sure if this is the type of question I can ask, but I have been trying to understand for some time this paper by Peter McCullagh (it's not as long as it looks): https://pdfs.semanticscholar.org/4a01/7dd1ace17979828bb9f57e26ccf9c91f0b3b.pdf I have read some basic category theory so am able to follow along with most of the definitions. However, I am confused about how one would model something with no covariates, say, parametric estimation for a sequence of i.i.d. random variables, in this formulation, since it seems to be necessary to have a nonempty covariate space in order to construct the parameter space. Furthermore, we couldn't even have arrows from $U$ to $\Omega$ if $\Omega$ were empty (""a design is a map associating with each unit $u \in U$ a point $x_u \in \Omega$""p.1234). This is where I begin to question whether I understand this formulation at all. Would love some clarification on these definitions if anyone has read this paper (or feels inclined to skim through it). The construction of the definition of a statistical model is on p.1235. edit: This is the basic setup: There are three main building blocks with three different categories $cat_U$ the category with sets of statistical units as objects, injective maps as morphisms $cat_\Omega$ the category of covaraite spaces $cat_V$ the category of response scales Then we define a design to be a map $x:U \rightarrow \Omega$ and the set of all such designs to be a category $cat_D$. This is all on p.1234-1236. A choice of response scale $V$ determines an object $V^\Omega$. In the linear model our parameter space $\Theta_\Omega$ is a subspace $\subset V^\Omega$. A linear model is determined by a subrepresentation $\Theta_\Omega \subset V^\Omega$, together with a design pullback $\psi^*$: $$ U \overset{\psi}{\rightarrow} \Omega \ \ \ \ \ \mathcal{S} = V^U \overset{\psi^*}{\leftarrow}\Theta_\Omega $$ When we add a dispersion parameter to the parameter space, $$ U \overset{\psi}{\rightarrow} \Omega \ \ \ \ \ \mathcal{P}(V^U) \overset{P_\psi}{\leftarrow}\Theta_\Omega $$ we get the probability distributions on the sample space.",,"['probability', 'probability-theory', 'statistics', 'category-theory']"
84,"In Wasserstein, what is the relationship between $W_2 (\widehat{\mathbb{P}}_{N},\mathbb{P})$ and $W_2 (\widehat{\mathbb{P}}_{N}^{x},\mathbb{P}^{x})$?","In Wasserstein, what is the relationship between  and ?","W_2 (\widehat{\mathbb{P}}_{N},\mathbb{P}) W_2 (\widehat{\mathbb{P}}_{N}^{x},\mathbb{P}^{x})","Before presenting my question (which I already formulate in the title of this post)  is important to establish the context of my problem: Definition: The $p$-Wasserstein metric $W_{p}(\mu,\nu)$ between $\mu,\nu\in\mathcal{P}_{p}(\Xi)$ is defined by   $$W_{p}^{p}(\mu,\nu):=\min_{\Pi\in\mathcal{P}(\Xi\times\Xi)}\left\{\int_{\Xi\times\Xi}d^{p}(\xi,\zeta)\Pi(d\xi,d\zeta)\: :\: \Pi(\cdot \times\Xi)=\mu(\cdot),\: \Pi(\Xi\times\cdot)=\nu(\cdot)\right\}$$   where    $$\mathcal{P}_{p}(\Xi):=\left\{\mu\in\mathcal{P}(\Xi)\: :\: \int_{\Xi}d^{p}(\xi,\zeta_{0})\mu(d\xi) < \infty\ \mbox{for some }\zeta_{0}\in\Xi\right\}$$   where $d$ is a metric on  $\Xi$. The $p$-Wasserstein metric (with $p\geq 1$) is also defined for any measure in $\mathcal{P}(\Xi)$ the space of all the measures of probability, the only difference is that in that set it can take values as infinite. We consider $\Xi=\mathbb{R}^{m}$ and $\xi$ a random vector with support in $\Xi$ and distribution $\mathbb{P}$, let $\widehat{\xi}_{1},\ldots,\widehat{\xi}_{N} $ be a sample of $\xi$, then we consider the empirical distributión given by $$\widehat{\mathbb{P}}_{N}:=\sum_{i=1}^{N}\delta_{\widehat{\xi}_{i}}.$$ Given $x\in\mathbb{R}^{m}$ we consider the random variable $\zeta^{x}:=\langle x,\xi\rangle$, let $\mathbb{P}^{x}$ the distribution of $\zeta^{x}$, note that as   $\widehat{\xi}_{1},\ldots,\widehat{\xi}_{N} $ is a sample of $\xi$,  then  $\widehat{\zeta}^{x}_{1},\ldots,\widehat{\zeta}^{x}_{N}$ given  by $\widehat{\zeta}^{x}_{i}:=\langle x,\widehat{\xi}_{i}\rangle$ is a sample of $\zeta^{x}$, therefore,  we have $\widehat{P}^{x}_{N}$ the empirical distribution given by $$\widehat{\mathbb{P}}_{N}^{x}:=\sum_{i=1}^{N}\delta_{\widehat{\zeta}_{i}^{x}}.$$ From now on we consider the $2$-Wasserstein metric with $d$ as euclidean distance, that is $d(x,y)=\left\|x-y\right\|$ where $\left\|\cdot\right\|$ is the euclidean norm. The question: What is the relationship between $W_2^2(\widehat{\mathbb{P}}_{N},\mathbb{P})$ and $W^{2}_{2}(\widehat{\mathbb{P}}_{N}^{x},\mathbb{P}^{x})$? My attepmt: I think that $$ W^{2}_{2}(\widehat{\mathbb{P}}_{N}^{x},\mathbb{P}^{x}) \leq \left\|x\right\|^{2}  W_{2}^{2}\left( \widehat{\mathbb{P}}_{N} ,\mathbb{P}\right)$$ The next is my attempt to prove it, although there are steps of which I do not feel safe. We consider $\widehat{\widehat{\xi}}_{1},\ldots,\widehat{\widehat{\xi}}_{M}$ other sample of $\xi$, then we consider $\widehat{\widehat{\mathbb{P}}}_{M}$ the empirical distribution determined by this sample. Also, we consider of sample   $\widehat{\widehat{\zeta}}^{x}_{1},\ldots,\widehat{\widehat{\zeta}}^{x}_{M}$ of $\zeta^{x}$ given by $\widehat{\widehat{\zeta}}^{x}_{i}:=\left\langle x, \widehat{\widehat{\xi}}_{i}\right\rangle$ and $\widehat{\widehat{\mathbb{P}}}^{x}_{M}$ the empirical distribution determined by this sample. We know that $ \widehat{\widehat{\mathbb{P}}}_{M} \rightarrow \mathbb{P}$ weakly, then,  by Corollary 6.11 in Villani , we have $$W_{2}^{2}\left( \widehat{\mathbb{P}}_{N} ,\widehat{\widehat{\mathbb{P}}}_{M}\right)\overset{{\scriptstyle M\rightarrow \infty}}{\longrightarrow}W_{2}^{2}\left( \widehat{\mathbb{P}}_{N} ,\mathbb{P}\right). \tag{I}$$ Analogously we have $ \widehat{\widehat{\mathbb{P}}}^{x}_{M} \rightarrow \mathbb{P}^{x}$ weakly, then,  by Corollary 6.11 in Villani , we have $$W_{2}^{2}\left( \widehat{\mathbb{P}}^{x}_{N} ,\widehat{\widehat{\mathbb{P}}}^{x}_{M}\right)\overset{{\scriptstyle M\rightarrow \infty}}{\longrightarrow}W_{2}^{2}\left( \widehat{\mathbb{P}}_{N}^{x} ,\mathbb{P}^{x}\right). \tag{II}$$ But note that $$ \begin{align} W_{2}^{2}\left( \widehat{\mathbb{P}}^{x}_{N} ,\widehat{\widehat{\mathbb{P}}}^{x}_{M}\right) &= \inf\left\{\sum_{i=1}^{N}\sum_{j=1}^{M} \lambda_{i,j}\left|\widehat{\zeta}^{x}_{i}-\widehat{\widehat{\zeta}}^{x}_{j}  \right|^{2} \:\left|\: \begin{array}{l} \sum_{i=1}^{N}\lambda_{i,j}=\frac{1}{M},\\ \sum_{j=1}^{M}\lambda_{i,j}=\frac{1}{N},\\ \lambda_{i,j}\geq 0, \\ i=1,\ldots,N,\\ j=1,\ldots,M \end{array}  \right.\right\} \\ &= \inf\left\{\sum_{i=1}^{N}\sum_{j=1}^{M} \lambda_{i,j}\left|\left\langle x, \widehat{\xi}_{i}\right\rangle-\left\langle x, \widehat{\widehat{\xi}}_{j}\right\rangle   \right|^{2} \:\left|\: \begin{array}{l} \sum_{i=1}^{N}\lambda_{i,j}=\frac{1}{M},\\ \sum_{j=1}^{M}\lambda_{i,j}=\frac{1}{N},\\ \lambda_{i,j}\geq 0, \\ i=1,\ldots,N,\\ j=1,\ldots,M \end{array}  \right.\right\}  \\ &\leq  \inf\left\{\sum_{i=1}^{N}\sum_{j=1}^{M} \lambda_{i,j} \left\|x\right\|^{2}\left\| \widehat{\xi}_{i}- \widehat{\widehat{\xi}}_{j}  \right\|^{2} \:\left| \: \begin{array}{l} \sum_{i=1}^{N}\lambda_{i,j}=\frac{1}{M},\\ \sum_{j=1}^{M}\lambda_{i,j}=\frac{1}{N},\\ \lambda_{i,j}\geq 0, \\ i=1,\ldots,N,\\ j=1,\ldots,M \end{array}  \right.\right\} \:\: \begin{array}{l}\mbox{by Hölder}\\ \mbox{inequality}\end{array}\\ &= \left\|x\right\|^{2}W_{2}^{2}\left( \widehat{\mathbb{P}}_{N} ,\widehat{\widehat{\mathbb{P}}}_{M}\right). \end{align}  $$ Therefore, by $(I)$ and $(II)$ we have  $$W_{2}^{2}\left( \widehat{\mathbb{P}}_{N}^{x} ,\mathbb{P}^{x}\right)\leq \left\|x\right\|^{2} W_{2}^{2}\left( \widehat{\mathbb{P}}_{N} ,\mathbb{P}\right).$$ Remark: I feel that it was a very simple demonstration, for this reason I do not trust my argument, I would like someone to help me see if my reasoning has errors. In my research this result is very important, for that reason I need to know if my idea is correct. I published my question here because I feel that here are the people trained to answer it.","Before presenting my question (which I already formulate in the title of this post)  is important to establish the context of my problem: Definition: The $p$-Wasserstein metric $W_{p}(\mu,\nu)$ between $\mu,\nu\in\mathcal{P}_{p}(\Xi)$ is defined by   $$W_{p}^{p}(\mu,\nu):=\min_{\Pi\in\mathcal{P}(\Xi\times\Xi)}\left\{\int_{\Xi\times\Xi}d^{p}(\xi,\zeta)\Pi(d\xi,d\zeta)\: :\: \Pi(\cdot \times\Xi)=\mu(\cdot),\: \Pi(\Xi\times\cdot)=\nu(\cdot)\right\}$$   where    $$\mathcal{P}_{p}(\Xi):=\left\{\mu\in\mathcal{P}(\Xi)\: :\: \int_{\Xi}d^{p}(\xi,\zeta_{0})\mu(d\xi) < \infty\ \mbox{for some }\zeta_{0}\in\Xi\right\}$$   where $d$ is a metric on  $\Xi$. The $p$-Wasserstein metric (with $p\geq 1$) is also defined for any measure in $\mathcal{P}(\Xi)$ the space of all the measures of probability, the only difference is that in that set it can take values as infinite. We consider $\Xi=\mathbb{R}^{m}$ and $\xi$ a random vector with support in $\Xi$ and distribution $\mathbb{P}$, let $\widehat{\xi}_{1},\ldots,\widehat{\xi}_{N} $ be a sample of $\xi$, then we consider the empirical distributión given by $$\widehat{\mathbb{P}}_{N}:=\sum_{i=1}^{N}\delta_{\widehat{\xi}_{i}}.$$ Given $x\in\mathbb{R}^{m}$ we consider the random variable $\zeta^{x}:=\langle x,\xi\rangle$, let $\mathbb{P}^{x}$ the distribution of $\zeta^{x}$, note that as   $\widehat{\xi}_{1},\ldots,\widehat{\xi}_{N} $ is a sample of $\xi$,  then  $\widehat{\zeta}^{x}_{1},\ldots,\widehat{\zeta}^{x}_{N}$ given  by $\widehat{\zeta}^{x}_{i}:=\langle x,\widehat{\xi}_{i}\rangle$ is a sample of $\zeta^{x}$, therefore,  we have $\widehat{P}^{x}_{N}$ the empirical distribution given by $$\widehat{\mathbb{P}}_{N}^{x}:=\sum_{i=1}^{N}\delta_{\widehat{\zeta}_{i}^{x}}.$$ From now on we consider the $2$-Wasserstein metric with $d$ as euclidean distance, that is $d(x,y)=\left\|x-y\right\|$ where $\left\|\cdot\right\|$ is the euclidean norm. The question: What is the relationship between $W_2^2(\widehat{\mathbb{P}}_{N},\mathbb{P})$ and $W^{2}_{2}(\widehat{\mathbb{P}}_{N}^{x},\mathbb{P}^{x})$? My attepmt: I think that $$ W^{2}_{2}(\widehat{\mathbb{P}}_{N}^{x},\mathbb{P}^{x}) \leq \left\|x\right\|^{2}  W_{2}^{2}\left( \widehat{\mathbb{P}}_{N} ,\mathbb{P}\right)$$ The next is my attempt to prove it, although there are steps of which I do not feel safe. We consider $\widehat{\widehat{\xi}}_{1},\ldots,\widehat{\widehat{\xi}}_{M}$ other sample of $\xi$, then we consider $\widehat{\widehat{\mathbb{P}}}_{M}$ the empirical distribution determined by this sample. Also, we consider of sample   $\widehat{\widehat{\zeta}}^{x}_{1},\ldots,\widehat{\widehat{\zeta}}^{x}_{M}$ of $\zeta^{x}$ given by $\widehat{\widehat{\zeta}}^{x}_{i}:=\left\langle x, \widehat{\widehat{\xi}}_{i}\right\rangle$ and $\widehat{\widehat{\mathbb{P}}}^{x}_{M}$ the empirical distribution determined by this sample. We know that $ \widehat{\widehat{\mathbb{P}}}_{M} \rightarrow \mathbb{P}$ weakly, then,  by Corollary 6.11 in Villani , we have $$W_{2}^{2}\left( \widehat{\mathbb{P}}_{N} ,\widehat{\widehat{\mathbb{P}}}_{M}\right)\overset{{\scriptstyle M\rightarrow \infty}}{\longrightarrow}W_{2}^{2}\left( \widehat{\mathbb{P}}_{N} ,\mathbb{P}\right). \tag{I}$$ Analogously we have $ \widehat{\widehat{\mathbb{P}}}^{x}_{M} \rightarrow \mathbb{P}^{x}$ weakly, then,  by Corollary 6.11 in Villani , we have $$W_{2}^{2}\left( \widehat{\mathbb{P}}^{x}_{N} ,\widehat{\widehat{\mathbb{P}}}^{x}_{M}\right)\overset{{\scriptstyle M\rightarrow \infty}}{\longrightarrow}W_{2}^{2}\left( \widehat{\mathbb{P}}_{N}^{x} ,\mathbb{P}^{x}\right). \tag{II}$$ But note that $$ \begin{align} W_{2}^{2}\left( \widehat{\mathbb{P}}^{x}_{N} ,\widehat{\widehat{\mathbb{P}}}^{x}_{M}\right) &= \inf\left\{\sum_{i=1}^{N}\sum_{j=1}^{M} \lambda_{i,j}\left|\widehat{\zeta}^{x}_{i}-\widehat{\widehat{\zeta}}^{x}_{j}  \right|^{2} \:\left|\: \begin{array}{l} \sum_{i=1}^{N}\lambda_{i,j}=\frac{1}{M},\\ \sum_{j=1}^{M}\lambda_{i,j}=\frac{1}{N},\\ \lambda_{i,j}\geq 0, \\ i=1,\ldots,N,\\ j=1,\ldots,M \end{array}  \right.\right\} \\ &= \inf\left\{\sum_{i=1}^{N}\sum_{j=1}^{M} \lambda_{i,j}\left|\left\langle x, \widehat{\xi}_{i}\right\rangle-\left\langle x, \widehat{\widehat{\xi}}_{j}\right\rangle   \right|^{2} \:\left|\: \begin{array}{l} \sum_{i=1}^{N}\lambda_{i,j}=\frac{1}{M},\\ \sum_{j=1}^{M}\lambda_{i,j}=\frac{1}{N},\\ \lambda_{i,j}\geq 0, \\ i=1,\ldots,N,\\ j=1,\ldots,M \end{array}  \right.\right\}  \\ &\leq  \inf\left\{\sum_{i=1}^{N}\sum_{j=1}^{M} \lambda_{i,j} \left\|x\right\|^{2}\left\| \widehat{\xi}_{i}- \widehat{\widehat{\xi}}_{j}  \right\|^{2} \:\left| \: \begin{array}{l} \sum_{i=1}^{N}\lambda_{i,j}=\frac{1}{M},\\ \sum_{j=1}^{M}\lambda_{i,j}=\frac{1}{N},\\ \lambda_{i,j}\geq 0, \\ i=1,\ldots,N,\\ j=1,\ldots,M \end{array}  \right.\right\} \:\: \begin{array}{l}\mbox{by Hölder}\\ \mbox{inequality}\end{array}\\ &= \left\|x\right\|^{2}W_{2}^{2}\left( \widehat{\mathbb{P}}_{N} ,\widehat{\widehat{\mathbb{P}}}_{M}\right). \end{align}  $$ Therefore, by $(I)$ and $(II)$ we have  $$W_{2}^{2}\left( \widehat{\mathbb{P}}_{N}^{x} ,\mathbb{P}^{x}\right)\leq \left\|x\right\|^{2} W_{2}^{2}\left( \widehat{\mathbb{P}}_{N} ,\mathbb{P}\right).$$ Remark: I feel that it was a very simple demonstration, for this reason I do not trust my argument, I would like someone to help me see if my reasoning has errors. In my research this result is very important, for that reason I need to know if my idea is correct. I published my question here because I feel that here are the people trained to answer it.",,"['probability', 'probability-theory', 'measure-theory', 'probability-distributions', 'weak-convergence']"
85,Implications of conditional independence between random variables,Implications of conditional independence between random variables,,"Consider two probability spaces $(\mathcal{I}, \mathbb{P}, \mathcal{F})$, $(\mathcal{J}, \mathbb{P}, \mathcal{G})$. Take any $a\in \mathbb{R}$. In the notation below, $1\{\cdots\}$ is $1$ if the condition inside is satisfied and $0$ otherwise. Consider the following random variables: 1) $\forall i \in \mathcal{I}$, $e^i: \mathcal{J}\rightarrow \mathbb{R}$ 2) $Z: \mathcal{J}\rightarrow \mathcal{Z}\subseteq \mathbb{R}$, with $\mathcal{Z}$ finite 3) $W: \mathcal{I}\rightarrow [0,1]$, where $W(i)\equiv \mathbb{P}\Big(\{j\in \mathcal{J} \text{ s.t. } e^i(j)\leq a\}\Big)$ 4) $Q: \mathcal{I}\rightarrow  \{0,1\}$, where $Q(i)\equiv 1\{W(i)>0\}$ 5) $\forall z \in \mathcal{Z}$, $W_z: \mathcal{I}\rightarrow [0,1]$, where $W_z(i)\equiv \mathbb{P}\Big(\{j\in \mathcal{J} \text{ s.t. } e^i(j)\leq a\}\Big| \{j\in \mathcal{J} \text{ s.t. } Z(j)=z\}\Big)$ 6) $\forall z \in \mathcal{Z}$, $Q_z: \mathcal{I}\rightarrow \{0,1\}$, where $Q_z(i)\equiv 1\{W_z(i)>0\}$ Assume that for some $z\in \mathcal{Z}$ $$ E\Big(W\Big)= E\Big(W_z\Big) $$ where $E$ denotes expectation. Does this imply $$ E\Big(Q \Big)= E\Big(Q_z\Big) \text{ ?} $$ I've done some simulations and it seems that the answer is no but I would like some help to formalise this. Also, are we using somewhere that $\mathcal{Z}$ is finite and that the two probability spaces have the same probability measure?","Consider two probability spaces $(\mathcal{I}, \mathbb{P}, \mathcal{F})$, $(\mathcal{J}, \mathbb{P}, \mathcal{G})$. Take any $a\in \mathbb{R}$. In the notation below, $1\{\cdots\}$ is $1$ if the condition inside is satisfied and $0$ otherwise. Consider the following random variables: 1) $\forall i \in \mathcal{I}$, $e^i: \mathcal{J}\rightarrow \mathbb{R}$ 2) $Z: \mathcal{J}\rightarrow \mathcal{Z}\subseteq \mathbb{R}$, with $\mathcal{Z}$ finite 3) $W: \mathcal{I}\rightarrow [0,1]$, where $W(i)\equiv \mathbb{P}\Big(\{j\in \mathcal{J} \text{ s.t. } e^i(j)\leq a\}\Big)$ 4) $Q: \mathcal{I}\rightarrow  \{0,1\}$, where $Q(i)\equiv 1\{W(i)>0\}$ 5) $\forall z \in \mathcal{Z}$, $W_z: \mathcal{I}\rightarrow [0,1]$, where $W_z(i)\equiv \mathbb{P}\Big(\{j\in \mathcal{J} \text{ s.t. } e^i(j)\leq a\}\Big| \{j\in \mathcal{J} \text{ s.t. } Z(j)=z\}\Big)$ 6) $\forall z \in \mathcal{Z}$, $Q_z: \mathcal{I}\rightarrow \{0,1\}$, where $Q_z(i)\equiv 1\{W_z(i)>0\}$ Assume that for some $z\in \mathcal{Z}$ $$ E\Big(W\Big)= E\Big(W_z\Big) $$ where $E$ denotes expectation. Does this imply $$ E\Big(Q \Big)= E\Big(Q_z\Big) \text{ ?} $$ I've done some simulations and it seems that the answer is no but I would like some help to formalise this. Also, are we using somewhere that $\mathcal{Z}$ is finite and that the two probability spaces have the same probability measure?",,"['probability', 'random-variables', 'conditional-expectation']"
86,Maximizing expectation integral given PDF inequality,Maximizing expectation integral given PDF inequality,,"I came across a paper that in the context of one of its proofs takes an upper bound on a random variable's probability distribution and turns it into a bound on its expected value. In particular, let $X$ be a random variable with PDF $f$. The result is that if $$P(X \geq x) = \int_x^\infty f(\gamma) d\gamma \leq 2 e^{-2mx^2}$$ then $$E\left[e^{2(m-1)X^2}\right] = \int_0^\infty f(\gamma)e^{2(m-1)\gamma^2} d\gamma \leq 4m.$$ The paper proposes a proof saying that the first constraint is met with equality for $g(\gamma) = 8 m \gamma e^{-2m\gamma^2}$ and that this distribution will maximize the expected value expression. For this distribution one can compute the expected value to be $E\left[e^{2(m-1)X^2}\right] = \int_0^\infty 8 m \gamma e^{-2m\gamma^2}e^{2(m-1)\gamma^2} d\gamma = 4m$. I believe that the result is true (I came up with a different proof) but I am confused about the validity of this argument. Why is it true that a distribution, $g$, that achieves equality in the first constraint will also maximize the expected value expression? If it was true that for such a $g$ we knew $g(\gamma) \geq f(\gamma)$ for any $f$ satisfying the first inequality then I could see why this argument is valid. However, I don't think that's necessarily true. Consider the following counterexample. Suppose that $\int_x^1 f(\gamma) d\gamma \leq 1 - x$. Equality can be achieved for $g(\gamma) = 1$. However, $f(\gamma) =  3/2 e^{-\gamma}$ also meets the inequality and yet it is not true that $ 3/2 e^{-\gamma}  \leq 1$ for all $\gamma$. So I am wondering if this argument is valid and if so how?","I came across a paper that in the context of one of its proofs takes an upper bound on a random variable's probability distribution and turns it into a bound on its expected value. In particular, let $X$ be a random variable with PDF $f$. The result is that if $$P(X \geq x) = \int_x^\infty f(\gamma) d\gamma \leq 2 e^{-2mx^2}$$ then $$E\left[e^{2(m-1)X^2}\right] = \int_0^\infty f(\gamma)e^{2(m-1)\gamma^2} d\gamma \leq 4m.$$ The paper proposes a proof saying that the first constraint is met with equality for $g(\gamma) = 8 m \gamma e^{-2m\gamma^2}$ and that this distribution will maximize the expected value expression. For this distribution one can compute the expected value to be $E\left[e^{2(m-1)X^2}\right] = \int_0^\infty 8 m \gamma e^{-2m\gamma^2}e^{2(m-1)\gamma^2} d\gamma = 4m$. I believe that the result is true (I came up with a different proof) but I am confused about the validity of this argument. Why is it true that a distribution, $g$, that achieves equality in the first constraint will also maximize the expected value expression? If it was true that for such a $g$ we knew $g(\gamma) \geq f(\gamma)$ for any $f$ satisfying the first inequality then I could see why this argument is valid. However, I don't think that's necessarily true. Consider the following counterexample. Suppose that $\int_x^1 f(\gamma) d\gamma \leq 1 - x$. Equality can be achieved for $g(\gamma) = 1$. However, $f(\gamma) =  3/2 e^{-\gamma}$ also meets the inequality and yet it is not true that $ 3/2 e^{-\gamma}  \leq 1$ for all $\gamma$. So I am wondering if this argument is valid and if so how?",,"['probability', 'expectation', 'integral-inequality']"
87,Heavy tailed distributions and their sum,Heavy tailed distributions and their sum,,"Let $X_{1}, X_{2}, \ldots, X_{n}$ be the sequence of i.i.d random variables with heavvy tailed distributions, i.e.  $$p(x_{i}) \sim \frac{A}{x_{i}^{\alpha}}$$  as $x_{i} \rightarrow \infty$, where $p(x_{i})$ stands for the density of $X_{i}$. The question is: how to estimate the asymtotic of the density $p(y)$, where $$Y = X_{1} + X_{2} + \ldots + X_{n}$$ ? A pretty straightforward approach is the following: calculate the characteristic function of $X_{i}$, since the random variables are i.i.d, then $$\varphi_{Y}(t) = \varphi_{X_{1} + X_{2} + \ldots + X_{n}}(t) = (\varphi_{X_{1}}(t))^{n}$$ then apply the inverse Fourier transform to figure out the distribution of sum. Are there any, say, 'elegant' ways to approach the problem above?","Let $X_{1}, X_{2}, \ldots, X_{n}$ be the sequence of i.i.d random variables with heavvy tailed distributions, i.e.  $$p(x_{i}) \sim \frac{A}{x_{i}^{\alpha}}$$  as $x_{i} \rightarrow \infty$, where $p(x_{i})$ stands for the density of $X_{i}$. The question is: how to estimate the asymtotic of the density $p(y)$, where $$Y = X_{1} + X_{2} + \ldots + X_{n}$$ ? A pretty straightforward approach is the following: calculate the characteristic function of $X_{i}$, since the random variables are i.i.d, then $$\varphi_{Y}(t) = \varphi_{X_{1} + X_{2} + \ldots + X_{n}}(t) = (\varphi_{X_{1}}(t))^{n}$$ then apply the inverse Fourier transform to figure out the distribution of sum. Are there any, say, 'elegant' ways to approach the problem above?",,"['probability', 'probability-theory', 'probability-distributions', 'characteristic-functions', 'distribution-tails']"
88,From conditional independence to independence,From conditional independence to independence,,"Let $(\Omega, \mathcal{F}, {\bf P})$ be a probability space. Let $A, B \in \mathcal{F}$ be two events and $Z$ a random variable with values in a nice space $(S, \mathcal{S})$. Suppose that $A$ and $B$ are conditionally independent given $Z$ (i.e.,  independent of the $\sigma$-algebra generated by $Z$). It is known that $A$ and $B$ are not necessarily independent. However, it seems possible to show that if $A$ and $B$ are themself independent of $Z$, then $A$ and $B$ are also independent. Is the proof below correct ? If yes could you give some intuition of why ? Thanks a lot ! ${\bf Proof:}$ Denote by ${\bf P}( . | Z = z)$ the regular conditional probability of ${\bf P}$ given $Z$ and by ${\bf P}(Z \in dz)$ the law of $Z$. We have $${\bf P}(A \cap B) = {\bf P}((A \cap B) \cap \{Z \in S\}) = \int_S{\bf P}(A \cap B | Z = z){\bf P}(Z \in dz)$$ $$= \int_S{\bf P}(A | Z = z){\bf P}(B | Z = z){\bf P}(Z \in dz)$$ $$= \int_S{\bf P}(A){\bf P}(B){\bf P}(Z \in dz) = {\bf P}(A){\bf P}(B).$$","Let $(\Omega, \mathcal{F}, {\bf P})$ be a probability space. Let $A, B \in \mathcal{F}$ be two events and $Z$ a random variable with values in a nice space $(S, \mathcal{S})$. Suppose that $A$ and $B$ are conditionally independent given $Z$ (i.e.,  independent of the $\sigma$-algebra generated by $Z$). It is known that $A$ and $B$ are not necessarily independent. However, it seems possible to show that if $A$ and $B$ are themself independent of $Z$, then $A$ and $B$ are also independent. Is the proof below correct ? If yes could you give some intuition of why ? Thanks a lot ! ${\bf Proof:}$ Denote by ${\bf P}( . | Z = z)$ the regular conditional probability of ${\bf P}$ given $Z$ and by ${\bf P}(Z \in dz)$ the law of $Z$. We have $${\bf P}(A \cap B) = {\bf P}((A \cap B) \cap \{Z \in S\}) = \int_S{\bf P}(A \cap B | Z = z){\bf P}(Z \in dz)$$ $$= \int_S{\bf P}(A | Z = z){\bf P}(B | Z = z){\bf P}(Z \in dz)$$ $$= \int_S{\bf P}(A){\bf P}(B){\bf P}(Z \in dz) = {\bf P}(A){\bf P}(B).$$",,"['probability', 'independence']"
89,Inverse bin ball problem,Inverse bin ball problem,,"(Sorry for the title. I has difficulty summarising this problem. I am open to suggestions for a new title.) Suppose there are a random number of bins of random discrete sizes, each of which contain a random number of balls. Every ball has size 1. The setup is as follows: Bin $i$ has size $S_i$ and contains $K_i$ balls. The total number of balls, $B$, is given by $\sum_{i=1}^N K_i$. The combined volume of all bins, $V$, is given by $\sum_{i=1}^N S_i$. Assume the following are given: $P(S_i = s)$ for all $s \in \{1, 2, 3,...\}$ $P(K_i = k | S_i = s)$ for all $k,s \in \{1, 2, 3,...\}$ $P(B = b)$ for all $b \in \{1, 2, 3,...\}$ $P(V = v)$ for all $v \in \{1, 2, 3,...\}$ You may also assume that none of the above variables can take the value $0$. The problem is this. In terms of the above, find an expression for: $P(N=n)$, the probability that the number of bins is $n$.","(Sorry for the title. I has difficulty summarising this problem. I am open to suggestions for a new title.) Suppose there are a random number of bins of random discrete sizes, each of which contain a random number of balls. Every ball has size 1. The setup is as follows: Bin $i$ has size $S_i$ and contains $K_i$ balls. The total number of balls, $B$, is given by $\sum_{i=1}^N K_i$. The combined volume of all bins, $V$, is given by $\sum_{i=1}^N S_i$. Assume the following are given: $P(S_i = s)$ for all $s \in \{1, 2, 3,...\}$ $P(K_i = k | S_i = s)$ for all $k,s \in \{1, 2, 3,...\}$ $P(B = b)$ for all $b \in \{1, 2, 3,...\}$ $P(V = v)$ for all $v \in \{1, 2, 3,...\}$ You may also assume that none of the above variables can take the value $0$. The problem is this. In terms of the above, find an expression for: $P(N=n)$, the probability that the number of bins is $n$.",,"['probability', 'combinatorics', 'probability-theory', 'probability-distributions', 'combinations']"
90,Expected duration of Gossip spread,Expected duration of Gossip spread,,"This is a repost of a question by me earlier(which I deleted as it had less details), Consider the following algorithm (phone call model) of spreading a gossip/rumor - On day $1$, only $1$ person knows the rumor, he calls randomly independently a person in the city of total $n$ persons and tells the rumor to that person. The process is repeated by all those that know the rumor on a given day (each knower calls a single person daily,who may or may not know the rumor already). This is repeated till all know the rumor after some days. I want to show the expected duration till all people in the city know, is $O(\log n)$. I wish to know if the following approach is correct : At any day, suppose I have $k$ number of people who know the rumor, and $T_{k+1}$ be the time(day number) when a new $(k+1)^{th}$ member comes to know. Then since, at this instant, for any person who knows the rumor, probability of selecting new person to inform rumor is $\frac{n-k}{n-1}$, and by linearity of expectation, number of new person increased in a day $= \frac {k(n-k)}{n-1} = d$(say). So time to get one additional member who knows is $= 1/d = E(T_{k+1} - T_{k}) $, as it is a geometric variable, then I can apply $E(T_n) = \sum_{k=0}^{k=n-1}E(T_{k+1}- T_k) $to get $E(T_n) =$ harmonic number $\approx \ln n$","This is a repost of a question by me earlier(which I deleted as it had less details), Consider the following algorithm (phone call model) of spreading a gossip/rumor - On day $1$, only $1$ person knows the rumor, he calls randomly independently a person in the city of total $n$ persons and tells the rumor to that person. The process is repeated by all those that know the rumor on a given day (each knower calls a single person daily,who may or may not know the rumor already). This is repeated till all know the rumor after some days. I want to show the expected duration till all people in the city know, is $O(\log n)$. I wish to know if the following approach is correct : At any day, suppose I have $k$ number of people who know the rumor, and $T_{k+1}$ be the time(day number) when a new $(k+1)^{th}$ member comes to know. Then since, at this instant, for any person who knows the rumor, probability of selecting new person to inform rumor is $\frac{n-k}{n-1}$, and by linearity of expectation, number of new person increased in a day $= \frac {k(n-k)}{n-1} = d$(say). So time to get one additional member who knows is $= 1/d = E(T_{k+1} - T_{k}) $, as it is a geometric variable, then I can apply $E(T_n) = \sum_{k=0}^{k=n-1}E(T_{k+1}- T_k) $to get $E(T_n) =$ harmonic number $\approx \ln n$",,"['probability', 'expectation']"
91,"zero-mean RV $𝑿$ with finite fourth moment, probability of being positive [closed]","zero-mean RV  with finite fourth moment, probability of being positive [closed]",𝑿,"Closed. This question is off-topic . It is not currently accepting answers. This question does not appear to be about math within the scope defined in the help center . Closed 6 years ago . Improve this question For zero-mean RV $X$ with finite fourth moment, prove that  $$ P(X>0)\ge \frac{\mathbb{E}(X^2)^2}{4\mathbb{E}(X^4)} $$ I tried Chebyshev with adding $t$ to both sides, but I could not get forth moment.","Closed. This question is off-topic . It is not currently accepting answers. This question does not appear to be about math within the scope defined in the help center . Closed 6 years ago . Improve this question For zero-mean RV $X$ with finite fourth moment, prove that  $$ P(X>0)\ge \frac{\mathbb{E}(X^2)^2}{4\mathbb{E}(X^4)} $$ I tried Chebyshev with adding $t$ to both sides, but I could not get forth moment.",,"['probability', 'inequality', 'expectation', 'upper-lower-bounds']"
92,2 people play a tennis match. Probability of winning?,2 people play a tennis match. Probability of winning?,,"Players A and B play a tennis match that consists of 5 SETS. The probability of A winning the first set is 1/2. If he wins this set, his probability of winning the next set remains 1/2. If he loses, his probability of winning the next set becomes 1/4. If he wins now, the probability of winning the next one goes back up to 1/2, otherwise, it stays at 1/4. What is the probability that A wins the MATCH? My attempt: I considered every possible configuration, like (WWW, WWLW, WLLWW....) and came to the answer 5/16, which is CORRECT. I have two doubts: Is there a more elegant solution to this problem? Surprisingly, if we were to just find the possible 5 letter permutations containing W and L, the ones containing 3 Ws are simply 5 C 3 and the total number of permutations are 2 5 . Hence the probability of a random permutation to have 3 Ws is 5 C 3 / 2 5 . Which is also 5/16! Does this have anything to do with anything? Note: This problem is not a duplicate, since whether the player wins/ loses, affects the probability of winning the next set.","Players A and B play a tennis match that consists of 5 SETS. The probability of A winning the first set is 1/2. If he wins this set, his probability of winning the next set remains 1/2. If he loses, his probability of winning the next set becomes 1/4. If he wins now, the probability of winning the next one goes back up to 1/2, otherwise, it stays at 1/4. What is the probability that A wins the MATCH? My attempt: I considered every possible configuration, like (WWW, WWLW, WLLWW....) and came to the answer 5/16, which is CORRECT. I have two doubts: Is there a more elegant solution to this problem? Surprisingly, if we were to just find the possible 5 letter permutations containing W and L, the ones containing 3 Ws are simply 5 C 3 and the total number of permutations are 2 5 . Hence the probability of a random permutation to have 3 Ws is 5 C 3 / 2 5 . Which is also 5/16! Does this have anything to do with anything? Note: This problem is not a duplicate, since whether the player wins/ loses, affects the probability of winning the next set.",,"['probability', 'combinatorics', 'permutations']"
93,What is the expected length of the diameter of a special random graph?,What is the expected length of the diameter of a special random graph?,,"Let $G=(n,p)$ be a random graph . For example, consider that $G$ is the following graph. Initially, the edges of $G$ is undirected. A random $id\in R$ is assigned to each vertex of $G$. The $id$ of vertex $v$ is denoted by $id_{v}$. For each edge $e=(v,u)$, if $id_{v}>id_{u}$, $e$ is converted to a directed edge from $u$ to $v$, and if $id_{v}<id_{u}$, $e$ is converted to a directed edge from $v$ to $u$. A root is a vertex which all incident edges are outgoing edges. In the example, $V_{1}$ is a root. Let $r$ be a root and $v$ be a none-root vertex which has the longest directed distance ($d_{r,v}$) from $r$. What is the expected value of $d_{r,v}$? In the example, $d_{V_{1},V_{2}} = 1, d_{V_{1},V_{3}} = 1,d_{V_{1},V_{4}} = 1,d_{V_{1},V_{5}} = 2$, so $V_{5}$ is a non-root vetex which has the longest directed distance from the root. The following solution is my solution. But I can't compute some parts of it. Is there another solution? There are $P(n,d_{r,v}-1)$ potential different paths with length $d_{r,v}$ from $r$ to $v$. The paths are labeled with numbers $1,...,P(n,d_{r,v}-1)$. The vertices of $i$th path are donoted by $r=\sigma^{i}_{1},\sigma^{i}_{2},...,\sigma^{i}_{d_{r,v}},\sigma^{i}_{d_{r,v}+1}=v$. $X_{i}\text{ }(1\leq i\leq P(n,d_{r,v}-1))$ is a random variable such that  \begin{align} X_{i}=\begin{cases} 1 \quad\text{$i$th potential path, $\sigma^{i}$, is a valid path from $r$ to $v$.}\\ 0 \quad\text{o.w.} \end{cases} \end{align} Thus, $E[d_{r,v}]$ is as follows $$E[d_{r,v}] = \sum_{i=1}^{P(n,d_{r,v}-1)}E[X_{i}]$$ What remains is computing $\Pr(X_{i}=1)$. $X_{i}=1$ when There is a path from $r$ to $v$. The probability of it is $p^{d_{r,v}}$. The $id$s of the path's vertices are monotonically increasing. The probability of it is $\frac{1}{(d_{r,v}+1)!}$. There is no edge from $\sigma^{i}_{j}$ to $\sigma^{i}_{j+2}$ ($1\leq j\leq d_{r,v}-1$), for if there is an edge $(\sigma^{i}_{j},\sigma^{i}_{j+2})$, then the length of the longest directed path from $r$ to $v$ is not $d_{r,v}$ and is $d_{r,v}-1$. The probability of it is $(1-p)^{\frac{(d_{r,v}-1)(d_{r,v}-2)}{2}}$. There is no path with length one from $\sigma^{i}_{j}$ to $\sigma^{i}_{j+3}$ ($1\leq j\leq d_{r,v}-2$). I can't compute it's probability! There is no path with length two from $\sigma^{i}_{j}$ to $\sigma^{i}_{j+4}$ ($1\leq j\leq d_{r,v}-3$). I can't compute it's probability! ...","Let $G=(n,p)$ be a random graph . For example, consider that $G$ is the following graph. Initially, the edges of $G$ is undirected. A random $id\in R$ is assigned to each vertex of $G$. The $id$ of vertex $v$ is denoted by $id_{v}$. For each edge $e=(v,u)$, if $id_{v}>id_{u}$, $e$ is converted to a directed edge from $u$ to $v$, and if $id_{v}<id_{u}$, $e$ is converted to a directed edge from $v$ to $u$. A root is a vertex which all incident edges are outgoing edges. In the example, $V_{1}$ is a root. Let $r$ be a root and $v$ be a none-root vertex which has the longest directed distance ($d_{r,v}$) from $r$. What is the expected value of $d_{r,v}$? In the example, $d_{V_{1},V_{2}} = 1, d_{V_{1},V_{3}} = 1,d_{V_{1},V_{4}} = 1,d_{V_{1},V_{5}} = 2$, so $V_{5}$ is a non-root vetex which has the longest directed distance from the root. The following solution is my solution. But I can't compute some parts of it. Is there another solution? There are $P(n,d_{r,v}-1)$ potential different paths with length $d_{r,v}$ from $r$ to $v$. The paths are labeled with numbers $1,...,P(n,d_{r,v}-1)$. The vertices of $i$th path are donoted by $r=\sigma^{i}_{1},\sigma^{i}_{2},...,\sigma^{i}_{d_{r,v}},\sigma^{i}_{d_{r,v}+1}=v$. $X_{i}\text{ }(1\leq i\leq P(n,d_{r,v}-1))$ is a random variable such that  \begin{align} X_{i}=\begin{cases} 1 \quad\text{$i$th potential path, $\sigma^{i}$, is a valid path from $r$ to $v$.}\\ 0 \quad\text{o.w.} \end{cases} \end{align} Thus, $E[d_{r,v}]$ is as follows $$E[d_{r,v}] = \sum_{i=1}^{P(n,d_{r,v}-1)}E[X_{i}]$$ What remains is computing $\Pr(X_{i}=1)$. $X_{i}=1$ when There is a path from $r$ to $v$. The probability of it is $p^{d_{r,v}}$. The $id$s of the path's vertices are monotonically increasing. The probability of it is $\frac{1}{(d_{r,v}+1)!}$. There is no edge from $\sigma^{i}_{j}$ to $\sigma^{i}_{j+2}$ ($1\leq j\leq d_{r,v}-1$), for if there is an edge $(\sigma^{i}_{j},\sigma^{i}_{j+2})$, then the length of the longest directed path from $r$ to $v$ is not $d_{r,v}$ and is $d_{r,v}-1$. The probability of it is $(1-p)^{\frac{(d_{r,v}-1)(d_{r,v}-2)}{2}}$. There is no path with length one from $\sigma^{i}_{j}$ to $\sigma^{i}_{j+3}$ ($1\leq j\leq d_{r,v}-2$). I can't compute it's probability! There is no path with length two from $\sigma^{i}_{j}$ to $\sigma^{i}_{j+4}$ ($1\leq j\leq d_{r,v}-3$). I can't compute it's probability! ...",,"['probability', 'algorithms', 'random-variables', 'expectation', 'random-graphs']"
94,Does a random variable come from a probability distribution or is it vice-versa?,Does a random variable come from a probability distribution or is it vice-versa?,,I am curious to know which statement is correct: a random variable come from a probability distribution OR a probability distribution is created from observing the behavior of a random variable,I am curious to know which statement is correct: a random variable come from a probability distribution OR a probability distribution is created from observing the behavior of a random variable,,"['probability', 'probability-distributions', 'random']"
95,"Estimate $P(A_1|A_2 \cup A_3 \cup A_4...)$, given $P(A_i|A_j)$","Estimate , given",P(A_1|A_2 \cup A_3 \cup A_4...) P(A_i|A_j),"This question is related to some undergraduate research on summary generation of documents of which I am a part of. I am trying to estimate $P(A_1|A_2 \cup A_3 \cup A_4...A_k)$, where I know the values $P(A_i|A_j)\ \forall i,j \in\{1,2,...,n\}$. I understand that it is not possible to evaluate this probability exactly. Are there methods that relate to the approximation of such an expression under certain assumptions? Eg: Assuming event $A_2,A_3$ are independent. I would be glad if someone could point me to such resources. (webpages,books,papers,etc)","This question is related to some undergraduate research on summary generation of documents of which I am a part of. I am trying to estimate $P(A_1|A_2 \cup A_3 \cup A_4...A_k)$, where I know the values $P(A_i|A_j)\ \forall i,j \in\{1,2,...,n\}$. I understand that it is not possible to evaluate this probability exactly. Are there methods that relate to the approximation of such an expression under certain assumptions? Eg: Assuming event $A_2,A_3$ are independent. I would be glad if someone could point me to such resources. (webpages,books,papers,etc)",,"['probability', 'statistics', 'machine-learning']"
96,Integral of exponential of Gaussian process has a known distribution?,Integral of exponential of Gaussian process has a known distribution?,,"If $f(x)$ is Gaussian process, i.e., $$ f(x) = GP [m(x), \kappa(x, x')] $$ can we say anything about the distribution of the integral of the exponential of this, i.e., $$ F = \int e^{f(x)} dx  $$ or, if it's more helpful, the logarithm $ln[F]$? This previous post uses the Riemann sum to show the distribution of the integral of a Gaussian process is Gaussian, though I've had no luck applying the same idea to the above problem. This post discusses a similar problem but for the complex case. A paper linked in that post discusses the distribution of $\int_0^T exp(u X(t)) dt$ where $X(t)$ is Brownian motion, though I'm not sure I understand the connection between Gaussian processes and Brownian motion enough for this to help.","If $f(x)$ is Gaussian process, i.e., $$ f(x) = GP [m(x), \kappa(x, x')] $$ can we say anything about the distribution of the integral of the exponential of this, i.e., $$ F = \int e^{f(x)} dx  $$ or, if it's more helpful, the logarithm $ln[F]$? This previous post uses the Riemann sum to show the distribution of the integral of a Gaussian process is Gaussian, though I've had no luck applying the same idea to the above problem. This post discusses a similar problem but for the complex case. A paper linked in that post discusses the distribution of $\int_0^T exp(u X(t)) dt$ where $X(t)$ is Brownian motion, though I'm not sure I understand the connection between Gaussian processes and Brownian motion enough for this to help.",,"['probability', 'probability-distributions', 'stochastic-processes']"
97,Condtional Expectation of Order Statistic,Condtional Expectation of Order Statistic,,"Given a a random sample $X_1, X_2, X_3, X_4$ and family of densities $\mathcal{P} = \left\{ f_\theta: \theta \in \Theta \right\}$, where $f_\theta(x) = \frac{1}{2}\mathbb{I}_{[\theta-1, \theta + 1]}$, we can estimate $\theta$ by $\hat{\theta} = X_{(2)} + \frac{1}{5}$. Now we also know that $(S, T)= (X_{(1)}, X_{(4)})$ is a sufficient statistic for $\theta$ so we may Rao-Blackwellize our estimator: $$ \hat{\theta}^*= \mathbb{E}(\hat{\theta}|(X_{(1)},X_{(4)})) = \mathbb{E}(X_{(2)}|(X_{(1)},X_{(4)})) + \frac{1}{5}.$$ Unfortunately I haven't been able to compute the last conditional expectation. Any input would be greatly appreciated! :)","Given a a random sample $X_1, X_2, X_3, X_4$ and family of densities $\mathcal{P} = \left\{ f_\theta: \theta \in \Theta \right\}$, where $f_\theta(x) = \frac{1}{2}\mathbb{I}_{[\theta-1, \theta + 1]}$, we can estimate $\theta$ by $\hat{\theta} = X_{(2)} + \frac{1}{5}$. Now we also know that $(S, T)= (X_{(1)}, X_{(4)})$ is a sufficient statistic for $\theta$ so we may Rao-Blackwellize our estimator: $$ \hat{\theta}^*= \mathbb{E}(\hat{\theta}|(X_{(1)},X_{(4)})) = \mathbb{E}(X_{(2)}|(X_{(1)},X_{(4)})) + \frac{1}{5}.$$ Unfortunately I haven't been able to compute the last conditional expectation. Any input would be greatly appreciated! :)",,"['probability', 'probability-theory', 'statistics', 'order-statistics']"
98,Expected number of coin tosses with a coin that changes over time,Expected number of coin tosses with a coin that changes over time,,"Imagine that I have a coin that changes monotonically over time. -- casino example -- (This is not necessary to understand mathematical problem, but just can help to imagine a real life situation, you can skip it) Imagine that you are in a very clever casino The coin is made of insulator, one side is charged with positive charges and another with negative ones. Croupier flips the coin on a table that is made of a metal board covered with an insulator. Casino charged the metal board with positive charges. The croupier tosses the coin - it has higher probability to end 'positive up' (the negative charges on coin are attracted by positive ones on table) and I am winning. Then between tosses casino slightly charges the metal board with negative charges, so the coin becomes more likely to end 'negative up'. And with time I start loosing - good fortune seems gone. -- end of example -- Let's note $p(t)$ the probability of tossing a tail in a discrete moment $t \ge 0$. We assume that: $p(t+1) > p(t)$, $\lim_{t\rightarrow\infty}p(t) = 1$ Questions: A. How to calculate an expected number of coin tosses to get $n$ consecutive tails? This is a general question, we are just assuming $p(t)$ as above. B. What would be a formula for this particular form of $p(t)$: $$ p(t) = \frac{1}{1+e^{-\frac{1}{\tau}(t-t_0)}} $$ with $t_0 \ge 0$, $\tau > 0$. I am interested in a formula of a form: $$ N(n, t_0, \tau) = \dots $$ I am looking for answers to both questions. But if you only give me an answer to  B., I will be very grateful too. PS This is a generalization of this question .","Imagine that I have a coin that changes monotonically over time. -- casino example -- (This is not necessary to understand mathematical problem, but just can help to imagine a real life situation, you can skip it) Imagine that you are in a very clever casino The coin is made of insulator, one side is charged with positive charges and another with negative ones. Croupier flips the coin on a table that is made of a metal board covered with an insulator. Casino charged the metal board with positive charges. The croupier tosses the coin - it has higher probability to end 'positive up' (the negative charges on coin are attracted by positive ones on table) and I am winning. Then between tosses casino slightly charges the metal board with negative charges, so the coin becomes more likely to end 'negative up'. And with time I start loosing - good fortune seems gone. -- end of example -- Let's note $p(t)$ the probability of tossing a tail in a discrete moment $t \ge 0$. We assume that: $p(t+1) > p(t)$, $\lim_{t\rightarrow\infty}p(t) = 1$ Questions: A. How to calculate an expected number of coin tosses to get $n$ consecutive tails? This is a general question, we are just assuming $p(t)$ as above. B. What would be a formula for this particular form of $p(t)$: $$ p(t) = \frac{1}{1+e^{-\frac{1}{\tau}(t-t_0)}} $$ with $t_0 \ge 0$, $\tau > 0$. I am interested in a formula of a form: $$ N(n, t_0, \tau) = \dots $$ I am looking for answers to both questions. But if you only give me an answer to  B., I will be very grateful too. PS This is a generalization of this question .",,"['probability', 'gambling']"
99,Greasy (not greedy) Traveling Salesman Problem,Greasy (not greedy) Traveling Salesman Problem,,"This is a variation upon the Traveling Salesman Problem. We've got a greasy salesman, Harold Hill, who is going to try to con towns into buying marching band equipment. To simplify things, the n towns are represented as $K_{n}$ complete graph with the distance between each town being the same. We define a step to be the following. For each step, Hill travels to another town that he has not visited before. Each town that he has visited has another salesman come from it, going to a different random town and spreading the word that Hill is a con man. Hill will never go to a town that knows he is a con, and he will never go to a town that another salesman is going to at the same time. What is the expected number of towns that Hill will be able to visit? For $n=2$, call the towns A and B. At $t=0$, Hill cons town A. At $t=1$, he travels to town $B$, conning them. Another salesman starts at town A. E[towns] = 2 For $n=3$ with towns A, B, and C. At $t=0$, Hill cons town A. At $t=1$, he travels WLOG to town $B$, conning them. Another salesman starts at town A. At $t=2$, Hill can only head to town C. However, the salesman at town A has a $\frac{1}{2}$ probability of going to town C, which would prevent Hill from going. Thus, E[towns] = 2.5. For $n=4$ with towns A, B, C, and D. At $t=0$, Hill cons town A. At $t=1$, he travels WLOG to town $B$, conning them. Another salesman starts at town A. At $t=2$, Hill can head to town C or D. If the salesman goes to one of these, he goes to the other, and at that point has no more towns to go to. Probability is (2/3) and E[towns] = 3. If the other salesman goes to town B, we say Hill goes to town C. Furthermore, as said, town B, having just been scammed by Hill, has another salesman come from it. Thus, we now have Hill at C and two salesmen at town B. Hill can only go to town D, but there is a 5/9 chance that at least one of the other two salesmen also head there, preventing him from going. Probability of Hill getting to 3 towns is (1/3)*(5/9) and probability of 4 towns is (1/3)(4/9). Thus, E[towns] = $(2/3)*3 + (5/27)*3+(4/27)*4 = 85/27 = 3.148\dots$. The specific question that I have is to calculate E[towns] when $n = 10$. I've got a simulation running to get a decent estimation, but I would like to know if there is a better way to calculate this.","This is a variation upon the Traveling Salesman Problem. We've got a greasy salesman, Harold Hill, who is going to try to con towns into buying marching band equipment. To simplify things, the n towns are represented as $K_{n}$ complete graph with the distance between each town being the same. We define a step to be the following. For each step, Hill travels to another town that he has not visited before. Each town that he has visited has another salesman come from it, going to a different random town and spreading the word that Hill is a con man. Hill will never go to a town that knows he is a con, and he will never go to a town that another salesman is going to at the same time. What is the expected number of towns that Hill will be able to visit? For $n=2$, call the towns A and B. At $t=0$, Hill cons town A. At $t=1$, he travels to town $B$, conning them. Another salesman starts at town A. E[towns] = 2 For $n=3$ with towns A, B, and C. At $t=0$, Hill cons town A. At $t=1$, he travels WLOG to town $B$, conning them. Another salesman starts at town A. At $t=2$, Hill can only head to town C. However, the salesman at town A has a $\frac{1}{2}$ probability of going to town C, which would prevent Hill from going. Thus, E[towns] = 2.5. For $n=4$ with towns A, B, C, and D. At $t=0$, Hill cons town A. At $t=1$, he travels WLOG to town $B$, conning them. Another salesman starts at town A. At $t=2$, Hill can head to town C or D. If the salesman goes to one of these, he goes to the other, and at that point has no more towns to go to. Probability is (2/3) and E[towns] = 3. If the other salesman goes to town B, we say Hill goes to town C. Furthermore, as said, town B, having just been scammed by Hill, has another salesman come from it. Thus, we now have Hill at C and two salesmen at town B. Hill can only go to town D, but there is a 5/9 chance that at least one of the other two salesmen also head there, preventing him from going. Probability of Hill getting to 3 towns is (1/3)*(5/9) and probability of 4 towns is (1/3)(4/9). Thus, E[towns] = $(2/3)*3 + (5/27)*3+(4/27)*4 = 85/27 = 3.148\dots$. The specific question that I have is to calculate E[towns] when $n = 10$. I've got a simulation running to get a decent estimation, but I would like to know if there is a better way to calculate this.",,"['probability', 'graph-theory', 'recreational-mathematics']"
