,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Finding a cubic where tangent line at one point is normal at another intersection,Finding a cubic where tangent line at one point is normal at another intersection,,"I have a cubic polynomial in terms of an unknown value $a>0$ that allows the tangent line at $x=1$ to also be the normal to the curve at $x=6$ . The equation is given as $$g(x)=ax^2(x-8)$$ I have found the derivative of the equation to be $g'(x)=3ax^2-16a$ , and the equation of the tangent at $(1,-7a)$ is $y=6a-13ax$ . This tangent line intersects $g(x)$ at $(1,-7a)$ and $(6,-72a)$ . I have tried equating the gradient of the tangent ( $-13a$ ) with the gradient of the normal ( $\frac{-1}{12a}$ ) which only results in $a$ being equal to $\frac{1}{156}$ , which is way too small. I have also tried finding equations for both the tangent and normal using the point-gradient formula: $y+y_1=m(x-x_1)$ . For the tangent at $(1,-7a)$ with gradient $-13a$ , $y=6a-3ax$ ; and the normal at $(6,-72a)$ with gradient $12a$ : $y=\frac{-x+6}{12a}-72a$ . Equating them at point $x=6$ gives $a=0$ . I have been stuck on this problem for a while now. Everything I try seems to yield the wrong answer. Thanks for any help.","I have a cubic polynomial in terms of an unknown value that allows the tangent line at to also be the normal to the curve at . The equation is given as I have found the derivative of the equation to be , and the equation of the tangent at is . This tangent line intersects at and . I have tried equating the gradient of the tangent ( ) with the gradient of the normal ( ) which only results in being equal to , which is way too small. I have also tried finding equations for both the tangent and normal using the point-gradient formula: . For the tangent at with gradient , ; and the normal at with gradient : . Equating them at point gives . I have been stuck on this problem for a while now. Everything I try seems to yield the wrong answer. Thanks for any help.","a>0 x=1 x=6 g(x)=ax^2(x-8) g'(x)=3ax^2-16a (1,-7a) y=6a-13ax g(x) (1,-7a) (6,-72a) -13a \frac{-1}{12a} a \frac{1}{156} y+y_1=m(x-x_1) (1,-7a) -13a y=6a-3ax (6,-72a) 12a y=\frac{-x+6}{12a}-72a x=6 a=0",['calculus']
1,proof of $d(xy) = x(dy) + y(dx)$,proof of,d(xy) = x(dy) + y(dx),"I was trying to prove $$d(xy) = x(dy) + y(dx)$$ earlier this morning and I used this post to help me understand the task. I understood the entirety of the post for my calculus class, apart from one step. Considering an area of a rectangle with dimensions $x$ and $y$ $xy$ makes sense. Likewise, the area of a rectangle with dimensions $(x+\Delta x)(y+\Delta y)$ giving $$A_1=(x+\Delta x)(y+\Delta y)=xy+x \Delta y+y\Delta x+\Delta x \Delta y$$ made sense too. I was confused however about the subtraction of the two areas that gives $$x \Delta y+y\Delta x+\Delta x \Delta y$$ but the small approximation of $\Delta x $ and $\Delta y$ very small, ensuring that $\Delta x \Delta y$ is negligible afterwards made sense. Why is there the need to subtract the areas as part of the proof for this differentiation property?","I was trying to prove earlier this morning and I used this post to help me understand the task. I understood the entirety of the post for my calculus class, apart from one step. Considering an area of a rectangle with dimensions and makes sense. Likewise, the area of a rectangle with dimensions giving made sense too. I was confused however about the subtraction of the two areas that gives but the small approximation of and very small, ensuring that is negligible afterwards made sense. Why is there the need to subtract the areas as part of the proof for this differentiation property?",d(xy) = x(dy) + y(dx) x y xy (x+\Delta x)(y+\Delta y) A_1=(x+\Delta x)(y+\Delta y)=xy+x \Delta y+y\Delta x+\Delta x \Delta y x \Delta y+y\Delta x+\Delta x \Delta y \Delta x  \Delta y \Delta x \Delta y,['derivatives']
2,"If $\frac{dy}{dx} = -0.02y^2+0.2y$ and $y(0)>0$, what is $\lim_{x\to\infty}y(x)$?","If  and , what is ?",\frac{dy}{dx} = -0.02y^2+0.2y y(0)>0 \lim_{x\to\infty}y(x),"If $\frac{dy}{dx} = -0.02y^2+0.2y$ and $y(0)>0$ , what is $\lim_{x\to\infty}y(x)$ ? I thought that the answer was unsolvable since we don't know our value of y(0) but apparently the answer is 10 and I have no clue how to go about this","If and , what is ? I thought that the answer was unsolvable since we don't know our value of y(0) but apparently the answer is 10 and I have no clue how to go about this",\frac{dy}{dx} = -0.02y^2+0.2y y(0)>0 \lim_{x\to\infty}y(x),"['calculus', 'integration', 'algebra-precalculus', 'derivatives']"
3,Where to learn about differential operators?,Where to learn about differential operators?,,"I was reading the wikipedia page on Differential Operators. . It introduces the general concept of a differential operator and then it goes through some examples. It mentions the derivative, the theta operator, the laplacian, the del operator, what is an adjoint of a linear operator, etc.. I'd like to know if there are some books,resources where these topics are treated in a general, related way.","I was reading the wikipedia page on Differential Operators. . It introduces the general concept of a differential operator and then it goes through some examples. It mentions the derivative, the theta operator, the laplacian, the del operator, what is an adjoint of a linear operator, etc.. I'd like to know if there are some books,resources where these topics are treated in a general, related way.",,"['derivatives', 'reference-request']"
4,"If $f(x)=\int_{0}^{x}\sqrt {f(t)}dt$, then find $f(6)$.","If , then find .",f(x)=\int_{0}^{x}\sqrt {f(t)}dt f(6),"Let $f:[0,\infty)\to[0,\infty)$ be continuous on $[0,\infty)$ and differentiable on $(0,\infty)$ . If $f(x)=\int_{0}^{x}\sqrt {f(t)}dt$ , then find $f(6)$ . $$f(x)=\int_{0}^{x}\sqrt {f(t)}dt$$ $$g(x):=\sqrt {f(x)}\implies(g(x))^2=f(x)=\int_{0}^{x}g(t)dt\implies2g(x)g'(x)=g(x)$$ [by FTC-1] $$\implies g=0 \vee g'(x)=\frac{1}{2} $$ $$(g(0))^2=f(0)=0\implies g(x)=\frac{x}{2}\vee g=0 \implies f(6)=9 \vee f(6)=0$$ Is this correct? Also, how do I rule out $f(6)=0$ since my source only gives $9$ as the answer. I found this post after writing mine but I still don't think that $f(6)=0$ can be ruled out.","Let be continuous on and differentiable on . If , then find . [by FTC-1] Is this correct? Also, how do I rule out since my source only gives as the answer. I found this post after writing mine but I still don't think that can be ruled out.","f:[0,\infty)\to[0,\infty) [0,\infty) (0,\infty) f(x)=\int_{0}^{x}\sqrt {f(t)}dt f(6) f(x)=\int_{0}^{x}\sqrt {f(t)}dt g(x):=\sqrt {f(x)}\implies(g(x))^2=f(x)=\int_{0}^{x}g(t)dt\implies2g(x)g'(x)=g(x) \implies g=0 \vee g'(x)=\frac{1}{2}  (g(0))^2=f(0)=0\implies g(x)=\frac{x}{2}\vee g=0 \implies f(6)=9 \vee f(6)=0 f(6)=0 9 f(6)=0","['integration', 'derivatives', 'definite-integrals', 'continuity']"
5,"If $g$ is the inverse of function $f$ and $f'(x)= \frac{1}{1+x^n}$, Find $g'(x)$","If  is the inverse of function  and , Find",g f f'(x)= \frac{1}{1+x^n} g'(x),"I tried the question and got an answer by the following steps: $f(g(x))=x$ Differentiating both sides w.r.t to $x$ , we get $f'(g(x)).g'(x)=1$ And therefore, $g'(x)=1+\left[{g(x)} \right]^n$ Now my question is, are there any alternate ways to approach this problem? Especially ones which may need some Integration?","I tried the question and got an answer by the following steps: Differentiating both sides w.r.t to , we get And therefore, Now my question is, are there any alternate ways to approach this problem? Especially ones which may need some Integration?",f(g(x))=x x f'(g(x)).g'(x)=1 g'(x)=1+\left[{g(x)} \right]^n,"['calculus', 'derivatives', 'inverse', 'inverse-function']"
6,Proof that two definitions of a point of inflection are equivalent,Proof that two definitions of a point of inflection are equivalent,,"I have seen that many online sources (including other math stack exchange questions) say that the following are equivalent definitions of a point of inflection: If $f$ is differentiable on $I$ we say that $f$ has a point of inflection at $a$ if $f'$ has an isolated local extremum at $a$ . We say that $f$ has a point of inflection at $a$ if $f$ changes concavity at $a$ . This definition is somewhat ambiguous to me, so I decided that it would mean that $\exists \delta>0$ such that $f$ is strictly convex on $[a-\delta,a]$ and strictly concave $[a,a+\delta]$ or $f$ is strictly concave on $[a-\delta,a]$ and strictly convex $[a,a+\delta]$ . I want to prove that these definitions are equivalent (I know that this might require clarifying definition 2. in a different way than I have). I was able to prove that 2. implies 1. This is my proof: Suppose that $\exists \delta>0$ such that $f$ is strictly convex on $[a-\delta,a]$ and strictly concave $[a,a+\delta]$ . Then $f'$ is strictly increasing on $[a-\delta,a]$ and $f'$ is strictly decreasing on $[a,a+\delta]$ , so $f'$ has an isolated local maximum at $a$ . The case when $f$ is strictly concave on $[a-\delta,a]$ and strictly convex $[a,a+\delta]$ is similar. Now I want to show that 1. implies 2. Suppose that $f'$ has a isolated relative extremum. Without loss of generality, suppose that $f'$ has a isolated relative minimum. I need to show that $f'$ is strictly increasing on $[a-\delta,a]$ and $f'$ is strictly decreasing on $[a,a+\delta]$ (which is equivalent to showing that $f$ is convex on $[a-\delta,a]$ and concave on $[a,a+\delta]$ ). I know that this is not true for a general function $g$ that if $g$ has an isolated extremum at $g$ , $g$ is strictly monotone on either side of $a$ . (for example, if the g has a jump discontinuity, which I know is not possible for $f'$ ). Why is true for $f'$ ?","I have seen that many online sources (including other math stack exchange questions) say that the following are equivalent definitions of a point of inflection: If is differentiable on we say that has a point of inflection at if has an isolated local extremum at . We say that has a point of inflection at if changes concavity at . This definition is somewhat ambiguous to me, so I decided that it would mean that such that is strictly convex on and strictly concave or is strictly concave on and strictly convex . I want to prove that these definitions are equivalent (I know that this might require clarifying definition 2. in a different way than I have). I was able to prove that 2. implies 1. This is my proof: Suppose that such that is strictly convex on and strictly concave . Then is strictly increasing on and is strictly decreasing on , so has an isolated local maximum at . The case when is strictly concave on and strictly convex is similar. Now I want to show that 1. implies 2. Suppose that has a isolated relative extremum. Without loss of generality, suppose that has a isolated relative minimum. I need to show that is strictly increasing on and is strictly decreasing on (which is equivalent to showing that is convex on and concave on ). I know that this is not true for a general function that if has an isolated extremum at , is strictly monotone on either side of . (for example, if the g has a jump discontinuity, which I know is not possible for ). Why is true for ?","f I f a f' a f a f a \exists \delta>0 f [a-\delta,a] [a,a+\delta] f [a-\delta,a] [a,a+\delta] \exists \delta>0 f [a-\delta,a] [a,a+\delta] f' [a-\delta,a] f' [a,a+\delta] f' a f [a-\delta,a] [a,a+\delta] f' f' f' [a-\delta,a] f' [a,a+\delta] f [a-\delta,a] [a,a+\delta] g g g g a f' f'","['real-analysis', 'derivatives', 'convex-analysis']"
7,"Derivative of positive, continuously differentiable function is positive close to zero","Derivative of positive, continuously differentiable function is positive close to zero",,"Consider a continuously differentiable function $f: [0, 1] \to \mathbf{R}_{\geq 0}$ , such that $f(0)=0, f'(0)=0$ and $f(x)>0$ for $x>0$ . I want to prove that there exists $\bar{x}>0$ such that $f(x)$ is increasing on $[0, \bar{x}]$ . (or, even better, strictly increasing) This question is nearly identical to this Derivative of positive function is positive close to zero However, apart from continuity, I have additionally continuous derivatives and, because of that, cannot make the counterexample proposed there to work. The answer to that question is (paraphrasing): 'for a function oscillating near 0 between $x^2$ and $x^4$ we have $f(0)=0$ , $f(x)>0$ for $x>0$ and the function is continuous'. However, I cannot construct such a function that would not violate continuity of the derivative. For example, consider: $$f(x)= (\sin(1/x)+1)x^2+(\cos(1/x)+1)x^4.$$ The derivative of this function oscillates between $0$ and $1$ and thus is not continuous (whilst all the other conditions are met). In short: is the continuity of the derivative sufficient to make the original statement  true, or does there exist a counterexample with continuous derivatives?","Consider a continuously differentiable function , such that and for . I want to prove that there exists such that is increasing on . (or, even better, strictly increasing) This question is nearly identical to this Derivative of positive function is positive close to zero However, apart from continuity, I have additionally continuous derivatives and, because of that, cannot make the counterexample proposed there to work. The answer to that question is (paraphrasing): 'for a function oscillating near 0 between and we have , for and the function is continuous'. However, I cannot construct such a function that would not violate continuity of the derivative. For example, consider: The derivative of this function oscillates between and and thus is not continuous (whilst all the other conditions are met). In short: is the continuity of the derivative sufficient to make the original statement  true, or does there exist a counterexample with continuous derivatives?","f: [0, 1] \to \mathbf{R}_{\geq 0} f(0)=0, f'(0)=0 f(x)>0 x>0 \bar{x}>0 f(x) [0, \bar{x}] x^2 x^4 f(0)=0 f(x)>0 x>0 f(x)= (\sin(1/x)+1)x^2+(\cos(1/x)+1)x^4. 0 1","['calculus', 'real-analysis', 'derivatives']"
8,"If $f:\mathbb R^2\to \mathbb R$ is defined by $f(x,y)=x^2+xy$. What is the meaning of the derivative/linear transformation, at the point $(1,2)$?","If  is defined by . What is the meaning of the derivative/linear transformation, at the point ?","f:\mathbb R^2\to \mathbb R f(x,y)=x^2+xy (1,2)","Suppose $f:\mathbb R^2\to \mathbb R$ is defined by $f(x,y)=x^2+xy$. The gradient is $(2x+y, x)$ and so the Jacobian is $\begin{bmatrix} 2x+y& x\end{bmatrix}$. At the point $(1,2)$, the derivative is the linear transformation $\begin{bmatrix} 4& 1\end{bmatrix}$, which is the matrix of the transformation $T(x,y)=4x+y$. For the sake of an example, the directional derivative in the direction of $(1,1)$ is $\begin{bmatrix} 4& 1\end{bmatrix}\begin{bmatrix} 1/\sqrt{2}\\ 1/\sqrt{2}\end{bmatrix}=5/\sqrt{2}$. Which I understand is the slope of the tangent plane in the direction of $(1,1)$. However, I am not quite understanding what the linear transformation $T(x,y)=4x+y$ represented by the matrix $\begin{bmatrix} 4& 1\end{bmatrix}$ is supposed to represent. The tangent plane lives tangent to the graph of $f$ at the point $f(1,2)=3$; i.e., at the point $(1,2,3) \in \mathbb R^3$. So, what exactly does the linear transformation $\begin{bmatrix} 4& 1\end{bmatrix}$ represent? What is being transformed?","Suppose $f:\mathbb R^2\to \mathbb R$ is defined by $f(x,y)=x^2+xy$. The gradient is $(2x+y, x)$ and so the Jacobian is $\begin{bmatrix} 2x+y& x\end{bmatrix}$. At the point $(1,2)$, the derivative is the linear transformation $\begin{bmatrix} 4& 1\end{bmatrix}$, which is the matrix of the transformation $T(x,y)=4x+y$. For the sake of an example, the directional derivative in the direction of $(1,1)$ is $\begin{bmatrix} 4& 1\end{bmatrix}\begin{bmatrix} 1/\sqrt{2}\\ 1/\sqrt{2}\end{bmatrix}=5/\sqrt{2}$. Which I understand is the slope of the tangent plane in the direction of $(1,1)$. However, I am not quite understanding what the linear transformation $T(x,y)=4x+y$ represented by the matrix $\begin{bmatrix} 4& 1\end{bmatrix}$ is supposed to represent. The tangent plane lives tangent to the graph of $f$ at the point $f(1,2)=3$; i.e., at the point $(1,2,3) \in \mathbb R^3$. So, what exactly does the linear transformation $\begin{bmatrix} 4& 1\end{bmatrix}$ represent? What is being transformed?",,"['calculus', 'linear-algebra', 'geometry', 'derivatives', 'linear-transformations']"
9,Radius of curvature of $r=2 \cdot e^{3 \phi}$,Radius of curvature of,r=2 \cdot e^{3 \phi},I have to find the curvature of $r=2 \cdot e^{3 \phi}$ $\dfrac{\left[1+\left(\dfrac{d y}{d x}\right)^2\right]^{\frac 3 2}}{\dfrac{d^2 y}{d x^2}}$ $x=r\cdot cos(\phi)=2 \cdot e^{3 \phi}\cdot cos(\phi)$ $y=r\cdot sin(\phi)=2 \cdot e^{3 \phi}\cdot sin(\phi)$ $\dfrac{d x}{d \phi}=-2 e^{3 \phi}\cdot (sin(\phi) - 3 cos(\phi))$ $\dfrac{d y}{d \phi}=2 e^{3 \phi}\cdot (3 sin(\phi) + cos(\phi))$ $\dfrac{d y}{d x}=\dfrac{2 e^{3 \phi}\cdot (3 sin(\phi) + cos(\phi))}{-2 e^{3 \phi}\cdot (sin(\phi) - 3 cos(\phi))} =\dfrac{-3 sin(\phi) - cos(\phi)}{sin(\phi) - 3 cos(\phi)}$ $\dfrac{d^2 y}{d \phi^2}=4 e^{3 \phi}\cdot (4 sin(\phi) + 3 cos(\phi))$ $\dfrac{d^2 y}{d ^2 x}=\dfrac{\dfrac{2 e^{3 \phi}\cdot (4 sin(\phi) - 3 cos(\phi))}{50}}{\dfrac{2 e^{3 \phi}\cdot (sin(\phi) + 3 cos(\phi))}{10}}=\dfrac{4 sin(\phi) - 3 cos(\phi)}{5\cdot (sin(\phi) + 3 cos(\phi))}$ now using the radius of curvature formula i find an incredibly complex formula: $\rho=\dfrac{\left[1+\left(\dfrac{d y}{d x}\right)^2\right]^{\frac 3 2}}{\dfrac{d^2 y}{d x^2}}=\dfrac{\left[1+\left(\dfrac{3 sin(\phi) - cos(\phi)}{sin(\phi) + 3 cos(\phi)}\right)^2\right]^{\frac 3 2}}{\dfrac{4 sin(\phi) - 3 cos(\phi)}{5\cdot (sin(\phi) + 3 cos(\phi))}}$ I can't find a way to write this in a more human way.,I have to find the curvature of $r=2 \cdot e^{3 \phi}$ $\dfrac{\left[1+\left(\dfrac{d y}{d x}\right)^2\right]^{\frac 3 2}}{\dfrac{d^2 y}{d x^2}}$ $x=r\cdot cos(\phi)=2 \cdot e^{3 \phi}\cdot cos(\phi)$ $y=r\cdot sin(\phi)=2 \cdot e^{3 \phi}\cdot sin(\phi)$ $\dfrac{d x}{d \phi}=-2 e^{3 \phi}\cdot (sin(\phi) - 3 cos(\phi))$ $\dfrac{d y}{d \phi}=2 e^{3 \phi}\cdot (3 sin(\phi) + cos(\phi))$ $\dfrac{d y}{d x}=\dfrac{2 e^{3 \phi}\cdot (3 sin(\phi) + cos(\phi))}{-2 e^{3 \phi}\cdot (sin(\phi) - 3 cos(\phi))} =\dfrac{-3 sin(\phi) - cos(\phi)}{sin(\phi) - 3 cos(\phi)}$ $\dfrac{d^2 y}{d \phi^2}=4 e^{3 \phi}\cdot (4 sin(\phi) + 3 cos(\phi))$ $\dfrac{d^2 y}{d ^2 x}=\dfrac{\dfrac{2 e^{3 \phi}\cdot (4 sin(\phi) - 3 cos(\phi))}{50}}{\dfrac{2 e^{3 \phi}\cdot (sin(\phi) + 3 cos(\phi))}{10}}=\dfrac{4 sin(\phi) - 3 cos(\phi)}{5\cdot (sin(\phi) + 3 cos(\phi))}$ now using the radius of curvature formula i find an incredibly complex formula: $\rho=\dfrac{\left[1+\left(\dfrac{d y}{d x}\right)^2\right]^{\frac 3 2}}{\dfrac{d^2 y}{d x^2}}=\dfrac{\left[1+\left(\dfrac{3 sin(\phi) - cos(\phi)}{sin(\phi) + 3 cos(\phi)}\right)^2\right]^{\frac 3 2}}{\dfrac{4 sin(\phi) - 3 cos(\phi)}{5\cdot (sin(\phi) + 3 cos(\phi))}}$ I can't find a way to write this in a more human way.,,"['calculus', 'derivatives']"
10,Let $\lambda_{i}(A) = i$-th eigenvalue of $A$. Show that $A \mapsto \lambda_{i}(A)$ is differentiable in $\mathcal{M}_{2 \times 2}(\mathbb{R})$,Let -th eigenvalue of . Show that  is differentiable in,\lambda_{i}(A) = i A A \mapsto \lambda_{i}(A) \mathcal{M}_{2 \times 2}(\mathbb{R}),"Let $\mathcal{L}_{\text{sim}}(\mathbb{R}) \subset \mathcal{M}_{2 \times 2}(\mathbb{R})$ the subspace of symmetric matrix of order $2$. Consider the aplication   $$\lambda_{i}: \mathcal{L}_{\text{sim}}(\mathbb{R}) \to \mathbb{R},\quad i=1,2,\;\; \lambda_{i}(A) = i\text{-th eigenvalue of }A.$$   Show that $A \mapsto \lambda_{i}(A)$ is differentiable in $\mathcal{M}_{2 \times 2}(\mathbb{R})$ and to calculate $\lambda'_{i}(A)$, for every $A \in \mathcal{L}_{\text{sim}}(\mathbb{R})$. I couldn't develop much, but I started thinking about dividing into two cases: $A$ is invertible and $A$ is not invertible. I can to use $\operatorname{tr}(A) = \lambda_{1} + \lambda_{2}$ and $\det(A) = \lambda_{1}\lambda_{2}$. I tried to write $\lambda_{i}(A + H) = \lambda_{i}(A) + L(H) + o(H)$ where $L(H)$ is a linear function and $\displaystyle \frac{o(H)}{||H||} \to 0$. My problem is: I couldn't to expand $\lambda_{i}(A + H)$ so that it was viable. There are probably some properties that should be used that I cannot remember. Can anybody help me?","Let $\mathcal{L}_{\text{sim}}(\mathbb{R}) \subset \mathcal{M}_{2 \times 2}(\mathbb{R})$ the subspace of symmetric matrix of order $2$. Consider the aplication   $$\lambda_{i}: \mathcal{L}_{\text{sim}}(\mathbb{R}) \to \mathbb{R},\quad i=1,2,\;\; \lambda_{i}(A) = i\text{-th eigenvalue of }A.$$   Show that $A \mapsto \lambda_{i}(A)$ is differentiable in $\mathcal{M}_{2 \times 2}(\mathbb{R})$ and to calculate $\lambda'_{i}(A)$, for every $A \in \mathcal{L}_{\text{sim}}(\mathbb{R})$. I couldn't develop much, but I started thinking about dividing into two cases: $A$ is invertible and $A$ is not invertible. I can to use $\operatorname{tr}(A) = \lambda_{1} + \lambda_{2}$ and $\det(A) = \lambda_{1}\lambda_{2}$. I tried to write $\lambda_{i}(A + H) = \lambda_{i}(A) + L(H) + o(H)$ where $L(H)$ is a linear function and $\displaystyle \frac{o(H)}{||H||} \to 0$. My problem is: I couldn't to expand $\lambda_{i}(A + H)$ so that it was viable. There are probably some properties that should be used that I cannot remember. Can anybody help me?",,"['real-analysis', 'derivatives']"
11,Maximizing by setting Derivative equal to 0. Stuck (struggling),Maximizing by setting Derivative equal to 0. Stuck (struggling),,"I'm having immense difficulty doing this question. Want to maximize (respect to x): $f(x)$=$(x-x_0)(x-x_1)(x-x_2)(x-x_3)$ where $x_1-x_0=h$ $x_2-x_1=h$ $x_3-x_2=h$. Want to get the value of x in terms of $x_1$ and $h$. First I take the derivative and get: $(x-x_0)(x-x_1)(x-x_2)$ +$(x-x_0)(x-x_1)(x-x_3)+(x-x_0)(x-x_2)(x-x_3)+ (x-x_1)(x-x_2)(x-x_3)=0$ Now using the facts 1,2, and 3, I substitute for $x_0, x_2$, and $x_3 $:(Need to write everything in terms of x1 and h) $x_0$=$x_1-h$, $x_1=x_1$, $x_2=x_1+h$, $x3=h+x2=h+(x_1)+h=2h+x_1$ Then we have for the derivative: $(x-x_1+h)(x-x_1)(x-(x_1+h))$ + $(x-x_1+h)(x-x1)(x-(2h+x_1))$+ $(x-x_1+h)(x-x_1-h)(x-(2h+x_1))$+$(x-x_1)(x-x_1-h)(x-(2h+x_1))$=0. Then we have: $[(x-x_1)^2+h(x-x_1)]((x-(x_1+h))$+$[(x-x_1)^2+h(x-x_1)]((x-(2h+x_1))$+ $[(x-x_1)^2-h^2](x-(2h+x_1))$+$[(x-x_1)^2-h(x-x_1)]((x-(2h+x_1))$=0 This is all I have but I can't solve for x still.  Wolfram Alpha won't give me answer.  Any help would be much appreciated thank you.  I believe the max value of f(x) should be (like max y value): $f(x)=9/16$. or $(f(x)=72/128)$ Thank you.  I have been working on this problem for over three hours so any help would really be appreciated. Here is the actual question I'm working on for reference: 5 Part c.  Where Theorem 1 is given to be:","I'm having immense difficulty doing this question. Want to maximize (respect to x): $f(x)$=$(x-x_0)(x-x_1)(x-x_2)(x-x_3)$ where $x_1-x_0=h$ $x_2-x_1=h$ $x_3-x_2=h$. Want to get the value of x in terms of $x_1$ and $h$. First I take the derivative and get: $(x-x_0)(x-x_1)(x-x_2)$ +$(x-x_0)(x-x_1)(x-x_3)+(x-x_0)(x-x_2)(x-x_3)+ (x-x_1)(x-x_2)(x-x_3)=0$ Now using the facts 1,2, and 3, I substitute for $x_0, x_2$, and $x_3 $:(Need to write everything in terms of x1 and h) $x_0$=$x_1-h$, $x_1=x_1$, $x_2=x_1+h$, $x3=h+x2=h+(x_1)+h=2h+x_1$ Then we have for the derivative: $(x-x_1+h)(x-x_1)(x-(x_1+h))$ + $(x-x_1+h)(x-x1)(x-(2h+x_1))$+ $(x-x_1+h)(x-x_1-h)(x-(2h+x_1))$+$(x-x_1)(x-x_1-h)(x-(2h+x_1))$=0. Then we have: $[(x-x_1)^2+h(x-x_1)]((x-(x_1+h))$+$[(x-x_1)^2+h(x-x_1)]((x-(2h+x_1))$+ $[(x-x_1)^2-h^2](x-(2h+x_1))$+$[(x-x_1)^2-h(x-x_1)]((x-(2h+x_1))$=0 This is all I have but I can't solve for x still.  Wolfram Alpha won't give me answer.  Any help would be much appreciated thank you.  I believe the max value of f(x) should be (like max y value): $f(x)=9/16$. or $(f(x)=72/128)$ Thank you.  I have been working on this problem for over three hours so any help would really be appreciated. Here is the actual question I'm working on for reference: 5 Part c.  Where Theorem 1 is given to be:",,"['calculus', 'sequences-and-series']"
12,Example of a differentiable $S(x)=\sum f_n(x)$ such that $S'(x)\ne\sum f_n'(x)$ for (at least almost) all x,Example of a differentiable  such that  for (at least almost) all x,S(x)=\sum f_n(x) S'(x)\ne\sum f_n'(x),"Let $S(x)=\sum f_n(x)$ be pointwise convergent and differentiable on some $A\subseteq\Bbb R$, with all the $f_n$ differentiable on $A$. In the comments to this answer a user asked for an $S$ such that $\sum f_n'$ is pointwise convergent but not identical to $S'$. Out of curiosity I gave it a try. For such an $S$, I thought it's convenient to start from $T=\sum f_n'$, assuming the $f_n'$ are integrable and $T$ does not converge uniformly, in order to integrate the $f_n'$ on a convenient interval and take the sum of these resulting functions as $S$. Soon enough I came up with $$T(x)=\sum_{n=1}^\infty f_n'(x)=\sum_{n=1}^\infty-x(1-x)^n=\begin{cases}x-1 &x\in(0,1] \\ 0&x=0.\end{cases} $$which suggests$$S(x)=\sum_{n=1}^\infty\frac{(1-x)^{n+1}(nx+x+1)}{(n+1)(n+2)}.$$ On $[0,1]$ one has $S(x)=\frac12(x-1)^2$ and thus $S'(x)=x-1$. In particular $S'(0)=-1\ne T(0),$ so $S'\ne T$. I briefly tried to find a much more pathological case, but it wasn't apparent to me. Question: Defining $S$ and $T$ as in the first paragraph, what is a specific $S$ such that $S'\ne T$ on the whole $A$, except maybe for countably many points?","Let $S(x)=\sum f_n(x)$ be pointwise convergent and differentiable on some $A\subseteq\Bbb R$, with all the $f_n$ differentiable on $A$. In the comments to this answer a user asked for an $S$ such that $\sum f_n'$ is pointwise convergent but not identical to $S'$. Out of curiosity I gave it a try. For such an $S$, I thought it's convenient to start from $T=\sum f_n'$, assuming the $f_n'$ are integrable and $T$ does not converge uniformly, in order to integrate the $f_n'$ on a convenient interval and take the sum of these resulting functions as $S$. Soon enough I came up with $$T(x)=\sum_{n=1}^\infty f_n'(x)=\sum_{n=1}^\infty-x(1-x)^n=\begin{cases}x-1 &x\in(0,1] \\ 0&x=0.\end{cases} $$which suggests$$S(x)=\sum_{n=1}^\infty\frac{(1-x)^{n+1}(nx+x+1)}{(n+1)(n+2)}.$$ On $[0,1]$ one has $S(x)=\frac12(x-1)^2$ and thus $S'(x)=x-1$. In particular $S'(0)=-1\ne T(0),$ so $S'\ne T$. I briefly tried to find a much more pathological case, but it wasn't apparent to me. Question: Defining $S$ and $T$ as in the first paragraph, what is a specific $S$ such that $S'\ne T$ on the whole $A$, except maybe for countably many points?",,"['real-analysis', 'sequences-and-series', 'derivatives', 'examples-counterexamples', 'uniform-convergence']"
13,How to find an inflection point (maximum slope) of a log-log plot?,How to find an inflection point (maximum slope) of a log-log plot?,,"I have a function $f(x)$ and I have plotted it on a log-log plot. I would like to find the location of the inflection point of the function. ( note: that means the inflection point in ""log-log space"" which is not the same as the inflection point of $f(x)$ itself. ) I know the following is true: $\frac {d \log y}{d \log x} = \frac{x}{y} \frac{dy}{dx}$. What is the correct formula to find the local maximum of $\frac {d \log y}{d \log x}$?","I have a function $f(x)$ and I have plotted it on a log-log plot. I would like to find the location of the inflection point of the function. ( note: that means the inflection point in ""log-log space"" which is not the same as the inflection point of $f(x)$ itself. ) I know the following is true: $\frac {d \log y}{d \log x} = \frac{x}{y} \frac{dy}{dx}$. What is the correct formula to find the local maximum of $\frac {d \log y}{d \log x}$?",,"['derivatives', 'logarithms']"
14,Question about identities regarding second covariant derivatives,Question about identities regarding second covariant derivatives,,"For a vector field $X$ and a covariant derivative(Levi-Civita connection for a given metric, for simplicity) $\nabla$ let us suppose that $\nabla_a X_b+\nabla_b X_a=0$ holds. Then it is said that $\nabla_c\nabla_a X_b+\nabla_c\nabla_b X_a=0$ holds trivially. But I cannot understand how this second derivative identity holds.... Isn't $\nabla_c\nabla_bX_a=(\nabla_c\nabla_bX)_a$? Then from by calculations regarding Christoffel symbols,  \begin{align} \partial_c(\nabla_aX_b)=\nabla_c\nabla_aX_b+\Gamma^f_{cb}\nabla_aX_f\\\partial_c(\nabla_bX_a)=\nabla_c\nabla_bX_a+\Gamma^f_{ca}\nabla_bX_f\\ \end{align} Since $\partial_c(\nabla_aX_b)=-\partial_c(\nabla_bX_a)$ we can equate the above two equations by - sign. However, $\Gamma^f_{cb}\nabla_aX_f+\Gamma^f_{ca}\nabla_bX_f$ simply doesn't vanish as should be....As a result, $\nabla_c\nabla_aX_b=\nabla_c\nabla_bX_a$ cannot be obtained.... It seems that I am terribly misunderstanding something and it is extremely frustrating.... Could anyone please help me?","For a vector field $X$ and a covariant derivative(Levi-Civita connection for a given metric, for simplicity) $\nabla$ let us suppose that $\nabla_a X_b+\nabla_b X_a=0$ holds. Then it is said that $\nabla_c\nabla_a X_b+\nabla_c\nabla_b X_a=0$ holds trivially. But I cannot understand how this second derivative identity holds.... Isn't $\nabla_c\nabla_bX_a=(\nabla_c\nabla_bX)_a$? Then from by calculations regarding Christoffel symbols,  \begin{align} \partial_c(\nabla_aX_b)=\nabla_c\nabla_aX_b+\Gamma^f_{cb}\nabla_aX_f\\\partial_c(\nabla_bX_a)=\nabla_c\nabla_bX_a+\Gamma^f_{ca}\nabla_bX_f\\ \end{align} Since $\partial_c(\nabla_aX_b)=-\partial_c(\nabla_bX_a)$ we can equate the above two equations by - sign. However, $\Gamma^f_{cb}\nabla_aX_f+\Gamma^f_{ca}\nabla_bX_f$ simply doesn't vanish as should be....As a result, $\nabla_c\nabla_aX_b=\nabla_c\nabla_bX_a$ cannot be obtained.... It seems that I am terribly misunderstanding something and it is extremely frustrating.... Could anyone please help me?",,"['derivatives', 'differential-geometry']"
15,Dini Derivatives,Dini Derivatives,,"Define $f:\mathbb R\rightarrow \mathbb R $ by $$ f(x)= \begin{cases} 0&,x\in \mathbb R \setminus \mathbb Q\\1&,x\in \mathbb Q \end{cases} $$ Let $x\in \mathbb R \setminus \mathbb Q$. I was asked to calculate Dini derivatives of $f$ at $x$, i.e. $(D^+f)(x), (D_+f)(x), (D^-f)(x),$ and $(D_-f)(x)$.  I obtained $(D_+f)(x)=0$ (which I'm not entirely sure is right) so far, and I can't seem to proceed further. I can kind of guess that $(D^+f)(x)=∞ $ but I don't know how to show my working to obtain this.  I think if I know how to compute the first two derivatives, the rest would be very much similar.  Can someone please help?  Thank you.","Define $f:\mathbb R\rightarrow \mathbb R $ by $$ f(x)= \begin{cases} 0&,x\in \mathbb R \setminus \mathbb Q\\1&,x\in \mathbb Q \end{cases} $$ Let $x\in \mathbb R \setminus \mathbb Q$. I was asked to calculate Dini derivatives of $f$ at $x$, i.e. $(D^+f)(x), (D_+f)(x), (D^-f)(x),$ and $(D_-f)(x)$.  I obtained $(D_+f)(x)=0$ (which I'm not entirely sure is right) so far, and I can't seem to proceed further. I can kind of guess that $(D^+f)(x)=∞ $ but I don't know how to show my working to obtain this.  I think if I know how to compute the first two derivatives, the rest would be very much similar.  Can someone please help?  Thank you.",,"['derivatives', 'limsup-and-liminf']"
16,Cauchy Remainder = Lagrange Remainder,Cauchy Remainder = Lagrange Remainder,,I was given a problem like this: Suppose $f$ has derivatives of all orders and all continuous. Suppose the sequence $\{a_n\}$ is bounded and $|f^{(n)}(x)| \le a_n$ for all $x$. Use the Cauchy Remainder Formula to show that $f$ equals its Taylor Expansion. So far I have been covered Lagrange Remainder and Cauchy Remainder Theorem. My intuition is showing the remainder part $R_n(x)$ of Cauchy equal $R_n(x)$ from Lagrange. But I don't know how to apply the info that $|f^{(n)}(x)| \le a_n$ for all $x$. Can someone please help with this? Thanks,I was given a problem like this: Suppose $f$ has derivatives of all orders and all continuous. Suppose the sequence $\{a_n\}$ is bounded and $|f^{(n)}(x)| \le a_n$ for all $x$. Use the Cauchy Remainder Formula to show that $f$ equals its Taylor Expansion. So far I have been covered Lagrange Remainder and Cauchy Remainder Theorem. My intuition is showing the remainder part $R_n(x)$ of Cauchy equal $R_n(x)$ from Lagrange. But I don't know how to apply the info that $|f^{(n)}(x)| \le a_n$ for all $x$. Can someone please help with this? Thanks,,"['calculus', 'derivatives']"
17,Real Analysis. Little hint for a question.,Real Analysis. Little hint for a question.,,"A question from my Analysis list that I could not have any idea. Any help would be great. I don't want a complete solution, just a little hint , because I need to do at least one part alone. Let $U = \lbrace x \in \mathbb{R}^{m}\,|\, |x_{i}| < 1, i =1,...,m\rbrace$ and $f: U \longrightarrow \mathbb{R}$ a function differentiable, with $\displaystyle \Bigg| \frac{\partial f}{\partial x_{i}}(x)\Bigg| \leq 3$ for all $x \in U$. Then $f(U)$ is an interval of length $\leq 3m$.","A question from my Analysis list that I could not have any idea. Any help would be great. I don't want a complete solution, just a little hint , because I need to do at least one part alone. Let $U = \lbrace x \in \mathbb{R}^{m}\,|\, |x_{i}| < 1, i =1,...,m\rbrace$ and $f: U \longrightarrow \mathbb{R}$ a function differentiable, with $\displaystyle \Bigg| \frac{\partial f}{\partial x_{i}}(x)\Bigg| \leq 3$ for all $x \in U$. Then $f(U)$ is an interval of length $\leq 3m$.",,"['real-analysis', 'derivatives']"
18,"$f$ has no interior extreme point, therefore, $f$ is strictly increasing or decreasing function","has no interior extreme point, therefore,  is strictly increasing or decreasing function",f f,"I would like some input in my proof for this question. I solved using two different approaches, and would like to know where can I improve. I'm specially interested in a proof that does not use the fact that a strictly increasing function has $f'(x) \gt 0, \forall x \in I$. Let $a \lt b, [a,b] \in \mathbb{R}$ and $f:[a,b] \to \mathbb{R}$ be continuous in $[a,b]$. Suppose that there are no interior points that are local extreme. Show that $f$ is strictly increasing or decreasing. $\textbf{Answer 1}$: Suppose by contradiction that $f$ is not strictly increasing. Let $x,y,z \in [a,b]$ and assume that $x \lt y \lt z$. Therefore, $f(x) \lt f(y) \gt f(z)$. Consider the closed interval $[x,z]$. Since $f(y)$ is greather than both $f(x)$ and $f(z)$, neither $f(x)$ or $f(z)$ can be the maximum value of $f$ over $[x,z]$. However, since the interval is compact, $f$ attains its maximum over this interval. Therefore, $\exists t \in [x,z]: f(t) \gt f(w), \forall w \in [x,z]$. Since $t$ is an interior point of $[x,z]$, it is an interior point of $[a,b]$, hence it is a local maximum of $f$ in $[a,b]$, which is a contradiction. The proof is analogous for the case where $f$ is strictly decreasing. $\textbf{Answer 2}$: Since $\nexists c \in (a,b):f'(c)=0$, $f'(c)\gt0$ or $f'(c) \lt0, \forall c \in (a,b)$ . Without loss of generality, assume that $f'(c) \gt 0, \forall c \in (a,b)$. Consider the closed interval $[x_{1},x_{2}] \subseteq (a,b)$, with $x_{1} \lt x_{2}$.By the Mean Value Theorem, $\exists d \in (x_{1},x_{2}):f'(d) = \frac{f(x_{2})-f(x_{1})}{x_{2}-x_{1}}$. Since $f'(d) \gt 0 \implies f(x_{2}) \gt f(x_{1})$, therefore, $f$ is strictly increasing.","I would like some input in my proof for this question. I solved using two different approaches, and would like to know where can I improve. I'm specially interested in a proof that does not use the fact that a strictly increasing function has $f'(x) \gt 0, \forall x \in I$. Let $a \lt b, [a,b] \in \mathbb{R}$ and $f:[a,b] \to \mathbb{R}$ be continuous in $[a,b]$. Suppose that there are no interior points that are local extreme. Show that $f$ is strictly increasing or decreasing. $\textbf{Answer 1}$: Suppose by contradiction that $f$ is not strictly increasing. Let $x,y,z \in [a,b]$ and assume that $x \lt y \lt z$. Therefore, $f(x) \lt f(y) \gt f(z)$. Consider the closed interval $[x,z]$. Since $f(y)$ is greather than both $f(x)$ and $f(z)$, neither $f(x)$ or $f(z)$ can be the maximum value of $f$ over $[x,z]$. However, since the interval is compact, $f$ attains its maximum over this interval. Therefore, $\exists t \in [x,z]: f(t) \gt f(w), \forall w \in [x,z]$. Since $t$ is an interior point of $[x,z]$, it is an interior point of $[a,b]$, hence it is a local maximum of $f$ in $[a,b]$, which is a contradiction. The proof is analogous for the case where $f$ is strictly decreasing. $\textbf{Answer 2}$: Since $\nexists c \in (a,b):f'(c)=0$, $f'(c)\gt0$ or $f'(c) \lt0, \forall c \in (a,b)$ . Without loss of generality, assume that $f'(c) \gt 0, \forall c \in (a,b)$. Consider the closed interval $[x_{1},x_{2}] \subseteq (a,b)$, with $x_{1} \lt x_{2}$.By the Mean Value Theorem, $\exists d \in (x_{1},x_{2}):f'(d) = \frac{f(x_{2})-f(x_{1})}{x_{2}-x_{1}}$. Since $f'(d) \gt 0 \implies f(x_{2}) \gt f(x_{1})$, therefore, $f$ is strictly increasing.",,"['real-analysis', 'derivatives', 'proof-verification', 'continuity']"
19,Would f ''(a)=0 and f ''(x) does not change sign at x=a result in an inflection point?,Would f ''(a)=0 and f ''(x) does not change sign at x=a result in an inflection point?,,"My note says: "" If f ''(a)=0  and f ''(x) changes sign at x=a, there is a point of inflection at (a, f(a))."" I was wondering how the original graph would appear if f ''(a)=0  and f ''(x) does not change sign at x=a, would there still be a point of inflection?","My note says: "" If f ''(a)=0  and f ''(x) changes sign at x=a, there is a point of inflection at (a, f(a))."" I was wondering how the original graph would appear if f ''(a)=0  and f ''(x) does not change sign at x=a, would there still be a point of inflection?",,"['calculus', 'derivatives']"
20,"How to show that $\Psi: E \rightarrow E$, $\Psi(f) = \sin(f(t))$ is continuous and differentiable?","How to show that ,  is continuous and differentiable?",\Psi: E \rightarrow E \Psi(f) = \sin(f(t)),"Let $E = \mathcal{C}([0,1],\mathbb{R})$ a Banach space of continuous fonctions mapping from $[0,1]$ to \mathbb{R}, with the norm $||f|| = \underset{t \in [0,1]}{\sup}|f(t)|$. Let $\Psi : E \rightarrow E$ defined as $\Psi(f) = \sin(f(t)), \forall f \in E, \forall t \in [0,1] $. 1) Verify that $\Psi$ is continuous 2) Show that $\Psi$ is Frechet differentiable on $E$, and find its differential. I trying to solve this exercise, but I struggle a lot. I think that to show its continuity, you have to show that $\Psi$ is a composite function of two continuous functions. Once you manage to describe $\Psi$ with the composition of two functions, answering the second question might be more easier. The issue I get with the composition, is that my intuition first tells me that $$\Psi(f) = g \circ u $$ where $u:E \rightarrow \mathbb{R}$ such that $u(f) = f(t)$ and $g: \mathbb{R} \rightarrow \mathbb{R}$ such that $g(x)=sin(x)$ Now this is obviously false as $\Psi$ maps to $E$ and not $\mathbb{R}$. Can someone help me to find the two functions that fit?","Let $E = \mathcal{C}([0,1],\mathbb{R})$ a Banach space of continuous fonctions mapping from $[0,1]$ to \mathbb{R}, with the norm $||f|| = \underset{t \in [0,1]}{\sup}|f(t)|$. Let $\Psi : E \rightarrow E$ defined as $\Psi(f) = \sin(f(t)), \forall f \in E, \forall t \in [0,1] $. 1) Verify that $\Psi$ is continuous 2) Show that $\Psi$ is Frechet differentiable on $E$, and find its differential. I trying to solve this exercise, but I struggle a lot. I think that to show its continuity, you have to show that $\Psi$ is a composite function of two continuous functions. Once you manage to describe $\Psi$ with the composition of two functions, answering the second question might be more easier. The issue I get with the composition, is that my intuition first tells me that $$\Psi(f) = g \circ u $$ where $u:E \rightarrow \mathbb{R}$ such that $u(f) = f(t)$ and $g: \mathbb{R} \rightarrow \mathbb{R}$ such that $g(x)=sin(x)$ Now this is obviously false as $\Psi$ maps to $E$ and not $\mathbb{R}$. Can someone help me to find the two functions that fit?",,"['derivatives', 'frechet-derivative']"
21,Finding the derivative of an integral function,Finding the derivative of an integral function,,"I had some homework for my differential equations class, and one of the questions completely stumped me, reproduced here: Find $\frac{dy}{dx}$ for $y = ce^{-x} + e^{-x}\int_0^x\frac{tan(t)}{t}dt$ My next line looked like $\frac{dy}{dx} = -ce^{-x} + \frac{d}{dx}(e^{-x}\int_0^x\frac{tan(t)}{t}dt)$ and by using the Fundamental Theorem of Calculus and the Product Rule (I don't know how correctly), my next line looked like $\frac{dy}{dx} = -ce^{-x} + (e^{-x}\frac{d}{dx}\int_0^x\frac{tan(t)}{t}dt - e^{-x}\int_0^x\frac{tan(t)}{t}dt )$ and subsequently $\frac{dy}{dx} = -ce^{-x} + e^{-x}\frac{tan(x)}{x} - e^{-x}\int_0^x\frac{tan(t)}{t}dt$ I tried evaluating the integral, since a simple derivation made it clear that it couldn't be avoided. However, I was not able to do so, and when looking it up on www.symbolab.com, it turns out to have no elementary antiderivative/is non-integrable. Does anyone know how to solve the original question? NOTE: the assignment deadline already passed and I have already been graded on my attempt at this question. This is not an attempt to pass off anyone's insights and work as my own.","I had some homework for my differential equations class, and one of the questions completely stumped me, reproduced here: Find $\frac{dy}{dx}$ for $y = ce^{-x} + e^{-x}\int_0^x\frac{tan(t)}{t}dt$ My next line looked like $\frac{dy}{dx} = -ce^{-x} + \frac{d}{dx}(e^{-x}\int_0^x\frac{tan(t)}{t}dt)$ and by using the Fundamental Theorem of Calculus and the Product Rule (I don't know how correctly), my next line looked like $\frac{dy}{dx} = -ce^{-x} + (e^{-x}\frac{d}{dx}\int_0^x\frac{tan(t)}{t}dt - e^{-x}\int_0^x\frac{tan(t)}{t}dt )$ and subsequently $\frac{dy}{dx} = -ce^{-x} + e^{-x}\frac{tan(x)}{x} - e^{-x}\int_0^x\frac{tan(t)}{t}dt$ I tried evaluating the integral, since a simple derivation made it clear that it couldn't be avoided. However, I was not able to do so, and when looking it up on www.symbolab.com, it turns out to have no elementary antiderivative/is non-integrable. Does anyone know how to solve the original question? NOTE: the assignment deadline already passed and I have already been graded on my attempt at this question. This is not an attempt to pass off anyone's insights and work as my own.",,"['derivatives', 'definite-integrals', 'improper-integrals']"
22,"What is the derivative of the inverse of dot (inner) product? $\frac{\partial}{\partial t}\left(\langle A,\;A\rangle\right)^{-1}=?$",What is the derivative of the inverse of dot (inner) product?,"\frac{\partial}{\partial t}\left(\langle A,\;A\rangle\right)^{-1}=?","What is the derivative of the inverse of dot (inner) product? $$\frac{\partial}{\partial t}\left(\langle A,\;A\rangle\right)^{-1}=?$$ where $A$ is a vector.","What is the derivative of the inverse of dot (inner) product? $$\frac{\partial}{\partial t}\left(\langle A,\;A\rangle\right)^{-1}=?$$ where $A$ is a vector.",,"['calculus', 'derivatives', 'partial-derivative']"
23,Determinant derivative in index notation,Determinant derivative in index notation,,"I'm new to index notation and i'm trying to prove the identity $$\frac{d}{dt}\det(A(t))=\det(A)\operatorname{tr} \left(A^{-1}\frac{dA}{dt}\right) $$ for the special case when $A$ is an invertible second order tensor, using just index notation in the following fashion: \begin{align} \frac{d}{dt}\det(A(t)) & =\frac{d}{dt}(\varepsilon_{ijk}A_{i1}A_{j2}A_{k3}) \\[10pt] & =\varepsilon_{ijk} \left(\frac{d}{dt}(A_{i1})A_{j2}A_{k3}+\frac{d}{dt}(A_{j2})A_{i1}A_{k3}+\frac{d}{dt}(A_{k3})A_{i1}A_{j2})\right) \\[10pt] & =\varepsilon_{ijk}A_{r1}A_{j2}A_{k3} \left( A^{-1}_{1r}\frac{A_{i1}}{dt} + A^{-1}_{2r} \frac{A_{j2}}{dt}+A^{-1}_{3r}\frac{A_{k3}}{dt} \right) \end{align} where $\varepsilon$ is the permutation symbol. This is as far as I can get and it frustrates me because if I just switch the index ""$r$"" to ""$i$"" then the proof is done. Have I missed something obvious or made some error? I don't know how to proceed from here. Best regards Bengt","I'm new to index notation and i'm trying to prove the identity $$\frac{d}{dt}\det(A(t))=\det(A)\operatorname{tr} \left(A^{-1}\frac{dA}{dt}\right) $$ for the special case when $A$ is an invertible second order tensor, using just index notation in the following fashion: \begin{align} \frac{d}{dt}\det(A(t)) & =\frac{d}{dt}(\varepsilon_{ijk}A_{i1}A_{j2}A_{k3}) \\[10pt] & =\varepsilon_{ijk} \left(\frac{d}{dt}(A_{i1})A_{j2}A_{k3}+\frac{d}{dt}(A_{j2})A_{i1}A_{k3}+\frac{d}{dt}(A_{k3})A_{i1}A_{j2})\right) \\[10pt] & =\varepsilon_{ijk}A_{r1}A_{j2}A_{k3} \left( A^{-1}_{1r}\frac{A_{i1}}{dt} + A^{-1}_{2r} \frac{A_{j2}}{dt}+A^{-1}_{3r}\frac{A_{k3}}{dt} \right) \end{align} where $\varepsilon$ is the permutation symbol. This is as far as I can get and it frustrates me because if I just switch the index ""$r$"" to ""$i$"" then the proof is done. Have I missed something obvious or made some error? I don't know how to proceed from here. Best regards Bengt",,"['derivatives', 'tensors', 'index-notation']"
24,Is there a way to avoid chain rules in finding this derivative of an integral?,Is there a way to avoid chain rules in finding this derivative of an integral?,,"The one technique of differentiation in first-year calculus that is not introduced until after integrals are mentioned is this: $$ \frac d {dx} \int_a^x f(u) \, du = f(x). \tag 1 $$ I am failing to see how to show the following without using anything other than $(1){:}$ $$ \frac d {dx} \int_0^x \left( \int_0^{x-u} f(u)g(v) \, dv \right) \, du = \int_0^x f(x-v) g(v) \, dv. \tag 2 $$ One can write $$ \begin{bmatrix} s \\ t \end{bmatrix} = \begin{bmatrix} u+v \\ v \end{bmatrix}, \qquad \begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} s-t \\ t \end{bmatrix} $$ and then $$ d(u,v) = \left|\frac{\partial(u,v)}{\partial(s,t)} \right| \, d(s,t) = 1 \, d(s,t) \tag 3 $$ and \begin{align} & \iint\limits_{u,v\,:\,0\,\le\,u,\,0\,\le\,v,\, u+v\,\le\,x} f(u)g(v)\, d(u,v) \\[10pt] = {} & \iint\limits_{s,t\,:\, 0\,\le\,t\,\le\,s\,\le\,x} f(s-t) g(t) \, d(s,t) \\[10pt] = {} & \int_0^x \left( \int_0^s f(s-t)g(t)\, dt \right) \, ds \end{align} and then apply $(1),$ getting $(2).$ However, I would prefer using only a one-variable chain rule rather than $(3),$ or better still, no chain rules. Is there a way to do that?","The one technique of differentiation in first-year calculus that is not introduced until after integrals are mentioned is this: $$ \frac d {dx} \int_a^x f(u) \, du = f(x). \tag 1 $$ I am failing to see how to show the following without using anything other than $(1){:}$ $$ \frac d {dx} \int_0^x \left( \int_0^{x-u} f(u)g(v) \, dv \right) \, du = \int_0^x f(x-v) g(v) \, dv. \tag 2 $$ One can write $$ \begin{bmatrix} s \\ t \end{bmatrix} = \begin{bmatrix} u+v \\ v \end{bmatrix}, \qquad \begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} s-t \\ t \end{bmatrix} $$ and then $$ d(u,v) = \left|\frac{\partial(u,v)}{\partial(s,t)} \right| \, d(s,t) = 1 \, d(s,t) \tag 3 $$ and \begin{align} & \iint\limits_{u,v\,:\,0\,\le\,u,\,0\,\le\,v,\, u+v\,\le\,x} f(u)g(v)\, d(u,v) \\[10pt] = {} & \iint\limits_{s,t\,:\, 0\,\le\,t\,\le\,s\,\le\,x} f(s-t) g(t) \, d(s,t) \\[10pt] = {} & \int_0^x \left( \int_0^s f(s-t)g(t)\, dt \right) \, ds \end{align} and then apply $(1),$ getting $(2).$ However, I would prefer using only a one-variable chain rule rather than $(3),$ or better still, no chain rules. Is there a way to do that?",,"['derivatives', 'convolution', 'chain-rule', 'jacobian']"
25,Dominated convergence theorem to show $F$ is integrable.,Dominated convergence theorem to show  is integrable.,F,"$(\Omega,\mathscr{F},\mu)$ measure space. We have a map $f : \Omega \times \mathbb{R} \rightarrow \mathbb{R}$ with these three properties: a) For all $x \in \mathbb{R}$ :  $f(x,\star) : \Omega \rightarrow \mathbb{R}$ is integrable. b) For all $\omega \in \Omega$: $f(\star, \omega) : \mathbb{R}\rightarrow \mathbb{R}$ is differentiable. c) There exists an integrable function $g$, so that for all $x \in \mathbb{R}$ : $|\frac{\partial}{\partial x} f(x, \star) | \le g$ $\mu$-almost everywhere. Show that: $F: \mathbb{R}\rightarrow \mathbb{R}$  is differentiable. $$F(x) := \int_{\Omega} f(x,\omega) d\mu(\omega).$$ And show that for all $x \in \mathbb{R}$: $$F '(x) = \int_{\Omega} \frac{\partial}{\partial x} f(x,\omega) d\mu(\omega)$$ So what I have tried: I'm pretty sure, that we have to use the dominated convergence theorem for Lebesgue integration. First of all we could start with $\lim_{x \rightarrow x_o} \frac{F(x)-F(x_o)}{x-x_o}$. Then we get $\lim_{x \rightarrow x_o} \frac{\int_{\Omega} f(x,\omega) d\mu(\omega) - \int_{\Omega} f(x_o,\omega) d\mu(\omega)}{x-x_o}$.  But what do I now? Maybe:  $\lim_{x \rightarrow x_o} \int_{\Omega} \frac{  f(x,\omega) dµ(\omega) - f(x_o,\omega) d\mu(\omega)}{x-x_0}$. Is this right? Could I use now the dominated convergence theorem? Then I could use the fact that $f$ is differentiable? But I'm not sure. Thank you for your reply.","$(\Omega,\mathscr{F},\mu)$ measure space. We have a map $f : \Omega \times \mathbb{R} \rightarrow \mathbb{R}$ with these three properties: a) For all $x \in \mathbb{R}$ :  $f(x,\star) : \Omega \rightarrow \mathbb{R}$ is integrable. b) For all $\omega \in \Omega$: $f(\star, \omega) : \mathbb{R}\rightarrow \mathbb{R}$ is differentiable. c) There exists an integrable function $g$, so that for all $x \in \mathbb{R}$ : $|\frac{\partial}{\partial x} f(x, \star) | \le g$ $\mu$-almost everywhere. Show that: $F: \mathbb{R}\rightarrow \mathbb{R}$  is differentiable. $$F(x) := \int_{\Omega} f(x,\omega) d\mu(\omega).$$ And show that for all $x \in \mathbb{R}$: $$F '(x) = \int_{\Omega} \frac{\partial}{\partial x} f(x,\omega) d\mu(\omega)$$ So what I have tried: I'm pretty sure, that we have to use the dominated convergence theorem for Lebesgue integration. First of all we could start with $\lim_{x \rightarrow x_o} \frac{F(x)-F(x_o)}{x-x_o}$. Then we get $\lim_{x \rightarrow x_o} \frac{\int_{\Omega} f(x,\omega) d\mu(\omega) - \int_{\Omega} f(x_o,\omega) d\mu(\omega)}{x-x_o}$.  But what do I now? Maybe:  $\lim_{x \rightarrow x_o} \int_{\Omega} \frac{  f(x,\omega) dµ(\omega) - f(x_o,\omega) d\mu(\omega)}{x-x_0}$. Is this right? Could I use now the dominated convergence theorem? Then I could use the fact that $f$ is differentiable? But I'm not sure. Thank you for your reply.",,"['integration', 'derivatives', 'lebesgue-integral']"
26,"$\inf\limits_{x \in [a,b]}|f^{\prime} (x)| \geq \frac{1}{b-a}$",,"\inf\limits_{x \in [a,b]}|f^{\prime} (x)| \geq \frac{1}{b-a}","Let $f \in C^1([0,1])$ be a non-decrease function such that $f(0)=0, f(1)=1$. Does there exist $[a,b] \subset [0,1]$ such that $\inf\limits_{x \in [a,b]}|f^{\prime} (x)| \geq \frac{1}{b-a}$?","Let $f \in C^1([0,1])$ be a non-decrease function such that $f(0)=0, f(1)=1$. Does there exist $[a,b] \subset [0,1]$ such that $\inf\limits_{x \in [a,b]}|f^{\prime} (x)| \geq \frac{1}{b-a}$?",,['derivatives']
27,"If the summation of $\frac{1}{^nC_r}$ is $a_n$, find the summation of $\frac{r}{^nC_r}$ in terms of $a_n$","If the summation of  is , find the summation of  in terms of",\frac{1}{^nC_r} a_n \frac{r}{^nC_r} a_n,"My attempt to solve this question: Write $a_n=\Sigma^n_{r=0}\frac{1}{^nC_r}$ as => $a_n=\frac{1}{^nC_0}+\frac{x}{^nC_1}+\frac{x^2}{^nC_2}...+\frac{x^n}{^nC_n}$ Differentiate $a_n$ with respect to x, obtaining: $a_n=\frac{1}{^nC_1}+\frac{2x}{^nC_2}+\frac{3x^2}{^nC_3}...+\frac{nx^{n-1}}{^nC_n}$ Replace $x=1$ in $\frac{d(a_n)}{dx}$, but now I don't have $a_n$ as a function of x. So I guess my basic question is which binomial expansion has $^nC_r$ in the denominator, which I can use to write $a_n$ as a function of x?","My attempt to solve this question: Write $a_n=\Sigma^n_{r=0}\frac{1}{^nC_r}$ as => $a_n=\frac{1}{^nC_0}+\frac{x}{^nC_1}+\frac{x^2}{^nC_2}...+\frac{x^n}{^nC_n}$ Differentiate $a_n$ with respect to x, obtaining: $a_n=\frac{1}{^nC_1}+\frac{2x}{^nC_2}+\frac{3x^2}{^nC_3}...+\frac{nx^{n-1}}{^nC_n}$ Replace $x=1$ in $\frac{d(a_n)}{dx}$, but now I don't have $a_n$ as a function of x. So I guess my basic question is which binomial expansion has $^nC_r$ in the denominator, which I can use to write $a_n$ as a function of x?",,"['derivatives', 'binomial-coefficients', 'binomial-theorem']"
28,Show that $\frac{d^{n}}{dx^{n}}\left[ x^{n-1}f\left(\frac{1}{x}\right)\right]=\frac{(-1)^{n}}{x^{n+1}}f^{(n)}\left(\frac{1}{x}\right)$,Show that,\frac{d^{n}}{dx^{n}}\left[ x^{n-1}f\left(\frac{1}{x}\right)\right]=\frac{(-1)^{n}}{x^{n+1}}f^{(n)}\left(\frac{1}{x}\right),How can I prove that $$\frac{d^{n}}{dx^{n}}\left[ x^{n-1}f\left(\frac{1}{x}\right)\right]=\frac{(-1)^{n}}{x^{n+1}}f^{(n)}\left(\frac{1}{x}\right)$$ without using mathematical induction? For any $n\in\mathbb{N}$.,How can I prove that $$\frac{d^{n}}{dx^{n}}\left[ x^{n-1}f\left(\frac{1}{x}\right)\right]=\frac{(-1)^{n}}{x^{n+1}}f^{(n)}\left(\frac{1}{x}\right)$$ without using mathematical induction? For any $n\in\mathbb{N}$.,,"['calculus', 'derivatives']"
29,Prove that $\nabla_{A} (XAY) = Y^{T}X^{T}$,Prove that,\nabla_{A} (XAY) = Y^{T}X^{T},"Equation (3) in Dawen Liang's Some Important Properties for Matrix Calculus is $$\nabla_{A} (XAY) = Y^{T}X^{T}$$ If you know how this can be derived, then please let me know. Thank you. Replying To H. H. Rugh Thanks for the reply, May I explain my derivative? Let suppose, $$ X\in \Re^{m\times n} A\in \Re^{n\times o} Y\in \Re^{o\times p} $$ and $$\widetilde { x } _{ i }^{T}$$ is an ith row vector of matrix X and $$y_{j} $$ is a ith coulmn vector of Y then I guess I can get a derivative of a component of matrix $(XAY)_{ij} $ like this $$\frac { \partial (XAY)_{ ij } }{ \partial A } =\frac { \partial \widetilde { x }_{ i }^{T} Ay_{ j } }{ \partial A } =\widetilde { x }_{ i}  y_{ j }^{ T }\in \Re ^{ n\times o } $$ So filnally I made a conclusion like $$\frac { \partial XAY }{ \partial A } =\begin{bmatrix} \widetilde { x } _{ 1 }y_{ 1 }^{ T } & \widetilde { x } _{ 2 }y_{ 1 }^{ T } & \cdots  & \widetilde { x } _{ m }y_{ 1 }^{ T } \\ \widetilde { x } _{ 1 }y_{ 2 }^{ T } & \widetilde { x } _{ 2 }y_{ 2 }^{ T } & \cdots  & \widetilde { x } _{ m }y_{ 2 }^{ T } \\ \vdots  & \vdots  & \ddots  & \vdots  \\ \widetilde { x } _{ 1 }y_{ p }^{ T } & \widetilde { x } _{ 2 }y_{ p }^{ T } & \cdots  & \widetilde { x } _{ m }y_{ p }^{ T } \end{bmatrix}\in \Re ^{ pn\times mo }$$ by denominator layout notation. Is my derivation wrong? See also https://en.wikipedia.org/wiki/Matrix_calculus#Other_matrix_derivatives The reason why I have been asking this is I guess there might be a typo in eq.(3). But I have no confidence. Because I'm not familiar with matrix calculus. So If you have the correct and a detail derivation, and please give that, It would be very helpful to me or if you give an information of documents, which include the derivation, it also good to me. I spend all day to find the derivation of eq.(3) yesterday. Because I guess, if the eq.(3) is one of the basic properties, then many documents may include it, but I couldn't find it at all.","Equation (3) in Dawen Liang's Some Important Properties for Matrix Calculus is $$\nabla_{A} (XAY) = Y^{T}X^{T}$$ If you know how this can be derived, then please let me know. Thank you. Replying To H. H. Rugh Thanks for the reply, May I explain my derivative? Let suppose, $$ X\in \Re^{m\times n} A\in \Re^{n\times o} Y\in \Re^{o\times p} $$ and $$\widetilde { x } _{ i }^{T}$$ is an ith row vector of matrix X and $$y_{j} $$ is a ith coulmn vector of Y then I guess I can get a derivative of a component of matrix $(XAY)_{ij} $ like this $$\frac { \partial (XAY)_{ ij } }{ \partial A } =\frac { \partial \widetilde { x }_{ i }^{T} Ay_{ j } }{ \partial A } =\widetilde { x }_{ i}  y_{ j }^{ T }\in \Re ^{ n\times o } $$ So filnally I made a conclusion like $$\frac { \partial XAY }{ \partial A } =\begin{bmatrix} \widetilde { x } _{ 1 }y_{ 1 }^{ T } & \widetilde { x } _{ 2 }y_{ 1 }^{ T } & \cdots  & \widetilde { x } _{ m }y_{ 1 }^{ T } \\ \widetilde { x } _{ 1 }y_{ 2 }^{ T } & \widetilde { x } _{ 2 }y_{ 2 }^{ T } & \cdots  & \widetilde { x } _{ m }y_{ 2 }^{ T } \\ \vdots  & \vdots  & \ddots  & \vdots  \\ \widetilde { x } _{ 1 }y_{ p }^{ T } & \widetilde { x } _{ 2 }y_{ p }^{ T } & \cdots  & \widetilde { x } _{ m }y_{ p }^{ T } \end{bmatrix}\in \Re ^{ pn\times mo }$$ by denominator layout notation. Is my derivation wrong? See also https://en.wikipedia.org/wiki/Matrix_calculus#Other_matrix_derivatives The reason why I have been asking this is I guess there might be a typo in eq.(3). But I have no confidence. Because I'm not familiar with matrix calculus. So If you have the correct and a detail derivation, and please give that, It would be very helpful to me or if you give an information of documents, which include the derivation, it also good to me. I spend all day to find the derivation of eq.(3) yesterday. Because I guess, if the eq.(3) is one of the basic properties, then many documents may include it, but I couldn't find it at all.",,"['matrices', 'derivatives', 'matrix-calculus']"
30,Prove $\sum_{n=1}^\infty\frac{(-1)^n}{\sqrt{n}}\arctan(\frac{x}{\sqrt{n}})$ converges and defines a continuously differentiable function,Prove  converges and defines a continuously differentiable function,\sum_{n=1}^\infty\frac{(-1)^n}{\sqrt{n}}\arctan(\frac{x}{\sqrt{n}}),"Prove $\sum_{n=1}^\infty\frac{(-1)^n}{\sqrt{n}}\arctan\big(\frac{x}{\sqrt{n}}\big)$ converges for all $x,$ and defines a continuously differentiable function on $\mathbb{R}.$ By Leibniz test, the series converges for all $x.$ For $x=0,$ we get series of $0$'s, and the series converges. Let us show the series of the derivatives converges uniformly: $$\bigg(\sum_{n=1}^\infty\frac{(-1)^n}{\sqrt{n}}\arctan\bigg(\frac{x}{\sqrt{n}}\bigg)\bigg)'=\sum_{n=1}^\infty \frac{(-1)^n}{\sqrt{n}\sqrt{n}\big(1+\frac{x^2}{n}\big)}=\sum_{n=1}^\infty \frac{(-1)^n}{x^2+n}$$ $\sum(-1)^n$ is bounded uniformly and $\frac{1}{x^2+n}$ decreasing to $0,$ so by Dirichlet's test the series of derivatives converges uniformly. By term-by-term differention theorem we conclude the original series is continuously differentiable. Is that correct?","Prove $\sum_{n=1}^\infty\frac{(-1)^n}{\sqrt{n}}\arctan\big(\frac{x}{\sqrt{n}}\big)$ converges for all $x,$ and defines a continuously differentiable function on $\mathbb{R}.$ By Leibniz test, the series converges for all $x.$ For $x=0,$ we get series of $0$'s, and the series converges. Let us show the series of the derivatives converges uniformly: $$\bigg(\sum_{n=1}^\infty\frac{(-1)^n}{\sqrt{n}}\arctan\bigg(\frac{x}{\sqrt{n}}\bigg)\bigg)'=\sum_{n=1}^\infty \frac{(-1)^n}{\sqrt{n}\sqrt{n}\big(1+\frac{x^2}{n}\big)}=\sum_{n=1}^\infty \frac{(-1)^n}{x^2+n}$$ $\sum(-1)^n$ is bounded uniformly and $\frac{1}{x^2+n}$ decreasing to $0,$ so by Dirichlet's test the series of derivatives converges uniformly. By term-by-term differention theorem we conclude the original series is continuously differentiable. Is that correct?",,"['real-analysis', 'sequences-and-series', 'derivatives', 'uniform-convergence']"
31,How to derive the Taylor's theorem logically?,How to derive the Taylor's theorem logically?,,"I can't understand how derive the Talyor's theorem logically: Well, the first two terms are quite easy to think of as $f(a)$ + correction term (i.e. slope times change in x-coordinate: $(x-a)$. But how to derive the other correction terms just by logic?","I can't understand how derive the Talyor's theorem logically: Well, the first two terms are quite easy to think of as $f(a)$ + correction term (i.e. slope times change in x-coordinate: $(x-a)$. But how to derive the other correction terms just by logic?",,['calculus']
32,Show that the minimum of (particular) program is differentiable w.r.t. constraint size,Show that the minimum of (particular) program is differentiable w.r.t. constraint size,,"Question Define the function $f: (0, \infty) \to \mathbb{R}$ by $$f(c) = \min_{x \in \mathbb{R}^n \, : \, \|x\| = c} \|b - A x\|_2^2,$$ for $A \in \mathbb{R}^{m \times n}$ with full rank, $b \in \mathbb{R}^m$, and $\|\cdot\|$ some norm. How do I show that $f(c)$ is differentiable? Is it possible to show that $f''(c) \ge 0$ for $c$ less than $\|\hat{x}\|$, to be defined later? Thoughts toward solution Plug into derivative Plugging this function into the definition of the derivative isn't illuminating. ""Geometric"" interpretation There is a clear geometric interpretation of this problem, since, $$f(c) = \|b - A \hat{x} \|_2^2 + \min_{\|x\| = c} (x - \hat{x})^T (A^T A) (x - \hat{x}),$$ for $\hat{x} = (A^TA)^{+}A^T b \in \arg\min_{x \in \mathbb{R}^n} \|b - A x\|_2^2$ and $\left( \cdot \right)^+$ the pseudo-inverse. Thus, we need only to consider the function  \begin{align*}  g(c)  & =  \min_{\|x\| = c} (x - \hat{x})^T (A^T A) (x - \hat{x}) \\  & = c^2 \min_{\|x\| = 1} (x - c^{-1} \hat{x})^T (A^T A) (x - c^{-1} \hat{x}) \\  & = c^2 \left( \mathbf{d}(c^{-1} \hat{x}, \Omega) \right)^2, \end{align*}  where $\mathbf{d}(x,y) = \|A(x-y)\|_2^2$ is a pseudo-metric (and a metric if $A$ is ""skinny"") and $\Omega = \{x \in \mathbb{R}^n \, : \, \|x\|=1 \}$. Notice that $g$ is differentiable if and only if the function $$c \mapsto  \mathbf{d}^2(c \hat{x}, \Omega) \tag{*}$$ is differentiable for $c \in (0, \infty)$ and a fixed point $\hat{x}$. A very special case as an example: note that if $A^T A = I$, $\|\cdot\| = \|\cdot\|_1,$ and $|\hat{x}_j| = |\hat{x}_k|$ for all $j,k$, then the projection $\Pi_\Omega(\hat{x}) = \mathrm{sgn}(\hat{x})$, and $\mathbf{d}^2(c\hat{x}, \Omega) = \left( \sqrt{2} \left| \frac{c}{\|\hat{x}\|}-1 \right| \right)^2$, which is differentiable and has positive second derivative. Later, I add this more general example: We will reexpress $\hat{x}$ in a basis where each basis vector is orthogonal to a face of the $\ell_1$ norm level set. Assume that $\|c \hat{x} \|_1 > 1$ so that the point $c \hat{x}$ is outside of the unit ball. Assume without loss of generality that $\hat{x}_j > 0$. Then, since now each component of $c\hat{x}$ measures it's distance from a face, we have that $d(c \hat{x}, \Omega) = \left[ \sum_{j=1}^n (c \hat{x}_j - 1)^2 \mathbf{1}_{c > d_j} \right]^{1/2},$ where $\{d_j\}$ is the set of knots of $\mathbf{proj}_\Omega c\hat{x}$. Therefore, the distance $d$ is has a derivative which increases across each knot, so that it's convex. Perhaps this is a convex program on part of its domain? I also think that it could be true that $f(c)$ is decreasing on $(0, \|\hat{x}\|)$ so that $$f(c) \stackrel{?}{=} \min_{x \in \mathbb{R}^n \, : \, \|x\| \leq c} \|b - A x\|_2^2,$$ for $c \in (0, \|\hat{x}\|)$. This would make this a convex program and hence more amenable to analysis. I think this could be true since as $C$ increases within $(0, \|\hat{x}\|)$, the value $f(c)$ will get ""closer"" to the unconstrained minimum $\|b - A \hat{x}\|_2^2$. Further comments on question If possible, it would be interesting to know how general the norm $\|\cdot\|$ could be while still having the result being provable. If it helps to simplify the problem, I'm particular interested in the case $\|x\| = \|x\|_1$. As Rodrigo kindly pointed out, the case of $\|x\| = \|x\|_2$ follows from noticing that ""ridge regression"" estimator $f(c)$ has a closed form.","Question Define the function $f: (0, \infty) \to \mathbb{R}$ by $$f(c) = \min_{x \in \mathbb{R}^n \, : \, \|x\| = c} \|b - A x\|_2^2,$$ for $A \in \mathbb{R}^{m \times n}$ with full rank, $b \in \mathbb{R}^m$, and $\|\cdot\|$ some norm. How do I show that $f(c)$ is differentiable? Is it possible to show that $f''(c) \ge 0$ for $c$ less than $\|\hat{x}\|$, to be defined later? Thoughts toward solution Plug into derivative Plugging this function into the definition of the derivative isn't illuminating. ""Geometric"" interpretation There is a clear geometric interpretation of this problem, since, $$f(c) = \|b - A \hat{x} \|_2^2 + \min_{\|x\| = c} (x - \hat{x})^T (A^T A) (x - \hat{x}),$$ for $\hat{x} = (A^TA)^{+}A^T b \in \arg\min_{x \in \mathbb{R}^n} \|b - A x\|_2^2$ and $\left( \cdot \right)^+$ the pseudo-inverse. Thus, we need only to consider the function  \begin{align*}  g(c)  & =  \min_{\|x\| = c} (x - \hat{x})^T (A^T A) (x - \hat{x}) \\  & = c^2 \min_{\|x\| = 1} (x - c^{-1} \hat{x})^T (A^T A) (x - c^{-1} \hat{x}) \\  & = c^2 \left( \mathbf{d}(c^{-1} \hat{x}, \Omega) \right)^2, \end{align*}  where $\mathbf{d}(x,y) = \|A(x-y)\|_2^2$ is a pseudo-metric (and a metric if $A$ is ""skinny"") and $\Omega = \{x \in \mathbb{R}^n \, : \, \|x\|=1 \}$. Notice that $g$ is differentiable if and only if the function $$c \mapsto  \mathbf{d}^2(c \hat{x}, \Omega) \tag{*}$$ is differentiable for $c \in (0, \infty)$ and a fixed point $\hat{x}$. A very special case as an example: note that if $A^T A = I$, $\|\cdot\| = \|\cdot\|_1,$ and $|\hat{x}_j| = |\hat{x}_k|$ for all $j,k$, then the projection $\Pi_\Omega(\hat{x}) = \mathrm{sgn}(\hat{x})$, and $\mathbf{d}^2(c\hat{x}, \Omega) = \left( \sqrt{2} \left| \frac{c}{\|\hat{x}\|}-1 \right| \right)^2$, which is differentiable and has positive second derivative. Later, I add this more general example: We will reexpress $\hat{x}$ in a basis where each basis vector is orthogonal to a face of the $\ell_1$ norm level set. Assume that $\|c \hat{x} \|_1 > 1$ so that the point $c \hat{x}$ is outside of the unit ball. Assume without loss of generality that $\hat{x}_j > 0$. Then, since now each component of $c\hat{x}$ measures it's distance from a face, we have that $d(c \hat{x}, \Omega) = \left[ \sum_{j=1}^n (c \hat{x}_j - 1)^2 \mathbf{1}_{c > d_j} \right]^{1/2},$ where $\{d_j\}$ is the set of knots of $\mathbf{proj}_\Omega c\hat{x}$. Therefore, the distance $d$ is has a derivative which increases across each knot, so that it's convex. Perhaps this is a convex program on part of its domain? I also think that it could be true that $f(c)$ is decreasing on $(0, \|\hat{x}\|)$ so that $$f(c) \stackrel{?}{=} \min_{x \in \mathbb{R}^n \, : \, \|x\| \leq c} \|b - A x\|_2^2,$$ for $c \in (0, \|\hat{x}\|)$. This would make this a convex program and hence more amenable to analysis. I think this could be true since as $C$ increases within $(0, \|\hat{x}\|)$, the value $f(c)$ will get ""closer"" to the unconstrained minimum $\|b - A \hat{x}\|_2^2$. Further comments on question If possible, it would be interesting to know how general the norm $\|\cdot\|$ could be while still having the result being provable. If it helps to simplify the problem, I'm particular interested in the case $\|x\| = \|x\|_1$. As Rodrigo kindly pointed out, the case of $\|x\| = \|x\|_2$ follows from noticing that ""ridge regression"" estimator $f(c)$ has a closed form.",,"['derivatives', 'optimization', 'duality-theorems', 'non-convex-optimization']"
33,parametric spiral arc length,parametric spiral arc length,,"I am attempting to calculate the arc length of a spiral. Given an Archamedean spiral of the parametric form: $x(t) = \sqrt{t}\cos\left(\omega\sqrt{t} \right)$ and $y(t) = \sqrt{t}\sin\left(\omega\sqrt{t} \right)$, the arc length $L$ is calculated as \begin{align}\label{parametricArcLength} 	L & = \int_a^b \sqrt{1 + \left(\frac{\frac{dy}{dt}}{\frac{dx}{dt}} \right)^2}\cdot \frac{dx}{dt}dt \\ 	& = \int_a^b \sqrt{\left(\frac{dx}{dt}\right)^2 + \left(\frac{dy}{dt}\right)^2} \ \ dt  \end{align} with  \begin{align} 	\frac{dx}{dt} & = \frac{1}{2\sqrt{t{}}} \cdot \cos (\omega \sqrt{t}) - \sqrt{t} \cdot \frac{\omega}{2\sqrt{t}} \sin (\omega \sqrt{t}) \\ 	& = \frac{\cos (\omega \sqrt{t})}{2\sqrt{t}} - \frac{\omega \sin (\omega \sqrt{t})}{2} \nonumber \end{align} \begin{align} \label{dy_dt} \frac{dy}{dt} & = \frac{1}{2\sqrt{t}} \cdot \sin (\omega \sqrt{t}) + \sqrt{t} \cdot \frac{\omega}{2\sqrt{t}} \cos (\omega \sqrt{t}) \\ 	& = \frac{\sin (\omega \sqrt{t})}{2 \sqrt{t}} + \frac{\omega \cos (\omega \sqrt{t})}{2} \nonumber \end{align} yielding \begin{split} L = &\int_{t_a}^{t_b} \sqrt{\left(\frac{dx}{dt}\right)^2 + \left(\frac{dy}{dt}\right)^2} \ \ dt \\    = & \int_{t_a}^{t_b} \left[\frac{1}{4}\left(\frac{\cos^2 \left(\omega \sqrt{t}\right)}{t} - \frac{2\omega\cos\left(\omega\sqrt{t} \right) \sin \left(\omega \sqrt{t}\right)}{\sqrt{t}} + \omega^2 \sin^2\left(\omega \sqrt{t}\right)\right) \right.\\ 	& \left. + \frac{1}{4}\left( \frac{\sin^2\left( \omega\sqrt{t} \right)}{t} + \frac{2\omega \sin \left(\omega \sqrt{t}\right) \cos\left(\omega \sqrt{t} \right)}{\sqrt{t}} + \omega^2 \cos\left(\omega \sqrt{t} \right)  \right) \right]^{1/2}dt\\ 	= & \int_{t_a}^{t_b} \left[\frac{1}{4}\left( \frac{\sin^2\left(\omega\sqrt{t}\right) + \cos^2\left(\omega\sqrt{t}\right)}{t}   + \omega^2\sin^2\left(\omega\sqrt{t}\right) + \omega^2\cos^2\left(\omega\sqrt{t}\right)\right)\right]^{1/2}dt\\ 	= & \int_{t_a}^{t_b} \left[ \frac{1}{4} \left( \frac{1}{t} + \omega^2 \right)\right]^{1/2}dt\\ 	= &  \frac{1}{2}\int_{t_a}^{t_b}\left[\frac{1}{t} + \omega^2 \right]^{1/2}dt \end{split} Is the above integral the correct solution? If so, what method should be used to solve it?","I am attempting to calculate the arc length of a spiral. Given an Archamedean spiral of the parametric form: $x(t) = \sqrt{t}\cos\left(\omega\sqrt{t} \right)$ and $y(t) = \sqrt{t}\sin\left(\omega\sqrt{t} \right)$, the arc length $L$ is calculated as \begin{align}\label{parametricArcLength} 	L & = \int_a^b \sqrt{1 + \left(\frac{\frac{dy}{dt}}{\frac{dx}{dt}} \right)^2}\cdot \frac{dx}{dt}dt \\ 	& = \int_a^b \sqrt{\left(\frac{dx}{dt}\right)^2 + \left(\frac{dy}{dt}\right)^2} \ \ dt  \end{align} with  \begin{align} 	\frac{dx}{dt} & = \frac{1}{2\sqrt{t{}}} \cdot \cos (\omega \sqrt{t}) - \sqrt{t} \cdot \frac{\omega}{2\sqrt{t}} \sin (\omega \sqrt{t}) \\ 	& = \frac{\cos (\omega \sqrt{t})}{2\sqrt{t}} - \frac{\omega \sin (\omega \sqrt{t})}{2} \nonumber \end{align} \begin{align} \label{dy_dt} \frac{dy}{dt} & = \frac{1}{2\sqrt{t}} \cdot \sin (\omega \sqrt{t}) + \sqrt{t} \cdot \frac{\omega}{2\sqrt{t}} \cos (\omega \sqrt{t}) \\ 	& = \frac{\sin (\omega \sqrt{t})}{2 \sqrt{t}} + \frac{\omega \cos (\omega \sqrt{t})}{2} \nonumber \end{align} yielding \begin{split} L = &\int_{t_a}^{t_b} \sqrt{\left(\frac{dx}{dt}\right)^2 + \left(\frac{dy}{dt}\right)^2} \ \ dt \\    = & \int_{t_a}^{t_b} \left[\frac{1}{4}\left(\frac{\cos^2 \left(\omega \sqrt{t}\right)}{t} - \frac{2\omega\cos\left(\omega\sqrt{t} \right) \sin \left(\omega \sqrt{t}\right)}{\sqrt{t}} + \omega^2 \sin^2\left(\omega \sqrt{t}\right)\right) \right.\\ 	& \left. + \frac{1}{4}\left( \frac{\sin^2\left( \omega\sqrt{t} \right)}{t} + \frac{2\omega \sin \left(\omega \sqrt{t}\right) \cos\left(\omega \sqrt{t} \right)}{\sqrt{t}} + \omega^2 \cos\left(\omega \sqrt{t} \right)  \right) \right]^{1/2}dt\\ 	= & \int_{t_a}^{t_b} \left[\frac{1}{4}\left( \frac{\sin^2\left(\omega\sqrt{t}\right) + \cos^2\left(\omega\sqrt{t}\right)}{t}   + \omega^2\sin^2\left(\omega\sqrt{t}\right) + \omega^2\cos^2\left(\omega\sqrt{t}\right)\right)\right]^{1/2}dt\\ 	= & \int_{t_a}^{t_b} \left[ \frac{1}{4} \left( \frac{1}{t} + \omega^2 \right)\right]^{1/2}dt\\ 	= &  \frac{1}{2}\int_{t_a}^{t_b}\left[\frac{1}{t} + \omega^2 \right]^{1/2}dt \end{split} Is the above integral the correct solution? If so, what method should be used to solve it?",,"['derivatives', 'definite-integrals', 'parametric']"
34,Familiarizing with the Fundamental Theorem of Calculus,Familiarizing with the Fundamental Theorem of Calculus,,"Consider the following form of the Fundamental Theorem of Calculus: ""Let $f:[a,b] \rightarrow \mathbb{R}$ be a differentiable function. Suppose that $F'$ is Riemann integrable over $[a,b]$. Then $\int^{b}_{a}F'=F(b)-F(a)$."" In order to familiarize with this theorem, it must be remarked that there is no condition for the integrand to be continuous. Now I am looking for a concrete example of a discontinuous, but integrable integrand $f$ which is the derivative of a differentiable function $F$. I came up with $f:[0,2]\rightarrow \mathbb{R}:x \mapsto\begin{cases}1 \text{ if } 0\leq x \leq 1 \\ 2 \text{ if }1<x \leq2\end{cases}$. This integrand is discontinuous, but it is the derivative of a function which is not everywhere differentiable on $[0,2]$ (i.e. on $1$). Can someone give a better example? Thank you!","Consider the following form of the Fundamental Theorem of Calculus: ""Let $f:[a,b] \rightarrow \mathbb{R}$ be a differentiable function. Suppose that $F'$ is Riemann integrable over $[a,b]$. Then $\int^{b}_{a}F'=F(b)-F(a)$."" In order to familiarize with this theorem, it must be remarked that there is no condition for the integrand to be continuous. Now I am looking for a concrete example of a discontinuous, but integrable integrand $f$ which is the derivative of a differentiable function $F$. I came up with $f:[0,2]\rightarrow \mathbb{R}:x \mapsto\begin{cases}1 \text{ if } 0\leq x \leq 1 \\ 2 \text{ if }1<x \leq2\end{cases}$. This integrand is discontinuous, but it is the derivative of a function which is not everywhere differentiable on $[0,2]$ (i.e. on $1$). Can someone give a better example? Thank you!",,"['integration', 'derivatives']"
35,Is it true that $\Delta\cos(k\|x\|)=-k\left(k\cos(k\|x\|)+2\frac{\sin(k\|x\|)}{\|x\|}\right)$?,Is it true that ?,\Delta\cos(k\|x\|)=-k\left(k\cos(k\|x\|)+2\frac{\sin(k\|x\|)}{\|x\|}\right),"As per the title: is it true that for $x\in\mathbb{R}^3$ and $k\in\mathbb{R}_{\ge 0}$, $$\Delta\cos(k\|x\|)=-k\left(k\cos(k\|x\|)+2\frac{\sin(k\|x\|)}{\|x\|}\right)\qquad(\star)$$ My own calculations yield that $$\begin{aligned} \Delta\cos(k\|x\|)&=-k\nabla\left\{\sin(k\|x\|)\frac{x}{\|x\|}\right\} \\ &=-k\left(k\cos(k\|x\|)\frac{x^2}{\|x\|^2}+\sin(k\|x\|)\frac{\|x\|+\frac{x^2}{\|x\|^3}}{\|x\|^2}\right) \end{aligned}$$ which is not the same as $(\star)$. Note that $(\star)$ is the value that I am supposed to verify, as per a textbook I am following.","As per the title: is it true that for $x\in\mathbb{R}^3$ and $k\in\mathbb{R}_{\ge 0}$, $$\Delta\cos(k\|x\|)=-k\left(k\cos(k\|x\|)+2\frac{\sin(k\|x\|)}{\|x\|}\right)\qquad(\star)$$ My own calculations yield that $$\begin{aligned} \Delta\cos(k\|x\|)&=-k\nabla\left\{\sin(k\|x\|)\frac{x}{\|x\|}\right\} \\ &=-k\left(k\cos(k\|x\|)\frac{x^2}{\|x\|^2}+\sin(k\|x\|)\frac{\|x\|+\frac{x^2}{\|x\|^3}}{\|x\|^2}\right) \end{aligned}$$ which is not the same as $(\star)$. Note that $(\star)$ is the value that I am supposed to verify, as per a textbook I am following.",,"['calculus', 'trigonometry', 'derivatives', 'chain-rule']"
36,How do I replace variables in $\frac{d^2 \psi}{dx^2}$?,How do I replace variables in ?,\frac{d^2 \psi}{dx^2},I have $$\frac{d^2 \psi}{dx^2}=a^3 x \psi$$ but I need to change to $z$ by $$z=ax$$ and end with $$\frac{d^2 \psi}{dz^2}=z \psi$$ How do I change variables here? This is what I tried: $$dz=adx$$ $$\frac{d^2 \psi}{dz^2} = \frac{d}{dz}\left (\frac{d \psi}{dz}  \right )$$ $$=\frac{d}{dz}\left (\frac{d \psi}{dx} \frac{dx}{dz}  \right )$$ $$=\frac{d}{dz}\left (\frac{d \psi}{dx} \frac{1}{a} \right )$$ ...but then what do I do with $\frac{d\psi}{dx}$??,I have $$\frac{d^2 \psi}{dx^2}=a^3 x \psi$$ but I need to change to $z$ by $$z=ax$$ and end with $$\frac{d^2 \psi}{dz^2}=z \psi$$ How do I change variables here? This is what I tried: $$dz=adx$$ $$\frac{d^2 \psi}{dz^2} = \frac{d}{dz}\left (\frac{d \psi}{dz}  \right )$$ $$=\frac{d}{dz}\left (\frac{d \psi}{dx} \frac{dx}{dz}  \right )$$ $$=\frac{d}{dz}\left (\frac{d \psi}{dx} \frac{1}{a} \right )$$ ...but then what do I do with $\frac{d\psi}{dx}$??,,"['derivatives', 'change-of-variable']"
37,When can you take the derivative of both sides of an equation?,When can you take the derivative of both sides of an equation?,,"I know in general you cannot take the derivative of an equation to solve it because the derivative at a point depends on neighboring points of a function. However, lots of the proofs done in my probability course, for example finding the variance of a geometric random variable is done by differentiating both sides. Why is this allowed?","I know in general you cannot take the derivative of an equation to solve it because the derivative at a point depends on neighboring points of a function. However, lots of the proofs done in my probability course, for example finding the variance of a geometric random variable is done by differentiating both sides. Why is this allowed?",,"['calculus', 'probability', 'derivatives']"
38,"$FTC$ question, what is $\frac d{dx} \int_{e^{-x}}^x \ln(t^2+1)dt$?","question, what is ?",FTC \frac d{dx} \int_{e^{-x}}^x \ln(t^2+1)dt,"Still working some basic Fundamental Theorem of Calculus problems and I would like to know if what I do is correct when I evaluate something like $$\frac d{dx} \int_{e^{-x}}^x \ln(t^2+1)dt$$ I think this calls for both $$\int_a^b f(x)dx = \int_c^b f(x)dx-\int_c^a f(x)dx = F(b)-F(a)$$ and $$\frac d{dx} \int_a^x f(t)dt=f(x)$$ which gives me $$\frac d{dx} \int_{e^{-x}}^x \ln(t^2+1)dt = \frac d{dx} \left[\int_{e^{-x}}^c \ln(t^2+1)dt + \int_c^x \ln(t^2+1)dt\right]$$ $$= \frac d{dx} \left[\int_c^x \ln(t^2+1)dt - \int_c^{e^{-x}} \ln(t^2+1)dt\right]= \ln \left(\frac {x^2+1}{e^{-2x}+1}\right)$$ The way I see it from $\int_a^b f(x)dx = F(b)-F(a)$ could I just do it directly like this? $$\frac d{dx} \int_{e^{-x}}^x \ln(t^2+1)dt = \frac d{dx} \left[F(x)-F(e^{-x})\right]$$ $$= f(x)-f(e^{-x}) = \ln \left(\frac {x^2+1}{e^{-2x}+1}\right)$$ In general terms can I always do this $$\frac d{dx} \int_{x_1}^{x_2} f(t)dt = \frac d{dx}\left[F(x_2)-F(x_1)\right] = f(x_2)-f(x_1)$$ or would it be abusing the definition? Thank you for taking the time to reply, it helps me a lot and I appreciate it very much! Edit (corrections) As pointed out in Simply Beautiful Art 's comment, I should apply the chain rule, so $$\frac d{dx} \int_{e^{-x}}^x \ln(t^2+1)dt = F'(x)-F'(e^{x})\frac d{dx}e^{-x} = \ln(x^2+1) + \frac{\ln(e^{-2x}+1)}{e^x}$$ and from Ethan Bolker 's answer, the last line should be changed to $$\frac d{dx} \int_{g(x)}^{h(x)} f(t)dt = \frac d{dx}\left[F(h(x)) - F(g(x))\right] =f( h(x))h'(x) - f(g(x))g'(x)$$","Still working some basic Fundamental Theorem of Calculus problems and I would like to know if what I do is correct when I evaluate something like I think this calls for both and which gives me The way I see it from could I just do it directly like this? In general terms can I always do this or would it be abusing the definition? Thank you for taking the time to reply, it helps me a lot and I appreciate it very much! Edit (corrections) As pointed out in Simply Beautiful Art 's comment, I should apply the chain rule, so and from Ethan Bolker 's answer, the last line should be changed to",\frac d{dx} \int_{e^{-x}}^x \ln(t^2+1)dt \int_a^b f(x)dx = \int_c^b f(x)dx-\int_c^a f(x)dx = F(b)-F(a) \frac d{dx} \int_a^x f(t)dt=f(x) \frac d{dx} \int_{e^{-x}}^x \ln(t^2+1)dt = \frac d{dx} \left[\int_{e^{-x}}^c \ln(t^2+1)dt + \int_c^x \ln(t^2+1)dt\right] = \frac d{dx} \left[\int_c^x \ln(t^2+1)dt - \int_c^{e^{-x}} \ln(t^2+1)dt\right]= \ln \left(\frac {x^2+1}{e^{-2x}+1}\right) \int_a^b f(x)dx = F(b)-F(a) \frac d{dx} \int_{e^{-x}}^x \ln(t^2+1)dt = \frac d{dx} \left[F(x)-F(e^{-x})\right] = f(x)-f(e^{-x}) = \ln \left(\frac {x^2+1}{e^{-2x}+1}\right) \frac d{dx} \int_{x_1}^{x_2} f(t)dt = \frac d{dx}\left[F(x_2)-F(x_1)\right] = f(x_2)-f(x_1) \frac d{dx} \int_{e^{-x}}^x \ln(t^2+1)dt = F'(x)-F'(e^{x})\frac d{dx}e^{-x} = \ln(x^2+1) + \frac{\ln(e^{-2x}+1)}{e^x} \frac d{dx} \int_{g(x)}^{h(x)} f(t)dt = \frac d{dx}\left[F(h(x)) - F(g(x))\right] =f( h(x))h'(x) - f(g(x))g'(x),['calculus']
39,Derivatives of exp(f(x)) and partitions of an set,Derivatives of exp(f(x)) and partitions of an set,,"I am trying to express the $k$-th derivative of $g(x) = exp(f(x))$ in a meaningful way. My logic got me so far. For the first derivative, the result would be $$ g' = f' g $$ Second derivative would be $$ g'' = (f'' + f'^2) g $$ Third derivative is $$ g''' = f''' + 3 f'' f' + f'^3 $$ and so on. The correspondence that I made was that each derivative can be expressed as $$ f ''' \equiv (0,0,3) $$ $$ f'f'' = (0,1,2) $$ and $$ f'^3 \equiv (1, 1, 1). $$ This is identical to representing the partitions of the integer 3.  This is similar to the partitions of a set; a set of size 3, $\left\{a,b,c\right\}$, can be partitioned into $$ \left\{a,b,c\right\} \equiv(0,0,3) $$ $$ \left\{a\right\} \left\{b,c\right\}, \left\{b\right\} \left\{a,c\right\}, \left\{c\right\} \left\{a,b\right\} \equiv(0,1,2) $$ $$ \left\{a\right\} \left\{b\right\}  \left\{c\right\} \equiv(1,1,1). $$ The coefficient, for example, of $f'f''$, is the number of partitions corresponding to $(0,1,2)$. Then, I know that these coefficients can be calculated with the help of multinomial coefficients . My question is: is there a proof of (or a reference about) this correspondence between the derivatives of $g$ and counting the number of partitions?","I am trying to express the $k$-th derivative of $g(x) = exp(f(x))$ in a meaningful way. My logic got me so far. For the first derivative, the result would be $$ g' = f' g $$ Second derivative would be $$ g'' = (f'' + f'^2) g $$ Third derivative is $$ g''' = f''' + 3 f'' f' + f'^3 $$ and so on. The correspondence that I made was that each derivative can be expressed as $$ f ''' \equiv (0,0,3) $$ $$ f'f'' = (0,1,2) $$ and $$ f'^3 \equiv (1, 1, 1). $$ This is identical to representing the partitions of the integer 3.  This is similar to the partitions of a set; a set of size 3, $\left\{a,b,c\right\}$, can be partitioned into $$ \left\{a,b,c\right\} \equiv(0,0,3) $$ $$ \left\{a\right\} \left\{b,c\right\}, \left\{b\right\} \left\{a,c\right\}, \left\{c\right\} \left\{a,b\right\} \equiv(0,1,2) $$ $$ \left\{a\right\} \left\{b\right\}  \left\{c\right\} \equiv(1,1,1). $$ The coefficient, for example, of $f'f''$, is the number of partitions corresponding to $(0,1,2)$. Then, I know that these coefficients can be calculated with the help of multinomial coefficients . My question is: is there a proof of (or a reference about) this correspondence between the derivatives of $g$ and counting the number of partitions?",,"['combinatorics', 'derivatives', 'integer-partitions', 'multinomial-coefficients']"
40,Valid to move $x$ to other side of equation in differentials?,Valid to move  to other side of equation in differentials?,x,"Say you have $dy/dx = x$, is it valid to write $dy/x = dx$?","Say you have $dy/dx = x$, is it valid to write $dy/x = dx$?",,"['calculus', 'derivatives']"
41,Finding the derivative of $e^{x}$ with by taking the limit of e and the derivative in the same limit; done before?,Finding the derivative of  with by taking the limit of e and the derivative in the same limit; done before?,e^{x},"I am a curious Calculus student with an interesting question. The derivative of $e^{x}$ = $e^{x}$, however taking the derivative using limits, and not sums or some other method, is difficult. Perhaps would taking the limit of e, which is $\lim_{n \to \infty} {(1+{1\over n})^n}$ and the $\lim_{h \to 0}  {(e^{x+h} - e^{x})\over h}$, which is the derivative formula. Now, these limits have different terms, n and h respectively, which go to 0 and infinity. In order to get similar terms, an equivalency of the limits could be set up. $\lim_{h \to 0} {1\over h} = \infty$, and $\lim_{n \to \infty} n = \infty$ $\lim_{h \to 0} {1\over h}$ = $\lim_{n \to \infty} n$ and vice versa for $1\over n$ to $h$. Rearranging the equation now gives: $\lim_{n \to \infty} n({(1+{1\over n})^{n(x+{1\over n})} - (1+{1 \over n})^{nx}})$ or $\lim_{n \to \infty} n({(1+{1\over n})^{nx + 1} - (1+{1 \over n})^{nx}})$ Factoring out $(1+{1 \over n})^{nx}$ gives: $\lim_{n \to \infty} n((1 + {1\over n})^{nx})((1 + {1 \over n})^1 - 1)$ or $\lim_{n \to \infty} n((1 + {1\over n})^{nx})(1 + {1 \over n} - 1)$ Removing the 1's gives $\lim_{n \to \infty} n((1 + {1\over n})^{nx})({1 \over n})$ Distributing out the n gives: $\lim_{n \to \infty} (1 + {1\over n})^{nx}$ = $e^x$ This method works for $e^{ax}$ as well, where a is equal to some constant coefficient. For example: $d\over{dx}$ $e^{3x}$ = $3e^{3x}$. I will apply the previous method to $e^{3x}$. Setting up the limit: $\lim_{n \to \infty} {n}({(1+{1\over n})^{3n(x+{1\over n})} - (1+{1 \over n})^{3nx}})$ or $\lim_{n \to \infty} {n}({(1+{1\over n})^{3nx + 3} - (1+{1 \over n})^{3nx}})$ Factoring out $(1+{1 \over n})^{3nx}$ gives: $\lim_{n \to \infty} {n}((1+{1\over n})^{3nx})((1+{1\over n})^3 - 1)$ Multiplying out the $(1+{1 \over n})^3$ gives: $\lim_{n \to \infty} {n}((1+{1\over n})^{3nx})(1+ {3 \over n} + {3 \over n^2} + {1 \over n^3}- 1)$ Subtracting the 1's gives: $\lim_{n \to \infty} {n}((1+{1\over n})^{3nx})({3 \over n} + {3 \over n^2} + {1 \over n^3})$ Distributing the n gives: $\lim_{n \to \infty} ((1+{1\over n})^{3nx})(3 + {3 \over n} + {1 \over n^2})$ As n approaches $\infty$, $3 \over n$ and $1 \over n^2$ approach $0$. With that in mind, and removing the diminishing numbers, gives us: $\lim_{n \to \infty} 3(1+{1\over n})^{3nx}$ = $3e^{3x}$. It seems that using this method, the only significant term in the expanded binomial is the second term. Using binomial theorem, the second term of an expanded binomial is $\binom{a}{1} \over n$ which multiplied by the $n$ just becomes $\binom{a}{1}$. The second term of any combination is just the ${a}$ in $\binom{a}{k}$ where $k$ is some number $0 \le k\le a$. Thus, for all possible values $a$ in $d \over {dx}$ $e^{ax}$, this method will yield the correct derivative, $a e^{ax}$. My question is, since this ""works"", is it even following the rules of mathematics? Nowhere else have I seen this method, I just tried it out one day and saw that it worked. Can limits be exchanged like that? If all these answers are yes, then has someone does this before, who, and when? I feel like this insight isn't something new, and it'll probably look elementary, but my math teachers haven't seen this method before, so who knows? Thanks for looking and responding! --Additional application for $e^{0x}$-- $\lim_{n \to \infty} n({(1+{1\over n})^{n(0+{1\over n})} - (1+{1 \over n})^{0}})$ which equals: $\lim_{n \to \infty} n(1 + {1 \over n} - 1)$ = $1$, which is equal to $e^{0x}$ = $e^0$.","I am a curious Calculus student with an interesting question. The derivative of $e^{x}$ = $e^{x}$, however taking the derivative using limits, and not sums or some other method, is difficult. Perhaps would taking the limit of e, which is $\lim_{n \to \infty} {(1+{1\over n})^n}$ and the $\lim_{h \to 0}  {(e^{x+h} - e^{x})\over h}$, which is the derivative formula. Now, these limits have different terms, n and h respectively, which go to 0 and infinity. In order to get similar terms, an equivalency of the limits could be set up. $\lim_{h \to 0} {1\over h} = \infty$, and $\lim_{n \to \infty} n = \infty$ $\lim_{h \to 0} {1\over h}$ = $\lim_{n \to \infty} n$ and vice versa for $1\over n$ to $h$. Rearranging the equation now gives: $\lim_{n \to \infty} n({(1+{1\over n})^{n(x+{1\over n})} - (1+{1 \over n})^{nx}})$ or $\lim_{n \to \infty} n({(1+{1\over n})^{nx + 1} - (1+{1 \over n})^{nx}})$ Factoring out $(1+{1 \over n})^{nx}$ gives: $\lim_{n \to \infty} n((1 + {1\over n})^{nx})((1 + {1 \over n})^1 - 1)$ or $\lim_{n \to \infty} n((1 + {1\over n})^{nx})(1 + {1 \over n} - 1)$ Removing the 1's gives $\lim_{n \to \infty} n((1 + {1\over n})^{nx})({1 \over n})$ Distributing out the n gives: $\lim_{n \to \infty} (1 + {1\over n})^{nx}$ = $e^x$ This method works for $e^{ax}$ as well, where a is equal to some constant coefficient. For example: $d\over{dx}$ $e^{3x}$ = $3e^{3x}$. I will apply the previous method to $e^{3x}$. Setting up the limit: $\lim_{n \to \infty} {n}({(1+{1\over n})^{3n(x+{1\over n})} - (1+{1 \over n})^{3nx}})$ or $\lim_{n \to \infty} {n}({(1+{1\over n})^{3nx + 3} - (1+{1 \over n})^{3nx}})$ Factoring out $(1+{1 \over n})^{3nx}$ gives: $\lim_{n \to \infty} {n}((1+{1\over n})^{3nx})((1+{1\over n})^3 - 1)$ Multiplying out the $(1+{1 \over n})^3$ gives: $\lim_{n \to \infty} {n}((1+{1\over n})^{3nx})(1+ {3 \over n} + {3 \over n^2} + {1 \over n^3}- 1)$ Subtracting the 1's gives: $\lim_{n \to \infty} {n}((1+{1\over n})^{3nx})({3 \over n} + {3 \over n^2} + {1 \over n^3})$ Distributing the n gives: $\lim_{n \to \infty} ((1+{1\over n})^{3nx})(3 + {3 \over n} + {1 \over n^2})$ As n approaches $\infty$, $3 \over n$ and $1 \over n^2$ approach $0$. With that in mind, and removing the diminishing numbers, gives us: $\lim_{n \to \infty} 3(1+{1\over n})^{3nx}$ = $3e^{3x}$. It seems that using this method, the only significant term in the expanded binomial is the second term. Using binomial theorem, the second term of an expanded binomial is $\binom{a}{1} \over n$ which multiplied by the $n$ just becomes $\binom{a}{1}$. The second term of any combination is just the ${a}$ in $\binom{a}{k}$ where $k$ is some number $0 \le k\le a$. Thus, for all possible values $a$ in $d \over {dx}$ $e^{ax}$, this method will yield the correct derivative, $a e^{ax}$. My question is, since this ""works"", is it even following the rules of mathematics? Nowhere else have I seen this method, I just tried it out one day and saw that it worked. Can limits be exchanged like that? If all these answers are yes, then has someone does this before, who, and when? I feel like this insight isn't something new, and it'll probably look elementary, but my math teachers haven't seen this method before, so who knows? Thanks for looking and responding! --Additional application for $e^{0x}$-- $\lim_{n \to \infty} n({(1+{1\over n})^{n(0+{1\over n})} - (1+{1 \over n})^{0}})$ which equals: $\lim_{n \to \infty} n(1 + {1 \over n} - 1)$ = $1$, which is equal to $e^{0x}$ = $e^0$.",,"['derivatives', 'binomial-theorem']"
42,Standard parametric equation of a parabola,Standard parametric equation of a parabola,,We've been shown in class the standard parametric form of the parabola $$x(t)=2at$$ $$y(t)=at^2$$ My question is: why is $x(t)$ the derivative of $y(t)$? Is it just a coincidence?,We've been shown in class the standard parametric form of the parabola $$x(t)=2at$$ $$y(t)=at^2$$ My question is: why is $x(t)$ the derivative of $y(t)$? Is it just a coincidence?,,"['derivatives', 'conic-sections', 'parametric']"
43,A few questions on Taylor's inequality.,A few questions on Taylor's inequality.,,"Taylor's Inequality. If $|f^{n + 1}(x)| \le M$ for $|x - a|\le d$, then the remainder $R_n(x)$ of the Taylor series satisfies the inequality$$|R_n(x)| \le {M\over{(n + 1)!}}|x - a|^{n + 1} \text{ for }|x - a| \le d.$$ I have a few questions surrounding Taylor's inequality. What is the intuition behind the proof? Why should I care about it, i.e. how does one apply it?","Taylor's Inequality. If $|f^{n + 1}(x)| \le M$ for $|x - a|\le d$, then the remainder $R_n(x)$ of the Taylor series satisfies the inequality$$|R_n(x)| \le {M\over{(n + 1)!}}|x - a|^{n + 1} \text{ for }|x - a| \le d.$$ I have a few questions surrounding Taylor's inequality. What is the intuition behind the proof? Why should I care about it, i.e. how does one apply it?",,"['calculus', 'real-analysis']"
44,Combining multiple multivariate integrals into a single function,Combining multiple multivariate integrals into a single function,,"Let $p$ be a multivariate polynomial (with property that for any $x\in[0, 1]^n$, $p(x) \in [0,1]$). If I want to find a $g$ function such that $\frac{\partial g}{\partial x_1} = p(x)$ then clearly $g=\int p(x) dx_1$. Q1: Suppose, instead I have two direction vectors $u$ and $v$, and I want $(v \cdot \nabla g(x)) = p(x)$ and $(u \cdot \nabla g(x)) = p(x)$, then what should g be? I can do this for one directional derivative, but not sure how to combine for multiple directional derivatives. Q2: Can I go one step further, where given $q:[0,1]^n \rightarrow [0, 1]^n$, define function $g$ such that $((q(x) - x)\cdot \nabla g(x))=p(x)$? Here each $q_i$ is a polynomial again.","Let $p$ be a multivariate polynomial (with property that for any $x\in[0, 1]^n$, $p(x) \in [0,1]$). If I want to find a $g$ function such that $\frac{\partial g}{\partial x_1} = p(x)$ then clearly $g=\int p(x) dx_1$. Q1: Suppose, instead I have two direction vectors $u$ and $v$, and I want $(v \cdot \nabla g(x)) = p(x)$ and $(u \cdot \nabla g(x)) = p(x)$, then what should g be? I can do this for one directional derivative, but not sure how to combine for multiple directional derivatives. Q2: Can I go one step further, where given $q:[0,1]^n \rightarrow [0, 1]^n$, define function $g$ such that $((q(x) - x)\cdot \nabla g(x))=p(x)$? Here each $q_i$ is a polynomial again.",,"['integration', 'derivatives', 'polynomials']"
45,$x(a^{1/x}-1)$ is decreasing,is decreasing,x(a^{1/x}-1),Prove that $f(x)=x(a^{1/x}-1)$ is decreasing on the positive $x$ axis for $a\geq 0$. My Try: I wanted to prove the first derivative is negative. $\displaystyle f'(x)=-\frac{1}{x}a^{1/x}\ln a+a^{1/x}-1$. But it was very difficult to show this is negative. Any suggestion please.,Prove that $f(x)=x(a^{1/x}-1)$ is decreasing on the positive $x$ axis for $a\geq 0$. My Try: I wanted to prove the first derivative is negative. $\displaystyle f'(x)=-\frac{1}{x}a^{1/x}\ln a+a^{1/x}-1$. But it was very difficult to show this is negative. Any suggestion please.,,"['calculus', 'derivatives']"
46,"Why $\frac{d}{dt}f(x+t(y−x))<0$ if $x < y, f(y) < f(x)$",Why  if,"\frac{d}{dt}f(x+t(y−x))<0 x < y, f(y) < f(x)","Here excerpt from a book: Аssume that $f$ satisfies $\nabla f(x) \ge 0$ for all $x$ , but is not nondecreasing, i.e., there exist $x,y$ with $x < y$ and $f(y) < f(x)$ . By differentiability of $f$ there exists at $t\in[0,1]$ with $$\frac{d}{dt}f\left(x+t(y−x)\right) =\nabla f\left(x+t\left(y−x\right)\right)^T(y−x)<0.$$ I don't understand why the derivative is less than zero? The function could look like on the image. The book is Convex Optimization by Stephen Boyd and Lieven Vandenberghe, page 109.","Here excerpt from a book: Аssume that satisfies for all , but is not nondecreasing, i.e., there exist with and . By differentiability of there exists at with I don't understand why the derivative is less than zero? The function could look like on the image. The book is Convex Optimization by Stephen Boyd and Lieven Vandenberghe, page 109.","f \nabla f(x) \ge 0 x x,y x < y f(y) < f(x) f t\in[0,1] \frac{d}{dt}f\left(x+t(y−x)\right) =\nabla f\left(x+t\left(y−x\right)\right)^T(y−x)<0.",['derivatives']
47,"Derivate a function defined by an integral, whose variable are the integration limits","Derivate a function defined by an integral, whose variable are the integration limits",,"I have to find the derivative of the following one-variable function and evalue it for $t=0$: $$g(t)=\int_t^{t^2} \cos(tx)dx$$ In class, we saw a formula that says that a function such as $$F(t)=\int_{a(t)}^{b(t)}f(x,t)dx$$ has for a derivative $$F'(t)=f(b(t),t) \cdot b'(t)-f(a(t),t) \cdot a'(t)+\int_{a(t)}^{b(t)}\frac{\partial f(x,t)}{\partial t}dx$$ I succeeded applying the formula but the last term - the integral term - gives $$ t\cos(t^3) - \cos(t^2)+\frac{\sin(t^3)-\sin(t^2)}{t^2}$$ The thing is I cannot evalue it for $t=0$ because of the division by $t^2$, though the answer to the question is $f'(0)=-1$. How do I get that result?","I have to find the derivative of the following one-variable function and evalue it for $t=0$: $$g(t)=\int_t^{t^2} \cos(tx)dx$$ In class, we saw a formula that says that a function such as $$F(t)=\int_{a(t)}^{b(t)}f(x,t)dx$$ has for a derivative $$F'(t)=f(b(t),t) \cdot b'(t)-f(a(t),t) \cdot a'(t)+\int_{a(t)}^{b(t)}\frac{\partial f(x,t)}{\partial t}dx$$ I succeeded applying the formula but the last term - the integral term - gives $$ t\cos(t^3) - \cos(t^2)+\frac{\sin(t^3)-\sin(t^2)}{t^2}$$ The thing is I cannot evalue it for $t=0$ because of the division by $t^2$, though the answer to the question is $f'(0)=-1$. How do I get that result?",,"['derivatives', 'definite-integrals']"
48,Derivative of rotating a time changing vector by a time changing quaternion,Derivative of rotating a time changing vector by a time changing quaternion,,"I have a quaternion $q(t)$ that is a function of $t$ and a vector $v(t)$ that is a function of $t$ and I rotate the vector by the quaternion: $f(t) = q(t) v(t) q^*(t)$ but now I want to find the derivative (and second derivative) of $f(t)$. Specifically, $v(t)$ is a polynomial with vector coefficients and $q(t)$ I have in the form of an initial quaternion $q_0$ with angular velocity and angular acceleration in axis magnitude form. I see I may be able to get the derivative of $q(t)$ by this formula $dq/dt = 1/2 w q$ (where w appears to be in the form I have it, thought its not clear). If that works, is it then a matter of applying the derivative product rule to $q v q^*$ to get $q (v {q^*}' + v' q^*) + q' (v q^*)$? If so, would the vectors be pure quaternions and addition would be elementwise and all the multiplications Hamilton ?","I have a quaternion $q(t)$ that is a function of $t$ and a vector $v(t)$ that is a function of $t$ and I rotate the vector by the quaternion: $f(t) = q(t) v(t) q^*(t)$ but now I want to find the derivative (and second derivative) of $f(t)$. Specifically, $v(t)$ is a polynomial with vector coefficients and $q(t)$ I have in the form of an initial quaternion $q_0$ with angular velocity and angular acceleration in axis magnitude form. I see I may be able to get the derivative of $q(t)$ by this formula $dq/dt = 1/2 w q$ (where w appears to be in the form I have it, thought its not clear). If that works, is it then a matter of applying the derivative product rule to $q v q^*$ to get $q (v {q^*}' + v' q^*) + q' (v q^*)$? If so, would the vectors be pure quaternions and addition would be elementwise and all the multiplications Hamilton ?",,"['derivatives', 'quaternions']"
49,When is $\frac{dx}{dt}=\frac{\Delta x}{\Delta t}$ a valid approximation?,When is  a valid approximation?,\frac{dx}{dt}=\frac{\Delta x}{\Delta t},It is often said that when the change in e.g. $\Delta x$ is small than we can make the approximation: $$\frac{dx}{dt}=\frac{\Delta x}{\Delta t}$$ But it is not enough to say $\Delta x$ is small because we can define units to make it large. So we need $\Delta x$ to be small compared to something else. What is this something else?,It is often said that when the change in e.g. $\Delta x$ is small than we can make the approximation: $$\frac{dx}{dt}=\frac{\Delta x}{\Delta t}$$ But it is not enough to say $\Delta x$ is small because we can define units to make it large. So we need $\Delta x$ to be small compared to something else. What is this something else?,,"['derivatives', 'approximation']"
50,$f(x)=\int_0^x\sin(t^2-t+x)dt$. Find $f''(x)+f'(x)$,. Find,f(x)=\int_0^x\sin(t^2-t+x)dt f''(x)+f'(x),"$f(x)=\int_0^x\sin(t^2-t+x)dt$. Find $f''(x)+f'(x)$. Using leibnitz integral rule, $$f'(x)=\int_0^x\cos(t^2-t+x)+\sin(x^2)dt$$ $$f''(x)=-\int_0^x\sin(t^2-t+x)dt+2x\cos(x^2)$$ Answer given is $(2x+1)\cos(x^2)$. I wont get this if I add $f''(x)+f'(x)$","$f(x)=\int_0^x\sin(t^2-t+x)dt$. Find $f''(x)+f'(x)$. Using leibnitz integral rule, $$f'(x)=\int_0^x\cos(t^2-t+x)+\sin(x^2)dt$$ $$f''(x)=-\int_0^x\sin(t^2-t+x)dt+2x\cos(x^2)$$ Answer given is $(2x+1)\cos(x^2)$. I wont get this if I add $f''(x)+f'(x)$",,"['calculus', 'integration', 'derivatives']"
51,Partial Derivative with product & chain rule,Partial Derivative with product & chain rule,,"I cannot for the life of me work out the answer to this partial derivative. $$\frac{\delta}{\delta x}\left(\frac{x^2}{(x+y)^2(x+z)^2}\right) $$ My first thought was: Split into two equations: $$\frac{\delta}{\delta x}\left(\frac{x}{(x+y)^2}\cdot \frac{x}{(x+z)^2}\right)$$ Apply chain rule to each side which gives me: $$\frac{1}{(x+y)^2}-\frac{2x}{(x+y)^3}$$ and $$ \frac{1}{(x+z)^2}-\frac{2x}{(x+z)^3} $$ I then try to use the product rule, so: $$ \left\lbrack\left(\frac{1}{(x+y)^2}-\frac{2x}{(x+y)^3}\right)\cdot \frac{x}{(x+z)^2}\right\rbrack + \left\lbrack\left(\frac{1}{(x+z)^2}-\frac{2x}{(x+z)^3}\right)\cdot \frac{x}{(x+y)^2}\right\rbrack $$ However I don't end up anywhere near an answer, let alone the right answer. Apparently the answer is: $$ -\frac{2(x^3-xyz)}{(x+y)^3(x+z)^3} $$ Is there an easier way to do this?","I cannot for the life of me work out the answer to this partial derivative. $$\frac{\delta}{\delta x}\left(\frac{x^2}{(x+y)^2(x+z)^2}\right) $$ My first thought was: Split into two equations: $$\frac{\delta}{\delta x}\left(\frac{x}{(x+y)^2}\cdot \frac{x}{(x+z)^2}\right)$$ Apply chain rule to each side which gives me: $$\frac{1}{(x+y)^2}-\frac{2x}{(x+y)^3}$$ and $$ \frac{1}{(x+z)^2}-\frac{2x}{(x+z)^3} $$ I then try to use the product rule, so: $$ \left\lbrack\left(\frac{1}{(x+y)^2}-\frac{2x}{(x+y)^3}\right)\cdot \frac{x}{(x+z)^2}\right\rbrack + \left\lbrack\left(\frac{1}{(x+z)^2}-\frac{2x}{(x+z)^3}\right)\cdot \frac{x}{(x+y)^2}\right\rbrack $$ However I don't end up anywhere near an answer, let alone the right answer. Apparently the answer is: $$ -\frac{2(x^3-xyz)}{(x+y)^3(x+z)^3} $$ Is there an easier way to do this?",,"['derivatives', 'partial-derivative']"
52,How to find the slope of curves at origin if the derivative becomes indeterminate,How to find the slope of curves at origin if the derivative becomes indeterminate,,"What's the general method to find the slope of a curve at the origin if the derivative at the origin becomes indeterminate. For Eg-- What is the slope of the curve $x^3 + y^3= 3axy$ at origin and how to find it because after following the process of implicit differentiation and plugging in $x=0$ and $y=0$ in the derivative we get $0/0$ . Actually this question has been asked by me before and a sort of satisfactory answer that I got was "" For small $x$ and $y$ , the values of $x^3$ and $y^3$ will be much smaller than $3axy$ , so the zeroes of the function will be approximately where the zeroes of $0=3axy$ are -- that is, near the origin the curve will look like the solutions to that, which is just the two coordinate axes. So the curve will cross itself at the origin, passing through the origin once horizontally and once vertically. (This is also why implicit differentiation can't work at the origin -- the solution set simply doesn't look like a straight line there under any magnification)."" Edit If I approximate the function by saying that at (0,0) , the behavior is dominated be 3axy term as x^3 and y^3 are very small and then 3axy=0 and then tangents are x=0 and y=0 . Is doing so (saying x=0 and y=0) linear Approximation only. Because I am approximating the curve with a straight line at origin . But linear Approximation is  1st derivative (1st term of Taylor series) . This cannot be right because Taylor series can't be formed where derivative doesn't exist*. And if this is right then the function is approximately given by 3axy=0 at (0,0). But how does this give the tangent at (0,0).How shall I go about ? Edit: Is the answer give  right because the solpe does exist.","What's the general method to find the slope of a curve at the origin if the derivative at the origin becomes indeterminate. For Eg-- What is the slope of the curve at origin and how to find it because after following the process of implicit differentiation and plugging in and in the derivative we get . Actually this question has been asked by me before and a sort of satisfactory answer that I got was "" For small and , the values of and will be much smaller than , so the zeroes of the function will be approximately where the zeroes of are -- that is, near the origin the curve will look like the solutions to that, which is just the two coordinate axes. So the curve will cross itself at the origin, passing through the origin once horizontally and once vertically. (This is also why implicit differentiation can't work at the origin -- the solution set simply doesn't look like a straight line there under any magnification)."" Edit If I approximate the function by saying that at (0,0) , the behavior is dominated be 3axy term as x^3 and y^3 are very small and then 3axy=0 and then tangents are x=0 and y=0 . Is doing so (saying x=0 and y=0) linear Approximation only. Because I am approximating the curve with a straight line at origin . But linear Approximation is  1st derivative (1st term of Taylor series) . This cannot be right because Taylor series can't be formed where derivative doesn't exist*. And if this is right then the function is approximately given by 3axy=0 at (0,0). But how does this give the tangent at (0,0).How shall I go about ? Edit: Is the answer give  right because the solpe does exist.",x^3 + y^3= 3axy x=0 y=0 0/0 x y x^3 y^3 3axy 0=3axy,"['derivatives', 'implicit-differentiation', 'tangent-line', 'slope']"
53,Find the derivative of $F(x) = \int_{a}^b \frac{x}{1+t^2+\sin^2{t}}dt$,Find the derivative of,F(x) = \int_{a}^b \frac{x}{1+t^2+\sin^2{t}}dt,Find the derivative of $$F(x) = \int_{a}^b \dfrac{x}{1+t^2+\sin^2{t}}dt.$$ Attempt: We use the product rule since $\displaystyle \int_{a}^b \dfrac{x}{1+t^2+\sin^2{t}}dt = x  \int_{a}^b \dfrac{1}{1+t^2+\sin^2{t}}dt$ to get that $$F'(x) = \int_{a}^b \dfrac{1}{1+t^2+\sin^2{t}}dt.$$,Find the derivative of $$F(x) = \int_{a}^b \dfrac{x}{1+t^2+\sin^2{t}}dt.$$ Attempt: We use the product rule since $\displaystyle \int_{a}^b \dfrac{x}{1+t^2+\sin^2{t}}dt = x  \int_{a}^b \dfrac{1}{1+t^2+\sin^2{t}}dt$ to get that $$F'(x) = \int_{a}^b \dfrac{1}{1+t^2+\sin^2{t}}dt.$$,,"['calculus', 'derivatives']"
54,Existence of Derivative for an Integral,Existence of Derivative for an Integral,,"Let $f$ be a Riemann integrable function defined on $[-2,2]$. Define a function $F \colon (-1,1) \to \mathbb{R}$ by $$F(h)=\int_0^1 h | f(x+h)-f(x)|\, dx.$$ Show that the derivative $F'(0)$ exists. I started form $\lim_{h\to 0}$ $\frac{F(h)-F(0)}{h}$ By definition, $F(0)=0$ since it $F(0)$ becomes a definite integral of zero. $\lim_{h\to 0}$ $\frac{F(h)-F(0)}{h} = \lim_{h\to 0}$ $\frac{F(h)}{h} = $$\lim_{h\to 0}\int_0^1 |f(x+h)-f(x)| dx$. Them I have no clue to continue, can anyone give me some hints?","Let $f$ be a Riemann integrable function defined on $[-2,2]$. Define a function $F \colon (-1,1) \to \mathbb{R}$ by $$F(h)=\int_0^1 h | f(x+h)-f(x)|\, dx.$$ Show that the derivative $F'(0)$ exists. I started form $\lim_{h\to 0}$ $\frac{F(h)-F(0)}{h}$ By definition, $F(0)=0$ since it $F(0)$ becomes a definite integral of zero. $\lim_{h\to 0}$ $\frac{F(h)-F(0)}{h} = \lim_{h\to 0}$ $\frac{F(h)}{h} = $$\lim_{h\to 0}\int_0^1 |f(x+h)-f(x)| dx$. Them I have no clue to continue, can anyone give me some hints?",,"['derivatives', 'riemann-integration']"
55,Cannot make sense of a derivative,Cannot make sense of a derivative,,"Short version of the question: In this presentation http://www.slideshare.net/ShangxuanZhang/xgboost (page 74-75)I cannot understand how the gradient of the L function is calculated. $$ L = y_i  \log  {{1}\over{1+e^{-\hat{y}_i}}}+(1-y_i)\log {{e^{-\hat{y}_i}}\over{1+e^{-\hat{y}_i}}} $$ On page 75 it reports that $$ \textrm{grad} = {{1}\over{1+e^{-\hat{y}_i}}}-y_i $$ But I have not been ale to reproduce the passages to get to that result. Please note that on page 73 is specified that the gradient is intended with respect to $ y_i $ , but I suspect that it should be calculated with respect to $ \hat{y}_i $ , because first it makes sense in the context, second in the whole presentation and in the paper that presents this method, gradients are always calculated with respect to $ \hat{y}_i $ . Either way I cannot understand how he obtain that result. Long version of the question: I am trying to use the xgboost R package for a little side project. One of the reasons that have lead me to try it out is the fact that it lets you define your own  customized objective function. Problem is that the example I reported above is pretty much the most detailed documentation that I were able to find. In the official documentation ( https://cran.r-project.org/web/packages/xgboost/xgboost.pdf , page 9)uses the same example and explains that you should define a function that returns gradient and second order gradient and just input it as a parameter of the training method. Making such a function wouldn't be a problem if only I were sure about what ""gradient and second order gradient"" means, and I am not sure since I cannot understand the only example I have, which is supposed to be quite simple, so if somone can explain me what calculations he permorms that would be much appreciated.","Short version of the question: In this presentation http://www.slideshare.net/ShangxuanZhang/xgboost (page 74-75)I cannot understand how the gradient of the L function is calculated. On page 75 it reports that But I have not been ale to reproduce the passages to get to that result. Please note that on page 73 is specified that the gradient is intended with respect to , but I suspect that it should be calculated with respect to , because first it makes sense in the context, second in the whole presentation and in the paper that presents this method, gradients are always calculated with respect to . Either way I cannot understand how he obtain that result. Long version of the question: I am trying to use the xgboost R package for a little side project. One of the reasons that have lead me to try it out is the fact that it lets you define your own  customized objective function. Problem is that the example I reported above is pretty much the most detailed documentation that I were able to find. In the official documentation ( https://cran.r-project.org/web/packages/xgboost/xgboost.pdf , page 9)uses the same example and explains that you should define a function that returns gradient and second order gradient and just input it as a parameter of the training method. Making such a function wouldn't be a problem if only I were sure about what ""gradient and second order gradient"" means, and I am not sure since I cannot understand the only example I have, which is supposed to be quite simple, so if somone can explain me what calculations he permorms that would be much appreciated.","
L = y_i  \log  {{1}\over{1+e^{-\hat{y}_i}}}+(1-y_i)\log {{e^{-\hat{y}_i}}\over{1+e^{-\hat{y}_i}}}
 
\textrm{grad} = {{1}\over{1+e^{-\hat{y}_i}}}-y_i
  y_i   \hat{y}_i   \hat{y}_i ","['calculus', 'derivatives']"
56,Suppose $f(0) = f(1) = 0$ and $f(x_0) = 1$. Show that there is $\rho$ with $\lvert f'(\rho) \rvert \geq 2$.,Suppose  and . Show that there is  with .,f(0) = f(1) = 0 f(x_0) = 1 \rho \lvert f'(\rho) \rvert \geq 2,"Suppose that $f : [0; 1] \rightarrow \mathbb{R}$ is continous and differentiable on $(0,1)$, that $f(0) = f(1) = 0$, and that $\exists_{x_0 \in (0; 1)} f(x_0) = 1$. Prove that $\exists_{\rho \in (0;1)}|f'(\rho)| > 2$. Any hints how to do that?","Suppose that $f : [0; 1] \rightarrow \mathbb{R}$ is continous and differentiable on $(0,1)$, that $f(0) = f(1) = 0$, and that $\exists_{x_0 \in (0; 1)} f(x_0) = 1$. Prove that $\exists_{\rho \in (0;1)}|f'(\rho)| > 2$. Any hints how to do that?",,"['calculus', 'derivatives']"
57,Lie derivative of a vectorfield in components,Lie derivative of a vectorfield in components,,"The lecturer here wants the viewer to derive the components of the Lie derivative of a (1,1) tensor-field. But even before that I have a little question about the components of the Lie derivative of a vector field: Careful: 1) and 2) are incorrect! let $(U,x)$ be a chart and $X,Y$ vector fields on the smooth manifold $(M,\mathcal{O},\mathcal{A})$, I get: $$ ({L}_X Y)^i = [X,Y]^i = (XY - YX)^i = X^m  \left(\frac{\partial}{\partial x^m}\right) Y^i - Y^m \left(\frac{\partial}{\partial x^m}\right) X^i $$ 1) is that correct? I suspect, since we write single components, which are real functions, that I can reorder the terms, as I please (commutativity of multiplication on $C^{\infty}M$)? And the derivatives, which are actually the basis vectors, act on the function to which this thing is applied to anyway, right? So for clarity I could move them to the far right to show this: $$ = X^m  Y^i  \left(\frac{\partial}{\partial x^m}\right) - Y^m  X^i  \left(\frac{\partial}{\partial x^m}\right) $$ 2) still correct? 3) Then is there any ""rule""/intuition or something why the contraction with the basis is over the ""outer"" field? If I take $(y \circ x^{-1})^i$ I can write this as $(y^i \circ x^{-1})$, basically the last function is responsible for the component I get out. In the above example it seems to be the first one applied.","The lecturer here wants the viewer to derive the components of the Lie derivative of a (1,1) tensor-field. But even before that I have a little question about the components of the Lie derivative of a vector field: Careful: 1) and 2) are incorrect! let $(U,x)$ be a chart and $X,Y$ vector fields on the smooth manifold $(M,\mathcal{O},\mathcal{A})$, I get: $$ ({L}_X Y)^i = [X,Y]^i = (XY - YX)^i = X^m  \left(\frac{\partial}{\partial x^m}\right) Y^i - Y^m \left(\frac{\partial}{\partial x^m}\right) X^i $$ 1) is that correct? I suspect, since we write single components, which are real functions, that I can reorder the terms, as I please (commutativity of multiplication on $C^{\infty}M$)? And the derivatives, which are actually the basis vectors, act on the function to which this thing is applied to anyway, right? So for clarity I could move them to the far right to show this: $$ = X^m  Y^i  \left(\frac{\partial}{\partial x^m}\right) - Y^m  X^i  \left(\frac{\partial}{\partial x^m}\right) $$ 2) still correct? 3) Then is there any ""rule""/intuition or something why the contraction with the basis is over the ""outer"" field? If I take $(y \circ x^{-1})^i$ I can write this as $(y^i \circ x^{-1})$, basically the last function is responsible for the component I get out. In the above example it seems to be the first one applied.",,"['differential-geometry', 'derivatives', 'lie-derivative']"
58,Correct typography for using Leibniz Notation,Correct typography for using Leibniz Notation,,"What is the correct way of writing derivatives in notation form? Should the 'd' be upright or italic, that is $\mathrm d$ or $d\,$? As an example, should we write: $$ \frac {\mathrm dy}{\mathrm dx}\qquad\text{or}\qquad \frac {dy}{dx} \, \text ?$$ I feel the first method (upright 'd') is correct, since italic letters are generally used for variables, but most people (and Wikipedia ) use the italic 'd'. Which is actually more correct? Also, how do we use this notation on notebooks (writing by hand) , while avoiding potential confusion in a variable $d$ and the differential symbol? Since mostly we write variables upright on notebooks, so $\displaystyle \mathrm{\frac{dd}{dt}}$ in a notebook would be potentially confusing. (Of course, we could avoid a variable 'd', but still, what would be another way out of this?)","What is the correct way of writing derivatives in notation form? Should the 'd' be upright or italic, that is $\mathrm d$ or $d\,$? As an example, should we write: $$ \frac {\mathrm dy}{\mathrm dx}\qquad\text{or}\qquad \frac {dy}{dx} \, \text ?$$ I feel the first method (upright 'd') is correct, since italic letters are generally used for variables, but most people (and Wikipedia ) use the italic 'd'. Which is actually more correct? Also, how do we use this notation on notebooks (writing by hand) , while avoiding potential confusion in a variable $d$ and the differential symbol? Since mostly we write variables upright on notebooks, so $\displaystyle \mathrm{\frac{dd}{dt}}$ in a notebook would be potentially confusing. (Of course, we could avoid a variable 'd', but still, what would be another way out of this?)",,"['calculus', 'derivatives', 'notation']"
59,Functional equations with nowhere differentiable solutions,Functional equations with nowhere differentiable solutions,,"As an example, the functional equation $f(x+y)=f(x)f(y)$, by declaring that $f$ is continuous and differentiable, we can arrive at the unique solution $f(x)=a^x$, by first showing that $f'(x)=f(0)f(x)$ and then using basic ODE theory to declare such an $f$ exists. I'm interested in an example of a functional equation whose only solution is a continuous nowhere differentiable function. While some gymnastics with trigonometry might arrive at a functional equation for something like the Weierstrass functions , uniqueness might be hard to prove. Are there any good ""easy"" examples of such functional equations? I'd strongly prefer something that uses basic operations and classical functions and not say, derivatives in the sense of formal power series.","As an example, the functional equation $f(x+y)=f(x)f(y)$, by declaring that $f$ is continuous and differentiable, we can arrive at the unique solution $f(x)=a^x$, by first showing that $f'(x)=f(0)f(x)$ and then using basic ODE theory to declare such an $f$ exists. I'm interested in an example of a functional equation whose only solution is a continuous nowhere differentiable function. While some gymnastics with trigonometry might arrive at a functional equation for something like the Weierstrass functions , uniqueness might be hard to prove. Are there any good ""easy"" examples of such functional equations? I'd strongly prefer something that uses basic operations and classical functions and not say, derivatives in the sense of formal power series.",,"['derivatives', 'functional-equations']"
60,Differentiable function under specific topological constraints,Differentiable function under specific topological constraints,,"Can you give an example of a set$\:\:\emptyset\neq \mathcal D\large⊂$$\:\mathbb{\Re}\:$ and a differentiable function $\:f$ : $\mathcal D → \mathbb{\Re}\:\:$ such that $$\mathcal D ⊂ \text{Acc}(\mathcal D),\:\frac{\text d}{\text dx}f(x) = 0\:\:\forall x∈\left(\mathcal D \land f\right)\neq  c \in\:\mathbb{\Re}$$. This was a bonus question on my exam and I'd appreciate any given tips on how to find such an example. Thanks!","Can you give an example of a set$\:\:\emptyset\neq \mathcal D\large⊂$$\:\mathbb{\Re}\:$ and a differentiable function $\:f$ : $\mathcal D → \mathbb{\Re}\:\:$ such that $$\mathcal D ⊂ \text{Acc}(\mathcal D),\:\frac{\text d}{\text dx}f(x) = 0\:\:\forall x∈\left(\mathcal D \land f\right)\neq  c \in\:\mathbb{\Re}$$. This was a bonus question on my exam and I'd appreciate any given tips on how to find such an example. Thanks!",,"['real-analysis', 'derivatives', 'continuity', 'differential-topology']"
61,differentiation problem,differentiation problem,,"If $x^{13}y^{7}=(x+y)^{20}$ , then $\frac{dy}{dx}$ directly doing it makes it very complicated so, I did this $\left(\frac{x}{y}\right)^{13}=\left(1+\frac{x}{y} \right)^{20}$.  following are the options for solution (a) $\frac{y^2}{x^2}$ (b)$\frac{x^2}{y^2}$ (c)$\frac{x}{y}$ (d)$\frac{y}{x}$ thanks for any hints.","If $x^{13}y^{7}=(x+y)^{20}$ , then $\frac{dy}{dx}$ directly doing it makes it very complicated so, I did this $\left(\frac{x}{y}\right)^{13}=\left(1+\frac{x}{y} \right)^{20}$.  following are the options for solution (a) $\frac{y^2}{x^2}$ (b)$\frac{x^2}{y^2}$ (c)$\frac{x}{y}$ (d)$\frac{y}{x}$ thanks for any hints.",,['derivatives']
62,Finding function for capital interest,Finding function for capital interest,,"Haven't fully grasped derivatives and I believe this question really holds the gist of it Your bank account has a continuous capital interest rate of 7%. The formula for this is $$\frac{dB}{dt} = 0.07B$$ where B stands for balance. I'm probably reading this wrong, but here's what I read: The change in your balance over time is 0.07B Shouldn't there be a $t$ factor on the RHS? i.e. $$0.07Bt$$","Haven't fully grasped derivatives and I believe this question really holds the gist of it Your bank account has a continuous capital interest rate of 7%. The formula for this is $$\frac{dB}{dt} = 0.07B$$ where B stands for balance. I'm probably reading this wrong, but here's what I read: The change in your balance over time is 0.07B Shouldn't there be a $t$ factor on the RHS? i.e. $$0.07Bt$$",,"['calculus', 'integration', 'derivatives']"
63,How to compute derivatives of functions with vectors inside?,How to compute derivatives of functions with vectors inside?,,"Suppose $\vec{w}=\frac{g}{||\vec{v}||} \vec{v}$, what is the derivative of $\vec{w}$ w.r.t. $\vec{v}$? Don't know how to deal with the norm of $\vec{v}$ here... Thanks in advance. :-) Edit: $L$ is a function of $\vec{w}$ and $g$. Based on $\vec{w}=\frac{g}{||\vec{v}||} \vec{v}$, we have $$\nabla{g}{L}=\frac{\nabla{\vec{w}}{L} \cdot \vec{v}}{||\vec{v}||}$$ $$\nabla{\vec{v}}{L}=\frac{g}{||\vec{v}||}\nabla{\vec{w}}{L}-\frac{g\nabla{g}{L}}{||\vec{v}||^2}\vec{v}$$ Could you show how to get exactly the second equation? It seems a bit weird to me.","Suppose $\vec{w}=\frac{g}{||\vec{v}||} \vec{v}$, what is the derivative of $\vec{w}$ w.r.t. $\vec{v}$? Don't know how to deal with the norm of $\vec{v}$ here... Thanks in advance. :-) Edit: $L$ is a function of $\vec{w}$ and $g$. Based on $\vec{w}=\frac{g}{||\vec{v}||} \vec{v}$, we have $$\nabla{g}{L}=\frac{\nabla{\vec{w}}{L} \cdot \vec{v}}{||\vec{v}||}$$ $$\nabla{\vec{v}}{L}=\frac{g}{||\vec{v}||}\nabla{\vec{w}}{L}-\frac{g\nabla{g}{L}}{||\vec{v}||^2}\vec{v}$$ Could you show how to get exactly the second equation? It seems a bit weird to me.",,"['derivatives', 'partial-derivative']"
64,A question on the assumptions in the theorem for changing limit and derivative for sequence of functions,A question on the assumptions in the theorem for changing limit and derivative for sequence of functions,,"I am looking for an example for a sequence of differentiable  functions $\{f_n\}$ on a closed bounded interval $[a,b]$ such that $\{f_n\}$ converges uniformly to a differentiable function $f$ , $\{f_n'\}$ converges point-wise to a function $g$ on $[a,b]$ such that $f'=g$ but $\{f_n'\}$ does not converge uniformly on $[a,b]$ . Please help . Thanks in advance","I am looking for an example for a sequence of differentiable  functions $\{f_n\}$ on a closed bounded interval $[a,b]$ such that $\{f_n\}$ converges uniformly to a differentiable function $f$ , $\{f_n'\}$ converges point-wise to a function $g$ on $[a,b]$ such that $f'=g$ but $\{f_n'\}$ does not converge uniformly on $[a,b]$ . Please help . Thanks in advance",,"['real-analysis', 'sequences-and-series']"
65,How is the second derivitive derived? [closed],How is the second derivitive derived? [closed],,Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 8 years ago . Improve this question As everyone knows that the derivitive of a function is notated as $\frac{dy}{dx}$ The question is: How is the second derivitive $\left(\frac{d^2y}{dx^2}\right)$ notation derived?,Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 8 years ago . Improve this question As everyone knows that the derivitive of a function is notated as $\frac{dy}{dx}$ The question is: How is the second derivitive $\left(\frac{d^2y}{dx^2}\right)$ notation derived?,,"['calculus', 'derivatives']"
66,A particle moves along the x-axis find t when acceleration of the particle equals 0,A particle moves along the x-axis find t when acceleration of the particle equals 0,,"A particle moves along the x-axis, its position at time t is given by $x(t)= \frac{3t}{6+8t^2}$, $t≥0$, where t is measured in seconds and x is in meters. Find time at which acceleration equals 0. I got the answer 0.5 and 0 but apparently i got the answer wrong. I know I need to use derivative twice. Can someone please help me Calculation $velocity = \frac {-3(4t^2-3)}{2(4t^2+3)^2}$ $Acceleration = 36x(16x^4+8x^2-3)$ answer 0.5 and 0 was found through wolfram","A particle moves along the x-axis, its position at time t is given by $x(t)= \frac{3t}{6+8t^2}$, $t≥0$, where t is measured in seconds and x is in meters. Find time at which acceleration equals 0. I got the answer 0.5 and 0 but apparently i got the answer wrong. I know I need to use derivative twice. Can someone please help me Calculation $velocity = \frac {-3(4t^2-3)}{2(4t^2+3)^2}$ $Acceleration = 36x(16x^4+8x^2-3)$ answer 0.5 and 0 was found through wolfram",,['derivatives']
67,"Derivative of incomplete Gamma function : $\Gamma(c,t/a)=\int_{0}^{t/a}x^{c-1}e^{-x}dx$",Derivative of incomplete Gamma function :,"\Gamma(c,t/a)=\int_{0}^{t/a}x^{c-1}e^{-x}dx","How we can calculate derivative of incomplete Gamma. Can anyone give me these derivative. $\frac{\partial\Gamma(c,t/a)}{\partial a}$ and $\frac{\partial\Gamma(c,t/a)}{\partial c}$ where $\Gamma(c,t/a)=\int_{0}^{t/a}x^{c-1}e^{-x}dx$","How we can calculate derivative of incomplete Gamma. Can anyone give me these derivative. $\frac{\partial\Gamma(c,t/a)}{\partial a}$ and $\frac{\partial\Gamma(c,t/a)}{\partial c}$ where $\Gamma(c,t/a)=\int_{0}^{t/a}x^{c-1}e^{-x}dx$",,"['calculus', 'integration', 'derivatives', 'definite-integrals']"
68,Implicit differentiation to find derivatives of a function whose only defined by its derivative,Implicit differentiation to find derivatives of a function whose only defined by its derivative,,"A question I have asks if $K(x)$ satisfies $K(1)=0$ and $K'(x)={1\over{x}}$ then show: If $f(x)=K(10x)$ then $f'(x)={1\over{x}}$ So Im not sure if I have to prove it or do something else but this is what I did. $$ f'(x)=K(10x)^0\cdot K'(10x) \cdot 10 \\ f'(x)={1\over{10x}}\cdot 10={1\over{x}} $$ The other questions are similar to this so if this is okay then im sure I did the rest okay as well. Then it asks: $$ Let \ g(x)=K(x^2+1)\ \ \ \ find \ g'(x) $$ So I do: $$ g'(x)=K'(x^2+1)\cdot 2x = {2x\over{x^2+1}} $$ At this point I havent used the $K(1)=0$ so im not sure if im doing this right. The last sub question asks: If $a,b >0$ show $K(ab) = K(a)+K(b)$. Which I have no idea where to start. Any feedback appreciated, thanks.","A question I have asks if $K(x)$ satisfies $K(1)=0$ and $K'(x)={1\over{x}}$ then show: If $f(x)=K(10x)$ then $f'(x)={1\over{x}}$ So Im not sure if I have to prove it or do something else but this is what I did. $$ f'(x)=K(10x)^0\cdot K'(10x) \cdot 10 \\ f'(x)={1\over{10x}}\cdot 10={1\over{x}} $$ The other questions are similar to this so if this is okay then im sure I did the rest okay as well. Then it asks: $$ Let \ g(x)=K(x^2+1)\ \ \ \ find \ g'(x) $$ So I do: $$ g'(x)=K'(x^2+1)\cdot 2x = {2x\over{x^2+1}} $$ At this point I havent used the $K(1)=0$ so im not sure if im doing this right. The last sub question asks: If $a,b >0$ show $K(ab) = K(a)+K(b)$. Which I have no idea where to start. Any feedback appreciated, thanks.",,"['calculus', 'real-analysis', 'derivatives', 'implicit-differentiation']"
69,a practical question about matrix derivative with inverse and chain rule: dimension mismatch,a practical question about matrix derivative with inverse and chain rule: dimension mismatch,,"Recently, I was trying to take the following derivative  $$ \dfrac{\partial (X^TV^{-1}X)^{-1}}{\partial V}  $$ I was referring to matrix cookbook to solve it, where I found several useful equations: Equation (59) says: $$ \dfrac{\partial Y^{-1}}{\partial x} = -Y^{-1}\dfrac{\partial Y}{\partial x}Y^{-1} $$ so, I think I have: $$ \dfrac{\partial (X^TV^{-1}X)^{-1}}{\partial V^{-1}} = -(X^TV^{-1}X)^{-1} X^TX(X^TV^{-1}X)^{-1} $$ and  $$ \dfrac{\partial V^{-1}}{\partial V} = -V^{-1}V^{-1} $$ According to the chain rule, it should be: $$ \dfrac{\partial (X^TV^{-1}X)^{-1}}{\partial V} =\dfrac{\partial (X^TV^{-1}X)^{-1}}{\partial V^{-1}}\dfrac{\partial V^{-1}}{\partial V} = ((X^TV^{-1}X)^{-1} X^TX(X^TV^{-1}X)^{-1})^T V^{-1}V^{-1} $$ However, I met one problem. $V$ is a matrix of size $(n, n)$ and $X$ is a matrix of size $(n, m)$.  Then, the first half of the chain rule is of size of $(m, m)$, while the second half of the chain rule is of size $(n, n)$. Please help me figure out what goes wrong. Thanks ahead.","Recently, I was trying to take the following derivative  $$ \dfrac{\partial (X^TV^{-1}X)^{-1}}{\partial V}  $$ I was referring to matrix cookbook to solve it, where I found several useful equations: Equation (59) says: $$ \dfrac{\partial Y^{-1}}{\partial x} = -Y^{-1}\dfrac{\partial Y}{\partial x}Y^{-1} $$ so, I think I have: $$ \dfrac{\partial (X^TV^{-1}X)^{-1}}{\partial V^{-1}} = -(X^TV^{-1}X)^{-1} X^TX(X^TV^{-1}X)^{-1} $$ and  $$ \dfrac{\partial V^{-1}}{\partial V} = -V^{-1}V^{-1} $$ According to the chain rule, it should be: $$ \dfrac{\partial (X^TV^{-1}X)^{-1}}{\partial V} =\dfrac{\partial (X^TV^{-1}X)^{-1}}{\partial V^{-1}}\dfrac{\partial V^{-1}}{\partial V} = ((X^TV^{-1}X)^{-1} X^TX(X^TV^{-1}X)^{-1})^T V^{-1}V^{-1} $$ However, I met one problem. $V$ is a matrix of size $(n, n)$ and $X$ is a matrix of size $(n, m)$.  Then, the first half of the chain rule is of size of $(m, m)$, while the second half of the chain rule is of size $(n, n)$. Please help me figure out what goes wrong. Thanks ahead.",,"['matrices', 'derivatives']"
70,Derivative with respect to entries of a matrix,Derivative with respect to entries of a matrix,,"What is the derivative of this matrix expression with respect to $\theta_k$ \begin{equation} \begin{aligned} \mathcal{J}(X, \theta) &= {\bf trace}\left( XX^TP(\theta)^{-1} \right) +{\bf trace}\left( (Y-H(\theta)X)(Y-H(\theta)X)^T \Sigma^{-1} \right)\\ & = X^TP(\theta)^{-1}X + (Y-H(\theta)X)^T \Sigma^{-1} (Y-H(\theta)X)^T  \end{aligned} \end{equation} $X$ and $Y$ are vectors $\theta$ is a vector with entries $\theta_k$ $P(\theta)$ and $H(\theta)$ are matrices constructed using some or all of the entries of $\theta$ and possibly other constants. The matrix $\Sigma$ is an invertible known constant matrix All vectors and matrices have compatible dimensions. I tried to use The Matrix Cookbook to calculate this derivative. Here is my result: \begin{equation} \begin{aligned} \frac{\partial \mathcal{J}(X,\theta)}{\partial \theta_k }  =&  - {\bf trace} \left( X X^T P(\theta)^{-1} \frac{\partial P(\theta)}{\partial \theta_k}P(\theta)^{-1}   \right) \\ & - 2\; {\bf trace} \left(\frac{\partial H(\theta)}{\partial \theta_k} X Y^T \Sigma_e^{-1}\right)\\ &+ 2\;  {\bf trace} \left(\frac{\partial H(\theta)}{\partial \theta_k} \Sigma_e^{-1} H(\theta) X X^T\right)  \end{aligned} \end{equation} Is this result correct? Can you explain if there is a mistake? Also I would like to know if there is a better way to write this derivative.","What is the derivative of this matrix expression with respect to $\theta_k$ \begin{equation} \begin{aligned} \mathcal{J}(X, \theta) &= {\bf trace}\left( XX^TP(\theta)^{-1} \right) +{\bf trace}\left( (Y-H(\theta)X)(Y-H(\theta)X)^T \Sigma^{-1} \right)\\ & = X^TP(\theta)^{-1}X + (Y-H(\theta)X)^T \Sigma^{-1} (Y-H(\theta)X)^T  \end{aligned} \end{equation} $X$ and $Y$ are vectors $\theta$ is a vector with entries $\theta_k$ $P(\theta)$ and $H(\theta)$ are matrices constructed using some or all of the entries of $\theta$ and possibly other constants. The matrix $\Sigma$ is an invertible known constant matrix All vectors and matrices have compatible dimensions. I tried to use The Matrix Cookbook to calculate this derivative. Here is my result: \begin{equation} \begin{aligned} \frac{\partial \mathcal{J}(X,\theta)}{\partial \theta_k }  =&  - {\bf trace} \left( X X^T P(\theta)^{-1} \frac{\partial P(\theta)}{\partial \theta_k}P(\theta)^{-1}   \right) \\ & - 2\; {\bf trace} \left(\frac{\partial H(\theta)}{\partial \theta_k} X Y^T \Sigma_e^{-1}\right)\\ &+ 2\;  {\bf trace} \left(\frac{\partial H(\theta)}{\partial \theta_k} \Sigma_e^{-1} H(\theta) X X^T\right)  \end{aligned} \end{equation} Is this result correct? Can you explain if there is a mistake? Also I would like to know if there is a better way to write this derivative.",,"['derivatives', 'matrix-calculus']"
71,differentiation under the integral sign conditions,differentiation under the integral sign conditions,,"I want to know that if I have $$\frac{d}{d\alpha}\int_u^vf(t,\alpha)dt$$  and I want to using Leibniz's theorem calculate it. How can I check the conditions of this theorem? Example:$$\frac{d}{d\alpha}\int_{\sin \alpha}^{\cos \alpha}\log(\alpha + t)dt$$  It accord with condition of the theorem or not? And how?","I want to know that if I have $$\frac{d}{d\alpha}\int_u^vf(t,\alpha)dt$$  and I want to using Leibniz's theorem calculate it. How can I check the conditions of this theorem? Example:$$\frac{d}{d\alpha}\int_{\sin \alpha}^{\cos \alpha}\log(\alpha + t)dt$$  It accord with condition of the theorem or not? And how?",,"['calculus', 'derivatives']"
72,"If $g$ is differentiable and $g(1/n)=0$ for all $n$, then $g(0)=0$ and $g'(0)=0$","If  is differentiable and  for all , then  and",g g(1/n)=0 n g(0)=0 g'(0)=0,"Suppose that $g:\mathbb{R}\rightarrow\mathbb{R}$ differentiable at $x=0$ and for each natural number $n$, $g(1/n)=0$. Prove that $g(0)=0$ and $g'(0)=0$ Since $g$ is differentiable at $x=0$, so $g$ is continuous at $x=0$ and gives $\lim\limits_{x\rightarrow0}g(x)-g(0)=0$. As for all $n\in\mathbb{N}$, $g(1/n)=0=\lim\limits_{x\rightarrow0}g(x)-g(0)$. I stuck at this step, I don't see any information that I can use to get further. Can someone give me a hit or suggestion to keep going? Thanks","Suppose that $g:\mathbb{R}\rightarrow\mathbb{R}$ differentiable at $x=0$ and for each natural number $n$, $g(1/n)=0$. Prove that $g(0)=0$ and $g'(0)=0$ Since $g$ is differentiable at $x=0$, so $g$ is continuous at $x=0$ and gives $\lim\limits_{x\rightarrow0}g(x)-g(0)=0$. As for all $n\in\mathbb{N}$, $g(1/n)=0=\lim\limits_{x\rightarrow0}g(x)-g(0)$. I stuck at this step, I don't see any information that I can use to get further. Can someone give me a hit or suggestion to keep going? Thanks",,"['real-analysis', 'derivatives']"
73,Partial derivatives of the hypergeometric ${_2F_1}$,Partial derivatives of the hypergeometric,{_2F_1},"Do formulas for the partial derivatives of the hypergeometric function ${_2F_1}$ exist? I mean I am interested in $$\frac{\partial}{\partial a}\  {_2F_1}(a,b,c,z)$$$$\frac{\partial}{\partial b}\  {_2F_1}(a,b,c,z)$$ $$\frac{\partial}{\partial c} \ {_2F_1}(a,b,c,z)$$","Do formulas for the partial derivatives of the hypergeometric function ${_2F_1}$ exist? I mean I am interested in $$\frac{\partial}{\partial a}\  {_2F_1}(a,b,c,z)$$$$\frac{\partial}{\partial b}\  {_2F_1}(a,b,c,z)$$ $$\frac{\partial}{\partial c} \ {_2F_1}(a,b,c,z)$$",,"['real-analysis', 'derivatives', 'definite-integrals', 'partial-derivative', 'hypergeometric-function']"
74,Gateaux derivative of integral operator,Gateaux derivative of integral operator,,"Let $\Omega\subset \mathbb{R}$ and consider an integral operator $E\colon L^{2}(\Omega) \rightarrow \mathbb{R}$ given by $E(u) = \int_{\Omega} F(u(x))\,dx$ for $u\in L^{2}(\Omega)$. Suppose $F\colon \mathbb{R}\rightarrow \mathbb{R}$ has a derivative $f$. What other conditions do I need for $E$ to be Gateaux differentiable or have directional derivatives in a given direction $\psi \in L^{2}(\Omega)$? In particular, without any further assumptions, the [wiki][1] article on Gateaux differentiability gives the derivative as $E^{\prime}(u)(\psi) = \lim\limits_{\tau\rightarrow 0} \int_{\Omega}\int^{1}_{0}f(u+ \tau s \psi)\psi \,ds\,dx = \int_{\Omega} f(u)\psi\,dx = \langle f(u),\psi \rangle$ Where the fundamental theorem of calculus was used to write the difference quotient as the inside integral. But it seems that to move the limit inside the integrals we need $f$ to be bounded or some growth condition to use dominated convergence theorem. Is this ${necessarily}$ true? Or can we get Gateaux differentiability without boundedness or any other conditions on $f$? In particular, I am worried that $f(u)\not\in L^{2}(\Omega)$. I note a previous post talks about a growth condition. Thanks!","Let $\Omega\subset \mathbb{R}$ and consider an integral operator $E\colon L^{2}(\Omega) \rightarrow \mathbb{R}$ given by $E(u) = \int_{\Omega} F(u(x))\,dx$ for $u\in L^{2}(\Omega)$. Suppose $F\colon \mathbb{R}\rightarrow \mathbb{R}$ has a derivative $f$. What other conditions do I need for $E$ to be Gateaux differentiable or have directional derivatives in a given direction $\psi \in L^{2}(\Omega)$? In particular, without any further assumptions, the [wiki][1] article on Gateaux differentiability gives the derivative as $E^{\prime}(u)(\psi) = \lim\limits_{\tau\rightarrow 0} \int_{\Omega}\int^{1}_{0}f(u+ \tau s \psi)\psi \,ds\,dx = \int_{\Omega} f(u)\psi\,dx = \langle f(u),\psi \rangle$ Where the fundamental theorem of calculus was used to write the difference quotient as the inside integral. But it seems that to move the limit inside the integrals we need $f$ to be bounded or some growth condition to use dominated convergence theorem. Is this ${necessarily}$ true? Or can we get Gateaux differentiability without boundedness or any other conditions on $f$? In particular, I am worried that $f(u)\not\in L^{2}(\Omega)$. I note a previous post talks about a growth condition. Thanks!",,"['functional-analysis', 'derivatives']"
75,Basic Derivatives-finding tangent lines,Basic Derivatives-finding tangent lines,,"For the function $f(x)=x^2$ and $f'(x)=2x$ find the equation of the tangent line(s) to $f(x)$ from $(\frac{3}{4}, -1)$ I just started basic derivatives in my calc class. Can someone explain this to me? I set up the value a to represent the point on the parabola $y=x^2$ where the tangent line passes through making the point $(a, a^2)$. I then used point slope form to try and find the $x$ value on the parabola $y=2x(x-8) + a^2$ then I plugged in $\frac{3}{4}$ in $a^2 -\frac{3}{2}a + \frac{9}{8}$ but I struggling to find a coherent answer from this","For the function $f(x)=x^2$ and $f'(x)=2x$ find the equation of the tangent line(s) to $f(x)$ from $(\frac{3}{4}, -1)$ I just started basic derivatives in my calc class. Can someone explain this to me? I set up the value a to represent the point on the parabola $y=x^2$ where the tangent line passes through making the point $(a, a^2)$. I then used point slope form to try and find the $x$ value on the parabola $y=2x(x-8) + a^2$ then I plugged in $\frac{3}{4}$ in $a^2 -\frac{3}{2}a + \frac{9}{8}$ but I struggling to find a coherent answer from this",,"['calculus', 'derivatives']"
76,Estimating derivatives with randomness,Estimating derivatives with randomness,,"Assume $Y$ is a continuously differential function of $X$. Given i.i.d. data $(x_i,y_i)_{i=1}^n$, I would like to estimate $E\left[g\left(\left.\frac{\partial Y}{\partial X}\right|_{X=X_0}\right)\right]$ for some known nonlinear function $g$ ($g$ can be assumed continuously differential, if needed, for example $g(x)=x^2$). This question is an extension of Estimating the expectation of a derivative where $g$ was linear and the derivative and integral could be exchanged and kernel techniques used. I am interested to know how the expectation can be estimated when $g$ is nonlinear and the integral and derivative cannot be exchanged. The underlying motivation for the question is estimation of $\frac{\partial Y}{\partial X}$ when there is randomness. My idea was to do something similar to kernel estimation by giving weights to points around $X_0$ based on how close they are to $X_0$ and then sample two points at a time based on the weights, say $(y_i,x_i)$ and $(y_j,x_j)$, and calculate $g(\frac{y_i-y_j}{x_i-x_j})$ and do this many times and take the sample average.","Assume $Y$ is a continuously differential function of $X$. Given i.i.d. data $(x_i,y_i)_{i=1}^n$, I would like to estimate $E\left[g\left(\left.\frac{\partial Y}{\partial X}\right|_{X=X_0}\right)\right]$ for some known nonlinear function $g$ ($g$ can be assumed continuously differential, if needed, for example $g(x)=x^2$). This question is an extension of Estimating the expectation of a derivative where $g$ was linear and the derivative and integral could be exchanged and kernel techniques used. I am interested to know how the expectation can be estimated when $g$ is nonlinear and the integral and derivative cannot be exchanged. The underlying motivation for the question is estimation of $\frac{\partial Y}{\partial X}$ when there is randomness. My idea was to do something similar to kernel estimation by giving weights to points around $X_0$ based on how close they are to $X_0$ and then sample two points at a time based on the weights, say $(y_i,x_i)$ and $(y_j,x_j)$, and calculate $g(\frac{y_i-y_j}{x_i-x_j})$ and do this many times and take the sample average.",,"['probability', 'derivatives', 'expectation']"
77,Prove the Lipschitz constant must be less than 1.,Prove the Lipschitz constant must be less than 1.,,"I've been set this problem recently and I'm having a lot of trouble with it. Any help would be much appreciated! Let $f:\mathbb{R} \rightarrow \mathbb{R}$ be a function with continuous derivatives of all orders and suppose that, for some $x_{0}\in \mathbb{R}$ the derivative $f'(x_{0})$ is non-zero. Write $f(x_{0})=y_{0}$. (a) Show there exists an open interval $D$ containing $x_{0}$ such that $f'(x)\neq 0$ for all $x\in D$. (b) Define $F_{y_{0}}:D\rightarrow \mathbb{R}$ by $F_{y_{0}}(x)=x-\frac{f(x)-y_{0}}{f'(x)}$. Show that $F_{y_{0}}$ is a Lipschitz function with Lipschitz constant less than 1. I've managed to prove the first part, but I'm having trouble with (b). I first used the Mean Value Theorem to get $|F_{y_{0}}(y)-F_{y_{0}}(x)| = | y-x | | F'_{y_{0}}(c)|$. I then found that $F'_{y_{0}}(c)=\frac{(f(c)-y_{0})f''(c)}{(f'(c))^2}$, but I'm unsure where to go from here.","I've been set this problem recently and I'm having a lot of trouble with it. Any help would be much appreciated! Let $f:\mathbb{R} \rightarrow \mathbb{R}$ be a function with continuous derivatives of all orders and suppose that, for some $x_{0}\in \mathbb{R}$ the derivative $f'(x_{0})$ is non-zero. Write $f(x_{0})=y_{0}$. (a) Show there exists an open interval $D$ containing $x_{0}$ such that $f'(x)\neq 0$ for all $x\in D$. (b) Define $F_{y_{0}}:D\rightarrow \mathbb{R}$ by $F_{y_{0}}(x)=x-\frac{f(x)-y_{0}}{f'(x)}$. Show that $F_{y_{0}}$ is a Lipschitz function with Lipschitz constant less than 1. I've managed to prove the first part, but I'm having trouble with (b). I first used the Mean Value Theorem to get $|F_{y_{0}}(y)-F_{y_{0}}(x)| = | y-x | | F'_{y_{0}}(c)|$. I then found that $F'_{y_{0}}(c)=\frac{(f(c)-y_{0})f''(c)}{(f'(c))^2}$, but I'm unsure where to go from here.",,"['real-analysis', 'functional-analysis', 'derivatives', 'continuity', 'lipschitz-functions']"
78,"Prove that continuity in x of the Gateaux derivative, $f'(x;y)$, implies Frechet differentiability","Prove that continuity in x of the Gateaux derivative, , implies Frechet differentiability",f'(x;y),"Prove that continuity in x of the Gateaux derivative implies Frechet differentiability Let $x$ be te point, $y$ the direction and $f'(x;y)=y·a(x)$. First, I considere the function $g(\varepsilon)=f(x+\varepsilon y)-f(x)-\varepsilon y·a(x)$. Then I have $g(0)=0$, $g(1)=f(x+ y)-f(x)-y·a(x)$, and $g(\varepsilon)=o(\varepsilon)$. Note: $o(h(x))$ is a function such that $$\lim\limits_{\|y\|\rightarrow 0}\dfrac{\|o(h(x))\|}{\|h(x)\|}=0.$$ And I want to show that $$f(x+y)-f(x)-y·a(x)=o(y)$$ So I think I have to show that $g(1)=o(y)$, but I don't know how can I do that.","Prove that continuity in x of the Gateaux derivative implies Frechet differentiability Let $x$ be te point, $y$ the direction and $f'(x;y)=y·a(x)$. First, I considere the function $g(\varepsilon)=f(x+\varepsilon y)-f(x)-\varepsilon y·a(x)$. Then I have $g(0)=0$, $g(1)=f(x+ y)-f(x)-y·a(x)$, and $g(\varepsilon)=o(\varepsilon)$. Note: $o(h(x))$ is a function such that $$\lim\limits_{\|y\|\rightarrow 0}\dfrac{\|o(h(x))\|}{\|h(x)\|}=0.$$ And I want to show that $$f(x+y)-f(x)-y·a(x)=o(y)$$ So I think I have to show that $g(1)=o(y)$, but I don't know how can I do that.",,"['real-analysis', 'derivatives', 'gateaux-derivative']"
79,How to derivative the linear equation of matrix,How to derivative the linear equation of matrix,,"I have the equation as $$F(w,x)=\sum_{i=1}^{N}\int_{x \in \Omega} \left (  Y(x)-w^TA(x)\right)^2u_i(x)dx$$ In which, $w$ is column vector that independent on $x$, denotes $w=[w_1,w_2...,w_M]^T$ $A$ is column vector value that denotes $A(x)=[a_1(x),a_2(x),...,a_M(x)]^T$ $(.)^T$ is transpose operator. $Y(x)$ and $u_i(x)$,$A(x)$ are independent on $w$. I want to perform derivative of $F(x,w)$ with respect to $w$. Could you see my answer and give me some suggestion that Is it correct? $$\frac {\partial F(x,w)}{\partial w}=?$$ That is what I done we see that $w^TA(x)=\sum_{i=1}^{M} w_ia_i$, then $$\frac {\partial (w^TA)}{\partial w_i}=ai$$ For $i=1$ to $M$, hence  $$\frac {\partial (w^TA)}{\partial w}=A$$ Hence,  $$\frac {\partial F(w,x)}{\partial w}=-2\sum_{i=1}^{N}\int_{x \in \Omega} \left ( Y(x)-w^TA(x)\right)u_i(x)\frac {\partial (w^TA)}{\partial w}dx$$ $$\frac {\partial F(w,x)}{\partial w}=-2\sum_{i=1}^{N}\int_{x \in \Omega} \left ( Y(x)-w^TA(x)\right)u_i(x)A(x)dx$$ $$\frac {\partial F(w,x)}{\partial w}=-2\sum_{i=1}^{N}\int_{x \in \Omega} \left ( Y(x)-w^TA(x)\right)A(x)u_i(x)dx$$ Because $w^TA(x)=A(x)w$ then $$\frac {\partial F(w,x)}{\partial w}=-2\sum_{i=1}^{N}\int_{x \in \Omega} \left ( Y(x)A(x)-A(x)A^T(x)w\right)u_i(x)dx$$ I'm wondering about the last two steps.","I have the equation as $$F(w,x)=\sum_{i=1}^{N}\int_{x \in \Omega} \left (  Y(x)-w^TA(x)\right)^2u_i(x)dx$$ In which, $w$ is column vector that independent on $x$, denotes $w=[w_1,w_2...,w_M]^T$ $A$ is column vector value that denotes $A(x)=[a_1(x),a_2(x),...,a_M(x)]^T$ $(.)^T$ is transpose operator. $Y(x)$ and $u_i(x)$,$A(x)$ are independent on $w$. I want to perform derivative of $F(x,w)$ with respect to $w$. Could you see my answer and give me some suggestion that Is it correct? $$\frac {\partial F(x,w)}{\partial w}=?$$ That is what I done we see that $w^TA(x)=\sum_{i=1}^{M} w_ia_i$, then $$\frac {\partial (w^TA)}{\partial w_i}=ai$$ For $i=1$ to $M$, hence  $$\frac {\partial (w^TA)}{\partial w}=A$$ Hence,  $$\frac {\partial F(w,x)}{\partial w}=-2\sum_{i=1}^{N}\int_{x \in \Omega} \left ( Y(x)-w^TA(x)\right)u_i(x)\frac {\partial (w^TA)}{\partial w}dx$$ $$\frac {\partial F(w,x)}{\partial w}=-2\sum_{i=1}^{N}\int_{x \in \Omega} \left ( Y(x)-w^TA(x)\right)u_i(x)A(x)dx$$ $$\frac {\partial F(w,x)}{\partial w}=-2\sum_{i=1}^{N}\int_{x \in \Omega} \left ( Y(x)-w^TA(x)\right)A(x)u_i(x)dx$$ Because $w^TA(x)=A(x)w$ then $$\frac {\partial F(w,x)}{\partial w}=-2\sum_{i=1}^{N}\int_{x \in \Omega} \left ( Y(x)A(x)-A(x)A^T(x)w\right)u_i(x)dx$$ I'm wondering about the last two steps.",,"['calculus', 'linear-algebra', 'derivatives', 'partial-derivative']"
80,Calculate derivative of integral,Calculate derivative of integral,,"I tried to calculate the derivative of this integral: $$\int_{2}^{3+\sqrt{r}}  (3 + \sqrt{r}-c) \frac{1}{2}\,{\rm d}c $$ First I took the anti-derivative of the integral: $$\frac{1}{2}\left(\frac{-c^2}{2}+c\sqrt{r}+3c\right)$$ Then I evaluated the integral: $$-\frac{1}{2}\left(\frac{3+\sqrt{r})^2}{2} + (3 + \sqrt{r})\sqrt{r}+3(3+\sqrt{r})\right)-\frac{1}{2}\left(-\frac{(2^2)}{2}+2\sqrt{r}+3\cdot 2\right)$$ After I simplified I got: $$\frac{1 + 2\sqrt{r}+r}{4}$$ I should get: $$\frac{1 + \sqrt{r}}{4\sqrt{r}}$$ But I cannot get this result. Can someone help? what am I missing?","I tried to calculate the derivative of this integral: $$\int_{2}^{3+\sqrt{r}}  (3 + \sqrt{r}-c) \frac{1}{2}\,{\rm d}c $$ First I took the anti-derivative of the integral: $$\frac{1}{2}\left(\frac{-c^2}{2}+c\sqrt{r}+3c\right)$$ Then I evaluated the integral: $$-\frac{1}{2}\left(\frac{3+\sqrt{r})^2}{2} + (3 + \sqrt{r})\sqrt{r}+3(3+\sqrt{r})\right)-\frac{1}{2}\left(-\frac{(2^2)}{2}+2\sqrt{r}+3\cdot 2\right)$$ After I simplified I got: $$\frac{1 + 2\sqrt{r}+r}{4}$$ I should get: $$\frac{1 + \sqrt{r}}{4\sqrt{r}}$$ But I cannot get this result. Can someone help? what am I missing?",,"['integration', 'derivatives']"
81,How do I differentiate the following implicit function,How do I differentiate the following implicit function,,$$\sqrt{x^2+y^2} = e^{\sin^{-1} \left( \frac{y}{\sqrt{x^2 + y^2}} \right) } \\ \text{find} \quad \frac{d^2x}{dy^2} \quad \text{i.e. prove} \\ \frac{d^2x}{dy^2} = \frac{2 \left( x^2 + y^2 \right) }{\left( x - y \right)^2}$$ I tried by differentiating directly and making trigonometric substitution  $x=y \cot a$ which gave parametric equation,$$\sqrt{x^2+y^2} = e^{\sin^{-1} \left( \frac{y}{\sqrt{x^2 + y^2}} \right) } \\ \text{find} \quad \frac{d^2x}{dy^2} \quad \text{i.e. prove} \\ \frac{d^2x}{dy^2} = \frac{2 \left( x^2 + y^2 \right) }{\left( x - y \right)^2}$$ I tried by differentiating directly and making trigonometric substitution  $x=y \cot a$ which gave parametric equation,,['derivatives']
82,Does there exist higher degree graded derivations on $\Omega(M) $,Does there exist higher degree graded derivations on,\Omega(M) ,"Does there exist any other graded derivation on $\Omega(M)$ other than the one of degree one which is the exterior derivative (i.e. maps such as $d: \Omega^p(M) \rightarrow \Omega^{(p+r)}(M) $, where $ r = 3,4,5...$). I've been reading a bit about graded derivations on graded algebras. However, I've not been able to find an answer for specifically for the algebra of differential forms on a smooth manifold. Any help would be appreciated. Thanks.","Does there exist any other graded derivation on $\Omega(M)$ other than the one of degree one which is the exterior derivative (i.e. maps such as $d: \Omega^p(M) \rightarrow \Omega^{(p+r)}(M) $, where $ r = 3,4,5...$). I've been reading a bit about graded derivations on graded algebras. However, I've not been able to find an answer for specifically for the algebra of differential forms on a smooth manifold. Any help would be appreciated. Thanks.",,"['differential-geometry', 'derivatives', 'algebras', 'differential-algebra']"
83,Existence of differentiable functions on $\mathbb R$ whose derivative is constant on the complement of uncountable set but not everywhere,Existence of differentiable functions on  whose derivative is constant on the complement of uncountable set but not everywhere,\mathbb R,"Let $ A $ be a countable subset of the set of real numbers and $f:\mathbb R \to \mathbb R$ be a differentiable function such that $f'$ is constant on $\mathbb R \setminus A$ , then I know that $f'$ is constant on $\mathbb R$ . My question is ; is it true that for every $c \in \mathbb R$ and uncountable set $B \subseteq \mathbb R$ , there exists a differentiable function $f:\mathbb R \to \mathbb R$ such that $f'(x)=c , \forall x \in \mathbb R \setminus B$ but $f'$ is not constant on $\mathbb R$ ?","Let $ A $ be a countable subset of the set of real numbers and $f:\mathbb R \to \mathbb R$ be a differentiable function such that $f'$ is constant on $\mathbb R \setminus A$ , then I know that $f'$ is constant on $\mathbb R$ . My question is ; is it true that for every $c \in \mathbb R$ and uncountable set $B \subseteq \mathbb R$ , there exists a differentiable function $f:\mathbb R \to \mathbb R$ such that $f'(x)=c , \forall x \in \mathbb R \setminus B$ but $f'$ is not constant on $\mathbb R$ ?",,['real-analysis']
84,"$\|f(x)-f(y)\|\ge c\|x-y\|$ for all $x,y\in U, c>0$-> continuously differentiable inverse function $g:f(U)\to U$",for all -> continuously differentiable inverse function,"\|f(x)-f(y)\|\ge c\|x-y\| x,y\in U, c>0 g:f(U)\to U","Let $U\subset\mathbb{R}^n$ open, $f:U\to \mathbb{R}^n$ continuously differentiable, $\|f(x)-f(y)\|\ge c\|x-y\|$ for all $x,y\in U, c>0$. Why is $\det(Df(x))\neq 0$ for all $x\in U$ and $f\colon U\mapsto f(U)$ global invertible with inverse function g, wich is continuously differentiable ? First of all, here If $f: U \rightarrow \mathbb{R}^n$ differentiable such that $|f(x)-f(y)| \geq c |x-y|$ for all $x,y \in U$, then $\det \mathbf{J}_f(x) \neq 0$ is a solution for 1. Now 2. My ideas: and from $\|f(x)-f(y)\|\ge c\|x-y\|$ for all $x$, $y\in U$, $c>0$ follows that f is injective. Because otherwise for $x\not=y$ such that $f(x)=f(y)$ it follows that $0=\|f(x)-f(y)\|\ge c\|x-y\|>0$ which is a contradiction. Clearly $f\colon U\to f(U)$ is surjective . And with the argument above $f$ onto $f(U)$ is bijective with inverse function $g$. But why is $g$ continuously differentiable ?","Let $U\subset\mathbb{R}^n$ open, $f:U\to \mathbb{R}^n$ continuously differentiable, $\|f(x)-f(y)\|\ge c\|x-y\|$ for all $x,y\in U, c>0$. Why is $\det(Df(x))\neq 0$ for all $x\in U$ and $f\colon U\mapsto f(U)$ global invertible with inverse function g, wich is continuously differentiable ? First of all, here If $f: U \rightarrow \mathbb{R}^n$ differentiable such that $|f(x)-f(y)| \geq c |x-y|$ for all $x,y \in U$, then $\det \mathbf{J}_f(x) \neq 0$ is a solution for 1. Now 2. My ideas: and from $\|f(x)-f(y)\|\ge c\|x-y\|$ for all $x$, $y\in U$, $c>0$ follows that f is injective. Because otherwise for $x\not=y$ such that $f(x)=f(y)$ it follows that $0=\|f(x)-f(y)\|\ge c\|x-y\|>0$ which is a contradiction. Clearly $f\colon U\to f(U)$ is surjective . And with the argument above $f$ onto $f(U)$ is bijective with inverse function $g$. But why is $g$ continuously differentiable ?",,['derivatives']
85,In which conditions $ f'(x)=O(g'(x))$ implies $f(x)=O(g(x))$?,In which conditions  implies ?, f'(x)=O(g'(x)) f(x)=O(g(x)),"First question, let $f,g:(-a,0)\rightarrow(0,\infty)$, introduce the notation: We say $f(x)=O(g(x))$ in $0$, if there exists constants $c,\epsilon>0$, such that $$f(x)\leq cg(x) \ \ \forall x\in(-\epsilon,0).$$ Suppose $f,g,f',g':(-a,0)\rightarrow(0,\infty)$, where by $f'$ we denote the derivative of $f$. And suppose $ f'(x)=O(g'(x))$ in $0$. Then, is it true that: $$f(x)=O(g(x)),  \text{ in } 0 ?$$ Second question: more generally, if $f,g:(-a,0)\rightarrow\mathbb{R}$, and $f(x)=O(g(x))$ in $0 $ is defined by $$ |f(x)|\leq c|g(x)| \ \ \forall x\in(-\epsilon,0).$$ And suppose $$ f'(x)=O(g'(x)) \text{  in   }  0.$$ Is it true that $$ f(x)=O(g(x)) \text{  in  } 0?$$ In some cases I think it's only true that $f(x)+\alpha=O(g(x))$ for some constant $\alpha$.","First question, let $f,g:(-a,0)\rightarrow(0,\infty)$, introduce the notation: We say $f(x)=O(g(x))$ in $0$, if there exists constants $c,\epsilon>0$, such that $$f(x)\leq cg(x) \ \ \forall x\in(-\epsilon,0).$$ Suppose $f,g,f',g':(-a,0)\rightarrow(0,\infty)$, where by $f'$ we denote the derivative of $f$. And suppose $ f'(x)=O(g'(x))$ in $0$. Then, is it true that: $$f(x)=O(g(x)),  \text{ in } 0 ?$$ Second question: more generally, if $f,g:(-a,0)\rightarrow\mathbb{R}$, and $f(x)=O(g(x))$ in $0 $ is defined by $$ |f(x)|\leq c|g(x)| \ \ \forall x\in(-\epsilon,0).$$ And suppose $$ f'(x)=O(g'(x)) \text{  in   }  0.$$ Is it true that $$ f(x)=O(g(x)) \text{  in  } 0?$$ In some cases I think it's only true that $f(x)+\alpha=O(g(x))$ for some constant $\alpha$.",,"['derivatives', 'asymptotics']"
86,What am I doing wrong with this derivative - Differential calculus (brush up),What am I doing wrong with this derivative - Differential calculus (brush up),,"today I felt like doing some maths and I thought to myself that practicing some derivatives would be neat-o. I sat myself the following question. $$\frac{d}{dx}\left(\frac{5x^4+4x^3+3x^2+2x+1}{x^3}\right)$$Here are my thought process to the problem: $$\frac{d}{dx}\left(5x+4+\frac{3}{x}+\frac{2}{x^2}+\frac{1}{x^3}\right)$$ $$5+\frac{d}{dx}(3x^{-1}+2x^{-2}+x^{-3})$$ $$5-3x^{-2}-4x^{-3}-3x^{-4}$$ Which then if you will could be rewritten again to the perhaps more beautiful $$5-\frac{3}{x^2}-\frac{4}{x^3}-\frac{3}{x^4}$$ Finally my question is, why is this incorrect?","today I felt like doing some maths and I thought to myself that practicing some derivatives would be neat-o. I sat myself the following question. $$\frac{d}{dx}\left(\frac{5x^4+4x^3+3x^2+2x+1}{x^3}\right)$$Here are my thought process to the problem: $$\frac{d}{dx}\left(5x+4+\frac{3}{x}+\frac{2}{x^2}+\frac{1}{x^3}\right)$$ $$5+\frac{d}{dx}(3x^{-1}+2x^{-2}+x^{-3})$$ $$5-3x^{-2}-4x^{-3}-3x^{-4}$$ Which then if you will could be rewritten again to the perhaps more beautiful $$5-\frac{3}{x^2}-\frac{4}{x^3}-\frac{3}{x^4}$$ Finally my question is, why is this incorrect?",,"['calculus', 'derivatives']"
87,How to prove this tedious (but easy) derivative theorem,How to prove this tedious (but easy) derivative theorem,,"I'm reading Fulton's algebraic curves book (page 3) and I'm having problems with (4), (5) and (6) part of this theorem. This proof seems really easy to demonstrate, but there are a lot of calculations. Is there some strategy to prove this theorem without tedious and tiresome computations. Thanks","I'm reading Fulton's algebraic curves book (page 3) and I'm having problems with (4), (5) and (6) part of this theorem. This proof seems really easy to demonstrate, but there are a lot of calculations. Is there some strategy to prove this theorem without tedious and tiresome computations. Thanks",,"['abstract-algebra', 'derivatives']"
88,Differentiation under the integral sign — where is my mistake?,Differentiation under the integral sign — where is my mistake?,,"I'm trying to find $$\int_0^\infty \sin \left( x^2 \right)\,dx$$ by the method of differentiation under the integral sign. The idea is to use differentiation with respect to $t$ on $A(t)$ — defined below — and then let $t$ approach infinity and take the square root to find the Fresnel integral. Let $$A(t) = \left( \int_0^t \sin(x^2)\,dx \right)^2$$ $$A'(t) = 2\sin(t^2) \int_0^t \sin(x^2)\,dx$$ Let $x=yt$ $A'(t) = 2\sin(t^2) \int_0^1 \sin(t^2y^2)t\,dy $ $A'(t) = \int_0^1 2t\sin(t^2)\sin(t^2y^2)\,dy $ $A'(t) = \int_0^1 t(\cos(t^2-t^2y^2)-\cos(t^2+t^2y^2))\,dy $ $A'(t) = \int_0^1 t(\cos(t^2(1-y^2))-\cos(t^2(1+y^2)))\,dy $ $A'(t) = \frac{1}{2}\int_0^1 \frac{\partial}{\partial t} (\frac{\sin(t^2(1-y^2))}{1-y^2}-\frac{\sin(t^2(1+y^2))}{y^2+1})\,dy $ $A'(t) = \frac{1}{2}\frac{\text{d}}{\text{d}t}\int_0^1 \frac{\sin(t^2(1-y^2))}{1-y^2}-\frac{\sin(t^2(1+y^2))}{y^2+1} \, dy $ By the Fundamental Theorem of Calculus: $\int A'(t)\,dt + C = A(t)$ If we take the limit as $\lim_{t\to 0}$ : EDIT: $\lim_{t\to 0} \int{A'(t)dt} + C = \lim_{t\to 0}A(t)$ $\lim_{t\to 0} \int{A'(t)dt} = 0$ and $\lim_{t\to 0}A(t) = 0$ So $0 + C = 0$ and $C=0$ Thus, $\int A'(t)\,dt = A(t)$ , But, if we take the limit as $\lim_{t\to \infty}$ : EDIT: $\lim_{t\to \infty} \int{A'(t)dt} = \lim_{t\to \infty}A(t)$ I haven't been able to confirm it, but I am pretty sure from numerical calculations that $\lim_{t\to \infty} \int{A'(t)dt} = 0$ . But we know that $\lim_{t\to \infty}A(t)$ should be $\pi/8$ . And $0\neq \pi/8$ . I may have made a simple algebra or calculus mistake, but I haven't caught it. You help is very much appreciated.","I'm trying to find by the method of differentiation under the integral sign. The idea is to use differentiation with respect to on — defined below — and then let approach infinity and take the square root to find the Fresnel integral. Let Let By the Fundamental Theorem of Calculus: If we take the limit as : EDIT: and So and Thus, , But, if we take the limit as : EDIT: I haven't been able to confirm it, but I am pretty sure from numerical calculations that . But we know that should be . And . I may have made a simple algebra or calculus mistake, but I haven't caught it. You help is very much appreciated.","\int_0^\infty \sin \left( x^2 \right)\,dx t A(t) t A(t) = \left( \int_0^t \sin(x^2)\,dx \right)^2 A'(t) = 2\sin(t^2) \int_0^t \sin(x^2)\,dx x=yt A'(t) = 2\sin(t^2) \int_0^1 \sin(t^2y^2)t\,dy  A'(t) = \int_0^1 2t\sin(t^2)\sin(t^2y^2)\,dy  A'(t) = \int_0^1 t(\cos(t^2-t^2y^2)-\cos(t^2+t^2y^2))\,dy  A'(t) = \int_0^1 t(\cos(t^2(1-y^2))-\cos(t^2(1+y^2)))\,dy  A'(t) = \frac{1}{2}\int_0^1 \frac{\partial}{\partial t} (\frac{\sin(t^2(1-y^2))}{1-y^2}-\frac{\sin(t^2(1+y^2))}{y^2+1})\,dy  A'(t) = \frac{1}{2}\frac{\text{d}}{\text{d}t}\int_0^1 \frac{\sin(t^2(1-y^2))}{1-y^2}-\frac{\sin(t^2(1+y^2))}{y^2+1} \, dy  \int A'(t)\,dt + C = A(t) \lim_{t\to 0} \lim_{t\to 0} \int{A'(t)dt} + C = \lim_{t\to 0}A(t) \lim_{t\to 0} \int{A'(t)dt} = 0 \lim_{t\to 0}A(t) = 0 0 + C = 0 C=0 \int A'(t)\,dt = A(t) \lim_{t\to \infty} \lim_{t\to \infty} \int{A'(t)dt} = \lim_{t\to \infty}A(t) \lim_{t\to \infty} \int{A'(t)dt} = 0 \lim_{t\to \infty}A(t) \pi/8 0\neq \pi/8","['calculus', 'derivatives', 'trigonometry', 'definite-integrals', 'fresnel-integrals']"
89,Integral and derivative,Integral and derivative,,"Let $g(x) = \int_{[0;2^x]}{\sin(t^2)} dt$ for $x \in \mathbb{R}$. I have to calculate $g'(0)$. So, $g'(0) = \lim_{h \to 0}{\frac{g(h) - \int_{[0;1]}{\sin(t^2)} dt}{h}}$. Maybe I should apply the formula $\lim_{n \to \infty} \sum_{k=1}^{n}\frac{1}{k}f\left(\frac{k}{n}\right)=\int_{0}^{1}f(x)dx$, but I don't think it would help. How to end the calculation?","Let $g(x) = \int_{[0;2^x]}{\sin(t^2)} dt$ for $x \in \mathbb{R}$. I have to calculate $g'(0)$. So, $g'(0) = \lim_{h \to 0}{\frac{g(h) - \int_{[0;1]}{\sin(t^2)} dt}{h}}$. Maybe I should apply the formula $\lim_{n \to \infty} \sum_{k=1}^{n}\frac{1}{k}f\left(\frac{k}{n}\right)=\int_{0}^{1}f(x)dx$, but I don't think it would help. How to end the calculation?",,"['integration', 'derivatives']"
90,I can't find the critical points for this function. I showed my work :),I can't find the critical points for this function. I showed my work :),,"So, I have to find Critical Points of $y=\frac{1}{(x^3-x)}$ I know the derivative. Derivative = $(3x^2-1)/(x^3-x)^2$ To find Critical Points I equal to $0$. $x=1/\sqrt3$ and $x=-1/\sqrt3 $ But Critical points are the Max and Min value of your graph... and the graph is a little tricky... I don't know what to do... Because in my opinion, there are no critical points. It goes to infinity and -infinity. Opinions? Help please!","So, I have to find Critical Points of $y=\frac{1}{(x^3-x)}$ I know the derivative. Derivative = $(3x^2-1)/(x^3-x)^2$ To find Critical Points I equal to $0$. $x=1/\sqrt3$ and $x=-1/\sqrt3 $ But Critical points are the Max and Min value of your graph... and the graph is a little tricky... I don't know what to do... Because in my opinion, there are no critical points. It goes to infinity and -infinity. Opinions? Help please!",,"['calculus', 'derivatives']"
91,Show that Vandermonde-like $3 \times 3$ determinant is non-zero,Show that Vandermonde-like  determinant is non-zero,3 \times 3,"I want to show that the determinant $$    f(t)=\det \begin{bmatrix} 1 & x & x^t \\                              1 & y & y^t \\                              1 & z & z^t      \end{bmatrix} $$ is nonzero for all $t \neq 0$, $1$ if $x$, $y$, $z$ are mutually distinct positive numbers. It can be seen that $2\times 2$ minors are nonzero. Now suppose, on the contrary, that $f(t) = 0$. Then there exist real nonzero $\lambda_1$, $\lambda_2$ such that $$    \lambda_1 + \lambda_2 = 1, \\    \lambda_1 x + \lambda_2 y = z, \\    \lambda_1 x^t + \lambda_2 y^t = z^t. $$ From the first two equations we find $$    \lambda_1 = \frac{y - z}{y-x}, \; \lambda_2 = \frac{z - x}{y-x}, $$ and from the first and last: $$    \lambda_1 = \frac{y^t-z^t}{y^t-x^t}, \; \lambda_2 = \frac{z^t-x^t}{y^t-x^t}. $$ It implies that $$    \frac{q^t - 1}{p^t - 1} = \frac{q-1}{p-1},\quad q = \frac{z}{y}, \; p = \frac{x}{y}. $$ Without loss of generality we can suppose that $p < 1 < q$. It is sufficient to show that the map $$    g(t) = \frac{q^t-1}{p^t-1} $$ is strictly decreasing. It is true in all the examples that I checked in Wolfram Alpha. The straightforward way to show that the map is decreasing is to compute $g'(t)$ and to show that $g'(t) < 0$ for all $t$. But this requires a lot of technical work, the arising expressions are huge. My question is whether it is possible to solve the initial problem using some other considerations or maybe there is an easy (technically) way to show that $g'(t)<0$?","I want to show that the determinant $$    f(t)=\det \begin{bmatrix} 1 & x & x^t \\                              1 & y & y^t \\                              1 & z & z^t      \end{bmatrix} $$ is nonzero for all $t \neq 0$, $1$ if $x$, $y$, $z$ are mutually distinct positive numbers. It can be seen that $2\times 2$ minors are nonzero. Now suppose, on the contrary, that $f(t) = 0$. Then there exist real nonzero $\lambda_1$, $\lambda_2$ such that $$    \lambda_1 + \lambda_2 = 1, \\    \lambda_1 x + \lambda_2 y = z, \\    \lambda_1 x^t + \lambda_2 y^t = z^t. $$ From the first two equations we find $$    \lambda_1 = \frac{y - z}{y-x}, \; \lambda_2 = \frac{z - x}{y-x}, $$ and from the first and last: $$    \lambda_1 = \frac{y^t-z^t}{y^t-x^t}, \; \lambda_2 = \frac{z^t-x^t}{y^t-x^t}. $$ It implies that $$    \frac{q^t - 1}{p^t - 1} = \frac{q-1}{p-1},\quad q = \frac{z}{y}, \; p = \frac{x}{y}. $$ Without loss of generality we can suppose that $p < 1 < q$. It is sufficient to show that the map $$    g(t) = \frac{q^t-1}{p^t-1} $$ is strictly decreasing. It is true in all the examples that I checked in Wolfram Alpha. The straightforward way to show that the map is decreasing is to compute $g'(t)$ and to show that $g'(t) < 0$ for all $t$. But this requires a lot of technical work, the arising expressions are huge. My question is whether it is possible to solve the initial problem using some other considerations or maybe there is an easy (technically) way to show that $g'(t)<0$?",,"['real-analysis', 'linear-algebra', 'derivatives']"
92,Finding the marked values of x on a graph,Finding the marked values of x on a graph,,"I would assume that since $x_3$ is the local maximum(or absolute maximum) on the graph of $f$ prime, that it would be the greatest on the graph of $f.$ However, this problem is online, and in order for me to get it right, I have to get all of the problems shown right. Would it be right to see the graph at local and absolute extrema? If not, how would I go about solving a problem like this?","I would assume that since $x_3$ is the local maximum(or absolute maximum) on the graph of $f$ prime, that it would be the greatest on the graph of $f.$ However, this problem is online, and in order for me to get it right, I have to get all of the problems shown right. Would it be right to see the graph at local and absolute extrema? If not, how would I go about solving a problem like this?",,"['calculus', 'derivatives']"
93,Differentiating inverse hyperbolic function,Differentiating inverse hyperbolic function,,"I am trying to differentiate $\tanh^{−1}\left(x/(1 + x^2)\right)$, but am finding it difficult understanding what to do. I think you have to place the differential of the angle of the hyperbolic function  as the numerator so i differentiated it and got $(1 - x^2)/(1 + x^2)^2$. But i am stuck about what to do next.","I am trying to differentiate $\tanh^{−1}\left(x/(1 + x^2)\right)$, but am finding it difficult understanding what to do. I think you have to place the differential of the angle of the hyperbolic function  as the numerator so i differentiated it and got $(1 - x^2)/(1 + x^2)^2$. But i am stuck about what to do next.",,"['derivatives', 'inverse', 'hyperbolic-functions']"
94,Matrix Derivative d(AXA)^(-1)/dX,Matrix Derivative d(AXA)^(-1)/dX,,"I am having trouble figuring out the following matrix derivative $\frac{\partial(B X A')(AX A')^{-1}}{\partial X}$, where $X$ is square $n\times n$, A is $m\times n$, with $m<n$. and B is dimension $l\times N$. I started with $\frac{\partial BX A'}{\partial X}(AX A')^{-1}+ (BX A')\frac{\partial (A XA)^{-1}}{\partial X}$, but I can't properly figure out the second term.","I am having trouble figuring out the following matrix derivative $\frac{\partial(B X A')(AX A')^{-1}}{\partial X}$, where $X$ is square $n\times n$, A is $m\times n$, with $m<n$. and B is dimension $l\times N$. I started with $\frac{\partial BX A'}{\partial X}(AX A')^{-1}+ (BX A')\frac{\partial (A XA)^{-1}}{\partial X}$, but I can't properly figure out the second term.",,"['derivatives', 'matrix-calculus']"
95,derivatives of non-analytic smooth functions,derivatives of non-analytic smooth functions,,"I would like to know how to calculate the derivative of a non-analytic smooth function? Suppose $f:\mathbb R\rightarrow \mathbb R$ is in $\mathcal C^\infty\backslash \mathcal C^\omega$ and in particular has no Taylor series expansion at $x$. The right (left) derivative at point $x$ is: \begin{equation} f'(x)=\lim_{\epsilon\rightarrow 0}\frac{1}{\epsilon}\Big(f(x+\epsilon)-f(x)\Big) \end{equation} The limit, due to smoothness, exists even though we can't expand, but there is no way to find the limit by means of a calculation or is there? thanks a lot!","I would like to know how to calculate the derivative of a non-analytic smooth function? Suppose $f:\mathbb R\rightarrow \mathbb R$ is in $\mathcal C^\infty\backslash \mathcal C^\omega$ and in particular has no Taylor series expansion at $x$. The right (left) derivative at point $x$ is: \begin{equation} f'(x)=\lim_{\epsilon\rightarrow 0}\frac{1}{\epsilon}\Big(f(x+\epsilon)-f(x)\Big) \end{equation} The limit, due to smoothness, exists even though we can't expand, but there is no way to find the limit by means of a calculation or is there? thanks a lot!",,"['real-analysis', 'derivatives', 'analyticity']"
96,Is this observation correct?,Is this observation correct?,,"I was revising some differentiation, when I observed something that I pretty much always took for granted, so I decided to write it in mathematical notation. Would you please tell me if I'm correct to say the following: If $f(X) = cg(X)$, $c$ is a constant then $f'(X) = cg'(x)$. Where $f'(x)$ and $g'(x)$ means 'gradient of each function'.","I was revising some differentiation, when I observed something that I pretty much always took for granted, so I decided to write it in mathematical notation. Would you please tell me if I'm correct to say the following: If $f(X) = cg(X)$, $c$ is a constant then $f'(X) = cg'(x)$. Where $f'(x)$ and $g'(x)$ means 'gradient of each function'.",,['derivatives']
97,Maximum volume of a box with a lid that can be made out of a square,Maximum volume of a box with a lid that can be made out of a square,,"Snacks will be provided in a box with a lid (made by removing squares from each corner of a rectangular piece of card and then folding up the sides) You have a piece of cardboard that is 40cm by 40 cm – what dimensions would give the  maximum volume? This is how I attempted it Let the length of the square to be cut off be x cm. V be the Volume in cm3 Volume = L x B x H $$L= 40 – 2x$$ $$B= 40 – 2x$$ $$H = x$$ So Volume $= x(40-2x)(40-2x)$ $$V = 4X^3 – 160x^2 + 1600x$$ Or $V= x^3 – 40x^2 + 400x$ $$V'  = 3x^2 – 80x + 400$$ $$V'' = 6x – 80$$ Solve for turning points by putting $V ' = 0$ $$3x^ 2 – 80x + 400 = 0 $$ $$(3x - 20)(x – 20) = 0$$ $x= 20/3$ and  $x = 20$ For Max Volume, $x = 20$ is a reasonable solution that can be apply  and discard the other root i.e. $x = 20$ 2nd derivative test  $x = 20/3$: $$V'' = 6(20/3) – 80= -20 < 0 $$ it will give Maximum Volume $$V =  (20/3)3 – 40(20/3)2 + 400(20/3)$$ $$V = 1186 \text{cm}^ 3$$ Have I done it right ?","Snacks will be provided in a box with a lid (made by removing squares from each corner of a rectangular piece of card and then folding up the sides) You have a piece of cardboard that is 40cm by 40 cm – what dimensions would give the  maximum volume? This is how I attempted it Let the length of the square to be cut off be x cm. V be the Volume in cm3 Volume = L x B x H $$L= 40 – 2x$$ $$B= 40 – 2x$$ $$H = x$$ So Volume $= x(40-2x)(40-2x)$ $$V = 4X^3 – 160x^2 + 1600x$$ Or $V= x^3 – 40x^2 + 400x$ $$V'  = 3x^2 – 80x + 400$$ $$V'' = 6x – 80$$ Solve for turning points by putting $V ' = 0$ $$3x^ 2 – 80x + 400 = 0 $$ $$(3x - 20)(x – 20) = 0$$ $x= 20/3$ and  $x = 20$ For Max Volume, $x = 20$ is a reasonable solution that can be apply  and discard the other root i.e. $x = 20$ 2nd derivative test  $x = 20/3$: $$V'' = 6(20/3) – 80= -20 < 0 $$ it will give Maximum Volume $$V =  (20/3)3 – 40(20/3)2 + 400(20/3)$$ $$V = 1186 \text{cm}^ 3$$ Have I done it right ?",,"['calculus', 'derivatives', 'polynomials', 'optimization']"
98,Arc length of $f(x) = x^3/3$ from $x=1$ to $x=2$,Arc length of  from  to,f(x) = x^3/3 x=1 x=2,"I found this on my test today, and I didn't manage to solve it. Find the arc length of $f(x) = x^3/3$ from $x=1$ to $x=2$ . If I use formula for arc length which is $\ell = \int_a^b \sqrt{1 + (f'(x))^2} \, dx$ where is $f'(x)=x^2$ I will get $\ell = \int_1^2 \sqrt{1+x^4} \, dx$ . I used everything I could (know), partition and substitution, but nothing works... How to solve this definite integral? $$\ell =\int_1^2 \sqrt{1+x^4} \, dx$$","I found this on my test today, and I didn't manage to solve it. Find the arc length of from to . If I use formula for arc length which is where is I will get . I used everything I could (know), partition and substitution, but nothing works... How to solve this definite integral?","f(x) = x^3/3 x=1 x=2 \ell = \int_a^b \sqrt{1 + (f'(x))^2} \, dx f'(x)=x^2 \ell = \int_1^2 \sqrt{1+x^4} \, dx \ell =\int_1^2 \sqrt{1+x^4} \, dx","['real-analysis', 'integration', 'derivatives', 'definite-integrals']"
99,$dx$ being a desginator (with respect to $x$) or being a term?,being a desginator (with respect to ) or being a term?,dx x,"I am confused as to what $dx$ truly is. I am doing some u-substitution problems and this is what I came across: $$\int 2x(x-1)^{1/2}\,dx$$ $u=x-1$ and therefore $du=1$ when we substitute we get: $$2 \int (u^{3/2}+u^{1/2}) \, du$$ (here the du simply replaces the dx because our variable changed) In another example: $$\int 4x^5(x^2+1)^{1/3} \, dx$$ $$u=x^3+1$$ $$du=3x^2$$ therefore it becomes: $$\int 4(u-1)(du/3)(u)^{1/3}\,$$ -here my teacher didn't put $d$x at the end, she just left it off So my question is this: why is it that sometimes $dx$ and $du$ are treated as values that can be multiplied to other terms in the integrand and sometimes they are simply treated as a command (do ""blank"" with respect to $x$, or $u$ or whatever is used)?","I am confused as to what $dx$ truly is. I am doing some u-substitution problems and this is what I came across: $$\int 2x(x-1)^{1/2}\,dx$$ $u=x-1$ and therefore $du=1$ when we substitute we get: $$2 \int (u^{3/2}+u^{1/2}) \, du$$ (here the du simply replaces the dx because our variable changed) In another example: $$\int 4x^5(x^2+1)^{1/3} \, dx$$ $$u=x^3+1$$ $$du=3x^2$$ therefore it becomes: $$\int 4(u-1)(du/3)(u)^{1/3}\,$$ -here my teacher didn't put $d$x at the end, she just left it off So my question is this: why is it that sometimes $dx$ and $du$ are treated as values that can be multiplied to other terms in the integrand and sometimes they are simply treated as a command (do ""blank"" with respect to $x$, or $u$ or whatever is used)?",,"['calculus', 'integration', 'derivatives']"
