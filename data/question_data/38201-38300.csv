,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Finding subgroups of a free group with a specific index,Finding subgroups of a free group with a specific index,,How many subgroups with index two are there of a free group on two generators? What are their generators? All I know is that the subgroups should have $(2 \times 2) + 1 - 2 = 3$ generators.,How many subgroups with index two are there of a free group on two generators? What are their generators? All I know is that the subgroups should have $(2 \times 2) + 1 - 2 = 3$ generators.,,['abstract-algebra']
1,Are associates unit multiples in a commutative ring with $1$?,Are associates unit multiples in a commutative ring with ?,1,"Recall the following relevant definitions. We say that $b$ is divisible by $a$ in $R$ , or $a\mid b$ in $R$ , if $b = r a$ for some $r\in R$ . $a$ and $b$ are associates in $R$ if $a\mid b$ and $b\mid a$ , (or, equivalently, if $aR = bR$ ). $u\in R$ is a unit if it has a multiplicative inverse (a $v\in R$ such that $uv=vu=1$ ). $a$ and $b$ are unit multiples in $R$ if $a = ub$ for some unit $u\in R$ . Given these definitions, my question is, If $R$ is a commutative ring with unity and $a,b\in R$ are associates in $R$ , are $a$ and $b$ unit multiples in $R$ ? I was told that this not always true. But I encountered some difficulties in finding a counterexample.","Recall the following relevant definitions. We say that is divisible by in , or in , if for some . and are associates in if and , (or, equivalently, if ). is a unit if it has a multiplicative inverse (a such that ). and are unit multiples in if for some unit . Given these definitions, my question is, If is a commutative ring with unity and are associates in , are and unit multiples in ? I was told that this not always true. But I encountered some difficulties in finding a counterexample.","b a R a\mid b R b = r a r\in R a b R a\mid b b\mid a aR = bR u\in R v\in R uv=vu=1 a b R a = ub u\in R R a,b\in R R a b R","['abstract-algebra', 'ring-theory', 'commutative-algebra', 'divisibility']"
2,Is it bad to keep aside Lang's Algebra in graduate school? [closed],Is it bad to keep aside Lang's Algebra in graduate school? [closed],,"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed last year . Improve this question Question is as it is stated in title. I will be joining for PhD program in this July 2014. I am interested in working in Algebra/Algebraic Geometry/Algebraic Number Theory. I tried to learn algebra from Serge Lang's book (some two and half years back), but due to lack of background, I could not understand a bit of it, and I lost interest. I always wanted to read it, but because I could not understand anything in it, and because most of my seniors keep saying ""Lang is difficult,"" I lost interest and hope in reading that. I easily get irritated by seeing that book . One of my friend gave me his copy of Abstract Algebra by Dummit and Foote. It was totally different from Lang, and I was comfortable reading that.  Now I have done almost all exercises in three fourths of the book (with help of MSE). The curriculum for coursework in the coming year is: Review of field and Galois theory: solvable and radical extensions, Kummer theory, Galois cohomology and Hilbert's Theorem 90, Normal Basis theorem. Infinite Galois extensions: Krull topology, projective limits, profinite groups, Fundamental Theorem of Galois theory for infinite extensions. Review of integral ring extensions: integral Galois extensions, prime ideals in integral ring extensions, decomposition and inertia groups, ramification index and residue class degree, Frobenius map, Dedekind domains, unique factorisation of ideals. Categories and functors: definitions and examples. Functors and natural transformations, equivalence of categories,. Products and coproducts, the hom functor, representable functors, universals and adjoints. Direct and inverse limits. Free objects. Homological algebra: Additive and abelian categories, Complexes and homology, long exact sequences, homotopy, resolutions, derived functors, Ext, Tor, cohomology of groups, extensions of groups. Valuations and completions: definitions, polynomials in complete fields (Hensel's Lemma, Krasner's Lemma), finite dimensional extensions of complete fields, local fields, discrete valuations rings. Transcendental extensions: transcendence bases, separating transcendence bases, Luroth's theorem. Derivations. Artinian and Noetherian modules, Krull-Schmidt theorem, completely reducible modules, projective modules, Wedderburn-Artin Theorem for simple rings. Representations of finite groups: complete reducibility, characters, orthogonal relations, induced modules, Frobenius reciprocity, representations of the symmetric group. The suggested book for this is S. Lang, Algebra, 3rd Ed., Addison Wesley, 1993. I do not know if I have to choose some other book or convince myself (I do not know how) to be with Lang's book.  I want to remind again that I have no motivation to. Another thing that I heard is that it is better to use Lang as reference book than a textbook for a course. I am in a confused state.","Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed last year . Improve this question Question is as it is stated in title. I will be joining for PhD program in this July 2014. I am interested in working in Algebra/Algebraic Geometry/Algebraic Number Theory. I tried to learn algebra from Serge Lang's book (some two and half years back), but due to lack of background, I could not understand a bit of it, and I lost interest. I always wanted to read it, but because I could not understand anything in it, and because most of my seniors keep saying ""Lang is difficult,"" I lost interest and hope in reading that. I easily get irritated by seeing that book . One of my friend gave me his copy of Abstract Algebra by Dummit and Foote. It was totally different from Lang, and I was comfortable reading that.  Now I have done almost all exercises in three fourths of the book (with help of MSE). The curriculum for coursework in the coming year is: Review of field and Galois theory: solvable and radical extensions, Kummer theory, Galois cohomology and Hilbert's Theorem 90, Normal Basis theorem. Infinite Galois extensions: Krull topology, projective limits, profinite groups, Fundamental Theorem of Galois theory for infinite extensions. Review of integral ring extensions: integral Galois extensions, prime ideals in integral ring extensions, decomposition and inertia groups, ramification index and residue class degree, Frobenius map, Dedekind domains, unique factorisation of ideals. Categories and functors: definitions and examples. Functors and natural transformations, equivalence of categories,. Products and coproducts, the hom functor, representable functors, universals and adjoints. Direct and inverse limits. Free objects. Homological algebra: Additive and abelian categories, Complexes and homology, long exact sequences, homotopy, resolutions, derived functors, Ext, Tor, cohomology of groups, extensions of groups. Valuations and completions: definitions, polynomials in complete fields (Hensel's Lemma, Krasner's Lemma), finite dimensional extensions of complete fields, local fields, discrete valuations rings. Transcendental extensions: transcendence bases, separating transcendence bases, Luroth's theorem. Derivations. Artinian and Noetherian modules, Krull-Schmidt theorem, completely reducible modules, projective modules, Wedderburn-Artin Theorem for simple rings. Representations of finite groups: complete reducibility, characters, orthogonal relations, induced modules, Frobenius reciprocity, representations of the symmetric group. The suggested book for this is S. Lang, Algebra, 3rd Ed., Addison Wesley, 1993. I do not know if I have to choose some other book or convince myself (I do not know how) to be with Lang's book.  I want to remind again that I have no motivation to. Another thing that I heard is that it is better to use Lang as reference book than a textbook for a course. I am in a confused state.",,"['abstract-algebra', 'reference-request']"
3,How many idempotent elements does the ring ${\bf Z}_n$ contain?,How many idempotent elements does the ring  contain?,{\bf Z}_n,"Let $R$ be a ring. An element $x$ in $R$ is said to be idempotent if $x^2=x$. For a specific $n\in{\bf Z}_+$ which is not very large, say, $n=20$, one can calculate one by one to find that there are four idempotent elements: $x=0,1,5,16$. So here is my question: Is there a general result which tells the number of the idempotent elements of ${\bf Z}_n$?","Let $R$ be a ring. An element $x$ in $R$ is said to be idempotent if $x^2=x$. For a specific $n\in{\bf Z}_+$ which is not very large, say, $n=20$, one can calculate one by one to find that there are four idempotent elements: $x=0,1,5,16$. So here is my question: Is there a general result which tells the number of the idempotent elements of ${\bf Z}_n$?",,['abstract-algebra']
4,Does $ab=1$ imply $ba=1$ in a ring?,Does  imply  in a ring?,ab=1 ba=1,"Let $R$ be a ring with unity $1$ and let $a,b\in R$ be not zero divisor s. Is there any counterexample for: $$ab=1\quad\Rightarrow \quad ba=1$$","Let $R$ be a ring with unity $1$ and let $a,b\in R$ be not zero divisor s. Is there any counterexample for: $$ab=1\quad\Rightarrow \quad ba=1$$",,['abstract-algebra']
5,Any Subgroup containing commutator subgroup is normal.,Any Subgroup containing commutator subgroup is normal.,,"I can prove that commutator is minimal subgroup such that factor group of it is abelian. I had encountered one statement as If $H$ is a subgroup containing commutator subgroup then $H$ is   normal. I.e. we have to show that $\forall g\in G$ such that $gHg^{-1}=H$ with fact that $G'\subset H$ It is for elements in $G'$ to show condition for normality. But how to do for elements not in $G'$ but in $H$ , that is in $H\setminus G'$ ?","I can prove that commutator is minimal subgroup such that factor group of it is abelian. I had encountered one statement as If is a subgroup containing commutator subgroup then is   normal. I.e. we have to show that such that with fact that It is for elements in to show condition for normality. But how to do for elements not in but in , that is in ?",H H \forall g\in G gHg^{-1}=H G'\subset H G' G' H H\setminus G',"['abstract-algebra', 'group-theory', 'normal-subgroups', 'derived-subgroup']"
6,Are certain integer functions well-defined modulo different primes necessarily polynomials?,Are certain integer functions well-defined modulo different primes necessarily polynomials?,,"Call a function $f : \mathbb Z \to \mathbb Z$ consistent if for every prime $p$ and integer $a, b$, when $a \equiv b \pmod p$ then $f(a) \equiv f(b) \pmod p$. The set $C$ of consistent functions is closed under addition, subtraction, composition, translation, and finite difference, and contains all univariate polynomials. Does $C$ contain only univariate polynomials, i.e. $C = \mathbb Z[x]$? My intuition is that this must be the case. Since $f$ is well-defined $\mod p$ for every prime $p$, then I feel that $f$ must be defined based only on ring operations generically, so that the same definition of $f$ (with ring operations) works for any ring $\mathbb Z / p\mathbb Z$. Since the ring operations include only using 0, 1, and the variable $x$, addition, multiplication, that would mean that $f$ must be a polynomial in $x$ with integer coefficients. Is this indeed the case?","Call a function $f : \mathbb Z \to \mathbb Z$ consistent if for every prime $p$ and integer $a, b$, when $a \equiv b \pmod p$ then $f(a) \equiv f(b) \pmod p$. The set $C$ of consistent functions is closed under addition, subtraction, composition, translation, and finite difference, and contains all univariate polynomials. Does $C$ contain only univariate polynomials, i.e. $C = \mathbb Z[x]$? My intuition is that this must be the case. Since $f$ is well-defined $\mod p$ for every prime $p$, then I feel that $f$ must be defined based only on ring operations generically, so that the same definition of $f$ (with ring operations) works for any ring $\mathbb Z / p\mathbb Z$. Since the ring operations include only using 0, 1, and the variable $x$, addition, multiplication, that would mean that $f$ must be a polynomial in $x$ with integer coefficients. Is this indeed the case?",,['abstract-algebra']
7,Polynomial with infinitely many zeros. [closed],Polynomial with infinitely many zeros. [closed],,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 22 days ago . Improve this question Can a polynomial in $ \mathbb{C}[x,y] $ have infinitely many zeros? This is clearly not true in the one-variable case, but what happens in two or more variables?","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 22 days ago . Improve this question Can a polynomial in $ \mathbb{C}[x,y] $ have infinitely many zeros? This is clearly not true in the one-variable case, but what happens in two or more variables?",,"['abstract-algebra', 'polynomials']"
8,Does localization preserve reducedness?,Does localization preserve reducedness?,,Is the localization of a reduced ring (no nilpotents) still reduced?,Is the localization of a reduced ring (no nilpotents) still reduced?,,"['abstract-algebra', 'ring-theory']"
9,Looking for a book that discusses differential topology/geometry from a heavy algebra/ category theory point of view,Looking for a book that discusses differential topology/geometry from a heavy algebra/ category theory point of view,,"So I am trying to learn differential geometry, stuff like manifolds, Lie groups, Stokes' theorem and so on. I have read about many books which discuss these topics, such as Lee's Smooth Manifolds and Tu's Introduction to Manifolds. But I have not seen any that is heavy in category theory or algebra. For example, Lee briefly introduces some basic category theory but it's only in a small section and he never mentions it again. I want a book that really uses category theory. The same with algebra. Most of these books try to minimize the algebra as much as possible. For example, Lee develops tensor products but I am pretty sure it is not in a very general/developed form. So is there a book which introduces differential geometry/topology from an algebraic / category theoretic point of view.","So I am trying to learn differential geometry, stuff like manifolds, Lie groups, Stokes' theorem and so on. I have read about many books which discuss these topics, such as Lee's Smooth Manifolds and Tu's Introduction to Manifolds. But I have not seen any that is heavy in category theory or algebra. For example, Lee briefly introduces some basic category theory but it's only in a small section and he never mentions it again. I want a book that really uses category theory. The same with algebra. Most of these books try to minimize the algebra as much as possible. For example, Lee develops tensor products but I am pretty sure it is not in a very general/developed form. So is there a book which introduces differential geometry/topology from an algebraic / category theoretic point of view.",,"['abstract-algebra', 'geometry', 'reference-request', 'category-theory', 'book-recommendation']"
10,Algebraic structure cheat sheet anyone?,Algebraic structure cheat sheet anyone?,,"Has anyone ever come across a good cheat sheet for a list of definitions for the various algebraic structures out there, i.e. groups, fields, rings etc. Every time I come across the name of some structure, I have to look it up on wikipedia just to be sure I'm thinking of the right one, figured it would be cool to print out a cheat sheet and hang it on a wall nearby. The table on the algebraic structure article on wikipedia is almost what I want, however it's a bit cryptic and lacks some structures, e.g. that of a vector space or a module.","Has anyone ever come across a good cheat sheet for a list of definitions for the various algebraic structures out there, i.e. groups, fields, rings etc. Every time I come across the name of some structure, I have to look it up on wikipedia just to be sure I'm thinking of the right one, figured it would be cool to print out a cheat sheet and hang it on a wall nearby. The table on the algebraic structure article on wikipedia is almost what I want, however it's a bit cryptic and lacks some structures, e.g. that of a vector space or a module.",,['abstract-algebra']
11,Find all subgroups of $\mathbb{Z}\times\mathbb{Z}$.,Find all subgroups of .,\mathbb{Z}\times\mathbb{Z},"Find all subgroups of $\mathbb{Z}\times\mathbb{Z}$. I can find all the infinite subgroups of the form $n\mathbb{Z}\times m\mathbb{Z}$, where $n,m$ run over $\mathbb{Z}$. But I don't know how to write out all the finite subgroups in a uniform expression. Any suggestions? Thanks in advance.","Find all subgroups of $\mathbb{Z}\times\mathbb{Z}$. I can find all the infinite subgroups of the form $n\mathbb{Z}\times m\mathbb{Z}$, where $n,m$ run over $\mathbb{Z}$. But I don't know how to write out all the finite subgroups in a uniform expression. Any suggestions? Thanks in advance.",,"['abstract-algebra', 'group-theory']"
12,Prove that there is no group $G$ s.t. $\operatorname{Aut}(G)=\mathbb{Q}$,Prove that there is no group  s.t.,G \operatorname{Aut}(G)=\mathbb{Q},Prove that there is no group $G$ s.t. $\operatorname{Aut}(G)=\mathbb{Q}$ I get the feeling that we should proceed by contradiction. So let $G$ be a group s.t. $\operatorname{Aut}(G)=\mathbb{Q}$ . Then we can identify elements of $\mathbb{Q}$ with automorphisms of $G$ ... and identities such as $\frac{1}{2}*2(g)=1(g)=g$ Can somebody help me find a contradiction?,Prove that there is no group s.t. I get the feeling that we should proceed by contradiction. So let be a group s.t. . Then we can identify elements of with automorphisms of ... and identities such as Can somebody help me find a contradiction?,G \operatorname{Aut}(G)=\mathbb{Q} G \operatorname{Aut}(G)=\mathbb{Q} \mathbb{Q} G \frac{1}{2}*2(g)=1(g)=g,['abstract-algebra']
13,Is it possible to construct an ordered field which is also algebraically closed?,Is it possible to construct an ordered field which is also algebraically closed?,,"It is well known that while the real numbers are totally ordered, they are not algebraically closed, and while the complex numbers are algebraically closed, they are not totally ordered. Is it possible to construct a totally ordered algebraically closed field? If so, an example would be appreciated. Thanks in advance!","It is well known that while the real numbers are totally ordered, they are not algebraically closed, and while the complex numbers are algebraically closed, they are not totally ordered. Is it possible to construct a totally ordered algebraically closed field? If so, an example would be appreciated. Thanks in advance!",,"['abstract-algebra', 'field-theory', 'complex-numbers', 'order-theory']"
14,Projective module over a PID is free? [duplicate],Projective module over a PID is free? [duplicate],,"This question already has an answer here : Submodule of free module over a p.i.d. is free even when the module is not finitely generated? (1 answer) Closed 10 years ago . A common result is that finitely generated modules over a PID $R$ are projective iff they are free. Is the same true that an arbitrary projective module over a PID is free? I can't find this fact anywhere, so I suspect it is false, but I can't construct an example. Does anyone have an example of a projective module over a PID which is not free? Thank you.","This question already has an answer here : Submodule of free module over a p.i.d. is free even when the module is not finitely generated? (1 answer) Closed 10 years ago . A common result is that finitely generated modules over a PID $R$ are projective iff they are free. Is the same true that an arbitrary projective module over a PID is free? I can't find this fact anywhere, so I suspect it is false, but I can't construct an example. Does anyone have an example of a projective module over a PID which is not free? Thank you.",,"['abstract-algebra', 'commutative-algebra', 'modules', 'principal-ideal-domains', 'projective-module']"
15,"If H is a subgroup of G, then H has no more Sylow subgroups than G","If H is a subgroup of G, then H has no more Sylow subgroups than G",,"If $H$ is a subgroup of the finite group $G$, then how do I show that $n_p(H) \leq n_p(G)$? Here $n_p(X)$ is the number of Sylow $p$-subgroups in the finite group $X$. Here is my attempt: Suppose the order of $G$ is $n$, the order of $H$ is $m$, then $m$ divides $n$ by Lagrange's theorem. By Sylow's counting theorem $n_p(H) = 1 \pmod p$ and $n_p(H)$ divides $m$, $n_p(G) =1 \pmod p$, $n_p(G)$ divides $n$. How do I continue after this?","If $H$ is a subgroup of the finite group $G$, then how do I show that $n_p(H) \leq n_p(G)$? Here $n_p(X)$ is the number of Sylow $p$-subgroups in the finite group $X$. Here is my attempt: Suppose the order of $G$ is $n$, the order of $H$ is $m$, then $m$ divides $n$ by Lagrange's theorem. By Sylow's counting theorem $n_p(H) = 1 \pmod p$ and $n_p(H)$ divides $m$, $n_p(G) =1 \pmod p$, $n_p(G)$ divides $n$. How do I continue after this?",,"['abstract-algebra', 'group-theory', 'finite-groups', 'sylow-theory']"
16,Choosing an abstract algebra text [closed],Choosing an abstract algebra text [closed],,"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 8 years ago . Improve this question I was wondering if some one here would please recommend an abstract algebra text book. In particular, I am seeking a text from among one of these: 1) Abstract Algebra by Dummit and Foote 2) Algebra by Michael Artin 3) Topics in Algebra by Herstein. I was wondering which text is best for a high school student and has great exposition and good problems. My goal is study some algebraic number theory some time later in the near future as I liked elementary number theory.","Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 8 years ago . Improve this question I was wondering if some one here would please recommend an abstract algebra text book. In particular, I am seeking a text from among one of these: 1) Abstract Algebra by Dummit and Foote 2) Algebra by Michael Artin 3) Topics in Algebra by Herstein. I was wondering which text is best for a high school student and has great exposition and good problems. My goal is study some algebraic number theory some time later in the near future as I liked elementary number theory.",,"['abstract-algebra', 'reference-request', 'book-recommendation']"
17,A group of order $p^2q$ will be abelian [duplicate],A group of order  will be abelian [duplicate],p^2q,"This question already has answers here : Finite group of order $p^2q$ (2 answers) Closed last month . This problem is not homework but, I was stuck to it when I reviewed the Sylow theorems and problems. I am really interested of finding a test in which we can examine whether a finite group of certain order is abelian or not. It tells: $G$ is a finite group of order $p^2q$ wherein $p$ and $q$ are distinct primes such that $p^2 \not\equiv 1$ (mod $q$ ) and $q \not\equiv 1$ (mod $p$ ). Then $G$ is an abelian group. We know that $n_p=1+kp$ and it must divide $p^2q$ . So, $1+kp\mid q$ and because of $q≢1$ (mod $p$ ), we get $n_p=1$ . This means that we have a unique $p$ -sylow of $G$ , for example $P$ , in the group and so is normal and ofcourse isomorphic to $\mathbb Z_{p^2}$ or $\mathbb Z_p×\mathbb Z_p$ . What should I do next? Thanks for helping me.","This question already has answers here : Finite group of order $p^2q$ (2 answers) Closed last month . This problem is not homework but, I was stuck to it when I reviewed the Sylow theorems and problems. I am really interested of finding a test in which we can examine whether a finite group of certain order is abelian or not. It tells: is a finite group of order wherein and are distinct primes such that (mod ) and (mod ). Then is an abelian group. We know that and it must divide . So, and because of (mod ), we get . This means that we have a unique -sylow of , for example , in the group and so is normal and ofcourse isomorphic to or . What should I do next? Thanks for helping me.",G p^2q p q p^2 \not\equiv 1 q q \not\equiv 1 p G n_p=1+kp p^2q 1+kp\mid q q≢1 p n_p=1 p G P \mathbb Z_{p^2} \mathbb Z_p×\mathbb Z_p,"['abstract-algebra', 'group-theory', 'finite-groups']"
18,Constructive Proof of Kronecker-Weber?,Constructive Proof of Kronecker-Weber?,,"This question is motivated by my attempt at solving Proving $2 ( \cos \frac{4\pi}{19} + \cos \frac{6\pi}{19}+\cos \frac{10\pi}{19} )$ is a root of$ \sqrt{ 4+ \sqrt{ 4 + \sqrt{ 4-x}}}=x$ Consider numbers expressible as exponential sums $$\sum_k a_k \exp(2 i \pi \theta_k),$$ with $a_k$,$\theta_k$ a finite list of rationals. These numbers are algebraic and satisfy some polynomial whose Galois group is abelian. The Kronecker-Weber theorem says the converse also holds. Given an abelian polynomial (especially quadratic or cubic), how can we solve it in terms of one of these sums? Basically I am looking for a proof of the Kronecker-Weber theorem that is constructive enough that I can compute with it.","This question is motivated by my attempt at solving Proving $2 ( \cos \frac{4\pi}{19} + \cos \frac{6\pi}{19}+\cos \frac{10\pi}{19} )$ is a root of$ \sqrt{ 4+ \sqrt{ 4 + \sqrt{ 4-x}}}=x$ Consider numbers expressible as exponential sums $$\sum_k a_k \exp(2 i \pi \theta_k),$$ with $a_k$,$\theta_k$ a finite list of rationals. These numbers are algebraic and satisfy some polynomial whose Galois group is abelian. The Kronecker-Weber theorem says the converse also holds. Given an abelian polynomial (especially quadratic or cubic), how can we solve it in terms of one of these sums? Basically I am looking for a proof of the Kronecker-Weber theorem that is constructive enough that I can compute with it.",,"['abstract-algebra', 'galois-theory', 'constructive-mathematics']"
19,"Intuition for idempotents, orthogonal idempotents?","Intuition for idempotents, orthogonal idempotents?",,"Given a ring $A$ , an element $e \in A$ is called an idempotent if one has $e^2 = e$ . If $e$ is an idempotent, then so is $1 - e$ , since $$(1 - e)^2 = 1 - 2e + e^2 = 1 - 2e + e = 1 - e.$$ Also, we have $e(1 - e) = 0$ . This is a special case of the following situation. A collection of elements $e_1, \dots, e_n \in A$ is said to be a set of orthogonal idempotents if one has $$e_i^2 = e_i \text{ and }e_ie_j = 0 \text{ for }i \neq j.$$ My question is, what is the underlying intuition behind idempotents and orthogonal idempotents? I am finding them quite hard to work with...","Given a ring , an element is called an idempotent if one has . If is an idempotent, then so is , since Also, we have . This is a special case of the following situation. A collection of elements is said to be a set of orthogonal idempotents if one has My question is, what is the underlying intuition behind idempotents and orthogonal idempotents? I am finding them quite hard to work with...","A e \in A e^2 = e e 1 - e (1 - e)^2 = 1 - 2e + e^2 = 1 - 2e + e = 1 - e. e(1 - e) = 0 e_1, \dots, e_n \in A e_i^2 = e_i \text{ and }e_ie_j = 0 \text{ for }i \neq j.","['abstract-algebra', 'ring-theory']"
20,"Why do my algebra proofs seem like ""magic""? Is this related to category theory?","Why do my algebra proofs seem like ""magic""? Is this related to category theory?",,"$\newcommand{Ab}{\operatorname{Ab}} \newcommand{Id}{\operatorname{Id}}$I'm self-studying Introduction to Topological Manifolds by John M. Lee, which includes quite a few exercises like this: 9-4(b) Let $S_1$ and $S_2$ be disjoint sets, and let $R_i$ be a subset of the free group $F(S_i)$ for $i=1,2$. Prove that $\langle S_1 \cup S_2 \mid R_1 \cup R_2 \rangle$ is a presentation of the free product group $\langle S_1 \mid R_1 \rangle * \langle S_2 \mid R_2 \rangle$. 10-17. For any groups $G_1$ and $G_2$, show that $\Ab(G_1*G_2) \cong \Ab(G_1) \oplus \Ab(G_2)$. 10-19. For any set $S$, show that the abelianization of the free group $F(S)$ is isomorphic to the free abelian group $\mathbb{Z}S$. ($*$ is free product, $\Ab$ is abelianization.) Here is my tedious proof of 10-17: For $i=1,2$, let    \begin{align*} \alpha_{i}:G_{i} & \to\Ab(G_{i}),\\ \alpha:G_{1}*G_{2} & \to\Ab(G_{1}*G_{2}),\\ j_{i}:\Ab(G_{i}) & \to\Ab(G_{1})\oplus\Ab(G_{2}),\\ k_{i}:G_{i} & \to G_{1}*G_{2} \end{align*}    be the canonical maps. There exists a homomorphism $\ell:G_{1}*G_{2}\to\Ab(G_{1})\oplus\Ab(G_{2})$   satisfying $\ell\circ k_{i}=j_{i}\circ\alpha_{i}$, and there exists   a homomorphism $\varphi:\Ab(G_{1}*G_{2})\to\Ab(G_{1})\oplus\Ab(G_{2})$   satisfying $\varphi\circ\alpha=\ell$. Also, there exist homomorphisms   $m_{i}:\Ab(G_{i})\to\Ab(G_{1}*G_{2})$ satisfying $m_{i}\circ\alpha_{i}=\alpha\circ k_{i}$,   so there exists a homomorphism $\psi:\Ab(G_{1})\oplus\Ab(G_{2})\to\Ab(G_{1}*G_{2})$   satisfying $\psi\circ j_{i}=m_{i}$. Now    $$ \varphi\circ\psi\circ j_{i}\circ\alpha_{i}=\varphi\circ m_{i}\circ\alpha_{i}=\varphi\circ\alpha\circ k_{i}=\ell\circ k_{i}=j_{i}\circ\alpha_{i}, $$    so $\varphi\circ\psi=\Id_{\Ab(G_{1})\oplus\Ab(G_{2})}$ by uniqueness.   Similarly,    $$ \psi\circ\varphi\circ\alpha\circ k_{i}=\psi\circ\ell\circ k_{i}=\psi\circ j_{i}\circ\alpha_{i}=m_{i}\circ\alpha_{i}=\alpha\circ k_{i}, $$    so $\psi\circ\varphi=\Id_{\Ab(G_{1}*G_{2})}$ by uniqueness. This is just one example - there are many other proofs which seem to follow the same pattern - use the universal properties to derive homomorphisms, compose a bunch of them together, simplify, and prove that you have isomorphisms. My questions are: Is there a name for this kind of proof? How can I understand what I'm doing? I feel like I'm just moving symbols around right now, matching up domains/codomains. Can these proofs be made less tedious? The reason I say these proofs are like ""magic"" is that everything seems to fit together perfectly when I'm composing the maps. Why does this happen?","$\newcommand{Ab}{\operatorname{Ab}} \newcommand{Id}{\operatorname{Id}}$I'm self-studying Introduction to Topological Manifolds by John M. Lee, which includes quite a few exercises like this: 9-4(b) Let $S_1$ and $S_2$ be disjoint sets, and let $R_i$ be a subset of the free group $F(S_i)$ for $i=1,2$. Prove that $\langle S_1 \cup S_2 \mid R_1 \cup R_2 \rangle$ is a presentation of the free product group $\langle S_1 \mid R_1 \rangle * \langle S_2 \mid R_2 \rangle$. 10-17. For any groups $G_1$ and $G_2$, show that $\Ab(G_1*G_2) \cong \Ab(G_1) \oplus \Ab(G_2)$. 10-19. For any set $S$, show that the abelianization of the free group $F(S)$ is isomorphic to the free abelian group $\mathbb{Z}S$. ($*$ is free product, $\Ab$ is abelianization.) Here is my tedious proof of 10-17: For $i=1,2$, let    \begin{align*} \alpha_{i}:G_{i} & \to\Ab(G_{i}),\\ \alpha:G_{1}*G_{2} & \to\Ab(G_{1}*G_{2}),\\ j_{i}:\Ab(G_{i}) & \to\Ab(G_{1})\oplus\Ab(G_{2}),\\ k_{i}:G_{i} & \to G_{1}*G_{2} \end{align*}    be the canonical maps. There exists a homomorphism $\ell:G_{1}*G_{2}\to\Ab(G_{1})\oplus\Ab(G_{2})$   satisfying $\ell\circ k_{i}=j_{i}\circ\alpha_{i}$, and there exists   a homomorphism $\varphi:\Ab(G_{1}*G_{2})\to\Ab(G_{1})\oplus\Ab(G_{2})$   satisfying $\varphi\circ\alpha=\ell$. Also, there exist homomorphisms   $m_{i}:\Ab(G_{i})\to\Ab(G_{1}*G_{2})$ satisfying $m_{i}\circ\alpha_{i}=\alpha\circ k_{i}$,   so there exists a homomorphism $\psi:\Ab(G_{1})\oplus\Ab(G_{2})\to\Ab(G_{1}*G_{2})$   satisfying $\psi\circ j_{i}=m_{i}$. Now    $$ \varphi\circ\psi\circ j_{i}\circ\alpha_{i}=\varphi\circ m_{i}\circ\alpha_{i}=\varphi\circ\alpha\circ k_{i}=\ell\circ k_{i}=j_{i}\circ\alpha_{i}, $$    so $\varphi\circ\psi=\Id_{\Ab(G_{1})\oplus\Ab(G_{2})}$ by uniqueness.   Similarly,    $$ \psi\circ\varphi\circ\alpha\circ k_{i}=\psi\circ\ell\circ k_{i}=\psi\circ j_{i}\circ\alpha_{i}=m_{i}\circ\alpha_{i}=\alpha\circ k_{i}, $$    so $\psi\circ\varphi=\Id_{\Ab(G_{1}*G_{2})}$ by uniqueness. This is just one example - there are many other proofs which seem to follow the same pattern - use the universal properties to derive homomorphisms, compose a bunch of them together, simplify, and prove that you have isomorphisms. My questions are: Is there a name for this kind of proof? How can I understand what I'm doing? I feel like I'm just moving symbols around right now, matching up domains/codomains. Can these proofs be made less tedious? The reason I say these proofs are like ""magic"" is that everything seems to fit together perfectly when I'm composing the maps. Why does this happen?",,['abstract-algebra']
21,"Show $\langle a^m \rangle \cap \langle a^n \rangle = \langle a^{\operatorname{lcm}(m, n)}\rangle$",Show,"\langle a^m \rangle \cap \langle a^n \rangle = \langle a^{\operatorname{lcm}(m, n)}\rangle","So I want to show that $\langle a^m \rangle \cap \langle a^n \rangle = \langle a^{\operatorname{lcm}(m, n)}\rangle$. My approach to this problem was to show a double containment, i.e. to show that $\langle a^m \rangle \cap \langle a^n \rangle \subseteq  \langle a^{\operatorname{lcm}(m, n)}\rangle$ and $ \langle a^{\operatorname{lcm}(m, n)}\rangle \subseteq \langle a^m \rangle \cap \langle a^n \rangle$. I would like to see a full proof for this, specifically $\langle a^m \rangle \cap \langle a^n \rangle \subseteq \langle a^{\operatorname{lcm}(m, n)}\rangle$. (I tried it with the approach of breaking it down into to cases; $a$ has infinite order and $a$ has finite order, the latter of which i would appreciate the most help on.) My approach to solving the whole problem: (I would appreciate any feedback on anything that is wrong, or a different approach to the proof.) To show the easier containment, $\langle a^{\operatorname{lcm}(m, n)}\rangle \subseteq \langle a^m \rangle \cap \langle a^n \rangle$, I did the following:  Let $l = \operatorname{lcm}(m, n)$. Let $j \in \langle a^l\rangle$, so $j = (a^l)^k = a^{lk}$ for some $k \in \mathbb Z$. Since $l$ is a multiple of $m$ and $n$ by definition, we can say $l = ms = nt$ for some $s, t \in \mathbb Z$. Now $j = a^{kl} = a^{kms} = (a^m)^{ks} \in \langle a^m\rangle$. Similarly, $j = a^{kl} = a^{knt} = (a^n)^{kt} \in \langle a^n\rangle$. Now, since $j \in \langle a^m\rangle$ and $j \in \langle a^n\rangle$, it follows that $j \in \langle a^m \rangle \cap \langle a^n \rangle$. Thus, by definition, $\langle a^{\operatorname{lcm}(m, n)}\rangle \subseteq \langle a^m \rangle \cap \langle a^n \rangle$. For the second containment, the one which I'm having more problems with, I attempted the following: Case in which $a$ is infinite: Suppose that $\vert a \vert = \infty$. Let $c \in \langle a^m \rangle \cap \langle a^n \rangle$. Then $c = a^{mx} = a^{ny}$ where $x, y \in \mathbb Z$. It follows that $a^{mx - ny} = e$ and so $mx = ny$ because if $mx  > ny$ then the difference would not be zero, and we would have an element that was finite, contradicting our hypothesis. And since $mx = ny$ we know $\operatorname{lcm}(mx, ny) = mx = ny$ and $\operatorname{lcm}(mx, ny)$ is a multiple of $\operatorname{lcm}(m, n)$. Hence $c \in \langle a^{\operatorname{lcm}(m, n)}\rangle$ and thus $\langle a^m \rangle \cap \langle a^n \rangle \subseteq  \langle a^{\operatorname{lcm}(m, n)}\rangle$. Case in which $a$ is finite: I tried starting it out the same as the previous case, but i could never reach my conclusion :(","So I want to show that $\langle a^m \rangle \cap \langle a^n \rangle = \langle a^{\operatorname{lcm}(m, n)}\rangle$. My approach to this problem was to show a double containment, i.e. to show that $\langle a^m \rangle \cap \langle a^n \rangle \subseteq  \langle a^{\operatorname{lcm}(m, n)}\rangle$ and $ \langle a^{\operatorname{lcm}(m, n)}\rangle \subseteq \langle a^m \rangle \cap \langle a^n \rangle$. I would like to see a full proof for this, specifically $\langle a^m \rangle \cap \langle a^n \rangle \subseteq \langle a^{\operatorname{lcm}(m, n)}\rangle$. (I tried it with the approach of breaking it down into to cases; $a$ has infinite order and $a$ has finite order, the latter of which i would appreciate the most help on.) My approach to solving the whole problem: (I would appreciate any feedback on anything that is wrong, or a different approach to the proof.) To show the easier containment, $\langle a^{\operatorname{lcm}(m, n)}\rangle \subseteq \langle a^m \rangle \cap \langle a^n \rangle$, I did the following:  Let $l = \operatorname{lcm}(m, n)$. Let $j \in \langle a^l\rangle$, so $j = (a^l)^k = a^{lk}$ for some $k \in \mathbb Z$. Since $l$ is a multiple of $m$ and $n$ by definition, we can say $l = ms = nt$ for some $s, t \in \mathbb Z$. Now $j = a^{kl} = a^{kms} = (a^m)^{ks} \in \langle a^m\rangle$. Similarly, $j = a^{kl} = a^{knt} = (a^n)^{kt} \in \langle a^n\rangle$. Now, since $j \in \langle a^m\rangle$ and $j \in \langle a^n\rangle$, it follows that $j \in \langle a^m \rangle \cap \langle a^n \rangle$. Thus, by definition, $\langle a^{\operatorname{lcm}(m, n)}\rangle \subseteq \langle a^m \rangle \cap \langle a^n \rangle$. For the second containment, the one which I'm having more problems with, I attempted the following: Case in which $a$ is infinite: Suppose that $\vert a \vert = \infty$. Let $c \in \langle a^m \rangle \cap \langle a^n \rangle$. Then $c = a^{mx} = a^{ny}$ where $x, y \in \mathbb Z$. It follows that $a^{mx - ny} = e$ and so $mx = ny$ because if $mx  > ny$ then the difference would not be zero, and we would have an element that was finite, contradicting our hypothesis. And since $mx = ny$ we know $\operatorname{lcm}(mx, ny) = mx = ny$ and $\operatorname{lcm}(mx, ny)$ is a multiple of $\operatorname{lcm}(m, n)$. Hence $c \in \langle a^{\operatorname{lcm}(m, n)}\rangle$ and thus $\langle a^m \rangle \cap \langle a^n \rangle \subseteq  \langle a^{\operatorname{lcm}(m, n)}\rangle$. Case in which $a$ is finite: I tried starting it out the same as the previous case, but i could never reach my conclusion :(",,"['abstract-algebra', 'group-theory', 'gcd-and-lcm', 'cyclic-groups']"
22,Division algorithm for multivariate polynomials?,Division algorithm for multivariate polynomials?,,"We know that if $F$ is a field and $f(X)$ a non-zero polynomial in $F[X]$, then for every polynomial $g(X)$ we can find $q(X),r(X)$ such that $$g(X)=f(X)\cdot q(X)+r(X)$$ with $r(X)$ the zero polynomial or $\deg r(X)<\deg f(X)$. My question is: the same algorithm holds for many variables? So can i write $$g(X_1,\ldots,X_n)=f(X_1,\ldots,X_n)\cdot q(X_1,\ldots,X_n)+r(X_1,\ldots,X_n)$$","We know that if $F$ is a field and $f(X)$ a non-zero polynomial in $F[X]$, then for every polynomial $g(X)$ we can find $q(X),r(X)$ such that $$g(X)=f(X)\cdot q(X)+r(X)$$ with $r(X)$ the zero polynomial or $\deg r(X)<\deg f(X)$. My question is: the same algorithm holds for many variables? So can i write $$g(X_1,\ldots,X_n)=f(X_1,\ldots,X_n)\cdot q(X_1,\ldots,X_n)+r(X_1,\ldots,X_n)$$",,"['abstract-algebra', 'polynomials', 'ring-theory', 'polynomial-rings', 'multivariate-polynomial']"
23,Lüroth's Theorem,Lüroth's Theorem,,"I have just begun to read Shafarevich's Basic Algebraic Geometry . In the first section of the first chapter, he quotes Lüroth's theorem, which states that any subfield of $k(x)$ that is not just $k$ is isomorphic to $k(x)$, i.e. is generated as a field over $k$ by a single rational function of $x$. I have been trying to find a proof. I am stuck, and would appreciate any hints to fill in the argument. (I have consulted Wikipedia, Wolfram Mathworld, and this MathOverflow question , but so far haven't been able to satisfy myself.) I have thought about two approaches so far. My question would be answered by a suggestion about how to complete either one of these ideas. Here they are: Let $k\subset L \subset k(x)$ be an intermediate field not equal to $k$. Approach #1: Any element of $k(x)$ not in $k$ is transcendental over $k$; meanwhile, $k(x)$ has transcendence degree 1 over $k$; it follows that $L$ has transcendence degree 1 over $k$. Thus $k(x)$ is algebraic over $L$. Let $p(t)$ be the minimal polynomial of $x$ over $L$. $$p(t)=t^n+l_1t^{n-1}+\dots+l_n$$ where $l_1,\dots,l_n\in L$ (and are thus rational functions of $x$). Now if the theorem is really true, $L=k(f)$ for some $f\in k(x)$; and $f=r/s$, with $r,s\in k[x]$. Then $p(t)=r(t)-fs(t)$. This is degree $n=\max(\deg r,\deg s)$ in $t$. Any coefficient of any power of $t$ in $p(t)$ is actually either in $k$ (if this power of $t$ does not appear in $s$), or else it is a linear function of $f$ and thus a field generator for $L$ and degree $n$ as a rational function of $x$. Thus I expect to be able to prove that, with $p(t)$ defined as above, actually any of the coefficients $l_1,\dots,l_n$ not contained in $k$, i.e. any of them (say $l_i$) that is a nonconstant function of $x$, is degree $n$ as a function of $x$ and is thus a field generator for $L$. (It would be sufficient to prove that it is degree $n$ as a function of $x$, because then $k(x)\supset L \supset k(l_i)$, but $[k(x):L]=[k(x):k(l_i)]=n$.) One internet source I found suggested that this is the right approach, but I can't seem to fill it in. Here's what I've got: $p(t)$ is divisible by $t-x$ over $k(x)$ (since $x$ is a root), and over $k(l_1,\dots,l_n)$ it is irreducible (since this field is contained in $L$). I can't see that there is anything else I know about it for sure. It must be that irreducibility over $k(l_1,\dots,l_n)$ implies that $l_1,\dots,l_n$ are all either degree $n$ or else in $k$; but I haven't figured out how. From examples I have worked out (in which I chose $l_1,\dots,l_n$ semi-arbitrarily to fulfill $(t-x)\mid p(t)$), this seems to be true; if I make any of them different in degree from $0$ or $n$, then usually I can also get $x$ as a rational function of them, thus in these examples $k(l_1,\dots,l_n)=k(x)$ and $p(t)$ is divisible by $t-x$ over $k(l_1,\dots,l_n)$. Of course I assume it can also happen that I choose $l_1,\dots,l_n$ so that $k(l_1,\dots,l_n)\neq k(x)$, but $p(x)$ will still factor over $k(l_1,\dots,l_n)$ as long as any of the $l_i$ not in $k$ differ in degree from $n$. In any case all the calculations have felt ad-hoc and I haven't so far seen a reason for what is happening. So any hints here would be appreciated. Approach #2: Because the theorem reminds me of the result that $k[x]$ is a p.i.d., I have also been unable to escape the following thought: let $f\in L$ be an element of $L$ of minimal degree as a function of $x$, and suppose that there is some other element $g\in L$ not in $k(f)$. Can I construct some element of $L$ using $f$ and $g$ (i.e. an element of $k(f,g)$) that contradicts $f$'s minimality in degree? I have not given this approach as much thought as the above, but again, so far I have not seen how to carry out the construction. The Euclidean-algorithm trick that proves $k[x]$ is a p.i.d. is unavailable here because I can't multiply $f$ or $g$ by anything that is not a rational function of one or the other of them. (In particular I can't see how to pass to a polynomial ring in $x$ but make sure I've stayed inside $k(f,g)$.) $g$ does have a minimal polynomial over $k(f)$, and if $g\notin k(f)$ then its degree is $>1$, so this could be a starting point for trying to construct the lower-degree element of $k(f,g)$, but again I haven't seen how to make this work. So here again, I would appreciate any thought that could be used to complete the argument. Thanks in advance!","I have just begun to read Shafarevich's Basic Algebraic Geometry . In the first section of the first chapter, he quotes Lüroth's theorem, which states that any subfield of $k(x)$ that is not just $k$ is isomorphic to $k(x)$, i.e. is generated as a field over $k$ by a single rational function of $x$. I have been trying to find a proof. I am stuck, and would appreciate any hints to fill in the argument. (I have consulted Wikipedia, Wolfram Mathworld, and this MathOverflow question , but so far haven't been able to satisfy myself.) I have thought about two approaches so far. My question would be answered by a suggestion about how to complete either one of these ideas. Here they are: Let $k\subset L \subset k(x)$ be an intermediate field not equal to $k$. Approach #1: Any element of $k(x)$ not in $k$ is transcendental over $k$; meanwhile, $k(x)$ has transcendence degree 1 over $k$; it follows that $L$ has transcendence degree 1 over $k$. Thus $k(x)$ is algebraic over $L$. Let $p(t)$ be the minimal polynomial of $x$ over $L$. $$p(t)=t^n+l_1t^{n-1}+\dots+l_n$$ where $l_1,\dots,l_n\in L$ (and are thus rational functions of $x$). Now if the theorem is really true, $L=k(f)$ for some $f\in k(x)$; and $f=r/s$, with $r,s\in k[x]$. Then $p(t)=r(t)-fs(t)$. This is degree $n=\max(\deg r,\deg s)$ in $t$. Any coefficient of any power of $t$ in $p(t)$ is actually either in $k$ (if this power of $t$ does not appear in $s$), or else it is a linear function of $f$ and thus a field generator for $L$ and degree $n$ as a rational function of $x$. Thus I expect to be able to prove that, with $p(t)$ defined as above, actually any of the coefficients $l_1,\dots,l_n$ not contained in $k$, i.e. any of them (say $l_i$) that is a nonconstant function of $x$, is degree $n$ as a function of $x$ and is thus a field generator for $L$. (It would be sufficient to prove that it is degree $n$ as a function of $x$, because then $k(x)\supset L \supset k(l_i)$, but $[k(x):L]=[k(x):k(l_i)]=n$.) One internet source I found suggested that this is the right approach, but I can't seem to fill it in. Here's what I've got: $p(t)$ is divisible by $t-x$ over $k(x)$ (since $x$ is a root), and over $k(l_1,\dots,l_n)$ it is irreducible (since this field is contained in $L$). I can't see that there is anything else I know about it for sure. It must be that irreducibility over $k(l_1,\dots,l_n)$ implies that $l_1,\dots,l_n$ are all either degree $n$ or else in $k$; but I haven't figured out how. From examples I have worked out (in which I chose $l_1,\dots,l_n$ semi-arbitrarily to fulfill $(t-x)\mid p(t)$), this seems to be true; if I make any of them different in degree from $0$ or $n$, then usually I can also get $x$ as a rational function of them, thus in these examples $k(l_1,\dots,l_n)=k(x)$ and $p(t)$ is divisible by $t-x$ over $k(l_1,\dots,l_n)$. Of course I assume it can also happen that I choose $l_1,\dots,l_n$ so that $k(l_1,\dots,l_n)\neq k(x)$, but $p(x)$ will still factor over $k(l_1,\dots,l_n)$ as long as any of the $l_i$ not in $k$ differ in degree from $n$. In any case all the calculations have felt ad-hoc and I haven't so far seen a reason for what is happening. So any hints here would be appreciated. Approach #2: Because the theorem reminds me of the result that $k[x]$ is a p.i.d., I have also been unable to escape the following thought: let $f\in L$ be an element of $L$ of minimal degree as a function of $x$, and suppose that there is some other element $g\in L$ not in $k(f)$. Can I construct some element of $L$ using $f$ and $g$ (i.e. an element of $k(f,g)$) that contradicts $f$'s minimality in degree? I have not given this approach as much thought as the above, but again, so far I have not seen how to carry out the construction. The Euclidean-algorithm trick that proves $k[x]$ is a p.i.d. is unavailable here because I can't multiply $f$ or $g$ by anything that is not a rational function of one or the other of them. (In particular I can't see how to pass to a polynomial ring in $x$ but make sure I've stayed inside $k(f,g)$.) $g$ does have a minimal polynomial over $k(f)$, and if $g\notin k(f)$ then its degree is $>1$, so this could be a starting point for trying to construct the lower-degree element of $k(f,g)$, but again I haven't seen how to make this work. So here again, I would appreciate any thought that could be used to complete the argument. Thanks in advance!",,"['abstract-algebra', 'algebraic-geometry', 'field-theory', 'rational-functions']"
24,Difference between abstract algebra and universal algebra,Difference between abstract algebra and universal algebra,,"Wikipedia give this answer ""Universal algebra (sometimes called general algebra) is the field of mathematics that studies algebraic structures themselves, not examples (""models"") of algebraic structures. For instance, rather than take particular groups as the object of study, in universal algebra one takes ""the theory of groups"" as an object of study."" Can someone please explain the difference in more specific terms.","Wikipedia give this answer ""Universal algebra (sometimes called general algebra) is the field of mathematics that studies algebraic structures themselves, not examples (""models"") of algebraic structures. For instance, rather than take particular groups as the object of study, in universal algebra one takes ""the theory of groups"" as an object of study."" Can someone please explain the difference in more specific terms.",,"['abstract-algebra', 'universal-algebra']"
25,Do automorphisms of a finitely generated free module restrict to automorphisms on every invariant submodule?,Do automorphisms of a finitely generated free module restrict to automorphisms on every invariant submodule?,,"Let $R$ be a (commutative) ring (with 1) and $M$ a free and finitely generated $R$-module. Let $\gamma$ be an $R$-module automorphism of $M$. If $N\subset M$ is a submodule invariant under $\gamma$, then is $\gamma|_N$ an automorphism of $N$? If this is false, are there reasonable conditions under which it would be true? (e.g., $R$ noetherian...etc?)","Let $R$ be a (commutative) ring (with 1) and $M$ a free and finitely generated $R$-module. Let $\gamma$ be an $R$-module automorphism of $M$. If $N\subset M$ is a submodule invariant under $\gamma$, then is $\gamma|_N$ an automorphism of $N$? If this is false, are there reasonable conditions under which it would be true? (e.g., $R$ noetherian...etc?)",,['abstract-algebra']
26,How to recognize a finitely generated abelian group as a product of cyclic groups.,How to recognize a finitely generated abelian group as a product of cyclic groups.,,"Let $G$ be the quotient group $G=\mathbb{Z}^5/N$, where $N$ is generated by $(6,0,-3,0,3)$ and $(0,0,8,4,2)$. Recognize $G$ as a product of cyclic groups. Honestly, I do not know how to solve these type of problems. But I know that this is somehow an application of Fundamental theorem of finitely generated abelian groups. That theorem states an existence of such a product as $\mathbb{Z}^r\times \mathbb{Z}_{n_1}\times ... \times \mathbb{Z}_{n_s}$, but does not states a way to find $r,n_1,...,n_s$. I know how to use this theorem for a finite abelian group. But could not find a way to solve these type of problems even in a book. Could somebody explain me?","Let $G$ be the quotient group $G=\mathbb{Z}^5/N$, where $N$ is generated by $(6,0,-3,0,3)$ and $(0,0,8,4,2)$. Recognize $G$ as a product of cyclic groups. Honestly, I do not know how to solve these type of problems. But I know that this is somehow an application of Fundamental theorem of finitely generated abelian groups. That theorem states an existence of such a product as $\mathbb{Z}^r\times \mathbb{Z}_{n_1}\times ... \times \mathbb{Z}_{n_s}$, but does not states a way to find $r,n_1,...,n_s$. I know how to use this theorem for a finite abelian group. But could not find a way to solve these type of problems even in a book. Could somebody explain me?",,"['abstract-algebra', 'group-theory', 'abelian-groups']"
27,Can a nonzero polynomial evaluate to the zero function in a suitable infinite ring of char 0?,Can a nonzero polynomial evaluate to the zero function in a suitable infinite ring of char 0?,,"I shall assume all rings to be commutative in this question. The impatient can scroll down to the ""blockquote"" to read the actual question. Whenever we have a polynomial over a ring, it defines a function from the ring to itself by evaluation. It's reasonable to ask when two different polynomials define the same function. From the factor theorem it follows that an $n^\text{th}$ degree polynomial over an integral domain has at most $n$ roots. Then it's easy to show this: Theorem. Let $R$ be an infinite integral domain and let $f \in R[X]$ such that $f(a)=0$ for all $a \in R$, then $f = 0$. Proof. $f$ has infinitely many roots, so it must be the zero polynomial. $\quad\square$ For finite rings a kind of opposite situation occurs: Theorem. For any finite ring $R$ there are polynomials over $R$ that are different but agree on all elements. Proof. There are only finitely many functions from $R$ to itself, but $R[X]$ is infinite. $\quad\square$ If we make further assumptions it's of course possible to prove more, as Pete L. Clark wrote in this post: [ 1 ] Then there is the question of infinite rings that are not integral domains. It's relatively easy to come up with examples of a ring $R$ with positive characteristic and a nonzero polynomial that evaluates to the zero function, e.g.: $$ R := \bigoplus_{n=1}^\infty \mathbb{Z}/6\mathbb{Z} \quad\text{and}\quad f(X) := X^3-X.$$ The Question: This leaves open the case alluded to in this post's title: Is there a commutative ring of characteristic $0$ (hence infinite) such that a nonzero polynomial evaluates to the zero function?","I shall assume all rings to be commutative in this question. The impatient can scroll down to the ""blockquote"" to read the actual question. Whenever we have a polynomial over a ring, it defines a function from the ring to itself by evaluation. It's reasonable to ask when two different polynomials define the same function. From the factor theorem it follows that an $n^\text{th}$ degree polynomial over an integral domain has at most $n$ roots. Then it's easy to show this: Theorem. Let $R$ be an infinite integral domain and let $f \in R[X]$ such that $f(a)=0$ for all $a \in R$, then $f = 0$. Proof. $f$ has infinitely many roots, so it must be the zero polynomial. $\quad\square$ For finite rings a kind of opposite situation occurs: Theorem. For any finite ring $R$ there are polynomials over $R$ that are different but agree on all elements. Proof. There are only finitely many functions from $R$ to itself, but $R[X]$ is infinite. $\quad\square$ If we make further assumptions it's of course possible to prove more, as Pete L. Clark wrote in this post: [ 1 ] Then there is the question of infinite rings that are not integral domains. It's relatively easy to come up with examples of a ring $R$ with positive characteristic and a nonzero polynomial that evaluates to the zero function, e.g.: $$ R := \bigoplus_{n=1}^\infty \mathbb{Z}/6\mathbb{Z} \quad\text{and}\quad f(X) := X^3-X.$$ The Question: This leaves open the case alluded to in this post's title: Is there a commutative ring of characteristic $0$ (hence infinite) such that a nonzero polynomial evaluates to the zero function?",,"['abstract-algebra', 'ring-theory']"
28,Acyclic vs Exact,Acyclic vs Exact,,"I have a question about the words ""acyclic"" and ""exact."" Why does Brown use the term ""acyclic"" instead of ""exact"" in his book Cohomology of Groups? It seems to me that these two terms exactly coincide. Are there examples(or topics in math) in which being acyclic means being sth1 and being exact means being sth2, and when restricted to the homology theory sth1 and sth2  coincide? Thank you.","I have a question about the words ""acyclic"" and ""exact."" Why does Brown use the term ""acyclic"" instead of ""exact"" in his book Cohomology of Groups? It seems to me that these two terms exactly coincide. Are there examples(or topics in math) in which being acyclic means being sth1 and being exact means being sth2, and when restricted to the homology theory sth1 and sth2  coincide? Thank you.",,"['abstract-algebra', 'homology-cohomology', 'group-cohomology']"
29,Coproduct in the category of (noncommutative) associative algebras,Coproduct in the category of (noncommutative) associative algebras,,"For the case of commutative algebras, I know that the coproduct is given by the tensor product, but how is the situation in the general case? (for associative, but not necessarily commutative algebras over a ring $A$). Does the coproduct even exist in general or if not, when does it exist? If it helps, we may assume that $A$ itself is commutative. I guess the construction would be something akin to the construction of the free products of groups in group theory, but it would be nice to see some more explicit details (but maybe that would be very messy?) I did not have much luck in finding information about it on the web anyway.","For the case of commutative algebras, I know that the coproduct is given by the tensor product, but how is the situation in the general case? (for associative, but not necessarily commutative algebras over a ring $A$). Does the coproduct even exist in general or if not, when does it exist? If it helps, we may assume that $A$ itself is commutative. I guess the construction would be something akin to the construction of the free products of groups in group theory, but it would be nice to see some more explicit details (but maybe that would be very messy?) I did not have much luck in finding information about it on the web anyway.",,"['abstract-algebra', 'ring-theory', 'category-theory', 'noncommutative-algebra']"
30,A module over an algebra. Is it a vector space?,A module over an algebra. Is it a vector space?,,"Let $A$ be an algebra over a field $k$. I would like to know if my understanding of the following correct or not. What I want to clarify is the definition of a module $M$ over $A$. I know the definition of a module over a ring. Is the definition of a module over an algebra $A$ the same as the ring theoretic definition replacing a ring by an algebra? Or, a module is a $k$-module, plus $A$ action? Or, are they the same? If it is the definition 2 above, then $M$ is a vector space over $k$. Is $M$ a vector space in the case of 1?","Let $A$ be an algebra over a field $k$. I would like to know if my understanding of the following correct or not. What I want to clarify is the definition of a module $M$ over $A$. I know the definition of a module over a ring. Is the definition of a module over an algebra $A$ the same as the ring theoretic definition replacing a ring by an algebra? Or, a module is a $k$-module, plus $A$ action? Or, are they the same? If it is the definition 2 above, then $M$ is a vector space over $k$. Is $M$ a vector space in the case of 1?",,['abstract-algebra']
31,"Fundamental Theorem of Algebra for fields other than $\Bbb{C}$, or how much does the Fundamental Theorem of Algebra depend on topology and analysis?","Fundamental Theorem of Algebra for fields other than , or how much does the Fundamental Theorem of Algebra depend on topology and analysis?",\Bbb{C},"When proving the Fundamental Theorem of Algebra, we need to appeal to analytic and/or topological properties of $\Bbb{C}$ and $\Bbb{C}[z]$. Is this going to be necessary in general, and if so, to what extent? That is, suppose a field $K$ is given, and we desire to show that $K$ is algebraically closed. Is there any amount of purely algebraic data (save that $K = F^{alg}$ for some field $F$) that will allow us to say that $K$ is algebraically closed? Of course, the phrase ""purely algebraic data"" isn't well-defined, but I loosely mean information about $K$ in relation to fields $E_i$ of which $K$ is an extension, such as the degrees of the extensions $K/E_i$, if $K = E_i(\alpha)$ for some $\alpha\in K$, their Galois groups $\operatorname{Gal}\left(K/E_i\right)$, what $\operatorname{char}K$ is, and so on (where this information isn't precisely the information that $K$ is constructed as the algebraic closure of some field). If this isn't possible, at what point does it become necessary to appeal to the topology and analysis of $K$ and $K[x]$, and how does this ""point"" depend on $K$? I realize that exactly how and where analysis and topology come into play for different $K$ will depend on the nature of the $K$ with which one is working, but I would be interested in knowing what properties of $K$ have the greatest effect on the necessity of analysis and topology in a proof that $K$ is algebraically closed. Of course, the more general these properties, the better. A note: this question deals with this idea to some extent, although the discussion there is more focused on $\Bbb{C}$, and I would like to consider a more general setting: the conditions required to show a certain type of field is algebraically closed, and how those conditions differ for different types of fields.","When proving the Fundamental Theorem of Algebra, we need to appeal to analytic and/or topological properties of $\Bbb{C}$ and $\Bbb{C}[z]$. Is this going to be necessary in general, and if so, to what extent? That is, suppose a field $K$ is given, and we desire to show that $K$ is algebraically closed. Is there any amount of purely algebraic data (save that $K = F^{alg}$ for some field $F$) that will allow us to say that $K$ is algebraically closed? Of course, the phrase ""purely algebraic data"" isn't well-defined, but I loosely mean information about $K$ in relation to fields $E_i$ of which $K$ is an extension, such as the degrees of the extensions $K/E_i$, if $K = E_i(\alpha)$ for some $\alpha\in K$, their Galois groups $\operatorname{Gal}\left(K/E_i\right)$, what $\operatorname{char}K$ is, and so on (where this information isn't precisely the information that $K$ is constructed as the algebraic closure of some field). If this isn't possible, at what point does it become necessary to appeal to the topology and analysis of $K$ and $K[x]$, and how does this ""point"" depend on $K$? I realize that exactly how and where analysis and topology come into play for different $K$ will depend on the nature of the $K$ with which one is working, but I would be interested in knowing what properties of $K$ have the greatest effect on the necessity of analysis and topology in a proof that $K$ is algebraically closed. Of course, the more general these properties, the better. A note: this question deals with this idea to some extent, although the discussion there is more focused on $\Bbb{C}$, and I would like to consider a more general setting: the conditions required to show a certain type of field is algebraically closed, and how those conditions differ for different types of fields.",,"['abstract-algebra', 'general-topology', 'analysis', 'field-theory']"
32,How can I prove that the additive group of rationals is not isomorphic to a direct product of two nontrivial groups?,How can I prove that the additive group of rationals is not isomorphic to a direct product of two nontrivial groups?,,"I am working through Paolo Aluffi's new GSM text on my own (self-study). On page 63, he asks the reader to Prove that $\Bbb{Q}$ is not the direct product of two nontrivial groups. For some context, this is an exercise following a section entitled ""The category Grp"".  I am assuming that he means ""is not isomorphic to the direct product of two nontrivial groups"", and I can see two possible ways to proceed with this proof, but have been unsuccessful with either approach. Approach 1:  Show that the additive group of rationals has a property that is preserved by isomorphism that the direct product of two nontrivial groups does not have or vice-versa.  This seems challenging unless I can significantly narrow down the properties that a direct product of two nontrivial groups that was isomorphic to $\Bbb{Q}$ would necessarily have. Approach 2:  Considering the section in which this question occurs, show that if $G$ and $H$ are nontrivial groups and $\Bbb{Q} \cong G \times H$, then there are homomorphisms $\varphi_{G}:\Bbb{Q} \rightarrow G$ and $\varphi_{H}:\Bbb{Q} \rightarrow H$ which do not factor or do not factor uniquely through the product $G \times H$.  This would be a contradiction, as $G \times H$ is a final object in the category Grp. I would greatly appreciate suggestions on how to proceed further with either of these approaches or with alternate approaches.","I am working through Paolo Aluffi's new GSM text on my own (self-study). On page 63, he asks the reader to Prove that $\Bbb{Q}$ is not the direct product of two nontrivial groups. For some context, this is an exercise following a section entitled ""The category Grp"".  I am assuming that he means ""is not isomorphic to the direct product of two nontrivial groups"", and I can see two possible ways to proceed with this proof, but have been unsuccessful with either approach. Approach 1:  Show that the additive group of rationals has a property that is preserved by isomorphism that the direct product of two nontrivial groups does not have or vice-versa.  This seems challenging unless I can significantly narrow down the properties that a direct product of two nontrivial groups that was isomorphic to $\Bbb{Q}$ would necessarily have. Approach 2:  Considering the section in which this question occurs, show that if $G$ and $H$ are nontrivial groups and $\Bbb{Q} \cong G \times H$, then there are homomorphisms $\varphi_{G}:\Bbb{Q} \rightarrow G$ and $\varphi_{H}:\Bbb{Q} \rightarrow H$ which do not factor or do not factor uniquely through the product $G \times H$.  This would be a contradiction, as $G \times H$ is a final object in the category Grp. I would greatly appreciate suggestions on how to proceed further with either of these approaches or with alternate approaches.",,"['abstract-algebra', 'group-theory']"
33,Sheaf Theory for Complex Analysis,Sheaf Theory for Complex Analysis,,"I've recently been reading Complex Analysis in One Variable by Narasimhan and Nievergelt. I've done enough work in complex analysis prior that I find the majority of the text quite understandable and beautiful; however, I was pleasantly shocked when I began reading the section entitled ""The sheaf of germs of holomorphic functions"" . After re-reading through the section a number of times, I have found I can formally follow the argument, but lack any intuition for the definitions or the mechanics. Unfortunately, my passion for analysis has meant the majority of my studies have been directed away from Algebra, and so I've only taken an introductory level course to Group Theory and Ring Theory. Nevertheless, I strongly desire to become more familiar with sheaf theoretic techniques in complex analysis. I could of course simply wait until I have taken more courses in Algebra to become acquainted with it enough to grasp Category Theory at an intuitive level and then begin chipping away at a Sheaf Theory text. However, I am presently only in my first year of college, and I don't wish to be forced to wait a few years until Grad School to appreciate basic applications of Sheaf Theory. As such, I am wondering if there is a more optimized route to learn the very basic Sheaf Theory I might need to appreciate it's applications to Analysis, and in particular single variable Complex Analysis. I imagine I will not need a complete understanding of modern Sheaf Theory to merely understand its applications to my field. As such, I am wondering what the minimally required topics are that I should study in order to build up some basic intitution for the uses of Sheaf Theory in N&N's text. Edit: Per the suggestions below, I have checked out copies of Gunning's and Forster's texts on Riemann Surfaces. My initial reaction to both texts has been quite positive; in particular I have found Gunning's text fantastic in that it provides the definition of a sheaf as soon as possible. I look forward to reading through the texts as I find time! However, I remain completely open to other perspectives on how to best attack the necessary Sheaf theory, and so other answers are welcome!","I've recently been reading Complex Analysis in One Variable by Narasimhan and Nievergelt. I've done enough work in complex analysis prior that I find the majority of the text quite understandable and beautiful; however, I was pleasantly shocked when I began reading the section entitled ""The sheaf of germs of holomorphic functions"" . After re-reading through the section a number of times, I have found I can formally follow the argument, but lack any intuition for the definitions or the mechanics. Unfortunately, my passion for analysis has meant the majority of my studies have been directed away from Algebra, and so I've only taken an introductory level course to Group Theory and Ring Theory. Nevertheless, I strongly desire to become more familiar with sheaf theoretic techniques in complex analysis. I could of course simply wait until I have taken more courses in Algebra to become acquainted with it enough to grasp Category Theory at an intuitive level and then begin chipping away at a Sheaf Theory text. However, I am presently only in my first year of college, and I don't wish to be forced to wait a few years until Grad School to appreciate basic applications of Sheaf Theory. As such, I am wondering if there is a more optimized route to learn the very basic Sheaf Theory I might need to appreciate it's applications to Analysis, and in particular single variable Complex Analysis. I imagine I will not need a complete understanding of modern Sheaf Theory to merely understand its applications to my field. As such, I am wondering what the minimally required topics are that I should study in order to build up some basic intitution for the uses of Sheaf Theory in N&N's text. Edit: Per the suggestions below, I have checked out copies of Gunning's and Forster's texts on Riemann Surfaces. My initial reaction to both texts has been quite positive; in particular I have found Gunning's text fantastic in that it provides the definition of a sheaf as soon as possible. I look forward to reading through the texts as I find time! However, I remain completely open to other perspectives on how to best attack the necessary Sheaf theory, and so other answers are welcome!",,"['abstract-algebra', 'complex-analysis', 'analysis', 'sheaf-theory']"
34,Localization commutes with Hom for finitely-generated modules,Localization commutes with Hom for finitely-generated modules,,"Let $M,N$ be $R$-modules with $M$ finitely generated and let $S\subseteq R$ be multiplicatively closed. Then there exists a module isomorphism   $$S^{-1}\text{Hom}_R(M,N) \xrightarrow{\sim} \text{Hom}_{S^{-1}R}(S^{-1}M,S^{-1}N).$$ As homework, I found a 'proof' of this statement. However, Does localisation commute with Hom for finitely-generated modules? claims to prove me wrong. I am having trouble in finding the error in my proof. Could someone help me out? I did prove there exists a map $\phi:S^{-1}\text{Hom}_R(M,N) \to\text{Hom}_{S^{-1}R}(S^{-1}M,S^{-1}N)$ defined by $f/s\mapsto(m/t\mapsto f(m)/st)$, and I am fairly confident that it contains no mistake. Proof : Let $m_1,\dotsc,m_n$ be the generators for M. Let $f\in \text{Hom}_{S^{-1}R}(S^{-1}M,S^{-1}N)$ be given.   Choose $n_i\in N$ and $s_i\in S$ such that $f(m_i/1)=n_i/s_i$.   Define $\pi_f = s_1\dotsm s_n$. Then   $$\pi f(m/1) = \pi f\left( \sum_{i=1}^n r_i m_i \right) = \pi \sum_{i=1}^n r_i f(m_i) = \pi \sum_{i=1}^n r_i n_i/s_i = \left(\sum_{i=1}\left(\prod_{j=1, j\neq i}^n s_i\right)r_in_i\right)/1,$$   so $\pi f(m/1) \in N$ using the canonical embedding in $S^{-1}N$, so $m\mapsto \pi f(m/1)$ is a map $M\to N$ and is clearly an homomorphism. If we define the map $\psi:\text{Hom}_{S^{-1}R}(S^{-1}M,S^{-1}N)\to S^{-1}\text{Hom}_{R}(M,N)$ as $$ f \mapsto (m\mapsto \pi_f f(m)) / \pi_f,$$ then we clearly have a homomorphism if it is well-defined. Proof : It remains to check that the element we map to does not depend on choice of $\pi_f$.   But if we have both $\pi_f$ and $\pi_f'$ for which $\pi_f f$ and $\pi_f' f$ are homomorphisms to $N$, then   $$(\pi_f f(m))/\pi_f = (\pi_f\pi_f' f(m))/(\pi_f\pi_f') = (\pi_f' f(m))/(\pi_f')$$   for all $m\in M$, so $\psi$ is in fact well defined. It is now easy to check that $\psi$ and $\phi$ are inverses, which makes them homomorphisms.","Let $M,N$ be $R$-modules with $M$ finitely generated and let $S\subseteq R$ be multiplicatively closed. Then there exists a module isomorphism   $$S^{-1}\text{Hom}_R(M,N) \xrightarrow{\sim} \text{Hom}_{S^{-1}R}(S^{-1}M,S^{-1}N).$$ As homework, I found a 'proof' of this statement. However, Does localisation commute with Hom for finitely-generated modules? claims to prove me wrong. I am having trouble in finding the error in my proof. Could someone help me out? I did prove there exists a map $\phi:S^{-1}\text{Hom}_R(M,N) \to\text{Hom}_{S^{-1}R}(S^{-1}M,S^{-1}N)$ defined by $f/s\mapsto(m/t\mapsto f(m)/st)$, and I am fairly confident that it contains no mistake. Proof : Let $m_1,\dotsc,m_n$ be the generators for M. Let $f\in \text{Hom}_{S^{-1}R}(S^{-1}M,S^{-1}N)$ be given.   Choose $n_i\in N$ and $s_i\in S$ such that $f(m_i/1)=n_i/s_i$.   Define $\pi_f = s_1\dotsm s_n$. Then   $$\pi f(m/1) = \pi f\left( \sum_{i=1}^n r_i m_i \right) = \pi \sum_{i=1}^n r_i f(m_i) = \pi \sum_{i=1}^n r_i n_i/s_i = \left(\sum_{i=1}\left(\prod_{j=1, j\neq i}^n s_i\right)r_in_i\right)/1,$$   so $\pi f(m/1) \in N$ using the canonical embedding in $S^{-1}N$, so $m\mapsto \pi f(m/1)$ is a map $M\to N$ and is clearly an homomorphism. If we define the map $\psi:\text{Hom}_{S^{-1}R}(S^{-1}M,S^{-1}N)\to S^{-1}\text{Hom}_{R}(M,N)$ as $$ f \mapsto (m\mapsto \pi_f f(m)) / \pi_f,$$ then we clearly have a homomorphism if it is well-defined. Proof : It remains to check that the element we map to does not depend on choice of $\pi_f$.   But if we have both $\pi_f$ and $\pi_f'$ for which $\pi_f f$ and $\pi_f' f$ are homomorphisms to $N$, then   $$(\pi_f f(m))/\pi_f = (\pi_f\pi_f' f(m))/(\pi_f\pi_f') = (\pi_f' f(m))/(\pi_f')$$   for all $m\in M$, so $\psi$ is in fact well defined. It is now easy to check that $\psi$ and $\phi$ are inverses, which makes them homomorphisms.",,"['abstract-algebra', 'commutative-algebra', 'modules', 'localization']"
35,"Let $R$ be a commutative ring. If $R[X]$ is a principal ideal domain, then $R$ is a field. [duplicate]","Let  be a commutative ring. If  is a principal ideal domain, then  is a field. [duplicate]",R R[X] R,"This question already has answers here : $\langle 2,x \rangle$ is a non-principal ideal in $\mathbb Z [x];\, $ $D[x]$ PID $\iff D$ field, for a domain $D$ (8 answers) Closed 8 months ago . I've just read a proof of the statement : Let $R$ be a commutative ring. If $R[X]$ is a principal ideal domain, then $R$ is a field. In one part of the proof there is a step which I don't understand. I'll copy the proof : Let $u\in R$ be non-zero. Then using the principal ideal property, for some $f\in R[X]$ we have $\langle u,X\rangle = \langle f \rangle \subseteq R[X]$ . Therefore, for some $p,q∈R[X], u=fp$ and $X=fq$ . By properties of degree we conclude that $f=a$ and $q=b+cX$ for some $a,b,c\in R$ . Substituting into the equation $X=fq$ we obtain $X=ab+acX$ which implies that $ac=1$ , i.e. $a\in R^\times$ , the group of units of $R$ . Therefore, $\langle f\rangle =\langle 1\rangle=R[X]$ . Therefore, there exist $r,s\in R[X]$ such that $ru+sX=1$ and if $d$ is the constant term of $r$ , then we have $ud=1$ . Therefore $u\in R^\times$ . Our choice of $u$ was arbitrary, so this shows that $R^{\times}⊇R \setminus \{0\}$ , which says precisely that $R$ is a field. I don't understand this: "" $X=ab+acX$ which implies that $ac=1$ , i.e. $a\in R^\times$ , the group of units of $R$ . Therefore, $\langle f\rangle = \langle 1\rangle =R[X]$ ."" Why does the fact that $a=f$ is a unit implies that $\langle f\rangle = \langle 1\rangle$ ? I would appreciate if someone could explain this to me. Thanks in advance.","This question already has answers here : $\langle 2,x \rangle$ is a non-principal ideal in $\mathbb Z [x];\, $ $D[x]$ PID $\iff D$ field, for a domain $D$ (8 answers) Closed 8 months ago . I've just read a proof of the statement : Let be a commutative ring. If is a principal ideal domain, then is a field. In one part of the proof there is a step which I don't understand. I'll copy the proof : Let be non-zero. Then using the principal ideal property, for some we have . Therefore, for some and . By properties of degree we conclude that and for some . Substituting into the equation we obtain which implies that , i.e. , the group of units of . Therefore, . Therefore, there exist such that and if is the constant term of , then we have . Therefore . Our choice of was arbitrary, so this shows that , which says precisely that is a field. I don't understand this: "" which implies that , i.e. , the group of units of . Therefore, ."" Why does the fact that is a unit implies that ? I would appreciate if someone could explain this to me. Thanks in advance.","R R[X] R u\in R f\in R[X] \langle u,X\rangle = \langle f \rangle \subseteq R[X] p,q∈R[X], u=fp X=fq f=a q=b+cX a,b,c\in R X=fq X=ab+acX ac=1 a\in R^\times R \langle f\rangle =\langle 1\rangle=R[X] r,s\in R[X] ru+sX=1 d r ud=1 u\in R^\times u R^{\times}⊇R \setminus \{0\} R X=ab+acX ac=1 a\in R^\times R \langle f\rangle = \langle 1\rangle =R[X] a=f \langle f\rangle = \langle 1\rangle","['abstract-algebra', 'ring-theory', 'principal-ideal-domains']"
36,Groups of order $p^3$,Groups of order,p^3,"The following is exercise 8 (section 2.6) in Algebra by Hungerford: Let $p$ be an odd prime. Prove that there are at most two nonabelian groups of order $p^3$ . (One has generators $a,b$ satisying $|a| = p^2; |b|=p;b^{-1}ab = a^{1+p}$ and the other has generators $a,b,c$ satisfying $|a|=|b| = |c|= p;c = a^{-1}b^{-1}ab;ca=ac;cb=bc$ ) I realize that there are many links online classifying groups of order $p^3$ . However, most of the solutions I have seen have required several pages of work.  Since this is just an exercise, I suspect there is a shorter solution. I have come up with the following: We know that $Z(G) = p$ because $G$ is a $p$ -group and if $|Z(G)| = p^2$ or $p^3$ , then $G/Z(G)$ would be cyclic. Thus, $G$ would be abelian, contrary to hypothesis. So, let $c$ be a generator for $Z(G)$ . Consider the quotient group $G/Z(G)$ which has order $p^2$ . Hence, $G/Z(G) \cong \mathbb{Z}_p \times \mathbb{Z}_p$ . Let $\overline{a},\overline{b}$ be generators for $G/Z(G)$ (the overlines denote passage to the quotient). Now, this means that every element in $G$ can be written as $a^ib^jc^k$ for $i,j,k \in \mathbb{Z}$ . Since $c \in Z(G)$ we cannot have that $a,b$ commute (this is important later). Notice that $\overline{a},\overline{b}$ both have order $p$ in $G/Z(G)$ . Thus, $a^p,b^p \in Z(G)$ . We consider cases of $a^p$ and $b^p$ . Case 1: both $a^p = b^p = 1$ Then, since $G/Z(G)$ is abelian, the commutator subgroup, $G'$ , is contained in $Z(G)$ so that $|G'| = 1 $ or $p$ . But, $|G'| = 1$ implies $G$ is abelian, contrary to hypothesis. So, $|G'| = p$ and consequently $G' = Z(G)$ . Thus, since $a,b$ do not commute, $a^{-1}b^{-1}ab$ is nontrivial and $Z(G) = \langle a^{-1}b^{-1}ab\rangle$ (this is by the previous exercise where we showed that the commutator subgroup is generated by elements of this form). Thus, by possibly switching generators for $Z(G)$ this group satisfies the second pair of generators and relations in the question. Case 2: $a^p \neq 1$ and $b^p = 1$ (or vice versa) In this case $a^p$ is a nontrivial element in $Z(G)$ and since $Z(G)$ is cyclic of order $p$ , $c = a^p$ and so $|a| = p^2$ . This gives rise to the first pair of generators and relations. It is here where I am stuck. These should be the only two, but I have not found a reason why we cannot have $a^p \neq 1$ and $b^p \neq 1$ . How can I guarantee that this case does not occur (or gives one of the two cases above)? I assume this has something to do with the oddness of $p$ , as I have not used this anywhere in the argument. Moreoever, any other suggestions or corrections on the current work would be greatly appreciated. Thank You","The following is exercise 8 (section 2.6) in Algebra by Hungerford: Let be an odd prime. Prove that there are at most two nonabelian groups of order . (One has generators satisying and the other has generators satisfying ) I realize that there are many links online classifying groups of order . However, most of the solutions I have seen have required several pages of work.  Since this is just an exercise, I suspect there is a shorter solution. I have come up with the following: We know that because is a -group and if or , then would be cyclic. Thus, would be abelian, contrary to hypothesis. So, let be a generator for . Consider the quotient group which has order . Hence, . Let be generators for (the overlines denote passage to the quotient). Now, this means that every element in can be written as for . Since we cannot have that commute (this is important later). Notice that both have order in . Thus, . We consider cases of and . Case 1: both Then, since is abelian, the commutator subgroup, , is contained in so that or . But, implies is abelian, contrary to hypothesis. So, and consequently . Thus, since do not commute, is nontrivial and (this is by the previous exercise where we showed that the commutator subgroup is generated by elements of this form). Thus, by possibly switching generators for this group satisfies the second pair of generators and relations in the question. Case 2: and (or vice versa) In this case is a nontrivial element in and since is cyclic of order , and so . This gives rise to the first pair of generators and relations. It is here where I am stuck. These should be the only two, but I have not found a reason why we cannot have and . How can I guarantee that this case does not occur (or gives one of the two cases above)? I assume this has something to do with the oddness of , as I have not used this anywhere in the argument. Moreoever, any other suggestions or corrections on the current work would be greatly appreciated. Thank You","p p^3 a,b |a| = p^2; |b|=p;b^{-1}ab = a^{1+p} a,b,c |a|=|b| = |c|= p;c = a^{-1}b^{-1}ab;ca=ac;cb=bc p^3 Z(G) = p G p |Z(G)| = p^2 p^3 G/Z(G) G c Z(G) G/Z(G) p^2 G/Z(G) \cong \mathbb{Z}_p \times \mathbb{Z}_p \overline{a},\overline{b} G/Z(G) G a^ib^jc^k i,j,k \in \mathbb{Z} c \in Z(G) a,b \overline{a},\overline{b} p G/Z(G) a^p,b^p \in Z(G) a^p b^p a^p = b^p = 1 G/Z(G) G' Z(G) |G'| = 1  p |G'| = 1 G |G'| = p G' = Z(G) a,b a^{-1}b^{-1}ab Z(G) = \langle a^{-1}b^{-1}ab\rangle Z(G) a^p \neq 1 b^p = 1 a^p Z(G) Z(G) p c = a^p |a| = p^2 a^p \neq 1 b^p \neq 1 p","['abstract-algebra', 'group-theory', 'finite-groups', 'groups-enumeration']"
37,Permutation of cosets,Permutation of cosets,,"Let $G$ be a finite group and $\gamma \in \text{Sym}(G)$ such that $\gamma (1) = 1$ and $\gamma (gH) = \gamma (g)H$ for all $g\in G$ and $H\leq G$ . This means $\gamma$ induces a permutation of the left cosets of any subgroup of $G$ . I need to show that $\gamma$ fixes any left coset of the Center $C(G)$ of $G$ , that is $$\gamma(g)C(G) = gC(G)\,.$$ I tried some simple calculations which did not work. Now I think some more structural arguments are necessary. Any ideas?","Let be a finite group and such that and for all and . This means induces a permutation of the left cosets of any subgroup of . I need to show that fixes any left coset of the Center of , that is I tried some simple calculations which did not work. Now I think some more structural arguments are necessary. Any ideas?","G \gamma \in \text{Sym}(G) \gamma (1) = 1 \gamma (gH) = \gamma (g)H g\in G H\leq G \gamma G \gamma C(G) G \gamma(g)C(G) = gC(G)\,.","['abstract-algebra', 'group-theory', 'finite-groups', 'group-actions']"
38,"If $H$ is a proper subgroup of a $p$-group $G$, then $H$ is proper in $N_G(H)$.","If  is a proper subgroup of a -group , then  is proper in .",H p G H N_G(H),"Let $H$ be a proper subgroup of $p$-group $G$. Show that the normalizer of $H$ in $G$, denoted $N_G(H)$, is strictly larger than $H$, and that $H$ is contained in a normal subgroup of index $p$. Here's what I've got so far: If $H$ is normal, $N_G(H)$ is all of $G$ and we are done. If $H$ is not normal, then suppose for the sake of contradiction that $N_G(H)=H$. Then there is no element outside of $H$ that fixes $H$ by conjugation. But the center $Z(G)$ of $G$ does fix $H$, so $Z(G)$ must be a subgroup of $H$. Don't know if I'm going on the right path or not, but either way can't really think my way out of this one... Any help would be appreciated.","Let $H$ be a proper subgroup of $p$-group $G$. Show that the normalizer of $H$ in $G$, denoted $N_G(H)$, is strictly larger than $H$, and that $H$ is contained in a normal subgroup of index $p$. Here's what I've got so far: If $H$ is normal, $N_G(H)$ is all of $G$ and we are done. If $H$ is not normal, then suppose for the sake of contradiction that $N_G(H)=H$. Then there is no element outside of $H$ that fixes $H$ by conjugation. But the center $Z(G)$ of $G$ does fix $H$, so $Z(G)$ must be a subgroup of $H$. Don't know if I'm going on the right path or not, but either way can't really think my way out of this one... Any help would be appreciated.",,"['abstract-algebra', 'group-theory', 'finite-groups', 'p-groups']"
39,How to prove a specific quotient of polynomial ring is a free module?,How to prove a specific quotient of polynomial ring is a free module?,,"Problem 1.19 in Eisenbud's Commutative Algebra asks the following. Given $R = k[x,y,z,w]$ and $I = (yw - z^2, xw - yz, xz - y^2)$ , show that $R/I$ is free as an $k[x,w]$ -module, and exhibit a basis. It is simple to show that under the desired substitutions any monomial can be reduced to $\{1, y, z\}$ with some $k[x,w]$ monomial scalar. Linear independence is implicitly a kind of converse: we must demonstrate the limits of the substitution rules, often by finding an invariant or canonical form. I've handled problems like this before using the following techniques: Prove that $z$ (or maybe $x, w$ ) is not a zero divisor. This would be a good start, as all monomials are of the form $w^a x^b z^{\{0, 1, 2\}}$ . Apply some kind of counting trick on the grading of the ring (since the ideal is homogeneous, the $R$ grading does descend). For example, if the parity of the total degree in $y$ and $z$ were preserved by the relations, then that could be used to show $y$ and $y^2$ are distinct, for example. I tried a number of such tricks, but there doesn't seem to be any obvious ""invariant"" of the substitutions like degree, or difference of degrees, etc. Can we reduce to the case of showing linear independence of the form $m_1 + m_2y + m_3 z = 0$ with $m_i$ monomials with scalar 1? This is true of pairwise linear independence, as the elements would have to cancel pairwise in each degree, so the general case of pairwise linear independence reduces to comparing two monomials. While the problem does not seem to be looking for it, there may well be a sophisticated argument using dimensionality arguments or something else algebraic-geometric. The problem is trying to conclude the ideal is prime, so a geometric argument could show the corresponding projective variety to be irreducible. Does one of the above techniques work? Is there an elementary proof of this fact? A high-tech slick proof?","Problem 1.19 in Eisenbud's Commutative Algebra asks the following. Given and , show that is free as an -module, and exhibit a basis. It is simple to show that under the desired substitutions any monomial can be reduced to with some monomial scalar. Linear independence is implicitly a kind of converse: we must demonstrate the limits of the substitution rules, often by finding an invariant or canonical form. I've handled problems like this before using the following techniques: Prove that (or maybe ) is not a zero divisor. This would be a good start, as all monomials are of the form . Apply some kind of counting trick on the grading of the ring (since the ideal is homogeneous, the grading does descend). For example, if the parity of the total degree in and were preserved by the relations, then that could be used to show and are distinct, for example. I tried a number of such tricks, but there doesn't seem to be any obvious ""invariant"" of the substitutions like degree, or difference of degrees, etc. Can we reduce to the case of showing linear independence of the form with monomials with scalar 1? This is true of pairwise linear independence, as the elements would have to cancel pairwise in each degree, so the general case of pairwise linear independence reduces to comparing two monomials. While the problem does not seem to be looking for it, there may well be a sophisticated argument using dimensionality arguments or something else algebraic-geometric. The problem is trying to conclude the ideal is prime, so a geometric argument could show the corresponding projective variety to be irreducible. Does one of the above techniques work? Is there an elementary proof of this fact? A high-tech slick proof?","R = k[x,y,z,w] I = (yw - z^2, xw - yz, xz - y^2) R/I k[x,w] \{1, y, z\} k[x,w] z x, w w^a x^b z^{\{0, 1, 2\}} R y z y y^2 m_1 + m_2y + m_3 z = 0 m_i","['abstract-algebra', 'algebraic-geometry', 'ring-theory', 'commutative-algebra', 'free-modules']"
40,Universal property of de Rham differential.,Universal property of de Rham differential.,,"Suppose $A$ is a commutative algebra over a field $k$. It is well known that there is a module that generalizes the notion of differential $1$-forms. It is denoted $\Omega^1_{k}(A)$ and is called the module of Kahler differentials. By definition, it is a module over $A$ generated by symbols $da,a\in A$ satisfying $dc=0$ if $c$ is ""constant"", i.e. $c\in k$ viewed as a subset of $A$. $d(a+b)=da+db$ $d(ab)=(da)b+a(db)$ $(da)b=b(da)$ There is also a map $d\colon A\to \Omega^1_{k}(A)$, $d(a):=da$ called the de Rham differential. It is well-known that $\Omega^1_{k}(A)$ and $d$ can be defined by a universal property. Recall first that a map $\phi\colon A\to M$ for some $A$-module $M$ is called a derivation of $A$ with values in $M$ if $\phi(ab)=\phi(a)b+\phi(b)a$. Then $\Omega^1_{k}(A)$ and $d$ are characterized by the property that for any derivation $X\colon A\to M$, there exists unique morphism $\mu_X\colon \Omega^1_{k}(A)\to M$ such that $X=\mu_x\circ d$. You can read all this in details here . My question is the following. Is there a similar universal description of the de Rham differential $d^1\colon \Omega^1_{k}(A)\to \Omega^2_{k}(A)$? What about $d^n\colon \Omega^n_{k}(A)\to \Omega^{n+1}_{k}(A)$? I would like to see a description like this: $d^1\colon \Omega^1_{k}(A)\to \Omega^2_{k}(A)$ is a map satisfying some properties, such that any other map $\Omega^1_{k}(A)\to M$ satisfying these properties factors through $d^1$. Thank you very much for your help!","Suppose $A$ is a commutative algebra over a field $k$. It is well known that there is a module that generalizes the notion of differential $1$-forms. It is denoted $\Omega^1_{k}(A)$ and is called the module of Kahler differentials. By definition, it is a module over $A$ generated by symbols $da,a\in A$ satisfying $dc=0$ if $c$ is ""constant"", i.e. $c\in k$ viewed as a subset of $A$. $d(a+b)=da+db$ $d(ab)=(da)b+a(db)$ $(da)b=b(da)$ There is also a map $d\colon A\to \Omega^1_{k}(A)$, $d(a):=da$ called the de Rham differential. It is well-known that $\Omega^1_{k}(A)$ and $d$ can be defined by a universal property. Recall first that a map $\phi\colon A\to M$ for some $A$-module $M$ is called a derivation of $A$ with values in $M$ if $\phi(ab)=\phi(a)b+\phi(b)a$. Then $\Omega^1_{k}(A)$ and $d$ are characterized by the property that for any derivation $X\colon A\to M$, there exists unique morphism $\mu_X\colon \Omega^1_{k}(A)\to M$ such that $X=\mu_x\circ d$. You can read all this in details here . My question is the following. Is there a similar universal description of the de Rham differential $d^1\colon \Omega^1_{k}(A)\to \Omega^2_{k}(A)$? What about $d^n\colon \Omega^n_{k}(A)\to \Omega^{n+1}_{k}(A)$? I would like to see a description like this: $d^1\colon \Omega^1_{k}(A)\to \Omega^2_{k}(A)$ is a map satisfying some properties, such that any other map $\Omega^1_{k}(A)\to M$ satisfying these properties factors through $d^1$. Thank you very much for your help!",,"['abstract-algebra', 'reference-request', 'commutative-algebra', 'universal-property']"
41,Under what conditions does a ring R have the property that every zero divisor is a nilpotent element?,Under what conditions does a ring R have the property that every zero divisor is a nilpotent element?,,"Under what conditions does a ring $R$ have the property that every zero divisor is a nilpotent element ? If we have a ring $R$, we know that every nilpotent element is either zero or a zero divisor. If $A$ is the set of all nilpotents except the zero, and $B$ is the set of all nonzero zero divisors, then under what conditions on $R$ do we have $A = B$ ?","Under what conditions does a ring $R$ have the property that every zero divisor is a nilpotent element ? If we have a ring $R$, we know that every nilpotent element is either zero or a zero divisor. If $A$ is the set of all nilpotents except the zero, and $B$ is the set of all nonzero zero divisors, then under what conditions on $R$ do we have $A = B$ ?",,"['abstract-algebra', 'ring-theory']"
42,Generalize exterior algebra: vectors are nilcube instead of nilsquare,Generalize exterior algebra: vectors are nilcube instead of nilsquare,,"The exterior product on a ( $d$ -dimensional) vector space $V$ is defined to be associative and bilinear, and to make any vector square to $0$ , and is otherwise unrestricted. Formally, the exterior algebra $\Lambda V$ is a quotient of the tensor algebra. What happens if, instead, the $n$ th power of any vector is $0$ ? Let's call this the $n$ th nilpotent algebra, $N_nV$ . The $0$ th nilpotent algebra is trivial, with only one element $1=a^0=0$ , so $N_0V=\{0\}$ , which is $0$ -dimensional. The $1$ st nilpotent algebra is the scalar field, because any vector is $a^1=0$ ; thus $N_1V=\mathbb F$ , which is $1$ -dimensional. The $2$ nd nilpotent algebra, with $a^2=a\wedge a=0$ , is the exterior algebra $N_2V=\Lambda V$ , which is $2^d$ -dimensional. What is the $3$ rd nilpotent algebra? Would $N_3V$ be $3^d$ -dimensional? Would $N_nV$ be $n^d$ -dimensional? If $d=1$ , so $V$ has a basis $\{a\}$ , then $N_nV$ has a basis $\{1,a,a^2,\cdots,a^{n-1}\}$ , so it is indeed $n^1$ -dimensional. If $d=2$ , so $V$ has a basis $\{a,b\}$ , then $N_0V$ has basis $\{\}$ , and $N_1V$ has basis $\{1\}$ , and $N_2V$ has basis $\{1,a,b,ab\}$ . What is a basis for $N_3V$ ? Since any vector cubes to $0$ , we have $$0=(a+b)^3=a^3+aab+aba+abb+baa+bab+bba+b^3$$ and $$0=(a-b)^3=a^3-aab-aba+abb-baa+bab+bba-b^3.$$ Subtract these and divide by $2$ to get $$0=aab+aba+baa.$$ Are there any other relations like this, not immediately obvious from the definition? Naively using only $a^3=b^3=0$ , we would expect this to be a basis: $$\{1,a,b,a^2,ab,ba,b^2,a^2b,aba,ab^2,ba^2,bab,b^2a,$$ $$a^2ba,a^2b^2,aba^2,abab,ab^2a,ba^2b,baba,bab^2,b^2a^2,b^2ab,\cdots\}$$ which would contain an infinite number of terms like $a^2ba^2ba^2b$ ; but some terms must be removed, being linearly dependent: $$ba^2=-a^2b-aba$$ $$b^2a=-ab^2-bab$$ $$a(ba^2)=0-a^2ba$$ $$a(b^2a)=-a^2b^2-abab$$ $$(ba^2)b=-a^2b^2-abab$$ $$b^2a^2=a^2b^2+abab-baba$$ $$(b^2a)b=0-bab^2$$ $$\vdots$$ Will there be only finitely many independent terms left?","The exterior product on a ( -dimensional) vector space is defined to be associative and bilinear, and to make any vector square to , and is otherwise unrestricted. Formally, the exterior algebra is a quotient of the tensor algebra. What happens if, instead, the th power of any vector is ? Let's call this the th nilpotent algebra, . The th nilpotent algebra is trivial, with only one element , so , which is -dimensional. The st nilpotent algebra is the scalar field, because any vector is ; thus , which is -dimensional. The nd nilpotent algebra, with , is the exterior algebra , which is -dimensional. What is the rd nilpotent algebra? Would be -dimensional? Would be -dimensional? If , so has a basis , then has a basis , so it is indeed -dimensional. If , so has a basis , then has basis , and has basis , and has basis . What is a basis for ? Since any vector cubes to , we have and Subtract these and divide by to get Are there any other relations like this, not immediately obvious from the definition? Naively using only , we would expect this to be a basis: which would contain an infinite number of terms like ; but some terms must be removed, being linearly dependent: Will there be only finitely many independent terms left?","d V 0 \Lambda V n 0 n N_nV 0 1=a^0=0 N_0V=\{0\} 0 1 a^1=0 N_1V=\mathbb F 1 2 a^2=a\wedge a=0 N_2V=\Lambda V 2^d 3 N_3V 3^d N_nV n^d d=1 V \{a\} N_nV \{1,a,a^2,\cdots,a^{n-1}\} n^1 d=2 V \{a,b\} N_0V \{\} N_1V \{1\} N_2V \{1,a,b,ab\} N_3V 0 0=(a+b)^3=a^3+aab+aba+abb+baa+bab+bba+b^3 0=(a-b)^3=a^3-aab-aba+abb-baa+bab+bba-b^3. 2 0=aab+aba+baa. a^3=b^3=0 \{1,a,b,a^2,ab,ba,b^2,a^2b,aba,ab^2,ba^2,bab,b^2a, a^2ba,a^2b^2,aba^2,abab,ab^2a,ba^2b,baba,bab^2,b^2a^2,b^2ab,\cdots\} a^2ba^2ba^2b ba^2=-a^2b-aba b^2a=-ab^2-bab a(ba^2)=0-a^2ba a(b^2a)=-a^2b^2-abab (ba^2)b=-a^2b^2-abab b^2a^2=a^2b^2+abab-baba (b^2a)b=0-bab^2 \vdots","['abstract-algebra', 'vector-spaces', 'tensor-products', 'quotient-spaces', 'nilpotence']"
43,Discovery of the first Janko Group,Discovery of the first Janko Group,,"Recently, I was reading about Janko's discovery of $J_1$, the first “modern” sporadic simple group. Janko and others were trying to classify all finite simple groups with an involution centralizer isomorphic to $C_2 \times \mathrm{PSL}(2,q)$. At some point in that process, it seemed possible that there exist a new sporadic simple group with involution centralizer $C_2 \times \mathrm{PSL}(2,5)$ among them. People (mostly Janko himself) started to investigate this hypothetical simple group $J_1$. They computed its order (175560) and drew conclusions about its subgroup structure. In the end, Janko was able to give concrete generators of $J_1$ as a subgroup of $\mathrm{GL}(7,11)$ (the matrices can be found on Wikipedia ). At this point it was known that if that new sporadic group $J_1$ really exists, then it has to be a concrete subgroup of $\mathrm{GL}(7,11)$. Afterwards, the existence of $J_1$ was confirmed by a computer program. Can someone offer more insights about how it was possible to get from some structural knowledge of this hypothetical group to concrete generators as a matrix group? I am curious to see how to derive a linear action out of a hypothetical simple group (of course one cannot just simply pick an elementary abelian normal subgroup on which it acts naturally).","Recently, I was reading about Janko's discovery of $J_1$, the first “modern” sporadic simple group. Janko and others were trying to classify all finite simple groups with an involution centralizer isomorphic to $C_2 \times \mathrm{PSL}(2,q)$. At some point in that process, it seemed possible that there exist a new sporadic simple group with involution centralizer $C_2 \times \mathrm{PSL}(2,5)$ among them. People (mostly Janko himself) started to investigate this hypothetical simple group $J_1$. They computed its order (175560) and drew conclusions about its subgroup structure. In the end, Janko was able to give concrete generators of $J_1$ as a subgroup of $\mathrm{GL}(7,11)$ (the matrices can be found on Wikipedia ). At this point it was known that if that new sporadic group $J_1$ really exists, then it has to be a concrete subgroup of $\mathrm{GL}(7,11)$. Afterwards, the existence of $J_1$ was confirmed by a computer program. Can someone offer more insights about how it was possible to get from some structural knowledge of this hypothetical group to concrete generators as a matrix group? I am curious to see how to derive a linear action out of a hypothetical simple group (of course one cannot just simply pick an elementary abelian normal subgroup on which it acts naturally).",,"['abstract-algebra', 'group-theory', 'math-history', 'simple-groups']"
44,Free lattice in three generators,Free lattice in three generators,,"By general results for every set $X$ there is a free bounded lattice $L(X)$ on $X$. I would like to understand the element structure of this lattice. The cases $X=\emptyset$, $X=\{x\}$ and $X=\{x,y\}$ are quite easy. But for $X=\{x,y,z\}$ we get an infinite lattice. But what are the elements explicitly? Is there any normal form available? Or, is there any natural representation of $L(X)$? Compare this to the free group on two generators, which might be quite abstract, but it can be explicitly realized as a certain subgroup of $\mathrm{SL}(2,\mathbb{Z})$, generated by two matrices (see Ping-pong lemma ). So is there a natural and non-abstract example of a bounded lattice which contains the free bounded lattice on three generators? Notice that the recursive description at Wikipedia doesn't answer these questions.","By general results for every set $X$ there is a free bounded lattice $L(X)$ on $X$. I would like to understand the element structure of this lattice. The cases $X=\emptyset$, $X=\{x\}$ and $X=\{x,y\}$ are quite easy. But for $X=\{x,y,z\}$ we get an infinite lattice. But what are the elements explicitly? Is there any normal form available? Or, is there any natural representation of $L(X)$? Compare this to the free group on two generators, which might be quite abstract, but it can be explicitly realized as a certain subgroup of $\mathrm{SL}(2,\mathbb{Z})$, generated by two matrices (see Ping-pong lemma ). So is there a natural and non-abstract example of a bounded lattice which contains the free bounded lattice on three generators? Notice that the recursive description at Wikipedia doesn't answer these questions.",,"['abstract-algebra', 'category-theory', 'lattice-orders']"
45,Is there a group which has precisely all finite groups as subgroups?,Is there a group which has precisely all finite groups as subgroups?,,"I would like to ask the following question: Does there exist a group $G$ such that every finite group can be embedded in $G$, and every proper subgroup of $G$ is finite? The closest example to this I have seen is the group $S_{\omega}$, i.e. bijections of $\mathbb{N}$ fixing all but finitely many elements. However, this group contains isomorphic copies of itself as its proper subgroups (even continuum many - for every $S \subseteq \mathbb{N}$ that is not cofinite, bijections from $S_{\omega}$ fixing all points in $S$ form a group isomorphic to $S_{\omega}$). So obviously such a group needs to contain some subgroup isomorphic to $S_n$ for every $n$, but not infinite ascending chain of these. I am not sure how to even look for something like that (to be honest, I am inclined to believe that such a group cannot exist). I would also appreciate pointing me towards some infinite subgroups of $S_{\omega}$ not isomorphic to $S_{\omega}$ (the more bizarre, the better. : ) ). Thanks in advance for any help. Edit: OK I see that even the $S_\omega$ is far from having these properties - one can, for example, take an arbitrary countable family of finite groups $G_n$ and then find $\bigoplus_{n<\omega}G_n$ as a subgroup by partitioning the set $\mathbb{N}=\bigcup_{n < \omega}M_n$ and realizing $G_n$ as a subgroup of $S(M_n). $ So there are in fact even uncountably many up to isomorphism infinite subgroups of $S_{\omega}$. Edit2: The only idea I came up with is the following: Obviously such a group, assuming it exists, is a union of all its proper subgroups, hence it is a direct limit (i.e. directed colimit) of some diagram consisting of a set of finite groups s.t. every finite group is isomorphic to some of theose groups. Finding the ""right diagram"" could resolve the problem (by showing the colimit has desired properties, or showing that if some group does, it is necessarily that one, and finding a proper infinite subgroup there). However, it is not clear to me which morphisms should one use. My first idea was to use all possible injections (something like ""whenever some injection exists, fix one""), but that obviosly doesn't work - the resulting limit would contain $S_{\omega}$ as a subgroup. So maybe one needs to use less injections (which I suspect would make the limit even bigger) or more general morphisms, than injections.","I would like to ask the following question: Does there exist a group $G$ such that every finite group can be embedded in $G$, and every proper subgroup of $G$ is finite? The closest example to this I have seen is the group $S_{\omega}$, i.e. bijections of $\mathbb{N}$ fixing all but finitely many elements. However, this group contains isomorphic copies of itself as its proper subgroups (even continuum many - for every $S \subseteq \mathbb{N}$ that is not cofinite, bijections from $S_{\omega}$ fixing all points in $S$ form a group isomorphic to $S_{\omega}$). So obviously such a group needs to contain some subgroup isomorphic to $S_n$ for every $n$, but not infinite ascending chain of these. I am not sure how to even look for something like that (to be honest, I am inclined to believe that such a group cannot exist). I would also appreciate pointing me towards some infinite subgroups of $S_{\omega}$ not isomorphic to $S_{\omega}$ (the more bizarre, the better. : ) ). Thanks in advance for any help. Edit: OK I see that even the $S_\omega$ is far from having these properties - one can, for example, take an arbitrary countable family of finite groups $G_n$ and then find $\bigoplus_{n<\omega}G_n$ as a subgroup by partitioning the set $\mathbb{N}=\bigcup_{n < \omega}M_n$ and realizing $G_n$ as a subgroup of $S(M_n). $ So there are in fact even uncountably many up to isomorphism infinite subgroups of $S_{\omega}$. Edit2: The only idea I came up with is the following: Obviously such a group, assuming it exists, is a union of all its proper subgroups, hence it is a direct limit (i.e. directed colimit) of some diagram consisting of a set of finite groups s.t. every finite group is isomorphic to some of theose groups. Finding the ""right diagram"" could resolve the problem (by showing the colimit has desired properties, or showing that if some group does, it is necessarily that one, and finding a proper infinite subgroup there). However, it is not clear to me which morphisms should one use. My first idea was to use all possible injections (something like ""whenever some injection exists, fix one""), but that obviosly doesn't work - the resulting limit would contain $S_{\omega}$ as a subgroup. So maybe one needs to use less injections (which I suspect would make the limit even bigger) or more general morphisms, than injections.",,"['abstract-algebra', 'group-theory', 'examples-counterexamples', 'permutations']"
46,Derivative of numbers,Derivative of numbers,,"I am not talking here about deriving a constant in the usual sense (which gives $0$ as everyone knows). I have once heard of a derivative operator with relaxed conditions that was defined over the real numbers whose purpose at first was to study number theory. Usually, two of the essential properties that we demand on a differential operator is its linearity and its way of acting over products, i.e. if I call my differential operator $D$, : $$ D(f+g) = Df + Dg, \quad D(fg) = (Df)g + fDg. $$ Let's suppose we delete the linearity condition and keep only the product rule. Define $D(1) = 0$, $D(p) = 1$ if $p$ is prime and suppose $D$ satisfies the product relation over products. It is quite unclear in my memory at which point this operator could do (i.e. on which things could we apply it, was it restricted to algebraic numbers, or could it go over any real, complex?) Between the two lines I'm detailing what I know about this operator. For instance, $D(4) = 2D(2) + 2(D(2)) = 2(1) + 2(1) = 4$. Hence $D(4) = 4$ is equal to its own derivative. Right now my definition only make sense with integers, since we can factor an integer $n$ in its prime decomposition and then apply the product rule to find the answer. We can also find the derivative of a fraction : $$ 0 = D(1) = D(q/q) = q D(1/q)  + 1/q D(q)  \quad \Rightarrow \quad D(1/q) = -\frac{D(q)}{q^2}. $$ hence we can deduce the usual formula $D(p/q) = \frac{qD(p) - pD(q)}{q^2}$ by a similar argument. By an inductive argument, we can also show things as $D(a^n) = na^{n-1}$, and define this operator on algebraic numbers (at least for some that I know), for instance if $a$ is positive,  $$ D(a) = a^{1/2} D(a^{1/2}) + a^{1/2} D(a^{1/2}) = 2a^{1/2} D(a^{1/2}), \quad \Rightarrow D(a^{1/2}) = \frac{D(a)}{2a^{1/2}} $$ and note here the similarity for the formula for deriving the function $1/f(x)$ in the real differentiable functions system. If anyone has heard of such an operator over numbers which I have tried to describe as much as possible, can anyone tell me if there are any known results related to number theory that uses this tool to lead to some demonstrations that are useful? Some facts about primes, disivibility, writing a number as a sum of things, I don't know, just tell me what you know. I'd love to hear about it.","I am not talking here about deriving a constant in the usual sense (which gives $0$ as everyone knows). I have once heard of a derivative operator with relaxed conditions that was defined over the real numbers whose purpose at first was to study number theory. Usually, two of the essential properties that we demand on a differential operator is its linearity and its way of acting over products, i.e. if I call my differential operator $D$, : $$ D(f+g) = Df + Dg, \quad D(fg) = (Df)g + fDg. $$ Let's suppose we delete the linearity condition and keep only the product rule. Define $D(1) = 0$, $D(p) = 1$ if $p$ is prime and suppose $D$ satisfies the product relation over products. It is quite unclear in my memory at which point this operator could do (i.e. on which things could we apply it, was it restricted to algebraic numbers, or could it go over any real, complex?) Between the two lines I'm detailing what I know about this operator. For instance, $D(4) = 2D(2) + 2(D(2)) = 2(1) + 2(1) = 4$. Hence $D(4) = 4$ is equal to its own derivative. Right now my definition only make sense with integers, since we can factor an integer $n$ in its prime decomposition and then apply the product rule to find the answer. We can also find the derivative of a fraction : $$ 0 = D(1) = D(q/q) = q D(1/q)  + 1/q D(q)  \quad \Rightarrow \quad D(1/q) = -\frac{D(q)}{q^2}. $$ hence we can deduce the usual formula $D(p/q) = \frac{qD(p) - pD(q)}{q^2}$ by a similar argument. By an inductive argument, we can also show things as $D(a^n) = na^{n-1}$, and define this operator on algebraic numbers (at least for some that I know), for instance if $a$ is positive,  $$ D(a) = a^{1/2} D(a^{1/2}) + a^{1/2} D(a^{1/2}) = 2a^{1/2} D(a^{1/2}), \quad \Rightarrow D(a^{1/2}) = \frac{D(a)}{2a^{1/2}} $$ and note here the similarity for the formula for deriving the function $1/f(x)$ in the real differentiable functions system. If anyone has heard of such an operator over numbers which I have tried to describe as much as possible, can anyone tell me if there are any known results related to number theory that uses this tool to lead to some demonstrations that are useful? Some facts about primes, disivibility, writing a number as a sum of things, I don't know, just tell me what you know. I'd love to hear about it.",,"['number-theory', 'abstract-algebra']"
47,"Why is surjectivity ""harder"" than injectivity?","Why is surjectivity ""harder"" than injectivity?",,"Injectivity and surjectivity are very intimately related, however, in any particular structure, one of them tends to be a much ""harder"" property - just looking at basic set theory, we have that a function is injective iff it has a left inverse, and surjective iff it has a right inverse. But immediately something crops up: the former is a harmless statement provable in ZF, but the latter is equivalent to the axiom of choice. Going on to basic algebra, we see that kernels  are totally related to the injectiveness of a function, and studying them tend to be much easier, while cokernels tend to bring in no new information (and at least, at the lower level, seem to be just a fancy but useless substitute for studying  surjectivity). We also see that theorems about surjectivity tend to be more important (off the top of my head, the isomorphism extension theorem in field theory). Surjectivity questions tend to have to be answered constructively, which in my experience, is generally hard. That is not to say that the opposite isn't true - from memory, projective resolutions were significantly easier than injective ones. My question is: is there a ""deep"" reason as to why one of the two tend to be much ""harder"" than the other?","Injectivity and surjectivity are very intimately related, however, in any particular structure, one of them tends to be a much ""harder"" property - just looking at basic set theory, we have that a function is injective iff it has a left inverse, and surjective iff it has a right inverse. But immediately something crops up: the former is a harmless statement provable in ZF, but the latter is equivalent to the axiom of choice. Going on to basic algebra, we see that kernels  are totally related to the injectiveness of a function, and studying them tend to be much easier, while cokernels tend to bring in no new information (and at least, at the lower level, seem to be just a fancy but useless substitute for studying  surjectivity). We also see that theorems about surjectivity tend to be more important (off the top of my head, the isomorphism extension theorem in field theory). Surjectivity questions tend to have to be answered constructively, which in my experience, is generally hard. That is not to say that the opposite isn't true - from memory, projective resolutions were significantly easier than injective ones. My question is: is there a ""deep"" reason as to why one of the two tend to be much ""harder"" than the other?",,"['abstract-algebra', 'elementary-set-theory', 'category-theory', 'soft-question']"
48,Show that if $p$ is prime then $\Bbb Z_p$ is a field,Show that if  is prime then  is a field,p \Bbb Z_p,"Check my proof please. Let $\Bbb Z_p:=\Bbb Z/p\Bbb Z$ be the quotient ring modulo $p$ . I want to prove that if $p$ is prime then $\Bbb Z_p$ is a field. Known facts about the quotient rings of the kind $\Bbb Z_n$ (I dont list all the characteristics of a ring, just some ones): They are commutative rings with unity. $[0]=[n]$ is the identity of addition or zero. Then $[n]+[a]=[a]$ and $[n]\cdot[a]=[n]$ for all $[a]\in\Bbb Z_n$ . $[1]$ is the multiplicative identity or unity. Then $[1]\cdot[a]=[a]$ for all $[a]\in\Bbb Z_n$ . The addition is defined as $[a]+[b]=[a+b]$ , and the multiplication is defined as $[a]\cdot[b]=[ab]$ . $[a]=[b]$ means that exists $z\in\Bbb Z$ such that $a=b+nz$ . $|\Bbb Z_p|=p$ . A ring $\Bbb Z_p$ dont have zero divisors , i.e. doesnt exist $[a],[b]\in\Bbb Z_p$ distinct of $[p]$ such that $[a]\cdot[b]=[p]$ . Proof by contradiction: suppose that exists such $[a],[b]$ distinct of $[p]$ that are zero divisors. Then exist some $z_j\in\Bbb Z$ such that: $$(a+pz_1)(b+pz_2)=pz_3\iff ab+pz_4=pz_3\iff ab=pz_5$$ but this is a contradiction with the assumption that $a,b\neq p$ and $p$ prime, so dont exist divisors of zero on any $\Bbb Z_p$ . $\Box$ For any $[a]\in\Bbb Z_p$ distinct of $[p]$ exist $[b]$ such that $[a][b]=[1]$ . Because $\Bbb Z_p$ dont have zero divisors and is finite for some $[a]\neq[p]$ we have that if $[b]\neq[c]$ then $[a][b]\neq[a][c]$ . Proof by contradiction: if $[a][b]=[a][c]$ in the above conditions then $$[a][c]=[ac]=[ab]=[a][b]\iff ac+pz_1=ab+pz_2\iff a(c-b)=pz_3$$ for some $z_j\in\Bbb Z$ . But $pz_3\in[p]$ , then $[a][c-b]=[p]$ but because there are no zero divisors in $\Bbb Z_p$ then or $[a]=[p]$ or $[c-b]=[p]$ what contradicts the above conditions. $\Box$ Because $[a][b]\neq[a][c]$ for $[a]\neq[p]$ and $[b]\neq[c]$ then for any $[a]\neq[p]$ we have that $$[a]\cdot\Bbb Z_p=\Bbb Z_p\implies \exists [b]\in\Bbb Z_p:[a][b]=[1]$$ In other words: every element of $\Bbb Z_p$ but $[p]$ have a multiplicative inverse as stated above. $\Box$ Because $\Bbb Z_p$ is a commutative ring with $[1]\neq[0]$ and have multiplicative inverse for all their elements but $[p]=[0]$ then $\Bbb Z_p$ is a field. $\Box$","Check my proof please. Let be the quotient ring modulo . I want to prove that if is prime then is a field. Known facts about the quotient rings of the kind (I dont list all the characteristics of a ring, just some ones): They are commutative rings with unity. is the identity of addition or zero. Then and for all . is the multiplicative identity or unity. Then for all . The addition is defined as , and the multiplication is defined as . means that exists such that . . A ring dont have zero divisors , i.e. doesnt exist distinct of such that . Proof by contradiction: suppose that exists such distinct of that are zero divisors. Then exist some such that: but this is a contradiction with the assumption that and prime, so dont exist divisors of zero on any . For any distinct of exist such that . Because dont have zero divisors and is finite for some we have that if then . Proof by contradiction: if in the above conditions then for some . But , then but because there are no zero divisors in then or or what contradicts the above conditions. Because for and then for any we have that In other words: every element of but have a multiplicative inverse as stated above. Because is a commutative ring with and have multiplicative inverse for all their elements but then is a field.","\Bbb Z_p:=\Bbb Z/p\Bbb Z p p \Bbb Z_p \Bbb Z_n [0]=[n] [n]+[a]=[a] [n]\cdot[a]=[n] [a]\in\Bbb Z_n [1] [1]\cdot[a]=[a] [a]\in\Bbb Z_n [a]+[b]=[a+b] [a]\cdot[b]=[ab] [a]=[b] z\in\Bbb Z a=b+nz |\Bbb Z_p|=p \Bbb Z_p [a],[b]\in\Bbb Z_p [p] [a]\cdot[b]=[p] [a],[b] [p] z_j\in\Bbb Z (a+pz_1)(b+pz_2)=pz_3\iff ab+pz_4=pz_3\iff ab=pz_5 a,b\neq p p \Bbb Z_p \Box [a]\in\Bbb Z_p [p] [b] [a][b]=[1] \Bbb Z_p [a]\neq[p] [b]\neq[c] [a][b]\neq[a][c] [a][b]=[a][c] [a][c]=[ac]=[ab]=[a][b]\iff ac+pz_1=ab+pz_2\iff a(c-b)=pz_3 z_j\in\Bbb Z pz_3\in[p] [a][c-b]=[p] \Bbb Z_p [a]=[p] [c-b]=[p] \Box [a][b]\neq[a][c] [a]\neq[p] [b]\neq[c] [a]\neq[p] [a]\cdot\Bbb Z_p=\Bbb Z_p\implies \exists [b]\in\Bbb Z_p:[a][b]=[1] \Bbb Z_p [p] \Box \Bbb Z_p [1]\neq[0] [p]=[0] \Bbb Z_p \Box","['abstract-algebra', 'ring-theory', 'modular-arithmetic', 'solution-verification']"
49,Two groups that are the automorphism groups of each other,Two groups that are the automorphism groups of each other,,"Let $H,K$ be two non-isomorphic  groups such that $H\cong Aut(K)$ and $K\cong Aut(H)$. Is there any example of such groups ?","Let $H,K$ be two non-isomorphic  groups such that $H\cong Aut(K)$ and $K\cong Aut(H)$. Is there any example of such groups ?",,"['abstract-algebra', 'group-theory']"
50,"$G$ is a group of odd order, show that $a^2=b^2 \Rightarrow a=b$","is a group of odd order, show that",G a^2=b^2 \Rightarrow a=b,"I've come across this question and I tried to prove it, but my solution seems a little stealthy to me, is it correct? Let $|G|=2k+1$, then we have: $a=ae=aa^{|G|}=a^{|G|+1}=a^{2k+2}=(a^2)^{k+1}=(b^2)^{k+1}=b^{2k+2}=b^{|G|+1}=eb=b$ I would like to know if I'm missing something","I've come across this question and I tried to prove it, but my solution seems a little stealthy to me, is it correct? Let $|G|=2k+1$, then we have: $a=ae=aa^{|G|}=a^{|G|+1}=a^{2k+2}=(a^2)^{k+1}=(b^2)^{k+1}=b^{2k+2}=b^{|G|+1}=eb=b$ I would like to know if I'm missing something",,"['abstract-algebra', 'group-theory', 'proof-verification', 'finite-groups']"
51,Rings with $a^5=a$ are commutative,Rings with  are commutative,a^5=a,"Update. I have generalized the method here and have shown the theorem for many more exponents: Equational proofs of Jacobson's Theorem Let $R$ be a ring such that $a^5=a$ for all $a \in R$ . Then it follows that $R$ is commutative. This is part of a more general well-known theorem by Jacobson for arbitrary exponents ( $a^n=a$ ), which appeared on math.stackexchange a couple of times . But what I would like to see is a proof which is (1) direct / elementary and  (2) in equational language (in particular, first-order). (As far as I know, no published proof of Jacobson's Theorem is equational, but such a proof has to exist ). See here and there for such proofs for the exponents $3$ and $4$ . I hope that these proofs make the ""rules"" clear. What I've done so far: Reduced to the case that $R$ has characteristic $5$ . Any element of the form $a^4$ is central, because it is idempotent and $R$ is reduced. Also, it suffices to prove $(ab)^2= b^2 a^2$ for $a,b \in R$ . In fact, this implies $$ab=(ab)^5=a(ba)^4 b = (ba)^4 ab  = b a b (ab)^2 a^2 b$$ $$ = b a b (b^2 a^2) a^2 b  = b a b^3 a^4 b  = b a a^4 b^3 b  = b a b^4  = b b^4 a = ba.$$ Perhaps someone can feed automatic theorem provers with this problem. But my experience is that their proofs are not so easy to follow, quite long and not intuitive. So I am actually looking for hand-made proofs which eventually might also work for rings satisfying $a^p=a$ where $p$ is any prime. Edit. Here is some progress, inspired by the proof by Nagahara and Tominaga. Consider $a \in R$ and decompose the finite commutative reduced ring $\langle a \rangle$ into a product of fields - this is just the Chinese Remainder Theorem. Hence, $a=a_1+\dotsc+a_n$ where each $\langle a_i \rangle$ is a field with unit $e_i=a_i^4$ . The only fields satisfying the equation are $\mathbb{F}_2$ , $\mathbb{F}_3$ and $\mathbb{F}_5$ , which are prime fields . Hence, $a_i = z_i e_i$ for some $z \in \mathbb{Z}$ is central and therefore $a$ is central. (This incredibly quick proof for the $(a^n=a)$ -problem works for all exponents $n$ where every prime power $q$ such that $q-1|n-1$ is actually a prime number.) 2nd Edit . This leads to an equational proof. For simplicity, let us assume that $R$ has characteristic $5$ .  (This reduction is not in equational language, but the cases of characteristic $2$ and $3$ can be dealt separately and in the end everything may be put into a single equational proof.) Then $\langle a \rangle$ is a quotient of $\mathbb{F}_5[T]/(T^4-1)$ , which is isomorphic to $\mathbb{F}_5 \times \mathbb{F}_5 \times \mathbb{F}_5 \times \mathbb{F}_5$ via $f \mapsto (f(1),f(2),f(3),f(4))$ . We can compute the corresponding idempotents: $$e_1 = (t-2)^2 (t-3)(t-4), e_2 = (t-1) (t-3) (t-4)^2, \\e_3 = (t-1)^2 (t-2) (t-4), e_4 = (t-1) (t-2) (t-3)^2.$$ Back in $R$ we obtain the equation $$a = e_1(a) + 2 e_2(a) + 3 e_3(a) + 4 e_4(a).$$ Each $e_i(a)$ is idempotent, hence central, and therefore $a$ is central. This proof may be carried out in equational language - but then we would have to verify the equation and that the $e_i(a)$ are idempotent, which is obviously very tedious. Is there also a proof which is (3) not tedious?","Update. I have generalized the method here and have shown the theorem for many more exponents: Equational proofs of Jacobson's Theorem Let be a ring such that for all . Then it follows that is commutative. This is part of a more general well-known theorem by Jacobson for arbitrary exponents ( ), which appeared on math.stackexchange a couple of times . But what I would like to see is a proof which is (1) direct / elementary and  (2) in equational language (in particular, first-order). (As far as I know, no published proof of Jacobson's Theorem is equational, but such a proof has to exist ). See here and there for such proofs for the exponents and . I hope that these proofs make the ""rules"" clear. What I've done so far: Reduced to the case that has characteristic . Any element of the form is central, because it is idempotent and is reduced. Also, it suffices to prove for . In fact, this implies Perhaps someone can feed automatic theorem provers with this problem. But my experience is that their proofs are not so easy to follow, quite long and not intuitive. So I am actually looking for hand-made proofs which eventually might also work for rings satisfying where is any prime. Edit. Here is some progress, inspired by the proof by Nagahara and Tominaga. Consider and decompose the finite commutative reduced ring into a product of fields - this is just the Chinese Remainder Theorem. Hence, where each is a field with unit . The only fields satisfying the equation are , and , which are prime fields . Hence, for some is central and therefore is central. (This incredibly quick proof for the -problem works for all exponents where every prime power such that is actually a prime number.) 2nd Edit . This leads to an equational proof. For simplicity, let us assume that has characteristic .  (This reduction is not in equational language, but the cases of characteristic and can be dealt separately and in the end everything may be put into a single equational proof.) Then is a quotient of , which is isomorphic to via . We can compute the corresponding idempotents: Back in we obtain the equation Each is idempotent, hence central, and therefore is central. This proof may be carried out in equational language - but then we would have to verify the equation and that the are idempotent, which is obviously very tedious. Is there also a proof which is (3) not tedious?","R a^5=a a \in R R a^n=a 3 4 R 5 a^4 R (ab)^2= b^2 a^2 a,b \in R ab=(ab)^5=a(ba)^4 b = (ba)^4 ab  = b a b (ab)^2 a^2 b  = b a b (b^2 a^2) a^2 b  = b a b^3 a^4 b  = b a a^4 b^3 b  = b a b^4  = b b^4 a = ba. a^p=a p a \in R \langle a \rangle a=a_1+\dotsc+a_n \langle a_i \rangle e_i=a_i^4 \mathbb{F}_2 \mathbb{F}_3 \mathbb{F}_5 a_i = z_i e_i z \in \mathbb{Z} a (a^n=a) n q q-1|n-1 R 5 2 3 \langle a \rangle \mathbb{F}_5[T]/(T^4-1) \mathbb{F}_5 \times \mathbb{F}_5 \times \mathbb{F}_5 \times \mathbb{F}_5 f \mapsto (f(1),f(2),f(3),f(4)) e_1 = (t-2)^2 (t-3)(t-4), e_2 = (t-1) (t-3) (t-4)^2, \\e_3 = (t-1)^2 (t-2) (t-4), e_4 = (t-1) (t-2) (t-3)^2. R a = e_1(a) + 2 e_2(a) + 3 e_3(a) + 4 e_4(a). e_i(a) a e_i(a)","['abstract-algebra', 'ring-theory', 'noncommutative-algebra', 'automated-theorem-proving']"
52,"Why is the Quasitriangular Hopf algebra called ""Quasitriangular""?","Why is the Quasitriangular Hopf algebra called ""Quasitriangular""?",,"The precise definition of a Quasitriangular Hopf algebra can be found on wikipedia . What is the reason behind the word ""Quasitriangular""? Is it because the R-matrix  is a triangular matrix, or is it related to some properties of triangles? Thanks for the enlightenment.","The precise definition of a Quasitriangular Hopf algebra can be found on wikipedia . What is the reason behind the word ""Quasitriangular""? Is it because the R-matrix  is a triangular matrix, or is it related to some properties of triangles? Thanks for the enlightenment.",,"['abstract-algebra', 'matrices', 'terminology', 'hopf-algebras', 'quantum-groups']"
53,Is it true that the order of $ab$ is always equal to the order of $ba$?,Is it true that the order of  is always equal to the order of ?,ab ba,"How do I prove that if $a$, $b$ are elements of group, then $o(ab) = o(ba)$? For some reason I end up doing the proof for abelian(ness?), i.e., I assume that the order of $ab$ is $2$ and do the steps that lead me to conclude that $ab=ba$, so the orders must be the same. Is that the right way to do it?","How do I prove that if $a$, $b$ are elements of group, then $o(ab) = o(ba)$? For some reason I end up doing the proof for abelian(ness?), i.e., I assume that the order of $ab$ is $2$ and do the steps that lead me to conclude that $ab=ba$, so the orders must be the same. Is that the right way to do it?",,"['abstract-algebra', 'group-theory']"
54,Can you construct a field with 6 elements? [duplicate],Can you construct a field with 6 elements? [duplicate],,This question already has answers here : Closed 11 years ago . Possible Duplicate: Is there anything like GF(6)? Could someone tell me if you can build a field with 6 elements.,This question already has answers here : Closed 11 years ago . Possible Duplicate: Is there anything like GF(6)? Could someone tell me if you can build a field with 6 elements.,,"['abstract-algebra', 'field-theory', 'finite-fields']"
55,Why $\mathbb{R}[X]/(X^2+1)\cong\mathbb{C}$?,Why ?,\mathbb{R}[X]/(X^2+1)\cong\mathbb{C},"There is this isomorphism in my notes but there is no explanation. So I tried to reason myself but still not convincing enough, or my reasoning may even be wrong. I will appreciate if anyone is willing to lend some helps. $\mathbb{R}[X]/(X^2+1)\cong\mathbb{C}$ . I have heard that we can achieve that by putting $i$ into $X$ in the denominator, but if we do that then the denominator will be $0$ , so $\mathbb{R}[X]/(0)=\mathbb{R}[X]$ ? Then how can it be isomorphic to $\mathbb{C}$ ? Thanks so much!","There is this isomorphism in my notes but there is no explanation. So I tried to reason myself but still not convincing enough, or my reasoning may even be wrong. I will appreciate if anyone is willing to lend some helps. . I have heard that we can achieve that by putting into in the denominator, but if we do that then the denominator will be , so ? Then how can it be isomorphic to ? Thanks so much!",\mathbb{R}[X]/(X^2+1)\cong\mathbb{C} i X 0 \mathbb{R}[X]/(0)=\mathbb{R}[X] \mathbb{C},"['abstract-algebra', 'ring-theory']"
56,"Examples of categories where epimorphism does not have a right inverse, not surjective","Examples of categories where epimorphism does not have a right inverse, not surjective",,"An epimorphism is defined as follows: $f \in \operatorname{Hom}_C(A,B)$ is an epimorphism if $\forall Z, \forall h', h'' \in \operatorname{Hom}_C(B, Z)$ then $h' f = h'' f \; \Rightarrow \; h' = h''$. I can't think of examples where epimorphism would not have a right inverse. Also, if I understand correctly, epimorphism is not surjective in the categories where we can't talk about surjection (where objects does not have internal structure?). Thanks in advance.","An epimorphism is defined as follows: $f \in \operatorname{Hom}_C(A,B)$ is an epimorphism if $\forall Z, \forall h', h'' \in \operatorname{Hom}_C(B, Z)$ then $h' f = h'' f \; \Rightarrow \; h' = h''$. I can't think of examples where epimorphism would not have a right inverse. Also, if I understand correctly, epimorphism is not surjective in the categories where we can't talk about surjection (where objects does not have internal structure?). Thanks in advance.",,"['category-theory', 'abstract-algebra']"
57,A new $4$-dimensional number system?,A new -dimensional number system?,4,"I want to create a new number system with the following rules: The number system is 4-dimensional over the reals, with $\{1,i,j,k\}$ as a basis. $i^2=j^2=k^2=1$ $ij=k$ The number system is a commutative ring. Is this number system valid? If so, does it have a name? And if not, why not? I tried to look for it online, but the closest I could get was the hyperbolic quaternions (which violate rule 4):","I want to create a new number system with the following rules: The number system is 4-dimensional over the reals, with $\{1,i,j,k\}$ as a basis. $i^2=j^2=k^2=1$ $ij=k$ The number system is a commutative ring. Is this number system valid? If so, does it have a name? And if not, why not? I tried to look for it online, but the closest I could get was the hyperbolic quaternions (which violate rule 4):",,"['abstract-algebra', 'ring-theory']"
58,Can a ring without a unit element have a subring with a unit element?,Can a ring without a unit element have a subring with a unit element?,,"A ring need not have a unit element, e.g. even numbers. Can someone give me an example of a ring which doesn't have a unit element but it's subring does have a unit element.","A ring need not have a unit element, e.g. even numbers. Can someone give me an example of a ring which doesn't have a unit element but it's subring does have a unit element.",,"['abstract-algebra', 'ring-theory']"
59,Can you give me an example of topological group which is not a Lie group.,Can you give me an example of topological group which is not a Lie group.,,I know the definitions of Lie group and topological group are different. Can you give me an example of topological group which is not a Lie group.,I know the definitions of Lie group and topological group are different. Can you give me an example of topological group which is not a Lie group.,,"['abstract-algebra', 'general-topology', 'group-theory', 'lie-groups', 'topological-groups']"
60,What's the name for the property of a function $f$ that means $f(f(x))=x$?,What's the name for the property of a function  that means ?,f f(f(x))=x,"I can think of several examples of functions such that twice application of the function is equivalent to no application of it. Additive inverse Multiplicative inverse Fourier transform Complex conjugation Any group built up from $\mathbb{Z}_2$, applying (one of) the $\mathbb{Z}_2$ parts' operation. ""Idempotent"" came to mind, but that's wrong. It means $f(f(x)) = f(x)$, not $f(f(x))=x$. What is the word for this ""flip-flop"" property?","I can think of several examples of functions such that twice application of the function is equivalent to no application of it. Additive inverse Multiplicative inverse Fourier transform Complex conjugation Any group built up from $\mathbb{Z}_2$, applying (one of) the $\mathbb{Z}_2$ parts' operation. ""Idempotent"" came to mind, but that's wrong. It means $f(f(x)) = f(x)$, not $f(f(x))=x$. What is the word for this ""flip-flop"" property?",,"['abstract-algebra', 'matrices', 'complex-numbers', 'definition']"
61,The proofs of the fundamental Theorem of Algebra [closed],The proofs of the fundamental Theorem of Algebra [closed],,Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 8 years ago . Improve this question There are many proofs of the fundamental theorem of algebra. Which are the most beautiful proofs?,Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 8 years ago . Improve this question There are many proofs of the fundamental theorem of algebra. Which are the most beautiful proofs?,,"['abstract-algebra', 'polynomials', 'complex-numbers', 'big-list']"
62,How to recognize adjointness?,How to recognize adjointness?,,"Reading math has gradually expanded my understanding of the word ""symmetry"", so that now I can recognize symmetries that I would have not noticed before, and without having them pointed out to me. I have a much harder time spotting ""adjointness"", even though I can generally get it once it is pointed out to me.  And I still have to ""work through"" it, ""flying by instruments"", so to speak; I don't ""see"" it. Is it possible to give an intuitive description of ""adjointness"" that would help one to spot ""adjoint pairs"" without their being labeled as such?  What is the concept of adjointness capturing that is worth having? Or if an intuitive description is not easy to come by, are there contrasting (but fundamentally equivalent) ways to arrive at the concept of adjointness?","Reading math has gradually expanded my understanding of the word ""symmetry"", so that now I can recognize symmetries that I would have not noticed before, and without having them pointed out to me. I have a much harder time spotting ""adjointness"", even though I can generally get it once it is pointed out to me.  And I still have to ""work through"" it, ""flying by instruments"", so to speak; I don't ""see"" it. Is it possible to give an intuitive description of ""adjointness"" that would help one to spot ""adjoint pairs"" without their being labeled as such?  What is the concept of adjointness capturing that is worth having? Or if an intuitive description is not easy to come by, are there contrasting (but fundamentally equivalent) ways to arrive at the concept of adjointness?",,"['abstract-algebra', 'soft-question', 'category-theory', 'adjoint-functors']"
63,On Similarities and Differences Between Right and Left Modules over a Ring,On Similarities and Differences Between Right and Left Modules over a Ring,,"I have read that for a ring $ R $ in general, right $ R $-modules are not the same things as left $ R $-modules. Why do we say that a right $ R $-module is equivalent to a left $ R $-module only when $ R $ is commutative? I feel that the commutativity of $ R $ is strictly an internal property of $ R $, so how can the commutativity of $ R $ affect scalar multiplication on an $ R $-module $ M $, which is an external operation?","I have read that for a ring $ R $ in general, right $ R $-modules are not the same things as left $ R $-modules. Why do we say that a right $ R $-module is equivalent to a left $ R $-module only when $ R $ is commutative? I feel that the commutativity of $ R $ is strictly an internal property of $ R $, so how can the commutativity of $ R $ affect scalar multiplication on an $ R $-module $ M $, which is an external operation?",,"['abstract-algebra', 'ring-theory', 'modules']"
64,Understanding quotients of $\mathbb{Q}[x]$,Understanding quotients of,\mathbb{Q}[x],"In general, I struggle working with quotients of polynomial rings, so I was hoping someone might be able to help with the following exercise: Show that $\mathbb{Q}[x]/(x^2-2x)$ is isomorphic to $\mathbb{Q}[x]/(x^2-1)$ , but the latter is not isomorphic to $\mathbb{Q}[x]/(x^2)$ . Mostly I want to understand what these quotients all ""look like."" We did few examples in class and few homework problems with such quotients, and the ones we do work with are much simpler (typically involving $\mathbb{Z}_2$ or $\mathbb{Z}_3$ or $\mathbb{R}$ instead of $\mathbb{Q}$ ). Any help is appreciated, thank you!","In general, I struggle working with quotients of polynomial rings, so I was hoping someone might be able to help with the following exercise: Show that is isomorphic to , but the latter is not isomorphic to . Mostly I want to understand what these quotients all ""look like."" We did few examples in class and few homework problems with such quotients, and the ones we do work with are much simpler (typically involving or or instead of ). Any help is appreciated, thank you!",\mathbb{Q}[x]/(x^2-2x) \mathbb{Q}[x]/(x^2-1) \mathbb{Q}[x]/(x^2) \mathbb{Z}_2 \mathbb{Z}_3 \mathbb{R} \mathbb{Q},"['abstract-algebra', 'ring-theory']"
65,"Infinite group has infinitely many subgroups, namely cyclic subgroups.","Infinite group has infinitely many subgroups, namely cyclic subgroups.",,"If $G$ is an infinite group then $G$ has infinitely many subgroups. Proof: Let's consider the following set: $C=\{\left \langle g \right \rangle: g\in G \}$ - collection of all cyclic subgroups in $G$ generated by elements of $G$. Two cases are possible: Exists infinitely many distinct cyclic subgroups $\Rightarrow$ We are done. Exists finitely many distinct cyclic subgroups for example $C=\{H_1, H_2,\dots, H_n\}$. Then $G=\bigcup \limits_{i=1}^{n}H_i$. Since $G$ is infinite then WLOG suppose that $H_1$ is also infinite, where $H_1=\left \langle g_1 \right \rangle$. Let's consider the following set $\{\left \langle g_1^n \right \rangle: n\in \mathbb{N}\}$ - the collection of all cyclic sugroups of $H_1\subset G.$  Let $K_1=\left \langle g_1 \right \rangle$, $K_2=\left \langle g_1^2 \right \rangle$, $K_3=\left \langle g_1^3 \right \rangle$, $\dots$. It's easy to show that $K_n$ and $K_m$ are distinct for $n\neq m$. Indeed, WLOG take $n<m$ and taking $g_1^n\in K_n$ but $g_1^n\notin K_m$ otherwise $g_1^n=g_1^{ml}$ where $l\in \mathbb{Z}$ $\Rightarrow$ $g_1^{n-ml}=e$ and since $H_1$ is infinite $\Rightarrow$ $n=ml$ which is contradiciton since $m>n$. Thus, the subgroups $K_n$ for any $n\in \mathbb{N}$ are cyclic subgroups of $H_1$ $\Rightarrow$ cyclic subgroups of $G$. Is this reasoning correct?","If $G$ is an infinite group then $G$ has infinitely many subgroups. Proof: Let's consider the following set: $C=\{\left \langle g \right \rangle: g\in G \}$ - collection of all cyclic subgroups in $G$ generated by elements of $G$. Two cases are possible: Exists infinitely many distinct cyclic subgroups $\Rightarrow$ We are done. Exists finitely many distinct cyclic subgroups for example $C=\{H_1, H_2,\dots, H_n\}$. Then $G=\bigcup \limits_{i=1}^{n}H_i$. Since $G$ is infinite then WLOG suppose that $H_1$ is also infinite, where $H_1=\left \langle g_1 \right \rangle$. Let's consider the following set $\{\left \langle g_1^n \right \rangle: n\in \mathbb{N}\}$ - the collection of all cyclic sugroups of $H_1\subset G.$  Let $K_1=\left \langle g_1 \right \rangle$, $K_2=\left \langle g_1^2 \right \rangle$, $K_3=\left \langle g_1^3 \right \rangle$, $\dots$. It's easy to show that $K_n$ and $K_m$ are distinct for $n\neq m$. Indeed, WLOG take $n<m$ and taking $g_1^n\in K_n$ but $g_1^n\notin K_m$ otherwise $g_1^n=g_1^{ml}$ where $l\in \mathbb{Z}$ $\Rightarrow$ $g_1^{n-ml}=e$ and since $H_1$ is infinite $\Rightarrow$ $n=ml$ which is contradiciton since $m>n$. Thus, the subgroups $K_n$ for any $n\in \mathbb{N}$ are cyclic subgroups of $H_1$ $\Rightarrow$ cyclic subgroups of $G$. Is this reasoning correct?",,"['abstract-algebra', 'group-theory', 'solution-verification', 'infinite-groups']"
66,"$\gcd(a,b,c)\!=\!1\Rightarrow \gcd(az+b,c)\!=\! 1$ for some $z$ [Coprime Dirichlet Theorem]",for some  [Coprime Dirichlet Theorem],"\gcd(a,b,c)\!=\!1\Rightarrow \gcd(az+b,c)\!=\! 1 z","I can't crack this one. Prove: If $\gcd(a,b,c)=1$ then there exists $z$ such that $\gcd(az+b,c) = 1$ (the only constraint is that $a,b,c,z \in \mathbb{Z}$ and $c\neq 0)$",I can't crack this one. Prove: If then there exists such that (the only constraint is that and,"\gcd(a,b,c)=1 z \gcd(az+b,c) = 1 a,b,c,z \in \mathbb{Z} c\neq 0)","['abstract-algebra', 'elementary-number-theory', 'divisibility', 'gcd-and-lcm']"
67,Finding the order of permutations in $S_8$,Finding the order of permutations in,S_8,"Consider the group $S_8$. What is the order of $\sigma = (4,5)(2,3,7)$ and $\tau = (1,4)(3,5,7,8)$? My book says I should just use a trick by the order of a permutation expressed as a product of disjoint cycles is the least common multiple of the lengths of the cycles . I don't understand the trick very well and would like to see how the counting process here actually works. I can count for non disjoint ones like say $(1,2,3,8)$ has order 4 I don't know how to count $\sigma = \begin{pmatrix} 1 &2  &3  &4  &5  &6  &7  &8 \\  1 &3  &7  &5  &4  &6  &2  &8  \end{pmatrix}$ EDIT Just so I am clear, the answer for the order $\sigma$ is 6 and $\tau$ is 4. I am still confused as to how they got this I have an extremely poor understanding of this subject so please help me! Thank you","Consider the group $S_8$. What is the order of $\sigma = (4,5)(2,3,7)$ and $\tau = (1,4)(3,5,7,8)$? My book says I should just use a trick by the order of a permutation expressed as a product of disjoint cycles is the least common multiple of the lengths of the cycles . I don't understand the trick very well and would like to see how the counting process here actually works. I can count for non disjoint ones like say $(1,2,3,8)$ has order 4 I don't know how to count $\sigma = \begin{pmatrix} 1 &2  &3  &4  &5  &6  &7  &8 \\  1 &3  &7  &5  &4  &6  &2  &8  \end{pmatrix}$ EDIT Just so I am clear, the answer for the order $\sigma$ is 6 and $\tau$ is 4. I am still confused as to how they got this I have an extremely poor understanding of this subject so please help me! Thank you",,"['abstract-algebra', 'permutations']"
68,"How to prove $\,{\rm order}(a^k) = n/\gcd(n,k)\,$ for $\,n={\rm order}(a)$?",How to prove  for ?,"\,{\rm order}(a^k) = n/\gcd(n,k)\, \,n={\rm order}(a)","This is an exercise from ""Contemporary Abstract Algebra"" I'm not sure how to solve. Exercise: Let $\langle a\rangle $ be a (cyclic) group of order $n$. Prove that the order of $a^k=\frac{n}{\gcd(n,k)}$. Direction: (1) Let $d=\gcd(n,k)$, thus by the Euclidian algorithm we can find $X,Y\in\mathbb{Z}$ s.t. $d=Xn+Yk$, thus, $a^d=a^{Xn+Yk}=a^{Xn}a^{Yk}=(a^n)^X(a^k)^Y=(a^k)^Y$. What to do from here? (2) We know that $d|n$, thus $\langle a^{n/d} \rangle$ is of order $d$ and $\langle a^d \rangle$ is of order $\frac{n}{d}=\frac{n}{\gcd(n,k)}$. Is it mean that $d=k$? Where is my mistake?","This is an exercise from ""Contemporary Abstract Algebra"" I'm not sure how to solve. Exercise: Let $\langle a\rangle $ be a (cyclic) group of order $n$. Prove that the order of $a^k=\frac{n}{\gcd(n,k)}$. Direction: (1) Let $d=\gcd(n,k)$, thus by the Euclidian algorithm we can find $X,Y\in\mathbb{Z}$ s.t. $d=Xn+Yk$, thus, $a^d=a^{Xn+Yk}=a^{Xn}a^{Yk}=(a^n)^X(a^k)^Y=(a^k)^Y$. What to do from here? (2) We know that $d|n$, thus $\langle a^{n/d} \rangle$ is of order $d$ and $\langle a^d \rangle$ is of order $\frac{n}{d}=\frac{n}{\gcd(n,k)}$. Is it mean that $d=k$? Where is my mistake?",,"['abstract-algebra', 'group-theory', 'finite-groups']"
69,Showing that $\sqrt \pi$ is transcendental,Showing that  is transcendental,\sqrt \pi,"I'm trying to use the fact that $\pi$ is transcendental to show that $\sqrt \pi$ is also transcendental over $\Bbb{Q}$ . I don't know any theorems about algebraic and non-algebraic numbers so I don't have much in my tool box. Here's what I have done so far : Let $P(x) \in \Bbb{Q}[x]$ and write $P(x)$ in the following way $$P(x)= \underbrace{ \left ( \sum_{\text{$j$ even}} a_j x^j +a_0 \right ) }_{Q(x)}+\underbrace{\sum_{\text{$i$ odd}} a_i x^i }_{G(x)}$$ Then $P(\sqrt \pi )= Q (\sqrt \pi) + G(\sqrt \pi)$ , $ Q (\sqrt \pi) =\displaystyle \sum_{\text{$j$ even}} a_j  \sqrt\pi ^j+a_0  =\sum_{\text{$j$ even}} a_j \pi^{\frac j 2} +a_0$ hence $Q (\sqrt \pi) \not= 0$ since $\pi$ is transcendental, Similarly I showed $G(\sqrt \pi)$ is also non zero by taking $\sqrt \pi$ as a common factor. I'm now stuck on showing that $G(\sqrt \pi) \not= -Q(\sqrt \pi) $ . If can prove that then $P(\sqrt \pi) \not= 0$.  How can it be done?","I'm trying to use the fact that $\pi$ is transcendental to show that $\sqrt \pi$ is also transcendental over $\Bbb{Q}$ . I don't know any theorems about algebraic and non-algebraic numbers so I don't have much in my tool box. Here's what I have done so far : Let $P(x) \in \Bbb{Q}[x]$ and write $P(x)$ in the following way $$P(x)= \underbrace{ \left ( \sum_{\text{$j$ even}} a_j x^j +a_0 \right ) }_{Q(x)}+\underbrace{\sum_{\text{$i$ odd}} a_i x^i }_{G(x)}$$ Then $P(\sqrt \pi )= Q (\sqrt \pi) + G(\sqrt \pi)$ , $ Q (\sqrt \pi) =\displaystyle \sum_{\text{$j$ even}} a_j  \sqrt\pi ^j+a_0  =\sum_{\text{$j$ even}} a_j \pi^{\frac j 2} +a_0$ hence $Q (\sqrt \pi) \not= 0$ since $\pi$ is transcendental, Similarly I showed $G(\sqrt \pi)$ is also non zero by taking $\sqrt \pi$ as a common factor. I'm now stuck on showing that $G(\sqrt \pi) \not= -Q(\sqrt \pi) $ . If can prove that then $P(\sqrt \pi) \not= 0$.  How can it be done?",,['abstract-algebra']
70,Inverse function of isomorphism is also isomorphism,Inverse function of isomorphism is also isomorphism,,"Let $G$ be a group, and let $p:G\rightarrow G$ be an isomorphism. Why is $p^{-1}$ also an isomorphism? We know that $p(a)p(b)=p(ab)$ for any elements $a,b\in G$. We also know $p(a^{-1})=p(a)^{-1}$ for any element $a\in G$ (follows from the first statement.) How would it show $p^{-1}(ab)=p^{-1}(a)p^{-1}(b)$?","Let $G$ be a group, and let $p:G\rightarrow G$ be an isomorphism. Why is $p^{-1}$ also an isomorphism? We know that $p(a)p(b)=p(ab)$ for any elements $a,b\in G$. We also know $p(a^{-1})=p(a)^{-1}$ for any element $a\in G$ (follows from the first statement.) How would it show $p^{-1}(ab)=p^{-1}(a)p^{-1}(b)$?",,"['abstract-algebra', 'group-theory']"
71,"When people say ""an algebra"" do they always mean ""an algebra over a field""?","When people say ""an algebra"" do they always mean ""an algebra over a field""?",,"I don't have much experience with abstract algebra. I'm only versed in linear algebra and vector spaces, and have had a tiny introduction to algebras over fields. However, this question is a purely terminological one: When people say ""an algebra"" do they always mean ""an algebra over a field""? If not, what other things can it refer to?","I don't have much experience with abstract algebra. I'm only versed in linear algebra and vector spaces, and have had a tiny introduction to algebras over fields. However, this question is a purely terminological one: When people say ""an algebra"" do they always mean ""an algebra over a field""? If not, what other things can it refer to?",,"['abstract-algebra', 'terminology']"
72,How to show every field is a Euclidean Domain.,How to show every field is a Euclidean Domain.,,I'm having trouble proving this. This is what I have so far: Let $F$ be a field. Let $v(x) \rightarrow 1$ for all $x$ not equal to $0$. So if we let $x$ be in $F$ where $x$ not zero then we can write $x$ as: $x=qy+r$ for some $y$ in $F$. If $r$ not zero then $V(r)=1$. Not sure what to do from here. I know eventually I'm supposed to get no remainder $(r=0)$ but I'm stuck at applying the definition and valuation map.,I'm having trouble proving this. This is what I have so far: Let $F$ be a field. Let $v(x) \rightarrow 1$ for all $x$ not equal to $0$. So if we let $x$ be in $F$ where $x$ not zero then we can write $x$ as: $x=qy+r$ for some $y$ in $F$. If $r$ not zero then $V(r)=1$. Not sure what to do from here. I know eventually I'm supposed to get no remainder $(r=0)$ but I'm stuck at applying the definition and valuation map.,,"['abstract-algebra', 'field-theory', 'integral-domain', 'euclidean-algorithm']"
73,"Let $H$ be a subgroup of a group $G$ such that $x^2 \in H$ , $\forall x\in G$ . Prove that $H$ is a normal subgroup of $G$","Let  be a subgroup of a group  such that  ,  . Prove that  is a normal subgroup of",H G x^2 \in H \forall x\in G H G,"Let $H$ be a subgroup of a group $G$ such that $x^2 \in H$, $\forall x\in G$. Prove that $H$ is a normal subgroup of $G$. I have tried to using the definition but failed. Can someone help me please.","Let $H$ be a subgroup of a group $G$ such that $x^2 \in H$, $\forall x\in G$. Prove that $H$ is a normal subgroup of $G$. I have tried to using the definition but failed. Can someone help me please.",,"['abstract-algebra', 'group-theory', 'normal-subgroups']"
74,Direct summand of a free module,Direct summand of a free module,,"Let $M$, $L$, $N$ be $A$-modules and $M=N\oplus L$. If $M$ and $N$ are free, is $L$ necessarily free?","Let $M$, $L$, $N$ be $A$-modules and $M=N\oplus L$. If $M$ and $N$ are free, is $L$ necessarily free?",,"['abstract-algebra', 'commutative-algebra', 'modules']"
75,Cayley Table Sudoku,Cayley Table Sudoku,,"Given an $n\times n$ grid partially filled with the numbers $0,\ldots, n-1$ , we can play a Sudoku-like game by trying to fill in the rest of the grid so that the end result is the Cayley table for a group (edit: where position $i,j$ in the table would represent the product of $i $ and $j$ in the group structure). I am curious about the following value: let $g(n)$ represent the smallest natural number such that if $g(n)$ squares in an $n\times n$ grid are filled, then the configuration always can be extended to a solution in at most $1$ way. Another way to think of this question is as follows: How similar can two distinct $n\times n$ Cayley tables be? $g(n)$ is the smallest natural number such that if two $n\times n$ Cayley tables agree for $g(n)$ entries, then they are identical. What does $g(n)$ look like? I'm most interested in the asymptotic behaviour (as I imagine $g(n)$ may fluctuate wildly as $n$ increases due to the fluctuation in the number of groups of order $n$ as $n$ increases). One particularly interesting question: can $g(n)\over n^2$ be bounded above by a constant (say, 0.99)? In other words, if two Cayley tables agree on $99\%$ of entries, must they be identical? And if so, how much lower than $0.99$ can we bring this constant?","Given an grid partially filled with the numbers , we can play a Sudoku-like game by trying to fill in the rest of the grid so that the end result is the Cayley table for a group (edit: where position in the table would represent the product of and in the group structure). I am curious about the following value: let represent the smallest natural number such that if squares in an grid are filled, then the configuration always can be extended to a solution in at most way. Another way to think of this question is as follows: How similar can two distinct Cayley tables be? is the smallest natural number such that if two Cayley tables agree for entries, then they are identical. What does look like? I'm most interested in the asymptotic behaviour (as I imagine may fluctuate wildly as increases due to the fluctuation in the number of groups of order as increases). One particularly interesting question: can be bounded above by a constant (say, 0.99)? In other words, if two Cayley tables agree on of entries, must they be identical? And if so, how much lower than can we bring this constant?","n\times n 0,\ldots, n-1 i,j i  j g(n) g(n) n\times n 1 n\times n g(n) n\times n g(n) g(n) g(n) n n n g(n)\over n^2 99\% 0.99","['abstract-algebra', 'group-theory', 'information-theory', 'cayley-table']"
76,Counterexample: $G \times K \cong H \times K \implies G \cong H$,Counterexample:,G \times K \cong H \times K \implies G \cong H,"I am having a hard time finding a counterexample for the statement: $G \times K \cong H \times K \implies G \cong H$ I think this should be true for abelian, finite groups. But is this true in general? What would be a counterexample? Any hints appreciated!  Thanks","I am having a hard time finding a counterexample for the statement: $G \times K \cong H \times K \implies G \cong H$ I think this should be true for abelian, finite groups. But is this true in general? What would be a counterexample? Any hints appreciated!  Thanks",,"['abstract-algebra', 'group-theory', 'examples-counterexamples']"
77,Question about perfect pairings,Question about perfect pairings,,Suppose we had a bilinear map $V \otimes W \rightarrow \mathbb{C}$. What is meant by the fact that this map is a perfect pairing? How does one go about and show that something is a perfect pairing?,Suppose we had a bilinear map $V \otimes W \rightarrow \mathbb{C}$. What is meant by the fact that this map is a perfect pairing? How does one go about and show that something is a perfect pairing?,,"['abstract-algebra', 'tensor-products']"
78,What does an outer automorphism look like?,What does an outer automorphism look like?,,"I am working on a project in my group theory class to find an outer automorphism of $S_6$, which has already been addressed at length on this site and others. I have a prescription for how to go about finding this guy, but I have a larger conceptual question - what does an outer automorphism really look like? Is there an intuitive way to understand the difference between an inner and an outer automorphism? Inner automorphisms have always seemed easier for me to understand since we have an explicit representation $ghg^{-1}$ for members of the group. I can also understand this representation in terms of the Rubik's cube - rotate an edge, rotate a perpendicular edge, and the rotate the other edge back (is not the same as just rotating the perpendicular edge). What does an ""outer automorphism"" look like?","I am working on a project in my group theory class to find an outer automorphism of $S_6$, which has already been addressed at length on this site and others. I have a prescription for how to go about finding this guy, but I have a larger conceptual question - what does an outer automorphism really look like? Is there an intuitive way to understand the difference between an inner and an outer automorphism? Inner automorphisms have always seemed easier for me to understand since we have an explicit representation $ghg^{-1}$ for members of the group. I can also understand this representation in terms of the Rubik's cube - rotate an edge, rotate a perpendicular edge, and the rotate the other edge back (is not the same as just rotating the perpendicular edge). What does an ""outer automorphism"" look like?",,"['abstract-algebra', 'group-theory', 'intuition', 'symmetric-groups', 'group-actions']"
79,How to solve a cyclic quintic in radicals?,How to solve a cyclic quintic in radicals?,,"Galois theory tells us that $\frac{z^{11}-1}{z-1} = z^{10} + z^9 + z^8 + z^7 + z^6 + z^5 + z^4 + z^3 + z^2 + z + 1$ can be solved in radicals because its group is solvable. Actually performing the calculation is beyond me, though - here what I have got so far: Let the roots be $\zeta^1,\zeta^2,\ldots,\zeta^{10}$, following Gauss we can split the problem into solving quintics and quadratics by looking at subgroups of the roots. Since 2 is a generator of the group $[2,4,8,5,10,9,7,3,6,1]$ we can partition into the five subgroups of conjugate pairs $[2,9]$,$[4,7]$,$[8,3]$,$[5,6]$,$[10,1]$. Now put $q_1 = \zeta^2+\zeta^9$,$q_2 = \zeta^4+\zeta^7$,$q_3 = \zeta^8+\zeta^3$,$q_4 = \zeta^5+\zeta^6$,$q_5 = \zeta^{10}+\zeta^1$. So if we can solve the quintic $(q - q_1)(q - q_2)(q - q_3)(q - q_4)(q - q_5) = q^5 + q^4 - 4q^3 - 3q^2 + 3q + 1 = 0$ we would just be left to solve a few quadratic equations. Now pari/gp tells me this quintic has the cyclic group C(5): ? polgalois(x^5 + x^4 - 4*x^3 - 3*x^2 + 3*x + 1) %1 = [5, 1, 1, ""C(5) = 5""] I've worked through examples of solving quadratic and cubic equations based on the galois group but when it comes to this quintic I'm completely stumped so any advice would be tremendously helpful! Thanks. Edit: Thanks to Robin Chapman the problem is reduced significantly. Let $\omega$ be a primitive 5th root of unity (which is easy to express in radicals), it only remains to express $(q_1 + \omega q_2  + \omega^2 q_3  + \omega^3 q_4  + \omega^4 q_5)^5$ in terms of rationals and powers of $\omega$ (and then everything can be substituted back and solved easily). We know this is possible because the term is fixed by the quintics galois group, how to actually perform this evades me but I will try to find a way.","Galois theory tells us that $\frac{z^{11}-1}{z-1} = z^{10} + z^9 + z^8 + z^7 + z^6 + z^5 + z^4 + z^3 + z^2 + z + 1$ can be solved in radicals because its group is solvable. Actually performing the calculation is beyond me, though - here what I have got so far: Let the roots be $\zeta^1,\zeta^2,\ldots,\zeta^{10}$, following Gauss we can split the problem into solving quintics and quadratics by looking at subgroups of the roots. Since 2 is a generator of the group $[2,4,8,5,10,9,7,3,6,1]$ we can partition into the five subgroups of conjugate pairs $[2,9]$,$[4,7]$,$[8,3]$,$[5,6]$,$[10,1]$. Now put $q_1 = \zeta^2+\zeta^9$,$q_2 = \zeta^4+\zeta^7$,$q_3 = \zeta^8+\zeta^3$,$q_4 = \zeta^5+\zeta^6$,$q_5 = \zeta^{10}+\zeta^1$. So if we can solve the quintic $(q - q_1)(q - q_2)(q - q_3)(q - q_4)(q - q_5) = q^5 + q^4 - 4q^3 - 3q^2 + 3q + 1 = 0$ we would just be left to solve a few quadratic equations. Now pari/gp tells me this quintic has the cyclic group C(5): ? polgalois(x^5 + x^4 - 4*x^3 - 3*x^2 + 3*x + 1) %1 = [5, 1, 1, ""C(5) = 5""] I've worked through examples of solving quadratic and cubic equations based on the galois group but when it comes to this quintic I'm completely stumped so any advice would be tremendously helpful! Thanks. Edit: Thanks to Robin Chapman the problem is reduced significantly. Let $\omega$ be a primitive 5th root of unity (which is easy to express in radicals), it only remains to express $(q_1 + \omega q_2  + \omega^2 q_3  + \omega^3 q_4  + \omega^4 q_5)^5$ in terms of rationals and powers of $\omega$ (and then everything can be substituted back and solved easily). We know this is possible because the term is fixed by the quintics galois group, how to actually perform this evades me but I will try to find a way.",,['abstract-algebra']
80,Simplifying Quotient of Tensor Products,Simplifying Quotient of Tensor Products,,"Consider $$(A\otimes C)/(B\otimes C)$$ where $B$ is a submodule of $A$. ($A,B,C$ are $R$-modules). Is it true that $$(A\otimes C)/(B\otimes C)\cong(A/B)\otimes C$$? Thanks. If no, are there any easy counter-examples?","Consider $$(A\otimes C)/(B\otimes C)$$ where $B$ is a submodule of $A$. ($A,B,C$ are $R$-modules). Is it true that $$(A\otimes C)/(B\otimes C)\cong(A/B)\otimes C$$? Thanks. If no, are there any easy counter-examples?",,"['abstract-algebra', 'tensor-products']"
81,Proof that every field is perfect?,Proof that every field is perfect?,,"The following must be wrong, since it shows that every field is perfect, which I gather is not so. But I can't find the error: Suppose $E/K$ is a field extension and $p\in K[x]$ is irreducible (in $K[x]$ ). Then every root of $p$ in $E$ is simple. Proof: Suppose OTOH that $\lambda\in E$ and $(x-\lambda)^2\mid p(x)$ . Then $(x-\lambda)\mid p'$ , so $\gcd_E(p,p')\ne1$ . But the euclidean algorithm shows that $\gcd_K(p,p')=\gcd_E(p,p')$ , hence $p$ is not irreducible.","The following must be wrong, since it shows that every field is perfect, which I gather is not so. But I can't find the error: Suppose is a field extension and is irreducible (in ). Then every root of in is simple. Proof: Suppose OTOH that and . Then , so . But the euclidean algorithm shows that , hence is not irreducible.","E/K p\in K[x] K[x] p E \lambda\in E (x-\lambda)^2\mid p(x) (x-\lambda)\mid p' \gcd_E(p,p')\ne1 \gcd_K(p,p')=\gcd_E(p,p') p","['abstract-algebra', 'field-theory', 'extension-field', 'fake-proofs']"
82,Is there a nontrivial semidirect product of two groups isomorphic to their direct product?,Is there a nontrivial semidirect product of two groups isomorphic to their direct product?,,"Suppose $N$ and $H$ are groups and $\phi: H \rightarrow \operatorname{Aut}(N)$ is a homomorphism. We know that $N \rtimes_{\phi} H = N \times H$ if and only if $\phi$ is trivial, but this question is a bit different. Does $N \rtimes_{\phi} H \cong N \times H$ imply that $\phi$ is trivial? My first idea is that there should be a counterexample, but I haven't been able to figure out anything yet. Since nontrivial semidirect products are always nonabelian, we definitely need at least one of $N$ or $H$ nonabelian. I think finding a counterexample to the statement would also be equivalent to finding $G$ such that $G = NH = N'H'$ where $N \cap H = N' \cap H' = 1$ $N \cong N'$ and $H \cong H'$ $N, N', H' \trianglelefteq G$  but $H$ is not normal in $G$","Suppose $N$ and $H$ are groups and $\phi: H \rightarrow \operatorname{Aut}(N)$ is a homomorphism. We know that $N \rtimes_{\phi} H = N \times H$ if and only if $\phi$ is trivial, but this question is a bit different. Does $N \rtimes_{\phi} H \cong N \times H$ imply that $\phi$ is trivial? My first idea is that there should be a counterexample, but I haven't been able to figure out anything yet. Since nontrivial semidirect products are always nonabelian, we definitely need at least one of $N$ or $H$ nonabelian. I think finding a counterexample to the statement would also be equivalent to finding $G$ such that $G = NH = N'H'$ where $N \cap H = N' \cap H' = 1$ $N \cong N'$ and $H \cong H'$ $N, N', H' \trianglelefteq G$  but $H$ is not normal in $G$",,"['abstract-algebra', 'group-theory']"
83,"If $(|G|, |H|) > 1$, does it follow that $\operatorname{Aut}(G \times H) \neq \operatorname{Aut}(G) \times \operatorname{Aut}(H)$?","If , does it follow that ?","(|G|, |H|) > 1 \operatorname{Aut}(G \times H) \neq \operatorname{Aut}(G) \times \operatorname{Aut}(H)","Let $G$ and $H$ be finite groups. If $|G|$ and $|H|$ are coprime, then $$\operatorname{Aut}(G \times H) \cong \operatorname{Aut}(G) \times \operatorname{Aut}(H)$$ holds. What about when $(|G|, |H|) > 1$? In this case we know that $\operatorname{Aut}(G) \times \operatorname{Aut}(H)$ is contained in $\operatorname{Aut}(G \times H)$, but the isomorphism above might not hold. For example $\operatorname{Aut}(C_2 \times C_2)$ has order $6$ but $\operatorname{Aut}(C_2) \times \operatorname{Aut}(C_2)$ is  trivial. Is the isomorphism possible at all when $(|G|, |H|) > 1$?","Let $G$ and $H$ be finite groups. If $|G|$ and $|H|$ are coprime, then $$\operatorname{Aut}(G \times H) \cong \operatorname{Aut}(G) \times \operatorname{Aut}(H)$$ holds. What about when $(|G|, |H|) > 1$? In this case we know that $\operatorname{Aut}(G) \times \operatorname{Aut}(H)$ is contained in $\operatorname{Aut}(G \times H)$, but the isomorphism above might not hold. For example $\operatorname{Aut}(C_2 \times C_2)$ has order $6$ but $\operatorname{Aut}(C_2) \times \operatorname{Aut}(C_2)$ is  trivial. Is the isomorphism possible at all when $(|G|, |H|) > 1$?",,"['abstract-algebra', 'group-theory', 'finite-groups']"
84,Understanding of graded algebra,Understanding of graded algebra,,"I am recently learning from Loring W. Tu's An Introduction to Manifolds the concept graded algebra , which is used for introducing exterior algebra. I don't understand the following definition: An algebra $A$ over a field $K$ is said to be graded if it can be written as a direct sum $A=\bigoplus_{k=0}^{\infty}A^k$ of vector spaces over $K$ such that the multiplication map sends $A^k\times A^l$ to $A^{k+l}$. Here are the questions: What does $k$ in $A^k$ mean? Is it a superscript or the power of $A$? (If it is the power, what does $A^0$ mean?) Since I don't know much but some very basic knowledge in abstract algebra, I am trying to understand the concept with some simple examples. (I don't quite understand the wiki article of graded algebra ). Is there any example as simple as the linear space ${\mathbb R}^n$ for the graded algebra? And what does it look like concretely?","I am recently learning from Loring W. Tu's An Introduction to Manifolds the concept graded algebra , which is used for introducing exterior algebra. I don't understand the following definition: An algebra $A$ over a field $K$ is said to be graded if it can be written as a direct sum $A=\bigoplus_{k=0}^{\infty}A^k$ of vector spaces over $K$ such that the multiplication map sends $A^k\times A^l$ to $A^{k+l}$. Here are the questions: What does $k$ in $A^k$ mean? Is it a superscript or the power of $A$? (If it is the power, what does $A^0$ mean?) Since I don't know much but some very basic knowledge in abstract algebra, I am trying to understand the concept with some simple examples. (I don't quite understand the wiki article of graded algebra ). Is there any example as simple as the linear space ${\mathbb R}^n$ for the graded algebra? And what does it look like concretely?",,['abstract-algebra']
85,About direct sum of abelian groups and quotient,About direct sum of abelian groups and quotient,,"I'm trying to understand properly the relations between quotient and direct sum. The first thing I wanted to know, and couldn't find online, is whether my guess is true or not: Assume $G_\alpha$ are abelian groups, and $H_\alpha \leq G_\alpha$ a subgroup, is $\frac{\bigoplus G_\alpha}{\bigoplus H_\alpha}\cong\bigoplus\frac{G_\alpha}{H_\alpha}$? the reason i thought it would be true is because for every $G_\alpha$ we have that the only group on the nominator that affects $G_\alpha$ is $H_\alpha$. and it just seems symetric and reasonable.. thanks","I'm trying to understand properly the relations between quotient and direct sum. The first thing I wanted to know, and couldn't find online, is whether my guess is true or not: Assume $G_\alpha$ are abelian groups, and $H_\alpha \leq G_\alpha$ a subgroup, is $\frac{\bigoplus G_\alpha}{\bigoplus H_\alpha}\cong\bigoplus\frac{G_\alpha}{H_\alpha}$? the reason i thought it would be true is because for every $G_\alpha$ we have that the only group on the nominator that affects $G_\alpha$ is $H_\alpha$. and it just seems symetric and reasonable.. thanks",,"['abstract-algebra', 'group-theory', 'abelian-groups', 'quotient-spaces', 'direct-sum']"
86,Learning Abstract Algebra for a graduate degree,Learning Abstract Algebra for a graduate degree,,"I would like to do a graduate degree in mathematics, and I have a full year before I will be able to do so (for personal reasons). I mainly have my weekends available to study. I am interested in Abstract Algebra in general, I really liked courses like groups, rings, field theory/Galois theory etc'. I have always found analysis to be difficult for me and courses in the scope of Abstract Algebra felt more natural and intuitive (though I did have some difficulty with some of the more advanced parts of those courses). I was told that other then taking some credit points I will get a question from the supervisor, one that is not solved yet but seems at a reasonable difficulty to him, and I will have and I will have to try and find a solution for it. I would like to prepare myself, in this year, to have control over varies subjects in Abstract Algebra so that I will be more likely to be able to solve such a question (or even understand it, as there are some subjects in Abstract Algebra that I have not learned in any of the courses I took as an undergraduate student). Also, I have lately been aware of some subjects that involve both analysis and Abstract Algebra, such as topological groups. I would be happy if I could avoid such topics, but I don't know what type of mathematics is studied in a graduate degree level, so this leads me to the following questions: What topics of Abstract Algebra should I study in depth ? what topics in Abstract Algebra should I be familiar with their basics ? Are there any topics in analysis, topology etc' that are likely to be needed for answering a graduate degree level type of questions ? What should be the focus of my work, should I try to do many exercises within the text, or focus on the proofs and the theory ? I have the book Abstract Algebra by Dummit and Foote to study with, as well as books in other area of mathematics such as Topology by Monkers that might help me with this goal. I would be extremely grateful for hearing opinions and advices on this matter! Added: Note 1: I would like to mention that although I try to avoid analysis, I still had to take courses in that, so I don't lack elementary knowledge in many topics, I have taken: Introduction to functional analysis, Real analysis (measure theory), complex analysis, ODE, Introduction to numerical analysis, Introduction to probability theory (the course didn't talk about $\sigma$ algebras and etc' but we did talk about random variables, CLT, etc'). But I don't consider myself to be good at those topics (except maybe probability that I really liked), I understand them, but I am about average at them, so I don't expect that I would be able to do something non-trivial at those topics. I would like to extend my questions to include the complement of my question to what to study for what I shouldn't spend my time on: 4) Are there topics in Abstract Algebra, or other in other areas that I would need to know (maybe topology ?) that I can skip some parts of (mainly non-core topics that are hard to learn) since they would probably not help me (and due to lack of time) ?","I would like to do a graduate degree in mathematics, and I have a full year before I will be able to do so (for personal reasons). I mainly have my weekends available to study. I am interested in Abstract Algebra in general, I really liked courses like groups, rings, field theory/Galois theory etc'. I have always found analysis to be difficult for me and courses in the scope of Abstract Algebra felt more natural and intuitive (though I did have some difficulty with some of the more advanced parts of those courses). I was told that other then taking some credit points I will get a question from the supervisor, one that is not solved yet but seems at a reasonable difficulty to him, and I will have and I will have to try and find a solution for it. I would like to prepare myself, in this year, to have control over varies subjects in Abstract Algebra so that I will be more likely to be able to solve such a question (or even understand it, as there are some subjects in Abstract Algebra that I have not learned in any of the courses I took as an undergraduate student). Also, I have lately been aware of some subjects that involve both analysis and Abstract Algebra, such as topological groups. I would be happy if I could avoid such topics, but I don't know what type of mathematics is studied in a graduate degree level, so this leads me to the following questions: What topics of Abstract Algebra should I study in depth ? what topics in Abstract Algebra should I be familiar with their basics ? Are there any topics in analysis, topology etc' that are likely to be needed for answering a graduate degree level type of questions ? What should be the focus of my work, should I try to do many exercises within the text, or focus on the proofs and the theory ? I have the book Abstract Algebra by Dummit and Foote to study with, as well as books in other area of mathematics such as Topology by Monkers that might help me with this goal. I would be extremely grateful for hearing opinions and advices on this matter! Added: Note 1: I would like to mention that although I try to avoid analysis, I still had to take courses in that, so I don't lack elementary knowledge in many topics, I have taken: Introduction to functional analysis, Real analysis (measure theory), complex analysis, ODE, Introduction to numerical analysis, Introduction to probability theory (the course didn't talk about $\sigma$ algebras and etc' but we did talk about random variables, CLT, etc'). But I don't consider myself to be good at those topics (except maybe probability that I really liked), I understand them, but I am about average at them, so I don't expect that I would be able to do something non-trivial at those topics. I would like to extend my questions to include the complement of my question to what to study for what I shouldn't spend my time on: 4) Are there topics in Abstract Algebra, or other in other areas that I would need to know (maybe topology ?) that I can skip some parts of (mainly non-core topics that are hard to learn) since they would probably not help me (and due to lack of time) ?",,"['abstract-algebra', 'soft-question', 'self-learning', 'advice', 'learning']"
87,What exactly is a tensor product?,What exactly is a tensor product?,,"This is a beginner's question on what exactly is a tensor product, in laymen's term, for a beginner who has just learned basic group theory and basic ring theory. I do understand from wikipedia that in some cases, the tensor product is an outer product, which takes two vectors, say $\textbf{u}$ and $\textbf{v}$, and outputs a matrix $\textbf{uv}^T$. ($\textbf{u}$ being a $m\times 1$ column vector and $\textbf{v}$ being a $n\times 1$ column vector) How about more general cases of tensor products, e.g. in the context of quantum groups? Sincere thanks.","This is a beginner's question on what exactly is a tensor product, in laymen's term, for a beginner who has just learned basic group theory and basic ring theory. I do understand from wikipedia that in some cases, the tensor product is an outer product, which takes two vectors, say $\textbf{u}$ and $\textbf{v}$, and outputs a matrix $\textbf{uv}^T$. ($\textbf{u}$ being a $m\times 1$ column vector and $\textbf{v}$ being a $n\times 1$ column vector) How about more general cases of tensor products, e.g. in the context of quantum groups? Sincere thanks.",,"['abstract-algebra', 'tensor-products', 'quantum-groups']"
88,"""categorical"" proof of a seemingly symmetric statement about Noetherian/Artinian modules","""categorical"" proof of a seemingly symmetric statement about Noetherian/Artinian modules",,"There are two statements which to me seem rather symmetric: Let $A$ be a ring, $M$ an $A$ -module, and $f : M \to M$ . If $M$ is Noetherian and $f$ is surjective, then $f$ is injective. If $M$ is Artinian and $f$ is injective, then $f$ is surjective. The proofs also seem symmetric in a sense: in the first case one constructs the increasing chain of ideals $0 \subset \ker f \subset \ker f^2 \subset \dots$ which is strict when $f$ is surjective but not injective. In the second case one uses the injectivity of $f$ to construct the decreasing chain of ideals $M \supset im \, f \supset im \, f^2 \supset \dots$ which is strict when $f$ is injective but not surjective. However, some symmetry is lost in the assertion of the last part (""which is strict when $f$ is __ but not __""). In the first case I use the fact $\ker f^n = \ker f^{n+1}$ implies that $f$ is injective on $im \, f^n = M$ . In the second case I use the fact that $M \supsetneq im \, f$ would imply that $im \, f^n \supsetneq im \, f^{n+1}$ because injective maps preserve strict inclusions. My question is, is there a way to prove one of the statements in the appropriate category/framework such that the other follows from some kind of formulaic reversal of arrows? This is definitely more of a soft question because I'm not sure what this might mean, but the two situations seem symmetric enough that this might be plausible.","There are two statements which to me seem rather symmetric: Let be a ring, an -module, and . If is Noetherian and is surjective, then is injective. If is Artinian and is injective, then is surjective. The proofs also seem symmetric in a sense: in the first case one constructs the increasing chain of ideals which is strict when is surjective but not injective. In the second case one uses the injectivity of to construct the decreasing chain of ideals which is strict when is injective but not surjective. However, some symmetry is lost in the assertion of the last part (""which is strict when is __ but not __""). In the first case I use the fact implies that is injective on . In the second case I use the fact that would imply that because injective maps preserve strict inclusions. My question is, is there a way to prove one of the statements in the appropriate category/framework such that the other follows from some kind of formulaic reversal of arrows? This is definitely more of a soft question because I'm not sure what this might mean, but the two situations seem symmetric enough that this might be plausible.","A M A f : M \to M M f f M f f 0 \subset \ker f \subset \ker f^2 \subset \dots f f M \supset im \, f \supset im \, f^2 \supset \dots f f \ker f^n = \ker f^{n+1} f im \, f^n = M M \supsetneq im \, f im \, f^n \supsetneq im \, f^{n+1}","['abstract-algebra', 'category-theory', 'modules', 'noetherian', 'artinian']"
89,Elementary approach to proving that a group of order 9 is Abelian,Elementary approach to proving that a group of order 9 is Abelian,,"The trick here is to provide an elementary solution; I'll explain what I mean. Prove that a group of order 9 must be Abelian.  The standard approach is to use the class equation to show that any $p$-group has a non-trivial center.  From that, it's easy to show that any group of order $p^2$ is Abelian.  If not, pick an element $a$ not in the center and look at its centralizer.  This includes the center and $a$, so it has at least $p+1$ elements; hence it's the whole group by Lagrange; hence $a$ is in the center, a contradiction. Okay, but the problem is given as an exercise very early in Herstein, before any of this is discussed.  Other than Lagrange and some easy consequences, all we really have to work with is the fundamental homomorphism theorem; in fact, the exercise is included in the set at the end of the section introducing the theorem.  So what I'm looking for is an approach that uses only very basic group properties and, especially, one that applies the homomorphism theorem.","The trick here is to provide an elementary solution; I'll explain what I mean. Prove that a group of order 9 must be Abelian.  The standard approach is to use the class equation to show that any $p$-group has a non-trivial center.  From that, it's easy to show that any group of order $p^2$ is Abelian.  If not, pick an element $a$ not in the center and look at its centralizer.  This includes the center and $a$, so it has at least $p+1$ elements; hence it's the whole group by Lagrange; hence $a$ is in the center, a contradiction. Okay, but the problem is given as an exercise very early in Herstein, before any of this is discussed.  Other than Lagrange and some easy consequences, all we really have to work with is the fundamental homomorphism theorem; in fact, the exercise is included in the set at the end of the section introducing the theorem.  So what I'm looking for is an approach that uses only very basic group properties and, especially, one that applies the homomorphism theorem.",,['abstract-algebra']
90,Video lectures on Group Theory,Video lectures on Group Theory,,"The web is full of video lectures these days but, try as I might, I can find very little for Introduction to Group Theory. The closest I found was http://www.extension.harvard.edu/open-learning-initiative/abstract-algebra . Are they any online introductory group theory lectures people would recommend?","The web is full of video lectures these days but, try as I might, I can find very little for Introduction to Group Theory. The closest I found was http://www.extension.harvard.edu/open-learning-initiative/abstract-algebra . Are they any online introductory group theory lectures people would recommend?",,['abstract-algebra']
91,What exactly is an $R$-algebra?,What exactly is an -algebra?,R,"I've looked up numerous definitions, and all of them talk of a new operator that is added to the $R$-module that is the $R$-algebra. One definition says An $R$-algebra, where $R$ is a commutative ring, is a ring with identity together with a ring homomorphism $f\colon R \to A$ such that the subring $f(R)$ of $A$ is contained within the center of $A$."" I don't see how the fact that an $R$-algebra is an $R$-module with a bilinear operator follows from this definition.  Thanks in advance.","I've looked up numerous definitions, and all of them talk of a new operator that is added to the $R$-module that is the $R$-algebra. One definition says An $R$-algebra, where $R$ is a commutative ring, is a ring with identity together with a ring homomorphism $f\colon R \to A$ such that the subring $f(R)$ of $A$ is contained within the center of $A$."" I don't see how the fact that an $R$-algebra is an $R$-module with a bilinear operator follows from this definition.  Thanks in advance.",,['abstract-algebra']
92,Self-study Dummit and Foote,Self-study Dummit and Foote,,"I am going to be taking a year off from my studies and would like to self study abstract algebra as it is right now the biggest gap in my math background. I have a copy of Dummit and Foote from which I would like to study, however I realize that it contains quite a large amount of material! I would thus like to put together a list of essential topics to cover so that at the end I would have covered a similar content to a third year undergraduate course for mathematicians. One thing I would like to do if possible is get an introduction to Galois theory, it is quite mysterious to me and I would love to get acquainted to the subject. I am (quite unfortunately) in electrical engineering, although I am directing myself to do a masters in math or perhaps control theory on the mathematical side of things. As such I have taken as many math course as I could and have done some self studying so that I think I now have a reasonable degree of mathematical maturity (real analysis, topology, differential geometry, linear algebra of course, probability and stats, discrete math, etc). Unfortunately I can't take as many pure math courses an as a math undergrad which is why I want to self-study abstract algebra. I know this is an ambitious project but I am quite motivated so any tips are very appreciated!","I am going to be taking a year off from my studies and would like to self study abstract algebra as it is right now the biggest gap in my math background. I have a copy of Dummit and Foote from which I would like to study, however I realize that it contains quite a large amount of material! I would thus like to put together a list of essential topics to cover so that at the end I would have covered a similar content to a third year undergraduate course for mathematicians. One thing I would like to do if possible is get an introduction to Galois theory, it is quite mysterious to me and I would love to get acquainted to the subject. I am (quite unfortunately) in electrical engineering, although I am directing myself to do a masters in math or perhaps control theory on the mathematical side of things. As such I have taken as many math course as I could and have done some self studying so that I think I now have a reasonable degree of mathematical maturity (real analysis, topology, differential geometry, linear algebra of course, probability and stats, discrete math, etc). Unfortunately I can't take as many pure math courses an as a math undergrad which is why I want to self-study abstract algebra. I know this is an ambitious project but I am quite motivated so any tips are very appreciated!",,"['abstract-algebra', 'self-learning']"
93,"For a morphism of affine schemes, the inverse of an open affine subscheme is affine","For a morphism of affine schemes, the inverse of an open affine subscheme is affine",,"This seems ridiculously simple, but it's eluding me. Suppose $f:X\rightarrow Y$ is a morphism of affine schemes. Let $V$ be an open affine subscheme of $Y$. Why is $f^{-1}(V)$ affine? I noted that $V$ is quasi-compact and wrote it as a finite union of principal open sets. Because the pullbacks of principal open sets are principal open sets, we can write $f^{-1}(V)$ as the union of such sets. But I'm not sure how to show this union is affine. I don't think this is the right way to go, because such unions are not affine in general. I'm particularly perplexed because this occurs as an exercise in the chapter on separated morphisms and base change in Liu. Of course, all affine schemes are separated, but I don't see the relevance of that here.","This seems ridiculously simple, but it's eluding me. Suppose $f:X\rightarrow Y$ is a morphism of affine schemes. Let $V$ be an open affine subscheme of $Y$. Why is $f^{-1}(V)$ affine? I noted that $V$ is quasi-compact and wrote it as a finite union of principal open sets. Because the pullbacks of principal open sets are principal open sets, we can write $f^{-1}(V)$ as the union of such sets. But I'm not sure how to show this union is affine. I don't think this is the right way to go, because such unions are not affine in general. I'm particularly perplexed because this occurs as an exercise in the chapter on separated morphisms and base change in Liu. Of course, all affine schemes are separated, but I don't see the relevance of that here.",,"['schemes', 'abstract-algebra', 'affine-schemes']"
94,What's an example of an ideal in $\mathbb{Z}[\sqrt{-n}]$ that is not principal?,What's an example of an ideal in  that is not principal?,\mathbb{Z}[\sqrt{-n}],"Earlier I asked a question which showed that $\mathbb{Z}[\sqrt{-n}]$ for $n$ a square free integer greater than 3 is not a UFD. Since PID implies UFD, this also means $\mathbb{Z}[\sqrt{-n}]$ is not a PID. Is there an example of an ideal in this ring that is not principal? My earlier question is here Why is $\mathbb{Z}[\sqrt{-n}], n\ge 3$ not a UFD?","Earlier I asked a question which showed that $\mathbb{Z}[\sqrt{-n}]$ for $n$ a square free integer greater than 3 is not a UFD. Since PID implies UFD, this also means $\mathbb{Z}[\sqrt{-n}]$ is not a PID. Is there an example of an ideal in this ring that is not principal? My earlier question is here Why is $\mathbb{Z}[\sqrt{-n}], n\ge 3$ not a UFD?",,"['abstract-algebra', 'ring-theory', 'examples-counterexamples']"
95,"If every polynomial in $k[x]$ has a root in $E$, is $E$ algebraically closed?","If every polynomial in  has a root in , is  algebraically closed?",k[x] E E,"If $E/k$ is algebraic and for all $f$ in $k[X]$ , all roots of $f$ lie in $E$ , then $E$ is algebraically closed. The question is: If $E/k$ is algebraic and for all $f$ in $k[X]$ , at least one root of $f$ lies in $E$ , then is $E$ algebraically closed?","If is algebraic and for all in , all roots of lie in , then is algebraically closed. The question is: If is algebraic and for all in , at least one root of lies in , then is algebraically closed?",E/k f k[X] f E E E/k f k[X] f E E,"['abstract-algebra', 'galois-theory', 'extension-field']"
96,good books on Abstract Algebra and Cryptography for self-study,good books on Abstract Algebra and Cryptography for self-study,,"I want to self-study some abstract algebra and cryptography during the summer, so what are some of books that are suitable for self-study? I have very limited background in algebra and none in cryptography, and like to learn by examples. Also, please let me know if there are any books on these subjects with solutions to most of the exercises, so I can check on myself. (I saw yunone 's post about good algebra books, but I would like to focus more on those with good exercises and detailed solutions.) All suggestions are welcome! Thank you!!","I want to self-study some abstract algebra and cryptography during the summer, so what are some of books that are suitable for self-study? I have very limited background in algebra and none in cryptography, and like to learn by examples. Also, please let me know if there are any books on these subjects with solutions to most of the exercises, so I can check on myself. (I saw yunone 's post about good algebra books, but I would like to focus more on those with good exercises and detailed solutions.) All suggestions are welcome! Thank you!!",,"['abstract-algebra', 'soft-question']"
97,A question from 1989 leningrad mathematical olympiad,A question from 1989 leningrad mathematical olympiad,,"Prove that we cannot define an binary operation $*$ on the set of integers Z satisfy all of the three properties below simultaneously: For any $A∈Z,B∈Z,C∈Z:$ 1. $A*B=-(B*A)$ 2. $(A*B)*C=A*(B*C)$ (Associative Law) 3.For every $A\in Z$ there exist $B∈Z,C∈Z$ such that $A=B*C$ I have got stucked for three days on this questions.Anyway,I will show some result and idea I had: 1.for any X∈Z,we have $X*X=-(X*X)$ .So we have $X*X=0$ 2.for any X∈Z,we have $X*0=-(0*X)=X*(X*X)=(X*X)*X=0*X$ .So we have $X*0=0*X=0$ 3.Now For any $X∈Z$ ( $X≠0$ ).We define the orbit of X-- $Ox$ to be the set $Ox$ ={S| $∃Y∈Z$ such that $X*Y=S$ },and the stabilizer of X-- $Fx$ to be the set $Fx$ ={T| $T*X=X*T=0$ }. My goal is to prove that actually $Ox=Fx$ .And therefore since $X∈Fx$ ,so $X∈Ox$ ,and we reach a contradiction since $X∉Ox$ (otherwise if $∃Y∈Z$ such that $X*Y=X$ ,then $(X*Y)*Y=X*(Y*Y)=X*0=X*Y=X=0$ ) It is easy to see that $Ox⊆Fx$ ,since for any $S∈Ox$ ,we have $X*S=X*(X*Y)=(X*X)*Y=0*Y=0$ ,so $S∈Fx$ However,for the other direction,I cannot deduce out,which I need help. I think the backgroud of this question is the orbit&stabilizer theorem in the course abstract algebra.So I have a strong intuition that I am on the right track.","Prove that we cannot define an binary operation on the set of integers Z satisfy all of the three properties below simultaneously: For any 1. 2. (Associative Law) 3.For every there exist such that I have got stucked for three days on this questions.Anyway,I will show some result and idea I had: 1.for any X∈Z,we have .So we have 2.for any X∈Z,we have .So we have 3.Now For any ( ).We define the orbit of X-- to be the set ={S| such that },and the stabilizer of X-- to be the set ={T| }. My goal is to prove that actually .And therefore since ,so ,and we reach a contradiction since (otherwise if such that ,then ) It is easy to see that ,since for any ,we have ,so However,for the other direction,I cannot deduce out,which I need help. I think the backgroud of this question is the orbit&stabilizer theorem in the course abstract algebra.So I have a strong intuition that I am on the right track.","* A∈Z,B∈Z,C∈Z: A*B=-(B*A) (A*B)*C=A*(B*C) A\in Z B∈Z,C∈Z A=B*C X*X=-(X*X) X*X=0 X*0=-(0*X)=X*(X*X)=(X*X)*X=0*X X*0=0*X=0 X∈Z X≠0 Ox Ox ∃Y∈Z X*Y=S Fx Fx T*X=X*T=0 Ox=Fx X∈Fx X∈Ox X∉Ox ∃Y∈Z X*Y=X (X*Y)*Y=X*(Y*Y)=X*0=X*Y=X=0 Ox⊆Fx S∈Ox X*S=X*(X*Y)=(X*X)*Y=0*Y=0 S∈Fx","['abstract-algebra', 'contest-math', 'binary-operations']"
98,Maximum number of Sylow subgroups,Maximum number of Sylow subgroups,,"I've been studying Sylow-$p$ subgroups, and I've come across this problem. Let $G$ be a finite group. Show that the number of Sylow subgroups of $G$ is at most $\frac{2}{3}|G|$ . ($|G|$ is the number of elements of $G$). I am having trouble figuring this one out, I was wondering if anyone could help?","I've been studying Sylow-$p$ subgroups, and I've come across this problem. Let $G$ be a finite group. Show that the number of Sylow subgroups of $G$ is at most $\frac{2}{3}|G|$ . ($|G|$ is the number of elements of $G$). I am having trouble figuring this one out, I was wondering if anyone could help?",,"['abstract-algebra', 'group-theory', 'finite-groups', 'sylow-theory']"
99,"Is a field perfect iff the primitive element theorem holds for all extensions, and what about function fields","Is a field perfect iff the primitive element theorem holds for all extensions, and what about function fields",,"Let $L/K$ be a finite separable extension of fields. Then we have the primitive element theorem , i.e., there exists an $x$ in $L$ such that $L=K(x)$. In particular, the primitive element theorem holds for all finite extensions of a perfect field. Question 1. Is a field $K$ perfect if and only if the primitive element theorem holds for all finite extensions of $K$? Question 2. Suppose that $K$ is a field extension of $\mathbf{F}_p$ of transcendence degree 1, i.e., a function field over a finite field. Does the primitive element theorem hold for any finite extension of $K$? In Question 2, I am actually only interested in the case $K=\mathbf{F}_q(t)$.","Let $L/K$ be a finite separable extension of fields. Then we have the primitive element theorem , i.e., there exists an $x$ in $L$ such that $L=K(x)$. In particular, the primitive element theorem holds for all finite extensions of a perfect field. Question 1. Is a field $K$ perfect if and only if the primitive element theorem holds for all finite extensions of $K$? Question 2. Suppose that $K$ is a field extension of $\mathbf{F}_p$ of transcendence degree 1, i.e., a function field over a finite field. Does the primitive element theorem hold for any finite extension of $K$? In Question 2, I am actually only interested in the case $K=\mathbf{F}_q(t)$.",,"['abstract-algebra', 'field-theory', 'finite-fields', 'function-fields']"
