,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Erwin Kreyszig Section 2.8, Problem 2: What is the norm of these two functionals?","Erwin Kreyszig Section 2.8, Problem 2: What is the norm of these two functionals?",,"Let $a$, $b$ be two real numbers such that $a<b$, and let $C[a,b]$ denote the normed space of all (real- or complex-valued) functions defined and continuous on the closed interval $[a,b]$ with the norm defined as  $$\Vert x \Vert_{C[a,b]} \colon= \max_{t\in[a,b]} \vert x(t) \vert \text{ for all } x \in C[a,b].$$ Let $f_1$ and $f_2$ be two functionals defined on $C[a,b]$ as follows:  $$f_1(x) \colon= \int_a^b x(t) y_0(t) dt \text{ for all } x \in C[a,b], \text{ where } y_0 \in C[a,b] \text{ is fixed.} $$ And $$f_2(x) \colon= \alpha x(a) + \beta x(b) \text{ for all } x \in C[a,b], \text{ where } \alpha, \beta \text{ are some fixed scalars.}$$ Then what is the value of $\Vert f_1 \Vert$ and $\Vert f_2 \Vert$? I know that $f_1$ and $f_2$ are both linear. And, for each $x \in C[a,b]$, we have  $$\begin{aligned}\vert f_1(x) \vert & = \left\vert \int_a^b x(t) y_0(t) dt \right\vert \\ & \leq \int_a^b \vert x(t) \vert \cdot \vert y_0(t) \vert dt \\ & \leq \int_a^b \left( \max_{\tau\in[a,b]} \vert x(\tau) \vert \right) \cdot \vert y_0(t) \vert dt \\ & = \int_a^b \Vert x \Vert_{C[a,b]} \cdot \vert y_0(t) \vert dt \\ & = \Vert x \Vert_{C[a,b]} \int_a^b \vert y_0(t) \vert dt. \end{aligned}$$  Thus it follows that $f_1$ is bounded and that $$\Vert f_1 \Vert \leq \int_a^b \vert y_0(t) \vert dt.$$ And, for each $x \in C[a,b]$, we also have  $$\begin{aligned} \vert f_2(x) \vert & = \vert \alpha x(a) + \beta x(b) \vert \\ & \leq \vert \alpha \vert \vert x(a) \vert + \vert \beta \vert \vert x(b) \vert \\ & \leq \left( \vert \alpha \vert + \vert \beta \vert \right) \max_{t\in[a,b]} \vert x(t) \vert \\ & = \left( \vert \alpha \vert + \vert \beta \vert \right)  \Vert x \Vert_{C[a,b]}, \end{aligned}$$ which shows that $f_2$ is bounded and that  $$\Vert f_2 \Vert \leq \left( \vert \alpha \vert + \vert \beta \vert \right).$$ What next? How to derive the reverse inequality in each case (if it holds in either case, that is!)?","Let $a$, $b$ be two real numbers such that $a<b$, and let $C[a,b]$ denote the normed space of all (real- or complex-valued) functions defined and continuous on the closed interval $[a,b]$ with the norm defined as  $$\Vert x \Vert_{C[a,b]} \colon= \max_{t\in[a,b]} \vert x(t) \vert \text{ for all } x \in C[a,b].$$ Let $f_1$ and $f_2$ be two functionals defined on $C[a,b]$ as follows:  $$f_1(x) \colon= \int_a^b x(t) y_0(t) dt \text{ for all } x \in C[a,b], \text{ where } y_0 \in C[a,b] \text{ is fixed.} $$ And $$f_2(x) \colon= \alpha x(a) + \beta x(b) \text{ for all } x \in C[a,b], \text{ where } \alpha, \beta \text{ are some fixed scalars.}$$ Then what is the value of $\Vert f_1 \Vert$ and $\Vert f_2 \Vert$? I know that $f_1$ and $f_2$ are both linear. And, for each $x \in C[a,b]$, we have  $$\begin{aligned}\vert f_1(x) \vert & = \left\vert \int_a^b x(t) y_0(t) dt \right\vert \\ & \leq \int_a^b \vert x(t) \vert \cdot \vert y_0(t) \vert dt \\ & \leq \int_a^b \left( \max_{\tau\in[a,b]} \vert x(\tau) \vert \right) \cdot \vert y_0(t) \vert dt \\ & = \int_a^b \Vert x \Vert_{C[a,b]} \cdot \vert y_0(t) \vert dt \\ & = \Vert x \Vert_{C[a,b]} \int_a^b \vert y_0(t) \vert dt. \end{aligned}$$  Thus it follows that $f_1$ is bounded and that $$\Vert f_1 \Vert \leq \int_a^b \vert y_0(t) \vert dt.$$ And, for each $x \in C[a,b]$, we also have  $$\begin{aligned} \vert f_2(x) \vert & = \vert \alpha x(a) + \beta x(b) \vert \\ & \leq \vert \alpha \vert \vert x(a) \vert + \vert \beta \vert \vert x(b) \vert \\ & \leq \left( \vert \alpha \vert + \vert \beta \vert \right) \max_{t\in[a,b]} \vert x(t) \vert \\ & = \left( \vert \alpha \vert + \vert \beta \vert \right)  \Vert x \Vert_{C[a,b]}, \end{aligned}$$ which shows that $f_2$ is bounded and that  $$\Vert f_2 \Vert \leq \left( \vert \alpha \vert + \vert \beta \vert \right).$$ What next? How to derive the reverse inequality in each case (if it holds in either case, that is!)?",,"['real-analysis', 'functional-analysis', 'operator-theory', 'normed-spaces']"
1,Functional derivatives on manifolds,Functional derivatives on manifolds,,"This might be more of a physics question, but it is mathematics-related, I hope I am not out of place with this. Let $(M,\mathcal{S},g)$ be a smooth, $n$-dimensional manifold equipped with a Riemann metric. Let us denote the vector space of $(p,q)$-type tensor fields on $M$ as $\mathcal{T}_{q}^{p}(M)$. Let $\Psi:\mathbb{R}\rightarrow\mathcal{T}_{q}^{p}(M),\varepsilon\mapsto\Psi(\varepsilon)$ be a smooth curve and let us use the notation where $\Psi$ denotes $\Psi(0)$. Let $S:\mathcal{T}_{q}^{p}(M)\rightarrow\mathbb{R}$ be a functional, in such way, that $$S[\Psi]=\int_{M}\mathcal{L}(\Psi,\nabla\Psi)\sqrt{|\det(g)|}\mathrm{d}x^{1}\wedge...\wedge\mathrm{d}x^{n}.$$ In this case, we say $S$ is functionally derivable at $\Psi$, if there exists a $\frac{dS[\Psi]}{d\Psi}\in\mathcal{T}_{p}^{q}(M)$ tensor field, that $$\left.\frac{dS[\Psi(\varepsilon)]}{d\varepsilon}\right|_{\varepsilon=0}=\int_M\frac{dS[\Psi]}{d\Psi}\bullet\left.\frac{d\Psi(\varepsilon)}{d\varepsilon}\right|_{\varepsilon=0}\sqrt{|\det(g)|}\mathrm{d}x^{1}\wedge...\wedge\mathrm{d}x^{n},$$ where $\bullet$ denotes full contraction. My questions are regarding technical details of this derivative. Physics books generally do not impose rigorous conditions on the space of tensor fields on which $S$ is defined. What structures does this space need to possess for this to make sense? I assume Hausdorff-topology is a must, but does it need to be normed? If so, what norm do we use, that does not conflict with physics? Wald mentions in a footnote, that in general, a tensor distribution needs exist, so that $$\left.\frac{dS[\Psi(\varepsilon)]}{d\varepsilon}\right|_{\varepsilon=0}=\frac{dS[\Psi]}{d\Psi}\left[\left.\frac{d\Psi(\varepsilon)}{d\varepsilon}\right|_{\varepsilon=0}\right].$$ Is there any conceivable situation within the bounds of physics, where this distribution is NOT regular?","This might be more of a physics question, but it is mathematics-related, I hope I am not out of place with this. Let $(M,\mathcal{S},g)$ be a smooth, $n$-dimensional manifold equipped with a Riemann metric. Let us denote the vector space of $(p,q)$-type tensor fields on $M$ as $\mathcal{T}_{q}^{p}(M)$. Let $\Psi:\mathbb{R}\rightarrow\mathcal{T}_{q}^{p}(M),\varepsilon\mapsto\Psi(\varepsilon)$ be a smooth curve and let us use the notation where $\Psi$ denotes $\Psi(0)$. Let $S:\mathcal{T}_{q}^{p}(M)\rightarrow\mathbb{R}$ be a functional, in such way, that $$S[\Psi]=\int_{M}\mathcal{L}(\Psi,\nabla\Psi)\sqrt{|\det(g)|}\mathrm{d}x^{1}\wedge...\wedge\mathrm{d}x^{n}.$$ In this case, we say $S$ is functionally derivable at $\Psi$, if there exists a $\frac{dS[\Psi]}{d\Psi}\in\mathcal{T}_{p}^{q}(M)$ tensor field, that $$\left.\frac{dS[\Psi(\varepsilon)]}{d\varepsilon}\right|_{\varepsilon=0}=\int_M\frac{dS[\Psi]}{d\Psi}\bullet\left.\frac{d\Psi(\varepsilon)}{d\varepsilon}\right|_{\varepsilon=0}\sqrt{|\det(g)|}\mathrm{d}x^{1}\wedge...\wedge\mathrm{d}x^{n},$$ where $\bullet$ denotes full contraction. My questions are regarding technical details of this derivative. Physics books generally do not impose rigorous conditions on the space of tensor fields on which $S$ is defined. What structures does this space need to possess for this to make sense? I assume Hausdorff-topology is a must, but does it need to be normed? If so, what norm do we use, that does not conflict with physics? Wald mentions in a footnote, that in general, a tensor distribution needs exist, so that $$\left.\frac{dS[\Psi(\varepsilon)]}{d\varepsilon}\right|_{\varepsilon=0}=\frac{dS[\Psi]}{d\Psi}\left[\left.\frac{d\Psi(\varepsilon)}{d\varepsilon}\right|_{\varepsilon=0}\right].$$ Is there any conceivable situation within the bounds of physics, where this distribution is NOT regular?",,"['functional-analysis', 'mathematical-physics']"
2,Resolvent operator,Resolvent operator,,"Let's consider the following operator on $L^2(\mathbb{R}^3)$ $$A(t)=\Delta+b(t,x)\cdot\nabla$$ where $\Delta$ is the Laplace operator and $b(\cdot,\cdot)$ a smooth vector field. How to compute the resolvent operator?","Let's consider the following operator on $L^2(\mathbb{R}^3)$ $$A(t)=\Delta+b(t,x)\cdot\nabla$$ where $\Delta$ is the Laplace operator and $b(\cdot,\cdot)$ a smooth vector field. How to compute the resolvent operator?",,"['analysis', 'functional-analysis', 'operator-theory', 'differential-operators']"
3,Spectra of periodic Schrödinger equations,Spectra of periodic Schrödinger equations,,"This question might be a little bit physics-related, but I kind of have a deep interest to ask this here, cause I would like to get an idea of the Mathematics behind the things I just covered in my physics lecture. Please do not refer me to Reed/Simon or anything else, since I know that this topic is highly specialised (even in Spectral theory). I am just curious and hope to understand this in the future, so I just want to get an appetizer, if you understand what I mean. So assume we have a periodic 1d Schrödinger operator $$- f'' + V(x) f(x)= \lambda f(x)$$  and we want $V$ to be periodic. Now if we assume that we are on a finite interval and that we have periodic boundary conditions where $R$ denotes the period of the potential, then we have eigenvalues $E_0 < E_1 \le E_2 < E_3 \le E_4...$ and so on. Okay, this is clear to me. Then, there is the case that such an operator is defined on the full interval. First question: Do we then need any boundary conditions? In my physics lecture we used so-called Born von Karmann boundary conditions (saying that $f(x+R) =f(x)$) in order to ""prove"" the Floquet or Bloch theorem which says that we can decompose $f(x) = e^{ikx} u_k(x)$. This theorem says that we can decompose the wavefunction in a periodic part$ u_k(x+R) = u_k$ and a complex exponential $e^{ikx}$. I somehow feel as if these Born von Karmann boundary conditions are not necessary in the sense that any eigenfunction to this Schrödinger operator is automatically periodic with the potential's period, is this true?- In that case: Why do we want Born von Karmann boundary conditions?- My problem with the Born von Karmann conditions is that I find that they are not really boundary conditions, as they don't act on some boundary. So what about the domain of such an operator? 2.) Actually, imagine the case $V=0$, then we are just left with $-f'' = \lambda f$. On the finite interval, this is alright, if we assume to have any periodic bounday conditions, we get a discrete spectrum. But on the infinite interval, there are obviously no square integrable eigenfunctions( as I would say). Thus, I have even troubles to understand this very simple example from a theoretical point of view. 3.) In my physics lecture we noticed that due to these Born von Karmann conditions the possible $k'$s for the problem (appearing in the exponentials) are discrete. Not sure if this is automatically satisfied, even if we don't assume Born von Karmann boundary conditions? Then we said that for every $k$, the Schrödinger operator equation that you get by pluggin in the ansatz from the Bloch or Floquet thoerem has a discrete spectrum. Is this true? If so, what does this all have to do with bands, if everything is so nicely discrete? - Or do we just cal these things bands, since the $k$'s get so close, that we cannot really resolve the discrete structure? 4.) Is there any relationship between the finite-interval problem and the infinite interval problem or are these two completely different things?","This question might be a little bit physics-related, but I kind of have a deep interest to ask this here, cause I would like to get an idea of the Mathematics behind the things I just covered in my physics lecture. Please do not refer me to Reed/Simon or anything else, since I know that this topic is highly specialised (even in Spectral theory). I am just curious and hope to understand this in the future, so I just want to get an appetizer, if you understand what I mean. So assume we have a periodic 1d Schrödinger operator $$- f'' + V(x) f(x)= \lambda f(x)$$  and we want $V$ to be periodic. Now if we assume that we are on a finite interval and that we have periodic boundary conditions where $R$ denotes the period of the potential, then we have eigenvalues $E_0 < E_1 \le E_2 < E_3 \le E_4...$ and so on. Okay, this is clear to me. Then, there is the case that such an operator is defined on the full interval. First question: Do we then need any boundary conditions? In my physics lecture we used so-called Born von Karmann boundary conditions (saying that $f(x+R) =f(x)$) in order to ""prove"" the Floquet or Bloch theorem which says that we can decompose $f(x) = e^{ikx} u_k(x)$. This theorem says that we can decompose the wavefunction in a periodic part$ u_k(x+R) = u_k$ and a complex exponential $e^{ikx}$. I somehow feel as if these Born von Karmann boundary conditions are not necessary in the sense that any eigenfunction to this Schrödinger operator is automatically periodic with the potential's period, is this true?- In that case: Why do we want Born von Karmann boundary conditions?- My problem with the Born von Karmann conditions is that I find that they are not really boundary conditions, as they don't act on some boundary. So what about the domain of such an operator? 2.) Actually, imagine the case $V=0$, then we are just left with $-f'' = \lambda f$. On the finite interval, this is alright, if we assume to have any periodic bounday conditions, we get a discrete spectrum. But on the infinite interval, there are obviously no square integrable eigenfunctions( as I would say). Thus, I have even troubles to understand this very simple example from a theoretical point of view. 3.) In my physics lecture we noticed that due to these Born von Karmann conditions the possible $k'$s for the problem (appearing in the exponentials) are discrete. Not sure if this is automatically satisfied, even if we don't assume Born von Karmann boundary conditions? Then we said that for every $k$, the Schrödinger operator equation that you get by pluggin in the ansatz from the Bloch or Floquet thoerem has a discrete spectrum. Is this true? If so, what does this all have to do with bands, if everything is so nicely discrete? - Or do we just cal these things bands, since the $k$'s get so close, that we cannot really resolve the discrete structure? 4.) Is there any relationship between the finite-interval problem and the infinite interval problem or are these two completely different things?",,"['real-analysis', 'functional-analysis']"
4,Maximal abelian subalgebras of SAW*-algebras,Maximal abelian subalgebras of SAW*-algebras,,"Pedersen distilled the following class of C*-algebras which he termed SAW*-algebras: A C*-algebra $A$ is an SAW*-algebra if for each pair of orthogonal, positive elements $x,y\in A$, there exists a positive element $e\in A$ such that $ex=x$ and $(1-e)y=y$. This looks very commutatively, hence my question: Let $B$ be a maximal abelian subalgebra of an SAW*-algebra. Is $B$ an SAW*-algebra? This is the case for AW* algebras which are SAW* a fotriori.","Pedersen distilled the following class of C*-algebras which he termed SAW*-algebras: A C*-algebra $A$ is an SAW*-algebra if for each pair of orthogonal, positive elements $x,y\in A$, there exists a positive element $e\in A$ such that $ex=x$ and $(1-e)y=y$. This looks very commutatively, hence my question: Let $B$ be a maximal abelian subalgebra of an SAW*-algebra. Is $B$ an SAW*-algebra? This is the case for AW* algebras which are SAW* a fotriori.",,"['functional-analysis', 'operator-algebras', 'c-star-algebras', 'open-problem']"
5,Relation between RKHS and space of continuous functions,Relation between RKHS and space of continuous functions,,"Consider a Mercer Kernel $K\colon \mathcal{X}\times \mathcal{X}\to \mathbb{R}$, $\mathcal{X}$ being a compact subset of $\mathbb{R}^m$, and its (unique) associated Reproducing Kernel HIlbert Space $H_K$. Then $H_k\subset \mathcal{C}^0(\mathcal{X})$, where $\mathcal{C}^0(\mathcal{X})$ is the space of continuous function on $\mathcal{X}$, i.e., there exist some functions of $\mathcal{C}^0(\mathcal{X})$ which are not in $H_K$. Could you give me an explanation of the latter statement? And/or could you give me an example of functions in $\mathcal{C}^0(\mathcal{X})$ which do not belong to $H_K$? Thank you.","Consider a Mercer Kernel $K\colon \mathcal{X}\times \mathcal{X}\to \mathbb{R}$, $\mathcal{X}$ being a compact subset of $\mathbb{R}^m$, and its (unique) associated Reproducing Kernel HIlbert Space $H_K$. Then $H_k\subset \mathcal{C}^0(\mathcal{X})$, where $\mathcal{C}^0(\mathcal{X})$ is the space of continuous function on $\mathcal{X}$, i.e., there exist some functions of $\mathcal{C}^0(\mathcal{X})$ which are not in $H_K$. Could you give me an explanation of the latter statement? And/or could you give me an example of functions in $\mathcal{C}^0(\mathcal{X})$ which do not belong to $H_K$? Thank you.",,"['functional-analysis', 'hilbert-spaces', 'machine-learning']"
6,Discontinuous functions with closed graphs,Discontinuous functions with closed graphs,,"I tried looking up a question regarding graphs of continuous functions on this site, but all the ones I found consider functions from $\mathbb{R}$ into $\mathbb{R}$. I have been pondering the following question: given a general topological spaces $X, Y$, and a function $f: X\to Y$, when does $Graph(f)$ closed in $X\times Y$ imply that $f$ is continuous. By the closed graph theorem, this is true whenever $X$ and $Y$ are both Banach spaces. Also, it is fairly easy to prove that whenever $Y$ is a Hausdorff space and $f$ is continuous, then $Graph(f)$ is closed, but I do not think that the converse is true, so I am trying to find an example where $X$ is some topological space, $Y$ a Hausdorff space, $f: X\to Y$ a function with a closed graph in $X\times Y$, but who fails to be continuous. As of yet I have not been able to find such a counterexample, partially because I have no clue where to look for such a counterexample. I would really appreciate getting some directions to go in.","I tried looking up a question regarding graphs of continuous functions on this site, but all the ones I found consider functions from $\mathbb{R}$ into $\mathbb{R}$. I have been pondering the following question: given a general topological spaces $X, Y$, and a function $f: X\to Y$, when does $Graph(f)$ closed in $X\times Y$ imply that $f$ is continuous. By the closed graph theorem, this is true whenever $X$ and $Y$ are both Banach spaces. Also, it is fairly easy to prove that whenever $Y$ is a Hausdorff space and $f$ is continuous, then $Graph(f)$ is closed, but I do not think that the converse is true, so I am trying to find an example where $X$ is some topological space, $Y$ a Hausdorff space, $f: X\to Y$ a function with a closed graph in $X\times Y$, but who fails to be continuous. As of yet I have not been able to find such a counterexample, partially because I have no clue where to look for such a counterexample. I would really appreciate getting some directions to go in.",,"['general-topology', 'examples-counterexamples', 'continuity']"
7,Banach spaces involving time,Banach spaces involving time,,"Let's suppoe $u\in L^2(0,T;H_0^1(\Omega))$ with $u'\in L^2(0,T;H^{-1}(\Omega))$. We know that $$u\in C([0,T];L^2(\Omega))$$. In this result can the set $\Omega$ be the whole $\mathbb{R}^n$ or we need some regularity and boundedness assumptions on it?","Let's suppoe $u\in L^2(0,T;H_0^1(\Omega))$ with $u'\in L^2(0,T;H^{-1}(\Omega))$. We know that $$u\in C([0,T];L^2(\Omega))$$. In this result can the set $\Omega$ be the whole $\mathbb{R}^n$ or we need some regularity and boundedness assumptions on it?",,"['real-analysis', 'analysis', 'functional-analysis', 'partial-differential-equations']"
8,Existence of a mapping in a nonseparable Banach space that moves all nearby points to far-away points,Existence of a mapping in a nonseparable Banach space that moves all nearby points to far-away points,,"Does there exist a nonseparable Banach space $X$, a mapping $F: X\to X$, and an open nonempty subset $D\subset X$ such that $$ \forall\,E>0 \quad \exists\,\delta>0: \quad \forall\,x,y\in D \quad (0<\|x-y\|<\delta \Rightarrow \|F(x)-F(y)\|>E) \, ? $$ Of course, it is impossible if $X$ is separable.","Does there exist a nonseparable Banach space $X$, a mapping $F: X\to X$, and an open nonempty subset $D\subset X$ such that $$ \forall\,E>0 \quad \exists\,\delta>0: \quad \forall\,x,y\in D \quad (0<\|x-y\|<\delta \Rightarrow \|F(x)-F(y)\|>E) \, ? $$ Of course, it is impossible if $X$ is separable.",,"['functional-analysis', 'metric-spaces', 'banach-spaces']"
9,"Prove that there exist linear functionals $L_1, L_2$ on $X$",Prove that there exist linear functionals  on,"L_1, L_2 X","Let $X$ be a linear space, $p, q$ sublinear functionals on $X$, and $L$ a linear functional on $X$ such that $|L(x)| ≤ p(x) + q(x),$ for all $x ∈ X$. Prove that there exist linear functionals $L_1, L_2$ on $X$ such that $L(x) = L_1(x) + L_2(x),$ and $|L_1(x)| ≤ p(x), |L_2(x)| ≤ q(x),$ for all $x ∈ X.$ My Work: First I thought to use Hahn Banach Theorem. But since there is no known subspace it was useless. Then I tried to make $L(x)$ as $L(x)=\frac{L(x+\lambda)+L(x-\lambda)}{2}$ for some scalar $\lambda$ but failed to find suitable $L_1$ and $L_2$. I think this problem is little bit tricky. I want to try it myself and I only need a hint to start. Can somebody please give me a hint?","Let $X$ be a linear space, $p, q$ sublinear functionals on $X$, and $L$ a linear functional on $X$ such that $|L(x)| ≤ p(x) + q(x),$ for all $x ∈ X$. Prove that there exist linear functionals $L_1, L_2$ on $X$ such that $L(x) = L_1(x) + L_2(x),$ and $|L_1(x)| ≤ p(x), |L_2(x)| ≤ q(x),$ for all $x ∈ X.$ My Work: First I thought to use Hahn Banach Theorem. But since there is no known subspace it was useless. Then I tried to make $L(x)$ as $L(x)=\frac{L(x+\lambda)+L(x-\lambda)}{2}$ for some scalar $\lambda$ but failed to find suitable $L_1$ and $L_2$. I think this problem is little bit tricky. I want to try it myself and I only need a hint to start. Can somebody please give me a hint?",,"['real-analysis', 'functional-analysis']"
10,Is exponential function in a C*-algebra injective on self-adjoint elements?,Is exponential function in a C*-algebra injective on self-adjoint elements?,,"Let $A$ be a C*-algebra and $\exp(x)=\sum_{n=0}\frac{x^n}{n!}$, the usual exponential function from $A$ into $A$. Is it true that if $x\ne y\in A$, $x^*=x$, $y^*=y$, then $\exp(x)\ne\exp(y)$?","Let $A$ be a C*-algebra and $\exp(x)=\sum_{n=0}\frac{x^n}{n!}$, the usual exponential function from $A$ into $A$. Is it true that if $x\ne y\in A$, $x^*=x$, $y^*=y$, then $\exp(x)\ne\exp(y)$?",,"['functional-analysis', 'c-star-algebras']"
11,Existence of functional implies non-negative supremum.,Existence of functional implies non-negative supremum.,,"$\ell_\infty(T) = \{f:T\to\mathbb{R} : \sup_{t \in T}|f(t)| < \infty\}$. Let $G$ be a family of transformation of set $T$ and $V$ linear subspace of $\ell_\infty(T)$ such that: $1_T \in V$, $\forall f, g \in V \Rightarrow f\circ g \in V$. Show that if there exists linear functional $\phi$ on $V$ such that $\phi(1_T)=1$, $\forall f\in V, g\in G \Rightarrow \phi(f\circ g) = \phi(f)$ which is non-negative ($f \ge 0 \Rightarrow \phi(f) \ge 0$) then $\forall n\in \mathbb{N}, f_1,\ldots,f_n\in V, g_1,\ldots,g_n\in G$ supremum of $\sum_{i=1}^{n}f_i\circ g_i - f_i$ is non-negative. Actually these conditions are equivalent, I proved opposite implication but got stuck with this one. My try was to assume that $\exists f_i,g_i$ such that $\sup \sum_i f_i\circ g_i - f_i < 0$ what would imply that $-\sum_i( f_i \circ g_i - f_i) > 0$ and after mapping this sum by $\phi$ one can obtain (using linearity and given condition on $\phi$) that $\phi \left(-\sum_i( f_i \circ g_i - f_i) \right)=0$ but this is not contradiction yet.  Any ideas how to use positivity of this sum? Edit: because of sharpness of inequality involving this sum and fact that $\sup_t|f(t)|<\infty$ one can obtain that $\forall i \exists \lambda>0$ such that $-\sum_i( f_i \circ g_i - f_i) + \lambda f_i \ge 0 \Rightarrow  \phi(\lambda f_i) \ge 0 \Rightarrow \phi(f_i) \ge 0$. It should be easy now. Edit2: reasoning above actually holds for every $f\in V$ so $\phi(f) \ge 0$ for every $f \in V$. Moreover it implies that $\phi \left(-\sum_i( f_i \circ g_i - f_i) \right) = 0 \Rightarrow \phi(f_i)=\phi(f_i \circ g_i) = 0 \forall i$. Edit3: but $-1_T \in V \Rightarrow \phi(-1_T)=-1 \ge 0$. Hence thesis holds. QED","$\ell_\infty(T) = \{f:T\to\mathbb{R} : \sup_{t \in T}|f(t)| < \infty\}$. Let $G$ be a family of transformation of set $T$ and $V$ linear subspace of $\ell_\infty(T)$ such that: $1_T \in V$, $\forall f, g \in V \Rightarrow f\circ g \in V$. Show that if there exists linear functional $\phi$ on $V$ such that $\phi(1_T)=1$, $\forall f\in V, g\in G \Rightarrow \phi(f\circ g) = \phi(f)$ which is non-negative ($f \ge 0 \Rightarrow \phi(f) \ge 0$) then $\forall n\in \mathbb{N}, f_1,\ldots,f_n\in V, g_1,\ldots,g_n\in G$ supremum of $\sum_{i=1}^{n}f_i\circ g_i - f_i$ is non-negative. Actually these conditions are equivalent, I proved opposite implication but got stuck with this one. My try was to assume that $\exists f_i,g_i$ such that $\sup \sum_i f_i\circ g_i - f_i < 0$ what would imply that $-\sum_i( f_i \circ g_i - f_i) > 0$ and after mapping this sum by $\phi$ one can obtain (using linearity and given condition on $\phi$) that $\phi \left(-\sum_i( f_i \circ g_i - f_i) \right)=0$ but this is not contradiction yet.  Any ideas how to use positivity of this sum? Edit: because of sharpness of inequality involving this sum and fact that $\sup_t|f(t)|<\infty$ one can obtain that $\forall i \exists \lambda>0$ such that $-\sum_i( f_i \circ g_i - f_i) + \lambda f_i \ge 0 \Rightarrow  \phi(\lambda f_i) \ge 0 \Rightarrow \phi(f_i) \ge 0$. It should be easy now. Edit2: reasoning above actually holds for every $f\in V$ so $\phi(f) \ge 0$ for every $f \in V$. Moreover it implies that $\phi \left(-\sum_i( f_i \circ g_i - f_i) \right) = 0 \Rightarrow \phi(f_i)=\phi(f_i \circ g_i) = 0 \forall i$. Edit3: but $-1_T \in V \Rightarrow \phi(-1_T)=-1 \ge 0$. Hence thesis holds. QED",,['functional-analysis']
12,Modes of convergence in infinite direct sums of $L^{2}$ spaces,Modes of convergence in infinite direct sums of  spaces,L^{2},"It is known that if a sequence of random variables converges in norm then there exists a subsequence which converges almost surely. That is: let $\left(X_{n}\right)_{n\in\mathbb{N}}\subseteq L^{2}\left(\Omega,\mathcal{F},P\right)$   be a sequence of square integrable random variables on the probability space $\left(\Omega,\mathcal{F},P\right)$  . If $X\in L^{2}\left(\Omega,\mathcal{F},P\right)$   is such that $\mathrm{E}_{P}\left(\left(X_{n}-X\right)^{2}\right)\longrightarrow0$   as $n\longrightarrow\infty$  , then there exists a subsequence $\left(X_{n_{k}}\right)_{k\in\mathbb{N}}\subseteq\left(X_{n}\right)_{n\in\mathbb{N}}$   such that $X_{n_{k}}\longrightarrow X$   almost surely as $k\longrightarrow\infty$  . My question is the following: does this holds for an infinite direct sum of $L^{2}$   spaces? To be precise: consider a filtered probability space $\left(\Omega,\left(\mathcal{F}_{t}\right)_{t\in\mathbb{N}},\mathcal{F},P\right)$  . Define: $$\oplus_{t=1}^{\infty}L^{2}\left(\Omega,\mathcal{F}_{t},P\right)\equiv\left\{ \left(X\left(t\right)\right)_{t=1}^{\infty}:\, X\left(t\right)\in L^{2}\left(\Omega,\mathcal{F}_{t},P\right)\forall t,\,\sum_{t=1}^{\infty}\mathrm{E}_{P}\left(X\left(t\right)^{2}\right)<\infty\right\}.$$  Let $\left(\left(X_{n}\left(t\right)\right)_{t=1}^{\infty}\right)_{n\in\mathbb{N}}\subseteq\oplus_{t=1}^{\infty}L^{2}\left(\Omega,\mathcal{F}_{t},P\right)$   be such that $\sum_{t=1}^{\infty}\mathrm{E}_{P}\left(\left(X_{n}\left(t\right)-X\left(t\right)\right)^{2}\right)\longrightarrow0$   as $n\longrightarrow\infty$   for some $\left(X\left(t\right)\right)_{t=1}^{\infty}\in\oplus_{t=1}^{\infty}L^{2}\left(\Omega,\mathcal{F}_{t},P\right).$ Does there exists a subsequence $\left(\left(X_{n_{k}}\left(t\right)\right)_{t=1}^{\infty}\right)_{k\in\mathbb{N}}\subseteq\left(\left(X_{n}\left(t\right)\right)_{t=1}^{\infty}\right)_{n\in\mathbb{N}}$   such that, for every $t\geq1$  , $X_{n_{k}}\left(t\right)\longrightarrow X\left(t\right)$   almost surely? I worked out a proof that seems to be sound, but I'm not sure of a couple of passage I did. The proof is as follows. I indicate inside parenthesis the passage I am not sure about. First I claim that $$\sum_{t=1}^{\infty}E\left(\left(X_{n}\left(t\right)-X\left(t\right)\right)^{2}\right)\rightarrow0\Rightarrow P\left[\sup_{t}\left|X_{n}\left(t\right)-X\left(t\right)\right|>\epsilon\right]\rightarrow0  $$ for every $\epsilon>0  $. Proof of the claim. I prove the counterpositive. Suppose that there exist $\epsilon,\delta>0$ such that, for every $\bar{n}$, there exists an $n\left(\bar{n}\right)\geq\bar{n}$ such that $$P\left[\sup_{t}\left|X_{n\left(\bar{n}\right)}\left(t\right)-X\left(t\right)\right|>\epsilon\right]\geq\delta.$$ Then it must be true that (this the passage I am not sure about) $$P\left[\sum_{t=1}^{\infty}\left(X_{n\left(\bar{n}\right)}\left(t\right)-X_{n\left(\bar{n}\right)}\left(t\right)\right)^{2}\geq\epsilon^{2}\right]\geq\delta.$$ Call $E=\left[\sum_{t=1}^{\infty}\left(X_{n\left(\bar{n}\right)}\left(t\right)-X\left(t\right)\right)^{2}\geq\epsilon^{2}\right]\in\mathcal{F}$. We have: $$\sum_{t=1}^{\infty}E\left(\left(X_{n\left(\bar{n}\right)}\left(t\right)-X\left(t\right)\right)^{2}\right)	=	E\left(\sum_{t=1}^{\infty}\left(X_{n\left(\bar{n}\right)}\left(t\right)-X\left(t\right)\right)^{2}\right) \geq\int_{E}\sum_{t=1}^{\infty}\left(X_{n\left(\bar{n}\right)}\left(t\right)-X\left(t\right)\right)^{2}\mathrm{d}P	\geq	\epsilon^{2}\delta.$$ Then, for every $\bar{n}$, there exists an $n\left(\bar{n}\right)\geq\bar{n}$ such that $$\sum_{t=1}^{\infty}E\left(\left(X_{n\left(\bar{n}\right)}\left(t\right)-X\left(t\right)\right)^{2}\right)\geq\epsilon^{2}\delta$$ and this completes the proof of the claim. $\square$ Now I show that if $P\left[\sup_{t}\left|X_{n}\left(t\right)-X\left(t\right)\right|>\epsilon\right]\rightarrow0$   then there exists a subsequence $\left(\left(X_{n_{k}}\left(t\right)\right)_{t\in\mathbb{N}}\right)_{k\in\mathbb{N}}\subseteq\left(\left(X_{n}\left(t\right)\right)_{t\in\mathbb{N}}\right)_{n\in\mathbb{N}}$   such that $$\sup_{t}\left|X_{n_{k}}\left(t\right)-X\left(t\right)\right|\longrightarrow0\qquad\mbox{a.s.}$$  First observe that, for every $k>0$ , there exists an integer $n_{k}$  such that$$P\left[\sup_{t}\left|X_{n_{k}}\left(t\right)-X\left(t\right)\right|>\frac{1}{k}\right]\leq\frac{1}{2^{k}}.$$  This implies that$$\sum_{k=1}^{\infty}P\left[\sup_{t}\left|X_{n_{k}}\left(t\right)-X\left(t\right)\right|>\frac{1}{k}\right]<\infty.$$  Finally, observe that, for every $\epsilon>0$ , $$\left[\sup_{t}\left|X_{n_{k}}\left(t\right)-X\left(t\right)\right|>\epsilon\,\mbox{i.o.}\right]\subseteq\left[\sup_{t}\left|X_{n_{k}}\left(t\right)-X\left(t\right)\right|>\frac{1}{k}\,\mbox{i.o.}\right],$$  so that$$\sum_{k=1}^{\infty}P\left[\sup_{t}\left|X_{n_{k}}\left(t\right)-X\left(t\right)\right|>\frac{1}{k}\right]<\infty		 \Rightarrow P\left[\sup_{t}\left|X_{n_{k}}\left(t\right)-X\left(t\right)\right|>\frac{1}{k}\,\mbox{i.o.}\right]=0		 \Rightarrow P\left[\sup_{t}\left|X_{n_{k}}\left(t\right)-X\left(t\right)\right|>\epsilon\,\mbox{i.o.}\right]=0		\forall\epsilon>0 \Rightarrow\sup_{t}\left|X_{n_{k}}\left(t\right)-X\left(t\right)\right|\rightarrow0		\mbox{a.s.}$$  and this completes the proof  $\square\square$","It is known that if a sequence of random variables converges in norm then there exists a subsequence which converges almost surely. That is: let $\left(X_{n}\right)_{n\in\mathbb{N}}\subseteq L^{2}\left(\Omega,\mathcal{F},P\right)$   be a sequence of square integrable random variables on the probability space $\left(\Omega,\mathcal{F},P\right)$  . If $X\in L^{2}\left(\Omega,\mathcal{F},P\right)$   is such that $\mathrm{E}_{P}\left(\left(X_{n}-X\right)^{2}\right)\longrightarrow0$   as $n\longrightarrow\infty$  , then there exists a subsequence $\left(X_{n_{k}}\right)_{k\in\mathbb{N}}\subseteq\left(X_{n}\right)_{n\in\mathbb{N}}$   such that $X_{n_{k}}\longrightarrow X$   almost surely as $k\longrightarrow\infty$  . My question is the following: does this holds for an infinite direct sum of $L^{2}$   spaces? To be precise: consider a filtered probability space $\left(\Omega,\left(\mathcal{F}_{t}\right)_{t\in\mathbb{N}},\mathcal{F},P\right)$  . Define: $$\oplus_{t=1}^{\infty}L^{2}\left(\Omega,\mathcal{F}_{t},P\right)\equiv\left\{ \left(X\left(t\right)\right)_{t=1}^{\infty}:\, X\left(t\right)\in L^{2}\left(\Omega,\mathcal{F}_{t},P\right)\forall t,\,\sum_{t=1}^{\infty}\mathrm{E}_{P}\left(X\left(t\right)^{2}\right)<\infty\right\}.$$  Let $\left(\left(X_{n}\left(t\right)\right)_{t=1}^{\infty}\right)_{n\in\mathbb{N}}\subseteq\oplus_{t=1}^{\infty}L^{2}\left(\Omega,\mathcal{F}_{t},P\right)$   be such that $\sum_{t=1}^{\infty}\mathrm{E}_{P}\left(\left(X_{n}\left(t\right)-X\left(t\right)\right)^{2}\right)\longrightarrow0$   as $n\longrightarrow\infty$   for some $\left(X\left(t\right)\right)_{t=1}^{\infty}\in\oplus_{t=1}^{\infty}L^{2}\left(\Omega,\mathcal{F}_{t},P\right).$ Does there exists a subsequence $\left(\left(X_{n_{k}}\left(t\right)\right)_{t=1}^{\infty}\right)_{k\in\mathbb{N}}\subseteq\left(\left(X_{n}\left(t\right)\right)_{t=1}^{\infty}\right)_{n\in\mathbb{N}}$   such that, for every $t\geq1$  , $X_{n_{k}}\left(t\right)\longrightarrow X\left(t\right)$   almost surely? I worked out a proof that seems to be sound, but I'm not sure of a couple of passage I did. The proof is as follows. I indicate inside parenthesis the passage I am not sure about. First I claim that $$\sum_{t=1}^{\infty}E\left(\left(X_{n}\left(t\right)-X\left(t\right)\right)^{2}\right)\rightarrow0\Rightarrow P\left[\sup_{t}\left|X_{n}\left(t\right)-X\left(t\right)\right|>\epsilon\right]\rightarrow0  $$ for every $\epsilon>0  $. Proof of the claim. I prove the counterpositive. Suppose that there exist $\epsilon,\delta>0$ such that, for every $\bar{n}$, there exists an $n\left(\bar{n}\right)\geq\bar{n}$ such that $$P\left[\sup_{t}\left|X_{n\left(\bar{n}\right)}\left(t\right)-X\left(t\right)\right|>\epsilon\right]\geq\delta.$$ Then it must be true that (this the passage I am not sure about) $$P\left[\sum_{t=1}^{\infty}\left(X_{n\left(\bar{n}\right)}\left(t\right)-X_{n\left(\bar{n}\right)}\left(t\right)\right)^{2}\geq\epsilon^{2}\right]\geq\delta.$$ Call $E=\left[\sum_{t=1}^{\infty}\left(X_{n\left(\bar{n}\right)}\left(t\right)-X\left(t\right)\right)^{2}\geq\epsilon^{2}\right]\in\mathcal{F}$. We have: $$\sum_{t=1}^{\infty}E\left(\left(X_{n\left(\bar{n}\right)}\left(t\right)-X\left(t\right)\right)^{2}\right)	=	E\left(\sum_{t=1}^{\infty}\left(X_{n\left(\bar{n}\right)}\left(t\right)-X\left(t\right)\right)^{2}\right) \geq\int_{E}\sum_{t=1}^{\infty}\left(X_{n\left(\bar{n}\right)}\left(t\right)-X\left(t\right)\right)^{2}\mathrm{d}P	\geq	\epsilon^{2}\delta.$$ Then, for every $\bar{n}$, there exists an $n\left(\bar{n}\right)\geq\bar{n}$ such that $$\sum_{t=1}^{\infty}E\left(\left(X_{n\left(\bar{n}\right)}\left(t\right)-X\left(t\right)\right)^{2}\right)\geq\epsilon^{2}\delta$$ and this completes the proof of the claim. $\square$ Now I show that if $P\left[\sup_{t}\left|X_{n}\left(t\right)-X\left(t\right)\right|>\epsilon\right]\rightarrow0$   then there exists a subsequence $\left(\left(X_{n_{k}}\left(t\right)\right)_{t\in\mathbb{N}}\right)_{k\in\mathbb{N}}\subseteq\left(\left(X_{n}\left(t\right)\right)_{t\in\mathbb{N}}\right)_{n\in\mathbb{N}}$   such that $$\sup_{t}\left|X_{n_{k}}\left(t\right)-X\left(t\right)\right|\longrightarrow0\qquad\mbox{a.s.}$$  First observe that, for every $k>0$ , there exists an integer $n_{k}$  such that$$P\left[\sup_{t}\left|X_{n_{k}}\left(t\right)-X\left(t\right)\right|>\frac{1}{k}\right]\leq\frac{1}{2^{k}}.$$  This implies that$$\sum_{k=1}^{\infty}P\left[\sup_{t}\left|X_{n_{k}}\left(t\right)-X\left(t\right)\right|>\frac{1}{k}\right]<\infty.$$  Finally, observe that, for every $\epsilon>0$ , $$\left[\sup_{t}\left|X_{n_{k}}\left(t\right)-X\left(t\right)\right|>\epsilon\,\mbox{i.o.}\right]\subseteq\left[\sup_{t}\left|X_{n_{k}}\left(t\right)-X\left(t\right)\right|>\frac{1}{k}\,\mbox{i.o.}\right],$$  so that$$\sum_{k=1}^{\infty}P\left[\sup_{t}\left|X_{n_{k}}\left(t\right)-X\left(t\right)\right|>\frac{1}{k}\right]<\infty		 \Rightarrow P\left[\sup_{t}\left|X_{n_{k}}\left(t\right)-X\left(t\right)\right|>\frac{1}{k}\,\mbox{i.o.}\right]=0		 \Rightarrow P\left[\sup_{t}\left|X_{n_{k}}\left(t\right)-X\left(t\right)\right|>\epsilon\,\mbox{i.o.}\right]=0		\forall\epsilon>0 \Rightarrow\sup_{t}\left|X_{n_{k}}\left(t\right)-X\left(t\right)\right|\rightarrow0		\mbox{a.s.}$$  and this completes the proof  $\square\square$",,"['sequences-and-series', 'functional-analysis', 'probability-theory', 'convergence-divergence', 'hilbert-spaces']"
13,Differentiation operator on smooth function with compact support,Differentiation operator on smooth function with compact support,,"Suppose $f$ is $C^\infty$ with compact support. Let $T_n$ be the operator which sends $f$ to its $n$-th derivative. Is $||T_nf||_\infty$ bounded? It seems like I should use Stone-Weierstrass, but perhaps I am going about this the wrong way.","Suppose $f$ is $C^\infty$ with compact support. Let $T_n$ be the operator which sends $f$ to its $n$-th derivative. Is $||T_nf||_\infty$ bounded? It seems like I should use Stone-Weierstrass, but perhaps I am going about this the wrong way.",,"['analysis', 'functional-analysis', 'normed-spaces']"
14,bounded operator $T$ is not compact then there exists an orthonormal sequence $e_n$ and $d>0$ such that $\|T(e_n)\|>d$ for all $n\in\Bbb{N}$?,bounded operator  is not compact then there exists an orthonormal sequence  and  such that  for all ?,T e_n d>0 \|T(e_n)\|>d n\in\Bbb{N},"I want to prove that if a bounded operator $T$ is not compact then there exists an orthonormal sequence $e_n$ and $d>0$ such that $\|T(e_n)\|>d$ for all $n\in\Bbb{N}$. Could someone helps me? I think that the fact that T is not compact implies that there exists an arbitrary sequence in H that doesn't contain convergent subsequence, not for a orthonormal sequence. Moreover, we know that if T is a compact operator and en an orthonormal sequence then ||T(en)|| converges strongly to 0 but not the inverse. We want to show this.","I want to prove that if a bounded operator $T$ is not compact then there exists an orthonormal sequence $e_n$ and $d>0$ such that $\|T(e_n)\|>d$ for all $n\in\Bbb{N}$. Could someone helps me? I think that the fact that T is not compact implies that there exists an arbitrary sequence in H that doesn't contain convergent subsequence, not for a orthonormal sequence. Moreover, we know that if T is a compact operator and en an orthonormal sequence then ||T(en)|| converges strongly to 0 but not the inverse. We want to show this.",,[]
15,bounded operator $T$ is not compact then there exists an orthonormal sequence $e_n$ and $d>0$ such that $\|T(e_n)\|>d$ for all $n\in\Bbb{N}$?,bounded operator  is not compact then there exists an orthonormal sequence  and  such that  for all ?,T e_n d>0 \|T(e_n)\|>d n\in\Bbb{N},"I want to prove that if a bounded operator $T$ is not compact then there exists an orthonormal sequence $e_n$ and $d>0$ such that $\|T(e_n)\|>d$ for all $n\in\Bbb{N}$. Could someone helps me? I think that the fact that T is not compact implies that there exists an arbitrary sequence in H that doesn't contain convergent subsequence, not for a orthonormal sequence. Moreover, we know that if T is a compact operator and en an orthonormal sequence then ||T(en)|| converges strongly to 0 but not the inverse. We want to show this.","I want to prove that if a bounded operator $T$ is not compact then there exists an orthonormal sequence $e_n$ and $d>0$ such that $\|T(e_n)\|>d$ for all $n\in\Bbb{N}$. Could someone helps me? I think that the fact that T is not compact implies that there exists an arbitrary sequence in H that doesn't contain convergent subsequence, not for a orthonormal sequence. Moreover, we know that if T is a compact operator and en an orthonormal sequence then ||T(en)|| converges strongly to 0 but not the inverse. We want to show this.",,[]
16,"Equivalence of two definitions of weak solution (from a book, I don't understand something!!!!)","Equivalence of two definitions of weak solution (from a book, I don't understand something!!!!)",,"Consider $$y_t - \Delta y = f$$ $$y(0) = y_0$$ with zero boundary condition. Let $a(t,.,.)$ be the bilinear form associated to $-\Delta$. We have two definitions of weak solutions: We have $y \in L^2(0,T;H^{1}_0)$ with $y_t \in L^2(0,T;H^{-1})$ is a A-weak solution of problem (1) if   $$\langle y_t(t), v \rangle + a(t,y(t),v) = \langle f(t), v \rangle$$   for all $v \in H^1_0$ and almost all $t \in [0,T]$. and We have $y \in L^2(0,T;H^{1}_0)$ with $y_t \in L^2(0,T;H^{-1})$ is a B-weak solution of problem (1) if   $$\int_0^T \langle y_t(t), v(t) \rangle + \int_0^T a(t,y(t),v(t)) = \int_0^T \langle f(t), v(t) \rangle$$   for all $v \in L^2(0,T;H^1_0)$. The claim is that these two notions of solution are the same. For one side, the proof is like this. Let $y$ be an A-weak solution of (1). We shall show that $$ \langle y_t(t), v(t) \rangle + a(t,y(t),v(t)) =  \langle f(t), v(t) \rangle \tag{1.61}$$ for all $v \in L^2(0,T;H^1_0)$ for a.a. $t$. This is because: So he proves (1.61) for all simple functions, and hence for the simple functions $v_k$ that converge to an arbitrary $v$,  and the null set does not depend on $v_k$. Then he passes to the limit $v_k(t) \to v(t)$ for a.e. $t$. So in the end the null set does depend on $v$ (as it must do). So why the whole fuss about getting rid of the null set and making it independent of $v_k$ in the first place? What gets messed up if he didn't do that? (This is on page 43 of ""Optimization with PDE constraints"" by Hinze, Pinnau, Ulbrich.)","Consider $$y_t - \Delta y = f$$ $$y(0) = y_0$$ with zero boundary condition. Let $a(t,.,.)$ be the bilinear form associated to $-\Delta$. We have two definitions of weak solutions: We have $y \in L^2(0,T;H^{1}_0)$ with $y_t \in L^2(0,T;H^{-1})$ is a A-weak solution of problem (1) if   $$\langle y_t(t), v \rangle + a(t,y(t),v) = \langle f(t), v \rangle$$   for all $v \in H^1_0$ and almost all $t \in [0,T]$. and We have $y \in L^2(0,T;H^{1}_0)$ with $y_t \in L^2(0,T;H^{-1})$ is a B-weak solution of problem (1) if   $$\int_0^T \langle y_t(t), v(t) \rangle + \int_0^T a(t,y(t),v(t)) = \int_0^T \langle f(t), v(t) \rangle$$   for all $v \in L^2(0,T;H^1_0)$. The claim is that these two notions of solution are the same. For one side, the proof is like this. Let $y$ be an A-weak solution of (1). We shall show that $$ \langle y_t(t), v(t) \rangle + a(t,y(t),v(t)) =  \langle f(t), v(t) \rangle \tag{1.61}$$ for all $v \in L^2(0,T;H^1_0)$ for a.a. $t$. This is because: So he proves (1.61) for all simple functions, and hence for the simple functions $v_k$ that converge to an arbitrary $v$,  and the null set does not depend on $v_k$. Then he passes to the limit $v_k(t) \to v(t)$ for a.e. $t$. So in the end the null set does depend on $v$ (as it must do). So why the whole fuss about getting rid of the null set and making it independent of $v_k$ in the first place? What gets messed up if he didn't do that? (This is on page 43 of ""Optimization with PDE constraints"" by Hinze, Pinnau, Ulbrich.)",,"['functional-analysis', 'measure-theory', 'partial-differential-equations', 'sobolev-spaces', 'bochner-spaces']"
17,Probability measure on the set of probability measures,Probability measure on the set of probability measures,,"Let $X$ be a metric space, $\mathcal{P}(X)$ be the the set of all Borel probability measures on $X$. I endow $\mathcal{P}(X)$ with the weak* topology. Question 1: Do I need to impose some further requirement on $X$ for $\mathcal{P}(X)$ to be metrizable? Let us say that $\mathcal{P}(X)$ (with the weak* topology) is metrizable, and so consider $\mathcal{P}(\mathcal{P}(X))$. Let $\mu \in \mathcal{P}(\mathcal{P}(X))$ and define, for every $B$ borel subset of $X$, $$ p_\mu(B)=\int_{\mathcal{P}(X)}\sigma(B)\mu(d\sigma). $$ This is null on the empty set, non-negative and countably additive, so a probability measure on $X$. Question 2: Am I right or am I missing some nuances?","Let $X$ be a metric space, $\mathcal{P}(X)$ be the the set of all Borel probability measures on $X$. I endow $\mathcal{P}(X)$ with the weak* topology. Question 1: Do I need to impose some further requirement on $X$ for $\mathcal{P}(X)$ to be metrizable? Let us say that $\mathcal{P}(X)$ (with the weak* topology) is metrizable, and so consider $\mathcal{P}(\mathcal{P}(X))$. Let $\mu \in \mathcal{P}(\mathcal{P}(X))$ and define, for every $B$ borel subset of $X$, $$ p_\mu(B)=\int_{\mathcal{P}(X)}\sigma(B)\mu(d\sigma). $$ This is null on the empty set, non-negative and countably additive, so a probability measure on $X$. Question 2: Am I right or am I missing some nuances?",,"['functional-analysis', 'probability-theory']"
18,Compactness of a set of bounded functions in the uniform norm,Compactness of a set of bounded functions in the uniform norm,,"Let $T$ be a non-degenerate compact interval in $\mathbb R$ and $f:\mathbb R^2\to\mathbb R$ a strictly concave function such that (a) $f(0,0)=0$, (b) $f$ strictly increases in the first argument, and (c) it strictly decreases in the second. Consider the following set of functions: \begin{align*} C\equiv\Big\{(a,b):T\to\mathbb R^2\,\Big|\,&\text{(i) $a$ and $b$ are bounded and Borel measurable functions;}\\ &\text{(ii) $\int_{t\in T}a(t)\,\mathrm d t=\int_{t\in T}b(t)\,\mathrm d t$;}\\ &\text{(iii) $\int_{t\in T}f(a(t),b(t))\,\mathrm dt\geq0$}\Big\}. \end{align*} Note that $C$ is not empty, as it contains the identically zero function $(0,0)$. My question is: envisioned as a subset of the Banach space of bounded functions from $T$ to $\mathbb R^2$ endowed with the (product) uniform norm, is $C$ compact? If not, would adding the following extra condition [which would imply also (i)]: “(iv) $a$ and $b$ are non-decreasing” make it compact? What about other possible extra conditions? Note that Arzelà—Ascoli (a famous theorem on the compactness of certain subsets of function spaces) is off the table with no assumption on continuity. I guess the norm-closedness (and hence the completeness) of $C$ is not difficult to see (I'm not 100% percent sure, though, whether it's true in the first place), but I'm stuck with total boundedness. My gut feeling tells me that $C$ “should” be compact given the assumptions on $f$, but I'm stuck. Any hints would be appreciated. (I don't need a complete solution just a direction to start.)","Let $T$ be a non-degenerate compact interval in $\mathbb R$ and $f:\mathbb R^2\to\mathbb R$ a strictly concave function such that (a) $f(0,0)=0$, (b) $f$ strictly increases in the first argument, and (c) it strictly decreases in the second. Consider the following set of functions: \begin{align*} C\equiv\Big\{(a,b):T\to\mathbb R^2\,\Big|\,&\text{(i) $a$ and $b$ are bounded and Borel measurable functions;}\\ &\text{(ii) $\int_{t\in T}a(t)\,\mathrm d t=\int_{t\in T}b(t)\,\mathrm d t$;}\\ &\text{(iii) $\int_{t\in T}f(a(t),b(t))\,\mathrm dt\geq0$}\Big\}. \end{align*} Note that $C$ is not empty, as it contains the identically zero function $(0,0)$. My question is: envisioned as a subset of the Banach space of bounded functions from $T$ to $\mathbb R^2$ endowed with the (product) uniform norm, is $C$ compact? If not, would adding the following extra condition [which would imply also (i)]: “(iv) $a$ and $b$ are non-decreasing” make it compact? What about other possible extra conditions? Note that Arzelà—Ascoli (a famous theorem on the compactness of certain subsets of function spaces) is off the table with no assumption on continuity. I guess the norm-closedness (and hence the completeness) of $C$ is not difficult to see (I'm not 100% percent sure, though, whether it's true in the first place), but I'm stuck with total boundedness. My gut feeling tells me that $C$ “should” be compact given the assumptions on $f$, but I'm stuck. Any hints would be appreciated. (I don't need a complete solution just a direction to start.)",,"['functional-analysis', 'metric-spaces', 'compactness']"
19,Comparison and maximum principle for parabolic pde,Comparison and maximum principle for parabolic pde,,"I was told the comparison principle can also be understood as the fact that the difference between a subsolution and a supersolution satisfies the maximum principle. I also know comparison principle can be regarded as the nonlinear version of maximum principle. I am vague about distinguishing these two theorem in a precise manner. Here is a version of weak MP from Evans p.368 Assume $u\in C^{(2,1)}(\Omega_T)\cap C(\bar{\Omega}_T)$ and  \begin{equation} 	c\equiv 0\quad\text{in }\Omega_T \end{equation} If     \begin{equation}%\label{subconw1} 		u_t+Lu\le 0\quad\text{in }\Omega_T 	\end{equation}     then     \begin{equation*}		\max_{\substack{\bar{\Omega}_T}}u=\max_{\substack{\bar{\Gamma}_T}}u 	\end{equation*}  Likewise, if     \begin{equation}%\label{supconw1} 		u_t+Lu\ge 0\quad\text{in }\Omega_T 	\end{equation}     then     \begin{equation*} \min_{\substack{\bar{\Omega}_T}}u=\min_{\substack{\bar{\Gamma}_T}}u		 	\end{equation*} I would like a comparison principle theorem by modifying above. Please help! (This is what I know https://www.ma.utexas.edu/mediawiki/index.php/Comparison_principle#Parabolic_case )","I was told the comparison principle can also be understood as the fact that the difference between a subsolution and a supersolution satisfies the maximum principle. I also know comparison principle can be regarded as the nonlinear version of maximum principle. I am vague about distinguishing these two theorem in a precise manner. Here is a version of weak MP from Evans p.368 Assume $u\in C^{(2,1)}(\Omega_T)\cap C(\bar{\Omega}_T)$ and  \begin{equation} 	c\equiv 0\quad\text{in }\Omega_T \end{equation} If     \begin{equation}%\label{subconw1} 		u_t+Lu\le 0\quad\text{in }\Omega_T 	\end{equation}     then     \begin{equation*}		\max_{\substack{\bar{\Omega}_T}}u=\max_{\substack{\bar{\Gamma}_T}}u 	\end{equation*}  Likewise, if     \begin{equation}%\label{supconw1} 		u_t+Lu\ge 0\quad\text{in }\Omega_T 	\end{equation}     then     \begin{equation*} \min_{\substack{\bar{\Omega}_T}}u=\min_{\substack{\bar{\Gamma}_T}}u		 	\end{equation*} I would like a comparison principle theorem by modifying above. Please help! (This is what I know https://www.ma.utexas.edu/mediawiki/index.php/Comparison_principle#Parabolic_case )",,"['functional-analysis', 'partial-differential-equations', 'maximum-principle', 'regularity-theory-of-pdes']"
20,Riesz-Markov-Kakutani Theorem: Various Versions,Riesz-Markov-Kakutani Theorem: Various Versions,,"The Riesz-Markov-Kakutani theorem usually comes in various versions. So I'm a little bit confused and wondering which of these are right. Let $\Omega$ be a locally compact space. Then: Complex Measures: $\mathcal{M}(\Omega,\mathcal{B}(\Omega))\cong\mathcal{C}_c(\Omega)^*$ Regular Measures: $\mathcal{M}_r(\Omega,\mathcal{B}(\Omega))\cong\mathcal{C}_c(\Omega)'$ Positive Measures: $\mathcal{M}_p(\Omega,\mathcal{B}(\Omega))\cong\mathcal{C}_c(\Omega)°$ where $X^*$ denotes the linear functionals, $X'$ the continuous linear functionals and $X^*$ the positive linear functionals $f:X\to\mathbb{C}$. The first two equivalences are meant in the sense of vector spaces whereas the last one only as something in the sense of cones I guess.","The Riesz-Markov-Kakutani theorem usually comes in various versions. So I'm a little bit confused and wondering which of these are right. Let $\Omega$ be a locally compact space. Then: Complex Measures: $\mathcal{M}(\Omega,\mathcal{B}(\Omega))\cong\mathcal{C}_c(\Omega)^*$ Regular Measures: $\mathcal{M}_r(\Omega,\mathcal{B}(\Omega))\cong\mathcal{C}_c(\Omega)'$ Positive Measures: $\mathcal{M}_p(\Omega,\mathcal{B}(\Omega))\cong\mathcal{C}_c(\Omega)°$ where $X^*$ denotes the linear functionals, $X'$ the continuous linear functionals and $X^*$ the positive linear functionals $f:X\to\mathbb{C}$. The first two equivalences are meant in the sense of vector spaces whereas the last one only as something in the sense of cones I guess.",,"['general-topology', 'functional-analysis', 'measure-theory']"
21,Motivation for the notion of locally convex topological vector space,Motivation for the notion of locally convex topological vector space,,"Is the only motivation for the notion of locally convex topological vector space that the local bases have some nice property i.e. convex, balanced, absorbing ?","Is the only motivation for the notion of locally convex topological vector space that the local bases have some nice property i.e. convex, balanced, absorbing ?",,"['functional-analysis', 'topological-vector-spaces', 'locally-convex-spaces']"
22,Proposed proof of analysis result,Proposed proof of analysis result,,"Hi please advise on my proof of the following result: Assume that $I \subset \mathbb{R}^{n}$ is convex, bounded open set with Lipschitz boundary and let $u_{m},u$ be such that $$u_{m} \rightharpoonup^{*} u \text{ in } W^{1,\infty}(I)$$ and $$\Vert \nabla u_{m} \Vert_{L^{\infty}(I)} \leq \alpha$$ for some fixed $\alpha > 0$. Show that $u_{m}$ and $u$ are Lipschitz continuous on $I$ with Lipschitz constant $\alpha$. Proposed Proof: I will first do the proof for the case of $u$. Note that since $u_{m} \rightharpoonup^{*} u$ in $W^{1,\infty}(I)$ it follows that $\nabla u_{m} \rightharpoonup^{*} \nabla u$ in $L^{\infty}(I)$, therefore $\Vert \nabla u \Vert_{L^{\infty}(I)} \leq \liminf\limits_{m \rightarrow \infty}\Vert \nabla u_{m} \Vert_{L^{\infty}(I)} \leq \alpha$. We can write $u(x) = u(a) + \int_{a}^{x}\nabla u(tx+(1-t)y)\cdot(x-y) dt$(Since $u$ is a Sobolev function, it is absolutely continuous on almost every line). We now use the following argument: Consider $$\psi(t) := u(tx + (1-t)y) ~~~ \text{ for } t \in [0,1]$$ Then $$\psi(1) - \psi(0) = \int_{0}^{1}\psi^{'}(t)dt = \int_{0}^{1}\nabla u(tx+(1-t)y)\cdot (x-y)dt$$  therefore $$|u(x)-u(y)| \leq \int_{0}^{1}|\nabla u(tx+(1-t)y)||x-y|dt \leq \Vert \nabla u \Vert_{L^{\infty}}|x-y| \leq \alpha |x-y|$$ This shows $u$ is Lipschitz on $I$ with Lipschitz constant $\alpha$. The same process can be carried out for the case involving $u_{m}$ by using the assumed condition $\Vert u_{m} \Vert_{L^{\infty}} \leq \alpha$. $\square$ Let me know if this proof is fine? Thanks for any assistance, also let me know if something is unclear.","Hi please advise on my proof of the following result: Assume that $I \subset \mathbb{R}^{n}$ is convex, bounded open set with Lipschitz boundary and let $u_{m},u$ be such that $$u_{m} \rightharpoonup^{*} u \text{ in } W^{1,\infty}(I)$$ and $$\Vert \nabla u_{m} \Vert_{L^{\infty}(I)} \leq \alpha$$ for some fixed $\alpha > 0$. Show that $u_{m}$ and $u$ are Lipschitz continuous on $I$ with Lipschitz constant $\alpha$. Proposed Proof: I will first do the proof for the case of $u$. Note that since $u_{m} \rightharpoonup^{*} u$ in $W^{1,\infty}(I)$ it follows that $\nabla u_{m} \rightharpoonup^{*} \nabla u$ in $L^{\infty}(I)$, therefore $\Vert \nabla u \Vert_{L^{\infty}(I)} \leq \liminf\limits_{m \rightarrow \infty}\Vert \nabla u_{m} \Vert_{L^{\infty}(I)} \leq \alpha$. We can write $u(x) = u(a) + \int_{a}^{x}\nabla u(tx+(1-t)y)\cdot(x-y) dt$(Since $u$ is a Sobolev function, it is absolutely continuous on almost every line). We now use the following argument: Consider $$\psi(t) := u(tx + (1-t)y) ~~~ \text{ for } t \in [0,1]$$ Then $$\psi(1) - \psi(0) = \int_{0}^{1}\psi^{'}(t)dt = \int_{0}^{1}\nabla u(tx+(1-t)y)\cdot (x-y)dt$$  therefore $$|u(x)-u(y)| \leq \int_{0}^{1}|\nabla u(tx+(1-t)y)||x-y|dt \leq \Vert \nabla u \Vert_{L^{\infty}}|x-y| \leq \alpha |x-y|$$ This shows $u$ is Lipschitz on $I$ with Lipschitz constant $\alpha$. The same process can be carried out for the case involving $u_{m}$ by using the assumed condition $\Vert u_{m} \Vert_{L^{\infty}} \leq \alpha$. $\square$ Let me know if this proof is fine? Thanks for any assistance, also let me know if something is unclear.",,"['real-analysis', 'functional-analysis']"
23,How to solve this finite-difference equation?,How to solve this finite-difference equation?,,"How to solve the following finite-differences equation: $$f(x) = f(x-1) + f(x-\sqrt{2}), \quad x\in [\sqrt{2}, +\infty) \,?$$ Let's say $f(x) = f_0(x)$ for $x \in [0, \sqrt{2})$ is a given function. My attempt: Let's write characteristic function $L(\lambda) = 1 - e^{-\lambda} - e^{-\lambda\sqrt{2}}$. We need to solve the equation $L(\lambda) = 0$. There should be a root $\lambda_0 > 0$ (because $L$ is monotonously increasing, $L(0) = -1$ and $\lim_{\lambda \to +\infty} L(\lambda) = 1$), but my first question there is: how many other roots it has? Is it true that it has infinite number of (complex) roots? why?..","How to solve the following finite-differences equation: $$f(x) = f(x-1) + f(x-\sqrt{2}), \quad x\in [\sqrt{2}, +\infty) \,?$$ Let's say $f(x) = f_0(x)$ for $x \in [0, \sqrt{2})$ is a given function. My attempt: Let's write characteristic function $L(\lambda) = 1 - e^{-\lambda} - e^{-\lambda\sqrt{2}}$. We need to solve the equation $L(\lambda) = 0$. There should be a root $\lambda_0 > 0$ (because $L$ is monotonously increasing, $L(0) = -1$ and $\lim_{\lambda \to +\infty} L(\lambda) = 1$), but my first question there is: how many other roots it has? Is it true that it has infinite number of (complex) roots? why?..",,"['functional-analysis', 'recurrence-relations', 'functional-equations', 'finite-differences']"
24,Cauchy nets in a metric space,Cauchy nets in a metric space,,"Say that a net $a_i$ in a metric space is cauchy if for every $\epsilon > 0$ there exists $I$ such that for all $i, j \geq I$ one has $d(a_i,a_j) \leq \epsilon$.  If the metric space is complete, does it hold (and in either case why) that every cauchy net converges?","Say that a net $a_i$ in a metric space is cauchy if for every $\epsilon > 0$ there exists $I$ such that for all $i, j \geq I$ one has $d(a_i,a_j) \leq \epsilon$.  If the metric space is complete, does it hold (and in either case why) that every cauchy net converges?",,"['metric-spaces', 'convergence-divergence']"
25,Distributions and primitives,Distributions and primitives,,"I was wondering: if distributions are seen as a generalization of functions that ""removes obstructions"" to the operation of derivation, is there a generalization of functions that would remove any obstruction to the operation of integration, i.e. that would make any ""function"" have a primitive?","I was wondering: if distributions are seen as a generalization of functions that ""removes obstructions"" to the operation of derivation, is there a generalization of functions that would remove any obstruction to the operation of integration, i.e. that would make any ""function"" have a primitive?",,"['integration', 'functional-analysis', 'derivatives', 'distribution-theory']"
26,Show $\lim_{n\to\infty} n^p f(nx) = 0$ exists in the distributional sense,Show  exists in the distributional sense,\lim_{n\to\infty} n^p f(nx) = 0,"Let $f\in C^\infty(\mathbb R)$ be periodic, with period $2\pi$ and have mean zero ($\int^{2\pi}_0 f(x)dx =0$).  Show that for any positive integer $p$ the following limit is valid in the distributional sense. $$ \lim_{n\to\infty} n^p f(nx)= 0$$ What I have so far is that   $$   \int^\infty_{-\infty} n^p f(nx)\phi(x)dx=\sum^{\infty}_{k=-\infty}n^p\int^{2\pi k+2\pi}_{2\pi k} f(nx)\phi(x)dx  $$ Using $y=x-2\pi k$ and $dy=dx$, $$ =\sum^{\infty}_{k=-\infty}n^{p}\int^{2\pi}_{0} f(ny+2\pi k n)\phi(y+2\pi k )dy $$ by periodicity of $f$, this becomes $$ =\sum^{\infty}_{k=-\infty}n^{p}\int^{2\pi n}_{0} f(ny)\phi(y+2\pi k )dy $$ Since $\phi\in C^\infty_0$, there exist integers $a,b$ such that $\phi(y+2\pi k )=0\forall k<a,k>b$ $$ =\sum^{b}_{k=a}n^{p}\int^{2\pi n}_{0} f(ny)\phi(y+2\pi k )dy $$ I am unsure how to proceed further.  I suspect a Fourier series is involved.","Let $f\in C^\infty(\mathbb R)$ be periodic, with period $2\pi$ and have mean zero ($\int^{2\pi}_0 f(x)dx =0$).  Show that for any positive integer $p$ the following limit is valid in the distributional sense. $$ \lim_{n\to\infty} n^p f(nx)= 0$$ What I have so far is that   $$   \int^\infty_{-\infty} n^p f(nx)\phi(x)dx=\sum^{\infty}_{k=-\infty}n^p\int^{2\pi k+2\pi}_{2\pi k} f(nx)\phi(x)dx  $$ Using $y=x-2\pi k$ and $dy=dx$, $$ =\sum^{\infty}_{k=-\infty}n^{p}\int^{2\pi}_{0} f(ny+2\pi k n)\phi(y+2\pi k )dy $$ by periodicity of $f$, this becomes $$ =\sum^{\infty}_{k=-\infty}n^{p}\int^{2\pi n}_{0} f(ny)\phi(y+2\pi k )dy $$ Since $\phi\in C^\infty_0$, there exist integers $a,b$ such that $\phi(y+2\pi k )=0\forall k<a,k>b$ $$ =\sum^{b}_{k=a}n^{p}\int^{2\pi n}_{0} f(ny)\phi(y+2\pi k )dy $$ I am unsure how to proceed further.  I suspect a Fourier series is involved.",,"['functional-analysis', 'fourier-analysis', 'fourier-series']"
27,Reference for simplicity of the principal eigenvalue of the Laplacian,Reference for simplicity of the principal eigenvalue of the Laplacian,,"i'm currently searching for a proper reference or proof to see that the first eigenvalue $\lambda \in \mathbb{R}$ of \begin{equation*}  - \Delta u = \lambda u  \text{ in } \Omega, \\ u \in H^1(\Omega), \end{equation*} (in a weak sense in the Sobolev-space) with homogenious Neumann or Dirichlet boundary conditions is simple in the case of a bounded and connected Domain $\Omega \subset \mathbb{R}^d$ (where we assume $\Omega$ to be a Lipschitz-Domain in the Neumann-case, so that the outer normals are well-defined). The only proofs i've seen to far all use the assumption that the boundary $\partial \Omega$ is smooth. Because than we have enough regularity for the eigenfunctions to be in $C^2(\overline{\Omega})$ and can show by using the strong Maximum-Principle , that we can assume the corresponding eigenfunctions of the first eigenvalue to be strict positive. And because two such strict positive eigenfunction can't be orthogonal in $L_2(\Omega)$, we have that the corresponding eigenspace is indeed one-dimensional. But what about results without the smooth boundary? Thank you in advance!","i'm currently searching for a proper reference or proof to see that the first eigenvalue $\lambda \in \mathbb{R}$ of \begin{equation*}  - \Delta u = \lambda u  \text{ in } \Omega, \\ u \in H^1(\Omega), \end{equation*} (in a weak sense in the Sobolev-space) with homogenious Neumann or Dirichlet boundary conditions is simple in the case of a bounded and connected Domain $\Omega \subset \mathbb{R}^d$ (where we assume $\Omega$ to be a Lipschitz-Domain in the Neumann-case, so that the outer normals are well-defined). The only proofs i've seen to far all use the assumption that the boundary $\partial \Omega$ is smooth. Because than we have enough regularity for the eigenfunctions to be in $C^2(\overline{\Omega})$ and can show by using the strong Maximum-Principle , that we can assume the corresponding eigenfunctions of the first eigenvalue to be strict positive. And because two such strict positive eigenfunction can't be orthogonal in $L_2(\Omega)$, we have that the corresponding eigenspace is indeed one-dimensional. But what about results without the smooth boundary? Thank you in advance!",,"['functional-analysis', 'reference-request', 'eigenvalues-eigenvectors', 'spectral-theory']"
28,a question about Tsirelson's space,a question about Tsirelson's space,,"Background. Let $T$ denote the Figiel-Johnson construction of the Tsirelson space, that is, the completion of $c_{00}$ under the implicitly-defined norm \begin{equation*}\|x\|_T=\max\left\{\|x\|_{\ell_\infty},\frac{1}{2}\sup\sum_i\|E_ix\|_T\right\}\end{equation*} where the ""sup"" is taken over all $j\in\mathbb{N}$ and all sequences $E_1<\cdots<E_j$ of finite subsets of $\mathbb{N}$ satisfying $j\leq\min E_1$. It is well-known that $T$ and its dual $T^*$ are reflexive spaces which each fail to contain any copy of $c_0$ or $\ell_p$, $1\leq p\leq\infty$.  Furthermore, $T^*$ can be represented as the completion of $c_{00}$ under some other norm $\|\cdot\|_{T^*}$, such that elements of $T^*$ act on elements of $T$ in the natural way, i.e. if $f=(\beta_n)\in T^*$ and $x=(\alpha_n)\in T$ then $f(x)=\sum_{n=1}^\infty\alpha_n\beta_n$. Problem. Given an arbitrary $C>0$ and an arbitrary subsequence $(e_{n_k})$, I would like to find the coordinates for a sequence $(\gamma_k)\in c_{00}$ such that \begin{equation*}C\|(\gamma_k)\|_{\ell_\infty}\leq\|\sum_{k=1}^\infty\gamma_ke_{n_k}\|_{T^*}.\end{equation*} Discussion. The existence of such a sequence is easy to show given the above facts about $T$ and $T^*$.  Notice that $\|f\|_{\ell_\infty}\leq\|f\|_{T^*}$ for all $f\in c_{00}$.  This is because, due to $c_{00}$ being dense in $T$, for any $\epsilon>0$ we can always find $x\in c_{00}$ with \begin{equation*}\|f\|_{T^*}\|x\|_T\geq|f(x)|\geq(\|f\|_{\ell_\infty}-\epsilon)\|x\|_{\ell_1}\geq(\|f\|_{\ell_\infty}-\epsilon)\|x\|_T.\end{equation*} Thus, since $T^*$ fails to contain a copy of $c_0$, we get the desired sequence $(\gamma_k)$. However, I am interested in a more constructive method of obtaining $(\gamma_k)$.  This is because I will be looking at variations on Tsirelson's space for which the deep result about $T^*$ containing no copy of $c_0$ is not always established.  Is there a way to actually write down the coordinates of $(\gamma_k)$?  For instance, what about something similar to Argyros's construction of repeated averages (special convex combinations)? Thanks!","Background. Let $T$ denote the Figiel-Johnson construction of the Tsirelson space, that is, the completion of $c_{00}$ under the implicitly-defined norm \begin{equation*}\|x\|_T=\max\left\{\|x\|_{\ell_\infty},\frac{1}{2}\sup\sum_i\|E_ix\|_T\right\}\end{equation*} where the ""sup"" is taken over all $j\in\mathbb{N}$ and all sequences $E_1<\cdots<E_j$ of finite subsets of $\mathbb{N}$ satisfying $j\leq\min E_1$. It is well-known that $T$ and its dual $T^*$ are reflexive spaces which each fail to contain any copy of $c_0$ or $\ell_p$, $1\leq p\leq\infty$.  Furthermore, $T^*$ can be represented as the completion of $c_{00}$ under some other norm $\|\cdot\|_{T^*}$, such that elements of $T^*$ act on elements of $T$ in the natural way, i.e. if $f=(\beta_n)\in T^*$ and $x=(\alpha_n)\in T$ then $f(x)=\sum_{n=1}^\infty\alpha_n\beta_n$. Problem. Given an arbitrary $C>0$ and an arbitrary subsequence $(e_{n_k})$, I would like to find the coordinates for a sequence $(\gamma_k)\in c_{00}$ such that \begin{equation*}C\|(\gamma_k)\|_{\ell_\infty}\leq\|\sum_{k=1}^\infty\gamma_ke_{n_k}\|_{T^*}.\end{equation*} Discussion. The existence of such a sequence is easy to show given the above facts about $T$ and $T^*$.  Notice that $\|f\|_{\ell_\infty}\leq\|f\|_{T^*}$ for all $f\in c_{00}$.  This is because, due to $c_{00}$ being dense in $T$, for any $\epsilon>0$ we can always find $x\in c_{00}$ with \begin{equation*}\|f\|_{T^*}\|x\|_T\geq|f(x)|\geq(\|f\|_{\ell_\infty}-\epsilon)\|x\|_{\ell_1}\geq(\|f\|_{\ell_\infty}-\epsilon)\|x\|_T.\end{equation*} Thus, since $T^*$ fails to contain a copy of $c_0$, we get the desired sequence $(\gamma_k)$. However, I am interested in a more constructive method of obtaining $(\gamma_k)$.  This is because I will be looking at variations on Tsirelson's space for which the deep result about $T^*$ containing no copy of $c_0$ is not always established.  Is there a way to actually write down the coordinates of $(\gamma_k)$?  For instance, what about something similar to Argyros's construction of repeated averages (special convex combinations)? Thanks!",,"['sequences-and-series', 'functional-analysis', 'banach-spaces']"
29,Limit of distribution,Limit of distribution,,"Let $T\in\mathcal{D}'(\mathbb{R})$ be a distribution on the set of smooth functions of compact support $\mathcal{D}(\mathbb{R})$ such that $$  \forall_{g\in\mathcal{D}(\mathbb{R})}~|\langle T, g \rangle| \leq \textrm{const}\|\tilde{g}\|_1,  $$ where $\tilde{g}$ is Fourier transform of $g$ and $\|\tilde{g}\|_1:=\int_\mathbb{R} |\tilde{g}(\omega)| \, d\omega$. What might be said about the existence of the limit $$  \lim_{a\rightarrow\infty} \langle T, g_a \rangle. $$ where $g_a(x):=g(ax)$.","Let $T\in\mathcal{D}'(\mathbb{R})$ be a distribution on the set of smooth functions of compact support $\mathcal{D}(\mathbb{R})$ such that $$  \forall_{g\in\mathcal{D}(\mathbb{R})}~|\langle T, g \rangle| \leq \textrm{const}\|\tilde{g}\|_1,  $$ where $\tilde{g}$ is Fourier transform of $g$ and $\|\tilde{g}\|_1:=\int_\mathbb{R} |\tilde{g}(\omega)| \, d\omega$. What might be said about the existence of the limit $$  \lim_{a\rightarrow\infty} \langle T, g_a \rangle. $$ where $g_a(x):=g(ax)$.",,"['functional-analysis', 'limits', 'distribution-theory']"
30,Complexity of a Borel linear subspace of a Banach space,Complexity of a Borel linear subspace of a Banach space,,"This question is inspired by the MO question Image of $L^1$ under the Fourier transform , but I think it might be much easier so I am posting it here instead. Let $(X, \|\cdot\|)$ be a separable Banach space, and $E$ a linear subspace. 1. Suppose $E$ is Borel.  What can be the complexity of $E$ , in the sense of the Borel hierarchy ?  (For example, as shown here , it cannot be properly $G_\delta$ .)  Can $E$ have arbitrarily high Borel rank? Let us say $E$ is Banachable if there is another norm $\|\cdot\|'$ on $E$ , stronger than $\|\cdot\|$ , under which $E$ is separable Banach.  Or equivalently, $E$ is Banachable iff there exists a separable Banach space $Y$ and a continuous injective linear map $T : Y \to X$ whose image is $E$ .  It is clear that every Banachable subspace is Borel. 2. Is every Borel subspace Banachable? Trivially no; consider any $E$ which is of countably infinite Hamel dimension. 3. If not, what can be the complexity of a Banachable subspace? 4. Do these answers change if I replace ""Banach"" by ""Hilbert"" throughout? One could also ask the analogous questions for Polish groups.","This question is inspired by the MO question Image of under the Fourier transform , but I think it might be much easier so I am posting it here instead. Let be a separable Banach space, and a linear subspace. 1. Suppose is Borel.  What can be the complexity of , in the sense of the Borel hierarchy ?  (For example, as shown here , it cannot be properly .)  Can have arbitrarily high Borel rank? Let us say is Banachable if there is another norm on , stronger than , under which is separable Banach.  Or equivalently, is Banachable iff there exists a separable Banach space and a continuous injective linear map whose image is .  It is clear that every Banachable subspace is Borel. 2. Is every Borel subspace Banachable? Trivially no; consider any which is of countably infinite Hamel dimension. 3. If not, what can be the complexity of a Banachable subspace? 4. Do these answers change if I replace ""Banach"" by ""Hilbert"" throughout? One could also ask the analogous questions for Polish groups.","L^1 (X, \|\cdot\|) E E E G_\delta E E \|\cdot\|' E \|\cdot\| E E Y T : Y \to X E E","['functional-analysis', 'banach-spaces', 'descriptive-set-theory']"
31,"Operators such that $\langle Ax,x \rangle=-\langle x,Ax \rangle$",Operators such that,"\langle Ax,x \rangle=-\langle x,Ax \rangle","Let $X$ be a Banach space. We consider the differential equation: $$x'(t)=Ax(t), \ \ \ t\in\mathbb{R}$$ where $A$ is a bounded operator on $X$. If $X$ is a Hilbert space, and  $x(t)$ is a solution of the differential equation, then  $$\frac{d}{dt}\|x(t)\|^2=\langle Ax(t),x(t) \rangle+\langle x(t),Ax(t) \rangle$$ If the operator $A$ has the property $\langle Ax,x \rangle=-\langle x,Ax \rangle$, then we will get  $$\frac{d}{dt}\|x(t)\|^2=0,$$ which means $|x(t)|$ is constant. So the condition $\langle Ax,x \rangle=-\langle x,Ax \rangle$ makes $|x(t)|$ constant for every $x(t)$ solution. Can we find a weaker property on $A$ which will make $|x(t)|$ constant only for the bounded solutions on $\mathbb{R}$? Is there any reference which deals with these problems ? Note: I am not looking for evident conditions like: The only bounded solution is $0$.","Let $X$ be a Banach space. We consider the differential equation: $$x'(t)=Ax(t), \ \ \ t\in\mathbb{R}$$ where $A$ is a bounded operator on $X$. If $X$ is a Hilbert space, and  $x(t)$ is a solution of the differential equation, then  $$\frac{d}{dt}\|x(t)\|^2=\langle Ax(t),x(t) \rangle+\langle x(t),Ax(t) \rangle$$ If the operator $A$ has the property $\langle Ax,x \rangle=-\langle x,Ax \rangle$, then we will get  $$\frac{d}{dt}\|x(t)\|^2=0,$$ which means $|x(t)|$ is constant. So the condition $\langle Ax,x \rangle=-\langle x,Ax \rangle$ makes $|x(t)|$ constant for every $x(t)$ solution. Can we find a weaker property on $A$ which will make $|x(t)|$ constant only for the bounded solutions on $\mathbb{R}$? Is there any reference which deals with these problems ? Note: I am not looking for evident conditions like: The only bounded solution is $0$.",,"['functional-analysis', 'ordinary-differential-equations', 'reference-request', 'operator-theory', 'hilbert-spaces']"
32,Is every unitary representation a direct sum of irreducible subprepresentations?,Is every unitary representation a direct sum of irreducible subprepresentations?,,"I've read that any unitary representation of a compact group decomposes as a Hilbert space direct sum of irreducible representations. In the book I'm reading this is stated as a prong of the Peter-Weyl theorem.  It seems to me that this should just be a straighforward application of Zorn's lemma and trans finite induction since the intersection of a nested sequence of closed invariant subspaces is again closed and invariant (edit: it just occurred to me that this intersection may be trivial).  This doesn't seem to use compactness of G. Is this right or am I missing something? If such an argument doesn't work, can I derive this fact for compact G from the other parts of the Peter Weyl theorem, namely that $L^2(G)$ is a Hilbert space direct sum of the irreducible unitary representations of G (which are finite dimensional)?","I've read that any unitary representation of a compact group decomposes as a Hilbert space direct sum of irreducible representations. In the book I'm reading this is stated as a prong of the Peter-Weyl theorem.  It seems to me that this should just be a straighforward application of Zorn's lemma and trans finite induction since the intersection of a nested sequence of closed invariant subspaces is again closed and invariant (edit: it just occurred to me that this intersection may be trivial).  This doesn't seem to use compactness of G. Is this right or am I missing something? If such an argument doesn't work, can I derive this fact for compact G from the other parts of the Peter Weyl theorem, namely that $L^2(G)$ is a Hilbert space direct sum of the irreducible unitary representations of G (which are finite dimensional)?",,"['functional-analysis', 'representation-theory', 'harmonic-analysis']"
33,Proof of a theorem about Baire categories,Proof of a theorem about Baire categories,,"Problem: prove that the set of $C([0, 1])$ functions whose derivative is defined at every point (and it is either finite or infinite) is of the first Baire category. I have no idea how to approach this and would be very grateful for any help.","Problem: prove that the set of $C([0, 1])$ functions whose derivative is defined at every point (and it is either finite or infinite) is of the first Baire category. I have no idea how to approach this and would be very grateful for any help.",,['functional-analysis']
34,Differentiable Path of Operators and their Inverses,Differentiable Path of Operators and their Inverses,,"Let $\mathcal{H}$ be a separable Hilbert space. Consider a differentiable map $\mathbb{R} \rightarrow \mathcal{B}(\mathcal{H}), t \mapsto A(t)$, where $\mathcal{B}(\mathcal{H})$ is the space of bounded linear operators from $\mathcal{H}$ into itself with respect to the operator norm (by differentiable I mean differentiable in the Frechet-sense). Furthermore, assume, for each $t$, that $A(t)$ has the following properties: (1) for each $t$ there is a orthogonal decomposition of $\mathcal{H} = \mathcal{H}_{t}^{1} \oplus \mathcal{H}_{t}^{2}$, where $\mathcal{H}_{t}^{1}$ is a finite dimensional subspace of $\mathcal{H}$; (2) $A(t)|_{\mathcal{H}_{t}^{1}} \equiv 0$, $A(t)$ are selfadjoint and $\dim{(\mathcal{H}_{t}^{1})} = n > 1$ (here $n$ is independent of $t$, hence for all $t$, $\mathcal{H}_{t}^{1}$ have the same dimension greater than $1$); (3) $A(t)|_{\mathcal{H}_{t}^{2}}:\mathcal{H}_{t}^{2} \rightarrow \mathcal{H}_{t}^{2}$ is an isomorphism; (4) The $(A(t)|_{\mathcal{H}_{t}^{2}})^{-1} : \mathcal{H}_{t}^{2} \rightarrow \mathcal{H}_{t}^{2}$ are bounded operators; (5) $(A(t)|_{\mathcal{H}_{t}^{2}})^{-1}A(t) : \mathcal{H} \rightarrow \mathcal{H}_{t}^{2}$ is an orthogonal projection. Extend now the oprators $(A(t)|_{\mathcal{H}_{t}^{2}})^{-1}$ to the whole $\mathcal{H}$ by: $A(t)^{-1}x = 0$ if $x \in \mathcal{H}_{t}^{1}$ and $A(t)^{-1}|_{\mathcal{H}_{t}^{2}} = (A(t)|_{\mathcal{H}_{t}^{2}})^{-1}$. Is it true that then the map $\mathbb{R} \rightarrow \mathcal{B}(\mathcal{H}), t \mapsto A(t)^{-1}$ is Frechet-differentiable? I'm sure I've mentioned too many assumptions. I know that if the operators $A(t)$ are invertible (on the whole space) then the map $t \mapsto A(t)^{-1}$ is Frechet-differentiable. But what about in this case? Clark","Let $\mathcal{H}$ be a separable Hilbert space. Consider a differentiable map $\mathbb{R} \rightarrow \mathcal{B}(\mathcal{H}), t \mapsto A(t)$, where $\mathcal{B}(\mathcal{H})$ is the space of bounded linear operators from $\mathcal{H}$ into itself with respect to the operator norm (by differentiable I mean differentiable in the Frechet-sense). Furthermore, assume, for each $t$, that $A(t)$ has the following properties: (1) for each $t$ there is a orthogonal decomposition of $\mathcal{H} = \mathcal{H}_{t}^{1} \oplus \mathcal{H}_{t}^{2}$, where $\mathcal{H}_{t}^{1}$ is a finite dimensional subspace of $\mathcal{H}$; (2) $A(t)|_{\mathcal{H}_{t}^{1}} \equiv 0$, $A(t)$ are selfadjoint and $\dim{(\mathcal{H}_{t}^{1})} = n > 1$ (here $n$ is independent of $t$, hence for all $t$, $\mathcal{H}_{t}^{1}$ have the same dimension greater than $1$); (3) $A(t)|_{\mathcal{H}_{t}^{2}}:\mathcal{H}_{t}^{2} \rightarrow \mathcal{H}_{t}^{2}$ is an isomorphism; (4) The $(A(t)|_{\mathcal{H}_{t}^{2}})^{-1} : \mathcal{H}_{t}^{2} \rightarrow \mathcal{H}_{t}^{2}$ are bounded operators; (5) $(A(t)|_{\mathcal{H}_{t}^{2}})^{-1}A(t) : \mathcal{H} \rightarrow \mathcal{H}_{t}^{2}$ is an orthogonal projection. Extend now the oprators $(A(t)|_{\mathcal{H}_{t}^{2}})^{-1}$ to the whole $\mathcal{H}$ by: $A(t)^{-1}x = 0$ if $x \in \mathcal{H}_{t}^{1}$ and $A(t)^{-1}|_{\mathcal{H}_{t}^{2}} = (A(t)|_{\mathcal{H}_{t}^{2}})^{-1}$. Is it true that then the map $\mathbb{R} \rightarrow \mathcal{B}(\mathcal{H}), t \mapsto A(t)^{-1}$ is Frechet-differentiable? I'm sure I've mentioned too many assumptions. I know that if the operators $A(t)$ are invertible (on the whole space) then the map $t \mapsto A(t)^{-1}$ is Frechet-differentiable. But what about in this case? Clark",,"['analysis', 'functional-analysis', 'derivatives', 'operator-theory']"
35,"Weak topology on $L^p,~p> 1$",Weak topology on,"L^p,~p> 1","How looks like the weak topology in the particular case  $X=L^p$, I mean, is possible to detail this topology beyond standar form: Arbitrary union of finite intersections  open pre-images of opens in the real line?","How looks like the weak topology in the particular case  $X=L^p$, I mean, is possible to detail this topology beyond standar form: Arbitrary union of finite intersections  open pre-images of opens in the real line?",,"['real-analysis', 'analysis', 'functional-analysis', 'reference-request']"
36,Does this characterize the operator norm of the inverse?,Does this characterize the operator norm of the inverse?,,"Let $A$ be an invertible operator (bounded with bounded inverse). Then $$\frac{1}{\|A^{-1}\|} = \inf\left\{\frac{\|Av\|}{\|v\|} : v \neq 0\right\}$$ I believe I have a proof as follows, but I just want to double check $$\begin{eqnarray*} \inf\left\{\frac{\|Av\|}{\|v\|} : v \neq 0\right\} &=& \inf\left\{\frac{\|u\|}{\|A^{-1}u\|} : u \neq 0\right\} \\ &=& 1 / \sup\{\left\{\frac{\|A^{-1} u\|}{\|u\|} : u \neq 0\right\} \\ &=& \frac{1}{\|A^{-1}\|}  \end{eqnarray*}$$ Is this correct?","Let $A$ be an invertible operator (bounded with bounded inverse). Then $$\frac{1}{\|A^{-1}\|} = \inf\left\{\frac{\|Av\|}{\|v\|} : v \neq 0\right\}$$ I believe I have a proof as follows, but I just want to double check $$\begin{eqnarray*} \inf\left\{\frac{\|Av\|}{\|v\|} : v \neq 0\right\} &=& \inf\left\{\frac{\|u\|}{\|A^{-1}u\|} : u \neq 0\right\} \\ &=& 1 / \sup\{\left\{\frac{\|A^{-1} u\|}{\|u\|} : u \neq 0\right\} \\ &=& \frac{1}{\|A^{-1}\|}  \end{eqnarray*}$$ Is this correct?",,"['linear-algebra', 'functional-analysis']"
37,The Gaussian Integral,The Gaussian Integral,,"Hi I am trying to calculate the expected value of  $$ \mathbb{E}\big[x_i x_j...x_N\big]=\int_{-\infty}^\infty x_ix_jx_k...x_N \exp\bigg({-\sum_{i,j=1}^N\frac{1}{2}x^\top_i A_{ij}x_j}-\sum_{i=1}^Nh_i x_i\bigg)\prod_{i=1}^Ndx_i, $$ note these are higher order correlation functions for a Gaussian generating functional.  Also the matrix $A_{ij}=A^\top_{ij}$ (real symmetric) and is also positive definite, thus the eigenvalues of $A_{ij}$ all satisfy $\lambda_i>0$.  Note the generating functional is given by $$ \mathcal{F}(h)=\int_{-\infty}^\infty \exp\bigg({-\sum_{i,j=1}^N\frac{1}{2}x^\top_i A_{ij}x_j}-\sum_{i=1}^Nh_i x_i\bigg)\prod_{i=1}^Ndx_i=\frac{(2\pi)^{N/2}}{\sqrt{\det A_{ij}}}\exp\big( \frac{1}{2}\sum_{i,j=1}^N h_i A^{-1}_{ij}h_j\big) $$ where I used $\det(A)=\prod_{i=1}^N \lambda_i$. We calculate this by finding the minimum of the quadratic form $$ \frac{\partial}{\partial x_k}\bigg(\sum_{i,j=1}^{N}	\frac{1}{2} x_i A_{ij} x_j-\sum_{i=1}^N  h_i x_i	\bigg)=\sum_{j=1}^{N}  A_{kj}x_j- h_k=0. $$ In order to solve this we need to introduce the inverse matrix of $A$ given by $A^{-1}$.  Thus we can write the solution as $$ x_i=\sum_{j=1}^{N} A^{-1}_{ij} h_j. $$ We can now make a change of variables $x_i \mapsto y_i$ to obtain $$ x_i=\sum_{j=1}^{N} K^{-1}_{ij}h_j+y_i. $$ Re-writing $\mathcal{F}(h)$ we obtain $$ \mathcal{F}(h)=\exp\bigg(\sum_{i,j=1}^{N} \frac{1}{2} h_i A^{-1}_{ij} h_j\bigg)\int_{-\infty}^\infty d^Ny \exp\bigg(-\sum_{i,j=1}^{N} \frac{1}{2} y_i A_{ij}y_j				\bigg). $$ This integral is now a simple gaussian which we diagonalize by an orthogonal transformation $A=O\lambda_i\delta_{ij}O^\top$ and a linear change of variables $x=Oy$.  The Jacobian of the transformation is unity since a rotation leaves the volume invariant.  We write the general result as \begin{equation} \mathcal{F}(h)=\big({2\pi}\big)^{N/2} (\det A_{ij})^{-1/2} \exp\left(\sum_{i,j=1}^{N} \frac{1}{2}  h_i A^{-1}_{ij} h_j\right). \end{equation} Having calculated this, I now need to calculate the expected value of the higher order moments which is what my question is. Note in 1 dimension, the expected value I am trying to calculate is similar to $$ \int_{-\infty}^\infty x^{n} e^{-x^2/2-\alpha x}dx,\quad \Re(n)>-1, \alpha\in \mathbb{R}. $$ I have found lower order expected values given by  $$ \big<x_i\big>=A^{-1}_{ij}h_j ,\quad \big<x_i x_j\big>=A^{-1}_{ik} h_k A^{-1}_{jl}h_l+A^{-1}_{ij}, $$ but am trying to generalize to higher orders.","Hi I am trying to calculate the expected value of  $$ \mathbb{E}\big[x_i x_j...x_N\big]=\int_{-\infty}^\infty x_ix_jx_k...x_N \exp\bigg({-\sum_{i,j=1}^N\frac{1}{2}x^\top_i A_{ij}x_j}-\sum_{i=1}^Nh_i x_i\bigg)\prod_{i=1}^Ndx_i, $$ note these are higher order correlation functions for a Gaussian generating functional.  Also the matrix $A_{ij}=A^\top_{ij}$ (real symmetric) and is also positive definite, thus the eigenvalues of $A_{ij}$ all satisfy $\lambda_i>0$.  Note the generating functional is given by $$ \mathcal{F}(h)=\int_{-\infty}^\infty \exp\bigg({-\sum_{i,j=1}^N\frac{1}{2}x^\top_i A_{ij}x_j}-\sum_{i=1}^Nh_i x_i\bigg)\prod_{i=1}^Ndx_i=\frac{(2\pi)^{N/2}}{\sqrt{\det A_{ij}}}\exp\big( \frac{1}{2}\sum_{i,j=1}^N h_i A^{-1}_{ij}h_j\big) $$ where I used $\det(A)=\prod_{i=1}^N \lambda_i$. We calculate this by finding the minimum of the quadratic form $$ \frac{\partial}{\partial x_k}\bigg(\sum_{i,j=1}^{N}	\frac{1}{2} x_i A_{ij} x_j-\sum_{i=1}^N  h_i x_i	\bigg)=\sum_{j=1}^{N}  A_{kj}x_j- h_k=0. $$ In order to solve this we need to introduce the inverse matrix of $A$ given by $A^{-1}$.  Thus we can write the solution as $$ x_i=\sum_{j=1}^{N} A^{-1}_{ij} h_j. $$ We can now make a change of variables $x_i \mapsto y_i$ to obtain $$ x_i=\sum_{j=1}^{N} K^{-1}_{ij}h_j+y_i. $$ Re-writing $\mathcal{F}(h)$ we obtain $$ \mathcal{F}(h)=\exp\bigg(\sum_{i,j=1}^{N} \frac{1}{2} h_i A^{-1}_{ij} h_j\bigg)\int_{-\infty}^\infty d^Ny \exp\bigg(-\sum_{i,j=1}^{N} \frac{1}{2} y_i A_{ij}y_j				\bigg). $$ This integral is now a simple gaussian which we diagonalize by an orthogonal transformation $A=O\lambda_i\delta_{ij}O^\top$ and a linear change of variables $x=Oy$.  The Jacobian of the transformation is unity since a rotation leaves the volume invariant.  We write the general result as \begin{equation} \mathcal{F}(h)=\big({2\pi}\big)^{N/2} (\det A_{ij})^{-1/2} \exp\left(\sum_{i,j=1}^{N} \frac{1}{2}  h_i A^{-1}_{ij} h_j\right). \end{equation} Having calculated this, I now need to calculate the expected value of the higher order moments which is what my question is. Note in 1 dimension, the expected value I am trying to calculate is similar to $$ \int_{-\infty}^\infty x^{n} e^{-x^2/2-\alpha x}dx,\quad \Re(n)>-1, \alpha\in \mathbb{R}. $$ I have found lower order expected values given by  $$ \big<x_i\big>=A^{-1}_{ij}h_j ,\quad \big<x_i x_j\big>=A^{-1}_{ik} h_k A^{-1}_{jl}h_l+A^{-1}_{ij}, $$ but am trying to generalize to higher orders.",,"['linear-algebra', 'integration', 'analysis', 'functional-analysis', 'mathematical-physics']"
38,Strongly convergence in L^2,Strongly convergence in L^2,,"Let the sequence $(f_n(x,u))$  such as, for all $n,$ $f_n$ is Caratheodory, and $|f_n|\leq g$ where $g \in L^1(\Omega)$ Let $u_n \in H^1_0$ such as it is strongly convergent to $u$ in $L^2$ and a.e $\Omega$. How we can use the Fatou lemma to prove that $f_n(x,u_n)u_n$ strongly converges in  $L^2$ to $f(x,u)u$ and a.e in $\Omega$? Thanks for the help.","Let the sequence $(f_n(x,u))$  such as, for all $n,$ $f_n$ is Caratheodory, and $|f_n|\leq g$ where $g \in L^1(\Omega)$ Let $u_n \in H^1_0$ such as it is strongly convergent to $u$ in $L^2$ and a.e $\Omega$. How we can use the Fatou lemma to prove that $f_n(x,u_n)u_n$ strongly converges in  $L^2$ to $f(x,u)u$ and a.e in $\Omega$? Thanks for the help.",,"['functional-analysis', 'measure-theory', 'convergence-divergence', 'lebesgue-measure']"
39,How to prove comparison principle for parabolic PDE (nonlinear),How to prove comparison principle for parabolic PDE (nonlinear),,"Suppose $F:\mathbb{R} \to \mathbb{R}$ is smooth with $F(x) > 0$ for $x > 0$ and $F:(0,\infty) \to (0,\infty)$ continuous and increasing with $F(0) = 0$. Consider the PDE  $$u_t = \Delta F(u) \quad \text{on $\Omega$}$$ $$u(x,0) = u_0$$ $$u(x,t) = C_1\quad \text{on $\partial \Omega$}$$ Suppose we have two weak solutions $u$ and $v$ in $L^2(0,T;H^1)\cap H^{-1}(0,T;H^{-1})$ with initial data $u_0$ and $v_0$ and boundary data $C_u$ and $C_v$ respectively, where $u_0 \leq v_0$ and $C_u \leq C_v$. How do I show that the comparison principle holds: that $u \leq v$? Assume more smoothness of the solutions if necessary. I can't do it. I know we need to test with $(u(t)-v(t))^+$ but I don't know what to do with the nonlinear term.","Suppose $F:\mathbb{R} \to \mathbb{R}$ is smooth with $F(x) > 0$ for $x > 0$ and $F:(0,\infty) \to (0,\infty)$ continuous and increasing with $F(0) = 0$. Consider the PDE  $$u_t = \Delta F(u) \quad \text{on $\Omega$}$$ $$u(x,0) = u_0$$ $$u(x,t) = C_1\quad \text{on $\partial \Omega$}$$ Suppose we have two weak solutions $u$ and $v$ in $L^2(0,T;H^1)\cap H^{-1}(0,T;H^{-1})$ with initial data $u_0$ and $v_0$ and boundary data $C_u$ and $C_v$ respectively, where $u_0 \leq v_0$ and $C_u \leq C_v$. How do I show that the comparison principle holds: that $u \leq v$? Assume more smoothness of the solutions if necessary. I can't do it. I know we need to test with $(u(t)-v(t))^+$ but I don't know what to do with the nonlinear term.",,"['functional-analysis', 'partial-differential-equations', 'sobolev-spaces', 'maximum-principle']"
40,Fractional Sobolev spaces and weighted L2 spaces,Fractional Sobolev spaces and weighted L2 spaces,,"For $s\in[0,1]$ define function spaces $H^s(\mathbb{R})=\{u\in L_2(\mathbb{R}): (1+|\cdot|^2)^{s/2}\mathcal{F}u\in L_2(\mathbb{R}) \}$ (where $\mathcal{F}$ denotes the Fourier transform) i.e. the space of Bessel potentials. $E^s(\mathbb{R})=\{u\in H^s(\mathbb{R}): |\cdot|^{-s}u\in L_2(\mathbb{R})\}$ Prove that $E^s(\mathbb{R})=H^s(\mathbb{R})$ for $s\in[0,1/2)$ $E^s(\mathbb{R})=H^s_0(\mathbb{R}):=\{u\in H^s(\mathbb{R}): u(0)=0\}$ for $s\in(1/2,1]$ (notice that $u(0)$ is well defined since $H^s(\mathbb{R})\subset C(\mathbb{R})$ for $s>1/2$). Thank You for any comments. It is not a homework. For the case $s\in[0,1/2)$ I have tried to use Sobolev embedding $H^s\subset L_q$ where $q=2/(1-2s)$ and Holder inequality. Then one gets $\int|x|^{-2s}|u(x)|^2\leq(\int|u(x)|^{2q/2})^{2/q}(\int|x|^{-2s(q/2)'})^{(2/q)'}$, where $1=1/(q/2)+1/(q/2)'$. Unfortunately $(q/2)'=(q/2)/(q/2-1)=q/(q-2)=1/(2s)$ so that on the right hand side we get $\int|x|^{-1}=\infty$.","For $s\in[0,1]$ define function spaces $H^s(\mathbb{R})=\{u\in L_2(\mathbb{R}): (1+|\cdot|^2)^{s/2}\mathcal{F}u\in L_2(\mathbb{R}) \}$ (where $\mathcal{F}$ denotes the Fourier transform) i.e. the space of Bessel potentials. $E^s(\mathbb{R})=\{u\in H^s(\mathbb{R}): |\cdot|^{-s}u\in L_2(\mathbb{R})\}$ Prove that $E^s(\mathbb{R})=H^s(\mathbb{R})$ for $s\in[0,1/2)$ $E^s(\mathbb{R})=H^s_0(\mathbb{R}):=\{u\in H^s(\mathbb{R}): u(0)=0\}$ for $s\in(1/2,1]$ (notice that $u(0)$ is well defined since $H^s(\mathbb{R})\subset C(\mathbb{R})$ for $s>1/2$). Thank You for any comments. It is not a homework. For the case $s\in[0,1/2)$ I have tried to use Sobolev embedding $H^s\subset L_q$ where $q=2/(1-2s)$ and Holder inequality. Then one gets $\int|x|^{-2s}|u(x)|^2\leq(\int|u(x)|^{2q/2})^{2/q}(\int|x|^{-2s(q/2)'})^{(2/q)'}$, where $1=1/(q/2)+1/(q/2)'$. Unfortunately $(q/2)'=(q/2)/(q/2-1)=q/(q-2)=1/(2s)$ so that on the right hand side we get $\int|x|^{-1}=\infty$.",,"['functional-analysis', 'partial-differential-equations']"
41,How to get a grip on codimensions,How to get a grip on codimensions,,"I am trying to find a proof for the following problem: Let $X,Y$ be Banach spaces  $A,B:X \rightarrow Y$ are bounded linear operators $Ran(A)$ is closed, and $\dim(\mathrm{Ker}(A))$ or $\dim(Y/\mathrm{Ran}(A))$ is finite (or at least one of them).  If $\left \| Bx\right \|<\left \| Ax\right \|$ for all $x \in X$  then $\dim(Y/\mathrm{Ran}(A+B))\le\dim(Y/\mathrm{Ran}(A))$ Can you give me some hints on how to get a grip on the codimensions for this? Thank you. Regarding my attempts:  It flows easily that $\mathrm{Ker}(A)=\mathrm{Ker}(A+B)$ hence we can assume the kernel is empty or else we factorize $X$ with it.  Than thanks to the closeness of $\mathrm{Ran}(A)$ there exists a continuous $A^{-1}$ on $\mathrm{Ran}(A)$, so we have a continuous bijection from $\mathrm{Ran}(A)$ to $\mathrm{Ran}(A+B)$. However this does not help if $\dim(Y)=\infty$ Also it is enough to consider the case where $\dim(Y/\mathrm{Ran}(A))$ is finite. In this case $Y/\mathrm{Ran}(A)\cong(Y/\mathrm{Ran}(A))^*\cong \mathrm{Ran}(A)^0 $ the last element is the annihilator. But this is just as hard to use as the cokernel in my opinion.","I am trying to find a proof for the following problem: Let $X,Y$ be Banach spaces  $A,B:X \rightarrow Y$ are bounded linear operators $Ran(A)$ is closed, and $\dim(\mathrm{Ker}(A))$ or $\dim(Y/\mathrm{Ran}(A))$ is finite (or at least one of them).  If $\left \| Bx\right \|<\left \| Ax\right \|$ for all $x \in X$  then $\dim(Y/\mathrm{Ran}(A+B))\le\dim(Y/\mathrm{Ran}(A))$ Can you give me some hints on how to get a grip on the codimensions for this? Thank you. Regarding my attempts:  It flows easily that $\mathrm{Ker}(A)=\mathrm{Ker}(A+B)$ hence we can assume the kernel is empty or else we factorize $X$ with it.  Than thanks to the closeness of $\mathrm{Ran}(A)$ there exists a continuous $A^{-1}$ on $\mathrm{Ran}(A)$, so we have a continuous bijection from $\mathrm{Ran}(A)$ to $\mathrm{Ran}(A+B)$. However this does not help if $\dim(Y)=\infty$ Also it is enough to consider the case where $\dim(Y/\mathrm{Ran}(A))$ is finite. In this case $Y/\mathrm{Ran}(A)\cong(Y/\mathrm{Ran}(A))^*\cong \mathrm{Ran}(A)^0 $ the last element is the annihilator. But this is just as hard to use as the cokernel in my opinion.",,"['functional-analysis', 'operator-theory', 'banach-spaces']"
42,On the weak convergence in reflexive Banach space,On the weak convergence in reflexive Banach space,,"Consider the following proposition: Proposition 1. Let $X$ be a reflexive Banach space and suppose that $\{x_n\}$ is a sequence in $X$ that is bounded and has at most one weakly sequentially cluster point $x$. Then $\{x_n\}$ converges weakly to $x$. We have known that Proposition 1 is a direct corollary of the following theorems and proposition: Theorem 1. (Kakutani) Let $X$ be a Banach space. Then $X$ is reflexive if and only if $$ B_X=\{x\in X: \|x\|\leq 1\} $$  is compact in the weak topology $\sigma(X, X^*)$. Theorem 2. (Eberline-Smulian) Let $X$ be a Banach space. Then the following statements are equivalent: (i) $B_X$ is weakly compact; (ii) $B_X$ is sequentially weakly compact. Proposition 2. Let $C$ be a sequentially compact subset of a Hausdorff space $X$ and suppose that $\{x_n\}$ is a sequence in $C$ that admits a unique sequential cluster point $x$. Then $\{x_n\}$ converges to $x$. I woulk like to ask where we can find the reference of Proposition 1 . Thank you for all comments and helping.","Consider the following proposition: Proposition 1. Let $X$ be a reflexive Banach space and suppose that $\{x_n\}$ is a sequence in $X$ that is bounded and has at most one weakly sequentially cluster point $x$. Then $\{x_n\}$ converges weakly to $x$. We have known that Proposition 1 is a direct corollary of the following theorems and proposition: Theorem 1. (Kakutani) Let $X$ be a Banach space. Then $X$ is reflexive if and only if $$ B_X=\{x\in X: \|x\|\leq 1\} $$  is compact in the weak topology $\sigma(X, X^*)$. Theorem 2. (Eberline-Smulian) Let $X$ be a Banach space. Then the following statements are equivalent: (i) $B_X$ is weakly compact; (ii) $B_X$ is sequentially weakly compact. Proposition 2. Let $C$ be a sequentially compact subset of a Hausdorff space $X$ and suppose that $\{x_n\}$ is a sequence in $C$ that admits a unique sequential cluster point $x$. Then $\{x_n\}$ converges to $x$. I woulk like to ask where we can find the reference of Proposition 1 . Thank you for all comments and helping.",,"['functional-analysis', 'reference-request', 'banach-spaces', 'weak-convergence']"
43,Poincaré inequality for a subspace of $H^2(\Omega)$,Poincaré inequality for a subspace of,H^2(\Omega),"Suppose that $\Omega\subset\mathbb{R}^d$ is a smooth, bounded, and connected domain. Let \begin{equation} H=\{u\in H^2(\Omega):\int_\Omega u(x) dx=0 ~\text{and}~ \nabla u\cdot v=0~ \text{on}~\partial\Omega\}. \end{equation} Show that $H$ is a Hilbert space, and prove that there exists $C>0$ such that for any $u\in H$,  \begin{equation} ||u||_{H^1(\Omega)}\le C\sum_{|\alpha|=2} ||D^\alpha u||_{L^2(\Omega)}. \end{equation} I can prove the space is a Hilbert space. How do I put my hands on the inequality?","Suppose that $\Omega\subset\mathbb{R}^d$ is a smooth, bounded, and connected domain. Let \begin{equation} H=\{u\in H^2(\Omega):\int_\Omega u(x) dx=0 ~\text{and}~ \nabla u\cdot v=0~ \text{on}~\partial\Omega\}. \end{equation} Show that $H$ is a Hilbert space, and prove that there exists $C>0$ such that for any $u\in H$,  \begin{equation} ||u||_{H^1(\Omega)}\le C\sum_{|\alpha|=2} ||D^\alpha u||_{L^2(\Omega)}. \end{equation} I can prove the space is a Hilbert space. How do I put my hands on the inequality?",,"['functional-analysis', 'inequality', 'hilbert-spaces', 'sobolev-spaces']"
44,Why $ \|x^* x \| = \|x\|\|x^*\|$ is equivalent to $\|xx^*\| = \|x\|^2$ in the definition of $C^*$ algebra?,Why  is equivalent to  in the definition of  algebra?, \|x^* x \| = \|x\|\|x^*\| \|xx^*\| = \|x\|^2 C^*,I read the definition of $C^*$ algebra in Wikipedia where it says $\|x^* x \| = \|x\|\|x^*\|$ is equivalent to $\|xx^*\| = \|x\|^2$ but I do not know why. Can you show me how to derive $\|xx^*\| = \|x\|^2$ from $\|x^* x \| = \|x\|\|x^*\|$?,I read the definition of $C^*$ algebra in Wikipedia where it says $\|x^* x \| = \|x\|\|x^*\|$ is equivalent to $\|xx^*\| = \|x\|^2$ but I do not know why. Can you show me how to derive $\|xx^*\| = \|x\|^2$ from $\|x^* x \| = \|x\|\|x^*\|$?,,"['functional-analysis', 'operator-theory', 'operator-algebras', 'c-star-algebras']"
45,Riesz Fischer theorem?,Riesz Fischer theorem?,,"I was wondering about the following: I read that Fischer-Riesz says that $L^2([0,1])$ is isomorphic to $l^2(\mathbb{N})$. Now it is obvious, that this should not depent on the fact which compact subset $K$ you choose in $L^2(K)$, but my question is: Is $L^2(\mathbb{R}^n)$ also isomorphic to $l^2(\mathbb{N})$?","I was wondering about the following: I read that Fischer-Riesz says that $L^2([0,1])$ is isomorphic to $l^2(\mathbb{N})$. Now it is obvious, that this should not depent on the fact which compact subset $K$ you choose in $L^2(K)$, but my question is: Is $L^2(\mathbb{R}^n)$ also isomorphic to $l^2(\mathbb{N})$?",,['calculus']
46,"$f_n \rightharpoonup f$ in $L^q(Q)$ $\forall q < \infty$ and $f_n' \rightharpoonup f'$ in $L^2(0,T;H^{-1})$ implies $f_n \to f$",in   and  in  implies,"f_n \rightharpoonup f L^q(Q) \forall q < \infty f_n' \rightharpoonup f' L^2(0,T;H^{-1}) f_n \to f","(... in $C^0([0,T]; H^{-1})$. ) Let $f_n$ be a sequence of functions defined on $Q:=(0,T)\times \Omega$, where $\Omega$ is a bounded domain. I have read this: Since $f_n \rightharpoonup f$ in $L^q(Q)$ for every $q < \infty$ and $f_n' \rightharpoonup f'$ in $L^2(0,T;H^{-1}(\Omega))$, using Aubin's lemma we deduce that $f_n \to f$ in $C^0([0,T]; H^{-1}(\Omega))$. Can I get a reference to this lemma? It isn't the Lions-Aubin result I don't think. Source is this paper on page 426.","(... in $C^0([0,T]; H^{-1})$. ) Let $f_n$ be a sequence of functions defined on $Q:=(0,T)\times \Omega$, where $\Omega$ is a bounded domain. I have read this: Since $f_n \rightharpoonup f$ in $L^q(Q)$ for every $q < \infty$ and $f_n' \rightharpoonup f'$ in $L^2(0,T;H^{-1}(\Omega))$, using Aubin's lemma we deduce that $f_n \to f$ in $C^0([0,T]; H^{-1}(\Omega))$. Can I get a reference to this lemma? It isn't the Lions-Aubin result I don't think. Source is this paper on page 426.",,"['functional-analysis', 'reference-request', 'partial-differential-equations', 'sobolev-spaces', 'bochner-spaces']"
47,Spectral decomposition of $TT^*$,Spectral decomposition of,TT^*,"On $l_{2}$ let $T$ be given by $Te_{n}=\frac{e_{n+1}}{n+1}$ where $(e_{n})_{n\ge1}$ is the canonical orthonormal basis. Find the spectral decomposition of $TT^*$. I find that $T^*(e_{n})=\frac{e_{n-1}}{n}$ for $n$ greater than 1 and $T^*(e_{1})=0$ I find that the $e_{n}$ are the orthonormal eigenvectors for $TT^*$ with corresponding eigenvalues $\frac{1}{n^2}$ for $n\ge2$ and $0$ for $n=1$ so that the spectral decomposition is $TT^*x=\sum_{i=2}^{{\infty}}\frac{1}{n^2}\langle x,e_{n}\rangle e_{n}$. Do you agree and if so, are there any which are a bit harder? Thanks","On $l_{2}$ let $T$ be given by $Te_{n}=\frac{e_{n+1}}{n+1}$ where $(e_{n})_{n\ge1}$ is the canonical orthonormal basis. Find the spectral decomposition of $TT^*$. I find that $T^*(e_{n})=\frac{e_{n-1}}{n}$ for $n$ greater than 1 and $T^*(e_{1})=0$ I find that the $e_{n}$ are the orthonormal eigenvectors for $TT^*$ with corresponding eigenvalues $\frac{1}{n^2}$ for $n\ge2$ and $0$ for $n=1$ so that the spectral decomposition is $TT^*x=\sum_{i=2}^{{\infty}}\frac{1}{n^2}\langle x,e_{n}\rangle e_{n}$. Do you agree and if so, are there any which are a bit harder? Thanks",,"['linear-algebra', 'functional-analysis']"
48,Interpolation inequality on Holder space [duplicate],Interpolation inequality on Holder space [duplicate],,"This question already has answers here : Prove an interpolation inequality (2 answers) Closed 7 years ago . Let $0< \beta < \gamma <1$. Show that the interpolation inequality holds. $$||U||_{C^{0,\gamma}(U)} \le ||U||^{\frac{1-\gamma}{1-\beta}}_{C^{0,\beta}(U)} ||U||^{\frac{\gamma-\beta}{1-\beta}}_{C^{0,1}(U)}.$$","This question already has answers here : Prove an interpolation inequality (2 answers) Closed 7 years ago . Let $0< \beta < \gamma <1$. Show that the interpolation inequality holds. $$||U||_{C^{0,\gamma}(U)} \le ||U||^{\frac{1-\gamma}{1-\beta}}_{C^{0,\beta}(U)} ||U||^{\frac{\gamma-\beta}{1-\beta}}_{C^{0,1}(U)}.$$",,"['real-analysis', 'functional-analysis', 'interpolation', 'holder-spaces']"
49,"A regularity result for a parabolic PDE? Want $u' \in L^\infty((0,T)\times \Omega)$",A regularity result for a parabolic PDE? Want,"u' \in L^\infty((0,T)\times \Omega)","Let $f \in L^\infty((0,T)\times \Omega)$ and let $g \in L^\infty((0,T)\times \Omega)$ satisfy  $$0 < a \leq g(x,t) \leq b\quad\text{for all $(x,t)$}$$ $$\frac{dg}{dt} \in L^\infty((0,T)\times \Omega).$$ We know that there exists a function $u \in L^2(0,T;H^1)\cap H^1(0,T;H^{-1}(\Omega))$ such that $$\int_0^T \int_\Omega u'(t) \varphi(t) + \int_0^T \int_\Omega g(t)\nabla u(t) \nabla \varphi(t) = \int_0^T\int_\Omega f(t)\varphi(t)$$ for all $\varphi \in L^2(0,T;H^1(\Omega))$ and $u$ satisfies $u(0) = u_0$ for a given $u_0 \in L^2(\Omega)$. My question is, if we know that $u_0 \in L^\infty(\Omega)$, does it follow that $u' \in L^\infty((0,T) \times \Omega)$? I.e. do we have the expected regularity of $u$? We expect this because $f$ is appropriately regular and as is $u_0$ so we expect the same for $u'$. If so can anyone refer me to a citation for this result? Thank you. If not, what additional smoothness do I need? Let us assume $\Omega$ is as nice as required.","Let $f \in L^\infty((0,T)\times \Omega)$ and let $g \in L^\infty((0,T)\times \Omega)$ satisfy  $$0 < a \leq g(x,t) \leq b\quad\text{for all $(x,t)$}$$ $$\frac{dg}{dt} \in L^\infty((0,T)\times \Omega).$$ We know that there exists a function $u \in L^2(0,T;H^1)\cap H^1(0,T;H^{-1}(\Omega))$ such that $$\int_0^T \int_\Omega u'(t) \varphi(t) + \int_0^T \int_\Omega g(t)\nabla u(t) \nabla \varphi(t) = \int_0^T\int_\Omega f(t)\varphi(t)$$ for all $\varphi \in L^2(0,T;H^1(\Omega))$ and $u$ satisfies $u(0) = u_0$ for a given $u_0 \in L^2(\Omega)$. My question is, if we know that $u_0 \in L^\infty(\Omega)$, does it follow that $u' \in L^\infty((0,T) \times \Omega)$? I.e. do we have the expected regularity of $u$? We expect this because $f$ is appropriately regular and as is $u_0$ so we expect the same for $u'$. If so can anyone refer me to a citation for this result? Thank you. If not, what additional smoothness do I need? Let us assume $\Omega$ is as nice as required.",,"['functional-analysis', 'reference-request', 'partial-differential-equations', 'sobolev-spaces', 'bochner-spaces']"
50,Exponential of the derivative operator on the Schwartz space?,Exponential of the derivative operator on the Schwartz space?,,"We consider the derivative operator $\mathrm{D}$ on the space of smooth and rapidly decreasing function $\mathcal{S}$. We denote by $P_n = \frac{1}{0!} + \frac{X}{1!} + \frac{X^2}{2!} + \cdots + \frac{X^n}{n!}$ and $\mathrm{T}_n = P_n(\mathrm{D})$. My question is the following. Can we show that the sequence of operators $(\mathrm{T}_n)$ converges to some operator $\mathrm{T}$ from $\mathcal{S}$ to itself (that we could then call $\mathrm{T}= \exp(\mathrm{D})$)? If so, can we easily extend that operator over $\mathcal{S}'$? I suspect, if the result is true, that we can prove it showing that the sequence $(\mathrm{T}_n)$ is a Cauchy sequence in the complete topological vector space $\mathcal{L}(\mathcal{S})$ of linear and continuous operator from $\mathcal{S}$ to $\mathcal{S}$. The topology of the space $\mathcal{S}$ is complete for the usual family of semi-norms $$\lVert f \lVert_{n,m} = \sup_{x\in \mathbb{R}} | (1+|r|^m) D^{(n)} f(r) |.$$ Thanks for your attention.","We consider the derivative operator $\mathrm{D}$ on the space of smooth and rapidly decreasing function $\mathcal{S}$. We denote by $P_n = \frac{1}{0!} + \frac{X}{1!} + \frac{X^2}{2!} + \cdots + \frac{X^n}{n!}$ and $\mathrm{T}_n = P_n(\mathrm{D})$. My question is the following. Can we show that the sequence of operators $(\mathrm{T}_n)$ converges to some operator $\mathrm{T}$ from $\mathcal{S}$ to itself (that we could then call $\mathrm{T}= \exp(\mathrm{D})$)? If so, can we easily extend that operator over $\mathcal{S}'$? I suspect, if the result is true, that we can prove it showing that the sequence $(\mathrm{T}_n)$ is a Cauchy sequence in the complete topological vector space $\mathcal{L}(\mathcal{S})$ of linear and continuous operator from $\mathcal{S}$ to $\mathcal{S}$. The topology of the space $\mathcal{S}$ is complete for the usual family of semi-norms $$\lVert f \lVert_{n,m} = \sup_{x\in \mathbb{R}} | (1+|r|^m) D^{(n)} f(r) |.$$ Thanks for your attention.",,"['functional-analysis', 'distribution-theory', 'schwartz-space']"
51,How do they call the topological tensor product that classifies operators from Hilbert space?,How do they call the topological tensor product that classifies operators from Hilbert space?,,"Let $V$ and $W$ be topological vector spaces. There are different ways to complete the tensor product $V \otimes W$, and the only ones that are usually discussed in introductory literature are the injective and projective tensor products of locally convex spaces. The tensor product defined below is very different, and I'd like to know how it is called and where to find information about it. Let $H$ be a Hilbert space, $\varphi: H \to V$ and $\psi: H \to W$ be continuous linear operators, and let $\{e_\alpha\}$ and $\{f_\beta\}$ be two orthonormal bases in $H$. Then formally the following identity should be true: $$ \sum_\alpha \varphi(e_\alpha) \otimes \psi(e_\alpha) = \sum_\beta \varphi(f_\beta) \otimes \psi(f_\beta) $$ The tensor product that I'm talking about is the space of all formal sums $ \sum_\alpha v_\alpha \otimes w_\alpha $, $v_\alpha \in V, w_\alpha \in W$, for which $\varphi(e_\alpha) := v_\alpha, \psi(e_\alpha) := w_\alpha$ extends to continuous operators from Hilbert space, modulo the identity above (and, of course, the obvious relation $\sum_\alpha v_\alpha \otimes 0 + \sum_\alpha 0 \otimes w_\alpha = 0$). The importance of this space stems from the fact that the positive elements of $V \otimes V$ (the ones representable as $\sum v_\alpha \otimes v_\alpha$) are essentially unitary equivalence classes of maps from Hilbert space to $V$, so the topic looks like it should have been studied extensively 50 years ago. What I know so far are just explicit descriptions of this space in several cases ($\ell^2 \otimes \ell^2$, $C \otimes C$ (RKHS theory), $\ell^1 \otimes \ell^1$ (Grothendieck), $L^0 \otimes L^0$, nuclear spaces). What should I google to find a general theory?","Let $V$ and $W$ be topological vector spaces. There are different ways to complete the tensor product $V \otimes W$, and the only ones that are usually discussed in introductory literature are the injective and projective tensor products of locally convex spaces. The tensor product defined below is very different, and I'd like to know how it is called and where to find information about it. Let $H$ be a Hilbert space, $\varphi: H \to V$ and $\psi: H \to W$ be continuous linear operators, and let $\{e_\alpha\}$ and $\{f_\beta\}$ be two orthonormal bases in $H$. Then formally the following identity should be true: $$ \sum_\alpha \varphi(e_\alpha) \otimes \psi(e_\alpha) = \sum_\beta \varphi(f_\beta) \otimes \psi(f_\beta) $$ The tensor product that I'm talking about is the space of all formal sums $ \sum_\alpha v_\alpha \otimes w_\alpha $, $v_\alpha \in V, w_\alpha \in W$, for which $\varphi(e_\alpha) := v_\alpha, \psi(e_\alpha) := w_\alpha$ extends to continuous operators from Hilbert space, modulo the identity above (and, of course, the obvious relation $\sum_\alpha v_\alpha \otimes 0 + \sum_\alpha 0 \otimes w_\alpha = 0$). The importance of this space stems from the fact that the positive elements of $V \otimes V$ (the ones representable as $\sum v_\alpha \otimes v_\alpha$) are essentially unitary equivalence classes of maps from Hilbert space to $V$, so the topic looks like it should have been studied extensively 50 years ago. What I know so far are just explicit descriptions of this space in several cases ($\ell^2 \otimes \ell^2$, $C \otimes C$ (RKHS theory), $\ell^1 \otimes \ell^1$ (Grothendieck), $L^0 \otimes L^0$, nuclear spaces). What should I google to find a general theory?",,"['functional-analysis', 'reference-request', 'tensor-products', 'topological-vector-spaces']"
52,Is the tensor product $L^\infty(X)\times L^\infty(Y)$ dense in $L^\infty(X\times Y)$?,Is the tensor product  dense in ?,L^\infty(X)\times L^\infty(Y) L^\infty(X\times Y),"Is tensor product $L^\infty(X)\times L^\infty(Y)$ dense in $L^\infty(X\times Y)$ where $X, Y\subset \mathbb R^n$? and does any body know an example of a function $\psi \in L^\infty(X)\times L^\infty(Y)$ such that it can not be written as a series $\psi=\sum_ix_i(t)y_i(s)$ where $x_i(t)\in L^\infty(X)$ and $y_i\in L^\infty(Y)$. I thank you in advance.","Is tensor product $L^\infty(X)\times L^\infty(Y)$ dense in $L^\infty(X\times Y)$ where $X, Y\subset \mathbb R^n$? and does any body know an example of a function $\psi \in L^\infty(X)\times L^\infty(Y)$ such that it can not be written as a series $\psi=\sum_ix_i(t)y_i(s)$ where $x_i(t)\in L^\infty(X)$ and $y_i\in L^\infty(Y)$. I thank you in advance.",,"['functional-analysis', 'tensor-products', 'lp-spaces']"
53,Simultaneous Orthogonal basis for $L^2(\mathbb{R}^n)$ and $H^1(\mathbb{R}^n)$,Simultaneous Orthogonal basis for  and,L^2(\mathbb{R}^n) H^1(\mathbb{R}^n),"Given a smooth bounded set $U\subset \mathbb{R}^n$, there is a simultaneous orthogonal basis for  $L^2(U)$ and $H^1_0(U)$ by the existence of eigenvectors to the Laplacian in a bounded domain, which particularly requires boundedness for compactness of the solution operator of the corresponding elliptic problem. Is it possible to construct a simultaneous orthogonal basis for $L^2(\mathbb{R}^n)$ and $H^1(\mathbb{R}^n)$ as well? I thought it might be possible to use a basis for $L^2(U)$ where U is a cube and then by translations and dilations construct an orthogonal basis for  $L^2(\mathbb{R}^n)$. I do not know if that will also be orthogonal basis for  $H^1(\mathbb{R}^n)$ or if there will be some edge effects creating trouble. I wanted to know because I was reading the existence of solutions to wave equations as given in Evans's book on Partial Differential Equations using the Galerkin Method and at one point it requires this simultaneous basis for $L^2(U)$ and $H^1_0(U)$, which is available only for bounded smooth domains $U$ and I wonder if that proof could be extended for existence in $[0,T]\times \mathbb{R}^n$. Any help would be most welcome. Possible Solution : Looking at the answer to this question , I split $\mathbb{R}^n$ into the integer lattice $U_k:=U+k$ for $k\in\mathbb{Z}^n$, and where $U$ is the unit cube. For $L^2(U_k)$, there is an orthonormal basis $\{e_l^k; k\in\mathbb{Z}^n, l\in\mathbb{Z}\}$ which are also eigenvectors of the Laplacian $-\Delta$, and therefore, it also forms an orthogonal basis for $H_0^1(U_k)$. Now, we may extend each $e_n^k$ outside $U_k$ by $0$ so that it belongs to $H^1(\mathbb{R}^n)$. These $\{e_l^k; k\in\mathbb{Z}^n, l\in\mathbb{Z}\}$ form an orthonormal basis for $L^2(\mathbb{R}^n)$ and also an orthogonal basis for $H^1(\mathbb{R}^n)$. Could someone confirm if this reasoning is correct or am I missing out some issue?","Given a smooth bounded set $U\subset \mathbb{R}^n$, there is a simultaneous orthogonal basis for  $L^2(U)$ and $H^1_0(U)$ by the existence of eigenvectors to the Laplacian in a bounded domain, which particularly requires boundedness for compactness of the solution operator of the corresponding elliptic problem. Is it possible to construct a simultaneous orthogonal basis for $L^2(\mathbb{R}^n)$ and $H^1(\mathbb{R}^n)$ as well? I thought it might be possible to use a basis for $L^2(U)$ where U is a cube and then by translations and dilations construct an orthogonal basis for  $L^2(\mathbb{R}^n)$. I do not know if that will also be orthogonal basis for  $H^1(\mathbb{R}^n)$ or if there will be some edge effects creating trouble. I wanted to know because I was reading the existence of solutions to wave equations as given in Evans's book on Partial Differential Equations using the Galerkin Method and at one point it requires this simultaneous basis for $L^2(U)$ and $H^1_0(U)$, which is available only for bounded smooth domains $U$ and I wonder if that proof could be extended for existence in $[0,T]\times \mathbb{R}^n$. Any help would be most welcome. Possible Solution : Looking at the answer to this question , I split $\mathbb{R}^n$ into the integer lattice $U_k:=U+k$ for $k\in\mathbb{Z}^n$, and where $U$ is the unit cube. For $L^2(U_k)$, there is an orthonormal basis $\{e_l^k; k\in\mathbb{Z}^n, l\in\mathbb{Z}\}$ which are also eigenvectors of the Laplacian $-\Delta$, and therefore, it also forms an orthogonal basis for $H_0^1(U_k)$. Now, we may extend each $e_n^k$ outside $U_k$ by $0$ so that it belongs to $H^1(\mathbb{R}^n)$. These $\{e_l^k; k\in\mathbb{Z}^n, l\in\mathbb{Z}\}$ form an orthonormal basis for $L^2(\mathbb{R}^n)$ and also an orthogonal basis for $H^1(\mathbb{R}^n)$. Could someone confirm if this reasoning is correct or am I missing out some issue?",,['functional-analysis']
54,"Vector-Lattices and ""Approximating $\mathscr{L^1(\mathbb{R}^k)}$"".","Vector-Lattices and ""Approximating "".",\mathscr{L^1(\mathbb{R}^k)},"In this question I asked whether $\mathscr{L}^1(\mathbb{R}^k)$ forms a category in any way. It was concluded that indeed it does not. I thought to myself, ""well, could we at least approximate the space with a category?"" I couldn't see any obvious reason why not. It then occurred to me to ask, ""what properties of $\mathscr{L}^1(\mathbb{R}^k)$ might I want to preserve if I try to approximate $\mathscr{L}^1(\mathbb{R}^k)$ with a category?"" So I had a look around and stumbled on vector-lattices (which are defined here ). They appear to have many of the properties I was interested in in the question I provided a link to above. So: Do vector-lattices form a category $\mathcal{C}$? Is $\mathscr{L^1(\mathbb{R}^k)}$ a vector-lattice? I believe so (in both cases, considering them as objects with structure-preserving linear transformations as morphisms) but I don't think I'm up to proving it. Please help. If the answer to the first question is ""yes,"" but the second, ""no,"" then does $\mathcal{C}$ contain an object that is approximately $\mathscr{L^1(\mathbb{R}^k)}$? What do I mean by ""approximately $\mathscr{L^1(\mathbb{R}^k)}$""? Okay, I'm not sure what I mean, but I guess ""such that enough of the salient features of $\mathscr{L^1(\mathbb{R}^k)}$ are reflected"" is about right. How close can we get? This bit can be ignored. Furthermore, - and to provide some extra motivation for these questions - there seems to be a heavy use of characteristic functions in the theory of Lebesgue Integration, which, to me, calls to mind topoi. I'm not sure why. So, yeah, wouldn't it be cool if we could find a sub-category of the category of vector-lattices (let's say over $\mathbb{R}^k$) that's also a topos, that has an object a bit like $\mathscr{L}^1(\mathbb{R}^k)$? Does such a thing exist? Nope: See the first comment! $\ddot\smile$ I find these questions interesting in their own right and I hope you agree :)","In this question I asked whether $\mathscr{L}^1(\mathbb{R}^k)$ forms a category in any way. It was concluded that indeed it does not. I thought to myself, ""well, could we at least approximate the space with a category?"" I couldn't see any obvious reason why not. It then occurred to me to ask, ""what properties of $\mathscr{L}^1(\mathbb{R}^k)$ might I want to preserve if I try to approximate $\mathscr{L}^1(\mathbb{R}^k)$ with a category?"" So I had a look around and stumbled on vector-lattices (which are defined here ). They appear to have many of the properties I was interested in in the question I provided a link to above. So: Do vector-lattices form a category $\mathcal{C}$? Is $\mathscr{L^1(\mathbb{R}^k)}$ a vector-lattice? I believe so (in both cases, considering them as objects with structure-preserving linear transformations as morphisms) but I don't think I'm up to proving it. Please help. If the answer to the first question is ""yes,"" but the second, ""no,"" then does $\mathcal{C}$ contain an object that is approximately $\mathscr{L^1(\mathbb{R}^k)}$? What do I mean by ""approximately $\mathscr{L^1(\mathbb{R}^k)}$""? Okay, I'm not sure what I mean, but I guess ""such that enough of the salient features of $\mathscr{L^1(\mathbb{R}^k)}$ are reflected"" is about right. How close can we get? This bit can be ignored. Furthermore, - and to provide some extra motivation for these questions - there seems to be a heavy use of characteristic functions in the theory of Lebesgue Integration, which, to me, calls to mind topoi. I'm not sure why. So, yeah, wouldn't it be cool if we could find a sub-category of the category of vector-lattices (let's say over $\mathbb{R}^k$) that's also a topos, that has an object a bit like $\mathscr{L}^1(\mathbb{R}^k)$? Does such a thing exist? Nope: See the first comment! $\ddot\smile$ I find these questions interesting in their own right and I hope you agree :)",,"['functional-analysis', 'soft-question', 'category-theory', 'lebesgue-integral', 'vector-lattices']"
55,Every closed separable subspace is complemented,Every closed separable subspace is complemented,,"Let $X$ be a Banach space. Suppose that every closed separable subspace $Y$ of $X$ is complemented in $X$ (i.e., there is a bounded linear projection of $X$ onto $Y$). Is $X$ necessarilly isomorphic to a Hilbert space? Note: If $X$ is separable, or if every closed subspace $Y$ of $X$ (not necessarilly separable) is complemented in $X$, then the answer is positive, as proved by Lindenstrauss and Tzafriri (1977) (and pointed out by Harald Hanche-Olsen in the comment below).","Let $X$ be a Banach space. Suppose that every closed separable subspace $Y$ of $X$ is complemented in $X$ (i.e., there is a bounded linear projection of $X$ onto $Y$). Is $X$ necessarilly isomorphic to a Hilbert space? Note: If $X$ is separable, or if every closed subspace $Y$ of $X$ (not necessarilly separable) is complemented in $X$, then the answer is positive, as proved by Lindenstrauss and Tzafriri (1977) (and pointed out by Harald Hanche-Olsen in the comment below).",,['functional-analysis']
56,operator on separable banach space whose spectrum and point spectrum is prescribed compact set,operator on separable banach space whose spectrum and point spectrum is prescribed compact set,,"I am interested in obtaining the following paper: G. K. Kalisch, ""On operators with large point spectrum,"" Scripta Math. 29 No. 3-4, (1973), 371-378. According to Ben Mathes, ""Strictly Cyclic Algebras with Arbitrary Prescribed Gelfand Spectrum"" (2008), it contains the following result.  Let $X=L_2([0,1]\times[0,1])$ and let $A\subset\mathbb{C}$ be any compact set.  Then there exists an operator $T\in\mathcal{L}(Y)$ satisfying $\sigma(T)=\sigma_p(T)=A$, i.e. where both the spectrum and the point spectrum of $T$ are equal to $A$, and $Y$ is a closed subspace of $X$. Actually, what I really need to do is find/construct an operator $T$ on a separable Banach space $X$ satisfying $\sigma(T)=\sigma_p(T)=$ the unit circle.  Kalisch's operator will do it, but then I'd need to verify that it really is what Mathes says it is.  But it doesn't have to be Kalisch's operator.  As long as $X$ is separable and $\sigma(T)=\sigma_p(T)=$ the unit circle, that is enough. Thanks!","I am interested in obtaining the following paper: G. K. Kalisch, ""On operators with large point spectrum,"" Scripta Math. 29 No. 3-4, (1973), 371-378. According to Ben Mathes, ""Strictly Cyclic Algebras with Arbitrary Prescribed Gelfand Spectrum"" (2008), it contains the following result.  Let $X=L_2([0,1]\times[0,1])$ and let $A\subset\mathbb{C}$ be any compact set.  Then there exists an operator $T\in\mathcal{L}(Y)$ satisfying $\sigma(T)=\sigma_p(T)=A$, i.e. where both the spectrum and the point spectrum of $T$ are equal to $A$, and $Y$ is a closed subspace of $X$. Actually, what I really need to do is find/construct an operator $T$ on a separable Banach space $X$ satisfying $\sigma(T)=\sigma_p(T)=$ the unit circle.  Kalisch's operator will do it, but then I'd need to verify that it really is what Mathes says it is.  But it doesn't have to be Kalisch's operator.  As long as $X$ is separable and $\sigma(T)=\sigma_p(T)=$ the unit circle, that is enough. Thanks!",,"['functional-analysis', 'reference-request', 'operator-theory', 'banach-spaces', 'spectral-theory']"
57,Would the transformation of a differential equation obey the same algebra?,Would the transformation of a differential equation obey the same algebra?,,"I've found that the algebra of this differential equation $$\frac{d^2y}{dz^2}-(3z^2+\gamma)\frac{dy}{dz}+(cz+\alpha)y=0$$ is in $sl(2)$ because it is possible to use the generators of the $sl(2)$ group $$J^+ =z\frac{d}{dz}-2jz, \quad J^0=z\frac{d}{dz}-j, \quad J^-=\frac{d}{dz}$$ to recover the aforementioned differential equation, if write it as the following combination I get the operator (note:$j$ represents the spin of a particle, and $\gamma, c$ and $\alpha$ $\in \mathbb{R}$): $$a_{--}J^-J^- + b_{-}J^-+b_{+}J^+ = a_{--}\frac{d^2}{dz^2}+(b_{+}z^2+b_-)\frac{d}{dz}-b_+jz$$ where $a_{--},b_+, b_- \in \mathbb{R}$. I was wondering if I made a transformation of the original differential equation say, $y(z) = h(z)$  would be true to say that this differential equation also follows this algebra as well??","I've found that the algebra of this differential equation $$\frac{d^2y}{dz^2}-(3z^2+\gamma)\frac{dy}{dz}+(cz+\alpha)y=0$$ is in $sl(2)$ because it is possible to use the generators of the $sl(2)$ group $$J^+ =z\frac{d}{dz}-2jz, \quad J^0=z\frac{d}{dz}-j, \quad J^-=\frac{d}{dz}$$ to recover the aforementioned differential equation, if write it as the following combination I get the operator (note:$j$ represents the spin of a particle, and $\gamma, c$ and $\alpha$ $\in \mathbb{R}$): $$a_{--}J^-J^- + b_{-}J^-+b_{+}J^+ = a_{--}\frac{d^2}{dz^2}+(b_{+}z^2+b_-)\frac{d}{dz}-b_+jz$$ where $a_{--},b_+, b_- \in \mathbb{R}$. I was wondering if I made a transformation of the original differential equation say, $y(z) = h(z)$  would be true to say that this differential equation also follows this algebra as well??",,"['group-theory', 'functional-analysis', 'ordinary-differential-equations', 'lie-groups', 'lie-algebras']"
58,Ultraweak topology on Banach spaces,Ultraweak topology on Banach spaces,,"If $X$ and $Y$ are Banach spaces with $Y$ reflexive, then the space $\mathcal{B}(X,Y)$ of bounded operators from $X$ to $Y$ is the dual of the projective tensor product of $X$ and $Y^{*}$. As in the case where $X=Y=\mathcal{H}$ is a Hilbert space, this induces a weak*-topology on $\mathcal{B}(X,Y)$, which in the Hilbert space setting is called the ultraweak topology. In the Hilbert space setting, the finite rank operators lie dense in $\mathcal{B}(\mathcal{H})$ with respect to this topology. Does the same statement hold for $\mathcal{B}(X,Y)$? If not in general, what if we assume that $Y$ has an (unconditional) Schauder basis? Does anyone know whether this topology on $\mathcal{B}(X,Y)$ has been studied in the Banach space setting?","If $X$ and $Y$ are Banach spaces with $Y$ reflexive, then the space $\mathcal{B}(X,Y)$ of bounded operators from $X$ to $Y$ is the dual of the projective tensor product of $X$ and $Y^{*}$. As in the case where $X=Y=\mathcal{H}$ is a Hilbert space, this induces a weak*-topology on $\mathcal{B}(X,Y)$, which in the Hilbert space setting is called the ultraweak topology. In the Hilbert space setting, the finite rank operators lie dense in $\mathcal{B}(\mathcal{H})$ with respect to this topology. Does the same statement hold for $\mathcal{B}(X,Y)$? If not in general, what if we assume that $Y$ has an (unconditional) Schauder basis? Does anyone know whether this topology on $\mathcal{B}(X,Y)$ has been studied in the Banach space setting?",,"['general-topology', 'functional-analysis', 'banach-spaces', 'hilbert-spaces']"
59,Is $\{f\in H^1;\;\int f=0\}$ dense in $\{f\in L^2;\;\int f=0\}$?,Is  dense in ?,\{f\in H^1;\;\int f=0\} \{f\in L^2;\;\int f=0\},"Let $L_*^2=\left\{f\in L^2(a,b);\;\int_a^b f\;dx=0\right\}$ and $H_*^1=\left\{f\in H^1(a,b);\;\int_a^b f\;dx=0\right\}$, where  $-\infty<a<b<\infty$. Is $H_*^1$ dense in $(L_*^2,\|\cdot\|_{L^2})$? Thanks.","Let $L_*^2=\left\{f\in L^2(a,b);\;\int_a^b f\;dx=0\right\}$ and $H_*^1=\left\{f\in H^1(a,b);\;\int_a^b f\;dx=0\right\}$, where  $-\infty<a<b<\infty$. Is $H_*^1$ dense in $(L_*^2,\|\cdot\|_{L^2})$? Thanks.",,"['functional-analysis', 'sobolev-spaces', 'lp-spaces']"
60,Tricky weak-* convergence question,Tricky weak-* convergence question,,"Let $K$ be a compact (let's say, $K=[0,1]$ to be concrete) and let $\mu_n$ be a sequence of Radon measures converging weakly-* to another Radon measure $\mu$ on $K$. Let $h : K \rightarrow \mathbb{R}$ be a continuous function vanishing only a finite set, and assume that  $\mu_n/h$ and $\mu/h$ are all well-defined Radon measures (ie $1/h$ is integrable for all those measures). Is it true that $\mu_n/h$ converges weakly-* to $\mu/h$ ? It should be noted that in general for a non-continuous function $g$, weak-* convergence of $\mu_n$ to $\mu$ does not imply weak-* convergence of $g \mu_n$ to $g \mu$ when those measures are well-defined : it's easy to construct counter examples with diracs and functions vanishin everywhere but on a finite set. However I think in my setting it should be true, intuitively because the $\mu_n$ and $\mu$ could not charge the zeroes of $h$ too much, and because elsewhere $1/h$ is continuous. My idea for proving this was to take a continuous function $f$ and try to verify convergence of $\int f/h d\mu_n$ to $\int f/h d\mu$ by truncating $1/h$ near its singularities.  However I lack uniformity in $n$ to gain the convergence. Any help appreciated.","Let $K$ be a compact (let's say, $K=[0,1]$ to be concrete) and let $\mu_n$ be a sequence of Radon measures converging weakly-* to another Radon measure $\mu$ on $K$. Let $h : K \rightarrow \mathbb{R}$ be a continuous function vanishing only a finite set, and assume that  $\mu_n/h$ and $\mu/h$ are all well-defined Radon measures (ie $1/h$ is integrable for all those measures). Is it true that $\mu_n/h$ converges weakly-* to $\mu/h$ ? It should be noted that in general for a non-continuous function $g$, weak-* convergence of $\mu_n$ to $\mu$ does not imply weak-* convergence of $g \mu_n$ to $g \mu$ when those measures are well-defined : it's easy to construct counter examples with diracs and functions vanishin everywhere but on a finite set. However I think in my setting it should be true, intuitively because the $\mu_n$ and $\mu$ could not charge the zeroes of $h$ too much, and because elsewhere $1/h$ is continuous. My idea for proving this was to take a continuous function $f$ and try to verify convergence of $\int f/h d\mu_n$ to $\int f/h d\mu$ by truncating $1/h$ near its singularities.  However I lack uniformity in $n$ to gain the convergence. Any help appreciated.",,"['functional-analysis', 'measure-theory']"
61,The disk algebra and continuous homomorphisms,The disk algebra and continuous homomorphisms,,The disk algebra is the set of continuous functions $f: D \to \mathbb C$ where $D$ is the closed unit disc in $\mathbb C$ and $f$ is analytic on the interior of $D$. It is endowed with the $\sup$-norm. Let $A$ denote the disk algebra. I read that every continuous homomorphism $\varphi : A \to \mathbb C$ is of the form $f \mapsto f(z_0)$ for some $z_0 \in D$. The problem is I tried to look up the proof but I can't remember where I read it and I also can't find an alternative source. I also can't seem to prove it. I'm even starting to doubt the truth of the statement. How to prove this?,The disk algebra is the set of continuous functions $f: D \to \mathbb C$ where $D$ is the closed unit disc in $\mathbb C$ and $f$ is analytic on the interior of $D$. It is endowed with the $\sup$-norm. Let $A$ denote the disk algebra. I read that every continuous homomorphism $\varphi : A \to \mathbb C$ is of the form $f \mapsto f(z_0)$ for some $z_0 \in D$. The problem is I tried to look up the proof but I can't remember where I read it and I also can't find an alternative source. I also can't seem to prove it. I'm even starting to doubt the truth of the statement. How to prove this?,,"['functional-analysis', 'banach-algebras']"
62,Shift and ergodic measures,Shift and ergodic measures,,"Let $X = \{0,1\}^{\mathbb{N}}$ endowed with the product topology and $\sigma : \left\{ \begin{array}{ccc} X & \to & X \\ (\epsilon_n) & \mapsto & (\epsilon_{n+1}) \end{array} \right.$. It is known that $$\mathcal{M}_{\sigma}(X)= \{ \mu \in \mathcal{M}(X) \mid \sigma_* \mu=\mu \} \subset \mathcal{M}(X)= \{ \text{Borelian probability measure} \} \subset C(X,\mathbb{R})^*$$ where $\mathcal{M}_{\sigma}(X)$ and $\mathcal{M}(X)$ are both non-empty compact convex subspaces. Moreover, $\mu \in \mathcal{M}(X)$ is ergodic iff it is an extremal point of $\mathcal{M}_{\sigma}(X)$. Is there an elementary way to show that such extremal points are dense in $\mathcal{M}_{\sigma}(X)$?","Let $X = \{0,1\}^{\mathbb{N}}$ endowed with the product topology and $\sigma : \left\{ \begin{array}{ccc} X & \to & X \\ (\epsilon_n) & \mapsto & (\epsilon_{n+1}) \end{array} \right.$. It is known that $$\mathcal{M}_{\sigma}(X)= \{ \mu \in \mathcal{M}(X) \mid \sigma_* \mu=\mu \} \subset \mathcal{M}(X)= \{ \text{Borelian probability measure} \} \subset C(X,\mathbb{R})^*$$ where $\mathcal{M}_{\sigma}(X)$ and $\mathcal{M}(X)$ are both non-empty compact convex subspaces. Moreover, $\mu \in \mathcal{M}(X)$ is ergodic iff it is an extremal point of $\mathcal{M}_{\sigma}(X)$. Is there an elementary way to show that such extremal points are dense in $\mathcal{M}_{\sigma}(X)$?",,"['functional-analysis', 'dynamical-systems', 'ergodic-theory']"
63,why is test function space $\mathcal{A}$ complete,why is test function space  complete,\mathcal{A},"I am trying to find out, why the space $$\mathcal{A}:=\left\{\phi\in C_0(\mathbb{R}^{2d})|\;\|\phi\|_\mathcal{A}:=\int_{\mathbb{R}^d}\sup_{x\in\mathbb{R}^d}|(\mathcal{F}_p\phi)(x,y)|\;\mathrm dy<\infty\right\}$$ is a Banach space. $\mathcal{F}_p\phi$ denotes the partial Fourier transform defined by $$(\mathcal{F}_p\phi)(x,y):=\int_{\mathbb{R}^d}e^{-ip\cdot y}\phi(x,p)\;\mathrm dp,$$ while $C_0(\mathbb R^{2d})$ denotes the space of continuous functions which vanish at infinity. I don't know how to show the completeness. Does anyone have a hint or a reference?","I am trying to find out, why the space $$\mathcal{A}:=\left\{\phi\in C_0(\mathbb{R}^{2d})|\;\|\phi\|_\mathcal{A}:=\int_{\mathbb{R}^d}\sup_{x\in\mathbb{R}^d}|(\mathcal{F}_p\phi)(x,y)|\;\mathrm dy<\infty\right\}$$ is a Banach space. $\mathcal{F}_p\phi$ denotes the partial Fourier transform defined by $$(\mathcal{F}_p\phi)(x,y):=\int_{\mathbb{R}^d}e^{-ip\cdot y}\phi(x,p)\;\mathrm dp,$$ while $C_0(\mathbb R^{2d})$ denotes the space of continuous functions which vanish at infinity. I don't know how to show the completeness. Does anyone have a hint or a reference?",,"['functional-analysis', 'fourier-analysis', 'banach-spaces', 'normed-spaces', 'distribution-theory']"
64,If $u$ has a weak derivative and $f$ is $C^1$ does $fu$ have a weak derivative (fractional Sobolev space and weak time derivatives),If  has a weak derivative and  is  does  have a weak derivative (fractional Sobolev space and weak time derivatives),u f C^1 fu,"Let $\Omega$ be an open bounded set. Let $s \in (0,1)$ and $H^s(\Omega) := W^{s,2}(\Omega).$ Let $f \in C^1([0,T]\times \Omega)$ and $u \in L^2(0,T;H^s(\Omega))$ with weak derivative $u' \in L^2(0,T;H^{-s}(\Omega))$. We also have that $f$ and $\nabla f$ (the spatial gradient) are uniformly bounded in time and space. Does it follow that $(fu)'$ exists and $(fu)' \in L^2(0,T;H^{-s}(\Omega))$? Any ideas? Maybe there is a reference for this already.","Let $\Omega$ be an open bounded set. Let $s \in (0,1)$ and $H^s(\Omega) := W^{s,2}(\Omega).$ Let $f \in C^1([0,T]\times \Omega)$ and $u \in L^2(0,T;H^s(\Omega))$ with weak derivative $u' \in L^2(0,T;H^{-s}(\Omega))$. We also have that $f$ and $\nabla f$ (the spatial gradient) are uniformly bounded in time and space. Does it follow that $(fu)'$ exists and $(fu)' \in L^2(0,T;H^{-s}(\Omega))$? Any ideas? Maybe there is a reference for this already.",,"['functional-analysis', 'partial-differential-equations', 'sobolev-spaces']"
65,$\gamma-$radonifying operators.,radonifying operators.,\gamma-,"I am reading about $\gamma$-radonifying operators, and came across 2 similar definitions. And I hoped to understand why they are equivalent. Let $H$ be a seperable real Hilbertspace, $E$ banach space. (1).  A bounded linear operator $T:H\to E $ is said to be $\gamma$-radonifying if $T_{\#}\mu$ is a Radon measure for every Gaussian cylindrical measure $\mu$ on $H$. (2). $T:H\to E$ (bounded linear) is $\gamma$-radonifying if $T_{\#}\gamma$ is a Radon-measure for the standard cylindrical gaussian $\gamma$. the image measure $T_{\#}\mu$ is the measure defined by $T_{\#}\mu(A) = \mu(T^{-1}(A))$. Anyone familiar with the Theory of Gaussian random variables on Banach-spaces, Gaussian measures, and radonifying operators? Any ideas on why  these definitions are equivalent are very welcome.","I am reading about $\gamma$-radonifying operators, and came across 2 similar definitions. And I hoped to understand why they are equivalent. Let $H$ be a seperable real Hilbertspace, $E$ banach space. (1).  A bounded linear operator $T:H\to E $ is said to be $\gamma$-radonifying if $T_{\#}\mu$ is a Radon measure for every Gaussian cylindrical measure $\mu$ on $H$. (2). $T:H\to E$ (bounded linear) is $\gamma$-radonifying if $T_{\#}\gamma$ is a Radon-measure for the standard cylindrical gaussian $\gamma$. the image measure $T_{\#}\mu$ is the measure defined by $T_{\#}\mu(A) = \mu(T^{-1}(A))$. Anyone familiar with the Theory of Gaussian random variables on Banach-spaces, Gaussian measures, and radonifying operators? Any ideas on why  these definitions are equivalent are very welcome.",,"['functional-analysis', 'operator-theory', 'banach-spaces', 'hilbert-spaces']"
66,What is compensated compactness?,What is compensated compactness?,,"As the title says, what is compensated compactness? I see people talk about it in the books and papers I am reading but I can only find hand wavy definitions when I look online. Is there a definition or is it just a general idea that covers theorems to give some nice convergence results given some sort of compactness? Thanks!","As the title says, what is compensated compactness? I see people talk about it in the books and papers I am reading but I can only find hand wavy definitions when I look online. Is there a definition or is it just a general idea that covers theorems to give some nice convergence results given some sort of compactness? Thanks!",,"['functional-analysis', 'partial-differential-equations', 'compactness', 'weak-convergence']"
67,A has an self-adjoint extension,A has an self-adjoint extension,,"Let $A$ be a symmetric operator satisfying $\langle \phi,A\phi\rangle\geq C\lVert \phi\rVert^{2}$ for all $\phi\in \mathcal{D}(A)$ and some $C\in \mathbb{R}$. Show that the deficiency indiecs are equal, i.e., $d_{+}(A)=d_{-}(A)$, and therefore $A$ has a self-adjoint extension. So the idea I have is to somehow show that from the condition that $A$ is closed but I can't seem to show it, then again I don't even know if I can conclude that $A$ is closed from that condition. Since $A$ is symmetric I know that $\mathcal{D}(A)$ is dense so then we can define the adjoint $A^*$. Now $\mathcal{D}(A^{*})=\{h\in \mathcal{H}:~\exists~ \eta~ st ~\forall ~\phi\in \mathcal{D}(A) ~ we ~have ~\langle A\phi,h\rangle =\langle \phi,\eta\rangle~ \}.$ from this we see that since $A$ is symmetric that $\mathcal{D}(A)\subset \mathcal{D}(A^{*})$ so we know that $A$ is closeable since $\mathcal{D}(A^{*})$ is dense.","Let $A$ be a symmetric operator satisfying $\langle \phi,A\phi\rangle\geq C\lVert \phi\rVert^{2}$ for all $\phi\in \mathcal{D}(A)$ and some $C\in \mathbb{R}$. Show that the deficiency indiecs are equal, i.e., $d_{+}(A)=d_{-}(A)$, and therefore $A$ has a self-adjoint extension. So the idea I have is to somehow show that from the condition that $A$ is closed but I can't seem to show it, then again I don't even know if I can conclude that $A$ is closed from that condition. Since $A$ is symmetric I know that $\mathcal{D}(A)$ is dense so then we can define the adjoint $A^*$. Now $\mathcal{D}(A^{*})=\{h\in \mathcal{H}:~\exists~ \eta~ st ~\forall ~\phi\in \mathcal{D}(A) ~ we ~have ~\langle A\phi,h\rangle =\langle \phi,\eta\rangle~ \}.$ from this we see that since $A$ is symmetric that $\mathcal{D}(A)\subset \mathcal{D}(A^{*})$ so we know that $A$ is closeable since $\mathcal{D}(A^{*})$ is dense.",,"['analysis', 'functional-analysis', 'operator-theory']"
68,$X$ be a reflexive space then show that $X$ is Banach Space and is reflexive in any equivalent norm.,be a reflexive space then show that  is Banach Space and is reflexive in any equivalent norm.,X X,"Let $X$ be a reflexive space then show that $X$ is Banach Space and is reflexive in any equivalent norm. ...................................................................... I am trying it in a following way... Since dual $X^{'}$ of any normed space $X$ is complete and therefore Banach Space. So, $X^{''}$(double dual) is also Banach space. Now, as $X$ is reflexive we have that $X$ is Banach Space. But, i am not able to find out any easy way to argue that ..If $X$ is reflexive then it is reflexive in any equivalent norm...","Let $X$ be a reflexive space then show that $X$ is Banach Space and is reflexive in any equivalent norm. ...................................................................... I am trying it in a following way... Since dual $X^{'}$ of any normed space $X$ is complete and therefore Banach Space. So, $X^{''}$(double dual) is also Banach space. Now, as $X$ is reflexive we have that $X$ is Banach Space. But, i am not able to find out any easy way to argue that ..If $X$ is reflexive then it is reflexive in any equivalent norm...",,['functional-analysis']
69,"Dual spaces of cartesian products, direct sums and tensor products","Dual spaces of cartesian products, direct sums and tensor products",,"For any topological linear space $X$, let $X^*$ denote its dual space of continuous linear functionals. Let $X \times Y$ denote the Cartesian product of two TVS's, and let $X \oplus Y$ denote their direct sum. Is there a characterization of the dual spaces $(X \times Y)^*$ and $(X \oplus Y)^*$ in terms of $X^*$ and $Y^*$? Suppose furthermore that the topologies on $X$ and $Y$ are locally convex. Let $X \otimes_p Y$ denote the projective tensor product, and let $X \otimes_i Y$ denote the injective tensor product (as defined here ). There is a natural map $X \otimes_p Y \to X \otimes_i Y$. Is there a characterization of the dual spaces $(X \otimes_p Y)^*$ and $(X \otimes_i Y)^*$ in terms of $X^*$ and $Y^*$?","For any topological linear space $X$, let $X^*$ denote its dual space of continuous linear functionals. Let $X \times Y$ denote the Cartesian product of two TVS's, and let $X \oplus Y$ denote their direct sum. Is there a characterization of the dual spaces $(X \times Y)^*$ and $(X \oplus Y)^*$ in terms of $X^*$ and $Y^*$? Suppose furthermore that the topologies on $X$ and $Y$ are locally convex. Let $X \otimes_p Y$ denote the projective tensor product, and let $X \otimes_i Y$ denote the injective tensor product (as defined here ). There is a natural map $X \otimes_p Y \to X \otimes_i Y$. Is there a characterization of the dual spaces $(X \otimes_p Y)^*$ and $(X \otimes_i Y)^*$ in terms of $X^*$ and $Y^*$?",,"['functional-analysis', 'tensor-products']"
70,Using Sobolev-Nirenberg-Gagliardo,Using Sobolev-Nirenberg-Gagliardo,,"I am currently studying a proof of a General Sobolev Inequality. I have the following question: Consider the Sobolev Space $W^{k,p}(U)$. With the added assumption that $k > \frac{n}{p}$. Let $l = \frac{n}{p} -1$ and take $u \in W^{k-l,r}$ where $r = \frac{pn}{n-pl} =n$. How does it follow using Sobolev-Nirenberg-Gagliardo inequality that $D^{\alpha}u \in L^{q}(U)$ for all $n \leq q < \infty$ and all $|\alpha| \leq k-l-1 = k -[\frac{n}{p}]$? Thanks a lot for any assistance!","I am currently studying a proof of a General Sobolev Inequality. I have the following question: Consider the Sobolev Space $W^{k,p}(U)$. With the added assumption that $k > \frac{n}{p}$. Let $l = \frac{n}{p} -1$ and take $u \in W^{k-l,r}$ where $r = \frac{pn}{n-pl} =n$. How does it follow using Sobolev-Nirenberg-Gagliardo inequality that $D^{\alpha}u \in L^{q}(U)$ for all $n \leq q < \infty$ and all $|\alpha| \leq k-l-1 = k -[\frac{n}{p}]$? Thanks a lot for any assistance!",,"['functional-analysis', 'inequality', 'sobolev-spaces']"
71,Nikolski class of probability measures - Metric and Topological Properties,Nikolski class of probability measures - Metric and Topological Properties,,"I am reading a book about non-parametric statistics (Tsybakov's Introduction to Non-Parametic Estimation), and in order to prove some important inequalities on mean-squared error, different classes of sufficiently smooth probability measures are introduced.  Not much is proven about the classes of probability measures in general, and I wanted to know whether much is known when they are considered as metric spaces or topological spaces. For example, given two positive real numbers $\beta, L > 0$, Tsybakov defines the ${\bf Nikol'ski ~ class}$ ${\cal H}(\beta, L)$ to be the set of functions $f:\mathbb{R} \rightarrow \mathbb{R}$ such that the derivative $f^{(l)}$ of order $l = \lfloor \beta \rfloor$ exists and satisfies $\left[ \int \left( f^{(l)}(x+t) - f^{(l)}(x) \right)^2 dx \right]^{\frac{1}{2}} \leq L|t|^{\beta - l}$ for all $t \in \mathbb{R}$. Let ${\cal P}(\beta, L) = \{p \in {\cal H}(\beta, L): \int p(x)dx = 1 \}$ be the set of probability measures in the Nikol'ski class. Suppose  ${\cal P}(\beta, L)$ is considered as a metric space under the $L_2$ distance. Is it the case that ${\cal P}(\beta, L)$ has no isolated points? Are open balls in ${\cal P}(\beta, L)$ connected? Is ${\cal P}(\beta, L)$ closed under convex combinations of measures? Is ${\cal P}(\beta, L)$ complete? I suspect the answer to the first two questions are ""yes"", but I don't know how to prove it if so.  For the first question, it strikes me that one could simply ""shift"" a probability measure to find another close measure (e.g., given a measure $p$, define $q(x)=p(x+\epsilon)$ for some small enough $\epsilon$).  The third also initially struck me as true, but when I tried to bound the convex combination of two measures in the obvious way, I got stuck.","I am reading a book about non-parametric statistics (Tsybakov's Introduction to Non-Parametic Estimation), and in order to prove some important inequalities on mean-squared error, different classes of sufficiently smooth probability measures are introduced.  Not much is proven about the classes of probability measures in general, and I wanted to know whether much is known when they are considered as metric spaces or topological spaces. For example, given two positive real numbers $\beta, L > 0$, Tsybakov defines the ${\bf Nikol'ski ~ class}$ ${\cal H}(\beta, L)$ to be the set of functions $f:\mathbb{R} \rightarrow \mathbb{R}$ such that the derivative $f^{(l)}$ of order $l = \lfloor \beta \rfloor$ exists and satisfies $\left[ \int \left( f^{(l)}(x+t) - f^{(l)}(x) \right)^2 dx \right]^{\frac{1}{2}} \leq L|t|^{\beta - l}$ for all $t \in \mathbb{R}$. Let ${\cal P}(\beta, L) = \{p \in {\cal H}(\beta, L): \int p(x)dx = 1 \}$ be the set of probability measures in the Nikol'ski class. Suppose  ${\cal P}(\beta, L)$ is considered as a metric space under the $L_2$ distance. Is it the case that ${\cal P}(\beta, L)$ has no isolated points? Are open balls in ${\cal P}(\beta, L)$ connected? Is ${\cal P}(\beta, L)$ closed under convex combinations of measures? Is ${\cal P}(\beta, L)$ complete? I suspect the answer to the first two questions are ""yes"", but I don't know how to prove it if so.  For the first question, it strikes me that one could simply ""shift"" a probability measure to find another close measure (e.g., given a measure $p$, define $q(x)=p(x+\epsilon)$ for some small enough $\epsilon$).  The third also initially struck me as true, but when I tried to bound the convex combination of two measures in the obvious way, I got stuck.",,"['real-analysis', 'functional-analysis', 'statistics', 'statistical-inference', 'holder-spaces']"
72,Show convergence of a sequence of continuous functions $f_n$ to a continuous function $f$ does not imply convergence of corresponding integrals.,Show convergence of a sequence of continuous functions  to a continuous function  does not imply convergence of corresponding integrals.,f_n f,"Let $f_n\in C([0,1])$ be a sequence of functions converging uniformly to a function $f$. Show that  $$\lim_{n\rightarrow\infty}\int_0^1f_n(x)dx = \int_0^1 f(x)dx.$$ Give a counterexample to show that the pointwise convergence of continuous functions $f_n$ to a continuous function $f$ does not imply the convergence of the corresponding integrals. I have the following counterexample: $$f_n(x) = \begin{cases} 2n^2 x & 0\leq x \leq\frac{1}{2n} \\ -2n^2(x-\frac{1}{n}) & \frac{1}{2n}\leq x \leq \frac{1}{n} \\ 0 & \frac{1}{n}\leq x \leq 1 \end{cases}$$ I am having trouble seeing how this function converges to 0. I may be looking at this wrong but when you take $n\rightarrow\infty$ doesn't this function look like this? $$f_n(x) = \begin{cases} \infty & 0\leq x \leq 0 \\ \infty & 0\leq x \leq 0 \\ 0 & 0 \leq x \leq 1 \end{cases}.$$ as $n\rightarrow\infty$. Where I am having trouble seeing $f_n\rightarrow 0$. Am I looking at the convergence of functions in a wrong way? If so how should I consider seeing how such functions converge? I know the integral of $\int_0^1 f_n = \frac{1}{2}$ and $\int_0^1\lim_{n\rightarrow\infty}f_n = 0$. Also I don't need help with the proof concerning integrals. Just on the counterexample. Thank you for any help and comments!","Let $f_n\in C([0,1])$ be a sequence of functions converging uniformly to a function $f$. Show that  $$\lim_{n\rightarrow\infty}\int_0^1f_n(x)dx = \int_0^1 f(x)dx.$$ Give a counterexample to show that the pointwise convergence of continuous functions $f_n$ to a continuous function $f$ does not imply the convergence of the corresponding integrals. I have the following counterexample: $$f_n(x) = \begin{cases} 2n^2 x & 0\leq x \leq\frac{1}{2n} \\ -2n^2(x-\frac{1}{n}) & \frac{1}{2n}\leq x \leq \frac{1}{n} \\ 0 & \frac{1}{n}\leq x \leq 1 \end{cases}$$ I am having trouble seeing how this function converges to 0. I may be looking at this wrong but when you take $n\rightarrow\infty$ doesn't this function look like this? $$f_n(x) = \begin{cases} \infty & 0\leq x \leq 0 \\ \infty & 0\leq x \leq 0 \\ 0 & 0 \leq x \leq 1 \end{cases}.$$ as $n\rightarrow\infty$. Where I am having trouble seeing $f_n\rightarrow 0$. Am I looking at the convergence of functions in a wrong way? If so how should I consider seeing how such functions converge? I know the integral of $\int_0^1 f_n = \frac{1}{2}$ and $\int_0^1\lim_{n\rightarrow\infty}f_n = 0$. Also I don't need help with the proof concerning integrals. Just on the counterexample. Thank you for any help and comments!",,"['analysis', 'functional-analysis', 'convergence-divergence', 'continuity', 'self-learning']"
73,Show that the following $u\in L^{\infty}\cap H^1(B)$ is a weak solution to the given system.,Show that the following  is a weak solution to the given system.,u\in L^{\infty}\cap H^1(B),"Let $B=B_{\exp(-2)}\subset\mathbb{R}^2$. I would like to show that a weak solution to the following system (in $B$): \begin{align*} \triangle u_1&=-2|Du|^2(u_1+u_2)/(1+|u|^2)\\ \triangle u_2&=2|Du|^2(u_1-u_2)/(1+|u|^2) \end{align*} is \begin{equation*} u_1=\sin \big(\log (\log |x|^{-1})\big), \quad u_2=\cos \big(\log (\log |x|^{-1})\big). \end{equation*}I have already shown that $u=(u_1, u_2)\in L^{\infty}(B)\cap H^1(B)$. Further, $u$ has certain second weak derivatives in $B$. Also,  \begin{equation} u\in L^{\infty}(B)\cap H^1(B)\cap C^{\infty}(B\setminus\{0\}) \end{equation}and $u$ satisfies the system outside the origin (I have shown this). To show that it is a weak solution, I would like to show that for any $\phi\in L^{\infty}(B)\cap H^1(B)$ with compact support in $B$ I have \begin{equation} -\int_BDu_i\cdot D\phi\ \mathrm{d}x=\int_B \triangle u_i\phi\ \mathrm{d}x \end{equation}for each $i=1, 2$. I started off by noting that if $\phi\in L^{\infty}(B)\cap H^1(B)$ with compact support in $B$, then $T\phi=0$ where $T$ is the trace operator, hence $\phi\in H_0^1(B)=\overline{C_c^{\infty}(B^0)}$. Consequently, there exists a sequence, $\{\phi_m\}_{m=1}^{\infty}\subset C_c^{\infty}(B^0),$ such that  \begin{equation} \|\phi_m-\phi\|_{H^1(B)}\rightarrow 0\quad(\text{as }m\rightarrow \infty). \end{equation}This gives us \begin{equation} -\int_BDu_i\cdot D\phi_m\ \mathrm{d}x=\int_B \triangle u_i\phi_m\ \mathrm{d}x \end{equation}since $u_i$ has second weak derivatives (in those directions). Then \begin{equation} \|Du_i(D\phi_m-D\phi)\|_{L^1(B)}\leq\|Du\|_{L^2(B)}\|D\phi_m-D\phi\|_{L^2(B)}\rightarrow 0 \end{equation}as $m\rightarrow\infty$. Therefore, the left hand side converges to \begin{equation} -\int_BDu_i\cdot D\phi\ \mathrm{d}x \end{equation} for each $i=1, 2$ and now I want to show that the right hand side converges to \begin{equation} \int_B\triangle u_i\phi\ \mathrm{d}x \end{equation}for each $i=1, 2$. This is where I am having some difficulty. I thought if I could show that the operator $L_i: H_0^1(B^0)\rightarrow\mathbb{R}$ defined for each $i=1, 2$ as \begin{equation} L_i\phi\equiv\int_B \triangle u_i\phi\ \mathrm{d}x, \end{equation}is a bounded linear operator then by the density of $C_c^{\infty}(B^0)$ in $H_0^1(B)$ I would have $L_i\phi_m\rightarrow L_i\phi$, as desired. However, in trying to show that $L_i$ is bounded, I get as far as \begin{equation} \int_{B}|\triangle u_i||\phi_m-\phi|\ \mathrm{d}x\leq C\int_B|Du|^2|\phi_m-\phi|\ \mathrm{d}x \end{equation}and then I'm not sure how to proceed from here.","Let $B=B_{\exp(-2)}\subset\mathbb{R}^2$. I would like to show that a weak solution to the following system (in $B$): \begin{align*} \triangle u_1&=-2|Du|^2(u_1+u_2)/(1+|u|^2)\\ \triangle u_2&=2|Du|^2(u_1-u_2)/(1+|u|^2) \end{align*} is \begin{equation*} u_1=\sin \big(\log (\log |x|^{-1})\big), \quad u_2=\cos \big(\log (\log |x|^{-1})\big). \end{equation*}I have already shown that $u=(u_1, u_2)\in L^{\infty}(B)\cap H^1(B)$. Further, $u$ has certain second weak derivatives in $B$. Also,  \begin{equation} u\in L^{\infty}(B)\cap H^1(B)\cap C^{\infty}(B\setminus\{0\}) \end{equation}and $u$ satisfies the system outside the origin (I have shown this). To show that it is a weak solution, I would like to show that for any $\phi\in L^{\infty}(B)\cap H^1(B)$ with compact support in $B$ I have \begin{equation} -\int_BDu_i\cdot D\phi\ \mathrm{d}x=\int_B \triangle u_i\phi\ \mathrm{d}x \end{equation}for each $i=1, 2$. I started off by noting that if $\phi\in L^{\infty}(B)\cap H^1(B)$ with compact support in $B$, then $T\phi=0$ where $T$ is the trace operator, hence $\phi\in H_0^1(B)=\overline{C_c^{\infty}(B^0)}$. Consequently, there exists a sequence, $\{\phi_m\}_{m=1}^{\infty}\subset C_c^{\infty}(B^0),$ such that  \begin{equation} \|\phi_m-\phi\|_{H^1(B)}\rightarrow 0\quad(\text{as }m\rightarrow \infty). \end{equation}This gives us \begin{equation} -\int_BDu_i\cdot D\phi_m\ \mathrm{d}x=\int_B \triangle u_i\phi_m\ \mathrm{d}x \end{equation}since $u_i$ has second weak derivatives (in those directions). Then \begin{equation} \|Du_i(D\phi_m-D\phi)\|_{L^1(B)}\leq\|Du\|_{L^2(B)}\|D\phi_m-D\phi\|_{L^2(B)}\rightarrow 0 \end{equation}as $m\rightarrow\infty$. Therefore, the left hand side converges to \begin{equation} -\int_BDu_i\cdot D\phi\ \mathrm{d}x \end{equation} for each $i=1, 2$ and now I want to show that the right hand side converges to \begin{equation} \int_B\triangle u_i\phi\ \mathrm{d}x \end{equation}for each $i=1, 2$. This is where I am having some difficulty. I thought if I could show that the operator $L_i: H_0^1(B^0)\rightarrow\mathbb{R}$ defined for each $i=1, 2$ as \begin{equation} L_i\phi\equiv\int_B \triangle u_i\phi\ \mathrm{d}x, \end{equation}is a bounded linear operator then by the density of $C_c^{\infty}(B^0)$ in $H_0^1(B)$ I would have $L_i\phi_m\rightarrow L_i\phi$, as desired. However, in trying to show that $L_i$ is bounded, I get as far as \begin{equation} \int_{B}|\triangle u_i||\phi_m-\phi|\ \mathrm{d}x\leq C\int_B|Du|^2|\phi_m-\phi|\ \mathrm{d}x \end{equation}and then I'm not sure how to proceed from here.",,"['analysis', 'functional-analysis', 'partial-differential-equations', 'sobolev-spaces']"
74,"Proving norm equivalence in $W^{1-1/p,p}(\Omega)$",Proving norm equivalence in,"W^{1-1/p,p}(\Omega)","Define for $p\in [1,\infty)$ and $\Omega=(0,1)^N\subset\mathbb{R}^N$  $$W^{1-1/p,p}(\Omega)=\left\{u\in L^p(\Omega): \ \int_\Omega\int_\Omega\frac{|u(x)-u(y)|^p}{|x-y|^{N-1+p}}dxdy<\infty\right\}$$ $W^{1-1/p,p}(\Omega)$ is a Banach space equipped with the norm $$\|u\|=\|u\|_p+\left(\int_\Omega\int_\Omega\frac{|u(x)-u(y)|^p}{|x-y|^{N-1+p}}dxdy\right)^{1/p}$$ It is also a Banach space equipped with the norm $$|u|=\|u\|_p+\sum_{j=1}^{N}\left(\int_0^1\int_{S_j^h}\frac{|u(x_1,...,x_j+h,...,x_N)-u(x_1,...,x_j,...,x_N)|^p}{h^p}dxdh\right)^{1/p}$$ where $S_j^h=\{x=(x_1,...,x_N)\in \Omega:\ 0\leq x_j\leq 1-h\}$. I could prove that there exist a positive constant $c>0$ such that $\|u\|\leq c|u|$ for all $u\in W^{1-1/p,p}(\Omega)$, which implies by the Open Mapping Theorem (OMT) that $\|\cdot\|$ and $|\cdot|$ are equivalents, however, I would like to prove it without using (OMT), i.e. I would like to prove the reverse inequality only by means of calculus. Until now, I could not porve it, any help is appreciated. Thank you","Define for $p\in [1,\infty)$ and $\Omega=(0,1)^N\subset\mathbb{R}^N$  $$W^{1-1/p,p}(\Omega)=\left\{u\in L^p(\Omega): \ \int_\Omega\int_\Omega\frac{|u(x)-u(y)|^p}{|x-y|^{N-1+p}}dxdy<\infty\right\}$$ $W^{1-1/p,p}(\Omega)$ is a Banach space equipped with the norm $$\|u\|=\|u\|_p+\left(\int_\Omega\int_\Omega\frac{|u(x)-u(y)|^p}{|x-y|^{N-1+p}}dxdy\right)^{1/p}$$ It is also a Banach space equipped with the norm $$|u|=\|u\|_p+\sum_{j=1}^{N}\left(\int_0^1\int_{S_j^h}\frac{|u(x_1,...,x_j+h,...,x_N)-u(x_1,...,x_j,...,x_N)|^p}{h^p}dxdh\right)^{1/p}$$ where $S_j^h=\{x=(x_1,...,x_N)\in \Omega:\ 0\leq x_j\leq 1-h\}$. I could prove that there exist a positive constant $c>0$ such that $\|u\|\leq c|u|$ for all $u\in W^{1-1/p,p}(\Omega)$, which implies by the Open Mapping Theorem (OMT) that $\|\cdot\|$ and $|\cdot|$ are equivalents, however, I would like to prove it without using (OMT), i.e. I would like to prove the reverse inequality only by means of calculus. Until now, I could not porve it, any help is appreciated. Thank you",,"['functional-analysis', 'partial-differential-equations', 'banach-spaces', 'sobolev-spaces', 'normed-spaces']"
75,an application of Hahn-Banach theorem,an application of Hahn-Banach theorem,,"Let $M$ be a subspace of $L^1(\mu)$. Construct a bounded linear functional on $M$ such that there are two (hence infinite) different linear extensions preserving norm to $L^1(\mu)$. Someone outlined me as follows: pick $\tilde{f}$ on $\bar{M}$ (the closure of $M$) such that $\tilde{f}=\lim_{n\to\infty}f_n$ such that $\parallel \tilde{f}\parallel_1=\parallel f_n\parallel_1$, where $f_n$ are bounded linear functional defined on $M$, then put $F(x)=\frac{n}{n+1}\tilde{f}(x)$. But I have some questions about it : (1) does an extension be in accordance with a functional or several functionals? (2) Why should we define an functional $\tilde{f}$ on $\bar{M}$ first? Or, can we define an extension just like this: $F(x)=\frac{n}{n+1}f(x)$ where $f$ is a bounded linear functional on $M$. We have known, by the Hahn-Banach theorem,we can find such an $F$. (3) There isn't requirement of $M$ being closed in the Hahn-Banach theorem, if $M$ is closed, can we get some special arguments or a simpler proof? Maybe the three questions have something resembly, but I haven't find it explicitly. I need your kind help, please.","Let $M$ be a subspace of $L^1(\mu)$. Construct a bounded linear functional on $M$ such that there are two (hence infinite) different linear extensions preserving norm to $L^1(\mu)$. Someone outlined me as follows: pick $\tilde{f}$ on $\bar{M}$ (the closure of $M$) such that $\tilde{f}=\lim_{n\to\infty}f_n$ such that $\parallel \tilde{f}\parallel_1=\parallel f_n\parallel_1$, where $f_n$ are bounded linear functional defined on $M$, then put $F(x)=\frac{n}{n+1}\tilde{f}(x)$. But I have some questions about it : (1) does an extension be in accordance with a functional or several functionals? (2) Why should we define an functional $\tilde{f}$ on $\bar{M}$ first? Or, can we define an extension just like this: $F(x)=\frac{n}{n+1}f(x)$ where $f$ is a bounded linear functional on $M$. We have known, by the Hahn-Banach theorem,we can find such an $F$. (3) There isn't requirement of $M$ being closed in the Hahn-Banach theorem, if $M$ is closed, can we get some special arguments or a simpler proof? Maybe the three questions have something resembly, but I haven't find it explicitly. I need your kind help, please.",,"['real-analysis', 'functional-analysis']"
76,Density of invertible differentiable functions.,Density of invertible differentiable functions.,,"Let's consider $\mathcal{C}'(\mathbb{R}^n,\mathbb{R}^n) $ and  a topology $\tau$ (I do believe it exists) such that $\circ: \mathcal{C}'(\mathbb{R}^n,\mathbb{R}^n) \times \mathcal{C}'(\mathbb{R}^n,\mathbb{R}^n) \longrightarrow \mathcal{C}'(\mathbb{R}^n,\mathbb{R}^n)$ is a continuous function. Is there any $\tau$ such that invertible function are dense? If there is one, is it in any way generalizable to $\mathcal{C}'(\mathbb{R}^n,\mathbb{R}^m)$?","Let's consider $\mathcal{C}'(\mathbb{R}^n,\mathbb{R}^n) $ and  a topology $\tau$ (I do believe it exists) such that $\circ: \mathcal{C}'(\mathbb{R}^n,\mathbb{R}^n) \times \mathcal{C}'(\mathbb{R}^n,\mathbb{R}^n) \longrightarrow \mathcal{C}'(\mathbb{R}^n,\mathbb{R}^n)$ is a continuous function. Is there any $\tau$ such that invertible function are dense? If there is one, is it in any way generalizable to $\mathcal{C}'(\mathbb{R}^n,\mathbb{R}^m)$?",,"['analysis', 'functional-analysis']"
77,"Polynomials dense in $C^\infty(U, V)$?",Polynomials dense in ?,"C^\infty(U, V)","Let $V$ be a Banach space and $U$ an open subset of $\mathbb{R}^n$. Let $C^\infty(U, V)$ denote the Fréchet space of $V$-valued smooth functions on $U$ with seminorms $$ \| u\|_{\alpha,K} = \sup_{y \in K} | D^\alpha u(y)|$$ for each compact subset $K$ of $U$. My question is: Are polynomials (or analytic functions) with values in $V$ dense in this space? I could not find a definite answer so far...","Let $V$ be a Banach space and $U$ an open subset of $\mathbb{R}^n$. Let $C^\infty(U, V)$ denote the Fréchet space of $V$-valued smooth functions on $U$ with seminorms $$ \| u\|_{\alpha,K} = \sup_{y \in K} | D^\alpha u(y)|$$ for each compact subset $K$ of $U$. My question is: Are polynomials (or analytic functions) with values in $V$ dense in this space? I could not find a definite answer so far...",,"['functional-analysis', 'polynomials']"
78,Is every irreducible operator unitary equivalent to a banded operator?,Is every irreducible operator unitary equivalent to a banded operator?,,"This issue continues this question . Let $H$ be an infinite dimensional separable Hilbert space and $B(H)$ the algebra of bounded operators. Definition : Let $(e_{n})_{n \in \mathbb{N}}$ be an orthonormal basis. $T \in B(H)$ is banded if $\exists r \in \mathbb{N}$ such that $ (Te_{n}, e_{m})\ne 0 \Rightarrow \vert n-m \vert \leq r$ . Definition : An operator $A \in B(H)$ is irreducible ( Halmos 1968 ) if its commutant $\{ A\}'$ does not contain projections other than $0$ and $I$ (i.e., $A \ne A_{1} \oplus A_{2}$ , or equivalently, $\{A,A^{*}\}''=B(H)$ ). Is every irreducible operator unitary equivalent to a banded operator ? Remark : A banded operator is a thick generalization of a diagonal operator. It's also a finite sum of finite product of weight shift operators.","This issue continues this question . Let be an infinite dimensional separable Hilbert space and the algebra of bounded operators. Definition : Let be an orthonormal basis. is banded if such that . Definition : An operator is irreducible ( Halmos 1968 ) if its commutant does not contain projections other than and (i.e., , or equivalently, ). Is every irreducible operator unitary equivalent to a banded operator ? Remark : A banded operator is a thick generalization of a diagonal operator. It's also a finite sum of finite product of weight shift operators.","H B(H) (e_{n})_{n \in \mathbb{N}} T \in B(H) \exists r \in \mathbb{N} 
(Te_{n}, e_{m})\ne 0 \Rightarrow \vert n-m \vert \leq r A \in B(H) \{ A\}' 0 I A \ne A_{1} \oplus A_{2} \{A,A^{*}\}''=B(H)","['functional-analysis', 'reference-request', 'operator-theory']"
79,Restriction on exponent; Weak Harnack inequality for strong solutions,Restriction on exponent; Weak Harnack inequality for strong solutions,,"The weak Harnack inequality for strong solutions goes as follows (Taking $Lu = a^{ij}(x)D_{ij}u + b^i(x)D_iu+c(x)u$ to be elliptic) Let $u\in W^{2,n}(\Omega)$ satisfy $Lu\leq f$ in $\Omega$ for some function $f\in L^n(\Omega)$ and suppose $u\geq 0$ in some $B = B_{2R}(y)\subseteq\Omega$. Then $$\left(\frac{1}{|B_R|}\int_{B_R} u^p\right)^{1/p}\leq C\left(\inf_{B_R} u + \frac{R}{\lambda}\|f\|_{L^n(B)}\right)$$ where $p$ and $C$ are positive constants. I am to show that $p\leq\frac{n}{(n-1)\frac{\Lambda}{\lambda}-1}$ where $\lambda,\Lambda$ denotes the least and greates eigenvalues of $L$, but somehow the proof escapes me. Any hints?","The weak Harnack inequality for strong solutions goes as follows (Taking $Lu = a^{ij}(x)D_{ij}u + b^i(x)D_iu+c(x)u$ to be elliptic) Let $u\in W^{2,n}(\Omega)$ satisfy $Lu\leq f$ in $\Omega$ for some function $f\in L^n(\Omega)$ and suppose $u\geq 0$ in some $B = B_{2R}(y)\subseteq\Omega$. Then $$\left(\frac{1}{|B_R|}\int_{B_R} u^p\right)^{1/p}\leq C\left(\inf_{B_R} u + \frac{R}{\lambda}\|f\|_{L^n(B)}\right)$$ where $p$ and $C$ are positive constants. I am to show that $p\leq\frac{n}{(n-1)\frac{\Lambda}{\lambda}-1}$ where $\lambda,\Lambda$ denotes the least and greates eigenvalues of $L$, but somehow the proof escapes me. Any hints?",,"['analysis', 'functional-analysis', 'partial-differential-equations']"
80,Banach space :space of all adapted processes continuous equipped wih specific norm is complete,Banach space :space of all adapted processes continuous equipped wih specific norm is complete,,"Let  $\mathbb{B}$ be space of all adapted processes continuous equipped with      the norm $\lVert Y\rVert_{\mathbb{B}}^2=E\left[\sup_{t\in [0,T]} |Y_{t}|^{2}\right] < \infty $, $(B,\lVert\cdot\rVert_{\mathbb{B}})$ is complete ( is Banach space ). Indeed,  suppose $(X^{n})_{n\in \mathbb{N}}$ is a Cauchy sequence in  $(\mathbb B,\lVert\cdot\rVert_{\mathbb{B}})$.  Then we can find a subsequence  $(n_{k})_{k\in \mathbb{N}}$ such that \begin{align} \sum_{n=1}^{\infty} \lVert X^{n_{k+1}}-X^{n_k}\rVert_{\mathbb B} &<\infty  \mbox{ by the triangular inequality, }\\  E\left[ \sum_{n=1}^{\infty}\sup_{t\geq  0}|X_{t}^{n_{k+1}}-X_{t}^{n_k}|^{2}]^{\frac{1}{2}}\right]  &\leq  \sum_{n=1}^{\infty}E\left[\sup_{t \geq 0}|X^{n_{k+1}}-X^{n_k}|^{2}\right]^{\frac{1}{2}} \\ &=\sum_{n=1}^{\infty}\lVert X^{n_{k+1}}-X^{n_k}\rVert_{\mathbb{B}}<\infty\\ \mbox{Then } E[ \sum_{n=1}^{\infty}\sup_{t \geq 0}|X_{t}^{n_{k+1}}-X_{t}^{n_k}|^{2}]^{\frac{1}{2}} &<\infty \end{align} and so for almost every $\omega \in \Omega$  $$  \sum_{k=1}^{\infty}\sup_{t \geq 0}|X_{t}^{n_{k+1}}-X_{t}^{n_{k}}|<\infty $$ then $(\mathbb{B},|.|_{\mathbb{\infty}})$ with $\lVert\cdot\rVert_{\infty}=\sup_{0\leq t \leq T }|X_{t}|$ then  $(B,\lVert\cdot\rVert_{\infty})$ is complete  then there exists a process $X\in \mathbb{B}$ such that $(X_{t}^{n_{k}}(\omega))_{k\in \mathbb{N}}$ $$ (X_{t}^{n_{k}}(\omega))_{k\in \mathbb{N}} \xrightarrow[n \longrightarrow  \infty]{C.U} X(\omega) \hspace{4mm} \forall t \geq 0$$ now: \begin{align} |X^{n}-X|_{\mathbb{B}}^{2}&=E[\sup_{t \geq 0}|X_{t}^{n}-X_{t}|^{2}]\\ \text{by Fatou's lemma} & \leq \lim_{k \to \infty} \inf E\left[ \sup_{t \leq 0}|X_{t}^{n}-X_{t}^{n_{k}}|^{2}\right]^{2}\\ &=\lim_{k \to \infty} \inf|X^{n}-X^{n_{k}}|_{\mathbb{B}}^{2}\xrightarrow[n \longrightarrow  \infty]{} 0 \end{align} since  $(X^{n})_{n\in \mathbb{N}}$ is Cauchy sequence $(\lim_{n\to \infty}\sup |X^{n}-X^{n_{k}}|=0)$. My question: Am i right? If not, please correct.  Please respond. I'll be grateful for any help offered!","Let  $\mathbb{B}$ be space of all adapted processes continuous equipped with      the norm $\lVert Y\rVert_{\mathbb{B}}^2=E\left[\sup_{t\in [0,T]} |Y_{t}|^{2}\right] < \infty $, $(B,\lVert\cdot\rVert_{\mathbb{B}})$ is complete ( is Banach space ). Indeed,  suppose $(X^{n})_{n\in \mathbb{N}}$ is a Cauchy sequence in  $(\mathbb B,\lVert\cdot\rVert_{\mathbb{B}})$.  Then we can find a subsequence  $(n_{k})_{k\in \mathbb{N}}$ such that \begin{align} \sum_{n=1}^{\infty} \lVert X^{n_{k+1}}-X^{n_k}\rVert_{\mathbb B} &<\infty  \mbox{ by the triangular inequality, }\\  E\left[ \sum_{n=1}^{\infty}\sup_{t\geq  0}|X_{t}^{n_{k+1}}-X_{t}^{n_k}|^{2}]^{\frac{1}{2}}\right]  &\leq  \sum_{n=1}^{\infty}E\left[\sup_{t \geq 0}|X^{n_{k+1}}-X^{n_k}|^{2}\right]^{\frac{1}{2}} \\ &=\sum_{n=1}^{\infty}\lVert X^{n_{k+1}}-X^{n_k}\rVert_{\mathbb{B}}<\infty\\ \mbox{Then } E[ \sum_{n=1}^{\infty}\sup_{t \geq 0}|X_{t}^{n_{k+1}}-X_{t}^{n_k}|^{2}]^{\frac{1}{2}} &<\infty \end{align} and so for almost every $\omega \in \Omega$  $$  \sum_{k=1}^{\infty}\sup_{t \geq 0}|X_{t}^{n_{k+1}}-X_{t}^{n_{k}}|<\infty $$ then $(\mathbb{B},|.|_{\mathbb{\infty}})$ with $\lVert\cdot\rVert_{\infty}=\sup_{0\leq t \leq T }|X_{t}|$ then  $(B,\lVert\cdot\rVert_{\infty})$ is complete  then there exists a process $X\in \mathbb{B}$ such that $(X_{t}^{n_{k}}(\omega))_{k\in \mathbb{N}}$ $$ (X_{t}^{n_{k}}(\omega))_{k\in \mathbb{N}} \xrightarrow[n \longrightarrow  \infty]{C.U} X(\omega) \hspace{4mm} \forall t \geq 0$$ now: \begin{align} |X^{n}-X|_{\mathbb{B}}^{2}&=E[\sup_{t \geq 0}|X_{t}^{n}-X_{t}|^{2}]\\ \text{by Fatou's lemma} & \leq \lim_{k \to \infty} \inf E\left[ \sup_{t \leq 0}|X_{t}^{n}-X_{t}^{n_{k}}|^{2}\right]^{2}\\ &=\lim_{k \to \infty} \inf|X^{n}-X^{n_{k}}|_{\mathbb{B}}^{2}\xrightarrow[n \longrightarrow  \infty]{} 0 \end{align} since  $(X^{n})_{n\in \mathbb{N}}$ is Cauchy sequence $(\lim_{n\to \infty}\sup |X^{n}-X^{n_{k}}|=0)$. My question: Am i right? If not, please correct.  Please respond. I'll be grateful for any help offered!",,"['functional-analysis', 'probability-theory', 'banach-spaces', 'stochastic-analysis']"
81,An element of $\ell^2$ wanted,An element of  wanted,\ell^2,"I am looking for an element $x=(x_0,x_1,x_2,\cdots)$ in $\ell^2$ such that the sequence $z_n, n=0,1,2,\cdots$ defined by  $$z_n=2^n(x_n, x_{n+1},\cdots)$$  is dense in $\ell^2$. It seems that this is hard to do, but such $x$ exists by an indirect method (using Birkhorff transitivity theorem), see Dynamics of Linear Operators by Bayart and Matheron, p. 6.","I am looking for an element $x=(x_0,x_1,x_2,\cdots)$ in $\ell^2$ such that the sequence $z_n, n=0,1,2,\cdots$ defined by  $$z_n=2^n(x_n, x_{n+1},\cdots)$$  is dense in $\ell^2$. It seems that this is hard to do, but such $x$ exists by an indirect method (using Birkhorff transitivity theorem), see Dynamics of Linear Operators by Bayart and Matheron, p. 6.",,"['analysis', 'functional-analysis', 'dynamical-systems']"
82,"What is $\int \frac{\delta F}{\delta u} \frac{\delta G}{\delta v} \, dx \; $?",What is ?,"\int \frac{\delta F}{\delta u} \frac{\delta G}{\delta v} \, dx \; ","Given $F[u]$ and $G[v]$ are functionals of a real-valued function, what is $$   \int \frac{\delta F}{\delta u} \frac{\delta G}{\delta v} \, dx \quad ? $$ I have encountered such expressions for example in ""Hamiltonian description of the ideal fluid"" by P. J. Morrison. There Poisson brackets for several systems (mostly some version of ideal fluid) are given, like $$   \{F, G\} = \int \left[\frac{\delta F}{\delta q} \cdot \frac{\delta G}{\delta \pi} - \frac{\delta G}{\delta q} \cdot \frac{\delta F}{\delta \pi} \right] \, d a^3 \qquad (119) $$ $$   \{F, G\} = - \int_D \frac{\delta F}{\delta u} \frac{\partial}{\partial x} \frac{\delta G}{\delta u} \, d x \qquad (156) $$ and, in general: $$   \{F, G\} = \int_D \frac{\delta F}{\delta \psi^i} \mathfrak{J}^{ij} \frac{\delta G}{\delta \psi^j} \, d \mu \qquad (169) $$ where $F$ and $G$ are functionals of appropriate functions, $\mathfrak{J}$ is a certain operator. Unfortunately it is not said what does it mean, or at least I overlooked it. Moreover, the biggest problem is that even types of expressions involved are not specified. I've tried to filled that gap. Considering functions $u,v : M$ and functionals $F, G : M \to R$, $\; M$ being a manifold of certain kind of functions, functional derivative $D$: $$D : (M \to R) \to M \to M \to R $$ $$ \left( \left(D \, F \right)\, u \right) \, [v] = \left[\frac{d}{d \varepsilon} F[u + \varepsilon v] \right]_{\varepsilon = 0}$$ thus $\delta F / \delta u$ stands likely for $$(D \, F)(u) : M \to R.$$ Now  $$   \int_D \frac{\delta F}{\delta u} v \, d x := \left< \frac{\delta F}{\delta u}, v \right> := \left[\frac{d}{d \varepsilon} F[u + \varepsilon v]\right]_{\varepsilon = 0}, $$ that is to some sort of contraction, analogous to covector and vector, and integral notation is just a reference to a finite-dimensional case. Thus I believe $$   \int_D \frac{\delta F}{\delta u} \frac{\delta G}{\delta v} \, dx := \left[ \frac{\delta F}{\delta u}, \frac{\delta G}{\delta v} \right], $$ where $[\,,]$ is some sort of scalar product. Indeed, I guess scalar product between functions is implied: $$   \left[u. v \right] := \int_D uv \, dx, $$ but I don't see from $$\langle \,, \rangle : M \to (M \to R) \to R $$ $$[\,,] : M \to M \to R $$ how $[\,,]$ can be lifted to act for two functional derivatives. That is I know the mapping from functions to functionals, but not vice versa. Probably given enough limitations on $M$ there is one? Maybe someone will point me an accessible, but rigorous introduction to a subject.","Given $F[u]$ and $G[v]$ are functionals of a real-valued function, what is $$   \int \frac{\delta F}{\delta u} \frac{\delta G}{\delta v} \, dx \quad ? $$ I have encountered such expressions for example in ""Hamiltonian description of the ideal fluid"" by P. J. Morrison. There Poisson brackets for several systems (mostly some version of ideal fluid) are given, like $$   \{F, G\} = \int \left[\frac{\delta F}{\delta q} \cdot \frac{\delta G}{\delta \pi} - \frac{\delta G}{\delta q} \cdot \frac{\delta F}{\delta \pi} \right] \, d a^3 \qquad (119) $$ $$   \{F, G\} = - \int_D \frac{\delta F}{\delta u} \frac{\partial}{\partial x} \frac{\delta G}{\delta u} \, d x \qquad (156) $$ and, in general: $$   \{F, G\} = \int_D \frac{\delta F}{\delta \psi^i} \mathfrak{J}^{ij} \frac{\delta G}{\delta \psi^j} \, d \mu \qquad (169) $$ where $F$ and $G$ are functionals of appropriate functions, $\mathfrak{J}$ is a certain operator. Unfortunately it is not said what does it mean, or at least I overlooked it. Moreover, the biggest problem is that even types of expressions involved are not specified. I've tried to filled that gap. Considering functions $u,v : M$ and functionals $F, G : M \to R$, $\; M$ being a manifold of certain kind of functions, functional derivative $D$: $$D : (M \to R) \to M \to M \to R $$ $$ \left( \left(D \, F \right)\, u \right) \, [v] = \left[\frac{d}{d \varepsilon} F[u + \varepsilon v] \right]_{\varepsilon = 0}$$ thus $\delta F / \delta u$ stands likely for $$(D \, F)(u) : M \to R.$$ Now  $$   \int_D \frac{\delta F}{\delta u} v \, d x := \left< \frac{\delta F}{\delta u}, v \right> := \left[\frac{d}{d \varepsilon} F[u + \varepsilon v]\right]_{\varepsilon = 0}, $$ that is to some sort of contraction, analogous to covector and vector, and integral notation is just a reference to a finite-dimensional case. Thus I believe $$   \int_D \frac{\delta F}{\delta u} \frac{\delta G}{\delta v} \, dx := \left[ \frac{\delta F}{\delta u}, \frac{\delta G}{\delta v} \right], $$ where $[\,,]$ is some sort of scalar product. Indeed, I guess scalar product between functions is implied: $$   \left[u. v \right] := \int_D uv \, dx, $$ but I don't see from $$\langle \,, \rangle : M \to (M \to R) \to R $$ $$[\,,] : M \to M \to R $$ how $[\,,]$ can be lifted to act for two functional derivatives. That is I know the mapping from functions to functionals, but not vice versa. Probably given enough limitations on $M$ there is one? Maybe someone will point me an accessible, but rigorous introduction to a subject.",,"['functional-analysis', 'physics', 'classical-mechanics', 'fluid-dynamics']"
83,About convergence of functions in $L_p$,About convergence of functions in,L_p,"Suppose we have a sequence of functions $f_n\ge 0$, $f_n\in L^p(\mu)$, $\int f_nd\mu=1$, and $f\ge 0$, $f\in L^p(\mu), \int fd\mu=1$, $0<p<1$  such that $$g_n:=\frac{f_n^p}{\int f_n^pd\mu}\to g:=\frac{f^p}{\int f^p d\mu}\text{ in } L^1(\mu)$$. Would this imply $$\int f_n^p d\mu\to \int f^p d\mu?$$ All I could show is that $\int f_n^p d\mu$ is bounded. Suppose $M_n:=\int f_n^p d\mu\to \infty$. Then $$\mu(g_n>\epsilon^p)=\mu(f_n>\epsilon M_n^{1/p})\le \frac{1}{\epsilon M_n^{1/p}}\to 0$$ which says that $g_n\to 0$ in $[\mu]$-measure which is not possible. I am wondering whether $\int f_n^p d\mu$ should, in fact, converge. Or, is there a counter-example to disprove this?","Suppose we have a sequence of functions $f_n\ge 0$, $f_n\in L^p(\mu)$, $\int f_nd\mu=1$, and $f\ge 0$, $f\in L^p(\mu), \int fd\mu=1$, $0<p<1$  such that $$g_n:=\frac{f_n^p}{\int f_n^pd\mu}\to g:=\frac{f^p}{\int f^p d\mu}\text{ in } L^1(\mu)$$. Would this imply $$\int f_n^p d\mu\to \int f^p d\mu?$$ All I could show is that $\int f_n^p d\mu$ is bounded. Suppose $M_n:=\int f_n^p d\mu\to \infty$. Then $$\mu(g_n>\epsilon^p)=\mu(f_n>\epsilon M_n^{1/p})\le \frac{1}{\epsilon M_n^{1/p}}\to 0$$ which says that $g_n\to 0$ in $[\mu]$-measure which is not possible. I am wondering whether $\int f_n^p d\mu$ should, in fact, converge. Or, is there a counter-example to disprove this?",,"['real-analysis', 'functional-analysis', 'measure-theory']"
84,Is this projection operator onto a subspace of a Hilbert space bounded?,Is this projection operator onto a subspace of a Hilbert space bounded?,,"(I copy and paste and edit from Is this operator bounded? Hilbert space projection , my question is almost the same) Let $V \subset H$ be Hilbert spaces (different inner products) with $V$ dense and continuously embedded in $H$. Let $\{b_j\}$ be a basis for $H$ and for $V$. Define $$P_n:H \to \text{span}(b_1,...,b_n)$$ by $$(P_n h-h,v_n)_H = 0\quad\text{for all $v_n \in \text{span}(b_1,...,b_n)$}$$ by truncation. Is it true that $$P_n:V \to V$$ is a bounded operator where the constant that bounds it does not depend on $n$ ? It is true as an operator from $H$ to $H$ (take $v_n = P_nh$ in the definition) but not sure for $V$.","(I copy and paste and edit from Is this operator bounded? Hilbert space projection , my question is almost the same) Let $V \subset H$ be Hilbert spaces (different inner products) with $V$ dense and continuously embedded in $H$. Let $\{b_j\}$ be a basis for $H$ and for $V$. Define $$P_n:H \to \text{span}(b_1,...,b_n)$$ by $$(P_n h-h,v_n)_H = 0\quad\text{for all $v_n \in \text{span}(b_1,...,b_n)$}$$ by truncation. Is it true that $$P_n:V \to V$$ is a bounded operator where the constant that bounds it does not depend on $n$ ? It is true as an operator from $H$ to $H$ (take $v_n = P_nh$ in the definition) but not sure for $V$.",,"['functional-analysis', 'hilbert-spaces']"
85,There exists unique $g$ s.t. $g(x) = f(x) + A\int_0^1\sin(x-y)g(y)dy$,There exists unique  s.t.,g g(x) = f(x) + A\int_0^1\sin(x-y)g(y)dy,"I'm doing past papers for a first course on functional analysis. We are not allowed to assume any results from real analysis or topology, so I was surprised to find an exam question, where I couldn't say ""follows from version 13 of Hahn-Banach"": Let $\sin(x-y)$ be defined on $[0,1]\times[0,1]$ and $A$ be a real constant such that $|A|<1$ . Show that for any $f\in C([0,1])$ , there exists a unique $g\in C([0,1])$ such that $$g(x) = f(x) + A\int_0^1\sin(x-y)g(y)dy$$ I have absolutely no idea how even to start. I tried integrating by parts (twice) and got nowhere. Could someone give me a hint?","I'm doing past papers for a first course on functional analysis. We are not allowed to assume any results from real analysis or topology, so I was surprised to find an exam question, where I couldn't say ""follows from version 13 of Hahn-Banach"": Let be defined on and be a real constant such that . Show that for any , there exists a unique such that I have absolutely no idea how even to start. I tried integrating by parts (twice) and got nowhere. Could someone give me a hint?","\sin(x-y) [0,1]\times[0,1] A |A|<1 f\in C([0,1]) g\in C([0,1]) g(x) = f(x) + A\int_0^1\sin(x-y)g(y)dy","['analysis', 'functional-analysis', 'integration']"
86,Which kind of additivity for measures is more natural,Which kind of additivity for measures is more natural,,"To start with, I have almost no any experience in the theory of finitely additive (f.a.) measures, but I work a bit with countably additive (c.a.) ones and find the theory in the latter case amazingly beautiful. My concern is that at the moment measures have been introduced as an extension of such notions as area and volume, I can understand that the additivity property came alone naturally. However, I believe, that at that moment the choice f.a. vs. c.a. might not have any strong arguments. Later, it appeared that in many cases the space of c.a. (but not f.a.) measures is the dual of a corresponding space of all bounded continuous functions. Since the latter is a pretty ""natural"" object, I would say that its dual is ""natural"" as well. Would it be right to say that c.a. measures are more ""natural"" than f.a. ones, or that it appeared to be more successful/useful, and if so - why do we need the f.a. measures? I hope, that a bit loose formulation of the question still allows for an answer.","To start with, I have almost no any experience in the theory of finitely additive (f.a.) measures, but I work a bit with countably additive (c.a.) ones and find the theory in the latter case amazingly beautiful. My concern is that at the moment measures have been introduced as an extension of such notions as area and volume, I can understand that the additivity property came alone naturally. However, I believe, that at that moment the choice f.a. vs. c.a. might not have any strong arguments. Later, it appeared that in many cases the space of c.a. (but not f.a.) measures is the dual of a corresponding space of all bounded continuous functions. Since the latter is a pretty ""natural"" object, I would say that its dual is ""natural"" as well. Would it be right to say that c.a. measures are more ""natural"" than f.a. ones, or that it appeared to be more successful/useful, and if so - why do we need the f.a. measures? I hope, that a bit loose formulation of the question still allows for an answer.",,"['functional-analysis', 'measure-theory']"
87,Differentiating an infinite series in Hilbert space,Differentiating an infinite series in Hilbert space,,"Suppose $H$ is separable Hilbert space and $w_j$ is a basis. Suppose we have $h=\sum a_j(t)w_j$ an infinite sum where the coefficients are functions of $t$. The sum makes sense in the sense that the partial sums converge to $h$. When are we allowed to differentiate term by term and write $h'=\sum a_j'(t)w_j$? Also, how about if the basis functions depended on $t$ too?","Suppose $H$ is separable Hilbert space and $w_j$ is a basis. Suppose we have $h=\sum a_j(t)w_j$ an infinite sum where the coefficients are functions of $t$. The sum makes sense in the sense that the partial sums converge to $h$. When are we allowed to differentiate term by term and write $h'=\sum a_j'(t)w_j$? Also, how about if the basis functions depended on $t$ too?",,"['functional-analysis', 'hilbert-spaces']"
88,Direct sum of Hilbert spaces,Direct sum of Hilbert spaces,,"Let $H$ be a separable Hilbert space with an orthonormal basis $\{e_n\}_{n =0}^{\infty}$. Consider a direct sum, $H \oplus H$. What is the orthonormal basis of $H \oplus H$ ? Is it $(e_n, e_m)_{n,m \in \mathbb{N}}$? In finite dimensional case, say the dimension of $H$ is $n$ we have that $\dim H \oplus H = 2n$, in infinite dimensional case the dimension will be the same, that is $\aleph_0$. Thank you for any help I try to understand the Hilbert space direct sum.","Let $H$ be a separable Hilbert space with an orthonormal basis $\{e_n\}_{n =0}^{\infty}$. Consider a direct sum, $H \oplus H$. What is the orthonormal basis of $H \oplus H$ ? Is it $(e_n, e_m)_{n,m \in \mathbb{N}}$? In finite dimensional case, say the dimension of $H$ is $n$ we have that $\dim H \oplus H = 2n$, in infinite dimensional case the dimension will be the same, that is $\aleph_0$. Thank you for any help I try to understand the Hilbert space direct sum.",,"['functional-analysis', 'hilbert-spaces']"
89,Fredholm operators and Compact operators,Fredholm operators and Compact operators,,"Suppose $X$ be an infinite  dimensional Banach space. How to prove that: $A$ and $B$ are two Fredholm operator on $X$, if  $\mathrm{index}(A)=\mathrm{index}(B)$, then there exists an invertible operator $C$ such that  $A-BC$ is compact??? I know that if $T$ is a Fredholm operator with $\mathrm{index}(T)=0$ iff there exist  an invertible operator A and a compact operator $K$ such that $T=A+K$.","Suppose $X$ be an infinite  dimensional Banach space. How to prove that: $A$ and $B$ are two Fredholm operator on $X$, if  $\mathrm{index}(A)=\mathrm{index}(B)$, then there exists an invertible operator $C$ such that  $A-BC$ is compact??? I know that if $T$ is a Fredholm operator with $\mathrm{index}(T)=0$ iff there exist  an invertible operator A and a compact operator $K$ such that $T=A+K$.",,['functional-analysis']
90,Does convex and radially open imply open?,Does convex and radially open imply open?,,"I want to show that a convex set $A$ is radially open iff $A\cap W$ is open in W, for every finite dimensional linear subspace. Here the 'openness' we are talking about is from any normed space. Help would be much appreciated. Definition : We call a set $A$ of a vector space $V$ $\underline{\text{radially open}}$ if $\forall v \in V, x \in A$ $\exists \lambda \in \mathbb{R}$ such that $x + \alpha v \in A$ for any $0 < \alpha < \lambda$.","I want to show that a convex set $A$ is radially open iff $A\cap W$ is open in W, for every finite dimensional linear subspace. Here the 'openness' we are talking about is from any normed space. Help would be much appreciated. Definition : We call a set $A$ of a vector space $V$ $\underline{\text{radially open}}$ if $\forall v \in V, x \in A$ $\exists \lambda \in \mathbb{R}$ such that $x + \alpha v \in A$ for any $0 < \alpha < \lambda$.",,['linear-algebra']
91,"SVD, infinite matrices and normal operators from a function","SVD, infinite matrices and normal operators from a function",,"I'm trying to understand the behavior the Singular Value Decomposition on a deeper level, and why it might give a particular result. Take the function $$ f(x,y) = \frac{1}{(1+2x+y)^2} $$ and consider a continous range of parameters over $x \in [0,1]$, $y \in [0,.5]$. Consider an ""infinite"" matrix $$ A = \begin{bmatrix} f(0,0) & f(\delta_1, 0) & f(2\delta_1,0) & ... & f(1,0) \\ f(\delta_1,\delta_2) & f(\delta_1, \delta_2) & f(2\delta_1, \delta_2) & ... & f(1,\delta_2) \\ ... & ... & ... & ... & ... \\ f(\delta_1,.5) & f(\delta_1, .5) & f(2\delta_1, .5) & ... & f(1,.5) \\ \end{bmatrix} $$ where $\delta_1$ and $\delta_2$ can be made as small as I like. I can certainly take a finite, but small value for the $\delta's$ and compute the SVD $$A=USV$$ This gives some interesting behavior (plotted below in clockwise order $f$, log of the singular values, and the corresponding vectors, $u_i$ and $v_i$): Questions: Terminology: Is it correct to think of $f$ as a (normal) operator, or is it $A$? What is this operator? Spectral theory: How do you compute the eigenvectors directly from $f$?","I'm trying to understand the behavior the Singular Value Decomposition on a deeper level, and why it might give a particular result. Take the function $$ f(x,y) = \frac{1}{(1+2x+y)^2} $$ and consider a continous range of parameters over $x \in [0,1]$, $y \in [0,.5]$. Consider an ""infinite"" matrix $$ A = \begin{bmatrix} f(0,0) & f(\delta_1, 0) & f(2\delta_1,0) & ... & f(1,0) \\ f(\delta_1,\delta_2) & f(\delta_1, \delta_2) & f(2\delta_1, \delta_2) & ... & f(1,\delta_2) \\ ... & ... & ... & ... & ... \\ f(\delta_1,.5) & f(\delta_1, .5) & f(2\delta_1, .5) & ... & f(1,.5) \\ \end{bmatrix} $$ where $\delta_1$ and $\delta_2$ can be made as small as I like. I can certainly take a finite, but small value for the $\delta's$ and compute the SVD $$A=USV$$ This gives some interesting behavior (plotted below in clockwise order $f$, log of the singular values, and the corresponding vectors, $u_i$ and $v_i$): Questions: Terminology: Is it correct to think of $f$ as a (normal) operator, or is it $A$? What is this operator? Spectral theory: How do you compute the eigenvectors directly from $f$?",,"['analysis', 'functional-analysis', 'spectral-theory', 'svd']"
92,"$f(a+b,\lambda) = f (a,\lambda) \cdot f(b,\lambda)$",,"f(a+b,\lambda) = f (a,\lambda) \cdot f(b,\lambda)","Consider the equation: $f(a+b,\lambda) = f (a,\lambda) \cdot f(b,\lambda)$, for $a \geq 0$ and $b \geq 0$. Is my understanding that this simple functional equation is important in analysis. Can someone give me examples where this can be used in practice, its theoretical importance and maybe some literature where I can find more? Also, if $f(\cdot,\lambda)$ is continuous, then one can show that $f(a,\lambda) =$exp$(ag(\lambda))$, for some function $g(\lambda)$. Is this the only type of function that will satisfy the equation above?","Consider the equation: $f(a+b,\lambda) = f (a,\lambda) \cdot f(b,\lambda)$, for $a \geq 0$ and $b \geq 0$. Is my understanding that this simple functional equation is important in analysis. Can someone give me examples where this can be used in practice, its theoretical importance and maybe some literature where I can find more? Also, if $f(\cdot,\lambda)$ is continuous, then one can show that $f(a,\lambda) =$exp$(ag(\lambda))$, for some function $g(\lambda)$. Is this the only type of function that will satisfy the equation above?",,"['real-analysis', 'functional-analysis', 'functional-equations']"
93,what to do if it's not direct sum?,what to do if it's not direct sum?,,"Suppose $X=Y+Z$ is Banach, $Y$ and $Z$ are closed subspaces. I want to show there exists $\alpha>0$ such that $\forall x \in X, \exists$ $y \in Y$ and $z \in Z$ such that  $x=y+z$ and $\|y\|+\|z\| \le \alpha\|x\|$. If that is direct sum, I can define $T:Y \times Z \to X$ by $T(y,z)=y+z$. It can be easily checked that $Y \times Z$ is Banach, $T$ is linear, bounded and bijective. Hence, by inverse mapping theorem, $T^{-1}$ exists and is bounded. Then we can take $\alpha$ to be $\|T^{-1}\|$, $y, z$ to be the unique decomposition, and it's done. But what to do if $Y\bigcap Z \ne\{0\}$ in general? I tried to define $T:(Y/Y\bigcap Z) \times (Z/Y\bigcap Z) \to X$ by $T([y],[z])=y+z$. Then this map is linear, bijective and between two Banach spaces. Then I ran into trouble showing it's continuous (I wanted to apply inverse mapping theorem). Am I on the right track?","Suppose $X=Y+Z$ is Banach, $Y$ and $Z$ are closed subspaces. I want to show there exists $\alpha>0$ such that $\forall x \in X, \exists$ $y \in Y$ and $z \in Z$ such that  $x=y+z$ and $\|y\|+\|z\| \le \alpha\|x\|$. If that is direct sum, I can define $T:Y \times Z \to X$ by $T(y,z)=y+z$. It can be easily checked that $Y \times Z$ is Banach, $T$ is linear, bounded and bijective. Hence, by inverse mapping theorem, $T^{-1}$ exists and is bounded. Then we can take $\alpha$ to be $\|T^{-1}\|$, $y, z$ to be the unique decomposition, and it's done. But what to do if $Y\bigcap Z \ne\{0\}$ in general? I tried to define $T:(Y/Y\bigcap Z) \times (Z/Y\bigcap Z) \to X$ by $T([y],[z])=y+z$. Then this map is linear, bijective and between two Banach spaces. Then I ran into trouble showing it's continuous (I wanted to apply inverse mapping theorem). Am I on the right track?",,"['linear-algebra', 'functional-analysis']"
94,Inversion formula for Schwartz-space $\mathcal{S}$.,Inversion formula for Schwartz-space .,\mathcal{S},"Let $T\phi = \overline{\mathcal{F}}\mathcal{F}\phi$ for $\phi \in \mathcal{S}(\mathbb{R}^n)$ (rapidly decreasing functions or Schwartz-functions, $\mathcal{F}$ fourier transform and $\overline{\mathcal{F}}$ co-fourier-transform). Prove: (1) $(TD_j - D_jT)\phi =0$ and (2) $T(x_j\phi)-x_jT(\phi)=0$ for $\phi\in \mathcal{S}$. What's there to say other than ""it's true because it's true"". ? Take for granted: For $\phi\in \mathcal{S}$ with $\phi(y) = 0$ we have $$\phi(x)=\sum_{j=1}^n(x_j-y_j)\phi_j(x), $$ for appropriate $\phi_j\in \mathcal{S}$. Show that (3) $\phi(x_0)=0$ implies $T(\phi)(x_0)=0$. Pick a positive function $\phi_0\in \mathcal{S}$, and apply this to $\phi(x_0)\phi_0-\phi_0(x_0)\phi$ to show there exist a function $c(x)$ so that $(T\phi)(x)=c(x)\phi(x)$ for all $\phi\in \mathcal{S}$. Use relation (1) to show that $c(x)$ is a constant $c$. Take $\phi=e^{-|x|^2}$ to determine $c$ and derive the inversion formula for $\mathcal{S}$. I understand why (3) holds by application of (2) but I don't see how to make an inversion-formula. I can also show $c= \frac{T(\phi_0)}  {\phi_0}$,and why it is a constant. But i don't quite understand how to show (1),(2). Thanks for help and/or suggestions.","Let $T\phi = \overline{\mathcal{F}}\mathcal{F}\phi$ for $\phi \in \mathcal{S}(\mathbb{R}^n)$ (rapidly decreasing functions or Schwartz-functions, $\mathcal{F}$ fourier transform and $\overline{\mathcal{F}}$ co-fourier-transform). Prove: (1) $(TD_j - D_jT)\phi =0$ and (2) $T(x_j\phi)-x_jT(\phi)=0$ for $\phi\in \mathcal{S}$. What's there to say other than ""it's true because it's true"". ? Take for granted: For $\phi\in \mathcal{S}$ with $\phi(y) = 0$ we have $$\phi(x)=\sum_{j=1}^n(x_j-y_j)\phi_j(x), $$ for appropriate $\phi_j\in \mathcal{S}$. Show that (3) $\phi(x_0)=0$ implies $T(\phi)(x_0)=0$. Pick a positive function $\phi_0\in \mathcal{S}$, and apply this to $\phi(x_0)\phi_0-\phi_0(x_0)\phi$ to show there exist a function $c(x)$ so that $(T\phi)(x)=c(x)\phi(x)$ for all $\phi\in \mathcal{S}$. Use relation (1) to show that $c(x)$ is a constant $c$. Take $\phi=e^{-|x|^2}$ to determine $c$ and derive the inversion formula for $\mathcal{S}$. I understand why (3) holds by application of (2) but I don't see how to make an inversion-formula. I can also show $c= \frac{T(\phi_0)}  {\phi_0}$,and why it is a constant. But i don't quite understand how to show (1),(2). Thanks for help and/or suggestions.",,"['real-analysis', 'functional-analysis', 'fourier-analysis', 'distribution-theory']"
95,Existence of dominating measure for weak*-compact set of measures,Existence of dominating measure for weak*-compact set of measures,,"Let $(\Omega,\mathcal F)$ be a measurable space and $\mathcal P$ a weak*-compact set of the set of all probability measures $\mathcal M_1(\Omega)$. Does there always exist a probability measure $\mathbb Q\in\mathcal M_1(\Omega)$ such that every $\mathbb P\in\mathcal P$ is absolutely continuous to $\mathbb Q$, i.e. such that $\mathbb Q$ dominates all measures in $\mathcal P$?","Let $(\Omega,\mathcal F)$ be a measurable space and $\mathcal P$ a weak*-compact set of the set of all probability measures $\mathcal M_1(\Omega)$. Does there always exist a probability measure $\mathbb Q\in\mathcal M_1(\Omega)$ such that every $\mathbb P\in\mathcal P$ is absolutely continuous to $\mathbb Q$, i.e. such that $\mathbb Q$ dominates all measures in $\mathcal P$?",,"['functional-analysis', 'measure-theory', 'probability-theory']"
96,Space of functions that are everywhere differentiable,Space of functions that are everywhere differentiable,,"Define the space $\beta^1([a,b])$ as the space of functions $f : [a,b] \mapsto \Bbb R$ which are everywhere differentiable and whose derivative $f'$ is a bounded function.  One equips this space with the metric $$ d(f,g) = \sup{|f(x)-g(x)|} + \sup{|f'(x) - g'(x)|}$$ Prove that this turns $\beta^1([a,b])$ into a complete metric space. Please help. I know what a complete metric space is, but I'm having trouble getting started with this proof. Is there a way to do this proof without knowing about Banach spaces? I am stuck and any help would be appreciated. Thanks","Define the space $\beta^1([a,b])$ as the space of functions $f : [a,b] \mapsto \Bbb R$ which are everywhere differentiable and whose derivative $f'$ is a bounded function.  One equips this space with the metric $$ d(f,g) = \sup{|f(x)-g(x)|} + \sup{|f'(x) - g'(x)|}$$ Prove that this turns $\beta^1([a,b])$ into a complete metric space. Please help. I know what a complete metric space is, but I'm having trouble getting started with this proof. Is there a way to do this proof without knowing about Banach spaces? I am stuck and any help would be appreciated. Thanks",,"['functional-analysis', 'derivatives', 'metric-spaces', 'banach-spaces']"
97,A normed space is not separable if and only if it contains an uncountable set of disjoint balls of the same radius,A normed space is not separable if and only if it contains an uncountable set of disjoint balls of the same radius,,"I want to show normed space $E$ is not separable iff $E$ contains uncountable set of pairwise non-intersecting balls of radius $r > 0$. Use contraposive: first prove $E$ is separable then $E...$countable There is uncoutablely many elments in $E$, removing any elements in the uncoutable set will end up in a subset whose closure is not $E$. Contradict $E$ is dense. The other direction: $E...$countable then $E$ is separable. How to prove this direction? Any comments about the first proof? Many thanks!","I want to show normed space $E$ is not separable iff $E$ contains uncountable set of pairwise non-intersecting balls of radius $r > 0$. Use contraposive: first prove $E$ is separable then $E...$countable There is uncoutablely many elments in $E$, removing any elements in the uncoutable set will end up in a subset whose closure is not $E$. Contradict $E$ is dense. The other direction: $E...$countable then $E$ is separable. How to prove this direction? Any comments about the first proof? Many thanks!",,"['general-topology', 'functional-analysis', 'normed-spaces']"
98,Sum of Closed Operators,Sum of Closed Operators,,"If $A$ and $B$ are two closed operators on a Hilbert space (not defined everywhere), is their sum closed as well? I think not, but cannot construct a counterexample. Some posts on this site do address this but they are not complete. For example, Sum of Closed Operators Closable? gives two operators and shows that their sum is not cloasable, but does not show that the operators themselves are closed. There is another post saying the same thing Counterexample for ""the sum of closed operators is closable"" but the question is still unresolved. Could someone give a different, simpler counterexample?","If $A$ and $B$ are two closed operators on a Hilbert space (not defined everywhere), is their sum closed as well? I think not, but cannot construct a counterexample. Some posts on this site do address this but they are not complete. For example, Sum of Closed Operators Closable? gives two operators and shows that their sum is not cloasable, but does not show that the operators themselves are closed. There is another post saying the same thing Counterexample for ""the sum of closed operators is closable"" but the question is still unresolved. Could someone give a different, simpler counterexample?",,"['functional-analysis', 'operator-theory']"
99,Find Lipshitz constant,Find Lipshitz constant,,"Let $x, y$ be two vectors in $R^n$. Let $\pi$ be permutation on $\{1, \ldots, n\}$ with uniform distribution. Find Lipshitz constaants of the following functions: $$ f(\pi)=\sum_{i=1}^nx_{\pi(i)}y_i $$ and $$ g(\pi)=\sum_{i=1}^nx_{\pi(i)}y_{\pi(i)}. $$ Thank you. My work so far: I've tried function $f$ (for $g$ I am confused); $$ |f(\sigma)-f(\pi)|\leq \left|\sum_{i=1}^ny_ix_{\sigma(i)}-y_ix_{\pi(i)}\right| \leq \sum_{i=1}^{n}|y_i||x_{\sigma(i)}-x_{\pi(i)}|\leq 2\|x\|_{\infty} \|y\|_{1}d_{n}(\sigma, \pi). $$ Can one do better?","Let $x, y$ be two vectors in $R^n$. Let $\pi$ be permutation on $\{1, \ldots, n\}$ with uniform distribution. Find Lipshitz constaants of the following functions: $$ f(\pi)=\sum_{i=1}^nx_{\pi(i)}y_i $$ and $$ g(\pi)=\sum_{i=1}^nx_{\pi(i)}y_{\pi(i)}. $$ Thank you. My work so far: I've tried function $f$ (for $g$ I am confused); $$ |f(\sigma)-f(\pi)|\leq \left|\sum_{i=1}^ny_ix_{\sigma(i)}-y_ix_{\pi(i)}\right| \leq \sum_{i=1}^{n}|y_i||x_{\sigma(i)}-x_{\pi(i)}|\leq 2\|x\|_{\infty} \|y\|_{1}d_{n}(\sigma, \pi). $$ Can one do better?",,"['functional-analysis', 'inequality', 'normed-spaces']"
