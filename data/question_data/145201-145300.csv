,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Use the $\epsilon$-$\delta$ definition of a limit to prove this.,Use the - definition of a limit to prove this.,\epsilon \delta,"I know to how prove normal limits using the epsilon-delta definition, say: $$\lim_{x\to a} f(x)=L$$ But, there was a question on my textbook which I couldn't quite figure out to do, even though I've thought about it for a while I don't even know how to go about starting it. Use $\varepsilon$ - $\delta$ definition of a limit to prove that $$\lim_{x\to c} f(x)=0 \iff \lim_{x\to c} \left\vert f(x)\right\vert=0.$$ Could anyone help me with this, even a hint on where to start? Thank you in advance","I know to how prove normal limits using the epsilon-delta definition, say: But, there was a question on my textbook which I couldn't quite figure out to do, even though I've thought about it for a while I don't even know how to go about starting it. Use - definition of a limit to prove that Could anyone help me with this, even a hint on where to start? Thank you in advance",\lim_{x\to a} f(x)=L \varepsilon \delta \lim_{x\to c} f(x)=0 \iff \lim_{x\to c} \left\vert f(x)\right\vert=0.,"['calculus', 'limits']"
1,Calculus limit homework problem,Calculus limit homework problem,,$$ \lim_{n→\infty} \frac1 n \left(\left(a + \frac 1 n\right)^2 + \left(a + \frac 2 n\right)^2 + ... + \left(a + \frac{n-1}{n}\right)^2\right)$$  $$ \text{hint: }\  1^2 + 2^2 + ... + n^2 = \frac{n(n+1)(2n+1)}{6} $$ I can't figure out how to find this problem's limit. Does anybody have ideas?,$$ \lim_{n→\infty} \frac1 n \left(\left(a + \frac 1 n\right)^2 + \left(a + \frac 2 n\right)^2 + ... + \left(a + \frac{n-1}{n}\right)^2\right)$$  $$ \text{hint: }\  1^2 + 2^2 + ... + n^2 = \frac{n(n+1)(2n+1)}{6} $$ I can't figure out how to find this problem's limit. Does anybody have ideas?,,"['calculus', 'limits']"
2,Using Central Limit Theorem when we NON-IID sample,Using Central Limit Theorem when we NON-IID sample,,"I'm trying to solve a CLT question and I've got some issues. I appreciate if you could help me on that. Consider the question below: $\epsilon_i$ 's are iid random variables with finite mean and variance. $X_i$'s are defined as: $$X_i = \frac{\epsilon_i + \epsilon_{i + 1} + \epsilon_{i + 2}}{3}$$ $$ S_n = \sum_{j =1}^{n} X_j$$ I need to find constants $a_n$ and $b_n$ such that $\frac{S_n - a_n}{b_n} \rightarrow N(0,1)$. Here is what I've done so far: Considering $S_{1,n} = \sum_{j = 1}^{n} \epsilon_j$, we can simply use CLT. We can also do the same for $S_{2,n} = \sum_{j = 2}^{n} \epsilon_j$ and for $S_{3,n} = \sum_{j = 3}^{n} \epsilon_j$. The issue is how to combine these three terms together. Obviously, there are some common terms when we combine all these three together which makes observations not to be IID anymore and that's the main challenge.","I'm trying to solve a CLT question and I've got some issues. I appreciate if you could help me on that. Consider the question below: $\epsilon_i$ 's are iid random variables with finite mean and variance. $X_i$'s are defined as: $$X_i = \frac{\epsilon_i + \epsilon_{i + 1} + \epsilon_{i + 2}}{3}$$ $$ S_n = \sum_{j =1}^{n} X_j$$ I need to find constants $a_n$ and $b_n$ such that $\frac{S_n - a_n}{b_n} \rightarrow N(0,1)$. Here is what I've done so far: Considering $S_{1,n} = \sum_{j = 1}^{n} \epsilon_j$, we can simply use CLT. We can also do the same for $S_{2,n} = \sum_{j = 2}^{n} \epsilon_j$ and for $S_{3,n} = \sum_{j = 3}^{n} \epsilon_j$. The issue is how to combine these three terms together. Obviously, there are some common terms when we combine all these three together which makes observations not to be IID anymore and that's the main challenge.",,"['probability', 'limits', 'probability-distributions', 'weak-convergence', 'probability-limit-theorems']"
3,Prove that the limit of ${ \frac{(-1)^n}{n}}= 0$,Prove that the limit of,{ \frac{(-1)^n}{n}}= 0,"Prove that the limit of $s_n = {\large \frac{(-1)^n}{n}}= 0$ . This is what I have so far, based on a walk-through of a similar question in my textbook, but I don't really understand what I'm doing. There exists a number $N$ such that $n\gt N$ implies: $$\left|\dfrac{(-1)^n}{n} - 0\right|\lt \epsilon \Rightarrow\left|\dfrac{(-1)^n}{n}\right|< \epsilon \Rightarrow \frac{1}{n} < \epsilon \Rightarrow \frac{1}{\epsilon} < n$$ Now, let $\epsilon > 0$, $N= \frac{1}{\epsilon}$. Then $n > N$ implies $n > \frac{1}{\epsilon}$ which implies $\epsilon > \frac{1}{n}$. If $n > N$, then $\left|\dfrac{(-1)^n}{n}-0\right| < \epsilon$. Therefore the limit of $s_n = 0$.","Prove that the limit of $s_n = {\large \frac{(-1)^n}{n}}= 0$ . This is what I have so far, based on a walk-through of a similar question in my textbook, but I don't really understand what I'm doing. There exists a number $N$ such that $n\gt N$ implies: $$\left|\dfrac{(-1)^n}{n} - 0\right|\lt \epsilon \Rightarrow\left|\dfrac{(-1)^n}{n}\right|< \epsilon \Rightarrow \frac{1}{n} < \epsilon \Rightarrow \frac{1}{\epsilon} < n$$ Now, let $\epsilon > 0$, $N= \frac{1}{\epsilon}$. Then $n > N$ implies $n > \frac{1}{\epsilon}$ which implies $\epsilon > \frac{1}{n}$. If $n > N$, then $\left|\dfrac{(-1)^n}{n}-0\right| < \epsilon$. Therefore the limit of $s_n = 0$.",,['real-analysis']
4,How find this positive $n$ such $n<6(1-1.001^{-1000})<n+1$,How find this positive  such,n n<6(1-1.001^{-1000})<n+1,"let $n$ is positive numbers,and such  $$n<6(1-1.001^{-1000})<n+1$$ find the value $n$ This problem is from china compition today,(some hours ago) my try:use this following  $$\lim_{n\to0}(1-n)^{\frac{1}{n}}=\dfrac{1}{e}$$ so $$6(1-1.001^{-1000})\approx 6(1-\dfrac{1}{e})$$ so we have $$n=3$$ other idea $$(1.001)^{-1000}=(1+0.001)^{-1000}=1-1000\cdot0.001+\dfrac{(-1000)(-1000-1)}{2}(0.001)^2+\cdots $$  this idea seems very ugly my question, Have other good nice methods? Thank you","let $n$ is positive numbers,and such  $$n<6(1-1.001^{-1000})<n+1$$ find the value $n$ This problem is from china compition today,(some hours ago) my try:use this following  $$\lim_{n\to0}(1-n)^{\frac{1}{n}}=\dfrac{1}{e}$$ so $$6(1-1.001^{-1000})\approx 6(1-\dfrac{1}{e})$$ so we have $$n=3$$ other idea $$(1.001)^{-1000}=(1+0.001)^{-1000}=1-1000\cdot0.001+\dfrac{(-1000)(-1000-1)}{2}(0.001)^2+\cdots $$  this idea seems very ugly my question, Have other good nice methods? Thank you",,"['calculus', 'limits']"
5,limit of 'exponential type' sequences,limit of 'exponential type' sequences,,"I am trying to study limits of the form $$ \sum_{i=1}^{\infty}\frac{p(i)}{i!}$$ where $p$ is a polynomial function of degree $d$. For example,  $$\sum_{i=1}^{\infty}\frac{i^2}{(i+1)!} = e-1$$ I want to study the limit for different polynomials. Currently, I am evaluating these limits by hand, by reducing them to partial fractions of factorials. Is there any 'ready-made' formula, or any other approach that will be suitable for coding?","I am trying to study limits of the form $$ \sum_{i=1}^{\infty}\frac{p(i)}{i!}$$ where $p$ is a polynomial function of degree $d$. For example,  $$\sum_{i=1}^{\infty}\frac{i^2}{(i+1)!} = e-1$$ I want to study the limit for different polynomials. Currently, I am evaluating these limits by hand, by reducing them to partial fractions of factorials. Is there any 'ready-made' formula, or any other approach that will be suitable for coding?",,"['sequences-and-series', 'limits']"
6,Calculating limits using the $\epsilon$-$\delta$ definition.,Calculating limits using the - definition.,\epsilon \delta,Suppose you have a function $f(x)=\frac{x^2-4}{x-2}$ . How then do we find the limit as $x\to2$ in accordance with the $\epsilon-\delta$ definition? I mean suppose we don't know how to calculate the limit and we have to derive a method to calculate the limit using $\epsilon-\delta$ definition of limit. Then what intuition will be used to derive it and what will be the value?,Suppose you have a function . How then do we find the limit as in accordance with the definition? I mean suppose we don't know how to calculate the limit and we have to derive a method to calculate the limit using definition of limit. Then what intuition will be used to derive it and what will be the value?,f(x)=\frac{x^2-4}{x-2} x\to2 \epsilon-\delta \epsilon-\delta,"['real-analysis', 'limits']"
7,"Limit, factorials","Limit, factorials",,"There is the following limit, I would like to calculate: $\lim_{n\rightarrow\infty}\frac{n!}{\left(n+1/6\right)!}$ I tried to use the Stirling approaximation formula $n!\approx\sqrt{2\pi n}\left(\frac{n}{e}\right)^{n}$ After the substituion I have got a relatively complex formula. I suppose that it may be solved by the  Hospital's rule... Is it the right method for the limit computation, if we don't want to use the Gamma function? Thanks for your help...","There is the following limit, I would like to calculate: $\lim_{n\rightarrow\infty}\frac{n!}{\left(n+1/6\right)!}$ I tried to use the Stirling approaximation formula $n!\approx\sqrt{2\pi n}\left(\frac{n}{e}\right)^{n}$ After the substituion I have got a relatively complex formula. I suppose that it may be solved by the  Hospital's rule... Is it the right method for the limit computation, if we don't want to use the Gamma function? Thanks for your help...",,"['limits', 'factorial']"
8,"Calculating the limit $\lim\limits_{a\to0} \int_0^2 {1 \over ax^4+2}\,\mathrm dx$",Calculating the limit,"\lim\limits_{a\to0} \int_0^2 {1 \over ax^4+2}\,\mathrm dx","I have some trouble calculating this integral: $$\lim_{a\to 0} \int_0^2 {1 \over ax^4+2}\,\mathrm dx$$ Got something divided by zero all of the time. Thanks in advance for any assistance!","I have some trouble calculating this integral: $$\lim_{a\to 0} \int_0^2 {1 \over ax^4+2}\,\mathrm dx$$ Got something divided by zero all of the time. Thanks in advance for any assistance!",,"['calculus', 'limits', 'integration', 'definite-integrals']"
9,On changing from '<' to '$\le$' when taking limits (with norm $|\bullet|_p$),On changing from '<' to '' when taking limits (with norm ),\le |\bullet|_p,"I'm reading Gouvêa's book on $p-$ adic, and there's one problem that I don't think I really get it. Here's a proposition, and the problem attached to it. It's on page 57, 58 of the book. Proposition 3.2.12 The image of $\mathbb{Q}$ under the inclusion $\mathbb{Q} \rightarrowtail \mathbb{Q}_p$ is a dense subset of $\mathbb{Q}_p$ . Proof Take any element $\lambda \in \mathbb{Q}_p$ , a Cauchy presentation $\{x_n \}$ of it, and any $\epsilon > 0$ . We'll prove that there's a constant sequence that belongs to $B(\lambda, \epsilon)$ . We now choose an $\epsilon'$ which is slightly less than $\epsilon$ . Since $\{ x_n \}$ is a Cauchy sequence, then there exists $N>0$ , such that for all $n, m \ge N$ , we'll have: $|x_m - x_n| < \epsilon'$ . We claim that the constant sequence $\{ x_N \}$ will indeed satisfy the requirement. For any $n \ge N$ , we'll have $|x_n - x_N| < \epsilon'$ . Taking the limit as $n \to \infty$ yields $\lim |x_n - x_N| \color{red}{\le} \epsilon' < \epsilon$ . Which means that $\{ x_N \} \in B (\lambda , \epsilon)$ . Problem 87. Why does < become $\le$ in the limit? Do we really need the business of decreasing $\epsilon$ slightly to $\epsilon'$ ? Ok, honestly, I think that we don't (but the answer states that we do). While it's true that I can find examples when the sign does change in taking limits, say: $x_n = p^n$ , then of course $|x_n|_p = |p^n|_p = \dfrac{1}{p^n}>0, \forall n$ , but $\lim |x_n|_p = 0$ . I think I can do this because 0 is in fact, a limit point of the set $\{p^z | z\in \mathbb{Z} \}$ . However, I don't think there's any example of a Cauchy sequence, such that $|x_n|_p < \epsilon, \forall n$ , but when we take the limit, it'll end up with $\lim |x_n|_p = \epsilon$ . Are there such sequences? Am I missing something here? Or is the book wrong? Thank you guys very much, And have a good day, :*","I'm reading Gouvêa's book on adic, and there's one problem that I don't think I really get it. Here's a proposition, and the problem attached to it. It's on page 57, 58 of the book. Proposition 3.2.12 The image of under the inclusion is a dense subset of . Proof Take any element , a Cauchy presentation of it, and any . We'll prove that there's a constant sequence that belongs to . We now choose an which is slightly less than . Since is a Cauchy sequence, then there exists , such that for all , we'll have: . We claim that the constant sequence will indeed satisfy the requirement. For any , we'll have . Taking the limit as yields . Which means that . Problem 87. Why does < become in the limit? Do we really need the business of decreasing slightly to ? Ok, honestly, I think that we don't (but the answer states that we do). While it's true that I can find examples when the sign does change in taking limits, say: , then of course , but . I think I can do this because 0 is in fact, a limit point of the set . However, I don't think there's any example of a Cauchy sequence, such that , but when we take the limit, it'll end up with . Are there such sequences? Am I missing something here? Or is the book wrong? Thank you guys very much, And have a good day, :*","p- \mathbb{Q} \mathbb{Q} \rightarrowtail \mathbb{Q}_p \mathbb{Q}_p \lambda \in \mathbb{Q}_p \{x_n \} \epsilon > 0 B(\lambda, \epsilon) \epsilon' \epsilon \{ x_n \} N>0 n, m \ge N |x_m - x_n| < \epsilon' \{ x_N \} n \ge N |x_n - x_N| < \epsilon' n \to \infty \lim |x_n - x_N| \color{red}{\le} \epsilon' < \epsilon \{ x_N \} \in B (\lambda , \epsilon) \le \epsilon \epsilon' x_n = p^n |x_n|_p = |p^n|_p = \dfrac{1}{p^n}>0, \forall n \lim |x_n|_p = 0 \{p^z | z\in \mathbb{Z} \} |x_n|_p < \epsilon, \forall n \lim |x_n|_p = \epsilon","['limits', 'p-adic-number-theory', 'cauchy-sequences']"
10,Show that: $\lim \limits_{n\to\infty}\frac{x_n-x_{n-1}}{n}=0 $,Show that:,\lim \limits_{n\to\infty}\frac{x_n-x_{n-1}}{n}=0 ,Here is an exercise: Suppose that $\{x_n\}$ is a sequence such that $\lim \limits_{n\to\infty}(x_n-x_{n-2})=0$.  Show that: $$\lim \limits_{n\to\infty}\frac{x_n-x_{n-1}}{n}=0 $$ Thanks.,Here is an exercise: Suppose that $\{x_n\}$ is a sequence such that $\lim \limits_{n\to\infty}(x_n-x_{n-2})=0$.  Show that: $$\lim \limits_{n\to\infty}\frac{x_n-x_{n-1}}{n}=0 $$ Thanks.,,[]
11,Indeterminate Limits,Indeterminate Limits,,I have been studying independently through various online courses and I still have trouble understanding what to do with certain limits. I am hoping for some guidance on the following two problems to help me solve them (I do not need to answer as much as help understanding where I am going). $$ \lim_{x\to 0}  \frac{\sqrt{16 + 4x} - \sqrt{16 - 4x}}{x} $$ $$ \lim_{x\to 0} \frac{\frac{1}{(x + 6)^2} - \frac{1}{36}}{x} $$ I can tell that these are both $ \frac{0}{0} $ indeterminate equations and I can simplify them in a number of ways but I cannot seem to get the 0 out of the bottom of the equation. Any help pointing me in the right direction would be greatly appreciated!,I have been studying independently through various online courses and I still have trouble understanding what to do with certain limits. I am hoping for some guidance on the following two problems to help me solve them (I do not need to answer as much as help understanding where I am going). $$ \lim_{x\to 0}  \frac{\sqrt{16 + 4x} - \sqrt{16 - 4x}}{x} $$ $$ \lim_{x\to 0} \frac{\frac{1}{(x + 6)^2} - \frac{1}{36}}{x} $$ I can tell that these are both $ \frac{0}{0} $ indeterminate equations and I can simplify them in a number of ways but I cannot seem to get the 0 out of the bottom of the equation. Any help pointing me in the right direction would be greatly appreciated!,,"['calculus', 'limits']"
12,Help finding the limit of a sum,Help finding the limit of a sum,,"Hi I'm trying to find the following limit: $$\lim_{n \rightarrow \infty} \frac{1}{n}  \sum_{j=1}^{ n } (1 - e^{\frac{-jt}{n}} )$$ expressed as a function of t. You may even be able to get it from Mathematica I don't have access to a copy at the moment. Attempts made: tried to justify using L'hopital's rule, attempted to convert to integral.","Hi I'm trying to find the following limit: expressed as a function of t. You may even be able to get it from Mathematica I don't have access to a copy at the moment. Attempts made: tried to justify using L'hopital's rule, attempted to convert to integral.",\lim_{n \rightarrow \infty} \frac{1}{n}  \sum_{j=1}^{ n } (1 - e^{\frac{-jt}{n}} ),"['limits', 'summation']"
13,"Integral, limit, sequence of functions","Integral, limit, sequence of functions",,"I'm not sure how to formulate the title, but here is a problem I've come across recently and I'm not sure how to go about solving it. Let $$P_n(x) = \frac{x^n(bx-a)^n}{n!}, \quad a,b,n \in \mathbb{N}.$$ Prove that $$\lim _{n \rightarrow \infty} \int_{0}^{\pi} P_n(x) \sin x d x = 0.$$ Could you help me with that?","I'm not sure how to formulate the title, but here is a problem I've come across recently and I'm not sure how to go about solving it. Let $$P_n(x) = \frac{x^n(bx-a)^n}{n!}, \quad a,b,n \in \mathbb{N}.$$ Prove that $$\lim _{n \rightarrow \infty} \int_{0}^{\pi} P_n(x) \sin x d x = 0.$$ Could you help me with that?",,"['real-analysis', 'sequences-and-series', 'integration', 'limits']"
14,"Use the $\varepsilon$ - $\delta$ definition to prove $\lim_{x\to\,-1}\frac{x}{2x+1}=1$",Use the  -  definition to prove,"\varepsilon \delta \lim_{x\to\,-1}\frac{x}{2x+1}=1","Use the $\varepsilon$ - $\delta$ definition of limit to prove that $\displaystyle\lim_{x\to\,-1}\frac{x}{2x+1}=1$. My working: $\left|\frac{x}{2x+1}-1\right|=\left|\frac{-x-1}{2x+1}\right|=\frac{1}{\left|2x+1\right|}\cdot \left|x+1\right|$ First restrict $x$ to  $0<\left|x+1\right|<\frac{1}{4}$ $\Rightarrow$ initial choice of $\delta=\frac{1}{4}$ $\left| 2x+1 \right|$ = $\left|2(x+1)-1\right|$ $\le$ $\left|2(x+1)\right|+\left|-1\right|$ = $2\left|x+1\right|+1$ $> 1$ Thus if $\left|x+1\right|<\frac{1}{4}$ , then $\left|\frac{x}{2x+1}-1\right|=\frac{1}{\left|2x+1\right|}.\left|x+1\right|$     $<1.\left|x+1\right|$. Therefore, $\delta = \min\{\frac{1}{4},\varepsilon\}$ $0<\left|x+1\right|<\delta$ $\Rightarrow$ $\left|\frac{x}{2x+1}-1\right|  < 1\cdot\left|x+1\right| < 1\cdot\varepsilon = \varepsilon$ Thus, the limit is 1.","Use the $\varepsilon$ - $\delta$ definition of limit to prove that $\displaystyle\lim_{x\to\,-1}\frac{x}{2x+1}=1$. My working: $\left|\frac{x}{2x+1}-1\right|=\left|\frac{-x-1}{2x+1}\right|=\frac{1}{\left|2x+1\right|}\cdot \left|x+1\right|$ First restrict $x$ to  $0<\left|x+1\right|<\frac{1}{4}$ $\Rightarrow$ initial choice of $\delta=\frac{1}{4}$ $\left| 2x+1 \right|$ = $\left|2(x+1)-1\right|$ $\le$ $\left|2(x+1)\right|+\left|-1\right|$ = $2\left|x+1\right|+1$ $> 1$ Thus if $\left|x+1\right|<\frac{1}{4}$ , then $\left|\frac{x}{2x+1}-1\right|=\frac{1}{\left|2x+1\right|}.\left|x+1\right|$     $<1.\left|x+1\right|$. Therefore, $\delta = \min\{\frac{1}{4},\varepsilon\}$ $0<\left|x+1\right|<\delta$ $\Rightarrow$ $\left|\frac{x}{2x+1}-1\right|  < 1\cdot\left|x+1\right| < 1\cdot\varepsilon = \varepsilon$ Thus, the limit is 1.",,"['real-analysis', 'limits', 'epsilon-delta']"
15,$\lim_{n\to\infty}\left(\frac{\log(n+1)}{\log n}\right)^{n}=1$,,\lim_{n\to\infty}\left(\frac{\log(n+1)}{\log n}\right)^{n}=1,Why is $$\lim_{n\to\infty}\left(\frac{\log(n+1)}{\log n}\right)^{n}=1$$ Can I compute the part inside the square bracket first? Thanks for helping.,Why is $$\lim_{n\to\infty}\left(\frac{\log(n+1)}{\log n}\right)^{n}=1$$ Can I compute the part inside the square bracket first? Thanks for helping.,,['limits']
16,"Does the limit as $(x,y) \to (1,2)$ of $3x^3-x^2 y^2$ exist?",Does the limit as  of  exist?,"(x,y) \to (1,2) 3x^3-x^2 y^2","The title says it all: does the following limit exist? $$\lim_{\large(x, y) \to (1, 2)} \; 3x^3 - x^2y^2$$ It approaches $\,-1\,$ with direct substitution, but if you approach the point with the curve $x = y^2$, you get $\,128\,$ which is not $\,-1,\,$ meaning the limit doesn't exist.  I got the problem wrong but I am curious to see if it does actually exist. I'm approaching with $\,x = y^2.\,$  So it ends up being $\;\displaystyle\lim_{y\to 2}\;3y^6-y^6,\,$ which is $128$.","The title says it all: does the following limit exist? $$\lim_{\large(x, y) \to (1, 2)} \; 3x^3 - x^2y^2$$ It approaches $\,-1\,$ with direct substitution, but if you approach the point with the curve $x = y^2$, you get $\,128\,$ which is not $\,-1,\,$ meaning the limit doesn't exist.  I got the problem wrong but I am curious to see if it does actually exist. I'm approaching with $\,x = y^2.\,$  So it ends up being $\;\displaystyle\lim_{y\to 2}\;3y^6-y^6,\,$ which is $128$.",,"['calculus', 'limits']"
17,How to calculate this limit ? $\lim_{n\to\infty}\sum_{k=1}^{2n}(-1)^k\left(\frac{k}{2n}\right)^{100}$ [closed],How to calculate this limit ?  [closed],\lim_{n\to\infty}\sum_{k=1}^{2n}(-1)^k\left(\frac{k}{2n}\right)^{100},"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question How to calculate this limit? $$\lim_{n\to\infty}\sum_{k=1}^{2n}(-1)^k\left(\frac{k}{2n}\right)^{100}$$ use integration  but difficult/","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question How to calculate this limit? $$\lim_{n\to\infty}\sum_{k=1}^{2n}(-1)^k\left(\frac{k}{2n}\right)^{100}$$ use integration  but difficult/",,['limits']
18,"Limit of $f(x,y)=\frac{~~x^3+\,y^2}{x^2+\,y}$ at $(0,0)$",Limit of  at,"f(x,y)=\frac{~~x^3+\,y^2}{x^2+\,y} (0,0)","Find the limit at the origin:   $$f(x,y)={\frac{x^3+y^2}{x^2+y}}$$ I already tried all the methods for proving that the limit is zero at the origin but had no success.","Find the limit at the origin:   $$f(x,y)={\frac{x^3+y^2}{x^2+y}}$$ I already tried all the methods for proving that the limit is zero at the origin but had no success.",,"['calculus', 'limits']"
19,How can one calculate this Limit?,How can one calculate this Limit?,,Let $f(n)$ denote the number of integer solutions of the equation $$3x^2+2xy+3y^2=n $$ How can one evaluate the limit $$\lim_{n\rightarrow\infty}\frac{f(1)+...f(n)}{n}$$ Thanks,Let $f(n)$ denote the number of integer solutions of the equation $$3x^2+2xy+3y^2=n $$ How can one evaluate the limit $$\lim_{n\rightarrow\infty}\frac{f(1)+...f(n)}{n}$$ Thanks,,['limits']
20,Computing a Double Limit,Computing a Double Limit,,"How would one compute $\lim_{\delta \rightarrow 0, k\rightarrow\infty} (1+\delta)^{ak}$, where $a$ is some positive constant? I am finding a lower-bound of the Hausdorff Dimension on a Cantor-like set and this expression appeared in my formula. Here's what I have, even though I'm not sure if I can use L'Hopital in this case (where $k, \delta$ are approaching $\infty, 0$, respectively.) $\lim (1+\delta)^{ak}= \lim e^{ak\log(1+\delta)}=\lim e^{ak\log(1+\delta)}=\lim e^\frac{a\log(1+\delta)}{\frac{1}{k}}=\lim e^\frac{-ak^2}{1+\delta}=0,$ which I find troubling since the base is always greater than 1. Would this change much if the limit as k tends to infinity is the liminf?","How would one compute $\lim_{\delta \rightarrow 0, k\rightarrow\infty} (1+\delta)^{ak}$, where $a$ is some positive constant? I am finding a lower-bound of the Hausdorff Dimension on a Cantor-like set and this expression appeared in my formula. Here's what I have, even though I'm not sure if I can use L'Hopital in this case (where $k, \delta$ are approaching $\infty, 0$, respectively.) $\lim (1+\delta)^{ak}= \lim e^{ak\log(1+\delta)}=\lim e^{ak\log(1+\delta)}=\lim e^\frac{a\log(1+\delta)}{\frac{1}{k}}=\lim e^\frac{-ak^2}{1+\delta}=0,$ which I find troubling since the base is always greater than 1. Would this change much if the limit as k tends to infinity is the liminf?",,"['calculus', 'limits']"
21,Limit of difference of two irrational functions,Limit of difference of two irrational functions,,"Firstly, this is not a homework. I just want to solve this limit for my own curiosity and self-learning. I have tried to solve this limit for 5-6 hours with no luck. Then I tried to read information about limits when there is case ""inifinity - inifinity"". Also I watched videos, but again with no luck. I think that this limit could be very difficult or very simple and my thinking is just wrong for finding it. So, it would be grateful if some of you could explain step by step solution of this limit. And there is the limit: $$ \lim_{x\rightarrow +\infty} (\sqrt[3]{x^3+3x^2} - \sqrt{x^2-2x}) $$ What I did for now. It is sad to say, but almost nothing. All my efforts where just trying to factorize, simplify, remove denominator, etc. The ""best"" achieved result was more and more polynomials and just increasing level of difficulty to solve such type of problem. I know that the answer for this limit is 2. I hope at least this will help. Thank you for your any help in advance.","Firstly, this is not a homework. I just want to solve this limit for my own curiosity and self-learning. I have tried to solve this limit for 5-6 hours with no luck. Then I tried to read information about limits when there is case ""inifinity - inifinity"". Also I watched videos, but again with no luck. I think that this limit could be very difficult or very simple and my thinking is just wrong for finding it. So, it would be grateful if some of you could explain step by step solution of this limit. And there is the limit: $$ \lim_{x\rightarrow +\infty} (\sqrt[3]{x^3+3x^2} - \sqrt{x^2-2x}) $$ What I did for now. It is sad to say, but almost nothing. All my efforts where just trying to factorize, simplify, remove denominator, etc. The ""best"" achieved result was more and more polynomials and just increasing level of difficulty to solve such type of problem. I know that the answer for this limit is 2. I hope at least this will help. Thank you for your any help in advance.",,"['calculus', 'limits', 'infinity']"
22,Find  $\lim_{x\to 0} \cos \big(\pi x^2 \csc (\frac {x} {2}) \cot (6x) \big) $,Find,\lim_{x\to 0} \cos \big(\pi x^2 \csc (\frac {x} {2}) \cot (6x) \big) ,"Find the limit $$\lim_{x\to 0} \cos \bigg(\pi x^2 \csc (\frac {x} {2}) \cot (6x) \bigg)$$ I dont even know where to get started... Some hints and solutions would be appreciated! Thanks in advance! P.S typed this on an iphone, sorry for any mistakes will edit soon. EDIT Here are my current workings. $$\lim_{x\to 0} \cos \bigg(\pi x^2 \csc (\frac {x} {2}) \cot (6x) \bigg)=\lim_{x\to 0} \cos \bigg(\pi x^2 (\frac {\cos (6x)} {\sin (\frac {x} {2}) \sin 6x}  \bigg)$$","Find the limit $$\lim_{x\to 0} \cos \bigg(\pi x^2 \csc (\frac {x} {2}) \cot (6x) \bigg)$$ I dont even know where to get started... Some hints and solutions would be appreciated! Thanks in advance! P.S typed this on an iphone, sorry for any mistakes will edit soon. EDIT Here are my current workings. $$\lim_{x\to 0} \cos \bigg(\pi x^2 \csc (\frac {x} {2}) \cot (6x) \bigg)=\lim_{x\to 0} \cos \bigg(\pi x^2 (\frac {\cos (6x)} {\sin (\frac {x} {2}) \sin 6x}  \bigg)$$",,['limits']
23,Did I underestimate the limit proof?,Did I underestimate the limit proof?,,"This is the problem: Prove that if $a_n \le b_n$ for $n \ge 1, L = \lim_{n \to \infty} a_n$ and $M = \lim_{n \to \infty} b_n$ , then $L \le M$ EDIT: Progress Proof Assume $L >M$ and $a_n \leq b_n$ , then (1) $|a_n -L| < \epsilon$ when n > $N_1$ (2) $|b_n-M| < \epsilon$ when n > $N_2$ Expanding (1) and (2) gives $L - \epsilon < a_n < \epsilon + L$ and $M - \epsilon < b_n < \epsilon + M$ Since $a_n \leq b_n$ , we have $L-\epsilon<a_n\leq b_n <\epsilon+M \implies L - \epsilon < \epsilon +M \implies L < M + 2\epsilon$ OKay I am stuck now, but I feel I am getting close EDIT: alternate proof from text Proof Let $\lim_{n\to\infty}b_n -a_n=M -L$ . Therefore for any $\epsilon > 0$ , $\exists N:$ $|b_n - a_n - (M - L)| <\epsilon$ whenever $n > N$ Take $\epsilon = L - M$ and we get $|b_n - a_n - (M - L)| <L -M$ whenever $n > N$ and since $a \leq |a|$ , we have $b_n - a_n - (M - L) < L -M \iff a_n >b_n$ , but this contradicts the assumption and therefore $L > M$ must be false","This is the problem: Prove that if for and , then EDIT: Progress Proof Assume and , then (1) when n > (2) when n > Expanding (1) and (2) gives and Since , we have OKay I am stuck now, but I feel I am getting close EDIT: alternate proof from text Proof Let . Therefore for any , whenever Take and we get whenever and since , we have , but this contradicts the assumption and therefore must be false","a_n \le b_n n \ge 1, L = \lim_{n \to \infty} a_n M = \lim_{n \to \infty} b_n L \le M L >M a_n \leq b_n |a_n -L| < \epsilon N_1 |b_n-M| < \epsilon N_2 L - \epsilon < a_n < \epsilon + L M - \epsilon < b_n < \epsilon + M a_n \leq b_n L-\epsilon<a_n\leq b_n <\epsilon+M \implies L - \epsilon < \epsilon +M \implies L < M + 2\epsilon \lim_{n\to\infty}b_n -a_n=M -L \epsilon > 0 \exists N: |b_n - a_n - (M - L)| <\epsilon n > N \epsilon = L - M |b_n - a_n - (M - L)| <L -M n > N a \leq |a| b_n - a_n - (M - L) < L -M \iff a_n >b_n L > M","['sequences-and-series', 'limits']"
24,Show $\lim_{h \to 0} \frac{f(h)+f(-h)}{h^2}=f''(0)$,Show,\lim_{h \to 0} \frac{f(h)+f(-h)}{h^2}=f''(0),"Let $f$ be a function such that $f(0)=0$ and $f$ has derivatives of all order .Show that  $$\lim_{h \to 0} \frac{f(h)+f(-h)}{h^2}=f''(0)$$ where $f''(0)$ is the second derivative of $f$ at $0$. I proceed in this way: Note that $$f''(0)=\lim_{h \to 0} \frac{f'(h)-f'(0)}{h}=\lim_{h \to 0} \frac{f'(0)-f'(-h)}{h}$$, by definition of derivative. So, L.H.S $$=\lim_{h \to 0} \frac{f(h)+f(-h)}{h^2}$$[$\frac{0}{0}$ form]$$=\lim_{h \to 0} \frac{f'(h)-f'(-h)}{2h}$$[Applying L'Hospital Rule]$$=\frac{1}{2}[\lim_{h \to 0} \frac{f'(h)-f'(0)}{h}+\lim_{h \to 0} \frac{f'(0)-f'(-h)}{h}]$$$$=\frac{1}{2}[f''(0)+f''(0)]$$$$=f''(0)=R.H.S$$. Can I write ?$$f''(0)=\lim_{h \to 0} \frac{f'(0)-f'(-h)}{h}$$","Let $f$ be a function such that $f(0)=0$ and $f$ has derivatives of all order .Show that  $$\lim_{h \to 0} \frac{f(h)+f(-h)}{h^2}=f''(0)$$ where $f''(0)$ is the second derivative of $f$ at $0$. I proceed in this way: Note that $$f''(0)=\lim_{h \to 0} \frac{f'(h)-f'(0)}{h}=\lim_{h \to 0} \frac{f'(0)-f'(-h)}{h}$$, by definition of derivative. So, L.H.S $$=\lim_{h \to 0} \frac{f(h)+f(-h)}{h^2}$$[$\frac{0}{0}$ form]$$=\lim_{h \to 0} \frac{f'(h)-f'(-h)}{2h}$$[Applying L'Hospital Rule]$$=\frac{1}{2}[\lim_{h \to 0} \frac{f'(h)-f'(0)}{h}+\lim_{h \to 0} \frac{f'(0)-f'(-h)}{h}]$$$$=\frac{1}{2}[f''(0)+f''(0)]$$$$=f''(0)=R.H.S$$. Can I write ?$$f''(0)=\lim_{h \to 0} \frac{f'(0)-f'(-h)}{h}$$",,"['calculus', 'limits', 'derivatives']"
25,"Limit of $ \int_0^a \sqrt{\frac{x^2+1}{x(a-x)}} \mathrm dx,a\rightarrow 0^+$",Limit of," \int_0^a \sqrt{\frac{x^2+1}{x(a-x)}} \mathrm dx,a\rightarrow 0^+",This question is similar to my previous one : I would like to find the limit of $$ \int_0^a \sqrt{\frac{x^2+1}{x(a-x)}} \mathrm dx$$ when $$ a\rightarrow 0^+$$ Once again it seems that $$  \int_0^a \sqrt{\frac{x^2+1}{x(a-x)}} \mathrm dx\sim_{a\rightarrow 1^+} \pi$$ We have: $$ \sqrt{\frac{x^2+1}{x(a-x)}}=\frac{2}{a}\sqrt{\frac{x^2+1}{1-(\frac{2x}{a}-1)^2}} $$ Does this help find a suitable change of variable?,This question is similar to my previous one : I would like to find the limit of $$ \int_0^a \sqrt{\frac{x^2+1}{x(a-x)}} \mathrm dx$$ when $$ a\rightarrow 0^+$$ Once again it seems that $$  \int_0^a \sqrt{\frac{x^2+1}{x(a-x)}} \mathrm dx\sim_{a\rightarrow 1^+} \pi$$ We have: $$ \sqrt{\frac{x^2+1}{x(a-x)}}=\frac{2}{a}\sqrt{\frac{x^2+1}{1-(\frac{2x}{a}-1)^2}} $$ Does this help find a suitable change of variable?,,"['integration', 'limits']"
26,Limit Evaluation,Limit Evaluation,,"Given $a>1$ and $f:\mathbb{R}\backslash{\{0}\} \rightarrow\mathbb{R}$ defined $f(x)=a^\frac{1}{x}$ how do I show that $\lim_{x \to 0^+}f(x)=\infty$? Also, is the following claim on sequences correct and can it be used somehow on the question above(by using Heine and the relationship between sequences and fucntions)? Given two sequences {$a_n$},{$b_n$} that converge to $a$ and $b$ respectively, then $a_n^{b_n}$ converges to $a^b$ as $n$ approces to $\infty$. I think about this claim because in respect to the original question I know that $a^\frac{1}{x}$ is composed of a constant function $a$ which at any point converges to a and $\frac{1}{x}$ which converges to infinity as x approaches to zero, however I'm not sure how to make the transition between sequences and fucntions in this particular case. I hope the question is clear. (By the way, this isn't an homework assignment). Thanks.","Given $a>1$ and $f:\mathbb{R}\backslash{\{0}\} \rightarrow\mathbb{R}$ defined $f(x)=a^\frac{1}{x}$ how do I show that $\lim_{x \to 0^+}f(x)=\infty$? Also, is the following claim on sequences correct and can it be used somehow on the question above(by using Heine and the relationship between sequences and fucntions)? Given two sequences {$a_n$},{$b_n$} that converge to $a$ and $b$ respectively, then $a_n^{b_n}$ converges to $a^b$ as $n$ approces to $\infty$. I think about this claim because in respect to the original question I know that $a^\frac{1}{x}$ is composed of a constant function $a$ which at any point converges to a and $\frac{1}{x}$ which converges to infinity as x approaches to zero, however I'm not sure how to make the transition between sequences and fucntions in this particular case. I hope the question is clear. (By the way, this isn't an homework assignment). Thanks.",,"['calculus', 'real-analysis', 'limits']"
27,Question Regarding Existence of One Sided Limits,Question Regarding Existence of One Sided Limits,,"In a Calculus book, I had read the following proposition: For a function $f:X\to \mathbb{R}$, $X\subseteq \mathbb{R}$ then $\lim\limits_{x\to x_0}f(x)$ exists if and only if $\lim\limits_{x\to x_0^+}f(x)$ and $\lim\limits_{x\to x_0^-}f(x)$ exist and $\lim\limits_{x\to x_0^+}f(x)=\lim\limits_{x\to x_0^-}f(x)$. It has come to my attention however that this is wrong. Let $f:\left[a,b\right] \to \mathbb{R}$, $f(x)=x$. Note that $a$ is an accumulation point (from the right) of the domain of $f$. Obviously, $\lim\limits_{x\to a^-}f(x)$ does not exist since $a$ is not an accumulation point from the left of the domain of $f$. Using the definition of a limit of a real function (or using the fact that $f$ is continuous on $a$) we can derive that $\lim\limits_{x\to a}f(x)=a$ which is a contradiction to the proposition above. My question is, in the proposition do we need to additionaly suppose that $x_0$ is an accumulation point from the right and the left of $X$? If $x_0$ is an accumulation point of $X$ only from the right and $\lim\limits_{x\to x_0^+}f(x)$ exists then is it true that $\lim\limits_{x\to x_0}f(x)$ exists and $\lim\limits_{x\to x_0}f(x)=\lim\limits_{x\to x_0^+}f(x)$?","In a Calculus book, I had read the following proposition: For a function $f:X\to \mathbb{R}$, $X\subseteq \mathbb{R}$ then $\lim\limits_{x\to x_0}f(x)$ exists if and only if $\lim\limits_{x\to x_0^+}f(x)$ and $\lim\limits_{x\to x_0^-}f(x)$ exist and $\lim\limits_{x\to x_0^+}f(x)=\lim\limits_{x\to x_0^-}f(x)$. It has come to my attention however that this is wrong. Let $f:\left[a,b\right] \to \mathbb{R}$, $f(x)=x$. Note that $a$ is an accumulation point (from the right) of the domain of $f$. Obviously, $\lim\limits_{x\to a^-}f(x)$ does not exist since $a$ is not an accumulation point from the left of the domain of $f$. Using the definition of a limit of a real function (or using the fact that $f$ is continuous on $a$) we can derive that $\lim\limits_{x\to a}f(x)=a$ which is a contradiction to the proposition above. My question is, in the proposition do we need to additionaly suppose that $x_0$ is an accumulation point from the right and the left of $X$? If $x_0$ is an accumulation point of $X$ only from the right and $\lim\limits_{x\to x_0^+}f(x)$ exists then is it true that $\lim\limits_{x\to x_0}f(x)$ exists and $\lim\limits_{x\to x_0}f(x)=\lim\limits_{x\to x_0^+}f(x)$?",,"['calculus', 'limits']"
28,Showing $\lim _{n\rightarrow \infty } \frac {S_{n}}{n^{2}} = \frac{1}{6}$,Showing,\lim _{n\rightarrow \infty } \frac {S_{n}}{n^{2}} = \frac{1}{6},I am trying to show that if the arithmetic mean of the products of all distinct pairs of positive integers whose sum is $n$ is denoted by $S_{n}$ then $$\lim _{n\rightarrow \infty } \dfrac {S_{n}}{n^{2}} = \dfrac{1}{6}$$  Solution attempt If $n$ was even then $$S_{n} = \sum _{i=1}^{i=\dfrac{n} {2}}\dfrac {\left( n-i\right) \left( n-\left( n-i\right) \right) } {\dfrac {n } {2}} = \sum _{i=1}^{i=\dfrac{n} {2}}2i\left( 1-\dfrac {i} {n}\right) $$ so  $$\lim _{n\rightarrow \infty } \dfrac {S_{n}}{n^{2}} = \lim _{n\rightarrow \infty } \sum _{i=1}^{i=\dfrac{n} {2}}\dfrac{2i}{n^{2}}\left( 1-\dfrac {i} {n}\right)$$ Similarly If $n$ was odd then  $$S_{n} = \sum _{i=1}^{i=\dfrac{\left(n+1\right)} {2}}\dfrac {\left( n-i\right) \left( n-\left( n-i\right) \right) } {\dfrac {\left(n+1\right) } {2}} = \sum _{i=1}^{i=\dfrac{\left(n+1\right)} {2}}2i\left(\dfrac {n-i} {n+1}\right)$$ so  $$\lim _{n\rightarrow \infty } \dfrac {S_{n}}{n^{2}} = \lim _{n\rightarrow \infty } \sum _{i=1}^{i=\dfrac{\left(n+1\right)} {2}}\dfrac{2i}{n^{2}}\left(\dfrac {n-i} {n+1}\right)$$ I am unsure how to proceed from here to show the result. Any help would be much appreciated.,I am trying to show that if the arithmetic mean of the products of all distinct pairs of positive integers whose sum is $n$ is denoted by $S_{n}$ then $$\lim _{n\rightarrow \infty } \dfrac {S_{n}}{n^{2}} = \dfrac{1}{6}$$  Solution attempt If $n$ was even then $$S_{n} = \sum _{i=1}^{i=\dfrac{n} {2}}\dfrac {\left( n-i\right) \left( n-\left( n-i\right) \right) } {\dfrac {n } {2}} = \sum _{i=1}^{i=\dfrac{n} {2}}2i\left( 1-\dfrac {i} {n}\right) $$ so  $$\lim _{n\rightarrow \infty } \dfrac {S_{n}}{n^{2}} = \lim _{n\rightarrow \infty } \sum _{i=1}^{i=\dfrac{n} {2}}\dfrac{2i}{n^{2}}\left( 1-\dfrac {i} {n}\right)$$ Similarly If $n$ was odd then  $$S_{n} = \sum _{i=1}^{i=\dfrac{\left(n+1\right)} {2}}\dfrac {\left( n-i\right) \left( n-\left( n-i\right) \right) } {\dfrac {\left(n+1\right) } {2}} = \sum _{i=1}^{i=\dfrac{\left(n+1\right)} {2}}2i\left(\dfrac {n-i} {n+1}\right)$$ so  $$\lim _{n\rightarrow \infty } \dfrac {S_{n}}{n^{2}} = \lim _{n\rightarrow \infty } \sum _{i=1}^{i=\dfrac{\left(n+1\right)} {2}}\dfrac{2i}{n^{2}}\left(\dfrac {n-i} {n+1}\right)$$ I am unsure how to proceed from here to show the result. Any help would be much appreciated.,,"['real-analysis', 'sequences-and-series', 'limits']"
29,When I can't convert a piece of a limit in a notable limit?,When I can't convert a piece of a limit in a notable limit?,,When I solve limits I often convert a block in a notable limit multiplying and dividing it by the same quantity. For example: $$ \lim_{x \to 0} \frac{e^{\sin(x)}-1}{x} = \lim_{x \to 0} \frac{e^{\frac{\sin(x)}{x}x}-1}{x} = \lim_{x \to 0} \frac{e^{x}-1}{x} = 1 $$ I've noticed that I can't always apply this. For example: $$ \lim_{x \to 0} \frac{x^3+9x-9 \tan(x)}{-8x^3} = \frac{1}{4}\neq \lim_{x \to 0} \frac{x^3+9x-9 \frac{\tan(x)}{x}x}{-8x^3} = \lim_{x \to 0} \frac{x^3+9x-9x}{-8x^3}= -\frac{1}{8} $$ What's wrong in that passage? What rule am I violating? Thanks,When I solve limits I often convert a block in a notable limit multiplying and dividing it by the same quantity. For example: $$ \lim_{x \to 0} \frac{e^{\sin(x)}-1}{x} = \lim_{x \to 0} \frac{e^{\frac{\sin(x)}{x}x}-1}{x} = \lim_{x \to 0} \frac{e^{x}-1}{x} = 1 $$ I've noticed that I can't always apply this. For example: $$ \lim_{x \to 0} \frac{x^3+9x-9 \tan(x)}{-8x^3} = \frac{1}{4}\neq \lim_{x \to 0} \frac{x^3+9x-9 \frac{\tan(x)}{x}x}{-8x^3} = \lim_{x \to 0} \frac{x^3+9x-9x}{-8x^3}= -\frac{1}{8} $$ What's wrong in that passage? What rule am I violating? Thanks,,"['analysis', 'limits']"
30,Is this use of L'Hôpital's rule incorrect?,Is this use of L'Hôpital's rule incorrect?,,"For the question: Find the $\lim_{x \to 1}  \frac{3x^3 - 5x^2 + x + 1}{x^2 - 2x + 1}$ I couldn't see how to factorize the numerator and noticed that it was in intermediate form $\frac{0}{0}$ so I applied L'Hôpital's rule twice which left me with: $\frac{18x - 10}{2}$ which I evaluated at $x \rightarrow 1$ to get the limit of 4. I then typed the problem into wolfram alpha and noticed that, although I got the same answer, WA had factored out $x^2 - 2x +1$ which cancels with the denominator and then the function can be evaluated to equal 4 without using L'Hôpital's. I was wondering if my use of L'Hôpital's rule here is unjustified, and if so, what would have been the method to factor the numerator?","For the question: Find the $\lim_{x \to 1}  \frac{3x^3 - 5x^2 + x + 1}{x^2 - 2x + 1}$ I couldn't see how to factorize the numerator and noticed that it was in intermediate form $\frac{0}{0}$ so I applied L'Hôpital's rule twice which left me with: $\frac{18x - 10}{2}$ which I evaluated at $x \rightarrow 1$ to get the limit of 4. I then typed the problem into wolfram alpha and noticed that, although I got the same answer, WA had factored out $x^2 - 2x +1$ which cancels with the denominator and then the function can be evaluated to equal 4 without using L'Hôpital's. I was wondering if my use of L'Hôpital's rule here is unjustified, and if so, what would have been the method to factor the numerator?",,"['calculus', 'limits']"
31,Finding $\lim\limits_{x\rightarrow 2} \frac{x^3-8}{x^2-x-2} $,Finding,\lim\limits_{x\rightarrow 2} \frac{x^3-8}{x^2-x-2} ,"I've done so many limit problems in calculus lately, but I can't wrap my mind around how to simplify this one in order to solve it: $$ \lim_{x\rightarrow 2} \dfrac{x^3-8}{x^2-x-2}  $$ I understand the $x^3-8$ factors down to $(x-2)(x^2+2x+4)$, but that still leaves us with $$ \lim_{x\rightarrow 2} \dfrac{(x-2)(x^2+2x+4)}{x^2-x-2},  $$ which I can't seem to find a way to simplify so that the denominator is not equal to 0. In case anyone figures out themselves, the answer is 4 (I was given the answer - this is on a review sheet for an upcoming exam).  Also, I tagged this as homework, even though it is not technically homework. So if anyone could help point me in the right direction here, that would be very helpful.","I've done so many limit problems in calculus lately, but I can't wrap my mind around how to simplify this one in order to solve it: $$ \lim_{x\rightarrow 2} \dfrac{x^3-8}{x^2-x-2}  $$ I understand the $x^3-8$ factors down to $(x-2)(x^2+2x+4)$, but that still leaves us with $$ \lim_{x\rightarrow 2} \dfrac{(x-2)(x^2+2x+4)}{x^2-x-2},  $$ which I can't seem to find a way to simplify so that the denominator is not equal to 0. In case anyone figures out themselves, the answer is 4 (I was given the answer - this is on a review sheet for an upcoming exam).  Also, I tagged this as homework, even though it is not technically homework. So if anyone could help point me in the right direction here, that would be very helpful.",,"['calculus', 'limits']"
32,Help me understand limits,Help me understand limits,,"Good day I'm currently doing some math homework (don't worry I won't ask anyone to solve anything) and I don't think I'm understanding limits correctly. More precisely how the l'Hôpital rule works. I know I can/should be able to apply it if it is either $\infty/\infty$ or $0/0$ but I was wondering does it have anything to do with $0/\infty$ or $\infty/0$? Anything you think might help me better understand limits would be appreciated. P.S. I'm sorry if this is a simple question, math is not my strongest point.","Good day I'm currently doing some math homework (don't worry I won't ask anyone to solve anything) and I don't think I'm understanding limits correctly. More precisely how the l'Hôpital rule works. I know I can/should be able to apply it if it is either $\infty/\infty$ or $0/0$ but I was wondering does it have anything to do with $0/\infty$ or $\infty/0$? Anything you think might help me better understand limits would be appreciated. P.S. I'm sorry if this is a simple question, math is not my strongest point.",,"['calculus', 'limits']"
33,Does $\lim_{x\rightarrow 0}\frac{c}{|x|}$ exist?,Does  exist?,\lim_{x\rightarrow 0}\frac{c}{|x|},"Does   $\lim_{x\rightarrow 0}\frac{c}{|x|}$ exist? I was under the impression that it does, because, even though $c/0$ is undefined, $c/|x|$ goes to infinity as $x$ approaches $0$. However, my calcus textbook says that $$\lim_{x\rightarrow a}\frac{f(x)}{g(x)}=\frac{\lim_{x\rightarrow a} f(x)}{\lim_{x\rightarrow a} g(x)} \operatorname{ if } g(x) \neq0$$ Which seems to suggest that $\lim_{x\rightarrow a}\frac{c}{|x|}$ would likewise not exist if $a=0$.","Does   $\lim_{x\rightarrow 0}\frac{c}{|x|}$ exist? I was under the impression that it does, because, even though $c/0$ is undefined, $c/|x|$ goes to infinity as $x$ approaches $0$. However, my calcus textbook says that $$\lim_{x\rightarrow a}\frac{f(x)}{g(x)}=\frac{\lim_{x\rightarrow a} f(x)}{\lim_{x\rightarrow a} g(x)} \operatorname{ if } g(x) \neq0$$ Which seems to suggest that $\lim_{x\rightarrow a}\frac{c}{|x|}$ would likewise not exist if $a=0$.",,"['functions', 'limits']"
34,Limit proof as $x$ goes to infinity,Limit proof as  goes to infinity,x,"Trying to help my girlfriend since we're both in Calculus this semester, but my class never went this in depth into limit proofs. A little help so I can pass it on? (: The limit at infinity $\lim_{x\to\infty} f(x) = L$ means that for any   $\varepsilon > 0$, there exists $N > 0$ such that $|f(x) - L| < \varepsilon$ whenever $x > N$. Use this definition to prove the following statements. $\lim\limits_{x \to +\infty} \frac{10}{x} = 0.$ $\lim\limits_{x \to +\infty} \frac{2x+1}{x} = 2.$ Thanks in advance!","Trying to help my girlfriend since we're both in Calculus this semester, but my class never went this in depth into limit proofs. A little help so I can pass it on? (: The limit at infinity $\lim_{x\to\infty} f(x) = L$ means that for any   $\varepsilon > 0$, there exists $N > 0$ such that $|f(x) - L| < \varepsilon$ whenever $x > N$. Use this definition to prove the following statements. $\lim\limits_{x \to +\infty} \frac{10}{x} = 0.$ $\lim\limits_{x \to +\infty} \frac{2x+1}{x} = 2.$ Thanks in advance!",,"['calculus', 'limits']"
35,Are these two sets same,Are these two sets same,,"I have a question about whether the following two sets on the two sides are the same: $$ \lim_{a \rightarrow \infty} \, \limsup_{n \rightarrow \infty} \, \{ x \in S \, | \, f_n(x)  > a \} = \left\{ x \in S \, \Bigm| \, \lim_{n \rightarrow \infty} f_n(x) =\infty \right\}\quad ? $$ where $\{f_n\}$ is a sequence of real-valued functions defined on a set $S$. How can you explain it? Thanks in advance!","I have a question about whether the following two sets on the two sides are the same: $$ \lim_{a \rightarrow \infty} \, \limsup_{n \rightarrow \infty} \, \{ x \in S \, | \, f_n(x)  > a \} = \left\{ x \in S \, \Bigm| \, \lim_{n \rightarrow \infty} f_n(x) =\infty \right\}\quad ? $$ where $\{f_n\}$ is a sequence of real-valued functions defined on a set $S$. How can you explain it? Thanks in advance!",,"['limits', 'functions', 'elementary-set-theory', 'limsup-and-liminf']"
36,Proof: Limit superior intersection,Proof: Limit superior intersection,,How to prove $\limsup(\{A_n \cup B_n\}) = \limsup(\{A_n\}) \cup \limsup(\{B_n\})$? Thanks!,How to prove $\limsup(\{A_n \cup B_n\}) = \limsup(\{A_n\}) \cup \limsup(\{B_n\})$? Thanks!,,['elementary-set-theory']
37,"Proving that a sequence converges, with the epsilon definition","Proving that a sequence converges, with the epsilon definition",,"I want to prove that the following sequence $x_n = \frac{3+\sqrt{n}}{2n-\sqrt{n}}$ converges and has a limit. $$\lim_{n \to \infty} \frac{3+\sqrt{n}}{2n-\sqrt{n}} \Rightarrow \lim_{n \to \infty}  \frac{\frac{3}{n}+\frac{\sqrt{n}}{n}}{\frac{2n}{n}-\frac{\sqrt{n}}{n}} \Rightarrow \lim_{n \to \infty}  \frac{0+0}{2+0}=0$$ I want to show that for any $\epsilon$ >0 there exists an N such that for all n > N, $|x_n-L|$ < $\epsilon$ where L = 0 Let $\epsilon$ >0. I want to find N such that for all n>N, $|x_0-0 |$ < $\epsilon$ $$|x_n-0| =|\frac{3+\sqrt{n}}{2n-\sqrt{n}}-0| = \frac{3+\sqrt{n}}{2n-\sqrt{n}} < \epsilon$$ I now choose N≥max( $\frac{6}{\epsilon},\frac{4}{\epsilon^2}$ ), so for all n>1, then if: $$n≥N \Rightarrow \frac{3+\sqrt{n}}{2n-\sqrt{n}} <\frac{3+\sqrt{n}}{n}= \frac{3}{n}+\frac{1}{\sqrt{n}}< \frac{3}{\frac{6}{\epsilon}} +\frac{1}{\sqrt{\frac{4}{\epsilon^2}}}=\frac{\epsilon}{2}+\frac{\epsilon}{2} = \epsilon $$ Is this the correct way to do it? I know I made a mistake,  because my TA told me that it seemed like a rough draft, but unfortunately didn't have enough time to explain it further.","I want to prove that the following sequence converges and has a limit. I want to show that for any >0 there exists an N such that for all n > N, < where L = 0 Let >0. I want to find N such that for all n>N, < I now choose N≥max( ), so for all n>1, then if: Is this the correct way to do it? I know I made a mistake,  because my TA told me that it seemed like a rough draft, but unfortunately didn't have enough time to explain it further.","x_n = \frac{3+\sqrt{n}}{2n-\sqrt{n}} \lim_{n \to \infty} \frac{3+\sqrt{n}}{2n-\sqrt{n}} \Rightarrow \lim_{n \to \infty}  \frac{\frac{3}{n}+\frac{\sqrt{n}}{n}}{\frac{2n}{n}-\frac{\sqrt{n}}{n}} \Rightarrow \lim_{n \to \infty}  \frac{0+0}{2+0}=0 \epsilon |x_n-L| \epsilon \epsilon |x_0-0 | \epsilon |x_n-0| =|\frac{3+\sqrt{n}}{2n-\sqrt{n}}-0| = \frac{3+\sqrt{n}}{2n-\sqrt{n}} < \epsilon \frac{6}{\epsilon},\frac{4}{\epsilon^2} n≥N \Rightarrow \frac{3+\sqrt{n}}{2n-\sqrt{n}} <\frac{3+\sqrt{n}}{n}= \frac{3}{n}+\frac{1}{\sqrt{n}}< \frac{3}{\frac{6}{\epsilon}} +\frac{1}{\sqrt{\frac{4}{\epsilon^2}}}=\frac{\epsilon}{2}+\frac{\epsilon}{2} = \epsilon ","['real-analysis', 'sequences-and-series', 'limits', 'solution-verification', 'epsilon-delta']"
38,Two Limits involving an integral.,Two Limits involving an integral.,,"Let $$I_n = \int_{0}^{1}\frac{x^{n-1}-x^{n+p-1}}{(1+x^{n})(1+x^{n+p})} \, \mathrm{d}x,$$ where $p$ is a positive natural number. Show that: $$\lim_{n \to \infty} n^2 I_n=p\ln{2} \qquad\text{and}\qquad \lim_{n \to \infty} \left({\frac{n^2I_n}{p\ln{2}}}\right)^n=e^{-p} $$ For the first one I substituted $t=x^n$ and we get: $$ n^2I_n=n\int_{0}^{1}\frac{1-x^{\frac{p}{n}}}{(1+x)(1+x^{1+\frac{p}{n}})} \, \mathrm{d}x .$$ Now I argue that $ x^{1+\frac{p}{n}} $ converges uniformly to $x$ on $[0,1]$ so we can substitute it with $x$ and in the limit they will be equal. I do have a rigorous proof for than using the definition for uniform convergence and then making an inequality involving $I_n$ , but its just too long for this post and I am also not sure its correct. So please tell me if such argument is possible. In the end we have: $$ n^2I_n = n\int_{0}^{1}\frac{1-x^{\frac{p}{n}}}{(1+x)^2} \, \mathrm{d}x $$ Now $\frac{1}{(1+x)^2}$ is the diff power series $\sum_{k\ge0} x^k$ , hence after multiplying by $1-x^{\frac{p}{n}}$ and integrating we get: $$ \lim_{n \to \infty}p\sum_{k\ge1} \frac{(-1)^{k+1}}{k+\frac{p}{n}}$$ and by the Weierstrass M-test, using the alternating harmonic series as for $M_n$ , we get that the limit is equal to $p\ln{2}$ If I try the same trick for the second limit (writing the integral as a series) I get a different result, namely $e^{\frac{-p\pi^2}{12\ln{2}}}$ ... It has to do with my shady argument above, that maybe works only for the first limit. Please help :(","Let where is a positive natural number. Show that: For the first one I substituted and we get: Now I argue that converges uniformly to on so we can substitute it with and in the limit they will be equal. I do have a rigorous proof for than using the definition for uniform convergence and then making an inequality involving , but its just too long for this post and I am also not sure its correct. So please tell me if such argument is possible. In the end we have: Now is the diff power series , hence after multiplying by and integrating we get: and by the Weierstrass M-test, using the alternating harmonic series as for , we get that the limit is equal to If I try the same trick for the second limit (writing the integral as a series) I get a different result, namely ... It has to do with my shady argument above, that maybe works only for the first limit. Please help :(","I_n = \int_{0}^{1}\frac{x^{n-1}-x^{n+p-1}}{(1+x^{n})(1+x^{n+p})} \, \mathrm{d}x, p \lim_{n \to \infty} n^2 I_n=p\ln{2} \qquad\text{and}\qquad \lim_{n \to \infty} \left({\frac{n^2I_n}{p\ln{2}}}\right)^n=e^{-p}  t=x^n  n^2I_n=n\int_{0}^{1}\frac{1-x^{\frac{p}{n}}}{(1+x)(1+x^{1+\frac{p}{n}})} \, \mathrm{d}x .  x^{1+\frac{p}{n}}  x [0,1] x I_n  n^2I_n = n\int_{0}^{1}\frac{1-x^{\frac{p}{n}}}{(1+x)^2} \, \mathrm{d}x  \frac{1}{(1+x)^2} \sum_{k\ge0} x^k 1-x^{\frac{p}{n}}  \lim_{n \to \infty}p\sum_{k\ge1} \frac{(-1)^{k+1}}{k+\frac{p}{n}} M_n p\ln{2} e^{\frac{-p\pi^2}{12\ln{2}}}","['real-analysis', 'integration', 'limits']"
39,Convergence of a sum as limit tends to infinity that seems to be harmonic series,Convergence of a sum as limit tends to infinity that seems to be harmonic series,,"I have come across a mathematical problem that is to evaluate the expression: $$ lim_{n\rightarrow\infty}  \left\{\frac{1}{\sqrt{2n-1^2}}+\frac{1}{\sqrt{4n-2^2}}+\frac{1}{\sqrt{6n-3^2}}+...+\frac{1}{\sqrt{2n^2-n^2}}\right\}\tag1$$ and I have considered the following approach: $$ y=lim_{n\rightarrow\infty} \left\{{\frac{1}{n}}\left[\frac{1}{\sqrt{\frac{2}{n}-\frac{1}{n^2}}}+\frac{1}{\sqrt{\frac{4}{n}-\frac{4}{n^2}}}+\frac{1}{\sqrt{\frac{6}{n}-\frac{9}{n^2}}}+...1\right]\right\} \tag2$$ $$ y=lim_{n\rightarrow\infty}\left\{\Sigma_{i=1}^{n}\frac{1}{\sqrt{\frac{2}{i}-\frac{1}{i^2}}}\frac{1}{i}\right\} \tag3$$ $$ y=\int_0^1f(x)dx \tag4$$ where it is considered that $$f(x)=\frac{1}{\sqrt{2x-x^2}}\tag5$$ and $a=0$ while $b=1$ . Integrating thus, we get: $$y=\int_0^1\frac{dx}{\sqrt{2x-x^2}}=[sin^{-1}(x-1)]_0^1=\pi/2\tag6$$ Therefore, the integration says that the sum up to infinite series is $\pi/2$ . But I have a query here, which is, if I write the terms of the series, this is what I get here: $$ y=lim_{n\rightarrow\infty}\left\{{1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\frac{1}{5}}+...\frac{1}{n}\right\} \tag7$$ Isn't this a harmonic series? But if so, harmonic series tend to diverge, then how can the infinite sum above be integrated and summed up to a definite value as it has converged? I am being confused.","I have come across a mathematical problem that is to evaluate the expression: and I have considered the following approach: where it is considered that and while . Integrating thus, we get: Therefore, the integration says that the sum up to infinite series is . But I have a query here, which is, if I write the terms of the series, this is what I get here: Isn't this a harmonic series? But if so, harmonic series tend to diverge, then how can the infinite sum above be integrated and summed up to a definite value as it has converged? I am being confused.","
lim_{n\rightarrow\infty}  \left\{\frac{1}{\sqrt{2n-1^2}}+\frac{1}{\sqrt{4n-2^2}}+\frac{1}{\sqrt{6n-3^2}}+...+\frac{1}{\sqrt{2n^2-n^2}}\right\}\tag1 
y=lim_{n\rightarrow\infty} \left\{{\frac{1}{n}}\left[\frac{1}{\sqrt{\frac{2}{n}-\frac{1}{n^2}}}+\frac{1}{\sqrt{\frac{4}{n}-\frac{4}{n^2}}}+\frac{1}{\sqrt{\frac{6}{n}-\frac{9}{n^2}}}+...1\right]\right\}
\tag2 
y=lim_{n\rightarrow\infty}\left\{\Sigma_{i=1}^{n}\frac{1}{\sqrt{\frac{2}{i}-\frac{1}{i^2}}}\frac{1}{i}\right\}
\tag3 
y=\int_0^1f(x)dx
\tag4 f(x)=\frac{1}{\sqrt{2x-x^2}}\tag5 a=0 b=1 y=\int_0^1\frac{dx}{\sqrt{2x-x^2}}=[sin^{-1}(x-1)]_0^1=\pi/2\tag6 \pi/2 
y=lim_{n\rightarrow\infty}\left\{{1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\frac{1}{5}}+...\frac{1}{n}\right\}
\tag7","['integration', 'sequences-and-series', 'limits', 'convergence-divergence', 'riemann-sum']"
40,Tricky Integral with parameter with difficult limits,Tricky Integral with parameter with difficult limits,,"I had to calculate: $$\int_0^\infty\textrm{d}x\,\frac{1-\exp(-t)}{t}\cos(t)$$ My approach was to define a function with parameter: $$\mathcal{F}(a)=\int_0^\infty\textrm{d}x\,\frac{1-\exp(-a t)}{t}\cos(t):=\int_0^\infty\textrm{d}x\,f(x,a)$$ Then we can consider: $$\partial_af(x,a)=\exp(-a t) \cos(t)$$ Which is clearly dominated for $a\in[a_0,\infty[$ (for $a_0>0$ ) by a function: $\exp(-a_0t)$ . Then there is a theorem that states that if there exists some $a_1$ for which: $$\mathcal{F}(a_1)\; \textrm{is well defined} \quad \textrm{and}\quad \partial_af(x,a)\;\textrm{is dominated}$$ Then $\mathcal{F}(a)$ can be redefined to: $$\mathcal{F}(a)=\mathcal{F}(a_1)+\int_{a_1}^{a}\textrm{d}y\int_0^\infty\textrm{d}x\;\partial_yf(x,y)$$ Which is then well defined on $a\in[a_0,\infty[$ . Then by exact calculation: $$\int_0^\infty\textrm{d}x\;\exp(-a t) \cos(t)=\frac{a}{1+a^2}$$ And now integrating I finally got expression: $$\mathcal{F}(a)=\frac{1}{2}\log(1+a^2)+C$$ And here emerges the problem to find the value of $C$ . Then the final integral would be for $\mathcal{F}(1)$ . With Mathematica I got that $\mathcal{F}(1)=\frac{1}{2}\log(2)$ therefore $C=0$ and if someone could prove that: $$\lim_{a\to0}\int_0^\infty\textrm{d}x\;f(x,a)=\int_0^\infty\textrm{d}x\;\lim_{a\to0}f(x,a)$$ I would be very grateful, because then $\lim_{a\to0}\;f(x,a)=0$ and $C=0$ .","I had to calculate: My approach was to define a function with parameter: Then we can consider: Which is clearly dominated for (for ) by a function: . Then there is a theorem that states that if there exists some for which: Then can be redefined to: Which is then well defined on . Then by exact calculation: And now integrating I finally got expression: And here emerges the problem to find the value of . Then the final integral would be for . With Mathematica I got that therefore and if someone could prove that: I would be very grateful, because then and .","\int_0^\infty\textrm{d}x\,\frac{1-\exp(-t)}{t}\cos(t) \mathcal{F}(a)=\int_0^\infty\textrm{d}x\,\frac{1-\exp(-a t)}{t}\cos(t):=\int_0^\infty\textrm{d}x\,f(x,a) \partial_af(x,a)=\exp(-a t) \cos(t) a\in[a_0,\infty[ a_0>0 \exp(-a_0t) a_1 \mathcal{F}(a_1)\; \textrm{is well defined} \quad \textrm{and}\quad \partial_af(x,a)\;\textrm{is dominated} \mathcal{F}(a) \mathcal{F}(a)=\mathcal{F}(a_1)+\int_{a_1}^{a}\textrm{d}y\int_0^\infty\textrm{d}x\;\partial_yf(x,y) a\in[a_0,\infty[ \int_0^\infty\textrm{d}x\;\exp(-a t) \cos(t)=\frac{a}{1+a^2} \mathcal{F}(a)=\frac{1}{2}\log(1+a^2)+C C \mathcal{F}(1) \mathcal{F}(1)=\frac{1}{2}\log(2) C=0 \lim_{a\to0}\int_0^\infty\textrm{d}x\;f(x,a)=\int_0^\infty\textrm{d}x\;\lim_{a\to0}f(x,a) \lim_{a\to0}\;f(x,a)=0 C=0","['limits', 'convergence-divergence', 'improper-integrals']"
41,Does $\ln(\frac{2x}{\ln(2x+1)})\sim W(x)$?,Does ?,\ln(\frac{2x}{\ln(2x+1)})\sim W(x),"The function $f(x)= \ln(\frac{2x}{ln(2x+1)})$ when plotted is similar to the plot of the Lambert W function. The Wolfram Alpha says that the limit $\lim_{x\rightarrow\infty} \frac{W(x)}{f(x)}=1$ , but on the graph it seems that the limit doesn't reach one. Someone can explain if the limit is really equal to $1$ or WFA is incorrect? An approach is to consider the function $y=\frac{W(x)}{f(x)}$ and see that $$yf(x)e^{yf(x)}=x$$ We can do the limit as $x\rightarrow\infty$ but it seems to make the problem a little bit too complex. I will be grateful if someone can help me.","The function when plotted is similar to the plot of the Lambert W function. The Wolfram Alpha says that the limit , but on the graph it seems that the limit doesn't reach one. Someone can explain if the limit is really equal to or WFA is incorrect? An approach is to consider the function and see that We can do the limit as but it seems to make the problem a little bit too complex. I will be grateful if someone can help me.",f(x)= \ln(\frac{2x}{ln(2x+1)}) \lim_{x\rightarrow\infty} \frac{W(x)}{f(x)}=1 1 y=\frac{W(x)}{f(x)} yf(x)e^{yf(x)}=x x\rightarrow\infty,"['limits', 'lambert-w']"
42,On kernels and stalks of sheaves.,On kernels and stalks of sheaves.,,"Suppose we're given sheaves $F,G$ on a space $X$ and a morphism of sheaves (of abelian groups) $\phi:F\to G$ . I want to prove two things : the presheaf $\ker \phi$ , defined by $(\ker\phi)(U):=\ker(\phi_U:FU\to GU)$ is also a sheaf. $(\ker\phi)_p \simeq \ker(\phi_p)$ I can do 1. by actually procing the axioms for a sheaf with the presheaf $\ker \phi$ . But I know this should follow formally from the fact sheafification $\tilde{\,}:Psh(X)\to Sh(X)$ and the forgetful functor or inclusion $i:Sh(X)\to Psh(X)$ form a pair of adjoint functors, $\tilde{}$ being left adjoint to $i$ . Because then $i$ preserves limits, and kernels are limits but the precise way to conclude here isn't clear, writing it down is a bit awkward. So $\ker \phi$ is the limit of $F\rightrightarrows G$ (taken in the category of presheaves?) where the arrows are given by $\phi$ and $0$ . Saying $i$ commutes with limits, would mean $i (\lim F\rightrightarrows G )\simeq \lim iF\rightrightarrows iG$ but I don't know à priori that this limit is a sheaf, in fact this is what I want to prove so writing $i(\lim...)$ just doesn't make sense, this is I what I mean when I say the setup feels awkard. I have a similar issue with 2. : writing down an actual map giving the isomorphism is ok, but I feel like this should formally follow from the fact that the functor $_p$ ""stalk at $p$ "" is a filtered colimit and $\ker$ is a limit, these should commute. But I ran into similar issues, when writing it down. Any help would be deeply appreciated.","Suppose we're given sheaves on a space and a morphism of sheaves (of abelian groups) . I want to prove two things : the presheaf , defined by is also a sheaf. I can do 1. by actually procing the axioms for a sheaf with the presheaf . But I know this should follow formally from the fact sheafification and the forgetful functor or inclusion form a pair of adjoint functors, being left adjoint to . Because then preserves limits, and kernels are limits but the precise way to conclude here isn't clear, writing it down is a bit awkward. So is the limit of (taken in the category of presheaves?) where the arrows are given by and . Saying commutes with limits, would mean but I don't know à priori that this limit is a sheaf, in fact this is what I want to prove so writing just doesn't make sense, this is I what I mean when I say the setup feels awkard. I have a similar issue with 2. : writing down an actual map giving the isomorphism is ok, but I feel like this should formally follow from the fact that the functor ""stalk at "" is a filtered colimit and is a limit, these should commute. But I ran into similar issues, when writing it down. Any help would be deeply appreciated.","F,G X \phi:F\to G \ker \phi (\ker\phi)(U):=\ker(\phi_U:FU\to GU) (\ker\phi)_p \simeq \ker(\phi_p) \ker \phi \tilde{\,}:Psh(X)\to Sh(X) i:Sh(X)\to Psh(X) \tilde{} i i \ker \phi F\rightrightarrows G \phi 0 i i (\lim F\rightrightarrows G )\simeq \lim iF\rightrightarrows iG i(\lim...) _p p \ker","['limits', 'category-theory', 'sheaf-theory', 'limits-colimits']"
43,Does this prove that the factorial grows faster than the exponential?,Does this prove that the factorial grows faster than the exponential?,,"I want to prove that the factorial grows faster than the exponential function. First, I introduce the ratio $$L = \frac{n!}{e^n}.$$ Then, I introduce another ratio : $$\frac{(n+1)!}{e^{n+1}} = \frac{(n+1) \cdot n!}{e \cdot e^n}     = \frac{(n+1)}{e} L.$$ When the value of $n$ take values bigger and bigger and $L$ gets bigger and bigger. In other words, $$\lim_{n \to \infty} \frac{(n+1)}{e} = \infty,$$ meaning that $L$ is divergent. Thus $n!$ grows faster than the exponential.","I want to prove that the factorial grows faster than the exponential function. First, I introduce the ratio Then, I introduce another ratio : When the value of take values bigger and bigger and gets bigger and bigger. In other words, meaning that is divergent. Thus grows faster than the exponential.","L = \frac{n!}{e^n}. \frac{(n+1)!}{e^{n+1}} = \frac{(n+1) \cdot n!}{e \cdot e^n}
    = \frac{(n+1)}{e} L. n L \lim_{n \to \infty} \frac{(n+1)}{e} = \infty, L n!","['limits', 'solution-verification', 'proof-writing', 'exponential-function', 'factorial']"
44,if $\lim\limits_{n \to \infty} b_n =0 $ then how to prove that $\lim\limits_{n \to \infty} \sum\limits_{k =1} ^n \frac{b_k}{n+1-k}=0$,if  then how to prove that,\lim\limits_{n \to \infty} b_n =0  \lim\limits_{n \to \infty} \sum\limits_{k =1} ^n \frac{b_k}{n+1-k}=0,"in Problems in Mathematical Analysis I problem 2.3.16 a), if $\lim\limits_{n \to \infty}a_n =a$ , then find $\lim\limits_{n \to \infty} \sum\limits_{k=1 }^n \frac{a_k}{(n+1-k)(n+2-k)}$ The proof that was on the answers:- by Toeplitz theorem let $c_{n,k} =\frac{1}{(n+1-k)(n+2-k)}$ and the answer is $a$ , here I didn't understand how this is possible because one condition for Toeplitz theorem is $ \lim\limits_{n \to \infty} c_{n,k} =0 \ \forall k \in \mathbb{N}$ doesn't hold if $k=n $ for example $\textbf{Toeplitz theorem :}$ let $\{ c_{n,k}: 1\leq k \leq n \}$ be an array of real numbers such that: 1- $ \lim\limits_{n \to \infty} c_{n,k} =0 \ \forall k \in \mathbb{N}$ 2- $\sum\limits_{k=1} ^{n} c_{n,k} \to 1$ as $n\to \infty$ 3- There is exist $C \in \mathbb{R}$ such that for all positive integer $n$ $\sum\limits_{k=1} ^{n} |c_{n,k}| \leq C$ then for any converging sequence $ \{a_n \}$ the sequence $\{ b_n \}$ given by $b_n:=\sum\limits_{k=1} ^{n} c_{n,k} a_k $ is convergent and $\lim\limits_{n \to \infty}b_n= \lim\limits_{n \to \infty}a_n$ $\textbf{ My Proof:-}$ First I have to prove  that the sequence $\{ b_n\}$ is convergent this can be shown by considering the sum $s_n =\sum\limits_{k=1} ^{n} |c_{n,k} a_k| $ since the terms of the sum consist only positive number and since $a_n$ is convergent sequence then there exist some $M \in  \mathbb{R} $ such that $a_k\leq M \ \forall k $ then the sequence $s_n <M ,  \sum\limits_{k=1} ^{n} |c_{n,k}| \leq C M $ then the sequence $s_n $ converge then $b_n$ converge Now lets proof that $\lim\limits_{n \to \infty}b_n= \lim\limits_{n \to \infty}a_n$ first let $a:=\lim\limits_{n \to \infty}a_n$ since $\sum\limits_{k=1} ^{n} c_{n,k} \to 1$ as $n\to \infty $ then I need to prove that $ \sum\limits_{k=1} ^{n} c_{n,k} a_k-a \sum\limits_{k=1} ^{n}  c_{n,k} \to 0 \text{ as } n \to \infty  $ $$S_n:=\sum\limits_{k=1} ^{n} c_{n,k} (a_k-a)$$ $$\text{since } a_k \to a \text{ then }  \forall \varepsilon_1 >0 \  \exists N \in \mathbb{N} \text{ such that } \forall n \geq N \ |a_k-a|<\varepsilon_1 $$ $$ \text{Choose $\varepsilon =\inf \{ \varepsilon_1 , \frac{\varepsilon_1}{ C M}\}$  } \text{Choose $n$ such that } \sup\{ |c_{n,k}| \} < \frac{\varepsilon_1}{NM}$$ $$-\varepsilon - \sum\limits_{k=N+1} ^{n} |c_{n,k} a_k| \leq S_n \leq \varepsilon + \sum\limits_{k=N+1} ^{n} |c_{n,k} a_k|$$ since $\sum\limits_{k=N+1} ^{n} |c_{n,k} a_k| <\varepsilon$ then $$-2\varepsilon<S_n< 2\varepsilon$$ I tried to prove it myself and this what I got let $b_{n-1}= a_n - a_{n-1}$ it is easy to see that $\lim\limits_{n \to \infty }b_n =0 $ (because any converging sequence in $\mathbb{R}$ is a Cauchy sequence) so $$ \lim\limits_{n \to \infty} \sum\limits_{k=1 }^n \frac{a_k}{(n+1-k)(n+2-k)}=\lim\limits_{n \to \infty} \left( a_n - \frac{a_1}{n+1} +b_n -\sum\limits_{k =1} ^n \frac{b_k}{n+1-k} \right)=a -\lim\limits_{n \to \infty} \sum\limits_{k =1} ^n \frac{b_k}{n+1-k} $$ and here I couldn't prove that $\lim\limits_{n \to \infty} \sum\limits_{k =1} ^n \frac{b_k}{n+1-k} =0$ I also want to ask why is the answers that the book provide is correct ? because one of the necessary condition of Toeplitz theorem doesn't hold EDIT @TheSilverDoe has an excellent answer but now I don't know where my mistake was since $\lim\limits_{n \to \infty} \sum\limits_{k =1} ^n \frac{b_k}{n+1-k} \neq 0$ and $$ \lim\limits_{n \to \infty} \sum\limits_{k=1 }^n \frac{a_k}{(n+1-k)(n+2-k)}= \lim\limits_{n \to \infty} \sum\limits_{k=1 }^n \frac{a_k}{(n+1-k)}- \frac{a_k}{(n+2-k)}$$ $$ =\lim\limits_{n \to \infty}+a_n -\frac{a_1}{n+1}- \sum\limits_{k=1 }^{n-1} \frac{a_{n+1-k}- a_{n-k}}{k+1} $$ $$= a-\lim\limits_{n \to \infty} \sum_{k=1}^{n-1}\frac{a_{k+1}- a_{k}}{n+1-k}=a-\lim\limits_{n \to \infty} \sum_{k=1}^{n-1}\frac{b_{k}}{n+1-k}=a-\lim\limits_{n \to \infty} \sum_{k=1}^{n}\frac{b_{k}}{n+1-k}- b_{n}=$$ $$a- \lim\limits_{n \to \infty} \sum\limits_{k =1} ^n \frac{b_k}{n+1-k}$$ I still can't figure out where is the mistake in my logic. Is the mistake related to conditional convergent series and Riemann rearrangement theorem ?","in Problems in Mathematical Analysis I problem 2.3.16 a), if , then find The proof that was on the answers:- by Toeplitz theorem let and the answer is , here I didn't understand how this is possible because one condition for Toeplitz theorem is doesn't hold if for example let be an array of real numbers such that: 1- 2- as 3- There is exist such that for all positive integer then for any converging sequence the sequence given by is convergent and First I have to prove  that the sequence is convergent this can be shown by considering the sum since the terms of the sum consist only positive number and since is convergent sequence then there exist some such that then the sequence then the sequence converge then converge Now lets proof that first let since as then I need to prove that since then I tried to prove it myself and this what I got let it is easy to see that (because any converging sequence in is a Cauchy sequence) so and here I couldn't prove that I also want to ask why is the answers that the book provide is correct ? because one of the necessary condition of Toeplitz theorem doesn't hold EDIT @TheSilverDoe has an excellent answer but now I don't know where my mistake was since and I still can't figure out where is the mistake in my logic. Is the mistake related to conditional convergent series and Riemann rearrangement theorem ?","\lim\limits_{n \to \infty}a_n =a \lim\limits_{n \to \infty} \sum\limits_{k=1 }^n \frac{a_k}{(n+1-k)(n+2-k)} c_{n,k} =\frac{1}{(n+1-k)(n+2-k)} a  \lim\limits_{n \to \infty} c_{n,k} =0 \ \forall k \in \mathbb{N} k=n  \textbf{Toeplitz theorem :} \{ c_{n,k}: 1\leq k \leq n \}  \lim\limits_{n \to \infty} c_{n,k} =0 \ \forall k \in \mathbb{N} \sum\limits_{k=1} ^{n} c_{n,k} \to 1 n\to \infty C \in \mathbb{R} n \sum\limits_{k=1} ^{n} |c_{n,k}| \leq C  \{a_n \} \{ b_n \} b_n:=\sum\limits_{k=1} ^{n} c_{n,k} a_k  \lim\limits_{n \to \infty}b_n= \lim\limits_{n \to \infty}a_n \textbf{ My Proof:-} \{ b_n\} s_n =\sum\limits_{k=1} ^{n} |c_{n,k} a_k|  a_n M \in  \mathbb{R}  a_k\leq M \ \forall k  s_n <M ,  \sum\limits_{k=1} ^{n} |c_{n,k}| \leq C M  s_n  b_n \lim\limits_{n \to \infty}b_n= \lim\limits_{n \to \infty}a_n a:=\lim\limits_{n \to \infty}a_n \sum\limits_{k=1} ^{n} c_{n,k} \to 1 n\to \infty   \sum\limits_{k=1} ^{n} c_{n,k} a_k-a \sum\limits_{k=1} ^{n}  c_{n,k} \to 0 \text{ as } n \to \infty   S_n:=\sum\limits_{k=1} ^{n} c_{n,k} (a_k-a) \text{since } a_k \to a \text{ then }  \forall \varepsilon_1 >0 \  \exists N \in \mathbb{N} \text{ such that } \forall n \geq N \ |a_k-a|<\varepsilon_1   \text{Choose \varepsilon =\inf \{ \varepsilon_1 , \frac{\varepsilon_1}{ C M}\}  } \text{Choose n such that } \sup\{ |c_{n,k}| \} < \frac{\varepsilon_1}{NM} -\varepsilon - \sum\limits_{k=N+1} ^{n} |c_{n,k} a_k| \leq S_n \leq \varepsilon + \sum\limits_{k=N+1} ^{n} |c_{n,k} a_k| \sum\limits_{k=N+1} ^{n} |c_{n,k} a_k| <\varepsilon -2\varepsilon<S_n< 2\varepsilon b_{n-1}= a_n - a_{n-1} \lim\limits_{n \to \infty }b_n =0  \mathbb{R}  \lim\limits_{n \to \infty} \sum\limits_{k=1 }^n \frac{a_k}{(n+1-k)(n+2-k)}=\lim\limits_{n \to \infty} \left( a_n - \frac{a_1}{n+1} +b_n -\sum\limits_{k =1} ^n \frac{b_k}{n+1-k} \right)=a -\lim\limits_{n \to \infty} \sum\limits_{k =1} ^n \frac{b_k}{n+1-k}  \lim\limits_{n \to \infty} \sum\limits_{k =1} ^n \frac{b_k}{n+1-k} =0 \lim\limits_{n \to \infty} \sum\limits_{k =1} ^n \frac{b_k}{n+1-k} \neq 0  \lim\limits_{n \to \infty} \sum\limits_{k=1 }^n \frac{a_k}{(n+1-k)(n+2-k)}= \lim\limits_{n \to \infty} \sum\limits_{k=1 }^n \frac{a_k}{(n+1-k)}- \frac{a_k}{(n+2-k)}  =\lim\limits_{n \to \infty}+a_n -\frac{a_1}{n+1}- \sum\limits_{k=1 }^{n-1} \frac{a_{n+1-k}- a_{n-k}}{k+1}  = a-\lim\limits_{n \to \infty} \sum_{k=1}^{n-1}\frac{a_{k+1}- a_{k}}{n+1-k}=a-\lim\limits_{n \to \infty} \sum_{k=1}^{n-1}\frac{b_{k}}{n+1-k}=a-\lim\limits_{n \to \infty} \sum_{k=1}^{n}\frac{b_{k}}{n+1-k}- b_{n}= a- \lim\limits_{n \to \infty} \sum\limits_{k =1} ^n \frac{b_k}{n+1-k}","['real-analysis', 'calculus', 'sequences-and-series', 'limits', 'summation']"
45,Is there a function with the following properties?,Is there a function with the following properties?,,"I am looking for a function $f(x,k)$ with the following properties: $$ f(0,k)=1$$ $$ f(1,k)=0$$ $$\lim_{k \to \infty}f(x,k)= \Bigg \{\begin{matrix} 1 & \text {if} & x=0\\ 0 & \text {if} & x>0 \end{matrix}$$ For my project, I am currently using the function: $$g(x,k) = 1-\tanh(k \cdot x)$$ which lacks the property $g(1,k) = 0$ (unless $k$ tends to infinity). Is there such a function?","I am looking for a function with the following properties: For my project, I am currently using the function: which lacks the property (unless tends to infinity). Is there such a function?","f(x,k)  f(0,k)=1  f(1,k)=0 \lim_{k \to \infty}f(x,k)= \Bigg \{\begin{matrix}
1 & \text {if} & x=0\\
0 & \text {if} & x>0
\end{matrix} g(x,k) = 1-\tanh(k \cdot x) g(1,k) = 0 k","['limits', 'functions']"
46,Find the limit in $\mathbb R^2$,Find the limit in,\mathbb R^2,"Let $x_1 = y_1 = 0$ and $$ (x_{n+1}, y_{n+1}) = \bigg(\bigg(1-\frac{2}{n}\bigg)x_n -\frac{y_n}{n} + \frac{4}{n}, \bigg(1-\frac{1}{n}\bigg)y_n - \frac{x_n}{n} + \frac{3}{n}\bigg). $$ Find the $\lim_{n \to \infty} (x_n, y_n)$ . Unfortunately, I have no idea how to solve this problem. Expressions like $$ x_{n+1} - x_n = -3\frac{x_n}{n} - \frac{1}{n}\sum_{k = 1}^{n-1} \frac{x_k}{k} + \frac{H_{n-1}}{n} + \frac{3}{n}, $$ that can be derived from the definition of a sequence do not seem to be helpful. $H_n$ is $n$ -th partial sum of Harmonic series. Would be happy to see some suggestions.","Let and Find the . Unfortunately, I have no idea how to solve this problem. Expressions like that can be derived from the definition of a sequence do not seem to be helpful. is -th partial sum of Harmonic series. Would be happy to see some suggestions.","x_1 = y_1 = 0 
(x_{n+1}, y_{n+1}) = \bigg(\bigg(1-\frac{2}{n}\bigg)x_n -\frac{y_n}{n} + \frac{4}{n}, \bigg(1-\frac{1}{n}\bigg)y_n - \frac{x_n}{n} + \frac{3}{n}\bigg).
 \lim_{n \to \infty} (x_n, y_n) 
x_{n+1} - x_n = -3\frac{x_n}{n} - \frac{1}{n}\sum_{k = 1}^{n-1} \frac{x_k}{k} + \frac{H_{n-1}}{n} + \frac{3}{n},
 H_n n","['limits', 'analysis']"
47,Compute a limit,Compute a limit,,"Fix $\delta >0$ . What is the value of the below limit? $$\lim_{s\downarrow 0}s\int_0^{\infty}e^{-st}\cdot\frac{e^{-\delta^2/(2t)}}{2\pi t}dt.$$ I believe the limit equals zero, but I do not see a simple way to evaluate the limit. It's important to note that dominated convergence does not apply because $t\mapsto \frac{e^{-\delta^2/(2t)}}{2\pi t}$ is not integrable, and monotone convergence gives $0\cdot \infty$ so we couldn't use it either. For those curious, this limit comes from Kai Lai Chung and Zhongxin Zhao's From Brownian Motion to Schrodinger Equation , page 40.","Fix . What is the value of the below limit? I believe the limit equals zero, but I do not see a simple way to evaluate the limit. It's important to note that dominated convergence does not apply because is not integrable, and monotone convergence gives so we couldn't use it either. For those curious, this limit comes from Kai Lai Chung and Zhongxin Zhao's From Brownian Motion to Schrodinger Equation , page 40.",\delta >0 \lim_{s\downarrow 0}s\int_0^{\infty}e^{-st}\cdot\frac{e^{-\delta^2/(2t)}}{2\pi t}dt. t\mapsto \frac{e^{-\delta^2/(2t)}}{2\pi t} 0\cdot \infty,"['calculus', 'limits']"
48,What value of δ should I pick?,What value of δ should I pick?,,"Given $f(x)=\frac{1}{x-1}$ , find $δ$ , such as if $0<|x-2|<δ$ , then $|f(x)-1|<0.01$ . Original problem is below picture: From figure or formula $f(x) = \frac{1}{x-1}$ we can get $f(2) = \frac{1}{2-1} = 1$ . Because we need to guarantee $|f(x)-1|<0.01$ , now we have know $f(2) = 1$ , so we can get $0.99 < f(x) < 1.01$ when x approach 2, namely $0.99 < f(x) = \frac{1}{x-1} < 1.01$ when x approach 2. Convert a decimal into a fraction, we can follow these steps: $\frac{99}{100} < \frac{1}{x-1} < \frac{101}{100}$ , so we can get two  inequality: $\frac{99}{100} < \frac{1}{x-1}$ and $\frac{1}{x-1} < \frac{101}{100}$ . Solve the first inequality $\frac{99}{100} < \frac{1}{x-1}$ , multiply both sides of an inequality by $\frac{100}{99}$ , we can get $\frac{99}{100}\cdot\frac{100}{99} < \frac{1}{x-1}\cdot\frac{100}{99}$ , so we get $1 < \frac{1}{x-1}\cdot\frac{100}{99}$ , namely $1 < \frac{100}{99\cdot{(x-1)}}$ , so we can get $100 > 99\cdot{(x-1)}$ . Now we can get $100 > 99\cdot{x} - 99$ , namely $199 > 99\cdot{x}$ , solve $x < \frac{199}{99}$ . Solve the second inequality $\frac{1}{x-1} < \frac{101}{100}$ by above approach that we can solve $x > \frac{201}{101}$ . The final range of values for x is $\frac{201}{101} < x < \frac{199}{99}$ . Because we need to find δ that it is related to $0<|x-2|<δ$ . so $\frac{201}{101} -2  < x-2 < \frac{199}{99} -2$ . Taking the absolute value of the inequality yields $|\frac{201}{101} -2| < |x-2| < |\frac{199}{99} -2|$ , namely $|\frac{201-202}{101}| < |x-2| < |\frac{199-198}{99}|$ , so we get $|\frac{-1}{101}| < |x-2| < |\frac{1}{99}|$ or $|\frac{1}{101}| < |x-2| < |\frac{1}{99}|$ . How should I proceed? Take δ = $\frac{1}{99}$ ?","Given , find , such as if , then . Original problem is below picture: From figure or formula we can get . Because we need to guarantee , now we have know , so we can get when x approach 2, namely when x approach 2. Convert a decimal into a fraction, we can follow these steps: , so we can get two  inequality: and . Solve the first inequality , multiply both sides of an inequality by , we can get , so we get , namely , so we can get . Now we can get , namely , solve . Solve the second inequality by above approach that we can solve . The final range of values for x is . Because we need to find δ that it is related to . so . Taking the absolute value of the inequality yields , namely , so we get or . How should I proceed? Take δ = ?",f(x)=\frac{1}{x-1} δ 0<|x-2|<δ |f(x)-1|<0.01 f(x) = \frac{1}{x-1} f(2) = \frac{1}{2-1} = 1 |f(x)-1|<0.01 f(2) = 1 0.99 < f(x) < 1.01 0.99 < f(x) = \frac{1}{x-1} < 1.01 \frac{99}{100} < \frac{1}{x-1} < \frac{101}{100} \frac{99}{100} < \frac{1}{x-1} \frac{1}{x-1} < \frac{101}{100} \frac{99}{100} < \frac{1}{x-1} \frac{100}{99} \frac{99}{100}\cdot\frac{100}{99} < \frac{1}{x-1}\cdot\frac{100}{99} 1 < \frac{1}{x-1}\cdot\frac{100}{99} 1 < \frac{100}{99\cdot{(x-1)}} 100 > 99\cdot{(x-1)} 100 > 99\cdot{x} - 99 199 > 99\cdot{x} x < \frac{199}{99} \frac{1}{x-1} < \frac{101}{100} x > \frac{201}{101} \frac{201}{101} < x < \frac{199}{99} 0<|x-2|<δ \frac{201}{101} -2  < x-2 < \frac{199}{99} -2 |\frac{201}{101} -2| < |x-2| < |\frac{199}{99} -2| |\frac{201-202}{101}| < |x-2| < |\frac{199-198}{99}| |\frac{-1}{101}| < |x-2| < |\frac{1}{99}| |\frac{1}{101}| < |x-2| < |\frac{1}{99}| \frac{1}{99},"['calculus', 'limits']"
49,Why can't I use the infinite geometric series formula for this question?,Why can't I use the infinite geometric series formula for this question?,,"$$\sum_{n=1}^{\infty} \frac{12}{(-5)^n}$$ If I rewrite this as $\sum_{n=1}^{\infty} 12(-\frac{1}{5})^n$ and then use the infinite geometric series formula, $\frac{a}{1-r}$ where $a = 12$ and $r = (-\frac{1}{5})$ ? I get 10 but since it starts at 0, I subtracted one from my total equation to get 9, when my textbook says the answer is -2. There must be something I'm doing wrong or misunderstanding.","If I rewrite this as and then use the infinite geometric series formula, where and ? I get 10 but since it starts at 0, I subtracted one from my total equation to get 9, when my textbook says the answer is -2. There must be something I'm doing wrong or misunderstanding.",\sum_{n=1}^{\infty} \frac{12}{(-5)^n} \sum_{n=1}^{\infty} 12(-\frac{1}{5})^n \frac{a}{1-r} a = 12 r = (-\frac{1}{5}),"['calculus', 'sequences-and-series', 'limits', 'summation', 'geometric-series']"
50,Finding $\lim_{x \to 0^{+}} x^{\frac{1}{x}}$ by L'Hôpital's Rule.,Finding  by L'Hôpital's Rule.,\lim_{x \to 0^{+}} x^{\frac{1}{x}},"I am a 76 years old retiree who loves to read math textbooks. Yeah, crazy, right? The current textbook I am using is Larson's Calculus, 12e. Section 5.6, exercise 50 is stumping me: $$\lim_{x \to 0^{+}} x^{\frac{1}{x}}$$ The thrust of this section is to learn how to use L'Hôpital's Rule, particularly to manipulate a limit in indeterminate form, like this one, ( $0^{\infty}$ ), into a suitable form. Since this exercise has a moving exponent, I've used Logarithmic Differentiation: $$\ln y = \lim_{x \to 0^{+}} \left(\frac{1}{x} \cdot \ln x\right) = \infty \cdot \left(-\infty\right)$$ and again: $$= \lim_{x \to 0^{+}} \frac{\ln x}{x} = \frac{-\infty}{0}$$ But here, I don't know what to do. My graphing calculator says that in the original form of the exercise, the limit = 1. So, I figure I have to come up with a way to get to: $$\ln y = 0$$ $$y = e^{0} = 1$$ Any help is appreciated. Thanks, Jose","I am a 76 years old retiree who loves to read math textbooks. Yeah, crazy, right? The current textbook I am using is Larson's Calculus, 12e. Section 5.6, exercise 50 is stumping me: The thrust of this section is to learn how to use L'Hôpital's Rule, particularly to manipulate a limit in indeterminate form, like this one, ( ), into a suitable form. Since this exercise has a moving exponent, I've used Logarithmic Differentiation: and again: But here, I don't know what to do. My graphing calculator says that in the original form of the exercise, the limit = 1. So, I figure I have to come up with a way to get to: Any help is appreciated. Thanks, Jose",\lim_{x \to 0^{+}} x^{\frac{1}{x}} 0^{\infty} \ln y = \lim_{x \to 0^{+}} \left(\frac{1}{x} \cdot \ln x\right) = \infty \cdot \left(-\infty\right) = \lim_{x \to 0^{+}} \frac{\ln x}{x} = \frac{-\infty}{0} \ln y = 0 y = e^{0} = 1,['limits']
51,"How to evaluate $\,\lim\limits_{x \to 1}\left(x^n-1\right)\ln^m(1-x)\,$ without L'Hopital? [duplicate]",How to evaluate  without L'Hopital? [duplicate],"\,\lim\limits_{x \to 1}\left(x^n-1\right)\ln^m(1-x)\,","This question already has answers here : What is the workings to solve $\lim\limits_{n\rightarrow \infty} \frac{n^t}{e^n}$? (5 answers) Closed 12 months ago . This question arose as part of the evaluation of $\int_{0}^{1} x^{n-1} \ln^{m}(1-x) \, \mathrm{d}x$ using integration by parts, where we would require to show that $$ L=\lim_{x \to 1} \left( x^n-1\right)\ln^m(1-x) \qquad n,m \in \mathbb{N} $$ Checking several cases with Wolfram Alpha, the claim seemed to hold. My attempt: \begin{align} L & \overset{\color{blue}{x = 1-e^{-t}}}{=}\lim_{t \to \infty} \left(\left(1-e^{-t} \right)^n-1 \right)\left(-t \right)^m\\ & = \sum_{k=1}^{n}\binom{n}{k}(-1)^{k+m}\lim_{t \to \infty}\frac{t^m}{e^{kt}}\\ & \overset{\color{green}{\text{L'H}}}{=}  \sum_{k=1}^{n}\binom{n}{k}(-1)^{k+m}\lim_{t \to \infty}\frac{m!}{k^me^{kt}}\\ & =0 \end{align} I was wondering if the limit in the title could be shown by a method (which could be completely different than the one I tried) that doesn't use L'Hopitals rule. The reason is that I believe another proof with L'H would be similar-ish in spirit to mine, and I was more interested in other ways of tackling the original limit altogether. Another idea I tried to make work expanding the natural log part in a series, but since the limit is on the border of the convergence region I wasn't sure if it would work. Any ideas or suggestions are welcome. Thanks!","This question already has answers here : What is the workings to solve $\lim\limits_{n\rightarrow \infty} \frac{n^t}{e^n}$? (5 answers) Closed 12 months ago . This question arose as part of the evaluation of using integration by parts, where we would require to show that Checking several cases with Wolfram Alpha, the claim seemed to hold. My attempt: I was wondering if the limit in the title could be shown by a method (which could be completely different than the one I tried) that doesn't use L'Hopitals rule. The reason is that I believe another proof with L'H would be similar-ish in spirit to mine, and I was more interested in other ways of tackling the original limit altogether. Another idea I tried to make work expanding the natural log part in a series, but since the limit is on the border of the convergence region I wasn't sure if it would work. Any ideas or suggestions are welcome. Thanks!","\int_{0}^{1} x^{n-1} \ln^{m}(1-x) \, \mathrm{d}x 
L=\lim_{x \to 1} \left( x^n-1\right)\ln^m(1-x) \qquad n,m \in \mathbb{N}
 \begin{align}
L & \overset{\color{blue}{x = 1-e^{-t}}}{=}\lim_{t \to \infty} \left(\left(1-e^{-t} \right)^n-1 \right)\left(-t \right)^m\\
& = \sum_{k=1}^{n}\binom{n}{k}(-1)^{k+m}\lim_{t \to \infty}\frac{t^m}{e^{kt}}\\
& \overset{\color{green}{\text{L'H}}}{=}  \sum_{k=1}^{n}\binom{n}{k}(-1)^{k+m}\lim_{t \to \infty}\frac{m!}{k^me^{kt}}\\
& =0
\end{align}","['real-analysis', 'calculus', 'limits', 'alternative-proof', 'limits-without-lhopital']"
52,"Sequence satisfying $ \lim_{n \to \infty}{x_{2n} + x_{2n + 1}} = M $, find $ \lim_{n \to \infty}{\frac{x_{2n}}{x_{2n + 1}}} $","Sequence satisfying , find", \lim_{n \to \infty}{x_{2n} + x_{2n + 1}} = M   \lim_{n \to \infty}{\frac{x_{2n}}{x_{2n + 1}}} ,"Suppose that $ \left(x_{n}\right) $ is a sequence satisfying $$ \lim_{n \to \infty}{x_{2n} + x_{2n + 1}} = M \quad \mbox{and} \quad \lim_{n \to \infty}{x_{2n} + x_{2n - 1}} = N $$ with $ M \ne N $ . Find $$ \lim_{n \to \infty}{\frac{x_{2n}}{x_{2n + 1}}}. $$ At first, I tried to subtract two limits and got $$ \lim_{n \to \infty}{x_{2n + 1} - x_{2n - 1}} = M - N. $$ We also know that $$ \lim_{n \to \infty}{\frac{x_{2n}}{x_{2n + 1}}} = \lim_{n \to \infty}{\frac{x_{2n} + x_{2n + 1}}{x_{2n + 1}} - 1}. $$ But since we don't know about the convergence of $ \left(x_{n}\right) $ , I can't continue the argument. I also tried using Stolz-Cesàro theorem, but since we don't know about the convergence and monotonicity of $ \left(x_{n}\right) $ , we can't use it. Could someone provide me with a helpful hint or strategy for tackling this problem? Thank you. Note: The original problem used $ M = 315 $ and $ N = 2016 $ .","Suppose that is a sequence satisfying with . Find At first, I tried to subtract two limits and got We also know that But since we don't know about the convergence of , I can't continue the argument. I also tried using Stolz-Cesàro theorem, but since we don't know about the convergence and monotonicity of , we can't use it. Could someone provide me with a helpful hint or strategy for tackling this problem? Thank you. Note: The original problem used and .", \left(x_{n}\right)   \lim_{n \to \infty}{x_{2n} + x_{2n + 1}} = M \quad \mbox{and} \quad \lim_{n \to \infty}{x_{2n} + x_{2n - 1}} = N   M \ne N   \lim_{n \to \infty}{\frac{x_{2n}}{x_{2n + 1}}}.   \lim_{n \to \infty}{x_{2n + 1} - x_{2n - 1}} = M - N.   \lim_{n \to \infty}{\frac{x_{2n}}{x_{2n + 1}}} = \lim_{n \to \infty}{\frac{x_{2n} + x_{2n + 1}}{x_{2n + 1}} - 1}.   \left(x_{n}\right)   \left(x_{n}\right)   M = 315   N = 2016 ,"['sequences-and-series', 'limits']"
53,prove that $\displaystyle\lim _{n \rightarrow \infty} n a_n=0$,prove that,\displaystyle\lim _{n \rightarrow \infty} n a_n=0,"Given a positive sequence $\{a_n\}$ , if $$\lim _{n \rightarrow \infty} \ln n \cdot\left(\frac{a_n}{a_{n+1}}-1\right)=\lambda>0,$$ prove that $\displaystyle\lim _{n \rightarrow \infty} n a_n=0$ . Although the given condition appears to have a similar form to the Raabe's test , there are significant differences between them which make it difficult to establish the proof. In this case, we can use the Raabe's test to show that the sequence is approaching zero and then attempt to apply the Stolz-Cesaro theorem. Specifically, we have $$\lim _{n \rightarrow \infty} n a_n=\lim _{n \rightarrow \infty} \frac{a_{n+1}-a_n}{\frac{1}{n+1}-\frac{1}{n}}=-n(n+1)(a_{n+1}-a_n).$$ However, there does not seem to be an obvious way forward from here. Any help or comments on this matter would be greatly appreciated to further advance the proof.","Given a positive sequence , if prove that . Although the given condition appears to have a similar form to the Raabe's test , there are significant differences between them which make it difficult to establish the proof. In this case, we can use the Raabe's test to show that the sequence is approaching zero and then attempt to apply the Stolz-Cesaro theorem. Specifically, we have However, there does not seem to be an obvious way forward from here. Any help or comments on this matter would be greatly appreciated to further advance the proof.","\{a_n\} \lim _{n \rightarrow \infty} \ln n \cdot\left(\frac{a_n}{a_{n+1}}-1\right)=\lambda>0, \displaystyle\lim _{n \rightarrow \infty} n a_n=0 \lim _{n \rightarrow \infty} n a_n=\lim _{n \rightarrow \infty} \frac{a_{n+1}-a_n}{\frac{1}{n+1}-\frac{1}{n}}=-n(n+1)(a_{n+1}-a_n).","['sequences-and-series', 'limits']"
54,Calculate the following limit: $\lim\limits_{x \to 0^+}\!\big((1+x)^x-1\big)^x $ [closed],Calculate the following limit:  [closed],\lim\limits_{x \to 0^+}\!\big((1+x)^x-1\big)^x ,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question This limit is from a college admission exam in Cluj, Romania. I've tried writing the limit as $\,e^{g\cdot\ln f}$ , so $\,e^{\lim\limits_{x \to 0^+}x\cdot\ln\left((1+x)^x-1\right)}$ , but then I have no idea how to write $(1+x)^x$ .","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question This limit is from a college admission exam in Cluj, Romania. I've tried writing the limit as , so , but then I have no idea how to write .","\,e^{g\cdot\ln f} \,e^{\lim\limits_{x \to 0^+}x\cdot\ln\left((1+x)^x-1\right)} (1+x)^x",['limits']
55,Limit of products of differences of logarithms,Limit of products of differences of logarithms,,"My first post here, hope doing everything the right way. I'm struggling with the following limit: $$\lim_{x\to+\infty}\ln(3^x-x)\left[\ln(x^4+1)-2\ln(x^2-x)\right]$$ First of all I used logarithm properties to transform the function in the form $$\ln(3^x-x)\ln\bigg(\frac{x^4+1}{x^4-2x^3+x^2}\bigg)$$ Here I noticed that the first logarithm goes to infinity for $x$ that goes to infinity and the second goes to zero because the argument goes to $1$ for $x$ to infinity. So it is an undefined form $0\cdot\infty$ . Next I tried to use algebraic transformations and known limits but I can't find anything useful to solve the limit because every try ends up in another undefined form. Any ideas? Thanks :)","My first post here, hope doing everything the right way. I'm struggling with the following limit: First of all I used logarithm properties to transform the function in the form Here I noticed that the first logarithm goes to infinity for that goes to infinity and the second goes to zero because the argument goes to for to infinity. So it is an undefined form . Next I tried to use algebraic transformations and known limits but I can't find anything useful to solve the limit because every try ends up in another undefined form. Any ideas? Thanks :)",\lim_{x\to+\infty}\ln(3^x-x)\left[\ln(x^4+1)-2\ln(x^2-x)\right] \ln(3^x-x)\ln\bigg(\frac{x^4+1}{x^4-2x^3+x^2}\bigg) x 1 x 0\cdot\infty,"['real-analysis', 'calculus', 'limits']"
56,Proving $\lim_{x\to\infty}\left(\frac{\sin(e^{-x})}{x^{p-2}(x-1)^q}\right) = 0$,Proving,\lim_{x\to\infty}\left(\frac{\sin(e^{-x})}{x^{p-2}(x-1)^q}\right) = 0,"How can I prove the following limit for every $p,q \in \mathbb R$ ? $$\lim_{x\to\infty}\left(\frac{\sin(e^{-x})}{x^{p-2}(x-1)^q}\right) = 0$$ For example, I tried using absolute value together with the squeeze theorem, while I believe it might work if $p-2+q > 0$ it would fail otherwise. Any suggestions, please?","How can I prove the following limit for every ? For example, I tried using absolute value together with the squeeze theorem, while I believe it might work if it would fail otherwise. Any suggestions, please?","p,q \in \mathbb R \lim_{x\to\infty}\left(\frac{\sin(e^{-x})}{x^{p-2}(x-1)^q}\right) = 0 p-2+q > 0","['calculus', 'limits']"
57,"If $\lim_{n\to \infty} a_n = 4$, can we say $\lim_{n\to \infty} a_{n-2} = 4$?","If , can we say ?",\lim_{n\to \infty} a_n = 4 \lim_{n\to \infty} a_{n-2} = 4,"I was doing this exponential tower equation: $$2^{x^{2^{x^{2^{...}}}}} = 4$$ $$\text {(each new exponent is the  power of the last exponent)}$$ The popular method is to break the tower at the first exponent and evaluate that what's left in the exponential tower is still the LHS if the tower length goes into infinity: $$2^{x^{2^{x^{2^{...}}}}} =  2^{x^{4}} = 4$$ Consider sequence $a_n$ with $a_1 = 2^x$ , $a_2 = 2^{x^{2}}$ , $a_3 = 2^{x^{2^{x}}}$ , and so on, then: $$LHS = \lim_{n\to ∞} a_n = 4$$ The method above seems to be reasoning that: $$\text{if}\;\;\;\;\;\;\;\;\;\;\lim_{n\to ∞} a_n = 4$$ $$\text{then}\;\;\;\;\;\;\;\;\;\lim_{n\to ∞} a_{n-2} = 4$$ (1) Is this something that we need to prove? I know this is intuitive enough, but is there a way to rigorously demonstrate this? (2) I was assuming that this is the reasoning behind the method above. If you think otherwise, please advise! (3) Unrelated, but is there a formula to define the sequence above with $a_n, a_{n+1}, a_{n+2}$ ? (I assume we cannot write a general case for $a_n$ alone.) Thank you!","I was doing this exponential tower equation: The popular method is to break the tower at the first exponent and evaluate that what's left in the exponential tower is still the LHS if the tower length goes into infinity: Consider sequence with , , , and so on, then: The method above seems to be reasoning that: (1) Is this something that we need to prove? I know this is intuitive enough, but is there a way to rigorously demonstrate this? (2) I was assuming that this is the reasoning behind the method above. If you think otherwise, please advise! (3) Unrelated, but is there a formula to define the sequence above with ? (I assume we cannot write a general case for alone.) Thank you!","2^{x^{2^{x^{2^{...}}}}} = 4 \text {(each new exponent is the  power of the last exponent)} 2^{x^{2^{x^{2^{...}}}}} =  2^{x^{4}} = 4 a_n a_1 = 2^x a_2 = 2^{x^{2}} a_3 = 2^{x^{2^{x}}} LHS = \lim_{n\to ∞} a_n = 4 \text{if}\;\;\;\;\;\;\;\;\;\;\lim_{n\to ∞} a_n = 4 \text{then}\;\;\;\;\;\;\;\;\;\lim_{n\to ∞} a_{n-2} = 4 a_n, a_{n+1}, a_{n+2} a_n","['calculus', 'sequences-and-series', 'limits', 'convergence-divergence', 'power-towers']"
58,The limit $\lim\limits_{n\to\infty}\frac{x^n}{n!}$,The limit,\lim\limits_{n\to\infty}\frac{x^n}{n!},"Evaluate $$\lim_{n\to\infty}\frac{x^n}{n!}$$ where $x\in\mathbb R$ This is a common limit and has been asked and answered many times here on this site. However, I present another approach with Stolz-Cesaro( $x\notin[-1,1]$ . When $x\in[-1,1]$ , the limit is trivially zero): $$L=\lim_{n\to\infty}\frac{x^n}{n!}=\lim_{n\to\infty}\frac{x^{n+1}-x^n}{(n+1)!-n!}$$ $$L=\lim_{n\to\infty}\frac{x^n}{n!}\frac{(x-1)}{n}=L\frac{(x-1)}{n}$$ The above equation holds for all $x$ in domain only when $L=0$ . Hence the limit is just $0$ . I have never seen Stolz-Cesaro being used this way to solve a limit, hence the lack of surety.  It would be great to get this verified!","Evaluate where This is a common limit and has been asked and answered many times here on this site. However, I present another approach with Stolz-Cesaro( . When , the limit is trivially zero): The above equation holds for all in domain only when . Hence the limit is just . I have never seen Stolz-Cesaro being used this way to solve a limit, hence the lack of surety.  It would be great to get this verified!","\lim_{n\to\infty}\frac{x^n}{n!} x\in\mathbb R x\notin[-1,1] x\in[-1,1] L=\lim_{n\to\infty}\frac{x^n}{n!}=\lim_{n\to\infty}\frac{x^{n+1}-x^n}{(n+1)!-n!} L=\lim_{n\to\infty}\frac{x^n}{n!}\frac{(x-1)}{n}=L\frac{(x-1)}{n} x L=0 0","['calculus', 'limits', 'solution-verification']"
59,How to find the limit to an undefined point (asymptotic),How to find the limit to an undefined point (asymptotic),,"I am wondering about: $$  \lim_{x \rightarrow -1} \frac{x+1}{ \sqrt{x+5}-2 } $$ which seems to be an asymptotic function with each side of its corresponding graph approaching negative or positive infinity at the value of -1 . Solved by WolframAlpha, the expected result was calculated: Click to see result . So, I was just learning for a math exam and using the website KhanAcademy to refresh my math skills, where above term was to be solved, and indeed a two-sided limit should be found, which apparently seems to be 4 . This was done by first rationalizing the term in this manner , which results in: $$  \lim_{x \rightarrow -1}( \sqrt{x+5}+2)\text{ for  }x \neq -1 $$ Substituting x for -1 in this simplified term doesn't result in getting $\frac{0}{0}$ but 4 , which apparently means that 4 is the both-sided limit (even though the root of 4 has 2 as well as -2 as the possible solution, making 0 another viable limit?). I can't proof which one of those two approaches contains an error and therefore wanted to ask here, I hope someone can clarify this matter :) Best regards","I am wondering about: which seems to be an asymptotic function with each side of its corresponding graph approaching negative or positive infinity at the value of -1 . Solved by WolframAlpha, the expected result was calculated: Click to see result . So, I was just learning for a math exam and using the website KhanAcademy to refresh my math skills, where above term was to be solved, and indeed a two-sided limit should be found, which apparently seems to be 4 . This was done by first rationalizing the term in this manner , which results in: Substituting x for -1 in this simplified term doesn't result in getting but 4 , which apparently means that 4 is the both-sided limit (even though the root of 4 has 2 as well as -2 as the possible solution, making 0 another viable limit?). I can't proof which one of those two approaches contains an error and therefore wanted to ask here, I hope someone can clarify this matter :) Best regards",  \lim_{x \rightarrow -1} \frac{x+1}{ \sqrt{x+5}-2 }    \lim_{x \rightarrow -1}( \sqrt{x+5}+2)\text{ for  }x \neq -1  \frac{0}{0},['limits']
60,Computing $\lim_{x\to 0}\left(\frac{\cot(x)}{x^3}-\frac{1}{x^4}+\frac{1}{3x^2}\right)$,Computing,\lim_{x\to 0}\left(\frac{\cot(x)}{x^3}-\frac{1}{x^4}+\frac{1}{3x^2}\right),"I am trying to compute the following limit: $$\lim_{x\to 0}\left(\frac{\cot(x)}{x^3}-\frac{1}{x^4}+\frac{1}{3x^2}\right)$$ I have tried by rewriting it as $\lim_{x\to 0}\left(\frac{3x+(x^3-3)\tan(x)}{3x^4 \tan(x)}\right)$ and applying De l'Hopital's Rule but the expression quickly becomes unmanageable: $$\lim_{x\to 0}\left(\frac{3x+(x^3-3)\tan(x)}{3x^4 \tan(x)}\right)\overset{H}{=}\lim_{x\to 0}\frac{3+(x^2 - 3) \sec^2(x) + 2 x \tan(x)}{3 x^3 (4 \tan(x) + x \sec^2(x))}$$ so I then tried by using the Maclaurin expression for $\tan(x)$ : \begin{align*} \lim_{x\to 0}\left(\frac{\cot(x)}{x^3}-\frac{1}{x^4}+\frac{1}{3x^2}\right)&=\lim_{x\to 0}\frac{1}{x^2}\left(\frac{x-\tan(x)}{x^2\tan(x)}+\frac{1}{3}\right)=\lim_{x\to 0}\frac{1}{x^2}\left(\frac{x-\left(x+\frac{x^3}{3}+\frac{2}{15}x^5\right)}{x\left(\frac{x^3}{3}+\frac{2}{15}x^5\right)}+\frac{1}{3}\right)\\ &=\lim_{x\to 0}\frac{1}{x^2}\left(\frac{-\frac{1}{3}-\frac{2}{15}x^2}{1+\frac{x^2}{3}+\frac{2}{15}x^4}+\frac{1}{3}\right)=+\infty\cdot 0 \end{align*} and I got an indeterminate form. I am currently out of ideas so I would appreciate some help in figuring this out, thanks. EDIT: It just occurred to me that \begin{align*} \lim_{x\to 0}\frac{1}{x^2}\left(\frac{-\frac{1}{3}-\frac{2}{15}x^2}{1+\frac{x^2}{3}+\frac{2}{15}x^4}+\frac{1}{3}\right)=\lim_{x\to 0}\frac{1}{x^2}\left(\frac{-\frac{1}{3}-\frac{2}{15}x^2+\frac{1}{3}+\frac{1}{9}x^2+\frac{2}{45}x^4}{1+\frac{x^2}{3}+\frac{2}{15}x^4}\right)\\ \lim_{x\to 0} \frac{1}{x^2}\left(\frac{-\frac{1}{45}x^2+\frac{2}{45}x^4}{1+\frac{x^2}{3}+\frac{2}{15}x^4}\right)=\lim_{x\to 0}\frac{-\frac{1}{45}+\frac{2}{45}x^2}{1+\frac{x^2}{3}+\frac{2}{15}x^4}=-\frac{1}{45}. \end{align*}","I am trying to compute the following limit: I have tried by rewriting it as and applying De l'Hopital's Rule but the expression quickly becomes unmanageable: so I then tried by using the Maclaurin expression for : and I got an indeterminate form. I am currently out of ideas so I would appreciate some help in figuring this out, thanks. EDIT: It just occurred to me that","\lim_{x\to 0}\left(\frac{\cot(x)}{x^3}-\frac{1}{x^4}+\frac{1}{3x^2}\right) \lim_{x\to 0}\left(\frac{3x+(x^3-3)\tan(x)}{3x^4 \tan(x)}\right) \lim_{x\to 0}\left(\frac{3x+(x^3-3)\tan(x)}{3x^4 \tan(x)}\right)\overset{H}{=}\lim_{x\to 0}\frac{3+(x^2 - 3) \sec^2(x) + 2 x \tan(x)}{3 x^3 (4 \tan(x) + x \sec^2(x))} \tan(x) \begin{align*}
\lim_{x\to 0}\left(\frac{\cot(x)}{x^3}-\frac{1}{x^4}+\frac{1}{3x^2}\right)&=\lim_{x\to 0}\frac{1}{x^2}\left(\frac{x-\tan(x)}{x^2\tan(x)}+\frac{1}{3}\right)=\lim_{x\to 0}\frac{1}{x^2}\left(\frac{x-\left(x+\frac{x^3}{3}+\frac{2}{15}x^5\right)}{x\left(\frac{x^3}{3}+\frac{2}{15}x^5\right)}+\frac{1}{3}\right)\\
&=\lim_{x\to 0}\frac{1}{x^2}\left(\frac{-\frac{1}{3}-\frac{2}{15}x^2}{1+\frac{x^2}{3}+\frac{2}{15}x^4}+\frac{1}{3}\right)=+\infty\cdot 0
\end{align*} \begin{align*}
\lim_{x\to 0}\frac{1}{x^2}\left(\frac{-\frac{1}{3}-\frac{2}{15}x^2}{1+\frac{x^2}{3}+\frac{2}{15}x^4}+\frac{1}{3}\right)=\lim_{x\to 0}\frac{1}{x^2}\left(\frac{-\frac{1}{3}-\frac{2}{15}x^2+\frac{1}{3}+\frac{1}{9}x^2+\frac{2}{45}x^4}{1+\frac{x^2}{3}+\frac{2}{15}x^4}\right)\\ \lim_{x\to 0} \frac{1}{x^2}\left(\frac{-\frac{1}{45}x^2+\frac{2}{45}x^4}{1+\frac{x^2}{3}+\frac{2}{15}x^4}\right)=\lim_{x\to 0}\frac{-\frac{1}{45}+\frac{2}{45}x^2}{1+\frac{x^2}{3}+\frac{2}{15}x^4}=-\frac{1}{45}.
\end{align*}","['calculus', 'limits']"
61,proof that $ \lim_{x \to \infty} \frac{f(x)}{g(x)} = 1 $ is an equivalence relation,proof that  is an equivalence relation, \lim_{x \to \infty} \frac{f(x)}{g(x)} = 1 ,"Define the following relation on $ \{f \in \mathbb{R}^\mathbb{R} \mid 0 \notin Image(f) \} $ : $ f \ $ is equivalent to $ \ g \ $ if and only if $ \displaystyle \lim_{x \to \infty} \frac{f(x)}{g(x)} = 1 $ . I want to prove that this is an equivalence relation, is the following proof valid? $ \underline{\text{Reflexivity:}} $ $ \forall x \in \mathbb{R}, \ \frac{f(x)}{f(x)} = 1 \Rightarrow \displaystyle \lim_{x \to \infty} \frac{f(x)}{f(x)} = \lim_{x \to \infty} 1 = 1 \Rightarrow f \ $ is equivalent to $ f $ . $ \underline{\text{Symmetry:}} $ Assume $ \displaystyle \lim_{x \to \infty} \frac{f(x)}{g(x)} = 1 $ . Therefore, $ \displaystyle \lim_{x \to \infty} \frac{g(x)}{f(x)} = \lim_{x \to \infty} \frac{1}{\frac{f(x)}{g(x)}} = \frac{\displaystyle \lim_{x \to \infty} 1}{\displaystyle \lim_{x \to \infty} \frac{f(x)}{g(x)}} = \frac{1}{1} = 1$ and therefore $ \ g \ $ is equivalent to $ \ f $ . $ \underline{\text{Transitivity:}} $ Assume $ \displaystyle \lim_{x \to \infty} \frac{f(x)}{g(x)} = 1 \land \lim_{x \to \infty} \frac{g(x)}{h(x)} = 1 $ . Therefore, $ \displaystyle \lim_{x \to \infty} \frac{f(x)}{h(x)} = \lim_{x \to \infty} \frac{f(x)}{g(x)} \frac{g(x)}{h(x)} = \lim_{x \to \infty} \frac{f(x)}{g(x)} \cdot \lim_{x \to \infty} \frac{g(x)}{h(x)} = 1 \cdot 1 = 1 $ . $$\tag*{$\blacksquare$}$$ I feel that in in ""Transitivity"", I am assuming the limit exists. Is that true?","Define the following relation on : is equivalent to if and only if . I want to prove that this is an equivalence relation, is the following proof valid? is equivalent to . Assume . Therefore, and therefore is equivalent to . Assume . Therefore, . I feel that in in ""Transitivity"", I am assuming the limit exists. Is that true?"," \{f \in \mathbb{R}^\mathbb{R} \mid 0 \notin Image(f) \}   f \   \ g \   \displaystyle \lim_{x \to \infty} \frac{f(x)}{g(x)} = 1   \underline{\text{Reflexivity:}}   \forall x \in \mathbb{R}, \ \frac{f(x)}{f(x)} = 1 \Rightarrow \displaystyle \lim_{x \to \infty} \frac{f(x)}{f(x)} = \lim_{x \to \infty} 1 = 1 \Rightarrow f \   f   \underline{\text{Symmetry:}}   \displaystyle \lim_{x \to \infty} \frac{f(x)}{g(x)} = 1   \displaystyle \lim_{x \to \infty} \frac{g(x)}{f(x)} = \lim_{x \to \infty} \frac{1}{\frac{f(x)}{g(x)}} = \frac{\displaystyle \lim_{x \to \infty} 1}{\displaystyle \lim_{x \to \infty} \frac{f(x)}{g(x)}} = \frac{1}{1} = 1  \ g \   \ f   \underline{\text{Transitivity:}}   \displaystyle \lim_{x \to \infty} \frac{f(x)}{g(x)} = 1 \land \lim_{x \to \infty} \frac{g(x)}{h(x)} = 1   \displaystyle \lim_{x \to \infty} \frac{f(x)}{h(x)} = \lim_{x \to \infty} \frac{f(x)}{g(x)} \frac{g(x)}{h(x)} = \lim_{x \to \infty} \frac{f(x)}{g(x)} \cdot \lim_{x \to \infty} \frac{g(x)}{h(x)} = 1 \cdot 1 = 1  \tag*{\blacksquare}","['real-analysis', 'limits', 'solution-verification', 'equivalence-relations']"
62,Find out limit of the following question,Find out limit of the following question,,"let, $f(x) =x^\frac{1}{3}$ be a diffrentiable function on $ (0, \infty).$ Given that $$\frac{f(3+h) -f(3)}{h}=f'(3+\theta(h)h)$$ Then find out $\lim_{h\to 0+} \theta(h) =? $ Since, $f$ is diffrentiable at $3$ , I think limit must be $0$ as it tends to $f'(3)$ Please help me","let, be a diffrentiable function on Given that Then find out Since, is diffrentiable at , I think limit must be as it tends to Please help me","f(x) =x^\frac{1}{3}  (0, \infty). \frac{f(3+h) -f(3)}{h}=f'(3+\theta(h)h) \lim_{h\to 0+} \theta(h) =?  f 3 0 f'(3)",['limits']
63,Find the value of $\lim_{n\rightarrow\infty}\left(\sum_{k=1}^n\frac{1}{n+k^{\alpha}}\right)$,Find the value of,\lim_{n\rightarrow\infty}\left(\sum_{k=1}^n\frac{1}{n+k^{\alpha}}\right),"Find the value of the expression $$\lim_{n\rightarrow\infty}\left(\sum_{k=1}^n\frac{1}{n+k^{\alpha}}\right)$$ where $\alpha$ is a positive number. It is my first time seeing a question like this. Normally, we just put $n=\infty$ in the summand which is same as calculating the series till infinite terms. But here it is different. The variable in the bound is also in the expression, meaning $n=\infty$ in the summand is not applicable in this case. I expanded the series as $$\frac{1}{n+1}+\frac{1}{n+2^{\alpha}}+\cdots$$ But had no luck. Any help is greatly appreciated.","Find the value of the expression where is a positive number. It is my first time seeing a question like this. Normally, we just put in the summand which is same as calculating the series till infinite terms. But here it is different. The variable in the bound is also in the expression, meaning in the summand is not applicable in this case. I expanded the series as But had no luck. Any help is greatly appreciated.",\lim_{n\rightarrow\infty}\left(\sum_{k=1}^n\frac{1}{n+k^{\alpha}}\right) \alpha n=\infty n=\infty \frac{1}{n+1}+\frac{1}{n+2^{\alpha}}+\cdots,"['real-analysis', 'sequences-and-series', 'limits']"
64,Use the formal definition of a limit for sequences to prove the limit of a sequence . Where Did I Go Wrong?,Use the formal definition of a limit for sequences to prove the limit of a sequence . Where Did I Go Wrong?,,"The formal definition of limit of a sequence says that  for every $\epsilon>0$ , there is a number $N>0$ such that if $n > N$ it is true that $|(a_n) - L| < 	ε$ In this particular case I have to demonstrate by the definition that this limit exist: $$\lim \limits_{n \to \infty}\frac{(1-4n-7n^2)}{1+2n+n^2}=-7$$ This is what I did : $\|\frac{1-4n-7n^2}{1+2n+n^2} + 7\|$ = $\|\frac{10n+8}{(n+1)^2}\|$ In the definition, we have that $N > 0$ , and $n>N$ , this implies that $$\|\frac{10n+8}{(n+1)^2}\| = \frac{10n+8}{(n+1)^2}$$ I then used the fact that : $\frac{10n+8}{(n+1)^2}$ < $10n+8$ $10n+8 < ε$ if and only if $n < \frac{ε - 8}{10}$ I dont know what to do after that. Also, I don't think that makes sense, In these types of proofs, my teacher always had n > in the end and not vice versa. Also, it doesn't make sense that n needs to be very small since we are observing to what value the sequence converges to when n is VERY LARGE  I don't know where I went wrong and how to do it otherwise.","The formal definition of limit of a sequence says that  for every , there is a number such that if it is true that In this particular case I have to demonstrate by the definition that this limit exist: This is what I did : = In the definition, we have that , and , this implies that I then used the fact that : < if and only if I dont know what to do after that. Also, I don't think that makes sense, In these types of proofs, my teacher always had n > in the end and not vice versa. Also, it doesn't make sense that n needs to be very small since we are observing to what value the sequence converges to when n is VERY LARGE  I don't know where I went wrong and how to do it otherwise.",\epsilon>0 N>0 n > N |(a_n) - L| < 	ε \lim \limits_{n \to \infty}\frac{(1-4n-7n^2)}{1+2n+n^2}=-7 \|\frac{1-4n-7n^2}{1+2n+n^2} + 7\| \|\frac{10n+8}{(n+1)^2}\| N > 0 n>N \|\frac{10n+8}{(n+1)^2}\| = \frac{10n+8}{(n+1)^2} \frac{10n+8}{(n+1)^2} 10n+8 10n+8 < ε n < \frac{ε - 8}{10},"['limits', 'solution-verification', 'proof-explanation', 'epsilon-delta']"
65,Evaluate $\lim\limits_{n \to \infty} \sqrt[n^2]{1! + 2! + 3! + \dots + n!}$,Evaluate,\lim\limits_{n \to \infty} \sqrt[n^2]{1! + 2! + 3! + \dots + n!},"$$\lim_{n \to \infty} \sqrt[n^2]{1! + 2! + 3! + \dots + n!}$$ I can try at least to evaluate it from bottom: $$1 < \sqrt[n^2]{n^2} < \sqrt[n^2]{n!} < \sqrt[n^2]{1! + 2! + \dots + n!}$$ Well, it doesn't say anything. Ok, gonna try evaluation from top. $$\sqrt[n^2]{1! + 2! + \cdots + n!} < \sqrt[n^2]{n \times n!}$$ More than that, $$\lim_{n \to \infty} \frac{n!}{1! + 2! + \cdots + n!} = 1$$ but this equation is always less than one.  So instead of writing $\sqrt[n^2]{n \times n!}$ I can try fixed coefficient $1 < a < n$ . It will work for $a = 2$ , $a =1.5$ , but I'm not sure though about $a = (1 + \frac{1}{n})$ $$\sqrt[n^2]{1! + 2! + \cdots + n!} < \sqrt[n^2]{a \times n!}$$ Just to get broad idea how it looks I'll assume that $a = 1$ and I'll try to use Stirling's approximation $$\sqrt[n^2]{1 \times n!} \approx \left((2\pi n)^{\frac{1}{2}}\left(\frac{n}{e}\right)^n \right)^\frac{1}{n^2} = (2 \pi n)^{\frac{1}{2n^2}} \times \left(\frac{n}{e}\right)^{\frac{1}{n}}$$ $\lim_{n \to \infty} \sqrt[n]{n} = 1$ , $2n^2 > 2 \pi n$ , $\frac{n}{e} < n$ (for big numbers) mean, that $$\lim_{n \to \infty}(2 \pi n)^{\frac{1}{2n^2}} \times (\frac{n}{e})^{\frac{1}{n}} \approx 1$$ So I can guess, that $$\lim_{n \to \infty} \sqrt[n^2]{1! + 2! + 3! + \cdots + n!}  = 1$$ But I sincerely doubt that what I wrote here is barely a solution. Would you mind helping me? Hope I didn't make huge mistakes","I can try at least to evaluate it from bottom: Well, it doesn't say anything. Ok, gonna try evaluation from top. More than that, but this equation is always less than one.  So instead of writing I can try fixed coefficient . It will work for , , but I'm not sure though about Just to get broad idea how it looks I'll assume that and I'll try to use Stirling's approximation , , (for big numbers) mean, that So I can guess, that But I sincerely doubt that what I wrote here is barely a solution. Would you mind helping me? Hope I didn't make huge mistakes",\lim_{n \to \infty} \sqrt[n^2]{1! + 2! + 3! + \dots + n!} 1 < \sqrt[n^2]{n^2} < \sqrt[n^2]{n!} < \sqrt[n^2]{1! + 2! + \dots + n!} \sqrt[n^2]{1! + 2! + \cdots + n!} < \sqrt[n^2]{n \times n!} \lim_{n \to \infty} \frac{n!}{1! + 2! + \cdots + n!} = 1 \sqrt[n^2]{n \times n!} 1 < a < n a = 2 a =1.5 a = (1 + \frac{1}{n}) \sqrt[n^2]{1! + 2! + \cdots + n!} < \sqrt[n^2]{a \times n!} a = 1 \sqrt[n^2]{1 \times n!} \approx \left((2\pi n)^{\frac{1}{2}}\left(\frac{n}{e}\right)^n \right)^\frac{1}{n^2} = (2 \pi n)^{\frac{1}{2n^2}} \times \left(\frac{n}{e}\right)^{\frac{1}{n}} \lim_{n \to \infty} \sqrt[n]{n} = 1 2n^2 > 2 \pi n \frac{n}{e} < n \lim_{n \to \infty}(2 \pi n)^{\frac{1}{2n^2}} \times (\frac{n}{e})^{\frac{1}{n}} \approx 1 \lim_{n \to \infty} \sqrt[n^2]{1! + 2! + 3! + \cdots + n!}  = 1,"['real-analysis', 'calculus', 'limits']"
66,How to solve $\lim\limits_{n\to∞}\frac1{n^{3/2}}(\sqrt{2n+1}+\sqrt{2n+2}+\cdots+\sqrt{2n+n})$,How to solve,\lim\limits_{n\to∞}\frac1{n^{3/2}}(\sqrt{2n+1}+\sqrt{2n+2}+\cdots+\sqrt{2n+n}),"How to solve $\lim\limits_{n\to∞}\dfrac1{n^{3/2}}(\sqrt{2n+1}+\sqrt{2n+2}+\cdots+\sqrt{2n+n})$ This problem was asked by another user, but was deleted when I found the answer below. So I thought I should post it here . Please check if my use of Riemann Integral was correct, since I have very little knowledge on it. Thanks.","How to solve This problem was asked by another user, but was deleted when I found the answer below. So I thought I should post it here . Please check if my use of Riemann Integral was correct, since I have very little knowledge on it. Thanks.",\lim\limits_{n\to∞}\dfrac1{n^{3/2}}(\sqrt{2n+1}+\sqrt{2n+2}+\cdots+\sqrt{2n+n}),"['calculus', 'sequences-and-series', 'limits', 'riemann-sum']"
67,Finding the limit in terms of a without using L'Hospitals,Finding the limit in terms of a without using L'Hospitals,,"Find $\;\lim_\limits{x \to 0} \dfrac{1-\cos(ax)}{1-\sqrt{1+x^2}}\;$ in terms of a without using L'Hospitals Rule. I first graphed the function and noticed that that limit tends to go towards $-a^2$ I also tried this approach: $\lim_\limits{x \to 0} \dfrac{1-\cos(ax)}{1-\sqrt{1+x^2}}=\lim_\limits{x \to 0} \dfrac{2 \cdot \frac{1-\cos(ax)}{2}}{1-\sqrt{1+x^2}} = \lim_\limits{x \to 0} \dfrac{2\sin^2(\frac{ax}{2})}{1 - \sqrt{1+x^2}} $ However, I'm not sure how to proceed. I tried rationalizing the denominator but it only makes it more tangled.","Find in terms of a without using L'Hospitals Rule. I first graphed the function and noticed that that limit tends to go towards I also tried this approach: However, I'm not sure how to proceed. I tried rationalizing the denominator but it only makes it more tangled.",\;\lim_\limits{x \to 0} \dfrac{1-\cos(ax)}{1-\sqrt{1+x^2}}\; -a^2 \lim_\limits{x \to 0} \dfrac{1-\cos(ax)}{1-\sqrt{1+x^2}}=\lim_\limits{x \to 0} \dfrac{2 \cdot \frac{1-\cos(ax)}{2}}{1-\sqrt{1+x^2}} = \lim_\limits{x \to 0} \dfrac{2\sin^2(\frac{ax}{2})}{1 - \sqrt{1+x^2}} ,"['calculus', 'limits', 'limits-without-lhopital']"
68,Proving $\lim _{x \to \infty }\frac{1 - 2x^2}{x^2 + 3}\ = -2$,Proving,\lim _{x \to \infty }\frac{1 - 2x^2}{x^2 + 3}\ = -2,I solved it using the definition of limit to the $\left|\frac{-7}{x^2 +3}\right| < \varepsilon$ Then $\varepsilon > \frac{7}{x + 3} > \frac{7}{x^2 +3}$ Then $x > \frac{7}{\varepsilon} - 3$ But my teacher said this was wrong and I have to fix it to $x >  \sqrt{\left|\frac{7}{\varepsilon} - 3\right|}$ Can anyone please explain why am I wrong here? Thank you very much,I solved it using the definition of limit to the Then Then But my teacher said this was wrong and I have to fix it to Can anyone please explain why am I wrong here? Thank you very much,\left|\frac{-7}{x^2 +3}\right| < \varepsilon \varepsilon > \frac{7}{x + 3} > \frac{7}{x^2 +3} x > \frac{7}{\varepsilon} - 3 x >  \sqrt{\left|\frac{7}{\varepsilon} - 3\right|},['limits']
69,Where is the flaw in my calculation of $\lim\limits_{x \to 0} \frac{x^2}{1-\cos(x)}$,Where is the flaw in my calculation of,\lim\limits_{x \to 0} \frac{x^2}{1-\cos(x)},"The limit at 0 may be correctly calculated this way: $$ \lim\limits_{x \to 0} \frac{x^2}{1-\cos(x)} = \lim\limits_{x \to 0} \frac{x^2}{1-\cos(x)}\cdot\frac{1+\cos(x)}{1+\cos(x)}=\lim\limits_{x \to 0} \frac{x^2(1+\cos(x))}{\sin^2(x)}=\\\lim\limits_{x \to 0} \left(\frac{x^2}{\sin^2(x)}\cdot(1+\cos(x)\right)=\lim\limits_{x \to 0} \frac{x^2}{\sin^2(x)}\cdot\lim\limits_{x \to 0}(1+\cos(x)=1\cdot2=2 $$ Where is the flaw in the following? $$\lim\limits_{x \to 0} \frac{x^2}{1-\cos(x)} = \lim\limits_{x \to 0} \frac{x}{1-\cos(x)}\cdot\lim\limits_{x \to 0}\,x$$ Thank you.",The limit at 0 may be correctly calculated this way: Where is the flaw in the following? Thank you.,"
\lim\limits_{x \to 0} \frac{x^2}{1-\cos(x)} = \lim\limits_{x \to 0} \frac{x^2}{1-\cos(x)}\cdot\frac{1+\cos(x)}{1+\cos(x)}=\lim\limits_{x \to 0} \frac{x^2(1+\cos(x))}{\sin^2(x)}=\\\lim\limits_{x \to 0} \left(\frac{x^2}{\sin^2(x)}\cdot(1+\cos(x)\right)=\lim\limits_{x \to 0} \frac{x^2}{\sin^2(x)}\cdot\lim\limits_{x \to 0}(1+\cos(x)=1\cdot2=2
 \lim\limits_{x \to 0} \frac{x^2}{1-\cos(x)} = \lim\limits_{x \to 0} \frac{x}{1-\cos(x)}\cdot\lim\limits_{x \to 0}\,x","['calculus', 'limits', 'solution-verification']"
70,Evaluating the limit of a polynomial fraction as the base goes to infinity where the polynomial orders are related,Evaluating the limit of a polynomial fraction as the base goes to infinity where the polynomial orders are related,,"I am attempting to evaluate the following limit: $$\lim_{x \rightarrow \infty} \frac{x^a}{x^b}$$ Where $a$ and $b$ are positive real numbers such that $a < b$ . It seems intuitive that this limit should evaluate to $0$ , as: $$\lim_{x \rightarrow \infty} a\log x - b \log x \leq 0$$ Because $a < b$ . I worry-- because it's been a while since I've proven anything mathematically-- that this isn't a rigorous/correct proof, and wonder if anyone would be kind enough to point me in the right direction if I am on the wrong track? Thank you!","I am attempting to evaluate the following limit: Where and are positive real numbers such that . It seems intuitive that this limit should evaluate to , as: Because . I worry-- because it's been a while since I've proven anything mathematically-- that this isn't a rigorous/correct proof, and wonder if anyone would be kind enough to point me in the right direction if I am on the wrong track? Thank you!",\lim_{x \rightarrow \infty} \frac{x^a}{x^b} a b a < b 0 \lim_{x \rightarrow \infty} a\log x - b \log x \leq 0 a < b,"['limits', 'solution-verification', 'proof-writing', 'logarithms', 'asymptotics']"
71,Can we define a limit of a sequence of groups? [closed],Can we define a limit of a sequence of groups? [closed],,"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 1 year ago . Improve this question This is just a random thought I had while procrastinating some group theory. Is there a meaningful way to talk about limits of groups? I don't know if this has any use, but here is my thought process so far. We would need a topology on the ""set"" of all groups (but already this might be a problem because this is too big to be a set); the only idea I had would be an order topology, where G is smaller than H if there is an injective homomorphism from G to H.","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 1 year ago . Improve this question This is just a random thought I had while procrastinating some group theory. Is there a meaningful way to talk about limits of groups? I don't know if this has any use, but here is my thought process so far. We would need a topology on the ""set"" of all groups (but already this might be a problem because this is too big to be a set); the only idea I had would be an order topology, where G is smaller than H if there is an injective homomorphism from G to H.",,"['group-theory', 'limits']"
72,Does a sequence with no limit points eventually become monotonic?,Does a sequence with no limit points eventually become monotonic?,,"A common definition for the limit point of a sequence is that a number $x$ is a limit point of the sequence $(x_n)_{n\in \mathbb{N}}$ if there exists a subsequence that converges to $x$ . Intuitively, it appears that if a sequence has no limit points then for some $N$ the sequence is monotonic for all $n\geq N$ . Is this true? Formally speaking, by the Bolzano Weierstrass Theorem $\#\{n \in \mathbb{N}:x_n \in [-K,K]\}<\infty$ for every $K>0$ . Is this fact sufficient to imply that $x_n$ must explode in a monotone manner eventually?","A common definition for the limit point of a sequence is that a number is a limit point of the sequence if there exists a subsequence that converges to . Intuitively, it appears that if a sequence has no limit points then for some the sequence is monotonic for all . Is this true? Formally speaking, by the Bolzano Weierstrass Theorem for every . Is this fact sufficient to imply that must explode in a monotone manner eventually?","x (x_n)_{n\in \mathbb{N}} x N n\geq N \#\{n \in \mathbb{N}:x_n \in [-K,K]\}<\infty K>0 x_n","['real-analysis', 'sequences-and-series', 'limits', 'solution-verification']"
73,"Prove/disprove that $f : (0 , \infty) \rightarrow \mathbb R$ , $f(x) = x \sin x$ is onto","Prove/disprove that  ,  is onto","f : (0 , \infty) \rightarrow \mathbb R f(x) = x \sin x","I need to find whether the function $f : (0 , \infty) \rightarrow \mathbb R$ , $f(x) = x \sin x$ is onto or not. Here is my approach: $f$ is an elementary function, and therefore continuous. It is continuous everywhere because it is defined everywhere in $\mathbb R$ . Now, I think the function has no limit. It diverges and is not bounded. I am not sure how to ""prove"" it mathematically and formally. But because it is unbounded (neither from above or below), then for every $y$ , there exists $x_1$ such that $f(x_1) < y$ . Similiarly, there exists $x_2$ such that $f(x_2) > y$ . Hence $f(x_1) < y < f(x_2)$ . Again, $f$ is continuous, then by the intermediate value theorem, for every such $y$ there must exist $x_0$ such that $f(x_0) = y$ . Therefore, the function $f$ is onto. But I think there is a flaw in my proof, because the intermediate value theorem requires a closed interval. How can I fix this? Is this approach valid and correct? If it is, I am struggling in writing in in formal, mathematical writing, specifically in proving it is not bounded (from both above and below), and that it has no limit (not even an infinite one)","I need to find whether the function , is onto or not. Here is my approach: is an elementary function, and therefore continuous. It is continuous everywhere because it is defined everywhere in . Now, I think the function has no limit. It diverges and is not bounded. I am not sure how to ""prove"" it mathematically and formally. But because it is unbounded (neither from above or below), then for every , there exists such that . Similiarly, there exists such that . Hence . Again, is continuous, then by the intermediate value theorem, for every such there must exist such that . Therefore, the function is onto. But I think there is a flaw in my proof, because the intermediate value theorem requires a closed interval. How can I fix this? Is this approach valid and correct? If it is, I am struggling in writing in in formal, mathematical writing, specifically in proving it is not bounded (from both above and below), and that it has no limit (not even an infinite one)","f : (0 , \infty) \rightarrow \mathbb R f(x) = x \sin x f \mathbb R y x_1 f(x_1) < y x_2 f(x_2) > y f(x_1) < y < f(x_2) f y x_0 f(x_0) = y f","['real-analysis', 'calculus', 'limits', 'functions', 'continuity']"
74,Find $\frac{m}{n}$ given $\lim_{x\to 0}\frac{e^{\cos(x^n)}-e}{x^m}=-\frac{e}{2}$,Find  given,\frac{m}{n} \lim_{x\to 0}\frac{e^{\cos(x^n)}-e}{x^m}=-\frac{e}{2},"Question. Find $\frac{m}{n}$ given $$\lim_{x\to 0}\frac{e^{\cos(x^n)}-e}{x^m}=-\frac{e}{2}$$ Attempt. So this is what I tried, using L'Hopital's rule: $$\lim_{x\to 0}\frac{-\sin(x^n)x^{n-1}ne^{\cos(x^n)}}{mx^{m-1}}=-\frac{e}{2}$$ $$\frac{n}{m}\lim_{x\to 0}\frac{-\sin(x^n)x^{n-1}e^{\cos(x^n)}}{x^{m-1}}=-\frac{e}{2}$$ and since the limit of $e^{\cos(x^n)}$ is 1, we can just take the $e$ outside and remove the minus and that $e$ from both sides: $$\frac{n}{m}\lim_{x\to 0}\frac{\sin(x^n)x^{n-1}}{x^{m-1}}=\frac{1}{2}$$ multiplying on the numerator and denominator by $x^n$ : $$\frac{n}{m}\lim_{x\to 0}\frac{\sin(x^n)}{x^n}\frac{x^{n-1}x^n}{x^{m-1}}=\frac{1}{2}$$ but I'm unsure whether you can use the common limit here of $\frac{\sin(x)}{x}$ or how to continue. Can someone shed some light?","Question. Find given Attempt. So this is what I tried, using L'Hopital's rule: and since the limit of is 1, we can just take the outside and remove the minus and that from both sides: multiplying on the numerator and denominator by : but I'm unsure whether you can use the common limit here of or how to continue. Can someone shed some light?",\frac{m}{n} \lim_{x\to 0}\frac{e^{\cos(x^n)}-e}{x^m}=-\frac{e}{2} \lim_{x\to 0}\frac{-\sin(x^n)x^{n-1}ne^{\cos(x^n)}}{mx^{m-1}}=-\frac{e}{2} \frac{n}{m}\lim_{x\to 0}\frac{-\sin(x^n)x^{n-1}e^{\cos(x^n)}}{x^{m-1}}=-\frac{e}{2} e^{\cos(x^n)} e e \frac{n}{m}\lim_{x\to 0}\frac{\sin(x^n)x^{n-1}}{x^{m-1}}=\frac{1}{2} x^n \frac{n}{m}\lim_{x\to 0}\frac{\sin(x^n)}{x^n}\frac{x^{n-1}x^n}{x^{m-1}}=\frac{1}{2} \frac{\sin(x)}{x},"['calculus', 'limits']"
75,Lim sup of infinite sums inequality,Lim sup of infinite sums inequality,,"We have the result that $\limsup_{n\rightarrow \infty} (a_n + b_n) \leq \limsup_{n\rightarrow \infty}(a_n) + \limsup_{n\rightarrow \infty}(b_n)$ if all sums exist. By induction, we can extend this to any finite sum of terms. Does this mean we can take the limit and say $\limsup_{n\rightarrow \infty} (\sum_{k=1}^{\infty} a_n^k) \leq \sum_{k=0}^\infty \limsup_{n\rightarrow \infty} a_n^k$ ? (if all sums exist) Intuitively I feel like this is true, but I don't know how to pass the limit through the $\limsup$ to prove this.","We have the result that if all sums exist. By induction, we can extend this to any finite sum of terms. Does this mean we can take the limit and say ? (if all sums exist) Intuitively I feel like this is true, but I don't know how to pass the limit through the to prove this.",\limsup_{n\rightarrow \infty} (a_n + b_n) \leq \limsup_{n\rightarrow \infty}(a_n) + \limsup_{n\rightarrow \infty}(b_n) \limsup_{n\rightarrow \infty} (\sum_{k=1}^{\infty} a_n^k) \leq \sum_{k=0}^\infty \limsup_{n\rightarrow \infty} a_n^k \limsup,"['real-analysis', 'limits', 'limsup-and-liminf']"
76,How to calculate a limit of a simple sequence?,How to calculate a limit of a simple sequence?,,"I have to admit that I have forget most of my limit knowledge and I would appreaciate an advice with this. The problem is: $a_1 := 1$ $a_{n+1} := \dfrac{a_n^2}{4} + 1$ Calculate the limit for $n \rightarrow \infty$ . My thoughts: I would say the limit is $\infty$ and maybe rewrite the problem as $a_{n+1} := \dfrac{a_n^2 + 4}{4}$ ? Also, I remember the ""known limits"" - i.e. $(1 + 1/x)^x$ converges to $e$ in $\infty$ etc. etc. But I cannot see anything useful to solve the problem above. Thank you for help!","I have to admit that I have forget most of my limit knowledge and I would appreaciate an advice with this. The problem is: Calculate the limit for . My thoughts: I would say the limit is and maybe rewrite the problem as ? Also, I remember the ""known limits"" - i.e. converges to in etc. etc. But I cannot see anything useful to solve the problem above. Thank you for help!",a_1 := 1 a_{n+1} := \dfrac{a_n^2}{4} + 1 n \rightarrow \infty \infty a_{n+1} := \dfrac{a_n^2 + 4}{4} (1 + 1/x)^x e \infty,"['limits', 'analysis']"
77,Find the limit of $(1+\frac{1}{n^2})(1+\frac{2}{n^2})...(1+\frac{n}{n^2})$.,Find the limit of .,(1+\frac{1}{n^2})(1+\frac{2}{n^2})...(1+\frac{n}{n^2}),To find the limit of $(1+\frac{1}{n^2})(1+\frac{2}{n^2})...(1+\frac{n}{n^2})$ . First notice that $(1+\frac{1}{n^2})$ is the smallest term in the product and $(1+\frac{n}{n^2})$ is the greatest. So: $ (1+\frac{1}{n^2})^n \leq (1+\frac{1}{n^2})(1+\frac{2}{n^2})...(1+\frac{n}{n^2}) \leq (1+\frac{n}{n^2})^n$ . First this seemed like a good idea but then I found $ 1 \leq (1+\frac{1}{n^2})(1+\frac{2}{n^2})...(1+\frac{n}{n^2}) \leq e$ . Which is not really helpful. How can this problem be done? P.S: In previous questions of the problem I had to prove that $\sum n^2 = \frac{n(n+1)(2n+1)}{6}$ and $x-\frac{x^2}{2} \leq  \ln(x+1) \leq x$ .,To find the limit of . First notice that is the smallest term in the product and is the greatest. So: . First this seemed like a good idea but then I found . Which is not really helpful. How can this problem be done? P.S: In previous questions of the problem I had to prove that and .,(1+\frac{1}{n^2})(1+\frac{2}{n^2})...(1+\frac{n}{n^2}) (1+\frac{1}{n^2}) (1+\frac{n}{n^2})  (1+\frac{1}{n^2})^n \leq (1+\frac{1}{n^2})(1+\frac{2}{n^2})...(1+\frac{n}{n^2}) \leq (1+\frac{n}{n^2})^n  1 \leq (1+\frac{1}{n^2})(1+\frac{2}{n^2})...(1+\frac{n}{n^2}) \leq e \sum n^2 = \frac{n(n+1)(2n+1)}{6} x-\frac{x^2}{2} \leq  \ln(x+1) \leq x,"['real-analysis', 'limits']"
78,Calculating a limit with exponents,Calculating a limit with exponents,,"There is a limit I want to calculate, and I've calculated it as follows: $$\lim_{x \to \infty} \frac{7^{-x+1}-2\cdot 5^{-x}}{3^{-x}-7^{-x}} = \lim_{x \to \infty} \frac{\frac{7}{7^x}-\frac{2}{5^x}}{\frac{1}{3^x}-\frac{1}{7^x}}$$ If I now multiply the denominator and numerator by $3^x$ , I get the following: $$\lim_{x \to \infty} \frac{7\cdot\frac{3^x}{7^x}-2\cdot\frac{3^x}{5^x}}{1-\frac{3^x}{7^x}} = \frac{0}{1}.$$ Is it correct to assume that the fractions in the numerator will limit to $0$ and not to infinity (because they are multiplied by $7$ and $2$ respectively)? Does this way of thinking apply to all limits of such format (except for those that limit to $e$ ) or are there exceptions?","There is a limit I want to calculate, and I've calculated it as follows: If I now multiply the denominator and numerator by , I get the following: Is it correct to assume that the fractions in the numerator will limit to and not to infinity (because they are multiplied by and respectively)? Does this way of thinking apply to all limits of such format (except for those that limit to ) or are there exceptions?",\lim_{x \to \infty} \frac{7^{-x+1}-2\cdot 5^{-x}}{3^{-x}-7^{-x}} = \lim_{x \to \infty} \frac{\frac{7}{7^x}-\frac{2}{5^x}}{\frac{1}{3^x}-\frac{1}{7^x}} 3^x \lim_{x \to \infty} \frac{7\cdot\frac{3^x}{7^x}-2\cdot\frac{3^x}{5^x}}{1-\frac{3^x}{7^x}} = \frac{0}{1}. 0 7 2 e,"['limits', 'exponentiation']"
79,How do I prove a sequence diverges to infinity?,How do I prove a sequence diverges to infinity?,,"I have been scratching my head for a couple of days on how to determine convergence/divergence of sequences. I made it to understand how to prove that a sequence converges, but still have numerous doubts about proof of divergence. Say I have $\lim_{n \to +\infty } \sqrt{n+1} = +\infty$ and I have to prove the sequence diverges. What I did is using the definition of converging sequence $| a_n - L | < \ \varepsilon$ Where L is a theoretical limit (Fixed, real number) and $\varepsilon$ is also a theoretical, real number bounding the sequence (am I understanding this correctly?) Then, I tried the proof by contradiction by doing $-\varepsilon \ <  \sqrt{n+1} - L < \varepsilon$ $ -\varepsilon + L <  \sqrt{n+1} < \varepsilon + L$ $(-\varepsilon + L)^2 - 1 < n < (\varepsilon + L)^2 - 1$ Assuming that $\varepsilon$ and L are fixed, real numbers, we can always come up with an n greater than any operation done between those numbers, thus contradicting the fact that a bound exists. Is the proof I've come up with valid and sufficient ? I want to apologize in advance to people familiar with this, in case I made a horrible mess.","I have been scratching my head for a couple of days on how to determine convergence/divergence of sequences. I made it to understand how to prove that a sequence converges, but still have numerous doubts about proof of divergence. Say I have and I have to prove the sequence diverges. What I did is using the definition of converging sequence Where L is a theoretical limit (Fixed, real number) and is also a theoretical, real number bounding the sequence (am I understanding this correctly?) Then, I tried the proof by contradiction by doing Assuming that and L are fixed, real numbers, we can always come up with an n greater than any operation done between those numbers, thus contradicting the fact that a bound exists. Is the proof I've come up with valid and sufficient ? I want to apologize in advance to people familiar with this, in case I made a horrible mess.",\lim_{n \to +\infty } \sqrt{n+1} = +\infty | a_n - L | < \ \varepsilon \varepsilon -\varepsilon \ <  \sqrt{n+1} - L < \varepsilon  -\varepsilon + L <  \sqrt{n+1} < \varepsilon + L (-\varepsilon + L)^2 - 1 < n < (\varepsilon + L)^2 - 1 \varepsilon,"['calculus', 'sequences-and-series', 'limits', 'analysis', 'solution-verification']"
80,Epsilon-delta proof of a limit of a quadratic function,Epsilon-delta proof of a limit of a quadratic function,,"I found the following example: picture Is the reason for getting an upper bound $|x+2|\leq 3+2=5$ (and not $|x-2|\leq 5$ [I've used the triangle inqeuality here]) that we want a bound on $|x-2|$ to depend on $\epsilon$ but we don't care about a bound on $|x+2|$ ? So if the problem were asking to prove $\lim_ {x\to -2}=4$ , I would have to say $|x-2|\leq 5$ and so $|x+2|\leq \epsilon/5$ ? If we had chosen the max $\delta$ value to be $1/2$ instead of $1$ , would we have to set $\delta=\min\{1/2,\epsilon/4.5\}$ ? I'm also not sure if I fully understand the point of setting $\delta=\min\{1,\epsilon/5\}$ . Let's say $\epsilon=10$ . Then if I take delta to be $\epsilon/5$ , I'll have $|x-2||x+2|\leq |2||5|=10$ . Is the only reason why I should take $\delta=1$ in this case that the inequality $|x-2||x+2| < \epsilon$ must be strict?","I found the following example: picture Is the reason for getting an upper bound (and not [I've used the triangle inqeuality here]) that we want a bound on to depend on but we don't care about a bound on ? So if the problem were asking to prove , I would have to say and so ? If we had chosen the max value to be instead of , would we have to set ? I'm also not sure if I fully understand the point of setting . Let's say . Then if I take delta to be , I'll have . Is the only reason why I should take in this case that the inequality must be strict?","|x+2|\leq 3+2=5 |x-2|\leq 5 |x-2| \epsilon |x+2| \lim_ {x\to -2}=4 |x-2|\leq 5 |x+2|\leq \epsilon/5 \delta 1/2 1 \delta=\min\{1/2,\epsilon/4.5\} \delta=\min\{1,\epsilon/5\} \epsilon=10 \epsilon/5 |x-2||x+2|\leq |2||5|=10 \delta=1 |x-2||x+2| < \epsilon","['calculus', 'limits', 'epsilon-delta']"
81,The sequence $n\sin(\sqrt{4\pi^2n^2 +x^2})$ converges on compacts:,The sequence  converges on compacts:,n\sin(\sqrt{4\pi^2n^2 +x^2}),"I'm not sure about the idea behind the sequence $f_n(x)=n\sin(\sqrt{4\pi^2n^2 +x^2})$ being uniformly convergent on every compact of the form $[0,a]$ for $a>0$ . On $\mathbb{R}$ we can find a sequence such that $\sin$ alternates the sign, thus making it impossible for $f_n$ to have a limit. But the compact case is a little more tricky: let $K=[0,a]$ , then by Weierstrass Thm $\forall n\exists M_n\mid$ $|f_n(x)|<M_n$ , thus we wish for $M_n$ to be convergent. As we are using Weierstrass Thm, we can take $x_n\in K$ and $f_n(x_n)=M_n$ . But $K$ is compact in $\mathbb{R}$ : $\exists x_{n_k}\mid x_{n_k}\rightarrow\bar{x}\in K$ , thus $f_{n_k}(x_{n_k})=M_{n_k}\rightarrow\overline{M}\in\mathbb{R}$ , making $f_n$ uniformly convergent. The proof seems to make sense but on the other hand it doesn't depend on $f_n$ , so it sounds like I'm either missing something.","I'm not sure about the idea behind the sequence being uniformly convergent on every compact of the form for . On we can find a sequence such that alternates the sign, thus making it impossible for to have a limit. But the compact case is a little more tricky: let , then by Weierstrass Thm , thus we wish for to be convergent. As we are using Weierstrass Thm, we can take and . But is compact in : , thus , making uniformly convergent. The proof seems to make sense but on the other hand it doesn't depend on , so it sounds like I'm either missing something.","f_n(x)=n\sin(\sqrt{4\pi^2n^2 +x^2}) [0,a] a>0 \mathbb{R} \sin f_n K=[0,a] \forall n\exists M_n\mid |f_n(x)|<M_n M_n x_n\in K f_n(x_n)=M_n K \mathbb{R} \exists x_{n_k}\mid x_{n_k}\rightarrow\bar{x}\in K f_{n_k}(x_{n_k})=M_{n_k}\rightarrow\overline{M}\in\mathbb{R} f_n f_n","['real-analysis', 'sequences-and-series', 'limits', 'solution-verification', 'uniform-convergence']"
82,If $(a_{2n} -a_n)$ is convergent to zero then $(a_n)$ is convergent . if $(a_n)$ is convergent then $(a_{2n}-a_n)$ is convergent to zero,If  is convergent to zero then  is convergent . if  is convergent then  is convergent to zero,(a_{2n} -a_n) (a_n) (a_n) (a_{2n}-a_n),"1.If $(a_{2n} -a_n)$ is convergent to zero then $(a_n)$ is convergent. True/False? 2.If $(a_n)$ is convergent then $(a_{2n}-a_n)$ is convergent. True/False? I solved $2$ this way - let $\lim\limits_{n\to\infty }a_n  =L$ and $(a_{2n})$ is a subsequence of $(a_n)$ then they must converge to the same limit , therefore $\lim\limits_{n\to\infty }a_{2n}  =L$ so we get that $\lim\limits_{n\to\infty }a_{2n}- a_n  =L-L=0$ so the statement is True for the first part I know that the statement is not true but I cannot find a counterexample or a general way to show it , the practice book used this counterexample and explanation: let $a_n=\begin{cases}   1 &\text{if $n=2^k$ $k \in \Bbb N$}\\   0&\text{otherwise}\\  \end{cases}$ if $n=2^k$ then for $k \in \Bbb N$ we get $2n=2^{k+1}$ therefore $a_{2n}=a_n=1$ and $a_{2n}-a_n=0$ if $n\not=2^k$ then $a_{2n}=a_n=0$ and $a_{2n}-a_n=0$ meaning for all $n$ we get $a_{2n}-a_n=0$ so $\lim\limits_{n\to\infty }a_{2n}- a_n  =0$ but $(a_n)$ is not convergent $(a_{2n-1})$ is a subsequence of $(a_n)$ in the odd indexes so $(a_{2n-1})=0$ and $\lim\limits_{n\to\infty }a_{2n-1}=0$ . the sequence $n_k =2^k$ is a sequence that is stricly increasing of natural indexes because for all $k$ we get $2^k$ is also a natural number and $n_{k+1}=2^{k+1}=2\cdot2^k <2^k =n_k$ , we get that $(a_{n_k})=(a_{2^k})$ is a subsequence of $(a_n)$ but all of its elements are the same $\lim\limits_{n\to\infty }a_{2^k}=\lim\limits_{n\to\infty }1=1$ but this subsequence has a different limit which means that $(a_n)$ is not convergent and the statement is false my question is if there is another way to show this? this counterexample seems too complicated , I understand the explanations but I would not have figured to use $a_n=\begin{cases}   1 &\text{if $n=2^k$ $k \in \Bbb N$}\\   0&\text{otherwise}\\  \end{cases}$ .. is there another counterexample? or a general way to show this other than the way they did in the book? thank you","1.If is convergent to zero then is convergent. True/False? 2.If is convergent then is convergent. True/False? I solved this way - let and is a subsequence of then they must converge to the same limit , therefore so we get that so the statement is True for the first part I know that the statement is not true but I cannot find a counterexample or a general way to show it , the practice book used this counterexample and explanation: let if then for we get therefore and if then and meaning for all we get so but is not convergent is a subsequence of in the odd indexes so and . the sequence is a sequence that is stricly increasing of natural indexes because for all we get is also a natural number and , we get that is a subsequence of but all of its elements are the same but this subsequence has a different limit which means that is not convergent and the statement is false my question is if there is another way to show this? this counterexample seems too complicated , I understand the explanations but I would not have figured to use .. is there another counterexample? or a general way to show this other than the way they did in the book? thank you","(a_{2n} -a_n) (a_n) (a_n) (a_{2n}-a_n) 2 \lim\limits_{n\to\infty }a_n  =L (a_{2n}) (a_n) \lim\limits_{n\to\infty }a_{2n}  =L \lim\limits_{n\to\infty }a_{2n}- a_n  =L-L=0 a_n=\begin{cases}
  1 &\text{if n=2^k k \in \Bbb N}\\ 
 0&\text{otherwise}\\ 
\end{cases} n=2^k k \in \Bbb N 2n=2^{k+1} a_{2n}=a_n=1 a_{2n}-a_n=0 n\not=2^k a_{2n}=a_n=0 a_{2n}-a_n=0 n a_{2n}-a_n=0 \lim\limits_{n\to\infty }a_{2n}- a_n  =0 (a_n) (a_{2n-1}) (a_n) (a_{2n-1})=0 \lim\limits_{n\to\infty }a_{2n-1}=0 n_k =2^k k 2^k n_{k+1}=2^{k+1}=2\cdot2^k <2^k =n_k (a_{n_k})=(a_{2^k}) (a_n) \lim\limits_{n\to\infty }a_{2^k}=\lim\limits_{n\to\infty }1=1 (a_n) a_n=\begin{cases}
  1 &\text{if n=2^k k \in \Bbb N}\\ 
 0&\text{otherwise}\\ 
\end{cases}","['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
83,What is $ \lim_{x \rightarrow 0}\left(\frac{1}{\ln\cos (x)}+\frac{2}{\sin ^{2}(x)}\right) $?,What is ?, \lim_{x \rightarrow 0}\left(\frac{1}{\ln\cos (x)}+\frac{2}{\sin ^{2}(x)}\right) ,"The answer of the following limit: $$ \lim_{x \rightarrow 0}\left(\frac{1}{\ln \cos (x)}+\frac{2}{\sin ^{2}(x)}\right) $$ is 1 by Wolfram Alpha. But I tried to find it and I got $2/3$ : My approach : $1)$ $ \ln(\cos x)=\ln\left(1-\frac{x^{2}}{2}+o\left(x^{3}\right)\right)=-\frac{x^{2}}{2}+o\left(x^{3}\right) $ $2)$ $ \sin ^{2}(x)=\left(x-\frac{x^{3}}{3!}+o\left(x^{3}\right)\right)^{2}=x^{2}-\frac{x^{4}}{3}+o\left(x^{4}\right) $ $3)$ $\begin{aligned} \frac{1}{-\frac{x^{2}}{2}+o\left(x^{3}\right)}+\frac{2}{x^{2}-\frac{x^{4}}{3}+o\left(x^{4}\right)}=\frac{-x^{2}+x^{2}-\frac{x^{4}}{3}+o\left(x^{3}\right)}{-\frac{x^{4}}{2}+o\left(x^{5}\right)}=\frac{-\frac{1}{3}+o\left(x^{3}\right)}{-\frac{1}{2}+o\left(x^{5}\right)} \end{aligned}$ $4)$ $\lim _{x \rightarrow 0} \frac{-\frac{1}{3}+o\left(x^{3}\right)}{-\frac{1}{2}+o\left(x^{5}\right)}=\frac{-\frac{1}{3}}{-\frac{1}{2}}=\frac{2}{3}$ So where is the mistake in my approach? Note: $o$ denotes the little-o notation Edit : I've understood where's my mistake is, but another question popped up reading the answers which is : does $o(1/x)$ tends to zero as x tends to zero?","The answer of the following limit: is 1 by Wolfram Alpha. But I tried to find it and I got : My approach : So where is the mistake in my approach? Note: denotes the little-o notation Edit : I've understood where's my mistake is, but another question popped up reading the answers which is : does tends to zero as x tends to zero?","
\lim_{x \rightarrow 0}\left(\frac{1}{\ln \cos (x)}+\frac{2}{\sin ^{2}(x)}\right)
 2/3 1) 
\ln(\cos x)=\ln\left(1-\frac{x^{2}}{2}+o\left(x^{3}\right)\right)=-\frac{x^{2}}{2}+o\left(x^{3}\right)
 2) 
\sin ^{2}(x)=\left(x-\frac{x^{3}}{3!}+o\left(x^{3}\right)\right)^{2}=x^{2}-\frac{x^{4}}{3}+o\left(x^{4}\right)
 3) \begin{aligned} \frac{1}{-\frac{x^{2}}{2}+o\left(x^{3}\right)}+\frac{2}{x^{2}-\frac{x^{4}}{3}+o\left(x^{4}\right)}=\frac{-x^{2}+x^{2}-\frac{x^{4}}{3}+o\left(x^{3}\right)}{-\frac{x^{4}}{2}+o\left(x^{5}\right)}=\frac{-\frac{1}{3}+o\left(x^{3}\right)}{-\frac{1}{2}+o\left(x^{5}\right)} \end{aligned} 4) \lim _{x \rightarrow 0} \frac{-\frac{1}{3}+o\left(x^{3}\right)}{-\frac{1}{2}+o\left(x^{5}\right)}=\frac{-\frac{1}{3}}{-\frac{1}{2}}=\frac{2}{3} o o(1/x)","['limits', 'taylor-expansion', 'limits-without-lhopital']"
84,Show that $\cos(2^n)$ diverges,Show that  diverges,\cos(2^n),Problem: I have sequence defined as follows: $$ a_n = \cos(2^n) $$ I need to show that it diverges. My progress: I tried common method here supposing that this sequence has limit $\lim_{n\to\infty}\cos(2^n) = a$ and then trying to get to contradiction with $\lim_{n\to\infty}(\cos(2^{n+1})-\cos(2^n))=0$ . But it led me nowhere. How can I show that this sequence diverges?,Problem: I have sequence defined as follows: I need to show that it diverges. My progress: I tried common method here supposing that this sequence has limit and then trying to get to contradiction with . But it led me nowhere. How can I show that this sequence diverges?,"
a_n = \cos(2^n)
 \lim_{n\to\infty}\cos(2^n) = a \lim_{n\to\infty}(\cos(2^{n+1})-\cos(2^n))=0","['calculus', 'sequences-and-series', 'limits', 'convergence-divergence']"
85,"Properly arguing for basic limit laws: We have to do it backwards? I mean, we're not sure the limits exist...","Properly arguing for basic limit laws: We have to do it backwards? I mean, we're not sure the limits exist...",,"Ok dumb questions given all the questions I've asked before on this account, but here goes: Example/Question 1 : When we evaluate things like $\lim_{x \to 0} [2x^2 + 5x]$ , is it actually improper to say like $\lim_{x \to 0} [2x^2 + 5x] = \lim_{x \to 0} 2x^2 + \lim_{x \to 0} 5x$ $ = 2 \lim_{x \to 0} x^2 + 5\lim_{x \to 0} x$ $ = 2 (0) + 5 (0) = 0+0 = 0$ ? Context : This seems to be how it is done in, say, Stewart Calculus . See the Stewart Calculus limit laws . The limit laws can be used ASSUMING certain limits involved exist in the 1st place. So for example, I don't see how can we possibly say $$\lim_{x \to 0} [2x^2 + 5x] = \lim_{x \to 0} 2x^2 + \lim_{x \to 0} 5x$$ when we haven't established that both $\lim_{x \to 0} 2x^2$ and $\lim_{x \to 0} 5x$ exist What I think we should do is that the above kind of argument is scratch work and then the proper argument is as follows (similar to the $\varepsilon-\delta$ thing where we argue backwards from $\varepsilon$ to $\delta$ as scratch and then write the formal proof forwards from $\delta$ to $\varepsilon$ ): $\lim_{x \to 0} [2x^2 + 5x]$ exists as the sum of the following limits, if the following limits exist: $\lim_{x \to 0} 2x^2$ , $\lim_{x \to 0} 5x$ . $\lim_{x \to 0} 2x^2$ exists as 2 times the following limit, if the following limit exists: $\lim_{x \to 0} x^2$ $\lim_{x \to 0} 5x$ exists as 5 times the following limit, if the following limit exists: $\lim_{x \to 0} x$ $\lim_{x \to 0} x^2 = 0$ $\lim_{x \to 0} x = 0$ By (5) and (3), $\lim_{x \to 0} 5x$ exists and is equal to $5(0)=0$ By (4) and (2), $\lim_{x \to 0} 2x^2$ exists and is equal to $2(0)=0$ By (1), (6) and (7), $\lim_{x \to 0} [2x^2 + 5x]$ exists and is equal to $0+0=0$ . This seems very weird, unnatural, etc. For some reason ever since elementary calculus this is not what is being done. Yet, I think this should be the case otherwise we may fall into traps like $\lim_{x \to 0} \frac{x}{1} \frac{1}{x} = \lim_{x \to 0} \frac{x}{1} \lim_{x \to 0} \frac{1}{x} = (1)$ (does not exist) = does not exist. I think I fell for this kind of trap here . Please explain what's going on. Example/Question 2 : (a real multivariable example. I think there's an easy way to do this in single real, but I can't think of an example right now.) Here , I am trying to argue that $\lim_{(x,y) \to (0,0)}e^{\frac{y}{x^2+y^2}}\cos(\frac{x}{x^2+y^2})$ doesn't exist because $\lim_{\substack{(x,y) \to (0,0) \\ y=0}}e^{\frac{y}{x^2+y^2}}\cos(\frac{x}{x^2+y^2})$ doesn't exist because $\lim_{\substack{(x,y) \to (0,0) \\ y=0}}e^{\frac{y}{x^2+y^2}}\cos(\frac{x}{x^2+y^2}) = \lim_{\substack{x \to 0}}\cos(\frac{1}{x})$ and then because $\lim_{\substack{x \to 0}}\cos(\frac{1}{x})$ doesn't exist. Actually, how is it even sensible to do this entire long list of limit equalities $$\lim_{\substack{(x,y) \to (0,0) \\ y=0}}e^{\frac{y}{x^2+y^2}}\cos(\frac{x}{x^2+y^2}) = (...) \text{long list of limit equalities} (...) = \lim_{\substack{x \to 0}}\cos(\frac{1}{x})$$ when we're not even sure that the limits exist? Guess : Perhaps there's some implicit reductio ad absurdum here like 'suppose on the contrary that this limit exists. Then this limit equals (...) that limit. But that limit doesn't exist! Contradiction.' Example/Question 3 : Actually now I want to ask about $\lim_{\substack{x \to 0}}\cos(\frac{1}{x})$ , but I'm afraid the post will become too broad (if it isn't already)... Update : Asked here . Maybe related: Limit laws when not both limits exist Why use limit laws to verify continuity instead of direct substitution? Is it enough to show that $\lim_{x\rightarrow 0}\cos(1/x)$ doesn't exist to show that $\lim_{x \rightarrow0}(2x\sin(1/x)-\cos(1/x))$ doesn't exist? Why does this limit exist? Product of limits.","Ok dumb questions given all the questions I've asked before on this account, but here goes: Example/Question 1 : When we evaluate things like , is it actually improper to say like ? Context : This seems to be how it is done in, say, Stewart Calculus . See the Stewart Calculus limit laws . The limit laws can be used ASSUMING certain limits involved exist in the 1st place. So for example, I don't see how can we possibly say when we haven't established that both and exist What I think we should do is that the above kind of argument is scratch work and then the proper argument is as follows (similar to the thing where we argue backwards from to as scratch and then write the formal proof forwards from to ): exists as the sum of the following limits, if the following limits exist: , . exists as 2 times the following limit, if the following limit exists: exists as 5 times the following limit, if the following limit exists: By (5) and (3), exists and is equal to By (4) and (2), exists and is equal to By (1), (6) and (7), exists and is equal to . This seems very weird, unnatural, etc. For some reason ever since elementary calculus this is not what is being done. Yet, I think this should be the case otherwise we may fall into traps like (does not exist) = does not exist. I think I fell for this kind of trap here . Please explain what's going on. Example/Question 2 : (a real multivariable example. I think there's an easy way to do this in single real, but I can't think of an example right now.) Here , I am trying to argue that doesn't exist because doesn't exist because and then because doesn't exist. Actually, how is it even sensible to do this entire long list of limit equalities when we're not even sure that the limits exist? Guess : Perhaps there's some implicit reductio ad absurdum here like 'suppose on the contrary that this limit exists. Then this limit equals (...) that limit. But that limit doesn't exist! Contradiction.' Example/Question 3 : Actually now I want to ask about , but I'm afraid the post will become too broad (if it isn't already)... Update : Asked here . Maybe related: Limit laws when not both limits exist Why use limit laws to verify continuity instead of direct substitution? Is it enough to show that $\lim_{x\rightarrow 0}\cos(1/x)$ doesn't exist to show that $\lim_{x \rightarrow0}(2x\sin(1/x)-\cos(1/x))$ doesn't exist? Why does this limit exist? Product of limits.","\lim_{x \to 0} [2x^2 + 5x] \lim_{x \to 0} [2x^2 + 5x] = \lim_{x \to 0} 2x^2 + \lim_{x \to 0} 5x  = 2 \lim_{x \to 0} x^2 + 5\lim_{x \to 0} x  = 2 (0) + 5 (0) = 0+0 = 0 \lim_{x \to 0} [2x^2 + 5x] = \lim_{x \to 0} 2x^2 + \lim_{x \to 0} 5x \lim_{x \to 0} 2x^2 \lim_{x \to 0} 5x \varepsilon-\delta \varepsilon \delta \delta \varepsilon \lim_{x \to 0} [2x^2 + 5x] \lim_{x \to 0} 2x^2 \lim_{x \to 0} 5x \lim_{x \to 0} 2x^2 \lim_{x \to 0} x^2 \lim_{x \to 0} 5x \lim_{x \to 0} x \lim_{x \to 0} x^2 = 0 \lim_{x \to 0} x = 0 \lim_{x \to 0} 5x 5(0)=0 \lim_{x \to 0} 2x^2 2(0)=0 \lim_{x \to 0} [2x^2 + 5x] 0+0=0 \lim_{x \to 0} \frac{x}{1} \frac{1}{x} = \lim_{x \to 0} \frac{x}{1} \lim_{x \to 0} \frac{1}{x} = (1) \lim_{(x,y) \to (0,0)}e^{\frac{y}{x^2+y^2}}\cos(\frac{x}{x^2+y^2}) \lim_{\substack{(x,y) \to (0,0) \\ y=0}}e^{\frac{y}{x^2+y^2}}\cos(\frac{x}{x^2+y^2}) \lim_{\substack{(x,y) \to (0,0) \\ y=0}}e^{\frac{y}{x^2+y^2}}\cos(\frac{x}{x^2+y^2}) = \lim_{\substack{x \to 0}}\cos(\frac{1}{x}) \lim_{\substack{x \to 0}}\cos(\frac{1}{x}) \lim_{\substack{(x,y) \to (0,0) \\ y=0}}e^{\frac{y}{x^2+y^2}}\cos(\frac{x}{x^2+y^2}) = (...) \text{long list of limit equalities} (...) = \lim_{\substack{x \to 0}}\cos(\frac{1}{x}) \lim_{\substack{x \to 0}}\cos(\frac{1}{x})","['calculus', 'limits']"
86,Conjecture: $\lim\limits_{x\to\infty}\operatorname{Re}\text W_x(x)\mathop=\limits^?-\ln(2\pi)$,Conjecture:,\lim\limits_{x\to\infty}\operatorname{Re}\text W_x(x)\mathop=\limits^?-\ln(2\pi),"The inspiration for the question is Closed form of $$\frac{d}{dk}\text W_k(z)$$ Derivative of W-Lambert function with respect to its branch cuts experiment. I also like making functions central to my work. So I though about “centralizing” the Generalized W-Lambert function so that the branch cut is the same as the argument: $$\text W_k(z)=\text W_x(x)$$ I then considered what the value of $$\text W_{\pm \infty}(\pm \infty)$$ would be. Assume that the signs can be chosen in any order. I found a conjectured closed form. For simplicity, let’s consider the convergent real part of the expression. Let’s also further constrain our problem by choosing signs to be $$\text{Re}(\text W_{ \infty}( \infty))$$ $$\lim_{x\to\pm \infty}\text {Re}(\text W_x(x))\mathop=^{\large ?} -\ln(2\pi)$$ The imaginary part turn out to be asymptotic to $$\pm 2\pi i \infty=\pm\infty i$$ so it is just an infinite complex number. I found the possible result after using a discrete limit . For example, the value at $x=10^{10}$ is the following. I used the the real an imaginary part as well as different signs for completion. $$\text W_x(x)= \text W_x(-x)= \text W_{x}(\pm ix) =-1.8378770663843454835603092354803385245074740939... + 6.2831853070225068442428720324366974719089270342... × 10^{10} i, \text W_{-x}(-x)= \text W_{-x}(x)=\text W_{-x}(\pm ix)=-1.8378770663343454835578092354801887222706941730... - 6.2831853067083475788838927085903664574425628653... × 10^{10} i $$ Note the $2\pi$ approximation in the scientific notation if the imaginary part. As said before, the real part is approximately: $$-\ln(2\pi)= -1.8378770663843454835603092354803385245074740939... $$ Here are the Inverse Symbolic Calculator results . How can you formally evaluate $$\text{Re}\lim_{x\to \infty} \text W_x(x)$$ using a discrete limit? Please correct me and give me feedback! Just for fun, the Wright Omega function can also have the interesting identity that: $$ω(z)\mathop=^\text{def} \text W_{\left \lceil\frac{\text{Im}(z)}{2\pi}-\frac12\right\rceil}\left(e^z\right)\implies -\ln(2\pi)=\lim_{x\to\infty} ω(\ln(x)+2i\pi  x)$$ Where $$\left\lceil\frac{\text{Im}(z)}{2\pi}-\frac12\right\rceil =\frac{i\left(\ln\left(e^z\right)-z\right)}{2\pi}=\text{unwindK(z)}=\text K(z)$$ is called the Unwinding Number","The inspiration for the question is Closed form of Derivative of W-Lambert function with respect to its branch cuts experiment. I also like making functions central to my work. So I though about “centralizing” the Generalized W-Lambert function so that the branch cut is the same as the argument: I then considered what the value of would be. Assume that the signs can be chosen in any order. I found a conjectured closed form. For simplicity, let’s consider the convergent real part of the expression. Let’s also further constrain our problem by choosing signs to be The imaginary part turn out to be asymptotic to so it is just an infinite complex number. I found the possible result after using a discrete limit . For example, the value at is the following. I used the the real an imaginary part as well as different signs for completion. Note the approximation in the scientific notation if the imaginary part. As said before, the real part is approximately: Here are the Inverse Symbolic Calculator results . How can you formally evaluate using a discrete limit? Please correct me and give me feedback! Just for fun, the Wright Omega function can also have the interesting identity that: Where is called the Unwinding Number","\frac{d}{dk}\text W_k(z) \text W_k(z)=\text W_x(x) \text W_{\pm \infty}(\pm \infty) \text{Re}(\text W_{ \infty}( \infty)) \lim_{x\to\pm \infty}\text {Re}(\text W_x(x))\mathop=^{\large ?} -\ln(2\pi) \pm 2\pi i \infty=\pm\infty i x=10^{10} \text W_x(x)= \text W_x(-x)= \text W_{x}(\pm ix) =-1.8378770663843454835603092354803385245074740939... +
6.2831853070225068442428720324366974719089270342... × 10^{10} i, \text W_{-x}(-x)= \text W_{-x}(x)=\text W_{-x}(\pm ix)=-1.8378770663343454835578092354801887222706941730... -
6.2831853067083475788838927085903664574425628653... × 10^{10} i  2\pi -\ln(2\pi)= -1.8378770663843454835603092354803385245074740939...  \text{Re}\lim_{x\to \infty} \text W_x(x) ω(z)\mathop=^\text{def} \text W_{\left \lceil\frac{\text{Im}(z)}{2\pi}-\frac12\right\rceil}\left(e^z\right)\implies -\ln(2\pi)=\lim_{x\to\infty} ω(\ln(x)+2i\pi  x) \left\lceil\frac{\text{Im}(z)}{2\pi}-\frac12\right\rceil =\frac{i\left(\ln\left(e^z\right)-z\right)}{2\pi}=\text{unwindK(z)}=\text K(z)","['limits', 'solution-verification', 'lambert-w', 'branch-cuts', 'discrete-calculus']"
87,Limit of expression as x approaches 0,Limit of expression as x approaches 0,,"I'm working with this limit, where I can't use any Maclaurin series, nor can I use L'Hopitals Rule. I'd be pleased if anyone could help me out with this one: $$\lim_{x\rightarrow 0}\frac{e^{x^{2}}-\cos(x)}{\sin^2x}$$ I'm trying to convert the problem into a limit where I can work with the standard limits, such as $cos(x)/x$ , but I'm not successful in doing so. Thanks for any potential tips!","I'm working with this limit, where I can't use any Maclaurin series, nor can I use L'Hopitals Rule. I'd be pleased if anyone could help me out with this one: I'm trying to convert the problem into a limit where I can work with the standard limits, such as , but I'm not successful in doing so. Thanks for any potential tips!",\lim_{x\rightarrow 0}\frac{e^{x^{2}}-\cos(x)}{\sin^2x} cos(x)/x,"['limits', 'limits-without-lhopital']"
88,When to apply l'Hôpital rule?,When to apply l'Hôpital rule?,,"I'm confused about when to apply l'Hôpital Rule, here is an example: \begin{align} L=\lim_{x \to\infty} \frac{1}{x^2}\int_0^x \frac{t}{2+\sin(t)}dt\\ \end{align} Approach 1 : Let $F(x)$ is antiderivative of $\frac{x}{2+\sin(x)}$ , then $\int_ 0^x \frac{t}{2+\sin(t)}dt= F(x)-F(0)= F(x)+C$ . So we got our limit: \begin{align} L=\lim_{x\to\infty} \frac{F(x)+C}{x^2}\\ \end{align} It's easy to see that this is $\frac{\infty}{\infty}$ case so we apply l'Hôpital Rule: \begin{align} L=\lim_{x\to\infty} \frac{\frac{d}{dx}(F(x)+C)}{\frac{d}{dx}(x^2)}=\lim_{x\to\infty} \frac{\frac{x}{2+\sin(x)}}{2x}= \lim_{x\to\infty} \frac{1}{4+2\sin(x)}\\ \end{align} However, the limit results in nothing. Approach 2 : Use $\lim _{x \to\infty} \frac{1}{x^2}\int_0^x tf(\sin(t))dt=\frac{1}{4\pi}\int_{-\pi}^{\pi} f(\sin(t))dt $ . We have: \begin{align} L= \frac{1}{4\pi}\int_{-\pi}^{\pi} \frac{1}{2+\sin(t)}dt=\frac{1}{2\sqrt{3}}\\ \end{align} Can you help me point out any mistakes in approach 1 and why the l'Hôpital Rule doesn't work?","I'm confused about when to apply l'Hôpital Rule, here is an example: Approach 1 : Let is antiderivative of , then . So we got our limit: It's easy to see that this is case so we apply l'Hôpital Rule: However, the limit results in nothing. Approach 2 : Use . We have: Can you help me point out any mistakes in approach 1 and why the l'Hôpital Rule doesn't work?",\begin{align} L=\lim_{x \to\infty} \frac{1}{x^2}\int_0^x \frac{t}{2+\sin(t)}dt\\ \end{align} F(x) \frac{x}{2+\sin(x)} \int_ 0^x \frac{t}{2+\sin(t)}dt= F(x)-F(0)= F(x)+C \begin{align} L=\lim_{x\to\infty} \frac{F(x)+C}{x^2}\\ \end{align} \frac{\infty}{\infty} \begin{align} L=\lim_{x\to\infty} \frac{\frac{d}{dx}(F(x)+C)}{\frac{d}{dx}(x^2)}=\lim_{x\to\infty} \frac{\frac{x}{2+\sin(x)}}{2x}= \lim_{x\to\infty} \frac{1}{4+2\sin(x)}\\ \end{align} \lim _{x \to\infty} \frac{1}{x^2}\int_0^x tf(\sin(t))dt=\frac{1}{4\pi}\int_{-\pi}^{\pi} f(\sin(t))dt  \begin{align} L= \frac{1}{4\pi}\int_{-\pi}^{\pi} \frac{1}{2+\sin(t)}dt=\frac{1}{2\sqrt{3}}\\ \end{align},"['calculus', 'limits']"
89,How to solve a limit that can be factored but doesn't help?,How to solve a limit that can be factored but doesn't help?,,"I saw examples that can be factored, eliminating the part that causes the indetermination, none of this type. The other option is by rationalize but dont know how to apply it here. $$\lim_{x \to 4} \frac{2x^2+7x+5}{x^2-16}$$ I tried by factoring, doesn't help $$\lim_{x \rightarrow 4} \frac{(2x+5)(x+1)}{(x-4)(x+4)} \\$$ UPDATE : I make a mistake, the numerator is ${2x^2+7x+5}$ not ${x^2+7x+5}$ , really sorry.","I saw examples that can be factored, eliminating the part that causes the indetermination, none of this type. The other option is by rationalize but dont know how to apply it here. I tried by factoring, doesn't help UPDATE : I make a mistake, the numerator is not , really sorry.",\lim_{x \to 4} \frac{2x^2+7x+5}{x^2-16} \lim_{x \rightarrow 4} \frac{(2x+5)(x+1)}{(x-4)(x+4)} \\ {2x^2+7x+5} {x^2+7x+5},['limits']
90,Use Epsilon Definition of Limit to Prove Limit Exists,Use Epsilon Definition of Limit to Prove Limit Exists,,"We have the function $$f(x,y)= \begin{cases} 0  & \textrm{if } x=y=0 \\[2ex] (x+y)\ln(x^2+y^2) & \textrm{otherwise} \end{cases}$$ and need to prove $\lim\limits_{(x,y) \to (0,0)} f(x,y) = 0$ . We need to show that for any $\epsilon>0$ we can find a $\delta > 0$ such that for any $(x.y)$ with $0< \sqrt{x^2+y^2}<\delta$ , we have $|f(x,y)| < \epsilon$ Some of the steps are: $$\begin{align}|f(x,y)| &= \left|(x+y)\ln(x^2+y^2)\right| \\&= \left|\frac{2(x+y)}{\sqrt{x^2+y^2}}\sqrt{x^2+y^2}\ln(\sqrt{x^2+y^2})\right| \\&\le 2\sqrt{2}\left|\sqrt{x^2+y^2}\ln(\sqrt{x^2+y^2})\right| \end{align}$$ but I do not understand how you get including expression including $\sqrt{x^2+y^2}$ after plugging in the value to the absolute value and how that relates to the inequality. There has to be a step I am missing.","We have the function and need to prove . We need to show that for any we can find a such that for any with , we have Some of the steps are: but I do not understand how you get including expression including after plugging in the value to the absolute value and how that relates to the inequality. There has to be a step I am missing.","f(x,y)=
\begin{cases}
0  & \textrm{if } x=y=0 \\[2ex]
(x+y)\ln(x^2+y^2) & \textrm{otherwise}
\end{cases} \lim\limits_{(x,y) \to (0,0)} f(x,y) = 0 \epsilon>0 \delta > 0 (x.y) 0< \sqrt{x^2+y^2}<\delta |f(x,y)| < \epsilon \begin{align}|f(x,y)| &= \left|(x+y)\ln(x^2+y^2)\right| \\&= \left|\frac{2(x+y)}{\sqrt{x^2+y^2}}\sqrt{x^2+y^2}\ln(\sqrt{x^2+y^2})\right| \\&\le 2\sqrt{2}\left|\sqrt{x^2+y^2}\ln(\sqrt{x^2+y^2})\right|
\end{align} \sqrt{x^2+y^2}","['real-analysis', 'limits', 'proof-explanation', 'epsilon-delta']"
91,Evaluating the limit $\lim\limits_{n\to\infty} \frac{2^{n^{k}}}{n!}$,Evaluating the limit,\lim\limits_{n\to\infty} \frac{2^{n^{k}}}{n!},"Determine, with proof, the value of the limit $L = \lim\limits_{n\to\infty} \dfrac{2^{n^{k}}}{n!}$ where $k=1.1.$ I think it's infinite because $\frac{2^{n^k}}{n!}\ge (\frac{2^{n^{0.1}}}{n})^n=(2^{n^{0.1}-\log n})^n\to \infty.$","Determine, with proof, the value of the limit where I think it's infinite because",L = \lim\limits_{n\to\infty} \dfrac{2^{n^{k}}}{n!} k=1.1. \frac{2^{n^k}}{n!}\ge (\frac{2^{n^{0.1}}}{n})^n=(2^{n^{0.1}-\log n})^n\to \infty.,"['real-analysis', 'calculus', 'limits', 'asymptotics']"
92,Solution verification for limit problem $\lim\limits_{x\to 0}\dfrac{\sin(4x)\tan^2(3x)+6x^2}{2x^2+\sin(3x)\cos(2x)}.$,Solution verification for limit problem,\lim\limits_{x\to 0}\dfrac{\sin(4x)\tan^2(3x)+6x^2}{2x^2+\sin(3x)\cos(2x)}.,Find the value of $$\lim\limits_{x\to 0}\dfrac{\sin(4x)\tan^2(3x)+6x^2}{2x^2+\sin(3x)\cos(2x)}.$$ I try as below. \begin{align} \lim\limits_{x\to 0}\dfrac{\sin(4x)\tan^2(3x)+6x^2}{2x^2+\sin(3x)\cos(2x)}&=\lim\limits_{x\to 0}\dfrac{6x^2\left(\dfrac{\sin(4x)\tan^2(3x)}{6x^2}+1\right)}{2x^2\left(1+\dfrac{\sin(3x)\cos(2x)}{2x^2}\right)}\\ &=3\lim\limits_{x\to 0}\dfrac{\left(\dfrac{\sin(4x)}{4x}\cdot\left(\dfrac{\tan(3x)}{3x}\right)^2 6x+1\right)}{\left(1+\dfrac{\dfrac{1}{2}\left(\sin(5x)+\sin(x)\right)}{2x^2}\right)}\\ &=3\lim\limits_{x\to 0}\dfrac{\left(\dfrac{\sin(4x)}{4x}\cdot\left(\dfrac{\tan(3x)}{3x}\right)^2 6x+1\right)}{\left(1+\dfrac{\sin(5x)}{4x^2}+\dfrac{\sin(x)}{4x^2}\right)}\\ &=3\lim\limits_{x\to 0}\dfrac{\left(\dfrac{\sin(4x)}{4x}\cdot\left(\dfrac{\tan(3x)}{3x}\right)^2 6x+1\right)}{\dfrac{1}{x}\left(x+\dfrac{5}{4}\dfrac{\sin(5x)}{5x}+\dfrac{1}{4}\dfrac{\sin(x)}{x}\right)}\\ &=3\lim\limits_{x\to 0}\dfrac{\left(\dfrac{\sin(4x)}{4x}\cdot\left(\dfrac{\tan(3x)}{3x}\right)^2 6x^2+x\right)}{\left(x+\dfrac{5}{4}\dfrac{\sin(5x)}{5x}+\dfrac{1}{4}\dfrac{\sin(x)}{x}\right)}\\ &=3\dfrac{\left(\lim\limits_{x\to 0}\dfrac{\sin(4x)}{4x}\left(\lim\limits_{x\to 0}\dfrac{\tan(3x)}{3x}\right)^2 \lim\limits_{x\to 0} 6x^2+\lim\limits_{x\to 0} x\right)}{\left(\lim\limits_{x\to 0} x+\dfrac{5}{4}\lim\limits_{x\to 0} \dfrac{\sin(5x)}{5x}+\dfrac{1}{4}\lim\limits_{x\to 0} \dfrac{\sin(x)}{x}\right)}\\ &=3\cdot\dfrac{1\cdot 1^2\cdot 0+0}{0+\dfrac{5}{4}+\dfrac{1}{4}}\\ &=0. \end{align} I'm not sure with my answer. Does my answer correct?,Find the value of I try as below. I'm not sure with my answer. Does my answer correct?,"\lim\limits_{x\to 0}\dfrac{\sin(4x)\tan^2(3x)+6x^2}{2x^2+\sin(3x)\cos(2x)}. \begin{align}
\lim\limits_{x\to 0}\dfrac{\sin(4x)\tan^2(3x)+6x^2}{2x^2+\sin(3x)\cos(2x)}&=\lim\limits_{x\to 0}\dfrac{6x^2\left(\dfrac{\sin(4x)\tan^2(3x)}{6x^2}+1\right)}{2x^2\left(1+\dfrac{\sin(3x)\cos(2x)}{2x^2}\right)}\\
&=3\lim\limits_{x\to 0}\dfrac{\left(\dfrac{\sin(4x)}{4x}\cdot\left(\dfrac{\tan(3x)}{3x}\right)^2 6x+1\right)}{\left(1+\dfrac{\dfrac{1}{2}\left(\sin(5x)+\sin(x)\right)}{2x^2}\right)}\\
&=3\lim\limits_{x\to 0}\dfrac{\left(\dfrac{\sin(4x)}{4x}\cdot\left(\dfrac{\tan(3x)}{3x}\right)^2 6x+1\right)}{\left(1+\dfrac{\sin(5x)}{4x^2}+\dfrac{\sin(x)}{4x^2}\right)}\\
&=3\lim\limits_{x\to 0}\dfrac{\left(\dfrac{\sin(4x)}{4x}\cdot\left(\dfrac{\tan(3x)}{3x}\right)^2 6x+1\right)}{\dfrac{1}{x}\left(x+\dfrac{5}{4}\dfrac{\sin(5x)}{5x}+\dfrac{1}{4}\dfrac{\sin(x)}{x}\right)}\\
&=3\lim\limits_{x\to 0}\dfrac{\left(\dfrac{\sin(4x)}{4x}\cdot\left(\dfrac{\tan(3x)}{3x}\right)^2 6x^2+x\right)}{\left(x+\dfrac{5}{4}\dfrac{\sin(5x)}{5x}+\dfrac{1}{4}\dfrac{\sin(x)}{x}\right)}\\
&=3\dfrac{\left(\lim\limits_{x\to 0}\dfrac{\sin(4x)}{4x}\left(\lim\limits_{x\to 0}\dfrac{\tan(3x)}{3x}\right)^2 \lim\limits_{x\to 0} 6x^2+\lim\limits_{x\to 0} x\right)}{\left(\lim\limits_{x\to 0} x+\dfrac{5}{4}\lim\limits_{x\to 0} \dfrac{\sin(5x)}{5x}+\dfrac{1}{4}\lim\limits_{x\to 0} \dfrac{\sin(x)}{x}\right)}\\
&=3\cdot\dfrac{1\cdot 1^2\cdot 0+0}{0+\dfrac{5}{4}+\dfrac{1}{4}}\\
&=0.
\end{align}","['limits', 'trigonometry', 'solution-verification']"
93,How do I evaluate this limit that seems to be indeterminate?,How do I evaluate this limit that seems to be indeterminate?,,"I've been trying to evaluate the below limit, which Mathematica claims is equals to $1$ . $$\underset{n\to \infty }{\text{lim}}\frac{\frac{1}{2} (n+1) \sin \left(\frac{2 \pi }{n+1}\right)-\pi }{\frac{1}{2} n \sin \left(\frac{2 \pi }{n}\right)-\pi }$$ However, my attempts, of which I attempted using L'Hopital's rule, always end with the indeterminate form of $\frac{0}{0}$ , because taking the limit to infinity of both the non- $\pi$ sections of the numerator and denominators gives $\pi$ . Is Mathematica correct, and if so, what is the solution?","I've been trying to evaluate the below limit, which Mathematica claims is equals to . However, my attempts, of which I attempted using L'Hopital's rule, always end with the indeterminate form of , because taking the limit to infinity of both the non- sections of the numerator and denominators gives . Is Mathematica correct, and if so, what is the solution?",1 \underset{n\to \infty }{\text{lim}}\frac{\frac{1}{2} (n+1) \sin \left(\frac{2 \pi }{n+1}\right)-\pi }{\frac{1}{2} n \sin \left(\frac{2 \pi }{n}\right)-\pi } \frac{0}{0} \pi \pi,['limits']
94,Algebraic function that acts like $\sin(1/x)$ near zero. (or non-trig function) [closed],Algebraic function that acts like  near zero. (or non-trig function) [closed],\sin(1/x),"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Trying to construct an example for a Business Calculus class (meaning trig functions are not necessary for the curriculum). However, I want to touch on the limit problem involved with the $\sin(1/x)$ function. I am sure there is a simple function, or there isn't... But would love some insight. I also understand that the functions that satisfy this condition are maybe way outside the scope of the course. I'm just looking for different ""flavors"" of showing limits that don't exist besides just showing the limit from the left and the limit from the right does not exists.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Trying to construct an example for a Business Calculus class (meaning trig functions are not necessary for the curriculum). However, I want to touch on the limit problem involved with the function. I am sure there is a simple function, or there isn't... But would love some insight. I also understand that the functions that satisfy this condition are maybe way outside the scope of the course. I'm just looking for different ""flavors"" of showing limits that don't exist besides just showing the limit from the left and the limit from the right does not exists.",\sin(1/x),"['calculus', 'limits', 'education']"
95,Erroneous functional limit involving floor function,Erroneous functional limit involving floor function,,"I am working through Understanding Analysis 2nd Ed. and I am having some difficulty working with functional limits involving floor functions. Exercise 4.2.4: Consider the reasonable but erroneous claim that $\lim_{x \to 10} \frac{1}{\lfloor x \rfloor} = \frac{1}{10}$ (a) Find the largest $\delta$ that represents a proper response to the challenge $\epsilon = \frac{1}{2}$ . Note: $x-1 < \lfloor x \rfloor \leq x$ Rough work: $$0 < \lvert x - 10 \rvert < \delta \rightarrow \lvert \frac{1}{ \lfloor x \rfloor} - \frac{1}{10} \rvert < \epsilon$$ $$\frac{1}{10} - \epsilon < \frac{1}{ \lfloor x \rfloor} < \epsilon + \frac{1}{10}$$ $$-\frac{2}{5} < \frac{1}{ \lfloor x \rfloor} < \frac{3}{5}$$ $$ \lfloor x \rfloor < -\frac{5}{2} \quad and \quad \lfloor x \rfloor > \frac{5}{3}$$ $$ x < -3 \quad and \quad x > 2$$ I'm looking for some way to formulate $0 < \lvert x - 10 \rvert < \delta$ again in order to determine a $\delta$ . Intuitively looking at this I don't seen how a delta-ball exists because of the disjoint-ness of the inequality. This makes me think that I either 1) incorrectly applied the inversion of $\frac{1}{ \lfloor x \rfloor}$ to the inequality, or 2) this is not a good approach to solving the problem. I've been able to solve some floor function limits intuitively (just looking at the behavior in general), but I'd like to be more rigorous in my explanations. Any guidance here would be greatly appreciated!","I am working through Understanding Analysis 2nd Ed. and I am having some difficulty working with functional limits involving floor functions. Exercise 4.2.4: Consider the reasonable but erroneous claim that (a) Find the largest that represents a proper response to the challenge . Note: Rough work: I'm looking for some way to formulate again in order to determine a . Intuitively looking at this I don't seen how a delta-ball exists because of the disjoint-ness of the inequality. This makes me think that I either 1) incorrectly applied the inversion of to the inequality, or 2) this is not a good approach to solving the problem. I've been able to solve some floor function limits intuitively (just looking at the behavior in general), but I'd like to be more rigorous in my explanations. Any guidance here would be greatly appreciated!",\lim_{x \to 10} \frac{1}{\lfloor x \rfloor} = \frac{1}{10} \delta \epsilon = \frac{1}{2} x-1 < \lfloor x \rfloor \leq x 0 < \lvert x - 10 \rvert < \delta \rightarrow \lvert \frac{1}{ \lfloor x \rfloor} - \frac{1}{10} \rvert < \epsilon \frac{1}{10} - \epsilon < \frac{1}{ \lfloor x \rfloor} < \epsilon + \frac{1}{10} -\frac{2}{5} < \frac{1}{ \lfloor x \rfloor} < \frac{3}{5}  \lfloor x \rfloor < -\frac{5}{2} \quad and \quad \lfloor x \rfloor > \frac{5}{3}  x < -3 \quad and \quad x > 2 0 < \lvert x - 10 \rvert < \delta \delta \frac{1}{ \lfloor x \rfloor},"['real-analysis', 'limits']"
96,Distributional Limit $T_{g_{n}} \rightarrow \delta_{0}$,Distributional Limit,T_{g_{n}} \rightarrow \delta_{0},"Consider $$ \psi(x)=\left\{\begin{array}{ll} e^{-\frac{1}{1-x^{2}}}, & |x|<1 \\ 0, & |x| \geqslant 1 \end{array}\right. $$ and $$ g_{n}(x)=\frac{\psi(n x)}{\displaystyle\int_{-\infty}^{\infty} \psi(n x) \: \mathrm{d} x} $$ for $n \in \mathbb{N}$ . I need to prove that $T_{g_{n}} \rightarrow \delta_{0}$ in $\mathscr{D}^{\prime}$ for $n \rightarrow+\infty$ . ( $T_{g_{n}}$ denotes the regular distribution induced by $g_n$ ). I have tried to write down the action of $T_{g_n}$ on a test function $\phi(x)$ as: $$\langle T_{g_n}, \phi(x)\rangle=\int_{-\frac 1n}^{\frac 1n} \frac{e^{-\frac{1}{1-n^2x^{2}}}}{\displaystyle\int_{-\frac 1n}^{\frac 1n} e^{-\frac{1}{1-n^2y^{2}}} \: \mathrm{d} y} \phi(x) \: \mathrm{d}x = \frac{1}{\displaystyle\int_{-\frac 1n}^{\frac 1n} e^{-\frac{1}{1-n^2y^{2}}} \: \mathrm{d} y} \int_{-\frac 1n}^{\frac 1n} e^{-\frac{1}{1-n^2x^{2}}} \phi(x) \: \mathrm{d} x $$ But I really don't know what to do from here. Any ideas?",Consider and for . I need to prove that in for . ( denotes the regular distribution induced by ). I have tried to write down the action of on a test function as: But I really don't know what to do from here. Any ideas?,"
\psi(x)=\left\{\begin{array}{ll}
e^{-\frac{1}{1-x^{2}}}, & |x|<1 \\
0, & |x| \geqslant 1
\end{array}\right.
 
g_{n}(x)=\frac{\psi(n x)}{\displaystyle\int_{-\infty}^{\infty} \psi(n x) \: \mathrm{d} x}
 n \in \mathbb{N} T_{g_{n}} \rightarrow \delta_{0} \mathscr{D}^{\prime} n \rightarrow+\infty T_{g_{n}} g_n T_{g_n} \phi(x) \langle T_{g_n}, \phi(x)\rangle=\int_{-\frac 1n}^{\frac 1n} \frac{e^{-\frac{1}{1-n^2x^{2}}}}{\displaystyle\int_{-\frac 1n}^{\frac 1n} e^{-\frac{1}{1-n^2y^{2}}} \: \mathrm{d} y} \phi(x) \: \mathrm{d}x = \frac{1}{\displaystyle\int_{-\frac 1n}^{\frac 1n} e^{-\frac{1}{1-n^2y^{2}}} \: \mathrm{d} y} \int_{-\frac 1n}^{\frac 1n} e^{-\frac{1}{1-n^2x^{2}}} \phi(x) \: \mathrm{d} x ","['real-analysis', 'limits', 'distribution-theory']"
97,Calculate $\lim_{n \to \infty} \dfrac{\ln (\log_a (n))-\ln (\log_n (a))}{\ln (n)}$,Calculate,\lim_{n \to \infty} \dfrac{\ln (\log_a (n))-\ln (\log_n (a))}{\ln (n)},"I have to calculate $\lim_{n \to \infty} \dfrac{\ln (\log_a (n))-\ln (\log_n (a))}{\ln (n)}$ My idea Let $a_n = \dfrac{\ln (\log_a (n))-\ln (\log_n (a))}{\ln (n)}$ , then $$e^{a_n}= (\log_a (n))^{1/\ln(n)} \cdot \left( \dfrac{1}{\log_n (a)} \right)^{1/\ln(n)} $$ And I don't know to continue","I have to calculate My idea Let , then And I don't know to continue",\lim_{n \to \infty} \dfrac{\ln (\log_a (n))-\ln (\log_n (a))}{\ln (n)} a_n = \dfrac{\ln (\log_a (n))-\ln (\log_n (a))}{\ln (n)} e^{a_n}= (\log_a (n))^{1/\ln(n)} \cdot \left( \dfrac{1}{\log_n (a)} \right)^{1/\ln(n)} ,"['limits', 'limits-without-lhopital']"
98,Why can $\lim_{x \to c} f(x)$ be found by simplifying $f(x)$?,Why can  be found by simplifying ?,\lim_{x \to c} f(x) f(x),"More formally, for $f(x)$ undefined at $x = c$ , why does $\lim_{x \to c} f(x) = g(c)$ , where $g(x)$ is an algebraic simplification of $f(x)$ ? To use an example of this simplification from the Wikepedia page about limits , $$   \lim_{x \to 1} \frac{x^2 - 1}{x - 1} = \lim_{x \to 1} \frac{(x+1)(x-1)}{x - 1} = \lim_{x \to 1} (x + 1) = 1 + 1 = 2. $$ Here, the function $f(x) = (x^2 - 1)/(x - 1)$ is simplified to $g(x) = x + 1$ , which is continuous at $x = 1$ . $g(1)$ is then evaluated to find the limit of $f(x)$ . How do we know that this results in the correct limit for every case where $f(x)$ can be simplified to a function defined at $x = c$ ?","More formally, for undefined at , why does , where is an algebraic simplification of ? To use an example of this simplification from the Wikepedia page about limits , Here, the function is simplified to , which is continuous at . is then evaluated to find the limit of . How do we know that this results in the correct limit for every case where can be simplified to a function defined at ?","f(x) x = c \lim_{x \to c} f(x) = g(c) g(x) f(x) 
  \lim_{x \to 1} \frac{x^2 - 1}{x - 1}
= \lim_{x \to 1} \frac{(x+1)(x-1)}{x - 1}
= \lim_{x \to 1} (x + 1)
= 1 + 1 = 2.
 f(x) = (x^2 - 1)/(x - 1) g(x) = x + 1 x = 1 g(1) f(x) f(x) x = c","['calculus', 'limits']"
99,Suppose $\sum_{n = 1}^{\infty} a_n$ is absolutely convergent. Prove $\sum_{n = 1}^{\infty} b_n$ is absolutely convergent if $|b_n| \leq |a_n|$.,Suppose  is absolutely convergent. Prove  is absolutely convergent if .,\sum_{n = 1}^{\infty} a_n \sum_{n = 1}^{\infty} b_n |b_n| \leq |a_n|,"I am having a bit of trouble with this proof and I am not exactly sure how to prove it. Suppose $\displaystyle\sum_{n = 1}^{\infty} a_n$ is absolutely convergent. Prove $\displaystyle\sum_{n = 1}^{\infty} b_n$ is absolutely convergent if $|b_n| \leq |a_n|$ for all $n \geq 1$ . I started the question by using the Ratio Test, and by definition, the series is absolutely convergent if $\displaystyle \left|\dfrac{a_{n + 1}}{a_n}\right| < 1$ . So if $\displaystyle \left|\dfrac{a_{n + 1}}{a_n}\right| < 1$ , then this means that \begin{equation*} |a_{n + 1}| < |a_n| \end{equation*} I am not exactly sure how to proceed from here. I would appreciate some help and advice.","I am having a bit of trouble with this proof and I am not exactly sure how to prove it. Suppose is absolutely convergent. Prove is absolutely convergent if for all . I started the question by using the Ratio Test, and by definition, the series is absolutely convergent if . So if , then this means that I am not exactly sure how to proceed from here. I would appreciate some help and advice.","\displaystyle\sum_{n = 1}^{\infty} a_n \displaystyle\sum_{n = 1}^{\infty} b_n |b_n| \leq |a_n| n \geq 1 \displaystyle \left|\dfrac{a_{n + 1}}{a_n}\right| < 1 \displaystyle \left|\dfrac{a_{n + 1}}{a_n}\right| < 1 \begin{equation*}
|a_{n + 1}| < |a_n|
\end{equation*}","['calculus', 'sequences-and-series', 'limits', 'proof-writing']"
