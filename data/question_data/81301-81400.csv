,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"If $\mathbf{A}$ is a $2\times 2$ matrix that satisfies $\mathbf{A}^2 - 4\mathbf{A} - 7\mathbf{I} = \mathbf{0}$, then $\mathbf{A}$ is invertible","If  is a  matrix that satisfies , then  is invertible",\mathbf{A} 2\times 2 \mathbf{A}^2 - 4\mathbf{A} - 7\mathbf{I} = \mathbf{0} \mathbf{A},"$\mathbf{A}$ is a $2\times 2$ matrix which satisfies   $\mathbf{A}^2 - 4\mathbf{A} - 7\mathbf{I} = \mathbf{0}$,   where $\mathbf{I}$ is the $2\times 2$ identity matrix. Prove that $\mathbf{A}$ is invertible. Hint: Find a matrix $\mathbf{B}$ such that $\mathbf{A}\mathbf{B}=\mathbf{I}$. I tried substituting a variable matrix for $\mathbf{A}$ and substituting $\mathbf{I}$'s value into both the original equation and the hint, but the result was full of equations and didn't seem to help much at all. I would appreciate advice for this problem. Thanks. --Grace","$\mathbf{A}$ is a $2\times 2$ matrix which satisfies   $\mathbf{A}^2 - 4\mathbf{A} - 7\mathbf{I} = \mathbf{0}$,   where $\mathbf{I}$ is the $2\times 2$ identity matrix. Prove that $\mathbf{A}$ is invertible. Hint: Find a matrix $\mathbf{B}$ such that $\mathbf{A}\mathbf{B}=\mathbf{I}$. I tried substituting a variable matrix for $\mathbf{A}$ and substituting $\mathbf{I}$'s value into both the original equation and the hint, but the result was full of equations and didn't seem to help much at all. I would appreciate advice for this problem. Thanks. --Grace",,"['linear-algebra', 'matrices']"
1,Find values of $t$ so that this matrix is positive definite,Find values of  so that this matrix is positive definite,t,"I will start from this point: $\det{\left(B-\lambda I\right)}=0\Longleftrightarrow\begin{vmatrix}t-\lambda&3&1\\3&t-\lambda&0\\1&0&t-\lambda\end{vmatrix}=0$ Now we will compute the determinant: $(t-\lambda)^3-9(t-\lambda)-(t-\lambda)=-\lambda^3+t^3-3\lambda t^2+3\lambda^2t-10t+10\lambda=$ $t^3 -  (-10\lambda + 3 \lambda^2 + \lambda^3) +    (-10t + 3 \lambda^2t)$ Now i have a problem, how can i simplify that equations so that i will find the values of t so that matrix B is positive definite?","I will start from this point: $\det{\left(B-\lambda I\right)}=0\Longleftrightarrow\begin{vmatrix}t-\lambda&3&1\\3&t-\lambda&0\\1&0&t-\lambda\end{vmatrix}=0$ Now we will compute the determinant: $(t-\lambda)^3-9(t-\lambda)-(t-\lambda)=-\lambda^3+t^3-3\lambda t^2+3\lambda^2t-10t+10\lambda=$ $t^3 -  (-10\lambda + 3 \lambda^2 + \lambda^3) +    (-10t + 3 \lambda^2t)$ Now i have a problem, how can i simplify that equations so that i will find the values of t so that matrix B is positive definite?",,"['linear-algebra', 'matrices']"
2,"Let matrix $A$ be normal. If $A{A^T}$ has $n$ distinct eigenvalues, why is $A$ symmetric? [closed]","Let matrix  be normal. If  has  distinct eigenvalues, why is  symmetric? [closed]",A A{A^T} n A,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Let matrix $A \in {M_n}(\Bbb R)$ be normal. If $A{A^T}$ has $n$ distinct eigenvalues, why is $A$ symmetric?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Let matrix be normal. If has distinct eigenvalues, why is symmetric?",A \in {M_n}(\Bbb R) A{A^T} n A,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'symmetric-matrices']"
3,Is there some sort of relationship between SVD decomposition and diagonalizability?,Is there some sort of relationship between SVD decomposition and diagonalizability?,,"If $L$ is a linear operator acting on a hilbert space $H$ of dimension $n$ ( $L: H \to H$ ), then I know the following If $L$ is normal operator then in some orthonormal basis $B$, matrix representation of $L$, $[L]_{B}$ will be a diagonal matrix. If sum of dimensions of eigen spaces of $L$ is equal to $n$ then in some non-orthonormal basis matrix representation of $L$ is diagonal. In general for any $L$, if its matrix representation in a basis $B$ is $[L]_B$ then $[L_B] = UDV$ where $U,V$ are unitary matrices and $D$ a diagonal matrix. I can see that point $1$ and $2$ are the same thing ie if $L$ is normal operator. Is there any other relationship between SVD decomposition and diagonalizability ?","If $L$ is a linear operator acting on a hilbert space $H$ of dimension $n$ ( $L: H \to H$ ), then I know the following If $L$ is normal operator then in some orthonormal basis $B$, matrix representation of $L$, $[L]_{B}$ will be a diagonal matrix. If sum of dimensions of eigen spaces of $L$ is equal to $n$ then in some non-orthonormal basis matrix representation of $L$ is diagonal. In general for any $L$, if its matrix representation in a basis $B$ is $[L]_B$ then $[L_B] = UDV$ where $U,V$ are unitary matrices and $D$ a diagonal matrix. I can see that point $1$ and $2$ are the same thing ie if $L$ is normal operator. Is there any other relationship between SVD decomposition and diagonalizability ?",,"['linear-algebra', 'matrices']"
4,If $A$ can be written as a sum of nilpotent matrices why $trcA=0$?,If  can be written as a sum of nilpotent matrices why ?,A trcA=0,"Let $A \in {M_n}$. If $A$ can be written as a sum of two nilpotent matrices, why $trcA=0$?","Let $A \in {M_n}$. If $A$ can be written as a sum of two nilpotent matrices, why $trcA=0$?",,"['linear-algebra', 'matrices']"
5,Is this matrix positive semidefinite for all $n$?,Is this matrix positive semidefinite for all ?,n,This is an extension of my previous question ( see here ). In this follow-up problem extra ones have been added in the non-diagonal matrix elements. We want to prove the positive semi-definiteness of the $n^2\times n^2$ matrix  $$Q=\left[\matrix{\mathbb{I}_n & e_1e_2^T+e_2e_1^T &  \cdots & e_1e_n^T+e_ne_1^T\\ e_2e_1^T+e_1e_2^T & \mathbb{I}_n & \cdots & e_2e_n^T+e_ne_2^T \\ \vdots & \vdots & \ddots & \vdots\\ e_ne_1^T+e_1e_n^T & e_ne_2^T+e_2e_n^T &\cdots & \mathbb{I}_n}\right]$$ where $\mathbb{I}_n$ is the  $n\times n$ identity matrix and $e_i$ is the $i$-th column of  $\mathbb{I}_n$. Numerical computations in MATLAB up to $n=4$ indicate that $Q$ is indeed positive semidefinite but I can't see a direct proof of this. Edit: This question is related to the proof of the fact  $$\min_{\|x\|=1}\lambda\left(xx^T-\textrm{diag}(xx^T)\right)=-\frac{1}{2}$$  See here for details.,This is an extension of my previous question ( see here ). In this follow-up problem extra ones have been added in the non-diagonal matrix elements. We want to prove the positive semi-definiteness of the $n^2\times n^2$ matrix  $$Q=\left[\matrix{\mathbb{I}_n & e_1e_2^T+e_2e_1^T &  \cdots & e_1e_n^T+e_ne_1^T\\ e_2e_1^T+e_1e_2^T & \mathbb{I}_n & \cdots & e_2e_n^T+e_ne_2^T \\ \vdots & \vdots & \ddots & \vdots\\ e_ne_1^T+e_1e_n^T & e_ne_2^T+e_2e_n^T &\cdots & \mathbb{I}_n}\right]$$ where $\mathbb{I}_n$ is the  $n\times n$ identity matrix and $e_i$ is the $i$-th column of  $\mathbb{I}_n$. Numerical computations in MATLAB up to $n=4$ indicate that $Q$ is indeed positive semidefinite but I can't see a direct proof of this. Edit: This question is related to the proof of the fact  $$\min_{\|x\|=1}\lambda\left(xx^T-\textrm{diag}(xx^T)\right)=-\frac{1}{2}$$  See here for details.,,"['linear-algebra', 'matrices']"
6,Determine the coefficient of polynomial det(I + xA),Determine the coefficient of polynomial det(I + xA),,Given matrix an n-by-n matrix $A$ and its $n$ eigenvalues. How do I determine the coefficient of the term $x^2$ of the polynomial given by $q(x) = \det(I_n + xA)$,Given matrix an n-by-n matrix $A$ and its $n$ eigenvalues. How do I determine the coefficient of the term $x^2$ of the polynomial given by $q(x) = \det(I_n + xA)$,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'determinant']"
7,Can someone explain Wishart distribution?,Can someone explain Wishart distribution?,,I have to use Wishart distribution to model noise in images. Can someone explain or give intuition behind wishart distribution. Thank you !!!,I have to use Wishart distribution to model noise in images. Can someone explain or give intuition behind wishart distribution. Thank you !!!,,"['probability', 'matrices', 'probability-distributions', 'random-variables', 'image-processing']"
8,"Let $A$ be a complex matrix such that $A^n = I$, show that $A$ is diagonalisable.","Let  be a complex matrix such that , show that  is diagonalisable.",A A^n = I A,"Let $A$ be a complex matrix such that $A^n = I$, show that $A$ is diagonalisable. How do I do this? I would hazard a guess that since $A^n = I$ that means that $A^n$ is obvious diagonal, and I suppose that a diagonal matrix can only be formed by the square of another diagonal matrix, but I am not sure if that is the case. Also I think this neglects the complex cases. Is there a Jordan block argument that I should use?","Let $A$ be a complex matrix such that $A^n = I$, show that $A$ is diagonalisable. How do I do this? I would hazard a guess that since $A^n = I$ that means that $A^n$ is obvious diagonal, and I suppose that a diagonal matrix can only be formed by the square of another diagonal matrix, but I am not sure if that is the case. Also I think this neglects the complex cases. Is there a Jordan block argument that I should use?",,"['abstract-algebra', 'matrices', 'diagonalization']"
9,About idempotent and invertible matrix,About idempotent and invertible matrix,,"So I get a square matrix $A$ is idempotent when $A^2 = A$. How can you prove that if $A$ is an $n \times n$ matrix that is idempotent and invertible, then  $A = I$?? I'm quite confused on this. Any help much appreciated, Thanks.","So I get a square matrix $A$ is idempotent when $A^2 = A$. How can you prove that if $A$ is an $n \times n$ matrix that is idempotent and invertible, then  $A = I$?? I'm quite confused on this. Any help much appreciated, Thanks.",,"['linear-algebra', 'matrices']"
10,Definition of an induced matrix norm.,Definition of an induced matrix norm.,,"Could someone explain the second equality in the definition of a induced matrix norm to me? Let $\| \cdot \|$ be a norm for $\cdot \in \mathbb{R}^n$,    then the induced matrixnorm for $A\in \mathbb{R}^{n\times n}$ is given by: $$\|A\| = \sup_{x\not = 0} \frac{\|Ax\|}{\|x\|} \color{red}{\stackrel{?}{=}} \max_{\|x\|= 1} \|Ax\|$$ Problem: Why does $\color{red}{=}$ hold? I know $\|Ax\|$ is continous for all $x\in \mathbb{R}^n$ and  $\{x \in \mathbb{R}^n :\|x\| = 1 \}$ is compact which implies $\dfrac{\|Ax\|}{\|x\|}$   to reach a maximum according to Weierstrass. But why is this maximum also the maximum for all $x$? (even those with norm bigger or less then 1?)","Could someone explain the second equality in the definition of a induced matrix norm to me? Let $\| \cdot \|$ be a norm for $\cdot \in \mathbb{R}^n$,    then the induced matrixnorm for $A\in \mathbb{R}^{n\times n}$ is given by: $$\|A\| = \sup_{x\not = 0} \frac{\|Ax\|}{\|x\|} \color{red}{\stackrel{?}{=}} \max_{\|x\|= 1} \|Ax\|$$ Problem: Why does $\color{red}{=}$ hold? I know $\|Ax\|$ is continous for all $x\in \mathbb{R}^n$ and  $\{x \in \mathbb{R}^n :\|x\| = 1 \}$ is compact which implies $\dfrac{\|Ax\|}{\|x\|}$   to reach a maximum according to Weierstrass. But why is this maximum also the maximum for all $x$? (even those with norm bigger or less then 1?)",,"['matrices', 'normed-spaces', 'definition', 'supremum-and-infimum', 'matrix-norms']"
11,Minimal polynomial of an $n\times n$ matrix $A$ is $x^3+2x+2$; then $3$ divides $n$,Minimal polynomial of an  matrix  is ; then  divides,n\times n A x^3+2x+2 3 n,Let $A$ be an $n × n$ matrix with rational entries such that the minimal polynomial of $A$ is $x^3 + 2x+2$. Prove that $3$ divides $n$. I think there is no rational root of this polynomial but how I use this. Please give me hint.,Let $A$ be an $n × n$ matrix with rational entries such that the minimal polynomial of $A$ is $x^3 + 2x+2$. Prove that $3$ divides $n$. I think there is no rational root of this polynomial but how I use this. Please give me hint.,,"['linear-algebra', 'matrices', 'minimal-polynomials']"
12,Norm of orthogonal projection,Norm of orthogonal projection,,Consider $\Bbb R^n$ with the standard inner product and let $P$ be an orthogonal projection defined on $\Bbb R^n$. It is known that the operator norm of $P$ induced by the inner product is less than or equal $1$. Let $\|\cdot\|$ be any arbitrary norm on $\Bbb R^n$. Is it true that the operator norm of $P$ induced by the new norm is less than or equal to $1$?,Consider $\Bbb R^n$ with the standard inner product and let $P$ be an orthogonal projection defined on $\Bbb R^n$. It is known that the operator norm of $P$ induced by the inner product is less than or equal $1$. Let $\|\cdot\|$ be any arbitrary norm on $\Bbb R^n$. Is it true that the operator norm of $P$ induced by the new norm is less than or equal to $1$?,,"['linear-algebra', 'matrices', 'operator-theory']"
13,"If $A,B$ are square matrices and $A^2=A,B^2=B,AB=BA$, then calculate $\det (A-B)$","If  are square matrices and , then calculate","A,B A^2=A,B^2=B,AB=BA \det (A-B)","If $A,B$ are square matrices and $A^2=A,B^2=B,AB=BA$, then calculate $\det (A-B)$. My solution: consider $(A-B)^3=A^3-3A^2B+3AB^2-B^3=A^3-B^3=A-B$, then $\det(A-B)=0\vee 1\vee -1$ The result of the book is the same as mine, but their solution is different. They begin: ""Since $A^2=A$ and $B^2=B$, $A$ and $B$ are diagonalizable; moreover, since $AB=BA$, then  there exists a invertible matrix $P$ such that $P^{-1}AP$ and $P^{-1}BP$ are diagonal matrices."" The statement has two parts, and I don't understand both. So I really need a specific explanation for those two parts. Thanks in advance.","If $A,B$ are square matrices and $A^2=A,B^2=B,AB=BA$, then calculate $\det (A-B)$. My solution: consider $(A-B)^3=A^3-3A^2B+3AB^2-B^3=A^3-B^3=A-B$, then $\det(A-B)=0\vee 1\vee -1$ The result of the book is the same as mine, but their solution is different. They begin: ""Since $A^2=A$ and $B^2=B$, $A$ and $B$ are diagonalizable; moreover, since $AB=BA$, then  there exists a invertible matrix $P$ such that $P^{-1}AP$ and $P^{-1}BP$ are diagonal matrices."" The statement has two parts, and I don't understand both. So I really need a specific explanation for those two parts. Thanks in advance.",,"['linear-algebra', 'matrices', 'determinant', 'diagonalization']"
14,Alternative matrix representation for translation,Alternative matrix representation for translation,,"The ''usual"" way to write translation for $\textbf{v}\in \mathbb{R}^2$ is with the following $3\times3$ matrix $$  \left( {\begin{array}{ccc}    1 & 0 & x_{0} \\    0 & 1 & y_{0} \\    0 & 0 & 1 \\   \end{array} } \right) \cdot \left( {\begin{array}{c}    x \\    y \\    1 \\   \end{array} } \right)= \left( {\begin{array}{c} x+x_{0}\\y+y_{0}\\1\end{array}}\right)$$ But also it seems to be possible to write this transformation with the following $2\times2$  matrix: $$  \left( {\begin{array}{cc}    1 & \frac{x_{0}}{y} \\    \frac{y_{0}}{x} & 1 \\    \end{array} } \right) \cdot \left( {\begin{array}{c}    x \\    y \\     \end{array} } \right)= \left( {\begin{array}{c} x+x_{0}\\y+y_{0}\end{array}}\right)$$ I didn't find a source using this as a version of translation, what are the problems associated with this representation?","The ''usual"" way to write translation for $\textbf{v}\in \mathbb{R}^2$ is with the following $3\times3$ matrix $$  \left( {\begin{array}{ccc}    1 & 0 & x_{0} \\    0 & 1 & y_{0} \\    0 & 0 & 1 \\   \end{array} } \right) \cdot \left( {\begin{array}{c}    x \\    y \\    1 \\   \end{array} } \right)= \left( {\begin{array}{c} x+x_{0}\\y+y_{0}\\1\end{array}}\right)$$ But also it seems to be possible to write this transformation with the following $2\times2$  matrix: $$  \left( {\begin{array}{cc}    1 & \frac{x_{0}}{y} \\    \frac{y_{0}}{x} & 1 \\    \end{array} } \right) \cdot \left( {\begin{array}{c}    x \\    y \\     \end{array} } \right)= \left( {\begin{array}{c} x+x_{0}\\y+y_{0}\end{array}}\right)$$ I didn't find a source using this as a version of translation, what are the problems associated with this representation?",,"['linear-algebra', 'matrices', 'linear-transformations']"
15,Why does Givens rotation avoid iteration and Jacobi rotation doesn't in case of reducing a symmetric matrix to tridiagonal?,Why does Givens rotation avoid iteration and Jacobi rotation doesn't in case of reducing a symmetric matrix to tridiagonal?,,"I am currently implementing symmetric matrix reduction to tridiagonal. I read that Givens rotation avoids iteration when it is used for reducing a matrix to tridiagonal whereas Jacobi rotation is iterative. Givens rotation try to annihilate the element $a_{i-1j}$ by a rotation in $ij-$plane and Jacobi rotation try to annihilate the element $a_{ij}$ by a rotation in $ij-$plane. But just because of that, how can Givens rotation avoid iteration as both use same equations? $a^{'}_{rp}=ca_{rp} - sa_{rq}$ $ a^{'}_{rq}=ca_{rp} + sa_{rq}$ $ a^{'}_{pp}=c^{2}a_{pp}+s^{2}a_{qq}-2sca_{pq}$ $a^{'}_qq=s^{2}a_{pp}+c^{2}a_{qq}+2sca_{pq}$ $a^{'}_{pq}=(c^{2}-s^{2})a_{pq}+sc(a_{pp}-a_{qq})$ Now in Jacobi rotation , they try to zero out $a^{'}_{pq}$ and in Givens rotation they try to zero out $a^{'}_{p-1q}$ i.e.,$ a^{'}_{rq}$.","I am currently implementing symmetric matrix reduction to tridiagonal. I read that Givens rotation avoids iteration when it is used for reducing a matrix to tridiagonal whereas Jacobi rotation is iterative. Givens rotation try to annihilate the element $a_{i-1j}$ by a rotation in $ij-$plane and Jacobi rotation try to annihilate the element $a_{ij}$ by a rotation in $ij-$plane. But just because of that, how can Givens rotation avoid iteration as both use same equations? $a^{'}_{rp}=ca_{rp} - sa_{rq}$ $ a^{'}_{rq}=ca_{rp} + sa_{rq}$ $ a^{'}_{pp}=c^{2}a_{pp}+s^{2}a_{qq}-2sca_{pq}$ $a^{'}_qq=s^{2}a_{pp}+c^{2}a_{qq}+2sca_{pq}$ $a^{'}_{pq}=(c^{2}-s^{2})a_{pq}+sc(a_{pp}-a_{qq})$ Now in Jacobi rotation , they try to zero out $a^{'}_{pq}$ and in Givens rotation they try to zero out $a^{'}_{p-1q}$ i.e.,$ a^{'}_{rq}$.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
16,"Matrix exponential: Formal notation for power series? Or, more?","Matrix exponential: Formal notation for power series? Or, more?",,"For a square matrix $A$, I'm already used to see and use: $$\sum_{n=0}^{\infty} \frac{A^n}{n!} = \lim_{n \to \infty} \left(I + \frac{A}{n}\right)^n = e^A$$ Which means a matrix $A$ is just like some complex number (apart from commutation) for which the above is factual. My question is simply whether the last $=$ is ""for real"", or should it really be replaced with $\equiv$ (definition)? (I remember that I have had a similar ""doubt"" regarding $e^{ix} = \cos x + i \sin x$ ($x \in \mathbb{R}$). But then I learned how to prove it.)","For a square matrix $A$, I'm already used to see and use: $$\sum_{n=0}^{\infty} \frac{A^n}{n!} = \lim_{n \to \infty} \left(I + \frac{A}{n}\right)^n = e^A$$ Which means a matrix $A$ is just like some complex number (apart from commutation) for which the above is factual. My question is simply whether the last $=$ is ""for real"", or should it really be replaced with $\equiv$ (definition)? (I remember that I have had a similar ""doubt"" regarding $e^{ix} = \cos x + i \sin x$ ($x \in \mathbb{R}$). But then I learned how to prove it.)",,"['matrices', 'limits', 'power-series', 'exponentiation']"
17,Why does this expression for the minimum eigenvalue work?,Why does this expression for the minimum eigenvalue work?,,"I read that we can have, for a column vector $d \in R^N$, \begin{eqnarray} min_d = \frac{d' A d}{d'd} = \lambda_{min} \end{eqnarray} , where $\lambda_{min}$ is the smallest eigenvalue of the symmetric, pos. def. matrix $A \in R^{n \times n}$. Now, why can we say that? When I apply the eigendecomposition theorem, I obtain: \begin{eqnarray} \frac{d' V \Lambda V' ~d}{d'd}, \end{eqnarray} where $\Lambda$ is a diagonal matrix stuffed with the eigenvalues and $V$ is the matrix of eigenvectors. So I can see where this is going by picking the elements of $d$ such that we pick out the smallest eigenvalue but I am disturbed by the $V$ matrices that somehow don't seem to go away. $A$ is symmetric and positive definite. I would greatly appreciate some guidance. Thank you so much! Hirek","I read that we can have, for a column vector $d \in R^N$, \begin{eqnarray} min_d = \frac{d' A d}{d'd} = \lambda_{min} \end{eqnarray} , where $\lambda_{min}$ is the smallest eigenvalue of the symmetric, pos. def. matrix $A \in R^{n \times n}$. Now, why can we say that? When I apply the eigendecomposition theorem, I obtain: \begin{eqnarray} \frac{d' V \Lambda V' ~d}{d'd}, \end{eqnarray} where $\Lambda$ is a diagonal matrix stuffed with the eigenvalues and $V$ is the matrix of eigenvectors. So I can see where this is going by picking the elements of $d$ such that we pick out the smallest eigenvalue but I am disturbed by the $V$ matrices that somehow don't seem to go away. $A$ is symmetric and positive definite. I would greatly appreciate some guidance. Thank you so much! Hirek",,"['matrices', 'eigenvalues-eigenvectors']"
18,Derivative of the trace of $X^TP^TPX$ with respect to $P$,Derivative of the trace of  with respect to,X^TP^TPX P,Consider $$\mbox{Tr} (X^TP^TPX)$$ where $X$ and $P$ are real matrices. What is the best way to approach the calculation of its derivative with respect to $P$?,Consider $$\mbox{Tr} (X^TP^TPX)$$ where $X$ and $P$ are real matrices. What is the best way to approach the calculation of its derivative with respect to $P$?,,"['matrices', 'derivatives', 'matrix-calculus', 'trace', 'scalar-fields']"
19,Questions on Jordan forms,Questions on Jordan forms,,"I am studying Jordan form of a matrix from wiki. I am wondering how could two matrices have same eigenvalues with same multiplicities, but have different Jordan form? Also, if two matrices have different Jordan forms, does this mean that these two matrices are not similar? Thank you so much!","I am studying Jordan form of a matrix from wiki. I am wondering how could two matrices have same eigenvalues with same multiplicities, but have different Jordan form? Also, if two matrices have different Jordan forms, does this mean that these two matrices are not similar? Thank you so much!",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'jordan-normal-form']"
20,Find $\min(\operatorname{trace}(AA^T))$ for invertible $A_{n\times n}$,Find  for invertible,\min(\operatorname{trace}(AA^T)) A_{n\times n},"For invertible $A_{n\times n}$ find $\min(\operatorname{trace}(AA^T))$ (a) $0$ (b) $1$ (c) $n$ (d) $n^2$ Clearly for $A=I$ , it is $n$ , and I am unable to get any lower values, but how do I prove it.","For invertible find (a) (b) (c) (d) Clearly for , it is , and I am unable to get any lower values, but how do I prove it.",A_{n\times n} \min(\operatorname{trace}(AA^T)) 0 1 n n^2 A=I n,"['linear-algebra', 'matrices', 'trace']"
21,"Is the set of diagonalizable, complex matrices open in the set of square matrices?","Is the set of diagonalizable, complex matrices open in the set of square matrices?",,"Is the set of complex, diagonalizable matrices open in the set of square matrices? I asked myself this question and I tried to prove it somehow. However, I don't have any good approach so far. What is the best way to prove this?","Is the set of complex, diagonalizable matrices open in the set of square matrices? I asked myself this question and I tried to prove it somehow. However, I don't have any good approach so far. What is the best way to prove this?",,"['linear-algebra', 'matrices', 'diagonalization']"
22,Uniqueness of Singular Values,Uniqueness of Singular Values,,"Given a matrix A, one inductively constructs (and thus proving its very existence) the singular value decomposition as follows: take $ \sigma_{1}=||A||_{2} $, and consider a couple of vectors such that  \begin{equation}  A\textbf{v}_{1} = \sigma_{1} \textbf{u}_{1}  \end{equation} Now extend to two orhogonal bases and write down the first step: \begin{equation}   U_{1}^{T} A V_{1} = S =   \begin{bmatrix}  \sigma_{1} & \textbf{0}^{T} \\   \textbf{0} & A_{1}  \end{bmatrix}   \end{equation}  Now take $A_{1}$, and repeat inductively, costructing \begin{equation} U_{2}^{T} A_{1} V_{2} = S_{2} =  \begin{bmatrix}  \sigma_{2} & \textbf{0}^{T} \\   \textbf{0} & A_{2}  \end{bmatrix}  \end{equation} which gives rise to second singular value, by selecting $\sigma_{2}=||A_{1}||$. My question is: suppose we choose another couple of extensions to   orthogonal bases, call them $\hat{U_{1}},\hat{V_{1}}$. Doing so, we'll   obtain another matrix $\hat{A_{1}}$: who guarantees this   $\hat{A_{1}}$ has the same norm of ${A_{1}}$, thus giving the same   second singular value $\sigma_{2}$? Thanks.","Given a matrix A, one inductively constructs (and thus proving its very existence) the singular value decomposition as follows: take $ \sigma_{1}=||A||_{2} $, and consider a couple of vectors such that  \begin{equation}  A\textbf{v}_{1} = \sigma_{1} \textbf{u}_{1}  \end{equation} Now extend to two orhogonal bases and write down the first step: \begin{equation}   U_{1}^{T} A V_{1} = S =   \begin{bmatrix}  \sigma_{1} & \textbf{0}^{T} \\   \textbf{0} & A_{1}  \end{bmatrix}   \end{equation}  Now take $A_{1}$, and repeat inductively, costructing \begin{equation} U_{2}^{T} A_{1} V_{2} = S_{2} =  \begin{bmatrix}  \sigma_{2} & \textbf{0}^{T} \\   \textbf{0} & A_{2}  \end{bmatrix}  \end{equation} which gives rise to second singular value, by selecting $\sigma_{2}=||A_{1}||$. My question is: suppose we choose another couple of extensions to   orthogonal bases, call them $\hat{U_{1}},\hat{V_{1}}$. Doing so, we'll   obtain another matrix $\hat{A_{1}}$: who guarantees this   $\hat{A_{1}}$ has the same norm of ${A_{1}}$, thus giving the same   second singular value $\sigma_{2}$? Thanks.",,"['linear-algebra', 'matrices', 'svd']"
23,why the matrix is diagonalizable?,why the matrix is diagonalizable?,,"show that matrix $$A = \begin{bmatrix} a & b \\         0 & a         \end{bmatrix}$$ is diagonalizable iff $b = 0.$ I do not understand why. Cuz if a, b are reals, I can always find a constant: $a = cb$ and row-reduce. Can anyone explain, please.","show that matrix $$A = \begin{bmatrix} a & b \\         0 & a         \end{bmatrix}$$ is diagonalizable iff $b = 0.$ I do not understand why. Cuz if a, b are reals, I can always find a constant: $a = cb$ and row-reduce. Can anyone explain, please.",,"['linear-algebra', 'matrices', 'diagonalization']"
24,Inverse of Triangular matrix [duplicate],Inverse of Triangular matrix [duplicate],,"This question already has answers here : Inverse of an invertible triangular matrix (either upper or lower) is triangular of the same kind (10 answers) Closed 9 years ago . I want to know if the inverse of a Triangular matrix (Cholesky decomposition of a symmetric, positive-definite matrix in my particular case) is still triangular. If so could someone provide a proof as well?","This question already has answers here : Inverse of an invertible triangular matrix (either upper or lower) is triangular of the same kind (10 answers) Closed 9 years ago . I want to know if the inverse of a Triangular matrix (Cholesky decomposition of a symmetric, positive-definite matrix in my particular case) is still triangular. If so could someone provide a proof as well?",,"['linear-algebra', 'matrices']"
25,eigenvalues of the sum of a diagonal and diagonalizable matrix,eigenvalues of the sum of a diagonal and diagonalizable matrix,,"Let $A$ be a matrix with a nonzero entry $t_0$ in the (1,1) entry and padded with zeros everywhere else; and let $B$ be a diagonalizable matrix, such that $VBV^T$ is diagonal. Is there anything we can say about the eigenvalues of $A+B$ if we know the eigenvalues of $A$ and $B$? What if $A$ is merely a diagonal matrix?","Let $A$ be a matrix with a nonzero entry $t_0$ in the (1,1) entry and padded with zeros everywhere else; and let $B$ be a diagonalizable matrix, such that $VBV^T$ is diagonal. Is there anything we can say about the eigenvalues of $A+B$ if we know the eigenvalues of $A$ and $B$? What if $A$ is merely a diagonal matrix?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
26,On a particular matrix homomorphism,On a particular matrix homomorphism,,"I recently noticed that, for $2\times 2$ matrices, dividing one off-diagonal element by a constant while multiplying the other off-diagonal element by the same constant produces a matrix homomorphism. i.e., if we call this operation $g$, then it's defined as $$g\left(\begin{bmatrix}a & b\\c & d\end{bmatrix}\right) =\begin{bmatrix}a & (b\cdot\lambda)\\(c/\lambda) & d\end{bmatrix} $$ and then the homomorphism properties are: $$ g(A) g(B) = g(AB) $$ (where juxtaposition indicates the usual matrix multiplication) and $$ g(A)+g(B) = g(A + B).$$ My question(s): Does this homomorphism have a name, any useful applications, and does it hint at any kind of deeper structure?  Are there others like it?","I recently noticed that, for $2\times 2$ matrices, dividing one off-diagonal element by a constant while multiplying the other off-diagonal element by the same constant produces a matrix homomorphism. i.e., if we call this operation $g$, then it's defined as $$g\left(\begin{bmatrix}a & b\\c & d\end{bmatrix}\right) =\begin{bmatrix}a & (b\cdot\lambda)\\(c/\lambda) & d\end{bmatrix} $$ and then the homomorphism properties are: $$ g(A) g(B) = g(AB) $$ (where juxtaposition indicates the usual matrix multiplication) and $$ g(A)+g(B) = g(A + B).$$ My question(s): Does this homomorphism have a name, any useful applications, and does it hint at any kind of deeper structure?  Are there others like it?",,['matrices']
27,The rank of the matrix $A$,The rank of the matrix,A,"Let $A$ be a $5\times 4$ matrix with real entries such that $Ax=0$ iff $x=0$ where $x$ is a $4\times 1$ vector and $0$ is a null vector. What is the rank of $A$? I can't understand how to do it, please someone help.","Let $A$ be a $5\times 4$ matrix with real entries such that $Ax=0$ iff $x=0$ where $x$ is a $4\times 1$ vector and $0$ is a null vector. What is the rank of $A$? I can't understand how to do it, please someone help.",,"['linear-algebra', 'matrices']"
28,Powers of matrices equality,Powers of matrices equality,,"let $A$ be a $3$ by $3$ matrix with two eigenvalues $\lambda _1, \lambda _2$ such that $\lambda _1$ has algebraic multiplicity $2$ and $\lambda _2$ has multiplicity $1$. I want to prove that $(A-\lambda _1I)^{n+2}=(\lambda _2-\lambda _1)^n(A-\lambda _1I)^2$. As a hint I'm given $$A-\lambda _2I=A-\lambda _1I-(\lambda _2-\lambda _1)I$$ $$(A-\lambda _1)^2(A-\lambda _2)=0$$ How can I use the above to prove that $$(A-\lambda _1I)^{n+2}=(\lambda _2-\lambda _1)^n(A-\lambda _1I)^2 \text{for all }n\in \mathbb N?$$ I have tried powering $A-\lambda _2I=A-\lambda _1I-(\lambda _2-\lambda _1)I$ but it didn't help at all.","let $A$ be a $3$ by $3$ matrix with two eigenvalues $\lambda _1, \lambda _2$ such that $\lambda _1$ has algebraic multiplicity $2$ and $\lambda _2$ has multiplicity $1$. I want to prove that $(A-\lambda _1I)^{n+2}=(\lambda _2-\lambda _1)^n(A-\lambda _1I)^2$. As a hint I'm given $$A-\lambda _2I=A-\lambda _1I-(\lambda _2-\lambda _1)I$$ $$(A-\lambda _1)^2(A-\lambda _2)=0$$ How can I use the above to prove that $$(A-\lambda _1I)^{n+2}=(\lambda _2-\lambda _1)^n(A-\lambda _1I)^2 \text{for all }n\in \mathbb N?$$ I have tried powering $A-\lambda _2I=A-\lambda _1I-(\lambda _2-\lambda _1)I$ but it didn't help at all.",,"['linear-algebra', 'matrices']"
29,Finding Jordan form,Finding Jordan form,,"Find Jordan form of the following matrix: $$\left(\begin{matrix}4&-5&2 \\ 5&-7&3\\ 6&-9&4 \end{matrix}\right)$$ So I got stuck pretty much trying to find the eigenvalues. Related question: Is the characteristic polynomial of the characteristic matrix, equals to the characteristic polynomial of the transpose of the characteristic matrix? Since their determinants are equal... Thanks!","Find Jordan form of the following matrix: $$\left(\begin{matrix}4&-5&2 \\ 5&-7&3\\ 6&-9&4 \end{matrix}\right)$$ So I got stuck pretty much trying to find the eigenvalues. Related question: Is the characteristic polynomial of the characteristic matrix, equals to the characteristic polynomial of the transpose of the characteristic matrix? Since their determinants are equal... Thanks!",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'jordan-normal-form']"
30,Do these matrices exist?,Do these matrices exist?,,"Say you have some non-zero vector in $x$. Can you have two matrices $A$ and $B$ such that: $$Ax = Bx$$ if $A$ and $B$ aren't the identity matrix? This isn't homework, just curiosity.","Say you have some non-zero vector in $x$. Can you have two matrices $A$ and $B$ such that: $$Ax = Bx$$ if $A$ and $B$ aren't the identity matrix? This isn't homework, just curiosity.",,"['linear-algebra', 'matrices', 'functions', 'vector-spaces']"
31,How find this matrix value of this $\det(A_{ij})$,How find this matrix value of this,\det(A_{ij}),"Find this value $$\det(A_{n\times n})=\begin{vmatrix} 0&a_{1}+a_{2}&a_{1}+a_{3}&\cdots&a_{1}+a_{n}\\ a_{2}+a_{1}&0&a_{2}+a_{3}&\cdots&a_{2}+a_{n}\\ a_{3}+a_{1}&a_{3}+a_{2}&0&\cdots&a_{3}+a_{n}\\ \cdots&\cdots&\cdots&\cdots&\cdots\\ a_{n}+a_{1}&a_{n}+a_{2}&a_{n}+a_{3}&\cdots&0 \end{vmatrix}$$ where $a_{j}\neq 0,j=1,2,\cdots,n$ My try: I found this: $$A_{n\times n}=diag(-2a_{1},-2a_{2},\cdots,-2a_{n})+B_{n\times n}$$ where $$B=\begin{bmatrix} a_{1}+a_{1}&a_{1}+a_{2}&\cdots& a_{1}+a_{n}\\ a_{2}+a_{1}&a_{2}+a_{2}&\cdots&a_{2}+a_{n}\\ \cdots&\cdots&\cdots&\cdots\\ a_{n}+a_{1}&a_{n}+a_{2}&\cdots&a_{n}+a_{n} \end{bmatrix}$$ But I can't,Thank you very much","Find this value $$\det(A_{n\times n})=\begin{vmatrix} 0&a_{1}+a_{2}&a_{1}+a_{3}&\cdots&a_{1}+a_{n}\\ a_{2}+a_{1}&0&a_{2}+a_{3}&\cdots&a_{2}+a_{n}\\ a_{3}+a_{1}&a_{3}+a_{2}&0&\cdots&a_{3}+a_{n}\\ \cdots&\cdots&\cdots&\cdots&\cdots\\ a_{n}+a_{1}&a_{n}+a_{2}&a_{n}+a_{3}&\cdots&0 \end{vmatrix}$$ where $a_{j}\neq 0,j=1,2,\cdots,n$ My try: I found this: $$A_{n\times n}=diag(-2a_{1},-2a_{2},\cdots,-2a_{n})+B_{n\times n}$$ where $$B=\begin{bmatrix} a_{1}+a_{1}&a_{1}+a_{2}&\cdots& a_{1}+a_{n}\\ a_{2}+a_{1}&a_{2}+a_{2}&\cdots&a_{2}+a_{n}\\ \cdots&\cdots&\cdots&\cdots\\ a_{n}+a_{1}&a_{n}+a_{2}&\cdots&a_{n}+a_{n} \end{bmatrix}$$ But I can't,Thank you very much",,"['linear-algebra', 'matrices', 'determinant']"
32,Does 1 distinct eigenvalue guarantee 1 eigenvector?,Does 1 distinct eigenvalue guarantee 1 eigenvector?,,"I am trying to figure out when 2x2 matrices are not diagonalizable. Right now, my conditions are: the matrix has only 1 distinct eigenvalue the matrix yields only 1 linearly independent eigenvector But when I know that there are 2 eigenvalues, can I safely assume that each eigenvalue will have at least 1 linearly independent eigenvector?","I am trying to figure out when 2x2 matrices are not diagonalizable. Right now, my conditions are: the matrix has only 1 distinct eigenvalue the matrix yields only 1 linearly independent eigenvector But when I know that there are 2 eigenvalues, can I safely assume that each eigenvalue will have at least 1 linearly independent eigenvector?",,"['linear-algebra', 'matrices']"
33,Why Gaussian method is recommended for $4\times4$ determinant?,Why Gaussian method is recommended for  determinant?,4\times4,"I wanted to know why Gaussian elimination method in Linear Algebra has order of $n^3$ for calculation and found this .But I don't know why this: From the table of $n^3$ vs $n!$ we see that $4^3 > n!$, yet the Gaussian method is the one that is   generally recommended for calculating $4 × 4$ determinants, why? Also, if you can suggest other link for ""why Gaussian elimination method in Linear Algebra has order of $n^3$"", I will be obliged.","I wanted to know why Gaussian elimination method in Linear Algebra has order of $n^3$ for calculation and found this .But I don't know why this: From the table of $n^3$ vs $n!$ we see that $4^3 > n!$, yet the Gaussian method is the one that is   generally recommended for calculating $4 × 4$ determinants, why? Also, if you can suggest other link for ""why Gaussian elimination method in Linear Algebra has order of $n^3$"", I will be obliged.",,"['linear-algebra', 'matrices']"
34,A problem about the matrix equation $a^{\sf T}A_1a=0$.,A problem about the matrix equation .,a^{\sf T}A_1a=0,"Let $A_1$ be an $m\times m$ real constant matrix in $\mathbb{R}^{m\times m}$. The matrix $A_1$ satisfies $a^{\sf T}A_1a=0$ for all $a\in\mathbb{R}^m$, where $a^{\sf T}$ is the transpose of $a$. Show that $$A_1=-A_1^{\sf T}.$$","Let $A_1$ be an $m\times m$ real constant matrix in $\mathbb{R}^{m\times m}$. The matrix $A_1$ satisfies $a^{\sf T}A_1a=0$ for all $a\in\mathbb{R}^m$, where $a^{\sf T}$ is the transpose of $a$. Show that $$A_1=-A_1^{\sf T}.$$",,"['linear-algebra', 'matrices', 'matrix-equations']"
35,A basic question on diagonalizability of a matrix,A basic question on diagonalizability of a matrix,,"I am following a book where the ""diagonalizability"" has been introduced as follows: Consider a basis formed by a linearly independent set of eigen vectors $\{v_1,v_2,\dots,v_n\}$ . Then it is claimed that with respect to this   basis, the matrix $A$ is diagonal. I am confused at the word ""basis"" here. In some other books it is said that a  matrix $A$ is called diagonalizable if there exists matrix $P$ such that $P^{-1}AP$ is a diagonal matrix. I don't think that diagonalizability has anything to do with basis. It just happened that the set of eigen vectors form a basis of $\Bbb R^n$ . Also I don't think having a set of $n$ linearly independent eigen vectors is a necessary condition for a matrix to be diagonalizable.","I am following a book where the ""diagonalizability"" has been introduced as follows: Consider a basis formed by a linearly independent set of eigen vectors . Then it is claimed that with respect to this   basis, the matrix is diagonal. I am confused at the word ""basis"" here. In some other books it is said that a  matrix is called diagonalizable if there exists matrix such that is a diagonal matrix. I don't think that diagonalizability has anything to do with basis. It just happened that the set of eigen vectors form a basis of . Also I don't think having a set of linearly independent eigen vectors is a necessary condition for a matrix to be diagonalizable.","\{v_1,v_2,\dots,v_n\} A A P P^{-1}AP \Bbb R^n n",['linear-algebra']
36,Eigenvalue of linear operator iff eigenvalue of matrix representation.,Eigenvalue of linear operator iff eigenvalue of matrix representation.,,"I'm trying to prove the following theorem, which seems straightforward enough, but I'm confused about the wording and proving the converse: Let T be a linear operator on a finite-dimensional vector space V, and let B be an ordered basis for V.  Prove that t is an eigenvalue of T iff t is an eigenvalue of the matrix representation of $T_B$.  (Sorry; I don't know how to make the subscript of B with [T].) Here's my confusion:  don't we have to have that V is a vector space consisting of column vectors in order for us to even speak of eigenvalues and eigenvectors of matrices?  This isn't specified in the theorem. Moreover, even if this were specified, I'm not sure how to get from the matrix multiplication form back to the linear operator form with my proof.  Any suggestions?","I'm trying to prove the following theorem, which seems straightforward enough, but I'm confused about the wording and proving the converse: Let T be a linear operator on a finite-dimensional vector space V, and let B be an ordered basis for V.  Prove that t is an eigenvalue of T iff t is an eigenvalue of the matrix representation of $T_B$.  (Sorry; I don't know how to make the subscript of B with [T].) Here's my confusion:  don't we have to have that V is a vector space consisting of column vectors in order for us to even speak of eigenvalues and eigenvectors of matrices?  This isn't specified in the theorem. Moreover, even if this were specified, I'm not sure how to get from the matrix multiplication form back to the linear operator form with my proof.  Any suggestions?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'linear-transformations']"
37,Finding a basis for a subspace in $\Bbb R^{2\times 2}$,Finding a basis for a subspace in,\Bbb R^{2\times 2},"Find a basis for the space and determine its dimension. The space of all matrices $$A=\begin{pmatrix}a& b\\c& d\end{pmatrix}$$ in $\Bbb R^{2\times 2}$ such that $a + d = 0$ (in other words, the [sum of the] diagonal = 0). I'm not sure how to even begin this question.  Please help!","Find a basis for the space and determine its dimension. The space of all matrices $$A=\begin{pmatrix}a& b\\c& d\end{pmatrix}$$ in $\Bbb R^{2\times 2}$ such that $a + d = 0$ (in other words, the [sum of the] diagonal = 0). I'm not sure how to even begin this question.  Please help!",,"['linear-algebra', 'matrices']"
38,Schur decomposition,Schur decomposition,,"If $A$ is real and nonsymmetric with Schur decomposition $UTU^H$, then what types of matrices are $U$ and $T$? How are the eigenvalues of $A$ related to $U$ and $T$?","If $A$ is real and nonsymmetric with Schur decomposition $UTU^H$, then what types of matrices are $U$ and $T$? How are the eigenvalues of $A$ related to $U$ and $T$?",,"['linear-algebra', 'matrices', 'matrix-decomposition', 'schur-decomposition']"
39,calculating the determinant of an $n \times n$ integer matrix,calculating the determinant of an  integer matrix,n \times n,"I want to write a polynomial algorithm for calculating the determinant of an $n \times n$ integer matrix. There are various codes in different programming languages on the web but unfortunately I am not good at understanding codes which are written by other people. Normally, what steps will it take to calculate the determinant of an $n \times n$ integer matrix, then I can write the algorithm and compute its time complexity on my own.","I want to write a polynomial algorithm for calculating the determinant of an $n \times n$ integer matrix. There are various codes in different programming languages on the web but unfortunately I am not good at understanding codes which are written by other people. Normally, what steps will it take to calculate the determinant of an $n \times n$ integer matrix, then I can write the algorithm and compute its time complexity on my own.",,"['matrices', 'computational-complexity']"
40,Matrix Norm Inequality $\lVert A\rVert_\infty \leq \sqrt{n} \lVert A\rVert_2$,Matrix Norm Inequality,\lVert A\rVert_\infty \leq \sqrt{n} \lVert A\rVert_2,"So I'm trying to prove that $\lVert A\rVert_\infty \leq \sqrt{n} \lVert A\rVert_2$. I've written the right hand side in terms of rows, but this method doesn't seem to be getting me anywhere. Where else should I go?","So I'm trying to prove that $\lVert A\rVert_\infty \leq \sqrt{n} \lVert A\rVert_2$. I've written the right hand side in terms of rows, but this method doesn't seem to be getting me anywhere. Where else should I go?",,"['linear-algebra', 'matrices', 'inequality', 'normed-spaces']"
41,A diagonal matrix can be expressed as a difference of two squares ($A=B^2 − C^2$ where $BC = CB = 0$),A diagonal matrix can be expressed as a difference of two squares ( where ),A=B^2 − C^2 BC = CB = 0,"I got one interesting question from matrix theory. I tried but I am not finding any clue to solve this question.  I need help and suggestions. Thanks in advance. Let $A$ be a real $n\times n$ matrix. We say that $A$ is a difference of two squares if there exist real $n\times n$ matrices $B$ and $C$ with $BC = CB = 0$ and $A = B^2 − C^2$ . Now If $A$ is a diagonal matrix, then I have to show that that it is a difference of two squares.","I got one interesting question from matrix theory. I tried but I am not finding any clue to solve this question.  I need help and suggestions. Thanks in advance. Let be a real matrix. We say that is a difference of two squares if there exist real matrices and with and . Now If is a diagonal matrix, then I have to show that that it is a difference of two squares.",A n\times n A n\times n B C BC = CB = 0 A = B^2 − C^2 A,"['linear-algebra', 'matrices']"
42,Square matrix multiplication when raised to a power,Square matrix multiplication when raised to a power,,"Is there any way to evaluate an expression that involves a matrix to power $n$, multiplied with some constant $D$:$$D\begin{pmatrix} a & b \\ c & d \end{pmatrix}^{n} $$ I can obviously do it if this power wasn't there but with it in place I am somewhat stuck, are there any helpful linear algebra rules that can be used? I know the eigen values and vectors for this matrix as well if that helps","Is there any way to evaluate an expression that involves a matrix to power $n$, multiplied with some constant $D$:$$D\begin{pmatrix} a & b \\ c & d \end{pmatrix}^{n} $$ I can obviously do it if this power wasn't there but with it in place I am somewhat stuck, are there any helpful linear algebra rules that can be used? I know the eigen values and vectors for this matrix as well if that helps",,[]
43,Diagonalizable Matrix $A^2$,Diagonalizable Matrix,A^2,"How can I find a matrix $A$ such that $A^2$ is diagonalizable but $A$ is not? I have tried many different ways, but to no avail. Is there something that I am missing in the question that gives a simple answer?","How can I find a matrix $A$ such that $A^2$ is diagonalizable but $A$ is not? I have tried many different ways, but to no avail. Is there something that I am missing in the question that gives a simple answer?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
44,Real symmmetric positive definite matices have all diagonal entries positive?,Real symmmetric positive definite matices have all diagonal entries positive?,,"If a real symmmetric matrix $ A $ is positive definite. Is it true that all diagonal entries are positive? I have only prove it for a matrix of order 2. How can I prove or disprove it, please give me only hints.","If a real symmmetric matrix $ A $ is positive definite. Is it true that all diagonal entries are positive? I have only prove it for a matrix of order 2. How can I prove or disprove it, please give me only hints.",,"['linear-algebra', 'matrices']"
45,How to solve for matrix D in $ABC^{T}DBA^{T}C=AB^{T}$?,How to solve for matrix D in ?,ABC^{T}DBA^{T}C=AB^{T},"So the problem I'm having is trying to solve for matrix $D$ in the following equation, assuming the matrices are all $n\times n$ size and invertible.  $$ D ~~\text{in}~~ ABC^{T}DBA^{T}C=AB^{T} $$","So the problem I'm having is trying to solve for matrix $D$ in the following equation, assuming the matrices are all $n\times n$ size and invertible.  $$ D ~~\text{in}~~ ABC^{T}DBA^{T}C=AB^{T} $$",,"['linear-algebra', 'matrices']"
46,What $h$ and $k$ would make this system have infinitely many solutions?,What  and  would make this system have infinitely many solutions?,h k,If there are an infinite number of solutions to the system $$\left|\begin{array}{cc|c} -5 & 6 & h\\ -8 & k & -7\end{array}\right|$$ then what do $h$ and $k$ equal? I know that for a system to have infinity many solutions then both of the rows must be equal.,If there are an infinite number of solutions to the system $$\left|\begin{array}{cc|c} -5 & 6 & h\\ -8 & k & -7\end{array}\right|$$ then what do $h$ and $k$ equal? I know that for a system to have infinity many solutions then both of the rows must be equal.,,['matrices']
47,Require brilliant resources to self teach.,Require brilliant resources to self teach.,,"I'm far from the level of mathematical knowledge every user on this website posseses, however I am very much determined to get there as my love for mathematics increases. These are the topics: http://tinyurl.com/brudel9 Thanks, Im not from US so I don't know what grade level this stuff is. However it is a further maths course here are resources for these topics are either scarce or of poor quality. I would like to improve my knowledge of mathematics, and not just learn to pass an exam. Simarly, is there a solid text I can use to self teach statistics? I have to cover basic topics like deviation/variance all the way to harder probability. Thank you very much!","I'm far from the level of mathematical knowledge every user on this website posseses, however I am very much determined to get there as my love for mathematics increases. These are the topics: http://tinyurl.com/brudel9 Thanks, Im not from US so I don't know what grade level this stuff is. However it is a further maths course here are resources for these topics are either scarce or of poor quality. I would like to improve my knowledge of mathematics, and not just learn to pass an exam. Simarly, is there a solid text I can use to self teach statistics? I have to cover basic topics like deviation/variance all the way to harder probability. Thank you very much!",,"['matrices', 'reference-request', 'statistics', 'complex-numbers', 'self-learning']"
48,Group does not contain any elements of order $p^2$?,Group does not contain any elements of order ?,p^2,"I am trying to show that the group of $3 \times 3 $ upper triangular matrices over the field $ \mathbb{F}_p $ with diagonal entries 1 does not contain any elements of order $p^2$ when $ p \geq 3$. I've tried to argue by contradiction: suppose it had an element $g$ of order $p^2$, then the subgroup generated by $g$ has index $p$ so it is normal, and from there I would like to find an element $h$ of order $p$ whose cyclic subgroup has trivial intersection with $ \langle g \rangle$. Then $G$ is the semi-direct product of $\langle g \rangle$ and $ \langle h \rangle$, and I am hoping this will give me a contradiction by telling me that $G$ is abelian or something similar. Any help is appreciated!","I am trying to show that the group of $3 \times 3 $ upper triangular matrices over the field $ \mathbb{F}_p $ with diagonal entries 1 does not contain any elements of order $p^2$ when $ p \geq 3$. I've tried to argue by contradiction: suppose it had an element $g$ of order $p^2$, then the subgroup generated by $g$ has index $p$ so it is normal, and from there I would like to find an element $h$ of order $p$ whose cyclic subgroup has trivial intersection with $ \langle g \rangle$. Then $G$ is the semi-direct product of $\langle g \rangle$ and $ \langle h \rangle$, and I am hoping this will give me a contradiction by telling me that $G$ is abelian or something similar. Any help is appreciated!",,"['abstract-algebra', 'matrices', 'finite-groups', 'finite-fields']"
49,Family of functions from Matrices to Reals,Family of functions from Matrices to Reals,,I was given this question in one of my interviews. I have tried a lot even after that but I am not able to get a hold of it. Can anybody help me with this. Function $f$ has the following properties $f:$ Square Matrix -> Real $f(I_n) = 1$ // Identity Matrix of size n maps to one : independent of n If $A$ and $B$ both are square matrices of same size then $f(AB) = f(BA)$ If $A$ and $B$ both are square matrices of same size then $f(mA+nB) = mf(A) + nf(B)$ Describe the family of functions $f$ that satisfies the above criteria. The only property I was able to obtain was that $f(-A) = -f(A)$ Thanks,I was given this question in one of my interviews. I have tried a lot even after that but I am not able to get a hold of it. Can anybody help me with this. Function $f$ has the following properties $f:$ Square Matrix -> Real $f(I_n) = 1$ // Identity Matrix of size n maps to one : independent of n If $A$ and $B$ both are square matrices of same size then $f(AB) = f(BA)$ If $A$ and $B$ both are square matrices of same size then $f(mA+nB) = mf(A) + nf(B)$ Describe the family of functions $f$ that satisfies the above criteria. The only property I was able to obtain was that $f(-A) = -f(A)$ Thanks,,"['matrices', 'functions']"
50,Understanding minimum polynomials and characteristic polynomials,Understanding minimum polynomials and characteristic polynomials,,"Here are two questions that I currently have about the minimum polynomial and characteristic polynomial of a linear endomorphism on a finite dimensional space: If the characteristic polynomial $c = p_1^{d_1} p_2^{d_2} \dotsm p_k^{d_k}$, where each $p_i$ is irreducible and $d_i \neq 0$, we know the minimum polynomial $m = p_1^{e_1} p_2^{e_2} \dotsm p_k^{e_k}$. Why can it not be the case that some $e_i = 0$? A proof of the primary decomposition theorem starts like this: For $k > 1$, let $q_i = m/p_i^{e_i}$. Then the $q_i$'s are coprime, so there are polynomials $a_i$ such that $a_1 q_1 + \dotsb + a_k q_k = 1$. Why it is wlog that all $a_i \neq 0$? Thank you for your help! Edit: Is there a simple explanation without extending the ground field?","Here are two questions that I currently have about the minimum polynomial and characteristic polynomial of a linear endomorphism on a finite dimensional space: If the characteristic polynomial $c = p_1^{d_1} p_2^{d_2} \dotsm p_k^{d_k}$, where each $p_i$ is irreducible and $d_i \neq 0$, we know the minimum polynomial $m = p_1^{e_1} p_2^{e_2} \dotsm p_k^{e_k}$. Why can it not be the case that some $e_i = 0$? A proof of the primary decomposition theorem starts like this: For $k > 1$, let $q_i = m/p_i^{e_i}$. Then the $q_i$'s are coprime, so there are polynomials $a_i$ such that $a_1 q_1 + \dotsb + a_k q_k = 1$. Why it is wlog that all $a_i \neq 0$? Thank you for your help! Edit: Is there a simple explanation without extending the ground field?",,"['linear-algebra', 'matrices', 'minimal-polynomials', 'characteristic-polynomial']"
51,Problem with powers of matrices.,Problem with powers of matrices.,,"Suppose that the matrix $A$ is as follows: $$A=\begin{bmatrix}     -3&4\\      2&3\end{bmatrix}$$ We need to prove that $A^{2n + 1}=A$. The way I tackled this problem is as follows: If $A^{2n + 1} =A$,  then $A^{2n}$ must be the same as the identity matrix $I$. Thus $(A^2)^n$ must be the same as $I$. By calculation $A^2=17I$. Thus the statement can't be proved. I'm not sure if I'm correct. I believe we have to make use of Cayley Hamilton's Theorem and diagonalization to solve the problem but I can't seem to wrap my head around it. Any help will be appreciated.","Suppose that the matrix $A$ is as follows: $$A=\begin{bmatrix}     -3&4\\      2&3\end{bmatrix}$$ We need to prove that $A^{2n + 1}=A$. The way I tackled this problem is as follows: If $A^{2n + 1} =A$,  then $A^{2n}$ must be the same as the identity matrix $I$. Thus $(A^2)^n$ must be the same as $I$. By calculation $A^2=17I$. Thus the statement can't be proved. I'm not sure if I'm correct. I believe we have to make use of Cayley Hamilton's Theorem and diagonalization to solve the problem but I can't seem to wrap my head around it. Any help will be appreciated.",,['matrices']
52,"For which $\mathbf X$ (matrix) does there exist a scalar $c$ such that $\mathbf{AX} = c\mathbf{X}$? ($\mathbf A$ is a matrix, in the message)","For which  (matrix) does there exist a scalar  such that ? ( is a matrix, in the message)",\mathbf X c \mathbf{AX} = c\mathbf{X} \mathbf A,"For which $\mathbf X$ (matrix) does there exist a scalar $c$ such that $\mathbf{AX} = c\mathbf{X}$? $$ \mathbf{A} = \begin{bmatrix} 5 & 0 & 0\\ 1 & 5 & 0\\ 0 & 1 & 5 \end{bmatrix} $$ I have thought about the idea that $\mathbf{X}$ is $0$ so $c$ is $0$, but is it the only solution? Thank you!","For which $\mathbf X$ (matrix) does there exist a scalar $c$ such that $\mathbf{AX} = c\mathbf{X}$? $$ \mathbf{A} = \begin{bmatrix} 5 & 0 & 0\\ 1 & 5 & 0\\ 0 & 1 & 5 \end{bmatrix} $$ I have thought about the idea that $\mathbf{X}$ is $0$ so $c$ is $0$, but is it the only solution? Thank you!",,"['linear-algebra', 'matrices']"
53,Conjugacy classes in a matrix group,Conjugacy classes in a matrix group,,Consider the matrix group $PGL_{2}(\mathbb{F}_{q})$ for $q$ odd. Why is it that $\begin{pmatrix} -1 & 0\\ 0 & 1\end{pmatrix}$ has $q(q + 1)/2$ elements in its conjugacy class while $\begin{pmatrix} 2 & 0\\ 0 & 1\end{pmatrix}$ has $q(q + 1)$ elements?,Consider the matrix group $PGL_{2}(\mathbb{F}_{q})$ for $q$ odd. Why is it that $\begin{pmatrix} -1 & 0\\ 0 & 1\end{pmatrix}$ has $q(q + 1)/2$ elements in its conjugacy class while $\begin{pmatrix} 2 & 0\\ 0 & 1\end{pmatrix}$ has $q(q + 1)$ elements?,,"['linear-algebra', 'abstract-algebra', 'group-theory', 'matrices']"
54,Linear Algebra: Find a matrix A such that T(x) is Ax for each x,Linear Algebra: Find a matrix A such that T(x) is Ax for each x,,"I am having difficulty solve this problem in my homework: (In my notation, $[x;y]$ represents a matrix of 2 rows, 1 column) Let $\mathbf{x}=[x_1;x_2]$, $v_1$=[−3;5] and $v_2=[7;−2]$ and let $T\colon\mathbb{R}^2\to\mathbb{R}^2$  be a linear transformation that maps $\mathbf{x}$ into $x_1v_1+x_2v_2$. Find a matrix $A$ such that $T(\mathbf{x})$ is $A\mathbf{x}$ for each $\mathbf{x}$. I am pretty clueless. So I assume that I start off with the following: $x_1v_1 + x_2v_2 = x_1[−3;5] + x_2[7;−2]$ But I do not know what to do from here, or if this is even the correct start!","I am having difficulty solve this problem in my homework: (In my notation, $[x;y]$ represents a matrix of 2 rows, 1 column) Let $\mathbf{x}=[x_1;x_2]$, $v_1$=[−3;5] and $v_2=[7;−2]$ and let $T\colon\mathbb{R}^2\to\mathbb{R}^2$  be a linear transformation that maps $\mathbf{x}$ into $x_1v_1+x_2v_2$. Find a matrix $A$ such that $T(\mathbf{x})$ is $A\mathbf{x}$ for each $\mathbf{x}$. I am pretty clueless. So I assume that I start off with the following: $x_1v_1 + x_2v_2 = x_1[−3;5] + x_2[7;−2]$ But I do not know what to do from here, or if this is even the correct start!",,"['linear-algebra', 'matrices']"
55,Determinant of a series of Hadamard matrix,Determinant of a series of Hadamard matrix,,"Given: $$H=\ \left[ \begin{array}{cc|r} 1 & 1  \\ 1 & -1  \end{array} \right]$$ a Hadamard $H_2$ matrix. and the series: $$S=\sum_{k=0}^{N}{\frac{H^k}{k!}}$$ Is it possible to calculate the quantity $D=\det(S)$  as a function of N? Thanks","Given: $$H=\ \left[ \begin{array}{cc|r} 1 & 1  \\ 1 & -1  \end{array} \right]$$ a Hadamard $H_2$ matrix. and the series: $$S=\sum_{k=0}^{N}{\frac{H^k}{k!}}$$ Is it possible to calculate the quantity $D=\det(S)$  as a function of N? Thanks",,"['matrices', 'power-series']"
56,Recurrence for a lagged Fibonacci sequence,Recurrence for a lagged Fibonacci sequence,,"I know how to do the matrix for the standard $f(n) = f(n-1) + f(n-2)$ relationship, but what if it's piecewise? For instance, $f(1)$ through $f(30)$ have some preset values, and then for $f(31)$ onward, the equation is $f(n) = f(n-15) + f(n-30)$ or something? I have absolutely no idea how to set this up in a matrix recurrence $MX$ although I think $X$ would just contain the preset values but then I'm not sure how to fill in $M$. Could really use a hand here — thanks!","I know how to do the matrix for the standard $f(n) = f(n-1) + f(n-2)$ relationship, but what if it's piecewise? For instance, $f(1)$ through $f(30)$ have some preset values, and then for $f(31)$ onward, the equation is $f(n) = f(n-15) + f(n-30)$ or something? I have absolutely no idea how to set this up in a matrix recurrence $MX$ although I think $X$ would just contain the preset values but then I'm not sure how to fill in $M$. Could really use a hand here — thanks!",,"['sequences-and-series', 'matrices', 'recurrence-relations', 'fibonacci-numbers']"
57,Non degenerate bilinear form,Non degenerate bilinear form,,"Problem: Let $F:V \times W \to \mathbb{R}$ be a non degenerate bilinear form. The question is: prove that $V$ and $W$ have the same dimension (the vector spaces $V$ and $W$ are finite dimensional) My answer is: $F$ is non degenerate, then the matrix of $F$ is invertible, which means it's square and this implies that $V$ and $W$ have the same dimension. Is my assumption that the matrix of a non degenerate bilinear form is invertible true? Also let me know if my answer is true?","Problem: Let $F:V \times W \to \mathbb{R}$ be a non degenerate bilinear form. The question is: prove that $V$ and $W$ have the same dimension (the vector spaces $V$ and $W$ are finite dimensional) My answer is: $F$ is non degenerate, then the matrix of $F$ is invertible, which means it's square and this implies that $V$ and $W$ have the same dimension. Is my assumption that the matrix of a non degenerate bilinear form is invertible true? Also let me know if my answer is true?",,"['linear-algebra', 'matrices', 'bilinear-form']"
58,Diagonalizing an Integer Matrix,Diagonalizing an Integer Matrix,,"This is, admittedly, not that interesting a question, but it's a small piece of a number theory problem I'm working on, and it's been rather frustrating. As it is technically ""homework,"" feel free to just give suggestions, unless of course it's something really obvious. I want to prove that for a general integer matrix, $A\in M_n(\mathbb{Z})$, there exist $U,V\in GL_n(\mathbb{Z})$ such that  $UAV = \left[ \begin{array}{rrrrrr}  d_1  &  &  & & & 0\\\  & \ddots  &  & & & \\\  &  &  d_r & & &\\\  &  &  & 0& &\\\  &  &  &  & \ddots &\\\ 0  &  &  & & & 0 \end{array} \right]$, where $r=\mathrm{rank}A$, $d_i\in\mathbb{N}$ and $d_i\vert d_{i+1}$. I feel like this should be just a linear algebra thing.  I tried to just break it down into elements for just a 2 by 2, and it got so messy, so I'm thinking that's not the way to do it, and I'm wondering if maybe it's just a well known theorem (the issue here of course being that everything is integers, so I can't really apply stuff about diagonalizing matrices over a field).  Any assistance on this would be dearly appreciated. Thanks!","This is, admittedly, not that interesting a question, but it's a small piece of a number theory problem I'm working on, and it's been rather frustrating. As it is technically ""homework,"" feel free to just give suggestions, unless of course it's something really obvious. I want to prove that for a general integer matrix, $A\in M_n(\mathbb{Z})$, there exist $U,V\in GL_n(\mathbb{Z})$ such that  $UAV = \left[ \begin{array}{rrrrrr}  d_1  &  &  & & & 0\\\  & \ddots  &  & & & \\\  &  &  d_r & & &\\\  &  &  & 0& &\\\  &  &  &  & \ddots &\\\ 0  &  &  & & & 0 \end{array} \right]$, where $r=\mathrm{rank}A$, $d_i\in\mathbb{N}$ and $d_i\vert d_{i+1}$. I feel like this should be just a linear algebra thing.  I tried to just break it down into elements for just a 2 by 2, and it got so messy, so I'm thinking that's not the way to do it, and I'm wondering if maybe it's just a well known theorem (the issue here of course being that everything is integers, so I can't really apply stuff about diagonalizing matrices over a field).  Any assistance on this would be dearly appreciated. Thanks!",,"['linear-algebra', 'matrices']"
59,Suggest a tricky procedure,Suggest a tricky procedure,,"If    $$A = \frac{1}{2} \times \begin{pmatrix}  -1& -\sqrt3  & 0 \\   -\sqrt3& 1 &0 \\   0 & 0 & 0 \end{pmatrix}  \text{ and } E = \frac{1}{2} \times \begin{pmatrix}  1& \sqrt3  & 0 \\   -\sqrt3& 1 &0 \\   0 & 0 & 1 \end{pmatrix}$$   then show that $E^{-1}AE$ is diagonal matrix. Can we prove this without actually computing the the inverses and the product? The actual problem is consist of four options of matrices and I think it's a bit tedious for objective type to do the actual computation, so I am more interested in using some property for the same. I know about $E^{T}AE$ depend on whether A is symmetric or skew-symmetric but is there anything like this for inverses?","If    $$A = \frac{1}{2} \times \begin{pmatrix}  -1& -\sqrt3  & 0 \\   -\sqrt3& 1 &0 \\   0 & 0 & 0 \end{pmatrix}  \text{ and } E = \frac{1}{2} \times \begin{pmatrix}  1& \sqrt3  & 0 \\   -\sqrt3& 1 &0 \\   0 & 0 & 1 \end{pmatrix}$$   then show that $E^{-1}AE$ is diagonal matrix. Can we prove this without actually computing the the inverses and the product? The actual problem is consist of four options of matrices and I think it's a bit tedious for objective type to do the actual computation, so I am more interested in using some property for the same. I know about $E^{T}AE$ depend on whether A is symmetric or skew-symmetric but is there anything like this for inverses?",,"['linear-algebra', 'matrices']"
60,"Any ""tricks"" for computing matrix commutators?","Any ""tricks"" for computing matrix commutators?",,"I happen to need to find the commutator of various 2x2 and 3x3 matrices relatively often. It is particularily tedious, but even after much practice, I am not finding it getting significantly faster. Are you aware of any ""tricks"" or shortcuts I can take when computing matrix commutators (or recognising that two matrices commute by looking at them)? EDIT thanks to comment, most of the matrices I deal with are hermititan ( $H_{ij} = H_{ji}^*$ ) Certainly two diagonal matrices will always commute, but I am not aware of many other properties.","I happen to need to find the commutator of various 2x2 and 3x3 matrices relatively often. It is particularily tedious, but even after much practice, I am not finding it getting significantly faster. Are you aware of any ""tricks"" or shortcuts I can take when computing matrix commutators (or recognising that two matrices commute by looking at them)? EDIT thanks to comment, most of the matrices I deal with are hermititan ( ) Certainly two diagonal matrices will always commute, but I am not aware of many other properties.",H_{ij} = H_{ji}^*,"['linear-algebra', 'matrices', 'quantum-mechanics']"
61,How can I prove that this matrix is idempotent?,How can I prove that this matrix is idempotent?,,"I have the following matrix $$A=\begin{equation} \begin{pmatrix} 0 & a & -b\\ -a & 0 & c\\ b & -c & 0 \end{pmatrix} \end{equation}$$ I have to prove that $M=A^2+I$ is idempotent knowing that $a^2+b^2+c^2=1$ . I can calculate M  using brute force as $$M=  \left( \begin{array}{ccc} 0 & a & -b \\ -a & 0 & c \\ b &-c & 0 \end{array} \right) % \left( \begin{array}{ccc} 0 & a & -b \\ -a & 0 & c \\ b &-c & 0 \end{array} \right) + % \left( \begin{array}{ccc} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{array} \right) $$ I obtain $$ M=\left( \begin{array}{ccc} c^2 & bc & ac \\ bc & b^2 & ab \\ ac & ab & a^2 \end{array} \right)$$ I have tried to solve this by brute force, but I cannot prove that $M=M^2$ , that is what I need to show that $M$ is idempotent since my result by brute force contains too many terms to simplify. Can someone explain to me how to do it?","I have the following matrix I have to prove that is idempotent knowing that . I can calculate M  using brute force as I obtain I have tried to solve this by brute force, but I cannot prove that , that is what I need to show that is idempotent since my result by brute force contains too many terms to simplify. Can someone explain to me how to do it?","A=\begin{equation}
\begin{pmatrix}
0 & a & -b\\
-a & 0 & c\\
b & -c & 0
\end{pmatrix}
\end{equation} M=A^2+I a^2+b^2+c^2=1 M=
 \left( \begin{array}{ccc}
0 & a & -b \\
-a & 0 & c \\
b &-c & 0
\end{array} \right)
%
\left( \begin{array}{ccc}
0 & a & -b \\
-a & 0 & c \\
b &-c & 0
\end{array} \right)
+
%
\left( \begin{array}{ccc}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{array} \right)
  M=\left( \begin{array}{ccc}
c^2 & bc & ac \\
bc & b^2 & ab \\
ac & ab & a^2
\end{array} \right) M=M^2 M","['matrices', 'proof-writing', 'products', 'idempotents']"
62,Prove that the generalized eigenvalue equation in normal mode analysis has positive eigenvalues,Prove that the generalized eigenvalue equation in normal mode analysis has positive eigenvalues,,"Context I am studying normal modes oscillations and normal modes [1,2]. I am trying to complete a proof to show that the eigenvalues (i.e., the eigenvalues are positive). There is a proof in [2], but I find it verbose and likely incorrect. There is a short proof of this in [4]. Nonetheless, I proceed with my questions on the topic. Questions Question 1 In my understanding an eigenvalue equation [3], I write the following definition. Definition If $T$ is a linear transformation from a finite-dimensional vector space $\mathbb{C}^n$ over the field of complex number into itself and $\mathbf{v}$ is a  nonzero  vector in $\mathbb{C}^n$ , then $\mathbf{v}$ is an eigenvector of $T$ if $T\,\mathbf{v}$ is a scalar multiple of $\mathbf{v}$ . This can be written as the eigenvalue equation $$T(\mathbf{v}) = \lambda \mathbf{v},$$ where $\lambda$ is a scalar in $\mathbb{C}$ , known as the eigenvalue associated with $\mathbf{v}$ . However, in the analysis of normal mode of dynamical systems, I have $n$ -dimensional real symmetric matrices $\mathbf{V}$ and $\mathbf{T} $ , and the "" eigenvalue equation "" $$\mathbf{V}\,\mathbf{a} = \lambda\,\mathbf{T}\,\mathbf{a}.\tag{1}$$ In my mind, if $T$ is invertible, I can make an actual eigenvalue equation $$\left( \mathbf{V}\,\mathbf{T}^{-1}\right)\left(\mathbf{T}\,\mathbf{a}\right) = \lambda\,\left(\mathbf{T}\,\mathbf{a}\right).\tag{2}$$ Now, I would have that $\mathbf{T}\mathbf{a}$ is an eigenvector of $\mathbf{V}\mathbf{T}^{-1}$ since $\mathbf{V} \mathbf{a}$ is a scalar multiple of $\mathbf{T}\mathbf{a}$ . So my first series of questions: (1) Can we agree that, as written, Equation 1 is not actually an eigenvalue equation? (2) Does this type of equation have a name? If so what is it? Question 2 In [2] there is a claim that $\lambda$ is always finite and positive. It comes down to faith. I really like [2], but when it comes to such matters as the proofs of mathematical claim, I just have no faith in [2]. For example, there is even an example later in the book when one of the eigenvalues is zero valued. (3) How can we prove or disprove the following propositions? [Please note that both propositions cannot be proven to be true.  An amended proposition is given and proven in OP's answer below.] Proposition. Given two $n$ dimensional real symmetric matrices $\mathbf{T}$ and $\mathbf{V}$ show that any solution of the equation $$\mathbf{V}\,\mathbf{a} = \lambda\,\mathbf{T}\,\mathbf{a}$$ has a scalar lambda that is positive. Proposition. Given two $n$ dimensional real symmetric matrices $\mathbf{T}$ and $\mathbf{V}$ show that any solution of the equation $$\mathbf{V}\,\mathbf{a} = \lambda\,\mathbf{T}\,\mathbf{a}$$ has a scalar lambda that is non-negative. These two propositions are closely related. Thus, I assume that some other predicates need to be set in order to distinguish the proofs. Bibliography [1] https://en.wikipedia.org/wiki/Normal_mode [2] Goldstein, ""Classical Mechanics,"" 3rd edition, page 242-3. [3] https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors#Formal_definition [4] Prove that the eigenvalues for $A\vec{w} = \lambda B\vec{w}$ are real and positive","Context I am studying normal modes oscillations and normal modes [1,2]. I am trying to complete a proof to show that the eigenvalues (i.e., the eigenvalues are positive). There is a proof in [2], but I find it verbose and likely incorrect. There is a short proof of this in [4]. Nonetheless, I proceed with my questions on the topic. Questions Question 1 In my understanding an eigenvalue equation [3], I write the following definition. Definition If is a linear transformation from a finite-dimensional vector space over the field of complex number into itself and is a  nonzero  vector in , then is an eigenvector of if is a scalar multiple of . This can be written as the eigenvalue equation where is a scalar in , known as the eigenvalue associated with . However, in the analysis of normal mode of dynamical systems, I have -dimensional real symmetric matrices and , and the "" eigenvalue equation "" In my mind, if is invertible, I can make an actual eigenvalue equation Now, I would have that is an eigenvector of since is a scalar multiple of . So my first series of questions: (1) Can we agree that, as written, Equation 1 is not actually an eigenvalue equation? (2) Does this type of equation have a name? If so what is it? Question 2 In [2] there is a claim that is always finite and positive. It comes down to faith. I really like [2], but when it comes to such matters as the proofs of mathematical claim, I just have no faith in [2]. For example, there is even an example later in the book when one of the eigenvalues is zero valued. (3) How can we prove or disprove the following propositions? [Please note that both propositions cannot be proven to be true.  An amended proposition is given and proven in OP's answer below.] Proposition. Given two dimensional real symmetric matrices and show that any solution of the equation has a scalar lambda that is positive. Proposition. Given two dimensional real symmetric matrices and show that any solution of the equation has a scalar lambda that is non-negative. These two propositions are closely related. Thus, I assume that some other predicates need to be set in order to distinguish the proofs. Bibliography [1] https://en.wikipedia.org/wiki/Normal_mode [2] Goldstein, ""Classical Mechanics,"" 3rd edition, page 242-3. [3] https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors#Formal_definition [4] Prove that the eigenvalues for $A\vec{w} = \lambda B\vec{w}$ are real and positive","T \mathbb{C}^n \mathbf{v} \mathbb{C}^n \mathbf{v} T T\,\mathbf{v} \mathbf{v} T(\mathbf{v}) = \lambda \mathbf{v}, \lambda \mathbb{C} \mathbf{v} n \mathbf{V} \mathbf{T}  \mathbf{V}\,\mathbf{a} = \lambda\,\mathbf{T}\,\mathbf{a}.\tag{1} T \left( \mathbf{V}\,\mathbf{T}^{-1}\right)\left(\mathbf{T}\,\mathbf{a}\right) = \lambda\,\left(\mathbf{T}\,\mathbf{a}\right).\tag{2} \mathbf{T}\mathbf{a} \mathbf{V}\mathbf{T}^{-1} \mathbf{V} \mathbf{a} \mathbf{T}\mathbf{a} \lambda n \mathbf{T} \mathbf{V} \mathbf{V}\,\mathbf{a} = \lambda\,\mathbf{T}\,\mathbf{a} n \mathbf{T} \mathbf{V} \mathbf{V}\,\mathbf{a} = \lambda\,\mathbf{T}\,\mathbf{a}","['linear-algebra', 'matrices', 'physics', 'positive-definite']"
63,Troubles with a degenerate conics,Troubles with a degenerate conics,,"I'm studying the following conics with respect upon a parameter $t$ (real). $$2tx^2 + 2txy + 4y + 1 = 0$$ For those kinds of problems I have always followed this Wikipedia page, which I find the most clear among all the notes and websistes I have been searching though, https://en.wikipedia.org/wiki/Matrix_representation_of_conic_sections Yet There is something I cannot catch. So the matrix invariants in this case are $$\text{det} \begin{pmatrix} 2t & t & 0 \\ t & 0 & 2 \\ 0 & 2 & 1 \end{pmatrix} = -t(t+8)$$ Thence I get a degenerate conics if $t = 0$ or $t = -8$ . So far so good. The second invariant is $$\text{det} \begin{pmatrix} 2t & t \\ t & 0 \end{pmatrix} = -t^2$$ Which is always negative for every real $t$ , and it's zero for $t =0$ Now I am interested in the degenerate cases, but when I am going to study the $t = 0$ case I get stuck. If $t = 0$ the conics is degenerate, and also the determinant of the second matrix is zero which means it's a pair of parallel lines. If I plot the resultin equation for $t = 0$ I simply get $4y + 1 = 0$ which I expect to be ONE single line. However, suppose they are two parallel lines. Next step tells me that those lines are distinct, coincident or imaginary respectively if $D^2 + E^2$ is greater, equal or less than $4 (A+C) F$ . Here $D = 0$ and $E = 4$ . $A + C = 0$ hence I get $16 > 0$ , which means two parallel distinct lines. I am stuck over this for I cannot get where those two lines are.","I'm studying the following conics with respect upon a parameter (real). For those kinds of problems I have always followed this Wikipedia page, which I find the most clear among all the notes and websistes I have been searching though, https://en.wikipedia.org/wiki/Matrix_representation_of_conic_sections Yet There is something I cannot catch. So the matrix invariants in this case are Thence I get a degenerate conics if or . So far so good. The second invariant is Which is always negative for every real , and it's zero for Now I am interested in the degenerate cases, but when I am going to study the case I get stuck. If the conics is degenerate, and also the determinant of the second matrix is zero which means it's a pair of parallel lines. If I plot the resultin equation for I simply get which I expect to be ONE single line. However, suppose they are two parallel lines. Next step tells me that those lines are distinct, coincident or imaginary respectively if is greater, equal or less than . Here and . hence I get , which means two parallel distinct lines. I am stuck over this for I cannot get where those two lines are.",t 2tx^2 + 2txy + 4y + 1 = 0 \text{det} \begin{pmatrix} 2t & t & 0 \\ t & 0 & 2 \\ 0 & 2 & 1 \end{pmatrix} = -t(t+8) t = 0 t = -8 \text{det} \begin{pmatrix} 2t & t \\ t & 0 \end{pmatrix} = -t^2 t t =0 t = 0 t = 0 t = 0 4y + 1 = 0 D^2 + E^2 4 (A+C) F D = 0 E = 4 A + C = 0 16 > 0,"['linear-algebra', 'matrices', 'geometry', 'conic-sections']"
64,Proof: Non-negative integer matrix representations (NIM Rep) of a finite group are always permutation matrices.,Proof: Non-negative integer matrix representations (NIM Rep) of a finite group are always permutation matrices.,,"Let $G$ be a finite group. Definition: Non-negative integer matrix representation (NIM rep) of a finite group $G$ is a group homomorphism $\varphi : G \to GL(n,\mathbb{Z}_{\geq 0})$ . Prove that every matrix in the representation i.e. $\varphi(g)$ for every $g \in G$ is always a permutation matrix. Phrased another way: Let $M \in  GL(n,\mathbb{Z}_{\geq 0})$ . If $M^k = I$ for some $k \in \mathbb{Z}_{\geq 0}$ , then prove that $M$ is a permutation matrix. Edit: The answer by Qiaochu Yuan proves that $GL(n,\mathbb{Z}_{\geq 0}) = S_n$ which is even stronger than what is asked to be proven.","Let be a finite group. Definition: Non-negative integer matrix representation (NIM rep) of a finite group is a group homomorphism . Prove that every matrix in the representation i.e. for every is always a permutation matrix. Phrased another way: Let . If for some , then prove that is a permutation matrix. Edit: The answer by Qiaochu Yuan proves that which is even stronger than what is asked to be proven.","G G \varphi : G \to GL(n,\mathbb{Z}_{\geq 0}) \varphi(g) g \in G M \in  GL(n,\mathbb{Z}_{\geq 0}) M^k = I k \in \mathbb{Z}_{\geq 0} M GL(n,\mathbb{Z}_{\geq 0}) = S_n","['matrices', 'finite-groups', 'representation-theory']"
65,How to prove that generalized Vandermonde matrix is invertible?,How to prove that generalized Vandermonde matrix is invertible?,,"Given $$A = \left( z_i^{\lambda_k}\right)_{i,j = 1,\ldots, n} =  \begin{pmatrix} z_1^{\lambda_1} & z_1^{\lambda_2} & \cdots & z_1^{\lambda_n} \\ z_2^{\lambda_1} & z_2^{\lambda_2} & \cdots & z_2^{\lambda_n} \\ \vdots  & \vdots  & \ddots & \vdots  \\ z_n^{\lambda_1} & z_n^{\lambda_2} & \cdots & z_n^{\lambda_n}  \end{pmatrix}.$$ with $z_1 < z_2 < ... < z_n \in \mathcal{R}_+$ and $\lambda_1 < \lambda_2 < ... \lambda_p \in \mathcal{R}$ ,  how to show that A is invertible? I think I probably need to show that the determinant is not zero but I am not sure how. I do some derivation and get Not sure how to continue. Also not sure if this is the right way. There is a similar question here Generalized Vandermonde-Matrix but the conditions are different.","Given with and ,  how to show that A is invertible? I think I probably need to show that the determinant is not zero but I am not sure how. I do some derivation and get Not sure how to continue. Also not sure if this is the right way. There is a similar question here Generalized Vandermonde-Matrix but the conditions are different.","A = \left( z_i^{\lambda_k}\right)_{i,j = 1,\ldots, n} = 
\begin{pmatrix}
z_1^{\lambda_1} & z_1^{\lambda_2} & \cdots & z_1^{\lambda_n} \\
z_2^{\lambda_1} & z_2^{\lambda_2} & \cdots & z_2^{\lambda_n} \\
\vdots  & \vdots  & \ddots & \vdots  \\
z_n^{\lambda_1} & z_n^{\lambda_2} & \cdots & z_n^{\lambda_n} 
\end{pmatrix}. z_1 < z_2 < ... < z_n \in \mathcal{R}_+ \lambda_1 < \lambda_2 < ... \lambda_p \in \mathcal{R}","['linear-algebra', 'matrices', 'determinant', 'matrix-analysis']"
66,"Finding pseudoinverse of ""non linearly-independent"" matrix","Finding pseudoinverse of ""non linearly-independent"" matrix",,"If: If $A$ has linearly independent columns, $A^+=\left(A^*A\right)^{-1}A^*$ If $A$ has linearly independent rows, $A^+=A^*\left(AA^*\right)^{-1}$ Otherwise, use the SVD decomposition. Is it possible to avoid using SVD decomposition? I've found the following method ( https://www.omnicalculator.com/math/pseudoinverse#how-to-calculate-the-pseudoinverse ): Start by calculating $AA^T$ and row reduce it to reduced row echelon form. Take the non-zero rows of the result and make them the columns of a new matrix $P$ . Similarly, row-reduce $A^TA$ and use its non-zero rows for the columns of the new matrix $Q$ . With your newly found $P$ and $Q$ , calculate $M=P^TAQ$ . Finally, calculate the pseudoinverse $A^+=QM^{-1}P^T$ . It works fine for most cases, but it doesn't work for $A=\left [ \begin{matrix}     0&1&0&-i\\0&0&1&0\\0&0&0&0\end{matrix} \right ]$ . Is the method wrong?","If: If has linearly independent columns, If has linearly independent rows, Otherwise, use the SVD decomposition. Is it possible to avoid using SVD decomposition? I've found the following method ( https://www.omnicalculator.com/math/pseudoinverse#how-to-calculate-the-pseudoinverse ): Start by calculating and row reduce it to reduced row echelon form. Take the non-zero rows of the result and make them the columns of a new matrix . Similarly, row-reduce and use its non-zero rows for the columns of the new matrix . With your newly found and , calculate . Finally, calculate the pseudoinverse . It works fine for most cases, but it doesn't work for . Is the method wrong?","A A^+=\left(A^*A\right)^{-1}A^* A A^+=A^*\left(AA^*\right)^{-1} AA^T P A^TA Q P Q M=P^TAQ A^+=QM^{-1}P^T A=\left [ \begin{matrix}
    0&1&0&-i\\0&0&1&0\\0&0&0&0\end{matrix} \right ]","['matrices', 'pseudoinverse']"
67,"Exercise 10, Section 3.4 of Hoffman’s Linear Algebra","Exercise 10, Section 3.4 of Hoffman’s Linear Algebra",,"We have seen that the linear operator $T$ on $R^2$ defined by $T(x_1, x_2) = (x_1, 0)$ is represented in the standard ordered basis by the matrix $A=\begin{bmatrix} 1 & 0\\ 0 & 0\\ \end{bmatrix}$ . This operator satisfies $T^2 = T$ . Prove that if $S$ is a linear operator on $R^2$ such that $S^2=S$ , then $S=0$ , or $S=I$ , or there is an ordered basis $B$ for $R^2$ such that $[S]_B = A$ (above). There are lots of way to prove this problem. I’m trying to show, if $S\neq 0$ and $S\neq I$ , then $\exists B$ ordered basis of $R^2$ such that $[S]_B=A$ . So $\exists \alpha_1,\alpha_2\in \Bbb{R}^2$ such that $S(\alpha_1)\neq (0,0)$ and $S(\alpha_2)\neq \alpha_2$ . How to progress from here?","We have seen that the linear operator on defined by is represented in the standard ordered basis by the matrix . This operator satisfies . Prove that if is a linear operator on such that , then , or , or there is an ordered basis for such that (above). There are lots of way to prove this problem. I’m trying to show, if and , then ordered basis of such that . So such that and . How to progress from here?","T R^2 T(x_1, x_2) = (x_1, 0) A=\begin{bmatrix} 1 & 0\\ 0 & 0\\ \end{bmatrix} T^2 = T S R^2 S^2=S S=0 S=I B R^2 [S]_B = A S\neq 0 S\neq I \exists B R^2 [S]_B=A \exists \alpha_1,\alpha_2\in \Bbb{R}^2 S(\alpha_1)\neq (0,0) S(\alpha_2)\neq \alpha_2","['linear-algebra', 'matrices', 'proof-writing', 'linear-transformations']"
68,$A_{n\times n}$ be a complex matrix with $A^2 = -I$,be a complex matrix with,A_{n\times n} A^2 = -I,"Let $A_{n\times n}$ be a complex matrix with $A^2 = -I$ . I want to prove that $A^3 = A^{-1}$ , and disprove that $I - A = A^{-1}$ . I do not know if the matrix is invertible, else, this will simplify the rest of the work. Suppose that its not invertible, and suppose that i cant deduce from $A^2 + I = 0$ that $A$ is in polynomial vector space, particularly for $x^2 + 1$ dimension. How can I prove the first part? For the second part: $(1)\quad I - A = A^{-1} / \cdot A$ $(2)\quad A^{-1} - A^2 = I /$ substitute the given argument $(3)\quad A^{-1} + I = I \quad \leftrightarrow \quad A^{-1} = 0$ Which is sort of ""reductio ad absurdum"" because the zero matrix is not invertible. Is this a correct proof? Any advice and help would be highly appreciated!","Let be a complex matrix with . I want to prove that , and disprove that . I do not know if the matrix is invertible, else, this will simplify the rest of the work. Suppose that its not invertible, and suppose that i cant deduce from that is in polynomial vector space, particularly for dimension. How can I prove the first part? For the second part: substitute the given argument Which is sort of ""reductio ad absurdum"" because the zero matrix is not invertible. Is this a correct proof? Any advice and help would be highly appreciated!",A_{n\times n} A^2 = -I A^3 = A^{-1} I - A = A^{-1} A^2 + I = 0 A x^2 + 1 (1)\quad I - A = A^{-1} / \cdot A (2)\quad A^{-1} - A^2 = I / (3)\quad A^{-1} + I = I \quad \leftrightarrow \quad A^{-1} = 0,"['linear-algebra', 'matrices']"
69,How do I find the rotation matrix between 2 normalized vectors coming from the origin,How do I find the rotation matrix between 2 normalized vectors coming from the origin,,"In my project, I have two vectors a normal vector $(0,0,1)$ (this is up in the program I'm using), and another normalized vector $(x,y,z)$ . Suppose I rotate $(0,0,1)$ to a new $(a,b,c)$ which is also a normalized vector from the origin, how do I find the position of the new $(x,y,z)$ which rotated along with it. What I think might work is finding the rotation matrix between $(0,0,1)$ and $(a,b,c)$ and applying that rotation matrix to $(x,y,z)$ , but I don't know how to find the rotation matrix between them. Below is an example, the red is $(0,0,1)$ in case 1 and $(a,b,c)$ in the rest of them, and blue is $(x,y,z)$ and $(x', y', z')$ (or what I think $(x',y',z')$ is). Also if it is indeed the rotation matrix between $(0,0,1)$ and $(a,b,c)$ that is being applied to $(x,y,z)$ I think it should be the simplest rotation matrix from $(0,0,1)$ to $(a,b,c)$ , because if I'm not wrong there should be infinitely many rotation matrices between those two. The motion of the rotation also matters, as I want $(0,0,1)$ to move to $(a,b,c)$ along a plane, as demonstrated below. The drawn orange vector on the right is perpendicular to this plane.","In my project, I have two vectors a normal vector (this is up in the program I'm using), and another normalized vector . Suppose I rotate to a new which is also a normalized vector from the origin, how do I find the position of the new which rotated along with it. What I think might work is finding the rotation matrix between and and applying that rotation matrix to , but I don't know how to find the rotation matrix between them. Below is an example, the red is in case 1 and in the rest of them, and blue is and (or what I think is). Also if it is indeed the rotation matrix between and that is being applied to I think it should be the simplest rotation matrix from to , because if I'm not wrong there should be infinitely many rotation matrices between those two. The motion of the rotation also matters, as I want to move to along a plane, as demonstrated below. The drawn orange vector on the right is perpendicular to this plane.","(0,0,1) (x,y,z) (0,0,1) (a,b,c) (x,y,z) (0,0,1) (a,b,c) (x,y,z) (0,0,1) (a,b,c) (x,y,z) (x', y', z') (x',y',z') (0,0,1) (a,b,c) (x,y,z) (0,0,1) (a,b,c) (0,0,1) (a,b,c)","['matrices', 'geometry', 'rotations']"
70,"An example of a non-diagonalisable matrix in $\mathrm{SL}(5, \mathbb{Z})$ whose Jordan blocks don't have determinant $1$",An example of a non-diagonalisable matrix in  whose Jordan blocks don't have determinant,"\mathrm{SL}(5, \mathbb{Z}) 1","Does there exists a matrix $M \in \mathrm{SL}(5, \mathbb{Z})$ , such that: $M$ is not diagonalisable; Let $J$ be the Jordan normal form of $M$ , $J$ has one Jordan block of size $3$ and one Jordan block of size $2$ , and none of the Jordan blocks have a determinant with an absolute value of $1$ ? So $J$ looks like this, $$ J = \left( \begin{array}{ccc} \lambda_1  & 1  & 0 & 0& 0\\  0 &  \lambda_1 & 0 & 0& 0 \\  0 & 0 & \lambda_2 & 1& 0 \\  0 &  0 & 0 & \lambda_2& 1 \\  0 &  0 & 0 & 0& \lambda_2 \\ \end{array} \right). $$ with $|\lambda_1|^2 \ne 1$ and $|\lambda_2|^3 \ne 1$ ? Where I got this question from : This is a follow-up question to the one I ask here . (Thank you, TheSilverDoe, who answered my previous questions). So far, all the examples are made up of ""smaller"" $2$ by $2$ submatrices. I was wondering if there are matrices that are not of this form, e.g. a matrix with an odd degree. Thanks for your time reading and help in advance.","Does there exists a matrix , such that: is not diagonalisable; Let be the Jordan normal form of , has one Jordan block of size and one Jordan block of size , and none of the Jordan blocks have a determinant with an absolute value of ? So looks like this, with and ? Where I got this question from : This is a follow-up question to the one I ask here . (Thank you, TheSilverDoe, who answered my previous questions). So far, all the examples are made up of ""smaller"" by submatrices. I was wondering if there are matrices that are not of this form, e.g. a matrix with an odd degree. Thanks for your time reading and help in advance.","M \in \mathrm{SL}(5, \mathbb{Z}) M J M J 3 2 1 J 
J =
\left(
\begin{array}{ccc}
\lambda_1  & 1  & 0 & 0& 0\\
 0 &  \lambda_1 & 0 & 0& 0 \\
 0 & 0 & \lambda_2 & 1& 0 \\
 0 &  0 & 0 & \lambda_2& 1 \\
 0 &  0 & 0 & 0& \lambda_2 \\
\end{array}
\right).
 |\lambda_1|^2 \ne 1 |\lambda_2|^3 \ne 1 2 2","['linear-algebra', 'abstract-algebra', 'matrices', 'group-theory', 'eigenvalues-eigenvectors']"
71,"If $A$ is a symmetric matrix, then $\det(A) \leq \prod\limits_{i = 1}^d a_{ii}.$","If  is a symmetric matrix, then",A \det(A) \leq \prod\limits_{i = 1}^d a_{ii}.,"Prove or provide a counterexample. If $A = (a_{ij})$ is a symmetric matrix, then $\det(A) \leq \prod\limits_{i = 1}^d a_{ii}.$ The result is obviously true for diagonal matrices and here is a proof for positive semidefinite matrices. Proof when $A$ is also positive semidefinite. If the smallest eigenvalue of $A$ is zero, then $\det(A) = 0$ and we only need to show that $\prod\limits_{i = 1}^d a_{ii} \geq 0,$ which follows from the fact that $a_{ii} = c_i^\intercal A c_i,$ where $c_i$ is the $i$ th canonical vector. Suppose that the smallest eigenvalue of $A$ is positive, this implies that $A$ is positive definite. We can orthogonally diagonalise $$ A = P\Lambda P^\intercal = \lambda_1 x_{(1)} x_{(1)}^\intercal + \ldots + \lambda_d x_{(d)}x_{(d)}^\intercal, $$ where $P = \left[x_{(1)}, \ldots, x_{(d)}\right]$ is an orthogonal matrix and $\lambda_1 \geq \ldots \geq \lambda_d > 0.$ Write $P^\intercal = [x_1, \ldots, x_d],$ i.e. the $x_i$ are the rows of $P.$ Then $$ a_{ii} = \sum_{j = 1}^d \lambda_j x_{ij}^2 = u(x_i), $$ where $u$ is the function $u(\xi_1, \ldots, \xi_d) = \lambda_1 \xi_1^2 + \ldots + \lambda_d \xi_d^2.$ Therefore, we have $$ \prod_{i = 1}^d a_{ii} = \prod_{i = 1}^d u(x_i). $$ We now show that $$ (\min u(x_1) \cdots u(x_d) \quad \text{subject to} \quad [x_1, \ldots, x_d] \quad \text{is orthogonal}) = \lambda_1 \cdots \lambda_d. $$ If this is true, we are done. We first minimise over the last column $x_d$ subject only to the contraint that $\|x_d\|^2 = 1.$ It is obvious that $u(x_d) \geq \lambda_d$ with equality when $x_d = c_d$ (the $d$ th canonical vector). We can substitute this partial result, and our problem now becomes $$ (\min u(x_1) \cdots u(x_{d-1}) u(c_d) \quad \text{subject to} \quad [x_1, \ldots, x_{d-1}, c_d] \quad \text{is orthogonal}) = \lambda_1 \cdots \lambda_d. $$ Since $u(c_d) = \lambda_d > 0,$ we may cancel it, and clearly the condition "" $[x_1, \ldots, x_{d-1}, c_d]$ is orthogonal"" is equivalent to "" $[x_1, \ldots, x_{d-1}]$ is orthogonal"" (in $\mathbf{R}^{d-1}$ where we assume all last entried of $x_1, \ldots, x_{d-1}$ are zero). Therefore, we have the same problem as before but one dimension less, and by an easy induction, we are done. QED","Prove or provide a counterexample. If is a symmetric matrix, then The result is obviously true for diagonal matrices and here is a proof for positive semidefinite matrices. Proof when is also positive semidefinite. If the smallest eigenvalue of is zero, then and we only need to show that which follows from the fact that where is the th canonical vector. Suppose that the smallest eigenvalue of is positive, this implies that is positive definite. We can orthogonally diagonalise where is an orthogonal matrix and Write i.e. the are the rows of Then where is the function Therefore, we have We now show that If this is true, we are done. We first minimise over the last column subject only to the contraint that It is obvious that with equality when (the th canonical vector). We can substitute this partial result, and our problem now becomes Since we may cancel it, and clearly the condition "" is orthogonal"" is equivalent to "" is orthogonal"" (in where we assume all last entried of are zero). Therefore, we have the same problem as before but one dimension less, and by an easy induction, we are done. QED","A = (a_{ij}) \det(A) \leq \prod\limits_{i = 1}^d a_{ii}. A A \det(A) = 0 \prod\limits_{i = 1}^d a_{ii} \geq 0, a_{ii} = c_i^\intercal A c_i, c_i i A A 
A = P\Lambda P^\intercal = \lambda_1 x_{(1)} x_{(1)}^\intercal + \ldots + \lambda_d x_{(d)}x_{(d)}^\intercal,
 P = \left[x_{(1)}, \ldots, x_{(d)}\right] \lambda_1 \geq \ldots \geq \lambda_d > 0. P^\intercal = [x_1, \ldots, x_d], x_i P. 
a_{ii} = \sum_{j = 1}^d \lambda_j x_{ij}^2 = u(x_i),
 u u(\xi_1, \ldots, \xi_d) = \lambda_1 \xi_1^2 + \ldots + \lambda_d \xi_d^2. 
\prod_{i = 1}^d a_{ii} = \prod_{i = 1}^d u(x_i).
 
(\min u(x_1) \cdots u(x_d) \quad \text{subject to} \quad [x_1, \ldots, x_d] \quad \text{is orthogonal}) = \lambda_1 \cdots \lambda_d.
 x_d \|x_d\|^2 = 1. u(x_d) \geq \lambda_d x_d = c_d d 
(\min u(x_1) \cdots u(x_{d-1}) u(c_d) \quad \text{subject to} \quad [x_1, \ldots, x_{d-1}, c_d] \quad \text{is orthogonal}) = \lambda_1 \cdots \lambda_d.
 u(c_d) = \lambda_d > 0, [x_1, \ldots, x_{d-1}, c_d] [x_1, \ldots, x_{d-1}] \mathbf{R}^{d-1} x_1, \ldots, x_{d-1}","['linear-algebra', 'matrices', 'optimization', 'determinant', 'symmetric-matrices']"
72,Are submatrices of an arbitrary complex unitary matrix diagonalizable?,Are submatrices of an arbitrary complex unitary matrix diagonalizable?,,"Consider a complex unitary matrix $U \in \mathbb{C}^{N\times N}$ and pick two diagonal and two off-diagonal elements from its $m$ th and $n$ th rows to construct a $2 \times 2$ submatrix: \begin{equation} M= \begin{bmatrix} U_{mm} & U_{mn}\\ U_{nm} & U_{nn} \end{bmatrix} = \begin{bmatrix} U_{mm} & U_{mn}\\ (U_{mn})^* & U_{nn} \end{bmatrix} \end{equation} (Here I consider the Hermitian case $U_{nm} = (U_{mn})^*$ .) Is $M$ always diagonalizable? Update : Conjecture: $M$ should be diagonalizable. Because $U$ is unitary, the eigenvectors of $U$ should span a linear space of dimension $N$ . If there exists any $M$ not diagonalizable, the eigenvectors of that $M$ does not span a dimension $2$ space. Brute force proof: find the eigenvectors of this sample $2\times 2$ submatrix. Its eigenvectors are also shown in the figure. ( $a_{1,2}, b_{1,2} \in \mathbb{R}$ , and $c \in \mathbb{C}$ ) For the two eigenvectors to be identical, the following two equations need to be satisfied: \begin{equation} (a_1-b_1)^2 - (a_2-b_2)^2 + 4|c|^2 = 0 \text{ and } (a_1-b_1)(a_2-b_2) = 0. \end{equation} Clearly, both equations can be simultaneously satisfied. When they are both satisfied, $M$ does not have two independent eigenvectors and therefore is not diagonalizable. This result looks counter-intuitive. Moreover, I started to think about this question when studying this historic paper . This paper is about a recursive way of building any $N\times N$ unitary matrix with optical experiments. Its equation 2 shows that by applying in total $N-1$ different $2\times 2$ rotational matrices to a $(N-1)\times (N-1)$ unitary matrix, one can construct a $N\times N$ unitary matrix. Performing this recursively, one can use $N(N-1)/2$ rotational matrices to construct a $N\times N$ unitary matrix starting from an identity matrix. However, it seems that we just showed that not all unitary matrices $U$ can be constructed in this way...","Consider a complex unitary matrix and pick two diagonal and two off-diagonal elements from its th and th rows to construct a submatrix: (Here I consider the Hermitian case .) Is always diagonalizable? Update : Conjecture: should be diagonalizable. Because is unitary, the eigenvectors of should span a linear space of dimension . If there exists any not diagonalizable, the eigenvectors of that does not span a dimension space. Brute force proof: find the eigenvectors of this sample submatrix. Its eigenvectors are also shown in the figure. ( , and ) For the two eigenvectors to be identical, the following two equations need to be satisfied: Clearly, both equations can be simultaneously satisfied. When they are both satisfied, does not have two independent eigenvectors and therefore is not diagonalizable. This result looks counter-intuitive. Moreover, I started to think about this question when studying this historic paper . This paper is about a recursive way of building any unitary matrix with optical experiments. Its equation 2 shows that by applying in total different rotational matrices to a unitary matrix, one can construct a unitary matrix. Performing this recursively, one can use rotational matrices to construct a unitary matrix starting from an identity matrix. However, it seems that we just showed that not all unitary matrices can be constructed in this way...","U \in \mathbb{C}^{N\times N} m n 2 \times 2 \begin{equation}
M=
\begin{bmatrix}
U_{mm} & U_{mn}\\
U_{nm} & U_{nn}
\end{bmatrix}
=
\begin{bmatrix}
U_{mm} & U_{mn}\\
(U_{mn})^* & U_{nn}
\end{bmatrix}
\end{equation} U_{nm} = (U_{mn})^* M M U U N M M 2 2\times 2 a_{1,2}, b_{1,2} \in \mathbb{R} c \in \mathbb{C} \begin{equation}
(a_1-b_1)^2 - (a_2-b_2)^2 + 4|c|^2 = 0 \text{ and } (a_1-b_1)(a_2-b_2) = 0.
\end{equation} M N\times N N-1 2\times 2 (N-1)\times (N-1) N\times N N(N-1)/2 N\times N U","['linear-algebra', 'matrices', 'diagonalization', 'unitary-matrices']"
73,Show that $\quad \vec{x}^TA\vec{x}=\frac{1}{2}\vec{x}^T(A^T+A)\vec{x}$,Show that,\quad \vec{x}^TA\vec{x}=\frac{1}{2}\vec{x}^T(A^T+A)\vec{x},Let $A\in\mathbb{R}^{n\times n}$ and $\vec{x}\in\mathbb{R}^{n\times 1}$ Show that $\quad \vec{x}^TA\vec{x}=\frac{1}{2}\vec{x}^T(A^T+A)\vec{x}$ My try: We know that $$A=\frac{1}{2}(A+A^T)+\frac{1}{2}(A-A^T)$$ Then $$\vec{x}^TA\vec{x}=\vec{x}^T(\frac{1}{2}(A+A^T)+\frac{1}{2}(A-A^T))\vec{x}$$ $$=\frac{1}{2}\vec{x}^T((A+A^T)+(A-A^T))\vec{x}$$ $$=\frac{1}{2}\vec{x}^T((A+A)\vec{x}$$ Which is not quite what I want. Any suggestions of how to keep going would be great!,Let and Show that My try: We know that Then Which is not quite what I want. Any suggestions of how to keep going would be great!,A\in\mathbb{R}^{n\times n} \vec{x}\in\mathbb{R}^{n\times 1} \quad \vec{x}^TA\vec{x}=\frac{1}{2}\vec{x}^T(A^T+A)\vec{x} A=\frac{1}{2}(A+A^T)+\frac{1}{2}(A-A^T) \vec{x}^TA\vec{x}=\vec{x}^T(\frac{1}{2}(A+A^T)+\frac{1}{2}(A-A^T))\vec{x} =\frac{1}{2}\vec{x}^T((A+A^T)+(A-A^T))\vec{x} =\frac{1}{2}\vec{x}^T((A+A)\vec{x},"['linear-algebra', 'matrices', 'symmetric-matrices']"
74,"Are there such things as 3-dimensional (and higher) analogues of matrices, and if so, do they have any applications?","Are there such things as 3-dimensional (and higher) analogues of matrices, and if so, do they have any applications?",,"A matrix is a group of numbers arranged in a rectangle. I wonder, has anyone studied 3-dimensional and higher analogues of matrices? For example, there could be such a thing as a 2 by 2 by 2 3d matrix, whose entries are all equal to 1. Has anyone else defined these entities, and more importantly, are they used in mathematics?","A matrix is a group of numbers arranged in a rectangle. I wonder, has anyone studied 3-dimensional and higher analogues of matrices? For example, there could be such a thing as a 2 by 2 by 2 3d matrix, whose entries are all equal to 1. Has anyone else defined these entities, and more importantly, are they used in mathematics?",,['matrices']
75,Are adjoint representation matrices the generators of the Adjoint representation?,Are adjoint representation matrices the generators of the Adjoint representation?,,"My question is, are the (Lie Algebra) adjoint representation matrices in general the generators of the (Lie Group) Adjoint representation (i.e. treating the Adjoint representation as a matrix Lie Group)? My justification is that $ad(\xi) = T_e (Ad(\xi))$ . Therefore we treat the Adjoint representation as its own matrix Lie Group (living in the Lie Algebra vector space, i.e. it acts on Lie Algebra elements like $\xi$ in the above statement). Therefore, since it is a matrix Lie Group, we can in general define a Lie Algebra representation on it by its generating matrices (with the Lie Bracket represented by the usual matrix commutator) The reason I ask this is because I was confused why the adjoint representation of su(2) (with the basis given below) could seemingly represent the Lie Bracket either by matrix-vector multiplication (with a vector in the Lie Algebra), or by their usual matrix commutator. (e.g. $[T_1,T_2]=T_3$ could be represented by $T_1*(0 1 0)= (0 0 1)$ or represented by $T_1*T_2-T_2*T_1 = T_3$ ) My solution is therefore that adjoint is defined so that the matrix-vector multiplication represents the Lie Bracket but, since (by different reasoning) the basis of the adjoint are also 'raw' elements of the Lie Algebra of the Adjoint matrix Lie Group, they can also be represented by generators of the Adjoint matrix Lie Group (and therefore represent the Lie Bracket by the usual commutator). $$ T_1 = \begin{bmatrix} 0&0&0\\0&0&-1\\0&1&0\end{bmatrix}, T_2 = \begin{bmatrix} 0&0&1\\0&0&0\\-1&0&0\end{bmatrix}, T_3 = \begin{bmatrix} 0&-1&0\\1&0&0\\0&0&0\end{bmatrix}$$","My question is, are the (Lie Algebra) adjoint representation matrices in general the generators of the (Lie Group) Adjoint representation (i.e. treating the Adjoint representation as a matrix Lie Group)? My justification is that . Therefore we treat the Adjoint representation as its own matrix Lie Group (living in the Lie Algebra vector space, i.e. it acts on Lie Algebra elements like in the above statement). Therefore, since it is a matrix Lie Group, we can in general define a Lie Algebra representation on it by its generating matrices (with the Lie Bracket represented by the usual matrix commutator) The reason I ask this is because I was confused why the adjoint representation of su(2) (with the basis given below) could seemingly represent the Lie Bracket either by matrix-vector multiplication (with a vector in the Lie Algebra), or by their usual matrix commutator. (e.g. could be represented by or represented by ) My solution is therefore that adjoint is defined so that the matrix-vector multiplication represents the Lie Bracket but, since (by different reasoning) the basis of the adjoint are also 'raw' elements of the Lie Algebra of the Adjoint matrix Lie Group, they can also be represented by generators of the Adjoint matrix Lie Group (and therefore represent the Lie Bracket by the usual commutator).","ad(\xi) = T_e (Ad(\xi)) \xi [T_1,T_2]=T_3 T_1*(0 1 0)= (0 0 1) T_1*T_2-T_2*T_1 = T_3  T_1 = \begin{bmatrix} 0&0&0\\0&0&-1\\0&1&0\end{bmatrix}, T_2 = \begin{bmatrix} 0&0&1\\0&0&0\\-1&0&0\end{bmatrix}, T_3 = \begin{bmatrix} 0&-1&0\\1&0&0\\0&0&0\end{bmatrix}","['matrices', 'manifolds', 'lie-groups', 'lie-algebras', 'smooth-manifolds']"
76,Is the set of all underdetermined matrices A that solve Ax = b (for a given x and b) dense?,Is the set of all underdetermined matrices A that solve Ax = b (for a given x and b) dense?,,"Let's say I have two fixed vectors $x \in \mathbb{R}^{n}$ and $b \in \mathbb{R}^{m}$ , where $m < n$ . There are an infinite number of matrices $A \in \mathbb{R}^{m \times n}$ that can solve this underdetermined system. My question is: are these solutions dense over the set of all matrices in $\mathbb{R}^{m \times n}$ ? Please be gentle, I am not very mathematical.","Let's say I have two fixed vectors and , where . There are an infinite number of matrices that can solve this underdetermined system. My question is: are these solutions dense over the set of all matrices in ? Please be gentle, I am not very mathematical.",x \in \mathbb{R}^{n} b \in \mathbb{R}^{m} m < n A \in \mathbb{R}^{m \times n} \mathbb{R}^{m \times n},"['linear-algebra', 'general-topology', 'matrices']"
77,Integer eigenvalues in Matlab,Integer eigenvalues in Matlab,,I am trying to write a Matlab program which decides if a given (integer) matrix A has integer eigenvalues and if this is the case calculates the eigenvalues and their multiplicities. Any ideas how to get started?,I am trying to write a Matlab program which decides if a given (integer) matrix A has integer eigenvalues and if this is the case calculates the eigenvalues and their multiplicities. Any ideas how to get started?,,"['matrices', 'eigenvalues-eigenvectors', 'matlab']"
78,"What does it means ""orthonormal"" vector?","What does it means ""orthonormal"" vector?",,"my teacher's slides say that ""The columns [u_1, u_2, . . . , u_m] of an orthogonal matrix are orthonormal"". The slides provide a rigorous definition of orthogonal matrix but they say nothing about orthonormal vectors. What does it means?","my teacher's slides say that ""The columns [u_1, u_2, . . . , u_m] of an orthogonal matrix are orthonormal"". The slides provide a rigorous definition of orthogonal matrix but they say nothing about orthonormal vectors. What does it means?",,"['matrices', 'terminology', 'orthonormal', 'orthogonal-matrices']"
79,Prove: $\det(I+A) = 2^{\text{rank}(A)}$ if $A$ is a square idempotent matrix. Find $(I+A)^{-1}$ such that the expression doesn't have inverses.,Prove:  if  is a square idempotent matrix. Find  such that the expression doesn't have inverses.,\det(I+A) = 2^{\text{rank}(A)} A (I+A)^{-1},To prove: $\det(I+A)$ = $2^{\operatorname{rank}(A)}$ if $A \in$ $\mathbb{R}^{n\times n}$ and $A^{2}=A$ . Find an expression for $(I+A)^{-1}$ such that it does not involve inverses. Is there any way I could use Jordan Block? I am not sure where I need to start in order to solve it.,To prove: = if and . Find an expression for such that it does not involve inverses. Is there any way I could use Jordan Block? I am not sure where I need to start in order to solve it.,\det(I+A) 2^{\operatorname{rank}(A)} A \in \mathbb{R}^{n\times n} A^{2}=A (I+A)^{-1},"['linear-algebra', 'matrices', 'determinant', 'jordan-normal-form', 'idempotents']"
80,If the trace of the matrices $A^k$ are equal to the size of $A$ why the spectrum of $A$ is $\{1\}$,If the trace of the matrices  are equal to the size of  why the spectrum of  is,A^k A A \{1\},"Let $A$ a matrix in $\mathcal{M}_n(\mathbb C)$ such that $\operatorname{Tr}(A^k)=n$ for all $k\in\{1,\ldots,n\}$ . I have to prove that $\operatorname{Sp}(A)=\{1\}$ . I denoted $\lambda_1,\ldots,\lambda_n$ the eigenvalues of $A$ with multiplicity so the hypothesis leads to the system of equations: $\lambda_1^k+\cdots+\lambda_n^k=n$ for all $k$ . I can see that $\lambda_1=\cdots=\lambda_n=1$ is a trivial solution of the system but it's not obvious for me why it would be the unique solution. Any suggestion?",Let a matrix in such that for all . I have to prove that . I denoted the eigenvalues of with multiplicity so the hypothesis leads to the system of equations: for all . I can see that is a trivial solution of the system but it's not obvious for me why it would be the unique solution. Any suggestion?,"A \mathcal{M}_n(\mathbb C) \operatorname{Tr}(A^k)=n k\in\{1,\ldots,n\} \operatorname{Sp}(A)=\{1\} \lambda_1,\ldots,\lambda_n A \lambda_1^k+\cdots+\lambda_n^k=n k \lambda_1=\cdots=\lambda_n=1","['matrices', 'eigenvalues-eigenvectors', 'trace']"
81,Prove a triangular matrix is invertable if and only if all diagonal values are non-zero. [duplicate],Prove a triangular matrix is invertable if and only if all diagonal values are non-zero. [duplicate],,"This question already has answers here : Prove that an upper triangular matrix is invertible if and only if every diagonal entry is non-zero. (4 answers) Closed 2 years ago . Let $A \in K^{n×n}$ be an (upper) triangular matrix, so $A = (a_{ij})_{i,j = 1,...,n}$ and $a_{ij} = 0$ for $i > j$ . Then A is invertable exactly then, when all $a_{ii} \neq 0; i = 1, . . . , n$ . I have proven that If A is invertable then all the diagonal elements have to be non-zero like this: Let A be an invertible Matrix and let one $x_{ii}=0$ . Because A is invertible, it represents a bijective linear transformation, so the kernel of T is trivial, which implies linear independence of the columns of the matrix. Because A is an upper triangular matrix, its diagonal entries represent a linearly independent set of vectors. But if one $x_{ii} = 0$ , then the set of vectors is no longer linearly independent, which is a contradiction. My problems are a) i dont know how to prove the other direction b) I am unsure as to wether this proof works. Any help is appreciated :) P.S.: since we didnt introduce the determinant yet, I am not allowed to just say that the diagonals have to be non-zero, so the determinant is non-zero, so it is invertible...","This question already has answers here : Prove that an upper triangular matrix is invertible if and only if every diagonal entry is non-zero. (4 answers) Closed 2 years ago . Let be an (upper) triangular matrix, so and for . Then A is invertable exactly then, when all . I have proven that If A is invertable then all the diagonal elements have to be non-zero like this: Let A be an invertible Matrix and let one . Because A is invertible, it represents a bijective linear transformation, so the kernel of T is trivial, which implies linear independence of the columns of the matrix. Because A is an upper triangular matrix, its diagonal entries represent a linearly independent set of vectors. But if one , then the set of vectors is no longer linearly independent, which is a contradiction. My problems are a) i dont know how to prove the other direction b) I am unsure as to wether this proof works. Any help is appreciated :) P.S.: since we didnt introduce the determinant yet, I am not allowed to just say that the diagonals have to be non-zero, so the determinant is non-zero, so it is invertible...","A \in K^{n×n} A = (a_{ij})_{i,j = 1,...,n} a_{ij} = 0 i > j a_{ii} \neq 0; i = 1, . . . , n x_{ii}=0 x_{ii} = 0","['matrices', 'inverse']"
82,Prove all $2\times2$ real matrices with eigenvalues $1$ and $-1$ can be represented as,Prove all  real matrices with eigenvalues  and  can be represented as,2\times2 1 -1,My exercise asks: Prove that all $2\times2$ real matrices with eigenvalues $\lambda_1=1$ and $\lambda_2=-1$ can be represented as \begin{equation} \begin{bmatrix} \cos\theta & a\sin\theta \\ \frac{1}{a}\sin\theta & -\cos\theta \end{bmatrix} \end{equation} Starting like \begin{equation} \begin{bmatrix} a & b \\ c & d \end{bmatrix} \end{equation} Using the fact that $\operatorname{Tr}(A) = a+d =\lambda_1+\lambda_2 = 0 \Rightarrow a=-d$ . And using the fact that $\det A = ad-bc=\lambda_1\lambda_2=-1 \Rightarrow-a^2-bc=-1$ \begin{equation} a^2+bc=1 \end{equation} How can I complete the proof?,My exercise asks: Prove that all real matrices with eigenvalues and can be represented as Starting like Using the fact that . And using the fact that How can I complete the proof?,"2\times2 \lambda_1=1 \lambda_2=-1 \begin{equation}
\begin{bmatrix}
\cos\theta & a\sin\theta \\
\frac{1}{a}\sin\theta & -\cos\theta
\end{bmatrix}
\end{equation} \begin{equation}
\begin{bmatrix}
a & b \\
c & d
\end{bmatrix}
\end{equation} \operatorname{Tr}(A) = a+d =\lambda_1+\lambda_2 = 0 \Rightarrow a=-d \det A = ad-bc=\lambda_1\lambda_2=-1 \Rightarrow-a^2-bc=-1 \begin{equation}
a^2+bc=1
\end{equation}","['linear-algebra', 'matrices', 'characteristic-polynomial']"
83,Kernel and image of matrix,Kernel and image of matrix,,We have the matrix \begin{equation*}M=\begin{pmatrix} \cos (\alpha )-1& \sin (\alpha ) \\ \sin (\alpha) & -\cos(\alpha)-1\end{pmatrix}\end{equation*} I want to calculate the kernel and  the image of the matrix. For the kernel we have to solve the system $(s_{\alpha}-u_2)x=0_{\mathbb{R}^2}$ . Using Gauss elimination algorithmwe get \begin{equation*}\begin{pmatrix} \cos (\alpha )-1& \sin (\alpha ) \\ \sin (\alpha) & -\cos(\alpha)-1\end{pmatrix} \rightarrow \begin{pmatrix} \cos (\alpha )-1& \sin (\alpha ) \\ 0 & 0\end{pmatrix}\end{equation*} or not? Is the kernel $\left \{\lambda \begin{pmatrix}\cos (\alpha)-1\\ \sin (\alpha)\end{pmatrix}\right \}$ ? Can  we write this vector in respect of $\frac{\alpha}{2}$ instad of $\alpha$ ? The solution must be $\left \{\lambda \begin{pmatrix}\cos \left (\frac{\alpha}{2}\right )\\ \sin \left (\frac{\alpha}{2}\right )\end{pmatrix}\right \}$,We have the matrix I want to calculate the kernel and  the image of the matrix. For the kernel we have to solve the system . Using Gauss elimination algorithmwe get or not? Is the kernel ? Can  we write this vector in respect of instad of ? The solution must be,\begin{equation*}M=\begin{pmatrix} \cos (\alpha )-1& \sin (\alpha ) \\ \sin (\alpha) & -\cos(\alpha)-1\end{pmatrix}\end{equation*} (s_{\alpha}-u_2)x=0_{\mathbb{R}^2} \begin{equation*}\begin{pmatrix} \cos (\alpha )-1& \sin (\alpha ) \\ \sin (\alpha) & -\cos(\alpha)-1\end{pmatrix} \rightarrow \begin{pmatrix} \cos (\alpha )-1& \sin (\alpha ) \\ 0 & 0\end{pmatrix}\end{equation*} \left \{\lambda \begin{pmatrix}\cos (\alpha)-1\\ \sin (\alpha)\end{pmatrix}\right \} \frac{\alpha}{2} \alpha \left \{\lambda \begin{pmatrix}\cos \left (\frac{\alpha}{2}\right )\\ \sin \left (\frac{\alpha}{2}\right )\end{pmatrix}\right \},"['linear-algebra', 'matrices', 'trigonometry', 'linear-transformations']"
84,"When a $A$ commute with $e^B$, does it also commute $B$?","When a  commute with , does it also commute ?",A e^B B,"Let $A$ and $B$ be $n\times n$ matrices, and let $[A,B]=AB-BA$ be the commutator of $A$ and $B$ . Then, if we consider the matrix exponential $e^B=\sum_n \frac{B^n}{n!} $ , we have $$ [A,e^B]=\sum_n \frac{1}{n!}[A,B^n] $$ Now, if $[A,B]=0$ , then $[A,B^n]=0$ $\forall n$ and therefore $[A,e^B]=0$ . But what about the converse? Is it true as well? In other words, does $[A,e^B]=0$ imply $[A,B]=0$ ?","Let and be matrices, and let be the commutator of and . Then, if we consider the matrix exponential , we have Now, if , then and therefore . But what about the converse? Is it true as well? In other words, does imply ?","A B n\times n [A,B]=AB-BA A B e^B=\sum_n \frac{B^n}{n!}   [A,e^B]=\sum_n \frac{1}{n!}[A,B^n]  [A,B]=0 [A,B^n]=0 \forall n [A,e^B]=0 [A,e^B]=0 [A,B]=0","['linear-algebra', 'matrices', 'exponential-function', 'matrix-exponential']"
85,To prove:- $|AB-I|+|BA-I| =0$ given that $A$ is any $3\times 2$ matrix and $B$ is any $2\times 3$ matrix.,To prove:-  given that  is any  matrix and  is any  matrix.,|AB-I|+|BA-I| =0 A 3\times 2 B 2\times 3,"I was just told that for any given matrix if it is $n\times (n-1)$ and another of $(n-1)\times n$ then also the above kind of situation that is $$\det(AB-I)+\det(BA-I)=0,$$ where $I$ is the corresponding identity matrix of the square matrix so formed. Also in order to prove it I tried to solve it by any theorem that I know or so but was unable. So, I checked it for any random matrix with random variable and calculated the same thing via a matrix calculator, and shockingly it was correct. I searched it on internet and was unable to find any explanation. So, I eventually turned to stack exchange. Any help would be appreciated!","I was just told that for any given matrix if it is and another of then also the above kind of situation that is where is the corresponding identity matrix of the square matrix so formed. Also in order to prove it I tried to solve it by any theorem that I know or so but was unable. So, I checked it for any random matrix with random variable and calculated the same thing via a matrix calculator, and shockingly it was correct. I searched it on internet and was unable to find any explanation. So, I eventually turned to stack exchange. Any help would be appreciated!","n\times (n-1) (n-1)\times n \det(AB-I)+\det(BA-I)=0, I","['real-analysis', 'matrices', 'algebra-precalculus', 'determinant']"
86,Does nuclear norm decrease as some elements in a matrix are set zero?,Does nuclear norm decrease as some elements in a matrix are set zero?,,Let $M$ be a generic nonzero real matrix and $M_{0}$ is constructed by replacing some elements in $M$ with zero. Let $||\cdot||_{*}$ denotes the nuclear norm operator. Is it true that $||M||_{*}\geq ||M_{0}||_{*}$ regardless of the replacement rule?,Let be a generic nonzero real matrix and is constructed by replacing some elements in with zero. Let denotes the nuclear norm operator. Is it true that regardless of the replacement rule?,M M_{0} M ||\cdot||_{*} ||M||_{*}\geq ||M_{0}||_{*},"['linear-algebra', 'matrices', 'normed-spaces', 'nuclear-norm']"
87,"How to generate a random vector, guaranteed to be within the hemisphere with respect to another vector?","How to generate a random vector, guaranteed to be within the hemisphere with respect to another vector?",,"Given a normalized vector N , how can one generate a random direction vector that is guaranteed to be in the hemisphere with respect to N (i.e. the hemisphere where N is exactly in the middle)? The way I am currently doing this is to sample a random direction vector d and dot it with N and keep that vector if the dot product is greater than 0 . This method is not guaranteed to generate a vector in the hemisphere that I am interested in as ~50% of the random vectors would have a 0 or negative dot product result. I saw somewhere that there is a way to transform the randomly generated vector d to place it in the right hemisphere using a transformation matrix but I don't know how to do it. Can someone write a [pseudo]code for how one generate a direction vector using the transformation method?","Given a normalized vector N , how can one generate a random direction vector that is guaranteed to be in the hemisphere with respect to N (i.e. the hemisphere where N is exactly in the middle)? The way I am currently doing this is to sample a random direction vector d and dot it with N and keep that vector if the dot product is greater than 0 . This method is not guaranteed to generate a vector in the hemisphere that I am interested in as ~50% of the random vectors would have a 0 or negative dot product result. I saw somewhere that there is a way to transform the randomly generated vector d to place it in the right hemisphere using a transformation matrix but I don't know how to do it. Can someone write a [pseudo]code for how one generate a direction vector using the transformation method?",,"['linear-algebra', 'matrices', 'linear-transformations', 'spherical-geometry']"
88,What is the relation between a matrix as a linear function versus the same matrix as a bilinear function?,What is the relation between a matrix as a linear function versus the same matrix as a bilinear function?,,"Given an $n \times n$ matrix $A$ , we can define a linear transformation $T: \mathbb{R}^n \rightarrow \mathbb{R}^n$ by $T(x)=Ax$ . We could also define a bilnear function $T: \mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R}$ by $T(x,y) = y^TAx$ . Is there a relation between these two uses of the matrix? Also, we could do the same thing with an $n \times m$ matrix and get a linear function $\mathbb{R}^m \rightarrow \mathbb{R}^n$ and a bilinear function $\mathbb{R}^m \times \mathbb{R}^n \rightarrow \mathbb{R}$ . Is the relationship between using $A$ to define a linear function versus a bilinear function the same in this case?","Given an matrix , we can define a linear transformation by . We could also define a bilnear function by . Is there a relation between these two uses of the matrix? Also, we could do the same thing with an matrix and get a linear function and a bilinear function . Is the relationship between using to define a linear function versus a bilinear function the same in this case?","n \times n A T: \mathbb{R}^n \rightarrow \mathbb{R}^n T(x)=Ax T: \mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R} T(x,y) = y^TAx n \times m \mathbb{R}^m \rightarrow \mathbb{R}^n \mathbb{R}^m \times \mathbb{R}^n \rightarrow \mathbb{R} A","['linear-algebra', 'matrices']"
89,Difficult matrix equation $A(x-x_0) + \alpha\left(\frac{\Sigma x}{\mu^\top x}\right) = 0$,Difficult matrix equation,A(x-x_0) + \alpha\left(\frac{\Sigma x}{\mu^\top x}\right) = 0,"Let $x,x_0,\theta,\mu \in \mathbb{R}^n$ vectors, $\alpha \in\mathbb{R}$ a scalar, and $\Sigma, A\in \mathbb{R}^{n\times n}$ Positive Semi-Definite matrices. Also, $\mu$ is a mean vector and $\Sigma$ is a covariance matrix of a Random Variable $Z$ (I don't know if this information may help). Is there a way to solve in closed form the following matrix equation for $x$ ? \begin{equation} A(x-x_0) + \alpha \left(\frac{\Sigma x}{\mu^\top x}\right) = 0.  \end{equation} Main difficulty: I find difficulties in handling the denominator. If the equation would have been simply $$A(x-x_0) + \alpha \Sigma x = 0,$$ the solution would be $x = (A+\alpha\Sigma)^{-1}Ax_0$ . I am looking for a similar solution for the first equation but the denominator complicates things and I don't know how to proceed. What I tried: \begin{aligned} & \mu^TxA(x-x_0) + \alpha \Sigma x = 0 \\ &\mu^TxAx - \mu^TxAx_0 - \alpha \Sigma x = 0 \\ &\mu^TxA(x-x_0) = \alpha \Sigma x  \end{aligned} Is this way of proceeding correct? If yes, how is it possible to solve this matrix equation for $x$ ? I am not an expert in solving matrix equations and any help would be greatly appreciated. Thanks!","Let vectors, a scalar, and Positive Semi-Definite matrices. Also, is a mean vector and is a covariance matrix of a Random Variable (I don't know if this information may help). Is there a way to solve in closed form the following matrix equation for ? Main difficulty: I find difficulties in handling the denominator. If the equation would have been simply the solution would be . I am looking for a similar solution for the first equation but the denominator complicates things and I don't know how to proceed. What I tried: Is this way of proceeding correct? If yes, how is it possible to solve this matrix equation for ? I am not an expert in solving matrix equations and any help would be greatly appreciated. Thanks!","x,x_0,\theta,\mu \in \mathbb{R}^n \alpha \in\mathbb{R} \Sigma, A\in \mathbb{R}^{n\times n} \mu \Sigma Z x \begin{equation}
A(x-x_0) + \alpha \left(\frac{\Sigma x}{\mu^\top x}\right) = 0. 
\end{equation} A(x-x_0) + \alpha \Sigma x = 0, x = (A+\alpha\Sigma)^{-1}Ax_0 \begin{aligned}
& \mu^TxA(x-x_0) + \alpha \Sigma x = 0
\\ &\mu^TxAx - \mu^TxAx_0 - \alpha \Sigma x = 0
\\ &\mu^TxA(x-x_0) = \alpha \Sigma x 
\end{aligned} x","['linear-algebra', 'matrices', 'systems-of-equations', 'matrix-equations', 'matrix-calculus']"
90,Characteristic polynomial of a A agrees with its minimal polynomial if and only if all matrices that commutes with A is a polynomial of A,Characteristic polynomial of a A agrees with its minimal polynomial if and only if all matrices that commutes with A is a polynomial of A,,"Let $A \in \mathcal{M}_n (\mathbb{C})$ . Over $\mathbb{C}$ , show that the following two statements are equivalent: Characteristic polynomial of a $A$ agrees with its minimal polynomial; All matrices that commutes with $A$ is a polynomial of $A$ . For 1 > 2, from 1 I can only get that $A$ has a Jordan canonical form that each eigenvalue has only one Jordan block (i.e. maximum size), and I have no idea afterwards. 2 > 1 is a complete no-go for me. Any hints or solutions are appreciated.","Let . Over , show that the following two statements are equivalent: Characteristic polynomial of a agrees with its minimal polynomial; All matrices that commutes with is a polynomial of . For 1 > 2, from 1 I can only get that has a Jordan canonical form that each eigenvalue has only one Jordan block (i.e. maximum size), and I have no idea afterwards. 2 > 1 is a complete no-go for me. Any hints or solutions are appreciated.",A \in \mathcal{M}_n (\mathbb{C}) \mathbb{C} A A A A,"['linear-algebra', 'matrices', 'jordan-normal-form', 'characteristic-polynomial']"
91,Finding four square roots of $\small\begin{bmatrix}3 & -4\\4 & 3\end{bmatrix}$,Finding four square roots of,\small\begin{bmatrix}3 & -4\\4 & 3\end{bmatrix},"This question is about finding the 4(!!) square roots of a 2 by 2 matrix. The method I use is through Diagonalization (or actually doing the ""opposite""). I have no problem finding the four roots of a 2 by 2 matrix that has real eigenvalues (and thus real eigen vectors). Heck, I found 8 roots of a 3 by 3 matrix that has real eigen values. The problem I am running into is if the to be square rooted matrix has complex eigenvalues. In my view, the method should work the same, but somehow the algebra is letting me down. Example: Find the four square roots of $ \begin{bmatrix}3 & -4\\4 & 3\end{bmatrix}$ . Note, I found this matrix by squaring $ \begin{bmatrix}2 & -1\\1 & 2\end{bmatrix}$ , so obviously this is one root as well as taking opposite of its entries. So here is my work: The characteristic equation of $ \begin{bmatrix}3 & -4\\4 & 3\end{bmatrix}$ is $(3-\lambda)^2+16=0$ from which we find $\lambda=3+4i$ and $\lambda=3-4i$ . An eigenvector for $\lambda=3+4i$ is $ \begin{bmatrix}1 \\-i  \end{bmatrix}$ and an eigenvector for $\lambda=3-4i$ is $ \begin{bmatrix}1 \\i  \end{bmatrix}$ . So the given matrix can now be diagonalized: $ \begin{bmatrix}3 & -4\\4 & 3\end{bmatrix}$ = $ \begin{bmatrix}1 & 1\\-i & i\end{bmatrix}$$ \begin{bmatrix}3+4i & 0\\0 & 3-4i\end{bmatrix}$$ \begin{bmatrix}1 & 1\\-i & i\end{bmatrix}^{-1}$ . I found $ \begin{bmatrix}1 & 1\\-i & i\end{bmatrix}^{-1}$ = $\frac{1}{2i} \begin{bmatrix}i & -1\\i & 1\end{bmatrix}$ which is $ \begin{bmatrix}0.5 & 0.5i\\0.5 & -0.5i\end{bmatrix}$ . It's easy to find out that the square roots of $3+4i$ are $2+i,-2-i$ , and of $3-4i$ are $2-i,-2+i$ . So now I would arrive at my 4 solutions: $$ \begin{bmatrix}1 & 1\\-i & i\end{bmatrix} \begin{bmatrix}2+i & 0\\0 & 2-i\end{bmatrix} \begin{bmatrix}0.5 & 0.5i\\0.5 & -0.5i\end{bmatrix}$$ $$ \begin{bmatrix}1 & 1\\-i & i\end{bmatrix} \begin{bmatrix}2+i & 0\\0 & -2+i\end{bmatrix} \begin{bmatrix}0.5 & 0.5i\\0.5 & -0.5i\end{bmatrix}$$ $$ \begin{bmatrix}1 & 1\\-i & i\end{bmatrix} \begin{bmatrix}-2-i & 0\\0 & 2-i\end{bmatrix} \begin{bmatrix}0.5 & 0.5i\\0.5 & -0.5i\end{bmatrix}$$ $$ \begin{bmatrix}1 & 1\\-i & i\end{bmatrix} \begin{bmatrix}-2-i & 0\\0 & -2+i\end{bmatrix} \begin{bmatrix}0.5 & 0.5i\\0.5 & -0.5i\end{bmatrix}$$ When I work out my first candidate, I get a matrix, but when squared it doesn't give me $ \begin{bmatrix}3 & -4\\4 & 3\end{bmatrix}$ and so something isn't right. I don't know what I did wrong and ANY help from the math community is greatly appreciated. I feel that my line of thinking is correct, where did I go wrong? And does anyone happen to know online software that can do matrix multiplications with complex entries, because the TI doesn't work","This question is about finding the 4(!!) square roots of a 2 by 2 matrix. The method I use is through Diagonalization (or actually doing the ""opposite""). I have no problem finding the four roots of a 2 by 2 matrix that has real eigenvalues (and thus real eigen vectors). Heck, I found 8 roots of a 3 by 3 matrix that has real eigen values. The problem I am running into is if the to be square rooted matrix has complex eigenvalues. In my view, the method should work the same, but somehow the algebra is letting me down. Example: Find the four square roots of . Note, I found this matrix by squaring , so obviously this is one root as well as taking opposite of its entries. So here is my work: The characteristic equation of is from which we find and . An eigenvector for is and an eigenvector for is . So the given matrix can now be diagonalized: = . I found = which is . It's easy to find out that the square roots of are , and of are . So now I would arrive at my 4 solutions: When I work out my first candidate, I get a matrix, but when squared it doesn't give me and so something isn't right. I don't know what I did wrong and ANY help from the math community is greatly appreciated. I feel that my line of thinking is correct, where did I go wrong? And does anyone happen to know online software that can do matrix multiplications with complex entries, because the TI doesn't work"," \begin{bmatrix}3 & -4\\4 & 3\end{bmatrix}  \begin{bmatrix}2 & -1\\1 & 2\end{bmatrix}  \begin{bmatrix}3 & -4\\4 & 3\end{bmatrix} (3-\lambda)^2+16=0 \lambda=3+4i \lambda=3-4i \lambda=3+4i  \begin{bmatrix}1 \\-i  \end{bmatrix} \lambda=3-4i  \begin{bmatrix}1 \\i  \end{bmatrix}  \begin{bmatrix}3 & -4\\4 & 3\end{bmatrix}  \begin{bmatrix}1 & 1\\-i & i\end{bmatrix} \begin{bmatrix}3+4i & 0\\0 & 3-4i\end{bmatrix} \begin{bmatrix}1 & 1\\-i & i\end{bmatrix}^{-1}  \begin{bmatrix}1 & 1\\-i & i\end{bmatrix}^{-1} \frac{1}{2i} \begin{bmatrix}i & -1\\i & 1\end{bmatrix}  \begin{bmatrix}0.5 & 0.5i\\0.5 & -0.5i\end{bmatrix} 3+4i 2+i,-2-i 3-4i 2-i,-2+i  \begin{bmatrix}1 & 1\\-i & i\end{bmatrix} \begin{bmatrix}2+i & 0\\0 & 2-i\end{bmatrix} \begin{bmatrix}0.5 & 0.5i\\0.5 & -0.5i\end{bmatrix}  \begin{bmatrix}1 & 1\\-i & i\end{bmatrix} \begin{bmatrix}2+i & 0\\0 & -2+i\end{bmatrix} \begin{bmatrix}0.5 & 0.5i\\0.5 & -0.5i\end{bmatrix}  \begin{bmatrix}1 & 1\\-i & i\end{bmatrix} \begin{bmatrix}-2-i & 0\\0 & 2-i\end{bmatrix} \begin{bmatrix}0.5 & 0.5i\\0.5 & -0.5i\end{bmatrix}  \begin{bmatrix}1 & 1\\-i & i\end{bmatrix} \begin{bmatrix}-2-i & 0\\0 & -2+i\end{bmatrix} \begin{bmatrix}0.5 & 0.5i\\0.5 & -0.5i\end{bmatrix}  \begin{bmatrix}3 & -4\\4 & 3\end{bmatrix}","['linear-algebra', 'matrices', 'diagonalization']"
92,Find the linear transformation of a matrix knowing 4 linear transformations,Find the linear transformation of a matrix knowing 4 linear transformations,,"I'm stuck with an exercise where they give me 4 linear transformations $T:M_{2\times2}\to\Bbb R$ for 4 matrices 2x2 and then they ask me for the linear transformation of a fifth matrix. $$ T   \begin{pmatrix}     1 &0\\     0 & 0\\   \end{pmatrix} =3, T\begin{pmatrix}     0 &1\\     1 & 0\\   \end{pmatrix} =-1, T   \begin{pmatrix}     1 &0\\    1 & 0\\   \end{pmatrix} =0, T   \begin{pmatrix}     0 &0\\     0 & 1\\   \end{pmatrix} =0, T   \begin{pmatrix}     a &b\\     c & d\\   \end{pmatrix} =?$$ and I have worked with this situation for $T:\Bbb R^n \to\Bbb R^m$ but never with matrices so I have the doubt that how is supposed to find 4 constants that will multiply de 4 matrices I know to compose the fifth matrix to finally can find the linear transformation of this last one. (I have tried adding up the 4 matrices as follows) $$    \begin{pmatrix}     1 &0\\     0 & 0\\   \end{pmatrix} + \begin{pmatrix}     0 &1\\     1 & 0\\   \end{pmatrix} +   \begin{pmatrix}     1 &0\\    1 & 0\\   \end{pmatrix} +   \begin{pmatrix}     0 &0\\     0 & 1\\   \end{pmatrix}=   \begin{pmatrix}     a &b\\     c & d\\   \end{pmatrix} $$ \begin{cases} 2=a  &  \\ 1=b & \\ 2=c\\ 1=d\\ \end{cases} and then use the coefficients $$    2\begin{pmatrix}     1 &0\\     0 & 0\\   \end{pmatrix} + 1\begin{pmatrix}     0 &1\\     1 & 0\\   \end{pmatrix} +   2\begin{pmatrix}     1 &0\\    1 & 0\\   \end{pmatrix} +   1\begin{pmatrix}     0 &0\\     0 & 1\\   \end{pmatrix}=   \begin{pmatrix}     a &b\\     c & d\\   \end{pmatrix} $$ and I just want to know if I continue in this way or I have committed any mistakes, thanks for your time <3","I'm stuck with an exercise where they give me 4 linear transformations for 4 matrices 2x2 and then they ask me for the linear transformation of a fifth matrix. and I have worked with this situation for but never with matrices so I have the doubt that how is supposed to find 4 constants that will multiply de 4 matrices I know to compose the fifth matrix to finally can find the linear transformation of this last one. (I have tried adding up the 4 matrices as follows) and then use the coefficients and I just want to know if I continue in this way or I have committed any mistakes, thanks for your time <3","T:M_{2\times2}\to\Bbb R  T
  \begin{pmatrix}
    1 &0\\
    0 & 0\\
  \end{pmatrix}
=3,
T\begin{pmatrix}
    0 &1\\
    1 & 0\\
  \end{pmatrix}
=-1,
T
  \begin{pmatrix}
    1 &0\\
   1 & 0\\
  \end{pmatrix}
=0,
T
  \begin{pmatrix}
    0 &0\\
    0 & 1\\
  \end{pmatrix}
=0,
T
  \begin{pmatrix}
    a &b\\
    c & d\\
  \end{pmatrix}
=? T:\Bbb R^n \to\Bbb R^m  
  \begin{pmatrix}
    1 &0\\
    0 & 0\\
  \end{pmatrix}
+
\begin{pmatrix}
    0 &1\\
    1 & 0\\
  \end{pmatrix}
+
  \begin{pmatrix}
    1 &0\\
   1 & 0\\
  \end{pmatrix}
+
  \begin{pmatrix}
    0 &0\\
    0 & 1\\
  \end{pmatrix}=
  \begin{pmatrix}
    a &b\\
    c & d\\
  \end{pmatrix}
 \begin{cases}
2=a  &  \\
1=b & \\
2=c\\
1=d\\
\end{cases}  
  2\begin{pmatrix}
    1 &0\\
    0 & 0\\
  \end{pmatrix}
+
1\begin{pmatrix}
    0 &1\\
    1 & 0\\
  \end{pmatrix}
+
  2\begin{pmatrix}
    1 &0\\
   1 & 0\\
  \end{pmatrix}
+
  1\begin{pmatrix}
    0 &0\\
    0 & 1\\
  \end{pmatrix}=
  \begin{pmatrix}
    a &b\\
    c & d\\
  \end{pmatrix}
","['linear-algebra', 'matrices', 'linear-transformations']"
93,Does conjugation preserve trace? [closed],Does conjugation preserve trace? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question If I were to conjugate by a matrix, what kind of matrix would it have to be in order to preserve trace? Does the trace of a matrix have anything to do with its spectrum? Any insights are appreciated, I'm just a bit confused.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question If I were to conjugate by a matrix, what kind of matrix would it have to be in order to preserve trace? Does the trace of a matrix have anything to do with its spectrum? Any insights are appreciated, I'm just a bit confused.",,"['linear-algebra', 'matrices', 'trace']"
94,Infinity norm of the inverse of a matrix with integer coefficients,Infinity norm of the inverse of a matrix with integer coefficients,,Let $A$ be a $k$ -dimensional non singualar matrix with integer coefficients. Is it true that $\|A^{-1}\|_\infty \leq 1$ ? How can I show that? Could you give me a counterexample?It is clear that $\|A^{-1}\|_{\infty}=\frac{1}{\min\{\|Ax\|_{\infty}:\|x\|_{\infty}=1\}}$ . My idea is to show that the minimum is obtained on an integer point so the denominator is bigger than $1$ . Is mi idea right? Thank you very much!,Let be a -dimensional non singualar matrix with integer coefficients. Is it true that ? How can I show that? Could you give me a counterexample?It is clear that . My idea is to show that the minimum is obtained on an integer point so the denominator is bigger than . Is mi idea right? Thank you very much!,A k \|A^{-1}\|_\infty \leq 1 \|A^{-1}\|_{\infty}=\frac{1}{\min\{\|Ax\|_{\infty}:\|x\|_{\infty}=1\}} 1,"['matrices', 'inequality', 'normed-spaces', 'numerical-linear-algebra']"
95,Power of sums of matrices,Power of sums of matrices,,"What is the theory behind the following expression, where A + B represents square matrices nxn I know how it is done in algebra but how to arrive at this expression? Obviously binomial theorem does not hold..","What is the theory behind the following expression, where A + B represents square matrices nxn I know how it is done in algebra but how to arrive at this expression? Obviously binomial theorem does not hold..",,"['matrices', 'power-series']"
96,Determining whether a subset is a subspace,Determining whether a subset is a subspace,,"The problems are as follows: Determine whether the following subsets of $\mathbb{R}^3$ are subspaces of $\mathbb{R}^3$ . $A = \{(u^2, v^2, w^2) \,|\, u, v, w \in \mathbb{R} \}$ , $B = \left\{(a, b, c) \,|\, \begin{pmatrix} a & b & c\\ 1 & 2 & 0\\ 0 & 1 & 2 \end{pmatrix} \text{is not invertible}\right\}$ , $C = \left\{(x, y, z) \,|\, \begin{pmatrix} 1 & 2 & 3\\ 4 & 5 & 6\\ 7 & 8 & 9 \end{pmatrix} \begin{pmatrix} x\\ y\\ z \end{pmatrix} = \begin{pmatrix} 2x\\ 2y\\ 2z \end{pmatrix}\right\}$ . Here's what I've tried so far: $A$ is a subspace of $\mathbb{R}^3$ as it contains the $0$ vector (?). The matrix is not invertible, meaning that the determinant is equal to $0$ . With this in mind, computing the determinant of the matrix yields $4a - 2b + c = 0$ . The original subset can thus be represented as $B = \left\{\left(\frac{2s - t}{4}, s, t\right) \,|\, s, t \in \mathbb{R}\right\}$ ; i.e. $B = \text{span}\left\{(\frac{1}{2}, 1, 0), (-\frac{1}{4}, 0, 1)\right\}$ , a plane in $\mathbb{R}^3$ . Solving for the linear system, $$ \begin{aligned} \begin{pmatrix} 1 & 2 & 3\\ 4 & 5 & 6\\ 7 & 8 & 9 \end{pmatrix} \begin{pmatrix} x\\ y\\ z \end{pmatrix} &= \begin{pmatrix} 2x\\ 2y\\ 2z \end{pmatrix}\\ x \begin{pmatrix} 1\\ 4\\ 7 \end{pmatrix} + y \begin{pmatrix} 2\\ 5\\ 8 \end{pmatrix} + z \begin{pmatrix} 3\\ 6\\ 9 \end{pmatrix} &=  \begin{pmatrix} 2x\\ 2y\\ 2z \end{pmatrix}\\ x \begin{pmatrix} -1\\ 4\\ 7 \end{pmatrix} + y \begin{pmatrix} 2\\ 3\\ 8 \end{pmatrix} + z \begin{pmatrix} 3\\ 6\\ 7 \end{pmatrix} &=  \begin{pmatrix} 0\\ 0\\ 0 \end{pmatrix} \end{aligned}$$ Converting to row echelon form gives the trivial solution $x = 0, y = 0, \text{ and } z = 0$ ; $C$ only contains the $0$ vector. Is my reasoning correct? Also, quite a bit of the computations have been omitted in this post for brevity. Regardless of this, are my answers well justified? Thank you.","The problems are as follows: Determine whether the following subsets of are subspaces of . , , . Here's what I've tried so far: is a subspace of as it contains the vector (?). The matrix is not invertible, meaning that the determinant is equal to . With this in mind, computing the determinant of the matrix yields . The original subset can thus be represented as ; i.e. , a plane in . Solving for the linear system, Converting to row echelon form gives the trivial solution ; only contains the vector. Is my reasoning correct? Also, quite a bit of the computations have been omitted in this post for brevity. Regardless of this, are my answers well justified? Thank you.","\mathbb{R}^3 \mathbb{R}^3 A = \{(u^2, v^2, w^2) \,|\, u, v, w \in \mathbb{R} \} B = \left\{(a, b, c) \,|\,
\begin{pmatrix}
a & b & c\\
1 & 2 & 0\\
0 & 1 & 2
\end{pmatrix} \text{is not invertible}\right\} C = \left\{(x, y, z) \,|\, \begin{pmatrix}
1 & 2 & 3\\
4 & 5 & 6\\
7 & 8 & 9
\end{pmatrix}
\begin{pmatrix}
x\\
y\\
z
\end{pmatrix} =
\begin{pmatrix}
2x\\
2y\\
2z
\end{pmatrix}\right\} A \mathbb{R}^3 0 0 4a - 2b + c = 0 B = \left\{\left(\frac{2s - t}{4}, s, t\right) \,|\, s, t \in \mathbb{R}\right\} B = \text{span}\left\{(\frac{1}{2}, 1, 0), (-\frac{1}{4}, 0, 1)\right\} \mathbb{R}^3 
\begin{aligned}
\begin{pmatrix}
1 & 2 & 3\\
4 & 5 & 6\\
7 & 8 & 9
\end{pmatrix}
\begin{pmatrix}
x\\
y\\
z
\end{pmatrix} &=
\begin{pmatrix}
2x\\
2y\\
2z
\end{pmatrix}\\
x
\begin{pmatrix}
1\\
4\\
7
\end{pmatrix}
+ y
\begin{pmatrix}
2\\
5\\
8
\end{pmatrix}
+ z
\begin{pmatrix}
3\\
6\\
9
\end{pmatrix}
&= 
\begin{pmatrix}
2x\\
2y\\
2z
\end{pmatrix}\\
x
\begin{pmatrix}
-1\\
4\\
7
\end{pmatrix}
+ y
\begin{pmatrix}
2\\
3\\
8
\end{pmatrix}
+ z
\begin{pmatrix}
3\\
6\\
7
\end{pmatrix}
&= 
\begin{pmatrix}
0\\
0\\
0
\end{pmatrix}
\end{aligned} x = 0, y = 0, \text{ and } z = 0 C 0","['linear-algebra', 'matrices']"
97,Can we prove that $\operatorname{Tr}(ABC) = \operatorname{Tr}(CBA)$?,Can we prove that ?,\operatorname{Tr}(ABC) = \operatorname{Tr}(CBA),"I would like to verify the claim: $$\operatorname{Tr}(ABC) = \operatorname{Tr}(CBA)$$ I tried verifying through an example: Given the following $3$ different matrices: \begin{align} A & = \begin{pmatrix} 1 & 2 & 3 & 4 \\ 5 & 6 & 7 & 8 \\ 9 & 10 & 11 & 12 \\ 13 & 14 & 15 & 16 \end{pmatrix}\\[2ex] B & = \begin{pmatrix} 4 & 3 & 2 & 1 \\ 3 & 6 & 4 & 2 \\ 2 & 4 & 6 & 3 \\ 1 & 2 & 3 & 4 \end{pmatrix}\\[2ex] C & = \begin{pmatrix} 4 & 3 & 2 & 1 \\ 5 & 6 & 7 & 8 \\ 8 & 7 & 6 & 5 \\ 4 & 3 & 2 & 1 \end{pmatrix} \end{align} I calculated $\operatorname{Tr}(ABC) = 7930$ and $\operatorname{Tr}(CBA) = 7510$ . Is there any thing wrong in my calculation, or does this prove that $\operatorname{Tr}(ABC) \neq \operatorname{Tr}(CBA)$ ? Many thanks!","I would like to verify the claim: I tried verifying through an example: Given the following different matrices: I calculated and . Is there any thing wrong in my calculation, or does this prove that ? Many thanks!","\operatorname{Tr}(ABC) = \operatorname{Tr}(CBA) 3 \begin{align}
A & = \begin{pmatrix}
1 & 2 & 3 & 4 \\
5 & 6 & 7 & 8 \\
9 & 10 & 11 & 12 \\
13 & 14 & 15 & 16
\end{pmatrix}\\[2ex]
B & = \begin{pmatrix}
4 & 3 & 2 & 1 \\
3 & 6 & 4 & 2 \\
2 & 4 & 6 & 3 \\
1 & 2 & 3 & 4
\end{pmatrix}\\[2ex]
C & = \begin{pmatrix}
4 & 3 & 2 & 1 \\
5 & 6 & 7 & 8 \\
8 & 7 & 6 & 5 \\
4 & 3 & 2 & 1
\end{pmatrix}
\end{align} \operatorname{Tr}(ABC) = 7930 \operatorname{Tr}(CBA) = 7510 \operatorname{Tr}(ABC) \neq \operatorname{Tr}(CBA)","['linear-algebra', 'matrices', 'trace']"
98,Is there a quick method to calculate the eigenvalues of this complex $4 \times 4$ matrix?,Is there a quick method to calculate the eigenvalues of this complex  matrix?,4 \times 4,"I want to calculate the eigenvalues of a matrix given below. $A=\begin{pmatrix}1&1&1&1 \\ 1 & i & -1 & -i \\ 1 & -1 & 1 & -1 \\ 1 & -i &-1 &i\end{pmatrix}$ We can calculate $\det{(A-tE)}$ by the Laplace expansion(or cofactor expansion), but that method takes too much time for me. Since this question is from an exam question, I need a quick(and preferably easy) method to calculate the eigenvalues. Of course, I have seen these questions: Eigenvalues for $4\times 4$ matrix Quick method for finding eigenvalues and eigenvectors in a symmetric $5 \times 5$ matrix? However, the answers there do not seem to be applicable to our matrix. Is there any method that can be applied to our matrix?","I want to calculate the eigenvalues of a matrix given below. We can calculate by the Laplace expansion(or cofactor expansion), but that method takes too much time for me. Since this question is from an exam question, I need a quick(and preferably easy) method to calculate the eigenvalues. Of course, I have seen these questions: Eigenvalues for $4\times 4$ matrix Quick method for finding eigenvalues and eigenvectors in a symmetric $5 \times 5$ matrix? However, the answers there do not seem to be applicable to our matrix. Is there any method that can be applied to our matrix?",A=\begin{pmatrix}1&1&1&1 \\ 1 & i & -1 & -i \\ 1 & -1 & 1 & -1 \\ 1 & -i &-1 &i\end{pmatrix} \det{(A-tE)},"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
99,For which $n \in N$ is the following matrix invertible?,For which  is the following matrix invertible?,n \in N,"For which $n \in N$ is the following matrix invertible? $$\left[\begin{array}{[c c c]}  10^{30}+5 & 10^{20}+4 & 10^{20}+6 \\ 10^{4}+2 & 10^{8}+7 & 10^{10}+2n \\ 10^{4}+8 & 10^{6}+4 & 10^{15}+9 \\ \end{array}\right]$$ My attempt : For the matrix to be invertible, it must be non-singular. To compute the determinant, I split the large determinant into smaller determinants using the column addition property but it got too tedious to compute (and too lengthy to type out here :P) The answer : Replacing even numbers by zero and odd numbers by one, we have $$|A| = \left| \begin{array}{c c c} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{array}\right|$$ which is an odd number and hence $|A|$ can not be zero. Hence A is invertible for all $n \in N$ . I did not understand how all the odd numbers were simply replaced with 1 and the even numbers with 0. I would appreciate it if someone could provide a different answer or explain the given answer.","For which is the following matrix invertible? My attempt : For the matrix to be invertible, it must be non-singular. To compute the determinant, I split the large determinant into smaller determinants using the column addition property but it got too tedious to compute (and too lengthy to type out here :P) The answer : Replacing even numbers by zero and odd numbers by one, we have which is an odd number and hence can not be zero. Hence A is invertible for all . I did not understand how all the odd numbers were simply replaced with 1 and the even numbers with 0. I would appreciate it if someone could provide a different answer or explain the given answer.","n \in N \left[\begin{array}{[c c c]} 
10^{30}+5 & 10^{20}+4 & 10^{20}+6 \\
10^{4}+2 & 10^{8}+7 & 10^{10}+2n \\
10^{4}+8 & 10^{6}+4 & 10^{15}+9 \\
\end{array}\right] |A| = \left| \begin{array}{c c c} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{array}\right| |A| n \in N","['linear-algebra', 'matrices', 'determinant']"
