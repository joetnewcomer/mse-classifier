,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Differentiability of a Uniformly Convergent Series,Differentiability of a Uniformly Convergent Series,,"I'm currently reviewing some elementary analysis (it's been a while), and I recall the tremendous significance of uniform convergence of a sequence of functions. If memory serves (though I can't find the exact theorem in baby Rudin), the following holds: Given $A, B \subseteq \mathbb{R}$. Given \begin{align} (1) \: \:f_k: A \rightarrow B, & \text{ differentiable on }A \\                (2) \: \: F_n : A \rightarrow B  & \text{ by }   \\                    & F_n(x) = \sum_{k=1}^{n}f_k(x)\\               (3)\:\:F: A \rightarrow B & \text{ with } \\                    & F(x) = \lim_{n \rightarrow \infty}F_n(x)   , \:\forall\: x\in A.\end{align} If $F_n$ converges uniformly to $F$ on A, then $F$ is differentiable on $A$, and \begin{align} F'(x) =  & \sum_{k=1}^{\infty}f_k'(x). \\                \end{align} I'm looking for an example that illustrates when this doesn't hold; in particular, I'm trying to find $f_k, A,$ and $B$ such that the above $(1), \:(2),$ and $(3)$ hold, however $F_n$ does not converge uniformly to $F$ on $A$, and as a result \begin{align} F'(x) \neq  & \sum_{k=1}^{\infty}f_k'(x). \\                \end{align} I've tried a few sequences of functions that I know are (and that I can show to be) pointwise convergent and not uniformly so, but I'm struggling to come up with a series of such functions. Any hints would be greatly appreciated. Also, this is not homework.  It is merely for my own edification.","I'm currently reviewing some elementary analysis (it's been a while), and I recall the tremendous significance of uniform convergence of a sequence of functions. If memory serves (though I can't find the exact theorem in baby Rudin), the following holds: Given $A, B \subseteq \mathbb{R}$. Given \begin{align} (1) \: \:f_k: A \rightarrow B, & \text{ differentiable on }A \\                (2) \: \: F_n : A \rightarrow B  & \text{ by }   \\                    & F_n(x) = \sum_{k=1}^{n}f_k(x)\\               (3)\:\:F: A \rightarrow B & \text{ with } \\                    & F(x) = \lim_{n \rightarrow \infty}F_n(x)   , \:\forall\: x\in A.\end{align} If $F_n$ converges uniformly to $F$ on A, then $F$ is differentiable on $A$, and \begin{align} F'(x) =  & \sum_{k=1}^{\infty}f_k'(x). \\                \end{align} I'm looking for an example that illustrates when this doesn't hold; in particular, I'm trying to find $f_k, A,$ and $B$ such that the above $(1), \:(2),$ and $(3)$ hold, however $F_n$ does not converge uniformly to $F$ on $A$, and as a result \begin{align} F'(x) \neq  & \sum_{k=1}^{\infty}f_k'(x). \\                \end{align} I've tried a few sequences of functions that I know are (and that I can show to be) pointwise convergent and not uniformly so, but I'm struggling to come up with a series of such functions. Any hints would be greatly appreciated. Also, this is not homework.  It is merely for my own edification.",,"['real-analysis', 'derivatives', 'uniform-convergence']"
1,Example of a measure on $\mathcal{P}(X)$,Example of a measure on,\mathcal{P}(X),Is there an example of a set $X$ and a measure $\mu$ on $\mathcal{P}(X)$ (collection of all the the subsets of $X$) such that $\mu(X)=1$ and $\mu(\{x\})=0$ for all $x$ in $X$? At first I thought it's obvious that there should be such a measure but then I couldn't find any. Now I have a feeling it has something to do with being countably additive. but I don't know why. Is there any lemma or theorem that shows there can't be any measure with this property?,Is there an example of a set $X$ and a measure $\mu$ on $\mathcal{P}(X)$ (collection of all the the subsets of $X$) such that $\mu(X)=1$ and $\mu(\{x\})=0$ for all $x$ in $X$? At first I thought it's obvious that there should be such a measure but then I couldn't find any. Now I have a feeling it has something to do with being countably additive. but I don't know why. Is there any lemma or theorem that shows there can't be any measure with this property?,,"['real-analysis', 'measure-theory']"
2,Baby Rudin Exercise 2.9: Do $E$ and $\bar{E}$ always have same interiors?,Baby Rudin Exercise 2.9: Do  and  always have same interiors?,E \bar{E},"The question asks: Do $E$ and $\bar E$ always have same interiors? Here, $\bar E$ denotes the closure of $E$. That is, $\bar E = E \cup E'$, where $E'$ denotes the set of limit points of $E$. Also, let $E^\circ$ denote the interior of $E$. My attempt at proof: Answer. Yes Clearly, $E^\circ \subseteq (\bar E)^\circ$. Therefore, we need to prove only that $(\bar E)^\circ \subseteq E^\circ$. If $(\bar E)^\circ$ is empty, then $E^\circ$ is also empty. Therefore, assume that $(\bar E)^\circ$ is non-empty. Consider a point $p \in (\overline E)^\circ$. For some $r > 0$, we have that for any $0 < r' \le r$, $N_{r'}(p) \subseteq \bar E$. Here, $N_a(p)$ denotes the (open) neighborhood of $p$. We need to show that $p \in E^\circ$. That is, there exists some $\rho > 0$ such that $N_\rho(p) \subseteq E$. Therefore, assume for contradiction that for all $\rho > 0$, there exists a point $p' \in N_\rho(p)$ such that $p' \not \in E$. In particular, for any $0 < \rho \le r$, there exists a $p' \in N_\rho(p)$, but $p' \not \in E$. Therefore, $N_\rho(p) \subseteq E'$. In particular, $p \in E'$, i.e. $p$ is a limit point of $E'$. However, this is a contradiction because for every neighborhood of a  limit point of $E$ must contain a point of $E$. But the result is not true as shown by the following example. $E = \mathbb Q \subset \mathbb R$. Here, $\mathbb Q^\circ = \emptyset$, but $\bar{\mathbb{Q}} = \mathbb R$. So what is wrong with my proof? I thought for a while (even considering the same example!), and I can't find my mistake.","The question asks: Do $E$ and $\bar E$ always have same interiors? Here, $\bar E$ denotes the closure of $E$. That is, $\bar E = E \cup E'$, where $E'$ denotes the set of limit points of $E$. Also, let $E^\circ$ denote the interior of $E$. My attempt at proof: Answer. Yes Clearly, $E^\circ \subseteq (\bar E)^\circ$. Therefore, we need to prove only that $(\bar E)^\circ \subseteq E^\circ$. If $(\bar E)^\circ$ is empty, then $E^\circ$ is also empty. Therefore, assume that $(\bar E)^\circ$ is non-empty. Consider a point $p \in (\overline E)^\circ$. For some $r > 0$, we have that for any $0 < r' \le r$, $N_{r'}(p) \subseteq \bar E$. Here, $N_a(p)$ denotes the (open) neighborhood of $p$. We need to show that $p \in E^\circ$. That is, there exists some $\rho > 0$ such that $N_\rho(p) \subseteq E$. Therefore, assume for contradiction that for all $\rho > 0$, there exists a point $p' \in N_\rho(p)$ such that $p' \not \in E$. In particular, for any $0 < \rho \le r$, there exists a $p' \in N_\rho(p)$, but $p' \not \in E$. Therefore, $N_\rho(p) \subseteq E'$. In particular, $p \in E'$, i.e. $p$ is a limit point of $E'$. However, this is a contradiction because for every neighborhood of a  limit point of $E$ must contain a point of $E$. But the result is not true as shown by the following example. $E = \mathbb Q \subset \mathbb R$. Here, $\mathbb Q^\circ = \emptyset$, but $\bar{\mathbb{Q}} = \mathbb R$. So what is wrong with my proof? I thought for a while (even considering the same example!), and I can't find my mistake.",,"['real-analysis', 'self-learning', 'real-numbers', 'fake-proofs']"
3,$f^{(n)}(0)=0$ and $|f^{(n)}(x)|\leq n!$ implies $f=0$,and  implies,f^{(n)}(0)=0 |f^{(n)}(x)|\leq n! f=0,"Let $f$ be a $C^\infty$ function such that $f^{(n)}(0)=0$ and $|f^{(n)}(x)|\leq n!$ for all integer $n$ and real $x$. How to prove that $f(x)=0$ for all real $x$. I proved this only for $x\in(-1,1)$ using Taylor-Lagrange Formula by showing that $$|f(x)|\leq x^{n+1}$$ for all integer $n$ and real $x$.","Let $f$ be a $C^\infty$ function such that $f^{(n)}(0)=0$ and $|f^{(n)}(x)|\leq n!$ for all integer $n$ and real $x$. How to prove that $f(x)=0$ for all real $x$. I proved this only for $x\in(-1,1)$ using Taylor-Lagrange Formula by showing that $$|f(x)|\leq x^{n+1}$$ for all integer $n$ and real $x$.",,"['real-analysis', 'derivatives', 'taylor-expansion']"
4,"The limit of the nth root of a to the n plus b to the n is the maximum of (a,b) [duplicate]","The limit of the nth root of a to the n plus b to the n is the maximum of (a,b) [duplicate]",,"This question already has answers here : How to evaluate $ \lim_{x\rightarrow +\infty } \sqrt[x]{a^x+b^x} = ? $ (3 answers) Closed 7 years ago . I've been asked to prove the following from Spivak's Calculus $$\lim_{n\to\infty}\sqrt[n]{a^n+b^n}=\max(a,b); a,b > 0$$ I understand that this is a proof by cases, and that our cases are $a=b$, $a>b$, and $b>a$. I have done the $a=b$ case, but I am stuck on the $a>b$ and $b>a$ cases. Some hints would be appreciated- Thanks!","This question already has answers here : How to evaluate $ \lim_{x\rightarrow +\infty } \sqrt[x]{a^x+b^x} = ? $ (3 answers) Closed 7 years ago . I've been asked to prove the following from Spivak's Calculus $$\lim_{n\to\infty}\sqrt[n]{a^n+b^n}=\max(a,b); a,b > 0$$ I understand that this is a proof by cases, and that our cases are $a=b$, $a>b$, and $b>a$. I have done the $a=b$ case, but I am stuck on the $a>b$ and $b>a$ cases. Some hints would be appreciated- Thanks!",,"['real-analysis', 'limits', 'proof-writing']"
5,Uniformly Convergent Subsequence,Uniformly Convergent Subsequence,,"This is almost surely an Arzela-Ascoli question, since it comes from an old exam of which such problems are quite common. Unfortunately, I can't seem to get it though. $\{ f_n \}$ is a sequence of functions $[0,1] \to \mathbb{R}$ satisfying $|f_n'(x)| \leq \frac{1 + |\ln (x)|}{\sqrt{x}}$ and $-10 \leq \int_0^1 f_n(x) dx \leq 10$ for every $n$. The question is to show the existence of a uniformly convergent subsequence. In these kinds of situations, the bounds given usually turn into something nice  to force uniform boundedness. Maybe I could hope to establish equicontinuity using the integral somehow, but I'm not told that the $f_n$ are always positive, so the absolute values would probably screw things up anyway. Is it an easy problem I'm missing or is this totally the wrong track? If so, what's the right track?","This is almost surely an Arzela-Ascoli question, since it comes from an old exam of which such problems are quite common. Unfortunately, I can't seem to get it though. $\{ f_n \}$ is a sequence of functions $[0,1] \to \mathbb{R}$ satisfying $|f_n'(x)| \leq \frac{1 + |\ln (x)|}{\sqrt{x}}$ and $-10 \leq \int_0^1 f_n(x) dx \leq 10$ for every $n$. The question is to show the existence of a uniformly convergent subsequence. In these kinds of situations, the bounds given usually turn into something nice  to force uniform boundedness. Maybe I could hope to establish equicontinuity using the integral somehow, but I'm not told that the $f_n$ are always positive, so the absolute values would probably screw things up anyway. Is it an easy problem I'm missing or is this totally the wrong track? If so, what's the right track?",,['real-analysis']
6,What is $\sup\limits_{n \in \mathbb{N}} \sum\limits_{k=0}^n \cos(k)$?,What is ?,\sup\limits_{n \in \mathbb{N}} \sum\limits_{k=0}^n \cos(k),"For the sake of curiosity, does anyone know what $\sup\limits_{n \in \mathbb{N}} \sum\limits_{k=0}^n \cos(k)$ or $\sup\limits_{n \in \mathbb{N}} \sum\limits_{k=0}^n \sin(k)$ are? It is pretty easy to see that both partial sums are bounded, but I don't even have a clue as to what their respective supremums should be. Definitely something not too large.","For the sake of curiosity, does anyone know what $\sup\limits_{n \in \mathbb{N}} \sum\limits_{k=0}^n \cos(k)$ or $\sup\limits_{n \in \mathbb{N}} \sum\limits_{k=0}^n \sin(k)$ are? It is pretty easy to see that both partial sums are bounded, but I don't even have a clue as to what their respective supremums should be. Definitely something not too large.",,"['real-analysis', 'number-theory']"
7,"Quantify how small are the integrals $\int_0^N e^{-Nx}\binom{x}{N}dx $, as $N\to\infty$","Quantify how small are the integrals , as",\int_0^N e^{-Nx}\binom{x}{N}dx  N\to\infty,"While I was reading the Wikipedia entry for Gregory coefficients I've thought that should be very nice and fun calculate definite integrals involving binomial coefficients. This is a simple exercise that I've thought after I did some experiments using Wolfram Alpha online calculator with codes like these: integrate   e^(-35 x) Binomial[x,35] dx, from x=0 to x=35 integrate   e^(-200 x) Binomial[x,200] dx, from x=0 to x=200 I believe that the absolute value of integrals is small . Question. (Being $N\geq 1$ integer) I would like to know how to quantify how small are these integrals. Does exist    $$\lim_{N\to\infty}\int_0^N e^{-Nx}\binom{x}{N}dx?$$ Alternatively, quantify $$ \left| \int_0^N e^{-Nx}\binom{x}{N}dx \right|$$ as $N$ tends to infinite. Thanks in advance.","While I was reading the Wikipedia entry for Gregory coefficients I've thought that should be very nice and fun calculate definite integrals involving binomial coefficients. This is a simple exercise that I've thought after I did some experiments using Wolfram Alpha online calculator with codes like these: integrate   e^(-35 x) Binomial[x,35] dx, from x=0 to x=35 integrate   e^(-200 x) Binomial[x,200] dx, from x=0 to x=200 I believe that the absolute value of integrals is small . Question. (Being $N\geq 1$ integer) I would like to know how to quantify how small are these integrals. Does exist    $$\lim_{N\to\infty}\int_0^N e^{-Nx}\binom{x}{N}dx?$$ Alternatively, quantify $$ \left| \int_0^N e^{-Nx}\binom{x}{N}dx \right|$$ as $N$ tends to infinite. Thanks in advance.",,"['real-analysis', 'integration']"
8,"Prove that $x,y$ are positive real, $x/y$ is irrational, then the set $\{ mx+ny:m,n\in \Bbb Z \}$ .is dense in $\Bbb R$","Prove that  are positive real,  is irrational, then the set  .is dense in","x,y x/y \{ mx+ny:m,n\in \Bbb Z \} \Bbb R","Anyone can help with the following problem, Prove that if $x,y$ are positive real, $x/y$ is irrational, then the set $\{ mx+ny:m,n\in \Bbb Z \}$ is dense in $\Bbb R$. Thanks a lot!","Anyone can help with the following problem, Prove that if $x,y$ are positive real, $x/y$ is irrational, then the set $\{ mx+ny:m,n\in \Bbb Z \}$ is dense in $\Bbb R$. Thanks a lot!",,['real-analysis']
9,"If $f(x)\leq g(x)$ for all $x\in [a,b]$ then $\displaystyle \int_a^b f(x)dx\leq \int_a^b g(x)dx$ [closed]",If  for all  then  [closed],"f(x)\leq g(x) x\in [a,b] \displaystyle \int_a^b f(x)dx\leq \int_a^b g(x)dx","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question I'm having trouble constructing a proof for the following theorem: Let $f$ and $g$ be Riemann integrable functions on $[a,b]$ . Show that:  if $f(x)\leq g(x)$ for all $x\in [a,b]$ then: $$\int_a^b f(x)dx\leq \int_a^b g(x)dx$$ Any suggestions on how to start, or an answer would be appreciated!","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question I'm having trouble constructing a proof for the following theorem: Let and be Riemann integrable functions on . Show that:  if for all then: Any suggestions on how to start, or an answer would be appreciated!","f g [a,b] f(x)\leq g(x) x\in [a,b] \int_a^b f(x)dx\leq \int_a^b g(x)dx","['real-analysis', 'integration', 'solution-verification', 'riemann-integration']"
10,Non-negative function with non-negative compactly supported Fourier transform,Non-negative function with non-negative compactly supported Fourier transform,,"In a paper I'm trying to understand, the author claimed (without proof) the existence of a Schwartz function $\varphi \colon \mathbb{R} \rightarrow \mathbb{R}$ with the following properties: $0 \leq \varphi(x) \leq 2$ for every $x \in \mathbb{R}$, $\varphi(x) \geq 1$ for every $x \in [0,1]$, $\widehat{\varphi} (\xi) \geq 0$ for every $\xi \in \mathbb{R}$, $\widehat{\varphi}(\xi)$ is compactly supported on (say) $[-10,10]$. I tried two methods to prove this, but in both cases there's been a small thing I've been unable to show. Firstly, I took an appropriately scaled Gaussian $G$ that satisfied properties 1 and 2 (with a little leeway) in real space, and considered its Fourier transform $\widehat{G}$, which is another Gaussian. I then added a smooth ""remainder"" term $\widehat{r}$, defined by $$\widehat{r}(\xi) = \begin{cases} 0 & |\xi| \leq 9\\ \textrm{smooth in-between} \\ -\widehat{G}(\xi) & |\xi|> 10\end{cases}$$ and defined $\widehat{\varphi} = \widehat{G} + \widehat{r}$, hence $\varphi = G + r$. Since the $L^\infty$ norm of $r$ is bounded by the $L^1$ norm of $\widehat{r}$ (which we know to be very small), we know that $\varphi$ satisfies properties 2, 3, and 4, and that $\varphi\leq 2$. However, I can't seem to show that $\varphi\geq0$ (and I'm not 100% convinced that it's the case anyway). The second approach I tried was to take a smooth, non-negative, even function in Fourier space, $\widehat{\eta}$, that is supported on $[-5,5]$, so that $\widehat{F} = \widehat{\eta}\ast\widehat{\eta}$ is supported on $[-10,10]$. Then $F = \eta^2 \geq 0$, and by scaling $\widehat{F}$ appropriately, we have $F\leq 2$. Then, assuming that the maximum of $F$ occurs at $x=0$, we can ""stretch"" $F$ horizontally such that it is greater than 1 on the interval $[0,1]$. However, I'm not sure how to show that the maximum is indeed at $x=0$, or whether that is even the case. Any ideas on how to plug the hole in either of these proofs, or alternative proofs, would be most welcome.","In a paper I'm trying to understand, the author claimed (without proof) the existence of a Schwartz function $\varphi \colon \mathbb{R} \rightarrow \mathbb{R}$ with the following properties: $0 \leq \varphi(x) \leq 2$ for every $x \in \mathbb{R}$, $\varphi(x) \geq 1$ for every $x \in [0,1]$, $\widehat{\varphi} (\xi) \geq 0$ for every $\xi \in \mathbb{R}$, $\widehat{\varphi}(\xi)$ is compactly supported on (say) $[-10,10]$. I tried two methods to prove this, but in both cases there's been a small thing I've been unable to show. Firstly, I took an appropriately scaled Gaussian $G$ that satisfied properties 1 and 2 (with a little leeway) in real space, and considered its Fourier transform $\widehat{G}$, which is another Gaussian. I then added a smooth ""remainder"" term $\widehat{r}$, defined by $$\widehat{r}(\xi) = \begin{cases} 0 & |\xi| \leq 9\\ \textrm{smooth in-between} \\ -\widehat{G}(\xi) & |\xi|> 10\end{cases}$$ and defined $\widehat{\varphi} = \widehat{G} + \widehat{r}$, hence $\varphi = G + r$. Since the $L^\infty$ norm of $r$ is bounded by the $L^1$ norm of $\widehat{r}$ (which we know to be very small), we know that $\varphi$ satisfies properties 2, 3, and 4, and that $\varphi\leq 2$. However, I can't seem to show that $\varphi\geq0$ (and I'm not 100% convinced that it's the case anyway). The second approach I tried was to take a smooth, non-negative, even function in Fourier space, $\widehat{\eta}$, that is supported on $[-5,5]$, so that $\widehat{F} = \widehat{\eta}\ast\widehat{\eta}$ is supported on $[-10,10]$. Then $F = \eta^2 \geq 0$, and by scaling $\widehat{F}$ appropriately, we have $F\leq 2$. Then, assuming that the maximum of $F$ occurs at $x=0$, we can ""stretch"" $F$ horizontally such that it is greater than 1 on the interval $[0,1]$. However, I'm not sure how to show that the maximum is indeed at $x=0$, or whether that is even the case. Any ideas on how to plug the hole in either of these proofs, or alternative proofs, would be most welcome.",,"['real-analysis', 'fourier-analysis', 'fourier-transform']"
11,Is the norm topology the same as the initial topology generated by the norm function?,Is the norm topology the same as the initial topology generated by the norm function?,,"I have a question about the topology on a normed vector space. A normed vector space $(X, \| \cdot \|_X)$ naturally comes with a topology, known as the norm topology or strong topology on $X$, generated by the open balls $B (x, r) := \{y \in X : \|y-x\|_X < r\}$. Is this the same as the intial topology on $X$ generated by the norm function $\| \cdot \|_X : X \to \mathbb{R}$, i.e., the smallest topology that makes this norm function continuous? I thought yes. But then, I noticed one property of initial topology as follows: If $X$ has the initial topology generated by a family $\mathcal{F}$ of functions,then a net $\langle x_\alpha\rangle$ converges to $x \in X$ iff $\langle f(x_\alpha)\rangle$ converges to $f(x)$ for all $f \in \mathcal{F}$. This would imply that a sequence $\{ x_n :n \in \mathbb{N}\}$ converges to $x \in X$ iff $\| x_n\|$ converges to $\|x\|$, which in general I think is not true. Why am I getting this contradiction? Where did I go wrong? Thanks in advance for any help!","I have a question about the topology on a normed vector space. A normed vector space $(X, \| \cdot \|_X)$ naturally comes with a topology, known as the norm topology or strong topology on $X$, generated by the open balls $B (x, r) := \{y \in X : \|y-x\|_X < r\}$. Is this the same as the intial topology on $X$ generated by the norm function $\| \cdot \|_X : X \to \mathbb{R}$, i.e., the smallest topology that makes this norm function continuous? I thought yes. But then, I noticed one property of initial topology as follows: If $X$ has the initial topology generated by a family $\mathcal{F}$ of functions,then a net $\langle x_\alpha\rangle$ converges to $x \in X$ iff $\langle f(x_\alpha)\rangle$ converges to $f(x)$ for all $f \in \mathcal{F}$. This would imply that a sequence $\{ x_n :n \in \mathbb{N}\}$ converges to $x \in X$ iff $\| x_n\|$ converges to $\|x\|$, which in general I think is not true. Why am I getting this contradiction? Where did I go wrong? Thanks in advance for any help!",,"['real-analysis', 'general-topology', 'functional-analysis', 'normed-spaces']"
12,Maximum principle for harmonic functions,Maximum principle for harmonic functions,,"I know the following classical maximum principle for harmonic functions: If $\Omega \subset \mathbb{C}$ is open and connected and $u \in  C^2(\Omega)$ is harmonic, then $u$ has maximum (or minimum) in $\Omega$ $\implies$ $u$ constant. How can I prove that the theorem is true if the hypothesis is that $u$ has a local maximum (or minimum).","I know the following classical maximum principle for harmonic functions: If $\Omega \subset \mathbb{C}$ is open and connected and $u \in  C^2(\Omega)$ is harmonic, then $u$ has maximum (or minimum) in $\Omega$ $\implies$ $u$ constant. How can I prove that the theorem is true if the hypothesis is that $u$ has a local maximum (or minimum).",,['real-analysis']
13,A question about André Weil’s converse to Haar’s Theorem on the existence of Haar measures,A question about André Weil’s converse to Haar’s Theorem on the existence of Haar measures,,"Let $ (G,\cdot,e) $ be a group, and suppose that there are a $ \sigma $-ring $ \Sigma $ on $ G $ and a measure $ \mu: \Sigma \to [0,\infty] $, non-trivial, such that the following properties hold: $ \Sigma $ is left-invariant w.r.t. $ \cdot $, i.e., $ x \cdot S \in \Sigma $ for every $ x \in G $ and $ S \in \Sigma $. $ \mu $ is left-invariant w.r.t. $ \cdot $, i.e., $ \mu(x \cdot S) = \mu(S) $ for every $ x \in G $ and $ S \in \Sigma $. The map $   \left\{ \begin{matrix}   G \times G & \to & G \times G \\ (x,y) & \mapsto & (x,x \cdot y)   \end{matrix} \right\}   $ is $ (\Sigma \times \Sigma,\Sigma \times \Sigma) $-measurable. For each $ x \in G \setminus \{ e \} $, there exists an $ S \in \Sigma $ with $$   0 < \mu(S) < \infty \qquad \text{and} \qquad   0 < \mu((x \cdot S) \triangle S) < \infty,   $$ where $ \triangle $ denotes the symmetric difference of sets. Then Weil’s converse to Haar’s Theorem states that there exists a topological group $ ((G',\bullet,e),\tau) $ with the following properties: $ \tau $ is a locally compact and Hausdorff group topology on $ G' $. $ (G,\cdot,e) $ is a subgroup of $ (G',\bullet,e) $, so that $ G \subseteq G' $ and $ \cdot = \bullet|_{G \times G} $. If $ \mathscr{B} $ denotes the $ \sigma $-ring on $ G' $ generated by the $ G_{\delta} $ compact (w.r.t. $ \tau $) subsets of $ G' $, then $$   \{ B \cap G \in \mathcal{P}(G) \mid B \in \mathscr{B} \} \subseteq \Sigma.   $$ Note: We call $ \mathscr{B} $ the $ \tau $-induced Baire $ \sigma $-ring on $ G' $ . There exists a (Baire) Haar measure $ \mu': \mathscr{B} \to [0,\infty] $, associated with $ ((G',\bullet,e),\tau) $, such that $$   \forall B \in \mathscr{B}: \qquad   \mu(B \cap G) = \mu'(B).   $$ This implies that $ G $ is a $ \mu' $-thick subset of $ G' $, as $ B \in \mathscr{B} $ and $ B \cap G = \varnothing $ imply $ \mu'(B) = 0 $. The version of Weil’s result presented here is taken from Halmos’s Measure Theory , which is rather antiquated but still remains a classic. Now, I would like to determine if one can simply replace every instance of ‘$ \sigma $-ring’ by ‘$ \sigma $-algebra’, as well as replace all Baire $ \sigma $-rings by Borel $ \sigma $-algebras, i.e., $ \sigma $-algebras on a set that are generated by a given locally compact and Hausdorff topology. Could someone kindly provide an authoritative reference to aid my query? Thank you very much!","Let $ (G,\cdot,e) $ be a group, and suppose that there are a $ \sigma $-ring $ \Sigma $ on $ G $ and a measure $ \mu: \Sigma \to [0,\infty] $, non-trivial, such that the following properties hold: $ \Sigma $ is left-invariant w.r.t. $ \cdot $, i.e., $ x \cdot S \in \Sigma $ for every $ x \in G $ and $ S \in \Sigma $. $ \mu $ is left-invariant w.r.t. $ \cdot $, i.e., $ \mu(x \cdot S) = \mu(S) $ for every $ x \in G $ and $ S \in \Sigma $. The map $   \left\{ \begin{matrix}   G \times G & \to & G \times G \\ (x,y) & \mapsto & (x,x \cdot y)   \end{matrix} \right\}   $ is $ (\Sigma \times \Sigma,\Sigma \times \Sigma) $-measurable. For each $ x \in G \setminus \{ e \} $, there exists an $ S \in \Sigma $ with $$   0 < \mu(S) < \infty \qquad \text{and} \qquad   0 < \mu((x \cdot S) \triangle S) < \infty,   $$ where $ \triangle $ denotes the symmetric difference of sets. Then Weil’s converse to Haar’s Theorem states that there exists a topological group $ ((G',\bullet,e),\tau) $ with the following properties: $ \tau $ is a locally compact and Hausdorff group topology on $ G' $. $ (G,\cdot,e) $ is a subgroup of $ (G',\bullet,e) $, so that $ G \subseteq G' $ and $ \cdot = \bullet|_{G \times G} $. If $ \mathscr{B} $ denotes the $ \sigma $-ring on $ G' $ generated by the $ G_{\delta} $ compact (w.r.t. $ \tau $) subsets of $ G' $, then $$   \{ B \cap G \in \mathcal{P}(G) \mid B \in \mathscr{B} \} \subseteq \Sigma.   $$ Note: We call $ \mathscr{B} $ the $ \tau $-induced Baire $ \sigma $-ring on $ G' $ . There exists a (Baire) Haar measure $ \mu': \mathscr{B} \to [0,\infty] $, associated with $ ((G',\bullet,e),\tau) $, such that $$   \forall B \in \mathscr{B}: \qquad   \mu(B \cap G) = \mu'(B).   $$ This implies that $ G $ is a $ \mu' $-thick subset of $ G' $, as $ B \in \mathscr{B} $ and $ B \cap G = \varnothing $ imply $ \mu'(B) = 0 $. The version of Weil’s result presented here is taken from Halmos’s Measure Theory , which is rather antiquated but still remains a classic. Now, I would like to determine if one can simply replace every instance of ‘$ \sigma $-ring’ by ‘$ \sigma $-algebra’, as well as replace all Baire $ \sigma $-rings by Borel $ \sigma $-algebras, i.e., $ \sigma $-algebras on a set that are generated by a given locally compact and Hausdorff topology. Could someone kindly provide an authoritative reference to aid my query? Thank you very much!",,"['real-analysis', 'measure-theory', 'topological-groups', 'locally-compact-groups', 'haar-measure']"
14,"$\mu$ absolutely continuous with regards to Lebesgue measure on $[0, 1]$?",absolutely continuous with regards to Lebesgue measure on ?,"\mu [0, 1]","Say we have that $\mu$ is a measure on the Borel $\sigma$-algebra on $[0, 1]$ and for every $f$ that is real-valued and continuously differentiable we have$$\left| \int_0^1 f'(x)\,d\mu(x)\right| \le \sqrt{\int_0^1 f(x)^2\,dx}.$$Is $\mu$ absolutely continuous with respect to Lebesgue measure on $[0, 1]$?","Say we have that $\mu$ is a measure on the Borel $\sigma$-algebra on $[0, 1]$ and for every $f$ that is real-valued and continuously differentiable we have$$\left| \int_0^1 f'(x)\,d\mu(x)\right| \le \sqrt{\int_0^1 f(x)^2\,dx}.$$Is $\mu$ absolutely continuous with respect to Lebesgue measure on $[0, 1]$?",,['real-analysis']
15,"The density of the fractional part of $x^n$ in $(0,1)$?",The density of the fractional part of  in ?,"x^n (0,1)","If $\{ \text{the fractional part of } x^n, n \in \mathbb{N} \}$ is dense in $(0,1)$, what is the range of $x$? Obviously, $[-1,1]$ is not the case, and all integers are not the cases either, what will happen if $x>1$?","If $\{ \text{the fractional part of } x^n, n \in \mathbb{N} \}$ is dense in $(0,1)$, what is the range of $x$? Obviously, $[-1,1]$ is not the case, and all integers are not the cases either, what will happen if $x>1$?",,"['real-analysis', 'sequences-and-series', 'exponentiation', 'fractional-part']"
16,Continuity of limit of continuous functions implies uniform convergence?,Continuity of limit of continuous functions implies uniform convergence?,,"We know that uniform convergence of continuous functions implies the continuity of the limit. I'm wondering if the inverse is true: if a sequence of continuous functions $\{f_n\}$ converges to continuous function $f$, then is it true that $\{f_n\}$ converges uniformly to $f$. My attempt is that for any $x$ in the domain and $\epsilon$, there exists a $a$ such that $|f(x)-f_n(x)|\le|f(x)-f(a)|+|f(a)-f_n(a)|+|f_n(a)-f_n(x)|$. But I can't estimate the distance between $|f(a)-f_n(a)|$. Any tips ?","We know that uniform convergence of continuous functions implies the continuity of the limit. I'm wondering if the inverse is true: if a sequence of continuous functions $\{f_n\}$ converges to continuous function $f$, then is it true that $\{f_n\}$ converges uniformly to $f$. My attempt is that for any $x$ in the domain and $\epsilon$, there exists a $a$ such that $|f(x)-f_n(x)|\le|f(x)-f(a)|+|f(a)-f_n(a)|+|f_n(a)-f_n(x)|$. But I can't estimate the distance between $|f(a)-f_n(a)|$. Any tips ?",,"['real-analysis', 'analysis', 'uniform-convergence']"
17,Does there exist a real valued function $f$ such that $\lim_{x\rightarrow q} f(x) = \infty$ for all $q\in\mathbb{Q}$,Does there exist a real valued function  such that  for all,f \lim_{x\rightarrow q} f(x) = \infty q\in\mathbb{Q},"Does there exist a real valued function $f$ such that $\lim_{x\rightarrow q} f(x)  = \infty$ for all $q\in\mathbb{Q}$? My answer is no. If $f$ exists, then let us define $$g(x) = \left\{    \begin{array}{l l}      \frac{1}{1+|f(x)|} & \quad \text{if } x\in \mathbb{Q}^c\\      0 & \quad \text{if } x\in \mathbb{Q}\\    \end{array} \right. \\$$ and we see $g$ is continuous only on rational numbers, which is a contradicts the fact that the set of continuity points is a $G_\delta$ set. Is my solution correct? And do you guys know if there is any good direct argument?","Does there exist a real valued function $f$ such that $\lim_{x\rightarrow q} f(x)  = \infty$ for all $q\in\mathbb{Q}$? My answer is no. If $f$ exists, then let us define $$g(x) = \left\{    \begin{array}{l l}      \frac{1}{1+|f(x)|} & \quad \text{if } x\in \mathbb{Q}^c\\      0 & \quad \text{if } x\in \mathbb{Q}\\    \end{array} \right. \\$$ and we see $g$ is continuous only on rational numbers, which is a contradicts the fact that the set of continuity points is a $G_\delta$ set. Is my solution correct? And do you guys know if there is any good direct argument?",,"['real-analysis', 'functional-analysis']"
18,Non-existence of $C^1$ injective mapping $\mathbb{R}^3 \to \mathbb{R}^2$.,Non-existence of  injective mapping .,C^1 \mathbb{R}^3 \to \mathbb{R}^2,"A friend of mine did a test yesterday where it asked to prove that there does not exist a $C^1$ injective mapping $\mathbb{R}^3 \to \mathbb{R}^2$. This is an immediate result from invariance of domain, but since this is a real analysis test (where people are being introduced to derivation in $\mathbb{R}^n$), I tried to come up with an elementary solution. However, none came to mind. I thought about using the local form of submersions (which, by the way, I wouldn't expect in this point in the course my friend is taking anyway), but we would need to have a regular value which is on the image, and this is not given by the hypotheses, neither by Sard's theorem. Since this was on the test, I have the feeling I may be letting something slip. My question therefore is to prove the given statement with only tools of differentiation in $\mathbb{R}^n$ (inverse function theorem, chain rule etc).","A friend of mine did a test yesterday where it asked to prove that there does not exist a $C^1$ injective mapping $\mathbb{R}^3 \to \mathbb{R}^2$. This is an immediate result from invariance of domain, but since this is a real analysis test (where people are being introduced to derivation in $\mathbb{R}^n$), I tried to come up with an elementary solution. However, none came to mind. I thought about using the local form of submersions (which, by the way, I wouldn't expect in this point in the course my friend is taking anyway), but we would need to have a regular value which is on the image, and this is not given by the hypotheses, neither by Sard's theorem. Since this was on the test, I have the feeling I may be letting something slip. My question therefore is to prove the given statement with only tools of differentiation in $\mathbb{R}^n$ (inverse function theorem, chain rule etc).",,"['real-analysis', 'general-topology', 'algebraic-topology']"
19,"Haar measure, can image of modular function be any subgroup of $(0,\infty)$?","Haar measure, can image of modular function be any subgroup of ?","(0,\infty)","It is easy to find examples of locally compact second countable Hausdorff topological groups $G$ whose modular function $\Delta$ has image $\{1\}$ or $(0,\infty)$. Are there groups $G$ of this kind for which the image of $\Delta$ is anything else?","It is easy to find examples of locally compact second countable Hausdorff topological groups $G$ whose modular function $\Delta$ has image $\{1\}$ or $(0,\infty)$. Are there groups $G$ of this kind for which the image of $\Delta$ is anything else?",,"['real-analysis', 'measure-theory', 'topological-groups', 'haar-measure', 'modular-function']"
20,$\int \limits_{E}|f|d\mu<\varepsilon$ whenever $\mu(E)<\delta$.,whenever .,\int \limits_{E}|f|d\mu<\varepsilon \mu(E)<\delta,"Suppose $f\in L^1(\mu)$. Prove that to each $\varepsilon>0$ there exists a $\delta>0$ such that $\int \limits_{E}|f|d\mu<\varepsilon$ whenever $\mu(E)<\delta$. Proof: Let $\varepsilon>0$ be given and $E$ be an arbitrary measurable set. Since $\int \limits_{E}|f|d\mu=\sup \limits_{0\leqslant s \leqslant |f|} \int \limits_{E}sd\mu$, there exists a simple measurable function $s(x)=\sum \limits_{i=1}^{n}\alpha_{i}1_{A_i}$ with $0\leqslant s \leqslant |f|$ such that: $$\left(\int \limits_{E}|f|d\mu\right)-\frac{\varepsilon}{2}\leqslant \int \limits_{E}sd\mu \Rightarrow \int \limits_{E}|f|d\mu\leqslant \left(\int \limits_{E}sd\mu\right)+\frac{\varepsilon}{2}$$ Let's wotk with last integral: $$\int \limits_{E}sd\mu=\sum \limits_{i=1}^{n}\alpha_i\mu(A_i)\leq \max\{\alpha_1,\dots, \alpha_n\}\sum \limits_{i=1}^{n}\mu(A_i)=\max\{\alpha_1,\dots, \alpha_n\}\mu(E)=L\mu(E)$$ where $L=\max\{\alpha_1,\dots, \alpha_n\}$. Taking $\delta=\frac{\varepsilon}{2L}$(if $L>0$, otherwise is obvious), for all $\mu(E)<\delta$, we get: $$\int \limits_{E}|f|d\mu<\frac{\varepsilon}{2}+L\mu(E)<\frac{\varepsilon}{2}+L\frac{\varepsilon}{2L}=\varepsilon$$ Sorry if this topic is repeated but I would like to know is my proof correct? EDITED VERSION: Since $|f|$ is measurable on $X$ and $\int \limits_{X}|f|d\mu=\sup \limits_{0\leqslant s\leqslant |f|}\int \limits_{X}sd\mu$. Let $\varepsilon>0$ be given then exists simple measurable function $s: 0\leqslant s\leqslant |f|$ on $X$ such that $\int \limits_{X}|f|d\mu-\dfrac{\varepsilon}{2}\leqslant \int \limits_{X}sd\mu$, where  $s=\sum \limits_{i=1}^{n}\alpha_i1_{A_i}$ where $\alpha_1, \dots, \alpha_n$ are distinct positive reals and $A_i=\{x: s(x)=\alpha_i\}$. Also $\sqcup_{i=1}^{n}A_i=X$. Note that for any $E\in \mathfrak{M}$ we have $$\int \limits_{E}(|f|-s)d\mu \leqslant \int \limits_{X}(|f|-s)d\mu\leqslant \frac{\varepsilon}{2}.$$ Taking $\delta=\dfrac{\varepsilon}{2L},$ where $L=\max\{\alpha_1,\dots,\alpha_n\}$. Note that $\delta$ in this case does not depends on $E$! Hence for $E$ with $\mu(E)<\delta$ we have: $$\int \limits_{E}|f|d\mu\leqslant \frac{\varepsilon}{2}+\int \limits_{E}sd\mu\leqslant \dfrac{\varepsilon}{2}+\sum \limits_{i=1}^{n}\alpha_i\mu(A_i\cap E)\leqslant \dfrac{\varepsilon}{2}+L\sum \limits_{i=1}^{n}\mu(A_i\cap E)$$$$\leqslant \dfrac{\varepsilon}{2}+L\mu((\sqcup_{i=1}^nA_i)\cap E)=\dfrac{\varepsilon}{2}+L\mu(E)=\varepsilon.$$","Suppose $f\in L^1(\mu)$. Prove that to each $\varepsilon>0$ there exists a $\delta>0$ such that $\int \limits_{E}|f|d\mu<\varepsilon$ whenever $\mu(E)<\delta$. Proof: Let $\varepsilon>0$ be given and $E$ be an arbitrary measurable set. Since $\int \limits_{E}|f|d\mu=\sup \limits_{0\leqslant s \leqslant |f|} \int \limits_{E}sd\mu$, there exists a simple measurable function $s(x)=\sum \limits_{i=1}^{n}\alpha_{i}1_{A_i}$ with $0\leqslant s \leqslant |f|$ such that: $$\left(\int \limits_{E}|f|d\mu\right)-\frac{\varepsilon}{2}\leqslant \int \limits_{E}sd\mu \Rightarrow \int \limits_{E}|f|d\mu\leqslant \left(\int \limits_{E}sd\mu\right)+\frac{\varepsilon}{2}$$ Let's wotk with last integral: $$\int \limits_{E}sd\mu=\sum \limits_{i=1}^{n}\alpha_i\mu(A_i)\leq \max\{\alpha_1,\dots, \alpha_n\}\sum \limits_{i=1}^{n}\mu(A_i)=\max\{\alpha_1,\dots, \alpha_n\}\mu(E)=L\mu(E)$$ where $L=\max\{\alpha_1,\dots, \alpha_n\}$. Taking $\delta=\frac{\varepsilon}{2L}$(if $L>0$, otherwise is obvious), for all $\mu(E)<\delta$, we get: $$\int \limits_{E}|f|d\mu<\frac{\varepsilon}{2}+L\mu(E)<\frac{\varepsilon}{2}+L\frac{\varepsilon}{2L}=\varepsilon$$ Sorry if this topic is repeated but I would like to know is my proof correct? EDITED VERSION: Since $|f|$ is measurable on $X$ and $\int \limits_{X}|f|d\mu=\sup \limits_{0\leqslant s\leqslant |f|}\int \limits_{X}sd\mu$. Let $\varepsilon>0$ be given then exists simple measurable function $s: 0\leqslant s\leqslant |f|$ on $X$ such that $\int \limits_{X}|f|d\mu-\dfrac{\varepsilon}{2}\leqslant \int \limits_{X}sd\mu$, where  $s=\sum \limits_{i=1}^{n}\alpha_i1_{A_i}$ where $\alpha_1, \dots, \alpha_n$ are distinct positive reals and $A_i=\{x: s(x)=\alpha_i\}$. Also $\sqcup_{i=1}^{n}A_i=X$. Note that for any $E\in \mathfrak{M}$ we have $$\int \limits_{E}(|f|-s)d\mu \leqslant \int \limits_{X}(|f|-s)d\mu\leqslant \frac{\varepsilon}{2}.$$ Taking $\delta=\dfrac{\varepsilon}{2L},$ where $L=\max\{\alpha_1,\dots,\alpha_n\}$. Note that $\delta$ in this case does not depends on $E$! Hence for $E$ with $\mu(E)<\delta$ we have: $$\int \limits_{E}|f|d\mu\leqslant \frac{\varepsilon}{2}+\int \limits_{E}sd\mu\leqslant \dfrac{\varepsilon}{2}+\sum \limits_{i=1}^{n}\alpha_i\mu(A_i\cap E)\leqslant \dfrac{\varepsilon}{2}+L\sum \limits_{i=1}^{n}\mu(A_i\cap E)$$$$\leqslant \dfrac{\varepsilon}{2}+L\mu((\sqcup_{i=1}^nA_i)\cap E)=\dfrac{\varepsilon}{2}+L\mu(E)=\varepsilon.$$",,"['real-analysis', 'measure-theory', 'proof-verification']"
21,"Any Video Lectures Of An MIT, Harvard, Stanford, UC Berkeley, Yale, or Princeton Analysis Course Based On Baby Rudin?","Any Video Lectures Of An MIT, Harvard, Stanford, UC Berkeley, Yale, or Princeton Analysis Course Based On Baby Rudin?",,"I'm learning analysis from the book Principles of Mathematical Analysis by Walter Rudin, third edition. This book, popularly known as Baby Rudin , is being used for analysis courses at such elite places as the MIT, Harvard, Stanford, UC Berkeley, Yale, and Princeton. Am I right? Now is there any video lecture analysis course based on Baby Rudin available on the Internet from any of the above-mentioned institutions? Which other text(s) treat the same material as Baby Rudin and so can be used in combination with it?","I'm learning analysis from the book Principles of Mathematical Analysis by Walter Rudin, third edition. This book, popularly known as Baby Rudin , is being used for analysis courses at such elite places as the MIT, Harvard, Stanford, UC Berkeley, Yale, and Princeton. Am I right? Now is there any video lecture analysis course based on Baby Rudin available on the Internet from any of the above-mentioned institutions? Which other text(s) treat the same material as Baby Rudin and so can be used in combination with it?",,"['real-analysis', 'analysis', 'reference-request', 'book-recommendation']"
22,"Can $f_n\to f$ uniformly, $f'_n\to g$ uniformly, but $f$ not being differentiable?","Can  uniformly,  uniformly, but  not being differentiable?",f_n\to f f'_n\to g f,"Just the question in the title, I know that if $f_n$ are differentiable, $f_n\to f$ uniformly, $f'_n\to g$ uniformly and $f$ is differentiable, then $f'=g$, so I'm looking for a counterexample if we remove that hypothesis.","Just the question in the title, I know that if $f_n$ are differentiable, $f_n\to f$ uniformly, $f'_n\to g$ uniformly and $f$ is differentiable, then $f'=g$, so I'm looking for a counterexample if we remove that hypothesis.",,"['real-analysis', 'analysis', 'examples-counterexamples']"
23,Infinite nested radical and infinite continued fractions,Infinite nested radical and infinite continued fractions,,"If $$a = \sqrt{k_0+\sqrt{k_1+\sqrt{k_2+\sqrt{k_3+\sqrt{\cdots}}}}}$$ and $$b = \cfrac{1}{k_0+\cfrac{1}{k_1+\cfrac{1}{k_2+\cfrac{1}{\cdots}}}}$$ what is the relation between $a$ and $b$ . What function always satisfies $a = f(b)$ ? Would $f(x)$ be bijective, injective, or niether? Edit: all values in the sequence $k_n$ are whole numbers.","If and what is the relation between and . What function always satisfies ? Would be bijective, injective, or niether? Edit: all values in the sequence are whole numbers.",a = \sqrt{k_0+\sqrt{k_1+\sqrt{k_2+\sqrt{k_3+\sqrt{\cdots}}}}} b = \cfrac{1}{k_0+\cfrac{1}{k_1+\cfrac{1}{k_2+\cfrac{1}{\cdots}}}} a b a = f(b) f(x) k_n,"['real-analysis', 'continued-fractions', 'nested-radicals']"
24,"Find polynomials $p_n$ on $[0,1]$ with integral $=3$ and converges pointwisely to $0$",Find polynomials  on  with integral  and converges pointwisely to,"p_n [0,1] =3 0","Prove that there  is a sequence of polynomials $\{p_{n}\}$ such that $p_{n} \to 0$ pointwise on $[0,1]$, but such that  $$\int_{0}^{1}p_{n}(x)dx=3.$$ My thought :    I was thinking of working with a seuqence of functions that has pointwise limits to equal to $0$ but the integral to equal $3$. And then using the Weiestrass approximation theorem to say that there is a $p_n$ that would approximate the functions. I just can't think of the functions that would work.","Prove that there  is a sequence of polynomials $\{p_{n}\}$ such that $p_{n} \to 0$ pointwise on $[0,1]$, but such that  $$\int_{0}^{1}p_{n}(x)dx=3.$$ My thought :    I was thinking of working with a seuqence of functions that has pointwise limits to equal to $0$ but the integral to equal $3$. And then using the Weiestrass approximation theorem to say that there is a $p_n$ that would approximate the functions. I just can't think of the functions that would work.",,"['real-analysis', 'polynomials']"
25,Haar functions form a complete orthonormal system,Haar functions form a complete orthonormal system,,"I want to show that the Haar functions in $L^2([0,1])$ forms an orthonormal basis: Let $$f = 1_{[0, 1/2)} - 1_{[1/2,0)} \ \ \mbox{,} \ \ f_{j,k}(t) = 2^{j/2}f(2^jt - k).$$ Let $\mathscr{A} = \{(j.k) : j \geq 0, k = 0, 1, 2, ..., 2^j -1\}.$ I can prove that $\ A := \{1_{[0,1]}\} \cup \{f_{j,k}: (j,k) \in \mathscr{A}\}$ is an orthonormal system in $L^2([0,1])$. (using the fact that each of them is supported on $[2^{-j}k, 2^{-j}(k+1))$, and each different pairs $i, j$ either has disjoint support or contained in each other support) I want to show that $A$ is complete. Let $g \in L^2([0,1])$ with $\left<g,f_{i,j}\right> = 0$ and $\left<g, 1_{[0,1]}\right> = 0$ for all $(i, j) \in A.$ I will show that $g = 0 $ a.e.  Let $$I^l_{j,k} = [2^{-j},2^{-j}k + 2^{-j-1}), I^r_{j,k} = [2^{-j}k + 2^{-j-1}, 2^{-j}(k+1)).$$ Then $$f_{i,j} = 2^{-j}(1_{I^l_{i,j}} - 1_{I^r_{i,j}}).$$ So I see that $$\int_{I^l_{i,j}} f = \int _{I^r_{i,j}} f$$ for all $(i,j) \in A $ and $$\int_{[0.1]} f = 0.$$ It just ""seems"" that $f$ should be $0$ a.e., but I cannot think of rigorous reasons for this to happen (how to clearly show that it is true).","I want to show that the Haar functions in $L^2([0,1])$ forms an orthonormal basis: Let $$f = 1_{[0, 1/2)} - 1_{[1/2,0)} \ \ \mbox{,} \ \ f_{j,k}(t) = 2^{j/2}f(2^jt - k).$$ Let $\mathscr{A} = \{(j.k) : j \geq 0, k = 0, 1, 2, ..., 2^j -1\}.$ I can prove that $\ A := \{1_{[0,1]}\} \cup \{f_{j,k}: (j,k) \in \mathscr{A}\}$ is an orthonormal system in $L^2([0,1])$. (using the fact that each of them is supported on $[2^{-j}k, 2^{-j}(k+1))$, and each different pairs $i, j$ either has disjoint support or contained in each other support) I want to show that $A$ is complete. Let $g \in L^2([0,1])$ with $\left<g,f_{i,j}\right> = 0$ and $\left<g, 1_{[0,1]}\right> = 0$ for all $(i, j) \in A.$ I will show that $g = 0 $ a.e.  Let $$I^l_{j,k} = [2^{-j},2^{-j}k + 2^{-j-1}), I^r_{j,k} = [2^{-j}k + 2^{-j-1}, 2^{-j}(k+1)).$$ Then $$f_{i,j} = 2^{-j}(1_{I^l_{i,j}} - 1_{I^r_{i,j}}).$$ So I see that $$\int_{I^l_{i,j}} f = \int _{I^r_{i,j}} f$$ for all $(i,j) \in A $ and $$\int_{[0.1]} f = 0.$$ It just ""seems"" that $f$ should be $0$ a.e., but I cannot think of rigorous reasons for this to happen (how to clearly show that it is true).",,"['real-analysis', 'hilbert-spaces', 'lp-spaces', 'orthonormal']"
26,Connecting the formula for the surface area of a sphere to differential forms,Connecting the formula for the surface area of a sphere to differential forms,,"I have a formula for computing the area of the surface of any object in $\Bbb R^3$, namely given some parametrization $\Phi(s,t)$, I take the cross product of the partial derivative with respect to each variable and norm it. This looks suspiciously like a wedge product, especially when you think of the cross product as the determinant of the $3\times 3$ with the top row as the $e_i$ vectors. How do I connect the two concepts? Concretely, I tried to integrate the surface area of a sphere using the differential 2-form $dx\wedge dy$ on the obvious parametrization and kept getting stuck, finding the area was zero.","I have a formula for computing the area of the surface of any object in $\Bbb R^3$, namely given some parametrization $\Phi(s,t)$, I take the cross product of the partial derivative with respect to each variable and norm it. This looks suspiciously like a wedge product, especially when you think of the cross product as the determinant of the $3\times 3$ with the top row as the $e_i$ vectors. How do I connect the two concepts? Concretely, I tried to integrate the surface area of a sphere using the differential 2-form $dx\wedge dy$ on the obvious parametrization and kept getting stuck, finding the area was zero.",,['real-analysis']
27,Triangle inequality and the square root of a metric space,Triangle inequality and the square root of a metric space,,for (i) I know that the square root part is true but I don't know how to put it into words to prove it. For (ii) I just don't know how top apply the requirements for a metric space to the square root of another metric space. Just kind of confusing me,for (i) I know that the square root part is true but I don't know how to put it into words to prove it. For (ii) I just don't know how top apply the requirements for a metric space to the square root of another metric space. Just kind of confusing me,,['real-analysis']
28,"Prove that a bounded, monotone increasing, and continuous function is uniformly continuous","Prove that a bounded, monotone increasing, and continuous function is uniformly continuous",,"I'm trying to prove that a function $f:[0,\infty) \to \mathbb R$ that is continuous, monotone increasing and bounded is uniformly continuous. Here's a skech of what I've got so far: $f(x) \to L$ for some $L \in \mathbb R$. Fix $\gamma>0$ then $[f^{-1}(0),L-\gamma]:=[a,b]$ is a compact interval and so $f$ is uniformly continuous on $[a,b]$. This is where I'm stuck, If I let $x,y \in [a,b]$ then $\forall \epsilon>0, \exists \delta>0$ such that $0<|x-y|<\delta \implies |f(x)-f(y)|<\epsilon$. If $x,y \notin[a,b]$ then $x,y>b$ and so I know $|f(x)-f(y)| \leq |f(x)-L|+|f(y)-L|$ which are each less than $L-\gamma$. What $\delta$ can I use to bound this expression? And what about the case where $x \in [a,b]$ and $y>b$?","I'm trying to prove that a function $f:[0,\infty) \to \mathbb R$ that is continuous, monotone increasing and bounded is uniformly continuous. Here's a skech of what I've got so far: $f(x) \to L$ for some $L \in \mathbb R$. Fix $\gamma>0$ then $[f^{-1}(0),L-\gamma]:=[a,b]$ is a compact interval and so $f$ is uniformly continuous on $[a,b]$. This is where I'm stuck, If I let $x,y \in [a,b]$ then $\forall \epsilon>0, \exists \delta>0$ such that $0<|x-y|<\delta \implies |f(x)-f(y)|<\epsilon$. If $x,y \notin[a,b]$ then $x,y>b$ and so I know $|f(x)-f(y)| \leq |f(x)-L|+|f(y)-L|$ which are each less than $L-\gamma$. What $\delta$ can I use to bound this expression? And what about the case where $x \in [a,b]$ and $y>b$?",,"['real-analysis', 'epsilon-delta', 'uniform-continuity']"
29,"The set of isomorphisms, $Iso(X,Y)$ is open.","The set of isomorphisms,  is open.","Iso(X,Y)","Let $X,Y $ be Banach. I want to show that, $GL(X,Y) = \{ A\in L(X,Y), B \in L(Y,X) : BA= id_{x} \ \ \text{and}  \ BA = id_{y} \} \subset^{open} L(X,Y)$ My attempt, Let $T \in Iso(X,Y), \ Q \in L(X,Y)$  \begin{equation} Q= T-(T-Q)\\ =T(id_{x} - T^{-1}(T-Q))\\ =T(id_{x} - \lambda ) \end{equation} Letting $\lambda = T^{-1}(T-Q) = id_{x} - T^{-1}Q$ Taking the norm of this, $||\lambda|| =||T^{-1}(T-Q)|| \\  \le ||T^{-1}||||T-Q|| \\  < ||T^{-1}||||T^{-1}||^{-1} = 1$ So $id - \lambda$ is invertible (and hence so is Q as it is the product of invertibles). As Q is continuous, is this justifiying that $Iso(X,Y) \subset^{open} $ of $L(X,Y)$? Any help would be appreciated!","Let $X,Y $ be Banach. I want to show that, $GL(X,Y) = \{ A\in L(X,Y), B \in L(Y,X) : BA= id_{x} \ \ \text{and}  \ BA = id_{y} \} \subset^{open} L(X,Y)$ My attempt, Let $T \in Iso(X,Y), \ Q \in L(X,Y)$  \begin{equation} Q= T-(T-Q)\\ =T(id_{x} - T^{-1}(T-Q))\\ =T(id_{x} - \lambda ) \end{equation} Letting $\lambda = T^{-1}(T-Q) = id_{x} - T^{-1}Q$ Taking the norm of this, $||\lambda|| =||T^{-1}(T-Q)|| \\  \le ||T^{-1}||||T-Q|| \\  < ||T^{-1}||||T^{-1}||^{-1} = 1$ So $id - \lambda$ is invertible (and hence so is Q as it is the product of invertibles). As Q is continuous, is this justifiying that $Iso(X,Y) \subset^{open} $ of $L(X,Y)$? Any help would be appreciated!",,"['real-analysis', 'functional-analysis']"
30,"continuity, essential supremum and supremum","continuity, essential supremum and supremum",,"Take $f: \mathbb{R}\to \mathbb{R}$ be continuous, then w.r.t. Lebesgue measure $L$, we have  $$\operatorname{ess sup} |f|=\sup |f|$$ w.r.t. $L$. I have been provided a proof in a Measure theory course I studied many years ago. Hence $M\le N$. I understood first part of the proof, but in the proof of the 2nd part, I do not understand why $2\epsilon$ is used. (Why not just $\epsilon?$)","Take $f: \mathbb{R}\to \mathbb{R}$ be continuous, then w.r.t. Lebesgue measure $L$, we have  $$\operatorname{ess sup} |f|=\sup |f|$$ w.r.t. $L$. I have been provided a proof in a Measure theory course I studied many years ago. Hence $M\le N$. I understood first part of the proof, but in the proof of the 2nd part, I do not understand why $2\epsilon$ is used. (Why not just $\epsilon?$)",,"['real-analysis', 'functional-analysis', 'measure-theory']"
31,"Real Analysis, Folland Proposition 2.1","Real Analysis, Folland Proposition 2.1",,"Background information: Any mapping $f:X\rightarrow Y$ between two sets induces a mapping $f^{-1}:P(Y)\rightarrow P(X)$, defined by $f^{-1}(E) = \{x\in X:f(x)\in E\}$, which preserves unions, intersections, and complements. Thus, if $N$ is a $\sigma$-algebra on $Y$, $\{f^{-1}(E):E\in N\}$ is a $\sigma$-algebra on $X$. If $(X,M)$ and $(Y,N)$ are measurable spaces, a mapping $f:X\rightarrow Y$ is called $(M,N)$-measurable or just measurable when $M$ and $N$ are understood, if $f^{-1}(E)\in M$ for all $E\in N$ Proposition 2.1 - If $N$ is generated by $\varepsilon$, then $f:X\rightarrow Y$ is $(M,N)$-measurable iff $f^{-1}(E)\in M$ for all $E\in \varepsilon$. Proof $\Rightarrow$ Let $N$ be a $\sigma$-algebra generated by $\varepsilon$ and suppose we have $f:X\rightarrow Y$ that is $(M,N)$- measurable. Then by definition, this mapping induces a mapping $f^{-1}:P(Y)\rightarrow P(X)\}$ defined by $$f^{-1}(E) = \{x\in X: f(x)\in E\}$$ then clearly $f^{-1}(E)\in M$ since $M$ is a $\sigma$-algebra containing $\varepsilon$; it therefore contains $N$. I am not sure if this part is true, also I am not sure how to prove the converse any suggestions is greatly appreciated.","Background information: Any mapping $f:X\rightarrow Y$ between two sets induces a mapping $f^{-1}:P(Y)\rightarrow P(X)$, defined by $f^{-1}(E) = \{x\in X:f(x)\in E\}$, which preserves unions, intersections, and complements. Thus, if $N$ is a $\sigma$-algebra on $Y$, $\{f^{-1}(E):E\in N\}$ is a $\sigma$-algebra on $X$. If $(X,M)$ and $(Y,N)$ are measurable spaces, a mapping $f:X\rightarrow Y$ is called $(M,N)$-measurable or just measurable when $M$ and $N$ are understood, if $f^{-1}(E)\in M$ for all $E\in N$ Proposition 2.1 - If $N$ is generated by $\varepsilon$, then $f:X\rightarrow Y$ is $(M,N)$-measurable iff $f^{-1}(E)\in M$ for all $E\in \varepsilon$. Proof $\Rightarrow$ Let $N$ be a $\sigma$-algebra generated by $\varepsilon$ and suppose we have $f:X\rightarrow Y$ that is $(M,N)$- measurable. Then by definition, this mapping induces a mapping $f^{-1}:P(Y)\rightarrow P(X)\}$ defined by $$f^{-1}(E) = \{x\in X: f(x)\in E\}$$ then clearly $f^{-1}(E)\in M$ since $M$ is a $\sigma$-algebra containing $\varepsilon$; it therefore contains $N$. I am not sure if this part is true, also I am not sure how to prove the converse any suggestions is greatly appreciated.",,"['real-analysis', 'measure-theory']"
32,Prove that $\lim(x_n)=0$ if and only if $\lim(|x_n|)=0$.,Prove that  if and only if .,\lim(x_n)=0 \lim(|x_n|)=0,"Prove that $\lim(x_n)=0$ if and only if $\lim(|x_n|)=0$. Definition: Let $X = (x_n)$ be a sequence in $\mathbb{R}$ and let $x\in\mathbb{R}$. Then $\lim(x_n) =x$ iff for all $\varepsilon>0$, $\exists k\in\mathbb{N}$ such that $|x_n-x|<\varepsilon$ for all $n\geq k$. I am wondering if this is sufficient: If we know that $\lim(x_n)=0$, we know for all $\varepsilon>0$, $\exists k\in\mathbb{N}$ such that $|x_n-0|<\varepsilon$ for all $n\geq k$. We can rewrite this as the following: $$|x_n-0|<\varepsilon \Leftrightarrow ||x_n|-0|<\varepsilon\Leftrightarrow \lim(|x_n|)=0$$ Then the conclusion seems to follow logically. Is the proof really that simple or are there some things I am missing?","Prove that $\lim(x_n)=0$ if and only if $\lim(|x_n|)=0$. Definition: Let $X = (x_n)$ be a sequence in $\mathbb{R}$ and let $x\in\mathbb{R}$. Then $\lim(x_n) =x$ iff for all $\varepsilon>0$, $\exists k\in\mathbb{N}$ such that $|x_n-x|<\varepsilon$ for all $n\geq k$. I am wondering if this is sufficient: If we know that $\lim(x_n)=0$, we know for all $\varepsilon>0$, $\exists k\in\mathbb{N}$ such that $|x_n-0|<\varepsilon$ for all $n\geq k$. We can rewrite this as the following: $$|x_n-0|<\varepsilon \Leftrightarrow ||x_n|-0|<\varepsilon\Leftrightarrow \lim(|x_n|)=0$$ Then the conclusion seems to follow logically. Is the proof really that simple or are there some things I am missing?",,['real-analysis']
33,Construct a function that takes any value even number of times.,Construct a function that takes any value even number of times.,,"I'm looking for a continous function $f: [0,1] \to \mathbb{R}$ such that it takes any value even (thus finite) number of times. I suppose that it exists in the class of Lipschitz functions. All my approaches have led to some value which is taken infinite number of times.","I'm looking for a continous function $f: [0,1] \to \mathbb{R}$ such that it takes any value even (thus finite) number of times. I suppose that it exists in the class of Lipschitz functions. All my approaches have led to some value which is taken infinite number of times.",,['real-analysis']
34,How does the Vitali set violate the definition of measurable sets?,How does the Vitali set violate the definition of measurable sets?,,"In my textbook the Vitali set is shown as a classic example of non-measurable sets. The proof is done by showing that you can derive an impossible measure of this set if it is measurable. I also searched the internet and there are other proofs more or less the same. One proof can be found here ""Is outer measure a measure?"" . However, no material I read directly shows how the Vitali set (denoted as $V$) violates the definition of measurable sets, that is for any set $A$ the equation $|A|_e=|A\bigcap V|_e+|A\bigcap V^C|_e$ holds, where $|A|_e$ denotes outer measure. Since the Vitali set is non-measurable, there must exist some set $A$ that does not satisfy the equation. Can anyone help find an example of such an $A$?","In my textbook the Vitali set is shown as a classic example of non-measurable sets. The proof is done by showing that you can derive an impossible measure of this set if it is measurable. I also searched the internet and there are other proofs more or less the same. One proof can be found here ""Is outer measure a measure?"" . However, no material I read directly shows how the Vitali set (denoted as $V$) violates the definition of measurable sets, that is for any set $A$ the equation $|A|_e=|A\bigcap V|_e+|A\bigcap V^C|_e$ holds, where $|A|_e$ denotes outer measure. Since the Vitali set is non-measurable, there must exist some set $A$ that does not satisfy the equation. Can anyone help find an example of such an $A$?",,"['real-analysis', 'measure-theory']"
35,Open set in a general metric space.,Open set in a general metric space.,,Let d define a metric on an infinite set $M$. Show that there exists an open set $U$ such that $U$ and its complement are infinite. (Infinite referring to cardinality in both instances) I know this is trivial with the discrete metric and also trivial on the reals with the usual metric but I'm not sure how to proceed for a general metric.,Let d define a metric on an infinite set $M$. Show that there exists an open set $U$ such that $U$ and its complement are infinite. (Infinite referring to cardinality in both instances) I know this is trivial with the discrete metric and also trivial on the reals with the usual metric but I'm not sure how to proceed for a general metric.,,"['real-analysis', 'metric-spaces']"
36,Extreme values of a two-variable polynomial,Extreme values of a two-variable polynomial,,"Is it possible to find a two-variable polynomial which has only two extreme values on the whole plane, one is a local maximum, another is a local minimum, and the local maximum is less than the local minimum?","Is it possible to find a two-variable polynomial which has only two extreme values on the whole plane, one is a local maximum, another is a local minimum, and the local maximum is less than the local minimum?",,"['calculus', 'real-analysis', 'multivariable-calculus', 'functions', 'derivatives']"
37,from Carathéodory Derivative definition to the derivative of $\sin(x)$,from Carathéodory Derivative definition to the derivative of,\sin(x),"A function $f$ is Carathéodory differentiable at $a$ if there exists a function $\phi$ which is continuous at a such that $$f(x)-f(a)=\phi(x)(x-a).$$ For $f(x) = x^n$, $\phi(x)  = x^{n-1} + ax^{n-2} + ... + a^{n-1}$. We can see that $f'(a) = \phi(a) = na^{n-1}$. We get the derivative of $x^n$ directly from this definition. For $\sin(x)$, can we do the same thing to get $\sin'(x) = \cos(x)$ without using limit? My question is raised from the booklet Calculus for Mathematicians by D.J.Bernstein, in which he defined derivative this way before introducing the concept of limits.","A function $f$ is Carathéodory differentiable at $a$ if there exists a function $\phi$ which is continuous at a such that $$f(x)-f(a)=\phi(x)(x-a).$$ For $f(x) = x^n$, $\phi(x)  = x^{n-1} + ax^{n-2} + ... + a^{n-1}$. We can see that $f'(a) = \phi(a) = na^{n-1}$. We get the derivative of $x^n$ directly from this definition. For $\sin(x)$, can we do the same thing to get $\sin'(x) = \cos(x)$ without using limit? My question is raised from the booklet Calculus for Mathematicians by D.J.Bernstein, in which he defined derivative this way before introducing the concept of limits.",,"['calculus', 'real-analysis', 'measure-theory']"
38,Help understanding an inequality on Rudin's construction of the Lebesgue measure,Help understanding an inequality on Rudin's construction of the Lebesgue measure,,"I am having trouble understanding an inequality in Theorem 2.20 from ""Real and Complex Analysis."" Rudin states that if $f\in\operatorname{C}_c(\mathbb{R}^k)$ , $f$ is real, $W$ is an open k-cell which contains the support of $f$, and $\epsilon>0$   , such that (i) $g$ and $h$ are constant on each box belonging to $\Omega_N$ (ii) $g\leq f\leq h$ (iii) $h-g<\epsilon$ If $n>N$, Property 2.19(c) shows that $$\Lambda_N g =  \Lambda_n g \leq \Lambda_n f \leq \Lambda_n h = \Lambda_N h$$ Here $\Lambda$ is defined as $$\Lambda_n f := \lim\limits_{n \to \infty} 2^{-nk} \sum\limits_{x \in P_n} f(x)$$ $\Omega_n$ is the collection of all $2^{-n}$ boxes with corners at $P_n$ $P_n$ is the set of all $x\in\mathbb{R^k}$ whose coordinates are integral multiples of $2^{-n}$ Property 2.19 (c) is For $\{\Omega_n\}$, if $Q\in \Omega_r$, then vol$(Q)=2^{-rk}$; and if $n>r$, the set $P_n$ has exactly $2^{(n-r)k}$ points in $Q$ What I don't understand is how Property 2.19(c) implies that $\Lambda_N g =  \Lambda_n g$ and $\Lambda_N h =  \Lambda_n h$","I am having trouble understanding an inequality in Theorem 2.20 from ""Real and Complex Analysis."" Rudin states that if $f\in\operatorname{C}_c(\mathbb{R}^k)$ , $f$ is real, $W$ is an open k-cell which contains the support of $f$, and $\epsilon>0$   , such that (i) $g$ and $h$ are constant on each box belonging to $\Omega_N$ (ii) $g\leq f\leq h$ (iii) $h-g<\epsilon$ If $n>N$, Property 2.19(c) shows that $$\Lambda_N g =  \Lambda_n g \leq \Lambda_n f \leq \Lambda_n h = \Lambda_N h$$ Here $\Lambda$ is defined as $$\Lambda_n f := \lim\limits_{n \to \infty} 2^{-nk} \sum\limits_{x \in P_n} f(x)$$ $\Omega_n$ is the collection of all $2^{-n}$ boxes with corners at $P_n$ $P_n$ is the set of all $x\in\mathbb{R^k}$ whose coordinates are integral multiples of $2^{-n}$ Property 2.19 (c) is For $\{\Omega_n\}$, if $Q\in \Omega_r$, then vol$(Q)=2^{-rk}$; and if $n>r$, the set $P_n$ has exactly $2^{(n-r)k}$ points in $Q$ What I don't understand is how Property 2.19(c) implies that $\Lambda_N g =  \Lambda_n g$ and $\Lambda_N h =  \Lambda_n h$",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure']"
39,Seeking a More Elegant Proof to an Expectation Inequality,Seeking a More Elegant Proof to an Expectation Inequality,,"Let $X$ and $Y$ be i.i.d. random variables, and $\mathbb E[|X|]<\infty$, prove that $$\mathbb E[|X+Y|]\geq\mathbb E[|X-Y|].$$ This question is a re-posting of An expectation inequality . I can prove this with integration. But there must be a more elegant proof via perhaps a symmetry argument. Can someone come up with such a one?","Let $X$ and $Y$ be i.i.d. random variables, and $\mathbb E[|X|]<\infty$, prove that $$\mathbb E[|X+Y|]\geq\mathbb E[|X-Y|].$$ This question is a re-posting of An expectation inequality . I can prove this with integration. But there must be a more elegant proof via perhaps a symmetry argument. Can someone come up with such a one?",,"['real-analysis', 'probability-theory', 'inequality', 'alternative-proof', 'integral-inequality']"
40,Harmonic Function with linear growth,Harmonic Function with linear growth,,"We want to find harmonic functions $w$ in (say) $\mathbb{R}^2$ that are zero on $\{y=0\}$ with the linear growth bound \begin{equation} \sup_{\mathbb{B}_R} |w| \leq C(1+R) \end{equation} where $C>0$ independent of $R$, are the only solutions to such a problem $w(x,y)=ay$ for some $a \in \mathbb{R}^2$? I cannot think of a method to even start this at the moment as the usual tools (like mean value property or Harnack) do not seem to give anything of use. I feel like it is possible an application of Harnacks inequality in a clever way might do it?","We want to find harmonic functions $w$ in (say) $\mathbb{R}^2$ that are zero on $\{y=0\}$ with the linear growth bound \begin{equation} \sup_{\mathbb{B}_R} |w| \leq C(1+R) \end{equation} where $C>0$ independent of $R$, are the only solutions to such a problem $w(x,y)=ay$ for some $a \in \mathbb{R}^2$? I cannot think of a method to even start this at the moment as the usual tools (like mean value property or Harnack) do not seem to give anything of use. I feel like it is possible an application of Harnacks inequality in a clever way might do it?",,"['real-analysis', 'complex-analysis', 'analysis', 'partial-differential-equations', 'harmonic-functions']"
41,Proving the existence of $b$ such that $\prod_{k=1}^n(1-\cos(a_k-b))=\frac{1}{2^n}$,Proving the existence of  such that,b \prod_{k=1}^n(1-\cos(a_k-b))=\frac{1}{2^n},"Let $n>0$ and $a_1,\ldots,a_n\in \mathbb R$. Prove there is some $b$ such that $\prod_{k=1}^n(1-\cos(a_k-b))=\frac{1}{2^n}$ This is motivated by this question Finding a point on the unit circle that achieves this equality Numerical trials suggest it is true indeed, and that for any choice of $a_1,\ldots,a_n$, $b\to \prod_{k=1}^n(1-\cos(a_k-b))$ achieves a maximum quite greater that $\frac{1}{2^n}$. EDIT : It may be worth noting that: $$\prod_{k=1}^n(1-\cos(a_k-b))=2^n\prod_{k=1}^n\sin^2(\frac{a_k-b}{2})$$ I attempted induction on $n$, to no avail.","Let $n>0$ and $a_1,\ldots,a_n\in \mathbb R$. Prove there is some $b$ such that $\prod_{k=1}^n(1-\cos(a_k-b))=\frac{1}{2^n}$ This is motivated by this question Finding a point on the unit circle that achieves this equality Numerical trials suggest it is true indeed, and that for any choice of $a_1,\ldots,a_n$, $b\to \prod_{k=1}^n(1-\cos(a_k-b))$ achieves a maximum quite greater that $\frac{1}{2^n}$. EDIT : It may be worth noting that: $$\prod_{k=1}^n(1-\cos(a_k-b))=2^n\prod_{k=1}^n\sin^2(\frac{a_k-b}{2})$$ I attempted induction on $n$, to no avail.",,"['real-analysis', 'trigonometry', 'optimization']"
42,the elements of Cantor's discontinuum,the elements of Cantor's discontinuum,,"Let $(A_n)_{n \in \mathbb{N}}$ the sequence of subsets of $\mathbb{R}$, given by $A_0 := \bigcup_{k \in \mathbb{Z}}[2k, 2k + 1]$ und $A_n := \frac{1}{3}A_{n-1}$ for $n ≥ 1$. Also, we define $$ A := \bigcap_{n=0}^\infty A_n, \,C:= A \cap [0, 1]$$ We call $C$ Cantor's discontinuum. Given this definition, I now want to prove that $C$ consists of all the real numbers of the form $x = \sum_{k=1}^\infty \frac{a_k}{3^k}$, where $a_k = 0$ or $a_k = 2$. Thanks in advance! I'm beginning to understand what $C$ actually looks like, but I don't really know how to show this. My idea was to maybe use the three-adic representation of numbers?","Let $(A_n)_{n \in \mathbb{N}}$ the sequence of subsets of $\mathbb{R}$, given by $A_0 := \bigcup_{k \in \mathbb{Z}}[2k, 2k + 1]$ und $A_n := \frac{1}{3}A_{n-1}$ for $n ≥ 1$. Also, we define $$ A := \bigcap_{n=0}^\infty A_n, \,C:= A \cap [0, 1]$$ We call $C$ Cantor's discontinuum. Given this definition, I now want to prove that $C$ consists of all the real numbers of the form $x = \sum_{k=1}^\infty \frac{a_k}{3^k}$, where $a_k = 0$ or $a_k = 2$. Thanks in advance! I'm beginning to understand what $C$ actually looks like, but I don't really know how to show this. My idea was to maybe use the three-adic representation of numbers?",,"['real-analysis', 'sequences-and-series', 'analysis', 'cantor-set']"
43,Checking if a vector field is conservative,Checking if a vector field is conservative,,"I have three different vector fields and I want to check if they are conservative: $$1)\space \space\vec{f}(\vec{x}):=\frac{1}{||\vec{x}||}\vec{a}, \space \space D=\mathbb R^2 / (\vec{0}), \space \space \vec{a}=(a_1,a_2)$$ $$2) \space \space \vec{g}(x,y)=(\frac{x}{y^2-x^2},\frac{-y}{y^2-x^2}), \space \space D=[(x,y) \in \mathbb R^2|\space y>|x|]$$ $$3) \space \space \vec{h}(x,y,z):=(2xz+y \cos (xy),\space x \cos(xy), \space x^2), D=[(x,y,z)\in \mathbb R^3 | \space x^2+y^2+z^2 \le 1]$$ I just learned/looked up the following on khanacademy: A vector field $\space \vec{f} \space $ is conservative $\iff$ $\underbrace{\int_{\gamma_2}\vec{v}(x) d\vec{x}=\int_{\gamma_1}\vec{v}(x) d\vec{x}}_{\text{Path independent}} \iff \underbrace{\oint_{\gamma}\vec{v}(x) d\vec{x}=0}_{\text{conservation of energy}}$ If my vector field $\vec{v}$ is the gradient of some scalar field $V$ i.e $\vec{v}=\vec{\nabla}V \implies \vec{v}$ is conservative. I understand what all of these things mean and how they were derived but I am somehow unable to properly apply them. Can someone maybe show me in a simple example how you would show that a vector field is conservative (doesn't have to be one of the fields I mentioned). Thanks in advance","I have three different vector fields and I want to check if they are conservative: $$1)\space \space\vec{f}(\vec{x}):=\frac{1}{||\vec{x}||}\vec{a}, \space \space D=\mathbb R^2 / (\vec{0}), \space \space \vec{a}=(a_1,a_2)$$ $$2) \space \space \vec{g}(x,y)=(\frac{x}{y^2-x^2},\frac{-y}{y^2-x^2}), \space \space D=[(x,y) \in \mathbb R^2|\space y>|x|]$$ $$3) \space \space \vec{h}(x,y,z):=(2xz+y \cos (xy),\space x \cos(xy), \space x^2), D=[(x,y,z)\in \mathbb R^3 | \space x^2+y^2+z^2 \le 1]$$ I just learned/looked up the following on khanacademy: A vector field $\space \vec{f} \space $ is conservative $\iff$ $\underbrace{\int_{\gamma_2}\vec{v}(x) d\vec{x}=\int_{\gamma_1}\vec{v}(x) d\vec{x}}_{\text{Path independent}} \iff \underbrace{\oint_{\gamma}\vec{v}(x) d\vec{x}=0}_{\text{conservation of energy}}$ If my vector field $\vec{v}$ is the gradient of some scalar field $V$ i.e $\vec{v}=\vec{\nabla}V \implies \vec{v}$ is conservative. I understand what all of these things mean and how they were derived but I am somehow unable to properly apply them. Can someone maybe show me in a simple example how you would show that a vector field is conservative (doesn't have to be one of the fields I mentioned). Thanks in advance",,"['real-analysis', 'analysis', 'functional-analysis', 'vector-fields']"
44,Convergence of $ L^{p} $-integrals implies convergence in $ L^{p} $-norm?,Convergence of -integrals implies convergence in -norm?, L^{p}   L^{p} ,"Let $E$ be a measurable set, $\{ f_n \}$ and $f$ are in $L^p(E)$ such that $f_n \to f$ pointwise a.e. If $\lim \|f_n \|_p = \| f \|_p$ ,  is it true that $\lim \| f_n - f \|_p = 0$ ? I have tried using Generalised Lebesgue Dominated Convergence Theorem,  for all $n$ , $|f_n-f|^p \leq g_n:=(|f_n|+|f|)^p$ ,  then $g_n \to g:=2^p|f|^p$ pointwise a.e.  But how to show $\lim \int g_n = \int g$ ? Thank you!!","Let be a measurable set, and are in such that pointwise a.e. If ,  is it true that ? I have tried using Generalised Lebesgue Dominated Convergence Theorem,  for all , ,  then pointwise a.e.  But how to show ? Thank you!!",E \{ f_n \} f L^p(E) f_n \to f \lim \|f_n \|_p = \| f \|_p \lim \| f_n - f \|_p = 0 n |f_n-f|^p \leq g_n:=(|f_n|+|f|)^p g_n \to g:=2^p|f|^p \lim \int g_n = \int g,"['real-analysis', 'convergence-divergence', 'lebesgue-integral', 'lebesgue-measure', 'lp-spaces']"
45,Trying to calculate the integral limit $\lim_{n\rightarrow\infty} \int_{-\sqrt n}^{\sqrt n}\left (1 - \frac{x^2}{2n}\right)^ndx$,Trying to calculate the integral limit,\lim_{n\rightarrow\infty} \int_{-\sqrt n}^{\sqrt n}\left (1 - \frac{x^2}{2n}\right)^ndx,"How to calculate following integral: $$\lim_{n\rightarrow\infty}\int_{-\sqrt{n}}^{\sqrt{n}}{\left(1-\frac{x^2}{2n}\right)^n}dx$$   Prove that this integral exists and compute its value. I just do not how start it. It is in real analysis , but I did not see any relation with real analsis.","How to calculate following integral: $$\lim_{n\rightarrow\infty}\int_{-\sqrt{n}}^{\sqrt{n}}{\left(1-\frac{x^2}{2n}\right)^n}dx$$   Prove that this integral exists and compute its value. I just do not how start it. It is in real analysis , but I did not see any relation with real analsis.",,"['real-analysis', 'integration', 'lebesgue-integral']"
46,Continuity and uniform continuity of $f(x)$ over $\mathbb Q$,Continuity and uniform continuity of  over,f(x) \mathbb Q,"Consider, the function $f:\mathbb Q\to \mathbb R$ defined by $$f(x)=\begin{cases}1 &\text{ if, } x<\pi\\2 &\text{ if, } x>\pi\end{cases}$$ Show that, $f$ is continuous but NOT uniformly continuous.. Let, $\epsilon >0$ be arbitrary. Then, we can always find a $x>\pi$ and a $y<\pi$ such that $|x-y|<\delta$ . But, $|f(x)-f(y)|=|2-1|=1\not <\epsilon$ . So , $f$ is NOT uniformly continuous.. But I am unable to find that $f$ is continuous....","Consider, the function defined by Show that, is continuous but NOT uniformly continuous.. Let, be arbitrary. Then, we can always find a and a such that . But, . So , is NOT uniformly continuous.. But I am unable to find that is continuous....","f:\mathbb Q\to \mathbb R f(x)=\begin{cases}1 &\text{ if, } x<\pi\\2 &\text{ if, } x>\pi\end{cases} f \epsilon >0 x>\pi y<\pi |x-y|<\delta |f(x)-f(y)|=|2-1|=1\not <\epsilon f f","['real-analysis', 'continuity', 'uniform-continuity']"
47,Show the Euclidean metric and maximum metric are strongly equivalent.,Show the Euclidean metric and maximum metric are strongly equivalent.,,"I need to show that the Euclidean metric and maximum metric (or square metric??) are strongly equivalent. I have no idea how to start this proof. Any help? $d_1, d_2$ are called strongly equivalent if there exist positive constants $K, M$ such that for all $x, y\in X$: $Md_1(x,y)\leq d_2(x,y)\leq Kd_1(x,y)$","I need to show that the Euclidean metric and maximum metric (or square metric??) are strongly equivalent. I have no idea how to start this proof. Any help? $d_1, d_2$ are called strongly equivalent if there exist positive constants $K, M$ such that for all $x, y\in X$: $Md_1(x,y)\leq d_2(x,y)\leq Kd_1(x,y)$",,"['real-analysis', 'general-topology', 'metric-spaces']"
48,Show the series is convergent and find the limit,Show the series is convergent and find the limit,,Sequence of real numbers $a_n$ defined recursively with $a_1=1/2$ and $a_{n+1}=\frac{a_n^2}{a_n^2-a_n+1}$  for all $n \geq 1$. Show that $\sum_{n=1}^\infty a_n$ is convergent and find its limit. I have tried to convert the recursive form to explicit form but it's too difficult.,Sequence of real numbers $a_n$ defined recursively with $a_1=1/2$ and $a_{n+1}=\frac{a_n^2}{a_n^2-a_n+1}$  for all $n \geq 1$. Show that $\sum_{n=1}^\infty a_n$ is convergent and find its limit. I have tried to convert the recursive form to explicit form but it's too difficult.,,"['real-analysis', 'sequences-and-series']"
49,Limit of $(\cos{xe^x} - \ln(1-x) -x)^{\frac{1}{x^3}}$,Limit of,(\cos{xe^x} - \ln(1-x) -x)^{\frac{1}{x^3}},"So I had the task to evaluate this limit $$ \lim_{x \to 0} (\cos{(xe^x)} - \ln(1-x) -x)^{\frac{1}{x^3}}$$ I tried transforming it to: $$ e^{\lim_{x \to 0} \frac{ \ln{(\cos{xe^x} - \ln(1-x) -x)}}{x^3}}$$ So I could use L'hospital's rule, but this would just be impossible to evaluate without a mistake. Also, I just noticed this expression is not of form $\frac{0}{0}$. Any solution is good ( I would like to avoid Taylor series but if that's the only way then that's okay). I had this task on a test today and I failed to do it.","So I had the task to evaluate this limit $$ \lim_{x \to 0} (\cos{(xe^x)} - \ln(1-x) -x)^{\frac{1}{x^3}}$$ I tried transforming it to: $$ e^{\lim_{x \to 0} \frac{ \ln{(\cos{xe^x} - \ln(1-x) -x)}}{x^3}}$$ So I could use L'hospital's rule, but this would just be impossible to evaluate without a mistake. Also, I just noticed this expression is not of form $\frac{0}{0}$. Any solution is good ( I would like to avoid Taylor series but if that's the only way then that's okay). I had this task on a test today and I failed to do it.",,"['calculus', 'real-analysis', 'limits']"
50,limit of a sequence $\ln n\cdot(\sin n)^2$,limit of a sequence,\ln n\cdot(\sin n)^2,"It's easy to see that $\lim\limits_{x\to \infty}\ln x\cdot(\sin x)^2$ does not exist (sin can take 0 and 1 as values, so liminf is $0$, limsup is $\infty$). how about the limit of a sequence: $\lim\limits_{n\to \infty}\ln n\cdot(\sin n)^2$ ?","It's easy to see that $\lim\limits_{x\to \infty}\ln x\cdot(\sin x)^2$ does not exist (sin can take 0 and 1 as values, so liminf is $0$, limsup is $\infty$). how about the limit of a sequence: $\lim\limits_{n\to \infty}\ln n\cdot(\sin n)^2$ ?",,"['real-analysis', 'limits']"
51,"How to prove the set $\{\cos(n) \mid n \in \mathbb{N}\}$ is dense in $[-1,1]$ [duplicate]",How to prove the set  is dense in  [duplicate],"\{\cos(n) \mid n \in \mathbb{N}\} [-1,1]","This question already has answers here : Show that a set is dense in $[-1,1]$ (4 answers) Closed 9 years ago . Prove that the set  $\{\cos(n) \mid n \in \mathbb{N}\}$ is dense in $[-1,1]$.","This question already has answers here : Show that a set is dense in $[-1,1]$ (4 answers) Closed 9 years ago . Prove that the set  $\{\cos(n) \mid n \in \mathbb{N}\}$ is dense in $[-1,1]$.",,"['real-analysis', 'metric-spaces']"
52,"How prove $ \int_0^1 f(x)dx-\exp\left(\int_0^1\log(f(x)) dx\right)\le\max_{0\le x,y\le 1}\left(\sqrt{f(x)}-\sqrt{f(y)}\right)^2 $",How prove," \int_0^1 f(x)dx-\exp\left(\int_0^1\log(f(x)) dx\right)\le\max_{0\le x,y\le 1}\left(\sqrt{f(x)}-\sqrt{f(y)}\right)^2 ","Consider a continuous function  $f:[0,1]\to\mathbb{R}^{+}$. How show that $\int_0^1 f(x)dx-\exp\left(\int_0^1 \log(f(x)) dx\right)\le \max_{0\le x,y\le 1}\left(\sqrt{f(x)}-\sqrt{f(y)}\right)^2$?","Consider a continuous function  $f:[0,1]\to\mathbb{R}^{+}$. How show that $\int_0^1 f(x)dx-\exp\left(\int_0^1 \log(f(x)) dx\right)\le \max_{0\le x,y\le 1}\left(\sqrt{f(x)}-\sqrt{f(y)}\right)^2$?",,"['real-analysis', 'integration']"
53,A power series that converges for $|x| \leq 1$ and diverges otherwise.,A power series that converges for  and diverges otherwise.,|x| \leq 1,"I need to find a power series $\sum a_n z^n$ that converges for $|x| \leq  1$ and diverges otherwise. I think I have one I just want to be sure. So, the series: $\sum \frac{z^n}{n^2}$ has radius of convergence of 1.  So it converges when $|z| <1$ and diverges when $|z| >1$, correct? And we know it converges at $z= \pm 1$ by the comparison test, correct?  This part is where I'm having trouble with.  Could someone explain in detail how to use the comparison test with this? I know the comparison test says, ""if you have two series $\sum a_n$ and $\sum b_n$ with $a_n, b_n \geq0$ and $a_n \leq b_n$, then if $\sum b_n$ converges, then $\sum a_n$ converges."" But what other series would you use in the comparison test.  I also know that $|\frac{z^n}{n^2}|= \frac{1}{n^2}$.  Can you use this fact? Please help! This series would work correct?","I need to find a power series $\sum a_n z^n$ that converges for $|x| \leq  1$ and diverges otherwise. I think I have one I just want to be sure. So, the series: $\sum \frac{z^n}{n^2}$ has radius of convergence of 1.  So it converges when $|z| <1$ and diverges when $|z| >1$, correct? And we know it converges at $z= \pm 1$ by the comparison test, correct?  This part is where I'm having trouble with.  Could someone explain in detail how to use the comparison test with this? I know the comparison test says, ""if you have two series $\sum a_n$ and $\sum b_n$ with $a_n, b_n \geq0$ and $a_n \leq b_n$, then if $\sum b_n$ converges, then $\sum a_n$ converges."" But what other series would you use in the comparison test.  I also know that $|\frac{z^n}{n^2}|= \frac{1}{n^2}$.  Can you use this fact? Please help! This series would work correct?",,['real-analysis']
54,A question about infinities and pots of paint,A question about infinities and pots of paint,,"This question is inspired by https://math.stackexchange.com/a/1052384/66307 and quotes from it heavily. Take a countably infinite paint box; this means that it has one color of paint for each positive integer; we can therefore call the colors $C_1, C_2, $ and so on.  Take the set of real numbers, and imagine that each real number is painted with one of the colors of paint. Now ask the question: Are there three distinct real numbers $a,b,c$, all painted the same color, such that $$a+b=c$$ Must such  $a,b,c$, not all zero, exist regardless of how cleverly  the numbers are   actually colored?","This question is inspired by https://math.stackexchange.com/a/1052384/66307 and quotes from it heavily. Take a countably infinite paint box; this means that it has one color of paint for each positive integer; we can therefore call the colors $C_1, C_2, $ and so on.  Take the set of real numbers, and imagine that each real number is painted with one of the colors of paint. Now ask the question: Are there three distinct real numbers $a,b,c$, all painted the same color, such that $$a+b=c$$ Must such  $a,b,c$, not all zero, exist regardless of how cleverly  the numbers are   actually colored?",,"['real-analysis', 'logic']"
55,Pointwise limit of a sequence of continuous functions is discontinuous at most finitely/countably many points.,Pointwise limit of a sequence of continuous functions is discontinuous at most finitely/countably many points.,,"Let $\{f_{n}\}$ be a sequence of functions in $C[0,1]$ such that $f_{n}\to f$ pointwise. Then $f$ has at most finitely (or probably countably) many discontinuities. Is this statement TRUE or FALSE? Any kind of help will be appreciated.","Let $\{f_{n}\}$ be a sequence of functions in $C[0,1]$ such that $f_{n}\to f$ pointwise. Then $f$ has at most finitely (or probably countably) many discontinuities. Is this statement TRUE or FALSE? Any kind of help will be appreciated.",,"['real-analysis', 'sequences-and-series', 'convergence-divergence', 'continuity']"
56,Showing a function bijective,Showing a function bijective,,"Given that $f:\mathbb R\to \mathbb R$ is continuous and $\vert x-y\vert \leq \vert f(x)-f(y)\vert$. We need to show that $f$ is bijective. It is easy to show that $f$ is one one. But how to show that $f$ is onto? I wanted to use intermediate value theorem. For that if $z\in \mathbb R$, I need to find $x,y\in \mathbb R$ such that $f(x)<z<f(y)$. But how to do so? Any hint will be appreciated.","Given that $f:\mathbb R\to \mathbb R$ is continuous and $\vert x-y\vert \leq \vert f(x)-f(y)\vert$. We need to show that $f$ is bijective. It is easy to show that $f$ is one one. But how to show that $f$ is onto? I wanted to use intermediate value theorem. For that if $z\in \mathbb R$, I need to find $x,y\in \mathbb R$ such that $f(x)<z<f(y)$. But how to do so? Any hint will be appreciated.",,"['real-analysis', 'analysis']"
57,Cauchy sequences. Show that $(x_n)$ is Cauchy.,Cauchy sequences. Show that  is Cauchy.,(x_n),Let $(x_n)$ and $(y_n)$ be sequences such that $\lim y_n = 0$. Suppose that for all $k \in  \Bbb N$ and all $m ≥ k$ we have $|x_m − x_k| ≤ y_k$. Show that $(x_n)$ is Cauchy. I need a little guidance on how to approach the problem.  As I see this is the same definition of Cauchy sequences.   But I do not see how to connect everything in a logic sequence in order to have a rigorous proof. My attempt of reasoning I started first defining the $\lim$ of $y_n$. For every $\varepsilon>0$ exists $N$ s.t. $n>N$   $|y_n|<\varepsilon$ for all $n>N$ Then I see that all terms of $y_n$ get smaller and smaller as $n$ gets larger.  So distance between $x_m$ and $x_k$ gets smaller as the terms get bigger. But one thing that puts me off is that  $m ≥ k$ and $| x_m − x_k| ≤ y_k$ why are they $\leq$? Thanks for help in advance',Let $(x_n)$ and $(y_n)$ be sequences such that $\lim y_n = 0$. Suppose that for all $k \in  \Bbb N$ and all $m ≥ k$ we have $|x_m − x_k| ≤ y_k$. Show that $(x_n)$ is Cauchy. I need a little guidance on how to approach the problem.  As I see this is the same definition of Cauchy sequences.   But I do not see how to connect everything in a logic sequence in order to have a rigorous proof. My attempt of reasoning I started first defining the $\lim$ of $y_n$. For every $\varepsilon>0$ exists $N$ s.t. $n>N$   $|y_n|<\varepsilon$ for all $n>N$ Then I see that all terms of $y_n$ get smaller and smaller as $n$ gets larger.  So distance between $x_m$ and $x_k$ gets smaller as the terms get bigger. But one thing that puts me off is that  $m ≥ k$ and $| x_m − x_k| ≤ y_k$ why are they $\leq$? Thanks for help in advance',,"['real-analysis', 'cauchy-sequences']"
58,"Is $d(x,y) = (x-y)^2$ a metric on $\Bbb R$?",Is  a metric on ?,"d(x,y) = (x-y)^2 \Bbb R","For $x,y,z \in \Bbb R$, define $d(x,y):= (x-y)^2$ Is this a metric on $\Bbb R$? It's clear that $d(x,x)=0$ and $d(x,y)=d(y,x)$ for all $x,y \in \Bbb R$. The triangle inequality seems to have a contradiction [$d(x,z) \leq d(x,y) +d(y,z)$] If I let $x=1$, $y=0$ and $z=-1$, then I will have $(1+1)^2 = 4 > (1-0)^2 + (0+1)^2 = 2$. So is this $d$ a metric? If it's, how can I prove it?","For $x,y,z \in \Bbb R$, define $d(x,y):= (x-y)^2$ Is this a metric on $\Bbb R$? It's clear that $d(x,x)=0$ and $d(x,y)=d(y,x)$ for all $x,y \in \Bbb R$. The triangle inequality seems to have a contradiction [$d(x,z) \leq d(x,y) +d(y,z)$] If I let $x=1$, $y=0$ and $z=-1$, then I will have $(1+1)^2 = 4 > (1-0)^2 + (0+1)^2 = 2$. So is this $d$ a metric? If it's, how can I prove it?",,"['real-analysis', 'general-topology', 'analysis']"
59,Characterization of measurability by closed sets.,Characterization of measurability by closed sets.,,"If $E \subseteq \Bbb R$ is measurable, then for all $\epsilon > 0$, exists $F \subseteq \Bbb R$ closed such that $F \subseteq E$ and ${\frak m}^*(E \setminus F) < \epsilon$. I have already made the characterization by open sets, that is, if $E$ is measurable, for all $\epsilon > 0$, exists $O \subseteq \Bbb R$ open, such that $E \subseteq O$ and ${\frak m}^*(O \setminus E) < \epsilon$. For that characterization, I used that for all $A \subset \Bbb R$, given $\epsilon > 0$, exists $O \subseteq \Bbb R$ open such that ${\frak m}^*O < {\frak m}^*A + \epsilon$. Just take intervals $(I_j)_{j \geq 1}$ covering $A$ such that $\sum_{j \geq 1} \ell (I_j) < {\frak m}^*A + \epsilon$, so let $O = \bigcup_{j \geq 1} I_j$ if ${\frak m}^*A < +\infty$, and $O = \Bbb R$ if ${\frak m}^*A = +\infty $. I was trying to ""copy"" this proof, and so I would begin proving the lemma (which I firmly believe that is true): For all $A \subseteq \Bbb R$, given $\epsilon > 0$, exists $F \subseteq \Bbb R$ closed such that ${\frak m}^*A < {\frak m}^*F + \epsilon$. Well, if ${\frak m}^*A = 0$, take $F = \varnothing$. But apart from this, my attempt isn't going well. Surely, if ${\frak m}^*A > 0$, we can take intervals $(I_j)_{j \geq 1}$ covering $A$ such that $\sum_{j \geq 1} \ell (I_j) < {\frak m}^*A - \epsilon$, but so what? Infinite union of closed sets need not be closed. Another idea is to use the result of open sets for $A^c$, so that $A^c \subseteq O \implies O^c \subseteq A $, then take $F = O^c$. But I don't see quickly how to get the relation between the measures. Am I going in the right way? Can someone help me please?","If $E \subseteq \Bbb R$ is measurable, then for all $\epsilon > 0$, exists $F \subseteq \Bbb R$ closed such that $F \subseteq E$ and ${\frak m}^*(E \setminus F) < \epsilon$. I have already made the characterization by open sets, that is, if $E$ is measurable, for all $\epsilon > 0$, exists $O \subseteq \Bbb R$ open, such that $E \subseteq O$ and ${\frak m}^*(O \setminus E) < \epsilon$. For that characterization, I used that for all $A \subset \Bbb R$, given $\epsilon > 0$, exists $O \subseteq \Bbb R$ open such that ${\frak m}^*O < {\frak m}^*A + \epsilon$. Just take intervals $(I_j)_{j \geq 1}$ covering $A$ such that $\sum_{j \geq 1} \ell (I_j) < {\frak m}^*A + \epsilon$, so let $O = \bigcup_{j \geq 1} I_j$ if ${\frak m}^*A < +\infty$, and $O = \Bbb R$ if ${\frak m}^*A = +\infty $. I was trying to ""copy"" this proof, and so I would begin proving the lemma (which I firmly believe that is true): For all $A \subseteq \Bbb R$, given $\epsilon > 0$, exists $F \subseteq \Bbb R$ closed such that ${\frak m}^*A < {\frak m}^*F + \epsilon$. Well, if ${\frak m}^*A = 0$, take $F = \varnothing$. But apart from this, my attempt isn't going well. Surely, if ${\frak m}^*A > 0$, we can take intervals $(I_j)_{j \geq 1}$ covering $A$ such that $\sum_{j \geq 1} \ell (I_j) < {\frak m}^*A - \epsilon$, but so what? Infinite union of closed sets need not be closed. Another idea is to use the result of open sets for $A^c$, so that $A^c \subseteq O \implies O^c \subseteq A $, then take $F = O^c$. But I don't see quickly how to get the relation between the measures. Am I going in the right way? Can someone help me please?",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
60,Characteristic function of Cantor set is Riemann integrable,Characteristic function of Cantor set is Riemann integrable,,"I want to prove that the characteristic function of the Cantor set is Riemann integrable on $[0,1]$ . Could somebody please tell me if my proof is correct? Let $f$ be the characteristic function of the Cantor set and let $L(f,P)$ denote the lower Riemann sum and $U(f,P)$ the upper Riemann sum with respect to partition $P$ . First note that in any subinterval of $[0,1]$ there are points that are not in $C$ , hence $L(f,P)=0$ for all partitions $P$ , and hence $L(f)=0$ . Let $\varepsilon > 0$ . If $C = \bigcap_n C_n$ then let $n$ be such that the length of $C_n$ is smaller than $\varepsilon$ : $|C_n|<\varepsilon$ . (Here $C_n$ is a union of $2^n$ closed intervals of length ${1 \over 3^n}$ .) Let $P$ be the partition consisting of the endpoints of the intervals in $C_n$ and let $I_k$ denote the intervals in $C_n$ . Then $$ U(f,P) = \sum_k |I_k| < \varepsilon$$ Hence $U(f) = 0$ and therefore $\int_0^1 f = 0$ . Please note that I am not looking for a proof. I am asking if somebody could please check my proof.","I want to prove that the characteristic function of the Cantor set is Riemann integrable on . Could somebody please tell me if my proof is correct? Let be the characteristic function of the Cantor set and let denote the lower Riemann sum and the upper Riemann sum with respect to partition . First note that in any subinterval of there are points that are not in , hence for all partitions , and hence . Let . If then let be such that the length of is smaller than : . (Here is a union of closed intervals of length .) Let be the partition consisting of the endpoints of the intervals in and let denote the intervals in . Then Hence and therefore . Please note that I am not looking for a proof. I am asking if somebody could please check my proof.","[0,1] f L(f,P) U(f,P) P [0,1] C L(f,P)=0 P L(f)=0 \varepsilon > 0 C = \bigcap_n C_n n C_n \varepsilon |C_n|<\varepsilon C_n 2^n {1 \over 3^n} P C_n I_k C_n  U(f,P) = \sum_k |I_k| < \varepsilon U(f) = 0 \int_0^1 f = 0","['real-analysis', 'solution-verification']"
61,How to prove that for $n \in \mathbb{N}$ we have $\sum_{k=2}^n \frac{1}{k}\leq \ln(n) \leq \sum_{k=1}^{n-1} \frac{1}{k}$,How to prove that for  we have,n \in \mathbb{N} \sum_{k=2}^n \frac{1}{k}\leq \ln(n) \leq \sum_{k=1}^{n-1} \frac{1}{k},Prove that for $n \in \mathbb{N}$ we have $\sum_{k=2}^n \frac{1}{k}\leq \ln(n) \leq \sum_{k=1}^{n-1} \frac{1}{k}$ by using Riemann integral?,Prove that for $n \in \mathbb{N}$ we have $\sum_{k=2}^n \frac{1}{k}\leq \ln(n) \leq \sum_{k=1}^{n-1} \frac{1}{k}$ by using Riemann integral?,,"['real-analysis', 'integration', 'inequality', 'harmonic-numbers']"
62,Quesrion about a complex integral: I am struggling with $\int _0^1 {\ln x\over{1-x^2}}dx=-{\pi^{2}\over 8}$,Quesrion about a complex integral: I am struggling with,\int _0^1 {\ln x\over{1-x^2}}dx=-{\pi^{2}\over 8},"How do I prove $$\int _0^1 {\ln x\over{1-x^2}}=-{\pi^{2}\over 8}$$ My solution: If we can prove $\int _0^1 {\ln x\over{1-x^2}}= \lim_{n\to \infty} \int _0^1\ln(x)(1+x^2+x^4+......+x^{2n})$ ,then I think we can prove the equality above. because the right hand = $\lim_{n\to \infty} $$(-1-{1\over{3^2}}-{1\over 5^2}......-{1\over{2n+1}^2})={-\pi^{2}\over 8}$ . Can someone help me prove why $\int _0^1 {\ln x\over{1-x^2}}= \lim_{n\to \infty} \int _0^1\ln(x)(1+x^2+x^4+......+x^{2n})$ ? I don't know how to prove it? Or, can someone use other methods to solve the equality above?","How do I prove My solution: If we can prove ,then I think we can prove the equality above. because the right hand = . Can someone help me prove why ? I don't know how to prove it? Or, can someone use other methods to solve the equality above?",\int _0^1 {\ln x\over{1-x^2}}=-{\pi^{2}\over 8} \int _0^1 {\ln x\over{1-x^2}}= \lim_{n\to \infty} \int _0^1\ln(x)(1+x^2+x^4+......+x^{2n}) \lim_{n\to \infty} (-1-{1\over{3^2}}-{1\over 5^2}......-{1\over{2n+1}^2})={-\pi^{2}\over 8} \int _0^1 {\ln x\over{1-x^2}}= \lim_{n\to \infty} \int _0^1\ln(x)(1+x^2+x^4+......+x^{2n}),"['real-analysis', 'integration', 'sequences-and-series', 'analysis', 'improper-integrals']"
63,A circular proof in Rudin that $\mathbb{R}$ is a field.,A circular proof in Rudin that  is a field.,\mathbb{R},"Today I'm afraid, I found a circular reasoning in Rudin's Principles of Mathematical Analysis( I found  no errata that mentions this). Before actually going through the actual question, I have compiled two documents one on fields (hereafter called document 1) and an incomplete document on construction of real numbers from rational numbers (hereafter called document 2), the second document contains the definitions and proofs that satisfy the criteria that are done before proving that $\mathbb{R}$ is a field (All theorems except the last one is okay). In the document 2 , the last theorem, I have tried to prove is that $\mathbb{R}$ satisfies all the Field axioms for addition.(Referring to step 4 of Apendix in chapter 1 of rudin's book) Rudin's book could only prove the Axiom-5 using the theorem 1.20(in Rudin's book) popularly known as the Archemedian property(You can see page-4 of my document 2 for a proof how it is done). Now I shall show that the proof of theorem 1.20 requires the fact that $\mathbb{R}$ satisfies at least the axioms of addition. Theorem 1.20 (a) If $x\in \mathbb{R}$, $y\in \mathbb{R}$, and $x>0$, then there is a positive integer $n$ such that $nx>y$. The proof follows from least-upper-bound property of real numbers(this can be proven with no problem, see my document) to claim that the set $A=\{nx:n\in\mathbb{N}\}$, if doesn't satisfy the theorem has a supremum and hence take $\alpha-x<\alpha$ and using definition of supremum to arrive at a contradiction. Now existence of $-x$ follows from A5 of field, and the property that If $y<z$ then $x+y<x+z$ where $x,y,z$ belong to a field(a part of definition of ordered field) is required for proving that $\alpha-x<\alpha$, to show that $\mathbb{R}$ satisfies this property we require the proposition 1.14-a (in rudin) or proposition 1-a (in document 1) whose proof again requires axioms A4, A5, A3, A2 and A1, existence of A5 is at stake right now. So to conclude things proving that $\mathbb{R}$ satisfies axiom A5 and the archemedian property are circular proofs. This is the question: Is there any way you can prove A5 without using archemedian property? ( I doubt this) If the above mentioned one is not possible, is there any way to prove archemedian property without help of field axiom A5? ( I seriously doubt this) Or am I missing something? If you have not understood my explanation then continue reading. I'm sorry, I admit I am bad at explaining things, so here is a problem I am encountering. See my document 2, page 4 theorem 2. Proving $A5$ requires a theorem called archemedian property, can you add the proof of Archemedian property, before the beginning of the theorem so that I can complete the proof of theorem 2. Note: To talk about the seriousness of the matter, to me right now the whole properties of $\mathbb{R}$ is at stake, especially the universally accepted fact that $\mathbb{R}$ is a field, an ordered one.","Today I'm afraid, I found a circular reasoning in Rudin's Principles of Mathematical Analysis( I found  no errata that mentions this). Before actually going through the actual question, I have compiled two documents one on fields (hereafter called document 1) and an incomplete document on construction of real numbers from rational numbers (hereafter called document 2), the second document contains the definitions and proofs that satisfy the criteria that are done before proving that $\mathbb{R}$ is a field (All theorems except the last one is okay). In the document 2 , the last theorem, I have tried to prove is that $\mathbb{R}$ satisfies all the Field axioms for addition.(Referring to step 4 of Apendix in chapter 1 of rudin's book) Rudin's book could only prove the Axiom-5 using the theorem 1.20(in Rudin's book) popularly known as the Archemedian property(You can see page-4 of my document 2 for a proof how it is done). Now I shall show that the proof of theorem 1.20 requires the fact that $\mathbb{R}$ satisfies at least the axioms of addition. Theorem 1.20 (a) If $x\in \mathbb{R}$, $y\in \mathbb{R}$, and $x>0$, then there is a positive integer $n$ such that $nx>y$. The proof follows from least-upper-bound property of real numbers(this can be proven with no problem, see my document) to claim that the set $A=\{nx:n\in\mathbb{N}\}$, if doesn't satisfy the theorem has a supremum and hence take $\alpha-x<\alpha$ and using definition of supremum to arrive at a contradiction. Now existence of $-x$ follows from A5 of field, and the property that If $y<z$ then $x+y<x+z$ where $x,y,z$ belong to a field(a part of definition of ordered field) is required for proving that $\alpha-x<\alpha$, to show that $\mathbb{R}$ satisfies this property we require the proposition 1.14-a (in rudin) or proposition 1-a (in document 1) whose proof again requires axioms A4, A5, A3, A2 and A1, existence of A5 is at stake right now. So to conclude things proving that $\mathbb{R}$ satisfies axiom A5 and the archemedian property are circular proofs. This is the question: Is there any way you can prove A5 without using archemedian property? ( I doubt this) If the above mentioned one is not possible, is there any way to prove archemedian property without help of field axiom A5? ( I seriously doubt this) Or am I missing something? If you have not understood my explanation then continue reading. I'm sorry, I admit I am bad at explaining things, so here is a problem I am encountering. See my document 2, page 4 theorem 2. Proving $A5$ requires a theorem called archemedian property, can you add the proof of Archemedian property, before the beginning of the theorem so that I can complete the proof of theorem 2. Note: To talk about the seriousness of the matter, to me right now the whole properties of $\mathbb{R}$ is at stake, especially the universally accepted fact that $\mathbb{R}$ is a field, an ordered one.",,"['real-analysis', 'definition', 'real-numbers']"
64,Does scaling lead to weak convergence to the null function?,Does scaling lead to weak convergence to the null function?,,"Let $f\in L^p(\mathbb{R}^d)$, with $1<p<\infty$. Is it true that    $$\lambda^{\frac{d}{p}}f(\lambda x ) \rightharpoonup 0\quad \text{ weakly in }L^p\text{ as }\lambda\to+\infty?$$ One has the easy case for $f\in L^{p-\epsilon}\cap L^p$. This condition allows the use of Hölder's inequality as follows (here $\frac{1}{p}+\frac{1}{p'}=1$ and $\phi$ is a continuous function with compact support):  $$\begin{split}\left\lvert \int_{\mathbb{R}^d} \lambda^{\frac{d}{p}} f(\lambda x)\phi(x)\, dx\right\rvert& \le \lambda^{\frac{d}{p}}\lVert f(\lambda x)\rVert_{L^{p-\epsilon}}\lVert \phi\rVert_{L^{(p-\epsilon)'}} \\  &= \lambda ^{\frac{d}{p}- \frac{d}{p-\epsilon} } \lVert f\rVert_{L^{p-\epsilon}}\lVert\phi\rVert_{L^{(p-\epsilon)'}}\to 0. \end{split} $$ But what happens if that condition is removed? I carried out an explicit check on the standard example of a function $f$ that belongs to $L^2(\mathbb{R})$ and does not belong to $L^{2-\epsilon}(\mathbb{R})$ for any $\epsilon > 0$, namely  $$f(x)=\begin{cases} 0 , & x<2 \\ \frac{1}{\sqrt{x}\log x}, & x \ge 2\end{cases}.$$ Taking $\phi=\chi_{[a, b]}$ with $2<a<b$ one has  $$\left \lvert \int_{\mathbb{R}} \lambda^{\frac{1}{2}}f(\lambda x)\phi(x)\, dx \right\rvert  = \dfrac{  \int_{\lambda a }^{\lambda b} \frac{dy}{\sqrt{y}\log(y)} } {\lambda^{\frac{1}{2}}}, $$ and an application of l'Hôpital's rule shows that the right hand side tends to $0$ as $\lambda \to \infty$. This implies weak convergence to $0$ by a standard density argument. This seems to point towards an affirmative answer to the question in the gray box.","Let $f\in L^p(\mathbb{R}^d)$, with $1<p<\infty$. Is it true that    $$\lambda^{\frac{d}{p}}f(\lambda x ) \rightharpoonup 0\quad \text{ weakly in }L^p\text{ as }\lambda\to+\infty?$$ One has the easy case for $f\in L^{p-\epsilon}\cap L^p$. This condition allows the use of Hölder's inequality as follows (here $\frac{1}{p}+\frac{1}{p'}=1$ and $\phi$ is a continuous function with compact support):  $$\begin{split}\left\lvert \int_{\mathbb{R}^d} \lambda^{\frac{d}{p}} f(\lambda x)\phi(x)\, dx\right\rvert& \le \lambda^{\frac{d}{p}}\lVert f(\lambda x)\rVert_{L^{p-\epsilon}}\lVert \phi\rVert_{L^{(p-\epsilon)'}} \\  &= \lambda ^{\frac{d}{p}- \frac{d}{p-\epsilon} } \lVert f\rVert_{L^{p-\epsilon}}\lVert\phi\rVert_{L^{(p-\epsilon)'}}\to 0. \end{split} $$ But what happens if that condition is removed? I carried out an explicit check on the standard example of a function $f$ that belongs to $L^2(\mathbb{R})$ and does not belong to $L^{2-\epsilon}(\mathbb{R})$ for any $\epsilon > 0$, namely  $$f(x)=\begin{cases} 0 , & x<2 \\ \frac{1}{\sqrt{x}\log x}, & x \ge 2\end{cases}.$$ Taking $\phi=\chi_{[a, b]}$ with $2<a<b$ one has  $$\left \lvert \int_{\mathbb{R}} \lambda^{\frac{1}{2}}f(\lambda x)\phi(x)\, dx \right\rvert  = \dfrac{  \int_{\lambda a }^{\lambda b} \frac{dy}{\sqrt{y}\log(y)} } {\lambda^{\frac{1}{2}}}, $$ and an application of l'Hôpital's rule shows that the right hand side tends to $0$ as $\lambda \to \infty$. This implies weak convergence to $0$ by a standard density argument. This seems to point towards an affirmative answer to the question in the gray box.",,"['real-analysis', 'functional-analysis']"
65,"Prove that there exists a sequence $\{x_{n}\}$ such that for every $n\,\quad f_{n}$ has a global maximum",Prove that there exists a sequence  such that for every  has a global maximum,"\{x_{n}\} n\,\quad f_{n}","For every positive integer $n$ consider function $f_{n}(x)=n^{\sin x}+n^{\cos x},\ x \in \mathbb{R}$ . Prove that there exists a sequence $\{x_{n}\}$ such that for every $n,\  f_{n}$ has a global maximum at $x_{n}$ and $x_{n}\to 0$ as $n \to\infty$ . $(f_n(x))'=n^{\sin(x)}\cos(x)\ln(n)-n^{\cos(x)}\sin(x)\ln(n)$ it doesn't seems very nice. I am stuck (inexperience I think..). Any hints would be very appreciated. Thanks.",For every positive integer consider function . Prove that there exists a sequence such that for every has a global maximum at and as . it doesn't seems very nice. I am stuck (inexperience I think..). Any hints would be very appreciated. Thanks.,"n f_{n}(x)=n^{\sin x}+n^{\cos x},\ x \in \mathbb{R} \{x_{n}\} n,\  f_{n} x_{n} x_{n}\to 0 n \to\infty (f_n(x))'=n^{\sin(x)}\cos(x)\ln(n)-n^{\cos(x)}\sin(x)\ln(n)","['real-analysis', 'functions']"
66,Discontinuous Sobolev Function,Discontinuous Sobolev Function,,"I'm trying to show that there's an $f \in H^1(\mathbb{R}^2)$ which is not ae equal to a continuous function.  Per a couple of suggestions, I've decided to look at the function $f(x) = (-\log(|x|))^{\frac{1}{2}}$ for $|x| < 1$ and $0$ otherwise.  It's not hard to see that $f \in L^2(\mathbb{R}^2)$ and that $f$ is differentiable everywhere on $\mathbb{R}^2 \setminus (\mathbb{S}^1 \cup \{0\})$. In $|x| < 1$, $\frac{\partial f}{\partial x_j}(x) = \frac{-x_j}{|x|^2 (-\log(|x|)^{\frac{1}{2}})} $.  But, unless I'm missing something really obvious (and I think I am!), this isn't square integrable on $|x| < 1$ (integrating in polar coordinates reduces to evaluating something of the form $\int_0^1 \frac{1}{r \log(r)} dr$.  I've also looked at $( - \log(|x|)^{\alpha}$ for various values of $\alpha$, but none of these functions seem to have a derivative which is square integrable on $|x| < 1$. Can someone point me in the right direction here? Edit: Suppose that $f(x) = (- \log(|x|))^{\alpha}$ for $|x| < \frac{1}{2}$, say.  Then for $f \in L^2$, we need to have that $\int_{B(0; \frac{1}{2})} (f(x))^2 < \infty$.  Integrating in polar coordinates, that is $\displaystyle \int_0^{\frac{1}{2}} r (-\log(r))^{2 \alpha} dr < \infty$.  So we can use any value of $\alpha > 0$ here. Next, in $\{|x| < \frac{1}{2}\}$, $\frac{\partial f}{\partial x_j}(x) = \alpha (-\log(|x|))^{\alpha - 1} (\frac{-1}{|x|}) \frac{x_j}{|x|} = \frac{ - \alpha x_j (-\log(|x|))^{\alpha - 1}}{|x|^2}$. For this to be in $L^2$, I'd need that $\displaystyle -\infty >  \alpha\pi \int_0^{\frac{1}{2}} r(\frac{r (-\log(r))^{\alpha - 1}}{r^2})^2 dr  = -\alpha \pi \int_0^{\frac{1}{2}} \frac{1}{r} (-\log(r))^{2\alpha - 2} dr$ but that never happens for any value of $\alpha$ (that's easy to see by making the substitution $u = (- \log(r))$. So I must be doing something horribly wrong here, but I don't see where.","I'm trying to show that there's an $f \in H^1(\mathbb{R}^2)$ which is not ae equal to a continuous function.  Per a couple of suggestions, I've decided to look at the function $f(x) = (-\log(|x|))^{\frac{1}{2}}$ for $|x| < 1$ and $0$ otherwise.  It's not hard to see that $f \in L^2(\mathbb{R}^2)$ and that $f$ is differentiable everywhere on $\mathbb{R}^2 \setminus (\mathbb{S}^1 \cup \{0\})$. In $|x| < 1$, $\frac{\partial f}{\partial x_j}(x) = \frac{-x_j}{|x|^2 (-\log(|x|)^{\frac{1}{2}})} $.  But, unless I'm missing something really obvious (and I think I am!), this isn't square integrable on $|x| < 1$ (integrating in polar coordinates reduces to evaluating something of the form $\int_0^1 \frac{1}{r \log(r)} dr$.  I've also looked at $( - \log(|x|)^{\alpha}$ for various values of $\alpha$, but none of these functions seem to have a derivative which is square integrable on $|x| < 1$. Can someone point me in the right direction here? Edit: Suppose that $f(x) = (- \log(|x|))^{\alpha}$ for $|x| < \frac{1}{2}$, say.  Then for $f \in L^2$, we need to have that $\int_{B(0; \frac{1}{2})} (f(x))^2 < \infty$.  Integrating in polar coordinates, that is $\displaystyle \int_0^{\frac{1}{2}} r (-\log(r))^{2 \alpha} dr < \infty$.  So we can use any value of $\alpha > 0$ here. Next, in $\{|x| < \frac{1}{2}\}$, $\frac{\partial f}{\partial x_j}(x) = \alpha (-\log(|x|))^{\alpha - 1} (\frac{-1}{|x|}) \frac{x_j}{|x|} = \frac{ - \alpha x_j (-\log(|x|))^{\alpha - 1}}{|x|^2}$. For this to be in $L^2$, I'd need that $\displaystyle -\infty >  \alpha\pi \int_0^{\frac{1}{2}} r(\frac{r (-\log(r))^{\alpha - 1}}{r^2})^2 dr  = -\alpha \pi \int_0^{\frac{1}{2}} \frac{1}{r} (-\log(r))^{2\alpha - 2} dr$ but that never happens for any value of $\alpha$ (that's easy to see by making the substitution $u = (- \log(r))$. So I must be doing something horribly wrong here, but I don't see where.",,"['real-analysis', 'sobolev-spaces']"
67,Question about Normed vector space.,Question about Normed vector space.,,"Here is the definition of a normed vector space my book uses: And here is a remark I do not understand: I do not understand that a sequence can converge to a vector in one norm, and not the other. For instance: Lets say $s_n$ converges to $u$ with the $\|\|_1$ -norm. From definition 4.5.2 (i) we must have that $s_n$ becomes closer and closer to $u$ . Why is it that it could fail in the other norm, when it can become as close as we want in the first norm?Are there any simple examples of this phenomenon? PS:I know that they say we will see examples of this later in the book, but what comes later is too hard for me to udnerstand now.","Here is the definition of a normed vector space my book uses: And here is a remark I do not understand: I do not understand that a sequence can converge to a vector in one norm, and not the other. For instance: Lets say converges to with the -norm. From definition 4.5.2 (i) we must have that becomes closer and closer to . Why is it that it could fail in the other norm, when it can become as close as we want in the first norm?Are there any simple examples of this phenomenon? PS:I know that they say we will see examples of this later in the book, but what comes later is too hard for me to udnerstand now.",s_n u \|\|_1 s_n u,"['real-analysis', 'vector-spaces', 'metric-spaces', 'normed-spaces']"
68,Integrability of function and its Fourier transform implies differentiabilty,Integrability of function and its Fourier transform implies differentiabilty,,"Is the the following  true: ""Assume $f\in L^1[0,1]$ and $\hat{f}\in L^1(\mathbb{R})$, then $f$ is differentiable a.e""","Is the the following  true: ""Assume $f\in L^1[0,1]$ and $\hat{f}\in L^1(\mathbb{R})$, then $f$ is differentiable a.e""",,"['real-analysis', 'fourier-analysis', 'harmonic-analysis']"
69,Relation between Right Riemann sum and definite integral,Relation between Right Riemann sum and definite integral,,"Let a partition $\{t_0,\ldots,t_n\}$ of the interval $[a,b]$ and let $f$ an integrable function. (we may also assume that $f$ is differentiable on $[a,b]$) We know that the Right Riemann sum is $$R(f)=\sum_{i=1}^n f(t_{i})(t_i-t_{i-1})$$ What's the relation ( in order sense terms) between $R(f)$ and the integral $\displaystyle \int_a^b f(x)dx$? Can I say that there exists a positive constant $C>0$ such that $\displaystyle \int_a^b f(x)dx\leq C\cdot R(f)$? I know that if $f$ is increasing function, this is correct, but in the general case, i may say that? EDIT: Assume that $f(t)\geq 0$ on $[a,b]$","Let a partition $\{t_0,\ldots,t_n\}$ of the interval $[a,b]$ and let $f$ an integrable function. (we may also assume that $f$ is differentiable on $[a,b]$) We know that the Right Riemann sum is $$R(f)=\sum_{i=1}^n f(t_{i})(t_i-t_{i-1})$$ What's the relation ( in order sense terms) between $R(f)$ and the integral $\displaystyle \int_a^b f(x)dx$? Can I say that there exists a positive constant $C>0$ such that $\displaystyle \int_a^b f(x)dx\leq C\cdot R(f)$? I know that if $f$ is increasing function, this is correct, but in the general case, i may say that? EDIT: Assume that $f(t)\geq 0$ on $[a,b]$",,"['real-analysis', 'integration', 'definite-integrals', 'riemann-sum']"
70,Real numbers via equivalence classes of Cauchy sequences and the Completeness Axiom,Real numbers via equivalence classes of Cauchy sequences and the Completeness Axiom,,"Hi so there's a question in my elementary real analysis course I'm a little bugged about. The question goes: Use the definition of the real numbers via equivalence classes of Cauchy sequences to prove the Completeness Axiom. I tried using proof by contradiction by first stating Completeness is false. Then every nonempty subset S of R that is bounded above has no least upper bound. I wanted to construct a Cauchy sequence of descending upperbounds and show that these upperbounds converge to real number which would be the least upperbound, resulting in a contradiction. Is this a valid method? If so, I'm a little stuck on the details of such a Cauchy sequence. Any help is much appreciated.","Hi so there's a question in my elementary real analysis course I'm a little bugged about. The question goes: Use the definition of the real numbers via equivalence classes of Cauchy sequences to prove the Completeness Axiom. I tried using proof by contradiction by first stating Completeness is false. Then every nonempty subset S of R that is bounded above has no least upper bound. I wanted to construct a Cauchy sequence of descending upperbounds and show that these upperbounds converge to real number which would be the least upperbound, resulting in a contradiction. Is this a valid method? If so, I'm a little stuck on the details of such a Cauchy sequence. Any help is much appreciated.",,['real-analysis']
71,A generalization of Cauchy's condensation test,A generalization of Cauchy's condensation test,,"If $(a_n)$ is a decreasing sequence of positive real numbers and $(u_n)$ is a strictly increasing sequence of positive integers such that $ \dfrac {u_{n+2}-u_{n+1}}{u_{n+1}-u_n}$ is bounded , then how do we prove that the convergence of $\sum_{n=1}^ \infty (u_{n+1}-u_n)a_{u_n}$ implies the convergence of $\sum_{n=1}^ \infty a_n$ ?","If $(a_n)$ is a decreasing sequence of positive real numbers and $(u_n)$ is a strictly increasing sequence of positive integers such that $ \dfrac {u_{n+2}-u_{n+1}}{u_{n+1}-u_n}$ is bounded , then how do we prove that the convergence of $\sum_{n=1}^ \infty (u_{n+1}-u_n)a_{u_n}$ implies the convergence of $\sum_{n=1}^ \infty a_n$ ?",,['real-analysis']
72,Invariance of the Lebesgue integral.,Invariance of the Lebesgue integral.,,"Problem Let $f\in L^1(\mathbb{R})$. Show that $\int_{\mathbb{R}}f(x)dx=\int_{\mathbb{R}}f(x-\frac{1}{x})dx$. Discussion I know the Lebesgue integral is translation invariant (as the Lebesgue measure is), but I have never encountered the above invariance. I thought maybe if I rewrote both integrals as the measure of a set I could show both sets had the same measure, or I could use a change of variables, but nothing has worked yet. The question is a small part of a bigger problem related to fourier transforms.","Problem Let $f\in L^1(\mathbb{R})$. Show that $\int_{\mathbb{R}}f(x)dx=\int_{\mathbb{R}}f(x-\frac{1}{x})dx$. Discussion I know the Lebesgue integral is translation invariant (as the Lebesgue measure is), but I have never encountered the above invariance. I thought maybe if I rewrote both integrals as the measure of a set I could show both sets had the same measure, or I could use a change of variables, but nothing has worked yet. The question is a small part of a bigger problem related to fourier transforms.",,"['real-analysis', 'lebesgue-integral', 'lebesgue-measure']"
73,Convergence from $L^p$ to $L^\infty$ [duplicate],Convergence from  to  [duplicate],L^p L^\infty,"This question already has answers here : Limit of $L^p$ norm (4 answers) Closed 10 years ago . If $f$ is a function such that $f \in L^\infty \cap L^ {p_0}$ where $L^\infty$ is the space of essentially bounded functions and $ 0 < p_0 < \infty$. Show that $ || f|| _{L^p} \to ||f || _{L^\infty} $ as $ p \to \infty$. Where $|| f||_{L^\infty} $ is the least $M \in R$ such that $|f(x)| \le M$ for almost every $x \in X$. The hint says to use the monotone convergence theorem, but i can't even see any pointwise convergence of functions. Any help is appreciated.","This question already has answers here : Limit of $L^p$ norm (4 answers) Closed 10 years ago . If $f$ is a function such that $f \in L^\infty \cap L^ {p_0}$ where $L^\infty$ is the space of essentially bounded functions and $ 0 < p_0 < \infty$. Show that $ || f|| _{L^p} \to ||f || _{L^\infty} $ as $ p \to \infty$. Where $|| f||_{L^\infty} $ is the least $M \in R$ such that $|f(x)| \le M$ for almost every $x \in X$. The hint says to use the monotone convergence theorem, but i can't even see any pointwise convergence of functions. Any help is appreciated.",,"['real-analysis', 'measure-theory']"
74,Intuition for differentiating beneath the integral,Intuition for differentiating beneath the integral,,"I apologize in advance for a vague question. There is a theorem: If both $f(x,s)$ and $\partial _sf(x,s)$ are continuous in $x$ and   $s$, then $$\partial_s\int_a^bf(x,s)\,dx=\int_a^b  \partial_sf(x,s)\,dx$$ If in addition $\int_{-\infty}^\infty \partial_s f(x,s)\,dx$ converges uniformly   in a neighborhood of $s_0$, then $$\partial_s \int_{-\infty}^\infty  f(x,s)\,dx=\int_{-\infty}^\infty \partial_s f(x,s)\,dx.$$ The proof I know relies on integrating in $s$ and then switching the order of integration by uniform convergence. But beyond the mechanics of the proof, I am trying to develop an intuition for this fact. It does not seem intuitive to me that $$\partial_s \int f(x,s)\,dx = \int \partial_s f(x,s)\,dx.$$ I think the reason why it seems surprising to me is that you're integrating with respect to a different variable than the integration. I am familiar with some real analysis and measure theory, so feel free to pitch an answer on that level.","I apologize in advance for a vague question. There is a theorem: If both $f(x,s)$ and $\partial _sf(x,s)$ are continuous in $x$ and   $s$, then $$\partial_s\int_a^bf(x,s)\,dx=\int_a^b  \partial_sf(x,s)\,dx$$ If in addition $\int_{-\infty}^\infty \partial_s f(x,s)\,dx$ converges uniformly   in a neighborhood of $s_0$, then $$\partial_s \int_{-\infty}^\infty  f(x,s)\,dx=\int_{-\infty}^\infty \partial_s f(x,s)\,dx.$$ The proof I know relies on integrating in $s$ and then switching the order of integration by uniform convergence. But beyond the mechanics of the proof, I am trying to develop an intuition for this fact. It does not seem intuitive to me that $$\partial_s \int f(x,s)\,dx = \int \partial_s f(x,s)\,dx.$$ I think the reason why it seems surprising to me is that you're integrating with respect to a different variable than the integration. I am familiar with some real analysis and measure theory, so feel free to pitch an answer on that level.",,"['calculus', 'real-analysis', 'soft-question', 'intuition']"
75,How to treat differentials?,How to treat differentials?,,"In my Calculus class, my math teacher said that differentials such as $dx$ are not numbers, and should not be treated as such. In my physics class, it seems like we treat differentials exactly like numbers, and my physics teacher even said that they are in essence very small numbers. Can someone give me an explanation which satisfies both classes, or do I just have to accept that the differentials are treated differently in different courses? For example, if the linear density of a solid rod is $d$, in Physics class we would say that the mass of a very small part of the rod $dx$, is $d*dx$, so my physics teacher would say $dm=d*dx$. P.S. I took Calculus 2 so please try to keep the answers around that level. P.S.S. Feel free to edit the tags if you think it is appropriate.","In my Calculus class, my math teacher said that differentials such as $dx$ are not numbers, and should not be treated as such. In my physics class, it seems like we treat differentials exactly like numbers, and my physics teacher even said that they are in essence very small numbers. Can someone give me an explanation which satisfies both classes, or do I just have to accept that the differentials are treated differently in different courses? For example, if the linear density of a solid rod is $d$, in Physics class we would say that the mass of a very small part of the rod $dx$, is $d*dx$, so my physics teacher would say $dm=d*dx$. P.S. I took Calculus 2 so please try to keep the answers around that level. P.S.S. Feel free to edit the tags if you think it is appropriate.",,"['calculus', 'real-analysis']"
76,Limit of sequence $\frac{1}{2^n} \sum\limits_\epsilon f(\epsilon_1\lambda+\dots+\epsilon_n\lambda^n)$,Limit of sequence,\frac{1}{2^n} \sum\limits_\epsilon f(\epsilon_1\lambda+\dots+\epsilon_n\lambda^n),"Let $0<\lambda<1$ and $f\in C(\mathbb{R},\mathbb{R})$. Consider   $I_{n,\lambda}=\frac{1}{2^n}  \sum\limits_{(\epsilon_1,\dots,\epsilon_n) \in \{-1,1\}^n} f(\epsilon_1\lambda+\dots+\epsilon_n\lambda^n)$.   Show that the sequence $(I_{n,\lambda}(f))_{n\in\mathbb{N}}$ has a limit. I found this exercise in a book (it is an exercise from ENS Paris), if someone has an idea please share it.","Let $0<\lambda<1$ and $f\in C(\mathbb{R},\mathbb{R})$. Consider   $I_{n,\lambda}=\frac{1}{2^n}  \sum\limits_{(\epsilon_1,\dots,\epsilon_n) \in \{-1,1\}^n} f(\epsilon_1\lambda+\dots+\epsilon_n\lambda^n)$.   Show that the sequence $(I_{n,\lambda}(f))_{n\in\mathbb{N}}$ has a limit. I found this exercise in a book (it is an exercise from ENS Paris), if someone has an idea please share it.",,['real-analysis']
77,Gram-Schmidt in Hilbert space?,Gram-Schmidt in Hilbert space?,,"EDIT: After some contemplation I decided to phrase the question better to avoid trivial answers. Consider a Hilbert space with a basis $\{v_{i}\}$ where $i\in I$ an index set, which could be uncountably infinite. We define a ""Gram-Schmidt for infinite dimension"" to consist of a total ordering $\leq$ on the index set $I$ and a set of orthonormal basis $\{b_{i}\}$ where $i\in I$, and the following condition is satisfied: for any $i\in I$, then $b_{i}$ can be written as a finite linear combination of $v_{i}$ and $b_{j}$ where $j<i$. We define a ""Gram-Schmidt for infinite dimension with series allowed"" to be just like above, except that $b_{i}$ is allowed to be written as series instead of just finite linear combination. So the question is, can ""Gram-Schmidt for infinite dimension"" be done in a general Hilbert space starting with an arbitrary basis $\{v_{i}\}$? What about ""Gram-Schmidt for infinite dimension with series allowed""? Clearly, if $I$ is countable, the answer is trivial. The interesting case is when $I$ is uncountable. Thank you for your answer. =========== Original question: In Real Analysis for Graduate Student (Bass) chapter 19 (Hilbert space) page 188, wrote was ""The Gram-Schmidt procedure from linear algebra also works in infinitely many dimensions"". So how exactly would such process get carried out? Thank you for your answer.","EDIT: After some contemplation I decided to phrase the question better to avoid trivial answers. Consider a Hilbert space with a basis $\{v_{i}\}$ where $i\in I$ an index set, which could be uncountably infinite. We define a ""Gram-Schmidt for infinite dimension"" to consist of a total ordering $\leq$ on the index set $I$ and a set of orthonormal basis $\{b_{i}\}$ where $i\in I$, and the following condition is satisfied: for any $i\in I$, then $b_{i}$ can be written as a finite linear combination of $v_{i}$ and $b_{j}$ where $j<i$. We define a ""Gram-Schmidt for infinite dimension with series allowed"" to be just like above, except that $b_{i}$ is allowed to be written as series instead of just finite linear combination. So the question is, can ""Gram-Schmidt for infinite dimension"" be done in a general Hilbert space starting with an arbitrary basis $\{v_{i}\}$? What about ""Gram-Schmidt for infinite dimension with series allowed""? Clearly, if $I$ is countable, the answer is trivial. The interesting case is when $I$ is uncountable. Thank you for your answer. =========== Original question: In Real Analysis for Graduate Student (Bass) chapter 19 (Hilbert space) page 188, wrote was ""The Gram-Schmidt procedure from linear algebra also works in infinitely many dimensions"". So how exactly would such process get carried out? Thank you for your answer.",,"['real-analysis', 'linear-algebra', 'hilbert-spaces', 'infinity']"
78,Schwartz class function convergence in $L^1$ and $L^2$,Schwartz class function convergence in  and,L^1 L^2,"Let $f:\mathbb{R}\rightarrow\mathbb{R}$ be a function in both $L^1(\mathbb{R})$ and $L^2(\mathbb{R})$. I want to show that there exists a sequence of functions $g_1,g_2,\ldots$ in the Schwartz class such that both $\|g_n-f\|_1\rightarrow 0$ and $\|g_n-f\|_2\rightarrow 0$ as $n\rightarrow \infty$. Following the suggestion given in this post , I'm looking at the function $$g_m(x) = f(x)\cdot \chi_{[-m,m]}(x)\cdot \chi_{\{ \lvert f(y)\rvert \leqslant m\}}(x),$$ where $\chi$ denotes the characteristic function. I can see that the convergence follows from the dominated convergence theorem. But why would $g_m$ be in the Schwartz class? Since there is no assumption on the differentiability of $f$, the function $g_m$ might not even have derivatives of any order, right?","Let $f:\mathbb{R}\rightarrow\mathbb{R}$ be a function in both $L^1(\mathbb{R})$ and $L^2(\mathbb{R})$. I want to show that there exists a sequence of functions $g_1,g_2,\ldots$ in the Schwartz class such that both $\|g_n-f\|_1\rightarrow 0$ and $\|g_n-f\|_2\rightarrow 0$ as $n\rightarrow \infty$. Following the suggestion given in this post , I'm looking at the function $$g_m(x) = f(x)\cdot \chi_{[-m,m]}(x)\cdot \chi_{\{ \lvert f(y)\rvert \leqslant m\}}(x),$$ where $\chi$ denotes the characteristic function. I can see that the convergence follows from the dominated convergence theorem. But why would $g_m$ be in the Schwartz class? Since there is no assumption on the differentiability of $f$, the function $g_m$ might not even have derivatives of any order, right?",,"['real-analysis', 'fourier-analysis']"
79,"Can we divided the irrationals into a countable disjoint union of subsets, none of which has a rational limit point?","Can we divided the irrationals into a countable disjoint union of subsets, none of which has a rational limit point?",,"If we partite $\mathbb{R}\backslash\mathbb{Q}$ as  $\cup_{i\in\mathbb{N}}A_i=\mathbb{R}\backslash\mathbb{Q}$, $A_i\cap A_j=\emptyset$ if $i\ne j$, can it hold that $A_i$ has no rational limit point ? Actually, we have irrational perfect sets, but they don't seem to be able to contain much of $\mathbb{R}\backslash\mathbb{Q}$. Or if we can express the irrationals as a disjoint union of such sets, then we are done. But this too seems hopeless. Furthurmore, does it hold that $\cup_{i\in\mathbb{N}}\bar{A_i}=\mathbb{R}$? Actually, this is a strengthening of the first problem. The first one seeks to prove that whatever partition we make, the outcome cannot be too coarse. The second one further discusses whether it is still dense.","If we partite $\mathbb{R}\backslash\mathbb{Q}$ as  $\cup_{i\in\mathbb{N}}A_i=\mathbb{R}\backslash\mathbb{Q}$, $A_i\cap A_j=\emptyset$ if $i\ne j$, can it hold that $A_i$ has no rational limit point ? Actually, we have irrational perfect sets, but they don't seem to be able to contain much of $\mathbb{R}\backslash\mathbb{Q}$. Or if we can express the irrationals as a disjoint union of such sets, then we are done. But this too seems hopeless. Furthurmore, does it hold that $\cup_{i\in\mathbb{N}}\bar{A_i}=\mathbb{R}$? Actually, this is a strengthening of the first problem. The first one seeks to prove that whatever partition we make, the outcome cannot be too coarse. The second one further discusses whether it is still dense.",,['real-analysis']
80,"Show that $f(x)=\sqrt{x}:[0,1]\rightarrow \mathbb{R}$ is not a Lipschitz function.",Show that  is not a Lipschitz function.,"f(x)=\sqrt{x}:[0,1]\rightarrow \mathbb{R}","A function $f:D\rightarrow \mathbb{R}$ is said to be a Lipschitz function provided that there is a nonnegative number $C$ such that $|f(u)-f(v)|\le C|u-v|$ for all $u,v\in D$ We want to show there are there exist $u,v\in [0,1]$ such that $|\sqrt{u}-\sqrt{v}|\le C|u-v|$ is false, but I can't find anything that works. Any suggestions?","A function $f:D\rightarrow \mathbb{R}$ is said to be a Lipschitz function provided that there is a nonnegative number $C$ such that $|f(u)-f(v)|\le C|u-v|$ for all $u,v\in D$ We want to show there are there exist $u,v\in [0,1]$ such that $|\sqrt{u}-\sqrt{v}|\le C|u-v|$ is false, but I can't find anything that works. Any suggestions?",,['real-analysis']
81,"Question from ""An introduction to measure theory"" by Terence Tao [duplicate]","Question from ""An introduction to measure theory"" by Terence Tao [duplicate]",,"This question already has an answer here : Summablity and countability [duplicate] (1 answer) Closed 10 years ago . If $(x_α)_{α \in A}$ is a collection of numbers $x_α ∈ [0, +\infty]$ such that $\sum_{α∈A}{x_α} < \infty$, show that $x_α = 0$ for all but at most countably many $α \in A$, even if $A$ itself is uncountable.","This question already has an answer here : Summablity and countability [duplicate] (1 answer) Closed 10 years ago . If $(x_α)_{α \in A}$ is a collection of numbers $x_α ∈ [0, +\infty]$ such that $\sum_{α∈A}{x_α} < \infty$, show that $x_α = 0$ for all but at most countably many $α \in A$, even if $A$ itself is uncountable.",,"['real-analysis', 'statistics', 'measure-theory']"
82,Free Metric Space?,Free Metric Space?,,"Do free metric spaces exist? Ie.: An object in the category of metric spaces and lipschitzian maps. If so would these be the complete metric spaces, since they satisfy a similar universal property?","Do free metric spaces exist? Ie.: An object in the category of metric spaces and lipschitzian maps. If so would these be the complete metric spaces, since they satisfy a similar universal property?",,"['real-analysis', 'analysis', 'metric-spaces', 'category-theory']"
83,Is axiom of completeness an axiom?,Is axiom of completeness an axiom?,,"The following statement is the axiom of completeness: Every non empty subset of $\mathbb R$ that is bounded above has a least upper bound. So I was wondering: is it an axiom or can it be proven? One can construct $\mathbb R$ as the set of equivalence classes of Cauchy sequence in $\mathbb Q$ . Two sequences are equivalent iff the have the same limit. Then, I tried to prove that a non empty set bounded above has a least upper bound but couldn't find a proof.  But I think it is possible because $\mathbb R$ is defined by construction and then there can not be axioms in it?","The following statement is the axiom of completeness: Every non empty subset of that is bounded above has a least upper bound. So I was wondering: is it an axiom or can it be proven? One can construct as the set of equivalence classes of Cauchy sequence in . Two sequences are equivalent iff the have the same limit. Then, I tried to prove that a non empty set bounded above has a least upper bound but couldn't find a proof.  But I think it is possible because is defined by construction and then there can not be axioms in it?",\mathbb R \mathbb R \mathbb Q \mathbb R,"['real-analysis', 'axioms']"
84,Show that the derivatives of a $C^1$ function vanish a.e. on the inverse image of a null set,Show that the derivatives of a  function vanish a.e. on the inverse image of a null set,C^1,"Let $A \subset \mathbb{R}$ be such that $\lambda(A) = 0$, where $\lambda$ is the Lebesgue measure on the real line. Let $\Omega \subset \mathbb{R}^N$ be an open set and let $u \colon \Omega \to \mathbb{R}$ be in $C^1(\Omega)$. Then, for every $i = 1, \dots , N$, we have that $$\frac{\partial u}{\partial x_i}(x) = 0$$ for $\lambda^N$-almost every $x \in u^{-1}(A)$. (where $\lambda^N$ is of course the lebesgue measure on $\mathbb{R}^N$) Does anyone know a proof of this fact? The lecture notes continue saying that since every singleton has Lebesgue measure $0$ in $\mathbb{R}$, we have that $$\frac{\partial u}{\partial x_i}(x) = 0$$ for $\lambda^N$-a.e. $x \in \Omega_c$, where $\Omega_c := \{x \in \Omega : u(x) = c\}$.","Let $A \subset \mathbb{R}$ be such that $\lambda(A) = 0$, where $\lambda$ is the Lebesgue measure on the real line. Let $\Omega \subset \mathbb{R}^N$ be an open set and let $u \colon \Omega \to \mathbb{R}$ be in $C^1(\Omega)$. Then, for every $i = 1, \dots , N$, we have that $$\frac{\partial u}{\partial x_i}(x) = 0$$ for $\lambda^N$-almost every $x \in u^{-1}(A)$. (where $\lambda^N$ is of course the lebesgue measure on $\mathbb{R}^N$) Does anyone know a proof of this fact? The lecture notes continue saying that since every singleton has Lebesgue measure $0$ in $\mathbb{R}$, we have that $$\frac{\partial u}{\partial x_i}(x) = 0$$ for $\lambda^N$-a.e. $x \in \Omega_c$, where $\Omega_c := \{x \in \Omega : u(x) = c\}$.",,['real-analysis']
85,Placing Ts on the $x$-axis,Placing Ts on the -axis,x,"A ""T"" consists of two perpendicular intervals $\{c\}\times[0,a]$ and $[b,d]\times \{a\}$ (with $b<c<d$) on the plane. We say that the T is placed on point $c$. Is it possible to place non-intersecting T's on all real numbers on the $[0,1]$ interval of the $x$-axis? I believe the answer should be ""no"". Suppose it were possible. We can assume that each T has equal left-halfwidth and right-halfwidth (i.e. half of the horizontal line of the T.) For each real number $x\in[0,1]$, let $h_x$ denote the height of its T and $w_x$ denote its halfwidth. Then two real numbers $x,y\in[0,1]$ have intersecting T's if $h_y< h_x$ and $|x-y|\le w_y$, and vice versa. So every time we have $h_x>h_y$, we must have $|x-y|>w_y$. How can we get a contradiction?","A ""T"" consists of two perpendicular intervals $\{c\}\times[0,a]$ and $[b,d]\times \{a\}$ (with $b<c<d$) on the plane. We say that the T is placed on point $c$. Is it possible to place non-intersecting T's on all real numbers on the $[0,1]$ interval of the $x$-axis? I believe the answer should be ""no"". Suppose it were possible. We can assume that each T has equal left-halfwidth and right-halfwidth (i.e. half of the horizontal line of the T.) For each real number $x\in[0,1]$, let $h_x$ denote the height of its T and $w_x$ denote its halfwidth. Then two real numbers $x,y\in[0,1]$ have intersecting T's if $h_y< h_x$ and $|x-y|\le w_y$, and vice versa. So every time we have $h_x>h_y$, we must have $|x-y|>w_y$. How can we get a contradiction?",,"['real-analysis', 'elementary-set-theory']"
86,Why doesn't Cauchy-Schwarz in $\mathbb{R}^n$ generalize to exponents $k>2$?,Why doesn't Cauchy-Schwarz in  generalize to exponents ?,\mathbb{R}^n k>2,"Given $(x_i)_{i=1}^n, (y_i)_{i=1}^n \in \mathbb{R}^n$, the Cauchy-Schwarz Inequality asserts $$\left( \sum_{i=1}^n x_i y_i \right)^2 \leq \left( \sum_{i=1}^n x_i \right)^2 \left( \sum_{i=1}^n y_i \right)^2.$$ Conspicuously absent from the Wikipedia page is a claim that $$\left( \sum_{i=1}^n x_i y_i \right)^k \leq \left( \sum_{i=1}^n x_i^k \right) \left( \sum_{i=1}^n y_i^k \right)$$ holds for $k>2$, which makes me think it's untrue (and similarly for the same formula with absolute value signs around the $x_i$'s and $y_i$'s).  Indeed, we can find random counter-examples on a computer. Question : Why doesn't Cauchy-Schwarz in $\mathbb{R}^n$ generalize to exponents $k>2$? Can we gain any insight into why it works for $k=2$ but not for $k>2$?","Given $(x_i)_{i=1}^n, (y_i)_{i=1}^n \in \mathbb{R}^n$, the Cauchy-Schwarz Inequality asserts $$\left( \sum_{i=1}^n x_i y_i \right)^2 \leq \left( \sum_{i=1}^n x_i \right)^2 \left( \sum_{i=1}^n y_i \right)^2.$$ Conspicuously absent from the Wikipedia page is a claim that $$\left( \sum_{i=1}^n x_i y_i \right)^k \leq \left( \sum_{i=1}^n x_i^k \right) \left( \sum_{i=1}^n y_i^k \right)$$ holds for $k>2$, which makes me think it's untrue (and similarly for the same formula with absolute value signs around the $x_i$'s and $y_i$'s).  Indeed, we can find random counter-examples on a computer. Question : Why doesn't Cauchy-Schwarz in $\mathbb{R}^n$ generalize to exponents $k>2$? Can we gain any insight into why it works for $k=2$ but not for $k>2$?",,"['real-analysis', 'linear-algebra', 'inequality', 'inner-products']"
87,A set containing one element is an open set. Why?,A set containing one element is an open set. Why?,,I asked a question last night about proving that a discrete metric space is both open and closed. Once or twice it was mentioned that a set that contains only one element is open. I'd like to know: a) is that always true? b) why? An explanation that is both a proof and a simple breakdown of it would be most helpful.,I asked a question last night about proving that a discrete metric space is both open and closed. Once or twice it was mentioned that a set that contains only one element is open. I'd like to know: a) is that always true? b) why? An explanation that is both a proof and a simple breakdown of it would be most helpful.,,"['real-analysis', 'general-topology', 'analysis']"
88,Showing $E(\Omega)$ is a Hilbert Space,Showing  is a Hilbert Space,E(\Omega),"Let $E(\Omega)=\{ u\in \{L^2 (\Omega)\}^n : \text{div } u \in L^2(\Omega)\}$, that is $E(\Omega)$ consists of vector valued functions $u=(u^1, \cdots , u^n)$ where each component function $u^i$, $i\in \{ 1, \cdots n\}$ is a $L^2(\Omega)$ function and $\frac{\partial u^1}{\partial x_1}+\cdots+\frac{\partial u^n}{\partial x_n}\in L^2(\Omega)$. We want to show that under the norm induced by this inner product $$ (u,v)_{E(\Omega)}=\sum_{i=1}^{n} (u_j, v_j)_{L^2(\Omega)}+(\text{div } u, \text{div }v)_{L^2(\Omega)} $$ $(E(\Omega), ||.||_{E(\Omega)})$ is a Hilbert Space. This is what I manage to do. Suppose $\{u_m=(u^1, \cdots , u^n)\}_{m\in \mathbb{N}}$ is a Cauchy sequence in $(E(\Omega), ||.||_{E(\Omega)})$. Fix $i \in \{1,\cdots, n \}$. Because $(u^i_m-u^i_l, u^i_m-u^i_l )$=$||u^i_m-u^i_l ||^2_{L^2(\Omega)} \leq  ||u_m-u_l||^2_{E(\Omega)}$ and the fact that $\{u_m=(u^1, \cdots , u^n)\}_{m\in \mathbb{N}}$ is a Cauchy sequence in $(E(\Omega), ||.||_{E(\Omega)})$, we see that $\{u^i_m\}_{m\in\mathbb{N}}$ is Cauchy in $L^2(\Omega)$. By the completeness of $L^2(\Omega)$, we concluded that $u^i_m \rightarrow u^i \in L^2(\Omega)$ in $L^2$ norm as $m\rightarrow \infty$. Hence we concluded that $u=(u^1, \cdots, u^n) \in \{L^2(\Omega)\}^n$. Also note that $$ (\text{div }(u_m-u_l), \text{div }(u_m-u_l))_{L^2(\Omega)}=||\text{div } u_m - \text{div } u_l||^2_{L^2(\Omega)}\leq ||u_m-u_l||^2_{E(\Omega)} $$ Hence $\{\text{div } u_m\}_{m\in\mathbb{N}}$ is Cauchy  in $L^2(\Omega)$. Again by the completeness of $L^2(\Omega)$, we can conclude that $\text{div } u_m \rightarrow g\in L^2(\Omega)$ in $L^2$ norm as $m \rightarrow \infty$. Now to show $u \in E(\Omega)$, we need to show $\text{div } u$ exists and is also in $L^2(\Omega)$. This is achieved by showing $g=\text{div }u$. But this is where I get stuck, how do we show this last part? I don't even know that the partial derivative of $u^i$ exists. Thanks.","Let $E(\Omega)=\{ u\in \{L^2 (\Omega)\}^n : \text{div } u \in L^2(\Omega)\}$, that is $E(\Omega)$ consists of vector valued functions $u=(u^1, \cdots , u^n)$ where each component function $u^i$, $i\in \{ 1, \cdots n\}$ is a $L^2(\Omega)$ function and $\frac{\partial u^1}{\partial x_1}+\cdots+\frac{\partial u^n}{\partial x_n}\in L^2(\Omega)$. We want to show that under the norm induced by this inner product $$ (u,v)_{E(\Omega)}=\sum_{i=1}^{n} (u_j, v_j)_{L^2(\Omega)}+(\text{div } u, \text{div }v)_{L^2(\Omega)} $$ $(E(\Omega), ||.||_{E(\Omega)})$ is a Hilbert Space. This is what I manage to do. Suppose $\{u_m=(u^1, \cdots , u^n)\}_{m\in \mathbb{N}}$ is a Cauchy sequence in $(E(\Omega), ||.||_{E(\Omega)})$. Fix $i \in \{1,\cdots, n \}$. Because $(u^i_m-u^i_l, u^i_m-u^i_l )$=$||u^i_m-u^i_l ||^2_{L^2(\Omega)} \leq  ||u_m-u_l||^2_{E(\Omega)}$ and the fact that $\{u_m=(u^1, \cdots , u^n)\}_{m\in \mathbb{N}}$ is a Cauchy sequence in $(E(\Omega), ||.||_{E(\Omega)})$, we see that $\{u^i_m\}_{m\in\mathbb{N}}$ is Cauchy in $L^2(\Omega)$. By the completeness of $L^2(\Omega)$, we concluded that $u^i_m \rightarrow u^i \in L^2(\Omega)$ in $L^2$ norm as $m\rightarrow \infty$. Hence we concluded that $u=(u^1, \cdots, u^n) \in \{L^2(\Omega)\}^n$. Also note that $$ (\text{div }(u_m-u_l), \text{div }(u_m-u_l))_{L^2(\Omega)}=||\text{div } u_m - \text{div } u_l||^2_{L^2(\Omega)}\leq ||u_m-u_l||^2_{E(\Omega)} $$ Hence $\{\text{div } u_m\}_{m\in\mathbb{N}}$ is Cauchy  in $L^2(\Omega)$. Again by the completeness of $L^2(\Omega)$, we can conclude that $\text{div } u_m \rightarrow g\in L^2(\Omega)$ in $L^2$ norm as $m \rightarrow \infty$. Now to show $u \in E(\Omega)$, we need to show $\text{div } u$ exists and is also in $L^2(\Omega)$. This is achieved by showing $g=\text{div }u$. But this is where I get stuck, how do we show this last part? I don't even know that the partial derivative of $u^i$ exists. Thanks.",,"['real-analysis', 'partial-differential-equations']"
89,Recognizing uppersemicontinuous function as a pointwise decreasing limit.,Recognizing uppersemicontinuous function as a pointwise decreasing limit.,,"Let $X$ be a compact metric space and $f:X\rightarrow \mathbb{R}$ be upper semicontinuous.  Then why is it that $f$ is the pointwise decreasing limit of continuous functions? My attempt has been to use totally boundedness of $X$ to get a finite cover by $1/n$ radius balls $B_{i, n}$.  Then find a subordinate continuous partition of unity $\phi_{i, n}$ where i runs over a finite index set.  Then let $g_n=\sum_{i}sup f|_{B_{i,n}} * \phi_{i, n}$. $g_n$ is continuous and at least $f$.  Therefore, the same holds of $f_n=min(g_1, ... g_n)$.  Furthermore, the last sequence is decreasing.  Why does $f_n$ converge to f pointwise? (Or you can suggest a different method of proof entirely.)  Towards this I was hoping for perhaps a proof of some sort of ""uniform upper semicontinuity"" in analogy to uniform continuity which follows from continuity on $X$, but that seems hopeless.","Let $X$ be a compact metric space and $f:X\rightarrow \mathbb{R}$ be upper semicontinuous.  Then why is it that $f$ is the pointwise decreasing limit of continuous functions? My attempt has been to use totally boundedness of $X$ to get a finite cover by $1/n$ radius balls $B_{i, n}$.  Then find a subordinate continuous partition of unity $\phi_{i, n}$ where i runs over a finite index set.  Then let $g_n=\sum_{i}sup f|_{B_{i,n}} * \phi_{i, n}$. $g_n$ is continuous and at least $f$.  Therefore, the same holds of $f_n=min(g_1, ... g_n)$.  Furthermore, the last sequence is decreasing.  Why does $f_n$ converge to f pointwise? (Or you can suggest a different method of proof entirely.)  Towards this I was hoping for perhaps a proof of some sort of ""uniform upper semicontinuity"" in analogy to uniform continuity which follows from continuity on $X$, but that seems hopeless.",,"['real-analysis', 'general-topology', 'analysis', 'metric-spaces']"
90,Definition of limit,Definition of limit,,"I learnt at school that this limit $\lim_{x\to 0}\frac{1}{x}$ doesn't exist, and intiuitively it seems that such is the case, but I just don't get it. To begin with, I understand the definition of limit in this way, please tell me where I'm wrong or if I'm missing something: Let $A, B\subseteq \mathbb{R}$ and $f:A\longrightarrow B$ a function such that $a\in A$ is an acummulation point. Then we say that $l\in B$ is the limit of the function $f$ when $x$ approches $a$ and is denoted by $\lim_{x\to a}f=l$ if and only if $\forall \epsilon\in \mathbb{R}(\epsilon>0)\exists\delta\in \mathbb{R}(\delta >0)\forall x\in A(0<|x-a|<\delta\longrightarrow |f(x)-l|<\epsilon)$. So, accordingly, I have the function $f:\mathbb{R}\setminus\{0\}\longrightarrow\mathbb{R}$ such that $f(x)=\frac{1}{x}$. Since $0\notin Dom (f)$ then it doesn't even make sense to talk about the definition of $\lim_{x\to 0}\frac{1}{x}$. Also I think that I probably need to change in my definition the part of $(\forall x\in A)$ for $(\forall x\in \mathbb{R})$. This is consistent because if my metric spaces were not subsets of $\mathbb{R}$, for example if I had $E_{1}, E_{2}$ metric spaces and $A\subseteq E_{1}, B\subseteq E_{2}$ such that $f:A\longrightarrow B$. For the part $|x-a|<\delta$ to make sense it's necessary that $x\in A$ or $x\in E_{1}$. The problem here is that taking $(\forall x\in E_{1})$ might turn undefined many points of the part $|f(x)-l|$ because it might be that $A\subseteq E_{1}$ but $A\neq E_{1}$. Edit: With all the suggestions - thank you so much guys - my new definition is this way: Let $A, B\subseteq \mathbb{R}$ and $f:A\longrightarrow B$ a function such that $a\in \mathbb{R}$ is an acummulation point of $A$. Then we say that $l\in \mathbb{R}$ is the limit of the function $f$ when $x$ approches $a$ and is denoted by $\lim_{x\to a}f=l$ if and only if $\forall \epsilon\in \mathbb{R}(\epsilon>0)\exists\delta\in \mathbb{R}(\delta >0)\forall x\in A(0<|x-a|<\delta\longrightarrow |f(x)-l|<\epsilon)$. Now, I have this problem. With this definition I can prove that given the function $f:\mathbb{R^{+}}\longrightarrow \mathbb{R}$ such that $f(x)=\sqrt{x}$, then $\lim_{x\to 0}\sqrt{x}=0$. But officially this limit doesn't exist, though $\lim_{x\to 0^{+}}\sqrt{x}=0$. If I substitute $\forall x\in A$ for $\forall x\in \mathbb{R}$ then the problem seems to be fixed. But now this doesn't allow to talk about rational functions, like for example if I take the function $f:\mathbb{Q}\longrightarrow \mathbb{R}$ such that $f(x)=x$ then $\lim_{x\to 0}f(x)$ doesn't exist. What am I missing?","I learnt at school that this limit $\lim_{x\to 0}\frac{1}{x}$ doesn't exist, and intiuitively it seems that such is the case, but I just don't get it. To begin with, I understand the definition of limit in this way, please tell me where I'm wrong or if I'm missing something: Let $A, B\subseteq \mathbb{R}$ and $f:A\longrightarrow B$ a function such that $a\in A$ is an acummulation point. Then we say that $l\in B$ is the limit of the function $f$ when $x$ approches $a$ and is denoted by $\lim_{x\to a}f=l$ if and only if $\forall \epsilon\in \mathbb{R}(\epsilon>0)\exists\delta\in \mathbb{R}(\delta >0)\forall x\in A(0<|x-a|<\delta\longrightarrow |f(x)-l|<\epsilon)$. So, accordingly, I have the function $f:\mathbb{R}\setminus\{0\}\longrightarrow\mathbb{R}$ such that $f(x)=\frac{1}{x}$. Since $0\notin Dom (f)$ then it doesn't even make sense to talk about the definition of $\lim_{x\to 0}\frac{1}{x}$. Also I think that I probably need to change in my definition the part of $(\forall x\in A)$ for $(\forall x\in \mathbb{R})$. This is consistent because if my metric spaces were not subsets of $\mathbb{R}$, for example if I had $E_{1}, E_{2}$ metric spaces and $A\subseteq E_{1}, B\subseteq E_{2}$ such that $f:A\longrightarrow B$. For the part $|x-a|<\delta$ to make sense it's necessary that $x\in A$ or $x\in E_{1}$. The problem here is that taking $(\forall x\in E_{1})$ might turn undefined many points of the part $|f(x)-l|$ because it might be that $A\subseteq E_{1}$ but $A\neq E_{1}$. Edit: With all the suggestions - thank you so much guys - my new definition is this way: Let $A, B\subseteq \mathbb{R}$ and $f:A\longrightarrow B$ a function such that $a\in \mathbb{R}$ is an acummulation point of $A$. Then we say that $l\in \mathbb{R}$ is the limit of the function $f$ when $x$ approches $a$ and is denoted by $\lim_{x\to a}f=l$ if and only if $\forall \epsilon\in \mathbb{R}(\epsilon>0)\exists\delta\in \mathbb{R}(\delta >0)\forall x\in A(0<|x-a|<\delta\longrightarrow |f(x)-l|<\epsilon)$. Now, I have this problem. With this definition I can prove that given the function $f:\mathbb{R^{+}}\longrightarrow \mathbb{R}$ such that $f(x)=\sqrt{x}$, then $\lim_{x\to 0}\sqrt{x}=0$. But officially this limit doesn't exist, though $\lim_{x\to 0^{+}}\sqrt{x}=0$. If I substitute $\forall x\in A$ for $\forall x\in \mathbb{R}$ then the problem seems to be fixed. But now this doesn't allow to talk about rational functions, like for example if I take the function $f:\mathbb{Q}\longrightarrow \mathbb{R}$ such that $f(x)=x$ then $\lim_{x\to 0}f(x)$ doesn't exist. What am I missing?",,"['calculus', 'real-analysis']"
91,Stuck at Evaluating the Riemann-Stieltjes Integral,Stuck at Evaluating the Riemann-Stieltjes Integral,,"Let $$\alpha(x) = \left\lbrace \begin{array}{cc} 0 & x=0\\ \dfrac{1}{2^n} & \dfrac{1}{3^n} < x \leq \dfrac{1}{3^{n-1}}\quad n=1,2,...\end{array}\right.$$ Evaluate $$\int_{0}^{1}{x\mathrm{d}\alpha(x)}$$ Attempt: I dont know how to put this formally, but I know it is similar to problem of evaluating the R-S integral where $\alpha(x)$ is the jump function.  So If I evaluate the function $\alpha(x)$ for some values of $n$ then the function looks like this $$\alpha(x) = \left\lbrace \begin{array}{cc} 0 & x=0\\ \vdots & \vdots \\\dfrac{1}{8} & \dfrac{1}{27} < x \leq \dfrac{1}{9} \\ \dfrac{1}{4} & \dfrac{1}{9} < x \leq \dfrac{1}{3} \\ \dfrac{1}{2} & \dfrac{1}{3} < x \leq 1 \end{array}\right.$$ So $f(x)=x$ is continuous and $\alpha(x)$ is monotonically increasing. Hence the answer should be $$\int_{0}^{1}{x \mathrm{d}\alpha(x)} = \sum_{n=1}^{\infty}{\dfrac{1}{3^n}\dfrac{1}{2^{n+1}}}=\dfrac{1}{2}\sum_{n=1}^{\infty}{\left(\dfrac{1}{6}\right)^n}=\dfrac{1}{10}$$ So my questions are: (a) Is the answer correct? If yes, then how can I write this formally?  (b) If the answer is wrong, what direction can you provide me to get the right answer? Thanks!","Let $$\alpha(x) = \left\lbrace \begin{array}{cc} 0 & x=0\\ \dfrac{1}{2^n} & \dfrac{1}{3^n} < x \leq \dfrac{1}{3^{n-1}}\quad n=1,2,...\end{array}\right.$$ Evaluate $$\int_{0}^{1}{x\mathrm{d}\alpha(x)}$$ Attempt: I dont know how to put this formally, but I know it is similar to problem of evaluating the R-S integral where $\alpha(x)$ is the jump function.  So If I evaluate the function $\alpha(x)$ for some values of $n$ then the function looks like this $$\alpha(x) = \left\lbrace \begin{array}{cc} 0 & x=0\\ \vdots & \vdots \\\dfrac{1}{8} & \dfrac{1}{27} < x \leq \dfrac{1}{9} \\ \dfrac{1}{4} & \dfrac{1}{9} < x \leq \dfrac{1}{3} \\ \dfrac{1}{2} & \dfrac{1}{3} < x \leq 1 \end{array}\right.$$ So $f(x)=x$ is continuous and $\alpha(x)$ is monotonically increasing. Hence the answer should be $$\int_{0}^{1}{x \mathrm{d}\alpha(x)} = \sum_{n=1}^{\infty}{\dfrac{1}{3^n}\dfrac{1}{2^{n+1}}}=\dfrac{1}{2}\sum_{n=1}^{\infty}{\left(\dfrac{1}{6}\right)^n}=\dfrac{1}{10}$$ So my questions are: (a) Is the answer correct? If yes, then how can I write this formally?  (b) If the answer is wrong, what direction can you provide me to get the right answer? Thanks!",,['real-analysis']
92,Use of the Littlewood-Paley decomposition to recover the $H^s$ norm,Use of the Littlewood-Paley decomposition to recover the  norm,H^s,"Let $\phi\in C^{\infty}_0(\mathbb{R}^n)$ be such that  $$\{\lvert \xi\rvert \le 1\} \prec \phi \prec \{\lvert \xi \rvert < 2\}^{[1]} $$ and define the Littlewood-Paley projectors as \begin{equation} (P_{2^j}f)^\wedge=\left[\phi\left( \frac{\xi}{2^j} \right)- \phi\left( \frac{\xi}{2^{j-1}}\right)\right] \widehat{f}(\xi)  \end{equation} where $j\in \mathbb{Z}$ and $f$ is Schwartz. ($\,^\wedge$ denotes Fourier transform.) Question . Suppose that $f$ is a Schwartz function such that $\widehat{f}$ is supported in $\{\lvert \xi\rvert >2\}$, and let $s>0$ be fixed. Does there exist an absolute constant $C$ such that the following inequality is true?   $$\sum_{j=1}^\infty \left( 2^j\right)^s \lVert P_{2^j} f\rVert_2\le C \lVert f \rVert_{H^s}.$$ EDIT I now believe that the answer is negative , see comments. Some motivation (you can safely omit reading this): I want to prove a maximal estimate for the Schrödinger group $S_t=e^{it\Delta}$, namely $$\tag{1}\lVert S^\star f\rVert_{L^p(B_1)}\le C \lVert f\rVert_{H^s},$$ where $S^\star f=\sup_{t\in (0, 1)}\lvert S_tf\rvert$ and $B_1$ is the unit ball. All the articles I am consulting claim without further explanation that it is enough to prove this apparently weaker fact:  $$\tag{2}\lVert S^\star f\rVert_{L^p(B_1)}\le C \lambda^s\lVert f\rVert_2,\qquad \operatorname{Spt}\widehat{f}\subset \{\lvert \xi\rvert \sim \lambda\}$$ (where $\lvert \xi\rvert \sim \lambda$ means that $\lambda\le \lvert \xi \rvert \le 2\lambda$). I would like to prove the implication $(2)\Rightarrow (1)$. Now if $f$ is a Schwartz function that is Fourier supported in $\{\lvert \xi \rvert> 2\}$, then  $$f=\sum_{j=1}^\infty P_{2^j}f.$$ Applying the sublinearity of $S^\star$ and (2) we arrive at  \begin{equation} \begin{split} \lVert S^\star f\rVert_{L^p(B_1)} &\le\sum_{j=1}^\infty \lVert S^\star P_{2^j} f\rVert_{L^p(B_1)} \\ &\le C \sum_{j=1}^\infty \left(2^j\right)^s\lVert P_{2^j}f\rVert_2, \end{split} \end{equation} and this is where the Question comes in. $\,^{[1]}$ Meaning that $\phi\ge 0$ everywhere, that $\phi(\xi)=1$ for all $\xi \in \{\lvert \xi\rvert \le 1\}$ and that the support of $\phi$ is contained in $\{\lvert \xi \rvert < 2\}$.","Let $\phi\in C^{\infty}_0(\mathbb{R}^n)$ be such that  $$\{\lvert \xi\rvert \le 1\} \prec \phi \prec \{\lvert \xi \rvert < 2\}^{[1]} $$ and define the Littlewood-Paley projectors as \begin{equation} (P_{2^j}f)^\wedge=\left[\phi\left( \frac{\xi}{2^j} \right)- \phi\left( \frac{\xi}{2^{j-1}}\right)\right] \widehat{f}(\xi)  \end{equation} where $j\in \mathbb{Z}$ and $f$ is Schwartz. ($\,^\wedge$ denotes Fourier transform.) Question . Suppose that $f$ is a Schwartz function such that $\widehat{f}$ is supported in $\{\lvert \xi\rvert >2\}$, and let $s>0$ be fixed. Does there exist an absolute constant $C$ such that the following inequality is true?   $$\sum_{j=1}^\infty \left( 2^j\right)^s \lVert P_{2^j} f\rVert_2\le C \lVert f \rVert_{H^s}.$$ EDIT I now believe that the answer is negative , see comments. Some motivation (you can safely omit reading this): I want to prove a maximal estimate for the Schrödinger group $S_t=e^{it\Delta}$, namely $$\tag{1}\lVert S^\star f\rVert_{L^p(B_1)}\le C \lVert f\rVert_{H^s},$$ where $S^\star f=\sup_{t\in (0, 1)}\lvert S_tf\rvert$ and $B_1$ is the unit ball. All the articles I am consulting claim without further explanation that it is enough to prove this apparently weaker fact:  $$\tag{2}\lVert S^\star f\rVert_{L^p(B_1)}\le C \lambda^s\lVert f\rVert_2,\qquad \operatorname{Spt}\widehat{f}\subset \{\lvert \xi\rvert \sim \lambda\}$$ (where $\lvert \xi\rvert \sim \lambda$ means that $\lambda\le \lvert \xi \rvert \le 2\lambda$). I would like to prove the implication $(2)\Rightarrow (1)$. Now if $f$ is a Schwartz function that is Fourier supported in $\{\lvert \xi \rvert> 2\}$, then  $$f=\sum_{j=1}^\infty P_{2^j}f.$$ Applying the sublinearity of $S^\star$ and (2) we arrive at  \begin{equation} \begin{split} \lVert S^\star f\rVert_{L^p(B_1)} &\le\sum_{j=1}^\infty \lVert S^\star P_{2^j} f\rVert_{L^p(B_1)} \\ &\le C \sum_{j=1}^\infty \left(2^j\right)^s\lVert P_{2^j}f\rVert_2, \end{split} \end{equation} and this is where the Question comes in. $\,^{[1]}$ Meaning that $\phi\ge 0$ everywhere, that $\phi(\xi)=1$ for all $\xi \in \{\lvert \xi\rvert \le 1\}$ and that the support of $\phi$ is contained in $\{\lvert \xi \rvert < 2\}$.",,"['real-analysis', 'fourier-analysis', 'harmonic-analysis']"
93,$p$-adic metric triangle inequality,-adic metric triangle inequality,p,"If $d$ denotes the $p$-adic metric and $d(m,o) = {1 \over p^{t_{mo}}}$, $d(m,n) = {1 \over p^{t_{mn}}}$ and $d(n,o) = {1 \over p^{t_{no}}}$ I am trying to show the triangle inequality. But I'm not sure the proof is correct. Can you help me and correct it if it's wrong? Thank you. Here is my proof: Have: $m-o = (m-n) + (n-o)$ where $p^{t_{mn}} | (m-n)$ and $p^{t_{no}} | (n-o)$. Then $m-o = p^{t_{mn}}k + p^{t_{no}}j$. Without loss of generality assume $t_{mn} \ge t_{no}$ so that $$m-o = p^{t_{no}}(p^{t_{mn}-t_{no}}k + j)$$ Particularly, the following step I am unsure of: It follows that $t_{mo} = t_{no}$ and therefore $d(m,o) = {1 \over p^{t_{mo}}} = {1 \over p^{t_{no}}} = d(n,o)$. And therefore $d(m,o) \le d(n,o) + d(m,n)$. qed","If $d$ denotes the $p$-adic metric and $d(m,o) = {1 \over p^{t_{mo}}}$, $d(m,n) = {1 \over p^{t_{mn}}}$ and $d(n,o) = {1 \over p^{t_{no}}}$ I am trying to show the triangle inequality. But I'm not sure the proof is correct. Can you help me and correct it if it's wrong? Thank you. Here is my proof: Have: $m-o = (m-n) + (n-o)$ where $p^{t_{mn}} | (m-n)$ and $p^{t_{no}} | (n-o)$. Then $m-o = p^{t_{mn}}k + p^{t_{no}}j$. Without loss of generality assume $t_{mn} \ge t_{no}$ so that $$m-o = p^{t_{no}}(p^{t_{mn}-t_{no}}k + j)$$ Particularly, the following step I am unsure of: It follows that $t_{mo} = t_{no}$ and therefore $d(m,o) = {1 \over p^{t_{mo}}} = {1 \over p^{t_{no}}} = d(n,o)$. And therefore $d(m,o) \le d(n,o) + d(m,n)$. qed",,"['real-analysis', 'metric-spaces']"
94,A question on total variation of signed measures,A question on total variation of signed measures,,"This is a variant on a problem from Royden's Real Analysis : Let $\nu$ be a signed measure on a measurable space $\left\langle X,\mathcal{M}\right\rangle$ , and let $$\Sigma:=\left\{{\left.\sum_{k=1}^{n}{\left|\nu{\left(E_k\right)}\right|}\ \right|}\ E_k\in\mathcal{M}\hspace{4pt}\forall k=1,2,\dots,n;n\in\mathbb{N}\right\}.$$ We define the total variation of $\nu$ , denoted by $|\nu|$ , as follows:  given the Jordan decomposition $\nu=\nu^+-\nu^-$ , $|\nu|:=\nu^++\nu^-$ .  I’m trying to prove that $|\nu|(X)=\sup\left(\Sigma\right)$ .  I have $\mathrm{LHS}\leq\mathrm{RHS}$ :  using a Hahn decomposition $X=A\sqcup B$ with $A\hspace{4pt}\nu$ -positive and $B\hspace{4pt}\nu$ -negative, I’ve proven that $|\nu|(X)=|\nu(A)|+|\nu(B)|\in\Sigma$ , and hence $|\nu|(X)\leq\sup\left(\Sigma\right)$ .  I’m not sure quite how to prove the opposite inequality.  I’ve tried to show that $|\nu|(X)$ majorizes $\Sigma$ , but not gotten anywhere with it.  Any suggestions would be appreciated.","This is a variant on a problem from Royden's Real Analysis : Let be a signed measure on a measurable space , and let We define the total variation of , denoted by , as follows:  given the Jordan decomposition , .  I’m trying to prove that .  I have :  using a Hahn decomposition with -positive and -negative, I’ve proven that , and hence .  I’m not sure quite how to prove the opposite inequality.  I’ve tried to show that majorizes , but not gotten anywhere with it.  Any suggestions would be appreciated.","\nu \left\langle X,\mathcal{M}\right\rangle \Sigma:=\left\{{\left.\sum_{k=1}^{n}{\left|\nu{\left(E_k\right)}\right|}\ \right|}\ E_k\in\mathcal{M}\hspace{4pt}\forall k=1,2,\dots,n;n\in\mathbb{N}\right\}. \nu |\nu| \nu=\nu^+-\nu^- |\nu|:=\nu^++\nu^- |\nu|(X)=\sup\left(\Sigma\right) \mathrm{LHS}\leq\mathrm{RHS} X=A\sqcup B A\hspace{4pt}\nu B\hspace{4pt}\nu |\nu|(X)=|\nu(A)|+|\nu(B)|\in\Sigma |\nu|(X)\leq\sup\left(\Sigma\right) |\nu|(X) \Sigma","['real-analysis', 'measure-theory']"
95,Continuity of Monotone Functions,Continuity of Monotone Functions,,"Let f be a monotone function on the open interval (a,b). Then f is continuous except possibly at a countable number of points in (a,b). Assume f is increasing. Furthermore, assume (a,b) is bounded and f is increasing on the closed interval [a,b]. Otherwise, express (a,b) as the union of an ascending sequence of open, bounded intervals, the closures of which are contained in (a,b), and take the union of the discontinuities in each of this countable collection of intervals. I was a little confused about the last sentence, because I wasn't sure of what ""closure"" meant. This is how my textbook defines it, For a set E of real number, a real number x is called a point of point of closure of E provided every open interval that contians x also contains a point in E. The collection of points of closure of E is called the closure of E and denoted by $\bar{E}$...It is clear that we always have $E \subseteq \bar{E}$. I'm not sure if I understood this theorem correctly, because...from what I understand the (a,b) needs to be included in the closures (since $E \subseteq \bar{E}$ ) of the ascending sequences, right? But it says the opposite (""closures of which are contained in (a,b)). So could anybody try the clarify this for me? Thanks in advance","Let f be a monotone function on the open interval (a,b). Then f is continuous except possibly at a countable number of points in (a,b). Assume f is increasing. Furthermore, assume (a,b) is bounded and f is increasing on the closed interval [a,b]. Otherwise, express (a,b) as the union of an ascending sequence of open, bounded intervals, the closures of which are contained in (a,b), and take the union of the discontinuities in each of this countable collection of intervals. I was a little confused about the last sentence, because I wasn't sure of what ""closure"" meant. This is how my textbook defines it, For a set E of real number, a real number x is called a point of point of closure of E provided every open interval that contians x also contains a point in E. The collection of points of closure of E is called the closure of E and denoted by $\bar{E}$...It is clear that we always have $E \subseteq \bar{E}$. I'm not sure if I understood this theorem correctly, because...from what I understand the (a,b) needs to be included in the closures (since $E \subseteq \bar{E}$ ) of the ascending sequences, right? But it says the opposite (""closures of which are contained in (a,b)). So could anybody try the clarify this for me? Thanks in advance",,[]
96,"When does pointwise convergence on $[a,b]$ imply uniform convergence on $[a,b]$",When does pointwise convergence on  imply uniform convergence on,"[a,b] [a,b]","Let $(f_n)$ be a sequence of continuous functions on $[a,b]$. Suppose that, for each $x\in[a,b]$, $(f_n(x))$ is a nonincreasing sequence of real numbers. a) Prove that if $f_n\to 0$ pointwise on $[a,b]$, then $f_n\to 0$ uniformly on $[a,b]$. b) Prove that if $f_n\to f$ pointwise on $[a,b]$, and if $f$ is continuous on $[a,b]$, then $f_n\to f$ uniformly on $[a,b]$.","Let $(f_n)$ be a sequence of continuous functions on $[a,b]$. Suppose that, for each $x\in[a,b]$, $(f_n(x))$ is a nonincreasing sequence of real numbers. a) Prove that if $f_n\to 0$ pointwise on $[a,b]$, then $f_n\to 0$ uniformly on $[a,b]$. b) Prove that if $f_n\to f$ pointwise on $[a,b]$, and if $f$ is continuous on $[a,b]$, then $f_n\to f$ uniformly on $[a,b]$.",,"['real-analysis', 'sequences-and-series', 'uniform-convergence']"
97,Series $\sum\frac{1}{b_n}$ where $ b_n=\sum_{k=1}^n(-1)^{k+1}\sqrt{k}.$,Series  where,\sum\frac{1}{b_n}  b_n=\sum_{k=1}^n(-1)^{k+1}\sqrt{k}.,Let  $$b_n=\sum_{k=1}^n(-1)^{k+1}\sqrt{k}.$$ I look for the following series $\displaystyle \sum\frac{1}{b_n}$ is convergent or divergent ? Any suggestion ?,Let  $$b_n=\sum_{k=1}^n(-1)^{k+1}\sqrt{k}.$$ I look for the following series $\displaystyle \sum\frac{1}{b_n}$ is convergent or divergent ? Any suggestion ?,,['calculus']
98,Solving the sequential equation $\sum_{k=0}^{n-1}\frac{a_{n-k}}{k+1}=\frac{1}{n+1}$,Solving the sequential equation,\sum_{k=0}^{n-1}\frac{a_{n-k}}{k+1}=\frac{1}{n+1},"$a_1,a_2,a_3,...$ is a sequence of real numbers such that for $n\in \mathbb{N}$: $$\sum_{k=0}^{n-1}\frac{a_{n-k}}{k+1}=\frac{1}{n+1}$$ How can I prove: $$a_n=\int_1^\infty{\frac{dx}{x^n(\pi^2+\ln^2(x-1))}}$$","$a_1,a_2,a_3,...$ is a sequence of real numbers such that for $n\in \mathbb{N}$: $$\sum_{k=0}^{n-1}\frac{a_{n-k}}{k+1}=\frac{1}{n+1}$$ How can I prove: $$a_n=\int_1^\infty{\frac{dx}{x^n(\pi^2+\ln^2(x-1))}}$$",,['calculus']
99,A limit involves series and factorials,A limit involves series and factorials,,Evaluate : $$\lim_{n\to \infty }\frac{n!}{{{n}^{n}}}\left( \sum\limits_{k=0}^{n}{\frac{{{n}^{k}}}{k!}-\sum\limits_{k=n+1}^{\infty }{\frac{{{n}^{k}}}{k!}}} \right)$$,Evaluate : $$\lim_{n\to \infty }\frac{n!}{{{n}^{n}}}\left( \sum\limits_{k=0}^{n}{\frac{{{n}^{k}}}{k!}-\sum\limits_{k=n+1}^{\infty }{\frac{{{n}^{k}}}{k!}}} \right)$$,,"['calculus', 'real-analysis', 'sequences-and-series', 'limits', 'factorial']"
