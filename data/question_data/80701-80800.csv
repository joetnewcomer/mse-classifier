,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,solvable subalgebra,solvable subalgebra,,"I want to show that a set $B\subset L$ is a maximal solvable subalgebra. With $L = \mathscr{o}(8,F)$, $F$ and algebraically closed field, and $\operatorname{char}(F)=0$ and  $$B= \left\{\begin{pmatrix}p&q\\0&s \end{pmatrix}\mid p \textrm{ upper triangular, }q,s\in\mathscr{gl}(4,F)\textrm{ and }p^t=-s, q^t=-q\right\}.$$ $B$ is a subalgebra by construction. So my problem now is how I can show that it is maximal solvable. I tried the approach by '$L$ solvable $\Leftrightarrow$ $[L,L]$ nilpotent'. I am not sure how good this idea is, but here what I have: $[B,B]= span\{[x,y]\mid x,y\in B\}$. So $x$ and $y$ are matrices of the form above. That means, that for the '$p$-part' we see already that it is nilpotent. Since with each multiplication there is one more zero-diagonal. But how can I show now, that we will also get rid of the $q$ and $s$ part? I tried to look at $[x,y] = xy-yx = \begin{pmatrix}\bigtriangledown&*\\0&ss' \end{pmatrix}-\begin{pmatrix}\bigtriangledown&*\\ 0&s's \end{pmatrix}$ (with $\bigtriangledown$ any upper triangle matrix minus one diagonal, $s$ the part in the $x$ matrix and $s'$ in the $y$ matrix). (Sorry for the chaotic notation. I don't realy know how to write it easier..) Is this a beginning where I should go on with? Or does someone have a hint how to approach this problem? I don't really know, how to go on from here on. So I'd be very happy for any hint :) Best, Luca","I want to show that a set $B\subset L$ is a maximal solvable subalgebra. With $L = \mathscr{o}(8,F)$, $F$ and algebraically closed field, and $\operatorname{char}(F)=0$ and  $$B= \left\{\begin{pmatrix}p&q\\0&s \end{pmatrix}\mid p \textrm{ upper triangular, }q,s\in\mathscr{gl}(4,F)\textrm{ and }p^t=-s, q^t=-q\right\}.$$ $B$ is a subalgebra by construction. So my problem now is how I can show that it is maximal solvable. I tried the approach by '$L$ solvable $\Leftrightarrow$ $[L,L]$ nilpotent'. I am not sure how good this idea is, but here what I have: $[B,B]= span\{[x,y]\mid x,y\in B\}$. So $x$ and $y$ are matrices of the form above. That means, that for the '$p$-part' we see already that it is nilpotent. Since with each multiplication there is one more zero-diagonal. But how can I show now, that we will also get rid of the $q$ and $s$ part? I tried to look at $[x,y] = xy-yx = \begin{pmatrix}\bigtriangledown&*\\0&ss' \end{pmatrix}-\begin{pmatrix}\bigtriangledown&*\\ 0&s's \end{pmatrix}$ (with $\bigtriangledown$ any upper triangle matrix minus one diagonal, $s$ the part in the $x$ matrix and $s'$ in the $y$ matrix). (Sorry for the chaotic notation. I don't realy know how to write it easier..) Is this a beginning where I should go on with? Or does someone have a hint how to approach this problem? I don't really know, how to go on from here on. So I'd be very happy for any hint :) Best, Luca",,"['matrices', 'lie-algebras', 'solvable-groups']"
1,Is there a name for such matrices,Is there a name for such matrices,,"Let $Z$ be a $K \times K$ matrix. All its left diagonal elements are zero. Further it satisfies the following properties: 1.) $Z[ i ][ j ] = Z[j][i] > 0$ for $1 ≤ i < j ≤ K$. 2.) $Z[i][j]$ belongs to ${1, 2, ... K-2}$ for each $1 ≤ i < j ≤ K$. 3.) For each $r \in {1, 2, ..., K − 2}$, there exist $i, j ∈ \{1, 2, ..., K\}$ such that $Z[i][j] = r$. 4.) $Z[i][j] ≤ \max(Z[i][l], Z[l][j])$ for $1 ≤ i, j, l ≤ N$. For example $$\quad Z=\left(\begin{matrix}0 &1&2 &2\\1 &0 &2 &2 \\ 2 &2& 0& 2\\ 2& 2& 2& 0\\\end{matrix}\right).$$ Is there any special name for such matrices? I came across them in my homework on permutations, so I believe they must be common. I would also be interested in number of such matrices for order K","Let $Z$ be a $K \times K$ matrix. All its left diagonal elements are zero. Further it satisfies the following properties: 1.) $Z[ i ][ j ] = Z[j][i] > 0$ for $1 ≤ i < j ≤ K$. 2.) $Z[i][j]$ belongs to ${1, 2, ... K-2}$ for each $1 ≤ i < j ≤ K$. 3.) For each $r \in {1, 2, ..., K − 2}$, there exist $i, j ∈ \{1, 2, ..., K\}$ such that $Z[i][j] = r$. 4.) $Z[i][j] ≤ \max(Z[i][l], Z[l][j])$ for $1 ≤ i, j, l ≤ N$. For example $$\quad Z=\left(\begin{matrix}0 &1&2 &2\\1 &0 &2 &2 \\ 2 &2& 0& 2\\ 2& 2& 2& 0\\\end{matrix}\right).$$ Is there any special name for such matrices? I came across them in my homework on permutations, so I believe they must be common. I would also be interested in number of such matrices for order K",,"['matrices', 'permutations']"
2,General Solution to $\operatorname{Tr} \ln ( I + A)$ where A is complex and symmetric and zero on the diagonal,General Solution to  where A is complex and symmetric and zero on the diagonal,\operatorname{Tr} \ln ( I + A),"Are there any useful identities which would help me to find a general formula for $ \operatorname{Tr} \ln ( I + A ) $ Where I is the identity matrix and A is some N by N complex and symmetric matrix, which has zeroes along the diagonal. I have tried splitting A up into upper and lower triangular components and expanding the log with the mercator series but it's starting to get pretty messy already so I think I missed something. Another property of A which could be useful is that it has a special repeating form along the rows and columns: \begin{array}{ccccccc} 0 & b & c & d & e ..\\ b & 0 & b & c & d.. \\ c & b & 0 & b & c...\\ d & c & b & 0 & b...\\ . & . & . & . & ...\  \end{array} It's been a while since I studied matrices so if there is a special name for this I've forgotten! EDIT: Did a bit of research, this type of matrix is a ""Symmetric Toeplitz Matrix"".","Are there any useful identities which would help me to find a general formula for $ \operatorname{Tr} \ln ( I + A ) $ Where I is the identity matrix and A is some N by N complex and symmetric matrix, which has zeroes along the diagonal. I have tried splitting A up into upper and lower triangular components and expanding the log with the mercator series but it's starting to get pretty messy already so I think I missed something. Another property of A which could be useful is that it has a special repeating form along the rows and columns: \begin{array}{ccccccc} 0 & b & c & d & e ..\\ b & 0 & b & c & d.. \\ c & b & 0 & b & c...\\ d & c & b & 0 & b...\\ . & . & . & . & ...\  \end{array} It's been a while since I studied matrices so if there is a special name for this I've forgotten! EDIT: Did a bit of research, this type of matrix is a ""Symmetric Toeplitz Matrix"".",,"['matrices', 'trace']"
3,Property of the trace of matrices,Property of the trace of matrices,,"Let $A(x,t),B(x,t)$ be matrix-valued functions that are independent of $\xi=x-t$ and satisfy $$A_t-B_x+AB-BA=0$$ where $X_q\equiv \frac{\partial X}{\partial q}$. Why does it then follow that  $$\frac{d }{d \eta}\textrm{Trace}[(A-B)^n]=0$$ where $n\in \mathbb N$ and $\eta=x+t$? Is there a neat way to see that this is true?","Let $A(x,t),B(x,t)$ be matrix-valued functions that are independent of $\xi=x-t$ and satisfy $$A_t-B_x+AB-BA=0$$ where $X_q\equiv \frac{\partial X}{\partial q}$. Why does it then follow that  $$\frac{d }{d \eta}\textrm{Trace}[(A-B)^n]=0$$ where $n\in \mathbb N$ and $\eta=x+t$? Is there a neat way to see that this is true?",,"['linear-algebra', 'matrices', 'partial-differential-equations', 'trace']"
4,How to solve cross-products including matrices?,How to solve cross-products including matrices?,,"I'm a programmer and I'm doing a whitebalance-transformation in RGB colorspace. This should work with this transformation matrix that I've found in literature: $$   \begin{pmatrix}   R \\   G \\   B  \end{pmatrix}  =  \begin{pmatrix}   \frac{255}{R_w} & 0 & 0 \\   0 & \frac{255}{G_w} & 0 \\   0 & 0 & \frac{255}{B_w}  \end{pmatrix}  \times \begin{pmatrix}   R´ \\   G´ \\   B´  \end{pmatrix} $$ With a colorspace where the range of all possible values is $ \Big[0, 255\Big]$ (which is 8 bit). I've been reading that a cross-product including matrices (as seen above) can be solved like a system of linear equations. $$ R = \big( \frac{255}{R_w} + 0 + 0 \big) \cdot R´ = \frac{255}{R_w} \cdot R´ \\ G = \big( 0 + \frac{255}{G_w} + 0 \big) \cdot G´ = \frac{255}{G_w} \cdot G´ \\ B = \big( 0 + 0 + \frac{255}{B_w} \big) \cdot B´ = \frac{255}{B_w} \cdot B´ \\ $$ But that seems to be a faulty approach as $R$, $G$ and $B$ can be bigger than 255 which is no valid result of that transformation. Example Original pixel: $$  R´ = 111; G´ = 154; B´ = 255 $$ Whitespace transformation input: $$  R_w = 123; G_w = 138; B_w = 217 $$ Results with my approach: $$ R = \frac{255}{123} \cdot 111 = 230 \\  G = \frac{255}{138} \cdot 154 = 284 \\  B = \frac{255}{217} \cdot 255 = 299$$ Obviously, the results are out of the range of my 8 bit colorspace. How to correctly solve this whitespace transformtation? I'm mainly lost as I have no idea how to handle the matrix within a cross-product.","I'm a programmer and I'm doing a whitebalance-transformation in RGB colorspace. This should work with this transformation matrix that I've found in literature: $$   \begin{pmatrix}   R \\   G \\   B  \end{pmatrix}  =  \begin{pmatrix}   \frac{255}{R_w} & 0 & 0 \\   0 & \frac{255}{G_w} & 0 \\   0 & 0 & \frac{255}{B_w}  \end{pmatrix}  \times \begin{pmatrix}   R´ \\   G´ \\   B´  \end{pmatrix} $$ With a colorspace where the range of all possible values is $ \Big[0, 255\Big]$ (which is 8 bit). I've been reading that a cross-product including matrices (as seen above) can be solved like a system of linear equations. $$ R = \big( \frac{255}{R_w} + 0 + 0 \big) \cdot R´ = \frac{255}{R_w} \cdot R´ \\ G = \big( 0 + \frac{255}{G_w} + 0 \big) \cdot G´ = \frac{255}{G_w} \cdot G´ \\ B = \big( 0 + 0 + \frac{255}{B_w} \big) \cdot B´ = \frac{255}{B_w} \cdot B´ \\ $$ But that seems to be a faulty approach as $R$, $G$ and $B$ can be bigger than 255 which is no valid result of that transformation. Example Original pixel: $$  R´ = 111; G´ = 154; B´ = 255 $$ Whitespace transformation input: $$  R_w = 123; G_w = 138; B_w = 217 $$ Results with my approach: $$ R = \frac{255}{123} \cdot 111 = 230 \\  G = \frac{255}{138} \cdot 154 = 284 \\  B = \frac{255}{217} \cdot 255 = 299$$ Obviously, the results are out of the range of my 8 bit colorspace. How to correctly solve this whitespace transformtation? I'm mainly lost as I have no idea how to handle the matrix within a cross-product.",,"['matrices', 'transformation', 'image-processing', 'cross-product']"
5,Passing from integration over hermitian matrices to integration over eigenvalues.,Passing from integration over hermitian matrices to integration over eigenvalues.,,"Integral over hermitian matrices $\int dH$ in terms of matrix $H$ components $h_{ij}$ is defined by $$\int dH f(H)=\int \prod_idh_{ii}\prod_{i<j}d^2h_{ij} f(H)$$ Integrals over diagonal components are one-dimensional since these components are obligatory real for hermitian matrix and integrals over independent non-diagonal elements are two-dimensional. Any hermitian matrix $H$ can be diagonalized by means of some unitary matrix $U$ such that $H=UDU^\dagger$ with $D=diag\{d_1,...\}$ and $d_1,...$ being eigenvalues of $H$. As far as I know there is standard procedure to pass from integration over $h_{ij}$ to integration over $d_i$ and independent components of $U$,  $u_{ij}$. Such transformation is useful if one integrates function that is invariant with respect to such unitary transformations, say $f_{inv}(H)= Tr(H) $. My question is how exactly one performs this change of variables. Technically, my problem is that I don't know the convenient way to parametrize arbitrary unitary matrix and therefore how to compute Jacobian of this variable change. As far as I know for an invariant function $f_{inv}$ the result is something like $$\int dH f_{inv}(H)\sim Vol(U)\int \prod_id_i \prod_{i<j}(d_i-d_j)^2f_{inv}(H)$$  where $Vol(U)$ is in a certain sense the ""volume"" of the group of unitary matrices and non-trivial factor being the Vandermonde determinant appeared. Any suggestions on how to approach this problem?","Integral over hermitian matrices $\int dH$ in terms of matrix $H$ components $h_{ij}$ is defined by $$\int dH f(H)=\int \prod_idh_{ii}\prod_{i<j}d^2h_{ij} f(H)$$ Integrals over diagonal components are one-dimensional since these components are obligatory real for hermitian matrix and integrals over independent non-diagonal elements are two-dimensional. Any hermitian matrix $H$ can be diagonalized by means of some unitary matrix $U$ such that $H=UDU^\dagger$ with $D=diag\{d_1,...\}$ and $d_1,...$ being eigenvalues of $H$. As far as I know there is standard procedure to pass from integration over $h_{ij}$ to integration over $d_i$ and independent components of $U$,  $u_{ij}$. Such transformation is useful if one integrates function that is invariant with respect to such unitary transformations, say $f_{inv}(H)= Tr(H) $. My question is how exactly one performs this change of variables. Technically, my problem is that I don't know the convenient way to parametrize arbitrary unitary matrix and therefore how to compute Jacobian of this variable change. As far as I know for an invariant function $f_{inv}$ the result is something like $$\int dH f_{inv}(H)\sim Vol(U)\int \prod_id_i \prod_{i<j}(d_i-d_j)^2f_{inv}(H)$$  where $Vol(U)$ is in a certain sense the ""volume"" of the group of unitary matrices and non-trivial factor being the Vandermonde determinant appeared. Any suggestions on how to approach this problem?",,"['matrices', 'integration']"
6,COLAMD matrix reordering algorithm in MATLAB,COLAMD matrix reordering algorithm in MATLAB,,"Background I'm dealing with some variable size square sparse matrices resulting from a FEM analysis, and my next step is optimizing the system solving in terms of speed. This is a visualization of some aspects of a typical matrix using MATLAB's spy : Please note I'm not using MATLAB (or this question would be useless), I just used it for generating the above graph. The top left graph show my matrix as-is (non zero elements are blue), and bottom left shows the LU decomposition of said matrix. On the top right I have applied the COLAMD algorithm (through MATLAB function colamd ) to generate a permutation to my matrix that would result in sparser LU factorized matrix, which indeed happens as the bottom right graph suggests (less than a third of the non zero elements I had before). Problem Now my problem is understanding how to implement the COLAMD function by hand in the language I'm using. I've been browsing through the COLAMD project page , read the paper on it , and tried to decypher the C source code to no avail. What I'm looking for is an explanation of the algorithm in the likes of the ones provided by Rosetta Code ( example of an algorithm ), or some pseudo-code, basically anything that clearly shows what steps must be implemented. I'm not sure if this belongs in Stack Overflow or here, please advise if it's in the wrong place. Thanks.","Background I'm dealing with some variable size square sparse matrices resulting from a FEM analysis, and my next step is optimizing the system solving in terms of speed. This is a visualization of some aspects of a typical matrix using MATLAB's spy : Please note I'm not using MATLAB (or this question would be useless), I just used it for generating the above graph. The top left graph show my matrix as-is (non zero elements are blue), and bottom left shows the LU decomposition of said matrix. On the top right I have applied the COLAMD algorithm (through MATLAB function colamd ) to generate a permutation to my matrix that would result in sparser LU factorized matrix, which indeed happens as the bottom right graph suggests (less than a third of the non zero elements I had before). Problem Now my problem is understanding how to implement the COLAMD function by hand in the language I'm using. I've been browsing through the COLAMD project page , read the paper on it , and tried to decypher the C source code to no avail. What I'm looking for is an explanation of the algorithm in the likes of the ones provided by Rosetta Code ( example of an algorithm ), or some pseudo-code, basically anything that clearly shows what steps must be implemented. I'm not sure if this belongs in Stack Overflow or here, please advise if it's in the wrong place. Thanks.",,"['matrices', 'permutations', 'numerical-linear-algebra', 'matlab', 'sparse-matrices']"
7,Eigenvalues of discretized linear integral operator,Eigenvalues of discretized linear integral operator,,"Suppose I have the following kernel operator: $Af(x) = \int_{-1}^1 K(x-y)f(y)dy$ which is also positive and compact. Hence, it has a countable set of positive eigenvalues. Suppose those eigenvalues are known and denoted by $\lambda_1, \lambda_2,...$ Now suppose I take a discrete version of the operator, which is represented by a matrix of size $N$ whose entries are given by $K_{mn} = K(x_m-x_n)$. where $x_m, x_n$ are at equispaced points (or any other way if it helps). What can we say about the eigenvalues of the matrix $K$ with respect to the eigenvalues of the continuous operator $\lambda_1,\lambda_2,...$ ?","Suppose I have the following kernel operator: $Af(x) = \int_{-1}^1 K(x-y)f(y)dy$ which is also positive and compact. Hence, it has a countable set of positive eigenvalues. Suppose those eigenvalues are known and denoted by $\lambda_1, \lambda_2,...$ Now suppose I take a discrete version of the operator, which is represented by a matrix of size $N$ whose entries are given by $K_{mn} = K(x_m-x_n)$. where $x_m, x_n$ are at equispaced points (or any other way if it helps). What can we say about the eigenvalues of the matrix $K$ with respect to the eigenvalues of the continuous operator $\lambda_1,\lambda_2,...$ ?",,"['linear-algebra', 'matrices', 'functional-analysis', 'eigenvalues-eigenvectors', 'numerical-linear-algebra']"
8,What is the name for a non-square permutation matrix?,What is the name for a non-square permutation matrix?,,"Consider a matrix that selects and permutes some but not all of the entries of a vector. That is a binary $n\times m$ matrix, where $n<m$, with a single one per row, for example $\begin{bmatrix}0&1&0\\1&0&0\end{bmatrix}$. Do such matrices have a name? I had a look online and couldn't find anything. Thanks.","Consider a matrix that selects and permutes some but not all of the entries of a vector. That is a binary $n\times m$ matrix, where $n<m$, with a single one per row, for example $\begin{bmatrix}0&1&0\\1&0&0\end{bmatrix}$. Do such matrices have a name? I had a look online and couldn't find anything. Thanks.",,"['linear-algebra', 'matrices', 'terminology']"
9,Why are there $736$ matrices $M\in \mathcal M_2(\mathbb{Z}_{26})$ for which it holds that $M=M^{-1}$?,Why are there  matrices  for which it holds that ?,736 M\in \mathcal M_2(\mathbb{Z}_{26}) M=M^{-1},"I'm currently trying to introduce myself to cryptography. I'm reading about the Hill Cipher currently in the book Applied Abstract Algebra. The Hill Cipher uses an invertible matrix $M$ for encryption and the inverse matrix $M^{-1}$ for decryption. The book recommends to use a matrix $M$ for which it holds that $M=M^{-1}$, since we then have the same key for encryption and decryption. In the book, they use a $2 \times 2$ matrix $M$ over $\mathbb{Z}_{26}$ as example, and state that for there are 736 $2 \times 2$ matrices for which it hold that $M=M^{-1}$. I'm trying to pick up on as much as possible when reading things, since I find it counter-productive for learning to skip something, when you don't get the theory behind it. Can someone enlighten to me, as to why it is that there are 736 possible $2 \times 2$ $M=M^{-1}$ matrices and how to find them?","I'm currently trying to introduce myself to cryptography. I'm reading about the Hill Cipher currently in the book Applied Abstract Algebra. The Hill Cipher uses an invertible matrix $M$ for encryption and the inverse matrix $M^{-1}$ for decryption. The book recommends to use a matrix $M$ for which it holds that $M=M^{-1}$, since we then have the same key for encryption and decryption. In the book, they use a $2 \times 2$ matrix $M$ over $\mathbb{Z}_{26}$ as example, and state that for there are 736 $2 \times 2$ matrices for which it hold that $M=M^{-1}$. I'm trying to pick up on as much as possible when reading things, since I find it counter-productive for learning to skip something, when you don't get the theory behind it. Can someone enlighten to me, as to why it is that there are 736 possible $2 \times 2$ $M=M^{-1}$ matrices and how to find them?",,"['linear-algebra', 'matrices', 'modular-arithmetic', 'cryptography']"
10,Simplifying covariance matrices in distributions,Simplifying covariance matrices in distributions,,"In the multivariate Gaussian distribution, it is required that the covariance matrix be positive semidefinite. I have read that a positive semidefinite matrix $\Sigma$ can be written as $LL^{T}$. I have also seen that $\Sigma=(C^{T}C)^{-1}$ where $C$ is positive definite (is this true?). However, I'm entirely sure why this is true. I want to know what facts do we know about positive semidefinite matrices that can be used in order to manipulate a multivariate Gaussian or similar distributions. For example, in a Gaussian distribution: $$f(x)=\exp{\{-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)\}}$$ we can use $\Sigma^{-1}=C^{T}C$ (although I'm not entirely certain this is correct). Therefore, a change of variable $y = C (x-\mu)$  can be used to write $f(x)$ as: $$f(x)=\exp{\{-\frac{1}{2}y^{T}y\}}$$ Another example could be using $\Sigma^{-1} = \sum_{i=1}^{D} \frac{1}{\lambda_{i}}u_{i}u_{i}^{T}$ where $u_{i}$ are the eigenvectors of $\Sigma$ and $\lambda_{i}$ its eigenvalues. This expression also can be used to simplify $f(x)$. What other representations are there for $\Sigma^{-1}$? Or what is the correct procedure in the examples I described above? UPDATE: A third example could be simply taking $\Sigma^{-1} = \Sigma^{-\frac{1}{2} T}\Sigma^{-\frac{1}{2}}$ (because $\Sigma$ is symmetric) and change variables with $y = \Sigma^{-1/2}(x-\mu)$. Initiallly I had written a fourth option with $\Sigma$ as $G^{T}DG$ with $G$ as a orthogonal matrix and $D$ as an diagonal matrix containing $\Sigma$'s eigenvalues. I think that's wrong and the correct decomposition should be $\Sigma=G\Lambda G^{T}$, however this still produces issues because a change of variable $y=G^{T}(x-\mu)$ leads to a differential volume in the following form: $$dy_{1}dy_{2}...dy_{n} = |G^{T}|dx_{1}dx_{2}...dx_{n}$$ but the determinant of $G$ might be 1 or -1, which is not good. Thanks!","In the multivariate Gaussian distribution, it is required that the covariance matrix be positive semidefinite. I have read that a positive semidefinite matrix $\Sigma$ can be written as $LL^{T}$. I have also seen that $\Sigma=(C^{T}C)^{-1}$ where $C$ is positive definite (is this true?). However, I'm entirely sure why this is true. I want to know what facts do we know about positive semidefinite matrices that can be used in order to manipulate a multivariate Gaussian or similar distributions. For example, in a Gaussian distribution: $$f(x)=\exp{\{-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)\}}$$ we can use $\Sigma^{-1}=C^{T}C$ (although I'm not entirely certain this is correct). Therefore, a change of variable $y = C (x-\mu)$  can be used to write $f(x)$ as: $$f(x)=\exp{\{-\frac{1}{2}y^{T}y\}}$$ Another example could be using $\Sigma^{-1} = \sum_{i=1}^{D} \frac{1}{\lambda_{i}}u_{i}u_{i}^{T}$ where $u_{i}$ are the eigenvectors of $\Sigma$ and $\lambda_{i}$ its eigenvalues. This expression also can be used to simplify $f(x)$. What other representations are there for $\Sigma^{-1}$? Or what is the correct procedure in the examples I described above? UPDATE: A third example could be simply taking $\Sigma^{-1} = \Sigma^{-\frac{1}{2} T}\Sigma^{-\frac{1}{2}}$ (because $\Sigma$ is symmetric) and change variables with $y = \Sigma^{-1/2}(x-\mu)$. Initiallly I had written a fourth option with $\Sigma$ as $G^{T}DG$ with $G$ as a orthogonal matrix and $D$ as an diagonal matrix containing $\Sigma$'s eigenvalues. I think that's wrong and the correct decomposition should be $\Sigma=G\Lambda G^{T}$, however this still produces issues because a change of variable $y=G^{T}(x-\mu)$ leads to a differential volume in the following form: $$dy_{1}dy_{2}...dy_{n} = |G^{T}|dx_{1}dx_{2}...dx_{n}$$ but the determinant of $G$ might be 1 or -1, which is not good. Thanks!",,"['matrices', 'statistics', 'probability-distributions', 'normal-distribution']"
11,counterexample for: integral forms that are equivalent as rational forms are also equivalent as integral forms,counterexample for: integral forms that are equivalent as rational forms are also equivalent as integral forms,,"I have the feeling that I'm missing something very obvious: I'm looking for a counterexmple for the following statement for some $n>1$ (it is trivially true for $n=1$): Let $A,B\in\mathbb Z^{n\times n}$ symmetric with determinant $\pm 1$ such that there exists an invertible matrix $C\in\mathbb Q^{n\times n}$ with $C^{-1}AC=B$. Then there exists $S\in\mathbb Z^{n\times n}$ with determinant $\pm 1$ such that $S^{-1}AS=B$. The motivation behind it is that I want to give an example for two quadratic integral forms of discriminant $\pm 1$ that are not equivalent as integral forms but as rational forms. This then gives that the notion of integral form equivalence is ""finer"" than that of rational form equivalence (a statement from Serre's book ""A course in arithmetic"" Chapter 5, 1.1). It may also be that my approach is clumsy, then I would be very thankful for inspiration for a different one.","I have the feeling that I'm missing something very obvious: I'm looking for a counterexmple for the following statement for some $n>1$ (it is trivially true for $n=1$): Let $A,B\in\mathbb Z^{n\times n}$ symmetric with determinant $\pm 1$ such that there exists an invertible matrix $C\in\mathbb Q^{n\times n}$ with $C^{-1}AC=B$. Then there exists $S\in\mathbb Z^{n\times n}$ with determinant $\pm 1$ such that $S^{-1}AS=B$. The motivation behind it is that I want to give an example for two quadratic integral forms of discriminant $\pm 1$ that are not equivalent as integral forms but as rational forms. This then gives that the notion of integral form equivalence is ""finer"" than that of rational form equivalence (a statement from Serre's book ""A course in arithmetic"" Chapter 5, 1.1). It may also be that my approach is clumsy, then I would be very thankful for inspiration for a different one.",,"['matrices', 'quadratic-forms']"
12,"Jacobi's Rotation has two possibilities, why do they both result in same upper triangular magnitude norm?","Jacobi's Rotation has two possibilities, why do they both result in same upper triangular magnitude norm?",,"The Jacobi's rotation is the complex Givens rotation (unitary similarity) that results in a zero for a specified element of a matrix. If the element  is not adjacent to the diagonal, then there are rows/columns below the diagonal that mix with rows/columns above the diagonal. Here is the general form (where eventually $x$ will be the element to zero): \begin{align} &    \pmatrix{c               & \mathbf{0} & s \\             \mathbf{0}^\top & \mathbf{I} & \mathbf{0}^\top \\             -\bar{s}        & \mathbf{0} & \bar{c} \\    }    \pmatrix{d_0             & \mathbf{g} & x \\             \mathbf{a}^\top & \mathbf{D} & \mathbf{b}^\top \\             y               & \mathbf{h} & d_1 \\    }    \pmatrix{\bar{c}         & \mathbf{0} & -s \\             \mathbf{0}^\top & \mathbf{I} & \mathbf{0}^\top \\             \bar{s}        & \mathbf{0} & c \\    } \\ =&    \pmatrix{c               & \mathbf{0} & s \\             \mathbf{0}^\top & \mathbf{I} & \mathbf{0}^\top \\             -\bar{s}        & \mathbf{0} & \bar{c} \\    }    \pmatrix{d_0\bar{c}+x\bar{s}   & \mathbf{g} & -d_0 s + x c \\             \mathbf{a}^\top\bar{c} + \mathbf{b}^\top\bar{s}& \mathbf{D} & -\mathbf{a}^\top s + \mathbf{b}^\top c \\             y \bar{c} + d_1 \bar{s}              & \mathbf{h} & -y s + d_1 c \\    } \\ =&    \pmatrix{d_0c\bar{c} + d_1s\bar{s} + c\bar{s}x + \bar{c}sy  & c\mathbf{g}+ s\mathbf{h}                & (d_1 - d_0)cs + x c^2 - ys^2 \\             \mathbf{a}^\top \bar{c} + \mathbf{b}^\top \bar{s}  &        \mathbf{D}                       & -\mathbf{a}^\top s + \mathbf{b}^\top c \\             (d_1 -d_0)\bar{c}\bar{s} -x\bar{s}^2 + y\bar{c}^2  & -\bar{s}\mathbf{g} + \bar{c}\mathbf{h}  & d_0s\bar{s} + d_1c\bar{c} - c\bar{s}x - \bar{c}sy    } \end{align} Specifically $\mathbf{g} \leftarrow c\mathbf{g}+ s\mathbf{h}$, so that row vector $\mathbf{g}$ is mixed with row vector $\mathbf{h}$. Now solving for $c$ and $s$ in the complex numbers that give $x \leftarrow 0$ results in two solutions. Both of which end with the same magnitude in the upper triangular portion $$\Vert \mathbf{g} \Vert^2 + \Vert \mathbf{b} \Vert^2 \quad\text{is same for both solutions that give $x=0$}$$ I have yet to find the formulation that shows this, but multiple experimentation has yet to fail. EDIT I have made a silly mistake in that all tests were done on Hermitian matrices. Now I guess I solved my question, but any discussions or error corrections are welcome... Here is the formula for the rotation: \begin{align}   \delta &= d_1 - d_0 \\   \Delta &= \pm\sqrt{\delta^2 + 4xy} \\   k^2 &= \delta\bar{\delta} + \delta\bar{\Delta} + \bar{\delta}\Delta + \Delta\bar{\Delta} +4y\bar{y}\\   c & = \frac{2y}{k} \\   s & = \frac{\delta + \Delta}{k} \\ \end{align} The following is the derivation. Using $d_1 - d_0 = \delta$, the top right element is desired to be zero $$\delta cs + x c^2 - ys^2 = 0$$ Use the substitution $$\alpha = \frac{s}{c}$$ then \begin{align}   \delta c^2\alpha + x c^2 - y\alpha^2c^2 = c^2\left(\delta\alpha + x - y\alpha^2\right) = 0 \end{align} The equation $\delta\alpha + x - y\alpha^2=0$ is easily solved using the quadratic formula: $$ \alpha = \frac{-\delta \pm \sqrt{\delta^2 - 4(-y)x}}{2(-y)} = \frac{\delta \mp \sqrt{\delta^2 + 4xy}}{2y} = \frac{k\cdot s}{k\cdot c}$$ Here $k$ is the real and non-zero scale factor that gives $c\bar{c} + s\bar{s} = 1$. Denote $\Delta = \pm\sqrt{\delta^2 + 4xy}$.  Then we have \begin{align}   c &= \frac{2y}{k} \\   s &= \frac{\delta+\Delta}{k} \\   s\bar{s} + c\bar{c} &= \frac{(\delta+\Delta)(\bar{\delta} + \bar{\Delta}) + 4y\bar{y}}{k^2} = 1 \\   \Rightarrow k^2 &= \delta\bar{\delta} + \delta\bar{\Delta} + \bar{\delta}\Delta + \Delta\bar{\Delta} +4y\bar{y}\\ \end{align} This formulation gives the desired results for both $\pm\Delta$ (as is confirmed with computations). Now looking at the magnitude $\Vert c\mathbf{g}+ s\mathbf{h} \Vert^2 + \Vert  c\mathbf{b}  -s\mathbf{a} \Vert^2$ we have \begin{align}   \Vert c\mathbf{g}+ s\mathbf{h} \Vert^2 + \Vert c\mathbf{b}  -s\mathbf{a} \Vert^2  = & (c\mathbf{g}+ s\mathbf{h} )(\bar{\mathbf{g}}^\top \bar{c}+ \bar{\mathbf{h}}^\top\bar{s}) \\   & + (c\mathbf{b} - s\mathbf{a} )(\bar{\mathbf{b}}^\top \bar{c} - \bar{\mathbf{a}}^\top\bar{s})\\   = & c\bar{c}\mathbf{g}\bar{\mathbf{g}}^\top + s\bar{s}\mathbf{h}\bar{\mathbf{h}}^\top + \bar{c}s\mathbf{h}\bar{\mathbf{g}}^\top + c\bar{s}\mathbf{g}\bar{\mathbf{h}}^\top\\   & + c\bar{c}\mathbf{b}\bar{\mathbf{b}}^\top + s\bar{s}\mathbf{a}\bar{\mathbf{a}}^\top - \bar{c}s\mathbf{a}\bar{\mathbf{b}}^\top - c\bar{s}\mathbf{b}\bar{\mathbf{a}}^\top \\   = & c\bar{c}(\underbrace{\mathbf{g}\bar{\mathbf{g}}^\top + \mathbf{b}\bar{\mathbf{b}}^\top}_{\omega}) + s\bar{s}(\underbrace{\mathbf{h}\bar{\mathbf{h}}^\top + \mathbf{a}\bar{\mathbf{a}}^\top}_{\theta}) + \bar{c}s(\underbrace{\mathbf{h}\bar{\mathbf{g}}^\top - \mathbf{a}\bar{\mathbf{b}}^\top}_{\mu}) + c\bar{s}(\underbrace{\mathbf{g}\bar{\mathbf{h}}^\top - \mathbf{b}\bar{\mathbf{a}}^\top}_{\bar{\mu}}) \\   = & c\bar{c}\omega + s\bar{s}\theta + \bar{c}s\mu + c\bar{s}\bar{\mu} \end{align} EXPERIMENT SHOWS that this magnitude is constant with choice of $\pm\Delta$ What is even more confounding to me is that when $x=0$ is true already, and the non trivial rotation that retains the zero is performed, in THAT case the magnitude does indeed change. Yet should it not be the same if the two choices from the original position of $x \ne 0$ give no change? The non-trivial rotation that retains the zero (and which actually gives a diagonal swap $d_0 \leftrightarrow d_1$ and $y \leftarrow \bar{y}$): \begin{align}   \delta cs + x c^2 - ys^2 &= s(\delta c + 0 - ys) = 0\\  \Rightarrow \frac{k\cdot s}{k\cdot c} &= \frac{\delta}{y} \\  c & = \frac{y}{k} \\  s & = \frac{\delta}{k} \\  k^2 & = y\bar{y} + \delta\bar{\delta} \\ \end{align} The magnitude $c\bar{c}\omega + s\bar{s}\theta + \bar{c}s\mu + c\bar{s}\bar{\mu}$ then is $$\frac{y\bar{y}\omega + \delta\bar{\delta}\theta + \bar{y}\delta\mu + y\bar{\delta}\bar{\mu}}{k^2} = \frac{y\bar{y}\omega + \delta\bar{\delta}\theta + \bar{y}\delta\mu + y\bar{\delta}\bar{\mu}}{y\bar{y} + \delta\bar{\delta}}$$ The change in the upper triangular norm then is \begin{align}   & \frac{y\bar{y}\omega + \delta\bar{\delta}\theta + \bar{y}\delta\mu + y\bar{\delta}\bar{\mu}}{y\bar{y} + \delta\bar{\delta}} - \omega \\   = & \frac{y\bar{y}\omega + \delta\bar{\delta}\theta + \bar{y}\delta\mu + y\bar{\delta}\bar{\mu}-\omega(y\bar{y} + \delta\bar{\delta})}{y\bar{y} + \delta\bar{\delta}} =\frac{N}{D}\\   N = & y\bar{y}\omega + \delta\bar{\delta}\theta + \bar{y}\delta\mu + y\bar{\delta}\bar{\mu}-\omega y\bar{y} -\omega \delta\bar{\delta}\\    = & \delta\bar{\delta}\theta + \bar{y}\delta\mu + y\bar{\delta}\bar{\mu} -\omega \delta\bar{\delta}\\    = & \delta\bar{\delta}(\theta - \omega) + \bar{y}\delta\mu + y\bar{\delta}\bar{\mu} \\ \end{align}","The Jacobi's rotation is the complex Givens rotation (unitary similarity) that results in a zero for a specified element of a matrix. If the element  is not adjacent to the diagonal, then there are rows/columns below the diagonal that mix with rows/columns above the diagonal. Here is the general form (where eventually $x$ will be the element to zero): \begin{align} &    \pmatrix{c               & \mathbf{0} & s \\             \mathbf{0}^\top & \mathbf{I} & \mathbf{0}^\top \\             -\bar{s}        & \mathbf{0} & \bar{c} \\    }    \pmatrix{d_0             & \mathbf{g} & x \\             \mathbf{a}^\top & \mathbf{D} & \mathbf{b}^\top \\             y               & \mathbf{h} & d_1 \\    }    \pmatrix{\bar{c}         & \mathbf{0} & -s \\             \mathbf{0}^\top & \mathbf{I} & \mathbf{0}^\top \\             \bar{s}        & \mathbf{0} & c \\    } \\ =&    \pmatrix{c               & \mathbf{0} & s \\             \mathbf{0}^\top & \mathbf{I} & \mathbf{0}^\top \\             -\bar{s}        & \mathbf{0} & \bar{c} \\    }    \pmatrix{d_0\bar{c}+x\bar{s}   & \mathbf{g} & -d_0 s + x c \\             \mathbf{a}^\top\bar{c} + \mathbf{b}^\top\bar{s}& \mathbf{D} & -\mathbf{a}^\top s + \mathbf{b}^\top c \\             y \bar{c} + d_1 \bar{s}              & \mathbf{h} & -y s + d_1 c \\    } \\ =&    \pmatrix{d_0c\bar{c} + d_1s\bar{s} + c\bar{s}x + \bar{c}sy  & c\mathbf{g}+ s\mathbf{h}                & (d_1 - d_0)cs + x c^2 - ys^2 \\             \mathbf{a}^\top \bar{c} + \mathbf{b}^\top \bar{s}  &        \mathbf{D}                       & -\mathbf{a}^\top s + \mathbf{b}^\top c \\             (d_1 -d_0)\bar{c}\bar{s} -x\bar{s}^2 + y\bar{c}^2  & -\bar{s}\mathbf{g} + \bar{c}\mathbf{h}  & d_0s\bar{s} + d_1c\bar{c} - c\bar{s}x - \bar{c}sy    } \end{align} Specifically $\mathbf{g} \leftarrow c\mathbf{g}+ s\mathbf{h}$, so that row vector $\mathbf{g}$ is mixed with row vector $\mathbf{h}$. Now solving for $c$ and $s$ in the complex numbers that give $x \leftarrow 0$ results in two solutions. Both of which end with the same magnitude in the upper triangular portion $$\Vert \mathbf{g} \Vert^2 + \Vert \mathbf{b} \Vert^2 \quad\text{is same for both solutions that give $x=0$}$$ I have yet to find the formulation that shows this, but multiple experimentation has yet to fail. EDIT I have made a silly mistake in that all tests were done on Hermitian matrices. Now I guess I solved my question, but any discussions or error corrections are welcome... Here is the formula for the rotation: \begin{align}   \delta &= d_1 - d_0 \\   \Delta &= \pm\sqrt{\delta^2 + 4xy} \\   k^2 &= \delta\bar{\delta} + \delta\bar{\Delta} + \bar{\delta}\Delta + \Delta\bar{\Delta} +4y\bar{y}\\   c & = \frac{2y}{k} \\   s & = \frac{\delta + \Delta}{k} \\ \end{align} The following is the derivation. Using $d_1 - d_0 = \delta$, the top right element is desired to be zero $$\delta cs + x c^2 - ys^2 = 0$$ Use the substitution $$\alpha = \frac{s}{c}$$ then \begin{align}   \delta c^2\alpha + x c^2 - y\alpha^2c^2 = c^2\left(\delta\alpha + x - y\alpha^2\right) = 0 \end{align} The equation $\delta\alpha + x - y\alpha^2=0$ is easily solved using the quadratic formula: $$ \alpha = \frac{-\delta \pm \sqrt{\delta^2 - 4(-y)x}}{2(-y)} = \frac{\delta \mp \sqrt{\delta^2 + 4xy}}{2y} = \frac{k\cdot s}{k\cdot c}$$ Here $k$ is the real and non-zero scale factor that gives $c\bar{c} + s\bar{s} = 1$. Denote $\Delta = \pm\sqrt{\delta^2 + 4xy}$.  Then we have \begin{align}   c &= \frac{2y}{k} \\   s &= \frac{\delta+\Delta}{k} \\   s\bar{s} + c\bar{c} &= \frac{(\delta+\Delta)(\bar{\delta} + \bar{\Delta}) + 4y\bar{y}}{k^2} = 1 \\   \Rightarrow k^2 &= \delta\bar{\delta} + \delta\bar{\Delta} + \bar{\delta}\Delta + \Delta\bar{\Delta} +4y\bar{y}\\ \end{align} This formulation gives the desired results for both $\pm\Delta$ (as is confirmed with computations). Now looking at the magnitude $\Vert c\mathbf{g}+ s\mathbf{h} \Vert^2 + \Vert  c\mathbf{b}  -s\mathbf{a} \Vert^2$ we have \begin{align}   \Vert c\mathbf{g}+ s\mathbf{h} \Vert^2 + \Vert c\mathbf{b}  -s\mathbf{a} \Vert^2  = & (c\mathbf{g}+ s\mathbf{h} )(\bar{\mathbf{g}}^\top \bar{c}+ \bar{\mathbf{h}}^\top\bar{s}) \\   & + (c\mathbf{b} - s\mathbf{a} )(\bar{\mathbf{b}}^\top \bar{c} - \bar{\mathbf{a}}^\top\bar{s})\\   = & c\bar{c}\mathbf{g}\bar{\mathbf{g}}^\top + s\bar{s}\mathbf{h}\bar{\mathbf{h}}^\top + \bar{c}s\mathbf{h}\bar{\mathbf{g}}^\top + c\bar{s}\mathbf{g}\bar{\mathbf{h}}^\top\\   & + c\bar{c}\mathbf{b}\bar{\mathbf{b}}^\top + s\bar{s}\mathbf{a}\bar{\mathbf{a}}^\top - \bar{c}s\mathbf{a}\bar{\mathbf{b}}^\top - c\bar{s}\mathbf{b}\bar{\mathbf{a}}^\top \\   = & c\bar{c}(\underbrace{\mathbf{g}\bar{\mathbf{g}}^\top + \mathbf{b}\bar{\mathbf{b}}^\top}_{\omega}) + s\bar{s}(\underbrace{\mathbf{h}\bar{\mathbf{h}}^\top + \mathbf{a}\bar{\mathbf{a}}^\top}_{\theta}) + \bar{c}s(\underbrace{\mathbf{h}\bar{\mathbf{g}}^\top - \mathbf{a}\bar{\mathbf{b}}^\top}_{\mu}) + c\bar{s}(\underbrace{\mathbf{g}\bar{\mathbf{h}}^\top - \mathbf{b}\bar{\mathbf{a}}^\top}_{\bar{\mu}}) \\   = & c\bar{c}\omega + s\bar{s}\theta + \bar{c}s\mu + c\bar{s}\bar{\mu} \end{align} EXPERIMENT SHOWS that this magnitude is constant with choice of $\pm\Delta$ What is even more confounding to me is that when $x=0$ is true already, and the non trivial rotation that retains the zero is performed, in THAT case the magnitude does indeed change. Yet should it not be the same if the two choices from the original position of $x \ne 0$ give no change? The non-trivial rotation that retains the zero (and which actually gives a diagonal swap $d_0 \leftrightarrow d_1$ and $y \leftarrow \bar{y}$): \begin{align}   \delta cs + x c^2 - ys^2 &= s(\delta c + 0 - ys) = 0\\  \Rightarrow \frac{k\cdot s}{k\cdot c} &= \frac{\delta}{y} \\  c & = \frac{y}{k} \\  s & = \frac{\delta}{k} \\  k^2 & = y\bar{y} + \delta\bar{\delta} \\ \end{align} The magnitude $c\bar{c}\omega + s\bar{s}\theta + \bar{c}s\mu + c\bar{s}\bar{\mu}$ then is $$\frac{y\bar{y}\omega + \delta\bar{\delta}\theta + \bar{y}\delta\mu + y\bar{\delta}\bar{\mu}}{k^2} = \frac{y\bar{y}\omega + \delta\bar{\delta}\theta + \bar{y}\delta\mu + y\bar{\delta}\bar{\mu}}{y\bar{y} + \delta\bar{\delta}}$$ The change in the upper triangular norm then is \begin{align}   & \frac{y\bar{y}\omega + \delta\bar{\delta}\theta + \bar{y}\delta\mu + y\bar{\delta}\bar{\mu}}{y\bar{y} + \delta\bar{\delta}} - \omega \\   = & \frac{y\bar{y}\omega + \delta\bar{\delta}\theta + \bar{y}\delta\mu + y\bar{\delta}\bar{\mu}-\omega(y\bar{y} + \delta\bar{\delta})}{y\bar{y} + \delta\bar{\delta}} =\frac{N}{D}\\   N = & y\bar{y}\omega + \delta\bar{\delta}\theta + \bar{y}\delta\mu + y\bar{\delta}\bar{\mu}-\omega y\bar{y} -\omega \delta\bar{\delta}\\    = & \delta\bar{\delta}\theta + \bar{y}\delta\mu + y\bar{\delta}\bar{\mu} -\omega \delta\bar{\delta}\\    = & \delta\bar{\delta}(\theta - \omega) + \bar{y}\delta\mu + y\bar{\delta}\bar{\mu} \\ \end{align}",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
13,Vandermonde matrix,Vandermonde matrix,,"Let ${\bf G} \in\mathbb{C}^{M\times K}$ and ${\bf H} \in\mathbb{C}^{N\times K}$ are full-rank Vandermode matrices where $MN-1=K>N\geq N$, that is, ${\bf G}$ and ${\bf H}$ are fat. Let ${\bf F}= {\bf H}\circ {\bf G}$ where $\circ$ denotes Khatri-Rao matrix product or column-wise Khatri-Rao matrix product. Is it true that there exists ${\bf g} \in\mathbb{C}^{M}$ and ${\bf h} \in\mathbb{C}^{N}$, which are different from the columns of ${\bf G}$ and ${\bf H}$, such that ${\bf h}\otimes{\bf g}$ belongs the range space of $\bf F$, that is, \begin{equation} {\bf F}{\bf a} = {\bf h}\otimes{\bf g}? \end{equation} Thanks.","Let ${\bf G} \in\mathbb{C}^{M\times K}$ and ${\bf H} \in\mathbb{C}^{N\times K}$ are full-rank Vandermode matrices where $MN-1=K>N\geq N$, that is, ${\bf G}$ and ${\bf H}$ are fat. Let ${\bf F}= {\bf H}\circ {\bf G}$ where $\circ$ denotes Khatri-Rao matrix product or column-wise Khatri-Rao matrix product. Is it true that there exists ${\bf g} \in\mathbb{C}^{M}$ and ${\bf h} \in\mathbb{C}^{N}$, which are different from the columns of ${\bf G}$ and ${\bf H}$, such that ${\bf h}\otimes{\bf g}$ belongs the range space of $\bf F$, that is, \begin{equation} {\bf F}{\bf a} = {\bf h}\otimes{\bf g}? \end{equation} Thanks.",,"['matrices', 'vector-spaces']"
14,Matrix trace minimization and zeros,Matrix trace minimization and zeros,,"I would like to minimize and find the zeros of the function $$F(S,P) = trace(S-SP^{T}(A+ PSP^{T})^{-1}PS)$$ in respect to $S$ and $P$. $S$ is symmetric square matrix. $P$ is a rectangular matrix Could you help me? Thank you very much All the best GoodSpirit","I would like to minimize and find the zeros of the function $$F(S,P) = trace(S-SP^{T}(A+ PSP^{T})^{-1}PS)$$ in respect to $S$ and $P$. $S$ is symmetric square matrix. $P$ is a rectangular matrix Could you help me? Thank you very much All the best GoodSpirit",,['matrices']
15,matrix representation of operator,matrix representation of operator,,"Vector $\vec v\ $ in basis E = $[\vec e_1 \vec e_2 \ldots \vec e_n]$ $$\vec v = E \ \begin{bmatrix}v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix}$$ Now, operator acts upon it $$A(\vec v) = v_1 A(\vec e_1) + v_2 A(\vec e_2) + \ldots + v_n A(\vec e_n) = [A(\vec e_1) A(\vec e_2) \ldots A(\vec e_n)] \ \begin{bmatrix}v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix}$$ Now, when v is a first standard vector, we get the first column, $A(\vec e_1)$. Everybody says that it is decomposable into sum, like I did for $\vec v$, $$A(e_1) = a_{11} e_1 + a_{21} e_2 + \dots + a_{n1} e_n = \sum a_{i1} e_i = E \vec a_1$$ This is a matrix E by vector (column) $a_1$ multiplication. It produces another vector $\vec {A(\vec e_1)}$). I just wonder what is the column $a_1$? Which matrix it belongs to? Everybody says that the resulting A($e_1$) column must be the first col of the wanted matrix A. So, $a_1$ cannot be part of A also. What is the matrix of columns $[a_1 a_2 ... a_n]$? How do yo call it? Dear, is it the real operator matrix whereas notorious $[A(\vec e_1) A(\vec e_2) \ldots A(\vec e_n)]$ is just A mixed up with the basis vectors? Is it a wrong matrix A then? Elaboration My purpose is to understand the matrix, which represents the operator. I follow the Miami tutorial and it seems that they assume that we must be able to determine the elements $a_{ij}$ by acting with operator A on every basis vector $|i\rangle$ and know the coordinates (or components?) $$\begin{bmatrix}a_{1i}\\a_{2i}\\a_{3i}\end{bmatrix}$$ of the response column vector $|a_i\rangle = A\,|i\rangle$ in the same basis $\begin{bmatrix}|1\rangle |2\rangle |3\rangle  \end{bmatrix} = E$: $$A\,|i\rangle =\begin{bmatrix}|1\rangle |2\rangle  |3\rangle \end{bmatrix} \  \begin{bmatrix}a_{1i}\\a_{2i}\\a_{3i}\end{bmatrix} = E\ \begin{bmatrix}a_{1i}\\a_{2i}\\a_{3i}\end{bmatrix}.$$ If we apply A to all basis vectors in series, that is if we apply A to matrix E, we get $$AE = A \begin{bmatrix}|1\rangle |2\rangle |3\rangle \end{bmatrix} = \begin{bmatrix}|1\rangle |2\rangle |3\rangle \end{bmatrix} \begin{bmatrix}a_{11}&a_{12}&a_{33} \\a_{21}&a_{22}&a_{23}\\a_{31}&a_{32}&a_{33} \end{bmatrix} = E \ \begin{bmatrix}a_{11}&a_{12}&a_{33}\\a_{21}&a_{22}&a_{23} \\a_{31}&a_{32}&a_{33} \end{bmatrix}. $$ Multiplying by $E^{-1}$, we get $$E^{-1}AE = \begin{bmatrix}a_{11}&a_{12}&a_{33}\\a_{21}&a_{22}&a_{23} \\a_{31}&a_{32}&a_{33} \end{bmatrix} = [A]$$ I have just introduced the shortcut $[A] = \begin{bmatrix}a_{11}&a_{12}&a_{33}\\a_{21}&a_{22}&a_{23} \\a_{31}&a_{32}&a_{33} \end{bmatrix}$ Now, any vector $|v\rangle = E\ \begin{bmatrix}v_1 \\ v_2 \\ v_3 \end{bmatrix} = E\ [v] $ where $[v] = \begin{bmatrix}v_1 \\ v_2 \\ v_3 \end{bmatrix}$ is the vector of coordinates (or components?) is translated to $|u\rangle = E\,[u] = A \,|v\rangle = A\,E\,[v] $ by the operator A. Therefore, the coordinates of new vector, $$[u] = E^{-1}AE[v] = [A]\,[v] $$ I have got $E^{-1}AE = [A]$ instead of get $A = [A]$ expected.  It seems that Miami tutorial treats $[A]$ as the matrix representation of the operator. My question is about the relationship between matrix of A and obtained matrix $[A]$. I see that $E^{-1}AE = A$ in case of standard basis, E = I. Should I compute the matrix A in standard basis and then translate it into the sought matrix of the operator A? Which of the matrices, $A$ or $[A] = E^{-1}AE$ is the operator matrix and which is the operator in the standard basis? Why I cannot find tutorial which says that I must use standard basis to look for the matrix of my operator? Quantum mechanics How is this approach related with Quantum mechanics , who use orthonormal bases, so that operator O elements are $o_{ij} = \langle i | O | j \rangle\ $? I see how similar this expression is to our $E^{-1}AE$. The inner product with $\langle i|$ gives i-th component of $O|j\rangle$ in case of orthonormal $|i\rangle$ and $|j\rangle$, as it is in quantum mechanics. This is not true in the general case of $E^{-1}AE$. I do not understand what is computed with $E^{-1}AE$. If $\langle i | O | j \rangle\ $ computes i-th component of O-transformed j-th basis vector then it is a number, the element of sought matrix. But what is $E^{-1}AE$? How do I find the elements of this matrix?","Vector $\vec v\ $ in basis E = $[\vec e_1 \vec e_2 \ldots \vec e_n]$ $$\vec v = E \ \begin{bmatrix}v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix}$$ Now, operator acts upon it $$A(\vec v) = v_1 A(\vec e_1) + v_2 A(\vec e_2) + \ldots + v_n A(\vec e_n) = [A(\vec e_1) A(\vec e_2) \ldots A(\vec e_n)] \ \begin{bmatrix}v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix}$$ Now, when v is a first standard vector, we get the first column, $A(\vec e_1)$. Everybody says that it is decomposable into sum, like I did for $\vec v$, $$A(e_1) = a_{11} e_1 + a_{21} e_2 + \dots + a_{n1} e_n = \sum a_{i1} e_i = E \vec a_1$$ This is a matrix E by vector (column) $a_1$ multiplication. It produces another vector $\vec {A(\vec e_1)}$). I just wonder what is the column $a_1$? Which matrix it belongs to? Everybody says that the resulting A($e_1$) column must be the first col of the wanted matrix A. So, $a_1$ cannot be part of A also. What is the matrix of columns $[a_1 a_2 ... a_n]$? How do yo call it? Dear, is it the real operator matrix whereas notorious $[A(\vec e_1) A(\vec e_2) \ldots A(\vec e_n)]$ is just A mixed up with the basis vectors? Is it a wrong matrix A then? Elaboration My purpose is to understand the matrix, which represents the operator. I follow the Miami tutorial and it seems that they assume that we must be able to determine the elements $a_{ij}$ by acting with operator A on every basis vector $|i\rangle$ and know the coordinates (or components?) $$\begin{bmatrix}a_{1i}\\a_{2i}\\a_{3i}\end{bmatrix}$$ of the response column vector $|a_i\rangle = A\,|i\rangle$ in the same basis $\begin{bmatrix}|1\rangle |2\rangle |3\rangle  \end{bmatrix} = E$: $$A\,|i\rangle =\begin{bmatrix}|1\rangle |2\rangle  |3\rangle \end{bmatrix} \  \begin{bmatrix}a_{1i}\\a_{2i}\\a_{3i}\end{bmatrix} = E\ \begin{bmatrix}a_{1i}\\a_{2i}\\a_{3i}\end{bmatrix}.$$ If we apply A to all basis vectors in series, that is if we apply A to matrix E, we get $$AE = A \begin{bmatrix}|1\rangle |2\rangle |3\rangle \end{bmatrix} = \begin{bmatrix}|1\rangle |2\rangle |3\rangle \end{bmatrix} \begin{bmatrix}a_{11}&a_{12}&a_{33} \\a_{21}&a_{22}&a_{23}\\a_{31}&a_{32}&a_{33} \end{bmatrix} = E \ \begin{bmatrix}a_{11}&a_{12}&a_{33}\\a_{21}&a_{22}&a_{23} \\a_{31}&a_{32}&a_{33} \end{bmatrix}. $$ Multiplying by $E^{-1}$, we get $$E^{-1}AE = \begin{bmatrix}a_{11}&a_{12}&a_{33}\\a_{21}&a_{22}&a_{23} \\a_{31}&a_{32}&a_{33} \end{bmatrix} = [A]$$ I have just introduced the shortcut $[A] = \begin{bmatrix}a_{11}&a_{12}&a_{33}\\a_{21}&a_{22}&a_{23} \\a_{31}&a_{32}&a_{33} \end{bmatrix}$ Now, any vector $|v\rangle = E\ \begin{bmatrix}v_1 \\ v_2 \\ v_3 \end{bmatrix} = E\ [v] $ where $[v] = \begin{bmatrix}v_1 \\ v_2 \\ v_3 \end{bmatrix}$ is the vector of coordinates (or components?) is translated to $|u\rangle = E\,[u] = A \,|v\rangle = A\,E\,[v] $ by the operator A. Therefore, the coordinates of new vector, $$[u] = E^{-1}AE[v] = [A]\,[v] $$ I have got $E^{-1}AE = [A]$ instead of get $A = [A]$ expected.  It seems that Miami tutorial treats $[A]$ as the matrix representation of the operator. My question is about the relationship between matrix of A and obtained matrix $[A]$. I see that $E^{-1}AE = A$ in case of standard basis, E = I. Should I compute the matrix A in standard basis and then translate it into the sought matrix of the operator A? Which of the matrices, $A$ or $[A] = E^{-1}AE$ is the operator matrix and which is the operator in the standard basis? Why I cannot find tutorial which says that I must use standard basis to look for the matrix of my operator? Quantum mechanics How is this approach related with Quantum mechanics , who use orthonormal bases, so that operator O elements are $o_{ij} = \langle i | O | j \rangle\ $? I see how similar this expression is to our $E^{-1}AE$. The inner product with $\langle i|$ gives i-th component of $O|j\rangle$ in case of orthonormal $|i\rangle$ and $|j\rangle$, as it is in quantum mechanics. This is not true in the general case of $E^{-1}AE$. I do not understand what is computed with $E^{-1}AE$. If $\langle i | O | j \rangle\ $ computes i-th component of O-transformed j-th basis vector then it is a number, the element of sought matrix. But what is $E^{-1}AE$? How do I find the elements of this matrix?",,"['matrices', 'terminology']"
16,What kind of matrix/tensor notation is this?,What kind of matrix/tensor notation is this?,,"I'm hoping someone on here recognises this and has an answer, because I'm having serious memory issues. About a year ago, I came across the following way of representing tensors of rank $n$ in matrix form, in a way similar to block matrices. Maybe this is just an alternative to block matrices actually. Consider a simple $2 \times 2$ matrix definition: I can represent a $2 \times 2 \times 2$ rank-3 tensor as a cuboid, but this is rather hard to show in a paper or proof, so a convenient notation exists that ""splits up"" the layers of the cuboid into $2 \times 2$ matrices, which can be flattened out and shown contiguously as follows: Ditto a $2 \times 2 \times 2 \times 2$ rank-4 tensor can be represented as follows: For both of these equalities, the notation on the left of the $=$ sign is the standard way of writing block matrices. However the explicit partition on the right is what I'm after: is there a particular name for this notation? Is it actually used somewhere to represent cuboids and other higher rank tensors or did I imagine it? Thanks in advance if anyone has any info!","I'm hoping someone on here recognises this and has an answer, because I'm having serious memory issues. About a year ago, I came across the following way of representing tensors of rank $n$ in matrix form, in a way similar to block matrices. Maybe this is just an alternative to block matrices actually. Consider a simple $2 \times 2$ matrix definition: I can represent a $2 \times 2 \times 2$ rank-3 tensor as a cuboid, but this is rather hard to show in a paper or proof, so a convenient notation exists that ""splits up"" the layers of the cuboid into $2 \times 2$ matrices, which can be flattened out and shown contiguously as follows: Ditto a $2 \times 2 \times 2 \times 2$ rank-4 tensor can be represented as follows: For both of these equalities, the notation on the left of the $=$ sign is the standard way of writing block matrices. However the explicit partition on the right is what I'm after: is there a particular name for this notation? Is it actually used somewhere to represent cuboids and other higher rank tensors or did I imagine it? Thanks in advance if anyone has any info!",,"['linear-algebra', 'matrices', 'notation', 'multilinear-algebra', 'block-matrices']"
17,"Show that $rank(A)+rank(B) \leq n$, when $A,B$ are $2$ matrices of size $n \times n$, and $AB=0$ [duplicate]","Show that , when  are  matrices of size , and  [duplicate]","rank(A)+rank(B) \leq n A,B 2 n \times n AB=0","This question already has answers here : Proof of: $AB=0 \Rightarrow Rank(A)+Rank(B) \leq n$ (3 answers) Closed 7 years ago . Question from homework in Linear Algebra: Let $A,B$ be two matrices of size $n \times n$ such that $AB=0$. Show that: $rank(A) + rank(B)  \le  n$ . It probably has something to do with the dim of the null space or column space but I can't put things together from what we've learned... Please help.. Thanks. :)","This question already has answers here : Proof of: $AB=0 \Rightarrow Rank(A)+Rank(B) \leq n$ (3 answers) Closed 7 years ago . Question from homework in Linear Algebra: Let $A,B$ be two matrices of size $n \times n$ such that $AB=0$. Show that: $rank(A) + rank(B)  \le  n$ . It probably has something to do with the dim of the null space or column space but I can't put things together from what we've learned... Please help.. Thanks. :)",,"['linear-algebra', 'matrices', 'inequality', 'matrix-rank']"
18,"Lattice Reduction Problem: Minimizing the ""Longest"" Basis Vector","Lattice Reduction Problem: Minimizing the ""Longest"" Basis Vector",,"Suppose we have a basis for an integer lattice formed by the vectors $\vec v_1, \vec v_2, \ldots,\vec v_n$. Then let $A$ be the augmented matrix $( \vec v_1| \space \vec v_2| \cdots |\space \vec v_n)$. Here is my question: is there an algorithm which performs elementary column operations on $A$ such that $\max(\|\vec u_1\|_p, \|\vec u_2\|_p, \ldots, \|\vec u_n\|_p)$ is a minimum, where $\vec u_i$ represent the new column vectors?  The specific cases $p=1$, $p=2$, and $p=\infty$ are of particular interest to me. Here are my thoughts for a slow-as-molasses approach: I could first apply the LLL algorithm to get my vectors within a reasonable distance of the origin.  Once that is done, a $L_p$ unit $n$-sphere could be drawn centered at the origin with radius stretched to the longest of the vectors.  We could then brute-force the answer by checking every possible basis within this $n$-sphere. EDIT: It looks like the $p=\infty$ case can be reduced to a simpler problem, which is simply minimizing the absolute value of the largest element in the matrix.  I have also found an article which looks promising in that it may have the answer for the $p=1$ case, but I'm having difficulty understanding some of the notation.","Suppose we have a basis for an integer lattice formed by the vectors $\vec v_1, \vec v_2, \ldots,\vec v_n$. Then let $A$ be the augmented matrix $( \vec v_1| \space \vec v_2| \cdots |\space \vec v_n)$. Here is my question: is there an algorithm which performs elementary column operations on $A$ such that $\max(\|\vec u_1\|_p, \|\vec u_2\|_p, \ldots, \|\vec u_n\|_p)$ is a minimum, where $\vec u_i$ represent the new column vectors?  The specific cases $p=1$, $p=2$, and $p=\infty$ are of particular interest to me. Here are my thoughts for a slow-as-molasses approach: I could first apply the LLL algorithm to get my vectors within a reasonable distance of the origin.  Once that is done, a $L_p$ unit $n$-sphere could be drawn centered at the origin with radius stretched to the longest of the vectors.  We could then brute-force the answer by checking every possible basis within this $n$-sphere. EDIT: It looks like the $p=\infty$ case can be reduced to a simpler problem, which is simply minimizing the absolute value of the largest element in the matrix.  I have also found an article which looks promising in that it may have the answer for the $p=1$ case, but I'm having difficulty understanding some of the notation.",,"['matrices', 'discrete-mathematics', 'optimization', 'integer-lattices', 'discrete-optimization']"
19,Condition number of $A^{-1} B$ where $A$ and $B$ are banded Toeplitz matrices.,Condition number of  where  and  are banded Toeplitz matrices.,A^{-1} B A B,"I'm looking at a filtering problem with feedback, which can be represented by the equation $A\underline{y} = B\underline{x}$ , where $A$ and $B$ are lower triangular banded Toeplitz matrices and $\underline{x}$ and $\underline{y}$ are vectors. As an example, $A$ and $B$ are of the form \begin{equation} \begin{pmatrix} a_1 &0 &0 & \ldots &0\\ a_2 &a_1&0& \ldots&0 \\ a_3&a_2 &a_1& \ldots&0 \\ 0&a_3&a_2 & \ldots&0 \\ 0&0&a_3 & \ldots&0 \\ \cdot&\cdot&\cdot&\cdot&a_1\\ \end{pmatrix} \end{equation} Here, the matrix $A$ (as well as $B$ ) is specified by the few non zero values in their first column. I want to look at the condition number of $A^{-1}B$ (as a way of measuring how sensitive the system is to the A and B coefficients). Are there are any results in the literature that would help characterize the condition number in the limit of large matrices $A$ and $B$ ? Thanks in Advance.","I'm looking at a filtering problem with feedback, which can be represented by the equation , where and are lower triangular banded Toeplitz matrices and and are vectors. As an example, and are of the form Here, the matrix (as well as ) is specified by the few non zero values in their first column. I want to look at the condition number of (as a way of measuring how sensitive the system is to the A and B coefficients). Are there are any results in the literature that would help characterize the condition number in the limit of large matrices and ? Thanks in Advance.","A\underline{y} = B\underline{x} A B \underline{x} \underline{y} A B \begin{equation}
\begin{pmatrix}
a_1 &0 &0 & \ldots &0\\
a_2 &a_1&0& \ldots&0 \\
a_3&a_2 &a_1& \ldots&0 \\
0&a_3&a_2 & \ldots&0 \\
0&0&a_3 & \ldots&0 \\
\cdot&\cdot&\cdot&\cdot&a_1\\
\end{pmatrix}
\end{equation} A B A^{-1}B A B","['linear-algebra', 'matrices', 'condition-number', 'toeplitz-matrices']"
20,What is the principal components matrix in PCA with SVD?,What is the principal components matrix in PCA with SVD?,,"Doing PCA on a matrix using SVD yields a result of three matrices, expressed as: $$ M = U \Sigma V^T $$ where $M$ is our initial data with zero mean. If we want to make a plot of the two principle components we project the data onto principal component space. $$ Z = M * V     $$ and then use the two first columns of Z for our plot. Maybe I have already answered my own question, but I am struggling to understand if $Z$ is what would be called the Principle Component matrix, and if not, how do we find that? Also, I am not sure what the operation $M*V$ does to the data. As I understand it, $V$ is an expression of the general trends of each of the attributes in the data set. By calculating the dot product between our data $M$ and the trends $V$ of the data, we end up with a matrix (PC matrix?) that captures the original data in a structured manner which allows for dimensionality reduction. Are my assumptions correct, or have I misread the theory?","Doing PCA on a matrix using SVD yields a result of three matrices, expressed as: $$ M = U \Sigma V^T $$ where $M$ is our initial data with zero mean. If we want to make a plot of the two principle components we project the data onto principal component space. $$ Z = M * V     $$ and then use the two first columns of Z for our plot. Maybe I have already answered my own question, but I am struggling to understand if $Z$ is what would be called the Principle Component matrix, and if not, how do we find that? Also, I am not sure what the operation $M*V$ does to the data. As I understand it, $V$ is an expression of the general trends of each of the attributes in the data set. By calculating the dot product between our data $M$ and the trends $V$ of the data, we end up with a matrix (PC matrix?) that captures the original data in a structured manner which allows for dimensionality reduction. Are my assumptions correct, or have I misread the theory?",,"['linear-algebra', 'matrices', 'principal-component-analysis']"
21,Solution to a Matrix equation,Solution to a Matrix equation,,"Is there a general solution to the following matrix equation. $A - BAB^T = C$ where B is known but can be any non-symmetric square matrix, C is known and invertible, all are n by n matrices. Is there a solution to A? or we need to use numerical methods?","Is there a general solution to the following matrix equation. $A - BAB^T = C$ where B is known but can be any non-symmetric square matrix, C is known and invertible, all are n by n matrices. Is there a solution to A? or we need to use numerical methods?",,"['linear-algebra', 'matrices']"
22,A MatrixExp question: simplifying $\int_0^t e^{A(t-t')} e^{A^T (t-t')} dt'$ for a real matrix A,A MatrixExp question: simplifying  for a real matrix A,\int_0^t e^{A(t-t')} e^{A^T (t-t')} dt',"I am interested in computing the following integral of a matrix exponential. \begin{equation} \int_0^t e^{A(t-t')} e^{A^T (t-t')} dt' \end{equation} The only assumption is that $A_{n\times n}$ is real. This is simple (albeit cumbersome) to compute, given a particular $A$. I was wondering, however, if there were any other steps I could take to progress the problem further a little bit further. For instance (assuming I did not make any mistakes), if $A$ is normal then $A = U \Lambda U^H$, where $\Lambda=\text{diag}(\lambda1,\lambda2,\ldots,\lambda_n)$ contains the eigenvalues and $U$ is unitary. Then \begin{equation} \int_0^t e^{A(t-t')} e^{A^T (t-t')} dt' = U \left[\int_0^t e^{2\Lambda(t-t')} dt'\right]U^H = U \left[(2\Lambda)^{-1} (e^{2\Lambda t} - I)\right]U^H \end{equation} However, in general $A$ is not normal. I appreciate the support in advance.","I am interested in computing the following integral of a matrix exponential. \begin{equation} \int_0^t e^{A(t-t')} e^{A^T (t-t')} dt' \end{equation} The only assumption is that $A_{n\times n}$ is real. This is simple (albeit cumbersome) to compute, given a particular $A$. I was wondering, however, if there were any other steps I could take to progress the problem further a little bit further. For instance (assuming I did not make any mistakes), if $A$ is normal then $A = U \Lambda U^H$, where $\Lambda=\text{diag}(\lambda1,\lambda2,\ldots,\lambda_n)$ contains the eigenvalues and $U$ is unitary. Then \begin{equation} \int_0^t e^{A(t-t')} e^{A^T (t-t')} dt' = U \left[\int_0^t e^{2\Lambda(t-t')} dt'\right]U^H = U \left[(2\Lambda)^{-1} (e^{2\Lambda t} - I)\right]U^H \end{equation} However, in general $A$ is not normal. I appreciate the support in advance.",,"['linear-algebra', 'matrices']"
23,"Computing $\mathbb{C}[x,y]^G$ or $\mathbb{C}[x,y,z]^G$ where $G$ is a finite subgroup of $GL_n(\mathbb{C})$",Computing  or  where  is a finite subgroup of,"\mathbb{C}[x,y]^G \mathbb{C}[x,y,z]^G G GL_n(\mathbb{C})","My question is related to this link: Ring of Invariant $\mathbf{Question \;1}$. Let  $$ A = \left( \begin{array}{cc} 0 & -1 \\ 1& 0 \\  \end{array} \right).  $$ Then $C= \langle A\rangle$ is a cyclic, finite group of order $4$. Suppose $A$ acts on $\mathbb{C}[x,y]$ linearly. Then what is the subring $\mathbb{C}[x,y]^C$ of invariant functions in $\mathbb{C}[x,y]$? What is the basic strategy? Note that  $$ C = \left\{  \left( \begin{array}{cc} 0 & -1 \\ 1& 0 \\  \end{array} \right),  \left( \begin{array}{cc} -1 & 0 \\ 0& -1 \\  \end{array} \right), \left( \begin{array}{cc} 0 & 1 \\ -1& 0 \\  \end{array} \right), \left( \begin{array}{cc} 1 & 0 \\ 0 & 1 \\  \end{array} \right) \right\}.  $$ $\mathbf{Question \;2}$. Now, suppose the dihedral group $D_6 = \langle \rho, \psi : \rho^6 = \psi^2 =e,\psi \rho\psi^{-1}=\rho^{-1} \rangle$ acts on $\mathbb{C}[x,y,z]$, with the action defined by the matrices  $$  \rho = \left( \begin{array}{ccc} 1/2 & -\sqrt{3}/2 & 0 \\  -\sqrt{3}/2 & 1/2 & 0 \\ 0 & 0 & 1 \\ \end{array} \right)   \mbox{ and } \psi = \left( \begin{array}{ccc}   1 & 0  & 0 \\ 0 & -1 & -1 \\ 0 & 0 & -1 \\ \end{array}\right).  $$ Then what is $\mathbb{C}[x,y,z]^{D_6}$? $\mathbf{Question \;3}$. What is the general strategy, if we have something like the subgroup generated by $B$ and $-B$ in $GL_3(\mathbb{C})$ acting on a polynomial ring $\mathbb{C}[x,y]$ of only two variables, where    $$ B = \left( \begin{array}{ccc} 1 & 0 & 0\\ 0 & 1 & -1 \\  0 & 0& 1 \\  \end{array} \right)?  $$","My question is related to this link: Ring of Invariant $\mathbf{Question \;1}$. Let  $$ A = \left( \begin{array}{cc} 0 & -1 \\ 1& 0 \\  \end{array} \right).  $$ Then $C= \langle A\rangle$ is a cyclic, finite group of order $4$. Suppose $A$ acts on $\mathbb{C}[x,y]$ linearly. Then what is the subring $\mathbb{C}[x,y]^C$ of invariant functions in $\mathbb{C}[x,y]$? What is the basic strategy? Note that  $$ C = \left\{  \left( \begin{array}{cc} 0 & -1 \\ 1& 0 \\  \end{array} \right),  \left( \begin{array}{cc} -1 & 0 \\ 0& -1 \\  \end{array} \right), \left( \begin{array}{cc} 0 & 1 \\ -1& 0 \\  \end{array} \right), \left( \begin{array}{cc} 1 & 0 \\ 0 & 1 \\  \end{array} \right) \right\}.  $$ $\mathbf{Question \;2}$. Now, suppose the dihedral group $D_6 = \langle \rho, \psi : \rho^6 = \psi^2 =e,\psi \rho\psi^{-1}=\rho^{-1} \rangle$ acts on $\mathbb{C}[x,y,z]$, with the action defined by the matrices  $$  \rho = \left( \begin{array}{ccc} 1/2 & -\sqrt{3}/2 & 0 \\  -\sqrt{3}/2 & 1/2 & 0 \\ 0 & 0 & 1 \\ \end{array} \right)   \mbox{ and } \psi = \left( \begin{array}{ccc}   1 & 0  & 0 \\ 0 & -1 & -1 \\ 0 & 0 & -1 \\ \end{array}\right).  $$ Then what is $\mathbb{C}[x,y,z]^{D_6}$? $\mathbf{Question \;3}$. What is the general strategy, if we have something like the subgroup generated by $B$ and $-B$ in $GL_3(\mathbb{C})$ acting on a polynomial ring $\mathbb{C}[x,y]$ of only two variables, where    $$ B = \left( \begin{array}{ccc} 1 & 0 & 0\\ 0 & 1 & -1 \\  0 & 0& 1 \\  \end{array} \right)?  $$",,"['group-theory', 'matrices', 'finite-groups', 'invariant-theory']"
24,Average transformation matrix?,Average transformation matrix?,,"I have several estimates of the transformation matrix between two planes and some values that give some indication of the error involved in the estimate. How can I use this information to gain the best average case estimate of the transformation matrix? Could I simply average all the matrices? If so, how would I go about it? Or would it be best to assign a probability proportional to the accuracy and include that in a model to estimate it or something along those lines.... ...any ideas?","I have several estimates of the transformation matrix between two planes and some values that give some indication of the error involved in the estimate. How can I use this information to gain the best average case estimate of the transformation matrix? Could I simply average all the matrices? If so, how would I go about it? Or would it be best to assign a probability proportional to the accuracy and include that in a model to estimate it or something along those lines.... ...any ideas?",,"['matrices', 'average', 'transformational-geometry']"
25,About Invariant Factors,About Invariant Factors,,"Suppose $A$ is a $2\times2$ matrix with minimal polynomial $x^2- 5x+4$, $B$ is a $2\times2$ matrix with minimal polynomial $x^2 -6x +8$, and let $O$ be the $2\times2$ matrix with all entries $0$. Find the invariant factors of the matrix $\begin{pmatrix}A & O \\ O & B \end{pmatrix}$. I thought the characteristic polynomial is $(x-1)(x-2)(x-4)^2$， then I get stuck...","Suppose $A$ is a $2\times2$ matrix with minimal polynomial $x^2- 5x+4$, $B$ is a $2\times2$ matrix with minimal polynomial $x^2 -6x +8$, and let $O$ be the $2\times2$ matrix with all entries $0$. Find the invariant factors of the matrix $\begin{pmatrix}A & O \\ O & B \end{pmatrix}$. I thought the characteristic polynomial is $(x-1)(x-2)(x-4)^2$， then I get stuck...",,"['linear-algebra', 'matrices']"
26,Reconstructing a matrix from random matrix vector products,Reconstructing a matrix from random matrix vector products,,"I am looking for new ideas how to construct a guess for a (positive, hermitian) matrix A given some matrix-vector products Ax (with random vectors x). One such method would be to perform rank one updates to A each time a new Ax becomes available, and this is what is used in for example the BFGS update formula in quasi-Newton methods. However, this just updates a one dimensional subspace of the matrix. In my application it would be more natural to scale the whole matrix instead of performing a rank one update, but just a simple scaling is not enough because it cannot be consistent for all the Ax's. I have a rather good approximation to A to start with, so I know roughly the distributions of eigenvalues if that can help. My biggest problem is that I don't know how to even define the problem properly.. Perhaps I can sharpen the question with some feelback. Edit: Learned about shrinkage estimation, perhaps that is the way to go.","I am looking for new ideas how to construct a guess for a (positive, hermitian) matrix A given some matrix-vector products Ax (with random vectors x). One such method would be to perform rank one updates to A each time a new Ax becomes available, and this is what is used in for example the BFGS update formula in quasi-Newton methods. However, this just updates a one dimensional subspace of the matrix. In my application it would be more natural to scale the whole matrix instead of performing a rank one update, but just a simple scaling is not enough because it cannot be consistent for all the Ax's. I have a rather good approximation to A to start with, so I know roughly the distributions of eigenvalues if that can help. My biggest problem is that I don't know how to even define the problem properly.. Perhaps I can sharpen the question with some feelback. Edit: Learned about shrinkage estimation, perhaps that is the way to go.",,"['matrices', 'tomography']"
27,On the distribution of unimodular matrices generated by the Hermite normal form,On the distribution of unimodular matrices generated by the Hermite normal form,,"A problem I'm currently considering requires me to generate (pseudo-)random Gaussian integer matrices with Gaussian integer matrix inverses. By analogy with an algorithm I know for generating random orthogonal matrices, I considered an algorithm where a random Gaussian integer matrix whose real and imaginary parts are from a uniform distribution, perform a reduction to the Hermite normal form, and then take the unimodular matrix that reduces the original random matrix to Hermite form as the result. I am aware that this method misses the matrices whose determinants are $\pm i$, but I am curious as to what probability distribution the matrices generated by this method follow. By analogy, I know that the orthogonal matrices obtained from a QR decomposition of a matrix with normally-distributed entries follows the Haar distribution; is there a way to characterize the probability distribution of the unimodular factors of the Hermite decomposition of a uniformly-distributed random Gaussian integer matrix?","A problem I'm currently considering requires me to generate (pseudo-)random Gaussian integer matrices with Gaussian integer matrix inverses. By analogy with an algorithm I know for generating random orthogonal matrices, I considered an algorithm where a random Gaussian integer matrix whose real and imaginary parts are from a uniform distribution, perform a reduction to the Hermite normal form, and then take the unimodular matrix that reduces the original random matrix to Hermite form as the result. I am aware that this method misses the matrices whose determinants are $\pm i$, but I am curious as to what probability distribution the matrices generated by this method follow. By analogy, I know that the orthogonal matrices obtained from a QR decomposition of a matrix with normally-distributed entries follows the Haar distribution; is there a way to characterize the probability distribution of the unimodular factors of the Hermite decomposition of a uniformly-distributed random Gaussian integer matrix?",,"['linear-algebra', 'matrices', 'probability-distributions', 'unimodular-matrices', 'hermite-normal-form']"
28,image of symmetric matrices under representation of $GL_2(\mathbb{R})$,image of symmetric matrices under representation of,GL_2(\mathbb{R}),"Let $W$ be a real vector space of dimension $2$ and let $\rho_k:GL_2(\mathbb{R}) \to GL(\mathbf{S}^kW)$ be the standard representation of $GL_2(\mathbb{R})$. Since $\rho_k$ is polynomial, it naturally extends to a map $\tilde \rho_k:Mat_2(\mathbb{R}) \to End(\mathbf{S}^kW)$.  Denote $Sym_2(\mathbb{R})$ the space of real symmetric $2 \times 2$ matrices. Do we know the dimension of the vector space in $End(\mathbf{S}^kW)$ generated by  $\tilde \rho_k(Sym_2(\mathbb{R}))$?","Let $W$ be a real vector space of dimension $2$ and let $\rho_k:GL_2(\mathbb{R}) \to GL(\mathbf{S}^kW)$ be the standard representation of $GL_2(\mathbb{R})$. Since $\rho_k$ is polynomial, it naturally extends to a map $\tilde \rho_k:Mat_2(\mathbb{R}) \to End(\mathbf{S}^kW)$.  Denote $Sym_2(\mathbb{R})$ the space of real symmetric $2 \times 2$ matrices. Do we know the dimension of the vector space in $End(\mathbf{S}^kW)$ generated by  $\tilde \rho_k(Sym_2(\mathbb{R}))$?",,"['linear-algebra', 'matrices', 'representation-theory']"
29,Matrix Diagonal Multiplication,Matrix Diagonal Multiplication,,I have a matrix-vector inner product multiplication $G = X D x$ where $D$ is a diagonal matrix. Now let's say I already know $E = Xx$. Is there a method that I can use to change $E$ into $G$ using $D$ without having to calculate $G$ in full?,I have a matrix-vector inner product multiplication $G = X D x$ where $D$ is a diagonal matrix. Now let's say I already know $E = Xx$. Is there a method that I can use to change $E$ into $G$ using $D$ without having to calculate $G$ in full?,,"['linear-algebra', 'matrices']"
30,How to bound the change between optimal points when perturbing an objective function?,How to bound the change between optimal points when perturbing an objective function?,,"Let $A,B \in \mathbb{R}^{n \times n}$ be two positive semi-definite matrices and let $a > 0$ be a constant. Consider the following maximization problem $$ \max_{x \in \mathbb{R}^n, \gamma}\ x^T \left(\frac{\gamma}{a + \gamma} A - \gamma B \right)x \qquad \text{subject to } \gamma > 0, \|x\| = 1 $$ and suppose that we know that $\hat x$ and $\hat \gamma$ maximize the expression.  In this case, we have that $\hat x$ is the largest eigenvector of the matrix $\frac{\hat \gamma}{a + \hat \gamma} A - \hat \gamma B$.   Suppose that we perturb the matrix A by another matrix $E$, which is supposed to be small in some norm (say operator). Consider again the following optimization problem $$ \max_{x \in \mathbb{R}^n, \gamma}\ x^T \left(\frac{\gamma}{a + \gamma} (A+E) - \gamma B \right)x \qquad \text{subject to } \gamma > 0, \|x\| = 1 $$ and denote $\tilde x$ and $\tilde \gamma$ the maximizers.   The question is how to bound the difference $|\hat \gamma - \tilde \gamma|$ as some function of $E$? Intuitively, if $E$ is small then $\hat \gamma$ and $\tilde \gamma$ should also be close. Any pointers to the relevant literature would be also appreciated.","Let $A,B \in \mathbb{R}^{n \times n}$ be two positive semi-definite matrices and let $a > 0$ be a constant. Consider the following maximization problem $$ \max_{x \in \mathbb{R}^n, \gamma}\ x^T \left(\frac{\gamma}{a + \gamma} A - \gamma B \right)x \qquad \text{subject to } \gamma > 0, \|x\| = 1 $$ and suppose that we know that $\hat x$ and $\hat \gamma$ maximize the expression.  In this case, we have that $\hat x$ is the largest eigenvector of the matrix $\frac{\hat \gamma}{a + \hat \gamma} A - \hat \gamma B$.   Suppose that we perturb the matrix A by another matrix $E$, which is supposed to be small in some norm (say operator). Consider again the following optimization problem $$ \max_{x \in \mathbb{R}^n, \gamma}\ x^T \left(\frac{\gamma}{a + \gamma} (A+E) - \gamma B \right)x \qquad \text{subject to } \gamma > 0, \|x\| = 1 $$ and denote $\tilde x$ and $\tilde \gamma$ the maximizers.   The question is how to bound the difference $|\hat \gamma - \tilde \gamma|$ as some function of $E$? Intuitively, if $E$ is small then $\hat \gamma$ and $\tilde \gamma$ should also be close. Any pointers to the relevant literature would be also appreciated.",,"['linear-algebra', 'matrices', 'optimization', 'perturbation-theory']"
31,The set of symmetric matrices as a manifold,The set of symmetric matrices as a manifold,,"How would I start off proving that the set $S$ , of symmetric $n\times n$ matrices, is a manifold. I tried using the definition directly by saying $M_n =$ the space of all $n\times n$ matrices. For every $A\in M_n$ there exists open sets $U=V=M_n$ and a bijection $F: U\to V$ by $F(A)= A-A^T$ Therefore we have $F(U \cap S) = F(S)$ since $S$ is a subset of $M_n=\{0\} \cap M_n$ this is where I get stuck. Also, I know that the set of all symmetric $n\times n$ matrices is $\frac{n^2+n}{2}$ , therefore that is the dimension of the manifold. Definition: A set $M$ (subset of $\Bbb{R}^n$ ) is a $k$ -dimensional manifold if for every $x\in M$ there exists open sets $U$ , $V$ and a bijection $h:U\to V$ with $x\in U$ and $H(U \cap M) = V \cap (\Bbb{R}^k \times \{c^{k+1},\ldots ,c^n\})$ for all $c$ 's constants","How would I start off proving that the set , of symmetric matrices, is a manifold. I tried using the definition directly by saying the space of all matrices. For every there exists open sets and a bijection by Therefore we have since is a subset of this is where I get stuck. Also, I know that the set of all symmetric matrices is , therefore that is the dimension of the manifold. Definition: A set (subset of ) is a -dimensional manifold if for every there exists open sets , and a bijection with and for all 's constants","S n\times n M_n = n\times n A\in M_n U=V=M_n F: U\to V F(A)= A-A^T F(U \cap S) = F(S) S M_n=\{0\} \cap M_n n\times n \frac{n^2+n}{2} M \Bbb{R}^n k x\in M U V h:U\to V x\in U H(U \cap M) = V \cap (\Bbb{R}^k \times \{c^{k+1},\ldots ,c^n\}) c","['matrices', 'differential-geometry']"
32,Is there an efficient method to find all the self-inverse matrices with integers in a given range?,Is there an efficient method to find all the self-inverse matrices with integers in a given range?,,"Given $n$ and a range, for example $[-10,10]$, is there an efficient method to find  all $n \times n$-matrices $A$ with integers in the given range, which are self-inverse (that means the equation $A=A^{-1}$ holds)? Some necessary conditions for $A$: $\det(A)=-1$ or $\det(A)=1$ $A$ has no eigenvalues other than $-1$ and $1$ The minimal polynomial of $A$ divides $x^2-1$ With $A$, the matrices $-A$ , $A^T$ and $B^{-1}AB$ for any invertible matrix B are also self-inverse. So, is there a method to find the matrices systematically without checking all possible matrices, which would be infeasible for, lets say $n = 4$ and range $[-10,10]$?","Given $n$ and a range, for example $[-10,10]$, is there an efficient method to find  all $n \times n$-matrices $A$ with integers in the given range, which are self-inverse (that means the equation $A=A^{-1}$ holds)? Some necessary conditions for $A$: $\det(A)=-1$ or $\det(A)=1$ $A$ has no eigenvalues other than $-1$ and $1$ The minimal polynomial of $A$ divides $x^2-1$ With $A$, the matrices $-A$ , $A^T$ and $B^{-1}AB$ for any invertible matrix B are also self-inverse. So, is there a method to find the matrices systematically without checking all possible matrices, which would be infeasible for, lets say $n = 4$ and range $[-10,10]$?",,"['linear-algebra', 'matrices', 'inverse']"
33,Positive semidefinite but non diagonalizable real matrix - proof real parts of eigenvalues are non-negative,Positive semidefinite but non diagonalizable real matrix - proof real parts of eigenvalues are non-negative,,I have a question about positive semidefinite matrices that are non diagonalizable.  Example:  \begin{equation} A= \left(\begin{array}{cc} 2 & 1\\ 0 & 2\\ \end{array}\right) \end{equation} Clearly the (real part of the) eigenvalues of $A$ are non-negative. But how do I prove in general that the real part of the Eigenvalues of a positive semi-definite real matrix are non-negative? (I have seen the proof where they use diagonalization of the matrix ($B=T^{-1}DT$) but this is not possible for all positive semi-definite real matrices.),I have a question about positive semidefinite matrices that are non diagonalizable.  Example:  \begin{equation} A= \left(\begin{array}{cc} 2 & 1\\ 0 & 2\\ \end{array}\right) \end{equation} Clearly the (real part of the) eigenvalues of $A$ are non-negative. But how do I prove in general that the real part of the Eigenvalues of a positive semi-definite real matrix are non-negative? (I have seen the proof where they use diagonalization of the matrix ($B=T^{-1}DT$) but this is not possible for all positive semi-definite real matrices.),,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'diagonalization']"
34,Eigenvalues of $AB$ from eigenvalues of $A$ and $B$,Eigenvalues of  from eigenvalues of  and,AB A B,"Is it possible to find the eigenvalues of $AB$ if we know the eigenvalues of $A$, say $\lambda_1, \lambda_2,...,\lambda_n$ and those of $B$ say $\lambda_1, \mu_2,...,\mu_n$ and given that $A$ and $B$ are positive semi/definite symmetric complex valued matrices. Even if not possible can we build a relation of magnitude of the eigenvalues? Thank you. Related to https://math.stackexchange.com/questions/492697/possible-determinant-inequality-det-leftiaaib-right-1-leq-det-l","Is it possible to find the eigenvalues of $AB$ if we know the eigenvalues of $A$, say $\lambda_1, \lambda_2,...,\lambda_n$ and those of $B$ say $\lambda_1, \mu_2,...,\mu_n$ and given that $A$ and $B$ are positive semi/definite symmetric complex valued matrices. Even if not possible can we build a relation of magnitude of the eigenvalues? Thank you. Related to https://math.stackexchange.com/questions/492697/possible-determinant-inequality-det-leftiaaib-right-1-leq-det-l",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
35,Can we find $x$ such that $\det[x^2 A + x B + C] = 0$? [closed],Can we find  such that ? [closed],x \det[x^2 A + x B + C] = 0,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question If $\mathbf{A}, \mathbf{B}, \mathbf{C} \in \mathbb{C}^{4 \times 4}$ are invertible matrices, is there a way to find $x$ such that the following determinant vanishes? $$\det \left( x^2 \mathbf{A} + x \mathbf{B} + \mathbf{C}\right) = 0$$","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question If are invertible matrices, is there a way to find such that the following determinant vanishes?","\mathbf{A}, \mathbf{B}, \mathbf{C} \in \mathbb{C}^{4 \times 4} x \det \left( x^2 \mathbf{A} + x \mathbf{B} + \mathbf{C}\right) = 0","['matrices', 'determinant']"
36,"Let $A^{27}=A^{64}=I$, show that $A=I$","Let , show that",A^{27}=A^{64}=I A=I,"Let $A$ be a square matrix, $A^{27}=A^{64}=I$, show that $A=I$","Let $A$ be a square matrix, $A^{27}=A^{64}=I$, show that $A=I$",,"['linear-algebra', 'matrices']"
37,Prove that $\operatorname{trace}(A^TA) = 0\ \iff\ A = 0$.,Prove that .,\operatorname{trace}(A^TA) = 0\ \iff\ A = 0,"Given that $A_{m \times n}$ has real entries, I want to prove that $\operatorname{trace}(A^TA) = 0$ if and only if $A = 0$ . In other words, I want to show that the only way for the trace of $(A^TA)$ to be zero is if $A$ is a zero matrix, and that if $A$ is a zero matrix then $A^TA$ has a trace of zero. Intuitively this makes sense to me. My idea is that in order to get zeros on the main diagonal of any product of matrices I'd need at least one of the matrices to have zeros on its main diagonal. In this case, because the product is between $A$ and its transpose, I figure it $A$ does indeed need to be a zero matrix. However, I'm having difficulty turning my intuition into an actual proof.","Given that has real entries, I want to prove that if and only if . In other words, I want to show that the only way for the trace of to be zero is if is a zero matrix, and that if is a zero matrix then has a trace of zero. Intuitively this makes sense to me. My idea is that in order to get zeros on the main diagonal of any product of matrices I'd need at least one of the matrices to have zeros on its main diagonal. In this case, because the product is between and its transpose, I figure it does indeed need to be a zero matrix. However, I'm having difficulty turning my intuition into an actual proof.",A_{m \times n} \operatorname{trace}(A^TA) = 0 A = 0 (A^TA) A A A^TA A A,"['linear-algebra', 'matrices']"
38,Find Matrix $A^{50}$?,Find Matrix ?,A^{50},Find the matrix $A^{50}$ given $$A = \begin{bmatrix} 2 & -1 \\ 0 & 1 \end{bmatrix}$$ as well as for $$A=\begin{bmatrix} 2 & 0 \\ 2 & 1\end{bmatrix}$$ I was practicing some questions for my exam and I found questions of this form in a previous year's paper. I don't know how to do such questions. Please assist over this question. Thank You,Find the matrix $A^{50}$ given $$A = \begin{bmatrix} 2 & -1 \\ 0 & 1 \end{bmatrix}$$ as well as for $$A=\begin{bmatrix} 2 & 0 \\ 2 & 1\end{bmatrix}$$ I was practicing some questions for my exam and I found questions of this form in a previous year's paper. I don't know how to do such questions. Please assist over this question. Thank You,,"['matrices', 'education']"
39,Show that this matrix is singular [closed],Show that this matrix is singular [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question If $\det P=-1$ and $P$ is an orthogonal matrix. Show that $P+I_n$ is singular matrix. Please help it with only matrix algebra.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question If $\det P=-1$ and $P$ is an orthogonal matrix. Show that $P+I_n$ is singular matrix. Please help it with only matrix algebra.",,"['linear-algebra', 'matrices', 'orthogonal-matrices']"
40,"If $A^{4}=I$, does this imply $A$ is invertible?","If , does this imply  is invertible?",A^{4}=I A,"If I have an $n\times n$ matrix, and $A^{4}=I_n$, does this imply that $A^{-1}$ exists? My reasoning is $A^{4}=I$, so $(A^{4})^{-1}=I=(A^{-1})^{4}$. Is this valid? Thanks for your time in answering what is probably a super simple question.","If I have an $n\times n$ matrix, and $A^{4}=I_n$, does this imply that $A^{-1}$ exists? My reasoning is $A^{4}=I$, so $(A^{4})^{-1}=I=(A^{-1})^{4}$. Is this valid? Thanks for your time in answering what is probably a super simple question.",,['matrices']
41,Is there always a matrix $X$ such that $X^2=A$?,Is there always a matrix  such that ?,X X^2=A,"Is it true that for every $A\in M_{2\times 2} (\mathbb{C})$ there's an $X\in M_{2\times 2} (\mathbb{C})$ such that $X^2=A$? For the matter of fact, I don't have a clue, other than evaluating the general case and solve the system of equations. I also thought about the fact that we're aobve $\mathbb{C}$ and therefore the characteristic polynomial can be factored to linear terms.","Is it true that for every $A\in M_{2\times 2} (\mathbb{C})$ there's an $X\in M_{2\times 2} (\mathbb{C})$ such that $X^2=A$? For the matter of fact, I don't have a clue, other than evaluating the general case and solve the system of equations. I also thought about the fact that we're aobve $\mathbb{C}$ and therefore the characteristic polynomial can be factored to linear terms.",,"['linear-algebra', 'matrices']"
42,Cayley–Hamilton And Invertible Matrix,Cayley–Hamilton And Invertible Matrix,,"In my lecture notes, it was mentioned that if the Cayley–Hamilton polynomial has a free element then it is invertible. Namely, $P_A(x) = a_n x^n + \dots + a_1 x + a_0$ there $a_0 \neq 0$. Why is it correct?","In my lecture notes, it was mentioned that if the Cayley–Hamilton polynomial has a free element then it is invertible. Namely, $P_A(x) = a_n x^n + \dots + a_1 x + a_0$ there $a_0 \neq 0$. Why is it correct?",,"['linear-algebra', 'matrices', 'inverse', 'cayley-hamilton']"
43,If $A^4=I$ then $A$ must be diagonalizable?,If  then  must be diagonalizable?,A^4=I A,"Suppose we have a real matrix $A$ which satisfies $A^4=I$, can we determine if $A$ is diagonalizable? I believe the answer is that we can't because all we know about the matrix $A$ is that it is invertible (otherwise $A^4$ couldn't be an invertible matrix).. How can I prove it? How can I find such a matrix $A$ which isn't diagonalizable but $A^4 = I$? The only matrix $A$ I was able to find which satisfies $A^4=I$ is the identity matrix itself but the identity matrix is diagonalizable.","Suppose we have a real matrix $A$ which satisfies $A^4=I$, can we determine if $A$ is diagonalizable? I believe the answer is that we can't because all we know about the matrix $A$ is that it is invertible (otherwise $A^4$ couldn't be an invertible matrix).. How can I prove it? How can I find such a matrix $A$ which isn't diagonalizable but $A^4 = I$? The only matrix $A$ I was able to find which satisfies $A^4=I$ is the identity matrix itself but the identity matrix is diagonalizable.",,"['linear-algebra', 'matrices', 'diagonalization']"
44,How to prove the inequality $\det (AA^T) \ge 0$? [closed],How to prove the inequality ? [closed],\det (AA^T) \ge 0,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 6 months ago . Improve this question How to prove for any matrix $A \in \Bbb R^{n \times n}$ , that the inequality $\det(AA^T) \ge 0$ is true?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 6 months ago . Improve this question How to prove for any matrix , that the inequality is true?",A \in \Bbb R^{n \times n} \det(AA^T) \ge 0,"['linear-algebra', 'matrices', 'determinant']"
45,Find the power of the matrix.,Find the power of the matrix.,,"Let $A = \left( {\begin{array}{*{20}{c}} 0&1&1\\ 1&0&1\\ 1&1&0 \end{array}} \right)$ . I want to find $A^k,$ where $k \in N$ . So far I calculated $A^2, A^3, A^4,...$ but I can not see the general formula for $A^k$ . Here are $A^2, A^3, A^4, A^5$ . Not sure if this leads to anything but I found the general formula for $B^k$ , where $B = \left( {\begin{array}{*{20}{c}} 1&1&1\\ 1&1&1\\ 1&1&1 \end{array}} \right)$ . ${B^k} = \left( {\begin{array}{*{20}{c}} {{3^{k - 1}}}&{{3^{k - 1}}}&{{3^{k - 1}}}\\ {{3^{k - 1}}}&{{3^{k - 1}}}&{{3^{k - 1}}}\\ {{3^{k - 1}}}&{{3^{k - 1}}}&{{3^{k - 1}}} \end{array}} \right)$ Thanks in advance.","Let . I want to find where . So far I calculated but I can not see the general formula for . Here are . Not sure if this leads to anything but I found the general formula for , where . Thanks in advance.","A = \left( {\begin{array}{*{20}{c}}
0&1&1\\
1&0&1\\
1&1&0
\end{array}} \right) A^k, k \in N A^2, A^3, A^4,... A^k A^2, A^3, A^4, A^5 B^k B = \left( {\begin{array}{*{20}{c}}
1&1&1\\
1&1&1\\
1&1&1
\end{array}} \right) {B^k} = \left( {\begin{array}{*{20}{c}}
{{3^{k - 1}}}&{{3^{k - 1}}}&{{3^{k - 1}}}\\
{{3^{k - 1}}}&{{3^{k - 1}}}&{{3^{k - 1}}}\\
{{3^{k - 1}}}&{{3^{k - 1}}}&{{3^{k - 1}}}
\end{array}} \right)","['linear-algebra', 'matrices']"
46,"Does $\exp(At)=\exp(Bt)$ for infinitely many $t$ imply $A=B$ where $A,B$ are square matrices? [closed]",Does  for infinitely many  imply  where  are square matrices? [closed],"\exp(At)=\exp(Bt) t A=B A,B","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Suppose $A$ and $B$ are two square matrices so that $e^{At}=e^{Bt}$ for infinite (countable or uncountable) values of $t$ where $t$ is positive. Do you think that $A$ has to be equal to $B$? Thanks, Trung Dung. Maybe I do not state clearly or correctly. I mean that the equality holds for all $t\in (0, T)$ where $T>0$ or $T=+\infty$, i.e. for uncountable $t$. In this case I think some of the counter-examples above do not work because it is correct for countable $t$.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Suppose $A$ and $B$ are two square matrices so that $e^{At}=e^{Bt}$ for infinite (countable or uncountable) values of $t$ where $t$ is positive. Do you think that $A$ has to be equal to $B$? Thanks, Trung Dung. Maybe I do not state clearly or correctly. I mean that the equality holds for all $t\in (0, T)$ where $T>0$ or $T=+\infty$, i.e. for uncountable $t$. In this case I think some of the counter-examples above do not work because it is correct for countable $t$.",,"['linear-algebra', 'matrices', 'matrix-equations', 'matrix-exponential']"
47,Prove that $\operatorname{Trace}(A^2) \le 0$,Prove that,\operatorname{Trace}(A^2) \le 0,"Let $A \in M_n(\mathbb{R})$  is a antisymmetric matrix such as $A^T=-A$. Prove that $\operatorname{Trace}(A^2) \le 0 $ I see that, for some matrix such as, their terms in diagonal are negative ?","Let $A \in M_n(\mathbb{R})$  is a antisymmetric matrix such as $A^T=-A$. Prove that $\operatorname{Trace}(A^2) \le 0 $ I see that, for some matrix such as, their terms in diagonal are negative ?",,"['linear-algebra', 'matrices', 'trace']"
48,Inverse of this $3\times 3$ matrix using the Cayley–Hamilton theorem,Inverse of this  matrix using the Cayley–Hamilton theorem,3\times 3,Find the inverse of the matrix $$\begin{pmatrix} -1 & 2& 0 \\ 1& 1 &0 \\ 2 & -1& 2 \end{pmatrix}$$ using the Cayley–Hamilton theorem. Thanks!,Find the inverse of the matrix $$\begin{pmatrix} -1 & 2& 0 \\ 1& 1 &0 \\ 2 & -1& 2 \end{pmatrix}$$ using the Cayley–Hamilton theorem. Thanks!,,"['linear-algebra', 'matrices', 'inverse']"
49,What are the eigenvalues of this $6 \times 6$ matrix?,What are the eigenvalues of this  matrix?,6 \times 6,"What are the eigenvalues of the following matrix? $$A=\left(\begin{matrix}    0 & 0 & 0 & 1 & 0&0\\   0 & 0 & 0 & 0 & 1&0\\   0 & 0& 0 & 0 & 0&1\\   1 & 0 & 0 & 0 & 0&0\\   0 & 1 & 0 & 0 & 0&0\\0&0&1&0&0&0 \end{matrix}\right)$$ My attempt: I know how  to find the eigenvalues of a $2 \times 2$ matrix and of a $3 \times 3$ matrix. But here I am very confused, as I don't know how to find the eigenvalues of a $6 \times 6$ matrix. Is there any easy method or some tricky method?","What are the eigenvalues of the following matrix? $$A=\left(\begin{matrix}    0 & 0 & 0 & 1 & 0&0\\   0 & 0 & 0 & 0 & 1&0\\   0 & 0& 0 & 0 & 0&1\\   1 & 0 & 0 & 0 & 0&0\\   0 & 1 & 0 & 0 & 0&0\\0&0&1&0&0&0 \end{matrix}\right)$$ My attempt: I know how  to find the eigenvalues of a $2 \times 2$ matrix and of a $3 \times 3$ matrix. But here I am very confused, as I don't know how to find the eigenvalues of a $6 \times 6$ matrix. Is there any easy method or some tricky method?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
50,Prove or disprove: If $A^2$ is normal matrix then $A$ is normal matrix,Prove or disprove: If  is normal matrix then  is normal matrix,A^2 A,Prove or disprove: If $A^2$ is normal matrix then $A$ is normal matrix. I think this is wrong but simply can't build a counterexample. Any hints on how to build a counterexample? There are many conditions and it's hard for me to find a matrix. Note that: $A$ is normal if $AA^* = A^*A$ .,Prove or disprove: If $A^2$ is normal matrix then $A$ is normal matrix. I think this is wrong but simply can't build a counterexample. Any hints on how to build a counterexample? There are many conditions and it's hard for me to find a matrix. Note that: $A$ is normal if $AA^* = A^*A$ .,,"['linear-algebra', 'matrices']"
51,Algorithm for generating positive semidefinite matrices,Algorithm for generating positive semidefinite matrices,,"I'm looking for an efficient algorithm to generate large positive semidefinite matrices. One possible way I know of is: generate a random square matrix multiply it with its transpose. calculate all eigenvalues of the result matrix and check if all of them are non-negative. However, this approach is infeasible given a large matrix, say $1000 \times 1000$ or more. Could anyone please suggest an efficient way to generate a positive semidefinite matrix ? Is there any MATLAB function for this job? Thanks,","I'm looking for an efficient algorithm to generate large positive semidefinite matrices. One possible way I know of is: generate a random square matrix multiply it with its transpose. calculate all eigenvalues of the result matrix and check if all of them are non-negative. However, this approach is infeasible given a large matrix, say $1000 \times 1000$ or more. Could anyone please suggest an efficient way to generate a positive semidefinite matrix ? Is there any MATLAB function for this job? Thanks,",,"['linear-algebra', 'matrices']"
52,Determinant composed from polynomials $p_1(x) = x + a$ and $p_2(x) = x^2 + bx + c$,Determinant composed from polynomials  and,p_1(x) = x + a p_2(x) = x^2 + bx + c,"Let $p_1(x) = x + a$ and $p_2(x) = x^2 + bx + c$ be two polynomials with real coefficients, and $x_1$ and $x_2$ be two arbitrary real numbers. Consider the following determinant $$D(x) = \begin{vmatrix}    1     & p_1(x_1) & p_2(x_1)\\    1 & p_1(x_2) & p_2(x_2)\\ 1 & p_1(x) & p_2(x)\ \end{vmatrix} $$ Show that $D(x) = m(x-x_1)(x-x_2)$","Let $p_1(x) = x + a$ and $p_2(x) = x^2 + bx + c$ be two polynomials with real coefficients, and $x_1$ and $x_2$ be two arbitrary real numbers. Consider the following determinant $$D(x) = \begin{vmatrix}    1     & p_1(x_1) & p_2(x_1)\\    1 & p_1(x_2) & p_2(x_2)\\ 1 & p_1(x) & p_2(x)\ \end{vmatrix} $$ Show that $D(x) = m(x-x_1)(x-x_2)$",,"['linear-algebra', 'matrices', 'polynomials', 'determinant']"
53,Row replacement operation not changing the determinant,Row replacement operation not changing the determinant,,Can someone prove why a row replacement operation does not change the determinant of a matrix? **row replacement operation being adding one row to another or something of that sort,Can someone prove why a row replacement operation does not change the determinant of a matrix? **row replacement operation being adding one row to another or something of that sort,,"['linear-algebra', 'matrices', 'determinant', 'gaussian-elimination']"
54,When a determinant is zero,When a determinant is zero,,"Is it true that if $C$ is a square matrix of size $n$ and $\det(C) = 0,$ then $C^n = O_n$ or the $0$ matrix? If yes, then why is that? I know that the reverse is obviously true, so I wondered if there is an equivalence relation between $\det(C) = 0$ and $C^n = \text{ the $0$ matrix. }$","Is it true that if $C$ is a square matrix of size $n$ and $\det(C) = 0,$ then $C^n = O_n$ or the $0$ matrix? If yes, then why is that? I know that the reverse is obviously true, so I wondered if there is an equivalence relation between $\det(C) = 0$ and $C^n = \text{ the $0$ matrix. }$",,"['matrices', 'determinant', 'equivalence-relations']"
55,What is the rank of a matrix for?,What is the rank of a matrix for?,,I am currently working with matrices. However I know how to calculate the rank.(By calculating the the row or colume with $0$) My question is: What is the rank of a matrix for? For what can I use it further?,I am currently working with matrices. However I know how to calculate the rank.(By calculating the the row or colume with $0$) My question is: What is the rank of a matrix for? For what can I use it further?,,"['linear-algebra', 'matrices']"
56,Matrix - Show $\det(A) =0$,Matrix - Show,\det(A) =0,"I am a little stuck on this Matrix problem. Suppose that for complex square matrix A,B the following holds: $AB -BA = A$ Show that $\det(A)=0$ That would mean that A has no inverse. So I thought, let's suppose there exists an Inverse element and that would lead me to a contradiction later on. $AB = A + BA \implies AB = A(I+B)$ multiply though with $(AB)^{-1}$ $I = B^{-1}A^{-1}A(I+B)$ $I=B^{-1}(I+B)$ $I=B^{-1} +I$ so $B^{-1}$ is equal to $0$. Contradiction in my view. Or what does this mean? Could you please help me interpret? :) Thanks in advance!","I am a little stuck on this Matrix problem. Suppose that for complex square matrix A,B the following holds: $AB -BA = A$ Show that $\det(A)=0$ That would mean that A has no inverse. So I thought, let's suppose there exists an Inverse element and that would lead me to a contradiction later on. $AB = A + BA \implies AB = A(I+B)$ multiply though with $(AB)^{-1}$ $I = B^{-1}A^{-1}A(I+B)$ $I=B^{-1}(I+B)$ $I=B^{-1} +I$ so $B^{-1}$ is equal to $0$. Contradiction in my view. Or what does this mean? Could you please help me interpret? :) Thanks in advance!",,"['matrices', 'determinant']"
57,Binomial formula for matrices,Binomial formula for matrices,,"I have two matrices $A$ and $B$ that do not commute, i.e., $A B \neq B A$ . Thus, if I apply the binomial formula, I get $$ (A+B)^{n} \neq (B+A)^{n}$$ This is my own deduction. I haven't found any theorem about this property. Do you know if there exists one?","I have two matrices and that do not commute, i.e., . Thus, if I apply the binomial formula, I get This is my own deduction. I haven't found any theorem about this property. Do you know if there exists one?",A B A B \neq B A  (A+B)^{n} \neq (B+A)^{n},"['linear-algebra', 'matrices', 'binomial-theorem']"
58,Why is the determinant of this matrix zero?,Why is the determinant of this matrix zero?,,"I have a system of equations to solve $Ax = b$, but the determinant of matrix $A$ is zero. $$A=\begin{bmatrix}1&1&0&0\\0&0&1&1\\1&0&1&0\\0&1&0&1\end{bmatrix}$$ To me, none of the rows/columns of this matrix look dependent. I am wondering why the determinant of matrix $A$ is zero? Also, how should I solve this system or estimate $x$?","I have a system of equations to solve $Ax = b$, but the determinant of matrix $A$ is zero. $$A=\begin{bmatrix}1&1&0&0\\0&0&1&1\\1&0&1&0\\0&1&0&1\end{bmatrix}$$ To me, none of the rows/columns of this matrix look dependent. I am wondering why the determinant of matrix $A$ is zero? Also, how should I solve this system or estimate $x$?",,"['linear-algebra', 'matrices', 'systems-of-equations', 'determinant']"
59,Determinant of the identity matrix with columns in reverse order,Determinant of the identity matrix with columns in reverse order,,"If $C \in M_n(\mathbb{R})$ such that $(C)_{ij} = (I)_{(i)(n+1-j)}$ , how do I prove that $\det (C) = -1$ . What I tried: I know that $\det(C) = -1$ for $C\in M_{2}(\mathbb{R})$ , but I don't know how to prove it for $M_n(\mathbb{R})$ .","If such that , how do I prove that . What I tried: I know that for , but I don't know how to prove it for .",C \in M_n(\mathbb{R}) (C)_{ij} = (I)_{(i)(n+1-j)} \det (C) = -1 \det(C) = -1 C\in M_{2}(\mathbb{R}) M_n(\mathbb{R}),"['linear-algebra', 'matrices', 'determinant', 'hankel-matrices']"
60,Matrix addition/multiplication with different sizes,Matrix addition/multiplication with different sizes,,I have the following two matrices: $$A=\begin{pmatrix}1 & -2\\3 & 1\end{pmatrix}\text{ and }B=\begin{pmatrix}1 & 3 &   2\\-1 & 0 & 2\end{pmatrix}$$ So I have two matrixes with different sizes. Multiple sources tell me that I can't do multiplication or addition with matrix of different sizes. So I'm a bit confused. Can I do it with these? How?,I have the following two matrices: So I have two matrixes with different sizes. Multiple sources tell me that I can't do multiplication or addition with matrix of different sizes. So I'm a bit confused. Can I do it with these? How?,"A=\begin{pmatrix}1 & -2\\3 & 1\end{pmatrix}\text{ and }B=\begin{pmatrix}1 & 3 & 
 2\\-1 & 0 & 2\end{pmatrix}",['matrices']
61,Artin's Algebra Exercise 1.1.16,Artin's Algebra Exercise 1.1.16,,"The question goes $A^k = 0$ , for some $k>0$ ( $A$ is square). Prove that $A+I$ is invertible. I did $A^k = 0 \implies A^k+I=I$ . So, $(A+I)(A^{k-1} +.... + I) = I$ . So it's invertible with the inverse given above. I feel like this is wrong. I'm not very confident that I'm allowed to factorise it like that since $\det{A^k}=(\det{A})^k=0.$ And doing $A^{k-1}$ during factorisation doesn't feel right since you are multiplying with $A^{-1}$ . Can you please direct me to the correct direction and tell me why the thing I did is incorrect? Thanks in advance. Edit: I see that my factorisation is incorrect (plus/minus signs) but the question remains the same.","The question goes , for some ( is square). Prove that is invertible. I did . So, . So it's invertible with the inverse given above. I feel like this is wrong. I'm not very confident that I'm allowed to factorise it like that since And doing during factorisation doesn't feel right since you are multiplying with . Can you please direct me to the correct direction and tell me why the thing I did is incorrect? Thanks in advance. Edit: I see that my factorisation is incorrect (plus/minus signs) but the question remains the same.",A^k = 0 k>0 A A+I A^k = 0 \implies A^k+I=I (A+I)(A^{k-1} +.... + I) = I \det{A^k}=(\det{A})^k=0. A^{k-1} A^{-1},"['linear-algebra', 'matrices', 'inverse']"
62,"Is it true that if $A^4 = I$, then $A^2 = \pm I$? [closed]","Is it true that if , then ? [closed]",A^4 = I A^2 = \pm I,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question So for linear algebra, I either need to prove that, for all square matrices, if $A^4 = I$, then $A^2 = \pm I$, or find a counterexample of this statement. Can anyone help please? Thanks!","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question So for linear algebra, I either need to prove that, for all square matrices, if $A^4 = I$, then $A^2 = \pm I$, or find a counterexample of this statement. Can anyone help please? Thanks!",,"['linear-algebra', 'matrices']"
63,"Let A and B are $2 \times 2$ matrices, where AB = BA, show that $\text{A B}^2 = \text{B}^2 \text{A}$","Let A and B are  matrices, where AB = BA, show that",2 \times 2 \text{A B}^2 = \text{B}^2 \text{A},"Let A and B are $2\times 2$ matrices, where $$\text{AB} = \text{BA}$$show that :   $$\text{A B}^2 = \text{B}^2 \text{A}$$ Well, I assumed A and B are symmetric matrices, so $$ AB^2 = A^T B pow(2T) = (B^ 2A)^T = B^2T = A^T = B^2 A $$ I thought for long time but I didn't get the right solution, any expert can help please!","Let A and B are $2\times 2$ matrices, where $$\text{AB} = \text{BA}$$show that :   $$\text{A B}^2 = \text{B}^2 \text{A}$$ Well, I assumed A and B are symmetric matrices, so $$ AB^2 = A^T B pow(2T) = (B^ 2A)^T = B^2T = A^T = B^2 A $$ I thought for long time but I didn't get the right solution, any expert can help please!",,"['linear-algebra', 'matrices']"
64,True or False - Matrix Equation,True or False - Matrix Equation,,$$A^2 - AB - BA + B^2 = 0 \implies A = B $$ I know this is false but how can I go about proving it. I got to: $$A^2 - AB - BA + B^2$$ $$ = (A - B)(A-B) = 0$$ I know that if A and B are matrices such that AB = 0 then AB need not be 0 but how can I show this equation wise without resorting to a counter examples which would a bit annoying in this case since it is matrix multiplication?,$$A^2 - AB - BA + B^2 = 0 \implies A = B $$ I know this is false but how can I go about proving it. I got to: $$A^2 - AB - BA + B^2$$ $$ = (A - B)(A-B) = 0$$ I know that if A and B are matrices such that AB = 0 then AB need not be 0 but how can I show this equation wise without resorting to a counter examples which would a bit annoying in this case since it is matrix multiplication?,,"['linear-algebra', 'matrices']"
65,Can every diagonalizable matrix be diagonalized into the identity matrix?,Can every diagonalizable matrix be diagonalized into the identity matrix?,,"I'm a chemistry major and I haven't taken much math, but this came up in a discussion of quantum chemistry and my professor said (not very confidently) that if a matrix is diagonalizable, then you should be able to diagonalize it to the identity matrix. I suspect this is true for symmetrical matrices, but not all matrices. Is that correct?","I'm a chemistry major and I haven't taken much math, but this came up in a discussion of quantum chemistry and my professor said (not very confidently) that if a matrix is diagonalizable, then you should be able to diagonalize it to the identity matrix. I suspect this is true for symmetrical matrices, but not all matrices. Is that correct?",,"['linear-algebra', 'matrices']"
66,Matrix exponential of sum of matrices,Matrix exponential of sum of matrices,,"I'm having trouble understanding why, in general, if $\mathbf A, \mathbf B$ are $n \times n$ matrices, it does not hold that $$\exp(\mathbf A + \mathbf B) = \exp(\mathbf A)\exp(\mathbf B)$$ but holds in general when $\mathbf A\mathbf B = \mathbf B \mathbf A$ . Specifically, I can try to prove this using the definition of matrix exponentials as follows, without using commutativity of the matrices. Notice that \begin{align*} \exp(\mathbf A + \mathbf B) &\equiv \sum_{i=0}^{\infty} \frac{(\mathbf A + \mathbf B)^i}{i!} \\ &= \sum_{i=0}^{\infty} \sum_{j = 0}^{i} \binom ij \frac{\mathbf A^{i-j}\mathbf B^{j}}{i!} \\ &= \sum_{i=0}^{\infty} \sum_{j = 0}^{i} \frac{\mathbf A^{i-j}}{(i-j)!} \frac{\mathbf B^j}{j!} \end{align*} which does not rely on the commutativity of multiplying $\mathbf A, \mathbf B$ , but only relies on the commutativity of matrix addition under absolute convergence. Meanwhile, \begin{align*} \exp(\mathbf A)\exp(\mathbf B) &\equiv\left(\sum_{k = 0}^{\infty} \frac{\mathbf A^k} {k!} \right)\left(\sum_{l=0}^\infty \frac{\mathbf B^l}{l!}\right)\\ &= \sum_{k=0}^\infty\sum_{l=0}^\infty \frac{\mathbf A^k}{k!} \frac{\mathbf B^l}{l!} \end{align*} which still does not rely on the commutativity of multiplying the two matrices. However, we can see that both sums converge (because the matrix exponential always converges) and also that we can rewrite the first expression to be equal to the second one, because we can set any values for $k, l$ and see that an equivalent term occurs in the first sum, and vice versa. Why does this fail when $\mathbf A, \mathbf B$ do not commute?","I'm having trouble understanding why, in general, if are matrices, it does not hold that but holds in general when . Specifically, I can try to prove this using the definition of matrix exponentials as follows, without using commutativity of the matrices. Notice that which does not rely on the commutativity of multiplying , but only relies on the commutativity of matrix addition under absolute convergence. Meanwhile, which still does not rely on the commutativity of multiplying the two matrices. However, we can see that both sums converge (because the matrix exponential always converges) and also that we can rewrite the first expression to be equal to the second one, because we can set any values for and see that an equivalent term occurs in the first sum, and vice versa. Why does this fail when do not commute?","\mathbf A, \mathbf B n \times n \exp(\mathbf A + \mathbf B) = \exp(\mathbf A)\exp(\mathbf B) \mathbf A\mathbf B = \mathbf B \mathbf A \begin{align*}
\exp(\mathbf A + \mathbf B) &\equiv \sum_{i=0}^{\infty} \frac{(\mathbf A + \mathbf B)^i}{i!} \\
&= \sum_{i=0}^{\infty} \sum_{j = 0}^{i} \binom ij \frac{\mathbf A^{i-j}\mathbf B^{j}}{i!} \\
&= \sum_{i=0}^{\infty} \sum_{j = 0}^{i} \frac{\mathbf A^{i-j}}{(i-j)!} \frac{\mathbf B^j}{j!}
\end{align*} \mathbf A, \mathbf B \begin{align*}
\exp(\mathbf A)\exp(\mathbf B) &\equiv\left(\sum_{k = 0}^{\infty} \frac{\mathbf A^k} {k!} \right)\left(\sum_{l=0}^\infty \frac{\mathbf B^l}{l!}\right)\\
&= \sum_{k=0}^\infty\sum_{l=0}^\infty \frac{\mathbf A^k}{k!} \frac{\mathbf B^l}{l!}
\end{align*} k, l \mathbf A, \mathbf B","['linear-algebra', 'matrices', 'taylor-expansion', 'matrix-exponential']"
67,Compute $e^{Ax}$ for the $2 \times 2$ matrix $A$,Compute  for the  matrix,e^{Ax} 2 \times 2 A,Consider the real $2 \times 2$ matrix $A$  $$A=\begin{pmatrix} 1 & 0 \\ 0 & -1\end{pmatrix}$$ I want to compute $e^{Ax}$. I can see that $A^2=I$ which looks like it should be useful. Using the definition of the matrix exponential $$e^{Ax}=I+Ax+\frac{(Ax)^2}{2!}+ \cdots$$ I find that $$e^{Ax}=I\left(1+\frac{x^2}{2!}+ \cdots\right)+A\left(x+\frac{x^3}{3!}+ \cdots\right)$$ Is there anything I can do from here?,Consider the real $2 \times 2$ matrix $A$  $$A=\begin{pmatrix} 1 & 0 \\ 0 & -1\end{pmatrix}$$ I want to compute $e^{Ax}$. I can see that $A^2=I$ which looks like it should be useful. Using the definition of the matrix exponential $$e^{Ax}=I+Ax+\frac{(Ax)^2}{2!}+ \cdots$$ I find that $$e^{Ax}=I\left(1+\frac{x^2}{2!}+ \cdots\right)+A\left(x+\frac{x^3}{3!}+ \cdots\right)$$ Is there anything I can do from here?,,"['matrices', 'matrix-exponential']"
68,$48$ reasons why a matrix is singular,reasons why a matrix is singular,48,"Currently using the MIT recordings from Prof.Strang to review deduction of the determinant formula . Let's say we take a closer look at $$ \left\vert\begin{array}{c c} a & 0\\ c & 0 \end{array}\right\vert $$ And to quote from the transcript: Why is that determinant nothing, forget him? Well, it has a column of zeros. And by the -- well, so one way to think is, well, it's a singular matrix. Oh, for, for like forty-eight different reasons. That determinant is zero. Now $48$, that's an oddly specific number. And while I do believe he was exaggerating, it did make me curious about just how many reasons (the determinant's being zero not included, of course) there really are for that matrix' being singular. What are they?","Currently using the MIT recordings from Prof.Strang to review deduction of the determinant formula . Let's say we take a closer look at $$ \left\vert\begin{array}{c c} a & 0\\ c & 0 \end{array}\right\vert $$ And to quote from the transcript: Why is that determinant nothing, forget him? Well, it has a column of zeros. And by the -- well, so one way to think is, well, it's a singular matrix. Oh, for, for like forty-eight different reasons. That determinant is zero. Now $48$, that's an oddly specific number. And while I do believe he was exaggerating, it did make me curious about just how many reasons (the determinant's being zero not included, of course) there really are for that matrix' being singular. What are they?",,"['linear-algebra', 'matrices', 'determinant']"
69,Eigenvectors for prime numbers matrices,Eigenvectors for prime numbers matrices,,"I have noticed that eigenvectors for matrices $2\times{2}$ made from $4$ consecutive big prime numbers, for example $\begin{bmatrix} 100003 & 100019 \\ 100043 &100049 \\  \end{bmatrix}$, have ""always"" approximate eigenvectors  presented below in the matrix $R=\begin{bmatrix}   v_1 & v_2 \\  \end{bmatrix} = \begin{bmatrix} \dfrac{\sqrt{2}}{2}  &-\dfrac{\sqrt{2}}{2} \\ \dfrac{\sqrt{2}}{2} & \dfrac{\sqrt{2}}{2}\\  \end{bmatrix} = Rotation(\pi/4) $ I suppose it's  not only characteristic for prime numbers but also for any big numbers with relatively small differences between consecutive numbers, but .... how to prove that it holds just for 4 consecutive big prime numbers? What are consequences of these approximate eigenvector forms? P.S. Please notice dear reader that I'm not asking whether property I have presented is valid exclusively for primes numbers (big ones), but ..  whether it is valid also for big prime numbers in any situation what requires however a little analysis of prime numbers properties. For example to use Legendre conjecture .","I have noticed that eigenvectors for matrices $2\times{2}$ made from $4$ consecutive big prime numbers, for example $\begin{bmatrix} 100003 & 100019 \\ 100043 &100049 \\  \end{bmatrix}$, have ""always"" approximate eigenvectors  presented below in the matrix $R=\begin{bmatrix}   v_1 & v_2 \\  \end{bmatrix} = \begin{bmatrix} \dfrac{\sqrt{2}}{2}  &-\dfrac{\sqrt{2}}{2} \\ \dfrac{\sqrt{2}}{2} & \dfrac{\sqrt{2}}{2}\\  \end{bmatrix} = Rotation(\pi/4) $ I suppose it's  not only characteristic for prime numbers but also for any big numbers with relatively small differences between consecutive numbers, but .... how to prove that it holds just for 4 consecutive big prime numbers? What are consequences of these approximate eigenvector forms? P.S. Please notice dear reader that I'm not asking whether property I have presented is valid exclusively for primes numbers (big ones), but ..  whether it is valid also for big prime numbers in any situation what requires however a little analysis of prime numbers properties. For example to use Legendre conjecture .",,"['linear-algebra', 'matrices', 'number-theory', 'prime-numbers']"
70,Understanding matrix multiplication [duplicate],Understanding matrix multiplication [duplicate],,"This question already has answers here : Matrix multiplication: interpreting and understanding the process (2 answers) Closed 8 years ago . I have a hard time understanding, on an intuitive level, what matrix multiplication actually does. I have used it a lot, but I do not really know what it does. I know that $Ax = y$, where $A$ is a matrix and $x$ is an $n$-tuple, is just another way writing a system of equations. Seeing it this way, matrix addition is quite intuitive. Since every elementary matrix can be seen as representing an elementary row operation, I can understand what matrix multiplication does with invertible matrices. However, that still does not explain anything when we are working with matrices that are not invertible. What is a good way of thinking about matrix multiplication?","This question already has answers here : Matrix multiplication: interpreting and understanding the process (2 answers) Closed 8 years ago . I have a hard time understanding, on an intuitive level, what matrix multiplication actually does. I have used it a lot, but I do not really know what it does. I know that $Ax = y$, where $A$ is a matrix and $x$ is an $n$-tuple, is just another way writing a system of equations. Seeing it this way, matrix addition is quite intuitive. Since every elementary matrix can be seen as representing an elementary row operation, I can understand what matrix multiplication does with invertible matrices. However, that still does not explain anything when we are working with matrices that are not invertible. What is a good way of thinking about matrix multiplication?",,"['linear-algebra', 'matrices']"
71,"If $A$ is nilpotent, why is $A$ non-invertible?","If  is nilpotent, why is  non-invertible?",A A,Here $A$ is a square matrix. I'm confused as to why $A$ would be non-invertible as well.,Here $A$ is a square matrix. I'm confused as to why $A$ would be non-invertible as well.,,"['linear-algebra', 'matrices', 'nilpotence']"
72,A projection $P$ is orthogonal if and only if its spectral norm is 1 [closed],A projection  is orthogonal if and only if its spectral norm is 1 [closed],P,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question I have to show what the title says. A projection $P$ is orthogonal if and only if its spectral norm is $1$. I suppose I have to use the following identity: $$\|P\|_{2}=(\lambda_{\max}(P^*P))^{\frac{1}{2}}.$$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question I have to show what the title says. A projection $P$ is orthogonal if and only if its spectral norm is $1$. I suppose I have to use the following identity: $$\|P\|_{2}=(\lambda_{\max}(P^*P))^{\frac{1}{2}}.$$",,"['matrices', 'analysis', 'numerical-linear-algebra']"
73,Find the axis of rotation of a rotation matrix by inspection (NOT by solving $Kv=v$),Find the axis of rotation of a rotation matrix by inspection (NOT by solving ),Kv=v,"$$K=\ \begin{pmatrix} 0 & 0 & 1\\  -1 & 0 & 0\\  0 & -1 & 0 \end{pmatrix}$$ Find the axis of rotation for the rotation matrix $K$ by INSPECTION . This is from my other thread click here to view it Everything you see below is me finding the axis of rotation by solving $Kv=v$ . Just to show you how much working it requires: Noting that the axis of rotation consists of vectors that remain unmoved. That is a vector $v$ satisfying $Kv = v$ . Or, $Kv - Iv=0$ where $I$ is the $3\times3$ identity matrix. For matrix $K$ after solving the homogeneous equations given by $(K-I)v=0$ and showing the working: $(K-I)v=0$ So $$K-I=\ \begin{pmatrix} 0 & 0 & 1\\  -1 & 0 & 0\\  0 & -1 & 0 \end{pmatrix}-\begin{pmatrix} 1 & 0 & 0\\  0 & 1 & 0\\  0 & 0 & 1 \end{pmatrix}=\ \begin{pmatrix} -1 & 0 & 1\\  -1 & -1 & 0\\  0 & -1 & -1 \end{pmatrix}$$ therefore $$\begin{pmatrix} -1 & 0 & 1\\  -1 & -1 & 0\\  0 & -1 & -1 \end{pmatrix}v=0$$ writing out the components for $v$ gives $$\begin{pmatrix} -1 & 0 & 1\\  -1 & -1 & 0\\  0 & -1 & -1 \end{pmatrix}\begin{pmatrix} x \\  y \\  z  \end{pmatrix}=0$$ Multiplying out gives three equations $-x+z=0$ $-x-y=0$ $-y-z=0$ Since $$ v=\begin{bmatrix}x\\y\\z\end{bmatrix} $$ Here's the solution parametrically in terms of $x$ \begin{align*} z&= x\\ y&=-x\\ x&=x \end{align*} Hence the axis of rotation is given by the line $$ \begin{bmatrix} x\\-x\\x \end{bmatrix}=x\begin{bmatrix}1\\-1\\1\end{bmatrix}\quad x\in\Bbb R $$ That is, the axis of rotation is $$ \operatorname{Span}\left\{\begin{bmatrix}1\\-1\\1\end{bmatrix}\right\} $$ As you can see this was a lot of work so i would be so grateful if someone could please explain in simple english how to get the answer: $$ \operatorname{Span}\left\{\begin{bmatrix}1\\-1\\1\end{bmatrix}\right\} $$ by using Inspection? Many thanks to all that helped so far particularly Brian Fitzpatrick in the last thread","Find the axis of rotation for the rotation matrix by INSPECTION . This is from my other thread click here to view it Everything you see below is me finding the axis of rotation by solving . Just to show you how much working it requires: Noting that the axis of rotation consists of vectors that remain unmoved. That is a vector satisfying . Or, where is the identity matrix. For matrix after solving the homogeneous equations given by and showing the working: So therefore writing out the components for gives Multiplying out gives three equations Since Here's the solution parametrically in terms of Hence the axis of rotation is given by the line That is, the axis of rotation is As you can see this was a lot of work so i would be so grateful if someone could please explain in simple english how to get the answer: by using Inspection? Many thanks to all that helped so far particularly Brian Fitzpatrick in the last thread","K=\
\begin{pmatrix}
0 & 0 & 1\\ 
-1 & 0 & 0\\ 
0 & -1 & 0
\end{pmatrix} K Kv=v v Kv = v Kv - Iv=0 I 3\times3 K (K-I)v=0 (K-I)v=0 K-I=\
\begin{pmatrix}
0 & 0 & 1\\ 
-1 & 0 & 0\\ 
0 & -1 & 0
\end{pmatrix}-\begin{pmatrix}
1 & 0 & 0\\ 
0 & 1 & 0\\ 
0 & 0 & 1
\end{pmatrix}=\
\begin{pmatrix}
-1 & 0 & 1\\ 
-1 & -1 & 0\\ 
0 & -1 & -1
\end{pmatrix} \begin{pmatrix}
-1 & 0 & 1\\ 
-1 & -1 & 0\\ 
0 & -1 & -1
\end{pmatrix}v=0 v \begin{pmatrix}
-1 & 0 & 1\\ 
-1 & -1 & 0\\ 
0 & -1 & -1
\end{pmatrix}\begin{pmatrix}
x \\ 
y \\ 
z 
\end{pmatrix}=0 -x+z=0 -x-y=0 -y-z=0 
v=\begin{bmatrix}x\\y\\z\end{bmatrix}
 x \begin{align*}
z&= x\\
y&=-x\\
x&=x
\end{align*} 
\begin{bmatrix}
x\\-x\\x
\end{bmatrix}=x\begin{bmatrix}1\\-1\\1\end{bmatrix}\quad x\in\Bbb R
 
\operatorname{Span}\left\{\begin{bmatrix}1\\-1\\1\end{bmatrix}\right\}
 
\operatorname{Span}\left\{\begin{bmatrix}1\\-1\\1\end{bmatrix}\right\}
","['linear-algebra', 'matrices']"
74,"Suppose $AB=I$, where A has full rank. Is $BA=I$?","Suppose , where A has full rank. Is ?",AB=I BA=I,"We know that the title statement is correct for square matrices $A,B$ . What about non-square matrices with full rank?",We know that the title statement is correct for square matrices . What about non-square matrices with full rank?,"A,B","['linear-algebra', 'matrices']"
75,"Do matrices have a ""to the power of"" operator?","Do matrices have a ""to the power of"" operator?",,"Well I was sure that saying ""$A^3$""  (where $A$ is an $n\times n$ matrix) is nonsense. Sure one could do $(A\cdot A) A$  But that contains different operators etc. So what did my prof mean by the following statement: show that $A^{25}\mathbf{x} = \mathbf{0}$ has only the trivial solution? (We're also given the determinant of A). I know the proof will probably end with stating: ""This means that $A^{25}$ is invertible, so $A^{25}\mathbf{x} = \mathbf{0}$ has only the trivial solution. And well I could state that $\det(A^{25}) = 5^{25} \neq 0$. But then again: I really wonder what the ""to the power of"" operator means? Or did my prof make a mistake here?","Well I was sure that saying ""$A^3$""  (where $A$ is an $n\times n$ matrix) is nonsense. Sure one could do $(A\cdot A) A$  But that contains different operators etc. So what did my prof mean by the following statement: show that $A^{25}\mathbf{x} = \mathbf{0}$ has only the trivial solution? (We're also given the determinant of A). I know the proof will probably end with stating: ""This means that $A^{25}$ is invertible, so $A^{25}\mathbf{x} = \mathbf{0}$ has only the trivial solution. And well I could state that $\det(A^{25}) = 5^{25} \neq 0$. But then again: I really wonder what the ""to the power of"" operator means? Or did my prof make a mistake here?",,"['linear-algebra', 'matrices', 'determinant', 'exponentiation']"
76,Matrix multiplied by itself n times equals identity,Matrix multiplied by itself n times equals identity,,"Given a $A \in \mathbb{R}^{100\times 100}$ and $A^{6} = I_{100}$ and $A^{14} = I_{100}$, ist say $A^{2} = I_{100}$ too? And what would then be the relationship governing whether $A^{2} = I_{100}$ or even $A^{n}$  is valid given two different conditions as above?","Given a $A \in \mathbb{R}^{100\times 100}$ and $A^{6} = I_{100}$ and $A^{14} = I_{100}$, ist say $A^{2} = I_{100}$ too? And what would then be the relationship governing whether $A^{2} = I_{100}$ or even $A^{n}$  is valid given two different conditions as above?",,"['linear-algebra', 'matrices']"
77,Can all Hermitian matrices $H$ be written as $H=A^* A$?,Can all Hermitian matrices  be written as ?,H H=A^* A,"All the matrices below are square, complex matrices. 1) Is it true that, for every Hermitian matrix $H$, there exists $A$, that $A^*A=H$? 2) For any $A$, does $A^*A$ always have a square root? If it's not, is there any simple presumption of $A$ that makes $A^*A$ always have a square root?","All the matrices below are square, complex matrices. 1) Is it true that, for every Hermitian matrix $H$, there exists $A$, that $A^*A=H$? 2) For any $A$, does $A^*A$ always have a square root? If it's not, is there any simple presumption of $A$ that makes $A^*A$ always have a square root?",,"['linear-algebra', 'matrices']"
78,Show that matrices are not similar,Show that matrices are not similar,,"I have to show that the following matrices are not similar: $$A = \left[\begin{matrix} 1 & 3 & -3 \\ -3 & 7 & -3 \\ -6 & 6 & -2\end{matrix}\right]$$ and $$A' = \left[\begin{matrix} 5 & 0 & 0 \\ 0 & 4 & 0 \\ 0 & 0 & 3\end{matrix}\right]$$ I know that 2 matrices $A$ and $A'$ are similar if there exists an invertible matrix $B$, such that $$A'= B^{-1}AB$$ According to Wikipedia , similar matrices share some properties (for example they have the same eigen values), but I don't know how to start, since I have missed the last lectures of my linear algebra course, unfortunetely. Unfortuntely, life has not been so completely fair with me :( Should I just check if they have the same eigen values?","I have to show that the following matrices are not similar: $$A = \left[\begin{matrix} 1 & 3 & -3 \\ -3 & 7 & -3 \\ -6 & 6 & -2\end{matrix}\right]$$ and $$A' = \left[\begin{matrix} 5 & 0 & 0 \\ 0 & 4 & 0 \\ 0 & 0 & 3\end{matrix}\right]$$ I know that 2 matrices $A$ and $A'$ are similar if there exists an invertible matrix $B$, such that $$A'= B^{-1}AB$$ According to Wikipedia , similar matrices share some properties (for example they have the same eigen values), but I don't know how to start, since I have missed the last lectures of my linear algebra course, unfortunetely. Unfortuntely, life has not been so completely fair with me :( Should I just check if they have the same eigen values?",,['linear-algebra']
79,"Prove that for every vector $V$, $||V||_{\infty} \leq ||V||_2 \leq || V||_1$","Prove that for every vector ,",V ||V||_{\infty} \leq ||V||_2 \leq || V||_1,"$\newcommand{\inf}{||V||_\infty}$ $\newcommand{\two}{||V||_2}$ $\newcommand{\one}{||V||_1}$ Prove that for every vector $V$, $\inf \leq \two \leq \one$ I have tried to look online for a solution to this question, but I only have figured out the easy part of it. I know by definition: $\inf = \text{max}|x_i|$ $\two = \displaystyle(\sum_{i=1}^n(x_i^2))^\frac{1}{2}$ $\one = \displaystyle\sum_{i=1}^n|x_i|$ Now, to me, it's obvious why $\inf \leq \one$. If the $\inf$ is the single maximum entry in the vector, and the $\one$ is the sum of all entries in the vector, it's clear that $\one$ contains $\inf$ in its sum. I suppose this logic would hold for why $\inf \leq \two$ also. I'm having a hard time seeing why $\two \leq \one$.","$\newcommand{\inf}{||V||_\infty}$ $\newcommand{\two}{||V||_2}$ $\newcommand{\one}{||V||_1}$ Prove that for every vector $V$, $\inf \leq \two \leq \one$ I have tried to look online for a solution to this question, but I only have figured out the easy part of it. I know by definition: $\inf = \text{max}|x_i|$ $\two = \displaystyle(\sum_{i=1}^n(x_i^2))^\frac{1}{2}$ $\one = \displaystyle\sum_{i=1}^n|x_i|$ Now, to me, it's obvious why $\inf \leq \one$. If the $\inf$ is the single maximum entry in the vector, and the $\one$ is the sum of all entries in the vector, it's clear that $\one$ contains $\inf$ in its sum. I suppose this logic would hold for why $\inf \leq \two$ also. I'm having a hard time seeing why $\two \leq \one$.",,"['matrices', 'vector-spaces', 'numerical-methods']"
80,Find all square roots of this matrix using some practical method,Find all square roots of this matrix using some practical method,,Find all matrices $A$ such that $$A=\displaystyle{\sqrt{\pmatrix{1 & 3 & -3 \\ 0 & 4 & 5 \\ 0 & 0 & 9}}} $$ This is a UC Berkeley qualifying exam question. Now I know one method to solve this. Let $$A=\pmatrix{a & b & c \\ d & e & f \\ g & h & i} $$ Now on squaring we'll get $9$ equations and can determine all variables. I would like to know that whether there is any practical approach for this question. My method is impractical and can be used only in theory. Any help is greatly appreciated.,Find all matrices such that This is a UC Berkeley qualifying exam question. Now I know one method to solve this. Let Now on squaring we'll get equations and can determine all variables. I would like to know that whether there is any practical approach for this question. My method is impractical and can be used only in theory. Any help is greatly appreciated.,"A A=\displaystyle{\sqrt{\pmatrix{1 & 3 & -3 \\ 0 & 4 & 5 \\ 0 & 0 & 9}}}
 A=\pmatrix{a & b & c \\ d & e & f \\ g & h & i}
 9","['linear-algebra', 'matrices']"
81,How to find out whether a matrix to the $4$th power is the identity matrix?,How to find out whether a matrix to the th power is the identity matrix?,4,"Last semester, I was following a Linear Algebra course and in the exam of that course, the following question was asked: Of the following 5 matrices A, how many of them satisfy $A^4 = I$ ? $$\begin{bmatrix}1&0\\0&-1\end{bmatrix}, \begin{bmatrix}\frac{\sqrt2}{2}&\frac{\sqrt2}{2}\\-\frac{\sqrt2}{2}&\frac{\sqrt2}{2}\end{bmatrix}, \begin{bmatrix}\frac{\sqrt2}{2}&\frac{\sqrt2}{2}\\-\frac{\sqrt2}{2}&-\frac{\sqrt2}{2}\end{bmatrix}, \begin{bmatrix}\frac{1}{2}&-\frac{\sqrt3}{2}\\\frac{\sqrt3}{2}&\frac{1}{2}\end{bmatrix}, \begin{bmatrix}0&-1\\1&0\end{bmatrix} $$ I answered this question by doing all the calculations to the power of 4, and as you would expect, this took way too much time but I eventually figured out the correct answer which is 2 of them, which are the following matrices: $$\begin{bmatrix}1&0\\0&-1\end{bmatrix} and \begin{bmatrix}0&-1\\1&0\end{bmatrix} $$ I'm taking the exam again and was wondering if there's a different and easier way to solve such a question?","Last semester, I was following a Linear Algebra course and in the exam of that course, the following question was asked: Of the following 5 matrices A, how many of them satisfy $A^4 = I$ ? $$\begin{bmatrix}1&0\\0&-1\end{bmatrix}, \begin{bmatrix}\frac{\sqrt2}{2}&\frac{\sqrt2}{2}\\-\frac{\sqrt2}{2}&\frac{\sqrt2}{2}\end{bmatrix}, \begin{bmatrix}\frac{\sqrt2}{2}&\frac{\sqrt2}{2}\\-\frac{\sqrt2}{2}&-\frac{\sqrt2}{2}\end{bmatrix}, \begin{bmatrix}\frac{1}{2}&-\frac{\sqrt3}{2}\\\frac{\sqrt3}{2}&\frac{1}{2}\end{bmatrix}, \begin{bmatrix}0&-1\\1&0\end{bmatrix} $$ I answered this question by doing all the calculations to the power of 4, and as you would expect, this took way too much time but I eventually figured out the correct answer which is 2 of them, which are the following matrices: $$\begin{bmatrix}1&0\\0&-1\end{bmatrix} and \begin{bmatrix}0&-1\\1&0\end{bmatrix} $$ I'm taking the exam again and was wondering if there's a different and easier way to solve such a question?",,['linear-algebra']
82,When is the Frobenius norm bounded by the nuclear norm?,When is the Frobenius norm bounded by the nuclear norm?,,"I am reading the Recht (2011) paper titled,  ""A Simpler Approach to Matrix Completion"", and I cannot figure out the last inequality of the last line on page 3422 (page 10 of the document). The intermediate step seems to be $$\| M \|_I -\frac{1}{2} \|\mathscr P_{T^\perp}(Z)\|_F + \frac{1}{2} \|\mathscr P_{T^\perp}(Z)\|_*  \geq \|M \|_* $$ where $\| \cdot \|_F$ refers to the Frobenius norm and $\| \cdot \|_*$  refers to the nuclear norm. This would be possible if $\|\mathscr P_{T^\perp}(Z)\|_* \geq \|\mathscr P_{T^\perp}(Z)\|_F $, but I do not think the nuclear norm is an upper bound for the Frobenius norm in general.  I believe it is true if the matrix has spectral norm of 1, but I don't think that is necessarily the case here. What is the relationship between the Frobenius norm and the nuclear norm that I am missing to explain this last step?","I am reading the Recht (2011) paper titled,  ""A Simpler Approach to Matrix Completion"", and I cannot figure out the last inequality of the last line on page 3422 (page 10 of the document). The intermediate step seems to be $$\| M \|_I -\frac{1}{2} \|\mathscr P_{T^\perp}(Z)\|_F + \frac{1}{2} \|\mathscr P_{T^\perp}(Z)\|_*  \geq \|M \|_* $$ where $\| \cdot \|_F$ refers to the Frobenius norm and $\| \cdot \|_*$  refers to the nuclear norm. This would be possible if $\|\mathscr P_{T^\perp}(Z)\|_* \geq \|\mathscr P_{T^\perp}(Z)\|_F $, but I do not think the nuclear norm is an upper bound for the Frobenius norm in general.  I believe it is true if the matrix has spectral norm of 1, but I don't think that is necessarily the case here. What is the relationship between the Frobenius norm and the nuclear norm that I am missing to explain this last step?",,"['matrices', 'matrix-norms', 'nuclear-norm']"
83,Finding $n$th power of a $3\times 3$ matrix,Finding th power of a  matrix,n 3\times 3,Find the $A^n$ if $$A=\begin{bmatrix}1 & a & b \\0 & 1 &a\\0 &0 &1\end{bmatrix}$$  I tried inductive method to show $$A^n=\begin{bmatrix}1 & na & nb+\frac{n(n-1)}{2}a^2 \\0 & 1 &na\\0 &0 &1\end{bmatrix}$$  now : My question is : Is there other method (idea ) to find $A^n$ ? Thanks in advance. Can the idea apply for $$A=\begin{bmatrix}1 & a & b \\0 & 1 &c\\0 &0 &1\end{bmatrix}$$ when $c \neq a$ ?,Find the $A^n$ if $$A=\begin{bmatrix}1 & a & b \\0 & 1 &a\\0 &0 &1\end{bmatrix}$$  I tried inductive method to show $$A^n=\begin{bmatrix}1 & na & nb+\frac{n(n-1)}{2}a^2 \\0 & 1 &na\\0 &0 &1\end{bmatrix}$$  now : My question is : Is there other method (idea ) to find $A^n$ ? Thanks in advance. Can the idea apply for $$A=\begin{bmatrix}1 & a & b \\0 & 1 &c\\0 &0 &1\end{bmatrix}$$ when $c \neq a$ ?,,"['linear-algebra', 'matrices', 'matrix-equations', 'matrix-calculus']"
84,Prove if $B^2=I+BA$ and $A^2=AB$ then $A=0$,Prove if  and  then,B^2=I+BA A^2=AB A=0,"I need to prove that if $B^2=I+BA$ and $A^2=AB$ then $A=0$, $A$ and $B$ are square matrices. I'm not sure if my answer is correct but I thought of this: $$ A^2-B^2=A^2+AB-BA-B^2=A^2+A^2-(B^2-I)-B^2=2(A^2-B^2)+I  $$ $$ \Rightarrow B^2-A^2=I=(B-A)(B+A) $$ This means that $(B-A)$ is invertible. It is also given that $I=B^2-AB$ then: $$ B^2-AB=B^2-A^2 $$ $$ \Leftrightarrow B(B-A)=(B+A)(B-A) $$ Because we proved that $(B-A)$ is invertible then we can simplify and get $B=B+A \Rightarrow A=0$","I need to prove that if $B^2=I+BA$ and $A^2=AB$ then $A=0$, $A$ and $B$ are square matrices. I'm not sure if my answer is correct but I thought of this: $$ A^2-B^2=A^2+AB-BA-B^2=A^2+A^2-(B^2-I)-B^2=2(A^2-B^2)+I  $$ $$ \Rightarrow B^2-A^2=I=(B-A)(B+A) $$ This means that $(B-A)$ is invertible. It is also given that $I=B^2-AB$ then: $$ B^2-AB=B^2-A^2 $$ $$ \Leftrightarrow B(B-A)=(B+A)(B-A) $$ Because we proved that $(B-A)$ is invertible then we can simplify and get $B=B+A \Rightarrow A=0$",,"['linear-algebra', 'matrices', 'proof-verification']"
85,"Different ways of seeing how $\text{SL}(2, \mathbb{R})$ is not simply connected?",Different ways of seeing how  is not simply connected?,"\text{SL}(2, \mathbb{R})","As the question title suggests, could anybody sketch some of the different ways of seeing how the group $\text{SL}(2, \mathbb{R})$ is not simply connected?","As the question title suggests, could anybody sketch some of the different ways of seeing how the group $\text{SL}(2, \mathbb{R})$ is not simply connected?",,"['real-analysis', 'matrices']"
86,Consider the trace map $M_n (\mathbb{R}) \to \mathbb{R}$. What is its kernel?,Consider the trace map . What is its kernel?,M_n (\mathbb{R}) \to \mathbb{R},"The map is the trace map. I.e, it takes any $n$ by $n$ matrix and associates to that matrix, a number of the form $\mathrm{Tr}(A) = \sum_{i=1}^n a_{ii}$, where $A \in M_n (\mathbb{R})$. I need to find the kernel of this map, give a basis and its dimension (which is easy once I have the basis. I also am asked if the trace is surjective (and to prove this), and to describe the elements in $M_n(\mathbb{R}) / Ker \,(\mathrm{Tr})$. Thanks in advance for any help I recieve. I love this community :) I will probably post many questions in regards to linear algebra this semester. EDIT. I know that for any real number, we can associate to it, a matrix whose trace is that number. (all we have to do is let the other elements on the diagonal be $0$). So I suppose the surjectivity isn't an issue here. However, finding the kernel and basis for the kernel seems tough for me. There are an awful lot of matrices whose trace is precisely $0$.","The map is the trace map. I.e, it takes any $n$ by $n$ matrix and associates to that matrix, a number of the form $\mathrm{Tr}(A) = \sum_{i=1}^n a_{ii}$, where $A \in M_n (\mathbb{R})$. I need to find the kernel of this map, give a basis and its dimension (which is easy once I have the basis. I also am asked if the trace is surjective (and to prove this), and to describe the elements in $M_n(\mathbb{R}) / Ker \,(\mathrm{Tr})$. Thanks in advance for any help I recieve. I love this community :) I will probably post many questions in regards to linear algebra this semester. EDIT. I know that for any real number, we can associate to it, a matrix whose trace is that number. (all we have to do is let the other elements on the diagonal be $0$). So I suppose the surjectivity isn't an issue here. However, finding the kernel and basis for the kernel seems tough for me. There are an awful lot of matrices whose trace is precisely $0$.",,"['linear-algebra', 'matrices', 'transformation', 'trace', 'matrix-calculus']"
87,How prove this matrix inequality $\det(B)>0$,How prove this matrix inequality,\det(B)>0,"Let $A=(a_{ij})_{n\times n}$ such $a_{ij}>0$ and $\det(A)>0$. Defining the matrix $B:=(a_{ij}^{\frac{1}{n}})$, show that $\det(B)>0?$. This problem is from my friend, and I have considered sometimes, but I can't. Thank you","Let $A=(a_{ij})_{n\times n}$ such $a_{ij}>0$ and $\det(A)>0$. Defining the matrix $B:=(a_{ij}^{\frac{1}{n}})$, show that $\det(B)>0?$. This problem is from my friend, and I have considered sometimes, but I can't. Thank you",,"['linear-algebra', 'matrices', 'inequality', 'determinant']"
88,Minimize $\|A-XB\|_F$ subject to $Xv=0$,Minimize  subject to,\|A-XB\|_F Xv=0,"Assume we are given two matrices $A, B \in \mathbb R^{n \times m}$ and a vector $v \in \mathbb R^n$. Let $\|\cdot\|_F$ be the Frobenius norm of a matrix. How can we solve the following optimization problem in $X \in \mathbb R^{n \times n}$? $$\begin{array}{ll} \text{minimize} & \|A-XB\|_F\\ \text{subject to} & Xv=0\end{array}$$ Can this problem be converted to a constrained least squares problem with the optimization variable being a vector instead of a matrix? If so, does this way work? Are there some references about solving such constrained linear least Frobenius norm problems? Thanks!","Assume we are given two matrices $A, B \in \mathbb R^{n \times m}$ and a vector $v \in \mathbb R^n$. Let $\|\cdot\|_F$ be the Frobenius norm of a matrix. How can we solve the following optimization problem in $X \in \mathbb R^{n \times n}$? $$\begin{array}{ll} \text{minimize} & \|A-XB\|_F\\ \text{subject to} & Xv=0\end{array}$$ Can this problem be converted to a constrained least squares problem with the optimization variable being a vector instead of a matrix? If so, does this way work? Are there some references about solving such constrained linear least Frobenius norm problems? Thanks!",,"['matrices', 'reference-request', 'convex-optimization', 'least-squares', 'quadratic-programming']"
89,Perron-Frobenius theorem,Perron-Frobenius theorem,,"In the proof of the Perron-Frobenius theorem why can we take a strictly positive eigenvector corresponding to the eigenvalue $1$? Before that, why can we even take a non-negative eigenvector? Books simply take such a vector, no explanation whatsoever. Pisses me off. Wikipedia only proves it assuming the matrix is irreducible. Any help? Thanks. EDIT: Nevermind, got it. Delete this question, please.","In the proof of the Perron-Frobenius theorem why can we take a strictly positive eigenvector corresponding to the eigenvalue $1$? Before that, why can we even take a non-negative eigenvector? Books simply take such a vector, no explanation whatsoever. Pisses me off. Wikipedia only proves it assuming the matrix is irreducible. Any help? Thanks. EDIT: Nevermind, got it. Delete this question, please.",,"['linear-algebra', 'matrices', 'markov-chains']"
90,"How do I prove that if $\det(A) < 0$, then $A\in\mathbb R^{2\times 2}$ is a diagonalizable matrix?","How do I prove that if , then  is a diagonalizable matrix?",\det(A) < 0 A\in\mathbb R^{2\times 2},"Suppose $A$ is a $2\times2$ matrix. How do I prove that, if $\det(A) < 0$, then $A$ is a diagonalizable matrix over $\mathbb{R}$?","Suppose $A$ is a $2\times2$ matrix. How do I prove that, if $\det(A) < 0$, then $A$ is a diagonalizable matrix over $\mathbb{R}$?",,"['linear-algebra', 'matrices']"
91,How to derive a certain determinantal identity?,How to derive a certain determinantal identity?,,"Does anybody know how to derive $$\det(\mathbf A)\cdot \det(\mathbf D + \mathbf E \cdot \mathbf A^{-1} \cdot \mathbf B) = \det(\mathbf D)\cdot \det(\mathbf A + \mathbf B \cdot \mathbf D^{-1} \cdot \mathbf E)$$ where $\mathbf A$ , $\mathbf B$ , $\mathbf C$ , $\mathbf D$ , $\mathbf E$ are non-singular matrices? Most likely, it requires the Sylvester determinant theorem).","Does anybody know how to derive where , , , , are non-singular matrices? Most likely, it requires the Sylvester determinant theorem).",\det(\mathbf A)\cdot \det(\mathbf D + \mathbf E \cdot \mathbf A^{-1} \cdot \mathbf B) = \det(\mathbf D)\cdot \det(\mathbf A + \mathbf B \cdot \mathbf D^{-1} \cdot \mathbf E) \mathbf A \mathbf B \mathbf C \mathbf D \mathbf E,"['linear-algebra', 'matrices', 'determinant']"
92,"$A$ is an $n$ by $n$ matrix over $\mathbb C$ such that $Rank(A)=1$ and $Tr(A)=0$, prove that $A^2=0$","is an  by  matrix over  such that  and , prove that",A n n \mathbb C Rank(A)=1 Tr(A)=0 A^2=0,"This is what I did: Because $\mbox{rank}(A)=1$ , then from rank nullity theorem $$\dim\ker A + \mbox{rank} A = n \implies \dim \ker A = n-1$$ and $gm(0)=dimE(0)=dimKerA=n-1$ where $E(0)$ is the eigenspace of the eigenvalue $0$ . So the characteristic polynomial of $A$ is: $C_A(x)=x^{n-1}(x-\lambda)$ where $\lambda$ could be $0$ . It is known that the sum of all eigenvalues of A equals the trace of A (Shown this using the fact that all matrices under $\mathbb C$ are triangularizable), so from that we'll conclude  that $\lambda=0$ and $C_A(x)=x^n$ From here the minimal polynomial of A is $M_A(x)=x^k$ where $k\leq n$ From Cayley–Hamilton $M_A(A)=0 \implies A^k=0$ I'm not sure how to progress from here, I don't really know what I can say about the index of a nilpotent matrix when I know its rank. And I don't think I can say anything about the minimal polynomial given that I know the geometric multiplicity of the only eigenvalue","This is what I did: Because , then from rank nullity theorem and where is the eigenspace of the eigenvalue . So the characteristic polynomial of is: where could be . It is known that the sum of all eigenvalues of A equals the trace of A (Shown this using the fact that all matrices under are triangularizable), so from that we'll conclude  that and From here the minimal polynomial of A is where From Cayley–Hamilton I'm not sure how to progress from here, I don't really know what I can say about the index of a nilpotent matrix when I know its rank. And I don't think I can say anything about the minimal polynomial given that I know the geometric multiplicity of the only eigenvalue",\mbox{rank}(A)=1 \dim\ker A + \mbox{rank} A = n \implies \dim \ker A = n-1 gm(0)=dimE(0)=dimKerA=n-1 E(0) 0 A C_A(x)=x^{n-1}(x-\lambda) \lambda 0 \mathbb C \lambda=0 C_A(x)=x^n M_A(x)=x^k k\leq n M_A(A)=0 \implies A^k=0,"['linear-algebra', 'matrices', 'matrix-rank', 'nilpotence']"
93,"If $A$, $B$ idempotent and $AB=0$, then $A+B$ idempotent.","If ,  idempotent and , then  idempotent.",A B AB=0 A+B,"We know that if $A,B$ idempotent then we have (see edit) $$(A+B)^2=A+B\implies AB=0,$$ and I'm wondering whether the converse, i.e. that if $A,B$ idempotent and $AB=0$ then $A+B$ idempotent is true. Expanding we get $$(A+B)^2=A^2+AB+BA+B^2=A+B+BA$$ so we just need to show that $BA=0$ . It certainly isn't true in general that $AB=0\implies BA=0$ , but I couldn't think of an example where $A$ and $B$ are idempotent. For orthogonal projections at least I think that the statement is true (by thinking geometrically), but I need to consider general projections. My intuition is that this statement probably is true. Is this correct, and if so how can I show this? Edit: $(A+B)^2=A+B\implies AB=-BA$ , but then $BAB=-BA$ and $AB=-BAB$ , so $AB=BA$ and therefore $AB=BA=0$ .","We know that if idempotent then we have (see edit) and I'm wondering whether the converse, i.e. that if idempotent and then idempotent is true. Expanding we get so we just need to show that . It certainly isn't true in general that , but I couldn't think of an example where and are idempotent. For orthogonal projections at least I think that the statement is true (by thinking geometrically), but I need to consider general projections. My intuition is that this statement probably is true. Is this correct, and if so how can I show this? Edit: , but then and , so and therefore .","A,B (A+B)^2=A+B\implies AB=0, A,B AB=0 A+B (A+B)^2=A^2+AB+BA+B^2=A+B+BA BA=0 AB=0\implies BA=0 A B (A+B)^2=A+B\implies AB=-BA BAB=-BA AB=-BAB AB=BA AB=BA=0","['linear-algebra', 'matrices', 'idempotents']"
94,Showing that the limit of non-eigenvector goes to infinity,Showing that the limit of non-eigenvector goes to infinity,,"Let $A$ be a $3$ by $3$ real matrix with the triple eigenvalue $1$ . Also, further suppose its eigenspace corresponding to $1$ is only of dimension $1$ . Thus, we can find a basis of $\mathbb{R}^3$ , denoted by $v$ . $w_1$ . $w_2$ where $v$ is an eigenvector of $A$ . Then I have to show that $\lim_{n \to \infty} \|A^n w_1\|=\lim_{n \to \infty} \|A^n w_2\|=\infty$ . How is this possible? I do not have any idea how the norm goes to infinity...Could anyone please help me?","Let be a by real matrix with the triple eigenvalue . Also, further suppose its eigenspace corresponding to is only of dimension . Thus, we can find a basis of , denoted by . . where is an eigenvector of . Then I have to show that . How is this possible? I do not have any idea how the norm goes to infinity...Could anyone please help me?",A 3 3 1 1 1 \mathbb{R}^3 v w_1 w_2 v A \lim_{n \to \infty} \|A^n w_1\|=\lim_{n \to \infty} \|A^n w_2\|=\infty,"['linear-algebra', 'matrices']"
95,Number of matrices with zero determinant,Number of matrices with zero determinant,,"I want to count matrices over finite field with zero determinant.  For example, the numbet of $2\times2$ matrices over $\Bbb{Z}_4$ with zero determinant is 88 by hand computations.  On the other hand the size of the group $GL(2,4)$ is $180$ and the total number of such $2\times2$ matrices is $256$ .  So something is wrong.  What is my mistake? Revised question:  I am looking for the number of $2×2$ matrices over $\Bbb{Z}_4$ (the ring of integers modulo 4), with zero determinant. By a quick computations, there are $88$ ones. So what is the formula in general?","I want to count matrices over finite field with zero determinant.  For example, the numbet of matrices over with zero determinant is 88 by hand computations.  On the other hand the size of the group is and the total number of such matrices is .  So something is wrong.  What is my mistake? Revised question:  I am looking for the number of matrices over (the ring of integers modulo 4), with zero determinant. By a quick computations, there are ones. So what is the formula in general?","2\times2 \Bbb{Z}_4 GL(2,4) 180 2\times2 256 2×2 \Bbb{Z}_4 88","['linear-algebra', 'matrices', 'determinant', 'finite-fields']"
96,Similarity of matrices based on polynomials,Similarity of matrices based on polynomials,,"For $A,B\in \mathbb{R}^{n,n}$ we know that characteristic polynomilas $p_A(x)=p_B(x)=(x-\lambda_1)(x-\lambda_2)\cdot\ldots\cdot(x-\lambda_n)$ , where $\lambda_i\neq\lambda_j$ for $i\neq j$ . Prove that $A$ and $B$ are similar matrices.","For we know that characteristic polynomilas , where for . Prove that and are similar matrices.","A,B\in \mathbb{R}^{n,n} p_A(x)=p_B(x)=(x-\lambda_1)(x-\lambda_2)\cdot\ldots\cdot(x-\lambda_n) \lambda_i\neq\lambda_j i\neq j A B","['matrices', 'eigenvalues-eigenvectors']"
97,If $A=A^2$ is then $A^T A = A$?,If  is then ?,A=A^2 A^T A = A,"I know that for a matrix $A$ : If $A^TA = A$ then $A=A^2$ but is it if and only if ? I mean: is this true that ""If $A=A^2$ then $A^TA = A$ ""?","I know that for a matrix : If then but is it if and only if ? I mean: is this true that ""If then ""?",A A^TA = A A=A^2 A=A^2 A^TA = A,"['linear-algebra', 'matrices', 'symmetric-matrices', 'transpose']"
98,"What is wrong with this ""proof""? $1$ is always an eigenvalue for $I + A$ ($A$ is nilpotent)?","What is wrong with this ""proof""?  is always an eigenvalue for  ( is nilpotent)?",1 I + A A,"Consider the nilpotent matrix $A$ ($A^k = 0$ for some positive $k$).   It is well known that the only eigenvalue of $A$ is $0$. Then suppose $\lambda$ is any eigenvalue of $I + A$ such that    $(I + A) \mathbf{v} = \lambda \mathbf{v}$ where ($\mathbf{v} \neq \mathbf{0}$). Then $I \mathbf{v} + A \mathbf{v} = \lambda \mathbf{v} \implies   \mathbf{v} + A \mathbf{v} = \lambda \mathbf{v} \implies A \mathbf{v} = (\lambda - 1) \mathbf{v}$ We know that $\lambda - 1 = 0$ because the only eigenvalue of a   nilpotent matrix is $0$. Therefore $\lambda = 1$ This ""proof"" seems to indicate that $1$ is always an eigenvalue for the sum of the identity matrix with any nilpotent matrix, but I believe I have a counterexample that disproves this. I believe my error was in assuming that $I + A$ has eigenvalues -- but I do not know how I could prove/disprove this. If someone could help me see where I've gone wrong I would greatly appreciate it!","Consider the nilpotent matrix $A$ ($A^k = 0$ for some positive $k$).   It is well known that the only eigenvalue of $A$ is $0$. Then suppose $\lambda$ is any eigenvalue of $I + A$ such that    $(I + A) \mathbf{v} = \lambda \mathbf{v}$ where ($\mathbf{v} \neq \mathbf{0}$). Then $I \mathbf{v} + A \mathbf{v} = \lambda \mathbf{v} \implies   \mathbf{v} + A \mathbf{v} = \lambda \mathbf{v} \implies A \mathbf{v} = (\lambda - 1) \mathbf{v}$ We know that $\lambda - 1 = 0$ because the only eigenvalue of a   nilpotent matrix is $0$. Therefore $\lambda = 1$ This ""proof"" seems to indicate that $1$ is always an eigenvalue for the sum of the identity matrix with any nilpotent matrix, but I believe I have a counterexample that disproves this. I believe my error was in assuming that $I + A$ has eigenvalues -- but I do not know how I could prove/disprove this. If someone could help me see where I've gone wrong I would greatly appreciate it!",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'fake-proofs']"
99,Solving for the inverse of a matrix,Solving for the inverse of a matrix,,"I'm currently working on a problem in special relativity and it requires me to find the inverse of a matrix of a very specific form as follows: Given a 4x4 matrix $A$ of the form $$A = I + a\cdot\vec{v}\cdot\vec{v}^T$$  I need to find a matrix $B$ where $$B = I + b\cdot\vec{v}\cdot\vec{v}^T$$ such that $$ AB = I$$ Where  $\vec{v}\in \mathbb{R}^4$, and $I$ denotes the identity matrix. We are looking to find $b$ in terms of $a$. I tried multipliying the two expressions together but I wasn't really sure how to solve for $b$ in a matrix equation or how the outer product between $\vec{v}$ with itself plays a role in this problem.","I'm currently working on a problem in special relativity and it requires me to find the inverse of a matrix of a very specific form as follows: Given a 4x4 matrix $A$ of the form $$A = I + a\cdot\vec{v}\cdot\vec{v}^T$$  I need to find a matrix $B$ where $$B = I + b\cdot\vec{v}\cdot\vec{v}^T$$ such that $$ AB = I$$ Where  $\vec{v}\in \mathbb{R}^4$, and $I$ denotes the identity matrix. We are looking to find $b$ in terms of $a$. I tried multipliying the two expressions together but I wasn't really sure how to solve for $b$ in a matrix equation or how the outer product between $\vec{v}$ with itself plays a role in this problem.",,"['linear-algebra', 'matrices']"
