,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Change the integration order of $\int^2_{-6} \int^{2-x}_{\frac{x^2}{4}- 1}f(x, y) \, dy\,dx$",Change the integration order of,"\int^2_{-6} \int^{2-x}_{\frac{x^2}{4}- 1}f(x, y) \, dy\,dx","This is a question from sample exam that I'm trying to solve but having difficults. Change the integration order of the integral: $$\int^2_{-6} \int^{2-x}_{\frac{x^2}{4}- 1}f(x, y) \, dy\,dx$$. $\implies -6 \le x \le 2, $ $\frac{x^2}{4} + 1 \le y \le 2 - x.$ I started to graph it but I stuck in graphing $y$. How can I change the integration order of the this integral? Thanks in advance.","This is a question from sample exam that I'm trying to solve but having difficults. Change the integration order of the integral: $$\int^2_{-6} \int^{2-x}_{\frac{x^2}{4}- 1}f(x, y) \, dy\,dx$$. $\implies -6 \le x \le 2, $ $\frac{x^2}{4} + 1 \le y \le 2 - x.$ I started to graph it but I stuck in graphing $y$. How can I change the integration order of the this integral? Thanks in advance.",,"['integration', 'multivariable-calculus', 'definite-integrals']"
1,transforming a vector from cartesian to spherical and cylindrical co-ordinate system,transforming a vector from cartesian to spherical and cylindrical co-ordinate system,,"I know the formula(which i don't know how to copy here but it was in matrix form) for transforming a vector from cartesian system to spherical or cylindrical coordinate system. But, I want to know its derivation . I tried searching it on web but all i got was some jacobian formulas , that i have no idea about.","I know the formula(which i don't know how to copy here but it was in matrix form) for transforming a vector from cartesian system to spherical or cylindrical coordinate system. But, I want to know its derivation . I tried searching it on web but all i got was some jacobian formulas , that i have no idea about.",,"['calculus', 'matrices', 'multivariable-calculus', 'vector-spaces']"
2,Integral over tetrahedron,Integral over tetrahedron,,"Let $S$ be the getrahedron in $\mathbb{R^3}$ having vertices $(0,0,0),(1,2,3),(0,1,2),(-1,1,1)$. Evaluate $\int_{S}f$ where $f(x,y,z) = x+2y-z$. You may use a suitable linear transformation as a coordinate change How do I solve this? As I've never studied mathematics before jumping into duplicate degree, I have very weak background on mathematics. Anybody help me? Thanks!","Let $S$ be the getrahedron in $\mathbb{R^3}$ having vertices $(0,0,0),(1,2,3),(0,1,2),(-1,1,1)$. Evaluate $\int_{S}f$ where $f(x,y,z) = x+2y-z$. You may use a suitable linear transformation as a coordinate change How do I solve this? As I've never studied mathematics before jumping into duplicate degree, I have very weak background on mathematics. Anybody help me? Thanks!",,"['integration', 'multivariable-calculus']"
3,sufficient condition for being an integral factor,sufficient condition for being an integral factor,,Let $ f: \mathbb {R}^m \rightarrow \mathbb {R}-\{0\} $ function $C^{\infty}$ class and $w$ a one-form $C^{\infty}$ class in $\mathbb {R}^m $. If $\alpha=w-\dfrac{1}{f}dx_{m+1} $ satisfies $\alpha \wedge d\alpha= w \wedge dw$ then $d(fw)=0$ Note: $\mathbb {R}^m \subseteq \mathbb {R}^{m+1}$ with $x_{m+1}=0$. Thanks for any suggestions.,Let $ f: \mathbb {R}^m \rightarrow \mathbb {R}-\{0\} $ function $C^{\infty}$ class and $w$ a one-form $C^{\infty}$ class in $\mathbb {R}^m $. If $\alpha=w-\dfrac{1}{f}dx_{m+1} $ satisfies $\alpha \wedge d\alpha= w \wedge dw$ then $d(fw)=0$ Note: $\mathbb {R}^m \subseteq \mathbb {R}^{m+1}$ with $x_{m+1}=0$. Thanks for any suggestions.,,"['analysis', 'multivariable-calculus', 'differential-geometry', 'manifolds', 'multilinear-algebra']"
4,Does there exist a $\nabla$-notation variant of the product rule applied to $\nabla[\mathbf{f}(\mathbf{x})\otimes\mathbf{g}(\mathbf{x})]$?,Does there exist a -notation variant of the product rule applied to ?,\nabla \nabla[\mathbf{f}(\mathbf{x})\otimes\mathbf{g}(\mathbf{x})],"This is a vector-calculus notation question; as a disclaimer, I am working in rectilinear space! For vector functions $\mathbf{f},\mathbf{g}:\mathbb{R}^n\rightarrow\mathbb{R}^n$, the chain rule for $\mathbf{f}(\mathbf{g}(\mathbf{x}))$ in $\nabla$-notation reads $$\nabla\left[\mathbf{f}(\mathbf{g}(\mathbf{x}))\right]=\nabla\mathbf{f}(\mathbf{g}(\mathbf{x}))\cdot\nabla\mathbf{g}(\mathbf{x})$$ where the usual convention that $\nabla$ appends an additional index to an array has been used. My question is, is there a similar version for the product rule? Ie,  $$\nabla[\mathbf{f}(\mathbf{x})\otimes\mathbf{g}(\mathbf{x})]=\mbox{}?$$ It is reasonably simple in indicial comma-derivative notation, whereupon we have $[f_i(\mathbf{x})g_j(\mathbf{x})]_{,k}=f_i(\mathbf{x})g_{j,k}(\mathbf{x})+f_{i,k}(\mathbf{x})g_{j}(\mathbf{x})$ which is reminiscent of the product rule in single-variable calculus. When I try rewriting this in $\nabla$-notation, the best I can come up with is  $$\nabla[\mathbf{f}(\mathbf{x})\otimes\mathbf{g}(\mathbf{x})]=\mathbf{f}(\mathbf{x})\otimes\nabla\mathbf{g}(\mathbf{x})+\mbox{Transpose}[\nabla\mathbf{f}(\mathbf{x})\otimes\mathbf{g}(\mathbf{x}),\{1,3,2\}]$$ where the Transpose operator permutes the indices of an array in the same manner as implemented in Mathematica. I find this a bit unsightly, although still usable for handwritten work. Is there a less hideous way of expressing this identity using $\nabla$ notation, or is indicial notation the only way to express this identity without resorting to expressions involving array index permutations?","This is a vector-calculus notation question; as a disclaimer, I am working in rectilinear space! For vector functions $\mathbf{f},\mathbf{g}:\mathbb{R}^n\rightarrow\mathbb{R}^n$, the chain rule for $\mathbf{f}(\mathbf{g}(\mathbf{x}))$ in $\nabla$-notation reads $$\nabla\left[\mathbf{f}(\mathbf{g}(\mathbf{x}))\right]=\nabla\mathbf{f}(\mathbf{g}(\mathbf{x}))\cdot\nabla\mathbf{g}(\mathbf{x})$$ where the usual convention that $\nabla$ appends an additional index to an array has been used. My question is, is there a similar version for the product rule? Ie,  $$\nabla[\mathbf{f}(\mathbf{x})\otimes\mathbf{g}(\mathbf{x})]=\mbox{}?$$ It is reasonably simple in indicial comma-derivative notation, whereupon we have $[f_i(\mathbf{x})g_j(\mathbf{x})]_{,k}=f_i(\mathbf{x})g_{j,k}(\mathbf{x})+f_{i,k}(\mathbf{x})g_{j}(\mathbf{x})$ which is reminiscent of the product rule in single-variable calculus. When I try rewriting this in $\nabla$-notation, the best I can come up with is  $$\nabla[\mathbf{f}(\mathbf{x})\otimes\mathbf{g}(\mathbf{x})]=\mathbf{f}(\mathbf{x})\otimes\nabla\mathbf{g}(\mathbf{x})+\mbox{Transpose}[\nabla\mathbf{f}(\mathbf{x})\otimes\mathbf{g}(\mathbf{x}),\{1,3,2\}]$$ where the Transpose operator permutes the indices of an array in the same manner as implemented in Mathematica. I find this a bit unsightly, although still usable for handwritten work. Is there a less hideous way of expressing this identity using $\nabla$ notation, or is indicial notation the only way to express this identity without resorting to expressions involving array index permutations?",,"['multivariable-calculus', 'tensor-products', 'matrix-calculus']"
5,Difference quotient (pde),Difference quotient (pde),,"Let $u: U\subset\mathbb{R}^n\rightarrow\mathbb{R}$. The Difference quotient of $u$ is defined by $D_k^hu(x)=\dfrac{u(x+he_k)-u(x)}{h}$ with $h\in\mathbb{R}$, $0<|h|<\textrm{dist}(V,\partial U)$. Here $x\in V\subset\!\subset U$ and $e_k$ is the canonical vector in $\mathbb{R}^n$. How can I prove that $\displaystyle\int_U vD_k^{-h}w\, dx=-\int_U wD_k^h v\,dx$","Let $u: U\subset\mathbb{R}^n\rightarrow\mathbb{R}$. The Difference quotient of $u$ is defined by $D_k^hu(x)=\dfrac{u(x+he_k)-u(x)}{h}$ with $h\in\mathbb{R}$, $0<|h|<\textrm{dist}(V,\partial U)$. Here $x\in V\subset\!\subset U$ and $e_k$ is the canonical vector in $\mathbb{R}^n$. How can I prove that $\displaystyle\int_U vD_k^{-h}w\, dx=-\int_U wD_k^h v\,dx$",,"['multivariable-calculus', 'partial-differential-equations']"
6,How would I undo a gradient function?,How would I undo a gradient function?,,"If we are given a vector, how can we tell if that is a gradient of a vector? And how would we find the original function? I was assigned this problem, and I know how to get a gradient of a function, but not how to go backwards. $$\bigl(6\cos(x^2+4y^2) - 12x^2 \sin(x^2+4y^2)\bigr)\vec \imath + \bigl(-48xy\sin(x^2+4y^2)\bigr)\vec\jmath$$ Thank you for all and any help you could provide.","If we are given a vector, how can we tell if that is a gradient of a vector? And how would we find the original function? I was assigned this problem, and I know how to get a gradient of a function, but not how to go backwards. $$\bigl(6\cos(x^2+4y^2) - 12x^2 \sin(x^2+4y^2)\bigr)\vec \imath + \bigl(-48xy\sin(x^2+4y^2)\bigr)\vec\jmath$$ Thank you for all and any help you could provide.",,"['calculus', 'multivariable-calculus']"
7,"$\nabla_y \left( \frac{1}{|x-y|} \right)= \frac{x-y}{|x-y|^3},$ right? Fundamental solution to Laplace in $\mathbb{R}^3$",right? Fundamental solution to Laplace in,"\nabla_y \left( \frac{1}{|x-y|} \right)= \frac{x-y}{|x-y|^3}, \mathbb{R}^3","OK, I can't figure out why I can't get this right: $$\nabla_y \left(  \frac{1}{|x-y|} \right)= \frac{x-y}{|x-y|^3},$$ right? I've checked the calculation several times, although this student-written article has a minus sign there. (Maybe they're confusing $\nabla_x$.) Now, supposing I'm right, I'm trying to show that for $u$ a harmonic function in $\Omega \subset \mathbb{R}^3$ a smooth bounded region with boundary $S$, we have $$\frac1{4\pi}\int_{y\in S}\left(  u(y) \frac{\cos \beta}{|x-y|^2}+\frac{1}{|x-y|}\frac{\partial u}{\partial \nu}(y) \right)dS(y), \tag{1}$$ where $\beta$ is the angle between $\nu$ and $x-y$. We use $$\int_{\Omega}u\Delta v - v\Delta udV = \int_S \left( u\frac{\partial v}{\partial \nu}- v\frac{\partial u}{\partial \nu} \right). \tag{2}$$ Let $v = \frac{1}{4\pi}\frac{1}{|x-y|}$. This is the fundamental solution in $\mathbb{R}^3$, so $\Delta v=-\delta_x$.  Then the left hand side of (2) is $-u(x)$. The right hand side of (2) should be (1) except with a minus sign on the first term, right? Either I've made a mistake somewhere, or both the question and the student-written article linked above are wrong.","OK, I can't figure out why I can't get this right: $$\nabla_y \left(  \frac{1}{|x-y|} \right)= \frac{x-y}{|x-y|^3},$$ right? I've checked the calculation several times, although this student-written article has a minus sign there. (Maybe they're confusing $\nabla_x$.) Now, supposing I'm right, I'm trying to show that for $u$ a harmonic function in $\Omega \subset \mathbb{R}^3$ a smooth bounded region with boundary $S$, we have $$\frac1{4\pi}\int_{y\in S}\left(  u(y) \frac{\cos \beta}{|x-y|^2}+\frac{1}{|x-y|}\frac{\partial u}{\partial \nu}(y) \right)dS(y), \tag{1}$$ where $\beta$ is the angle between $\nu$ and $x-y$. We use $$\int_{\Omega}u\Delta v - v\Delta udV = \int_S \left( u\frac{\partial v}{\partial \nu}- v\frac{\partial u}{\partial \nu} \right). \tag{2}$$ Let $v = \frac{1}{4\pi}\frac{1}{|x-y|}$. This is the fundamental solution in $\mathbb{R}^3$, so $\Delta v=-\delta_x$.  Then the left hand side of (2) is $-u(x)$. The right hand side of (2) should be (1) except with a minus sign on the first term, right? Either I've made a mistake somewhere, or both the question and the student-written article linked above are wrong.",,"['multivariable-calculus', 'partial-differential-equations', 'proof-verification', 'harmonic-functions', 'potential-theory']"
8,"Chain Rule, Reciprocals of Derivatives, and Differentiating w.r.t a Function","Chain Rule, Reciprocals of Derivatives, and Differentiating w.r.t a Function",,"I'm vey confused by this. $\frac{df^2}{df}=2f$, by the power rule. If $f=x+y$, it turns into $\frac{d(x+y)^2}{d(x+y)} = 2(x+y)$. With the chain rule and expanding, this equals $\frac{\partial (x^2+2xy+y^2)}{\partial x} \frac{\partial x}{\partial (x+y)} + \frac{\partial (x^2+2xy+y^2)}{\partial y} \frac{\partial y}{\partial (x+y)}$. $\frac{\partial (x+y)}{\partial x} = 1$, so $\frac{\partial x}{\partial (x+y)} = 1$. Plugging this in gives $\frac{\partial (x^2+2xy+y^2)}{\partial x} + \frac{\partial (x^2+2xy+y^2)}{\partial y} = (2x+2y) + (2x+2y) = 4x + 4y$, which is twice the correct answer. I've tried this with more than two variables and differentiating w.r.t. more complex functions, and each time it is equal to the correct answer multiplied by the number of variables, even if x, y, etc. are treated differently. I don't think it's an issue with the chain rule, as it's used verbatim in this, and I looked up the rule for the reciprocals of partial derivatives and that is valid too, so I can't find the issue. Any insights? Edit: Actually, I'm still confused. If $u=x+y$, then $x=u-y$. This seems to say that $\frac{\partial x}{\partial u}=1$. Also, according to the implicit function theorem (I think), $\frac{\partial x}{\partial u}=1$: $F(u, x, y) = u-(x+y) = 0 \rightarrow \frac{\partial x}{\partial u} = -\frac{\partial F / \partial u}{\partial F / \partial x} = -\frac{1}{-1} = 1$. This brings me right back to the initial issue that I had.","I'm vey confused by this. $\frac{df^2}{df}=2f$, by the power rule. If $f=x+y$, it turns into $\frac{d(x+y)^2}{d(x+y)} = 2(x+y)$. With the chain rule and expanding, this equals $\frac{\partial (x^2+2xy+y^2)}{\partial x} \frac{\partial x}{\partial (x+y)} + \frac{\partial (x^2+2xy+y^2)}{\partial y} \frac{\partial y}{\partial (x+y)}$. $\frac{\partial (x+y)}{\partial x} = 1$, so $\frac{\partial x}{\partial (x+y)} = 1$. Plugging this in gives $\frac{\partial (x^2+2xy+y^2)}{\partial x} + \frac{\partial (x^2+2xy+y^2)}{\partial y} = (2x+2y) + (2x+2y) = 4x + 4y$, which is twice the correct answer. I've tried this with more than two variables and differentiating w.r.t. more complex functions, and each time it is equal to the correct answer multiplied by the number of variables, even if x, y, etc. are treated differently. I don't think it's an issue with the chain rule, as it's used verbatim in this, and I looked up the rule for the reciprocals of partial derivatives and that is valid too, so I can't find the issue. Any insights? Edit: Actually, I'm still confused. If $u=x+y$, then $x=u-y$. This seems to say that $\frac{\partial x}{\partial u}=1$. Also, according to the implicit function theorem (I think), $\frac{\partial x}{\partial u}=1$: $F(u, x, y) = u-(x+y) = 0 \rightarrow \frac{\partial x}{\partial u} = -\frac{\partial F / \partial u}{\partial F / \partial x} = -\frac{1}{-1} = 1$. This brings me right back to the initial issue that I had.",,"['multivariable-calculus', 'partial-derivative']"
9,Evaluating $\iint_D \sqrt{4x^2-y^2}\;\ \mathrm dx \ \mathrm dy$,Evaluating,\iint_D \sqrt{4x^2-y^2}\;\ \mathrm dx \ \mathrm dy,"I have to evaluate $\displaystyle\iint_Df(x,y)\;dxdy$ for $f(x,y) = \sqrt{4x^2-y^2}$ with $D = \{(x,y)\in\mathbb{R}^2: 0\leq x \leq 1, 0\leq y \leq x\}$. It seems that i can't solve for $\displaystyle\int_0^1 \displaystyle\int_0 ^x\sqrt{4x^2-y^2} dydx$ but working with iterated integrals I could solve for $\displaystyle\int_0^1 \displaystyle\int_{\color{red}{1}} ^{\color{red}{y}}\sqrt{4x^2-y^2} dxdy$. A small sketch of what i did until now: $\displaystyle\int_{\color{red}{1}} ^{\color{red}{y}}\sqrt{4x^2-y^2} dx = $ $\displaystyle\int_{\color{red}{1}} ^{\color{red}{y}}\displaystyle\frac{4x^2}{\sqrt{4x^2-y^2}}dx - \displaystyle\int_{\color{red}{1}} ^{\color{red}{y}}\displaystyle\frac{y^2}{\sqrt{4x^2-y^2}}dx = $ $ 4\left(\displaystyle\frac{x^2}{\sqrt{4x^2-y^2}} -\displaystyle\frac{1}{4}\displaystyle\int \sqrt{4x^2-y^2}dx\right)-y^2\displaystyle\frac{y}{2|y|}\arcsin\left(\displaystyle\frac{2}{y}x \right) $ $\implies \displaystyle\int_{\color{red}{1}} ^{\color{red}{y}}\sqrt{4x^2-y^2} dx = $ $\displaystyle\frac{1}{2}\left(\displaystyle\frac{4x^2}{\sqrt{4x^2-y^2}}-y^2\displaystyle\frac{y}{2|y|}\arcsin\left(\displaystyle\frac{2}{y}x \right) \right)$. [wrong] And it seems that after I evaluate the last one i get $\displaystyle\frac{y^2}{\sqrt{3y^2}} = \displaystyle\frac{y^2}{\sqrt{3}|y|}$ Then if I solve the integral of the expresion above for x I get $\displaystyle\frac{1}{2\sqrt{3}}$ if $y>0$ and $\displaystyle\frac{-1}{2\sqrt{3}}$ if $y<0$.[/wrong] I'm almost sure i made a mistake somewhere. Can someone find any errors?","I have to evaluate $\displaystyle\iint_Df(x,y)\;dxdy$ for $f(x,y) = \sqrt{4x^2-y^2}$ with $D = \{(x,y)\in\mathbb{R}^2: 0\leq x \leq 1, 0\leq y \leq x\}$. It seems that i can't solve for $\displaystyle\int_0^1 \displaystyle\int_0 ^x\sqrt{4x^2-y^2} dydx$ but working with iterated integrals I could solve for $\displaystyle\int_0^1 \displaystyle\int_{\color{red}{1}} ^{\color{red}{y}}\sqrt{4x^2-y^2} dxdy$. A small sketch of what i did until now: $\displaystyle\int_{\color{red}{1}} ^{\color{red}{y}}\sqrt{4x^2-y^2} dx = $ $\displaystyle\int_{\color{red}{1}} ^{\color{red}{y}}\displaystyle\frac{4x^2}{\sqrt{4x^2-y^2}}dx - \displaystyle\int_{\color{red}{1}} ^{\color{red}{y}}\displaystyle\frac{y^2}{\sqrt{4x^2-y^2}}dx = $ $ 4\left(\displaystyle\frac{x^2}{\sqrt{4x^2-y^2}} -\displaystyle\frac{1}{4}\displaystyle\int \sqrt{4x^2-y^2}dx\right)-y^2\displaystyle\frac{y}{2|y|}\arcsin\left(\displaystyle\frac{2}{y}x \right) $ $\implies \displaystyle\int_{\color{red}{1}} ^{\color{red}{y}}\sqrt{4x^2-y^2} dx = $ $\displaystyle\frac{1}{2}\left(\displaystyle\frac{4x^2}{\sqrt{4x^2-y^2}}-y^2\displaystyle\frac{y}{2|y|}\arcsin\left(\displaystyle\frac{2}{y}x \right) \right)$. [wrong] And it seems that after I evaluate the last one i get $\displaystyle\frac{y^2}{\sqrt{3y^2}} = \displaystyle\frac{y^2}{\sqrt{3}|y|}$ Then if I solve the integral of the expresion above for x I get $\displaystyle\frac{1}{2\sqrt{3}}$ if $y>0$ and $\displaystyle\frac{-1}{2\sqrt{3}}$ if $y<0$.[/wrong] I'm almost sure i made a mistake somewhere. Can someone find any errors?",,"['calculus', 'integration', 'multivariable-calculus']"
10,Multivariable Mean Value Theorem With Equalities,Multivariable Mean Value Theorem With Equalities,,"I'm having a lot of trouble deciphering the notation in this proof of the mean value theorem in several variables. For example, on page 2 of this link we see an example of why the multivariable mean value equality fails & a claim that the best we can do is to find an inequality, yet the pages I've posted provide an equality. I can't understand the notation, & I'm afraid to learn something like this when a ton of sources clearly claim a mean value equality theorem doesn't hold, for example on stack , so I'd sincerely appreciate some help with this, like a comprehensive explanation by someone interested, because if this proof is valid then an extremely easy proof of the inverse function theorem analogous to the single-variable version follows automatically , many thanks!","I'm having a lot of trouble deciphering the notation in this proof of the mean value theorem in several variables. For example, on page 2 of this link we see an example of why the multivariable mean value equality fails & a claim that the best we can do is to find an inequality, yet the pages I've posted provide an equality. I can't understand the notation, & I'm afraid to learn something like this when a ton of sources clearly claim a mean value equality theorem doesn't hold, for example on stack , so I'd sincerely appreciate some help with this, like a comprehensive explanation by someone interested, because if this proof is valid then an extremely easy proof of the inverse function theorem analogous to the single-variable version follows automatically , many thanks!",,['multivariable-calculus']
11,How to find the normal plane from a tangent plane?,How to find the normal plane from a tangent plane?,,"$$f(x,y,z)=\frac{x^2}{4} +\frac{y^2}{9} +\frac{z^2}{25}=3 $$ I found the tangent plane from this surface at $P(2,3,5)$ by using the gradient vector, $\nabla F=\langle f_x, f_y, f_z\rangle$. I was wondering if I could find the normal plane at that same point using the information already given. The tangent plane is $(x-2)+2/3(y-3)+2/5(z-5)=0$ The missing key is to find the vector that is perpendicular to the normal.","$$f(x,y,z)=\frac{x^2}{4} +\frac{y^2}{9} +\frac{z^2}{25}=3 $$ I found the tangent plane from this surface at $P(2,3,5)$ by using the gradient vector, $\nabla F=\langle f_x, f_y, f_z\rangle$. I was wondering if I could find the normal plane at that same point using the information already given. The tangent plane is $(x-2)+2/3(y-3)+2/5(z-5)=0$ The missing key is to find the vector that is perpendicular to the normal.",,"['multivariable-calculus', 'differential-geometry']"
12,Differentiation in several variables using projection,Differentiation in several variables using projection,,"Could you tell me how to differentiate a function with several variables? Our teacher gave us an example: $\pi_1 : (x,y) \rightarrow x, \ \ \ \pi_2: (x,y) \rightarrow y$ - these are differentiable, because they are linear, Consider $f(x,y) = e^x \cos y$ Let $f_1: t \rightarrow e^t, \ \ \ f_2: t \rightarrow \cos t$. Now $f(x,y) = F(f_1 \circ \pi_1, f_2 \circ \pi_2$), where $F(x,y)=xy$, So $F'(x,y)(h,k) = F(x,k) + F(h,y) = xk+hy$, so $f'(x,y)(h,k)=he^x \cos y - k e^x \sin y$. I understand this, because we had this theorem on the analysis lecture: If $E_1, E_2, F$ - Banach spaces, $\phi \in \mathcal{L}(E_1, E_2; F)$ - linear and continuous, then $\phi$ is $C^1$ and $d_{(a_1, a_2)}\phi.(h_1, h_2) = \phi'(a_1,a_2)(h_1,h_2) = \phi(a_1, h_2) + \phi(h_1, a_2), \ \ (a_1, a_2), (h_1, h_2) \in E_1 \times E_2$. My problem is - what should I do when I have three, four variables. Is there any other way to quickly and safely determine derivatives? When doing it by calculating partial derivatives I first need to check if they are continuous and then $d_af = (\frac{ \partial f }{\partial x_1 }, ..., \frac{ \partial f }{\partial x_1 })$ where $\frac{ \partial f }{\partial x_1 } = d_af.e_i$ or $d_af = \sum _{i=1} ^m \frac{\partial f}{\partial e_i}(a) \circ \pi _i$, where $e_i$ is the standard basis. Is this the correct approach? I'd really appreciate all your help here Thank you a lot.","Could you tell me how to differentiate a function with several variables? Our teacher gave us an example: $\pi_1 : (x,y) \rightarrow x, \ \ \ \pi_2: (x,y) \rightarrow y$ - these are differentiable, because they are linear, Consider $f(x,y) = e^x \cos y$ Let $f_1: t \rightarrow e^t, \ \ \ f_2: t \rightarrow \cos t$. Now $f(x,y) = F(f_1 \circ \pi_1, f_2 \circ \pi_2$), where $F(x,y)=xy$, So $F'(x,y)(h,k) = F(x,k) + F(h,y) = xk+hy$, so $f'(x,y)(h,k)=he^x \cos y - k e^x \sin y$. I understand this, because we had this theorem on the analysis lecture: If $E_1, E_2, F$ - Banach spaces, $\phi \in \mathcal{L}(E_1, E_2; F)$ - linear and continuous, then $\phi$ is $C^1$ and $d_{(a_1, a_2)}\phi.(h_1, h_2) = \phi'(a_1,a_2)(h_1,h_2) = \phi(a_1, h_2) + \phi(h_1, a_2), \ \ (a_1, a_2), (h_1, h_2) \in E_1 \times E_2$. My problem is - what should I do when I have three, four variables. Is there any other way to quickly and safely determine derivatives? When doing it by calculating partial derivatives I first need to check if they are continuous and then $d_af = (\frac{ \partial f }{\partial x_1 }, ..., \frac{ \partial f }{\partial x_1 })$ where $\frac{ \partial f }{\partial x_1 } = d_af.e_i$ or $d_af = \sum _{i=1} ^m \frac{\partial f}{\partial e_i}(a) \circ \pi _i$, where $e_i$ is the standard basis. Is this the correct approach? I'd really appreciate all your help here Thank you a lot.",,"['real-analysis', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
13,Weak implicit function theorem. Is my proof alright?,Weak implicit function theorem. Is my proof alright?,,"I want to prove that if we have $ f \in C(A \times U,Y)$, where $x_0 \in U$, $\lambda_0 \in A$ and A and U are open sets in Banach spaces and Y is a Banach space too and we have that: $f(\lambda_0,x_0)=0$ and $f_x(\lambda_0,x_0)$ is invertible and the inverse map is bounded and $f_x \in C(A \times U, L(U,Y))$, then we have that there is a continuous curve $x(\lambda)$ in a surrounding of $x_0$, such that $f(\lambda, x(\lambda))=0$ My idea was the following: Look at $F(x,\lambda):= x - (f_x (\lambda_0,x_0))^{-1}f(\lambda,x)$  (The subscript x always denotes the partial derivative with respect to this coordinate). Then we have: $F_x(x,\lambda):= \operatorname{id} - (f_x (\lambda_0,x_0))^{-1}f_x(\lambda,x)$ Therefore we have that $\| F(\lambda,x) - F(\lambda,x')\| \le \sup_{\xi} \| F_x(\lambda,\xi)\| \|x-x'\|$ Now we want to get a contraction by choosing $x \in B_{\epsilon}(x_0)$ and $\lambda \in B_{\delta}(\lambda_0)$( notice that both balls shall be closed in order to apply Banach's fixed point theorem) such that: $$\|F_x(\lambda, x)\| =\| \operatorname{id} - (f_x (\lambda_0,x_0))^{-1}f_x(\lambda,x)\| \le \|f_x (\lambda_0,x_0))^{-1}\| \|f_x (\lambda_0,x_0) -f_x (\lambda,x))\| \le \|f_x (\lambda_0,x_0))^{-1}\|( \|f_x (\lambda_0,x_0) -f_x (\lambda,x_0))\| +  \|f_x (\lambda,x_0) -f_x (\lambda,x)\|) =: L < 1$$ This is possible since $f_x$ is continuous. Therefore, we have a contraction and we get by Banach's fixed point theorem a unique continuous function $x$ such that $x(\lambda) = F(\lambda,x(\lambda))$ for every lambda and since we know from $f(\lambda_0,x_0)=0$ that for $\lambda_0$ we have $F(\lambda_0,x(\lambda_0)) = x(\lambda_0)$, this function $x$ does what I want. Is this approach correct so far? If something is unclear, please let me know. It is not unlikely that there is something wrong. I will also award the bounty if somebody has a good and helpful comment on this. If you think that everything is okay, then it is even better.","I want to prove that if we have $ f \in C(A \times U,Y)$, where $x_0 \in U$, $\lambda_0 \in A$ and A and U are open sets in Banach spaces and Y is a Banach space too and we have that: $f(\lambda_0,x_0)=0$ and $f_x(\lambda_0,x_0)$ is invertible and the inverse map is bounded and $f_x \in C(A \times U, L(U,Y))$, then we have that there is a continuous curve $x(\lambda)$ in a surrounding of $x_0$, such that $f(\lambda, x(\lambda))=0$ My idea was the following: Look at $F(x,\lambda):= x - (f_x (\lambda_0,x_0))^{-1}f(\lambda,x)$  (The subscript x always denotes the partial derivative with respect to this coordinate). Then we have: $F_x(x,\lambda):= \operatorname{id} - (f_x (\lambda_0,x_0))^{-1}f_x(\lambda,x)$ Therefore we have that $\| F(\lambda,x) - F(\lambda,x')\| \le \sup_{\xi} \| F_x(\lambda,\xi)\| \|x-x'\|$ Now we want to get a contraction by choosing $x \in B_{\epsilon}(x_0)$ and $\lambda \in B_{\delta}(\lambda_0)$( notice that both balls shall be closed in order to apply Banach's fixed point theorem) such that: $$\|F_x(\lambda, x)\| =\| \operatorname{id} - (f_x (\lambda_0,x_0))^{-1}f_x(\lambda,x)\| \le \|f_x (\lambda_0,x_0))^{-1}\| \|f_x (\lambda_0,x_0) -f_x (\lambda,x))\| \le \|f_x (\lambda_0,x_0))^{-1}\|( \|f_x (\lambda_0,x_0) -f_x (\lambda,x_0))\| +  \|f_x (\lambda,x_0) -f_x (\lambda,x)\|) =: L < 1$$ This is possible since $f_x$ is continuous. Therefore, we have a contraction and we get by Banach's fixed point theorem a unique continuous function $x$ such that $x(\lambda) = F(\lambda,x(\lambda))$ for every lambda and since we know from $f(\lambda_0,x_0)=0$ that for $\lambda_0$ we have $F(\lambda_0,x(\lambda_0)) = x(\lambda_0)$, this function $x$ does what I want. Is this approach correct so far? If something is unclear, please let me know. It is not unlikely that there is something wrong. I will also award the bounty if somebody has a good and helpful comment on this. If you think that everything is okay, then it is even better.",,"['calculus', 'real-analysis']"
14,Is there a continuous function $f$ from {$x\in \mathbb{R}^n :||x||\le 1$} onto $\mathbb{R}^n$,Is there a continuous function  from {} onto,f x\in \mathbb{R}^n :||x||\le 1 \mathbb{R}^n,"True or False:- There is a continuous function $f$ from {$x\in \mathbb{R}^n :||x||\le 1$} onto $\mathbb{R}^n$. where $||x||=(x_1^2+...+x_n^2)^{1/2}$. I think it is not possible as the domain is compact set and $f$ is continuous , so range set will also be compact.But $\mathbb{R}^n$ is not compact. Am I right?","True or False:- There is a continuous function $f$ from {$x\in \mathbb{R}^n :||x||\le 1$} onto $\mathbb{R}^n$. where $||x||=(x_1^2+...+x_n^2)^{1/2}$. I think it is not possible as the domain is compact set and $f$ is continuous , so range set will also be compact.But $\mathbb{R}^n$ is not compact. Am I right?",,['multivariable-calculus']
15,"Proving that $f(x,y) = \frac{xy^2}{x^2 + y^2}$ with $f(0,0)=0$ is a continuous function using epsilon-delta. [duplicate]",Proving that  with  is a continuous function using epsilon-delta. [duplicate],"f(x,y) = \frac{xy^2}{x^2 + y^2} f(0,0)=0","This question already has answers here : For which $a\in \mathbb{R}$ is $f(x,y) = \frac{xy^a}{x^2+y^2}$ with $f(0,0)=0$ continuous? [duplicate] (4 answers) Closed 7 years ago . THE QUESTION: Use the metric $(x,y)$ = $\rho(x,y)=|x-y|$ for the reals and use the metric $\rho((x_1,y_1),(x_2,y_2)) = \sqrt{(x_1-x_2)^2 + (y_1-y_2)^2}$ for the plane. Define $f:R\times R \to R$ as $f(x,y)=\frac{xy^2}{(x^2 + y^2)}$ for $f(x,y)$ $\neq$ $(0,0)$ and set $f(0,0) = 0$. Determine whether $f$ is continuous using $\epsilon$-$\delta$ proof. WHAT I'VE DONE: I've tried using a direct approach by using the definition of a continuous function: $f:R\times R \to R$ is continuous at $(x_1,y_1) \in R \times R$, $\forall \epsilon>0, \exists  \delta >0$ such that $\forall (x_2,y_2) \in R \times R$ if $\sqrt{(x_1-x_2)^2 + (y_1 - y_2)^2 } <\delta$, then $|f(x_1,y_1)-f(x_2,y_2)|<\epsilon$ However, with the $f(x,y)$ as defined above, it seems impossible with the messy algebra since I want to find a $\delta$ that satisfies $|\frac{x_1y_1^2}{(x_1^2 + y_1^2)}-\frac{x_2y_2^2}{(x_2^2 + y_2^2)}| < \epsilon$ using the metrics defined above. Is there a trick I am supposed to be seeing? Would really appreciate if anyone could show me the proof for this question. Thank you!","This question already has answers here : For which $a\in \mathbb{R}$ is $f(x,y) = \frac{xy^a}{x^2+y^2}$ with $f(0,0)=0$ continuous? [duplicate] (4 answers) Closed 7 years ago . THE QUESTION: Use the metric $(x,y)$ = $\rho(x,y)=|x-y|$ for the reals and use the metric $\rho((x_1,y_1),(x_2,y_2)) = \sqrt{(x_1-x_2)^2 + (y_1-y_2)^2}$ for the plane. Define $f:R\times R \to R$ as $f(x,y)=\frac{xy^2}{(x^2 + y^2)}$ for $f(x,y)$ $\neq$ $(0,0)$ and set $f(0,0) = 0$. Determine whether $f$ is continuous using $\epsilon$-$\delta$ proof. WHAT I'VE DONE: I've tried using a direct approach by using the definition of a continuous function: $f:R\times R \to R$ is continuous at $(x_1,y_1) \in R \times R$, $\forall \epsilon>0, \exists  \delta >0$ such that $\forall (x_2,y_2) \in R \times R$ if $\sqrt{(x_1-x_2)^2 + (y_1 - y_2)^2 } <\delta$, then $|f(x_1,y_1)-f(x_2,y_2)|<\epsilon$ However, with the $f(x,y)$ as defined above, it seems impossible with the messy algebra since I want to find a $\delta$ that satisfies $|\frac{x_1y_1^2}{(x_1^2 + y_1^2)}-\frac{x_2y_2^2}{(x_2^2 + y_2^2)}| < \epsilon$ using the metrics defined above. Is there a trick I am supposed to be seeing? Would really appreciate if anyone could show me the proof for this question. Thank you!",,"['real-analysis', 'multivariable-calculus', 'continuity']"
16,Average arc length between two random points on a unit sphere?,Average arc length between two random points on a unit sphere?,,"I'm trying to find the average arc length between two random points on a unit sphere. The solution I've come up with is rather ugly. Consider a parametric surface: $$X(u,v)=\sin u\cos v\\Y(u,v)=\cos u\cos v\\Z(u,v)=\sin v$$ (For a sphere, $u\in[0,2\pi]$, $v\in\left[\frac{-\pi}{2},\frac{\pi}{2}\right]$). Then, integrate the distance formula for two points $(x_1,y_1,z_1),(x_2,y_2,z_2)$along all three axes. This turns into a nasty integral, though. $$F(\ldots)=\int_0^{2\pi}     \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}         D(X_1,Y_1,Z_1,X_2,Y_2,Z_2)dv\mbox{ }du$$ $$\int_0^{2\pi}\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}F(\ldots)\mbox{ }dv\mbox{ }du$$ (Where $D(\ldots)$ is the arc length formula.) I could plug this into mathematica or something, but I have a feeling this would take too long to compute be too complex probably also be wrong What is a more efficient way to do this? How should I go about solving this problem?","I'm trying to find the average arc length between two random points on a unit sphere. The solution I've come up with is rather ugly. Consider a parametric surface: $$X(u,v)=\sin u\cos v\\Y(u,v)=\cos u\cos v\\Z(u,v)=\sin v$$ (For a sphere, $u\in[0,2\pi]$, $v\in\left[\frac{-\pi}{2},\frac{\pi}{2}\right]$). Then, integrate the distance formula for two points $(x_1,y_1,z_1),(x_2,y_2,z_2)$along all three axes. This turns into a nasty integral, though. $$F(\ldots)=\int_0^{2\pi}     \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}         D(X_1,Y_1,Z_1,X_2,Y_2,Z_2)dv\mbox{ }du$$ $$\int_0^{2\pi}\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}F(\ldots)\mbox{ }dv\mbox{ }du$$ (Where $D(\ldots)$ is the arc length formula.) I could plug this into mathematica or something, but I have a feeling this would take too long to compute be too complex probably also be wrong What is a more efficient way to do this? How should I go about solving this problem?",,['multivariable-calculus']
17,Minimizing a specific function over n variables,Minimizing a specific function over n variables,,"Experimenting with something related to probability theory I came across the following $n$-variable function $$ f(p_1,\ldots,p_n) = \sum_{i=2}^{n-1} \left ( (1-p_i)(1-p_{i-1})^i (1-p_{i+1})^{n-i} + p_i {n \choose i} \right ) $$ where $p_1,\ldots,p_n \in [0,1].$ The natural thing is to use the inequality $(1-p_i)^t \leq e^{-p_i t}$ but I don't know what to do from there. Since I am very rusty with such optimizations I would like to ask if someone could find a good lower bound for this function ? What is a good approximation of the minimum of $f$ and how to obtain it? If this is too hard, it would be nice to see if there is a set of values for $p_1,\ldots,p_n$ such that $f(p_1,\ldots,p_n)$ is sub-exponential? Thank you.","Experimenting with something related to probability theory I came across the following $n$-variable function $$ f(p_1,\ldots,p_n) = \sum_{i=2}^{n-1} \left ( (1-p_i)(1-p_{i-1})^i (1-p_{i+1})^{n-i} + p_i {n \choose i} \right ) $$ where $p_1,\ldots,p_n \in [0,1].$ The natural thing is to use the inequality $(1-p_i)^t \leq e^{-p_i t}$ but I don't know what to do from there. Since I am very rusty with such optimizations I would like to ask if someone could find a good lower bound for this function ? What is a good approximation of the minimum of $f$ and how to obtain it? If this is too hard, it would be nice to see if there is a set of values for $p_1,\ldots,p_n$ such that $f(p_1,\ldots,p_n)$ is sub-exponential? Thank you.",,"['real-analysis', 'probability', 'multivariable-calculus', 'optimization']"
18,Why isn't the partial derivative of a coordinate patch a vector field?,Why isn't the partial derivative of a coordinate patch a vector field?,,"Let $D$ be an open set of $R^2$ and M a surface in $R^3$. Let $\mathbf{x(u,v)}: D \rightarrow M $ be a coordinate patch in M. Let $\mathbf{x}_u, \mathbf{x}_v$ be partial derivatives of the patch. Define $N(u,v):=\mathbf{x}_u(u,v) \times \mathbf{x}_v(u,v)$. The book I am learning says that $\mathbf{x}_u, \mathbf{x}_v$ and $N$ are not vector fields on  $\mathbf{x}(D)$. They are only vector valued functions on $D$. Why isn't $\mathbf{x}_u$ a vector field on $\mathbf{x}(D)$?","Let $D$ be an open set of $R^2$ and M a surface in $R^3$. Let $\mathbf{x(u,v)}: D \rightarrow M $ be a coordinate patch in M. Let $\mathbf{x}_u, \mathbf{x}_v$ be partial derivatives of the patch. Define $N(u,v):=\mathbf{x}_u(u,v) \times \mathbf{x}_v(u,v)$. The book I am learning says that $\mathbf{x}_u, \mathbf{x}_v$ and $N$ are not vector fields on  $\mathbf{x}(D)$. They are only vector valued functions on $D$. Why isn't $\mathbf{x}_u$ a vector field on $\mathbf{x}(D)$?",,"['multivariable-calculus', 'differential-geometry']"
19,Delta-Epsilon Proofs,Delta-Epsilon Proofs,,"I am trying to prove the following limits using the delta-epsilon method. Can you help me out? 1.$$ \lim_{(x,y)\to(2,3)}(3x^2y^2 + 4xy-12) = 120$$ 2.$$ \lim_{(x,y)\to(0,0)}\frac{5x^2y}{x^2+y^2} = 0$$ How do I work this out with the upper and lower boundaries?","I am trying to prove the following limits using the delta-epsilon method. Can you help me out? 1.$$ \lim_{(x,y)\to(2,3)}(3x^2y^2 + 4xy-12) = 120$$ 2.$$ \lim_{(x,y)\to(0,0)}\frac{5x^2y}{x^2+y^2} = 0$$ How do I work this out with the upper and lower boundaries?",,['multivariable-calculus']
20,Determine all the extrema of a function subject to a non-linear constraint.,Determine all the extrema of a function subject to a non-linear constraint.,,"QUESTION Determine all extrema of the function $$f(x,y) = x+ 2y $$ subject to $$x^2 + y^2 - 80 = 0$$ ATTEMPT I don't think I understand what I'm supposed to do. This was in a test and I ended up trying to ""graphically""or ïntuitively"" find out how the $f(x,y)$ would behave in a circle of radius $\sqrt80$ Which left me with some pretty random numbers that turned out to also be wrong. What could I have done differently? (If possible could I get a bit of a detailed explanation or some links to that info.) Lagrange Multipliers: I tried that too on the paper and it also went horribly wrong (but I did get some points, though) but I couldn't quite figure it out.","QUESTION Determine all extrema of the function $$f(x,y) = x+ 2y $$ subject to $$x^2 + y^2 - 80 = 0$$ ATTEMPT I don't think I understand what I'm supposed to do. This was in a test and I ended up trying to ""graphically""or ïntuitively"" find out how the $f(x,y)$ would behave in a circle of radius $\sqrt80$ Which left me with some pretty random numbers that turned out to also be wrong. What could I have done differently? (If possible could I get a bit of a detailed explanation or some links to that info.) Lagrange Multipliers: I tried that too on the paper and it also went horribly wrong (but I did get some points, though) but I couldn't quite figure it out.",,"['calculus', 'multivariable-calculus', 'optimization', 'lagrange-multiplier']"
21,Differentiability + Lipschitz = C^1?,Differentiability + Lipschitz = C^1?,,"It is well know that: Let $E \subset \mathbb{R}^N$, let $f \colon E \to \mathbb{R}$, let $x_0 \in E^o$. Assume that there exists $r>0$ such that $B(x_0,r) \subset E$ and the partial derivatives $\frac{\partial f}{\partial x_i}$, $i=1 \dots N$, exist for every $x \in B(x_0,r)$ and are continuous at $x_0$. Then f is differentiable at $x_0$. It is also known that the converse is not true, meaning that differentiability in one point does not imply that the partial derivatives are continuous at that point (here is a counterexample: http://web.mit.edu/watko/Public/024/notes/n03.pdf ) I was wondering if it is possible to proof a partial converse of the theorem, more precisely, my question is: is one of the two following statements true? $1)$ Lipschitz continuity and differentiability in $B(x_0,r)$ imply continuity of the partial derivatives at $x_0$. $2)$ Lipschitz continuity and differentiability in one point imply continuity of the partial derivatives in the same point. Proofs and counterexamples are welcome!","It is well know that: Let $E \subset \mathbb{R}^N$, let $f \colon E \to \mathbb{R}$, let $x_0 \in E^o$. Assume that there exists $r>0$ such that $B(x_0,r) \subset E$ and the partial derivatives $\frac{\partial f}{\partial x_i}$, $i=1 \dots N$, exist for every $x \in B(x_0,r)$ and are continuous at $x_0$. Then f is differentiable at $x_0$. It is also known that the converse is not true, meaning that differentiability in one point does not imply that the partial derivatives are continuous at that point (here is a counterexample: http://web.mit.edu/watko/Public/024/notes/n03.pdf ) I was wondering if it is possible to proof a partial converse of the theorem, more precisely, my question is: is one of the two following statements true? $1)$ Lipschitz continuity and differentiability in $B(x_0,r)$ imply continuity of the partial derivatives at $x_0$. $2)$ Lipschitz continuity and differentiability in one point imply continuity of the partial derivatives in the same point. Proofs and counterexamples are welcome!",,['multivariable-calculus']
22,Unit speed curves and Frenet frames,Unit speed curves and Frenet frames,,"Let $\alpha(s)$ and $\beta(s)$ be two unit speed curves and assume   that $\kappa_{\alpha}(s)=\kappa_{\beta}(s)$ and   $\tau_{\alpha}(s)=\tau_{\beta}(s)$, where $\kappa$ and $\tau$ are   respectively the curvature and torsion. Let $$J(s) =  T_{\alpha}(s)\dot\ T_{\beta}(s)+N_{\alpha}(s) \dot\ N_{\beta}(s)  +B_{\alpha}(s) \dot\ B_{\beta}(s).$$ Show that: $J(0)=3$ and  $J(s)=3$ implies that the Frenet frames of $\alpha$ and $\beta$ agree at   $s$ $J'(s) = 0$ and $\alpha(s) = \beta(s)$ for all $s$. How can I show this?","Let $\alpha(s)$ and $\beta(s)$ be two unit speed curves and assume   that $\kappa_{\alpha}(s)=\kappa_{\beta}(s)$ and   $\tau_{\alpha}(s)=\tau_{\beta}(s)$, where $\kappa$ and $\tau$ are   respectively the curvature and torsion. Let $$J(s) =  T_{\alpha}(s)\dot\ T_{\beta}(s)+N_{\alpha}(s) \dot\ N_{\beta}(s)  +B_{\alpha}(s) \dot\ B_{\beta}(s).$$ Show that: $J(0)=3$ and  $J(s)=3$ implies that the Frenet frames of $\alpha$ and $\beta$ agree at   $s$ $J'(s) = 0$ and $\alpha(s) = \beta(s)$ for all $s$. How can I show this?",,"['geometry', 'multivariable-calculus', 'differential-geometry']"
23,Divergence theorem: why is it $F_n \cdot dS$?,Divergence theorem: why is it ?,F_n \cdot dS,"I am learning the calculus 3 interpretation of Stokes' theorems while just dipping my toe in the water of differential forms. Since I have only limited knowledge of the real differential geometry involved, I'm having trouble putting things together. Reading the treatment of Stokes' theorems in Buck's Advanced Calculus , I find the following lemma relating the 2-form to the normal: $$\int_{\Sigma} f dy dz + g dz dx + hdx dy \\= \int_{D} f \frac{ \partial(y,z) }{ \partial(u,v) }du dv + g \frac{ \partial(z,x) }{ \partial(u,v) }du dv + h\frac{ \partial(x,y) }{ \partial(u,v) }du dv\\ = \int_D \vec{U} \cdot \mathbf{n}(u,v)\, dA$$ where $\vec{U} = (f,g,h)$ and $\sigma: \mathbb{R}^2 \to \mathbb{R}^3$ is a parameterized surface in $\mathbb{R}^3$ given by $\sigma(u,v) = (x(u,v), y(u,v), z(u,v))$. The referring to $dudv$ as ""$dA$"" is a little confusing to me. ""$dA$"" seems to be something that if we integrated it, we should end up with the surface area of $\Sigma$ (I would like to think of it like ""parameterizing by arclength"".) But if we integrate $dudv$, we end up with the area in the parameter space. OK, so perhaps the $dA$ refers to the area of the parameter space. But then here on Wikipedia I find the statement $$\iiint _V (\nabla \cdot F) dV = \iint_{S} F \cdot n \,dS.$$ which I read as translating $$\int_V (f_x + g_y + h_z) dx dy dz \\=\int_V d(f\, dy dz + g \,dz dx + h \,dx dy) + \int_{\partial V}f\, dy dz + g \,dz dx + h \,dx dy.$$ Surely this $dS$ here does not refer to any particular parameterization space...? Here $dS$ is something which, if we integrated it, would yield the surface area of $S$, right? Question 2: What 2-form would we integrate if we want the surface area of a parameterized surface in $\mathbb{R}^3$?","I am learning the calculus 3 interpretation of Stokes' theorems while just dipping my toe in the water of differential forms. Since I have only limited knowledge of the real differential geometry involved, I'm having trouble putting things together. Reading the treatment of Stokes' theorems in Buck's Advanced Calculus , I find the following lemma relating the 2-form to the normal: $$\int_{\Sigma} f dy dz + g dz dx + hdx dy \\= \int_{D} f \frac{ \partial(y,z) }{ \partial(u,v) }du dv + g \frac{ \partial(z,x) }{ \partial(u,v) }du dv + h\frac{ \partial(x,y) }{ \partial(u,v) }du dv\\ = \int_D \vec{U} \cdot \mathbf{n}(u,v)\, dA$$ where $\vec{U} = (f,g,h)$ and $\sigma: \mathbb{R}^2 \to \mathbb{R}^3$ is a parameterized surface in $\mathbb{R}^3$ given by $\sigma(u,v) = (x(u,v), y(u,v), z(u,v))$. The referring to $dudv$ as ""$dA$"" is a little confusing to me. ""$dA$"" seems to be something that if we integrated it, we should end up with the surface area of $\Sigma$ (I would like to think of it like ""parameterizing by arclength"".) But if we integrate $dudv$, we end up with the area in the parameter space. OK, so perhaps the $dA$ refers to the area of the parameter space. But then here on Wikipedia I find the statement $$\iiint _V (\nabla \cdot F) dV = \iint_{S} F \cdot n \,dS.$$ which I read as translating $$\int_V (f_x + g_y + h_z) dx dy dz \\=\int_V d(f\, dy dz + g \,dz dx + h \,dx dy) + \int_{\partial V}f\, dy dz + g \,dz dx + h \,dx dy.$$ Surely this $dS$ here does not refer to any particular parameterization space...? Here $dS$ is something which, if we integrated it, would yield the surface area of $S$, right? Question 2: What 2-form would we integrate if we want the surface area of a parameterized surface in $\mathbb{R}^3$?",,"['multivariable-calculus', 'differential-geometry', 'vector-analysis']"
24,Study of Matrix Calculus,Study of Matrix Calculus,,"I need to study matrix calculus such as integration, differentiation, differentiation  of functions of determinants and inverse matrices and then also other matrix based calculations such as decomposition techniques. And I need to know these operations for matrices defined in the complex domain rather than real valued. Could someone please recommend me a book or a comprehensive online resource ? Thank you","I need to study matrix calculus such as integration, differentiation, differentiation  of functions of determinants and inverse matrices and then also other matrix based calculations such as decomposition techniques. And I need to know these operations for matrices defined in the complex domain rather than real valued. Could someone please recommend me a book or a comprehensive online resource ? Thank you",,"['calculus', 'linear-algebra', 'matrices', 'reference-request', 'multivariable-calculus']"
25,Average value using triple integrals,Average value using triple integrals,,"Find the average value of the function $f(x,y,z)=xyz$ over the tetrahedron with vertices, $(0,0,0),(1,0,0), (1,1,0), \text{and }(1,1,1)$","Find the average value of the function $f(x,y,z)=xyz$ over the tetrahedron with vertices, $(0,0,0),(1,0,0), (1,1,0), \text{and }(1,1,1)$",,['multivariable-calculus']
26,surfaces $F$ and $G$ are tangent if and only if $\nabla{F}\times{\nabla{G}}=\mathbf{0}$,surfaces  and  are tangent if and only if,F G \nabla{F}\times{\nabla{G}}=\mathbf{0},"Suppose that two surfaces are given by the equations $F(x,y,z)=c$ and $G(x,y,z)=k$.  Moreover, suppose that these surfaces intersect at the point $(x_0,y_0,z_0)$.  Show that the surfaces are tangent at $(x_0,y_0,z_z)$ if and only if $$\nabla{F}\times\nabla{G}=\mathbf{0}$$ Here's my attempt... $\Rightarrow$ Since the surfaces are tangent, they have the same tangent plane.  These tangent planes can be represented by different equations, but must only differ by a scalar.  Let $\mathbf{x}_0 = (x_0,y_0,z_0), a,b \in\mathbb{R}$. Thus $$a[F_x(\mathbf{x_0})(x-x_o)+F_y(\mathbf{x_0})(y-y_o)+F_z(\mathbf{x_0})(z-z_o)]=b[G_x(\mathbf{x_0})(x-x_o)+G_y(\mathbf{x_0})(y-y_o)+G_z(\mathbf{x_0})(z-z_o)]$$ Which implies  $$F_i(\mathbf{x}_0)=\frac{b}{a}G_i(\mathbf{x}_0), i=x,y,z.$$ Since these partial derivatives only differ by a scalar, they are linearly dependent and thus parallel.  Since they are parallel, $\nabla{F}\times\nabla{G}=\mathbf{0}$ Now the other way is where I'm having trouble $\Leftarrow$  Suppose $\nabla{F}\times\nabla{G}=\mathbf{0}$.  This implies three things; either $\nabla{F}=\mathbf{0}, \nabla{G}=\mathbf{0}$ (or both), or $\nabla{F}||\nabla{G}$.  If either $\nabla{F}$ or $\nabla{G}=\mathbf{0}$, the surface represented by $F$ or $G$ is a plane.  It is from here I get lost as to where to go.  If I assume that they are parallel, I feel as though my only path is the reverse of $\Rightarrow$, which I don't believe is sufficient.","Suppose that two surfaces are given by the equations $F(x,y,z)=c$ and $G(x,y,z)=k$.  Moreover, suppose that these surfaces intersect at the point $(x_0,y_0,z_0)$.  Show that the surfaces are tangent at $(x_0,y_0,z_z)$ if and only if $$\nabla{F}\times\nabla{G}=\mathbf{0}$$ Here's my attempt... $\Rightarrow$ Since the surfaces are tangent, they have the same tangent plane.  These tangent planes can be represented by different equations, but must only differ by a scalar.  Let $\mathbf{x}_0 = (x_0,y_0,z_0), a,b \in\mathbb{R}$. Thus $$a[F_x(\mathbf{x_0})(x-x_o)+F_y(\mathbf{x_0})(y-y_o)+F_z(\mathbf{x_0})(z-z_o)]=b[G_x(\mathbf{x_0})(x-x_o)+G_y(\mathbf{x_0})(y-y_o)+G_z(\mathbf{x_0})(z-z_o)]$$ Which implies  $$F_i(\mathbf{x}_0)=\frac{b}{a}G_i(\mathbf{x}_0), i=x,y,z.$$ Since these partial derivatives only differ by a scalar, they are linearly dependent and thus parallel.  Since they are parallel, $\nabla{F}\times\nabla{G}=\mathbf{0}$ Now the other way is where I'm having trouble $\Leftarrow$  Suppose $\nabla{F}\times\nabla{G}=\mathbf{0}$.  This implies three things; either $\nabla{F}=\mathbf{0}, \nabla{G}=\mathbf{0}$ (or both), or $\nabla{F}||\nabla{G}$.  If either $\nabla{F}$ or $\nabla{G}=\mathbf{0}$, the surface represented by $F$ or $G$ is a plane.  It is from here I get lost as to where to go.  If I assume that they are parallel, I feel as though my only path is the reverse of $\Rightarrow$, which I don't believe is sufficient.",,"['multivariable-calculus', 'partial-derivative']"
27,help with a problem on implicit fn theorem,help with a problem on implicit fn theorem,,"Let $f: {\mathbb R}^{n+k} \rightarrow {\mathbb R}^n$ be a $C^1$ map. Suppose that $f(a)=0$ and $Df(a)$ has rank n . Show that if $c$ is a point in ${\mathbb R}^n$ sufficiently close to $0$, then the equation $f(x)=c$ has a solution. I can only see that since $Df(a)$ has rank n , hence by implicit function thm, some n of the variables are actually functions of the remaining k variables, but I can't see how to use it. Thanks in advance.","Let $f: {\mathbb R}^{n+k} \rightarrow {\mathbb R}^n$ be a $C^1$ map. Suppose that $f(a)=0$ and $Df(a)$ has rank n . Show that if $c$ is a point in ${\mathbb R}^n$ sufficiently close to $0$, then the equation $f(x)=c$ has a solution. I can only see that since $Df(a)$ has rank n , hence by implicit function thm, some n of the variables are actually functions of the remaining k variables, but I can't see how to use it. Thanks in advance.",,['multivariable-calculus']
28,"Show $\partial _x \int_{(x_0, y_0)}^{(x,y)}P(s,t)ds + Q(s,t)dt = P(x,y)$",Show,"\partial _x \int_{(x_0, y_0)}^{(x,y)}P(s,t)ds + Q(s,t)dt = P(x,y)","There is a theorem from advanced calculus that I'm trying to prove. Suppose $P(x,y)$, $Q(x,y) \in C^2$ on a simply connected domain $D$, and suppose that $P_y = Q_x$ (i.e. $\omega = Pdx + Qdy$ is closed), we have that $$\partial _x \int_{(x_0, y_0)}^{(x,y)}P(s,t)ds + Q(s,t)dt = P(x,y)  \\ \partial _y \int_{(x_0, y_0)}^{(x,y)}P(s,t)ds + Q(s,t)dt = Q(x,y).$$ (Note the integral is well-defined since $\omega$ closed means that the integral is path-independent.) I looked for a proof of this property of the derivatives but didn't find it. If anyone could provide a reference or a quick demonstration, it would be much appreciated. Thanks!","There is a theorem from advanced calculus that I'm trying to prove. Suppose $P(x,y)$, $Q(x,y) \in C^2$ on a simply connected domain $D$, and suppose that $P_y = Q_x$ (i.e. $\omega = Pdx + Qdy$ is closed), we have that $$\partial _x \int_{(x_0, y_0)}^{(x,y)}P(s,t)ds + Q(s,t)dt = P(x,y)  \\ \partial _y \int_{(x_0, y_0)}^{(x,y)}P(s,t)ds + Q(s,t)dt = Q(x,y).$$ (Note the integral is well-defined since $\omega$ closed means that the integral is path-independent.) I looked for a proof of this property of the derivatives but didn't find it. If anyone could provide a reference or a quick demonstration, it would be much appreciated. Thanks!",,"['reference-request', 'multivariable-calculus', 'derivatives']"
29,Evaluate a triple integral,Evaluate a triple integral,,"Given $f(x,y,z) = \sqrt{1+(x^2+y^2+z^2)^{\frac{3}{2}}}$ and $D=\{(x,y,z) : x^2+y^2+z^2 \leq r^2\}$, evaluate $\int\int\int_D f(x,y,z)dxdydz$. I've thought that spherical coordinates would be the best way to go, then $x=\rho \cos\theta \sin\varphi$, $y=\rho\sin\theta\sin\varphi$ and $z=\rho\cos\varphi$. Now because of $x^2+y^2+z^2 \leq r^2$ follows $\rho^2 \cos^2\theta\sin^2\varphi  + \rho^2\sin^2\theta\sin^2\varphi + \rho^2\cos^2\varphi = \rho^2\sin^2\varphi(\cos^2\theta+\sin^2\theta)+\rho^2\cos^2\varphi = \rho^2 \leq r^2 \implies -r \leq \rho \leq r$. If both 'dissapear' from the equation above does that mean that $0\leq \theta \leq 2\pi$ and and $0\leq \varphi \leq 2\pi$?  Otherwise, how can i get the limits for $\theta$ and $\varphi$? And the first question doesn't seem likely since if i put the former limits i'd have $\displaystyle\int_{0}^r\int_0^{2\pi}\int_0^{2\pi} \sqrt{1+\rho^\frac{3}{2}}\;\rho^2\sin\varphi\;d\varphi d\theta d\rho = -\displaystyle\int_{0}^r\int_0^{2\pi}\bigg(\sqrt{1+\rho^\frac{3}{2}}\;\rho^2\cos 2\pi - \sqrt{1+\rho^\frac{3}{2}}\;\rho^2\cos 0\bigg) d\theta d\rho = 0$ Another question: does it matter if i evaulate $\int\int\int_D f(\varphi,\theta,d\rho)d\varphi d\theta d\rho$ or $\int\int\int_D f(\varphi,\theta,d\rho)d\theta d\varphi d\rho$?","Given $f(x,y,z) = \sqrt{1+(x^2+y^2+z^2)^{\frac{3}{2}}}$ and $D=\{(x,y,z) : x^2+y^2+z^2 \leq r^2\}$, evaluate $\int\int\int_D f(x,y,z)dxdydz$. I've thought that spherical coordinates would be the best way to go, then $x=\rho \cos\theta \sin\varphi$, $y=\rho\sin\theta\sin\varphi$ and $z=\rho\cos\varphi$. Now because of $x^2+y^2+z^2 \leq r^2$ follows $\rho^2 \cos^2\theta\sin^2\varphi  + \rho^2\sin^2\theta\sin^2\varphi + \rho^2\cos^2\varphi = \rho^2\sin^2\varphi(\cos^2\theta+\sin^2\theta)+\rho^2\cos^2\varphi = \rho^2 \leq r^2 \implies -r \leq \rho \leq r$. If both 'dissapear' from the equation above does that mean that $0\leq \theta \leq 2\pi$ and and $0\leq \varphi \leq 2\pi$?  Otherwise, how can i get the limits for $\theta$ and $\varphi$? And the first question doesn't seem likely since if i put the former limits i'd have $\displaystyle\int_{0}^r\int_0^{2\pi}\int_0^{2\pi} \sqrt{1+\rho^\frac{3}{2}}\;\rho^2\sin\varphi\;d\varphi d\theta d\rho = -\displaystyle\int_{0}^r\int_0^{2\pi}\bigg(\sqrt{1+\rho^\frac{3}{2}}\;\rho^2\cos 2\pi - \sqrt{1+\rho^\frac{3}{2}}\;\rho^2\cos 0\bigg) d\theta d\rho = 0$ Another question: does it matter if i evaulate $\int\int\int_D f(\varphi,\theta,d\rho)d\varphi d\theta d\rho$ or $\int\int\int_D f(\varphi,\theta,d\rho)d\theta d\varphi d\rho$?",,['multivariable-calculus']
30,Verify solution: Is this gradient correct?,Verify solution: Is this gradient correct?,,"So I want to calculate minus the gradient of $$\Phi_1=\sum_{l=0}^{\infty}f(l)r^{l}P_l(\cos(\theta))$$ where $P_n$ is the $n$-th Legendre polynomial then we have $$-\nabla \Phi_1=-\left(\begin{array}{c}\sum_{l=1}^{\infty}f(l)\cdot l \cdot r^{l-1}P_l(\cos(\theta)\\ -\sum_{l=0}^{\infty}f(l)\cdot (l+1) \cdot r^{l-1}\frac{\cos(\theta)P_n(\cos(\theta))-P_{n-1}(\cos(\theta))}{\sin(\theta)}\end{array}\right)?$$ Or is it rather: $$-\nabla \Phi_1=-\left(\begin{array}{c}\sum_{l=1}^{\infty}f(l)\cdot l \cdot r^{l-1}P_l(\cos(\theta))\\  \sum_{l=1}^{\infty}f(l)\cdot l \cdot r^{l-1}\frac{\cos(\theta)P_n(\cos(\theta))-P_{n-1}(\cos(\theta))}{\sin(\theta)}\end{array}\right)$$ Notice that the only change that there is, is in the second component, the first might be right. $f$ is a function that does neither depend on $r$ nor does it depend on $\theta$.","So I want to calculate minus the gradient of $$\Phi_1=\sum_{l=0}^{\infty}f(l)r^{l}P_l(\cos(\theta))$$ where $P_n$ is the $n$-th Legendre polynomial then we have $$-\nabla \Phi_1=-\left(\begin{array}{c}\sum_{l=1}^{\infty}f(l)\cdot l \cdot r^{l-1}P_l(\cos(\theta)\\ -\sum_{l=0}^{\infty}f(l)\cdot (l+1) \cdot r^{l-1}\frac{\cos(\theta)P_n(\cos(\theta))-P_{n-1}(\cos(\theta))}{\sin(\theta)}\end{array}\right)?$$ Or is it rather: $$-\nabla \Phi_1=-\left(\begin{array}{c}\sum_{l=1}^{\infty}f(l)\cdot l \cdot r^{l-1}P_l(\cos(\theta))\\  \sum_{l=1}^{\infty}f(l)\cdot l \cdot r^{l-1}\frac{\cos(\theta)P_n(\cos(\theta))-P_{n-1}(\cos(\theta))}{\sin(\theta)}\end{array}\right)$$ Notice that the only change that there is, is in the second component, the first might be right. $f$ is a function that does neither depend on $r$ nor does it depend on $\theta$.",,"['calculus', 'real-analysis']"
31,How to prove $f'(a)=0$?,How to prove ?,f'(a)=0,"Let $f:I\to\mathbb{R}^n$ be a differentiable function, where $I\subset\mathbb{R}$ is a interval. For each $c\in \mathbb{R}^n$, define $X_c=\{x\in I;\;\;f(x)=c\}$. The problem asks to show that if there exists $c\in \mathbb{R}^n$ such that $a\in I\cap \left (X_c\right)'$, then $f'(a)=0$. I don't know how to start, so I would like hints. Can someone help me? Thanks.","Let $f:I\to\mathbb{R}^n$ be a differentiable function, where $I\subset\mathbb{R}$ is a interval. For each $c\in \mathbb{R}^n$, define $X_c=\{x\in I;\;\;f(x)=c\}$. The problem asks to show that if there exists $c\in \mathbb{R}^n$ such that $a\in I\cap \left (X_c\right)'$, then $f'(a)=0$. I don't know how to start, so I would like hints. Can someone help me? Thanks.",,"['analysis', 'multivariable-calculus']"
32,basic integral inequality,basic integral inequality,,"Consider $0 < r <R $. I have a function $u \in C_{0}^{\infty}(B(x_0,R))$ such that $u=1$ on $\overline{B(x_0,r)}$  . Consider $y \in \partial B(0,1)$ (fixed) . My book says : $$ 1 \leq  \int_{r}^{R} |\frac{d}{ds}u(sy)| \ ds$$ I tried to begin with $\frac{d}{ds}u(sy) = \nabla u(sy) . y$ and this is zero if $s \leq r$. but I dont know what happen if $s > r$ ... Someone can give me a hand to prove the inequality ? thanks in advance","Consider $0 < r <R $. I have a function $u \in C_{0}^{\infty}(B(x_0,R))$ such that $u=1$ on $\overline{B(x_0,r)}$  . Consider $y \in \partial B(0,1)$ (fixed) . My book says : $$ 1 \leq  \int_{r}^{R} |\frac{d}{ds}u(sy)| \ ds$$ I tried to begin with $\frac{d}{ds}u(sy) = \nabla u(sy) . y$ and this is zero if $s \leq r$. but I dont know what happen if $s > r$ ... Someone can give me a hand to prove the inequality ? thanks in advance",,"['real-analysis', 'multivariable-calculus', 'partial-differential-equations']"
33,Is there a transformation that makes $\frac{1}{t}(e^{t^2}-1)e^\frac{1}{t}(1-\frac{1}{t})$ equal to $ t^{-3}(e^{t^2}-e^t)+t^{-2}-t^{-1}$?,Is there a transformation that makes  equal to ?,\frac{1}{t}(e^{t^2}-1)e^\frac{1}{t}(1-\frac{1}{t})  t^{-3}(e^{t^2}-e^t)+t^{-2}-t^{-1},"Is there a transformation that makes $\frac{1}{t}(e^{t^2}-1)e^{\frac{1}{t}}(1-\frac{1}{t})$ equal to $t^{-3}(e^{t^2}-e^t)+t^{-2}-t^{-1}$? The reason I ask is that for the integral $$\int_1^t{\int_0^t{\frac{e^{(tx)/y}}{y^{3}}dx}dy}$$, I get the former while Apostol gets the latter (Apostol Calculus Vol 2, 1st Edition, Section 2.9, #6). There graphs look different, so I am guessing not. Still I feel like I solved this problem, but I'd like to know how Apostol got his answer in the form he presented. First the inner integral: $$y^{-3}e^{1/y}\int_0^t{e^{tx}dx}$$ y^(-3)e^(1/y)1/t[e^(tx)]_x=0^x=t y^(-3)e^(1/y)1/t(e^t^2-1) Now for the outer integral: 1/t(e^t^2-1)*Integrate[y^(-3)e^(1/y),{y,1,t}] u-substitution: u=1/y, du=-1/y^2 -Integrate[e^u*u,u] IBP: f=u, dg=e^u df=du, g=e^u -(u*e^u-Integral[e^u,u])=e^u-u*e^u Plug u back in: e^(1/y)(1-1/y)|_1^t Combine it all for final result: 1/t(e^t^2-1)e^(1/t)(1-1/t)","Is there a transformation that makes $\frac{1}{t}(e^{t^2}-1)e^{\frac{1}{t}}(1-\frac{1}{t})$ equal to $t^{-3}(e^{t^2}-e^t)+t^{-2}-t^{-1}$? The reason I ask is that for the integral $$\int_1^t{\int_0^t{\frac{e^{(tx)/y}}{y^{3}}dx}dy}$$, I get the former while Apostol gets the latter (Apostol Calculus Vol 2, 1st Edition, Section 2.9, #6). There graphs look different, so I am guessing not. Still I feel like I solved this problem, but I'd like to know how Apostol got his answer in the form he presented. First the inner integral: $$y^{-3}e^{1/y}\int_0^t{e^{tx}dx}$$ y^(-3)e^(1/y)1/t[e^(tx)]_x=0^x=t y^(-3)e^(1/y)1/t(e^t^2-1) Now for the outer integral: 1/t(e^t^2-1)*Integrate[y^(-3)e^(1/y),{y,1,t}] u-substitution: u=1/y, du=-1/y^2 -Integrate[e^u*u,u] IBP: f=u, dg=e^u df=du, g=e^u -(u*e^u-Integral[e^u,u])=e^u-u*e^u Plug u back in: e^(1/y)(1-1/y)|_1^t Combine it all for final result: 1/t(e^t^2-1)e^(1/t)(1-1/t)",,['multivariable-calculus']
34,Linear dependence of multivariable functions,Linear dependence of multivariable functions,,"It is well known that the Wronskian is a great tool for checking the linear dependence between a set of functions of one variable. Is there a similar way of checking linear dependance between two functions of two variables (e.g. $P(x,y),Q(x,y)$)? Thanks.","It is well known that the Wronskian is a great tool for checking the linear dependence between a set of functions of one variable. Is there a similar way of checking linear dependance between two functions of two variables (e.g. $P(x,y),Q(x,y)$)? Thanks.",,"['linear-algebra', 'ordinary-differential-equations', 'multivariable-calculus']"
35,Can't see how this function is differentiable Spivak's Calculus on Manifolds Exercise 2-4,Can't see how this function is differentiable Spivak's Calculus on Manifolds Exercise 2-4,,"The problem is as follows: Let $g$ be a continuous real-valued function on the unit circle $\{x \in \mathbb{R}^2 : \lvert x \rvert = 1\}$ such that $g(0,1) = g(1,0) = 0$ and $g(-x) = -g(x)$. Define $f: \mathbb{R}^2 \to \mathbb{R}$ by $$f(x) =           \begin{cases}            \lvert x \rvert \cdot g\left(\frac{x}{\lvert x \rvert}\right) & : x \neq 0\\             0 & : x = 0            \end{cases}$$ The question is: If $x \in \mathbb{R}^2$ and $h: \mathbb{R} \to \mathbb{R}$ is defined by $h(t) = f(tx)$, show that $h$ is differentiable. I'm not sure which definition of differentiation I should use–the usual single variable one or the general one defined in the chapter? I went with the single variable definition since $h$ is a single variable function and I get: $$\lim_{k \to 0} \frac{h(t+k) -h(t)}{k} = \lim_{k \to 0} \frac{f((t+k)x) - f(tx)}{k} = \lim_{k \to 0} \frac{\lvert tx-kx \rvert \cdot g\left( \frac{tx-kx}{\lvert tx-kx \rvert} \right) - \lvert tx \rvert \cdot g\left( \frac{tx}{\lvert tx \rvert} \right)}{k}$$ I don't know what to do after this. We don't know if g is differentiable. If I use the other limit definition, I run into the same problem. If the information given for $g$ is supposed to somehow imply that $g$ is differentiable, I don't see it. Any hints?","The problem is as follows: Let $g$ be a continuous real-valued function on the unit circle $\{x \in \mathbb{R}^2 : \lvert x \rvert = 1\}$ such that $g(0,1) = g(1,0) = 0$ and $g(-x) = -g(x)$. Define $f: \mathbb{R}^2 \to \mathbb{R}$ by $$f(x) =           \begin{cases}            \lvert x \rvert \cdot g\left(\frac{x}{\lvert x \rvert}\right) & : x \neq 0\\             0 & : x = 0            \end{cases}$$ The question is: If $x \in \mathbb{R}^2$ and $h: \mathbb{R} \to \mathbb{R}$ is defined by $h(t) = f(tx)$, show that $h$ is differentiable. I'm not sure which definition of differentiation I should use–the usual single variable one or the general one defined in the chapter? I went with the single variable definition since $h$ is a single variable function and I get: $$\lim_{k \to 0} \frac{h(t+k) -h(t)}{k} = \lim_{k \to 0} \frac{f((t+k)x) - f(tx)}{k} = \lim_{k \to 0} \frac{\lvert tx-kx \rvert \cdot g\left( \frac{tx-kx}{\lvert tx-kx \rvert} \right) - \lvert tx \rvert \cdot g\left( \frac{tx}{\lvert tx \rvert} \right)}{k}$$ I don't know what to do after this. We don't know if g is differentiable. If I use the other limit definition, I run into the same problem. If the information given for $g$ is supposed to somehow imply that $g$ is differentiable, I don't see it. Any hints?",,"['real-analysis', 'multivariable-calculus']"
36,curl(fF) with Einstein Summation Notation,curl(fF) with Einstein Summation Notation,,"I considered the $k$th component of $\text{curl $f\mathbf{F}$}$. $f$ is a scalar field and $\mathbf{F}$ a vector field. $\color{green}{[}\nabla \times (fF)\color{green}{]} _{\LARGE{\color{green}{k}}} = \epsilon_{ij\LARGE{\color{green}{k}}}\partial_i(f\mathbf{F})_j $ $= \epsilon_{ij\LARGE{\color{green}{k}}}\partial_i(fF_j) \qquad \qquad \qquad \qquad (\text{since $\mathbf{(F)}_j :=$ the $j$th component of $\mathbf{F} = F_j$})$ $= \epsilon_{ij\LARGE{\color{green}{k}}}(F_j\partial_if + f\partial_iF_j) \qquad \qquad  (\text{since $f$ scalar})$ $= \underbrace{\epsilon_{ij\LARGE{\color{green}{k}}}F_j\partial_if}_{\Large{\bigstar}} + f\underbrace{{\epsilon_{ij\LARGE{\color{green}{k}}}\partial_iF_j}}_{\LARGE{\color{green}{[}\nabla \times \mathbf{F}\color{green}{]}_{\LARGE{\color{green}{k}}}}} $ Hereafter, I refer only to the term with the star underneath.  Since [$\color{#007FFF}{F_j}$ corresponding to $\color{#007FFF}{\mathbf{F}}$] appears before [$\color{#FF00FF}{\partial_if}$ corresponding to $\color{#FF00FF}{\nabla f}$], thus ${\epsilon_{ij\LARGE{\color{green}{k}}}\color{#007FFF}{F_j}\color{#FF00FF}{\partial_if}} = {\color{green}{[}\color{#007FFF}{\mathbf{F}} \times \color{#FF00FF}{\nabla f}\color{green}{]} _{\LARGE{\color{green}{k}}}}$. But the answer states $\color{green}{[}\nabla f \times \mathbf{F}\color{green}{]} _{\LARGE{\color{green}{k}}}$. What went wrong? $\large{\text{Supplement to Andrew D's response :}}$ Here's my understanding of your answer : In ${\epsilon_{ij\LARGE{\color{green}{k}}}F_j\partial_if}, \; {(i, j, \LARGE{\color{green}{k}})}$ (in the subscript of the Levi-Civita symbol) denotes the order of the components. So the $i$th component must appear first, and the $j$th component second. However, since $(i, j, k) = \color{brown}{(j, k, i)}$, therefore  $\epsilon_{ijk} = \epsilon_{\color{brown}{\LARGE{jki}}}$. $\color{brown}{\text{Now, $j$ precedes $i$, so wouldn't this result in the wrong order of the components?}}$ $\large{\text{2nd Supplement to Andrew D's Comment beneath his Answer :}}$ $\color{#3EB489}{\text{The variable in the permutation succeeding the variable that's not summed}}$ corresponds to the first component to appear. Here, $k$ denotes the component being analysed so is not summed. Since I am looking at $\color{brown}{(j, k, i)}$, $\color{#3EB489}{i}$ succeeds $k$ so the $\color{#3EB489}{i}$th component is the first. Therefore, ${\epsilon_{ijk}F_j\color{#3EB489}{\partial_{\LARGE{i}}}f}$ = ${\epsilon_{\color{brown}{\LARGE{jki}}}\color{#3EB489}{\partial_{\LARGE{i}}}F_jf} = \color{green}{[}\color{#3EB489}{\nabla f} \times \mathbf{F}\color{green}{]} _{\LARGE{\color{green}{k}}} $ However, this appears to discord with Steven Stadnicki's 2nd comment, according to which: $ {\epsilon_{\color{brown}{\LARGE{jki}}}F_j\partial_if} = {\color{green}{[}\mathbf{F} \times \nabla f\color{green}{]} _{\LARGE{\color{green}{k}}}}$?","I considered the $k$th component of $\text{curl $f\mathbf{F}$}$. $f$ is a scalar field and $\mathbf{F}$ a vector field. $\color{green}{[}\nabla \times (fF)\color{green}{]} _{\LARGE{\color{green}{k}}} = \epsilon_{ij\LARGE{\color{green}{k}}}\partial_i(f\mathbf{F})_j $ $= \epsilon_{ij\LARGE{\color{green}{k}}}\partial_i(fF_j) \qquad \qquad \qquad \qquad (\text{since $\mathbf{(F)}_j :=$ the $j$th component of $\mathbf{F} = F_j$})$ $= \epsilon_{ij\LARGE{\color{green}{k}}}(F_j\partial_if + f\partial_iF_j) \qquad \qquad  (\text{since $f$ scalar})$ $= \underbrace{\epsilon_{ij\LARGE{\color{green}{k}}}F_j\partial_if}_{\Large{\bigstar}} + f\underbrace{{\epsilon_{ij\LARGE{\color{green}{k}}}\partial_iF_j}}_{\LARGE{\color{green}{[}\nabla \times \mathbf{F}\color{green}{]}_{\LARGE{\color{green}{k}}}}} $ Hereafter, I refer only to the term with the star underneath.  Since [$\color{#007FFF}{F_j}$ corresponding to $\color{#007FFF}{\mathbf{F}}$] appears before [$\color{#FF00FF}{\partial_if}$ corresponding to $\color{#FF00FF}{\nabla f}$], thus ${\epsilon_{ij\LARGE{\color{green}{k}}}\color{#007FFF}{F_j}\color{#FF00FF}{\partial_if}} = {\color{green}{[}\color{#007FFF}{\mathbf{F}} \times \color{#FF00FF}{\nabla f}\color{green}{]} _{\LARGE{\color{green}{k}}}}$. But the answer states $\color{green}{[}\nabla f \times \mathbf{F}\color{green}{]} _{\LARGE{\color{green}{k}}}$. What went wrong? $\large{\text{Supplement to Andrew D's response :}}$ Here's my understanding of your answer : In ${\epsilon_{ij\LARGE{\color{green}{k}}}F_j\partial_if}, \; {(i, j, \LARGE{\color{green}{k}})}$ (in the subscript of the Levi-Civita symbol) denotes the order of the components. So the $i$th component must appear first, and the $j$th component second. However, since $(i, j, k) = \color{brown}{(j, k, i)}$, therefore  $\epsilon_{ijk} = \epsilon_{\color{brown}{\LARGE{jki}}}$. $\color{brown}{\text{Now, $j$ precedes $i$, so wouldn't this result in the wrong order of the components?}}$ $\large{\text{2nd Supplement to Andrew D's Comment beneath his Answer :}}$ $\color{#3EB489}{\text{The variable in the permutation succeeding the variable that's not summed}}$ corresponds to the first component to appear. Here, $k$ denotes the component being analysed so is not summed. Since I am looking at $\color{brown}{(j, k, i)}$, $\color{#3EB489}{i}$ succeeds $k$ so the $\color{#3EB489}{i}$th component is the first. Therefore, ${\epsilon_{ijk}F_j\color{#3EB489}{\partial_{\LARGE{i}}}f}$ = ${\epsilon_{\color{brown}{\LARGE{jki}}}\color{#3EB489}{\partial_{\LARGE{i}}}F_jf} = \color{green}{[}\color{#3EB489}{\nabla f} \times \mathbf{F}\color{green}{]} _{\LARGE{\color{green}{k}}} $ However, this appears to discord with Steven Stadnicki's 2nd comment, according to which: $ {\epsilon_{\color{brown}{\LARGE{jki}}}F_j\partial_if} = {\color{green}{[}\mathbf{F} \times \nabla f\color{green}{]} _{\LARGE{\color{green}{k}}}}$?",,[]
37,Wave equation from linear Euler equations,Wave equation from linear Euler equations,,"The linear Euler equations for pressure $p = p_0 + p_a$, density $\rho = \rho_0 + \rho_a$, and velocity field $\mathbf{v}$ are \begin{gather} \frac{\partial\rho_a}{\partial t} + \rho_0\nabla\cdot\mathbf{v} = 0 \\ \rho_0\frac{\partial\mathbf{v}}{\partial t} + \nabla p = \mathbf{0}. \end{gather} By taking the gradient of the first equation, and substituting the expression for $\nabla p$ from the second equation (while also using $p_a = c^2\rho_a$), I find that \begin{align} \frac{\partial\mathbf{v}}{\partial t} = c^2\nabla^2\mathbf{v}. \end{align} However, this text (pages 17-18) says that my wave equation form only holds when $\nabla\times\mathbf{v}=\mathbf{0}$.  In general, it states, only  \begin{align} \frac{\partial\left(\nabla\cdot\mathbf{v}\right)}{\partial t} = c^2\nabla^2\left(\nabla\cdot\mathbf{v}\right) \end{align} holds.  What then did I miss?","The linear Euler equations for pressure $p = p_0 + p_a$, density $\rho = \rho_0 + \rho_a$, and velocity field $\mathbf{v}$ are \begin{gather} \frac{\partial\rho_a}{\partial t} + \rho_0\nabla\cdot\mathbf{v} = 0 \\ \rho_0\frac{\partial\mathbf{v}}{\partial t} + \nabla p = \mathbf{0}. \end{gather} By taking the gradient of the first equation, and substituting the expression for $\nabla p$ from the second equation (while also using $p_a = c^2\rho_a$), I find that \begin{align} \frac{\partial\mathbf{v}}{\partial t} = c^2\nabla^2\mathbf{v}. \end{align} However, this text (pages 17-18) says that my wave equation form only holds when $\nabla\times\mathbf{v}=\mathbf{0}$.  In general, it states, only  \begin{align} \frac{\partial\left(\nabla\cdot\mathbf{v}\right)}{\partial t} = c^2\nabla^2\left(\nabla\cdot\mathbf{v}\right) \end{align} holds.  What then did I miss?",,"['multivariable-calculus', 'partial-differential-equations', 'fluid-dynamics']"
38,Travel distance of a particle,Travel distance of a particle,,"How can we show that a projectile fired at an angle $\theta$ with initial speed $v_0$ travels a total distance $\frac{v_0^2}{g}\sin2\theta$ before hitting the ground? The way I set it up is: direction is $(\cos\theta,\sin\theta)$. By using Newton's law, $$r(t) = (0,-\frac{1}{2} gt^2)+tv_0(\cos\theta,\sin\theta) = \frac{v_0^2}{g}\sin2\theta$$ I tried to solve the LHS so that it matches the RHS, the total distance. But obviously, there is something wrong with the way I tried to approach it. But cannot see it. Any hints and pointers in terms of setting it up would be great. Also I do not want the full solution but rather few small hints to be able to set it up correctly.","How can we show that a projectile fired at an angle $\theta$ with initial speed $v_0$ travels a total distance $\frac{v_0^2}{g}\sin2\theta$ before hitting the ground? The way I set it up is: direction is $(\cos\theta,\sin\theta)$. By using Newton's law, $$r(t) = (0,-\frac{1}{2} gt^2)+tv_0(\cos\theta,\sin\theta) = \frac{v_0^2}{g}\sin2\theta$$ I tried to solve the LHS so that it matches the RHS, the total distance. But obviously, there is something wrong with the way I tried to approach it. But cannot see it. Any hints and pointers in terms of setting it up would be great. Also I do not want the full solution but rather few small hints to be able to set it up correctly.",,"['multivariable-calculus', 'physics']"
39,Derive the curl in general coordinates.,Derive the curl in general coordinates.,,"Many mathematical physics texts inform the orthogonal curvilinar coordinates system and differential operators. But, they don't use the precise mathematical method for the derivation of the curl and divergence in orthogonal curvilinear coordinats. They only give the intuative pictures. Example: Arfken-texts. I want to find the precise derivation of these forms.","Many mathematical physics texts inform the orthogonal curvilinar coordinates system and differential operators. But, they don't use the precise mathematical method for the derivation of the curl and divergence in orthogonal curvilinear coordinats. They only give the intuative pictures. Example: Arfken-texts. I want to find the precise derivation of these forms.",,"['multivariable-calculus', 'vector-analysis']"
40,Line integal of vector field depends only on the shape and orientation of the curve?,Line integal of vector field depends only on the shape and orientation of the curve?,,"My question is about line integral of vector field in multi-variable calculus. As you know well, the line integral of a vector field over some parametrized curve $X(t)$ is independent of reparametrization preserving same orientation. It sounds that it does not ensure that the value of line integral is not completely determined by the image of curve, but equivalent class of parametrization. I am wondering that if there is a theorem that for two $C^1$-parametrizations $X_1:[a,b]\rightarrow \mathbb{R}$, $X_2:[c,d]\rightarrow\mathbb{R}$ of some fixed curve which having same orienatation, is there some $C^1$-function $g:[c,d]\rightarrow[a,b]$ such that $X_1 \circ g=X_2$? If not, can you have some easy example which gives different values according to two different orientation preserving parametrization of some curve? More precisely, Let $F:\mathbb{R^n}\rightarrow \mathbb{R^n}$ be a continuous-function and C be a curve (the set of points) parametrized by $X_1(t):[a,b]\rightarrow\mathbb{R^n}$ and it is $C^1$-curve (differentiable and its partial derivatives are continuous). Is there another $C^1$ parametrization $X_2(t):[c,d]\rightarrow\mathbb{R^n}$ of $C$ having the same orientation with $X_1(t)$ but $\int_{X_1}F\cdot d\mathbf{s}\ne\int_{X_2}F\cdot d\mathbf{s}$? My question comes from the confusing notation that many author write the line integral over the curve(the set of points) not explicating any parametrization. Any comment will be greatly appreciated! Similar question can be asked in line integral in complex analysis.","My question is about line integral of vector field in multi-variable calculus. As you know well, the line integral of a vector field over some parametrized curve $X(t)$ is independent of reparametrization preserving same orientation. It sounds that it does not ensure that the value of line integral is not completely determined by the image of curve, but equivalent class of parametrization. I am wondering that if there is a theorem that for two $C^1$-parametrizations $X_1:[a,b]\rightarrow \mathbb{R}$, $X_2:[c,d]\rightarrow\mathbb{R}$ of some fixed curve which having same orienatation, is there some $C^1$-function $g:[c,d]\rightarrow[a,b]$ such that $X_1 \circ g=X_2$? If not, can you have some easy example which gives different values according to two different orientation preserving parametrization of some curve? More precisely, Let $F:\mathbb{R^n}\rightarrow \mathbb{R^n}$ be a continuous-function and C be a curve (the set of points) parametrized by $X_1(t):[a,b]\rightarrow\mathbb{R^n}$ and it is $C^1$-curve (differentiable and its partial derivatives are continuous). Is there another $C^1$ parametrization $X_2(t):[c,d]\rightarrow\mathbb{R^n}$ of $C$ having the same orientation with $X_1(t)$ but $\int_{X_1}F\cdot d\mathbf{s}\ne\int_{X_2}F\cdot d\mathbf{s}$? My question comes from the confusing notation that many author write the line integral over the curve(the set of points) not explicating any parametrization. Any comment will be greatly appreciated! Similar question can be asked in line integral in complex analysis.",,"['calculus', 'complex-analysis', 'multivariable-calculus']"
41,How The Jacobian of the transformation can be shown to not depend on $X_i$ or $\bar X $ and is equal to the constant $n$,How The Jacobian of the transformation can be shown to not depend on  or  and is equal to the constant,X_i \bar X  n,"Transform the random variables, $X_i$, $i=1,2,\ldots,n$ to $$ \begin{align} Y_1 & =\bar X \\ Y_2 & =X_2-\bar X \\ Y_3 & = X_3-\bar X \\ & {}\  \vdots \\ Y_n & =X_n-\bar X \end{align} $$ Find the Jacobian of transformation.","Transform the random variables, $X_i$, $i=1,2,\ldots,n$ to $$ \begin{align} Y_1 & =\bar X \\ Y_2 & =X_2-\bar X \\ Y_3 & = X_3-\bar X \\ & {}\  \vdots \\ Y_n & =X_n-\bar X \end{align} $$ Find the Jacobian of transformation.",,"['calculus', 'multivariable-calculus']"
42,Using the arc length function to find a parameterization of C in terms of s,Using the arc length function to find a parameterization of C in terms of s,,"The problem: Find the arc length function $s(t)$ for the curve defined by $\vec r(t)$. Then use this result to find a parametrization of $C$ in terms of $s$. $$\vec r(t) = a\cos^3t\,\hat i + a\sin^3t\,\hat j+ \hat k,\quad 0\le t\le 2\pi$$ Attempt at Solution: For the first part, I have produced $$\vec r'(t) = -3a \cos^2t \sin t\,\hat i + 3a\sin^2t \cos t \,\hat j$$ and $$|\vec r'(t)| = 3a\sin t \cos t$$ then $$\int^t_0|\vec r'(u)| du = \left. -\frac 32a\cos^2u \right|^t_0 = \frac 32a\sin^2t$$ At this point I need to find a parametrization in terms of $s$. There may be some identities that could help make this cleaner, but, given my current knowledge, this is what I have come up with: $$t = \arcsin\left(\sqrt \frac 23 \sqrt s\right)$$ If I was successful in solving for t, then the answer would be: $$\vec r(t(s)) = a \cos^3 \left[\arcsin\left(\sqrt \frac 23 \sqrt s\right)\right]\,\hat i + a\sin^3\left[\arcsin\left(\sqrt \frac 23 \sqrt s\right)\right]\,\hat j + \hat k$$ I hope I have given enough information and I greatly appreciate any advice or direction.","The problem: Find the arc length function $s(t)$ for the curve defined by $\vec r(t)$. Then use this result to find a parametrization of $C$ in terms of $s$. $$\vec r(t) = a\cos^3t\,\hat i + a\sin^3t\,\hat j+ \hat k,\quad 0\le t\le 2\pi$$ Attempt at Solution: For the first part, I have produced $$\vec r'(t) = -3a \cos^2t \sin t\,\hat i + 3a\sin^2t \cos t \,\hat j$$ and $$|\vec r'(t)| = 3a\sin t \cos t$$ then $$\int^t_0|\vec r'(u)| du = \left. -\frac 32a\cos^2u \right|^t_0 = \frac 32a\sin^2t$$ At this point I need to find a parametrization in terms of $s$. There may be some identities that could help make this cleaner, but, given my current knowledge, this is what I have come up with: $$t = \arcsin\left(\sqrt \frac 23 \sqrt s\right)$$ If I was successful in solving for t, then the answer would be: $$\vec r(t(s)) = a \cos^3 \left[\arcsin\left(\sqrt \frac 23 \sqrt s\right)\right]\,\hat i + a\sin^3\left[\arcsin\left(\sqrt \frac 23 \sqrt s\right)\right]\,\hat j + \hat k$$ I hope I have given enough information and I greatly appreciate any advice or direction.",,['multivariable-calculus']
43,"For 1 form $\xi$, $F^{*}(d\xi)=d(F^{*}\xi)$","For 1 form ,",\xi F^{*}(d\xi)=d(F^{*}\xi),"Let $F:M \to N$ be a mapping of surfaces, and $\xi$ be a function. I want to show the following identity. $$F^{*}(d\xi)=d(F^{*}\xi)$$ What I did : Fix a tangent vector $v$. $$F^{*}(d\xi)(v)=(d\xi)(F_{*}v)=\frac{d}{dt}(\xi(y(t))_{t=0}$$ where $y(t)=F(\alpha(t)), \alpha'(0)=v$ I used that $y'(t)=F_{*}(\alpha'(t))$. Next, consider the right term. $$d(F^{*}\xi)(v)=\frac{d}{dt}(F^{*}\xi)(\alpha(t))_{t=0}=\frac{d}{dt}(\xi(F_{*}\alpha(t))_{t=0}$$ where $\alpha'(0)=v$. But $y'(0)=F_{*}(\alpha'(0)) \neq \frac{d}{dt}(F_{*}\alpha(t))_{t=0}$. What is my mistake?","Let $F:M \to N$ be a mapping of surfaces, and $\xi$ be a function. I want to show the following identity. $$F^{*}(d\xi)=d(F^{*}\xi)$$ What I did : Fix a tangent vector $v$. $$F^{*}(d\xi)(v)=(d\xi)(F_{*}v)=\frac{d}{dt}(\xi(y(t))_{t=0}$$ where $y(t)=F(\alpha(t)), \alpha'(0)=v$ I used that $y'(t)=F_{*}(\alpha'(t))$. Next, consider the right term. $$d(F^{*}\xi)(v)=\frac{d}{dt}(F^{*}\xi)(\alpha(t))_{t=0}=\frac{d}{dt}(\xi(F_{*}\alpha(t))_{t=0}$$ where $\alpha'(0)=v$. But $y'(0)=F_{*}(\alpha'(0)) \neq \frac{d}{dt}(F_{*}\alpha(t))_{t=0}$. What is my mistake?",,"['analysis', 'differential-geometry', 'multivariable-calculus']"
44,Does setting derivative to zero suffice always for minimization of convex functions?,Does setting derivative to zero suffice always for minimization of convex functions?,,"I have this convex function in $X$, given by $Trace(AX^TBX)$ where $A$, $B$ are p.s.d and all entries are real. Now if I had a linear function $l(X)$ that prevents a trivial zero-matrix solution for $X$ in the minimization of  $Trace(AX^TBX)$ w.r.t $X$. Now would setting the first-derivative(gradient) of $Trace(AX^TBX) + \lambda l(X)$ w.r.t $X$ as zero and solving for $X$  suffice to get the optimal solution for $X$ given the convexity, or are there more conditions/directional derivatives and so forth to consider? For your convenience, the gradient is $(B \otimes A^T + B^T \otimes A)vecX + \lambda \nabla_Xl(X)$ and the gradient, $\nabla_Xl(X)$ does not contain $X$ in it, but has other terms. $\lambda$ is the Lagrange multiplier for enforcing the linear constraint.","I have this convex function in $X$, given by $Trace(AX^TBX)$ where $A$, $B$ are p.s.d and all entries are real. Now if I had a linear function $l(X)$ that prevents a trivial zero-matrix solution for $X$ in the minimization of  $Trace(AX^TBX)$ w.r.t $X$. Now would setting the first-derivative(gradient) of $Trace(AX^TBX) + \lambda l(X)$ w.r.t $X$ as zero and solving for $X$  suffice to get the optimal solution for $X$ given the convexity, or are there more conditions/directional derivatives and so forth to consider? For your convenience, the gradient is $(B \otimes A^T + B^T \otimes A)vecX + \lambda \nabla_Xl(X)$ and the gradient, $\nabla_Xl(X)$ does not contain $X$ in it, but has other terms. $\lambda$ is the Lagrange multiplier for enforcing the linear constraint.",,"['multivariable-calculus', 'optimization', 'derivatives', 'convex-analysis', 'calculus-of-variations']"
45,Jacobian determinant and orientation,Jacobian determinant and orientation,,"So in Jacobian determinant, it is often said that it gives information about whether Jacobian matrix changes orientation, but I cannot get what orientation exactly in this context.","So in Jacobian determinant, it is often said that it gives information about whether Jacobian matrix changes orientation, but I cannot get what orientation exactly in this context.",,"['linear-algebra', 'multivariable-calculus']"
46,Second point of intersection - parameter removes second solution?,Second point of intersection - parameter removes second solution?,,"$$\mathbf r_1 (t) = (t^2 - t, t^2 + t) \\ \mathbf r_2(u) = (u+u^2, u-u^2)$$ I'm trying to find two of the intersection points, but I'm lost as to how to approach the question. Is it possible to remove the parameter? $$\mathbf r_1 - \mathbf r_2 = (t^2-t-u-u^2)\mathbf{\hat i} + (t^2+t-u+u^2)\mathbf{\hat j} = \mathbf0 $$So, $$ t^2 -t - u - u^2 = 0 \\t^2+t-u+u^2 = 0 $$ Solving, $$ t=0,u=0 \\ t=-1, u = 1$$ When I sub back into the equations, I don't get the same points for $t=-1, u =1$, so I'm doing something wrong...The answers say intersection points are $(0,0)$ and $(2,0)$. Where does $(2,0)$ come from?","$$\mathbf r_1 (t) = (t^2 - t, t^2 + t) \\ \mathbf r_2(u) = (u+u^2, u-u^2)$$ I'm trying to find two of the intersection points, but I'm lost as to how to approach the question. Is it possible to remove the parameter? $$\mathbf r_1 - \mathbf r_2 = (t^2-t-u-u^2)\mathbf{\hat i} + (t^2+t-u+u^2)\mathbf{\hat j} = \mathbf0 $$So, $$ t^2 -t - u - u^2 = 0 \\t^2+t-u+u^2 = 0 $$ Solving, $$ t=0,u=0 \\ t=-1, u = 1$$ When I sub back into the equations, I don't get the same points for $t=-1, u =1$, so I'm doing something wrong...The answers say intersection points are $(0,0)$ and $(2,0)$. Where does $(2,0)$ come from?",,['multivariable-calculus']
47,smooth approximate parameterization to polygonal boundary,smooth approximate parameterization to polygonal boundary,,"I can ""almost"" parameterize the boundary of a square using $${\bf r}(t) = (\cos t)^{1/p} {\bf i} + (\sin t)^{1/p} {\bf j},$$ $0\leq t\leq 2 \pi$, and $p$ is odd. This parameterization is smooth (or at least $C^1$), and of course is the unit ball in the $L^p$ norm. Letting $p\rightarrow \infty$ makes our approximation better. Now suppose I have a triangle, or in general, a convex polygon with vertices $a_1, a_2, \dots, a_n$. Is there some relatively simple, explicit smooth approximate parameterization of the boundary? It should, of course, have some tweakable parameter that allows for convergence, like $p$ in the example I gave. The simpler, and the ""cuter"", the better. Another issue is the following. With my parameterization of the square, if I want to get a list of roughly equally distributed points on the boundary, I cannot do this by letting $t=2 \pi i/N$. In that case the points collect around the corners on the square. Is there some nice way of fixing this for the square, and for my polygon in general?","I can ""almost"" parameterize the boundary of a square using $${\bf r}(t) = (\cos t)^{1/p} {\bf i} + (\sin t)^{1/p} {\bf j},$$ $0\leq t\leq 2 \pi$, and $p$ is odd. This parameterization is smooth (or at least $C^1$), and of course is the unit ball in the $L^p$ norm. Letting $p\rightarrow \infty$ makes our approximation better. Now suppose I have a triangle, or in general, a convex polygon with vertices $a_1, a_2, \dots, a_n$. Is there some relatively simple, explicit smooth approximate parameterization of the boundary? It should, of course, have some tweakable parameter that allows for convergence, like $p$ in the example I gave. The simpler, and the ""cuter"", the better. Another issue is the following. With my parameterization of the square, if I want to get a list of roughly equally distributed points on the boundary, I cannot do this by letting $t=2 \pi i/N$. In that case the points collect around the corners on the square. Is there some nice way of fixing this for the square, and for my polygon in general?",,"['geometry', 'multivariable-calculus', 'intuition']"
48,"Lagrange multiplier constrain critical point: $f(x,y)=x^2+y$ with the constraint $ x^2+y^2\le1$",Lagrange multiplier constrain critical point:  with the constraint,"f(x,y)=x^2+y  x^2+y^2\le1","When using Lagrange multipliers in an inequality, $$ f(x,y) = x^2+y $$ with the constraint $$ x^2+y^2 \leq 1. $$ I have to find the critical points inside the ""disk"" right? I've done $$ f_x = 2x \Rightarrow 2x = 0 \Rightarrow x = 0, \\f_y = 1. $$ So there is no critical point and I should treat this as an equality. Am I right?","When using Lagrange multipliers in an inequality, with the constraint I have to find the critical points inside the ""disk"" right? I've done So there is no critical point and I should treat this as an equality. Am I right?"," f(x,y) = x^2+y   x^2+y^2 \leq 1.   f_x = 2x \Rightarrow 2x = 0 \Rightarrow x = 0, \\f_y = 1. ","['multivariable-calculus', 'lagrange-multiplier']"
49,Calculus book for people who know limits,Calculus book for people who know limits,,"I have the probably slightly unusual background of being quite comfortable with real numbers, functions, limits, sequences, series, etc, but having no knowledge of calculus beyond the definitions of continuity and the derivative. Could anyone recommend an interesting book on calculus for someone with this sort of background? As opposed to, say, standard texts like Spivak's Calculus or Burkill's A First Course in Mathematical Analysis , which spend a lot of time teaching numbers, functions, limits, and sequences, and not so much time focusing on calculus per se , I'm interested in something that would show the beauty and perhaps a few of the applications of calculus itself, probably including multivariable calculus. Concise and reasonable rigorous (i.e. not a cookbook) would be pros too. Thank you!","I have the probably slightly unusual background of being quite comfortable with real numbers, functions, limits, sequences, series, etc, but having no knowledge of calculus beyond the definitions of continuity and the derivative. Could anyone recommend an interesting book on calculus for someone with this sort of background? As opposed to, say, standard texts like Spivak's Calculus or Burkill's A First Course in Mathematical Analysis , which spend a lot of time teaching numbers, functions, limits, and sequences, and not so much time focusing on calculus per se , I'm interested in something that would show the beauty and perhaps a few of the applications of calculus itself, probably including multivariable calculus. Concise and reasonable rigorous (i.e. not a cookbook) would be pros too. Thank you!",,"['calculus', 'reference-request', 'multivariable-calculus']"
50,Finding a direct basis for tangent space of piece with boundary of an oriented manifold.,Finding a direct basis for tangent space of piece with boundary of an oriented manifold.,,"I have the following definition (from Hubbard's vector calculus book) for an oriented boundary of piece with boundary of an oriented manifold: Let $M$ be a $k$ dimensional manifold oriented by $\Omega$ and $P$ a piece with boundary of $M$. Let $x$ be a point of the smooth boundary $\partial^{ \ S}_MP$ and let $\vec{V}_{\text{out}}\in T_xM$ be an outward pointing bector. Then the function $\Omega^\partial : \mathcal{B}(T_x\partial P)\to\left\{+1,-1\right\}$ given by   $$ \Omega_x^\partial(\vec{v}_1,...,\vec{v}_{k-1}) = \Omega_x(\vec{V}_{\text{out}},\vec{v}_1,...,\vec{v}_{k-1}) $$   defines an orientation on the smooth boundary $\partial_M^{ \ S}P,$ where $\vec{v}_1,...,\vec{v}_{k-1}$ is an ordered basis of $T_x\partial_M^{ \ S}P$. I'm working on a problem that asks me to find a basis for the $T_x\partial P$ that is direct to a certain orientation (given by an elementary 3-form). My question is this: When I choose a basis for $T_x\partial P$, does this basis also need to lie in $T_xM$? Also, are there restrictions to how I should choose $\vec{V}_{\text{out}}$? In other words, does $\vec{V}_{\text{out}}$ need only lie in $T_xM$ and not in $T_x\partial P$?","I have the following definition (from Hubbard's vector calculus book) for an oriented boundary of piece with boundary of an oriented manifold: Let $M$ be a $k$ dimensional manifold oriented by $\Omega$ and $P$ a piece with boundary of $M$. Let $x$ be a point of the smooth boundary $\partial^{ \ S}_MP$ and let $\vec{V}_{\text{out}}\in T_xM$ be an outward pointing bector. Then the function $\Omega^\partial : \mathcal{B}(T_x\partial P)\to\left\{+1,-1\right\}$ given by   $$ \Omega_x^\partial(\vec{v}_1,...,\vec{v}_{k-1}) = \Omega_x(\vec{V}_{\text{out}},\vec{v}_1,...,\vec{v}_{k-1}) $$   defines an orientation on the smooth boundary $\partial_M^{ \ S}P,$ where $\vec{v}_1,...,\vec{v}_{k-1}$ is an ordered basis of $T_x\partial_M^{ \ S}P$. I'm working on a problem that asks me to find a basis for the $T_x\partial P$ that is direct to a certain orientation (given by an elementary 3-form). My question is this: When I choose a basis for $T_x\partial P$, does this basis also need to lie in $T_xM$? Also, are there restrictions to how I should choose $\vec{V}_{\text{out}}$? In other words, does $\vec{V}_{\text{out}}$ need only lie in $T_xM$ and not in $T_x\partial P$?",,['differential-geometry']
51,"Need help on setting up a double integral over a given region, in polar coordinates","Need help on setting up a double integral over a given region, in polar coordinates",,"The task is as follows: Find double integral (in polar coordinates) over region $D$ of $f(x,y)  = (x^2 + y)$ by $dxdy$ . The region $D$ is bounded by a circle of radius $3$ and a circle of radius $2$ , with upper angle $a1$ being $\frac{\pi}{4}$ and lower angle $a2$ being $\frac{\pi}{6}$ The region is as the sketch below: I need help on setting up the double integral. My work so far: (1) Let $x = r \cos \theta$ and $y = r \sin \theta$ (2) The double integral in polar coordinates should be something in nature of $$ \int_{\theta_1} ^ {\theta_2} \int_{r_1}^{r_2} f(r\cos\theta,r\sin\theta) r dr d{\theta}$$ (3) Base on the sketch of the region, $r$ should go from $2$ to $3$ , But I don't see the lower and upper bounds for $\theta$ .  Should it be from $0$ to $a1 + a2$ ? Or should I consider two integrals instead?  One with $\theta$ from $0$ to $a1$ and another integral with $\theta$ from $0$ to $a2$ ? I just want to make sure that my original thoughts are right (or not). Would someone please help me on this? Thank you.","The task is as follows: Find double integral (in polar coordinates) over region of by . The region is bounded by a circle of radius and a circle of radius , with upper angle being and lower angle being The region is as the sketch below: I need help on setting up the double integral. My work so far: (1) Let and (2) The double integral in polar coordinates should be something in nature of (3) Base on the sketch of the region, should go from to , But I don't see the lower and upper bounds for .  Should it be from to ? Or should I consider two integrals instead?  One with from to and another integral with from to ? I just want to make sure that my original thoughts are right (or not). Would someone please help me on this? Thank you.","D f(x,y)
 = (x^2 + y) dxdy D 3 2 a1 \frac{\pi}{4} a2 \frac{\pi}{6} x = r \cos \theta y = r \sin \theta  \int_{\theta_1} ^ {\theta_2} \int_{r_1}^{r_2} f(r\cos\theta,r\sin\theta) r dr d{\theta} r 2 3 \theta 0 a1 + a2 \theta 0 a1 \theta 0 a2",['multivariable-calculus']
52,3D line integral question,3D line integral question,,"Let $C$ be the curve of intersection of the cylinder $x^2 + y^2 = 1$ and the surface $z = xy$, oriented counterclockwise around the cylinder. Compute the integral $\int_C y\,dx + z\,dy + x\,dz$.","Let $C$ be the curve of intersection of the cylinder $x^2 + y^2 = 1$ and the surface $z = xy$, oriented counterclockwise around the cylinder. Compute the integral $\int_C y\,dx + z\,dy + x\,dz$.",,"['calculus', 'integration', 'multivariable-calculus']"
53,Integral sign with circle (AND arrow on the circle) through it,Integral sign with circle (AND arrow on the circle) through it,,"I know from multivariable calculus that the integral sign with circle in its middle means integrating along a closed path. So when I encountered in complex analysis the above integral sign but with an arrow on the circle to indicate orientation , I automatically assumed oriented closed contour. But now on reading my textbook carefully, it only says that the arrowed circle actually means oriented contour (and doesn't say anything about whether the contour is necessarily closed). So what does the arrowed circle really mean? Can it really also refer to a smooth oriented arc (i.e. not a closed curve)?","I know from multivariable calculus that the integral sign with circle in its middle means integrating along a closed path. So when I encountered in complex analysis the above integral sign but with an arrow on the circle to indicate orientation , I automatically assumed oriented closed contour. But now on reading my textbook carefully, it only says that the arrowed circle actually means oriented contour (and doesn't say anything about whether the contour is necessarily closed). So what does the arrowed circle really mean? Can it really also refer to a smooth oriented arc (i.e. not a closed curve)?",,"['complex-analysis', 'integration', 'multivariable-calculus', 'notation']"
54,Can Green's theorem be used in a plane other than the xy-plane?,Can Green's theorem be used in a plane other than the xy-plane?,,"In the following 2D case, Green's theorem solves the following problem: $$\vec{F}=\langle{xy+\ln{(\sin{e^{x})},x^2+e^{y^2}}}\rangle$$ $$\oint_C\vec{F}\cdot{d\vec{r}}=\iint_Dx\space{dA}$$ where C is the unit circle $x^2+y^2=1$, and D is the unit disk $x^2+y^2\le{1}$. But what if instead the problem were: $$\vec{F}=\langle{\frac{\sqrt{2}}{2}(xy+\ln{(\sin{e^x}}),x^2+e^{y^2},-\frac{\sqrt{2}}{2}(xy+\ln{(\sin{e^x}})}\rangle$$ $$\oint_C\vec{F}\cdot{d\vec{r}}$$ And $C$ is instead the unit circle $\vec{r}(t)=\langle{\frac{\sqrt{2}}{2}\cos{t},\sin{t},-\frac{\sqrt{2}}{2}\cos{t}}\rangle$. This is the same problem as the first one, but rotated 45 degrees about the y-axis. I know I could solve the second problem by simply rotating the path and the vector field back 45 degrees. But is there any way to apply Green's theorem to the second problem directly?","In the following 2D case, Green's theorem solves the following problem: $$\vec{F}=\langle{xy+\ln{(\sin{e^{x})},x^2+e^{y^2}}}\rangle$$ $$\oint_C\vec{F}\cdot{d\vec{r}}=\iint_Dx\space{dA}$$ where C is the unit circle $x^2+y^2=1$, and D is the unit disk $x^2+y^2\le{1}$. But what if instead the problem were: $$\vec{F}=\langle{\frac{\sqrt{2}}{2}(xy+\ln{(\sin{e^x}}),x^2+e^{y^2},-\frac{\sqrt{2}}{2}(xy+\ln{(\sin{e^x}})}\rangle$$ $$\oint_C\vec{F}\cdot{d\vec{r}}$$ And $C$ is instead the unit circle $\vec{r}(t)=\langle{\frac{\sqrt{2}}{2}\cos{t},\sin{t},-\frac{\sqrt{2}}{2}\cos{t}}\rangle$. This is the same problem as the first one, but rotated 45 degrees about the y-axis. I know I could solve the second problem by simply rotating the path and the vector field back 45 degrees. But is there any way to apply Green's theorem to the second problem directly?",,['multivariable-calculus']
55,Vector Field Generating Variation Along Curve,Vector Field Generating Variation Along Curve,,"I'm learning a proof of the fact that length extremising curves are geodesics of the Levi-Civita connection, and have found something I don't understand. The argument states the following. Suppose $\gamma$ is an arbitrary curve in a manifold $M$ and $N$ is an arbitrary vector field along $\gamma$ which vanishes at the endpoints. Then there must exist a variation $\tilde{\gamma}$ of $\gamma$ such that $\gamma'$ and $N$ are the pushforwards of the coordinate vector fields on the domain of the variation. I can't see how this is true, because it would immediately imply that all vector fields commute (I think!). Am I missing something or is it false? If it's wrong, does anyone know how to prove that statement that length extremizing implies geodesic. The relevant lectures notes from which I've taken this argument are here , page 32. Thanks in advance!","I'm learning a proof of the fact that length extremising curves are geodesics of the Levi-Civita connection, and have found something I don't understand. The argument states the following. Suppose $\gamma$ is an arbitrary curve in a manifold $M$ and $N$ is an arbitrary vector field along $\gamma$ which vanishes at the endpoints. Then there must exist a variation $\tilde{\gamma}$ of $\gamma$ such that $\gamma'$ and $N$ are the pushforwards of the coordinate vector fields on the domain of the variation. I can't see how this is true, because it would immediately imply that all vector fields commute (I think!). Am I missing something or is it false? If it's wrong, does anyone know how to prove that statement that length extremizing implies geodesic. The relevant lectures notes from which I've taken this argument are here , page 32. Thanks in advance!",,"['multivariable-calculus', 'differential-geometry', 'manifolds', 'calculus-of-variations', 'geodesic']"
56,Find $y$-Lipschitz constant,Find -Lipschitz constant,y,"$$f(x,y)=x^3e^{-xy^2}, 0\leq x\leq a, y\in \mathbb R, a>0$$ I need to find $K>0$ such that $$|f(x,y_1)-f(x, y_2)|\leq K|y_1-y_2|$$ for all $0\leq x\leq a$ and $y_1,y_2\in \mathbb R$ I did this $$|x^3e^{-xy_1^2}-x^3e^{-xy_2^2}|=x^3|e^{-xy_1^2}-e^{-xy_2^2}|\leq a^3|e^{-xy_1^2}-e^{-xy_2^2}|$$ now I don't know what to do.","$$f(x,y)=x^3e^{-xy^2}, 0\leq x\leq a, y\in \mathbb R, a>0$$ I need to find $K>0$ such that $$|f(x,y_1)-f(x, y_2)|\leq K|y_1-y_2|$$ for all $0\leq x\leq a$ and $y_1,y_2\in \mathbb R$ I did this $$|x^3e^{-xy_1^2}-x^3e^{-xy_2^2}|=x^3|e^{-xy_1^2}-e^{-xy_2^2}|\leq a^3|e^{-xy_1^2}-e^{-xy_2^2}|$$ now I don't know what to do.",,"['real-analysis', 'multivariable-calculus']"
57,"Finding partial derivatives for equations expressed in terms of $z$ where $z=f(x,y)$ to find tangent plane",Finding partial derivatives for equations expressed in terms of  where  to find tangent plane,"z z=f(x,y)","I am having troubles finding partial derivatives. If $f(x,y)=2x^2+y^2$ then, $$f_x=4x$$ $$f_y=2y$$ That's simple enough. But when I see a $z$ in the equation, I get stumped. I know $z=f(x,y)$. I don't really see the process. For example, if $z=2x^2+y^2$ then do we differentiate both sides with respect to $x$ like this? $$f_x:\frac{dz}{dx}=4x$$ $$f_y:\frac{dz}{dy}=2y$$ Even worse, what am I supposed to do for something like this? Same thing? $$x+y^2+z^3=3$$ $$z^3=-x-y^2+3$$ $$f_x:3z^2\frac{dz}{dx}=-1$$ $$f_x:\frac{dz}{dx}=\frac{-1}{3z^2}$$ Likewise, $$f_y:\frac{dz}{dy}=\frac{-2y}{3z^2}$$ I'm pretty sure that's wrong but I don't know why. Can someone please help me understand? Thanks. Edit Some context for the last example (it's from a homework problem): Find the equation of the tangent plane to the surface with equation $x+y^2+z^3=3$ at the point (2,1,0). I know that the equation for the tangent plane is $$z=f(x_0,y_0)+[f_x(x_0,y_0)](x-x_0)+[f_y(x_0,y_0)](y-y_0)$$ Since the $f_x$ and $f_y$ found above contain z, do I plug in $z_0$? So I'm trying to find $f_x$ and $f_y$. Edit 2 I get it now","I am having troubles finding partial derivatives. If $f(x,y)=2x^2+y^2$ then, $$f_x=4x$$ $$f_y=2y$$ That's simple enough. But when I see a $z$ in the equation, I get stumped. I know $z=f(x,y)$. I don't really see the process. For example, if $z=2x^2+y^2$ then do we differentiate both sides with respect to $x$ like this? $$f_x:\frac{dz}{dx}=4x$$ $$f_y:\frac{dz}{dy}=2y$$ Even worse, what am I supposed to do for something like this? Same thing? $$x+y^2+z^3=3$$ $$z^3=-x-y^2+3$$ $$f_x:3z^2\frac{dz}{dx}=-1$$ $$f_x:\frac{dz}{dx}=\frac{-1}{3z^2}$$ Likewise, $$f_y:\frac{dz}{dy}=\frac{-2y}{3z^2}$$ I'm pretty sure that's wrong but I don't know why. Can someone please help me understand? Thanks. Edit Some context for the last example (it's from a homework problem): Find the equation of the tangent plane to the surface with equation $x+y^2+z^3=3$ at the point (2,1,0). I know that the equation for the tangent plane is $$z=f(x_0,y_0)+[f_x(x_0,y_0)](x-x_0)+[f_y(x_0,y_0)](y-y_0)$$ Since the $f_x$ and $f_y$ found above contain z, do I plug in $z_0$? So I'm trying to find $f_x$ and $f_y$. Edit 2 I get it now",,"['multivariable-calculus', 'partial-derivative']"
58,Computing $\iiint_\mathbb{R^3} e^{-x^2-y^2-z^2}dxdydz$ using substitution,Computing  using substitution,\iiint_\mathbb{R^3} e^{-x^2-y^2-z^2}dxdydz,"Consider this integral: $$\iiint_\mathbb{R^3} e^{-x^2-y^2-z^2}dxdydz$$ How would you compute it? I already solved this problem this way: $$\iiint_\mathbb{R^3} e^{-x^2-y^2-z^2}dxdydz = \left( \int_\infty^\infty e^{-x^2} \right)^3 = \pi^{3/2}$$ But I wanted to find it using substitution (spherical coordinates) but this is all I could do: $$\iiint_\mathbb{R^3} e^{-x^2-y^2-z^2}dxdydz = \lim_{j\,\rightarrow\infty}\int_0^jdu\int_0^\pi dv\int_0^{2\pi} e^{-u^2}u^2\sin(v) dw=$$ $$=2\pi\lim_{j\,\rightarrow\infty}\int_0^jdu\int_0^\pi e^{-u^2}u^2\sin(v)dv=4\pi\lim_{j\,\rightarrow\infty}\int_0^je^{-u^2}u^2du$$ But it doesn't get me anywhere. Help would be very appreciated, thanks in advance.","Consider this integral: $$\iiint_\mathbb{R^3} e^{-x^2-y^2-z^2}dxdydz$$ How would you compute it? I already solved this problem this way: $$\iiint_\mathbb{R^3} e^{-x^2-y^2-z^2}dxdydz = \left( \int_\infty^\infty e^{-x^2} \right)^3 = \pi^{3/2}$$ But I wanted to find it using substitution (spherical coordinates) but this is all I could do: $$\iiint_\mathbb{R^3} e^{-x^2-y^2-z^2}dxdydz = \lim_{j\,\rightarrow\infty}\int_0^jdu\int_0^\pi dv\int_0^{2\pi} e^{-u^2}u^2\sin(v) dw=$$ $$=2\pi\lim_{j\,\rightarrow\infty}\int_0^jdu\int_0^\pi e^{-u^2}u^2\sin(v)dv=4\pi\lim_{j\,\rightarrow\infty}\int_0^je^{-u^2}u^2du$$ But it doesn't get me anywhere. Help would be very appreciated, thanks in advance.",,['real-analysis']
59,Gradient in Cylindrical ccordinates,Gradient in Cylindrical ccordinates,,How can I express grad of $\Phi$ in cylindrical coordinates?. In fact I wanted to check the vector $\nabla\Phi$ is perpendicular to a surface $\Phi=c$ where $c$ is a constant. Where to start? Thank you.,How can I express grad of $\Phi$ in cylindrical coordinates?. In fact I wanted to check the vector $\nabla\Phi$ is perpendicular to a surface $\Phi=c$ where $c$ is a constant. Where to start? Thank you.,,"['calculus', 'multivariable-calculus']"
60,Taking a complicated partial derivative,Taking a complicated partial derivative,,"Mostly I believe in math. However I have trouble in my economic textbook (which really should be right). I have the following equation: $$ u(c,d)=\left(ac^{\frac{1-\gamma}{\theta}}+b d^{\frac{1-\gamma}{\theta}}\right)^{\frac{\theta}{1-\gamma}}$$ where $\theta=(1-\gamma)/(1-1/\psi)$, $a\in (0,1),b\in (0,1), c>0, d>0, \gamma>0$, and $\psi\in(0,\infty)$. Is the derivative of $u$ wrt. $c$ really   $$ u_c(c,d)=ac^{\frac{1-\gamma}{\theta}-1}\left(ac^{\frac{1-\gamma}{\theta}}+b d^{\frac{1-\gamma}{\theta}}\right)^{\frac{\theta}{1-\gamma}-1}=ac^{-1/\psi}u(c,d)^{1/\psi}$$ If so, can someone tell me why? I would write it as $$ u_c(c,d)=ac^{\frac{1-\gamma}{\theta}-1}\left(ac^{\frac{1-\gamma}{\theta}}+b d^{\frac{1-\gamma}{\theta}}\right)^{\frac{\theta}{1-\gamma}-1}=ac^{-1/\psi}u(c,d)^{-1}$$ Hope to hear from someone, thanks in advance.","Mostly I believe in math. However I have trouble in my economic textbook (which really should be right). I have the following equation: $$ u(c,d)=\left(ac^{\frac{1-\gamma}{\theta}}+b d^{\frac{1-\gamma}{\theta}}\right)^{\frac{\theta}{1-\gamma}}$$ where $\theta=(1-\gamma)/(1-1/\psi)$, $a\in (0,1),b\in (0,1), c>0, d>0, \gamma>0$, and $\psi\in(0,\infty)$. Is the derivative of $u$ wrt. $c$ really   $$ u_c(c,d)=ac^{\frac{1-\gamma}{\theta}-1}\left(ac^{\frac{1-\gamma}{\theta}}+b d^{\frac{1-\gamma}{\theta}}\right)^{\frac{\theta}{1-\gamma}-1}=ac^{-1/\psi}u(c,d)^{1/\psi}$$ If so, can someone tell me why? I would write it as $$ u_c(c,d)=ac^{\frac{1-\gamma}{\theta}-1}\left(ac^{\frac{1-\gamma}{\theta}}+b d^{\frac{1-\gamma}{\theta}}\right)^{\frac{\theta}{1-\gamma}-1}=ac^{-1/\psi}u(c,d)^{-1}$$ Hope to hear from someone, thanks in advance.",,"['multivariable-calculus', 'derivatives']"
61,A parameterized elliptical integral (Legendre Elliptical Integral),A parameterized elliptical integral (Legendre Elliptical Integral),,"$$ K(a,\theta)=\int_{0}^{\infty}\frac{t^{-a}}{1+2t\cos(\theta)+t^{2}}dt $$ For $$ -1<a<1;$$  $$-\pi<\theta<\pi$$ I know this integral to be a known tabulated Legendre elliptic integral, however the very fact that the numerator is parameterized completely throws a curveball. Using: $$  K(a,\theta)=\int_{0}^{\infty}\frac{t^{-a}}{(1+t^{2}) + 2t\cos(\theta)}dt $$ letting $2\gamma$ = $\theta$  $$ \rightarrow K(a,\theta)=\int_{0}^{\infty}\frac{t^{-a}}{(1+t^{2}) + 2t \cos(2\gamma)}dt $$ in which case the trig function can be later manipulated using the double angle identity, turning it into a sine function $$ \rightarrow K(a,\theta)=\int_{0}^{\infty}\frac{t^{-a}}{(1+t^{2}) + 2t \cos(2\gamma)}dt $$ $$ \rightarrow K(a,\theta)=\int_{0}^{\infty}\frac{t^{-a}}{(1+t^{2}) + 2t(1-(\sin(\gamma))^{2})}dt $$ So it does have a sine in the denominator that is sufficient for a Legendre Elliptical integral.  The rest is just solving for $k$ and simplifying the expression.  Which leaves me with the parameter $a$.  I have no idea what to do there. Any help is certainly appreciated.","$$ K(a,\theta)=\int_{0}^{\infty}\frac{t^{-a}}{1+2t\cos(\theta)+t^{2}}dt $$ For $$ -1<a<1;$$  $$-\pi<\theta<\pi$$ I know this integral to be a known tabulated Legendre elliptic integral, however the very fact that the numerator is parameterized completely throws a curveball. Using: $$  K(a,\theta)=\int_{0}^{\infty}\frac{t^{-a}}{(1+t^{2}) + 2t\cos(\theta)}dt $$ letting $2\gamma$ = $\theta$  $$ \rightarrow K(a,\theta)=\int_{0}^{\infty}\frac{t^{-a}}{(1+t^{2}) + 2t \cos(2\gamma)}dt $$ in which case the trig function can be later manipulated using the double angle identity, turning it into a sine function $$ \rightarrow K(a,\theta)=\int_{0}^{\infty}\frac{t^{-a}}{(1+t^{2}) + 2t \cos(2\gamma)}dt $$ $$ \rightarrow K(a,\theta)=\int_{0}^{\infty}\frac{t^{-a}}{(1+t^{2}) + 2t(1-(\sin(\gamma))^{2})}dt $$ So it does have a sine in the denominator that is sufficient for a Legendre Elliptical integral.  The rest is just solving for $k$ and simplifying the expression.  Which leaves me with the parameter $a$.  I have no idea what to do there. Any help is certainly appreciated.",,"['multivariable-calculus', 'integration', 'special-functions', 'improper-integrals']"
62,"Divergence free, smooth functions on unit circle.","Divergence free, smooth functions on unit circle.",,"I need to construct a divergence free, smooth, vector function on a unit circle such that $ \mathbf{u} = (u_1,u_2) = (0,0) $ on $\partial B$, and $\int\limits_B u_i \neq 0, \ i=1,2$. I was able to find/construct few such functions, but all of them fail to satisfy the last condition. Examples: 1) Hill's Spherical vortex $u = ( y(1-x^2-y^2), -x(1-x^2-y^2))$. 2) In 2d polar coordinates, $\nabla \cdot u = 0 \Rightarrow \frac{\partial (r v_r )}{\partial r} = - \frac{\partial v_\theta}{\partial \theta}$. So, e.g. let $ v_r = r^4(r-1)^3 \sin\theta \Rightarrow v_\theta = \left(5r^4(r-1)^3+ 3r^5(r-1)^2\right)\cos\theta $. From these, $v_1 = r^2(r-1)^2xy(4-7r), v_2 = r^2(r-1)^2\left( (r-1)y^2 + (8r-5)x^2 \right)$, where $r = \sqrt{x^2+y^2}$. But, $v_1, v_2$ have average value zero over the unit circle. So, is it always the case that average value is zero if we require smoothness? Anybody know any such function which does satisfy all conditions( maybe from Fluid Mechanics textbooks)? Any help on these would be appreciated. Thanks!","I need to construct a divergence free, smooth, vector function on a unit circle such that $ \mathbf{u} = (u_1,u_2) = (0,0) $ on $\partial B$, and $\int\limits_B u_i \neq 0, \ i=1,2$. I was able to find/construct few such functions, but all of them fail to satisfy the last condition. Examples: 1) Hill's Spherical vortex $u = ( y(1-x^2-y^2), -x(1-x^2-y^2))$. 2) In 2d polar coordinates, $\nabla \cdot u = 0 \Rightarrow \frac{\partial (r v_r )}{\partial r} = - \frac{\partial v_\theta}{\partial \theta}$. So, e.g. let $ v_r = r^4(r-1)^3 \sin\theta \Rightarrow v_\theta = \left(5r^4(r-1)^3+ 3r^5(r-1)^2\right)\cos\theta $. From these, $v_1 = r^2(r-1)^2xy(4-7r), v_2 = r^2(r-1)^2\left( (r-1)y^2 + (8r-5)x^2 \right)$, where $r = \sqrt{x^2+y^2}$. But, $v_1, v_2$ have average value zero over the unit circle. So, is it always the case that average value is zero if we require smoothness? Anybody know any such function which does satisfy all conditions( maybe from Fluid Mechanics textbooks)? Any help on these would be appreciated. Thanks!",,"['multivariable-calculus', 'partial-differential-equations', 'vector-analysis']"
63,Moment of inertia of a circle,Moment of inertia of a circle,,"A wire has the shape of the circle $x^2+y^2=a^2$. Determine the moment of inertia about a diameter if the density at $(x,y)$ is $|x|+|y|$ Thank you","A wire has the shape of the circle $x^2+y^2=a^2$. Determine the moment of inertia about a diameter if the density at $(x,y)$ is $|x|+|y|$ Thank you",,"['geometry', 'integration', 'multivariable-calculus', 'physics', 'polar-coordinates']"
64,Line integral of $F = r \times k$ on hemisphere,Line integral of  on hemisphere,F = r \times k,"Exam revision - Verify Stokes theorem directly by explicit calculation of the surface and line integrals for the hemisphere $r=c$, with $z \geq 0$, where $F = r \times k$ and $k$ is the unit vector in the positive z-direction. Attempt: $$F = r \times k = r Cos(\theta) \ \hat{\phi}$$ Where $r, \theta, \phi$ are defined in the normal way. We need to check that $$\int^{}_{C}F.dl = \int^{}_{S}\nabla \times F.ds$$ I'm getting stuck even on the line integral. $$dl = dr \ \hat{r} + r d\theta \ \hat{\theta} + r Sin(\theta) d\phi \ \hat{\phi}$$ $$\int^{\frac{\pi}{2}}_{0} r^2 Cos(\theta) Sin(\theta) d \phi$$ $$\frac{\pi}{2}r^2 Cos(\theta) Sin(\theta)$$ This seems wrong to me but I can't figure out why?","Exam revision - Verify Stokes theorem directly by explicit calculation of the surface and line integrals for the hemisphere $r=c$, with $z \geq 0$, where $F = r \times k$ and $k$ is the unit vector in the positive z-direction. Attempt: $$F = r \times k = r Cos(\theta) \ \hat{\phi}$$ Where $r, \theta, \phi$ are defined in the normal way. We need to check that $$\int^{}_{C}F.dl = \int^{}_{S}\nabla \times F.ds$$ I'm getting stuck even on the line integral. $$dl = dr \ \hat{r} + r d\theta \ \hat{\theta} + r Sin(\theta) d\phi \ \hat{\phi}$$ $$\int^{\frac{\pi}{2}}_{0} r^2 Cos(\theta) Sin(\theta) d \phi$$ $$\frac{\pi}{2}r^2 Cos(\theta) Sin(\theta)$$ This seems wrong to me but I can't figure out why?",,"['calculus', 'integration', 'multivariable-calculus', 'polar-coordinates']"
65,Multivariable calculus directional derivatives,Multivariable calculus directional derivatives,,I have a problem figuring out what happens after cos$\theta$+sin$\theta$ where did the $\sqrt{2}$ come from? where did cossin+cossin come from? Thank you!,I have a problem figuring out what happens after cos$\theta$+sin$\theta$ where did the $\sqrt{2}$ come from? where did cossin+cossin come from? Thank you!,,['multivariable-calculus']
66,Cylindrical coordinates where $z$ axis is not axis of symmetry.,Cylindrical coordinates where  axis is not axis of symmetry.,z,"I'm a little bit uncertain of how to set up the limits of integration when the axis of symmetry of the region is not centered at $z$ (this is for cylindrical coordinates). The region is bounded by $(x-1)^2+y^2=1$ and $x^2+y^2+z^2=4$. This is my attempt: Let $x=r\cos\theta$ and $y=r\sin\theta$. We are bounded on $z$ by $\pm\sqrt{4-r^2}$. We take $0\leq r\leq 2\cos\theta$ and $-\pi/2\leq \theta \leq \pi/2$ to account for the fact that the cylinder is not centered with the $z$ axis (shift on the $x$ axis). The volume is given by $$ V = \int\limits_{-\pi/2}^{\pi/2} \int\limits_0^{2\cos\theta}\int\limits_{-\sqrt{4-r^2}}^{\sqrt{4-r^2}} dz\,(r\,dr)\,d\theta \ . $$","I'm a little bit uncertain of how to set up the limits of integration when the axis of symmetry of the region is not centered at $z$ (this is for cylindrical coordinates). The region is bounded by $(x-1)^2+y^2=1$ and $x^2+y^2+z^2=4$. This is my attempt: Let $x=r\cos\theta$ and $y=r\sin\theta$. We are bounded on $z$ by $\pm\sqrt{4-r^2}$. We take $0\leq r\leq 2\cos\theta$ and $-\pi/2\leq \theta \leq \pi/2$ to account for the fact that the cylinder is not centered with the $z$ axis (shift on the $x$ axis). The volume is given by $$ V = \int\limits_{-\pi/2}^{\pi/2} \int\limits_0^{2\cos\theta}\int\limits_{-\sqrt{4-r^2}}^{\sqrt{4-r^2}} dz\,(r\,dr)\,d\theta \ . $$",,['multivariable-calculus']
67,Gradient and curl operators,Gradient and curl operators,,"I have some troubles with vector identities for the gradient and curl operators, for example something like the gradient of the vector or the cross product : Vector calculus identities since i have not took any course in multivariate calculus (in fact i need such formulas in electromagnetic) , i am looking for some ""trick"" for remembering such identities for differential operators .","I have some troubles with vector identities for the gradient and curl operators, for example something like the gradient of the vector or the cross product : Vector calculus identities since i have not took any course in multivariate calculus (in fact i need such formulas in electromagnetic) , i am looking for some ""trick"" for remembering such identities for differential operators .",,"['multivariable-calculus', 'vector-analysis']"
68,Show that anecessary and sufficient condition for $x_{p}$ to be tangent to $S^{n}$ at $p$,Show that anecessary and sufficient condition for  to be tangent to  at,x_{p} S^{n} p,"Please help me! How do I solve this problem? I didnt produce any idea because I didnt understand this topic properly. Thus, please can you explain the solution explicitly? Thank you for help:)","Please help me! How do I solve this problem? I didnt produce any idea because I didnt understand this topic properly. Thus, please can you explain the solution explicitly? Thank you for help:)",,"['differential-geometry', 'multivariable-calculus', 'vector-analysis']"
69,Finding image of rectangle under non-linear transformation,Finding image of rectangle under non-linear transformation,,"For the function $f(x,y)=(xy+x+y,y-x)$ on $\mathbb R^2$, how would I go about finding the image of the region $\{(x,y):1<x,y<2\}$? I am not looking for a solution, just a hint. EDIT: If I parameterize the sides, I get $f(1,y)=(2y+1,y-1)$ $f(2,y)=(3y+2,y-2)$ $f(x,1)=(2x+1,1-x)$ $f(x,2)=(3x+2,2-x)$ Where do I go from here?","For the function $f(x,y)=(xy+x+y,y-x)$ on $\mathbb R^2$, how would I go about finding the image of the region $\{(x,y):1<x,y<2\}$? I am not looking for a solution, just a hint. EDIT: If I parameterize the sides, I get $f(1,y)=(2y+1,y-1)$ $f(2,y)=(3y+2,y-2)$ $f(x,1)=(2x+1,1-x)$ $f(x,2)=(3x+2,2-x)$ Where do I go from here?",,['multivariable-calculus']
70,Equation of the line passing through the origin and parallel to the planes $x+y+z=-1$ and $x-y+z=1$,Equation of the line passing through the origin and parallel to the planes  and,x+y+z=-1 x-y+z=1,"Find a vector equation of the line that passes through the origin and is parallel to the planes $x+y+z=-1, x-y+z=1$ Is the answer $2x-2z=0$? I took the normals of the two planes which are $(1, 1, 1)$ and $(1, -1, 1)$ and used the cross-product to get the normal of the new plane, which is $(2, 0, -2)$. Since the line passes the origin, I would get $2x-2z=0$. Is this the right approach?","Find a vector equation of the line that passes through the origin and is parallel to the planes $x+y+z=-1, x-y+z=1$ Is the answer $2x-2z=0$? I took the normals of the two planes which are $(1, 1, 1)$ and $(1, -1, 1)$ and used the cross-product to get the normal of the new plane, which is $(2, 0, -2)$. Since the line passes the origin, I would get $2x-2z=0$. Is this the right approach?",,"['linear-algebra', 'multivariable-calculus']"
71,Property regarding partial derivatives,Property regarding partial derivatives,,"Let $f: \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}$ be a differentiable function. For each $x \in \mathbb{R}$, define a function $g_x: \mathbb{R} \rightarrow \mathbb{R}$ by $g_x(y)=f(x,y)$. Suppose that for each $x$, there is a unique y such that $g_x'(y)=0$; let $c(x)$ be this $y$. Suppose the partial derivative $\frac{\partial^2 f}{\partial y^2} \neq 0$ for all $(x,y)$. Show that $c$ is a differentiable function and $$c'(x)=-\frac{\frac{\partial^2f}{\partial x \partial y}(x,c(x))}{\frac{\partial^2f}{\partial^2 y}(x,c(x))}$$","Let $f: \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}$ be a differentiable function. For each $x \in \mathbb{R}$, define a function $g_x: \mathbb{R} \rightarrow \mathbb{R}$ by $g_x(y)=f(x,y)$. Suppose that for each $x$, there is a unique y such that $g_x'(y)=0$; let $c(x)$ be this $y$. Suppose the partial derivative $\frac{\partial^2 f}{\partial y^2} \neq 0$ for all $(x,y)$. Show that $c$ is a differentiable function and $$c'(x)=-\frac{\frac{\partial^2f}{\partial x \partial y}(x,c(x))}{\frac{\partial^2f}{\partial^2 y}(x,c(x))}$$",,[]
72,Finding dimensions of a rectangular box,Finding dimensions of a rectangular box,,"Find the dimensions of a rectangular box without a top, of the maximum capacity with a surface area of $108 \, cm^2$. This is my attempt at solving the problem :  If $x,y,z$ are the dimensions of the box, Surface Area : $$xy + 2xz + 2yz = 108\,cm^2$$ Volume : $$V = xyz$$","Find the dimensions of a rectangular box without a top, of the maximum capacity with a surface area of $108 \, cm^2$. This is my attempt at solving the problem :  If $x,y,z$ are the dimensions of the box, Surface Area : $$xy + 2xz + 2yz = 108\,cm^2$$ Volume : $$V = xyz$$",,['multivariable-calculus']
73,Flux through the sphere,Flux through the sphere,,"I have the vector field in spherical coordinates $$ \mathbf{F} (r, \theta, \phi) = r^2\cos(\phi) \, \mathbf{e}_{\theta} $$ Why is the flux through the sphere zero?",I have the vector field in spherical coordinates Why is the flux through the sphere zero?," \mathbf{F} (r, \theta, \phi) = r^2\cos(\phi) \, \mathbf{e}_{\theta} ","['multivariable-calculus', 'vector-fields', 'spherical-coordinates', 'divergence-theorem']"
74,Need help on Chain Rule for multivariate function.,Need help on Chain Rule for multivariate function.,,"I am given the function $S(t) = v\begin{pmatrix}t, 1-t, \frac{1}{1-t}\end{pmatrix}$. I need to find $S'(t)$ in terms of $t,\,v'_1,\,v'_2\,v'_3$. How do I do so? I am thinking of the following: $$ \begin{align*} S'(t) &= v'_1 +v'_2\dot{} -1 + v'_3\dot{}\frac{1}{(1-t)^2}\\&=v'_1-v'_2+\frac{v'_3}{(1-t)^2} \end{align*} $$ Any ideas if this sounds correct?","I am given the function $S(t) = v\begin{pmatrix}t, 1-t, \frac{1}{1-t}\end{pmatrix}$. I need to find $S'(t)$ in terms of $t,\,v'_1,\,v'_2\,v'_3$. How do I do so? I am thinking of the following: $$ \begin{align*} S'(t) &= v'_1 +v'_2\dot{} -1 + v'_3\dot{}\frac{1}{(1-t)^2}\\&=v'_1-v'_2+\frac{v'_3}{(1-t)^2} \end{align*} $$ Any ideas if this sounds correct?",,['multivariable-calculus']
75,Two small Divergence Theorem questions,Two small Divergence Theorem questions,,"$1)$Use the divergence theorem to evaluate the flux $\int_S \underline{F} \cdot \underline{dS} $where the vector field $\underline{F} = xz\, e_1 + 3x\, e_2 - 2z \,e_3$ i)$S$ is the closed cylinder bounded by the surface $x^2 + y^2 = 1$ and the planes $z=0$ and $z=3$. ii)$S$ is the open curved cylindrical surface $x^2 + y^2 = 1,\,\, 0 \leq z \leq 3$ i) is fine, but I am unsure about how top use the div theorem for ii).  What I did was compute $\int_V \nabla \cdot F\,dV$ for both the top and bottom circles and subtract their contributions from i).  This gives an answer of $3 \pi/2$.  My worry is that when I do the actual surface integral I get something completely different. My working for the surface integral: On the curved surface; $\underline{dS} = cos \phi\, e_1 + sin \phi\,e_2\,d\phi dz $  So, $F \cdot dS = (cos^2 \phi z + 3 cos\phi sin \phi)d \phi dz.  $ Integrating with $z \in [0,3]$ and $\phi \in [0,2\pi]$ gives $9\pi/2.$ Similarly, on top surface I get $F \cdot dS = -6 d\phi d\rho$ which gives $-12 \pi$ and $0$ contribution from the bottom surface. Adding these do not give the $-3\pi/2$ attained in i) Many thanks.","$1)$Use the divergence theorem to evaluate the flux $\int_S \underline{F} \cdot \underline{dS} $where the vector field $\underline{F} = xz\, e_1 + 3x\, e_2 - 2z \,e_3$ i)$S$ is the closed cylinder bounded by the surface $x^2 + y^2 = 1$ and the planes $z=0$ and $z=3$. ii)$S$ is the open curved cylindrical surface $x^2 + y^2 = 1,\,\, 0 \leq z \leq 3$ i) is fine, but I am unsure about how top use the div theorem for ii).  What I did was compute $\int_V \nabla \cdot F\,dV$ for both the top and bottom circles and subtract their contributions from i).  This gives an answer of $3 \pi/2$.  My worry is that when I do the actual surface integral I get something completely different. My working for the surface integral: On the curved surface; $\underline{dS} = cos \phi\, e_1 + sin \phi\,e_2\,d\phi dz $  So, $F \cdot dS = (cos^2 \phi z + 3 cos\phi sin \phi)d \phi dz.  $ Integrating with $z \in [0,3]$ and $\phi \in [0,2\pi]$ gives $9\pi/2.$ Similarly, on top surface I get $F \cdot dS = -6 d\phi d\rho$ which gives $-12 \pi$ and $0$ contribution from the bottom surface. Adding these do not give the $-3\pi/2$ attained in i) Many thanks.",,['multivariable-calculus']
76,Shortes distance between two points on a cone,Shortes distance between two points on a cone,,"We have a cone in the $3$-dim space whose base circle is on the $x$-$y$ plane and has a rotational symmetry by the z-axis. We need to calculate the shortest distance bewteen $(15,8,25)$ and $(20,-21,13)$ on the cone surface. That's all the info I have. No function of the cone is given. Can someone help me with this? Thanks :)","We have a cone in the $3$-dim space whose base circle is on the $x$-$y$ plane and has a rotational symmetry by the z-axis. We need to calculate the shortest distance bewteen $(15,8,25)$ and $(20,-21,13)$ on the cone surface. That's all the info I have. No function of the cone is given. Can someone help me with this? Thanks :)",,['multivariable-calculus']
77,Derivation of 2nd derivative of directional derivative,Derivation of 2nd derivative of directional derivative,,"I have a small problem with directional derivatives $-$ finding the 2nd derivative of a directional derivative. Consider $z=f(x,y)$, $x = x(t)=x_0+th$ and $y=y(t) =y_0+tk$. I am required to prove that the 2nd derivative is $\frac{d^2z}{dt^2}=f''_{xx}h^2+2f''_{xy}hk+f''_{yy}k^2$. I have worked until the first derivative, $\frac{dz}{dt}$, by: $$ \begin{align*} \frac{dz}{dt}&=f'_x \cdot\frac{dx}{dt} + f'_y \cdot\frac{dy}{dt} \\&=hf'_x + kf'_y \end{align*} $$ From here, how do you do the 2nd derivative? I attempted it with: $$ \begin{align*} \frac{d^2z}{dt^2}&=\frac{d}{dt}\begin{bmatrix}hf'_x + kf'_y\end{bmatrix} \\&= h\begin{bmatrix}f''_{xx} + f''_{xy}\end{bmatrix} + k\begin{bmatrix}f''_{yx} + f''_{yy}\end{bmatrix}  \end{align*} $$ I can't continue from here.","I have a small problem with directional derivatives $-$ finding the 2nd derivative of a directional derivative. Consider $z=f(x,y)$, $x = x(t)=x_0+th$ and $y=y(t) =y_0+tk$. I am required to prove that the 2nd derivative is $\frac{d^2z}{dt^2}=f''_{xx}h^2+2f''_{xy}hk+f''_{yy}k^2$. I have worked until the first derivative, $\frac{dz}{dt}$, by: $$ \begin{align*} \frac{dz}{dt}&=f'_x \cdot\frac{dx}{dt} + f'_y \cdot\frac{dy}{dt} \\&=hf'_x + kf'_y \end{align*} $$ From here, how do you do the 2nd derivative? I attempted it with: $$ \begin{align*} \frac{d^2z}{dt^2}&=\frac{d}{dt}\begin{bmatrix}hf'_x + kf'_y\end{bmatrix} \\&= h\begin{bmatrix}f''_{xx} + f''_{xy}\end{bmatrix} + k\begin{bmatrix}f''_{yx} + f''_{yy}\end{bmatrix}  \end{align*} $$ I can't continue from here.",,['multivariable-calculus']
78,Is $\phi : \mathbb R^2 \rightarrow \mathbb R$ a differentiable function?,Is  a differentiable function?,\phi : \mathbb R^2 \rightarrow \mathbb R,"We have the following example on the book ""Matrix Differential Calculus with Applications in Statistics and Econometrics"", 3rd edition, p. 100. Let $\phi : \mathbb R^2 \rightarrow \mathbb R$ be a real-valued function defined by   $$ \phi(x,y) = \left\{  \begin{array}{ll} x^2[y+\text{sin}(1/x)] &  \text{if } x\neq 0 \\  0& \text{if } x= 0 \end{array} \right.  $$   Then $\phi$ is differentiable at every point in $\mathbb R^2$ with partial derivatives   $$ \mathrm D_1\phi(x,y) = \left\{ \begin{array}{ll} 2x[y+\text{sin}(1/x)] - \text{cos}(1/x) & \text{if } x\neq 0\\ 0                         & \text{if } x= 0 \end{array} \right. $$   and $\mathrm D_2\phi(x,y) = x^2$. We see that $\mathrm D_1\phi$ is not continuous at any point on the $y$-axis since cos(1/x) in $\mathrm D_1\phi(x,y)$ does not tend to a limit as $x\rightarrow 0$. Letting $c_1\neq 0$ and considering the open ball in $\mathbb R^2$ where $\sqrt{x^2+y^2}<c_1$ (to make $x\neq -c_1$), we have \begin{align} \phi(c_1+x,c_2+y) &= \phi(c_1,c_2) + [\mathrm D_1\phi(c_1,c_2) \quad \mathrm D_2\phi(c_1,c_2)] \left[\begin{array}{c}x\\ y \end{array}\right] + r(x,y)\\ (c_1+x)^2(c_2+y+\sin\frac{1}{c_1+x}) &= c_1^2(c_2+\sin\frac{1}{c_1}) + x(2c_1(c_2+\sin\frac{1}{c_1})-\cos\frac{1}{c_1})+yc_1^2+r(x,y). \end{align} Thus, $$ r(x,y) = (c_1+x)^2(c_2+y+\sin\frac{1}{c_1+x}) - c_1^2(c_2+\sin\frac{1}{c_1})-x(2c_1(c_2+\sin\frac{1}{c_1})-\cos\frac{1}{c_1})-yc_1^2. $$ However, it seems that $$ \lim_{(x,y)\rightarrow (0,0)} \frac{r(x,y)}{\sqrt{x^2+y^2}}=0 $$ is not true, which is required to $\phi$ be differentiable everywhere as stated by the above example. How to verify the differentiability of $\phi$?","We have the following example on the book ""Matrix Differential Calculus with Applications in Statistics and Econometrics"", 3rd edition, p. 100. Let $\phi : \mathbb R^2 \rightarrow \mathbb R$ be a real-valued function defined by   $$ \phi(x,y) = \left\{  \begin{array}{ll} x^2[y+\text{sin}(1/x)] &  \text{if } x\neq 0 \\  0& \text{if } x= 0 \end{array} \right.  $$   Then $\phi$ is differentiable at every point in $\mathbb R^2$ with partial derivatives   $$ \mathrm D_1\phi(x,y) = \left\{ \begin{array}{ll} 2x[y+\text{sin}(1/x)] - \text{cos}(1/x) & \text{if } x\neq 0\\ 0                         & \text{if } x= 0 \end{array} \right. $$   and $\mathrm D_2\phi(x,y) = x^2$. We see that $\mathrm D_1\phi$ is not continuous at any point on the $y$-axis since cos(1/x) in $\mathrm D_1\phi(x,y)$ does not tend to a limit as $x\rightarrow 0$. Letting $c_1\neq 0$ and considering the open ball in $\mathbb R^2$ where $\sqrt{x^2+y^2}<c_1$ (to make $x\neq -c_1$), we have \begin{align} \phi(c_1+x,c_2+y) &= \phi(c_1,c_2) + [\mathrm D_1\phi(c_1,c_2) \quad \mathrm D_2\phi(c_1,c_2)] \left[\begin{array}{c}x\\ y \end{array}\right] + r(x,y)\\ (c_1+x)^2(c_2+y+\sin\frac{1}{c_1+x}) &= c_1^2(c_2+\sin\frac{1}{c_1}) + x(2c_1(c_2+\sin\frac{1}{c_1})-\cos\frac{1}{c_1})+yc_1^2+r(x,y). \end{align} Thus, $$ r(x,y) = (c_1+x)^2(c_2+y+\sin\frac{1}{c_1+x}) - c_1^2(c_2+\sin\frac{1}{c_1})-x(2c_1(c_2+\sin\frac{1}{c_1})-\cos\frac{1}{c_1})-yc_1^2. $$ However, it seems that $$ \lim_{(x,y)\rightarrow (0,0)} \frac{r(x,y)}{\sqrt{x^2+y^2}}=0 $$ is not true, which is required to $\phi$ be differentiable everywhere as stated by the above example. How to verify the differentiability of $\phi$?",,"['trigonometry', 'multivariable-calculus']"
79,shortest distance between two points on $S^2$,shortest distance between two points on,S^2,"Length of Curve in  $2D$ is $l_{\gamma}(\mathbb{R}^2)=\int_{0}^{1}\sqrt{(dr/dt)^2+r^2(d\theta/dt)^2}$ Length of a curve in $3D$ is $l_{\gamma}(\mathbb{R}^3)=\int_{0}^{1}\sqrt{(dr/dt)^2+r^2(d\phi/dt)^2+r^2\sin^2\phi(d\theta/dt)^2}$ so when the curve lie on $S^2$ the second expression becomes $$l_{\gamma}(S^2)=\int_{0}^{1}\sqrt{(d\phi/dt)^2+\sin^2\phi(d\theta/dt)^2}$$  I myself calculated that shortest distance between any two points must be straight line by analyzing the formula  $l_{\gamma}(\mathbb{R}^2)$, could any one tell me how to analyze and find the shortest distance between any two points on $S^2$ by analyzing the formula $2D$ is $l_{\gamma}(\mathbb{R}^2)$ and  $l_{\gamma}(S^2)$?","Length of Curve in  $2D$ is $l_{\gamma}(\mathbb{R}^2)=\int_{0}^{1}\sqrt{(dr/dt)^2+r^2(d\theta/dt)^2}$ Length of a curve in $3D$ is $l_{\gamma}(\mathbb{R}^3)=\int_{0}^{1}\sqrt{(dr/dt)^2+r^2(d\phi/dt)^2+r^2\sin^2\phi(d\theta/dt)^2}$ so when the curve lie on $S^2$ the second expression becomes $$l_{\gamma}(S^2)=\int_{0}^{1}\sqrt{(d\phi/dt)^2+\sin^2\phi(d\theta/dt)^2}$$  I myself calculated that shortest distance between any two points must be straight line by analyzing the formula  $l_{\gamma}(\mathbb{R}^2)$, could any one tell me how to analyze and find the shortest distance between any two points on $S^2$ by analyzing the formula $2D$ is $l_{\gamma}(\mathbb{R}^2)$ and  $l_{\gamma}(S^2)$?",,"['real-analysis', 'differential-geometry', 'multivariable-calculus', 'analytic-geometry']"
80,In Helmholtz decomposition - why are the second components of the decomposition zero when $V$ is $\mathbb{R}^3$?,In Helmholtz decomposition - why are the second components of the decomposition zero when  is ?,V \mathbb{R}^3,"$\varphi(\mathbf{r})=\frac{1}{4\pi}\int_{V}\frac{\nabla'\cdot\mathbf{F}(\mathbf{r}')}{\left|\mathbf{r}-\mathbf{r}'\right|}\mathrm{d}V'-\frac{1}{4\pi}\int_{S}\frac{\mathbf{F}(\mathbf{r}')\cdot\mathbf{\mathrm{d}S}'}{\left|\mathbf{r}-\mathbf{r}'\right|},$ $\mathbf{A}(\mathbf{r})=\frac{1}{4\pi}\int_{V}\frac{\nabla'\times\mathbf{F}(\mathbf{r}')}{\left|\mathbf{r}-\mathbf{r}'\right|}\mathrm{d}V'+\frac{1}{4\pi}\int_{S}\frac{\mathbf{F}(\mathbf{r}')\times\mathbf{\mathrm{d}S}'}{\left|\mathbf{r}-\mathbf{r}'\right|}.$ If $V$ is $\mathbb{R}^3$ itself (unbounded), and $F$ vanishes sufficiently fast at infinity, then the second component of both scalar and vector potential are zero. That is, $\varphi(\mathbf{r})=\frac{1}{4\pi}\int_{V}\frac{\nabla'\cdot\mathbf{F}(\mathbf{r}')}{\left|\mathbf{r}-\mathbf{r}'\right|}\mathrm{d}V',$ $\mathbf{A}(\mathbf{r})=\frac{1}{4\pi}\int_{V}\frac{\nabla'\times\mathbf{F}(\mathbf{r}')}{\left|\mathbf{r}-\mathbf{r}'\right|}\mathrm{d}V'.$ Why is it true that second components of these two potentials are zero?","$\varphi(\mathbf{r})=\frac{1}{4\pi}\int_{V}\frac{\nabla'\cdot\mathbf{F}(\mathbf{r}')}{\left|\mathbf{r}-\mathbf{r}'\right|}\mathrm{d}V'-\frac{1}{4\pi}\int_{S}\frac{\mathbf{F}(\mathbf{r}')\cdot\mathbf{\mathrm{d}S}'}{\left|\mathbf{r}-\mathbf{r}'\right|},$ $\mathbf{A}(\mathbf{r})=\frac{1}{4\pi}\int_{V}\frac{\nabla'\times\mathbf{F}(\mathbf{r}')}{\left|\mathbf{r}-\mathbf{r}'\right|}\mathrm{d}V'+\frac{1}{4\pi}\int_{S}\frac{\mathbf{F}(\mathbf{r}')\times\mathbf{\mathrm{d}S}'}{\left|\mathbf{r}-\mathbf{r}'\right|}.$ If $V$ is $\mathbb{R}^3$ itself (unbounded), and $F$ vanishes sufficiently fast at infinity, then the second component of both scalar and vector potential are zero. That is, $\varphi(\mathbf{r})=\frac{1}{4\pi}\int_{V}\frac{\nabla'\cdot\mathbf{F}(\mathbf{r}')}{\left|\mathbf{r}-\mathbf{r}'\right|}\mathrm{d}V',$ $\mathbf{A}(\mathbf{r})=\frac{1}{4\pi}\int_{V}\frac{\nabla'\times\mathbf{F}(\mathbf{r}')}{\left|\mathbf{r}-\mathbf{r}'\right|}\mathrm{d}V'.$ Why is it true that second components of these two potentials are zero?",,['multivariable-calculus']
81,Derivative of trace of matrix product including inverse,Derivative of trace of matrix product including inverse,,"Let $A,B,X$ be n-by-n matrices, $X$ is nonsingular so $X^{-1}$ exist. What will $\frac{\partial Tr(XAX^{-1}B)} {\partial X}$ be?","Let $A,B,X$ be n-by-n matrices, $X$ is nonsingular so $X^{-1}$ exist. What will $\frac{\partial Tr(XAX^{-1}B)} {\partial X}$ be?",,"['matrices', 'multivariable-calculus', 'derivatives']"
82,Finding directional derivative,Finding directional derivative,,"One of the tough ones.. Find a directional derivative of $f=x^2+y^2$ in the direction of $\vec{a}=\hat{i}-\hat{j}$ (vector symbols should be above the letters) at the point $(1,2)$ any ideas?","One of the tough ones.. Find a directional derivative of $f=x^2+y^2$ in the direction of $\vec{a}=\hat{i}-\hat{j}$ (vector symbols should be above the letters) at the point $(1,2)$ any ideas?",,['multivariable-calculus']
83,Interpreting the integral of a vector-valued function,Interpreting the integral of a vector-valued function,,"Say we have some vector-valued function that, when feed a $t$-value, gives a particle's position. So, this vector function measures the distance from the origin to a point on the curve that the vector function traces. Now, if we were to integrate this function from a point ""$a$"" to a point ""$b$"", where $a > b$, would this act correspond to me adding up all the vectors that define the curve between ""$a$"" and ""$b$""? And would this tell me how far the particle has traveled relative to the origin? If this isn't the case, how should I be viewing the act of integrating a vector function?","Say we have some vector-valued function that, when feed a $t$-value, gives a particle's position. So, this vector function measures the distance from the origin to a point on the curve that the vector function traces. Now, if we were to integrate this function from a point ""$a$"" to a point ""$b$"", where $a > b$, would this act correspond to me adding up all the vectors that define the curve between ""$a$"" and ""$b$""? And would this tell me how far the particle has traveled relative to the origin? If this isn't the case, how should I be viewing the act of integrating a vector function?",,"['calculus', 'integration', 'multivariable-calculus']"
84,Is this claim true for multivariable functions,Is this claim true for multivariable functions,,"Today, I am asking to verify the continuity of the following multivariable function: $$f(x,y,z)=\begin{cases}\frac{xz-y^2}{x^2+y^2+z^2}&,\quad(x,y,z)\neq(0,0,0)\\{}\\0&,\quad(x,y,z)=(0,0,0)\end{cases}$$ The continuity can be easily rejected by seeing that the function has no limit at origin when we consider it on two paths: $$(x,0,0), ~~f\to 0 \quad \text{and} \quad (0,y,0), ~~f\to -1$$  As it is clear, this function is homogenous of degree zero also, so: Can we say all the multivariable functions   having the property above has no limit at the origin? Indeed, we encounter many multivariable functions $\Bbb R^2\to\mathbb R$ or $\Bbb R^3\to\mathbb R$ and are asked to probe the continuity at the origin. I'd like you to light my mind about this point. Thanks for the time.","Today, I am asking to verify the continuity of the following multivariable function: $$f(x,y,z)=\begin{cases}\frac{xz-y^2}{x^2+y^2+z^2}&,\quad(x,y,z)\neq(0,0,0)\\{}\\0&,\quad(x,y,z)=(0,0,0)\end{cases}$$ The continuity can be easily rejected by seeing that the function has no limit at origin when we consider it on two paths: $$(x,0,0), ~~f\to 0 \quad \text{and} \quad (0,y,0), ~~f\to -1$$  As it is clear, this function is homogenous of degree zero also, so: Can we say all the multivariable functions   having the property above has no limit at the origin? Indeed, we encounter many multivariable functions $\Bbb R^2\to\mathbb R$ or $\Bbb R^3\to\mathbb R$ and are asked to probe the continuity at the origin. I'd like you to light my mind about this point. Thanks for the time.",,"['calculus', 'limits', 'multivariable-calculus']"
85,Derivative of $F(Ax)$,Derivative of,F(Ax),"What is the identity for $$ \frac{\partial \mathbf{F}(\mathbf{A}\mathbf{x})}{\partial \mathbf{x}} = ?$$ If $\mathbf{A} \in \mathbb{R}_{mn}$, $\mathbf{x} \in \mathbb{R}_n$, and $\mathbf{F}: \mathbb{R}^m \rightarrow \mathbb{R}^m $ , where $\mathbf{F}(\mathbf{x}) = [f(x_1) \; f(x_2) \; ... \; f(x_m)]^T $ and $ f: \mathbb{R} \rightarrow \mathbb{R} $ I didn't find it here or there .","What is the identity for $$ \frac{\partial \mathbf{F}(\mathbf{A}\mathbf{x})}{\partial \mathbf{x}} = ?$$ If $\mathbf{A} \in \mathbb{R}_{mn}$, $\mathbf{x} \in \mathbb{R}_n$, and $\mathbf{F}: \mathbb{R}^m \rightarrow \mathbb{R}^m $ , where $\mathbf{F}(\mathbf{x}) = [f(x_1) \; f(x_2) \; ... \; f(x_m)]^T $ and $ f: \mathbb{R} \rightarrow \mathbb{R} $ I didn't find it here or there .",,"['matrices', 'multivariable-calculus', 'derivatives']"
86,Question on Stokes theorem,Question on Stokes theorem,,I have a question relating an identity using Stokes theorem. $$\iint_A \left( \nabla \otimes \vec{F}\right) \cdot \vec{dA} = \oint_c \vec{F} \cdot \vec{dr}$$ But my professor mentioned something about the divergence theorem $$\int_V \left( \nabla \cdot \vec{F} \right)dV = \oint_{\partial V} \vec{F} \cdot \vec{dA}$$ so does this mean I can say: $$\iint_A \left( \nabla \otimes \vec{F}\right) \cdot \vec{dA} = \oint_c \vec{F} \cdot \vec{dr} = \iint_A \left( \nabla \cdot \vec{F} \right) dA$$ Because this would imply: $$\left( \nabla \otimes \vec{F}\right) \cdot \vec{dA}= \left( \nabla \cdot \vec{F} \right) dA$$ And I don't believe this to be correct. Thank you for your help in advance!,I have a question relating an identity using Stokes theorem. $$\iint_A \left( \nabla \otimes \vec{F}\right) \cdot \vec{dA} = \oint_c \vec{F} \cdot \vec{dr}$$ But my professor mentioned something about the divergence theorem $$\int_V \left( \nabla \cdot \vec{F} \right)dV = \oint_{\partial V} \vec{F} \cdot \vec{dA}$$ so does this mean I can say: $$\iint_A \left( \nabla \otimes \vec{F}\right) \cdot \vec{dA} = \oint_c \vec{F} \cdot \vec{dr} = \iint_A \left( \nabla \cdot \vec{F} \right) dA$$ Because this would imply: $$\left( \nabla \otimes \vec{F}\right) \cdot \vec{dA}= \left( \nabla \cdot \vec{F} \right) dA$$ And I don't believe this to be correct. Thank you for your help in advance!,,"['multivariable-calculus', 'vector-analysis']"
87,How does one apply the Bessel and the Parseval equality to a function?,How does one apply the Bessel and the Parseval equality to a function?,,"Given the system: $$\{\frac{1}{\sqrt{2\pi}}e^{int} \}_{n\in \mathbb{Z} } \ \text{in} \ C[0,2\pi]$$ How does one apply the Bessel and the Parseval equality to the functions $f(t) = t $ and $g(t)=t^2$ ? Attempt so far: for : $s_{f,m} := \frac{1}{2} a_{0} + \sum_{n=1}^{m} (a_{n}cos(nt)+b_{n}sin(nt))$ the Bessel equality is : $$||f - S_{f,m} ||^2 = ||f||^2 - [\frac{1}{2}a_{0}^2 + \sum_{k=1}^{m} (a_{k}^2 + b_{k}^2)]$$ and the Parseval equality is given by: $$\frac{1}{2} a_{0}^2 + \sum_{k=1}^{\infty} (a_{k}^2 + b_{k}^2) = || f||^2 $$ where  $||f|| = (f,f)^{1/2} = (\int_{0}^{2\pi}f^2)^{1/2}$ $$||t||^2 = \int_{0}^{2\pi}t^2dt = \frac{8}{3}\pi^3, ||t^2||^2=\int_{0}^{2\pi}t^4 dt = \frac{32}{5}\pi^5$$ I don't see how to continue from here if this is correct at all","Given the system: $$\{\frac{1}{\sqrt{2\pi}}e^{int} \}_{n\in \mathbb{Z} } \ \text{in} \ C[0,2\pi]$$ How does one apply the Bessel and the Parseval equality to the functions $f(t) = t $ and $g(t)=t^2$ ? Attempt so far: for : $s_{f,m} := \frac{1}{2} a_{0} + \sum_{n=1}^{m} (a_{n}cos(nt)+b_{n}sin(nt))$ the Bessel equality is : $$||f - S_{f,m} ||^2 = ||f||^2 - [\frac{1}{2}a_{0}^2 + \sum_{k=1}^{m} (a_{k}^2 + b_{k}^2)]$$ and the Parseval equality is given by: $$\frac{1}{2} a_{0}^2 + \sum_{k=1}^{\infty} (a_{k}^2 + b_{k}^2) = || f||^2 $$ where  $||f|| = (f,f)^{1/2} = (\int_{0}^{2\pi}f^2)^{1/2}$ $$||t||^2 = \int_{0}^{2\pi}t^2dt = \frac{8}{3}\pi^3, ||t^2||^2=\int_{0}^{2\pi}t^4 dt = \frac{32}{5}\pi^5$$ I don't see how to continue from here if this is correct at all",,"['real-analysis', 'analysis', 'multivariable-calculus']"
88,Implicit function theorem-show that in a neighbourhood of the point -can be described by a pair of functions,Implicit function theorem-show that in a neighbourhood of the point -can be described by a pair of functions,,"Let $g_1(x,y_1,y_2)= x^2(y_1^2+y_2^2)-5$ and $g_2(x,y_1,y_2)=(x-y_2)^2+y_1^2-2.$ Use implicit function theorem to show that in a neighbourhood of the point $x=1, y_1=-1, y_2=2,$ the curve of intersection of two surface $g_1(x,y_1,y_2)=0$ and $g_2(x,y_1,y_2)=0,$ can be described by a pair of functions $y_1=\psi_1(x)$ and $y_2=\psi_2(x)$ .",Let and Use implicit function theorem to show that in a neighbourhood of the point the curve of intersection of two surface and can be described by a pair of functions and .,"g_1(x,y_1,y_2)= x^2(y_1^2+y_2^2)-5 g_2(x,y_1,y_2)=(x-y_2)^2+y_1^2-2. x=1, y_1=-1, y_2=2, g_1(x,y_1,y_2)=0 g_2(x,y_1,y_2)=0, y_1=\psi_1(x) y_2=\psi_2(x)","['real-analysis', 'analysis', 'multivariable-calculus', 'implicit-differentiation']"
89,Question on lagrange multipliers,Question on lagrange multipliers,,"Maximize $f(x,y,z) = x^4 + y^4 + z^4$ subject to $g(x,y,z) = x^2 + y^2 + z^2 = 1$ it is required that $$\partial_xf = \lambda \partial_xg$$ $$4x^3 = (2x) \lambda \implies x^2 = y^2 = z^2 = \frac{\lambda}{2}$$ $$x^2 + y^2 + z^2 = \frac{ 3 \lambda}{2} = 1 \implies \lambda = \frac{2}{3}$$ $$\therefore x^2 = y^2 = z^2 = \frac{1}{3}$$ and my maximum value is: $\dfrac{3}{81}$ My question is why do I only have one extrema? Also, How do I show this is a max or min.","Maximize $f(x,y,z) = x^4 + y^4 + z^4$ subject to $g(x,y,z) = x^2 + y^2 + z^2 = 1$ it is required that $$\partial_xf = \lambda \partial_xg$$ $$4x^3 = (2x) \lambda \implies x^2 = y^2 = z^2 = \frac{\lambda}{2}$$ $$x^2 + y^2 + z^2 = \frac{ 3 \lambda}{2} = 1 \implies \lambda = \frac{2}{3}$$ $$\therefore x^2 = y^2 = z^2 = \frac{1}{3}$$ and my maximum value is: $\dfrac{3}{81}$ My question is why do I only have one extrema? Also, How do I show this is a max or min.",,['multivariable-calculus']
90,Multivariable Jacobian Problem: tranpose of the Jacobian times $f(x)$ is zero,Multivariable Jacobian Problem: tranpose of the Jacobian times  is zero,f(x),"Here's a little problem I ran across and could use a hint or two on: Let $U \subset \mathfrak{R}^{n}$ be open, $f:U \rightarrow \mathfrak{R}^{m}$ differential on $U$ and satisfying $\|f(X)\|=1$ on $U$. Then $Df(X)^{T}f(X)=0$ on $U$, where $Df(X)^{T}$ is the transpose of the Jacobian matrix of $f$ at $X$. Thanks!","Here's a little problem I ran across and could use a hint or two on: Let $U \subset \mathfrak{R}^{n}$ be open, $f:U \rightarrow \mathfrak{R}^{m}$ differential on $U$ and satisfying $\|f(X)\|=1$ on $U$. Then $Df(X)^{T}f(X)=0$ on $U$, where $Df(X)^{T}$ is the transpose of the Jacobian matrix of $f$ at $X$. Thanks!",,"['analysis', 'multivariable-calculus']"
91,Is the quarter disk diffeomorphic to the half disk?,Is the quarter disk diffeomorphic to the half disk?,,"I am having trouble with the following easy question: consider a quarter of an open disk. Is there a diffeomorphism between it and half of an open disk? I think not since this would make the square $[0,1]\times[0,1]$ a manifold with boundary. The obvious map I could think of which takes a point and doubles the angle it makes with the $x$-axis, that is, $\begin{bmatrix} \cos(2(\arccos(\frac{x}{\sqrt{x^2+y^2}})&  -\sin(2(\arccos(\frac{x}{\sqrt{x^2+y^2}})\\ \sin(2(\arccos(\frac{x}{\sqrt{x^2+y^2}})& \cos(2(\arccos(\frac{x}{\sqrt{x^2+y^2}})\end{bmatrix}$ fails to be continuous at the origin. But,  i don't know what the problem would be in general in finding such a map. Thanks. added Quarter of an open disk is the open disk centred at the origin  intersection the unit square.","I am having trouble with the following easy question: consider a quarter of an open disk. Is there a diffeomorphism between it and half of an open disk? I think not since this would make the square $[0,1]\times[0,1]$ a manifold with boundary. The obvious map I could think of which takes a point and doubles the angle it makes with the $x$-axis, that is, $\begin{bmatrix} \cos(2(\arccos(\frac{x}{\sqrt{x^2+y^2}})&  -\sin(2(\arccos(\frac{x}{\sqrt{x^2+y^2}})\\ \sin(2(\arccos(\frac{x}{\sqrt{x^2+y^2}})& \cos(2(\arccos(\frac{x}{\sqrt{x^2+y^2}})\end{bmatrix}$ fails to be continuous at the origin. But,  i don't know what the problem would be in general in finding such a map. Thanks. added Quarter of an open disk is the open disk centred at the origin  intersection the unit square.",,['multivariable-calculus']
92,Hint for integral,Hint for integral,,"Could someone provide a hint as to why $$\nabla \cdot  \vec a(\vec x) =  -i\,\,\,b\,\,\,c(\vec x)$$ where $b$ is a constant, $i$ is $\sqrt {-1}$, implies that $$2\int d^3x \,\,x_ia_j(\vec x)=\epsilon_{ijk} \Big[\int \,\,d^3x \,\, \vec x\times \vec a(\vec x)\Big]_k-ib \int\,\,d^3x\,\, x_ix_jc(\vec x)$$? Context/Interpretation: $$\nabla \cdot  \vec a(\vec x) =  -i\,\,\,b\,\,\,c(\vec x)$$ is obtained from $$\nabla \cdot  [\vec a(\vec x) \exp(-ibt)]=  {\partial\over\partial t}[c(\vec x)\exp(-ibt)]$$ which can be interpreted as a conservation law. I don't have any more context information...","Could someone provide a hint as to why $$\nabla \cdot  \vec a(\vec x) =  -i\,\,\,b\,\,\,c(\vec x)$$ where $b$ is a constant, $i$ is $\sqrt {-1}$, implies that $$2\int d^3x \,\,x_ia_j(\vec x)=\epsilon_{ijk} \Big[\int \,\,d^3x \,\, \vec x\times \vec a(\vec x)\Big]_k-ib \int\,\,d^3x\,\, x_ix_jc(\vec x)$$? Context/Interpretation: $$\nabla \cdot  \vec a(\vec x) =  -i\,\,\,b\,\,\,c(\vec x)$$ is obtained from $$\nabla \cdot  [\vec a(\vec x) \exp(-ibt)]=  {\partial\over\partial t}[c(\vec x)\exp(-ibt)]$$ which can be interpreted as a conservation law. I don't have any more context information...",,"['multivariable-calculus', 'integration', 'partial-differential-equations', 'physics']"
93,Derivative of composition with holomorphic function of several variables,Derivative of composition with holomorphic function of several variables,,"Let $f : \mathbb C^n \rightarrow \mathbb C^m$ be holomorphic and $g : \mathbb C^m \rightarrow \mathbb C$ be smooth. I am looking for a simple formula for the mixed partials $\partial_i \partial_{\bar j}(g\circ f)$, where $\partial_i$ and $\partial_{\bar j}$ denote complex differentials, $$\partial_i u = \frac{\partial u}{\partial x_i} - \sqrt{-1} \frac{\partial u}{\partial y_i}$$ and $$\partial_{\bar j} u = \frac{\partial u}{\partial x_j} + \sqrt{-1} \frac{\partial u}{\partial y_j}.$$ Applying the chain rule seems to give me a rather nasty looking expression involving summation over several different indices. Is there a simpler way to write this?","Let $f : \mathbb C^n \rightarrow \mathbb C^m$ be holomorphic and $g : \mathbb C^m \rightarrow \mathbb C$ be smooth. I am looking for a simple formula for the mixed partials $\partial_i \partial_{\bar j}(g\circ f)$, where $\partial_i$ and $\partial_{\bar j}$ denote complex differentials, $$\partial_i u = \frac{\partial u}{\partial x_i} - \sqrt{-1} \frac{\partial u}{\partial y_i}$$ and $$\partial_{\bar j} u = \frac{\partial u}{\partial x_j} + \sqrt{-1} \frac{\partial u}{\partial y_j}.$$ Applying the chain rule seems to give me a rather nasty looking expression involving summation over several different indices. Is there a simpler way to write this?",,"['complex-analysis', 'multivariable-calculus', 'several-complex-variables']"
94,Show the function $x_1\sin(1/x_2)+x_2\sin(1/x_1)$ is continuous everywhere,Show the function  is continuous everywhere,x_1\sin(1/x_2)+x_2\sin(1/x_1),"I am learning continuous function, please help me. Show that the following function is continuous everywhere: $\vec{F}(x_1,x_2)=x_1\sin{\left(\frac{1}{x_2}\right)}+x_2\sin{\left(\frac{1}{x_1}\right)}$ if $x_1x_2\neq 0$ and $\vec{F}(x_1,x_2)=0$ if $x_1x_2 = 0$","I am learning continuous function, please help me. Show that the following function is continuous everywhere: $\vec{F}(x_1,x_2)=x_1\sin{\left(\frac{1}{x_2}\right)}+x_2\sin{\left(\frac{1}{x_1}\right)}$ if $x_1x_2\neq 0$ and $\vec{F}(x_1,x_2)=0$ if $x_1x_2 = 0$",,"['multivariable-calculus', 'continuity']"
95,Particular Solution of Differential Equations,Particular Solution of Differential Equations,,"While solving second order non-homogenous differential equations of the form $y''+y'=5$, I realized that unlike while solving the ones of the form $y''+y'+y=Ax^n$ where we assume, $y_p=Ax^n+Bx^{n-1}+\cdots$, we assume in this case that $y_p=Ax$ instead of $A$. Can some one give me some insight. Or if this is wrong, how to find particular solution of this form (even though this is very basic).","While solving second order non-homogenous differential equations of the form $y''+y'=5$, I realized that unlike while solving the ones of the form $y''+y'+y=Ax^n$ where we assume, $y_p=Ax^n+Bx^{n-1}+\cdots$, we assume in this case that $y_p=Ax$ instead of $A$. Can some one give me some insight. Or if this is wrong, how to find particular solution of this form (even though this is very basic).",,"['ordinary-differential-equations', 'multivariable-calculus']"
96,Using Lagrange multipliers for restricted extrema,Using Lagrange multipliers for restricted extrema,,"Consider the function $f(x,y) = x^2 + xy + y^2$ defined on the unit disc $D = \{(x,y) \mid x^2 + y^2 \leq 1\}$. I can not simplify the equations to the point where I find a constant for the lagrange multiplier and thus can't find the points of the extrema. I used the method that we can create a new function $L$ with the variables $x$, $y$ and $\lambda$ where $\lambda$ is the Lagrange multiplier. Then found the partials of $L$ with respect to each variable. This is where I am stuck because I can not simplify enough to find $x$ and $y$. Are there any tricks for this type of question?","Consider the function $f(x,y) = x^2 + xy + y^2$ defined on the unit disc $D = \{(x,y) \mid x^2 + y^2 \leq 1\}$. I can not simplify the equations to the point where I find a constant for the lagrange multiplier and thus can't find the points of the extrema. I used the method that we can create a new function $L$ with the variables $x$, $y$ and $\lambda$ where $\lambda$ is the Lagrange multiplier. Then found the partials of $L$ with respect to each variable. This is where I am stuck because I can not simplify enough to find $x$ and $y$. Are there any tricks for this type of question?",,"['calculus', 'multivariable-calculus', 'optimization']"
97,"laplacian of a function, relation to the function","laplacian of a function, relation to the function",,"Suppose for some function $\Phi$ we have: $$ \nabla^2 \Phi(\mathbf{r})=\phi(\mathbf{r}) $$ where $\phi(\mathbf{r})$ is some well-behaved smooth function, which is finite everywhere. Does this mean that $\Phi(\mathbf{r})$ itself doesn't have any singularities? Could you please point me out any useful theorems?","Suppose for some function $\Phi$ we have: $$ \nabla^2 \Phi(\mathbf{r})=\phi(\mathbf{r}) $$ where $\phi(\mathbf{r})$ is some well-behaved smooth function, which is finite everywhere. Does this mean that $\Phi(\mathbf{r})$ itself doesn't have any singularities? Could you please point me out any useful theorems?",,"['multivariable-calculus', 'vector-analysis']"
98,Maximum of strictly subharmonic function,Maximum of strictly subharmonic function,,"Let $u\in C^2(D)$, $D$ is the closed unit disk in $\mathbf{R}^2$. Assume that $\Delta u>0$. Show that $u$ cannot have a maximum point in $D\setminus\partial D$. This statement is in a calculus book, after the discussion of extremal values of multivariable functions. So my guess is that I should use the Hessian of $u$ somehow. I started to proof indirectly. Assume that $(x_0,y_0)\in D\setminus\partial D$ is a maximum point. Then $\frac{\partial u}{\partial x}(x_0,y_0),\,\frac{\partial u}{\partial y}(x_0,y_0)=0$. Now I want to investigate the positive/negative definiteness of Hessian and deduce contradiction, but I got stuck.","Let $u\in C^2(D)$, $D$ is the closed unit disk in $\mathbf{R}^2$. Assume that $\Delta u>0$. Show that $u$ cannot have a maximum point in $D\setminus\partial D$. This statement is in a calculus book, after the discussion of extremal values of multivariable functions. So my guess is that I should use the Hessian of $u$ somehow. I started to proof indirectly. Assume that $(x_0,y_0)\in D\setminus\partial D$ is a maximum point. Then $\frac{\partial u}{\partial x}(x_0,y_0),\,\frac{\partial u}{\partial y}(x_0,y_0)=0$. Now I want to investigate the positive/negative definiteness of Hessian and deduce contradiction, but I got stuck.",,"['multivariable-calculus', 'harmonic-functions']"
99,Triple integral of a function,Triple integral of a function,,"Upon integration, $\int f(x) \implies $ area of curve. $\iint f(x) \implies $ volume under the curve. $\iiint f(x) \implies $ ? . We are expected to come up with something 4 dimensional? I simply know. $\iiint 1 \implies $ Volume. $\iiint \rho(x) \implies $ gives mass.","Upon integration, $\int f(x) \implies $ area of curve. $\iint f(x) \implies $ volume under the curve. $\iiint f(x) \implies $ ? . We are expected to come up with something 4 dimensional? I simply know. $\iiint 1 \implies $ Volume. $\iiint \rho(x) \implies $ gives mass.",,[]
