,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Is it true that $0.999999999\ldots=1$?,Is it true that ?,0.999999999\ldots=1,"I'm told by smart people that $$0.999999999\ldots=1$$ and I believe them, but is there a proof that explains why this is?","I'm told by smart people that and I believe them, but is there a proof that explains why this is?",0.999999999\ldots=1,"['real-analysis', 'algebra-precalculus', 'real-numbers', 'decimal-expansion']"
1,How can you prove that a function has no closed form integral?,How can you prove that a function has no closed form integral?,,"In the past, I've come across statements  along the lines of ""function $f(x)$ has no closed form integral"", which I assume means that there is no combination of the operations: addition/subtraction multiplication/division raising to powers and roots trigonometric functions exponential functions logarithmic functions which when differentiated gives the function $f(x)$ . I've heard this said about the function $f(x) = x^x$ , for example. What sort of techniques are used to prove statements like this? What is this branch of mathematics called? Merged with "" How to prove that some functions don't have a primitive "" by Ismael : Sometimes we are told that some functions like $\dfrac{\sin(x)}{x}$ don't have an indefinite integral, or that it can't be expressed in term of other simple functions. I wonder how we can prove that kind of assertion?","In the past, I've come across statements  along the lines of ""function has no closed form integral"", which I assume means that there is no combination of the operations: addition/subtraction multiplication/division raising to powers and roots trigonometric functions exponential functions logarithmic functions which when differentiated gives the function . I've heard this said about the function , for example. What sort of techniques are used to prove statements like this? What is this branch of mathematics called? Merged with "" How to prove that some functions don't have a primitive "" by Ismael : Sometimes we are told that some functions like don't have an indefinite integral, or that it can't be expressed in term of other simple functions. I wonder how we can prove that kind of assertion?",f(x) f(x) f(x) = x^x \dfrac{\sin(x)}{x},"['real-analysis', 'calculus', 'integration', 'faq', 'differential-algebra']"
2,Any open subset of $\Bbb R$ is a countable union of disjoint open intervals,Any open subset of  is a countable union of disjoint open intervals,\Bbb R,"Let $U$ be an open set in $\mathbb R$ . Then $U$ is a countable union of disjoint intervals. This question has probably been asked. However, I am not interested in just getting the answer to it. Rather, I am interested in collecting as many different proofs of it which are as diverse as possible. A professor told me that there are many. So, I invite everyone who has seen proofs of this fact to share them with the community. I think it is a result worth knowing how to prove in many different ways and having a post that combines as many of them as possible will, no doubt, be quite useful. After two days, I will place a bounty on this question to attract as many people as possible. Of course, any comments, corrections, suggestions, links to papers/notes etc. are more than welcome.","Let be an open set in . Then is a countable union of disjoint intervals. This question has probably been asked. However, I am not interested in just getting the answer to it. Rather, I am interested in collecting as many different proofs of it which are as diverse as possible. A professor told me that there are many. So, I invite everyone who has seen proofs of this fact to share them with the community. I think it is a result worth knowing how to prove in many different ways and having a post that combines as many of them as possible will, no doubt, be quite useful. After two days, I will place a bounty on this question to attract as many people as possible. Of course, any comments, corrections, suggestions, links to papers/notes etc. are more than welcome.",U \mathbb R U,"['real-analysis', 'general-topology', 'big-list', 'faq']"
3,How discontinuous can a derivative be?,How discontinuous can a derivative be?,,"There is a well-known result in elementary analysis due to Darboux which says if $f$ is a differentiable function then $f'$ satisfies the intermediate value property.  To my knowledge, not many ""highly"" discontinuous Darboux functions are known--the only one I am aware of being the Conway base 13 function--and few (none?) of these are derivatives of differentiable functions.  In fact they generally cannot be since an application of Baire's theorem gives that the set of continuity points of the derivative is dense $G_\delta$. Is it known how sharp that last result is?  Are there known Darboux functions which are derivatives and are discontinuous on ""large"" sets in some appropriate sense?","There is a well-known result in elementary analysis due to Darboux which says if $f$ is a differentiable function then $f'$ satisfies the intermediate value property.  To my knowledge, not many ""highly"" discontinuous Darboux functions are known--the only one I am aware of being the Conway base 13 function--and few (none?) of these are derivatives of differentiable functions.  In fact they generally cannot be since an application of Baire's theorem gives that the set of continuity points of the derivative is dense $G_\delta$. Is it known how sharp that last result is?  Are there known Darboux functions which are derivatives and are discontinuous on ""large"" sets in some appropriate sense?",,"['real-analysis', 'derivatives', 'examples-counterexamples']"
4,"Evaluate $\int_{0}^{\frac{\pi}2}\frac1{(1+x^2)(1+\tan x)}\,\Bbb dx$",Evaluate,"\int_{0}^{\frac{\pi}2}\frac1{(1+x^2)(1+\tan x)}\,\Bbb dx","Evaluate the following integral $$ \tag1\int_{0}^{\frac{\pi}{2}}\frac1{(1+x^2)(1+\tan x)}\,\Bbb dx $$ My Attempt: Letting $x=\frac{\pi}{2}-x$ and using the property that $$ \int_{0}^{a}f(x)\,\Bbb dx = \int_{0}^{a}f(a-x)\,\Bbb dx $$ we obtain $$ \tag2\int_{0}^{\frac{\pi}{2}}\frac{\tan x}{\left(1+\left(\frac{\pi}{2}-x\right)^2\right)(1+\tan x)}\,\Bbb dx $$ Now, add equation $(1)$ and $(2)$ . After that I do not understand how I can proceed further.","Evaluate the following integral My Attempt: Letting and using the property that we obtain Now, add equation and . After that I do not understand how I can proceed further.","
\tag1\int_{0}^{\frac{\pi}{2}}\frac1{(1+x^2)(1+\tan x)}\,\Bbb dx
 x=\frac{\pi}{2}-x 
\int_{0}^{a}f(x)\,\Bbb dx = \int_{0}^{a}f(a-x)\,\Bbb dx
 
\tag2\int_{0}^{\frac{\pi}{2}}\frac{\tan x}{\left(1+\left(\frac{\pi}{2}-x\right)^2\right)(1+\tan x)}\,\Bbb dx
 (1) (2)","['calculus', 'real-analysis', 'integration', 'definite-integrals', 'improper-integrals']"
5,Evaluating $\lim\limits_{n\to\infty} e^{-n} \sum\limits_{k=0}^{n} \frac{n^k}{k!}$,Evaluating,\lim\limits_{n\to\infty} e^{-n} \sum\limits_{k=0}^{n} \frac{n^k}{k!},"I'm supposed to calculate: $$\lim_{n\to\infty} e^{-n} \sum_{k=0}^{n} \frac{n^k}{k!}$$ By using WolframAlpha, I might guess that the limit is $\frac{1}{2}$ , which is a pretty interesting and nice result. I wonder in which ways we may approach it.","I'm supposed to calculate: By using WolframAlpha, I might guess that the limit is , which is a pretty interesting and nice result. I wonder in which ways we may approach it.",\lim_{n\to\infty} e^{-n} \sum_{k=0}^{n} \frac{n^k}{k!} \frac{1}{2},"['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
6,The Integral that Stumped Feynman?,The Integral that Stumped Feynman?,,"In "" Surely You're Joking, Mr. Feynman! ,"" Nobel-prize winning Physicist Richard Feynman said that he challenged his colleagues to give him an integral that they could evaluate with only complex methods that he could not do with real methods: One time I boasted, ""I can do by other methods any integral anybody else needs contour integration to do."" So Paul [Olum] puts up this tremendous damn integral he had obtained by starting out with a complex function that he knew the answer to, taking out the real part of it and leaving only the complex part. He had unwrapped it so it was only possible by contour integration! He was always deflating me like that. He was a very smart fellow. Does anyone happen to know what this integral was?","In "" Surely You're Joking, Mr. Feynman! ,"" Nobel-prize winning Physicist Richard Feynman said that he challenged his colleagues to give him an integral that they could evaluate with only complex methods that he could not do with real methods: One time I boasted, ""I can do by other methods any integral anybody else needs contour integration to do."" So Paul [Olum] puts up this tremendous damn integral he had obtained by starting out with a complex function that he knew the answer to, taking out the real part of it and leaving only the complex part. He had unwrapped it so it was only possible by contour integration! He was always deflating me like that. He was a very smart fellow. Does anyone happen to know what this integral was?",,"['real-analysis', 'complex-analysis', 'reference-request', 'integration', 'contour-integration']"
7,When can you switch the order of limits?,When can you switch the order of limits?,,"Suppose you have a double sequence $\displaystyle a_{nm}$.  What are sufficient conditions for you to be able to say that $\displaystyle \lim_{n\to \infty}\,\lim_{m\to \infty}{a_{nm}} = \lim_{m\to \infty}\,\lim_{n\to \infty}{a_{nm}}$?  Bonus points for necessary and sufficient conditions. For an example of a sequence where this is not the case, consider $\displaystyle a_{nm}=\left(\frac{1}{n}\right)^{\frac{1}{m}}$.  $\displaystyle \lim_{n\to \infty}\,\lim_{m\to \infty}{a_{nm}}=\lim_{n\to \infty}{\left(\frac{1}{n}\right)^0}=\lim_{n\to \infty}{1}=1$, but $\displaystyle \lim_{m\to \infty}\,\lim_{n\to \infty}{a_{nm}}=\lim_{m\to \infty}{0^{\frac{1}{m}}}=\lim_{m\to \infty}{0}=0$.","Suppose you have a double sequence $\displaystyle a_{nm}$.  What are sufficient conditions for you to be able to say that $\displaystyle \lim_{n\to \infty}\,\lim_{m\to \infty}{a_{nm}} = \lim_{m\to \infty}\,\lim_{n\to \infty}{a_{nm}}$?  Bonus points for necessary and sufficient conditions. For an example of a sequence where this is not the case, consider $\displaystyle a_{nm}=\left(\frac{1}{n}\right)^{\frac{1}{m}}$.  $\displaystyle \lim_{n\to \infty}\,\lim_{m\to \infty}{a_{nm}}=\lim_{n\to \infty}{\left(\frac{1}{n}\right)^0}=\lim_{n\to \infty}{1}=1$, but $\displaystyle \lim_{m\to \infty}\,\lim_{n\to \infty}{a_{nm}}=\lim_{m\to \infty}{0^{\frac{1}{m}}}=\lim_{m\to \infty}{0}=0$.",,"['real-analysis', 'sequences-and-series', 'limits']"
8,When can a sum and integral be interchanged?,When can a sum and integral be interchanged?,,"Let's say I have $\int_{0}^{\infty}\sum_{n = 0}^{\infty} f_{n}(x)\, dx$ with $f_{n}(x)$ being continuous functions. When can we interchange the integral and summation? Is $f_{n}(x) \geq 0$ for all $x$ and for all $n$ sufficient? How about when $\sum f_{n}(x)$ converges absolutely? If so why?",Let's say I have with being continuous functions. When can we interchange the integral and summation? Is for all and for all sufficient? How about when converges absolutely? If so why?,"\int_{0}^{\infty}\sum_{n = 0}^{\infty} f_{n}(x)\, dx f_{n}(x) f_{n}(x) \geq 0 x n \sum f_{n}(x)","['real-analysis', 'analysis']"
9,What does $2^x$ really mean when $x$ is not an integer?,What does  really mean when  is not an integer?,2^x x,"We all know that $2^5$ means $2\times 2\times 2\times 2\times 2 = 32$, but what does $2^\pi$ mean? How is it possible to calculate that without using a calculator? I am really curious about this, so please let me know what you think.","We all know that $2^5$ means $2\times 2\times 2\times 2\times 2 = 32$, but what does $2^\pi$ mean? How is it possible to calculate that without using a calculator? I am really curious about this, so please let me know what you think.",,"['real-analysis', 'algebra-precalculus', 'exponentiation']"
10,The sum of an uncountable number of positive numbers,The sum of an uncountable number of positive numbers,,"Claim: If $(x_\alpha)_{\alpha\in A}$ is a collection of real numbers $x_\alpha\in [0,\infty]$ such that $\sum_{\alpha\in A}x_\alpha<\infty$ , then $x_\alpha=0$ for all but at most countably many $\alpha\in A$ ( $A$ need not be countable). Proof: Let $\sum_{\alpha\in A}x_\alpha=M<\infty$ . Consider $S_n=\{\alpha\in A \mid x_\alpha>1/n\}$ . Then $M\geq\sum_{\alpha\in S_n}x_\alpha>\sum_{\alpha\in S_n}1/n=\frac{N}{n}$ , where $N\in\mathbb{N}\cup\{\infty\}$ is the number of elements in $S_n$ . Thus $S_n$ has at most $Mn$ elements. Hence $\{\alpha\in A \mid x_\alpha>0\}=\bigcup_{n\in\mathbb{N}}S_n$ is countable as the countable union of finite sets. $\square$ First, is my proof correct? Second, are there more concise/elegant proofs?","Claim: If is a collection of real numbers such that , then for all but at most countably many ( need not be countable). Proof: Let . Consider . Then , where is the number of elements in . Thus has at most elements. Hence is countable as the countable union of finite sets. First, is my proof correct? Second, are there more concise/elegant proofs?","(x_\alpha)_{\alpha\in A} x_\alpha\in [0,\infty] \sum_{\alpha\in A}x_\alpha<\infty x_\alpha=0 \alpha\in A A \sum_{\alpha\in A}x_\alpha=M<\infty S_n=\{\alpha\in A \mid x_\alpha>1/n\} M\geq\sum_{\alpha\in S_n}x_\alpha>\sum_{\alpha\in S_n}1/n=\frac{N}{n} N\in\mathbb{N}\cup\{\infty\} S_n S_n Mn \{\alpha\in A \mid x_\alpha>0\}=\bigcup_{n\in\mathbb{N}}S_n \square","['real-analysis', 'measure-theory', 'summation']"
11,Limit of $L^p$ norm,Limit of  norm,L^p,"Could someone help me prove that given a finite measure space $(X, \mathcal{M}, \sigma)$ and a measurable function $f:X\to\mathbb{R}$ in $L^\infty$ and some $L^q$ , $\displaystyle\lim_{p\to\infty}\|f\|_p=\|f\|_\infty$ ? I don't know where to start.","Could someone help me prove that given a finite measure space and a measurable function in and some , ? I don't know where to start.","(X, \mathcal{M}, \sigma) f:X\to\mathbb{R} L^\infty L^q \displaystyle\lim_{p\to\infty}\|f\|_p=\|f\|_\infty","['real-analysis', 'functional-analysis', 'limits', 'measure-theory', 'lp-spaces']"
12,Are there any series whose convergence is unknown?,Are there any series whose convergence is unknown?,,"Are there any infinite series about which we don't know whether it converges or not? Or are the convergence tests exhaustive, so that in the hands of a competent mathematician any series will eventually be shown to converge or diverge? EDIT: People were kind enough to point out that without imposing restrictions on the terms it's trivial to find such ""open problem"" sequences. So, to clarify, what I had in mind were sequences whose terms are composed of ""simple"" functions, the kind you would find in an introductory calculus text: exponential, factorial, etc.","Are there any infinite series about which we don't know whether it converges or not? Or are the convergence tests exhaustive, so that in the hands of a competent mathematician any series will eventually be shown to converge or diverge? EDIT: People were kind enough to point out that without imposing restrictions on the terms it's trivial to find such ""open problem"" sequences. So, to clarify, what I had in mind were sequences whose terms are composed of ""simple"" functions, the kind you would find in an introductory calculus text: exponential, factorial, etc.",,"['real-analysis', 'sequences-and-series', 'soft-question', 'infinity']"
13,Self-Contained Proof that $\sum\limits_{n=1}^{\infty} \frac1{n^p}$ Converges for $p > 1$,Self-Contained Proof that  Converges for,\sum\limits_{n=1}^{\infty} \frac1{n^p} p > 1,"To prove the convergence of the p-series $$\sum_{n=1}^{\infty} \frac1{n^p}$$ for $p > 1$, one typically appeals to either the Integral Test or the Cauchy Condensation Test. I am wondering if there is a self-contained proof that this series converges which does not rely on either test. I suspect that any proof would have to use the ideas behind one of these two tests.","To prove the convergence of the p-series $$\sum_{n=1}^{\infty} \frac1{n^p}$$ for $p > 1$, one typically appeals to either the Integral Test or the Cauchy Condensation Test. I am wondering if there is a self-contained proof that this series converges which does not rely on either test. I suspect that any proof would have to use the ideas behind one of these two tests.",,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
14,Discontinuous derivative. [duplicate],Discontinuous derivative. [duplicate],,This question already has an answer here : How discontinuous can a derivative be? (1 answer) Closed 5 years ago . Could someone give an example of a ‘very’ discontinuous derivative? I myself can only come up with examples where the derivative is discontinuous at only one point. I am assuming the function is real-valued and defined on a bounded interval.,This question already has an answer here : How discontinuous can a derivative be? (1 answer) Closed 5 years ago . Could someone give an example of a ‘very’ discontinuous derivative? I myself can only come up with examples where the derivative is discontinuous at only one point. I am assuming the function is real-valued and defined on a bounded interval.,,"['real-analysis', 'derivatives', 'examples-counterexamples']"
15,Good book for self study of a First Course in Real Analysis,Good book for self study of a First Course in Real Analysis,,"Does anyone have a recommendation for a book to use for the self study of real analysis? Several years ago when I completed about half a semester of Real Analysis I, the instructor used ""Introduction to Analysis"" by Gaughan. While it's a good book, I'm not sure it's suited for self study by itself.  I know it's a rigorous subject, but I'd like to try and find something that ""dumbs down"" the material a bit, then between the two books I might be able to make some headway.","Does anyone have a recommendation for a book to use for the self study of real analysis? Several years ago when I completed about half a semester of Real Analysis I, the instructor used ""Introduction to Analysis"" by Gaughan. While it's a good book, I'm not sure it's suited for self study by itself.  I know it's a rigorous subject, but I'd like to try and find something that ""dumbs down"" the material a bit, then between the two books I might be able to make some headway.",,"['real-analysis', 'reference-request', 'soft-question', 'book-recommendation', 'learning']"
16,"Why is Euler's Gamma function the ""best"" extension of the factorial function to the reals?","Why is Euler's Gamma function the ""best"" extension of the factorial function to the reals?",,"There are lots (an infinitude) of smooth functions that coincide with $f(n)=n!$ on the integers. Is there a simple reason why Euler's Gamma function $\Gamma (z) = \int_0^\infty t^{z-1} e^{-t} dt$ is the ""best""?  In particular, I'm looking for reasons that I can explain to first-year calculus students.","There are lots (an infinitude) of smooth functions that coincide with on the integers. Is there a simple reason why Euler's Gamma function is the ""best""?  In particular, I'm looking for reasons that I can explain to first-year calculus students.",f(n)=n! \Gamma (z) = \int_0^\infty t^{z-1} e^{-t} dt,"['real-analysis', 'education', 'gamma-function', 'special-functions']"
17,Induction on Real Numbers,Induction on Real Numbers,,"One of my Fellows asked me whether total induction is applicable to real numbers, too ( or at least all real numbers ≥ 0) . We only used that for natural numbers so far.  Of course you have to change some things in the inductive step, when you want to use it on real numbers. I guess that using induction on real numbers isn't really possible, since $[r,r+\epsilon]$ with $\epsilon > 0$, $r \in \mathbb R$ is never empty. Can you either give a good reason, why it isn't possible or provide an example where it is used?","One of my Fellows asked me whether total induction is applicable to real numbers, too ( or at least all real numbers ≥ 0) . We only used that for natural numbers so far.  Of course you have to change some things in the inductive step, when you want to use it on real numbers. I guess that using induction on real numbers isn't really possible, since $[r,r+\epsilon]$ with $\epsilon > 0$, $r \in \mathbb R$ is never empty. Can you either give a good reason, why it isn't possible or provide an example where it is used?",,"['real-analysis', 'induction', 'real-numbers']"
18,Why is $1^{\infty}$ considered to be an indeterminate form,Why is  considered to be an indeterminate form,1^{\infty},"From Wikipedia : In calculus and other branches of mathematical analysis, an indeterminate form is an algebraic expression obtained in the context of limits. Limits involving algebraic operations are often performed by replacing subexpressions by their limits; if the expression obtained after this substitution does not give enough information to determine the original limit, it is known as an indeterminate form. The indeterminate forms include $0^{0},\frac{0}{0},(\infty - \infty),1^{\infty}, \ \text{etc}\cdots$ My question is can anyone give me a nice explanation of why $1^{\infty}$ is considered to be an indeterminate form? Because, i don't see any justification of this fact. I am still perplexed.","From Wikipedia : In calculus and other branches of mathematical analysis, an indeterminate form is an algebraic expression obtained in the context of limits. Limits involving algebraic operations are often performed by replacing subexpressions by their limits; if the expression obtained after this substitution does not give enough information to determine the original limit, it is known as an indeterminate form. The indeterminate forms include $0^{0},\frac{0}{0},(\infty - \infty),1^{\infty}, \ \text{etc}\cdots$ My question is can anyone give me a nice explanation of why $1^{\infty}$ is considered to be an indeterminate form? Because, i don't see any justification of this fact. I am still perplexed.",,"['calculus', 'real-analysis']"
19,Find a real function $f:\mathbb{R}\to\mathbb{R}$ such that $f(f(x)) = -x$?,Find a real function  such that ?,f:\mathbb{R}\to\mathbb{R} f(f(x)) = -x,"I've been perusing the internet looking for interesting problems to solve. I found the following problem and have been going at it for the past 30 minutes with no success: Find a function $f: \mathbb{R} \to \mathbb{R}$ satisfying $f(f(x)) = -x$ for all $x \in \mathbb{R}$. I am also wondering, can we find $f$ so that is continuous? I was thinking of letting $f$ be a periodic function, and adding half the period to x each time. I had no success with this, and am now thinking that such a function does not exist. Source: http://www.halfaya.org/Casti/CalculusTheory2/challenge.pdf","I've been perusing the internet looking for interesting problems to solve. I found the following problem and have been going at it for the past 30 minutes with no success: Find a function $f: \mathbb{R} \to \mathbb{R}$ satisfying $f(f(x)) = -x$ for all $x \in \mathbb{R}$. I am also wondering, can we find $f$ so that is continuous? I was thinking of letting $f$ be a periodic function, and adding half the period to x each time. I had no success with this, and am now thinking that such a function does not exist. Source: http://www.halfaya.org/Casti/CalculusTheory2/challenge.pdf",,"['real-analysis', 'continuity', 'functional-equations']"
20,Examples of bijective map from $\mathbb{R}^3\rightarrow \mathbb{R}$,Examples of bijective map from,\mathbb{R}^3\rightarrow \mathbb{R},Could any one give an example of a bijective map from $\mathbb{R}^3\rightarrow \mathbb{R}$? Thank you.,Could any one give an example of a bijective map from $\mathbb{R}^3\rightarrow \mathbb{R}$? Thank you.,,"['real-analysis', 'elementary-set-theory', 'examples-counterexamples']"
21,"What functions can be made continuous by ""mixing up their domain""?","What functions can be made continuous by ""mixing up their domain""?",,"Definition . A function $f:\Bbb R\to\Bbb R$ will be called potentially continuous if there is a bijection $\phi:\Bbb R\to\Bbb R$ such that $f\circ \phi$ is continuous. So one could say a potentially continuous (p.c.) function is ""a continuous function with a mixed up domain"". I was wondering whether there is an easy way to characterize such functions. Some thoughts $\DeclareMathOperator{\im}{im}$ If the image $\im(f)$ is not connected (i.e. no interval), then $f$ is not p.c. because even mixing the domain cannot make $f$ satisfy the intermediate value theorem. Bijective functions are always p.c. because we can choose $\phi=f^{-1}$ . Every injective function with an open connected image is p.c. for a similar reason. However, only having a connected image is not enough, as e.g. there are bijections, but no continuous bijections $f:\Bbb R \to [0,1]$ . Initially I thought a function can never be p.c. if it attains every value (or at least uncountably many values) uncountably often, e.g. like Conways base 13 function . But then I discovered this: take a Peano curve like function $c$ (or any other continuous surjection $\Bbb R\to\Bbb R^2$ ) and only look at the $x$ -component $c_x:\Bbb R\to\Bbb R$ . This is a continuous function which attains every value uncountably often. The question can also be asked this way. Given a family of pairs $(r_i,\kappa_i),i\in I$ of real numbers $r_i$ and cardinal numbers $\kappa_i\le\mathfrak c$ so that $\{r_i\mid i\in I\}$ is connected. Can we find a continuous function $f:\Bbb R\to\Bbb R$ with $|f^{-1}(r_i)|=\kappa_i$ ? There is no continuous function which attains each real number exactly once except zero which is attained twice. So, e.g. the function $$f(x)=\begin{cases}x-1&\text{for $x\in\Bbb N$}\\x&\text{otherwise}\end{cases}$$ is not p.c., even though its image is all of $\Bbb R$ .","Definition . A function will be called potentially continuous if there is a bijection such that is continuous. So one could say a potentially continuous (p.c.) function is ""a continuous function with a mixed up domain"". I was wondering whether there is an easy way to characterize such functions. Some thoughts If the image is not connected (i.e. no interval), then is not p.c. because even mixing the domain cannot make satisfy the intermediate value theorem. Bijective functions are always p.c. because we can choose . Every injective function with an open connected image is p.c. for a similar reason. However, only having a connected image is not enough, as e.g. there are bijections, but no continuous bijections . Initially I thought a function can never be p.c. if it attains every value (or at least uncountably many values) uncountably often, e.g. like Conways base 13 function . But then I discovered this: take a Peano curve like function (or any other continuous surjection ) and only look at the -component . This is a continuous function which attains every value uncountably often. The question can also be asked this way. Given a family of pairs of real numbers and cardinal numbers so that is connected. Can we find a continuous function with ? There is no continuous function which attains each real number exactly once except zero which is attained twice. So, e.g. the function is not p.c., even though its image is all of .","f:\Bbb R\to\Bbb R \phi:\Bbb R\to\Bbb R f\circ \phi \DeclareMathOperator{\im}{im} \im(f) f f \phi=f^{-1} f:\Bbb R \to [0,1] c \Bbb R\to\Bbb R^2 x c_x:\Bbb R\to\Bbb R (r_i,\kappa_i),i\in I r_i \kappa_i\le\mathfrak c \{r_i\mid i\in I\} f:\Bbb R\to\Bbb R |f^{-1}(r_i)|=\kappa_i f(x)=\begin{cases}x-1&\text{for x\in\Bbb N}\\x&\text{otherwise}\end{cases} \Bbb R","['real-analysis', 'general-topology', 'functions', 'continuity']"
22,Sum of random decreasing numbers between 0 and 1: does it converge??,Sum of random decreasing numbers between 0 and 1: does it converge??,,"Let's define a sequence of numbers between 0 and 1. The first term, $r_1$ will be chosen uniformly randomly from $(0, 1)$, but now we iterate this process choosing $r_2$ from $(0, r_1)$, and so on, so $r_3\in(0, r_2)$, $r_4\in(0, r_3)$... The set of all possible sequences generated this way contains the sequence of the reciprocals of all natural numbers, which sum diverges; but it also contains all geometric sequences in which all terms are less than 1, and they all have convergent sums. The question is: does $\sum_{n=1}^{\infty} r_n$ converge in general? (I think this is called almost sure convergence ?) If so, what is the distribution of the limits of all convergent series from this family?","Let's define a sequence of numbers between 0 and 1. The first term, $r_1$ will be chosen uniformly randomly from $(0, 1)$, but now we iterate this process choosing $r_2$ from $(0, r_1)$, and so on, so $r_3\in(0, r_2)$, $r_4\in(0, r_3)$... The set of all possible sequences generated this way contains the sequence of the reciprocals of all natural numbers, which sum diverges; but it also contains all geometric sequences in which all terms are less than 1, and they all have convergent sums. The question is: does $\sum_{n=1}^{\infty} r_n$ converge in general? (I think this is called almost sure convergence ?) If so, what is the distribution of the limits of all convergent series from this family?",,"['real-analysis', 'probability', 'sequences-and-series', 'convergence-divergence']"
23,Can someone clearly explain about the lim sup and lim inf?,Can someone clearly explain about the lim sup and lim inf?,,"Can some explain the lim sup and lim inf ? In my text book the definition of these two is this. Let $(s_n)$ be a sequence in $\mathbb{R}$. We define  $$\lim \sup\ s_n = \lim_{N \rightarrow \infty} \sup\{s_n:n>N\}$$ and $$\lim\inf\ s_n = \lim_{N\rightarrow \infty}\inf\{s_n:n>N\}$$ The right side of these two equality, can I think $\sup\{s_n:n>N\}$ and $\inf\{s_n:n>N\}$ as a sequence after $n>N$? And how these two behave as $ n$ increases? My professor said that these two get smaller as $n$ increases.","Can some explain the lim sup and lim inf ? In my text book the definition of these two is this. Let $(s_n)$ be a sequence in $\mathbb{R}$. We define  $$\lim \sup\ s_n = \lim_{N \rightarrow \infty} \sup\{s_n:n>N\}$$ and $$\lim\inf\ s_n = \lim_{N\rightarrow \infty}\inf\{s_n:n>N\}$$ The right side of these two equality, can I think $\sup\{s_n:n>N\}$ and $\inf\{s_n:n>N\}$ as a sequence after $n>N$? And how these two behave as $ n$ increases? My professor said that these two get smaller as $n$ increases.",,"['real-analysis', 'limsup-and-liminf']"
24,Is it possible for a function to be in $L^p$ for only one $p$?,Is it possible for a function to be in  for only one ?,L^p p,"I'm wondering if it's possible for a function to be an $L^p$ space for only one value of $p \in [1,\infty)$ (on either a bounded domain or an unbounded domain). One can use interpolation to show that if a function is in two $L^p$ spaces, (e.g. $p_1$ and $p_2$,with $p_1 \leq p_2$ then it is in all $p_1\leq p \leq p_2$). Moreover, if we're on a bounded domain, we also have the relatively standard result that if $f \in L^{p_1}$ for some $p_1 \in [1,\infty)$, then it is in $L^p$ for every $p\leq p_1$ (which can be shown using Hölder's inequality). Thus, I think that the question can be reduced to unbounded domains if we consider the question for any $p>1$. Intuitively, a function on an unbounded domain is inside an $L^p$ space if it decrease quickly enough toward infinity. This makes it seem like we might be able to multiply the function by a slightly larger exponent. At the same time, doing this might cause the function to blow up near zero. That's not precise/rigorous at all though. So I'm wondering if it is possible to either construct an example or prove that this can't be true.","I'm wondering if it's possible for a function to be an $L^p$ space for only one value of $p \in [1,\infty)$ (on either a bounded domain or an unbounded domain). One can use interpolation to show that if a function is in two $L^p$ spaces, (e.g. $p_1$ and $p_2$,with $p_1 \leq p_2$ then it is in all $p_1\leq p \leq p_2$). Moreover, if we're on a bounded domain, we also have the relatively standard result that if $f \in L^{p_1}$ for some $p_1 \in [1,\infty)$, then it is in $L^p$ for every $p\leq p_1$ (which can be shown using Hölder's inequality). Thus, I think that the question can be reduced to unbounded domains if we consider the question for any $p>1$. Intuitively, a function on an unbounded domain is inside an $L^p$ space if it decrease quickly enough toward infinity. This makes it seem like we might be able to multiply the function by a slightly larger exponent. At the same time, doing this might cause the function to blow up near zero. That's not precise/rigorous at all though. So I'm wondering if it is possible to either construct an example or prove that this can't be true.",,"['real-analysis', 'measure-theory', 'functional-analysis', 'banach-spaces', 'examples-counterexamples']"
25,Proving an alternating Euler sum: $\sum_{k=1}^{\infty} \frac{(-1)^{k+1} H_k}{k} = \frac{1}{2} \zeta(2) - \frac{1}{2} \log^2 2$,Proving an alternating Euler sum:,\sum_{k=1}^{\infty} \frac{(-1)^{k+1} H_k}{k} = \frac{1}{2} \zeta(2) - \frac{1}{2} \log^2 2,"Let $$A(p,q) = \sum_{k=1}^{\infty} \frac{(-1)^{k+1}H^{(p)}_k}{k^q},$$ where $H^{(p)}_n = \sum_{i=1}^n i^{-p}$, the $n$th $p$-harmonic number.  The $A(p,q)$'s are known as alternating Euler sums . Can someone provide a nice proof that    $$A(1,1) = \sum_{k=1}^{\infty} \frac{(-1)^{k+1} H_k}{k} = \frac{1}{2} \zeta(2) - \frac{1}{2} \log^2 2?$$ I worked for a while on this today but was unsuccessful.  Summation by parts, swapping the order of summation, and approximating $H_k$ by $\log k$ were my best ideas, but I could not get any of them to work.  (Perhaps someone else can?)  I would like a nice proof in order to complete my answer here . Bonus points for proving $A(1,2) = \frac{5}{8} \zeta(3)$ and $A(2,1) =  \zeta(3) - \frac{1}{2}\zeta(2) \log 2$, as those are the other two alternating Euler sums needed to complete my answer. Added : I'm going to change the accepted answer to robjohn's $A(1,1)$ calculation as a proxy for the three answers he gave here.  Notwithstanding the other great answers (especially the currently most-upvoted one, the one I first accepted), robjohn's approach is the one I was originally trying.  I am pleased to see that it can be used to do the $A(1,1)$, $A(1,2)$, and $A(2,1)$ derivations.","Let $$A(p,q) = \sum_{k=1}^{\infty} \frac{(-1)^{k+1}H^{(p)}_k}{k^q},$$ where $H^{(p)}_n = \sum_{i=1}^n i^{-p}$, the $n$th $p$-harmonic number.  The $A(p,q)$'s are known as alternating Euler sums . Can someone provide a nice proof that    $$A(1,1) = \sum_{k=1}^{\infty} \frac{(-1)^{k+1} H_k}{k} = \frac{1}{2} \zeta(2) - \frac{1}{2} \log^2 2?$$ I worked for a while on this today but was unsuccessful.  Summation by parts, swapping the order of summation, and approximating $H_k$ by $\log k$ were my best ideas, but I could not get any of them to work.  (Perhaps someone else can?)  I would like a nice proof in order to complete my answer here . Bonus points for proving $A(1,2) = \frac{5}{8} \zeta(3)$ and $A(2,1) =  \zeta(3) - \frac{1}{2}\zeta(2) \log 2$, as those are the other two alternating Euler sums needed to complete my answer. Added : I'm going to change the accepted answer to robjohn's $A(1,1)$ calculation as a proxy for the three answers he gave here.  Notwithstanding the other great answers (especially the currently most-upvoted one, the one I first accepted), robjohn's approach is the one I was originally trying.  I am pleased to see that it can be used to do the $A(1,1)$, $A(1,2)$, and $A(2,1)$ derivations.",,"['real-analysis', 'calculus', 'sequences-and-series', 'harmonic-numbers', 'euler-sums']"
26,Construction of a Borel set with positive but not full measure in each interval,Construction of a Borel set with positive but not full measure in each interval,,"I was wondering how one can construct a Borel set that doesn't have full measure on any interval of the real line but does have positive measure everywhere. To be precise, if $\mu$ denotes Lebesgue measure, how would one construct a Borel set $A \subset \mathbb{R}$ such that $$0 < \mu(A \cap I) < \mu(I)$$ for every interval $I$ in $\mathbb{R}$? Moreover, would such a set necessarily have to contain infinite measure?","I was wondering how one can construct a Borel set that doesn't have full measure on any interval of the real line but does have positive measure everywhere. To be precise, if $\mu$ denotes Lebesgue measure, how would one construct a Borel set $A \subset \mathbb{R}$ such that $$0 < \mu(A \cap I) < \mu(I)$$ for every interval $I$ in $\mathbb{R}$? Moreover, would such a set necessarily have to contain infinite measure?",,"['real-analysis', 'measure-theory']"
27,Is there an integral that proves $\pi > 333/106$?,Is there an integral that proves ?,\pi > 333/106,"The following integral, $$ \int_0^1 \frac{x^4(1-x)^4}{x^2 + 1} \mathrm{d}x = \frac{22}{7} - \pi $$ is clearly positive, which proves that $\pi < 22/7$. Is there a similar integral which proves $\pi > 333/106$?","The following integral, $$ \int_0^1 \frac{x^4(1-x)^4}{x^2 + 1} \mathrm{d}x = \frac{22}{7} - \pi $$ is clearly positive, which proves that $\pi < 22/7$. Is there a similar integral which proves $\pi > 333/106$?",,"['calculus', 'real-analysis']"
28,Proofs of AM-GM inequality,Proofs of AM-GM inequality,,"The arithmetic - geometric mean inequality states that $$\frac{x_1+ \ldots + x_n}{n} \geq \sqrt[n]{x_1 \cdots x_n}$$ I'm looking for some original proofs of this inequality. I can find the usual proofs on the internet but I was wondering if someone knew a proof that is unexpected in some way. e.g. can you link the theorem to some famous theorem, can you find a non-trivial geometric proof (I can find some of those), proofs that use theory that doesn't link to this inequality at first sight (e.g. differential equations …)? Induction, backward induction, use of Jensen inequality, swapping terms, use of Lagrange multiplier, proof using thermodynamics (yeah, I know, it's rather some physical argument that this theorem might be true, not really a proof), convexity, … are some of the proofs I know.","The arithmetic - geometric mean inequality states that $$\frac{x_1+ \ldots + x_n}{n} \geq \sqrt[n]{x_1 \cdots x_n}$$ I'm looking for some original proofs of this inequality. I can find the usual proofs on the internet but I was wondering if someone knew a proof that is unexpected in some way. e.g. can you link the theorem to some famous theorem, can you find a non-trivial geometric proof (I can find some of those), proofs that use theory that doesn't link to this inequality at first sight (e.g. differential equations …)? Induction, backward induction, use of Jensen inequality, swapping terms, use of Lagrange multiplier, proof using thermodynamics (yeah, I know, it's rather some physical argument that this theorem might be true, not really a proof), convexity, … are some of the proofs I know.",,"['real-analysis', 'inequality', 'alternative-proof', 'means', 'a.m.-g.m.-inequality']"
29,How to show that a set of discontinuous points of an increasing function is at most countable,How to show that a set of discontinuous points of an increasing function is at most countable,,"I would like to prove the following: Let $g$ be a monotone increasing function on $[0,1]$. Then the set of points where $g$ is not continuous is at most countable. My attempt: Let $g(x^-)~,g(x^+)$ denote the left and right hand limits of $g$ respectively. Let $A$ be the set of points where $g$ is not continuous. Then for any $x\in A$, there is a rational, say, $f(x)$ such that $g(x^-)\lt f(x)\lt g(x^+)$. For $x_1\lt x_2$, we have that $g(x_1^+)\leq g(x_2^-)$. Thus $f(x_1)\neq f(x_2)$ if $x_1\neq x_2$. This shows an injection between $A$ and a subset of the rationals. Since the rationals are countable, $A$ is countable, being a subset of a countable set. Is my work okay? Are there better/cleaner ways of approaching it?","I would like to prove the following: Let $g$ be a monotone increasing function on $[0,1]$. Then the set of points where $g$ is not continuous is at most countable. My attempt: Let $g(x^-)~,g(x^+)$ denote the left and right hand limits of $g$ respectively. Let $A$ be the set of points where $g$ is not continuous. Then for any $x\in A$, there is a rational, say, $f(x)$ such that $g(x^-)\lt f(x)\lt g(x^+)$. For $x_1\lt x_2$, we have that $g(x_1^+)\leq g(x_2^-)$. Thus $f(x_1)\neq f(x_2)$ if $x_1\neq x_2$. This shows an injection between $A$ and a subset of the rationals. Since the rationals are countable, $A$ is countable, being a subset of a countable set. Is my work okay? Are there better/cleaner ways of approaching it?",,['real-analysis']
30,Prove that $||x|-|y||\le |x-y|$,Prove that,||x|-|y||\le |x-y|,"I've seen the full proof of the Triangle Inequality  \begin{equation*} |x+y|\le|x|+|y|.  \end{equation*} However, I haven't seen the proof of the reverse triangle inequality:  \begin{equation*} ||x|-|y||\le|x-y|. \end{equation*} Would you please prove this using only the Triangle Inequality above? Thank you very much.","I've seen the full proof of the Triangle Inequality  \begin{equation*} |x+y|\le|x|+|y|.  \end{equation*} However, I haven't seen the proof of the reverse triangle inequality:  \begin{equation*} ||x|-|y||\le|x-y|. \end{equation*} Would you please prove this using only the Triangle Inequality above? Thank you very much.",,"['real-analysis', 'inequality', 'absolute-value']"
31,Nice proofs of $\zeta(4) = \frac{\pi^4}{90}$?,Nice proofs of ?,\zeta(4) = \frac{\pi^4}{90},"I know some nice ways to prove that $\zeta(2) = \sum_{n=1}^{\infty} \frac{1}{n^2} = \pi^2/6$.  For example, see Robin Chapman's list or the answers to the question "" Different methods to compute $\sum_{n=1}^{\infty} \frac{1}{n^2}$ ?"" Are there any nice ways to prove that $$\zeta(4) = \sum_{n=1}^{\infty} \frac{1}{n^4} = \frac{\pi^4}{90}?$$ I already know some proofs that give all values of $\zeta(n)$ for positive even integers $n$ (like #7 on Robin Chapman's list or Qiaochu Yuan's answer in the linked question).  I'm not so much interested in those kinds of proofs as I am those that are specifically for $\zeta(4)$. I would be particularly interested in a proof that isn't an adaption of one that $\zeta(2) = \pi^2/6$.","I know some nice ways to prove that $\zeta(2) = \sum_{n=1}^{\infty} \frac{1}{n^2} = \pi^2/6$.  For example, see Robin Chapman's list or the answers to the question "" Different methods to compute $\sum_{n=1}^{\infty} \frac{1}{n^2}$ ?"" Are there any nice ways to prove that $$\zeta(4) = \sum_{n=1}^{\infty} \frac{1}{n^4} = \frac{\pi^4}{90}?$$ I already know some proofs that give all values of $\zeta(n)$ for positive even integers $n$ (like #7 on Robin Chapman's list or Qiaochu Yuan's answer in the linked question).  I'm not so much interested in those kinds of proofs as I am those that are specifically for $\zeta(4)$. I would be particularly interested in a proof that isn't an adaption of one that $\zeta(2) = \pi^2/6$.",,"['calculus', 'real-analysis', 'sequences-and-series', 'complex-analysis', 'riemann-zeta']"
32,Why can't calculus be done on the rational numbers?,Why can't calculus be done on the rational numbers?,,"I was once told that one must have a notion of the reals to take limits of functions. I don't see how this is true since it can be written for all functions from the rationals to the rationals, which I will denote $f$ , that $$\forall L,a,x:(L,a,x\in\mathbb{Q})\forall \epsilon:(\epsilon>0)\space \exists\delta:(\delta>0)$$ $$\lim_{x\rightarrow a}f(x)=L\leftrightarrow(\mid x-a\mid<\epsilon\leftrightarrow\mid f(x)-L\mid<\delta) $$ Since, as far as I know, functions like $\mid x\mid$ and relations like $<$ can be defined on the rationals. Is it true you couldn't do calculus on just the rational numbers? At the moment I can't think of any rational functions that differentiate to real functions. If it's true that it isn't formally constructible on the rationals, what about the algebraic numbers? Edit Thanks for all the help, but I haven't seen anyone explicitly address whether or not we could construct integrals with only algebraic numbers. Please explain why or why not this is possible.","I was once told that one must have a notion of the reals to take limits of functions. I don't see how this is true since it can be written for all functions from the rationals to the rationals, which I will denote , that Since, as far as I know, functions like and relations like can be defined on the rationals. Is it true you couldn't do calculus on just the rational numbers? At the moment I can't think of any rational functions that differentiate to real functions. If it's true that it isn't formally constructible on the rationals, what about the algebraic numbers? Edit Thanks for all the help, but I haven't seen anyone explicitly address whether or not we could construct integrals with only algebraic numbers. Please explain why or why not this is possible.","f \forall L,a,x:(L,a,x\in\mathbb{Q})\forall \epsilon:(\epsilon>0)\space \exists\delta:(\delta>0) \lim_{x\rightarrow a}f(x)=L\leftrightarrow(\mid x-a\mid<\epsilon\leftrightarrow\mid f(x)-L\mid<\delta)  \mid x\mid <","['calculus', 'real-analysis', 'limits', 'rational-numbers']"
33,Motivation for the rigour of real analysis,Motivation for the rigour of real analysis,,"I am about to finish my first year of studying mathematics at university and have completed the basic linear algebra/calculus sequence. I have started to look at some real analysis and have really enjoyed it so far. One thing I feel I am lacking in is motivation. That is, the difference in rigour between the usual introduction to calculus class and real analysis seems to be quite strong. While I appreciate rigour for aesthetic reasons, I have trouble understanding why the transition from the  18th century Euler style calculus to the rigorous ""delta-epsilon"" formulation of calculus was necessary. Is there a book that provides some historical motivation for the rigorous developement of calculus? Perhaps something that gives several counterexamples that occur when one is only equipped with a non-rigorous (i.e. first year undergraduate) formulation of calculus. For example, were there results that were thought to be true but turned out to be false when the foundations of calculus were strengthened? I suppose if anyone knows good counterexamples themselves they could list them here as well.","I am about to finish my first year of studying mathematics at university and have completed the basic linear algebra/calculus sequence. I have started to look at some real analysis and have really enjoyed it so far. One thing I feel I am lacking in is motivation. That is, the difference in rigour between the usual introduction to calculus class and real analysis seems to be quite strong. While I appreciate rigour for aesthetic reasons, I have trouble understanding why the transition from the  18th century Euler style calculus to the rigorous ""delta-epsilon"" formulation of calculus was necessary. Is there a book that provides some historical motivation for the rigorous developement of calculus? Perhaps something that gives several counterexamples that occur when one is only equipped with a non-rigorous (i.e. first year undergraduate) formulation of calculus. For example, were there results that were thought to be true but turned out to be false when the foundations of calculus were strengthened? I suppose if anyone knows good counterexamples themselves they could list them here as well.",,"['calculus', 'real-analysis', 'soft-question', 'big-list', 'motivation']"
34,Does convergence in $L^p$ imply convergence almost everywhere?,Does convergence in  imply convergence almost everywhere?,L^p,"If I know $\| f_n - f \|_{L^p(\mathbb{R})} \to 0$ as $n \to \infty$ , do I know that $\lim_{n \to \infty}f_n(x) = f(x)$ for almost every $x$ ?","If I know as , do I know that for almost every ?",\| f_n - f \|_{L^p(\mathbb{R})} \to 0 n \to \infty \lim_{n \to \infty}f_n(x) = f(x) x,"['real-analysis', 'measure-theory', 'lebesgue-integral']"
35,Lebesgue integral basics,Lebesgue integral basics,,"I'm having trouble finding a good explanation of the Lebesgue integral. As per the definition, it is the expectation of a random variable. Then how does it model the area under the curve? Let's take for example a function $f(x) = x^2$.  How do we find the integral of $f(x)$ under $[0,1]$ using the Lebesgue integral?","I'm having trouble finding a good explanation of the Lebesgue integral. As per the definition, it is the expectation of a random variable. Then how does it model the area under the curve? Let's take for example a function $f(x) = x^2$.  How do we find the integral of $f(x)$ under $[0,1]$ using the Lebesgue integral?",,"['real-analysis', 'integration', 'probability-theory', 'lebesgue-integral']"
36,The deep reason why $\int \frac{1}{x}\operatorname{d}x$ is a transcendental function ($\log$) [duplicate],The deep reason why  is a transcendental function () [duplicate],\int \frac{1}{x}\operatorname{d}x \log,"This question already has answers here : Demystify integration of $\int \frac{1}{x} \mathrm dx$ (11 answers) Closed 3 years ago . In general, the indefinite integral of $x^n$  has power $n+1$.  This is the standard power rule.  Why does it ""break"" for $n=-1$?  In other words, the derivative rule $$\frac{d}{dx} x^{n} = nx^{n-1}$$ fails to hold for $n=0$. Is there some deep reason for this discontinuity?","This question already has answers here : Demystify integration of $\int \frac{1}{x} \mathrm dx$ (11 answers) Closed 3 years ago . In general, the indefinite integral of $x^n$  has power $n+1$.  This is the standard power rule.  Why does it ""break"" for $n=-1$?  In other words, the derivative rule $$\frac{d}{dx} x^{n} = nx^{n-1}$$ fails to hold for $n=0$. Is there some deep reason for this discontinuity?",,"['calculus', 'real-analysis', 'logarithms', 'indefinite-integrals']"
37,Proof of Frullani's theorem,Proof of Frullani's theorem,,"How can I prove the Theorem of Frullani? I did not even know all the hypothesis that $f$ must satisfy, but I think that this are Let $\,f:\left[ {0,\infty } \right) \to \mathbb R$ be a a continuously differentiable function such that $$ \mathop {\lim }\limits_{x \to \infty } f\left( x \right) = 0, $$ and let $ a,b \in \left( {0,\infty } \right)$ . Prove that $$ \int\limits_0^{\infty}  {\frac{{f\left( {ax} \right) - f\left( {bx} \right)}} {x}}dx  = f\left( 0 \right)\left[ {\ln \frac{b} {a}} \right] $$ If you know a more general version please give it to me )= I can't prove it.","How can I prove the Theorem of Frullani? I did not even know all the hypothesis that must satisfy, but I think that this are Let be a a continuously differentiable function such that and let . Prove that If you know a more general version please give it to me )= I can't prove it.","f \,f:\left[ {0,\infty } \right) \to \mathbb R 
\mathop {\lim }\limits_{x \to \infty } f\left( x \right) = 0,
 
a,b \in \left( {0,\infty } \right) 
\int\limits_0^{\infty}  {\frac{{f\left( {ax} \right) - f\left( {bx} \right)}}
{x}}dx  = f\left( 0 \right)\left[ {\ln \frac{b}
{a}} \right]
","['calculus', 'real-analysis', 'integration', 'improper-integrals']"
38,On the Constant Rank Theorem and the Frobenius Theorem for differential equations.,On the Constant Rank Theorem and the Frobenius Theorem for differential equations.,,"Recently I was reading chapter $4$ (p. $60$ ) of The Implicit Function Theorem: History, Theorem, and Applications (By Steven George Krantz, Harold R. Parks) on proof's of the equivalence of the Implicit Function Theorem (finite-dimensional vector spaces) and the Picard Theorem  for ordinary differential equations . We know that the Implicit Function Theorem (finite-dimensional vector spaces) is a particular case of the Constant Rank Theorem. We also know that the Frobenius Theorem is a generalization of Picard's Theorem for ordinary differential equations. Based on these facts follow my question. The Constant Rank Theorem and the Frobenius Theorem for differential equations ( ODE's or/and PDE's) are equivalent? Is there any reference which provides a solution to this question? If the Frobenius theorem does not imply the Constant Rank Theorem there is some explanation for the negative? Conversely, if the Constant Rank Theorem does not imply the Frobenius theorem there is some explanation for the negative?","Recently I was reading chapter (p. ) of The Implicit Function Theorem: History, Theorem, and Applications (By Steven George Krantz, Harold R. Parks) on proof's of the equivalence of the Implicit Function Theorem (finite-dimensional vector spaces) and the Picard Theorem  for ordinary differential equations . We know that the Implicit Function Theorem (finite-dimensional vector spaces) is a particular case of the Constant Rank Theorem. We also know that the Frobenius Theorem is a generalization of Picard's Theorem for ordinary differential equations. Based on these facts follow my question. The Constant Rank Theorem and the Frobenius Theorem for differential equations ( ODE's or/and PDE's) are equivalent? Is there any reference which provides a solution to this question? If the Frobenius theorem does not imply the Constant Rank Theorem there is some explanation for the negative? Conversely, if the Constant Rank Theorem does not imply the Frobenius theorem there is some explanation for the negative?",4 60,"['real-analysis', 'ordinary-differential-equations', 'analysis', 'partial-differential-equations', 'implicit-function-theorem']"
39,"Calculating the integral $\int_0^\infty \frac{\cos x}{1+x^2}\, \mathrm{d}x$ without using complex analysis",Calculating the integral  without using complex analysis,"\int_0^\infty \frac{\cos x}{1+x^2}\, \mathrm{d}x","Suppose that we do not know anything about the complex analysis (numbers). In this case, how to calculate the following integral in closed form? $$\int_0^\infty\frac{\cos x}{1+x^2}\,\mathrm{d}x$$","Suppose that we do not know anything about the complex analysis (numbers). In this case, how to calculate the following integral in closed form?","\int_0^\infty\frac{\cos x}{1+x^2}\,\mathrm{d}x","['calculus', 'real-analysis', 'integration', 'definite-integrals', 'improper-integrals']"
40,Compute $\int_0^{\pi/4}\frac{(1-x^2)\ln(1+x^2)+(1+x^2)-(1-x^2)\ln(1-x^2)}{(1-x^4)(1+x^2)} x\exp(\frac{x^2-1}{x^2+1}) dx$,Compute,\int_0^{\pi/4}\frac{(1-x^2)\ln(1+x^2)+(1+x^2)-(1-x^2)\ln(1-x^2)}{(1-x^4)(1+x^2)} x\exp(\frac{x^2-1}{x^2+1}) dx,"Compute the following integral   \begin{equation} \int_0^{\Large\frac{\pi}{4}}\left[\frac{(1-x^2)\ln(1+x^2)+(1+x^2)-(1-x^2)\ln(1-x^2)}{(1-x^4)(1+x^2)}\right] x\, \exp\left[\frac{x^2-1}{x^2+1}\right]\, dx \end{equation} I was given two integral questions by my teacher. I can answer this one although it took a lot of time to compute it. I want to share this problem to the other users here and I would love to see how Mathematics SE users compute this monster. Thank you.","Compute the following integral   \begin{equation} \int_0^{\Large\frac{\pi}{4}}\left[\frac{(1-x^2)\ln(1+x^2)+(1+x^2)-(1-x^2)\ln(1-x^2)}{(1-x^4)(1+x^2)}\right] x\, \exp\left[\frac{x^2-1}{x^2+1}\right]\, dx \end{equation} I was given two integral questions by my teacher. I can answer this one although it took a lot of time to compute it. I want to share this problem to the other users here and I would love to see how Mathematics SE users compute this monster. Thank you.",,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'closed-form']"
41,Prove that every convex function is continuous,Prove that every convex function is continuous,,"A function $f : (a,b) \to \Bbb R$ is said to be convex if $$f(\lambda x+(1-\lambda)y)\le \lambda f(x)+(1-\lambda)f(y)$$ whenever $a < x, y < b$ and $0 < \lambda <1$ . Prove that every convex function is continuous. Usually it uses the fact: If $a < s < t < u < b$ then $$\frac{f(t)-f(s)}{t-s}\le \frac{f(u)-f(s)}{u-s}\le\frac{f(u)-f(t)}{u-t}.$$ I wonder whether any other version of this proof exists or not?",A function is said to be convex if whenever and . Prove that every convex function is continuous. Usually it uses the fact: If then I wonder whether any other version of this proof exists or not?,"f : (a,b) \to \Bbb R f(\lambda x+(1-\lambda)y)\le \lambda f(x)+(1-\lambda)f(y) a < x, y < b 0 < \lambda <1 a < s < t < u < b \frac{f(t)-f(s)}{t-s}\le \frac{f(u)-f(s)}{u-s}\le\frac{f(u)-f(t)}{u-t}.","['real-analysis', 'continuity', 'convex-analysis']"
42,Convergence of $\sum_{n=1}^{\infty} \frac{\sin(n!)}{n}$,Convergence of,\sum_{n=1}^{\infty} \frac{\sin(n!)}{n},Is there a way to assess the convergence of the following series? $$\sum_{n=1}^{\infty} \frac{\sin(n!)}{n}$$ From numerical estimations it seems to be convergent but I don't know how to prove it.,Is there a way to assess the convergence of the following series? $$\sum_{n=1}^{\infty} \frac{\sin(n!)}{n}$$ From numerical estimations it seems to be convergent but I don't know how to prove it.,,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
43,Can an infinite sum of irrational numbers be rational?,Can an infinite sum of irrational numbers be rational?,,"Let $S = \sum_ {k=1}^\infty a_k $ where each $a_k$ is positive and irrational. Is it possible for $S$ to be rational, considering the additional restriction that none of the $a_k$'s is a linear combination of the other ? By linear combination, we mean there exists some rational numbers $u,v$ such that $a_i = ua_j + v$.","Let $S = \sum_ {k=1}^\infty a_k $ where each $a_k$ is positive and irrational. Is it possible for $S$ to be rational, considering the additional restriction that none of the $a_k$'s is a linear combination of the other ? By linear combination, we mean there exists some rational numbers $u,v$ such that $a_i = ua_j + v$.",,"['real-analysis', 'sequences-and-series', 'summation']"
44,"If $(a_n)\subset[0,\infty)$ is non-increasing and $\sum_{n=1}^\infty a_n<\infty$, then $\lim\limits_{n\to\infty}{n a_n} = 0$","If  is non-increasing and , then","(a_n)\subset[0,\infty) \sum_{n=1}^\infty a_n<\infty \lim\limits_{n\to\infty}{n a_n} = 0","I'm studying for qualifying exams and ran into this problem. Show that if $\{a_n\}$ is a nonincreasing sequence of positive real numbers such that $\sum_n a_n$ converges, then $\lim\limits_{n \rightarrow \infty} n a_n = 0$ . Using the definition of the limit, this is equivalent to showing \begin{equation} \forall \varepsilon > 0, \; \exists n_0\;\text{such that}\; |n a_n| < \varepsilon,\; \forall n > n_0 \end{equation} or \begin{equation} \forall \varepsilon > 0, \; \exists n_0\;\text{such that}\; a_n < \frac{\varepsilon}{n},\; \forall n > n_0 \end{equation} Basically, the terms must be bounded by the harmonic series. Thanks, I'm really stuck on this seemingly simple problem!","I'm studying for qualifying exams and ran into this problem. Show that if is a nonincreasing sequence of positive real numbers such that converges, then . Using the definition of the limit, this is equivalent to showing or Basically, the terms must be bounded by the harmonic series. Thanks, I'm really stuck on this seemingly simple problem!","\{a_n\} \sum_n a_n \lim\limits_{n \rightarrow \infty} n a_n = 0 \begin{equation}
\forall \varepsilon > 0, \; \exists n_0\;\text{such that}\;
|n a_n| < \varepsilon,\; \forall n > n_0
\end{equation} \begin{equation}
\forall \varepsilon > 0, \; \exists n_0\;\text{such that}\;
a_n < \frac{\varepsilon}{n},\; \forall n > n_0
\end{equation}","['real-analysis', 'sequences-and-series']"
45,"Is $[0,1]$ a countable disjoint union of closed sets?",Is  a countable disjoint union of closed sets?,"[0,1]","Can you express $[0,1]$ as a countable disjoint union of closed sets, other than the trivial way of doing this?","Can you express $[0,1]$ as a countable disjoint union of closed sets, other than the trivial way of doing this?",,"['real-analysis', 'general-topology', 'analysis', 'baire-category']"
46,Getting Students to Not Fear Confusion,Getting Students to Not Fear Confusion,,"I'm a fifth year grad student, and I've taught several classes for freshmen and sophomores. This summer, as an ""advanced"" (whatever that means) grad student I got to teach an upper level class: Intro to Real Analysis. Since this was essentially these student's first ""real"" math class, they haven't really learned how to study for or learn this type of thing. I've continually emphasized throughout the summer that they need to put in more work than just doing a few homework problems a week. Getting a feel for the definitions and concepts involved takes time and effort of going through proofs of theorems and figuring out why things were needed. You need to build up an arsenal of examples so some general picture of the ideas are in your head. Most importantly, in my opinion, is that you wallow in your confusion for a bit when struggling with problems. Spending time with your confusion and trying to pull yourself out of it (even if it doesn't work!) is a huge part of the learning process. Of course asking for help after a point is important too. Question: What is a good way to convince students that spending time lost and confused is a reasonable thing and how do you actually motivate them to do it? Anecdote: Despite trying all quarter to explain this in various ways, I would consistently have people come in to office hours who had barely touched the homework because ""they were confused"". But they hadn't tried anything. Then when I talk around an answer to try to get them to do certain key parts on their own or get them to understand the concept involved, they would get frustrated and ask ""so does it converge or not?!"" It is incredibly hard to shake their firm belief that the answer is the important thing. Those that do get out of this belief seem to get stuck at writing down a correct proof is the important thing. None seem to make it to wanting to understand it as the important thing. (Probably a good community wiki question? Also, real-analysis might be an inappropriate tag, do what you will)","I'm a fifth year grad student, and I've taught several classes for freshmen and sophomores. This summer, as an ""advanced"" (whatever that means) grad student I got to teach an upper level class: Intro to Real Analysis. Since this was essentially these student's first ""real"" math class, they haven't really learned how to study for or learn this type of thing. I've continually emphasized throughout the summer that they need to put in more work than just doing a few homework problems a week. Getting a feel for the definitions and concepts involved takes time and effort of going through proofs of theorems and figuring out why things were needed. You need to build up an arsenal of examples so some general picture of the ideas are in your head. Most importantly, in my opinion, is that you wallow in your confusion for a bit when struggling with problems. Spending time with your confusion and trying to pull yourself out of it (even if it doesn't work!) is a huge part of the learning process. Of course asking for help after a point is important too. Question: What is a good way to convince students that spending time lost and confused is a reasonable thing and how do you actually motivate them to do it? Anecdote: Despite trying all quarter to explain this in various ways, I would consistently have people come in to office hours who had barely touched the homework because ""they were confused"". But they hadn't tried anything. Then when I talk around an answer to try to get them to do certain key parts on their own or get them to understand the concept involved, they would get frustrated and ask ""so does it converge or not?!"" It is incredibly hard to shake their firm belief that the answer is the important thing. Those that do get out of this belief seem to get stuck at writing down a correct proof is the important thing. None seem to make it to wanting to understand it as the important thing. (Probably a good community wiki question? Also, real-analysis might be an inappropriate tag, do what you will)",,"['real-analysis', 'soft-question', 'education']"
47,How to prove that exponential grows faster than polynomial?,How to prove that exponential grows faster than polynomial?,,"In other words, how to prove: For all real constants $a$ and $b$ such that $a > 1$ , $$\lim_{n\to\infty}\frac{n^b}{a^n} = 0$$ I know the definition of limit but I feel that it's not enough to prove this theorem.","In other words, how to prove: For all real constants and such that , I know the definition of limit but I feel that it's not enough to prove this theorem.",a b a > 1 \lim_{n\to\infty}\frac{n^b}{a^n} = 0,"['real-analysis', 'calculus', 'limits', 'inequality', 'exponential-function']"
48,Self-studying real analysis — Tao or Rudin?,Self-studying real analysis — Tao or Rudin?,,"The reference requests for analysis books have become so numerous as to blot out any usefulness they could conceivably have had. So here comes another one. Recently I've began to learn real analysis via Rudin. I would do all the exercises, and if I was unable to do them within a time limit (usually about 30 min) I would look the answers up. Combined with the excellent online lectures by Francis Su, I made rapid progress. Encouraged I now intend to self-study analysis II and function theory. However apart from its uninformative and dry style, Rudin's does not cover everything I intend to study. After searching for a suitable textbook, I was particularly attracted to Analysis I&II by Terry Tao. His breadth of knowledge and his nack for clear exposition are famous but I particularly like that he starts from the very beginning and builds it up from there, as well as putting real analysis inside a greater unified whole. His books would cover exactly what I intend to study. For instance, he covers fourier series, which Rudin's doesn't. However after searching for hours I've been unable to find any solutions sets. (apart from a few on the earliest chapters). It is my experience that is almost impossible to self-study a subject thoroughly without solutions or constant feedback, even with an outstanding textbook.   Which leaves me with few options: Proceed with Rudin's, perhaps with some supplementary book. Try to work with Terry Tao's Analysis I&II without solutions. Find a different book altogether that is both comprehensive and readable as well as having at least a partial solution set. I know a lot of people will recommend Rudin but I have to doubt their experience with self-study: yes it is possible to learn directly from Rudin but it's painful and slow. And quite frankly I feel that a lot of people have poured a lot of time and effort in Rudin and feel that more than teach them analysis it has brought them mathematical maturity. That is all well and good but it's not what I'm interested in. Another idea would be to get both and read Tao, while doing the exercises in Rudin's. I don't think that would be a good idea however, a lot of theorems in Tao are left to the reader and the pace and coverage of both books are very different. In general I dislike getting more than one book. Does anyone know of an extended (partial) solutions set to Terry's analysis I&II or otherwise a reference for another book that would be suitable?","The reference requests for analysis books have become so numerous as to blot out any usefulness they could conceivably have had. So here comes another one. Recently I've began to learn real analysis via Rudin. I would do all the exercises, and if I was unable to do them within a time limit (usually about 30 min) I would look the answers up. Combined with the excellent online lectures by Francis Su, I made rapid progress. Encouraged I now intend to self-study analysis II and function theory. However apart from its uninformative and dry style, Rudin's does not cover everything I intend to study. After searching for a suitable textbook, I was particularly attracted to Analysis I&II by Terry Tao. His breadth of knowledge and his nack for clear exposition are famous but I particularly like that he starts from the very beginning and builds it up from there, as well as putting real analysis inside a greater unified whole. His books would cover exactly what I intend to study. For instance, he covers fourier series, which Rudin's doesn't. However after searching for hours I've been unable to find any solutions sets. (apart from a few on the earliest chapters). It is my experience that is almost impossible to self-study a subject thoroughly without solutions or constant feedback, even with an outstanding textbook.   Which leaves me with few options: Proceed with Rudin's, perhaps with some supplementary book. Try to work with Terry Tao's Analysis I&II without solutions. Find a different book altogether that is both comprehensive and readable as well as having at least a partial solution set. I know a lot of people will recommend Rudin but I have to doubt their experience with self-study: yes it is possible to learn directly from Rudin but it's painful and slow. And quite frankly I feel that a lot of people have poured a lot of time and effort in Rudin and feel that more than teach them analysis it has brought them mathematical maturity. That is all well and good but it's not what I'm interested in. Another idea would be to get both and read Tao, while doing the exercises in Rudin's. I don't think that would be a good idea however, a lot of theorems in Tao are left to the reader and the pace and coverage of both books are very different. In general I dislike getting more than one book. Does anyone know of an extended (partial) solutions set to Terry's analysis I&II or otherwise a reference for another book that would be suitable?",,"['real-analysis', 'reference-request', 'self-learning', 'education', 'book-recommendation']"
49,If $S$ is an infinite $\sigma$ algebra on $X$ then $S$ is not countable,If  is an infinite  algebra on  then  is not countable,S \sigma X S,I am going over a tutorial in my real analysis course. There is an proof in which I don't understand some parts of it. The proof relates to the following proposition: ( $S$ - infinite $\sigma$ -algebra on $X$ ) $\implies $ $S$ is uncountable. Proof : Assume: $S=\{A_{i}\}_{i=1}^{+\infty}$ . $\forall x\in X: B_{x}:=\cap_{x\in A_{i}}A_{i}$ . [Note: $B_{x}\in S$ $\impliedby$ ( $B_{x}$ - countable intersection]. Lemma : $B_{x}\cap B_{y}\neq\emptyset \implies B_{x}=B_{y}$ . Proof(of lemma): $z\in B_{x}\cap B_{y} \implies B_{z}\subseteq B_{x}\cap B_{y}$ . 1. $x\not\in B_{z} \implies x\in B_{x}\setminus B_{z} \wedge B_{x}\setminus B_{z} \subset S \wedge  B_{x}\setminus B_{z} \subset B_{x}$ (contradiction: $\space$ definition of $B_{x}$ ) $\implies$ $B_{z}=B_{x}$ 2. $y\not\in B_{z} \implies y\in B_{y} \setminus B_{z} \space \wedge \space B_{y}\setminus B_{z} \subset S \space\wedge\space B_{y} \setminus B_{z}\subset B_{y} $ (contradiction: definition of $B_{y}$ ) $\implies$ $B_{z}=B_{y}$ $\implies B_{x}=B_{y} \space \square$ Consider: $\{B_{x}\}_{x\in X}$ . If: there are finite sets of the form $B_{x}$ then: $S$ is a union of a finite number of disjoint sets $\implies$ $S$ is finite $\implies$ there is an infinite number of sets of the form $B_{x}$ . $\implies$ $|\bigcup\limits_{i\in A \subseteq\mathbb{N}}B_{x_{i}}| \geq \aleph_{0}$ .(contradiction) $\square$ There are couple of things I don't understand in this proof: Why the fact that we found a set ( $B_{x}\setminus B_{z}$ ) in $S$ containing $x$ and is strictly contained in $B_{x}$ a contradiction ? Why if there are only a finite number of different sets of the form $B_{x}$ then $S$ is a union of a finite number of disjoint sets and is finite ?,I am going over a tutorial in my real analysis course. There is an proof in which I don't understand some parts of it. The proof relates to the following proposition: ( - infinite -algebra on ) is uncountable. Proof : Assume: . . [Note: ( - countable intersection]. Lemma : . Proof(of lemma): . 1. (contradiction: definition of ) 2. (contradiction: definition of ) Consider: . If: there are finite sets of the form then: is a union of a finite number of disjoint sets is finite there is an infinite number of sets of the form . .(contradiction) There are couple of things I don't understand in this proof: Why the fact that we found a set ( ) in containing and is strictly contained in a contradiction ? Why if there are only a finite number of different sets of the form then is a union of a finite number of disjoint sets and is finite ?,"S \sigma X \implies  S S=\{A_{i}\}_{i=1}^{+\infty} \forall x\in X:
B_{x}:=\cap_{x\in A_{i}}A_{i} B_{x}\in S \impliedby B_{x} B_{x}\cap B_{y}\neq\emptyset \implies B_{x}=B_{y} z\in B_{x}\cap B_{y} \implies B_{z}\subseteq B_{x}\cap B_{y} x\not\in B_{z} \implies x\in B_{x}\setminus B_{z} \wedge B_{x}\setminus B_{z} \subset S \wedge  B_{x}\setminus B_{z} \subset B_{x} \space B_{x} \implies B_{z}=B_{x} y\not\in B_{z} \implies y\in B_{y} \setminus B_{z} \space \wedge \space B_{y}\setminus B_{z} \subset S \space\wedge\space B_{y} \setminus B_{z}\subset B_{y}  B_{y} \implies B_{z}=B_{y} \implies B_{x}=B_{y} \space \square \{B_{x}\}_{x\in X} B_{x} S \implies S \implies B_{x} \implies |\bigcup\limits_{i\in A \subseteq\mathbb{N}}B_{x_{i}}| \geq \aleph_{0} \square B_{x}\setminus B_{z} S x B_{x} B_{x} S","['real-analysis', 'measure-theory', 'proof-explanation']"
50,Are calculus and real analysis the same thing?,Are calculus and real analysis the same thing?,,"I guess this may seem stupid, but how calculus and real analysis are different from and related to each other? I tend to think they are the same because all I know is that the objects of both are real-valued functions defined on $\mathbb{R}^n$, and their topics are continuity, differentiation and integration of such functions. Isn't it? But there is also $\lambda$-calculus, about which I honestly don't quite know. Does it belong to calculus? If not, why is it called *-calculus? I have heard at the undergraduate course level, some people mentioned the topics in linear algebra as calculus. Is that correct? Thanks and regards!","I guess this may seem stupid, but how calculus and real analysis are different from and related to each other? I tend to think they are the same because all I know is that the objects of both are real-valued functions defined on $\mathbb{R}^n$, and their topics are continuity, differentiation and integration of such functions. Isn't it? But there is also $\lambda$-calculus, about which I honestly don't quite know. Does it belong to calculus? If not, why is it called *-calculus? I have heard at the undergraduate course level, some people mentioned the topics in linear algebra as calculus. Is that correct? Thanks and regards!",,"['real-analysis', 'calculus', 'soft-question', 'lambda-calculus']"
51,Comparing $\pi^e$ and $e^\pi$ without calculating them,Comparing  and  without calculating them,\pi^e e^\pi,How can I compare (without calculator or similar device) the values of $\pi^e$ and $e^\pi$ ?,How can I compare (without calculator or similar device) the values of and ?,\pi^e e^\pi,"['real-analysis', 'inequality', 'exponential-function', 'exponentiation', 'pi']"
52,"Continuous bijection from $(0,1)$ to $[0,1]$",Continuous bijection from  to,"(0,1) [0,1]","Does there exist a continuous bijection from $(0,1)$ to $[0,1]$? Of course the map should not be a proper map.","Does there exist a continuous bijection from $(0,1)$ to $[0,1]$? Of course the map should not be a proper map.",,"['real-analysis', 'general-topology', 'continuity', 'examples-counterexamples']"
53,References for multivariable calculus,References for multivariable calculus,,"Due to my ignorance, I find that most of the references for mathematical analysis (real analysis or advanced calculus) I have read do not talk much about the ""multivariate calculus"". After dealing with the single variable calculus theoretically, it usually directly goes to the topic of measure theory. After reading the wiki article "" Second partial derivative test "", I'd like to find the rigorous proof for this test. The first book comes to my mind is Courant's Introduction to Calculus and Analysis which includes the multivariate case in the second volume. Motivated by this, I'd like to put the question here: What are the usual references for the theoretical treatment for Multivariable calculus ?","Due to my ignorance, I find that most of the references for mathematical analysis (real analysis or advanced calculus) I have read do not talk much about the ""multivariate calculus"". After dealing with the single variable calculus theoretically, it usually directly goes to the topic of measure theory. After reading the wiki article "" Second partial derivative test "", I'd like to find the rigorous proof for this test. The first book comes to my mind is Courant's Introduction to Calculus and Analysis which includes the multivariate case in the second volume. Motivated by this, I'd like to put the question here: What are the usual references for the theoretical treatment for Multivariable calculus ?",,"['real-analysis', 'multivariable-calculus']"
54,"If $f_k \to f$ a.e. and the $L^p$ norms converge, then $f_k \to f$ in $L^p$","If  a.e. and the  norms converge, then  in",f_k \to f L^p f_k \to f L^p,"Let $1\leq p < \infty$ . Suppose that $\{f_k, f\} \subset L^p$ (the domain here does not necessarily have to be finite), $f_k \to f$ almost everywhere, and $\|f_k\|_{L^p} \to \|f\|_{L^p}$ . Why is it the case that $$\|f_k - f\|_{L^p} \to 0?$$ A statement in the other direction (i.e. $\|f_k - f\|_{L^p} \to 0 \Rightarrow \|f_k\|_{L^p} \to \|f\|_{L^p}$ ) follows pretty easily and is the one that I've seen most of the time. I'm not how to show the result above though.","Let . Suppose that (the domain here does not necessarily have to be finite), almost everywhere, and . Why is it the case that A statement in the other direction (i.e. ) follows pretty easily and is the one that I've seen most of the time. I'm not how to show the result above though.","1\leq p < \infty \{f_k, f\} \subset L^p f_k \to f \|f_k\|_{L^p} \to \|f\|_{L^p} \|f_k - f\|_{L^p} \to 0? \|f_k - f\|_{L^p} \to 0 \Rightarrow \|f_k\|_{L^p} \to \|f\|_{L^p}","['real-analysis', 'functional-analysis', 'measure-theory', 'convergence-divergence', 'lp-spaces']"
55,Evaluate the integral: $\int_{0}^{1} \frac{\ln(x+1)}{x^2+1} \mathrm dx$,Evaluate the integral:,\int_{0}^{1} \frac{\ln(x+1)}{x^2+1} \mathrm dx,Compute $$\int_{0}^{1} \frac{\ln(x+1)}{x^2+1} \mathrm dx$$,Compute $$\int_{0}^{1} \frac{\ln(x+1)}{x^2+1} \mathrm dx$$,,"['calculus', 'real-analysis', 'integration', 'definite-integrals']"
56,Continuous mapping on a compact metric space is uniformly continuous,Continuous mapping on a compact metric space is uniformly continuous,,"I am struggling with this question: Prove or give a counterexample: If $f : X \to Y$ is a continuous mapping from a compact metric space $X$, then $f$ is uniformly continuous on $X$. Thanks for your help in advance.","I am struggling with this question: Prove or give a counterexample: If $f : X \to Y$ is a continuous mapping from a compact metric space $X$, then $f$ is uniformly continuous on $X$. Thanks for your help in advance.",,"['real-analysis', 'general-topology', 'metric-spaces', 'uniform-continuity', 'faq']"
57,How did Hermite calculate $e^{\pi\sqrt{163}}$ in 1859?,How did Hermite calculate  in 1859?,e^{\pi\sqrt{163}},"Pretend you are in 1859. What is a fast, efficient, and accurate way to numerically evaluate constants like that to, say, 20 decimal places, using ONLY pen and paper?","Pretend you are in 1859. What is a fast, efficient, and accurate way to numerically evaluate constants like that to, say, 20 decimal places, using ONLY pen and paper?",,"['real-analysis', 'math-history']"
58,"How to evaluate $\int_{0}^{\infty} \frac{x^{-\mathfrak{i}a}}{x^2+bx+1} \,\mathrm{d}x$ using complex analysis?",How to evaluate  using complex analysis?,"\int_{0}^{\infty} \frac{x^{-\mathfrak{i}a}}{x^2+bx+1} \,\mathrm{d}x","We were told today by our teacher (I suppose to scare us) that in certain schools for physics in Soviet Russia there was as an entry examination the following integral given $$\int\limits_{0}^{\infty} \frac{x^{-\mathfrak{i}a}}{x^2+bx+1} \,\mathrm{d}x\,,$$ where $a \in \mathbb{R}$, $b \in [0,2)$, and $\mathfrak{i}$ is the imaginary unit. And since we are doing complex analysis at the moment, it can, according to my teacher, be calculated using complex methods. I was wondering how this could work? It seems hard to me to find a good curve to apply the residue theorem for this object, I suppose. Is there a trick to compute this integral?","We were told today by our teacher (I suppose to scare us) that in certain schools for physics in Soviet Russia there was as an entry examination the following integral given $$\int\limits_{0}^{\infty} \frac{x^{-\mathfrak{i}a}}{x^2+bx+1} \,\mathrm{d}x\,,$$ where $a \in \mathbb{R}$, $b \in [0,2)$, and $\mathfrak{i}$ is the imaginary unit. And since we are doing complex analysis at the moment, it can, according to my teacher, be calculated using complex methods. I was wondering how this could work? It seems hard to me to find a good curve to apply the residue theorem for this object, I suppose. Is there a trick to compute this integral?",,"['real-analysis', 'integration']"
59,Relations between p norms,Relations between p norms,,"The $p$ -norm on $\mathbb R^n$ is given by $\|x\|_{p}=\big(\sum_{k=1}^n |x_{k}|^p\big)^{1/p}$ . For $0 < p < q$ it can be shown that $\|x\|_p\geq\|x\|_q$ ( 1 , 2 ). It appears that in $\mathbb{R}^n$ a number of opposite inequalities can also be obtained. In fact, since all norms in a finite-dimensional vector space are equivalent, this must be the case. So far, I only found the following: $\|x\|_{1} \leq\sqrt n\,\|x\|_{2}$ ( 3 ), $\|x\|_{2} \leq \sqrt n\,\|x\|_\infty$ ( 4 ). Geometrically, it is easy to see that opposite inequalities must hold in $\mathbb R^n$ . For instance, for $n=2$ and $n=3$ one can see that for $0 < p < q$ , the spheres with radius $\sqrt n$ with $\|\cdot\|_p$ inscribe spheres with radius $1$ with $\|\cdot\|_q$ . It is not hard to prove the inequality (4). According to Wikipedia, inequality (3) follows directly from Cauchy-Schwarz, but I don't see how. For $n=2$ it is easily proven (see below), but not for $n>2$ . So my questions are: How can relation (3) be proven for arbitrary $n\,$ ? Can this be generalized into something of the form $\|x\|_{p} \leq C \|x\|_{q}$ for arbitrary $0<p < q\,$ ? Do any of the relations also hold for infinite-dimensional spaces, i.e. in $l^p$ spaces? Notes: $\|x\|_{1}^{2} = |x_{1}|^2 + |x_{2}|^2 + 2|x_{1}||x_{2}| \leq |x_{1}|^2 + |x_{2}|^2 + \big(|x_{1}|^2 + |x_{2}|^2\big) = 2|x_{1}|^2 + 2|x_{2}|^2$ , hence $=2\|x\|_{2}^{2}$ $\|x\|_{1} \leq \sqrt 2 \|x\|_{2}$ . This works because $|x_{1}|^2 + |x_{2}|^2 \geq 2|x_{1}\|x_{2}|$ , but only because $(|x_{1}| - |x_{2}|)^2 \geq 0$ , while for more than two terms $\big(|x_{1}| \pm |x_{2}| \pm \dotsb \pm |x_{n}|\big)^2 \geq 0$ gives an inequality that never gives the right signs for the cross terms.","The -norm on is given by . For it can be shown that ( 1 , 2 ). It appears that in a number of opposite inequalities can also be obtained. In fact, since all norms in a finite-dimensional vector space are equivalent, this must be the case. So far, I only found the following: ( 3 ), ( 4 ). Geometrically, it is easy to see that opposite inequalities must hold in . For instance, for and one can see that for , the spheres with radius with inscribe spheres with radius with . It is not hard to prove the inequality (4). According to Wikipedia, inequality (3) follows directly from Cauchy-Schwarz, but I don't see how. For it is easily proven (see below), but not for . So my questions are: How can relation (3) be proven for arbitrary ? Can this be generalized into something of the form for arbitrary ? Do any of the relations also hold for infinite-dimensional spaces, i.e. in spaces? Notes: , hence . This works because , but only because , while for more than two terms gives an inequality that never gives the right signs for the cross terms.","p \mathbb R^n \|x\|_{p}=\big(\sum_{k=1}^n
|x_{k}|^p\big)^{1/p} 0 < p < q \|x\|_p\geq\|x\|_q \mathbb{R}^n \|x\|_{1} \leq\sqrt n\,\|x\|_{2} \|x\|_{2} \leq \sqrt n\,\|x\|_\infty \mathbb R^n n=2 n=3 0 < p < q \sqrt n \|\cdot\|_p 1 \|\cdot\|_q n=2 n>2 n\, \|x\|_{p} \leq C \|x\|_{q} 0<p < q\, l^p \|x\|_{1}^{2} = |x_{1}|^2 + |x_{2}|^2 + 2|x_{1}||x_{2}| \leq |x_{1}|^2 + |x_{2}|^2 + \big(|x_{1}|^2 + |x_{2}|^2\big) = 2|x_{1}|^2 + 2|x_{2}|^2 =2\|x\|_{2}^{2} \|x\|_{1} \leq \sqrt 2 \|x\|_{2} |x_{1}|^2 + |x_{2}|^2 \geq 2|x_{1}\|x_{2}| (|x_{1}| - |x_{2}|)^2 \geq 0 \big(|x_{1}| \pm |x_{2}| \pm \dotsb \pm |x_{n}|\big)^2 \geq 0","['real-analysis', 'functional-analysis', 'normed-spaces']"
60,"Is the ""determinant"" that shows up accidental?","Is the ""determinant"" that shows up accidental?",,"Consider the class of rational functions that are the result of dividing one linear function by another: $$\frac{a + bx}{c + dx}$$ One can easily compute that, for $\displaystyle x \neq \frac cd$ $$\frac{\mathrm d}{\mathrm dx}\left(\frac{a + bx}{c + dx}\right) = \frac{bc - ad}{(c+dx)^2} \lessgtr 0 \text{ as } ad - bc \gtrless 0$$ Thus, we can easily check whether such a rational function is increasing or decreasing (on any connected interval in its domain) by checking the determinant of a corresponding matrix \begin{pmatrix}a & b \\ c & d\end{pmatrix} This made me wonder whether there is some known deeper principle that is behind this connection between linear algebra and rational functions (seemingly distant topics), or is this probably just a coincidence?","Consider the class of rational functions that are the result of dividing one linear function by another: $$\frac{a + bx}{c + dx}$$ One can easily compute that, for $\displaystyle x \neq \frac cd$ $$\frac{\mathrm d}{\mathrm dx}\left(\frac{a + bx}{c + dx}\right) = \frac{bc - ad}{(c+dx)^2} \lessgtr 0 \text{ as } ad - bc \gtrless 0$$ Thus, we can easily check whether such a rational function is increasing or decreasing (on any connected interval in its domain) by checking the determinant of a corresponding matrix \begin{pmatrix}a & b \\ c & d\end{pmatrix} This made me wonder whether there is some known deeper principle that is behind this connection between linear algebra and rational functions (seemingly distant topics), or is this probably just a coincidence?",,"['calculus', 'real-analysis', 'linear-algebra', 'rational-functions']"
61,"Is there a bijective map from $(0,1)$ to $\mathbb{R}$?",Is there a bijective map from  to ?,"(0,1) \mathbb{R}","I couldn't find a bijective map from $(0,1)$ to $\mathbb{R}$. Is there any example?","I couldn't find a bijective map from $(0,1)$ to $\mathbb{R}$. Is there any example?",,"['real-analysis', 'functions', 'elementary-set-theory']"
62,Cardinality of set of real continuous functions,Cardinality of set of real continuous functions,,"I believe that the set of all $\mathbb{R\to R}$ continuous functions is $\mathfrak c$ , the cardinality of the continuum . However, I read in the book ""Metric spaces"" by Ó Searcóid that set of all $[0, 1]\to\mathbb{R}$ continuous functions is greater than $\mathfrak c$ : ""It is demonstrated in many textbooks that $\mathbb{Q}$ is countable, that $\mathbb{R}$ is uncountable, that every non-degenerate interval is uncountable, that the collection of continuous functions deﬁned on $[0,1]$ is of a greater cardinality than $\mathbb{R}$ , and that there are sets of greater and greater cardinality."" I understand that (via composition with the continuous function $\tan$ or $\arctan$ ) these sets of continuous functions have the same cardinality. Therefore, which claim is correct, and how do I prove this?","I believe that the set of all continuous functions is , the cardinality of the continuum . However, I read in the book ""Metric spaces"" by Ó Searcóid that set of all continuous functions is greater than : ""It is demonstrated in many textbooks that is countable, that is uncountable, that every non-degenerate interval is uncountable, that the collection of continuous functions deﬁned on is of a greater cardinality than , and that there are sets of greater and greater cardinality."" I understand that (via composition with the continuous function or ) these sets of continuous functions have the same cardinality. Therefore, which claim is correct, and how do I prove this?","\mathbb{R\to R} \mathfrak c [0, 1]\to\mathbb{R} \mathfrak c \mathbb{Q} \mathbb{R} [0,1] \mathbb{R} \tan \arctan","['real-analysis', 'elementary-set-theory', 'cardinals']"
63,"Contest problem: Show $\sum_{n = 1}^\infty \frac{n^2a_n}{(a_1+\cdots+a_n)^2}<\infty$ s.t., $a_i>0$, $\sum_{n = 1}^\infty \frac{1}{a_n}<\infty$ [closed]","Contest problem: Show  s.t., ,  [closed]",\sum_{n = 1}^\infty \frac{n^2a_n}{(a_1+\cdots+a_n)^2}<\infty a_i>0 \sum_{n = 1}^\infty \frac{1}{a_n}<\infty,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question The following is probably a math contest problem. I have been unable to locate the original source. Suppose that $\{a_i\}$ is a sequence of positive real numbers and the series $\displaystyle\sum_{n = 1}^\infty \frac{1}{a_n}$ converges. Show that $$\sum_{n = 1}^\infty \frac{n^2a_n}{(a_1+\cdots+a_n)^2}$$ also converges.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question The following is probably a math contest problem. I have been unable to locate the original source. Suppose that is a sequence of positive real numbers and the series converges. Show that also converges.",\{a_i\} \displaystyle\sum_{n = 1}^\infty \frac{1}{a_n} \sum_{n = 1}^\infty \frac{n^2a_n}{(a_1+\cdots+a_n)^2},"['real-analysis', 'sequences-and-series', 'contest-math']"
64,Why not include as a requirement that all functions must be continuous to be differentiable?,Why not include as a requirement that all functions must be continuous to be differentiable?,,"Theorem: Suppose that  $f : A \to \mathbb{R}$ where $A \subseteq \mathbb{R}$. If $f$ is differentiable at $x \in A$, then $f$ is continuous at $x$. This theorem is equivalent (by the contrapositive) to the result that if $f$ is not continuous at $x \in A$ then $f$ is not differentiable at $x$. Why then do authors in almost every analysis book, not take continuity of $f$ as a requirement in the definition of the derivative of $f$ when we (seemingly) end up with equivalent results? For example I don't see why this wouldn't be a good definition of the derivative of a function Definition: Let $A \subseteq \mathbb{R}$ and let $f : A \to \mathbb{R}$ be a function continuous at $a$. Let $a \in \operatorname{Int}(A)$. We define the derivative of $f$ at $a$ to be $$f'(a) = \lim_{t \to 0}\frac{f(a+t)-f(a)}{t}$$   provided the limit exists. I know this is probably a pedagogical issue, but why not take this instead as the definition of the derivative of a function?","Theorem: Suppose that  $f : A \to \mathbb{R}$ where $A \subseteq \mathbb{R}$. If $f$ is differentiable at $x \in A$, then $f$ is continuous at $x$. This theorem is equivalent (by the contrapositive) to the result that if $f$ is not continuous at $x \in A$ then $f$ is not differentiable at $x$. Why then do authors in almost every analysis book, not take continuity of $f$ as a requirement in the definition of the derivative of $f$ when we (seemingly) end up with equivalent results? For example I don't see why this wouldn't be a good definition of the derivative of a function Definition: Let $A \subseteq \mathbb{R}$ and let $f : A \to \mathbb{R}$ be a function continuous at $a$. Let $a \in \operatorname{Int}(A)$. We define the derivative of $f$ at $a$ to be $$f'(a) = \lim_{t \to 0}\frac{f(a+t)-f(a)}{t}$$   provided the limit exists. I know this is probably a pedagogical issue, but why not take this instead as the definition of the derivative of a function?",,"['calculus', 'real-analysis', 'derivatives', 'continuity', 'definition']"
65,How to prove that $\frac{\zeta(2) }{2}+\frac{\zeta (4)}{2^3}+\frac{\zeta (6)}{2^5}+\frac{\zeta (8)}{2^7}+\cdots=1$?,How to prove that ?,\frac{\zeta(2) }{2}+\frac{\zeta (4)}{2^3}+\frac{\zeta (6)}{2^5}+\frac{\zeta (8)}{2^7}+\cdots=1,"How can one prove this identity? $$\frac{\zeta(2) }{2}+\frac{\zeta (4)}{2^3}+\frac{\zeta (6)}{2^5}+\frac{\zeta (8)}{2^7}+\cdots=1$$ There is a formula for $\zeta$ values at even integers , but it involves Bernoulli numbers; simply plugging it in does not appear to be an efficient approach.","How can one prove this identity? $$\frac{\zeta(2) }{2}+\frac{\zeta (4)}{2^3}+\frac{\zeta (6)}{2^5}+\frac{\zeta (8)}{2^7}+\cdots=1$$ There is a formula for $\zeta$ values at even integers , but it involves Bernoulli numbers; simply plugging it in does not appear to be an efficient approach.",,"['real-analysis', 'sequences-and-series', 'complex-analysis', 'zeta-functions']"
66,Why did mathematicians introduce the concept of uniform continuity?,Why did mathematicians introduce the concept of uniform continuity?,,"I have solved many problems regarding uniform continuity, but still I can't understand the following: Is there any practical application of this concept, or it is just a theoretical concept? Is there any wide application of this concept in any theorem or problem? In short, how did the concept of uniform continuity arise?","I have solved many problems regarding uniform continuity, but still I can't understand the following: Is there any practical application of this concept, or it is just a theoretical concept? Is there any wide application of this concept in any theorem or problem? In short, how did the concept of uniform continuity arise?",,"['real-analysis', 'general-topology', 'continuity', 'uniform-continuity']"
67,Every subsequence of $x_n$ has a further subsequence which converges to $x$. Then the sequence $x_n$ converges to $x$.,Every subsequence of  has a further subsequence which converges to . Then the sequence  converges to .,x_n x x_n x,Is the following true? Let $x_n$ be a sequence with the following property: Every subsequence of $x_n$ has a further subsequence which converges to $x$. Then the sequence $x_n$ converges to $x$. I guess that it is true but I am not sure how to prove this.,Is the following true? Let $x_n$ be a sequence with the following property: Every subsequence of $x_n$ has a further subsequence which converges to $x$. Then the sequence $x_n$ converges to $x$. I guess that it is true but I am not sure how to prove this.,,"['real-analysis', 'sequences-and-series', 'convergence-divergence', 'metric-spaces']"
68,Terence Tao–type books in other fields?,Terence Tao–type books in other fields?,,"I have looked at Tao's book on Measure Theory, and they are perhaps the best math books I have ever seen. Besides the extremely clear and motivated presentation, the main feature of the book is that there is no big list of exercises at the end of each chapter; the exercises are dispersed throughout the text, and they are actually critical in developing the theory. Question: What are some other math books written in this style, or other authors who write in this way? I am open to any fields of math, since I will use this question in the future as a reference. That was the question; the following is just why I think Tao's style is so great. When you come to an exercise, you know that you are ready for it. There is no doubt in the back of your mind that ""maybe I haven't read enough of the chapter to solve this exercise"" Similarly, there is no bad feeling of ""maybe I wasn't supposed to use this more advanced theorem for this exercise, maybe I was supposed to do it from the basic definitions but I can't"". It makes everything feel ""fair game"" It makes it difficult to be a passive reader It makes you become invested in the development of the theory, as if you are living back in 1900 and trying to develop this stuff for the first time I think you can achieve a similar effect with almost any other book, if you try to prove every theorem by yourself before you read the proof and stuff like that,  but at least for me there are some severe psychological barriers that prevent me from doing that. For example, if I try to prove a theorem without reading the proof, I always have the doubt that ""this proof may be too hard, it would not be expected of the reader to come up with this proof"". In Tao's book, the proofs are conciously left to you, so you know that you can do it, which is a big encouragement.","I have looked at Tao's book on Measure Theory, and they are perhaps the best math books I have ever seen. Besides the extremely clear and motivated presentation, the main feature of the book is that there is no big list of exercises at the end of each chapter; the exercises are dispersed throughout the text, and they are actually critical in developing the theory. Question: What are some other math books written in this style, or other authors who write in this way? I am open to any fields of math, since I will use this question in the future as a reference. That was the question; the following is just why I think Tao's style is so great. When you come to an exercise, you know that you are ready for it. There is no doubt in the back of your mind that ""maybe I haven't read enough of the chapter to solve this exercise"" Similarly, there is no bad feeling of ""maybe I wasn't supposed to use this more advanced theorem for this exercise, maybe I was supposed to do it from the basic definitions but I can't"". It makes everything feel ""fair game"" It makes it difficult to be a passive reader It makes you become invested in the development of the theory, as if you are living back in 1900 and trying to develop this stuff for the first time I think you can achieve a similar effect with almost any other book, if you try to prove every theorem by yourself before you read the proof and stuff like that,  but at least for me there are some severe psychological barriers that prevent me from doing that. For example, if I try to prove a theorem without reading the proof, I always have the doubt that ""this proof may be too hard, it would not be expected of the reader to come up with this proof"". In Tao's book, the proofs are conciously left to you, so you know that you can do it, which is a big encouragement.",,"['real-analysis', 'abstract-algebra', 'complex-analysis', 'reference-request', 'soft-question']"
69,Difference between continuity and uniform continuity,Difference between continuity and uniform continuity,,"I understand the geometric differences between continuity and uniform continuity, but I don't quite see how the differences between those two are apparent from their definitions. For example, my book defines continuity as: Definition 4.3.1. A function $f:A \to \mathbb R$ is continuous at a point $c \in A$ if, for all $\epsilon > 0$, there exists a $\delta > 0$ such that whenever $|x-c| < \delta$ (and $x \in A$) it follows that $|f(x)-f(c)| < \epsilon$. Uniform continuity is defined as: Definition 4.4.5. A function $f:A \to \mathbb R$ is uniformly continuous on $A$ if for every $\epsilon > 0$ there exists a $\delta > 0$ such that $|x-y| < \delta$ implies $|f(x)-f(y)| < \epsilon$. I know that in Definition 4.3.1, $\delta$ can depend on $c$, while in definition 4.4.5, $\delta$ cannot depend on $x$ or $y$, but how is this apparent from the definition? From what appears to me, it just seems like the only difference between Definition 4.3.1 and Definition 4.4.5 is that the letter $c$ was changed to a $y$. My guess is that the first definition treats $c$ as a fixed point and it is only $x$ that varies, so in this case, $\delta$ can depend on $c$ since $c$ doesn't change. Whereas for the second definition, neither $x$ or $y$ are fixed, rather they can take on values across the whole domain, $A$. In this case, if we set a $\delta$ such that it depended on $y$, then when we pick a different $y$, the same $\delta$ may not work anymore. Is this somewhat a correct interpretation? Anymore clarifications, examples, would be appreciated.","I understand the geometric differences between continuity and uniform continuity, but I don't quite see how the differences between those two are apparent from their definitions. For example, my book defines continuity as: Definition 4.3.1. A function $f:A \to \mathbb R$ is continuous at a point $c \in A$ if, for all $\epsilon > 0$, there exists a $\delta > 0$ such that whenever $|x-c| < \delta$ (and $x \in A$) it follows that $|f(x)-f(c)| < \epsilon$. Uniform continuity is defined as: Definition 4.4.5. A function $f:A \to \mathbb R$ is uniformly continuous on $A$ if for every $\epsilon > 0$ there exists a $\delta > 0$ such that $|x-y| < \delta$ implies $|f(x)-f(y)| < \epsilon$. I know that in Definition 4.3.1, $\delta$ can depend on $c$, while in definition 4.4.5, $\delta$ cannot depend on $x$ or $y$, but how is this apparent from the definition? From what appears to me, it just seems like the only difference between Definition 4.3.1 and Definition 4.4.5 is that the letter $c$ was changed to a $y$. My guess is that the first definition treats $c$ as a fixed point and it is only $x$ that varies, so in this case, $\delta$ can depend on $c$ since $c$ doesn't change. Whereas for the second definition, neither $x$ or $y$ are fixed, rather they can take on values across the whole domain, $A$. In this case, if we set a $\delta$ such that it depended on $y$, then when we pick a different $y$, the same $\delta$ may not work anymore. Is this somewhat a correct interpretation? Anymore clarifications, examples, would be appreciated.",,"['real-analysis', 'self-learning']"
70,"What is integration by parts, really?","What is integration by parts, really?",,"Integration by parts comes up a lot - for instance, it appears in the definition of a weak derivative / distributional derivative, or as a tool that one can use to turn information about higher derivatives of a function into information about an integral of that function. Concrete examples of this latter category include: proving that $f \in C^2(S^1)$ implies that the Fourier series of $f$ converges absolutely and uniformly, and the Taylor series expansion with the integral formula for remainder. However, I don't feel like I really understand what integration by parts is really doing. To me, it is just an algebraic trick that follows from the fundamental theorem of calculus and the product rule. Is there some more conceptual way to think about it? How do you think about this useful idea?","Integration by parts comes up a lot - for instance, it appears in the definition of a weak derivative / distributional derivative, or as a tool that one can use to turn information about higher derivatives of a function into information about an integral of that function. Concrete examples of this latter category include: proving that $f \in C^2(S^1)$ implies that the Fourier series of $f$ converges absolutely and uniformly, and the Taylor series expansion with the integral formula for remainder. However, I don't feel like I really understand what integration by parts is really doing. To me, it is just an algebraic trick that follows from the fundamental theorem of calculus and the product rule. Is there some more conceptual way to think about it? How do you think about this useful idea?",,"['real-analysis', 'integration', 'analysis', 'soft-question']"
71,What is the intuition behind uniform continuity?,What is the intuition behind uniform continuity?,,"There’s another post asking for the motivation behind uniform continuity. I’m not a huge fan of it since the top-rated comment spoke about local and global interactions of information, and frankly I just did not get it. Playing with the definition, I want to say uniform continuity implies there’s a maximum “average rate of change”. Not literally a derivative, but the rate of change between two points is bounded in the domain. I’m aware that this is essentially Lipschitz continuity, and that Lipschitz implies uniform. This implies there’s more to uniform continuity than just having a bounded average rate of change. And also, how is it that $ f(x)=x$ is uniform yet $f(x)f(x)=g(x)=x^2$ is not? I understand why it isn’t, I can prove it. But I just don’t understand the motivation and importance of uniform continuity.","There’s another post asking for the motivation behind uniform continuity. I’m not a huge fan of it since the top-rated comment spoke about local and global interactions of information, and frankly I just did not get it. Playing with the definition, I want to say uniform continuity implies there’s a maximum “average rate of change”. Not literally a derivative, but the rate of change between two points is bounded in the domain. I’m aware that this is essentially Lipschitz continuity, and that Lipschitz implies uniform. This implies there’s more to uniform continuity than just having a bounded average rate of change. And also, how is it that is uniform yet is not? I understand why it isn’t, I can prove it. But I just don’t understand the motivation and importance of uniform continuity.", f(x)=x f(x)f(x)=g(x)=x^2,"['real-analysis', 'intuition', 'uniform-continuity', 'lipschitz-functions']"
72,Direct proof that $\pi$ is not constructible,Direct proof that  is not constructible,\pi,"Is there a direct proof that $\pi$ is not constructible, that is, that squaring the circle cannot be done by rule and compass? Of course, $\pi$ is not constructible because it is transcendental and so is not a root of any polynomial with rational coefficients. But is there a simple direct proof that $\pi$ is not a root of polynomial of degree $2^n$ with rational coefficients? The kind of proof I seek is one by induction on the height of a tower of quadratic extensions, one that ultimately relies on a proof that $\pi$ is not rational. Does any one know of a proof along these lines or any other direct proof? I just want a direct proof that $\pi$ is not constructible without appealing to transcendence.","Is there a direct proof that $\pi$ is not constructible, that is, that squaring the circle cannot be done by rule and compass? Of course, $\pi$ is not constructible because it is transcendental and so is not a root of any polynomial with rational coefficients. But is there a simple direct proof that $\pi$ is not a root of polynomial of degree $2^n$ with rational coefficients? The kind of proof I seek is one by induction on the height of a tower of quadratic extensions, one that ultimately relies on a proof that $\pi$ is not rational. Does any one know of a proof along these lines or any other direct proof? I just want a direct proof that $\pi$ is not constructible without appealing to transcendence.",,"['real-analysis', 'number-theory', 'irrational-numbers', 'geometric-construction']"
73,Conjectured formula for the Fabius function,Conjectured formula for the Fabius function,,"The Fabius function is the unique  function ${\bf F}:\mathbb R\to[-1, 1]$ satisfying the following conditions: a functional–integral equation $\require{action} \require{enclose}{^{\texttip{\dagger}{a poet or philosopher could say ""it knows and replays its own future""}}}$ $\displaystyle{\bf F}(x)={\small\int_0^{2{\tiny\text{ }}x}}{\bf F}\left(t\right)dt,\,$ and a normalization condition ${\bf F}(1)=1.$ It is noteworthy that despite being smooth (class $C^\infty$ ) ${^{\texttip{\dagger}{i.e. continuous and having continuous derivatives of any order}}}$ everywhere, ${\bf F}(x)$ is not real-analytic at any point $x\ge0$ — its Taylor series there is either divergent , or a polynomial with a finite number of terms. ${^{\texttip{\dagger}{the latter happens at dyadic rational points}}}$ Unlike some other functions that appear specifically constructed for that purpose, the Fabius function naturally occurs in research of several seemingly unrelated problems — perhaps, this is why it has been discovered and re-discovered independently many times by several mathematicians. It also has some practical applications in computational mathematics. The Fabius function ${\bf F}(x)$ is constant $0$ for all $x\le0,$ but some authors prefer to define it only on $[0,\infty)$ or only on $[0,1].$ Some authors study a very closely related Rvachev ${^{\texttip{\dagger}{also variously spelled as Rvachëv or Rvachyov, a.k.a. ""atomic function""}}}$ function $\operatorname{up}(x)$ defined as $\operatorname{up}(x) = {\bf F}(x+1)$ for $-1\le x\le1$ and $\operatorname{up}(x) = 0$ otherwise. It is known that the Fabius function assumes rational values at dyadic rational arguments — efficient algorithms to compute those values have been published or posted online, and have been discussed on this site. $^{[1]}$ $\!^{[2]}$ $\!^{[3]}$ $\!^{[4]}$ $\!^{[5]}$ $\!^{[6]}$ $\!^{[7]}$ $\!^{[8]}$ $\!^{[9]}$ I have been looking for a non-recursive, self-contained formula for the Fabius function for quite a long time. After lots of experimentation and looking for patterns in its values I came up with a conjectured empirical formula. Let ${^{\texttip{\dagger}{the superscript 𝑚 is just an index here, not to be confused with a power}}}$ $$\mathscr F^m_n = \frac1{2^{n^2}\left(\frac12;{\tiny\text{ }}\frac12\right)_n}\,\sum _{k=0}^n\frac{\binom n k_{1/2}}{2^{{\tiny\text{ }}k{\tiny\text{ }}(k-1)}(n+k)!}\,\sum _{\ell=0}^{2^k{\tiny\text{ }}m-1}\,(-1)^{s_2\left(\ell\right)}\,\left(\ell-2^km+\tfrac12\right)^{n+k}{\small,}\tag{$\small\spadesuit$}$$ where $k,\,\ell,\,m,\,n$ are non-negative integers, $\displaystyle\small\left(a;{\tiny\text{ }}q\right)_n=\prod_{k=0}^{n-1} (1-a{\tiny\text{ }}q^k)$ is the q -Pochhammer symbol ${^{\texttip{\dagger}{it assumes only rational values in this formula}}}$ , ${\binom n k}_q=\displaystyle\small\frac{\left(q;{\tiny\text{ }}q\right)_n}{\left(q;{\tiny\text{ }}q\right)_k\left(q;{\tiny\text{ }}q\right)_{n-k}}\vphantom{\Huge|}$ is the q -binomial coefficient ${^{\texttip{\dagger}{it assumes only rational values in this formula}}}$ , and $s_2(n)\vphantom{\Huge|}$ is the sum of binary digits ${^{\texttip{\dagger}{i.e. the number of 1's in the base-2 representation}}}$ of $n$ (note that $(-1)^{s_2\left(n\right)} = t_n\vphantom{\Huge|}$ is just the signed version of the Thue–Morse sequence , satisfying recurrence $t_0 = 1,\,t_n = (-1)^n \, t_{\lfloor n/2\rfloor};\vphantom{\Huge|}$ see also ${^{[10]}}$ ${\!^{[11]}}$ ${\!^{[12]}}$ ). I conjecture that for all non-negative integers $m,\,n,$ the following identity holds: $${\bf F}\!\left({\small\frac m{2^n}}\right)=\mathscr F^m_n.\tag{$\small\diamondsuit$}$$ Note that there is no requirement for $\frac m{2^n}$ to be a proper irreducible fraction — it might well be that $m$ is even, or $m>2^n$ . I have not been able to rigorously prove it yet, but it produces exact rational values that agree with those computed using known correct algorithms for all dyadic rational arguments I have tried. Of course, this formula did not just appear out of the blue — its structure was inspired by known algorithms, and it went a long way to this relatively concise form (at some point it had about $5$ levels of nested summations/products). Can we prove that $\small(\diamondsuit)$ is indeed a correct representation of the Fabius function at dyadic rationals? If the conjecture is difficult to tackle at once, we can try to at least prove that the function defined by $\small(\spadesuit)$ shares some known properties with the Fabius function, e.g.: $0\le\mathscr F^m_n\le1,$ $\mathscr F^m_n = \mathscr F^{2{\tiny\text{ }}m}_{n+1}\vphantom{\Large|}$ — the result is the same across all representations of a rational number $\frac m{2^n}$ , $\mathscr F^m_n = 1-\mathscr F^{(2^n-m)}_n\vphantom{\Large|}$ for all $\small0\le m\le2^{n}$ — rotation symmetry, $\mathscr F^m_n = \mathscr F^{(2^{n+1}-m)}_n\vphantom{\Large|}$ for all $\small0\le m\le2^{n+1}$ — reflection symmetry, for a fixed $n,$ and $\small0\le m\le2^n$ the values of $\mathscr F^m_n\vphantom{\Large|}$ strictly increase, all points $\small\left(\frac m{2^n},\,\mathscr F^m_n\right)\vphantom{\Large|}$ lie on a continuous curve — for any convergent sequence of dyadic rationals $\frac m{2^n}\vphantom{\Large|}$ , the sequence of corresponding values $\mathscr F^m_n\vphantom{\Large|}$ also converges. If the conjecture $\small(\diamondsuit)$ it true, then for all $0\le x\le1$ (not necessary rational) $${\bf F}\!\left(x\right)=\lim_{n\to\infty}\,\frac1{2^{n^2}\left(\frac12;{\tiny\text{ }}\frac12\right)_n}\,\sum _{k=0}^n\frac{\binom n k_{1/2}}{2^{{\tiny\text{ }}k{\tiny\text{ }}(k-1)}(n+k)!}\,\sum _{\ell=0}^{\left\lfloor2^{n+k}{\tiny\text{ }}x-1\right\rfloor}\,(-1)^{s_2\left(\ell\right)}\,\left(\ell-2^{n+k}{\tiny\text{ }}x+\tfrac12\right)^{n+k}.\tag{$\small\heartsuit$}$$ For dyadic rational $x$ the limit is trivial, because the sequence under the limit becomes constant for large enough $n$ . Also, there is a known series (by Rvachev) that expresses values of the Fabius function for all $0\le x\le1$ via its values at negative powers of $2$ : $${\bf F}\!\left(x\right)=\sum _{n=1}^\infty\frac{(-1)^{\left\lfloor 2^n{\tiny\text{ }}x\right\rfloor }-1}{2}\,(-1)^{s_2\left(\lfloor2^n{\tiny\text{ }}x\rfloor\right)}\;\sum_{k=0}^n\frac{2^{\frac{k{\tiny\text{ }}(k+1)}2}}{k!}\,\left(x-2^{-n}\left\lfloor 2^n{\tiny\text{ }}x\right\rfloor\right)^k\;{\bf F}\!\left(2^{{\tiny\text{ }}k-n}\right).\tag{$\small\clubsuit$}$$ Again, for dyadic rational $x$ this series terminates after a finite number of terms, producing an exact rational value.","The Fabius function is the unique  function satisfying the following conditions: a functional–integral equation and a normalization condition It is noteworthy that despite being smooth (class ) everywhere, is not real-analytic at any point — its Taylor series there is either divergent , or a polynomial with a finite number of terms. Unlike some other functions that appear specifically constructed for that purpose, the Fabius function naturally occurs in research of several seemingly unrelated problems — perhaps, this is why it has been discovered and re-discovered independently many times by several mathematicians. It also has some practical applications in computational mathematics. The Fabius function is constant for all but some authors prefer to define it only on or only on Some authors study a very closely related Rvachev function defined as for and otherwise. It is known that the Fabius function assumes rational values at dyadic rational arguments — efficient algorithms to compute those values have been published or posted online, and have been discussed on this site. I have been looking for a non-recursive, self-contained formula for the Fabius function for quite a long time. After lots of experimentation and looking for patterns in its values I came up with a conjectured empirical formula. Let where are non-negative integers, is the q -Pochhammer symbol , is the q -binomial coefficient , and is the sum of binary digits of (note that is just the signed version of the Thue–Morse sequence , satisfying recurrence see also ). I conjecture that for all non-negative integers the following identity holds: Note that there is no requirement for to be a proper irreducible fraction — it might well be that is even, or . I have not been able to rigorously prove it yet, but it produces exact rational values that agree with those computed using known correct algorithms for all dyadic rational arguments I have tried. Of course, this formula did not just appear out of the blue — its structure was inspired by known algorithms, and it went a long way to this relatively concise form (at some point it had about levels of nested summations/products). Can we prove that is indeed a correct representation of the Fabius function at dyadic rationals? If the conjecture is difficult to tackle at once, we can try to at least prove that the function defined by shares some known properties with the Fabius function, e.g.: — the result is the same across all representations of a rational number , for all — rotation symmetry, for all — reflection symmetry, for a fixed and the values of strictly increase, all points lie on a continuous curve — for any convergent sequence of dyadic rationals , the sequence of corresponding values also converges. If the conjecture it true, then for all (not necessary rational) For dyadic rational the limit is trivial, because the sequence under the limit becomes constant for large enough . Also, there is a known series (by Rvachev) that expresses values of the Fabius function for all via its values at negative powers of : Again, for dyadic rational this series terminates after a finite number of terms, producing an exact rational value.","{\bf F}:\mathbb R\to[-1, 1] \require{action}
\require{enclose}{^{\texttip{\dagger}{a poet or philosopher could say ""it knows and replays its own future""}}} \displaystyle{\bf F}(x)={\small\int_0^{2{\tiny\text{ }}x}}{\bf F}\left(t\right)dt,\, {\bf F}(1)=1. C^\infty {^{\texttip{\dagger}{i.e. continuous and having continuous derivatives of any order}}} {\bf F}(x) x\ge0 {^{\texttip{\dagger}{the latter happens at dyadic rational points}}} {\bf F}(x) 0 x\le0, [0,\infty) [0,1]. {^{\texttip{\dagger}{also variously spelled as Rvachëv or Rvachyov, a.k.a. ""atomic function""}}} \operatorname{up}(x) \operatorname{up}(x) = {\bf F}(x+1) -1\le x\le1 \operatorname{up}(x) = 0 ^{[1]} \!^{[2]} \!^{[3]} \!^{[4]} \!^{[5]} \!^{[6]} \!^{[7]} \!^{[8]} \!^{[9]} {^{\texttip{\dagger}{the superscript 𝑚 is just an index here, not to be confused with a power}}} \mathscr F^m_n = \frac1{2^{n^2}\left(\frac12;{\tiny\text{ }}\frac12\right)_n}\,\sum _{k=0}^n\frac{\binom n k_{1/2}}{2^{{\tiny\text{ }}k{\tiny\text{ }}(k-1)}(n+k)!}\,\sum _{\ell=0}^{2^k{\tiny\text{ }}m-1}\,(-1)^{s_2\left(\ell\right)}\,\left(\ell-2^km+\tfrac12\right)^{n+k}{\small,}\tag{\small\spadesuit} k,\,\ell,\,m,\,n \displaystyle\small\left(a;{\tiny\text{ }}q\right)_n=\prod_{k=0}^{n-1} (1-a{\tiny\text{ }}q^k) {^{\texttip{\dagger}{it assumes only rational values in this formula}}} {\binom n k}_q=\displaystyle\small\frac{\left(q;{\tiny\text{ }}q\right)_n}{\left(q;{\tiny\text{ }}q\right)_k\left(q;{\tiny\text{ }}q\right)_{n-k}}\vphantom{\Huge|} {^{\texttip{\dagger}{it assumes only rational values in this formula}}} s_2(n)\vphantom{\Huge|} {^{\texttip{\dagger}{i.e. the number of 1's in the base-2 representation}}} n (-1)^{s_2\left(n\right)} = t_n\vphantom{\Huge|} t_0 = 1,\,t_n = (-1)^n \, t_{\lfloor n/2\rfloor};\vphantom{\Huge|} {^{[10]}} {\!^{[11]}} {\!^{[12]}} m,\,n, {\bf F}\!\left({\small\frac m{2^n}}\right)=\mathscr F^m_n.\tag{\small\diamondsuit} \frac m{2^n} m m>2^n 5 \small(\diamondsuit) \small(\spadesuit) 0\le\mathscr F^m_n\le1, \mathscr F^m_n = \mathscr F^{2{\tiny\text{ }}m}_{n+1}\vphantom{\Large|} \frac m{2^n} \mathscr F^m_n = 1-\mathscr F^{(2^n-m)}_n\vphantom{\Large|} \small0\le m\le2^{n} \mathscr F^m_n = \mathscr F^{(2^{n+1}-m)}_n\vphantom{\Large|} \small0\le m\le2^{n+1} n, \small0\le m\le2^n \mathscr F^m_n\vphantom{\Large|} \small\left(\frac m{2^n},\,\mathscr F^m_n\right)\vphantom{\Large|} \frac m{2^n}\vphantom{\Large|} \mathscr F^m_n\vphantom{\Large|} \small(\diamondsuit) 0\le x\le1 {\bf F}\!\left(x\right)=\lim_{n\to\infty}\,\frac1{2^{n^2}\left(\frac12;{\tiny\text{ }}\frac12\right)_n}\,\sum _{k=0}^n\frac{\binom n k_{1/2}}{2^{{\tiny\text{ }}k{\tiny\text{ }}(k-1)}(n+k)!}\,\sum _{\ell=0}^{\left\lfloor2^{n+k}{\tiny\text{ }}x-1\right\rfloor}\,(-1)^{s_2\left(\ell\right)}\,\left(\ell-2^{n+k}{\tiny\text{ }}x+\tfrac12\right)^{n+k}.\tag{\small\heartsuit} x n 0\le x\le1 2 {\bf F}\!\left(x\right)=\sum _{n=1}^\infty\frac{(-1)^{\left\lfloor 2^n{\tiny\text{ }}x\right\rfloor }-1}{2}\,(-1)^{s_2\left(\lfloor2^n{\tiny\text{ }}x\rfloor\right)}\;\sum_{k=0}^n\frac{2^{\frac{k{\tiny\text{ }}(k+1)}2}}{k!}\,\left(x-2^{-n}\left\lfloor 2^n{\tiny\text{ }}x\right\rfloor\right)^k\;{\bf F}\!\left(2^{{\tiny\text{ }}k-n}\right).\tag{\small\clubsuit} x","['real-analysis', 'sequences-and-series', 'conjectures', 'experimental-mathematics', 'q-analogs']"
74,"If a two variable smooth function has two global minima, will it necessarily have a third critical point?","If a two variable smooth function has two global minima, will it necessarily have a third critical point?",,"Assume that $f:\mathbb{R}^2\to\mathbb{R}$ a $C^{\infty}$ function that has exactly two minimum global points. Is it true that $f$ has always another critical point? A standard visualization trick is to imagine a terrain of height $f(x,y)$ at the point $(x,y)$ , and then imagine an endless rain pouring with water level rising steadily on the entire plane. Because there are only two global minima, they must both be isolated local minima also. Therefore, initially the water will collect into two small lakes around the minima. Those two points are connected by a compact line segment $K$ . As a continuous function, $f$ attains a maximum value $M$ on the set $K$ . This means that when the water level has reached $M$ , the two lakes will have been merged. The set $S$ of water levels $z$ such that two lakes are connected is thus non-empty and bounded from below. Therefore it has an infimum $m$ . It is natural to think that at water height $m$ there should be a critical point. A saddle point is easy to visualize. For example the function (originally suggested in a deleted answer) $f(x,y)=x^2+y^2(1-y)^2$ has a saddle point at the midway point between the two local minima at $(0,0)$ and $(0,1)$ . But, can we prove that one always exists? Follow-ups: Does the answer change, if we replace $\Bbb{R}^2$ with a compact domain? What if $f$ is a $C^\infty$ function on a torus ( $S^1\times S^1$ ) or the surface of a sphere ( $S^2$ ). Ok, on a compact domain the function will have a maximum, but if we assume only isolated critical points, what else is implied by the presence of two global minima? Similarly, what if we have local minima instead of global? If it makes a difference you are also welcome to introduce an extra condition (like when the domain is not compact you could still assume the derivatives to be bounded - not sure that would be at all relevant, but who knows).","Assume that a function that has exactly two minimum global points. Is it true that has always another critical point? A standard visualization trick is to imagine a terrain of height at the point , and then imagine an endless rain pouring with water level rising steadily on the entire plane. Because there are only two global minima, they must both be isolated local minima also. Therefore, initially the water will collect into two small lakes around the minima. Those two points are connected by a compact line segment . As a continuous function, attains a maximum value on the set . This means that when the water level has reached , the two lakes will have been merged. The set of water levels such that two lakes are connected is thus non-empty and bounded from below. Therefore it has an infimum . It is natural to think that at water height there should be a critical point. A saddle point is easy to visualize. For example the function (originally suggested in a deleted answer) has a saddle point at the midway point between the two local minima at and . But, can we prove that one always exists? Follow-ups: Does the answer change, if we replace with a compact domain? What if is a function on a torus ( ) or the surface of a sphere ( ). Ok, on a compact domain the function will have a maximum, but if we assume only isolated critical points, what else is implied by the presence of two global minima? Similarly, what if we have local minima instead of global? If it makes a difference you are also welcome to introduce an extra condition (like when the domain is not compact you could still assume the derivatives to be bounded - not sure that would be at all relevant, but who knows).","f:\mathbb{R}^2\to\mathbb{R} C^{\infty} f f(x,y) (x,y) K f M K M S z m m f(x,y)=x^2+y^2(1-y)^2 (0,0) (0,1) \Bbb{R}^2 f C^\infty S^1\times S^1 S^2","['real-analysis', 'multivariable-calculus', 'differential-topology', 'morse-theory']"
75,Is there a function with a removable discontinuity at every point?,Is there a function with a removable discontinuity at every point?,,"If memory serves, ten years ago to the week (or so), I taught first semester freshman calculus for the first time.  As many calculus instructors do, I decided I should ask some extra credit questions to get students to think more deeply about the material.  The first one I asked was this: 1) Recall that a function $f: \mathbb{R} \rightarrow \mathbb{R}$ is said to have a removable discontinuity at a point $x_0 \in \mathbb{R}$ if $\lim_{x \rightarrow x_0} f(x)$ exists but not does not equal $f(x_0)$.  Does there exist a function $f$ which has a removable discontinuity at $x_0$ for every $x_0 \in \mathbb{R}$? Commentary: if so, we could define a new function $\tilde{f}(x_0) = \lim_{x \rightarrow x_0} f(x)$ and it seems at least that $\tilde{f}$ has a fighting chance to be continuous on $\mathbb{R}$.  Thus we have successfully ""removed the discontinuities"" of $f$, but in so doing we have changed the value at every point! Remark: Lest you think this is too silly to even seriously contemplate, consider the function $f: \mathbb{Q} \rightarrow \mathbb{Q}$ given by $f(0) = 1$ and for a nonzero  rational number $\frac{p}{q}$, $f(\frac{p}{q}) = \frac{1}{q}$.  It is easy to see that this function has limit $0$ at every (rational) point! So I mentioned this problem to my students.  A week later, the only person who asked me about it at all was my Teaching Assistant, who was an older undergraduate, not even a math major, I think.  (I hasten to add that this was not in any sense an honors calculus class, i.e., I was pretty clueless back then.)  Thinking about it a bit, I asked him if he knew about uncountable sets, and he said that he didn't.  At that point I realized that I didn't have a solution in mind that he would understand (so still less so for the freshman calculus students) and I advised him to forget all about it. So my actual question is : can you solve this problem using only the concepts in a non-honors freshman calculus textbook?  (In particular, without using notions of un/countability?) [ Addendum : Let me say explicitly that I would welcome an answer that proceeds directly in terms of the least upper bound axiom.  Most freshman calculus books do include this, albeit somewhere hidden from view of the casual readers, i.e., actual freshman calculus students.] If you can't figure out how to answer the question at all, I think the following related question helps. 2) Define a function $f: \mathbb{R} \rightarrow \mathbb{R}$ to be precontinuous if the limit exists at every point.  For such a function, we can define $\tilde{f}$ as above.  Prove/disprove that, as suggested above, $\tilde{f}$ is indeed continuous.  [Then think about $f - \tilde{f}$.] Now that I think about it, there is an entire little area here that I don't know anything about, e.g. 3) The set of discontinuities of an arbitrary function is known -- any $F_{\sigma}$ set inside $\mathbb{R}$ can serve.  What can we say about the set of discontinuities of a ""precontinuous function""?  [Edit: from the link provided in Chandru1's answer, we see that it is countable.  What else can we say?  Note that taking the above example and extending by $0$ to the irrationals, we see that the set of points of discontinuity of a precontinuous function can be dense.]","If memory serves, ten years ago to the week (or so), I taught first semester freshman calculus for the first time.  As many calculus instructors do, I decided I should ask some extra credit questions to get students to think more deeply about the material.  The first one I asked was this: 1) Recall that a function $f: \mathbb{R} \rightarrow \mathbb{R}$ is said to have a removable discontinuity at a point $x_0 \in \mathbb{R}$ if $\lim_{x \rightarrow x_0} f(x)$ exists but not does not equal $f(x_0)$.  Does there exist a function $f$ which has a removable discontinuity at $x_0$ for every $x_0 \in \mathbb{R}$? Commentary: if so, we could define a new function $\tilde{f}(x_0) = \lim_{x \rightarrow x_0} f(x)$ and it seems at least that $\tilde{f}$ has a fighting chance to be continuous on $\mathbb{R}$.  Thus we have successfully ""removed the discontinuities"" of $f$, but in so doing we have changed the value at every point! Remark: Lest you think this is too silly to even seriously contemplate, consider the function $f: \mathbb{Q} \rightarrow \mathbb{Q}$ given by $f(0) = 1$ and for a nonzero  rational number $\frac{p}{q}$, $f(\frac{p}{q}) = \frac{1}{q}$.  It is easy to see that this function has limit $0$ at every (rational) point! So I mentioned this problem to my students.  A week later, the only person who asked me about it at all was my Teaching Assistant, who was an older undergraduate, not even a math major, I think.  (I hasten to add that this was not in any sense an honors calculus class, i.e., I was pretty clueless back then.)  Thinking about it a bit, I asked him if he knew about uncountable sets, and he said that he didn't.  At that point I realized that I didn't have a solution in mind that he would understand (so still less so for the freshman calculus students) and I advised him to forget all about it. So my actual question is : can you solve this problem using only the concepts in a non-honors freshman calculus textbook?  (In particular, without using notions of un/countability?) [ Addendum : Let me say explicitly that I would welcome an answer that proceeds directly in terms of the least upper bound axiom.  Most freshman calculus books do include this, albeit somewhere hidden from view of the casual readers, i.e., actual freshman calculus students.] If you can't figure out how to answer the question at all, I think the following related question helps. 2) Define a function $f: \mathbb{R} \rightarrow \mathbb{R}$ to be precontinuous if the limit exists at every point.  For such a function, we can define $\tilde{f}$ as above.  Prove/disprove that, as suggested above, $\tilde{f}$ is indeed continuous.  [Then think about $f - \tilde{f}$.] Now that I think about it, there is an entire little area here that I don't know anything about, e.g. 3) The set of discontinuities of an arbitrary function is known -- any $F_{\sigma}$ set inside $\mathbb{R}$ can serve.  What can we say about the set of discontinuities of a ""precontinuous function""?  [Edit: from the link provided in Chandru1's answer, we see that it is countable.  What else can we say?  Note that taking the above example and extending by $0$ to the irrationals, we see that the set of points of discontinuity of a precontinuous function can be dense.]",,"['calculus', 'real-analysis']"
76,Completion of rational numbers via Cauchy sequences,Completion of rational numbers via Cauchy sequences,,Can anyone recommend a good self-contained reference for completion of rationals to get reals using Cauchy sequences?,Can anyone recommend a good self-contained reference for completion of rationals to get reals using Cauchy sequences?,,"['real-analysis', 'reference-request', 'real-numbers', 'cauchy-sequences']"
77,Simplest proof of Taylor's theorem,Simplest proof of Taylor's theorem,,"I have for some time been trawling through the Internet looking for an aesthetic proof of Taylor's theorem. By which I mean this: there are plenty of proofs that introduce some arbitrary construct: no mention is given of from whence this beast came.  and you can logically hack away line by line until the thing is solved. but this kind of proof is ugly.  a beautiful proof should rise naturally from the ground. I've seen one proof claiming to do it from the fundamental theorem of calculus. It looked messy. I've seen several attempts to use integration by parts repeatedly. But surely it would be tidier to do this without bringing in  all of that extra machinery. The nicest two approaches seem to involve using the mean value theorem and Rolle's theorem.  but I can't find a lucid presentation of either approach. Maybe my brain is unusually stupid, and the approaches on Wikipedia etc are perfectly good enough for everyone else. Does anyone have a crystal clear understanding of this phenomenon? Or a web-link to such an understanding? *EDIT*: Eventually a Cambridge mathematician explained it to me in a way that I could understand, and I have written up the proof here . To my mind it is the most instructional proof I have encountered, yet putting it as an answer received mostly downvotes. It seems strange to me that no one else seems to concur.  But it should be up to the keenest mathematical minds to choose which answer should be accepted. It shouldn't be up to me. Therefore I will bow to the wisdom of the community, and accept the currently most-upvoted answer. I have learned from Machine Learning that a ""Committee of Experts"" outperforms any one expert, and I am certainly no expert.","I have for some time been trawling through the Internet looking for an aesthetic proof of Taylor's theorem. By which I mean this: there are plenty of proofs that introduce some arbitrary construct: no mention is given of from whence this beast came.  and you can logically hack away line by line until the thing is solved. but this kind of proof is ugly.  a beautiful proof should rise naturally from the ground. I've seen one proof claiming to do it from the fundamental theorem of calculus. It looked messy. I've seen several attempts to use integration by parts repeatedly. But surely it would be tidier to do this without bringing in  all of that extra machinery. The nicest two approaches seem to involve using the mean value theorem and Rolle's theorem.  but I can't find a lucid presentation of either approach. Maybe my brain is unusually stupid, and the approaches on Wikipedia etc are perfectly good enough for everyone else. Does anyone have a crystal clear understanding of this phenomenon? Or a web-link to such an understanding? *EDIT*: Eventually a Cambridge mathematician explained it to me in a way that I could understand, and I have written up the proof here . To my mind it is the most instructional proof I have encountered, yet putting it as an answer received mostly downvotes. It seems strange to me that no one else seems to concur.  But it should be up to the keenest mathematical minds to choose which answer should be accepted. It shouldn't be up to me. Therefore I will bow to the wisdom of the community, and accept the currently most-upvoted answer. I have learned from Machine Learning that a ""Committee of Experts"" outperforms any one expert, and I am certainly no expert.",,"['real-analysis', 'soft-question', 'taylor-expansion']"
78,"Evaluating $\int_0^\infty \sin x^2\, dx$ with real methods?",Evaluating  with real methods?,"\int_0^\infty \sin x^2\, dx","I have seen the Fresnel integral $$\int_0^\infty \sin x^2\, dx = \sqrt{\frac{\pi}{8}}$$ evaluated by contour integration and other complex analysis methods, and I have found these methods to be the standard way to evaluate this integral.  I was wondering, however, does anyone know a real analysis method to evaluate this integral?","I have seen the Fresnel integral $$\int_0^\infty \sin x^2\, dx = \sqrt{\frac{\pi}{8}}$$ evaluated by contour integration and other complex analysis methods, and I have found these methods to be the standard way to evaluate this integral.  I was wondering, however, does anyone know a real analysis method to evaluate this integral?",,"['real-analysis', 'integration', 'trigonometry']"
79,"Does every Cauchy sequence converge to *something*, just possibly in a different space?","Does every Cauchy sequence converge to *something*, just possibly in a different space?",,"Question. If I attempt to prove that space $X$ is complete by pursuing the strategy, “Assume $x_n \rightarrow x$ ; the space $X$ is complete if $x \in X$ ,” then why is that wrong? Context. I know the definition of Cauchy sequences and convergent sequences, and that the definition of completeness is that Cauchy sequences in the space converge. And so I know that if one is attempting to prove that a space is complete, then the usual proof should start, “Assume that $x_n$ is a Cauchy sequence; we will show that $x_n$ converges in $X$ .” The misconception I seem to be battling is this: It seems like a Cauchy sequence must converge to something , just that the something might not be in the space. So it seems to me like the question really is, “Is the limit in the space or is it not in the space?” The classic example is the sequence of rationals that converges to $\sqrt{2}$ . The sequence is Cauchy within the space of the rationals, and also the sequence does converge , just to a limit that is outside the space in consideration. So recently, I began a proof of completeness with the line, “Assume $f_n \rightarrow f$ . We want to show that $f \in X$ .” And the feedback was, “Unclear what is being proved. Nothing related to completeness. 0/4 points.” It seems to me that showing that the limit of a convergent sequence resides in the space is equivalent to saying that Cauchy sequences converge. Why is that wrong? Thank you!","Question. If I attempt to prove that space is complete by pursuing the strategy, “Assume ; the space is complete if ,” then why is that wrong? Context. I know the definition of Cauchy sequences and convergent sequences, and that the definition of completeness is that Cauchy sequences in the space converge. And so I know that if one is attempting to prove that a space is complete, then the usual proof should start, “Assume that is a Cauchy sequence; we will show that converges in .” The misconception I seem to be battling is this: It seems like a Cauchy sequence must converge to something , just that the something might not be in the space. So it seems to me like the question really is, “Is the limit in the space or is it not in the space?” The classic example is the sequence of rationals that converges to . The sequence is Cauchy within the space of the rationals, and also the sequence does converge , just to a limit that is outside the space in consideration. So recently, I began a proof of completeness with the line, “Assume . We want to show that .” And the feedback was, “Unclear what is being proved. Nothing related to completeness. 0/4 points.” It seems to me that showing that the limit of a convergent sequence resides in the space is equivalent to saying that Cauchy sequences converge. Why is that wrong? Thank you!",X x_n \rightarrow x X x \in X x_n x_n X \sqrt{2} f_n \rightarrow f f \in X,"['real-analysis', 'functional-analysis', 'analysis', 'cauchy-sequences']"
80,The closed form of $\int_0^{\pi/4}\frac{\log(1-x) \tan^2(x)}{1-x\tan^2(x)} \ dx$,The closed form of,\int_0^{\pi/4}\frac{\log(1-x) \tan^2(x)}{1-x\tan^2(x)} \ dx,"What tools or ways would you propose for getting the closed form of this integral? $$\int_0^{\pi/4}\frac{\log(1-x) \tan^2(x)}{1-x\tan^2(x)} \ dx$$ EDIT : It took a while since I made this post. I'll give a little bounty for the solver of the problem, 500 points bounty. Supplementary question : Calculate $$\int_0^{\pi/4}\frac{\log(1-x)\log(x)\log(1+x) \tan^2(x)}{1-x\tan^2(x)} \ dx$$","What tools or ways would you propose for getting the closed form of this integral? $$\int_0^{\pi/4}\frac{\log(1-x) \tan^2(x)}{1-x\tan^2(x)} \ dx$$ EDIT : It took a while since I made this post. I'll give a little bounty for the solver of the problem, 500 points bounty. Supplementary question : Calculate $$\int_0^{\pi/4}\frac{\log(1-x)\log(x)\log(1+x) \tan^2(x)}{1-x\tan^2(x)} \ dx$$",,"['calculus', 'real-analysis', 'complex-analysis', 'definite-integrals', 'closed-form']"
81,"The limit of truncated sums of harmonic series, $\lim\limits_{k\to\infty}\sum_{n=k+1}^{2k}{\frac{1}{n}}$","The limit of truncated sums of harmonic series,",\lim\limits_{k\to\infty}\sum_{n=k+1}^{2k}{\frac{1}{n}},"What is the sum of the 'second half' of the harmonic series? $$\lim_{k\to\infty}\sum\limits_{n=k+1}^{2k}{\frac{1}{n}} =~ ?$$ More precisely, what is the limit of the above sequence of partial sums?","What is the sum of the 'second half' of the harmonic series? More precisely, what is the limit of the above sequence of partial sums?",\lim_{k\to\infty}\sum\limits_{n=k+1}^{2k}{\frac{1}{n}} =~ ?,"['real-analysis', 'sequences-and-series', 'limits', 'harmonic-numbers']"
82,What's the intuition with partitions of unity?,What's the intuition with partitions of unity?,,"I've been studying Spivak's Calculus on Manifolds and I'm really not getting what's behind partitions of unity. Spivak introduces the topic with the following theorem: Let $A\subset \Bbb R^n$ and let $\mathcal{O}$ be an open cover of $A$ . Then there is a collection $\Phi$ of $C^\infty$ functions $\varphi$ defined in an open set containing $A$ , with the following properties: For each $x \in A$ we have $0 \leq \varphi(x) \leq 1$ . For each $x \in A$ there is an open set $V$ containing $x$ such that all but finitely many $\varphi \in \Phi$ are $0$ on $V$ . For each $x \in A$ we have $\sum_{\varphi \in \Phi}\varphi(x)=1$ (by 2 for each $x$ their sum is finite in some open set containing $x$ ). For each $\varphi \in \Phi$ there is an open set $U$ in $\mathcal{O}$ such that $\varphi = 0$ outside of some closed set contained in $U$ . The point is that I've heard that partitions of unity are able to transfer local results to global results, and this is of great importance, but I'm not really getting the intution behind this theorem. I mean, why a collection of functions with these four properties is able to do such job? When I see a theorem/definition, I try to really get the intution behind it: ""why we should really think about doing things this way"", because I think that this is a good way to understand what we are doing, but with partitions of unity I'm really not getting the idea. While Spivak uses this just for integration on Calculus on Manifolds for what I've seem, in his Differential Geometry books he starts to use it really more generally to get global results from local ones (obtained with charts). So, given the great importance of this topic, what's the real intuition behind this theorem and partitions of unity in general?","I've been studying Spivak's Calculus on Manifolds and I'm really not getting what's behind partitions of unity. Spivak introduces the topic with the following theorem: Let and let be an open cover of . Then there is a collection of functions defined in an open set containing , with the following properties: For each we have . For each there is an open set containing such that all but finitely many are on . For each we have (by 2 for each their sum is finite in some open set containing ). For each there is an open set in such that outside of some closed set contained in . The point is that I've heard that partitions of unity are able to transfer local results to global results, and this is of great importance, but I'm not really getting the intution behind this theorem. I mean, why a collection of functions with these four properties is able to do such job? When I see a theorem/definition, I try to really get the intution behind it: ""why we should really think about doing things this way"", because I think that this is a good way to understand what we are doing, but with partitions of unity I'm really not getting the idea. While Spivak uses this just for integration on Calculus on Manifolds for what I've seem, in his Differential Geometry books he starts to use it really more generally to get global results from local ones (obtained with charts). So, given the great importance of this topic, what's the real intuition behind this theorem and partitions of unity in general?",A\subset \Bbb R^n \mathcal{O} A \Phi C^\infty \varphi A x \in A 0 \leq \varphi(x) \leq 1 x \in A V x \varphi \in \Phi 0 V x \in A \sum_{\varphi \in \Phi}\varphi(x)=1 x x \varphi \in \Phi U \mathcal{O} \varphi = 0 U,['real-analysis']
83,Bag of tricks in Advanced Calculus/ Real Analysis/Complex Analysis,Bag of tricks in Advanced Calculus/ Real Analysis/Complex Analysis,,"I am studying for an exam and I have been studying my butt off during the winter break for it. During the course of my study I have written down quite a number of tricks, which in my opinion were 'outrageous' :-). Meaning there was no way I would come up with that during an exam if I hadn't seen that before. Couple of examples. Sometimes, when you want to prove something about $\max$, $\min$, you write ( I got this from Baby Rudin) $$ \max(a,b)=\frac{a+b+ \vert a-b \vert} {2} $$ $$ \min (a,b)= \frac{a+b-|a-b|} {2} $$ To prove Hölder's inequality (in its simplest case) You write $\int (f+tg)^2 \geq 0$ and since this stays positive you get that the discriminant of this must be negative, and magically you get your Hölder inequality. When you want to show something about distinct zeroes of complex functions you kind of eliminate the zeroes of f by dividing them with the appropriate Möbius transforms and you still get an analytic functions which has nice properties. The value of these is that they can be used in other contexts to write neat proofs. That's what I mean by ""tricks"". This might be difficult to answer, but what are some of the tricks you wise folks have up your sleeve when it comes to Advanced Calculus (Both single variable, multivariable) and complex Analysis. Anything you have to share will be greatly appreciated. Thanks so much for all your help.","I am studying for an exam and I have been studying my butt off during the winter break for it. During the course of my study I have written down quite a number of tricks, which in my opinion were 'outrageous' :-). Meaning there was no way I would come up with that during an exam if I hadn't seen that before. Couple of examples. Sometimes, when you want to prove something about $\max$, $\min$, you write ( I got this from Baby Rudin) $$ \max(a,b)=\frac{a+b+ \vert a-b \vert} {2} $$ $$ \min (a,b)= \frac{a+b-|a-b|} {2} $$ To prove Hölder's inequality (in its simplest case) You write $\int (f+tg)^2 \geq 0$ and since this stays positive you get that the discriminant of this must be negative, and magically you get your Hölder inequality. When you want to show something about distinct zeroes of complex functions you kind of eliminate the zeroes of f by dividing them with the appropriate Möbius transforms and you still get an analytic functions which has nice properties. The value of these is that they can be used in other contexts to write neat proofs. That's what I mean by ""tricks"". This might be difficult to answer, but what are some of the tricks you wise folks have up your sleeve when it comes to Advanced Calculus (Both single variable, multivariable) and complex Analysis. Anything you have to share will be greatly appreciated. Thanks so much for all your help.",,"['real-analysis', 'calculus', 'complex-analysis', 'soft-question', 'big-list']"
84,Integration of forms and integration on a measure space,Integration of forms and integration on a measure space,,"In Terence Tao's PCM article: DIFFERENTIAL FORMS AND INTEGRATION , it is pointed out that there are three concepts of integration which appear in the subject (single-variable calculus): the indefinite integral $\int f$ (also known as the anti-derivative), the unsigned definite integral $\int_{[a,b]} f(x) dx$ (which one would use to find area under a curve, or the mass of a one-dimensional object of varying density), and the signed definite integral $\int _a^b f(x) dx$ (which one would use for instance to compute the work required to move a particle from a to b). When one moves from single-variable calculus to several-variable calculus: The indefinite integral generalises to the notion of a solution to a differential equation , or of an integral of a connection, vector field, or bundle . The unsigned definite integral generalises to the Lebesgue integral , or more generally to integration on a measure space . Finally, the signed definite integral generalises to the integration of forms . While learning from this article, I tried to find the counterpart of the later two kinds of integration(as the title indicates) in the several-variable calculus I learned before. Now I am considering the following four kinds of integration: Line integral of a scalar field Line integral of a vector field Surface integrals of scalar fields Surface integrals of vector fields Here are my questions : What kinds of integration are these four ones according to the categories in the article? (I just guess generally the scalar one is the integration on a measure space and the vector one is the integration of forms.) How do they belong to the category respectively? (For example, if it is the integration on a measure space, then what exactly is the underlying measure space?)","In Terence Tao's PCM article: DIFFERENTIAL FORMS AND INTEGRATION , it is pointed out that there are three concepts of integration which appear in the subject (single-variable calculus): the indefinite integral (also known as the anti-derivative), the unsigned definite integral (which one would use to find area under a curve, or the mass of a one-dimensional object of varying density), and the signed definite integral (which one would use for instance to compute the work required to move a particle from a to b). When one moves from single-variable calculus to several-variable calculus: The indefinite integral generalises to the notion of a solution to a differential equation , or of an integral of a connection, vector field, or bundle . The unsigned definite integral generalises to the Lebesgue integral , or more generally to integration on a measure space . Finally, the signed definite integral generalises to the integration of forms . While learning from this article, I tried to find the counterpart of the later two kinds of integration(as the title indicates) in the several-variable calculus I learned before. Now I am considering the following four kinds of integration: Line integral of a scalar field Line integral of a vector field Surface integrals of scalar fields Surface integrals of vector fields Here are my questions : What kinds of integration are these four ones according to the categories in the article? (I just guess generally the scalar one is the integration on a measure space and the vector one is the integration of forms.) How do they belong to the category respectively? (For example, if it is the integration on a measure space, then what exactly is the underlying measure space?)","\int f \int_{[a,b]} f(x) dx \int _a^b f(x) dx","['real-analysis', 'integration']"
85,Cardinality of Borel sigma algebra,Cardinality of Borel sigma algebra,,"It seems it's well known that if a sigma algebra is generated by countably many sets, then the cardinality of it is either finite or $c$ (the cardinality of continuum). But it seems hard to prove it, and actually hard to find a proof of it. Can anyone help me out?","It seems it's well known that if a sigma algebra is generated by countably many sets, then the cardinality of it is either finite or $c$ (the cardinality of continuum). But it seems hard to prove it, and actually hard to find a proof of it. Can anyone help me out?",,"['real-analysis', 'measure-theory', 'probability-theory', 'set-theory', 'descriptive-set-theory']"
86,"Is it possible for a function to be smooth everywhere, analytic nowhere, yet Taylor series at any point converges in a nonzero radius?","Is it possible for a function to be smooth everywhere, analytic nowhere, yet Taylor series at any point converges in a nonzero radius?",,"It is well-known that the function $$f(x) = \begin{cases} e^{-1/x^2}, \mbox{if } x \ne 0 \\ 0, \mbox{if } x = 0\end{cases}$$ is smooth everywhere, yet not analytic at $x = 0$. In particular, its Taylor series exists there, but it equals $0 + 0x + 0x^2 + 0x^3 + ... = 0$, so while it has radius of convergence $\infty$, it is not equal to $f$ even in a tiny neighborhood of $0$. There is also a function $$f(x) = \sum_{n=0}^{\infty} e^{-\sqrt{2^n}} \cos(2^n x)$$ which is smooth everywhere (that is, $C^{\infty}$) yet analytic nowhere . In particular, the Taylor series at every point has radius of convergence $0$. In fact, ""most"" smooth functions are not analytic. But this gets me wondering. Could there exist some function which is smooth everywhere, analytic nowhere, yet its Taylor series at any point has nonzero radius of convergence, and so converges to something , but that something is not the function, not even in a tiny neighborhood about the point of expansion? If yes, what is an example of such a function? If no, what is the proof that such a thing is impossible? And also, if no, what sort of restrictions exist on the convergence of the T.s.? At how many/what distribution of points can it converge to something which is not the function? I note that if we multiply together the two functions just given above, we have another smooth-everywhere, analytic-nowhere function, but this time at $0$ we have a convergent Taylor series (the same zero series as before -- just use the generalized Leibniz rule) which doesn't converge to the function in even a tiny neighborhood of $0$. EDIT (Dec 31, 2013): With some Googling I came across a post to mathoverflow: https://mathoverflow.net/a/81465 The Taylor series of the Fabius function at any dyadic rational actually has infinite radius of convergence (only finitely many terms are nonzero) but does not represent the function on any interval. So it seems it is possible to have a function whose Taylor series converges to ""the wrong thing"" at a dense set of expansion points. But it still doesn't answer the question of whether that is possible for all expansion points on the entire real line.","It is well-known that the function $$f(x) = \begin{cases} e^{-1/x^2}, \mbox{if } x \ne 0 \\ 0, \mbox{if } x = 0\end{cases}$$ is smooth everywhere, yet not analytic at $x = 0$. In particular, its Taylor series exists there, but it equals $0 + 0x + 0x^2 + 0x^3 + ... = 0$, so while it has radius of convergence $\infty$, it is not equal to $f$ even in a tiny neighborhood of $0$. There is also a function $$f(x) = \sum_{n=0}^{\infty} e^{-\sqrt{2^n}} \cos(2^n x)$$ which is smooth everywhere (that is, $C^{\infty}$) yet analytic nowhere . In particular, the Taylor series at every point has radius of convergence $0$. In fact, ""most"" smooth functions are not analytic. But this gets me wondering. Could there exist some function which is smooth everywhere, analytic nowhere, yet its Taylor series at any point has nonzero radius of convergence, and so converges to something , but that something is not the function, not even in a tiny neighborhood about the point of expansion? If yes, what is an example of such a function? If no, what is the proof that such a thing is impossible? And also, if no, what sort of restrictions exist on the convergence of the T.s.? At how many/what distribution of points can it converge to something which is not the function? I note that if we multiply together the two functions just given above, we have another smooth-everywhere, analytic-nowhere function, but this time at $0$ we have a convergent Taylor series (the same zero series as before -- just use the generalized Leibniz rule) which doesn't converge to the function in even a tiny neighborhood of $0$. EDIT (Dec 31, 2013): With some Googling I came across a post to mathoverflow: https://mathoverflow.net/a/81465 The Taylor series of the Fabius function at any dyadic rational actually has infinite radius of convergence (only finitely many terms are nonzero) but does not represent the function on any interval. So it seems it is possible to have a function whose Taylor series converges to ""the wrong thing"" at a dense set of expansion points. But it still doesn't answer the question of whether that is possible for all expansion points on the entire real line.",,"['calculus', 'real-analysis', 'power-series', 'analyticity']"
87,Why can't the second fundamental theorem of calculus be proved in just two lines?,Why can't the second fundamental theorem of calculus be proved in just two lines?,,"The second fundamental theorem of calculus states that if $f$ is continuous on $[a,b]$ and $F$ is an antiderivative of $f$ on the same interval, then: $$\int_a^b f(x) dx= F(b)-F(a).$$ The proof of this theorem in both my textbook and Wikipedia is pretty complex and long. It uses the mean value theorem of integration and the limit of an infinite Riemann summation. But I tried coming up with a proof and it was barely two lines. Here it goes: Since $F$ is an antiderivative of $f$ , we have $\frac{dF}{dx} = f(x)$ . Multiplying both sides by $dx$ ,  we obtain $dF = f(x)dx$ . Now, $dF$ is just the small change in $F$ and $f(x)dx$ represents the infinitesimal area bounded by the curve and the $x$ axis. So integrating both sides, we arrive at the required result. First, what is wrong with my proof? And if it is so simple, what is so fundamental about it? Multiplying the equation by $dx$ should be an obvious step to find infinitesimal area, right? Why is the Wikipedia (and textbook) proof so long? I have also read that the connection between differential and integral calculus is not obvious, making the fundamental theorem a surprising result. But to me, it seems trivial. So, what were the wrong assumptions I made in the proof and what am I taking for granted? It should be noted that I have already learnt differential and integral calculus and I am being taught the ""fundamental theorem"" in the end and not as the first link between the two realms of calculus. In response to the answers below: If expressing infinitesimals on their own is not ""rigorous"" enough to be used in a proof, then what more sense do they make when written along with an integral sign, or even in the notation for the derivative? The integral is just the continuous sum of infinitesimals, correct? And the derivative is just the quotient of two. How else should these be defined or intuitively explained? It seems to me that one needs to learn an entirely new part of mathematics before diving into differential or integral calculus. Plus we do this sort of thing in physics all the time.","The second fundamental theorem of calculus states that if is continuous on and is an antiderivative of on the same interval, then: The proof of this theorem in both my textbook and Wikipedia is pretty complex and long. It uses the mean value theorem of integration and the limit of an infinite Riemann summation. But I tried coming up with a proof and it was barely two lines. Here it goes: Since is an antiderivative of , we have . Multiplying both sides by ,  we obtain . Now, is just the small change in and represents the infinitesimal area bounded by the curve and the axis. So integrating both sides, we arrive at the required result. First, what is wrong with my proof? And if it is so simple, what is so fundamental about it? Multiplying the equation by should be an obvious step to find infinitesimal area, right? Why is the Wikipedia (and textbook) proof so long? I have also read that the connection between differential and integral calculus is not obvious, making the fundamental theorem a surprising result. But to me, it seems trivial. So, what were the wrong assumptions I made in the proof and what am I taking for granted? It should be noted that I have already learnt differential and integral calculus and I am being taught the ""fundamental theorem"" in the end and not as the first link between the two realms of calculus. In response to the answers below: If expressing infinitesimals on their own is not ""rigorous"" enough to be used in a proof, then what more sense do they make when written along with an integral sign, or even in the notation for the derivative? The integral is just the continuous sum of infinitesimals, correct? And the derivative is just the quotient of two. How else should these be defined or intuitively explained? It seems to me that one needs to learn an entirely new part of mathematics before diving into differential or integral calculus. Plus we do this sort of thing in physics all the time.","f [a,b] F f \int_a^b f(x) dx= F(b)-F(a). F f \frac{dF}{dx} = f(x) dx dF = f(x)dx dF F f(x)dx x dx","['calculus', 'real-analysis', 'infinitesimals']"
88,How to stop forgetting proofs - for a first course in Real Analysis?,How to stop forgetting proofs - for a first course in Real Analysis?,,"I am taking my first course in analysis. I like the subject. I study it almost on a daily basis. I try to prove theorems on my own without even looking at the hints. If I really get stuck I just read the first line of the proof and then try to continue on my own. I find this approach to be very rewarding. The problem is that I tend to forget all the proofs later. Some I still remember because I liked the idea of the proof, but most of the others I will forget fast. What am I doing wrong? Any tips on how to study analysis?","I am taking my first course in analysis. I like the subject. I study it almost on a daily basis. I try to prove theorems on my own without even looking at the hints. If I really get stuck I just read the first line of the proof and then try to continue on my own. I find this approach to be very rewarding. The problem is that I tend to forget all the proofs later. Some I still remember because I liked the idea of the proof, but most of the others I will forget fast. What am I doing wrong? Any tips on how to study analysis?",,"['real-analysis', 'soft-question', 'self-learning', 'learning']"
89,Are there any valid continuous Sudoku grids?,Are there any valid continuous Sudoku grids?,,"A standard Sudoku is a $9\times 9$ grid filled with digits such that every row, column, and $3\times 3$ box contains all the integers from $1$ to $9$ . I am thinking about a generalization of Sudoku which I call ""continuous Sudoku"", which consists of a unit square where every point on that square corresponds to a real number. The rules for continuous Sudoku are designed to be analogous to the rules for standard Sudoku, and I've devised two different rulesets: The first ruleset I call ""weak"" continuous Sudoku. In weak continuous Sudoku, the only restriction is that every row and column of the square contains every real number in the interval $[0,1]$ exactly once. The second ruleset I call ""strong"" continuous Sudoku. In strong continuous Sudoku, the rules of weak continuous Sudoku apply, and, in addition, every square sub-region of the unit square contains every real number in the interval $[0,1]$ at least once. This is analogous to the $3\times 3$ box restriction in standard Sudoku. Let $U = [0,1]$ and $U^2 = U\times U$ . More precisely, a weak continuous Sudoku is essentially a function $f:U^2\to U$ , which satisfies the following four properties: If $x,y_1,y_2\in U$ and $y_1\neq y_2$ , then $f(x,y_1)\neq f(x,y_2)$ . If $x_1,x_2,y\in U$ and $x_1\neq x_2$ , then $f(x_1,y)\neq f(x_2,y)$ . If $x\in U$ then $\{z: f(x,y)=z,y\in U\} = U$ . If $y\in U$ then $\{z: f(x,y)=z,x\in U\} = U$ . Now, strong continuous Sudoku is a bit harder to define precisely. A set $S$ is a square sub-region of $U^2$ iff $S\subseteq U^2$ and there exists $z = (z_1,z_2)\in U^2$ and $r>0$ such that $S = \{(x,y)\in U^2:z_1\leq x\leq z_1+r,z_2\leq y\leq z_2+r\}$ . Thus, using this definition, a strong continuous Sudoku is a weak continuous Sudoku which satisfies the following additional property: If $S$ is a square sub-region of $U^2$ , then $f(S) = U$ . I've been trying to look for specific examples of both strong and weak continuous Sudoku grids, but have so far been unsucessful. I'm not sure whether any weak continuous Sudoku exists. My first attempt: $$ f(x,y)=\begin{cases} x+y &\text{if }x+y\leq 1 \\ x+y-1 & \text{if }x+y>1\end{cases} $$ almost works. It satisfies properties $3$ and $4$ , and almost, but not quite, satisfies $1$ and $2$ . The issue occurs only at boundaries of the square, for example, $f(0.5,0) = 0.5$ and $f(0.5,1)=0.5$ . Any example of a strong continuous Sudoku will likely need to be an extremely discontinuous pathological function, similar to the Conway base 13 function . Obviously, if there are no weak continuous Sudoku grids, then there are no strong continuous Sudoku grids. Even if there are no weak Sudoku grids, it may be possible to slightly modify the definitions to permit small exceptions such as in the above example. The main question I'm asking is: Do any weak continuous Sudoku grids exist, and if they do, do any strong continuous Sudoku grids exist?","A standard Sudoku is a grid filled with digits such that every row, column, and box contains all the integers from to . I am thinking about a generalization of Sudoku which I call ""continuous Sudoku"", which consists of a unit square where every point on that square corresponds to a real number. The rules for continuous Sudoku are designed to be analogous to the rules for standard Sudoku, and I've devised two different rulesets: The first ruleset I call ""weak"" continuous Sudoku. In weak continuous Sudoku, the only restriction is that every row and column of the square contains every real number in the interval exactly once. The second ruleset I call ""strong"" continuous Sudoku. In strong continuous Sudoku, the rules of weak continuous Sudoku apply, and, in addition, every square sub-region of the unit square contains every real number in the interval at least once. This is analogous to the box restriction in standard Sudoku. Let and . More precisely, a weak continuous Sudoku is essentially a function , which satisfies the following four properties: If and , then . If and , then . If then . If then . Now, strong continuous Sudoku is a bit harder to define precisely. A set is a square sub-region of iff and there exists and such that . Thus, using this definition, a strong continuous Sudoku is a weak continuous Sudoku which satisfies the following additional property: If is a square sub-region of , then . I've been trying to look for specific examples of both strong and weak continuous Sudoku grids, but have so far been unsucessful. I'm not sure whether any weak continuous Sudoku exists. My first attempt: almost works. It satisfies properties and , and almost, but not quite, satisfies and . The issue occurs only at boundaries of the square, for example, and . Any example of a strong continuous Sudoku will likely need to be an extremely discontinuous pathological function, similar to the Conway base 13 function . Obviously, if there are no weak continuous Sudoku grids, then there are no strong continuous Sudoku grids. Even if there are no weak Sudoku grids, it may be possible to slightly modify the definitions to permit small exceptions such as in the above example. The main question I'm asking is: Do any weak continuous Sudoku grids exist, and if they do, do any strong continuous Sudoku grids exist?","9\times 9 3\times 3 1 9 [0,1] [0,1] 3\times 3 U = [0,1] U^2 = U\times U f:U^2\to U x,y_1,y_2\in U y_1\neq y_2 f(x,y_1)\neq f(x,y_2) x_1,x_2,y\in U x_1\neq x_2 f(x_1,y)\neq f(x_2,y) x\in U \{z: f(x,y)=z,y\in U\} = U y\in U \{z: f(x,y)=z,x\in U\} = U S U^2 S\subseteq U^2 z = (z_1,z_2)\in U^2 r>0 S = \{(x,y)\in U^2:z_1\leq x\leq z_1+r,z_2\leq y\leq z_2+r\} S U^2 f(S) = U 
f(x,y)=\begin{cases} x+y &\text{if }x+y\leq 1 \\
x+y-1 & \text{if }x+y>1\end{cases}
 3 4 1 2 f(0.5,0) = 0.5 f(0.5,1)=0.5","['real-analysis', 'sudoku']"
90,"Why can a real number be defined as a Dedekind cut, that is, as a set of rational numbers?","Why can a real number be defined as a Dedekind cut, that is, as a set of rational numbers?",,"I don't know if my textbook is written poorly or I'm dumb. But I can't bring myself to understand the following definition. A real number is a cut , which parts the rational numbers into two classes. Let $\mathbb{R}$ be the set of cuts. A cut is a set of rational numbers $A \subset \mathbb{Q}$ with the following properties: i) $A \neq \emptyset$ and $A \neq \mathbb{Q}$ . ii) if $p \in A$ and $q < p$ then $q \in A$ . iii) if $p \in A$ , there exists some $r \in A$ so that $p < r$ (i.e. $A$ doesn't contain the ""biggest"" number). That's a literal translation from my textbook (which is written in Slovenian). All seems fine and I can get my head around all of the postulations except for one. The definition states in the beginning ""A real number is a cut..."", but then it also states ""A cut is a set of rational numbers..."" So a real number is 'a set of rational numbers'?! It's not my bad translation, I swear, I'm quite good at English. Either the textbook is written in such a convoluted manner that I can't properly understand the wording the author chose or I'm overlooking something big . Could you please clarify and explain the definition in full detail?","I don't know if my textbook is written poorly or I'm dumb. But I can't bring myself to understand the following definition. A real number is a cut , which parts the rational numbers into two classes. Let be the set of cuts. A cut is a set of rational numbers with the following properties: i) and . ii) if and then . iii) if , there exists some so that (i.e. doesn't contain the ""biggest"" number). That's a literal translation from my textbook (which is written in Slovenian). All seems fine and I can get my head around all of the postulations except for one. The definition states in the beginning ""A real number is a cut..."", but then it also states ""A cut is a set of rational numbers..."" So a real number is 'a set of rational numbers'?! It's not my bad translation, I swear, I'm quite good at English. Either the textbook is written in such a convoluted manner that I can't properly understand the wording the author chose or I'm overlooking something big . Could you please clarify and explain the definition in full detail?",\mathbb{R} A \subset \mathbb{Q} A \neq \emptyset A \neq \mathbb{Q} p \in A q < p q \in A p \in A r \in A p < r A,"['real-analysis', 'real-numbers']"
91,Why do engineers use derivatives in discontinuous functions? Is it correct?,Why do engineers use derivatives in discontinuous functions? Is it correct?,,"I am a Software Engineering student and this year I learned about how CPUs work, it turns out that electronic engineers and I also see it a lot in my field, we do use derivatives with discontinuous functions. For instance in order to calculate the optimal amount of ripple adders so as to minimise the execution time of the addition process: $$\text{ExecutionTime}(n, k) = \Delta(4k+\frac{2n}{k}-4)$$ $$\frac{d\,\text{ExecutionTime}(n, k)}{dk}=4\Delta-\frac{2n\Delta}{k^2}=0$$ $$k= \sqrt{\frac{n}{2}}$$ where $n$ is the number of bits in the numbers to add, $k$ is the amount of adders in ripple and $\Delta$ is the ""delta gate"" (the time that takes to a gate to operate). Clearly you can see that the execution time function is not continuous at all because $k$ is a natural number and so is $n$ . This is driving me crazy because on the one hand I understand that I can analyse the function as a continuous one and get results in that way, and indeed I think that's what we do (""I think"", that's why I am asking), but my intuition and knowledge about mathematical analysis tells me that this is completely wrong, because the truth is that the function is not continuous and will never be and because of that, the derivative with respect to $k$ or $n$ does not exist because there is no rate of change. If someone could explain me if my first guess is correct or not and why, I'd appreciate it a lot, thanks for reading and helping!","I am a Software Engineering student and this year I learned about how CPUs work, it turns out that electronic engineers and I also see it a lot in my field, we do use derivatives with discontinuous functions. For instance in order to calculate the optimal amount of ripple adders so as to minimise the execution time of the addition process: where is the number of bits in the numbers to add, is the amount of adders in ripple and is the ""delta gate"" (the time that takes to a gate to operate). Clearly you can see that the execution time function is not continuous at all because is a natural number and so is . This is driving me crazy because on the one hand I understand that I can analyse the function as a continuous one and get results in that way, and indeed I think that's what we do (""I think"", that's why I am asking), but my intuition and knowledge about mathematical analysis tells me that this is completely wrong, because the truth is that the function is not continuous and will never be and because of that, the derivative with respect to or does not exist because there is no rate of change. If someone could explain me if my first guess is correct or not and why, I'd appreciate it a lot, thanks for reading and helping!","\text{ExecutionTime}(n, k) = \Delta(4k+\frac{2n}{k}-4) \frac{d\,\text{ExecutionTime}(n, k)}{dk}=4\Delta-\frac{2n\Delta}{k^2}=0 k= \sqrt{\frac{n}{2}} n k \Delta k n k n","['real-analysis', 'calculus', 'functions', 'derivatives', 'optimization']"
92,What is the limit of $n \sin (2 \pi \cdot e \cdot n!)$ as $n$ goes to infinity?,What is the limit of  as  goes to infinity?,n \sin (2 \pi \cdot e \cdot n!) n,I tried and got this $$e=\sum_{k=0}^\infty\frac{1}{k!}=\lim_{n\to\infty}\sum_{k=0}^n\frac{1}{k!}$$ $$n!\sum_{k=0}^n\frac{1}{k!}=\frac{n!}{0!}+\frac{n!}{1!}+\cdots+\frac{n!}{n!}=m$$ where $m$ is an integer. $$\lim_{n\to\infty}n\sin(2\pi en!)=\lim_{n\to\infty}n\sin\left(2\pi n!\sum_{k=0}^n\frac{1}{k!}\right)=\lim_{n\to\infty}n\sin(2\pi m)=\lim_{n\to\infty}n\cdot0=0$$ Is it correct?,I tried and got this $$e=\sum_{k=0}^\infty\frac{1}{k!}=\lim_{n\to\infty}\sum_{k=0}^n\frac{1}{k!}$$ $$n!\sum_{k=0}^n\frac{1}{k!}=\frac{n!}{0!}+\frac{n!}{1!}+\cdots+\frac{n!}{n!}=m$$ where $m$ is an integer. $$\lim_{n\to\infty}n\sin(2\pi en!)=\lim_{n\to\infty}n\sin\left(2\pi n!\sum_{k=0}^n\frac{1}{k!}\right)=\lim_{n\to\infty}n\sin(2\pi m)=\lim_{n\to\infty}n\cdot0=0$$ Is it correct?,,"['real-analysis', 'analysis', 'limits', 'trigonometry', 'convergence-divergence']"
93,Under what condition we can interchange order of a limit and a summation?,Under what condition we can interchange order of a limit and a summation?,,"Suppose f(m,n) is a double sequence in $\mathbb R$. Under what condition do we have $\lim\limits_{n\to\infty}\sum\limits_{m=1}^\infty f(m,n)=\sum\limits_{m=1}^\infty \lim\limits_{n\to\infty} f(m,n)$? Thanks!","Suppose f(m,n) is a double sequence in $\mathbb R$. Under what condition do we have $\lim\limits_{n\to\infty}\sum\limits_{m=1}^\infty f(m,n)=\sum\limits_{m=1}^\infty \lim\limits_{n\to\infty} f(m,n)$? Thanks!",,"['calculus', 'real-analysis', 'analysis', 'measure-theory']"
94,"Prove: If a sequence converges, then every subsequence converges to the same limit.","Prove: If a sequence converges, then every subsequence converges to the same limit.",,"I need some help understanding this proof: Prove: If a sequence converges, then every subsequence converges to the same limit. Proof: Let $s_{n_k}$ denote a subsequence of $s_n$. Note that $n_k \geq k$ for all $k$. This easy to prove by induction: in fact, $n_1 \geq 1$ and $n_k \geq k$ implies $n_{k+1} > n_k \geq k$ and hence $n_{k+1} \geq k+1$. Let $\lim s_n = s$ and let $\epsilon > 0$. There exists $N$ so that $n>N$ implies $|s_n - s| < \epsilon$. Now $k > N \implies n_k > N \implies |s_{n_k} - s| < \epsilon$. Therefore: $\lim_{k \to \infty} s_{n_k} = s$. What is the intuition that each subsequence will converge to the same limit I do not understand the induction that claims $n_k \geq k$","I need some help understanding this proof: Prove: If a sequence converges, then every subsequence converges to the same limit. Proof: Let $s_{n_k}$ denote a subsequence of $s_n$. Note that $n_k \geq k$ for all $k$. This easy to prove by induction: in fact, $n_1 \geq 1$ and $n_k \geq k$ implies $n_{k+1} > n_k \geq k$ and hence $n_{k+1} \geq k+1$. Let $\lim s_n = s$ and let $\epsilon > 0$. There exists $N$ so that $n>N$ implies $|s_n - s| < \epsilon$. Now $k > N \implies n_k > N \implies |s_{n_k} - s| < \epsilon$. Therefore: $\lim_{k \to \infty} s_{n_k} = s$. What is the intuition that each subsequence will converge to the same limit I do not understand the induction that claims $n_k \geq k$",,"['real-analysis', 'limits', 'proof-writing']"
95,"""Gaps"" or ""holes"" in rational number system","""Gaps"" or ""holes"" in rational number system",,"In Rudin's Principles of Mathematical Analysis 1.1, he first shows that there is no rational number $p$ with $p^2=2$ . Then he creates two sets: $A$ is the set of all positive rationals $p$ such that $p^2<2$ , and $B$ consists of all positive rationals $p$ such that $p^2>2$ . He shows that $A$ contains no largest number and $B$ contains no smallest. And then in 1.2, Rudin remarks that what he has done above is to show that the rational number system has certain gaps. His remarks confused me. My questions are: If he had shown that no rational number $p$ with $p^2=2$ , this already gave the conclusion that rational number system has ""gaps"" or ""holes"". Why did he need to set up the second argument about the two sets $A$ and $B$ ? How does the second argument that "" $A$ contains no largest number and $B$ contains no smallest"" showed gaps in rational number system? My intuition does not work here. Or it is nothing to do with intuition?","In Rudin's Principles of Mathematical Analysis 1.1, he first shows that there is no rational number with . Then he creates two sets: is the set of all positive rationals such that , and consists of all positive rationals such that . He shows that contains no largest number and contains no smallest. And then in 1.2, Rudin remarks that what he has done above is to show that the rational number system has certain gaps. His remarks confused me. My questions are: If he had shown that no rational number with , this already gave the conclusion that rational number system has ""gaps"" or ""holes"". Why did he need to set up the second argument about the two sets and ? How does the second argument that "" contains no largest number and contains no smallest"" showed gaps in rational number system? My intuition does not work here. Or it is nothing to do with intuition?",p p^2=2 A p p^2<2 B p p^2>2 A B p p^2=2 A B A B,"['real-analysis', 'analysis', 'rational-numbers']"
96,Integration with respect to counting measure.,Integration with respect to counting measure.,,"I am having trouble computing integration w.r.t. counting measure. Let $(\mathbb{N},\scr{P}(\mathbb{N}),\mu)$ be a measure space where $\mu$ is counting measure. Let $f:\mathbb{N}\rightarrow{\mathbb{R}}$ be a non-negative bounded measurable function. Then, what is $\int_{\mathbb{N}}fd\mu$? What's gonna happen if we remove the boundedness in $f$, i.e. just let $f$ be an arbitrary non-negative measurable function? What happens if we relace $\mathbb{N}$ by a general set $X$?","I am having trouble computing integration w.r.t. counting measure. Let $(\mathbb{N},\scr{P}(\mathbb{N}),\mu)$ be a measure space where $\mu$ is counting measure. Let $f:\mathbb{N}\rightarrow{\mathbb{R}}$ be a non-negative bounded measurable function. Then, what is $\int_{\mathbb{N}}fd\mu$? What's gonna happen if we remove the boundedness in $f$, i.e. just let $f$ be an arbitrary non-negative measurable function? What happens if we relace $\mathbb{N}$ by a general set $X$?",,['real-analysis']
97,"Let, $A\subset\mathbb{R}^2$. Show that $A$ can contain at most one point $p$ such that $A$ is isometric to $A \setminus \{p\}$.","Let, . Show that  can contain at most one point  such that  is isometric to .",A\subset\mathbb{R}^2 A p A A \setminus \{p\},"A challenge problem from Sally's Fundamentals of Mathematical Analysis. Problem reads: Suppose $A$ is a subset of $\mathbb{R}^2$. Show that $A$ can contain at most one point $p$ such that $A$ is isometric to $A \setminus \{p\}$ with the usual metric. I'm really not sure where to begin. I've found a fairly trivial example of a set for which this is true: let $A$, for example, be $\{(n,0) : n \in \{0\} \cup \mathbb{Z}^+\}$. Then we may remove the point $(0,0)$ and construct the isometry $f(n,0) = (n+1, 0)$. This is clearly an isometry because $d((n,0),(m,0)) = d((n+1,0),(m+1,0))$, in other words, we are just shifting to the right. But now suppose we remove some $(p,0) \neq (0,0)$. Then we must have $d((m,0), (m+1,0)) = 1$ for all points $(m,0), (m+1,0)$, but since $(p,0)$ was removed we will always have a ""jump"" point where the distance between two successive points is $2$. But I'm not sure where to proceed. Isometries are equivalence relations, so maybe we can show that if $A \setminus \{p\}$ is isometric to $A \setminus \{q\}$, then $p = q$? I will say that given how often Sally's errant in his book and that some of the other challenge problems are open problems, this might not have a reasonable solution (if it's even true). Any ideas? To avoid any confusion, the problem isn't asking a proof for the set not being isometric to itself minus two points at the same time. It's asking for a proof that there is at most one unique point that you can remove from the set and then create an isometry. This was something I misinterpreted for a while. Edit: This is still stumping me. I'm beginning to wonder whether it's even true at all. Well, I've put a bounty on it, which hopefully serves as bit more incentive to try this problem out!","A challenge problem from Sally's Fundamentals of Mathematical Analysis. Problem reads: Suppose $A$ is a subset of $\mathbb{R}^2$. Show that $A$ can contain at most one point $p$ such that $A$ is isometric to $A \setminus \{p\}$ with the usual metric. I'm really not sure where to begin. I've found a fairly trivial example of a set for which this is true: let $A$, for example, be $\{(n,0) : n \in \{0\} \cup \mathbb{Z}^+\}$. Then we may remove the point $(0,0)$ and construct the isometry $f(n,0) = (n+1, 0)$. This is clearly an isometry because $d((n,0),(m,0)) = d((n+1,0),(m+1,0))$, in other words, we are just shifting to the right. But now suppose we remove some $(p,0) \neq (0,0)$. Then we must have $d((m,0), (m+1,0)) = 1$ for all points $(m,0), (m+1,0)$, but since $(p,0)$ was removed we will always have a ""jump"" point where the distance between two successive points is $2$. But I'm not sure where to proceed. Isometries are equivalence relations, so maybe we can show that if $A \setminus \{p\}$ is isometric to $A \setminus \{q\}$, then $p = q$? I will say that given how often Sally's errant in his book and that some of the other challenge problems are open problems, this might not have a reasonable solution (if it's even true). Any ideas? To avoid any confusion, the problem isn't asking a proof for the set not being isometric to itself minus two points at the same time. It's asking for a proof that there is at most one unique point that you can remove from the set and then create an isometry. This was something I misinterpreted for a while. Edit: This is still stumping me. I'm beginning to wonder whether it's even true at all. Well, I've put a bounty on it, which hopefully serves as bit more incentive to try this problem out!",,"['real-analysis', 'general-topology', 'analysis', 'contest-math', 'isometry']"
98,"Can there be two distinct, continuous functions that are equal at all rationals?","Can there be two distinct, continuous functions that are equal at all rationals?",,"Akhil showed that the Cardinality of set of real continuous functions is the same as the continuum, using as a step the observation that continuous functions that agree at rational points must agree everywhere, since the rationals are dense in the reals. This isn't an obvious step, so why is it true?","Akhil showed that the Cardinality of set of real continuous functions is the same as the continuum, using as a step the observation that continuous functions that agree at rational points must agree everywhere, since the rationals are dense in the reals. This isn't an obvious step, so why is it true?",,"['calculus', 'real-analysis']"
99,Discontinuous linear functional,Discontinuous linear functional,,"I'm trying to find a discontinuous linear functional into $\mathbb{R}$ as a prep question for a test. I know that I need an infinite-dimensional Vector Space. Since $\ell_2$ is infinite-dimensional, there must exist a linear functional from $\ell_2$ into $\mathbb{R}$. However, I'm having trouble actually coming up with it. I believe I'm supposed to find an unbounded function (although I'm not sure why an unbounded function is necessarily not continuous; some light in that regard would be appreciated too), so I thought of using the vectors $e^i$, which have all entries equal to zero, except for the $i$-th one. Then, you can define $f(e^i)=i$.  That'd be unbounded, but I'm not sure if it'd be linear, and even if it is, I'm not sure how to define it for all the other vectors in $\ell_2$. A friend mentioned that at some point the question of whether the set $E=\{e^i:i\in\mathbb{Z}^+\}$ is a basis would come up, but I'm not sure what a basis has to do with continuity of $f$. I'm just learning this topic for the first time, so bear with me please. The space of sequences that are eventually zero (suggested by a few people) turned out to be exactly what I needed.  It also helped to cement the notions of Hamel basis, not continuous, etc.","I'm trying to find a discontinuous linear functional into $\mathbb{R}$ as a prep question for a test. I know that I need an infinite-dimensional Vector Space. Since $\ell_2$ is infinite-dimensional, there must exist a linear functional from $\ell_2$ into $\mathbb{R}$. However, I'm having trouble actually coming up with it. I believe I'm supposed to find an unbounded function (although I'm not sure why an unbounded function is necessarily not continuous; some light in that regard would be appreciated too), so I thought of using the vectors $e^i$, which have all entries equal to zero, except for the $i$-th one. Then, you can define $f(e^i)=i$.  That'd be unbounded, but I'm not sure if it'd be linear, and even if it is, I'm not sure how to define it for all the other vectors in $\ell_2$. A friend mentioned that at some point the question of whether the set $E=\{e^i:i\in\mathbb{Z}^+\}$ is a basis would come up, but I'm not sure what a basis has to do with continuity of $f$. I'm just learning this topic for the first time, so bear with me please. The space of sequences that are eventually zero (suggested by a few people) turned out to be exactly what I needed.  It also helped to cement the notions of Hamel basis, not continuous, etc.",,"['real-analysis', 'functional-analysis', 'examples-counterexamples']"
