,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,If $\gamma_n$ are roots of $\tan x = x$ can every function be expanded in form of $\sum_n a_n \sin(\gamma_n x)$?,If  are roots of  can every function be expanded in form of ?,\gamma_n \tan x = x \sum_n a_n \sin(\gamma_n x),"Solving a linear PDE, I got the general solution $$f(x,t)=\frac{e^{-t/\tau}}x\sum^\infty_{n=1}a_n\sin(\gamma_n x)$$ where $\gamma_n$ is the nth positive root of $\tan x=x$ . To satisfy the initial condition we require $$\phi(x)=\frac1x\sum^\infty_{n=1}a_n\sin(\gamma_n x)$$ From a physical point of view, $\phi(x)=f(x,0)$ represents the initial temperature profile of the object, which can be arbitrarily defined. Hence, I assumed such a expansion form is possible for most sufficiently well-behaving $\phi(x)$ , but I cannot prove it. What are the necessary and sufficient conditions for expanding a function $\phi(x)$ into $\frac1x\sum^\infty_{n=1}a_n\sin(\gamma_n x)$ ? Noteworthily, $\gamma_n\sim\frac{\pi}{2}+n\pi$ thus this series is ‘almost’ a Fourier sine series. When this expansion turns out to be mathematically possible, how could one extract the $a_n$ s? One possible way is performing fourier transform on both sides, so that $$\int^\infty_{-\infty}x\phi(x)e^{-i\omega x}dx = \sum^\infty_{n=1}a_n\delta(\omega-\gamma_n)$$ assuming $\omega>0$ , but often does one have knowledge of $\phi(x)$ only on a physically meaningful region $[0,a]$ .","Solving a linear PDE, I got the general solution where is the nth positive root of . To satisfy the initial condition we require From a physical point of view, represents the initial temperature profile of the object, which can be arbitrarily defined. Hence, I assumed such a expansion form is possible for most sufficiently well-behaving , but I cannot prove it. What are the necessary and sufficient conditions for expanding a function into ? Noteworthily, thus this series is ‘almost’ a Fourier sine series. When this expansion turns out to be mathematically possible, how could one extract the s? One possible way is performing fourier transform on both sides, so that assuming , but often does one have knowledge of only on a physically meaningful region .","f(x,t)=\frac{e^{-t/\tau}}x\sum^\infty_{n=1}a_n\sin(\gamma_n x) \gamma_n \tan x=x \phi(x)=\frac1x\sum^\infty_{n=1}a_n\sin(\gamma_n x) \phi(x)=f(x,0) \phi(x) \phi(x) \frac1x\sum^\infty_{n=1}a_n\sin(\gamma_n x) \gamma_n\sim\frac{\pi}{2}+n\pi a_n \int^\infty_{-\infty}x\phi(x)e^{-i\omega x}dx = \sum^\infty_{n=1}a_n\delta(\omega-\gamma_n) \omega>0 \phi(x) [0,a]",['real-analysis']
1,Prove that $ y = x^{\frac{1}{n}} \Rightarrow y^n = x $,Prove that, y = x^{\frac{1}{n}} \Rightarrow y^n = x ,"Tao, Analysis I, exercise 5.6.1 I have to prove the following where $x,y \in \Bbb R^+, \ n \in \Bbb Z^+$ $$ y = x^{\frac{1}{n}} \Rightarrow  y^n = x $$ Hints: review the proof of Proposition   5.5.12. Also, you will find proof by contradiction a useful tool, especially when   combined with the trichotomy of order in Proposition 5.4.7 and Proposition   5.4.12. My attempt: Assume that $ y = x^{\frac{1}{n}}  \Rightarrow  y^n > x $ . By definition: $$x^{\frac 1n}= \sup \{ y \in \mathbb R, s.t.  y \geq 0, y^n \leq x \}$$ Thus $ y^n > x $ contradicts the above definition, since $x$ is upper bound of $y^n$ . Does this part seem ok? I next need to get to a contradiction starting from here: Now assume that $ y = x^{\frac{1}{n}}  \Rightarrow  y^n < x $","Tao, Analysis I, exercise 5.6.1 I have to prove the following where Hints: review the proof of Proposition   5.5.12. Also, you will find proof by contradiction a useful tool, especially when   combined with the trichotomy of order in Proposition 5.4.7 and Proposition   5.4.12. My attempt: Assume that . By definition: Thus contradicts the above definition, since is upper bound of . Does this part seem ok? I next need to get to a contradiction starting from here: Now assume that","x,y \in \Bbb R^+, \ n \in \Bbb Z^+  y = x^{\frac{1}{n}} \Rightarrow  y^n = x   y = x^{\frac{1}{n}}  \Rightarrow  y^n > x  x^{\frac 1n}= \sup \{ y \in \mathbb R, s.t.  y \geq 0, y^n \leq x \}  y^n > x  x y^n  y = x^{\frac{1}{n}}  \Rightarrow  y^n < x ","['real-analysis', 'solution-verification']"
2,Help with filling in the details to show that $\lim\limits_{n\to\infty} \sum\limits_{k=1}^{n}\left(\frac{k}{n}\right)^n=\frac{e}{e-1}$,Help with filling in the details to show that,\lim\limits_{n\to\infty} \sum\limits_{k=1}^{n}\left(\frac{k}{n}\right)^n=\frac{e}{e-1},"So we have, $$\begin{align} \lim_{n\to\infty} \sum_{k=1}^{n}\left(\frac{k}{n}\right)^n &= \lim_{n\to\infty} \sum_{j=0}^{n-1}\bigg(\frac{n-j}{n}\bigg)^n \\ &= \lim_{n\to\infty} \bigg(1+\bigg(1-\frac{1}{n}\bigg)^n+...+\bigg(1-\frac{n-1}{n}\bigg)^n\bigg) \\ &= 1+e^{-1}+e^{-2}+... \\ &= \frac{e}{e-1} \end{align},$$ but my problem is going from the second to the third line. As the limit involves both the summand and the sum and I am not sure how this line is ""legal"", that is what is it that allows is to apply the limit first to each term then to the sum, viz why is that $\lim\limits_{n\to\infty} \sum\limits_{k=0}^{n-1}\bigg(1+\frac{-k}{n}\bigg)^n=\sum\limits_{k=0}^\infty\lim\limits_{n\to\infty}\bigg(1+\frac{-k}{n}\bigg)^n$ ? Is there a double limit? But can you have a double limit involving the same variable? Is this some special case of Fubini's Theorem, if so I am really struggling to see how? Or maybe it is a Riemann sum? If so so I'm not sure how to show t manipulate it to show that.  Any help will be greatly appreciated. Thanks in advance.","So we have, but my problem is going from the second to the third line. As the limit involves both the summand and the sum and I am not sure how this line is ""legal"", that is what is it that allows is to apply the limit first to each term then to the sum, viz why is that ? Is there a double limit? But can you have a double limit involving the same variable? Is this some special case of Fubini's Theorem, if so I am really struggling to see how? Or maybe it is a Riemann sum? If so so I'm not sure how to show t manipulate it to show that.  Any help will be greatly appreciated. Thanks in advance.","\begin{align}
\lim_{n\to\infty} \sum_{k=1}^{n}\left(\frac{k}{n}\right)^n &= \lim_{n\to\infty} \sum_{j=0}^{n-1}\bigg(\frac{n-j}{n}\bigg)^n \\
&= \lim_{n\to\infty} \bigg(1+\bigg(1-\frac{1}{n}\bigg)^n+...+\bigg(1-\frac{n-1}{n}\bigg)^n\bigg) \\
&= 1+e^{-1}+e^{-2}+... \\
&= \frac{e}{e-1}
\end{align}, \lim\limits_{n\to\infty} \sum\limits_{k=0}^{n-1}\bigg(1+\frac{-k}{n}\bigg)^n=\sum\limits_{k=0}^\infty\lim\limits_{n\to\infty}\bigg(1+\frac{-k}{n}\bigg)^n","['real-analysis', 'limits']"
3,"Let $X$ be a random variable, $\frac{\mathbb{E}[e^{sX}-e^{tX}]}{s-t}$ for $s \approx t$. As","Let  be a random variable,  for . As",X \frac{\mathbb{E}[e^{sX}-e^{tX}]}{s-t} s \approx t,"Question Let $X$ be a random variable for which we only have the value of its Moment Generating Function $M_X$ on a discrete set of points, I am looking for a stable method to compute: $$\frac{M_X(s) - M_X(t)}{s-t}=\frac{\mathbb{E}[e^{sX}-e^{tX}]}{s-t}, \qquad s \approx t \approx 0.$$ Thoughts/ Attempts The problem is, clearly, that both the numerator as the denumerator becomes small as $|s-t| \rightarrow 0$. We can however use the Taylor expansion of $e^{sX}$ to overcome this problem, indeed we have (with $ \gamma_n(x,y) = \sum_{m=0}^{n-1} x^m y^{n-1-m} $): \begin{align*} \frac{\mathbb{E}[e^{sX}-e^{tX}]}{s-t} &= - \sum_{n=1}^\infty \frac{\gamma_{n-1}(s, t)}{n!} \left(  \frac{d^n M_X}{ds^n} \right)\bigg|_{s=0}. \end{align*} This resolves our original problem. However it introduces another problem, namely the computation of: $\left(  \frac{d^n M_X}{ds^n} \right)\bigg|_{s=0}$, which again causes numerical instability. I have tried to stabilize this differentiation. I see no method to do this (except by applying Cauchy's integral formula , which can not be applied here as we only know the value of $M_X(s)$ for real values $s$). Maybe we can rewrite this formula again in function of $M_X(s)$ but I am not sure how. I have also implemented the suggestions found here but high order derivatives unfortunately can not be computed this way. Background/Reason For Question I am using this quantity in a recursion and numerical errors introduced by it make the recurrence fail. This happens after $10-20$ steps, some precision is lost in each subsequent step and this inaccuracy explodes..","Question Let $X$ be a random variable for which we only have the value of its Moment Generating Function $M_X$ on a discrete set of points, I am looking for a stable method to compute: $$\frac{M_X(s) - M_X(t)}{s-t}=\frac{\mathbb{E}[e^{sX}-e^{tX}]}{s-t}, \qquad s \approx t \approx 0.$$ Thoughts/ Attempts The problem is, clearly, that both the numerator as the denumerator becomes small as $|s-t| \rightarrow 0$. We can however use the Taylor expansion of $e^{sX}$ to overcome this problem, indeed we have (with $ \gamma_n(x,y) = \sum_{m=0}^{n-1} x^m y^{n-1-m} $): \begin{align*} \frac{\mathbb{E}[e^{sX}-e^{tX}]}{s-t} &= - \sum_{n=1}^\infty \frac{\gamma_{n-1}(s, t)}{n!} \left(  \frac{d^n M_X}{ds^n} \right)\bigg|_{s=0}. \end{align*} This resolves our original problem. However it introduces another problem, namely the computation of: $\left(  \frac{d^n M_X}{ds^n} \right)\bigg|_{s=0}$, which again causes numerical instability. I have tried to stabilize this differentiation. I see no method to do this (except by applying Cauchy's integral formula , which can not be applied here as we only know the value of $M_X(s)$ for real values $s$). Maybe we can rewrite this formula again in function of $M_X(s)$ but I am not sure how. I have also implemented the suggestions found here but high order derivatives unfortunately can not be computed this way. Background/Reason For Question I am using this quantity in a recursion and numerical errors introduced by it make the recurrence fail. This happens after $10-20$ steps, some precision is lost in each subsequent step and this inaccuracy explodes..",,"['real-analysis', 'probability', 'derivatives', 'numerical-methods', 'recursive-algorithms']"
4,On convergence of series of the generalized mean $\sum_{n=1}^{\infty} \left(\frac{a_1^{1/s}+a_2^{1/s}+\cdots +a_n^{1/s}}{n}\right)^s.$,On convergence of series of the generalized mean,\sum_{n=1}^{\infty} \left(\frac{a_1^{1/s}+a_2^{1/s}+\cdots +a_n^{1/s}}{n}\right)^s.,"Assume that $a_n>0$ such that $\sum_{n=1}^{\infty}a_n $ converges. Question: For what values of $s\in \Bbb R$ does the following series :   $$ I_s= \sum_{n=1}^{\infty} \left(\frac{a_1^{1/s}+a_2^{1/s}+\cdots +a_n^{1/s}}{n}\right)^s.$$ converges or diverges? This question is partially  motivated by some comments on this post where it is shown that $I_s$ converges for $s>1$. Moreover, it is well known that  $$\lim_{s\to\infty}\left(\frac{a_1^{1/s}+a_2^{1/s}+\cdots +a_n^{1/s}}{n}\right)^s = \left(a_1a_2\cdots a_n\right)^{1/n}$$ Accordingly, Taking  $b_n= 1/a_n$ is  one readily get, $$\lim_{\color{red}{s\to-\infty}}\left(\frac{a_1^{1/s}+a_2^{1/s}+\cdots +a_n^{1/s}}{n}\right)^s = \left(a_1a_2\cdots a_n\right)^{1/n}$$  it draws from Carleman's inequality that : $$\color{red}{ I_{-\infty}}=I_\infty= \sum_{n=1}^{\infty}\left(a_1a_2\cdots a_n\right)^{1/n} \le e  \sum_{n=1}^{\infty} a_n<\infty .$$ Patently it is also true that the convergence holds for $s=-1$ this is proven here .  Whereas the convergence fails for $0<s<1$  Indeed, $$\sum_{n=1}^{\infty} \left(\frac{a_1^{1/s}+a_2^{1/s}+\cdots +a_n^{1/s}}{n}\right)^s \ge  \sum_{n=1}^{\infty} \frac{a_1}{n^s}=\infty$$ So we have that $I_s$ converges for $1<s\le\infty$ or $s=\in\{-1,-\infty\}$ and diverges for $0<s<1$ . Hence the original question reduces on studying $I_s$ for $s\le0$ can anyone help? Clearly the hope is that $I_s$ converges for for $-\infty\le s\le -1 $ and  diverges for $-1<s<0.$` I don't know if one could infer some conjecture for the case $s=0$ since it seems pathological.","Assume that $a_n>0$ such that $\sum_{n=1}^{\infty}a_n $ converges. Question: For what values of $s\in \Bbb R$ does the following series :   $$ I_s= \sum_{n=1}^{\infty} \left(\frac{a_1^{1/s}+a_2^{1/s}+\cdots +a_n^{1/s}}{n}\right)^s.$$ converges or diverges? This question is partially  motivated by some comments on this post where it is shown that $I_s$ converges for $s>1$. Moreover, it is well known that  $$\lim_{s\to\infty}\left(\frac{a_1^{1/s}+a_2^{1/s}+\cdots +a_n^{1/s}}{n}\right)^s = \left(a_1a_2\cdots a_n\right)^{1/n}$$ Accordingly, Taking  $b_n= 1/a_n$ is  one readily get, $$\lim_{\color{red}{s\to-\infty}}\left(\frac{a_1^{1/s}+a_2^{1/s}+\cdots +a_n^{1/s}}{n}\right)^s = \left(a_1a_2\cdots a_n\right)^{1/n}$$  it draws from Carleman's inequality that : $$\color{red}{ I_{-\infty}}=I_\infty= \sum_{n=1}^{\infty}\left(a_1a_2\cdots a_n\right)^{1/n} \le e  \sum_{n=1}^{\infty} a_n<\infty .$$ Patently it is also true that the convergence holds for $s=-1$ this is proven here .  Whereas the convergence fails for $0<s<1$  Indeed, $$\sum_{n=1}^{\infty} \left(\frac{a_1^{1/s}+a_2^{1/s}+\cdots +a_n^{1/s}}{n}\right)^s \ge  \sum_{n=1}^{\infty} \frac{a_1}{n^s}=\infty$$ So we have that $I_s$ converges for $1<s\le\infty$ or $s=\in\{-1,-\infty\}$ and diverges for $0<s<1$ . Hence the original question reduces on studying $I_s$ for $s\le0$ can anyone help? Clearly the hope is that $I_s$ converges for for $-\infty\le s\le -1 $ and  diverges for $-1<s<0.$` I don't know if one could infer some conjecture for the case $s=0$ since it seems pathological.",,"['calculus', 'real-analysis', 'sequences-and-series', 'convergence-divergence', 'summation']"
5,Combining the extreme value and intermediate value theorems,Combining the extreme value and intermediate value theorems,,"The extreme value and intermediate value theorems are two of the most important theorems in calculus. They are generally regarded as separate theorems. However, there is a very natural way to combine them: Let $f:[a,b] \to \mathbb{R}$ be a continuous mapping. Then $f([a,b]) = [c, d]$ where $c \leq d$. Two questions: $(1)$ Why isn't this combination more common? $(2)$ Can we prove this in one go (ie., without first proving the IVT and EVT and deducing this as a corollary)? From the perspective of topology, this theorem follows from the fact that the continuous image of a connected and compact space is connected and compact. I'm looking for a ""calculus"" style proof (ie., using an $\epsilon-\delta$ argument, least upper bound property and/or sequences).","The extreme value and intermediate value theorems are two of the most important theorems in calculus. They are generally regarded as separate theorems. However, there is a very natural way to combine them: Let $f:[a,b] \to \mathbb{R}$ be a continuous mapping. Then $f([a,b]) = [c, d]$ where $c \leq d$. Two questions: $(1)$ Why isn't this combination more common? $(2)$ Can we prove this in one go (ie., without first proving the IVT and EVT and deducing this as a corollary)? From the perspective of topology, this theorem follows from the fact that the continuous image of a connected and compact space is connected and compact. I'm looking for a ""calculus"" style proof (ie., using an $\epsilon-\delta$ argument, least upper bound property and/or sequences).",,"['calculus', 'real-analysis', 'continuity']"
6,Prove a probability inequality,Prove a probability inequality,,"Let $X,Y\sim\phi(x)$ be i.i.d. ($\phi:\Bbb R\to\Bbb R_{\ge 0}$ denotes the PDF), show that    $$P(|X+Y|\le1)\le2P(|X-Y|\le1).$$ My thoughts are standard,  $$P(|X+Y|\le1)=\int_\Bbb R \rm dx\phi(x)\int_{-1-x}^{1-x}\rm dy\phi(y)=\int_\Bbb R \rm dx\phi(x)(\Phi(1-x)-\Phi(-1-x)),$$ in which $\Phi$ denotes CDF. And $$P(|X-Y|\le1)=\int_\Bbb R \rm dx\phi(x)\int_{x-1}^{x+1}\rm dy\phi(y)=\int_\Bbb R \rm dx\phi(x)(\Phi(x-1)-\Phi(x+1)).$$ Then naturally, $$2P(|X-Y|\le1)-P(|X+Y|\le1)=\int_\Bbb R\rm dx\phi(x)(2(\Phi(x+1)-\Phi(x-1))-\Phi(1-x)+\Phi(-1-x)). $$ Then I don't know what to do. Am I even on the right track? (I slightly doubt it because my professor labelled this problem as ""hard"" so I think it's kinda resistant to standard approaches. ) By the way, $2$ is said to be optimal here.","Let $X,Y\sim\phi(x)$ be i.i.d. ($\phi:\Bbb R\to\Bbb R_{\ge 0}$ denotes the PDF), show that    $$P(|X+Y|\le1)\le2P(|X-Y|\le1).$$ My thoughts are standard,  $$P(|X+Y|\le1)=\int_\Bbb R \rm dx\phi(x)\int_{-1-x}^{1-x}\rm dy\phi(y)=\int_\Bbb R \rm dx\phi(x)(\Phi(1-x)-\Phi(-1-x)),$$ in which $\Phi$ denotes CDF. And $$P(|X-Y|\le1)=\int_\Bbb R \rm dx\phi(x)\int_{x-1}^{x+1}\rm dy\phi(y)=\int_\Bbb R \rm dx\phi(x)(\Phi(x-1)-\Phi(x+1)).$$ Then naturally, $$2P(|X-Y|\le1)-P(|X+Y|\le1)=\int_\Bbb R\rm dx\phi(x)(2(\Phi(x+1)-\Phi(x-1))-\Phi(1-x)+\Phi(-1-x)). $$ Then I don't know what to do. Am I even on the right track? (I slightly doubt it because my professor labelled this problem as ""hard"" so I think it's kinda resistant to standard approaches. ) By the way, $2$ is said to be optimal here.",,"['calculus', 'real-analysis', 'probability-theory']"
7,Prove that exist a function $f:\mathbb{R}\rightarrow \mathbb{R}$ infinitely differentiable.,Prove that exist a function  infinitely differentiable.,f:\mathbb{R}\rightarrow \mathbb{R},"Prove that exist a function $f:\mathbb{R}\rightarrow \mathbb{R}$ infinitely  differentiable, such that $$\int_{\mathbb{R}}f(t)\,dt=1\;\mbox{ and }\; \int_{\mathbb{R}}t^nf(t)dt=0,\, \forall\, n\geq 1\; \mbox{integer}.$$ Some ideas?  Thank you.","Prove that exist a function $f:\mathbb{R}\rightarrow \mathbb{R}$ infinitely  differentiable, such that $$\int_{\mathbb{R}}f(t)\,dt=1\;\mbox{ and }\; \int_{\mathbb{R}}t^nf(t)dt=0,\, \forall\, n\geq 1\; \mbox{integer}.$$ Some ideas?  Thank you.",,['real-analysis']
8,"If $f$ is continuous on $[a,b]$ then $f$ is uniformly continuous on $[a,b]$.",If  is continuous on  then  is uniformly continuous on .,"f [a,b] f [a,b]","So I want to prove that continuity on $[a,b]$ implies uniform continuity with only using the least upper bound property of the reals. I know the basic idea of this, but am getting confused with choosing the right $\delta$. Here's where I am so far: Proof. Let $\epsilon >0$ and define $$A(\delta) = \{u \in [a,b] ~| \text{ if } x,t \in [a,u] \text{ and } |x-t| < \delta, \text{ then } |f(x)-f(t)| < \epsilon \},$$ and $$A = \bigcup_{\delta >0} A(\delta).$$ Since $a \in A$ and $b$ is an upper bound for $A$, $\alpha = \sup(A)$ exists. Now I need to show two things: first that $\alpha = b$, and then that $\alpha \in A$. To show $\alpha = b$ assume that $\alpha <b$. Then by continuity there exists some $\delta(\alpha) > 0$ such that if $|x-\alpha|<\delta(\alpha)$, then $|f(x)-f(\alpha)|< \epsilon$. Now since $\alpha = \sup(A)$, there exists some $x_0 \in A$ such that $\alpha - \delta(\alpha) < x_0 \le \alpha$. Then there exists some $\delta(x_0)>0$ such that $x_0 \in A(\delta(x_0))$. Now let $\delta_{\text{min}} = \min\{\delta(x_0), \delta(\alpha)\}$... So here is where I am stuck. For starters I'm not sure if this $\delta$ will work. Also, I am imagining that I will need to use the triangle inequality to show that $\alpha \in A(\delta^*)$ where $\delta^*$ is whichever $\delta$ that will do the trick, but I'm not sure what to use the triangle inequality on. Basically I've confused myself. Help?","So I want to prove that continuity on $[a,b]$ implies uniform continuity with only using the least upper bound property of the reals. I know the basic idea of this, but am getting confused with choosing the right $\delta$. Here's where I am so far: Proof. Let $\epsilon >0$ and define $$A(\delta) = \{u \in [a,b] ~| \text{ if } x,t \in [a,u] \text{ and } |x-t| < \delta, \text{ then } |f(x)-f(t)| < \epsilon \},$$ and $$A = \bigcup_{\delta >0} A(\delta).$$ Since $a \in A$ and $b$ is an upper bound for $A$, $\alpha = \sup(A)$ exists. Now I need to show two things: first that $\alpha = b$, and then that $\alpha \in A$. To show $\alpha = b$ assume that $\alpha <b$. Then by continuity there exists some $\delta(\alpha) > 0$ such that if $|x-\alpha|<\delta(\alpha)$, then $|f(x)-f(\alpha)|< \epsilon$. Now since $\alpha = \sup(A)$, there exists some $x_0 \in A$ such that $\alpha - \delta(\alpha) < x_0 \le \alpha$. Then there exists some $\delta(x_0)>0$ such that $x_0 \in A(\delta(x_0))$. Now let $\delta_{\text{min}} = \min\{\delta(x_0), \delta(\alpha)\}$... So here is where I am stuck. For starters I'm not sure if this $\delta$ will work. Also, I am imagining that I will need to use the triangle inequality to show that $\alpha \in A(\delta^*)$ where $\delta^*$ is whichever $\delta$ that will do the trick, but I'm not sure what to use the triangle inequality on. Basically I've confused myself. Help?",,"['real-analysis', 'continuity', 'uniform-continuity']"
9,Linear differential equations of the $n$th order,Linear differential equations of the th order,n,"$$ L(x)=x^{(n)}+a_1(t)x^{(n-1)}+\cdots +a_{n-1}(t)x'+a_n(t)x;\qquad a_1(t),a_2(t),\ldots\in C$$ $$U_j(\varphi)= \sum_{k=0}^{n-1}(M_{jk} \varphi^{k}(\alpha)-N_{jk} \varphi^{k}(\beta))= \gamma_j\quad \text{or} \quad U(\varphi)= \gamma$$ $$L(x)=b(t) \\ U(x)=0 \tag{1}$$ $(1)$ has to be solved. This is how the answer starts off, if anyone can pinpoint the detail, step that I am not seeing feel free to notify me. Let $\varphi_{1}(t), \varphi_2(t),\ldots,\varphi_n(t)$ be the fundamental set of answers, the Vronski matrix : $W(t)=W(\varphi_1(t),\varphi_2(t),\ldots,\varphi_n(t))$ and the algebraic complement of the element $._{ni},\ \ $ $W_{ni}(t)$. We have the given formula: $$\varphi(t)= \sum_{i=1}^{n} \gamma_i \varphi_i(t)+ \sum_{i=1}^{n}\varphi_i(t) \int_{\alpha}^{t}\frac{W_{ni}(s)b(s)}{W(s)}ds $$ from here we have: $$\varphi(t)= \sum_{i=1}^{n} \gamma_i \varphi_i(t)+ \int_{\alpha}^{t}K^{*}(t,s)b(s)ds \tag{${*}{*}{*}{*}$}$$ where: $$K^{*}(t,s)=\frac{1}{W(s)} \begin{vmatrix} \varphi_1(s) & \varphi_2(s)& \cdots& \varphi_n(s)\\ \varphi_1'(s)& \varphi_2'(s)& \cdots & \varphi'_n(s)\\ \vdots& \vdots & \ddots & \vdots\\  \varphi^{(n-2)}_1(s)& \varphi^{(n-2)}_2(s)& \cdots & \varphi^{(n-2)}_n(s)\\  \varphi_1(s) & \varphi_2(s)& \cdots & \varphi_n(s) \end{vmatrix}$$ Who understands: $({*}{*}{*}{*})$ ? Made an important update.","$$ L(x)=x^{(n)}+a_1(t)x^{(n-1)}+\cdots +a_{n-1}(t)x'+a_n(t)x;\qquad a_1(t),a_2(t),\ldots\in C$$ $$U_j(\varphi)= \sum_{k=0}^{n-1}(M_{jk} \varphi^{k}(\alpha)-N_{jk} \varphi^{k}(\beta))= \gamma_j\quad \text{or} \quad U(\varphi)= \gamma$$ $$L(x)=b(t) \\ U(x)=0 \tag{1}$$ $(1)$ has to be solved. This is how the answer starts off, if anyone can pinpoint the detail, step that I am not seeing feel free to notify me. Let $\varphi_{1}(t), \varphi_2(t),\ldots,\varphi_n(t)$ be the fundamental set of answers, the Vronski matrix : $W(t)=W(\varphi_1(t),\varphi_2(t),\ldots,\varphi_n(t))$ and the algebraic complement of the element $._{ni},\ \ $ $W_{ni}(t)$. We have the given formula: $$\varphi(t)= \sum_{i=1}^{n} \gamma_i \varphi_i(t)+ \sum_{i=1}^{n}\varphi_i(t) \int_{\alpha}^{t}\frac{W_{ni}(s)b(s)}{W(s)}ds $$ from here we have: $$\varphi(t)= \sum_{i=1}^{n} \gamma_i \varphi_i(t)+ \int_{\alpha}^{t}K^{*}(t,s)b(s)ds \tag{${*}{*}{*}{*}$}$$ where: $$K^{*}(t,s)=\frac{1}{W(s)} \begin{vmatrix} \varphi_1(s) & \varphi_2(s)& \cdots& \varphi_n(s)\\ \varphi_1'(s)& \varphi_2'(s)& \cdots & \varphi'_n(s)\\ \vdots& \vdots & \ddots & \vdots\\  \varphi^{(n-2)}_1(s)& \varphi^{(n-2)}_2(s)& \cdots & \varphi^{(n-2)}_n(s)\\  \varphi_1(s) & \varphi_2(s)& \cdots & \varphi_n(s) \end{vmatrix}$$ Who understands: $({*}{*}{*}{*})$ ? Made an important update.",,['calculus']
10,Inequality involving exponential partial sums,Inequality involving exponential partial sums,,"Consider the exponential partial sums $E_n(x) = \sum_{i=0}^n \frac{x^i}{i!}$. I want to prove that for all $x \ge 0$: $$2 \frac {E_{n-1}(x)} {E_n(x)} \ge \frac {E_{n}(x)} {E_{n+1}(x)} + \frac {E_{n-2}(x)} {E_{n-1}(x)}$$ My approach so far First observe that $E_{n-1}(x) = E_{n}(x) - \frac {x^n}{n!}$. So the inequality becomes:  $$2 \frac {E_{n}(x) - \frac {x^n}{n!}} {E_n(x)} \ge \frac {E_{n+1}(x) - \frac {x^{n+1}}{(n+1)!}} {E_{n+1}(x)} + \frac {E_{n-1}(x) - \frac {x^{n-1}}{(n-1)!}} {E_{n-1}(x)}$$ which leads to  $$2 \frac {\frac {x^n}{n!}} {E_n(x)} \le \frac {\frac {x^{n+1}}{(n+1)!}} {E_{n+1}(x)} + \frac {\frac {x^{n-1}}{(n-1)!}} {E_{n-1}(x)}$$ So all we need to show is that $\frac {x^n} {n! E_n(x)}$ is convex in $n$. Unfortunately, I didn't have much luck going forward. A good direction could be to use the fact that $n! E_n(x) = e^x \Gamma(n+1,x)$, where $\Gamma(n+1,x) = \int_x^\infty t^n e^{-t} \textrm{dt}$ is the incomplete gamma function . I feel that this way, I will be able to prove the inequality analytically without working painfully with factorials and large sums. So it suffices to show that the following is convex as a function of $n$: $$\frac {x^n} {\int_x^\infty t^n e^{-t} \textrm{dt}}$$ Any ideas on how to continue? Unfortunately, derivatives of the incomplete gamma function with respect to $n$ are not as nice as those with respect to $x$.","Consider the exponential partial sums $E_n(x) = \sum_{i=0}^n \frac{x^i}{i!}$. I want to prove that for all $x \ge 0$: $$2 \frac {E_{n-1}(x)} {E_n(x)} \ge \frac {E_{n}(x)} {E_{n+1}(x)} + \frac {E_{n-2}(x)} {E_{n-1}(x)}$$ My approach so far First observe that $E_{n-1}(x) = E_{n}(x) - \frac {x^n}{n!}$. So the inequality becomes:  $$2 \frac {E_{n}(x) - \frac {x^n}{n!}} {E_n(x)} \ge \frac {E_{n+1}(x) - \frac {x^{n+1}}{(n+1)!}} {E_{n+1}(x)} + \frac {E_{n-1}(x) - \frac {x^{n-1}}{(n-1)!}} {E_{n-1}(x)}$$ which leads to  $$2 \frac {\frac {x^n}{n!}} {E_n(x)} \le \frac {\frac {x^{n+1}}{(n+1)!}} {E_{n+1}(x)} + \frac {\frac {x^{n-1}}{(n-1)!}} {E_{n-1}(x)}$$ So all we need to show is that $\frac {x^n} {n! E_n(x)}$ is convex in $n$. Unfortunately, I didn't have much luck going forward. A good direction could be to use the fact that $n! E_n(x) = e^x \Gamma(n+1,x)$, where $\Gamma(n+1,x) = \int_x^\infty t^n e^{-t} \textrm{dt}$ is the incomplete gamma function . I feel that this way, I will be able to prove the inequality analytically without working painfully with factorials and large sums. So it suffices to show that the following is convex as a function of $n$: $$\frac {x^n} {\int_x^\infty t^n e^{-t} \textrm{dt}}$$ Any ideas on how to continue? Unfortunately, derivatives of the incomplete gamma function with respect to $n$ are not as nice as those with respect to $x$.",,"['real-analysis', 'sequences-and-series', 'inequality']"
11,Example of function with a hundred minima,Example of function with a hundred minima,,"Find a function $f\colon\mathbb{R}^2 \to\mathbb{R}$, $f \in C^{\infty}$, such that $\nabla f = 0$ for exactly $100$ points and in these points there are only local minima.","Find a function $f\colon\mathbb{R}^2 \to\mathbb{R}$, $f \in C^{\infty}$, such that $\nabla f = 0$ for exactly $100$ points and in these points there are only local minima.",,"['real-analysis', 'functions']"
12,Finding the closed form of $\sum_{k=1}^{\infty} \sum_{n=1}^{\infty}(-1)^{k+n} \frac{\log(k+n)}{k n}$,Finding the closed form of,\sum_{k=1}^{\infty} \sum_{n=1}^{\infty}(-1)^{k+n} \frac{\log(k+n)}{k n},"A while ago I computed pretty easily the series $\sum_{k=1}^{\infty} \sum_{n=1}^{\infty}(-1)^{k+n} \frac{\log(k+n)}{k+ n}$  and then I thought of tackling the case where we have the product instead of sum in denominator, but this one  seems far harder than the previous one. What would you suggest me to do? $$\sum_{k=1}^{\infty} \sum_{n=1}^{\infty}(-1)^{k+n} \frac{\log(k+n)}{k n}$$","A while ago I computed pretty easily the series $\sum_{k=1}^{\infty} \sum_{n=1}^{\infty}(-1)^{k+n} \frac{\log(k+n)}{k+ n}$  and then I thought of tackling the case where we have the product instead of sum in denominator, but this one  seems far harder than the previous one. What would you suggest me to do? $$\sum_{k=1}^{\infty} \sum_{n=1}^{\infty}(-1)^{k+n} \frac{\log(k+n)}{k n}$$",,"['calculus', 'real-analysis', 'integration', 'sequences-and-series', 'improper-integrals']"
13,How prove $g(x)$ is odd function :$g(x)=-g(-x)$,How prove  is odd function :,g(x) g(x)=-g(-x),"QUestion: let $f(x),g(x)$ is continuous on $R$,and such   $$f(x-y)=f(x)f(y)-g(x)g(y)$$   and $f(0)=1$ show that: for any $x\in R$, have $g(x)=-g(-x)$ my try: let $x=y=0$,then $$f(0)-[f(0)]^2=g(0)g(0)\Longrightarrow g(0)=0$$ and let $x=0$, note $f(0)=1,g(0)=0$,so $$f(-y)=f(y)$$ since $$g(x)g(y)=f(x)f(y)-f(x-y)$$ so $$g(-x)g(y)=f(-x)f(y)-f(-x-y)=f(x)f(y)-f(x+y)$$ since $$f(x+y)=f(x)f(-y)-g(x)g(-y)\Longrightarrow g(x)g(-y)=f(x)f(y)-f(x+y)$$ so $$g(-x)g(y)=g(x)g(-y)$$ But I can't have $g(x)=-g(-x)$","QUestion: let $f(x),g(x)$ is continuous on $R$,and such   $$f(x-y)=f(x)f(y)-g(x)g(y)$$   and $f(0)=1$ show that: for any $x\in R$, have $g(x)=-g(-x)$ my try: let $x=y=0$,then $$f(0)-[f(0)]^2=g(0)g(0)\Longrightarrow g(0)=0$$ and let $x=0$, note $f(0)=1,g(0)=0$,so $$f(-y)=f(y)$$ since $$g(x)g(y)=f(x)f(y)-f(x-y)$$ so $$g(-x)g(y)=f(-x)f(y)-f(-x-y)=f(x)f(y)-f(x+y)$$ since $$f(x+y)=f(x)f(-y)-g(x)g(-y)\Longrightarrow g(x)g(-y)=f(x)f(y)-f(x+y)$$ so $$g(-x)g(y)=g(x)g(-y)$$ But I can't have $g(x)=-g(-x)$",,['real-analysis']
14,Ideal in compact Hausdorff space,Ideal in compact Hausdorff space,,"This is exercise 70, chapter 4. from Folland (page 142) Let $X$ be a compact Hausdorff space. An ideal in $C(X, \mathbb{R})$ is a subalgebra $J$ of  $C(X, \mathbb{R})$ such that if $f\in J$ and $g\in C(X, \mathbb{R})$ then $fg\in J$. If $J$ is an ideal in $C(X, \mathbb{R})$, let $h(J) = \{x \in X: f(x) = 0,\ \forall f \in J\}$. Then $h(J)$ is a closed subset of $X$, called the hull of $J$. If $E\subset X$, let $k(E)=\{f \in C(X, \mathbb{R}) : f(x)=0,\  \forall x \in E\}$. Then $k(E)$ is a closed ideal in $C(X, \mathbb{R})$, called the kernel of $E$. If $E\subset X$, then $h(k(E)) =\overline{E}$. If $J$ is an ideal in $C(X, \mathbb{R})$ then $k(h(J))=\overline{J}$. (Hint: $k(h(J))$ may be identified with a subalgebra of $C_0(U, \mathbb{R})$ where $U=X\setminus h(J)$) I've managed to prove assignments 1-3. In (4) if $f$ is from $J$ then for each $y\in h(J)$ is $f(y)=0$, then must be $f\in k(h(J))$. Since $k(h(J))$ is closed, also $\overline J\subset k(h(J))$. For other way around, I've proven the hint ($k(h(J))$ may be identified with a subalgebra of $C_0(U, \mathbb{R})$ where $U=X\setminus h(J)$), using some corollary fo Stone-Weierstrass), but don't understand how to make conection between $C_0(X\setminus h(J), \mathbb{R})$ and $\overline{J}$ ANY HELP WOULD BE APPRECIATED. P.S. This is my first time asking question, but I've found this site very helpfull, so thanks!","This is exercise 70, chapter 4. from Folland (page 142) Let $X$ be a compact Hausdorff space. An ideal in $C(X, \mathbb{R})$ is a subalgebra $J$ of  $C(X, \mathbb{R})$ such that if $f\in J$ and $g\in C(X, \mathbb{R})$ then $fg\in J$. If $J$ is an ideal in $C(X, \mathbb{R})$, let $h(J) = \{x \in X: f(x) = 0,\ \forall f \in J\}$. Then $h(J)$ is a closed subset of $X$, called the hull of $J$. If $E\subset X$, let $k(E)=\{f \in C(X, \mathbb{R}) : f(x)=0,\  \forall x \in E\}$. Then $k(E)$ is a closed ideal in $C(X, \mathbb{R})$, called the kernel of $E$. If $E\subset X$, then $h(k(E)) =\overline{E}$. If $J$ is an ideal in $C(X, \mathbb{R})$ then $k(h(J))=\overline{J}$. (Hint: $k(h(J))$ may be identified with a subalgebra of $C_0(U, \mathbb{R})$ where $U=X\setminus h(J)$) I've managed to prove assignments 1-3. In (4) if $f$ is from $J$ then for each $y\in h(J)$ is $f(y)=0$, then must be $f\in k(h(J))$. Since $k(h(J))$ is closed, also $\overline J\subset k(h(J))$. For other way around, I've proven the hint ($k(h(J))$ may be identified with a subalgebra of $C_0(U, \mathbb{R})$ where $U=X\setminus h(J)$), using some corollary fo Stone-Weierstrass), but don't understand how to make conection between $C_0(X\setminus h(J), \mathbb{R})$ and $\overline{J}$ ANY HELP WOULD BE APPRECIATED. P.S. This is my first time asking question, but I've found this site very helpfull, so thanks!",,"['real-analysis', 'general-topology', 'compactness', 'ideals']"
15,How to show product of two nonmeasurable sets is nonmeasurable?,How to show product of two nonmeasurable sets is nonmeasurable?,,How can I show that the cartesian product of a nonmeasurable set in $\mathbb R$ and a nonmeasurable set or a measurable set with nonzero measure in $\mathbb R$ is nonmeasurable? I have only learned some basic measure theory (i.e. the definitions and some very basic theorems like measure continuity). Can anyone give me a hint?,How can I show that the cartesian product of a nonmeasurable set in $\mathbb R$ and a nonmeasurable set or a measurable set with nonzero measure in $\mathbb R$ is nonmeasurable? I have only learned some basic measure theory (i.e. the definitions and some very basic theorems like measure continuity). Can anyone give me a hint?,,"['real-analysis', 'measure-theory']"
16,Relation between total variation and absolute continuity,Relation between total variation and absolute continuity,,"Happy Thanksgiving to you all. I have made an attempt to a homework problem, that I'll need someone look over for me. The question is a follows. If $f$ is of bounded variation on $[a,b]$ then $f'(x)$ exists a.e. and$$\int_a^b |f'|~dx\leq T_a^b(f),$$  where $T(f)$ is the total variation. This is my attempt. Since $f$ is of bounded variation, we can write $f(x)=g(x)-h(x)$ where $g$ and $h$ are monotone increasing functions on $[a,b]$. Thus $f'(x)=g'(x)-h'(x)$ a.e.  Furthermore, $|f(x)|\leq |g'(x)|+|h'(x)|=g'(x)+h'(x)$. So $$ \begin{align*} \int_a^b|f'(x)|~dx &  \leq \int_a^b g'(x) + \int_a^b h'(x)\\ & \leq g(b)-g(a)+h(b)-h(a)\\ & = T_a^b(f). \end{align*} $$ I'm also required to find additional conditions on $f$ needed for equality to hold. Is it enough to say that equality holds if $f$ is absolutely continuous, or must I justify it?","Happy Thanksgiving to you all. I have made an attempt to a homework problem, that I'll need someone look over for me. The question is a follows. If $f$ is of bounded variation on $[a,b]$ then $f'(x)$ exists a.e. and$$\int_a^b |f'|~dx\leq T_a^b(f),$$  where $T(f)$ is the total variation. This is my attempt. Since $f$ is of bounded variation, we can write $f(x)=g(x)-h(x)$ where $g$ and $h$ are monotone increasing functions on $[a,b]$. Thus $f'(x)=g'(x)-h'(x)$ a.e.  Furthermore, $|f(x)|\leq |g'(x)|+|h'(x)|=g'(x)+h'(x)$. So $$ \begin{align*} \int_a^b|f'(x)|~dx &  \leq \int_a^b g'(x) + \int_a^b h'(x)\\ & \leq g(b)-g(a)+h(b)-h(a)\\ & = T_a^b(f). \end{align*} $$ I'm also required to find additional conditions on $f$ needed for equality to hold. Is it enough to say that equality holds if $f$ is absolutely continuous, or must I justify it?",,['real-analysis']
17,When is $ \sum_{k \in \mathbb{Z}}\left(\frac{\sin(k)}{k}\right)^{n}=2 \int_{0}^{\infty}\left(\frac{\sin(x)}{x}\right)^{n}dx$?,When is ?, \sum_{k \in \mathbb{Z}}\left(\frac{\sin(k)}{k}\right)^{n}=2 \int_{0}^{\infty}\left(\frac{\sin(x)}{x}\right)^{n}dx,"Define sequences $$a_n = \sum_{k \in \mathbb{Z}}\left(\dfrac{\sin(k)}{k}\right)^{n},  b_n = \int_{0}^{\infty}\left(\dfrac{\sin(x)}{x}\right)^{n}dx, \quad n \in \mathbb{N}.$$ I am trying to see if there is a relation between these two sequences by studying sequence $c_n = a_n-2b_n$ . From [1 , 2 , 3 , 4 , 5] , sequence $b_n$ is completely understood, as $$ b_n=\frac{\pi}{2^n (n-1)!}       \sum_{k=0}^{\lfloor n/2\rfloor} (-1)^k {n \choose k} (n-2k)^{n-1}$$ and the first few values are $$ b_1 = b_2 = \frac{\pi}{2}, b_3=\frac{3\pi}{8}, b_4 = \frac{\pi}{3}, b_5 = \frac{115\pi}{384}, b_6 = \frac{11\pi}{40}.$$ Now coming to the other sequence, [6 , 7 ], $a_n$ is also completely known, $$ a_n= \frac{(-1)^{n}\pi}{2^{n}(n-1)!}\sum_{\ell = -\lfloor n/(2\pi)\rfloor}^{\lfloor n/(2\pi)\rfloor}\left(\sum_{k = 0}^n(-1)^k{n\choose k} (2\pi \ell - n+2k)^{n-1}\operatorname{sign}(2\pi \ell-n+2k)\right).$$ First few values of the sequence are $$ a_1 = a_2 = \pi, a_3=\frac{3\pi}{4}, a_4 = \frac{2\pi}{3}, a_5 = \frac{115\pi}{192}, a_6 = \frac{11\pi}{20}.$$ This implies $c_{n}=0$ for $n=1,2,3,4,5,6$ . However $c_7 \approx 0.0000185$ , which seemed out of place from the pattern we observed before. why is the case $n=7$ special, as in why is this the first natural number where the pattern differed? Does the sequence $c_n$ converge? (If it does then behavior of $a_n$ is completely understood by $b_n$ ) Addendum : Thanks to @Dave, this paper (pg 8, example 3) clearly explains why $n=7$ is the first integer when $c_n \ne 0$ . Moreover, the last line of the example states that these two quantities are quite different (possibly suggesting $c_n \ne 0$ for $n \ge 7$ ). The authors however don't delve into analysis of sequence $c_n$ .","Define sequences I am trying to see if there is a relation between these two sequences by studying sequence . From [1 , 2 , 3 , 4 , 5] , sequence is completely understood, as and the first few values are Now coming to the other sequence, [6 , 7 ], is also completely known, First few values of the sequence are This implies for . However , which seemed out of place from the pattern we observed before. why is the case special, as in why is this the first natural number where the pattern differed? Does the sequence converge? (If it does then behavior of is completely understood by ) Addendum : Thanks to @Dave, this paper (pg 8, example 3) clearly explains why is the first integer when . Moreover, the last line of the example states that these two quantities are quite different (possibly suggesting for ). The authors however don't delve into analysis of sequence .","a_n = \sum_{k \in \mathbb{Z}}\left(\dfrac{\sin(k)}{k}\right)^{n},  b_n = \int_{0}^{\infty}\left(\dfrac{\sin(x)}{x}\right)^{n}dx, \quad n \in \mathbb{N}. c_n = a_n-2b_n b_n  b_n=\frac{\pi}{2^n (n-1)!}  
    \sum_{k=0}^{\lfloor n/2\rfloor} (-1)^k {n \choose k} (n-2k)^{n-1}  b_1 = b_2 = \frac{\pi}{2}, b_3=\frac{3\pi}{8}, b_4 = \frac{\pi}{3}, b_5 = \frac{115\pi}{384}, b_6 = \frac{11\pi}{40}. a_n  a_n= \frac{(-1)^{n}\pi}{2^{n}(n-1)!}\sum_{\ell = -\lfloor n/(2\pi)\rfloor}^{\lfloor n/(2\pi)\rfloor}\left(\sum_{k = 0}^n(-1)^k{n\choose k} (2\pi \ell - n+2k)^{n-1}\operatorname{sign}(2\pi \ell-n+2k)\right).  a_1 = a_2 = \pi, a_3=\frac{3\pi}{4}, a_4 = \frac{2\pi}{3}, a_5 = \frac{115\pi}{192}, a_6 = \frac{11\pi}{20}. c_{n}=0 n=1,2,3,4,5,6 c_7 \approx 0.0000185 n=7 c_n a_n b_n n=7 c_n \ne 0 c_n \ne 0 n \ge 7 c_n","['real-analysis', 'integration', 'sequences-and-series', 'combinatorics']"
18,Asymptotic of specific type of recurrence using Riemann integral approximation,Asymptotic of specific type of recurrence using Riemann integral approximation,,"So I've seen this technique used before, but can't find where I saw it nor do I remember how to finish. The general idea was let's say you have a recurrence of the form $$a_n=a_{n-1}+\frac{f(n)}{g(a_{n-1})}$$ Where $g$ is an increasing and positive function and $f$ is positive (it's usually something simple like the identity or constant). You want to compute an asymptotic for $a_n$ . The idea is that you can rewrite it as $$g(a_{n-1})(a_n-a_{n-1})=f(n)$$ And then sum from $n=1$ to $N$ to get $$\sum_{n=1}^N g(a_{n-1})(a_n-a_{n-1})=\sum_{n=1}^N f(n)$$ Now the LHS looks like a riemann sum, and since $g$ is increasing, we have that it is bounded above by $\int_{a_0}^{a_N} g(x)\, dx$ . If $\frac{d}{dx} G(x)=g(x)$ , we can say that $$G(a_N)-G(a_0)\geq \sum_{n=1}^N f(n)$$ $$G(a_N)\geq G(a_0)+\sum_{n=1}^N f(n)$$ $$a_N\geq G^{-1}\left(G(a_0)+\sum_{n=1}^N f(n)\right)$$ And this gives a lower bound. But to make any conclusions about the asymptotics we would also need an upper bound and I am stuck there. We can't really use the same approach for the upperbound because we can't make it a right riemann sum. I'm honestly just curious if anyone has a source for this type of approach or solutions that solve this type of problem this way as I can't remember where I initially saw this approach. But also if anyone knows how to solve it, that would of course also be appreciated","So I've seen this technique used before, but can't find where I saw it nor do I remember how to finish. The general idea was let's say you have a recurrence of the form Where is an increasing and positive function and is positive (it's usually something simple like the identity or constant). You want to compute an asymptotic for . The idea is that you can rewrite it as And then sum from to to get Now the LHS looks like a riemann sum, and since is increasing, we have that it is bounded above by . If , we can say that And this gives a lower bound. But to make any conclusions about the asymptotics we would also need an upper bound and I am stuck there. We can't really use the same approach for the upperbound because we can't make it a right riemann sum. I'm honestly just curious if anyone has a source for this type of approach or solutions that solve this type of problem this way as I can't remember where I initially saw this approach. But also if anyone knows how to solve it, that would of course also be appreciated","a_n=a_{n-1}+\frac{f(n)}{g(a_{n-1})} g f a_n g(a_{n-1})(a_n-a_{n-1})=f(n) n=1 N \sum_{n=1}^N g(a_{n-1})(a_n-a_{n-1})=\sum_{n=1}^N f(n) g \int_{a_0}^{a_N} g(x)\, dx \frac{d}{dx} G(x)=g(x) G(a_N)-G(a_0)\geq \sum_{n=1}^N f(n) G(a_N)\geq G(a_0)+\sum_{n=1}^N f(n) a_N\geq G^{-1}\left(G(a_0)+\sum_{n=1}^N f(n)\right)","['real-analysis', 'asymptotics', 'recurrence-relations']"
19,Non trivial $f$ such that $\int_0^1 x^n f(x) dx=a_n\pi+b_n$,Non trivial  such that,f \int_0^1 x^n f(x) dx=a_n\pi+b_n,"I need a non trivial function $f(x)$ such that $$\int_0^1 x^n f(x) dx=a_n\pi+b_n$$ where $a_n,b_n\in\mathbb{Z}$ and $n\in\mathbb{N}$ We know that $$\pi=4\int_0^1 \sqrt{1-x^2}\ dx$$ By Binomial theorem for rational index $$\pi= 4\int_0^1 \sum_{r=0}^\infty\binom{1/2}{r} x^{2r}\ dx$$ Any help would be highly appreciated. Edit If $f(x)=\sin^{-1} x$ then see here $$\int_0^1 x^n \sin^{-1} x \ dx= c_n \pi +d_n$$ but $c_n,d_n\in\mathbb{Q}$",I need a non trivial function such that where and We know that By Binomial theorem for rational index Any help would be highly appreciated. Edit If then see here but,"f(x) \int_0^1 x^n f(x) dx=a_n\pi+b_n a_n,b_n\in\mathbb{Z} n\in\mathbb{N} \pi=4\int_0^1 \sqrt{1-x^2}\ dx \pi= 4\int_0^1 \sum_{r=0}^\infty\binom{1/2}{r} x^{2r}\ dx f(x)=\sin^{-1} x \int_0^1 x^n \sin^{-1} x \ dx= c_n \pi +d_n c_n,d_n\in\mathbb{Q}","['real-analysis', 'calculus', 'integration', 'examples-counterexamples', 'pi']"
20,Can we prove Weierstrass function is not of bounded variation according to the definition of bounded variation rather than its indifferentiability?,Can we prove Weierstrass function is not of bounded variation according to the definition of bounded variation rather than its indifferentiability?,,"As we know, the Weierstrass function defined by $$ f(x) = \sum_{n=0}^{\infty} a^n \cos(b^n \pi x), 0<a<1,b \in 2\mathbb N+1,ab > 1+\frac{3\pi}{2} $$ is differentiable nowhere (see here for a proof of the same). As a consequence, $f$ is not of bounded variation on any finite interval, ( $f$ is said to be of bounded variation on $[a,b]$ if for any partition $a=x_0<x_1<x_2<\cdots<x_{n-1}<x_n=b$ , $\sum_{k=1}^n|f(x_k)-f(x_{k-1})|\leq M$ for a constant $M$ independent of the choice of partition) since functions of bounded variation are differentiable a.e. (as a corollary of this and the Lebesgue theorem ). In particular, the proof that $f$ is not of bounded variation is not shown explicitly via the definition of functions of bounded variation. Question: Can we construct a sequence of partitions on any interval $[a,b]$ for which the sequence of sums $\sum_{k=1}^n|f(x_k)-f(x_{k-1})|$ increases to positive infinite, showing that $f$ is not of bounded variation on $[a,b]$ ?","As we know, the Weierstrass function defined by is differentiable nowhere (see here for a proof of the same). As a consequence, is not of bounded variation on any finite interval, ( is said to be of bounded variation on if for any partition , for a constant independent of the choice of partition) since functions of bounded variation are differentiable a.e. (as a corollary of this and the Lebesgue theorem ). In particular, the proof that is not of bounded variation is not shown explicitly via the definition of functions of bounded variation. Question: Can we construct a sequence of partitions on any interval for which the sequence of sums increases to positive infinite, showing that is not of bounded variation on ?","
f(x) = \sum_{n=0}^{\infty} a^n \cos(b^n \pi x), 0<a<1,b \in 2\mathbb N+1,ab > 1+\frac{3\pi}{2}
 f f [a,b] a=x_0<x_1<x_2<\cdots<x_{n-1}<x_n=b \sum_{k=1}^n|f(x_k)-f(x_{k-1})|\leq M M f [a,b] \sum_{k=1}^n|f(x_k)-f(x_{k-1})| f [a,b]","['real-analysis', 'bounded-variation']"
21,Does a distribution in the dual space of $C_0^\infty$ extend continuously to $C_b^\infty$?,Does a distribution in the dual space of  extend continuously to ?,C_0^\infty C_b^\infty,"I want to understand distributions that are bounded linear functionals on smooth functions whose all derivatives vanish at infinity. Before asking questions, let me first define the terms used in this post. A continuous function $\phi$ on $\mathbb{R}^n$ is said to vanish at infinity if $\phi(x) \to 0$ as $|x| \to \infty$ . Denote $C_0$ the space of all continuous functions on $\mathbb{R}^n$ that vanish at infinity. This is a Banach space with the uniform norm. Denote $C_0^\infty$ the space of all smooth functions whose all derivatives vanish at infinity, i.e. $$ C_0^\infty = \{ \phi \in C^\infty : \forall \alpha, \partial^\alpha \phi \in C_0 \}. $$ This is a Frechet space with the semi-norms $$ \|\phi\|_m = \sup\{ |\partial^\alpha \phi(x)| : |\alpha| \leq m, x \in \mathbb{R}^n \}. $$ Clearly, Schwartz functions form a dense subspace in $C_0^\infty$ , and the injection $ \mathcal{S} \to C_0^\infty $ is continuous. Thus, we have the continuous injection $ (C_0^\infty)' \to \mathcal{S}' $ , i.e. any continuous linear functional on $C_0^\infty$ is a tempered distribution. Moreover, a linear functional $u$ on $C_0^\infty$ is continuous if and only if there exists $m \in \mathbb{N}, C > 0$ such that $$  \forall \phi \in C_0^\infty, |u(\phi)| \leq C \|\phi\|_m. $$ Thus, any distribution that belongs to $(C_0^\infty)'$ has a finite order. Question 1 : Is there any special name for distributions that belong to $(C_0^\infty)'$ ? The next question is about the extension of distributions in $(C_0^\infty)'$ to smooth functions with bounded derivatives. Denote $C_b$ the space of all bounded continuous functions on $\mathbb{R}^n$ . Denote $C_b^\infty$ the space of all smooth functions with bounded derivatives, i.e. $$ C_b^\infty = \{ \phi \in C^\infty : \forall \alpha, \partial^\alpha \phi \in C_b \}. $$ Clearly, we have the embeddings $C_0 \subset C_b$ and $C_0^\infty \subset C_b^\infty$ . Note that $C_0, C_0^\infty$ are not dense in $C_b, C_b^\infty$ because they are closed subspaces. Now, by Riesz-Markov representation theorem (and also by the fact that $\mathbb{R}^n$ is a locally compact Polish space), the dual space $(C_0)'$ is isomorphic to the space of all finite Borel measures on $\mathbb{R}^n$ . Here is the thing: any measure $\mu \in (C_0)'$ actually can be extended to $C_b$ by $$ \phi \in C_b \mapsto \int \phi d\mu $$ and the extension is a bounded linear functional on $C_b$ . Thus, we have the embedding $(C_0)' \subset (C_b)'$ . By analogy, I expect any distribution in $(C_0^\infty)'$ can be canonically extended to a continuous linear functional on $C_b^\infty$ . Question 2 : Can any distribution in $(C_0^\infty)'$ be canonically extended to a continuous linear functional on $C_b^\infty$ ? Edit: I expect $(C_0^\infty)'$ to be canonically identified with a closed subspace of $(C_b^\infty)'$ . Moreover, the extension should respect some distributional identities. For example, the Fourier transform of $u \in (C_0^\infty)'$ should be given by a smooth function $ \hat{u}(\xi) = \langle u, e^{ix \xi} \rangle $ .","I want to understand distributions that are bounded linear functionals on smooth functions whose all derivatives vanish at infinity. Before asking questions, let me first define the terms used in this post. A continuous function on is said to vanish at infinity if as . Denote the space of all continuous functions on that vanish at infinity. This is a Banach space with the uniform norm. Denote the space of all smooth functions whose all derivatives vanish at infinity, i.e. This is a Frechet space with the semi-norms Clearly, Schwartz functions form a dense subspace in , and the injection is continuous. Thus, we have the continuous injection , i.e. any continuous linear functional on is a tempered distribution. Moreover, a linear functional on is continuous if and only if there exists such that Thus, any distribution that belongs to has a finite order. Question 1 : Is there any special name for distributions that belong to ? The next question is about the extension of distributions in to smooth functions with bounded derivatives. Denote the space of all bounded continuous functions on . Denote the space of all smooth functions with bounded derivatives, i.e. Clearly, we have the embeddings and . Note that are not dense in because they are closed subspaces. Now, by Riesz-Markov representation theorem (and also by the fact that is a locally compact Polish space), the dual space is isomorphic to the space of all finite Borel measures on . Here is the thing: any measure actually can be extended to by and the extension is a bounded linear functional on . Thus, we have the embedding . By analogy, I expect any distribution in can be canonically extended to a continuous linear functional on . Question 2 : Can any distribution in be canonically extended to a continuous linear functional on ? Edit: I expect to be canonically identified with a closed subspace of . Moreover, the extension should respect some distributional identities. For example, the Fourier transform of should be given by a smooth function .","\phi \mathbb{R}^n \phi(x) \to 0 |x| \to \infty C_0 \mathbb{R}^n C_0^\infty  C_0^\infty = \{ \phi \in C^\infty : \forall \alpha, \partial^\alpha \phi \in C_0 \}.
  \|\phi\|_m = \sup\{ |\partial^\alpha \phi(x)| : |\alpha| \leq m, x \in \mathbb{R}^n \}.
 C_0^\infty  \mathcal{S} \to C_0^\infty   (C_0^\infty)' \to \mathcal{S}'  C_0^\infty u C_0^\infty m \in \mathbb{N}, C > 0  
\forall \phi \in C_0^\infty, |u(\phi)| \leq C \|\phi\|_m.
 (C_0^\infty)' (C_0^\infty)' (C_0^\infty)' C_b \mathbb{R}^n C_b^\infty  C_b^\infty = \{ \phi \in C^\infty : \forall \alpha, \partial^\alpha \phi \in C_b \}.
 C_0 \subset C_b C_0^\infty \subset C_b^\infty C_0, C_0^\infty C_b, C_b^\infty \mathbb{R}^n (C_0)' \mathbb{R}^n \mu \in (C_0)' C_b  \phi \in C_b \mapsto \int \phi d\mu  C_b (C_0)' \subset (C_b)' (C_0^\infty)' C_b^\infty (C_0^\infty)' C_b^\infty (C_0^\infty)' (C_b^\infty)' u \in (C_0^\infty)'  \hat{u}(\xi) = \langle u, e^{ix \xi} \rangle ","['real-analysis', 'functional-analysis', 'distribution-theory']"
22,Is there any example of a real-analytic approach to evaluate a definite integral (with an elementary integrand) whose value involves Lambert W?,Is there any example of a real-analytic approach to evaluate a definite integral (with an elementary integrand) whose value involves Lambert W?,,"I have never seen a real-analytic approach before to evaluate integrals of the form below $$\int_a^b\text{elementary function}(x)\,dx=\text{constant involving}\,W(\cdot)\,\text{in its simplest form}\tag1.$$ For instance, on MSE, all use the residue theorem: $\int_{-\infty}^{\infty}{e^x+1\over (e^x-x+1)^2+\pi^2}\mathrm dx=\int_{-\infty}^{\infty}{e^x+1\over (e^x+x+1)^2+\pi^2}\mathrm dx=1$ Interesting integral related to the Omega Constant/Lambert W Function Prove that $\int_0^\infty \frac{1+2\cos x+x\sin x}{1+2x\sin x +x^2}dx=\frac{\pi}{1+\Omega}$ where $\Omega e^\Omega=1$ Proof for integral representation of Lambert W function Evaluate $\int_{0}^{\infty} \ln(1+\frac{2\cos x}{x^2} +\frac{1}{x^4}) \, dx$ And the same applies to some of the wider literature I have come across: Stieltjes, Poisson and other integral representations for functions of Lambert W An Integral Representation of the Lambert $W$ Function Note: the proof is real-analytic, but the very first line assumes the validity of an integral identity which was only proven using complex analysis (Hankel contour). So, my question is this: Does anyone know of a proof of an identity of the form in $(1)$ that involves only real analysis (i.e. does not assume the existence of $\sqrt{-1}$ )?","I have never seen a real-analytic approach before to evaluate integrals of the form below For instance, on MSE, all use the residue theorem: $\int_{-\infty}^{\infty}{e^x+1\over (e^x-x+1)^2+\pi^2}\mathrm dx=\int_{-\infty}^{\infty}{e^x+1\over (e^x+x+1)^2+\pi^2}\mathrm dx=1$ Interesting integral related to the Omega Constant/Lambert W Function Prove that $\int_0^\infty \frac{1+2\cos x+x\sin x}{1+2x\sin x +x^2}dx=\frac{\pi}{1+\Omega}$ where $\Omega e^\Omega=1$ Proof for integral representation of Lambert W function Evaluate $\int_{0}^{\infty} \ln(1+\frac{2\cos x}{x^2} +\frac{1}{x^4}) \, dx$ And the same applies to some of the wider literature I have come across: Stieltjes, Poisson and other integral representations for functions of Lambert W An Integral Representation of the Lambert Function Note: the proof is real-analytic, but the very first line assumes the validity of an integral identity which was only proven using complex analysis (Hankel contour). So, my question is this: Does anyone know of a proof of an identity of the form in that involves only real analysis (i.e. does not assume the existence of )?","\int_a^b\text{elementary function}(x)\,dx=\text{constant involving}\,W(\cdot)\,\text{in its simplest form}\tag1. W (1) \sqrt{-1}","['real-analysis', 'integration', 'definite-integrals', 'lambert-w', 'elementary-functions']"
23,Can the Cauchy product of a conditionally convergent series with itself be absolutely convergent?,Can the Cauchy product of a conditionally convergent series with itself be absolutely convergent?,,"If $\sum_{n\ge 0} a_n$ and $\sum_{n\ge 0} b_n$ are two series, their Cauchy product is defined as $\sum_{n\ge 0} c_n$ , where $c_n = \sum^n_{k=0} a_k b_{n-k}$ . As this question points out, finding two conditionally convergent series whose Cauchy product is absolutely convergent is quite hard, but examples do indeed exist. I also learned that the Cauchy product of a divergent series with itself can be absolutely convergent (see here ). So do there exists a conditionally convergent series $\sum_{n\ge 0} a_n$ such that the Cauchy product of $\sum_{n\ge 0} a_n$ with itself is absolutely convergent?","If and are two series, their Cauchy product is defined as , where . As this question points out, finding two conditionally convergent series whose Cauchy product is absolutely convergent is quite hard, but examples do indeed exist. I also learned that the Cauchy product of a divergent series with itself can be absolutely convergent (see here ). So do there exists a conditionally convergent series such that the Cauchy product of with itself is absolutely convergent?",\sum_{n\ge 0} a_n \sum_{n\ge 0} b_n \sum_{n\ge 0} c_n c_n = \sum^n_{k=0} a_k b_{n-k} \sum_{n\ge 0} a_n \sum_{n\ge 0} a_n,"['real-analysis', 'convergence-divergence', 'conditional-convergence', 'cauchy-product']"
24,Relation between the measures of two sets defined via Lebesgue integration,Relation between the measures of two sets defined via Lebesgue integration,,"Suppose $a : \mathbb R_+ \to \{-1,1\}$ is a measurable function. Let $X_0 =\frac12$ . Consider a particle that moves on the $X-$ axis as follows. $$X_t = X_0 + \int_0^t a_s ds$$ where the integral is a Lebesgue integral. Fix a $T=\frac12$ . So, $X_t \in [0,1]$ for all $t \le T$ . Let $S \subset [0,1]$ be a set such that $\ell(S) =1$ , where $\ell(\cdot)$ is the Lebesgue measure. Define, $$G:= \{t \le T: X_t \in S\}.$$ Is it the case that $\ell(G) = \ell([0,T]) = \frac12$ ? That is, the particle spends almost no time outside $S$ ?","Suppose is a measurable function. Let . Consider a particle that moves on the axis as follows. where the integral is a Lebesgue integral. Fix a . So, for all . Let be a set such that , where is the Lebesgue measure. Define, Is it the case that ? That is, the particle spends almost no time outside ?","a : \mathbb R_+ \to \{-1,1\} X_0 =\frac12 X- X_t = X_0 + \int_0^t a_s ds T=\frac12 X_t \in [0,1] t \le T S \subset [0,1] \ell(S) =1 \ell(\cdot) G:= \{t \le T: X_t \in S\}. \ell(G) = \ell([0,T]) = \frac12 S","['real-analysis', 'lebesgue-integral', 'lebesgue-measure', 'dynamical-systems']"
25,Bijective local diffeomorphism is a diffeomorphism,Bijective local diffeomorphism is a diffeomorphism,,"Let $X,\ Y$ be manifolds in $\mathbb R^n$ (they are locally diffeomorphic to open subsets of some euclidean space.) I have a question about this fact (an exercise in Guillemin and Pollack): An injective local diffeomorphism $f: X\rightarrow Y$ is a diffeomorphism onto an open subset of $Y$ . This seems too trivial to me and hence I think I musunderstand something. I would prove this claim as follows. The map $f: X\rightarrow f(X)$ is bijective. It is differentiable at any point since it is locally smooth (and even locally diffeomorphic), and the inverse $f^{-1}: f(X)\rightarrow X$ is also differentiable at any point because given any point $f(x)\in f(X)$ , the map $f$ is a diffeomorphism near $x$ , so $f^{-1}$ is differentiable at $f(x)$ . Thus $f$ is a global diffeomorphism. Because of this and since $X$ is open in itself, $f(X)$ is open in $Y$ . How bad/good does this proof look?","Let be manifolds in (they are locally diffeomorphic to open subsets of some euclidean space.) I have a question about this fact (an exercise in Guillemin and Pollack): An injective local diffeomorphism is a diffeomorphism onto an open subset of . This seems too trivial to me and hence I think I musunderstand something. I would prove this claim as follows. The map is bijective. It is differentiable at any point since it is locally smooth (and even locally diffeomorphic), and the inverse is also differentiable at any point because given any point , the map is a diffeomorphism near , so is differentiable at . Thus is a global diffeomorphism. Because of this and since is open in itself, is open in . How bad/good does this proof look?","X,\ Y \mathbb R^n f: X\rightarrow Y Y f: X\rightarrow f(X) f^{-1}: f(X)\rightarrow X f(x)\in f(X) f x f^{-1} f(x) f X f(X) Y","['real-analysis', 'manifolds', 'differential-topology', 'smooth-manifolds']"
26,The Hahn-Banach theorem and the axiom of choice,The Hahn-Banach theorem and the axiom of choice,,"Let us recall the Hahn-Banach theorem about extensions of linear functionals: Theorem: Let $E$ be a real vector space and $F$ a subspace. If $p:E\to \mathbb{R}$ is a sublinear function, and $g:F\to \mathbb{R}$ is a linear functional on $F$ which is dominated by $p$ on $F$ i.e. $$g(x)\leq p(x)\qquad \forall x\in F$$ then there exists a linear extension $f:E\to\mathbb{R}$ of $g$ to the whole space $E$ , i.e., there exists a linear functional $f$ such that \begin{gather*} f(x)=g(x)\qquad \forall x\in F,\\ f(x)\leq p(x)\qquad \forall x\in E. \end{gather*} During the proof of this theorem, we use Zorn's lemma. It has been shown that Hahn-Banach's theorem is not equivalent to the axiom of choice. There is also some work which has been done showing that for a separable Banach space, a more direct proof can be made. An article by Douglas K. Brown and Stephen G. Simpson studies these kinds of questions of equivalence of axioms in a logic oriented framework. For my part, I am looking for a more practical example. Since the proof uses Zorn's lemma, we could (in theory) give a well enough built example of a linear functional which extension is ""not so obvious"". I will elaborate on what I mean by this. As of most of functional analysis is, extending a linear functional is pretty straightforward in the finite dimensional case. For this reason, students do not have much trouble accepting a proof requiring this kind of machinery. Although, seeing how non-trivial linear functionals behave in the infinite dimensional case gave me the idea to look around for a ""simple, yet troubling"" example. Edit: I think I should clarify what I want. The axiom of choice does not help us with the construction of an extension. I want an example where the extended linear functional is (almost) impossible to figure out/write down on paper. This would show students a problematic with non-constructive mathematics, i.e. they are unpractical in a computational framework. I've been doing my part of work, and I've looked up a bit. Here's what I came up with: Let $g:C_c^{\infty}(\mathbb{R})\to \mathbb{R}$ be defined by $$g(\phi)\mapsto \phi(0)$$ where $C_c^\infty(\mathbb{R})$ is the set of compactly supported smooth functions from $\mathbb{R}$ to $\mathbb{R}$ . We may see this real vector space as a subspace of $L^\infty(\mathbb{R})$ . We know that $L^\infty$ is not separable and that it's dual is the space of Radon measures. We could extended $g$ to all of $L^\infty(\mathbb{R})$ using Hahn-Banach. I do not feel completely satisfied by this example because it may be just me who didn't think far enough to see an adequate extension since I did not spend enough time studying Radon measures. So, as a conclusion of this somewhat long post, I am asking for the following: A well built example which clearly shows the problems we may encounter when using the axiom of choice A computation of the extension of the $g$ I've given or a proof that the extension isn't constructible if the computation isn't possible. Any other ideas which could be added as comments on the axiom of choice and Hahn-Banach's theorem, whatever viewpoint it may come from.","Let us recall the Hahn-Banach theorem about extensions of linear functionals: Theorem: Let be a real vector space and a subspace. If is a sublinear function, and is a linear functional on which is dominated by on i.e. then there exists a linear extension of to the whole space , i.e., there exists a linear functional such that During the proof of this theorem, we use Zorn's lemma. It has been shown that Hahn-Banach's theorem is not equivalent to the axiom of choice. There is also some work which has been done showing that for a separable Banach space, a more direct proof can be made. An article by Douglas K. Brown and Stephen G. Simpson studies these kinds of questions of equivalence of axioms in a logic oriented framework. For my part, I am looking for a more practical example. Since the proof uses Zorn's lemma, we could (in theory) give a well enough built example of a linear functional which extension is ""not so obvious"". I will elaborate on what I mean by this. As of most of functional analysis is, extending a linear functional is pretty straightforward in the finite dimensional case. For this reason, students do not have much trouble accepting a proof requiring this kind of machinery. Although, seeing how non-trivial linear functionals behave in the infinite dimensional case gave me the idea to look around for a ""simple, yet troubling"" example. Edit: I think I should clarify what I want. The axiom of choice does not help us with the construction of an extension. I want an example where the extended linear functional is (almost) impossible to figure out/write down on paper. This would show students a problematic with non-constructive mathematics, i.e. they are unpractical in a computational framework. I've been doing my part of work, and I've looked up a bit. Here's what I came up with: Let be defined by where is the set of compactly supported smooth functions from to . We may see this real vector space as a subspace of . We know that is not separable and that it's dual is the space of Radon measures. We could extended to all of using Hahn-Banach. I do not feel completely satisfied by this example because it may be just me who didn't think far enough to see an adequate extension since I did not spend enough time studying Radon measures. So, as a conclusion of this somewhat long post, I am asking for the following: A well built example which clearly shows the problems we may encounter when using the axiom of choice A computation of the extension of the I've given or a proof that the extension isn't constructible if the computation isn't possible. Any other ideas which could be added as comments on the axiom of choice and Hahn-Banach's theorem, whatever viewpoint it may come from.","E F p:E\to \mathbb{R} g:F\to \mathbb{R} F p F g(x)\leq p(x)\qquad \forall x\in F f:E\to\mathbb{R} g E f \begin{gather*}
f(x)=g(x)\qquad \forall x\in F,\\
f(x)\leq p(x)\qquad \forall x\in E.
\end{gather*} g:C_c^{\infty}(\mathbb{R})\to \mathbb{R} g(\phi)\mapsto \phi(0) C_c^\infty(\mathbb{R}) \mathbb{R} \mathbb{R} L^\infty(\mathbb{R}) L^\infty g L^\infty(\mathbb{R}) g","['real-analysis', 'functional-analysis', 'logic', 'axiom-of-choice', 'hahn-banach-theorem']"
27,Is $\sum_{k=1}^{n} \sin(k^2)$ bounded by a constant $M$?,Is  bounded by a constant ?,\sum_{k=1}^{n} \sin(k^2) M,I know $\sum_{k=1}^{n} \sin(k)$ is bounded by a constant . How about $\sum_{k=1}^{n} \sin(k^2)$?,I know $\sum_{k=1}^{n} \sin(k)$ is bounded by a constant . How about $\sum_{k=1}^{n} \sin(k^2)$?,,"['calculus', 'sequences-and-series']"
28,Chapman Pugh Solutions,Chapman Pugh Solutions,,"I am currently teaching myself Real Analysis with Chapman Pugh's wonderful book and I am trying to solve every exercise. However, it would be nice to have the solutions to at least a number of the problems so see if I am at least solving them in somewhat of the right manner. Also, would anyone happen to know if skipping the starred sections (Cantor Set lore etc) affect the read much? Thanks!","I am currently teaching myself Real Analysis with Chapman Pugh's wonderful book and I am trying to solve every exercise. However, it would be nice to have the solutions to at least a number of the problems so see if I am at least solving them in somewhat of the right manner. Also, would anyone happen to know if skipping the starred sections (Cantor Set lore etc) affect the read much? Thanks!",,"['real-analysis', 'self-learning']"
29,Proving inner measure equal to outer measure if a set is measurable,Proving inner measure equal to outer measure if a set is measurable,,"I'm doing the problem 19 in Real Analysis of Folland like below: Let $\mu^*$ be an outer measure on $X$ induced from a finite premeasure $\mu_0$. If $E \subset X$, define the inner measure of $E$ to be $\mu_*(E) = \mu_0(X) - \mu^*(E^c)$. Then $E$ is $\mu*$-measurable iff $\mu^*(E) = \mu_*(E)$ For the converse, I found the solution for it. For the $==>$ direction. I have a very simple solution compared to other solution I found on the Internet which I think may be wrong, but I can't find the hole in that solution. I post it here, so I hope someone can help me judge it. Thanks a lot. Because $E$ is $\mu*$-measurable, we have $$\mu_0(X) = \mu^*(X) = \mu^*(X \cap E) + \mu^*(X \cap E^c) = \mu^*(E) + \mu^*(E^c)$$ Therefore $$\mu_*(E) = \mu_0(X) - \mu^*(E^c) = \mu^*(E)$$ Because $\mu_0$ is a finite premeasure, $\mu^*$ is a finite measure","I'm doing the problem 19 in Real Analysis of Folland like below: Let $\mu^*$ be an outer measure on $X$ induced from a finite premeasure $\mu_0$. If $E \subset X$, define the inner measure of $E$ to be $\mu_*(E) = \mu_0(X) - \mu^*(E^c)$. Then $E$ is $\mu*$-measurable iff $\mu^*(E) = \mu_*(E)$ For the converse, I found the solution for it. For the $==>$ direction. I have a very simple solution compared to other solution I found on the Internet which I think may be wrong, but I can't find the hole in that solution. I post it here, so I hope someone can help me judge it. Thanks a lot. Because $E$ is $\mu*$-measurable, we have $$\mu_0(X) = \mu^*(X) = \mu^*(X \cap E) + \mu^*(X \cap E^c) = \mu^*(E) + \mu^*(E^c)$$ Therefore $$\mu_*(E) = \mu_0(X) - \mu^*(E^c) = \mu^*(E)$$ Because $\mu_0$ is a finite premeasure, $\mu^*$ is a finite measure",,"['real-analysis', 'measure-theory']"
30,Riesz's 1909 proof of the Riesz Representation Theorem,Riesz's 1909 proof of the Riesz Representation Theorem,,"Frigyes Riesz originally proved the Riesz Representation Theorem on $ C[0,1] $ -- here is his 1909 paper in English ( original French ). He builds a real valued function $ \text{A} $ on $ [0,1] $ satisfying $$ {{\LARGE{\sum_i}} {{\left|\text{A}(x_i)-2\text{A}\left({{x_i+x_{i+1}}\over 2}\right)+\text{A}(x_{i+1})\right|} \over {x_{i+1}-x_i}}} \leq {M_{\mathcal{A}} \over 2} $$ for any partition $ \{0=x_0, x_1, x_2, \cdots x_{n-1} = 1\} $ and a fixed constant $ M_{\mathcal{A}} $, then says, ""It follows that the derived numbers of the function $ \text{A}(x) $ exist and that these derivatives are functions of bounded variation."" My problem: By ""derived numbers"" he means the Dini derivative (upper right, say). Granted the expression in the sum is close to a difference quotient, I'm still not seeing why the upper right Dini derivative exists and is of bounded variation. Any insight would be greatly appreciated. Update: Riesz's 1909 paper cited above announced the Riesz Representation Theorem in a brief note. He gave more details in a followup paper in 1911 . There he rewites the sum above as: $${{\LARGE{\sum_{\large{k=1}}^{\large{m-1}}}} \left|{{\text{A}(x_{k+1}) - \text{A}(x_k)} \over {x_{k+1}-x_k}} - {{\text{A}(x_k) - \text{A}(x_{k-1})} \over {x_k-x_{k-1}}} \right|} \leq G,$$ where $x_k$ is the midpoint. Putting $D_k = \large{{\text{A}(x_k)-\text{A}(x_{k-1})} \over {x_k-x_{k-1}}}$, this reads: $$\sum_{k=1}^{m-1} |D_{k+1} - D_k| \leq G,$$ which makes it clear that the difference quotients are of bounded variation, hence bounded, so $\text{A}$ has finite upper Dini derivatives. I wrote up the recapituation here: Riesz Proves the Riesz Representation Theorem . His proof was highly concrete, based on simple piecewise linear functions on $[0,1]$ — easy to understand and a real work of art.","Frigyes Riesz originally proved the Riesz Representation Theorem on $ C[0,1] $ -- here is his 1909 paper in English ( original French ). He builds a real valued function $ \text{A} $ on $ [0,1] $ satisfying $$ {{\LARGE{\sum_i}} {{\left|\text{A}(x_i)-2\text{A}\left({{x_i+x_{i+1}}\over 2}\right)+\text{A}(x_{i+1})\right|} \over {x_{i+1}-x_i}}} \leq {M_{\mathcal{A}} \over 2} $$ for any partition $ \{0=x_0, x_1, x_2, \cdots x_{n-1} = 1\} $ and a fixed constant $ M_{\mathcal{A}} $, then says, ""It follows that the derived numbers of the function $ \text{A}(x) $ exist and that these derivatives are functions of bounded variation."" My problem: By ""derived numbers"" he means the Dini derivative (upper right, say). Granted the expression in the sum is close to a difference quotient, I'm still not seeing why the upper right Dini derivative exists and is of bounded variation. Any insight would be greatly appreciated. Update: Riesz's 1909 paper cited above announced the Riesz Representation Theorem in a brief note. He gave more details in a followup paper in 1911 . There he rewites the sum above as: $${{\LARGE{\sum_{\large{k=1}}^{\large{m-1}}}} \left|{{\text{A}(x_{k+1}) - \text{A}(x_k)} \over {x_{k+1}-x_k}} - {{\text{A}(x_k) - \text{A}(x_{k-1})} \over {x_k-x_{k-1}}} \right|} \leq G,$$ where $x_k$ is the midpoint. Putting $D_k = \large{{\text{A}(x_k)-\text{A}(x_{k-1})} \over {x_k-x_{k-1}}}$, this reads: $$\sum_{k=1}^{m-1} |D_{k+1} - D_k| \leq G,$$ which makes it clear that the difference quotients are of bounded variation, hence bounded, so $\text{A}$ has finite upper Dini derivatives. I wrote up the recapituation here: Riesz Proves the Riesz Representation Theorem . His proof was highly concrete, based on simple piecewise linear functions on $[0,1]$ — easy to understand and a real work of art.",,"['real-analysis', 'functional-analysis', 'math-history', 'riesz-representation-theorem']"
31,"Prob. 10, Sec. 3.2, in Erwin Kreyszig's ""Introductory functional analysis with applications""","Prob. 10, Sec. 3.2, in Erwin Kreyszig's ""Introductory functional analysis with applications""",,"Here is Prob. 10, Sec. 3.2, in the book Introductory Functional Analysis With Applications by Erwin Kreyszig: ... Let $T \colon X \to X$ be a bounded linear operator on a complex inner product space $X$ . If $\langle Tx, x \rangle = 0$ for all $x \in X$ , show that $T=0$ . ... My Solution: For any $x, y \in X$ and scalar $\alpha$ , we have $$ \big\langle T(\alpha x + y), \alpha x + y \big\rangle = 0. \tag{1}$$ But using the linearity of $T$ and the properties of a complex inner product, we obtain $$ \begin{align} & \ \ \  \big\langle T(\alpha x + y), \alpha x + y \big\rangle \\  &= \big\langle \alpha Tx  + Ty , \alpha x + y \big\rangle \qquad \mbox{[because $T$ is linear]} \\  &= \big\langle \alpha Tx, \alpha x \big\rangle + \big\langle \alpha Tx, y \big\rangle + \big\langle Ty, \alpha x \big\rangle + \big\langle Ty, y \big\rangle \\ &= \alpha \overline{\alpha} \langle Tx, x \rangle + \alpha \langle Tx, y \rangle + \overline{\alpha} \langle Ty, x \rangle + \langle Ty, y \rangle \\ &= \lvert \alpha \rvert^2 \langle Tx, x \rangle + \alpha \langle Tx, y \rangle + \overline{\alpha} \langle Ty, x \rangle + \langle Ty, y \rangle \\ &= \lvert \alpha \rvert^2 \cdot 0 +  \alpha \langle Tx, y \rangle + \overline{\alpha} \langle Ty, x \rangle + 0 \\  & \qquad \qquad \mbox{[using the condition on $T$]} \\ &= \alpha \langle Tx, y \rangle + \overline{\alpha} \langle Ty, x \rangle. \tag{2} \end{align} $$ Using (1) and (2), we obtain $$ \alpha \langle Tx, y \rangle + \overline{\alpha} \langle Ty, x \rangle = 0. \tag{3} $$ In the last relation, putting $\alpha = 1$ and $\alpha = \iota$ , respectively, we obtain $$ \langle Tx, y \rangle +  \langle Ty, x \rangle = 0, \tag{4} $$ and $$ \iota \langle Tx, y \rangle - \iota  \langle Ty, x \rangle = 0, $$ and this last equation upon dividing both sides by $\iota$ yields $$  \langle Tx, y \rangle -  \langle Ty, x \rangle = 0, \tag{5} $$ Now adding (4) and (5), we get $$  \langle Tx, y \rangle = 0 \ \ \ \mbox{ for all } \ x, y \in X. \tag{6} $$ But for each $x \in X$ , the image $Tx$ also is in $X$ ; so we can put $y = T(x)$ in (6) to obtain $$ \Vert Tx \Vert^2 = \langle Tx, Tx \rangle = 0 \ \ \ \mbox{ for all } \ x \in X, $$ and therefore $$ Tx = \mathbf{0} \ \ \ \mbox{ for all } \ x \in X,  $$ Therefore we have shown that $T \colon X \to X$ is the zero operator, as required. Is the above proof correct? If so, then where does the boundedness of $T$ come in?","Here is Prob. 10, Sec. 3.2, in the book Introductory Functional Analysis With Applications by Erwin Kreyszig: ... Let be a bounded linear operator on a complex inner product space . If for all , show that . ... My Solution: For any and scalar , we have But using the linearity of and the properties of a complex inner product, we obtain Using (1) and (2), we obtain In the last relation, putting and , respectively, we obtain and and this last equation upon dividing both sides by yields Now adding (4) and (5), we get But for each , the image also is in ; so we can put in (6) to obtain and therefore Therefore we have shown that is the zero operator, as required. Is the above proof correct? If so, then where does the boundedness of come in?","T \colon X \to X X \langle Tx, x \rangle = 0 x \in X T=0 x, y \in X \alpha  \big\langle T(\alpha x + y), \alpha x + y \big\rangle = 0. \tag{1} T 
\begin{align}
& \ \ \  \big\langle T(\alpha x + y), \alpha x + y \big\rangle \\ 
&= \big\langle \alpha Tx  + Ty , \alpha x + y \big\rangle \qquad \mbox{[because T is linear]} \\ 
&= \big\langle \alpha Tx, \alpha x \big\rangle + \big\langle \alpha Tx, y \big\rangle + \big\langle Ty, \alpha x \big\rangle + \big\langle Ty, y \big\rangle \\
&= \alpha \overline{\alpha} \langle Tx, x \rangle + \alpha \langle Tx, y \rangle + \overline{\alpha} \langle Ty, x \rangle + \langle Ty, y \rangle \\
&= \lvert \alpha \rvert^2 \langle Tx, x \rangle + \alpha \langle Tx, y \rangle + \overline{\alpha} \langle Ty, x \rangle + \langle Ty, y \rangle \\
&= \lvert \alpha \rvert^2 \cdot 0 +  \alpha \langle Tx, y \rangle + \overline{\alpha} \langle Ty, x \rangle + 0 \\ 
& \qquad \qquad \mbox{[using the condition on T]} \\
&= \alpha \langle Tx, y \rangle + \overline{\alpha} \langle Ty, x \rangle. \tag{2}
\end{align}
  \alpha \langle Tx, y \rangle + \overline{\alpha} \langle Ty, x \rangle = 0. \tag{3}  \alpha = 1 \alpha = \iota 
\langle Tx, y \rangle +  \langle Ty, x \rangle = 0, \tag{4}
 
\iota \langle Tx, y \rangle - \iota  \langle Ty, x \rangle = 0,
 \iota 
 \langle Tx, y \rangle -  \langle Ty, x \rangle = 0, \tag{5}
 
 \langle Tx, y \rangle = 0 \ \ \ \mbox{ for all } \ x, y \in X. \tag{6}
 x \in X Tx X y = T(x) 
\Vert Tx \Vert^2 = \langle Tx, Tx \rangle = 0 \ \ \ \mbox{ for all } \ x \in X,
 
Tx = \mathbf{0} \ \ \ \mbox{ for all } \ x \in X, 
 T \colon X \to X T","['real-analysis', 'linear-algebra', 'analysis', 'functional-analysis', 'inner-products']"
32,Two (strictly related) proofs by induction of inequalities.,Two (strictly related) proofs by induction of inequalities.,,"This is a question I originally asked on MSE, receiving no answer, even with a bounty (which expired) on it. Therefore I am crosslinking in order to prevent duplication of effort: see here for the original question. Predictably, I am stuck with the inductive steps. Let $a_n=\prod_{i=1}^m p_i^{b_i}$, where $b_1=9;b_2=5;b_3,b_4=3;b_5,b_6=2;b_7,\ldots ,b_{m}=1$, $p_i$ is the $i$-th prime and set $\lim_{n\to \infty}\frac{\log a_n}{p_m}=1$. Suppose also this ratio converges to $1$ faster than $\displaystyle\frac{p_{m(n)+1}}{p_{m(n)}}$, so that if $n$ is large enough, we always have $\log a_n<p_{m+1}$. I want to prove that for sufficiently large $n$, with $c$ being a constant and $q(n)<m$, if $$\frac{c}{\log \log a_n}<\frac{\left(1+{\prod_{i=1}^m\left(p_i^{b_i+1}-1\right)^{1/m}}\right)^m}{\prod_{i=1}^m\left(p_i^{b_i+1}-1\right)}, \tag{1}$$ then the following statements are true: $$ \frac{c}{\log \left(\log a_n+\log p_q\right)}<\\\frac{\left(1+\prod_{i=1}^{q-1}\left(p_i^{b_i+1}-1\right)^{1/m}\cdot\left(p_q^{b_q+2}-1\right)^{1/m}\cdot\prod_{i=q+1}^{m}\left(p_i^{b_i+1}-1\right)^{1/m}\right)^m}{\prod_{i=1}^{q-1}\left(p_i^{b_i+1}-1\right)\cdot\left(p_q^{b_q+2}-1\right)\cdot\prod_{i=q+1}^{m}\left(p_i^{b_i+1}-1\right)}; \tag{2}$$ $$ \frac{p_{m(n)+1}}{p_{m(n)+1}-1}\frac{c}{\log \left(\log a_n+\log p_{m(n)+1}\right)}<\frac{\left(1+\prod_{i=1}^{m(n)+1}\left(p_i^{b_i+1}-1\right)^{1/(m(n)+1)}\right)^{m(n)+1}}{\prod_{i=1}^{m(n)+1}\left(p_i^{b_i+1}-1\right)}. \tag{3}$$ To clear it up, in $(2)$ we have $a_n\cdot p_q=a_{n+1}$, in $(3)$ instead $a_n\cdot p_{m(n)+1}=a_{n+1}$. Namely, we're considering two different cases of how the sequence $a_n$ evolves: in $(3)$ the $n+1$-th term is given by the product of the $n-$th term times the $n+1$-th prime; in $(2)$ the $n$-th term is multiplied by a prime less than the $n+1$-th. $(2)$ is fairly intuitive, as the LHS goes to $0$ as $n\to \infty$ while the RHS goes to $1$, but that doesn't tell me so much since if the former is larger than $1$ and slightly smaller than the latter, I cannot say a priori that the LHS is sufficiently fast in its convergence to $0$, to be always less than the RHS. On the other hand, it is only my istinct that says $(3)$ holds, but I might be wrong. Here is how I tackled both inequalities, hoping to simplify things a bit (and not ""too much""). Call $L_t$ and $R_t$ respectively the LHS and RHS of $(1)$, $(2)$ and $(3)$. So $(2)$ is the same as $$ L_1 \frac{L_2}{L_1}<R_1\frac{R_2}{R_1},$$ and since $L_1<R_1$ by hypothesis, $(2)$ is implied by $$ \frac{\log \log a_n}{\log \left(\log a_n+\log p_q\right)}<\\ \frac{p_q^{b_q+1}-1}{p_q^{b_q+2}-1}\left(\frac{1+\prod_{i=1}^{q-1}\left(p_i^{b_i+1}-1\right)^{1/m}\cdot\left(p_q^{b_q+2}-1\right)^{1/m}\cdot\prod_{i=q+1}^{m}\left(p_i^{b_i+1}-1\right)^{1/m}}{1+{\prod_{i=1}^m\left(p_i^{b_i+1}-1\right)^{1/m}}}\right)^m.\tag{4}$$ Similarly, $(3)$ follows from $$ \frac{p_{m+1}}{p_{m+1}-1}\frac{\log \log a_n}{\log \left(\log a_n+\log p_{m+1}\right)}<\\ \left(1+\prod_{i=1}^{m+1}\left(p_i^{b_i+1}-1\right)^{1/(m+1)}\right)\left(\frac{1+\prod_{i=1}^{m+1}\left(p_i^{b_i+1}-1\right)^{1/(m+1)}}{1+{\prod_{i=1}^m\left(p_i^{b_i+1}-1\right)^{1/m}}}\right)^m.\tag{5}$$ This said, I do not know how to prove $(4)$ and $(5)$ either. Any ideas? Thanks in advance. EDIT : I have succeeded in making sufficient to prove the inequalities having $p_{m+1}-1$ instead of $p_m$, which somewhat might be a tiny bit easier.","This is a question I originally asked on MSE, receiving no answer, even with a bounty (which expired) on it. Therefore I am crosslinking in order to prevent duplication of effort: see here for the original question. Predictably, I am stuck with the inductive steps. Let $a_n=\prod_{i=1}^m p_i^{b_i}$, where $b_1=9;b_2=5;b_3,b_4=3;b_5,b_6=2;b_7,\ldots ,b_{m}=1$, $p_i$ is the $i$-th prime and set $\lim_{n\to \infty}\frac{\log a_n}{p_m}=1$. Suppose also this ratio converges to $1$ faster than $\displaystyle\frac{p_{m(n)+1}}{p_{m(n)}}$, so that if $n$ is large enough, we always have $\log a_n<p_{m+1}$. I want to prove that for sufficiently large $n$, with $c$ being a constant and $q(n)<m$, if $$\frac{c}{\log \log a_n}<\frac{\left(1+{\prod_{i=1}^m\left(p_i^{b_i+1}-1\right)^{1/m}}\right)^m}{\prod_{i=1}^m\left(p_i^{b_i+1}-1\right)}, \tag{1}$$ then the following statements are true: $$ \frac{c}{\log \left(\log a_n+\log p_q\right)}<\\\frac{\left(1+\prod_{i=1}^{q-1}\left(p_i^{b_i+1}-1\right)^{1/m}\cdot\left(p_q^{b_q+2}-1\right)^{1/m}\cdot\prod_{i=q+1}^{m}\left(p_i^{b_i+1}-1\right)^{1/m}\right)^m}{\prod_{i=1}^{q-1}\left(p_i^{b_i+1}-1\right)\cdot\left(p_q^{b_q+2}-1\right)\cdot\prod_{i=q+1}^{m}\left(p_i^{b_i+1}-1\right)}; \tag{2}$$ $$ \frac{p_{m(n)+1}}{p_{m(n)+1}-1}\frac{c}{\log \left(\log a_n+\log p_{m(n)+1}\right)}<\frac{\left(1+\prod_{i=1}^{m(n)+1}\left(p_i^{b_i+1}-1\right)^{1/(m(n)+1)}\right)^{m(n)+1}}{\prod_{i=1}^{m(n)+1}\left(p_i^{b_i+1}-1\right)}. \tag{3}$$ To clear it up, in $(2)$ we have $a_n\cdot p_q=a_{n+1}$, in $(3)$ instead $a_n\cdot p_{m(n)+1}=a_{n+1}$. Namely, we're considering two different cases of how the sequence $a_n$ evolves: in $(3)$ the $n+1$-th term is given by the product of the $n-$th term times the $n+1$-th prime; in $(2)$ the $n$-th term is multiplied by a prime less than the $n+1$-th. $(2)$ is fairly intuitive, as the LHS goes to $0$ as $n\to \infty$ while the RHS goes to $1$, but that doesn't tell me so much since if the former is larger than $1$ and slightly smaller than the latter, I cannot say a priori that the LHS is sufficiently fast in its convergence to $0$, to be always less than the RHS. On the other hand, it is only my istinct that says $(3)$ holds, but I might be wrong. Here is how I tackled both inequalities, hoping to simplify things a bit (and not ""too much""). Call $L_t$ and $R_t$ respectively the LHS and RHS of $(1)$, $(2)$ and $(3)$. So $(2)$ is the same as $$ L_1 \frac{L_2}{L_1}<R_1\frac{R_2}{R_1},$$ and since $L_1<R_1$ by hypothesis, $(2)$ is implied by $$ \frac{\log \log a_n}{\log \left(\log a_n+\log p_q\right)}<\\ \frac{p_q^{b_q+1}-1}{p_q^{b_q+2}-1}\left(\frac{1+\prod_{i=1}^{q-1}\left(p_i^{b_i+1}-1\right)^{1/m}\cdot\left(p_q^{b_q+2}-1\right)^{1/m}\cdot\prod_{i=q+1}^{m}\left(p_i^{b_i+1}-1\right)^{1/m}}{1+{\prod_{i=1}^m\left(p_i^{b_i+1}-1\right)^{1/m}}}\right)^m.\tag{4}$$ Similarly, $(3)$ follows from $$ \frac{p_{m+1}}{p_{m+1}-1}\frac{\log \log a_n}{\log \left(\log a_n+\log p_{m+1}\right)}<\\ \left(1+\prod_{i=1}^{m+1}\left(p_i^{b_i+1}-1\right)^{1/(m+1)}\right)\left(\frac{1+\prod_{i=1}^{m+1}\left(p_i^{b_i+1}-1\right)^{1/(m+1)}}{1+{\prod_{i=1}^m\left(p_i^{b_i+1}-1\right)^{1/m}}}\right)^m.\tag{5}$$ This said, I do not know how to prove $(4)$ and $(5)$ either. Any ideas? Thanks in advance. EDIT : I have succeeded in making sufficient to prove the inequalities having $p_{m+1}-1$ instead of $p_m$, which somewhat might be a tiny bit easier.",,"['real-analysis', 'prime-numbers', 'induction', 'logarithms', 'products']"
33,"""Angle-preserving"" equivalent to conformal?","""Angle-preserving"" equivalent to conformal?",,"I'd like to investigate the common turn of phrase that conflates ""angle-preserving map"" with ""conformal map"". Let $f:\Bbb R^2\to\Bbb R^2$ be a continuous function. I'll define $f$ to be angle-preserving at some $p\in\Bbb R^2$ if $$\lim_{\substack{x,y\to0\\\langle x,y\rangle=\phi|x||y|}}\frac{\langle f(p+x)-f(p),f(p+y)-f(p)\rangle}{|f(p+x)-f(p)||f(p+y)-f(p)|}=\phi$$ for all $\phi\in\Bbb R$, where the limit is over sufficiently small nonzero $x,y\in\Bbb R^2$ satisfying $\langle x,y\rangle=\phi|x||y|$ and such that $f(p+x)\ne f(p)\ne f(p+y)$, so that the division is well-defined, and by convention the ""empty limit"" (when the conditions are impossible to satisfy) vacuously limits to anything (and in particular limits to $\phi$), i.e. a constant function is angle preserving. This (admittedly complex) definition is about the weakest reasonable interpretation of ""angle-preserving"" that I can think of, and it doesn't require $f$ to be differentiable (or even continuous, although I'll assume that for convenience). In words it says that the angle $\angle f(p+x),f(p),f(p+y)$ is close to $\cos^{-1}\phi$ whenever $x,y$ are sufficiently small and have angle $\cos^{-1}\phi$ between them. For example, the function $f(x)=xg(x/|x|)$ where $g$ is a nonconstant continuous positive real function on the unit circle is a non-differentiable, non-conformal continuous map that is angle-preserving at $0$. For comparison, a map is conformal at $p$ if it is differentiable at $p$ and there is a $\lambda$ such that $\langle df_p(x),df_p(y)\rangle=\lambda\langle x,y\rangle$ for all $x,y\in\Bbb R^2$, i.e. it preserves inner products up to a scale, which implies angle-preservation and length-preservation (up to an overall local scale factor). Now, the example above shows that ""angle-preserving at $p$"" is strictly weaker than ""conformal at $p$"", but the statement I'd like to prove is that ""locally angle-preserving at $p$"" and ""locally conformal at $p$"" are equivalent, which boils down to: If $f:\Bbb R^2\to\Bbb R^2$ is an angle-preserving continuous function, then $f$ is conformal. This question covers the case when $f$ is additionally known to be differentiable, but I am interested in this looser interpretation of ""angle-preserving"".","I'd like to investigate the common turn of phrase that conflates ""angle-preserving map"" with ""conformal map"". Let $f:\Bbb R^2\to\Bbb R^2$ be a continuous function. I'll define $f$ to be angle-preserving at some $p\in\Bbb R^2$ if $$\lim_{\substack{x,y\to0\\\langle x,y\rangle=\phi|x||y|}}\frac{\langle f(p+x)-f(p),f(p+y)-f(p)\rangle}{|f(p+x)-f(p)||f(p+y)-f(p)|}=\phi$$ for all $\phi\in\Bbb R$, where the limit is over sufficiently small nonzero $x,y\in\Bbb R^2$ satisfying $\langle x,y\rangle=\phi|x||y|$ and such that $f(p+x)\ne f(p)\ne f(p+y)$, so that the division is well-defined, and by convention the ""empty limit"" (when the conditions are impossible to satisfy) vacuously limits to anything (and in particular limits to $\phi$), i.e. a constant function is angle preserving. This (admittedly complex) definition is about the weakest reasonable interpretation of ""angle-preserving"" that I can think of, and it doesn't require $f$ to be differentiable (or even continuous, although I'll assume that for convenience). In words it says that the angle $\angle f(p+x),f(p),f(p+y)$ is close to $\cos^{-1}\phi$ whenever $x,y$ are sufficiently small and have angle $\cos^{-1}\phi$ between them. For example, the function $f(x)=xg(x/|x|)$ where $g$ is a nonconstant continuous positive real function on the unit circle is a non-differentiable, non-conformal continuous map that is angle-preserving at $0$. For comparison, a map is conformal at $p$ if it is differentiable at $p$ and there is a $\lambda$ such that $\langle df_p(x),df_p(y)\rangle=\lambda\langle x,y\rangle$ for all $x,y\in\Bbb R^2$, i.e. it preserves inner products up to a scale, which implies angle-preservation and length-preservation (up to an overall local scale factor). Now, the example above shows that ""angle-preserving at $p$"" is strictly weaker than ""conformal at $p$"", but the statement I'd like to prove is that ""locally angle-preserving at $p$"" and ""locally conformal at $p$"" are equivalent, which boils down to: If $f:\Bbb R^2\to\Bbb R^2$ is an angle-preserving continuous function, then $f$ is conformal. This question covers the case when $f$ is additionally known to be differentiable, but I am interested in this looser interpretation of ""angle-preserving"".",,"['real-analysis', 'complex-analysis', 'derivatives', 'conformal-geometry', 'quasiconformal-maps']"
34,A hard problem on exponential integration,A hard problem on exponential integration,,"Suppose $a : [0 , 1] \to \Bbb R$ is an infinitely smooth function. For $\lambda\ge1$, define $$F(\lambda) := \lambda \int_0^1 e^{\lambda t} a(t) \, dt.$$ If $\sup_{\lambda\ge1}|F(\lambda)|\lt\infty$, then $a$ is the identically zero function. Below are the some results I have derived: Derivatives of any order of $a$ vanishes at $t = 1.$ For any $\delta\lt1$, $a(t)$ has a zero in the open interval $(\delta , 1).$  The same is true for all the derivatives of $a(t)$. This tells us that the $n^\text{th}$ derivative of $a$ has infinitely many distinct zeros for all natural $n$. If $a$ is analytic, then I can show that $a(t)\equiv 0.$ If $a(t)\ge0$ on $[0 , 1]$, then it is obvious that $a\equiv 0.$ I would appreciate any hint. Actually $(1)$ and $(3)$ follows from $(2)$. For $(2)$, suppose that $a(1)\gt0$ (the case that $a(1)\lt0$ can be argued in the same way as the following), then there exists $\delta\lt1$ such that $a(t)$ is strictly positive on $I = [1-\delta , 1]$. Then write $F(\lambda) = \lambda\int_0^{1-\delta}e^{\lambda t}a(t)dt + \lambda\int_{1-\delta}^1e^{\lambda t}a(t)dt$. Now there exists $c\gt0$ such that $a(t)>c$ on $I$ since $I$ is compact, so that the second term is bounded below by $c (e^{\lambda}-e^{\lambda(1-\delta)})$. But the first term is only $O(e^{\lambda(1-\delta)})$, so this contradicts the fact that $F(\lambda)$ is bounded. Morever, using the same idea and integration by parts, one can see that $n^{th}$ derivative of $a$ must vanish at $1$.","Suppose $a : [0 , 1] \to \Bbb R$ is an infinitely smooth function. For $\lambda\ge1$, define $$F(\lambda) := \lambda \int_0^1 e^{\lambda t} a(t) \, dt.$$ If $\sup_{\lambda\ge1}|F(\lambda)|\lt\infty$, then $a$ is the identically zero function. Below are the some results I have derived: Derivatives of any order of $a$ vanishes at $t = 1.$ For any $\delta\lt1$, $a(t)$ has a zero in the open interval $(\delta , 1).$  The same is true for all the derivatives of $a(t)$. This tells us that the $n^\text{th}$ derivative of $a$ has infinitely many distinct zeros for all natural $n$. If $a$ is analytic, then I can show that $a(t)\equiv 0.$ If $a(t)\ge0$ on $[0 , 1]$, then it is obvious that $a\equiv 0.$ I would appreciate any hint. Actually $(1)$ and $(3)$ follows from $(2)$. For $(2)$, suppose that $a(1)\gt0$ (the case that $a(1)\lt0$ can be argued in the same way as the following), then there exists $\delta\lt1$ such that $a(t)$ is strictly positive on $I = [1-\delta , 1]$. Then write $F(\lambda) = \lambda\int_0^{1-\delta}e^{\lambda t}a(t)dt + \lambda\int_{1-\delta}^1e^{\lambda t}a(t)dt$. Now there exists $c\gt0$ such that $a(t)>c$ on $I$ since $I$ is compact, so that the second term is bounded below by $c (e^{\lambda}-e^{\lambda(1-\delta)})$. But the first term is only $O(e^{\lambda(1-\delta)})$, so this contradicts the fact that $F(\lambda)$ is bounded. Morever, using the same idea and integration by parts, one can see that $n^{th}$ derivative of $a$ must vanish at $1$.",,"['real-analysis', 'integration']"
35,"Computing the integral $ \int_0^{\infty} e^{-\phi^2+\phi}\cdot \phi^{2} \ln(1-2x\cos\phi+x^2)\, d\phi. $",Computing the integral," \int_0^{\infty} e^{-\phi^2+\phi}\cdot \phi^{2} \ln(1-2x\cos\phi+x^2)\, d\phi. ","Integrate $$ \int_0^{\infty} e^{-\phi^2+\phi}\cdot \phi^{2} \ln(1-2x\cos\phi+x^2) \, d\phi. $$ Something that may help $(1-2x\cos\phi+x^2)=(1-xe^{i\phi})(1-xe^{-i\phi})$.  And using the series expansion $$ \ln(1-z)=-\sum_{n=1}^\infty \frac{z^n}{n} $$ where $|z=xe^{\pm i\phi}| \leq 1$.  The series is absolutely and uniformly convergent. Any method is okay.","Integrate $$ \int_0^{\infty} e^{-\phi^2+\phi}\cdot \phi^{2} \ln(1-2x\cos\phi+x^2) \, d\phi. $$ Something that may help $(1-2x\cos\phi+x^2)=(1-xe^{i\phi})(1-xe^{-i\phi})$.  And using the series expansion $$ \ln(1-z)=-\sum_{n=1}^\infty \frac{z^n}{n} $$ where $|z=xe^{\pm i\phi}| \leq 1$.  The series is absolutely and uniformly convergent. Any method is okay.",,"['real-analysis', 'integration', 'definite-integrals', 'contest-math', 'contour-integration']"
36,My proof of the First Fundamental Theorem of Calculus,My proof of the First Fundamental Theorem of Calculus,,"I've tried to prove the theorem in advance at the level that satisfies me. The notation used might not be correct, but I hope all major steps are correct. DEFINITION (from Apostol's Calculus I ): Let $f$ be a function that is integrable on $[a,x]$, for each $x$ in $[a,b]$. Let $c$ be such that $a \le c \le b$ and define a new function $A$ as follows: $$A(x) = \int_c^xf(t)dt, a \le x \le b$$ Then the derivative $A'(x)$ exists at each point $x$ in the open interval $(a,b)$ where $f$ is continuous, and for such $x$ we have $$A'(x) = f(x).$$ PROOF: I will show that the theorem is a consequence of the property of continuity of the function at some point. First we make some derivations. For simplicity lets assume that $h$ is positive. The proof for negative $h$ is no harder. $$A'(x) = \lim_{h\to0} \frac{A(x+h) - A(x)}{h} =  \lim_{h\to0}\frac{\int_c^{x+h}f(t)dt - \int_c^{x}f(t)dt}{h} = \lim_{h\to0}\frac{\int_x^{x+h}f(t)dt}{h}$$ Since $f$ is continuous at $x$, for any number $\epsilon$ there is some $\delta$, such that when $t \in [x-\delta, x+\delta]$, $$f(x) - \epsilon \le f(t) \le f(x) + \epsilon$$ Then for any $h < \delta$, $$\int_x^{x+h} [f(x) - \epsilon]dt \le \int_x^{x+h} f(t)dt \le \int_x^{x+h} [f(x) + \epsilon]dt$$ $$[f(x) - \epsilon]h \le \int_x^{x+h} f(t)dt \le [f(x) + \epsilon]h$$ $$f(x) - \epsilon \le \frac {\int_x^{x+h} f(t)dt}{h} \le f(x) + \epsilon$$ I can make $\epsilon$ as small as possible, hence $$\lim_{h\to0}\frac{\int_x^{x+h}f(t)dt}{h} = A'(x) = f(x)$$","I've tried to prove the theorem in advance at the level that satisfies me. The notation used might not be correct, but I hope all major steps are correct. DEFINITION (from Apostol's Calculus I ): Let $f$ be a function that is integrable on $[a,x]$, for each $x$ in $[a,b]$. Let $c$ be such that $a \le c \le b$ and define a new function $A$ as follows: $$A(x) = \int_c^xf(t)dt, a \le x \le b$$ Then the derivative $A'(x)$ exists at each point $x$ in the open interval $(a,b)$ where $f$ is continuous, and for such $x$ we have $$A'(x) = f(x).$$ PROOF: I will show that the theorem is a consequence of the property of continuity of the function at some point. First we make some derivations. For simplicity lets assume that $h$ is positive. The proof for negative $h$ is no harder. $$A'(x) = \lim_{h\to0} \frac{A(x+h) - A(x)}{h} =  \lim_{h\to0}\frac{\int_c^{x+h}f(t)dt - \int_c^{x}f(t)dt}{h} = \lim_{h\to0}\frac{\int_x^{x+h}f(t)dt}{h}$$ Since $f$ is continuous at $x$, for any number $\epsilon$ there is some $\delta$, such that when $t \in [x-\delta, x+\delta]$, $$f(x) - \epsilon \le f(t) \le f(x) + \epsilon$$ Then for any $h < \delta$, $$\int_x^{x+h} [f(x) - \epsilon]dt \le \int_x^{x+h} f(t)dt \le \int_x^{x+h} [f(x) + \epsilon]dt$$ $$[f(x) - \epsilon]h \le \int_x^{x+h} f(t)dt \le [f(x) + \epsilon]h$$ $$f(x) - \epsilon \le \frac {\int_x^{x+h} f(t)dt}{h} \le f(x) + \epsilon$$ I can make $\epsilon$ as small as possible, hence $$\lim_{h\to0}\frac{\int_x^{x+h}f(t)dt}{h} = A'(x) = f(x)$$",,"['calculus', 'real-analysis', 'proof-verification']"
37,Sequence of convex functions converges uniformly,Sequence of convex functions converges uniformly,,"I am working on the following problem. Let $f_{n}: [a, b] \rightarrow \mathbb{R}$ be a sequence of convex functions. Furthermore, for each fixed $x \in [a, b]$, suppose   $f(x) = \lim_{n \rightarrow \infty}f_{n}(x)$ exists and $f(x)$ is continuous on $[a, b]$. Show that $f_{n} \rightarrow f$ uniformly. The solution seems to have been mentioned here and here . However, the solutions don't seem very satisfactory to me, so I decided to write out explicitly my own solution (partially because they start with a proof by contradiction). Can anyone check my solution to make sure it is correct? I've tried to formulate a direct proof. Fix $\varepsilon > 0$. Since $f$ is continuous on $[a, b]$, it is uniformly continuous there, and hence there exists an $\delta > 0$ such that when $|x - y| < \delta$,   $|f(x) - f(y)| <\varepsilon/10$. Partition $[a, b]$ into $\{\alpha_{0}, \alpha_{1}, \ldots, \alpha_{m}\}$ where $\alpha_{0} = a$ and $\alpha_{m} = b$   such that $|\alpha_{i + 1} - \alpha_{i}| = \delta/2$, $i = 0, 1, 2, \ldots, m - 2$ and possibly $|\alpha_{m} - \alpha_{m - 1}| < \delta/2$. Since for each fixed $x$, $f_{n}(x)$ converges to $f(x)$, there exists an integer $N$ such that $|f_{n}(\alpha_{i}) - f(\alpha_{i})| < \varepsilon/10$   for all $i = 0, 1, \ldots, m$ and $n \geq N$. For $x \in [a, b]$, $x \in [\alpha_{i}, \alpha_{i + 1}]$ for some fixed $i$. Then $x = \lambda_{x} \alpha_{i} + (1 - \lambda_{x})\alpha_{i + 1}$ for $\lambda_{x}$ between 0 and 1.   Therefore by convexity of $f_{n}$, for $n \geq N$ (which is independent of $x$), we have   \begin{align*} f_{n}(x) - f(x) &\leq \lambda_{x} (f_{n}(\alpha_{i}) - f(x)) + (1 - \lambda_{x})(f_{n}(\alpha_{i + 1}) - f(x))\\ &\leq |f_{n}(\alpha_{i}) - f(x)| + |f_{n}(\alpha_{i + 1}) - f(x)|\\ &\leq |f_{n}(\alpha_{i}) - f(\alpha_{i})| + |f(\alpha_{i}) - f(x)| + |f_{n}(\alpha_{i + 1}) - f(\alpha_{i + 1})| + |f(\alpha_{i + 1}) - f(x)|\\ & < 2\varepsilon/5 \end{align*}   where the first and third terms in the sum is $< \varepsilon/10$ since $n \geq N$ and the second and fourth terms in the sum is $< \varepsilon/10$ since $x$ is between $\alpha_{i}$ and $\alpha_{i + 1}$ and $|\alpha_{i + 1} - \alpha_{i}| \leq \delta/2$ (and hence apply uniform continuity of $f$). We now show the other direction and consider $f(x) - f_{n}(x)$. Since $|f_{n}(\alpha_{i}) - f(\alpha_{i})| < \varepsilon/10$ for $i = 0, 1, \ldots, m$ and $n \geq N$,   we may assume that $x \neq \alpha_{i}$. Fix $x \in [a, b]$, then $x \in (\alpha_{i}, \alpha_{i + 1})$ for some fixed $i$. Then there exists a $\mu_{x}$   such that $f_{n}(\alpha_{i}) \leq \mu_{x}f_{n}(x) + (1 - \mu_{x})f_{n}(\alpha_{i - 1})$. That is   $$\frac{f_{n}(\alpha_{i}) - (1 - \mu_{x})f_{n}(\alpha_{i - 1})}{\mu_{x}} \leq f_{n}(x).$$   Therefore   \begin{align*} f(x) - f_{n}(x) &\leq f(x) - \frac{1}{\mu_{x}}f_{n}(\alpha_{i}) + \frac{1 - \mu_{x}}{\mu_{x}}f_{n}(\alpha_{i - 1})\\ &= \frac{1}{\mu_{x}}(f(x) - f_{n}(\alpha_{i})) + \left(1 - \frac{1}{\mu_{x}}\right)(f(x) - f_{n}(\alpha_{i - 1}))\\ &\leq \frac{1}{\mu_{x}}|f(x) - f_{n}(\alpha_{i})| + \left|1 - \frac{1}{\mu_{x}}\right||f(x) - f_{n}(\alpha_{i - 1})|\\ &\leq \frac{1}{\mu_{x}}\frac{\varepsilon}{5} + \left|1 - \frac{1}{\mu_{x}}\right|\frac{\varepsilon}{5} \end{align*}   where the last inequality is for $n \geq N$ and is by the same reasoning as in the $f_{n}(x) - f(x) < 2\varepsilon/5$ case. Since $x \in (\alpha_{i}, \alpha_{i + 1})$,   \begin{align*} \frac{1}{\mu_{x}} = \frac{x - \alpha_{i - 1}}{\alpha_{i} - \alpha_{i - 1}} > 1 \end{align*}   and   \begin{align*} \left|\frac{1}{\mu_{x}}\right| \leq \left|\frac{\alpha_{i + 1} - \alpha_{i - 1}}{\alpha_{i} - \alpha_{i - 1}}\right| \leq \frac{|\alpha_{i + 1} - \alpha_{i}| + |\alpha_{i} - \alpha_{i - 1}|}{\alpha_{i} - \alpha_{i - 1}} = 1 + \left|\frac{\alpha_{i + 1} - \alpha_{i}}{\alpha_{i} - \alpha_{i - 1}}\right| < 2. \end{align*}   Therefore, we have   \begin{align*} \frac{1}{\mu_{x}}\frac{\varepsilon}{5} + \left|1 - \frac{1}{\mu_{x}}\right|\frac{\varepsilon}{5} = \left(\frac{2}{\mu_{x}} - 1\right)\frac{\varepsilon}{5} \leq \frac{3\varepsilon}{5}. \end{align*}   That is, we have found an $N$ (independent of $x$) such that for $n \geq N$,   $f(x) - f_{n}(x) \leq 3\varepsilon/5$. Therefore $f_{n} \rightarrow f$ uniformly.","I am working on the following problem. Let $f_{n}: [a, b] \rightarrow \mathbb{R}$ be a sequence of convex functions. Furthermore, for each fixed $x \in [a, b]$, suppose   $f(x) = \lim_{n \rightarrow \infty}f_{n}(x)$ exists and $f(x)$ is continuous on $[a, b]$. Show that $f_{n} \rightarrow f$ uniformly. The solution seems to have been mentioned here and here . However, the solutions don't seem very satisfactory to me, so I decided to write out explicitly my own solution (partially because they start with a proof by contradiction). Can anyone check my solution to make sure it is correct? I've tried to formulate a direct proof. Fix $\varepsilon > 0$. Since $f$ is continuous on $[a, b]$, it is uniformly continuous there, and hence there exists an $\delta > 0$ such that when $|x - y| < \delta$,   $|f(x) - f(y)| <\varepsilon/10$. Partition $[a, b]$ into $\{\alpha_{0}, \alpha_{1}, \ldots, \alpha_{m}\}$ where $\alpha_{0} = a$ and $\alpha_{m} = b$   such that $|\alpha_{i + 1} - \alpha_{i}| = \delta/2$, $i = 0, 1, 2, \ldots, m - 2$ and possibly $|\alpha_{m} - \alpha_{m - 1}| < \delta/2$. Since for each fixed $x$, $f_{n}(x)$ converges to $f(x)$, there exists an integer $N$ such that $|f_{n}(\alpha_{i}) - f(\alpha_{i})| < \varepsilon/10$   for all $i = 0, 1, \ldots, m$ and $n \geq N$. For $x \in [a, b]$, $x \in [\alpha_{i}, \alpha_{i + 1}]$ for some fixed $i$. Then $x = \lambda_{x} \alpha_{i} + (1 - \lambda_{x})\alpha_{i + 1}$ for $\lambda_{x}$ between 0 and 1.   Therefore by convexity of $f_{n}$, for $n \geq N$ (which is independent of $x$), we have   \begin{align*} f_{n}(x) - f(x) &\leq \lambda_{x} (f_{n}(\alpha_{i}) - f(x)) + (1 - \lambda_{x})(f_{n}(\alpha_{i + 1}) - f(x))\\ &\leq |f_{n}(\alpha_{i}) - f(x)| + |f_{n}(\alpha_{i + 1}) - f(x)|\\ &\leq |f_{n}(\alpha_{i}) - f(\alpha_{i})| + |f(\alpha_{i}) - f(x)| + |f_{n}(\alpha_{i + 1}) - f(\alpha_{i + 1})| + |f(\alpha_{i + 1}) - f(x)|\\ & < 2\varepsilon/5 \end{align*}   where the first and third terms in the sum is $< \varepsilon/10$ since $n \geq N$ and the second and fourth terms in the sum is $< \varepsilon/10$ since $x$ is between $\alpha_{i}$ and $\alpha_{i + 1}$ and $|\alpha_{i + 1} - \alpha_{i}| \leq \delta/2$ (and hence apply uniform continuity of $f$). We now show the other direction and consider $f(x) - f_{n}(x)$. Since $|f_{n}(\alpha_{i}) - f(\alpha_{i})| < \varepsilon/10$ for $i = 0, 1, \ldots, m$ and $n \geq N$,   we may assume that $x \neq \alpha_{i}$. Fix $x \in [a, b]$, then $x \in (\alpha_{i}, \alpha_{i + 1})$ for some fixed $i$. Then there exists a $\mu_{x}$   such that $f_{n}(\alpha_{i}) \leq \mu_{x}f_{n}(x) + (1 - \mu_{x})f_{n}(\alpha_{i - 1})$. That is   $$\frac{f_{n}(\alpha_{i}) - (1 - \mu_{x})f_{n}(\alpha_{i - 1})}{\mu_{x}} \leq f_{n}(x).$$   Therefore   \begin{align*} f(x) - f_{n}(x) &\leq f(x) - \frac{1}{\mu_{x}}f_{n}(\alpha_{i}) + \frac{1 - \mu_{x}}{\mu_{x}}f_{n}(\alpha_{i - 1})\\ &= \frac{1}{\mu_{x}}(f(x) - f_{n}(\alpha_{i})) + \left(1 - \frac{1}{\mu_{x}}\right)(f(x) - f_{n}(\alpha_{i - 1}))\\ &\leq \frac{1}{\mu_{x}}|f(x) - f_{n}(\alpha_{i})| + \left|1 - \frac{1}{\mu_{x}}\right||f(x) - f_{n}(\alpha_{i - 1})|\\ &\leq \frac{1}{\mu_{x}}\frac{\varepsilon}{5} + \left|1 - \frac{1}{\mu_{x}}\right|\frac{\varepsilon}{5} \end{align*}   where the last inequality is for $n \geq N$ and is by the same reasoning as in the $f_{n}(x) - f(x) < 2\varepsilon/5$ case. Since $x \in (\alpha_{i}, \alpha_{i + 1})$,   \begin{align*} \frac{1}{\mu_{x}} = \frac{x - \alpha_{i - 1}}{\alpha_{i} - \alpha_{i - 1}} > 1 \end{align*}   and   \begin{align*} \left|\frac{1}{\mu_{x}}\right| \leq \left|\frac{\alpha_{i + 1} - \alpha_{i - 1}}{\alpha_{i} - \alpha_{i - 1}}\right| \leq \frac{|\alpha_{i + 1} - \alpha_{i}| + |\alpha_{i} - \alpha_{i - 1}|}{\alpha_{i} - \alpha_{i - 1}} = 1 + \left|\frac{\alpha_{i + 1} - \alpha_{i}}{\alpha_{i} - \alpha_{i - 1}}\right| < 2. \end{align*}   Therefore, we have   \begin{align*} \frac{1}{\mu_{x}}\frac{\varepsilon}{5} + \left|1 - \frac{1}{\mu_{x}}\right|\frac{\varepsilon}{5} = \left(\frac{2}{\mu_{x}} - 1\right)\frac{\varepsilon}{5} \leq \frac{3\varepsilon}{5}. \end{align*}   That is, we have found an $N$ (independent of $x$) such that for $n \geq N$,   $f(x) - f_{n}(x) \leq 3\varepsilon/5$. Therefore $f_{n} \rightarrow f$ uniformly.",,"['real-analysis', 'analysis', 'proof-verification']"
38,How to calculate $f(x)$ in $f(f(x)) = e^x$?,How to calculate  in ?,f(x) f(f(x)) = e^x,How would I calculate the power series of $f(x)$ if $f(f(x)) = e^x$?  Is there a faster-converging method than power series for fractional iteration/functional square roots?,How would I calculate the power series of $f(x)$ if $f(f(x)) = e^x$?  Is there a faster-converging method than power series for fractional iteration/functional square roots?,,"['power-series', 'functional-equations', 'tetration']"
39,An example of a bounded countably infinite subset of the real numbers.,An example of a bounded countably infinite subset of the real numbers.,,"I've been trying to think of an example of bounded, countably infinite subset of the real numbers.  However, knowing that countably infinite means can be put into 1-1 correspondence with the naturals, this doesn't seem intuitively obvious. Thanks in advance.","I've been trying to think of an example of bounded, countably infinite subset of the real numbers.  However, knowing that countably infinite means can be put into 1-1 correspondence with the naturals, this doesn't seem intuitively obvious. Thanks in advance.",,['real-analysis']
40,"Is the function $ f(x,y)=\frac{xy}{x^{2}+y^{2}}$ where $f(0,0)$ is defined to be $0$ continuous?",Is the function  where  is defined to be  continuous?," f(x,y)=\frac{xy}{x^{2}+y^{2}} f(0,0) 0","Is the function $ f(x,y)=\frac{xy}{x^{2}+y^{2}}$ where $f(0,0)$ is defined to be $0$ continuous? I don't think it is and I am trying to either show this by the definition or by showing that maybe a close set in $\mathbb{R}$ has an inverse set that is not closed in $ \mathbb{R} ^{2}$ . I tried the point $0$ but this is open in $\mathbb{R}$ . Any hints or ideas? Thanks!",Is the function where is defined to be continuous? I don't think it is and I am trying to either show this by the definition or by showing that maybe a close set in has an inverse set that is not closed in . I tried the point but this is open in . Any hints or ideas? Thanks!," f(x,y)=\frac{xy}{x^{2}+y^{2}} f(0,0) 0 \mathbb{R}  \mathbb{R} ^{2} 0 \mathbb{R}","['real-analysis', 'calculus', 'general-topology']"
41,Is diameter of a set a measure?,Is diameter of a set a measure?,,"Suppose the diameter of a nonempty set $A$ is defined as $$\sigma(A) := \sup_{x,y \in A} d(x,y)$$ where $d(x,y)$ is a metric. Is $\sigma(.)$ a 'measurement'? I.e., how do I prove the countable additivity for this particular case?","Suppose the diameter of a nonempty set $A$ is defined as $$\sigma(A) := \sup_{x,y \in A} d(x,y)$$ where $d(x,y)$ is a metric. Is $\sigma(.)$ a 'measurement'? I.e., how do I prove the countable additivity for this particular case?",,"['real-analysis', 'functional-analysis', 'measure-theory', 'metric-spaces', 'lebesgue-measure']"
42,How to find $\lim\limits_{x\to0}\frac{e^x-1-x}{x^2}$ without using l'Hopital's rule nor any series expansion?,How to find  without using l'Hopital's rule nor any series expansion?,\lim\limits_{x\to0}\frac{e^x-1-x}{x^2},"Is it possible to determine the limit $$\lim_{x\to0}\frac{e^x-1-x}{x^2}$$ without using l'Hopital's rule nor any series expansion? For example, suppose you are a student that has not studied derivative yet (and so not even Taylor formula and Taylor series).","Is it possible to determine the limit $$\lim_{x\to0}\frac{e^x-1-x}{x^2}$$ without using l'Hopital's rule nor any series expansion? For example, suppose you are a student that has not studied derivative yet (and so not even Taylor formula and Taylor series).",,"['calculus', 'real-analysis', 'limits', 'exponential-function', 'limits-without-lhopital']"
43,"If $g\geq2$ is an integer, then $\sum\limits_{n=0}^{\infty} \frac{1}{g^{n^{2}}} $ and $ \sum\limits_{n=0}^{\infty} \frac{1}{g^{n!}}$ are irrational","If  is an integer, then  and  are irrational",g\geq2 \sum\limits_{n=0}^{\infty} \frac{1}{g^{n^{2}}}   \sum\limits_{n=0}^{\infty} \frac{1}{g^{n!}},"How do we show that if $g \geq 2$ is an integer, then the two series $$\sum\limits_{n=0}^{\infty} \frac{1}{g^{n^{2}}} \quad  \ \text{and} \ \sum\limits_{n=0}^{\infty} \frac{1}{g^{n!}}$$ both converge to irrational numbers. Well, i tried to see what happens, if they converge to a rational but couldn't get anything out it.","How do we show that if $g \geq 2$ is an integer, then the two series $$\sum\limits_{n=0}^{\infty} \frac{1}{g^{n^{2}}} \quad  \ \text{and} \ \sum\limits_{n=0}^{\infty} \frac{1}{g^{n!}}$$ both converge to irrational numbers. Well, i tried to see what happens, if they converge to a rational but couldn't get anything out it.",,['real-analysis']
44,How would I prove that this function is affine if $f(x+h)-f(x)=hf'(x)$?,How would I prove that this function is affine if ?,f(x+h)-f(x)=hf'(x),"Let $f$ be a differentiable function such that for every $x$ and $h$ it holds that $f(x+h)-f(x)=hf'(x)$ . Prove that $f(x)=kx+n$ where $k$ and $n$ are constants. I get it why this is true, and I tried to prove it somehow, but I can't seem to prove it rigorously with analysis. Any ideas?","Let be a differentiable function such that for every and it holds that . Prove that where and are constants. I get it why this is true, and I tried to prove it somehow, but I can't seem to prove it rigorously with analysis. Any ideas?",f x h f(x+h)-f(x)=hf'(x) f(x)=kx+n k n,['real-analysis']
45,How to prove that $e^x=\sum_{k=0}^\infty \frac{x^k}{k!}$?,How to prove that ?,e^x=\sum_{k=0}^\infty \frac{x^k}{k!},"How to prove that $e^x=\sum_{k=0}^\infty \frac{x^k}{k!}$ using the fact that $e^x=\lim_{n\to\infty }\left(1+\frac{x}{n}\right)^n$ ? So, $$e^x=\lim_{n\to\infty }\sum_{k=0}^n \binom{n}{k}\left(\frac{x}{n}\right)^k=\lim_{n\to\infty }\sum_{k=0}^n\frac{n!}{(n-k)!n^k}\frac{x^k}{k!}.$$ I think that I have to prove that $\frac{n!}{(n-k)!n^k}=1$ but I didn't have success.","How to prove that using the fact that ? So, I think that I have to prove that but I didn't have success.",e^x=\sum_{k=0}^\infty \frac{x^k}{k!} e^x=\lim_{n\to\infty }\left(1+\frac{x}{n}\right)^n e^x=\lim_{n\to\infty }\sum_{k=0}^n \binom{n}{k}\left(\frac{x}{n}\right)^k=\lim_{n\to\infty }\sum_{k=0}^n\frac{n!}{(n-k)!n^k}\frac{x^k}{k!}. \frac{n!}{(n-k)!n^k}=1,"['real-analysis', 'limits', 'exponential-function']"
46,"is $T:C[0,1]\rightarrow C[0,1]$: $x(t)\mapsto x(t^2)$ compact?",is :  compact?,"T:C[0,1]\rightarrow C[0,1] x(t)\mapsto x(t^2)","Is $T$ a compact operator? $T:C[0,1]\rightarrow C[0,1]$: $x(t)\mapsto x(t^2)$ where $t\in[0,1]$ with supremum norm.","Is $T$ a compact operator? $T:C[0,1]\rightarrow C[0,1]$: $x(t)\mapsto x(t^2)$ where $t\in[0,1]$ with supremum norm.",,"['real-analysis', 'functional-analysis', 'operator-theory', 'compact-operators']"
47,Find the limit without use of L'Hôpital or Taylor series: $\lim \limits_{x\rightarrow 0} \left(\frac{1}{x^2}-\frac{1}{\sin^2 x}\right)$,Find the limit without use of L'Hôpital or Taylor series:,\lim \limits_{x\rightarrow 0} \left(\frac{1}{x^2}-\frac{1}{\sin^2 x}\right),Find the limit without the use of L'Hôpital's rule or Taylor series $$\lim \limits_{x\rightarrow 0} \left(\frac{1}{x^2}-\frac{1}{\sin^2x}\right)$$,Find the limit without the use of L'Hôpital's rule or Taylor series $$\lim \limits_{x\rightarrow 0} \left(\frac{1}{x^2}-\frac{1}{\sin^2x}\right)$$,,"['calculus', 'real-analysis', 'limits']"
48,Infinite Fibonacci sums $\sum_{n=1}^{\infty} \frac{1}{f_nf_{n+2}}$ - diverge or converge,Infinite Fibonacci sums  - diverge or converge,\sum_{n=1}^{\infty} \frac{1}{f_nf_{n+2}},"I am currently going through exercises regarding convergence/divergence. For my previous question I used the ratio test, and managed to get through it all okay (I think). I proved that: $$\sum_{n=1}^{\infty} \frac{n!}{n^n}$$  converges, and now I have to show whether or not an inverse Fibonacci sum converges/diverges and I'm not sure what method to use. What is the best way to tackle this problem? $$\sum_{n=1}^{\infty} \frac{1}{f_nf_{n+2}}$$  Where $f_n$ is the Fibonacci sequence, $f_n = f_{n-1} + f_{n-2}$ with initial terms $f_1 = f_2 = 1$ I don't believe it's similar to how I completed $\sum_{n=1}^{\infty} \frac{n!}{n^n}$ but let me know if I'm wrong. Based on looking at fairly similar questions on this website I have started trying to use proof by contradiction.","I am currently going through exercises regarding convergence/divergence. For my previous question I used the ratio test, and managed to get through it all okay (I think). I proved that: $$\sum_{n=1}^{\infty} \frac{n!}{n^n}$$  converges, and now I have to show whether or not an inverse Fibonacci sum converges/diverges and I'm not sure what method to use. What is the best way to tackle this problem? $$\sum_{n=1}^{\infty} \frac{1}{f_nf_{n+2}}$$  Where $f_n$ is the Fibonacci sequence, $f_n = f_{n-1} + f_{n-2}$ with initial terms $f_1 = f_2 = 1$ I don't believe it's similar to how I completed $\sum_{n=1}^{\infty} \frac{n!}{n^n}$ but let me know if I'm wrong. Based on looking at fairly similar questions on this website I have started trying to use proof by contradiction.",,"['real-analysis', 'sequences-and-series', 'fibonacci-numbers']"
49,product rule for matrix functions?,product rule for matrix functions?,,"Given a real rectangular matrix $X$, and two scalar-valued matrix functions, $f(X)$ and $g(X)$, does the product rule for differentiation of a product of scalar valued functions, hold when differentiating the product, $f(X)g(X)$ w.r.t $X$ ? If not, what would be the corresponding product rule? Let's assume that the product $f(X)g(X)$ gives a real valued scalar, and is well-defined in terms of the dimensions. Note: $f(.)$ and $g(.)$ can be the matrix trace function for example.","Given a real rectangular matrix $X$, and two scalar-valued matrix functions, $f(X)$ and $g(X)$, does the product rule for differentiation of a product of scalar valued functions, hold when differentiating the product, $f(X)g(X)$ w.r.t $X$ ? If not, what would be the corresponding product rule? Let's assume that the product $f(X)g(X)$ gives a real valued scalar, and is well-defined in terms of the dimensions. Note: $f(.)$ and $g(.)$ can be the matrix trace function for example.",,"['calculus', 'real-analysis', 'analysis', 'functions', 'multivariable-calculus']"
50,"Is $[0,1] \cap \Bbb Q$ a compact subset of $\Bbb Q$?",Is  a compact subset of ?,"[0,1] \cap \Bbb Q \Bbb Q","Is $[0,1] \cap \Bbb Q$ a compact subset of $\Bbb Q$?  I get the feeling it isn't compact, but I can't figure out a way to prove this.  I understand that,  in $\Bbb Q$, any open set $U = \Bbb Q \cap O_i$ such that $O_i \subseteq R$.  But I can't figure out a way to construct a finite set that doesn't contain the intersection set.","Is $[0,1] \cap \Bbb Q$ a compact subset of $\Bbb Q$?  I get the feeling it isn't compact, but I can't figure out a way to prove this.  I understand that,  in $\Bbb Q$, any open set $U = \Bbb Q \cap O_i$ such that $O_i \subseteq R$.  But I can't figure out a way to construct a finite set that doesn't contain the intersection set.",,"['real-analysis', 'general-topology', 'compactness']"
51,Continuity vs. Mapping open sets to open sets?,Continuity vs. Mapping open sets to open sets?,,"I have a question and I have no idea how to solve this: One problem in my Real Analysis text book says: Show that if $\ell$ is a nonzero linear functional on a normed vector space not necessarily continuous, then $\ell$ maps open sets into open sets. Isn't it against the definition? I mean saying that $f$ is continuous is equivalent to say that it maps open sets into open sets, right? Is there any mistake in this problem? Or is just me that I'm misreading something? Thank you for your help!!","I have a question and I have no idea how to solve this: One problem in my Real Analysis text book says: Show that if $\ell$ is a nonzero linear functional on a normed vector space not necessarily continuous, then $\ell$ maps open sets into open sets. Isn't it against the definition? I mean saying that $f$ is continuous is equivalent to say that it maps open sets into open sets, right? Is there any mistake in this problem? Or is just me that I'm misreading something? Thank you for your help!!",,"['real-analysis', 'general-topology', 'measure-theory', 'continuity', 'uniform-continuity']"
52,Does the multiplication of countably infinite many numbers between $0$ and $1$ equal $0$?,Does the multiplication of countably infinite many numbers between  and  equal ?,0 1 0,"Suppose every term of a countably infinite sequence $x_1,x_2,\dots$ is between $0$ and $1$, i.e. $0<x_i<1$ for every $i$. Does $\mathop {\lim }\limits_{n \to \infty } \prod\limits_{i = 1}^n {{x_i}} $ equal $0$ or there is counterexample that it might not be $0$? Thank you!","Suppose every term of a countably infinite sequence $x_1,x_2,\dots$ is between $0$ and $1$, i.e. $0<x_i<1$ for every $i$. Does $\mathop {\lim }\limits_{n \to \infty } \prod\limits_{i = 1}^n {{x_i}} $ equal $0$ or there is counterexample that it might not be $0$? Thank you!",,"['real-analysis', 'infinite-product']"
53,Open sets do not have measure zero,Open sets do not have measure zero,,"I want to show that if $a\lt b$, then $(a,b)$ is not of measure zero. My idea was to show that any interval covering $(a,b)$ is such that the sum of the lengths of the intervals is always greater than $b-a$. So I tried induction. Suppose $n=1$, where $n$ is the number of intervals covering $(a,b)$. Then the interval consists of a single set of the form, say,  $(c,d)$ such that $(a,b)\subset (c,d)$. If this is the case then obviously, we would have $d-c \gt b-a$, where I have assumed that $c\lt a$ and $b \lt d$. Now my questions: I am a little bit lost as to how to show it for $n+1$. Am I even approaching it correctly? Thanks for your help.","I want to show that if $a\lt b$, then $(a,b)$ is not of measure zero. My idea was to show that any interval covering $(a,b)$ is such that the sum of the lengths of the intervals is always greater than $b-a$. So I tried induction. Suppose $n=1$, where $n$ is the number of intervals covering $(a,b)$. Then the interval consists of a single set of the form, say,  $(c,d)$ such that $(a,b)\subset (c,d)$. If this is the case then obviously, we would have $d-c \gt b-a$, where I have assumed that $c\lt a$ and $b \lt d$. Now my questions: I am a little bit lost as to how to show it for $n+1$. Am I even approaching it correctly? Thanks for your help.",,"['real-analysis', 'measure-theory']"
54,$\lim_{x\to0} \frac{x-\sin x}{x-\tan x}$ without using L'Hopital,without using L'Hopital,\lim_{x\to0} \frac{x-\sin x}{x-\tan x},$$\lim_{x\to0} \frac{x-\sin x}{x-\tan x}=?$$ I tried using $\lim\limits_{x\to0} \frac{\sin x}x=1$ . But it doesn't work :/,I tried using . But it doesn't work :/,\lim_{x\to0} \frac{x-\sin x}{x-\tan x}=? \lim\limits_{x\to0} \frac{\sin x}x=1,"['real-analysis', 'limits', 'trigonometry', 'limits-without-lhopital']"
55,Example of a metric space where Heine-Borel theorem does not hold,Example of a metric space where Heine-Borel theorem does not hold,,"In Rudin, it says the Heine-Borel theorem holds for Euclidean metric spaces. What is an example of a metric space where Heine-Borel does not hold true?","In Rudin, it says the Heine-Borel theorem holds for Euclidean metric spaces. What is an example of a metric space where Heine-Borel does not hold true?",,"['real-analysis', 'metric-spaces', 'compactness']"
56,Is the series $\sqrt{1} - \sqrt{2} + \sqrt{3} - \sqrt{4} + \dots$ summable?,Is the series  summable?,\sqrt{1} - \sqrt{2} + \sqrt{3} - \sqrt{4} + \dots,"Is the series $\sqrt{1} - \sqrt{2} + \sqrt{3} - \sqrt{4} + \dots$ summable?  I think it diverges although: $$ \sqrt{n+1} - \sqrt{n} \approx \frac{1}{2\sqrt{n}}$$ for example by the Mean Value Theorem $f(x+1)-f(x) \approx f'(x)$ and then I might argue: $$ \sum_{n \geq 1} (-1)^{n+1} \sqrt{n} = \frac{1}{2}\sum_{m \geq 1} \frac{1}{\sqrt{2m}} = \infty $$ Are these Cesaro summable ? For an even number of terms: $$\sqrt{1} - \sqrt{2} + \sqrt{3} - \sqrt{4} + \dots - \sqrt{2n} \approx - \frac{1}{2\sqrt{2}}\left( \frac{1}{\sqrt{1}} +   \frac{1}{\sqrt{2}} + \dots +  \frac{1}{\sqrt{n}} \right) \approx  \sqrt{\frac{n}{2}}$$ so the Cesaro means tend to infinity.  Does any more creative summation method work? The result is from paper called ""The Second Theorem of Consistency for Summable Series"" in Vol 6 of the Collected Works of GH Hardy the series $1 - 1 +1 - 1 \dots$ is summable $(1,k)$ for any $k$ but not summable $(e^n, k)$ for any value of $k$. The series  $\sqrt{1} - \sqrt{2} + \sqrt{3} - \sqrt{4} + \dots$ is summable $(n,1)$ but not $(e^{\sqrt{n}},1)$ and so on... Here things like $(1,k), (n,1)$ refer to certain averaging procedures, IDK","Is the series $\sqrt{1} - \sqrt{2} + \sqrt{3} - \sqrt{4} + \dots$ summable?  I think it diverges although: $$ \sqrt{n+1} - \sqrt{n} \approx \frac{1}{2\sqrt{n}}$$ for example by the Mean Value Theorem $f(x+1)-f(x) \approx f'(x)$ and then I might argue: $$ \sum_{n \geq 1} (-1)^{n+1} \sqrt{n} = \frac{1}{2}\sum_{m \geq 1} \frac{1}{\sqrt{2m}} = \infty $$ Are these Cesaro summable ? For an even number of terms: $$\sqrt{1} - \sqrt{2} + \sqrt{3} - \sqrt{4} + \dots - \sqrt{2n} \approx - \frac{1}{2\sqrt{2}}\left( \frac{1}{\sqrt{1}} +   \frac{1}{\sqrt{2}} + \dots +  \frac{1}{\sqrt{n}} \right) \approx  \sqrt{\frac{n}{2}}$$ so the Cesaro means tend to infinity.  Does any more creative summation method work? The result is from paper called ""The Second Theorem of Consistency for Summable Series"" in Vol 6 of the Collected Works of GH Hardy the series $1 - 1 +1 - 1 \dots$ is summable $(1,k)$ for any $k$ but not summable $(e^n, k)$ for any value of $k$. The series  $\sqrt{1} - \sqrt{2} + \sqrt{3} - \sqrt{4} + \dots$ is summable $(n,1)$ but not $(e^{\sqrt{n}},1)$ and so on... Here things like $(1,k), (n,1)$ refer to certain averaging procedures, IDK",,"['real-analysis', 'sequences-and-series', 'radicals', 'divergent-series']"
57,"Is there a way to prove this exponential inequality: if $a>b$ then $a^a>b^b$ for $a,b>1$?",Is there a way to prove this exponential inequality: if  then  for ?,"a>b a^a>b^b a,b>1","I came across this proposition while trying to prove that a function was injective: if $a>b$ then $a^a>b^b$, where $a$ and $b$ are real numbers bigger than $1$. Intuitively it (somehow) makes sense but I wonder if a rigorous proof can be made. But, the initial problem I was trying to solve was to show that $f(x)=x^x$, where $x$ is just a positive real number, is injective . As the ""contrapositive method"" from the definition of an injective function didn't work out, I figured I could just show that my function was strictly increasing or decreasing, therefore the function would be injective. I looked at the graph of this function and I noticed I have a turning point at $x=1/e$ (as the user MXYMXY pointed out). Thus I had two cases for my function.","I came across this proposition while trying to prove that a function was injective: if $a>b$ then $a^a>b^b$, where $a$ and $b$ are real numbers bigger than $1$. Intuitively it (somehow) makes sense but I wonder if a rigorous proof can be made. But, the initial problem I was trying to solve was to show that $f(x)=x^x$, where $x$ is just a positive real number, is injective . As the ""contrapositive method"" from the definition of an injective function didn't work out, I figured I could just show that my function was strictly increasing or decreasing, therefore the function would be injective. I looked at the graph of this function and I noticed I have a turning point at $x=1/e$ (as the user MXYMXY pointed out). Thus I had two cases for my function.",,"['real-analysis', 'algebra-precalculus', 'functions', 'inequality', 'exponential-function']"
58,Image of open set is not open?,Image of open set is not open?,,"I'm confused by the proof that $\epsilon$-$\delta$ continuity is equivalent to open-set continuity. One can prove that a function is $\epsilon$-$\delta$-continuous if and only if the preimage of any open set is open. However, it is not true that the image of any open set is open. My question is, what is the difference between image and preimage? The duality of image and preimage suggests to me that a proof that works for one of them should work for the other, but clearly this is not the case. Why not?","I'm confused by the proof that $\epsilon$-$\delta$ continuity is equivalent to open-set continuity. One can prove that a function is $\epsilon$-$\delta$-continuous if and only if the preimage of any open set is open. However, it is not true that the image of any open set is open. My question is, what is the difference between image and preimage? The duality of image and preimage suggests to me that a proof that works for one of them should work for the other, but clearly this is not the case. Why not?",,"['real-analysis', 'analysis']"
59,Must all Lebesgue integrable functions really be invertible?,Must all Lebesgue integrable functions really be invertible?,,"I am studying Lebesgue integration after a course on Riemann integration, and the definition of measurable function is given as follows: $f:{\mathbb R}\rightarrow {\mathbb R}$ is measurable if the pre-image $f^{-1}((-\infty, a))$ is measurable for each $a\in {\mathbb R}.$ So this necessarily implies that a function would not be Lebesgue integrable if it does not have an inverse, since it would not be measurable. But I don't recall this restriction for Riemann integrals. Or was it just implicitly assumed that every Riemann integrable function is invertible?","I am studying Lebesgue integration after a course on Riemann integration, and the definition of measurable function is given as follows: $f:{\mathbb R}\rightarrow {\mathbb R}$ is measurable if the pre-image $f^{-1}((-\infty, a))$ is measurable for each $a\in {\mathbb R}.$ So this necessarily implies that a function would not be Lebesgue integrable if it does not have an inverse, since it would not be measurable. But I don't recall this restriction for Riemann integrals. Or was it just implicitly assumed that every Riemann integrable function is invertible?",,"['real-analysis', 'measure-theory', 'functions', 'notation', 'inverse']"
60,"If the product of two continuous functions is zero, must one of the functions be zero? [duplicate]","If the product of two continuous functions is zero, must one of the functions be zero? [duplicate]",,"This question already has answers here : product of two non zero continuous function is zero [closed] (3 answers) Closed 5 years ago . Suppose that I have two continuous functions $$f : \left[ a, b \right] \rightarrow \mathbb{R} \quad \text{and} \quad g : \left[ a, b \right] \rightarrow \mathbb{R}$$ and they have the following property $$f(x) \times g(x) = 0 \space , \forall x \space \in \left[ a, b \right]$$ Can I say that one of the functions necessarily has to be equal to $0$? For example, $f(x) = 0 \space , \forall x \space \in \left[ a, b \right]$. UPDATE: Ok, I can see from the counterexamples that the affirmation is not true, but now I cannot see in which cases it is true. If I let the function $g : \left[ a, b \right] \rightarrow \mathbb{R}$ be any continuous function, then in that case must I have $f(x) = 0$ ?","This question already has answers here : product of two non zero continuous function is zero [closed] (3 answers) Closed 5 years ago . Suppose that I have two continuous functions $$f : \left[ a, b \right] \rightarrow \mathbb{R} \quad \text{and} \quad g : \left[ a, b \right] \rightarrow \mathbb{R}$$ and they have the following property $$f(x) \times g(x) = 0 \space , \forall x \space \in \left[ a, b \right]$$ Can I say that one of the functions necessarily has to be equal to $0$? For example, $f(x) = 0 \space , \forall x \space \in \left[ a, b \right]$. UPDATE: Ok, I can see from the counterexamples that the affirmation is not true, but now I cannot see in which cases it is true. If I let the function $g : \left[ a, b \right] \rightarrow \mathbb{R}$ be any continuous function, then in that case must I have $f(x) = 0$ ?",,['real-analysis']
61,Prove that $\lim\limits_{x \to 0} \sinh(x)/x =1$. [closed],Prove that . [closed],\lim\limits_{x \to 0} \sinh(x)/x =1,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Prove that $ \lim\limits_{x \to 0} \frac{\sinh x}{x} =1.$ I am having some trouble proving this without derivative. Some help would be much appreciate!","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Prove that $ \lim\limits_{x \to 0} \frac{\sinh x}{x} =1.$ I am having some trouble proving this without derivative. Some help would be much appreciate!",,"['real-analysis', 'limits', 'hyperbolic-functions']"
62,Show polynomial is a Lipschitz function,Show polynomial is a Lipschitz function,,"If $A\subseteq \mathbb{R}$ is a bounded set and $p$ is a polynomial, then show that $p:A\to \mathbb{R}$ is a Lipschitz function.","If $A\subseteq \mathbb{R}$ is a bounded set and $p$ is a polynomial, then show that $p:A\to \mathbb{R}$ is a Lipschitz function.",,"['real-analysis', 'polynomials', 'holder-spaces']"
63,Integral inequality of length of curve,Integral inequality of length of curve,,"Let $f:\mathbb{R}\to \mathbb{R}$ be a continuously differentiable function. Prove that for any $a.b\in \mathbb{R}$ $$\left (\int_a^b\sqrt{1+(f'(x))^2}\,dx\right)^2\ge (a-b)^2+(f(b)-f(a))^2$$ . source: This is from TIFR GS stage 2. I think mean value theorem kills it but can't do it ...even try Cauchy-Schwarz inequality but nothing conclusion.Geometrically its true since distance  between two point $(a,f(a))$ & $(b,f(b))$ is greater or equal to any arc length of same endpoints.",Let be a continuously differentiable function. Prove that for any . source: This is from TIFR GS stage 2. I think mean value theorem kills it but can't do it ...even try Cauchy-Schwarz inequality but nothing conclusion.Geometrically its true since distance  between two point & is greater or equal to any arc length of same endpoints.,"f:\mathbb{R}\to \mathbb{R} a.b\in \mathbb{R} \left (\int_a^b\sqrt{1+(f'(x))^2}\,dx\right)^2\ge (a-b)^2+(f(b)-f(a))^2 (a,f(a)) (b,f(b))","['real-analysis', 'inequality', 'arc-length']"
64,Inequality for harmonic means,Inequality for harmonic means,,"Prove that for real numbers $a_1 ,a_2 ,...,a_n >0$ the following inequality holds $$\frac{1}{a_1 } +\frac{2}{a_1 +a_2 } +...+\frac{n}{a_1 +a_2 +...+a_n }\leq 4\cdot \left(\frac{1}{a_1} +\frac{1}{a_2 } +...+\frac{1}{a_n} \right).$$","Prove that for real numbers $a_1 ,a_2 ,...,a_n >0$ the following inequality holds $$\frac{1}{a_1 } +\frac{2}{a_1 +a_2 } +...+\frac{n}{a_1 +a_2 +...+a_n }\leq 4\cdot \left(\frac{1}{a_1} +\frac{1}{a_2 } +...+\frac{1}{a_n} \right).$$",,['real-analysis']
65,Compute $\int_0^{\pi/2}\frac{\sin 2013x }{\sin x} \ dx\space$,Compute,\int_0^{\pi/2}\frac{\sin 2013x }{\sin x} \ dx\space,"How would you approach $$\int_0^{\pi/2}\frac{\sin 2013x }{\sin x} \ dx\space?$$ The way I see here involves Dirichlet kernel . I wonder what else can we do, maybe some easy/elementary approaching ways. Thanks !","How would you approach The way I see here involves Dirichlet kernel . I wonder what else can we do, maybe some easy/elementary approaching ways. Thanks !",\int_0^{\pi/2}\frac{\sin 2013x }{\sin x} \ dx\space?,"['calculus', 'real-analysis', 'integration', 'definite-integrals']"
66,Why do we need compactness?,Why do we need compactness?,,"There's a theorem that says if $E$ is an infinite subset of a compact set $K$, then $E$ has a limit point in $K$. Why can it not be done with $K$ only being closed? If $K$ is closed then $\overline{E} \subset K$, so if $x$ is a limit point of $E$, then $x \in \overline{E} \implies x \in K$.","There's a theorem that says if $E$ is an infinite subset of a compact set $K$, then $E$ has a limit point in $K$. Why can it not be done with $K$ only being closed? If $K$ is closed then $\overline{E} \subset K$, so if $x$ is a limit point of $E$, then $x \in \overline{E} \implies x \in K$.",,"['real-analysis', 'general-topology', 'compactness', 'examples-counterexamples']"
67,Construct a convergent series of positive terms with $\displaystyle\limsup_{n\to\infty} {a_{n+1}\over{a_n}}=\infty$,Construct a convergent series of positive terms with,\displaystyle\limsup_{n\to\infty} {a_{n+1}\over{a_n}}=\infty,"Construct a convergent series of positive terms with $\displaystyle\limsup_{n\to\infty} {a_{n+1}\over{a_n}}=\infty$ My thoughts: By the theorem: Suppose $a_n\ge0$ for all $n$, and let $l=\displaystyle\limsup{\sqrt[n]{a_n}}$. If $l<1$, then $\displaystyle\sum_{n=1}^\infty a_n$ converges; and if $l>1$, then $\displaystyle\sum_{n=1}^\infty a_n$ diverges. Thus, we know $\displaystyle\sum_{n=1}^\infty {a_{n+1}\over{a_n}}$ diverges I guess it might works, $(x_n)_{n=1}^{+\infty}$, $x_{n+1} = x_n^2 + x_n$ for all $n\ge1$..","Construct a convergent series of positive terms with $\displaystyle\limsup_{n\to\infty} {a_{n+1}\over{a_n}}=\infty$ My thoughts: By the theorem: Suppose $a_n\ge0$ for all $n$, and let $l=\displaystyle\limsup{\sqrt[n]{a_n}}$. If $l<1$, then $\displaystyle\sum_{n=1}^\infty a_n$ converges; and if $l>1$, then $\displaystyle\sum_{n=1}^\infty a_n$ diverges. Thus, we know $\displaystyle\sum_{n=1}^\infty {a_{n+1}\over{a_n}}$ diverges I guess it might works, $(x_n)_{n=1}^{+\infty}$, $x_{n+1} = x_n^2 + x_n$ for all $n\ge1$..",,"['real-analysis', 'sequences-and-series', 'divergent-series']"
68,Can I skip the first chapter in Rudin's Principles of Mathematical Analysis?,Can I skip the first chapter in Rudin's Principles of Mathematical Analysis?,,"I am a statistician who wishes to learn real analysis in order to better understand the foundations of statistics. With that aim in mind I plan to go through Rudin's classic on ""Principles of Mathematical Analysis"". Given the above context can I skip chapter 1? It seems to me that the material in chapter 1 is  not as important to someone with my goals. I understand that it may help in establishing the need for rigor in mathematical arguments but that is something I already appreciate. In particular, I wonder will skipping chapter 1 impede my understanding of the material in subsequent chapters? Any advice will be appreciated.","I am a statistician who wishes to learn real analysis in order to better understand the foundations of statistics. With that aim in mind I plan to go through Rudin's classic on ""Principles of Mathematical Analysis"". Given the above context can I skip chapter 1? It seems to me that the material in chapter 1 is  not as important to someone with my goals. I understand that it may help in establishing the need for rigor in mathematical arguments but that is something I already appreciate. In particular, I wonder will skipping chapter 1 impede my understanding of the material in subsequent chapters? Any advice will be appreciated.",,"['soft-question', 'real-analysis']"
69,Help on a Putnam Problem from the 90s,Help on a Putnam Problem from the 90s,,"Let $f$ be an infinitely differentiable real-valued function defined on the real numbers. If $$f\left(\frac{1}{n}\right) = \frac{n^2}{n^2+1}, \quad n = 1,2,3,...,$$ compute the values of the derivatives $f^{(k)}(0)$ for $k = 1,2,3,\dots$ . I had a friend who gave me the recommendations to consider instances where we will have $$f\left(\frac{1}{n}\right) = \frac{1}{n}$$ $$\implies \frac{1}{n} = \frac{n^2}{n^2 + 1}$$ $$\implies n + \frac{1}{n} = n^2$$ From what I got on Wolfram, there are no integer solutions to this problem, and thus I was back square one on this problem. I guess my real problem is that I have no way of inferring what $f(x)$ looks like for all $x \in \mathbb{R}$ based on my current information of $f$ . Any recommendations on this problem?","Let be an infinitely differentiable real-valued function defined on the real numbers. If compute the values of the derivatives for . I had a friend who gave me the recommendations to consider instances where we will have From what I got on Wolfram, there are no integer solutions to this problem, and thus I was back square one on this problem. I guess my real problem is that I have no way of inferring what looks like for all based on my current information of . Any recommendations on this problem?","f f\left(\frac{1}{n}\right) = \frac{n^2}{n^2+1}, \quad n = 1,2,3,..., f^{(k)}(0) k = 1,2,3,\dots f\left(\frac{1}{n}\right) = \frac{1}{n} \implies \frac{1}{n} = \frac{n^2}{n^2 + 1} \implies n + \frac{1}{n} = n^2 f(x) x \in \mathbb{R} f","['calculus', 'real-analysis', 'contest-math']"
70,Can the Identity Map be a repeated composition one other function?,Can the Identity Map be a repeated composition one other function?,,"Consider the mapping $f:x\to\frac{1}{x}, (x\ne0)$. It is trivial to see that $f(f(x))=x$. My question is whether or not there exists a continuous map $g$ such that $g(g(g(x)))\equiv g^{3}(x)=x$? Furthermore, is there a way to find out if there is such a function that $g^{p}(x)=x$ for a prime $p$? Edit: I realise I was a little unclear - I meant to specify that it was apart from the identity map. The other 'condition' I wanted to impose isn't very precise; I was hoping for a function that didn't seem defined for the purpose. However, the ones that are work perfectly well and they certainly answer the question.","Consider the mapping $f:x\to\frac{1}{x}, (x\ne0)$. It is trivial to see that $f(f(x))=x$. My question is whether or not there exists a continuous map $g$ such that $g(g(g(x)))\equiv g^{3}(x)=x$? Furthermore, is there a way to find out if there is such a function that $g^{p}(x)=x$ for a prime $p$? Edit: I realise I was a little unclear - I meant to specify that it was apart from the identity map. The other 'condition' I wanted to impose isn't very precise; I was hoping for a function that didn't seem defined for the purpose. However, the ones that are work perfectly well and they certainly answer the question.",,"['real-analysis', 'functions', 'function-and-relation-composition']"
71,"Is a function necessarily measurable, given that all of its level sets are measurable?","Is a function necessarily measurable, given that all of its level sets are measurable?",,"Let $f: \mathbb{R}^n \rightarrow \mathbb{R}$ be a function such that the set $$T_{\alpha} \equiv \{ x \in \mathbb{R}^n : f(x) = \alpha\}$$ is measurable $\forall \alpha \in \mathbb{R}$. Is $f$ measurable? Here's the proof I've sketched, but I'd like to know whether I'm on the right way or not. Since $T_\alpha$ is measurable $\forall \alpha \in \mathbb{R}$, the set $$T^{+}_{\beta} \equiv \mathbb{R}/\bigcup_{\alpha = \beta}^{+\infty}T_{\alpha} = \{x \in \mathbb{R} : f(x) < \beta\}$$ is also measurable $\forall \beta \in \mathbb{R}$, therefore $f$ is measurable.","Let $f: \mathbb{R}^n \rightarrow \mathbb{R}$ be a function such that the set $$T_{\alpha} \equiv \{ x \in \mathbb{R}^n : f(x) = \alpha\}$$ is measurable $\forall \alpha \in \mathbb{R}$. Is $f$ measurable? Here's the proof I've sketched, but I'd like to know whether I'm on the right way or not. Since $T_\alpha$ is measurable $\forall \alpha \in \mathbb{R}$, the set $$T^{+}_{\beta} \equiv \mathbb{R}/\bigcup_{\alpha = \beta}^{+\infty}T_{\alpha} = \{x \in \mathbb{R} : f(x) < \beta\}$$ is also measurable $\forall \beta \in \mathbb{R}$, therefore $f$ is measurable.",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
72,Why is the set of integers in $\mathbb{R}$ closed?,Why is the set of integers in  closed?,\mathbb{R},A point $p$ is a limit point of the set $E$ if every neighbourhood of $p$ contains a point $q \not= p$ such that $q \in E$ . So there are no limit points in $\mathbb{Z} \in \mathbb{R}$ . Is that why it's closed? A point $p$ is an interior point of the set $E$ if there is a neighbourhood $N$ of $p$ such that $N \subset E$ . So every point in $\mathbb{Z} \in \mathbb{R}$ is an interior point. So shouldn't $\mathbb{Z}$ be open as well?,A point is a limit point of the set if every neighbourhood of contains a point such that . So there are no limit points in . Is that why it's closed? A point is an interior point of the set if there is a neighbourhood of such that . So every point in is an interior point. So shouldn't be open as well?,p E p q \not= p q \in E \mathbb{Z} \in \mathbb{R} p E N p N \subset E \mathbb{Z} \in \mathbb{R} \mathbb{Z},"['real-analysis', 'general-topology']"
73,Find $ \lim\limits_{n\to\infty}\sum\limits_{i=1}^{n}{\frac{2n}{(n+2i)^2}} $,Find, \lim\limits_{n\to\infty}\sum\limits_{i=1}^{n}{\frac{2n}{(n+2i)^2}} ,"Find $$ \lim\limits_{n\to\infty}\sum\limits_{i=1}^{n}{\frac{2n}{(n+2i)^2}}.$$ I have tried dividing through by $1/n^2$ and various other algebraic tricks but cannot seem to make any progress on this limit. Wolfram Alpha gives the value as $2/3$, but I could not understand its derivation.  Any insight is welcome.","Find $$ \lim\limits_{n\to\infty}\sum\limits_{i=1}^{n}{\frac{2n}{(n+2i)^2}}.$$ I have tried dividing through by $1/n^2$ and various other algebraic tricks but cannot seem to make any progress on this limit. Wolfram Alpha gives the value as $2/3$, but I could not understand its derivation.  Any insight is welcome.",,"['real-analysis', 'sequences-and-series', 'limits']"
74,"Evaluate $\sum_{n,k} \binom{n}{k}^{-1} $",Evaluate,"\sum_{n,k} \binom{n}{k}^{-1} ","Evaluate $$\sum_{n,k}  \frac{1}{\binom{n}{k}}, $$ where the summation ranges over all positive integers $n,k$ with $1<k<n-1$ . Thouhgts: We are trying to evaluate $$\sum_{n=4}^{\infty} \sum_{k=2}^{n-2} \binom{n}{k}^{-1}$$ We may try to find a closed form of the inner summation which is of the form : $$ \frac{1}{ {n \choose 2} } +  \frac{1}{{n \choose 3} }+ \dotsb + \frac{1}{{n \choose n-2} }.   $$ Notice that we may write $\frac{1}{ {n \choose 2} } = \frac{2! }{n(n-1)}$ and if keep doing the same for the other terms we obtain the following: $$ \frac{ (n-2)! + (n-3)! (n-(n-2)) + (n-4)!(n-(n-2))(n-(n-3)) + \dotsb + 2! (n-3)! }{n!}, $$ which equals $$ \frac{ (n-2)! + 2!(n-3)! + 3! (n-4)! + \dotsb + (n-3)! 2! }{n!} $$ and so this equals: $$ \frac{1}{n(n-1)} + \frac{2}{n(n-1)(n-2)} + \dfrac{6}{n(n-1)(n-2)} + \dotsb + \dfrac{2}{n(n-1)(n-2) }. $$ But half of this terms are identical. Therefore, we are trying to sum up series of the form $$\sum_{n \geq k} \frac{1}{(n-1)(n-2)(n-3)\dotso(n-k)} ,$$ which can be done by a telescoping trick, but it seems very formidable. Am I approaching this problem the right way? Any hints/suggestions?","Evaluate where the summation ranges over all positive integers with . Thouhgts: We are trying to evaluate We may try to find a closed form of the inner summation which is of the form : Notice that we may write and if keep doing the same for the other terms we obtain the following: which equals and so this equals: But half of this terms are identical. Therefore, we are trying to sum up series of the form which can be done by a telescoping trick, but it seems very formidable. Am I approaching this problem the right way? Any hints/suggestions?","\sum_{n,k}  \frac{1}{\binom{n}{k}},  n,k 1<k<n-1 \sum_{n=4}^{\infty} \sum_{k=2}^{n-2} \binom{n}{k}^{-1}  \frac{1}{ {n \choose 2} } +  \frac{1}{{n \choose 3} }+ \dotsb + \frac{1}{{n \choose n-2} }.    \frac{1}{ {n \choose 2} } = \frac{2! }{n(n-1)}  \frac{ (n-2)! + (n-3)! (n-(n-2)) + (n-4)!(n-(n-2))(n-(n-3)) + \dotsb + 2! (n-3)! }{n!},   \frac{ (n-2)! + 2!(n-3)! + 3! (n-4)! + \dotsb + (n-3)! 2! }{n!}   \frac{1}{n(n-1)} + \frac{2}{n(n-1)(n-2)} + \dfrac{6}{n(n-1)(n-2)} + \dotsb + \dfrac{2}{n(n-1)(n-2) }.  \sum_{n \geq k} \frac{1}{(n-1)(n-2)(n-3)\dotso(n-k)} ,","['real-analysis', 'calculus', 'sequences-and-series', 'summation']"
75,Evaluating Integrals using Lebesgue Integration,Evaluating Integrals using Lebesgue Integration,,"Suppose we are to evaluate: $$I = \int_{0}^{1} f(x) dx$$ Where $$f(x)=\begin{cases}1 \space \text{if} \space x\space \text{is rational}, & \newline  0  \space \text{if} \space x \space \text{is irrational} \\ \end{cases}$$ I have been told that this can be done using measure theory. Will anyone care to explain how possibly? I am new to measure theory, so I am just researching, please do not say ""no attempt shown"" this is because I dont know Lebesgue yet, but I heard it has great applications on this?","Suppose we are to evaluate: $$I = \int_{0}^{1} f(x) dx$$ Where $$f(x)=\begin{cases}1 \space \text{if} \space x\space \text{is rational}, & \newline  0  \space \text{if} \space x \space \text{is irrational} \\ \end{cases}$$ I have been told that this can be done using measure theory. Will anyone care to explain how possibly? I am new to measure theory, so I am just researching, please do not say ""no attempt shown"" this is because I dont know Lebesgue yet, but I heard it has great applications on this?",,"['calculus', 'real-analysis', 'integration', 'measure-theory', 'lebesgue-integral']"
76,"Bounded sequence which is not convergent, but differences of consecutive terms converge to zero","Bounded sequence which is not convergent, but differences of consecutive terms converge to zero",,"I have a question that says ""Show that there is a bounded sequence $x_n$ which is not convergent but has the property that $x_n - x_{n+1} \to 0$ as $n \to 0$. What does this mean? Do I need to come up with an example or does the problem actually want me to prove such proposition? By the way, I see that this sequence looks like Cauchy because of $x_n - x_{n+1} \to 0$ as $n \to 0$, but it is obviously not.","I have a question that says ""Show that there is a bounded sequence $x_n$ which is not convergent but has the property that $x_n - x_{n+1} \to 0$ as $n \to 0$. What does this mean? Do I need to come up with an example or does the problem actually want me to prove such proposition? By the way, I see that this sequence looks like Cauchy because of $x_n - x_{n+1} \to 0$ as $n \to 0$, but it is obviously not.",,['real-analysis']
77,Show that $A = \{p\in\mathbb{ Q}^+\mid p^2<2\}$ contains no largest number and $B= \{p\in\mathbb {Q}^+\mid p^2>2\}$ contains no smallest number [duplicate],Show that  contains no largest number and  contains no smallest number [duplicate],A = \{p\in\mathbb{ Q}^+\mid p^2<2\} B= \{p\in\mathbb {Q}^+\mid p^2>2\},"This question already has answers here : Choice of $q$ in Baby Rudin's Example 1.1 (16 answers) Closed 3 years ago . I was reading Principles of Mathematical Analysis - Walter Rudin. In the start (pg-11), it is shown that the equation $p^2 = 2$ is not satisfied by any rational $p$ i.e. $\sqrt{2}$ is irrational. After this the book says: ""We now examine this situation a little more closely. Let $A$ be the set of all positive rationals $p$ such that $p^2 < 2$ and let $В$ consist of all positive rationals $p$ such that $p^2 > 2$. We shall show that $A$ contains no largest number and $В$ contains no smallest. More explicitly, for every $p \in A$ we can find a rational $q \in A$ such that $p < q$, and for every $p \in В$ we can find a rational $q \in В$ such that $q < p$. To do this, we associate with each rational $p > 0$ the number $$ \begin{align} q &= p - \frac{(p^2 - 2)}{p + 2} &(2)\\ &=\frac{2p + 2}{p + 2} &(3)\\ &\Rightarrow q^2 - 2 = \frac{2(p^2 - 2)}{(p + 2)^2} &(4) \end{align} $$ If $p \in A$ then $p^2 — 2 < 0$, (3) shows that $q > p$, and (4) shows that $q^2 < 2$. Thus $q \in A$. If $p \in В$ then $p^2 — 2 > 0$, (3) shows that $0 < q < p$, and (4) shows that $q^2 > 2$. Thus $q \in B$."" I am unable to follow this part, specially how we construct the equations (3) and (4). Can somebody explain?","This question already has answers here : Choice of $q$ in Baby Rudin's Example 1.1 (16 answers) Closed 3 years ago . I was reading Principles of Mathematical Analysis - Walter Rudin. In the start (pg-11), it is shown that the equation $p^2 = 2$ is not satisfied by any rational $p$ i.e. $\sqrt{2}$ is irrational. After this the book says: ""We now examine this situation a little more closely. Let $A$ be the set of all positive rationals $p$ such that $p^2 < 2$ and let $В$ consist of all positive rationals $p$ such that $p^2 > 2$. We shall show that $A$ contains no largest number and $В$ contains no smallest. More explicitly, for every $p \in A$ we can find a rational $q \in A$ such that $p < q$, and for every $p \in В$ we can find a rational $q \in В$ such that $q < p$. To do this, we associate with each rational $p > 0$ the number $$ \begin{align} q &= p - \frac{(p^2 - 2)}{p + 2} &(2)\\ &=\frac{2p + 2}{p + 2} &(3)\\ &\Rightarrow q^2 - 2 = \frac{2(p^2 - 2)}{(p + 2)^2} &(4) \end{align} $$ If $p \in A$ then $p^2 — 2 < 0$, (3) shows that $q > p$, and (4) shows that $q^2 < 2$. Thus $q \in A$. If $p \in В$ then $p^2 — 2 > 0$, (3) shows that $0 < q < p$, and (4) shows that $q^2 > 2$. Thus $q \in B$."" I am unable to follow this part, specially how we construct the equations (3) and (4). Can somebody explain?",,"['real-analysis', 'analysis']"
78,Proving the positivity of a twice-differentiable real-valued function,Proving the positivity of a twice-differentiable real-valued function,,"This is a problem from Berkeley prelim exams, Spring '99 Suppose that $ f $ is a twice differentiable real-valued function on   $\mathbb{R}$ such that $ f(0) = 0 $, $ f'(0) > 0 $, and $ f''(x) \geq f(x) $ for all $ x \geq 0 $. Prove that   $ f(x) > 0 $ for all $ x > 0 $. Are there general techniques to solve problems like this?","This is a problem from Berkeley prelim exams, Spring '99 Suppose that $ f $ is a twice differentiable real-valued function on   $\mathbb{R}$ such that $ f(0) = 0 $, $ f'(0) > 0 $, and $ f''(x) \geq f(x) $ for all $ x \geq 0 $. Prove that   $ f(x) > 0 $ for all $ x > 0 $. Are there general techniques to solve problems like this?",,"['real-analysis', 'analysis']"
79,"Compute the following integral: $I = \int_1^\infty \log^2 \left(1-\frac 1 x\right) \, dx$",Compute the following integral:,"I = \int_1^\infty \log^2 \left(1-\frac 1 x\right) \, dx","Compute $$I = \int_1^\infty \log^2 \left(1-\frac 1 x\right) \, dx$$ I made the substitution: $$t=\frac 1 x$$ It follows: $$I=\int_0^1 \frac{\log^2(1-t)}{t^2} \, dt$$ My next step would be to compute the derivative of the following integral with parameter $y$, w.r.t to $y$: $$F(y)=\int_0^1 \frac{\log^2(y-t)}{t^2} \, dt$$ Or something like this. I think would be a nice solution to use this kind of approach. But I am getting stuck after computing the derivative.","Compute $$I = \int_1^\infty \log^2 \left(1-\frac 1 x\right) \, dx$$ I made the substitution: $$t=\frac 1 x$$ It follows: $$I=\int_0^1 \frac{\log^2(1-t)}{t^2} \, dt$$ My next step would be to compute the derivative of the following integral with parameter $y$, w.r.t to $y$: $$F(y)=\int_0^1 \frac{\log^2(y-t)}{t^2} \, dt$$ Or something like this. I think would be a nice solution to use this kind of approach. But I am getting stuck after computing the derivative.",,"['real-analysis', 'integration', 'definite-integrals', 'improper-integrals']"
80,Convergence of $\sin{\pi\sqrt{n}}$,Convergence of,\sin{\pi\sqrt{n}},"Revising for an exam: Let $a_n = \sin{(\pi\sqrt{n})}.$ Show that: (i) $a_{n+1} - a_{n} \rightarrow 0$ (ii) The sequence $(a_n)$ is bounded. (iii) $(a_n)$ does not converge. My attempt: (i) ??? (ii) min($\sin(x)$) = -1, max($\sin{x}$) = 1, so $-1 \leq a_n \leq 1,  \forall n \in \mathbb{N}$. Thus 1 is an upper bound and -1 is a lower bound. (iii) $a_n$ has a monotonic subsequence which converges to 1 by the Bolanzo-Weierstrass theorem. Note that the subsequence $a_{n^2}$ converges to 0. Since $0 \neq 1$, $a_n$ does not converge.","Revising for an exam: Let $a_n = \sin{(\pi\sqrt{n})}.$ Show that: (i) $a_{n+1} - a_{n} \rightarrow 0$ (ii) The sequence $(a_n)$ is bounded. (iii) $(a_n)$ does not converge. My attempt: (i) ??? (ii) min($\sin(x)$) = -1, max($\sin{x}$) = 1, so $-1 \leq a_n \leq 1,  \forall n \in \mathbb{N}$. Thus 1 is an upper bound and -1 is a lower bound. (iii) $a_n$ has a monotonic subsequence which converges to 1 by the Bolanzo-Weierstrass theorem. Note that the subsequence $a_{n^2}$ converges to 0. Since $0 \neq 1$, $a_n$ does not converge.",,"['calculus', 'real-analysis', 'sequences-and-series', 'trigonometry', 'radicals']"
81,Can we always multiply some function that is not differentiable everywhere with function that is to obtain differentiable product?,Can we always multiply some function that is not differentiable everywhere with function that is to obtain differentiable product?,,"First of all, I think that before stating the general question it would be okay to make some concrete example of what do I have in mind. Let us take the function $f(x)=|x|$. We could write this function as $f(x)= \begin{cases}x&{x>0}\\-x&x<0 \\ 0& {x=0} \end{cases}$. This function is differentiable everywhere except at the point $x=0$. Now let us multiply this function with, say, function $g(x)=x$. Now we have $f(x)g(x)=\begin{cases}{x^2}&{x>0}\\{-x^2}&{x<0}\\ 0&{x=0} \end{cases}$. Clearly $f(x)g(x)$ is differentiable everywhere. What did we do here? We multiplied function which was not differentiable at one point with some other function which is of class $C^{\infty}$ and we obtained function which is everywhere differentiable. Now I would like to ask the question which deals with the general case: Suppose that we have some real function of a real variable $f$ that is continuous on some set $(a,b)$ and that is not differentiable on some subset of $(a,b)$ (the subset could be only one point as in the above described example or it could be the whole set $(a,b)$ so that we have everywhere continuous but nowhere differentiable function). Could it be that there always exists some function $g$ (which could depend on $f$) of class $C^{\infty}$ (function that is infinitely times differentiable) which is not the zero function (so $g$ is not the function $g(x)=0$) and which is such that we have that the function $fg$ (the product of the functions $f$ and $g$) is differentiable on the set $(a,b)$?","First of all, I think that before stating the general question it would be okay to make some concrete example of what do I have in mind. Let us take the function $f(x)=|x|$. We could write this function as $f(x)= \begin{cases}x&{x>0}\\-x&x<0 \\ 0& {x=0} \end{cases}$. This function is differentiable everywhere except at the point $x=0$. Now let us multiply this function with, say, function $g(x)=x$. Now we have $f(x)g(x)=\begin{cases}{x^2}&{x>0}\\{-x^2}&{x<0}\\ 0&{x=0} \end{cases}$. Clearly $f(x)g(x)$ is differentiable everywhere. What did we do here? We multiplied function which was not differentiable at one point with some other function which is of class $C^{\infty}$ and we obtained function which is everywhere differentiable. Now I would like to ask the question which deals with the general case: Suppose that we have some real function of a real variable $f$ that is continuous on some set $(a,b)$ and that is not differentiable on some subset of $(a,b)$ (the subset could be only one point as in the above described example or it could be the whole set $(a,b)$ so that we have everywhere continuous but nowhere differentiable function). Could it be that there always exists some function $g$ (which could depend on $f$) of class $C^{\infty}$ (function that is infinitely times differentiable) which is not the zero function (so $g$ is not the function $g(x)=0$) and which is such that we have that the function $fg$ (the product of the functions $f$ and $g$) is differentiable on the set $(a,b)$?",,"['real-analysis', 'derivatives', 'continuity']"
82,Fourier transform of signum function,Fourier transform of signum function,,"If we treat fourier transform as an operator on $L^1(\mathbb{R})$ , then its image under fourier transform is the set of continuous functions which will vanish at infinity. It is well known that the fourier transform of signum function is $$\mathcal{F} (sgn)(u) =\frac{2}{ui}. $$ I know that signum function is not integrable over the real line. So, to evaluate its fourier transform, one can use limiting argument, say a sequence of functions that converges to signum function, because fourier transform is a bounded linear operator, and hence is continuous. What puzzles me is that when using continuity, don't we need to ensure that the fourier transform is defined on the limiting function? In this case, fourier transform of signum function is not defined due to reason given above. If this is the case, how would one obtain formula of the fourier transform of signum function?","If we treat fourier transform as an operator on , then its image under fourier transform is the set of continuous functions which will vanish at infinity. It is well known that the fourier transform of signum function is I know that signum function is not integrable over the real line. So, to evaluate its fourier transform, one can use limiting argument, say a sequence of functions that converges to signum function, because fourier transform is a bounded linear operator, and hence is continuous. What puzzles me is that when using continuity, don't we need to ensure that the fourier transform is defined on the limiting function? In this case, fourier transform of signum function is not defined due to reason given above. If this is the case, how would one obtain formula of the fourier transform of signum function?",L^1(\mathbb{R}) \mathcal{F} (sgn)(u) =\frac{2}{ui}. ,"['real-analysis', 'fourier-transform']"
83,Convergence of $a_{n+1}=\frac{1}{1+a_n}$,Convergence of,a_{n+1}=\frac{1}{1+a_n},"We define $$a_{n+1}=\frac{1}{1+a_n}, a_0=c> 0$$ I stumbled across that sequence and Mathematica gives me that it converges against $\frac{1}{2} \left(\sqrt{5}-1\right)$ which doesn't even depend on $a_0$. Normally I show that sequences converge by seeing that they are monotonic and then the limit can be easily found by setting $a_n=a_{n+1}$, but this one seems to be alternating. Also by checking OEIS I noticed that $a_n=1/g(n)$ where $g(n)$ gives the $n+1$'th Golden Rectangle Number . I would be glad if someone can show me how to show that such sequences converge and how to find the limit, also maybe this is a well known sequence, as it has a quite simple form, too bad that google is very bad for looking for sequences and OEIS doesn't mention anything.","We define $$a_{n+1}=\frac{1}{1+a_n}, a_0=c> 0$$ I stumbled across that sequence and Mathematica gives me that it converges against $\frac{1}{2} \left(\sqrt{5}-1\right)$ which doesn't even depend on $a_0$. Normally I show that sequences converge by seeing that they are monotonic and then the limit can be easily found by setting $a_n=a_{n+1}$, but this one seems to be alternating. Also by checking OEIS I noticed that $a_n=1/g(n)$ where $g(n)$ gives the $n+1$'th Golden Rectangle Number . I would be glad if someone can show me how to show that such sequences converge and how to find the limit, also maybe this is a well known sequence, as it has a quite simple form, too bad that google is very bad for looking for sequences and OEIS doesn't mention anything.",,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
84,Why can’t a non-constant polynomial have a constant interval?,Why can’t a non-constant polynomial have a constant interval?,,"Given a polynomial that is not constant, of course, it doesn’t contain a constant interval. But how can we prove it? Since the polynomial is differentiable over R, I came up with a solution that uses Lagrange mean value theorem n times and reduces the nth derivative to a constant. Since the leading term is not zero, there can not be a zero in the nth derivative, and that contradicts the mean value theorem. Therefore, the polynomial must not contain a constant interval. However, I do realize that this solution is a bit complicated,  so is there a simpler solution(possibly elementary) that can prove this?","Given a polynomial that is not constant, of course, it doesn’t contain a constant interval. But how can we prove it? Since the polynomial is differentiable over R, I came up with a solution that uses Lagrange mean value theorem n times and reduces the nth derivative to a constant. Since the leading term is not zero, there can not be a zero in the nth derivative, and that contradicts the mean value theorem. Therefore, the polynomial must not contain a constant interval. However, I do realize that this solution is a bit complicated,  so is there a simpler solution(possibly elementary) that can prove this?",,"['real-analysis', 'calculus', 'polynomials']"
85,Calculation of Limit of a Repeating Continued Fraction,Calculation of Limit of a Repeating Continued Fraction,,"I'm sorry if this is a duplicate. I have no idea on what kind of ""name"" i should give to this, and therefore i have no idea on how to search on the internet for help on understanding it. If it happens that this is a duplicate, i would be grateful if you could link me to where there are any solutions for this. I need to prove for an exercise on my analysis book that the following sequence $$ {\cfrac{1}{1+\cfrac{1}{5}}},\quad {\cfrac{1}{1+\cfrac{1}{5+\cfrac{1}{1+\cfrac{1}{5}}}}},\dotsc $$ is monotone and converges to ${\frac{-5+\sqrt{45}}{2}}$ I imagine that once i get on how to determine it's limit, it will be easy to prove that it is in fact monotone. I have no idea on how to approach it though. Any tips?","I'm sorry if this is a duplicate. I have no idea on what kind of ""name"" i should give to this, and therefore i have no idea on how to search on the internet for help on understanding it. If it happens that this is a duplicate, i would be grateful if you could link me to where there are any solutions for this. I need to prove for an exercise on my analysis book that the following sequence $$ {\cfrac{1}{1+\cfrac{1}{5}}},\quad {\cfrac{1}{1+\cfrac{1}{5+\cfrac{1}{1+\cfrac{1}{5}}}}},\dotsc $$ is monotone and converges to ${\frac{-5+\sqrt{45}}{2}}$ I imagine that once i get on how to determine it's limit, it will be easy to prove that it is in fact monotone. I have no idea on how to approach it though. Any tips?",,"['real-analysis', 'convergence-divergence', 'continued-fractions']"
86,Trouble understanding Borel sets definition,Trouble understanding Borel sets definition,,"The definition in my book is: The collection $B$ of Borel sets of real numbers is the smallest   $\sigma$-algebra of sets of real numbers that contains all of the open   sets of real numbers. First, I'm a bit confused by the wording here. Does this mean that if a collection $B$ is a $\sigma$-algebra and it contains all of the open sets of real numbers, then the sets in $B$ are called Borel sets? I'm reading through some of the other answers to this question now, but it seems like different texts use different definitions, so if anyone could shed some light on the definition I provided, that would be helpful.","The definition in my book is: The collection $B$ of Borel sets of real numbers is the smallest   $\sigma$-algebra of sets of real numbers that contains all of the open   sets of real numbers. First, I'm a bit confused by the wording here. Does this mean that if a collection $B$ is a $\sigma$-algebra and it contains all of the open sets of real numbers, then the sets in $B$ are called Borel sets? I'm reading through some of the other answers to this question now, but it seems like different texts use different definitions, so if anyone could shed some light on the definition I provided, that would be helpful.",,"['real-analysis', 'measure-theory', 'elementary-set-theory']"
87,Why do we use $\mathbb{R}$?,Why do we use ?,\mathbb{R},Since there are holes in the $\mathbb{Q}$ we have constructed $\mathbb{R}$ in order to fill in these holes. But I was wondering why we don't use $\mathbb{C}$ or some other number system that is even larger as the main number system. I was also wondering what are the implications of discovering a larger number system. Could someone briefly explain it to me?,Since there are holes in the $\mathbb{Q}$ we have constructed $\mathbb{R}$ in order to fill in these holes. But I was wondering why we don't use $\mathbb{C}$ or some other number system that is even larger as the main number system. I was also wondering what are the implications of discovering a larger number system. Could someone briefly explain it to me?,,"['real-analysis', 'analysis']"
88,How to find $\int_{0}^{\pi/2} \log ({1+\cos x}) dx$ using real-variable methods?,How to find  using real-variable methods?,\int_{0}^{\pi/2} \log ({1+\cos x}) dx,"How do you find the value of this integral, using real methods? $$I=\displaystyle\int_{0}^{\pi/2} \log ({1+\cos x}) dx$$ The answer is $2C-\dfrac{\pi}{2}\log {2}$ where $C$ is Catalan's constant.","How do you find the value of this integral, using real methods? $$I=\displaystyle\int_{0}^{\pi/2} \log ({1+\cos x}) dx$$ The answer is $2C-\dfrac{\pi}{2}\log {2}$ where $C$ is Catalan's constant.",,"['real-analysis', 'calculus', 'integration', 'definite-integrals', 'catalans-constant']"
89,Set of rapidly increasing functions is uncountable?,Set of rapidly increasing functions is uncountable?,,"Let $$ S=\{f \colon \mathbb{N} \mapsto \mathbb{R} \mid f(n+1) \ge 2^{f(n)} \}.$$ How to prove $S$ is uncountable ? I tried proving by contradiction, but not able to construct a new function different from the countable collection. Any help would be appreciated.","Let How to prove is uncountable ? I tried proving by contradiction, but not able to construct a new function different from the countable collection. Any help would be appreciated.", S=\{f \colon \mathbb{N} \mapsto \mathbb{R} \mid f(n+1) \ge 2^{f(n)} \}. S,"['real-analysis', 'elementary-set-theory']"
90,Set has Measure Zero iff There is a Collection of Intervals,Set has Measure Zero iff There is a Collection of Intervals,,"I'm stuck on the following real analysis problem. Show that a set $E$ of real numbers has Lebesgue measure zero if and only if there is a countable collection $\{I_{k}\}_{k=1}^{\infty}$ of open intervals for which each point in $E$ belongs to infinitely many of the $I_{k}$'s and $\sum_{k=1}^{\infty}\ell(I_{k})<\infty$, where $\ell(I_{k})$ is the length of the interval $I_{k}$. I think the converse implication $(\Leftarrow)$ should be trivial by the definition of the Lebesgue measure, but I'm not sure. I also think that I should be using the Vitali Covering Lemma somewhere (possibly in the forward implication?). Thanks in advance for any help!","I'm stuck on the following real analysis problem. Show that a set $E$ of real numbers has Lebesgue measure zero if and only if there is a countable collection $\{I_{k}\}_{k=1}^{\infty}$ of open intervals for which each point in $E$ belongs to infinitely many of the $I_{k}$'s and $\sum_{k=1}^{\infty}\ell(I_{k})<\infty$, where $\ell(I_{k})$ is the length of the interval $I_{k}$. I think the converse implication $(\Leftarrow)$ should be trivial by the definition of the Lebesgue measure, but I'm not sure. I also think that I should be using the Vitali Covering Lemma somewhere (possibly in the forward implication?). Thanks in advance for any help!",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
91,Why is this proof incorrect? (limit product is product of the limits),Why is this proof incorrect? (limit product is product of the limits),,"I want to prove that if: $$\lim_{n \to \infty}s_n = L_1, \lim_{n \to \infty}t_n = L_2$$ then $$\lim_{n \to \infty}(s_n t_n) = L_1L_2$$ Wrong (?) proof: Fix $\epsilon >0$. By definition, there are integers $N_1,N_2$ such that: $$n>N_1 \implies |s_n-L_1|< \frac{\epsilon}{|s_n|+|L_2|}$$ $$n>N_2 \implies |t_n-L_2|< \frac{\epsilon}{|s_n|+|L_2|}$$ Hence, for $n > \max\{N_1,N_2\}$, we have: $$|s_nt_n - L_1L_2| = |s_n(t_n - L_2) + s_nL_2 - L_1L_2|$$ $$\leq |s_n||t_n - L_2| + |L_2||s_n - L_1|$$ $$< \frac{\epsilon}{|s_n|+|L_2|} (|s_n| + |L_2|) = \epsilon$$ I was taught that the $\epsilon$ can't depend on $n$, but I can't see why. What goes wrong? EDIT: I know how to fix the proof, I made a post on this one: Limit of product of sequences is the product of the limits of the sequences","I want to prove that if: $$\lim_{n \to \infty}s_n = L_1, \lim_{n \to \infty}t_n = L_2$$ then $$\lim_{n \to \infty}(s_n t_n) = L_1L_2$$ Wrong (?) proof: Fix $\epsilon >0$. By definition, there are integers $N_1,N_2$ such that: $$n>N_1 \implies |s_n-L_1|< \frac{\epsilon}{|s_n|+|L_2|}$$ $$n>N_2 \implies |t_n-L_2|< \frac{\epsilon}{|s_n|+|L_2|}$$ Hence, for $n > \max\{N_1,N_2\}$, we have: $$|s_nt_n - L_1L_2| = |s_n(t_n - L_2) + s_nL_2 - L_1L_2|$$ $$\leq |s_n||t_n - L_2| + |L_2||s_n - L_1|$$ $$< \frac{\epsilon}{|s_n|+|L_2|} (|s_n| + |L_2|) = \epsilon$$ I was taught that the $\epsilon$ can't depend on $n$, but I can't see why. What goes wrong? EDIT: I know how to fix the proof, I made a post on this one: Limit of product of sequences is the product of the limits of the sequences",,"['real-analysis', 'sequences-and-series']"
92,Why is $\mathbb N$ a closed set?,Why is  a closed set?,\mathbb N,"I know that we can prove that $\mathbb N$ is a closed set through the use of the compliment of $\mathbb N$. Since $\mathbb R$\ $\mathbb N$ is open, $\mathbb N$ must be closed. However, the question arises: If that is so, then $\mathbb N$ must contain its limit points. Then what are the limit points of $\mathbb N$ ? I believe they are every element of $\mathbb N$: 1,2,3,4,5,6,... are all limit points of $\mathbb N$. But remember the definition of a limit point: x is a limit point of $A$ if $\forall \epsilon \gt 0$, $V_{\epsilon}(x) \cap A =$ points that are different from $x$. Now take $\epsilon = 0.5$ Then we see that $V_{\epsilon =0,5}(1) \cap \mathbb N = {1}$. Hence, EVERY points in $\mathbb N$ is an isolated point, and there is no limit point: A contradiction. So, please point out what is wrong with my thoughts ? I thank you very much for your help. I am just an introductory real analysis learner, so please help me. I thank you.","I know that we can prove that $\mathbb N$ is a closed set through the use of the compliment of $\mathbb N$. Since $\mathbb R$\ $\mathbb N$ is open, $\mathbb N$ must be closed. However, the question arises: If that is so, then $\mathbb N$ must contain its limit points. Then what are the limit points of $\mathbb N$ ? I believe they are every element of $\mathbb N$: 1,2,3,4,5,6,... are all limit points of $\mathbb N$. But remember the definition of a limit point: x is a limit point of $A$ if $\forall \epsilon \gt 0$, $V_{\epsilon}(x) \cap A =$ points that are different from $x$. Now take $\epsilon = 0.5$ Then we see that $V_{\epsilon =0,5}(1) \cap \mathbb N = {1}$. Hence, EVERY points in $\mathbb N$ is an isolated point, and there is no limit point: A contradiction. So, please point out what is wrong with my thoughts ? I thank you very much for your help. I am just an introductory real analysis learner, so please help me. I thank you.",,"['calculus', 'real-analysis', 'general-topology']"
93,"For continuous function $ f:\mathbb S^1 \to \mathbb R$ there exists uncountably many distinct points $x,y$ such that $f(x)=f(y)$",For continuous function  there exists uncountably many distinct points  such that," f:\mathbb S^1 \to \mathbb R x,y f(x)=f(y)","Let $\mathbb S^1$ denote the unit circle in $\mathbb R^2$. Then prove that for every  continuous function $f:\mathbb S^1 \to \mathbb R$, there exist uncountably many pairs of distinct points $x, y$ in $S^1$, such that $f(x)=f(y)$.","Let $\mathbb S^1$ denote the unit circle in $\mathbb R^2$. Then prove that for every  continuous function $f:\mathbb S^1 \to \mathbb R$, there exist uncountably many pairs of distinct points $x, y$ in $S^1$, such that $f(x)=f(y)$.",,"['real-analysis', 'general-topology']"
94,What's the rationale behind rejecting limits of recurrence relation?,What's the rationale behind rejecting limits of recurrence relation?,,"I found this problem in Understanding Analysis by Stephen Abbott. Define a recurrence relation as $x_1=3$ and $x_{n+1}=\frac{1}{4-x_n}$ . Find the limit of the sequence $(x_n)$ . There are other parts to the question, but what I want to ask is: why do we reject a limit and not another when solving for the limit? If $(x_n) \rightarrow L$ , it is not too hard to show that $(x_{n+1}) \rightarrow L$ . Thus we can take the limit of both sides of $x_{n+1}=\frac{1}{4-x_n}$ to get $L=\frac{1}{4-L}$ . We can solve for $L$ to get $L=0.268$ or $L=3.732$ . My question is why $L=0.268$ is the answer, and not $3.732$ . Is there any significance behind $3.732$ popping out here? Logically, we can conclude that $x_n$ is strictly decreasing and reject $3.732$ on the basis that it is greater than $x_1$ , but is that a valid reason though? What about sequences whose behaviour is neither monotone decreasing/increasing? How do we tell if a particular limit we solved for is correct or wrong?","I found this problem in Understanding Analysis by Stephen Abbott. Define a recurrence relation as and . Find the limit of the sequence . There are other parts to the question, but what I want to ask is: why do we reject a limit and not another when solving for the limit? If , it is not too hard to show that . Thus we can take the limit of both sides of to get . We can solve for to get or . My question is why is the answer, and not . Is there any significance behind popping out here? Logically, we can conclude that is strictly decreasing and reject on the basis that it is greater than , but is that a valid reason though? What about sequences whose behaviour is neither monotone decreasing/increasing? How do we tell if a particular limit we solved for is correct or wrong?",x_1=3 x_{n+1}=\frac{1}{4-x_n} (x_n) (x_n) \rightarrow L (x_{n+1}) \rightarrow L x_{n+1}=\frac{1}{4-x_n} L=\frac{1}{4-L} L L=0.268 L=3.732 L=0.268 3.732 3.732 x_n 3.732 x_1,['real-analysis']
95,Are there uncountable antichains in $\mathcal{P}(\mathbb{N})$,Are there uncountable antichains in,\mathcal{P}(\mathbb{N}),"As the title states, I am unsure of how to construct an uncountable antichain in the power set $\mathcal{P}(\mathbb{N})$. Recall that an antichain is a set $\mathcal{A}\subset\mathcal{P}(\mathbb{N})$ where each element of $\mathcal{A}$ is not a subset of another element of $\mathcal{A}$. It seems that everything I attempt requires me to assign a property to the ""$n$-th set"" but that already makes it countable...","As the title states, I am unsure of how to construct an uncountable antichain in the power set $\mathcal{P}(\mathbb{N})$. Recall that an antichain is a set $\mathcal{A}\subset\mathcal{P}(\mathbb{N})$ where each element of $\mathcal{A}$ is not a subset of another element of $\mathcal{A}$. It seems that everything I attempt requires me to assign a property to the ""$n$-th set"" but that already makes it countable...",,"['real-analysis', 'cardinals']"
96,Proof of Young's inequality,Proof of Young's inequality,,"The following problem is from Spivak's Calculus. Suppose that $f$ is a continuous increasing function with $f(0)=0$. Prove that for $a,b \gt 0$ we have Young's inequality $$ ab \le \int_0^af(x)dx+\int_0^bf^{-1}(x)dx$$, and that equality holds if and only if $b=f(a)$. It is enough to consider the case $f(a) \gt b$, and show that the strict inequality occurs in this case. I've tried proving this using the theorem  $$ \int_a^bf^{-1}=bf^{-1}(b)-af^{-1}(a)-\int_{f^{-1}(a)}^{f^{-1}(b)}f$$ but I got stuck along the way. How may I show this rigorously using the definition or properties of integrals? Any hint, suggestions or solutions would be appreciated.","The following problem is from Spivak's Calculus. Suppose that $f$ is a continuous increasing function with $f(0)=0$. Prove that for $a,b \gt 0$ we have Young's inequality $$ ab \le \int_0^af(x)dx+\int_0^bf^{-1}(x)dx$$, and that equality holds if and only if $b=f(a)$. It is enough to consider the case $f(a) \gt b$, and show that the strict inequality occurs in this case. I've tried proving this using the theorem  $$ \int_a^bf^{-1}=bf^{-1}(b)-af^{-1}(a)-\int_{f^{-1}(a)}^{f^{-1}(b)}f$$ but I got stuck along the way. How may I show this rigorously using the definition or properties of integrals? Any hint, suggestions or solutions would be appreciated.",,"['calculus', 'real-analysis']"
97,Can a function be continuous but not Hölder on a compact set?,Can a function be continuous but not Hölder on a compact set?,,"Is it possible to construct a function $f: K \to \mathbb{R}$, where $K \subset \mathbb{R}$ is compact, such that  $f$ is continuous but not Hölder continuous of any order?  It seems like there should be such a function--it would probably oscillate wildly, like the Weierstrass-Mandlebrot function.  However, the W-M function itself doesn't work, since it is Hölder. Edit: I guess I did have in mind for the function to not be Hölder anywhere, even though I didn't explicitly say so.","Is it possible to construct a function $f: K \to \mathbb{R}$, where $K \subset \mathbb{R}$ is compact, such that  $f$ is continuous but not Hölder continuous of any order?  It seems like there should be such a function--it would probably oscillate wildly, like the Weierstrass-Mandlebrot function.  However, the W-M function itself doesn't work, since it is Hölder. Edit: I guess I did have in mind for the function to not be Hölder anywhere, even though I didn't explicitly say so.",,"['calculus', 'real-analysis', 'continuity', 'uniform-continuity']"
98,An improper integral : $\int_{0}^\infty {\ln(a^2+x^2)\over{b^2+x^2}}dx$,An improper integral :,\int_{0}^\infty {\ln(a^2+x^2)\over{b^2+x^2}}dx,"How to evaluate the following improper integral:$$\int_{0}^\infty {\ln(a^2+x^2)\over{b^2+x^2}}dx,$$ where $a,b>0$. I tried to suppose $$f(a)=\int_0^\infty {\ln(a^2+x^2)\over{b^2+x^2}}dx,$$ based on the convergence theorem, and then I tried $${df(a)\over da}=\int_0^\infty {2a\over {(a^2+x^2)(b^2+x^2)}}dx = {\pi\over b(b+a)},$$and then $$f(a)={\pi\over b}\ln(b+a)+C,$$where $C$ is a constant, but I don't know how to find the constant $C$. Could anyone tell me that, and explain why? Or could anyone find other methods to evaluate the integral? If you could, please explain. Thanks.","How to evaluate the following improper integral:$$\int_{0}^\infty {\ln(a^2+x^2)\over{b^2+x^2}}dx,$$ where $a,b>0$. I tried to suppose $$f(a)=\int_0^\infty {\ln(a^2+x^2)\over{b^2+x^2}}dx,$$ based on the convergence theorem, and then I tried $${df(a)\over da}=\int_0^\infty {2a\over {(a^2+x^2)(b^2+x^2)}}dx = {\pi\over b(b+a)},$$and then $$f(a)={\pi\over b}\ln(b+a)+C,$$where $C$ is a constant, but I don't know how to find the constant $C$. Could anyone tell me that, and explain why? Or could anyone find other methods to evaluate the integral? If you could, please explain. Thanks.",,"['calculus', 'real-analysis', 'integration', 'improper-integrals']"
99,Evaluate $\int_{-\pi}^\pi \big|\sum^\infty_{n=1} \frac{1}{2^n} e^{inx}\big|^2 \operatorname d\!x$,Evaluate,\int_{-\pi}^\pi \big|\sum^\infty_{n=1} \frac{1}{2^n} e^{inx}\big|^2 \operatorname d\!x,"I am trying to solve exercises for the coming exam, and I am stuck on this exercise: Evaluate the integral $$\int_{-\pi}^\pi \Big|\sum^\infty_{n=1} \frac{1}{2^n} \mathrm{e}^{inx}\,\Big|^2 \operatorname d\!x$$ A page before it intoduced the Parseval's identity , so I guess it is related to it. I tried to solve it, but whan ever it try is bad. Can you please give me some hints? Thanks!","I am trying to solve exercises for the coming exam, and I am stuck on this exercise: Evaluate the integral $$\int_{-\pi}^\pi \Big|\sum^\infty_{n=1} \frac{1}{2^n} \mathrm{e}^{inx}\,\Big|^2 \operatorname d\!x$$ A page before it intoduced the Parseval's identity , so I guess it is related to it. I tried to solve it, but whan ever it try is bad. Can you please give me some hints? Thanks!",,"['calculus', 'real-analysis', 'sequences-and-series', 'definite-integrals', 'fourier-series']"
