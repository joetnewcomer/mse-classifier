,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Properties of projection matrix for linear models,Properties of projection matrix for linear models,,"1) Let's say we have the linear model $y=Xβ$. Suppose the model has full rank $X$ and projection matrix $P_X$, show that $P_XX = X$ and that $C(P_X) = C(X)$. 2) Suppose that $P_1$ and $P_2$ are projections for the model $Q_1$ nested within model $Q_2$. Show that $P_1$, $P_2 - P_1$, and $I-P_1$ are all projection matrices and are all mutually orthogonal. I've been having some real difficulties with these 2 problems. I think the first one might be simple (I have a guess for the column space equivalency) but I don't even know where to start for the second one-the mutual orthogonality is what gets me. I'd appreciate any help I can get.","1) Let's say we have the linear model $y=Xβ$. Suppose the model has full rank $X$ and projection matrix $P_X$, show that $P_XX = X$ and that $C(P_X) = C(X)$. 2) Suppose that $P_1$ and $P_2$ are projections for the model $Q_1$ nested within model $Q_2$. Show that $P_1$, $P_2 - P_1$, and $I-P_1$ are all projection matrices and are all mutually orthogonal. I've been having some real difficulties with these 2 problems. I think the first one might be simple (I have a guess for the column space equivalency) but I don't even know where to start for the second one-the mutual orthogonality is what gets me. I'd appreciate any help I can get.",,"['linear-algebra', 'matrices', 'statistics', 'vector-spaces', 'regression']"
1,"What do local coordinates look like for the quotient space $\operatorname{SL}(2, R) / \operatorname{SL}(2, Z)$?",What do local coordinates look like for the quotient space ?,"\operatorname{SL}(2, R) / \operatorname{SL}(2, Z)","OK, let me see first if what I understand is correct. So start with $\operatorname{SL}(2, \mathbb{R})$. This is literally the subset $$\{\begin{pmatrix}a & b \\ c & d\end{pmatrix} ``\in"" \mathbb{R}^4 \mid a d - b c = 1\}.$$ It inherits a subspace topology from $\mathbb{R}^4$, and that is fine. Now let me see if I got the coordinate system correctly. One of $a, b, c, d$ will be non-zero, say $a$. In that case, $d = \frac{1 + bc}{a}$. Let $\epsilon > 0$ be small enough so that $0 \not\in [a - \epsilon, a + \epsilon]$, and define $p : (a-\epsilon, a + \epsilon) \times (-\epsilon, \epsilon) \times (-\epsilon, \epsilon) \to \operatorname{SL}(2, \mathbb{R})$ by $(x, y, z) \mapsto \begin{pmatrix} x & y \\ z & \frac{1 + yz}{x}\end{pmatrix}$. This defines a coordinate map around any $\begin{pmatrix} a & b \\ c & d\end{pmatrix} \in \operatorname{SL}(2, \mathbb{R})$. I have not done the arithmetic, but I believe that considering the different cases will show that the transition maps are diffeomorphisms, and hence that what we are dealing with is a 3-manifold. There must be nicer coordinate systems, but I digress. Now, we take the quotient of $\operatorname{SL}(2, \mathbb{R})$ by $\operatorname{SL}(2, \mathbb{Z})$. It gets the quotient topology, and that is also fine. But what about the parametrization? How do we give it local coordinates? Actually, what does this space look like (geometrically speaking)?","OK, let me see first if what I understand is correct. So start with $\operatorname{SL}(2, \mathbb{R})$. This is literally the subset $$\{\begin{pmatrix}a & b \\ c & d\end{pmatrix} ``\in"" \mathbb{R}^4 \mid a d - b c = 1\}.$$ It inherits a subspace topology from $\mathbb{R}^4$, and that is fine. Now let me see if I got the coordinate system correctly. One of $a, b, c, d$ will be non-zero, say $a$. In that case, $d = \frac{1 + bc}{a}$. Let $\epsilon > 0$ be small enough so that $0 \not\in [a - \epsilon, a + \epsilon]$, and define $p : (a-\epsilon, a + \epsilon) \times (-\epsilon, \epsilon) \times (-\epsilon, \epsilon) \to \operatorname{SL}(2, \mathbb{R})$ by $(x, y, z) \mapsto \begin{pmatrix} x & y \\ z & \frac{1 + yz}{x}\end{pmatrix}$. This defines a coordinate map around any $\begin{pmatrix} a & b \\ c & d\end{pmatrix} \in \operatorname{SL}(2, \mathbb{R})$. I have not done the arithmetic, but I believe that considering the different cases will show that the transition maps are diffeomorphisms, and hence that what we are dealing with is a 3-manifold. There must be nicer coordinate systems, but I digress. Now, we take the quotient of $\operatorname{SL}(2, \mathbb{R})$ by $\operatorname{SL}(2, \mathbb{Z})$. It gets the quotient topology, and that is also fine. But what about the parametrization? How do we give it local coordinates? Actually, what does this space look like (geometrically speaking)?",,"['matrices', 'geometry']"
2,Minimizers of polynomial functions?,Minimizers of polynomial functions?,,"Let $p: \mathbb{R}^2 \to \mathbb{R} $ be a polynomial function of two real variables. Suppose that (P) $\; \; \; \; \; \; p(x,y) \ge 0, \; \; \forall \; (x,y) \in \mathbb{R}^2 $ 1: Prove that if $p$ is of degree two, it has a minimizer over $\mathbb{R}$. 2: Prove that if a polynomial function $p$ verifies property (P) its degree is necessarily even. 3: Suppose that degree $p=2n$ with $n \gt 1  $. Does property (P) imply the existence of a minimizer for $p$ over $\mathbb{R}$? $$\\ \\$$ My ideas: Since $f(x,y)$ is a degree two polynomial, it'd be of the form $$ f(x,y) = ax^2 +by^2 + cx + dy + kxy + l  $$ Taking all the second derivatives to compute the Hessian gives: $$\left\lbrack \matrix{2a & k\cr k & 2b} \right\rbrack$$ My hunch is that since the function has to be equal or greater than zero, $a, b$ and $k$ have to be non-negative, which would mean the Hessian is always non-negative, which I think would mean that there has to be a minimizer. I'm not really sure about the other parts, but I think that the answer to part ""3"" is YES. At least a local minimizer I think","Let $p: \mathbb{R}^2 \to \mathbb{R} $ be a polynomial function of two real variables. Suppose that (P) $\; \; \; \; \; \; p(x,y) \ge 0, \; \; \forall \; (x,y) \in \mathbb{R}^2 $ 1: Prove that if $p$ is of degree two, it has a minimizer over $\mathbb{R}$. 2: Prove that if a polynomial function $p$ verifies property (P) its degree is necessarily even. 3: Suppose that degree $p=2n$ with $n \gt 1  $. Does property (P) imply the existence of a minimizer for $p$ over $\mathbb{R}$? $$\\ \\$$ My ideas: Since $f(x,y)$ is a degree two polynomial, it'd be of the form $$ f(x,y) = ax^2 +by^2 + cx + dy + kxy + l  $$ Taking all the second derivatives to compute the Hessian gives: $$\left\lbrack \matrix{2a & k\cr k & 2b} \right\rbrack$$ My hunch is that since the function has to be equal or greater than zero, $a, b$ and $k$ have to be non-negative, which would mean the Hessian is always non-negative, which I think would mean that there has to be a minimizer. I'm not really sure about the other parts, but I think that the answer to part ""3"" is YES. At least a local minimizer I think",,"['matrices', 'polynomials', 'optimization']"
3,When would you do matrix addition and multiplication to the same matrix?,When would you do matrix addition and multiplication to the same matrix?,,"Matrix addition, where we add corresponding elements to eachother, is much more similar to scalar addition, than matrix multiplication, where we add linear combinations of sub-vectors of the matrices, is to scalar multiplication. It seems to me that matrix multiplication and addition are used for completely different purposes. The typical application of matrix multiplication is to matrices that represent a system of linear equations, but the typical application of matrix addition is to matrices that represent quantities of variables that have some 2 dimensional relation to eachother. An  example of the latter is a matrix who's row represents gender, and who's column represents an age group, and where each entry represents the amount of people in that age group and gender. This example does not represent a system of linear equations, so matrix multiplication seems to be meaningless. So my question is, is there ever a situation when the matrix addition and multiplication operation are both meaningful on a certain matrix?","Matrix addition, where we add corresponding elements to eachother, is much more similar to scalar addition, than matrix multiplication, where we add linear combinations of sub-vectors of the matrices, is to scalar multiplication. It seems to me that matrix multiplication and addition are used for completely different purposes. The typical application of matrix multiplication is to matrices that represent a system of linear equations, but the typical application of matrix addition is to matrices that represent quantities of variables that have some 2 dimensional relation to eachother. An  example of the latter is a matrix who's row represents gender, and who's column represents an age group, and where each entry represents the amount of people in that age group and gender. This example does not represent a system of linear equations, so matrix multiplication seems to be meaningless. So my question is, is there ever a situation when the matrix addition and multiplication operation are both meaningful on a certain matrix?",,['matrices']
4,Is $\sqrt{\lambda_{\max}(A^2)} = \lambda_{\max}(A)$?,Is ?,\sqrt{\lambda_{\max}(A^2)} = \lambda_{\max}(A),"I'm trying to prove a statement on the condition number of a matrix in the 2-norm for a symmetric positive definite matrix $A$. I nearly have the proof completed, if $\sqrt{\lambda_{\max}(A^2)} = \lambda_{\max}(A)$ then the proof is finished. But I'm not sure if this is the case? Is $\sqrt{\lambda_{\max}(A^2)} = \lambda_{\max}(A)$ for a symmetric positive definite matrix?","I'm trying to prove a statement on the condition number of a matrix in the 2-norm for a symmetric positive definite matrix $A$. I nearly have the proof completed, if $\sqrt{\lambda_{\max}(A^2)} = \lambda_{\max}(A)$ then the proof is finished. But I'm not sure if this is the case? Is $\sqrt{\lambda_{\max}(A^2)} = \lambda_{\max}(A)$ for a symmetric positive definite matrix?",,"['linear-algebra', 'matrices']"
5,Householder's transformations of space,Householder's transformations of space,,"Let we have two vectors $u, v \in \mathbb{R}^n$ such that the angle between them is acute ($u^Tv > 0$). I want to prove that there are such unitary transformation, that maps this vector to the first octant. Moreover I want to prove that such map can be represented as a product of two Householder reflections. My idea is that if we choose a 2-dimentional plane containing vectors $(1, 0,0,...,0)^T$ and $(0, \frac{1}{\sqrt{n-1}}, \frac{1}{\sqrt{n-1}}, ..., \frac{1}{\sqrt{n-1}})^T$ and map vectors $u$ and $v$ such way that they lies in this plane and bisetor between $u$ and $v$ maps to vector $(\frac{1}{2}, \frac{1}{2\sqrt{n-1}}, \frac{1}{2\sqrt{n-1}}, ..., \frac{1}{2\sqrt{n-1}})^T$ then we get necessary transform. So because of length are reserved, this map is unitary. This is proof of existing such map. But I still can not find such two Householder matrices that represent it. Thanks for the help!","Let we have two vectors $u, v \in \mathbb{R}^n$ such that the angle between them is acute ($u^Tv > 0$). I want to prove that there are such unitary transformation, that maps this vector to the first octant. Moreover I want to prove that such map can be represented as a product of two Householder reflections. My idea is that if we choose a 2-dimentional plane containing vectors $(1, 0,0,...,0)^T$ and $(0, \frac{1}{\sqrt{n-1}}, \frac{1}{\sqrt{n-1}}, ..., \frac{1}{\sqrt{n-1}})^T$ and map vectors $u$ and $v$ such way that they lies in this plane and bisetor between $u$ and $v$ maps to vector $(\frac{1}{2}, \frac{1}{2\sqrt{n-1}}, \frac{1}{2\sqrt{n-1}}, ..., \frac{1}{2\sqrt{n-1}})^T$ then we get necessary transform. So because of length are reserved, this map is unitary. This is proof of existing such map. But I still can not find such two Householder matrices that represent it. Thanks for the help!",,"['linear-algebra', 'matrices', 'linear-transformations']"
6,show that an orthogonal matrix $Q$ that maximizes $f(Q)=tr(QB)$ satisfies $ Q = \sqrt{B^{\top}B}^{-1}B^{\top} = B^{\top}\sqrt{BB^{\top}}^{-1}$,show that an orthogonal matrix  that maximizes  satisfies,Q f(Q)=tr(QB)  Q = \sqrt{B^{\top}B}^{-1}B^{\top} = B^{\top}\sqrt{BB^{\top}}^{-1},"I can't solve the following problem. In this problem, matrices are supposed to be over the real numers. Let $A$ be a symmetric positive definite matrix of order $n$ and $B$ be a nonsingular matrix of order $n$. there exists a unique symmetric positive definite matrix $R$ such that $R^2=A$. We denote such R by $\sqrt{A}$. Show that an orthogonal matrix $Q$ that maximizes $f(Q)=tr(QB)$ satisfies $$ Q = \sqrt{B^{\top}B}^{-1}B^{\top} = B^{\top}\sqrt{BB^{\top}}^{-1}.$$","I can't solve the following problem. In this problem, matrices are supposed to be over the real numers. Let $A$ be a symmetric positive definite matrix of order $n$ and $B$ be a nonsingular matrix of order $n$. there exists a unique symmetric positive definite matrix $R$ such that $R^2=A$. We denote such R by $\sqrt{A}$. Show that an orthogonal matrix $Q$ that maximizes $f(Q)=tr(QB)$ satisfies $$ Q = \sqrt{B^{\top}B}^{-1}B^{\top} = B^{\top}\sqrt{BB^{\top}}^{-1}.$$",,"['linear-algebra', 'matrices']"
7,"Find $\sum\limits_{k=0}^{n-1}\,\omega_n^{k^2\ell}$, where $\omega_n:=\exp\left(\frac{2\pi\text{i}}{n}\right)$.","Find , where .","\sum\limits_{k=0}^{n-1}\,\omega_n^{k^2\ell} \omega_n:=\exp\left(\frac{2\pi\text{i}}{n}\right)","Let $n$ be a positive integer.  Define the Gauss sum $$g(\ell,n):=\sum_{k=0}^{n-1}\,\omega_n^{k^2\ell}=\sum_{k=0}^{n-1}\,\exp\left(\frac{2\pi\text{i}k^2\ell}{n}\right)\,,$$   for every integer $\ell$.  Here, $\omega_n$ is the primitive $n$-th root of unity $\exp\left(\frac{2\pi\text{i}}{n}\right)$, and $\text{i}$ is the imaginary unit $\sqrt{-1}$.   From my observation (using Mathematica), for every integer $n>0$, we have   $$\frac{g(1,n)}{\sqrt{n}}=\begin{cases} 1+\text{i}\,,&\mbox{if }n\equiv 0\pmod{4}\,, \\ 1\,,&\mbox{if }n\equiv 1\pmod{4}\,, \\ 0\,,&\mbox{if }n\equiv 2\pmod{4}\,, \\ \text{i}\,,&\mbox{if }n\equiv 3\pmod{4}\,. \end{cases}\tag{*}$$   The question is to compute $g(\ell,n)$ for general $\ell$ and $n$. Note also that, for every $\ell\in\mathbb{Z}$, $g(\ell+n,n)=g(\ell,n)$ and $g(-\ell,n)=\overline{g(\ell,n)}$, where $\bar{z}$ is the complex conjugate of $z\in\mathbb{C}$.  Thus, it suffices to evaluate $g(\ell,n)$ for integers $\ell$ with $0\leq \ell\leq \frac{n}{2}$.  The value of $g(\ell,n)$ is of great interest for integers $\ell$ with $\gcd(\ell,n)=1$. If the claim (*) is true, then I can find the multiplicities of the eigenvalues of the (inverse) discrete Fourier transform matrix $\mathbf{A}$ in my answer here with $\omega:=\omega_n$.  That is, $+\sqrt{n}$ will have multiplicity $\left\lfloor\frac{n+4}{4}\right\rfloor$, $-\sqrt{n}$ will be of multiplicity $\left\lfloor\frac{n+2}{4}\right\rfloor$, $+\sqrt{n}\text{i}$ will be of multiplicity $\left\lfloor\frac{n+1}{4}\right\rfloor$, and $-\sqrt{n}\text{i}$ will be of multiplicity $\left\lfloor\frac{n-1}{4}\right\rfloor$.  I am sure there are different approaches of getting these multiplicities, but I would like to see how the sum above is evaluated. Notes: The only case in (*) I am able to prove is when $n\equiv 2\pmod{4}$.  That is because $$\omega^{(k+n/2)^2}=-\omega^{k^2}$$ for any primitive $n$-th root of unity $\omega$ and for all $k=0,1,2,\ldots,n-1$.  Consequently, if $n\equiv 2\pmod{4}$, then $g(\ell,n)=0$ for all $\ell\in\mathbb{Z}$ with $\gcd(\ell,n)=1$. i707107 has provided me with a great reference.  Hence, I no longer need a proof for (*), except a different proof.  However, I will still greatly appreciate if anybody can determine $g(\ell,n)$ for $\ell\not\equiv\pm1\pmod{n}$. As arthur's link shows, if $n:=p$ is an odd prime and $p\nmid \ell$, then  $$g(\ell,p)=\left(\frac{\ell}{p}\right)\,g(1,p)\,,$$ where $\left(\frac{\ell}{p}\right)$ denotes the Legendre symbol of $\ell$ modulo $p$.","Let $n$ be a positive integer.  Define the Gauss sum $$g(\ell,n):=\sum_{k=0}^{n-1}\,\omega_n^{k^2\ell}=\sum_{k=0}^{n-1}\,\exp\left(\frac{2\pi\text{i}k^2\ell}{n}\right)\,,$$   for every integer $\ell$.  Here, $\omega_n$ is the primitive $n$-th root of unity $\exp\left(\frac{2\pi\text{i}}{n}\right)$, and $\text{i}$ is the imaginary unit $\sqrt{-1}$.   From my observation (using Mathematica), for every integer $n>0$, we have   $$\frac{g(1,n)}{\sqrt{n}}=\begin{cases} 1+\text{i}\,,&\mbox{if }n\equiv 0\pmod{4}\,, \\ 1\,,&\mbox{if }n\equiv 1\pmod{4}\,, \\ 0\,,&\mbox{if }n\equiv 2\pmod{4}\,, \\ \text{i}\,,&\mbox{if }n\equiv 3\pmod{4}\,. \end{cases}\tag{*}$$   The question is to compute $g(\ell,n)$ for general $\ell$ and $n$. Note also that, for every $\ell\in\mathbb{Z}$, $g(\ell+n,n)=g(\ell,n)$ and $g(-\ell,n)=\overline{g(\ell,n)}$, where $\bar{z}$ is the complex conjugate of $z\in\mathbb{C}$.  Thus, it suffices to evaluate $g(\ell,n)$ for integers $\ell$ with $0\leq \ell\leq \frac{n}{2}$.  The value of $g(\ell,n)$ is of great interest for integers $\ell$ with $\gcd(\ell,n)=1$. If the claim (*) is true, then I can find the multiplicities of the eigenvalues of the (inverse) discrete Fourier transform matrix $\mathbf{A}$ in my answer here with $\omega:=\omega_n$.  That is, $+\sqrt{n}$ will have multiplicity $\left\lfloor\frac{n+4}{4}\right\rfloor$, $-\sqrt{n}$ will be of multiplicity $\left\lfloor\frac{n+2}{4}\right\rfloor$, $+\sqrt{n}\text{i}$ will be of multiplicity $\left\lfloor\frac{n+1}{4}\right\rfloor$, and $-\sqrt{n}\text{i}$ will be of multiplicity $\left\lfloor\frac{n-1}{4}\right\rfloor$.  I am sure there are different approaches of getting these multiplicities, but I would like to see how the sum above is evaluated. Notes: The only case in (*) I am able to prove is when $n\equiv 2\pmod{4}$.  That is because $$\omega^{(k+n/2)^2}=-\omega^{k^2}$$ for any primitive $n$-th root of unity $\omega$ and for all $k=0,1,2,\ldots,n-1$.  Consequently, if $n\equiv 2\pmod{4}$, then $g(\ell,n)=0$ for all $\ell\in\mathbb{Z}$ with $\gcd(\ell,n)=1$. i707107 has provided me with a great reference.  Hence, I no longer need a proof for (*), except a different proof.  However, I will still greatly appreciate if anybody can determine $g(\ell,n)$ for $\ell\not\equiv\pm1\pmod{n}$. As arthur's link shows, if $n:=p$ is an odd prime and $p\nmid \ell$, then  $$g(\ell,p)=\left(\frac{\ell}{p}\right)\,g(1,p)\,,$$ where $\left(\frac{\ell}{p}\right)$ denotes the Legendre symbol of $\ell$ modulo $p$.",,"['matrices', 'complex-numbers', 'summation', 'eigenvalues-eigenvectors', 'roots-of-unity']"
8,What can be said about a convex combination of orthogonal matrices?,What can be said about a convex combination of orthogonal matrices?,,"Let $A$ and $B$ be two orthogonal matrices (of order $n\geqslant 2$ ) such that $$\det A=1 \qquad\qquad \det B=-1$$ Can we say that: there is $\lambda \in [0,1]$ such that $\lambda A + (1-\lambda) B$ defines a projection operator? there is $\lambda \in [0,1]$ such that $\lambda A + (1-\lambda) B$ is a singular matrix? I was wondering why can't we say that every orthogonal matrix is a projection matrix that projects onto $\mathbb{R^n}$ (so that the first question is answered by $\lambda = 0$ or $\lambda = 1$ )?",Let and be two orthogonal matrices (of order ) such that Can we say that: there is such that defines a projection operator? there is such that is a singular matrix? I was wondering why can't we say that every orthogonal matrix is a projection matrix that projects onto (so that the first question is answered by or )?,"A B n\geqslant 2 \det A=1 \qquad\qquad \det B=-1 \lambda \in [0,1] \lambda A + (1-\lambda) B \lambda \in [0,1] \lambda A + (1-\lambda) B \mathbb{R^n} \lambda = 0 \lambda = 1","['linear-algebra', 'matrices', 'orthogonal-matrices']"
9,SVD and non-negative matrix factorization,SVD and non-negative matrix factorization,,"The SVD and NMF are seem to be very close, so the question: how can I obtain NMF of given matrix from its SVD decomposition? I've tried to zero-in all negative parts of SVD decomposition, but this gives bad results and iterative approach (zero-in, correct, zero-in) does not help either. Am I missing something or there is no meaningful relation between SVD and NMF?","The SVD and NMF are seem to be very close, so the question: how can I obtain NMF of given matrix from its SVD decomposition? I've tried to zero-in all negative parts of SVD decomposition, but this gives bad results and iterative approach (zero-in, correct, zero-in) does not help either. Am I missing something or there is no meaningful relation between SVD and NMF?",,"['linear-algebra', 'matrices', 'optimization', 'eigenvalues-eigenvectors', 'svd']"
10,Solving the quadratic formula to determine stability of a system,Solving the quadratic formula to determine stability of a system,,"I am trying to solve the $2\times 2$ matrix $$\begin{bmatrix} 0 &1 \\ -k &-b \end{bmatrix}$$ for a relationship between the variables $k$ and $b$ to determine when a system is stable. Stability means that the real component of the eigenvalues are all $0$ or less. I have sought the eigenvalues and have gotten to the quadratic formula here: eigens $= \frac{-b \pm \sqrt{b^2-4k}}{2}$ I know that the eigenvalues must be less than $0$, but I am having trouble with the algebra to get to a clear relationship. Any tips?","I am trying to solve the $2\times 2$ matrix $$\begin{bmatrix} 0 &1 \\ -k &-b \end{bmatrix}$$ for a relationship between the variables $k$ and $b$ to determine when a system is stable. Stability means that the real component of the eigenvalues are all $0$ or less. I have sought the eigenvalues and have gotten to the quadratic formula here: eigens $= \frac{-b \pm \sqrt{b^2-4k}}{2}$ I know that the eigenvalues must be less than $0$, but I am having trouble with the algebra to get to a clear relationship. Any tips?",,"['matrices', 'eigenvalues-eigenvectors', 'quadratics']"
11,"Solve linear system with $A_{i,j} = \langle e_i, e_j\rangle^2$, edges of a triangle","Solve linear system with , edges of a triangle","A_{i,j} = \langle e_i, e_j\rangle^2","I have three vectors in $e_i\in\mathbb{R}^3$ that form a triangle. Let us consider now the linear equation system $Ax=b$ with $$ A_{i,j} = \langle e_i, e_j\rangle^2,\\ b_i = \langle e_i, e_i\rangle. $$ I can solve these problems numerically, but somehow I feel I'm missing out. Perhaps the solution can even be constructed explicitly from $e_i$? Any ideas?","I have three vectors in $e_i\in\mathbb{R}^3$ that form a triangle. Let us consider now the linear equation system $Ax=b$ with $$ A_{i,j} = \langle e_i, e_j\rangle^2,\\ b_i = \langle e_i, e_i\rangle. $$ I can solve these problems numerically, but somehow I feel I'm missing out. Perhaps the solution can even be constructed explicitly from $e_i$? Any ideas?",,"['linear-algebra', 'matrices', 'geometry', 'triangles', 'simplex']"
12,Powers of 2 matrices having identical row sums,Powers of 2 matrices having identical row sums,,"I have been working on a research problem and have encountered a situation in which two matrices $A, B \in \mathbb{R}^{n \times n}$ are such that  \begin{equation} A^k \mathbf{1} = B^k \mathbf{1} \quad \forall 0 \leq k \leq n  \end{equation} where $\mathbf{1}$ is the vector of ones. Can I now say anything about the relationship between $A$ and $B$? Obviously, no explicit relationship need exist between the two. If $\mathbf{1}$ were an eigenvector of both, then clearly the equations above hold. Suppose $\mathbf{1}$ weren't an eigenvector of at least one of $A,B$. Is it still possible for the relationships above to hold?","I have been working on a research problem and have encountered a situation in which two matrices $A, B \in \mathbb{R}^{n \times n}$ are such that  \begin{equation} A^k \mathbf{1} = B^k \mathbf{1} \quad \forall 0 \leq k \leq n  \end{equation} where $\mathbf{1}$ is the vector of ones. Can I now say anything about the relationship between $A$ and $B$? Obviously, no explicit relationship need exist between the two. If $\mathbf{1}$ were an eigenvector of both, then clearly the equations above hold. Suppose $\mathbf{1}$ weren't an eigenvector of at least one of $A,B$. Is it still possible for the relationships above to hold?",,"['matrices', 'eigenvalues-eigenvectors']"
13,Nonsingular matrices with bounded coefficients,Nonsingular matrices with bounded coefficients,,"I can show that there exists $n^2$ positive integers $a_1,\ldots ,a_{n^2}$, such that each $n\times n$ matrix with coefficients $a_i$ (used once and only once) is nonsingular. Two questions: Could we find the smallest integer $M_n$ such that we can state that there exists $n^2$ positive integers $a_1,\ldots ,a_{n^2} \leqslant M_n$, such that each $n\times n$ matrix with coefficients $a_i$ (used once and only once) is nonsingular. What conditions on $a,b\in \mathbb R$ are necessary and sufficient to state that there exists $n^2$ real numbers $a_1,\ldots ,a_{n^2}$ in $[a,b]$, such that each $n\times n$ matrix with coefficients $a_i$ (used once and only once) is non singular. Also, do you know if these properties are used ""somewhere""?","I can show that there exists $n^2$ positive integers $a_1,\ldots ,a_{n^2}$, such that each $n\times n$ matrix with coefficients $a_i$ (used once and only once) is nonsingular. Two questions: Could we find the smallest integer $M_n$ such that we can state that there exists $n^2$ positive integers $a_1,\ldots ,a_{n^2} \leqslant M_n$, such that each $n\times n$ matrix with coefficients $a_i$ (used once and only once) is nonsingular. What conditions on $a,b\in \mathbb R$ are necessary and sufficient to state that there exists $n^2$ real numbers $a_1,\ldots ,a_{n^2}$ in $[a,b]$, such that each $n\times n$ matrix with coefficients $a_i$ (used once and only once) is non singular. Also, do you know if these properties are used ""somewhere""?",,['matrices']
14,Can I factor a rational expression of the form...,Can I factor a rational expression of the form...,,"Given two equations $\displaystyle P_1 = \frac{1-X^2}{1-X^2}$ and $\displaystyle P_2 = \frac{1-aX^2}{1-bX^2}$ I am told that there is a relationship between P1 and P2 $P_1 \geq P_2$ It is clear that by visually comparing P1 and P2 , the coefficients a and b are responsible for determining the relationship between P1 and P2 . That being said, I would like to express the relationship $P_1 \geq P_2$ as $P_1 \geq P_1f(a,b)$ $1 \geq f(a,b)$ Clearly f(a,b) can be defined as P2/P1 , so disregard this solution for the time being. My question is, can I express f(a,b) as a function of a and b alone? In other words, can I factor out P1 from P2 and be left with a function of a and b alone? If so, what is the form for f(a,b) ? Is it a rational expression? Is it a transform matrix? Is it another polynomial? A convolution? My objective is to evaluate f(a,b) , thus interpret the relationship between P1 and P2 . In other words if f(a,b)=1 then I know P1=P2 . I don't think I can make it any more clear. If I can't extract an expression for f(a,b) as a function of a and b alone, that's fine. I'm under the impression I can't but I thought someone on here might know more than I, so I figured I would ask.","Given two equations $\displaystyle P_1 = \frac{1-X^2}{1-X^2}$ and $\displaystyle P_2 = \frac{1-aX^2}{1-bX^2}$ I am told that there is a relationship between P1 and P2 $P_1 \geq P_2$ It is clear that by visually comparing P1 and P2 , the coefficients a and b are responsible for determining the relationship between P1 and P2 . That being said, I would like to express the relationship $P_1 \geq P_2$ as $P_1 \geq P_1f(a,b)$ $1 \geq f(a,b)$ Clearly f(a,b) can be defined as P2/P1 , so disregard this solution for the time being. My question is, can I express f(a,b) as a function of a and b alone? In other words, can I factor out P1 from P2 and be left with a function of a and b alone? If so, what is the form for f(a,b) ? Is it a rational expression? Is it a transform matrix? Is it another polynomial? A convolution? My objective is to evaluate f(a,b) , thus interpret the relationship between P1 and P2 . In other words if f(a,b)=1 then I know P1=P2 . I don't think I can make it any more clear. If I can't extract an expression for f(a,b) as a function of a and b alone, that's fine. I'm under the impression I can't but I thought someone on here might know more than I, so I figured I would ask.",,"['linear-algebra', 'matrices', 'linear-transformations', 'matrix-decomposition']"
15,General solution of a vectorial ecuation,General solution of a vectorial ecuation,,"This is the first time that I ask a question here. When I was looking for the maximum of a multivariable vector function, I encountered the following problem: I cannot find the general solution of the following vectorial equation: $$ \boldsymbol{A}^t \boldsymbol{A} \boldsymbol{z} - \frac{1-q}{\phi} \boldsymbol{z}^t \boldsymbol{A}^t \boldsymbol{A} \boldsymbol{z} . \boldsymbol{z} = \boldsymbol{0} , $$ where the matrix $\boldsymbol{A} \in \mathbb{R}^{p \times (p+1)}$, the unknown vector $ \boldsymbol{z} = (x, \boldsymbol{y}^t)^t \in \mathbb{R}^{p+1}$ given that $x \in \mathbb{R}$ and $\boldsymbol{y} \in \mathbb{R}^{p})$, $q \in (0 , 1)$ and $\phi >0$. I could solve it for the case $ p =1$. In this case $\boldsymbol{A} = (a_1, a_2)$, $\boldsymbol{z} = (x, y)^t$ and the solutions are the point $ \sqrt{\frac{\phi}{(1-q) (a_1^2 + a_2^2)}} . (a_1, a_2)^t$, its opposite and all the points of the line $ y = - \frac{a_1}{a_2} x$. My problem is that no matter how much I try I cannot generalize it for $p>1$. Any advice?","This is the first time that I ask a question here. When I was looking for the maximum of a multivariable vector function, I encountered the following problem: I cannot find the general solution of the following vectorial equation: $$ \boldsymbol{A}^t \boldsymbol{A} \boldsymbol{z} - \frac{1-q}{\phi} \boldsymbol{z}^t \boldsymbol{A}^t \boldsymbol{A} \boldsymbol{z} . \boldsymbol{z} = \boldsymbol{0} , $$ where the matrix $\boldsymbol{A} \in \mathbb{R}^{p \times (p+1)}$, the unknown vector $ \boldsymbol{z} = (x, \boldsymbol{y}^t)^t \in \mathbb{R}^{p+1}$ given that $x \in \mathbb{R}$ and $\boldsymbol{y} \in \mathbb{R}^{p})$, $q \in (0 , 1)$ and $\phi >0$. I could solve it for the case $ p =1$. In this case $\boldsymbol{A} = (a_1, a_2)$, $\boldsymbol{z} = (x, y)^t$ and the solutions are the point $ \sqrt{\frac{\phi}{(1-q) (a_1^2 + a_2^2)}} . (a_1, a_2)^t$, its opposite and all the points of the line $ y = - \frac{a_1}{a_2} x$. My problem is that no matter how much I try I cannot generalize it for $p>1$. Any advice?",,['matrices']
16,"Let $A$=[$a_{ij}$]$_{n x n}$ where $a_{ii}$=$1$, $i=\overline {1,n}$, $a_{ij}=a\not=1, i\not=j$. Find $A^n$, $n\in \mathbb N$","Let =[] where =, , . Find ,","A a_{ij} _{n x n} a_{ii} 1 i=\overline {1,n} a_{ij}=a\not=1, i\not=j A^n n\in \mathbb N","Problem: Let $A$=[$a_{ij}$]$_{n x n}$ where $a_{ii}$=$1$, $i=\overline {1,n}$, $a_{ij}=a\not=1, i\not=j$. Find $A^n$, $n\in \mathbb N$ . I need a little help solving this problem. Now, I know how to find $A^n$ if this was matrix 2 by 2, or 3 by 3.  I know there is a way to find the eigenvalues and the eigenvectors and form the matrices $P$ (made of eigenvectors) and $D=diag(\lambda_1,...,\lambda_n)$ using $A^n$=$PD^n$$P^{-1}$ (correct me if I am wrong). However, I do not know how to find the eigenvalues and eigen vectors. So, I tried this: My matrix $A$ would have this form:$$A=         \begin{bmatrix}         1 & a & a & \cdots &a \\         a & 1 & a &\cdots &a \\         \vdots & \vdots &\vdots &\ddots&\vdots\\     a & a & a &\cdots &1 \\         \end{bmatrix} $$ So, that is symmetric matrix. If I look at my rows, I see that sum of all elements in each row is $(n-1)a+1$. Could I then say that my eigenvalue is $(n-1)a+1$ ? I was thinking about using $A^n\overrightarrow v=\lambda^n \overrightarrow v$, where $\overrightarrow v$ is my eigenvector, to get the result but I am really not sure how to go from here. Any help is greatly appreciated.","Problem: Let $A$=[$a_{ij}$]$_{n x n}$ where $a_{ii}$=$1$, $i=\overline {1,n}$, $a_{ij}=a\not=1, i\not=j$. Find $A^n$, $n\in \mathbb N$ . I need a little help solving this problem. Now, I know how to find $A^n$ if this was matrix 2 by 2, or 3 by 3.  I know there is a way to find the eigenvalues and the eigenvectors and form the matrices $P$ (made of eigenvectors) and $D=diag(\lambda_1,...,\lambda_n)$ using $A^n$=$PD^n$$P^{-1}$ (correct me if I am wrong). However, I do not know how to find the eigenvalues and eigen vectors. So, I tried this: My matrix $A$ would have this form:$$A=         \begin{bmatrix}         1 & a & a & \cdots &a \\         a & 1 & a &\cdots &a \\         \vdots & \vdots &\vdots &\ddots&\vdots\\     a & a & a &\cdots &1 \\         \end{bmatrix} $$ So, that is symmetric matrix. If I look at my rows, I see that sum of all elements in each row is $(n-1)a+1$. Could I then say that my eigenvalue is $(n-1)a+1$ ? I was thinking about using $A^n\overrightarrow v=\lambda^n \overrightarrow v$, where $\overrightarrow v$ is my eigenvector, to get the result but I am really not sure how to go from here. Any help is greatly appreciated.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
17,"Finding the coordinate vector of a 2x2 matrix in a basis of 2, 2x2 matrices","Finding the coordinate vector of a 2x2 matrix in a basis of 2, 2x2 matrices",,"Set M to be the set of all matrices of the form: $\bigl( \begin{smallmatrix} 0 & b \\ c & 0 \end{smallmatrix} \bigr)$, with b, c being real numbers. The basis for M (given) is $\epsilon$ = $\{[\begin{smallmatrix} 0 & 1 \\ 0 & 0 \end{smallmatrix}] ,[\begin{smallmatrix} 0 & 0 \\ 1 & 0 \end{smallmatrix}] \}$, with the elements referred to as $\epsilon = \{e_2, e_3\}$ Find the coordinate vector of A = $[\begin{smallmatrix} 0 & 2 \\ 70 & 0 \end{smallmatrix}]$ in the basis $\epsilon$. I'm not really sure how to approach this problem, as I've never seen matrices being used as parts of a basis. I read on this question that I can think of these as 4-tuples, so there's nothing ""special"" going on, so is the answer simply: $[A]_\epsilon$ = $[\begin{smallmatrix} 2 \\ 70 \end{smallmatrix}]$ since $A = 2e_2 + 70e_3$?","Set M to be the set of all matrices of the form: $\bigl( \begin{smallmatrix} 0 & b \\ c & 0 \end{smallmatrix} \bigr)$, with b, c being real numbers. The basis for M (given) is $\epsilon$ = $\{[\begin{smallmatrix} 0 & 1 \\ 0 & 0 \end{smallmatrix}] ,[\begin{smallmatrix} 0 & 0 \\ 1 & 0 \end{smallmatrix}] \}$, with the elements referred to as $\epsilon = \{e_2, e_3\}$ Find the coordinate vector of A = $[\begin{smallmatrix} 0 & 2 \\ 70 & 0 \end{smallmatrix}]$ in the basis $\epsilon$. I'm not really sure how to approach this problem, as I've never seen matrices being used as parts of a basis. I read on this question that I can think of these as 4-tuples, so there's nothing ""special"" going on, so is the answer simply: $[A]_\epsilon$ = $[\begin{smallmatrix} 2 \\ 70 \end{smallmatrix}]$ since $A = 2e_2 + 70e_3$?",,"['linear-algebra', 'matrices']"
18,How to prove this result about the interlacing of eigenvalues.,How to prove this result about the interlacing of eigenvalues.,,"Let $A$ be a real symmetric matrix of order $n$ with eigenvalues $\mu_1\ge \mu_2,\ldots, \mu_{n}$. $B=\begin{bmatrix} x&x&x&x&x \end{bmatrix}$, where $x$ is non zero column vector in $R^n$. Construct a new matrix: $$C=\begin{bmatrix} A&B\\B^T &0 \end{bmatrix}$$ Removing four zeros of $C$ (due to identical columns) arrange the remaining eigenvalues as $\lambda_1\ge \lambda_2,\ldots, \lambda_{n+1}$. How to show that $\lambda_i\ge\mu_i\ge\lambda_{i+1}$ for $i=1,2,\ldots,n$. I know that removing $4$ identical columns, we are left with matrix of order $n+1$ and there this result hold due to interlacing. But eigenvalues of that matrix and of matrix $C$ are different. So , how can I conclude here?","Let $A$ be a real symmetric matrix of order $n$ with eigenvalues $\mu_1\ge \mu_2,\ldots, \mu_{n}$. $B=\begin{bmatrix} x&x&x&x&x \end{bmatrix}$, where $x$ is non zero column vector in $R^n$. Construct a new matrix: $$C=\begin{bmatrix} A&B\\B^T &0 \end{bmatrix}$$ Removing four zeros of $C$ (due to identical columns) arrange the remaining eigenvalues as $\lambda_1\ge \lambda_2,\ldots, \lambda_{n+1}$. How to show that $\lambda_i\ge\mu_i\ge\lambda_{i+1}$ for $i=1,2,\ldots,n$. I know that removing $4$ identical columns, we are left with matrix of order $n+1$ and there this result hold due to interlacing. But eigenvalues of that matrix and of matrix $C$ are different. So , how can I conclude here?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
19,Characterizing orthogonally-invariant norms on the space of matrices,Characterizing orthogonally-invariant norms on the space of matrices,,"Denote by $M_n$ the space of $n \times n$ real matrices. We say a norm on $M_n$ is orthogonal invariant if: $$\|OX \|=\| XO\|=\|X \|  \, \, \forall O \in O_n,X \in M_n$$ I am trying to characterize all such norms. Let $\|\cdot \|$ be one. Using singular values decomposition, we get: $\|X\|=\|U\Sigma V^T\|=\|\Sigma \|$ (when $U,V \in O_n, \Sigma$ is a diagonal with non-negative entries, i.e $U \Sigma V^T$ is a SVD of $X$) So, if we define $f:\mathbb{R}^n \to \mathbb{R}^{\ge 0}$ by $f(\sigma_1,\dots,\sigma_n)=\|\operatorname{diag}(\sigma_1,\dots,\sigma_n) \|$ we get that $\|\cdot\|$ is uniquely determined by $f$. Note that $f$ must be a norm which is invariant under signed permutations, i.e: $f(\pm x_{\tau(1)},\dots,\pm x_{\tau(n)})=f(x_1,\dots,x_n)$ for every $\tau \in S_n$. (The reason is that $\| \cdot\|$ is invariant under orthogonal multiplication, in particular by signed permutation matrices). Question: Is it true that any norm on $\mathbb{R}^n$ which is invariant under signed permutations induces an orhtogonal-invariant norm on $M_n$? (in the obvious way as described above). If not, can we characterize which norms are possible? Added Clarification: Let $f$ be a norm on $\mathbb{R}^n$. The possible candidate for a norm on $M_n$ induced by $f$ is: $\| X\|=\|U\Sigma V^T \|=f(\sigma_1,\dots,\sigma_n)$ where $\Sigma=\operatorname{diag}(\sigma_1,\dots,\sigma_n) $. The non trivial part seems to be verifying the triangle inequality, since SVD does not behave in a structured way (known to me) w.r.t sums. Remarks and partial results: 1) Choosing $f$ to be the maximumn norm induces the Euclidean operator norm. (see here ). 2) Choosing $f$ to be the standard $p$-norm, one gets the $p-$ Schatten norm . 3) Here is a sufficient condition (which is not necessary) inducing a norm:  $$\sum_{i=1}^n z_i \le \sum_{i=1}^n x_i + \sum_{i=1}^n y_i \Rightarrow f(z_1,\dots,z_n) \le f(x_1,\dots,x_n) + f(y_1,\dots,y_n)$$ Any $f$ which satisfies the above condition induces a norm. This follows from Lidskii inequality which says: $$\sum_{i=1}^n \sigma_i(A+B) \le \sum_{i=1}^n \sigma_i(A) + \sum_{i=1}^n \sigma_i(B) $$ Note this condition is not necessary: The maximum norm does not satisfy it, but induces a norm.","Denote by $M_n$ the space of $n \times n$ real matrices. We say a norm on $M_n$ is orthogonal invariant if: $$\|OX \|=\| XO\|=\|X \|  \, \, \forall O \in O_n,X \in M_n$$ I am trying to characterize all such norms. Let $\|\cdot \|$ be one. Using singular values decomposition, we get: $\|X\|=\|U\Sigma V^T\|=\|\Sigma \|$ (when $U,V \in O_n, \Sigma$ is a diagonal with non-negative entries, i.e $U \Sigma V^T$ is a SVD of $X$) So, if we define $f:\mathbb{R}^n \to \mathbb{R}^{\ge 0}$ by $f(\sigma_1,\dots,\sigma_n)=\|\operatorname{diag}(\sigma_1,\dots,\sigma_n) \|$ we get that $\|\cdot\|$ is uniquely determined by $f$. Note that $f$ must be a norm which is invariant under signed permutations, i.e: $f(\pm x_{\tau(1)},\dots,\pm x_{\tau(n)})=f(x_1,\dots,x_n)$ for every $\tau \in S_n$. (The reason is that $\| \cdot\|$ is invariant under orthogonal multiplication, in particular by signed permutation matrices). Question: Is it true that any norm on $\mathbb{R}^n$ which is invariant under signed permutations induces an orhtogonal-invariant norm on $M_n$? (in the obvious way as described above). If not, can we characterize which norms are possible? Added Clarification: Let $f$ be a norm on $\mathbb{R}^n$. The possible candidate for a norm on $M_n$ induced by $f$ is: $\| X\|=\|U\Sigma V^T \|=f(\sigma_1,\dots,\sigma_n)$ where $\Sigma=\operatorname{diag}(\sigma_1,\dots,\sigma_n) $. The non trivial part seems to be verifying the triangle inequality, since SVD does not behave in a structured way (known to me) w.r.t sums. Remarks and partial results: 1) Choosing $f$ to be the maximumn norm induces the Euclidean operator norm. (see here ). 2) Choosing $f$ to be the standard $p$-norm, one gets the $p-$ Schatten norm . 3) Here is a sufficient condition (which is not necessary) inducing a norm:  $$\sum_{i=1}^n z_i \le \sum_{i=1}^n x_i + \sum_{i=1}^n y_i \Rightarrow f(z_1,\dots,z_n) \le f(x_1,\dots,x_n) + f(y_1,\dots,y_n)$$ Any $f$ which satisfies the above condition induces a norm. This follows from Lidskii inequality which says: $$\sum_{i=1}^n \sigma_i(A+B) \le \sum_{i=1}^n \sigma_i(A) + \sum_{i=1}^n \sigma_i(B) $$ Note this condition is not necessary: The maximum norm does not satisfy it, but induces a norm.",,"['linear-algebra', 'matrices', 'normed-spaces', 'symmetry']"
20,Finding eigenvalues of a nearly tridiagonal matrix,Finding eigenvalues of a nearly tridiagonal matrix,,"Consider the $2N\times 2N$ matrix $$A=\begin{pmatrix} a &1  &0&0&0&\ldots&0&1  \\1 &-a&1  & 0 &0 & \ldots & 0&0  \\0 &1&a&1&0 &\cdots &0&0 \\ 0&0&1&-a &1 & \ldots &0&0 \\& & &   \cdots \\ 1&0 &0&0&0&\ldots &1&-a\end{pmatrix}$$ Hopefully the structure is clear, but if not I can clarify further. I am trying to find the eigenvalues of $A$ analytically. There is a lot of literature exclusively on eigenvalues of tridiagonal matrices and circulant matrices, however $A$ is neither exactly circulant nor is it exactly tridiagonal. However it is very close to being both. I have worked out a few cases: For $N=2$, the eigenvalues are $$\lambda_{1,2} = \pm a$$ $$\lambda_{3,4} = \pm \sqrt{a^2+4}$$ For $N = 3$, the eigenvalues are $$\lambda_{1,2} = -\sqrt{1+a^2}$$ $$\lambda_{3,4} = \sqrt{1+a^2}$$ $$\lambda_{5,6} = \pm \sqrt{a^2+4}$$ So it seems there is some sort of 'pattern'. Any ideas on how I would advance?","Consider the $2N\times 2N$ matrix $$A=\begin{pmatrix} a &1  &0&0&0&\ldots&0&1  \\1 &-a&1  & 0 &0 & \ldots & 0&0  \\0 &1&a&1&0 &\cdots &0&0 \\ 0&0&1&-a &1 & \ldots &0&0 \\& & &   \cdots \\ 1&0 &0&0&0&\ldots &1&-a\end{pmatrix}$$ Hopefully the structure is clear, but if not I can clarify further. I am trying to find the eigenvalues of $A$ analytically. There is a lot of literature exclusively on eigenvalues of tridiagonal matrices and circulant matrices, however $A$ is neither exactly circulant nor is it exactly tridiagonal. However it is very close to being both. I have worked out a few cases: For $N=2$, the eigenvalues are $$\lambda_{1,2} = \pm a$$ $$\lambda_{3,4} = \pm \sqrt{a^2+4}$$ For $N = 3$, the eigenvalues are $$\lambda_{1,2} = -\sqrt{1+a^2}$$ $$\lambda_{3,4} = \sqrt{1+a^2}$$ $$\lambda_{5,6} = \pm \sqrt{a^2+4}$$ So it seems there is some sort of 'pattern'. Any ideas on how I would advance?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
21,find 3 angles to rotate vector to align with second vector,find 3 angles to rotate vector to align with second vector,,"First I would like to say that I have seen posts such as that found here: Calculate Rotation Matrix to align Vector A to Vector B in 3d? As well as formulas such as: https://en.wikipedia.org/wiki/Rodrigues%27_rotation_formula What I would like is some method to rotate a given vector, A, around each axis in 3 dimensional space (X, Y, Z) so that it aligns with a second vector B. I am NOT looking for a rotation matrix because, as I understand, this does not actually give you the angles of rotation around the three axis'. If there is a way to extrapolate the angles of rotation from a rotation matrix that would work perfectly. One approach I have tried is to take the two 3D vectors and project them onto a 2D plane, in essence ignoring one of the axis. Once in 2D an attempt was made to find an angle of rotation that would align the now 2D vectors. Using this found angle, a 3D rotation was performed on the original 3D vector around the axis that was extrapolated out during the 2D projection. Repeating this three times, once for each axis. This seemed to work for simple single dimensional vectors but for multidimensional vectors incorrect angles were calculated. By single dimensional vectors working, I am referring to finding the angles of rotation needed to rotate for example {1,0,0} to align with {0,1,0}, and multidimensional vectors that are not working for example would be {1,2,3} to {3,1,2}","First I would like to say that I have seen posts such as that found here: Calculate Rotation Matrix to align Vector A to Vector B in 3d? As well as formulas such as: https://en.wikipedia.org/wiki/Rodrigues%27_rotation_formula What I would like is some method to rotate a given vector, A, around each axis in 3 dimensional space (X, Y, Z) so that it aligns with a second vector B. I am NOT looking for a rotation matrix because, as I understand, this does not actually give you the angles of rotation around the three axis'. If there is a way to extrapolate the angles of rotation from a rotation matrix that would work perfectly. One approach I have tried is to take the two 3D vectors and project them onto a 2D plane, in essence ignoring one of the axis. Once in 2D an attempt was made to find an angle of rotation that would align the now 2D vectors. Using this found angle, a 3D rotation was performed on the original 3D vector around the axis that was extrapolated out during the 2D projection. Repeating this three times, once for each axis. This seemed to work for simple single dimensional vectors but for multidimensional vectors incorrect angles were calculated. By single dimensional vectors working, I am referring to finding the angles of rotation needed to rotate for example {1,0,0} to align with {0,1,0}, and multidimensional vectors that are not working for example would be {1,2,3} to {3,1,2}",,"['matrices', '3d']"
22,Find a matrix for the linear transformation of reflection about a $\theta$ line using the matrix for projection,Find a matrix for the linear transformation of reflection about a  line using the matrix for projection,\theta,"Projection matrix: $$         \begin{pmatrix}         \cos^2\theta & \cos\theta \sin\theta\\         \cos\theta \sin\theta & \sin^2\theta\\         \end{pmatrix} $$ Reflection matrix: $$         \begin{pmatrix}         2\cos^2\theta -1 & 2\cos\theta sin\theta\\         2\cos\theta \sin\theta & 2\sin^2\theta -1\\         \end{pmatrix} $$ which is equivalent to $$         \begin{pmatrix}         \cos2\theta & \sin2\theta\\         \sin2\theta & -\cos2\theta\\         \end{pmatrix} $$ I can arrive at the second form geometrically, but the problem asks that I do it with the projection matrix and vector addition. Any hints/tips? It could also be that the problem is poorly worded, in which case a geometrical analysis is all that is necessary. Find a formula for $R$ based on $P$ . Explain your arguments by drawing a   graph, using the rules of sums of vectors. I drew the graph and was able to successfully explain it in terms of sums of vectors.","Projection matrix: Reflection matrix: which is equivalent to I can arrive at the second form geometrically, but the problem asks that I do it with the projection matrix and vector addition. Any hints/tips? It could also be that the problem is poorly worded, in which case a geometrical analysis is all that is necessary. Find a formula for based on . Explain your arguments by drawing a   graph, using the rules of sums of vectors. I drew the graph and was able to successfully explain it in terms of sums of vectors.","
        \begin{pmatrix}
        \cos^2\theta & \cos\theta \sin\theta\\
        \cos\theta \sin\theta & \sin^2\theta\\
        \end{pmatrix}
 
        \begin{pmatrix}
        2\cos^2\theta -1 & 2\cos\theta sin\theta\\
        2\cos\theta \sin\theta & 2\sin^2\theta -1\\
        \end{pmatrix}
 
        \begin{pmatrix}
        \cos2\theta & \sin2\theta\\
        \sin2\theta & -\cos2\theta\\
        \end{pmatrix}
 R P","['linear-algebra', 'matrices', 'linear-transformations', 'reflection']"
23,Nonnegative determinant of a symmetric matrix,Nonnegative determinant of a symmetric matrix,,"Consider the following matrix with nonnegative entries: $$ M=\begin{pmatrix} a & b & c & d \\ b & c & d & e \\ c & d & e & f \\ d & e & f & g \\ \end{pmatrix}. $$ Can we prove that, if each minor $2\times 2$ has nonnegative determinant, then the determinant of $M$ itself is nonnegative?","Consider the following matrix with nonnegative entries: $$ M=\begin{pmatrix} a & b & c & d \\ b & c & d & e \\ c & d & e & f \\ d & e & f & g \\ \end{pmatrix}. $$ Can we prove that, if each minor $2\times 2$ has nonnegative determinant, then the determinant of $M$ itself is nonnegative?",,"['matrices', 'determinant']"
24,Can transposition $GL_n(k)\to GL_n(k)$ ever be realized as conjugation by some element in $S_n$?,Can transposition  ever be realized as conjugation by some element in ?,GL_n(k)\to GL_n(k) S_n,"Can the transpose operation on $GL_n(k)$ ever be expressed as conjugation by some element of $S_n$, identified as the group of permutation matrices? That is, does there exist $\sigma\in S_n$ such that $A^T=\sigma A\sigma^{-1}$ for all $A$? I'm doubtful, because the tranpositions in $S_n$ act on the right by swapping columns, and act on the left by swapping rows, hence any permutation matrix does some sequence of such swaps. So I don't see a way to flip a column across the main diagonal using just these types of moves, since all the entries in a given column will stay together (as possibly a different column, and in a different ""vertical"" order after swapping rows around.)","Can the transpose operation on $GL_n(k)$ ever be expressed as conjugation by some element of $S_n$, identified as the group of permutation matrices? That is, does there exist $\sigma\in S_n$ such that $A^T=\sigma A\sigma^{-1}$ for all $A$? I'm doubtful, because the tranpositions in $S_n$ act on the right by swapping columns, and act on the left by swapping rows, hence any permutation matrix does some sequence of such swaps. So I don't see a way to flip a column across the main diagonal using just these types of moves, since all the entries in a given column will stay together (as possibly a different column, and in a different ""vertical"" order after swapping rows around.)",,"['abstract-algebra', 'matrices', 'symmetric-groups']"
25,obtaining Bernoulli numbers from determinant,obtaining Bernoulli numbers from determinant,,"I am reading a paper entitled Bernoulli Numbers Via Determinants by Hongwei Chen and I'm confused about a particular step.  The author sets up a system of equations via the following: first let $B_n$ represent the $n$-th Bernoulli number.  Then $$x=(e^x-1)\sum_{n=0}^\infty B_n\frac{x^n}{n!}$$ Letting $b_n=B_n/n!$ and expanding we get $$x=\left(x+\frac{x^2}{2!}+\frac{x^3}{3!}+...\right)\left(b_0+b_1x+b_2x^2+...\right)$$ Immediately we can see that $b_0=1$.  Using Cauchy Products, we then obtain an infinite sequence of equations which are coefficients of the powers of $x$. But we don't need infinite, we can look at the system for coefficients up to $x^n$.   Therefore \begin{cases} b_1=-\frac{1}{2!} \\[2ex] \frac{b_1}{2!}+b_2=-\frac{1}{3!} \\[2ex] \frac{b_1}{3!}+\frac{b_2}{2!}+b_3=-\frac{1}{4!} \\[2ex] \vdots \\[2ex] \frac{b_1}{n!}+\frac{b_2}{(n-1)!}+...+b_n=-\frac{1}{(n+1)!} \\[2ex] \end{cases} Then the author goes on to state that applying Cramers rule produces $$b_n= \begin{vmatrix} 1 & 0 & 0 & \cdots & -\frac{1}{2!} \\ \frac{1}{2!} & 1 & 0 & \cdots & -\frac{1}{3!} \\ \frac{1}{3!} & \frac{1}{2!} & 1 & \cdots & -\frac{1}{4!} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ \frac{1}{n!} & \frac{1}{(n-1)!} & \frac{1}{(n-2)!} & \cdots & -\frac{1}{(n+1)!} \\ \end{vmatrix} $$ Then $$B_n=n! \begin{vmatrix} 1 & 0 & 0 & \cdots & -\frac{1}{2!} \\ \frac{1}{2!} & 1 & 0 & \cdots & -\frac{1}{3!} \\ \frac{1}{3!} & \frac{1}{2!} & 1 & \cdots & -\frac{1}{4!} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ \frac{1}{n!} & \frac{1}{(n-1)!} & \frac{1}{(n-2)!} & \cdots & -\frac{1}{(n+1)!} \\ \end{vmatrix} $$ And finally, $$B_n=(-1)^n n! \begin{vmatrix} \frac{1}{2!} & 1 & 0 & \cdots & 0 \\ \frac{1}{3!} & \frac{1}{2!} & 1 & \cdots & 0 \\ \frac{1}{4!} & \frac{1}{3!} & \frac{1}{2!} & \cdots & 0 \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ \frac{1}{(n+1)!} &\frac{1}{n!} & \frac{1}{(n-1)!} &  \cdots & \frac{1}{2!} \\ \end{vmatrix} $$ Can someone please explain the last step?  I just don't see how that last step comes from the previous.  I figure it has something to do with properties of determinants, and i do know that $|cA|=c^n|A|$, but I don't see where that is coming from here.","I am reading a paper entitled Bernoulli Numbers Via Determinants by Hongwei Chen and I'm confused about a particular step.  The author sets up a system of equations via the following: first let $B_n$ represent the $n$-th Bernoulli number.  Then $$x=(e^x-1)\sum_{n=0}^\infty B_n\frac{x^n}{n!}$$ Letting $b_n=B_n/n!$ and expanding we get $$x=\left(x+\frac{x^2}{2!}+\frac{x^3}{3!}+...\right)\left(b_0+b_1x+b_2x^2+...\right)$$ Immediately we can see that $b_0=1$.  Using Cauchy Products, we then obtain an infinite sequence of equations which are coefficients of the powers of $x$. But we don't need infinite, we can look at the system for coefficients up to $x^n$.   Therefore \begin{cases} b_1=-\frac{1}{2!} \\[2ex] \frac{b_1}{2!}+b_2=-\frac{1}{3!} \\[2ex] \frac{b_1}{3!}+\frac{b_2}{2!}+b_3=-\frac{1}{4!} \\[2ex] \vdots \\[2ex] \frac{b_1}{n!}+\frac{b_2}{(n-1)!}+...+b_n=-\frac{1}{(n+1)!} \\[2ex] \end{cases} Then the author goes on to state that applying Cramers rule produces $$b_n= \begin{vmatrix} 1 & 0 & 0 & \cdots & -\frac{1}{2!} \\ \frac{1}{2!} & 1 & 0 & \cdots & -\frac{1}{3!} \\ \frac{1}{3!} & \frac{1}{2!} & 1 & \cdots & -\frac{1}{4!} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ \frac{1}{n!} & \frac{1}{(n-1)!} & \frac{1}{(n-2)!} & \cdots & -\frac{1}{(n+1)!} \\ \end{vmatrix} $$ Then $$B_n=n! \begin{vmatrix} 1 & 0 & 0 & \cdots & -\frac{1}{2!} \\ \frac{1}{2!} & 1 & 0 & \cdots & -\frac{1}{3!} \\ \frac{1}{3!} & \frac{1}{2!} & 1 & \cdots & -\frac{1}{4!} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ \frac{1}{n!} & \frac{1}{(n-1)!} & \frac{1}{(n-2)!} & \cdots & -\frac{1}{(n+1)!} \\ \end{vmatrix} $$ And finally, $$B_n=(-1)^n n! \begin{vmatrix} \frac{1}{2!} & 1 & 0 & \cdots & 0 \\ \frac{1}{3!} & \frac{1}{2!} & 1 & \cdots & 0 \\ \frac{1}{4!} & \frac{1}{3!} & \frac{1}{2!} & \cdots & 0 \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ \frac{1}{(n+1)!} &\frac{1}{n!} & \frac{1}{(n-1)!} &  \cdots & \frac{1}{2!} \\ \end{vmatrix} $$ Can someone please explain the last step?  I just don't see how that last step comes from the previous.  I figure it has something to do with properties of determinants, and i do know that $|cA|=c^n|A|$, but I don't see where that is coming from here.",,"['linear-algebra', 'matrices', 'systems-of-equations', 'determinant']"
26,Finding the Determinant of a particular Matrix,Finding the Determinant of a particular Matrix,,"I've come across the question of finding the determinant of the $(n\times n)$ matrix, given by $$A:= \begin{pmatrix} x & 1 & 1 & \dots & 1 \\ 1 & x & 1 & \dots & 1 \\ \vdots && \ddots &  & \vdots \\ 1  & \dots  & \dots & x & 1\\  1 &\dots & \dots & 1&x^2  \end{pmatrix}$$ for all $x \in \mathbb R$ and for all $n\geq2$. I know that the answer is $$\det A =(x-1)^{n-1} \cdot (x^2+(n-1) x+n-1) \tag 1$$ and I can prove it by induction if I'm allow to make some hand-wavy assumptions, which I'm not allowed to do because it was an exam question of a pure maths class (linear algebra). However I think that I'm missing something because without having a computer algebra system, coming up with the equation $(1)$ is hard enough, let alone the two inductions I need to do in order to show the equation $(1)$ with some hand-wavy assumptions, which makes the question way too hard for a 2 hour exam. Is there an easier way to calculate this determinant?","I've come across the question of finding the determinant of the $(n\times n)$ matrix, given by $$A:= \begin{pmatrix} x & 1 & 1 & \dots & 1 \\ 1 & x & 1 & \dots & 1 \\ \vdots && \ddots &  & \vdots \\ 1  & \dots  & \dots & x & 1\\  1 &\dots & \dots & 1&x^2  \end{pmatrix}$$ for all $x \in \mathbb R$ and for all $n\geq2$. I know that the answer is $$\det A =(x-1)^{n-1} \cdot (x^2+(n-1) x+n-1) \tag 1$$ and I can prove it by induction if I'm allow to make some hand-wavy assumptions, which I'm not allowed to do because it was an exam question of a pure maths class (linear algebra). However I think that I'm missing something because without having a computer algebra system, coming up with the equation $(1)$ is hard enough, let alone the two inductions I need to do in order to show the equation $(1)$ with some hand-wavy assumptions, which makes the question way too hard for a 2 hour exam. Is there an easier way to calculate this determinant?",,"['linear-algebra', 'matrices', 'determinant', 'problem-solving']"
27,Finding a matrix given eigenvalues and eigenvectors.,Finding a matrix given eigenvalues and eigenvectors.,,"I am asked to construct a $4 \times 4$ symmetric matrix, with given eigenvalues and eigenvectors. I understand how to actually get $A$ as a product of $P^T, D$ and $P$, when $D$ is the diagonal matrix, and $P$ is a matrix with the eigenvectors as columns. The problem is that there is only three given eigenvectors, along with three eigenvalues (one is repeated), so my question is, how do you construct a $4 \times 4$ matrix with three eigenvectors? For more information here is the actual question: Let $A$ be a symmetric $4 \times 4$ matrix with real entries whose eigenvalues are $−1$ and $2$. If $(1, 0, 0, −1)$, $(0, 1, 1, 0)$ is a basis for the eigenspace of eigenvalue $-1$ and $(1, 0, 0, 1)$ is an eigenvector of $A$ with eigenvalue $2$, find the matrix $A$. Thank you.","I am asked to construct a $4 \times 4$ symmetric matrix, with given eigenvalues and eigenvectors. I understand how to actually get $A$ as a product of $P^T, D$ and $P$, when $D$ is the diagonal matrix, and $P$ is a matrix with the eigenvectors as columns. The problem is that there is only three given eigenvectors, along with three eigenvalues (one is repeated), so my question is, how do you construct a $4 \times 4$ matrix with three eigenvectors? For more information here is the actual question: Let $A$ be a symmetric $4 \times 4$ matrix with real entries whose eigenvalues are $−1$ and $2$. If $(1, 0, 0, −1)$, $(0, 1, 1, 0)$ is a basis for the eigenspace of eigenvalue $-1$ and $(1, 0, 0, 1)$ is an eigenvector of $A$ with eigenvalue $2$, find the matrix $A$. Thank you.",,"['matrices', 'eigenvalues-eigenvectors', 'diagonalization', 'symmetry']"
28,"Let $A$ be an $8 \times 5$ matrix of rank 3, and let $b$ be a nonzero vector in $N(A^T)$. Show $Ax=b$ must be inconsistent.","Let  be an  matrix of rank 3, and let  be a nonzero vector in . Show  must be inconsistent.",A 8 \times 5 b N(A^T) Ax=b,"Here's the entire question: Let $A$ be an 8 $\times$ 5 matrix of rank 3, and let $b$ be a nonzero vector in $N(A^T)$ . a) Show that the system $Ax = b$ must be inconsistent. Gonna take a wild stab at this one... If the rank is 3, that means the dimension of the column space is 3. But $A$ has 5 columns, so they are not all linearly independent and therefore $Ax = b$ is inconsistent. b) How many least squares solutions will the system $Ax = b$ have? Explain. On previous problems, I found the best least squares linear fit, where the approximation of $x$ was a vector that contained sometimes regular numbers, and sometimes variables. Does this mean that there must be either 1 linear solution or infinite (because you can always find an approximation)? In the example that apparently had an infinite number of least squares solutions, it appeared that one row of $A^TA$ was a constant multiple of another row, leading to a row of zeros in reduced row echelon form. From this problem I know that $A^TA$ is a 5x5 matrix, but I don't think I can prove that any rows are a scalar multiple of other rows, so I'm guessing I have to use some other means of figuring this out. Sorry if I sound like I have no idea what I'm talking about. Just wanted to try out the problem to my best ability before asking about it.","Here's the entire question: Let be an 8 5 matrix of rank 3, and let be a nonzero vector in . a) Show that the system must be inconsistent. Gonna take a wild stab at this one... If the rank is 3, that means the dimension of the column space is 3. But has 5 columns, so they are not all linearly independent and therefore is inconsistent. b) How many least squares solutions will the system have? Explain. On previous problems, I found the best least squares linear fit, where the approximation of was a vector that contained sometimes regular numbers, and sometimes variables. Does this mean that there must be either 1 linear solution or infinite (because you can always find an approximation)? In the example that apparently had an infinite number of least squares solutions, it appeared that one row of was a constant multiple of another row, leading to a row of zeros in reduced row echelon form. From this problem I know that is a 5x5 matrix, but I don't think I can prove that any rows are a scalar multiple of other rows, so I'm guessing I have to use some other means of figuring this out. Sorry if I sound like I have no idea what I'm talking about. Just wanted to try out the problem to my best ability before asking about it.",A \times b N(A^T) Ax = b A Ax = b Ax = b x A^TA A^TA,"['linear-algebra', 'matrices', 'matrix-rank', 'least-squares', 'transpose']"
29,Give bases for col(A) and null(A),Give bases for col(A) and null(A),,I have A= $\begin{bmatrix}1&1&-3\\0&2&1\\1&-1&-4\end{bmatrix}$ I row reduce it to $\begin{bmatrix}1 &0& -3.5\\0&1&.5\\0&0&0\end{bmatrix}$ How do I find col(A) from the above info? Is it the pivot points correspond to the columns? So col(A) would be $\begin{bmatrix}1\\0\\1\end{bmatrix}$and $\begin{bmatrix}1\\2\\-1\end{bmatrix}$ and for null(A) I got $\begin{bmatrix}3.5\\-.5\\1\end{bmatrix}$,I have A= $\begin{bmatrix}1&1&-3\\0&2&1\\1&-1&-4\end{bmatrix}$ I row reduce it to $\begin{bmatrix}1 &0& -3.5\\0&1&.5\\0&0&0\end{bmatrix}$ How do I find col(A) from the above info? Is it the pivot points correspond to the columns? So col(A) would be $\begin{bmatrix}1\\0\\1\end{bmatrix}$and $\begin{bmatrix}1\\2\\-1\end{bmatrix}$ and for null(A) I got $\begin{bmatrix}3.5\\-.5\\1\end{bmatrix}$,,"['linear-algebra', 'matrices']"
30,"$a_{12},a_{23}$ and $a_{31}$ are the positive roots of the equation $x^3-6x^2+px-8=0,p\in R$ then find the value of $\det(A)$",and  are the positive roots of the equation  then find the value of,"a_{12},a_{23} a_{31} x^3-6x^2+px-8=0,p\in R \det(A)","Let $A=[a_{ij}]_{3\times 3}$ be a matrix.If $A+A^T=   \begin{bmatrix}     6 & 4 & 4\\     a_{21}+a_{12} & 10 & a_{23}+a_{32}\\     a_{31}+a_{13} &4&8   \end{bmatrix}$ where $a_{12},a_{23}$ and $a_{31}$ are the positive roots of the equation $x^3-6x^2+px-8=0,p\in R$ then find the value of $\det(A)$.Here $A^T$ denotes the transpose of matrix $A.$ My Attempt:Let $A=\begin{bmatrix}     a_{11} & a_{12} & a_{13}\\     a_{21} & a_{22} & a_{23}\\     a_{31} &a_{32}&a_{33}   \end{bmatrix}$,then $A^T=\begin{bmatrix}     a_{11} & a_{21} & a_{31}\\     a_{12} & a_{22} & a_{32}\\     a_{13} &a_{23}&a_{33}   \end{bmatrix}$ $A+A^T=\begin{bmatrix}     2a_{11} & a_{12}+a_{21} & a_{13}+a_{31}\\     a_{12}+a_{21} & 2a_{22} & a_{23}+a_{32}\\     a_{13}+a_{31} &a_{23}+a_{32}&2a_{33}   \end{bmatrix}$ but $A+A^T=   \begin{bmatrix}     6 & 4 & 4\\     a_{21}+a_{12} & 10 & a_{23}+a_{32}\\     a_{31}+a_{13} &4&8   \end{bmatrix}$ so $a_{11}=3,a_{22}=5,a_{33}=4,a_{21}+a_{12}=4,a_{31}+a_{13}=4,a_{23}+a_{32}=4$ Also $a_{12},a_{23} \text{and} a_{31}$ are the positive roots of the equation $x^3-6x^2+px-8=0$,so $a_{12}+a_{23}+a_{31}=6,a_{12}.a_{23}.a_{31}=8,a_{12}a_{23}+a_{23}a_{31}+a_{31}a_{12}=p$ For finding the determinant of $A$,i need to find the values of $a_{12},a_{13},a_{21},a_{23},a_{31},a_{32}$ also which i am not able to find. Please help me.Thanks.","Let $A=[a_{ij}]_{3\times 3}$ be a matrix.If $A+A^T=   \begin{bmatrix}     6 & 4 & 4\\     a_{21}+a_{12} & 10 & a_{23}+a_{32}\\     a_{31}+a_{13} &4&8   \end{bmatrix}$ where $a_{12},a_{23}$ and $a_{31}$ are the positive roots of the equation $x^3-6x^2+px-8=0,p\in R$ then find the value of $\det(A)$.Here $A^T$ denotes the transpose of matrix $A.$ My Attempt:Let $A=\begin{bmatrix}     a_{11} & a_{12} & a_{13}\\     a_{21} & a_{22} & a_{23}\\     a_{31} &a_{32}&a_{33}   \end{bmatrix}$,then $A^T=\begin{bmatrix}     a_{11} & a_{21} & a_{31}\\     a_{12} & a_{22} & a_{32}\\     a_{13} &a_{23}&a_{33}   \end{bmatrix}$ $A+A^T=\begin{bmatrix}     2a_{11} & a_{12}+a_{21} & a_{13}+a_{31}\\     a_{12}+a_{21} & 2a_{22} & a_{23}+a_{32}\\     a_{13}+a_{31} &a_{23}+a_{32}&2a_{33}   \end{bmatrix}$ but $A+A^T=   \begin{bmatrix}     6 & 4 & 4\\     a_{21}+a_{12} & 10 & a_{23}+a_{32}\\     a_{31}+a_{13} &4&8   \end{bmatrix}$ so $a_{11}=3,a_{22}=5,a_{33}=4,a_{21}+a_{12}=4,a_{31}+a_{13}=4,a_{23}+a_{32}=4$ Also $a_{12},a_{23} \text{and} a_{31}$ are the positive roots of the equation $x^3-6x^2+px-8=0$,so $a_{12}+a_{23}+a_{31}=6,a_{12}.a_{23}.a_{31}=8,a_{12}a_{23}+a_{23}a_{31}+a_{31}a_{12}=p$ For finding the determinant of $A$,i need to find the values of $a_{12},a_{13},a_{21},a_{23},a_{31},a_{32}$ also which i am not able to find. Please help me.Thanks.",,"['matrices', 'determinant']"
31,Linear Algebra Invertible Matrix Theorem Proof,Linear Algebra Invertible Matrix Theorem Proof,,"Part of the proof for this theorem asks you to show that if $A$ is an $n \times n$-matrix and there exists an $n \times n$-matrix $D$ such that $AD = I$ (the $n \times n$-identity matrix), then the equation $Ax = b$ has at least one solution for each $b$ in $\mathbb{R}^n$. In order to prove this, I simply started with the hypothesis that $AD = I$ and right-multiplied each side of the equation by the $\mathbb{R}^n$ vector $b$ to get: $AD(b) = I(b) = b.$ By the associative property for matrix multiplication, this leads to $A(Db) = b$, which means that for every $b$ in $\mathbb{R}^n$, the vector $Db$ is a solution so that $Ax = b$ has at least that one solution for every $b$ in $\mathbb{R}^n$. I'm pretty sure that's one way of proving it, but can anyone give me any hints about a different or possibly more insightful way of proving this step? It's definitely one of the least straight-forward inferences in the theorem.","Part of the proof for this theorem asks you to show that if $A$ is an $n \times n$-matrix and there exists an $n \times n$-matrix $D$ such that $AD = I$ (the $n \times n$-identity matrix), then the equation $Ax = b$ has at least one solution for each $b$ in $\mathbb{R}^n$. In order to prove this, I simply started with the hypothesis that $AD = I$ and right-multiplied each side of the equation by the $\mathbb{R}^n$ vector $b$ to get: $AD(b) = I(b) = b.$ By the associative property for matrix multiplication, this leads to $A(Db) = b$, which means that for every $b$ in $\mathbb{R}^n$, the vector $Db$ is a solution so that $Ax = b$ has at least that one solution for every $b$ in $\mathbb{R}^n$. I'm pretty sure that's one way of proving it, but can anyone give me any hints about a different or possibly more insightful way of proving this step? It's definitely one of the least straight-forward inferences in the theorem.",,"['linear-algebra', 'matrices', 'linear-transformations']"
32,Higher order of Jordan form,Higher order of Jordan form,,"Let $\lambda =1$ is the eigenvalue corresponding to the single Jordan block $J$. Prove $J^m \sim J$ with an arbitrary positive integer $m$. My try: Because $\lambda = 1$ is eigenvalue, $(J-I)^m =0$. After that $(J-I)^{m-1} J = (J-I)^{m-1}$. At this point, I do not know how to continue.","Let $\lambda =1$ is the eigenvalue corresponding to the single Jordan block $J$. Prove $J^m \sim J$ with an arbitrary positive integer $m$. My try: Because $\lambda = 1$ is eigenvalue, $(J-I)^m =0$. After that $(J-I)^{m-1} J = (J-I)^{m-1}$. At this point, I do not know how to continue.",,['linear-algebra']
33,What do we call the result of wedging together the columns of a matrix?,What do we call the result of wedging together the columns of a matrix?,,"We can wedge together the columns of a square matrix to compute its determinant. More generally, the exterior product of the columns of a $b \times a$ matrix tells us the determinant of each $a \times a$ submatrix. This can also be used to test linear dependence among the columns, and to compute the rank. Question. The result of wedging together the columns of a matrix $A$ in the order they appear is called the [what] of $A$? I'm also interested in answers of the form: ""Really, this should be thought of as something you can do to a linear transform, and its called the [whatever] of a linear transform.""","We can wedge together the columns of a square matrix to compute its determinant. More generally, the exterior product of the columns of a $b \times a$ matrix tells us the determinant of each $a \times a$ submatrix. This can also be used to test linear dependence among the columns, and to compute the rank. Question. The result of wedging together the columns of a matrix $A$ in the order they appear is called the [what] of $A$? I'm also interested in answers of the form: ""Really, this should be thought of as something you can do to a linear transform, and its called the [whatever] of a linear transform.""",,"['matrices', 'terminology', 'determinant', 'multilinear-algebra', 'exterior-algebra']"
34,Why we need Invertible Matrices,Why we need Invertible Matrices,,"In linear algebra, an n-by-n square matrix A is called invertible (also nonsingular or non degenerate) if there exists an n-by-n square matrix B such that $$A B = B A=I_n$$ I know the definition. But what are the practical applications of of invertible matrix.","In linear algebra, an n-by-n square matrix A is called invertible (also nonsingular or non degenerate) if there exists an n-by-n square matrix B such that $$A B = B A=I_n$$ I know the definition. But what are the practical applications of of invertible matrix.",,['matrices']
35,What are the rules for taking derivatives in linear algebra?,What are the rules for taking derivatives in linear algebra?,,"I was reading through a paper on beamforming and came across an equation whose derivative I don't fully understand. A cost function is given as: $$ J(\mathbf{w}) = \mathbf{w}^HR\mathbf{w} +\lambda^*[\mathbf{w}^H\mathbf{s}(\theta_0-1] + \lambda[\mathbf{s}(\theta_0)^H\mathbf{w}-1] $$ and the derivative w.r.t. $\mathbf{w}^*$ is found and set to equal 0: $$\frac{\partial}{\partial\mathbf{w}^*}J(\mathbf{w}) = R\mathbf{w} + \lambda^*\mathbf{s}(\theta_0) = 0$$ It seems to me that $\mathbf{w}$ and $\mathbf{w}^*$ (or $\mathbf{w}^H$) are treated as essentially independent variables, although in my (perhaps naïve) mind they should be related somehow. Is there a fundamental reason they're separate? Perhaps some linear algebra rules for derivatives would be wonderful for me, I just don't know where to look. Thanks! Edit: To clarify some, $\mathbf{w}$ are the beamforming weights, $\mathbf{s}(\theta_0)$ is the steering vector towards the angle of interest, $R$ is the covariance matrix of the input signal, and $\lambda$ is a tuning parameter.","I was reading through a paper on beamforming and came across an equation whose derivative I don't fully understand. A cost function is given as: $$ J(\mathbf{w}) = \mathbf{w}^HR\mathbf{w} +\lambda^*[\mathbf{w}^H\mathbf{s}(\theta_0-1] + \lambda[\mathbf{s}(\theta_0)^H\mathbf{w}-1] $$ and the derivative w.r.t. $\mathbf{w}^*$ is found and set to equal 0: $$\frac{\partial}{\partial\mathbf{w}^*}J(\mathbf{w}) = R\mathbf{w} + \lambda^*\mathbf{s}(\theta_0) = 0$$ It seems to me that $\mathbf{w}$ and $\mathbf{w}^*$ (or $\mathbf{w}^H$) are treated as essentially independent variables, although in my (perhaps naïve) mind they should be related somehow. Is there a fundamental reason they're separate? Perhaps some linear algebra rules for derivatives would be wonderful for me, I just don't know where to look. Thanks! Edit: To clarify some, $\mathbf{w}$ are the beamforming weights, $\mathbf{s}(\theta_0)$ is the steering vector towards the angle of interest, $R$ is the covariance matrix of the input signal, and $\lambda$ is a tuning parameter.",,"['linear-algebra', 'matrices', 'derivatives', 'matrix-calculus']"
36,Remove scale transformation from a complex transform matrix $ 4\times 4$,Remove scale transformation from a complex transform matrix, 4\times 4,"My common task is I have a rect with coordinates of its $2$ points:  $(x, y, z), (x + a, y + b, z)$. I applied a $4\times4$ transform matrix to it and it became a quadrilateral. Now for some reasons I can't use it this way and must apply scaling before other transformations. The list of operations is given below (it is used in computer graphics so I don't know exactly if it is a correct order or if it should be reversed). How to solve this issue? perspective ( m34 = -0.0001 ); translation by z = -radius ; rotation around x axis; rotation around y axis; scale (decrease the size); translation by z = radius . This $4\times4$ matrix is transposed (for example, m41 means translation by x, m42 translation by y and m43  translation by z).","My common task is I have a rect with coordinates of its $2$ points:  $(x, y, z), (x + a, y + b, z)$. I applied a $4\times4$ transform matrix to it and it became a quadrilateral. Now for some reasons I can't use it this way and must apply scaling before other transformations. The list of operations is given below (it is used in computer graphics so I don't know exactly if it is a correct order or if it should be reversed). How to solve this issue? perspective ( m34 = -0.0001 ); translation by z = -radius ; rotation around x axis; rotation around y axis; scale (decrease the size); translation by z = radius . This $4\times4$ matrix is transposed (for example, m41 means translation by x, m42 translation by y and m43  translation by z).",,"['matrices', 'geometry', 'trigonometry', 'matrix-calculus', 'matrix-decomposition']"
37,Matrix Ambiguity?,Matrix Ambiguity?,,"I realize that a matrix of objects, where each slot could either be occupied or empty, is impossible to resolve if you are only given the amount of occupied slots in each row and each column. Ex: A $2 \times 2$ matrix with one object in each column and one in each row could be: $$ \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix}$$     Or $$ \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}$$ My question is, if the amount of objects in all of the possible diagonals is also known, can the contents of the matrix then always be known? If not, is there a sizelimit where the ambiguity starts?","I realize that a matrix of objects, where each slot could either be occupied or empty, is impossible to resolve if you are only given the amount of occupied slots in each row and each column. Ex: A $2 \times 2$ matrix with one object in each column and one in each row could be: $$ \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix}$$     Or $$ \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}$$ My question is, if the amount of objects in all of the possible diagonals is also known, can the contents of the matrix then always be known? If not, is there a sizelimit where the ambiguity starts?",,['matrices']
38,Linear Transformation Matrix with polynomials,Linear Transformation Matrix with polynomials,,"A linear transformation $T : P_2 \to P_2$ has matrix with respect to $S$ given by: $$[T]\,( S) = \begin{bmatrix} 1/2&-3&1/2\\ -1&4&-1\\ 1/2&2&1/2\\ \end{bmatrix} $$ How do you find $T(a+bx+cx^2)$? Thank you!!","A linear transformation $T : P_2 \to P_2$ has matrix with respect to $S$ given by: $$[T]\,( S) = \begin{bmatrix} 1/2&-3&1/2\\ -1&4&-1\\ 1/2&2&1/2\\ \end{bmatrix} $$ How do you find $T(a+bx+cx^2)$? Thank you!!",,"['linear-algebra', 'matrices', 'matrix-equations', 'linear-transformations']"
39,Second order derivation of Quadratic form,Second order derivation of Quadratic form,,"I would like to find the second order derivative of a Quadratic form. Assume we have a random complex column vector $x$ and a real constant value $C$. I am interested in computing the following: $$ \frac{\partial^2}{\partial x^H \partial x}(x^H C x) = ~? $$ The differentiation of the quadratic form is: $$ d(x^H C x) = 2 \mathcal{R}( x^H C dx) $$ where $\mathcal{R}(\cdot)$ denotes the real part. Are the following expressions correct? $$d^2(x^H C x) = 2\mathcal{R}(C dx^H dx)$$ since $C$ is real and also the outcome of $dx^H dx$ is real, the operator $\mathcal{R}(\cdot)$ can be ignored and therefore: $$ \frac{\partial^2}{\partial x^H \partial x}(x^H C x) = 2 C $$ And how about, if $C$ is a constant matrix with property $C = C^H$?","I would like to find the second order derivative of a Quadratic form. Assume we have a random complex column vector $x$ and a real constant value $C$. I am interested in computing the following: $$ \frac{\partial^2}{\partial x^H \partial x}(x^H C x) = ~? $$ The differentiation of the quadratic form is: $$ d(x^H C x) = 2 \mathcal{R}( x^H C dx) $$ where $\mathcal{R}(\cdot)$ denotes the real part. Are the following expressions correct? $$d^2(x^H C x) = 2\mathcal{R}(C dx^H dx)$$ since $C$ is real and also the outcome of $dx^H dx$ is real, the operator $\mathcal{R}(\cdot)$ can be ignored and therefore: $$ \frac{\partial^2}{\partial x^H \partial x}(x^H C x) = 2 C $$ And how about, if $C$ is a constant matrix with property $C = C^H$?",,"['linear-algebra', 'matrices', 'quadratic-forms', 'random-matrices']"
40,Determinant of matrices without expanding [duplicate],Determinant of matrices without expanding [duplicate],,This question already has an answer here : How to evaluate the determinant (1 answer) Closed 9 years ago . Show that $$\begin{array}{|ccc|} -2a & a + b & c + a \\ a + b & -2b & b + c \\ c + a & c + b & -2c \end{array} = 4(a+b)(b+c)(c+a)\text{.}$$ I added the all rows but couldn't get it.,This question already has an answer here : How to evaluate the determinant (1 answer) Closed 9 years ago . Show that $$\begin{array}{|ccc|} -2a & a + b & c + a \\ a + b & -2b & b + c \\ c + a & c + b & -2c \end{array} = 4(a+b)(b+c)(c+a)\text{.}$$ I added the all rows but couldn't get it.,,"['matrices', 'determinant']"
41,Building matrices from eigenvalues,Building matrices from eigenvalues,,"I saw a question some time ago, asking about the eigenvalues of the matrix $$A=\begin{pmatrix}5&-3&0\\-3&5&0\\0&0&2\end{pmatrix}$$ which were then shown to be $\lambda=\left\{8,2,2\right\}$; and I thought, what a nice feature, we have a symmetric matrix and the eigenvalues seem to be $\lambda=\left\{5-(-3),5+(-3),2\right\}$. So I tried to show that it was indeed a property of the more general matrix $$M=\begin{pmatrix}a&b&0\\b&a&0\\0&0&c\end{pmatrix}$$ and sure enough, I found $\lambda=\left\{a+b,a-b,c\right\}$. And I remembered that in fact, I had seen a few of these matrices in a Quantum Mechanics course, in the context of perturbation theory , where an Hamiltonian $\hat{H_0}=\text{diag}(a,a,c)$ was said to be degenerate as its eigenvalues were $\lambda=\left\{a,a,c\right\}$, but it was possible to lift the degeneracy by applying a perturbation $\hat{H_p}=\begin{pmatrix}0&b&0\\b&0&0\\0&0&0\end{pmatrix}$, leading to an overall Hamiltonian described by the matrix $M$ above, with distinct eigenvalues. But I digress. My question is: knowing the eigenvalues, can we build a corresponding matrix ? This question (or something close to it) has probably already been asked, but I'm not only interested in building one matrix from a set of eigenvalues, but all possible matrices . Going back to perturbation theory, my question is directly motivated by trying to find perturbation matrices $\hat{H_p}$ that do not modify the eigenvalues of the unperturbed system $\hat{H_0}$. An (almost trivial) example is $$\hat{H}=\begin{pmatrix}a&0&b\\0&a&0\\0&0&c\end{pmatrix},\quad\hat{H_0}=\begin{pmatrix}a&0&0\\0&a&0\\0&0&c\end{pmatrix},\quad\hat{H_p}=\begin{pmatrix}0&0&b\\0&0&0\\0&0&0\end{pmatrix}$$ where $\hat{H}$ and $\hat{H_0}$ obviously have the same eigenvalues. Thus, given a set of eigenvalues, is there a way of determining the set of matrices to which they belong?","I saw a question some time ago, asking about the eigenvalues of the matrix $$A=\begin{pmatrix}5&-3&0\\-3&5&0\\0&0&2\end{pmatrix}$$ which were then shown to be $\lambda=\left\{8,2,2\right\}$; and I thought, what a nice feature, we have a symmetric matrix and the eigenvalues seem to be $\lambda=\left\{5-(-3),5+(-3),2\right\}$. So I tried to show that it was indeed a property of the more general matrix $$M=\begin{pmatrix}a&b&0\\b&a&0\\0&0&c\end{pmatrix}$$ and sure enough, I found $\lambda=\left\{a+b,a-b,c\right\}$. And I remembered that in fact, I had seen a few of these matrices in a Quantum Mechanics course, in the context of perturbation theory , where an Hamiltonian $\hat{H_0}=\text{diag}(a,a,c)$ was said to be degenerate as its eigenvalues were $\lambda=\left\{a,a,c\right\}$, but it was possible to lift the degeneracy by applying a perturbation $\hat{H_p}=\begin{pmatrix}0&b&0\\b&0&0\\0&0&0\end{pmatrix}$, leading to an overall Hamiltonian described by the matrix $M$ above, with distinct eigenvalues. But I digress. My question is: knowing the eigenvalues, can we build a corresponding matrix ? This question (or something close to it) has probably already been asked, but I'm not only interested in building one matrix from a set of eigenvalues, but all possible matrices . Going back to perturbation theory, my question is directly motivated by trying to find perturbation matrices $\hat{H_p}$ that do not modify the eigenvalues of the unperturbed system $\hat{H_0}$. An (almost trivial) example is $$\hat{H}=\begin{pmatrix}a&0&b\\0&a&0\\0&0&c\end{pmatrix},\quad\hat{H_0}=\begin{pmatrix}a&0&0\\0&a&0\\0&0&c\end{pmatrix},\quad\hat{H_p}=\begin{pmatrix}0&0&b\\0&0&0\\0&0&0\end{pmatrix}$$ where $\hat{H}$ and $\hat{H_0}$ obviously have the same eigenvalues. Thus, given a set of eigenvalues, is there a way of determining the set of matrices to which they belong?",,"['matrices', 'eigenvalues-eigenvectors', 'perturbation-theory']"
42,Question about eigenvalue of Hermitian matrix,Question about eigenvalue of Hermitian matrix,,This is an eigenvalue problem I found. Let $A$ be an $n$-by-$n$ Hermitian complex matrix and $u$ is a vector in $C^n$ such that $u^*u=1$. Let $k=u^*Au$. Show that there exists an eigenvalue $r$ of $A$ such that $|r-k| \le ||Au-ku||_2$ (norm-2). I've been trying to use some facts about maximum or minimum eigenvalue but got no clue at the end. I think it talks about Rayleigh quotient tho. How should it be done?,This is an eigenvalue problem I found. Let $A$ be an $n$-by-$n$ Hermitian complex matrix and $u$ is a vector in $C^n$ such that $u^*u=1$. Let $k=u^*Au$. Show that there exists an eigenvalue $r$ of $A$ such that $|r-k| \le ||Au-ku||_2$ (norm-2). I've been trying to use some facts about maximum or minimum eigenvalue but got no clue at the end. I think it talks about Rayleigh quotient tho. How should it be done?,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
43,Does a matrix with negative eigenvalues have singular values?,Does a matrix with negative eigenvalues have singular values?,,What if the eigenvalues of a matrix $A$ are all negative? Does that simply mean there is no singular value for this particular matrix? I can't calculate the conditional number or matrix $2$ -norm for this matrix. Please clarify.,What if the eigenvalues of a matrix are all negative? Does that simply mean there is no singular value for this particular matrix? I can't calculate the conditional number or matrix -norm for this matrix. Please clarify.,A 2,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'definition', 'singular-values']"
44,Which matrices are covariances matrices?,Which matrices are covariances matrices?,,"Let $V$ be a matrix. What conditions should we require so that we can find a random vector $X = (X_1, \dots, X_n)$ so that $V = Var(X)$? Of course necessary conditions are: All the elements on the diagonal should be positive The matrix has to be symmetric $v_{ij} \le \sqrt{v_{ii}v_{jj}}$ (Because of $Cov(X_i, X_j) \le \sqrt{Var(X_i) Var(X_j)})$ But I am sure these are not sufficient as I have a counterexample. So what other properties we should require on a matrix so that it can be considered a covariance matrix?","Let $V$ be a matrix. What conditions should we require so that we can find a random vector $X = (X_1, \dots, X_n)$ so that $V = Var(X)$? Of course necessary conditions are: All the elements on the diagonal should be positive The matrix has to be symmetric $v_{ij} \le \sqrt{v_{ii}v_{jj}}$ (Because of $Cov(X_i, X_j) \le \sqrt{Var(X_i) Var(X_j)})$ But I am sure these are not sufficient as I have a counterexample. So what other properties we should require on a matrix so that it can be considered a covariance matrix?",,"['linear-algebra', 'matrices', 'probability-theory', 'random-variables', 'covariance']"
45,Did I do something wrong solving this PDE in MATLAB?,Did I do something wrong solving this PDE in MATLAB?,,"I have the following PDE problem on a practice exam: I have completed the problem using MATLAB to the best of my ability.  Here is the code I used M = [0 0 0 0 0 0 0 0 0 0 0;      0 0 0 0 0 0 0 0 0 0 0;      0 0 0 0 0 0 0 0 0 0 0;      0 0 0 0 0 0 0 0 0 0 0;      0 0 0 0 0 0 0 0 0 0 0;      0 0 0 0 0 0 0 0 0 0 0;      0 0 0 0 0 0 0 0 0 0 0;      0 0 0 0 0 0 0 0 0 0 0;      0 0 0 0 0 0 0 0 0 0 0;      0 0 0 0 0 0 0 0 0 0 0;      0 0 0 0 0 0 0 0 0 0 0];  h = 0.1; k = 0.1;  %bottom row initial condition for i=1:11    x = (i-1) * 0.1;    M(11,i) =  (0.1)*(x^2); end  %right column initial condition for i=1:10    realI = 11-i;    t = (11-realI) * 0.1;    M(realI,11) = (0.1) * (1+t)^2;    end  %n+1 row using u_t boundry condition for i=2:10    x = (i-1) * 0.1;    left = M(11,i-1);    right = M(11,i+1);    M(10,i) = (left + right + 0.04*x)/2;    end  %calculate the remaining n+1 row point (leftmost point) M(10,1) = 0.1/5; %(1/5)t  %Now, just use the scheme to solve the rest of the points, and the t/5 %to calculate the edges for n=1:9    real_n = 10-n; %count from 9 to 1 rather than 1 to 9    for m=2:10               M(real_n,m) = M(real_n + 1, m-1) + M(real_n + 1, m+1) - M(real_n + 2, m);    end    %leftmost point    t = (n+1)/10;    M(real_n, 1) = t/5;  end  M surf(M); The problem is that I have no way of knowing that I am correct as my professor does not release solutions for practice exams. My specific problem is I am not confident that I got the left column correct, but I'm also hoping to get feedback on my answer as a whole.  Can someone either replicate the problem or check over my code? Here are my results with the code that I posted: Did I do the left column correctly? Does my algorithm look correctly matched to the equations outlined in the problem?  Are there any ways I can improve the code that I wrote if it actually is correct?  MATLAB is a bit of a second language to me. (har har)","I have the following PDE problem on a practice exam: I have completed the problem using MATLAB to the best of my ability.  Here is the code I used M = [0 0 0 0 0 0 0 0 0 0 0;      0 0 0 0 0 0 0 0 0 0 0;      0 0 0 0 0 0 0 0 0 0 0;      0 0 0 0 0 0 0 0 0 0 0;      0 0 0 0 0 0 0 0 0 0 0;      0 0 0 0 0 0 0 0 0 0 0;      0 0 0 0 0 0 0 0 0 0 0;      0 0 0 0 0 0 0 0 0 0 0;      0 0 0 0 0 0 0 0 0 0 0;      0 0 0 0 0 0 0 0 0 0 0;      0 0 0 0 0 0 0 0 0 0 0];  h = 0.1; k = 0.1;  %bottom row initial condition for i=1:11    x = (i-1) * 0.1;    M(11,i) =  (0.1)*(x^2); end  %right column initial condition for i=1:10    realI = 11-i;    t = (11-realI) * 0.1;    M(realI,11) = (0.1) * (1+t)^2;    end  %n+1 row using u_t boundry condition for i=2:10    x = (i-1) * 0.1;    left = M(11,i-1);    right = M(11,i+1);    M(10,i) = (left + right + 0.04*x)/2;    end  %calculate the remaining n+1 row point (leftmost point) M(10,1) = 0.1/5; %(1/5)t  %Now, just use the scheme to solve the rest of the points, and the t/5 %to calculate the edges for n=1:9    real_n = 10-n; %count from 9 to 1 rather than 1 to 9    for m=2:10               M(real_n,m) = M(real_n + 1, m-1) + M(real_n + 1, m+1) - M(real_n + 2, m);    end    %leftmost point    t = (n+1)/10;    M(real_n, 1) = t/5;  end  M surf(M); The problem is that I have no way of knowing that I am correct as my professor does not release solutions for practice exams. My specific problem is I am not confident that I got the left column correct, but I'm also hoping to get feedback on my answer as a whole.  Can someone either replicate the problem or check over my code? Here are my results with the code that I posted: Did I do the left column correctly? Does my algorithm look correctly matched to the equations outlined in the problem?  Are there any ways I can improve the code that I wrote if it actually is correct?  MATLAB is a bit of a second language to me. (har har)",,"['calculus', 'matrices', 'ordinary-differential-equations', 'numerical-methods', 'matlab']"
46,"What is the significance of $SL(2, \mathbb{R} / SL(2, \mathbb{Z}))$ in studying lattices in geometry of numbers?",What is the significance of  in studying lattices in geometry of numbers?,"SL(2, \mathbb{R} / SL(2, \mathbb{Z}))","I was listening to a talk about lattices and the geometry of numbers and at one point they jumped from discussing a 2d lattice into discussing $SL(2, \mathbb{R})\ /\ SL(2, \mathbb{Z}))$ and it was not clear why they would do that? I thought about it and can see how we can identify unimodular lattices with elements of $SL(2, \mathbb{Z})$. Why would we then want to quotient by that space?","I was listening to a talk about lattices and the geometry of numbers and at one point they jumped from discussing a 2d lattice into discussing $SL(2, \mathbb{R})\ /\ SL(2, \mathbb{Z}))$ and it was not clear why they would do that? I thought about it and can see how we can identify unimodular lattices with elements of $SL(2, \mathbb{Z})$. Why would we then want to quotient by that space?",,"['matrices', 'geometry']"
47,Efficient computation of Cholesky decomposition during tridiagonal matrix inverse,Efficient computation of Cholesky decomposition during tridiagonal matrix inverse,,"I have a symmetric, block tridiagonal matrix $A$ . I am interested in computing the Cholesky decomposition of $A^{-1}$ (that is, I want to compute $R$ , where $A^{-1}=RR^T$ ). I know how to compute the blocks of the inverse efficiently using an iterative algorithm. However, is there an efficient algorithm for computing the Cholesky factors $R$ directly (rather than first computing the inverse, and then performing the Cholesky decomposition)?","I have a symmetric, block tridiagonal matrix . I am interested in computing the Cholesky decomposition of (that is, I want to compute , where ). I know how to compute the blocks of the inverse efficiently using an iterative algorithm. However, is there an efficient algorithm for computing the Cholesky factors directly (rather than first computing the inverse, and then performing the Cholesky decomposition)?",A A^{-1} R A^{-1}=RR^T R,"['matrices', 'numerical-linear-algebra', 'matrix-decomposition', 'cholesky-decomposition']"
48,Help with a tricky matrix equation,Help with a tricky matrix equation,,"Say I have the following variable length vectors containing unknown values: $$ A=\left (\begin{array}{c} a_1 \\ a_2 \\ \vdots\\ a_i\\ \end{array}\right) B=\left (\begin{array}{c} b_1 \\ b_2 \\ \vdots\\ b_j\\ \end{array}\right) $$ And the following variable length vectors and matrix (according to the size of the preceding vectors) containing known values: $$ A_T=\left (\begin{array}{c} a'_1 \\ a'_2 \\ \vdots\\ a'_i\\ \end{array}\right) B_T=\left (\begin{array}{c} b'_1 \\ b'_2 \\ \vdots\\ b'_j\\ \end{array}\right) K=\left (\begin{array}{ccc} k_{11} & \cdots & k_{1j} \\ \vdots & \ddots & \vdots \\ k_{i1} & \cdots & k_{ij}\\ \end{array}\right) $$ These are all related by the following two equations: $$ A_T = A + diag(A)KB\\ B_T = B + diag(B)K^TA $$ Given all of the known values, I want to know if it is possible to solve for A and B. If so how should I go about figuring this out? I only have a basic education on matrix math.","Say I have the following variable length vectors containing unknown values: $$ A=\left (\begin{array}{c} a_1 \\ a_2 \\ \vdots\\ a_i\\ \end{array}\right) B=\left (\begin{array}{c} b_1 \\ b_2 \\ \vdots\\ b_j\\ \end{array}\right) $$ And the following variable length vectors and matrix (according to the size of the preceding vectors) containing known values: $$ A_T=\left (\begin{array}{c} a'_1 \\ a'_2 \\ \vdots\\ a'_i\\ \end{array}\right) B_T=\left (\begin{array}{c} b'_1 \\ b'_2 \\ \vdots\\ b'_j\\ \end{array}\right) K=\left (\begin{array}{ccc} k_{11} & \cdots & k_{1j} \\ \vdots & \ddots & \vdots \\ k_{i1} & \cdots & k_{ij}\\ \end{array}\right) $$ These are all related by the following two equations: $$ A_T = A + diag(A)KB\\ B_T = B + diag(B)K^TA $$ Given all of the known values, I want to know if it is possible to solve for A and B. If so how should I go about figuring this out? I only have a basic education on matrix math.",,"['linear-algebra', 'matrices', 'matrix-equations']"
49,Invertible block matrix,Invertible block matrix,,"Could I find $A_1,A_2,A_3,A_4 \in M_n(\mathbb C)$, such that, for all $z_1,z_2,z_3,z_4\in \mathbb C$, $\det(z_1A_1+z_2A_2+z_3A_3+z_4A_4)=0$ and $\det \begin{pmatrix} A_1&A_2\\A_3&A_4\end{pmatrix}\neq 0$? If so, could this result be extended to $m^2$ matrices $A_1,\ldots,A_{m^2} \in M_n(\mathbb C)$?","Could I find $A_1,A_2,A_3,A_4 \in M_n(\mathbb C)$, such that, for all $z_1,z_2,z_3,z_4\in \mathbb C$, $\det(z_1A_1+z_2A_2+z_3A_3+z_4A_4)=0$ and $\det \begin{pmatrix} A_1&A_2\\A_3&A_4\end{pmatrix}\neq 0$? If so, could this result be extended to $m^2$ matrices $A_1,\ldots,A_{m^2} \in M_n(\mathbb C)$?",,"['linear-algebra', 'matrices', 'determinant']"
50,Looking for at least one surjective ring homomorphism from $M_n(R)$ to $R$,Looking for at least one surjective ring homomorphism from  to,M_n(R) R,"Let $R$ be a ring , I am looking for a surjective ring homomporphism from $M_n(R)$ to $R$ . Please help . Thanks in advance .","Let $R$ be a ring , I am looking for a surjective ring homomporphism from $M_n(R)$ to $R$ . Please help . Thanks in advance .",,"['matrices', 'functions']"
51,On diagonizability of commutating matrices,On diagonizability of commutating matrices,,Let $A$ and $B$ be $n\times n$ matrices over the Galois Field of order $p$ ($p$ is a prime). Suppose that $A$ and $B$ are diagonizable matrices and that they commutate. Is it possible to make them simultaneously diagonizable in $GF(p)$? I know that when we have an algebraically closed field the things go fine. What in this case? I find somethings that lead me to think that this is true.,Let $A$ and $B$ be $n\times n$ matrices over the Galois Field of order $p$ ($p$ is a prime). Suppose that $A$ and $B$ are diagonizable matrices and that they commutate. Is it possible to make them simultaneously diagonizable in $GF(p)$? I know that when we have an algebraically closed field the things go fine. What in this case? I find somethings that lead me to think that this is true.,,"['abstract-algebra', 'matrices', 'field-theory', 'finite-fields']"
52,Logic supporting column operations on matrices,Logic supporting column operations on matrices,,"In matrices, we justify row operations by drawing parallels with solving a system of equations i.e.: 1.Interchanging rows = Interchanging equations \ 2.Adding one multiple of a row to another = Adding one multiple of an equation to another \ 3.Multiplying all terms of a row by a constant = multiplying both sides of an equation by that constant. Now, we can do much the same with columns as well. What's the logic supporting column operations? Can we draw a parallel to solving  a system of equations for column operations as well? Thanks!","In matrices, we justify row operations by drawing parallels with solving a system of equations i.e.: 1.Interchanging rows = Interchanging equations \ 2.Adding one multiple of a row to another = Adding one multiple of an equation to another \ 3.Multiplying all terms of a row by a constant = multiplying both sides of an equation by that constant. Now, we can do much the same with columns as well. What's the logic supporting column operations? Can we draw a parallel to solving  a system of equations for column operations as well? Thanks!",,"['linear-algebra', 'matrices', 'systems-of-equations']"
53,Rank in row echelon form,Rank in row echelon form,,"$$A= \begin{bmatrix}   a & 1 & a & 0 & 0 & 0 \\   0 & b & 1 & b & 0 & 0 \\   0 & 0 & c & 1 & c & 0 \\   0 & 0 & 0 & d & 1 & d \\ \end{bmatrix} $$ Let $A$ be the matrix above and $r$ be the number of non-zero rows in row echelon form. Show that $$r>2 \quad\text{always}$$ $$r=3 \quad\text{iff }a=d=0 \text{ and } bc = 1$$ $$r=4 \quad\text{otherwise}$$ Is there a way to prove this question without listing out all the possible combinations of a,b,c,d being zero or non-zero. Can anyone explain please?","$$A= \begin{bmatrix}   a & 1 & a & 0 & 0 & 0 \\   0 & b & 1 & b & 0 & 0 \\   0 & 0 & c & 1 & c & 0 \\   0 & 0 & 0 & d & 1 & d \\ \end{bmatrix} $$ Let $A$ be the matrix above and $r$ be the number of non-zero rows in row echelon form. Show that $$r>2 \quad\text{always}$$ $$r=3 \quad\text{iff }a=d=0 \text{ and } bc = 1$$ $$r=4 \quad\text{otherwise}$$ Is there a way to prove this question without listing out all the possible combinations of a,b,c,d being zero or non-zero. Can anyone explain please?",,"['linear-algebra', 'matrices']"
54,Eigenvalue of a matrix that the sum of each column is equal?,Eigenvalue of a matrix that the sum of each column is equal?,,"Let $A$ be an $n \times n$ matrix. i) Prove that if the sum of each row of $A$ equals $s$, then $s$ is an eigenvalue of $A$. ii) Prove that if the sum of each column of $A$ equals $s$, then $s$ is an eigenvalue of $A$. The first question isn't a problem, but I've been banging my head against the wall for an hour trying to find the answer to the second one. I'd really appreciate some help.","Let $A$ be an $n \times n$ matrix. i) Prove that if the sum of each row of $A$ equals $s$, then $s$ is an eigenvalue of $A$. ii) Prove that if the sum of each column of $A$ equals $s$, then $s$ is an eigenvalue of $A$. The first question isn't a problem, but I've been banging my head against the wall for an hour trying to find the answer to the second one. I'd really appreciate some help.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
55,Subspace for a matrix representation of a linear transformation,Subspace for a matrix representation of a linear transformation,,"Can someone help me with this problem? Let $V$ be an $n$-dimensional vector space, and let $T: V \rightarrow V$ be a linear transformation. Suppose that $W$ is a $T$-invariant subspace of $V$ having dimension $k$. Show that there is a basis $\beta$ for $V$ such that $[T]_{\beta}$ has the form $\begin{pmatrix} A & B \\ O & C \end{pmatrix}$, where $A$ is a $k \times k$-matrix and $O$ is the $(n-k) \times k$ zero matrix. Attempt at a solution: Let $(v_1, v_2, \ldots, v_k)$ be a basis for $W$ and extend it to a basis $\beta = (v_1, v_2, \ldots, v_n)$ for $V$. Since $W$ is $T$-invariant, we have $\forall v_j \in W$, $T(v_j) \in W$ for $j= 1, 2, \ldots, k$. Hence $T(v_j) = \sum_{i=1}^{k} a_{ij} v_i$. Don't know where to go from here. How to find A, B, C and $O$?","Can someone help me with this problem? Let $V$ be an $n$-dimensional vector space, and let $T: V \rightarrow V$ be a linear transformation. Suppose that $W$ is a $T$-invariant subspace of $V$ having dimension $k$. Show that there is a basis $\beta$ for $V$ such that $[T]_{\beta}$ has the form $\begin{pmatrix} A & B \\ O & C \end{pmatrix}$, where $A$ is a $k \times k$-matrix and $O$ is the $(n-k) \times k$ zero matrix. Attempt at a solution: Let $(v_1, v_2, \ldots, v_k)$ be a basis for $W$ and extend it to a basis $\beta = (v_1, v_2, \ldots, v_n)$ for $V$. Since $W$ is $T$-invariant, we have $\forall v_j \in W$, $T(v_j) \in W$ for $j= 1, 2, \ldots, k$. Hence $T(v_j) = \sum_{i=1}^{k} a_{ij} v_i$. Don't know where to go from here. How to find A, B, C and $O$?",,"['linear-algebra', 'matrices', 'linear-transformations']"
56,Interlacing of eigenvalues for Hermitian matrices,Interlacing of eigenvalues for Hermitian matrices,,"This is a problem from Matrix Analysis by Horn and Johnson. Let $A \in M_n$ be Hermitian, let $a_k$$=$det$A$[{$1$, $\dots$,$k$}] be the leading principal minor of $A$ of size $k$, $k = 1, \dots, n$, and suppose that all $a_k \neq 0$. Show that the number of negative eigenvalues of $A$ is equal to the number of sign changes in the sequence $+1, a_1, a_2, \dots, a_n$. Explain why $A$ is positive definite if and only if every principal minor of $A$ is positive. What happens if some $a_i =0?$ The hint on the book tells me to use the interlacing theorem, which is the following. Let $B \in M_n$ be Hermitian, let $y\in \mathbb C^n$ and $a \in \mathbb R$ be given, and let $A$ $=$ $$ \begin{pmatrix}         B & y \\         y* & a \\         \end{pmatrix}  \in M_{n+1} $$ Then, $\lambda_1(A) \le \lambda_1(B) \le \lambda_2(A) \le \cdots \le \lambda_n(A) \le \lambda_n(B) \le \lambda_{n+1}(A)$. However, I have not been able to solve this so far and I would greatly appreciate any solution, hint, or suggestions.","This is a problem from Matrix Analysis by Horn and Johnson. Let $A \in M_n$ be Hermitian, let $a_k$$=$det$A$[{$1$, $\dots$,$k$}] be the leading principal minor of $A$ of size $k$, $k = 1, \dots, n$, and suppose that all $a_k \neq 0$. Show that the number of negative eigenvalues of $A$ is equal to the number of sign changes in the sequence $+1, a_1, a_2, \dots, a_n$. Explain why $A$ is positive definite if and only if every principal minor of $A$ is positive. What happens if some $a_i =0?$ The hint on the book tells me to use the interlacing theorem, which is the following. Let $B \in M_n$ be Hermitian, let $y\in \mathbb C^n$ and $a \in \mathbb R$ be given, and let $A$ $=$ $$ \begin{pmatrix}         B & y \\         y* & a \\         \end{pmatrix}  \in M_{n+1} $$ Then, $\lambda_1(A) \le \lambda_1(B) \le \lambda_2(A) \le \cdots \le \lambda_n(A) \le \lambda_n(B) \le \lambda_{n+1}(A)$. However, I have not been able to solve this so far and I would greatly appreciate any solution, hint, or suggestions.",,"['linear-algebra', 'matrices']"
57,Symmetric matrix with same diagonal elements,Symmetric matrix with same diagonal elements,,"A paper I was reading made the claim that the eigenvector of a symmetric matrix with same diagonal elements is : $$i_n = {1, e^{jna}, e^{2jna}, ..., e^{j(N-1)na}}$$ $$ n =0, 1, 2 , ...N-1$$ $$a = \frac{2\pi}{N}$$ Why is this true? I attach a screenshot:","A paper I was reading made the claim that the eigenvector of a symmetric matrix with same diagonal elements is : $$i_n = {1, e^{jna}, e^{2jna}, ..., e^{j(N-1)na}}$$ $$ n =0, 1, 2 , ...N-1$$ $$a = \frac{2\pi}{N}$$ Why is this true? I attach a screenshot:",,"['linear-algebra', 'matrices']"
58,"Show that $\|B^{-1/2}AB^{-1/2} - I\| >1/2$, when $\|A\|>2\|B\|$","Show that , when",\|B^{-1/2}AB^{-1/2} - I\| >1/2 \|A\|>2\|B\|,"Let $A,B$ be two positive-definite matrices. Suppose that $\|A\|>2\|B\|$. Is it possible to show that $\|B^{-1/2}AB^{-1/2} - I\| >1/2$, where $I$ is an identity matrix and the norm is the supremum norm.","Let $A,B$ be two positive-definite matrices. Suppose that $\|A\|>2\|B\|$. Is it possible to show that $\|B^{-1/2}AB^{-1/2} - I\| >1/2$, where $I$ is an identity matrix and the norm is the supremum norm.",,"['matrices', 'operator-theory']"
59,Reduce matrix to Smith Normal form.,Reduce matrix to Smith Normal form.,,"I've been given the finitely generated abelian group: $$\langle x_1, x_2 \mid 6x_1-6x_2, -6x_1-12x_2, 4x_1-8x_2\rangle$$ and written the corresponding matrix: $$A=\begin{pmatrix}   6 & -6  \\ -6 & -12 \\ 4 & -8 \end{pmatrix}$$ I now need to reduce this to Smith Normal form using the unimodular elementary row and column operations. I keep running into difficulties that are usually because I'm not allowed to multiply rows or columns by fractions. How do I do it?","I've been given the finitely generated abelian group: $$\langle x_1, x_2 \mid 6x_1-6x_2, -6x_1-12x_2, 4x_1-8x_2\rangle$$ and written the corresponding matrix: $$A=\begin{pmatrix}   6 & -6  \\ -6 & -12 \\ 4 & -8 \end{pmatrix}$$ I now need to reduce this to Smith Normal form using the unimodular elementary row and column operations. I keep running into difficulties that are usually because I'm not allowed to multiply rows or columns by fractions. How do I do it?",,"['linear-algebra', 'matrices']"
60,Bilinear functional to be elementary,Bilinear functional to be elementary,,"Let $V$ be a $n$ dimensional vector space over $\Bbb C$. We say a functional $f:V\times V\to \Bbb C$ is bilinear if $f$ is linear in each variable if the other variable is fixed. And $$f$$ is called elementary if there exists some $x,y\in V$ such that $$f(u,v)=<x,u>\cdot <y,v>.$$ Here $<x,u>=x^*u$, $x^*$ is the conjugate transpose of $x$. Now my problem is as follows. Let $x,y,z$ be three linearly independent vectors in $V$. What conditions on $w$ should satisfy to ensure the following functional $$f(u,v)=<x,u>\cdot <y,v>+<z,u>\cdot <w,v>$$ to be elementary. My first idea is as follows. If the above $f$ is elementary, then there exists some $a,b\in V$ such that  $$f(u,v)=<a,u>\cdot <b,v>.$$ And thus $$<x,u>\cdot <y,v>+<z,u>\cdot <w,v>=f(u,v)=<a,u>\cdot <b,v>$$ Since $f$ is bilinear, we need only to find conditions on $w$ such that there exists some $a,b\in\Bbb C^n$ such that  $$\bar x_iy_j+\bar z_iw_j=\bar a_ib_j,\forall\ 1\leq i,j\leq n.$$ These are $n^2$ equations, but only with $2n$ variables, but I could not derive further.. I suspect that $w$ is a multiplier of $y$... This is exercise I.4.1 of Bhatia: Matrix Analysis.","Let $V$ be a $n$ dimensional vector space over $\Bbb C$. We say a functional $f:V\times V\to \Bbb C$ is bilinear if $f$ is linear in each variable if the other variable is fixed. And $$f$$ is called elementary if there exists some $x,y\in V$ such that $$f(u,v)=<x,u>\cdot <y,v>.$$ Here $<x,u>=x^*u$, $x^*$ is the conjugate transpose of $x$. Now my problem is as follows. Let $x,y,z$ be three linearly independent vectors in $V$. What conditions on $w$ should satisfy to ensure the following functional $$f(u,v)=<x,u>\cdot <y,v>+<z,u>\cdot <w,v>$$ to be elementary. My first idea is as follows. If the above $f$ is elementary, then there exists some $a,b\in V$ such that  $$f(u,v)=<a,u>\cdot <b,v>.$$ And thus $$<x,u>\cdot <y,v>+<z,u>\cdot <w,v>=f(u,v)=<a,u>\cdot <b,v>$$ Since $f$ is bilinear, we need only to find conditions on $w$ such that there exists some $a,b\in\Bbb C^n$ such that  $$\bar x_iy_j+\bar z_iw_j=\bar a_ib_j,\forall\ 1\leq i,j\leq n.$$ These are $n^2$ equations, but only with $2n$ variables, but I could not derive further.. I suspect that $w$ is a multiplier of $y$... This is exercise I.4.1 of Bhatia: Matrix Analysis.",,"['linear-algebra', 'matrices', 'matrix-calculus']"
61,The spectral radius of $A$ and its transpose,The spectral radius of  and its transpose,A,"Let $A$ be a non-negative irreducible $n\times n$ matrix. Then the function $$f(t)=\rho(tA+(1-t)A^T)$$ is increasing on $[0,1/2]$, and is decreasing on $[1/2,1]$. Here are the notations. $A$ is non-negative if any entry of $A$ is greater than or equal to $0$. $A$ is irreducible if $A$ is not reducible; and $A$ is reducible if there exists a permutation matrix $P$ such that $$P^T AP=\begin{pmatrix} B&0\\ C&D\end{pmatrix},$$ or equivalently, there exists a permutation $\sigma$ of $\{1,2,\cdots,n\}$ and a $1\leq k\leq n-1$ such that the sub-matrix of $A$ in rows $\sigma(1),\cdots,\sigma(k)$ and columns $\sigma(k+1),\cdots,\sigma(n)$ being $0$. $A^T$ is the transpose of $A$. $\rho(A)$ is the spectral radius of $A$, that is, the largest modulus of the eigenvalues of $A$. And now I have no idea on it. However, it is intuitively right. As there are more symmetry in the matrix, the spectral radius becomes larger.","Let $A$ be a non-negative irreducible $n\times n$ matrix. Then the function $$f(t)=\rho(tA+(1-t)A^T)$$ is increasing on $[0,1/2]$, and is decreasing on $[1/2,1]$. Here are the notations. $A$ is non-negative if any entry of $A$ is greater than or equal to $0$. $A$ is irreducible if $A$ is not reducible; and $A$ is reducible if there exists a permutation matrix $P$ such that $$P^T AP=\begin{pmatrix} B&0\\ C&D\end{pmatrix},$$ or equivalently, there exists a permutation $\sigma$ of $\{1,2,\cdots,n\}$ and a $1\leq k\leq n-1$ such that the sub-matrix of $A$ in rows $\sigma(1),\cdots,\sigma(k)$ and columns $\sigma(k+1),\cdots,\sigma(n)$ being $0$. $A^T$ is the transpose of $A$. $\rho(A)$ is the spectral radius of $A$, that is, the largest modulus of the eigenvalues of $A$. And now I have no idea on it. However, it is intuitively right. As there are more symmetry in the matrix, the spectral radius becomes larger.",,"['linear-algebra', 'matrices']"
62,Range of vectors that turn into eigenvectors after recursive multiplication by a matrix,Range of vectors that turn into eigenvectors after recursive multiplication by a matrix,,"Suppose $\mathbf{x}$ is a vector, and $\mathbf{A}$ is a square matrix. Which $\mathbf{x}$'s will satisfy the equation $\mathbf{A}^n\mathbf{x} = \lambda\mathbf{A}^{n-1}\mathbf{x}$, where $\lambda$ is an eigenvalue of the matrix and $n \in\mathbb{N^+}$? So I want to determine what range of vectors $\mathbf{x}$ will generate an eigenvector $\mathbf{y}$ for the matrix $\mathbf{A}$ where $\mathbf{y}=\mathbf{A^{n-1}x}$. Also, is the value of $n$ needed for a specific vector predictable?","Suppose $\mathbf{x}$ is a vector, and $\mathbf{A}$ is a square matrix. Which $\mathbf{x}$'s will satisfy the equation $\mathbf{A}^n\mathbf{x} = \lambda\mathbf{A}^{n-1}\mathbf{x}$, where $\lambda$ is an eigenvalue of the matrix and $n \in\mathbb{N^+}$? So I want to determine what range of vectors $\mathbf{x}$ will generate an eigenvector $\mathbf{y}$ for the matrix $\mathbf{A}$ where $\mathbf{y}=\mathbf{A^{n-1}x}$. Also, is the value of $n$ needed for a specific vector predictable?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
63,Estimate the Cauchy integral for matrix-valued functions,Estimate the Cauchy integral for matrix-valued functions,,"Recently, I became familiar with the concept of the ""matrix function via Cauchy integral"", i.e., $$f(A):=\frac{1}{2\pi i}\int_\varGamma f(z)(zI-A)^{-1} \mathrm{d}z$$ Furthermore, it can be shown that $$\left\|\frac{1}{2\pi i}\int_\varGamma f(z)(zI-A)^{-1} \mathrm{d}z\right\| \leq\frac{1}{2\pi}\int_\varGamma \lvert f(z)\rvert \|(zI-A)^{-1} \|\mathrm{d}z$$  where $\|\cdot \|$ denotes the induced $2-$norm. But I can not prove and understand the reason of the above inequality. Can you help me prove it? I really appreciate the time you spend to read this note.","Recently, I became familiar with the concept of the ""matrix function via Cauchy integral"", i.e., $$f(A):=\frac{1}{2\pi i}\int_\varGamma f(z)(zI-A)^{-1} \mathrm{d}z$$ Furthermore, it can be shown that $$\left\|\frac{1}{2\pi i}\int_\varGamma f(z)(zI-A)^{-1} \mathrm{d}z\right\| \leq\frac{1}{2\pi}\int_\varGamma \lvert f(z)\rvert \|(zI-A)^{-1} \|\mathrm{d}z$$  where $\|\cdot \|$ denotes the induced $2-$norm. But I can not prove and understand the reason of the above inequality. Can you help me prove it? I really appreciate the time you spend to read this note.",,"['integration', 'complex-analysis', 'matrices', 'inequality', 'normed-spaces']"
64,Finding unknown matrices in a set of simultaneous matrix equations,Finding unknown matrices in a set of simultaneous matrix equations,,"I've come across a thorny problem in my research, which is too complicated and specific to ask here. However, it bears some similarity to the following problem, and understanding how to solve this ""toy"" version might help me to solve my actual problem. So, suppose I have three real matrices $X$ $(n\times p)$, $Y$ $(m\times n)$ and $Z$ $(p\times m)$, and I wish to find another three (real) matrices $A$ $(n\times m)$, $B$ $(m\times p)$ and $C$ $(p\times n)$ such that $$ AB = X\\ BC = Y\\ CA = Z. $$ How can I find $A$, $B$ and $C$? This is effectively $mn+np+pm$ equations in $mn+np+pm$ unknowns, so it seems like it might have a unique solution, but the equations are not linear. We can't assume the matrices are invertible, because in general they're not square. How could such a problem be approached? If there's no analytical solution, hints on how to do solve this sort of thing numerically would be appreciated.","I've come across a thorny problem in my research, which is too complicated and specific to ask here. However, it bears some similarity to the following problem, and understanding how to solve this ""toy"" version might help me to solve my actual problem. So, suppose I have three real matrices $X$ $(n\times p)$, $Y$ $(m\times n)$ and $Z$ $(p\times m)$, and I wish to find another three (real) matrices $A$ $(n\times m)$, $B$ $(m\times p)$ and $C$ $(p\times n)$ such that $$ AB = X\\ BC = Y\\ CA = Z. $$ How can I find $A$, $B$ and $C$? This is effectively $mn+np+pm$ equations in $mn+np+pm$ unknowns, so it seems like it might have a unique solution, but the equations are not linear. We can't assume the matrices are invertible, because in general they're not square. How could such a problem be approached? If there's no analytical solution, hints on how to do solve this sort of thing numerically would be appreciated.",,"['linear-algebra', 'matrices', 'matrix-equations']"
65,change the basis of a matrix,change the basis of a matrix,,"Is the methodology to change the basis of a matrix the same as changing the basis of a vector?  For example, if I had $A : \mathbb{R}^2 \to \mathbb{R}^2$ $$A=\begin{pmatrix} 3 & -5 \\ 2 & 7 \end{pmatrix}$$ in the standard basis and wanted it in the basis $v_1 = (1,3), v_2=(2,5)$. To do this, I simply multiply $A * \begin{pmatrix} 1 & 2 \\ 3 & 5 \end{pmatrix}$ to get $\begin{pmatrix} -12 & -19 \\ 23 & 39 \end{pmatrix}$?  Is this correct?","Is the methodology to change the basis of a matrix the same as changing the basis of a vector?  For example, if I had $A : \mathbb{R}^2 \to \mathbb{R}^2$ $$A=\begin{pmatrix} 3 & -5 \\ 2 & 7 \end{pmatrix}$$ in the standard basis and wanted it in the basis $v_1 = (1,3), v_2=(2,5)$. To do this, I simply multiply $A * \begin{pmatrix} 1 & 2 \\ 3 & 5 \end{pmatrix}$ to get $\begin{pmatrix} -12 & -19 \\ 23 & 39 \end{pmatrix}$?  Is this correct?",,"['linear-algebra', 'matrices']"
66,Solve the system of linear equations by Gaussian elimination and back-substitution.,Solve the system of linear equations by Gaussian elimination and back-substitution.,,"Question 1 :Form the adjunct matrix and reduce it to echelon form. I dont know how to write matrices here, so i snap a picture of my operation. Did I do it right? Question 2 : Use back-substitution to solve for x, y and z For this question, i got z = $ {11\over 3} $ , y = $ {28\over 3} $ and x = $ {-26\over 3} $ Question 3: Geometrically, what does the solution represent? I'm a bit confuse about this question. Did the solution represent a rectangle? Can someone care to explain it to me?","Question 1 :Form the adjunct matrix and reduce it to echelon form. I dont know how to write matrices here, so i snap a picture of my operation. Did I do it right? Question 2 : Use back-substitution to solve for x, y and z For this question, i got z = $ {11\over 3} $ , y = $ {28\over 3} $ and x = $ {-26\over 3} $ Question 3: Geometrically, what does the solution represent? I'm a bit confuse about this question. Did the solution represent a rectangle? Can someone care to explain it to me?",,"['linear-algebra', 'matrices']"
67,Negative determinant,Negative determinant,,"Let $$ A = \begin{bmatrix} -a_{12}-a_{13}-a_{14} & a_{12} & a_{13} & 1\\ a_{21} & -a_{21}-a_{23}-a_{24} & a_{23} & 1\\ a_{31} & a_{32} & -a_{31} - a_{32} - a_{34} & 1\\ a_{41} & a_{42} & a_{43} & 1 \end{bmatrix}, $$ where all $a_{ij}$'s are positive reals. If we explicitly calculate the determinant of $A$ and factor whole expression, then we can easily see that $\det(A) < 0$. Is it possible to prove that $\det(A) < 0$ (or that $\det(A) \neq 0$ if it is easier) using some matrix manipulations without calculating it directly?","Let $$ A = \begin{bmatrix} -a_{12}-a_{13}-a_{14} & a_{12} & a_{13} & 1\\ a_{21} & -a_{21}-a_{23}-a_{24} & a_{23} & 1\\ a_{31} & a_{32} & -a_{31} - a_{32} - a_{34} & 1\\ a_{41} & a_{42} & a_{43} & 1 \end{bmatrix}, $$ where all $a_{ij}$'s are positive reals. If we explicitly calculate the determinant of $A$ and factor whole expression, then we can easily see that $\det(A) < 0$. Is it possible to prove that $\det(A) < 0$ (or that $\det(A) \neq 0$ if it is easier) using some matrix manipulations without calculating it directly?",,"['matrices', 'determinant']"
68,Partition of a Matrix,Partition of a Matrix,,"In Linear Algebra, we have been taught that the partition of a matrix $A$ consists of matrices,or blocks. In other words, its elements are matrices. This same, partitioned matrix, however is said to be equal to the original matrix. But their elements are different, as one contains scalars and the other matrices. Please help me understand.","In Linear Algebra, we have been taught that the partition of a matrix $A$ consists of matrices,or blocks. In other words, its elements are matrices. This same, partitioned matrix, however is said to be equal to the original matrix. But their elements are different, as one contains scalars and the other matrices. Please help me understand.",,"['linear-algebra', 'matrices', 'block-matrices']"
69,Is $\mathrm{col}(\lambda I_n-A)\subseteq \mathrm{col}(B) $ for a complex $\lambda$?,Is  for a complex ?,\mathrm{col}(\lambda I_n-A)\subseteq \mathrm{col}(B)  \lambda,"Let $A\in\mathbb{R}^{n\times n}$, let $I_n$ denote the identity matrix of order $n$, and let $ \mathrm{col}$ denote column space. I'm interested in understanding for what values of $\lambda \in \mathbb{C} $ there exists a full column rank matrix $B\in\mathbb{R}^{n\times m}$, with $m<n$, such that $\mathrm{col}(\lambda I_n-A)\subseteq \mathrm{col}(B) $. Clearly $\lambda$ must be an eigenvalue of $A$ because otherwise $\mathrm{col}(\lambda I_n-A)$ is $n$-dimensional and hence cannot be in the $m$-dimensional subspace $\mathrm{col}(B) $. If $\lambda$ is a real eigenvalue of $A$, we can certainly find a $B$ such that $\mathrm{col}(\lambda I_n-A)\subseteq \mathrm{col}(B) $. What if $\lambda$ is a (non-real) complex eigenvalues of $A$?","Let $A\in\mathbb{R}^{n\times n}$, let $I_n$ denote the identity matrix of order $n$, and let $ \mathrm{col}$ denote column space. I'm interested in understanding for what values of $\lambda \in \mathbb{C} $ there exists a full column rank matrix $B\in\mathbb{R}^{n\times m}$, with $m<n$, such that $\mathrm{col}(\lambda I_n-A)\subseteq \mathrm{col}(B) $. Clearly $\lambda$ must be an eigenvalue of $A$ because otherwise $\mathrm{col}(\lambda I_n-A)$ is $n$-dimensional and hence cannot be in the $m$-dimensional subspace $\mathrm{col}(B) $. If $\lambda$ is a real eigenvalue of $A$, we can certainly find a $B$ such that $\mathrm{col}(\lambda I_n-A)\subseteq \mathrm{col}(B) $. What if $\lambda$ is a (non-real) complex eigenvalues of $A$?",,"['linear-algebra', 'matrices', 'vector-spaces', 'complex-numbers', 'eigenvalues-eigenvectors']"
70,Mutually commuting matrices,Mutually commuting matrices,,"Let $A_{1},..., A_{m}$ be $n \times n$ matrices with entries in a field $K$ such that $A_{i}A_{j} = A_{j}A_{i}$ for all $ 1 \leq i, j \leq n$ and the product $A_{1}A_{2} ... A_{m} = 0$ is the zero matrix. Prove that there are $h \leq n$ distinct indices $i_{1}, ..., i_{h}$ such that $A_{i_{1}} ... A_{i_{h}} = 0$. I showed, by inducting on $m$, that the matrices in question have a set of common eigenvectors, say $B$ and not necessarily same eigenvalues. Then on $B$, we see that some eigenvalues have to be zeros. From here, I can use a hint. Someone mentioned a hint: reduce the rank to zero, which did not get me very far [sorry to you, I posted this problem in math overflow, which was not appropriate for a mere linear algebra qual prep problems and so I moved it here].","Let $A_{1},..., A_{m}$ be $n \times n$ matrices with entries in a field $K$ such that $A_{i}A_{j} = A_{j}A_{i}$ for all $ 1 \leq i, j \leq n$ and the product $A_{1}A_{2} ... A_{m} = 0$ is the zero matrix. Prove that there are $h \leq n$ distinct indices $i_{1}, ..., i_{h}$ such that $A_{i_{1}} ... A_{i_{h}} = 0$. I showed, by inducting on $m$, that the matrices in question have a set of common eigenvectors, say $B$ and not necessarily same eigenvalues. Then on $B$, we see that some eigenvalues have to be zeros. From here, I can use a hint. Someone mentioned a hint: reduce the rank to zero, which did not get me very far [sorry to you, I posted this problem in math overflow, which was not appropriate for a mere linear algebra qual prep problems and so I moved it here].",,"['linear-algebra', 'matrices']"
71,Determinant (or positive definiteness) of a Hankel matrix,Determinant (or positive definiteness) of a Hankel matrix,,"I need to prove that the Hankel matrix given by $a_{ij}=\frac{1}{i+j}$ is positive definite. It turns out that it is a special case of the Cauchy matrices, and the determinant is given by the Cauchy determinant. But I also want to use some problem-specific techniques. Inspired by the Hilbert matrix, I want to find a function $u(x)$, such that $\int_0^1 x^{k}u(x)dx=1/k$ for any $k \in \mathbb{N}$, so that $a_{ij}=\int_0^1 x^{i}x^{j}u(x)dx$, and $A$ can be considered as the Gramian of $\{ 1, x, \dots, x^n \}$ in the space of polynomials, with the inner product $(f,g)=\int_0^1 f(x)g(x)u(x)dx$. Is it possible?","I need to prove that the Hankel matrix given by $a_{ij}=\frac{1}{i+j}$ is positive definite. It turns out that it is a special case of the Cauchy matrices, and the determinant is given by the Cauchy determinant. But I also want to use some problem-specific techniques. Inspired by the Hilbert matrix, I want to find a function $u(x)$, such that $\int_0^1 x^{k}u(x)dx=1/k$ for any $k \in \mathbb{N}$, so that $a_{ij}=\int_0^1 x^{i}x^{j}u(x)dx$, and $A$ can be considered as the Gramian of $\{ 1, x, \dots, x^n \}$ in the space of polynomials, with the inner product $(f,g)=\int_0^1 f(x)g(x)u(x)dx$. Is it possible?",,"['linear-algebra', 'matrices', 'determinant', 'inner-products', 'positive-definite']"
72,"Determine the values of $k$ so that the following linear system has unique, infinite and no solutions.","Determine the values of  so that the following linear system has unique, infinite and no solutions.",k,"Determine the values of $k$ so that the following linear system has a unique solution, infinite solutions and no solution. $2x + (k + 1)y + 2z = 3$ $2x + 3y + kz = 3$ $3x + 3y − 3z = 3$ I have tried to use the determinant of the matrix to solve but have got stuck. \begin{bmatrix} 2 & k+1 & 2 \\ 2 & 3 & k \\ 3 & 3 & 3 \end{bmatrix} I have found the determinant to be $3(k^2-3k+2)$ which I'm pretty sure is correct. I start getting confused with what to do with the determinant now though. I've read a few of the questions here but I just don't get it - solving equal to $0$ gives $k=1$ and $k=2$, but how do I interpret these values?","Determine the values of $k$ so that the following linear system has a unique solution, infinite solutions and no solution. $2x + (k + 1)y + 2z = 3$ $2x + 3y + kz = 3$ $3x + 3y − 3z = 3$ I have tried to use the determinant of the matrix to solve but have got stuck. \begin{bmatrix} 2 & k+1 & 2 \\ 2 & 3 & k \\ 3 & 3 & 3 \end{bmatrix} I have found the determinant to be $3(k^2-3k+2)$ which I'm pretty sure is correct. I start getting confused with what to do with the determinant now though. I've read a few of the questions here but I just don't get it - solving equal to $0$ gives $k=1$ and $k=2$, but how do I interpret these values?",,"['linear-algebra', 'matrices', 'determinant', 'systems-of-equations']"
73,Best way to solve specific block-tridiagonal linear system (10000x10000 and larger),Best way to solve specific block-tridiagonal linear system (10000x10000 and larger),,"To provide more context, this system came from energy balance equation on a mesh with (n,m) nodes in each direction. It's a linear system that looks like this (size of system in blocks n = 4, size of blocks m = 4, 'd' means 'some value'): $$ \left| \begin{array}{cccc|cccc|cccc|cccc}\hline 1 &   &   &   & d & d & d & d &  &  &  &  &  &  &  & \\ 1 &-1 &   &   &   &   &   &   &  &  &  &  &  &  &  & \\ 1 &   &-1 &   &   &   &   &   &  &  &  &  &  &  &  & \\ 1 &   &   &-1 &   &   &   &   &  &  &  &  &  &  &  & \\ \hline 1 &   &   &   & d & d &   & d & d &  &  &  &  &  &  & \\   & 1 &   &   & d & d & d &   &  & d &  &  &  &  &  & \\   &   & 1 &   &   & d & d & d &  &  & d &  &  &  &  & \\   &   &   & 1 & d &   & d & d &  &  &  & d &  &  &  & \\ \hline &  &  &  & 1 &   &   &   & d & d &   & d & d &  &  &  \\ &  &  &  &   & 1 &   &   & d & d & d &   &  & d &  &  \\ &  &  &  &   &   & 1 &   &   & d & d & d &  &  & d &  \\ &  &  &  &   &   &   & 1 & d &   & d & d &  &  &  & d \\ \hline &  &  &  &   &  &  &  & 1 &   &   &   & d & d &   & d \\ &  &  &  &   &  &  &  &   & 1 &   &   & d & d & d &   \\ &  &  &  &   &  &  &  &   &   & 1 &   &   & d & d & d \\ &  &  &  &   &  &  &  &   &   &   & 1 & d &   & d & d \\ \hline \end{array} \right| $$ [Part of Update 3: further down you may find a genius idea of an uninformed lunatic. The idea below ruins the diagonal dominance for the matrix, so while this New Revolutionary Elimination is significantly faster, it's useless.] By moving first m equations to the last line, i get almost-upper-triangular system: $$ \left| \begin{array}{cccc|cccc|cccc|cccc}\hline 1 &   &   &   & d & d &   & d & d &  &  &  &  &  &  & \\   & 1 &   &   & d & d & d &   &  & d &  &  &  &  &  & \\   &   & 1 &   &   & d & d & d &  &  & d &  &  &  &  & \\   &   &   & 1 & d &   & d & d &  &  &  & d &  &  &  & \\ \hline &  &  &  & 1 &   &   &   & d & d &   & d & d &  &  &  \\ &  &  &  &   & 1 &   &   & d & d & d &   &  & d &  &  \\ &  &  &  &   &   & 1 &   &   & d & d & d &  &  & d &  \\ &  &  &  &   &   &   & 1 & d &   & d & d &  &  &  & d \\ \hline &  &  &  &   &  &  &  & 1 &   &   &   & d & d &   & d \\ &  &  &  &   &  &  &  &   & 1 &   &   & d & d & d &   \\ &  &  &  &   &  &  &  &   &   & 1 &   &   & d & d & d \\ &  &  &  &   &  &  &  &   &   &   & 1 & d &   & d & d \\ \hline 1 &   &   &   & d & d & d & d &  &  &  &  &  &  &  & \\ 1 &-1 &   &   &   &   &   &   &  &  &  &  &  &  &  & \\ 1 &   &-1 &   &   &   &   &   &  &  &  &  &  &  &  & \\ 1 &   &   &-1 &   &   &   &   &  &  &  &  &  &  &  & \\ \hline \end{array} \right| $$ Using the gaussian eliminiation, i can upper-triangularize it in O(n m^2 + m^3) operations. Other approach i've tested is to use matrix version of Thomas algorithm, but it costs O(n m^3) actions. I really need this speed-up of gaussian elimination, as both n and m are going to be somewhat like 100-200-300, but gaussian elimination aggregates double rounding error tremendously fast. With N and M over 15, roots of such system calculated with the use of elimination already differ from ""true"" roots (calculated with the use of LU decomposition) by relative difference of 0.1. If i scale system to even larger M,N, the only thing that i get right with elimination is an order of magnitude of roots, and even that i wont get anymore around N=M=70. Thomas algorithm works perfect, but i need this speed-up badly. So, the question is. Is there a way to solve such system in less time than O(n*m^3) without losing precision? Mathematically such thing is possible (gaussian elimination ftw), but is there a way i can get precise for at least 3-4-5 significant values roots fast? Update 1: forgot to post some more info: precision fall comes from pure gaussian elimination of last lower-right m-sized block. I could solve that last block as separate system with some precise algorithm, but that would complicate overall method a lot. Is there any other ways? Update 2: Sadly, information in Update 1 is incomplete. Further test have shown that, while last block indeed causes most of error, the whole elimination process wrecks precision. If i do not triangularize last block and try to solve the entire system from this point with QR/LU, around N=M=50 i only get order of magnitude of roots correctly. Update 3: Huh, wasn't expecting any answers after such a pause :D But @Armadillo Jim noticing this post means it still comes up on some searches, so i guess i can provide an update for everyone interested. Answering his question on why i don't use iterative solvers: The matrices are very badly conditioned. From time to time i have to solve batches of 100+ such systems corresponding to radiative transfer in different parts of spectrum, and noble gases' spectra are not the smoothest things that exist. This results in a ton of systems that only have their shape similar. That means if i am to use iterative solver, i have to recalculate preconditioner for every matrix each time i want to solve aforementioned batch of systems, and preconditioners are just wasted after that, as i am solving those systems exactly once for any given left part (A in AX=B). Of course i could use no preconditioner at all, but this results in thousands of iterations and outright diverging. If somebody is interested, possible ways of solving those systems on regular meshes efficiently are the family of O(N^1.5)..(N^2) Nested Dissection methods (Nested Dissection of a Regular Finite Element Mesh, Alan George, Siam J. on Numerical Analysis, vol. 10, #2, Apr. 1973, 345-363) and their somewhat descendant based on specific properties of hierarchicaly-blockseparable matrices (O(N), A.Gillman, Fast direct solvers for elliptic partial differential equations, phd thesis, 2011). Sadly i may have to move to adaptive meshes and cell-based approach, as now i have to add gas dynamics to the model and single test may run for a week straight on our department's cluster, and if that's the case, i can as well raise mesh resolution and use iterative solvers or what else is used for sparse matrices.","To provide more context, this system came from energy balance equation on a mesh with (n,m) nodes in each direction. It's a linear system that looks like this (size of system in blocks n = 4, size of blocks m = 4, 'd' means 'some value'): $$ \left| \begin{array}{cccc|cccc|cccc|cccc}\hline 1 &   &   &   & d & d & d & d &  &  &  &  &  &  &  & \\ 1 &-1 &   &   &   &   &   &   &  &  &  &  &  &  &  & \\ 1 &   &-1 &   &   &   &   &   &  &  &  &  &  &  &  & \\ 1 &   &   &-1 &   &   &   &   &  &  &  &  &  &  &  & \\ \hline 1 &   &   &   & d & d &   & d & d &  &  &  &  &  &  & \\   & 1 &   &   & d & d & d &   &  & d &  &  &  &  &  & \\   &   & 1 &   &   & d & d & d &  &  & d &  &  &  &  & \\   &   &   & 1 & d &   & d & d &  &  &  & d &  &  &  & \\ \hline &  &  &  & 1 &   &   &   & d & d &   & d & d &  &  &  \\ &  &  &  &   & 1 &   &   & d & d & d &   &  & d &  &  \\ &  &  &  &   &   & 1 &   &   & d & d & d &  &  & d &  \\ &  &  &  &   &   &   & 1 & d &   & d & d &  &  &  & d \\ \hline &  &  &  &   &  &  &  & 1 &   &   &   & d & d &   & d \\ &  &  &  &   &  &  &  &   & 1 &   &   & d & d & d &   \\ &  &  &  &   &  &  &  &   &   & 1 &   &   & d & d & d \\ &  &  &  &   &  &  &  &   &   &   & 1 & d &   & d & d \\ \hline \end{array} \right| $$ [Part of Update 3: further down you may find a genius idea of an uninformed lunatic. The idea below ruins the diagonal dominance for the matrix, so while this New Revolutionary Elimination is significantly faster, it's useless.] By moving first m equations to the last line, i get almost-upper-triangular system: $$ \left| \begin{array}{cccc|cccc|cccc|cccc}\hline 1 &   &   &   & d & d &   & d & d &  &  &  &  &  &  & \\   & 1 &   &   & d & d & d &   &  & d &  &  &  &  &  & \\   &   & 1 &   &   & d & d & d &  &  & d &  &  &  &  & \\   &   &   & 1 & d &   & d & d &  &  &  & d &  &  &  & \\ \hline &  &  &  & 1 &   &   &   & d & d &   & d & d &  &  &  \\ &  &  &  &   & 1 &   &   & d & d & d &   &  & d &  &  \\ &  &  &  &   &   & 1 &   &   & d & d & d &  &  & d &  \\ &  &  &  &   &   &   & 1 & d &   & d & d &  &  &  & d \\ \hline &  &  &  &   &  &  &  & 1 &   &   &   & d & d &   & d \\ &  &  &  &   &  &  &  &   & 1 &   &   & d & d & d &   \\ &  &  &  &   &  &  &  &   &   & 1 &   &   & d & d & d \\ &  &  &  &   &  &  &  &   &   &   & 1 & d &   & d & d \\ \hline 1 &   &   &   & d & d & d & d &  &  &  &  &  &  &  & \\ 1 &-1 &   &   &   &   &   &   &  &  &  &  &  &  &  & \\ 1 &   &-1 &   &   &   &   &   &  &  &  &  &  &  &  & \\ 1 &   &   &-1 &   &   &   &   &  &  &  &  &  &  &  & \\ \hline \end{array} \right| $$ Using the gaussian eliminiation, i can upper-triangularize it in O(n m^2 + m^3) operations. Other approach i've tested is to use matrix version of Thomas algorithm, but it costs O(n m^3) actions. I really need this speed-up of gaussian elimination, as both n and m are going to be somewhat like 100-200-300, but gaussian elimination aggregates double rounding error tremendously fast. With N and M over 15, roots of such system calculated with the use of elimination already differ from ""true"" roots (calculated with the use of LU decomposition) by relative difference of 0.1. If i scale system to even larger M,N, the only thing that i get right with elimination is an order of magnitude of roots, and even that i wont get anymore around N=M=70. Thomas algorithm works perfect, but i need this speed-up badly. So, the question is. Is there a way to solve such system in less time than O(n*m^3) without losing precision? Mathematically such thing is possible (gaussian elimination ftw), but is there a way i can get precise for at least 3-4-5 significant values roots fast? Update 1: forgot to post some more info: precision fall comes from pure gaussian elimination of last lower-right m-sized block. I could solve that last block as separate system with some precise algorithm, but that would complicate overall method a lot. Is there any other ways? Update 2: Sadly, information in Update 1 is incomplete. Further test have shown that, while last block indeed causes most of error, the whole elimination process wrecks precision. If i do not triangularize last block and try to solve the entire system from this point with QR/LU, around N=M=50 i only get order of magnitude of roots correctly. Update 3: Huh, wasn't expecting any answers after such a pause :D But @Armadillo Jim noticing this post means it still comes up on some searches, so i guess i can provide an update for everyone interested. Answering his question on why i don't use iterative solvers: The matrices are very badly conditioned. From time to time i have to solve batches of 100+ such systems corresponding to radiative transfer in different parts of spectrum, and noble gases' spectra are not the smoothest things that exist. This results in a ton of systems that only have their shape similar. That means if i am to use iterative solver, i have to recalculate preconditioner for every matrix each time i want to solve aforementioned batch of systems, and preconditioners are just wasted after that, as i am solving those systems exactly once for any given left part (A in AX=B). Of course i could use no preconditioner at all, but this results in thousands of iterations and outright diverging. If somebody is interested, possible ways of solving those systems on regular meshes efficiently are the family of O(N^1.5)..(N^2) Nested Dissection methods (Nested Dissection of a Regular Finite Element Mesh, Alan George, Siam J. on Numerical Analysis, vol. 10, #2, Apr. 1973, 345-363) and their somewhat descendant based on specific properties of hierarchicaly-blockseparable matrices (O(N), A.Gillman, Fast direct solvers for elliptic partial differential equations, phd thesis, 2011). Sadly i may have to move to adaptive meshes and cell-based approach, as now i have to add gas dynamics to the model and single test may run for a week straight on our department's cluster, and if that's the case, i can as well raise mesh resolution and use iterative solvers or what else is used for sparse matrices.",,"['linear-algebra', 'matrices', 'numerical-linear-algebra']"
74,fast multiplication for a matrix and its transpose.,fast multiplication for a matrix and its transpose.,,I know Strassen and other methods can achieve better than $O(n^3)$ for general square matrix multiplication. I am curious of the spacial case where the multiplication is between  a $n*m$ matrix $A$ with its transpose $A^T$ . Is there known algorithm that computes this case fast? thanks,I know Strassen and other methods can achieve better than $O(n^3)$ for general square matrix multiplication. I am curious of the spacial case where the multiplication is between  a $n*m$ matrix $A$ with its transpose $A^T$ . Is there known algorithm that computes this case fast? thanks,,"['linear-algebra', 'matrices', 'numerical-linear-algebra']"
75,"Spectral norm of symmetric matrices only with the diagonal, the first column, and the first row non-zero","Spectral norm of symmetric matrices only with the diagonal, the first column, and the first row non-zero",,"Consider a real symmetric matrix $$\mathbf{M}=\left[\array{a_0&a_1&a_2&\cdots&a_n\\ a_1&b_1&0&\cdots&0\\ a_2&0& b_2&\cdots&0\\ \vdots &\vdots &\vdots&\ddots&\vdots\\ a_n&0&0&\cdots&b_n}\right].$$ Deriving bounds -- probably loose ones -- for the spectral norm is not that difficult. I wonder if there an explicit formula that gives the spectral norm of $\mathbf{M}$ (i.e., $\left\|\mathbf{M}\right\|)$ in terms of $a_i$s and $b_i$s? If necessary, $a_i$s and $b_i$s may assumed to be non-negative as well. Also, finding reasonably tight bounds would be fine. Update : The simplest bound I can think of is $$\left\|\mathbf{M}\right\|\leq \max\{|a_0|,|b_1|,|b_2|,\dotsc,|b_n|\}+\sqrt{a_1^2+a_2^2+\dotso+a_n^2}.$$","Consider a real symmetric matrix $$\mathbf{M}=\left[\array{a_0&a_1&a_2&\cdots&a_n\\ a_1&b_1&0&\cdots&0\\ a_2&0& b_2&\cdots&0\\ \vdots &\vdots &\vdots&\ddots&\vdots\\ a_n&0&0&\cdots&b_n}\right].$$ Deriving bounds -- probably loose ones -- for the spectral norm is not that difficult. I wonder if there an explicit formula that gives the spectral norm of $\mathbf{M}$ (i.e., $\left\|\mathbf{M}\right\|)$ in terms of $a_i$s and $b_i$s? If necessary, $a_i$s and $b_i$s may assumed to be non-negative as well. Also, finding reasonably tight bounds would be fine. Update : The simplest bound I can think of is $$\left\|\mathbf{M}\right\|\leq \max\{|a_0|,|b_1|,|b_2|,\dotsc,|b_n|\}+\sqrt{a_1^2+a_2^2+\dotso+a_n^2}.$$",,"['linear-algebra', 'matrices', 'numerical-linear-algebra']"
76,Matrix multiplication makes zero to specific elements,Matrix multiplication makes zero to specific elements,,"I have this matrix $A = \pmatrix{3&2&5\\ 1&2&3\\ 2&3&5\\ 1&3&4}$. I want to multiply this matrix by another one, say $M$, which must have its diagonal elements = 1 , and the result must be: $MA = \pmatrix{3&2&5\\ 1&0&3\\ 2&0&5\\ 1&0&4}$. Should I multiply the inv(A)x(result_matrix) to get the matrix $M$ or is there any other procedure? Thanks in advance.","I have this matrix $A = \pmatrix{3&2&5\\ 1&2&3\\ 2&3&5\\ 1&3&4}$. I want to multiply this matrix by another one, say $M$, which must have its diagonal elements = 1 , and the result must be: $MA = \pmatrix{3&2&5\\ 1&0&3\\ 2&0&5\\ 1&0&4}$. Should I multiply the inv(A)x(result_matrix) to get the matrix $M$ or is there any other procedure? Thanks in advance.",,"['linear-algebra', 'matrices']"
77,How to calculate Rotation Matrix in android from accelerometer and magnetometer sensor,How to calculate Rotation Matrix in android from accelerometer and magnetometer sensor,,"I found the rotation matrix returned by SensorManager.getRotationMatrix from link: What's the best 3D angular co-ordinate system for working with smartphone apps The rotation matrix is: But I cannot find the steps to reconstruct this matrix when I used the rotation matrix for each axis as specified in link: http://www.freescale.com/files/sensors/doc/app_note/AN3461.pdf Rotation matrix for each axis I really appreciate if anyone can show me the steps so that I can construct the above rotation matrix because I cannot find any document that shows me the steps to create this matrix. And at the above link, there is also a matrix as following: How can I calculate the matrix Matrix .What does it mean?","I found the rotation matrix returned by SensorManager.getRotationMatrix from link: What's the best 3D angular co-ordinate system for working with smartphone apps The rotation matrix is: But I cannot find the steps to reconstruct this matrix when I used the rotation matrix for each axis as specified in link: http://www.freescale.com/files/sensors/doc/app_note/AN3461.pdf Rotation matrix for each axis I really appreciate if anyone can show me the steps so that I can construct the above rotation matrix because I cannot find any document that shows me the steps to create this matrix. And at the above link, there is also a matrix as following: How can I calculate the matrix Matrix .What does it mean?",,['matrices']
78,What is a reducible algebra?,What is a reducible algebra?,,"In my matrix analysis book, a set of complex matrices is said to be an ""algebra"" if 1)it is a subspace, 2)whenever A and B are members, so is AB. Then it uses the terms reducible and irreducible algebra, without defining them. What are the definitions? EDIT: It's being used within the context of proving Burnside's theorem on matrix algebras... which is written here as... A set S of nxn complex matrices is irreducible if and only if it is the set of all nxn complex matrices.","In my matrix analysis book, a set of complex matrices is said to be an ""algebra"" if 1)it is a subspace, 2)whenever A and B are members, so is AB. Then it uses the terms reducible and irreducible algebra, without defining them. What are the definitions? EDIT: It's being used within the context of proving Burnside's theorem on matrix algebras... which is written here as... A set S of nxn complex matrices is irreducible if and only if it is the set of all nxn complex matrices.",,"['linear-algebra', 'matrices']"
79,Finding the matrix exponantial,Finding the matrix exponantial,,"I have to calculate the following: $e^{xA} ,  A=  \begin{pmatrix}1 & 1 & 1 \\ 1&1&1\\ 1&1&1\\ \end{pmatrix}$ I use the following rule: $e^{xA} = \sum \frac{x^k}{k!}A^k$ Now I'm looking for a matrix such that: $A=S D S^{-1}$. So I calculated the eigenvalues and eigenvectors of A, these are: $\lambda _0=3 , \lambda_{1,2} = 0 $ $\vec{y_0} = \begin{pmatrix} 1\\ 1\\ 1 \end{pmatrix} $,$\vec{y_1} = \begin{pmatrix} 1\\ -1\\ 0 \end{pmatrix} $,$\vec{y_2} = \begin{pmatrix} 1\\ 0\\ -1 \end{pmatrix}$ So I have the following 3 matrices: $  S = \begin{pmatrix} 1&1&1\\ 1&-1&0\\ 1&0&-1 \end{pmatrix} , D = \begin{pmatrix} 3&0&0\\ 0&0&1\\ 0&0&0 \end{pmatrix} , S^{-1}= \begin{pmatrix}  \frac{1}{3} & \frac{1}{3} & \frac{1}{3} \\  \frac{1}{3} & \frac{1}{3} & -\frac{2}{3} \\  \frac{1}{3} & -\frac{2}{3} & \frac{1}{3} \\ \end{pmatrix} $ But when I calculate $SDS^{-1}$ I don't get A. Can someone help me where I have gone wrong. I thought I understood it up until this point, but there must be something wrong with my reasoning because I have checked every thing with mathematica. I also don't know what to do when I have found the correct S and D.","I have to calculate the following: $e^{xA} ,  A=  \begin{pmatrix}1 & 1 & 1 \\ 1&1&1\\ 1&1&1\\ \end{pmatrix}$ I use the following rule: $e^{xA} = \sum \frac{x^k}{k!}A^k$ Now I'm looking for a matrix such that: $A=S D S^{-1}$. So I calculated the eigenvalues and eigenvectors of A, these are: $\lambda _0=3 , \lambda_{1,2} = 0 $ $\vec{y_0} = \begin{pmatrix} 1\\ 1\\ 1 \end{pmatrix} $,$\vec{y_1} = \begin{pmatrix} 1\\ -1\\ 0 \end{pmatrix} $,$\vec{y_2} = \begin{pmatrix} 1\\ 0\\ -1 \end{pmatrix}$ So I have the following 3 matrices: $  S = \begin{pmatrix} 1&1&1\\ 1&-1&0\\ 1&0&-1 \end{pmatrix} , D = \begin{pmatrix} 3&0&0\\ 0&0&1\\ 0&0&0 \end{pmatrix} , S^{-1}= \begin{pmatrix}  \frac{1}{3} & \frac{1}{3} & \frac{1}{3} \\  \frac{1}{3} & \frac{1}{3} & -\frac{2}{3} \\  \frac{1}{3} & -\frac{2}{3} & \frac{1}{3} \\ \end{pmatrix} $ But when I calculate $SDS^{-1}$ I don't get A. Can someone help me where I have gone wrong. I thought I understood it up until this point, but there must be something wrong with my reasoning because I have checked every thing with mathematica. I also don't know what to do when I have found the correct S and D.",,"['linear-algebra', 'matrices']"
80,Using SVD in PCA for image compression,Using SVD in PCA for image compression,,"I found some help material and guided by it tried to implement PCA using SVD in MAtlab for image compression. I did it in this way: I = imread('1.jpg'); I2 = rgb2gray(I); inImageD=double(I3);  [U,S,V]=svd(inImageD);  C = S;  C(5+1:end,:)=0; C(:,5+1:end)=0;  D=U*C*V'; And now D contain a matrix that looks like my image but kindly smoothed, but this matrix has the same dimesion as my start image. I expected that PCA will reduce not only number of features but also dimension.. Obviosly I do not understand something. Well - reduction here means that I can restore initial data with some precision from matrices that much less than my data? But multiplication of these matrices gives me matrix with same dimension as my initial matrix. I want to use reduced data for training a classifier. In this case what I shloud use dor training? In a ML course on Coursera we learned to use PCA with SVD for dimensionality redcution: Steps: 1) Suppose we have four images 50x50. We did form a matrix X 4x2500; First step is normalazing this matrix: X(:,j) = X(:,j) - Mean(X(:,j)); 2) Calculate covariance matrix: Sigma = (1 / m) * X' * X; 3) [U, S, V] = svd(sigma); 4) Obtain reduced examples of images: Zi = U'reduce * X(1,:); where Ureduce is subset of matrix U. If we had an image with 2500 pixels then with this action we can obtain an image of, for example, 1600 pixels(any number, depends of how many columns we will leave in Ureduce from U).","I found some help material and guided by it tried to implement PCA using SVD in MAtlab for image compression. I did it in this way: I = imread('1.jpg'); I2 = rgb2gray(I); inImageD=double(I3);  [U,S,V]=svd(inImageD);  C = S;  C(5+1:end,:)=0; C(:,5+1:end)=0;  D=U*C*V'; And now D contain a matrix that looks like my image but kindly smoothed, but this matrix has the same dimesion as my start image. I expected that PCA will reduce not only number of features but also dimension.. Obviosly I do not understand something. Well - reduction here means that I can restore initial data with some precision from matrices that much less than my data? But multiplication of these matrices gives me matrix with same dimension as my initial matrix. I want to use reduced data for training a classifier. In this case what I shloud use dor training? In a ML course on Coursera we learned to use PCA with SVD for dimensionality redcution: Steps: 1) Suppose we have four images 50x50. We did form a matrix X 4x2500; First step is normalazing this matrix: X(:,j) = X(:,j) - Mean(X(:,j)); 2) Calculate covariance matrix: Sigma = (1 / m) * X' * X; 3) [U, S, V] = svd(sigma); 4) Obtain reduced examples of images: Zi = U'reduce * X(1,:); where Ureduce is subset of matrix U. If we had an image with 2500 pixels then with this action we can obtain an image of, for example, 1600 pixels(any number, depends of how many columns we will leave in Ureduce from U).",,"['matrices', 'matlab', 'svd']"
81,Tricky question on induction and characteristic polynomials,Tricky question on induction and characteristic polynomials,,"I am to prove via induction that for any $n \times n$ matrix $A$, the characteristic polynomial of $A$ has degree $n$; $(-1)^n$ as the coefficient of the $\lambda ^n$ terms; $(-1)^{n-1}\cdot \text{Trace}(A)$ as the coefficient of $\lambda ^{n-1}$. I have shown the first two, but can't seem to prove the third.  Could somebody give me a hint?","I am to prove via induction that for any $n \times n$ matrix $A$, the characteristic polynomial of $A$ has degree $n$; $(-1)^n$ as the coefficient of the $\lambda ^n$ terms; $(-1)^{n-1}\cdot \text{Trace}(A)$ as the coefficient of $\lambda ^{n-1}$. I have shown the first two, but can't seem to prove the third.  Could somebody give me a hint?",,"['linear-algebra', 'matrices', 'polynomials', 'trace', 'characteristic-polynomial']"
82,How to find the inverse of a symmetric tridiagonal Toeplitz matrix?,How to find the inverse of a symmetric tridiagonal Toeplitz matrix?,,"Let $a, b>0$ , and the matrix $A_{n \times n}$ and such $$A=\begin{bmatrix} a&b&0&\cdots&0&0\\ b&a&b&\cdots&0&0\\ 0&b&a&\cdots&0&0\\ \cdots&\cdots&\cdots&\cdots&\cdots&\cdots\\ \cdots&\cdots&\cdots&b&a&b\\ \cdots&\cdots&\cdots&\cdots&b&a \end{bmatrix}$$ Find the inverse $A^{-1}$ . My idea: $$A^{-1}=\dfrac{A^{*}}{|A|}$$ and let $|A|=D_{n}$ , then we have $$D_{n}=aD_{n-1}-b^2D_{n-2}$$ so $$D_{n}=\begin{cases} (n+1)\left(\dfrac{a}{2}\right)^n,a^2=4b^2\\ \dfrac{(a+\sqrt{a^2-4b^2})^{n+1}-(a-\sqrt{a^2-4b^2})^{n+1}}{2^{n-1}\sqrt{a^2-4b^2}}&a^2\neq 4b^2 \end{cases}$$ and then I fell very ugly, do you have other methods? Thank you","Let , and the matrix and such Find the inverse . My idea: and let , then we have so and then I fell very ugly, do you have other methods? Thank you","a, b>0 A_{n \times n} A=\begin{bmatrix}
a&b&0&\cdots&0&0\\
b&a&b&\cdots&0&0\\
0&b&a&\cdots&0&0\\
\cdots&\cdots&\cdots&\cdots&\cdots&\cdots\\
\cdots&\cdots&\cdots&b&a&b\\
\cdots&\cdots&\cdots&\cdots&b&a
\end{bmatrix} A^{-1} A^{-1}=\dfrac{A^{*}}{|A|} |A|=D_{n} D_{n}=aD_{n-1}-b^2D_{n-2} D_{n}=\begin{cases}
(n+1)\left(\dfrac{a}{2}\right)^n,a^2=4b^2\\
\dfrac{(a+\sqrt{a^2-4b^2})^{n+1}-(a-\sqrt{a^2-4b^2})^{n+1}}{2^{n-1}\sqrt{a^2-4b^2}}&a^2\neq 4b^2
\end{cases}","['linear-algebra', 'matrices', 'inverse', 'tridiagonal-matrices', 'toeplitz-matrices']"
83,Gershgorin Circle Theorem: counterexample to a statement in the proof?,Gershgorin Circle Theorem: counterexample to a statement in the proof?,,"I have been struggling to comprehend the proof of Gershgorin Circle Theorem for a long time now, but I think I have come upon a counterexample. I'm probably wrong, but please tell me where I'm wrong... Theorem: Every eigenvalue of A lies within at least one of the Gershgorin discs $D(a_{ii},R_i)$. Proof: $$Av=\lambda v \implies \sum_{j=1}^n a_{ij}x_j = \lambda x_i \ \ \forall \ \ i \in \{1,2,...,n\}$$ where $|x_i|=\max_{j} {x_j}$ for $x_j$ elements of the eigenvector. And so on. But what if $$A=\left(\begin{matrix} 1 & 0 \\ 1 & -1 \end{matrix}\right), \ \ \lambda_1 = -1, \lambda_2=1 \ \ , v_1=(0,1)^T?$$ Then $x_1=0$ always, $x_2$ is free. Then $x_i = 1$, and $a_{11}+a_{12}=0 \neq (-1)(1) = 1$. What am I missing?","I have been struggling to comprehend the proof of Gershgorin Circle Theorem for a long time now, but I think I have come upon a counterexample. I'm probably wrong, but please tell me where I'm wrong... Theorem: Every eigenvalue of A lies within at least one of the Gershgorin discs $D(a_{ii},R_i)$. Proof: $$Av=\lambda v \implies \sum_{j=1}^n a_{ij}x_j = \lambda x_i \ \ \forall \ \ i \in \{1,2,...,n\}$$ where $|x_i|=\max_{j} {x_j}$ for $x_j$ elements of the eigenvector. And so on. But what if $$A=\left(\begin{matrix} 1 & 0 \\ 1 & -1 \end{matrix}\right), \ \ \lambda_1 = -1, \lambda_2=1 \ \ , v_1=(0,1)^T?$$ Then $x_1=0$ always, $x_2$ is free. Then $x_i = 1$, and $a_{11}+a_{12}=0 \neq (-1)(1) = 1$. What am I missing?",,['linear-algebra']
84,Linear Algebra: Identity map,Linear Algebra: Identity map,,"I was asked to prove that the identity map $id : \Bbb R^n \to \Bbb R^n $ can be represented by the the identity matrix regardless of the basis My Attempt: Let $\mathcal B  = \lbrace v_1 , ...,v_n \rbrace $ be a basis for $\Bbb R^n$. The image under $id$ of each element in $\mathcal B$ can be expressed as follows $id(v_1 ) = 1v_1 + 0 v_2 +... + 0v_n$ . . . $id(v_n) = 0v_1 + ... +0v_{n-1}+1v_n$ So $id$ can be represented by $I_n$ However, I am not convinced that this a rigorous and complete enough proof. If not, how would I go about proving this?","I was asked to prove that the identity map $id : \Bbb R^n \to \Bbb R^n $ can be represented by the the identity matrix regardless of the basis My Attempt: Let $\mathcal B  = \lbrace v_1 , ...,v_n \rbrace $ be a basis for $\Bbb R^n$. The image under $id$ of each element in $\mathcal B$ can be expressed as follows $id(v_1 ) = 1v_1 + 0 v_2 +... + 0v_n$ . . . $id(v_n) = 0v_1 + ... +0v_{n-1}+1v_n$ So $id$ can be represented by $I_n$ However, I am not convinced that this a rigorous and complete enough proof. If not, how would I go about proving this?",,"['linear-algebra', 'matrices', 'proof-verification']"
85,Proof Strategy - Prove that each eigenvalue of $A^{2}$ is real and is less than or equal to zero - 2011 8C,Proof Strategy - Prove that each eigenvalue of  is real and is less than or equal to zero - 2011 8C,A^{2},"Remember that we've already proven the following, for  any real symmetric $n\times n$ matrix $M$: (i) Each eigenvalue of $M$ is real. (ii) Each eigenvector can be chosen to be real. (iii) Eigenvectors with different eigenvalues are orthogonal. (b) Let $A$ be a real antisymmetric $n\times n$ matrix. Prove that each eigenvalue of $A^{2}$ is real and is less than or equal to zero. $(A^2)^T = (A^T)^2 = (-A)^2 = A^2$, so $A^2$ is real symmetric. By virtue of (i) above, the eigenvalues of $A^2$ must be real. $1$. How would you determine to prove that $A^2$ is symmetric, so that you can benefit from (i) ? Let $A^2v = \color{orangered}{ k \; \mathbf{ v } }$, where k is a scalar. By (ii) above, hypothesise that $\mathbf{ v }$ is real. Then $\begin{align} k \mathbf{ v^Tv } & = v^T \; \color{orangered}{ k \; \mathbf{ v } } =  v^T \; \color{forestgreen}{ A^2 }v = v^T \color{forestgreen}{ AA } v  = v^T\color{forestgreen}{ (-A^T)A }v \\ & = -(Av)^T(Av) < 0 \end{align}$. $2.$ The question asks us to prove $k < 0$, but what's the proof strategy? The trick looks like to consider $k \mathbf{ v^Tv } $, but how would you determine/divine/previse this? I remember $\langle v,v \rangle := v^Tv \ge 0$. $3.$ I'm not asking about the algebra itself, but what's the strategy behind it here?    The last few steps feel too clever/guileful?","Remember that we've already proven the following, for  any real symmetric $n\times n$ matrix $M$: (i) Each eigenvalue of $M$ is real. (ii) Each eigenvector can be chosen to be real. (iii) Eigenvectors with different eigenvalues are orthogonal. (b) Let $A$ be a real antisymmetric $n\times n$ matrix. Prove that each eigenvalue of $A^{2}$ is real and is less than or equal to zero. $(A^2)^T = (A^T)^2 = (-A)^2 = A^2$, so $A^2$ is real symmetric. By virtue of (i) above, the eigenvalues of $A^2$ must be real. $1$. How would you determine to prove that $A^2$ is symmetric, so that you can benefit from (i) ? Let $A^2v = \color{orangered}{ k \; \mathbf{ v } }$, where k is a scalar. By (ii) above, hypothesise that $\mathbf{ v }$ is real. Then $\begin{align} k \mathbf{ v^Tv } & = v^T \; \color{orangered}{ k \; \mathbf{ v } } =  v^T \; \color{forestgreen}{ A^2 }v = v^T \color{forestgreen}{ AA } v  = v^T\color{forestgreen}{ (-A^T)A }v \\ & = -(Av)^T(Av) < 0 \end{align}$. $2.$ The question asks us to prove $k < 0$, but what's the proof strategy? The trick looks like to consider $k \mathbf{ v^Tv } $, but how would you determine/divine/previse this? I remember $\langle v,v \rangle := v^Tv \ge 0$. $3.$ I'm not asking about the algebra itself, but what's the strategy behind it here?    The last few steps feel too clever/guileful?",,"['linear-algebra', 'matrices']"
86,What is the dual matrix (of a sample covariance matrix)?,What is the dual matrix (of a sample covariance matrix)?,,"Let $A$ be a matrix. I am most interested in the real, symmetric case, but for full understanding let's let $A$ be complex. What does it mean for $A^D$ to be the dual matrix of $A$? Can we interpret it in terms of the SVD of $A=U\Sigma U^T$? Note: This is not merely the transpose. See 6.1 in http://arxiv-web3.library.cornell.edu/pdf/1211.2671v4.pdf for an example of this term. I've included the tags random matrices and probability distributions since that has something to do with the unconventional context in which I found this term used. I do not know to what extent they are relevant.","Let $A$ be a matrix. I am most interested in the real, symmetric case, but for full understanding let's let $A$ be complex. What does it mean for $A^D$ to be the dual matrix of $A$? Can we interpret it in terms of the SVD of $A=U\Sigma U^T$? Note: This is not merely the transpose. See 6.1 in http://arxiv-web3.library.cornell.edu/pdf/1211.2671v4.pdf for an example of this term. I've included the tags random matrices and probability distributions since that has something to do with the unconventional context in which I found this term used. I do not know to what extent they are relevant.",,"['linear-algebra', 'matrices', 'probability-distributions', 'random-matrices', 'symplectic-linear-algebra']"
87,zero matrix to the power of 0,zero matrix to the power of 0,,Why $0^0=I$? I'd tried prove that considering $N^0$ where N is a Nilpotent matrix and then using the Cayley -Hamilton theorem Thanks in advance.,Why $0^0=I$? I'd tried prove that considering $N^0$ where N is a Nilpotent matrix and then using the Cayley -Hamilton theorem Thanks in advance.,,"['linear-algebra', 'matrices']"
88,Best algorithm for computing eigenvalue decomposition of a $3 \times 3$ symmetrix matrix,Best algorithm for computing eigenvalue decomposition of a  symmetrix matrix,3 \times 3,"In one of my applications, I need to compute the eigenvalue decomposition of a $3 \times 3$ symmetric matrix. What algorithms can I use? Which is the most efficient one? More specifically, the matrix is a Hessian matrix of a 3d function $f(x_1,x_2,x_3)$.  $$\begin{bmatrix} \dfrac{\partial^2 f}{\partial x_1^2} & \dfrac{\partial^2 f}{\partial x_1\,\partial x_2} & \dfrac{\partial^2 f}{\partial x_1\,\partial x_3} \\[2.2ex] \dfrac{\partial^2 f}{\partial x_2\,\partial x_1} & \dfrac{\partial^2 f}{\partial x_2^2} & \dfrac{\partial^2 f}{\partial x_2\,\partial x_3} \\[2.2ex] \dfrac{\partial^2 f}{\partial x_3\,\partial x_1} & \dfrac{\partial^2 f}{\partial x_3\,\partial x_2} & \dfrac{\partial^2 f}{\partial x_3^2} \end{bmatrix} $$","In one of my applications, I need to compute the eigenvalue decomposition of a $3 \times 3$ symmetric matrix. What algorithms can I use? Which is the most efficient one? More specifically, the matrix is a Hessian matrix of a 3d function $f(x_1,x_2,x_3)$.  $$\begin{bmatrix} \dfrac{\partial^2 f}{\partial x_1^2} & \dfrac{\partial^2 f}{\partial x_1\,\partial x_2} & \dfrac{\partial^2 f}{\partial x_1\,\partial x_3} \\[2.2ex] \dfrac{\partial^2 f}{\partial x_2\,\partial x_1} & \dfrac{\partial^2 f}{\partial x_2^2} & \dfrac{\partial^2 f}{\partial x_2\,\partial x_3} \\[2.2ex] \dfrac{\partial^2 f}{\partial x_3\,\partial x_1} & \dfrac{\partial^2 f}{\partial x_3\,\partial x_2} & \dfrac{\partial^2 f}{\partial x_3^2} \end{bmatrix} $$",,"['matrices', 'eigenvalues-eigenvectors']"
89,"let $A$ be an n by n matrix, show that $||A||_{OP} \leq ||A||_{HS} \leq \sqrt{n} ||A||_{OP}$","let  be an n by n matrix, show that",A ||A||_{OP} \leq ||A||_{HS} \leq \sqrt{n} ||A||_{OP},"We are given $A \in M_{n}(\mathbb R)$ and the following norms: $||.||_{e}$ is the standard euclidean norm of $\mathbb R^n$. $||A||_{OP}$ is the operator norm of $A$, meaning $||A||_{OP} = sup_{||v||_e=1} ||Av||_e$ $||A||_{HS}$ is the Hilbert-Schmidt norm. Meaning $||A||_{HS} = trace(A^TA)$ Show that $||A||_{OP} \leq ||A||_{HS} \leq \sqrt{n} ||A||_{OP}$ Firstly I found out what the hilbert-schmidt norm is, and $trace(A^TA) = \sum a^2, a\in A$ How can I show it is bigger than the operator norm?","We are given $A \in M_{n}(\mathbb R)$ and the following norms: $||.||_{e}$ is the standard euclidean norm of $\mathbb R^n$. $||A||_{OP}$ is the operator norm of $A$, meaning $||A||_{OP} = sup_{||v||_e=1} ||Av||_e$ $||A||_{HS}$ is the Hilbert-Schmidt norm. Meaning $||A||_{HS} = trace(A^TA)$ Show that $||A||_{OP} \leq ||A||_{HS} \leq \sqrt{n} ||A||_{OP}$ Firstly I found out what the hilbert-schmidt norm is, and $trace(A^TA) = \sum a^2, a\in A$ How can I show it is bigger than the operator norm?",,"['linear-algebra', 'matrices', 'normed-spaces']"
90,"Intuition & Proof of rank(AB) $\le$ min{rank(A), rank(B)} (without inverses or maps) [Poole P217 3.6.59, 60]","Intuition & Proof of rank(AB)  min{rank(A), rank(B)} (without inverses or maps) [Poole P217 3.6.59, 60]",\le,"I'm aware of analogous threads; I hope that mine is specific enough not to be esteemed one. $\mathbf{a^i}$ is a row vector. $A, B$ are matrices. Prove: $1$. $\mathbf{a^i}B$ is a linear combination of the rows of $B$. $2.$ Row space of $AB \subseteq$ row space of $B$. $\qquad$ $3.$ Column space of $AB \subseteq$ Column space of $A$. $4.$ If $\mathbf{a_i}$ is a column vector, then  $A\mathbf{a_i}$ is a linear combination of the columns of $A$. $5. \operatorname{rank}(A\color{#B8860B}{B}) \color{#B8860B}{\le} \operatorname{rank}\color{#B8860B}{B}  \qquad \qquad$ $6.\operatorname{rank}(AB) \leq \operatorname{rank} A$. In general, $x \leq a \text{ & } x \le b \implies x \le \min\{a, b\}$. So by $5 \, \& \, 6$, $\operatorname{rank}(AB) \leq \min\{\operatorname{rank}A,\operatorname{rank} B\}$. $\bbox[2px,border:2px solid grey]{\text{ Proof of #5 :}} \;$ The rank of a matrix is the dimension of its row space. Need to show : If $\operatorname{rowsp}(AB) \subseteq\operatorname{rowsp}(B)$, then $\operatorname{dim rowspace}(AB) \le \operatorname{dim rowspace}(B). $ Pick a basis for $\operatorname{rowsp}(AB)$. Say there are $p$ vectors in this basis. By $\#2$, row space of $AB \subseteq$ row space of $B$, $\color{green}{\text{so all of these $p$ vectors also $\in \operatorname{rowsp}(B)$}}$. Moreover, they must be linearly independent (hereafter dubbed l-ind). ${\Large{\color{red}{[}}} \;$    Since the dimension of a space $=$ the maximum number of l-ind vectors in that space,    $\; {\Large{{\color{red}{]}}}}$ and $\color{green}{\text{$\operatorname{rowsp}(B)$ has $\ge p$ l-ind vectors}}$, thus $ \operatorname{dim rowspace}(B) \; \ge \; \operatorname{dim rowspace}(AB) = p. $ $\bbox[2px,border:2px solid grey]{\text{ Proof of #6 :}} \;$ Apply $ \operatorname{rank}M = \operatorname{rank}M^T$ and $\#5$: $  \operatorname{rank}(AB)^T = \operatorname{rank}(B^T\color{#B8860B}{A^T}) \quad \color{#B8860B}{\le} \quad \operatorname{rank}\color{#B8860B}{A^T} = \operatorname{rank}(A)$. $Q1.$ Please elucidate the above proof of $5$? I'm bewildered. What's the strategy? $Q2.$ On P209, Poole defines dimension as the number of vectors in a basis. So shouldn't the red bracket refer to a basis? If so, why doesn't the proof simply declare: By $2$, the basis for $\operatorname{rowsp}(AB)$ can be reused as a basis for $\operatorname{rowsp}(B).$ ? $Q3.$ How'd one previse to invert $AB$ and apply $\#5$ (the key strategem) for #6? $Q4.$ What's the intuition behind results $5$ and $6$? I'd be grateful for pictures. Sources: P147, 4.48, Schaum's Outline to Lin Alg , web.mit.edu/18.06/www/Spring01/Sol-S01-5.ps","I'm aware of analogous threads; I hope that mine is specific enough not to be esteemed one. $\mathbf{a^i}$ is a row vector. $A, B$ are matrices. Prove: $1$. $\mathbf{a^i}B$ is a linear combination of the rows of $B$. $2.$ Row space of $AB \subseteq$ row space of $B$. $\qquad$ $3.$ Column space of $AB \subseteq$ Column space of $A$. $4.$ If $\mathbf{a_i}$ is a column vector, then  $A\mathbf{a_i}$ is a linear combination of the columns of $A$. $5. \operatorname{rank}(A\color{#B8860B}{B}) \color{#B8860B}{\le} \operatorname{rank}\color{#B8860B}{B}  \qquad \qquad$ $6.\operatorname{rank}(AB) \leq \operatorname{rank} A$. In general, $x \leq a \text{ & } x \le b \implies x \le \min\{a, b\}$. So by $5 \, \& \, 6$, $\operatorname{rank}(AB) \leq \min\{\operatorname{rank}A,\operatorname{rank} B\}$. $\bbox[2px,border:2px solid grey]{\text{ Proof of #5 :}} \;$ The rank of a matrix is the dimension of its row space. Need to show : If $\operatorname{rowsp}(AB) \subseteq\operatorname{rowsp}(B)$, then $\operatorname{dim rowspace}(AB) \le \operatorname{dim rowspace}(B). $ Pick a basis for $\operatorname{rowsp}(AB)$. Say there are $p$ vectors in this basis. By $\#2$, row space of $AB \subseteq$ row space of $B$, $\color{green}{\text{so all of these $p$ vectors also $\in \operatorname{rowsp}(B)$}}$. Moreover, they must be linearly independent (hereafter dubbed l-ind). ${\Large{\color{red}{[}}} \;$    Since the dimension of a space $=$ the maximum number of l-ind vectors in that space,    $\; {\Large{{\color{red}{]}}}}$ and $\color{green}{\text{$\operatorname{rowsp}(B)$ has $\ge p$ l-ind vectors}}$, thus $ \operatorname{dim rowspace}(B) \; \ge \; \operatorname{dim rowspace}(AB) = p. $ $\bbox[2px,border:2px solid grey]{\text{ Proof of #6 :}} \;$ Apply $ \operatorname{rank}M = \operatorname{rank}M^T$ and $\#5$: $  \operatorname{rank}(AB)^T = \operatorname{rank}(B^T\color{#B8860B}{A^T}) \quad \color{#B8860B}{\le} \quad \operatorname{rank}\color{#B8860B}{A^T} = \operatorname{rank}(A)$. $Q1.$ Please elucidate the above proof of $5$? I'm bewildered. What's the strategy? $Q2.$ On P209, Poole defines dimension as the number of vectors in a basis. So shouldn't the red bracket refer to a basis? If so, why doesn't the proof simply declare: By $2$, the basis for $\operatorname{rowsp}(AB)$ can be reused as a basis for $\operatorname{rowsp}(B).$ ? $Q3.$ How'd one previse to invert $AB$ and apply $\#5$ (the key strategem) for #6? $Q4.$ What's the intuition behind results $5$ and $6$? I'd be grateful for pictures. Sources: P147, 4.48, Schaum's Outline to Lin Alg , web.mit.edu/18.06/www/Spring01/Sol-S01-5.ps",,"['linear-algebra', 'matrices']"
91,A problem with the geometric series and matrices?,A problem with the geometric series and matrices?,,"Let $n$ be a positive integer. Let $A$ be a square matrix. Let $I$ be the identity matrix with the same size as $A$. I want to simplify $f_n(A) = I + A + A^2 + A^3 + A^4 + \cdots + A^n$ Now I know that for a complex number $z$ we have $1 + z + z^2 + z^3 + z^4 + \cdots + z^n= \dfrac{z^{n+1}-1}{z-1}$ when $z$ is not equal to $1$ ... and even if $z=1$ then by computing the limit we get the right answer : $n+1$. This is well known as the geometric series and leads to the so-called q-analogue ideas. But for matrices $A$ this does not work if the determinant of $A-I$ is $0$ , because then $\dfrac{A^{n+1} - I}{A - I}$ is not well defined. This frustrates me a lot. Now I have heard about pseudoinverses but I do not know much about them and I even wonder if they can be of any help here. I tried some things from calculus and continued fractions too, but nothing worked for me. I also considered this problem for infinite (square) matrices but I assume that is analogue to this problem and so is its solution.","Let $n$ be a positive integer. Let $A$ be a square matrix. Let $I$ be the identity matrix with the same size as $A$. I want to simplify $f_n(A) = I + A + A^2 + A^3 + A^4 + \cdots + A^n$ Now I know that for a complex number $z$ we have $1 + z + z^2 + z^3 + z^4 + \cdots + z^n= \dfrac{z^{n+1}-1}{z-1}$ when $z$ is not equal to $1$ ... and even if $z=1$ then by computing the limit we get the right answer : $n+1$. This is well known as the geometric series and leads to the so-called q-analogue ideas. But for matrices $A$ this does not work if the determinant of $A-I$ is $0$ , because then $\dfrac{A^{n+1} - I}{A - I}$ is not well defined. This frustrates me a lot. Now I have heard about pseudoinverses but I do not know much about them and I even wonder if they can be of any help here. I tried some things from calculus and continued fractions too, but nothing worked for me. I also considered this problem for infinite (square) matrices but I assume that is analogue to this problem and so is its solution.",,"['matrices', 'inverse', 'q-series']"
92,Find Jordan form - Check my solution,Find Jordan form - Check my solution,,"We are given $A$ is a 6 by 6 matrix with values from $\mathbb C$, we are also given $rank(A-3I_6)=4$, and the minimal polynomial of $A$ is $m_A=(x-1)^2(x-3)^2$ We are asked to find all possible jordan forms of $A$. My solution We know that $rank((A-\lambda I)^{i-1})-rank((A-\lambda I)^i) =$ number of blocks corresponding to the eigenvalue $\lambda$ of order that is at most i. $rank((A-3I)^0) - rank(A-3I) = rank(I_6)-4=6-4=2$ So there are 2 blocks corresponding to the eigenvalue $3$ and we know that there is a block sized 2x2 corresponding to the eigenvalue $3$ because that is the exponent of $3$ in the minimal polynomial. And due to the same logic, we can say the same thing about the eigenvalue 1. Summary: We must have a jordan block sized 2x2 corresponding the eigenvalue 1, we must have a jordan block sized 2x2 corresponding to the eigenvalue $3$, we must have overall 2 jordan blocks corresponding to eigenvalue $3$, and the matrix has to be a 6x6 matrix. Solution: $\begin{pmatrix} 1 & 1 & 0 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 & 0 & 0 \\ 0 & 0 & 3 & 1 & 0 & 0 \\ 0 & 0 & 0 & 3 & 0 & 0 \\ 0 & 0 & 0 & 0 &3 & 1 \\ 0 & 0 & 0 & 0 & 0 & 3\end{pmatrix}$","We are given $A$ is a 6 by 6 matrix with values from $\mathbb C$, we are also given $rank(A-3I_6)=4$, and the minimal polynomial of $A$ is $m_A=(x-1)^2(x-3)^2$ We are asked to find all possible jordan forms of $A$. My solution We know that $rank((A-\lambda I)^{i-1})-rank((A-\lambda I)^i) =$ number of blocks corresponding to the eigenvalue $\lambda$ of order that is at most i. $rank((A-3I)^0) - rank(A-3I) = rank(I_6)-4=6-4=2$ So there are 2 blocks corresponding to the eigenvalue $3$ and we know that there is a block sized 2x2 corresponding to the eigenvalue $3$ because that is the exponent of $3$ in the minimal polynomial. And due to the same logic, we can say the same thing about the eigenvalue 1. Summary: We must have a jordan block sized 2x2 corresponding the eigenvalue 1, we must have a jordan block sized 2x2 corresponding to the eigenvalue $3$, we must have overall 2 jordan blocks corresponding to eigenvalue $3$, and the matrix has to be a 6x6 matrix. Solution: $\begin{pmatrix} 1 & 1 & 0 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 & 0 & 0 \\ 0 & 0 & 3 & 1 & 0 & 0 \\ 0 & 0 & 0 & 3 & 0 & 0 \\ 0 & 0 & 0 & 0 &3 & 1 \\ 0 & 0 & 0 & 0 & 0 & 3\end{pmatrix}$",,"['linear-algebra', 'matrices', 'solution-verification']"
93,Is there a non trivial ideal for the set of upper triangular matrices?,Is there a non trivial ideal for the set of upper triangular matrices?,,"Is there a non trivial ideal for the set of upper triangular matrices? the zero matrix is a trivial ideal. Also, the set of upper triangular matrices is an ideal. Are there any other ideals?","Is there a non trivial ideal for the set of upper triangular matrices? the zero matrix is a trivial ideal. Also, the set of upper triangular matrices is an ideal. Are there any other ideals?",,"['linear-algebra', 'abstract-algebra', 'matrices']"
94,Direct Sums of Matrix Algebra,Direct Sums of Matrix Algebra,,"This is the first half of the question introduced in https://math.stackexchange.com/questions/258893/representations-of-direct-sums-of-matrix-algebras Let $A_1, A_2....A_n$ be n algebras with units $u_1, u_2,...u_n$ respectively. Let $A = A_1 \bigoplus A_2 \bigoplus....\bigoplus A_n$. Show that a representation $V$ of $A$ is irreducible if and only if $u_iV$ is an irreducible representation of $A_i$ for exactly one $i \in \{1,2..n\}$, while  $u_i V = 0$ for all other $i$. It seems that this has something to do with the fact that an irreducible module associated with the representation cannot be written as the direct sum of two strictly smaller modules.","This is the first half of the question introduced in https://math.stackexchange.com/questions/258893/representations-of-direct-sums-of-matrix-algebras Let $A_1, A_2....A_n$ be n algebras with units $u_1, u_2,...u_n$ respectively. Let $A = A_1 \bigoplus A_2 \bigoplus....\bigoplus A_n$. Show that a representation $V$ of $A$ is irreducible if and only if $u_iV$ is an irreducible representation of $A_i$ for exactly one $i \in \{1,2..n\}$, while  $u_i V = 0$ for all other $i$. It seems that this has something to do with the fact that an irreducible module associated with the representation cannot be written as the direct sum of two strictly smaller modules.",,"['abstract-algebra', 'matrices', 'representation-theory']"
95,Consider the quadratic form $q $ and $p$ given by,Consider the quadratic form  and  given by,q  p,"Problem :Consider the quadratic form $q $ and $p$ given by $q(x,y,z,w)=x^2+y^2+z^2+bw^2$ $p(x,y,z,w)=x^2+y^2+czw$ Which of the following is true $?$ $1)p,q$ are equivalent over $\Bbb C$ if $b$ and $c$ are non zero complex numbers $2)p,q $ are equivalent over $\Bbb R$ if $b$ and $c$ are non zero real numbers $3)p,q $ are equivalent over $\Bbb R$ if $b$ and $c$ are non zero real numbers with $b$ negative $4)p,q $ are not equivalent over $\Bbb R$ if  $c=0$ Solution : $q(x,y,z,w)=x^2+y^2+z^2+bw^2$ It will give us diagonal matrix $$         \begin{pmatrix}         1 & 0 & 0 & 0 \\         0 & 1 & 0 & 0  \\         0 & 0 & 1 & 0 \\  0 & 0 & 0 & b \\         \end{pmatrix} $$ $p(x,y,z,w)=x^2+y^2+czw$ $p(x,y,z,w)=x^2+y^2+0z^2+0xy+0xz+0xw+0yz+0yw+czw$ $$         \begin{pmatrix}         1 & 0 & 0 & 0 \\         0 & 1 & 0 & 0  \\         0 & 0 & 0 & \frac{c}{2} \\  0 & 0 & \frac{c}{2}& 0 \\         \end{pmatrix} $$ Am I doing right $?$","Problem :Consider the quadratic form $q $ and $p$ given by $q(x,y,z,w)=x^2+y^2+z^2+bw^2$ $p(x,y,z,w)=x^2+y^2+czw$ Which of the following is true $?$ $1)p,q$ are equivalent over $\Bbb C$ if $b$ and $c$ are non zero complex numbers $2)p,q $ are equivalent over $\Bbb R$ if $b$ and $c$ are non zero real numbers $3)p,q $ are equivalent over $\Bbb R$ if $b$ and $c$ are non zero real numbers with $b$ negative $4)p,q $ are not equivalent over $\Bbb R$ if  $c=0$ Solution : $q(x,y,z,w)=x^2+y^2+z^2+bw^2$ It will give us diagonal matrix $$         \begin{pmatrix}         1 & 0 & 0 & 0 \\         0 & 1 & 0 & 0  \\         0 & 0 & 1 & 0 \\  0 & 0 & 0 & b \\         \end{pmatrix} $$ $p(x,y,z,w)=x^2+y^2+czw$ $p(x,y,z,w)=x^2+y^2+0z^2+0xy+0xz+0xw+0yz+0yw+czw$ $$         \begin{pmatrix}         1 & 0 & 0 & 0 \\         0 & 1 & 0 & 0  \\         0 & 0 & 0 & \frac{c}{2} \\  0 & 0 & \frac{c}{2}& 0 \\         \end{pmatrix} $$ Am I doing right $?$",,"['linear-algebra', 'matrices']"
96,What have Vectors and Matrices got to do with each other?,What have Vectors and Matrices got to do with each other?,,"In my undergraduate course work I learnt Vectors (as in those in vector space with magnitude and direction) separately from Matrices - an $n \times m$ array of numbers. However, after sitting in for a class on optimization theory the faculty member seems to use the term row/column vectors as if to refer to vectors in vector space. I always assumed row/column vector to represent an 'array' of number arranged as such and not really as vectors in vector space and definitely not as a matrix of vectors. Is this interpretation common? Is this more like a dual interpretation? That any arbitrary matrix can be interpreted as vectors in vector space and can be operated upon using vector arithmetic and have a valid interpretation of the results of the operations? Or is the term just overloaded? Am I missing something here or do I have it wrong or was I just missing some intuition :)","In my undergraduate course work I learnt Vectors (as in those in vector space with magnitude and direction) separately from Matrices - an $n \times m$ array of numbers. However, after sitting in for a class on optimization theory the faculty member seems to use the term row/column vectors as if to refer to vectors in vector space. I always assumed row/column vector to represent an 'array' of number arranged as such and not really as vectors in vector space and definitely not as a matrix of vectors. Is this interpretation common? Is this more like a dual interpretation? That any arbitrary matrix can be interpreted as vectors in vector space and can be operated upon using vector arithmetic and have a valid interpretation of the results of the operations? Or is the term just overloaded? Am I missing something here or do I have it wrong or was I just missing some intuition :)",,"['matrices', 'vector-spaces', 'vector-analysis']"
97,Characteristic Polynomial of $A$ and polynomials annihilating $A$,Characteristic Polynomial of  and polynomials annihilating,A A,"If $A$ is a real $3 \times 3$ matrix which is not diagonal. $p$ is a polynomial of degree 3 with real coefficients which is annihilating $A$. I have proved that if $A$ has a complex root (with non zero imaginary part) then $p$ must be the characteristic polynomial of $A$ upto a real scalar multiple, by using the result that $A$ is diagonalizable over $\mathbb C$. Is this true even if $A$ is not diagonalizable? Thanks in advance.","If $A$ is a real $3 \times 3$ matrix which is not diagonal. $p$ is a polynomial of degree 3 with real coefficients which is annihilating $A$. I have proved that if $A$ has a complex root (with non zero imaginary part) then $p$ must be the characteristic polynomial of $A$ upto a real scalar multiple, by using the result that $A$ is diagonalizable over $\mathbb C$. Is this true even if $A$ is not diagonalizable? Thanks in advance.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
98,How to solve the matrix equation $ABA^{-1}=C$ with $\operatorname{Tr}(A)=a$,How to solve the matrix equation  with,ABA^{-1}=C \operatorname{Tr}(A)=a,I have the following matrix equation: $$ABA^{-1}=C$$ with $B$ and $C$ given and $A$ unknown. The constraint on $A$ is $\operatorname{Tr}(A)=a$ with $a\in\mathbb{R}$. The matrices are $N\times N$.,I have the following matrix equation: $$ABA^{-1}=C$$ with $B$ and $C$ given and $A$ unknown. The constraint on $A$ is $\operatorname{Tr}(A)=a$ with $a\in\mathbb{R}$. The matrices are $N\times N$.,,"['matrices', 'matrix-equations']"
99,Positive definite matrix product,Positive definite matrix product,,"I want to prove that the matrix product $A^{-1}B$ is positive definite where A is a symmetric positive definite matrix and B is a symmetric matrix. I have tried to use the following theorem but I did not come up with a good proof. Any good reference it would be appreciated also. Theorem1: If A is positive definite, then A is invertible and $A^{-1}$ is positive definite. Thank you in advance! EDIT: (added after Bertrand R answer) Theorem2: Let C be positive definite and D symmetric of the same order. Then there exist a non-singular matrix P and a diagonal matrix Λ such that  $$ C = PP^{T} \, and \,D = PΛP^{T} $$ Theorem2 is extracted from the book ""Matrix Differential Calculus with Applications in Statistics and Econometrics"" and the proof is highly interesting. Also the first equality that refers to the positive definite matrix is also called the Cholesky decomposition .","I want to prove that the matrix product $A^{-1}B$ is positive definite where A is a symmetric positive definite matrix and B is a symmetric matrix. I have tried to use the following theorem but I did not come up with a good proof. Any good reference it would be appreciated also. Theorem1: If A is positive definite, then A is invertible and $A^{-1}$ is positive definite. Thank you in advance! EDIT: (added after Bertrand R answer) Theorem2: Let C be positive definite and D symmetric of the same order. Then there exist a non-singular matrix P and a diagonal matrix Λ such that  $$ C = PP^{T} \, and \,D = PΛP^{T} $$ Theorem2 is extracted from the book ""Matrix Differential Calculus with Applications in Statistics and Econometrics"" and the proof is highly interesting. Also the first equality that refers to the positive definite matrix is also called the Cholesky decomposition .",,"['linear-algebra', 'matrices']"
