,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,How to find $A$ such that $A^2$ is the zero matrix?,How to find  such that  is the zero matrix?,A A^2,"Let $\space t:\mathbb{R^3} \to \mathbb{R^3}$ such that $\space t(v)=0 \space \forall \space v \in \mathbb{R^3}$. The matrix form would be a $3$ by $3$, zero matrix. If $\space t=f \circ f$, where $f:\mathbb{R^3} \to \mathbb{R^3}$ and $f\neq t$, what is the matrix form of $f$? I know that $\space t=f \circ f= f\cdot f$ and if one let $A$ to be the matrix of $f$ and $\space T$ the matrix of $t$, then $T=A^2$. And so $A^2$ would be a zero matrix. Is there a systematic way to solve this problem? Thanks for the hints.","Let $\space t:\mathbb{R^3} \to \mathbb{R^3}$ such that $\space t(v)=0 \space \forall \space v \in \mathbb{R^3}$. The matrix form would be a $3$ by $3$, zero matrix. If $\space t=f \circ f$, where $f:\mathbb{R^3} \to \mathbb{R^3}$ and $f\neq t$, what is the matrix form of $f$? I know that $\space t=f \circ f= f\cdot f$ and if one let $A$ to be the matrix of $f$ and $\space T$ the matrix of $t$, then $T=A^2$. And so $A^2$ would be a zero matrix. Is there a systematic way to solve this problem? Thanks for the hints.",,['linear-algebra']
1,Matrix $BA\neq$$I_{3}$,Matrix,BA\neq I_{3},"If $\text{A}$ is a $2\times3$ matrix and $\text{B}$ is a $3\times2$ matrix, prove that $\text{BA}=I_{3}$ is impossible. So I've been thinking about this, and so far I'm thinking that a homogenous system is going to be involved in this proof. Maybe something about one of the later steps being that the last row of the matrix would be $0\neq \text{a}$, where a is any real number. I've also been thinking that for a $2\times3$ matrix, there is a (non-zero) vector $[x,y,z]$ such that $\text{A}[x,y,z]=[0,0]$ because the dot product could possibly yield $0$. I'm not sure if that's helpful at all though. Trouble is I'm not really too sure how to continue, or even begin. Any help would be appreciated.","If $\text{A}$ is a $2\times3$ matrix and $\text{B}$ is a $3\times2$ matrix, prove that $\text{BA}=I_{3}$ is impossible. So I've been thinking about this, and so far I'm thinking that a homogenous system is going to be involved in this proof. Maybe something about one of the later steps being that the last row of the matrix would be $0\neq \text{a}$, where a is any real number. I've also been thinking that for a $2\times3$ matrix, there is a (non-zero) vector $[x,y,z]$ such that $\text{A}[x,y,z]=[0,0]$ because the dot product could possibly yield $0$. I'm not sure if that's helpful at all though. Trouble is I'm not really too sure how to continue, or even begin. Any help would be appreciated.",,"['linear-algebra', 'matrices']"
2,Interchange of sum and integral in Cauchy integral formula,Interchange of sum and integral in Cauchy integral formula,,"I am working with the Cauchy Integral Formula for a matrix $A$ over a closed contour $C$. I have the following calculation, I believe this is correct, but I don't understand why I am allowed to interchange the sum and integral? $f(A)=\frac{1}{2\pi i}\int\limits_Cf(z)(zI-A)^{-1}dz \\ =  \frac{1}{2\pi i}\int\limits_Cf(z) \frac{1}{z} \sum_{n=0}^\infty \frac{A^n}{z^n} dz \\ =  \frac{1}{2\pi i} \sum_{n=0}^\infty \left(\int\limits_Cf(z) \frac{1}{z^{n+1}} dz \right) A^n \\ =   \sum_{n=0}^\infty \left( \frac{1}{2\pi i}\int\limits_Cf(z) \frac{1}{z^{n+1}} dz \right) A^n \\ =  \sum_{n=0}^\infty \frac{f^{(n)}(0)}{n!}  A^n.$ I believe it is something to do with having $||A||<|z|$ and also uniform convergence, but don't really know why it is uniformly convergent, and why that means we can interchange the sum and integral. Any explanation would be appreciated. Thanks.","I am working with the Cauchy Integral Formula for a matrix $A$ over a closed contour $C$. I have the following calculation, I believe this is correct, but I don't understand why I am allowed to interchange the sum and integral? $f(A)=\frac{1}{2\pi i}\int\limits_Cf(z)(zI-A)^{-1}dz \\ =  \frac{1}{2\pi i}\int\limits_Cf(z) \frac{1}{z} \sum_{n=0}^\infty \frac{A^n}{z^n} dz \\ =  \frac{1}{2\pi i} \sum_{n=0}^\infty \left(\int\limits_Cf(z) \frac{1}{z^{n+1}} dz \right) A^n \\ =   \sum_{n=0}^\infty \left( \frac{1}{2\pi i}\int\limits_Cf(z) \frac{1}{z^{n+1}} dz \right) A^n \\ =  \sum_{n=0}^\infty \frac{f^{(n)}(0)}{n!}  A^n.$ I believe it is something to do with having $||A||<|z|$ and also uniform convergence, but don't really know why it is uniformly convergent, and why that means we can interchange the sum and integral. Any explanation would be appreciated. Thanks.",,"['analysis', 'matrices', 'functional-analysis', 'uniform-convergence']"
3,How to quantify the distance between matrices with an irrelevant rotation factor?,How to quantify the distance between matrices with an irrelevant rotation factor?,,"Suppose you have two invertible matrices $A$, $B$ in $\mathbb{R}^{n\times n}$, that is, $A,B\in GL(n)$.  You want to define a distance between them that ignores arbitrary rotational factors, so basically if $M\in O(n)$ is an orthogonal matrix and $A = MB$, $||A-B|| = 0$.  One way to do this would be to take the geodesic length between $A, B \in GL(n)/O(n)$, the quotient group of invertible matrices over orthogonal matrices.  How would you compute this geodesic distance?","Suppose you have two invertible matrices $A$, $B$ in $\mathbb{R}^{n\times n}$, that is, $A,B\in GL(n)$.  You want to define a distance between them that ignores arbitrary rotational factors, so basically if $M\in O(n)$ is an orthogonal matrix and $A = MB$, $||A-B|| = 0$.  One way to do this would be to take the geodesic length between $A, B \in GL(n)/O(n)$, the quotient group of invertible matrices over orthogonal matrices.  How would you compute this geodesic distance?",,"['matrices', 'differential-geometry', 'lie-groups']"
4,How do collinear points on a matrix affect its rank?,How do collinear points on a matrix affect its rank?,,"Consider the matrix \begin{matrix} x_1 & y_1 & 1 \\ x_2 & y_2 & 1 \\ x_3 & y_3 & 1 \\ \end{matrix} what effect does $({x_1},{y_1})$,$({x_2},{y_2})$,$({x_3},{y_3})$ being collinear have on the rank of the above matrix ?","Consider the matrix \begin{matrix} x_1 & y_1 & 1 \\ x_2 & y_2 & 1 \\ x_3 & y_3 & 1 \\ \end{matrix} what effect does $({x_1},{y_1})$,$({x_2},{y_2})$,$({x_3},{y_3})$ being collinear have on the rank of the above matrix ?",,"['linear-algebra', 'matrices']"
5,"How do you construct the quaternion and the multiplication rules, like Hamilton did?","How do you construct the quaternion and the multiplication rules, like Hamilton did?",,"So, I understand complex number multiplication, and how it represents $2D$ rotations. What I don't understand is, how you add two more imaginary numbers $j$ and $k$, and get $3D$ rotations. I believe Hamilton ""made up rules"" on how to add these extra imaginary numbers, but how did he construct it in a way that worked? I also don't understand the purpose of adding two imaginary numbers instead of just $1$ more for $3D$ rotations. In this paragraph, I will explain my attempt to visualize / construct the quaternion, you don't have to read this, it's probably utterly wrong. I tried to imagine a $3D$ space with the axis vectors  $1$ (the real number line, side to side), $i$ (up), and $j$ (depth). Next, I imagined a vector pointing out from the origin with a real component and a $j$ component. Next, I imagined multiplying this vector by $i$ to rotate it $90$ degrees up, so I would get a resultant vector with an '$i$' component and an '$j$' component. The problem is, multiplying (real + $j$) with ($i$) algebraically gives you an '$i$' component and an '$ij$' component, which does not make sense to me. This is where I got stuck.","So, I understand complex number multiplication, and how it represents $2D$ rotations. What I don't understand is, how you add two more imaginary numbers $j$ and $k$, and get $3D$ rotations. I believe Hamilton ""made up rules"" on how to add these extra imaginary numbers, but how did he construct it in a way that worked? I also don't understand the purpose of adding two imaginary numbers instead of just $1$ more for $3D$ rotations. In this paragraph, I will explain my attempt to visualize / construct the quaternion, you don't have to read this, it's probably utterly wrong. I tried to imagine a $3D$ space with the axis vectors  $1$ (the real number line, side to side), $i$ (up), and $j$ (depth). Next, I imagined a vector pointing out from the origin with a real component and a $j$ component. Next, I imagined multiplying this vector by $i$ to rotate it $90$ degrees up, so I would get a resultant vector with an '$i$' component and an '$j$' component. The problem is, multiplying (real + $j$) with ($i$) algebraically gives you an '$i$' component and an '$ij$' component, which does not make sense to me. This is where I got stuck.",,"['linear-algebra', 'matrices', 'vector-spaces', 'complex-numbers', 'quaternions']"
6,A quantitative measure of rank of a matrix,A quantitative measure of rank of a matrix,,"The rank of a matrix is only defined as integers. Is there some other criteria that is more quantitative. E.g. $$A = \begin{bmatrix} 1 & 1\\ 1 & 0\\ \end{bmatrix} $$ $$B= \begin{bmatrix} 1 & 1\\ 1 & 0.999\\ \end{bmatrix} $$ $A$ is ""better ranked"" than $B$ EDIT: Context : I am judging the observability $O$ of a system. I have two matrices $$ dim(A) = n\times n$$ $$ dim(C) = m\times n$$ $$O = \begin{bmatrix} C \\ C A \\ C A^2 \\ \vdots \\ C A^{n-1} \end{bmatrix}$$ The system is observable if $O$ is full ranked. What I want is to be able to differentiate between observabilities of systems","The rank of a matrix is only defined as integers. Is there some other criteria that is more quantitative. E.g. $$A = \begin{bmatrix} 1 & 1\\ 1 & 0\\ \end{bmatrix} $$ $$B= \begin{bmatrix} 1 & 1\\ 1 & 0.999\\ \end{bmatrix} $$ $A$ is ""better ranked"" than $B$ EDIT: Context : I am judging the observability $O$ of a system. I have two matrices $$ dim(A) = n\times n$$ $$ dim(C) = m\times n$$ $$O = \begin{bmatrix} C \\ C A \\ C A^2 \\ \vdots \\ C A^{n-1} \end{bmatrix}$$ The system is observable if $O$ is full ranked. What I want is to be able to differentiate between observabilities of systems",,['matrices']
7,Construction of Hadamard Matrices of Order $n!$,Construction of Hadamard Matrices of Order,n!,"I'm trying to get a hand on Hadamard matrices of order $n!$, with $n>3$. Payley's construction says that there is a Hadamard matrix for $q+1$, with $q$ being a prime power. Since $$ n!-1 \bmod 4 = 3 $$ construction 1 has to be chosen: If $q$ is congruent to $3 (\bmod 4)$ [and $Q$ is the corresponding Jacobsthal matrix ] then   $$    H=I+\begin{bmatrix} 0 & j^T\\ -j & Q\end{bmatrix} $$   is a Hadamard matrix of size $q + 1$. Here $j$ is the all-1 column vector of length $q$ and $I$ is the $(q+1)×(q+1)$ identity matrix. The matrix $H$ is a skew Hadamard matrix, which means it satisfies $H+H^T = 2I$. The problem is that the number of primes among $n!-1$ is restricted (see A002982 ). I checked the values of $n!-1$ given by Wolfram|Alpha w.r.t. be a prime power, without success, so Payley's construction won't work for all $n$. Is there a general way to get the matrices, or is it case by case different? I haven't yet looked into Williamson's construction nor Turyn type constructions. Would it be worth a closer look (sure it would, but) concerning my problem? Where can I find their constructions? PS for the interested reader: I've found a nice compilation of Hadamard matrices here: http://neilsloane.com/hadamard/","I'm trying to get a hand on Hadamard matrices of order $n!$, with $n>3$. Payley's construction says that there is a Hadamard matrix for $q+1$, with $q$ being a prime power. Since $$ n!-1 \bmod 4 = 3 $$ construction 1 has to be chosen: If $q$ is congruent to $3 (\bmod 4)$ [and $Q$ is the corresponding Jacobsthal matrix ] then   $$    H=I+\begin{bmatrix} 0 & j^T\\ -j & Q\end{bmatrix} $$   is a Hadamard matrix of size $q + 1$. Here $j$ is the all-1 column vector of length $q$ and $I$ is the $(q+1)×(q+1)$ identity matrix. The matrix $H$ is a skew Hadamard matrix, which means it satisfies $H+H^T = 2I$. The problem is that the number of primes among $n!-1$ is restricted (see A002982 ). I checked the values of $n!-1$ given by Wolfram|Alpha w.r.t. be a prime power, without success, so Payley's construction won't work for all $n$. Is there a general way to get the matrices, or is it case by case different? I haven't yet looked into Williamson's construction nor Turyn type constructions. Would it be worth a closer look (sure it would, but) concerning my problem? Where can I find their constructions? PS for the interested reader: I've found a nice compilation of Hadamard matrices here: http://neilsloane.com/hadamard/",,"['matrices', 'reference-request', 'hadamard-matrices']"
8,is matrix invertible?,is matrix invertible?,,"The characteristic polynomial of a $3\times3$ matrix $A$ is given by $$ \chi_A(x) = x^3 + ax^2 +bx +c $$ and takes the values $\chi_A(-1) = 4$, $\chi_A(2)=4$ and $\chi_A(-3) = -16$.  Is $A$ invertible? I got $0$ points at this question on a quiz and I want to understand why. What is the correct way to do this question? Thanks so much.","The characteristic polynomial of a $3\times3$ matrix $A$ is given by $$ \chi_A(x) = x^3 + ax^2 +bx +c $$ and takes the values $\chi_A(-1) = 4$, $\chi_A(2)=4$ and $\chi_A(-3) = -16$.  Is $A$ invertible? I got $0$ points at this question on a quiz and I want to understand why. What is the correct way to do this question? Thanks so much.",,"['linear-algebra', 'matrices']"
9,"$A$ be a $n \times n$ matrix with $\text{rank}\,(A)\lt n$: multiple choice question",be a  matrix with : multiple choice question,"A n \times n \text{rank}\,(A)\lt n","Let $A$ be an $n \times n$ real non-zero matrix of rank less than $n$. Then one of the following is true? : (A) there exists an $n \times n$ real non-zero matrix $B$ such that $BA = 0$. (B) there may not always exist an $n \times n$ real non-zero matrix $B$ such that $BA = 0$. (C) there exists an $n \times n$ real non-zero matrix $B$ such that $BA = I$. (D) if $B$ is such that $BA = 0$, then $AB = 0$. My Attempt: Consider a $2\times 2$ matrix $A$ of the form  $\begin{pmatrix} 0 &1 \\  0 & 0 \end{pmatrix}$ and a $2\times 2$ matrix $B$ of the form  $\begin{pmatrix} 0 &a \\  0 & c \end{pmatrix},$ where $a,c$ are any non zero real numbers. Then I have that $BA=0$ holds. So option $(A)$ may hold. Option $(B)$ can also hold if we take $2\times 2$ matrix $B$ of the form  $\begin{pmatrix} 1 & a \\  2 & c \end{pmatrix}$ $(C)$ is clearly false. Also $(D)$ is also false as is evident if we take $2 \times 2$ matrix $A$ of the form  $\begin{pmatrix} 0 &1 \\  0 & 0 \end{pmatrix}$ and $2 \times 2$ matrix $B$ of the form  $\begin{pmatrix} 0 &a \\  0 & c \end{pmatrix}$ So I am confused between $(A)$ and $(B)$. Which one should be the right choice?","Let $A$ be an $n \times n$ real non-zero matrix of rank less than $n$. Then one of the following is true? : (A) there exists an $n \times n$ real non-zero matrix $B$ such that $BA = 0$. (B) there may not always exist an $n \times n$ real non-zero matrix $B$ such that $BA = 0$. (C) there exists an $n \times n$ real non-zero matrix $B$ such that $BA = I$. (D) if $B$ is such that $BA = 0$, then $AB = 0$. My Attempt: Consider a $2\times 2$ matrix $A$ of the form  $\begin{pmatrix} 0 &1 \\  0 & 0 \end{pmatrix}$ and a $2\times 2$ matrix $B$ of the form  $\begin{pmatrix} 0 &a \\  0 & c \end{pmatrix},$ where $a,c$ are any non zero real numbers. Then I have that $BA=0$ holds. So option $(A)$ may hold. Option $(B)$ can also hold if we take $2\times 2$ matrix $B$ of the form  $\begin{pmatrix} 1 & a \\  2 & c \end{pmatrix}$ $(C)$ is clearly false. Also $(D)$ is also false as is evident if we take $2 \times 2$ matrix $A$ of the form  $\begin{pmatrix} 0 &1 \\  0 & 0 \end{pmatrix}$ and $2 \times 2$ matrix $B$ of the form  $\begin{pmatrix} 0 &a \\  0 & c \end{pmatrix}$ So I am confused between $(A)$ and $(B)$. Which one should be the right choice?",,['linear-algebra']
10,Prove an identity including determinant,Prove an identity including determinant,,"Prove that: $$\begin{equation}   \begin{vmatrix}     x_0^{2n+1}&x_0^{2n}&\cdots&x_0&1\\ x_1^{2n+1}&x_1^{2n}&\cdots&x_1&1\\ \vdots&\vdots&\cdots&\vdots\\ x_n^{2n+1}&x_n^{2n}&\cdots&x_n&1\\ (2n+1)x_0^{2n}&2nx_0^{2n-1}&\cdots&1&0\\ \vdots&\vdots&\cdots&\vdots&\cdots\\ (2n+1)x_n^{2n}&2nx_n^{2n-1}&\cdots&1&0\\   \end{vmatrix}=(-1)^{n}\prod_{n\geq i>j\geq 0}(x_i-x_j)^4 \end{equation}$$ where $(x_i^{2n+1})'=(2n+1)x_i^{2n}$,$\cdots$,$x_i'=1$,$1'=0$.","Prove that: $$\begin{equation}   \begin{vmatrix}     x_0^{2n+1}&x_0^{2n}&\cdots&x_0&1\\ x_1^{2n+1}&x_1^{2n}&\cdots&x_1&1\\ \vdots&\vdots&\cdots&\vdots\\ x_n^{2n+1}&x_n^{2n}&\cdots&x_n&1\\ (2n+1)x_0^{2n}&2nx_0^{2n-1}&\cdots&1&0\\ \vdots&\vdots&\cdots&\vdots&\cdots\\ (2n+1)x_n^{2n}&2nx_n^{2n-1}&\cdots&1&0\\   \end{vmatrix}=(-1)^{n}\prod_{n\geq i>j\geq 0}(x_i-x_j)^4 \end{equation}$$ where $(x_i^{2n+1})'=(2n+1)x_i^{2n}$,$\cdots$,$x_i'=1$,$1'=0$.",,"['matrices', 'determinant']"
11,Matrix Equation with Quadratic form,Matrix Equation with Quadratic form,,"I am working in a problem that involves multivariate normal distributions and, at a given point, I need to solve the following matrix equation: $$x=\sqrt{x^{\prime}\Sigma^{-1}x} \cdot y$$ Where $x$ is a $N\times1$  vector, $\Sigma$ is a $N\times N$ variance-covariance matrix (therefore symmetric, positive definite and with each element being positive), and $y$ is a different $N\times1$ vector ($y$ can take positive and negative values). I am interested in a (hopefully explicit) solution for $x$ as a function of $\Sigma$ and $y$, which are known parameters. There is a trivial solution $x=0$, and this is the only solution when $N=1$. My question is: Are there any other solutions when $N>1$? How can they be characterized?","I am working in a problem that involves multivariate normal distributions and, at a given point, I need to solve the following matrix equation: $$x=\sqrt{x^{\prime}\Sigma^{-1}x} \cdot y$$ Where $x$ is a $N\times1$  vector, $\Sigma$ is a $N\times N$ variance-covariance matrix (therefore symmetric, positive definite and with each element being positive), and $y$ is a different $N\times1$ vector ($y$ can take positive and negative values). I am interested in a (hopefully explicit) solution for $x$ as a function of $\Sigma$ and $y$, which are known parameters. There is a trivial solution $x=0$, and this is the only solution when $N=1$. My question is: Are there any other solutions when $N>1$? How can they be characterized?",,"['matrices', 'quadratic-forms']"
12,"Proof that a Hermitian Matrix has orthogonal eigenvectors, real eigenvalues","Proof that a Hermitian Matrix has orthogonal eigenvectors, real eigenvalues",,Here's my feeble attempt: A = A* is the condition for a Hermitian matrix. So I try expanding both sides in terms of their SVD: A = USV* and A* = (USV*)* = ... = VSU* So equating I get: VSU* = USV* Now maybe through this I can conclude something about how V and U relate to each other but I don't know how... Thanks for any help.,Here's my feeble attempt: A = A* is the condition for a Hermitian matrix. So I try expanding both sides in terms of their SVD: A = USV* and A* = (USV*)* = ... = VSU* So equating I get: VSU* = USV* Now maybe through this I can conclude something about how V and U relate to each other but I don't know how... Thanks for any help.,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
13,a mathematical object that becomes zero when squared,a mathematical object that becomes zero when squared,,"1) is there a math object or number (any type; quaternion or its further generalization, for example) that becomes zero when some operation is applied to the object using itself? The object must not be zero. also, when does such object not become zero when operated on it by another object? 2) is there an object that becomes zero when operated(multiplied) by another object? 3) is there any math object or matrix that satisfies the following property (I ask for the object that satisfies both constraints): a)$ab+ba=0$ where a and b are math objects or matrices b) $A^2=B^2$ for all objects I assume nonzero.","1) is there a math object or number (any type; quaternion or its further generalization, for example) that becomes zero when some operation is applied to the object using itself? The object must not be zero. also, when does such object not become zero when operated on it by another object? 2) is there an object that becomes zero when operated(multiplied) by another object? 3) is there any math object or matrix that satisfies the following property (I ask for the object that satisfies both constraints): a)$ab+ba=0$ where a and b are math objects or matrices b) $A^2=B^2$ for all objects I assume nonzero.",,"['linear-algebra', 'abstract-algebra', 'number-theory', 'matrices']"
14,Positive definite of infinite sum of matrices,Positive definite of infinite sum of matrices,,Let $A$ be a matrix and let $C$ be a positive definite symmetric matrix. Then $B = \sum_{r=0}^ \infty (A^T)^rCA^r$ is symmetric and positive definite if $\|A\|<1.$ Any hints or proof? Thanks!,Let $A$ be a matrix and let $C$ be a positive definite symmetric matrix. Then $B = \sum_{r=0}^ \infty (A^T)^rCA^r$ is symmetric and positive definite if $\|A\|<1.$ Any hints or proof? Thanks!,,"['linear-algebra', 'matrices']"
15,basis for the solution space,basis for the solution space,,Find the basis for the solution space of the system and describe all solutions: $3x_1 - x_2 + x_4 = 0$ $x_1 + x_2 + x_3 + x_4 = 0$ I row reduce the matrix: $\begin{pmatrix} 1&1&1&1\\ 0&-4&-3&-2 \end{pmatrix}$ And from here I do not know what to do.,Find the basis for the solution space of the system and describe all solutions: $3x_1 - x_2 + x_4 = 0$ $x_1 + x_2 + x_3 + x_4 = 0$ I row reduce the matrix: $\begin{pmatrix} 1&1&1&1\\ 0&-4&-3&-2 \end{pmatrix}$ And from here I do not know what to do.,,"['linear-algebra', 'matrices']"
16,Eigenvalues of $A^T A$,Eigenvalues of,A^T A,"I have a real, positive-definite, symmetric matrix $M$, calculated from a matrix $A$ which is known: $M = A^\top A$ Given the nature of $M$, is there any specialised (read: fast code and/or simple code) way to find the largest eigenvalue of $M$?  It's years since I did linear algebra, so I'm a bit rusty.  I've had a look in Numerical Recipes, Wikipedia and the Matrix cookbook. I found the following in the Matrix cookbook and got excited too early: For a symmetric, positive-definite matrix $A$: $eig\left(A A^\top\right) = eig\left(A^\top A\right) = eig\left(A\right) \cdot eig\left(A\right)$ Of course, it is $M$ that is positive-definite and symmetric, not $A$.  $A$ isn't even square.  M will probably be 1024x1024 or larger.  The size of $M$ (i.e. width of $A$) can be constrained by the algorithm if needed, i.e. I don't mind if your suggestions require that it be a multiple of 4 or a power of 2. [Edit:] I'm mainly looking for something that I can code natively (taking advantage of SSE/AVX where possible), although your MATLAB/Octave suggestions will definitely be useful to aid my understanding of the algorithms!","I have a real, positive-definite, symmetric matrix $M$, calculated from a matrix $A$ which is known: $M = A^\top A$ Given the nature of $M$, is there any specialised (read: fast code and/or simple code) way to find the largest eigenvalue of $M$?  It's years since I did linear algebra, so I'm a bit rusty.  I've had a look in Numerical Recipes, Wikipedia and the Matrix cookbook. I found the following in the Matrix cookbook and got excited too early: For a symmetric, positive-definite matrix $A$: $eig\left(A A^\top\right) = eig\left(A^\top A\right) = eig\left(A\right) \cdot eig\left(A\right)$ Of course, it is $M$ that is positive-definite and symmetric, not $A$.  $A$ isn't even square.  M will probably be 1024x1024 or larger.  The size of $M$ (i.e. width of $A$) can be constrained by the algorithm if needed, i.e. I don't mind if your suggestions require that it be a multiple of 4 or a power of 2. [Edit:] I'm mainly looking for something that I can code natively (taking advantage of SSE/AVX where possible), although your MATLAB/Octave suggestions will definitely be useful to aid my understanding of the algorithms!",,"['matrices', 'eigenvalues-eigenvectors']"
17,The determinant of a block Matrix of Order $2n$,The determinant of a block Matrix of Order,2n,"Let us consider the $2n \times 2n$ block matrix $$\mathbf{X} = \pmatrix{\mathbf{P} & \mathbf{Q}\\ \mathbf{R} & \mathbf{S} }$$ where $\mathbf{P} , \mathbf{Q} , \mathbf{R} , \mathbf{S}$ are square  matrices of order $n$ which commute pairwise. Show that $$\det \mathbf{X} = \det (\mathbf{PS} - \mathbf{QR}).$$ The problem in this case I am having is when none of the matrices $ \mathbf{P} , \mathbf{Q} , \mathbf{R} , \mathbf{S} $ are invertible. Any help will be greatly appreciated.","Let us consider the $2n \times 2n$ block matrix $$\mathbf{X} = \pmatrix{\mathbf{P} & \mathbf{Q}\\ \mathbf{R} & \mathbf{S} }$$ where $\mathbf{P} , \mathbf{Q} , \mathbf{R} , \mathbf{S}$ are square  matrices of order $n$ which commute pairwise. Show that $$\det \mathbf{X} = \det (\mathbf{PS} - \mathbf{QR}).$$ The problem in this case I am having is when none of the matrices $ \mathbf{P} , \mathbf{Q} , \mathbf{R} , \mathbf{S} $ are invertible. Any help will be greatly appreciated.",,"['linear-algebra', 'matrices']"
18,Show that $\operatorname{rank}(A) = \operatorname{rank}(B)$,Show that,\operatorname{rank}(A) = \operatorname{rank}(B),Let $A$ and $B$ be $n\times n$ real matrices such that $A^2=A$ and $B^2=B$. Suppose that $I-(A+B)$ is invertible .    Show that $\operatorname{Rank}(A)=\operatorname{Rank}(B)$. I proceed in this way: Note that $A(I-(A+B))=A-A^2-AB=A-A-AB=-AB$ and similarly $B(I-(A+B))=-AB$. So $A(I-(A+B))=B(I-(A+B))$. Since $I-(A+B)$ is invertible we get $A=B \Rightarrow \operatorname{Rank}(A)=\operatorname{Rank}(B)$.,Let $A$ and $B$ be $n\times n$ real matrices such that $A^2=A$ and $B^2=B$. Suppose that $I-(A+B)$ is invertible .    Show that $\operatorname{Rank}(A)=\operatorname{Rank}(B)$. I proceed in this way: Note that $A(I-(A+B))=A-A^2-AB=A-A-AB=-AB$ and similarly $B(I-(A+B))=-AB$. So $A(I-(A+B))=B(I-(A+B))$. Since $I-(A+B)$ is invertible we get $A=B \Rightarrow \operatorname{Rank}(A)=\operatorname{Rank}(B)$.,,"['linear-algebra', 'matrices']"
19,Why can we write $B=UAU^{-1}$?,Why can we write ?,B=UAU^{-1},"This question is from the 1979 Berkeley Problems in Mathematics. It asked me to prove that every complex matrix $A$ can be written in the form $$B=UAU^{-1}$$ with $U$ unitary and $B$ upper triangular. The Jordan decomposition only gives me that $U$ invertible, which is not enough in this case. I do not know a simple trick to turn invertible matrices into unitary matrices (like the one that works for the real case $C=\sqrt{DD^{T}}$).","This question is from the 1979 Berkeley Problems in Mathematics. It asked me to prove that every complex matrix $A$ can be written in the form $$B=UAU^{-1}$$ with $U$ unitary and $B$ upper triangular. The Jordan decomposition only gives me that $U$ invertible, which is not enough in this case. I do not know a simple trick to turn invertible matrices into unitary matrices (like the one that works for the real case $C=\sqrt{DD^{T}}$).",,"['linear-algebra', 'matrices']"
20,Behavior of the spectral radius of a convergent matrix when some of the elements of the matrix change sign,Behavior of the spectral radius of a convergent matrix when some of the elements of the matrix change sign,,"I want to prove (or disprove) the following statement: If $A$ is a square matrix with non-negative elements that has spectral   radius less then $1$, then any matrix obtained from $A$ by arbitrarily   changing the sign of the elements has the same property. This problem appeared recently when studying the convergence of some matrices and I would like to believe that is true.","I want to prove (or disprove) the following statement: If $A$ is a square matrix with non-negative elements that has spectral   radius less then $1$, then any matrix obtained from $A$ by arbitrarily   changing the sign of the elements has the same property. This problem appeared recently when studying the convergence of some matrices and I would like to believe that is true.",,"['matrices', 'spectral-theory']"
21,Index notation clarification,Index notation clarification,,"Previously, I have seen matrix notation of the form $T_{ij}$ and all the indices have been in the form of subscripts, such that $T_{ij}x_j$ implies contraction over $j$. However, recently I saw something of the form $T_i^j$ which seems to work not entirely differently from what I was used to. What is the difference? and how do they decide which index to write as a superscript and which a subscript? What is the point of writing them this way? Is there a difference? (A link to a good reference explaining how these indices work would also be appreciated!) Thanks.","Previously, I have seen matrix notation of the form $T_{ij}$ and all the indices have been in the form of subscripts, such that $T_{ij}x_j$ implies contraction over $j$. However, recently I saw something of the form $T_i^j$ which seems to work not entirely differently from what I was used to. What is the difference? and how do they decide which index to write as a superscript and which a subscript? What is the point of writing them this way? Is there a difference? (A link to a good reference explaining how these indices work would also be appreciated!) Thanks.",,"['matrices', 'reference-request', 'notation', 'tensors']"
22,Determinant of symmetrical factorized matrix,Determinant of symmetrical factorized matrix,,"Given $A, B \in \mathbb{R}^{n\times n}, t \in \mathbb{R}\setminus \{0\}$ with $b_{ij} = t^{i-j}\cdot a_{ij}$. Prove $\det(A) = \det(B)$. I first thought of induction. I can easily prove this for $n \le 2$. My induction hypothesis: $\det(A) = \det(B)$ with $A, B \in \mathbb{R}^{n\times n}$ Induction step: $\det(B) = \sum_{i=1}^{n+1} b_{ij} \cdot (-1)^{i+j} \cdot \det(B_{ij})\overset{IH}{=} \sum_{i=1}^{n+1} t^{i-j} a_{ij} \cdot (-1)^{i+j} \cdot \det(A_{ij})$ So far, so good, but I can't seem to get rid of the exponentiation of $t$. Any thoughts?","Given $A, B \in \mathbb{R}^{n\times n}, t \in \mathbb{R}\setminus \{0\}$ with $b_{ij} = t^{i-j}\cdot a_{ij}$. Prove $\det(A) = \det(B)$. I first thought of induction. I can easily prove this for $n \le 2$. My induction hypothesis: $\det(A) = \det(B)$ with $A, B \in \mathbb{R}^{n\times n}$ Induction step: $\det(B) = \sum_{i=1}^{n+1} b_{ij} \cdot (-1)^{i+j} \cdot \det(B_{ij})\overset{IH}{=} \sum_{i=1}^{n+1} t^{i-j} a_{ij} \cdot (-1)^{i+j} \cdot \det(A_{ij})$ So far, so good, but I can't seem to get rid of the exponentiation of $t$. Any thoughts?",,"['matrices', 'determinant']"
23,Why one would want to normalize a matrix by dividing it by its Frobenius norm?,Why one would want to normalize a matrix by dividing it by its Frobenius norm?,,"I am currently reading a scientific paper about clustering of brain signals, which consist on long time series across many channels (each signal is a matrix of C channels by T time samples). In the preprocessing of their datas, the authors normalize each signal matrix by dividing it with its Frobenius norm. My problem is that they don't even say why they do so... is this so obvious that I can't see it? Any thought? Thanks!","I am currently reading a scientific paper about clustering of brain signals, which consist on long time series across many channels (each signal is a matrix of C channels by T time samples). In the preprocessing of their datas, the authors normalize each signal matrix by dividing it with its Frobenius norm. My problem is that they don't even say why they do so... is this so obvious that I can't see it? Any thought? Thanks!",,"['matrices', 'normed-spaces', 'clustering']"
24,Matrix Multiplication for n-dimensional arrays,Matrix Multiplication for n-dimensional arrays,,"Background Within the context of my research, I have been working with a vector-based model that treats entities of a function-like language as vectors: Some of these entities are ""objects"", represented as simple vectors in an object space $A$. Others act as linear maps $f : A \to B$  over objects, and are vectors in some space $B \otimes A$. Such vectors can be represented as $\dim(B)$ by $\dim(A)$ matrices, such that for any vector $\mathbf{v} \in A$ and any vector $\mathbf{f} \in B \otimes A$ represented as a matrix $M_{\mathbf{f}}$, the application of the function that $\mathbf{f}$ encodes to the object $\mathbf{v}$ encodes is simply the matrix multiplication $M_{\mathbf{f}} \times \mathbf{v}$. Trivially, the result of this computation is a vector in $B$, hence fitting the interpretation that $\mathbf{f}$ encodes a function $f: A \to B$. Problem We want this sort of representation to scale with the arity of the function. For example, functions of type $g: A \times B \to C$ are represented by vectors in $C \otimes B \otimes A$. Within the formalism we work with, there is a way of effectively computing the equivalent of the operation performed above to apply $f$ to some object $v \in A$. However, it would be convenient (and more elegant) if there were some way of representing such an application with matrix-like operations. This is to say, either: To allow the curried application of $\mathbf{g} \in C \otimes B \otimes A$ to some $\mathbf{v} \in A$, thereby obtaining vector $\mathbf{h} \in C \otimes B$, which would effectively be a $\dim(C)$ by $\dim(B)$ matrix $M_\mathbf{h}$ representing a function $h: B \to C$. To allow the application of $\mathbf{g} \in C \otimes B \otimes A$ to some $\mathbf{v \otimes w} \in A \otimes B$ (representable as a $\dim(A)$ by $\dim(B)$ matrix $M_{\mathbf{v \otimes w}}$), thereby obtaining a result vector $\mathbf{r} \in C$. Ideally, there exists some notion of representing rank $n$ tensors as $n$ dimensional matrices (i.e. seeing vectors as 1-dimensional matrices, regular matrices as 2-dimensional matrices, etc) which would allow us to represent the ternary function $g$ described above as some 3-dimensional array $M_{\mathbf{g}}$ such that: $M_{\mathbf{g}}$ can be applied to a vector $\mathbf{v} \in A$ to obtain $M_{\mathbf{g}} \times \mathbf{v} = M_{\mathbf{h}}$, a 2-dimensional matrix which in turn can be applied to $\mathbf{w} \in B$ so that $M_{\mathbf{h}} \times \mathbf{w} = \mathbf{r}$ (curried function application). $M_{\mathbf{g}}$ can be applied directly to the matrix representation of $\mathbf{v \otimes w} \in A \otimes B$ to obtain the result vector $M_{\mathbf{g}} \times M_{\mathbf{v \otimes w}} = \mathbf{r}$ (regular function application). Questions Is this concept of treating rank $n$ tensors as $n$-dimensional matrices (or, if this term is somehow inappropriate, $n$-dimensional arrays) discussed anywhere in the mathematics literature? I have looked around, but have been unable to find any canonical definition. Likewise, is there anything in the mathematics literature about doing matrix multiplication with $n$-dimensional arrays, as described above? In practice, the notion of $n$-dimensional matrices is present in several programming languages (e.g. in Matlab , and in Numpy/SciPy for Python ), where they can be multiplied with $k$-dimensional matrices as well as lower dimension matrices, with the same sort of constraints as normal matrix multiplication (namely that the ""length"" of the ""last"" dimension of a matrix must match that of the ""first"" dimension of the matrix it is applied to, just like an $m \times n$ matrix can only be multiplied with a $n \times l$ matrix). These sort of data structures fit the bill, but I cannot find anything in the literature that describes such structures formally. I would appreciate your help if you know of any. I appreciate the time you took to look over this rather long problem specification. I hope I was clear in explaining what I'm looking for, and look forward to reading your answers. Thanks in advance for any help you can provide me with!","Background Within the context of my research, I have been working with a vector-based model that treats entities of a function-like language as vectors: Some of these entities are ""objects"", represented as simple vectors in an object space $A$. Others act as linear maps $f : A \to B$  over objects, and are vectors in some space $B \otimes A$. Such vectors can be represented as $\dim(B)$ by $\dim(A)$ matrices, such that for any vector $\mathbf{v} \in A$ and any vector $\mathbf{f} \in B \otimes A$ represented as a matrix $M_{\mathbf{f}}$, the application of the function that $\mathbf{f}$ encodes to the object $\mathbf{v}$ encodes is simply the matrix multiplication $M_{\mathbf{f}} \times \mathbf{v}$. Trivially, the result of this computation is a vector in $B$, hence fitting the interpretation that $\mathbf{f}$ encodes a function $f: A \to B$. Problem We want this sort of representation to scale with the arity of the function. For example, functions of type $g: A \times B \to C$ are represented by vectors in $C \otimes B \otimes A$. Within the formalism we work with, there is a way of effectively computing the equivalent of the operation performed above to apply $f$ to some object $v \in A$. However, it would be convenient (and more elegant) if there were some way of representing such an application with matrix-like operations. This is to say, either: To allow the curried application of $\mathbf{g} \in C \otimes B \otimes A$ to some $\mathbf{v} \in A$, thereby obtaining vector $\mathbf{h} \in C \otimes B$, which would effectively be a $\dim(C)$ by $\dim(B)$ matrix $M_\mathbf{h}$ representing a function $h: B \to C$. To allow the application of $\mathbf{g} \in C \otimes B \otimes A$ to some $\mathbf{v \otimes w} \in A \otimes B$ (representable as a $\dim(A)$ by $\dim(B)$ matrix $M_{\mathbf{v \otimes w}}$), thereby obtaining a result vector $\mathbf{r} \in C$. Ideally, there exists some notion of representing rank $n$ tensors as $n$ dimensional matrices (i.e. seeing vectors as 1-dimensional matrices, regular matrices as 2-dimensional matrices, etc) which would allow us to represent the ternary function $g$ described above as some 3-dimensional array $M_{\mathbf{g}}$ such that: $M_{\mathbf{g}}$ can be applied to a vector $\mathbf{v} \in A$ to obtain $M_{\mathbf{g}} \times \mathbf{v} = M_{\mathbf{h}}$, a 2-dimensional matrix which in turn can be applied to $\mathbf{w} \in B$ so that $M_{\mathbf{h}} \times \mathbf{w} = \mathbf{r}$ (curried function application). $M_{\mathbf{g}}$ can be applied directly to the matrix representation of $\mathbf{v \otimes w} \in A \otimes B$ to obtain the result vector $M_{\mathbf{g}} \times M_{\mathbf{v \otimes w}} = \mathbf{r}$ (regular function application). Questions Is this concept of treating rank $n$ tensors as $n$-dimensional matrices (or, if this term is somehow inappropriate, $n$-dimensional arrays) discussed anywhere in the mathematics literature? I have looked around, but have been unable to find any canonical definition. Likewise, is there anything in the mathematics literature about doing matrix multiplication with $n$-dimensional arrays, as described above? In practice, the notion of $n$-dimensional matrices is present in several programming languages (e.g. in Matlab , and in Numpy/SciPy for Python ), where they can be multiplied with $k$-dimensional matrices as well as lower dimension matrices, with the same sort of constraints as normal matrix multiplication (namely that the ""length"" of the ""last"" dimension of a matrix must match that of the ""first"" dimension of the matrix it is applied to, just like an $m \times n$ matrix can only be multiplied with a $n \times l$ matrix). These sort of data structures fit the bill, but I cannot find anything in the literature that describes such structures formally. I would appreciate your help if you know of any. I appreciate the time you took to look over this rather long problem specification. I hope I was clear in explaining what I'm looking for, and look forward to reading your answers. Thanks in advance for any help you can provide me with!",,"['linear-algebra', 'matrices', 'tensor-products']"
25,"Name of the condition $AB=BA$, $AB^*=B^*A$","Name of the condition ,",AB=BA AB^*=B^*A,"For two square complex matrices $A, B$ of the same order, what is the name of the condition $AB=BA$, $AB^*=B^*A$? Is this condition popular? Here $A^*$ means the transpose conjugate of $A$.","For two square complex matrices $A, B$ of the same order, what is the name of the condition $AB=BA$, $AB^*=B^*A$? Is this condition popular? Here $A^*$ means the transpose conjugate of $A$.",,['matrices']
26,Why are cluster co-occurrence matrices positive semidefinite?,Why are cluster co-occurrence matrices positive semidefinite?,,"A cluster (aka a partition) co-occurrence matrix $A$ for $N$ points $\{x_1, \dots x_n\}$ is an $N\times N$ matrix that encodes a partitioning of these points into $k$ separate clusters ($k\ge 1$) as follows: $A(i,j) = 1$ if $x_i$ and $x_j$ belong to the same cluster, otherwise $A(i,j) = 0$ I have seen texts that say that $A$ is positive semidefinite. My intuition tells me that this has something to do with transitive relation encoded in the matrix, i.e.: If $A(i,j) = 1$, and $A(j,k) = 1$, then $A(i,k) = 1$ $\forall (i,j,k)$ But I don't see how the above can be derived from the definition of positive semidefinite matrices, i.e. $z^T A z > 0$ $\forall z\in R^N$ Any thoughts?","A cluster (aka a partition) co-occurrence matrix $A$ for $N$ points $\{x_1, \dots x_n\}$ is an $N\times N$ matrix that encodes a partitioning of these points into $k$ separate clusters ($k\ge 1$) as follows: $A(i,j) = 1$ if $x_i$ and $x_j$ belong to the same cluster, otherwise $A(i,j) = 0$ I have seen texts that say that $A$ is positive semidefinite. My intuition tells me that this has something to do with transitive relation encoded in the matrix, i.e.: If $A(i,j) = 1$, and $A(j,k) = 1$, then $A(i,k) = 1$ $\forall (i,j,k)$ But I don't see how the above can be derived from the definition of positive semidefinite matrices, i.e. $z^T A z > 0$ $\forall z\in R^N$ Any thoughts?",,['matrices']
27,Quick way of showing an $n\times n$ Jordan block associated to $1$ is similar to the companion matrix of $(x-1)^n$,Quick way of showing an  Jordan block associated to  is similar to the companion matrix of,n\times n 1 (x-1)^n,"Is there a quick, clean way of proving that the $n\times n$ Jordan block with $1$'s on the diagonal and the Frobenius companion matrix corresponding to the polynomial $(x-1)^n$ are similar matrices? Apparently, the (triangular) Pascal matrix is the required similarity transformation, but I'm stuck showing why it works (mostly in constructing the sums implied by the needed matrix multiplications). Or better yet, can this assertion be proven without explicitly invoking the Pascal matrix?","Is there a quick, clean way of proving that the $n\times n$ Jordan block with $1$'s on the diagonal and the Frobenius companion matrix corresponding to the polynomial $(x-1)^n$ are similar matrices? Apparently, the (triangular) Pascal matrix is the required similarity transformation, but I'm stuck showing why it works (mostly in constructing the sums implied by the needed matrix multiplications). Or better yet, can this assertion be proven without explicitly invoking the Pascal matrix?",,"['linear-algebra', 'matrices', 'binomial-coefficients', 'companion-matrices', 'similar-matrices']"
28,An elementary proof in matrix algebra,An elementary proof in matrix algebra,,"If $A$ and $B$ are two conformable matrices,and $AB = A$ and $BA = B$ then prove that A and B are idempotent matrices. This may be trivial but I am not sure how to proceed on this.","If $A$ and $B$ are two conformable matrices,and $AB = A$ and $BA = B$ then prove that A and B are idempotent matrices. This may be trivial but I am not sure how to proceed on this.",,"['linear-algebra', 'matrices']"
29,How to find all $2 \times 2$ matrices whose $4^{\text{th}}$ power is the identity matrix but its lower powers are not?,How to find all  matrices whose  power is the identity matrix but its lower powers are not?,2 \times 2 4^{\text{th}},"Question. To put the question in another way, let $G$ be the set of all invertible linear maps of $\mathbb{R}^2\to\mathbb{R}^2$ , and it is not hard to show that $G$ is a group under composition, and the question asks for those elements of $G$ which have order $4$ . My Attempt. First of all, I can show that every element of $G$ is equivalent to left-multiply by a $2 \times 2$ invertible matrix. Then, I was able to find elements of $G$ of order $2$ by brutal-force algebra. However, the brutal-force method becomes tedious to be used to tackle the given question. I know two elements of $G$ which have order $4$ , left-multiplying by a matrix which represents either a clockwise rotation of $90$ degrees or an anticlockwise rotation of $90$ degrees, but are they the only elements of order $4$ ? I have no clue how to proceed. Note. I am merely asking for a hint rather than a solution. Thank you!","Question. To put the question in another way, let be the set of all invertible linear maps of , and it is not hard to show that is a group under composition, and the question asks for those elements of which have order . My Attempt. First of all, I can show that every element of is equivalent to left-multiply by a invertible matrix. Then, I was able to find elements of of order by brutal-force algebra. However, the brutal-force method becomes tedious to be used to tackle the given question. I know two elements of which have order , left-multiplying by a matrix which represents either a clockwise rotation of degrees or an anticlockwise rotation of degrees, but are they the only elements of order ? I have no clue how to proceed. Note. I am merely asking for a hint rather than a solution. Thank you!",G \mathbb{R}^2\to\mathbb{R}^2 G G 4 G 2 \times 2 G 2 G 4 90 90 4,['matrices']
30,$A$ is diagonalizable $\iff A^{-1}$ is diagonalizable.,is diagonalizable  is diagonalizable.,A \iff A^{-1},"I used that proposition while doing an exercise and later realized it hadn't been demonstrated in class so i proceeded trying to prove it myself as it follows: Let $A\in M_n(\Bbb{K})$ be invertible, than exists $A^{-1}\in M_n(\Bbb{K})$ ; Let then  V be a vector space and $L_A=:f\in End(V)$ . Now if $A$ is diagonalizable there exist a base $\mathscr{B}=\left( v_1,\dots,v_n \right)$ for V such that $$M_{\mathscr{B}}\left(f\right)= \begin{pmatrix} \lambda_1 & 0 & \dots & \dots & 0 \\ 0 & \lambda_2 & 0 & \dots & 0 \\ \vdots & 0 & \ddots & & \vdots \\ \vdots & \vdots & & \ddots & 0\\ 0 & 0 & \dots & 0 & \lambda_n \\ \end{pmatrix} $$ Now since $f\circ f^{-1}=Id_V$ , it must be true that $M_{\mathscr{B}}\left(f\right) \cdot M_{\mathscr{B}}\left(f^{-1}\right)=Id_n$ ; therefore $$ M_{\mathscr{B}}\left(f^{-1}\right)= \begin{pmatrix} 1\over\lambda_1 & 0 & \dots & \dots & 0 \\ 0 & 1\over\lambda_2 & 0 & \dots & 0 \\ \vdots & 0 & \ddots & & \vdots \\ \vdots & \vdots & & \ddots & 0\\ 0 & 0 & \dots & 0 & 1\over\lambda_n \\ \end{pmatrix}  $$ and so we found that $A$ being diagonalizable implies $A^{-1}$ is diagonalizable. Now, since the inverse of $A^{-1}$ is actually $A$ , for the same argument we can conclude that $A^{-1}$ being diagonalizable implies $A$ is diagonalizable and therefore the thesis. Is my argument correct? Could I prove it quicker? Because I believe my proof is correct as much as I believe it could be done in a smarter way.","I used that proposition while doing an exercise and later realized it hadn't been demonstrated in class so i proceeded trying to prove it myself as it follows: Let be invertible, than exists ; Let then  V be a vector space and . Now if is diagonalizable there exist a base for V such that Now since , it must be true that ; therefore and so we found that being diagonalizable implies is diagonalizable. Now, since the inverse of is actually , for the same argument we can conclude that being diagonalizable implies is diagonalizable and therefore the thesis. Is my argument correct? Could I prove it quicker? Because I believe my proof is correct as much as I believe it could be done in a smarter way.","A\in M_n(\Bbb{K}) A^{-1}\in M_n(\Bbb{K}) L_A=:f\in End(V) A \mathscr{B}=\left( v_1,\dots,v_n \right) M_{\mathscr{B}}\left(f\right)=
\begin{pmatrix}
\lambda_1 & 0 & \dots & \dots & 0 \\
0 & \lambda_2 & 0 & \dots & 0 \\
\vdots & 0 & \ddots & & \vdots \\
\vdots & \vdots & & \ddots & 0\\
0 & 0 & \dots & 0 & \lambda_n \\
\end{pmatrix}
 f\circ f^{-1}=Id_V M_{\mathscr{B}}\left(f\right) \cdot M_{\mathscr{B}}\left(f^{-1}\right)=Id_n  M_{\mathscr{B}}\left(f^{-1}\right)=
\begin{pmatrix}
1\over\lambda_1 & 0 & \dots & \dots & 0 \\
0 & 1\over\lambda_2 & 0 & \dots & 0 \\
\vdots & 0 & \ddots & & \vdots \\
\vdots & \vdots & & \ddots & 0\\
0 & 0 & \dots & 0 & 1\over\lambda_n \\
\end{pmatrix} 
 A A^{-1} A^{-1} A A^{-1} A","['linear-algebra', 'matrices', 'diagonalization']"
31,"For what $\lambda \in \mathbb{R}$ is this a spanning set (Erzeugendensystem) of the vector space $\mathbb{R}^3$ ? {(1, 3, 4), (1, λ, 5), (1, 4, 3)}","For what  is this a spanning set (Erzeugendensystem) of the vector space  ? {(1, 3, 4), (1, λ, 5), (1, 4, 3)}",\lambda \in \mathbb{R} \mathbb{R}^3,"The set forms a spanning set (German: Erzeugendensystem) if the matrix is linearly independent, which is true if the determinant is non-zero. We have $$1 \cdot (\lambda \cdot 3 - 4 \cdot 5) - 1 \cdot (3 \cdot 3 - 4 \cdot 4) + 1 \cdot (3 \cdot 5 - \lambda \cdot 4) = 3\lambda - 20 - 9 + 16 + 15 - 4\lambda = -\lambda + 2 \neq 0 \implies \lambda \neq 2$$ So we have $\lambda \in \mathbb{R}$ \ { $2$ } Is this correct ? The course I take is in German, but it is not my mother tongue. Is ""spanning set of vector space"" an accurate translation for ""Erzeugendensystem"" ?","The set forms a spanning set (German: Erzeugendensystem) if the matrix is linearly independent, which is true if the determinant is non-zero. We have So we have \ { } Is this correct ? The course I take is in German, but it is not my mother tongue. Is ""spanning set of vector space"" an accurate translation for ""Erzeugendensystem"" ?",1 \cdot (\lambda \cdot 3 - 4 \cdot 5) - 1 \cdot (3 \cdot 3 - 4 \cdot 4) + 1 \cdot (3 \cdot 5 - \lambda \cdot 4) = 3\lambda - 20 - 9 + 16 + 15 - 4\lambda = -\lambda + 2 \neq 0 \implies \lambda \neq 2 \lambda \in \mathbb{R} 2,"['linear-algebra', 'matrices', 'vector-spaces', 'vectors', 'systems-of-equations']"
32,"Suppose $A$ is positive definite with $a_{ij}\leqslant 0 (\forall i\neq j)$. Is the matrix $-A+2\mathrm{diag}(a_{11},\dots,a_{nn})$ positive definite?",Suppose  is positive definite with . Is the matrix  positive definite?,"A a_{ij}\leqslant 0 (\forall i\neq j) -A+2\mathrm{diag}(a_{11},\dots,a_{nn})","Suppose $A=(a_{ij})$ is positive definite with $a_{ij}\leqslant 0\ (i\neq j)$ . Is the matrix $-A+2\mathrm{diag}(a_{11},\dots,a_{nn})$ positive definite? My attempt: I have shown that the proposition is true for $n\leqslant 3$ , using Hurwitz criterion for positive definite matrices. But I have no idea about bigger $n$ . Maybe another way is to show $\lambda_i<2a_{ii}$ , where $\lambda_i$ are eigenvalues, but I have no progress. Any hints/counterexamples are welcomed. THANKS!","Suppose is positive definite with . Is the matrix positive definite? My attempt: I have shown that the proposition is true for , using Hurwitz criterion for positive definite matrices. But I have no idea about bigger . Maybe another way is to show , where are eigenvalues, but I have no progress. Any hints/counterexamples are welcomed. THANKS!","A=(a_{ij}) a_{ij}\leqslant 0\ (i\neq j) -A+2\mathrm{diag}(a_{11},\dots,a_{nn}) n\leqslant 3 n \lambda_i<2a_{ii} \lambda_i","['linear-algebra', 'matrices']"
33,Spectrum of $A + s N$ constant in $s$ when?,Spectrum of  constant in  when?,A + s N s,"Let $A$ , $N$ matrices in $M_n(\mathbb{C})$ such that the spectrum ( with multiplicities) of the matrix $A + s N$ does not depend on $s$ . I am trying to show that $A$ , $N$ can be simultaneously reduced to an upper triangular form, with $N$ strictly upper triangular. Notes: The hypothesis is equivalent to "" the characteristic polynomial of $A + s N$ does not depend on $s$ "". It is easy to see that condition is equivalent to : for every $1\le m \le n$ the trace $\operatorname{tr} ( A + s N)^m$ is constant in $s$ .  This implies right away that $\operatorname{tr} N^m = 0$ for all $1\le m \le N$ , so $N$ is nilpotent. It would be enough to show that $A$ and $N$ have a common eigenvector.  One could try to prove this: if $\det (A + s N) = 0$ for all $s$ , then $A$ , $N$ have a common zero eigenvector. That would in fact prove the result, so it might be a viable route.  (In fact not viable !, see below) Any feedback would be appreciated! $\bf{Added:}$ I forgot that $\det(A+s N) = 0$ for all $s$ does not imply that the kernels of $A$ , $N$ intersect non-trivially. There may be the issue that the images of $A$ , $N$ do not generate the whole space. $\bf{Added:}$ The assertion is wrong, as showed by the counterexample of @just a user. With Mathematica, I found some counterexample in size $3 \times 3$ as follows: take $A$ a Jordan cell of size $3$ ( the $\lambda$ will not matter).  Now find all those $N$ satisfying $$\operatorname{tr}(N, A N, N^2, A^2 N, A N^2, N^3) = 0$$ Clearly any such $N$ is nilpotent. However, it may happen that $A$ and $N$ do not share an eigenvector. A generic solution of such $N$ is $$N = \begin{pmatrix} q^2 r & q^2 s & r^3\\ - q^3 & - 2 q^2 r & q( - 3 r^2 + q s)\\0 & q^3 & q^2 r \end{pmatrix}$$ with unique eingenvector ( up to proportionality) $$v = ( r^2 - q s, q r, - q^2)$$ However, $v$ is not an eigenvector of $A$ . Hence, $A$ , $N$ cannot be simultaneously brought to an upper triangular form. $\bf{Added:}$ With the above $A$ ( for $\lambda= 0$ ) $N$ , we have the kernels of $A$ do not intersect, the ranges of $A$ together span the full space, yet $a A + s N$ is nilpotent for all $a$ , $s$ .","Let , matrices in such that the spectrum ( with multiplicities) of the matrix does not depend on . I am trying to show that , can be simultaneously reduced to an upper triangular form, with strictly upper triangular. Notes: The hypothesis is equivalent to "" the characteristic polynomial of does not depend on "". It is easy to see that condition is equivalent to : for every the trace is constant in .  This implies right away that for all , so is nilpotent. It would be enough to show that and have a common eigenvector.  One could try to prove this: if for all , then , have a common zero eigenvector. That would in fact prove the result, so it might be a viable route.  (In fact not viable !, see below) Any feedback would be appreciated! I forgot that for all does not imply that the kernels of , intersect non-trivially. There may be the issue that the images of , do not generate the whole space. The assertion is wrong, as showed by the counterexample of @just a user. With Mathematica, I found some counterexample in size as follows: take a Jordan cell of size ( the will not matter).  Now find all those satisfying Clearly any such is nilpotent. However, it may happen that and do not share an eigenvector. A generic solution of such is with unique eingenvector ( up to proportionality) However, is not an eigenvector of . Hence, , cannot be simultaneously brought to an upper triangular form. With the above ( for ) , we have the kernels of do not intersect, the ranges of together span the full space, yet is nilpotent for all , .","A N M_n(\mathbb{C}) A + s N s A N N A + s N s 1\le m \le n \operatorname{tr} ( A + s N)^m s \operatorname{tr} N^m = 0 1\le m \le N N A N \det (A + s N) = 0 s A N \bf{Added:} \det(A+s N) = 0 s A N A N \bf{Added:} 3 \times 3 A 3 \lambda N \operatorname{tr}(N, A N, N^2, A^2 N, A N^2, N^3) = 0 N A N N N = \begin{pmatrix} q^2 r & q^2 s & r^3\\ - q^3 & - 2 q^2 r & q( - 3 r^2 + q s)\\0 & q^3 & q^2 r \end{pmatrix} v = ( r^2 - q s, q r, - q^2) v A A N \bf{Added:} A \lambda= 0 N A A a A + s N a s","['linear-algebra', 'abstract-algebra', 'matrices']"
34,Linearly independent vectors generated from matrices,Linearly independent vectors generated from matrices,,"I came across this question recently: Do there exist $n$ real $n\times n$ matrices $A_1, A_2, \cdots, A_n$ , such that for any $n$ -dimensional non-zero vector $v$ , $A_1 v, A_2 v, \cdots, A_n v$ are always linearly independent? It seems like a usual linear algebra question, but I can't think of any idea to approach it, and I can't come up with a counterexample either. Is there anything I've missed? Any help would be appreciated. Edit: So for $n=2$ , this is obvious as we can just take two rotation matrices with different angles. The resulting two vectors will always be on different lines. For $n>2$ however, rotation won't work as there'll be an axis of rotation, on which the vectors are eigenvectors, i.e., on the same line after rotation.","I came across this question recently: Do there exist real matrices , such that for any -dimensional non-zero vector , are always linearly independent? It seems like a usual linear algebra question, but I can't think of any idea to approach it, and I can't come up with a counterexample either. Is there anything I've missed? Any help would be appreciated. Edit: So for , this is obvious as we can just take two rotation matrices with different angles. The resulting two vectors will always be on different lines. For however, rotation won't work as there'll be an axis of rotation, on which the vectors are eigenvectors, i.e., on the same line after rotation.","n n\times n A_1, A_2, \cdots, A_n n v A_1 v, A_2 v, \cdots, A_n v n=2 n>2","['linear-algebra', 'abstract-algebra', 'matrices', 'vector-spaces', 'vectors']"
35,Positive semidefiniteness of the product of a symmetric positive semidefinite matrix and a nonnegative diagonal matrix,Positive semidefiniteness of the product of a symmetric positive semidefinite matrix and a nonnegative diagonal matrix,,"Is it true that the product of a symmetric PSD matrix and nonnegative diagonal matrix? For $A\in\Bbb R^{n\times n}$ and $D\in\Bbb R^{n\times n}$ , let $A\succeq O$ and $D:=\mathrm{diag}(d_1,\dots,d_n)$ , where $d_i\ge0$ for all $i$ . Then, is $AD$ positive semidefinite? It is known that when two psd matrices $A$ and $B$ are non-commuting, $AB$ is not psd. Although $AD\neq DA$ , how about the case where $B$ is a nonnegative diagonal matrix? I found that the hypothesis is true when $n=2$ . Given $$ A:=\left(\begin{array}{cc}a & b\\ b & c\end{array}\right)\succeq O,\quad D:=\left(\begin{array}{cc}d_1 & 0 \\ 0 & d_2\end{array}\right)\ge O. $$ Since $A\succeq O$ , $a\ge 0$ , $c\ge 0$ , and $ac-b^2\ge 0$ . The product $AD$ is $$ AD=\left(\begin{array}{cc}d_1 a & d_2 b \\ d_1 b & d_2 c\end{array}\right). $$ All the determinants of principal minors for $AD$ are nonnegative because $$ d_1a\ge0,\ d_2c\ge 0,\ d_1d_2(ac-b^2)\ge 0. $$ Is this result possible to generalize $n\ge 2$ ?","Is it true that the product of a symmetric PSD matrix and nonnegative diagonal matrix? For and , let and , where for all . Then, is positive semidefinite? It is known that when two psd matrices and are non-commuting, is not psd. Although , how about the case where is a nonnegative diagonal matrix? I found that the hypothesis is true when . Given Since , , , and . The product is All the determinants of principal minors for are nonnegative because Is this result possible to generalize ?","A\in\Bbb R^{n\times n} D\in\Bbb R^{n\times n} A\succeq O D:=\mathrm{diag}(d_1,\dots,d_n) d_i\ge0 i AD A B AB AD\neq DA B n=2 
A:=\left(\begin{array}{cc}a & b\\ b & c\end{array}\right)\succeq O,\quad D:=\left(\begin{array}{cc}d_1 & 0 \\ 0 & d_2\end{array}\right)\ge O.
 A\succeq O a\ge 0 c\ge 0 ac-b^2\ge 0 AD 
AD=\left(\begin{array}{cc}d_1 a & d_2 b \\ d_1 b & d_2 c\end{array}\right).
 AD 
d_1a\ge0,\ d_2c\ge 0,\ d_1d_2(ac-b^2)\ge 0.
 n\ge 2","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
36,The permutation matrices are the doubly stochastic matrices with the highest Frobenius norm,The permutation matrices are the doubly stochastic matrices with the highest Frobenius norm,,"In a 2013 talk , Alexandre d'Aspremont did claim the following: Among all doubly stochastic matrices, the rotations, hence, the permutation matrices, have the highest Frobenius norm I had never encountered this result.  Searching for it on Mathematics SE produced nothing of interest.  Assuming this result is indeed correct, a reference would be most appreciated. Related: How would you encourage an orthogonal matrix to be a permutation matrix? Which probability mass function has the largest Euclidean norm?","In a 2013 talk , Alexandre d'Aspremont did claim the following: Among all doubly stochastic matrices, the rotations, hence, the permutation matrices, have the highest Frobenius norm I had never encountered this result.  Searching for it on Mathematics SE produced nothing of interest.  Assuming this result is indeed correct, a reference would be most appreciated. Related: How would you encourage an orthogonal matrix to be a permutation matrix? Which probability mass function has the largest Euclidean norm?",,"['matrices', 'reference-request', 'permutation-matrices', 'stochastic-matrices', 'birkhoff-polytopes']"
37,Reference for inequality on Frobenius/Hilbert-Schmidt matrix norm,Reference for inequality on Frobenius/Hilbert-Schmidt matrix norm,,"It is not hard to see that if ${\bf A}$ is a $d\times d$ matrix with unit determinant, $\det({\bf A})=1,$ then one has an upper bound on the Hilbert-Schmidt (Frobenius) norm of the inverse matrix ${\bf A}^{-1}$ in terms of the Hilbert-Schmidt norm of the matrix itself in the form $$ \Vert {\bf A}^{-1}\Vert_{\bf{HS}}^2 \leq C_d \Vert {\bf A}\Vert_{\bf{HS}}^{2(d-1)} $$ For some appropriately chosen constant $C_d.$ I am interested in the best constants. Obviously $C_2=1$ and I computed that $C_3=\frac{1}{3}$ . I believe that the best constant is $C_d=\frac{1}{d^{d-2}}$ ; one can obviously not do better, as the identity saturates this. I actually only need the case $d=3$ , and I don't even really need the best constant, but I am curious, and my calculation for $d=3$ is a bit of a mess. Does anyone have a reference for this inequality, ideally the best constants, or a slick proof of this? Update I have done the calculation for all $d$ using Lagrange multipliers with the singular values as coordinates but I would still appreciate either a reference or a slick proof.","It is not hard to see that if is a matrix with unit determinant, then one has an upper bound on the Hilbert-Schmidt (Frobenius) norm of the inverse matrix in terms of the Hilbert-Schmidt norm of the matrix itself in the form For some appropriately chosen constant I am interested in the best constants. Obviously and I computed that . I believe that the best constant is ; one can obviously not do better, as the identity saturates this. I actually only need the case , and I don't even really need the best constant, but I am curious, and my calculation for is a bit of a mess. Does anyone have a reference for this inequality, ideally the best constants, or a slick proof of this? Update I have done the calculation for all using Lagrange multipliers with the singular values as coordinates but I would still appreciate either a reference or a slick proof.","{\bf A} d\times d \det({\bf A})=1, {\bf A}^{-1} 
\Vert {\bf A}^{-1}\Vert_{\bf{HS}}^2 \leq C_d \Vert {\bf A}\Vert_{\bf{HS}}^{2(d-1)}
 C_d. C_2=1 C_3=\frac{1}{3} C_d=\frac{1}{d^{d-2}} d=3 d=3 d","['linear-algebra', 'matrices', 'inequality']"
38,Are $\begin{bmatrix} 1 & 0 \\ 1 & 0 \end{bmatrix}$ and $\begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix}$ similar?,Are  and  similar?,\begin{bmatrix} 1 & 0 \\ 1 & 0 \end{bmatrix} \begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix},"I have two matrices: $$ A =\begin{bmatrix} 1 & 0 \\ 1 & 0 \end{bmatrix}~~~~~~~ B = \begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix} $$ I am to show they are similar or not similar. I set the following up: $$ B = Q^{-1}AQ $$ where $$ Q =  \begin{bmatrix} a & b \\ c & d \end{bmatrix} $$ I solved that $d = -b$ and $c=b$ and $-b(b-a) \ne 0$ . I set $b = 2$ , $a =1$ and attempted to solve $B=Q^{-1}AQ$ , but I end up getting: $$ \begin{bmatrix} 1 & 0 \\ 1 & 0 \end{bmatrix} =  \begin{bmatrix} 3 & 0 \\ 3 & 0 \end{bmatrix} $$ I wanted to ask about my general strategy, and if at this point, I must simply plug in different values to see what works. Calculation: $$ \begin{bmatrix} 1 & 0 \\ 1 & 0 \end{bmatrix} = \frac{1}{ad-bc} \begin{bmatrix} d & -b \\ -c & a \end{bmatrix} \begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} a & b \\ c & d \end{bmatrix} $$ $$ \begin{bmatrix} 1 & 0 \\ 1 & 0 \end{bmatrix} = \frac{1}{ad-bc} \begin{bmatrix} (a+c)d & (b+d)d \\ -c(a+c) & -c(b+d) \end{bmatrix} $$ I then solved the linear system. Likewise, solving $AQ = QB$ , I get: $$ \begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} a & b \\ a & b \end{bmatrix} = \begin{bmatrix} a & b \\ a & b \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 1 & 0 \end{bmatrix} $$ $$ \begin{bmatrix} a+c & b+d \\ 0 & 0 \end{bmatrix} = \begin{bmatrix} a+b & 0 \\ c+d & 0 \end{bmatrix} $$ which only gives that $b=c$ .","I have two matrices: I am to show they are similar or not similar. I set the following up: where I solved that and and . I set , and attempted to solve , but I end up getting: I wanted to ask about my general strategy, and if at this point, I must simply plug in different values to see what works. Calculation: I then solved the linear system. Likewise, solving , I get: which only gives that .","
A =\begin{bmatrix}
1 & 0 \\
1 & 0
\end{bmatrix}~~~~~~~
B = \begin{bmatrix}
1 & 1 \\
0 & 0
\end{bmatrix}
 
B = Q^{-1}AQ
 
Q = 
\begin{bmatrix}
a & b \\
c & d
\end{bmatrix}
 d = -b c=b -b(b-a) \ne 0 b = 2 a =1 B=Q^{-1}AQ 
\begin{bmatrix}
1 & 0 \\
1 & 0
\end{bmatrix} = 
\begin{bmatrix}
3 & 0 \\
3 & 0
\end{bmatrix}
 
\begin{bmatrix}
1 & 0 \\
1 & 0
\end{bmatrix}
=
\frac{1}{ad-bc}
\begin{bmatrix}
d & -b \\
-c & a
\end{bmatrix}
\begin{bmatrix}
1 & 1 \\
0 & 0
\end{bmatrix}
\begin{bmatrix}
a & b \\
c & d
\end{bmatrix}
 
\begin{bmatrix}
1 & 0 \\
1 & 0
\end{bmatrix}
=
\frac{1}{ad-bc}
\begin{bmatrix}
(a+c)d & (b+d)d \\
-c(a+c) & -c(b+d)
\end{bmatrix}
 AQ = QB 
\begin{bmatrix}
1 & 1 \\
0 & 0
\end{bmatrix}
\begin{bmatrix}
a & b \\
a & b
\end{bmatrix}
=
\begin{bmatrix}
a & b \\
a & b
\end{bmatrix}
\begin{bmatrix}
1 & 0 \\
1 & 0
\end{bmatrix}
 
\begin{bmatrix}
a+c & b+d \\
0 & 0
\end{bmatrix}
=
\begin{bmatrix}
a+b & 0 \\
c+d & 0
\end{bmatrix}
 b=c","['linear-algebra', 'matrices', 'similar-matrices']"
39,Lyapunov analysis of marginally stable linear systems,Lyapunov analysis of marginally stable linear systems,,"Before I start, I want to emphasize I'm dealing with marginally stable linear systems, so many theorems about stable systems simply do not apply. Let $\dot{x} = Ax$ be a marginally stable. That is, for any $x(0) \in \mathbb{R}^n$ , there exists $M > 0$ such that $\|x(t)\|_2 \leq M$ for all $t \geq 0$ . This is equivalent to saying that all eigenvalues of $A$ have non-positive real part, and the eigenvalues with zero real part have $1 \times 1$ Jordan blocks. (I am particularly dealing with a matrix $A$ with real eigenvalues, and all but one eigenvalues are negative, and the the remaining (simple) eigenvalue is $0$ .) This manuscript states $A$ is marginally stable if and only if there exists $Q \geq 0, P >0$ such that $$A^TP + PA = -Q.$$ My question is, is there a procedure to find such $Q, P$ ? Edit: The linked manuscript has incorrect statements. @user1551 cleared everything up in their answer.","Before I start, I want to emphasize I'm dealing with marginally stable linear systems, so many theorems about stable systems simply do not apply. Let be a marginally stable. That is, for any , there exists such that for all . This is equivalent to saying that all eigenvalues of have non-positive real part, and the eigenvalues with zero real part have Jordan blocks. (I am particularly dealing with a matrix with real eigenvalues, and all but one eigenvalues are negative, and the the remaining (simple) eigenvalue is .) This manuscript states is marginally stable if and only if there exists such that My question is, is there a procedure to find such ? Edit: The linked manuscript has incorrect statements. @user1551 cleared everything up in their answer.","\dot{x} = Ax x(0) \in \mathbb{R}^n M > 0 \|x(t)\|_2 \leq M t \geq 0 A 1 \times 1 A 0 A Q \geq 0, P >0 A^TP + PA = -Q. Q, P","['linear-algebra', 'matrices', 'ordinary-differential-equations', 'stability-in-odes', 'lyapunov-functions']"
40,Missing step in this proof of $\operatorname*{rank}(A+B) \leq \operatorname*{rank}A + \operatorname*{rank}B$,Missing step in this proof of,\operatorname*{rank}(A+B) \leq \operatorname*{rank}A + \operatorname*{rank}B,"I know how to prove this inequality using bases for the row space, however, my professor presented a proof of \begin{align*}   \operatorname*{rank}(A+B) \leq \operatorname*{rank}A + \operatorname*{rank}B \end{align*} using rank-nullity theorem. He fell sick so he just sent the notes, which I don't understand, can someone please help me. \begin{align*}   n - \operatorname*{null}(A+B) &\leq 2n - \operatorname*{null}A -\operatorname*{null}B \\   \operatorname*{null}(A+B) &\geq -n + \operatorname*{null}A + \operatorname*{null}B \\   \operatorname*{null}A + \operatorname*{null}B &\leq n + \operatorname*{null}(A+B) \end{align*} Then it suffices to prove this inequality. \begin{align*}   N(A) \cap N(B) \subseteq N(A+B) \end{align*} I understand why this is true, but I don't understand the next line \begin{align*}   \operatorname*{null}A + \operatorname*{null}B \setminus (N(A) \cap N(B)) \leq n  \end{align*} Not only does this not make sense from the sense of 'where does $n$ come from', it also, if I understand correctly, makes no sense from the mathematical standpoint. He probably meant $\dim(N(A) \cap N(B))$ Hence, \begin{align*}   \operatorname*{null}A + \operatorname*{null}B \leq n + \dim(N(A) \cap N(B)) \leq n + {\rm null}(A+B) \end{align*} This last statement makes sense from the fact above that $N(A) \cap N(B) \subseteq N(A+B)$ If anyone knows how to prove this using rank-nullity theorem or understands what happened here, please let me know.","I know how to prove this inequality using bases for the row space, however, my professor presented a proof of using rank-nullity theorem. He fell sick so he just sent the notes, which I don't understand, can someone please help me. Then it suffices to prove this inequality. I understand why this is true, but I don't understand the next line Not only does this not make sense from the sense of 'where does come from', it also, if I understand correctly, makes no sense from the mathematical standpoint. He probably meant Hence, This last statement makes sense from the fact above that If anyone knows how to prove this using rank-nullity theorem or understands what happened here, please let me know.","\begin{align*}
  \operatorname*{rank}(A+B) \leq \operatorname*{rank}A + \operatorname*{rank}B
\end{align*} \begin{align*}
  n - \operatorname*{null}(A+B) &\leq 2n - \operatorname*{null}A -\operatorname*{null}B \\
  \operatorname*{null}(A+B) &\geq -n + \operatorname*{null}A + \operatorname*{null}B \\
  \operatorname*{null}A + \operatorname*{null}B &\leq n + \operatorname*{null}(A+B)
\end{align*} \begin{align*}
  N(A) \cap N(B) \subseteq N(A+B)
\end{align*} \begin{align*}
  \operatorname*{null}A + \operatorname*{null}B \setminus (N(A) \cap N(B)) \leq n 
\end{align*} n \dim(N(A) \cap N(B)) \begin{align*}
  \operatorname*{null}A + \operatorname*{null}B \leq n + \dim(N(A) \cap N(B)) \leq n + {\rm null}(A+B)
\end{align*} N(A) \cap N(B) \subseteq N(A+B)","['linear-algebra', 'matrices', 'proof-explanation', 'matrix-rank']"
41,The orthogonal standard form of antisymmetric matrix,The orthogonal standard form of antisymmetric matrix,,"Matrix $A=\left( a_{ij} \right) \in M_n\left( \mathbb{R} \right)  $ ,if $A$ is an antisymmetric matrix,then $a_{ij}=-a_{ji}$ . If $B$ is a real symmetric matrix,then exist $P\in O_n\left( \mathbb{R} \right) $ , $P^TBP=\mathrm{diag}\left\{ \lambda _1,...,\lambda _n \right\}  $ , $\mathrm{diag}\left\{ \lambda _1,...,\lambda _n \right\} $ is called the orthogonal standard form of $B$ I want to prove that if $A$ is a antisymmetric matrix, then exist $P\in O_n\left( \mathbb{R} \right)  $ , $P^TAP=\mathrm{diag}\left\{ \left( \begin{matrix} 	0&		a_1\\ 	-a_1&		0\\ \end{matrix} \right) ,...,\left( \begin{matrix} 	0&		a_m\\ 	-a_m&		0\\ \end{matrix} \right) ,0,..,.0 \right\}  $ .the block diagonal matrix called the orthogonal standard form of $A$ . There is my method: I want to prove it by induction, $A$ is an antisymmetric matrix,suppose $A=\left( \begin{matrix} 	A_1&		\alpha _1\\ 	-{\alpha _1}^T&		0\\ \end{matrix} \right)$ ,where $A_1\in \,\,M_{n-1}\left( \mathbb{R} \right)  $ , $A_1$ is an antisymmetric matrix,by induction,there exist $P=\left( \begin{matrix} 	P_1&		\\ 	&		1\\ \end{matrix} \right) \in O_n\left( \mathbb{R} \right)  $ $s.t.$ $P^TAP=\left( \begin{matrix} 	{P_1}^TA_1P_1&		{P_1}^T\alpha _1\\ 	-{\alpha _1}^TP_1&		0\\ \end{matrix} \right)  $ . where ${P_1}^TA_1P_1 $ become the standard form, but I don't know how to deal with ${P_1}^T\alpha _1 $ and $-{\alpha _1}^TP_1 $ . Thank you for sharing your mind.","Matrix ,if is an antisymmetric matrix,then . If is a real symmetric matrix,then exist , , is called the orthogonal standard form of I want to prove that if is a antisymmetric matrix, then exist , .the block diagonal matrix called the orthogonal standard form of . There is my method: I want to prove it by induction, is an antisymmetric matrix,suppose ,where , is an antisymmetric matrix,by induction,there exist . where become the standard form, but I don't know how to deal with and . Thank you for sharing your mind.","A=\left( a_{ij} \right) \in M_n\left( \mathbb{R} \right) 
 A a_{ij}=-a_{ji} B P\in O_n\left( \mathbb{R} \right)  P^TBP=\mathrm{diag}\left\{ \lambda _1,...,\lambda _n \right\} 
 \mathrm{diag}\left\{ \lambda _1,...,\lambda _n \right\}  B A P\in O_n\left( \mathbb{R} \right) 
 P^TAP=\mathrm{diag}\left\{ \left( \begin{matrix}
	0&		a_1\\
	-a_1&		0\\
\end{matrix} \right) ,...,\left( \begin{matrix}
	0&		a_m\\
	-a_m&		0\\
\end{matrix} \right) ,0,..,.0 \right\} 
 A A A=\left( \begin{matrix}
	A_1&		\alpha _1\\
	-{\alpha _1}^T&		0\\
\end{matrix} \right) A_1\in \,\,M_{n-1}\left( \mathbb{R} \right) 
 A_1 P=\left( \begin{matrix}
	P_1&		\\
	&		1\\
\end{matrix} \right) \in O_n\left( \mathbb{R} \right) 
 s.t. P^TAP=\left( \begin{matrix}
	{P_1}^TA_1P_1&		{P_1}^T\alpha _1\\
	-{\alpha _1}^TP_1&		0\\
\end{matrix} \right) 
 {P_1}^TA_1P_1
 {P_1}^T\alpha _1
 -{\alpha _1}^TP_1
","['linear-algebra', 'matrices', 'orthogonality']"
42,Proving existence of matrix N such that M = MNM,Proving existence of matrix N such that M = MNM,,"Doing practice problems for my qualifying exam and am a bit stumped by the following: Let $n \ge 1$ , $F$ be a field, and let $M$ be a matrix in $M_n(F)$ . Show that there exists an $N \in M_n(F)$ such that $M = MNM$ . Here is my attempt at a solution: We first define the vector space $V = F^n$ on which the matrices in $M_n(F)$ act. Now take $M \in M_n(F)$ and by the rank-nullity theorem we have that $$ker(M) \oplus Im(M) = V$$ Now, first note that if $M$ is invertible, then there exists $N = M^{-1}$ such that $NM = I$ and moreover $$M = MNM$$ giving us the result. We now assume that $M$ is not invertible (ker $(M) \neq 0$ ). We have by the First isomorphism theorem that $$V / ker(M) \cong Im(M)$$ I want to use the above fact with the property that ideals of fields are trivial to give us that $Im(M) = 0$ and therefore $M$ is just the zero map or something to this effect?","Doing practice problems for my qualifying exam and am a bit stumped by the following: Let , be a field, and let be a matrix in . Show that there exists an such that . Here is my attempt at a solution: We first define the vector space on which the matrices in act. Now take and by the rank-nullity theorem we have that Now, first note that if is invertible, then there exists such that and moreover giving us the result. We now assume that is not invertible (ker ). We have by the First isomorphism theorem that I want to use the above fact with the property that ideals of fields are trivial to give us that and therefore is just the zero map or something to this effect?",n \ge 1 F M M_n(F) N \in M_n(F) M = MNM V = F^n M_n(F) M \in M_n(F) ker(M) \oplus Im(M) = V M N = M^{-1} NM = I M = MNM M (M) \neq 0 V / ker(M) \cong Im(M) Im(M) = 0 M,"['abstract-algebra', 'matrices', 'vector-fields', 'direct-sum']"
43,Does the Kronecker product preserve common irreducibility,Does the Kronecker product preserve common irreducibility,,"A set of $d\times d$ real or complex matrices is called “commonly irreducible” if those matrices do not jointly preserve a linear subspace with dimension strictly between $0$ and $d$ . I wanted to know whether the Kronecker product preserves this property. In other words, given two sets of commonly irreducible matrices $S$ and $Q$ , is the set $  S\otimes Q = \{P_1 \otimes P_2 \mid P_1 \in S, P_2\in Q\}  $ commonly irreducible as well?","A set of real or complex matrices is called “commonly irreducible” if those matrices do not jointly preserve a linear subspace with dimension strictly between and . I wanted to know whether the Kronecker product preserves this property. In other words, given two sets of commonly irreducible matrices and , is the set commonly irreducible as well?","d\times d 0 d S Q  
S\otimes Q = \{P_1 \otimes P_2 \mid P_1 \in S, P_2\in Q\} 
","['linear-algebra', 'matrices', 'kronecker-product']"
44,Efficient recomputation of row-space projection matrix given a new row vector,Efficient recomputation of row-space projection matrix given a new row vector,,The projection matrix that projects onto the row space of a matrix $A$ is $$P_A = A^* (A A^*)^{-1} A$$ Let $B$ be $A$ with a new row vector $v$ appended. Is there an efficient way to compute $P_B$ from $P_A$ without starting from scratch? Something like the matrix determinant lemma or Sherman–Morrison formula ?,The projection matrix that projects onto the row space of a matrix is Let be with a new row vector appended. Is there an efficient way to compute from without starting from scratch? Something like the matrix determinant lemma or Sherman–Morrison formula ?,A P_A = A^* (A A^*)^{-1} A B A v P_B P_A,"['linear-algebra', 'matrices', 'algorithms', 'computational-mathematics', 'projection-matrices']"
45,Expectation of $e^{i\alpha H}$,Expectation of,e^{i\alpha H},"I have $3\times 3$ a matrix $A$ defined as $$A=e^{i\alpha H}, $$ with $H$ a $3\times 3$ random Hermitian matrix, and $\alpha\in[0,\infty]$ . I am trying to determine two things: Can we say anything on the expectation of $A$ in terms of the statistics of $H$ ? What conditions do $H$ need to satisfy so that the expectation of $A$ is diagonal? Numerically I find that if the elements of $H$ have zero mean then the expectation of $A$ is diagonal. But I would like to know the general conditions for this (and why), and an explicit expression in terms of the statistics of $H$ , if it exists!","I have a matrix defined as with a random Hermitian matrix, and . I am trying to determine two things: Can we say anything on the expectation of in terms of the statistics of ? What conditions do need to satisfy so that the expectation of is diagonal? Numerically I find that if the elements of have zero mean then the expectation of is diagonal. But I would like to know the general conditions for this (and why), and an explicit expression in terms of the statistics of , if it exists!","3\times 3 A A=e^{i\alpha H},  H 3\times 3 \alpha\in[0,\infty] A H H A H A H","['matrices', 'random-matrices', 'matrix-exponential', 'unitary-matrices', 'hermitian-matrices']"
46,The dimension of space of polynomials with matrix.,The dimension of space of polynomials with matrix.,,"Let $V:=M_{3\times 3}\ (\mathbb C)$ , i.e., $V$ is a set of $3\times 3$ matrices of complex number. Let $A=\begin{pmatrix}0&-2&0\\1&3&0\\0&0&2\end{pmatrix}$ , $W:=\{p(A)\mid p(t)\in \mathbb C [t]\}$ , where $\mathbb C[t]$ is the set of polynomials whose cooefficients are complex numbers. Then, $W$ is a subspace of $V$ . Calculate $\dim W.$ I think the characteristic polynomial of $A$ is necessary so I calculated it : $(x-2)^2(x-1)$ . And from Cayley-Hamilton, I get $(A-2I)^2(A-I)=O.$ I don't know what should I do next. For this $A$ , ・ $A$ is not a nilpotent matrix ・ $A$ doesn't seem to have periodicity. ( $n\in \mathbb N$ s.t. $A^n=A$ doesn't seem to exist.) So I'm having difficulty finding what $W$ is like. Thanks for any help.","Let , i.e., is a set of matrices of complex number. Let , , where is the set of polynomials whose cooefficients are complex numbers. Then, is a subspace of . Calculate I think the characteristic polynomial of is necessary so I calculated it : . And from Cayley-Hamilton, I get I don't know what should I do next. For this , ・ is not a nilpotent matrix ・ doesn't seem to have periodicity. ( s.t. doesn't seem to exist.) So I'm having difficulty finding what is like. Thanks for any help.",V:=M_{3\times 3}\ (\mathbb C) V 3\times 3 A=\begin{pmatrix}0&-2&0\\1&3&0\\0&0&2\end{pmatrix} W:=\{p(A)\mid p(t)\in \mathbb C [t]\} \mathbb C[t] W V \dim W. A (x-2)^2(x-1) (A-2I)^2(A-I)=O. A A A n\in \mathbb N A^n=A W,"['linear-algebra', 'matrices', 'polynomials']"
47,"Prove that $\kappa (A) = \sup\Big\{ \frac{||Ax||}{||Ay||},\ ||x|| = ||y||\Big\}$.",Prove that .,"\kappa (A) = \sup\Big\{ \frac{||Ax||}{||Ay||},\ ||x|| = ||y||\Big\}","I am trying to prove this for my numerical analysis class. This is from chapter 4.4 of Kincaid and Cheney's book. So far I haven´t got any good idea. I have tried $$ \|A\| \|A^{-1}\| = \sup \|A\frac{u}{\|v\|}\|\cdot \|A^{-1}\frac{v}{\|u\|}\|,\quad \|u\|=\|v\| $$ But I coudn't advance any further. What confuses me the most is that there is not $||A^{-1}||$ in the equation. As far as I know there is not relation between a matrix norm and its inverse norm besides $\|A^{-1}\| \geq (\|A\|)^{-1}$ which haven't been useful. Please help.",I am trying to prove this for my numerical analysis class. This is from chapter 4.4 of Kincaid and Cheney's book. So far I haven´t got any good idea. I have tried But I coudn't advance any further. What confuses me the most is that there is not in the equation. As far as I know there is not relation between a matrix norm and its inverse norm besides which haven't been useful. Please help.,"
\|A\| \|A^{-1}\| = \sup \|A\frac{u}{\|v\|}\|\cdot \|A^{-1}\frac{v}{\|u\|}\|,\quad \|u\|=\|v\|
 ||A^{-1}|| \|A^{-1}\| \geq (\|A\|)^{-1}","['linear-algebra', 'matrices', 'numerical-methods', 'matrix-norms', 'condition-number']"
48,Prove the given matrix is invertible,Prove the given matrix is invertible,,"Question. Let $A$ be an $n\times n$ matrix such that each row and each column of $A$ contains all of the entries $1,2,2^2,...,2^{n-1}$ in some order. Prove that $A$ is invertible. An easy fact is that $(2^n-1,e)$ is an eigenpair of $A$ where $e\in\mathbb{R}^n$ is a vector of all-one entries. However, this is not sufficient for proving $A$ is invertible. One possible route is to prove $0$ is not an eigenvalue of $A$ . I am also thinking about Gershgorin discs. The deleted absolute row sum on the $k$ -th row is $2^n-1-a_{kk}$ . Thus the Gershgorin disc is $$D_k:=\{z\in\mathbb{C}\mid|z-a_{kk}|\le2^n-1-a_{kk}\}.$$ If $0\notin D_k$ , then $$a_{kk}=|0-a_{kk}|>2^n-1-a_{kk}\implies a_{kk}>2^{n-1}-\frac12.$$ In other words, only if $a_{kk}=2^{n-1}$ will $0\notin D_k$ happen. For the structure of $A$ , I am thinking about a sudoku-like array, so it is not guaranteed that the diagonal entries of $A$ are all $2^{n-1}$ . For instance, $$\begin{bmatrix} 1 & 2 \\ 2 & 1 \end{bmatrix},\quad\begin{bmatrix} 1 & 2 & 4 \\ 4 & 1 & 2 \\ 2 & 4 & 1 \end{bmatrix},\quad\begin{bmatrix} 1 & 2 & 4 & 8 \\ 2 & 4 & 8 & 1 \\ 4 & 8 & 1 & 2 \\ 8 &1 &2 & 4 \end{bmatrix}.$$ In particular, permutation similarity does not change the diagonal entries. Currently I am stuck at this point. Any clever ideas or suggestions are highly welcomed. Update. Thanks for all the comments and solutions suggested below! Jimmy’s hint is almost detailed and close to the arguments for proving the Gershgorin disc theorem. I am also conjecturing if there is no dominating terms among a given sequence (here $2^{n-1}$ is dominating as the sum of all other terms is $2^{n-1}-1$ ), then such matrix $A$ might be singular. Micheal’s smart comment exploits the special structure here. Let me write down the details for future readers. By taking all the entries modulo 2, we get a permutation matrix, whose determinant is $\pm 1$ . Thus the determinant of $A$ is odd and cannot be zero. Update 2. My conjecture is true. The following matrix is singular : $$\begin{bmatrix} 1 & 2 & 3 & 4 \\ 2 & 1 & 4 & 3 \\ 3 & 4 & 1 & 2 \\ 4 & 3 & 2 & 1 \end{bmatrix}.$$ I am beyond happy that I could communicate with your guys about this question!","Question. Let be an matrix such that each row and each column of contains all of the entries in some order. Prove that is invertible. An easy fact is that is an eigenpair of where is a vector of all-one entries. However, this is not sufficient for proving is invertible. One possible route is to prove is not an eigenvalue of . I am also thinking about Gershgorin discs. The deleted absolute row sum on the -th row is . Thus the Gershgorin disc is If , then In other words, only if will happen. For the structure of , I am thinking about a sudoku-like array, so it is not guaranteed that the diagonal entries of are all . For instance, In particular, permutation similarity does not change the diagonal entries. Currently I am stuck at this point. Any clever ideas or suggestions are highly welcomed. Update. Thanks for all the comments and solutions suggested below! Jimmy’s hint is almost detailed and close to the arguments for proving the Gershgorin disc theorem. I am also conjecturing if there is no dominating terms among a given sequence (here is dominating as the sum of all other terms is ), then such matrix might be singular. Micheal’s smart comment exploits the special structure here. Let me write down the details for future readers. By taking all the entries modulo 2, we get a permutation matrix, whose determinant is . Thus the determinant of is odd and cannot be zero. Update 2. My conjecture is true. The following matrix is singular : I am beyond happy that I could communicate with your guys about this question!","A n\times n A 1,2,2^2,...,2^{n-1} A (2^n-1,e) A e\in\mathbb{R}^n A 0 A k 2^n-1-a_{kk} D_k:=\{z\in\mathbb{C}\mid|z-a_{kk}|\le2^n-1-a_{kk}\}. 0\notin D_k a_{kk}=|0-a_{kk}|>2^n-1-a_{kk}\implies a_{kk}>2^{n-1}-\frac12. a_{kk}=2^{n-1} 0\notin D_k A A 2^{n-1} \begin{bmatrix} 1 & 2 \\ 2 & 1 \end{bmatrix},\quad\begin{bmatrix} 1 & 2 & 4 \\ 4 & 1 & 2 \\ 2 & 4 & 1 \end{bmatrix},\quad\begin{bmatrix} 1 & 2 & 4 & 8 \\ 2 & 4 & 8 & 1 \\ 4 & 8 & 1 & 2 \\ 8 &1 &2 & 4 \end{bmatrix}. 2^{n-1} 2^{n-1}-1 A \pm 1 A \begin{bmatrix} 1 & 2 & 3 & 4 \\ 2 & 1 & 4 & 3 \\ 3 & 4 & 1 & 2 \\ 4 & 3 & 2 & 1 \end{bmatrix}.","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
49,Does a singular value decomposition always exist for complex matrix?,Does a singular value decomposition always exist for complex matrix?,,"I know for real matrix A, SVD always exists, but I am wondering for any complex matrix, will SVD still exist for any scenarios? Thanks.","I know for real matrix A, SVD always exists, but I am wondering for any complex matrix, will SVD still exist for any scenarios? Thanks.",,"['linear-algebra', 'matrices', 'matrix-decomposition', 'svd', 'singular-values']"
50,Light bulbs in a rectangular grid,Light bulbs in a rectangular grid,,"Suppose we have a grid of $5 \times 5$ light bulbs with a switch to each bulb. If we press a switch it toggles all the lights in its row and column. Given any initial configuration, is it possible to make all the lights on or all the lights off by a particular sequence of switches? I do not remember whether I have read about this problem somewhere or it just came up in my mind by some similar problems. Any hints/suggestions/links are highly appreciated.","Suppose we have a grid of light bulbs with a switch to each bulb. If we press a switch it toggles all the lights in its row and column. Given any initial configuration, is it possible to make all the lights on or all the lights off by a particular sequence of switches? I do not remember whether I have read about this problem somewhere or it just came up in my mind by some similar problems. Any hints/suggestions/links are highly appreciated.",5 \times 5,"['linear-algebra', 'matrices', 'discrete-mathematics', 'finite-fields', 'puzzle']"
51,hard olympiad level matrix problem,hard olympiad level matrix problem,,"Let a matrix $A\in\Bbb M_n(R) $ , $n>2$ for which exists a number $a\in\Bbb [-2,2 ]$ so that : $A^2 -aA + I_n = O_n$ Prove that for any natural number $m\in\Bbb N$ there exists a unique $a_m\in\Bbb [-2 , 2 ]$ for which $A^{2m}-a_mA^m + I_n = O_n$ . How I tried solving it : I've written $A^2 = aA - I_n$ from which I generalized $A^n$ as the following series : $A^n = x_nA - x_{n-1}I_n$ where $x_1 = 1$ and $x_2 = a$ and where $x_{n+1} = ax_n - x_{n-1}$ after which i applied the characteristic equation and got : $r^2-ar=1=0$ so naturally $r_{1,2}=\frac{a\pm i\sqrt{4-a^2}}{2}$ We can write $r_{1,2} = \cos{t} \pm i\sin{t}$ where : $\sin{t} = \frac{\sqrt{4-a^2}}{2}$ , $\cos{t} = \frac{a}{2}$ and $t\in [0,\pi]$ so $x_n = C_1\cos{nt} + C_2\sin{nt}$ where if we replace $n$ with $1$ and $2$ and solve the system we get $C_1= 0$ and $C_2= \frac{2}{\sqrt{4-a_2} }$ so we easily get $x_n = \frac{2}{\sqrt{4-a_2} }\sin{nt}$ so $A^n = \frac{2}{\sqrt{4-a_2} }$ $[\sin{nt} A - \sin{(n-1)t} I_n]$ Furthermore we have that $A^2 -aA = I_n$ so $A(aI_n-A) = I_n$ so $\det{A}$ can't be equal to $0$ so we pretty much know the value of $A^{-n}$ , can someone help me proceed ? I've tried to calculate $A^m + A^{-m}$ and got $a_mI_n$ as an answer, if this enough to prove the statement correct ? And if it isn't i could really use a hand here.","Let a matrix , for which exists a number so that : Prove that for any natural number there exists a unique for which . How I tried solving it : I've written from which I generalized as the following series : where and and where after which i applied the characteristic equation and got : so naturally We can write where : , and so where if we replace with and and solve the system we get and so we easily get so Furthermore we have that so so can't be equal to so we pretty much know the value of , can someone help me proceed ? I've tried to calculate and got as an answer, if this enough to prove the statement correct ? And if it isn't i could really use a hand here.","A\in\Bbb M_n(R)  n>2 a\in\Bbb [-2,2 ] A^2 -aA + I_n = O_n m\in\Bbb N a_m\in\Bbb [-2 , 2 ] A^{2m}-a_mA^m + I_n = O_n A^2 = aA - I_n A^n A^n = x_nA - x_{n-1}I_n x_1 = 1 x_2 = a x_{n+1} = ax_n - x_{n-1} r^2-ar=1=0 r_{1,2}=\frac{a\pm i\sqrt{4-a^2}}{2} r_{1,2} = \cos{t} \pm i\sin{t} \sin{t} = \frac{\sqrt{4-a^2}}{2} \cos{t} = \frac{a}{2} t\in [0,\pi] x_n = C_1\cos{nt} + C_2\sin{nt} n 1 2 C_1= 0 C_2= \frac{2}{\sqrt{4-a_2} } x_n = \frac{2}{\sqrt{4-a_2} }\sin{nt} A^n = \frac{2}{\sqrt{4-a_2} } [\sin{nt} A - \sin{(n-1)t} I_n] A^2 -aA = I_n A(aI_n-A) = I_n \det{A} 0 A^{-n} A^m + A^{-m} a_mI_n","['linear-algebra', 'matrices']"
52,"If $\lvert X_{ij}\rvert \leq 1$ for all $i, j$, how do we show that the diagonal elements of $(X^T X)^{-1}$ are at least $\frac{1}{n}$?","If  for all , how do we show that the diagonal elements of  are at least ?","\lvert X_{ij}\rvert \leq 1 i, j (X^T X)^{-1} \frac{1}{n}","I am reformulating the question in what I hope will be a simpler, clearer way. However, I am also leaving the original formulation below. Given an $n \times p$ matrix $X$ , if $\lvert X_{ij}\rvert \leq 1$ for all $i, j$ , how do we show that $(X^T X)^{-1}_{j,j} \geq \frac{1}{n}$ for all $j$ ? Original formulation: I have a linear model $Y = X\beta + \epsilon$ where $\epsilon \sim (0_n, \sigma^2 I_n)$ . The matrix $X$ is $n \times p$ . If $\hat \beta$ is the least squares estimator of $\beta$ and $\lvert x_{ij} \rvert \leq 1$ for all $i, j$ , then I want to show that $\text{Var}(\hat \beta_j) \geq \sigma^2 / n$ for $j = 1, \dots, p$ . (I guess the point here is that matrices $X$ with small values lead to large variability in the least squares estimators for $\beta$ .) I know that $\hat \beta = (X^T X)^{-1} X^T Y$ , and $\text{Var}(\hat \beta) = \sigma^2 (X^T X)^{-1}$ . I was thinking that if I had a formula for the diagonal entry of $(X^T X)^{-1}$ that might be handy, but I'm not sure where I am going with that. I was also thinking about the triangle inequality, but I'm not sure how to use it. One consequence of the assumption $\lvert x_{ij} \rvert \leq 1$ is that all entries in $X^T X$ will have absolute value less than $n$ , right? I was also playing with a toy $3 \times 2$ example for X, but that didn't seem to lead anywhere. I appreciate any help. Edit: Response to Hyperplane's answer (NOTE: the answer was deleted) I think it is clear that proving that $(X^T X)^{-1}_{j,j} \geq \frac{1}{n}$ for all $j$ will suffice, because multiplying both sides by $\sigma^2$ will give us the desired inequality. I see that you have argued that all the diagonal elements of $(X^T X)^{-1}$ are trapped between the smallest and largest eigenvalues of $(X^T X)^{-1}$ . In particular, \begin{align*} (X^T X)^{-1}_{j,j} \geq \frac{1}{\lambda_{\max}(X^T X)} \end{align*} because eigenvalues are inverted for an inverse matrix. Therefore, if we can show that \begin{align*} \frac{1}{\lambda_{\max}(X^T X)} &\geq \frac{1}{n}, \text{ or equivalently}\\ n &\geq \lambda_{\max}(X^T X) \end{align*} we'll essentially be done. (There is an assumption here that $\lambda_{\max}(X^T X) \neq 0$ ; I am not entirely sure how to argue this away.) My main concern is in your second last line. Matrix norms are new to me, but according to Wikipedia , we should have $\lambda_{\max}(X^T X) = \Vert X \Vert_2^2$ , which (unless I'm mistaken) means you've shown that $\lambda_{\max}(X^T X) \leq n^2$ , which is not good enough. I am hoping this is a simple misunderstanding on my part.","I am reformulating the question in what I hope will be a simpler, clearer way. However, I am also leaving the original formulation below. Given an matrix , if for all , how do we show that for all ? Original formulation: I have a linear model where . The matrix is . If is the least squares estimator of and for all , then I want to show that for . (I guess the point here is that matrices with small values lead to large variability in the least squares estimators for .) I know that , and . I was thinking that if I had a formula for the diagonal entry of that might be handy, but I'm not sure where I am going with that. I was also thinking about the triangle inequality, but I'm not sure how to use it. One consequence of the assumption is that all entries in will have absolute value less than , right? I was also playing with a toy example for X, but that didn't seem to lead anywhere. I appreciate any help. Edit: Response to Hyperplane's answer (NOTE: the answer was deleted) I think it is clear that proving that for all will suffice, because multiplying both sides by will give us the desired inequality. I see that you have argued that all the diagonal elements of are trapped between the smallest and largest eigenvalues of . In particular, because eigenvalues are inverted for an inverse matrix. Therefore, if we can show that we'll essentially be done. (There is an assumption here that ; I am not entirely sure how to argue this away.) My main concern is in your second last line. Matrix norms are new to me, but according to Wikipedia , we should have , which (unless I'm mistaken) means you've shown that , which is not good enough. I am hoping this is a simple misunderstanding on my part.","n \times p X \lvert X_{ij}\rvert \leq 1 i, j (X^T X)^{-1}_{j,j} \geq \frac{1}{n} j Y = X\beta + \epsilon \epsilon \sim (0_n, \sigma^2 I_n) X n \times p \hat \beta \beta \lvert x_{ij} \rvert \leq 1 i, j \text{Var}(\hat \beta_j) \geq \sigma^2 / n j = 1, \dots, p X \beta \hat \beta = (X^T X)^{-1} X^T Y \text{Var}(\hat \beta) = \sigma^2 (X^T X)^{-1} (X^T X)^{-1} \lvert x_{ij} \rvert \leq 1 X^T X n 3 \times 2 (X^T X)^{-1}_{j,j} \geq \frac{1}{n} j \sigma^2 (X^T X)^{-1} (X^T X)^{-1} \begin{align*}
(X^T X)^{-1}_{j,j} \geq \frac{1}{\lambda_{\max}(X^T X)}
\end{align*} \begin{align*}
\frac{1}{\lambda_{\max}(X^T X)} &\geq \frac{1}{n}, \text{ or equivalently}\\
n &\geq \lambda_{\max}(X^T X)
\end{align*} \lambda_{\max}(X^T X) \neq 0 \lambda_{\max}(X^T X) = \Vert X \Vert_2^2 \lambda_{\max}(X^T X) \leq n^2","['linear-algebra', 'matrices', 'statistics', 'inequality', 'least-squares']"
53,set of invertible diagonal matrices,set of invertible diagonal matrices,,"Let $\mathcal{T}$ be the set of invertible diagonal matrices. Show that for any invertible matrix $B$ such that $B\mathcal{T}B^{-1} = \mathcal{T}, B=PT$ for some permutation matrix $P$ and invertible diagonal matrix $T.$ Here, $AB := \{ab: a\in A, b\in B\}$ when $A$ and $B$ are sets. I'm not sure how to show this result, though I'm pretty sure I need to consider eigenvalues, diagonalizable matrices, and change of basis matrices. Using the definition alone, I get stuck quite easily; I only know that for any invertible diagonal matrix $T, BTB^{-1}$ is an invertible diagonal matrix and for any invertible diagonal matrix $T', T' = BT''B^{-1}$ for some invertible diagonal matrix $T''.$ I know how to show that if an $n\times n$ matrix has $n$ distinct eigenvalues, then it has $n$ distinct eigenvectors (an eigenvector can correspond to only one eigenvalue) and thus these $n$ eigenvectors are linearly independent, so the matrix is diagonalizable, but I'm not sure if this is useful. Clarification: Here $B\mathcal{T}B^{-1} := \{BTB^{-1} : T\in \mathcal{T}\}$","Let be the set of invertible diagonal matrices. Show that for any invertible matrix such that for some permutation matrix and invertible diagonal matrix Here, when and are sets. I'm not sure how to show this result, though I'm pretty sure I need to consider eigenvalues, diagonalizable matrices, and change of basis matrices. Using the definition alone, I get stuck quite easily; I only know that for any invertible diagonal matrix is an invertible diagonal matrix and for any invertible diagonal matrix for some invertible diagonal matrix I know how to show that if an matrix has distinct eigenvalues, then it has distinct eigenvectors (an eigenvector can correspond to only one eigenvalue) and thus these eigenvectors are linearly independent, so the matrix is diagonalizable, but I'm not sure if this is useful. Clarification: Here","\mathcal{T} B B\mathcal{T}B^{-1} = \mathcal{T}, B=PT P T. AB := \{ab: a\in A, b\in B\} A B T, BTB^{-1} T', T' = BT''B^{-1} T''. n\times n n n n B\mathcal{T}B^{-1} := \{BTB^{-1} : T\in \mathcal{T}\}","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'permutations', 'linear-transformations']"
54,A determinant of a $3\times 3$ matrix,A determinant of a  matrix,3\times 3,"I have a $3\times3$ real matrix of the following form $$ A=\left( \begin{matrix} a & b & c \\ * & * & * \\ * & * & * \end{matrix} \right), $$ where $a,b,c$ are fixed. I would like to find the other entries of the matrix in such a way that $\det A = 0$ if and only if $a=b=c=0$ and no combination of $a,b,c$ appears at the denominator of the other entries. So far I have tried with different combinations of $a,b,c$ , but nothing seems to work. In particular, I always find a determinant that vanishes e.g. when $a=0$ but the others need not be zero, or something like that. Is there a clever choice I do not see?","I have a real matrix of the following form where are fixed. I would like to find the other entries of the matrix in such a way that if and only if and no combination of appears at the denominator of the other entries. So far I have tried with different combinations of , but nothing seems to work. In particular, I always find a determinant that vanishes e.g. when but the others need not be zero, or something like that. Is there a clever choice I do not see?","3\times3 
A=\left(
\begin{matrix}
a & b & c \\
* & * & * \\
* & * & *
\end{matrix}
\right),
 a,b,c \det A = 0 a=b=c=0 a,b,c a,b,c a=0","['linear-algebra', 'matrices']"
55,Positive semi-definite conditional covariance matrix,Positive semi-definite conditional covariance matrix,,"Let $X$ be an $n\times m$ random matrix, where each entry is a real square integrable random variable on the probability space $(\Omega,\mathcal A,P)$ . Consider the following matrix: $$E[XX'\mid\mathcal F],$$ where $\mathcal F$ is a sub- $\sigma$ algebra of $\mathcal A$ . If $a\in\mathbb{R}^{n}$ , then from the linearity of conditional expectations we have $$a'E[XX'\mid\mathcal F]a=E[(a'X)^2\mid\mathcal F]\geq 0 \quad P\text{-almost surely,}$$ but the null set might depend on $a$ . Can I find a version of $E[XX'\mid\mathcal F]$ which is positive semi-definite almost surely?","Let be an random matrix, where each entry is a real square integrable random variable on the probability space . Consider the following matrix: where is a sub- algebra of . If , then from the linearity of conditional expectations we have but the null set might depend on . Can I find a version of which is positive semi-definite almost surely?","X n\times m (\Omega,\mathcal A,P) E[XX'\mid\mathcal F], \mathcal F \sigma \mathcal A a\in\mathbb{R}^{n} a'E[XX'\mid\mathcal F]a=E[(a'X)^2\mid\mathcal F]\geq 0 \quad P\text{-almost surely,} a E[XX'\mid\mathcal F]","['matrices', 'probability-theory', 'measure-theory', 'conditional-expectation']"
56,Inverse Matrix of a Special Matrix for Optimization,Inverse Matrix of a Special Matrix for Optimization,,"For some portfolio optimization problems, it finally comes to an inverse matrix of a special matrix $$B=\begin{bmatrix} A & \mathbf{1} \\ \mathbf{1}' & 0 \end{bmatrix}$$ where $A$ is a symmetric matrix and $\mathbf{1}=(1,\cdots,1)'\in\mathbb{R}^n$ . Can anyone help to find the inverse matrix of matrix $B$ ? Thanks a lot! Below is the background of the above problem: Assume $\omega$ is the weight vector of the portfolio, $\mu$ is the return vector and $\Sigma$ is the covariance matrix. The constraint is $\sum\omega=1$ . For different optimization problems, we will have different targets: For the optimization problem of minimum variance: $$\min_\omega\omega'\Sigma\omega$$ applying the Lagrange multiplier, we have the matrix equation $$\begin{bmatrix} 2\Sigma & \mathbf{1} \\ \mathbf{1}' & 0 \end{bmatrix} \begin{bmatrix} \omega \\ \lambda \end{bmatrix}= \begin{bmatrix} \mathbf{0} \\ 1 \end{bmatrix}$$ For the optimization problem of mean-variance optimization: $$\max_\omega\omega'\mu-\frac{1}{2}\delta\omega'\Sigma\omega$$ applying the Lagrange multiplier, we have the matrix equation $$\begin{bmatrix} -\delta\Sigma & \mathbf{1} \\ \mathbf{1}' & 0 \end{bmatrix} \begin{bmatrix} \omega \\ \lambda \end{bmatrix}= \begin{bmatrix} -\mu \\ 1 \end{bmatrix}$$ Anyway, it comes to the problem of inverse matrix of $$B=\begin{bmatrix} A & \mathbf{1} \\ \mathbf{1}' & 0 \end{bmatrix}$$","For some portfolio optimization problems, it finally comes to an inverse matrix of a special matrix where is a symmetric matrix and . Can anyone help to find the inverse matrix of matrix ? Thanks a lot! Below is the background of the above problem: Assume is the weight vector of the portfolio, is the return vector and is the covariance matrix. The constraint is . For different optimization problems, we will have different targets: For the optimization problem of minimum variance: applying the Lagrange multiplier, we have the matrix equation For the optimization problem of mean-variance optimization: applying the Lagrange multiplier, we have the matrix equation Anyway, it comes to the problem of inverse matrix of","B=\begin{bmatrix}
A & \mathbf{1} \\
\mathbf{1}' & 0
\end{bmatrix} A \mathbf{1}=(1,\cdots,1)'\in\mathbb{R}^n B \omega \mu \Sigma \sum\omega=1 \min_\omega\omega'\Sigma\omega \begin{bmatrix}
2\Sigma & \mathbf{1} \\
\mathbf{1}' & 0
\end{bmatrix}
\begin{bmatrix}
\omega \\
\lambda
\end{bmatrix}=
\begin{bmatrix}
\mathbf{0} \\
1
\end{bmatrix} \max_\omega\omega'\mu-\frac{1}{2}\delta\omega'\Sigma\omega \begin{bmatrix}
-\delta\Sigma & \mathbf{1} \\
\mathbf{1}' & 0
\end{bmatrix}
\begin{bmatrix}
\omega \\
\lambda
\end{bmatrix}=
\begin{bmatrix}
-\mu \\
1
\end{bmatrix} B=\begin{bmatrix}
A & \mathbf{1} \\
\mathbf{1}' & 0
\end{bmatrix}","['linear-algebra', 'matrices', 'matrix-equations', 'matrix-calculus']"
57,Matrix equation with diagonal matrix,Matrix equation with diagonal matrix,,"Let $A \in\mathbb{R}^{n\times n}$ be a given Positive-Semi Definite matrix , $\theta\in\mathbb{R}^n$ a vector, $x\in\mathbb{R}$ an unknown variable, and $a\in\mathbb{R}^+$ a positive constant. I have the following equation $$ \theta^TA(A+xI)^{-1}(A+xI)^{-1}A\theta  = a $$ where $I$ is the identity matrix. Is it possible to solve this equation for $x?$ What I have tried: $$ \text{trace}(\theta^TA(A+xI)^{-1}(A+xI)^{-1}A\theta)  = \text{trace}(a) \\\theta^TAA\theta  \cdot\text{trace}((A+xI)^{-1}(A+xI)^{-1})  = a \\ \sum_{i = 1}^n\frac{1}{(x+s_i)^2}=\frac{a}{\|\theta\|_{A^2}^2} $$ where $s_i$ are the singular values of the $A$ . Is it correct? How can i proceed from here?","Let be a given Positive-Semi Definite matrix , a vector, an unknown variable, and a positive constant. I have the following equation where is the identity matrix. Is it possible to solve this equation for What I have tried: where are the singular values of the . Is it correct? How can i proceed from here?","A \in\mathbb{R}^{n\times n} \theta\in\mathbb{R}^n x\in\mathbb{R} a\in\mathbb{R}^+ 
\theta^TA(A+xI)^{-1}(A+xI)^{-1}A\theta  = a
 I x? 
\text{trace}(\theta^TA(A+xI)^{-1}(A+xI)^{-1}A\theta)  = \text{trace}(a)
\\\theta^TAA\theta  \cdot\text{trace}((A+xI)^{-1}(A+xI)^{-1})  = a
\\ \sum_{i = 1}^n\frac{1}{(x+s_i)^2}=\frac{a}{\|\theta\|_{A^2}^2}
 s_i A","['linear-algebra', 'matrices', 'matrix-equations', 'matrix-calculus']"
58,"Proof verification: $su(2)$ is not isomorphic to $sl(2,R)$",Proof verification:  is not isomorphic to,"su(2) sl(2,R)","I’m trying to prove Lie algebra $sl(2,R)$ is not isomorphic to $su(2)$ by showing that all Lie algebra isomorphism will have determinant $0$ , hence a contradiction. However, my computation gives a ridiculous result that said such isomorphism always exist. Please help me checking what’s wrong here. I take the usual basis of $sl(2,R)$ such that $$[X_1,X_2]=X_2$$ $$[X_1,X_3]=-X_3$$ $$[X_2,X_3]=2X_1$$ And the usual basis of $su(2)$ such that $$[Y_i,Y_j]=\epsilon_{ijk}Y_k$$ Now suppose $sl(2,R)$ is isomorphic to $su(2)$ , then there must exist a set of basis in $sl(2,R)$ , which can be written as $$X’_1=a_1X_1+b_1X_2+c_1X_3$$ $$X’_2=a_2X_1+b_2X_2+c_2X_3$$ $$X’_3=a_3X_1+b_3X_2+c_3X_3$$ satisfies $$[X’_i,X’_j]=\epsilon_{ijk}X’_k$$ We then have $$[a_1X_1+b_1X_2+c_1X_3,a_2X_1+b_2X_2+c_2X_3]\\=a_1b_2[X_1,X_2]+a_1c_2[X_1,X_3]+b_1a_2[X_2,X_1]+b_1c_2[X_2,X_3]+c_1a_2[X_3,X_1]+c_1b_2[X_3,X_2]\\=a_1b_2X_2+a_1c_2(-X_3)+b_1a_2(-X_2)+b_1c_2(2X_1)+c_1a_2(X_3)+c_1b_2(-2X_1)\\=(2b_1c_2-2c_1b_2)X_1+(a_1b_2-b_1a_2)X_2+(c_1a_2-a_1c_2)X_3$$ Which implies $$a_3=2b_1c_2-2b_2c_1$$ $$b_3=a_1b_2-a_2b_1$$ $$c_3=a_2c_1-a_1c_2$$ The other $6$ conditions are obtained just by permutation $(1,2,3)$ . By above relationships, I finally get $$det(a_i,b_j,c_k)=\frac{1}{2}a_1^2+2b_1c_1= \frac{1}{2}a_2^2+2b_2c_2= \frac{1}{2}a_3^2+2b_3c_3\\=\frac{1}{2}a_1^2+\frac{1}{2}a_2^2+\frac{1}{2}a_3^2=b_1c_1+b_2c_2+b_3c_3 $$ by all possible cofactors expansions. This result seems not going to give $det(a_i,b_j,c_k)=0$ . I’m not sure what is going wrong here. I have checked my calculations more than $10$ times to make sure nothing goes wrong. Please help me, really thanks!","I’m trying to prove Lie algebra is not isomorphic to by showing that all Lie algebra isomorphism will have determinant , hence a contradiction. However, my computation gives a ridiculous result that said such isomorphism always exist. Please help me checking what’s wrong here. I take the usual basis of such that And the usual basis of such that Now suppose is isomorphic to , then there must exist a set of basis in , which can be written as satisfies We then have Which implies The other conditions are obtained just by permutation . By above relationships, I finally get by all possible cofactors expansions. This result seems not going to give . I’m not sure what is going wrong here. I have checked my calculations more than times to make sure nothing goes wrong. Please help me, really thanks!","sl(2,R) su(2) 0 sl(2,R) [X_1,X_2]=X_2 [X_1,X_3]=-X_3 [X_2,X_3]=2X_1 su(2) [Y_i,Y_j]=\epsilon_{ijk}Y_k sl(2,R) su(2) sl(2,R) X’_1=a_1X_1+b_1X_2+c_1X_3 X’_2=a_2X_1+b_2X_2+c_2X_3 X’_3=a_3X_1+b_3X_2+c_3X_3 [X’_i,X’_j]=\epsilon_{ijk}X’_k [a_1X_1+b_1X_2+c_1X_3,a_2X_1+b_2X_2+c_2X_3]\\=a_1b_2[X_1,X_2]+a_1c_2[X_1,X_3]+b_1a_2[X_2,X_1]+b_1c_2[X_2,X_3]+c_1a_2[X_3,X_1]+c_1b_2[X_3,X_2]\\=a_1b_2X_2+a_1c_2(-X_3)+b_1a_2(-X_2)+b_1c_2(2X_1)+c_1a_2(X_3)+c_1b_2(-2X_1)\\=(2b_1c_2-2c_1b_2)X_1+(a_1b_2-b_1a_2)X_2+(c_1a_2-a_1c_2)X_3 a_3=2b_1c_2-2b_2c_1 b_3=a_1b_2-a_2b_1 c_3=a_2c_1-a_1c_2 6 (1,2,3) det(a_i,b_j,c_k)=\frac{1}{2}a_1^2+2b_1c_1= \frac{1}{2}a_2^2+2b_2c_2= \frac{1}{2}a_3^2+2b_3c_3\\=\frac{1}{2}a_1^2+\frac{1}{2}a_2^2+\frac{1}{2}a_3^2=b_1c_1+b_2c_2+b_3c_3  det(a_i,b_j,c_k)=0 10","['matrices', 'lie-groups', 'lie-algebras']"
59,Infinite minimal left ideals,Infinite minimal left ideals,,"Let $R$ be an infinite field and let $S$ be the ring $$S = \begin{pmatrix} R & R & R\\ 0 & R & 0\\ 0 & 0 & R \end{pmatrix}$$ Show that there are infinite minimal left ideals of $S$ . I tried to consider the possible minimal left ideals of $S$ . Given a nonzero matrix $A$ in one of these ideals, say $I$ , I think I could multiply only once, or maybe twice in the left by some matrices of $S$ to get a matrix $B$ whose nonzero entries are in the same positions as $A$ , but $B's$ nonzero entries are simply $1$ 's, so that, since $B$ is in the ideal $I$ , we can get any matrix with the nonzero entries in the same positions as $A$ but being any elements of $R$ . Hence, since there are finitely many possible choices for the positions of the nonzero entries of a matrix $3$ x $3$ , it seems to follow that $S$ has finitely many minimal left ideals... contradicting what I'm supposed to prove. Any help would be appreciated.","Let be an infinite field and let be the ring Show that there are infinite minimal left ideals of . I tried to consider the possible minimal left ideals of . Given a nonzero matrix in one of these ideals, say , I think I could multiply only once, or maybe twice in the left by some matrices of to get a matrix whose nonzero entries are in the same positions as , but nonzero entries are simply 's, so that, since is in the ideal , we can get any matrix with the nonzero entries in the same positions as but being any elements of . Hence, since there are finitely many possible choices for the positions of the nonzero entries of a matrix x , it seems to follow that has finitely many minimal left ideals... contradicting what I'm supposed to prove. Any help would be appreciated.","R S S = \begin{pmatrix}
R & R & R\\
0 & R & 0\\
0 & 0 & R
\end{pmatrix} S S A I S B A B's 1 B I A R 3 3 S","['abstract-algebra', 'matrices', 'ring-theory', 'field-theory', 'ideals']"
60,"If $A$ is an idempotent matrix i.e, $A^2=A$, and given that $|A| \ne 0$, prove that $A=I$, where $I$ is the identity matrix.","If  is an idempotent matrix i.e, , and given that , prove that , where  is the identity matrix.",A A^2=A |A| \ne 0 A=I I,"If $A$ is an idempotent matrix i.e, $A^2=A$ , and given that $|A| \ne 0$ , prove that $A=I$ , where $I$ is the identity matrix. Now, clearly if $|A| \ne 0$ , $A^{-1}$ exists and multiplying with $A^{-1}$ in the equation $A^2=A$ yields the result. But I wanted to proceed differently. If we take the determinant on both sides of $A^2=A$ , we get either $|A|=0$ or $|A|=1$ and if we rewrite the equation as $A^2-A=A(A-I)=O$ and then taking the determinant on both sides yields $|A|=0$ and $|A-I|=0$ but since $|A| \ne 0$ , we get from above two conditions i.e $|A|=1$ and $|A-I|=0$ . So my question is, do these two conditions together imply that $A=I$ ? Or are they still insufficient?","If is an idempotent matrix i.e, , and given that , prove that , where is the identity matrix. Now, clearly if , exists and multiplying with in the equation yields the result. But I wanted to proceed differently. If we take the determinant on both sides of , we get either or and if we rewrite the equation as and then taking the determinant on both sides yields and but since , we get from above two conditions i.e and . So my question is, do these two conditions together imply that ? Or are they still insufficient?",A A^2=A |A| \ne 0 A=I I |A| \ne 0 A^{-1} A^{-1} A^2=A A^2=A |A|=0 |A|=1 A^2-A=A(A-I)=O |A|=0 |A-I|=0 |A| \ne 0 |A|=1 |A-I|=0 A=I,"['matrices', 'determinant']"
61,Eigenvalues of Symmetric/Hermitian Matrices,Eigenvalues of Symmetric/Hermitian Matrices,,"I am new to stack exchange so please excuse me as I write in JAX I have a question on linear algebra that goes like this :  If $A$ is an $n\times n$ real symmetric matrix or complex Hermitian matrix, then its eigenvalues are real. I want to prove this theorem but I hope someone can this proof and correct me if I am wrong : We have that By the Schur’s decomposition, $A=QRQ^{*}$ however $A=A^{*}=(QRQ^{*})^{*}=QR^{*}Q^{*}$ where we have that $R$ an upper triangular matrix with eigenvalues of $A$ on its diagonal entries, so $R^{*}$ is a lower triangular matrix thus $A-A^{*}=Q(R-R^{*})Q^{*}=0\implies R-R^{*}=0\implies r_{i,i}=\overline{r_{i,i}}$ . Therefore, $R$ is a diagonal matrix and its diagonal entries are real. I would like to thank whoever checks my proof. Best regards!","I am new to stack exchange so please excuse me as I write in JAX I have a question on linear algebra that goes like this :  If is an real symmetric matrix or complex Hermitian matrix, then its eigenvalues are real. I want to prove this theorem but I hope someone can this proof and correct me if I am wrong : We have that By the Schur’s decomposition, however where we have that an upper triangular matrix with eigenvalues of on its diagonal entries, so is a lower triangular matrix thus . Therefore, is a diagonal matrix and its diagonal entries are real. I would like to thank whoever checks my proof. Best regards!","A n\times n A=QRQ^{*} A=A^{*}=(QRQ^{*})^{*}=QR^{*}Q^{*} R A R^{*} A-A^{*}=Q(R-R^{*})Q^{*}=0\implies R-R^{*}=0\implies r_{i,i}=\overline{r_{i,i}} R","['linear-algebra', 'matrices']"
62,Find positive definite $X$ such that $AXA^T=\alpha X$.,Find positive definite  such that .,X AXA^T=\alpha X,"Given an invertible real matrix $A\in\mathbb{R}^n$ , is it possible to obtain a (real) positive definite matrix $X$ (and scalar $\alpha>0$ ) such that $$ AXA^T=\alpha X? $$ Attempt : The previous equation can be written as $AXA^T-X +Q=0$ with $Q=(1-\alpha)X$ . So, if $(1-\alpha)>0$ this looks like the Lyapunov equation here . However, the analytic solution to the Lypunov equation has the form $$ X = \sum_{k=0}^\infty A^kQ(A^T)^k = (1-\alpha)\sum_{k=0}^\infty A^kX(A^T)^k $$ So, even if such equation have a solution, I don't have a clue on how to obtain it from here. Do you have other thoughts/suggestions?","Given an invertible real matrix , is it possible to obtain a (real) positive definite matrix (and scalar ) such that Attempt : The previous equation can be written as with . So, if this looks like the Lyapunov equation here . However, the analytic solution to the Lypunov equation has the form So, even if such equation have a solution, I don't have a clue on how to obtain it from here. Do you have other thoughts/suggestions?","A\in\mathbb{R}^n X \alpha>0 
AXA^T=\alpha X?
 AXA^T-X +Q=0 Q=(1-\alpha)X (1-\alpha)>0 
X = \sum_{k=0}^\infty A^kQ(A^T)^k = (1-\alpha)\sum_{k=0}^\infty A^kX(A^T)^k
","['linear-algebra', 'matrices', 'matrix-equations', 'positive-definite']"
63,How do I write this equation in einstein notation?,How do I write this equation in einstein notation?,,"I am a programmer who does quite a bit of maths. One of the operations in my code is the operation: C = np.einsum(""ij,kj->ki"",A,B) ( the documentation for his function ). The dimensions of the matricies (is it right to call these matricies?) are: A : m by n B : p by n C : m by p Ultimately my question is, how do I write the operation of C = np.einsum(""ij,kj->ki"",A,B) in a purely mathematical context? I have looked through a few examples, but I have had difficulty connecting these to what I am doing. (I apologise if the tags and/or the title are/is bad, please drop a comment and I'll correct it)","I am a programmer who does quite a bit of maths. One of the operations in my code is the operation: C = np.einsum(""ij,kj->ki"",A,B) ( the documentation for his function ). The dimensions of the matricies (is it right to call these matricies?) are: A : m by n B : p by n C : m by p Ultimately my question is, how do I write the operation of C = np.einsum(""ij,kj->ki"",A,B) in a purely mathematical context? I have looked through a few examples, but I have had difficulty connecting these to what I am doing. (I apologise if the tags and/or the title are/is bad, please drop a comment and I'll correct it)",,"['linear-algebra', 'matrices']"
64,Error bounds on the Expansion of Square Root of Matrix,Error bounds on the Expansion of Square Root of Matrix,,"Wikipedia gives the expansion of the square root matrix as follows: $$\frac{A^{1/2}}{\|A\|^{1/2}} = I - \sum_{n = 1}^{\infty}\left\lvert {1/2 \choose n}\right\rvert  \left(I-\frac{A}{\|A\|}\right)^n = I - \frac 1 2\left(I-\frac{A}{\|A\|}\right) - \frac 1 8 \left(I-\frac{A}{\|A\|}\right)^2 \,...$$ In this post, I'm taking the matrix norm to be the max of the absolute values of the column-sums of matrix $A$ . I thought I might be able to get an estimate of square root matrix by taking the first two terms and using the third term as an error bound. But a simple numerical example appears to not work. I'm not sure if I'm using the expansion incorrectly, or there's some hypothesis I'm missing. In particular for $A = \begin{pmatrix} 4 & 0 \\ 0 & 16  \end{pmatrix}, \|A\| = 16$ I get an error: $$\|A^{1/2}/\|A\|^{1/2} - I + (I-A/\|A\|)/2 \| = 1/8$$ Error bound using the third term: $$\|(I-A/\|A\|)^2/8 \| = 9/128$$","Wikipedia gives the expansion of the square root matrix as follows: In this post, I'm taking the matrix norm to be the max of the absolute values of the column-sums of matrix . I thought I might be able to get an estimate of square root matrix by taking the first two terms and using the third term as an error bound. But a simple numerical example appears to not work. I'm not sure if I'm using the expansion incorrectly, or there's some hypothesis I'm missing. In particular for I get an error: Error bound using the third term:","\frac{A^{1/2}}{\|A\|^{1/2}} = I - \sum_{n = 1}^{\infty}\left\lvert {1/2 \choose n}\right\rvert  \left(I-\frac{A}{\|A\|}\right)^n = I - \frac 1 2\left(I-\frac{A}{\|A\|}\right) - \frac 1 8 \left(I-\frac{A}{\|A\|}\right)^2 \,... A A = \begin{pmatrix}
4 & 0 \\
0 & 16 
\end{pmatrix}, \|A\| = 16 \|A^{1/2}/\|A\|^{1/2} - I + (I-A/\|A\|)/2 \| = 1/8 \|(I-A/\|A\|)^2/8 \| = 9/128","['linear-algebra', 'matrices', 'functional-analysis', 'operator-theory']"
65,"Looking for the Kernel, Basis, Range, and Dimension","Looking for the Kernel, Basis, Range, and Dimension",,"$$ L: \mathbb{R}^4 → \mathbb{R}^4  $$ defined by $$ L \begin{pmatrix} \begin{bmatrix} x\\y\\z\\w \end{bmatrix} \end{pmatrix} = \begin{bmatrix} 1&2&1&3\\ 2&1&-1&2\\ 1&0&0&-1\\ 4&1&-1&0 \end{bmatrix} \begin{bmatrix} x\\y\\z\\w \end{bmatrix} $$ I'm interested in looking for its kernel Ker(L) , find a basis for Ker(L) and the dimension of Ker(L) . Same as for its range, range(L) and its dimension range(L) . I'm not really confident with my answer so I'm asking so that I could verify if my solution is correct. What I did is equate the matrix above to a zero matrix. $$ \begin{bmatrix} 1&2&1&3\\ 2&1&-1&2\\ 1&0&0&-1\\ 4&1&-1&0 \end{bmatrix} \begin{bmatrix} x\\y\\z\\w \end{bmatrix} = \begin{bmatrix} 0\\0\\0\\0 \end{bmatrix} \Rightarrow \left[ \begin{array}{cccc|c}   1&2&1&3&0\\ 2&1&-1&2&0\\ 1&0&0&-1&0\\ 4&1&-1&0&0 \end{array} \right] $$ Then I looked for its RREF and the result is $$ \left[ \begin{array}{cccc|c}   1&0&0&-1&0\\ 0&1&0&\frac{8}{3}&0\\ 0&0&1&\frac{-4}{3}&0\\ 0&0&0&0&0 \end{array} \right] $$ and decided the following: $$ \begin{bmatrix} x = r\\ y = \frac{8}{3}r\\ z = \frac{-4}{3}r\\ w = r \end{bmatrix} \Rightarrow r \begin{bmatrix} 1\\ \frac{8}{3}\\ \frac{-4}{3}\\ 1 \end{bmatrix} \Rightarrow \ker(L) \begin{bmatrix} 1\\ \frac{8}{3}\\ \frac{-4}{3}\\ 1 \end{bmatrix} $$ And from there I'm not sure if I did right, and I'm confused on where next I should go in finding the rest of the problem. I apologize if its a stupid question.","defined by I'm interested in looking for its kernel Ker(L) , find a basis for Ker(L) and the dimension of Ker(L) . Same as for its range, range(L) and its dimension range(L) . I'm not really confident with my answer so I'm asking so that I could verify if my solution is correct. What I did is equate the matrix above to a zero matrix. Then I looked for its RREF and the result is and decided the following: And from there I'm not sure if I did right, and I'm confused on where next I should go in finding the rest of the problem. I apologize if its a stupid question.","
L: \mathbb{R}^4 → \mathbb{R}^4 
 
L \begin{pmatrix}
\begin{bmatrix}
x\\y\\z\\w
\end{bmatrix}
\end{pmatrix}
=
\begin{bmatrix}
1&2&1&3\\
2&1&-1&2\\
1&0&0&-1\\
4&1&-1&0
\end{bmatrix}
\begin{bmatrix}
x\\y\\z\\w
\end{bmatrix}
 
\begin{bmatrix}
1&2&1&3\\
2&1&-1&2\\
1&0&0&-1\\
4&1&-1&0
\end{bmatrix}
\begin{bmatrix}
x\\y\\z\\w
\end{bmatrix}
=
\begin{bmatrix}
0\\0\\0\\0
\end{bmatrix}
\Rightarrow
\left[
\begin{array}{cccc|c}
  1&2&1&3&0\\
2&1&-1&2&0\\
1&0&0&-1&0\\
4&1&-1&0&0
\end{array}
\right]
 
\left[
\begin{array}{cccc|c}
  1&0&0&-1&0\\
0&1&0&\frac{8}{3}&0\\
0&0&1&\frac{-4}{3}&0\\
0&0&0&0&0
\end{array}
\right]
  \begin{bmatrix}
x = r\\
y = \frac{8}{3}r\\
z = \frac{-4}{3}r\\
w = r \end{bmatrix}
\Rightarrow r
\begin{bmatrix}
1\\
\frac{8}{3}\\
\frac{-4}{3}\\
1 \end{bmatrix}
\Rightarrow \ker(L)
\begin{bmatrix}
1\\
\frac{8}{3}\\
\frac{-4}{3}\\
1 \end{bmatrix}
","['matrices', 'matrix-equations']"
66,How to understand an example in Krattenthaler's “Advanced Determinant Calculus”?,How to understand an example in Krattenthaler's “Advanced Determinant Calculus”?,,"In C. Krattenthaler's 1999 paper entitled “Advanced Determinant Calculus”, an example is given on p. 8 to illustrate Lemma 3 (p. 7). I don't quite understand this example, and I'm trying to wrap my head around it. The author illustrates the aforementioned lemma by means of the computation of $$\det_{1 \leq i,j \leq n} \bigg{(} {a+b \choose a-i+j} \bigg{)} .$$ To make everything a bit more palpable, let's take $a=2, b=3,$ and $n=3$ . The first step of the 'recipe' described right above the example is to “take as many factors out of rows and/or columns of your determinant, so that all denominators are cleared”. In this case, we do so by multiplying the matrix with the product \begin{align*} \prod_{i=1}^{n} \frac{(a+b)!}{(a-i+n)!(b+i-1)!} &= \prod_{i=1}^{n} \frac{(2+3)!}{(2-i+3)(3+i-1)!} \\ &= \prod_{i=1}^{3} \frac{5!}{(5-i)!(2+i)!} \\ &= \frac{5!}{4!3!}\cdot\frac{5!}{3!4!}\cdot\frac{5!}{2!5!}. \end{align*} So far, so good. The trouble starts with what happens inside the new matrix that is multiplied by this product. I don't understand what happens inside this matrix, of which the determinant can be calculated. Upon trying to understand it, I assumed that every term from the product corresponds to the respective rows of the matrix. So $\frac{5!}{4!3!}$ belongs to the first row, $ \frac{5!}{3!4!}$ belongs to the second one, and $ \frac{5!}{2!5!}$ corresponds to the third and last one. In order for the equality to hold, we must divide the rows inside the new matrix by their respective terms in the product. According to my calculations (which may or may not be right, please correct me if the latter is the case), the matrix becomes: $$ A =  \begin{pmatrix} 4\cdot 3 & 3\cdot4 & 3\cdot 2 \\ 3\cdot2 & 3\cdot 4 & 4\cdot3 \\ 2\cdot1 & 2\cdot5 & 5\cdot4 \end{pmatrix} .$$ Now, I don't understand how this relates to the author's description of the new matrix $$B = \big{(} (a-i+n)(a-i+n-1) \dots (a-i+j+1) \cdot (b+i-j+1)(b+i-j+2) \dots (b+i-1) \big{)} ,$$ and how it arises after the product is introduced like we did above. In other words, my overall question would be: why is matrix $A$ equal to matrix $B$ ? Or perhaps: why are their determinants equal? I'm surprised this should be the case, because I computed the entries that correspond to the different terms in the product inside matrix $B$ and the term-wise multiplications of the resulting matrices don't seem to be equal to matrix $A$ . To illustrate, let's consider some terms. For term $(a-i+n)$ , we have the matrix $b_{a_{n}} := \begin{pmatrix} 4 & 4 & 4 \\ 3 & 3 & 3 \\ 2 & 2 & 2 \end{pmatrix}  $ . For term $(a - i + n -1)$ , there is the matrix $b_{a_{n-1}} := \begin{pmatrix} 3 & 3 & 3 \\ 2 & 2 & 2 \\ 1 & 1 & 1  \end{pmatrix} $ . For term $(a-i+j+1)$ , the corresponding matrix is $b_{a_{j+1}} := \begin{pmatrix} 3 & 4 & 5 \\ 2 & 3 & 4 \\ 1 & 2 & 3 \end{pmatrix} $ For term $(i+b-j+1)$ , we find the matrix $b_{b_{j-1}} :=\begin{pmatrix} 4 & 3 & 2 \\ 5 & 4 & 3 \\ 6 & 5 & 4 \end{pmatrix} $ Etc. It seems to me that the element-wise multiplication of these matrices does not equal matrix $B$ . Questions Have I made a mistake in my calculations somewhere? Or am I making other errors? If so, please let me know. Is the author indeed performing row-wise multiplications of the matrix in question? If so, are they multiplied by the terms that I indicated? Or do different rows correspond to other term-wise multiplications? Why does the element-wise multiplication of the matrices $b_{a_{n}} \dots b_{b_{1}} $ not equal matrix $A$ according to my computations? How -- if at all -- should I interpret the products inside matrix $B$ differently? Could you please explain why matrix $\det(A)$ is equal to matrix $\det(B)$ ? Or why are their determinants equal? Both in the general case, and in the specific case of my toy example?","In C. Krattenthaler's 1999 paper entitled “Advanced Determinant Calculus”, an example is given on p. 8 to illustrate Lemma 3 (p. 7). I don't quite understand this example, and I'm trying to wrap my head around it. The author illustrates the aforementioned lemma by means of the computation of To make everything a bit more palpable, let's take and . The first step of the 'recipe' described right above the example is to “take as many factors out of rows and/or columns of your determinant, so that all denominators are cleared”. In this case, we do so by multiplying the matrix with the product So far, so good. The trouble starts with what happens inside the new matrix that is multiplied by this product. I don't understand what happens inside this matrix, of which the determinant can be calculated. Upon trying to understand it, I assumed that every term from the product corresponds to the respective rows of the matrix. So belongs to the first row, belongs to the second one, and corresponds to the third and last one. In order for the equality to hold, we must divide the rows inside the new matrix by their respective terms in the product. According to my calculations (which may or may not be right, please correct me if the latter is the case), the matrix becomes: Now, I don't understand how this relates to the author's description of the new matrix and how it arises after the product is introduced like we did above. In other words, my overall question would be: why is matrix equal to matrix ? Or perhaps: why are their determinants equal? I'm surprised this should be the case, because I computed the entries that correspond to the different terms in the product inside matrix and the term-wise multiplications of the resulting matrices don't seem to be equal to matrix . To illustrate, let's consider some terms. For term , we have the matrix . For term , there is the matrix . For term , the corresponding matrix is For term , we find the matrix Etc. It seems to me that the element-wise multiplication of these matrices does not equal matrix . Questions Have I made a mistake in my calculations somewhere? Or am I making other errors? If so, please let me know. Is the author indeed performing row-wise multiplications of the matrix in question? If so, are they multiplied by the terms that I indicated? Or do different rows correspond to other term-wise multiplications? Why does the element-wise multiplication of the matrices not equal matrix according to my computations? How -- if at all -- should I interpret the products inside matrix differently? Could you please explain why matrix is equal to matrix ? Or why are their determinants equal? Both in the general case, and in the specific case of my toy example?","\det_{1 \leq i,j \leq n} \bigg{(} {a+b \choose a-i+j} \bigg{)} . a=2, b=3, n=3 \begin{align*} \prod_{i=1}^{n} \frac{(a+b)!}{(a-i+n)!(b+i-1)!} &= \prod_{i=1}^{n} \frac{(2+3)!}{(2-i+3)(3+i-1)!} \\
&= \prod_{i=1}^{3} \frac{5!}{(5-i)!(2+i)!} \\
&= \frac{5!}{4!3!}\cdot\frac{5!}{3!4!}\cdot\frac{5!}{2!5!}. \end{align*} \frac{5!}{4!3!}  \frac{5!}{3!4!}  \frac{5!}{2!5!}  A = 
\begin{pmatrix} 4\cdot 3 & 3\cdot4 & 3\cdot 2 \\
3\cdot2 & 3\cdot 4 & 4\cdot3 \\
2\cdot1 & 2\cdot5 & 5\cdot4 \end{pmatrix} . B = \big{(} (a-i+n)(a-i+n-1) \dots (a-i+j+1) \cdot (b+i-j+1)(b+i-j+2) \dots (b+i-1) \big{)} , A B B A (a-i+n) b_{a_{n}} := \begin{pmatrix} 4 & 4 & 4 \\
3 & 3 & 3 \\
2 & 2 & 2 \end{pmatrix}   (a - i + n -1) b_{a_{n-1}} := \begin{pmatrix} 3 & 3 & 3 \\
2 & 2 & 2 \\
1 & 1 & 1  \end{pmatrix}  (a-i+j+1) b_{a_{j+1}} := \begin{pmatrix} 3 & 4 & 5 \\
2 & 3 & 4 \\
1 & 2 & 3 \end{pmatrix}  (i+b-j+1) b_{b_{j-1}} :=\begin{pmatrix} 4 & 3 & 2 \\
5 & 4 & 3 \\
6 & 5 & 4 \end{pmatrix}  B b_{a_{n}} \dots b_{b_{1}}  A B \det(A) \det(B)","['linear-algebra', 'matrices', 'proof-explanation', 'determinant']"
67,Condition of positive definiteness based upon diagonal elements of the original and inverse matrices,Condition of positive definiteness based upon diagonal elements of the original and inverse matrices,,This is a sequel to this question in which I sought to expand on this question . Let me put it straight. Given a non-singular symmetric real matrix $A\in\mathbb{R}^{n\times n}$ such that $A_{ii}>0$ . Can we conclude that $A$ is positive definite if $$(A^{-1})_{ii}\ge \frac1{A_{ii}}$$ holds for all $1\le i\le n$ ?,This is a sequel to this question in which I sought to expand on this question . Let me put it straight. Given a non-singular symmetric real matrix such that . Can we conclude that is positive definite if holds for all ?,A\in\mathbb{R}^{n\times n} A_{ii}>0 A (A^{-1})_{ii}\ge \frac1{A_{ii}} 1\le i\le n,"['linear-algebra', 'matrices', 'reference-request', 'inverse', 'positive-definite']"
68,Is the product of graph Laplacians positive semidefinite?,Is the product of graph Laplacians positive semidefinite?,,"For any weighted directed graph $G = (V,E,w)$ we can define the weighted Laplacian matrix $L=D-A$ , where $A=[a_{ij}]$ is the adjacency matrix and $D=\text{diag}( d_1, ..., d_n )$ is the in-degree matrix with $d_i = \sum_j a_{ij}$ . However, $L$ is not in general positive semi-definite. If we include the out-degree through the out-Laplacian matrix $L^o = D^o - A^\top$ , where $D^o=\text{diag}( d_1^o, ..., d_n^o )$ is the out-degree matrix with $d_i^o = \sum_j a_{ji}$ , the quadratic form $x^\top ( L + L^o )x$ is positive semidefinite since, $$ x^\top ( L + L^o )x = \sum_i\sum_j a_{ij}(x_i-x_j)^2 \geq 0.$$ If we define $Q=L+L^o$ , I want to proof if $(Qx)^\top(-Lx) \leq 0$ . This is equivalent to \begin{align*} (Qx)^\top(-Lx) \leq 0 &\iff -x^\top QLx \leq 0 \\ &\iff x^\top QLx \geq 0 \\ &\iff x^\top ( QL + L^\top Q )x \geq 0 \\ &\iff QL + L^\top Q \succeq 0 \end{align*} Some properties that I have found $Q=Q^\top$ is positive semidefinite. By Gershgorin's Theorem we know that all the eigenvalues of $L$ have positive real part. Both, $L$ and $Q$ are diagonally dominant. $QL + L^\top Q \succeq 0$ is very similar to Lyapunov equation. This can be useful since $-L$ is a Hurwitz matrix and $Q\succeq 0 $ .","For any weighted directed graph we can define the weighted Laplacian matrix , where is the adjacency matrix and is the in-degree matrix with . However, is not in general positive semi-definite. If we include the out-degree through the out-Laplacian matrix , where is the out-degree matrix with , the quadratic form is positive semidefinite since, If we define , I want to proof if . This is equivalent to Some properties that I have found is positive semidefinite. By Gershgorin's Theorem we know that all the eigenvalues of have positive real part. Both, and are diagonally dominant. is very similar to Lyapunov equation. This can be useful since is a Hurwitz matrix and .","G = (V,E,w) L=D-A A=[a_{ij}] D=\text{diag}( d_1, ..., d_n ) d_i = \sum_j a_{ij} L L^o = D^o - A^\top D^o=\text{diag}( d_1^o, ..., d_n^o ) d_i^o = \sum_j a_{ji} x^\top ( L + L^o )x  x^\top ( L + L^o )x = \sum_i\sum_j a_{ij}(x_i-x_j)^2 \geq 0. Q=L+L^o (Qx)^\top(-Lx) \leq 0 \begin{align*}
(Qx)^\top(-Lx) \leq 0 &\iff -x^\top QLx \leq 0 \\
&\iff x^\top QLx \geq 0 \\
&\iff x^\top ( QL + L^\top Q )x \geq 0 \\
&\iff QL + L^\top Q \succeq 0
\end{align*} Q=Q^\top L L Q QL + L^\top Q \succeq 0 -L Q\succeq 0 ","['linear-algebra', 'matrices', 'graph-theory', 'positive-semidefinite', 'graph-laplacian']"
69,Prove that the identity matrix is the only matrix such that $IA = A$ for all $A.$,Prove that the identity matrix is the only matrix such that  for all,IA = A A.,How do I prove that $I$ is the only matrix such that $IA = A$ for all $A?$ I keep getting tripped up on the index notation. Thanks!,How do I prove that is the only matrix such that for all I keep getting tripped up on the index notation. Thanks!,I IA = A A?,"['matrices', 'proof-writing']"
70,"$AD$ has exactly one negative eigenvalue if $v^T A v > 0$ and $D = \mbox{diag}(-1,1,1)$",has exactly one negative eigenvalue if  and,"AD v^T A v > 0 D = \mbox{diag}(-1,1,1)","Let $A$ be $3 \times 3$ real matrix (which is not necessarily symmetric or diagonalizable) such that $v^T A v>0$ for every $v \in \mathbb R^3 - \{0\}$ . Show that $AD$ has exactly one negative eigenvalue, where $D = \mbox{diag}(-1,1,1)$ . I can prove that $AD$ has a negative eigenvalue. If $\det(A) \leq 0$ , then characteristic polynomial $f(t) = \det(tI-A)$ satisfies $f(0) \geq 0$ . Since $f$ is polynomial of degree $3$ and $$\lim_{t \to -\infty} f(t) = -\infty$$ we can find a eigenvalue $\lambda \leq 0$ of $A$ with eigenvector $v$ . Then $v^TAv=\lambda v^Tv \leq 0$ , contradiction. Therefore $\det(AD) = \det(A) \det(D)<0$ . let $g(t)$ be characteristic polynomial of $AD$ . Then $g(0) = - \det(AD)>0$ so same argument produce a result. However, I cannot solve uniqueness part. How to solve it?","Let be real matrix (which is not necessarily symmetric or diagonalizable) such that for every . Show that has exactly one negative eigenvalue, where . I can prove that has a negative eigenvalue. If , then characteristic polynomial satisfies . Since is polynomial of degree and we can find a eigenvalue of with eigenvector . Then , contradiction. Therefore . let be characteristic polynomial of . Then so same argument produce a result. However, I cannot solve uniqueness part. How to solve it?","A 3 \times 3 v^T A v>0 v \in \mathbb R^3 - \{0\} AD D = \mbox{diag}(-1,1,1) AD \det(A) \leq 0 f(t) = \det(tI-A) f(0) \geq 0 f 3 \lim_{t \to -\infty} f(t) = -\infty \lambda \leq 0 A v v^TAv=\lambda v^Tv \leq 0 \det(AD) = \det(A) \det(D)<0 g(t) AD g(0) = - \det(AD)>0","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
71,Is $(A+B)$ necessarily singular?,Is  necessarily singular?,(A+B),"Let $A, B$ be two orthogonal matrices over a field $F$ of characteristic $2$ such that $$\det (A) + \det (B) = 0.$$ Is $(A+B)$ necessarily a singular matrix? I have proved the result to be true for real matrices anf the result also holds for complex matrices. I even proved the result over any field of characteristic $\neq 2.$ Can it hold for matrices over a field of characteristic $2$ ? I am asking this question because at the fag end of the proof of this result for real matrices I got a relation $2 \det (A + B) = 0,$ since $2 \neq 0$ over $\Bbb R$ we have the required result. But for any field $F$ of characteristic $2$ we have $2 = 0$ and hence we can't say whether or not $\det (A+B) = 0$ so that $(A+B)$ is a singular matrix. Any help or suggestion in this regard will be highly appreciated. Thanks in advance.",Let be two orthogonal matrices over a field of characteristic such that Is necessarily a singular matrix? I have proved the result to be true for real matrices anf the result also holds for complex matrices. I even proved the result over any field of characteristic Can it hold for matrices over a field of characteristic ? I am asking this question because at the fag end of the proof of this result for real matrices I got a relation since over we have the required result. But for any field of characteristic we have and hence we can't say whether or not so that is a singular matrix. Any help or suggestion in this regard will be highly appreciated. Thanks in advance.,"A, B F 2 \det (A) + \det (B) = 0. (A+B) \neq 2. 2 2 \det (A + B) = 0, 2 \neq 0 \Bbb R F 2 2 = 0 \det (A+B) = 0 (A+B)","['matrices', 'finite-fields', 'examples-counterexamples']"
72,Is there a matrix $C$ that can make $AB$ return the same result as $BA$ where $C$ is based on $B$?,Is there a matrix  that can make  return the same result as  where  is based on ?,C AB BA C B,"I am given an arbitrary matrix $A$ that I will be multiplying by a rotational matrix $B$ ( both $4\times4$ ) Is there any matrix $C$ , based only on manipulation of matrix $B$ , that when doing $A(BC)$ will produce the same result as $BA$ ? $BA = A(BC)$ I am trying to find some sort of abstract solution for making matrix multiplication commutative If it can not only be based on matrix $B$ , is there any way to find that matrix $C$ if using matrix $A$ as well? To clarify, let's define a matrix $D = BC$ . Is there a method to obtain a matrix $C$ such that $BA = AD$","I am given an arbitrary matrix that I will be multiplying by a rotational matrix ( both ) Is there any matrix , based only on manipulation of matrix , that when doing will produce the same result as ? I am trying to find some sort of abstract solution for making matrix multiplication commutative If it can not only be based on matrix , is there any way to find that matrix if using matrix as well? To clarify, let's define a matrix . Is there a method to obtain a matrix such that",A B 4\times4 C B A(BC) BA BA = A(BC) B C A D = BC C BA = AD,['matrices']
73,Can Cramer's Rule really distinguish between infinite no. of solutions and no solution?,Can Cramer's Rule really distinguish between infinite no. of solutions and no solution?,,"This is a question which was asked in a high-school exam held in India( JEE ADVANCED ). Going by Cramer's rule, for infinite solutions, I should get $D=D_1=D_2=D_3=0$ (where $D$ is the original determinant and $D_1, D_2, D_3$ are the respective coefficient determinants). Using these, I arrive at $\alpha^2$ =1, so that $\alpha$ =1,-1. But, $\alpha=1$ yields no solution here (if I write down the system of equations using $\alpha=1$ ). Why does it happen so? Is this a rare failure of Cramer's rule? How should we explain this unexpected result?","This is a question which was asked in a high-school exam held in India( JEE ADVANCED ). Going by Cramer's rule, for infinite solutions, I should get (where is the original determinant and are the respective coefficient determinants). Using these, I arrive at =1, so that =1,-1. But, yields no solution here (if I write down the system of equations using ). Why does it happen so? Is this a rare failure of Cramer's rule? How should we explain this unexpected result?","D=D_1=D_2=D_3=0 D D_1, D_2, D_3 \alpha^2 \alpha \alpha=1 \alpha=1","['matrices', 'contest-math', 'determinant']"
74,"Confused about the relation between linear transformations, matrices and basis vectors","Confused about the relation between linear transformations, matrices and basis vectors",,"I was watching 3blue1brown's video series on linear algebra. My understanding till now is :- A linear transformation takes in a vector and outputs another vector. The above statement is equivalent to multiplying a unique matrix to the given vector. 3b1b shows the linear transformation using a new coordinate system, and shows that $\hat{i}$ and $\hat{j}$ change. When he discusses change of basis, he states that it helps us move between different coordinate systems. 3b1b also states that a matrix implicitly assumes coordinate systems, as it represents the landing spots of basis vectors after linear transformation. He shows how to transform a rotation matrix in a conventional Cartesian coordinate system, to Jennifer's coordinate system (one where basis vectors are not perpendicular to one another). Points 4,5 and 6 have really confused me and now I doubt even points 1,2 and 3. When we write a matrix what basis vectors does it assume? I have never seen any text stating that this assumes a Cartesian coordinate system. I always assumed that it is somehow independent of coordinate systems. My second question: I thought that a linear transformation doing a 90° counter-clockwise rotation is represented by a unique matrix \begin{equation*} A =  \begin{pmatrix} 0 & -1  \\ 1 & 0 \end{pmatrix}, \end{equation*} but, as was shown in the video for Jennifer's choice of basis vectors the same 90° counter-clockwise rotation linear transformation is in fact \begin{equation*} B =  \begin{pmatrix} 1/3 & -2/3  \\ 5/3 & -1/3 \end{pmatrix}. \end{equation*} It seems like a linear transformation has a one-one mapping to a unique matrix only for a given set of basis vectors. Thus, the same matrix can refer to different linear transformations if we choose a different basis vector. In case, I am correct, could you provide a mathematically rigorous way of writing this down (using math symbols). I feel that I understand concepts better if I can write it in a mathematical form, instead of relying solely on intuition.","I was watching 3blue1brown's video series on linear algebra. My understanding till now is :- A linear transformation takes in a vector and outputs another vector. The above statement is equivalent to multiplying a unique matrix to the given vector. 3b1b shows the linear transformation using a new coordinate system, and shows that and change. When he discusses change of basis, he states that it helps us move between different coordinate systems. 3b1b also states that a matrix implicitly assumes coordinate systems, as it represents the landing spots of basis vectors after linear transformation. He shows how to transform a rotation matrix in a conventional Cartesian coordinate system, to Jennifer's coordinate system (one where basis vectors are not perpendicular to one another). Points 4,5 and 6 have really confused me and now I doubt even points 1,2 and 3. When we write a matrix what basis vectors does it assume? I have never seen any text stating that this assumes a Cartesian coordinate system. I always assumed that it is somehow independent of coordinate systems. My second question: I thought that a linear transformation doing a 90° counter-clockwise rotation is represented by a unique matrix but, as was shown in the video for Jennifer's choice of basis vectors the same 90° counter-clockwise rotation linear transformation is in fact It seems like a linear transformation has a one-one mapping to a unique matrix only for a given set of basis vectors. Thus, the same matrix can refer to different linear transformations if we choose a different basis vector. In case, I am correct, could you provide a mathematically rigorous way of writing this down (using math symbols). I feel that I understand concepts better if I can write it in a mathematical form, instead of relying solely on intuition.","\hat{i} \hat{j} \begin{equation*}
A = 
\begin{pmatrix}
0 & -1  \\
1 & 0
\end{pmatrix},
\end{equation*} \begin{equation*}
B = 
\begin{pmatrix}
1/3 & -2/3  \\
5/3 & -1/3
\end{pmatrix}.
\end{equation*}","['linear-algebra', 'matrices', 'linear-transformations', 'coordinate-systems']"
75,Spectral norm of projected matrix,Spectral norm of projected matrix,,"Let $M_{n,m}$ be the set of real matrices of $n\times m$ , and let $T:M_{n,m}\to M_{n,m}$ be a orthogonal projection operator, i.e., $T$ is such that for any $A,B\in M_{n,m}$ $$T(A+B)=T(A)+T(B),$$ $$T(T(A))=T(A),$$ $$\langle T(A),B\rangle = \langle A,T(B)\rangle.$$ where $\langle A,B\rangle=tr(A^{\top}B)$ . For $A\in M_{n,m}$ let $\|A\|$ be its spectral norm and $\|A\|_F$ its Frobenius norm. I want to prove that $$ \|T(A)\|\leq \|A\|. $$ I've been able to prove that $\|T(A)\|_F\leq \|A\|_F$ which is immediate since $\langle T(A),(I-T)(A)\rangle=0$ which implies that $\|A\|_F=\|T(A)+(I-T)(A)\|_F=\|T(A)\|_F+\|(I-T)(A)\|_F$ . For the spectral norm maybe I can use that $\|A\|=\sup_{\|x\|_2=1}\|Ax\|_2$ but I can't prove that $x^{\top}T(A)^{\top}(I-T)(A)x=0$ . Any help will be appreciated. Edit : If it helps, the form of $T$ is $T(A)=A-P_1AP_2$ , where $P_1$ and $P_2$ are some $n\times n$ and $m\times m$ projection matrices. This question is motivated by the paper https://arxiv.org/pdf/1011.6256.pdf , proof of Theorem 1, page 9, inequality after equation (2.15).","Let be the set of real matrices of , and let be a orthogonal projection operator, i.e., is such that for any where . For let be its spectral norm and its Frobenius norm. I want to prove that I've been able to prove that which is immediate since which implies that . For the spectral norm maybe I can use that but I can't prove that . Any help will be appreciated. Edit : If it helps, the form of is , where and are some and projection matrices. This question is motivated by the paper https://arxiv.org/pdf/1011.6256.pdf , proof of Theorem 1, page 9, inequality after equation (2.15).","M_{n,m} n\times m T:M_{n,m}\to M_{n,m} T A,B\in M_{n,m} T(A+B)=T(A)+T(B), T(T(A))=T(A), \langle T(A),B\rangle = \langle A,T(B)\rangle. \langle A,B\rangle=tr(A^{\top}B) A\in M_{n,m} \|A\| \|A\|_F 
\|T(A)\|\leq \|A\|.
 \|T(A)\|_F\leq \|A\|_F \langle T(A),(I-T)(A)\rangle=0 \|A\|_F=\|T(A)+(I-T)(A)\|_F=\|T(A)\|_F+\|(I-T)(A)\|_F \|A\|=\sup_{\|x\|_2=1}\|Ax\|_2 x^{\top}T(A)^{\top}(I-T)(A)x=0 T T(A)=A-P_1AP_2 P_1 P_2 n\times n m\times m","['linear-algebra', 'matrices', 'normed-spaces', 'orthogonality', 'matrix-analysis']"
76,Solution of equation system Ax=12x,Solution of equation system Ax=12x,,The question is formulated as follows. Find all solutions in $x$ = $\begin{bmatrix}x_1\\x_2\\x_3\\\end{bmatrix}$ $\in R^3$ of the equation system $Ax=12x$ $A$ = $\begin{bmatrix}6 & 4 & 3\\6 & 0 & 9\\0 & 8 & 0\\\end{bmatrix}$ and $\sum\limits_{i=1}^3 x_i = 1$ . I have started with finding all of the vectors of $A$ in the nullspace of $Ax=12x$ which means solving for $A-12I=0$ and converting the resultant matrix into a reduced row-echelon form which is given as follows. $rref(A)$ = $\begin{bmatrix}1 & 0 & -3/2\\0 & 1 & 3/2\\0 & 0 & 0\\\end{bmatrix}$ Now this says that $x_1=(3/2) x_3$ and $x_2= -(3/2) x_3$ but this does not satisfy the summation constraint what am i doing wrong here?.,The question is formulated as follows. Find all solutions in = of the equation system = and . I have started with finding all of the vectors of in the nullspace of which means solving for and converting the resultant matrix into a reduced row-echelon form which is given as follows. = Now this says that and but this does not satisfy the summation constraint what am i doing wrong here?.,x \begin{bmatrix}x_1\\x_2\\x_3\\\end{bmatrix} \in R^3 Ax=12x A \begin{bmatrix}6 & 4 & 3\\6 & 0 & 9\\0 & 8 & 0\\\end{bmatrix} \sum\limits_{i=1}^3 x_i = 1 A Ax=12x A-12I=0 rref(A) \begin{bmatrix}1 & 0 & -3/2\\0 & 1 & 3/2\\0 & 0 & 0\\\end{bmatrix} x_1=(3/2) x_3 x_2= -(3/2) x_3,"['linear-algebra', 'matrices']"
77,How to show that $AB=BA$ if $(A-3I)(B-3I)=2B+9I$,How to show that  if,AB=BA (A-3I)(B-3I)=2B+9I,"How to show $AB=BA$ if: $$(A-3I)(B-3I)=2B+9I$$ where $A,B\in M_n(\mathbb{R})$ I've been doing lots of arithmetical transformations but cannot see the way...",How to show if: where I've been doing lots of arithmetical transformations but cannot see the way...,"AB=BA (A-3I)(B-3I)=2B+9I A,B\in M_n(\mathbb{R})","['linear-algebra', 'matrices']"
78,Lowest upper bound on matrix norm,Lowest upper bound on matrix norm,,"Let $A \in \mathbb{R}^{d \times d}$ be an invertible real matrix and $A'$ the matrix obtained from $A$ by setting all diagonal elements to $0$ , namely $$A'_{ij} = \begin{cases} A_{ij} & \text{if } i \neq j \\ 0 & \text{otherwise.} \end{cases}$$ I can prove that $\lVert A' \rVert_2 \leq \min(2, \sqrt{d})\lVert A \rVert_2$ where $\lVert \cdot \rVert_2$ is the $2$ -norm (operator norm), but I don't think the bound is tight. I ran $20$ million samples with entries of $A$ generated both uniformly across integers from $-10$ to $10$ and from a standard normal distribution, and got $$\lVert A' \rVert_2 \leq \begin{cases} \lVert A \rVert_2 & \text{for } d=2 \\ \approx 1.29 \lVert A \rVert_2 & \text{for } d=3 \\ \approx 1.339 \lVert A \rVert_2 & \text{for } d=4 \\ \approx 1.346 \lVert A \rVert_2 & \text{for } d=5 \\ \approx 1.28 \lVert A \rVert_2 & \text{for } d=6. \end{cases} $$ Any ideas on what the lowest upper bound would be as a function of $d$ ?","Let be an invertible real matrix and the matrix obtained from by setting all diagonal elements to , namely I can prove that where is the -norm (operator norm), but I don't think the bound is tight. I ran million samples with entries of generated both uniformly across integers from to and from a standard normal distribution, and got Any ideas on what the lowest upper bound would be as a function of ?","A \in \mathbb{R}^{d \times d} A' A 0 A'_{ij} = \begin{cases} A_{ij} & \text{if } i \neq j \\
0 & \text{otherwise.}
\end{cases} \lVert A' \rVert_2 \leq \min(2, \sqrt{d})\lVert A \rVert_2 \lVert \cdot \rVert_2 2 20 A -10 10 \lVert A' \rVert_2 \leq \begin{cases} \lVert A \rVert_2 & \text{for } d=2 \\
\approx 1.29 \lVert A \rVert_2 & \text{for } d=3 \\
\approx 1.339 \lVert A \rVert_2 & \text{for } d=4 \\
\approx 1.346 \lVert A \rVert_2 & \text{for } d=5 \\
\approx 1.28 \lVert A \rVert_2 & \text{for } d=6.
\end{cases}  d","['matrices', 'upper-lower-bounds', 'matrix-norms', 'spectral-norm']"
79,If A and B are $n \times n$ matrices where each column sums to p. Then for what values of p will the matrix AB also have all columns that sum to p?,If A and B are  matrices where each column sums to p. Then for what values of p will the matrix AB also have all columns that sum to p?,n \times n,"I have no idea how to approach this question. I've tried working through it with sum notation but it became jumbled. I assume there's another property of matrices that I can use to make this simpler? Since using the basic properties of matrix multiplication seems convoluted. Using generic 2x2 matrices, I was able to find that for p=0 and p=1 AB has columns that add to p. But I'm unsure on how to do this working for a generic nxn matrix.","I have no idea how to approach this question. I've tried working through it with sum notation but it became jumbled. I assume there's another property of matrices that I can use to make this simpler? Since using the basic properties of matrix multiplication seems convoluted. Using generic 2x2 matrices, I was able to find that for p=0 and p=1 AB has columns that add to p. But I'm unsure on how to do this working for a generic nxn matrix.",,"['linear-algebra', 'matrices']"
80,"Change of Coordinates and property of invertible matrices (Sec 2.4 Theorem 8, Hoffman Kunze, Linear Algebra)","Change of Coordinates and property of invertible matrices (Sec 2.4 Theorem 8, Hoffman Kunze, Linear Algebra)",,"I was reading Linear Algebra by Hoffman Kunze, and encountered this in the Theorem 8 of the Chapter Coordinates, the theorem is stated below : What I get about is in the most bottom line (uniqueness), it says $""... it\: is\: clear\: that $ $$ \alpha'_{i}=\sum_{i=1}^{n} P_{ij}\alpha_{i}."" \tag{a}$$ Is there any easy way to see why it is clear? I tried many ways and what I found was that we start from scratch, (starting the same with the proof, with $\scr \overline{B}$ and then we find an invertible matrix, say $Q$ , which we don't know equal to $P$ or not, such that the property (a) above holds with $Q_{ij}$ instead of $P_{ij}$ . Then now what we are left to show is that $P$ = $Q$ , and we currently have: $$ x_{i}=\sum_{j=1}^{n} P_{ij} x'_{j}\tag{from (i)}$$ and, $$ x_{i}=\sum_{j=1}^{n} Q_{ij} x'_{j}$$ So together, $$ \sum_{j=1}^{n} Q_{ij} x'_{j}=\sum_{j=1}^{n} P_{ij} x'_{j}$$ $$ \sum_{j=1}^{n} (Q_{ij} - P_{ij}) x'_{j}= 0$$ The way I showed that this implies that $P$ = $Q$ is by asserting that $P - Q \neq 0^{n\times n}$ and find a contradiction. Suppose $A := P - Q \neq 0^{n\times n}$ then we can choose a row $r$ such that the k-th entry is non-zero, we can plug in $0$ to any other entries other then the k-th and we are left with something non zero equals to $0$ which is a contradiction, so that $A = 0^{n\times n}$ and $P = Q$ , and since $(a)$ holds for $Q$ and $P = Q$ , it follows that $(a)$ holds for $P$ . I'm pretty sure that the proof is correct since we can plug in various values to $x'_{1}, ... , x'_{n} \in F$ , since $F$ is a field, it sure contains $0$ and $1$ , but this proof seems to be lengthy and is not as clear as how it was written to be by Hoffman and Kunze, I think I'm missing something here, and will be very thankful for a good explanation. Thanks!","I was reading Linear Algebra by Hoffman Kunze, and encountered this in the Theorem 8 of the Chapter Coordinates, the theorem is stated below : What I get about is in the most bottom line (uniqueness), it says Is there any easy way to see why it is clear? I tried many ways and what I found was that we start from scratch, (starting the same with the proof, with and then we find an invertible matrix, say , which we don't know equal to or not, such that the property (a) above holds with instead of . Then now what we are left to show is that = , and we currently have: and, So together, The way I showed that this implies that = is by asserting that and find a contradiction. Suppose then we can choose a row such that the k-th entry is non-zero, we can plug in to any other entries other then the k-th and we are left with something non zero equals to which is a contradiction, so that and , and since holds for and , it follows that holds for . I'm pretty sure that the proof is correct since we can plug in various values to , since is a field, it sure contains and , but this proof seems to be lengthy and is not as clear as how it was written to be by Hoffman and Kunze, I think I'm missing something here, and will be very thankful for a good explanation. Thanks!","""... it\: is\: clear\: that   \alpha'_{i}=\sum_{i=1}^{n} P_{ij}\alpha_{i}."" \tag{a} \scr \overline{B} Q P Q_{ij} P_{ij} P Q  x_{i}=\sum_{j=1}^{n} P_{ij} x'_{j}\tag{from (i)}  x_{i}=\sum_{j=1}^{n} Q_{ij} x'_{j}  \sum_{j=1}^{n} Q_{ij} x'_{j}=\sum_{j=1}^{n} P_{ij} x'_{j}  \sum_{j=1}^{n} (Q_{ij} - P_{ij}) x'_{j}= 0 P Q P - Q \neq 0^{n\times n} A := P - Q \neq 0^{n\times n} r 0 0 A = 0^{n\times n} P = Q (a) Q P = Q (a) P x'_{1}, ... , x'_{n} \in F F 0 1","['linear-algebra', 'matrices', 'change-of-basis']"
81,Finding the largest minimum,Finding the largest minimum,,"Let $A$ be the $5\times 5$ matrix $\begin{bmatrix}11& 17 & 25 & 19 & 16\\ 24& 10 & 13 & 15 &3 \\ 12& 5 & 14 & 2 & 18\\ 23 & 4 & 1 & 8 & 22\\ 6& 20 & 7 & 21 & 9\end{bmatrix}.$ Find the group of $5$ elements, one from each row and column, whose minimum is maximized and prove it so. I think this problem involves maximizing the choices picked per column. For the $2$ nd and $4$ th columns, we may only pick one of $20$ and $21.$ Similarly, for the $2$ and $4$ th rows, we may only pick one of $23$ and $24.$ For the first row, we can pick only one of $25, 16, 17, 19.$ how can i continue from here?","Let be the matrix Find the group of elements, one from each row and column, whose minimum is maximized and prove it so. I think this problem involves maximizing the choices picked per column. For the nd and th columns, we may only pick one of and Similarly, for the and th rows, we may only pick one of and For the first row, we can pick only one of how can i continue from here?","A 5\times 5 \begin{bmatrix}11& 17 & 25 & 19 & 16\\
24& 10 & 13 & 15 &3 \\
12& 5 & 14 & 2 & 18\\
23 & 4 & 1 & 8 & 22\\
6& 20 & 7 & 21 & 9\end{bmatrix}. 5 2 4 20 21. 2 4 23 24. 25, 16, 17, 19.",['matrices']
82,Can I apply elementary row operation then find eigenvalues of a matrix?,Can I apply elementary row operation then find eigenvalues of a matrix?,,Suppose if a matrix is given as $$ \begin{bmatrix}  4 & 6\\ 2 & 9  \end{bmatrix}$$ We have to find its eigenvalues and eigenvectors. Can we first apply elementary row operation . Then find eigenvalues. Is their any relation on the matrix if it is diagonalized or not.,Suppose if a matrix is given as We have to find its eigenvalues and eigenvectors. Can we first apply elementary row operation . Then find eigenvalues. Is their any relation on the matrix if it is diagonalized or not.," \begin{bmatrix} 
4 & 6\\
2 & 9 
\end{bmatrix}","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'gaussian-elimination']"
83,natural map $SL_2(\mathbb{Z})\to SL_2(\mathbb{Z}/n\mathbb{Z})$ is surjective,natural map  is surjective,SL_2(\mathbb{Z})\to SL_2(\mathbb{Z}/n\mathbb{Z}),"This is a problem in my past Qual exam ""Let $n\geq 2$ be an integer. Prove that the natural map $SL_2(\mathbb{Z})\to SL_2(\mathbb{Z}/n\mathbb{Z})$ is surjective."" I approach the problem naturally. Take a matrix $\begin{pmatrix} \bar{a}&\bar{b} \\\bar{c}&\bar{d} \end{pmatrix}\in SL_2(\mathbb{Z}/n\mathbb{Z})$ . Then the preimage must be $\begin{pmatrix} a+xn&b+yn\\c+zn&d+tn \end{pmatrix}$ . We need to prove there exists $x,y,z,t$ s.t. the determinant is $1$ , i.e. $$(xt-yz)n^2+(at+dx-cy-bz)n+(ad-bc)=1$$ Clealy I just complicate the problem, making it into finding 4 values that satisfy an equation. I do not how to proceed. Is my approach right? Other ways would be awesome, too.","This is a problem in my past Qual exam ""Let be an integer. Prove that the natural map is surjective."" I approach the problem naturally. Take a matrix . Then the preimage must be . We need to prove there exists s.t. the determinant is , i.e. Clealy I just complicate the problem, making it into finding 4 values that satisfy an equation. I do not how to proceed. Is my approach right? Other ways would be awesome, too.","n\geq 2 SL_2(\mathbb{Z})\to SL_2(\mathbb{Z}/n\mathbb{Z}) \begin{pmatrix} \bar{a}&\bar{b} \\\bar{c}&\bar{d} \end{pmatrix}\in SL_2(\mathbb{Z}/n\mathbb{Z}) \begin{pmatrix} a+xn&b+yn\\c+zn&d+tn \end{pmatrix} x,y,z,t 1 (xt-yz)n^2+(at+dx-cy-bz)n+(ad-bc)=1","['matrices', 'finite-groups', 'determinant', 'group-homomorphism', 'integral-equations']"
84,"Is the semigroup of lines $\mathcal{M}_n(\mathbb{Q})$, finitely generated","Is the semigroup of lines , finitely generated",\mathcal{M}_n(\mathbb{Q}),"I was researching $\mathcal{M}_n(\mathbb{Q})$ , the set of square $n\times n$ matrices with rational entries, as a semigroup with matrix multiplication. For $A,B\in\mathcal{M}_n(\mathbb{Q})$ , the equivalence relation $A\sim B$ will be true when $A=cB$ for $c\in\mathbb{Q}$ , non-zero. The quotient, $\mathcal{M}_n(\mathbb{Q})/\sim$ , is also a semigroup (looks like lines in $\mathbb{R}^{n^2}$ , when $\mathcal{M}_n(\mathbb{Q})$ is interpretted in Euclidean space). I was wondering if this quotient semigroup is finitely generated (the set if finitely generated and one only has to hit a point on each line to generate it). I tried to prove by contradiction with finite generators and trying to find a non-generated element but to no avail.","I was researching , the set of square matrices with rational entries, as a semigroup with matrix multiplication. For , the equivalence relation will be true when for , non-zero. The quotient, , is also a semigroup (looks like lines in , when is interpretted in Euclidean space). I was wondering if this quotient semigroup is finitely generated (the set if finitely generated and one only has to hit a point on each line to generate it). I tried to prove by contradiction with finite generators and trying to find a non-generated element but to no avail.","\mathcal{M}_n(\mathbb{Q}) n\times n A,B\in\mathcal{M}_n(\mathbb{Q}) A\sim B A=cB c\in\mathbb{Q} \mathcal{M}_n(\mathbb{Q})/\sim \mathbb{R}^{n^2} \mathcal{M}_n(\mathbb{Q})","['linear-algebra', 'abstract-algebra', 'matrices', 'semigroups', 'finitely-generated']"
85,"What does $A+B=I$ imply for positive matrices $A,B$?",What does  imply for positive matrices ?,"A+B=I A,B","Let $A,B$ be positive matrices on a finite-dimensional space, and suppose that $A+B=I$ . In the special case of $A,B$ being projectors, we know that this implies that they must be orthogonal, as shown for example here and links therein. Can something be said about the more general case of $A,B\ge0$ ? If $A,B$ have orthogonal support, it is not hard to see that they must each equal the identity on their supports. We can therefore, I think, restrict ourselves to consider only the cases in which $\operatorname{im}(A)=\operatorname{im}(B)$ , as we know that the restriction of the operators on every subspace in which only one of the two acts equals the identity (more precisely, one of the two operators will act like the identity and the other like $0$ ).","Let be positive matrices on a finite-dimensional space, and suppose that . In the special case of being projectors, we know that this implies that they must be orthogonal, as shown for example here and links therein. Can something be said about the more general case of ? If have orthogonal support, it is not hard to see that they must each equal the identity on their supports. We can therefore, I think, restrict ourselves to consider only the cases in which , as we know that the restriction of the operators on every subspace in which only one of the two acts equals the identity (more precisely, one of the two operators will act like the identity and the other like ).","A,B A+B=I A,B A,B\ge0 A,B \operatorname{im}(A)=\operatorname{im}(B) 0","['linear-algebra', 'matrices', 'operator-theory', 'positive-semidefinite']"
86,Positive definite when multiplying by two matrices,Positive definite when multiplying by two matrices,,"I have a square $n \times n$ matrix $\mathbf{X}$ and a non-square $n \times m$ matrix $\mathbf{A}$ . The product $\mathbf{M} = \mathbf{A}^T \mathbf{X} \mathbf{A}$ gives a $m \times m$ matrix. If I know that $\mathbf{X}$ is positive definite (or positive-semi definite), is there any way to know $\mathbf{M}$ is positive definite (or positive-semi definite). If it is not possible, is there any constraints we can make on $\mathbf{A}$ (or maybe $\mathbf{X}$ ) to have this property? Any pointers will be helpful. Thanks!","I have a square matrix and a non-square matrix . The product gives a matrix. If I know that is positive definite (or positive-semi definite), is there any way to know is positive definite (or positive-semi definite). If it is not possible, is there any constraints we can make on (or maybe ) to have this property? Any pointers will be helpful. Thanks!",n \times n \mathbf{X} n \times m \mathbf{A} \mathbf{M} = \mathbf{A}^T \mathbf{X} \mathbf{A} m \times m \mathbf{X} \mathbf{M} \mathbf{A} \mathbf{X},"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'positive-definite', 'positive-semidefinite']"
87,Prove that J is a subgroup of G,Prove that J is a subgroup of G,,"Could somebody please help me with the following question? Let $G$ be the group consisting of all invertible $2\times2$ matrices with coefficients in $\mathbb Z/3\mathbb Z$ . Here the group is the usual matrix multiplication, and the unit element is $e:=\begin{bmatrix}1&0\\0&1\end{bmatrix} $ Consider the matrices $g:=\begin{bmatrix}0&1\\1&1\end{bmatrix} $ $h:=\begin{bmatrix}1&1\\0&2\end{bmatrix} $ Show that $J:=\{h^n* g^m : m,n \in \mathbb{Z}_{>0} \}$ is a subgroup of $G$ . I have determined that the unit element is in $J$ , but I don't know how to show the other axioms of the subgroup criterion as the group is not commutative. Thank you!","Could somebody please help me with the following question? Let be the group consisting of all invertible matrices with coefficients in . Here the group is the usual matrix multiplication, and the unit element is Consider the matrices Show that is a subgroup of . I have determined that the unit element is in , but I don't know how to show the other axioms of the subgroup criterion as the group is not commutative. Thank you!","G 2\times2 \mathbb Z/3\mathbb Z e:=\begin{bmatrix}1&0\\0&1\end{bmatrix}  g:=\begin{bmatrix}0&1\\1&1\end{bmatrix}  h:=\begin{bmatrix}1&1\\0&2\end{bmatrix}  J:=\{h^n* g^m : m,n \in \mathbb{Z}_{>0} \} G J","['abstract-algebra', 'matrices', 'group-theory', 'finite-groups', 'modular-arithmetic']"
88,Hard Inverse Matrix calculation,Hard Inverse Matrix calculation,,I tried so hard but I am unable to solve this problem. Find the inverse of the matrix $I+ab^T$ . Hint : Try the form $cI+dab^T$ and find $c$ and $d$ .   What happens if $a^T b = −1$ ? This is an exercise from A Primer  on  Linear models by Monahan ( Appendix A.69),I tried so hard but I am unable to solve this problem. Find the inverse of the matrix . Hint : Try the form and find and .   What happens if ? This is an exercise from A Primer  on  Linear models by Monahan ( Appendix A.69),I+ab^T cI+dab^T c d a^T b = −1,"['matrices', 'self-learning', 'inverse', 'matrix-decomposition']"
89,Determining multiplicity of 1 as an eigenvalue for a certain matrix,Determining multiplicity of 1 as an eigenvalue for a certain matrix,,"By Matlab, I know that the eigenvalues of the matrix $B^{-1}A$ are 2.457, 0.542, and 1 (multiplicity 3) where $A$ and $B$ are defined as: \begin{equation} A= \begin{pmatrix} 1 & 0 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 & 0 \\ 0 & 0 & 2 & 1 & 1 \\ 0 & 0 & 1 & 2 & 1 \\ 0 & 0 & 1 & 1 & 2 \\ \end{pmatrix}, B= \begin{pmatrix} 1 & 0 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 & 0 \\ 0 & 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 2 & 1 \\ 0 & 0 & 0 & 1 & 2 \\ \end{pmatrix} \end{equation} Similarly, the eigenvalues of the matrix $B^{-1}A$ are 4.56, 0.43, and 1 (multiplicity 4) where $A$ and $B$ are defined as: \begin{equation} A= \begin{pmatrix} 1 & 0 & 0 & 0 & 0 & 0\\ 0 & 2 & 1 & 1 & 1 & 1 \\ 0 & 1 & 2 & 1 & 1 & 1 \\ 0 & 1 & 1 & 2 & 1 & 1\\ 0 & 1 & 1 & 1 & 2 & 1\\ 0 & 1 & 1 & 1 & 1 & 2\\ \end{pmatrix}, B= \begin{pmatrix} 1 & 0 & 0 & 0 & 0 & 0\\ 0 & 1 & 0 & 0 & 0 & 0\\ 0 & 0 & 1 & 0 & 0 & 0\\ 0 & 0 & 0 & 1 & 0 & 0\\ 0 & 0 & 0 & 0 & 2 & 1\\ 0 & 0 & 0 & 0 & 1 & 2\\ \end{pmatrix} \end{equation} In general, given $n$ , the matrices are defined as follows: \begin{equation}  A = \begin{pmatrix} I_{n-m_A} & 0 \\ 0 & I_{m_A} + J_{m_A} \\ \end{pmatrix}, B = \begin{pmatrix} I_{n-m_B} & 0 \\ 0 & I_{m_B} + J_{m_B} \\ \end{pmatrix}, \end{equation} where $m_A \ne m_B$ and they can be $1,...,n-1$ (so it can be that $m_A < m_B$ ). $J_m$ is a $m \times m$ matrix of ones.  Is there any explanation to why the multiplicity of 1 as an eigenvalue is always $n-2$ where $n$ is the dimension of the matrices?","By Matlab, I know that the eigenvalues of the matrix are 2.457, 0.542, and 1 (multiplicity 3) where and are defined as: Similarly, the eigenvalues of the matrix are 4.56, 0.43, and 1 (multiplicity 4) where and are defined as: In general, given , the matrices are defined as follows: where and they can be (so it can be that ). is a matrix of ones.  Is there any explanation to why the multiplicity of 1 as an eigenvalue is always where is the dimension of the matrices?","B^{-1}A A B \begin{equation}
A=
\begin{pmatrix}
1 & 0 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 \\
0 & 0 & 2 & 1 & 1 \\
0 & 0 & 1 & 2 & 1 \\
0 & 0 & 1 & 1 & 2 \\
\end{pmatrix},
B=
\begin{pmatrix}
1 & 0 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 2 & 1 \\
0 & 0 & 0 & 1 & 2 \\
\end{pmatrix}
\end{equation} B^{-1}A A B \begin{equation}
A=
\begin{pmatrix}
1 & 0 & 0 & 0 & 0 & 0\\
0 & 2 & 1 & 1 & 1 & 1 \\
0 & 1 & 2 & 1 & 1 & 1 \\
0 & 1 & 1 & 2 & 1 & 1\\
0 & 1 & 1 & 1 & 2 & 1\\
0 & 1 & 1 & 1 & 1 & 2\\
\end{pmatrix},
B=
\begin{pmatrix}
1 & 0 & 0 & 0 & 0 & 0\\
0 & 1 & 0 & 0 & 0 & 0\\
0 & 0 & 1 & 0 & 0 & 0\\
0 & 0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & 0 & 2 & 1\\
0 & 0 & 0 & 0 & 1 & 2\\
\end{pmatrix}
\end{equation} n \begin{equation} 
A =
\begin{pmatrix}
I_{n-m_A} & 0 \\
0 & I_{m_A} + J_{m_A} \\
\end{pmatrix},
B =
\begin{pmatrix}
I_{n-m_B} & 0 \\
0 & I_{m_B} + J_{m_B} \\
\end{pmatrix},
\end{equation} m_A \ne m_B 1,...,n-1 m_A < m_B J_m m \times m n-2 n","['matrices', 'eigenvalues-eigenvectors']"
90,"Noncommutative rings, matrices and homomorphisms of free modules","Noncommutative rings, matrices and homomorphisms of free modules",,"In Blyth's book ""Module Theory: An Approach to Linear Algebra"" matrix theory is developed generally over a noncommutative ring $R$ with $1$ . However, it seems that there is a mistake in way that an important property doesn't carry over to noncommutative rings, namely the isomorphic of a ring of $n\times n$ matrices with coefficients in $R$ and the endomorphism ring of a free module over $R$ Given an a homomorphism $\phi\colon M\to N$ of free $R$ -modules with respective bases $(a_i)_m$ and $(b_i)_n$ , the matrix $\mathrm{Mat}(\phi,(b_i)_n,(a_i)_m)$ of this homomorphism with respect to said bases is a matrix $(r_{ij})$ such that $r_{ij}$ is the unique element of $R$ so that $\phi(a_i) = s_{1i}b_1 + ... + r_{ij}b_j + ... + s_{ni}$ . One can prove that, for the respective bases, there is an isomorphism $\vartheta\colon\mathrm{Hom}_R(M,N)\to\mathrm{Mat}_{n\times n}(R), \phi \mapsto \mathrm{Mat}(\phi,(b_i)_n,(a_i)_m)$ . But am I right to assume that this is not a ring homomorphism? It seems modules over noncommutative rings lack the multiplicative property $$\mathrm{Mat}(\psi\circ\phi, (c_i)_p,(a_i)_n) = \mathrm{Mat}(\psi,(c_i)_p,(b_i)_m)\mathrm{Mat}(\phi,(b_i)_m,(a_i)_n)$$ for free modules $M,N,P$ with respective bases $(a_i)_n, (b_i)_m$ and $(c_i)_p$ and their homomorphisms $\phi\colon M\to N, \psi\colon N\to P$ . Am I right or there is a mistake there? I was doing all the matrix theory for commutative rings before now, but encountered a proof in the aforementioned book which uses (probably wrong) ring isomorphism. Let $\mathrm{Mat}(\phi,(b_i)_m,(a_i)_n) = (r_{ij})$ and $\mathrm{Mat}(\psi,(c_i)_p, (b_i)_m) = (s_{ij})$ . Then we have $\mathrm{Mat}(\psi,(c_i)_p, (b_i)_m)\mathrm{Mat}(\phi,(b_i)_m,(a_i)_n) = (t_{ik})$ where $t_{ik} = \sum_{j = 1}^m s_{ij}r_{jk}$ . Also, $$(\psi\circ\phi)(a_i) = \psi\left(\sum_{i = 1}^n r_{ij}b_i\right) \\ = \sum_{i = 1}^n r_{ij}\psi(b_i) \\ = \sum_{i = 1}^n r_{ij}\left(\sum_{k = 1}^p s_{ki}c_k\right) \\ = \sum_{k = 1}^p \left(\sum_{i = 1}^n r_{ij}s_{ki}\right)c_k \\ \neq \sum_{k = 1}^p \left(\sum_{i = 1}^n s_{ki}r_{ij}\right)c_k $$ generally.","In Blyth's book ""Module Theory: An Approach to Linear Algebra"" matrix theory is developed generally over a noncommutative ring with . However, it seems that there is a mistake in way that an important property doesn't carry over to noncommutative rings, namely the isomorphic of a ring of matrices with coefficients in and the endomorphism ring of a free module over Given an a homomorphism of free -modules with respective bases and , the matrix of this homomorphism with respect to said bases is a matrix such that is the unique element of so that . One can prove that, for the respective bases, there is an isomorphism . But am I right to assume that this is not a ring homomorphism? It seems modules over noncommutative rings lack the multiplicative property for free modules with respective bases and and their homomorphisms . Am I right or there is a mistake there? I was doing all the matrix theory for commutative rings before now, but encountered a proof in the aforementioned book which uses (probably wrong) ring isomorphism. Let and . Then we have where . Also, generally.","R 1 n\times n R R \phi\colon M\to N R (a_i)_m (b_i)_n \mathrm{Mat}(\phi,(b_i)_n,(a_i)_m) (r_{ij}) r_{ij} R \phi(a_i) = s_{1i}b_1 + ... + r_{ij}b_j + ... + s_{ni} \vartheta\colon\mathrm{Hom}_R(M,N)\to\mathrm{Mat}_{n\times n}(R), \phi \mapsto \mathrm{Mat}(\phi,(b_i)_n,(a_i)_m) \mathrm{Mat}(\psi\circ\phi, (c_i)_p,(a_i)_n) = \mathrm{Mat}(\psi,(c_i)_p,(b_i)_m)\mathrm{Mat}(\phi,(b_i)_m,(a_i)_n) M,N,P (a_i)_n, (b_i)_m (c_i)_p \phi\colon M\to N, \psi\colon N\to P \mathrm{Mat}(\phi,(b_i)_m,(a_i)_n) = (r_{ij}) \mathrm{Mat}(\psi,(c_i)_p, (b_i)_m) = (s_{ij}) \mathrm{Mat}(\psi,(c_i)_p, (b_i)_m)\mathrm{Mat}(\phi,(b_i)_m,(a_i)_n) = (t_{ik}) t_{ik} = \sum_{j = 1}^m s_{ij}r_{jk} (\psi\circ\phi)(a_i) = \psi\left(\sum_{i = 1}^n r_{ij}b_i\right) \\
= \sum_{i = 1}^n r_{ij}\psi(b_i) \\
= \sum_{i = 1}^n r_{ij}\left(\sum_{k = 1}^p s_{ki}c_k\right) \\
= \sum_{k = 1}^p \left(\sum_{i = 1}^n r_{ij}s_{ki}\right)c_k \\
\neq \sum_{k = 1}^p \left(\sum_{i = 1}^n s_{ki}r_{ij}\right)c_k
","['linear-algebra', 'matrices', 'modules', 'noncommutative-algebra', 'free-modules']"
91,Possible Jordan Canonical Forms: Intuition,Possible Jordan Canonical Forms: Intuition,,"As I was reviewing linear algebra before I head off to grad school in the fall, I came across a question about Jordan Canonical Forms. It reads: ""Suppose that A is a square complex matrix with characteristic polynomial $c_A(x) = (x−1)^4(x+ 3)^5$ . Assume also that $A−I$ has nullity 4 and $A+3I$ has nullity 1, where $I$ is the identity matrix of the same size as $A$ . Find, with justification, all possible Jordan canonical forms of $A$ , and give the minimal polynomial for each."" I believe that there will be 2 Jordan Blocks, for each eigenvalue of $A$ . Since the rank of the null space of the linear operator $A-I$ is 4, then the dimension of the eigenspace $E_1$ will be four. So there will be four linearly independent eigenvectors with eigenvalue 1. Thus there will one 4 by 4 Jordan Block with no ones in the super-diagonal. Since the dimension of the null space of $A+3I$ has rank 1, then there is only one linearly independent eigenvector with eigenvalue -3. If $K_{-3}$ is the generalized eigenspace, then $dim(K_{-3})=5$ . Does this implies that there are 5 linearly independent generalized eigenvectors with corresponding to -3? Is so, then there will be one 5 by 5 Jordan block for the eigenvalue -3 with ones in the superdiagonal. So we get \begin{bmatrix}      1& 0 &0 &0 &0 &0 &0 &0 &0   \\  0& 1 &0 &0 &0 &0 &0 &0 &0  \\  0& 0 &1 &0 &0 &0 &0 &0 &0   \\  0& 0 &0 &1 &0 &0 &0 &0 &0   \\  0& 0 &0 &0 &-3 &1 &0 &0 &0   \\  0& 0 &0 &0 &0 &-3 &1 &0 &0   \\  0& 0 &0 &0 &0 &0 &-3 &1 &0   \\  0& 0 &0 &0 &0 &0 &0 &-3 &1   \\ 0& 0 &0 &0 &0 &0 &0 &0 &-3    \end{bmatrix} How do I determine the minimal polynomial? Are there any other options for the Jordan blocks corresponding to the eigenvalue -3? Is it possible for a different linear transformation with the same characteristic polynomial to have the dimension of the null space of $A-3I$ to be 2,3 or 4? Is so, how does this affect the Jordan form and why?","As I was reviewing linear algebra before I head off to grad school in the fall, I came across a question about Jordan Canonical Forms. It reads: ""Suppose that A is a square complex matrix with characteristic polynomial . Assume also that has nullity 4 and has nullity 1, where is the identity matrix of the same size as . Find, with justification, all possible Jordan canonical forms of , and give the minimal polynomial for each."" I believe that there will be 2 Jordan Blocks, for each eigenvalue of . Since the rank of the null space of the linear operator is 4, then the dimension of the eigenspace will be four. So there will be four linearly independent eigenvectors with eigenvalue 1. Thus there will one 4 by 4 Jordan Block with no ones in the super-diagonal. Since the dimension of the null space of has rank 1, then there is only one linearly independent eigenvector with eigenvalue -3. If is the generalized eigenspace, then . Does this implies that there are 5 linearly independent generalized eigenvectors with corresponding to -3? Is so, then there will be one 5 by 5 Jordan block for the eigenvalue -3 with ones in the superdiagonal. So we get How do I determine the minimal polynomial? Are there any other options for the Jordan blocks corresponding to the eigenvalue -3? Is it possible for a different linear transformation with the same characteristic polynomial to have the dimension of the null space of to be 2,3 or 4? Is so, how does this affect the Jordan form and why?","c_A(x) =
(x−1)^4(x+ 3)^5 A−I A+3I I A A A A-I E_1 A+3I K_{-3} dim(K_{-3})=5 \begin{bmatrix}
     1& 0 &0 &0 &0 &0 &0 &0 &0   \\
 0& 1 &0 &0 &0 &0 &0 &0 &0  \\
 0& 0 &1 &0 &0 &0 &0 &0 &0   \\
 0& 0 &0 &1 &0 &0 &0 &0 &0   \\
 0& 0 &0 &0 &-3 &1 &0 &0 &0   \\
 0& 0 &0 &0 &0 &-3 &1 &0 &0   \\
 0& 0 &0 &0 &0 &0 &-3 &1 &0   \\
 0& 0 &0 &0 &0 &0 &0 &-3 &1   \\
0& 0 &0 &0 &0 &0 &0 &0 &-3 
  \end{bmatrix} A-3I","['linear-algebra', 'matrices', 'jordan-normal-form']"
92,Derivative of matrix expression involving Kronecker products,Derivative of matrix expression involving Kronecker products,,"Given the matrix expression $C(f) = R  \left(  X f \otimes I \nonumber +I \otimes X f \right) X$ , where $R$ is $n\times n^2$ , $X$ is $n\times k$ , $f$ is $k\times 1$ and $I$ is an $n \times n$ identity matrix. What is $\frac{\partial C(f)}{\partial f}$ ? Thanks in advance!","Given the matrix expression , where is , is , is and is an identity matrix. What is ? Thanks in advance!",C(f) = R  \left(  X f \otimes I \nonumber +I \otimes X f \right) X R n\times n^2 X n\times k f k\times 1 I n \times n \frac{\partial C(f)}{\partial f},"['matrices', 'multivariable-calculus', 'derivatives', 'kronecker-product']"
93,Notation for element-wise multiplication of vector and matrix columns,Notation for element-wise multiplication of vector and matrix columns,,"What is a clear and concise notation for the element wise multiplication (Hadamard product) of a column vector $v$ and each column of a matrix $F$ . What I want to achieve it this: $$ v\odot F= \begin{bmatrix} v_1\\  v_2 \\  v_3 \end{bmatrix} \odot  \begin{bmatrix} f_{1,1} & f_{1,2}  & f_{1,3}\\  f_{2,1} & f_{2,2}  & f_{2,3}\\ f_{3,1} & f_{3,2}  & f_{3,3} \end{bmatrix} = \begin{bmatrix} v_1f_{1,1} & v_1f_{1,2}  & v_1f_{1,3}\\  v_2f_{2,1} & v_2f_{2,2}  & v_2f_{2,3}\\ v_3f_{3,1} & v_3f_{3,2}  & v_3f_{3,3} \end{bmatrix} $$ My question is essentially the same as this one , but I don't think the answer there actually answers the question and I don't have enough reputation to comment.","What is a clear and concise notation for the element wise multiplication (Hadamard product) of a column vector and each column of a matrix . What I want to achieve it this: My question is essentially the same as this one , but I don't think the answer there actually answers the question and I don't have enough reputation to comment.","v F 
v\odot F=
\begin{bmatrix}
v_1\\ 
v_2 \\ 
v_3
\end{bmatrix} \odot 
\begin{bmatrix}
f_{1,1} & f_{1,2}  & f_{1,3}\\ 
f_{2,1} & f_{2,2}  & f_{2,3}\\
f_{3,1} & f_{3,2}  & f_{3,3}
\end{bmatrix} =
\begin{bmatrix}
v_1f_{1,1} & v_1f_{1,2}  & v_1f_{1,3}\\ 
v_2f_{2,1} & v_2f_{2,2}  & v_2f_{2,3}\\
v_3f_{3,1} & v_3f_{3,2}  & v_3f_{3,3}
\end{bmatrix}
","['matrices', 'notation', 'vectors']"
94,Let $A=\begin{bmatrix} 1 & 2\\ 3& 4 \end{bmatrix}$ then det$(A^3-6A^2+5A+3I)=3$,Let  then det,A=\begin{bmatrix} 1 & 2\\ 3& 4 \end{bmatrix} (A^3-6A^2+5A+3I)=3,"Let $A=\begin{bmatrix} 1 & 2\\   3& 4 \end{bmatrix}$ then det $(A^3-6A^2+5A+3I)=3$ det $(A^3-6A^2+5A+3I)=$ det $((A^2-5A-2I)(A-I)+2A+I)= $ det $(2A+I)=3$ , Since a matrix satisfies its characteristic polynomial. Is this right?","Let then det det det det , Since a matrix satisfies its characteristic polynomial. Is this right?","A=\begin{bmatrix}
1 & 2\\ 
 3& 4
\end{bmatrix} (A^3-6A^2+5A+3I)=3 (A^3-6A^2+5A+3I)= ((A^2-5A-2I)(A-I)+2A+I)=  (2A+I)=3","['linear-algebra', 'matrices', 'determinant', 'cayley-hamilton']"
95,Interesting matrix construction question,Interesting matrix construction question,,"We are told that an $n \times n$ matrix $P$ satisfies $P^3=P$ . Can we construct such a matrix $P$ ? Of course, we see that $-1,0,1$ are its only eigenvalues and, thus, a diagonal matrix with diagonal elements $0,1,-1$ would do. However, if I were told that the given matrix should not have $0,1,-1$ as its diagonal elements, then what would be the way out?","We are told that an matrix satisfies . Can we construct such a matrix ? Of course, we see that are its only eigenvalues and, thus, a diagonal matrix with diagonal elements would do. However, if I were told that the given matrix should not have as its diagonal elements, then what would be the way out?","n \times n P P^3=P P -1,0,1 0,1,-1 0,1,-1","['linear-algebra', 'matrices']"
96,Show the matrix commutes with companion matrix is a polynomial,Show the matrix commutes with companion matrix is a polynomial,,"Let $A$ be a linear transform on $n$ -dimensional $V$ over a field $F$ . Under a basis $\alpha_1, \cdots, \alpha_n$ , the matrix representation of $A$ is as follows: $$A = \begin{bmatrix} 0 & 0 & \dots & 0 & -a_0 \\ 1 & 0 & \dots & 0 & -a_1 \\ 0 & 1 & \dots & 0 & -a_2 \\ \vdots & \vdots & \ddots & \vdots & \vdots \\ 0 & 0 & \dots & 1 & -a_{n-1} \end{bmatrix}.$$ Let $C(A):= \{T: T\text{ is a linear transform on $V$ and } TA = AT \}$ , and let $F[A]$ denotes all the polynomials in $A$ . Show that: $$C(A) = F[A]; \dim(C(A)) = n.$$ First of all, the minimal polynomial $m(\lambda)$ of $A$ is the same as its characteristic polynomial $f(\lambda)$ , namely $m(\lambda) = f(\lambda) = \lambda^n + a_{n-1}\lambda^{n-1} + \cdots a_0$ . Thus, plugging in $A$ , we see that all $A^{k}$ with $k \geq n$ could be expressed by $I, A, A^2, \cdots, A^{n-1}$ . So $\dim F[A] \leq n$ . If $\dim F[A] < n$ , say $k_0 I + k_1 A + \cdots + k_r A^r = 0$ with $r < n$ and some $k_j \neq 0$ , then we have that $g(\lambda) = k_0 + k_1 \lambda + \cdots + k_r \lambda^r$ is another polynomial with $g(A) = 0$ . By the definition of minimal polynomial, we must have that $r \geq n$ , a contradiction. So $\dim(F[A]) = n$ , and it remains to show the first equality $C(A) = F[A]$ . Also, one could see that $F[A] \subseteq C(A)$ . But I am not sure how to show the other direction. Could someone give me a hint?","Let be a linear transform on -dimensional over a field . Under a basis , the matrix representation of is as follows: Let , and let denotes all the polynomials in . Show that: First of all, the minimal polynomial of is the same as its characteristic polynomial , namely . Thus, plugging in , we see that all with could be expressed by . So . If , say with and some , then we have that is another polynomial with . By the definition of minimal polynomial, we must have that , a contradiction. So , and it remains to show the first equality . Also, one could see that . But I am not sure how to show the other direction. Could someone give me a hint?","A n V F \alpha_1, \cdots, \alpha_n A A = \begin{bmatrix}
0 & 0 & \dots & 0 & -a_0 \\
1 & 0 & \dots & 0 & -a_1 \\
0 & 1 & \dots & 0 & -a_2 \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & \dots & 1 & -a_{n-1}
\end{bmatrix}. C(A):= \{T: T\text{ is a linear transform on V and } TA = AT \} F[A] A C(A) = F[A]; \dim(C(A)) = n. m(\lambda) A f(\lambda) m(\lambda) = f(\lambda) = \lambda^n + a_{n-1}\lambda^{n-1} + \cdots a_0 A A^{k} k \geq n I, A, A^2, \cdots, A^{n-1} \dim F[A] \leq n \dim F[A] < n k_0 I + k_1 A + \cdots + k_r A^r = 0 r < n k_j \neq 0 g(\lambda) = k_0 + k_1 \lambda + \cdots + k_r \lambda^r g(A) = 0 r \geq n \dim(F[A]) = n C(A) = F[A] F[A] \subseteq C(A)","['linear-algebra', 'matrices', 'minimal-polynomials', 'companion-matrices']"
97,Finding matrix with lowest possible rank,Finding matrix with lowest possible rank,,"Find the values of $x$ for which the matrix \begin{bmatrix}         x & 0 & 1 \\         0 & 1 & 0 \\         1 & 0 & x \\   \end{bmatrix} has the lowest rank. Since the rank is the dimension of the row space of $A$ , I should find $x$ for which the dimension of the row space is as low as it can possibly be. I performed Gaussian elimination on $A$ via the following steps: \begin{bmatrix}         x & 0 & 1 \\         0 & 1 & 0 \\         1 & 0 & x \\ \end{bmatrix} \begin{bmatrix}         1 & 0 & \frac{1}{x} \\         0 & 1 & 0 \\         1 & 0 & x \\ \end{bmatrix} \begin{bmatrix}         1 & 0 & \frac{1}{x} \\         0 & 1 & 0 \\         0 & 0 & x -\frac{1}{x}  \\ \end{bmatrix} So I should now let $x - \frac{1}{x} = 0 $ , more specifically let $x^2 = 1$ and thus $x = \pm 1$ . Overall resulting in an overall lowest rank of $2$ . Is this the correct answer? And is it the correct way of thinking about questions such as these? Should I have considered the null space instead? Thanks in advance.","Find the values of for which the matrix has the lowest rank. Since the rank is the dimension of the row space of , I should find for which the dimension of the row space is as low as it can possibly be. I performed Gaussian elimination on via the following steps: So I should now let , more specifically let and thus . Overall resulting in an overall lowest rank of . Is this the correct answer? And is it the correct way of thinking about questions such as these? Should I have considered the null space instead? Thanks in advance.","x \begin{bmatrix}
        x & 0 & 1 \\
        0 & 1 & 0 \\
        1 & 0 & x \\
  \end{bmatrix} A x A \begin{bmatrix}
        x & 0 & 1 \\
        0 & 1 & 0 \\
        1 & 0 & x \\
\end{bmatrix} \begin{bmatrix}
        1 & 0 & \frac{1}{x} \\
        0 & 1 & 0 \\
        1 & 0 & x \\
\end{bmatrix} \begin{bmatrix}
        1 & 0 & \frac{1}{x} \\
        0 & 1 & 0 \\
        0 & 0 & x -\frac{1}{x}  \\
\end{bmatrix} x - \frac{1}{x} = 0  x^2 = 1 x = \pm 1 2","['linear-algebra', 'matrices', 'matrix-rank', 'matrix-completion']"
98,How to prove this rank inequality? [duplicate],How to prove this rank inequality? [duplicate],,"This question already has an answer here : Matrix problem similar to Problem 3, SEEMOUS 2019 (1 answer) Closed 4 years ago . Let $n\geq2$ and $A,B\in M_{n}(\mathbb{C})$ such that $B^2=B$ . Prove that $$\mbox{ rank }(AB-BA)\leq\mbox{ rank }(AB+BA).$$ If $B$ is zero or the identity matrix, we are done. But $B$ will always be diagonalisable. From this, how we can proceed?","This question already has an answer here : Matrix problem similar to Problem 3, SEEMOUS 2019 (1 answer) Closed 4 years ago . Let and such that . Prove that If is zero or the identity matrix, we are done. But will always be diagonalisable. From this, how we can proceed?","n\geq2 A,B\in M_{n}(\mathbb{C}) B^2=B \mbox{ rank }(AB-BA)\leq\mbox{ rank }(AB+BA). B B","['linear-algebra', 'matrices', 'inequality', 'matrix-rank', 'idempotents']"
99,Possible eigenvalues of matrix $C = A(B^TA)^{-1}B^T$,Possible eigenvalues of matrix,C = A(B^TA)^{-1}B^T,"We also have the conditions that $A$ and $B$ are rectangular matrices such that $B^TA$ is square and invertible. Thus, we can assume that $A$ and $B$ have dimensions $p \times n$ and hence $B^TA$ has dimensions $n \times n$ . So, what I have done so far is that since $$C \times C = A(B^TA)^{-1}B^TA(B^TA)^{-1}B^T = A(B^TA)^{-1}B^T = C$$ we have $$ C^2 = C$$ So, by the fact that eigenvalues are roots of the characteristic polynomial, possible eigenvalues can be $0$ or $1$ . But isn't there a problem for the eigenvalue $0$ as if $Cx = 0$ for some non-zero vector $x$ then multiplying $B^T$ from the left (this can be done as $C$ has dimensions $p \times p$ and $B^T$ has dimensions $n \times p$ ) gives $B^TCx = 0$ which means $$B^TA(B^TA)^{-1}B^Tx = 0 \implies B^TAy = 0$$ for some non-zero $y$ , so this is contradicting that $B^TA$ is invertible. So does this mean that only possible eigenvalue is $1$ ? Or is it possible that $y$ can be a zero vector so I can't conclude anything about that? Edit : I think that $y$ can be the zero vector because in that case, we just need $B^Tx$ to be zero as $B^TA$ is invertible. $B^Tx = 0$ is possible for a non-zero $x$ , isn't it?","We also have the conditions that and are rectangular matrices such that is square and invertible. Thus, we can assume that and have dimensions and hence has dimensions . So, what I have done so far is that since we have So, by the fact that eigenvalues are roots of the characteristic polynomial, possible eigenvalues can be or . But isn't there a problem for the eigenvalue as if for some non-zero vector then multiplying from the left (this can be done as has dimensions and has dimensions ) gives which means for some non-zero , so this is contradicting that is invertible. So does this mean that only possible eigenvalue is ? Or is it possible that can be a zero vector so I can't conclude anything about that? Edit : I think that can be the zero vector because in that case, we just need to be zero as is invertible. is possible for a non-zero , isn't it?",A B B^TA A B p \times n B^TA n \times n C \times C = A(B^TA)^{-1}B^TA(B^TA)^{-1}B^T = A(B^TA)^{-1}B^T = C  C^2 = C 0 1 0 Cx = 0 x B^T C p \times p B^T n \times p B^TCx = 0 B^TA(B^TA)^{-1}B^Tx = 0 \implies B^TAy = 0 y B^TA 1 y y B^Tx B^TA B^Tx = 0 x,"['linear-algebra', 'matrices', 'proof-verification', 'eigenvalues-eigenvectors']"
