,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,What's the Clifford algebra?,What's the Clifford algebra?,,"I'm reading a book on Clifford algebra for physicists. I don't quite understand it conceptually even if I can do most algebraic manipulations. Can some-one teach me what the Clifford algebra really is? (Keep in mind that I don't know abstract algebra, nothing except some group theory.)  Does it make sense to write the sum of a scalar and a bivector in the Clifford product? Both are very different things.","I'm reading a book on Clifford algebra for physicists. I don't quite understand it conceptually even if I can do most algebraic manipulations. Can some-one teach me what the Clifford algebra really is? (Keep in mind that I don't know abstract algebra, nothing except some group theory.)  Does it make sense to write the sum of a scalar and a bivector in the Clifford product? Both are very different things.",,"['linear-algebra', 'abstract-algebra', 'intuition', 'mathematical-physics', 'clifford-algebras']"
1,Empty Set $\{\}$ is the Only Basis of the Zero Vector Space $\{0\}$,Empty Set  is the Only Basis of the Zero Vector Space,\{\} \{0\},"Question Suppose we want to find a basis for the vector space $\{0\}$ . I know that the answer is that the only basis is the empty set. Is this answer a definition itself or it is a result of the definitions for linearly independent/dependent sets and Spanning/Generating sets ? If it is a result then would you mind mentioning the definitions of bold items from which this answer can be deduced. Useful Links I found the links Link 1 , Link 2 , Link 3 , Link 4 , Link 5 , and Link 6 useful for answering this question. It needs some elementary background form mathematical logic. You can learn it by spending a few hours on this Wikipedia page .","Question Suppose we want to find a basis for the vector space . I know that the answer is that the only basis is the empty set. Is this answer a definition itself or it is a result of the definitions for linearly independent/dependent sets and Spanning/Generating sets ? If it is a result then would you mind mentioning the definitions of bold items from which this answer can be deduced. Useful Links I found the links Link 1 , Link 2 , Link 3 , Link 4 , Link 5 , and Link 6 useful for answering this question. It needs some elementary background form mathematical logic. You can learn it by spending a few hours on this Wikipedia page .",\{0\},"['linear-algebra', 'vector-spaces']"
2,$3 \times 3$ matrix with determinant a large power of $2$,matrix with determinant a large power of,3 \times 3 2,"Here's a little curiosity I found.  The following $3 \times 3$ matrix has entries that are distinct primes $< 100$ and its determinant is $2^{19}$ . $$ \pmatrix{71 & 31 & 97\cr 61 & 67 & 23\cr 7 & 83 & 73}$$ Does anyone know of another such matrix whose determinant is nonzero and divisible by a larger power of $2$ ?  If not, what is the least $p$ such that there is a $3 \times 3$ matrix with entries distinct primes $\le p$ and determinant nonzero and divisible by a larger power of $2$ ?  What if we drop the requirement of the entries being primes (and allow distinct positive integers $\le p$ )?","Here's a little curiosity I found.  The following matrix has entries that are distinct primes and its determinant is . Does anyone know of another such matrix whose determinant is nonzero and divisible by a larger power of ?  If not, what is the least such that there is a matrix with entries distinct primes and determinant nonzero and divisible by a larger power of ?  What if we drop the requirement of the entries being primes (and allow distinct positive integers )?",3 \times 3 < 100 2^{19}  \pmatrix{71 & 31 & 97\cr 61 & 67 & 23\cr 7 & 83 & 73} 2 p 3 \times 3 \le p 2 \le p,"['linear-algebra', 'matrices', 'elementary-number-theory', 'determinant']"
3,Rank of a $n! \times n$ matrix,Rank of a  matrix,n! \times n,"This question is about showing that $n!$ points resulting from applying a function (defined below) to the permutations of $n$ numbers lie on a $n-1$ dimensional hyperplane. Let $X=\langle x_1,\cdots,x_n \rangle$ and $Y=\langle y_1,\cdots,y_n \rangle$ be vectors of positive reals and let $P=\langle p_1,\cdots,p_n \rangle$ be a permutation of the numbers $\{1,\cdots,n\}$. Let, $$ y_i=\log\left(1+\frac{x_i}{1+\sum\limits_{p_j<\,p_i}x_j}\right) $$ be a mapping from $X,P$ to $Y$. So, for any given vector $X$, we have $n!$ (one for each permutation) output vectors $Y$.  Let $Y_P$ denote a vector resulting from permutation $P$. Now, for a given $X$, create a matrix $M$ such that each row of it is $Y_P-Y_{P_1}$ for a fixed $P_1$ and for all $P$. $$ M=\begin{bmatrix} Y_{P_1}-Y_{P_1} \\ Y_{P_2}-Y_{P_1} \\ \vdots \\ Y_{P_{n!}}-Y_{P_1} \end{bmatrix} $$ So, $M$ is a $n! \times n$ matrix. I have two questions: I guess the rank of $M$ is at most $n-1$, that is all of $Y$'s lie on a hyperplane of dimension $n-1$ (I verified it for $n=3$ and $n=4$). Is that true or not? Why? For what sort of mappings from $X,P$ to $Y$, is the answer to the previous question positive? Edit 3: I realized that for all of the permutations, $\sum_i y_i = \log(1+\sum_i x_i)$ and as Chris Culter commented below, this might be the solution to the problem. The proof of inequality is long but an example clears its correctness: consider $n=3$ and $P=\langle 1, 2, 3 \rangle$, $$ \begin{align} y_1+y_2+y_3&= \log(1+\frac{x_1}{1+x_2+x_3})+ \log(1+\frac{x_2}{1+x_3})+ \log(1+\frac{x_3}{1})\\ &= \log(\frac{1+x_1+x_2+x_3}{1+x_2+x_3})+ \log(\frac{1+x_2+x_3}{1+x_3})+ \log(\frac{1+x_3}{1})\\ &= \log(1+x_1+x_2+x_3). \end{align} $$ Since any other permutation only renames $x_1$, $x_2$, and $x_3$, the output remains the same. Edit 2: The problem has a specific structure which may help solving it: The $n!$ points consist of $n$ ensembles of $(n-1)!$ points. Ensemble $i$ consists of all of the points $Y_P$ such that $i^{th}$ element of $P$ is 1. Therefore, all of the points in ensemble $i$ has the same $y_i$. For example, for $n=3$, there are 3 ensembles of 2 points (6 points in total). All of the points lie on a 2D plane. The following figure shows an example in 3D: Points in the same ensemble have a same colour. The labels beside the points denote the permutation that generates the point. So, points 123 and 132 are in ensemble 1, points 213 and 312 are in ensemble 2 (1 is the second element of their permutation), and points 231 and 321 are in ensemble 3 (1 is the third element of their permutation). For $n=4$, there are 4 ensembles of 6 points (24 points in total). Since all of the points in 4D lie on a 3D plane, we can project the points into the 3D space. Here is an example: Higher dimensions are recursively constructed as described above. So, for $n=5$, there are 5 ensembles of the form shown above (in 4D). This recursive nature can be useful for solving the problem. Edit 3: I used Mathematica to verify my conjecture for $n=3$ to $8$ and it's correct. See my other question on Mathematica.SE . For $n>8$, because of the exponential running time of the algorithm, it's hard to verify the conjecture. The above conjecture also holds for the following mapping functions: $$ y_i={x_i}-{\sum\limits_{p_j<\,p_i}x_j} $$ $$ y_i={x_i}^3-{\sum\limits_{p_j<\,p_i}{x_j}^2} $$ But it does not hold for the following functions: $$ y_i=\log\left(\frac{x_i}{1+\sum\limits_{p_j<\,p_i}x_j}\right) $$ $$ y_i={x_i}^3-\left(\sum\limits_{p_j<\,p_i}x_j\right)^2 $$","This question is about showing that $n!$ points resulting from applying a function (defined below) to the permutations of $n$ numbers lie on a $n-1$ dimensional hyperplane. Let $X=\langle x_1,\cdots,x_n \rangle$ and $Y=\langle y_1,\cdots,y_n \rangle$ be vectors of positive reals and let $P=\langle p_1,\cdots,p_n \rangle$ be a permutation of the numbers $\{1,\cdots,n\}$. Let, $$ y_i=\log\left(1+\frac{x_i}{1+\sum\limits_{p_j<\,p_i}x_j}\right) $$ be a mapping from $X,P$ to $Y$. So, for any given vector $X$, we have $n!$ (one for each permutation) output vectors $Y$.  Let $Y_P$ denote a vector resulting from permutation $P$. Now, for a given $X$, create a matrix $M$ such that each row of it is $Y_P-Y_{P_1}$ for a fixed $P_1$ and for all $P$. $$ M=\begin{bmatrix} Y_{P_1}-Y_{P_1} \\ Y_{P_2}-Y_{P_1} \\ \vdots \\ Y_{P_{n!}}-Y_{P_1} \end{bmatrix} $$ So, $M$ is a $n! \times n$ matrix. I have two questions: I guess the rank of $M$ is at most $n-1$, that is all of $Y$'s lie on a hyperplane of dimension $n-1$ (I verified it for $n=3$ and $n=4$). Is that true or not? Why? For what sort of mappings from $X,P$ to $Y$, is the answer to the previous question positive? Edit 3: I realized that for all of the permutations, $\sum_i y_i = \log(1+\sum_i x_i)$ and as Chris Culter commented below, this might be the solution to the problem. The proof of inequality is long but an example clears its correctness: consider $n=3$ and $P=\langle 1, 2, 3 \rangle$, $$ \begin{align} y_1+y_2+y_3&= \log(1+\frac{x_1}{1+x_2+x_3})+ \log(1+\frac{x_2}{1+x_3})+ \log(1+\frac{x_3}{1})\\ &= \log(\frac{1+x_1+x_2+x_3}{1+x_2+x_3})+ \log(\frac{1+x_2+x_3}{1+x_3})+ \log(\frac{1+x_3}{1})\\ &= \log(1+x_1+x_2+x_3). \end{align} $$ Since any other permutation only renames $x_1$, $x_2$, and $x_3$, the output remains the same. Edit 2: The problem has a specific structure which may help solving it: The $n!$ points consist of $n$ ensembles of $(n-1)!$ points. Ensemble $i$ consists of all of the points $Y_P$ such that $i^{th}$ element of $P$ is 1. Therefore, all of the points in ensemble $i$ has the same $y_i$. For example, for $n=3$, there are 3 ensembles of 2 points (6 points in total). All of the points lie on a 2D plane. The following figure shows an example in 3D: Points in the same ensemble have a same colour. The labels beside the points denote the permutation that generates the point. So, points 123 and 132 are in ensemble 1, points 213 and 312 are in ensemble 2 (1 is the second element of their permutation), and points 231 and 321 are in ensemble 3 (1 is the third element of their permutation). For $n=4$, there are 4 ensembles of 6 points (24 points in total). Since all of the points in 4D lie on a 3D plane, we can project the points into the 3D space. Here is an example: Higher dimensions are recursively constructed as described above. So, for $n=5$, there are 5 ensembles of the form shown above (in 4D). This recursive nature can be useful for solving the problem. Edit 3: I used Mathematica to verify my conjecture for $n=3$ to $8$ and it's correct. See my other question on Mathematica.SE . For $n>8$, because of the exponential running time of the algorithm, it's hard to verify the conjecture. The above conjecture also holds for the following mapping functions: $$ y_i={x_i}-{\sum\limits_{p_j<\,p_i}x_j} $$ $$ y_i={x_i}^3-{\sum\limits_{p_j<\,p_i}{x_j}^2} $$ But it does not hold for the following functions: $$ y_i=\log\left(\frac{x_i}{1+\sum\limits_{p_j<\,p_i}x_j}\right) $$ $$ y_i={x_i}^3-\left(\sum\limits_{p_j<\,p_i}x_j\right)^2 $$",,"['linear-algebra', 'matrices', 'matrix-rank']"
4,Show that a matrix $A$ is singular if and only if $0$ is an eigenvalue.,Show that a matrix  is singular if and only if  is an eigenvalue.,A 0,I can't find the missing link between singularity and zero eigenvalues as is stated in the following proposition: A matrix $A$ is singular if and only if $0$ is an eigenvalue. Could anyone shed some light?,I can't find the missing link between singularity and zero eigenvalues as is stated in the following proposition: A matrix $A$ is singular if and only if $0$ is an eigenvalue. Could anyone shed some light?,,['linear-algebra']
5,Is basis change ever useful in practical linear algebra?,Is basis change ever useful in practical linear algebra?,,"In layman's terms, why would anyone ever want to change basis? Do eigenvalues have to do with changing basis?","In layman's terms, why would anyone ever want to change basis? Do eigenvalues have to do with changing basis?",,"['linear-algebra', 'soft-question', 'change-of-basis']"
6,If I generate a random matrix what is the probability of it to be singular?,If I generate a random matrix what is the probability of it to be singular?,,"Just a random question which came to my mind while watching a linear algebra lecture online.  The lecturer said that MATLAB always generates non-singular matrices. I wish to know that in the space of random matrices, what percentage are singular? Is there any work related to this?","Just a random question which came to my mind while watching a linear algebra lecture online.  The lecturer said that MATLAB always generates non-singular matrices. I wish to know that in the space of random matrices, what percentage are singular? Is there any work related to this?",,"['linear-algebra', 'matrices', 'random-matrices']"
7,How is the null space related to singular value decomposition?,How is the null space related to singular value decomposition?,,"It is said that a matrix's null space can be derived from QR or SVD. I tried an example: $$A= \begin{bmatrix}  1&3\\ 1&2\\ 1&-1\\ 2&1\\ \end{bmatrix}  $$ I'm convinced that QR (more precisely, the last two columns of Q) gives the null space: $$Q= \begin{bmatrix}  -0.37796&   -0.68252&   -0.17643&   -0.60015\\    -0.37796&   -0.36401&   0.73034&   0.43731\\    -0.37796&   0.59152&   0.43629&   -0.56293\\    -0.75593&   0.22751&   -0.4951&   0.36288\\    \end{bmatrix}  $$ However, neither $U$ nor $V$ produced by SVD ( $A=U\Sigma V^*$ ) make $A$ zero (I tested with 3 libraries: JAMA, EJML, and Commons): $$ U= \begin{bmatrix}  0.73039&   0.27429\\    0.52378&   0.03187\\    -0.09603&   -0.69536\\    0.42775&   -0.66349\\ \end{bmatrix}  $$ $$ \Sigma= \begin{bmatrix}  4.26745&   0\\    0&   1.94651\\    \end{bmatrix}  $$ $$ V= \begin{bmatrix}  0.47186&   -0.88167\\    0.88167&   0.47186\\   \end{bmatrix}  $$ This is contradiction to Using the SVD, if $A=U\Sigma V^*$ , then columns of $V^*$ corresponding to small singular values (i.e., small diagonal entries of $\Sigma$ ) make up the a basis for the null space.","It is said that a matrix's null space can be derived from QR or SVD. I tried an example: I'm convinced that QR (more precisely, the last two columns of Q) gives the null space: However, neither nor produced by SVD ( ) make zero (I tested with 3 libraries: JAMA, EJML, and Commons): This is contradiction to Using the SVD, if , then columns of corresponding to small singular values (i.e., small diagonal entries of ) make up the a basis for the null space.","A= \begin{bmatrix} 
1&3\\
1&2\\
1&-1\\
2&1\\
\end{bmatrix} 
 Q= \begin{bmatrix} 
-0.37796&   -0.68252&   -0.17643&   -0.60015\\   
-0.37796&   -0.36401&   0.73034&   0.43731\\   
-0.37796&   0.59152&   0.43629&   -0.56293\\   
-0.75593&   0.22751&   -0.4951&   0.36288\\   
\end{bmatrix} 
 U V A=U\Sigma V^* A  U= \begin{bmatrix} 
0.73039&   0.27429\\   
0.52378&   0.03187\\   
-0.09603&   -0.69536\\   
0.42775&   -0.66349\\
\end{bmatrix} 
  \Sigma= \begin{bmatrix} 
4.26745&   0\\   
0&   1.94651\\   
\end{bmatrix} 
  V= \begin{bmatrix} 
0.47186&   -0.88167\\   
0.88167&   0.47186\\  
\end{bmatrix} 
 A=U\Sigma V^* V^* \Sigma","['linear-algebra', 'matrix-decomposition', 'svd']"
8,What is the geometric meaning of singular matrix,What is the geometric meaning of singular matrix,,"Could anyone help explain what is the geometric meaning of singular matrix ? What's the difference between singular and non-singular matrix ? I know the definition, but couldn't understand it very well. Thanks.","Could anyone help explain what is the geometric meaning of singular matrix ? What's the difference between singular and non-singular matrix ? I know the definition, but couldn't understand it very well. Thanks.",,"['linear-algebra', 'matrices']"
9,Gram matrix invertible iff set of vectors linearly independent,Gram matrix invertible iff set of vectors linearly independent,,"Given a set of vectors $v_1 \cdots v_n$, the $n\times n$ Gram matrix $G$ is defined as $G_{i,j}=v_i \cdot v_j$ Due to symmetry in the dot product, $G$ is Hermitian. I'm trying to remember why $|G|=0$ iff the set of vectors are not linearly independent.","Given a set of vectors $v_1 \cdots v_n$, the $n\times n$ Gram matrix $G$ is defined as $G_{i,j}=v_i \cdot v_j$ Due to symmetry in the dot product, $G$ is Hermitian. I'm trying to remember why $|G|=0$ iff the set of vectors are not linearly independent.",,"['linear-algebra', 'matrices']"
10,Why does cross product give a vector which is perpendicular to a plane,Why does cross product give a vector which is perpendicular to a plane,,"I was wondering if anyone could give me the intuition behind the cross product of two vectors $\textbf{a}$ and $\textbf{b}$. Why does their cross product $\textbf{n} = \textbf{a} \times \textbf{b}$  give me a vector which is perpendicular to a plane? I know I can just check this by using dot product but I'm not totally satisfied with ""it just works"" answer =) Thank you for any help! =)","I was wondering if anyone could give me the intuition behind the cross product of two vectors $\textbf{a}$ and $\textbf{b}$. Why does their cross product $\textbf{n} = \textbf{a} \times \textbf{b}$  give me a vector which is perpendicular to a plane? I know I can just check this by using dot product but I'm not totally satisfied with ""it just works"" answer =) Thank you for any help! =)",,"['linear-algebra', 'inner-products', 'cross-product']"
11,Determinant of the Kronecker Product of Two Matrices,Determinant of the Kronecker Product of Two Matrices,,"I'd like to know how can be shown that $\det(A \otimes B) = \det(A)^m \det(B)^n$ when $A$ and $B$ are square matrices of size $n$ and $m$ respectively and $\otimes$ represents the Kronecker product of $A$ and $B$. I've seen some proofs using eigenvalues of $A$ and $B$, but since not every matrix has eigenvalues (at least not in all rings), I'd like a more intrinsic proof, maybe using the fact that $M_{(nm)^2} \cong M_{n^2} \otimes M_{m^2}$. It all comes down to showing that $\det(A \otimes I) = \det(A)^m$ and using some smart property (e.g. $(A\otimes B)(C\otimes D) = (AC) \otimes (BD)$), but I wasn't able to do even that.","I'd like to know how can be shown that $\det(A \otimes B) = \det(A)^m \det(B)^n$ when $A$ and $B$ are square matrices of size $n$ and $m$ respectively and $\otimes$ represents the Kronecker product of $A$ and $B$. I've seen some proofs using eigenvalues of $A$ and $B$, but since not every matrix has eigenvalues (at least not in all rings), I'd like a more intrinsic proof, maybe using the fact that $M_{(nm)^2} \cong M_{n^2} \otimes M_{m^2}$. It all comes down to showing that $\det(A \otimes I) = \det(A)^m$ and using some smart property (e.g. $(A\otimes B)(C\otimes D) = (AC) \otimes (BD)$), but I wasn't able to do even that.",,"['linear-algebra', 'matrices', 'determinant', 'tensor-products', 'kronecker-product']"
12,Eigenspace. What is it?,Eigenspace. What is it?,,"What is an eigenspace? No video or anything out there really explains what an eigenspace is. From what I have understood, it is just a direction. But why do we need it? The following questions have been bugging me for quite a while, and I can't find a real straightforward answer to them. Hopefully, one of you can help me. What is an eigenspace? Why are the eigenvectors calculated in a diagonal? What is the practical use of the eigenspace? Like what does it do or what is it used for? other than calculating the diagonal of a matrix. Why is it important o calculate the diagonal of a matrix? I want to know mainly because I just passed a linear algebra course and I have no idea what an eigenspace is, which is embarrassing for me and for my professor because he didn't explain what they were, he just basically said: ""This is how you calculate it and if you want to know more then read about it in the book"".","What is an eigenspace? No video or anything out there really explains what an eigenspace is. From what I have understood, it is just a direction. But why do we need it? The following questions have been bugging me for quite a while, and I can't find a real straightforward answer to them. Hopefully, one of you can help me. What is an eigenspace? Why are the eigenvectors calculated in a diagonal? What is the practical use of the eigenspace? Like what does it do or what is it used for? other than calculating the diagonal of a matrix. Why is it important o calculate the diagonal of a matrix? I want to know mainly because I just passed a linear algebra course and I have no idea what an eigenspace is, which is embarrassing for me and for my professor because he didn't explain what they were, he just basically said: ""This is how you calculate it and if you want to know more then read about it in the book"".",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
13,Orthogonality and linear independence,Orthogonality and linear independence,,"[Theorem] Let $V$ be an inner product space, and let $S$ be an orthogonal subset of $V$ consisting of nonzero vectors. Then $S$ is linearly independent. Also, orthogonal set and linearly independent set both generate the same subspace.  (Is that right?) Then orthogonal $\rightarrow$ linearly independent but orthogonal $\nleftarrow$ linearly independent is that right? One more question. For T/F, Every orthogonal set is linearly independent (F) Every orthonormal set is linearly independent (T) Why?","[Theorem] Let $V$ be an inner product space, and let $S$ be an orthogonal subset of $V$ consisting of nonzero vectors. Then $S$ is linearly independent. Also, orthogonal set and linearly independent set both generate the same subspace.  (Is that right?) Then orthogonal $\rightarrow$ linearly independent but orthogonal $\nleftarrow$ linearly independent is that right? One more question. For T/F, Every orthogonal set is linearly independent (F) Every orthonormal set is linearly independent (T) Why?",,"['linear-algebra', 'orthogonality']"
14,Vector Spaces and AC,Vector Spaces and AC,,"I know that the proof that every vector space has a basis uses the Axiom of Choice, or Zorn's Lemma. If we consider an axiom system without the Axiom of Choice, are there vector spaces that provably have no basis?","I know that the proof that every vector space has a basis uses the Axiom of Choice, or Zorn's Lemma. If we consider an axiom system without the Axiom of Choice, are there vector spaces that provably have no basis?",,"['linear-algebra', 'vector-spaces']"
15,What is the kernel of the tensor product of two maps?,What is the kernel of the tensor product of two maps?,,"Assume that $f_1\colon V_1\to W_1, f_2\colon V_2\to W_2$ are $k$-linear maps between $k$-vector spaces (over the same field $k$, but the dimension may be infinity). Then the tensor product $f_1\otimes f_2\colon V_1\otimes V_2\to W_1\otimes W_2$ is defined, and it's obvious that $\ker f_1\otimes V_2+ V_1\otimes \ker f_2 \subseteq \ker (f_1\otimes f_2)$. My question is whether the relation $\subseteq$ is in fact $=$. If this does not hold, how about assuming all these vector spaces are commutative associated $k$-algebras with identity and that all the maps are $k$-algebra homomorphisms? Or can you give a ""right"" form of the kernel $\ker (f_1\otimes f_2)$?","Assume that $f_1\colon V_1\to W_1, f_2\colon V_2\to W_2$ are $k$-linear maps between $k$-vector spaces (over the same field $k$, but the dimension may be infinity). Then the tensor product $f_1\otimes f_2\colon V_1\otimes V_2\to W_1\otimes W_2$ is defined, and it's obvious that $\ker f_1\otimes V_2+ V_1\otimes \ker f_2 \subseteq \ker (f_1\otimes f_2)$. My question is whether the relation $\subseteq$ is in fact $=$. If this does not hold, how about assuming all these vector spaces are commutative associated $k$-algebras with identity and that all the maps are $k$-algebra homomorphisms? Or can you give a ""right"" form of the kernel $\ker (f_1\otimes f_2)$?",,"['linear-algebra', 'abstract-algebra', 'vector-spaces', 'tensor-products', 'multilinear-algebra']"
16,Does the rank-nullity theorem hold for infinite dimensional $V$?,Does the rank-nullity theorem hold for infinite dimensional ?,V,"The rank nullity theorem states that for vector spaces $V$ and $W$ with $V$ finite dimensional, and $T: V \to W$ a linear map, $$\dim V = \dim \ker T + \dim \operatorname{im} T.$$ Does this hold for infinite dimensional $V$?  According to this , the statement is false.  But according to this , page 4, the statement is still true.  I'm thoroughly confused.","The rank nullity theorem states that for vector spaces $V$ and $W$ with $V$ finite dimensional, and $T: V \to W$ a linear map, $$\dim V = \dim \ker T + \dim \operatorname{im} T.$$ Does this hold for infinite dimensional $V$?  According to this , the statement is false.  But according to this , page 4, the statement is still true.  I'm thoroughly confused.",,['linear-algebra']
17,If two matrices have the same eigenvalues and eigenvectors are they equal?,If two matrices have the same eigenvalues and eigenvectors are they equal?,,"The question stems from a problem i stumbled upon while working with eigenvalues. Asking to explain why $A^{100}$ is close to $A^\infty$ $$A=   \left[ \begin{array}{cc}    .6 & .2 \\    .4 & .8   \end{array} \right] $$  $$A^\infty=   \left[ \begin{array}{cc}    1/3 & 1/3 \\    2/3 & 2/3   \end{array} \right] $$  Answer being given that (skipping calculations) that $A$ has eigenvalues $\lambda_1=1$ and $\lambda_2=0.4$ with eigenvectors $x_1=(1,2)$ and $x_2=(1,-1)$, and $A^{\infty}$ has eigenvalues $\lambda_1=1$ and $\lambda_2=0$, with same eigenvectors, while $A^{100}$ has eigenvalues $\lambda_1=1$ and $\lambda_2=(0.4)^{100}$ with same eigenvectors as the others, concluding that as the eigenvectors are the same and the eigenvalues are close comparing $A^\infty$ and $A^{100}$ they must be close. Creating the basis for my question, how can one conclude that two matrices with same eigenvectors and close/equal eigenvalues are close/equal to each other? My initial thoughts is that two matrices with equal eigenvectors and eigenvalues founds the basis for the same transformation which is why they are equal - Am I completly off?","The question stems from a problem i stumbled upon while working with eigenvalues. Asking to explain why $A^{100}$ is close to $A^\infty$ $$A=   \left[ \begin{array}{cc}    .6 & .2 \\    .4 & .8   \end{array} \right] $$  $$A^\infty=   \left[ \begin{array}{cc}    1/3 & 1/3 \\    2/3 & 2/3   \end{array} \right] $$  Answer being given that (skipping calculations) that $A$ has eigenvalues $\lambda_1=1$ and $\lambda_2=0.4$ with eigenvectors $x_1=(1,2)$ and $x_2=(1,-1)$, and $A^{\infty}$ has eigenvalues $\lambda_1=1$ and $\lambda_2=0$, with same eigenvectors, while $A^{100}$ has eigenvalues $\lambda_1=1$ and $\lambda_2=(0.4)^{100}$ with same eigenvectors as the others, concluding that as the eigenvectors are the same and the eigenvalues are close comparing $A^\infty$ and $A^{100}$ they must be close. Creating the basis for my question, how can one conclude that two matrices with same eigenvectors and close/equal eigenvalues are close/equal to each other? My initial thoughts is that two matrices with equal eigenvectors and eigenvalues founds the basis for the same transformation which is why they are equal - Am I completly off?",,"['linear-algebra', 'eigenvalues-eigenvectors', 'linear-transformations', 'diagonalization']"
18,What is the most rigorous definition of a matrix?,What is the most rigorous definition of a matrix?,,"Is matrix just a rectangular array of symbols and expressions, or one can define it in a more formal way?","Is matrix just a rectangular array of symbols and expressions, or one can define it in a more formal way?",,"['linear-algebra', 'matrices', 'definition']"
19,Prove that $\det( M) \in \mathbb{Z}$,Prove that,\det( M) \in \mathbb{Z},Let $\Gamma$ be a finite multiplicative group of matrices with complex entries. Let $M$ denote the sum of the matrices in $\Gamma$ . Prove that det $M$ is an integer. Hint: square $M$ and use the distributivity of multiplication with respect to the sum of matrices. I tried using the hint but I am not able to get to anything.,Let be a finite multiplicative group of matrices with complex entries. Let denote the sum of the matrices in . Prove that det is an integer. Hint: square and use the distributivity of multiplication with respect to the sum of matrices. I tried using the hint but I am not able to get to anything.,\Gamma M \Gamma M M,"['linear-algebra', 'abstract-algebra']"
20,An orthonormal set cannot be a basis in an infinite dimension vector space?,An orthonormal set cannot be a basis in an infinite dimension vector space?,,"I'm reading the Algebra book by Knapp and he mentions in passing that an orthonormal set in an infinite dimension vector space is ""never large enough"" to be a vector-space basis (i.e. that every vector can be written as a finite sum of vectors from the basis; such bases do exist for infinite dimension vector spaces by virtue of the axiom of choice, but usually one works with orthonormal sets for which infinite sums yield all the elements of the vector space). So my question is - how can we prove this claim? (That an orthonormal set in an infinite dimension vector space is not a vector space basis). Edit (by Jonas Meyer): Knapp says in a footnote on page 92 of Basic algebra : In the infinite-dimensional theory the term ""orthonormal basis"" is used for an orthonormal set that spans $V$ when limits of finite sums are allowed, in addition to finite sums themselves; when $V$ is infinite-dimensional, an orthonormal basis is never large enough to be a vector-space basis. Thus, without explicitly using the word, Knapp is referring only to complete infinite-dimensional inner product spaces.","I'm reading the Algebra book by Knapp and he mentions in passing that an orthonormal set in an infinite dimension vector space is ""never large enough"" to be a vector-space basis (i.e. that every vector can be written as a finite sum of vectors from the basis; such bases do exist for infinite dimension vector spaces by virtue of the axiom of choice, but usually one works with orthonormal sets for which infinite sums yield all the elements of the vector space). So my question is - how can we prove this claim? (That an orthonormal set in an infinite dimension vector space is not a vector space basis). Edit (by Jonas Meyer): Knapp says in a footnote on page 92 of Basic algebra : In the infinite-dimensional theory the term ""orthonormal basis"" is used for an orthonormal set that spans $V$ when limits of finite sums are allowed, in addition to finite sums themselves; when $V$ is infinite-dimensional, an orthonormal basis is never large enough to be a vector-space basis. Thus, without explicitly using the word, Knapp is referring only to complete infinite-dimensional inner product spaces.",,"['linear-algebra', 'hilbert-spaces', 'inner-products', 'orthonormal']"
21,Every Function in a Finite Field is a Polynomial Function,Every Function in a Finite Field is a Polynomial Function,,"From a bank of past master's exams I am going through: Let $F$ be a finite field. Show that any function from $F$ to $F$ is a polynomial function. I know that finite fields are fields of $p$ elements for $p$ prime [EDIT: It's actually $p^n$ for $p$ prime; see comment below]. Since I have $p$ choices for the $p$ elements to map to, then I have $p^p$ distinct functions. I think every function can be written in the form $f(x) = a_{p-1}x^{p-1} + \dots + a_0x^0$. For then given the values $f(0), f(1), \ldots f(p-1)$, I can solve for the coefficents by the linear system of equations $$ a_0 + \sum_{i=1}^{p-1} n^i a_i = f(n).$$ This then gives me a $p-1 \times p-1$ square matrix over the field $\mathbb{F}_p$: $$\left( \begin{array}{ccccc} 1& 0 & 0 & \ldots & 0 \\ 1& 1 & 1 & \dots & 1 \\ 1& 2 & 4 & \dots & 2^{p-1} \\ \vdots & \vdots & \vdots & & \vdots \\ 1 & p-1 & (p-1)^2 & \dots & (p-1)^{p-1} \end{array} \right) \left( \begin{array}{c} a_0 \\ a_1 \\ a_2 \\ \vdots \\ a_{p-1} \end{array}\right)  = \left( \begin{array}{c} f(0) \\ f(1) \\ f(2) \\ \vdots \\ f(p-1) \end{array} \right)$$ If I can show this matrix is invertible, then I can always find the $a_i$. But I am a bit stumped on how to show this (partially because I don't think I've ever done linear algebra in a vector space over a finite field). It does not seem easy to show linear independence, or nonzero determinant, or full row rank. Alternatively (I just thought of this), can I show this is true by arguing that the map between the two sets (the set of polynomials of degree $p-1$; and the set of functions $F \to F$) is injective, and that it must be a bijection because the sets have the same cardinality $p^p$?","From a bank of past master's exams I am going through: Let $F$ be a finite field. Show that any function from $F$ to $F$ is a polynomial function. I know that finite fields are fields of $p$ elements for $p$ prime [EDIT: It's actually $p^n$ for $p$ prime; see comment below]. Since I have $p$ choices for the $p$ elements to map to, then I have $p^p$ distinct functions. I think every function can be written in the form $f(x) = a_{p-1}x^{p-1} + \dots + a_0x^0$. For then given the values $f(0), f(1), \ldots f(p-1)$, I can solve for the coefficents by the linear system of equations $$ a_0 + \sum_{i=1}^{p-1} n^i a_i = f(n).$$ This then gives me a $p-1 \times p-1$ square matrix over the field $\mathbb{F}_p$: $$\left( \begin{array}{ccccc} 1& 0 & 0 & \ldots & 0 \\ 1& 1 & 1 & \dots & 1 \\ 1& 2 & 4 & \dots & 2^{p-1} \\ \vdots & \vdots & \vdots & & \vdots \\ 1 & p-1 & (p-1)^2 & \dots & (p-1)^{p-1} \end{array} \right) \left( \begin{array}{c} a_0 \\ a_1 \\ a_2 \\ \vdots \\ a_{p-1} \end{array}\right)  = \left( \begin{array}{c} f(0) \\ f(1) \\ f(2) \\ \vdots \\ f(p-1) \end{array} \right)$$ If I can show this matrix is invertible, then I can always find the $a_i$. But I am a bit stumped on how to show this (partially because I don't think I've ever done linear algebra in a vector space over a finite field). It does not seem easy to show linear independence, or nonzero determinant, or full row rank. Alternatively (I just thought of this), can I show this is true by arguing that the map between the two sets (the set of polynomials of degree $p-1$; and the set of functions $F \to F$) is injective, and that it must be a bijection because the sets have the same cardinality $p^p$?",,"['linear-algebra', 'abstract-algebra', 'matrices', 'finite-fields']"
22,"Using the Determinant to verify Linear Independence, Span and Basis","Using the Determinant to verify Linear Independence, Span and Basis",,"Can the determinant (assuming it's non-zero) be used to determine that the vectors given are linearly independent, span the subspace and are a basis of that subspace? (In other words assuming I have a set which I can make into a square matrix, can I use the determinant to determine these three properties?) Here are two examples: Span Does the following set of vectors span $\mathbb R^4$: $[1,1,0,0],[1,2,-1,1],[0,0,1,1],[2,1,2,-1]$? Now the determinant here is $1$, so the set of vectors span $\mathbb R^4$. Linear Independence Given the following augmented matrix: $$\left[\begin{array}{ccc|c} 1 & 2  & 1 & 0 \\ 0 & -1 & 0 & 0 \\ 0 &  0 & 2 & 0  \end{array}\right], $$ where again the determinant is non-zero ($-2$) so this set S is linearly independent. Of course I am in trouble if you can't make a square matrix - I figure for spans you can just rref it, and I suppose so for linear independence and basis?","Can the determinant (assuming it's non-zero) be used to determine that the vectors given are linearly independent, span the subspace and are a basis of that subspace? (In other words assuming I have a set which I can make into a square matrix, can I use the determinant to determine these three properties?) Here are two examples: Span Does the following set of vectors span $\mathbb R^4$: $[1,1,0,0],[1,2,-1,1],[0,0,1,1],[2,1,2,-1]$? Now the determinant here is $1$, so the set of vectors span $\mathbb R^4$. Linear Independence Given the following augmented matrix: $$\left[\begin{array}{ccc|c} 1 & 2  & 1 & 0 \\ 0 & -1 & 0 & 0 \\ 0 &  0 & 2 & 0  \end{array}\right], $$ where again the determinant is non-zero ($-2$) so this set S is linearly independent. Of course I am in trouble if you can't make a square matrix - I figure for spans you can just rref it, and I suppose so for linear independence and basis?",,"['linear-algebra', 'vector-spaces', 'determinant']"
23,Eigenvalues in orthogonal matrices,Eigenvalues in orthogonal matrices,,"Let $A \in M_n(\Bbb R)$. How can I prove, that 1) if $ \forall {b \in \Bbb R^n}, b^{t}Ab>0$, then all eigenvalues $>0$. 2) if $A$ is orthogonal, then all eigenvalues are equal to $-1$ or $1$","Let $A \in M_n(\Bbb R)$. How can I prove, that 1) if $ \forall {b \in \Bbb R^n}, b^{t}Ab>0$, then all eigenvalues $>0$. 2) if $A$ is orthogonal, then all eigenvalues are equal to $-1$ or $1$",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'orthogonal-matrices']"
24,Cool mathematics I can show to calculus students.,Cool mathematics I can show to calculus students.,,"I am a TA for theoretical linear algebra and calculus course this semester. This is an advanced course for strong freshmen. Every discussion section I am trying to show my students (give them as a series of exercises that we do on the blackboard) some serious math that they can understand and appreciate. For example, when we were talking about cross products, I showed them the isomorphism between $\mathbb{R}^3$ with cross product and Lie algebra $so(3)$. Of course I didn't use fancy language, but we have proved Jacobi identity for cross product, then we picked a basis in skew-symmetric matrices $3\times 3$ and checked that when you commute these three basis vectors, you get exactly the ""right hand rule"" for cross product. So these two things are the same. For the next recitation the topics are 1) eigenvalues and eigenvectors; 2) real quadratic forms. Can you, please, recommend some cool things that we can discuss with them so that they will learn about eigenvalues and forms, but without doing just boring calculations? By ""cool things"" I mean some problems coming from serious mathematics, but when explained properly they might be a nice exercise for the students. Moreover, do you know if there is a book with serious math given in accessible for freshmen form (in the form of exercises would be absolutely great!)? Thank you very much!","I am a TA for theoretical linear algebra and calculus course this semester. This is an advanced course for strong freshmen. Every discussion section I am trying to show my students (give them as a series of exercises that we do on the blackboard) some serious math that they can understand and appreciate. For example, when we were talking about cross products, I showed them the isomorphism between $\mathbb{R}^3$ with cross product and Lie algebra $so(3)$. Of course I didn't use fancy language, but we have proved Jacobi identity for cross product, then we picked a basis in skew-symmetric matrices $3\times 3$ and checked that when you commute these three basis vectors, you get exactly the ""right hand rule"" for cross product. So these two things are the same. For the next recitation the topics are 1) eigenvalues and eigenvectors; 2) real quadratic forms. Can you, please, recommend some cool things that we can discuss with them so that they will learn about eigenvalues and forms, but without doing just boring calculations? By ""cool things"" I mean some problems coming from serious mathematics, but when explained properly they might be a nice exercise for the students. Moreover, do you know if there is a book with serious math given in accessible for freshmen form (in the form of exercises would be absolutely great!)? Thank you very much!",,"['linear-algebra', 'soft-question', 'education', 'advice']"
25,Integer matrices with integer inverses,Integer matrices with integer inverses,,"If all entries of an invertible matrix $A$ are rational, then all the entries of $A^{-1}$ are also rational. Now suppose that all entries of an invertible matrix $A$ are integers. Then it's not necessary that all the entries of $A^{-1}$ are integers. My question is: What are all the invertible integer matrices such that their inverses are also integer matrices?","If all entries of an invertible matrix are rational, then all the entries of are also rational. Now suppose that all entries of an invertible matrix are integers. Then it's not necessary that all the entries of are integers. My question is: What are all the invertible integer matrices such that their inverses are also integer matrices?",A A^{-1} A A^{-1},"['linear-algebra', 'matrices']"
26,Proof that determinant rank equals row/column rank,Proof that determinant rank equals row/column rank,,"Let $A$ be a $m \times n$ matrix with entries from some field $F$. Define the determinant rank of $A$ to be the largest possible size of a nonzero minor, i.e. the size of the largest invertible square submatrix of $A$. It is true that the determinant rank is equal to the rank of a matrix, which we define to be the dimension of the row/column space. It's not difficult to see that $\text{rank} \geq \text{determinant rank}$. If some submatrix of $A$ is invertible, then its columns/rows are linearly indepedent, which implies that the corresponding rows/columns of $A$ are also linearly indepedent. Is there a nice proof for the converse?","Let $A$ be a $m \times n$ matrix with entries from some field $F$. Define the determinant rank of $A$ to be the largest possible size of a nonzero minor, i.e. the size of the largest invertible square submatrix of $A$. It is true that the determinant rank is equal to the rank of a matrix, which we define to be the dimension of the row/column space. It's not difficult to see that $\text{rank} \geq \text{determinant rank}$. If some submatrix of $A$ is invertible, then its columns/rows are linearly indepedent, which implies that the corresponding rows/columns of $A$ are also linearly indepedent. Is there a nice proof for the converse?",,"['linear-algebra', 'matrices', 'determinant', 'matrix-rank']"
27,"If $A$ is full column rank, then $A^TA$ is always invertible","If  is full column rank, then  is always invertible",A A^TA,"I need to prove that If $A$ is full column rank, then $A^TA$ is always invertible. I know when an $m \times n$ matrix is full column rank, then its columns are linearly independent. But nothing more to use to prove the above theorem. I'd appreciate if you could give me some hints.","I need to prove that If $A$ is full column rank, then $A^TA$ is always invertible. I know when an $m \times n$ matrix is full column rank, then its columns are linearly independent. But nothing more to use to prove the above theorem. I'd appreciate if you could give me some hints.",,['linear-algebra']
28,Plücker Relations,Plücker Relations,,"Let $K$ be a field, $1 \leq d \leq n$ integers and $V$ an $n$-dimensional vector space. The Plücker relations are quadratic forms on $\wedge^d V$ whose zero set is exactly the set of decomposable vectors in $\wedge^d V$ (i.e. which are of the form $v_1 \wedge ... \wedge v_d$), thus describing the ideal corresponding to the Plücker embedding $\text{Gr}_d(V) \to \mathbb{P}(\wedge^d V)$. But in every book I've read so far, these Plücker relations are constructed by means of many identifications between duals, exterior powers, etc. so that I am not able to write them down explicitely. Although I've tried it, many signs and sums confuse me. Question. Is it possible to write down these Plücker relations explicitely as a set of polynomials in the ring $K[\{x_H\}]$, where $H$ runs through the subsets of $\{1,...,n\}$ with $d$ elements? (Of course it is possible, but I wonder how do this in general) Edit : Following the answer below, here is the Answer : Instead of using these subsets $H$, use indices $1 \leq i_1 < ... < i_d \leq n$, and extend the definition of $x_{i_1,...,i_d}$ to all $d$-tuples in such a way that $x_{i_1,...,i_d}=0$ if these $i_j$ are not pairwise distinct, and otherwise $x_{i_1,....,i_d} = sign(\sigma) \cdot x_{i_{\sigma(1)},...,i_{\sigma(d)}}$, where $\sigma$ is the unique permutation of $1,...,d$ which makes $i_{\sigma(1)} < ... < i_{\sigma(d)}$. Then the Plücker relations are $\sum\limits_{j=0}^{d} (-1)^j x_{i_1,...,i_{d-1},k_j} * x_{k_0,...,\hat{k_j},...,k_d} = 0$ for integers $i_1,...,i_{d-1},k_0,...,k_d$ between $1,...,n$.","Let $K$ be a field, $1 \leq d \leq n$ integers and $V$ an $n$-dimensional vector space. The Plücker relations are quadratic forms on $\wedge^d V$ whose zero set is exactly the set of decomposable vectors in $\wedge^d V$ (i.e. which are of the form $v_1 \wedge ... \wedge v_d$), thus describing the ideal corresponding to the Plücker embedding $\text{Gr}_d(V) \to \mathbb{P}(\wedge^d V)$. But in every book I've read so far, these Plücker relations are constructed by means of many identifications between duals, exterior powers, etc. so that I am not able to write them down explicitely. Although I've tried it, many signs and sums confuse me. Question. Is it possible to write down these Plücker relations explicitely as a set of polynomials in the ring $K[\{x_H\}]$, where $H$ runs through the subsets of $\{1,...,n\}$ with $d$ elements? (Of course it is possible, but I wonder how do this in general) Edit : Following the answer below, here is the Answer : Instead of using these subsets $H$, use indices $1 \leq i_1 < ... < i_d \leq n$, and extend the definition of $x_{i_1,...,i_d}$ to all $d$-tuples in such a way that $x_{i_1,...,i_d}=0$ if these $i_j$ are not pairwise distinct, and otherwise $x_{i_1,....,i_d} = sign(\sigma) \cdot x_{i_{\sigma(1)},...,i_{\sigma(d)}}$, where $\sigma$ is the unique permutation of $1,...,d$ which makes $i_{\sigma(1)} < ... < i_{\sigma(d)}$. Then the Plücker relations are $\sum\limits_{j=0}^{d} (-1)^j x_{i_1,...,i_{d-1},k_j} * x_{k_0,...,\hat{k_j},...,k_d} = 0$ for integers $i_1,...,i_{d-1},k_0,...,k_d$ between $1,...,n$.",,"['linear-algebra', 'projective-geometry']"
29,Proof that $\text{det}(AB) = \text{det}(A)\text{det}(B)$ without explicit expression for $\text{det}$,Proof that  without explicit expression for,\text{det}(AB) = \text{det}(A)\text{det}(B) \text{det},"Overview I am seeking an approach to linear algebra along the lines of Down with the determinant! by Sheldon Axler. I am following his textbook Linear Algebra Done Right . In these references the author takes an approach to linear algebra that avoids using the explicit expression for determinants as much as possible. The motivation for this is that the the explicit expressions for the determinant, while useful for calculating things, are hard to glean intuition from. This question is not about debating the merits of this approach however! I use Axler's terminology which is a little different from what I am used to. What he calls an isometry I would call unitary and what he calls positive I would call positive semi-definite. Further Motivation I would like to provide further information for why I am seeking this proof. I know that in highly general settings (differential geometry for example) it is possible to perform integrals on manifolds using differential $N$ -forms from $\Lambda^N$ . The essential feature of these objects is that they are alternating and multilinear. I want to understand why there is a deep relationship between alternating and multilinear forms. The suggestion is that this relationship occurs because volumes can be characterized, fundamnetally, as something that transforms in an alternating and multilinear way. Unfortunately I don't find this to be a compelling statement about volumes. See my searching here. Help proving that signed volume of n-parallelepiped is multilinear . Rather, the fact that volumes transform in a multilinear and alternating way is actually hard to prove directly and seems to be a little bit particular to the properties of parallelepipeds specifically. And then of course we can tile $\mathbb{R}^N$ using parallelepipeds so the general reesult follows. What seems more intuitive to me is to ""fundamentally"" characterize the signed volume as something scales linearly with coordinate axis scalings (multiplication by a diagonal matrix) and either stays the same or flips sign under a proper or improper rotation (multiplication by a unitary matrix with positive or negative product of eigenvalues). These two properties SEEM like they can proven by only looking at the eigenvalues of transformation matrices but a proof does not seem evident without relying in some way on multilinear/alternating functions. This would imply that the multilinear/alternating characterization of volumes is somehow more fundamental than the one I am proposing. This is just very surprising to me, especially given how tricky it is to prove directly for volumes... Hence my search for a proof about the product of eigenvalues of a product of matrices. Problem Setup My question: Every complex $N\times N$ matrix $A$ has $N$ eigenvalues $\lambda_1, \ldots, \lambda_N$ . Some of these eigenvalues may be repeated and some of them may be equal to zero. Define the determinant of $A$ as $$ \text{det}(A) = \prod_{i=1}^N\lambda_i $$ It is well known that for two $N\times N$ matrices $A$ and $B$ that $$ \text{det}(AB) = \text{det}(A)\text{det}(B) $$ However, this proof typically (see below) relies on knowing an explicit expression for $\text{det}$ in terms of the matrix elements of $A$ and $B$ such as $$ \text{det}(A) = \sum_{\sigma \in S_N}\prod_{i=1}^N A_{i, \sigma(i)} = \sum_{i_1, \ldots, i_N=1}^N \epsilon_{i_1\ldots i_N} \prod_{j=1}^N A_{j, i_j}. $$ I am curious if there is a proof of $\text{det}(AB)  = \text{det}(A)\text{det}(B)$ using the definition I give above and which does not rely on these explicit alternating summations, e.g. in the vein of the Axler approach. One Attempt at a Solution In his textbook, Axler proves a few key theorems, all without using the alternating expressions above. These include facts about eigenvalues of matrices and various spectral and singular value decomposition theoerems. I think polar decomposition may be useful here. Something like: \begin{align} A =& S_A P_A\\ B =& P_B S_B\\ AB =& S_A P_A P_B S_B \end{align} With $S_{A,B}$ isometries (unitary) and $P_A$ and $P_B$ are positive (positive semi-definite) matrices. With two facts the proof would be complete: If we could prove $\text{det}(PS) = \text{det}(PS) = \text{det}(S)\text{det}(P)$ for isometry $S$ and positive $P$ (or arbitrary $P$ ) then we would have $$ \text{det}(AB) = \text{det}(S_A)\text{det}(S_B)\text{det}(P_A P_B) $$ If we could then also show $\text{det}(P_A P_B) = \text{det}(P_A)\text{det}(P_B)$ then the proof would be complete. Other Attempts at a Solution Some other useful things I've learned: It is clear that multiplying a matrix by a unitary matrix does not change the singular values. Also, if I could prove that the product of eigenvalues or singular values of $C = \Sigma_1 U \Sigma_2$ with $U$ unitary and $\Sigma_{1, 2}$ diagonal and positive, is equal to the product of the product of eigenvalues/singular values of $\Sigma_1$ and $\Sigma_2$ I would be able to show that multiplying $A$ and $B$ arbitrary results in multiplying the product of their singular values. The $\text{det(A)}$ could then alternately be defined as something like $A = U \Sigma V^*$ and $\text{det}(A)$ is equal to the product of diagonal elements on $\Sigma$ times the product of eigenvalues of $U$ and $V$ . Finally another approach. All of my approaches so far have basically relied on properties of matrices and matrix manipulations. It might be that this apporach is misguided. Eigenvalues are related to the characteristic polynomial of a matrix (which Axler defines without needing the determinant) and the fundamental theorem of algebra. Perhaps one needs to return to this algebraic domain to get the answer to what I am looking for. I'm pretty inexperienced in that domain, so any ideas would be greatly helpful. Explicit Questions My questions are: Can someone provide a proof for $\text{det}(PS)=\text{det}(SP) = \text{det}(S)\text{det}(P)$ for isometry $S$ and positive (or arbitary) $P$ ? Can someone provide a proof for $\text{det}(P_A P_B) = \text{det}(P_A)\text{det}(P_B)$ for positive $P_A$ and $P_B$ ? Let $C = \Sigma_1 U \Sigma_2$ with $\Sigma_{1, 2}$ diagonal (with positive non-zero entries) and $U$ unitary. Let $\Pi(A)$ equal the product of the singular values of $A$ . Knowing that $\Pi(UA)=\Pi(A)$ for unitary $U$ , Can someone provide a proof that $\Pi(C) = \Pi(\Sigma_1)\Pi(\Sigma_2)$ ? Alternatively, maybe there's an entirely different approach to proving this without the calculational alternating formula? If someone has reason to believe what I am asking for is impossible for some reason I would appreciate comments or answers explaining why it might be impossible. Proofs involving explicit alternating expressions for $\text{det}$ This website builds up the fact that $\text{det}(A)\text{det}(B)$ by expanding $A$ and $B$ into elementary matrices. It then proves that multiplying a matrix by an elementary matrix multiplies the determinant by a known value. The result follows. The issue is that the effect of multiplying by elementary matrix on the determinant is proven using an expansion by minors definition or property of the determinant. https://sharmaeklavya2.github.io/theoremdep/nodes/linear-algebra/matrices/determinants/elementary-rowop.html Pretty much all the proofs on this MSE use some alternating formula or another How to show that $\det(AB) =\det(A) \det(B)$? Wikipedia and Hubbard and Hubbard give a nice proof which (1) uses knowledge that the determinant is the unique alternating multilinear normalized $N$ -form on the columns of a matrix shows that the function $D(A) = \text{det}(AB)$ is alternating and multilinear so that $D(A)$ must be a multiple of $\text{det}(A)$ . We also have $D(I) = \text{det}(B)$ so $D(A) =\text{det}(AB) = \text{det}(A)\text{det}(B)$ . This proof is nice in that it doesn't rely on the explicit alternating expression for $\text{det}$ , but it has the problem that I don't know how to prove the product of eigenvalues satisfies the alternating multilinear property without providing the explicit alternating expression for $\text{det}$ . Other example of this sort of proof can be found on MSE such as Determinant of matrix product Here is a proof that uses an identity on the levi-civita symbol: proving the determinant of a product of matrices is the product of their determinants in suffix / index notation Recent Insights After thinking more about this problem I think I am becoming convinced that what I am asking for is not possible, but I'm realizing a more convincing way to think about thing. There are three related concepts. (1) How a linear transformation scales volumes, $\text{svol}$ and $\text{vol}$ , (2) The product of eigenvalues or singular values of a linear transformation, $\Pi_e$ and $\Pi_s$ , and (3) the alternating multilinear form, $\text{det}$ and $|\text{det}|$ . Above, I have been hoping to give a treatment in which $\Pi_e$ is somehow the most fundamental property and to derive all properties about $\text{svol}$ and $\text{det}$ from the properties of eigenvalues. As I said above, it's starting to look like this may not be possible. Instead I think a more satisfactory approach may be the following. Take $\text{vol}$ or $\text{svol}$ to be fundamental and work from there. The interesting insight I've had is that there are two ways to break down the properties of $\text{vol}$ or under linear transformations. In the first approach, we can see that (a1) when scaling by a diagonal matrix $\text{vol}$ scales by the absolute value of the product of the entries and $\text{svol}$ scales by the product, (b1) when multiplying by a reflection on a single axis that $\text{vol}$ is unchanged while $\text{svol}$ flips sign, and (c1) both $\text{vol}$ and $\text{svol}$ are unchanged by proper rotations. In the second approach we have that (a2) when scaling by a diagonal matrix $\text{vol}$ scales by the absolute value of the product of the entries and $\text{svol}$ scales by the product, (b2) when two axes are swapped $\text{vol}$ is unchanged while $\text{svol}$ flips sign and (c2) $\text{vol}$ and $\text{svol}$ are both unchanged by shear operations. The first approach lends itself to a polar decomposition of the form $$ A = RUP $$ Where $R$ is either the identity or a reflection across a single axis, $U$ is a special orthogonal matrix, and $P$ is positive-semi-definite. This decomposition lends itself to show the relationship between the volume scaling properties of a transformation and the product of singular values or eigenvalues. The second approach lends itself do a decomposition of the form $$ A = E_1\ldots E_k $$ Where $E_i$ are elementary matrices. This decomposition lends itself to show the relationship between the volume transforming properties of a transformation and the alternating multilinear form. One can see that properties (a1), (a2) are the same, (b1) and (b2) are essentially the same, and that (c1) and (c2) are similar and equivalent when considered in light of the other properties. So the first approach focuses on rotations while the second focuses on shears. Considering positive definite matrices for the moment, I think one way to think about the relationship between the product of eigenvalues, $\Pi_e$ and the alternating multilinear form $\text{det}$ is that they are equivalent because (1) $\Pi_e$ is related to rotations and axis stretching of volumes and (2) $\text{det}$ is related to shears and axis stretching of volumes. It is still a little bit unsatisfying that we would need to make this relationship with volumes and shears to explain why $\Pi_e(AB) = \Pi_e(A) \Pi_e(B)$ , but one reason I could see for it is that $\Pi_e$ is a very complicated object algebraically. The existence of eigenvalues is only guaranteed by the very non-constructive fundamental theorem of algebra and triangularizability of complex matrices also feels a bit abstract. We know how to work with a transformation and a single eigenvalue or eigenvector, but there is not much machinery to work with ALL the eigenvalues at once, i.e. $\Pi_e$ . Rather, we can use the fact that the product of eigenvalues stretch volumes, and the fact that signed volume are alternating and multilinear, to get at an explicit expression for the product of eigenvalues that allows us to derive properties we wouldn't be able to otherwise, such as $\Pi_e(AB) = \Pi_e(A)\Pi_e(B)$ . Curious for others' thoughts on this. I may be asking (and answering perhaps) a new question related to this last point. I have some partial proofs and will hopefully have a full, hopefully intuitively satisfying, explanation a bit later but not sure.","Overview I am seeking an approach to linear algebra along the lines of Down with the determinant! by Sheldon Axler. I am following his textbook Linear Algebra Done Right . In these references the author takes an approach to linear algebra that avoids using the explicit expression for determinants as much as possible. The motivation for this is that the the explicit expressions for the determinant, while useful for calculating things, are hard to glean intuition from. This question is not about debating the merits of this approach however! I use Axler's terminology which is a little different from what I am used to. What he calls an isometry I would call unitary and what he calls positive I would call positive semi-definite. Further Motivation I would like to provide further information for why I am seeking this proof. I know that in highly general settings (differential geometry for example) it is possible to perform integrals on manifolds using differential -forms from . The essential feature of these objects is that they are alternating and multilinear. I want to understand why there is a deep relationship between alternating and multilinear forms. The suggestion is that this relationship occurs because volumes can be characterized, fundamnetally, as something that transforms in an alternating and multilinear way. Unfortunately I don't find this to be a compelling statement about volumes. See my searching here. Help proving that signed volume of n-parallelepiped is multilinear . Rather, the fact that volumes transform in a multilinear and alternating way is actually hard to prove directly and seems to be a little bit particular to the properties of parallelepipeds specifically. And then of course we can tile using parallelepipeds so the general reesult follows. What seems more intuitive to me is to ""fundamentally"" characterize the signed volume as something scales linearly with coordinate axis scalings (multiplication by a diagonal matrix) and either stays the same or flips sign under a proper or improper rotation (multiplication by a unitary matrix with positive or negative product of eigenvalues). These two properties SEEM like they can proven by only looking at the eigenvalues of transformation matrices but a proof does not seem evident without relying in some way on multilinear/alternating functions. This would imply that the multilinear/alternating characterization of volumes is somehow more fundamental than the one I am proposing. This is just very surprising to me, especially given how tricky it is to prove directly for volumes... Hence my search for a proof about the product of eigenvalues of a product of matrices. Problem Setup My question: Every complex matrix has eigenvalues . Some of these eigenvalues may be repeated and some of them may be equal to zero. Define the determinant of as It is well known that for two matrices and that However, this proof typically (see below) relies on knowing an explicit expression for in terms of the matrix elements of and such as I am curious if there is a proof of using the definition I give above and which does not rely on these explicit alternating summations, e.g. in the vein of the Axler approach. One Attempt at a Solution In his textbook, Axler proves a few key theorems, all without using the alternating expressions above. These include facts about eigenvalues of matrices and various spectral and singular value decomposition theoerems. I think polar decomposition may be useful here. Something like: With isometries (unitary) and and are positive (positive semi-definite) matrices. With two facts the proof would be complete: If we could prove for isometry and positive (or arbitrary ) then we would have If we could then also show then the proof would be complete. Other Attempts at a Solution Some other useful things I've learned: It is clear that multiplying a matrix by a unitary matrix does not change the singular values. Also, if I could prove that the product of eigenvalues or singular values of with unitary and diagonal and positive, is equal to the product of the product of eigenvalues/singular values of and I would be able to show that multiplying and arbitrary results in multiplying the product of their singular values. The could then alternately be defined as something like and is equal to the product of diagonal elements on times the product of eigenvalues of and . Finally another approach. All of my approaches so far have basically relied on properties of matrices and matrix manipulations. It might be that this apporach is misguided. Eigenvalues are related to the characteristic polynomial of a matrix (which Axler defines without needing the determinant) and the fundamental theorem of algebra. Perhaps one needs to return to this algebraic domain to get the answer to what I am looking for. I'm pretty inexperienced in that domain, so any ideas would be greatly helpful. Explicit Questions My questions are: Can someone provide a proof for for isometry and positive (or arbitary) ? Can someone provide a proof for for positive and ? Let with diagonal (with positive non-zero entries) and unitary. Let equal the product of the singular values of . Knowing that for unitary , Can someone provide a proof that ? Alternatively, maybe there's an entirely different approach to proving this without the calculational alternating formula? If someone has reason to believe what I am asking for is impossible for some reason I would appreciate comments or answers explaining why it might be impossible. Proofs involving explicit alternating expressions for This website builds up the fact that by expanding and into elementary matrices. It then proves that multiplying a matrix by an elementary matrix multiplies the determinant by a known value. The result follows. The issue is that the effect of multiplying by elementary matrix on the determinant is proven using an expansion by minors definition or property of the determinant. https://sharmaeklavya2.github.io/theoremdep/nodes/linear-algebra/matrices/determinants/elementary-rowop.html Pretty much all the proofs on this MSE use some alternating formula or another How to show that $\det(AB) =\det(A) \det(B)$? Wikipedia and Hubbard and Hubbard give a nice proof which (1) uses knowledge that the determinant is the unique alternating multilinear normalized -form on the columns of a matrix shows that the function is alternating and multilinear so that must be a multiple of . We also have so . This proof is nice in that it doesn't rely on the explicit alternating expression for , but it has the problem that I don't know how to prove the product of eigenvalues satisfies the alternating multilinear property without providing the explicit alternating expression for . Other example of this sort of proof can be found on MSE such as Determinant of matrix product Here is a proof that uses an identity on the levi-civita symbol: proving the determinant of a product of matrices is the product of their determinants in suffix / index notation Recent Insights After thinking more about this problem I think I am becoming convinced that what I am asking for is not possible, but I'm realizing a more convincing way to think about thing. There are three related concepts. (1) How a linear transformation scales volumes, and , (2) The product of eigenvalues or singular values of a linear transformation, and , and (3) the alternating multilinear form, and . Above, I have been hoping to give a treatment in which is somehow the most fundamental property and to derive all properties about and from the properties of eigenvalues. As I said above, it's starting to look like this may not be possible. Instead I think a more satisfactory approach may be the following. Take or to be fundamental and work from there. The interesting insight I've had is that there are two ways to break down the properties of or under linear transformations. In the first approach, we can see that (a1) when scaling by a diagonal matrix scales by the absolute value of the product of the entries and scales by the product, (b1) when multiplying by a reflection on a single axis that is unchanged while flips sign, and (c1) both and are unchanged by proper rotations. In the second approach we have that (a2) when scaling by a diagonal matrix scales by the absolute value of the product of the entries and scales by the product, (b2) when two axes are swapped is unchanged while flips sign and (c2) and are both unchanged by shear operations. The first approach lends itself to a polar decomposition of the form Where is either the identity or a reflection across a single axis, is a special orthogonal matrix, and is positive-semi-definite. This decomposition lends itself to show the relationship between the volume scaling properties of a transformation and the product of singular values or eigenvalues. The second approach lends itself do a decomposition of the form Where are elementary matrices. This decomposition lends itself to show the relationship between the volume transforming properties of a transformation and the alternating multilinear form. One can see that properties (a1), (a2) are the same, (b1) and (b2) are essentially the same, and that (c1) and (c2) are similar and equivalent when considered in light of the other properties. So the first approach focuses on rotations while the second focuses on shears. Considering positive definite matrices for the moment, I think one way to think about the relationship between the product of eigenvalues, and the alternating multilinear form is that they are equivalent because (1) is related to rotations and axis stretching of volumes and (2) is related to shears and axis stretching of volumes. It is still a little bit unsatisfying that we would need to make this relationship with volumes and shears to explain why , but one reason I could see for it is that is a very complicated object algebraically. The existence of eigenvalues is only guaranteed by the very non-constructive fundamental theorem of algebra and triangularizability of complex matrices also feels a bit abstract. We know how to work with a transformation and a single eigenvalue or eigenvector, but there is not much machinery to work with ALL the eigenvalues at once, i.e. . Rather, we can use the fact that the product of eigenvalues stretch volumes, and the fact that signed volume are alternating and multilinear, to get at an explicit expression for the product of eigenvalues that allows us to derive properties we wouldn't be able to otherwise, such as . Curious for others' thoughts on this. I may be asking (and answering perhaps) a new question related to this last point. I have some partial proofs and will hopefully have a full, hopefully intuitively satisfying, explanation a bit later but not sure.","N \Lambda^N \mathbb{R}^N N\times N A N \lambda_1, \ldots, \lambda_N A 
\text{det}(A) = \prod_{i=1}^N\lambda_i
 N\times N A B 
\text{det}(AB) = \text{det}(A)\text{det}(B)
 \text{det} A B 
\text{det}(A) = \sum_{\sigma \in S_N}\prod_{i=1}^N A_{i, \sigma(i)} = \sum_{i_1, \ldots, i_N=1}^N \epsilon_{i_1\ldots i_N} \prod_{j=1}^N A_{j, i_j}.
 \text{det}(AB)  = \text{det}(A)\text{det}(B) \begin{align}
A =& S_A P_A\\
B =& P_B S_B\\
AB =& S_A P_A P_B S_B
\end{align} S_{A,B} P_A P_B \text{det}(PS) = \text{det}(PS) = \text{det}(S)\text{det}(P) S P P 
\text{det}(AB) = \text{det}(S_A)\text{det}(S_B)\text{det}(P_A P_B)
 \text{det}(P_A P_B) = \text{det}(P_A)\text{det}(P_B) C = \Sigma_1 U \Sigma_2 U \Sigma_{1, 2} \Sigma_1 \Sigma_2 A B \text{det(A)} A = U \Sigma V^* \text{det}(A) \Sigma U V \text{det}(PS)=\text{det}(SP) = \text{det}(S)\text{det}(P) S P \text{det}(P_A P_B) = \text{det}(P_A)\text{det}(P_B) P_A P_B C = \Sigma_1 U \Sigma_2 \Sigma_{1, 2} U \Pi(A) A \Pi(UA)=\Pi(A) U \Pi(C) = \Pi(\Sigma_1)\Pi(\Sigma_2) \text{det} \text{det}(A)\text{det}(B) A B N D(A) = \text{det}(AB) D(A) \text{det}(A) D(I) = \text{det}(B) D(A) =\text{det}(AB) = \text{det}(A)\text{det}(B) \text{det} \text{det} \text{svol} \text{vol} \Pi_e \Pi_s \text{det} |\text{det}| \Pi_e \text{svol} \text{det} \text{vol} \text{svol} \text{vol} \text{vol} \text{svol} \text{vol} \text{svol} \text{vol} \text{svol} \text{vol} \text{svol} \text{vol} \text{svol} \text{vol} \text{svol} 
A = RUP
 R U P 
A = E_1\ldots E_k
 E_i \Pi_e \text{det} \Pi_e \text{det} \Pi_e(AB) = \Pi_e(A) \Pi_e(B) \Pi_e \Pi_e \Pi_e(AB) = \Pi_e(A)\Pi_e(B)","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'determinant', 'matrix-decomposition']"
30,Determining the kernel of a Vandermonde-like matrix,Determining the kernel of a Vandermonde-like matrix,,"The kernel of a Vandermonde matrix can be determined using this formula. The following type of matrix has a similar structure, and should also have a one-dimensional kernel. $$V=  \begin{bmatrix}  1 & 1 & 1 & \ldots & 1 \\ x_1 & x_2 & x_3 & \ldots & x_n \\ x_1^2 & x_2^2 & x_3^2 & \ldots & x_n^2 \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ x_1^{m-1} & x_2^{m-1} & x_3^{m-1} & \ldots & x_n^{m-1}\\ y_1 & y_2 & y_3 & \ldots & y_n \\ y_1x_1 & y_2x_2 & y_3x_3 & \ldots & y_nx_n \\ y_1x_1^2 & y_2x_2^2 & y_3x_3^2 & \ldots & y_nx_n^2 \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ y_1x_1^{m-1} & y_2x_2^{m-1} & y_3x_3^{m-1} & \ldots & y_nx_n^{m-1}\\ y_1^2x_1 & y_2^2x_2 & y_3^2x_3 & \ldots & y_n^2x_n\\ \vdots & \vdots & \vdots & \ddots & \vdots \\ y_1^{m-1}x_1^{m-1} & y_2^{m-1}x_2^{m-1} & y_3^{m-1}x_3^{m-1} & \ldots & y_n^{m-1}x_n^{m-1}\\ \end{bmatrix} \in \mathbb{R}^{(n-1)\times n}$$ where $n = m^2+1$ and $(x_i, y_i) \neq (x_j, y_j)$ for $i \neq j$ ; i.e. there are $m$ groups of $m$ rows with all possible combinations of powers $y^ax^b$ and one more column than rows. Does a similar analytical form exist for it? Or, would additional constraints be required, like $x_i^ay_i^b \neq x_i^cy_i^d$ for $i \neq j$ ? ( Crossposted to MO )","The kernel of a Vandermonde matrix can be determined using this formula. The following type of matrix has a similar structure, and should also have a one-dimensional kernel. where and for ; i.e. there are groups of rows with all possible combinations of powers and one more column than rows. Does a similar analytical form exist for it? Or, would additional constraints be required, like for ? ( Crossposted to MO )","V= 
\begin{bmatrix} 
1 & 1 & 1 & \ldots & 1 \\
x_1 & x_2 & x_3 & \ldots & x_n \\
x_1^2 & x_2^2 & x_3^2 & \ldots & x_n^2 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
x_1^{m-1} & x_2^{m-1} & x_3^{m-1} & \ldots & x_n^{m-1}\\
y_1 & y_2 & y_3 & \ldots & y_n \\
y_1x_1 & y_2x_2 & y_3x_3 & \ldots & y_nx_n \\
y_1x_1^2 & y_2x_2^2 & y_3x_3^2 & \ldots & y_nx_n^2 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
y_1x_1^{m-1} & y_2x_2^{m-1} & y_3x_3^{m-1} & \ldots & y_nx_n^{m-1}\\
y_1^2x_1 & y_2^2x_2 & y_3^2x_3 & \ldots & y_n^2x_n\\
\vdots & \vdots & \vdots & \ddots & \vdots \\
y_1^{m-1}x_1^{m-1} & y_2^{m-1}x_2^{m-1} & y_3^{m-1}x_3^{m-1} & \ldots & y_n^{m-1}x_n^{m-1}\\
\end{bmatrix} \in \mathbb{R}^{(n-1)\times n} n = m^2+1 (x_i, y_i) \neq (x_j, y_j) i \neq j m m y^ax^b x_i^ay_i^b \neq x_i^cy_i^d i \neq j","['linear-algebra', 'matrices', 'determinant']"
31,Vandermonde determinant by induction,Vandermonde determinant by induction,,"For $n$ variables $x_1,\ldots,x_n$ , the determinant $$  \det\left((x_i^{j-1})_{i,j=1}^n\right) =   \left|\begin{matrix}      1&x_1&x_1^2&\cdots & x_1^{n-1}\\      1&x_2&x_2^2&\cdots & x_2^{n-1} \\      \vdots&\vdots&\vdots&\ddots&\vdots\\      1&x_{n-1}&x_{n-1}^2&\cdots&x_{n-1}^{n-1}\\      1 &x_n&x_n^2&\cdots&x_n^{n-1}   \end{matrix}\right| $$ can be computed by induction; the image below says it shows that. I have done this before, if I submit this will I get marks? MORE IMPORTANTLY how do I do it by induction? The ""hint"" is to get the first row to $(1,0,0,...,0)$ . I think there are the grounds of induction in there, but I'm not sure how (I'm not very confident with induction when the structure isn't shown for $n=k$ , assume for $n=r$ , show if $n=r$ then $n=r+1$ is true. By the way the question is to show the determinant at the start equals the product $$\prod_{1\leq i<j\leq n}(x_j-x_i)$$ (but again, explicitly by induction)","For variables , the determinant can be computed by induction; the image below says it shows that. I have done this before, if I submit this will I get marks? MORE IMPORTANTLY how do I do it by induction? The ""hint"" is to get the first row to . I think there are the grounds of induction in there, but I'm not sure how (I'm not very confident with induction when the structure isn't shown for , assume for , show if then is true. By the way the question is to show the determinant at the start equals the product (but again, explicitly by induction)","n x_1,\ldots,x_n 
 \det\left((x_i^{j-1})_{i,j=1}^n\right) =
  \left|\begin{matrix}
     1&x_1&x_1^2&\cdots & x_1^{n-1}\\
     1&x_2&x_2^2&\cdots & x_2^{n-1} \\
     \vdots&\vdots&\vdots&\ddots&\vdots\\
     1&x_{n-1}&x_{n-1}^2&\cdots&x_{n-1}^{n-1}\\
     1 &x_n&x_n^2&\cdots&x_n^{n-1}
  \end{matrix}\right|
 (1,0,0,...,0) n=k n=r n=r n=r+1 \prod_{1\leq i<j\leq n}(x_j-x_i)","['linear-algebra', 'matrices', 'induction', 'determinant']"
32,Intuition behind a matrix being invertible iff its determinant is non-zero,Intuition behind a matrix being invertible iff its determinant is non-zero,,"Question I have been wondering about this question since I was in school. How can one number tell so much about the whole matrix being invertible or not? I know the proof of this statement now. But I would like to know the intuition behind this result and why this result is actually true. My Proof If $A$ is invertible, then $$ 1 = \det(I) = \det(AA^{-1}) = \det(A)\cdot\det(A^{-1})$$ whence $\det(A) \neq 0$ . Conversely, if $\det(A) \neq 0$ , we have $$ A adj(A) = adj(A)A = \det(A)I$$ whence $A$ is invertible. $adj(A)$ is the adjugate matrix of $A$ . $$ adj(A)_{ji} = (-1)^{i+j}\det(A_{ij})$$ where $A_{ij}$ is the matrix obtained from $A$ by deleting $ith$ row and $jth$ column. Any other insightful proofs are also welcome.","Question I have been wondering about this question since I was in school. How can one number tell so much about the whole matrix being invertible or not? I know the proof of this statement now. But I would like to know the intuition behind this result and why this result is actually true. My Proof If is invertible, then whence . Conversely, if , we have whence is invertible. is the adjugate matrix of . where is the matrix obtained from by deleting row and column. Any other insightful proofs are also welcome.",A  1 = \det(I) = \det(AA^{-1}) = \det(A)\cdot\det(A^{-1}) \det(A) \neq 0 \det(A) \neq 0  A adj(A) = adj(A)A = \det(A)I A adj(A) A  adj(A)_{ji} = (-1)^{i+j}\det(A_{ij}) A_{ij} A ith jth,"['linear-algebra', 'matrices', 'determinant', 'intuition']"
33,Why does the $n$-th power of a Jordan matrix involve the binomial coefficient?,Why does the -th power of a Jordan matrix involve the binomial coefficient?,n,"I've searched a lot for a simple explanation of this. Given a Jordan block $J_k(\lambda)$, its $n$-th power is: $$ J_k(\lambda)^n = \begin{bmatrix} \lambda^n & \binom{n}{1}\lambda^{n-1} & \binom{n}{2}\lambda^{n-2} & \cdots & \cdots & \binom{n}{k-1}\lambda^{n-k+1} \\  & \lambda^n & \binom{n}{1}\lambda^{n-1} & \cdots & \cdots & \binom{n}{k-2}\lambda^{n-k+2} \\  &  & \ddots & \ddots & \vdots & \vdots\\  &  & & \ddots & \ddots & \vdots\\  &  & &  & \lambda^n & \binom{n}{1}\lambda^{n-1}\\  &  &  &  &  & \lambda^n \end{bmatrix}$$ Why does the $n$th power involve the binomial coefficient?","I've searched a lot for a simple explanation of this. Given a Jordan block $J_k(\lambda)$, its $n$-th power is: $$ J_k(\lambda)^n = \begin{bmatrix} \lambda^n & \binom{n}{1}\lambda^{n-1} & \binom{n}{2}\lambda^{n-2} & \cdots & \cdots & \binom{n}{k-1}\lambda^{n-k+1} \\  & \lambda^n & \binom{n}{1}\lambda^{n-1} & \cdots & \cdots & \binom{n}{k-2}\lambda^{n-k+2} \\  &  & \ddots & \ddots & \vdots & \vdots\\  &  & & \ddots & \ddots & \vdots\\  &  & &  & \lambda^n & \binom{n}{1}\lambda^{n-1}\\  &  &  &  &  & \lambda^n \end{bmatrix}$$ Why does the $n$th power involve the binomial coefficient?",,"['linear-algebra', 'matrices', 'binomial-coefficients', 'exponentiation', 'jordan-normal-form']"
34,The milk sharing problem,The milk sharing problem,,"I found a book with math quizzes. It was my father's when he was young. I encountered a problem with the following quiz. I solved it, but I wonder, is there a faster way to do it? If so, how can I compute the time (polynomial time) that is needed to solve it? Can we build an algorithm? The problem is this one: Two brothers have a cow that produces 10 kilos of milk per day. Because they are fair, they share the milk and each one gets 5 kilos. They only have available three bottles. $A$ that fits 10 kilos. $B$ that fits 7 kilos. $C$ that fits 3 kilos. How do they share it? What I did is these steps: Bottle Sizes       A   B   C      10   7   3  Moves    Bottles        What we do:  1st    10   0   0      Fill the bottle A.  2nd     7   0   3      Fill the bottle C.  3rd     7   3   0      Empty the index of C in B.  4th     4   3   3      Refill C.  5th     4   6   0      Empty C in B.  6th     1   6   3      Refill C from A.  7th     1   7   2      Refill B from C.  8th     8   0   2      Empty the index of B in A.  9th     8   2   0      Empty the index of C in B. 10th     5   2   3      Refill C from A. 11th     5   5   0      Empty C in B.","I found a book with math quizzes. It was my father's when he was young. I encountered a problem with the following quiz. I solved it, but I wonder, is there a faster way to do it? If so, how can I compute the time (polynomial time) that is needed to solve it? Can we build an algorithm? The problem is this one: Two brothers have a cow that produces 10 kilos of milk per day. Because they are fair, they share the milk and each one gets 5 kilos. They only have available three bottles. $A$ that fits 10 kilos. $B$ that fits 7 kilos. $C$ that fits 3 kilos. How do they share it? What I did is these steps: Bottle Sizes       A   B   C      10   7   3  Moves    Bottles        What we do:  1st    10   0   0      Fill the bottle A.  2nd     7   0   3      Fill the bottle C.  3rd     7   3   0      Empty the index of C in B.  4th     4   3   3      Refill C.  5th     4   6   0      Empty C in B.  6th     1   6   3      Refill C from A.  7th     1   7   2      Refill B from C.  8th     8   0   2      Empty the index of B in A.  9th     8   2   0      Empty the index of C in B. 10th     5   2   3      Refill C from A. 11th     5   5   0      Empty C in B.",,"['linear-algebra', 'algorithms']"
35,Intuition for the Cauchy-Schwarz inequality,Intuition for the Cauchy-Schwarz inequality,,"I'm not looking for a mathematical proof; I'm looking for a visual one. I'm having trouble understanding (in my mind's eye) why the dot product of two vectors V and W produces a scalar that is less than the length of V multiplied by the length of W. In using the dot product, we are producing a parallel vector, correct? Could we not further say that we are simply applying vector W to vector V in order to produce a vector that is the original length of V multiplied by the length of W -- thus a vector parallel to V? For example, if we let vector W be a unit vector (with length of one), then the dot product of V and W would give us a scalar that, when applied to V, produces V again. Would this not be the same as the length of V multiplied by the length of W (given that the length of W is equal to one)? For that reason, why wouldn't the dot product of V and W always be equal to the length of V multiplied by the length of W? Why would it be less (unless V = cW for any scalar c?)","I'm not looking for a mathematical proof; I'm looking for a visual one. I'm having trouble understanding (in my mind's eye) why the dot product of two vectors V and W produces a scalar that is less than the length of V multiplied by the length of W. In using the dot product, we are producing a parallel vector, correct? Could we not further say that we are simply applying vector W to vector V in order to produce a vector that is the original length of V multiplied by the length of W -- thus a vector parallel to V? For example, if we let vector W be a unit vector (with length of one), then the dot product of V and W would give us a scalar that, when applied to V, produces V again. Would this not be the same as the length of V multiplied by the length of W (given that the length of W is equal to one)? For that reason, why wouldn't the dot product of V and W always be equal to the length of V multiplied by the length of W? Why would it be less (unless V = cW for any scalar c?)",,"['linear-algebra', 'inequality', 'inner-products', 'cauchy-schwarz-inequality']"
36,What can be said about the dual space of an infinite-dimensional real vector space?,What can be said about the dual space of an infinite-dimensional real vector space?,,What can be said about the dual space of an infinite-dimensional real vector space? Are both spaces isomorphic? Or shall we have something like the dual of the dual is isomorphic to the initial vector space (same as with the perpendicular subspace in a Hilbert space)?,What can be said about the dual space of an infinite-dimensional real vector space? Are both spaces isomorphic? Or shall we have something like the dual of the dual is isomorphic to the initial vector space (same as with the perpendicular subspace in a Hilbert space)?,,"['linear-algebra', 'duality-theorems']"
37,Why are orthogonal matrices generalizations of rotations and reflections?,Why are orthogonal matrices generalizations of rotations and reflections?,,"I recently took linear algebra course, all that I learned about orthogonal matrix is that Q transposed is Q inverse, and therefore it has a nice computational property. Recently, to my surprise, I learned that transformations by orthogonal matrices are generalizations of rotations and reflections. They preserve lengths and angles. Why is this true? On a side note, I would like to learn more about linear algebra, in particular with emphasis on visualization or geometrical interpretations such as above.Is there any good textbook or resources for that?","I recently took linear algebra course, all that I learned about orthogonal matrix is that Q transposed is Q inverse, and therefore it has a nice computational property. Recently, to my surprise, I learned that transformations by orthogonal matrices are generalizations of rotations and reflections. They preserve lengths and angles. Why is this true? On a side note, I would like to learn more about linear algebra, in particular with emphasis on visualization or geometrical interpretations such as above.Is there any good textbook or resources for that?",,"['linear-algebra', 'matrices', 'rotations', 'reflection', 'orthogonal-matrices']"
38,Transpose of block matrix,Transpose of block matrix,,"I'm attempting to prove that $$ \left[ \begin{array}{c c} A & B \\ C & D \\ \end{array} \right]^\top = \left[ \begin{array}{c c} A^\top & C^\top \\ B^\top & D^\top \\ \end{array} \right]. $$ Intuitively, I can see that it's true. However, when I try to formally prove it, I quickly get lost in the indices. What tricks can I use to keep things straight? Source: Exercise 2.6.16, P116, Intro to Linear Algebra, 4th Ed by Strang","I'm attempting to prove that $$ \left[ \begin{array}{c c} A & B \\ C & D \\ \end{array} \right]^\top = \left[ \begin{array}{c c} A^\top & C^\top \\ B^\top & D^\top \\ \end{array} \right]. $$ Intuitively, I can see that it's true. However, when I try to formally prove it, I quickly get lost in the indices. What tricks can I use to keep things straight? Source: Exercise 2.6.16, P116, Intro to Linear Algebra, 4th Ed by Strang",,"['linear-algebra', 'matrices', 'proof-writing', 'transpose']"
39,How does Dummit and Foote's abstract algebra text compare to others? [closed],How does Dummit and Foote's abstract algebra text compare to others? [closed],,"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 8 years ago . Improve this question I am looking for a good book on abstract algebra (and if possible linear algebra). Obviously as most of these texts are fairly expensive I want to know for sure which one is best for me. Could someone here give me a rough overview of the strengths and weaknesses of Dummit and Foote's ""abstract algebra"" compared with, for instance, Fraleigh's ""A first course in abstract algebra"" and maybe give some advice as to which is best for my current level. I'm not yet an undergraduate, but I've read the book ""An introduction to abstract algebra"" by W. Nicholson, as well as having done many of the exercises. The book seems to cover a lot of the introductory stuff for groups, rings and fields, as well as coverage of other material such as the sylow theorems and some Galois Theory. I want to move onto a book which is more advanced, though preferably one that I can successfully self study and which maybe contains the introductory stuff so I can review it (I don't $\textit{own}$ my textbook, I have to give it back soon). I am also reading some introductory analysis, but any textbook which does not reference too much analysis without explanation would be good. If linear algebra is not contained in the book, could one also direct me to a suitable text on that, please? Thank you","Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 8 years ago . Improve this question I am looking for a good book on abstract algebra (and if possible linear algebra). Obviously as most of these texts are fairly expensive I want to know for sure which one is best for me. Could someone here give me a rough overview of the strengths and weaknesses of Dummit and Foote's ""abstract algebra"" compared with, for instance, Fraleigh's ""A first course in abstract algebra"" and maybe give some advice as to which is best for my current level. I'm not yet an undergraduate, but I've read the book ""An introduction to abstract algebra"" by W. Nicholson, as well as having done many of the exercises. The book seems to cover a lot of the introductory stuff for groups, rings and fields, as well as coverage of other material such as the sylow theorems and some Galois Theory. I want to move onto a book which is more advanced, though preferably one that I can successfully self study and which maybe contains the introductory stuff so I can review it (I don't $\textit{own}$ my textbook, I have to give it back soon). I am also reading some introductory analysis, but any textbook which does not reference too much analysis without explanation would be good. If linear algebra is not contained in the book, could one also direct me to a suitable text on that, please? Thank you",,"['linear-algebra', 'abstract-algebra', 'book-recommendation']"
40,When does the inverse of a covariance matrix exist?,When does the inverse of a covariance matrix exist?,,We know that a square matrix is a covariance matrix of some random vector if and only if it is symmetric and positive semi-definite (see Covariance matrix ). We also know that every symmetric positive definite matrix is invertible (see Positive definite ). It seems that the inverse of a covariance matrix sometimes does not exist. Does the inverse of a covariance matrix exist if and only if the covariance matrix is positive definite? How can I intuitively understand the situation when the inverse of a covariance matrix does not exist (does it mean that some of the random variables of the random vector are equal to a constant almost surely)? Any help will be much appreciated!,We know that a square matrix is a covariance matrix of some random vector if and only if it is symmetric and positive semi-definite (see Covariance matrix ). We also know that every symmetric positive definite matrix is invertible (see Positive definite ). It seems that the inverse of a covariance matrix sometimes does not exist. Does the inverse of a covariance matrix exist if and only if the covariance matrix is positive definite? How can I intuitively understand the situation when the inverse of a covariance matrix does not exist (does it mean that some of the random variables of the random vector are equal to a constant almost surely)? Any help will be much appreciated!,,"['linear-algebra', 'matrices', 'probability-theory', 'covariance', 'positive-definite']"
41,How many k-dimensional subspaces there are in n-dimensional vector space over $\mathbb F_p$?,How many k-dimensional subspaces there are in n-dimensional vector space over ?,\mathbb F_p,"I am asked to find how many there are $k$-dimensional subspaces in vector space $V$ over $\mathbb F_p$, $\dim V = n$. My attempt: 1) Let's find a total number of elements in $V$: assume that $\{v_1, v_2, \cdots, v_n\}$ is a basis in $V$. Then, for every $v \in V$ we can write down $$ v = a_1 v_1 + a_2 v_2 + \cdots + a_n v_n $$ and since the coordinates ($a_1, \cdots, a_n$) are from $\mathbb F_p$ there are $p^n$ vectors in $V$;  $p-1$ without the zero vector. 2) Let's look at a situation where $k=1$. Let's call this 1-dimensional space $V'$. $$\forall v' \in V'. v' = a_1 v_1$$ where $v_1$ is a basis in $V'$. We know that if there are two non-zero vectors $u \in V'_1$ and $v \in V'_2$,  they are not linear dependant. So, every 1-dimensional subspace has $(p-1)$ basises. Therefore, there are $\frac{p^n - 1}{p-1}$ possible 1-dimensional subspaces in $V$ 3) k-dimensional subspace is defined by the set of it's basises. Since basis can not contain zero vectors we can write down the formula for selecting $k$ linear independent vectors: $C^k_m (p-1)^k$, where $m = \frac{p^n - 1}{p-1}$. Here we first choose $k$ 1-dimentioanl subspaces and then we choose one of $(p-1)$ non-zero vectors from each of the subspaces. 4) .. unfortunately, this is where I am stuck. My intuition says that the answer may be  $\frac{p^n - 1}{(p-1)^k}$, but this might be completely wrong and I don't know how to go about finishing the problem. Thanks in advance.","I am asked to find how many there are $k$-dimensional subspaces in vector space $V$ over $\mathbb F_p$, $\dim V = n$. My attempt: 1) Let's find a total number of elements in $V$: assume that $\{v_1, v_2, \cdots, v_n\}$ is a basis in $V$. Then, for every $v \in V$ we can write down $$ v = a_1 v_1 + a_2 v_2 + \cdots + a_n v_n $$ and since the coordinates ($a_1, \cdots, a_n$) are from $\mathbb F_p$ there are $p^n$ vectors in $V$;  $p-1$ without the zero vector. 2) Let's look at a situation where $k=1$. Let's call this 1-dimensional space $V'$. $$\forall v' \in V'. v' = a_1 v_1$$ where $v_1$ is a basis in $V'$. We know that if there are two non-zero vectors $u \in V'_1$ and $v \in V'_2$,  they are not linear dependant. So, every 1-dimensional subspace has $(p-1)$ basises. Therefore, there are $\frac{p^n - 1}{p-1}$ possible 1-dimensional subspaces in $V$ 3) k-dimensional subspace is defined by the set of it's basises. Since basis can not contain zero vectors we can write down the formula for selecting $k$ linear independent vectors: $C^k_m (p-1)^k$, where $m = \frac{p^n - 1}{p-1}$. Here we first choose $k$ 1-dimentioanl subspaces and then we choose one of $(p-1)$ non-zero vectors from each of the subspaces. 4) .. unfortunately, this is where I am stuck. My intuition says that the answer may be  $\frac{p^n - 1}{(p-1)^k}$, but this might be completely wrong and I don't know how to go about finishing the problem. Thanks in advance.",,['linear-algebra']
42,Why does the inverse of the Hilbert matrix have integer entries?,Why does the inverse of the Hilbert matrix have integer entries?,,Let $A$ be the $n\times n$ matrix given by $$A_{ij}=\frac{1}{i + j - 1}$$ Show that $A$ is invertible and that the inverse has integer entries. I was able to show that $A$ is invertible. How do I show that $A^{-1}$ has integer entries? This matrix is called the Hilbert matrix. The problem appears as exercise 12 in section 1.6 of Hoffman and Kunze's Linear Algebra (2nd edition).,Let be the matrix given by Show that is invertible and that the inverse has integer entries. I was able to show that is invertible. How do I show that has integer entries? This matrix is called the Hilbert matrix. The problem appears as exercise 12 in section 1.6 of Hoffman and Kunze's Linear Algebra (2nd edition).,A n\times n A_{ij}=\frac{1}{i + j - 1} A A A^{-1},"['linear-algebra', 'matrices', 'inverse', 'hilbert-matrices']"
43,What is the geometric meaning of this vector equality? $\vec{BC}\cdot\vec{AD}+\vec{CA}\cdot\vec{BD}+\vec{AB}\cdot\vec{CD}=0$,What is the geometric meaning of this vector equality?,\vec{BC}\cdot\vec{AD}+\vec{CA}\cdot\vec{BD}+\vec{AB}\cdot\vec{CD}=0,"I was doing some exercises for linear algebra. One of them was to prove that for any four points $A, B, C, D \in \mathbb{R}^3$ the following equality holds: $$\overrightarrow{BC} \cdot \overrightarrow{AD}\ +\ \overrightarrow{CA} \cdot \overrightarrow{BD}\ +\ \overrightarrow{AB} \cdot \overrightarrow{CD}\ = 0$$ The proof is easy; you just make three vectors starting in $A$ and then see that all the terms cancel out. My question is: what is the geometric interpretation of this equality? How can I visualize it or understand its deeper meaning? Does this equality have a name or where can I read more about it? I'm asking this because it turns out that it is not just a random equality and is rather useful. For example, if we want to prove the existence of orthocenter, we can do it surprisingly easily and quickly using this equality.","I was doing some exercises for linear algebra. One of them was to prove that for any four points the following equality holds: The proof is easy; you just make three vectors starting in and then see that all the terms cancel out. My question is: what is the geometric interpretation of this equality? How can I visualize it or understand its deeper meaning? Does this equality have a name or where can I read more about it? I'm asking this because it turns out that it is not just a random equality and is rather useful. For example, if we want to prove the existence of orthocenter, we can do it surprisingly easily and quickly using this equality.","A, B, C, D \in \mathbb{R}^3 \overrightarrow{BC} \cdot \overrightarrow{AD}\ +\ \overrightarrow{CA} \cdot \overrightarrow{BD}\ +\ \overrightarrow{AB} \cdot \overrightarrow{CD}\ = 0 A","['linear-algebra', 'vector-spaces', 'vectors']"
44,The longest list of analogies between vector spaces and categories ever made,The longest list of analogies between vector spaces and categories ever made,,"I suspect this question exists in different forms, elsewhere. I would like to know what's going on with this table, how to fill the missing items and how to continue the list, and what is the analogy that underlies it. ╔═══════════════════════╦══════════════════════╗ ║ vector spaces         ║ categories           ║ ╠═══════════════════════╬══════════════════════╣ ║ tensor product        ║ product              ║ ╠═══════════════════════╬══════════════════════╣ ║ linear map            ║ functor              ║ ╠═══════════════════════╬══════════════════════╣ ║ dual space            ║ opposite category    ║ ╠═══════════════════════╬══════════════════════╣ ║ canonical pairing     ║ hom functor          ║ ╠═══════════════════════╬══════════════════════╣ ║ ground field          ║ category of sets     ║ ╠═══════════════════════╬══════════════════════╣ ║ bidual injection      ║ Yoneda embedding     ║ ╠═══════════════════════╬══════════════════════╣ ║ ev(v) -> f = f(v)     ║ Yoneda lemma         ║ ╠═══════════════════════╬══════════════════════╣ ║ V~V** in finite dim   ║  ???                 ║ ╠═══════════════════════╬══════════════════════╣ ║ bilinear map          ║ profunctor           ║ ╠═══════════════════════╬══════════════════════╣ ║ ???                   ║ co/complete category ║ ╠═══════════════════════╬══════════════════════╣ ║ linear representation ║ ???                  ║ ╠═══════════════════════╬══════════════════════╣ ║ ???                   ║ adjoint functors     ║ ╠═══════════════════════╬══════════════════════╣ ║ ???                   ║ Kan extensions       ║ ╠═══════════════════════╬══════════════════════╣ ║ ???                   ║        coend         ║ ╚═══════════════════════╩══════════════════════╝","I suspect this question exists in different forms, elsewhere. I would like to know what's going on with this table, how to fill the missing items and how to continue the list, and what is the analogy that underlies it. ╔═══════════════════════╦══════════════════════╗ ║ vector spaces         ║ categories           ║ ╠═══════════════════════╬══════════════════════╣ ║ tensor product        ║ product              ║ ╠═══════════════════════╬══════════════════════╣ ║ linear map            ║ functor              ║ ╠═══════════════════════╬══════════════════════╣ ║ dual space            ║ opposite category    ║ ╠═══════════════════════╬══════════════════════╣ ║ canonical pairing     ║ hom functor          ║ ╠═══════════════════════╬══════════════════════╣ ║ ground field          ║ category of sets     ║ ╠═══════════════════════╬══════════════════════╣ ║ bidual injection      ║ Yoneda embedding     ║ ╠═══════════════════════╬══════════════════════╣ ║ ev(v) -> f = f(v)     ║ Yoneda lemma         ║ ╠═══════════════════════╬══════════════════════╣ ║ V~V** in finite dim   ║  ???                 ║ ╠═══════════════════════╬══════════════════════╣ ║ bilinear map          ║ profunctor           ║ ╠═══════════════════════╬══════════════════════╣ ║ ???                   ║ co/complete category ║ ╠═══════════════════════╬══════════════════════╣ ║ linear representation ║ ???                  ║ ╠═══════════════════════╬══════════════════════╣ ║ ???                   ║ adjoint functors     ║ ╠═══════════════════════╬══════════════════════╣ ║ ???                   ║ Kan extensions       ║ ╠═══════════════════════╬══════════════════════╣ ║ ???                   ║        coend         ║ ╚═══════════════════════╩══════════════════════╝",,"['linear-algebra', 'category-theory']"
45,Geometric interpretation of the determinant of a complex matrix,Geometric interpretation of the determinant of a complex matrix,,"A complex $n$-dimensional vector space $V$ can be thought of as a real $2n$-dimensional vector space equipped with a map $J:V \to V$ with $J^2 = -I$. Complex-linear maps are then linear maps $V \to V$ which commute with $J$. One can think of $J$ as an infinitesimal rotation, so that $\exp(tJ)$ gives a family of rotations of this space, and $\mathbb R$-linear maps $V \to V$ are complex-linear if they respect this family. From this point of view, or some other geometric point of view, is there a nice interpretation of the complex determinant $\det_{\mathbb C} L$ of a complex-linear map $L: V \to V$? Or, almost the same question, is there a geometric interpretation of the unique (up to scaling by complex numbers) antisymmetric complex-multilinear $n$-form $\operatorname{vol}_{\mathbb C}: V \times V \times ... \times V \to \mathbb C$? The norm is fairly easy to interpret. $| \det_{\mathbb C} L |^2 = |\det_{\mathbb R} L|$. One way to see this is to look at the diagonalization of $L$ over $\mathbb C$. This also gives you a way to interpret the argument, as the total amount of rotation in all the invariant subspaces of $L$. Is there a geometric interpretation of $\det_{\mathbb C} L$, not just its norm, which does not require one to diagonalize the matrix first? Even the special case when $L$ is unitary is of interest.","A complex $n$-dimensional vector space $V$ can be thought of as a real $2n$-dimensional vector space equipped with a map $J:V \to V$ with $J^2 = -I$. Complex-linear maps are then linear maps $V \to V$ which commute with $J$. One can think of $J$ as an infinitesimal rotation, so that $\exp(tJ)$ gives a family of rotations of this space, and $\mathbb R$-linear maps $V \to V$ are complex-linear if they respect this family. From this point of view, or some other geometric point of view, is there a nice interpretation of the complex determinant $\det_{\mathbb C} L$ of a complex-linear map $L: V \to V$? Or, almost the same question, is there a geometric interpretation of the unique (up to scaling by complex numbers) antisymmetric complex-multilinear $n$-form $\operatorname{vol}_{\mathbb C}: V \times V \times ... \times V \to \mathbb C$? The norm is fairly easy to interpret. $| \det_{\mathbb C} L |^2 = |\det_{\mathbb R} L|$. One way to see this is to look at the diagonalization of $L$ over $\mathbb C$. This also gives you a way to interpret the argument, as the total amount of rotation in all the invariant subspaces of $L$. Is there a geometric interpretation of $\det_{\mathbb C} L$, not just its norm, which does not require one to diagonalize the matrix first? Even the special case when $L$ is unitary is of interest.",,"['linear-algebra', 'complex-numbers', 'complex-geometry', 'multilinear-algebra']"
46,Are there any other methods to apply to solving simultaneous equations?,Are there any other methods to apply to solving simultaneous equations?,,"We are asked to solve for $x$ and $y$ in the following pair of simultaneous equations: $$\begin{align}3x+2y&=36 \tag1\\ 5x+4y&=64\tag2\end{align}$$ I can multiply $(1)$ by $2$ , yielding $6x + 4y = 72$ , and subtracting $(2)$ from this new equation eliminates $4y$ to solve strictly for $x$ ; i.e. $6x - 5x = 72 - 64 \Rightarrow x = 8$ . Substituting $x=8$ into $(2)$ reveals that $y=6$ . I could also subtract $(1)$ from $(2)$ and divide by $2$ , yielding $x+y=14$ . Let $$\begin{align}3x+3y - y &= 36 \tag{1a}\\ 5x + 5y - y &= 64\tag{1b}\end{align}$$ then expand brackets, and it follows that $42 - y = 36$ and $70 - y = 64$ , thus revealing $y=6$ and so $x = 14 - 6 = 8$ . I can even use matrices ! $(1)$ and $(2)$ could be written in matrix form: $$\begin{align}\begin{bmatrix} 3 &2 \\ 5 &4\end{bmatrix}\begin{bmatrix} x \\ y\end{bmatrix}&=\begin{bmatrix}36 \\ 64\end{bmatrix}\tag3 \\ \begin{bmatrix} x \\ y\end{bmatrix} &= {\begin{bmatrix} 3 &2 \\ 5 &4\end{bmatrix}}^{-1}\begin{bmatrix}36 \\ 64\end{bmatrix} \\ &= \frac{1}{2}\begin{bmatrix}4 &-2 \\ -5 &3\end{bmatrix}\begin{bmatrix}36 \\ 64\end{bmatrix} \\ &=\frac12\begin{bmatrix} 16 \\ 12\end{bmatrix} \\ &= \begin{bmatrix} 8 \\ 6\end{bmatrix} \\ \\ \therefore x&=8 \\ \therefore y&= 6\end{align}$$ Question Are there any other methods to solve for both $x$ and $y$ ?","We are asked to solve for and in the following pair of simultaneous equations: I can multiply by , yielding , and subtracting from this new equation eliminates to solve strictly for ; i.e. . Substituting into reveals that . I could also subtract from and divide by , yielding . Let then expand brackets, and it follows that and , thus revealing and so . I can even use matrices ! and could be written in matrix form: Question Are there any other methods to solve for both and ?",x y \begin{align}3x+2y&=36 \tag1\\ 5x+4y&=64\tag2\end{align} (1) 2 6x + 4y = 72 (2) 4y x 6x - 5x = 72 - 64 \Rightarrow x = 8 x=8 (2) y=6 (1) (2) 2 x+y=14 \begin{align}3x+3y - y &= 36 \tag{1a}\\ 5x + 5y - y &= 64\tag{1b}\end{align} 42 - y = 36 70 - y = 64 y=6 x = 14 - 6 = 8 (1) (2) \begin{align}\begin{bmatrix} 3 &2 \\ 5 &4\end{bmatrix}\begin{bmatrix} x \\ y\end{bmatrix}&=\begin{bmatrix}36 \\ 64\end{bmatrix}\tag3 \\ \begin{bmatrix} x \\ y\end{bmatrix} &= {\begin{bmatrix} 3 &2 \\ 5 &4\end{bmatrix}}^{-1}\begin{bmatrix}36 \\ 64\end{bmatrix} \\ &= \frac{1}{2}\begin{bmatrix}4 &-2 \\ -5 &3\end{bmatrix}\begin{bmatrix}36 \\ 64\end{bmatrix} \\ &=\frac12\begin{bmatrix} 16 \\ 12\end{bmatrix} \\ &= \begin{bmatrix} 8 \\ 6\end{bmatrix} \\ \\ \therefore x&=8 \\ \therefore y&= 6\end{align} x y,"['linear-algebra', 'systems-of-equations', 'big-list']"
47,The characteristic and minimal polynomial of a companion matrix,The characteristic and minimal polynomial of a companion matrix,,"The companion matrix of a monic polynomial $f \in \mathbb F\left[x\right]$ in $1$ variable $x$ over a field $\mathbb F$ plays an important role in understanding the structure of finite dimensional $\mathbb F[x]$ -modules. It is an important fact that the characteristic polynomial and the minimal polynomial of $C(f)$ are both equal to $f$ . This can be seen quite easily by induction on the degree of $f$ . Does anyone know a different proof of this fact? I would love to see a graph theoretic proof or a non inductive algebraic proof, but I would be happy with anything that makes it seem like more than a coincidence!","The companion matrix of a monic polynomial in variable over a field plays an important role in understanding the structure of finite dimensional -modules. It is an important fact that the characteristic polynomial and the minimal polynomial of are both equal to . This can be seen quite easily by induction on the degree of . Does anyone know a different proof of this fact? I would love to see a graph theoretic proof or a non inductive algebraic proof, but I would be happy with anything that makes it seem like more than a coincidence!",f \in \mathbb F\left[x\right] 1 x \mathbb F \mathbb F[x] C(f) f f,"['linear-algebra', 'abstract-algebra', 'matrices', 'minimal-polynomials', 'companion-matrices']"
48,Are idempotent matrices always diagonalizable?,Are idempotent matrices always diagonalizable?,,How to prove that any idempotent matrix is diagonalizable?,How to prove that any idempotent matrix is diagonalizable?,,"['linear-algebra', 'matrices', 'diagonalization', 'idempotents']"
49,Mathematical explanation for the Repertoire Method,Mathematical explanation for the Repertoire Method,,"There are a few questions already about this method, which has stumped me for a long while. The process is explained, for instance, here: Repertoire Method Clarification Required ( Concrete Mathematics ) What I absolutely don't get is: What's the meaning of plugging in simple functions (which are not solutions) in the recurrence and how would that help finding what the right solution really is? As I understand it, we have a function determined by a recurrence relation and we want to find a closed formula for it. The recurrence being linear, we assume this closed form is a linear combination of 3 other functions, with coefficients $\alpha, \beta$ and $ \gamma$. The first method suggested in the book is clear to me: set simple values for the constants, since the function will be the same for all of them, and try to guess A, B and C and prove it by induction. What I don't get is the ""dual"" repertoire method. $f$ could be anything, but it's something fixed, so I don't see any meaning in this process of plugging in $f(n) = 1$. $f$ clearly is not 1. What is going on? I expect everything coming down to seeing a vector space / module in two different ways and then somehow picking some basis vectors from each. However, I can't work out the right abstraction. I'd like to ask for a very explicit top down explanation of this repertoire method, if possible with linear algebra vocabulary.","There are a few questions already about this method, which has stumped me for a long while. The process is explained, for instance, here: Repertoire Method Clarification Required ( Concrete Mathematics ) What I absolutely don't get is: What's the meaning of plugging in simple functions (which are not solutions) in the recurrence and how would that help finding what the right solution really is? As I understand it, we have a function determined by a recurrence relation and we want to find a closed formula for it. The recurrence being linear, we assume this closed form is a linear combination of 3 other functions, with coefficients $\alpha, \beta$ and $ \gamma$. The first method suggested in the book is clear to me: set simple values for the constants, since the function will be the same for all of them, and try to guess A, B and C and prove it by induction. What I don't get is the ""dual"" repertoire method. $f$ could be anything, but it's something fixed, so I don't see any meaning in this process of plugging in $f(n) = 1$. $f$ clearly is not 1. What is going on? I expect everything coming down to seeing a vector space / module in two different ways and then somehow picking some basis vectors from each. However, I can't work out the right abstraction. I'd like to ask for a very explicit top down explanation of this repertoire method, if possible with linear algebra vocabulary.",,"['linear-algebra', 'discrete-mathematics']"
50,Show that the eigenvalues of a unitary matrix have modulus $1$,Show that the eigenvalues of a unitary matrix have modulus,1,"Show that the eigenvalues of a unitary matrix have modulus $1$ . I know that a unitary matrix can be defined as a square complex matrix $A$ , such that $$AA^*=A^*A=I$$ where $A^*$ is the conjugate transpose of $A$ , and $I$ is the identity matrix. Furthermore, for a square matrix $A$ , the eigenvalue equation is expressed by $$Av=\lambda v$$ If I use the relationship $(u v)^*=v^*{u^*}$ and take the conjugate transpose of this equation then $$v^*A^*=\lambda^*v^*$$ But now I got stuck. Can someone help?","Show that the eigenvalues of a unitary matrix have modulus . I know that a unitary matrix can be defined as a square complex matrix , such that where is the conjugate transpose of , and is the identity matrix. Furthermore, for a square matrix , the eigenvalue equation is expressed by If I use the relationship and take the conjugate transpose of this equation then But now I got stuck. Can someone help?",1 A AA^*=A^*A=I A^* A I A Av=\lambda v (u v)^*=v^*{u^*} v^*A^*=\lambda^*v^*,"['linear-algebra', 'matrices', 'proof-writing', 'eigenvalues-eigenvectors', 'unitary-matrices']"
51,What is the purpose of defining a Hilbert Space?,What is the purpose of defining a Hilbert Space?,,"I understand that the Hilbert space is an infinite dimensional analogue of Euclidean space. However, one thing I have thought about for a while is why certain problems define a variable living within a Hilbert space. In other words, why do we need to formally define a vector space before working with certain problems? Why can we not just directly do calculations without having to first formally define it? It seems to me often times that problems involving vectors would be defined as living in a vector space, as a formality, before the real math occurs. Why do we constantly need to mention vector spaces?","I understand that the Hilbert space is an infinite dimensional analogue of Euclidean space. However, one thing I have thought about for a while is why certain problems define a variable living within a Hilbert space. In other words, why do we need to formally define a vector space before working with certain problems? Why can we not just directly do calculations without having to first formally define it? It seems to me often times that problems involving vectors would be defined as living in a vector space, as a formality, before the real math occurs. Why do we constantly need to mention vector spaces?",,"['linear-algebra', 'functional-analysis']"
52,How prove this matrix $\det (A)=\left(\frac{1}{\ln{(a_{i}+a_{j})}}\right)_{n\times n}\neq 0$,How prove this matrix,\det (A)=\left(\frac{1}{\ln{(a_{i}+a_{j})}}\right)_{n\times n}\neq 0,"Question: let $a_{i}>1,i=1,2,3,\cdots,n$,and such $a_{i}\neq a_{j}$,for any $i\neq j$ define the matrix $$A=\left(\dfrac{1}{\ln{(a_{i}+a_{j})}}\right)_{n\times n}$$ show that: $$\det(A)\neq 0$$ My try: I know this matrix $A$ is similar this Cauchy determinants： http://en.wikipedia.org/wiki/Cauchy_matrix and $$\det(A)=\begin{vmatrix} \dfrac{1}{\ln{(a_{1}+a_{1})}}&\dfrac{1}{\ln{(a_{1}+a_{2})}}&\cdots&\dfrac{1}{\ln{(a_{1}+a_{n})}}\\ \dfrac{1}{\ln{(a_{2}+a_{1})}}&\dfrac{1}{\ln{(a_{2}+a_{2})}}&\cdots&\dfrac{1}{\ln{(a_{2}+a_{n})}}\\ \cdots&\cdots&\cdots&\cdots\\ \dfrac{1}{\ln{(a_{n}+a_{1})}}&\dfrac{1}{\ln{(a_{n}+a_{2})}}&\cdots&\dfrac{1}{\ln{(a_{n}+a_{n})}} \end{vmatrix}$$ but I can't,Thank you.and this problem is my frend ask me. this is he ask me  is second problem .and I think  this problem is interesting. Now  this problem is up $21$. that's mean this problem is hard.I hope someone can solve it.Good luck!Thank you","Question: let $a_{i}>1,i=1,2,3,\cdots,n$,and such $a_{i}\neq a_{j}$,for any $i\neq j$ define the matrix $$A=\left(\dfrac{1}{\ln{(a_{i}+a_{j})}}\right)_{n\times n}$$ show that: $$\det(A)\neq 0$$ My try: I know this matrix $A$ is similar this Cauchy determinants： http://en.wikipedia.org/wiki/Cauchy_matrix and $$\det(A)=\begin{vmatrix} \dfrac{1}{\ln{(a_{1}+a_{1})}}&\dfrac{1}{\ln{(a_{1}+a_{2})}}&\cdots&\dfrac{1}{\ln{(a_{1}+a_{n})}}\\ \dfrac{1}{\ln{(a_{2}+a_{1})}}&\dfrac{1}{\ln{(a_{2}+a_{2})}}&\cdots&\dfrac{1}{\ln{(a_{2}+a_{n})}}\\ \cdots&\cdots&\cdots&\cdots\\ \dfrac{1}{\ln{(a_{n}+a_{1})}}&\dfrac{1}{\ln{(a_{n}+a_{2})}}&\cdots&\dfrac{1}{\ln{(a_{n}+a_{n})}} \end{vmatrix}$$ but I can't,Thank you.and this problem is my frend ask me. this is he ask me  is second problem .and I think  this problem is interesting. Now  this problem is up $21$. that's mean this problem is hard.I hope someone can solve it.Good luck!Thank you",,"['linear-algebra', 'matrices', 'determinant']"
53,Is there a constructive way to exhibit a basis for $\mathbb{R}^\mathbb{N}$?,Is there a constructive way to exhibit a basis for ?,\mathbb{R}^\mathbb{N},"Assuming the Axiom of Choice, every vector space has a basis, though it can be troublesome to show one explicitly. Is there any constructive way to exhibit a basis for $\mathbb{R}^\mathbb{N}$, the vector space of real sequences?","Assuming the Axiom of Choice, every vector space has a basis, though it can be troublesome to show one explicitly. Is there any constructive way to exhibit a basis for $\mathbb{R}^\mathbb{N}$, the vector space of real sequences?",,"['linear-algebra', 'axiom-of-choice', 'hamel-basis']"
54,Isomorphisms Between a Finite-Dimensional Vector Space and its Dual,Isomorphisms Between a Finite-Dimensional Vector Space and its Dual,,"So, we know that there is no ""natural"" isomorphism between a finite dimensional vector space $X$ and its dual $X^{\vee}$. However, given a basis for $X$ $(e_1, \dots, e_n)$ we can always construct a (dual) basis $(e^1, \dots, e^n)$ for $X^{\vee}$ that satisfies $e^i(e_j) = \delta^i_j$. It follows that $V \approx V^{\vee}$ simply because they have the same finite dimension $n$. Now, it seems to me that we can construct an isomorphism by assigning $$ e_i \mapsto e^i $$ and extending by linearity. That is, for an arbitrary vector $v \in X$ given by $v = \sum v^ie_i$ we can specify a linear map $$\phi:V \rightarrow V^{\vee}$$ by $$ \phi(\sum v^ie_i) := \sum v^ie^i $$ It is a quick exercise to check that $\phi$ is an isomorphism. Now, although this construction seems correct, there is something that doesn't quite sit right with me about it. For one thing, there is no way to really make sense of the summation convention as the right side of the function will always contain two superscripts thus requiring an explicit summation sign. The other thing that feels rather off is actually taking a lower index and moving it to an upper index, i.e., $e_i \mapsto e^i$ (which, of course, is the reason that the summation convention doesn't work). So, with this background my questions are Is there any sense in which the above isomorphism is ""favored"" or ""canonical""? Is there another way to construct an isomorphism between a vector space and its dual, that would ""conserve indexes"", for lack of a better way to state it. My guess here is that the answer is no but becomes possible if one assumes $V$ has an inner product.","So, we know that there is no ""natural"" isomorphism between a finite dimensional vector space $X$ and its dual $X^{\vee}$. However, given a basis for $X$ $(e_1, \dots, e_n)$ we can always construct a (dual) basis $(e^1, \dots, e^n)$ for $X^{\vee}$ that satisfies $e^i(e_j) = \delta^i_j$. It follows that $V \approx V^{\vee}$ simply because they have the same finite dimension $n$. Now, it seems to me that we can construct an isomorphism by assigning $$ e_i \mapsto e^i $$ and extending by linearity. That is, for an arbitrary vector $v \in X$ given by $v = \sum v^ie_i$ we can specify a linear map $$\phi:V \rightarrow V^{\vee}$$ by $$ \phi(\sum v^ie_i) := \sum v^ie^i $$ It is a quick exercise to check that $\phi$ is an isomorphism. Now, although this construction seems correct, there is something that doesn't quite sit right with me about it. For one thing, there is no way to really make sense of the summation convention as the right side of the function will always contain two superscripts thus requiring an explicit summation sign. The other thing that feels rather off is actually taking a lower index and moving it to an upper index, i.e., $e_i \mapsto e^i$ (which, of course, is the reason that the summation convention doesn't work). So, with this background my questions are Is there any sense in which the above isomorphism is ""favored"" or ""canonical""? Is there another way to construct an isomorphism between a vector space and its dual, that would ""conserve indexes"", for lack of a better way to state it. My guess here is that the answer is no but becomes possible if one assumes $V$ has an inner product.",,['linear-algebra']
55,How to determine the diagonalizability of these two matrices?,How to determine the diagonalizability of these two matrices?,,"I am trying to figure out how to determine the diagonalizability of the following two matrices. For the first matrix $$\left[\begin{matrix} 0 & 1 & 0\\0 & 0 & 1\\2 & -5 & 4\end{matrix}\right]$$ There are two distinct eigenvalues, $\lambda_1 = \lambda_2 = 1$ and $\lambda_3 = 2$. According to the theorem, If $A$ is an $n \times n$ matrix with $n$ distinct eigenvalues, then $A$ is diagonalizable. For the next one $3 \times 3$ matrix $$\left[\begin{matrix} -1 & 0 & 1\\3 & 0 & -3\\1 & 0 & -1\end{matrix}\right]$$ We also have two eigenvalues $\lambda_1 = \lambda_2 = 0$ and $\lambda_3 = -2$. For the first matrix, the algebraic multiplicity of the $\lambda_1$ is $2$ and the geometric multiplicity is $1$. So according to the theorem, this would not be diagonalizable since the geometric multiplicity is not equal to the algebraic multiplicity. For the second matrix, the algebraic multiplicity and the geometric multiplicity of both lambdas are equal, so this is diagonalizable according to my textbook. But there are still only two distinct eigenvalues in $3 \times 3$ matrix, so why is this diagonalizable if we are to accept the first theorem? Also, how to determine the geometric multiplicity of a matrix?","I am trying to figure out how to determine the diagonalizability of the following two matrices. For the first matrix $$\left[\begin{matrix} 0 & 1 & 0\\0 & 0 & 1\\2 & -5 & 4\end{matrix}\right]$$ There are two distinct eigenvalues, $\lambda_1 = \lambda_2 = 1$ and $\lambda_3 = 2$. According to the theorem, If $A$ is an $n \times n$ matrix with $n$ distinct eigenvalues, then $A$ is diagonalizable. For the next one $3 \times 3$ matrix $$\left[\begin{matrix} -1 & 0 & 1\\3 & 0 & -3\\1 & 0 & -1\end{matrix}\right]$$ We also have two eigenvalues $\lambda_1 = \lambda_2 = 0$ and $\lambda_3 = -2$. For the first matrix, the algebraic multiplicity of the $\lambda_1$ is $2$ and the geometric multiplicity is $1$. So according to the theorem, this would not be diagonalizable since the geometric multiplicity is not equal to the algebraic multiplicity. For the second matrix, the algebraic multiplicity and the geometric multiplicity of both lambdas are equal, so this is diagonalizable according to my textbook. But there are still only two distinct eigenvalues in $3 \times 3$ matrix, so why is this diagonalizable if we are to accept the first theorem? Also, how to determine the geometric multiplicity of a matrix?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'diagonalization']"
56,Difference between orthogonal and orthonormal matrices,Difference between orthogonal and orthonormal matrices,,"Let $Q$ be an $N \times N$ unitary matrix (its columns are orthonormal). Since $Q$ is unitary, it would preserve the norm of any vector $X$ , i.e., $\|QX\|^2 = \|X\|^2$ . My confusion comes when the columns of $Q$ are orthogonal, but not orthonormal, i.e., if the columns are weighted by weights $w_1,\dots,w_N$ , the dot product of any two different columns would still be zero, but $Q^H Q \neq I$ anymore. What are these matrices called? The literature always refers to matrices with orthonormal columns as orthogonal, however I think that's not quite accurate. Would a square matrix with orthogonal columns, but not orthonormal, change the norm of a vector?","Let be an unitary matrix (its columns are orthonormal). Since is unitary, it would preserve the norm of any vector , i.e., . My confusion comes when the columns of are orthogonal, but not orthonormal, i.e., if the columns are weighted by weights , the dot product of any two different columns would still be zero, but anymore. What are these matrices called? The literature always refers to matrices with orthonormal columns as orthogonal, however I think that's not quite accurate. Would a square matrix with orthogonal columns, but not orthonormal, change the norm of a vector?","Q N \times N Q X \|QX\|^2 = \|X\|^2 Q w_1,\dots,w_N Q^H Q \neq I","['linear-algebra', 'matrices', 'isometry', 'orthogonal-matrices', 'unitary-matrices']"
57,If $A$ and $B$ are matrices such that $AB^2=BA$ and $A^4=I$ then find $B^{16}$.,If  and  are matrices such that  and  then find .,A B AB^2=BA A^4=I B^{16},"If $A$ and $B$ are matrices such that $AB^2=BA$ and $A^4=I$ , then find $B^{16}$ . My Method: Given $$AB^2=BA \tag{1}$$ Post multiplying with $B^2$ we get $$AB^4=BAB^2=B^2A$$ Hence $$AB^4=B^2A$$ Pre Multiplying with $A$ and using $(1)$ we get $$A^2B^4=(AB^2)A=BA^2$$ hence $$A^2B^4=BA^2 \tag{2}$$ Now post multiplying with $B^4$ and using $(2)$ we get $$A^2B^8=B(A^2B^4)=B^2A^2$$ hence $$A^2B^8=B^2A^2 \tag{3}$$ Now Pre Multiply with $B^2$ and use $(3)$ we get $$B^2A^2B^8=B^4A^2$$ $\implies$ $$A^2B^8B^8=B^4A^2$$ $$A^2B^{16}=B^4A^2$$ Now pre multiply with $A^2$ and use $(2)$ we get $$A^4B^{16}=A^2B^4A^2$$ $\implies$ $$B^{16}=BA^4=B$$ is there any other approach to solve this?","If and are matrices such that and , then find . My Method: Given Post multiplying with we get Hence Pre Multiplying with and using we get hence Now post multiplying with and using we get hence Now Pre Multiply with and use we get Now pre multiply with and use we get is there any other approach to solve this?",A B AB^2=BA A^4=I B^{16} AB^2=BA \tag{1} B^2 AB^4=BAB^2=B^2A AB^4=B^2A A (1) A^2B^4=(AB^2)A=BA^2 A^2B^4=BA^2 \tag{2} B^4 (2) A^2B^8=B(A^2B^4)=B^2A^2 A^2B^8=B^2A^2 \tag{3} B^2 (3) B^2A^2B^8=B^4A^2 \implies A^2B^8B^8=B^4A^2 A^2B^{16}=B^4A^2 A^2 (2) A^4B^{16}=A^2B^4A^2 \implies B^{16}=BA^4=B,"['linear-algebra', 'abstract-algebra', 'matrices', 'proof-writing', 'determinant']"
58,Why teach linear algebra before abstract algebra?,Why teach linear algebra before abstract algebra?,,Is there a reason why most undergraduate curriculums put linear algebra before abstract algebra? I'm asking this because personally it seems to be much easier to understand the architecture behind linear algebra as supposed to simply solving problems after a course in abstract algebra.,Is there a reason why most undergraduate curriculums put linear algebra before abstract algebra? I'm asking this because personally it seems to be much easier to understand the architecture behind linear algebra as supposed to simply solving problems after a course in abstract algebra.,,"['linear-algebra', 'abstract-algebra']"
59,Is a linear transformation onto or one-to-one?,Is a linear transformation onto or one-to-one?,,"The definition of one-to-one was pretty straight forward. If the vectors are lin.indep the transformation would be one-to-one. But could it also be onto? The definition of onto was a little more abstract. Would a zero-row in reduced echelon form have any effect on this? I just assumed that because it has a couple of free variables it would be onto, but that zero-row set me off a bit. Say if the matrix is 4 by 5. With two free variables. And the zero-row in echelon form. (Sorry for not posting the given matrix, but that is to specific. And I don't want to get a ban from uni for asking online. The task is determine the onto/one-to-one of to matrices) I'll check back after class and update the question if more information is desirable. Update The definitions in the book is this;  Onto:  $T: \mathbb R^n \to \mathbb R^m $ is said to be onto $\mathbb R^m  $ if each b in  $\mathbb R^m  $ is the image of at least one x in $\mathbb R^n  $ One-to-one: $T: \mathbb R^n \to \mathbb R^m $ is said to be one-to-one $\mathbb R^m  $ if each b in  $\mathbb R^m  $ is the image of at most one x in $\mathbb R^n  $ And then, there is another theorem that states that a linear transformation is one-to-one iff the equation T(x) = 0 has only the trivial solution. That doesn't say anything about onto. Then there is this bit that confused be about onto:  Let $T: \mathbb R^n \to \mathbb R^m $ be a linear transformation and let A be the standard matrix for T. Then:  T maps $T: \mathbb R^n$ onto $\mathbb R^m $ iff the columns of A span $\mathbb R^m $.","The definition of one-to-one was pretty straight forward. If the vectors are lin.indep the transformation would be one-to-one. But could it also be onto? The definition of onto was a little more abstract. Would a zero-row in reduced echelon form have any effect on this? I just assumed that because it has a couple of free variables it would be onto, but that zero-row set me off a bit. Say if the matrix is 4 by 5. With two free variables. And the zero-row in echelon form. (Sorry for not posting the given matrix, but that is to specific. And I don't want to get a ban from uni for asking online. The task is determine the onto/one-to-one of to matrices) I'll check back after class and update the question if more information is desirable. Update The definitions in the book is this;  Onto:  $T: \mathbb R^n \to \mathbb R^m $ is said to be onto $\mathbb R^m  $ if each b in  $\mathbb R^m  $ is the image of at least one x in $\mathbb R^n  $ One-to-one: $T: \mathbb R^n \to \mathbb R^m $ is said to be one-to-one $\mathbb R^m  $ if each b in  $\mathbb R^m  $ is the image of at most one x in $\mathbb R^n  $ And then, there is another theorem that states that a linear transformation is one-to-one iff the equation T(x) = 0 has only the trivial solution. That doesn't say anything about onto. Then there is this bit that confused be about onto:  Let $T: \mathbb R^n \to \mathbb R^m $ be a linear transformation and let A be the standard matrix for T. Then:  T maps $T: \mathbb R^n$ onto $\mathbb R^m $ iff the columns of A span $\mathbb R^m $.",,['linear-algebra']
60,"Is ""Linear Algebra Done Right"" good for a beginner?","Is ""Linear Algebra Done Right"" good for a beginner?",,"Amazon book reviews say it takes unorthodox approach and is for a second exposure to linear algebra. I didn't have a first exposure to linear algebra. Is this book going to be bad for me, then? Or, should I read another linear algebra book after reading it? I want to avoid reading two linear algebra books because reading such a textbook consumes a lot of time.","Amazon book reviews say it takes unorthodox approach and is for a second exposure to linear algebra. I didn't have a first exposure to linear algebra. Is this book going to be bad for me, then? Or, should I read another linear algebra book after reading it? I want to avoid reading two linear algebra books because reading such a textbook consumes a lot of time.",,['linear-algebra']
61,Suppose $M$ is an $m \times n$ matrix such that all rows and columns of $M$ sum to $1$. Show that $m=n$,Suppose  is an  matrix such that all rows and columns of  sum to . Show that,M m \times n M 1 m=n,"I find this a rather awkward question,  from the book ""Mathematical Circles"" by Fomin, Genkin and Itenberg. The question number is Question number 23 from  Chapter 12 (""Invariants""). I was given a hint: use invariants, which I found even more awkward. There was also a remark : ""strange as it may seem, this is an invariants problem"". Funny , because I don't know what to expect now! Suppose $M$ is an $m \times n$ matrix such that all rows and columns of $M$ sum to $1$ . Show that $m=n$ . I have no clue how this is a problem on invariants, let alone how to solve this problem. I'll need hints on why this is the case.","I find this a rather awkward question,  from the book ""Mathematical Circles"" by Fomin, Genkin and Itenberg. The question number is Question number 23 from  Chapter 12 (""Invariants""). I was given a hint: use invariants, which I found even more awkward. There was also a remark : ""strange as it may seem, this is an invariants problem"". Funny , because I don't know what to expect now! Suppose is an matrix such that all rows and columns of sum to . Show that . I have no clue how this is a problem on invariants, let alone how to solve this problem. I'll need hints on why this is the case.",M m \times n M 1 m=n,"['linear-algebra', 'matrices', 'invariance']"
62,Can a matrix be invertible but not diagonalizable? [duplicate],Can a matrix be invertible but not diagonalizable? [duplicate],,"This question already has an answer here : If matrix A is invertible, is it diagonalizable as well? (1 answer) Closed 7 years ago . While reading a chapter on diagonalizable matrices, I found myself wondering: Can a matrix $A \in \mathbb R^{n \times n}$ be invertible but not diagonalizable? My quick Google search did not return a clear answer.","This question already has an answer here : If matrix A is invertible, is it diagonalizable as well? (1 answer) Closed 7 years ago . While reading a chapter on diagonalizable matrices, I found myself wondering: Can a matrix $A \in \mathbb R^{n \times n}$ be invertible but not diagonalizable? My quick Google search did not return a clear answer.",,"['linear-algebra', 'eigenvalues-eigenvectors', 'diagonalization']"
63,Difference between least squares and minimum norm solution,Difference between least squares and minimum norm solution,,"Consider a linear system of equations $Ax = b$. If the system is overdetermined, the least squares (approximate) solution minimizes $||b - Ax||^2$. Some source sources also mention $||b - Ax||$. If the system is underdetermined one can calculate the minimum norm solution. But it does also minimize  $||b - Ax||$, or am I wrong? But if least squares is also a minimum norm, what is the difference, or the rationale of the different naming?","Consider a linear system of equations $Ax = b$. If the system is overdetermined, the least squares (approximate) solution minimizes $||b - Ax||^2$. Some source sources also mention $||b - Ax||$. If the system is underdetermined one can calculate the minimum norm solution. But it does also minimize  $||b - Ax||$, or am I wrong? But if least squares is also a minimum norm, what is the difference, or the rationale of the different naming?",,"['linear-algebra', 'systems-of-equations', 'least-squares']"
64,Determinant of rank-one perturbations of (invertible) matrices,Determinant of rank-one perturbations of (invertible) matrices,,"I read something that suggests that if $I$ is the $n$ -by- $n$ identity matrix, $v$ is an $n$ -dimensional real column vector with $\|v\| = 1$ (standard Euclidean norm), and $t > 0$ , then $$\det(I + t v v^T) = 1 + t$$ Can anyone prove this or provide a reference? More generally, is there also an (easy) formula for calculating $\det(A + wv^T)$ for $v,w \in \mathbb{K}^{d \times 1}$ and some (invertible) matrix $A \in \Bbb{K}^{d \times d}$ ?","I read something that suggests that if is the -by- identity matrix, is an -dimensional real column vector with (standard Euclidean norm), and , then Can anyone prove this or provide a reference? More generally, is there also an (easy) formula for calculating for and some (invertible) matrix ?","I n n v n \|v\| = 1 t > 0 \det(I + t v v^T) = 1 + t \det(A + wv^T) v,w \in \mathbb{K}^{d \times 1} A \in \Bbb{K}^{d \times d}","['linear-algebra', 'matrices', 'determinant']"
65,Euclidean distance and dot product,Euclidean distance and dot product,,"I've been reading that the Euclidean distance between two points, and the dot product of the two points, are related. Specifically, the Euclidean distance is equal to the square root of the dot product. But this doesn't work for me in practice. For example, let's say the points are $(3, 5)$ and $(6, 9)$ . The Euclidean distance is $\sqrt{(3 - 6)^2 + (5 - 9)^2}$ , which is equal to $\sqrt{9 + 16}$ , or $5$ . However, the dot product is $(3 * 6 + 5 * 9)$ , which is $63$ , and the square root of this is not $5$ . What am I getting wrong?","I've been reading that the Euclidean distance between two points, and the dot product of the two points, are related. Specifically, the Euclidean distance is equal to the square root of the dot product. But this doesn't work for me in practice. For example, let's say the points are and . The Euclidean distance is , which is equal to , or . However, the dot product is , which is , and the square root of this is not . What am I getting wrong?","(3, 5) (6, 9) \sqrt{(3 - 6)^2 + (5 - 9)^2} \sqrt{9 + 16} 5 (3 * 6 + 5 * 9) 63 5","['linear-algebra', 'vectors']"
66,Is the cross product of two unit vectors itself a unit vector?,Is the cross product of two unit vectors itself a unit vector?,,"Or, in general, what does the magnitude of the cross product mean? How would you prove or disprove this?","Or, in general, what does the magnitude of the cross product mean? How would you prove or disprove this?",,"['linear-algebra', 'cross-product']"
67,What does the sign $\propto$ mean?,What does the sign  mean?,\propto,"I am a computer scientist, and one of my professors today used the symbol $\propto$. I tried to search that using google, but it returns no results, and I do not even know its name. So, I would like to ask, what does the operator $\propto$ mean? What is its name, and how is it used?","I am a computer scientist, and one of my professors today used the symbol $\propto$. I tried to search that using google, but it returns no results, and I do not even know its name. So, I would like to ask, what does the operator $\propto$ mean? What is its name, and how is it used?",,"['linear-algebra', 'soft-question']"
68,Quick way to find eigenvalues of anti-diagonal matrix,Quick way to find eigenvalues of anti-diagonal matrix,,"If $A \in M_n(\mathbb{R})$ is an anti-diagonal $n \times n$ matrix, is there a quick way to find its eigenvalues in a way similar to finding the eigenvalues of a diagonal matrix? The standard way for finding the eigenvalues for any $n \times n$ is usually straightforward, but may sometimes lead to painstaking computational time. Just wondering if there was a quicker way in doing so for any anti-diagonal matrix without having to resort to the standard method of finding the determinant of $A - \lambda I$, where $I$ is the $n \times n$ identity matrix, and setting it equal to $0$ and solving for $\lambda$.","If $A \in M_n(\mathbb{R})$ is an anti-diagonal $n \times n$ matrix, is there a quick way to find its eigenvalues in a way similar to finding the eigenvalues of a diagonal matrix? The standard way for finding the eigenvalues for any $n \times n$ is usually straightforward, but may sometimes lead to painstaking computational time. Just wondering if there was a quicker way in doing so for any anti-diagonal matrix without having to resort to the standard method of finding the determinant of $A - \lambda I$, where $I$ is the $n \times n$ identity matrix, and setting it equal to $0$ and solving for $\lambda$.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
69,Show that the area of a triangle is given by this determinant,Show that the area of a triangle is given by this determinant,,"I'm not sure how to solve this problem. Can you guys provide some input/hints? Let $A=(x_1,y_1)$, $B=(x_2,y_2)$ and $C=(x_3,y_3)$ be three points in $\mathbb{R}^{2}$. Show that the area of $\Delta ABC$ is given by   $\frac{1}{2}\left| \det(M) \right|$, where   $$M = \begin{pmatrix} 1 & 1  & 1\\  x_1 & x_2 & x_3\\  y_1 & y_2 & y_3 \end{pmatrix}$$","I'm not sure how to solve this problem. Can you guys provide some input/hints? Let $A=(x_1,y_1)$, $B=(x_2,y_2)$ and $C=(x_3,y_3)$ be three points in $\mathbb{R}^{2}$. Show that the area of $\Delta ABC$ is given by   $\frac{1}{2}\left| \det(M) \right|$, where   $$M = \begin{pmatrix} 1 & 1  & 1\\  x_1 & x_2 & x_3\\  y_1 & y_2 & y_3 \end{pmatrix}$$",,"['linear-algebra', 'determinant', 'triangles', 'area']"
70,Why do we need a Jordan normal form? [duplicate],Why do we need a Jordan normal form? [duplicate],,"This question already has answers here : What is the purpose of Jordan Canonical Form? (2 answers) Closed 6 years ago . My professor said that the main idea of finding a Jordan normal form is to find the closest 'diagonal' matrix that is similar to a given matrix that does not have  a similar matrix that is diagonal. I know that using a diagonal matrix is good for computations and simplifying powers of matrices.  But what is the potential of finding a matrix with Jordan form? what is this 'almost diagonal' matrix gives me? We learnt how to find it, without knowing what is the main idea behind it and what are the applications used with it, so I can't really understand it.","This question already has answers here : What is the purpose of Jordan Canonical Form? (2 answers) Closed 6 years ago . My professor said that the main idea of finding a Jordan normal form is to find the closest 'diagonal' matrix that is similar to a given matrix that does not have  a similar matrix that is diagonal. I know that using a diagonal matrix is good for computations and simplifying powers of matrices.  But what is the potential of finding a matrix with Jordan form? what is this 'almost diagonal' matrix gives me? We learnt how to find it, without knowing what is the main idea behind it and what are the applications used with it, so I can't really understand it.",,"['linear-algebra', 'matrices', 'jordan-normal-form']"
71,Why is the matrix norm $||A||_1$ maximum absolute column sum of the matrix?,Why is the matrix norm  maximum absolute column sum of the matrix?,||A||_1,"By definition, we have $$ \|V\|_p := \sqrt[p]{\displaystyle \sum_{i=1}^{n}|v_i|^p} \qquad \text{and} \qquad \|A\|_p := \sup_{x\not=0}\frac{||Ax||_p}{||x||_p} $$ and if $A$ is finite, we change sup to max. However I don't really get how we get to the definition of $||A||_1$ as the maximum absolute column sum of the matrix as stated in Wikipedia For example, assume $A=\begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22}\end{bmatrix}$ . Then $$ ||A||_1 = \max_{x\not=0} \frac{\left\|\begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22}\end{bmatrix}\cdot \begin{bmatrix} x_{1} \\ x_{2}\end{bmatrix}\right\| }{\left\|\begin{bmatrix} x_{1} \\ x_{2}\end{bmatrix}\right\|} = \max_{x\not=0} \frac{|a_{11}x_1+a_{12}x_2|+|a_{21}x_1+a_{22}x_2|}{|x_1|+|x_2|} $$ That's what I have gotten so far, but I don't really see how this is related to the max of the column sum. Can anyone help me explain this?","By definition, we have and if is finite, we change sup to max. However I don't really get how we get to the definition of as the maximum absolute column sum of the matrix as stated in Wikipedia For example, assume . Then That's what I have gotten so far, but I don't really see how this is related to the max of the column sum. Can anyone help me explain this?","
\|V\|_p
:= \sqrt[p]{\displaystyle \sum_{i=1}^{n}|v_i|^p}
\qquad \text{and} \qquad
\|A\|_p
:= \sup_{x\not=0}\frac{||Ax||_p}{||x||_p}
 A ||A||_1 A=\begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22}\end{bmatrix} 
||A||_1
= \max_{x\not=0} \frac{\left\|\begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22}\end{bmatrix}\cdot \begin{bmatrix} x_{1} \\ x_{2}\end{bmatrix}\right\| }{\left\|\begin{bmatrix} x_{1} \\ x_{2}\end{bmatrix}\right\|}
= \max_{x\not=0} \frac{|a_{11}x_1+a_{12}x_2|+|a_{21}x_1+a_{22}x_2|}{|x_1|+|x_2|}
","['linear-algebra', 'matrices', 'normed-spaces', 'matrix-norms', 'matrix-analysis']"
72,Basis of primitive nth Roots in a Cyclotomic Extension?,Basis of primitive nth Roots in a Cyclotomic Extension?,,"While reading one of Keith Conrad's great blurbs, Linear Independence of Characters , there is a footnote at the bottom of page 2 saying In general, the primitive $n$ th roots of unity in the $n$ th cyclotomic field form a normal basis over $\mathbf{Q}$ if and only if $n$ is squarefree. A little bit of research didn't turn up any results, except apparently the result is in the paper K. Johnsen, Lineare Abhängigkeiten von Einheitswurzeln. Elem. Math. 40 (1985), 57–59 which I can't find anywhere. (J.M. found it here .) Could someone be kind enough to give a proof of why this statement is true? I made this attempt at the if direction. I try induction on $n$ . If $n$ is prime, then the result follows from field theory, since the powers of $\alpha$ form a $F$ -basis of $F(\alpha)$ over $F$ . So let $n=mp$ where $m$ is coprime to $p$ . If $n$ is squarefree, so is $m$ , so by induction, the $m$ th primitive roots of units form a basis for $\mathbb{Q}(\zeta_m)$ over $\mathbb{Q}$ . Then I think the primitive $p$ th roots of unity form a basis of $\mathbb{Q}(\zeta_m,\zeta_p)$ over $\mathbb{Q}(\zeta_m)$ , so taking pairwise products of the two bases gives a basis of primitive $n$ th roots of $\mathbb{Q}(\zeta_m,\zeta_p)$ over $\mathbb{Q}$ ? Does $\mathbb{Q}(\zeta_m,\zeta_p)=\mathbb{Q}(\zeta_n)$ ? I know $\mathbb{Q}(\zeta_m,\zeta_p)\subseteq\mathbb{Q}(\zeta_n)$ , but am not sure of the other containment, or if this argument is oversimplifying things.","While reading one of Keith Conrad's great blurbs, Linear Independence of Characters , there is a footnote at the bottom of page 2 saying In general, the primitive th roots of unity in the th cyclotomic field form a normal basis over if and only if is squarefree. A little bit of research didn't turn up any results, except apparently the result is in the paper K. Johnsen, Lineare Abhängigkeiten von Einheitswurzeln. Elem. Math. 40 (1985), 57–59 which I can't find anywhere. (J.M. found it here .) Could someone be kind enough to give a proof of why this statement is true? I made this attempt at the if direction. I try induction on . If is prime, then the result follows from field theory, since the powers of form a -basis of over . So let where is coprime to . If is squarefree, so is , so by induction, the th primitive roots of units form a basis for over . Then I think the primitive th roots of unity form a basis of over , so taking pairwise products of the two bases gives a basis of primitive th roots of over ? Does ? I know , but am not sure of the other containment, or if this argument is oversimplifying things.","n n \mathbf{Q} n n n \alpha F F(\alpha) F n=mp m p n m m \mathbb{Q}(\zeta_m) \mathbb{Q} p \mathbb{Q}(\zeta_m,\zeta_p) \mathbb{Q}(\zeta_m) n \mathbb{Q}(\zeta_m,\zeta_p) \mathbb{Q} \mathbb{Q}(\zeta_m,\zeta_p)=\mathbb{Q}(\zeta_n) \mathbb{Q}(\zeta_m,\zeta_p)\subseteq\mathbb{Q}(\zeta_n)","['linear-algebra', 'field-theory', 'galois-theory']"
73,If a field $F$ is such that $\left|F\right|>n-1$ why is $V$ a vector space over $F$ not equal to the union of $n$ proper subspaces of $V$,If a field  is such that  why is  a vector space over  not equal to the union of  proper subspaces of,F \left|F\right|>n-1 V F n V,"If $U_1$, $U_2,\ldots,U_n$ are proper subspaces of a vector space $V$ over a field $F$, and $|F|\gt n-1$, why is $V$ not equal to the union of the subspaces $U_1$, $U_2,\ldots,U_n$?","If $U_1$, $U_2,\ldots,U_n$ are proper subspaces of a vector space $V$ over a field $F$, and $|F|\gt n-1$, why is $V$ not equal to the union of the subspaces $U_1$, $U_2,\ldots,U_n$?",,"['linear-algebra', 'vector-spaces']"
74,"Etymology of the word ""normal"" (perpendicular)","Etymology of the word ""normal"" (perpendicular)",,"While the word ""normal"" is one of the most overloaded mathematical terms, in linear algebra, it is usually associated with the notion of being perpendicular to something, as in ""normal vector"" or ""normal matrix"" (which has a set of mutually perpendicular eigenvectors). However, the word ""normal"" does not seem to mean its normal/ordinary/usual meaning in English. So, why is a normal vector or matrix called ""normal""?","While the word ""normal"" is one of the most overloaded mathematical terms, in linear algebra, it is usually associated with the notion of being perpendicular to something, as in ""normal vector"" or ""normal matrix"" (which has a set of mutually perpendicular eigenvectors). However, the word ""normal"" does not seem to mean its normal/ordinary/usual meaning in English. So, why is a normal vector or matrix called ""normal""?",,"['linear-algebra', 'terminology', 'education']"
75,Coefficients of characteristic polynomial of a matrix,Coefficients of characteristic polynomial of a matrix,,"For a given $n \times n$-matrix $A$, and $J\subseteq\{1,...,n\}$ let us denote by $A[J]$ its principal minor formed by the columns and rows with indices from $J$. If the characteristic polynomial of $A$ is $x^n+a_{n-1}x^{n-1}+\cdots+a_1x+a_0$, then why $$a_k=(-1)^{n-k}\sum_{|J|=n-k}A[J],$$ that is, why is each coefficient the sum of the appropriately sized principal minors of $A$?","For a given $n \times n$-matrix $A$, and $J\subseteq\{1,...,n\}$ let us denote by $A[J]$ its principal minor formed by the columns and rows with indices from $J$. If the characteristic polynomial of $A$ is $x^n+a_{n-1}x^{n-1}+\cdots+a_1x+a_0$, then why $$a_k=(-1)^{n-k}\sum_{|J|=n-k}A[J],$$ that is, why is each coefficient the sum of the appropriately sized principal minors of $A$?",,"['linear-algebra', 'matrices', 'determinant', 'characteristic-polynomial']"
76,Converting recursive equations into matrices,Converting recursive equations into matrices,,"How do we convert recursive equations into matrix forms? For instance, consider this recursive equation(Fibonacci Series): $$F_n = F_{n-1} + F_{n-2}$$ And it comes out to be that the following that $$\begin{bmatrix}1&1\\1&0\end{bmatrix}^n = \begin{bmatrix}F_{n+1}&F_{n}\\F_{n}&F_{n-1}\end{bmatrix}$$ Can please anyone tell me how do we derive such a base matrix for recursive equations? How can we determine the order of the matrix for the recursive equation, as well as the elements of the matrix?","How do we convert recursive equations into matrix forms? For instance, consider this recursive equation(Fibonacci Series): $$F_n = F_{n-1} + F_{n-2}$$ And it comes out to be that the following that $$\begin{bmatrix}1&1\\1&0\end{bmatrix}^n = \begin{bmatrix}F_{n+1}&F_{n}\\F_{n}&F_{n-1}\end{bmatrix}$$ Can please anyone tell me how do we derive such a base matrix for recursive equations? How can we determine the order of the matrix for the recursive equation, as well as the elements of the matrix?",,"['linear-algebra', 'matrices', 'recurrence-relations', 'fibonacci-numbers']"
77,Does equality of characteristic polynomials guarantee equivalence of matrices?,Does equality of characteristic polynomials guarantee equivalence of matrices?,,"I have a qualifying exam coming up in a couple days and I am just trying to understand some pathological examples I have in my notes. I will list a similar problem which I know the solution to and then the question. True or False:   let $A,B \in \mathbb{Q}^{n \times n }$  Suppose $xI-A$ and $xI-B$ are equivalent then $\det(xI-A) = \det(xI-B)$. I wrote in my notes this is true because any two matrices are equivalent in $\mathbb{Q[x]}^{n \times n}$ if and only if they have the same invariant factors.  Since the characteristic polynomial is a product of the invariant factors it follows that $\det(xI-A) = \det(xI-B)$. True or False:   let $A,B \in \mathbb{Q}^{n \times n }$. If $\det(xI-A) = \det(xI-B)$ (in $\mathbb{Q}[x]$) then $xI-A$ and $xI-B$ are equivalent  in $\mathbb{Q}[x]^{n \times n}$ I think this is false because of the fact that equality of determinants is not strong enough to guarantee all invariant factors are equal but I do not have an example.","I have a qualifying exam coming up in a couple days and I am just trying to understand some pathological examples I have in my notes. I will list a similar problem which I know the solution to and then the question. True or False:   let $A,B \in \mathbb{Q}^{n \times n }$  Suppose $xI-A$ and $xI-B$ are equivalent then $\det(xI-A) = \det(xI-B)$. I wrote in my notes this is true because any two matrices are equivalent in $\mathbb{Q[x]}^{n \times n}$ if and only if they have the same invariant factors.  Since the characteristic polynomial is a product of the invariant factors it follows that $\det(xI-A) = \det(xI-B)$. True or False:   let $A,B \in \mathbb{Q}^{n \times n }$. If $\det(xI-A) = \det(xI-B)$ (in $\mathbb{Q}[x]$) then $xI-A$ and $xI-B$ are equivalent  in $\mathbb{Q}[x]^{n \times n}$ I think this is false because of the fact that equality of determinants is not strong enough to guarantee all invariant factors are equal but I do not have an example.",,"['linear-algebra', 'matrices', 'characteristic-polynomial']"
78,Under what conditions does a matrix $A$ have a square root?,Under what conditions does a matrix  have a square root?,A,"Under what conditions does a matrix $A$ have a square root? I saw somewhere that this is true for Hermitian positive definite matrices(whose definition I just looked up). Moreover, is it possible that for some subspace $X \subset M_n(\mathbb R)$ of $n\times n$ matrices over $\mathbb R$, the map $A \mapsto \sqrt{A}$ is continuous? People who want to consider more generality can also look at matrices over $\mathbb C$. Thank you.","Under what conditions does a matrix $A$ have a square root? I saw somewhere that this is true for Hermitian positive definite matrices(whose definition I just looked up). Moreover, is it possible that for some subspace $X \subset M_n(\mathbb R)$ of $n\times n$ matrices over $\mathbb R$, the map $A \mapsto \sqrt{A}$ is continuous? People who want to consider more generality can also look at matrices over $\mathbb C$. Thank you.",,"['linear-algebra', 'matrices']"
79,Why does SVD provide the least squares and least norm solution to $ A x = b $?,Why does SVD provide the least squares and least norm solution to ?, A x = b ,"I am studying the Singular Value Decomposition and its properties. It is widely used in order to solve equations of the form $Ax=b$ . I have seen the following: When we have the equation system $Ax=b$ , we calculate the SVD of A as $A=U\Sigma V^T$ . Then we calculate $x'= V \Sigma^{+}U^Tb$ . $\Sigma^{+}$ has the reciprocals ( $\dfrac{1}{\sigma_i}$ ) of the singular values in its diagonal and zeros where $\sigma_i=0$ .  If the $b$ is in the range of $A$ then it is the solution that has the minimum norm (closest to origin). If it is not in the range, then it is the least-squares solution. I fail to see how exactly this procedure always produces a $x'$ which is closest to origin if $b$ is in the range of A. (I can see the least-squares solution is an extension of this ""closest to origin"" property). From a geometric intuitive way if possible, how can we show this property of SVD?","I am studying the Singular Value Decomposition and its properties. It is widely used in order to solve equations of the form . I have seen the following: When we have the equation system , we calculate the SVD of A as . Then we calculate . has the reciprocals ( ) of the singular values in its diagonal and zeros where .  If the is in the range of then it is the solution that has the minimum norm (closest to origin). If it is not in the range, then it is the least-squares solution. I fail to see how exactly this procedure always produces a which is closest to origin if is in the range of A. (I can see the least-squares solution is an extension of this ""closest to origin"" property). From a geometric intuitive way if possible, how can we show this property of SVD?",Ax=b Ax=b A=U\Sigma V^T x'= V \Sigma^{+}U^Tb \Sigma^{+} \dfrac{1}{\sigma_i} \sigma_i=0 b A x' b,"['linear-algebra', 'optimization', 'svd', 'least-squares']"
80,Eigenvalues of $A$ and $A + A^T$,Eigenvalues of  and,A A + A^T,"This question has popped up at me several times in my research in differential equations and other areas: Let $A$ be a real $N \times N$ matrix.  How are the eigenvalues of $A$ and $A + A^T$ related? Of course, if $A$ is symmetric, the answer is easy:  they are the same up to factor or $2$, since then $A + A^T = 2A$.  But if $A \ne A^T$? I'm particularly interested in the question of the real parts of the eigenvalues.  How are the real parts of the eigenvalues of $A$ related to the (necessarily) real eigenvalues of $A + A^T$? Answers for complex matrices appreciated as well. Any references, citings, or explanations at any level of detail will be appreciated. Thanks in advance.","This question has popped up at me several times in my research in differential equations and other areas: Let $A$ be a real $N \times N$ matrix.  How are the eigenvalues of $A$ and $A + A^T$ related? Of course, if $A$ is symmetric, the answer is easy:  they are the same up to factor or $2$, since then $A + A^T = 2A$.  But if $A \ne A^T$? I'm particularly interested in the question of the real parts of the eigenvalues.  How are the real parts of the eigenvalues of $A$ related to the (necessarily) real eigenvalues of $A + A^T$? Answers for complex matrices appreciated as well. Any references, citings, or explanations at any level of detail will be appreciated. Thanks in advance.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
81,Need verification - Prove a Hermitian matrix $(\textbf{A}^\ast = \textbf{A})$ has only real eigenvalues,Need verification - Prove a Hermitian matrix  has only real eigenvalues,(\textbf{A}^\ast = \textbf{A}),Proof: Let eigenvalue $\lambda \neq 0$ such as  $$\textbf{A}\vec{v}  = \lambda\vec{v}$$ $$\Rightarrow (\textbf{A}\vec{v})^\ast = (\lambda\vec{v})^\ast$$ $$\Rightarrow (\vec{v}^\ast\textbf{A}^\ast)=(\lambda^\ast\vec{v}^\ast)$$ Right-multiply both sides by $\color{orangered}{\vec{v}}$$$\Rightarrow (\vec{v}^\ast\textbf{A}^\ast \color{orangered}{\vec{v}} )=(\lambda^\ast\vec{v}^\ast \color{orangered}{\vec{v}} )$$ $$\textbf{A}^\ast=\textbf{A}$$ $$\Rightarrow(\vec{v}^\ast\textbf{A}\vec{v})=(\lambda^\ast\vec{v}^\ast\vec{v})$$ $$\Rightarrow(\vec{v}^\ast\lambda\vec{v}) = (\lambda^\ast\vec{v}^\ast\vec{v})$$ $$\Rightarrow(\lambda\vec{v}^\ast\vec{v}) =  (\lambda^\ast\vec{v}^\ast\vec{v})$$ $$\Rightarrow \lambda = \lambda^\ast$$ $$\Rightarrow \lambda\in\mathbb{R}$$,Proof: Let eigenvalue $\lambda \neq 0$ such as  $$\textbf{A}\vec{v}  = \lambda\vec{v}$$ $$\Rightarrow (\textbf{A}\vec{v})^\ast = (\lambda\vec{v})^\ast$$ $$\Rightarrow (\vec{v}^\ast\textbf{A}^\ast)=(\lambda^\ast\vec{v}^\ast)$$ Right-multiply both sides by $\color{orangered}{\vec{v}}$$$\Rightarrow (\vec{v}^\ast\textbf{A}^\ast \color{orangered}{\vec{v}} )=(\lambda^\ast\vec{v}^\ast \color{orangered}{\vec{v}} )$$ $$\textbf{A}^\ast=\textbf{A}$$ $$\Rightarrow(\vec{v}^\ast\textbf{A}\vec{v})=(\lambda^\ast\vec{v}^\ast\vec{v})$$ $$\Rightarrow(\vec{v}^\ast\lambda\vec{v}) = (\lambda^\ast\vec{v}^\ast\vec{v})$$ $$\Rightarrow(\lambda\vec{v}^\ast\vec{v}) =  (\lambda^\ast\vec{v}^\ast\vec{v})$$ $$\Rightarrow \lambda = \lambda^\ast$$ $$\Rightarrow \lambda\in\mathbb{R}$$,,"['linear-algebra', 'proof-verification']"
82,Proving a matrix inequality,Proving a matrix inequality,,"Let $A, B \in \mathbb R^{m\times m}$ be symmetric positive semi-definite matrices. Is it true that $$\sup_{\|x\| = 1} \left| \|Ax\| - \|Bx\| \right| \geq c(m) \|A-B\|,$$ with $c(m) > 0$ and where $\|\cdot\|$ denotes the 2-norm? Here is how I approached the problem. Let us introduce the notation $\Delta = A -B$ for convenience. Without loss of generality, we can assume that $B$ is diagonal, that $\| \Delta\|$ = 1, and that $\Delta$ has an eigenvalue equal to $+1$. We have $$ \begin{align}                 \|Ax\| = \sqrt{x^T (B+\Delta)^2 x} &= \sqrt{x^T B^2 x + x^T (B \Delta + \Delta B) x + x^T \Delta^2 x} \\                                           &= \sqrt{\left(\sqrt {x^T B^2 x} + \sqrt{x^T \Delta^2 x}\right)^2 - 2 \left(\sqrt{x^T B^2 x} \sqrt{x^T \Delta^2 x} - x^T B \Delta x\right)}. \end{align} $$         By Cauchy-Schwarz inequality, the expression in the second brackets is non-negative, so $$ \begin{align}          \sup_{\|x\| = 1} \, \left| \|Ax\| - \|Bx\| \right|   &\geq \sup_{\|x\| = 1}\frac{|2 \, x^T B \Delta x + x^T \Delta^2 x|}{2(\sqrt {x^T B^2 x} + \sqrt{x^T \Delta^2 x})} \end{align} $$ From here things get more complicated and any help would be appreciated. Of course, there might be other approaches to the problem. Thank you. Bonus question: Is the statement true for positive self-adjoint operators on a Hilbert space?","Let $A, B \in \mathbb R^{m\times m}$ be symmetric positive semi-definite matrices. Is it true that $$\sup_{\|x\| = 1} \left| \|Ax\| - \|Bx\| \right| \geq c(m) \|A-B\|,$$ with $c(m) > 0$ and where $\|\cdot\|$ denotes the 2-norm? Here is how I approached the problem. Let us introduce the notation $\Delta = A -B$ for convenience. Without loss of generality, we can assume that $B$ is diagonal, that $\| \Delta\|$ = 1, and that $\Delta$ has an eigenvalue equal to $+1$. We have $$ \begin{align}                 \|Ax\| = \sqrt{x^T (B+\Delta)^2 x} &= \sqrt{x^T B^2 x + x^T (B \Delta + \Delta B) x + x^T \Delta^2 x} \\                                           &= \sqrt{\left(\sqrt {x^T B^2 x} + \sqrt{x^T \Delta^2 x}\right)^2 - 2 \left(\sqrt{x^T B^2 x} \sqrt{x^T \Delta^2 x} - x^T B \Delta x\right)}. \end{align} $$         By Cauchy-Schwarz inequality, the expression in the second brackets is non-negative, so $$ \begin{align}          \sup_{\|x\| = 1} \, \left| \|Ax\| - \|Bx\| \right|   &\geq \sup_{\|x\| = 1}\frac{|2 \, x^T B \Delta x + x^T \Delta^2 x|}{2(\sqrt {x^T B^2 x} + \sqrt{x^T \Delta^2 x})} \end{align} $$ From here things get more complicated and any help would be appreciated. Of course, there might be other approaches to the problem. Thank you. Bonus question: Is the statement true for positive self-adjoint operators on a Hilbert space?",,"['linear-algebra', 'matrices', 'inequality']"
83,Is a field determined by its family of general linear groups?,Is a field determined by its family of general linear groups?,,"Assume that $K,L$ are fields such that there is an isomorphism of groups $\mathrm{GL}_n(K) \cong \mathrm{GL}_n(L)$ for all $n \in \mathbb{N}$. Does it follow that $K \cong L$? I am also interested in related results, for example if it is enough to test small values for $n$ and what happens for $\mathrm{PGL}_n$.","Assume that $K,L$ are fields such that there is an isomorphism of groups $\mathrm{GL}_n(K) \cong \mathrm{GL}_n(L)$ for all $n \in \mathbb{N}$. Does it follow that $K \cong L$? I am also interested in related results, for example if it is enough to test small values for $n$ and what happens for $\mathrm{PGL}_n$.",,"['linear-algebra', 'abstract-algebra', 'group-theory', 'field-theory']"
84,How do I find the common invariant subspaces of a span of matrices?,How do I find the common invariant subspaces of a span of matrices?,,"Let $G_1, \ldots, G_n$ be a set of $m\times m$ linearly-independent complex matrices. Let $\mathcal{G} = \operatorname{span}\left\{ G_1, \ldots , G_n\right\}$ be the vector space that spans the set of $G$ matrices, and let $\mathcal{S}$ be any linear subspace of $\mathcal{G}$. Finally, let $\mathcal{I}$ be a linear subspace of $\mathbb{C}^m$. Is there a way to determine all pairs $\left( \mathcal{S}, \mathcal{I}\right)$ such that for all $S \in \mathcal{S}$, the subspace $\mathcal{I}$ is an invariant subspace of $S$? In other words, is there a way to determine the linear subspaces of $\mathcal{G}$ whose elements share a (non-trivial) invariant subspace? This problem is vaguely related to this one posted in 2013. I know that if we arbitrarily pick an $\mathcal{I}$, we can determine the largest subspace $\mathcal{S}$ for which $\mathcal{I}$ is invariant. The problem is that there are infinitely many possible $\mathcal{I}$ to choose from. Likewise, if we arbitrarily choose an $\mathcal{S}$, we can deduce $\mathcal{I}$, but there are infinitely many $\mathcal{S}$ to choose from. I haven't made any progress trying to determine both subspaces simultaneously. Any assistance would be greatly appreciated. Even some suggestions on where I might look to find a solution would be much appreciated.","Let $G_1, \ldots, G_n$ be a set of $m\times m$ linearly-independent complex matrices. Let $\mathcal{G} = \operatorname{span}\left\{ G_1, \ldots , G_n\right\}$ be the vector space that spans the set of $G$ matrices, and let $\mathcal{S}$ be any linear subspace of $\mathcal{G}$. Finally, let $\mathcal{I}$ be a linear subspace of $\mathbb{C}^m$. Is there a way to determine all pairs $\left( \mathcal{S}, \mathcal{I}\right)$ such that for all $S \in \mathcal{S}$, the subspace $\mathcal{I}$ is an invariant subspace of $S$? In other words, is there a way to determine the linear subspaces of $\mathcal{G}$ whose elements share a (non-trivial) invariant subspace? This problem is vaguely related to this one posted in 2013. I know that if we arbitrarily pick an $\mathcal{I}$, we can determine the largest subspace $\mathcal{S}$ for which $\mathcal{I}$ is invariant. The problem is that there are infinitely many possible $\mathcal{I}$ to choose from. Likewise, if we arbitrarily choose an $\mathcal{S}$, we can deduce $\mathcal{I}$, but there are infinitely many $\mathcal{S}$ to choose from. I haven't made any progress trying to determine both subspaces simultaneously. Any assistance would be greatly appreciated. Even some suggestions on where I might look to find a solution would be much appreciated.",,"['linear-algebra', 'matrices', 'vector-spaces', 'eigenvalues-eigenvectors']"
85,How many points are needed to uniquely define an ellipse?,How many points are needed to uniquely define an ellipse?,,"I recently asked a question on this forum regarding why 3 points guaranteed the presence or absence of a unique equation representing a specific circle. (link here What do ""3 different points"" have to do with linear dependence in determining a unique circle? ) Shortly after this, I came across a question in my book that provided a picture of 4 red dots (image below) and asked, ""How many ellipses do these 4 red points define"". Having read the comments on my post with the circle, I thought that this was fairly straight forward. I chose "" 1 "". This was wrong. The answer was infinite. This caught me as surprising as I didn't think of the equations for a circle and an ellipse as differing by much beyond a scaling factor for each quadratic term. I know that the general equation for an ellipse is as follows: $$\left(\frac{x-h}a\right)^2 + \left(\frac{y-k}b\right)^2 = 1$$ The only thing I can think of is that because of the added scaling factors, there are now technically two additional unknowns (for a total of 4 different unknowns... h, a, k, and b), and therefore I need 4 points to specify an unique ellipse. However, I thought to myself again, even if the ellipse is not centered at the origin, if all 4 given points happened to coincide with the intersection between the major axis and the ellipse and the minor axis and the ellipse, then certainly that would specify an unique ellipse. If this is true, then why does the arrangement of the points matter in determining whether or not an unique ellipse is specified? Visual explanations would be greatly appreciated!","I recently asked a question on this forum regarding why 3 points guaranteed the presence or absence of a unique equation representing a specific circle. (link here What do ""3 different points"" have to do with linear dependence in determining a unique circle? ) Shortly after this, I came across a question in my book that provided a picture of 4 red dots (image below) and asked, ""How many ellipses do these 4 red points define"". Having read the comments on my post with the circle, I thought that this was fairly straight forward. I chose "" 1 "". This was wrong. The answer was infinite. This caught me as surprising as I didn't think of the equations for a circle and an ellipse as differing by much beyond a scaling factor for each quadratic term. I know that the general equation for an ellipse is as follows: The only thing I can think of is that because of the added scaling factors, there are now technically two additional unknowns (for a total of 4 different unknowns... h, a, k, and b), and therefore I need 4 points to specify an unique ellipse. However, I thought to myself again, even if the ellipse is not centered at the origin, if all 4 given points happened to coincide with the intersection between the major axis and the ellipse and the minor axis and the ellipse, then certainly that would specify an unique ellipse. If this is true, then why does the arrangement of the points matter in determining whether or not an unique ellipse is specified? Visual explanations would be greatly appreciated!",\left(\frac{x-h}a\right)^2 + \left(\frac{y-k}b\right)^2 = 1,"['linear-algebra', 'conic-sections']"
86,The need for the Gram–Schmidt process,The need for the Gram–Schmidt process,,"As far as I understood Gram–Schmidt orthogonalization starts with a set of linearly independent vectors and produces a set of mutually orthonormal vectors that spans the same space that starting vectors did. I have no problem understanding the algorithm, but here is a thing I fail to get. Why do I need to do all these calculations? For example, instead of doing the calculations provided in that wiki page in example section, why can't I just grab two basis vectors $w_1 = (1, 0)'$ and $w_2 = (0, 1)'$? They are clearly orthonormal and span the same subspace as the original vectors $v_1 = (3, 1)'$, $v_2 = (2, 2)'$. It is clear that I'm missing something important, but I can't see what exactly.","As far as I understood Gram–Schmidt orthogonalization starts with a set of linearly independent vectors and produces a set of mutually orthonormal vectors that spans the same space that starting vectors did. I have no problem understanding the algorithm, but here is a thing I fail to get. Why do I need to do all these calculations? For example, instead of doing the calculations provided in that wiki page in example section, why can't I just grab two basis vectors $w_1 = (1, 0)'$ and $w_2 = (0, 1)'$? They are clearly orthonormal and span the same subspace as the original vectors $v_1 = (3, 1)'$, $v_2 = (2, 2)'$. It is clear that I'm missing something important, but I can't see what exactly.",,"['linear-algebra', 'orthonormal', 'gram-schmidt']"
87,Determinant of large matrices: there's GOTTA be a faster way,Determinant of large matrices: there's GOTTA be a faster way,,"WARNING this is a very long report and is likely going to cause boredom. Be warned!! I've heard of the determinant of small matrices, such as: $$\det \begin{pmatrix}  a&b\\ c&d\\ \end{pmatrix} = ad-bc $$ case in point: $$\det \begin{pmatrix}  57&48\\ 79&102\\ \end{pmatrix} = 57\times 102-48\times 79 =5814-3792 =2022 $$ This is a pretty hefty example i found in one of my books on vectors and matrices. And there are much more complex examples. for instance, to find the determinant of a matrix of order 3, you do this: $$\begin{align} &\det \begin{pmatrix} a&b&c\\ d&e&f\\ g&h&i\\ \end{pmatrix}\\ &=a\times \det \begin{bmatrix} e&f\\ h&i\\ \end{bmatrix}\\ &-b\times \det \begin{bmatrix} d&f\\ g&i\\ \end{bmatrix}\\ &+c\times \det \begin{bmatrix} d&e\\ g&h\\ \end{bmatrix} \end{align}$$ This sequence looks a bit simple, but in reality it blows up(becoimes increasingly large) after a while. for instance, with a $5\times 5$ matrix someone asked me to model, this is how my 'fun time' went: $$ \begin{align} &\det \begin{Bmatrix} a&b&c&d&e\\ f&g&h&i&j\\ k&l&m&n&o\\ p&q&r&s&t\\ u&v&w&x&y\\ \end{Bmatrix}\\ &=a\times \det \begin{Bmatrix} g&h&i&j\\ l&m&n&o\\ q&r&s&t\\ v&w&x&y\\ \end{Bmatrix} -b\times \det \begin{Bmatrix} f&h&i&j\\ k&m&n&o\\ p&r&s&t\\ u&w&x&y\\ \end{Bmatrix} +c\times \det \begin{Bmatrix} f&g&i&j\\ k&l&n&o\\ p&q&s&t\\ u&v&x&y\\ \end{Bmatrix}\\ &-d\times \det \begin{Bmatrix} f&g&h&j\\ k&l&m&o\\ p&q&r&t\\ u&v&w&y\\ \end{Bmatrix} +e\times \det \begin{Bmatrix} f&g&h&i\\ k&l&m&n\\ p&q&r&s\\ u&v&w&x\\ \end{Bmatrix} \end{align} $$ This is a complex wad of calculations for me to completely do. so I'll break it down into the 5 conponents: A, B, C, D, and E, respectively. $$ A=a\times \det \begin{Bmatrix} g&h&i&j\\ l&m&n&o\\ q&r&s&t\\ v&w&x&y\\ \end{Bmatrix} \\ =a\left( g\times \det \begin{Bmatrix} m&n&o\\ r&s&t\\ w&x&y\\ \end{Bmatrix} -h\times \det \begin{Bmatrix} l&n&o\\ q&s&t\\ v&x&y\\ \end{Bmatrix} +i\times \det \begin{Bmatrix} l&m&o\\ q&r&t\\ v&w&y\\ \end{Bmatrix} -j\times \det \begin{Bmatrix} l&m&n\\ q&r&s\\ v&w&x\\ \end{Bmatrix} \right)\\ =a\left( g\left( m\times \det \begin{Bmatrix} s&t\\ x&y\\ \end{Bmatrix} -n\times \det \begin{Bmatrix} r&t\\ w&y\\ \end{Bmatrix} +o\times \det \begin{Bmatrix} r&s\\ w&x\\ \end{Bmatrix} \right)\\ -h\left( l\times \det \begin{Bmatrix} s&t\\ x&y\\ \end{Bmatrix} -n\times \det \begin{Bmatrix} q&t\\ v&y\\ \end{Bmatrix} +o\times \det \begin{Bmatrix} q&s\\ v&x\\ \end{Bmatrix} \right)\\ +i\left( l\times \det \begin{Bmatrix} r&t\\ w&y\\ \end{Bmatrix} -m\times \det \begin{Bmatrix} q&t\\ v&y\\ \end{Bmatrix} +o\times \det \begin{Bmatrix} q&r\\ v&w\\ \end{Bmatrix} \right) -j\left( l\times \det \begin{Bmatrix} r&s\\ w&x\\ \end{Bmatrix} -m\times \det \begin{Bmatrix} q&s\\ v&x\\ \end{Bmatrix} +n\times \det \begin{Bmatrix} q&r\\ v&w\\ \end{Bmatrix} \right) \right)\\ =a\left( g\left(m(sy-xt)-n(ry-wt)+o(rx-ws)\right)\\ -h\left(l(sy-xt)-n(qy-vt)+o(qx-vs)\right)\\ +i\left(l(ry-wt)-m(qy-vt)+o(qw-vr)\right)\\ -j\left(l(rx-ws)-m(qx-vs)+n(qw-vr)\right) \right) $$ (If you want to see this behemoth in code form, go to this page , but i'm not $100$ % sure that it will work.) $$ B= -b\times \det \begin{Bmatrix} f&h&i&j\\ k&m&n&o\\ p&r&s&t\\ u&w&x&y\\ \end{Bmatrix}\\ -b\left( f\times \det \begin{Bmatrix} m&n&o\\ r&s&t\\ w&x&y\\ \end{Bmatrix} -h\times \det \begin{Bmatrix} k&n&o\\ p&s&t\\ u&x&y\\ \end{Bmatrix} +i\times \det \begin{Bmatrix} k&m&o\\ p&r&t\\ u&w&y\\ \end{Bmatrix} -j\times \det \begin{Bmatrix} k&m&n\\ p&r&s\\ u&w&x\\ \end{Bmatrix} \right)\\ =-b\left( f\left( m\times \det \begin{Bmatrix} s&t\\ x&y\\ \end{Bmatrix} -n\times \det \begin{Bmatrix} r&t\\ w&y\\ \end{Bmatrix} +o\times \det \begin{Bmatrix} r&s\\ w&x\\ \end{Bmatrix} \right)\\ -h\left( k\times \det \begin{Bmatrix} s&t\\ x&y\\ \end{Bmatrix} -n\times \det \begin{Bmatrix} p&t\\ u&y\\ \end{Bmatrix} +o\times \det \begin{Bmatrix} p&s\\ u&x\\ \end{Bmatrix} \right)\\ +i\left( k\times \det \begin{Bmatrix} r&t\\ w&y\\ \end{Bmatrix} -m\times \det \begin{Bmatrix} p&t\\ u&y\\ \end{Bmatrix} +o\times \det \begin{Bmatrix} p&r\\ u&w\\ \end{Bmatrix} \right) -j\left( k\times \det \begin{Bmatrix} r&s\\ w&x\\ \end{Bmatrix} -m\times \det \begin{Bmatrix} p&s\\ u&x\\ \end{Bmatrix} +n\times \det \begin{Bmatrix} p&r\\ u&w\\ \end{Bmatrix} \right) \right)\\ =-b\left( f\left(m(sy-xt)-n(ry-wt)+o(rx-ws)\right)\\ -h\left(k(sy-xt)-n(py-ut)+o(px-us)\right)\\ +i\left(k(ry-wt)-m(py-ut)+o(pw-ur)\right)\\ -j\left(k(rx-ws)-m(px-us)+n(pw-ur)\right) \right) $$ and that is part b! this is a grueling amount of code for me to place. $\frac{3}{5}$ way to go... $$ C=c\times \det \begin{Bmatrix} f&g&i&j\\ k&l&n&o\\ p&q&s&t\\ u&v&x&y\\ \end{Bmatrix}\\ =c\left( f\times \det \begin{Bmatrix} l&n&o\\ q&s&t\\ v&x&y\\ \end{Bmatrix} -g\times \det \begin{Bmatrix} k&n&o\\ p&s&t\\ u&x&y\\ \end{Bmatrix} +i\times \det \begin{Bmatrix} k&l&o\\ p&q&t\\ u&v&y\\ \end{Bmatrix} -j\times \det \begin{Bmatrix} k&l&n\\ p&q&s\\ u&v&x\\ \end{Bmatrix} \right)\\ =c\left( f\left( l\times \det \begin{Bmatrix} s&t\\ x&y\\ \end{Bmatrix} -n\times \det \begin{Bmatrix} q&t\\ v&y\\ \end{Bmatrix} +o\times \det \begin{Bmatrix} q&s\\ v&x\\ \end{Bmatrix} \right)\\ -g\left( k\times \det \begin{Bmatrix} s&t\\ x&y\\ \end{Bmatrix} -n\times \det \begin{Bmatrix} p&t\\ u&y\\ \end{Bmatrix} +o\times \det \begin{Bmatrix} p&s\\ u&x\\ \end{Bmatrix} \right)\\ +i\left( k\times \det \begin{Bmatrix} q&t\\ v&y\\ \end{Bmatrix} -l\times \det \begin{Bmatrix} p&t\\ u&y\\ \end{Bmatrix} +o\times \det \begin{Bmatrix} p&q\\ u&v\\ \end{Bmatrix} \right)\\ -j\left( k\times \det \begin{Bmatrix} q&s\\ v&x\\ \end{Bmatrix} -l\times \det \begin{Bmatrix} p&s\\ u&x\\ \end{Bmatrix} +n\times \det \begin{Bmatrix} p&q\\ u&v\\ \end{Bmatrix} \right) \right)\\ =c\left( f\left(l(sy-xt)-n(qy-vt)+o(qx-vs)\right)\\ -g\left(k(sy-xt)-n(py-ut)+o(px-us)\right)\\ +i\left(k(qy-vt)-l(py-ut)+o(pv-uq)\right)\\ -j\left(k(qx-vs)-l(px-us)+n(pv-uq)\right) \right) $$ That's the C-section. now to get to the D-section... $$ D=-d\times \det \begin{Bmatrix} f&g&h&j\\ k&l&m&o\\ p&q&r&t\\ u&v&w&y\\ \end{Bmatrix}\\ =-d\left( f\times \det \begin{Bmatrix} l&m&o\\ q&r&t\\ v&w&y\\ \end{Bmatrix} -g\times \det \begin{Bmatrix} k&m&o\\ p&r&t\\ u&w&y\\ \end{Bmatrix} +h\times \det \begin{Bmatrix} k&l&o\\ p&q&t\\ u&v&y\\ \end{Bmatrix} -j\times \det \begin{Bmatrix} k&l&m\\ p&q&r\\ u&v&w\\ \end{Bmatrix} \right)\\ =-d\left( f\left( l\times \det \begin{Bmatrix} r&t\\ w&y\\ \end{Bmatrix} -m\times \det \begin{Bmatrix} q&t\\ v&y\\ \end{Bmatrix} +o\times \det \begin{Bmatrix} q&r\\ v&w\\ \end{Bmatrix} \right)\\ -g\left( k\times \det \begin{Bmatrix} r&t\\ w&y\\ \end{Bmatrix} -m\times \det \begin{Bmatrix} p&t\\ u&y\\ \end{Bmatrix} +o\times \det \begin{Bmatrix} p&r\\ u&w\\ \end{Bmatrix} \right)\\ +h\left( k\times \det \begin{Bmatrix} q&t\\ v&y\\ \end{Bmatrix} -l\times \det \begin{Bmatrix} p&t\\ u&y\\ \end{Bmatrix} +o\times \det \begin{Bmatrix} p&q\\ u&v\\ \end{Bmatrix} \right)\\ -j\left( k\times \det \begin{Bmatrix} q&r\\ v&w\\ \end{Bmatrix} -l\times \det \begin{Bmatrix} p&r\\ u&w\\ \end{Bmatrix} +m\times \det \begin{Bmatrix} p&q\\ u&v\\ \end{Bmatrix} \right) \right)\\ =-d\left( f\left(l(ry-wt)-m(qy-vt)+o(qw-vr)\right)\\ -g\left(k(ry-wt)-m(py-ut)+o(pw-ur)\right)\\ +h\left(k(qy-vt)-l(py-ut)+o(pv-uq)\right)\\ -j\left(k(qw-vr)-l(pw-ur)+m(pv-uq)\right) \right) $$ Are you bored yet? I am. Luckily, I got one more section to go... $$ E=e\times \det \begin{Bmatrix} f&g&h&i\\ k&l&m&n\\ p&q&r&s\\ u&v&w&x\\ \end{Bmatrix} =e\left( f\times \det \begin{Bmatrix} l&m&n\\ q&r&s\\ v&w&x\\ \end{Bmatrix} -g\times \det \begin{Bmatrix} k&m&n\\ p&r&s\\ u&w&x\\ \end{Bmatrix} +h\times \det \begin{Bmatrix} k&l&n\\ p&q&s\\ u&v&x\\ \end{Bmatrix} -i\times \det \begin{Bmatrix} k&l&m\\ p&q&r\\ u&v&w\\ \end{Bmatrix} \right)\\ =e\left( f\left( l\times \det \begin{Bmatrix} r&s\\ w&x\\ \end{Bmatrix} -m\times \det \begin{Bmatrix} q&s\\ v&x\\ \end{Bmatrix} +n\times \det \begin{Bmatrix} q&r\\ v&w\\ \end{Bmatrix} \right)\\ -g\left( k\times \det \begin{Bmatrix} r&s\\ w&x\\ \end{Bmatrix} -m\times \det \begin{Bmatrix} p&s\\ u&x\\ \end{Bmatrix} +n\times \det \begin{Bmatrix} p&r\\ u&w\\ \end{Bmatrix} \right)\\ +h\left( k\times \det \begin{Bmatrix} q&s\\ v&x\\ \end{Bmatrix} -l\times \det \begin{Bmatrix} p&s\\ u&x\\ \end{Bmatrix} +n\times \det \begin{Bmatrix} p&q\\ u&v\\ \end{Bmatrix} \right)\\ -i\left( k\times \det \begin{Bmatrix} q&r\\ v&w\\ \end{Bmatrix} -l\times \det \begin{Bmatrix} p&r\\ u&w\\ \end{Bmatrix} +m\times \det \begin{Bmatrix} p&q\\ u&v\\ \end{Bmatrix} \right) \right)\\ =e\left( f\left(l(rx-ws)-m(qx-vs)+n(qw-vr)\right)\\ -g\left(k(rx-ws)-m(px-us)+n(pw-ur)\right)\\ +h\left(k(qx-vs)-l(px-us)+n(pv-uq)\right)\\ -i\left(k(qw-vr)-l(pw-ur)+m(pv-uq)\right) \right) $$ ZZZZZZZZZZZZ... GAH! okay... to recap: $$ \det \begin{Bmatrix} a&b&c&d&e\\ f&g&h&i&j\\ k&l&m&n&o\\ p&q&r&s&t\\ u&v&w&x&y\\ \end{Bmatrix}\\ =a\left( g\left(m(sy-xt)-n(ry-wt)+o(rx-ws)\right)\\ -h\left(l(sy-xt)-n(qy-vt)+o(qx-vs)\right)\\ +i\left(l(ry-wt)-m(qy-vt)+o(qw-vr)\right)\\ -j\left(l(rx-ws)-m(qx-vs)+n(qw-vr)\right) \right)\\ -b\left( f\left(m(sy-xt)-n(ry-wt)+o(rx-ws)\right)\\ -h\left(k(sy-xt)-n(py-ut)+o(px-us)\right)\\ +i\left(k(ry-wt)-m(py-ut)+o(pw-ur)\right)\\ -j\left(k(rx-ws)-m(px-us)+n(pw-ur)\right) \right)\\ +c\left( f\left(l(sy-xt)-n(qy-vt)+o(qx-vs)\right)\\ -g\left(k(sy-xt)-n(py-ut)+o(px-us)\right)\\ +i\left(k(qy-vt)-l(py-ut)+o(pv-uq)\right)\\ -j\left(k(qx-vs)-l(px-us)+n(pv-uq)\right) \right)\\ -d\left( f\left(l(ry-wt)-m(qy-vt)+o(qw-vr)\right)\\ -g\left(k(ry-wt)-m(py-ut)+o(pw-ur)\right)\\ +h\left(k(qy-vt)-l(py-ut)+o(pv-uq)\right)\\ -j\left(k(qw-vr)-l(pw-ur)+m(pv-uq)\right) \right)\\ +e\left( f\left(l(rx-ws)-m(qx-vs)+n(qw-vr)\right)\\ -g\left(k(rx-ws)-m(px-us)+n(pw-ur)\right)\\ +h\left(k(qx-vs)-l(px-us)+n(pv-uq)\right)\\ -i\left(k(qw-vr)-l(pw-ur)+m(pv-uq)\right) \right) $$ Now that THAT'S over ( STOP SCROLLING!! ), I must mention that I pretty much blew my friend's mind showing him this. NOW he wants me to figure out a matrix of order 10. AURRRRRRRRUUUUUUUUUUUUGGGGGGGGGGHHHHHHHHHH!!!!!!! I DONT HAVE THE TIME!!!! Therefore, I am wondering if there is a faster way to calculate the determinant of a HUGE matrix. hope there is. Thanks in advance. EDIT i was conversating with my friend, explaining how timewasting calculating a matrix of order 10 is, and i convinced him to drop the 'do by hand' idea, and instead do it on the computer.","WARNING this is a very long report and is likely going to cause boredom. Be warned!! I've heard of the determinant of small matrices, such as: case in point: This is a pretty hefty example i found in one of my books on vectors and matrices. And there are much more complex examples. for instance, to find the determinant of a matrix of order 3, you do this: This sequence looks a bit simple, but in reality it blows up(becoimes increasingly large) after a while. for instance, with a matrix someone asked me to model, this is how my 'fun time' went: This is a complex wad of calculations for me to completely do. so I'll break it down into the 5 conponents: A, B, C, D, and E, respectively. (If you want to see this behemoth in code form, go to this page , but i'm not % sure that it will work.) and that is part b! this is a grueling amount of code for me to place. way to go... That's the C-section. now to get to the D-section... Are you bored yet? I am. Luckily, I got one more section to go... ZZZZZZZZZZZZ... GAH! okay... to recap: Now that THAT'S over ( STOP SCROLLING!! ), I must mention that I pretty much blew my friend's mind showing him this. NOW he wants me to figure out a matrix of order 10. AURRRRRRRRUUUUUUUUUUUUGGGGGGGGGGHHHHHHHHHH!!!!!!! I DONT HAVE THE TIME!!!! Therefore, I am wondering if there is a faster way to calculate the determinant of a HUGE matrix. hope there is. Thanks in advance. EDIT i was conversating with my friend, explaining how timewasting calculating a matrix of order 10 is, and i convinced him to drop the 'do by hand' idea, and instead do it on the computer.","\det
\begin{pmatrix} 
a&b\\
c&d\\
\end{pmatrix}
=
ad-bc
 \det
\begin{pmatrix} 
57&48\\
79&102\\
\end{pmatrix}
=
57\times 102-48\times 79
=5814-3792
=2022
 \begin{align}
&\det
\begin{pmatrix}
a&b&c\\
d&e&f\\
g&h&i\\
\end{pmatrix}\\
&=a\times
\det
\begin{bmatrix}
e&f\\
h&i\\
\end{bmatrix}\\
&-b\times
\det
\begin{bmatrix}
d&f\\
g&i\\
\end{bmatrix}\\
&+c\times
\det
\begin{bmatrix}
d&e\\
g&h\\
\end{bmatrix}
\end{align} 5\times 5 
\begin{align}
&\det
\begin{Bmatrix}
a&b&c&d&e\\
f&g&h&i&j\\
k&l&m&n&o\\
p&q&r&s&t\\
u&v&w&x&y\\
\end{Bmatrix}\\
&=a\times
\det
\begin{Bmatrix}
g&h&i&j\\
l&m&n&o\\
q&r&s&t\\
v&w&x&y\\
\end{Bmatrix}
-b\times
\det
\begin{Bmatrix}
f&h&i&j\\
k&m&n&o\\
p&r&s&t\\
u&w&x&y\\
\end{Bmatrix}
+c\times
\det
\begin{Bmatrix}
f&g&i&j\\
k&l&n&o\\
p&q&s&t\\
u&v&x&y\\
\end{Bmatrix}\\
&-d\times
\det
\begin{Bmatrix}
f&g&h&j\\
k&l&m&o\\
p&q&r&t\\
u&v&w&y\\
\end{Bmatrix}
+e\times
\det
\begin{Bmatrix}
f&g&h&i\\
k&l&m&n\\
p&q&r&s\\
u&v&w&x\\
\end{Bmatrix}
\end{align}
 
A=a\times
\det
\begin{Bmatrix}
g&h&i&j\\
l&m&n&o\\
q&r&s&t\\
v&w&x&y\\
\end{Bmatrix}
\\
=a\left(
g\times
\det
\begin{Bmatrix}
m&n&o\\
r&s&t\\
w&x&y\\
\end{Bmatrix}
-h\times
\det
\begin{Bmatrix}
l&n&o\\
q&s&t\\
v&x&y\\
\end{Bmatrix}
+i\times
\det
\begin{Bmatrix}
l&m&o\\
q&r&t\\
v&w&y\\
\end{Bmatrix}
-j\times
\det
\begin{Bmatrix}
l&m&n\\
q&r&s\\
v&w&x\\
\end{Bmatrix}
\right)\\
=a\left(
g\left(
m\times
\det
\begin{Bmatrix}
s&t\\
x&y\\
\end{Bmatrix}
-n\times
\det
\begin{Bmatrix}
r&t\\
w&y\\
\end{Bmatrix}
+o\times
\det
\begin{Bmatrix}
r&s\\
w&x\\
\end{Bmatrix}
\right)\\
-h\left(
l\times
\det
\begin{Bmatrix}
s&t\\
x&y\\
\end{Bmatrix}
-n\times
\det
\begin{Bmatrix}
q&t\\
v&y\\
\end{Bmatrix}
+o\times
\det
\begin{Bmatrix}
q&s\\
v&x\\
\end{Bmatrix}
\right)\\
+i\left(
l\times
\det
\begin{Bmatrix}
r&t\\
w&y\\
\end{Bmatrix}
-m\times
\det
\begin{Bmatrix}
q&t\\
v&y\\
\end{Bmatrix}
+o\times
\det
\begin{Bmatrix}
q&r\\
v&w\\
\end{Bmatrix}
\right)
-j\left(
l\times
\det
\begin{Bmatrix}
r&s\\
w&x\\
\end{Bmatrix}
-m\times
\det
\begin{Bmatrix}
q&s\\
v&x\\
\end{Bmatrix}
+n\times
\det
\begin{Bmatrix}
q&r\\
v&w\\
\end{Bmatrix}
\right)
\right)\\
=a\left(
g\left(m(sy-xt)-n(ry-wt)+o(rx-ws)\right)\\
-h\left(l(sy-xt)-n(qy-vt)+o(qx-vs)\right)\\
+i\left(l(ry-wt)-m(qy-vt)+o(qw-vr)\right)\\
-j\left(l(rx-ws)-m(qx-vs)+n(qw-vr)\right)
\right)
 100 
B=
-b\times
\det
\begin{Bmatrix}
f&h&i&j\\
k&m&n&o\\
p&r&s&t\\
u&w&x&y\\
\end{Bmatrix}\\
-b\left(
f\times
\det
\begin{Bmatrix}
m&n&o\\
r&s&t\\
w&x&y\\
\end{Bmatrix}
-h\times
\det
\begin{Bmatrix}
k&n&o\\
p&s&t\\
u&x&y\\
\end{Bmatrix}
+i\times
\det
\begin{Bmatrix}
k&m&o\\
p&r&t\\
u&w&y\\
\end{Bmatrix}
-j\times
\det
\begin{Bmatrix}
k&m&n\\
p&r&s\\
u&w&x\\
\end{Bmatrix}
\right)\\
=-b\left(
f\left(
m\times
\det
\begin{Bmatrix}
s&t\\
x&y\\
\end{Bmatrix}
-n\times
\det
\begin{Bmatrix}
r&t\\
w&y\\
\end{Bmatrix}
+o\times
\det
\begin{Bmatrix}
r&s\\
w&x\\
\end{Bmatrix}
\right)\\
-h\left(
k\times
\det
\begin{Bmatrix}
s&t\\
x&y\\
\end{Bmatrix}
-n\times
\det
\begin{Bmatrix}
p&t\\
u&y\\
\end{Bmatrix}
+o\times
\det
\begin{Bmatrix}
p&s\\
u&x\\
\end{Bmatrix}
\right)\\
+i\left(
k\times
\det
\begin{Bmatrix}
r&t\\
w&y\\
\end{Bmatrix}
-m\times
\det
\begin{Bmatrix}
p&t\\
u&y\\
\end{Bmatrix}
+o\times
\det
\begin{Bmatrix}
p&r\\
u&w\\
\end{Bmatrix}
\right)
-j\left(
k\times
\det
\begin{Bmatrix}
r&s\\
w&x\\
\end{Bmatrix}
-m\times
\det
\begin{Bmatrix}
p&s\\
u&x\\
\end{Bmatrix}
+n\times
\det
\begin{Bmatrix}
p&r\\
u&w\\
\end{Bmatrix}
\right)
\right)\\
=-b\left(
f\left(m(sy-xt)-n(ry-wt)+o(rx-ws)\right)\\
-h\left(k(sy-xt)-n(py-ut)+o(px-us)\right)\\
+i\left(k(ry-wt)-m(py-ut)+o(pw-ur)\right)\\
-j\left(k(rx-ws)-m(px-us)+n(pw-ur)\right)
\right)
 \frac{3}{5} 
C=c\times
\det
\begin{Bmatrix}
f&g&i&j\\
k&l&n&o\\
p&q&s&t\\
u&v&x&y\\
\end{Bmatrix}\\
=c\left(
f\times
\det
\begin{Bmatrix}
l&n&o\\
q&s&t\\
v&x&y\\
\end{Bmatrix}
-g\times
\det
\begin{Bmatrix}
k&n&o\\
p&s&t\\
u&x&y\\
\end{Bmatrix}
+i\times
\det
\begin{Bmatrix}
k&l&o\\
p&q&t\\
u&v&y\\
\end{Bmatrix}
-j\times
\det
\begin{Bmatrix}
k&l&n\\
p&q&s\\
u&v&x\\
\end{Bmatrix}
\right)\\
=c\left(
f\left(
l\times
\det
\begin{Bmatrix}
s&t\\
x&y\\
\end{Bmatrix}
-n\times
\det
\begin{Bmatrix}
q&t\\
v&y\\
\end{Bmatrix}
+o\times
\det
\begin{Bmatrix}
q&s\\
v&x\\
\end{Bmatrix}
\right)\\
-g\left(
k\times
\det
\begin{Bmatrix}
s&t\\
x&y\\
\end{Bmatrix}
-n\times
\det
\begin{Bmatrix}
p&t\\
u&y\\
\end{Bmatrix}
+o\times
\det
\begin{Bmatrix}
p&s\\
u&x\\
\end{Bmatrix}
\right)\\
+i\left(
k\times
\det
\begin{Bmatrix}
q&t\\
v&y\\
\end{Bmatrix}
-l\times
\det
\begin{Bmatrix}
p&t\\
u&y\\
\end{Bmatrix}
+o\times
\det
\begin{Bmatrix}
p&q\\
u&v\\
\end{Bmatrix}
\right)\\
-j\left(
k\times
\det
\begin{Bmatrix}
q&s\\
v&x\\
\end{Bmatrix}
-l\times
\det
\begin{Bmatrix}
p&s\\
u&x\\
\end{Bmatrix}
+n\times
\det
\begin{Bmatrix}
p&q\\
u&v\\
\end{Bmatrix}
\right)
\right)\\
=c\left(
f\left(l(sy-xt)-n(qy-vt)+o(qx-vs)\right)\\
-g\left(k(sy-xt)-n(py-ut)+o(px-us)\right)\\
+i\left(k(qy-vt)-l(py-ut)+o(pv-uq)\right)\\
-j\left(k(qx-vs)-l(px-us)+n(pv-uq)\right)
\right)
 
D=-d\times
\det
\begin{Bmatrix}
f&g&h&j\\
k&l&m&o\\
p&q&r&t\\
u&v&w&y\\
\end{Bmatrix}\\
=-d\left(
f\times
\det
\begin{Bmatrix}
l&m&o\\
q&r&t\\
v&w&y\\
\end{Bmatrix}
-g\times
\det
\begin{Bmatrix}
k&m&o\\
p&r&t\\
u&w&y\\
\end{Bmatrix}
+h\times
\det
\begin{Bmatrix}
k&l&o\\
p&q&t\\
u&v&y\\
\end{Bmatrix}
-j\times
\det
\begin{Bmatrix}
k&l&m\\
p&q&r\\
u&v&w\\
\end{Bmatrix}
\right)\\
=-d\left(
f\left(
l\times
\det
\begin{Bmatrix}
r&t\\
w&y\\
\end{Bmatrix}
-m\times
\det
\begin{Bmatrix}
q&t\\
v&y\\
\end{Bmatrix}
+o\times
\det
\begin{Bmatrix}
q&r\\
v&w\\
\end{Bmatrix}
\right)\\
-g\left(
k\times
\det
\begin{Bmatrix}
r&t\\
w&y\\
\end{Bmatrix}
-m\times
\det
\begin{Bmatrix}
p&t\\
u&y\\
\end{Bmatrix}
+o\times
\det
\begin{Bmatrix}
p&r\\
u&w\\
\end{Bmatrix}
\right)\\
+h\left(
k\times
\det
\begin{Bmatrix}
q&t\\
v&y\\
\end{Bmatrix}
-l\times
\det
\begin{Bmatrix}
p&t\\
u&y\\
\end{Bmatrix}
+o\times
\det
\begin{Bmatrix}
p&q\\
u&v\\
\end{Bmatrix}
\right)\\
-j\left(
k\times
\det
\begin{Bmatrix}
q&r\\
v&w\\
\end{Bmatrix}
-l\times
\det
\begin{Bmatrix}
p&r\\
u&w\\
\end{Bmatrix}
+m\times
\det
\begin{Bmatrix}
p&q\\
u&v\\
\end{Bmatrix}
\right)
\right)\\
=-d\left(
f\left(l(ry-wt)-m(qy-vt)+o(qw-vr)\right)\\
-g\left(k(ry-wt)-m(py-ut)+o(pw-ur)\right)\\
+h\left(k(qy-vt)-l(py-ut)+o(pv-uq)\right)\\
-j\left(k(qw-vr)-l(pw-ur)+m(pv-uq)\right)
\right)
 
E=e\times
\det
\begin{Bmatrix}
f&g&h&i\\
k&l&m&n\\
p&q&r&s\\
u&v&w&x\\
\end{Bmatrix}
=e\left(
f\times
\det
\begin{Bmatrix}
l&m&n\\
q&r&s\\
v&w&x\\
\end{Bmatrix}
-g\times
\det
\begin{Bmatrix}
k&m&n\\
p&r&s\\
u&w&x\\
\end{Bmatrix}
+h\times
\det
\begin{Bmatrix}
k&l&n\\
p&q&s\\
u&v&x\\
\end{Bmatrix}
-i\times
\det
\begin{Bmatrix}
k&l&m\\
p&q&r\\
u&v&w\\
\end{Bmatrix}
\right)\\
=e\left(
f\left(
l\times
\det
\begin{Bmatrix}
r&s\\
w&x\\
\end{Bmatrix}
-m\times
\det
\begin{Bmatrix}
q&s\\
v&x\\
\end{Bmatrix}
+n\times
\det
\begin{Bmatrix}
q&r\\
v&w\\
\end{Bmatrix}
\right)\\
-g\left(
k\times
\det
\begin{Bmatrix}
r&s\\
w&x\\
\end{Bmatrix}
-m\times
\det
\begin{Bmatrix}
p&s\\
u&x\\
\end{Bmatrix}
+n\times
\det
\begin{Bmatrix}
p&r\\
u&w\\
\end{Bmatrix}
\right)\\
+h\left(
k\times
\det
\begin{Bmatrix}
q&s\\
v&x\\
\end{Bmatrix}
-l\times
\det
\begin{Bmatrix}
p&s\\
u&x\\
\end{Bmatrix}
+n\times
\det
\begin{Bmatrix}
p&q\\
u&v\\
\end{Bmatrix}
\right)\\
-i\left(
k\times
\det
\begin{Bmatrix}
q&r\\
v&w\\
\end{Bmatrix}
-l\times
\det
\begin{Bmatrix}
p&r\\
u&w\\
\end{Bmatrix}
+m\times
\det
\begin{Bmatrix}
p&q\\
u&v\\
\end{Bmatrix}
\right)
\right)\\
=e\left(
f\left(l(rx-ws)-m(qx-vs)+n(qw-vr)\right)\\
-g\left(k(rx-ws)-m(px-us)+n(pw-ur)\right)\\
+h\left(k(qx-vs)-l(px-us)+n(pv-uq)\right)\\
-i\left(k(qw-vr)-l(pw-ur)+m(pv-uq)\right)
\right)
 
\det
\begin{Bmatrix}
a&b&c&d&e\\
f&g&h&i&j\\
k&l&m&n&o\\
p&q&r&s&t\\
u&v&w&x&y\\
\end{Bmatrix}\\
=a\left(
g\left(m(sy-xt)-n(ry-wt)+o(rx-ws)\right)\\
-h\left(l(sy-xt)-n(qy-vt)+o(qx-vs)\right)\\
+i\left(l(ry-wt)-m(qy-vt)+o(qw-vr)\right)\\
-j\left(l(rx-ws)-m(qx-vs)+n(qw-vr)\right)
\right)\\
-b\left(
f\left(m(sy-xt)-n(ry-wt)+o(rx-ws)\right)\\
-h\left(k(sy-xt)-n(py-ut)+o(px-us)\right)\\
+i\left(k(ry-wt)-m(py-ut)+o(pw-ur)\right)\\
-j\left(k(rx-ws)-m(px-us)+n(pw-ur)\right)
\right)\\
+c\left(
f\left(l(sy-xt)-n(qy-vt)+o(qx-vs)\right)\\
-g\left(k(sy-xt)-n(py-ut)+o(px-us)\right)\\
+i\left(k(qy-vt)-l(py-ut)+o(pv-uq)\right)\\
-j\left(k(qx-vs)-l(px-us)+n(pv-uq)\right)
\right)\\
-d\left(
f\left(l(ry-wt)-m(qy-vt)+o(qw-vr)\right)\\
-g\left(k(ry-wt)-m(py-ut)+o(pw-ur)\right)\\
+h\left(k(qy-vt)-l(py-ut)+o(pv-uq)\right)\\
-j\left(k(qw-vr)-l(pw-ur)+m(pv-uq)\right)
\right)\\
+e\left(
f\left(l(rx-ws)-m(qx-vs)+n(qw-vr)\right)\\
-g\left(k(rx-ws)-m(px-us)+n(pw-ur)\right)\\
+h\left(k(qx-vs)-l(px-us)+n(pv-uq)\right)\\
-i\left(k(qw-vr)-l(pw-ur)+m(pv-uq)\right)
\right)
","['linear-algebra', 'matrices', 'determinant', 'numerical-linear-algebra']"
88,Does $\det(A + B) = \det(A) + \det(B)$ hold?,Does  hold?,\det(A + B) = \det(A) + \det(B),Well considering two $n \times n$ matrices does the following hold true: $$\det(A+B) = \det(A) + \det(B)$$ Can there be said anything about $\det(A+B)$? If $A/B$ are symmetric (or maybe even of the form $\lambda I$) - can then things be said?,Well considering two $n \times n$ matrices does the following hold true: $$\det(A+B) = \det(A) + \det(B)$$ Can there be said anything about $\det(A+B)$? If $A/B$ are symmetric (or maybe even of the form $\lambda I$) - can then things be said?,,"['linear-algebra', 'determinant']"
89,"What makes a linear system of equations ""unsolvable""?","What makes a linear system of equations ""unsolvable""?",,"I've been studying simple systems of equations, so I came up with this example off the top of my head: \begin{cases} x + y + z = 1 \\[4px] x + y + 2z = 3 \\[4px] x + y + 3z = -1 \end{cases} Combining the first two equations yields  \begin{gather} z = 2 \\ x + y = -1. \end{gather} But substituting $z = 2$ in the third equation implies $$x + y = -7,$$ while substituting $x + y = -1$ in the third equation implies $$z = 0.$$ I notice that $x + y$ appears in all three equations, so if we define $w = x + y$ then this essentially becomes three equations in two variables, which explains why I could solve for a variable using only the first two equations, and why the third equation doesn't agree. So here is my question: What is the distinguishing feature of systems of equations that determines whether or not they have a solution? Perhaps put another way: in general, is there a ""check"" one can do on a system (aside from actually trying to solve it) to determine if there will be a solution? Edit: Thanks for all the input everyone. I'm satisfied knowing that in general, there is no way to simply inspect such a linear system to determine if it has a solution - rather, some work is required. The geometric interpretations of linear systems of equations given in several answers were very helpful. Specifically: the interpretation of trying to find a vector $x$ in $Ax = b$ so that that the matrix $A$ represents a linear transformation mapping the vector $x$ to the vector $b$ (which may not be possible if $A$ maps $n$-dimensions to $n-1$ dimensions).","I've been studying simple systems of equations, so I came up with this example off the top of my head: \begin{cases} x + y + z = 1 \\[4px] x + y + 2z = 3 \\[4px] x + y + 3z = -1 \end{cases} Combining the first two equations yields  \begin{gather} z = 2 \\ x + y = -1. \end{gather} But substituting $z = 2$ in the third equation implies $$x + y = -7,$$ while substituting $x + y = -1$ in the third equation implies $$z = 0.$$ I notice that $x + y$ appears in all three equations, so if we define $w = x + y$ then this essentially becomes three equations in two variables, which explains why I could solve for a variable using only the first two equations, and why the third equation doesn't agree. So here is my question: What is the distinguishing feature of systems of equations that determines whether or not they have a solution? Perhaps put another way: in general, is there a ""check"" one can do on a system (aside from actually trying to solve it) to determine if there will be a solution? Edit: Thanks for all the input everyone. I'm satisfied knowing that in general, there is no way to simply inspect such a linear system to determine if it has a solution - rather, some work is required. The geometric interpretations of linear systems of equations given in several answers were very helpful. Specifically: the interpretation of trying to find a vector $x$ in $Ax = b$ so that that the matrix $A$ represents a linear transformation mapping the vector $x$ to the vector $b$ (which may not be possible if $A$ maps $n$-dimensions to $n-1$ dimensions).",,"['linear-algebra', 'systems-of-equations']"
90,$I_m - AB$ is invertible if and only if $I_n - BA$ is invertible.,is invertible if and only if  is invertible.,I_m - AB I_n - BA,"The problem is from  Algebra by Artin, Chapter 1, Miscellaneous Problems, Exercise 8. I have been trying for a long time now to solve it but I am unsuccessful. Let $A,B$ be $m \times n$ and $n \times m$ matrices. Prove that $I_m - AB$ is invertible if and only if $I_n - BA$ is invertible. Please provide with only a hint.","The problem is from  Algebra by Artin, Chapter 1, Miscellaneous Problems, Exercise 8. I have been trying for a long time now to solve it but I am unsuccessful. Let $A,B$ be $m \times n$ and $n \times m$ matrices. Prove that $I_m - AB$ is invertible if and only if $I_n - BA$ is invertible. Please provide with only a hint.",,['linear-algebra']
91,What is the norm of a complex number?,What is the norm of a complex number?,,"Many articles on the web state that the norm of $z = a +bi$ is \begin{align} \sqrt{z*z^*}  &= \sqrt{(a + bi)(a - bi)}\\ &= \sqrt{a^2 - abi + abi - b^2i^2}\\ &= \sqrt{a^2 - b^2\sqrt{-1}^2}\\ &= \sqrt{a^2 + b^2},\\ \end{align} $$$$ where $z^* = a - bi$ is the conjugate of $z$. However, my textbook states that the norm of a complex number $z = a +bi$ is $a^2 + b^2$, i.e. without the square root. What is the norm of a complex number? Which of the two versions is correct?","Many articles on the web state that the norm of $z = a +bi$ is \begin{align} \sqrt{z*z^*}  &= \sqrt{(a + bi)(a - bi)}\\ &= \sqrt{a^2 - abi + abi - b^2i^2}\\ &= \sqrt{a^2 - b^2\sqrt{-1}^2}\\ &= \sqrt{a^2 + b^2},\\ \end{align} $$$$ where $z^* = a - bi$ is the conjugate of $z$. However, my textbook states that the norm of a complex number $z = a +bi$ is $a^2 + b^2$, i.e. without the square root. What is the norm of a complex number? Which of the two versions is correct?",,"['linear-algebra', 'elementary-number-theory', 'complex-numbers']"
92,Why can't linear maps map to higher dimensions?,Why can't linear maps map to higher dimensions?,,"I've been trying to wrap my head around this for a while now. Apparently, a map is a linear map if it preserves scalar multiplication and addition. So let's say I have the mapping: $$f(x) = (x,x)$$ This is not a mapping to a lower or equal dimension, but to a higher one. Yet it seems to preserve scalar multiplication and addition: $$f(ax) = (ax,ax) = a(x,x) = af(x)$$ $$f(x+y) = (x+y,x+y) = (x,x) + (y,y) = f(x) + f(y)$$ I must have made an error in my logic somewhere, but I can't seem to find it. Or are linear maps simply defined this way? I would really appreciate to know this.","I've been trying to wrap my head around this for a while now. Apparently, a map is a linear map if it preserves scalar multiplication and addition. So let's say I have the mapping: $$f(x) = (x,x)$$ This is not a mapping to a lower or equal dimension, but to a higher one. Yet it seems to preserve scalar multiplication and addition: $$f(ax) = (ax,ax) = a(x,x) = af(x)$$ $$f(x+y) = (x+y,x+y) = (x,x) + (y,y) = f(x) + f(y)$$ I must have made an error in my logic somewhere, but I can't seem to find it. Or are linear maps simply defined this way? I would really appreciate to know this.",,"['linear-algebra', 'linear-transformations']"
93,"Is a Fourier transform a change of basis, or is it a linear transformation?","Is a Fourier transform a change of basis, or is it a linear transformation?",,"I've frequently heard that a Fourier transform is ""just a change of basis"". However, I'm not sure whether that's correct, in terms of the terminology of ""change of basis"" versus ""transformation"" in linear algebra. Is a Fourier transform of a function merely a change of basis (i.e. the identity transformation from the original vector space onto itself, with merely a change of basis vectors), or is indeed a linear transformation from the original vector space onto another vector space (the ""Fourier"" vector space, so to speak)? Also: Does the same  answer hold for other similar transforms (e.g. Laplace transforms)? Or is there a different terminology for those?","I've frequently heard that a Fourier transform is ""just a change of basis"". However, I'm not sure whether that's correct, in terms of the terminology of ""change of basis"" versus ""transformation"" in linear algebra. Is a Fourier transform of a function merely a change of basis (i.e. the identity transformation from the original vector space onto itself, with merely a change of basis vectors), or is indeed a linear transformation from the original vector space onto another vector space (the ""Fourier"" vector space, so to speak)? Also: Does the same  answer hold for other similar transforms (e.g. Laplace transforms)? Or is there a different terminology for those?",,"['linear-algebra', 'fourier-analysis', 'transformation']"
94,How do you formally prove that rotation is a linear transformation?,How do you formally prove that rotation is a linear transformation?,,"The fact that rotation about an angle is a linear transformation is both important (for example, this is used to prove the sine/cosine angle addition formulas; see How can I understand and prove the ""sum and difference formulas"" in trigonometry? ) and somewhat intuitive geometrically.  However, even if this fact seems fairly obvious (at least from a diagram), how does one turn the picture proof into a formal proof?  On a related note, it seems likely that many formal proofs using a diagram will end up relying on Euclidean geometry (using angle/side congruence properties), but isn't one of the points of linear algebra to avoid using Euclidean geometry explicitly?","The fact that rotation about an angle is a linear transformation is both important (for example, this is used to prove the sine/cosine angle addition formulas; see How can I understand and prove the ""sum and difference formulas"" in trigonometry? ) and somewhat intuitive geometrically.  However, even if this fact seems fairly obvious (at least from a diagram), how does one turn the picture proof into a formal proof?  On a related note, it seems likely that many formal proofs using a diagram will end up relying on Euclidean geometry (using angle/side congruence properties), but isn't one of the points of linear algebra to avoid using Euclidean geometry explicitly?",,['linear-algebra']
95,When the product of dice rolls yields a square,When the product of dice rolls yields a square,,"Succinct Question: Suppose you roll a fair six-sided die $n$ times. What is the probability that the product of the rolls is a square? Context: I used this as one question in a course for elementary school teachers when $n=2$ , and thought the generalization might be a good follow-up question for secondary school math teachers. But I encountered quite a bit of difficulty in tackling it, and I am wondering if there is a neater solution than what I have already seen, and to what deeper phemonena it connects. Known: Since the six sides of a die are $1, 2, 3, 2^2, 5,$ and $2\cdot3$ , the product of the rolls is always of the form $2^{A}3^{B}5^{C}$ , and the question is now transformed into the probability that $A, B, C$ are all even. The actual ""probability"" component is mostly for ease of phrasing; its only contribution is a $6^n$ in the denominator, and my true question is of a more combinatorial nature: namely, In how many ways can the product of $n$ rolls yield a square? One approach that I have seen involves first creating an $8 \times 8$ matrix corresponding to the eight cases around the parity of $A, B, C$ ; one can then take the dot product of each roll with this matrix, and hope to spot a pattern. In this way, one may discover the formula: $$\frac{6^n + 4^n + 3\cdot2^n}{8}$$ and the ""probability"" version is simply this formula with another $6^n$ multiplied in the denominator. As for proving this: Some guesswork around linear combinations of the numerator yields a formula for each of the eight cases concerning $A,B,C$ parity, and one can then prove all eight of them by induction. And so I ""know"" the answer in the sense that I have all eight of the formulae (and the particular one listed above is correct) but they were not found in a particularly organized fashion. My Actual Question: What is a systematic way to deduce the formula, given above, for the number of ways the product of $n$ rolls yields a square, and to what deeper phenomena does this connect?","Succinct Question: Suppose you roll a fair six-sided die times. What is the probability that the product of the rolls is a square? Context: I used this as one question in a course for elementary school teachers when , and thought the generalization might be a good follow-up question for secondary school math teachers. But I encountered quite a bit of difficulty in tackling it, and I am wondering if there is a neater solution than what I have already seen, and to what deeper phemonena it connects. Known: Since the six sides of a die are and , the product of the rolls is always of the form , and the question is now transformed into the probability that are all even. The actual ""probability"" component is mostly for ease of phrasing; its only contribution is a in the denominator, and my true question is of a more combinatorial nature: namely, In how many ways can the product of rolls yield a square? One approach that I have seen involves first creating an matrix corresponding to the eight cases around the parity of ; one can then take the dot product of each roll with this matrix, and hope to spot a pattern. In this way, one may discover the formula: and the ""probability"" version is simply this formula with another multiplied in the denominator. As for proving this: Some guesswork around linear combinations of the numerator yields a formula for each of the eight cases concerning parity, and one can then prove all eight of them by induction. And so I ""know"" the answer in the sense that I have all eight of the formulae (and the particular one listed above is correct) but they were not found in a particularly organized fashion. My Actual Question: What is a systematic way to deduce the formula, given above, for the number of ways the product of rolls yields a square, and to what deeper phenomena does this connect?","n n=2 1, 2, 3, 2^2, 5, 2\cdot3 2^{A}3^{B}5^{C} A, B, C 6^n n 8 \times 8 A, B, C \frac{6^n + 4^n + 3\cdot2^n}{8} 6^n A,B,C n","['linear-algebra', 'combinatorics', 'number-theory', 'markov-chains', 'generating-functions']"
96,Polynomials as vector spaces?,Polynomials as vector spaces?,,"Can someone please explain how polynomials are vector spaces? I don't understand this at all. Vectors are straight, so how could a polynomial be a vector. Is it just that the coefficients are vector spaces? I'm really not understanding how polynomials tie in to linear algebra at all? I can do all the problems with them, because they are done the same way as any other vector problem, but I can't understand intuitively what I'm doing at all.","Can someone please explain how polynomials are vector spaces? I don't understand this at all. Vectors are straight, so how could a polynomial be a vector. Is it just that the coefficients are vector spaces? I'm really not understanding how polynomials tie in to linear algebra at all? I can do all the problems with them, because they are done the same way as any other vector problem, but I can't understand intuitively what I'm doing at all.",,"['linear-algebra', 'abstract-algebra', 'polynomials']"
97,$\operatorname{tr}(AABABB) = \operatorname{tr}(AABBAB)$ for $2×2$ matrices,for  matrices,\operatorname{tr}(AABABB) = \operatorname{tr}(AABBAB) 2×2,"Similar to a previous question here , I wonder if cyclic permutations are the only relations amongst traces of (non-commutative) monomials.  Since the evaluations $\operatorname{tr}:k\langle x,y,\dots \rangle \to k$ take an infinite dimensional vector space to a one-dimensional vector space there must be quite a few relations, but I wonder if any of them are on binomials other than the cyclic permutations. At any rate, for small dimensions, we probably get some extra relations. It appears that $\operatorname{tr}(AABABB−AABBAB) = 0$ for all $2×2$ matrices.  Is this true?  How does one prove it?","Similar to a previous question here , I wonder if cyclic permutations are the only relations amongst traces of (non-commutative) monomials.  Since the evaluations $\operatorname{tr}:k\langle x,y,\dots \rangle \to k$ take an infinite dimensional vector space to a one-dimensional vector space there must be quite a few relations, but I wonder if any of them are on binomials other than the cyclic permutations. At any rate, for small dimensions, we probably get some extra relations. It appears that $\operatorname{tr}(AABABB−AABBAB) = 0$ for all $2×2$ matrices.  Is this true?  How does one prove it?",,"['linear-algebra', 'matrices', 'trace']"
98,Show that the direct sum of a kernel of a projection and its image create the originating vector space.,Show that the direct sum of a kernel of a projection and its image create the originating vector space.,,"I got the following question as my homework. Given $V$ is a vector space with $P \in \operatorname{End} V$. $P \circ P = P$ ( ""P is idempotent"" ). Show that $V = \operatorname{Ker} P \oplus \operatorname{Im} P$. One $P$ I can imagine is a projection from 3d-space to plane, that just sets some coordinates to zero. For example $\begin{pmatrix} x \\ y \\ z\end{pmatrix} \mapsto \begin{pmatrix}x \\ y \\ 0\end{pmatrix}$. Then $\operatorname{Ker} P$ would give the line $\begin{pmatrix} 0 \\ 0 \\ z\end{pmatrix}$ and $\operatorname{Im} P$ would contain all $\begin{pmatrix}x \\ y \\ 0\end{pmatrix}$. So the result of $\operatorname{Ker} P \oplus \operatorname{Im} P$ is of course $V$. But how do I prove that in a mathematical way?","I got the following question as my homework. Given $V$ is a vector space with $P \in \operatorname{End} V$. $P \circ P = P$ ( ""P is idempotent"" ). Show that $V = \operatorname{Ker} P \oplus \operatorname{Im} P$. One $P$ I can imagine is a projection from 3d-space to plane, that just sets some coordinates to zero. For example $\begin{pmatrix} x \\ y \\ z\end{pmatrix} \mapsto \begin{pmatrix}x \\ y \\ 0\end{pmatrix}$. Then $\operatorname{Ker} P$ would give the line $\begin{pmatrix} 0 \\ 0 \\ z\end{pmatrix}$ and $\operatorname{Im} P$ would contain all $\begin{pmatrix}x \\ y \\ 0\end{pmatrix}$. So the result of $\operatorname{Ker} P \oplus \operatorname{Im} P$ is of course $V$. But how do I prove that in a mathematical way?",,['linear-algebra']
99,Every $R$-module is free $\implies$ $R$ is a division ring,Every -module is free   is a division ring,R \implies R,"From Grillet's Abstract Algebra , section VIII.5. Definitions. A division ring is a ring with identity in which every nonzero element is a unit. A vector space is a unital module over a division ring. Theorem 5.2. Every vector space has a basis. Exercises. (7.) Show that $R$ is a division ring if and only if it has no left ideal $L \neq 0, R$. (*9.) Prove that $R$ is a division ring if and only if every left $R$-module is free. I'm trying to solve exercise 9. I suspect it should be formulated: Let  $R$ be a unital ring. Then $R$ is a division ring $\iff$ every unital left $R$-module is free. Attempt of proof: $(\implies)$: Theorem 5.2. $(\impliedby)$: By exercise 7, it suffices to show that every non-zero left ideal of $R$ is equal to $R$. Let $I\!\neq\!0$ be a left ideal. As a $R$-module, $I$ has a basis $B$. How can I show that $I=R$?","From Grillet's Abstract Algebra , section VIII.5. Definitions. A division ring is a ring with identity in which every nonzero element is a unit. A vector space is a unital module over a division ring. Theorem 5.2. Every vector space has a basis. Exercises. (7.) Show that $R$ is a division ring if and only if it has no left ideal $L \neq 0, R$. (*9.) Prove that $R$ is a division ring if and only if every left $R$-module is free. I'm trying to solve exercise 9. I suspect it should be formulated: Let  $R$ be a unital ring. Then $R$ is a division ring $\iff$ every unital left $R$-module is free. Attempt of proof: $(\implies)$: Theorem 5.2. $(\impliedby)$: By exercise 7, it suffices to show that every non-zero left ideal of $R$ is equal to $R$. Let $I\!\neq\!0$ be a left ideal. As a $R$-module, $I$ has a basis $B$. How can I show that $I=R$?",,"['linear-algebra', 'abstract-algebra', 'modules']"
