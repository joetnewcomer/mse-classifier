,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Number of solutions of a linear equation $AX=B$,Number of solutions of a linear equation,AX=B,"I have a question and a proposed solution - Please tell me if I'm correct. Problem: Prove that if $A$ and $B$ are real matrices and the system of equations $AX=B$ has more than one solution, then it has infinitely many. Solution: Assume that the system of equations $AX=B$ has more than one solution. This means that the reduced row echelon matrix form of the solution to the equation $AX=B$ has at least one free variable, because if all of the variables were pivot variables, then we would be left with a set of unique values for the variables and hence one solution. Therefore, with at least one free variable, we have that the variable(s) can vary over the real numbers. Hence, there are infinitely many solutions. Thank you!","I have a question and a proposed solution - Please tell me if I'm correct. Problem: Prove that if $A$ and $B$ are real matrices and the system of equations $AX=B$ has more than one solution, then it has infinitely many. Solution: Assume that the system of equations $AX=B$ has more than one solution. This means that the reduced row echelon matrix form of the solution to the equation $AX=B$ has at least one free variable, because if all of the variables were pivot variables, then we would be left with a set of unique values for the variables and hence one solution. Therefore, with at least one free variable, we have that the variable(s) can vary over the real numbers. Hence, there are infinitely many solutions. Thank you!",,[]
1,"$V=\{A\in M_{3\times 3}(\mathbb{R}):\text{trace}(A)=0\}$ is isomorphic to $\text{span}\{AB-BA:A,B\in V\}$",is isomorphic to,"V=\{A\in M_{3\times 3}(\mathbb{R}):\text{trace}(A)=0\} \text{span}\{AB-BA:A,B\in V\}","Background: Let $$V:=\{A\in M_{3\times 3}(\mathbb{R}):\text{trace}(A)=0\}$$ be the vector space of $3\times 3$ real matrices with vanishing trace, and let $[\cdot,\cdot]:V\times V\to V$ be defined by $$[A,B]=AB-BA$$ Note that this is well defined since $\text{trace}(AB-BA)=\text{trace}(AB)-\text{trace}(AB)=0$ for any matrices $A,B$. Finally, let $$\hat{V}:=\text{span}\{[A,B]:A,B\in V\}$$ Question: I have to give an isomorphism $$\varphi:(V,[\cdot,\cdot])\to(\hat{V},[\cdot,\cdot]),$$ i.e. a bijective map such that $\varphi([A,B])=[\varphi(A),\varphi(B)]$, $\forall A,B\in V$. I am aware that this is related to Lie algebra; namely $V=\mathfrak{sl}(3,\mathbb{R})$. However, I did not study Lie algebra yet. I am suppose to be able to prove it without referring to any well-known theorem of Lie algebra (unless I prove it first). I used a very computational argument. I am wondering if My argument is correct. There is a more fundamental way to prove it, i.e. without having to tediously compute all possible $[X_i,X_j]$ from a given basis. Atempt: Let $$ X_1 = \left( \begin{array}{ccc}  1 & 0 & 0 \\  0 & -1 & 0 \\  0 & 0 & 0 \end{array} \right),\quad X_2 = \left( \begin{array}{ccc}  1 & 0 & 0 \\  0 & 0 & 0 \\  0 & 0 & -1 \end{array} \right),\quad X_3 = \left( \begin{array}{ccc}  0 & 1 & 0 \\  0 & 0 & 0 \\  0 & 0 & 0 \end{array} \right),$$ $$\quad X_4 = \left( \begin{array}{ccc}  0 & 0 & 1 \\  0 & 0 & 0 \\  0 & 0 & 0 \end{array} \right),\quad X_5 = \left( \begin{array}{ccc}  0 & 0 & 0 \\  0 & 0 & 1 \\  0 & 0 & 0 \end{array} \right),\quad X_6 = \left( \begin{array}{ccc}  0 & 0 & 0 \\  1 & 0 & 0 \\  0 & 0 & 0 \end{array} \right),$$ $$\quad X_7 = \left( \begin{array}{ccc}  0 & 0 & 0 \\  0 & 0 & 0 \\  1 & 0 & 0 \end{array} \right),\quad X_8 = \left( \begin{array}{ccc}  0 & 0 & 0 \\  0 & 0 & 0 \\  0 & 1 & 0 \end{array} \right) $$ Then, $\{X_1,\ldots,X_8\}$ forms a basis for $V$, and we find $$[X_1,X_2]=0,\quad[X_1,X_3]=2X_3,\quad[X_1,X_4]=X_4,\quad[X_1,X_5]=-X_5$$ $$[X_1,X_6]=-2X_6,\quad[X_1,X_7]=-X_7,\quad[X_1,X_8]=X_8,\quad[X_2,X_3]=X_3$$ $$[X_2,X_4]=2X_4,\quad[X_2,X_5]=X_5,\quad[X_2,X_6]=-X_6,\quad[X_2,X_7]=-2X_7$$ $$[X_2,X_8]=-X_8,\quad[X_3,X_4]=0,\quad[X_3,X_5]=X_4,\quad[X_3,X_6]=X_1$$ $$[X_3,X_7]=-X_8,\quad[X_3,X_8]=0,\quad[X_4,X_5]=0,\quad[X_4,X_6]=-X_5$$ $$[X_4,X_7]=X_2,\quad[X_4,X_8]=X_3,\quad[X_5,X_6]=0,\quad[X_5,X_7]=X_6$$ $$[X_5,X_8]=X_2-X_1,\quad[X_6,X_7]=0,\quad[X_6,X_8]=-X_7,\quad[X_7,X_8]=0$$ Thus, we have $$ \begin{array}{cccc} X_1=[X_3,X_6],& X_2=[X_4,X_7],& X_3=[X_2,X_3],& X_4=[X_1,X_4] \\ X_5=[X_2,X_5],& X_6=[X_5,X_7],& X_7=[X_7,X_1],& X_8=[X_7,X_3] \end{array} $$ which shows that $V$ is a subspace of $\hat{V}$, and since $\hat{V}$ is clearly a subspace of $V$, then $V$ and $\hat{V}$ are in fact the same vector space. Thus, we can take $\varphi$ to be simply the inclusion map. Indeed, $\varphi$ is then clearly bijective and we have $$\varphi([A,B])=[A,B]=[\varphi(A),\varphi(B)]$$","Background: Let $$V:=\{A\in M_{3\times 3}(\mathbb{R}):\text{trace}(A)=0\}$$ be the vector space of $3\times 3$ real matrices with vanishing trace, and let $[\cdot,\cdot]:V\times V\to V$ be defined by $$[A,B]=AB-BA$$ Note that this is well defined since $\text{trace}(AB-BA)=\text{trace}(AB)-\text{trace}(AB)=0$ for any matrices $A,B$. Finally, let $$\hat{V}:=\text{span}\{[A,B]:A,B\in V\}$$ Question: I have to give an isomorphism $$\varphi:(V,[\cdot,\cdot])\to(\hat{V},[\cdot,\cdot]),$$ i.e. a bijective map such that $\varphi([A,B])=[\varphi(A),\varphi(B)]$, $\forall A,B\in V$. I am aware that this is related to Lie algebra; namely $V=\mathfrak{sl}(3,\mathbb{R})$. However, I did not study Lie algebra yet. I am suppose to be able to prove it without referring to any well-known theorem of Lie algebra (unless I prove it first). I used a very computational argument. I am wondering if My argument is correct. There is a more fundamental way to prove it, i.e. without having to tediously compute all possible $[X_i,X_j]$ from a given basis. Atempt: Let $$ X_1 = \left( \begin{array}{ccc}  1 & 0 & 0 \\  0 & -1 & 0 \\  0 & 0 & 0 \end{array} \right),\quad X_2 = \left( \begin{array}{ccc}  1 & 0 & 0 \\  0 & 0 & 0 \\  0 & 0 & -1 \end{array} \right),\quad X_3 = \left( \begin{array}{ccc}  0 & 1 & 0 \\  0 & 0 & 0 \\  0 & 0 & 0 \end{array} \right),$$ $$\quad X_4 = \left( \begin{array}{ccc}  0 & 0 & 1 \\  0 & 0 & 0 \\  0 & 0 & 0 \end{array} \right),\quad X_5 = \left( \begin{array}{ccc}  0 & 0 & 0 \\  0 & 0 & 1 \\  0 & 0 & 0 \end{array} \right),\quad X_6 = \left( \begin{array}{ccc}  0 & 0 & 0 \\  1 & 0 & 0 \\  0 & 0 & 0 \end{array} \right),$$ $$\quad X_7 = \left( \begin{array}{ccc}  0 & 0 & 0 \\  0 & 0 & 0 \\  1 & 0 & 0 \end{array} \right),\quad X_8 = \left( \begin{array}{ccc}  0 & 0 & 0 \\  0 & 0 & 0 \\  0 & 1 & 0 \end{array} \right) $$ Then, $\{X_1,\ldots,X_8\}$ forms a basis for $V$, and we find $$[X_1,X_2]=0,\quad[X_1,X_3]=2X_3,\quad[X_1,X_4]=X_4,\quad[X_1,X_5]=-X_5$$ $$[X_1,X_6]=-2X_6,\quad[X_1,X_7]=-X_7,\quad[X_1,X_8]=X_8,\quad[X_2,X_3]=X_3$$ $$[X_2,X_4]=2X_4,\quad[X_2,X_5]=X_5,\quad[X_2,X_6]=-X_6,\quad[X_2,X_7]=-2X_7$$ $$[X_2,X_8]=-X_8,\quad[X_3,X_4]=0,\quad[X_3,X_5]=X_4,\quad[X_3,X_6]=X_1$$ $$[X_3,X_7]=-X_8,\quad[X_3,X_8]=0,\quad[X_4,X_5]=0,\quad[X_4,X_6]=-X_5$$ $$[X_4,X_7]=X_2,\quad[X_4,X_8]=X_3,\quad[X_5,X_6]=0,\quad[X_5,X_7]=X_6$$ $$[X_5,X_8]=X_2-X_1,\quad[X_6,X_7]=0,\quad[X_6,X_8]=-X_7,\quad[X_7,X_8]=0$$ Thus, we have $$ \begin{array}{cccc} X_1=[X_3,X_6],& X_2=[X_4,X_7],& X_3=[X_2,X_3],& X_4=[X_1,X_4] \\ X_5=[X_2,X_5],& X_6=[X_5,X_7],& X_7=[X_7,X_1],& X_8=[X_7,X_3] \end{array} $$ which shows that $V$ is a subspace of $\hat{V}$, and since $\hat{V}$ is clearly a subspace of $V$, then $V$ and $\hat{V}$ are in fact the same vector space. Thus, we can take $\varphi$ to be simply the inclusion map. Indeed, $\varphi$ is then clearly bijective and we have $$\varphi([A,B])=[A,B]=[\varphi(A),\varphi(B)]$$",,"['linear-algebra', 'abstract-algebra', 'lie-algebras']"
2,$V=W_1\oplus\cdots\oplus W_k$ if and only if $\dim(V)=\sum{\dim(W_i)}$,if and only if,V=W_1\oplus\cdots\oplus W_k \dim(V)=\sum{\dim(W_i)},"If $W_1,\dots, W_k$ are subspaces of a finite dimensional vector space $V$ such that $W_1+\cdots+W_k=V$, and I want to show that $V=W_1\oplus\cdots\oplus W_k$ if and only if $\dim(V)=\sum{W_i}$, then will what's displayed below suffice? $$V=W_1\oplus\cdots\oplus W_k$$ $$\iff$$ $$V=W_1+\cdots+W_k~\text{and}~W_i \cap (W_1 + \ldots + W_{i-1} + W_{i+1} + \ldots + W_k) = \{0\}$$ $$\iff$$ $$\text{The subspaces $W_i$ are independent; that is, no sum $w_1+\cdots+w_k$ with $w_i$ in $W_i$ is zero except the trivial sum.}$$ $$\iff$$ $${\scr{B}}=\{\beta_1,\dots,\beta_k\}~\text{is a basis for $V$, where $\beta_i$ is a basis for $W_i$}$$ $$\iff$$ $$\dim{V}=\dim{(W_1+\cdots+W_k)}=\dim{W_1}+\cdots+\dim{W_k}=~\mid\beta_1\mid+\cdots+\mid\beta_k\mid=k$$ $$\iff$$ $$\overset{\text{Does this belong here?}}{\dim{\scr{B}}=k}$$","If $W_1,\dots, W_k$ are subspaces of a finite dimensional vector space $V$ such that $W_1+\cdots+W_k=V$, and I want to show that $V=W_1\oplus\cdots\oplus W_k$ if and only if $\dim(V)=\sum{W_i}$, then will what's displayed below suffice? $$V=W_1\oplus\cdots\oplus W_k$$ $$\iff$$ $$V=W_1+\cdots+W_k~\text{and}~W_i \cap (W_1 + \ldots + W_{i-1} + W_{i+1} + \ldots + W_k) = \{0\}$$ $$\iff$$ $$\text{The subspaces $W_i$ are independent; that is, no sum $w_1+\cdots+w_k$ with $w_i$ in $W_i$ is zero except the trivial sum.}$$ $$\iff$$ $${\scr{B}}=\{\beta_1,\dots,\beta_k\}~\text{is a basis for $V$, where $\beta_i$ is a basis for $W_i$}$$ $$\iff$$ $$\dim{V}=\dim{(W_1+\cdots+W_k)}=\dim{W_1}+\cdots+\dim{W_k}=~\mid\beta_1\mid+\cdots+\mid\beta_k\mid=k$$ $$\iff$$ $$\overset{\text{Does this belong here?}}{\dim{\scr{B}}=k}$$",,"['linear-algebra', 'abstract-algebra', 'vector-spaces']"
3,Number of elements of a matrix subset with field $\mathbb{Z}_p$,Number of elements of a matrix subset with field,\mathbb{Z}_p,"Can anybody please help me this problem? Let $K = \mathbb{F}_p$ be the field of integers module an odd prime $p$, and $G = \mathcal{M}^*_n(\mathbb{F}_p)$ the set of $n\times n$ invertible matrices with components in $\mathbb{F}_p$. Based on the linear (in)dependence of the columns of a matrix $M\in G$, get the number of matrices in $G$. Thanx in advance.","Can anybody please help me this problem? Let $K = \mathbb{F}_p$ be the field of integers module an odd prime $p$, and $G = \mathcal{M}^*_n(\mathbb{F}_p)$ the set of $n\times n$ invertible matrices with components in $\mathbb{F}_p$. Based on the linear (in)dependence of the columns of a matrix $M\in G$, get the number of matrices in $G$. Thanx in advance.",,['linear-algebra']
4,Non-binary Hamming codes?,Non-binary Hamming codes?,,"Is there such a thing as a nonlinear Hamming code? Please give me a formal definition of Hamming code. Many that I've seen say that a Hamming code is binary, yet I've been given examples of non-binary Hamming codes.","Is there such a thing as a nonlinear Hamming code? Please give me a formal definition of Hamming code. Many that I've seen say that a Hamming code is binary, yet I've been given examples of non-binary Hamming codes.",,"['linear-algebra', 'combinatorics', 'discrete-mathematics', 'finite-fields', 'coding-theory']"
5,Tensor Product universal property,Tensor Product universal property,,"Let $V,W$ be vector spaces and $T$ the following mapping: $$  \begin{align*} T:V\times W&\to V\otimes W\\ (v,w)&\mapsto v\otimes w \end{align*} $$ Then $(V\otimes W,T)$ Satisfies the universal property of the tensor product, but if $T$ were surjective, then every element of $V\otimes W$ would be of the form $v\otimes w$ which I am aware is not the case.  However, $\tilde T:V\times W\to Im(T)$ with $\tilde T(v,w):=T(v,w)$ is bilinear and therefore, there exists a linear mapping $\phi:V\otimes W\to Im(T)$ so that $\phi\circ T = \tilde T$. Let $j:Im(T)\to V\otimes W$ be the inclusion map. We then have $id\circ T=T=j\circ\tilde T=j\circ\phi\circ T$ which implies $id=j\circ\phi$ by uniqueness of the mapping $\phi$. It follows that $j$ is surjective, and hence $Im(T)=V\otimes W$. This means that $T$ has to be surjective... Where is the mistake that is leading to my utter confusion? Thanks!","Let $V,W$ be vector spaces and $T$ the following mapping: $$  \begin{align*} T:V\times W&\to V\otimes W\\ (v,w)&\mapsto v\otimes w \end{align*} $$ Then $(V\otimes W,T)$ Satisfies the universal property of the tensor product, but if $T$ were surjective, then every element of $V\otimes W$ would be of the form $v\otimes w$ which I am aware is not the case.  However, $\tilde T:V\times W\to Im(T)$ with $\tilde T(v,w):=T(v,w)$ is bilinear and therefore, there exists a linear mapping $\phi:V\otimes W\to Im(T)$ so that $\phi\circ T = \tilde T$. Let $j:Im(T)\to V\otimes W$ be the inclusion map. We then have $id\circ T=T=j\circ\tilde T=j\circ\phi\circ T$ which implies $id=j\circ\phi$ by uniqueness of the mapping $\phi$. It follows that $j$ is surjective, and hence $Im(T)=V\otimes W$. This means that $T$ has to be surjective... Where is the mistake that is leading to my utter confusion? Thanks!",,"['linear-algebra', 'tensor-products', 'multilinear-algebra']"
6,Determining if a Matrix is Diagonalizable without computing Eigenvalues,Determining if a Matrix is Diagonalizable without computing Eigenvalues,,"Is there any simple way to determine if a matrix is diagonalizable without having to compute eigenvalues? I'm motivated by the idea that for $\mathbb{R}^n$, to determine if a matrix is diagonalizable via an orthogonal transformation, you just need to check if it's symmetric. Also, for $\mathbb{C}^n$, to determine if a matrix is diagonalizable via a unitary transformation, you just need to check if it's normal. So I'm just curious if one can drop the orthogonal/unitary requirements while still having an easy method to check if a matrix is diagonalizable.","Is there any simple way to determine if a matrix is diagonalizable without having to compute eigenvalues? I'm motivated by the idea that for $\mathbb{R}^n$, to determine if a matrix is diagonalizable via an orthogonal transformation, you just need to check if it's symmetric. Also, for $\mathbb{C}^n$, to determine if a matrix is diagonalizable via a unitary transformation, you just need to check if it's normal. So I'm just curious if one can drop the orthogonal/unitary requirements while still having an easy method to check if a matrix is diagonalizable.",,['linear-algebra']
7,Rational Canonical Form over finite field,Rational Canonical Form over finite field,,"Let $\phi: F_{p^n} \to F_{p^n}$ be a linear operator (where $F_{p^n}$ is considered as vector space over $F_p$) given by $x \mapsto x^p.$  In a previous exercise we prove that $\phi^n = I$ and that no lower power of $\phi$ is the identity. Then we are asked to give the rational canonical form of $\phi$ over $F_p.$  I am struggling with figuring out what the minimum polynomial is, I suspect that it is $x^n-1$ but I haven't been able to prove it.  Here is what I've tried so far: We know the minimum polynomial $m(x)$ divides $x^n-1.$ Let $n= p^kr$ where $r$ is relatively prime to $p.$  Then $$x^n-1 = x^{p^kr} - 1 = (x^r-1)^{p^k}$$ so $m(x) | (x^r -1)^{p^k}.$  Now I know that $$m(x) = (x^r-1)^l$$ where $l>p^{k-1}$ since if $l \leq p^{k-1}$ then we could multiply $m(x)$ be the appropriate factor to obtain $$m(x)p(x) = (x^r - 1)^{p^{k-1}} = x^{p^{k-1}} - 1.$$ and since $\phi$ satisfies $m(x)$ it satisfies the right side which is a contradiction to $n$ being the smallest power of $\phi$ which is the identity.  This is where I run out of ideas and cant seem to show $m(x) = x^n-1.$  Perhaps thats not the minimum polynomial but then it seems the possibilities for invariant factors becomes wildy complicated. Could someone people provide some guidance? Thanks!","Let $\phi: F_{p^n} \to F_{p^n}$ be a linear operator (where $F_{p^n}$ is considered as vector space over $F_p$) given by $x \mapsto x^p.$  In a previous exercise we prove that $\phi^n = I$ and that no lower power of $\phi$ is the identity. Then we are asked to give the rational canonical form of $\phi$ over $F_p.$  I am struggling with figuring out what the minimum polynomial is, I suspect that it is $x^n-1$ but I haven't been able to prove it.  Here is what I've tried so far: We know the minimum polynomial $m(x)$ divides $x^n-1.$ Let $n= p^kr$ where $r$ is relatively prime to $p.$  Then $$x^n-1 = x^{p^kr} - 1 = (x^r-1)^{p^k}$$ so $m(x) | (x^r -1)^{p^k}.$  Now I know that $$m(x) = (x^r-1)^l$$ where $l>p^{k-1}$ since if $l \leq p^{k-1}$ then we could multiply $m(x)$ be the appropriate factor to obtain $$m(x)p(x) = (x^r - 1)^{p^{k-1}} = x^{p^{k-1}} - 1.$$ and since $\phi$ satisfies $m(x)$ it satisfies the right side which is a contradiction to $n$ being the smallest power of $\phi$ which is the identity.  This is where I run out of ideas and cant seem to show $m(x) = x^n-1.$  Perhaps thats not the minimum polynomial but then it seems the possibilities for invariant factors becomes wildy complicated. Could someone people provide some guidance? Thanks!",,"['linear-algebra', 'abstract-algebra']"
8,eigenvalues of the sum of a matrix with known eigenvalues and a diagonal matrix,eigenvalues of the sum of a matrix with known eigenvalues and a diagonal matrix,,"Suppose $B = A+D$, where all the eigenvalues of $A$ are already known and $D$ is a diagonal matrix, how to compute the eigenvalues of B without diagonalizing $B$ directly?","Suppose $B = A+D$, where all the eigenvalues of $A$ are already known and $D$ is a diagonal matrix, how to compute the eigenvalues of B without diagonalizing $B$ directly?",,"['linear-algebra', 'matrices', 'summation', 'eigenvalues-eigenvectors']"
9,Bounding $x^TAx$ when A is not a symmetric matrix,Bounding  when A is not a symmetric matrix,x^TAx,"If $n \times n$ matrix $A$ is a real and symmetric, then we know that for all $x \in \mathbb{R}^n$ $$\lambda_{\min} (A) \|x\|^2 \le x^T A x \le \lambda_{\max} (A) \|x\|^2$$ where $\lambda_{\max}$ and $\lambda_{\min}$ are the maximum and minimum eigenvalues of $A$ , respectively. However if matrix $A$ is real and non-symmetric, as answered in this question , we identify that $$x^T A x = x^T \left( \frac{A+A^T}{2} \right) x$$ So, for all $x \in \mathbb{R}^n$ , we have $$\Re(\lambda_{\min} (A)) \|x\|^2 \le \hat{\lambda}_{\min} \|x\|^2 \le x^T A x \le \hat{\lambda}_{\max}  \|x\|^2 \le \Re(\lambda_{\max} (A))\|x\|^2$$ where $\hat{\lambda}_{\min}$ and $\hat{\lambda}_{\max}$ are, respectively, the minimum and maximum eigenvalues of $\frac{A+A^T}{2}$ . Is there anything more we could say about this bound? Is it possible to tighten or simplify it somehow?","If matrix is a real and symmetric, then we know that for all where and are the maximum and minimum eigenvalues of , respectively. However if matrix is real and non-symmetric, as answered in this question , we identify that So, for all , we have where and are, respectively, the minimum and maximum eigenvalues of . Is there anything more we could say about this bound? Is it possible to tighten or simplify it somehow?",n \times n A x \in \mathbb{R}^n \lambda_{\min} (A) \|x\|^2 \le x^T A x \le \lambda_{\max} (A) \|x\|^2 \lambda_{\max} \lambda_{\min} A A x^T A x = x^T \left( \frac{A+A^T}{2} \right) x x \in \mathbb{R}^n \Re(\lambda_{\min} (A)) \|x\|^2 \le \hat{\lambda}_{\min} \|x\|^2 \le x^T A x \le \hat{\lambda}_{\max}  \|x\|^2 \le \Re(\lambda_{\max} (A))\|x\|^2 \hat{\lambda}_{\min} \hat{\lambda}_{\max} \frac{A+A^T}{2},"['linear-algebra', 'matrices', 'quadratic-forms', 'upper-lower-bounds']"
10,"let $A,B\in M_{n}(C)$ such that c is complex field and $AB^2-B^2A=B$ how prove $B^n=0$",let  such that c is complex field and  how prove,"A,B\in M_{n}(C) AB^2-B^2A=B B^n=0","Let $A,B\in M_{n}(C)$  such that $C$ is complex field and $AB^2-B^2A=B$. How prove $B^n=0$. thanks in advance","Let $A,B\in M_{n}(C)$  such that $C$ is complex field and $AB^2-B^2A=B$. How prove $B^n=0$. thanks in advance",,"['linear-algebra', 'contest-math']"
11,Completing squares by symplectic transformations,Completing squares by symplectic transformations,,"A quadratic polynomial of $2n$ variables is given as $$ H = \sum_{i,j=1}^{2n} A_{ij} x_i x_j = x^T A x, $$ where $A$ is a symmetric matrix.  I am looking for a symplectic transformation of these variables into $y = Cx$--i.e., $C^TJC=J$ where $J=\begin{pmatrix}0 & I\\ -I & 0\end{pmatrix}$--such that $H$ becomes diagonal in $y$'s: $C^TAC = D$ for some diagonal matrix $D$. It is clear that an orthogonal transformation doing the job always exists, but the question is about symplectic transformations. In addition I think $D$ cannot be the Jordan normal form of $A$, since in that case $C$ can (must?) be orthogonal and $C^TC=I$ is generically in conflict with $C^TJC=J$. The question arises naturally if you want to use canonical transformations of classical mechanics to convert the most general quadratic Hamiltonian of a set of coordinates and momenta into non-interacting harmonic oscillators.","A quadratic polynomial of $2n$ variables is given as $$ H = \sum_{i,j=1}^{2n} A_{ij} x_i x_j = x^T A x, $$ where $A$ is a symmetric matrix.  I am looking for a symplectic transformation of these variables into $y = Cx$--i.e., $C^TJC=J$ where $J=\begin{pmatrix}0 & I\\ -I & 0\end{pmatrix}$--such that $H$ becomes diagonal in $y$'s: $C^TAC = D$ for some diagonal matrix $D$. It is clear that an orthogonal transformation doing the job always exists, but the question is about symplectic transformations. In addition I think $D$ cannot be the Jordan normal form of $A$, since in that case $C$ can (must?) be orthogonal and $C^TC=I$ is generically in conflict with $C^TJC=J$. The question arises naturally if you want to use canonical transformations of classical mechanics to convert the most general quadratic Hamiltonian of a set of coordinates and momenta into non-interacting harmonic oscillators.",,"['linear-algebra', 'quadratic-forms', 'classical-mechanics']"
12,Uniqueness of symmetric positive definite matrix decomposition,Uniqueness of symmetric positive definite matrix decomposition,,"We know that any symmetric positive semi-definite matrix $K$ can be written as $K= AA^T$, where $A$ has real components.  One way to get to $A$ is to compute eigen value decomposition of $K= P^T DP$ and define $A= P^T \sqrt{D}$, where $\sqrt{D}$ simply computes the square roots of diagonal elements. Now, I wonder to what extent such a decomposition is unique. Of course if $AA^T=K$ then $-A$ also works. My questions are: Up to what transformation the above matrix decomposition is unique. Is positive definiteness (PD) and positive-semi definiteness (PSD) of $K$ makes difference in uniqueness of this decomposition? To have a unique solution, do we need to fix the number of columns of $A$ (for a PSD or PD matrix)? Is the decomposition unique only if we are given this dimension? $A$ is different from square root of $K$, right? Because square root does not have to be symmetric?! Answering any part will be useful for me. Specially part 2.","We know that any symmetric positive semi-definite matrix $K$ can be written as $K= AA^T$, where $A$ has real components.  One way to get to $A$ is to compute eigen value decomposition of $K= P^T DP$ and define $A= P^T \sqrt{D}$, where $\sqrt{D}$ simply computes the square roots of diagonal elements. Now, I wonder to what extent such a decomposition is unique. Of course if $AA^T=K$ then $-A$ also works. My questions are: Up to what transformation the above matrix decomposition is unique. Is positive definiteness (PD) and positive-semi definiteness (PSD) of $K$ makes difference in uniqueness of this decomposition? To have a unique solution, do we need to fix the number of columns of $A$ (for a PSD or PD matrix)? Is the decomposition unique only if we are given this dimension? $A$ is different from square root of $K$, right? Because square root does not have to be symmetric?! Answering any part will be useful for me. Specially part 2.",,"['linear-algebra', 'matrices', 'hilbert-spaces', 'numerical-linear-algebra', 'machine-learning']"
13,Inequality involving eigenvalues and trace of an operator with its adjoint,Inequality involving eigenvalues and trace of an operator with its adjoint,,"I have come across a problem which I can't properly solve: Let $\tau$ be a linear operator on $\mathbb{C}^n$ and let $\lambda_{1},...,\lambda_{n}$ be the eigenvalues of $\tau$, each one written a number of times equal to its algebraic multiplicity. I should show that: $\sum_{i}|\lambda_{i}|^2\leq tr(\tau^*\tau)$ Also, one should show that the equality holds iff $\tau$ is normal. First I felt that this might use singular values, but I have no success with this. My idea then was that Cauchy-Schwarz may be useful. (I work with matrices, this is clearly not a restriction to the problem.) So I defined the inner-product $\langle A,B\rangle=tr(B^*A)$, which I know to be acceptable. Elementary operations on Cauchy-Schwarz inequality $|\langle A,A^*\rangle|^2$$\leq \langle A,A\rangle\langle A^*,A^*\rangle$ then give that  $|\sum_{i}\lambda_{i}^2|^2$$\leq (tr(A^*A))^2$ (I may be mistaken). This is not what I want. In the question ''equality holds iff $\tau$ is normal'', one way (right to left) is easy. I highly appreciate any suggestion!","I have come across a problem which I can't properly solve: Let $\tau$ be a linear operator on $\mathbb{C}^n$ and let $\lambda_{1},...,\lambda_{n}$ be the eigenvalues of $\tau$, each one written a number of times equal to its algebraic multiplicity. I should show that: $\sum_{i}|\lambda_{i}|^2\leq tr(\tau^*\tau)$ Also, one should show that the equality holds iff $\tau$ is normal. First I felt that this might use singular values, but I have no success with this. My idea then was that Cauchy-Schwarz may be useful. (I work with matrices, this is clearly not a restriction to the problem.) So I defined the inner-product $\langle A,B\rangle=tr(B^*A)$, which I know to be acceptable. Elementary operations on Cauchy-Schwarz inequality $|\langle A,A^*\rangle|^2$$\leq \langle A,A\rangle\langle A^*,A^*\rangle$ then give that  $|\sum_{i}\lambda_{i}^2|^2$$\leq (tr(A^*A))^2$ (I may be mistaken). This is not what I want. In the question ''equality holds iff $\tau$ is normal'', one way (right to left) is easy. I highly appreciate any suggestion!",,"['linear-algebra', 'inequality']"
14,Rotation matrix from an inertia tensor,Rotation matrix from an inertia tensor,,"I have a set of weighted points in 3D space (in fact, a molecule) and I'm trying to align the principal axes of this set with the $x$, and $y$ and $z$ axes. To do so, I've first translated my points so their barycenter coincides with the origin. Then, I've calculated the moment of inertia tensor $I$ and its eigenvalues ($\lambda_i$) and eigenvectors ($V_i$). Then, I need to build the rotation matrix associated with the rotation bringing my vectors $V_i$ onto $(x,y,z)$. I assumed that would be the inverse of the matrix formed by the vectors $V_i$ in columns, but it is not. So, how would you write this matrix from what I have already calculated?","I have a set of weighted points in 3D space (in fact, a molecule) and I'm trying to align the principal axes of this set with the $x$, and $y$ and $z$ axes. To do so, I've first translated my points so their barycenter coincides with the origin. Then, I've calculated the moment of inertia tensor $I$ and its eigenvalues ($\lambda_i$) and eigenvectors ($V_i$). Then, I need to build the rotation matrix associated with the rotation bringing my vectors $V_i$ onto $(x,y,z)$. I assumed that would be the inverse of the matrix formed by the vectors $V_i$ in columns, but it is not. So, how would you write this matrix from what I have already calculated?",,"['linear-algebra', 'physics', 'rotations']"
15,Does similarity of integer matrices with square $-I$ imply the transition matrix is an integer matrix?,Does similarity of integer matrices with square  imply the transition matrix is an integer matrix?,-I,"I'm working on a homework question, and I'm stuck. The question is: Let $A$ and $B$ be $2n \times 2n$ rational matrices with $A^2=B^2=-I$. The first part of the question asks to show that $A$ and $B$ are similar, and that the transition matrix is rational. I believe I've done that. However it's this second part that has me stumped: Suppose $A$ and $B$ have integer coefficients. Can we assume that $C$ and $C^{-1}$ ($C$ is the transition matrix) have integer coefficients as well? The hint given was to convert $\mathbb{Z}^{2n}$ into a $\mathbb{Z}[x]$-module. While I'm sure there are other methods to doing this, I'm interested in following the direction of the hint, it seems like an interesting method, and I would like to get better at using module theory as a practical tool. What I've Done So Far: So for starters, in order to make $\mathbb{Z}^{2n}$ into a $\mathbb{Z}[x]$-module, we need a linear map. But it seems like there is a very natural choice in this case, namely, the map $T$ underlying the similar matrices $A$ and $B$. However, I'm not sure where to go from here. The only thing I can think of it using the structure theorem for finitely -generated modules over a PID, but $\mathbb{Z}[x]$ is not a PID, so that won't work.","I'm working on a homework question, and I'm stuck. The question is: Let $A$ and $B$ be $2n \times 2n$ rational matrices with $A^2=B^2=-I$. The first part of the question asks to show that $A$ and $B$ are similar, and that the transition matrix is rational. I believe I've done that. However it's this second part that has me stumped: Suppose $A$ and $B$ have integer coefficients. Can we assume that $C$ and $C^{-1}$ ($C$ is the transition matrix) have integer coefficients as well? The hint given was to convert $\mathbb{Z}^{2n}$ into a $\mathbb{Z}[x]$-module. While I'm sure there are other methods to doing this, I'm interested in following the direction of the hint, it seems like an interesting method, and I would like to get better at using module theory as a practical tool. What I've Done So Far: So for starters, in order to make $\mathbb{Z}^{2n}$ into a $\mathbb{Z}[x]$-module, we need a linear map. But it seems like there is a very natural choice in this case, namely, the map $T$ underlying the similar matrices $A$ and $B$. However, I'm not sure where to go from here. The only thing I can think of it using the structure theorem for finitely -generated modules over a PID, but $\mathbb{Z}[x]$ is not a PID, so that won't work.",,"['linear-algebra', 'matrices', 'modules']"
16,(good) numerical inversion of an almost singular matrix: is it possible?,(good) numerical inversion of an almost singular matrix: is it possible?,,"Ok, so I know that if I have a system that looks like Ax=b it is foolish to solve it by solving for the inverse of A and one should instead use something like Gaussian elimination, or something like that. However, I am trying to calculate H(s) = B*(sI-A)^-1*C , where s is some scalar and A , B , C are matrices (of dimensions such that H(s) is a scalar too, if that helps). Now, this is a large system and I can't do this calculation symbolically, but when I try and plug in the s values that I am interested in, the (sI-A) matrix has really bad condition numbers (something like 10^10 ) and I highly doubt that my numeric results are in any way accurate --- it's not just the condition numbers, the answer comes out completely unlike what I would expect it to be. Do you have any ideas on either some algorithm that could help me out, or a math trick that would scale things in a such a way that inversion would not run into machine precision problems? Just so everyone knows, this comes up when trying to calculate a transfer function from the state space representation of a linear system.","Ok, so I know that if I have a system that looks like Ax=b it is foolish to solve it by solving for the inverse of A and one should instead use something like Gaussian elimination, or something like that. However, I am trying to calculate H(s) = B*(sI-A)^-1*C , where s is some scalar and A , B , C are matrices (of dimensions such that H(s) is a scalar too, if that helps). Now, this is a large system and I can't do this calculation symbolically, but when I try and plug in the s values that I am interested in, the (sI-A) matrix has really bad condition numbers (something like 10^10 ) and I highly doubt that my numeric results are in any way accurate --- it's not just the condition numbers, the answer comes out completely unlike what I would expect it to be. Do you have any ideas on either some algorithm that could help me out, or a math trick that would scale things in a such a way that inversion would not run into machine precision problems? Just so everyone knows, this comes up when trying to calculate a transfer function from the state space representation of a linear system.",,['matrices']
17,Dimension of a vector space,Dimension of a vector space,,My question is: Is the vector space containing all periodic complex sequences a finite-dimensional vector space?,My question is: Is the vector space containing all periodic complex sequences a finite-dimensional vector space?,,['linear-algebra']
18,"If a matrix is upper-triangular, does its diagonal contain all the eigenvalues? If so, why?","If a matrix is upper-triangular, does its diagonal contain all the eigenvalues? If so, why?",,"If a matrix is upper-triangular, does its diagonal contain its eigenvalues? If yes, how can this be proven? My textbook and teacher just jumped over this statement (we are working over complex numbers, does the answer change if it's over reals?) and I was wondering if someone could provide a proof.","If a matrix is upper-triangular, does its diagonal contain its eigenvalues? If yes, how can this be proven? My textbook and teacher just jumped over this statement (we are working over complex numbers, does the answer change if it's over reals?) and I was wondering if someone could provide a proof.",,['linear-algebra']
19,Thoroughly understand the concepts and formulas in Linear Algebra [duplicate],Thoroughly understand the concepts and formulas in Linear Algebra [duplicate],,"This question already has answers here : What's an intuitive way to think about the determinant? (18 answers) Closed 9 years ago . In linear algebra(I'm the beginner so far), I feel the concepts and ideas are based on on another, like layer by layer , and there are many relations among the properties. For example, Matrix Multiplication, AB is essentially the composition of Linear Transformation（$A(Bx)=(AB)x$ to manually define AB) , and Linear Transformation is also from Matrix Equation($Ax=b, T(x)=b=Ax$)，and Matrix Equation is also from Vector Equation($x_1a_1+...+x_na_n=b$ equivalent to $Ax=b$)，and finally Vector Equation is the basic System of Linear Equations. so Vector Equation could be intuitively understood as Linear System，Matrix Equation is just another formation。 Linear Transformation is fine too，but when it comes to Matrix Algebra， like Inverse Matrix and its properties, it seems very hard to be intuitively understood through the basic System of Linear Equations. It'll be only fine when I admit the manually definition the matrix multiplication from composition of linear transformation. Because as the process moves on, it's more and more like everything is constructed layer by layer, and more and more complicated as it's very hard to try to understand it from the very basic layer (e.g. linear system) intuitively . I'm not sure whether it's the normal situation, because there are also intersected properties among many 'layers'. Therefore the whole 'knowledge structure' is not very clear in my brain. And, when I think of the the basic Quadratic Formula in elementary algebra, ， its meaning is just to solve the quadratic equation with one variable, and then it is proved from completing square of $ax^2+bx+c=0$ to find general formation of $x$. But it is still impossible to 'understand thoroughly' from the original equation like how the relation among each variable and coefficient. So when I need to use it, I just refer to the book or prove it myself (assume if I forget) So will it the same thing as learning the Linear Algebra, like the determinant of matrix,  $\det A = \sum_{j=1}^{n}(-1)^{1+j}a_{1j}\det A_{1j}$. Is it okay that what I need to do is only to know two things, What it really is. How to prove it. (When I use it, just refer to book or prove it, however, prove the determinant takes a little long time). Because this formula ($\det A$) is already impossible to be thoroughly understood from the very basic 'layer' like system of linear equations or even vector equation. I only know, Firstly its meaning is to determine whether a matrix is invertible (because invertible matrix must has $n\times n$ size and has pivot position in each row and no zeros in diagonal entries.), Secondly it's proved from $n\times n$ matrix row reduced to echelon form, then the right-bottom corner must be nonzero, the above formula is just the brief formation of the item in right-bottom corner. I know this two things, could I say I'm already understand it ? Because even though I know 1.what is really is and 2.how to prove it , but I'm still not able to remember it when use it.","This question already has answers here : What's an intuitive way to think about the determinant? (18 answers) Closed 9 years ago . In linear algebra(I'm the beginner so far), I feel the concepts and ideas are based on on another, like layer by layer , and there are many relations among the properties. For example, Matrix Multiplication, AB is essentially the composition of Linear Transformation（$A(Bx)=(AB)x$ to manually define AB) , and Linear Transformation is also from Matrix Equation($Ax=b, T(x)=b=Ax$)，and Matrix Equation is also from Vector Equation($x_1a_1+...+x_na_n=b$ equivalent to $Ax=b$)，and finally Vector Equation is the basic System of Linear Equations. so Vector Equation could be intuitively understood as Linear System，Matrix Equation is just another formation。 Linear Transformation is fine too，but when it comes to Matrix Algebra， like Inverse Matrix and its properties, it seems very hard to be intuitively understood through the basic System of Linear Equations. It'll be only fine when I admit the manually definition the matrix multiplication from composition of linear transformation. Because as the process moves on, it's more and more like everything is constructed layer by layer, and more and more complicated as it's very hard to try to understand it from the very basic layer (e.g. linear system) intuitively . I'm not sure whether it's the normal situation, because there are also intersected properties among many 'layers'. Therefore the whole 'knowledge structure' is not very clear in my brain. And, when I think of the the basic Quadratic Formula in elementary algebra, ， its meaning is just to solve the quadratic equation with one variable, and then it is proved from completing square of $ax^2+bx+c=0$ to find general formation of $x$. But it is still impossible to 'understand thoroughly' from the original equation like how the relation among each variable and coefficient. So when I need to use it, I just refer to the book or prove it myself (assume if I forget) So will it the same thing as learning the Linear Algebra, like the determinant of matrix,  $\det A = \sum_{j=1}^{n}(-1)^{1+j}a_{1j}\det A_{1j}$. Is it okay that what I need to do is only to know two things, What it really is. How to prove it. (When I use it, just refer to book or prove it, however, prove the determinant takes a little long time). Because this formula ($\det A$) is already impossible to be thoroughly understood from the very basic 'layer' like system of linear equations or even vector equation. I only know, Firstly its meaning is to determine whether a matrix is invertible (because invertible matrix must has $n\times n$ size and has pivot position in each row and no zeros in diagonal entries.), Secondly it's proved from $n\times n$ matrix row reduced to echelon form, then the right-bottom corner must be nonzero, the above formula is just the brief formation of the item in right-bottom corner. I know this two things, could I say I'm already understand it ? Because even though I know 1.what is really is and 2.how to prove it , but I'm still not able to remember it when use it.",,"['linear-algebra', 'matrices']"
20,Determining possible minimal polynomials for a rank one linear operator,Determining possible minimal polynomials for a rank one linear operator,,"I have come across a question about determining possible minimal polynomials for a rank one linear operator and I am wondering if I am using the correct proof method.  I think that the facts needed to solve this problem come from the section on Nilpotent operators from Hoffman and Kunze's ""Linear Algebra"". Question: Let $V$ be a vector space of dimension $n$ over the field $F$ and consider a linear operator $T$ on $V$ such that $\mathrm{rank}(T) = 1$.  List all possible minimal polynomials for $T$. Sketch of Proof:   If $T$ is nilpotent then the minimal polynomial of $T$ is $x^k$ for some $k\leq n$.  So suppose $T$ is not nilpotent, then we can argue that $T$ is diagonalizable based on the fact that $T$ must have one nonzero eigenvalue otherwise it would be nilpotent (I am leaving details of the proof of diagonalization but it is the observation that the characteristic space of the nonzero eigenvalue is the range of T and has dimension $1$).  Thus the minimal polynomial of $T$ is just a linear term $x-c$. Did I make a mistake assuming that $T$ can have only one nonzero eigenvalue? Thanks for your help","I have come across a question about determining possible minimal polynomials for a rank one linear operator and I am wondering if I am using the correct proof method.  I think that the facts needed to solve this problem come from the section on Nilpotent operators from Hoffman and Kunze's ""Linear Algebra"". Question: Let $V$ be a vector space of dimension $n$ over the field $F$ and consider a linear operator $T$ on $V$ such that $\mathrm{rank}(T) = 1$.  List all possible minimal polynomials for $T$. Sketch of Proof:   If $T$ is nilpotent then the minimal polynomial of $T$ is $x^k$ for some $k\leq n$.  So suppose $T$ is not nilpotent, then we can argue that $T$ is diagonalizable based on the fact that $T$ must have one nonzero eigenvalue otherwise it would be nilpotent (I am leaving details of the proof of diagonalization but it is the observation that the characteristic space of the nonzero eigenvalue is the range of T and has dimension $1$).  Thus the minimal polynomial of $T$ is just a linear term $x-c$. Did I make a mistake assuming that $T$ can have only one nonzero eigenvalue? Thanks for your help",,"['linear-algebra', 'linear-transformations', 'minimal-polynomials']"
21,Proof that a linear transformation is isomorphic,Proof that a linear transformation is isomorphic,,"As homework, I need to proove whether a few linear transformations are isomorphic or not, however i do not know how to achieve this. First of all i have proven that the following map is linear: $$f:\mathbb{R}^2\mapsto\mathbb{R}^2, f\left( \begin{bmatrix}x\\y\end{bmatrix} \right)=x\begin{bmatrix}1\\1\end{bmatrix} + y \begin{bmatrix}-1\\1\end{bmatrix}$$ via the definition that a linear transformation $T: X\mapsto Y$ must satisfy the following condition (let $X$ and $Y$ be linear spaces over the field $\mathbb{K}$ ) $$\forall\alpha,\beta\in\mathbb{K}\wedge x_1,x_2\in X\,:\,T(\alpha x_1+\beta x_2) = \alpha T x_1+\beta T x_2$$ Can you explain me where to go ahead? (I am from germany so please make your explanations not that difficult :-))","As homework, I need to proove whether a few linear transformations are isomorphic or not, however i do not know how to achieve this. First of all i have proven that the following map is linear: via the definition that a linear transformation must satisfy the following condition (let and be linear spaces over the field ) Can you explain me where to go ahead? (I am from germany so please make your explanations not that difficult :-))","f:\mathbb{R}^2\mapsto\mathbb{R}^2, f\left( \begin{bmatrix}x\\y\end{bmatrix} \right)=x\begin{bmatrix}1\\1\end{bmatrix} + y \begin{bmatrix}-1\\1\end{bmatrix} T: X\mapsto Y X Y \mathbb{K} \forall\alpha,\beta\in\mathbb{K}\wedge x_1,x_2\in X\,:\,T(\alpha x_1+\beta x_2) = \alpha T x_1+\beta T x_2",['linear-algebra']
22,Prove Pythagoras theorem through dimensional analysis,Prove Pythagoras theorem through dimensional analysis,,I've recently become acquainted with Buckingham's Pi theorem for the first time . Then I've found an excercise that says: Use dimensional analysis to prove the Pythagoras theorem. [Hint: Drop a perpendicular to the hypotenuse of a right-angle triangle and consider the resulting similar triangles.] Any ideas? Thanks.,I've recently become acquainted with Buckingham's Pi theorem for the first time . Then I've found an excercise that says: Use dimensional analysis to prove the Pythagoras theorem. [Hint: Drop a perpendicular to the hypotenuse of a right-angle triangle and consider the resulting similar triangles.] Any ideas? Thanks.,,"['linear-algebra', 'geometry', 'physics']"
23,1-dimensional solution space of homogeneous system Ax=0?,1-dimensional solution space of homogeneous system Ax=0?,,"Given is an almost-square matrix $A$ with $n$ columns and $n-1$ rows with maximum rank. The solutions of the homogeneous system $Ax = 0$ form a 1-dimensional subspace of $\mathbb{R}^n$. I've discovered the following which I believe to be true but I can't prove: the components of the vector $x$ that spans the (1D) solution space are given by: $x_i = (-1)^{i-1} |A_i|$ in which $|A_i|$ is the determinant of the square submatrix of A obtained by removing the i-th column from A. For example, in $\mathbb{R^3}$, $A$ is a 2x3 matrix, and $x$ as defined above turns out to be the crossproduct of the two row vectors of $A$. Is this true, and if so, how can it be proved?","Given is an almost-square matrix $A$ with $n$ columns and $n-1$ rows with maximum rank. The solutions of the homogeneous system $Ax = 0$ form a 1-dimensional subspace of $\mathbb{R}^n$. I've discovered the following which I believe to be true but I can't prove: the components of the vector $x$ that spans the (1D) solution space are given by: $x_i = (-1)^{i-1} |A_i|$ in which $|A_i|$ is the determinant of the square submatrix of A obtained by removing the i-th column from A. For example, in $\mathbb{R^3}$, $A$ is a 2x3 matrix, and $x$ as defined above turns out to be the crossproduct of the two row vectors of $A$. Is this true, and if so, how can it be proved?",,[]
24,How to identify all the right circular cones passing through six arbitrary points,How to identify all the right circular cones passing through six arbitrary points,,"I have this interesting question.  Given $6$ arbitrary points, I want to identify all the possible circular cones passing through them. The equation of a right circular cone whose vertex is at $\mathbf{r_0}$ and whose axis is along the unit vector $\mathbf{a}$ , and whose semi-vertical angle is $\theta$ is given by $$ (\mathbf{r} - \mathbf{r_0})^T ( (\cos \theta)^2 \ \mathbf{I} - \mathbf{aa}^T ) (\mathbf{r} - \mathbf{r_0}) = 0 $$ where $\mathbf{r}$ is the position vector of a point on the surface of the cone.  Counting the number of parameters, we have $3$ for $\mathbf{r_0}$ , $2$ for $\mathbf{a}$ and $1$ for $\theta$ .  We therefore need at least $6$ points on the surface of the cone to specify it. Question :  What is the procedure for extracting the parameters of a right circular cone passing through $6$ arbitrary points? What I have tried :  I have parameterized the axis unit vector as $ \mathbf{a} = ( \sin t \cos s, \sin t \sin s , \cos t ) $ and written $\mathbf{r_0} = (x, y, z) $ Now define the functions $ f_1 =  (\mathbf{r_1} - \mathbf{r_0})^T ( (\cos \theta)^2 \ \mathbf{I} - \mathbf{aa}^T ) (\mathbf{r_1} - \mathbf{r_0})  $ $ f_2 =  (\mathbf{r_2} - \mathbf{r_0})^T ( (\cos \theta)^2 \ \mathbf{I} - \mathbf{aa}^T ) (\mathbf{r_2} - \mathbf{r_0})  $ $ f_3 =  (\mathbf{r_3} - \mathbf{r_0})^T ( (\cos \theta)^2 \ \mathbf{I} - \mathbf{aa}^T ) (\mathbf{r_3} - \mathbf{r_0})  $ $ f_4 =  (\mathbf{r_4} - \mathbf{r_0})^T ( (\cos \theta)^2 \ \mathbf{I} - \mathbf{aa}^T ) (\mathbf{r_4} - \mathbf{r_0})  $ $ f_5 =  (\mathbf{r_5} - \mathbf{r_0})^T ( (\cos \theta)^2 \ \mathbf{I} - \mathbf{aa}^T ) (\mathbf{r_5} - \mathbf{r_0})  $ $ f_6 =  (\mathbf{r_6} - \mathbf{r_0})^T ( (\cos \theta)^2 \ \mathbf{I} - \mathbf{aa}^T ) (\mathbf{r_6} - \mathbf{r_0})  $ Now using the multivariate Newton-Raphson method I could iterate to find a solution for the parameter vector $(t, s, x, y, z , \theta )$ that will make $f_1, f_2, f_3, f_4, f_5, f_6$ all zero. This works, but it's iterative and at best converges to one of the possible right circular cones. Is there a way to generate all the possible right circular cones that are solutions, i.e. passing the $6$ given points?","I have this interesting question.  Given arbitrary points, I want to identify all the possible circular cones passing through them. The equation of a right circular cone whose vertex is at and whose axis is along the unit vector , and whose semi-vertical angle is is given by where is the position vector of a point on the surface of the cone.  Counting the number of parameters, we have for , for and for .  We therefore need at least points on the surface of the cone to specify it. Question :  What is the procedure for extracting the parameters of a right circular cone passing through arbitrary points? What I have tried :  I have parameterized the axis unit vector as and written Now define the functions Now using the multivariate Newton-Raphson method I could iterate to find a solution for the parameter vector that will make all zero. This works, but it's iterative and at best converges to one of the possible right circular cones. Is there a way to generate all the possible right circular cones that are solutions, i.e. passing the given points?","6 \mathbf{r_0} \mathbf{a} \theta  (\mathbf{r} - \mathbf{r_0})^T ( (\cos \theta)^2 \ \mathbf{I} - \mathbf{aa}^T ) (\mathbf{r} - \mathbf{r_0}) = 0  \mathbf{r} 3 \mathbf{r_0} 2 \mathbf{a} 1 \theta 6 6  \mathbf{a} = ( \sin t \cos s, \sin t \sin s , \cos t )  \mathbf{r_0} = (x, y, z)   f_1 =  (\mathbf{r_1} - \mathbf{r_0})^T ( (\cos \theta)^2 \ \mathbf{I} - \mathbf{aa}^T ) (\mathbf{r_1} - \mathbf{r_0})    f_2 =  (\mathbf{r_2} - \mathbf{r_0})^T ( (\cos \theta)^2 \ \mathbf{I} - \mathbf{aa}^T ) (\mathbf{r_2} - \mathbf{r_0})    f_3 =  (\mathbf{r_3} - \mathbf{r_0})^T ( (\cos \theta)^2 \ \mathbf{I} - \mathbf{aa}^T ) (\mathbf{r_3} - \mathbf{r_0})    f_4 =  (\mathbf{r_4} - \mathbf{r_0})^T ( (\cos \theta)^2 \ \mathbf{I} - \mathbf{aa}^T ) (\mathbf{r_4} - \mathbf{r_0})    f_5 =  (\mathbf{r_5} - \mathbf{r_0})^T ( (\cos \theta)^2 \ \mathbf{I} - \mathbf{aa}^T ) (\mathbf{r_5} - \mathbf{r_0})    f_6 =  (\mathbf{r_6} - \mathbf{r_0})^T ( (\cos \theta)^2 \ \mathbf{I} - \mathbf{aa}^T ) (\mathbf{r_6} - \mathbf{r_0})   (t, s, x, y, z , \theta ) f_1, f_2, f_3, f_4, f_5, f_6 6","['linear-algebra', 'geometry', 'quadrics', 'system-identification']"
25,Show that all the eigenvalues of a matrix but one (which is null) have negative real part,Show that all the eigenvalues of a matrix but one (which is null) have negative real part,,"I have a situation where a matrix $A=[a_{ij}]$ arises. From the physics of the problem, I expect this matrix to have one null eigenvalue, while the remaining eigenvalues have negative real part. However I have not been able to prove the second part of the statement. I appreciate any help / hint /guidance on how to approach the problem. The off-diagonal elements are given by $$ a_{ij} = \begin{cases} n_{ji}, & \text{if } j<i \\ n_{ji}+1, & \text{if } j>i \end{cases} $$ Whereas the diagonal elements are given by $ a_{ii} = -\sum_{k \neq i =1}^N a_{ki} $ . It is clear that the row vector of all-ones $\mathbb 1_N$ is always a left eigenvector with null eigenvalue. Furthermore, the $n_{ij}$ are such that: $\bullet$ $ n_{ij}>0 $ $\bullet$ $ n_{ij} $ increase with $i: n_{ij} < n_{(i+1)j}$ $\bullet$ $ n_{ij} $ decrease with $j: n_{ij} > n_{i(j+1)}$ Case N=2 $$ \begin{bmatrix} -n_{12} & 1+n_{12} \\ n_{12} & -1-n_{12} \\ \end{bmatrix} $$ The eigenvalues are $0$ and $-1-2 n_{12}$ . Case N=3 $$ \begin{bmatrix} -n_{12}-n_{13} & 1+n_{12} & 1+n_{13} \\ n_{12} & -1-n_{12}-n_{23} & 1+n_{23} \\ n_{13} & n_{23} & -2-n_{13}-n_{23} \\ \end{bmatrix} $$ The non-zero eigenvalues are given by (after some messy computations, or after asking Wolfram Mathematica): $$ (-3 - 2 n_{12} - 2 n_{13} - 2 n_{23} \pm \sqrt{1 - 4 n_{12} + 4 n_{12}^2 - 4 n_{12} n_{13} + 4 n_{13}^2 + 4 n_{23} - 4 n_{12} n_{23} - 4 n_{13} n_{23} + 4 n_{23}^2})/2 $$ We can see that the real part must be negative by noting that $(3 + 2 n_{12} + 2 n_{13} + 2 n_{23})^2$ is strictly greater than $(1 - 4 n_{12} + 4 n_{12}^2 - 4 n_{12} n_{13} + 4 n_{13}^2 + 4 n_{23} - 4 n_{12} n_{23} - 4 n_{13} n_{23} + 4 n_{23}^2)$ . As $N$ increases, the eigenvalue computations becomes messier... Is there a simpler way to show that they will be negative?","I have a situation where a matrix arises. From the physics of the problem, I expect this matrix to have one null eigenvalue, while the remaining eigenvalues have negative real part. However I have not been able to prove the second part of the statement. I appreciate any help / hint /guidance on how to approach the problem. The off-diagonal elements are given by Whereas the diagonal elements are given by . It is clear that the row vector of all-ones is always a left eigenvector with null eigenvalue. Furthermore, the are such that: increase with decrease with Case N=2 The eigenvalues are and . Case N=3 The non-zero eigenvalues are given by (after some messy computations, or after asking Wolfram Mathematica): We can see that the real part must be negative by noting that is strictly greater than . As increases, the eigenvalue computations becomes messier... Is there a simpler way to show that they will be negative?","A=[a_{ij}] 
a_{ij} =
\begin{cases}
n_{ji}, & \text{if } j<i \\
n_{ji}+1, & \text{if } j>i
\end{cases}
  a_{ii} = -\sum_{k \neq i =1}^N a_{ki}  \mathbb 1_N n_{ij} \bullet  n_{ij}>0  \bullet  n_{ij}  i: n_{ij} < n_{(i+1)j} \bullet  n_{ij}  j: n_{ij} > n_{i(j+1)} 
\begin{bmatrix}
-n_{12} & 1+n_{12} \\
n_{12} & -1-n_{12} \\
\end{bmatrix}
 0 -1-2 n_{12} 
\begin{bmatrix}
-n_{12}-n_{13} & 1+n_{12} & 1+n_{13} \\
n_{12} & -1-n_{12}-n_{23} & 1+n_{23} \\
n_{13} & n_{23} & -2-n_{13}-n_{23} \\
\end{bmatrix}
 
(-3 - 2 n_{12} - 2 n_{13} - 2 n_{23} \pm \sqrt{1 - 4 n_{12} + 4 n_{12}^2 - 4 n_{12} n_{13} + 4 n_{13}^2 + 4 n_{23} - 4 n_{12} n_{23} - 4 n_{13} n_{23} + 4 n_{23}^2})/2
 (3 + 2 n_{12} + 2 n_{13} + 2 n_{23})^2 (1 - 4 n_{12} + 4 n_{12}^2 - 4 n_{12} n_{13} + 4 n_{13}^2 + 4 n_{23} - 4 n_{12} n_{23} - 4 n_{13} n_{23} + 4 n_{23}^2) N","['linear-algebra', 'eigenvalues-eigenvectors', 'control-theory']"
26,"Vector that maximizes $(x^TAx)(x^TBx)$ subject to $x^Tx=1$ ? (A,B symmetric, positive definite)","Vector that maximizes  subject to  ? (A,B symmetric, positive definite)",(x^TAx)(x^TBx) x^Tx=1,"I've been playing with eigenvector-type optimisation problems where a vector maximally projects onto two matrices. The sum version of this problem (maximize $x^TAx + x^TBx$ ) is fairly straightforward (leading eigenvector of $A+B$ ), but the product version in the title has completely stumped me. To restate the problem, I aim to find a vector $x$ such that $x^Tx=1$ that maximizes the quantity $(x^TAx)(x^TBx)$ . Here A and B can be taken to be symmetric and positive definite matrices. My attempted solution involved a substitution $p=B^{-1/2}x$ , such that the quantity to be maximized is $(p^TB^{-1/2}AB^{-1/2}p)(p^Tp)$ , but $p^Tp=x^TB^{-1}x$ is unknown, so we are back to square one. I also considered that, as A and B are positive definite, perhaps the vector that maximizes the sum also maximizes the product. This of course doesn't hold though, as either of the terms may be <1. Any pointers would be received gratefully.","I've been playing with eigenvector-type optimisation problems where a vector maximally projects onto two matrices. The sum version of this problem (maximize ) is fairly straightforward (leading eigenvector of ), but the product version in the title has completely stumped me. To restate the problem, I aim to find a vector such that that maximizes the quantity . Here A and B can be taken to be symmetric and positive definite matrices. My attempted solution involved a substitution , such that the quantity to be maximized is , but is unknown, so we are back to square one. I also considered that, as A and B are positive definite, perhaps the vector that maximizes the sum also maximizes the product. This of course doesn't hold though, as either of the terms may be <1. Any pointers would be received gratefully.",x^TAx + x^TBx A+B x x^Tx=1 (x^TAx)(x^TBx) p=B^{-1/2}x (p^TB^{-1/2}AB^{-1/2}p)(p^Tp) p^Tp=x^TB^{-1}x,"['linear-algebra', 'optimization', 'eigenvalues-eigenvectors', 'nonlinear-optimization']"
27,Can we always find this kind of unitary matrix to diagonalize this special form hermitian matrix?,Can we always find this kind of unitary matrix to diagonalize this special form hermitian matrix?,,"Suppose we have a hermitian matrix $M$ of form $$\left( \begin{matrix} 	\alpha&		-\beta ^*\\ 	\beta&		-\alpha ^*\\ \end{matrix} \right) $$ where $\alpha$ is a hermitian matrix and $\beta$ is an antisymmetric matrix with equal dimension, and I use the star $*$ for complex conjugate . Since $M$ is hermitian, then it can be diagonalized into $$TMT^{\dagger}=\left( \begin{matrix} 	d&		0\\ 	0&		-d\\ \end{matrix} \right)$$ where $d$ is diagonal, and I used the special form of $M$ to deduce that the eigenvalues are real and appear in matched pairs $\pm \lambda $ . But I want to say more about the unitary matrix $T$ , i.e., can $T$ always be chosen to be of form $\left( \begin{matrix} 	\gamma&		\mu\\ 	\mu ^*&		\gamma ^*\\ \end{matrix} \right) $ where $\gamma$ and $\mu$ are two matrices with equal dimensions? Edit The question is found in this notes(from the bottom of page 6 to 7) for introduction of Jordan-Wigner transformation. The author states the above rules while I really can't see how to show that. I quoted the closely related part for your convenience: One way of obtaining a rigorous proof is to find a $T$ satisfying $$ T M T^{\dagger}=\left[\begin{array}{cc} d & 0 \\ 0 & -d \end{array}\right] $$ and then to apply the cosine-sine (or CS) decomposition from linear algebra, which provides a beautiful way of representing block unitary matrices, and which, in this instance, allows us to obtain a $T$ of the desired form with just a little more work. Thanks in advance!","Suppose we have a hermitian matrix of form where is a hermitian matrix and is an antisymmetric matrix with equal dimension, and I use the star for complex conjugate . Since is hermitian, then it can be diagonalized into where is diagonal, and I used the special form of to deduce that the eigenvalues are real and appear in matched pairs . But I want to say more about the unitary matrix , i.e., can always be chosen to be of form where and are two matrices with equal dimensions? Edit The question is found in this notes(from the bottom of page 6 to 7) for introduction of Jordan-Wigner transformation. The author states the above rules while I really can't see how to show that. I quoted the closely related part for your convenience: One way of obtaining a rigorous proof is to find a satisfying and then to apply the cosine-sine (or CS) decomposition from linear algebra, which provides a beautiful way of representing block unitary matrices, and which, in this instance, allows us to obtain a of the desired form with just a little more work. Thanks in advance!","M \left( \begin{matrix}
	\alpha&		-\beta ^*\\
	\beta&		-\alpha ^*\\
\end{matrix} \right)  \alpha \beta * M TMT^{\dagger}=\left( \begin{matrix}
	d&		0\\
	0&		-d\\
\end{matrix} \right) d M \pm \lambda  T T \left( \begin{matrix}
	\gamma&		\mu\\
	\mu ^*&		\gamma ^*\\
\end{matrix} \right)  \gamma \mu T 
T M T^{\dagger}=\left[\begin{array}{cc}
d & 0 \\
0 & -d
\end{array}\right]
 T","['linear-algebra', 'matrices', 'matrix-decomposition', 'unitary-matrices', 'hermitian-matrices']"
28,Show the Dimension of a Finite Vector Space is Even,Show the Dimension of a Finite Vector Space is Even,,"Let $V$ be a finite-dimensional vector space over a field $\mathbb{F}$ of characteristic $\ne 2$ . Let $X,Y: V \rightarrow V$ be linear transformations such that $X^2=Y^2=\rm Id$ , where $\rm Id$ is the identity transformation. Suppose that $XY=-YX$ . Show that $\dim V$ is even, and that there is a basis of $V$ in which the matrices of $X$ and $Y$ are $$\begin{bmatrix} 0 & I_n \\ I_n & 0\end{bmatrix} \quad \& \quad \begin{bmatrix} I_n & 0 \\ 0 & -I_n\end{bmatrix}$$ respectively, where $I_n$ is the identity matrix and $\dim V=2n$ . Things I know: I've showed that both $X$ and $Y$ are diagonalizable with eigenvalues $\lambda= \pm 1$ . I have also thought the since $X,Y$ are diagonalizable, we may use the Jordan canonical form. But from here, I'm not sure how to proceed. Any help will be appreciated!","Let be a finite-dimensional vector space over a field of characteristic . Let be linear transformations such that , where is the identity transformation. Suppose that . Show that is even, and that there is a basis of in which the matrices of and are respectively, where is the identity matrix and . Things I know: I've showed that both and are diagonalizable with eigenvalues . I have also thought the since are diagonalizable, we may use the Jordan canonical form. But from here, I'm not sure how to proceed. Any help will be appreciated!","V \mathbb{F} \ne 2 X,Y: V \rightarrow V X^2=Y^2=\rm Id \rm Id XY=-YX \dim V V X Y \begin{bmatrix} 0 & I_n \\ I_n & 0\end{bmatrix} \quad \& \quad \begin{bmatrix} I_n & 0 \\ 0 & -I_n\end{bmatrix} I_n \dim V=2n X Y \lambda= \pm 1 X,Y","['linear-algebra', 'vector-spaces', 'linear-transformations', 'diagonalization']"
29,Permanent of a low-rank matrix is easy to calculate?,Permanent of a low-rank matrix is easy to calculate?,,"In this paper , Aaronson and Arkhipov use the following lemma (lemma 67 in the paper): Let $V \in \mathbb{C}^{n \times n}$ be a matrix of rank $k$ . Then $\operatorname{Per}(V + I)$ is computable exactly in $n^{O(k)}$ time. Unfortunately, they refer to a ""forthcoming paper"" which I was unable to find. Is there a publication where this or a similar statement is proven?","In this paper , Aaronson and Arkhipov use the following lemma (lemma 67 in the paper): Let be a matrix of rank . Then is computable exactly in time. Unfortunately, they refer to a ""forthcoming paper"" which I was unable to find. Is there a publication where this or a similar statement is proven?",V \in \mathbb{C}^{n \times n} k \operatorname{Per}(V + I) n^{O(k)},"['linear-algebra', 'reference-request', 'computer-science', 'computational-complexity', 'permanent']"
30,Maximum number of linearly independent non-commuting matrices,Maximum number of linearly independent non-commuting matrices,,"Let $S$ be a set of non-commuting, linearly independent $d \times d$ positive definite matrices (i.e., for any $A \neq B$ , $[A, B] = AB - BA \neq 0$ ). Is there any upper bound for the number of elements the set $S$ contains? (It is clear that it must be equal to or smaller than $d^2$ . Any reference to a book/article is appreciated). By positive definite matrix I mean matrices in the form $A = M^{\dagger}M$ where $M$ is a $d \times d$ invertible matrix over the field $\mathbb{C}$ . Linear independence, however, is (necessarily) considered over $\mathbb{R}$ , e.g., $A \neq a_1 B + a_2 C$ for any $a_1, a_2 \in \mathbb{R}$ and $A,B,C \in S$ .","Let be a set of non-commuting, linearly independent positive definite matrices (i.e., for any , ). Is there any upper bound for the number of elements the set contains? (It is clear that it must be equal to or smaller than . Any reference to a book/article is appreciated). By positive definite matrix I mean matrices in the form where is a invertible matrix over the field . Linear independence, however, is (necessarily) considered over , e.g., for any and .","S d \times d A \neq B [A, B] = AB - BA \neq 0 S d^2 A = M^{\dagger}M M d \times d \mathbb{C} \mathbb{R} A \neq a_1 B + a_2 C a_1, a_2 \in \mathbb{R} A,B,C \in S",['linear-algebra']
31,Help with understanding the Gram-Schmidt Process,Help with understanding the Gram-Schmidt Process,,"Let $U=\langle x_1,x_2,x_3\rangle \subseteq \mathbb{R^4},$ where $$x_1=\begin {pmatrix} 3\\4 \\0\\0 \end {pmatrix}, \ x_2=\begin {pmatrix} 1\\3 \\1\\1 \end {pmatrix},\ x_3=\begin {pmatrix} 0\\5 \\5\\7 \end {pmatrix}.$$ Use the Gram-Schmidt Process to determine an orthogonal basis of $U$ with respect to the bilinear form given by the identity $I_4\in \mathbb{R^{4\times4}}$ . I think I understand the process at hand ok enough but I'm unsure of my work and need someone to check it. First I look at the Gram Matrix which is just the $4\times4 $ identity matrix. Then based on the definition of the Gram matrix I turn it into $ \mathfrak{b}(v_i,y_i)= v_1y_1+v_2y_2+v_3y_3+v_4y_4.$ Then I start looking at the actual process: $$B'=\langle w_1,w_2,w_3\rangle \text{ where } w_1= \begin {pmatrix} 3\\4\\0\\0 \end {pmatrix} $$ $$w_2=x_2-\frac{\mathfrak{b}(x_2,w_1)}{\mathfrak{b}(w_1,w_1)}w_1 \implies w_2= \begin {pmatrix} 1\\3\\1\\1 \end {pmatrix} - \frac{(1 \times3)+(3\times4)+(1\times0)+(1\times0)}{(3\times3)+(4\times4)+(0\times0)+(0\times0)}\begin {pmatrix} 3\\4\\0\\0 \end {pmatrix}.$$ $$w_2= \begin {pmatrix} -4/5\\3/5\\1\\1 \end {pmatrix} $$ $$ w_3= \begin {pmatrix} 0\\5\\5\\7 \end {pmatrix} - \frac{(5\times4)}{25}\begin {pmatrix} 3\\4\\0\\0 \end {pmatrix}- \frac{(0)+(3)+(5)+(7)}{(16/25)+(9/25)+1+1} \begin {pmatrix} -4/5\\3/5\\1\\1 \end {pmatrix}$$ $$w_3= \begin {pmatrix} 8/5\\-6/5\\0\\2 \end {pmatrix},\ B'= \langle w_1,w_2,w_3\rangle.$$ What I'm most particularly unsure about is whether or not I had to turn the Gram matrix into a bilinear form and also I'm just generally unsure about what I've turned it into as well. I suspect I should actually be doing vector-matrix multiplication or something but I don't really know.",Let where Use the Gram-Schmidt Process to determine an orthogonal basis of with respect to the bilinear form given by the identity . I think I understand the process at hand ok enough but I'm unsure of my work and need someone to check it. First I look at the Gram Matrix which is just the identity matrix. Then based on the definition of the Gram matrix I turn it into Then I start looking at the actual process: What I'm most particularly unsure about is whether or not I had to turn the Gram matrix into a bilinear form and also I'm just generally unsure about what I've turned it into as well. I suspect I should actually be doing vector-matrix multiplication or something but I don't really know.,"U=\langle x_1,x_2,x_3\rangle \subseteq \mathbb{R^4}, x_1=\begin {pmatrix} 3\\4 \\0\\0
\end {pmatrix}, \ x_2=\begin {pmatrix} 1\\3 \\1\\1 \end {pmatrix},\ x_3=\begin {pmatrix} 0\\5 \\5\\7 \end {pmatrix}. U I_4\in \mathbb{R^{4\times4}} 4\times4   \mathfrak{b}(v_i,y_i)= v_1y_1+v_2y_2+v_3y_3+v_4y_4. B'=\langle w_1,w_2,w_3\rangle \text{ where } w_1= \begin {pmatrix} 3\\4\\0\\0 \end {pmatrix}  w_2=x_2-\frac{\mathfrak{b}(x_2,w_1)}{\mathfrak{b}(w_1,w_1)}w_1 \implies w_2= \begin {pmatrix} 1\\3\\1\\1 \end {pmatrix} - \frac{(1 \times3)+(3\times4)+(1\times0)+(1\times0)}{(3\times3)+(4\times4)+(0\times0)+(0\times0)}\begin {pmatrix} 3\\4\\0\\0 \end {pmatrix}. w_2= \begin {pmatrix} -4/5\\3/5\\1\\1 \end {pmatrix}  
w_3= \begin {pmatrix} 0\\5\\5\\7 \end {pmatrix} - \frac{(5\times4)}{25}\begin {pmatrix} 3\\4\\0\\0 \end {pmatrix}- \frac{(0)+(3)+(5)+(7)}{(16/25)+(9/25)+1+1} \begin {pmatrix} -4/5\\3/5\\1\\1 \end {pmatrix} w_3= \begin {pmatrix} 8/5\\-6/5\\0\\2 \end {pmatrix},\ B'= \langle w_1,w_2,w_3\rangle.","['linear-algebra', 'solution-verification', 'orthogonality', 'gram-schmidt']"
32,How do I formally show that the Zariski tangent space of the intersection of two closed subschemes is the intersection of the tangent spaces?,How do I formally show that the Zariski tangent space of the intersection of two closed subschemes is the intersection of the tangent spaces?,,"I am looking for help in writing down the following formally/mathematically: Let $(A, \frak p)$ be a local ring with $I \subset \frak p$ and $J \subset \frak p$ . The Zariski cotangent space of $A/I$ at $\frak p$ can be identified with $\frac{\frak p/\frak p^2}{(I+\frak p^2)/\frak p^2}$ which tells us that the tangent space of $A/I$ at $\frak p$ is a subspace of the tangent space of $A$ at $\frak p$ . Similarly for the  closed subscheme cut out by $J$ . The cotangent space of $A/(I+J)$ at $\frak p$ can be identified with $\frac{\frak p/\frak p^2}{(I+J+\frak p^2)/\frak p^2}$ . Informally the tangent space of $A/I$ at $\frak p$ is cut out by the ideal $(I+\frak p^2)/\frak p^2$ , the tangent space of $A/J$ at $\frak p$ is cut out by the ideal $(J+\frak p^2)/\frak p^2$ , the tangent space of $A/(I+J)$ at $\frak p$ is cut out by the ideal $(I+J+\frak p^2)/\frak p^2$ . From this I gather that the intersection of the tangent spaces of $A/I$ and $A/J$ at $\frak p$ will be cut out by  the ideal $(I+J+\frak p^2)/\frak p^2$ ? Not sure how to really write this out. Therefore, the tangent space of the intersection $\operatorname{Spec}A/I  \cap \operatorname{Spec} A/J=\operatorname{Spec}A/(I+J)$ at $\frak p$ is the intersection of the tangent spaces of $\operatorname{Spec}A/I$ and $\operatorname{Spec}A/J$ at $\frak p$ .","I am looking for help in writing down the following formally/mathematically: Let be a local ring with and . The Zariski cotangent space of at can be identified with which tells us that the tangent space of at is a subspace of the tangent space of at . Similarly for the  closed subscheme cut out by . The cotangent space of at can be identified with . Informally the tangent space of at is cut out by the ideal , the tangent space of at is cut out by the ideal , the tangent space of at is cut out by the ideal . From this I gather that the intersection of the tangent spaces of and at will be cut out by  the ideal ? Not sure how to really write this out. Therefore, the tangent space of the intersection at is the intersection of the tangent spaces of and at .","(A, \frak p) I \subset \frak p J \subset \frak p A/I \frak p \frac{\frak p/\frak p^2}{(I+\frak p^2)/\frak p^2} A/I \frak p A \frak p J A/(I+J) \frak p \frac{\frak p/\frak p^2}{(I+J+\frak p^2)/\frak p^2} A/I \frak p (I+\frak p^2)/\frak p^2 A/J \frak p (J+\frak p^2)/\frak p^2 A/(I+J) \frak p (I+J+\frak p^2)/\frak p^2 A/I A/J \frak p (I+J+\frak p^2)/\frak p^2 \operatorname{Spec}A/I  \cap \operatorname{Spec} A/J=\operatorname{Spec}A/(I+J) \frak p \operatorname{Spec}A/I \operatorname{Spec}A/J \frak p","['linear-algebra', 'algebraic-geometry', 'commutative-algebra', 'proof-writing', 'solution-verification']"
33,Understanding determinant $=0$,Understanding determinant,=0,"I am playing around with determinants to see if I can get a better grasp of it and would appreciate some thoughts. $$A=\begin{pmatrix}a_1&b_1&c_1\\a_2&b_2&c_2\\a_3&b_3&c_3\end{pmatrix}$$ Column vectors $\vec{a},\vec{b},\vec{c}$ If det(A)=0, then $\vec{a},\vec{b},\vec{c}$ are linearly dependent and can be written $\alpha\vec{a}=\beta\vec{b}+\gamma\vec{c}$ . It also means that $A\vec{x}=0$ has a non trivial solution $$\vec{x_0}=\begin{pmatrix}\alpha\\-\beta\\-\gamma\end{pmatrix}$$ and all vectors $t\vec{x_0},\ t\in R$ Suppose we have an equation $A\vec{x}=\vec{y}$ , with $\vec{x_1}$ as a solution, then all $\vec{x}=\vec{x_1}+t\vec{x_0}$ are also solutions. This explanations makes it clear in my mind why, if there is one solution, there are infinite solutions in any dimension when det(A)=0. Suppose $\alpha\ne0$ , one of $\alpha,\beta,\gamma$ must be non zero. We could then describe $$A=\begin{pmatrix}\beta_1b_1+\gamma_1c_1&b_1&c_1\\\beta_1b_2+\gamma_1c_2&b_2&c_2\\\beta_1b_3+\gamma_3c_1&b_3&c_3\end{pmatrix}, \beta_1=\frac{\beta}{\alpha},\gamma_1=\frac{\gamma}{\alpha}$$ Then $$A\vec{x}=\vec{y}\Leftrightarrow$$ $$(\beta_1x_1+x_2)\vec{b}+(\gamma_1x_1+x_3)\vec{c}=\vec{y}$$ This is an equations system with two vectors in three dimensions describing a third, which can only lie in the plane the two vectors, $\vec{b},\vec{c}$ , span. This makes sense in two and three dimensions, but the thing I am having trouble visualising is why you need an equal number of linearly independent vectors to the number of the dimension to describe all vectors in higher than three dimensions. It makes sense, but in a ephemeral way that is not completely satisfying to me. When it comes to higher than three dimensions I start to view the equation $A\vec{x}=\vec{y}$ as an equations systems instead of geometric representations. So that is where my thoughts go. You need an equal number of equations to variables to have a possible single solution. But how to state this cleanly and intuitively? Edit: I thing the solution I am seeking is something like: if you have less vectors than the number of dimensions or if the vectors are linearly dependent, then you can not use Gaussian elimination to ever get an overly triangular matrix without zeros in the diagonal.","I am playing around with determinants to see if I can get a better grasp of it and would appreciate some thoughts. Column vectors If det(A)=0, then are linearly dependent and can be written . It also means that has a non trivial solution and all vectors Suppose we have an equation , with as a solution, then all are also solutions. This explanations makes it clear in my mind why, if there is one solution, there are infinite solutions in any dimension when det(A)=0. Suppose , one of must be non zero. We could then describe Then This is an equations system with two vectors in three dimensions describing a third, which can only lie in the plane the two vectors, , span. This makes sense in two and three dimensions, but the thing I am having trouble visualising is why you need an equal number of linearly independent vectors to the number of the dimension to describe all vectors in higher than three dimensions. It makes sense, but in a ephemeral way that is not completely satisfying to me. When it comes to higher than three dimensions I start to view the equation as an equations systems instead of geometric representations. So that is where my thoughts go. You need an equal number of equations to variables to have a possible single solution. But how to state this cleanly and intuitively? Edit: I thing the solution I am seeking is something like: if you have less vectors than the number of dimensions or if the vectors are linearly dependent, then you can not use Gaussian elimination to ever get an overly triangular matrix without zeros in the diagonal.","A=\begin{pmatrix}a_1&b_1&c_1\\a_2&b_2&c_2\\a_3&b_3&c_3\end{pmatrix} \vec{a},\vec{b},\vec{c} \vec{a},\vec{b},\vec{c} \alpha\vec{a}=\beta\vec{b}+\gamma\vec{c} A\vec{x}=0 \vec{x_0}=\begin{pmatrix}\alpha\\-\beta\\-\gamma\end{pmatrix} t\vec{x_0},\ t\in R A\vec{x}=\vec{y} \vec{x_1} \vec{x}=\vec{x_1}+t\vec{x_0} \alpha\ne0 \alpha,\beta,\gamma A=\begin{pmatrix}\beta_1b_1+\gamma_1c_1&b_1&c_1\\\beta_1b_2+\gamma_1c_2&b_2&c_2\\\beta_1b_3+\gamma_3c_1&b_3&c_3\end{pmatrix}, \beta_1=\frac{\beta}{\alpha},\gamma_1=\frac{\gamma}{\alpha} A\vec{x}=\vec{y}\Leftrightarrow (\beta_1x_1+x_2)\vec{b}+(\gamma_1x_1+x_3)\vec{c}=\vec{y} \vec{b},\vec{c} A\vec{x}=\vec{y}","['linear-algebra', 'matrices', 'vector-spaces', 'determinant']"
34,Orthogonal eigendecomposition of self-adjoint operator with indefinite scalar product,Orthogonal eigendecomposition of self-adjoint operator with indefinite scalar product,,"Let $V$ be a real vector space, of finite dimension $d$ , equipped with a nondegenerate symmetric bilinear form $q$ , and let $A$ be a linear map $V\to V$ that is self-adjoint with respect to $q$ , i.e., such that $q(A(x),y)=q(x,A(y))$ for all $x,y \in V$ . I know that, if $q$ is positive definite, then the classic spectral theorem applies; thus $A$ is diagonalizable and the eigenvectors can be chosen to be orthonormal. I also know that, without the assumption of positive definiteness, diagonalizability of $A$ is not guaranteed, as shown here . I would like to ask the following questions about the general case. Question 1 . Suppose that $A$ is diagonalizable. Is it then possible to choose an orthonormal basis of eigenvectors? Here by orthonormal I mean a basis $(v_{1},\dotsc,v_{d})$ such that $\lvert q(v_{i},v_{j}) \rvert = \delta_{ij}$ . Question 2 . Suppose that $F,G$ are two commuting linear maps on $V$ , i.e., $F(G(x))=G(F(x))$ for every $x\in V$ . If each of $F$ and $G$ has an orthonormal basis of eigenvectors, do they then share a common orthonormal basis of eigenvectors? Clearly, the answer to both questions is yes in the positive definite case.","Let be a real vector space, of finite dimension , equipped with a nondegenerate symmetric bilinear form , and let be a linear map that is self-adjoint with respect to , i.e., such that for all . I know that, if is positive definite, then the classic spectral theorem applies; thus is diagonalizable and the eigenvectors can be chosen to be orthonormal. I also know that, without the assumption of positive definiteness, diagonalizability of is not guaranteed, as shown here . I would like to ask the following questions about the general case. Question 1 . Suppose that is diagonalizable. Is it then possible to choose an orthonormal basis of eigenvectors? Here by orthonormal I mean a basis such that . Question 2 . Suppose that are two commuting linear maps on , i.e., for every . If each of and has an orthonormal basis of eigenvectors, do they then share a common orthonormal basis of eigenvectors? Clearly, the answer to both questions is yes in the positive definite case.","V d q A V\to V q q(A(x),y)=q(x,A(y)) x,y \in V q A A A (v_{1},\dotsc,v_{d}) \lvert q(v_{i},v_{j}) \rvert = \delta_{ij} F,G V F(G(x))=G(F(x)) x\in V F G","['linear-algebra', 'eigenvalues-eigenvectors']"
35,When do two triangles reflected over midpoints have the same area?,When do two triangles reflected over midpoints have the same area?,,"Suppose I have a triangle ABC. I have points C', A', and B' on segments AB, BC, and CA, respectively. Suppose I reflect C' about the midpoint of AB to get point C'' (also on AB); similarly for the other two points to get points A'' and B''. Is the area of A'B'C' the same as the area of A''B''C''? I tried some programming and it looks like it is. How can I prove this? I tried proving this showing that Area(AC'B')+Area(BA'C')+Area(CA'B') = Area(AC''B'')+Area(BA''C'')+Area(CA''B''). I did this by saying that AB has length c, BC has length A, and CA has length b. Then I decided to show that the sum of the areas of the three outside triangles of each triangle is the same. For instance the area of AC'B' is $1/2 (1/2 c - \delta_{B'})(1/2 b - \delta_{C'})\sin A$ , where $\delta_{B'}$ is the distance of $B'$ from the midpoint of AC. I wrote out all of the areas of the triangles like this, but I cannot show that they are the same.","Suppose I have a triangle ABC. I have points C', A', and B' on segments AB, BC, and CA, respectively. Suppose I reflect C' about the midpoint of AB to get point C'' (also on AB); similarly for the other two points to get points A'' and B''. Is the area of A'B'C' the same as the area of A''B''C''? I tried some programming and it looks like it is. How can I prove this? I tried proving this showing that Area(AC'B')+Area(BA'C')+Area(CA'B') = Area(AC''B'')+Area(BA''C'')+Area(CA''B''). I did this by saying that AB has length c, BC has length A, and CA has length b. Then I decided to show that the sum of the areas of the three outside triangles of each triangle is the same. For instance the area of AC'B' is , where is the distance of from the midpoint of AC. I wrote out all of the areas of the triangles like this, but I cannot show that they are the same.",1/2 (1/2 c - \delta_{B'})(1/2 b - \delta_{C'})\sin A \delta_{B'} B',"['linear-algebra', 'geometry', 'trigonometry', 'physics', 'hypothesis-testing']"
36,"Do there exist integers $a_1,a_2,\ldots,a_{n-1},a_n$ such that $a_i+a_{i+1}+a_{i+3}=1$?",Do there exist integers  such that ?,"a_1,a_2,\ldots,a_{n-1},a_n a_i+a_{i+1}+a_{i+3}=1","I am curious about the existence of a sequence of integers $a_1,a_2,\ldots,a_{n-1},a_n$ for some $n\ge1$ with the property that for all positive integers $i$ , $$a_i+a_{i+1}+a_{i+3}=1$$ where we compute indices modulo $n$ as necessary. My interest lies in the more general case of any finite weighted sum of indices relative to $i$ ; the case above is simply the first which seems nontrivial. (The motivation for this comes in thinking about ""tiling"" $\mathbb{Z}$ with finite weighted tiles, where we want to lay down an integer number of copies in each position: the existence of such a series of $a_i$ amounts to the existence of a periodic tiling.) It is easy to see that $n$ must be a multiple of three, by adding all congruences together; I can also rule out individual cases of $n=3$ or $n=6$ , but I don't see how to get all $n$ in general. It would suffice to show that the vectors $(1,1,0,1,0,\ldots,0)$ and its cyclic rotations are linearly independent over $\mathbb{R}$ , but I'm not sure how to do this either. As a remark, this is not true if we replace $(i,i+1,i+3)$ with $(i,i+1,i+5)$ , so the argument must hinge on the specific offsets somehow.","I am curious about the existence of a sequence of integers for some with the property that for all positive integers , where we compute indices modulo as necessary. My interest lies in the more general case of any finite weighted sum of indices relative to ; the case above is simply the first which seems nontrivial. (The motivation for this comes in thinking about ""tiling"" with finite weighted tiles, where we want to lay down an integer number of copies in each position: the existence of such a series of amounts to the existence of a periodic tiling.) It is easy to see that must be a multiple of three, by adding all congruences together; I can also rule out individual cases of or , but I don't see how to get all in general. It would suffice to show that the vectors and its cyclic rotations are linearly independent over , but I'm not sure how to do this either. As a remark, this is not true if we replace with , so the argument must hinge on the specific offsets somehow.","a_1,a_2,\ldots,a_{n-1},a_n n\ge1 i a_i+a_{i+1}+a_{i+3}=1 n i \mathbb{Z} a_i n n=3 n=6 n (1,1,0,1,0,\ldots,0) \mathbb{R} (i,i+1,i+3) (i,i+1,i+5)","['linear-algebra', 'elementary-number-theory']"
37,Tensor product and dual of $k$-vector spaces over a $k$-algebra: what is the dimension?,Tensor product and dual of -vector spaces over a -algebra: what is the dimension?,k k,"Here is the setting: let $k$ be a field, $V$ a finite dimensional $k$ -vector space with $k$ -dual $V^*$ , $E$ a $k$ -subalgebra of $\mathrm{End}_k(V)$ . $V$ has thus a canonical structure of faithful left $E$ -module. The object that we are interested in is $$ V \otimes_E V^* = V \otimes_k V^* / \langle \{f(v) \otimes \eta - v \otimes f^*(\eta) \mid f \in E, v \in V, \eta \in V^*\} \rangle,$$ where $f^*: \eta \mapsto \eta \circ f$ is the canonical faithful right action of $E$ on $V^*$ . If $V$ is free as an $E$ -module, then so is $V^*$ , and they have the same $E$ -dimension, so that $$ \dim_k(V \otimes_E V^*) = \dim_E(V)^2 \cdot \dim_k(E) = \frac{\dim_k(V)^2}{\dim_k(E)}.$$ If $V$ is not $E$ -free, then the central expression is not defined, but the first and the last one are. Which leads to the question: Question 1: is it always true that $\dim_k(V \otimes_E V^*) =\frac{\dim_k(V)^2}{\dim_k(E)}$ ? If not, does this at least give an upper or a lower bound? Remark: More in general, one could ask whether, given two finite-dimensional $E$ -modules (with $E$ a finite-dimensional $k$ -algebra), we always have $$\dim_k(V \otimes_E W) =\frac{\dim_k(V) \cdot \dim_k(W)}{\dim_k(E)}.$$ This, however, is false , see for instance here for an example. However, maybe the fact that in my case the action of $E$ is faithful and $W=V^*$ are somehow of help... So far for the tensor product. My second question, which is probably closely related, concerns the dual: Question 2: Consider the dual of $V$ as an $E$ -module, i.e., $V^\circ:= \mathrm{Hom}_E(V,E)$ . What is its relationship with $V^*$ ?","Here is the setting: let be a field, a finite dimensional -vector space with -dual , a -subalgebra of . has thus a canonical structure of faithful left -module. The object that we are interested in is where is the canonical faithful right action of on . If is free as an -module, then so is , and they have the same -dimension, so that If is not -free, then the central expression is not defined, but the first and the last one are. Which leads to the question: Question 1: is it always true that ? If not, does this at least give an upper or a lower bound? Remark: More in general, one could ask whether, given two finite-dimensional -modules (with a finite-dimensional -algebra), we always have This, however, is false , see for instance here for an example. However, maybe the fact that in my case the action of is faithful and are somehow of help... So far for the tensor product. My second question, which is probably closely related, concerns the dual: Question 2: Consider the dual of as an -module, i.e., . What is its relationship with ?","k V k k V^* E k \mathrm{End}_k(V) V E  V \otimes_E V^* = V \otimes_k V^* / \langle \{f(v) \otimes \eta - v \otimes f^*(\eta) \mid f \in E, v \in V, \eta \in V^*\} \rangle, f^*: \eta \mapsto \eta \circ f E V^* V E V^* E  \dim_k(V \otimes_E V^*) = \dim_E(V)^2 \cdot \dim_k(E) = \frac{\dim_k(V)^2}{\dim_k(E)}. V E \dim_k(V \otimes_E V^*) =\frac{\dim_k(V)^2}{\dim_k(E)} E E k \dim_k(V \otimes_E W) =\frac{\dim_k(V) \cdot \dim_k(W)}{\dim_k(E)}. E W=V^* V E V^\circ:= \mathrm{Hom}_E(V,E) V^*","['linear-algebra', 'abstract-algebra', 'modules', 'tensor-products']"
38,Invariant subspace of $T$ (normal) is also an invariant subspace of $T^\ast$.,Invariant subspace of  (normal) is also an invariant subspace of .,T T^\ast,"I am struggling with the following question Let $V$ be a finite dimensional vector space and $T:V\rightarrow V$ be a linear normal operator ( $T^\ast T = TT^\ast$ ), and $W$ an invariant subspace of $T$ ( $T(W)\subseteq W)$ . Prove $W$ is also an invariant subspace of $T^\ast$ . The problem I have is characterizing something like $T^\ast w \in W$ when all that is given is in the language of inner products. I thought of maybe decomposing $T^\ast w = u+v$ where $u \in W,\ v\in W^\perp$ , and showing $v=0$ by $\left <T^\ast w, v \right >=0$ , but $T$ being normal does not help when there is ""only one $T$ "" inside the inner product. Is multiplying both sides by $T$ any help? because then we can use normality but I don't know where it leads us. I know this is true since using the unitary diagonalization, I can express $T^\ast$ as a polynomial in $T$ and from there it's easy ( $W$ is $p(T)$ invariant regardless of the polynomial itself), but I would like to see a more fundamental solution.","I am struggling with the following question Let be a finite dimensional vector space and be a linear normal operator ( ), and an invariant subspace of ( . Prove is also an invariant subspace of . The problem I have is characterizing something like when all that is given is in the language of inner products. I thought of maybe decomposing where , and showing by , but being normal does not help when there is ""only one "" inside the inner product. Is multiplying both sides by any help? because then we can use normality but I don't know where it leads us. I know this is true since using the unitary diagonalization, I can express as a polynomial in and from there it's easy ( is invariant regardless of the polynomial itself), but I would like to see a more fundamental solution.","V T:V\rightarrow V T^\ast T = TT^\ast W T T(W)\subseteq W) W T^\ast T^\ast w \in W T^\ast w = u+v u \in W,\ v\in W^\perp v=0 \left <T^\ast w, v \right >=0 T T T T^\ast T W p(T)","['linear-algebra', 'inner-products', 'invariant-subspace', 'normal-operator']"
39,Does eigenvalues of matrix change after multiplication by unitary matrix?,Does eigenvalues of matrix change after multiplication by unitary matrix?,,"For a matrix $A \in \mathbb{C}^{n \times n}$ , does multiplication by a unitary matrix $U$ change the eigenvalues of $A$ ? So for: $$Ax = \lambda x \qquad \mathrm{and} \qquad AUy = \mu y $$ does $\lambda = \mu$ for some $x,y \in \mathbb{C}^n$ ? I know the above is true for doing left and right multiplication by $U$ : $$ UAU^*y = \mu y \\  AU^*y = \mu U^* y \\ Az = \mu z \\ \therefore \mu = \lambda$$ (defining $z = U^* y$ ) Under the guise that unitary matrices are simply rotations, it logically makes sense to me that $\mu$ and $\lambda$ should be identical, and only the the eigenvectors should be different. The statement is true for singular values (see here ), but I'm having trouble proving it for eigenvalues (if it even is true). Edit After a quick example in python, I understand that the above is not true. So instead: where is my thought process going wrong with regard to how unitary matrices/rotations effect eigenvalues? Edit 2 What I was really going for, but did not state correctly was that: $$AX = \Lambda X \qquad \mathrm{and} \qquad AUY = MY \\$$ such that $ \lambda \in M, \forall \lambda \in \Lambda$ , where $M$ and $\Lambda$ are diagonal matrices.","For a matrix , does multiplication by a unitary matrix change the eigenvalues of ? So for: does for some ? I know the above is true for doing left and right multiplication by : (defining ) Under the guise that unitary matrices are simply rotations, it logically makes sense to me that and should be identical, and only the the eigenvectors should be different. The statement is true for singular values (see here ), but I'm having trouble proving it for eigenvalues (if it even is true). Edit After a quick example in python, I understand that the above is not true. So instead: where is my thought process going wrong with regard to how unitary matrices/rotations effect eigenvalues? Edit 2 What I was really going for, but did not state correctly was that: such that , where and are diagonal matrices.","A \in \mathbb{C}^{n \times n} U A Ax = \lambda x \qquad \mathrm{and} \qquad AUy = \mu y  \lambda = \mu x,y \in \mathbb{C}^n U  UAU^*y = \mu y \\ 
AU^*y = \mu U^* y \\
Az = \mu z \\
\therefore \mu = \lambda z = U^* y \mu \lambda AX = \Lambda X \qquad \mathrm{and} \qquad AUY = MY \\  \lambda \in M, \forall \lambda \in \Lambda M \Lambda","['linear-algebra', 'eigenvalues-eigenvectors']"
40,Applying the Normal Equations to solve the Linear Regression Problems.,Applying the Normal Equations to solve the Linear Regression Problems.,,"I am new to machine learning and I am currently studying the gradient descent method and its application for linear regression. An iterative method known as gradient descent is finding the linear function: $$ J(\theta)=\underset{\theta}{\operatorname{argmin}}\frac{1}{2}\sum_{i=1}^{n}\left(h_{\theta}(x^{(i)})-y^{(i)}\right) \tag1$$ However, I came to notice of an explicit non-iterative scheme in $\text{Andrew Ng's}$ lecture notes right here: https://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf for which he concluded in page $11$ that the gradient descent is now equivalent to finding the vector $\theta$ where: $$ \theta=(X^{T}X)^{-1}X^{T}y \tag2$$ Terminology: $\theta$ are the parameters. $n$ are number of training examples. $x$ are input features and $y$ are output variables. $(x^{(i)},y^{(i)})$ is the $i^{th}$ training example. $X\in\mathbb{R}^{m\times n}$ is the design matrix where $\mathbf{X}=\begin{bmatrix} --(x^{(1)})^{T}-- \\ --(x^{(2)})^{T}-- \\ \vdots\\ --(x^{(n)})^{T}-- \end{bmatrix} $ I have two questions. $(1)$ : Say I have $(1,2)$ , $(2,1.5)$ , and $(3,2.5)$ I wish someone can demonstrate this procedure to find $\theta$ by solving $\theta=(X^{T}X)^{-1}X^{T}y$ $(2)$ : I would really hope if a Python,MATLAB, or C $++$ algorithm for this procedure can be provided.","I am new to machine learning and I am currently studying the gradient descent method and its application for linear regression. An iterative method known as gradient descent is finding the linear function: However, I came to notice of an explicit non-iterative scheme in lecture notes right here: https://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf for which he concluded in page that the gradient descent is now equivalent to finding the vector where: Terminology: are the parameters. are number of training examples. are input features and are output variables. is the training example. is the design matrix where I have two questions. : Say I have , , and I wish someone can demonstrate this procedure to find by solving : I would really hope if a Python,MATLAB, or C algorithm for this procedure can be provided.","
J(\theta)=\underset{\theta}{\operatorname{argmin}}\frac{1}{2}\sum_{i=1}^{n}\left(h_{\theta}(x^{(i)})-y^{(i)}\right)
\tag1 \text{Andrew Ng's} 11 \theta 
\theta=(X^{T}X)^{-1}X^{T}y
\tag2 \theta n x y (x^{(i)},y^{(i)}) i^{th} X\in\mathbb{R}^{m\times n} \mathbf{X}=\begin{bmatrix}
--(x^{(1)})^{T}-- \\
--(x^{(2)})^{T}-- \\
\vdots\\
--(x^{(n)})^{T}--
\end{bmatrix}  (1) (1,2) (2,1.5) (3,2.5) \theta \theta=(X^{T}X)^{-1}X^{T}y (2) ++","['linear-algebra', 'algorithms', 'linear-regression', 'gradient-descent']"
41,Is this true: adding a random DIAGONAL perturbation can make a nondiagonalizable matrix diagonalizable?,Is this true: adding a random DIAGONAL perturbation can make a nondiagonalizable matrix diagonalizable?,,"For any nondiagonalizable matrix $A\in\mathbb{R}^{n\times n}$ , let $\hat{A}=A+E$ where $E$ is a small random DIAGONAL matrix (e.g. diagonal entries of $E$ are i.i.d. sampled from $N(0,\epsilon^2)$ for arbitrarilly small $\epsilon>0$ ). Can we conclude that $\hat{A}$ is diagonalizable with probability $1$ (Here $A$ is fixed and the randomness is only from $E$ )? My question is similar to this question , but here the perturbation matrix is required to be diagonal.","For any nondiagonalizable matrix , let where is a small random DIAGONAL matrix (e.g. diagonal entries of are i.i.d. sampled from for arbitrarilly small ). Can we conclude that is diagonalizable with probability (Here is fixed and the randomness is only from )? My question is similar to this question , but here the perturbation matrix is required to be diagonal.","A\in\mathbb{R}^{n\times n} \hat{A}=A+E E E N(0,\epsilon^2) \epsilon>0 \hat{A} 1 A E","['linear-algebra', 'probability', 'matrices']"
42,"A matrix with all entries being positive, each row has the same sum.","A matrix with all entries being positive, each row has the same sum.",,"A matrix $A$ with all entries being positive, each row has the same sum, that is, $\sum_j a_{ij}=c>0$ . Show that up to constant multiplier, the eigenvector $(1,\cdots,1)'$ is the unique eigenvector with all entries being positive. Let $x=(x_1,\cdots,x_n)'$ being the eigenvector of $A$ , with eigenvalue $\mu>0$ , $x_i$ being not identically the same, How to derive a contradiction?","A matrix with all entries being positive, each row has the same sum, that is, . Show that up to constant multiplier, the eigenvector is the unique eigenvector with all entries being positive. Let being the eigenvector of , with eigenvalue , being not identically the same, How to derive a contradiction?","A \sum_j a_{ij}=c>0 (1,\cdots,1)' x=(x_1,\cdots,x_n)' A \mu>0 x_i","['linear-algebra', 'eigenvalues-eigenvectors']"
43,"Linear algebra: proof of sufficient conditions for $x,Ax,A^{2}x,\dots,$ being linearly independent",Linear algebra: proof of sufficient conditions for  being linearly independent,"x,Ax,A^{2}x,\dots,","This questsion is an extension of a previously asked question that I think needs a more rigorous proof (please see the link and the answer by user Aaron). The question is, given a little bit of different notation (in accordance with control theory), for $b\in\mathbb{R}^{n}$ and $A\in\mathbb{R}^{n\times n}$ , what is a sufficient condition for \begin{align} b,Ab,A^{2}b,\dots,A^{n-1}b \end{align} all being linearly independent? Furthermore, if we assume that $A$ has $n$ distinct eigenvalues $\lambda_{i}$ , $i=1,\dots,n$ , is this condition sufficient for the above to hold and if so how do we prove it? In the above linked question Aaron's answer was: If the characteristic polynomial of $A$ has no repeated roots, then we have a basis $\{v_{i}\;\vert\;1\leq i\leq n\}$ of eigenvectors for $\mathbb{C}^{n}$ , and we can express $x=\sum \alpha_{i}v_{i}$ in terms of the basis. If all of the $\alpha_{i}$ are non-zero, then all of your vectors will be linearly independent. In fact, with this condition on $A$ , this is a necessary and sufficient condition on $x$ . If $A$ has an eigenvalue with geometric multiplicity greater than 1, then no $x$ will work. If $A$ has repeated eigenvalues but all eigenvalues have geometric multiplicity of 1, then it is still possible to find such an $x$ , but things are a bit more complicated. I would be very happy if someone could fill in the blanks for his statements, it is not immediately evident that a $b$ with nonzero entries leads to all vectors $b,Ab,\dots,A^{n-1}b$ being linearly independent.","This questsion is an extension of a previously asked question that I think needs a more rigorous proof (please see the link and the answer by user Aaron). The question is, given a little bit of different notation (in accordance with control theory), for and , what is a sufficient condition for all being linearly independent? Furthermore, if we assume that has distinct eigenvalues , , is this condition sufficient for the above to hold and if so how do we prove it? In the above linked question Aaron's answer was: If the characteristic polynomial of has no repeated roots, then we have a basis of eigenvectors for , and we can express in terms of the basis. If all of the are non-zero, then all of your vectors will be linearly independent. In fact, with this condition on , this is a necessary and sufficient condition on . If has an eigenvalue with geometric multiplicity greater than 1, then no will work. If has repeated eigenvalues but all eigenvalues have geometric multiplicity of 1, then it is still possible to find such an , but things are a bit more complicated. I would be very happy if someone could fill in the blanks for his statements, it is not immediately evident that a with nonzero entries leads to all vectors being linearly independent.","b\in\mathbb{R}^{n} A\in\mathbb{R}^{n\times n} \begin{align}
b,Ab,A^{2}b,\dots,A^{n-1}b
\end{align} A n \lambda_{i} i=1,\dots,n A \{v_{i}\;\vert\;1\leq i\leq n\} \mathbb{C}^{n} x=\sum \alpha_{i}v_{i} \alpha_{i} A x A x A x b b,Ab,\dots,A^{n-1}b","['linear-algebra', 'control-theory']"
44,Kernels of commuting linear operators on infinite dimensional vector space,Kernels of commuting linear operators on infinite dimensional vector space,,"If $S$ and $T$ are commuting operators on an infinite dimensional vector space $V$ , it is in general true that $$\ker S + \ker T \subseteq \ker(ST),$$ but in general equality does not hold. A simple example is given by $S = T = \frac{d}{dx}$ on $C^\infty(\mathbb{R})$ . I am looking for conditions on $S$ and $T$ that will give equality in the above equation, ie: $$\ker S + \ker T = \ker (ST)$$ Writing $\ker T^\infty$ for $\cup_n \ker T^n$ , I am currently trying to show that the conditions $\mathrm{im} S = \mathrm{im} T = V$ , $\ker S^\infty \cap \ker T^\infty = \{ 0 \}$ , $\dim \ker S < \infty$ and $\dim \ker T < \infty$ , $ST = TS$ imply that $\ker S + \ker T = \ker(ST)$ . I think the second condition can be weakened to $\ker S^2 \cap \ker T^2 = \{ 0 \}$ , but I have this stronger condition for some operators I am interested in. Any help would be appreciated, thanks. -edit- I am not confident that all these conditions are necessary.","If and are commuting operators on an infinite dimensional vector space , it is in general true that but in general equality does not hold. A simple example is given by on . I am looking for conditions on and that will give equality in the above equation, ie: Writing for , I am currently trying to show that the conditions , , and , imply that . I think the second condition can be weakened to , but I have this stronger condition for some operators I am interested in. Any help would be appreciated, thanks. -edit- I am not confident that all these conditions are necessary.","S T V \ker S + \ker T \subseteq \ker(ST), S = T = \frac{d}{dx} C^\infty(\mathbb{R}) S T \ker S + \ker T = \ker (ST) \ker T^\infty \cup_n \ker T^n \mathrm{im} S = \mathrm{im} T = V \ker S^\infty \cap \ker T^\infty = \{ 0 \} \dim \ker S < \infty \dim \ker T < \infty ST = TS \ker S + \ker T = \ker(ST) \ker S^2 \cap \ker T^2 = \{ 0 \}",['linear-algebra']
45,Which linear maps on a finite field are field multiplications?,Which linear maps on a finite field are field multiplications?,,"I am mainly interested in the fields $\mathrm{GF}(2^n)$ , but the question can be asked for any prime. We can write out each element $x\in\mathrm{GF}(2^n)$ in base $2$ and note that its additive group combined with multiplication by elements of $\mathrm{GF}(2)$ is isomorphic to the vector space $\left(\mathbb{Z}/(2\mathbb{Z})\right)^n$ . Let $v:\mathrm{GF}(2^n)\to\left(\mathbb{Z}/(2\mathbb{Z})\right)^n$ stand for this ""vectorisation"" operation. Linear maps on $\left(\mathbb{Z}/(2\mathbb{Z})\right)^n$ may be represented by $n\times n$ , $\{0,1\}$ -valued matrices. Since field multiplication is linear for any $x\in\mathrm{GF}(2^n)$ there is a matrix $M_x$ such that for all $y\in\mathrm{GF}(2^n)$ \begin{align}     M_x v(y) = v(x\cdot y), \end{align} There are, however $2^{n\times n}$ matrices and only $2^{n}$ field elements, so the question is what can we say about the structure of the set of matrices $\{M_x \mid x\in \mathrm{GF}(2^n)\}$ as a subset of the full set of matrices? Loosely speaking - if I give you a matrix then how can you tell if it represents a field element?","I am mainly interested in the fields , but the question can be asked for any prime. We can write out each element in base and note that its additive group combined with multiplication by elements of is isomorphic to the vector space . Let stand for this ""vectorisation"" operation. Linear maps on may be represented by , -valued matrices. Since field multiplication is linear for any there is a matrix such that for all There are, however matrices and only field elements, so the question is what can we say about the structure of the set of matrices as a subset of the full set of matrices? Loosely speaking - if I give you a matrix then how can you tell if it represents a field element?","\mathrm{GF}(2^n) x\in\mathrm{GF}(2^n) 2 \mathrm{GF}(2) \left(\mathbb{Z}/(2\mathbb{Z})\right)^n v:\mathrm{GF}(2^n)\to\left(\mathbb{Z}/(2\mathbb{Z})\right)^n \left(\mathbb{Z}/(2\mathbb{Z})\right)^n n\times n \{0,1\} x\in\mathrm{GF}(2^n) M_x y\in\mathrm{GF}(2^n) \begin{align}
    M_x v(y) = v(x\cdot y),
\end{align} 2^{n\times n} 2^{n} \{M_x \mid x\in \mathrm{GF}(2^n)\}","['linear-algebra', 'abstract-algebra', 'vector-spaces', 'finite-fields']"
46,Product of block matrices,Product of block matrices,,"Let $A$ be a $(n\times k)$ -matrix, $B$ a $(n\times d)$ -matrix and $M=[A \quad B]$ the block matrix (or the augmented matrix). Doing the calculations, I obtained that $$M'M= \left ( \begin{array}{cc} A'A & A'B\\ B'A & B'B \end{array}\right ), $$ where $'$ stands for transposition. Do you have a good justification for this result rather than observe it by brute force?","Let be a -matrix, a -matrix and the block matrix (or the augmented matrix). Doing the calculations, I obtained that where stands for transposition. Do you have a good justification for this result rather than observe it by brute force?","A (n\times k) B (n\times d) M=[A \quad B] M'M=
\left ( \begin{array}{cc}
A'A & A'B\\
B'A & B'B
\end{array}\right ),
 '","['linear-algebra', 'matrices', 'block-matrices']"
47,How to do eigendecomposition on giant dense PSD matrices?,How to do eigendecomposition on giant dense PSD matrices?,,"I need to perform eigendecomposition on giant matrices (at least with dimensions of 600K by 600K). I need both eigenvalues and eigenvectors, however, only top k of them, e.g. k=100. In addition, the matrices are known to be positive definite (if that helps in any way). However, unfortunately the matrices are dense and thus sparsity-based approaches do not apply. I think the only hope is to perform some form of random sampling of the giant matrix (e.g. rows or columns, or small sub-matrices), then do some computation on it, and repeat this in a loop in a way that each iteration can progressively produce a better estimate of the top k eigenvalues and eigenvectors. Is this possible? If so, could you please advise me about the method? Thank you! Golabi","I need to perform eigendecomposition on giant matrices (at least with dimensions of 600K by 600K). I need both eigenvalues and eigenvectors, however, only top k of them, e.g. k=100. In addition, the matrices are known to be positive definite (if that helps in any way). However, unfortunately the matrices are dense and thus sparsity-based approaches do not apply. I think the only hope is to perform some form of random sampling of the giant matrix (e.g. rows or columns, or small sub-matrices), then do some computation on it, and repeat this in a loop in a way that each iteration can progressively produce a better estimate of the top k eigenvalues and eigenvectors. Is this possible? If so, could you please advise me about the method? Thank you! Golabi",,"['linear-algebra', 'numerical-methods', 'numerical-linear-algebra']"
48,Bourbaki's construction of generalized tensor product of modules,Bourbaki's construction of generalized tensor product of modules,,"Let $(G_\lambda)_{\lambda\in L}$ be a family of $\mathbf{Z}$ -modules. Let $\phi:\prod_{\lambda\in L}G_\lambda\rightarrow\mathbf{Z}^{(\prod_{\lambda\in L} G_\lambda)},\,x\mapsto e_x:=(\delta_x(x'))_{x'\in\prod_\lambda G_\lambda}$ , be the canonical injection. Let $C$ be the sub- $\mathbf{Z}$ -module of $\mathbf{Z}^{(\prod_{\lambda\in L} G_\lambda)}$ generated by elements of the form $$e_{x+y,(z_\lambda)_{\lambda\ne\mu}}-e_{x,(z_\lambda)_{\lambda\ne\mu}}-e_{y,(z_\lambda)_{\lambda\ne\mu}}$$ for $\mu\in L$ , $x,y\in G_\mu$ , and $z\in\prod_{\lambda\ne\mu}G_\lambda$ . Write $\bigotimes_{\lambda\in L}G_{\lambda}:=\mathbf{Z}^{(\prod_{\lambda\in L} G_\lambda)}/C$ and let $\pi:\mathbf{Z}^{(\prod_{\lambda\in L} G_\lambda)}\rightarrow\bigotimes_{\lambda\in L}G_{\lambda}$ be the canonical surjection. Then the mapping $$\pi\circ\phi:\prod_{\lambda\in L}G_\lambda\rightarrow\bigotimes_{\lambda\in L}G_\lambda$$ is $\mathbf{Z}$ -multilinear ( definition ). The $\mathbf{Z}$ -module is called the tensor product (over $\mathbf{Z}$ ) of the family $(G_\lambda)_{\lambda\in L}$ of $\mathbf{Z}$ -modules . For $x\in\prod_{\lambda\in L} G_\lambda$ , write $\bigotimes_{\lambda\in L}x_\lambda:=\pi(\phi(x))$ . Let $(H_\lambda)_{\lambda\in L}$ be another family of $\mathbf{Z}$ -modules and $(v_\lambda:G_\lambda\rightarrow H_\lambda)_{\lambda\in L}$ a family of $\mathbf{Z}$ -linear mappings. Then there exists a unique $\mathbf{Z}$ -linear mapping $$\bigotimes_{\lambda\in L}v_\lambda:\bigotimes_{\lambda\in L}G_\lambda\rightarrow\bigotimes_{\lambda\in L}H_\lambda$$ such that $\left(\bigotimes_{\lambda\in L}v_\lambda\right)\left(\bigotimes_{\lambda\in L}x_\lambda\right)=\bigotimes_{\lambda\in L}v_{\lambda}(x_{\lambda})$ for all $x\in\prod_{\lambda\in L}G_\lambda$ . In particular, let $\mu\in L$ and $\theta$ be an endomorphism of $G_\mu$ . We denote by $\tilde{\theta}$ the endomorphism of $\bigotimes_{\lambda\in L}G_\lambda$ equal to $\bigotimes_{\lambda\in L}v'_{\lambda}$ where $v'_\mu=\theta$ and $v'_\lambda=1_{G_\lambda}$ for $\lambda\ne\mu$ . Now, suppose we are given a set $\Omega$ , a mapping $c:\Omega\rightarrow L\times L,\,\omega\mapsto(\rho(\omega),\sigma(\omega))$ and, for all $\omega\in\Omega$ , an endomorphism $p_\omega$ of $G_{\rho(\omega)}$ and an endomorphism $q_\omega$ of $G_{\sigma(\omega)}$ ; there correspond to them two endomorphisms $\tilde{p}_\omega$ and $\tilde{q}_\omega$ of $\bigotimes_{\lambda\in L}G_\lambda$ . Set $$\bigotimes_{(c,p,q)}G_\lambda:=\left(\bigotimes_{\lambda\in L}G_\lambda\right)/\left(\sum_{\omega\in\Omega}\text{Im}(\tilde{p}_\omega-\tilde{q}_\omega)\right)$$ and let $\psi:\bigotimes_{\lambda\in L}G_\lambda\rightarrow\bigotimes_{(c,p,q)}G_\lambda$ be the canonical surjection. Then the mapping $$\varphi_{(c,p,q)}:=\psi\circ\pi\circ\phi:\prod_{\lambda\in L}G_\lambda\rightarrow\bigotimes_{(c,p,q)}G_\lambda$$ is $\mathbf{Z}$ -multilinear. Ok.–This construction seems rather involved, especially if you take into account that one still has to develop the notions of ""associativity"" and ""commutativity""..Is there a way to clean this up? (Perhaps using some category theory?) If not, can someone suggest alternative constructions that don't sacrifice generality (e.g. by restricting to finite cases, etc.)?","Let be a family of -modules. Let , be the canonical injection. Let be the sub- -module of generated by elements of the form for , , and . Write and let be the canonical surjection. Then the mapping is -multilinear ( definition ). The -module is called the tensor product (over ) of the family of -modules . For , write . Let be another family of -modules and a family of -linear mappings. Then there exists a unique -linear mapping such that for all . In particular, let and be an endomorphism of . We denote by the endomorphism of equal to where and for . Now, suppose we are given a set , a mapping and, for all , an endomorphism of and an endomorphism of ; there correspond to them two endomorphisms and of . Set and let be the canonical surjection. Then the mapping is -multilinear. Ok.–This construction seems rather involved, especially if you take into account that one still has to develop the notions of ""associativity"" and ""commutativity""..Is there a way to clean this up? (Perhaps using some category theory?) If not, can someone suggest alternative constructions that don't sacrifice generality (e.g. by restricting to finite cases, etc.)?","(G_\lambda)_{\lambda\in L} \mathbf{Z} \phi:\prod_{\lambda\in L}G_\lambda\rightarrow\mathbf{Z}^{(\prod_{\lambda\in L} G_\lambda)},\,x\mapsto e_x:=(\delta_x(x'))_{x'\in\prod_\lambda G_\lambda} C \mathbf{Z} \mathbf{Z}^{(\prod_{\lambda\in L} G_\lambda)} e_{x+y,(z_\lambda)_{\lambda\ne\mu}}-e_{x,(z_\lambda)_{\lambda\ne\mu}}-e_{y,(z_\lambda)_{\lambda\ne\mu}} \mu\in L x,y\in G_\mu z\in\prod_{\lambda\ne\mu}G_\lambda \bigotimes_{\lambda\in L}G_{\lambda}:=\mathbf{Z}^{(\prod_{\lambda\in L} G_\lambda)}/C \pi:\mathbf{Z}^{(\prod_{\lambda\in L} G_\lambda)}\rightarrow\bigotimes_{\lambda\in L}G_{\lambda} \pi\circ\phi:\prod_{\lambda\in L}G_\lambda\rightarrow\bigotimes_{\lambda\in L}G_\lambda \mathbf{Z} \mathbf{Z} \mathbf{Z} (G_\lambda)_{\lambda\in L} \mathbf{Z} x\in\prod_{\lambda\in L} G_\lambda \bigotimes_{\lambda\in L}x_\lambda:=\pi(\phi(x)) (H_\lambda)_{\lambda\in L} \mathbf{Z} (v_\lambda:G_\lambda\rightarrow H_\lambda)_{\lambda\in L} \mathbf{Z} \mathbf{Z} \bigotimes_{\lambda\in L}v_\lambda:\bigotimes_{\lambda\in L}G_\lambda\rightarrow\bigotimes_{\lambda\in L}H_\lambda \left(\bigotimes_{\lambda\in L}v_\lambda\right)\left(\bigotimes_{\lambda\in L}x_\lambda\right)=\bigotimes_{\lambda\in L}v_{\lambda}(x_{\lambda}) x\in\prod_{\lambda\in L}G_\lambda \mu\in L \theta G_\mu \tilde{\theta} \bigotimes_{\lambda\in L}G_\lambda \bigotimes_{\lambda\in L}v'_{\lambda} v'_\mu=\theta v'_\lambda=1_{G_\lambda} \lambda\ne\mu \Omega c:\Omega\rightarrow L\times L,\,\omega\mapsto(\rho(\omega),\sigma(\omega)) \omega\in\Omega p_\omega G_{\rho(\omega)} q_\omega G_{\sigma(\omega)} \tilde{p}_\omega \tilde{q}_\omega \bigotimes_{\lambda\in L}G_\lambda \bigotimes_{(c,p,q)}G_\lambda:=\left(\bigotimes_{\lambda\in L}G_\lambda\right)/\left(\sum_{\omega\in\Omega}\text{Im}(\tilde{p}_\omega-\tilde{q}_\omega)\right) \psi:\bigotimes_{\lambda\in L}G_\lambda\rightarrow\bigotimes_{(c,p,q)}G_\lambda \varphi_{(c,p,q)}:=\psi\circ\pi\circ\phi:\prod_{\lambda\in L}G_\lambda\rightarrow\bigotimes_{(c,p,q)}G_\lambda \mathbf{Z}","['linear-algebra', 'abstract-algebra', 'category-theory', 'tensor-products', 'multilinear-algebra']"
49,Are 4x4 matrices useful in 3D only because of translation?,Are 4x4 matrices useful in 3D only because of translation?,,"Having interest in 3D computer graphics, I've stumbled upon four dimensional matrices. After a bit of research, I've found out that this was a trick to represent translations, but no more than a trick, which doesn't seem very satisfaying, because of the fourth components of any vector always being one. Is there any other more fundamental reason for the use of a 4D matrix in 3D? I'm not asking particularly in the context of computer graphics since this is a math forum.","Having interest in 3D computer graphics, I've stumbled upon four dimensional matrices. After a bit of research, I've found out that this was a trick to represent translations, but no more than a trick, which doesn't seem very satisfaying, because of the fourth components of any vector always being one. Is there any other more fundamental reason for the use of a 4D matrix in 3D? I'm not asking particularly in the context of computer graphics since this is a math forum.",,"['linear-algebra', 'geometry', '3d']"
50,Prove that $W$ is $T$-invariant if and only if $W^0$ is $T^t$-invariant.,Prove that  is -invariant if and only if  is -invariant.,W T W^0 T^t,"Let $T$ be a linear operator on a finite-dimensional vector space $V$ , and let $W$ be a subspace of $V$ . Let $W^0 \subset V^*$ be the annihilator of $W$ . Prove that $W$ is $T$ -invariant if and only if $W^0$ is $T^t$ -invariant. A hint is appreciated for the converse direction.","Let be a linear operator on a finite-dimensional vector space , and let be a subspace of . Let be the annihilator of . Prove that is -invariant if and only if is -invariant. A hint is appreciated for the converse direction.",T V W V W^0 \subset V^* W W T W^0 T^t,"['linear-algebra', 'dual-spaces']"
51,Frobenius norm and operator norm inequality,Frobenius norm and operator norm inequality,,"Let $A$ be a $k\times m$ matrix and B be a $m\times n$ matrix, I wonder how to prove the following inequality $$\|AB\|_F\le\|A\| \|B\|_F,$$ where $\|\cdot\|_F$ is the Frobenius norm (square root of the sum of all squared entries and $\|\cdot\|$ is the 2-operator norm ) Note if $n=1$ , i.e when $B$ is a column vector, this just follows from the definition of the operator norm. But I don't know how to deal with the general case. I have thought about using SVD of $A,B$ but don't know how to simplify the LHS. Any approach will be appreciated!","Let be a matrix and B be a matrix, I wonder how to prove the following inequality where is the Frobenius norm (square root of the sum of all squared entries and is the 2-operator norm ) Note if , i.e when is a column vector, this just follows from the definition of the operator norm. But I don't know how to deal with the general case. I have thought about using SVD of but don't know how to simplify the LHS. Any approach will be appreciated!","A k\times m m\times n \|AB\|_F\le\|A\| \|B\|_F, \|\cdot\|_F \|\cdot\| n=1 B A,B","['linear-algebra', 'functional-analysis', 'matrix-calculus']"
52,How to write the inverse matrix $A^{-1}$ as a polynomial in $A$?,How to write the inverse matrix  as a polynomial in ?,A^{-1} A,"I came a cross a question that gives a matrix $A$ , and asks to write $A^{-1}$ as a polynomial in $A$ with real coefficients. I don't know what this means, and googling didn't clarify very much. I found the inverse $A$ , but I do not know what it means to write it as a polynomial in $A$ .","I came a cross a question that gives a matrix , and asks to write as a polynomial in with real coefficients. I don't know what this means, and googling didn't clarify very much. I found the inverse , but I do not know what it means to write it as a polynomial in .",A A^{-1} A A A,['linear-algebra']
53,Where am I going wrong in calculating the projection of a vector onto a subspace?,Where am I going wrong in calculating the projection of a vector onto a subspace?,,"I am currently working my way through Poole's Linear Algebra, 4th Edition , and I am hitting a bit of a wall in regards to a particular example in the chapter on least squares solutions. The line $y=a+bx$ that ""best fits"" the data points $(1,2)$ , $(2,2)$ , and $(3,4)$ can be related to the (inconsistent) system of linear equations $$a+b=2$$ $$a+2b=2$$ $$a+3b=4$$ with matrix representation $$A\mathbf{x}=\begin{bmatrix}1&1\\1&2\\1&3\\\end{bmatrix}\begin{bmatrix}a\\b\\\end{bmatrix}=\begin{bmatrix}2\\2\\4\\\end{bmatrix}=\mathbf{b}$$ Using the least squares theorem, Poole shows that the least squares solution of the system is $$\overline{\mathbf{x}}=\left(A^T A \right)^{-1} A^T \mathbf{b}=\left(\begin{bmatrix}3&6\\6&14\\\end{bmatrix}\right)^{-1}\begin{bmatrix}8\\18\\\end{bmatrix}=\begin{bmatrix}\frac{7}{3}&-1\\-1&\frac{1}{2}\\\end{bmatrix}\begin{bmatrix}8\\18\\\end{bmatrix}=\begin{bmatrix} \frac{2}{3}\\1\\\end{bmatrix}$$ so that the desired line has the equation $y=a+bx=\frac{2}{3} +x$ . The components of the vector $\overline{\mathbf{x}}$ can also be interpreted as the coefficients of the columns of $A$ in the linear combination of the columns of $A$ that produces the projection of $\mathbf{b}$ onto the column space of $A$ [which the Best Approximation Theorem identifies as the best approximation to $\mathbf{b}$ in the subspace $\mathrm{col}(A)$ ]. In other words, the projection of $\mathbf{b}$ onto $\mathrm{col}(A)$ can be found from the coefficients of $\overline{\mathbf{x}}$ by $$\mathrm{proj}_{\mathrm{col}(A)}(\mathbf{b})=\frac{2}{3}\begin{bmatrix}1\\1\\1\\\end{bmatrix}+1\begin{bmatrix}1\\2\\3\\\end{bmatrix}=\begin{bmatrix}\frac{5}{3}\\\frac{8}{3}\\\frac{11}{3}\\\end{bmatrix}$$ But when I try to calculate $\mathrm{proj}_{\mathrm{col}(A)}(\mathbf{b})$ directly [taking $\mathbf{a}_{1}$ and $\mathbf{a}_{2}$ to be the first and second columns of $A$ , respectively], I get $$\mathrm{proj}_{\mathrm{col}(A)}(\mathbf{b})=\left(\frac{\mathbf{a}_{1}\cdot\mathbf{b}}{\mathbf{a}_{1}\cdot\mathbf{a}_{1}}\right)\mathbf{a}_{1}+\left(\frac{\mathbf{a}_{2}\cdot\mathbf{b}}{\mathbf{a}_{2}\cdot\mathbf{a}_{2}}\right)\mathbf{a}_{2}=\left(\frac{\begin{bmatrix}1\\1\\1\\\end{bmatrix}\cdot\begin{bmatrix}2\\2\\4\\\end{bmatrix}}{\begin{bmatrix}1\\1\\1\\\end{bmatrix}\cdot\begin{bmatrix}1\\1\\1\\\end{bmatrix}}\right)\begin{bmatrix}1\\1\\1\\\end{bmatrix}+\left(\frac{\begin{bmatrix}1\\2\\3\\\end{bmatrix}\cdot\begin{bmatrix}2\\2\\4\\\end{bmatrix}}{\begin{bmatrix}1\\2\\3\\\end{bmatrix}\cdot\begin{bmatrix}1\\2\\3\\\end{bmatrix}}\right)\begin{bmatrix}1\\2\\3\\\end{bmatrix}$$ $$=\frac{8}{3}\begin{bmatrix}1\\1\\1\\\end{bmatrix}+\frac{18}{14}\begin{bmatrix}1\\2\\3\\\end{bmatrix}=\begin{bmatrix}\frac{8}{3}\\\frac{8}{3}\\\frac{8}{3}\\\end{bmatrix}+\begin{bmatrix}\frac{9}{7}\\\frac{18}{7}\\\frac{27}{7}\\\end{bmatrix}=\begin{bmatrix}\frac{83}{21}\\\frac{110}{21}\\\frac{137}{21}\\\end{bmatrix}$$ I am quite confident that my calculation is incorrect, for a number of reasons. For example, when I take the component of $\mathbf{b}$ orthogonal to $\mathrm{col}(A)$ $$\mathrm{perp}_{\mathrm{col}(A)}(\mathbf{b})=\mathbf{b}-\mathrm{proj}_{\mathrm{col}(A)}(\mathbf{b})=\begin{bmatrix}2\\2\\4\\\end{bmatrix}-\begin{bmatrix}\frac{83}{21}\\\frac{110}{21}\\\frac{137}{21}\\\end{bmatrix}=\begin{bmatrix}-\frac{41}{21}\\-\frac{68}{21}\\-\frac{53}{21}\\\end{bmatrix}$$ I get a vector that is not perpendicular to either $\mathbf{a}_{1}$ or $\mathbf{a}_{2}$ , indicating that this vector is not in the orthogonal complement of $\mathrm{col}(A)$ . Can somebody help me identify where I'm going wrong in my attempt to calculate the projection of $\mathbf{b}$ onto $\mathrm{col}(A)$ ?","I am currently working my way through Poole's Linear Algebra, 4th Edition , and I am hitting a bit of a wall in regards to a particular example in the chapter on least squares solutions. The line that ""best fits"" the data points , , and can be related to the (inconsistent) system of linear equations with matrix representation Using the least squares theorem, Poole shows that the least squares solution of the system is so that the desired line has the equation . The components of the vector can also be interpreted as the coefficients of the columns of in the linear combination of the columns of that produces the projection of onto the column space of [which the Best Approximation Theorem identifies as the best approximation to in the subspace ]. In other words, the projection of onto can be found from the coefficients of by But when I try to calculate directly [taking and to be the first and second columns of , respectively], I get I am quite confident that my calculation is incorrect, for a number of reasons. For example, when I take the component of orthogonal to I get a vector that is not perpendicular to either or , indicating that this vector is not in the orthogonal complement of . Can somebody help me identify where I'm going wrong in my attempt to calculate the projection of onto ?","y=a+bx (1,2) (2,2) (3,4) a+b=2 a+2b=2 a+3b=4 A\mathbf{x}=\begin{bmatrix}1&1\\1&2\\1&3\\\end{bmatrix}\begin{bmatrix}a\\b\\\end{bmatrix}=\begin{bmatrix}2\\2\\4\\\end{bmatrix}=\mathbf{b} \overline{\mathbf{x}}=\left(A^T A \right)^{-1} A^T \mathbf{b}=\left(\begin{bmatrix}3&6\\6&14\\\end{bmatrix}\right)^{-1}\begin{bmatrix}8\\18\\\end{bmatrix}=\begin{bmatrix}\frac{7}{3}&-1\\-1&\frac{1}{2}\\\end{bmatrix}\begin{bmatrix}8\\18\\\end{bmatrix}=\begin{bmatrix}
\frac{2}{3}\\1\\\end{bmatrix} y=a+bx=\frac{2}{3} +x \overline{\mathbf{x}} A A \mathbf{b} A \mathbf{b} \mathrm{col}(A) \mathbf{b} \mathrm{col}(A) \overline{\mathbf{x}} \mathrm{proj}_{\mathrm{col}(A)}(\mathbf{b})=\frac{2}{3}\begin{bmatrix}1\\1\\1\\\end{bmatrix}+1\begin{bmatrix}1\\2\\3\\\end{bmatrix}=\begin{bmatrix}\frac{5}{3}\\\frac{8}{3}\\\frac{11}{3}\\\end{bmatrix} \mathrm{proj}_{\mathrm{col}(A)}(\mathbf{b}) \mathbf{a}_{1} \mathbf{a}_{2} A \mathrm{proj}_{\mathrm{col}(A)}(\mathbf{b})=\left(\frac{\mathbf{a}_{1}\cdot\mathbf{b}}{\mathbf{a}_{1}\cdot\mathbf{a}_{1}}\right)\mathbf{a}_{1}+\left(\frac{\mathbf{a}_{2}\cdot\mathbf{b}}{\mathbf{a}_{2}\cdot\mathbf{a}_{2}}\right)\mathbf{a}_{2}=\left(\frac{\begin{bmatrix}1\\1\\1\\\end{bmatrix}\cdot\begin{bmatrix}2\\2\\4\\\end{bmatrix}}{\begin{bmatrix}1\\1\\1\\\end{bmatrix}\cdot\begin{bmatrix}1\\1\\1\\\end{bmatrix}}\right)\begin{bmatrix}1\\1\\1\\\end{bmatrix}+\left(\frac{\begin{bmatrix}1\\2\\3\\\end{bmatrix}\cdot\begin{bmatrix}2\\2\\4\\\end{bmatrix}}{\begin{bmatrix}1\\2\\3\\\end{bmatrix}\cdot\begin{bmatrix}1\\2\\3\\\end{bmatrix}}\right)\begin{bmatrix}1\\2\\3\\\end{bmatrix} =\frac{8}{3}\begin{bmatrix}1\\1\\1\\\end{bmatrix}+\frac{18}{14}\begin{bmatrix}1\\2\\3\\\end{bmatrix}=\begin{bmatrix}\frac{8}{3}\\\frac{8}{3}\\\frac{8}{3}\\\end{bmatrix}+\begin{bmatrix}\frac{9}{7}\\\frac{18}{7}\\\frac{27}{7}\\\end{bmatrix}=\begin{bmatrix}\frac{83}{21}\\\frac{110}{21}\\\frac{137}{21}\\\end{bmatrix} \mathbf{b} \mathrm{col}(A) \mathrm{perp}_{\mathrm{col}(A)}(\mathbf{b})=\mathbf{b}-\mathrm{proj}_{\mathrm{col}(A)}(\mathbf{b})=\begin{bmatrix}2\\2\\4\\\end{bmatrix}-\begin{bmatrix}\frac{83}{21}\\\frac{110}{21}\\\frac{137}{21}\\\end{bmatrix}=\begin{bmatrix}-\frac{41}{21}\\-\frac{68}{21}\\-\frac{53}{21}\\\end{bmatrix} \mathbf{a}_{1} \mathbf{a}_{2} \mathrm{col}(A) \mathbf{b} \mathrm{col}(A)","['linear-algebra', 'vectors', 'orthogonality', 'least-squares', 'projection']"
54,Notation or matrix operations for product of all elements in a vector?,Notation or matrix operations for product of all elements in a vector?,,Let $a=\left[\begin{array}.a_1\\\vdots\\a_n\end{array}\right]$ be a vector. Is there any standard notation for or any simple matrix operations that gives the product $$\prod_{i=1}^{n}a_i ?$$,Let be a vector. Is there any standard notation for or any simple matrix operations that gives the product,a=\left[\begin{array}.a_1\\\vdots\\a_n\end{array}\right] \prod_{i=1}^{n}a_i ?,"['linear-algebra', 'notation', 'vectors', 'products']"
55,Does the form $XAX^\top$ have a name?,Does the form  have a name?,XAX^\top,"The pattern $XAX^\top$ where $A$ and $X$ are matrices, arises in various areas and applications. I am aware that if $A$ has special properties (definiteness, ...), then $XAX^\top$ becomes more interesting. Does this pattern have a name? Is it studied formally in any way? I have noticed that some matrix functions $f$ such as the Matrix Exponential can benefit from $f(XAX^\top)=Xf(A)X^\top$ . When and why does this occur? I have also noticed that when $X$ has special properties such as orthogonality, other interesting features arise. I am asking for more experienced people to guide me towards a more formal direction of finding the answers to my questions. Thank you.","The pattern where and are matrices, arises in various areas and applications. I am aware that if has special properties (definiteness, ...), then becomes more interesting. Does this pattern have a name? Is it studied formally in any way? I have noticed that some matrix functions such as the Matrix Exponential can benefit from . When and why does this occur? I have also noticed that when has special properties such as orthogonality, other interesting features arise. I am asking for more experienced people to guide me towards a more formal direction of finding the answers to my questions. Thank you.",XAX^\top A X A XAX^\top f f(XAX^\top)=Xf(A)X^\top X,"['linear-algebra', 'abstract-algebra', 'matrices']"
56,Dual of a positive semidefinite cone,Dual of a positive semidefinite cone,,"The PSD cone is the set of all positive semidefinite matrices. The dual is the set of all matrices $A$ such that tr( $A^T X$ ) $\geq 0$ for all positive semidefinite matrices $X$ . How to prove that the PSD cone is self-dual,i.e. the dual is also the set of all positive semidefinite matrices?","The PSD cone is the set of all positive semidefinite matrices. The dual is the set of all matrices such that tr( ) for all positive semidefinite matrices . How to prove that the PSD cone is self-dual,i.e. the dual is also the set of all positive semidefinite matrices?",A A^T X \geq 0 X,"['linear-algebra', 'dual-cone']"
57,Orthogonality in complex numbers geometry,Orthogonality in complex numbers geometry,,"Under the standard definition of the inner product as: $⟨(w_1,...,w_n),(z_1,...,z_n)⟩=w_1 \bar z_1 +···+w_n\bar z_n$ What does it mean when $⟨(w_1,...,w_n),(z_1,...,z_n)⟩=0$ ? For real vectors it means that there is a right angle between the two vectors in space they are in. On the complex place, however, there is a different interpretation of this as (1,0) can be multiplied by $i$ to get to (0,1). So we have a rotation operation that can be linearly multiplied. For two complex 1-d vectors to be orthogonal: $(a+bi)(c-di)=0$ $(ac+bd)+i(ad-bc)=0$ $ac+bd=0$ and $ad-bc=0$ We have real parts( $a$ ) interacting with complex(like $adi$ ) and vice versa. So what does orthogonality mean geometrically here? I'm sure its different that the real case. Here is a quote from an answer I found very interesting: If ${\bf x}={\bf a}+{\bf b}i$ , ${\bf y}={\bf c}+{\bf d}i$ in $C^n$ ,   define vectors ${\bf u}=({\bf a},{\bf b})$ , ${\bf v}=({\bf c},{\bf  d})$ , ${\bf w}=(-{\bf d},{\bf c})$ in $R^{2n}$ .  Then $$          {\bf x}\cdot {\bf y} = {\bf u}\cdot {\bf v}+i{\bf u}\cdot {\bf w}  $$ so the real and imaginary parts of ${\bf x}\cdot {\bf y}$ have   exactly  the geometric interpretations you are familiar with,  as   applied to ${\bf u}$ , ${\bf v}$ and ${\bf w}$ , where ${\bf w}$ is a   rotation of ${\bf v}$ . Source: interpretation of dot product of complex vectors Why does $w$ need to be such? Bonus Questions The inner product can also be complex itself. What does this mean in terms of projections? Is it the real projection + the complex projection ? If two complex vectors have a real inner product what does this say about them?","Under the standard definition of the inner product as: What does it mean when ? For real vectors it means that there is a right angle between the two vectors in space they are in. On the complex place, however, there is a different interpretation of this as (1,0) can be multiplied by to get to (0,1). So we have a rotation operation that can be linearly multiplied. For two complex 1-d vectors to be orthogonal: and We have real parts( ) interacting with complex(like ) and vice versa. So what does orthogonality mean geometrically here? I'm sure its different that the real case. Here is a quote from an answer I found very interesting: If , in ,   define vectors , , in .  Then so the real and imaginary parts of have   exactly  the geometric interpretations you are familiar with,  as   applied to , and , where is a   rotation of . Source: interpretation of dot product of complex vectors Why does need to be such? Bonus Questions The inner product can also be complex itself. What does this mean in terms of projections? Is it the real projection + the complex projection ? If two complex vectors have a real inner product what does this say about them?","⟨(w_1,...,w_n),(z_1,...,z_n)⟩=w_1 \bar z_1 +···+w_n\bar z_n ⟨(w_1,...,w_n),(z_1,...,z_n)⟩=0 i (a+bi)(c-di)=0 (ac+bd)+i(ad-bc)=0 ac+bd=0 ad-bc=0 a adi {\bf x}={\bf a}+{\bf b}i {\bf y}={\bf c}+{\bf d}i C^n {\bf u}=({\bf a},{\bf b}) {\bf v}=({\bf c},{\bf
 d}) {\bf w}=(-{\bf d},{\bf c}) R^{2n} 
         {\bf x}\cdot {\bf y} = {\bf u}\cdot {\bf v}+i{\bf u}\cdot {\bf w}   {\bf x}\cdot {\bf y} {\bf u} {\bf v} {\bf w} {\bf w} {\bf v} w","['linear-algebra', 'abstract-algebra']"
58,On dual cones: $(K\cap L)^+\subseteq K^+ + L^+$,On dual cones:,(K\cap L)^+\subseteq K^+ + L^+,"I am trying to derive the conditions under which $(K\cap L)^+\subseteq K^+ + L^+$ where $K^+$ (and $L^+$ ) denotes the dual cone of convex cone $K$ (and $L$ ), and $K+L$ denotes the Minkowski sum of $K$ and $L$ . (I've read somewhere that the condition is $K\cap \textrm{int} L\neq \emptyset.$ ) To start off, suppose $x\in(K\cap L)^+ $ . Then, $\langle\ x, \phi\rangle \geq 0,\ \forall \phi\in K\cap L$ . In order for $x$ to be in $K^+ + L^+$ , it should be decomposable into $x=x_1+x_2$ such that $x_1\in K^+,\ x_2\in L^+$ . One way I can think of to find such a decomposition is to project $x $ on to $K^+$ and $L^+$ and (since dual cones are cones themselves) use appropriate scalars on the respective projections to get $x_1$ and $x_2$ such that their sum is equal to $x$ . For these projections to add up nicely to $x$ , I'm thinking that: either $x\in K^+ \cap  L^+$ or if $ x\notin K^+ \cap  L^+$ , $x$ should make an acute angle with the closest boundaries/faces of the dual cones. Where am I going wrong?","I am trying to derive the conditions under which where (and ) denotes the dual cone of convex cone (and ), and denotes the Minkowski sum of and . (I've read somewhere that the condition is ) To start off, suppose . Then, . In order for to be in , it should be decomposable into such that . One way I can think of to find such a decomposition is to project on to and and (since dual cones are cones themselves) use appropriate scalars on the respective projections to get and such that their sum is equal to . For these projections to add up nicely to , I'm thinking that: either or if , should make an acute angle with the closest boundaries/faces of the dual cones. Where am I going wrong?","(K\cap L)^+\subseteq K^+ + L^+ K^+ L^+ K L K+L K L K\cap \textrm{int} L\neq \emptyset. x\in(K\cap L)^+  \langle\ x, \phi\rangle \geq 0,\ \forall \phi\in K\cap L x K^+ + L^+ x=x_1+x_2 x_1\in K^+,\ x_2\in L^+ x  K^+ L^+ x_1 x_2 x x x\in K^+ \cap  L^+  x\notin K^+ \cap  L^+ x","['linear-algebra', 'abstract-algebra', 'geometry', 'optimization', 'convex-analysis']"
59,What is $\tilde{T}$ in Axler's Linear Algebra?,What is  in Axler's Linear Algebra?,\tilde{T},"On page 97 in section 3.E Axler gives definition 3.90: ""Suppose $T \in L(V,W)$ . Define $\tilde{T}: V/($ null $T) \to W$ by $\tilde{T}(v+$ null $T)=Tv$ ."" Immediately preceding this was just some theorems about the dimensions of quotient spaces. I don't quite understand what the purpose $\tilde{T}$ is, as it doesn't seem to be present in any of the exercises (at least not for 3.E), and it's not expanded upon afterward aside from presenting some statements about it's properties (e.g. $V/($ null $T)$ is isomorphic to range $T$ ). I don't even understand what the map ""represents"". Isn't $v+$ null $T$ a set? But $\tilde{T}$ doesn't equal another set, but rather a particular vector, $Tv$ . Does anyone have insight to this? Or a more common name for $\tilde{T}$ ? I haven't been able to find much on it anywhere.","On page 97 in section 3.E Axler gives definition 3.90: ""Suppose . Define null by null ."" Immediately preceding this was just some theorems about the dimensions of quotient spaces. I don't quite understand what the purpose is, as it doesn't seem to be present in any of the exercises (at least not for 3.E), and it's not expanded upon afterward aside from presenting some statements about it's properties (e.g. null is isomorphic to range ). I don't even understand what the map ""represents"". Isn't null a set? But doesn't equal another set, but rather a particular vector, . Does anyone have insight to this? Or a more common name for ? I haven't been able to find much on it anywhere.","T \in L(V,W) \tilde{T}: V/( T) \to W \tilde{T}(v+ T)=Tv \tilde{T} V/( T) T v+ T \tilde{T} Tv \tilde{T}","['linear-algebra', 'linear-transformations', 'quotient-spaces']"
60,Proving isomorphism of vector fields on a smooth manifold with derivations,Proving isomorphism of vector fields on a smooth manifold with derivations,,"This is problem 19.11 from Loring Tu's An Introduction to Manifolds [1 ed.]. Vector fields as derivations of $C^\infty$ functions. In Subsection 14.1 we showed that a $C^\infty$ vector field $X$ on a manifold $M$ gives rise to a derivation of $C^\infty(M)$ . We will now show that every derivation of $C^\infty (M)$ arises from one and only one vector field. To distinguish the vector filed from the derivation, we will temporarily denote the derivation arising from $X$ by $\phi(X)$ . Thus, for any $f \in C^\infty(M)$ , $$(\phi(X)f)(p)=X_p f \; \text{for all } p \in M.$$ (a) Let $\mathscr{F} = C^\infty (M).$ Prove that $\phi: \mathfrak{X}(M) \to Der(C^\infty(M))$ is an $\mathscr{F}-$ linear map. (b) Show that $\phi$ is injective. (c) If $D$ is a derivation of $C^\infty(M)$ and $p \in M$ , define $D_p : C_p^\infty(M) \to C_p^\infty(M)$ by $$D_p[f]=[D\tilde{f}]\in C_p^\infty(M),$$ where $[f]$ is the germ of $f$ at $p$ and $\tilde{f}$ is a global extension of $f$ . Show that $D_p[f]$ is well-defined. (d) Show that $D_p$ is a derivation of $C_p^\infty(M)$ . (e) Prove that $\phi : \mathfrak{X} \to Der(C^\infty(M))$ is an isomorphism of $\mathscr{F}-$ modules. I proved (a)-(d), however, I can't figure out how to show the isomorphism in (e). How can we prove surjectivity of the linear map $\phi$ using (c) and (d)?","This is problem 19.11 from Loring Tu's An Introduction to Manifolds [1 ed.]. Vector fields as derivations of functions. In Subsection 14.1 we showed that a vector field on a manifold gives rise to a derivation of . We will now show that every derivation of arises from one and only one vector field. To distinguish the vector filed from the derivation, we will temporarily denote the derivation arising from by . Thus, for any , (a) Let Prove that is an linear map. (b) Show that is injective. (c) If is a derivation of and , define by where is the germ of at and is a global extension of . Show that is well-defined. (d) Show that is a derivation of . (e) Prove that is an isomorphism of modules. I proved (a)-(d), however, I can't figure out how to show the isomorphism in (e). How can we prove surjectivity of the linear map using (c) and (d)?","C^\infty C^\infty X M C^\infty(M) C^\infty (M) X \phi(X) f \in C^\infty(M) (\phi(X)f)(p)=X_p f \; \text{for all } p \in M. \mathscr{F} = C^\infty (M). \phi: \mathfrak{X}(M) \to Der(C^\infty(M)) \mathscr{F}- \phi D C^\infty(M) p \in M D_p : C_p^\infty(M) \to C_p^\infty(M) D_p[f]=[D\tilde{f}]\in C_p^\infty(M), [f] f p \tilde{f} f D_p[f] D_p C_p^\infty(M) \phi : \mathfrak{X} \to Der(C^\infty(M)) \mathscr{F}- \phi","['linear-algebra', 'abstract-algebra', 'differential-geometry', 'manifolds', 'smooth-manifolds']"
61,let $M$ be a Hermitian matrix of order $n\times n$ with rank $k (\neq n)$,let  be a Hermitian matrix of order  with rank,M n\times n k (\neq n),"let $M$ be a $n \times n$ matrix of rank $k (\neq n)$ if $\lambda \neq 0$ is an eigenvalue of $M$ with corresponding unit column vector $u$ . with $Mu=\lambda u$ ,then which of the following is\are true?. 1). $rank(M-\lambda uu^{*})=k-1$ 2). $rank(M-\lambda uu^{*})=k$ 3). $rank(M-\lambda uu^{*})=k+1$ 4). $(M-\lambda uu^{*})^{n}$$=M^{n}-\lambda^{n}uu^{*}$ solution I tried in the given question rank $(M-\lambda uu^{*})$$=$ rank $(M-Muu^{*})$ taking $M$ common we get rank $[M(I-uu^{*})]$ further that I  don't know how to proceed please help! Thank you","let be a matrix of rank if is an eigenvalue of with corresponding unit column vector . with ,then which of the following is\are true?. 1). 2). 3). 4). solution I tried in the given question rank rank taking common we get rank further that I  don't know how to proceed please help! Thank you",M n \times n k (\neq n) \lambda \neq 0 M u Mu=\lambda u rank(M-\lambda uu^{*})=k-1 rank(M-\lambda uu^{*})=k rank(M-\lambda uu^{*})=k+1 (M-\lambda uu^{*})^{n}=M^{n}-\lambda^{n}uu^{*} (M-\lambda uu^{*})= (M-Muu^{*}) M [M(I-uu^{*})],['linear-algebra']
62,"Is $A^2-B^2$ positive definite too when $A-B,B$ is positive definite?",Is  positive definite too when  is positive definite?,"A^2-B^2 A-B,B","Denote $A,B\in M_n(\mathbb{R})$ If $A-B,B$ is positive definite, it's easy to see $A^2-B^2$ is symmetric. Now the question is: Prove or disprove: $A^2-B^2$ is positive definite. I have checked some easy examples(mostly 2x2), and now I believe this is true. But we know here $A,B$ do not necessarily commute. I tried to write $A$ as $A=P^\mathrm{T}P$ , and then I know all the eigenvalues of $(P^\mathrm{T})^{-1}BP^{-1}$ all lay in the interval $(0,1)$ . Another question show me that when $A,B$ is positive definite, $AB+BA$ can also be no positive definite. I can't move forward. Hint will also be appreciated.","Denote If is positive definite, it's easy to see is symmetric. Now the question is: Prove or disprove: is positive definite. I have checked some easy examples(mostly 2x2), and now I believe this is true. But we know here do not necessarily commute. I tried to write as , and then I know all the eigenvalues of all lay in the interval . Another question show me that when is positive definite, can also be no positive definite. I can't move forward. Hint will also be appreciated.","A,B\in M_n(\mathbb{R}) A-B,B A^2-B^2 A^2-B^2 A,B A A=P^\mathrm{T}P (P^\mathrm{T})^{-1}BP^{-1} (0,1) A,B AB+BA","['linear-algebra', 'positive-definite']"
63,The Dimension of Vector Space,The Dimension of Vector Space,,"Question: Let $A$ be an $n×n$ complex matrix with $n$ distinct eigenvalues. Let $V$ be the set of all $n×n$ complex matrices $B$ that commute with $A$ . Prove that V is a vector space and find its dimension (Justify your answer). My Answer: I know how to show that V is a vector space, but I don't know how to find its dimension. I tried showing that if v is an eigenvector corresponding to some eigenvalue, so is Bv, and got that for all B, Bv = kv for some scalar v. But I'm not sure if this helps.","Question: Let be an complex matrix with distinct eigenvalues. Let be the set of all complex matrices that commute with . Prove that V is a vector space and find its dimension (Justify your answer). My Answer: I know how to show that V is a vector space, but I don't know how to find its dimension. I tried showing that if v is an eigenvector corresponding to some eigenvalue, so is Bv, and got that for all B, Bv = kv for some scalar v. But I'm not sure if this helps.",A n×n n V n×n B A,"['linear-algebra', 'abstract-algebra']"
64,Null spaces and invertible matrix,Null spaces and invertible matrix,,"If $A$ and $B$ are $n×n$ matrices, show that they have the same null   space if and only if $A = UB$ for some invertible matrix $U$ . I started the question by saying $Ax = 0$ for some vector $x$ in $\text {null}(A)$ . Now I'm lost. Could someone please help me out with this question? Thank you very much.","If and are matrices, show that they have the same null   space if and only if for some invertible matrix . I started the question by saying for some vector in . Now I'm lost. Could someone please help me out with this question? Thank you very much.",A B n×n A = UB U Ax = 0 x \text {null}(A),['linear-algebra']
65,Sum of 5 square roots equals another square root. What is the minimum possible value of the summed square root?,Sum of 5 square roots equals another square root. What is the minimum possible value of the summed square root?,,"In the equation $\sqrt{a}+\sqrt{b}+\sqrt{c}+\sqrt{d}+\sqrt{e}=\sqrt{f}$ , each variable is a distinct positive integer. What is the least possible value for $f?$ Out of purely trial and error, I have come to the solution of a = 1, b= 4, c= 9, d = 16, e = 25 which sums to 15 thus $\sqrt{f} = 15$ and $f = 225$ . I suspect the final answer will turn out to be $f = 225 = 15^2$ , but I have not yet managed to come up with a rigorous proof that no smaller value of $f$ is possible. The main source of inspiration I've been trying work from so far is to consider a simpler variation of this problem where you only have two square roots on the left hand side, $$\sqrt{a}+\sqrt{b} = \sqrt{f}.$$ In this variation of the problem, we can square both sides to find that $$a+b+2\sqrt{ab} = f.$$ Therefore, $$2\sqrt{ab} = f-a-b,$$ consequently, $$4ab = (f-a-b)^2.$$ Since $a,b,f$ are integers and the left hand side ( $4ab$ ) is an integer that is divisible by $2$ , we see that the product $(f-a-b)^2 = (f-a-b)\cdot (f-a-b)$ is divisible by $2$ . A product of integers is only divisible by a prime $p$ if and only if at least one of the factors is divisible by $p$ . (This is a simple consequence of the Fundamental Theorem of Arithmetic, which is just the official name for the fact that prime factorizations of integers are unique.) Therefore, the integer factor $f-a-b$ itself must be divisible by $2$ . In other words, we have deduced that $k = \frac{f-a-b}{2}$ must be an integer. Thus we see that $$ab = k^2$$ is the square of an integer. Therefore, as a consequence of the Fundamental Theorem of Arithmetic (which is just the official name for the fact that prime factorizations of integers are unique), it must be the case that there exist integers $\alpha, \beta,\gamma$ such that $a = \alpha^2 \gamma$ and $b = \beta^2 \gamma,$ so \begin{align*}f &= a+b+2\sqrt{ab}\\ &= \alpha^2\gamma + \beta^2\gamma+2\sqrt{\alpha^2\beta^2\gamma^2}\\ &= (\alpha^2+\beta^2+2\alpha\beta)\gamma\\ &= (\alpha^2+\beta^2)\gamma\end{align*} Thus, in this simpler version of the problem where there is only a sum of two square roots, clearly the smallest possible value of $f$ comes from taking $\gamma = 1, \alpha = 1, \beta = 2$ which gives the solution $$\sqrt{1}+\sqrt{4} = \sqrt{9}.$$ I've been looking for ways to then bootstrap this argument (or something like it) up to something that can be applied to solving the original problem with a sum of five square roots. So far, I haven't found the right path forward. I have also been looking for ways to exploit some routine square root inequalities. In particular, for any positive numbers $a_1, a_2,\dots,a_n$ , it is the case that $$\sqrt{a_1+a_2+\dots+a_n} < \sqrt{a_1}+\sqrt{a_2}+\dots+\sqrt{a_n} \le \sqrt{n} \sqrt{a_1+a_2+\dots+a_n}.$$ This also has not yet led to progress. I am consequently stuck here with no ideas how to continue. Your help is appreciated! Also, can you also help me on this ( $N$'s base-5 and base-6 representations, treated as base-10, yield sum $S$. For which $N$ are $S$'s rightmost two digits the same as $2N$'s? ) problem too? Thanks! Max0815","In the equation , each variable is a distinct positive integer. What is the least possible value for Out of purely trial and error, I have come to the solution of a = 1, b= 4, c= 9, d = 16, e = 25 which sums to 15 thus and . I suspect the final answer will turn out to be , but I have not yet managed to come up with a rigorous proof that no smaller value of is possible. The main source of inspiration I've been trying work from so far is to consider a simpler variation of this problem where you only have two square roots on the left hand side, In this variation of the problem, we can square both sides to find that Therefore, consequently, Since are integers and the left hand side ( ) is an integer that is divisible by , we see that the product is divisible by . A product of integers is only divisible by a prime if and only if at least one of the factors is divisible by . (This is a simple consequence of the Fundamental Theorem of Arithmetic, which is just the official name for the fact that prime factorizations of integers are unique.) Therefore, the integer factor itself must be divisible by . In other words, we have deduced that must be an integer. Thus we see that is the square of an integer. Therefore, as a consequence of the Fundamental Theorem of Arithmetic (which is just the official name for the fact that prime factorizations of integers are unique), it must be the case that there exist integers such that and so Thus, in this simpler version of the problem where there is only a sum of two square roots, clearly the smallest possible value of comes from taking which gives the solution I've been looking for ways to then bootstrap this argument (or something like it) up to something that can be applied to solving the original problem with a sum of five square roots. So far, I haven't found the right path forward. I have also been looking for ways to exploit some routine square root inequalities. In particular, for any positive numbers , it is the case that This also has not yet led to progress. I am consequently stuck here with no ideas how to continue. Your help is appreciated! Also, can you also help me on this ( $N$'s base-5 and base-6 representations, treated as base-10, yield sum $S$. For which $N$ are $S$'s rightmost two digits the same as $2N$'s? ) problem too? Thanks! Max0815","\sqrt{a}+\sqrt{b}+\sqrt{c}+\sqrt{d}+\sqrt{e}=\sqrt{f} f? \sqrt{f} = 15 f = 225 f = 225 = 15^2 f \sqrt{a}+\sqrt{b} = \sqrt{f}. a+b+2\sqrt{ab} = f. 2\sqrt{ab} = f-a-b, 4ab = (f-a-b)^2. a,b,f 4ab 2 (f-a-b)^2 = (f-a-b)\cdot (f-a-b) 2 p p f-a-b 2 k = \frac{f-a-b}{2} ab = k^2 \alpha, \beta,\gamma a = \alpha^2 \gamma b = \beta^2 \gamma, \begin{align*}f &= a+b+2\sqrt{ab}\\
&= \alpha^2\gamma + \beta^2\gamma+2\sqrt{\alpha^2\beta^2\gamma^2}\\
&= (\alpha^2+\beta^2+2\alpha\beta)\gamma\\
&= (\alpha^2+\beta^2)\gamma\end{align*} f \gamma = 1, \alpha = 1, \beta = 2 \sqrt{1}+\sqrt{4} = \sqrt{9}. a_1, a_2,\dots,a_n \sqrt{a_1+a_2+\dots+a_n} < \sqrt{a_1}+\sqrt{a_2}+\dots+\sqrt{a_n} \le \sqrt{n} \sqrt{a_1+a_2+\dots+a_n}.","['linear-algebra', 'radicals']"
66,"Why we only need to verify additive identity, and closed under addition and scalar multiplication for subspace?","Why we only need to verify additive identity, and closed under addition and scalar multiplication for subspace?",,"In the book Linear Algebra Done Right , it is said that to determine quickly whether a given subset of $V$ is a subspace of $V$ , the three conditions, namely additive identity, closed under addition, and closed under scalar multiplication, should be satisfied. The other parts of the definition of a vector space are automatically satisfied. I think I understand why commutativity, associativity, distributive properties, and multiplicative identity works because their operations are still within the subspace. But, why don't we need to verify additive inverse, similar to verifying additive identity? Could there be cases where there will be no $v + w = 0$ in the new subspace, $v, w \in U$ , $U$ is a subspace?","In the book Linear Algebra Done Right , it is said that to determine quickly whether a given subset of is a subspace of , the three conditions, namely additive identity, closed under addition, and closed under scalar multiplication, should be satisfied. The other parts of the definition of a vector space are automatically satisfied. I think I understand why commutativity, associativity, distributive properties, and multiplicative identity works because their operations are still within the subspace. But, why don't we need to verify additive inverse, similar to verifying additive identity? Could there be cases where there will be no in the new subspace, , is a subspace?","V V v + w = 0 v, w \in U U",['linear-algebra']
67,"Proving that ${\rm vec}(A\,{\rm Diag}(b)\,C) = ((C^T\otimes 1_a)\odot(1_c\otimes A))\,b$",Proving that,"{\rm vec}(A\,{\rm Diag}(b)\,C) = ((C^T\otimes 1_a)\odot(1_c\otimes A))\,b","Given the following vectors and matrices $$\eqalign{  &A\in{\mathbb R}^{a\times b},\,\,\,\,  &B\in{\mathbb R}^{b\times b},\,\,\,\,  &C\in{\mathbb R}^{b\times c} \cr  &1_a\in{\mathbb R}^{a\times 1},\,\,\,\,  &b\in{\mathbb R}^{b\times 1},\,\,\,\,  &1_c\in{\mathbb R}^{c\times 1} \cr }$$ where $B={\rm Diag}(b)\,$ and $\,1_n$ denotes a vector of all ones of length $n$ . I would like to show that the vector $\,v={\rm vec}(ABC)\,$ can be expanded as $$\eqalign{ v &= \Big((C^T\otimes 1_a)\odot(1_c\otimes A)\Big)\,b }$$ where $(\otimes, \odot)$ denote the Kronecker and Hadamard products, respectively. I am aware of several other expressions for this vector $$\eqalign{  v &= (C^T\otimes A)\,{\rm vec}(B) \cr  v &= \Big((C^T\otimes 1_a1_b^T)\odot(1_c1_b^T\otimes A)\Big)\,{\rm vec}(B) \cr }$$ but I don't see how to arrive at the desired formula. Update After studying Omnomnomnom's answer, I realized that I needed to exploit several esoteric properties to prove the formula. 1) The outer product of two vectors vectorizes to their Kronecker product $$\eqalign{  {\rm vec}(ab^T) &= b\otimes a \cr }$$ 2) Vectors from the canonical basis distribute over a Hadamard product $$\eqalign{  (M\odot N)e_k &= (Me_k)\odot(Ne_k) \cr }$$ 3) The distribution property of the Kronecker product of 2 arbitrary vectors and a matrix $$\eqalign{  (C^T\otimes 1)e &= {\rm vec}(1(e^TC)) = (C^Te)\otimes 1 \cr  (1\otimes A)e &= {\rm vec}((Ae)1^T) = 1\otimes(Ae) \cr }$$ 4) A rule for mixed Kronecker/Hadamard products $$\eqalign{  (M\odot N)\otimes(P\odot Q) &= (M\otimes P)\odot(N\otimes Q) \cr\cr }$$ Use these rules to evaluate the $k^{th}$ column of the anticipated solution $$\eqalign{ &\big((C^T\otimes 1_a)\odot(1_c\otimes A)\big)\,e_k \cr &(C^T\otimes 1_a)e_k\odot(1_c\otimes A)e_k \cr &(C^Te_k\otimes 1_a)\odot(1_c\otimes Ae_k) \cr &(C^Te_k\odot 1_c)\otimes(1_a\odot Ae_k) \cr &(C^Te_k)\otimes(Ae_k) \cr }$$ which matches the $k^{th}$ column of Omnomnomnom's matrix. Update #2 I also like O's second approach, which I interpret as $$\eqalign{ &\Big((C^T\otimes 1_a1_b^T)\odot(1_c1_b^T\otimes A)\Big)\,{\rm vec}(B)\cr &=\sum_k b_k\,\,\big((C^T\otimes 1_a1_b^T)\odot(1_c1_b^T\otimes A)\big)\,\big(e_k\otimes e_k\big) \cr &=\sum_k b_k\,\,\big((C^T\otimes 1_a1_b^T)(e_k\otimes e_k)\big)\odot\big((1_c1_b^T\otimes A)(e_k\otimes e_k)\big) \cr &=\sum_k b_k\,\,\big(C^Te_k\otimes 1_a1_b^Te_k\big)\odot\big(1_c1_b^Te_k\otimes Ae_k\big) \cr &=\sum_k b_k\,\,\big(C^Te_k\otimes 1_a\big)\odot\big(1_c\otimes Ae_k\big) \cr &=\sum_k b_k\,\,\big((C^T\otimes 1_c)\odot(1_a\otimes A)\big)e_k \cr &=\big((C^T\otimes 1_c)\odot(1_a\otimes A)\big)\,b \cr }$$","Given the following vectors and matrices where and denotes a vector of all ones of length . I would like to show that the vector can be expanded as where denote the Kronecker and Hadamard products, respectively. I am aware of several other expressions for this vector but I don't see how to arrive at the desired formula. Update After studying Omnomnomnom's answer, I realized that I needed to exploit several esoteric properties to prove the formula. 1) The outer product of two vectors vectorizes to their Kronecker product 2) Vectors from the canonical basis distribute over a Hadamard product 3) The distribution property of the Kronecker product of 2 arbitrary vectors and a matrix 4) A rule for mixed Kronecker/Hadamard products Use these rules to evaluate the column of the anticipated solution which matches the column of Omnomnomnom's matrix. Update #2 I also like O's second approach, which I interpret as","\eqalign{
 &A\in{\mathbb R}^{a\times b},\,\,\,\,
 &B\in{\mathbb R}^{b\times b},\,\,\,\,
 &C\in{\mathbb R}^{b\times c} \cr
 &1_a\in{\mathbb R}^{a\times 1},\,\,\,\,
 &b\in{\mathbb R}^{b\times 1},\,\,\,\,
 &1_c\in{\mathbb R}^{c\times 1} \cr
} B={\rm Diag}(b)\, \,1_n n \,v={\rm vec}(ABC)\, \eqalign{
v &= \Big((C^T\otimes 1_a)\odot(1_c\otimes A)\Big)\,b
} (\otimes, \odot) \eqalign{
 v &= (C^T\otimes A)\,{\rm vec}(B) \cr
 v &= \Big((C^T\otimes 1_a1_b^T)\odot(1_c1_b^T\otimes A)\Big)\,{\rm vec}(B) \cr
} \eqalign{
 {\rm vec}(ab^T) &= b\otimes a \cr
} \eqalign{
 (M\odot N)e_k &= (Me_k)\odot(Ne_k) \cr
} \eqalign{
 (C^T\otimes 1)e &= {\rm vec}(1(e^TC)) = (C^Te)\otimes 1 \cr
 (1\otimes A)e &= {\rm vec}((Ae)1^T) = 1\otimes(Ae) \cr
} \eqalign{
 (M\odot N)\otimes(P\odot Q) &= (M\otimes P)\odot(N\otimes Q) \cr\cr
} k^{th} \eqalign{
&\big((C^T\otimes 1_a)\odot(1_c\otimes A)\big)\,e_k \cr
&(C^T\otimes 1_a)e_k\odot(1_c\otimes A)e_k \cr
&(C^Te_k\otimes 1_a)\odot(1_c\otimes Ae_k) \cr
&(C^Te_k\odot 1_c)\otimes(1_a\odot Ae_k) \cr
&(C^Te_k)\otimes(Ae_k) \cr
} k^{th} \eqalign{
&\Big((C^T\otimes 1_a1_b^T)\odot(1_c1_b^T\otimes A)\Big)\,{\rm vec}(B)\cr
&=\sum_k b_k\,\,\big((C^T\otimes 1_a1_b^T)\odot(1_c1_b^T\otimes A)\big)\,\big(e_k\otimes e_k\big) \cr
&=\sum_k b_k\,\,\big((C^T\otimes 1_a1_b^T)(e_k\otimes e_k)\big)\odot\big((1_c1_b^T\otimes A)(e_k\otimes e_k)\big) \cr
&=\sum_k b_k\,\,\big(C^Te_k\otimes 1_a1_b^Te_k\big)\odot\big(1_c1_b^Te_k\otimes Ae_k\big) \cr
&=\sum_k b_k\,\,\big(C^Te_k\otimes 1_a\big)\odot\big(1_c\otimes Ae_k\big) \cr
&=\sum_k b_k\,\,\big((C^T\otimes 1_c)\odot(1_a\otimes A)\big)e_k \cr
&=\big((C^T\otimes 1_c)\odot(1_a\otimes A)\big)\,b \cr
}","['linear-algebra', 'matrices']"
68,"Q Exercise 4, Hecke Algebras - Daniel Bump","Q Exercise 4, Hecke Algebras - Daniel Bump",,"I'm struggling with exercise 4 in Bump's Stanford Hecke Algebra notes linked here It states the following: Let $G$ be a finite group and $V,W$ vector spaces. Let $C(G,V)$ denote the spaces of maps from $G$ to $V$, which has the $G$-representation $\rho_{V}$ given by $$  (\rho_{V}(g)f)(x) = f(xg)  $$ Suppose that $T$ is a linear map from $C(G,V)$ to $C(G,W)$ that commutes with the $G$-action, i.e that $$  T(\rho_{V}(g)f) = \rho_{W}(g)T(f) $$ Prove that there exists a map $\lambda : G \rightarrow \operatorname{Hom}(V,W)$ such that $T(f) = \lambda * f$, where $*$ denotes convolution. The notes also has a lemma (Lemma 4) proving the above result in the case that $V$ and $W$ are 1-dimensional, and my hope was that I could adapt that proof to tackle this exercise but it has thus far left me stumped. In the linear case $\operatorname{Hom}(V,W)$ becomes $\mathbb{C}$ and so the space of maps $f : G \mapsto \mathbb{C}$ is a unital convolution algebra (i.e the group algebra $\mathbb{C}[G]$), with the unit given by the characteristic function of the identity element of $G$, $\delta$ say. Then $\delta *f = f * \delta$ for all $f \in \mathbb{C}[G]$, and so in particular if such a $\lambda$ existed, then $\lambda = \lambda * \delta = T(\delta)$, and then one checks that $T(\delta)$ satisfies the requirements. I would appreciate some assistance \ advice on tackling this exercise. Lemma 4 would suggest that we might want to try and find an element of $C(G,V)$, $\tau$ say, such that $F * \tau = F$ for every $F : G \rightarrow \operatorname{Hom}(V,W)$, and then continue on as in Lemma 4. But I have thus far been unable to construct such a $\tau$ (if one even exists).","I'm struggling with exercise 4 in Bump's Stanford Hecke Algebra notes linked here It states the following: Let $G$ be a finite group and $V,W$ vector spaces. Let $C(G,V)$ denote the spaces of maps from $G$ to $V$, which has the $G$-representation $\rho_{V}$ given by $$  (\rho_{V}(g)f)(x) = f(xg)  $$ Suppose that $T$ is a linear map from $C(G,V)$ to $C(G,W)$ that commutes with the $G$-action, i.e that $$  T(\rho_{V}(g)f) = \rho_{W}(g)T(f) $$ Prove that there exists a map $\lambda : G \rightarrow \operatorname{Hom}(V,W)$ such that $T(f) = \lambda * f$, where $*$ denotes convolution. The notes also has a lemma (Lemma 4) proving the above result in the case that $V$ and $W$ are 1-dimensional, and my hope was that I could adapt that proof to tackle this exercise but it has thus far left me stumped. In the linear case $\operatorname{Hom}(V,W)$ becomes $\mathbb{C}$ and so the space of maps $f : G \mapsto \mathbb{C}$ is a unital convolution algebra (i.e the group algebra $\mathbb{C}[G]$), with the unit given by the characteristic function of the identity element of $G$, $\delta$ say. Then $\delta *f = f * \delta$ for all $f \in \mathbb{C}[G]$, and so in particular if such a $\lambda$ existed, then $\lambda = \lambda * \delta = T(\delta)$, and then one checks that $T(\delta)$ satisfies the requirements. I would appreciate some assistance \ advice on tackling this exercise. Lemma 4 would suggest that we might want to try and find an element of $C(G,V)$, $\tau$ say, such that $F * \tau = F$ for every $F : G \rightarrow \operatorname{Hom}(V,W)$, and then continue on as in Lemma 4. But I have thus far been unable to construct such a $\tau$ (if one even exists).",,"['linear-algebra', 'representation-theory', 'hecke-algebras']"
69,Deciding which statements about matrix A is true where $A^3+A^2-3A+I=0$,Deciding which statements about matrix A is true where,A^3+A^2-3A+I=0,"A is $2 \times 2$ matrix. $$A^3+A^2-3A+I=0$$ Decide which statements is true. $\quad$ A) 1 is eigenvalue of A. $\quad$ B) Det(A) is 1. $\quad$ C) $A^{-1}$ exists. $\quad$ D) If B is inverse of A, $B^3-3B^2+B+I=0$ . Choices are {A,B}, {A,C}, {C,D}, and {B,C,D}. What I've did so far is, A : If I multiply eigenvector v $_{(2 \times 1)}$ to given equation, It'll satisfy the equation if eigenvalue of A is 1. $\quad$ But I'm not sure if it's enough to say statement A is true. B: (?) C: $A(-A^2-A+3)=I$ , so it's true. D: $B=-A^2-A+3. $ $\quad B^3-3B^2+B+I=0=B(B^2-3B+1)+I\;$ , If I substitute B then $\quad =(-A^2-A+3)(A^4+2A^3-2A^2-3A+1)=-A^3-A^2+3A=I$ . $\quad$ So it's true. Have I done correctly? and How should I go for statements A and B?","A is matrix. Decide which statements is true. A) 1 is eigenvalue of A. B) Det(A) is 1. C) exists. D) If B is inverse of A, . Choices are {A,B}, {A,C}, {C,D}, and {B,C,D}. What I've did so far is, A : If I multiply eigenvector v to given equation, It'll satisfy the equation if eigenvalue of A is 1. But I'm not sure if it's enough to say statement A is true. B: (?) C: , so it's true. D: , If I substitute B then . So it's true. Have I done correctly? and How should I go for statements A and B?",2 \times 2 A^3+A^2-3A+I=0 \quad \quad \quad A^{-1} \quad B^3-3B^2+B+I=0 _{(2 \times 1)} \quad A(-A^2-A+3)=I B=-A^2-A+3.  \quad B^3-3B^2+B+I=0=B(B^2-3B+1)+I\; \quad =(-A^2-A+3)(A^4+2A^3-2A^2-3A+1)=-A^3-A^2+3A=I \quad,['linear-algebra']
70,If $A+B = AB$ then proving that $AB = BA$?,If  then proving that ?,A+B = AB AB = BA,"Even though this has been asked before in main site If $A+B=AB$ then $AB=BA$ , still I had a query? If $A,B$ are both $n \times n$ matrix and the entries are from $\Bbb{R}$. If it satisfies $A+B = AB$, then can we say that $A$ and $B$ commute? that is $AB = BA$ ? I thought of this that as we are given the rule $A+B = AB$, so that also implies that $B+A =BA$ just interchanging the roles of $A$ and $B$? from which we get $AB = BA$ hence $A$ and $B$ commute. Any flaw in this? How do we approach this problem?","Even though this has been asked before in main site If $A+B=AB$ then $AB=BA$ , still I had a query? If $A,B$ are both $n \times n$ matrix and the entries are from $\Bbb{R}$. If it satisfies $A+B = AB$, then can we say that $A$ and $B$ commute? that is $AB = BA$ ? I thought of this that as we are given the rule $A+B = AB$, so that also implies that $B+A =BA$ just interchanging the roles of $A$ and $B$? from which we get $AB = BA$ hence $A$ and $B$ commute. Any flaw in this? How do we approach this problem?",,['linear-algebra']
71,"There are $n$ different $3$-element subsets $A_1,A_2,...,A_n$ of the set $\{1,2,...,n\}$, with $|A_i \cap A_j| \not= 1$ for all $i \not= j$.","There are  different -element subsets  of the set , with  for all .","n 3 A_1,A_2,...,A_n \{1,2,...,n\} |A_i \cap A_j| \not= 1 i \not= j","Determine all possible values of positive integer $n$ , such that there are $n$ different $3$ -element subsets $A_1,A_2,...,A_n$ of the set $\{1,2,...,n\}$ , with $|A_i \cap A_j| \not= 1$ for all $i \not= j$ . Source: China Western Olympiad 2010 Attempt: It is quite clear that for $n=4k$ such a system exist. For $n=4$ , we have $A_1 =\{1,2,3\}$ , $A_2 =\{1,2,4\}$ , $A_3 =\{2,3,4\}$ , $A_4 =\{1,3,4\}$ . It is not hard to see that induction $n\to n+4$ works. Now I would like to prove that there is no such system if $4\nmid n$ . I thought about linear algebra approach. Observe the given sets as vectors in $\mathbb{F}_2^n$ . Then since $A_i\cdot A_i =1$ and $A_i\cdot A_j = 0$ for each $i\ne j$ these vectors are linear independent: $$ b_1A_1+b_2A_2+...+b_nA_n = 0\;\;\;\; /\cdot A_i$$ $$ b_1\cdot 0+b_2\cdot 0+...+b_i\cdot 1+...b_n\cdot 0 =0\implies b_i=0$$ But now, I'm not sure what to do...","Determine all possible values of positive integer , such that there are different -element subsets of the set , with for all . Source: China Western Olympiad 2010 Attempt: It is quite clear that for such a system exist. For , we have , , , . It is not hard to see that induction works. Now I would like to prove that there is no such system if . I thought about linear algebra approach. Observe the given sets as vectors in . Then since and for each these vectors are linear independent: But now, I'm not sure what to do...","n n 3 A_1,A_2,...,A_n \{1,2,...,n\} |A_i \cap A_j| \not= 1 i \not= j n=4k n=4 A_1 =\{1,2,3\} A_2 =\{1,2,4\} A_3 =\{2,3,4\} A_4 =\{1,3,4\} n\to n+4 4\nmid n \mathbb{F}_2^n A_i\cdot A_i =1 A_i\cdot A_j = 0 i\ne j  b_1A_1+b_2A_2+...+b_nA_n = 0\;\;\;\; /\cdot A_i  b_1\cdot 0+b_2\cdot 0+...+b_i\cdot 1+...b_n\cdot 0 =0\implies b_i=0","['linear-algebra', 'combinatorics', 'contest-math', 'algebraic-combinatorics']"
72,Issue with Matrix Multiplication using the Formal Definition,Issue with Matrix Multiplication using the Formal Definition,,"I am writing a formal proof to show that if $B$ is the matrix obtained by interchanging the rows of a $2\times2$ matrix $A$, then $\det(B)=-\det(A)$.  My reasoning and proof are coming along nicely but I hit a bump in the road that highlighted to me a gap in my knowledge - that is, I guess I do not completely understand the definition of matrix multiplication.  Note I went the rigorous route here only because I wanted to prove to myself I fully understood matrix multiplication... and I don't.  My proof thus far is: Let $E$ be the elementary matrix obtained by performing a type 1 elementary row operation on $I_2$.  By Theorem 3.1 (Friedberg), $B=EA$.  Note $$\det(A) =\det\begin{pmatrix}     a & b \\     c & d     \end{pmatrix}=ad-bc$$ By the definition of matrix multiplication, \begin{align} & B_{ij}=(EA)_{ij} \\[10pt] = {} & \sum_{k=1}^2 E_{ik}A_{kj} \text{ for } 1\le i\le2\text{, }1\le j\le2 \\[10pt] = {} & E_{i1}A_{1j}+E_{i2}A_{2j}\text{ for } 1\le i\le2\text{, }1\le j\le2 \\[10pt] \vdots\,\,\, \\[10pt] = {} & \begin{pmatrix}     c & d \\     a & b     \end{pmatrix}_{ij} \end{align} If $B=EA=\begin{pmatrix}     c & d \\     a & b \\     \end{pmatrix}$, then by the definition of a determinant of a $2\times2$ matrix, \begin{align} \det(B) & =\det(EA)=bc-ad \\[10pt] & =-(ad-bc) \\[10pt] & =-\det(A) \end{align} My issue is, how do I formally express the steps where I put my ""$\cdots$""?  That is, the column and row vector multiplication and their sum? Maybe I'm wrong, but I feel most sources don't fully explain all the steps of matrix multiplication and just resort to hand-waving.  The way I think about it - the column and row vectors I will be multiplying in my proof are actually just $2\times1$ and $1\times2$ matrices, respectively.  I know they result in a $2\times2$ matrix, but how?  And why?","I am writing a formal proof to show that if $B$ is the matrix obtained by interchanging the rows of a $2\times2$ matrix $A$, then $\det(B)=-\det(A)$.  My reasoning and proof are coming along nicely but I hit a bump in the road that highlighted to me a gap in my knowledge - that is, I guess I do not completely understand the definition of matrix multiplication.  Note I went the rigorous route here only because I wanted to prove to myself I fully understood matrix multiplication... and I don't.  My proof thus far is: Let $E$ be the elementary matrix obtained by performing a type 1 elementary row operation on $I_2$.  By Theorem 3.1 (Friedberg), $B=EA$.  Note $$\det(A) =\det\begin{pmatrix}     a & b \\     c & d     \end{pmatrix}=ad-bc$$ By the definition of matrix multiplication, \begin{align} & B_{ij}=(EA)_{ij} \\[10pt] = {} & \sum_{k=1}^2 E_{ik}A_{kj} \text{ for } 1\le i\le2\text{, }1\le j\le2 \\[10pt] = {} & E_{i1}A_{1j}+E_{i2}A_{2j}\text{ for } 1\le i\le2\text{, }1\le j\le2 \\[10pt] \vdots\,\,\, \\[10pt] = {} & \begin{pmatrix}     c & d \\     a & b     \end{pmatrix}_{ij} \end{align} If $B=EA=\begin{pmatrix}     c & d \\     a & b \\     \end{pmatrix}$, then by the definition of a determinant of a $2\times2$ matrix, \begin{align} \det(B) & =\det(EA)=bc-ad \\[10pt] & =-(ad-bc) \\[10pt] & =-\det(A) \end{align} My issue is, how do I formally express the steps where I put my ""$\cdots$""?  That is, the column and row vector multiplication and their sum? Maybe I'm wrong, but I feel most sources don't fully explain all the steps of matrix multiplication and just resort to hand-waving.  The way I think about it - the column and row vectors I will be multiplying in my proof are actually just $2\times1$ and $1\times2$ matrices, respectively.  I know they result in a $2\times2$ matrix, but how?  And why?",,"['linear-algebra', 'matrices', 'proof-writing', 'vectors']"
73,Proof any five points determine a conic,Proof any five points determine a conic,,"Theorem There is a conic through any five $\color{red}{\textit{pairwise distinct }}$ points in a real plane. $(1)$  It is unique if and only if no four of the points are colinear;  $(2)$  It is  nondegenerate if and only if no three of the points are collinear . How to prove this theorem , only applying some theories of linear algebra(including matrix)？ Let $\textbf{p}_{i}=(x_{i},y_{i}),i=1,2,3,4,5.$ be the five points in the plane . Consider a homogeneous system $\textbf{Ax}=\textbf{0}$ whose coefficient matrix is $\textbf{A}= \begin{pmatrix}  x_{1}^{2}&  x_{1}y_{1}&  y_{1}^{2}&  x_{1}&  y_{1}&  1\\   x_{2}^{2}&  x_{2}y_{2}&  y_{2}^{2}&  x_{2}&  y_{2}&  1 \\   x_{3}^{2}&  x_{3}y_{3}&  y_{3}^{2}&  x_{3}&  y_{3}&  1 \\   x_{4}^{2}&  x_{4}y_{4}&  y_{4}^{2}&  x_{4}&  y_{4}&  1\\   x_{5}^{2}&  x_{5}y_{5}&  y_{5}^{2}&  x_{5}&  y_{5}&  1  \end{pmatrix}; $ $\qquad\qquad\textbf{x}$ = $\begin{pmatrix} A\\  B\\  C\\  D\\  E\\ F  \end{pmatrix};$ $\qquad\qquad\textbf{0}$=$\begin{pmatrix} 0\\  0\\  0\\  0\\  0 \end{pmatrix}.$ $\textbf{A}_{i}(i\in\{1,2,3,4,5,6\})$ is the $5\times 5$  square matrix obtained by deleting the $i^{th}$ column of $\textbf{A}.$ $\\$ Then the above theorem  is equalvlity of the following statement from the algebraic point of view. $(1){'} $ $rank(A)=5$ and at least one of $\textbf{A}_{1},\textbf{A}_{2},\textbf{A}_{3}$ is nonsingular. $\Longleftrightarrow $ $\forall\quad k_{1},k_{2},k_{3},k_{4}\in \{1,2,3,4,5\} $and $k_{1}<k_{2}<k_{3}<k_{4}$,$ rank(\begin{pmatrix}  x_{k_{1}}&  y_{k_{1}}& 1\\   x_{k_{2}}&  y_{k_{2}}& 1 \\   x_{k_{3}}&  y_{k_{3}}& 1\\   x_{k_{4}}&  y_{k_{4}}& 1  \end{pmatrix})=3.$ $(2){'}$ A conic through $\textbf{p}_{i}=(x_{i},y_{i}),i=1,2,3,4,5$ is nondegenerate.$\Longleftrightarrow $ $ \forall \quad k_{1},k_{2},k_{3}\in \{1,2,3,4,5\} $and $k_{1}<k_{2}<k_{3}$,$ rank(\begin{pmatrix}  x_{k_{1}}&  y_{k_{1}}& 1\\   x_{k_{2}}&  y_{k_{2}}& 1 \\   x_{k_{3}}&  y_{k_{3}}& 1  \end{pmatrix})=3.$ Until now, how to prove $(1){'}$,$(2){'}$ hold, only using some theories of linear algebra(including matrix)？","Theorem There is a conic through any five $\color{red}{\textit{pairwise distinct }}$ points in a real plane. $(1)$  It is unique if and only if no four of the points are colinear;  $(2)$  It is  nondegenerate if and only if no three of the points are collinear . How to prove this theorem , only applying some theories of linear algebra(including matrix)？ Let $\textbf{p}_{i}=(x_{i},y_{i}),i=1,2,3,4,5.$ be the five points in the plane . Consider a homogeneous system $\textbf{Ax}=\textbf{0}$ whose coefficient matrix is $\textbf{A}= \begin{pmatrix}  x_{1}^{2}&  x_{1}y_{1}&  y_{1}^{2}&  x_{1}&  y_{1}&  1\\   x_{2}^{2}&  x_{2}y_{2}&  y_{2}^{2}&  x_{2}&  y_{2}&  1 \\   x_{3}^{2}&  x_{3}y_{3}&  y_{3}^{2}&  x_{3}&  y_{3}&  1 \\   x_{4}^{2}&  x_{4}y_{4}&  y_{4}^{2}&  x_{4}&  y_{4}&  1\\   x_{5}^{2}&  x_{5}y_{5}&  y_{5}^{2}&  x_{5}&  y_{5}&  1  \end{pmatrix}; $ $\qquad\qquad\textbf{x}$ = $\begin{pmatrix} A\\  B\\  C\\  D\\  E\\ F  \end{pmatrix};$ $\qquad\qquad\textbf{0}$=$\begin{pmatrix} 0\\  0\\  0\\  0\\  0 \end{pmatrix}.$ $\textbf{A}_{i}(i\in\{1,2,3,4,5,6\})$ is the $5\times 5$  square matrix obtained by deleting the $i^{th}$ column of $\textbf{A}.$ $\\$ Then the above theorem  is equalvlity of the following statement from the algebraic point of view. $(1){'} $ $rank(A)=5$ and at least one of $\textbf{A}_{1},\textbf{A}_{2},\textbf{A}_{3}$ is nonsingular. $\Longleftrightarrow $ $\forall\quad k_{1},k_{2},k_{3},k_{4}\in \{1,2,3,4,5\} $and $k_{1}<k_{2}<k_{3}<k_{4}$,$ rank(\begin{pmatrix}  x_{k_{1}}&  y_{k_{1}}& 1\\   x_{k_{2}}&  y_{k_{2}}& 1 \\   x_{k_{3}}&  y_{k_{3}}& 1\\   x_{k_{4}}&  y_{k_{4}}& 1  \end{pmatrix})=3.$ $(2){'}$ A conic through $\textbf{p}_{i}=(x_{i},y_{i}),i=1,2,3,4,5$ is nondegenerate.$\Longleftrightarrow $ $ \forall \quad k_{1},k_{2},k_{3}\in \{1,2,3,4,5\} $and $k_{1}<k_{2}<k_{3}$,$ rank(\begin{pmatrix}  x_{k_{1}}&  y_{k_{1}}& 1\\   x_{k_{2}}&  y_{k_{2}}& 1 \\   x_{k_{3}}&  y_{k_{3}}& 1  \end{pmatrix})=3.$ Until now, how to prove $(1){'}$,$(2){'}$ hold, only using some theories of linear algebra(including matrix)？",,"['linear-algebra', 'matrices', 'algebraic-geometry', 'projective-geometry', 'matrix-rank']"
74,An isometry between linear subspaces that is not very far from orthogonal projection,An isometry between linear subspaces that is not very far from orthogonal projection,,"Let $\mathcal M$ and $\mathcal N$ be two linear subspaces of $\mathbb{R}^n$ such that $\dim \mathcal M = \dim \mathcal{N}$. Such subspaces are known to be isometric: that is, there exists a bijective linear operator $T:\mathcal M\to\mathcal N$ such that $\|Tx\|=\|x\|$ for all $x\in\mathcal M$. But I want to choose such $T$ so it's not too far from the orthogonal projection onto $\mathcal N$, denoted $P_{\mathcal{N}}$. Is there an isometry $T:\mathcal M\to\mathcal N$ such that $$ \|Tx-P_\mathcal Nx\|\le \|x-P_\mathcal Nx\|\quad \forall x\in\mathcal M \tag{?} $$ Progress so far If $\mathcal M \perp \mathcal N$, the answer is trivially yes: any isometry works because $P_\mathcal Nx=0$ for all $x\in \mathcal M$. If $\dim \mathcal M = 1 =\dim \mathcal N$, the answer is also yes. Indeed, we can arrange for $\mathcal N$ to be the horizontal axis in $\mathbb{R}^2$, and for $\mathcal N$ to be the line $\{(t\cos \theta, t\sin\theta) : t\in\mathbb{R}\}$ where $\theta\in [0, \pi/2]$ is fixed. The isometry $(t \cos\theta, t\sin\theta)\mapsto (t, 0)$ satisfies (?) because  $1-\cos \theta \le \sin\theta $. It suffices to consider the case $\mathcal M\cap \mathcal N = \{0\}$. Otherwise, let $\widetilde{\mathcal M}$ be the orthogonal complement of $\mathcal M\cap \mathcal N$ in $\mathcal M$, and similarly for $\widetilde{\mathcal N}$. Pick an isometry $\widetilde{T} : \widetilde{\mathcal M} \to \widetilde{\mathcal N}$ for which (?) holds, and extend it to $T:\mathcal M\to\mathcal N$ by letting $Tx=x$ on $\mathcal M\cap \mathcal N$. Then $T$ is equivariant with respect to shifts along  $\mathcal M\cap \mathcal N$, and so is $P_{\mathcal N}$, so the property (?) holds for $T$ as well.","Let $\mathcal M$ and $\mathcal N$ be two linear subspaces of $\mathbb{R}^n$ such that $\dim \mathcal M = \dim \mathcal{N}$. Such subspaces are known to be isometric: that is, there exists a bijective linear operator $T:\mathcal M\to\mathcal N$ such that $\|Tx\|=\|x\|$ for all $x\in\mathcal M$. But I want to choose such $T$ so it's not too far from the orthogonal projection onto $\mathcal N$, denoted $P_{\mathcal{N}}$. Is there an isometry $T:\mathcal M\to\mathcal N$ such that $$ \|Tx-P_\mathcal Nx\|\le \|x-P_\mathcal Nx\|\quad \forall x\in\mathcal M \tag{?} $$ Progress so far If $\mathcal M \perp \mathcal N$, the answer is trivially yes: any isometry works because $P_\mathcal Nx=0$ for all $x\in \mathcal M$. If $\dim \mathcal M = 1 =\dim \mathcal N$, the answer is also yes. Indeed, we can arrange for $\mathcal N$ to be the horizontal axis in $\mathbb{R}^2$, and for $\mathcal N$ to be the line $\{(t\cos \theta, t\sin\theta) : t\in\mathbb{R}\}$ where $\theta\in [0, \pi/2]$ is fixed. The isometry $(t \cos\theta, t\sin\theta)\mapsto (t, 0)$ satisfies (?) because  $1-\cos \theta \le \sin\theta $. It suffices to consider the case $\mathcal M\cap \mathcal N = \{0\}$. Otherwise, let $\widetilde{\mathcal M}$ be the orthogonal complement of $\mathcal M\cap \mathcal N$ in $\mathcal M$, and similarly for $\widetilde{\mathcal N}$. Pick an isometry $\widetilde{T} : \widetilde{\mathcal M} \to \widetilde{\mathcal N}$ for which (?) holds, and extend it to $T:\mathcal M\to\mathcal N$ by letting $Tx=x$ on $\mathcal M\cap \mathcal N$. Then $T$ is equivariant with respect to shifts along  $\mathcal M\cap \mathcal N$, and so is $P_{\mathcal N}$, so the property (?) holds for $T$ as well.",,['linear-algebra']
75,$A^2-B^2=\alpha(AB-BA)$,,A^2-B^2=\alpha(AB-BA),"Let $A, B \in M_n(\mathbb{R})$ , $\alpha\in\mathbb{R}$ , such that $A^2-B^2=\alpha(AB-BA)$ . Prove that $a)$ If $\alpha=0$ and $n$ odd, then $\det(AB-BA)=0$ $b)$ If $\alpha\neq0$ then $(AB-BA)^n=0_n$ For $a)$ we use the fact that $$\det(A+B)(A-B)=\det(A-B)(A+B)$$ which means that $$\det(AB-BA)=\det(-(AB-BA))$$ and since $n$ is odd we obtain the conclusion. The second point is, however, a little bit trickier. I managed to show just that $\det(AB-BA)=0$ . Using the same method as for $a)$ , we observe that $$\det((\alpha+1)(AB-BA))=\det((\alpha-1)(AB-BA))$$ and since $\alpha\neq0$ , we obtain that our determinant is $0$ , but from here I don't have any idea what should I do next.","Let , , such that . Prove that If and odd, then If then For we use the fact that which means that and since is odd we obtain the conclusion. The second point is, however, a little bit trickier. I managed to show just that . Using the same method as for , we observe that and since , we obtain that our determinant is , but from here I don't have any idea what should I do next.","A, B \in M_n(\mathbb{R}) \alpha\in\mathbb{R} A^2-B^2=\alpha(AB-BA) a) \alpha=0 n \det(AB-BA)=0 b) \alpha\neq0 (AB-BA)^n=0_n a) \det(A+B)(A-B)=\det(A-B)(A+B) \det(AB-BA)=\det(-(AB-BA)) n \det(AB-BA)=0 a) \det((\alpha+1)(AB-BA))=\det((\alpha-1)(AB-BA)) \alpha\neq0 0","['linear-algebra', 'matrices', 'determinant']"
76,Nullity and rank bounds for a nilpotent matrix,Nullity and rank bounds for a nilpotent matrix,,"Let $A=\mathbb R^{11}\to \mathbb R^{11}$ be a linear transformation such that $A^5=0$ and $A^4\neq 0$. Which of the following is true? a) $\operatorname{null}A\le7$ b) $2\le\operatorname{null}A$ c) $2\le\operatorname{rk}A\le9$ I don't know how i think about $A$ from this information. I think construct such as example where this hold but I can't construct any such of type example which satisfies $A^5=0$ but $A^4\ne0$. I have an idea now: consider $T:\mathbb R^{11}\to\mathbb R^{11}$ such that $$T(x_1,x_2,...,x_{11})=(x_2,x_3,x_4,x_5,0,0,..,0)$$ $\Rightarrow T^5(x_1,x_2,...,x_{11})=(0,0,...,0)$. So we can say $\operatorname{null}T\le11-\operatorname{rk}T=7$","Let $A=\mathbb R^{11}\to \mathbb R^{11}$ be a linear transformation such that $A^5=0$ and $A^4\neq 0$. Which of the following is true? a) $\operatorname{null}A\le7$ b) $2\le\operatorname{null}A$ c) $2\le\operatorname{rk}A\le9$ I don't know how i think about $A$ from this information. I think construct such as example where this hold but I can't construct any such of type example which satisfies $A^5=0$ but $A^4\ne0$. I have an idea now: consider $T:\mathbb R^{11}\to\mathbb R^{11}$ such that $$T(x_1,x_2,...,x_{11})=(x_2,x_3,x_4,x_5,0,0,..,0)$$ $\Rightarrow T^5(x_1,x_2,...,x_{11})=(0,0,...,0)$. So we can say $\operatorname{null}T\le11-\operatorname{rk}T=7$",,['linear-algebra']
77,"If $A$ has eigenvalues $\{\lambda_1,\lambda_2,...,\lambda_k\}$ then does $A^n$ have only $\{\lambda^n_1,\lambda^n_2,...,\lambda^n_k\}$ as eigenvalues? [duplicate]",If  has eigenvalues  then does  have only  as eigenvalues? [duplicate],"A \{\lambda_1,\lambda_2,...,\lambda_k\} A^n \{\lambda^n_1,\lambda^n_2,...,\lambda^n_k\}","This question already has answers here : Eigenvalues and power of a matrix (3 answers) Closed 6 years ago . If $A$ has $k$ distinct eigenvalues $\{\lambda_1,\lambda_2,...,\lambda_k\}$ then does $A^n$ have only $\{\lambda^n_1,\lambda^n_2,...,\lambda^n_k\}$ as eigenvalues? It is well know that if $\lambda$ is an eigenvalues of $A$ then $\lambda^n$ is an eigenvalues of $A^n$ for $n\in \mathbb{N}$. But I am wondering whether $A^n$ has only $\lambda^n$ as eigenvalues, or are there more?","This question already has answers here : Eigenvalues and power of a matrix (3 answers) Closed 6 years ago . If $A$ has $k$ distinct eigenvalues $\{\lambda_1,\lambda_2,...,\lambda_k\}$ then does $A^n$ have only $\{\lambda^n_1,\lambda^n_2,...,\lambda^n_k\}$ as eigenvalues? It is well know that if $\lambda$ is an eigenvalues of $A$ then $\lambda^n$ is an eigenvalues of $A^n$ for $n\in \mathbb{N}$. But I am wondering whether $A^n$ has only $\lambda^n$ as eigenvalues, or are there more?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
78,"Eigenvector, eigenvalue and matrix of $(\mathbf A+\mathbf I)^{-1}$ where $\mathbf A=\mathbf{vv}^\top$","Eigenvector, eigenvalue and matrix of  where",(\mathbf A+\mathbf I)^{-1} \mathbf A=\mathbf{vv}^\top,"Given $\mathbf v=\begin{bmatrix}v_1\\v_2\\\vdots\\v_n\end{bmatrix}$ and $\mathbf A=\mathbf{vv}^\top$, find the matrix $(\mathbf A+\mathbf I)^{-1}$ and its eigenvectors and eigenvalues. This is a follow up of questions of the previous one . Previous example told us $\mathbf A+\mathbf I$ has eigenvalue $\lambda_1=(\|\mathbf v\|^2+1)$ with corresponding eigenvector $\mathbf v_1=\mathbf v$. And eigenvalue $1$ with multiplicity $n-1$. We also know that if a matrix $\mathbf B$ has eigenvector $\mathbf v$ and eigenvalue $\lambda$, then the inverse matrix $\mathbf B^{-1}$ will have same eigenvector $\mathbf v$ and eigenvalue $\frac1\lambda$. Based on the above knowledge, we can easily to get eigenvalue of $(\mathbf A+\mathbf I)^{-1}$ will be $1$ with multiplicty $n-1$ and $\frac1{(\|\mathbf v\|^2+1)}$. They share the same eigenvectors. Now, I want to know how does the matrix  $(\mathbf A+\mathbf I)^{-1}$ look like by using minimal polynomial. I am guessing I can use the same techniques here . First, I find out the characteristic polynomial for $(\mathbf A+\mathbf I)$. $f(\lambda)=(\lambda-1)^{n-1}(\lambda-1-\|\mathbf v\|^2)$ Then we know the possible candidate for minimal polynomial will be $(\lambda-1)^{k}(\lambda-1-\|\mathbf v\|^2)$ where $k \ge 1$. How do I know the $k$ for minimal polynomial from here? Is minimal polynomial a good starting point to find the inverse of the matrix?","Given $\mathbf v=\begin{bmatrix}v_1\\v_2\\\vdots\\v_n\end{bmatrix}$ and $\mathbf A=\mathbf{vv}^\top$, find the matrix $(\mathbf A+\mathbf I)^{-1}$ and its eigenvectors and eigenvalues. This is a follow up of questions of the previous one . Previous example told us $\mathbf A+\mathbf I$ has eigenvalue $\lambda_1=(\|\mathbf v\|^2+1)$ with corresponding eigenvector $\mathbf v_1=\mathbf v$. And eigenvalue $1$ with multiplicity $n-1$. We also know that if a matrix $\mathbf B$ has eigenvector $\mathbf v$ and eigenvalue $\lambda$, then the inverse matrix $\mathbf B^{-1}$ will have same eigenvector $\mathbf v$ and eigenvalue $\frac1\lambda$. Based on the above knowledge, we can easily to get eigenvalue of $(\mathbf A+\mathbf I)^{-1}$ will be $1$ with multiplicty $n-1$ and $\frac1{(\|\mathbf v\|^2+1)}$. They share the same eigenvectors. Now, I want to know how does the matrix  $(\mathbf A+\mathbf I)^{-1}$ look like by using minimal polynomial. I am guessing I can use the same techniques here . First, I find out the characteristic polynomial for $(\mathbf A+\mathbf I)$. $f(\lambda)=(\lambda-1)^{n-1}(\lambda-1-\|\mathbf v\|^2)$ Then we know the possible candidate for minimal polynomial will be $(\lambda-1)^{k}(\lambda-1-\|\mathbf v\|^2)$ where $k \ge 1$. How do I know the $k$ for minimal polynomial from here? Is minimal polynomial a good starting point to find the inverse of the matrix?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'inverse', 'minimal-polynomials']"
79,"Given a $3 \times 3$ matrix, left multiply or right multiply unitary matrices (e.g. householder reflector) to introduce zeros.","Given a  matrix, left multiply or right multiply unitary matrices (e.g. householder reflector) to introduce zeros.",3 \times 3,"Given a $3 \times 3$ matrix $A$, we would like to left-multiply or right-multiply unitary matrices to introduce zero elements in specific forms as the following, $\left( {\begin{array}{*{20}{c}}   {\text{x}}&{\text{x}}&0 \\    {\text{x}}&0&{\text{x}} \\    0&{\text{x}}&{\text{x}}  \end{array}} \right)$ and $\left( {\begin{array}{*{20}{c}}   {\text{x}}&{\text{x}}&0 \\    {\text{0}}&0&{\text{x}} \\    0&{\text{0}}&{\text{x}}  \end{array}} \right)$ where $\text x$ represents a non-zero element. My attempt : For the first form, First apply a householder reflector for the 1st column of the 2nd and 3rd rows but keeps the 1st row unchanged,then we have $\left( {\begin{array}{*{20}{c}}   {\text{x}}&{\text{x}}&{\text{x}} \\    {\text{x}}&{\text{x}}&{\text{x}} \\    {\text{0}}&{\text{x}}&{\text{x}}  \end{array}} \right)$, then similarly householder on the last column of the first two rows but keeps the 3rd row unchanged, and we have $\left( {\begin{array}{*{20}{c}}   {\text{x}}&{\text{x}}&0 \\    {\text{x}}&{\text{x}}&{\text{x}} \\    {\text{0}}&{\text{x}}&{\text{x}}  \end{array}} \right)$. I cannot figure out how to make the center element zero while preserving the zeros introduced earlier. For the second form, First apply householder reflectors to make it upper triangular $\left( {\begin{array}{*{20}{c}}   {\text{x}}&{\text{x}}&{\text{x}} \\    {\text{0}}&{\text{x}}&{\text{x}} \\    {\text{0}}&{\text{0}}&{\text{x}}  \end{array}} \right)$, then apply householder on the first row of the last two columns to have $\left( {\begin{array}{*{20}{c}}   {\text{x}}&{\text{x}}&{\text{0}} \\    {\text{0}}&{\text{x}}&{\text{x}} \\    {\text{0}}&{\text{x}}&{\text{x}}  \end{array}} \right)$, then householder again to make it $\left( {\begin{array}{*{20}{c}}   {\text{x}}&{\text{x}}&{\text{0}} \\    {\text{0}}&{\text{x}}&{\text{x}} \\    {\text{0}}&{\text{0}}&{\text{x}}  \end{array}} \right)$. I have the same problem to make the center element zero.","Given a $3 \times 3$ matrix $A$, we would like to left-multiply or right-multiply unitary matrices to introduce zero elements in specific forms as the following, $\left( {\begin{array}{*{20}{c}}   {\text{x}}&{\text{x}}&0 \\    {\text{x}}&0&{\text{x}} \\    0&{\text{x}}&{\text{x}}  \end{array}} \right)$ and $\left( {\begin{array}{*{20}{c}}   {\text{x}}&{\text{x}}&0 \\    {\text{0}}&0&{\text{x}} \\    0&{\text{0}}&{\text{x}}  \end{array}} \right)$ where $\text x$ represents a non-zero element. My attempt : For the first form, First apply a householder reflector for the 1st column of the 2nd and 3rd rows but keeps the 1st row unchanged,then we have $\left( {\begin{array}{*{20}{c}}   {\text{x}}&{\text{x}}&{\text{x}} \\    {\text{x}}&{\text{x}}&{\text{x}} \\    {\text{0}}&{\text{x}}&{\text{x}}  \end{array}} \right)$, then similarly householder on the last column of the first two rows but keeps the 3rd row unchanged, and we have $\left( {\begin{array}{*{20}{c}}   {\text{x}}&{\text{x}}&0 \\    {\text{x}}&{\text{x}}&{\text{x}} \\    {\text{0}}&{\text{x}}&{\text{x}}  \end{array}} \right)$. I cannot figure out how to make the center element zero while preserving the zeros introduced earlier. For the second form, First apply householder reflectors to make it upper triangular $\left( {\begin{array}{*{20}{c}}   {\text{x}}&{\text{x}}&{\text{x}} \\    {\text{0}}&{\text{x}}&{\text{x}} \\    {\text{0}}&{\text{0}}&{\text{x}}  \end{array}} \right)$, then apply householder on the first row of the last two columns to have $\left( {\begin{array}{*{20}{c}}   {\text{x}}&{\text{x}}&{\text{0}} \\    {\text{0}}&{\text{x}}&{\text{x}} \\    {\text{0}}&{\text{x}}&{\text{x}}  \end{array}} \right)$, then householder again to make it $\left( {\begin{array}{*{20}{c}}   {\text{x}}&{\text{x}}&{\text{0}} \\    {\text{0}}&{\text{x}}&{\text{x}} \\    {\text{0}}&{\text{0}}&{\text{x}}  \end{array}} \right)$. I have the same problem to make the center element zero.",,"['linear-algebra', 'numerical-linear-algebra']"
80,If $\lambda$ has algebraic multiplicity $m$ has $\lambda^k$ the same multiplicity?,If  has algebraic multiplicity  has  the same multiplicity?,\lambda m \lambda^k,Let $A\in M_n(\mathbb{C})$ and $\lambda$ a eigenvalue of $A$ with the algebraic multiplicity equal to $m$. I know that $\lambda^k$ is an eigenvalue of $A^k$. What can I say about its algebraic multiplicity?,Let $A\in M_n(\mathbb{C})$ and $\lambda$ a eigenvalue of $A$ with the algebraic multiplicity equal to $m$. I know that $\lambda^k$ is an eigenvalue of $A^k$. What can I say about its algebraic multiplicity?,,"['linear-algebra', 'abstract-algebra', 'matrices', 'eigenvalues-eigenvectors']"
81,Eigenvectors for a sum of diagonal and anti-diagonal matrices,Eigenvectors for a sum of diagonal and anti-diagonal matrices,,"Consider the case of invertible matrix which is the sum of diagonal and anti-diagonal matrices, e.g., $$\begin{bmatrix} \color{red}1 &   0 &  0 &  0 &  \color{red}6 \\ 0 &  \color{red}2 &  0 &  \color{red}7 &  0 \\ 0  & 0 & \color{red}3 &  0 &  0 \\ 0 &  \color{red}8  & 0 & \color{red}4 &  0 \\ \color{red}9 &  0  & 0 & 0 &  \color{red}5\end{bmatrix}$$ Such matrices I name shortly $X$ -matrices (even shorter $X$ - do they have more official name?) and it's easy to check that the sum of two $X$ -matrices is an $X$ -matrix. Also the product of two matrices is   an $X$ -matrix as $$X_1X_2=(D_1+A_1)   (D_2+A_2)=(D_1D_2+A_1A_2)+(D_1A_2+A_1D_2)$$ ( $D,A$ denoted here as diagonal and antidiagonal part of $X$ ) and product of two diagonal or two anti-diagonal is always diagonal and product of diagonal and anti-diagonal is anti-diagonal. Further if $X$ -matrix is invertible also its inverse is an $X$ -matrix because inverse can be presented as a polynomial of $X$ from Cayley-Hamilton theorem. Making calculations I have found one more property of these matrices: i.e. also eigenvectors $v_1, v_2, \dots $ for this type of matrix can be grouped  to make $X$ -matrix. For instance for the matrix listed above we have eigenvectors as colummns of $$V=\begin{bmatrix}  \color{red}{-0.730}  &  \color{red}{0.529}   &  0.000  &   0.000   &  0.000 \\   0.000  &  0.000   &  \color{red}{0.730}  &  \color{red}{-0.633}  &   0.000  \\   0.000 &   0.000   & 0.000  &   0.000  &  \color{red}{1.000}  \\   0.000  &   0.000  & \color{red}{-0.683}  &  \color{red}{-0.774}  &  0.000  \\   \color{red}{0.683} &   \color{red}{0.848}   &  0.000  &   0.000  &   0.000  \\ \end{bmatrix}$$ and it's possible to permute columns in order to obtain  from them an $X$ -matrix. How this last property can be proved? How  can we prove that there is a permutation of eigenvectors of $X$ -matrix which is also an $X$ -matrix? Could we use for proof the equation $X=VDV^{-1}$ where however $V$ , if columns are chosen randomly, can be in the form which is not an $X$ -matrix? (but its some permutation supposedly is ...)","Consider the case of invertible matrix which is the sum of diagonal and anti-diagonal matrices, e.g., Such matrices I name shortly -matrices (even shorter - do they have more official name?) and it's easy to check that the sum of two -matrices is an -matrix. Also the product of two matrices is   an -matrix as ( denoted here as diagonal and antidiagonal part of ) and product of two diagonal or two anti-diagonal is always diagonal and product of diagonal and anti-diagonal is anti-diagonal. Further if -matrix is invertible also its inverse is an -matrix because inverse can be presented as a polynomial of from Cayley-Hamilton theorem. Making calculations I have found one more property of these matrices: i.e. also eigenvectors for this type of matrix can be grouped  to make -matrix. For instance for the matrix listed above we have eigenvectors as colummns of and it's possible to permute columns in order to obtain  from them an -matrix. How this last property can be proved? How  can we prove that there is a permutation of eigenvectors of -matrix which is also an -matrix? Could we use for proof the equation where however , if columns are chosen randomly, can be in the form which is not an -matrix? (but its some permutation supposedly is ...)","\begin{bmatrix} \color{red}1 &   0 &  0 &  0 &  \color{red}6 \\ 0 &  \color{red}2 &  0 &  \color{red}7 &  0 \\ 0  & 0 & \color{red}3 &  0 &  0 \\ 0 &  \color{red}8  & 0 & \color{red}4 &  0 \\ \color{red}9 &  0  & 0 & 0 &  \color{red}5\end{bmatrix} X X X X X X_1X_2=(D_1+A_1)   (D_2+A_2)=(D_1D_2+A_1A_2)+(D_1A_2+A_1D_2) D,A X X X X v_1, v_2, \dots  X V=\begin{bmatrix}
 \color{red}{-0.730}  &  \color{red}{0.529}   &  0.000  &   0.000   &  0.000 \\
  0.000  &  0.000   &  \color{red}{0.730}  &  \color{red}{-0.633}  &   0.000  \\
  0.000 &   0.000   & 0.000  &   0.000  &  \color{red}{1.000}  \\
  0.000  &   0.000  & \color{red}{-0.683}  &  \color{red}{-0.774}  &  0.000  \\
  \color{red}{0.683} &   \color{red}{0.848}   &  0.000  &   0.000  &   0.000  \\
\end{bmatrix} X X X X=VDV^{-1} V X","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'permutations']"
82,Definition of sum of subspaces.,Definition of sum of subspaces.,,"The definition of sum of subspaces in Axler Sheldon book ""Linear Algebra Done Right"" is: The sum of $U_1,\dots,U_m,$ denoted $U_1+\cdots+U_m$, is defined to be the set of all possible sums of elements of $U_1,\dots,U_m$. More precisely, $$U_1 + \cdots + U_m = \{u_1 + \cdots + u_m : u_1 \in U_i,\dots, u_m \in U_m\}.$$ Right after the definition, the  following example is provided: Suppose $U$ and $W$ are subspaces of $F^3$ given by $$ U = \{(x,0,0) \in F^3 : x \in F\} \text{ and } W = \{(0,y,0)\in F^3: y \in F\}.$$ then \begin{equation}\tag{1}\label{1}U+W = \{(x,y,0) : x,y \in F\}.\end{equation} Then let $V$ be $$V = \{(y,y,0)\in F^3: y \in F\}.$$ the book says that $U+V$ is still given by equation \eqref{1}. I am confused with the previous statement, as I would expect that $U+V = \{(x+y,y,0):x,y\in F\}.$ I would appreciate an explanation of this example.","The definition of sum of subspaces in Axler Sheldon book ""Linear Algebra Done Right"" is: The sum of $U_1,\dots,U_m,$ denoted $U_1+\cdots+U_m$, is defined to be the set of all possible sums of elements of $U_1,\dots,U_m$. More precisely, $$U_1 + \cdots + U_m = \{u_1 + \cdots + u_m : u_1 \in U_i,\dots, u_m \in U_m\}.$$ Right after the definition, the  following example is provided: Suppose $U$ and $W$ are subspaces of $F^3$ given by $$ U = \{(x,0,0) \in F^3 : x \in F\} \text{ and } W = \{(0,y,0)\in F^3: y \in F\}.$$ then \begin{equation}\tag{1}\label{1}U+W = \{(x,y,0) : x,y \in F\}.\end{equation} Then let $V$ be $$V = \{(y,y,0)\in F^3: y \in F\}.$$ the book says that $U+V$ is still given by equation \eqref{1}. I am confused with the previous statement, as I would expect that $U+V = \{(x+y,y,0):x,y\in F\}.$ I would appreciate an explanation of this example.",,"['linear-algebra', 'vector-spaces']"
83,Inequality between Frobenius and nuclear norm,Inequality between Frobenius and nuclear norm,,"Let $M$ be a square matrix, $\|\cdot\|_*$ be the nuclear (trace) norm, and $\|\cdot\|_F$ be the Frobenius norm. The following inequality holds between the norms: $$\|M\|^2_* \leq \text{rank}(M) \|M\|^2_F.$$ This is pretty easy to show by using the definitions of the norms in terms of the singular values $\sigma_i$, since $\|M\|_* = \sum_i \sigma_i$ and $\|M\|_F = \sqrt{\sum_i \sigma_i^2}$ and the result follows by Cauchy-Schwarz. However, out of curiosity I have been trying to prove this using the definitions of the norms as $\|M\|_* = \text{trace} (\sqrt{M^* M})$ and $\|M\|_F = \sqrt{\text{trace}(M^* M)}$. Can the above inequality be shown using these definitions and without invoking the singular values explicitly?","Let $M$ be a square matrix, $\|\cdot\|_*$ be the nuclear (trace) norm, and $\|\cdot\|_F$ be the Frobenius norm. The following inequality holds between the norms: $$\|M\|^2_* \leq \text{rank}(M) \|M\|^2_F.$$ This is pretty easy to show by using the definitions of the norms in terms of the singular values $\sigma_i$, since $\|M\|_* = \sum_i \sigma_i$ and $\|M\|_F = \sqrt{\sum_i \sigma_i^2}$ and the result follows by Cauchy-Schwarz. However, out of curiosity I have been trying to prove this using the definitions of the norms as $\|M\|_* = \text{trace} (\sqrt{M^* M})$ and $\|M\|_F = \sqrt{\text{trace}(M^* M)}$. Can the above inequality be shown using these definitions and without invoking the singular values explicitly?",,"['linear-algebra', 'matrices', 'alternative-proof', 'matrix-norms', 'nuclear-norm']"
84,"$P_n(x,y)$ denote the vector space of polynomial with degree less than equal to $n $, then $\dim (P_n(x,y))=\frac{(n+1)(n+2)}{2!}$","denote the vector space of polynomial with degree less than equal to , then","P_n(x,y) n  \dim (P_n(x,y))=\frac{(n+1)(n+2)}{2!}","$P_n(x,y)$  denote the vector space of polynomial with degree less than equal to $n $, then $\dim (P_n(x,y))=\dfrac{(n+1)(n+2)}{2!}$ How this answer is coming? My attempt, Total base elements (of degree${}=n)= n+1$ Total base elements (of degree${}=n-1)= n$ $\ldots$ Total base elements (of degree${}=1)= 2$. Therefore total possible base elements${}=\dfrac{((n+1)+1)(n+1)}{2}$ Am I Right? How to generalize the formula. Please explain.","$P_n(x,y)$  denote the vector space of polynomial with degree less than equal to $n $, then $\dim (P_n(x,y))=\dfrac{(n+1)(n+2)}{2!}$ How this answer is coming? My attempt, Total base elements (of degree${}=n)= n+1$ Total base elements (of degree${}=n-1)= n$ $\ldots$ Total base elements (of degree${}=1)= 2$. Therefore total possible base elements${}=\dfrac{((n+1)+1)(n+1)}{2}$ Am I Right? How to generalize the formula. Please explain.",,['linear-algebra']
85,Counter-example for a matrix not being a correlation matrix,Counter-example for a matrix not being a correlation matrix,,"Is there an example of an $n \times n$ matrix that: is real-valued and symmetric with entries between $-1$ and $+1$ has diagonal elements equal to $1$ has a non-negative determinant has non-negative determinant for each leading principal minor but is not a correlation matrix , i.e., is not positive semidefinite ? I am well-aware of Sylvester's criterion for positive definiteness, which requires all principal minors to be non-negative to ensure positive semidefiniteness. However, for structured matrices of the specified format (correlation matrix type), I never came across an actual example illustrating the subtle difference.","Is there an example of an $n \times n$ matrix that: is real-valued and symmetric with entries between $-1$ and $+1$ has diagonal elements equal to $1$ has a non-negative determinant has non-negative determinant for each leading principal minor but is not a correlation matrix , i.e., is not positive semidefinite ? I am well-aware of Sylvester's criterion for positive definiteness, which requires all principal minors to be non-negative to ensure positive semidefiniteness. However, for structured matrices of the specified format (correlation matrix type), I never came across an actual example illustrating the subtle difference.",,"['linear-algebra', 'matrices', 'determinant', 'correlation', 'positive-semidefinite']"
86,Exercise: If the linear span is finite-dimensional then it's closed,Exercise: If the linear span is finite-dimensional then it's closed,,"Let $\;H\;$ be a Hilbert space and $\;φ_1,φ_2, \dots ,φ_m\in H\;$   where $\;m \lt \infty\;$. Prove  that the linear span of $\;φ_1,φ_2,  \dots ,φ_m\; \equiv\langle φ_1,φ_2, \dots ,φ_m \rangle \;$ is a closed   subset of $\;H\;$ My attempt Consider $\;y_n \in \langle φ_1,φ_2, \dots ,φ_m \rangle \;$ such that $\;y_n \rightarrow y \in H\;$. It is sufficient to show $\;y \in \langle φ_1,φ_2, \dots ,φ_m \rangle \;$. Since $\;y_n \in \langle φ_1,φ_2, \dots ,φ_m \rangle \;$ , there are $\;a_i \in \mathbb C \; \forall 1\le i \le n\;$ such that $\;y_n=\sum_{i=1}^m a_iφ_i \;$ (*) . But $\;y_n \rightarrow y \;$ and so $\;y=\sum_{i=1}^m a_iφ_i \;$. This means $\;y\in \langle φ_1,φ_2, \dots ,φ_m \rangle \;$ I'm a bit unsure if the above is right. I know it's something quite elementary but I've been stuck. If the dimension of $\;\langle φ_1,φ_2, \dots ,φ_m \rangle \;$ wasn't finite then my proof wouldn't be valid because $\;m\;$ in (*) would be $\; \infty\;$? Any help would be valuable! Thanks in advance!","Let $\;H\;$ be a Hilbert space and $\;φ_1,φ_2, \dots ,φ_m\in H\;$   where $\;m \lt \infty\;$. Prove  that the linear span of $\;φ_1,φ_2,  \dots ,φ_m\; \equiv\langle φ_1,φ_2, \dots ,φ_m \rangle \;$ is a closed   subset of $\;H\;$ My attempt Consider $\;y_n \in \langle φ_1,φ_2, \dots ,φ_m \rangle \;$ such that $\;y_n \rightarrow y \in H\;$. It is sufficient to show $\;y \in \langle φ_1,φ_2, \dots ,φ_m \rangle \;$. Since $\;y_n \in \langle φ_1,φ_2, \dots ,φ_m \rangle \;$ , there are $\;a_i \in \mathbb C \; \forall 1\le i \le n\;$ such that $\;y_n=\sum_{i=1}^m a_iφ_i \;$ (*) . But $\;y_n \rightarrow y \;$ and so $\;y=\sum_{i=1}^m a_iφ_i \;$. This means $\;y\in \langle φ_1,φ_2, \dots ,φ_m \rangle \;$ I'm a bit unsure if the above is right. I know it's something quite elementary but I've been stuck. If the dimension of $\;\langle φ_1,φ_2, \dots ,φ_m \rangle \;$ wasn't finite then my proof wouldn't be valid because $\;m\;$ in (*) would be $\; \infty\;$? Any help would be valuable! Thanks in advance!",,"['linear-algebra', 'functional-analysis', 'proof-verification', 'hilbert-spaces']"
87,Finding a matrix $Q \in \mathbb{R}^{d\times r}$ such that $Q^\top Q=I_r$ and $(QQ^\top)_{ii}=h_{ii}$,Finding a matrix  such that  and,Q \in \mathbb{R}^{d\times r} Q^\top Q=I_r (QQ^\top)_{ii}=h_{ii},"Given $\{h_{ii}\}_{i=1}^d$, where $h_{ii}\in[0,1],$ and $\displaystyle\sum_{i=1}^d h_{ii}=r<d,$ does there exist a matrix $Q\in\mathbb{R}^{d\times r},$ s.t. $$Q^\top Q=I_r, \qquad (QQ^\top)_{ii}=h_{ii}?$$","Given $\{h_{ii}\}_{i=1}^d$, where $h_{ii}\in[0,1],$ and $\displaystyle\sum_{i=1}^d h_{ii}=r<d,$ does there exist a matrix $Q\in\mathbb{R}^{d\times r},$ s.t. $$Q^\top Q=I_r, \qquad (QQ^\top)_{ii}=h_{ii}?$$",,"['linear-algebra', 'matrices']"
88,Non-backtracking closed walks and the Ihara zeta function (Updated with partial attempt),Non-backtracking closed walks and the Ihara zeta function (Updated with partial attempt),,"For a connected $d$-regular graph $G=(V,E)$ with adjacency matrix $A$, we defined a sequence of matrices $$A_0,A_1,A_2,A_3,\dots$$ defined using powers of $A$ inductively as follows: $$A_0=I$$ $$A_1=A$$ $$A_2=A^2-dI$$ For $k \geq 3$, $$A_{k}=A_{k-1}A-(d-1)A_{k-2}$$ Just like $(A^k)_{v,w}$ counts the number of walks on $G$ from $v$ to $w$, the value $(A_k)_{v,w}$ counts the number of walks on $G$ from $v$ to $w$ without backtracking. The recurrence relation above can be used to easily show that the ordinary (matrix) generating function for the above sequence is $$\sum \limits_{k=0}^{\infty} t^k A_k = (1- t^2)I. \left( I-tA + (d-1)t^2 I \right)^{-1}$$ With some abuse of notation, we can rewrite this generating function as $$\frac{1-t^2}{I-At+(d-1)t^2}$$ On the other hand, the Ihara zeta function of the graph $G$ is given by $$\zeta_G(t) = exp \left( \sum \limits_{k=1}^{\infty} N_k \frac{t^k}{k} \right)$$ where $N_k$ is the number of closed non-backtracking walks on $G$ of length $k$. It is known that $\zeta_G(t)$ has an alternate expression using determinants as $$\zeta_G(t) = \frac{(1-t^2)^{|V|-|E|}}{det(I-At+(d-1)t^2)}$$ My question is: can the determinant formula for the Ihara zeta function be derived from the generating function for the matrices $A_k$? What exactly is the relationship between $A_k$ and $N_k$? A similar question has been asked here How to get from Chebyshev to Ihara? and I have also been trying out ideas from here Proof of 2 Matrix identities (Traces, Logs, Determinants) But I am not interested in the Chebychev polynomial connection here: just whether the generating function can be manipulated using logarithms and traces to obtain the expression for the zeta function. Thanks. UPDATE: Here's a partial attempt of mine. Using Chebychev polynomials of the second kind defined as  $$U_0(x)=1$$ $$U_1(x) = 2x$$ and for $k \geq 2$, $$U_k(x) = U_{k-1}(x)U_1(x) - U_{k-2}(x)$$ and with generating function $$\sum \limits_{k=0}^{\infty} U_k(x)t^k = \frac{1}{1-2xt+t^2}$$ we can express the matrix $A_k$ as $$A_k = (d-1)^{k/2} U_k \left( \frac{A}{2 \sqrt{d-1}} \right) - (d-1)^{k/2-1} U_{k-2} \left( \frac{A}{2 \sqrt{d-1}} \right)$$ and so $$Tr(A_k) = (d-1)^{k/2} \sum \limits_{i=0}^{n-1} U_k \left( \frac{\mu_i}{2 \sqrt{d-1}} \right) - (d-1)^{k/2-1} \sum \limits_{i=0}^{n-1} U_{k-2} \left( \frac{\mu_i}{2 \sqrt{d-1}} \right) $$ where  $$d=\mu_0 \geq \mu_1 \geq \dots \geq \mu_{n-1} \geq d$$ are the $n$ eigenvalues of the adjacency matrix $A$. Thus we have an expression for the trace of $A_k$ as a polynomial in the eigenvalues of $A$. In a similar way, working backwards fro the definition of the Ihara zeta function, we can define another matrix sequence $B_k$ as follows:  $$B_1=A$$ $$B_2 = A^2-dI$$ and for $k \geq 3$, $$B_k = \begin{cases} B_{k-1}A - (d-1)B_{k-2} - (d-2)A & \text{ if k is odd}\\ B_{k-1}A - (d-1)B_{k-2} + d(d-2)I & \text{ if k is even} \end{cases}$$ Just like $A_k$ could be expressed in terms of Chebychev polynomials $U_k$ of the second kind, the matrices $B_k$ can be expressed using Chebychev polynomials $T_k$ of the first kind defined by the recurrence $$T_0(x)=1$$ $$T_1(x)=x$$ and for $k \geq 2$, $$T_k(x)=2xT_{k-1}(x)-T_{k-2}(x)$$  The expression for $B_k$ is $$B_k=\begin{cases} 2(d-1)^{k/2}T_k \left( \frac{A}{2\sqrt{d-1}} \right) & \text{ if k is odd}\\ 2(d-1)^{k/2}T_k \left( \frac{A}{2\sqrt{d-1}} \right)+(d-2)I & \text{ if k is even} \end{cases}$$ All this is simply by reverse engineering the expression for the Ihara zeta function to obtain $N_k$ as the trace of some matrix. My question now, modulo correctness of my calculations, is whether the matrices $B_k$ as defined above have any natural interpretation in terms of walks on the graph.","For a connected $d$-regular graph $G=(V,E)$ with adjacency matrix $A$, we defined a sequence of matrices $$A_0,A_1,A_2,A_3,\dots$$ defined using powers of $A$ inductively as follows: $$A_0=I$$ $$A_1=A$$ $$A_2=A^2-dI$$ For $k \geq 3$, $$A_{k}=A_{k-1}A-(d-1)A_{k-2}$$ Just like $(A^k)_{v,w}$ counts the number of walks on $G$ from $v$ to $w$, the value $(A_k)_{v,w}$ counts the number of walks on $G$ from $v$ to $w$ without backtracking. The recurrence relation above can be used to easily show that the ordinary (matrix) generating function for the above sequence is $$\sum \limits_{k=0}^{\infty} t^k A_k = (1- t^2)I. \left( I-tA + (d-1)t^2 I \right)^{-1}$$ With some abuse of notation, we can rewrite this generating function as $$\frac{1-t^2}{I-At+(d-1)t^2}$$ On the other hand, the Ihara zeta function of the graph $G$ is given by $$\zeta_G(t) = exp \left( \sum \limits_{k=1}^{\infty} N_k \frac{t^k}{k} \right)$$ where $N_k$ is the number of closed non-backtracking walks on $G$ of length $k$. It is known that $\zeta_G(t)$ has an alternate expression using determinants as $$\zeta_G(t) = \frac{(1-t^2)^{|V|-|E|}}{det(I-At+(d-1)t^2)}$$ My question is: can the determinant formula for the Ihara zeta function be derived from the generating function for the matrices $A_k$? What exactly is the relationship between $A_k$ and $N_k$? A similar question has been asked here How to get from Chebyshev to Ihara? and I have also been trying out ideas from here Proof of 2 Matrix identities (Traces, Logs, Determinants) But I am not interested in the Chebychev polynomial connection here: just whether the generating function can be manipulated using logarithms and traces to obtain the expression for the zeta function. Thanks. UPDATE: Here's a partial attempt of mine. Using Chebychev polynomials of the second kind defined as  $$U_0(x)=1$$ $$U_1(x) = 2x$$ and for $k \geq 2$, $$U_k(x) = U_{k-1}(x)U_1(x) - U_{k-2}(x)$$ and with generating function $$\sum \limits_{k=0}^{\infty} U_k(x)t^k = \frac{1}{1-2xt+t^2}$$ we can express the matrix $A_k$ as $$A_k = (d-1)^{k/2} U_k \left( \frac{A}{2 \sqrt{d-1}} \right) - (d-1)^{k/2-1} U_{k-2} \left( \frac{A}{2 \sqrt{d-1}} \right)$$ and so $$Tr(A_k) = (d-1)^{k/2} \sum \limits_{i=0}^{n-1} U_k \left( \frac{\mu_i}{2 \sqrt{d-1}} \right) - (d-1)^{k/2-1} \sum \limits_{i=0}^{n-1} U_{k-2} \left( \frac{\mu_i}{2 \sqrt{d-1}} \right) $$ where  $$d=\mu_0 \geq \mu_1 \geq \dots \geq \mu_{n-1} \geq d$$ are the $n$ eigenvalues of the adjacency matrix $A$. Thus we have an expression for the trace of $A_k$ as a polynomial in the eigenvalues of $A$. In a similar way, working backwards fro the definition of the Ihara zeta function, we can define another matrix sequence $B_k$ as follows:  $$B_1=A$$ $$B_2 = A^2-dI$$ and for $k \geq 3$, $$B_k = \begin{cases} B_{k-1}A - (d-1)B_{k-2} - (d-2)A & \text{ if k is odd}\\ B_{k-1}A - (d-1)B_{k-2} + d(d-2)I & \text{ if k is even} \end{cases}$$ Just like $A_k$ could be expressed in terms of Chebychev polynomials $U_k$ of the second kind, the matrices $B_k$ can be expressed using Chebychev polynomials $T_k$ of the first kind defined by the recurrence $$T_0(x)=1$$ $$T_1(x)=x$$ and for $k \geq 2$, $$T_k(x)=2xT_{k-1}(x)-T_{k-2}(x)$$  The expression for $B_k$ is $$B_k=\begin{cases} 2(d-1)^{k/2}T_k \left( \frac{A}{2\sqrt{d-1}} \right) & \text{ if k is odd}\\ 2(d-1)^{k/2}T_k \left( \frac{A}{2\sqrt{d-1}} \right)+(d-2)I & \text{ if k is even} \end{cases}$$ All this is simply by reverse engineering the expression for the Ihara zeta function to obtain $N_k$ as the trace of some matrix. My question now, modulo correctness of my calculations, is whether the matrices $B_k$ as defined above have any natural interpretation in terms of walks on the graph.",,"['linear-algebra', 'graph-theory', 'generating-functions', 'algebraic-graph-theory', 'zeta-functions']"
89,Gerschgorin's Theorem (Round 1),Gerschgorin's Theorem (Round 1),,"In looking for a proof to the Gerschgorin's Theorem, I stumbled across this paper: http://buzzard.ups.edu/courses/2007spring/projects/brakkenthal-paper.pdf I don't quite buy the proof for Theorem 2.1 in that paper (yet). Theorem 2.1: every eigenvalue $\lambda$ of a square matrix $A\in \mathbb{C}^{n\times n}$ satisfies    $$ \vert \lambda - A_{ii}\vert \leq \sum_{j\neq i}\vert A_{i,j}\vert , i\in \{1,2,\ldots,n\} $$ And the proof offered: If Theorem 2.1 is not satisfied, then $\lambda I-A$ is strictly diagonally dominant (SDD). $\Rightarrow$ $\lambda I - A$ is non-singular $\Rightarrow$ $\lambda$ is not an eigenvalue of $A$ My problem lies with Theorem not satisfied $\Rightarrow$ $\lambda I - A$ is SDD part. I mean, the author also states that the matrix $\lambda I - A$ is SDD if $\vert \lambda - A_{ii}\vert > \sum_{j\neq i}\vert A_{i,j}\vert$ for every i But the way I see it, the theorem is already not satisfied if $$\vert \lambda - A_{ii}\vert > \sum_{j\neq i}\vert A_{i,j}\vert$$ were to hold for some $i$ while $$\vert \lambda - A_{kk}\vert \leq \sum_{j\neq k}\vert A_{k,j}\vert$$ for some $k\neq i$, in which case $\lambda I - A$ would not be SDD. What am I missing?","In looking for a proof to the Gerschgorin's Theorem, I stumbled across this paper: http://buzzard.ups.edu/courses/2007spring/projects/brakkenthal-paper.pdf I don't quite buy the proof for Theorem 2.1 in that paper (yet). Theorem 2.1: every eigenvalue $\lambda$ of a square matrix $A\in \mathbb{C}^{n\times n}$ satisfies    $$ \vert \lambda - A_{ii}\vert \leq \sum_{j\neq i}\vert A_{i,j}\vert , i\in \{1,2,\ldots,n\} $$ And the proof offered: If Theorem 2.1 is not satisfied, then $\lambda I-A$ is strictly diagonally dominant (SDD). $\Rightarrow$ $\lambda I - A$ is non-singular $\Rightarrow$ $\lambda$ is not an eigenvalue of $A$ My problem lies with Theorem not satisfied $\Rightarrow$ $\lambda I - A$ is SDD part. I mean, the author also states that the matrix $\lambda I - A$ is SDD if $\vert \lambda - A_{ii}\vert > \sum_{j\neq i}\vert A_{i,j}\vert$ for every i But the way I see it, the theorem is already not satisfied if $$\vert \lambda - A_{ii}\vert > \sum_{j\neq i}\vert A_{i,j}\vert$$ were to hold for some $i$ while $$\vert \lambda - A_{kk}\vert \leq \sum_{j\neq k}\vert A_{k,j}\vert$$ for some $k\neq i$, in which case $\lambda I - A$ would not be SDD. What am I missing?",,"['linear-algebra', 'eigenvalues-eigenvectors', 'proof-explanation']"
90,$nth$ root of matrices in characteristic $p$,root of matrices in characteristic,nth p,"We know that for any matrix $A\in GL_n(\mathbb C)$, and any natural number $m$ there exists a matrix $B\in GL_n(\mathbb C)$ such that $B^m=A$, can we extend this  to any algebraically closed field with positive characteristic?","We know that for any matrix $A\in GL_n(\mathbb C)$, and any natural number $m$ there exists a matrix $B\in GL_n(\mathbb C)$ such that $B^m=A$, can we extend this  to any algebraically closed field with positive characteristic?",,"['linear-algebra', 'matrices']"
91,Linear Transformation from Infinite dimensional to Finite dimensional Space,Linear Transformation from Infinite dimensional to Finite dimensional Space,,"Let $T:V\to V$ be a linear transformation, where $V$ is an infinite -dimensional vector space over a field $F$. Assume that $T(V)=\{T(v):v\in V\}$ is finite -dimensional. Show that $T$ satisfies a nonzero polynomial over $F$, that is, there exists $a_0,\dots, a_n\in F$, with $a_n\neq 0_F$ such that $$a_0v+a_1T(v)+\dots+a_nT^n(v)=0_V$$ for all $v\in V$. I am not very sure how to approach this question. Suppose the dimension of $T(V)$ is $n$. I tried considering the set $\{T(v),T^2(v),\dots,T^{n+1}(v)\}$ which has to be linearly dependent thus there exists $a_i$ such that  $a_1T(v)+\dots+a_{n+1}T^{n+1}(v)=0$. This seems to be similar to what the question whats, except that the polynomial is dependent on $v$, while the question wants a polynomial that works for all $v\in V$. Thanks for any help.","Let $T:V\to V$ be a linear transformation, where $V$ is an infinite -dimensional vector space over a field $F$. Assume that $T(V)=\{T(v):v\in V\}$ is finite -dimensional. Show that $T$ satisfies a nonzero polynomial over $F$, that is, there exists $a_0,\dots, a_n\in F$, with $a_n\neq 0_F$ such that $$a_0v+a_1T(v)+\dots+a_nT^n(v)=0_V$$ for all $v\in V$. I am not very sure how to approach this question. Suppose the dimension of $T(V)$ is $n$. I tried considering the set $\{T(v),T^2(v),\dots,T^{n+1}(v)\}$ which has to be linearly dependent thus there exists $a_i$ such that  $a_1T(v)+\dots+a_{n+1}T^{n+1}(v)=0$. This seems to be similar to what the question whats, except that the polynomial is dependent on $v$, while the question wants a polynomial that works for all $v\in V$. Thanks for any help.",,"['linear-algebra', 'abstract-algebra', 'linear-transformations']"
92,What is the inverse of the covariance matrix generated by the exponential covariance function?,What is the inverse of the covariance matrix generated by the exponential covariance function?,,"I'm trying to analytically find the inverse of the covariance matrix generated by the exponential covariance function (also known as Ornstein-Uhlenbeck kernel) in $\mathbb{R}$, that is $K_{ij} = k(x_i, x_j) = \exp(-\frac{|x_i - x_j|}{\theta})$ Where $x_i \in \mathbb{R}$ and that $K$ is an $n\times n$ matrix. Here is what I've thought about doing: Since $K$ is a covariance matrix, I know it is positive semidefinite and so is $K^{-1}$. We can do a singular value decomposition on $K = UDU^*$ and $K^{-1} = VCV^*$ where $U, V$ are unitary matrices and $D, C$ are diagonal matrices. But that I haven't been able to develop the idea further. What would be right approach to get an analytical expression for $K^{-1}$?","I'm trying to analytically find the inverse of the covariance matrix generated by the exponential covariance function (also known as Ornstein-Uhlenbeck kernel) in $\mathbb{R}$, that is $K_{ij} = k(x_i, x_j) = \exp(-\frac{|x_i - x_j|}{\theta})$ Where $x_i \in \mathbb{R}$ and that $K$ is an $n\times n$ matrix. Here is what I've thought about doing: Since $K$ is a covariance matrix, I know it is positive semidefinite and so is $K^{-1}$. We can do a singular value decomposition on $K = UDU^*$ and $K^{-1} = VCV^*$ where $U, V$ are unitary matrices and $D, C$ are diagonal matrices. But that I haven't been able to develop the idea further. What would be right approach to get an analytical expression for $K^{-1}$?",,"['linear-algebra', 'machine-learning', 'covariance']"
93,Matrices that Differ only in Diagonal of Decomposition,Matrices that Differ only in Diagonal of Decomposition,,"Suppose that $\mathbf A_1$ and $\mathbf A_2$ are $n \times n$ matrices. Are there necessary and sufficient conditions such that there exists $n \times n$ matrices $\mathbf U$ and $\mathbf V$ and $n \times n$ diagonal matrices $\mathbf D_1$ and $\mathbf D_2$ that satisfy $$ \mathbf A_1 = \mathbf U \mathbf D_1 \mathbf V , \quad \text{and} \quad \mathbf A_2 = \mathbf U \mathbf D_2 \mathbf V?$$ A sufficient condition is for $\mathbf A_1$ and $\mathbf A_2$ to be invertible and have the same eigenvectors, then the result follows from the eigendecomposition. I was hoping to find a necessary condition perhaps using the LDU decomposition .","Suppose that $\mathbf A_1$ and $\mathbf A_2$ are $n \times n$ matrices. Are there necessary and sufficient conditions such that there exists $n \times n$ matrices $\mathbf U$ and $\mathbf V$ and $n \times n$ diagonal matrices $\mathbf D_1$ and $\mathbf D_2$ that satisfy $$ \mathbf A_1 = \mathbf U \mathbf D_1 \mathbf V , \quad \text{and} \quad \mathbf A_2 = \mathbf U \mathbf D_2 \mathbf V?$$ A sufficient condition is for $\mathbf A_1$ and $\mathbf A_2$ to be invertible and have the same eigenvectors, then the result follows from the eigendecomposition. I was hoping to find a necessary condition perhaps using the LDU decomposition .",,"['linear-algebra', 'matrices', 'matrix-decomposition']"
94,$n$-th root of $3 \times 3$ invertible matrix,-th root of  invertible matrix,n 3 \times 3,"Yo, I couldn't solve this exercise after thinking for a while. For every $A \in GL_{3} (\mathbb{C})$ and $n$, there's a $B \in Mat_{3, 3}(\mathbb{C})$ such that $B^n = A$ The previous exercise was that for every nilpotent $N \in Mat_{3, 3} (\mathbb{C})$ and every $n$, $C = 1 + \frac{1}{n}N + \frac{1-n}{2n^2}N^2$ satisfies $C^n = 1 + N$, so I suppose there's a trick using this result. I tried to play a little with the splitting of of $A$ as a nilpotent plus a semisimple, however I couldn't get anything useful. Thanks in advance.","Yo, I couldn't solve this exercise after thinking for a while. For every $A \in GL_{3} (\mathbb{C})$ and $n$, there's a $B \in Mat_{3, 3}(\mathbb{C})$ such that $B^n = A$ The previous exercise was that for every nilpotent $N \in Mat_{3, 3} (\mathbb{C})$ and every $n$, $C = 1 + \frac{1}{n}N + \frac{1-n}{2n^2}N^2$ satisfies $C^n = 1 + N$, so I suppose there's a trick using this result. I tried to play a little with the splitting of of $A$ as a nilpotent plus a semisimple, however I couldn't get anything useful. Thanks in advance.",,"['linear-algebra', 'jordan-normal-form', 'nilpotence', 'unipotent-matrices']"
95,Is there a general rule for how to write high order polynomials in matrix form?,Is there a general rule for how to write high order polynomials in matrix form?,,Is there a general rule for how to write high order polynomials in matrix form? For example a linear combination of parameters: $$a_1 x_1+a_2 x_2+a_3 x_3 + \cdots+ a_n x_n$$ Can be written as $$\sum^n_{i=1} a_i x_i = \vec{a}^T\vec{x} $$ Second order forms are given by $$ (a_1 x_1+a_2 x_2+a_3 x_3 + \cdots+ a_n x_n)^2 = \vec{x}^T {\mathbf A} \vec{x}$$ Which ensures all combinations of second order terms. What about the higher orders? i.e. $$(a_1 x_1+a_2 x_2+a_3 x_3 + \dots +a_n x_n)^k$$ What forms ensure all combinations of terms. Is there a general rule to this? Does it have a name?,Is there a general rule for how to write high order polynomials in matrix form? For example a linear combination of parameters: $$a_1 x_1+a_2 x_2+a_3 x_3 + \cdots+ a_n x_n$$ Can be written as $$\sum^n_{i=1} a_i x_i = \vec{a}^T\vec{x} $$ Second order forms are given by $$ (a_1 x_1+a_2 x_2+a_3 x_3 + \cdots+ a_n x_n)^2 = \vec{x}^T {\mathbf A} \vec{x}$$ Which ensures all combinations of second order terms. What about the higher orders? i.e. $$(a_1 x_1+a_2 x_2+a_3 x_3 + \dots +a_n x_n)^k$$ What forms ensure all combinations of terms. Is there a general rule to this? Does it have a name?,,['linear-algebra']
96,Prove that there is a vector $v\in \mathbb{R}^k$ such that $u \cdot v =0$,Prove that there is a vector  such that,v\in \mathbb{R}^k u \cdot v =0,"Let $u \in \mathbb{R}^k$ be a vector with one component positive, one component negative, and the remaining $k-2$ can have at most one component that is equal to zero. Then is there a vector $v \in \mathbb{R}^k$ such that all its components are strictly positive and $u \cdot v= 0$? Intuitively this seems to be true. But how can I go about showing this formally?","Let $u \in \mathbb{R}^k$ be a vector with one component positive, one component negative, and the remaining $k-2$ can have at most one component that is equal to zero. Then is there a vector $v \in \mathbb{R}^k$ such that all its components are strictly positive and $u \cdot v= 0$? Intuitively this seems to be true. But how can I go about showing this formally?",,['linear-algebra']
97,Is every totally ordered finite dimensional vector space a lexicographic order for some basis?,Is every totally ordered finite dimensional vector space a lexicographic order for some basis?,,"Let's say we have a finite-dimensional vector space $V$ over a totally ordered field $\mathbb{K}$. Is every choice of totally ordered vector space structure (i.e compatible with the addition and scalar multiplication) on $V$ a lexicographic order for some ordered basis? By ""compatible"" I mean that the translation and multiplication by non-negative scalars are order homomorphisms. Sorry if this is a dumb question; I know next to nothing about order theory. The reason I'm wondering is that I'm curious if a choice of base (i.e. simple roots) for a root system $\Phi$ in $V$ is equivalent to a choice of totally ordered vector space structure on $V$ by lexicographic ordering.","Let's say we have a finite-dimensional vector space $V$ over a totally ordered field $\mathbb{K}$. Is every choice of totally ordered vector space structure (i.e compatible with the addition and scalar multiplication) on $V$ a lexicographic order for some ordered basis? By ""compatible"" I mean that the translation and multiplication by non-negative scalars are order homomorphisms. Sorry if this is a dumb question; I know next to nothing about order theory. The reason I'm wondering is that I'm curious if a choice of base (i.e. simple roots) for a root system $\Phi$ in $V$ is equivalent to a choice of totally ordered vector space structure on $V$ by lexicographic ordering.",,"['linear-algebra', 'order-theory']"
98,Algorithm for finding orthogonal complement in inner product spaces,Algorithm for finding orthogonal complement in inner product spaces,,"Is there a general procedure for calculating a basis for the orthogonal complement of a given subspace (with a given basis)? For $\mathbb R^n$ this amounts to finding the nullspace of a matrix with the basis vectors for rows, but this is just because of the definition of the standard inner product. What about the general case?","Is there a general procedure for calculating a basis for the orthogonal complement of a given subspace (with a given basis)? For $\mathbb R^n$ this amounts to finding the nullspace of a matrix with the basis vectors for rows, but this is just because of the definition of the standard inner product. What about the general case?",,"['linear-algebra', 'orthogonality']"
99,Polar decomposition-uniqueness,Polar decomposition-uniqueness,,"Let $A$ be arbitrary $n \times n$ matrix (with complex entries). Then it can be expressed as $A=UP$ where $P=\sqrt{A^*A}$ and $U$ unitary. If $A$ is invertible then $U$ is uniquely determined. If $A$ is real then $P=\sqrt{A^TA}$ and $U$ is orthogonal. I'm interested in the following question: Is it always possible for a singular matrix $A$ to produce two different unitary matrices $U_1,U_2$ such that $A=U_1P=U_2P$ where $P$ as before?","Let $A$ be arbitrary $n \times n$ matrix (with complex entries). Then it can be expressed as $A=UP$ where $P=\sqrt{A^*A}$ and $U$ unitary. If $A$ is invertible then $U$ is uniquely determined. If $A$ is real then $P=\sqrt{A^TA}$ and $U$ is orthogonal. I'm interested in the following question: Is it always possible for a singular matrix $A$ to produce two different unitary matrices $U_1,U_2$ such that $A=U_1P=U_2P$ where $P$ as before?",,"['linear-algebra', 'matrices']"
