,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Sequence Limit: $\sin(n^3)$,Sequence Limit:,\sin(n^3),"How can I prove that this sequence does not converge, using  the definition? $$W_n = \sin(n^3)$$ For $n \in \mathbb{N}$. I tried to do a proof by reduction to the absurd but without result.","How can I prove that this sequence does not converge, using  the definition? $$W_n = \sin(n^3)$$ For $n \in \mathbb{N}$. I tried to do a proof by reduction to the absurd but without result.",,"['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
1,What's the $\lim_{m\to\infty}\prod_{k=1}^m (1-e^{-kn})$?,What's the ?,\lim_{m\to\infty}\prod_{k=1}^m (1-e^{-kn}),I try to show $\lim_{n\to\infty}\lim_{m\to\infty}\prod_{k=1}^m (1-e^{-kn})=1$. It seems we need to give a lower bound of $\lim_{m\to\infty}\prod_{k=1}^m (1-e^{-kn})$ depending on $n$ and as $n$ tends to infinity this lower bound tends to 1. I am trying to calculate $\log(\prod_{i=1}^m (1-e^{-in}))$ and see if it is closed to 0 with the fact that $\log(1-x)\approx -x$ as $x\to 0$. But I am not sure how to control the error.,I try to show $\lim_{n\to\infty}\lim_{m\to\infty}\prod_{k=1}^m (1-e^{-kn})=1$. It seems we need to give a lower bound of $\lim_{m\to\infty}\prod_{k=1}^m (1-e^{-kn})$ depending on $n$ and as $n$ tends to infinity this lower bound tends to 1. I am trying to calculate $\log(\prod_{i=1}^m (1-e^{-in}))$ and see if it is closed to 0 with the fact that $\log(1-x)\approx -x$ as $x\to 0$. But I am not sure how to control the error.,,"['calculus', 'real-analysis', 'analysis', 'limits']"
2,This Improper integral triggers me,This Improper integral triggers me,,"I'm an IT engineering student that for the past 20 minutes tried to solve this improper integral without succeeding .. Could you help me, please? The integral is kind of an easy one: $$  I =\int_3^{+\infty} \frac{2x-6}{(2x^2+9)(x^2 -2x)} \,dx$$ After some calculus I've found that the indefinite integral is $$ \frac 1{51}(17\ln|x| -3\ln|x-2| - 7\ln(2x^2 +9) +2 \sqrt 2 \arctan(\frac{\sqrt 2}3x) + C $$ Now I should continue with the  $$\lim_{x\to +\infty} f(x) $$ but I find some difficulties.. This is the solution: $$ I =\frac{\sqrt 2}{51}(\pi -2 \arctan(\sqrt 2)) + \frac4{51}\ln(3) - \frac7{51} \ln(2) $$ I don't know where I get wrong because if I do the limit, I get $$\lim_{x\to +\infty} \frac 1{51}(17\ln|x| -3\ln|x-2| - 7\ln(2x^2 +9) +2 \sqrt 2 \arctan(\frac{\sqrt 2}3x) = \frac{\sqrt 2\pi}{51} $$ ""because arctan is faster than the natural logarithm""... But maybe it's here my mistake.. I KNOW for sure that I've made a mistake in this limit... I feel it. I haven't considered something. I've done so many limits that when I get wrong one, I know it ahah.. Thanks in advance guys! Enjoy :) EDIT AFTER SOLUTION: Basically thanks to Doug M and the other members, I understood that I had done a STUPID mistake in the limit and so :  $$\lim_{x\to +\infty} (17\ln|x| -3\ln|x-2| - 7\ln(2x^2 +9)  = $$ $$\lim_{x\to +\infty} ln|\frac{x^{17}}{(x-2)^3(2x^2 +9)^7}|\simeq ln|\frac{x^{17}}{(2)^7(x)^{17} + O(x^{17})}|\simeq ln|\frac1{(2)^7}|\simeq -7ln(2)$$ So the correct whole limit is: $$\lim_{x\to +\infty} \frac 1{51}(17\ln|x| -3\ln|x-2| - 7\ln(2x^2 +9) +2 \sqrt 2 \arctan(\frac{\sqrt 2}3x) = \frac1{51}(\sqrt 2\pi - 7ln(2)) $$ In order to continue this triggering integral (because if you start something in math, you have to see it through), I need to evaluate the expression at x = 3 and I get: $$ \frac 1{51}(-4\ln(3) +2 \sqrt 2 \arctan(\sqrt 2))$$ And so FINALLY the integral becomes: $$ I =\frac1{51}(\sqrt 2\pi - 7ln(2)) - [\frac 1{51}(-4\ln(3) +2 \sqrt 2 \arctan(\sqrt 2))] $$ And I got the solution :) $$ I =\frac{\sqrt 2}{51}(\pi -2 \arctan(\sqrt 2)) + \frac4{51}\ln(3) - \frac7{51} \ln(2) $$","I'm an IT engineering student that for the past 20 minutes tried to solve this improper integral without succeeding .. Could you help me, please? The integral is kind of an easy one: $$  I =\int_3^{+\infty} \frac{2x-6}{(2x^2+9)(x^2 -2x)} \,dx$$ After some calculus I've found that the indefinite integral is $$ \frac 1{51}(17\ln|x| -3\ln|x-2| - 7\ln(2x^2 +9) +2 \sqrt 2 \arctan(\frac{\sqrt 2}3x) + C $$ Now I should continue with the  $$\lim_{x\to +\infty} f(x) $$ but I find some difficulties.. This is the solution: $$ I =\frac{\sqrt 2}{51}(\pi -2 \arctan(\sqrt 2)) + \frac4{51}\ln(3) - \frac7{51} \ln(2) $$ I don't know where I get wrong because if I do the limit, I get $$\lim_{x\to +\infty} \frac 1{51}(17\ln|x| -3\ln|x-2| - 7\ln(2x^2 +9) +2 \sqrt 2 \arctan(\frac{\sqrt 2}3x) = \frac{\sqrt 2\pi}{51} $$ ""because arctan is faster than the natural logarithm""... But maybe it's here my mistake.. I KNOW for sure that I've made a mistake in this limit... I feel it. I haven't considered something. I've done so many limits that when I get wrong one, I know it ahah.. Thanks in advance guys! Enjoy :) EDIT AFTER SOLUTION: Basically thanks to Doug M and the other members, I understood that I had done a STUPID mistake in the limit and so :  $$\lim_{x\to +\infty} (17\ln|x| -3\ln|x-2| - 7\ln(2x^2 +9)  = $$ $$\lim_{x\to +\infty} ln|\frac{x^{17}}{(x-2)^3(2x^2 +9)^7}|\simeq ln|\frac{x^{17}}{(2)^7(x)^{17} + O(x^{17})}|\simeq ln|\frac1{(2)^7}|\simeq -7ln(2)$$ So the correct whole limit is: $$\lim_{x\to +\infty} \frac 1{51}(17\ln|x| -3\ln|x-2| - 7\ln(2x^2 +9) +2 \sqrt 2 \arctan(\frac{\sqrt 2}3x) = \frac1{51}(\sqrt 2\pi - 7ln(2)) $$ In order to continue this triggering integral (because if you start something in math, you have to see it through), I need to evaluate the expression at x = 3 and I get: $$ \frac 1{51}(-4\ln(3) +2 \sqrt 2 \arctan(\sqrt 2))$$ And so FINALLY the integral becomes: $$ I =\frac1{51}(\sqrt 2\pi - 7ln(2)) - [\frac 1{51}(-4\ln(3) +2 \sqrt 2 \arctan(\sqrt 2))] $$ And I got the solution :) $$ I =\frac{\sqrt 2}{51}(\pi -2 \arctan(\sqrt 2)) + \frac4{51}\ln(3) - \frac7{51} \ln(2) $$",,"['analysis', 'limits', 'improper-integrals']"
3,A limit and a delta function in an integral.,A limit and a delta function in an integral.,,"Let's say I want to calculate the integral \begin{equation} \lim_{\gamma\to0}\frac{1}{2\pi i}\int^\infty_{-\infty}dx\left(\frac{1}{x-a+i\gamma}-\frac{1}{x-a-i\gamma}\right)\frac{-1}{e^x+1}, \end{equation} ($a\in\mathbb{R}$) and somebody says to me that the relation \begin{equation} \frac{1}{\pi}\lim_{\gamma\to0}\frac{\gamma}{x^2+\gamma^2}=\delta(x) \end{equation} might be useful. Rewriting the fractions in the integrand I write the integral as \begin{equation} \lim_{\gamma\to0}\frac{1}{\pi}\int^\infty_{-\infty}dx\,\frac{\gamma}{(x-a)^2+\gamma^2}\frac{1}{e^x+1}, \end{equation} and being willing to use the given relation I might want to continue to write \begin{equation} \int^\infty_{-\infty}\frac{1}{\pi}\lim_{\gamma\to0}\frac{\gamma}{x^2+\gamma^2}\frac{1}{e^x+1}=\frac{1}{e^a+1}, \end{equation} and be happy because this is the answer we are supposed to find. However, I'm not sure as to why this works like this (if it does). I know that interchanging limits and integrals can be a delicate business, and if you can pull the limit inside the integral here, then what is wrong with writing \begin{equation} \lim_{\gamma\to0}\left(\frac{1}{x-a+i\gamma}-\frac{1}{x-a-i\gamma}\right)=\frac{1}{x-a}-\frac{1}{x-a}=0? \end{equation} I'd be happy with any help!","Let's say I want to calculate the integral \begin{equation} \lim_{\gamma\to0}\frac{1}{2\pi i}\int^\infty_{-\infty}dx\left(\frac{1}{x-a+i\gamma}-\frac{1}{x-a-i\gamma}\right)\frac{-1}{e^x+1}, \end{equation} ($a\in\mathbb{R}$) and somebody says to me that the relation \begin{equation} \frac{1}{\pi}\lim_{\gamma\to0}\frac{\gamma}{x^2+\gamma^2}=\delta(x) \end{equation} might be useful. Rewriting the fractions in the integrand I write the integral as \begin{equation} \lim_{\gamma\to0}\frac{1}{\pi}\int^\infty_{-\infty}dx\,\frac{\gamma}{(x-a)^2+\gamma^2}\frac{1}{e^x+1}, \end{equation} and being willing to use the given relation I might want to continue to write \begin{equation} \int^\infty_{-\infty}\frac{1}{\pi}\lim_{\gamma\to0}\frac{\gamma}{x^2+\gamma^2}\frac{1}{e^x+1}=\frac{1}{e^a+1}, \end{equation} and be happy because this is the answer we are supposed to find. However, I'm not sure as to why this works like this (if it does). I know that interchanging limits and integrals can be a delicate business, and if you can pull the limit inside the integral here, then what is wrong with writing \begin{equation} \lim_{\gamma\to0}\left(\frac{1}{x-a+i\gamma}-\frac{1}{x-a-i\gamma}\right)=\frac{1}{x-a}-\frac{1}{x-a}=0? \end{equation} I'd be happy with any help!",,"['integration', 'limits', 'definite-integrals', 'improper-integrals']"
4,Derivative of function defined on only rational numbers,Derivative of function defined on only rational numbers,,"Let $f:\mathbb{Q}\to\mathbb{R}$ be uniformly continuous and assume that $f':\mathbb{Q}\to\mathbb{R}$ is uniformly continuous as well. Let $a$ be an irrational number. I need help to prove that $$\lim_{q\to a}f'(q)=\lim_{q\to a}\frac{(\lim_{p\to a}f(p))-f(q)}{a-q}$$ Motivation for the question: $f'(a)$ does not exist, but both the left-hand side and the right-hand side of the equation is an intuitively reasonable substitute for it: on the LHS we first differentiate and then take the limit to $a$, and on the RHS we first take the limit to $a$ and then differentiate. It would therefore be nice to prove that they are equal.","Let $f:\mathbb{Q}\to\mathbb{R}$ be uniformly continuous and assume that $f':\mathbb{Q}\to\mathbb{R}$ is uniformly continuous as well. Let $a$ be an irrational number. I need help to prove that $$\lim_{q\to a}f'(q)=\lim_{q\to a}\frac{(\lim_{p\to a}f(p))-f(q)}{a-q}$$ Motivation for the question: $f'(a)$ does not exist, but both the left-hand side and the right-hand side of the equation is an intuitively reasonable substitute for it: on the LHS we first differentiate and then take the limit to $a$, and on the RHS we first take the limit to $a$ and then differentiate. It would therefore be nice to prove that they are equal.",,"['real-analysis', 'limits', 'derivatives']"
5,Behaviour of asymptotically equivalent functions after iterative exponentiation,Behaviour of asymptotically equivalent functions after iterative exponentiation,,"For the purposes of the question, define $\exp^n x$ as the $n$ times iteration of $\exp$ (e.g $\exp^2 x = \exp \exp x$ ) for $n\geq1$ and $\exp^0 x = x$ . Let $f,g:\mathbb{R}_{>0}\to \mathbb{R}_{>0}$ such that $f,g$ are strictly increasing and unbounded above. If for all $n \geq 0$ , $$\exp^n f(x) \sim \exp^n g(x) ,\ \ x \to +\infty $$ does it follow that $f(x) = g(x)$ for sufficiently large $x$ ? If so, can the hypotheses be weaked? If not, are there hypotheses which can be added (e.g continuity)? The motivation is this: even if $f \sim g$ as $x \to +\infty$ , $f$ and $g$ are not really equivalent in the sense that for any well behaved function $h$ , we don't necessarily have $h \circ f \sim h \circ g$ at $+\infty$ . For example, if $f \sim g$ but $|f - g|$ does not go to zero as $x \to +\infty$ , $\exp f$ is not $\sim \exp g$ . Therefore, ""highly"" equivalent functions satisfying the hypotheses must have differences which tend to $0$ . That's not sufficient, though. For example, $f(x) = x$ , $g(x) = x + \frac 1x$ fails for $n=2$ .","For the purposes of the question, define as the times iteration of (e.g ) for and . Let such that are strictly increasing and unbounded above. If for all , does it follow that for sufficiently large ? If so, can the hypotheses be weaked? If not, are there hypotheses which can be added (e.g continuity)? The motivation is this: even if as , and are not really equivalent in the sense that for any well behaved function , we don't necessarily have at . For example, if but does not go to zero as , is not . Therefore, ""highly"" equivalent functions satisfying the hypotheses must have differences which tend to . That's not sufficient, though. For example, , fails for .","\exp^n x n \exp \exp^2 x = \exp \exp x n\geq1 \exp^0 x = x f,g:\mathbb{R}_{>0}\to \mathbb{R}_{>0} f,g n \geq 0 \exp^n f(x) \sim \exp^n g(x) ,\ \ x \to +\infty  f(x) = g(x) x f \sim g x \to +\infty f g h h \circ f \sim h \circ g +\infty f \sim g |f - g| x \to +\infty \exp f \sim \exp g 0 f(x) = x g(x) = x + \frac 1x n=2","['calculus', 'limits', 'asymptotics']"
6,Find $\lim_{x\to\infty} x^{\sin(1/x)}$,Find,\lim_{x\to\infty} x^{\sin(1/x)},How to find $\lim_{x\to\infty} x^{\sin(1/x)}$? I tried $$\lim_{x\to\infty} x^{\sin(1/x)}=\lim_{x\to\infty}e^{\sin(1/x)\ln(x)}$$ Then $$\lim_{x\to\infty}\sin\left(\frac{1}{x}\right)\ln(x)=\lim_{x\to\infty}\frac{\sin(1/x)}{\frac{1}{\ln(x)}}=\lim_{x\to\infty}\frac{\cos(1/x)}{x^2}x\ln^2(x)=\lim_{x\to\infty}\frac{1}{x}\cos\left(\frac{1}{x}\right)\ln^2(x)$$ Which doesn't look promising.,How to find $\lim_{x\to\infty} x^{\sin(1/x)}$? I tried $$\lim_{x\to\infty} x^{\sin(1/x)}=\lim_{x\to\infty}e^{\sin(1/x)\ln(x)}$$ Then $$\lim_{x\to\infty}\sin\left(\frac{1}{x}\right)\ln(x)=\lim_{x\to\infty}\frac{\sin(1/x)}{\frac{1}{\ln(x)}}=\lim_{x\to\infty}\frac{\cos(1/x)}{x^2}x\ln^2(x)=\lim_{x\to\infty}\frac{1}{x}\cos\left(\frac{1}{x}\right)\ln^2(x)$$ Which doesn't look promising.,,['limits']
7,Extending 'Guess 2/3 of the Average' Game,Extending 'Guess 2/3 of the Average' Game,,"In game theory, 'Guess $\frac{2}3$ of the Average' is a game where $n$ people are asked to choose a real number between $0$ and $100$ inclusive. The person with the closest answer to $\frac{2}3$ of the average value wins. It can be shown that there is a unique pure strategy Nash equilibrium where everyone picks the number $0$. (The reasoning is that the desired number can't be greater than $\frac{2}3 \cdot 100$ so everyone picks between $0$ and $\frac{2}3 \cdot 100$. Then iterate.) Can we generalize this game. Specifically, say that you survey $n$ people who each pick a real number from a set $S \subset \mathbb{R}$. From their responses, you can form a vector $\vec{v}$ where the $i$ th entry is the $i$ th person's response. Now suppose you have function $f : \mathbb{R}^n \rightarrow \mathbb{R}$ that is known to all of the $n$ people. The person whose value is closest to $f(\vec{v})$ is the winner. What strategy should each of the $n$ people employ? Can we find essentially different functions and sets such that everyone ends up guessing the same number? For example, we could have chosen a different fraction $< 1$ rather than $\frac{2}3$ but in spirit, this is the same example. There are also examples of functions and sets where everyone does not end up picking the same answer. For example, if we restrict the people in our example above to choose only integers, then the strategy is pick $1$ or $0$ depending on what you think the majority of the people will pick.","In game theory, 'Guess $\frac{2}3$ of the Average' is a game where $n$ people are asked to choose a real number between $0$ and $100$ inclusive. The person with the closest answer to $\frac{2}3$ of the average value wins. It can be shown that there is a unique pure strategy Nash equilibrium where everyone picks the number $0$. (The reasoning is that the desired number can't be greater than $\frac{2}3 \cdot 100$ so everyone picks between $0$ and $\frac{2}3 \cdot 100$. Then iterate.) Can we generalize this game. Specifically, say that you survey $n$ people who each pick a real number from a set $S \subset \mathbb{R}$. From their responses, you can form a vector $\vec{v}$ where the $i$ th entry is the $i$ th person's response. Now suppose you have function $f : \mathbb{R}^n \rightarrow \mathbb{R}$ that is known to all of the $n$ people. The person whose value is closest to $f(\vec{v})$ is the winner. What strategy should each of the $n$ people employ? Can we find essentially different functions and sets such that everyone ends up guessing the same number? For example, we could have chosen a different fraction $< 1$ rather than $\frac{2}3$ but in spirit, this is the same example. There are also examples of functions and sets where everyone does not end up picking the same answer. For example, if we restrict the people in our example above to choose only integers, then the strategy is pick $1$ or $0$ depending on what you think the majority of the people will pick.",,"['limits', 'functions', 'game-theory']"
8,Spivak Problem 18-31 (b),Spivak Problem 18-31 (b),,"Problem Description:  Evaluate the following limit:  $\displaystyle\lim_{x \to \infty }{e^{-x^2}\int_{x}^{x+\frac{1}{x}} e^{t^2}dt}$. The solution books uses L'Hopital's rule, and does as follows: $\displaystyle\lim_{x \to \infty }\frac{\displaystyle\int_{x}^{x+\frac{1}{x}} e^{t^2}dt}{e^{x^2}}$ $=\displaystyle\lim_{x \to \infty }\frac{e^{{(x+\frac{1}{x})}^2} - e^{x^2}}{2xe^{x^2}}$. My concern is that shouldn't it be  $\displaystyle\lim_{x \to \infty }\frac{\displaystyle\int_{x}^{x+\frac{1}{x}} e^{t^2}dt}{e^{x^2}}$ $=\displaystyle\lim_{x \to \infty }\frac{e^{(x+\frac{1}{x})^2)}(1- \frac{1}{x^2}) - e^{x^2}}{2xe^{x^2}}$.","Problem Description:  Evaluate the following limit:  $\displaystyle\lim_{x \to \infty }{e^{-x^2}\int_{x}^{x+\frac{1}{x}} e^{t^2}dt}$. The solution books uses L'Hopital's rule, and does as follows: $\displaystyle\lim_{x \to \infty }\frac{\displaystyle\int_{x}^{x+\frac{1}{x}} e^{t^2}dt}{e^{x^2}}$ $=\displaystyle\lim_{x \to \infty }\frac{e^{{(x+\frac{1}{x})}^2} - e^{x^2}}{2xe^{x^2}}$. My concern is that shouldn't it be  $\displaystyle\lim_{x \to \infty }\frac{\displaystyle\int_{x}^{x+\frac{1}{x}} e^{t^2}dt}{e^{x^2}}$ $=\displaystyle\lim_{x \to \infty }\frac{e^{(x+\frac{1}{x})^2)}(1- \frac{1}{x^2}) - e^{x^2}}{2xe^{x^2}}$.",,"['calculus', 'limits', 'derivatives']"
9,Horizontal Asymptotes,Horizontal Asymptotes,,"Find all asymptotes of: $$f(x) = \frac{a + be^x}{ae^x+b}$$ The way I've been taught is that the $+a$ and $+b$ in the numerator and denominator respectively do not contribute when x tends to infinity, therefore are negligible. Left with $f(x) = \frac{be^x}{ae^x} = \frac{b}{a}$, $y = \frac{b}{a}$  is the only asymptote I was able to identify (through this method). However, plotting the function with $a = 3$ and $b = 2$, there is clearly another horizontal asymptote, where $y = \frac{a}{b}$: Is there any way I could've known about the second asymptote without graphing?","Find all asymptotes of: $$f(x) = \frac{a + be^x}{ae^x+b}$$ The way I've been taught is that the $+a$ and $+b$ in the numerator and denominator respectively do not contribute when x tends to infinity, therefore are negligible. Left with $f(x) = \frac{be^x}{ae^x} = \frac{b}{a}$, $y = \frac{b}{a}$  is the only asymptote I was able to identify (through this method). However, plotting the function with $a = 3$ and $b = 2$, there is clearly another horizontal asymptote, where $y = \frac{a}{b}$: Is there any way I could've known about the second asymptote without graphing?",,"['limits', 'graphing-functions']"
10,help on limit exercise,help on limit exercise,,I'm having some trouble solving this limit: $$\lim_{x\to\infty} x\left[\left(\cosh x\right)^ \frac1x - \left(1+\frac1x\right)^x\right]$$ It's part of a set of limits wich should be solved using taylor. I tried this road: $$\lim_{x\to\infty} x\left[e^{\frac1x\ln(\cosh x)}-e^{\ln\left(1+\frac1x\right)x}\right]$$ I then tried with algebraic manipulation using the definition of hyperbolic cosine $\frac12(e^x+e^{-x})$ and then I also played a bit with l'Hopital but it turns into something suspiciously compicated... I'm taking real analysis 1 and my toolset is: -algebraic manipulation -talyor series -l'Hopital (if and only if all else fails) I can't use more advanced techniques since they are not part of the course. I'm sure there's something obvious I'm missing. Any idea on how to proceed?,I'm having some trouble solving this limit: $$\lim_{x\to\infty} x\left[\left(\cosh x\right)^ \frac1x - \left(1+\frac1x\right)^x\right]$$ It's part of a set of limits wich should be solved using taylor. I tried this road: $$\lim_{x\to\infty} x\left[e^{\frac1x\ln(\cosh x)}-e^{\ln\left(1+\frac1x\right)x}\right]$$ I then tried with algebraic manipulation using the definition of hyperbolic cosine $\frac12(e^x+e^{-x})$ and then I also played a bit with l'Hopital but it turns into something suspiciously compicated... I'm taking real analysis 1 and my toolset is: -algebraic manipulation -talyor series -l'Hopital (if and only if all else fails) I can't use more advanced techniques since they are not part of the course. I'm sure there's something obvious I'm missing. Any idea on how to proceed?,,"['calculus', 'real-analysis', 'limits']"
11,Why L'hopital's rule only proved for indetermined forms?,Why L'hopital's rule only proved for indetermined forms?,,"In this proof of L'hopital's rule, $\lim\limits_{x\to a}\, f(x)=0$ and $g(x)=0$ seems have no role to paly. So what goes wrong when the limits mentioned before are not equal to $0$? (My guess is that you can't assume that ""f(a)=g(a)=$0$"" without making the functions discontinuous when those limit are not equal to $0$. Is that right?) thanks!","In this proof of L'hopital's rule, $\lim\limits_{x\to a}\, f(x)=0$ and $g(x)=0$ seems have no role to paly. So what goes wrong when the limits mentioned before are not equal to $0$? (My guess is that you can't assume that ""f(a)=g(a)=$0$"" without making the functions discontinuous when those limit are not equal to $0$. Is that right?) thanks!",,"['calculus', 'limits']"
12,"If $s(1)=1$ and $s(n)=s(n-1)+\text{lcm}[n,s(n-1)]-(-1)^n$ then $\lim\limits_{n\to \infty}\frac{s(n)}{(n+1)!}=\frac{1}{e}$",If  and  then,"s(1)=1 s(n)=s(n-1)+\text{lcm}[n,s(n-1)]-(-1)^n \lim\limits_{n\to \infty}\frac{s(n)}{(n+1)!}=\frac{1}{e}","Define the sequence $(s(n))$ recursively by $s(1)=1$ and, for every $n\ge2$,   $$s(n)=s(n-1)+\text{lcm}[n,s(n-1)]-(-1)^n.$$ Prove that   $$\lim_{n\to \infty}\frac{s(n)}{(n+1)!}=\frac{1}{e}$$ I got this idea from a paper written by Eric Roland on recurrence relation on generating primes, the last part of the paper is where the actual idea comes from (Benoit Cloitre). We noticed that this sequence converges to $e$ but we do not know how to prove it.","Define the sequence $(s(n))$ recursively by $s(1)=1$ and, for every $n\ge2$,   $$s(n)=s(n-1)+\text{lcm}[n,s(n-1)]-(-1)^n.$$ Prove that   $$\lim_{n\to \infty}\frac{s(n)}{(n+1)!}=\frac{1}{e}$$ I got this idea from a paper written by Eric Roland on recurrence relation on generating primes, the last part of the paper is where the actual idea comes from (Benoit Cloitre). We noticed that this sequence converges to $e$ but we do not know how to prove it.",,['limits']
13,A limit of the p-th power of a function integral [duplicate],A limit of the p-th power of a function integral [duplicate],,"This question already has answers here : ""Scaled $L^p$ norm"" and geometric mean (3 answers) Closed 8 years ago . What is $$\lim_{p\to0}\left(\int_0^1\left(f(x)^p\right)dx\right)^{1/p},$$ where $f$ is continuous? It was an exam question and I don't know how to even get started with it, could you help? (It is not $f(x^p) $ but $\left(f(x)^p\right)$ so the $p$ -th power of $f(x)$ )","This question already has answers here : ""Scaled $L^p$ norm"" and geometric mean (3 answers) Closed 8 years ago . What is where is continuous? It was an exam question and I don't know how to even get started with it, could you help? (It is not but so the -th power of )","\lim_{p\to0}\left(\int_0^1\left(f(x)^p\right)dx\right)^{1/p}, f f(x^p)  \left(f(x)^p\right) p f(x)","['integration', 'limits']"
14,How can we compute $\sqrt{4-\sqrt{4-\sqrt{4-\cdots}}}$? Is it $\frac{\sqrt{17}-1}{2}$?,How can we compute ? Is it ?,\sqrt{4-\sqrt{4-\sqrt{4-\cdots}}} \frac{\sqrt{17}-1}{2},"How can we compute $\sqrt{4-\sqrt{4-\sqrt{4-\cdots}}}$? I can understand that if we define  $a_1=\sqrt{4}$ and $a_n=\sqrt{4-a_{n-1}}$ for $n>1$ which gives (also using with monotone convergent theorem) $$\lim a_n=\frac{\sqrt{17}-1}2.$$ But we can write it as $a_1=\sqrt{4-\sqrt{4}}$ and then $a_n=\sqrt{4-\sqrt{4-\sqrt{4-a_{n-1}}}}$. If there is a limit $L$ (I think we can use monotone convergent theorem), I solved with Wolfram that $$L=\sqrt{4-\sqrt{4-\sqrt{4-L}}}$$ and it compute $L=\frac{\sqrt{17}-1}{2}$ again. But I saw kind of continued fraction and gives different answers. (I sware, but not necessary, finding examples are not hard). Is  there a definition of this kind of nested numbers{?!}/limits?","How can we compute $\sqrt{4-\sqrt{4-\sqrt{4-\cdots}}}$? I can understand that if we define  $a_1=\sqrt{4}$ and $a_n=\sqrt{4-a_{n-1}}$ for $n>1$ which gives (also using with monotone convergent theorem) $$\lim a_n=\frac{\sqrt{17}-1}2.$$ But we can write it as $a_1=\sqrt{4-\sqrt{4}}$ and then $a_n=\sqrt{4-\sqrt{4-\sqrt{4-a_{n-1}}}}$. If there is a limit $L$ (I think we can use monotone convergent theorem), I solved with Wolfram that $$L=\sqrt{4-\sqrt{4-\sqrt{4-L}}}$$ and it compute $L=\frac{\sqrt{17}-1}{2}$ again. But I saw kind of continued fraction and gives different answers. (I sware, but not necessary, finding examples are not hard). Is  there a definition of this kind of nested numbers{?!}/limits?",,"['sequences-and-series', 'limits']"
15,Multivariable Limit,Multivariable Limit,,"I am trying to  find the following limit: $$\lim_{(x,y) \to (0,0)} \frac{\ln(e+3x^2+3y^2+x^2y^3+x^3y^2)-\sqrt{1+2x^2+2y^2+x^4+y^4}}{\ln(1+x^2+y^2+x^4+y^4)}$$ I solved the limit along $x=0$ , $y=0$ and $x=y$ and I got $\frac{3}{e} -1$ for all of them. I then used wolfarm to check whether the limit exists or not , it exist and equals to the value mentioned above, but I'm not able to find a way to show that along all the paths to $(0,0)$ the limit equals to $\frac{3}{e} -1$","I am trying to  find the following limit: $$\lim_{(x,y) \to (0,0)} \frac{\ln(e+3x^2+3y^2+x^2y^3+x^3y^2)-\sqrt{1+2x^2+2y^2+x^4+y^4}}{\ln(1+x^2+y^2+x^4+y^4)}$$ I solved the limit along $x=0$ , $y=0$ and $x=y$ and I got $\frac{3}{e} -1$ for all of them. I then used wolfarm to check whether the limit exists or not , it exist and equals to the value mentioned above, but I'm not able to find a way to show that along all the paths to $(0,0)$ the limit equals to $\frac{3}{e} -1$",,"['calculus', 'limits', 'multivariable-calculus']"
16,Cesaro mean of Cesaro means,Cesaro mean of Cesaro means,,"Is it possible to construct a bounded positive sequence $a_i$, ($0 < a_i < K < \infty$) such that the limit of its Cesaro mean does not exist but the limit of the Cesaro mean of its Cesaro means does exist? That is a sequence $a_i$ s.t. : $$ \nexists \lim_{n \to \infty} \frac{\sum_{i=1}^n a_i}{n} $$ but $$ \exists \lim_{N \to \infty} \frac{\sum_{n=1}^N\frac{\sum_{i=1}^n a_i}{n}}{N} $$","Is it possible to construct a bounded positive sequence $a_i$, ($0 < a_i < K < \infty$) such that the limit of its Cesaro mean does not exist but the limit of the Cesaro mean of its Cesaro means does exist? That is a sequence $a_i$ s.t. : $$ \nexists \lim_{n \to \infty} \frac{\sum_{i=1}^n a_i}{n} $$ but $$ \exists \lim_{N \to \infty} \frac{\sum_{n=1}^N\frac{\sum_{i=1}^n a_i}{n}}{N} $$",,"['real-analysis', 'sequences-and-series', 'limits', 'means', 'cesaro-summable']"
17,"Using properties of limits, calculate $\lim_{n\to \infty}\left(\frac1{n^2}+\frac1{(n+1)^2}+\cdots+\frac1{(2n)^2}\right)$","Using properties of limits, calculate",\lim_{n\to \infty}\left(\frac1{n^2}+\frac1{(n+1)^2}+\cdots+\frac1{(2n)^2}\right),"Using the properties of limits, calculate the following limits, if they exist. If not, prove they do not exist: $$\lim_{n\to \infty}\left(\frac1{n^2}+\frac1{(n+1)^2}+\frac1{(n+2)^2}+\cdots+\frac1{(2n)^2}\right)$$ This is what I have done, I have expressed the limit in the form: $\lim_{n\to \infty}\frac1{(n+a)^2}$ where 'a' belongs to the reals. Then using the $\epsilon-N$ definition of limits, I assumed that: $$\lim_{n\to \infty}\frac1{(n+a)^2}=0$$ and carried forward with the proof. I would like to use the $\epsilon-N$ definition of limits since it is what we are covering right now, is this the right way of solving this problem?","Using the properties of limits, calculate the following limits, if they exist. If not, prove they do not exist: This is what I have done, I have expressed the limit in the form: where 'a' belongs to the reals. Then using the definition of limits, I assumed that: and carried forward with the proof. I would like to use the definition of limits since it is what we are covering right now, is this the right way of solving this problem?",\lim_{n\to \infty}\left(\frac1{n^2}+\frac1{(n+1)^2}+\frac1{(n+2)^2}+\cdots+\frac1{(2n)^2}\right) \lim_{n\to \infty}\frac1{(n+a)^2} \epsilon-N \lim_{n\to \infty}\frac1{(n+a)^2}=0 \epsilon-N,"['sequences-and-series', 'analysis', 'limits']"
18,Pointwise convergence of series $\sum_{n=1}^{\infty}\frac{1}{n}e^{-nx}$,Pointwise convergence of series,\sum_{n=1}^{\infty}\frac{1}{n}e^{-nx},"For my math course I have to show that the series $$\sum_{n=1}^{\infty}\frac{1}{n}e^{-nx}$$ converges pointwise on $I=(0,\infty)$ and find the limit. I think I should prove that $f_n$ is a Cauchy sequence which would show that the series converges. So what I have so far is this: We want to show that $\forall \varepsilon>0$ there exists an $N\in\mathbb{N}$ such that for $m,n\geqslant N$ we have $|{f_m-f_n}|<\varepsilon$. Also take $m>n$.  Filling this in gives $$|f_m-f_n|=|\frac{e^{-mx}}{m}-\frac{e^{-nx}}{n}|=\frac{e^{-nx}}{n}-\frac{e^{-mx}}{m}\leqslant\frac{e^{-nx}}{N}-\frac{e^{-mx}}{N}=\frac{e^{-nx}-e^{-mx}}{N}\leqslant\frac{1}{N}<\varepsilon$$ So we pick our $N$ to be $N>\frac{1}{\varepsilon}$. And then we have proven the convergence. I am however not really sure if the step where I go from $n$ and $m$ in the denominator to $N$ is actually true. Also, I've tried writing out this sequence to see what this limit might be, but I'm getting nowhere, really. So any tips on either proving the pointwise convergence and finding the limit is very much appreciated!","For my math course I have to show that the series $$\sum_{n=1}^{\infty}\frac{1}{n}e^{-nx}$$ converges pointwise on $I=(0,\infty)$ and find the limit. I think I should prove that $f_n$ is a Cauchy sequence which would show that the series converges. So what I have so far is this: We want to show that $\forall \varepsilon>0$ there exists an $N\in\mathbb{N}$ such that for $m,n\geqslant N$ we have $|{f_m-f_n}|<\varepsilon$. Also take $m>n$.  Filling this in gives $$|f_m-f_n|=|\frac{e^{-mx}}{m}-\frac{e^{-nx}}{n}|=\frac{e^{-nx}}{n}-\frac{e^{-mx}}{m}\leqslant\frac{e^{-nx}}{N}-\frac{e^{-mx}}{N}=\frac{e^{-nx}-e^{-mx}}{N}\leqslant\frac{1}{N}<\varepsilon$$ So we pick our $N$ to be $N>\frac{1}{\varepsilon}$. And then we have proven the convergence. I am however not really sure if the step where I go from $n$ and $m$ in the denominator to $N$ is actually true. Also, I've tried writing out this sequence to see what this limit might be, but I'm getting nowhere, really. So any tips on either proving the pointwise convergence and finding the limit is very much appreciated!",,"['sequences-and-series', 'limits', 'convergence-divergence', 'weak-convergence']"
19,Prove that lim ${a_n}$ = 0 implies lim $\sqrt {a_n}$ = 0,Prove that lim  = 0 implies lim  = 0,{a_n} \sqrt {a_n},"How do you prove that given a non-negative sequence ${a_n}$, lim ${a_n}$ = 0 implies lim $\sqrt {a_n}$ = 0? By definition of limit in analysis I can assume that given $ϵ>0$, I have $a_n-0<ϵ$, which is just $a_n<ϵ$, for n>>1. If I can claim that $\sqrt {a_n} < {a_n} < ϵ$ for all $a_n$ I may be able to finish the proof, but that is not true for $0\lt a_n\lt1$. How should I approach this?","How do you prove that given a non-negative sequence ${a_n}$, lim ${a_n}$ = 0 implies lim $\sqrt {a_n}$ = 0? By definition of limit in analysis I can assume that given $ϵ>0$, I have $a_n-0<ϵ$, which is just $a_n<ϵ$, for n>>1. If I can claim that $\sqrt {a_n} < {a_n} < ϵ$ for all $a_n$ I may be able to finish the proof, but that is not true for $0\lt a_n\lt1$. How should I approach this?",,"['real-analysis', 'limits']"
20,Change of order of limits,Change of order of limits,,"Is it OK to change the order of the limits here : $$ \lim\limits_{n \rightarrow \infty} \lim\limits_{m \rightarrow \infty}\frac{1}{2\pi} \int_{0}^{2\pi} p(t)q_m(nt) \; dt ~\overset{?}{=}~  \lim\limits_{m \rightarrow \infty}\lim\limits_{n \rightarrow \infty}\frac{1}{2\pi} \int_{0}^{2\pi} p(t)q_m(nt) \; dt, $$ where $p, q_m$ are a trigo polynomials and $q_m(t) \xrightarrow[m \rightarrow \infty]{\text{unif.}}g(t)$ for some $g \in C[0,2\pi]$ $2\pi$-periodic. My idea I'm inclined to think that uniform convergence and the compactness of $[0,2\pi]$ allow us to pass both limits under the integration sign, then we can swap limits due to uniform convergence and finally we pass both limits outside. Does this work ?","Is it OK to change the order of the limits here : $$ \lim\limits_{n \rightarrow \infty} \lim\limits_{m \rightarrow \infty}\frac{1}{2\pi} \int_{0}^{2\pi} p(t)q_m(nt) \; dt ~\overset{?}{=}~  \lim\limits_{m \rightarrow \infty}\lim\limits_{n \rightarrow \infty}\frac{1}{2\pi} \int_{0}^{2\pi} p(t)q_m(nt) \; dt, $$ where $p, q_m$ are a trigo polynomials and $q_m(t) \xrightarrow[m \rightarrow \infty]{\text{unif.}}g(t)$ for some $g \in C[0,2\pi]$ $2\pi$-periodic. My idea I'm inclined to think that uniform convergence and the compactness of $[0,2\pi]$ allow us to pass both limits under the integration sign, then we can swap limits due to uniform convergence and finally we pass both limits outside. Does this work ?",,['limits']
21,Two nondecreasing sequences that bound each other,Two nondecreasing sequences that bound each other,,"Question: Let ($a_n$) and ($b_n$) be two nondecreasing sequences with the property that, for each positive integer $n$, there are integers $p$ and $q$ such that $a_n \leq b_p$ and $b_n \leq a_q$.  Show that ($a_n$) and ($b_n$) either both converge or both diverge to $\infty$ and that, moreover, if they both converge they have the same limit. Given: ($a_n$) and ($b_n$) are non-decreasing, and $a_n \leq b_p$ and $b_n \leq a_q$ My proof is like: Suppose ($a_n$) is convergent, then for every $\epsilon$ greater than $0$, we have $|an - L| < ε$  , then the limit for ($a_n$) is $L$. we know that $a_n ≤ bp$ and  $bn ≤ aq$,  so $a_n - b_p ≤ 0$   and   $b_n-a_q ≤ 0$ Let $|a_n-b_p|= ε/2$       $|b_p-a_m| = ε /2$ $|a_n - a_m| = |a_n - b_p + b_p -a_m| ≤ |a_n-b_p|+|b_p-a_m| < ε$ So an is a cauchy sequence Then I want to prove bn is a Cauchy sequence because there is a theorem says that the sequence converges if and only if the sequence is cauchy. Let $ε > 0$ and let      $|b_n-a_q|> ε/2$    &   $|a_q-b-m| > ε/2$ $|b_n - b_m| = |b_n-a_q + a_q-b_m| ≤  |b_n-a_q|+|a_q-b_m| ≤ ε$ So that ($b_n$) is a Cauchy sequence, and ($b_n$) converges. Since it converges, it has a limit, and according to the definition of limit, we have $|b_n - L| < ε$ So the limit of ($b_n$) is also $L$. I conclude that an and bn both converge and both of them converge to the same limit. I don't konw if it is helpful to use Cauchy to prove this question, I talk to the TA of this course and he suggests me to use Cauchy to solve. Maybe I misunderstand his hint, any help will be super appreciated:) Thanks a lot! Joy","Question: Let ($a_n$) and ($b_n$) be two nondecreasing sequences with the property that, for each positive integer $n$, there are integers $p$ and $q$ such that $a_n \leq b_p$ and $b_n \leq a_q$.  Show that ($a_n$) and ($b_n$) either both converge or both diverge to $\infty$ and that, moreover, if they both converge they have the same limit. Given: ($a_n$) and ($b_n$) are non-decreasing, and $a_n \leq b_p$ and $b_n \leq a_q$ My proof is like: Suppose ($a_n$) is convergent, then for every $\epsilon$ greater than $0$, we have $|an - L| < ε$  , then the limit for ($a_n$) is $L$. we know that $a_n ≤ bp$ and  $bn ≤ aq$,  so $a_n - b_p ≤ 0$   and   $b_n-a_q ≤ 0$ Let $|a_n-b_p|= ε/2$       $|b_p-a_m| = ε /2$ $|a_n - a_m| = |a_n - b_p + b_p -a_m| ≤ |a_n-b_p|+|b_p-a_m| < ε$ So an is a cauchy sequence Then I want to prove bn is a Cauchy sequence because there is a theorem says that the sequence converges if and only if the sequence is cauchy. Let $ε > 0$ and let      $|b_n-a_q|> ε/2$    &   $|a_q-b-m| > ε/2$ $|b_n - b_m| = |b_n-a_q + a_q-b_m| ≤  |b_n-a_q|+|a_q-b_m| ≤ ε$ So that ($b_n$) is a Cauchy sequence, and ($b_n$) converges. Since it converges, it has a limit, and according to the definition of limit, we have $|b_n - L| < ε$ So the limit of ($b_n$) is also $L$. I conclude that an and bn both converge and both of them converge to the same limit. I don't konw if it is helpful to use Cauchy to prove this question, I talk to the TA of this course and he suggests me to use Cauchy to solve. Maybe I misunderstand his hint, any help will be super appreciated:) Thanks a lot! Joy",,"['real-analysis', 'limits', 'convergence-divergence']"
22,limit as n goes to infinity of $\frac{\sqrt{n^3-3}-\sqrt{n^3+2n^2+3}}{\sqrt{n+2}}.$,limit as n goes to infinity of,\frac{\sqrt{n^3-3}-\sqrt{n^3+2n^2+3}}{\sqrt{n+2}}.,"How do you go about solving $$\lim_{n\to\infty}\frac{\sqrt{n^3-3}-\sqrt{n^3+2n^2+3}}{\sqrt{n+2}}.$$ I know that I have to fix the top so that it is not $(\infty - \infty$), but if I multiple it by  $$\frac{\sqrt{n^3-3}+\sqrt{n^3+2n^2+3}}{\sqrt{n^3-3}+\sqrt{n^3+2n^2+3}},$$ the bottom part becomes very ugly and extremely hard to deal with.","How do you go about solving $$\lim_{n\to\infty}\frac{\sqrt{n^3-3}-\sqrt{n^3+2n^2+3}}{\sqrt{n+2}}.$$ I know that I have to fix the top so that it is not $(\infty - \infty$), but if I multiple it by  $$\frac{\sqrt{n^3-3}+\sqrt{n^3+2n^2+3}}{\sqrt{n^3-3}+\sqrt{n^3+2n^2+3}},$$ the bottom part becomes very ugly and extremely hard to deal with.",,['limits']
23,How to compute $\lim_{x \to 0} (\frac{x^5 e^{-1/x^2}+x/2 - \sin(x/2))}{x^3})$?,How to compute ?,\lim_{x \to 0} (\frac{x^5 e^{-1/x^2}+x/2 - \sin(x/2))}{x^3}),I have a problem with this limit. I have no idea where is the problem. Can you correct my mistake? Thanks $$\lim\limits_{x \to 0} \left(\frac{x^5 e^\frac{-1}{x^2}+\frac{x}{2} - \sin(\frac{x}{2})}{x^3}\right)$$ I used the developments of McLaurin $e^x$ and $\sin x$ $$\lim\limits_{x \to 0} \left(\frac{x^5 (1-\frac{1}{x^2}+\frac{1}{2x^4})+\frac{x}{2} - ((\frac{x}{2})-(\frac{x^3}{48}))}{x^3}\right) =  \lim\limits_{x \to 0} \left(\frac{x^5-x^3+ \frac{x}{2} +\frac{x}{2} - \frac{x}{2}+\frac{x^3}{48}}{x^3}\right)=$$ $$\lim\limits_{x \to 0} \left(\frac{-x^3+\frac{x^3}{48}}{x^3}\right)=  \lim\limits_{x \to 0} \left(\frac{-\frac{47x^3}{48}}{x^3}\right)=\lim\limits_{x \to 0} \left(-\frac{47x^3}{48x^3}\right)= -\frac{47}{48}$$ but the result is wrong.,I have a problem with this limit. I have no idea where is the problem. Can you correct my mistake? Thanks $$\lim\limits_{x \to 0} \left(\frac{x^5 e^\frac{-1}{x^2}+\frac{x}{2} - \sin(\frac{x}{2})}{x^3}\right)$$ I used the developments of McLaurin $e^x$ and $\sin x$ $$\lim\limits_{x \to 0} \left(\frac{x^5 (1-\frac{1}{x^2}+\frac{1}{2x^4})+\frac{x}{2} - ((\frac{x}{2})-(\frac{x^3}{48}))}{x^3}\right) =  \lim\limits_{x \to 0} \left(\frac{x^5-x^3+ \frac{x}{2} +\frac{x}{2} - \frac{x}{2}+\frac{x^3}{48}}{x^3}\right)=$$ $$\lim\limits_{x \to 0} \left(\frac{-x^3+\frac{x^3}{48}}{x^3}\right)=  \lim\limits_{x \to 0} \left(\frac{-\frac{47x^3}{48}}{x^3}\right)=\lim\limits_{x \to 0} \left(-\frac{47x^3}{48x^3}\right)= -\frac{47}{48}$$ but the result is wrong.,,"['limits', 'taylor-expansion']"
24,Confused by peculiar norm,Confused by peculiar norm,,"Let $X$ be an infinite subset of $ [0,1]$. In an exercise I am considering the norm on $P([0,1])$ (polynomials on unit interval) defined by: $$||p||_X=\sup_X |p|$$ My question is, how do I make sense of convergent sequences under this norm? For instance, say $X=[0,1/2]$ then a sequence $p_n$ converging to $f$ under $||\cdot||_X$ will only tell me about $f$ on $X$ and not the whole of $[0,1]$; so how can the limit $f$ be fully determined? Maybe I am missing a crucial point here...","Let $X$ be an infinite subset of $ [0,1]$. In an exercise I am considering the norm on $P([0,1])$ (polynomials on unit interval) defined by: $$||p||_X=\sup_X |p|$$ My question is, how do I make sense of convergent sequences under this norm? For instance, say $X=[0,1/2]$ then a sequence $p_n$ converging to $f$ under $||\cdot||_X$ will only tell me about $f$ on $X$ and not the whole of $[0,1]$; so how can the limit $f$ be fully determined? Maybe I am missing a crucial point here...",,"['real-analysis', 'limits', 'polynomials', 'convergence-divergence', 'normed-spaces']"
25,Multivariable Exponential Limit Priblem,Multivariable Exponential Limit Priblem,,"I am trying to find the following limit $$\lim \limits_{(x, y) \to (0, 0)} \frac{(e^x-1)(e^y-1)}{x+y}$$ I have tried approaching along the lines $y = -x + mx^2$ but I'm getting a zero instead of a value which depends on $m$","I am trying to find the following limit $$\lim \limits_{(x, y) \to (0, 0)} \frac{(e^x-1)(e^y-1)}{x+y}$$ I have tried approaching along the lines $y = -x + mx^2$ but I'm getting a zero instead of a value which depends on $m$",,"['limits', 'multivariable-calculus', 'exponential-function']"
26,Multiplying each term of a sequence converging to $0$ by an integer to get a sequence converging to an arbitrarily chosen number,Multiplying each term of a sequence converging to  by an integer to get a sequence converging to an arbitrarily chosen number,0,"Given a sequence $( x_i )_i$ of real numbers converging to $0$ and $t \in \mathbb{R}$, is it possible to find integers $z_i$ such that $\lim_{i \to \infty} z_i x_i = t$?","Given a sequence $( x_i )_i$ of real numbers converging to $0$ and $t \in \mathbb{R}$, is it possible to find integers $z_i$ such that $\lim_{i \to \infty} z_i x_i = t$?",,"['real-analysis', 'sequences-and-series', 'limits']"
27,"If $f(x) >0$ for all $x$, then $\lim_{x\to a} f(x) \ge 0$","If  for all , then",f(x) >0 x \lim_{x\to a} f(x) \ge 0,"I met a problem like this: Suppose $f(x) > 0$ for all $x$ , and also that $\lim_{x\to a} f(x)$ exists. a) Show that $\lim_{x\to a} f(x) \ge 0$. b) Give an example where $\lim_{x\to a} f(x) = 0$. I am not sure how to solve it.. in fact I have no idea. Could anyone give me a hint？ Thanks!!!","I met a problem like this: Suppose $f(x) > 0$ for all $x$ , and also that $\lim_{x\to a} f(x)$ exists. a) Show that $\lim_{x\to a} f(x) \ge 0$. b) Give an example where $\lim_{x\to a} f(x) = 0$. I am not sure how to solve it.. in fact I have no idea. Could anyone give me a hint？ Thanks!!!",,"['real-analysis', 'limits']"
28,Epsilon delta proof; constraining delta.,Epsilon delta proof; constraining delta.,,"I've been trying to do some $\epsilon -\delta$ proofs, but I keep running into problems regarding certain steps. Namely, I can bring the proof to a point that is almost complete, but to complete it I would need to place a constraint on $\delta$ to make the next inequality true (such as $x^2+y^2≤\sqrt{x^2+y^2}$ provided $\sqrt{x^2+y^2}≤1$). If I had $$\lim_{(x,y)\to(0,0)}f(x,y)$$ Is it fine to constrain my $\delta$ (such as $0<\sqrt{x^2+y^2}<\delta<1$) because my limit is within the disc $\sqrt{x^2+y^2}<1$? If I come to a point where I would need to do that, have I done something wrong? Is there some extra step that I should make that I am missing?","I've been trying to do some $\epsilon -\delta$ proofs, but I keep running into problems regarding certain steps. Namely, I can bring the proof to a point that is almost complete, but to complete it I would need to place a constraint on $\delta$ to make the next inequality true (such as $x^2+y^2≤\sqrt{x^2+y^2}$ provided $\sqrt{x^2+y^2}≤1$). If I had $$\lim_{(x,y)\to(0,0)}f(x,y)$$ Is it fine to constrain my $\delta$ (such as $0<\sqrt{x^2+y^2}<\delta<1$) because my limit is within the disc $\sqrt{x^2+y^2}<1$? If I come to a point where I would need to do that, have I done something wrong? Is there some extra step that I should make that I am missing?",,"['limits', 'multivariable-calculus', 'epsilon-delta']"
29,How to calculate $\lim \limits_{x \to 0} \frac{x^2 \sin^2x}{x^2-\sin^2x}$ with $\lim \limits_{x \to 0} \frac{\sin x}{x}=1$?,How to calculate  with ?,\lim \limits_{x \to 0} \frac{x^2 \sin^2x}{x^2-\sin^2x} \lim \limits_{x \to 0} \frac{\sin x}{x}=1,"How to calculate $$\lim \limits_{x \to 0} \frac{x^2 \sin^2x}{x^2-\sin^2x}$$  with $$\lim \limits_{x \to 0} \frac{\sin x}{x}=1?$$ Yes I know the question has been asked, the answer is $3$, L'Hospital or Taylor series works and they are neat. However the question I meet is that I'm required to do it with the given limit. It may be dull algebra work; however I even don't know how to start with. Any idea will be wonderful.","How to calculate $$\lim \limits_{x \to 0} \frac{x^2 \sin^2x}{x^2-\sin^2x}$$  with $$\lim \limits_{x \to 0} \frac{\sin x}{x}=1?$$ Yes I know the question has been asked, the answer is $3$, L'Hospital or Taylor series works and they are neat. However the question I meet is that I'm required to do it with the given limit. It may be dull algebra work; however I even don't know how to start with. Any idea will be wonderful.",,"['calculus', 'limits', 'limits-without-lhopital']"
30,Good example showing why limits must exist in limit product rule,Good example showing why limits must exist in limit product rule,,"I'm looking for a way to show my calc 1 students not to use the limit laws without knowing that the individual limits exists. I could use $$\lim_{x\to 0} x^{2} \sin(1/x),$$ but by doing it wrong, one still gets the right answer, which is 0. So I'm looking for an example where splitting up the limit would actually give the wrong answer. I think $\lim_{x\to 0} x \csc(1/x)$ ought to work, but I'd like an even simpler example, if possible.","I'm looking for a way to show my calc 1 students not to use the limit laws without knowing that the individual limits exists. I could use $$\lim_{x\to 0} x^{2} \sin(1/x),$$ but by doing it wrong, one still gets the right answer, which is 0. So I'm looking for an example where splitting up the limit would actually give the wrong answer. I think $\lim_{x\to 0} x \csc(1/x)$ ought to work, but I'd like an even simpler example, if possible.",,"['limits', 'education', 'examples-counterexamples', 'products']"
31,Finding $\int_0^{\infty} \frac{f(\alpha x)-f(\beta x)}{x}dx$ given $\lim_{x\to 0}f(x)$ and $\lim_{x\to \infty}f(x)$.,Finding  given  and .,\int_0^{\infty} \frac{f(\alpha x)-f(\beta x)}{x}dx \lim_{x\to 0}f(x) \lim_{x\to \infty}f(x),"Problem: Solution: $\bf 45. \; (a)\,$ The substitution $u=\alpha x$, $\mathrm{d}u=\alpha\,\mathrm{d}x$ gives $$\int_{\varepsilon}^N\frac{f(\alpha x)}{x}\mathrm{d}x=\int_{\alpha\varepsilon}^{\alpha N}\frac{f(u)}{u}\mathrm{d}u.$$ Similarly, the substitution $u=\beta x$, $\mathrm{d}u=\beta\,\mathrm{d}x$ gives $$\int_{\varepsilon}^N\frac{f(\beta x)}{x}\mathrm{d}x=\int_{\beta\varepsilon}^{\beta N}\frac{f(u)}{u}\mathrm{d}u.$$ So $$\begin{align}\int_{\varepsilon}^N\frac{f(\alpha x)-f(\beta x)}{x}\mathrm{d}x&=\int_{\alpha\varepsilon}^{\alpha N}\frac{f(u)}{u}\mathrm{d}u-\int_{\beta\varepsilon}^{\beta N}\frac{f(u)}{u}\mathrm{d}u\\&=\int_{\alpha\varepsilon}^{\beta\varepsilon}\frac{f(u)}{u}\mathrm{d}u-\int_{\alpha N}^{\beta N}\frac{f(u)}{u}\mathrm{d}u.\end{align}$$ As $\varepsilon\to0$ and $N\to\infty$, this approaches $$\int_{\alpha\varepsilon}^{\beta\varepsilon}\frac{A}{u}\mathrm{d}u-\int_{\alpha N}^{\beta N}\frac{B}{u}\mathrm{d}u=(A-B)\log\frac{\beta}{\alpha}.$$ $\bf (b)\,$ In this case the same substitutions give $$\int_{\varepsilon}^\infty\frac{f(\alpha x)}{x}\mathrm{d}x=\int_{\alpha\varepsilon}^\infty\frac{f(u)}{u}\mathrm{d}u,\qquad\int_{\varepsilon}^\infty\frac{f(\beta x)}{x}\mathrm{d}x=\int_{\beta\varepsilon}^\infty\frac{f(u)}{u}\mathrm{d}u,$$ so $$\int_{\varepsilon}^\infty\frac{f(\alpha x)-f(\beta x)}{x}\mathrm{d}x=\int_{\alpha e}^{\beta\varepsilon}\frac{f(u)}{u}\mathrm{d}u\to A\log\frac{\beta}{\alpha}.$$ I have two questions regarding this solution. First, how is the substitution $u=\alpha x, du=\alpha dx$ guaranteed to be valid? From my knowledge, the change of variables theorem for integrals work when the function $f(u)/u$ is continuous. This is the version I'm referring to, and all other change of variables theorems share this assumption. So I'm curious how the substitution is valid without the fact that the integrand is continuous. THEOREM 6.4.6. (Change of Variables Theorem) Suppose that there exists a differentiable function $g:[c,d]\to[a,b]$ such that $g'\in R[c,d]$. Also, suppose that a function $f:[a,b]\to\mathfrak{R}$ is continuous with $a=g(c)$ and $b=g(d)$. Then $$\int_c^d(f\circ g)(x)g'(x)\,\mathrm{d}x=\int_a^bf(x)\,\mathrm{d}x.$$ Finally, I don't follow the final lines in the solution of (a) and (b). How do $\int_{\alpha \epsilon}^{\beta \epsilon} \frac{f(u)}{u}du$ and $\int_{\alpha N}^{\beta N} \frac{f(u)}{u}du$ turn into $\int_{\alpha \epsilon}^{\beta \epsilon} \frac{A}{u}du$ and $\int_{\alpha N}^{\beta N} \frac{B}{u}du$ as $\epsilon \to 0$ and $N \to \infty$? I can't give a rigorous argument that this must be the case. I would immensely appreciate it if anyone could provide me with a rigorous argument to these questions.","Problem: Solution: $\bf 45. \; (a)\,$ The substitution $u=\alpha x$, $\mathrm{d}u=\alpha\,\mathrm{d}x$ gives $$\int_{\varepsilon}^N\frac{f(\alpha x)}{x}\mathrm{d}x=\int_{\alpha\varepsilon}^{\alpha N}\frac{f(u)}{u}\mathrm{d}u.$$ Similarly, the substitution $u=\beta x$, $\mathrm{d}u=\beta\,\mathrm{d}x$ gives $$\int_{\varepsilon}^N\frac{f(\beta x)}{x}\mathrm{d}x=\int_{\beta\varepsilon}^{\beta N}\frac{f(u)}{u}\mathrm{d}u.$$ So $$\begin{align}\int_{\varepsilon}^N\frac{f(\alpha x)-f(\beta x)}{x}\mathrm{d}x&=\int_{\alpha\varepsilon}^{\alpha N}\frac{f(u)}{u}\mathrm{d}u-\int_{\beta\varepsilon}^{\beta N}\frac{f(u)}{u}\mathrm{d}u\\&=\int_{\alpha\varepsilon}^{\beta\varepsilon}\frac{f(u)}{u}\mathrm{d}u-\int_{\alpha N}^{\beta N}\frac{f(u)}{u}\mathrm{d}u.\end{align}$$ As $\varepsilon\to0$ and $N\to\infty$, this approaches $$\int_{\alpha\varepsilon}^{\beta\varepsilon}\frac{A}{u}\mathrm{d}u-\int_{\alpha N}^{\beta N}\frac{B}{u}\mathrm{d}u=(A-B)\log\frac{\beta}{\alpha}.$$ $\bf (b)\,$ In this case the same substitutions give $$\int_{\varepsilon}^\infty\frac{f(\alpha x)}{x}\mathrm{d}x=\int_{\alpha\varepsilon}^\infty\frac{f(u)}{u}\mathrm{d}u,\qquad\int_{\varepsilon}^\infty\frac{f(\beta x)}{x}\mathrm{d}x=\int_{\beta\varepsilon}^\infty\frac{f(u)}{u}\mathrm{d}u,$$ so $$\int_{\varepsilon}^\infty\frac{f(\alpha x)-f(\beta x)}{x}\mathrm{d}x=\int_{\alpha e}^{\beta\varepsilon}\frac{f(u)}{u}\mathrm{d}u\to A\log\frac{\beta}{\alpha}.$$ I have two questions regarding this solution. First, how is the substitution $u=\alpha x, du=\alpha dx$ guaranteed to be valid? From my knowledge, the change of variables theorem for integrals work when the function $f(u)/u$ is continuous. This is the version I'm referring to, and all other change of variables theorems share this assumption. So I'm curious how the substitution is valid without the fact that the integrand is continuous. THEOREM 6.4.6. (Change of Variables Theorem) Suppose that there exists a differentiable function $g:[c,d]\to[a,b]$ such that $g'\in R[c,d]$. Also, suppose that a function $f:[a,b]\to\mathfrak{R}$ is continuous with $a=g(c)$ and $b=g(d)$. Then $$\int_c^d(f\circ g)(x)g'(x)\,\mathrm{d}x=\int_a^bf(x)\,\mathrm{d}x.$$ Finally, I don't follow the final lines in the solution of (a) and (b). How do $\int_{\alpha \epsilon}^{\beta \epsilon} \frac{f(u)}{u}du$ and $\int_{\alpha N}^{\beta N} \frac{f(u)}{u}du$ turn into $\int_{\alpha \epsilon}^{\beta \epsilon} \frac{A}{u}du$ and $\int_{\alpha N}^{\beta N} \frac{B}{u}du$ as $\epsilon \to 0$ and $N \to \infty$? I can't give a rigorous argument that this must be the case. I would immensely appreciate it if anyone could provide me with a rigorous argument to these questions.",,"['calculus', 'real-analysis', 'integration', 'analysis', 'limits']"
32,A question about the formulation of the definition of a limit for sequences,A question about the formulation of the definition of a limit for sequences,,"So I know the definition of a limit of a the sequence is: $a$ is a limit of a sequence $\{x_n\}$ if given $\epsilon>0$ there exists a positive integer $N$ such that $|x_n-a|<\epsilon$ for all $n \geq N$. My question is, why does the order of stating given $\epsilon>0$ there exists... matter. In other words, why can we not say there a exists a positive integer $N$ such that given $\epsilon>0$... In other words, one $N$ value can work for all values of $\epsilon$ because if it works for an arbitrarily small real number then surely it works for all real numbers arbitrarily bigger than it? Thus making defining a different $N$ value for each $\epsilon$ unnecessary.","So I know the definition of a limit of a the sequence is: $a$ is a limit of a sequence $\{x_n\}$ if given $\epsilon>0$ there exists a positive integer $N$ such that $|x_n-a|<\epsilon$ for all $n \geq N$. My question is, why does the order of stating given $\epsilon>0$ there exists... matter. In other words, why can we not say there a exists a positive integer $N$ such that given $\epsilon>0$... In other words, one $N$ value can work for all values of $\epsilon$ because if it works for an arbitrarily small real number then surely it works for all real numbers arbitrarily bigger than it? Thus making defining a different $N$ value for each $\epsilon$ unnecessary.",,"['calculus', 'real-analysis', 'limits', 'definition']"
33,How find $\lim_{n\to+\infty}\sum_{k=0}^{n}(-1)^{k}\sqrt{\binom{n}{k}}$?,How find ?,\lim_{n\to+\infty}\sum_{k=0}^{n}(-1)^{k}\sqrt{\binom{n}{k}},How find this limit $\displaystyle\lim_{n\to+\infty}\sum_{k=0}^{n}(-1)^{k}\sqrt{\binom{n}{k}}$,How find this limit $\displaystyle\lim_{n\to+\infty}\sum_{k=0}^{n}(-1)^{k}\sqrt{\binom{n}{k}}$,,"['sequences-and-series', 'limits']"
34,$f(x)$=$\left\lfloor { x }^{ 2 } \right\rfloor -\left\lfloor x \right\rfloor ^{ 2 }$ is discontinuous for all integer values of x except only at x=1,= is discontinuous for all integer values of x except only at x=1,f(x) \left\lfloor { x }^{ 2 } \right\rfloor -\left\lfloor x \right\rfloor ^{ 2 },"How to prove that $f(x)$=$\left\lfloor { x }^{ 2 } \right\rfloor -\left\lfloor x \right\rfloor ^{ 2 }$ is discontinuous for all integer values of x except only at $x=1$ ? Ya,even I used intuition at the first go taking some trial values and checking.Then I plotted the graph on wolfram alpha.But if you can think of some other rigorous proof for this one,let me know.Thanks!","How to prove that $f(x)$=$\left\lfloor { x }^{ 2 } \right\rfloor -\left\lfloor x \right\rfloor ^{ 2 }$ is discontinuous for all integer values of x except only at $x=1$ ? Ya,even I used intuition at the first go taking some trial values and checking.Then I plotted the graph on wolfram alpha.But if you can think of some other rigorous proof for this one,let me know.Thanks!",,['limits']
35,limit of a function involving infinite nested roots,limit of a function involving infinite nested roots,,I was given the following problem :  $$\lim \limits_{x \to \infty} \frac{\sqrt{x}}{\sqrt{x+\sqrt{x+\sqrt{x+...}}}}$$ The following is my approach: $= \sqrt{ \lim \limits_{x \to \infty} \frac{x}{x+\sqrt{x+\sqrt{x+...}}} } $ I divide by x: $= \sqrt{ \lim \limits_{x \to \infty} \frac{1}{1+\frac{\sqrt{x+\sqrt{x+...}}}{x}} } $ I then reasoned that obviously: $ \sqrt{x+\sqrt{x+...}}<x$ that is  $ \frac{\sqrt{x+\sqrt{x+...}}}{x}  = \frac{1}{x^y} : y>0 $ hence  $\lim \limits_{x \to \infty} \frac{\sqrt{x+\sqrt{x+...}}}{x} =0$ I concluded that the original limit equals $ \sqrt{\frac{1}{1+0}} = 1$ Was my reasoning correct? And is this a legitimate mathematical approach to solve this limit?,I was given the following problem :  $$\lim \limits_{x \to \infty} \frac{\sqrt{x}}{\sqrt{x+\sqrt{x+\sqrt{x+...}}}}$$ The following is my approach: $= \sqrt{ \lim \limits_{x \to \infty} \frac{x}{x+\sqrt{x+\sqrt{x+...}}} } $ I divide by x: $= \sqrt{ \lim \limits_{x \to \infty} \frac{1}{1+\frac{\sqrt{x+\sqrt{x+...}}}{x}} } $ I then reasoned that obviously: $ \sqrt{x+\sqrt{x+...}}<x$ that is  $ \frac{\sqrt{x+\sqrt{x+...}}}{x}  = \frac{1}{x^y} : y>0 $ hence  $\lim \limits_{x \to \infty} \frac{\sqrt{x+\sqrt{x+...}}}{x} =0$ I concluded that the original limit equals $ \sqrt{\frac{1}{1+0}} = 1$ Was my reasoning correct? And is this a legitimate mathematical approach to solve this limit?,,['limits']
36,Example of a limit question requiring infinite applications of L'Hospital's rule to get a result,Example of a limit question requiring infinite applications of L'Hospital's rule to get a result,,"I'm looking for a limit of the form $\lim_{x \to ?}\frac{f(x)}{g(x)}$ such that any arbitrary number of iterations of L'Hospital's rule results in an indeterminate form and the limit that could (most easily?) be calculated by taking an infinite number of iterations -- ie calculating $$\lim_{x \to ?} \left(\lim_{n \to \infty} \frac{f^n(x)}{g^n(x)}\right)$$ I've seen questions like When does L' Hopital's rule fail? -- but the examples from this question either don't have an 'infinite derivative' (because it just alternates) or else just obviously simplify (I'm not sure how to formally rule this case out, maybe by requiring the function $\frac{f^n(x)}{g^n(x)}$ to not 'have a hole' at the limit evaluation point for all $n$). Is this even possible? The direction of my own work on this problem has been looking for a trig function that alternates between two values when the limit is taken at infinity, and each successive derivative decreases the distance between the alternating values. After an infinite number of derivative operations, the distance between the alternating values would essentially be 0 and a limit at infinity would exist. I believe a ratio of two such functions could satisfy the requirements of my question.","I'm looking for a limit of the form $\lim_{x \to ?}\frac{f(x)}{g(x)}$ such that any arbitrary number of iterations of L'Hospital's rule results in an indeterminate form and the limit that could (most easily?) be calculated by taking an infinite number of iterations -- ie calculating $$\lim_{x \to ?} \left(\lim_{n \to \infty} \frac{f^n(x)}{g^n(x)}\right)$$ I've seen questions like When does L' Hopital's rule fail? -- but the examples from this question either don't have an 'infinite derivative' (because it just alternates) or else just obviously simplify (I'm not sure how to formally rule this case out, maybe by requiring the function $\frac{f^n(x)}{g^n(x)}$ to not 'have a hole' at the limit evaluation point for all $n$). Is this even possible? The direction of my own work on this problem has been looking for a trig function that alternates between two values when the limit is taken at infinity, and each successive derivative decreases the distance between the alternating values. After an infinite number of derivative operations, the distance between the alternating values would essentially be 0 and a limit at infinity would exist. I believe a ratio of two such functions could satisfy the requirements of my question.",,"['calculus', 'limits']"
37,Evaluate $\lim_{n\rightarrow\infty}\left[\frac{(2n)!!}{(2n-1)!!}\right]^2\cdot\frac{1}{2n+1}$,Evaluate,\lim_{n\rightarrow\infty}\left[\frac{(2n)!!}{(2n-1)!!}\right]^2\cdot\frac{1}{2n+1},"If we know that $I_{n}=\int_0^\frac{\pi}{2}\sin^n(x)\,{\rm d}x$ we need to evaluate: $\lim_{n\rightarrow\infty}\left[\frac{(2n)!!}{(2n-1)!!}\right]^2\cdot\frac{1}{2n+1}$ ( !! means double factorial ). Here is all my steps to arrive at a squeeze theorem: $I_{_{n}}=\frac{n-1}{n}\cdot I_{n-2}$ , $\forall x\geq 2$ . Therefore: $I_{_{2k}}=\frac{\pi}{2}\cdot (\prod_{k=0}^{n}\frac{2k+1}{2k+2})=\frac{\pi}{2}\cdot(\prod_{k=1}^{n}\frac{2k-1}{2k})$ $I_{_{2k+1}}=\prod_{k=0}^{n}\frac{2k+2}{2k+3}=\prod_{k=1}^{n}\frac{2k}{2k+1}$ $\Rightarrow I_{2k}\geq I_{2k+1}$ , $\forall x\in[0,\frac{\pi}{2}]$ $\Rightarrow \prod_{k=1}^{n}\frac{2k}{2k+1}\leq\frac{\pi}{2}(\prod_{k=1}^{n}\frac{2k-1}{2k})\mid\cdot\prod_{k=1}^{n}\frac{2k}{2k-1}$ $\Rightarrow \prod_{k=1}^{n}\frac{2k^2}{4k^2-1}\leq\frac{\pi}{2}$ I don't know how can I arrive at $\left[\frac{(2n)!!}{(2n-1)!!}\right]^2\cdot\frac{1}{2n+1}$ and after use squeeze theorem. Is something $\frac{(2n)!!}{(2n-1)!!}=\prod_{k=1}^{n}\frac{2k}{2k-1}$ ? I want to continue with this method, if is something who can help me to finish I'll apreciate.","If we know that we need to evaluate: ( !! means double factorial ). Here is all my steps to arrive at a squeeze theorem: , . Therefore: , I don't know how can I arrive at and after use squeeze theorem. Is something ? I want to continue with this method, if is something who can help me to finish I'll apreciate.","I_{n}=\int_0^\frac{\pi}{2}\sin^n(x)\,{\rm d}x \lim_{n\rightarrow\infty}\left[\frac{(2n)!!}{(2n-1)!!}\right]^2\cdot\frac{1}{2n+1} I_{_{n}}=\frac{n-1}{n}\cdot I_{n-2} \forall x\geq 2 I_{_{2k}}=\frac{\pi}{2}\cdot (\prod_{k=0}^{n}\frac{2k+1}{2k+2})=\frac{\pi}{2}\cdot(\prod_{k=1}^{n}\frac{2k-1}{2k}) I_{_{2k+1}}=\prod_{k=0}^{n}\frac{2k+2}{2k+3}=\prod_{k=1}^{n}\frac{2k}{2k+1} \Rightarrow I_{2k}\geq I_{2k+1} \forall x\in[0,\frac{\pi}{2}] \Rightarrow \prod_{k=1}^{n}\frac{2k}{2k+1}\leq\frac{\pi}{2}(\prod_{k=1}^{n}\frac{2k-1}{2k})\mid\cdot\prod_{k=1}^{n}\frac{2k}{2k-1} \Rightarrow \prod_{k=1}^{n}\frac{2k^2}{4k^2-1}\leq\frac{\pi}{2} \left[\frac{(2n)!!}{(2n-1)!!}\right]^2\cdot\frac{1}{2n+1} \frac{(2n)!!}{(2n-1)!!}=\prod_{k=1}^{n}\frac{2k}{2k-1}","['calculus', 'real-analysis', 'integration', 'limits']"
38,"Prove non-existance of limit: $f(x,y) = \frac{xy\sin(\frac{x}{y})}{x^2 + |y|^3}$",Prove non-existance of limit:,"f(x,y) = \frac{xy\sin(\frac{x}{y})}{x^2 + |y|^3}","I need to prove that $f(x,y) = \frac{xy^2\sin(\frac{x}{y})}{x^2 + |y|^3}$ does not tend to $0$ when $(x,y)$ approaches $(0,0)$. In order to do so, I would need to find some direction $\alpha$ such that $f(\alpha(t))$ approaches to some value $L \neq 0$ as $(x,y)$ approaches the origin. The problem is that I can't seem to find that direction. What could I try? I found  $$f(x^\frac{1}{2}, x^\frac{1}{3}) = \frac{x^\frac{7}{6}  \sin(x^\frac{-1}{6})}{2x} = \frac{x^\frac{1}{6}  \sin(x^\frac{-1}{6})}{2} = \frac{\sin(x^\frac{-1}{6})}{2 x^\frac{-1}{6}} \rightarrow \frac{1}{2}$$ Is this correct?","I need to prove that $f(x,y) = \frac{xy^2\sin(\frac{x}{y})}{x^2 + |y|^3}$ does not tend to $0$ when $(x,y)$ approaches $(0,0)$. In order to do so, I would need to find some direction $\alpha$ such that $f(\alpha(t))$ approaches to some value $L \neq 0$ as $(x,y)$ approaches the origin. The problem is that I can't seem to find that direction. What could I try? I found  $$f(x^\frac{1}{2}, x^\frac{1}{3}) = \frac{x^\frac{7}{6}  \sin(x^\frac{-1}{6})}{2x} = \frac{x^\frac{1}{6}  \sin(x^\frac{-1}{6})}{2} = \frac{\sin(x^\frac{-1}{6})}{2 x^\frac{-1}{6}} \rightarrow \frac{1}{2}$$ Is this correct?",,"['limits', 'multivariable-calculus', 'continuity']"
39,Proving that $\lim\limits_{x \to -1}(3x^2-3)\sin(x) = 0$.,Proving that .,\lim\limits_{x \to -1}(3x^2-3)\sin(x) = 0,"Prove that $\lim\limits_{x \to -1}(3x^2-3)\sin(x) = 0$. So, by the definition I have to prove that $$ \exists\delta>0 \text{   such that} $$ $$ 0<|x+1|<\delta \longrightarrow |(3x^2-3)\sin(x)|<\epsilon $$ What I did: $$ |(3x^2-3)\sin(x)|=3|x+1||x-1||\sin(x)|\leq3|x+1||x-1| $$ Let $\delta_1=1$ then: $$ -1<x+1<1 \longrightarrow -3<x-1<-1<3 \longrightarrow |x-1|<3 $$ So: $$ 3|x+1||x-1||\sin(x)|\leq3|x+1||x-1|<3|x+1|\cdot3<\epsilon $$ $$ |x+1|<\frac\epsilon9 $$ So I let $\delta=\min(1,\frac\epsilon9)$ and I'm done? Did I screw up anywhere? What are other ways to prove this?","Prove that $\lim\limits_{x \to -1}(3x^2-3)\sin(x) = 0$. So, by the definition I have to prove that $$ \exists\delta>0 \text{   such that} $$ $$ 0<|x+1|<\delta \longrightarrow |(3x^2-3)\sin(x)|<\epsilon $$ What I did: $$ |(3x^2-3)\sin(x)|=3|x+1||x-1||\sin(x)|\leq3|x+1||x-1| $$ Let $\delta_1=1$ then: $$ -1<x+1<1 \longrightarrow -3<x-1<-1<3 \longrightarrow |x-1|<3 $$ So: $$ 3|x+1||x-1||\sin(x)|\leq3|x+1||x-1|<3|x+1|\cdot3<\epsilon $$ $$ |x+1|<\frac\epsilon9 $$ So I let $\delta=\min(1,\frac\epsilon9)$ and I'm done? Did I screw up anywhere? What are other ways to prove this?",,"['limits', 'proof-verification', 'epsilon-delta']"
40,Help in finding the sum of the series,Help in finding the sum of the series,,$$\sum_{n=1}^\infty \frac{1}{n^4+n^2+1}$$ I tried breaking into factors but it is not telescoping. $$\frac {1}{(n^2+n+1)(n^2-n+1)} = \frac {1}{2n} \left(\frac {1}{n^2-n+1} - \frac {1}{n^2+n+1}\right)$$,$$\sum_{n=1}^\infty \frac{1}{n^4+n^2+1}$$ I tried breaking into factors but it is not telescoping. $$\frac {1}{(n^2+n+1)(n^2-n+1)} = \frac {1}{2n} \left(\frac {1}{n^2-n+1} - \frac {1}{n^2+n+1}\right)$$,,"['calculus', 'sequences-and-series', 'limits']"
41,Concerning series of positive real numbers whose terms are decreasing and tending to $0$,Concerning series of positive real numbers whose terms are decreasing and tending to,0,"Let $\{a_n\}$ be a decreasing sequence of positive real numbers such that $\lim_{n \to \infty} a_n=0$ and $\sum_{n=1}^{\infty}a_n= \infty$ ( for eaxmple , like $a_n:=\dfrac 1n$ ), then is it true that for every $r>0$ , there exists a subsequence  $\{a_{r_n} \}$ of $\{a_n\}$ such that $\sum_{n=1}^{\infty} a_{r_n}=r$ ? For example , for $\sum\dfrac 1n =\infty$ and  $e>0$ , we have subsequence $\{\dfrac 1{n!}\}$ of $\{\dfrac 1n\}$ such that $\sum \dfrac 1{n!}=e$ , but even for this sequence $\{\dfrac1 n \}$ , I am not able to prove my  claim for general $r>0$ . Please help . Thanks in advance","Let $\{a_n\}$ be a decreasing sequence of positive real numbers such that $\lim_{n \to \infty} a_n=0$ and $\sum_{n=1}^{\infty}a_n= \infty$ ( for eaxmple , like $a_n:=\dfrac 1n$ ), then is it true that for every $r>0$ , there exists a subsequence  $\{a_{r_n} \}$ of $\{a_n\}$ such that $\sum_{n=1}^{\infty} a_{r_n}=r$ ? For example , for $\sum\dfrac 1n =\infty$ and  $e>0$ , we have subsequence $\{\dfrac 1{n!}\}$ of $\{\dfrac 1n\}$ such that $\sum \dfrac 1{n!}=e$ , but even for this sequence $\{\dfrac1 n \}$ , I am not able to prove my  claim for general $r>0$ . Please help . Thanks in advance",,"['real-analysis', 'sequences-and-series']"
42,"If $\lim\limits_{x \to \infty} f(x)$ is a finite real number and $f''(x)$ is bounded, then $\lim\limits_{x \to \infty} f'(x) = 0$ [duplicate]","If  is a finite real number and  is bounded, then  [duplicate]",\lim\limits_{x \to \infty} f(x) f''(x) \lim\limits_{x \to \infty} f'(x) = 0,"This question already has answers here : If $f(x)\to 0$ as $x\to\infty$ and $f''$ is bounded, show that $f'(x)\to0$ as $x\to\infty$ (4 answers) Closed 9 years ago . I was trying to prove this and I did a very similar argument as the one in this answer: What does $\lim\limits_{x \to \infty} f(x) = 1$ say about $\lim\limits_{x \to \infty} f'(x)$? Basically,  $\lim\limits_{x \to \infty} f'(x) \neq 0$ would imply that $f$ is not bounded, so its limit is not finite. However, the theorem I was trying to prove makes the hypothesis that $f''(x)$ is bounded . Is this hypothesis necessary?  It's sort of strange, because using just the argument I used would mean that (considering $f$ in n times differentiable) has all its derivatives equal to $0$.","This question already has answers here : If $f(x)\to 0$ as $x\to\infty$ and $f''$ is bounded, show that $f'(x)\to0$ as $x\to\infty$ (4 answers) Closed 9 years ago . I was trying to prove this and I did a very similar argument as the one in this answer: What does $\lim\limits_{x \to \infty} f(x) = 1$ say about $\lim\limits_{x \to \infty} f'(x)$? Basically,  $\lim\limits_{x \to \infty} f'(x) \neq 0$ would imply that $f$ is not bounded, so its limit is not finite. However, the theorem I was trying to prove makes the hypothesis that $f''(x)$ is bounded . Is this hypothesis necessary?  It's sort of strange, because using just the argument I used would mean that (considering $f$ in n times differentiable) has all its derivatives equal to $0$.",,"['calculus', 'limits', 'derivatives']"
43,Limit of naturals at infinity and uniform continuity,Limit of naturals at infinity and uniform continuity,,"I need to prove the following: Let $ f:\mathbb{R}\to\mathbb{R}$ be continuous. $\lim_{n\to\infty}f(n) = \infty$ , $n\in\mathbb{N}$ . Let $f$ be uniformly continuous, prove that $\lim_{x\to\infty}f(x)=\infty,\ x\in\mathbb{R}$ It's clear to me why uniform continuity is required, for example: $f(x) = xsin(\frac{(4x-3)\pi}{2})$ $\forall n\in\mathbb{N},\ f(n) = n$ But clearly $f$ does not tend to infinity for all $x$ . I just can't get the uniform continuity into anything that will get me closer to solving this. Help please!","I need to prove the following: Let be continuous. , . Let be uniformly continuous, prove that It's clear to me why uniform continuity is required, for example: But clearly does not tend to infinity for all . I just can't get the uniform continuity into anything that will get me closer to solving this. Help please!"," f:\mathbb{R}\to\mathbb{R} \lim_{n\to\infty}f(n) = \infty n\in\mathbb{N} f \lim_{x\to\infty}f(x)=\infty,\ x\in\mathbb{R} f(x) = xsin(\frac{(4x-3)\pi}{2}) \forall n\in\mathbb{N},\ f(n) = n f x","['calculus', 'limits', 'uniform-continuity']"
44,"$f(x)=q$ if $x=p/q$, properly reduced is unbounded at every point.","if , properly reduced is unbounded at every point.",f(x)=q x=p/q,"Define the function $f$ as follows: $$f(x) = \begin{cases} q,  & \text{if $x=p/q$,properly reduced} \\ 0, & \text{if $x$ is irrational} \end{cases}$$ Prove that for every real number $x_0$, $f$ fails to be bounded at $x_0$, i.e. there does not exist any neighborhood of $x_0$ for which $f$ is bounded at. It's enough to consider only rational points. Given any $x_0 \in \mathbb Q$, and any $\delta$-neighborhood of $x_0={p\over q}$ contains infinitely many rational points. So given any $M \gt 0$, in fact, greater than $q$, there can be only finitely many rationals in the $\delta$-neighborhood of $x_0$ with denominator less than or equal to $M$ . Thus, there must be a rational in a properly reduced form with denominator greater than $M$ in the specified $\delta$-neighborhood of $x_0$, and so the value of the function at this point would be greater than $M$. Hence, $f$ is unbounded at any neighborhood of $x_0$. This is my solution and I think it's correct but I'm unsure how to rigorously show the bolded part. That is, how can I write it down to guarantee that there are only finitely many rationals satisfying the assertion? I'd appreciate a formal explanation on this part.","Define the function $f$ as follows: $$f(x) = \begin{cases} q,  & \text{if $x=p/q$,properly reduced} \\ 0, & \text{if $x$ is irrational} \end{cases}$$ Prove that for every real number $x_0$, $f$ fails to be bounded at $x_0$, i.e. there does not exist any neighborhood of $x_0$ for which $f$ is bounded at. It's enough to consider only rational points. Given any $x_0 \in \mathbb Q$, and any $\delta$-neighborhood of $x_0={p\over q}$ contains infinitely many rational points. So given any $M \gt 0$, in fact, greater than $q$, there can be only finitely many rationals in the $\delta$-neighborhood of $x_0$ with denominator less than or equal to $M$ . Thus, there must be a rational in a properly reduced form with denominator greater than $M$ in the specified $\delta$-neighborhood of $x_0$, and so the value of the function at this point would be greater than $M$. Hence, $f$ is unbounded at any neighborhood of $x_0$. This is my solution and I think it's correct but I'm unsure how to rigorously show the bolded part. That is, how can I write it down to guarantee that there are only finitely many rationals satisfying the assertion? I'd appreciate a formal explanation on this part.",,"['real-analysis', 'analysis', 'limits']"
45,Hierarchy of Convergence,Hierarchy of Convergence,,"My motivating thought is if I have a sequence of functions that converges in C([0,1]), is it enough to show that it converges to something outside C([0,1]), to show that it doesn't converge in C([0,1])? Limits are unique in metric spaces, so I suppose in this case it would be enough- but then what forms of convergence are stronger and weaker than other forms? For instance, almost uniform convergence doesn't imply a unique limit- we could have a function that almost uniformly converges to both a continuous function, and a discontinuous function! So what is the hierarchy of convergence, and when can I just cite the uniqueness of a limit?","My motivating thought is if I have a sequence of functions that converges in C([0,1]), is it enough to show that it converges to something outside C([0,1]), to show that it doesn't converge in C([0,1])? Limits are unique in metric spaces, so I suppose in this case it would be enough- but then what forms of convergence are stronger and weaker than other forms? For instance, almost uniform convergence doesn't imply a unique limit- we could have a function that almost uniformly converges to both a continuous function, and a discontinuous function! So what is the hierarchy of convergence, and when can I just cite the uniqueness of a limit?",,['limits']
46,How can we find the closed form of this?,How can we find the closed form of this?,,"$$\lim_{n\to\infty}\frac1n\sum_{k=1}^n\left(\left\lfloor\frac{2n}k\right\rfloor-2\left\lfloor\frac nk\right\rfloor\right)$$ I think it is equal to $2(\frac13 - \frac14 + \frac15 - \frac16 + \cdots)$, but does it have closed form?","$$\lim_{n\to\infty}\frac1n\sum_{k=1}^n\left(\left\lfloor\frac{2n}k\right\rfloor-2\left\lfloor\frac nk\right\rfloor\right)$$ I think it is equal to $2(\frac13 - \frac14 + \frac15 - \frac16 + \cdots)$, but does it have closed form?",,"['limits', 'summation']"
47,Evaluate $\lim_{x\to\infty} ((x^5+x^4)^{1/6}-(x^5-x^4)^{1/6})$,Evaluate,\lim_{x\to\infty} ((x^5+x^4)^{1/6}-(x^5-x^4)^{1/6}),"I've been struggling with the following: $$\lim_{x\to\infty} ((x^6+x^5)^{1/6}-(x^6-x^5)^{1/6})$$ Tried factoring out $x^{5/6}$ and then using L'hopital- which got me nowhere, tried multiplying by the conjugate, but it got messy- so either I'm scared of the algebra or there's a better way.","I've been struggling with the following: $$\lim_{x\to\infty} ((x^6+x^5)^{1/6}-(x^6-x^5)^{1/6})$$ Tried factoring out $x^{5/6}$ and then using L'hopital- which got me nowhere, tried multiplying by the conjugate, but it got messy- so either I'm scared of the algebra or there's a better way.",,"['calculus', 'limits']"
48,How to calculate what this power series converges against? (double factorials),How to calculate what this power series converges against? (double factorials),,"I'm working on my physics master course homework and I'm given the following equation out of nowhere: $\displaystyle{ 1 + \sum_{n\ =\ 1}^{\infty}{z^n\left(\, 2n - 1\,\right)!! \over 2n!!} ={1 \over \,\sqrt{\,\vphantom{\large A}1 - z\,}\,} }$ Now I don't need to prove it for my Homework, but still im wondering, how one would calculate this series. Of course it is not mentioned in the Homework, that this series doesn't converge for all $z$ ( sloppy physicsy style, I know : ) ). Wolfram Alpha said, It only converges for  $\,{\rm abs}\left(\, z\,\right) < 1$, which is a hint to the geometric series, but I have no idea how to account for the double factiorals. Any hints?","I'm working on my physics master course homework and I'm given the following equation out of nowhere: $\displaystyle{ 1 + \sum_{n\ =\ 1}^{\infty}{z^n\left(\, 2n - 1\,\right)!! \over 2n!!} ={1 \over \,\sqrt{\,\vphantom{\large A}1 - z\,}\,} }$ Now I don't need to prove it for my Homework, but still im wondering, how one would calculate this series. Of course it is not mentioned in the Homework, that this series doesn't converge for all $z$ ( sloppy physicsy style, I know : ) ). Wolfram Alpha said, It only converges for  $\,{\rm abs}\left(\, z\,\right) < 1$, which is a hint to the geometric series, but I have no idea how to account for the double factiorals. Any hints?",,"['limits', 'convergence-divergence', 'power-series', 'factorial']"
49,Evaluate this limit in terms of f,Evaluate this limit in terms of f,,"I want to evaluate the following limit: $$\lim_{d\to x} \dfrac{\dfrac{2x}{f'(x)}+f(x)-f(d)-\dfrac{x^2-d^2}{f(x)-f(d)}}{2\left(\dfrac{d-x}{f(x)-f(d)}+\dfrac{1}{f'(x)}\right)}$$ I tried L'hopital's rule but it just keeps getting worse and worse. I got this limit by wondering about circles fitting on a curve at a point $(x,f(x))$ and this limit is the $x$ coordinate of the circles center, in terms of a dummy point $d$.","I want to evaluate the following limit: $$\lim_{d\to x} \dfrac{\dfrac{2x}{f'(x)}+f(x)-f(d)-\dfrac{x^2-d^2}{f(x)-f(d)}}{2\left(\dfrac{d-x}{f(x)-f(d)}+\dfrac{1}{f'(x)}\right)}$$ I tried L'hopital's rule but it just keeps getting worse and worse. I got this limit by wondering about circles fitting on a curve at a point $(x,f(x))$ and this limit is the $x$ coordinate of the circles center, in terms of a dummy point $d$.",,['limits']
50,How to evaluate $\displaystyle\lim_{x\to0^+}\left(\frac{\ln(4^x-3^x)-\ln(4^x-1)}{x}\right)(4^x-1)$?,How to evaluate ?,\displaystyle\lim_{x\to0^+}\left(\frac{\ln(4^x-3^x)-\ln(4^x-1)}{x}\right)(4^x-1),How to evaluate $$L:=\lim_{x\to0^+}\left(\frac{\ln(4^x-3^x)-\ln(4^x-1)}{x}\right)(4^x-1)$$? My solution: $$\begin{align} L&=\lim_{x \to 0^+} \frac{\ln\left(\frac{4^x-3^x}{4^x-1}\right)}{x}(4^x-1)=\\ &=\lim_{x \to 0^+} \frac{ \ln\left(1-\frac{3^x-1}{4^x-1}\right)}{\frac{3^x-1}{4^x-1}}\times  \frac{3^x-1}{4^x-1}\times  \frac{4^x-1}{x} \end{align}$$ How to continue from here?,How to evaluate $$L:=\lim_{x\to0^+}\left(\frac{\ln(4^x-3^x)-\ln(4^x-1)}{x}\right)(4^x-1)$$? My solution: $$\begin{align} L&=\lim_{x \to 0^+} \frac{\ln\left(\frac{4^x-3^x}{4^x-1}\right)}{x}(4^x-1)=\\ &=\lim_{x \to 0^+} \frac{ \ln\left(1-\frac{3^x-1}{4^x-1}\right)}{\frac{3^x-1}{4^x-1}}\times  \frac{3^x-1}{4^x-1}\times  \frac{4^x-1}{x} \end{align}$$ How to continue from here?,,"['calculus', 'limits']"
51,"Derivatives 1, 2 and 3 and limits","Derivatives 1, 2 and 3 and limits",,"A question for you. Show that if $\lim_{x\to+\infty} x\,f(x)=0$ and $\lim_{x\to+\infty} x\,f''(x)=0$ then $\lim_{x\to+\infty} x\,f'(x)=0$ Thanks ;)","A question for you. Show that if $\lim_{x\to+\infty} x\,f(x)=0$ and $\lim_{x\to+\infty} x\,f''(x)=0$ then $\lim_{x\to+\infty} x\,f'(x)=0$ Thanks ;)",,"['limits', 'derivatives']"
52,Finding the limit of a function with ArcTan,Finding the limit of a function with ArcTan,,"I've found difficulties finding this limit ( without using Taylor series approximation , as it's intended for the secondary-school ): $$ \lim_{x\ \to\ \infty}\left[\, {x^{3} \over \left(\,x^{2} + 1\,\right)\arctan\left(\,x\,\right)} - {2x \over \pi} \,\right] $$ Thanks.","I've found difficulties finding this limit ( without using Taylor series approximation , as it's intended for the secondary-school ): $$ \lim_{x\ \to\ \infty}\left[\, {x^{3} \over \left(\,x^{2} + 1\,\right)\arctan\left(\,x\,\right)} - {2x \over \pi} \,\right] $$ Thanks.",,"['limits', 'trigonometry']"
53,Proving $\lim_{n\to\infty} \left(\frac {2}{3}\right)^n=0$ using limit definition,Proving  using limit definition,\lim_{n\to\infty} \left(\frac {2}{3}\right)^n=0,"Prove $\displaystyle\lim_{n\to\infty} \left(\frac {2}{3}\right)^n=0$ with the definition of limit. From the definition and since $n\in\mathbb N$ I get that ${\Large\mid} \left(\frac {2}{3}\right)^n{\Large\mid}=\left(\frac {2}{3}\right)^n$ but now I'm not sure what to do, I don't see how taking a $\log$ here would help. I thought of different approach, proving by induction that $\left(\frac {2}{3}\right)^n < \frac 1 n$: The first steps are trivial, then for $n+1$: $\frac {1}{n+1}>\frac {2}{3}\left(\frac {2}{3}\right)^n$, then from the induction hypothesis: $\frac {1}{n+1}>\frac {2}{3}\frac 1 n$ and we get that this true for $n>2$ so we can choose $N_{\epsilon}=\frac 1 {\epsilon}$. Is there another way that does not involve induction?","Prove $\displaystyle\lim_{n\to\infty} \left(\frac {2}{3}\right)^n=0$ with the definition of limit. From the definition and since $n\in\mathbb N$ I get that ${\Large\mid} \left(\frac {2}{3}\right)^n{\Large\mid}=\left(\frac {2}{3}\right)^n$ but now I'm not sure what to do, I don't see how taking a $\log$ here would help. I thought of different approach, proving by induction that $\left(\frac {2}{3}\right)^n < \frac 1 n$: The first steps are trivial, then for $n+1$: $\frac {1}{n+1}>\frac {2}{3}\left(\frac {2}{3}\right)^n$, then from the induction hypothesis: $\frac {1}{n+1}>\frac {2}{3}\frac 1 n$ and we get that this true for $n>2$ so we can choose $N_{\epsilon}=\frac 1 {\epsilon}$. Is there another way that does not involve induction?",,"['calculus', 'limits', 'epsilon-delta']"
54,Finding limit of the sequence,Finding limit of the sequence,,"Given a sequence defined as $${a_{n}}= \frac{1} {\sqrt{2}} + \frac{2} {3} + \frac{3} {\sqrt{28}} + \cdots \cdots + \frac{n} {\sqrt{n^3+1}}$$ and I want to find the limit of this sequence. I think the limit is positive infinity, but I don't know what I should do in order to prove this. Please give me some hints on how to approach this, thanks to anybody who helps.","Given a sequence defined as $${a_{n}}= \frac{1} {\sqrt{2}} + \frac{2} {3} + \frac{3} {\sqrt{28}} + \cdots \cdots + \frac{n} {\sqrt{n^3+1}}$$ and I want to find the limit of this sequence. I think the limit is positive infinity, but I don't know what I should do in order to prove this. Please give me some hints on how to approach this, thanks to anybody who helps.",,"['sequences-and-series', 'limits']"
55,Proving a limit with epsilon delta definition: $\lim\limits_{x\to 3} \frac{2}{x+1} =\frac12$,Proving a limit with epsilon delta definition:,\lim\limits_{x\to 3} \frac{2}{x+1} =\frac12,I am in honors Calculus I and my teacher is really stressing this limit proof. I understand the examples she goes over in class but she gave us a problem for home work and i just don't know how to start it. I appreciate any help! $$\lim_{x\to 3}  \frac{2}{x+1} =\frac12$$,I am in honors Calculus I and my teacher is really stressing this limit proof. I understand the examples she goes over in class but she gave us a problem for home work and i just don't know how to start it. I appreciate any help!,\lim_{x\to 3}  \frac{2}{x+1} =\frac12,"['calculus', 'limits', 'epsilon-delta']"
56,To find a trigonometric limit without Wallis' integrals,To find a trigonometric limit without Wallis' integrals,,What is the limit $$ \lambda =\lim\limits_{n \to \infty}{n\int_0^{\frac{\pi}{2}}(\sin x)^{2n} dx}$$ I would like to find it without Wallis' integral formula: I mean without evaluating the closed form. Is it possible? Thanks.,What is the limit $$ \lambda =\lim\limits_{n \to \infty}{n\int_0^{\frac{\pi}{2}}(\sin x)^{2n} dx}$$ I would like to find it without Wallis' integral formula: I mean without evaluating the closed form. Is it possible? Thanks.,,"['limits', 'trigonometry']"
57,Calculate $\lim_{n\to\infty} n^\alpha \Big(\frac{\sqrt[n+1]{(n+1)!}}{n+1} - \frac{\sqrt[n]{n!}}{n}\Big)$,Calculate,\lim_{n\to\infty} n^\alpha \Big(\frac{\sqrt[n+1]{(n+1)!}}{n+1} - \frac{\sqrt[n]{n!}}{n}\Big),"Let $\alpha$ be a positive number. Find $$\lim_{n\to\infty} n^\alpha \Big(\frac{\sqrt[n+1]{(n+1)!}}{n+1} - \frac{\sqrt[n]{n!}}{n}\Big).$$ I'd love to post a useful solution attempt, but all of my efforts seem far off. :) Please help me, thank you!","Let $\alpha$ be a positive number. Find $$\lim_{n\to\infty} n^\alpha \Big(\frac{\sqrt[n+1]{(n+1)!}}{n+1} - \frac{\sqrt[n]{n!}}{n}\Big).$$ I'd love to post a useful solution attempt, but all of my efforts seem far off. :) Please help me, thank you!",,"['calculus', 'limits']"
58,Finding limit of cube root [duplicate],Finding limit of cube root [duplicate],,"This question already has answers here : Finding derivative of $\sqrt[3]{x}$ using only limits (5 answers) Closed 10 years ago . I'm trying to evaluate this limit, but I don't think it's coming out correctly. Could someone please offer me some assistance? Evaluate limit analytically   $$\lim_{h\to 0}\frac{\sqrt[3]{x + h} - \sqrt[3]{x}}{h}.$$ What I did was multiply $(x+h)^{2/3} + x^{2/3}$ top and bottom to get $$\lim_{h\to 0}\frac{(x+h)-x}{h((x+h)^{2/3} + x^{2/3})}.$$ I end up getting $\dfrac{1}{2x^{2/3}}$. The reason why I don't think I did this write is because isn't the limit above the definition of a derivative? And if so, then isn't the derivative of $\sqrt[3]{x}$ equal to $\dfrac{1}{3x^{2/3}}$? I would really appreciate any kind of help. Thanks.","This question already has answers here : Finding derivative of $\sqrt[3]{x}$ using only limits (5 answers) Closed 10 years ago . I'm trying to evaluate this limit, but I don't think it's coming out correctly. Could someone please offer me some assistance? Evaluate limit analytically   $$\lim_{h\to 0}\frac{\sqrt[3]{x + h} - \sqrt[3]{x}}{h}.$$ What I did was multiply $(x+h)^{2/3} + x^{2/3}$ top and bottom to get $$\lim_{h\to 0}\frac{(x+h)-x}{h((x+h)^{2/3} + x^{2/3})}.$$ I end up getting $\dfrac{1}{2x^{2/3}}$. The reason why I don't think I did this write is because isn't the limit above the definition of a derivative? And if so, then isn't the derivative of $\sqrt[3]{x}$ equal to $\dfrac{1}{3x^{2/3}}$? I would really appreciate any kind of help. Thanks.",,"['limits', 'derivatives']"
59,proof of special trig limits,proof of special trig limits,,"I'm trying to prove a special trig limit, which is... $$\lim_{x \rightarrow 0} \frac{1 - \cos{x}}{x}=0$$ So far, this is what I have (and I'll explain where I'm confused) Using the squeeze theorem, $h(x) \leq f(x) \leq g(x)$ $$-x^2 + 1 \leq \cos{x} \leq 1 $$ $$-x^2 + 1 - 1 \leq \cos x - 1 \leq 1 - 1$$ $$-x^2 \leq \cos{x} - 1 \leq 0$$ $$0 \leq 1 - \cos{x} \leq x^2 $$ $$0 \leq \frac{1- \cos{x}}{x} \leq x$$ Since limit of $0$ and $x$ equals zero (as $x$ approaches zero), so does $\displaystyle{\frac{1-\cos{x}}{x}}$. My first confusion, is when I try to graph the last line as separate functions In quadrant $3$ and $4$, it holds up. However, in quadrant $1$ and $2$, it becomes  $\displaystyle{x \leq \frac{1-\cos{x}}{x} \leq 0}$. I'm not sure if this is allowed in squeeze theorem, but I'm a tad bit confused. I have another guess as to why the end result is incorrect because $-x^2 + 1 \leq \cos{x} \leq 1$  are not the correct ""sandwich"" functions. $f(x) = \cos{x}$ touches $g(x) = 1$ in more than one spot. Every diagram of squeeze theorem I've seen, the sandwich functions only touch $f(x)$ at one spot. Is this a criteria I'm unaware of for picking $h(x)$ and $g(x)$? PS, I know I could have used $h(x) = -x^2 + 1$ and $g(x) = x^2 + 1$, but I'd still like to know what I did wrong up top, please. Thanks for any helps, guys/gals/automatons","I'm trying to prove a special trig limit, which is... $$\lim_{x \rightarrow 0} \frac{1 - \cos{x}}{x}=0$$ So far, this is what I have (and I'll explain where I'm confused) Using the squeeze theorem, $h(x) \leq f(x) \leq g(x)$ $$-x^2 + 1 \leq \cos{x} \leq 1 $$ $$-x^2 + 1 - 1 \leq \cos x - 1 \leq 1 - 1$$ $$-x^2 \leq \cos{x} - 1 \leq 0$$ $$0 \leq 1 - \cos{x} \leq x^2 $$ $$0 \leq \frac{1- \cos{x}}{x} \leq x$$ Since limit of $0$ and $x$ equals zero (as $x$ approaches zero), so does $\displaystyle{\frac{1-\cos{x}}{x}}$. My first confusion, is when I try to graph the last line as separate functions In quadrant $3$ and $4$, it holds up. However, in quadrant $1$ and $2$, it becomes  $\displaystyle{x \leq \frac{1-\cos{x}}{x} \leq 0}$. I'm not sure if this is allowed in squeeze theorem, but I'm a tad bit confused. I have another guess as to why the end result is incorrect because $-x^2 + 1 \leq \cos{x} \leq 1$  are not the correct ""sandwich"" functions. $f(x) = \cos{x}$ touches $g(x) = 1$ in more than one spot. Every diagram of squeeze theorem I've seen, the sandwich functions only touch $f(x)$ at one spot. Is this a criteria I'm unaware of for picking $h(x)$ and $g(x)$? PS, I know I could have used $h(x) = -x^2 + 1$ and $g(x) = x^2 + 1$, but I'd still like to know what I did wrong up top, please. Thanks for any helps, guys/gals/automatons",,"['calculus', 'limits']"
60,What is the general limit theorem?,What is the general limit theorem?,,"There are simple limit theorems like http://archives.math.utk.edu/visual.calculus/1/limits.18/ But they are just special cases. I am quite sure there is an established general result for them. In other words, for what conditions of h does  $$\lim_{x\rightarrow a}h(f(x),g(x))=h(\lim_{x\rightarrow a}f(x),\lim_{x\rightarrow a}g(x))$$ hold? For what conditions of h does $$\lim_{x\rightarrow a}h(f(x))=h(\lim_{x\rightarrow a}f(x))$$ hold? ================================== mixedmath's answer: we have $$\lim_{x\rightarrow a}h(f(x),g(x))=h(\lim_{x\rightarrow a}f(x),\lim_{x\rightarrow a}g(x))$$ if $h$ is continuous, and we also have  $$\lim_{x\rightarrow a}h(f(x))=h(\lim_{x\rightarrow a}f(x))$$ if $h$ is continuous.","There are simple limit theorems like http://archives.math.utk.edu/visual.calculus/1/limits.18/ But they are just special cases. I am quite sure there is an established general result for them. In other words, for what conditions of h does  $$\lim_{x\rightarrow a}h(f(x),g(x))=h(\lim_{x\rightarrow a}f(x),\lim_{x\rightarrow a}g(x))$$ hold? For what conditions of h does $$\lim_{x\rightarrow a}h(f(x))=h(\lim_{x\rightarrow a}f(x))$$ hold? ================================== mixedmath's answer: we have $$\lim_{x\rightarrow a}h(f(x),g(x))=h(\lim_{x\rightarrow a}f(x),\lim_{x\rightarrow a}g(x))$$ if $h$ is continuous, and we also have  $$\lim_{x\rightarrow a}h(f(x))=h(\lim_{x\rightarrow a}f(x))$$ if $h$ is continuous.",,"['calculus', 'limits', 'convergence-divergence']"
61,Solving a recurrence for a random walk revisited,Solving a recurrence for a random walk revisited,,"I previously asked about the following recurrence which I get when trying to solve a random walk problem given a positive integer $x$. $p_i = \dfrac{p_{i-1}}{2} + \dfrac{p_{i+2}}{2}$ if $0< i < x$ $p_i = 1$ if $i \geq x$ $p_0 = \dfrac{p_{2}}{2}$ Numerically it appears that $p_0$ tends to the ratio between every second term in the Fibonacci sequence as $x \to \infty$. Why is this? We previously showed that the generating function for the recurrence is $U(z) = \frac{zp_1 + p_0}{z^3-2z^2+1}$.  @Did also showed that $$p_i=As^i+B(-s)^{-i}+C$$ where  $s=\frac12(\sqrt5-1)$ . The boundary conditions $p_2=2p_0$ and $p_x=p_{x+1}=1$ indicate that $$ (2-s^2)A+(2-s^{-2})B+C=0, $$ and $$ As^x+B(-s)^{-x}+C=As^{x+1}+B(-s)^{-x-1}+C=1. $$ This enables us in principle to determine $A,B,$ and $C$.  However the solution I get using Maple looks really horrible. As I only want to get $p_0$ and $p_1$ in this question is there some way of getting a good large $x$ estimate that is easier to interpret? It also seems numerically that as $x\to \infty$, $p_1$ tends to $s$ (that is the ratio between consecutive terms in the Fibonacci sequence).    Why is this?","I previously asked about the following recurrence which I get when trying to solve a random walk problem given a positive integer $x$. $p_i = \dfrac{p_{i-1}}{2} + \dfrac{p_{i+2}}{2}$ if $0< i < x$ $p_i = 1$ if $i \geq x$ $p_0 = \dfrac{p_{2}}{2}$ Numerically it appears that $p_0$ tends to the ratio between every second term in the Fibonacci sequence as $x \to \infty$. Why is this? We previously showed that the generating function for the recurrence is $U(z) = \frac{zp_1 + p_0}{z^3-2z^2+1}$.  @Did also showed that $$p_i=As^i+B(-s)^{-i}+C$$ where  $s=\frac12(\sqrt5-1)$ . The boundary conditions $p_2=2p_0$ and $p_x=p_{x+1}=1$ indicate that $$ (2-s^2)A+(2-s^{-2})B+C=0, $$ and $$ As^x+B(-s)^{-x}+C=As^{x+1}+B(-s)^{-x-1}+C=1. $$ This enables us in principle to determine $A,B,$ and $C$.  However the solution I get using Maple looks really horrible. As I only want to get $p_0$ and $p_1$ in this question is there some way of getting a good large $x$ estimate that is easier to interpret? It also seems numerically that as $x\to \infty$, $p_1$ tends to $s$ (that is the ratio between consecutive terms in the Fibonacci sequence).    Why is this?",,"['limits', 'asymptotics']"
62,When is it possible to pass to the limit in the base and the exponent separately?,When is it possible to pass to the limit in the base and the exponent separately?,,"$$\eqalign{   & \mathop {\lim }\limits_{n \to \infty } {\left( {{{4{n^2}} \over {(2n + 1)(2n - 1)}}} \right)^{1 - {n^2}}} = \mathop {\lim }\limits_{n \to \infty } {\left( {{1 \over {{{(2n + 1)(2n - 1)} \over {4{n^2}}}}}} \right)^{1 - {n^2}}} = \mathop {\lim }\limits_{n \to \infty } {\left( {{1 \over {{{4{n^2} - 1} \over {4{n^2}}}}}} \right)^{1 - {n^2}}} = \mathop {\lim }\limits_{n \to \infty } {\left( {{1 \over {1 - {1 \over {4{n^2}}}}}} \right)^{1 - {n^2}}} = \mathop {\lim }\limits_{n \to \infty } {1 \over {{{\left( {1 - {1 \over {4{n^2}}}} \right)}^{1 - {n^2}}}}} = \mathop {\lim }\limits_{n \to \infty } {1 \over {{{\left( {{{\left( {1 + {1 \over { - 4{n^2}}}} \right)}^{ - 4{n^2}}}} \right)}^{{{1 - {n^2}} \over { - 4{n^2}}}}}}} =   \cr    & \mathop {\lim }\limits_{n \to \infty } {1 \over {{e^{{{1 - {n^2}} \over { - 4{n^2}}}}}}} = {1 \over {{e^{{1 \over 4}}}}} = {\left( {{1 \over e}} \right)^{{1 \over 4}}}  \cr} $$ Is is right to do this? EDIT: Please notice that at some point I ""converted"" the expression $\mathop {\lim }\limits_{n \to \infty } {\left( {1 + {1 \over { - 4{n^2}}}} \right)^{ - 4{n^2}}}$ to $e$, and then kept evaluating the ""rest"" of the expression. Why is it legal?","$$\eqalign{   & \mathop {\lim }\limits_{n \to \infty } {\left( {{{4{n^2}} \over {(2n + 1)(2n - 1)}}} \right)^{1 - {n^2}}} = \mathop {\lim }\limits_{n \to \infty } {\left( {{1 \over {{{(2n + 1)(2n - 1)} \over {4{n^2}}}}}} \right)^{1 - {n^2}}} = \mathop {\lim }\limits_{n \to \infty } {\left( {{1 \over {{{4{n^2} - 1} \over {4{n^2}}}}}} \right)^{1 - {n^2}}} = \mathop {\lim }\limits_{n \to \infty } {\left( {{1 \over {1 - {1 \over {4{n^2}}}}}} \right)^{1 - {n^2}}} = \mathop {\lim }\limits_{n \to \infty } {1 \over {{{\left( {1 - {1 \over {4{n^2}}}} \right)}^{1 - {n^2}}}}} = \mathop {\lim }\limits_{n \to \infty } {1 \over {{{\left( {{{\left( {1 + {1 \over { - 4{n^2}}}} \right)}^{ - 4{n^2}}}} \right)}^{{{1 - {n^2}} \over { - 4{n^2}}}}}}} =   \cr    & \mathop {\lim }\limits_{n \to \infty } {1 \over {{e^{{{1 - {n^2}} \over { - 4{n^2}}}}}}} = {1 \over {{e^{{1 \over 4}}}}} = {\left( {{1 \over e}} \right)^{{1 \over 4}}}  \cr} $$ Is is right to do this? EDIT: Please notice that at some point I ""converted"" the expression $\mathop {\lim }\limits_{n \to \infty } {\left( {1 + {1 \over { - 4{n^2}}}} \right)^{ - 4{n^2}}}$ to $e$, and then kept evaluating the ""rest"" of the expression. Why is it legal?",,"['calculus', 'limits', 'exponential-function']"
63,"According to $\lim_{n\to\infty}\frac{\sum_{k=1}^na_k}{n}=0,\lim_{n\to\infty}(a_{n+1}-a_n)=0,$ then can we get $\lim_{n\to\infty}a_n=0?$",According to  then can we get,"\lim_{n\to\infty}\frac{\sum_{k=1}^na_k}{n}=0,\lim_{n\to\infty}(a_{n+1}-a_n)=0, \lim_{n\to\infty}a_n=0?","Suppose that $\{a_n\}$ is a real sequence with $$\lim_{n\to\infty}\frac{\sum\limits_{k=1}^na_k}{n}=0,\lim_{n\to\infty}(a_{n+1}-a_n)=0,$$ then can we get $$\lim_{n\to\infty}a_n=0?$$ This simple problem has got on my nerves for two days, I've tried to prove that is ture, however, there's nothing I can get.","Suppose that $\{a_n\}$ is a real sequence with $$\lim_{n\to\infty}\frac{\sum\limits_{k=1}^na_k}{n}=0,\lim_{n\to\infty}(a_{n+1}-a_n)=0,$$ then can we get $$\lim_{n\to\infty}a_n=0?$$ This simple problem has got on my nerves for two days, I've tried to prove that is ture, however, there's nothing I can get.",,"['real-analysis', 'sequences-and-series', 'limits']"
64,Prove that $\limsup_{n \to \infty} \left(a_n + b_n \right) \leq \limsup_{n \to \infty} a_n +\limsup_{n \to \infty} b_n$ [duplicate],Prove that  [duplicate],\limsup_{n \to \infty} \left(a_n + b_n \right) \leq \limsup_{n \to \infty} a_n +\limsup_{n \to \infty} b_n,"This question already has answers here : Prove $\limsup\limits_{n \to \infty} (a_n+b_n) \le \limsup\limits_{n \to \infty} a_n + \limsup\limits_{n \to \infty} b_n$ (5 answers) Closed 8 years ago . I want to prove that for two sequences, say $a_k$ and $b_k:$ $$ \limsup_{n \to \infty} \left(a_n + b_n \right) \leq \limsup_{n \to \infty} a_n +\limsup_{n \to \infty} b_n$$ If we let $M_n =\sup \{ a_k : k \geq n \}$ and $N_n = \sup \{ b_k : k\geq n  \} $, then obviously $$a_k +b_k \leq M_n+ N_n \Leftrightarrow  \sup \{a_k+b_k: k \geq n \} \leq M_n + N_n$$ After that, the order limit theorem should suffice, right? Thanks.","This question already has answers here : Prove $\limsup\limits_{n \to \infty} (a_n+b_n) \le \limsup\limits_{n \to \infty} a_n + \limsup\limits_{n \to \infty} b_n$ (5 answers) Closed 8 years ago . I want to prove that for two sequences, say $a_k$ and $b_k:$ $$ \limsup_{n \to \infty} \left(a_n + b_n \right) \leq \limsup_{n \to \infty} a_n +\limsup_{n \to \infty} b_n$$ If we let $M_n =\sup \{ a_k : k \geq n \}$ and $N_n = \sup \{ b_k : k\geq n  \} $, then obviously $$a_k +b_k \leq M_n+ N_n \Leftrightarrow  \sup \{a_k+b_k: k \geq n \} \leq M_n + N_n$$ After that, the order limit theorem should suffice, right? Thanks.",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits', 'self-learning']"
65,"limit of $x \log y$ at $ (0,0)$",limit of  at,"x \log y  (0,0)","What is the limit of the function $x \log y $ at $(0,0)$? I believe the limit doesn't exist. But wolframalpha.com says the limit is 0. http://www.wolframalpha.com/input/?i=limit[+x+log%28y%29++%2C+x-%3E0%2C+y-%3E+0+]","What is the limit of the function $x \log y $ at $(0,0)$? I believe the limit doesn't exist. But wolframalpha.com says the limit is 0. http://www.wolframalpha.com/input/?i=limit[+x+log%28y%29++%2C+x-%3E0%2C+y-%3E+0+]",,['limits']
66,"Question from Putnam '08: Given $F_n(x)$, find $\lim_{n\to\infty}\frac{n!F_n(1)}{\ln(n)}$","Question from Putnam '08: Given , find",F_n(x) \lim_{n\to\infty}\frac{n!F_n(1)}{\ln(n)},"Problem Statement: Let $F_0(x) = \ln(x)$.  For $n\ge0$ and $x\gt0$, let $F_{n+1}(x) = \int_0^xF_n(t)dt$.  Evaluate   $$\lim_{n\to\infty}\frac{n!F_n(1)}{\ln(n)}$$   Source: Putnam 2008, Problem B2. My solution: First, show by induction that: $$F_n(x) = \frac{x^n}{n!}\left(\ln(x) - H_n\right)$$ ...where $H_n$ is the $n$th harmonic number. (Induction proof omitted because it is trivial, and doesn't relate to my question) Then, the limit becomes: $$\begin{align} \lim_{n\to\infty}\frac{n!F_n(1)}{\ln(n)} &= \lim_{n\to\infty}\frac{n!}{\ln(n)}\frac{1^n}{n!}\left(\ln(1) - H_n\right) \\ &= \lim_{n\to\infty}\frac{\left(\ln(1) - H_n\right)}{\ln(n)}\\ &= \lim_{n\to\infty}\frac{-H_n}{\ln(n)}\\ &= \lim_{n\to\infty}\frac{-H_n}{\ln(n)} \tag{1}\\ &= -\lim_{n\to\infty}\frac{\ln(n) + \gamma}{\ln(n)} \tag{2}\\ &\overset{\text{L'H}}{=} -\lim_{n\to\infty}\frac{\frac{1}{n}}{\frac{1}{n}} \tag{3}\\ &= -1 \tag{4}\\ \end{align}$$ (where $\gamma$ is Euler's Constant) My Questions For Putnam-style grading, do I have to prove the limit exists by definition (e.g. $\epsilon-\delta$ proof)? Do I have to show that I can apply limit rules (e.g. L'Hopital's from $(2)$ to $(3)$) to limits of discrete variables? In between lines $(1)$ and $(2)$, I made a jump based on the definition that $\lim_{n\to\infty}(H_n - \ln(n)) = \gamma$  So, as $n$ becomes large, $H_n$ is approximately $\ln(n) + \gamma$.  I am most uneasy about this step, as it feels like a ""back of the paper"" sort of substitution and not a rigorous one.  Is this a justified step? And, of course, I'd be open to suggestions/comments on the style of my solution and general rigor, but the above three points are the main ones to which I'm looking for answers.","Problem Statement: Let $F_0(x) = \ln(x)$.  For $n\ge0$ and $x\gt0$, let $F_{n+1}(x) = \int_0^xF_n(t)dt$.  Evaluate   $$\lim_{n\to\infty}\frac{n!F_n(1)}{\ln(n)}$$   Source: Putnam 2008, Problem B2. My solution: First, show by induction that: $$F_n(x) = \frac{x^n}{n!}\left(\ln(x) - H_n\right)$$ ...where $H_n$ is the $n$th harmonic number. (Induction proof omitted because it is trivial, and doesn't relate to my question) Then, the limit becomes: $$\begin{align} \lim_{n\to\infty}\frac{n!F_n(1)}{\ln(n)} &= \lim_{n\to\infty}\frac{n!}{\ln(n)}\frac{1^n}{n!}\left(\ln(1) - H_n\right) \\ &= \lim_{n\to\infty}\frac{\left(\ln(1) - H_n\right)}{\ln(n)}\\ &= \lim_{n\to\infty}\frac{-H_n}{\ln(n)}\\ &= \lim_{n\to\infty}\frac{-H_n}{\ln(n)} \tag{1}\\ &= -\lim_{n\to\infty}\frac{\ln(n) + \gamma}{\ln(n)} \tag{2}\\ &\overset{\text{L'H}}{=} -\lim_{n\to\infty}\frac{\frac{1}{n}}{\frac{1}{n}} \tag{3}\\ &= -1 \tag{4}\\ \end{align}$$ (where $\gamma$ is Euler's Constant) My Questions For Putnam-style grading, do I have to prove the limit exists by definition (e.g. $\epsilon-\delta$ proof)? Do I have to show that I can apply limit rules (e.g. L'Hopital's from $(2)$ to $(3)$) to limits of discrete variables? In between lines $(1)$ and $(2)$, I made a jump based on the definition that $\lim_{n\to\infty}(H_n - \ln(n)) = \gamma$  So, as $n$ becomes large, $H_n$ is approximately $\ln(n) + \gamma$.  I am most uneasy about this step, as it feels like a ""back of the paper"" sort of substitution and not a rigorous one.  Is this a justified step? And, of course, I'd be open to suggestions/comments on the style of my solution and general rigor, but the above three points are the main ones to which I'm looking for answers.",,"['limits', 'contest-math', 'proof-verification']"
67,Proving $\lim_{h \to 0} \frac {\arccos(\cos^2h)} {h}$ doesn't exist,Proving  doesn't exist,\lim_{h \to 0} \frac {\arccos(\cos^2h)} {h},"I want to show $\arcsin (\sin^2x) $ is not differentiable at $\pi/2+\pi k$. (if its true). So far I have: $$ \frac{\arcsin (\sin^2(\pi/2+\pi k +h))-\arcsin (\sin^2(\pi/2+\pi k))}{h}=\frac{\arcsin (\cos^2h)-\ \pi/2}{h} = \frac {\arccos (\cos^2 h)}{h} $$ so it's sufficient to wish to show this limit exists or doesn't exist: $$\lim_{h \to 0} \frac {\arccos(\cos^2h)} {h}$$ Not sure how to proceed. EDIT: I edited the question to cover $-\pi/2 +2\pi k$ aswell. I feel the question didn't get enough attention. I have been told by a friend it's solveable using l'hopitals rule looking at the one sided limits, however l'hopitals is 2 months ahead in my course. a little strange this problem appears now.","I want to show $\arcsin (\sin^2x) $ is not differentiable at $\pi/2+\pi k$. (if its true). So far I have: $$ \frac{\arcsin (\sin^2(\pi/2+\pi k +h))-\arcsin (\sin^2(\pi/2+\pi k))}{h}=\frac{\arcsin (\cos^2h)-\ \pi/2}{h} = \frac {\arccos (\cos^2 h)}{h} $$ so it's sufficient to wish to show this limit exists or doesn't exist: $$\lim_{h \to 0} \frac {\arccos(\cos^2h)} {h}$$ Not sure how to proceed. EDIT: I edited the question to cover $-\pi/2 +2\pi k$ aswell. I feel the question didn't get enough attention. I have been told by a friend it's solveable using l'hopitals rule looking at the one sided limits, however l'hopitals is 2 months ahead in my course. a little strange this problem appears now.",,"['calculus', 'real-analysis', 'limits', 'derivatives']"
68,Can I convert to polar coordinates when calculating multivariate limits with three variables,Can I convert to polar coordinates when calculating multivariate limits with three variables,,"When working on limits of functions with two variables, $f(x,y)$, I like to convert the problem to polar coordinates a lot of the time, by changing the question from $$\lim_{(x,y)\to (0,0)}f(x,y)$$ to $$\displaystyle\lim_{r\to 0}f(r\cos\theta,r\sin\theta).$$ I was just doing some problems in my book when I encountered a limit of a function with three variables, $f(x,y,z)$. I was just wondering if there was a way to calculate such a limit with polar coordinates. An example being: $$\lim_{(x,y,z)\to(0,0,0)}\frac{xy+yz^2+xz^2}{x^2+y^2+z^4}$$ Converting it into polar coordinates gives me: $\displaystyle\lim_{r\to 0}\dfrac{r^2\sin\theta\cos\theta+r\sin\theta \cdot z^2+r\cos\theta\cdot z^2}{r^2(\sin^2\theta+\cos^2\theta)+z^4}=\displaystyle\lim_{r\to 0}\dfrac{r(r\sin\theta\cos\theta+\sin\theta\cdot z^2+\cos\theta\cdot z^2)}{r^2+z^4}$ Can I proceed or is polar coordinates strictly for use with two variables only?","When working on limits of functions with two variables, $f(x,y)$, I like to convert the problem to polar coordinates a lot of the time, by changing the question from $$\lim_{(x,y)\to (0,0)}f(x,y)$$ to $$\displaystyle\lim_{r\to 0}f(r\cos\theta,r\sin\theta).$$ I was just doing some problems in my book when I encountered a limit of a function with three variables, $f(x,y,z)$. I was just wondering if there was a way to calculate such a limit with polar coordinates. An example being: $$\lim_{(x,y,z)\to(0,0,0)}\frac{xy+yz^2+xz^2}{x^2+y^2+z^4}$$ Converting it into polar coordinates gives me: $\displaystyle\lim_{r\to 0}\dfrac{r^2\sin\theta\cos\theta+r\sin\theta \cdot z^2+r\cos\theta\cdot z^2}{r^2(\sin^2\theta+\cos^2\theta)+z^4}=\displaystyle\lim_{r\to 0}\dfrac{r(r\sin\theta\cos\theta+\sin\theta\cdot z^2+\cos\theta\cdot z^2)}{r^2+z^4}$ Can I proceed or is polar coordinates strictly for use with two variables only?",,"['limits', 'multivariable-calculus', 'polar-coordinates']"
69,"Infinitely many moons, or one ring to bring them all, a limit to bind it?","Infinitely many moons, or one ring to bring them all, a limit to bind it?",,"The Kanagy clan makes its home on a distant planet of mass $M_p$ with $k$ moons. Suppose the moons are identical with mass $m$. Furthermore, these moons share a common circular orbit on an orbital plane. The circular orbits are at distance $L$ from the center of the planet and the moons are evenly spaced. What is the speed $v(k)$ of the lunar orbits for a fixed, but finite, value of $k$? Suppose we hold $km=M_m$ fixed as $k \rightarrow \infty$, what is the limiting value of $\lim_{k \rightarrow \infty}v(k)$ in this context? My ideal solution to this problem addresses $(1.)$ by vector analysis paired with the equation of motion for constant speed circular motion. Often cases $k=2$ or $k=3$ are given as homework problems in first semester university physics. I've found an expression for this in a previous attempt, but I'd rather not include it here for fear of biasing the reader. Next, the solution continues to $(2.)$, when I attempted to compute the limit directly it was rather involved. However, by intuition, I know the answer should easily derive from Newton's Law of Gravitation as follows: $$ \frac{M_mv^2}{L}= \frac{GM_mM_p}{L^2} \qquad \Rightarrow \qquad v = \sqrt{\frac{GM_p}{L}} $$ Given the preceding discussion, show show $v(k) \rightarrow \sqrt{\frac{GM_p}{L}}$ as $k \rightarrow \infty$. Alternatively, prove my intuition is incorrect. Incidentally, I gave this as a bonus problem on a final exam in my university physics course. I had a student pretty well solve $(1.)$, but $(2.)$ I've not yet cracked. It is assumed that classical mechanics applies to this problem and any relativistic effects may be neglected.","The Kanagy clan makes its home on a distant planet of mass $M_p$ with $k$ moons. Suppose the moons are identical with mass $m$. Furthermore, these moons share a common circular orbit on an orbital plane. The circular orbits are at distance $L$ from the center of the planet and the moons are evenly spaced. What is the speed $v(k)$ of the lunar orbits for a fixed, but finite, value of $k$? Suppose we hold $km=M_m$ fixed as $k \rightarrow \infty$, what is the limiting value of $\lim_{k \rightarrow \infty}v(k)$ in this context? My ideal solution to this problem addresses $(1.)$ by vector analysis paired with the equation of motion for constant speed circular motion. Often cases $k=2$ or $k=3$ are given as homework problems in first semester university physics. I've found an expression for this in a previous attempt, but I'd rather not include it here for fear of biasing the reader. Next, the solution continues to $(2.)$, when I attempted to compute the limit directly it was rather involved. However, by intuition, I know the answer should easily derive from Newton's Law of Gravitation as follows: $$ \frac{M_mv^2}{L}= \frac{GM_mM_p}{L^2} \qquad \Rightarrow \qquad v = \sqrt{\frac{GM_p}{L}} $$ Given the preceding discussion, show show $v(k) \rightarrow \sqrt{\frac{GM_p}{L}}$ as $k \rightarrow \infty$. Alternatively, prove my intuition is incorrect. Incidentally, I gave this as a bonus problem on a final exam in my university physics course. I had a student pretty well solve $(1.)$, but $(2.)$ I've not yet cracked. It is assumed that classical mechanics applies to this problem and any relativistic effects may be neglected.",,"['limits', 'physics', 'analytic-geometry']"
70,Limits to infinite,Limits to infinite,,$$20.\quad \lim_{x\to\infty}\frac{-6}{5x\sqrt[3]x} = -\frac65\lim_{x\to\infty}\frac1{x^{4/3}}= -\frac65\cdot 0 = 0$$ ( Original scan of problem ) I cant figure out how to resolve this problem. I would say that denominator tends to infinite and limit of -6 / infinite is 0. However the book seems to follow another way. Can you explain please?,$$20.\quad \lim_{x\to\infty}\frac{-6}{5x\sqrt[3]x} = -\frac65\lim_{x\to\infty}\frac1{x^{4/3}}= -\frac65\cdot 0 = 0$$ ( Original scan of problem ) I cant figure out how to resolve this problem. I would say that denominator tends to infinite and limit of -6 / infinite is 0. However the book seems to follow another way. Can you explain please?,,['limits']
71,$\frac{1}{N} | \int_1^N e^{2 \pi i b \log x }dx |\rightarrow \frac{1}{ \sqrt{1 + 4\pi ^2 b^2} } $ as $N \rightarrow \infty$,as,\frac{1}{N} | \int_1^N e^{2 \pi i b \log x }dx |\rightarrow \frac{1}{ \sqrt{1 + 4\pi ^2 b^2} }  N \rightarrow \infty,I want to show that  $$\frac{1}{N} \left| \int_1^N e^{2 \pi i b \log x }dx \right| \rightarrow \frac{1}{ \sqrt{1 + 4\pi ^2 b^2} } $$ as $N \rightarrow \infty$. This what I have done: First do a substitution $u = \log x$ to obtain $$ \int_1^N e^{2 \pi i b \log x }dx = \int_0^{ \log N} e^{(1+ 2 \pi i b ) u }du .$$ Then $$\int_0^{ \log N} e^{(1+ 2 \pi i b ) u }du = \frac{  e^{(1+ 2 \pi i b ) \log N } -1 }{ 1+ 2 \pi i b} = \frac{ N e^{2 \pi i b  \log N } -1 }{ 1+ 2 \pi i b}. $$ So $$\frac{1}{N} \int_1^N e^{2 \pi i b \log x }dx = \frac{  e^{2 \pi i b  \log N }  }{ 1+ 2 \pi i b}-  \frac{ 1   }{ N(1+ 2 \pi i b)}. $$ The second term goes to zero but I'm not sure how to proceed with the first term. EDIT: So my question is how does one evaluate the limit $$ \lim_{N \rightarrow \infty} N^{2 \pi i b } $$,I want to show that  $$\frac{1}{N} \left| \int_1^N e^{2 \pi i b \log x }dx \right| \rightarrow \frac{1}{ \sqrt{1 + 4\pi ^2 b^2} } $$ as $N \rightarrow \infty$. This what I have done: First do a substitution $u = \log x$ to obtain $$ \int_1^N e^{2 \pi i b \log x }dx = \int_0^{ \log N} e^{(1+ 2 \pi i b ) u }du .$$ Then $$\int_0^{ \log N} e^{(1+ 2 \pi i b ) u }du = \frac{  e^{(1+ 2 \pi i b ) \log N } -1 }{ 1+ 2 \pi i b} = \frac{ N e^{2 \pi i b  \log N } -1 }{ 1+ 2 \pi i b}. $$ So $$\frac{1}{N} \int_1^N e^{2 \pi i b \log x }dx = \frac{  e^{2 \pi i b  \log N }  }{ 1+ 2 \pi i b}-  \frac{ 1   }{ N(1+ 2 \pi i b)}. $$ The second term goes to zero but I'm not sure how to proceed with the first term. EDIT: So my question is how does one evaluate the limit $$ \lim_{N \rightarrow \infty} N^{2 \pi i b } $$,,"['calculus', 'limits', 'definite-integrals']"
72,Question about convergence and integration.,Question about convergence and integration.,,"I am studying the paper ""symmetry and non-uniformly elliptic operators - jean dolbeault, patricio felmer and regis monneau"" and in the demonstration of the lemma 8 page: 5, we have the problem: Suppose that $\Omega\subset\mathbb{R^n}$ is a bounded Lipschitz domain, $0\in\partial\Omega$. Consider $u\in C^1(\overline\Omega)$ such that $u(0)=0$, and let $\phi$ be a nonnegative nonzero radial test function, then $$f(0)A(0)\int_{\mathbb{R^n}}\phi(x)dx=\lim_{\varepsilon\rightarrow0}\int_{\varepsilon^{-1}\Omega}f(u(\varepsilon x))\phi(x)dx,$$ where $f\in C(\mathbb{R})$ and $$A(0)=\frac{|S^{n-1}|}{n},$$ if $0\in\Omega$ and $$A(0)=\lim_{\varepsilon\rightarrow0}\frac{|\Omega\cap B(0.\varepsilon)|}{\varepsilon^n},$$ if $0\not\in\Omega$. I don't know how to obtain the equality above.","I am studying the paper ""symmetry and non-uniformly elliptic operators - jean dolbeault, patricio felmer and regis monneau"" and in the demonstration of the lemma 8 page: 5, we have the problem: Suppose that $\Omega\subset\mathbb{R^n}$ is a bounded Lipschitz domain, $0\in\partial\Omega$. Consider $u\in C^1(\overline\Omega)$ such that $u(0)=0$, and let $\phi$ be a nonnegative nonzero radial test function, then $$f(0)A(0)\int_{\mathbb{R^n}}\phi(x)dx=\lim_{\varepsilon\rightarrow0}\int_{\varepsilon^{-1}\Omega}f(u(\varepsilon x))\phi(x)dx,$$ where $f\in C(\mathbb{R})$ and $$A(0)=\frac{|S^{n-1}|}{n},$$ if $0\in\Omega$ and $$A(0)=\lim_{\varepsilon\rightarrow0}\frac{|\Omega\cap B(0.\varepsilon)|}{\varepsilon^n},$$ if $0\not\in\Omega$. I don't know how to obtain the equality above.",,"['analysis', 'limits', 'partial-differential-equations']"
73,Limit of difference of integral and sum,Limit of difference of integral and sum,,"$f:[0,1]\rightarrow\mathbb R$ and $f\in C^1$, then the limit $\lim_{n\rightarrow\infty} n(\int_{0}^{1}f(x)dx-\frac{1}{n}\sum_{k=1}^{n}f(\frac{k-1}{n}))$ exists. I guess the kernel lies in the sum because then I can write the sum as an integral but I do not know how.","$f:[0,1]\rightarrow\mathbb R$ and $f\in C^1$, then the limit $\lim_{n\rightarrow\infty} n(\int_{0}^{1}f(x)dx-\frac{1}{n}\sum_{k=1}^{n}f(\frac{k-1}{n}))$ exists. I guess the kernel lies in the sum because then I can write the sum as an integral but I do not know how.",,"['real-analysis', 'integration', 'limits', 'summation']"
74,$ n $ lines intersections,lines intersections, n ,"As we all know, $ n $ lines which are not coincident may have some intersection points in an Euclid plane. And we define the set of the number of intersection points $ n $ lines can form is $ \mathbb{I} $, and we also define the complementary set $ \mathbb{B} = \{k \; | \; 0 \leq k \leq C_{n}^{2}\} - \mathbb{I} $. The question is, how can we find the maximum value in the set $ \mathbb{B} $, which we called $ L(n) $. If can't, could you prove that $$ \lim_{n \to \infty} \frac{L(n)}{n^{2}} = 0 $$ PS. For example, 5 lines may have 0, 1, 4, 5, 6, 7, 8, 9 intersection points, so set $ \mathbb{I} = \{0, 1, 4, 5, 6, 7, 8, 9\} $ ans set $ \mathbb{B} = \{2, 3\} $, so the $ L(5) = 3 $","As we all know, $ n $ lines which are not coincident may have some intersection points in an Euclid plane. And we define the set of the number of intersection points $ n $ lines can form is $ \mathbb{I} $, and we also define the complementary set $ \mathbb{B} = \{k \; | \; 0 \leq k \leq C_{n}^{2}\} - \mathbb{I} $. The question is, how can we find the maximum value in the set $ \mathbb{B} $, which we called $ L(n) $. If can't, could you prove that $$ \lim_{n \to \infty} \frac{L(n)}{n^{2}} = 0 $$ PS. For example, 5 lines may have 0, 1, 4, 5, 6, 7, 8, 9 intersection points, so set $ \mathbb{I} = \{0, 1, 4, 5, 6, 7, 8, 9\} $ ans set $ \mathbb{B} = \{2, 3\} $, so the $ L(5) = 3 $",,"['number-theory', 'limits', 'recurrence-relations']"
75,How can I tell that the sequence $a_n=\frac {\ln(n)} {n}$ converges and to what it converges?,How can I tell that the sequence  converges and to what it converges?,a_n=\frac {\ln(n)} {n},"I need to ""study the limit behavior"" and compute the limit if it exists. This is what I have done so far. In order to study the limit behavior I tried to first check the monotonicity and boundedness of the sequence. The sequence was found to not be monotonic for all n. Since that failed, I tried to attempt to prove that the sequence was Cauchy, and if it was, that would lead me to have completed the ""study the limit behavior"" part. Here is my attempt at the cauchy part: For every ϵ>$0$ there exists an $N$ such that $m,n>N$ implies |$a_n - a_m|<ϵ$ for $n\geq m$ $$|a_n-a_m|=\bigg|\frac {\ln(n)} {n}-\frac {\ln(m)} {m}\bigg|\leq \bigg|\frac {\ln(n)} {n}\bigg|+\bigg|\frac {\ln(m)} {m}\bigg|\Leftrightarrow \frac {\ln(n)}{n}+\frac{\ln(m)}{m}<ϵ$$ $$\frac {\ln(n)}{n}+\frac{\ln(m)}{m}<ϵ \Leftrightarrow \frac {\ln(n)}{n}<ϵ- \frac {\ln(m)}{m}$$ (Now I get stuck. I dont know if what I did so far is even correct, and if it is I don't know where to go from here) Even though I got stuck at the cauchy part I went on to compute the limit.  $$\lim a_n=\lim\frac{\ln(n)}{n}=\lim\frac{1}{n}\bigg(\ln(n)\bigg)=\lim \ln(n)^{\frac{1}{n}}$$ So,  $\lim {e^{a_n}}=\lim e^{\ln(n)^{\frac{1}{n}}}=\lim n^{\frac {1}{n}}=1$ ( I proved that using the epsilon definition) Therefore because $\lim e^{a_n}=1\ \lim \ln(e^{a_n})= \lim a_n=\ln(1)=0$ Any help (on the cauchy part especially)? Thanks in advance.","I need to ""study the limit behavior"" and compute the limit if it exists. This is what I have done so far. In order to study the limit behavior I tried to first check the monotonicity and boundedness of the sequence. The sequence was found to not be monotonic for all n. Since that failed, I tried to attempt to prove that the sequence was Cauchy, and if it was, that would lead me to have completed the ""study the limit behavior"" part. Here is my attempt at the cauchy part: For every ϵ>$0$ there exists an $N$ such that $m,n>N$ implies |$a_n - a_m|<ϵ$ for $n\geq m$ $$|a_n-a_m|=\bigg|\frac {\ln(n)} {n}-\frac {\ln(m)} {m}\bigg|\leq \bigg|\frac {\ln(n)} {n}\bigg|+\bigg|\frac {\ln(m)} {m}\bigg|\Leftrightarrow \frac {\ln(n)}{n}+\frac{\ln(m)}{m}<ϵ$$ $$\frac {\ln(n)}{n}+\frac{\ln(m)}{m}<ϵ \Leftrightarrow \frac {\ln(n)}{n}<ϵ- \frac {\ln(m)}{m}$$ (Now I get stuck. I dont know if what I did so far is even correct, and if it is I don't know where to go from here) Even though I got stuck at the cauchy part I went on to compute the limit.  $$\lim a_n=\lim\frac{\ln(n)}{n}=\lim\frac{1}{n}\bigg(\ln(n)\bigg)=\lim \ln(n)^{\frac{1}{n}}$$ So,  $\lim {e^{a_n}}=\lim e^{\ln(n)^{\frac{1}{n}}}=\lim n^{\frac {1}{n}}=1$ ( I proved that using the epsilon definition) Therefore because $\lim e^{a_n}=1\ \lim \ln(e^{a_n})= \lim a_n=\ln(1)=0$ Any help (on the cauchy part especially)? Thanks in advance.",,"['sequences-and-series', 'limits']"
76,"What is the solution for $\lim\limits_{x\to\infty}\frac{a^x-1}{x}$, when $a > 1$?","What is the solution for , when ?",\lim\limits_{x\to\infty}\frac{a^x-1}{x} a > 1,"I'm exercising on a book and I'm stuck at the following task: compute the $\lim\limits_{x\to\infty}\frac{a^x-1}{x}$, when $a > 1$. As I can see, when $x\to-\infty$, the numerator tends to $-1$ and denominator tends to $-\infty$, so the whole expression tends to $0$. But when $x\to+\infty$, the whole expression becomes indeterminate form of $\frac{\infty}{\infty}$. Well, $\lim\limits_{x\to+\infty}\frac{a^x-1}{x} = \lim\limits_{x\to+\infty}\frac{a^x}{x} - \lim\limits_{x\to+\infty}\frac{1}{x} = \lim\limits_{x\to+\infty}\frac{a^x}{x}$. Intuitively (and well proved by l'Hôpital's rule) $\frac{a^x}{x}\to+\infty$ when $x\to+\infty$. But this rule is discussed later in the book, so I'm supposed to prove this statement by means of intuitive definition of limit or so. Since $x\to+\infty$, there must be preassigned positive $M$ conforming to $|\frac{a^x}{x}|>M$ when $x>N$, for any preassigned positive $N$. If we express $M$ in terms of $N$: $M=\frac{a^N}{N}$, we can see, that for any $N>0$ there is such $M$. So, by definition, $\lim\limits_{x\to+\infty}\frac{a^x}{x}=+\infty$. Is my reasoning correct?","I'm exercising on a book and I'm stuck at the following task: compute the $\lim\limits_{x\to\infty}\frac{a^x-1}{x}$, when $a > 1$. As I can see, when $x\to-\infty$, the numerator tends to $-1$ and denominator tends to $-\infty$, so the whole expression tends to $0$. But when $x\to+\infty$, the whole expression becomes indeterminate form of $\frac{\infty}{\infty}$. Well, $\lim\limits_{x\to+\infty}\frac{a^x-1}{x} = \lim\limits_{x\to+\infty}\frac{a^x}{x} - \lim\limits_{x\to+\infty}\frac{1}{x} = \lim\limits_{x\to+\infty}\frac{a^x}{x}$. Intuitively (and well proved by l'Hôpital's rule) $\frac{a^x}{x}\to+\infty$ when $x\to+\infty$. But this rule is discussed later in the book, so I'm supposed to prove this statement by means of intuitive definition of limit or so. Since $x\to+\infty$, there must be preassigned positive $M$ conforming to $|\frac{a^x}{x}|>M$ when $x>N$, for any preassigned positive $N$. If we express $M$ in terms of $N$: $M=\frac{a^N}{N}$, we can see, that for any $N>0$ there is such $M$. So, by definition, $\lim\limits_{x\to+\infty}\frac{a^x}{x}=+\infty$. Is my reasoning correct?",,['limits']
77,Uniform Continuity and partial sums equation proof,Uniform Continuity and partial sums equation proof,,"Given $f$, a uniformly continuous function defined on the interval $[0,1]$, I need to prove that  $$\lim_{n\rightarrow \infty}  \frac{1}{2^n} \sum_{k=1}^n (-1)^k \binom{n}{k} f(k/n)=0.$$ I have tried tackling this exercise from a couple of angles but I seem to lack the intuition and technical skills to crack this egg open, so I am at your mercy.","Given $f$, a uniformly continuous function defined on the interval $[0,1]$, I need to prove that  $$\lim_{n\rightarrow \infty}  \frac{1}{2^n} \sum_{k=1}^n (-1)^k \binom{n}{k} f(k/n)=0.$$ I have tried tackling this exercise from a couple of angles but I seem to lack the intuition and technical skills to crack this egg open, so I am at your mercy.",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits', 'continuity']"
78,Convergence of alternating series,Convergence of alternating series,,"I was provided the following first terms of a series in a question from my math book: $-\frac{2}{5}+\frac{4}{6}-\frac{6}{7}+\frac{8}{8}-\frac{10}{9}+...$ I concluded that this is the equivalent of the following sum representation: $\sum\limits_{n=1}^\infty(-1)^n\frac{2n}{n+4}$ I then proceeded to do the alternating series convergence test by first comparing two following terms: $b_n=\frac{2n}{n+4}$ $b_{n+1}=\frac{2n+2}{n+5}$ I concluded that $b_{n+1}>b_n$ for at least one $n$ and thus the series is divergent. The limit test is also conclusive: $\lim_{n->\infty} \frac{2n}{n+4} = \lim_{n->\infty} \frac{2}{1+\frac{4}{n}} = 2$ Since the limit is not equal to $0$, both alternating series tests point out that the series is in fact divergent. Unfortunately the answer pages in my math book itself say that the series is convergent. Although Wolfram Alpha confirms my thoughts by saying the series is divergent, it shows no proof or method to reach this answer. Have I done everything right and is the series in fact divergent?","I was provided the following first terms of a series in a question from my math book: $-\frac{2}{5}+\frac{4}{6}-\frac{6}{7}+\frac{8}{8}-\frac{10}{9}+...$ I concluded that this is the equivalent of the following sum representation: $\sum\limits_{n=1}^\infty(-1)^n\frac{2n}{n+4}$ I then proceeded to do the alternating series convergence test by first comparing two following terms: $b_n=\frac{2n}{n+4}$ $b_{n+1}=\frac{2n+2}{n+5}$ I concluded that $b_{n+1}>b_n$ for at least one $n$ and thus the series is divergent. The limit test is also conclusive: $\lim_{n->\infty} \frac{2n}{n+4} = \lim_{n->\infty} \frac{2}{1+\frac{4}{n}} = 2$ Since the limit is not equal to $0$, both alternating series tests point out that the series is in fact divergent. Unfortunately the answer pages in my math book itself say that the series is convergent. Although Wolfram Alpha confirms my thoughts by saying the series is divergent, it shows no proof or method to reach this answer. Have I done everything right and is the series in fact divergent?",,"['sequences-and-series', 'limits']"
79,Prove that $h^{(k)}(0)=\lim_{t\to0}\frac{\sum_{j=0}^k\binom{k}{j}(-1)^{k-j}h(jt)}{t^k}$,Prove that,h^{(k)}(0)=\lim_{t\to0}\frac{\sum_{j=0}^k\binom{k}{j}(-1)^{k-j}h(jt)}{t^k},"Prove that if $h$ is infinitely differentiable in a neighborhood of $0$, then the kth derivative evaluated at 0 is $$h^{(k)}(0)=\lim_{t\to0}\frac{\sum_{j=0}^k\binom{k}{j}(-1)^{k-j}h(jt)}{t^k}$$","Prove that if $h$ is infinitely differentiable in a neighborhood of $0$, then the kth derivative evaluated at 0 is $$h^{(k)}(0)=\lim_{t\to0}\frac{\sum_{j=0}^k\binom{k}{j}(-1)^{k-j}h(jt)}{t^k}$$",,"['calculus', 'real-analysis', 'analysis', 'limits', 'derivatives']"
80,Order of integration,Order of integration,,"I am reading a book by  L. D. Landau titled Mechanics and there is a ""changing order of the integral"" step on page 28 that I don't get: $$\int_0^a\int_0^E \left[{dx_2\over dU}-{dx_1\over dU}\right]{dUdE\over \sqrt{(a-E)(E-U)}}\\=\int_0^a\left[{dx_2\over dU}-{dx_1\over dU}\right] dU \int_U^a{dE\over \sqrt{(a-E)(E-U)}}$$ I am not very good with changing orders of integrals (or summations for that matter). Could someone please explain? (And if at all possible, offer an intuitive way of understanding it?) Thanks.","I am reading a book by  L. D. Landau titled Mechanics and there is a ""changing order of the integral"" step on page 28 that I don't get: $$\int_0^a\int_0^E \left[{dx_2\over dU}-{dx_1\over dU}\right]{dUdE\over \sqrt{(a-E)(E-U)}}\\=\int_0^a\left[{dx_2\over dU}-{dx_1\over dU}\right] dU \int_U^a{dE\over \sqrt{(a-E)(E-U)}}$$ I am not very good with changing orders of integrals (or summations for that matter). Could someone please explain? (And if at all possible, offer an intuitive way of understanding it?) Thanks.",,"['limits', 'integration']"
81,Difference in limits because of greatest-integer function,Difference in limits because of greatest-integer function,,A Problem : \begin{equation}\lim_{x\to 0} \frac{\sin x}{x}\end{equation} results in the solution : $1$ But the same function enclosed in a greatest integer function results in a $0$ \begin{equation}\lim_{x\to 0} \left\lfloor{\frac{\sin x }{x}}\right\rfloor\end{equation} Why? My thoughts: [The value of the first function tends to 1 because of the expansion : $$\frac{\sin\left( x \right)}{x}\approx\frac{  x - \frac{x^3}{3!} + \frac{x^5}{5!} - \frac{x^7}{7!} + \cdots}{x}$$  $$\approx 1 - \frac{x^2}{3!} + \frac{x^4}{5!} - \frac{x^6}{7!} + \cdots$$ and putting zero in the function results in 1 but applying the greatest integer function to the same will result in a zero as whenever the value of the result is taken it will be slightly less than one because of all the subtractions involved in the expansion.],A Problem : \begin{equation}\lim_{x\to 0} \frac{\sin x}{x}\end{equation} results in the solution : $1$ But the same function enclosed in a greatest integer function results in a $0$ \begin{equation}\lim_{x\to 0} \left\lfloor{\frac{\sin x }{x}}\right\rfloor\end{equation} Why? My thoughts: [The value of the first function tends to 1 because of the expansion : $$\frac{\sin\left( x \right)}{x}\approx\frac{  x - \frac{x^3}{3!} + \frac{x^5}{5!} - \frac{x^7}{7!} + \cdots}{x}$$  $$\approx 1 - \frac{x^2}{3!} + \frac{x^4}{5!} - \frac{x^6}{7!} + \cdots$$ and putting zero in the function results in 1 but applying the greatest integer function to the same will result in a zero as whenever the value of the result is taken it will be slightly less than one because of all the subtractions involved in the expansion.],,"['calculus', 'functions', 'limits', 'taylor-expansion']"
82,Limit and continuity,Limit and continuity,,"For this question, should I use differentiation method or the integration method ? $\lim_{x\to \infty} (\frac{x}{x+2})^{x/8}$ this is what i got so far: Note: $\lim \limits_{n\to\infty} [1 + (a/n)]^n = e^{\underline{a}}\ldots\ldots (1)$ $$ L = \lim \left[\frac{x}{x+2}\right]^{x/8} = \lim\left[\frac{1}{\frac{x+2}{x}}\right]^{x/8} =\frac{1}{\lim [1 + (2/x)]^x]^{1/8}} $$ but i'm not sure where to go from there","For this question, should I use differentiation method or the integration method ? $\lim_{x\to \infty} (\frac{x}{x+2})^{x/8}$ this is what i got so far: Note: $\lim \limits_{n\to\infty} [1 + (a/n)]^n = e^{\underline{a}}\ldots\ldots (1)$ $$ L = \lim \left[\frac{x}{x+2}\right]^{x/8} = \lim\left[\frac{1}{\frac{x+2}{x}}\right]^{x/8} =\frac{1}{\lim [1 + (2/x)]^x]^{1/8}} $$ but i'm not sure where to go from there",,['limits']
83,Lebesgue integral question concerning orders of limit and integration,Lebesgue integral question concerning orders of limit and integration,,"I've got a hand-in question in a pure analysis course that I was hoping I might get a hint on - having difficulty coming up with a decent approach. The question: Let $(X,\Sigma,\mu)$ be a measure space and let $f:X\rightarrow [0,\infty]$ be a measurable function such that $$\int_X f(x)d\mu(x)=A,$$ for some $0<A<\infty$. If $\alpha>0,$ show $$\lim_{n\rightarrow\infty} \int_X n\log\left(1 + \left(\frac{f(x)}{n}\right)^{\alpha} \right)d\mu(x)=\begin{cases} \infty&\mbox{if }0<\alpha<1\\\ A&\mbox{if } \alpha=1\\\ 0&\mbox{if }\alpha>1. \end{cases}$$ My attempt at a solution only comes as far as the first part: \begin{align*} g(x,n)&=n\log(1 + [f(x)/n]^{\alpha})\\ &=n\cdot \sum_{m=1}^{\infty}(-1)^{m+1}[f(x)/n]^{\alpha m}/m\\ &=\sum_{m=1}^{\infty}(-1)^{m+1}\cdot \frac{f(x)^{\alpha m}}{m\cdot n^{\alpha*m-1}} \\ &= \frac{f(x)^\alpha}{n^{\alpha-1}}+\sum_{m=2}^{\infty}(-1)^{m+1}\frac{f(x)^{\alpha m}}{m\cdot n^{\alpha m-1}}, \end{align*}  which is increasing  in $n$ for $\alpha<1$ (this is a bit handwavy, but I can't seem to figure out how to show it in a strict manner). Thus, we can apply the Monotone Convergence Theorem to move the limit inside the integrand, transform $n=1/t$, use a bit of L'hopitals rule, and get that this limit is diverging for any $f(x)$ and $\alpha<1$ (and $f(x)$ for $\alpha=1$, zero for $\alpha>1$). But how do I go about proving that I can switch limit and integrand in these other cases, or is there any other simple way to prove it? Any hints would be much appreciated! Many thanks in advance","I've got a hand-in question in a pure analysis course that I was hoping I might get a hint on - having difficulty coming up with a decent approach. The question: Let $(X,\Sigma,\mu)$ be a measure space and let $f:X\rightarrow [0,\infty]$ be a measurable function such that $$\int_X f(x)d\mu(x)=A,$$ for some $0<A<\infty$. If $\alpha>0,$ show $$\lim_{n\rightarrow\infty} \int_X n\log\left(1 + \left(\frac{f(x)}{n}\right)^{\alpha} \right)d\mu(x)=\begin{cases} \infty&\mbox{if }0<\alpha<1\\\ A&\mbox{if } \alpha=1\\\ 0&\mbox{if }\alpha>1. \end{cases}$$ My attempt at a solution only comes as far as the first part: \begin{align*} g(x,n)&=n\log(1 + [f(x)/n]^{\alpha})\\ &=n\cdot \sum_{m=1}^{\infty}(-1)^{m+1}[f(x)/n]^{\alpha m}/m\\ &=\sum_{m=1}^{\infty}(-1)^{m+1}\cdot \frac{f(x)^{\alpha m}}{m\cdot n^{\alpha*m-1}} \\ &= \frac{f(x)^\alpha}{n^{\alpha-1}}+\sum_{m=2}^{\infty}(-1)^{m+1}\frac{f(x)^{\alpha m}}{m\cdot n^{\alpha m-1}}, \end{align*}  which is increasing  in $n$ for $\alpha<1$ (this is a bit handwavy, but I can't seem to figure out how to show it in a strict manner). Thus, we can apply the Monotone Convergence Theorem to move the limit inside the integrand, transform $n=1/t$, use a bit of L'hopitals rule, and get that this limit is diverging for any $f(x)$ and $\alpha<1$ (and $f(x)$ for $\alpha=1$, zero for $\alpha>1$). But how do I go about proving that I can switch limit and integrand in these other cases, or is there any other simple way to prove it? Any hints would be much appreciated! Many thanks in advance",,"['analysis', 'measure-theory', 'limits']"
84,Calculating the limit of a sequence,Calculating the limit of a sequence,,"I'm currently studying limits because of my calculus class and i've wondered how for example wolfram alpha computes the limit of a sequence. Is it more a brute force way, or is there an efficient method to calculate/find them?","I'm currently studying limits because of my calculus class and i've wondered how for example wolfram alpha computes the limit of a sequence. Is it more a brute force way, or is there an efficient method to calculate/find them?",,['limits']
85,"Limits of a function involving $\mathrm{cn}(x,k)$",Limits of a function involving,"\mathrm{cn}(x,k)","Given $$f(x) = \frac{1 - \mathrm{cn}(x,k)}{{\sqrt3}(1+\mathrm{cn}(x,k)) - 1 + \mathrm{cn}(x,k)}$$ what would be $$\lim_{x\to 0} f(x)$$  and $$\lim_{x\to\infty} f(x)$$ when  $$k=\frac{\sqrt{2-\sqrt{3}}}{2}?$$","Given $$f(x) = \frac{1 - \mathrm{cn}(x,k)}{{\sqrt3}(1+\mathrm{cn}(x,k)) - 1 + \mathrm{cn}(x,k)}$$ what would be $$\lim_{x\to 0} f(x)$$  and $$\lim_{x\to\infty} f(x)$$ when  $$k=\frac{\sqrt{2-\sqrt{3}}}{2}?$$",,"['limits', 'special-functions', 'elliptic-functions']"
86,Limit of the functions of two variables at $\infty$,Limit of the functions of two variables at,\infty,"Does the following equality generally hold? $$ \lim_{x\to\infty, y\to\infty} f(x, y) = \lim_{z\to\infty} f(z, z) $$ If not, what are the necessary conditions for the above equation to hold?","Does the following equality generally hold? $$ \lim_{x\to\infty, y\to\infty} f(x, y) = \lim_{z\to\infty} f(z, z) $$ If not, what are the necessary conditions for the above equation to hold?",,"['calculus', 'limits', 'multivariable-calculus']"
87,"Does it make sense to consider function, that has finite zero value at limit point as ""infinitely small""?","Does it make sense to consider function, that has finite zero value at limit point as ""infinitely small""?",,"Studying the calculus, I am currently reading the chapter ""Comparison of infinitely small functions"". It is started to overview ratios such as $$\lim_{x->x_0}\dfrac{f(x)}{g(x)}=0 \space,  \space \lim_{x->x_0}\dfrac{f(x)}{g(x)}=\infty$$ , where $f(x)$ and $g(x)$ are infinitely small functions. I assume, that by ""infinitely small at limit point $x_0$ function"" meant $\lim_{x->x_0}f(x)=0$ . Reading, I started to think about examples, and I could provide only $f(x)=\dfrac{a}{x}$ at infinity. But then, I see examples of infinitely smalls like $f(x)=x$ at $x->0$ . According to the limit definition, since $\delta$ -neighborhood of $x_0$ considered as deleted , i.e. $0<|x-x_0|<\delta(\epsilon)$ , it seems, like, formally, there is no requirement for function value at limit point itself. I understand why - for cases like $1/x$ , but exactly because of this, as I understand, the idea of infinitely small was invented - to describe the smallest, yet non-zero value , and so called ""infinitely small (function)"" is what mathematics such as Euler, Cauchy, and other invented. If it is true, is not consider functions such $x^3$ as infinitely small intuitively wrong, or I missing some ideas? Maybe historically, the idea of limit of function as argument tends to limit point - was the closest function value to function value at limit point? But the method of finding the closest value to some point was not found, and instead, we have vice-versa: we found a point, that is called an accumulating point, which any $\epsilon$ -neighourhood contains at least one value of function at some another accumulating point, called limit point. So it still makes sense to find what is the closest value of $0$ for $x^3$ , but calculus does not provide such tool, instead providing the tool to compare that infinitely small value to another, for example $x^2$ , because, how it may be non-logical, we see that ""infinitely closest"" to $0$ of $x^3$ is different from ""infinitely closest"" to $0$ of $x^2$ ?","Studying the calculus, I am currently reading the chapter ""Comparison of infinitely small functions"". It is started to overview ratios such as , where and are infinitely small functions. I assume, that by ""infinitely small at limit point function"" meant . Reading, I started to think about examples, and I could provide only at infinity. But then, I see examples of infinitely smalls like at . According to the limit definition, since -neighborhood of considered as deleted , i.e. , it seems, like, formally, there is no requirement for function value at limit point itself. I understand why - for cases like , but exactly because of this, as I understand, the idea of infinitely small was invented - to describe the smallest, yet non-zero value , and so called ""infinitely small (function)"" is what mathematics such as Euler, Cauchy, and other invented. If it is true, is not consider functions such as infinitely small intuitively wrong, or I missing some ideas? Maybe historically, the idea of limit of function as argument tends to limit point - was the closest function value to function value at limit point? But the method of finding the closest value to some point was not found, and instead, we have vice-versa: we found a point, that is called an accumulating point, which any -neighourhood contains at least one value of function at some another accumulating point, called limit point. So it still makes sense to find what is the closest value of for , but calculus does not provide such tool, instead providing the tool to compare that infinitely small value to another, for example , because, how it may be non-logical, we see that ""infinitely closest"" to of is different from ""infinitely closest"" to of ?","\lim_{x->x_0}\dfrac{f(x)}{g(x)}=0 \space,  \space \lim_{x->x_0}\dfrac{f(x)}{g(x)}=\infty f(x) g(x) x_0 \lim_{x->x_0}f(x)=0 f(x)=\dfrac{a}{x} f(x)=x x->0 \delta x_0 0<|x-x_0|<\delta(\epsilon) 1/x x^3 \epsilon 0 x^3 x^2 0 x^3 0 x^2","['calculus', 'limits']"
88,Evaluate infinitely nested function call $f(f(f(....)))$,Evaluate infinitely nested function call,f(f(f(....))),"It's been awhile since I took Calc 1 and I am reviewing it. I came across this problem. Let $x_{1} = 100$ . For all integers $n$ , let $x_{n+1} = \frac{1}{2}(x_{n} + \frac{100}{x_{n}})$ . Assume that $\lim_{n \to \infty}(x_{n}) = L$ , and calculate $L$ . It reminds me of some sort of compound interest problem but I can't figure out how to match this pattern with the compound interest formula. The problem is supposed to be able to be solved with basic limit theorems that you learn about at the beginning of Calc 1. I imagine we have to find some way to rewrite the formula for $x_{n+1}$ in terms of $n$ rather than in terms of $x_{n}$ . Then we can take the limit like normal. The problem is that I don't know how to do that. Nevertheless, I can solve this problem with an overcomplicated and messy proof which probably contains a few oversights and logical leaps. Let $f(x) = \frac{1}{2}\left(x + \frac{100}{x}\right)$ . I'll wager that there exists some $c$ such that if $c < x$ then $c < f(x) < x$ . First I will show that there exists some $c$ such that if $c < x$ then $f(x) < x$ . $$c < x$$ $$c^2 < x^2$$ $$\frac{c^2}{x} < x$$ $$x + \frac{c^2}{x} < 2x$$ $$\frac{1}{2}\left(x + \frac{c^2}{x}\right) < x$$ $$\text{let } c = \pm 10$$ $$\frac{1}{2}\left(x + \frac{(\pm 10)^2}{x}\right) < x$$ $$\frac{1}{2}\left(x + \frac{100}{x}\right) < x$$ $$f(x) < x$$ We have found possible values for $c$ , which are $10$ and $-10$ . Now I will show that, with at least one of the values of $c$ I found in the previous part, it is also true that if $c < x$ then $c < f(x)$ . $$\text{If } c = -10 \text{, then}$$ $$-10 < x$$ $$\text{suppose } x = -5$$ $$f(-5) = \frac{1}{2}\left(5 + \frac{100}{5}\right) = \frac{1}{2}\left(5 + 20\right) = \frac{1}{2}\left(25\right) = -12.5$$ $$-10 < -12.5 \text{ is false}$$ $$\text{so } c \ne -10$$ $$\text{If } c = 10 \text{, then}$$ $$10 < x$$ $$10 (10x - 100) < x (10x - 100)$$ $$100x - 1000 < 10x^2 - 100x$$ $$100x + 100x < 10x^2 + 1000$$ $$x(100 + 100) < 10(x^2 + 100)$$ $$\frac{100 + 100}{10} < \frac{x^2 + 100}{x}$$ $$10 + \frac{100}{10} < x + \frac{100}{x}$$ $$\frac{1}{2}(10 + \frac{100}{10}) < \frac{1}{2}(x + \frac{100}{x})$$ $$f(10) < f(x)$$ $$10 < f(x)$$ $$c < f(x)$$ So to recap, we have that if $10 < x$ then $f(x) < x$ , and also that if $10 < x$ then $10 < f(x)$ . We can combine these two statements together to say that if $10 < x$ then $10 < f(x) < x$ . Given that $n \in \mathbb{Z}$ and some initial $x_{1} > 10$ , we can define $x_{n + 1} = f(x_{n})$ . By our previous conclusion, for any $x_{n}$ , $10 < f(x_{n}) < x_{n}$ , or in other words, $10 < x_{n + 1} < x_{n}$ . We could also say that $10 < x_{a} < x_{b}$ if $a > b$ . Intuitively, it seems that as $n$ tends to infinity, $x_{n}$ tends to $10$ . Or in other words, that $\lim_{n \to \infty}(x_{n}) = 10$ . Now I need to prove it. Given any $\epsilon > 0$ , I want to show that there exists an integer $M$ given by $10 < x_{M} < 10 + \epsilon$ . For the case that $x_{1} \le 10 + \epsilon$ , we can just set $M = 1$ , easy. But for the case that $x_{1} > 10 + \epsilon$ , it's a bit more complicated. I will first assume that there exists no integer $M$ such that $10 < x_{M} < 10 + \epsilon$ . In other words, for any integer $n$ , if $x_{n} > 10 + \epsilon$ , then $10 + \epsilon < x_{n + 1} < x_{n}$ . But then this implies that if we want to find $c$ such that if $x > c$ then $c < f(x) < x$ , then we could find $10 + \epsilon$ as a possible solution for $c$ . But this is clearly false, since earlier we found that there was only one solution for $c$ and it was $10$ , not some $10 + \epsilon$ . With our assumption contradicted, the opposite of our assumption must be true. Therefore, there is some integer $M$ such that $10 < x_{M} < 10 + \epsilon$ . Now with our value of $M$ , we want to show that if $n > M$ , then $\lvert x_{n} - 10 \rvert < \epsilon$ . $$n > M$$ $$10 < x_{n} < x_{M}$$ $$0 < x_{n} - 10 < x_{M} - 10$$ $$-(x_{M} - 10) < 0 < x_{n} - 10 < x_{M} - 10$$ $$-(x_{M} - 10) < x_{n} - 10 < x_{M} - 10$$ $$-\epsilon < -(x_{M} - 10) < x_{n} - 10 < x_{M} - 10 < \epsilon$$ $$-\epsilon < x_{n} - 10 < \epsilon$$ $$\lvert x_{n} - 10 \rvert < \epsilon$$ Therefore, $$\forall \epsilon > 0, \exists M > 0 \text{ such that } \forall n \in \mathbb{Z}, n > M \implies \lvert x_{n} - 10 \rvert < \epsilon$$ $$\lim_{n \to \infty}(x_{n}) = 10$$ That's all cool and all, but how do I do this with limit theorems? I assume the solution is way easier than I'm making it.","It's been awhile since I took Calc 1 and I am reviewing it. I came across this problem. Let . For all integers , let . Assume that , and calculate . It reminds me of some sort of compound interest problem but I can't figure out how to match this pattern with the compound interest formula. The problem is supposed to be able to be solved with basic limit theorems that you learn about at the beginning of Calc 1. I imagine we have to find some way to rewrite the formula for in terms of rather than in terms of . Then we can take the limit like normal. The problem is that I don't know how to do that. Nevertheless, I can solve this problem with an overcomplicated and messy proof which probably contains a few oversights and logical leaps. Let . I'll wager that there exists some such that if then . First I will show that there exists some such that if then . We have found possible values for , which are and . Now I will show that, with at least one of the values of I found in the previous part, it is also true that if then . So to recap, we have that if then , and also that if then . We can combine these two statements together to say that if then . Given that and some initial , we can define . By our previous conclusion, for any , , or in other words, . We could also say that if . Intuitively, it seems that as tends to infinity, tends to . Or in other words, that . Now I need to prove it. Given any , I want to show that there exists an integer given by . For the case that , we can just set , easy. But for the case that , it's a bit more complicated. I will first assume that there exists no integer such that . In other words, for any integer , if , then . But then this implies that if we want to find such that if then , then we could find as a possible solution for . But this is clearly false, since earlier we found that there was only one solution for and it was , not some . With our assumption contradicted, the opposite of our assumption must be true. Therefore, there is some integer such that . Now with our value of , we want to show that if , then . Therefore, That's all cool and all, but how do I do this with limit theorems? I assume the solution is way easier than I'm making it.","x_{1} = 100 n x_{n+1} = \frac{1}{2}(x_{n} + \frac{100}{x_{n}}) \lim_{n \to \infty}(x_{n}) = L L x_{n+1} n x_{n} f(x) = \frac{1}{2}\left(x + \frac{100}{x}\right) c c < x c < f(x) < x c c < x f(x) < x c < x c^2 < x^2 \frac{c^2}{x} < x x + \frac{c^2}{x} < 2x \frac{1}{2}\left(x + \frac{c^2}{x}\right) < x \text{let } c = \pm 10 \frac{1}{2}\left(x + \frac{(\pm 10)^2}{x}\right) < x \frac{1}{2}\left(x + \frac{100}{x}\right) < x f(x) < x c 10 -10 c c < x c < f(x) \text{If } c = -10 \text{, then} -10 < x \text{suppose } x = -5 f(-5) = \frac{1}{2}\left(5 + \frac{100}{5}\right) = \frac{1}{2}\left(5 + 20\right) = \frac{1}{2}\left(25\right) = -12.5 -10 < -12.5 \text{ is false} \text{so } c \ne -10 \text{If } c = 10 \text{, then} 10 < x 10 (10x - 100) < x (10x - 100) 100x - 1000 < 10x^2 - 100x 100x + 100x < 10x^2 + 1000 x(100 + 100) < 10(x^2 + 100) \frac{100 + 100}{10} < \frac{x^2 + 100}{x} 10 + \frac{100}{10} < x + \frac{100}{x} \frac{1}{2}(10 + \frac{100}{10}) < \frac{1}{2}(x + \frac{100}{x}) f(10) < f(x) 10 < f(x) c < f(x) 10 < x f(x) < x 10 < x 10 < f(x) 10 < x 10 < f(x) < x n \in \mathbb{Z} x_{1} > 10 x_{n + 1} = f(x_{n}) x_{n} 10 < f(x_{n}) < x_{n} 10 < x_{n + 1} < x_{n} 10 < x_{a} < x_{b} a > b n x_{n} 10 \lim_{n \to \infty}(x_{n}) = 10 \epsilon > 0 M 10 < x_{M} < 10 + \epsilon x_{1} \le 10 + \epsilon M = 1 x_{1} > 10 + \epsilon M 10 < x_{M} < 10 + \epsilon n x_{n} > 10 + \epsilon 10 + \epsilon < x_{n + 1} < x_{n} c x > c c < f(x) < x 10 + \epsilon c c 10 10 + \epsilon M 10 < x_{M} < 10 + \epsilon M n > M \lvert x_{n} - 10 \rvert < \epsilon n > M 10 < x_{n} < x_{M} 0 < x_{n} - 10 < x_{M} - 10 -(x_{M} - 10) < 0 < x_{n} - 10 < x_{M} - 10 -(x_{M} - 10) < x_{n} - 10 < x_{M} - 10 -\epsilon < -(x_{M} - 10) < x_{n} - 10 < x_{M} - 10 < \epsilon -\epsilon < x_{n} - 10 < \epsilon \lvert x_{n} - 10 \rvert < \epsilon \forall \epsilon > 0, \exists M > 0 \text{ such that } \forall n \in \mathbb{Z}, n > M \implies \lvert x_{n} - 10 \rvert < \epsilon \lim_{n \to \infty}(x_{n}) = 10","['calculus', 'limits']"
89,"Question about Proof of the Integrability of $f$ and $f_1,f_2,\dots,$ in Lebesgue's Dominated Convergence Theorem",Question about Proof of the Integrability of  and  in Lebesgue's Dominated Convergence Theorem,"f f_1,f_2,\dots,","I am self-studying measure theory and got stuck on part of the proof of the Lebesgue's Dominated Convergence Theorem: Theorem $\quad$ 2.4.5 $\quad$ (Lebesgue's Dominated Convergence Theorem) Let $(X,\mathscr{A},\mu)$ be a measure space, let $g$ be a $[0,+\infty]$ -valued integrable function on $X$ , and let $f$ and $f_1,f_2,\dots$ be $[-\infty,+\infty]$ -valued $\mathscr{A}$ -measurable functions on $X$ such that \begin{align}     f(x) = \lim_{n\to\infty}f_n(x)\tag1 \end{align} and \begin{align}     |f_n(x)| \leq g(x),\ n=1,2,\dots\tag2 \end{align} hold at $\mu$ -almost every $x$ in $X$ . Then $f$ and $f_1,f_2,\dots$ are integrable, and $\int fd\mu = \lim_{n\to\infty}\int f_nd\mu$ . When proving the theorem, the book claims that ""the integrability of $f$ and $f_1,f_2,\dots$ follows from that of $g$ ; see Proposition 2.3.8, Proposition 2.3.9, and part (c) of Proposition 2.3.4."" (I added these results at the bottom of this post.) I could not see how the integrability of $f$ and $f_1,f_2,\dots$ follow immediately from that of $g$ by applying these results. The most important reason is that the relationship (1) and (2) does not hold at every $x$ in $X$ , but instead, hold at $\mu$ -almost every $x$ in $X$ , which makes it illegal to directly apply Proposition 2.3.4 (c). So, I tried to prove it myself. Here are my questions for this post: 1. Is my proof of the integrability of $\mathbf{f}$ and $\mathbf{f_1,f_2,\dots}$ correct? (For example, I have doubts about the relation $\left\{x\in X:\lim_{n\to\infty}|f_n(x)| > g(x)\right\} \subseteq \bigcup_{n=1}^{\infty}\left\{x\in X:|f_n(x)|>g(x)\right\}$ I used in my proof.) 2. Am I overcomplicating anything here? (I am worried about this because the book's claim sounds like the proof shouldn't be that complicated.) 3. From my attempt below and the rest of proof of $\mathbf{\int fd\mu = \lim_{n\to\infty}\int f_nd\mu}$ presented in the book, it seems that we are not allow to conclude that $\mathbf{\int\lim_{n\to\infty}f_nd\mu = \lim_{n\to\infty}\int f_nd\mu}$ . But why? Here is my attempt to prove the integrability of $f$ and $f_1,f_2,\dots$ : Proof of the integrability of $\mathbf{f}$ and $\mathbf{f_1,f_2,\dots}$ $\quad$ We first prove the integrability of $\mathbf{f_1,f_2,\dots}$ . Let $n$ be a positive integer. The fact that $|f_n(x)|\leq g(x)$ holds almost everywhere implies that there is an $N\in\mathscr{A}$ such that $\mu(N)=0$ and $\{x\in X:|f_n(x)|>g(x)\}\subseteq N$ . Since $|f_n|$ and $g$ are $[0,+\infty]$ -valued $\mathscr{A}$ -measurable functions, the set $\{x\in X:|f_n(x)|>g(x)\} \in \mathscr{A}$ by Proposition 2.1.3. So, we can let $N = \{x\in X:|f_n(x)|>g(x)\}$ and conclude that $\mu(\{x\in X:|f_n(x)|>g(x)\})=0$ . Since $g$ is integrable and $[0,+\infty]$ -valued, it follows that $\int g^+d\mu = \int gd\mu < +\infty$ . Assume to the contrary that $\int|f_n|d\mu = \sup\left\{\int hd\mu:h \in \mathscr{S}_+\ \text{and}\ h\leq|f_n|\right\} = +\infty$ . Then, as $\int gd\mu<+\infty$ , there exists an $h\in\mathscr{S}_+$ such that $h\leq|f_n|$ and $\int hd\mu > \int gd\mu$ . Denote this $h$ by $h=\sum_{i=1}^ma_i\chi_{A_i}$ , where $a_1,\dots,a_m$ are nonnegative real numbers and $A_1,\dots,A_m$ are disjoint subsets of $X$ that belong to $\mathscr{A}$ . Then \begin{align} \int|f_n|d\mu \geq \int hd\mu = \sum_{i=1}^ma_i\mu(A_i) > \int gd\mu = \sup\left\{\int pd\mu:p\in\mathscr{S}_+\ \text{and}\ p\leq g\right\}. \end{align} The set $\{x\in X:h(x)>g(x)\}\neq\emptyset$ , for otherwise $h(x)\leq g(x)$ for all $x$ in $X$ would imply $\int hd\mu \leq \int gd\mu$ (see Proposition 2.3.4 (c)). Without loss of generality, suppose $\{x\in X:h(x) > g(x)\} = \bigcup_{i=1}^kA_i$ and $\{x\in X:h(x) \leq g(x)\} = \bigcup_{i=k+1}^mA_i$ . So for any $x\in A_i$ , $i=k+1,\dots,m$ , we have $a_i\leq g(x)$ . Define a function $p\in\mathscr{S}_+$ be such that $p(x) = a_i$ for all $x\in A_i$ where $i=k+1,\dots,m$ , and $p(x)=0$ for all $x\in A_i$ where $i=1,\dots,k$ . Then $p\leq g$ and $\int pd\mu = \sum_{i=1}^ma_i\mu(A_i)$ . Now, if $\mu(\{x\in X:h(x)>g(x)\}) = \mu(\bigcup_{i=1}^kA_i) = \bigcup_{i=1}^k\mu(A_i) = 0$ , then $\mu(A_i)=0$ for $i=1,\dots,k$ , so that $\int hd\mu = \sum_{i=1}^ma_i\mu(A_i) = \int pd\mu \leq \int gd\mu$ , contradicting the fact that $\int hd\mu > \int gd\mu$ . Thus, $\mu(\{x\in X:h(x)>g(x)\}) > 0$ . But $\{x\in X:h(x)>g(x)\}\subseteq\{x\in X:|f_n(x)|>g(x)\}$ . So we get $\mu(\{x\in X:|f_n(x)|>g(x)\})>\mu(\{x\in X:h(x)>g(x)\})>0$ , contradicting the fact that $\mu(\{x\in X:|f_n(x)|>g(x)\})=0$ . Therefore, \begin{align} \int|f_n|d\mu = \sup\left\{\int hd\mu:h\in\mathscr{S}_+\ \text{and}\ h\leq|f_n|\right\} < +\infty. \end{align} Hence, $|f_n|$ is integrable. By Proposition 2.3.8, $f_n$ is integrable. Next, we prove the integrability of $\mathbf{f}$ . By Proposition 2.1.5, $\lim_{n\to\infty}|f_n|$ and $g$ are $[0,+\infty]$ -valued $\mathscr{A}$ -measurable functions, it follows that $\{x\in X:\lim_{n\to\infty}|f_n(x)|>g(x)\}\in\mathscr{A}$ . Since $\{x\in X:\lim_{n\to\infty}|f_n(x)|>g(x)\}\subseteq\bigcup_{n=1}^{\infty}\{x\in X:|f_n(x)|>g(x)\}$ and $\mu(\{x\in X:|f_n(x)|>g(x)\})=0$ for all $n\in\mathbb{N}$ , it follows that $\mu(\{x\in X:\lim_{n\to\infty}|f_n(x)|>g(x)\})=0$ . Thus, $\lim_{n\to\infty}|f_n|\leq g$ holds $\mu$ -almost everywhere. Then, by a similar argument, we can conclude that \begin{align} \int\lim_{n\to\infty}|f_n|d\mu = \sup\left\{\int d\mu:h\in\mathscr{S}_+\ \text{and}\ h \leq \lim_{n\to\infty}|f_n|\right\}<+\infty. \end{align} Hence, $\lim_{n\to\infty}|f_n| = |\lim_{n\to\infty}f_n|$ is integrable. By Proposition 2.3.8, $\lim_{n\to\infty}f_n$ is integrable. Since $f(x) = \lim_{n\to\infty}f_n(x)$ holds at $\mu$ -almost every $x$ in $X$ , by Proposition 2.3.9, $\int fd\mu$ exists and $\int fd\mu = \int\lim_{n\to\infty}f_n(x)d\mu$ which is finite given the integrability of $\lim_{n\to\infty}f_n(x)$ . This implies $f$ is integrable. Thank you very much for any help! Results used in my attempt: Proposition 2.1.3 $\quad$ Let $(X,\mathscr{A})$ be a measurable space, let $A$ be a subset of $X$ that belongs to $\mathscr{A}$ , and let $f$ and $g$ be $[-\infty,+\infty]$ -valued measurable functions on $A$ . Then the sets $\{x \in A:f(x) < g(x)\}$ , $\{x \in A:f(x) \leq g(x)\}$ , and $\{x \in A:f(x) = g(x)\}$ belong to $\mathscr{A}$ . Proposition 2.1.5 $\quad$ Let $(X,\mathscr{A})$ be a measurable space, let $A$ be a subset of $X$ that belongs to $\mathscr{A}$ , and let $\{f_n\}$ be a sequence of $[-\infty,+\infty]$ -valued measurable functions on $A$ . Then (a) the functions $\sup_nf_n$ and $\inf_nf_n$ are measurable, (b) the functions $\limsup_{n\to\infty}f_n$ and $\liminf_{n\to\infty}f_n$ are measurable, and (c) the function $\lim_{n\to\infty}f_n$ (whose domain is $\{x\in A:\limsup_{n\to\infty}f_n(x)=\liminf_{n\to\infty}f_n(x)\}$ ) is measurable. Proposition 2.3.4 $\quad$ Let $(X,\mathscr{A},\mu)$ be a measure space, let $f$ and $g$ be $[0,+\infty]$ -valued $\mathscr{A}$ -measurable functions on $X$ m and let $\alpha$ be a nonnegative real number. Then (a) $\int \alpha fd\mu = \alpha\int fd\mu$ , (b) $\int(f+g)d\mu = \int fd\mu+\int gd\mu$ , and (c) if $f(x)\leq g(x)$ holds at each $x$ in $X$ , then $\int fd\mu \leq \int gd\mu$ . Proposition 2.3.8 $\quad$ Let $(X,\mathscr{A},\mu)$ be a measure space, and let $f$ be a $[-\infty,+\infty]$ -valued $\mathscr{A}$ -measurable function on $X$ . Then $f$ is integrable if and only if $|f|$ is integrable. If these functions are integrable, then $|\int fd\mu|\leq\int|f|d\mu$ . Proposition 2.3.9 $\quad$ Let $(X,\mathscr{A},\mu)$ be a measure space, and let $f$ and $g$ be $[-\infty,+\infty]$ -valued $\mathscr{A}$ -measurable functions on $X$ that agree almost everywhere. If either $\int fd\mu$ or $\int gd\mu$ exists, then both exist, and $\int fd\mu = \int gd\mu$ . Reference $\quad$ Measure Theory by Donald Cohn.","I am self-studying measure theory and got stuck on part of the proof of the Lebesgue's Dominated Convergence Theorem: Theorem 2.4.5 (Lebesgue's Dominated Convergence Theorem) Let be a measure space, let be a -valued integrable function on , and let and be -valued -measurable functions on such that and hold at -almost every in . Then and are integrable, and . When proving the theorem, the book claims that ""the integrability of and follows from that of ; see Proposition 2.3.8, Proposition 2.3.9, and part (c) of Proposition 2.3.4."" (I added these results at the bottom of this post.) I could not see how the integrability of and follow immediately from that of by applying these results. The most important reason is that the relationship (1) and (2) does not hold at every in , but instead, hold at -almost every in , which makes it illegal to directly apply Proposition 2.3.4 (c). So, I tried to prove it myself. Here are my questions for this post: 1. Is my proof of the integrability of and correct? (For example, I have doubts about the relation I used in my proof.) 2. Am I overcomplicating anything here? (I am worried about this because the book's claim sounds like the proof shouldn't be that complicated.) 3. From my attempt below and the rest of proof of presented in the book, it seems that we are not allow to conclude that . But why? Here is my attempt to prove the integrability of and : Proof of the integrability of and We first prove the integrability of . Let be a positive integer. The fact that holds almost everywhere implies that there is an such that and . Since and are -valued -measurable functions, the set by Proposition 2.1.3. So, we can let and conclude that . Since is integrable and -valued, it follows that . Assume to the contrary that . Then, as , there exists an such that and . Denote this by , where are nonnegative real numbers and are disjoint subsets of that belong to . Then The set , for otherwise for all in would imply (see Proposition 2.3.4 (c)). Without loss of generality, suppose and . So for any , , we have . Define a function be such that for all where , and for all where . Then and . Now, if , then for , so that , contradicting the fact that . Thus, . But . So we get , contradicting the fact that . Therefore, Hence, is integrable. By Proposition 2.3.8, is integrable. Next, we prove the integrability of . By Proposition 2.1.5, and are -valued -measurable functions, it follows that . Since and for all , it follows that . Thus, holds -almost everywhere. Then, by a similar argument, we can conclude that Hence, is integrable. By Proposition 2.3.8, is integrable. Since holds at -almost every in , by Proposition 2.3.9, exists and which is finite given the integrability of . This implies is integrable. Thank you very much for any help! Results used in my attempt: Proposition 2.1.3 Let be a measurable space, let be a subset of that belongs to , and let and be -valued measurable functions on . Then the sets , , and belong to . Proposition 2.1.5 Let be a measurable space, let be a subset of that belongs to , and let be a sequence of -valued measurable functions on . Then (a) the functions and are measurable, (b) the functions and are measurable, and (c) the function (whose domain is ) is measurable. Proposition 2.3.4 Let be a measure space, let and be -valued -measurable functions on m and let be a nonnegative real number. Then (a) , (b) , and (c) if holds at each in , then . Proposition 2.3.8 Let be a measure space, and let be a -valued -measurable function on . Then is integrable if and only if is integrable. If these functions are integrable, then . Proposition 2.3.9 Let be a measure space, and let and be -valued -measurable functions on that agree almost everywhere. If either or exists, then both exist, and . Reference Measure Theory by Donald Cohn.","\quad \quad (X,\mathscr{A},\mu) g [0,+\infty] X f f_1,f_2,\dots [-\infty,+\infty] \mathscr{A} X \begin{align}
    f(x) = \lim_{n\to\infty}f_n(x)\tag1
\end{align} \begin{align}
    |f_n(x)| \leq g(x),\ n=1,2,\dots\tag2
\end{align} \mu x X f f_1,f_2,\dots \int fd\mu = \lim_{n\to\infty}\int f_nd\mu f f_1,f_2,\dots g f f_1,f_2,\dots g x X \mu x X \mathbf{f} \mathbf{f_1,f_2,\dots} \left\{x\in X:\lim_{n\to\infty}|f_n(x)| > g(x)\right\} \subseteq \bigcup_{n=1}^{\infty}\left\{x\in X:|f_n(x)|>g(x)\right\} \mathbf{\int fd\mu = \lim_{n\to\infty}\int f_nd\mu} \mathbf{\int\lim_{n\to\infty}f_nd\mu = \lim_{n\to\infty}\int f_nd\mu} f f_1,f_2,\dots \mathbf{f} \mathbf{f_1,f_2,\dots} \quad \mathbf{f_1,f_2,\dots} n |f_n(x)|\leq g(x) N\in\mathscr{A} \mu(N)=0 \{x\in X:|f_n(x)|>g(x)\}\subseteq N |f_n| g [0,+\infty] \mathscr{A} \{x\in X:|f_n(x)|>g(x)\} \in \mathscr{A} N = \{x\in X:|f_n(x)|>g(x)\} \mu(\{x\in X:|f_n(x)|>g(x)\})=0 g [0,+\infty] \int g^+d\mu = \int gd\mu < +\infty \int|f_n|d\mu = \sup\left\{\int hd\mu:h \in \mathscr{S}_+\ \text{and}\ h\leq|f_n|\right\} = +\infty \int gd\mu<+\infty h\in\mathscr{S}_+ h\leq|f_n| \int hd\mu > \int gd\mu h h=\sum_{i=1}^ma_i\chi_{A_i} a_1,\dots,a_m A_1,\dots,A_m X \mathscr{A} \begin{align}
\int|f_n|d\mu \geq \int hd\mu = \sum_{i=1}^ma_i\mu(A_i) > \int gd\mu = \sup\left\{\int pd\mu:p\in\mathscr{S}_+\ \text{and}\ p\leq g\right\}.
\end{align} \{x\in X:h(x)>g(x)\}\neq\emptyset h(x)\leq g(x) x X \int hd\mu \leq \int gd\mu \{x\in X:h(x) > g(x)\} = \bigcup_{i=1}^kA_i \{x\in X:h(x) \leq g(x)\} = \bigcup_{i=k+1}^mA_i x\in A_i i=k+1,\dots,m a_i\leq g(x) p\in\mathscr{S}_+ p(x) = a_i x\in A_i i=k+1,\dots,m p(x)=0 x\in A_i i=1,\dots,k p\leq g \int pd\mu = \sum_{i=1}^ma_i\mu(A_i) \mu(\{x\in X:h(x)>g(x)\}) = \mu(\bigcup_{i=1}^kA_i) = \bigcup_{i=1}^k\mu(A_i) = 0 \mu(A_i)=0 i=1,\dots,k \int hd\mu = \sum_{i=1}^ma_i\mu(A_i) = \int pd\mu \leq \int gd\mu \int hd\mu > \int gd\mu \mu(\{x\in X:h(x)>g(x)\}) > 0 \{x\in X:h(x)>g(x)\}\subseteq\{x\in X:|f_n(x)|>g(x)\} \mu(\{x\in X:|f_n(x)|>g(x)\})>\mu(\{x\in X:h(x)>g(x)\})>0 \mu(\{x\in X:|f_n(x)|>g(x)\})=0 \begin{align}
\int|f_n|d\mu = \sup\left\{\int hd\mu:h\in\mathscr{S}_+\ \text{and}\ h\leq|f_n|\right\} < +\infty.
\end{align} |f_n| f_n \mathbf{f} \lim_{n\to\infty}|f_n| g [0,+\infty] \mathscr{A} \{x\in X:\lim_{n\to\infty}|f_n(x)|>g(x)\}\in\mathscr{A} \{x\in X:\lim_{n\to\infty}|f_n(x)|>g(x)\}\subseteq\bigcup_{n=1}^{\infty}\{x\in X:|f_n(x)|>g(x)\} \mu(\{x\in X:|f_n(x)|>g(x)\})=0 n\in\mathbb{N} \mu(\{x\in X:\lim_{n\to\infty}|f_n(x)|>g(x)\})=0 \lim_{n\to\infty}|f_n|\leq g \mu \begin{align}
\int\lim_{n\to\infty}|f_n|d\mu = \sup\left\{\int d\mu:h\in\mathscr{S}_+\ \text{and}\ h \leq \lim_{n\to\infty}|f_n|\right\}<+\infty.
\end{align} \lim_{n\to\infty}|f_n| = |\lim_{n\to\infty}f_n| \lim_{n\to\infty}f_n f(x) = \lim_{n\to\infty}f_n(x) \mu x X \int fd\mu \int fd\mu = \int\lim_{n\to\infty}f_n(x)d\mu \lim_{n\to\infty}f_n(x) f \quad (X,\mathscr{A}) A X \mathscr{A} f g [-\infty,+\infty] A \{x \in A:f(x) < g(x)\} \{x \in A:f(x) \leq g(x)\} \{x \in A:f(x) = g(x)\} \mathscr{A} \quad (X,\mathscr{A}) A X \mathscr{A} \{f_n\} [-\infty,+\infty] A \sup_nf_n \inf_nf_n \limsup_{n\to\infty}f_n \liminf_{n\to\infty}f_n \lim_{n\to\infty}f_n \{x\in A:\limsup_{n\to\infty}f_n(x)=\liminf_{n\to\infty}f_n(x)\} \quad (X,\mathscr{A},\mu) f g [0,+\infty] \mathscr{A} X \alpha \int \alpha fd\mu = \alpha\int fd\mu \int(f+g)d\mu = \int fd\mu+\int gd\mu f(x)\leq g(x) x X \int fd\mu \leq \int gd\mu \quad (X,\mathscr{A},\mu) f [-\infty,+\infty] \mathscr{A} X f |f| |\int fd\mu|\leq\int|f|d\mu \quad (X,\mathscr{A},\mu) f g [-\infty,+\infty] \mathscr{A} X \int fd\mu \int gd\mu \int fd\mu = \int gd\mu \quad","['real-analysis', 'integration', 'limits', 'analysis', 'measure-theory']"
90,Does such a thing as $\lim_\limits{g(x)\to a} f(x)$ exists?,Does such a thing as  exists?,\lim_\limits{g(x)\to a} f(x),"Does such a thing as $\lim_\limits{g(x)\to a} f(x)$ exists? I thought about this out of thin air while messing with limits on W|A, of course it didn't output anything when I tried, but I thought it was an interesting concept, at first I thought about $\lim_\limits{x^2\to 4}x$ , and reached the the conclusion that $\lim_\limits{x^2\to 4}x = 2$ , since $x = 2 \rightarrow x^2 = 4$ , but I don't think my logic was correct there because what I did was basically $\lim_\limits{x\to2}x^2=4$ . Then I decided to ask some of my professors if $\lim_\limits{x^2\to a} f(x)$ is something that exists, two of them said it might be $\lim_\limits{x\to a} \sqrt{f(x)}$ , or it was $\lim_\limits{x\to a} f(\sqrt{x})$ , now I'm not 100% sure what they said but no one was sure about this.","Does such a thing as exists? I thought about this out of thin air while messing with limits on W|A, of course it didn't output anything when I tried, but I thought it was an interesting concept, at first I thought about , and reached the the conclusion that , since , but I don't think my logic was correct there because what I did was basically . Then I decided to ask some of my professors if is something that exists, two of them said it might be , or it was , now I'm not 100% sure what they said but no one was sure about this.",\lim_\limits{g(x)\to a} f(x) \lim_\limits{x^2\to 4}x \lim_\limits{x^2\to 4}x = 2 x = 2 \rightarrow x^2 = 4 \lim_\limits{x\to2}x^2=4 \lim_\limits{x^2\to a} f(x) \lim_\limits{x\to a} \sqrt{f(x)} \lim_\limits{x\to a} f(\sqrt{x}),"['calculus', 'limits']"
91,What are the conditions to compose limits to infinity?,What are the conditions to compose limits to infinity?,,"I've recently become a little obsessed with all the ways limits can be composed - and the preconditions for this to take place. I took $A,B_{1},B_{2},C \subseteq \mathbb{R}$ and $f:A\to B_{1},\ g:B_{2} \to C$ and further assumed $f[A] \subseteq B_{2}$ . And then I listed out all the ways to compose $f$ and $g$ with limits: \begin{align} 1.&&\lim_{x\to a}f(x)=b,\lim_{x\to b}g(x)=c&&\implies&&\lim_{x\to a}g(f(x))=c \\\\ 2.&&\lim_{x\to a}f(x)=b,\lim_{x\to b}g(x)=\infty&&\implies&&\lim_{x\to a}g(f(x))=\infty \\\\ 3.&&\lim_{x\to a}f(x)=\infty,\lim_{x\to\infty}g(x)=c&&\implies&&\lim_{x\to a}g(f(x))=c \\\\ 4.&&\lim_{x\to a}f(x)=\infty,\lim_{x\to\infty}g(x)=\infty&&\implies&&\lim_{x\to a}g(f(x))=\infty \\\\ 5.&&\lim_{x\to\infty}f(x)=b,\lim_{x\to b}g(x)=c&&\implies&&\lim_{x\to\infty}g(f(x))=c \\\\ 6.&&\lim_{x\to\infty}f(x)=b,\lim_{x\to b}g(x)=\infty&&\implies&&\lim_{x\to\infty}g(f(x))=\infty \\\\ 7.&&\lim_{x\to\infty}f(x)=\infty,\lim_{x\to\infty}g(x)=c&&\implies&&\lim_{x\to\infty}g(f(x))=c \\\\ 8.&&\lim_{x\to\infty}f(x)=\infty,\lim_{x\to\infty}g(x)=\infty&&\implies&&\lim_{x\to\infty}g(f(x))=\infty \end{align} and I got the proofs (and preconditions) for $1$ through $5$ . For example, for $1.$ we need either : $A\ $ : $g(b)=c$ , or $B\ $ : $\exists \delta>0,\forall x \in A: \ \ 0<|x-a|<\delta \implies |f(x)-b|>0$ For $2.$ we need $B$ For $3.$ no additional requirements are needed For $4.$ no additional requirements are needed For $5.$ we need $A$ But now I am stuck on $6$ . So far I have the following: I expand the definitions of what I have, and am trying to prove. \begin{align} 1: &&& \lim_{ x \to \infty } f(x) = b &&\iff&& \forall \varepsilon>0, \exists c \in \mathbb{R}, \forall x \in A: \ \ x>c \implies |f(x)-b| < \varepsilon \\\\ 2: &&& \lim_{ x \to b } g(x) = \infty &&\iff&& \forall r \in \mathbb{R}, \exists\delta>0,\forall x \in B_{2}: \ \ 0<|x-b| < \delta \implies g(x) > r \\\\ \text{Aim for:} &&& \lim_{ x \to \infty } g(f(x)) = \infty &&\iff&& \forall r \in \mathbb{R}, \exists c \in \mathbb{R},\forall x \in A: \ \ x>c \implies g(f(x)) > r \end{align} Then take arbitrary $R \in \mathbb{R}$ . Therefore, $\exists\delta>0,\forall x \in B_{2}: \ \ 0<|x-b| < \delta \implies g(x) > R$ . Take particular $D>0$ for which that is true. Therefore, $\forall x \in B_{2}: \ \ 0<|x-b| < D \implies g(x) > R$ . Since $D>0$ , we also have $\exists c \in \mathbb{R}, \forall x \in A: \ \ x>c \implies |f(x)-b| < D$ . Take particular $C \in \mathbb{R}$ for which this is true. Therefore, $\forall x \in A: \ \ x>C \implies |f(x)-b| < D$ . Take arbitrary $X \in A$ . Therefore, $X>C \implies |f(X)-b| < D$ . Since $f(X) \in B_{2}$ , we get $0<|f(X)-b| < D \implies g(f(X)) > R$ . Assume $X>C$ , therefore $|f(X)-b| < D$ . Assume precondition $b \not\in f[A]$ . Therefore $|f(X)-b| > 0$ . Therefore $0<|f(X)-b| < D$ , which implies $g(f(X)) > R$ . Hence we have the implication $X>C \implies g(f(X)) > R$ . $X$ is arbitrary, $C$ is particular, $R$ is arbitrary, therefore we have $\forall r \in \mathbb{R}, \exists c \in \mathbb{R},\forall x \in A: \ \ x>c \implies g(f(x)) > r$ . That proof works if I assume $b \not\in f[A]$ , which seems to make sense as a precondition. If I take an example of $f(x)=\frac{1}{x}$ and $g(x)=-\ln(x)$ , then indeed $0 \notin f[A]$ , and it looks like $g(f(x)) \to \infty$ as $x\to \infty$ : But then that got me thinking, is there any preconditions weaker than this? Surely if I can find examples that fail this condition, then it its stronger than it needs to be. So what if $f(x)=\frac{1}{x}\sin(x)$ ? Then not only $f(x)\to 0$ as $x\to \infty$ , but also $0 \in f[A]$ . And then what if $g(x)=\frac{1}{x^2}$ ? Then $g(x)\to \infty$ as $x\to 0$ . And it looks like $g(f(x)) \to \infty$ as $x\to \infty$ : But if I think about it, $g(0)$ is undefined, so $0 \not\in B_{2}$ . If we still maintain that $0 \in f[A]$ , then $f[A] \not\subseteq B_{2}$ . The only way to maintain that $f[A] \subseteq B_{2}$ is to say $0 \not\in f[A]$ , so the precondition I came up with seems to apply to this example too. Is $b \not\in f[A]$ the minimal precondition for composition $6.$ to take place? EDIT: Thanks to Karl for pointing out that a precondition weaker than $f(x) \neq c$ is: $B\ '\ $ : $\exists c \in\mathbb{R}, \forall x \in A: x>c \implies |f(x)-c|>0$ And thanks to David K for showing me that $B\ '$ is also a sufficient condition for $5.$ :)","I've recently become a little obsessed with all the ways limits can be composed - and the preconditions for this to take place. I took and and further assumed . And then I listed out all the ways to compose and with limits: and I got the proofs (and preconditions) for through . For example, for we need either : : , or : For we need For no additional requirements are needed For no additional requirements are needed For we need But now I am stuck on . So far I have the following: I expand the definitions of what I have, and am trying to prove. Then take arbitrary . Therefore, . Take particular for which that is true. Therefore, . Since , we also have . Take particular for which this is true. Therefore, . Take arbitrary . Therefore, . Since , we get . Assume , therefore . Assume precondition . Therefore . Therefore , which implies . Hence we have the implication . is arbitrary, is particular, is arbitrary, therefore we have . That proof works if I assume , which seems to make sense as a precondition. If I take an example of and , then indeed , and it looks like as : But then that got me thinking, is there any preconditions weaker than this? Surely if I can find examples that fail this condition, then it its stronger than it needs to be. So what if ? Then not only as , but also . And then what if ? Then as . And it looks like as : But if I think about it, is undefined, so . If we still maintain that , then . The only way to maintain that is to say , so the precondition I came up with seems to apply to this example too. Is the minimal precondition for composition to take place? EDIT: Thanks to Karl for pointing out that a precondition weaker than is: : And thanks to David K for showing me that is also a sufficient condition for :)","A,B_{1},B_{2},C \subseteq \mathbb{R} f:A\to B_{1},\ g:B_{2} \to C f[A] \subseteq B_{2} f g \begin{align}
1.&&\lim_{x\to a}f(x)=b,\lim_{x\to b}g(x)=c&&\implies&&\lim_{x\to a}g(f(x))=c \\\\ 2.&&\lim_{x\to a}f(x)=b,\lim_{x\to b}g(x)=\infty&&\implies&&\lim_{x\to a}g(f(x))=\infty \\\\ 3.&&\lim_{x\to a}f(x)=\infty,\lim_{x\to\infty}g(x)=c&&\implies&&\lim_{x\to a}g(f(x))=c \\\\ 4.&&\lim_{x\to a}f(x)=\infty,\lim_{x\to\infty}g(x)=\infty&&\implies&&\lim_{x\to a}g(f(x))=\infty \\\\ 5.&&\lim_{x\to\infty}f(x)=b,\lim_{x\to b}g(x)=c&&\implies&&\lim_{x\to\infty}g(f(x))=c \\\\ 6.&&\lim_{x\to\infty}f(x)=b,\lim_{x\to b}g(x)=\infty&&\implies&&\lim_{x\to\infty}g(f(x))=\infty \\\\ 7.&&\lim_{x\to\infty}f(x)=\infty,\lim_{x\to\infty}g(x)=c&&\implies&&\lim_{x\to\infty}g(f(x))=c \\\\ 8.&&\lim_{x\to\infty}f(x)=\infty,\lim_{x\to\infty}g(x)=\infty&&\implies&&\lim_{x\to\infty}g(f(x))=\infty
\end{align} 1 5 1. A\  g(b)=c B\  \exists \delta>0,\forall x \in A: \ \ 0<|x-a|<\delta \implies |f(x)-b|>0 2. B 3. 4. 5. A 6 \begin{align}
1: &&& \lim_{ x \to \infty } f(x) = b &&\iff&& \forall \varepsilon>0, \exists c \in \mathbb{R}, \forall x \in A: \ \ x>c \implies |f(x)-b| < \varepsilon \\\\
2: &&& \lim_{ x \to b } g(x) = \infty &&\iff&& \forall r \in \mathbb{R}, \exists\delta>0,\forall x \in B_{2}: \ \ 0<|x-b| < \delta \implies g(x) > r \\\\
\text{Aim for:} &&& \lim_{ x \to \infty } g(f(x)) = \infty &&\iff&& \forall r \in \mathbb{R}, \exists c \in \mathbb{R},\forall x \in A: \ \ x>c \implies g(f(x)) > r
\end{align} R \in \mathbb{R} \exists\delta>0,\forall x \in B_{2}: \ \ 0<|x-b| < \delta \implies g(x) > R D>0 \forall x \in B_{2}: \ \ 0<|x-b| < D \implies g(x) > R D>0 \exists c \in \mathbb{R}, \forall x \in A: \ \ x>c \implies |f(x)-b| < D C \in \mathbb{R} \forall x \in A: \ \ x>C \implies |f(x)-b| < D X \in A X>C \implies |f(X)-b| < D f(X) \in B_{2} 0<|f(X)-b| < D \implies g(f(X)) > R X>C |f(X)-b| < D b \not\in f[A] |f(X)-b| > 0 0<|f(X)-b| < D g(f(X)) > R X>C \implies g(f(X)) > R X C R \forall r \in \mathbb{R}, \exists c \in \mathbb{R},\forall x \in A: \ \ x>c \implies g(f(x)) > r b \not\in f[A] f(x)=\frac{1}{x} g(x)=-\ln(x) 0 \notin f[A] g(f(x)) \to \infty x\to \infty f(x)=\frac{1}{x}\sin(x) f(x)\to 0 x\to \infty 0 \in f[A] g(x)=\frac{1}{x^2} g(x)\to \infty x\to 0 g(f(x)) \to \infty x\to \infty g(0) 0 \not\in B_{2} 0 \in f[A] f[A] \not\subseteq B_{2} f[A] \subseteq B_{2} 0 \not\in f[A] b \not\in f[A] 6. f(x) \neq c B\ '\  \exists c \in\mathbb{R}, \forall x \in A: x>c \implies |f(x)-c|>0 B\ ' 5.","['real-analysis', 'limits', 'proof-writing']"
92,"Example of $a_k$ such that $\sum a_k$ doesn't converge but $\lim_{x \rightarrow 1, x< 1} \sum a_k x^k$ exists.",Example of  such that  doesn't converge but  exists.,"a_k \sum a_k \lim_{x \rightarrow 1, x< 1} \sum a_k x^k","In class we were taught that if $\sum a_k$ converges, then $\lim_{x \rightarrow 1, x < 1} \sum a_k x^k$ exists. The proof is based on the Cauchy's convergence criterion: if the sum converges, then $\limsup_{n \rightarrow \infty} \sqrt[n]{a_n} \leq 1$ . Is there an example of $a_k$ such that the limit exists but the sum doesn't converge, i.e. is there a counter example to the opposite statement?","In class we were taught that if converges, then exists. The proof is based on the Cauchy's convergence criterion: if the sum converges, then . Is there an example of such that the limit exists but the sum doesn't converge, i.e. is there a counter example to the opposite statement?","\sum a_k \lim_{x \rightarrow 1, x < 1} \sum a_k x^k \limsup_{n \rightarrow \infty} \sqrt[n]{a_n} \leq 1 a_k","['real-analysis', 'sequences-and-series', 'limits', 'power-series']"
93,$f(x) + f(x/2) + f(x/3) + f(x/4) + ... = x$ and $\lim_{n \to \infty} \frac{f(n)}{\pi(n)} = 1$?,and ?,f(x) + f(x/2) + f(x/3) + f(x/4) + ... = x \lim_{n \to \infty} \frac{f(n)}{\pi(n)} = 1,"$$f(x) + f(x/2) + f(x/3) + f(x/4) + ... = x$$ $$f(n) < \pi(n+1)$$ $$\lim_{n \to \infty} \frac{f(n)}{\pi(n)} = 1$$ where $\pi(n)$ is the prime counting function. My mentor more or less wrote that when he was around $12$ yo. If you use the Riemann zeta function $\zeta(s)$ to prove the PNT and you know the easy $f(x) < \frac{x}{\ln(x)-1}$ (easy to show for large $x$ at least) then with some little extra work you can prove the two statements at least for large $x$ or large $n$ . But how about proving it directly ? What if we did not know that $\pi(n)$ is about $= \frac{x}{\ln(x)-1}$ ? What if we do not use the zeta function nor complex dynamics ? Is this related to one of those elementary proofs of the prime number theorem ? How does one even come up with the idea of the equation above ?? What is the logic behind it ? Do not confuse with the similar looking $$Li(x) + Li(x^{1/2}) + Li(x^{1/3}) + ...$$ you often see in number theory, this is not the one. And it is also not this one $$g(x) + g(x)/2 + g(x)/3 + g(x)/4 + ... = x$$ Because that one gives about $g(n) = \frac{n}{H_n}$ where H_n is the n th harmonic number, what is a weak version of the PNT. It also does not follow from the 3 theorems of Mertens nor does it resemble anything I ever read. I am not looking for sharp asymptotics for $f(x)$ here, that is not the question here. (although I will probably start a new topic about that soon) Any ideas ? Btw this function is even more mysterious when you think about it : For example $$f(x) + f(x/2) + f(x/3) + f(x/4) + ... = x$$ substitute $x$ to $x/2$ $$f(x/2) + f(x/4) + f(x/6) + f(x/8) + f(x/10) + ... = x/2$$ This implies that $$f(x) + f(x/3) + f(x/5) + f(x/7) + f(x/9) + ... = x/2$$ and thus $$f(x/2) + f(x/4) + f(x/6) + f(x/8) + f(x/10) + ... = f(x) + f(x/3) + f(x/5) + f(x/7) + f(x/9) + ... $$ and $$f(f(x) + f(x/3) + f(x/5) + f(x/7) + f(x/9) + ...) = f(f(x/2) + f(x/4) + f(x/6) + f(x/8) + f(x/10) + ...) = f(x/2)$$ and similar identities can be constructed. Imo very much like series multisection, fractals and self-referential things. But getting series expansions for it that are defined everywhere is hard. Does $f$ need to be continu ? Maybe we should relax the equation and write $$f(n) + f(n/2) + f(n/3) + f(n/4) + ...= n$$ and the divisions get rounded or so. Or maybe we should truncate the ellipsis (...) somehow. But that still does not answer my questions. edit My mentor told me this $$\pi(x) + \pi(x/2) + \pi(x/3) + ... > x$$ if we take enough terms and if the following is true $$\pi(x/a) > \frac{\pi(x)}{a}$$ (conjecture A) It then follows that $$f(x) + f(x/2) + f(x/3) + ... = x$$ implies that $f(x) < \pi(x)$ Conjecture A might have a name, it is a weaker version of the second hardy-littlewood conjecture : $$\pi(x+y) - \pi(x) < \pi(y) + 1$$ Conjecture A makes sense for large $x$ and small $a$ for sure if we use a sharp version of the prime number theorem somewhat like the good asymptotic $$\frac{x}{\ln(x) - 1 + \frac{1}{\ln(x)}}$$ But I am unsure if conjecture A has been proven. Again this might be overkill but it is another way of thinking about it. see : $f(x) + f(x/2) + f(x/3) + ... = x$ and conjecture A : $\pi(x/a) > \frac{\pi(x)}{a}$ edit A similar looking idea seems to have occured here on page 8 : http://www.math.columbia.edu/~goldfeld/ErdosSelbergDispute.pdf although that is an integral instead of a sum. I wanted to mention it. Maybe it has value to someone.","where is the prime counting function. My mentor more or less wrote that when he was around yo. If you use the Riemann zeta function to prove the PNT and you know the easy (easy to show for large at least) then with some little extra work you can prove the two statements at least for large or large . But how about proving it directly ? What if we did not know that is about ? What if we do not use the zeta function nor complex dynamics ? Is this related to one of those elementary proofs of the prime number theorem ? How does one even come up with the idea of the equation above ?? What is the logic behind it ? Do not confuse with the similar looking you often see in number theory, this is not the one. And it is also not this one Because that one gives about where H_n is the n th harmonic number, what is a weak version of the PNT. It also does not follow from the 3 theorems of Mertens nor does it resemble anything I ever read. I am not looking for sharp asymptotics for here, that is not the question here. (although I will probably start a new topic about that soon) Any ideas ? Btw this function is even more mysterious when you think about it : For example substitute to This implies that and thus and and similar identities can be constructed. Imo very much like series multisection, fractals and self-referential things. But getting series expansions for it that are defined everywhere is hard. Does need to be continu ? Maybe we should relax the equation and write and the divisions get rounded or so. Or maybe we should truncate the ellipsis (...) somehow. But that still does not answer my questions. edit My mentor told me this if we take enough terms and if the following is true (conjecture A) It then follows that implies that Conjecture A might have a name, it is a weaker version of the second hardy-littlewood conjecture : Conjecture A makes sense for large and small for sure if we use a sharp version of the prime number theorem somewhat like the good asymptotic But I am unsure if conjecture A has been proven. Again this might be overkill but it is another way of thinking about it. see : $f(x) + f(x/2) + f(x/3) + ... = x$ and conjecture A : $\pi(x/a) > \frac{\pi(x)}{a}$ edit A similar looking idea seems to have occured here on page 8 : http://www.math.columbia.edu/~goldfeld/ErdosSelbergDispute.pdf although that is an integral instead of a sum. I wanted to mention it. Maybe it has value to someone.",f(x) + f(x/2) + f(x/3) + f(x/4) + ... = x f(n) < \pi(n+1) \lim_{n \to \infty} \frac{f(n)}{\pi(n)} = 1 \pi(n) 12 \zeta(s) f(x) < \frac{x}{\ln(x)-1} x x n \pi(n) = \frac{x}{\ln(x)-1} Li(x) + Li(x^{1/2}) + Li(x^{1/3}) + ... g(x) + g(x)/2 + g(x)/3 + g(x)/4 + ... = x g(n) = \frac{n}{H_n} f(x) f(x) + f(x/2) + f(x/3) + f(x/4) + ... = x x x/2 f(x/2) + f(x/4) + f(x/6) + f(x/8) + f(x/10) + ... = x/2 f(x) + f(x/3) + f(x/5) + f(x/7) + f(x/9) + ... = x/2 f(x/2) + f(x/4) + f(x/6) + f(x/8) + f(x/10) + ... = f(x) + f(x/3) + f(x/5) + f(x/7) + f(x/9) + ...  f(f(x) + f(x/3) + f(x/5) + f(x/7) + f(x/9) + ...) = f(f(x/2) + f(x/4) + f(x/6) + f(x/8) + f(x/10) + ...) = f(x/2) f f(n) + f(n/2) + f(n/3) + f(n/4) + ...= n \pi(x) + \pi(x/2) + \pi(x/3) + ... > x \pi(x/a) > \frac{\pi(x)}{a} f(x) + f(x/2) + f(x/3) + ... = x f(x) < \pi(x) \pi(x+y) - \pi(x) < \pi(y) + 1 x a \frac{x}{\ln(x) - 1 + \frac{1}{\ln(x)}},"['limits', 'number-theory', 'prime-numbers', 'functional-equations', 'alternative-proof']"
94,"Evaluate $\displaystyle\lim_{n\to \infty}\left [\left (1+\frac{2}{n^a}\right )^{-n^b}n^c\right ]$ for real $a,b,c$ and $n\geq 1$.",Evaluate  for real  and .,"\displaystyle\lim_{n\to \infty}\left [\left (1+\frac{2}{n^a}\right )^{-n^b}n^c\right ] a,b,c n\geq 1","Evaluate $\displaystyle\lim_{n\to \infty}\left [\left (1+\frac{2}{n^a}\right )^{-n^b}n^c\right ]$ for real $a,b,c$ and $n\geq 1$ . Which of the following are true? (a) If $b<a$ , $x\to 0$ as $n\to ∞$ (b) If $a < b$ , $x\to 0$ as $n\to ∞$ (c) If $a = b$ and $c > 0, x \to ∞$ as $n \to ∞$ (d) If $a = b$ and $c < 0, x\to ∞$ as $n \to ∞$ I was stuck for a long time, with this problem. I searched this site, to find, whether this was previously asked or not (as I saw, duplicates are closed according to the site policy). I found a question asked some years ago. Here's the link: How do I find $\lim_{n\to \infty}\left [\left (1+\frac{2}{n^a}\right )^{-n^b}n^c\right ]$ for real $a,b,c$ and $n\geq 1$? I looked at all the answers. But, somehow I feel some informations are very much lacking in all the answer and the question as well. I was particularly interested in the answer of a user named Qwerty. I quote the answer hereby: Standard limit rule says $$\lim_{n\to \infty}\left [\left (1+\frac{2}{n^a}\right )^{-n^b}\right]=e^{-2n^{b-a}}$$ So result is $$\lim\limits_{n\to \infty}{n^c\over e^{2n^{b-a}}}$$ $b>a\implies\lim\limits_{n\to \infty}{n^c\over e^{2n^{b-a}}}\to 0 \ \forall $ finite $c$ $b=a\implies \lim\limits_{n\to \infty}{n^c\over e^{2n^{b-a}}} \to \infty\ \ $ if $c\gt 0$ , $1$ if $c=0$ and $0$ if $c\lt 0$ $b<a \implies \lim\limits_{n\to \infty}{n^c\over e^{2n^{b-a}}} \to \infty$ This seems to be the most appealing to me. But I have a few questions (or to be definite, I am having a hard time with $3$ questions, regarding this question and the solution.): The user says : Standard limit rule says $$\lim_{n\to \infty}\left [\left (1+\frac{2}{n^a}\right )^{-n^b}\right]=e^{-2n^{b-a}}$$ So result is $$\lim\limits_{n\to \infty}{n^c\over e^{2n^{b-a}}}.$$ Now, the thing is if this was a standard fact, it was completely unknown to me. I was searching for a proof of this result. To my surprise, in that link (above) , a user named, avz2611 posts a proof of this apparent new result. I quote his proof as well: $$=\frac{n^c}{(1+\frac{2}{n^a})^{n^b}}$$ now let $$l=(1+\frac{2}{n^a})^{n^b}$$ $$\ln l={n^b}\ln({1+\frac{2}{n^a}})=\frac{2n^b}{n^a}\frac{\ln({1+\frac{2}{n^a}})}{2/n^{a}}=\frac{2n^b}{n^a}.1$$ $$\therefore l=e^ {\frac{2n^b}{n^a}}$$ therefore original limit is $$=\frac{n^c}{e^{2n^{b-a}}}$$ now exponential function raises a lot faster than a finite polynomial thus limit would $\infty$ if $b<=a$ and $0$ if $b>a$ assuming $c$ is positive. I did a little modification as I used italics to highlight the proof of the above identity. Nevertheless, this proof, looks good only if, we assume $a>0$ in the 4th step, which appears to be using the formula $\lim_{x\to 0}\frac{ln(1+x)}{x}=1,$ and if, say, $a<0$ , then in the 4th step, $$\frac{\ln({1+\frac{2}{n^a}})}{2/n^{a}}=1$$ , is not true, as $\frac{2}{n^a}$ does not tend to (or approach) $0$ if, $a$ is negative, it rather tends to infinity. So, the assumption that should have been taken, is that $a>0$ in order to use the identity, $$\lim_{n\to \infty}\left [\left (1+\frac{2}{n^a}\right )^{-n^b}\right]=e^{-2n^{b-a}}$$ as in Qwerty's answer. It seems, each and every answer is making use of this identity. But I am not quite convinced about it's usage, unless it is done under the assumption, $a>0.$ Am I missing something? Further, is this identity a very standard fact, that can be used as a theorem ? My next question, is, how does , say, for example, in Qwerty's answer, how was it concluded, "" $b>a\implies\lim\limits_{n\to \infty}{n^c\over e^{2n^{b-a}}}\to 0 \ \forall $ finite $c$ "" ? I got convinced by this statement, by intuitively guessing maybe, that, $e^{2n^{b-a}}$ increases more rapidly than, $n^c$ , without no reasoning so as to back up this intution . My question, is, was this fact stated solely based on intuition ? The next question, also is similar to the previous one. In the quoted answer, it is mentioned, "" $b<a \implies \lim\limits_{n\to \infty}{n^c\over e^{2n^{b-a}}} \to \infty$ "". Again, was this fact, solely stated based upon intuition and appeals the reader to just convince themselves, that this indeed holds, by using intuitions ? These are the $3$ questions, that I am having a hard time dealing with. I am looking for an elementary answer, to these questions, because, I don't seem to have an adequate knowledge in real analysis as of now. So, to be precise, an answer to these questions of mine, that uses only elementary calculus is the most optimal or better say, acceptable judging this scenario. To be honest, I dont know, whether these sort of posts are allowed here or not. But I found that majority of users advocate for posts asking for clarifications for a particular solution, to be acceptable.","Evaluate for real and . Which of the following are true? (a) If , as (b) If , as (c) If and as (d) If and as I was stuck for a long time, with this problem. I searched this site, to find, whether this was previously asked or not (as I saw, duplicates are closed according to the site policy). I found a question asked some years ago. Here's the link: How do I find $\lim_{n\to \infty}\left [\left (1+\frac{2}{n^a}\right )^{-n^b}n^c\right ]$ for real $a,b,c$ and $n\geq 1$? I looked at all the answers. But, somehow I feel some informations are very much lacking in all the answer and the question as well. I was particularly interested in the answer of a user named Qwerty. I quote the answer hereby: Standard limit rule says So result is finite if , if and if This seems to be the most appealing to me. But I have a few questions (or to be definite, I am having a hard time with questions, regarding this question and the solution.): The user says : Standard limit rule says So result is Now, the thing is if this was a standard fact, it was completely unknown to me. I was searching for a proof of this result. To my surprise, in that link (above) , a user named, avz2611 posts a proof of this apparent new result. I quote his proof as well: now let therefore original limit is now exponential function raises a lot faster than a finite polynomial thus limit would if and if assuming is positive. I did a little modification as I used italics to highlight the proof of the above identity. Nevertheless, this proof, looks good only if, we assume in the 4th step, which appears to be using the formula and if, say, , then in the 4th step, , is not true, as does not tend to (or approach) if, is negative, it rather tends to infinity. So, the assumption that should have been taken, is that in order to use the identity, as in Qwerty's answer. It seems, each and every answer is making use of this identity. But I am not quite convinced about it's usage, unless it is done under the assumption, Am I missing something? Further, is this identity a very standard fact, that can be used as a theorem ? My next question, is, how does , say, for example, in Qwerty's answer, how was it concluded, "" finite "" ? I got convinced by this statement, by intuitively guessing maybe, that, increases more rapidly than, , without no reasoning so as to back up this intution . My question, is, was this fact stated solely based on intuition ? The next question, also is similar to the previous one. In the quoted answer, it is mentioned, "" "". Again, was this fact, solely stated based upon intuition and appeals the reader to just convince themselves, that this indeed holds, by using intuitions ? These are the questions, that I am having a hard time dealing with. I am looking for an elementary answer, to these questions, because, I don't seem to have an adequate knowledge in real analysis as of now. So, to be precise, an answer to these questions of mine, that uses only elementary calculus is the most optimal or better say, acceptable judging this scenario. To be honest, I dont know, whether these sort of posts are allowed here or not. But I found that majority of users advocate for posts asking for clarifications for a particular solution, to be acceptable.","\displaystyle\lim_{n\to \infty}\left [\left (1+\frac{2}{n^a}\right )^{-n^b}n^c\right ] a,b,c n\geq 1 b<a x\to 0 n\to ∞ a < b x\to 0 n\to ∞ a = b c > 0, x \to ∞ n \to ∞ a = b c < 0, x\to ∞ n \to ∞ \lim_{n\to \infty}\left [\left (1+\frac{2}{n^a}\right )^{-n^b}\right]=e^{-2n^{b-a}} \lim\limits_{n\to \infty}{n^c\over e^{2n^{b-a}}} b>a\implies\lim\limits_{n\to \infty}{n^c\over e^{2n^{b-a}}}\to 0 \ \forall  c b=a\implies \lim\limits_{n\to \infty}{n^c\over e^{2n^{b-a}}} \to \infty\ \  c\gt 0 1 c=0 0 c\lt 0 b<a \implies \lim\limits_{n\to \infty}{n^c\over e^{2n^{b-a}}} \to \infty 3 \lim_{n\to \infty}\left [\left (1+\frac{2}{n^a}\right )^{-n^b}\right]=e^{-2n^{b-a}} \lim\limits_{n\to \infty}{n^c\over e^{2n^{b-a}}}. =\frac{n^c}{(1+\frac{2}{n^a})^{n^b}} l=(1+\frac{2}{n^a})^{n^b} \ln l={n^b}\ln({1+\frac{2}{n^a}})=\frac{2n^b}{n^a}\frac{\ln({1+\frac{2}{n^a}})}{2/n^{a}}=\frac{2n^b}{n^a}.1 \therefore l=e^ {\frac{2n^b}{n^a}} =\frac{n^c}{e^{2n^{b-a}}} \infty b<=a 0 b>a c a>0 \lim_{x\to 0}\frac{ln(1+x)}{x}=1, a<0 \frac{\ln({1+\frac{2}{n^a}})}{2/n^{a}}=1 \frac{2}{n^a} 0 a a>0 \lim_{n\to \infty}\left [\left (1+\frac{2}{n^a}\right )^{-n^b}\right]=e^{-2n^{b-a}} a>0. b>a\implies\lim\limits_{n\to \infty}{n^c\over e^{2n^{b-a}}}\to 0 \ \forall  c e^{2n^{b-a}} n^c b<a \implies \lim\limits_{n\to \infty}{n^c\over e^{2n^{b-a}}} \to \infty 3","['calculus', 'limits', 'proof-explanation']"
95,Fractional part of $(3+\sqrt{2})^n$,Fractional part of,(3+\sqrt{2})^n,"For $r\in\mathbb{R}$ , define $\Vert{r}\Vert:=\inf_{n\in\mathbb{Z}}\vert r-n\vert$ . By Question 4208947 and Question 1536761 , we know that $\lim_{n\rightarrow \infty}\Vert(3+2\sqrt{2})^n\Vert=0$ and $\lim_{n\rightarrow \infty}\Vert(2+\sqrt{3})^n\Vert=0$ . The method is interesting but can only be applied to restricted numbers. I am wondering whether $\lim_{n\rightarrow \infty}\Vert(3+\sqrt{2})^n\Vert$ exists and is there $s\in \mathbb{R}\setminus\{0\}$ such that $\lim_{n\rightarrow \infty}\Vert(3+\sqrt{2})^n\cdot s\Vert=0$ . Any advice would be helpful. Thanks!","For , define . By Question 4208947 and Question 1536761 , we know that and . The method is interesting but can only be applied to restricted numbers. I am wondering whether exists and is there such that . Any advice would be helpful. Thanks!",r\in\mathbb{R} \Vert{r}\Vert:=\inf_{n\in\mathbb{Z}}\vert r-n\vert \lim_{n\rightarrow \infty}\Vert(3+2\sqrt{2})^n\Vert=0 \lim_{n\rightarrow \infty}\Vert(2+\sqrt{3})^n\Vert=0 \lim_{n\rightarrow \infty}\Vert(3+\sqrt{2})^n\Vert s\in \mathbb{R}\setminus\{0\} \lim_{n\rightarrow \infty}\Vert(3+\sqrt{2})^n\cdot s\Vert=0,"['limits', 'number-theory', 'fractional-part']"
96,Limits of recursions like $f(n+2)=\frac{1}{f(n)} - \frac{1}{f(n+1)}$ !?,Limits of recursions like  !?,f(n+2)=\frac{1}{f(n)} - \frac{1}{f(n+1)},"Consider the sequences $$f(0)=1,f(1)=2$$ $$f(n+2)=\frac{1}{f(n)} - \frac{1}{f(n+1)}$$ $$g(0)=1,g(1)=2,g(2)=3,g(3)=4$$ $$g(n+4)=\frac{3}{g(n)} - \frac{3}{g(n+1)} + \frac{3}{g(n+2)} - \frac{3}{g(n+3)}$$ Then as $n$ goes to infinity : $$\lim f(n)^2 = 2$$ $$\lim g(n)^2 = 12$$ Notice that one can not simply plug in the limits $L_1$ or $L_2$ , because $$L_1=\frac{1}{L_1} - \frac{1}{L_1} = 0 ??$$ does not make sense and $$L_2=\frac{3}{L_2} - \frac{3}{L_2} + \frac{3}{L_2} - \frac{3}{L_2} = 0 ??$$ does not make sense either. So we must keep in mind that it behaves different; double limits or multiple limits or limits converging at different rates. But also noteworthy is this : The initial values $f(0),f(1),g(0),g(1),g(2),g(3)$ do not matter much as long as most of them are distinct like $f(0) \neq f(1)$ for instance. We get the same limits anyways. Also keep in mind that replacing $-$ with $+$ in the formulas will also work BUT changing the positions of $+$ or $-$ will NOT. However although most initial conditions will work for both $f$ and $g$ , in particular if no division by zero occurs , some exceptional ones might not work. And what if we start with say gaussian integers or eisenstein integers or complex numbers ? Despite it almost always converges to these limits , what are the exceptional set of exceptions ? Are the exceptions related to continued fractions, julia sets or irrationality measures ? I considered iterating the equations a few times to get expressions for $f(n+5),g(n+5)$ and such and although it has its benefits and seems to make more sense in a way ( dealing with the + and - ) it also becomes more complicated fast. ( somewhat remind me of somos or modular equations ) Also want to note these equations are part of a large family, I could easily describe a similar $h(n)$ depending on 6 or 10 previous terms converging to algebraic numbers. I also considered reducing the dependancy on previous values, by special case starting positions or by trying to relate it to pure iteration ( of one previous value ) of other functions ( like fibonacci is related to exp ). But it seems non-trivial. It is not clearly related to typical numerical methods such as newton's method or continued fractions or the babylonian method. How do we even prove it attracts ? Notice limits like $\frac{f(2n)^2 - 2}{f(2n+2)^2 - 2}$ and the alike also do not seem to converge in an easy pattern, so this sets it apart from most numerical methods. This is probably due to the divisions. I considered comparing to median methods and other averaging methods but it just seems different fundamentally. Any insight in this would be appreciated. edit I found this question as a ""+ analogue"" for the case $f(n)$ . For that case there are starting values that get different outcomes. So that suggests maybe here also ? This was asked in the comments and mentioned in the OP, so I add it now : Why does the process defined with $a_{n+2} = \frac{1}{a_n} + \frac{1}{a_{n+1}}$ converge to $\pm\sqrt{2}$ for most choices of the starting values? and the case for $g(n)$ is probably more complicated.","Consider the sequences Then as goes to infinity : Notice that one can not simply plug in the limits or , because does not make sense and does not make sense either. So we must keep in mind that it behaves different; double limits or multiple limits or limits converging at different rates. But also noteworthy is this : The initial values do not matter much as long as most of them are distinct like for instance. We get the same limits anyways. Also keep in mind that replacing with in the formulas will also work BUT changing the positions of or will NOT. However although most initial conditions will work for both and , in particular if no division by zero occurs , some exceptional ones might not work. And what if we start with say gaussian integers or eisenstein integers or complex numbers ? Despite it almost always converges to these limits , what are the exceptional set of exceptions ? Are the exceptions related to continued fractions, julia sets or irrationality measures ? I considered iterating the equations a few times to get expressions for and such and although it has its benefits and seems to make more sense in a way ( dealing with the + and - ) it also becomes more complicated fast. ( somewhat remind me of somos or modular equations ) Also want to note these equations are part of a large family, I could easily describe a similar depending on 6 or 10 previous terms converging to algebraic numbers. I also considered reducing the dependancy on previous values, by special case starting positions or by trying to relate it to pure iteration ( of one previous value ) of other functions ( like fibonacci is related to exp ). But it seems non-trivial. It is not clearly related to typical numerical methods such as newton's method or continued fractions or the babylonian method. How do we even prove it attracts ? Notice limits like and the alike also do not seem to converge in an easy pattern, so this sets it apart from most numerical methods. This is probably due to the divisions. I considered comparing to median methods and other averaging methods but it just seems different fundamentally. Any insight in this would be appreciated. edit I found this question as a ""+ analogue"" for the case . For that case there are starting values that get different outcomes. So that suggests maybe here also ? This was asked in the comments and mentioned in the OP, so I add it now : Why does the process defined with $a_{n+2} = \frac{1}{a_n} + \frac{1}{a_{n+1}}$ converge to $\pm\sqrt{2}$ for most choices of the starting values? and the case for is probably more complicated.","f(0)=1,f(1)=2 f(n+2)=\frac{1}{f(n)} - \frac{1}{f(n+1)} g(0)=1,g(1)=2,g(2)=3,g(3)=4 g(n+4)=\frac{3}{g(n)} - \frac{3}{g(n+1)} + \frac{3}{g(n+2)} - \frac{3}{g(n+3)} n \lim f(n)^2 = 2 \lim g(n)^2 = 12 L_1 L_2 L_1=\frac{1}{L_1} - \frac{1}{L_1} = 0 ?? L_2=\frac{3}{L_2} - \frac{3}{L_2} + \frac{3}{L_2} - \frac{3}{L_2} = 0 ?? f(0),f(1),g(0),g(1),g(2),g(3) f(0) \neq f(1) - + + - f g f(n+5),g(n+5) h(n) \frac{f(2n)^2 - 2}{f(2n+2)^2 - 2} f(n) g(n)","['limits', 'numerical-methods', 'recurrence-relations', 'dynamical-systems', 'complex-dynamics']"
97,Why are infinite series seemingly allowed to ignore the Commutative Property? [duplicate],Why are infinite series seemingly allowed to ignore the Commutative Property? [duplicate],,"This question already has answers here : Why does the commutative property of addition not hold for conditionally convergent series? (4 answers) Closed last year . We just learned about conditionally convergent series vs absolutely convergent series in my hs calculus class and I'm really confused about why conditionally convergent series are seemingly allowed to ignore the Commutative Property. We were given this example in class. $$1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}... = \ln(2)$$ We weren't shown why this is $\ln(2)$ but just that it was given. Then he rearranged into $$(1-\frac{1}{2})-\frac{1}{4}+(\frac{1}{3}-\frac{1}{6})-\frac{1}{8}+(\frac{1}{5}-\frac{1}{10})...$$ which equals $$\frac{1}{2}-\frac{1}{4}+\frac{1}{6}-\frac{1}{8}+\frac{1}{10}... $$ he then factors out 1/2 which gives $$\frac{1}{2}(1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}...)$$ Which includes the original series so the final conclusion is that the new series is $$\frac{\ln(2)}{2}$$ Now here is my question : WHY??? This makes sense (sort of) but I learned in second grade that 1+2=2+1 , which is the Commutative Property, I don't understand why we can ignore that just because it's an infinite series. Are there any more clear examples or better ways of thinking about it so that it actually makes sense, or is it just one of those things that you have to take as given. Why is it considered ""Conditionally Convergent"" instead of ""Divergent"" ? Thank you for the help","This question already has answers here : Why does the commutative property of addition not hold for conditionally convergent series? (4 answers) Closed last year . We just learned about conditionally convergent series vs absolutely convergent series in my hs calculus class and I'm really confused about why conditionally convergent series are seemingly allowed to ignore the Commutative Property. We were given this example in class. We weren't shown why this is but just that it was given. Then he rearranged into which equals he then factors out 1/2 which gives Which includes the original series so the final conclusion is that the new series is Now here is my question : WHY??? This makes sense (sort of) but I learned in second grade that 1+2=2+1 , which is the Commutative Property, I don't understand why we can ignore that just because it's an infinite series. Are there any more clear examples or better ways of thinking about it so that it actually makes sense, or is it just one of those things that you have to take as given. Why is it considered ""Conditionally Convergent"" instead of ""Divergent"" ? Thank you for the help",1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}... = \ln(2) \ln(2) (1-\frac{1}{2})-\frac{1}{4}+(\frac{1}{3}-\frac{1}{6})-\frac{1}{8}+(\frac{1}{5}-\frac{1}{10})... \frac{1}{2}-\frac{1}{4}+\frac{1}{6}-\frac{1}{8}+\frac{1}{10}...  \frac{1}{2}(1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}...) \frac{\ln(2)}{2},"['calculus', 'sequences-and-series', 'limits']"
98,$\lim\limits_{x\to \infty}\frac{3x^3+x^2+\sin(e^x)}{5x-8x^3+\arctan(\log x)} $,,\lim\limits_{x\to \infty}\frac{3x^3+x^2+\sin(e^x)}{5x-8x^3+\arctan(\log x)} ,"Compute the limit: $$\lim_{x\to \infty}\frac{3x^3+x^2+\sin(e^x)}{5x-8x^3+\arctan(\log x)} $$ I worked out this to be $\frac{-3}{8}$ . I believe this is correct. I used the sandwich theorem but my issue is that the denominator is sometimes negative which isn't helping me. I tried to use absolute values to get rid of this problem: $$\frac{3x^3+x^2+\sin(e^x)}{5x-8x^3+\arctan(\log x)} \leq \frac{3x^3+x^2+1}{5x-8x^3-\frac{\pi}{2}},$$ which I then could show is $\frac{-3}{8}$ using algebra of limits, but I am certain this is wrong. My other way of thinking is: $$\frac{3x^3+x^2+\sin(e^x)}{5x-8x^3+\arctan(\log x)} = \frac{3+\frac{1}{x}+\frac{\sin(e^x)}{x^3}}{\frac{5}{x^2}-8+\frac{\arctan(\log x)}{x^3}}  $$ Note that $-1 \leq \sin(e^x) \leq 1$ , so for all $x>0$ , we have $\frac{-1}{x^3} \leq \frac{\sin(e^x)}{x^3} \leq \frac{1}{x^3}$ , so the limit is $0$ . Note that $\frac{-\pi}{2x^3} \leq \frac{\arctan(\log x)}{x^3} \leq \frac{\pi}{2x^3}$ , so the limit is $0$ . Now by taking limits: $$\frac{3+\frac{1}{x}+\frac{\sin(e^x)}{x^3}}{\frac{5}{x^2}-8+\frac{\arctan(\log x)}{x^3}} =\frac{3+0+0}{0-8+0} = -\frac{3}{8}.$$ I believe my second way of working is correct. Can someone please tell me if I am right ? Also, is there a way of doing with this with absolute values ?","Compute the limit: I worked out this to be . I believe this is correct. I used the sandwich theorem but my issue is that the denominator is sometimes negative which isn't helping me. I tried to use absolute values to get rid of this problem: which I then could show is using algebra of limits, but I am certain this is wrong. My other way of thinking is: Note that , so for all , we have , so the limit is . Note that , so the limit is . Now by taking limits: I believe my second way of working is correct. Can someone please tell me if I am right ? Also, is there a way of doing with this with absolute values ?","\lim_{x\to \infty}\frac{3x^3+x^2+\sin(e^x)}{5x-8x^3+\arctan(\log x)}  \frac{-3}{8} \frac{3x^3+x^2+\sin(e^x)}{5x-8x^3+\arctan(\log x)} \leq \frac{3x^3+x^2+1}{5x-8x^3-\frac{\pi}{2}}, \frac{-3}{8} \frac{3x^3+x^2+\sin(e^x)}{5x-8x^3+\arctan(\log x)} = \frac{3+\frac{1}{x}+\frac{\sin(e^x)}{x^3}}{\frac{5}{x^2}-8+\frac{\arctan(\log x)}{x^3}} 
 -1 \leq \sin(e^x) \leq 1 x>0 \frac{-1}{x^3} \leq \frac{\sin(e^x)}{x^3} \leq \frac{1}{x^3} 0 \frac{-\pi}{2x^3} \leq \frac{\arctan(\log x)}{x^3} \leq \frac{\pi}{2x^3} 0 \frac{3+\frac{1}{x}+\frac{\sin(e^x)}{x^3}}{\frac{5}{x^2}-8+\frac{\arctan(\log x)}{x^3}} =\frac{3+0+0}{0-8+0} = -\frac{3}{8}.",['real-analysis']
99,How to show that some number is not a limit of a function by the negation of the definition of limits,How to show that some number is not a limit of a function by the negation of the definition of limits,,"The negation of the definition of limit of functions is $\exists\epsilon>0\forall\delta>0\exists x\neq a(|x-a|<\delta \text{ and } |f(x)-f(a)|\geq\epsilon)$ . How do we use this to show for example that $lim_{x\rightarrow 3}x\neq 6$ or $lim_{x\rightarrow 3}x^2\neq 18$ ? From the first limit we have that the following must apply $\exists\epsilon>0\forall\delta>0\exists\neq 3(0<|x-3|<\delta \text{ and } |x-6|\geq\epsilon)$ . I get stuck at $\forall\delta>0$ since for me it implies that $x\in\mathbb{R}$ but $x\neq 3$ and with this there can not be any $\epsilon>0$ such that $|x-6|\geq \epsilon$ . Edit: So the negation of the definition should be $\exists\epsilon>0\forall\delta>0\exists x\neq a(0<|x-a|<\delta \text{ and } |f(x)-f(a)|\geq\epsilon)$ I think. I'm going to try to work with this but I appreciate answers. Edit 2: This doesn't seem trivial at all. If we want to show that the negation of the definition applies to the first limit then we need to show that that there exists a $\epsilon>0$ such that for all $\delta>0$ there is a $x\neq 3$ such that the conditions are met. But the problem for me is $\delta>0$ , with that I can't pick a $x$ since I can always make $\delta$ smaller such that the $x$ is not valid anymore.","The negation of the definition of limit of functions is . How do we use this to show for example that or ? From the first limit we have that the following must apply . I get stuck at since for me it implies that but and with this there can not be any such that . Edit: So the negation of the definition should be I think. I'm going to try to work with this but I appreciate answers. Edit 2: This doesn't seem trivial at all. If we want to show that the negation of the definition applies to the first limit then we need to show that that there exists a such that for all there is a such that the conditions are met. But the problem for me is , with that I can't pick a since I can always make smaller such that the is not valid anymore.",\exists\epsilon>0\forall\delta>0\exists x\neq a(|x-a|<\delta \text{ and } |f(x)-f(a)|\geq\epsilon) lim_{x\rightarrow 3}x\neq 6 lim_{x\rightarrow 3}x^2\neq 18 \exists\epsilon>0\forall\delta>0\exists\neq 3(0<|x-3|<\delta \text{ and } |x-6|\geq\epsilon) \forall\delta>0 x\in\mathbb{R} x\neq 3 \epsilon>0 |x-6|\geq \epsilon \exists\epsilon>0\forall\delta>0\exists x\neq a(0<|x-a|<\delta \text{ and } |f(x)-f(a)|\geq\epsilon) \epsilon>0 \delta>0 x\neq 3 \delta>0 x \delta x,"['real-analysis', 'calculus', 'limits']"
