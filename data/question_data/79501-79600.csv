,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Finding a nullspace of a matrix.,Finding a nullspace of a matrix.,,"I am given the following matrix $A$ and I need to find a nullspace of this matrix. $$A =  \begin{pmatrix}   2&1&4&-1 \\   1&1&1&1 \\   1&0&3&-2  \\   -3&-2&-5&0  \end{pmatrix}$$ I have found a row reduced form of this matrix, which is: $$A' =  \begin{pmatrix}   1&0&3&-2 \\   0&1&-2&3 \\   0&0&0&0  \\   0&0&0&0  \end{pmatrix}$$ And then I used the formula $A'x=0$, which gave me: $$A'x =  \begin{pmatrix}   1&0&3&-2 \\   0&1&-2&3 \\   0&0&0&0  \\   0&0&0&0  \end{pmatrix} \begin{pmatrix}   x_1 \\   x_2 \\   x_3  \\   x_4  \end{pmatrix}= \begin{pmatrix}   0 \\   0 \\   0  \\   0  \end{pmatrix}$$ Hence I obtained the following system of linear equations: $$\begin{cases} x_1+3x_3-2x_4=0 \\ x_2-2x_3+3x_4=0 \end{cases}$$ So I just said that $x_3=\alpha$, $x_4=\beta$ and the nullspace is: $$nullspace(A)=\{2\beta-3\alpha,2\alpha-3\beta,\alpha,\beta) \ | \ \alpha,\beta \in \mathbb{R}\}$$ Is my thinking correct? Thank you guys!","I am given the following matrix $A$ and I need to find a nullspace of this matrix. $$A =  \begin{pmatrix}   2&1&4&-1 \\   1&1&1&1 \\   1&0&3&-2  \\   -3&-2&-5&0  \end{pmatrix}$$ I have found a row reduced form of this matrix, which is: $$A' =  \begin{pmatrix}   1&0&3&-2 \\   0&1&-2&3 \\   0&0&0&0  \\   0&0&0&0  \end{pmatrix}$$ And then I used the formula $A'x=0$, which gave me: $$A'x =  \begin{pmatrix}   1&0&3&-2 \\   0&1&-2&3 \\   0&0&0&0  \\   0&0&0&0  \end{pmatrix} \begin{pmatrix}   x_1 \\   x_2 \\   x_3  \\   x_4  \end{pmatrix}= \begin{pmatrix}   0 \\   0 \\   0  \\   0  \end{pmatrix}$$ Hence I obtained the following system of linear equations: $$\begin{cases} x_1+3x_3-2x_4=0 \\ x_2-2x_3+3x_4=0 \end{cases}$$ So I just said that $x_3=\alpha$, $x_4=\beta$ and the nullspace is: $$nullspace(A)=\{2\beta-3\alpha,2\alpha-3\beta,\alpha,\beta) \ | \ \alpha,\beta \in \mathbb{R}\}$$ Is my thinking correct? Thank you guys!",,"['linear-algebra', 'matrices', 'solution-verification']"
1,Prove matrices are of equal rank,Prove matrices are of equal rank,,"Suppose $P$ and $Q$ are $n \times n$ matrices of real numbers such that $P^2 = P$, $Q^2=Q$ and   $I-P-Q$ is invertible, where $I$ is the $n × n$ identity matrix. Show that $P$ and $Q$ have the same rank. Since $I-P-Q$ is invertible, it has rank $n$. Also, $det(I-P-Q) \neq 0$. Can we get the result from these facts?","Suppose $P$ and $Q$ are $n \times n$ matrices of real numbers such that $P^2 = P$, $Q^2=Q$ and   $I-P-Q$ is invertible, where $I$ is the $n × n$ identity matrix. Show that $P$ and $Q$ have the same rank. Since $I-P-Q$ is invertible, it has rank $n$. Also, $det(I-P-Q) \neq 0$. Can we get the result from these facts?",,"['linear-algebra', 'matrices', 'matrix-rank']"
2,Cholesky Factorization with submatrices,Cholesky Factorization with submatrices,,"Let $\mathbf{A}\in\mathbb{R}^{N \times N}$ be symmetric positive definite. For some $1\leq k<N$, partition  $$\mathbf{A}=\begin{pmatrix}\mathbf{A}_{11} & \mathbf{A}_{12} \\ \mathbf{A}_{12}^T & \mathbf{A}_{22}\end{pmatrix},$$ where $\mathbf{A}_{11}$ is $k\times k$ and $\mathbf{A}_{22}$ is $(N-k)\times (N-k)$. Let $\mathbf{A}=\mathbf{L}\mathbf{L}^T$ be a Cholesky factorization, where  $$\mathbf{L}=\begin{pmatrix}\mathbf{L}_{11} & \mathbf{0} \\ \mathbf{L}_{21} & \mathbf{L}_{22}\end{pmatrix}.$$ The $k\times k$ matrix $\mathbf{L}_{11}$  and the $(N-k)\times (N-k)$ matrix $\mathbf{L}_{22}$  are lower triangular. Express the submatrices  $\mathbf{L}_{ij}$ in terms of the submatrices $\mathbf{A}_{ij}$ and  appropriate Cholesky factors. I've been able to find that, $$\mathbf{A}_{11} = \mathbf{L}_{11}\mathbf{L}_{11}^T$$ $$\mathbf{A}_{12} = \mathbf{L}_{11}\mathbf{L}_{21}^T$$ $$\mathbf{A}_{12}^T = \mathbf{L}_{21}\mathbf{L}_{11}^T$$ $$\mathbf{A}_{22} = \mathbf{L}_{21}\mathbf{L}_{21}^T + \mathbf{L}_{22}\mathbf{L}_{22}^T,$$ but I am unsure of how to solve for the $\mathbf{L}_{ij}$'s. Does anyone have any ideas?","Let $\mathbf{A}\in\mathbb{R}^{N \times N}$ be symmetric positive definite. For some $1\leq k<N$, partition  $$\mathbf{A}=\begin{pmatrix}\mathbf{A}_{11} & \mathbf{A}_{12} \\ \mathbf{A}_{12}^T & \mathbf{A}_{22}\end{pmatrix},$$ where $\mathbf{A}_{11}$ is $k\times k$ and $\mathbf{A}_{22}$ is $(N-k)\times (N-k)$. Let $\mathbf{A}=\mathbf{L}\mathbf{L}^T$ be a Cholesky factorization, where  $$\mathbf{L}=\begin{pmatrix}\mathbf{L}_{11} & \mathbf{0} \\ \mathbf{L}_{21} & \mathbf{L}_{22}\end{pmatrix}.$$ The $k\times k$ matrix $\mathbf{L}_{11}$  and the $(N-k)\times (N-k)$ matrix $\mathbf{L}_{22}$  are lower triangular. Express the submatrices  $\mathbf{L}_{ij}$ in terms of the submatrices $\mathbf{A}_{ij}$ and  appropriate Cholesky factors. I've been able to find that, $$\mathbf{A}_{11} = \mathbf{L}_{11}\mathbf{L}_{11}^T$$ $$\mathbf{A}_{12} = \mathbf{L}_{11}\mathbf{L}_{21}^T$$ $$\mathbf{A}_{12}^T = \mathbf{L}_{21}\mathbf{L}_{11}^T$$ $$\mathbf{A}_{22} = \mathbf{L}_{21}\mathbf{L}_{21}^T + \mathbf{L}_{22}\mathbf{L}_{22}^T,$$ but I am unsure of how to solve for the $\mathbf{L}_{ij}$'s. Does anyone have any ideas?",,"['linear-algebra', 'matrices', 'numerical-linear-algebra', 'matrix-decomposition', 'cholesky-decomposition']"
3,Is the trace of a matrix a norm?,Is the trace of a matrix a norm?,,"If the matrix norm of A is defined as $\|A\|=\sum_{i,j}|Aij|$ then  how do I determine if the sum of the diagonal elements, i.e., the trace is a valid norm? I am not really sure how to approach this problem. Would it have to do anything with the max elements of a particular row/column of the matrix?","If the matrix norm of A is defined as $\|A\|=\sum_{i,j}|Aij|$ then  how do I determine if the sum of the diagonal elements, i.e., the trace is a valid norm? I am not really sure how to approach this problem. Would it have to do anything with the max elements of a particular row/column of the matrix?",,"['matrices', 'normed-spaces', 'trace']"
4,Prove $\operatorname{rank}(AB) = \operatorname{rank}(B)$,Prove,\operatorname{rank}(AB) = \operatorname{rank}(B),"I am given the info that A is an invertible  m by m matrix and B is a m by n matrix. By solving for the null space $ABx=0$, I get $A^{-1}ABx=A^{-1}0$, therefore $Bx=0$. I concluded that the null spaces of A and AB must be equivalent, and therefore the nullity(AB) = nullity(B) = some number p. Therefore rank(AB) = n-p = rank(B)   (because both AB and B are m by n matrices).   Is this proof valid, specifically the null space step?","I am given the info that A is an invertible  m by m matrix and B is a m by n matrix. By solving for the null space $ABx=0$, I get $A^{-1}ABx=A^{-1}0$, therefore $Bx=0$. I concluded that the null spaces of A and AB must be equivalent, and therefore the nullity(AB) = nullity(B) = some number p. Therefore rank(AB) = n-p = rank(B)   (because both AB and B are m by n matrices).   Is this proof valid, specifically the null space step?",,"['linear-algebra', 'matrices', 'proof-verification', 'matrix-rank']"
5,Rank as norm on matrix,Rank as norm on matrix,,"Could we consider matrix rank $r$ a norm? Is other norm similar to rank $r$ possible to associate with a finite matrix? (We denote trivial valuation $|\alpha|=1,\quad\forall\alpha\in\Bbb F^*$ where $\Bbb F$ is field/division algebra). It seems to satisfy norm axioms: $1$ $\mathsf{rank}(M)=0\iff M=0$. $2$ $\mathsf{rank}(\alpha M)=|\alpha|\mathsf{rank}(M)=\mathsf{rank}(M)$. $3$ $\mathsf{rank}(M+N)\leq \mathsf{rank}(M)+\mathsf{rank}(N)$.","Could we consider matrix rank $r$ a norm? Is other norm similar to rank $r$ possible to associate with a finite matrix? (We denote trivial valuation $|\alpha|=1,\quad\forall\alpha\in\Bbb F^*$ where $\Bbb F$ is field/division algebra). It seems to satisfy norm axioms: $1$ $\mathsf{rank}(M)=0\iff M=0$. $2$ $\mathsf{rank}(\alpha M)=|\alpha|\mathsf{rank}(M)=\mathsf{rank}(M)$. $3$ $\mathsf{rank}(M+N)\leq \mathsf{rank}(M)+\mathsf{rank}(N)$.",,"['linear-algebra', 'matrices', 'metric-spaces', 'normed-spaces', 'noncommutative-algebra']"
6,Real matrix with the property that every nonzero vector in $\mathbb{R}^n$ is an eigenvector of $A$. [duplicate],Real matrix with the property that every nonzero vector in  is an eigenvector of . [duplicate],\mathbb{R}^n A,"This question already has answers here : Demonstration: If all vectors of $V$ are eigenvectors of $T$, then there is one $\lambda$ such that $T(v) = \lambda v$ for all $v \in V$. (3 answers) Closed 9 years ago . so I'm supposed to let $A$ be a square real matrix with the property that every nonzero vector in $\mathbb{R}^n$ is an eigenvector of $A$. And I'm supposed to show that $A=\lambda I$ for a constant $\lambda$ in $\mathbb{R}$. But I am having a lot of trouble with how to start this proof... Does it have anything to do with $Av=\lambda v$? I'm really confused. Any help would be appreciated.","This question already has answers here : Demonstration: If all vectors of $V$ are eigenvectors of $T$, then there is one $\lambda$ such that $T(v) = \lambda v$ for all $v \in V$. (3 answers) Closed 9 years ago . so I'm supposed to let $A$ be a square real matrix with the property that every nonzero vector in $\mathbb{R}^n$ is an eigenvector of $A$. And I'm supposed to show that $A=\lambda I$ for a constant $\lambda$ in $\mathbb{R}$. But I am having a lot of trouble with how to start this proof... Does it have anything to do with $Av=\lambda v$? I'm really confused. Any help would be appreciated.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
7,"Matrix function to express pair-wise distances of rows in $X, Y$",Matrix function to express pair-wise distances of rows in,"X, Y","There are two real matrices: $X, Y$ with $X$ being of dimension $n_1$ x $p$, $Y$ of dimension $n_2$ x $p$. The goal is to form the matrix $D$ of dimension $n_1$ x $n_2$ where each element $d_{ij}$ is computed as the (L2-) norm of the difference of row i of X and row j of Y: $$d_{ij} = \lVert x_i - y_j \rVert_2^2$$ My question is how to derive a matrix formula for $D$. I.e $$D = g(X,Y)$$ Here is my attempt at starting: I can expand $\lVert v\rVert_2^2$ as $v^Tv$ where $v = x_i - y_j$ to obtain: $$d_{ij} = x_i^Tx_i + y_j^Ty_j - 2x_i^Ty_j$$ But I'm not sure how to proceed.","There are two real matrices: $X, Y$ with $X$ being of dimension $n_1$ x $p$, $Y$ of dimension $n_2$ x $p$. The goal is to form the matrix $D$ of dimension $n_1$ x $n_2$ where each element $d_{ij}$ is computed as the (L2-) norm of the difference of row i of X and row j of Y: $$d_{ij} = \lVert x_i - y_j \rVert_2^2$$ My question is how to derive a matrix formula for $D$. I.e $$D = g(X,Y)$$ Here is my attempt at starting: I can expand $\lVert v\rVert_2^2$ as $v^Tv$ where $v = x_i - y_j$ to obtain: $$d_{ij} = x_i^Tx_i + y_j^Ty_j - 2x_i^Ty_j$$ But I'm not sure how to proceed.",,"['linear-algebra', 'matrices', 'normed-spaces', 'matrix-equations']"
8,proof on similarity of matrices,proof on similarity of matrices,,"Could you please help me with the following problem? Let $A$ be an $n$$\times$$n$ complex matrix. Prove that $A$ is similar to $B$, which is an $n$ $\times$ $n$ real matrix, if and only if $A$ is similar to its conjugate transpose. I have been trying to solve it for quite a long time, I tried to expand the definition of similarity but I did not  manage to get anything useful. I have no better idea.","Could you please help me with the following problem? Let $A$ be an $n$$\times$$n$ complex matrix. Prove that $A$ is similar to $B$, which is an $n$ $\times$ $n$ real matrix, if and only if $A$ is similar to its conjugate transpose. I have been trying to solve it for quite a long time, I tried to expand the definition of similarity but I did not  manage to get anything useful. I have no better idea.",,"['linear-algebra', 'matrices']"
9,Prove that a graph which is constructed with matrices is strongly regular,Prove that a graph which is constructed with matrices is strongly regular,,"Suppose that $F_q$ is a field with $q$ elements. Consider all $2\times d$ matrices with entries in $F_q$, so we have $q^{2d}$ matrices. Consider each matrix as a vertex, and two vertices $A$ and $B$ are adjacent when $\operatorname{rank}(A-B)=1$. We want to prove that this graph which was introduced is strongly regular. I fix an arbitrary vertex $A$. Because these matrices have two rows, I put one of rows (first row) of matrix $B$ with identity element of adding (first) operation of field, then put the other row (second) the converse of second row entries of $A$. In this case $\operatorname{rank}(A-B)$ will be 1. Now substitute first and second in above paragraph, then we have another $B$ with the properties we want, so I get this result that this graph is 2-regular, but I have no idea about $\lambda$ and $\mu$. Please check my answer and if this is wrong please make it right, and please help me with other part, thanks.","Suppose that $F_q$ is a field with $q$ elements. Consider all $2\times d$ matrices with entries in $F_q$, so we have $q^{2d}$ matrices. Consider each matrix as a vertex, and two vertices $A$ and $B$ are adjacent when $\operatorname{rank}(A-B)=1$. We want to prove that this graph which was introduced is strongly regular. I fix an arbitrary vertex $A$. Because these matrices have two rows, I put one of rows (first row) of matrix $B$ with identity element of adding (first) operation of field, then put the other row (second) the converse of second row entries of $A$. In this case $\operatorname{rank}(A-B)$ will be 1. Now substitute first and second in above paragraph, then we have another $B$ with the properties we want, so I get this result that this graph is 2-regular, but I have no idea about $\lambda$ and $\mu$. Please check my answer and if this is wrong please make it right, and please help me with other part, thanks.",,"['linear-algebra', 'abstract-algebra', 'matrices', 'graph-theory', 'algebraic-graph-theory']"
10,$A$ be a $10*10$ matrix with complex entries s.t. all eigenvalues are non negative real and at least one eigenvalue is positive.,be a  matrix with complex entries s.t. all eigenvalues are non negative real and at least one eigenvalue is positive.,A 10*10,"Let $A$ be a $10 \times 10$ matrix with complex entries s.t. all eigenvalues are non negative real and at least one eigenvalue is positive. Then which of the following statements is always false? A. there is  a matrix $B$ s.t. $AB-BA=B$ B.there is  a matrix $B$ s.t. $AB-BA=A$ C.there is  a matrix $B$ s.t. $AB+BA=A$ D.there is  a matrix $B$ s.t. $AB+BA=B$ I am new comer in Liner algebra. I have studied finite dimensional vector space, eigen value eigen vector from a book of G. Strang. I have found this in a competitive exam. I have no idea how to tackle this question. Can anybody help to solve this problem.","Let $A$ be a $10 \times 10$ matrix with complex entries s.t. all eigenvalues are non negative real and at least one eigenvalue is positive. Then which of the following statements is always false? A. there is  a matrix $B$ s.t. $AB-BA=B$ B.there is  a matrix $B$ s.t. $AB-BA=A$ C.there is  a matrix $B$ s.t. $AB+BA=A$ D.there is  a matrix $B$ s.t. $AB+BA=B$ I am new comer in Liner algebra. I have studied finite dimensional vector space, eigen value eigen vector from a book of G. Strang. I have found this in a competitive exam. I have no idea how to tackle this question. Can anybody help to solve this problem.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
11,Find the determinant of a matrix definition [duplicate],Find the determinant of a matrix definition [duplicate],,"This question already has answers here : Determinant of a rank $1$ update of a scalar matrix, or characteristic polynomial of a rank $1$ matrix (2 answers) Closed 9 years ago . Let $A$ be a matrix that is defined like this: $$A_{ij}=\begin{cases} \alpha,  & \text{if i=j} \\ \beta , & \text{if i $\ne$ j} \end{cases} $$ So I realized this matrix looks somehow like this $$         \begin{pmatrix}         \alpha & \beta & \beta \\         \beta & \alpha & \beta \\         \beta & \beta & \alpha \\         \end{pmatrix} $$ I tried to manipulate the rows to get an upper triangular matrix but couldn't succeed, am I in the right direction... some help?:)","This question already has answers here : Determinant of a rank $1$ update of a scalar matrix, or characteristic polynomial of a rank $1$ matrix (2 answers) Closed 9 years ago . Let $A$ be a matrix that is defined like this: $$A_{ij}=\begin{cases} \alpha,  & \text{if i=j} \\ \beta , & \text{if i $\ne$ j} \end{cases} $$ So I realized this matrix looks somehow like this $$         \begin{pmatrix}         \alpha & \beta & \beta \\         \beta & \alpha & \beta \\         \beta & \beta & \alpha \\         \end{pmatrix} $$ I tried to manipulate the rows to get an upper triangular matrix but couldn't succeed, am I in the right direction... some help?:)",,"['linear-algebra', 'matrices']"
12,Help with resolving an n x n determinant?,Help with resolving an n x n determinant?,,"I'm still a beginner, and would appreciate any tips regarding this. (Full solution appreciated, but hints more so!) This is the problem. \begin{equation}{D_n} = \begin{vmatrix} 1+{a_1} & 1  & ... & 1 \  1& 1+{a_2} &... & \vdots \  \vdots &...  &\ddots  &1 \  1&1  & ... & 1+{a_n} \end{vmatrix} So using elementary operations, I simply multipled the last row by -1 and added it to all other rows to get the below matrix, so we're left with two columns to expand. \begin{equation}{D_n} = \end{equation} \begin{vmatrix} {a_1} & 0  & ... & 0 & -{a_n} \\  0& {a_2} & 0 & ... & \vdots \\  \vdots &...  &\ddots  &... &\vdots \\  0 &...  &0  &{a_{n-1}} & -{a_n}\\  1&  ... & ... & 1& 1+{a_n} \end{vmatrix} This is where I got stuck - if I expanded them, I would get \begin{equation}{a_1}\times cofactor(a_{11}) + (-1)^n \times -{a_n} \times cofactor(a_{1n})\end{equation} Both of which would turn into recursive formulae. Is there an alternative way of simplifying the original matrix to get a more easily calculable format? EDIT : Following Alex's tip, I expanded it further by multiplying each row by its negative reciprocal to cancel out the 1's in the last row, so the determinant is simply equal to the product of the diagonal cells. The final answer I got was \begin{equation}\prod_{k=1}^{n-1} {a_k} \times \left(1 + {a_n} + {a_n} \left( \frac{1}{a_1}+ \frac{1}{a_2} + \dots + \frac{1}{a_n}\right)\right)\end{equation}","I'm still a beginner, and would appreciate any tips regarding this. (Full solution appreciated, but hints more so!) This is the problem. \begin{equation}{D_n} = \begin{vmatrix} 1+{a_1} & 1  & ... & 1 \  1& 1+{a_2} &... & \vdots \  \vdots &...  &\ddots  &1 \  1&1  & ... & 1+{a_n} \end{vmatrix} So using elementary operations, I simply multipled the last row by -1 and added it to all other rows to get the below matrix, so we're left with two columns to expand. \begin{equation}{D_n} = \end{equation} \begin{vmatrix} {a_1} & 0  & ... & 0 & -{a_n} \\  0& {a_2} & 0 & ... & \vdots \\  \vdots &...  &\ddots  &... &\vdots \\  0 &...  &0  &{a_{n-1}} & -{a_n}\\  1&  ... & ... & 1& 1+{a_n} \end{vmatrix} This is where I got stuck - if I expanded them, I would get \begin{equation}{a_1}\times cofactor(a_{11}) + (-1)^n \times -{a_n} \times cofactor(a_{1n})\end{equation} Both of which would turn into recursive formulae. Is there an alternative way of simplifying the original matrix to get a more easily calculable format? EDIT : Following Alex's tip, I expanded it further by multiplying each row by its negative reciprocal to cancel out the 1's in the last row, so the determinant is simply equal to the product of the diagonal cells. The final answer I got was \begin{equation}\prod_{k=1}^{n-1} {a_k} \times \left(1 + {a_n} + {a_n} \left( \frac{1}{a_1}+ \frac{1}{a_2} + \dots + \frac{1}{a_n}\right)\right)\end{equation}",,"['linear-algebra', 'matrices', 'determinant']"
13,Every unitarily invariant matrix norm is sub-multiplicative?,Every unitarily invariant matrix norm is sub-multiplicative?,,"Every unitarily invariant matrix norm is sub-multiplicative? In R. Bhatia, Matrix Analysis, after Proposition IV.2.4, it says that ""Every unitarily invariant matrix norm is sub-multiplicative"". But I could not verify... Here, a norm $||\cdot||$ is called unitarily invariant if $$||UAV||=||A||$$ for all matrix $A$, and unitary matrix $U,V$. And a norm is called sub-multiplicative if  $$||AB||\leq ||A|| \cdot ||B||.$$","Every unitarily invariant matrix norm is sub-multiplicative? In R. Bhatia, Matrix Analysis, after Proposition IV.2.4, it says that ""Every unitarily invariant matrix norm is sub-multiplicative"". But I could not verify... Here, a norm $||\cdot||$ is called unitarily invariant if $$||UAV||=||A||$$ for all matrix $A$, and unitary matrix $U,V$. And a norm is called sub-multiplicative if  $$||AB||\leq ||A|| \cdot ||B||.$$",,"['matrices', 'matrix-calculus']"
14,Elements of $GL_{2}(\mathbb{Z})$ of finite order,Elements of  of finite order,GL_{2}(\mathbb{Z}),"Prove that any element of $GL_{2}(\mathbb{Z})$ of finite order has order $1,2,3,4,6$ using Field Theory. My idea is to reduce such a finite order matrix say $A$ with order $n$ to modulo a prime $p$ . $\det A=\pm1$ so $A$ will land inside $G=\{M\in GL_{2}(\mathbb{F}_{p}):\det M=\pm 1\}$ . $|G|=2p(p^{2}-1)$ . So $n\mid 2p(p^{2}-1)$ for all prime $p$ . I don't know how to proceed after this.",Prove that any element of of finite order has order using Field Theory. My idea is to reduce such a finite order matrix say with order to modulo a prime . so will land inside . . So for all prime . I don't know how to proceed after this.,"GL_{2}(\mathbb{Z}) 1,2,3,4,6 A n p \det A=\pm1 A G=\{M\in GL_{2}(\mathbb{F}_{p}):\det M=\pm 1\} |G|=2p(p^{2}-1) n\mid 2p(p^{2}-1) p","['abstract-algebra', 'matrices', 'field-theory', 'linear-groups']"
15,Why are matrices written as such?,Why are matrices written as such?,,"Another thread has talked about the purpose of a matrix . Dr. Math roughly summarized it as: A matrix is just a compact notation, which allows you to specify several linear equations at once without having to write them all out. For example, instead of writing 3x + 4y + 5z =  7    2x - 3y + 6z =  6    2x + 5y - 9z = 11 I can write the same thing more compactly using matrices: $$ \begin{bmatrix} 3 & 4 & 5\\  2 & -3 & 6\\ 2 & 5 & -9\\  \end{bmatrix} \begin{bmatrix}x\\ y\\  z\\ \end{bmatrix} = \begin{bmatrix}7\\ 6\\  11\\  \end{bmatrix} $$ However, it seems that the more obvious way to do it would be: $$ \begin{array}{ll} \begin{bmatrix} x & y & -z\\  \end{bmatrix} \\ \begin{bmatrix} 3 & 4 & 5\\  2 & -3 & 6\\ 2 & 5 & -9\\  \end{bmatrix} = \begin{bmatrix}7\\ 6\\  11\\  \end{bmatrix} \end{array}  $$ Why are matrices written in a seemingly more complex way than the alternative? What's the advantage of doing so?","Another thread has talked about the purpose of a matrix . Dr. Math roughly summarized it as: A matrix is just a compact notation, which allows you to specify several linear equations at once without having to write them all out. For example, instead of writing 3x + 4y + 5z =  7    2x - 3y + 6z =  6    2x + 5y - 9z = 11 I can write the same thing more compactly using matrices: However, it seems that the more obvious way to do it would be: Why are matrices written in a seemingly more complex way than the alternative? What's the advantage of doing so?","
\begin{bmatrix}
3 & 4 & 5\\ 
2 & -3 & 6\\
2 & 5 & -9\\ 
\end{bmatrix}
\begin{bmatrix}x\\
y\\ 
z\\
\end{bmatrix} = \begin{bmatrix}7\\
6\\ 
11\\ 
\end{bmatrix}
 
\begin{array}{ll}
\begin{bmatrix}
x & y & -z\\ 
\end{bmatrix}
\\
\begin{bmatrix}
3 & 4 & 5\\ 
2 & -3 & 6\\
2 & 5 & -9\\ 
\end{bmatrix} = \begin{bmatrix}7\\
6\\ 
11\\ 
\end{bmatrix}
\end{array} 
","['linear-algebra', 'matrices', 'analysis', 'systems-of-equations', 'intuition']"
16,Null space basis,Null space basis,,"Let $V\in\mathbb{R}^{a\times b}$   be a matrix such that it is not a full column rank. Then there will be a nonsingular matrix $H$ such that $$VH=\left[\begin{array}{cc} V_{1} & 0_{a\times q}\end{array}\right]$$  where $q$ is the nullity of $V$. Now let $W\in\mathbb{R}^{b\times b}$ be some arbitrary square matrix such that $\left[\begin{array}{c} V\\ W \end{array}\right]$ is full column rank. Then $$\left[\begin{array}{c} V\\ W \end{array}\right]H=\left[\begin{array}{cc} V_{1} & 0_{a\times q}\\ W_{1} & W_{2} \end{array}\right]$$ where $W_{2}$ is full column rank. Do the columns of $W_{2}$ form the basis for the null space of the matrix $V$? If yes, how do we prove that? If no, what do the columns of $W_2$ signify?","Let $V\in\mathbb{R}^{a\times b}$   be a matrix such that it is not a full column rank. Then there will be a nonsingular matrix $H$ such that $$VH=\left[\begin{array}{cc} V_{1} & 0_{a\times q}\end{array}\right]$$  where $q$ is the nullity of $V$. Now let $W\in\mathbb{R}^{b\times b}$ be some arbitrary square matrix such that $\left[\begin{array}{c} V\\ W \end{array}\right]$ is full column rank. Then $$\left[\begin{array}{c} V\\ W \end{array}\right]H=\left[\begin{array}{cc} V_{1} & 0_{a\times q}\\ W_{1} & W_{2} \end{array}\right]$$ where $W_{2}$ is full column rank. Do the columns of $W_{2}$ form the basis for the null space of the matrix $V$? If yes, how do we prove that? If no, what do the columns of $W_2$ signify?",,"['linear-algebra', 'matrices', 'vector-spaces', 'matrix-equations', 'block-matrices']"
17,Maximal dimension and diagonalisable matrices,Maximal dimension and diagonalisable matrices,,"What is the maximmal dimension of a vector subspace of $\mathcal{M}_n(\Bbb{R})$ formed by diagonalisable matrices $\mathcal{D}_n(\Bbb{R})$? Attempt : Let $\mathcal{S}_n(\Bbb{R})$ the set of symmetric matrices, wich is a subspace of $\mathcal{D}_n(\Bbb{R})$ of dimension $\frac{n(n+1)}{2}$ and denote $\mathcal{T}_n(\Bbb{R})$ the set of upper triangular matrice with zero diagonal , wich is a subspace of $\mathcal{D}_n(\Bbb{R})$ of dimension $\frac{n(n-1)}{2}$. Then $\mathcal{S}_n(\Bbb{R})$ and $\mathcal{M}_n(\Bbb{R})$ are in direct sum. How can I continue ? NB: I am also curious if we replace $\Bbb{R}$ by $\Bbb{C}$ ?","What is the maximmal dimension of a vector subspace of $\mathcal{M}_n(\Bbb{R})$ formed by diagonalisable matrices $\mathcal{D}_n(\Bbb{R})$? Attempt : Let $\mathcal{S}_n(\Bbb{R})$ the set of symmetric matrices, wich is a subspace of $\mathcal{D}_n(\Bbb{R})$ of dimension $\frac{n(n+1)}{2}$ and denote $\mathcal{T}_n(\Bbb{R})$ the set of upper triangular matrice with zero diagonal , wich is a subspace of $\mathcal{D}_n(\Bbb{R})$ of dimension $\frac{n(n-1)}{2}$. Then $\mathcal{S}_n(\Bbb{R})$ and $\mathcal{M}_n(\Bbb{R})$ are in direct sum. How can I continue ? NB: I am also curious if we replace $\Bbb{R}$ by $\Bbb{C}$ ?",,['linear-algebra']
18,Determinant and trace as conjugations?,Determinant and trace as conjugations?,,"For real matrices $A$ it holds that $$\det\,\big(e^A\big)=e^{\mathrm{tr}\,A}$$ so we can write $$\mathrm{tr}=(\exp)^{-1}\circ \;\det\;\circ\;(\exp).$$ Is this interpretation of trace as the ""conjugate"" of the determinant under the exponential map used anywhere, or useful for anything? It seems neat but I have not come across it before. Edit: note that the $(\exp)$ on the left (of which an inverse is taken) is the natural logarithm $\ln$ for real numbers, not for matrices (I think!) because it is applied after finding the determinant.","For real matrices $A$ it holds that $$\det\,\big(e^A\big)=e^{\mathrm{tr}\,A}$$ so we can write $$\mathrm{tr}=(\exp)^{-1}\circ \;\det\;\circ\;(\exp).$$ Is this interpretation of trace as the ""conjugate"" of the determinant under the exponential map used anywhere, or useful for anything? It seems neat but I have not come across it before. Edit: note that the $(\exp)$ on the left (of which an inverse is taken) is the natural logarithm $\ln$ for real numbers, not for matrices (I think!) because it is applied after finding the determinant.",,"['linear-algebra', 'matrices']"
19,Rotation matrices are similar if and only if their angles add up to 2 pi,Rotation matrices are similar if and only if their angles add up to 2 pi,,"Let $\theta_0, \theta_1 \in [0, 2\pi)$ and $\theta_0 \ne \theta_1$. Consider the rotation matrices $$M_0 = \left[ \begin{matrix}\cos(\theta_0) & -\sin(\theta_0) \\ \sin(\theta_0) & \cos(\theta_0) \\ \end{matrix} \right],M_1 = \left[ \begin{matrix}\cos(\theta_1) & -\sin(\theta_1) \\ \sin(\theta_1) & \cos(\theta_1) \\ \end{matrix} \right] \in SO(2). $$ Prove that $M_0$ and $M_1$ are similar if and only if $\theta_0+\theta_1=2\pi$. I think I've proved the $""<=""$ direction. Using that $\sin(2\pi-\theta_0) = -\sin(\theta_0)$ and $\cos(2\pi-\theta_0) = \cos(\theta_0)$; and with $P:= \left[ \begin{matrix} 1&0\\0&-1  \end{matrix}\right]=P^{-1} $ I have found that $$P^{-1}M_0P = M_1,$$so that $M_0$ and $M_1$ are similar. However, for the $""=>""$ direction, I feel like I am missing something. What conclusions can we draw from the fact that $M_0$ and $M_1$ are similar? A hint would be much appreciated.","Let $\theta_0, \theta_1 \in [0, 2\pi)$ and $\theta_0 \ne \theta_1$. Consider the rotation matrices $$M_0 = \left[ \begin{matrix}\cos(\theta_0) & -\sin(\theta_0) \\ \sin(\theta_0) & \cos(\theta_0) \\ \end{matrix} \right],M_1 = \left[ \begin{matrix}\cos(\theta_1) & -\sin(\theta_1) \\ \sin(\theta_1) & \cos(\theta_1) \\ \end{matrix} \right] \in SO(2). $$ Prove that $M_0$ and $M_1$ are similar if and only if $\theta_0+\theta_1=2\pi$. I think I've proved the $""<=""$ direction. Using that $\sin(2\pi-\theta_0) = -\sin(\theta_0)$ and $\cos(2\pi-\theta_0) = \cos(\theta_0)$; and with $P:= \left[ \begin{matrix} 1&0\\0&-1  \end{matrix}\right]=P^{-1} $ I have found that $$P^{-1}M_0P = M_1,$$so that $M_0$ and $M_1$ are similar. However, for the $""=>""$ direction, I feel like I am missing something. What conclusions can we draw from the fact that $M_0$ and $M_1$ are similar? A hint would be much appreciated.",,['matrices']
20,Swapping rows or columns of Toeplitz matrix changes sign of one eigenvalue,Swapping rows or columns of Toeplitz matrix changes sign of one eigenvalue,,"Given some arbitrary Toeplitz matrix, if I swap two rows, one of the eigenvalues change its sign. For example, $$X = \begin{bmatrix} A & B & C \\ D & A & B \\ E & D & A \end{bmatrix}$$ and $$Y = \begin{bmatrix} D & A & B \\ A & B & C \\ E & D & A \end{bmatrix}$$ have the same eigenvalues up to sign. I can see this for small examples, but how would I go about proving this? In particular, if I flip the matrix upside down or left-side-right, half (rounding down) of the eigenvalues flip sign, and the singular values are the same. (It becomes a Hankel matrix.) Numerical demonstration with MATLAB code: d = 5; X = toeplitz(randn(d,1));    -0.8655   -0.1765    0.7914   -1.3320   -2.3299    -0.1765   -0.8655   -0.1765    0.7914   -1.3320     0.7914   -0.1765   -0.8655   -0.1765    0.7914    -1.3320    0.7914   -0.1765   -0.8655   -0.1765    -2.3299   -1.3320    0.7914   -0.1765   -0.8655  J = flipud(eye(d));    0     0     0     0     1    0     0     0     1     0    0     0     1     0     0    0     1     0     0     0    1     0     0     0     0  svd(X)'     4.0897    2.0381    1.8456    0.8649    0.8198  svd(X*J)'     4.0897    2.0381    1.8456    0.8649    0.8198  eig(X)'    -4.0897   -2.0381   -0.8649    0.8198    1.8456  eig(X*J)'    -4.0897   -1.8456   -0.8649    0.8198    2.0381  eig(J*X)'    -4.0897   -1.8456   -0.8649    0.8198    2.0381 EDIT: non-symmetric toeplitz example","Given some arbitrary Toeplitz matrix, if I swap two rows, one of the eigenvalues change its sign. For example, and have the same eigenvalues up to sign. I can see this for small examples, but how would I go about proving this? In particular, if I flip the matrix upside down or left-side-right, half (rounding down) of the eigenvalues flip sign, and the singular values are the same. (It becomes a Hankel matrix.) Numerical demonstration with MATLAB code: d = 5; X = toeplitz(randn(d,1));    -0.8655   -0.1765    0.7914   -1.3320   -2.3299    -0.1765   -0.8655   -0.1765    0.7914   -1.3320     0.7914   -0.1765   -0.8655   -0.1765    0.7914    -1.3320    0.7914   -0.1765   -0.8655   -0.1765    -2.3299   -1.3320    0.7914   -0.1765   -0.8655  J = flipud(eye(d));    0     0     0     0     1    0     0     0     1     0    0     0     1     0     0    0     1     0     0     0    1     0     0     0     0  svd(X)'     4.0897    2.0381    1.8456    0.8649    0.8198  svd(X*J)'     4.0897    2.0381    1.8456    0.8649    0.8198  eig(X)'    -4.0897   -2.0381   -0.8649    0.8198    1.8456  eig(X*J)'    -4.0897   -1.8456   -0.8649    0.8198    2.0381  eig(J*X)'    -4.0897   -1.8456   -0.8649    0.8198    2.0381 EDIT: non-symmetric toeplitz example","X =
\begin{bmatrix}
A & B & C \\
D & A & B \\
E & D & A
\end{bmatrix} Y = \begin{bmatrix}
D & A & B \\
A & B & C \\
E & D & A
\end{bmatrix}","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'toeplitz-matrices']"
21,Prove that matrices have equal rank.,Prove that matrices have equal rank.,,"If $P$ and $Q$ are $n \times n$ matrices of real numbers such that $P^2=P$ and $Q^2=Q$ and $I-P-Q$ is invertible where $I$ is an $n \times n$ identity matrix, Show that $P$ and $Q$ have the same rank. If $P$ is non-singular, then it can be shown that $P=Q=I$, so they have same rank. But I can't prove it when $P$ is singular.","If $P$ and $Q$ are $n \times n$ matrices of real numbers such that $P^2=P$ and $Q^2=Q$ and $I-P-Q$ is invertible where $I$ is an $n \times n$ identity matrix, Show that $P$ and $Q$ have the same rank. If $P$ is non-singular, then it can be shown that $P=Q=I$, so they have same rank. But I can't prove it when $P$ is singular.",,"['linear-algebra', 'matrices']"
22,Real world situation with System of Equation with 3 variables?,Real world situation with System of Equation with 3 variables?,,"Where do you run into a real world situation involving 3 variables and 3 equations?  Can someone think of a specific example from business, etc?  I recall taking an operations research course that seemed to involve optimization of 3 variables, but do not recall a single example or theme.  Any help is appreciated.","Where do you run into a real world situation involving 3 variables and 3 equations?  Can someone think of a specific example from business, etc?  I recall taking an operations research course that seemed to involve optimization of 3 variables, but do not recall a single example or theme.  Any help is appreciated.",,"['calculus', 'algebra-precalculus', 'matrices', 'systems-of-equations', 'applications']"
23,Online tools for generating the NULL SPACE of the matrix over Finite Field of size 2,Online tools for generating the NULL SPACE of the matrix over Finite Field of size 2,,"Is there any online tool where I just enter the values in (0,1) Finite Field of size 2 and it's give me the NULL SPACE matrix ? I have 25x25 , 36x36 , 25x36 , 36x25 matrix. Below is my 25 x 25 matrix 1   1   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0    1   1   1   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0    0   1   1   1   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0    0   0   1   1   1   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0    0   0   0   1   1   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0    1   0   0   0   0   1   1   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0    0   1   0   0   0   1   1   1   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0    0   0   1   0   0   0   1   1   1   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0    0   0   0   1   0   0   0   1   1   1   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0    0   0   0   0   1   0   0   0   1   1   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0    0   0   0   0   0   1   0   0   0   0   1   1   0   0   0   1   0   0   0   0   0   0   0   0   0    0   0   0   0   0   0   1   0   0   0   1   1   1   0   0   0   1   0   0   0   0   0   0   0   0    0   0   0   0   0   0   0   1   0   0   0   1   1   1   0   0   0   1   0   0   0   0   0   0   0    0   0   0   0   0   0   0   0   1   0   0   0   1   1   1   0   0   0   1   0   0   0   0   0   0    0   0   0   0   0   0   0   0   0   1   0   0   0   1   1   0   0   0   0   1   0   0   0   0   0    0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   1   1   0   0   0   1   0   0   0   0    0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   1   1   1   0   0   0   1   0   0   0    0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   1   1   1   0   0   0   1   0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   1   1   1   0   0   0   1   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   1   1   0   0   0   0   1    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   1   1   0   0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   1   1   1   0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   1   1   1   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   1   1   1    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   1   1 Below is my 36 x 36 matrix. 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1","Is there any online tool where I just enter the values in (0,1) Finite Field of size 2 and it's give me the NULL SPACE matrix ? I have 25x25 , 36x36 , 25x36 , 36x25 matrix. Below is my 25 x 25 matrix 1   1   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0    1   1   1   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0    0   1   1   1   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0    0   0   1   1   1   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0    0   0   0   1   1   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0    1   0   0   0   0   1   1   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0    0   1   0   0   0   1   1   1   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0    0   0   1   0   0   0   1   1   1   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0    0   0   0   1   0   0   0   1   1   1   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0    0   0   0   0   1   0   0   0   1   1   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0    0   0   0   0   0   1   0   0   0   0   1   1   0   0   0   1   0   0   0   0   0   0   0   0   0    0   0   0   0   0   0   1   0   0   0   1   1   1   0   0   0   1   0   0   0   0   0   0   0   0    0   0   0   0   0   0   0   1   0   0   0   1   1   1   0   0   0   1   0   0   0   0   0   0   0    0   0   0   0   0   0   0   0   1   0   0   0   1   1   1   0   0   0   1   0   0   0   0   0   0    0   0   0   0   0   0   0   0   0   1   0   0   0   1   1   0   0   0   0   1   0   0   0   0   0    0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   1   1   0   0   0   1   0   0   0   0    0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   1   1   1   0   0   0   1   0   0   0    0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   1   1   1   0   0   0   1   0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   1   1   1   0   0   0   1   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   1   1   0   0   0   0   1    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   1   1   0   0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   1   1   1   0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   1   1   1   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   1   1   1    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   1   1 Below is my 36 x 36 matrix. 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1",,"['linear-algebra', 'matrices', 'online-resources', 'matrix-equations']"
24,Distance or Similarity between matrices that are not the same size,Distance or Similarity between matrices that are not the same size,,"I have many matrices that have different size. Specifically, those matrices have the same number of rows but vary in the number of column. In another word, I have matrices $A_1,\dots,A_n$ where $A_i\in R^{n*k}$, $k$ is a constant, $n \in [\min,\max], \min, \max \in N^+$ and  $\max>\min$. Is there any method to calculate the distance or similarity among those matrices? What are the advantages and disadvantages of those methods? Or just give me a hint where to find the reference to learn. I think I could take each row as a vector and calculate the cosine similarity of 2 vectors that come from 2 different matrices. It's kind of like distance matrix. But I discard this way because I think this way split my matrix and I want my matrix to be an entire entity that can be applied to similarity calculation. Thank you all.","I have many matrices that have different size. Specifically, those matrices have the same number of rows but vary in the number of column. In another word, I have matrices $A_1,\dots,A_n$ where $A_i\in R^{n*k}$, $k$ is a constant, $n \in [\min,\max], \min, \max \in N^+$ and  $\max>\min$. Is there any method to calculate the distance or similarity among those matrices? What are the advantages and disadvantages of those methods? Or just give me a hint where to find the reference to learn. I think I could take each row as a vector and calculate the cosine similarity of 2 vectors that come from 2 different matrices. It's kind of like distance matrix. But I discard this way because I think this way split my matrix and I want my matrix to be an entire entity that can be applied to similarity calculation. Thank you all.",,"['linear-algebra', 'matrices']"
25,Show that if $\operatorname{tr}(A+B) > \operatorname{tr}(A)$ then $\operatorname{tr}((A+B)^k)\geq \operatorname{tr}(A^k)$ for any $k\geq 1$,Show that if  then  for any,\operatorname{tr}(A+B) > \operatorname{tr}(A) \operatorname{tr}((A+B)^k)\geq \operatorname{tr}(A^k) k\geq 1,"This may be a stupid question, but I am completely stuck, I don't even know where to start. I have to show that if $\operatorname{tr}(A+B) > \operatorname{tr}(A)$ then $\operatorname{tr}((A+B)^k)\geq \operatorname{tr}(A^k)$ for any $k\geq 1$ , where $A$ and $B$ are square symmetric matrices with real entries. Any help is welcome, thank you very much in advance In response to Calvin's answer, I checked the condition again, in fact, the first inequality is a strict one. Furthermore, $A$ is positive semi-definite. Sorry for my mistake.","This may be a stupid question, but I am completely stuck, I don't even know where to start. I have to show that if then for any , where and are square symmetric matrices with real entries. Any help is welcome, thank you very much in advance In response to Calvin's answer, I checked the condition again, in fact, the first inequality is a strict one. Furthermore, is positive semi-definite. Sorry for my mistake.",\operatorname{tr}(A+B) > \operatorname{tr}(A) \operatorname{tr}((A+B)^k)\geq \operatorname{tr}(A^k) k\geq 1 A B A,"['matrices', 'summation']"
26,Determinant of 4x4 Matrix by Expansion Method,Determinant of 4x4 Matrix by Expansion Method,,"Find det(B) = \begin{bmatrix} 2 & 5 & -3 & -2 \\ -2 & -3 & 2 & -5 \\ 1 & 3 & -2 & 0 \\ -1 & -6 & 4 & 0 \\ \end{bmatrix} I chose the 4th column because it has the most 0s. Using basketweave, I solved for the determinants of the minor 3x3 matrices of entry B 14 and B 24 . det(B 14 ) = \begin{bmatrix} -2 & -3 & 2 \\ 1 & 3 & -2 \\ -1 & -6 & 4 \\ \end{bmatrix} det(B 14 ) = (-24 - 6 - 12) - (-12 - 24 - 6) = -42 - (-42) = 0 det(B 24 ) = \begin{bmatrix} 2 & 5 & -3 \\ 1 & 3 & -2 \\ -1 & -6 & 4 \\ \end{bmatrix} det(B 24 ) = (24 + 10 + 18) - (20 + 24 + 9) = 52 - 53 = -1 I have checked with a matrix calculator and the the determinants of the 3x3 minor matrices are correct. To find the det(B), I multiplied B 14 by det(B 14 ) and B 24 by det(B 24 ) and followed the + - + - pattern as showed by the formula here (scroll below for 4x4 formula). The rest will be 0s anyway. det(B) = [-2(0)] - [-5(-1)] + [0] - [0] = -5 Checking again with the matrix calculator, the correct answer is +5 . I am confused as to how the signs apply. How did det(B) arrive to +5?","Find det(B) = \begin{bmatrix} 2 & 5 & -3 & -2 \\ -2 & -3 & 2 & -5 \\ 1 & 3 & -2 & 0 \\ -1 & -6 & 4 & 0 \\ \end{bmatrix} I chose the 4th column because it has the most 0s. Using basketweave, I solved for the determinants of the minor 3x3 matrices of entry B 14 and B 24 . det(B 14 ) = \begin{bmatrix} -2 & -3 & 2 \\ 1 & 3 & -2 \\ -1 & -6 & 4 \\ \end{bmatrix} det(B 14 ) = (-24 - 6 - 12) - (-12 - 24 - 6) = -42 - (-42) = 0 det(B 24 ) = \begin{bmatrix} 2 & 5 & -3 \\ 1 & 3 & -2 \\ -1 & -6 & 4 \\ \end{bmatrix} det(B 24 ) = (24 + 10 + 18) - (20 + 24 + 9) = 52 - 53 = -1 I have checked with a matrix calculator and the the determinants of the 3x3 minor matrices are correct. To find the det(B), I multiplied B 14 by det(B 14 ) and B 24 by det(B 24 ) and followed the + - + - pattern as showed by the formula here (scroll below for 4x4 formula). The rest will be 0s anyway. det(B) = [-2(0)] - [-5(-1)] + [0] - [0] = -5 Checking again with the matrix calculator, the correct answer is +5 . I am confused as to how the signs apply. How did det(B) arrive to +5?",,"['linear-algebra', 'matrices', 'determinant', 'laplace-expansion']"
27,Which of the following staements are true (NBHM-$2014$),Which of the following staements are true (NBHM-),2014,"Let $A \in \mathrm{GL}\,_n(\mathbb R)$ have integer entries. Let  $b \in \mathbb R^n $ be a column vector also with integer entries. Then If $Ax = b$ , then entries of $x$ are also integers. if $Ax = b$ , then the entries of $x$ are rational. The matrix $A^{-1}$  has integer entries  iff  $\det(A) = \pm 1$. For (1)  is false  by Cramer's Rule. For (2)  is true by Cramer Rule. For (3), I think it is also true. Thank you for sparing your valuable time in checking my solutions","Let $A \in \mathrm{GL}\,_n(\mathbb R)$ have integer entries. Let  $b \in \mathbb R^n $ be a column vector also with integer entries. Then If $Ax = b$ , then entries of $x$ are also integers. if $Ax = b$ , then the entries of $x$ are rational. The matrix $A^{-1}$  has integer entries  iff  $\det(A) = \pm 1$. For (1)  is false  by Cramer's Rule. For (2)  is true by Cramer Rule. For (3), I think it is also true. Thank you for sparing your valuable time in checking my solutions",,['matrices']
28,Prove this matrix is invertible for $n < m-1$,Prove this matrix is invertible for,n < m-1,"Prove this $(n+1)\times (n+1)$ matrix $\bf{A}$ is invertible for $n < m-1$ and the $x_k$ distinct, \begin{bmatrix} m &\sum_{k=1}^mx_k  &\sum_{k=1}^mx_k^2  &\cdots &\sum_{k=1}^mx_k^n \\\\  \sum_{k=1}^mx_k &\sum_{k=1}^mx_k^2  & \cdots &\cdots&\sum_{k=1}^mx_k^{n+1} \\\\  \vdots &\vdots  &\ddots  &&\vdots \\   &  &  & &&\\\\ \sum_{k=1}^mx_k^n&\sum_{k=1}^mx_k^{n+1}&\cdots&\cdots&\sum_{k=1}^mx_k^{2n} \end{bmatrix} I'm sure many of you recognize this as the normal matrix for polynomial least squares. The hint in the book is as follows: Suppose $\bf{A}$ is singular and that $\bf{c}\neq\bf{0}$ is such that   $\bf{c}$$^\text{T}$$\bf{Ac}$$\;=0$.  Show that the $n$th-degree   polynomial whose coefficients are the coordinates of $\bf{c}$ has more   than $n$ roots, and use this to establish a contradiction. I worked on this for quite a while to no avail.  First off I can't figure out why the matrix is multiplied on the left by the transpose of $\bf{c}$, I'm assuming $\bf{c}$ is chosen because it's a non-trivial element in the kernel, but that doesn't require its transpose on the left. Second, things get pretty complicated once you start messing with the series, so I was hoping for something that avoided messy computations.  It's the product of the Vandermonde matrix and its transpose, but they're not square so that seemed useless. The closest I could get to even relating this to the polynomial $c_0+c_1t+...+c_nt^n$ was to consider the rows of the vector $\bf{A\cdot c}$.  By that I mean consider the first row (set equal to zero), then dividing by $m$ we get: $$c_0 + c_1\frac{\sum_{k=1}^mx_k}{m}+...+c_n\frac{\sum_{k=1}^mx_k^n}{m}=0.$$ If the various $\frac{\sum_{k=1}^mx_k^r}{m}$ were powers of $\frac{\sum_{k=1}^mx_k}{m}$, then that would be a root, but they're clearly not.  Or instead of dividing by $m$ you could split it into $m>n+1$ expressions of the form $$c_0 + c_1x_k + ... + c_nx_k^n\; ,$$ such that their sum is equal to zero, but that certainly doesn't imply they all have to be zero individually. Anyways as you can see I'm stumped. **Also note that while $n=m-1$ will most of the time be invertible, if one of the data points is zero, then it won't be.","Prove this $(n+1)\times (n+1)$ matrix $\bf{A}$ is invertible for $n < m-1$ and the $x_k$ distinct, \begin{bmatrix} m &\sum_{k=1}^mx_k  &\sum_{k=1}^mx_k^2  &\cdots &\sum_{k=1}^mx_k^n \\\\  \sum_{k=1}^mx_k &\sum_{k=1}^mx_k^2  & \cdots &\cdots&\sum_{k=1}^mx_k^{n+1} \\\\  \vdots &\vdots  &\ddots  &&\vdots \\   &  &  & &&\\\\ \sum_{k=1}^mx_k^n&\sum_{k=1}^mx_k^{n+1}&\cdots&\cdots&\sum_{k=1}^mx_k^{2n} \end{bmatrix} I'm sure many of you recognize this as the normal matrix for polynomial least squares. The hint in the book is as follows: Suppose $\bf{A}$ is singular and that $\bf{c}\neq\bf{0}$ is such that   $\bf{c}$$^\text{T}$$\bf{Ac}$$\;=0$.  Show that the $n$th-degree   polynomial whose coefficients are the coordinates of $\bf{c}$ has more   than $n$ roots, and use this to establish a contradiction. I worked on this for quite a while to no avail.  First off I can't figure out why the matrix is multiplied on the left by the transpose of $\bf{c}$, I'm assuming $\bf{c}$ is chosen because it's a non-trivial element in the kernel, but that doesn't require its transpose on the left. Second, things get pretty complicated once you start messing with the series, so I was hoping for something that avoided messy computations.  It's the product of the Vandermonde matrix and its transpose, but they're not square so that seemed useless. The closest I could get to even relating this to the polynomial $c_0+c_1t+...+c_nt^n$ was to consider the rows of the vector $\bf{A\cdot c}$.  By that I mean consider the first row (set equal to zero), then dividing by $m$ we get: $$c_0 + c_1\frac{\sum_{k=1}^mx_k}{m}+...+c_n\frac{\sum_{k=1}^mx_k^n}{m}=0.$$ If the various $\frac{\sum_{k=1}^mx_k^r}{m}$ were powers of $\frac{\sum_{k=1}^mx_k}{m}$, then that would be a root, but they're clearly not.  Or instead of dividing by $m$ you could split it into $m>n+1$ expressions of the form $$c_0 + c_1x_k + ... + c_nx_k^n\; ,$$ such that their sum is equal to zero, but that certainly doesn't imply they all have to be zero individually. Anyways as you can see I'm stumped. **Also note that while $n=m-1$ will most of the time be invertible, if one of the data points is zero, then it won't be.",,"['linear-algebra', 'matrices', 'numerical-linear-algebra', 'least-squares']"
29,"The dimension of the subvariety of matrices of rank 3 in M(n, m)","The dimension of the subvariety of matrices of rank 3 in M(n, m)",,"Consider the space $M(m, n)$ of matrices of size $m \times n$ over field $K$. Let $X \subset M(m, n)$ be the subset of matrices of rank $3$. Show that $X$ is an algebraic subvariety of $M(m, n)$. Compute the dimension of the variety $X$.","Consider the space $M(m, n)$ of matrices of size $m \times n$ over field $K$. Let $X \subset M(m, n)$ be the subset of matrices of rank $3$. Show that $X$ is an algebraic subvariety of $M(m, n)$. Compute the dimension of the variety $X$.",,"['matrices', 'algebraic-geometry']"
30,Jordan's decomposition,Jordan's decomposition,,"I have a matrix $A\in R^{n,n}$. $A= \begin{bmatrix} 1&0&-2&0&0&\dots&0\\ 0&1&0&-6&0&\dots&0\\ 0&0&1&0&-12&\dots&0\\ \vdots&\vdots&\vdots&\ddots&\ddots&\ddots&\vdots\\ 0&0&0&0&1&0&-(n-2)(n-1)\\ 0&0&0&0&0&1&0\\ 0&0&0&0&0&0&1 \end{bmatrix}$ $A$ is a matrix with $1$ on diagonal and two spots to the right is $-i(i+1)$, where $i$ is the line number. It has a characteristic polynomial $p_A(t)=(1-t)^n$. So jordan's form of this matrix has $1$ on diagonal, and $0$ or $1$ above it. EDIT: $J= \begin{bmatrix} J_1 & 0 & 0 &\dots& 0\\ 0&J_2&0&\dots&0\\ \vdots & \vdots &\ddots & \dots &0\\ 0&0&0&\dots&J_n \end{bmatrix}$ Where  $J_i= \begin{bmatrix} 1 & 1 & 0&0 & \dots & 0\\ 0 &1 &1&0&\dots&0\\ 0&0&1&1&\dots&0\\ \vdots&\vdots&\dots&\ddots&\ddots&\vdots\\ 0&0&0&0&1&1\\ 0&0&0&0&0&1 \end{bmatrix}$ How do I know how many blocks $J_i$(and of what size) are used in $J$ matrix?","I have a matrix $A\in R^{n,n}$. $A= \begin{bmatrix} 1&0&-2&0&0&\dots&0\\ 0&1&0&-6&0&\dots&0\\ 0&0&1&0&-12&\dots&0\\ \vdots&\vdots&\vdots&\ddots&\ddots&\ddots&\vdots\\ 0&0&0&0&1&0&-(n-2)(n-1)\\ 0&0&0&0&0&1&0\\ 0&0&0&0&0&0&1 \end{bmatrix}$ $A$ is a matrix with $1$ on diagonal and two spots to the right is $-i(i+1)$, where $i$ is the line number. It has a characteristic polynomial $p_A(t)=(1-t)^n$. So jordan's form of this matrix has $1$ on diagonal, and $0$ or $1$ above it. EDIT: $J= \begin{bmatrix} J_1 & 0 & 0 &\dots& 0\\ 0&J_2&0&\dots&0\\ \vdots & \vdots &\ddots & \dots &0\\ 0&0&0&\dots&J_n \end{bmatrix}$ Where  $J_i= \begin{bmatrix} 1 & 1 & 0&0 & \dots & 0\\ 0 &1 &1&0&\dots&0\\ 0&0&1&1&\dots&0\\ \vdots&\vdots&\dots&\ddots&\ddots&\vdots\\ 0&0&0&0&1&1\\ 0&0&0&0&0&1 \end{bmatrix}$ How do I know how many blocks $J_i$(and of what size) are used in $J$ matrix?",,"['linear-algebra', 'matrices', 'jordan-normal-form']"
31,Question about Jordan base - Why is finding the kernel and finding the generelized eigenvectors the same thing?,Question about Jordan base - Why is finding the kernel and finding the generelized eigenvectors the same thing?,,"Quick question, perhaps a bit general but I don't understand something. When finding a jordan basis for a matrix $A$ , what i do is for each eigenvalue $\lambda$ I find vectors such that: $(A-\lambda I)v_1=\lambda v_1$ $(A-\lambda I)v_2=v_1$ $(A-\lambda I)v_3=v_2$ and so on...until $(A-\lambda I)v_{r-1}=v_r$ where $r$ is the algebraic multiplicity of $\lambda$ in the characteristic polynomial. But I've seen people do it differently. I've seen people finding jordan basis by finding: $\operatorname{ker}(A-\lambda I), \operatorname{ker}(A-\lambda I)^2,\ldots,\operatorname{ker}(A-\lambda I)^r$ , why is this the same thing? Why are both methods working? Thank you.","Quick question, perhaps a bit general but I don't understand something. When finding a jordan basis for a matrix , what i do is for each eigenvalue I find vectors such that: and so on...until where is the algebraic multiplicity of in the characteristic polynomial. But I've seen people do it differently. I've seen people finding jordan basis by finding: , why is this the same thing? Why are both methods working? Thank you.","A \lambda (A-\lambda I)v_1=\lambda v_1 (A-\lambda I)v_2=v_1 (A-\lambda I)v_3=v_2 (A-\lambda I)v_{r-1}=v_r r \lambda \operatorname{ker}(A-\lambda I), \operatorname{ker}(A-\lambda I)^2,\ldots,\operatorname{ker}(A-\lambda I)^r","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'jordan-normal-form']"
32,Jordan Normal Form,Jordan Normal Form,,I'm asked to find the Jordan Normal form of $A\in M_5(\mathbb{C}^{5x5})$  with the characteristic polynomial: $p(A)=(\lambda-1)^3(\lambda+1)^2$ and minimum polynomial $m(A)=(\lambda-1)^2(\lambda+1)$ I got so far: $$m_A(x)=(x-1)^2(x+1)\;\;\;:\;\;\;\;\begin{pmatrix}1&1&0&0&0\\ 0&1&0&0&0\\ 0&0&1&0&0\\ 0&0&0&-1&0\\ 0&0&0&0&-1\\ \end{pmatrix}$$ The above matrix i got it by substitution of $m(A)$ in a $A\in M_5(\mathbb{C}^{5x5})$ matrix. And i think it's correct(if it isn't please let me know) but i don't know why does it works? Can you please give me a hint of why does this works?,I'm asked to find the Jordan Normal form of $A\in M_5(\mathbb{C}^{5x5})$  with the characteristic polynomial: $p(A)=(\lambda-1)^3(\lambda+1)^2$ and minimum polynomial $m(A)=(\lambda-1)^2(\lambda+1)$ I got so far: $$m_A(x)=(x-1)^2(x+1)\;\;\;:\;\;\;\;\begin{pmatrix}1&1&0&0&0\\ 0&1&0&0&0\\ 0&0&1&0&0\\ 0&0&0&-1&0\\ 0&0&0&0&-1\\ \end{pmatrix}$$ The above matrix i got it by substitution of $m(A)$ in a $A\in M_5(\mathbb{C}^{5x5})$ matrix. And i think it's correct(if it isn't please let me know) but i don't know why does it works? Can you please give me a hint of why does this works?,,"['linear-algebra', 'matrices', 'jordan-normal-form']"
33,Are there non-affine matrices?,Are there non-affine matrices?,,"Matrices are useful for proving statements like The ratio between  the areas of a parallelogram and the quadrilateral formed by joining their midpoints is $2$ . The ratio between the volumes of a parallelepiped and the cuboctahedron formed by joining its midpoints is $\frac{6}{5}$ . , because one can apply some matrix $T^{-1}$ to the vertices of the shapes to turn them into cubes/squares (i.e. via an affine transformation). Then, solve the problem in this now simpler case. As the ratios of areas are unchanged under $T$ acting on a shape, the conclusion holds in the general case. Now, is there a mathematical object, similar to matrices, that deals with non-affine transformations? For example, could similar method be used to solve the first problem for any arbitrary quadrilateral? The theory of non-affine transformations is probably going to be much more complex than in the affine case, as the ratios of areas are not preserved, with some areas ballooning out and some squashed (although I conjecture a non-affine transformation acts locally as an affine transformation). However, I ask in hope that there exist theorems that could help simplify the situation. I have heard of the shoelace theorem, but am not sure how generally it could be applied to solve general problems.","Matrices are useful for proving statements like The ratio between  the areas of a parallelogram and the quadrilateral formed by joining their midpoints is . The ratio between the volumes of a parallelepiped and the cuboctahedron formed by joining its midpoints is . , because one can apply some matrix to the vertices of the shapes to turn them into cubes/squares (i.e. via an affine transformation). Then, solve the problem in this now simpler case. As the ratios of areas are unchanged under acting on a shape, the conclusion holds in the general case. Now, is there a mathematical object, similar to matrices, that deals with non-affine transformations? For example, could similar method be used to solve the first problem for any arbitrary quadrilateral? The theory of non-affine transformations is probably going to be much more complex than in the affine case, as the ratios of areas are not preserved, with some areas ballooning out and some squashed (although I conjecture a non-affine transformation acts locally as an affine transformation). However, I ask in hope that there exist theorems that could help simplify the situation. I have heard of the shoelace theorem, but am not sure how generally it could be applied to solve general problems.",2 \frac{6}{5} T^{-1} T,"['geometry', 'matrices', 'transformation']"
34,How to construct the graph from an adjacency matrix?,How to construct the graph from an adjacency matrix?,,"I have the following adjacency matrix: a  b  c  d  a [0, 0, 1, 1] b [0, 0, 1, 0] c [1, 1, 0, 1] d [1, 1, 1, 0] How do I draw the graph, given its adjacency matrix above (I've added a,b,c,d to label vertices). I don't understand how the vertex $d$ (e.g., the row $d$) is adjacent to the vertex $b$, but the vertex $b$ (the row $b$) is not adjacent to the vertex $d$ (the column $d$). Is this possible? Thanks! EDIT:  Maybe it's directed?  If so, would that explain why d --> b, but b =/ d?","I have the following adjacency matrix: a  b  c  d  a [0, 0, 1, 1] b [0, 0, 1, 0] c [1, 1, 0, 1] d [1, 1, 1, 0] How do I draw the graph, given its adjacency matrix above (I've added a,b,c,d to label vertices). I don't understand how the vertex $d$ (e.g., the row $d$) is adjacent to the vertex $b$, but the vertex $b$ (the row $b$) is not adjacent to the vertex $d$ (the column $d$). Is this possible? Thanks! EDIT:  Maybe it's directed?  If so, would that explain why d --> b, but b =/ d?",,"['matrices', 'graph-theory']"
35,The eigenvalues of the product of a positive definite and a symmetric matrix.,The eigenvalues of the product of a positive definite and a symmetric matrix.,,"A fellow student posed the following question and I'd like to stop thinking about it so I can get back to work on my own research! Suppose that $A>0$, i.e. $A$ is a real symmetric positive definite matrix, and $B$ is a real symmetric nonsingular matrix. What can we say about the eigenvalues of $AB$?  For instance, suppose $B$ has $n$ positive and $m$ negative eigenvalues.  Will $AB$ have the same number of positive and negative eigenvalues? Obviously if $B$ is either positive or negative definite, the result is straightforward, i.e. we have the `matrix sign rules' $$ (+)\cdot (+)=(+)\qquad\text{and}\qquad (+)\cdot(-)=(-) $$ Whereby we mean `a positive definite times a positive definite has positive eigenvalues' and 'a positive definite times a negative definite has negative eigenvalues'. Playing around with matrix decompositions such as polar, spectral, etc, and identities such as $\{\lambda(AB)\}=\{\lambda(\sqrt{A}B\sqrt{A})\}$ (here $\lambda(\cdot)$ meaning `eigenvalues of') doesn't seem to lead to a quick result. Any ideas?","A fellow student posed the following question and I'd like to stop thinking about it so I can get back to work on my own research! Suppose that $A>0$, i.e. $A$ is a real symmetric positive definite matrix, and $B$ is a real symmetric nonsingular matrix. What can we say about the eigenvalues of $AB$?  For instance, suppose $B$ has $n$ positive and $m$ negative eigenvalues.  Will $AB$ have the same number of positive and negative eigenvalues? Obviously if $B$ is either positive or negative definite, the result is straightforward, i.e. we have the `matrix sign rules' $$ (+)\cdot (+)=(+)\qquad\text{and}\qquad (+)\cdot(-)=(-) $$ Whereby we mean `a positive definite times a positive definite has positive eigenvalues' and 'a positive definite times a negative definite has negative eigenvalues'. Playing around with matrix decompositions such as polar, spectral, etc, and identities such as $\{\lambda(AB)\}=\{\lambda(\sqrt{A}B\sqrt{A})\}$ (here $\lambda(\cdot)$ meaning `eigenvalues of') doesn't seem to lead to a quick result. Any ideas?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
36,minimal polynomial of a matrix with some unknown entries,minimal polynomial of a matrix with some unknown entries,,"Question is to prove that  : characteristic and minimal polynomial of $ \left( \begin{array}{cccc} 0 & 0 & c  \\ 1 & 0 & b \\ 0 & 1 & a \end{array} \right) $ is $x^3-ax^2-bx-c$. what i have done so far is : characteristic polynomial of a matrix $A$ is given by $\det(A-xI)$ in case of $A= \left( \begin{array}{cccc} 0 & 0 & c  \\ 1 & 0 & b \\ 0 & 1 & a \end{array} \right)$ we have $\det(A-xI)=\det\left( \begin{array}{cccc} -x & 0 & c  \\ 1 & -x & b \\ 0 & 1 & a-x \end{array} \right)=-(x^3-ax^2-bx-c)$ So, i have got the characteristic polynomial as $x^3-ax^2-bx-c$. Now, the problem is how do i find minimal polynomial. As $a,b,c$ are arbitrary, I can not factorize $x^3-ax^2-bx-c$ so as to see which factor gives me minimal polynomial. I am confused. please suggest me some hint. EDIT : This is just after Mr.Will Jagyy's hint : I have $A= \left( \begin{array}{cccc} 0 & 0 & c  \\ 1 & 0 & b \\ 0 & 1 & a \end{array} \right)$ then, $A^2= \left( \begin{array}{cccc} 0 & 0 & c  \\ 1 & 0 & b \\ 0 & 1 & a \end{array} \right)\left( \begin{array}{cccc} 0 & 0 & c  \\ 1 & 0 & b \\ 0 & 1 & a \end{array} \right)=\left( \begin{array}{cccc} 0 & c & ac  \\ 0 & b & c+ab \\ 1 & a & b+a^2 \end{array} \right)$ Now, $A^2+rA+sI=\left( \begin{array}{cccc} 0 & c & ac  \\ 0 & b & c+ab \\ 1 & a & b+a^2 \end{array} \right)+r\left( \begin{array}{cccc} 0 & 0 & c  \\ 1 & 0 & b \\ 0 & 1 & a \end{array} \right)+s\left( \begin{array}{cccc} 1 & 0 & 0  \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{array} \right)=\left( \begin{array}{cccc} s & c & *  \\ r & b+s & * \\ 1& * & * \end{array} \right)$ As element of $3^{rd}$ row $1^{st}$ column is $1$ in above matrix, this can never be $0$ i.e., $A^2+rA+sI$ can never be $0$. Now, $A+rI=\left( \begin{array}{cccc} 0 & 0 & c  \\ 1 & 0 & b \\ 0 & 1 & a \end{array} \right)+r\left( \begin{array}{cccc} 1 & 0 & 0  \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{array} \right)=\left( \begin{array}{cccc} r & *& * \\ \color{magenta}{1} & * & * \\ * & \color{magenta}{1} & * \end{array} \right)\neq 0$ if $r\neq 0$ Thus, $A^2+rA+sI\neq 0$ and $A+rI\neq 0$ for any $r,s$. Thus, minimal polynomial for $A$ can not be of order less than $3$. Thus, minimal polynomial for $A$ has to be $x^3-ax^2-bx-c$. I have written this just to make sure i have tried in correct way as i can not write this in a comment. I would be thankful if there is any other way to proceed further.. Thank you :)","Question is to prove that  : characteristic and minimal polynomial of $ \left( \begin{array}{cccc} 0 & 0 & c  \\ 1 & 0 & b \\ 0 & 1 & a \end{array} \right) $ is $x^3-ax^2-bx-c$. what i have done so far is : characteristic polynomial of a matrix $A$ is given by $\det(A-xI)$ in case of $A= \left( \begin{array}{cccc} 0 & 0 & c  \\ 1 & 0 & b \\ 0 & 1 & a \end{array} \right)$ we have $\det(A-xI)=\det\left( \begin{array}{cccc} -x & 0 & c  \\ 1 & -x & b \\ 0 & 1 & a-x \end{array} \right)=-(x^3-ax^2-bx-c)$ So, i have got the characteristic polynomial as $x^3-ax^2-bx-c$. Now, the problem is how do i find minimal polynomial. As $a,b,c$ are arbitrary, I can not factorize $x^3-ax^2-bx-c$ so as to see which factor gives me minimal polynomial. I am confused. please suggest me some hint. EDIT : This is just after Mr.Will Jagyy's hint : I have $A= \left( \begin{array}{cccc} 0 & 0 & c  \\ 1 & 0 & b \\ 0 & 1 & a \end{array} \right)$ then, $A^2= \left( \begin{array}{cccc} 0 & 0 & c  \\ 1 & 0 & b \\ 0 & 1 & a \end{array} \right)\left( \begin{array}{cccc} 0 & 0 & c  \\ 1 & 0 & b \\ 0 & 1 & a \end{array} \right)=\left( \begin{array}{cccc} 0 & c & ac  \\ 0 & b & c+ab \\ 1 & a & b+a^2 \end{array} \right)$ Now, $A^2+rA+sI=\left( \begin{array}{cccc} 0 & c & ac  \\ 0 & b & c+ab \\ 1 & a & b+a^2 \end{array} \right)+r\left( \begin{array}{cccc} 0 & 0 & c  \\ 1 & 0 & b \\ 0 & 1 & a \end{array} \right)+s\left( \begin{array}{cccc} 1 & 0 & 0  \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{array} \right)=\left( \begin{array}{cccc} s & c & *  \\ r & b+s & * \\ 1& * & * \end{array} \right)$ As element of $3^{rd}$ row $1^{st}$ column is $1$ in above matrix, this can never be $0$ i.e., $A^2+rA+sI$ can never be $0$. Now, $A+rI=\left( \begin{array}{cccc} 0 & 0 & c  \\ 1 & 0 & b \\ 0 & 1 & a \end{array} \right)+r\left( \begin{array}{cccc} 1 & 0 & 0  \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{array} \right)=\left( \begin{array}{cccc} r & *& * \\ \color{magenta}{1} & * & * \\ * & \color{magenta}{1} & * \end{array} \right)\neq 0$ if $r\neq 0$ Thus, $A^2+rA+sI\neq 0$ and $A+rI\neq 0$ for any $r,s$. Thus, minimal polynomial for $A$ can not be of order less than $3$. Thus, minimal polynomial for $A$ has to be $x^3-ax^2-bx-c$. I have written this just to make sure i have tried in correct way as i can not write this in a comment. I would be thankful if there is any other way to proceed further.. Thank you :)",,['linear-algebra']
37,Finding matrix norm equivalence constants,Finding matrix norm equivalence constants,,"I've been given the following: ""Find the best positive constants $\alpha$ and $\beta$ such that $\alpha\left\|A\right\|_2\leq\left\|A\right\|_1\leq\beta\left\|A\right\|_2$ for all $A\in\mathbb{R}^{m\times n}$. This is part b of a question along with ""Prove that $\left\|x\right\|_2\leq\left\|x\right\|_1\leq\sqrt{n}\left\|x\right\|_2$ for all $x\in\mathbb{R}^n$,"" which I was able to get after awhile by expanding out the first half and using Cauchy-Schwarz on the second, which would lead me to believe that at least one of those would be related to solving this. My book provides solutions for $\frac{1}{\sqrt{n}}\left\|A\right\|_\infty\leq\left\|A\right\|_2\leq\sqrt{m}\left\|A\right\|_\infty$ and $\left\|A\right\|_2\leq\left\|A\right\|_F\leq\sqrt{n}\left\|A\right\|_2$ but they use $\left\|x\right\|_2\leq\sqrt{n}\left\|x\right\|_\infty$ which I can see being true but isn't explicitly stated prior to this, and I don't think I can use without proving first. Would I simply be able to use the corresponding ratios from the first part to show this? I'm kinda lost to be honest. (Also, I'm taking ""best"" to mean closest-bounding, unless there's only one valid pair of them, though it remains somewhat ambiguous.)","I've been given the following: ""Find the best positive constants $\alpha$ and $\beta$ such that $\alpha\left\|A\right\|_2\leq\left\|A\right\|_1\leq\beta\left\|A\right\|_2$ for all $A\in\mathbb{R}^{m\times n}$. This is part b of a question along with ""Prove that $\left\|x\right\|_2\leq\left\|x\right\|_1\leq\sqrt{n}\left\|x\right\|_2$ for all $x\in\mathbb{R}^n$,"" which I was able to get after awhile by expanding out the first half and using Cauchy-Schwarz on the second, which would lead me to believe that at least one of those would be related to solving this. My book provides solutions for $\frac{1}{\sqrt{n}}\left\|A\right\|_\infty\leq\left\|A\right\|_2\leq\sqrt{m}\left\|A\right\|_\infty$ and $\left\|A\right\|_2\leq\left\|A\right\|_F\leq\sqrt{n}\left\|A\right\|_2$ but they use $\left\|x\right\|_2\leq\sqrt{n}\left\|x\right\|_\infty$ which I can see being true but isn't explicitly stated prior to this, and I don't think I can use without proving first. Would I simply be able to use the corresponding ratios from the first part to show this? I'm kinda lost to be honest. (Also, I'm taking ""best"" to mean closest-bounding, unless there's only one valid pair of them, though it remains somewhat ambiguous.)",,"['linear-algebra', 'matrices', 'normed-spaces']"
38,The Principle of Mathematical Induction,The Principle of Mathematical Induction,,"The question is Let $( F_0, F_1, F_2,... )$ be the Fibonacci sequence defined by $F_0=0,\, F_1=1, and F_{n+1}=F_n+F_{n-1}$, n greater than or equal to 1. Prove the following identities.  $$\begin{bmatrix} 1 & 1\\  1 & 0 \end{bmatrix}^n =\begin{bmatrix} F_{n+1} &F_n \\  F_n &F_{n-1}  \end{bmatrix}$$ This is what I tried. Proof:  Base case: If $n=1$ the formula says $$\begin{bmatrix} 1 & 1\\   1& 0 \end{bmatrix}^{1}=\begin{bmatrix} F_{1+1} & F_1\\  F_1 &F_0  \end{bmatrix}=\begin{bmatrix} 1+0 & 1\\  1 & 0 \end{bmatrix}=\begin{bmatrix} 1 & 1\\  1 & 0 \end{bmatrix}$$ which is true. Inductive Step: Suppose the formula holds for $n=k$ i.e. that $$\begin{bmatrix} 1 & 1\\   1& 0 \end{bmatrix}^{k}=\begin{bmatrix} F_{k+1} &F_k\\  F_k &F_{k-1}  \end{bmatrix}$$ is true. We have to show that the formula holds for $n=k+1$ that is $$\begin{bmatrix} 1 &1 \\   1&0  \end{bmatrix}^{k+1}= \begin{bmatrix} F_{k+2} & F_{k+1} \\   F_{k+1}&F_k  \end{bmatrix}$$ is true. Adding $\begin{bmatrix} 1 & 1\\  1 & 0 \end{bmatrix}^{k+1}$ both sides give $$\begin{bmatrix} 1 & 1\\  1 & 0 \end{bmatrix}^k + \begin{bmatrix} 1 & 1\\  1 & 0 \end{bmatrix}^{k+1}= \begin{bmatrix} F_{k+1} &F_k\\  F_k &F_{k-1}  \end{bmatrix} + \begin{bmatrix} 1 & 1\\  1 & 0 \end{bmatrix}^{k+1}$$ This is where I'm stuck. From here I don't know how to proceed. Please let me know if I'm in the right direction.","The question is Let $( F_0, F_1, F_2,... )$ be the Fibonacci sequence defined by $F_0=0,\, F_1=1, and F_{n+1}=F_n+F_{n-1}$, n greater than or equal to 1. Prove the following identities.  $$\begin{bmatrix} 1 & 1\\  1 & 0 \end{bmatrix}^n =\begin{bmatrix} F_{n+1} &F_n \\  F_n &F_{n-1}  \end{bmatrix}$$ This is what I tried. Proof:  Base case: If $n=1$ the formula says $$\begin{bmatrix} 1 & 1\\   1& 0 \end{bmatrix}^{1}=\begin{bmatrix} F_{1+1} & F_1\\  F_1 &F_0  \end{bmatrix}=\begin{bmatrix} 1+0 & 1\\  1 & 0 \end{bmatrix}=\begin{bmatrix} 1 & 1\\  1 & 0 \end{bmatrix}$$ which is true. Inductive Step: Suppose the formula holds for $n=k$ i.e. that $$\begin{bmatrix} 1 & 1\\   1& 0 \end{bmatrix}^{k}=\begin{bmatrix} F_{k+1} &F_k\\  F_k &F_{k-1}  \end{bmatrix}$$ is true. We have to show that the formula holds for $n=k+1$ that is $$\begin{bmatrix} 1 &1 \\   1&0  \end{bmatrix}^{k+1}= \begin{bmatrix} F_{k+2} & F_{k+1} \\   F_{k+1}&F_k  \end{bmatrix}$$ is true. Adding $\begin{bmatrix} 1 & 1\\  1 & 0 \end{bmatrix}^{k+1}$ both sides give $$\begin{bmatrix} 1 & 1\\  1 & 0 \end{bmatrix}^k + \begin{bmatrix} 1 & 1\\  1 & 0 \end{bmatrix}^{k+1}= \begin{bmatrix} F_{k+1} &F_k\\  F_k &F_{k-1}  \end{bmatrix} + \begin{bmatrix} 1 & 1\\  1 & 0 \end{bmatrix}^{k+1}$$ This is where I'm stuck. From here I don't know how to proceed. Please let me know if I'm in the right direction.",,"['matrices', 'problem-solving', 'fibonacci-numbers', 'solution-verification']"
39,Can this matrix inverse be re-written?,Can this matrix inverse be re-written?,,"I have a matrix inverse of the form $(\mathbf{AB}+\mathbf{C})^{-1}$, where each matrix is $2\times 2$ and each of the subelements below are known a priori . $$ \left( \left[ \begin{array}{cc} A_1&A_2\\ A_3&A_4\\ \end{array} \right]\mathbf{B}+ \left[ \begin{array}{cc} C_1&C_2\\ C_3&C_4\\ \end{array} \right] \right)^{-1} $$ $\mathbf{B}$ is also known, but it varies in each iteration, whereas $\mathbf{A}$ and $\mathbf{C}$ are fixed for all. When I implement the above in code, I run into instability issues with numerical matrix inverses. I was wondering if it would be possible to re-write the above inverse using all the known terms so that the inverse is implemented as a series of matrix/scalar multiplications instead of finding the inverse numerically.","I have a matrix inverse of the form $(\mathbf{AB}+\mathbf{C})^{-1}$, where each matrix is $2\times 2$ and each of the subelements below are known a priori . $$ \left( \left[ \begin{array}{cc} A_1&A_2\\ A_3&A_4\\ \end{array} \right]\mathbf{B}+ \left[ \begin{array}{cc} C_1&C_2\\ C_3&C_4\\ \end{array} \right] \right)^{-1} $$ $\mathbf{B}$ is also known, but it varies in each iteration, whereas $\mathbf{A}$ and $\mathbf{C}$ are fixed for all. When I implement the above in code, I run into instability issues with numerical matrix inverses. I was wondering if it would be possible to re-write the above inverse using all the known terms so that the inverse is implemented as a series of matrix/scalar multiplications instead of finding the inverse numerically.",,['linear-algebra']
40,Applications of companion matrices,Applications of companion matrices,,I'm looking for interesting applications of companion matrices. I can also use the Frobenius Normal Form. I already covered the Cayley-Hamilton Theorem and the application to linearly recursive sequences and high-order scalar linear differential equations.,I'm looking for interesting applications of companion matrices. I can also use the Frobenius Normal Form. I already covered the Cayley-Hamilton Theorem and the application to linearly recursive sequences and high-order scalar linear differential equations.,,"['linear-algebra', 'matrices', 'applications', 'companion-matrices']"
41,Repeated Eigenvalues 2,Repeated Eigenvalues 2,,"Two problema from Differential Equations; Dynamical Systems,  and an Introduction to Chaos (Morris W. Hirsch,Stephen Smale.Robert L. Devaney). Examples (pages 112-113): If $$A= \begin{pmatrix}         1 & 1& 0\\         -1 & 3 &  0\\         -1 & 1 & 2 \\         \end{pmatrix}$$ … $\lambda =2 , m_{\lambda}=3$ $\operatorname{rank}(A-2I)=1 , n_{\lambda}=2 ,k= m_{\lambda}- n_{\lambda}+1=2$ so  $$\color{#C00}{p= \begin{pmatrix}         -1& 1 & 0 \\         -1 & 0 &  0 \\         -1 & 0 & 1 \\         \end{pmatrix}}$$ by $ v_1=(A-2I)v_2, v_2 : \begin{cases} (A-2I)^2v_2= 0\\ (A-2I)v_2 \not = 0\\ \end{cases} $ But $$p^{-1}Ap \not =J$$  $$\begin{pmatrix}         0& 2 & 0 \\         2& 1 &  0 \\         0 & 0 & 2 \\         \end{pmatrix} \not = \begin{pmatrix}         2& 1 & 0 \\         0 & 2 &  0 \\         0 & 0 & 2 \\         \end{pmatrix}$$  what’s wrong?! two problems exist: first why $v_3= \begin{pmatrix}         0 \\         0  \\         1  \\         \end{pmatrix}$ from $(A-2I)v_2 = 0$ why this way?(is not be mentioned in definition) second why this order(from definition) is not true (for $v_i$) in fact $\color{#C00}{p= \begin{pmatrix}         1& -1 & 0 \\         0 & -1 &  0 \\         0 & -1 & 1 \\         \end{pmatrix}}$","Two problema from Differential Equations; Dynamical Systems,  and an Introduction to Chaos (Morris W. Hirsch,Stephen Smale.Robert L. Devaney). Examples (pages 112-113): If $$A= \begin{pmatrix}         1 & 1& 0\\         -1 & 3 &  0\\         -1 & 1 & 2 \\         \end{pmatrix}$$ … $\lambda =2 , m_{\lambda}=3$ $\operatorname{rank}(A-2I)=1 , n_{\lambda}=2 ,k= m_{\lambda}- n_{\lambda}+1=2$ so  $$\color{#C00}{p= \begin{pmatrix}         -1& 1 & 0 \\         -1 & 0 &  0 \\         -1 & 0 & 1 \\         \end{pmatrix}}$$ by $ v_1=(A-2I)v_2, v_2 : \begin{cases} (A-2I)^2v_2= 0\\ (A-2I)v_2 \not = 0\\ \end{cases} $ But $$p^{-1}Ap \not =J$$  $$\begin{pmatrix}         0& 2 & 0 \\         2& 1 &  0 \\         0 & 0 & 2 \\         \end{pmatrix} \not = \begin{pmatrix}         2& 1 & 0 \\         0 & 2 &  0 \\         0 & 0 & 2 \\         \end{pmatrix}$$  what’s wrong?! two problems exist: first why $v_3= \begin{pmatrix}         0 \\         0  \\         1  \\         \end{pmatrix}$ from $(A-2I)v_2 = 0$ why this way?(is not be mentioned in definition) second why this order(from definition) is not true (for $v_i$) in fact $\color{#C00}{p= \begin{pmatrix}         1& -1 & 0 \\         0 & -1 &  0 \\         0 & -1 & 1 \\         \end{pmatrix}}$",,"['linear-algebra', 'matrices', 'dynamical-systems']"
42,"If $A$ is skew-symmetric, its determinant remains unchanged when the same number is added to all its entries","If  is skew-symmetric, its determinant remains unchanged when the same number is added to all its entries",A,"If we have a skew-symmetric matrix, $A^t = -A$, of size $2n\times 2n$ and we add the same number to every entry in the matrix and take the determinant, i'm told we get the same determinant as $\det A$. A couple of small examples have confirmed this to be true for me, but I can't fully see why. By making use of the multilinearity of the determinant function we get that the new determinat is equal to $\det A$ + the determinants of $2n$ matrices, each a permutation of the original matrix with one row swapped with a  row of the added constant. I can't see the symmetry as to why these all cancel to 0.","If we have a skew-symmetric matrix, $A^t = -A$, of size $2n\times 2n$ and we add the same number to every entry in the matrix and take the determinant, i'm told we get the same determinant as $\det A$. A couple of small examples have confirmed this to be true for me, but I can't fully see why. By making use of the multilinearity of the determinant function we get that the new determinat is equal to $\det A$ + the determinants of $2n$ matrices, each a permutation of the original matrix with one row swapped with a  row of the added constant. I can't see the symmetry as to why these all cancel to 0.",,"['linear-algebra', 'matrices', 'determinant']"
43,Eigenvectors of inverse complex matrix,Eigenvectors of inverse complex matrix,,"For a non-singular matrix, its pretty straightforward to prove that $\lambda$ is eigenvalue of $A$ if and only if $\frac{1}{\lambda}$ is eigenvalue of $A^{-1}$. Let $A$ be a non-singular matrix, $x$ an eigenvector of $A$ and $\lambda \neq 0$ its eigenvalue : $$Ax = \lambda x \iff A^{-1}Ax = \lambda A^{-1}x \iff x \frac{1}{\lambda} = A^{-1}x$$ from this approach it seems to me that inverse matrix has reciprocal eigenvalues, but same eigenvectors. However, consider the following matrix $$ A = \begin{pmatrix}1 & 0 & 0\\ -2 & -1 & -1\\ 4 & 2 & 1 \end{pmatrix} $$ whose eigenvalues are $\left\{ 1,i,-i \right\} $.  So eigenvalues of $A^{-1}$ are $\left\{1,\frac{1}{i},\frac{1}{-i} \right\} = \left\{ 1,-i,i \right\} $. I tried putting this matrix to MATLAB and computing eigenvectors of it's inverse, but the elements in eigenvectors of $A^{-1}$ seem to be complex conjugates of the elements in eigenvectors of $A$. Where is the hidden flaw in the proof?","For a non-singular matrix, its pretty straightforward to prove that $\lambda$ is eigenvalue of $A$ if and only if $\frac{1}{\lambda}$ is eigenvalue of $A^{-1}$. Let $A$ be a non-singular matrix, $x$ an eigenvector of $A$ and $\lambda \neq 0$ its eigenvalue : $$Ax = \lambda x \iff A^{-1}Ax = \lambda A^{-1}x \iff x \frac{1}{\lambda} = A^{-1}x$$ from this approach it seems to me that inverse matrix has reciprocal eigenvalues, but same eigenvectors. However, consider the following matrix $$ A = \begin{pmatrix}1 & 0 & 0\\ -2 & -1 & -1\\ 4 & 2 & 1 \end{pmatrix} $$ whose eigenvalues are $\left\{ 1,i,-i \right\} $.  So eigenvalues of $A^{-1}$ are $\left\{1,\frac{1}{i},\frac{1}{-i} \right\} = \left\{ 1,-i,i \right\} $. I tried putting this matrix to MATLAB and computing eigenvectors of it's inverse, but the elements in eigenvectors of $A^{-1}$ seem to be complex conjugates of the elements in eigenvectors of $A$. Where is the hidden flaw in the proof?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'spectral-theory', 'fake-proofs']"
44,An inequality for symmetric matrices: $ \mbox{trace}(XY)\leq \lambda(X)^T\lambda(Y) $.,An inequality for symmetric matrices: ., \mbox{trace}(XY)\leq \lambda(X)^T\lambda(Y) ,"Let the vector of eigenvalues of a $n\times n$ matrix $U$ is denoted by  $$ \lambda(U)=\big(\lambda_1(U),\lambda_2(U),\ldots,\lambda_i(U),\ldots\lambda_{n-1}(U),\lambda_n(U)\big)^T. $$ where the eigenvalues are ordered as $\lambda_1(U)\leq\lambda_2(U)\leq\ldots\leq\lambda_i(U)\leq\ldots\lambda_{n-1}(U)\leq\lambda_n(U)$. I would like to prove ( or get a counterexample) the following inequality for symmetric matrices $X$ and $Y$  $$ \mbox{trace}(XY)\leq \lambda(X)^T\lambda(Y). $$ Thanks in advance.","Let the vector of eigenvalues of a $n\times n$ matrix $U$ is denoted by  $$ \lambda(U)=\big(\lambda_1(U),\lambda_2(U),\ldots,\lambda_i(U),\ldots\lambda_{n-1}(U),\lambda_n(U)\big)^T. $$ where the eigenvalues are ordered as $\lambda_1(U)\leq\lambda_2(U)\leq\ldots\leq\lambda_i(U)\leq\ldots\lambda_{n-1}(U)\leq\lambda_n(U)$. I would like to prove ( or get a counterexample) the following inequality for symmetric matrices $X$ and $Y$  $$ \mbox{trace}(XY)\leq \lambda(X)^T\lambda(Y). $$ Thanks in advance.",,"['linear-algebra', 'matrices', 'inequality', 'trace']"
45,Given $T(A) = A^t$ in $M_{n\times n}(\mathbb R)$. Find the polynomials and find if it's diagonalizable,Given  in . Find the polynomials and find if it's diagonalizable,T(A) = A^t M_{n\times n}(\mathbb R),"Given the vector space $M_{n\times n} (\mathbb R)$ and a transformation $T(A) = A^t$ (transpose): Find $m_T$ , $P_T$ (the minimum polynomial and the characteristic polynomial respectively.) Find if $T$ is diagonalizable, if so, find a diagonalization basis and the representation matrix in that basis. But how can I exactly find what $T$ does on $A^t$ if the basis has $n$ vectors? Any help will be much appreciated!","Given the vector space and a transformation (transpose): Find , (the minimum polynomial and the characteristic polynomial respectively.) Find if is diagonalizable, if so, find a diagonalization basis and the representation matrix in that basis. But how can I exactly find what does on if the basis has vectors? Any help will be much appreciated!",M_{n\times n} (\mathbb R) T(A) = A^t m_T P_T T T A^t n,"['linear-algebra', 'matrices', 'vector-spaces']"
46,"Proof of Sum, Difference, Scalar Multiple of Diagonal Matrices","Proof of Sum, Difference, Scalar Multiple of Diagonal Matrices",,"Assumming A and B are diagonal matrices of the same size, please prove that the following are diagonal matrices as well. a) $A+B$ b) $A-B$ c) $kA$ , for a scalar $k$ It's not homework- just a problem I didn't know how to do in my textbook. Thanks in advance!!","Assumming A and B are diagonal matrices of the same size, please prove that the following are diagonal matrices as well. a) $A+B$ b) $A-B$ c) $kA$ , for a scalar $k$ It's not homework- just a problem I didn't know how to do in my textbook. Thanks in advance!!",,"['linear-algebra', 'matrices']"
47,"Show that $\exp: \mathfrak{sl}(n,\mathbb R)\to \operatorname{SL}(n,\mathbb R)$ is not surjective",Show that  is not surjective,"\exp: \mathfrak{sl}(n,\mathbb R)\to \operatorname{SL}(n,\mathbb R)","It is well known that for $n=2$, this holds. The polar decomposition provides the topology of $\operatorname{SL}(n,\mathbb R)$ as the product of symmetric matrices and orthogonal matrices, which can be written as the product of exponentials of skew symmetric and symmetric traceless matrices. However I could not find  out the proof that  $\exp: \mathfrak{sl}(n,\mathbb R)\to\operatorname{SL}(n,\mathbb R)$ is not surjective for $n\geq 3$.","It is well known that for $n=2$, this holds. The polar decomposition provides the topology of $\operatorname{SL}(n,\mathbb R)$ as the product of symmetric matrices and orthogonal matrices, which can be written as the product of exponentials of skew symmetric and symmetric traceless matrices. However I could not find  out the proof that  $\exp: \mathfrak{sl}(n,\mathbb R)\to\operatorname{SL}(n,\mathbb R)$ is not surjective for $n\geq 3$.",,"['linear-algebra', 'matrices', 'lie-groups', 'lie-algebras']"
48,Are there any properties of the diag operator?,Are there any properties of the diag operator?,,"Let $u$ and $v$ be a column vector of same dimension. 1.) Can anyone give some properties about the operations of function, such as $\text{diag}(u)+\text{diag}(v)=\text{diag}(u+v)$? 2.) Is there any mathematical representation  to express the function diag? Thanks a lot.","Let $u$ and $v$ be a column vector of same dimension. 1.) Can anyone give some properties about the operations of function, such as $\text{diag}(u)+\text{diag}(v)=\text{diag}(u+v)$? 2.) Is there any mathematical representation  to express the function diag? Thanks a lot.",,['matrices']
49,Matrix being not diagonalizable in ${\Bbb F}_2$,Matrix being not diagonalizable in,{\Bbb F}_2,"We were talking about how the symmetric matrix $$A=\begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix}$$ is not diagonalizable in the field consisting of only $0$ and $1$, since the eigenvalues are $0$ and $2$, but in this field $0=2$, and the two eigenvectors are the same $$\begin{bmatrix} -1 \\ 1 \end{bmatrix} = \begin{bmatrix} 1 \\ 1 \end{bmatrix}$$ and I was able to find the jordan normal form of the matrix being $$J = P^{-1} A P$$ $J=\begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix}$, $P^{-1}=\begin{bmatrix} 1 & 0 \\ 1 & 1 \end{bmatrix}$, $A=\begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix}$, $P=\begin{bmatrix} 1 & 0 \\ 1 & 1 \end{bmatrix}$ since again in the field, $0=2=4=\dots$ and $-1=1=3=\dots$. However I need to find another symmetric matrix in the same field that is also not diagonalizable, and also find its jordan normal form. I'm having a hard time with looking for this matrix. The hint also said that  $$ \begin{bmatrix} 1 & 1 \\ 1 & 0 \end{bmatrix} \quad\text{ or }\quad \begin{bmatrix} 0 & 1 \\ 1 & 1 \end{bmatrix} $$ wouldnt work, even though they have no eigenvalues in ${\Bbb F}_2$; but they have two distinct eigenvalues in ${\Bbb F}_4$. so they are diagonalizable in ${\Bbb F}_4$. What is ${\Bbb F}_4$? Is it a finite field that has only $0,1,2,3$? If that's the case, the eigenvalues for  \begin{bmatrix} 1 & 1 \\ 1 & 0 \end{bmatrix} are not even integers, how are they in ${\Bbb F}_4$? Thanks in advance!","We were talking about how the symmetric matrix $$A=\begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix}$$ is not diagonalizable in the field consisting of only $0$ and $1$, since the eigenvalues are $0$ and $2$, but in this field $0=2$, and the two eigenvectors are the same $$\begin{bmatrix} -1 \\ 1 \end{bmatrix} = \begin{bmatrix} 1 \\ 1 \end{bmatrix}$$ and I was able to find the jordan normal form of the matrix being $$J = P^{-1} A P$$ $J=\begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix}$, $P^{-1}=\begin{bmatrix} 1 & 0 \\ 1 & 1 \end{bmatrix}$, $A=\begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix}$, $P=\begin{bmatrix} 1 & 0 \\ 1 & 1 \end{bmatrix}$ since again in the field, $0=2=4=\dots$ and $-1=1=3=\dots$. However I need to find another symmetric matrix in the same field that is also not diagonalizable, and also find its jordan normal form. I'm having a hard time with looking for this matrix. The hint also said that  $$ \begin{bmatrix} 1 & 1 \\ 1 & 0 \end{bmatrix} \quad\text{ or }\quad \begin{bmatrix} 0 & 1 \\ 1 & 1 \end{bmatrix} $$ wouldnt work, even though they have no eigenvalues in ${\Bbb F}_2$; but they have two distinct eigenvalues in ${\Bbb F}_4$. so they are diagonalizable in ${\Bbb F}_4$. What is ${\Bbb F}_4$? Is it a finite field that has only $0,1,2,3$? If that's the case, the eigenvalues for  \begin{bmatrix} 1 & 1 \\ 1 & 0 \end{bmatrix} are not even integers, how are they in ${\Bbb F}_4$? Thanks in advance!",,"['matrices', 'finite-fields', 'diagonalization']"
50,Does there exist an $n\times n$ real matrix $A$ exist such that $e^{e^{A}} - I_n$ is singular?,Does there exist an  real matrix  exist such that  is singular?,n\times n A e^{e^{A}} - I_n,I have one doubt whether an $n\times n$ real matrix $A$ exist such that $e^{e^{A}} - I_n$ is singular? I think I have to show that 1 is the eigenvalue of  $e^{e^{A}}$ in case answer is yes. But I  am not finding a way to proceed with this idea. I need help with this. Thanks for your time.,I have one doubt whether an $n\times n$ real matrix $A$ exist such that $e^{e^{A}} - I_n$ is singular? I think I have to show that 1 is the eigenvalue of  $e^{e^{A}}$ in case answer is yes. But I  am not finding a way to proceed with this idea. I need help with this. Thanks for your time.,,"['linear-algebra', 'matrices']"
51,How to prove that $\|AB-B^{-1}A^{-1}\|_F\geq\|AB-I\|_F$ when $A$ and $B$ are symmetric positive definite?,How to prove that  when  and  are symmetric positive definite?,\|AB-B^{-1}A^{-1}\|_F\geq\|AB-I\|_F A B,Let $A$ and $B$ be two symmetric positive definite $n \times n$ matrices. Prove or disprove that $$\|AB-B^{-1}A^{-1}\|_F\geq\|AB-I\|_F$$ where $\|\cdot\|_F$ denotes Frobenius norm. I believe it is true but I have no clue how to prove it. Thanks for your help.,Let and be two symmetric positive definite matrices. Prove or disprove that where denotes Frobenius norm. I believe it is true but I have no clue how to prove it. Thanks for your help.,A B n \times n \|AB-B^{-1}A^{-1}\|_F\geq\|AB-I\|_F \|\cdot\|_F,"['linear-algebra', 'matrices', 'inequality']"
52,"Prove that if $y=(y_1, \ldots, y_n)$ is such that $y_1a_1 + \cdots + y_na_n = 0$, then $∀x ∈ \mathbb{R}^n$, $Ax · y = 0$","Prove that if  is such that , then ,","y=(y_1, \ldots, y_n) y_1a_1 + \cdots + y_na_n = 0 ∀x ∈ \mathbb{R}^n Ax · y = 0","I have no idea how to start the following question.  Any help will be greatly appreciated. (a) Let $A$ be a $n\times n$ matrix and let $a_1,\ldots,a_n$ be the rows of $A.$ Suppose $y=(y_1, \ldots, y_n)$ is such that $y_1a_1 + \cdots + y_na_n = 0$. Prove that, $∀x ∈ \mathbb{R}^n$, $Ax · y = 0.$ (b) For $n≥2$, find a nonzero $n×n$ matrix $A$ such that $∀x ∈ \mathbb{R}^n, Ax·x=0.$","I have no idea how to start the following question.  Any help will be greatly appreciated. (a) Let $A$ be a $n\times n$ matrix and let $a_1,\ldots,a_n$ be the rows of $A.$ Suppose $y=(y_1, \ldots, y_n)$ is such that $y_1a_1 + \cdots + y_na_n = 0$. Prove that, $∀x ∈ \mathbb{R}^n$, $Ax · y = 0.$ (b) For $n≥2$, find a nonzero $n×n$ matrix $A$ such that $∀x ∈ \mathbb{R}^n, Ax·x=0.$",,"['linear-algebra', 'matrices', 'vector-spaces', 'inner-products']"
53,Determinant of the transpose of elementary matrices,Determinant of the transpose of elementary matrices,,"Is there a 'nice' proof to show that $\det(E^T) = \det(E)$ where $E$ is an elementary matrix? Clearly it's true for the elementary matrix representing a row being multiplied by a constant, because then $E^T = E$ as it is diagonal. I was thinking for the ""row-addition"" type, it's clearly true because if $E_1$ is a matrix representing row-addition then it is either an upper/lower triangular matrix and so $\det(E_1)$ is equal to the product of the diagonals. If $E_1$ is an upper/lower triangular matrix, then $E_1^T$ is a lower/upper triangular matrix and so $\det(E_1^T) = \det(E_1)$ as the diagonal entries remain the same when the matrix is transposed. How about for the ""row-switching"" matrix where rows $i$ and $j$ have been swapped on the identity matrix? Can we use the linearity of the rows in a matrix somehow? Thanks for any help!","Is there a 'nice' proof to show that $\det(E^T) = \det(E)$ where $E$ is an elementary matrix? Clearly it's true for the elementary matrix representing a row being multiplied by a constant, because then $E^T = E$ as it is diagonal. I was thinking for the ""row-addition"" type, it's clearly true because if $E_1$ is a matrix representing row-addition then it is either an upper/lower triangular matrix and so $\det(E_1)$ is equal to the product of the diagonals. If $E_1$ is an upper/lower triangular matrix, then $E_1^T$ is a lower/upper triangular matrix and so $\det(E_1^T) = \det(E_1)$ as the diagonal entries remain the same when the matrix is transposed. How about for the ""row-switching"" matrix where rows $i$ and $j$ have been swapped on the identity matrix? Can we use the linearity of the rows in a matrix somehow? Thanks for any help!",,"['linear-algebra', 'matrices']"
54,"How to compute determinant of $A$ such that $A=(I+\ [c_ic_j])\in M_n(\mathbb R)$ ,$c_i\in\mathbb R$","How to compute determinant of  such that  ,",A A=(I+\ [c_ic_j])\in M_n(\mathbb R) c_i\in\mathbb R,"assume   $\ [c_ic_j]_{n\times n}\in M_n(\mathbb R)$ such that $c_1,c_2,\ldots,c_n\in\mathbb R$  and $I$ be identity matrix how compute  $$\det (I+\ [c_ic_j])=?$$  Thanks in advance","assume   $\ [c_ic_j]_{n\times n}\in M_n(\mathbb R)$ such that $c_1,c_2,\ldots,c_n\in\mathbb R$  and $I$ be identity matrix how compute  $$\det (I+\ [c_ic_j])=?$$  Thanks in advance",,"['linear-algebra', 'matrices']"
55,Are all Toeplitz matrices diagonalizable?,Are all Toeplitz matrices diagonalizable?,,"As in the title. Also, if anyone knows if all Hermitian-symmetric matrices with distinct diagonal elements are diagonalizable, that'd be great to know. Thanks. Edit: Never mind about the Hermitian one, answered here: Are singular matrices diagonalizable?","As in the title. Also, if anyone knows if all Hermitian-symmetric matrices with distinct diagonal elements are diagonalizable, that'd be great to know. Thanks. Edit: Never mind about the Hermitian one, answered here: Are singular matrices diagonalizable?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'diagonalization', 'toeplitz-matrices']"
56,Derivative of matrix inverse,Derivative of matrix inverse,,I am trying to find the derivative of a matrix with respect to the inverse of the same matrix.  The matrix in question is a non singular symmetric matrix. Any thoughts?,I am trying to find the derivative of a matrix with respect to the inverse of the same matrix.  The matrix in question is a non singular symmetric matrix. Any thoughts?,,"['calculus', 'matrices', 'inverse']"
57,Questions related to nilpotent and idempotent matrices,Questions related to nilpotent and idempotent matrices,,"I have several questions on an assignment that I just can't seem to figure out. 1) Let $A$ be $2\times 2$ matrix. $A$ is nilpotent if $A^2=0$. Find all symmetric $2\times 2$ nilpotent matrices. It is symmetric, meaning the matrix $A$ should look like $A=\begin{bmatrix} a & b \\ b & c\end{bmatrix}$. Thus, by working out $A^2$ I find that $a^2 + b^2 = 0$ and $ab + bc = 0$. This tells me that $a^2 = - b^2$ and $a = -c$. I'm not sure how to progress from here. 2)Suppose $A$ is a nilpotent $2\times 2$ matrix and let $B = 2A$ - I. Express $B^2$ in terms of $B$ and $I$. Show that $B$ is invertible and find $B$ inverse. To find $B^2$ can I simply do $(2A -I)(2A - I)$ and expand as I would regular numbers? This should give $4A^2 - 4A + I^2$. Using the fact that $A^2$ is zero and $I^2$ returns $I$, the result is $I - 4A$. From here do I simply use the original expression to form an equation for $A$ in terms of $B$ and $I$ and substitute it in? Unless I am mistaken $4A$ cannot be treated as $2A^2$ and simplified to a zero matrix. 3) We say that a matrix $A$ is an idempotent matrix if $A^2 = A$. Prove that an idempotent matrix $A$ is invertible if and only if $A = I$. I have no idea how to begin on this one. 4) Suppose that $A$ and $B$ are idempotent matrices such that $A+B$ is idempotent, prove that $AB = BA = 0$. Again, I don't really have any idea how to begin on this one.","I have several questions on an assignment that I just can't seem to figure out. 1) Let $A$ be $2\times 2$ matrix. $A$ is nilpotent if $A^2=0$. Find all symmetric $2\times 2$ nilpotent matrices. It is symmetric, meaning the matrix $A$ should look like $A=\begin{bmatrix} a & b \\ b & c\end{bmatrix}$. Thus, by working out $A^2$ I find that $a^2 + b^2 = 0$ and $ab + bc = 0$. This tells me that $a^2 = - b^2$ and $a = -c$. I'm not sure how to progress from here. 2)Suppose $A$ is a nilpotent $2\times 2$ matrix and let $B = 2A$ - I. Express $B^2$ in terms of $B$ and $I$. Show that $B$ is invertible and find $B$ inverse. To find $B^2$ can I simply do $(2A -I)(2A - I)$ and expand as I would regular numbers? This should give $4A^2 - 4A + I^2$. Using the fact that $A^2$ is zero and $I^2$ returns $I$, the result is $I - 4A$. From here do I simply use the original expression to form an equation for $A$ in terms of $B$ and $I$ and substitute it in? Unless I am mistaken $4A$ cannot be treated as $2A^2$ and simplified to a zero matrix. 3) We say that a matrix $A$ is an idempotent matrix if $A^2 = A$. Prove that an idempotent matrix $A$ is invertible if and only if $A = I$. I have no idea how to begin on this one. 4) Suppose that $A$ and $B$ are idempotent matrices such that $A+B$ is idempotent, prove that $AB = BA = 0$. Again, I don't really have any idea how to begin on this one.",,"['linear-algebra', 'matrices']"
58,I am not sure how to calculate this norm?,I am not sure how to calculate this norm?,,"I have the following matrix: $$A=         \begin{bmatrix}         1 & 0 & 0 \\         0 & 1 & 1 \\         0 & 0 & 1 \\         \end{bmatrix} $$ What is the norm of $A$? I need to show the steps, should not use Matlab... I know that the answer is $\sqrt{\sqrt{5}/2+3/2}$. I am using the simple version to calculate the norm but getting different answer: $\sum_{i=0}^3\sum_{j=0}^3(a_{ij})^2=\sqrt{1+1+1+1}=2$ Maybe this is some different kind of norm, not sure. This might help - i need to get a condition number of $A$, which is $k(A)=\|A\|\|A^{-1}\|$...that is why i need to calculate the norm of $A$.","I have the following matrix: $$A=         \begin{bmatrix}         1 & 0 & 0 \\         0 & 1 & 1 \\         0 & 0 & 1 \\         \end{bmatrix} $$ What is the norm of $A$? I need to show the steps, should not use Matlab... I know that the answer is $\sqrt{\sqrt{5}/2+3/2}$. I am using the simple version to calculate the norm but getting different answer: $\sum_{i=0}^3\sum_{j=0}^3(a_{ij})^2=\sqrt{1+1+1+1}=2$ Maybe this is some different kind of norm, not sure. This might help - i need to get a condition number of $A$, which is $k(A)=\|A\|\|A^{-1}\|$...that is why i need to calculate the norm of $A$.",,"['linear-algebra', 'matrices', 'normed-spaces']"
59,Inverse of a 3 x 3 Covariance Matrix (Or Any Positive Definite [PD] Matrix),Inverse of a 3 x 3 Covariance Matrix (Or Any Positive Definite [PD] Matrix),,"I have 2 pixels with size 1x3 called $A$ and $B$ and I have to compute the following equation: $$  A^T *(\Sigma+ I_3*\lambda)^{-1}*B $$ where $\Sigma$ is the covariance matrix (3x3) between vectors $A$ and $B$. $I_3$ is the 3x3 identity matrix. $\lambda$ is a constant (therefore, the matrix is not singular). At the moment, i'm computing the inverse of the $\Sigma +I_3*\lambda$ using the Gauss-Jordan elimination. I wanna know if there is a trick to compute this equation without computing the inverse. I'm also limited in memory so the Gauss-Jordan elimination is not a really good solution. I also tried to compute straight the inverse using the rule of Sarrus but the result was not enought accurate. My aim is to resolve this equation with the highest speed and the minimum memory space. EDIT: Anyone knows a fast and good way to inverse a 3x3 symmetric matrix ? EDIT 2: I'm thinking about making a Cholesky decomposition of my matrix but after that, I don't understand how to compute the inverse of $(\Sigma +I_3*\lambda)$ from Cholesky matrix.","I have 2 pixels with size 1x3 called $A$ and $B$ and I have to compute the following equation: $$  A^T *(\Sigma+ I_3*\lambda)^{-1}*B $$ where $\Sigma$ is the covariance matrix (3x3) between vectors $A$ and $B$. $I_3$ is the 3x3 identity matrix. $\lambda$ is a constant (therefore, the matrix is not singular). At the moment, i'm computing the inverse of the $\Sigma +I_3*\lambda$ using the Gauss-Jordan elimination. I wanna know if there is a trick to compute this equation without computing the inverse. I'm also limited in memory so the Gauss-Jordan elimination is not a really good solution. I also tried to compute straight the inverse using the rule of Sarrus but the result was not enought accurate. My aim is to resolve this equation with the highest speed and the minimum memory space. EDIT: Anyone knows a fast and good way to inverse a 3x3 symmetric matrix ? EDIT 2: I'm thinking about making a Cholesky decomposition of my matrix but after that, I don't understand how to compute the inverse of $(\Sigma +I_3*\lambda)$ from Cholesky matrix.",,"['matrices', 'inverse']"
60,"What's the relationship between singular, nontrivial and linear dependent?","What's the relationship between singular, nontrivial and linear dependent?",,"I understand that if a matrix is singular, it has no inverse.  If it has nontrivial solutions, it means at least one solutions exists. If it is linearly dependent, it means that for $a_1 \mathbf{v_1}+a_2  \mathbf{v_2} + ... + a_n \mathbf{ v_n} =\mathbf{ 0}$. Not all the $a$'s are 0. (Not all the coefficients of v_k are zero to satisfy the equation. How does singular relate to nontrivial solutions and nontrivial solutions relate to linear dependent? Let's say you have 3 vectors: $$\vec p_1(x)=a_1x^2+b_1x+3$$ $$\vec p_2(x)=a_2x^2+b_2x+4$$ $$\vec p_3(x)=a_3x^2+b_3x+99$$ We multiply all the stuff with c and get $$c_1\vec p_1(x)+c_2\vec p_2(x)+c_3\vec p_3(x)=0$$ Then, we make $$a_1c_1+a_2c_2+a_3c_3=0$$ $$b_1c_1+b_2c_2+b_3c_3=0$$ $$3c_1+4c_2+99c_3=0$$ This coefficient matrix can be singular hence there are nontrivial solutions . So, $\vec p_1$, $\vec p_2$ and $\vec p_3$ are linearly dependent . OR This coefficient matrix can be nonsingular hence there are trivial solutions . So, $\vec p_1$, $\vec p_2$ and $\vec p_3$ are linearly independent .","I understand that if a matrix is singular, it has no inverse.  If it has nontrivial solutions, it means at least one solutions exists. If it is linearly dependent, it means that for $a_1 \mathbf{v_1}+a_2  \mathbf{v_2} + ... + a_n \mathbf{ v_n} =\mathbf{ 0}$. Not all the $a$'s are 0. (Not all the coefficients of v_k are zero to satisfy the equation. How does singular relate to nontrivial solutions and nontrivial solutions relate to linear dependent? Let's say you have 3 vectors: $$\vec p_1(x)=a_1x^2+b_1x+3$$ $$\vec p_2(x)=a_2x^2+b_2x+4$$ $$\vec p_3(x)=a_3x^2+b_3x+99$$ We multiply all the stuff with c and get $$c_1\vec p_1(x)+c_2\vec p_2(x)+c_3\vec p_3(x)=0$$ Then, we make $$a_1c_1+a_2c_2+a_3c_3=0$$ $$b_1c_1+b_2c_2+b_3c_3=0$$ $$3c_1+4c_2+99c_3=0$$ This coefficient matrix can be singular hence there are nontrivial solutions . So, $\vec p_1$, $\vec p_2$ and $\vec p_3$ are linearly dependent . OR This coefficient matrix can be nonsingular hence there are trivial solutions . So, $\vec p_1$, $\vec p_2$ and $\vec p_3$ are linearly independent .",,"['linear-algebra', 'matrices', 'vector-spaces']"
61,How to remove linearly dependent rows/cols,How to remove linearly dependent rows/cols,,"How would one remove linearly dependent rows/columns from a rank-deficient matrix. For example, (from wikipedia ): $$   A =   \begin{bmatrix}     2 & 4 & 1 & 3 \\     -1 & -2 & 1 & 0 \\     0 & 0 & 2 & 2 \\     3 & 6 & 2 & 5   \end{bmatrix}. $$ If you convert it to row-echelon-form, you get: $$   A =   \begin{bmatrix}     1 & 2 & 0 & 1 \\     0 & 0 & 1 & 1 \\     0 & 0 & 0 & 0 \\     0 & 0 & 0 & 0   \end{bmatrix} $$ This demonstrates that there are two linearly independent columns. As noted in the wikipedia article, ""We see that the second column is twice the first column, and that the fourth column equals the sum of the first and the third"". How can one extract the linearly independent columns (or rows) and get a result like: $$   A =   \begin{bmatrix}     2 & 1 \\     -1 & 1 \\     0 & 2 \\     3  & 2   \end{bmatrix}. $$","How would one remove linearly dependent rows/columns from a rank-deficient matrix. For example, (from wikipedia ): $$   A =   \begin{bmatrix}     2 & 4 & 1 & 3 \\     -1 & -2 & 1 & 0 \\     0 & 0 & 2 & 2 \\     3 & 6 & 2 & 5   \end{bmatrix}. $$ If you convert it to row-echelon-form, you get: $$   A =   \begin{bmatrix}     1 & 2 & 0 & 1 \\     0 & 0 & 1 & 1 \\     0 & 0 & 0 & 0 \\     0 & 0 & 0 & 0   \end{bmatrix} $$ This demonstrates that there are two linearly independent columns. As noted in the wikipedia article, ""We see that the second column is twice the first column, and that the fourth column equals the sum of the first and the third"". How can one extract the linearly independent columns (or rows) and get a result like: $$   A =   \begin{bmatrix}     2 & 1 \\     -1 & 1 \\     0 & 2 \\     3  & 2   \end{bmatrix}. $$",,"['linear-algebra', 'matrices', 'numerical-linear-algebra']"
62,Attain matrix transpose by row swaps and column swaps,Attain matrix transpose by row swaps and column swaps,,"Given a matrix and the two operations row swap and column swap, how to prove the possibility or impossibility of getting the transpose via a composition of these operations?","Given a matrix and the two operations row swap and column swap, how to prove the possibility or impossibility of getting the transpose via a composition of these operations?",,['matrices']
63,"$T :\mathbb {R^7}\rightarrow \mathbb {R^7} $ is defined by $T(x_1,x_2,\ldots x_6,x_7) = (x_7,x_6,\ldots x_2,x_1)$ pick out the true statements.",is defined by  pick out the true statements.,"T :\mathbb {R^7}\rightarrow \mathbb {R^7}  T(x_1,x_2,\ldots x_6,x_7) = (x_7,x_6,\ldots x_2,x_1)","Consider the linear  transformations $T :\mathbb {R^7}\rightarrow \mathbb {R^7} $ defined by $T(x_1,x_2,\ldots x_6,x_7) = (x_7,x_6,\ldots x_2,x_1)$. Which of the following statements are true. 1-  $\det T = 1$ 2 - There is a basis of $\mathbb {R^7}$ with respect to which $T$ is a diagonal matrix, 3- $T^7=I$ 4- The smallest $n$ such that $T^n = I$ is even. What i have done so for is I have tried for $T :\mathbb {R^2}\rightarrow \mathbb {R^2} $  and found that all the statments are true. Can i generalize my conclusion to $\mathbb {R^7} $. Do i need to find $7\times 7$  matrix? Is there any other approach?","Consider the linear  transformations $T :\mathbb {R^7}\rightarrow \mathbb {R^7} $ defined by $T(x_1,x_2,\ldots x_6,x_7) = (x_7,x_6,\ldots x_2,x_1)$. Which of the following statements are true. 1-  $\det T = 1$ 2 - There is a basis of $\mathbb {R^7}$ with respect to which $T$ is a diagonal matrix, 3- $T^7=I$ 4- The smallest $n$ such that $T^n = I$ is even. What i have done so for is I have tried for $T :\mathbb {R^2}\rightarrow \mathbb {R^2} $  and found that all the statments are true. Can i generalize my conclusion to $\mathbb {R^7} $. Do i need to find $7\times 7$  matrix? Is there any other approach?",,"['linear-algebra', 'matrices']"
64,Proving two results about the spectral radius,Proving two results about the spectral radius,,"How do I prove these two theorems? Furthermore, can I apply them to infinite-dimensional spaces, such as Banach spaces? Theorem 1. Let $M\in \mathbb{C}_{n\times n}$ be a matrix and $\epsilon > 0$ be given. There is at least one matrix norm $||\cdot||$ such that   $$\rho(M) \leq ||M|| \leq \rho(M) + \epsilon$$   where $\rho(M) = \max\{|\lambda_1(M)|, \dots , |\lambda_n(M)|\}$ denotes the spectral radius of $M$. Theorem 2. If $P \in \mathbb{C}_{n\times n}$ and $S\in \mathbb{C}_{n\times n}$ are such that $P = P^2$ and $PS = SP$ then   $$\rho(PS) \leq \rho(S).$$ I have used these results in finite dimensional spaces and want to use them in a Banach space.","How do I prove these two theorems? Furthermore, can I apply them to infinite-dimensional spaces, such as Banach spaces? Theorem 1. Let $M\in \mathbb{C}_{n\times n}$ be a matrix and $\epsilon > 0$ be given. There is at least one matrix norm $||\cdot||$ such that   $$\rho(M) \leq ||M|| \leq \rho(M) + \epsilon$$   where $\rho(M) = \max\{|\lambda_1(M)|, \dots , |\lambda_n(M)|\}$ denotes the spectral radius of $M$. Theorem 2. If $P \in \mathbb{C}_{n\times n}$ and $S\in \mathbb{C}_{n\times n}$ are such that $P = P^2$ and $PS = SP$ then   $$\rho(PS) \leq \rho(S).$$ I have used these results in finite dimensional spaces and want to use them in a Banach space.",,"['matrices', 'functional-analysis', 'normed-spaces']"
65,Comparing the spectrum of the (non)centered matrices,Comparing the spectrum of the (non)centered matrices,,"Suppose a symmetric matrix $A\in\mathbb{R}^{n\times n}$ is given. Let $J=I-\frac{1}{n}\cdot 1_n1_n^T\in\mathbb{R}^{n\times n}$ be the centering matrix, with $I$ being the identity matrix, and $1_n=[1 \dots 1]^T\in\mathbb{R}^n$. Now, suppose $B=JAJ$, and $C=AJ$. How do the eigenvalues of $B$ and $C$ compare? I'm interested in the zero and non-zero spectrum behaviour. With a simple $3\times 3$ example I obtain identical eigenvalues, but different eigenvectors. How could this be formalized mathematically?","Suppose a symmetric matrix $A\in\mathbb{R}^{n\times n}$ is given. Let $J=I-\frac{1}{n}\cdot 1_n1_n^T\in\mathbb{R}^{n\times n}$ be the centering matrix, with $I$ being the identity matrix, and $1_n=[1 \dots 1]^T\in\mathbb{R}^n$. Now, suppose $B=JAJ$, and $C=AJ$. How do the eigenvalues of $B$ and $C$ compare? I'm interested in the zero and non-zero spectrum behaviour. With a simple $3\times 3$ example I obtain identical eigenvalues, but different eigenvectors. How could this be formalized mathematically?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
66,Taylor expansion for matrices,Taylor expansion for matrices,,"Is it possible to define a Taylor expansion for matrices ? Can I use functional derivative ? More precisely I have to calculate something like : $\ln(A+B)$ using a Taylor expansion, where $A$ and $B$ are hermitian matrices which depend also on $x\in \mathbb{R}^3$. My idea is to use functional derivative but I don't know if the result is correct!","Is it possible to define a Taylor expansion for matrices ? Can I use functional derivative ? More precisely I have to calculate something like : $\ln(A+B)$ using a Taylor expansion, where $A$ and $B$ are hermitian matrices which depend also on $x\in \mathbb{R}^3$. My idea is to use functional derivative but I don't know if the result is correct!",,"['matrices', 'operator-theory', 'taylor-expansion', 'operator-algebras']"
67,When are two diagonal matrices congruent?,When are two diagonal matrices congruent?,,"This is probably a question that does not admit a simple answer. However, I'd like to know whether there exist criteria that determine when two diagonal matrices are congruent. I have the suspicion that, if such criteria exist, they depend strongly from the base field. In fact, over a quadratically closed field two diagonal matrices are congruent iff they have the same rank and over real closed fields Sylvester theorem answers completely the question. But over other fields? By Witt decomposition, we can restrict ourselves to consider only the case in which $A$ and $B$ are anisotropic. Hence the question is: Let $\mathbb{K}$ be a field and let $A = \mathrm{diag}(a_1, \dots, a_n)$ and $B = \mathrm{diag}(b_1,\dots,b_n)$ be two diagonal matrices over the field $\mathbb{K}$ such that $^t x A x \neq 0$ and $^t x B x \neq 0$ for all $x \in \mathbb{K}^n \setminus \{ 0 \}$. (In particular $a_i \neq 0$ and $b_i \neq 0$). What are necessary or sufficient conditions to have that $A$ and $B$ are congruent?","This is probably a question that does not admit a simple answer. However, I'd like to know whether there exist criteria that determine when two diagonal matrices are congruent. I have the suspicion that, if such criteria exist, they depend strongly from the base field. In fact, over a quadratically closed field two diagonal matrices are congruent iff they have the same rank and over real closed fields Sylvester theorem answers completely the question. But over other fields? By Witt decomposition, we can restrict ourselves to consider only the case in which $A$ and $B$ are anisotropic. Hence the question is: Let $\mathbb{K}$ be a field and let $A = \mathrm{diag}(a_1, \dots, a_n)$ and $B = \mathrm{diag}(b_1,\dots,b_n)$ be two diagonal matrices over the field $\mathbb{K}$ such that $^t x A x \neq 0$ and $^t x B x \neq 0$ for all $x \in \mathbb{K}^n \setminus \{ 0 \}$. (In particular $a_i \neq 0$ and $b_i \neq 0$). What are necessary or sufficient conditions to have that $A$ and $B$ are congruent?",,"['linear-algebra', 'matrices', 'vector-spaces', 'quadratic-forms']"
68,Inverse of a symmetric positive semi-definite matrix,Inverse of a symmetric positive semi-definite matrix,,"I would like to compute a partial inverse of a symmetric semi-definite matrix. I read about computing the pseudoinverse of a rectangular matrix by using SVD, however with a symmetric matrix I could apply a similar technique using instead the eigenvalue decomposition, i.e. compute the eigenvalues, discard the smallest and invert the remaining. Does this make sense? If so can you point me to a reference that explains how to achieve this in detail? Any help appreciated.","I would like to compute a partial inverse of a symmetric semi-definite matrix. I read about computing the pseudoinverse of a rectangular matrix by using SVD, however with a symmetric matrix I could apply a similar technique using instead the eigenvalue decomposition, i.e. compute the eigenvalues, discard the smallest and invert the remaining. Does this make sense? If so can you point me to a reference that explains how to achieve this in detail? Any help appreciated.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
69,Inverting a Vandermonde matrix,Inverting a Vandermonde matrix,,"Is there a quick way to invert the $n\times n$ Vandermonde matrix with columns $(1, x, x^2, ..., x^{n-1})$ where $x$ takes values $0,1,...,n-1$ (in ascending order from left to right)? Perhaps by row operations $(A|I)\to (I|A^{-1})$ where $A$ is the Vandermonde matrix, and $I$ the identity matrix? I don't see how to do it though... or maybe there is another way? Thank you.","Is there a quick way to invert the $n\times n$ Vandermonde matrix with columns $(1, x, x^2, ..., x^{n-1})$ where $x$ takes values $0,1,...,n-1$ (in ascending order from left to right)? Perhaps by row operations $(A|I)\to (I|A^{-1})$ where $A$ is the Vandermonde matrix, and $I$ the identity matrix? I don't see how to do it though... or maybe there is another way? Thank you.",,"['linear-algebra', 'matrices']"
70,"Given $XX^\top=A$, solving for $X$","Given , solving for",XX^\top=A X,"Not equal to this (my) own question . It's more general, probably more easy than the original question . All of the elements of $X$ and $A$ are integers. $XX^\top=A$ and $A$ is a symmetric matrix. How to find all possible $X$ matrices? Maybe a Gram-Schmidt method to keep only integer solutions. An example: $$ XX^\top= \left( \begin{array}{ccc}  0 & 1 & 1 \  1 & 0 & 1 \  1 & 1 & 0 \end{array} \right) \left( \begin{array}{ccc}  0 & 1 & 1 \\  1 & 0 & 1 \\  1 & 1 & 0 \end{array} \right) \left( \begin{array}{ccc}  2 & 1 & 1 \\  1 & 2 & 1 \\  1 & 1 & 2 \end{array} \right)=A $$","Not equal to this (my) own question . It's more general, probably more easy than the original question . All of the elements of $X$ and $A$ are integers. $XX^\top=A$ and $A$ is a symmetric matrix. How to find all possible $X$ matrices? Maybe a Gram-Schmidt method to keep only integer solutions. An example: $$ XX^\top= \left( \begin{array}{ccc}  0 & 1 & 1 \  1 & 0 & 1 \  1 & 1 & 0 \end{array} \right) \left( \begin{array}{ccc}  0 & 1 & 1 \\  1 & 0 & 1 \\  1 & 1 & 0 \end{array} \right) \left( \begin{array}{ccc}  2 & 1 & 1 \\  1 & 2 & 1 \\  1 & 1 & 2 \end{array} \right)=A $$",,"['linear-algebra', 'matrices', 'diophantine-equations']"
71,Prove $\text{rank}(A) \geq \frac{(\text{tr}(A))^2}{\text{tr}(A^2)}$ when $A$ is Hermitian,Prove  when  is Hermitian,\text{rank}(A) \geq \frac{(\text{tr}(A))^2}{\text{tr}(A^2)} A,"If $A \in \mathbb{C}^{n \times n}$ is a non-zero Hermitian matrix, I need to show that  $$\text{rank}(A) \geq \frac{(\text{tr}(A))^2}{\text{tr}(A^2)}$$ and reason out when the equality is attained? Can anyone help me in showing this result?","If $A \in \mathbb{C}^{n \times n}$ is a non-zero Hermitian matrix, I need to show that  $$\text{rank}(A) \geq \frac{(\text{tr}(A))^2}{\text{tr}(A^2)}$$ and reason out when the equality is attained? Can anyone help me in showing this result?",,"['linear-algebra', 'matrices']"
72,Solutions to $\boldsymbol{\mathbf{A}}\boldsymbol{\mathbf{x}} = \boldsymbol{\mathbf{b}}$,Solutions to,\boldsymbol{\mathbf{A}}\boldsymbol{\mathbf{x}} = \boldsymbol{\mathbf{b}},"I'm self-studying a bit of introductory linear algebra, watching the lectures on MIT OCW given by Gilbert Strang. The course isn't too rigourous and gives many things without proof, most of which I can reason through and convince myself of so far, but I've run into one thing that I can't wrap my head around. So he's discussing an algorithm to solve the system of linear equations $\boldsymbol{\mathbf{A}}\boldsymbol{\mathbf{x}} = \boldsymbol{\mathbf{b}}$. And basically he says that all we need to do is find a particular solution to the equation (after elimination) and then add it to any vector in the nullspace of the matrix. It's obvious why the sum of the particular solution and the nullspace vector is part of the solution set to $\boldsymbol{\mathbf{A}}\boldsymbol{\mathbf{x}} = \boldsymbol{\mathbf{b}}$. It's not obvious to me, however, why all solutions to the equation can be described as that type of sum. Could someone explain to me why this is true?","I'm self-studying a bit of introductory linear algebra, watching the lectures on MIT OCW given by Gilbert Strang. The course isn't too rigourous and gives many things without proof, most of which I can reason through and convince myself of so far, but I've run into one thing that I can't wrap my head around. So he's discussing an algorithm to solve the system of linear equations $\boldsymbol{\mathbf{A}}\boldsymbol{\mathbf{x}} = \boldsymbol{\mathbf{b}}$. And basically he says that all we need to do is find a particular solution to the equation (after elimination) and then add it to any vector in the nullspace of the matrix. It's obvious why the sum of the particular solution and the nullspace vector is part of the solution set to $\boldsymbol{\mathbf{A}}\boldsymbol{\mathbf{x}} = \boldsymbol{\mathbf{b}}$. It's not obvious to me, however, why all solutions to the equation can be described as that type of sum. Could someone explain to me why this is true?",,"['linear-algebra', 'matrices']"
73,"Is a ""sign matrix"" obtained from a symmetric positive-semidefinite matrix itself symmetic positive-semidefinite?","Is a ""sign matrix"" obtained from a symmetric positive-semidefinite matrix itself symmetic positive-semidefinite?",,"Suppose that $A \in {\cal S}_+^n$ is a symmetric positive semidefinite matrix. Let $B = {\rm sign}(A)$, where the sign is taken elementwise. Is the resulting matrix $B$ always positive semidefinite? If not, under what conditions can we say that $B \in {\cal S}_+^n$ ?","Suppose that $A \in {\cal S}_+^n$ is a symmetric positive semidefinite matrix. Let $B = {\rm sign}(A)$, where the sign is taken elementwise. Is the resulting matrix $B$ always positive semidefinite? If not, under what conditions can we say that $B \in {\cal S}_+^n$ ?",,"['linear-algebra', 'matrices']"
74,"A question about the diagonal of the powers of real, symmetric matrices","A question about the diagonal of the powers of real, symmetric matrices",,"Let $A \in M_{n\times n}(\mathbb{R})$ . Define $$\mathrm{diag}(A): M_{n\times n} \to M_{n \times 1} \text{ where } \mathrm{diag}(A) = \{a_{ii}\}_{i=1}^{n}$$ I have the following $\textbf{conjecture}$ : Let $A,B$ be real symmetric. There exists a unique permutation matrix $\pi$ for all $k\geq 0$ such that $$\mathrm{diag}(A^k) = \pi~ \mathrm{diag}(B^k)$$ if and only if $$\mathrm{diag}(A^k) = \pi~ \mathrm{diag}(B^k) ~~~(\star)$$ for $0\leq k \leq n$ . $\textbf{My approach:}$ ( $\implies$ ) follows by defintion. To show ( $\impliedby$ ), let $\ell > n$ . We can write the matrix $A^\ell $ as a linear combination of powers of $A$ , using Cayley-Hamilton theorem: $$A^\ell = c_{n-1}A^{n-1} + \cdots + c_1 A + c_0 I_n $$ where $c_i$ are real coefficients. We need a small lemma before we go: $\textbf{Lemma:}$ $$\mathrm{diag}: M_{n\times n} \to M_{n \times 1}$$ is a linear map. Then it follows that $$\mathrm{diag} (A^\ell) = \sum_{i=0}^{n-1} c_i \mathrm{diag}(A^i)$$ Similarly, we get that $$\mathrm{diag}(B^\ell) = \sum_{i=0}^{n-1} d_i \mathrm{diag}(B^i)$$ Using $(\star)$ we get that $$\mathrm{diag} (A^\ell) = \sum_{i=0}^{n-1} c_i \mathrm{diag}(A^i) = \sum_{i=0}^{n-1} c_i \pi(\mathrm{diag}(B^i)) = \pi(\sum_{i=0}^{n-1} c_i \mathrm{diag}(B^i)) $$ So it only remains to show that $$ \sum_{i=0}^{n-1} c_i \mathrm{diag}(B^i) = \sum_{i=0}^{n-1} d_i \mathrm{diag}(B^i)$$ I don't immidiately see why\if this is true. Would appreciate some general advice or reference.","Let . Define I have the following : Let be real symmetric. There exists a unique permutation matrix for all such that if and only if for . ( ) follows by defintion. To show ( ), let . We can write the matrix as a linear combination of powers of , using Cayley-Hamilton theorem: where are real coefficients. We need a small lemma before we go: is a linear map. Then it follows that Similarly, we get that Using we get that So it only remains to show that I don't immidiately see why\if this is true. Would appreciate some general advice or reference.","A \in M_{n\times n}(\mathbb{R}) \mathrm{diag}(A): M_{n\times n} \to M_{n \times 1} \text{ where } \mathrm{diag}(A) = \{a_{ii}\}_{i=1}^{n} \textbf{conjecture} A,B \pi k\geq 0 \mathrm{diag}(A^k) = \pi~ \mathrm{diag}(B^k) \mathrm{diag}(A^k) = \pi~ \mathrm{diag}(B^k) ~~~(\star) 0\leq k \leq n \textbf{My approach:} \implies \impliedby \ell > n A^\ell  A A^\ell = c_{n-1}A^{n-1} + \cdots + c_1 A + c_0 I_n  c_i \textbf{Lemma:} \mathrm{diag}: M_{n\times n} \to M_{n \times 1} \mathrm{diag} (A^\ell) = \sum_{i=0}^{n-1} c_i \mathrm{diag}(A^i) \mathrm{diag}(B^\ell) = \sum_{i=0}^{n-1} d_i \mathrm{diag}(B^i) (\star) \mathrm{diag} (A^\ell) = \sum_{i=0}^{n-1} c_i \mathrm{diag}(A^i) = \sum_{i=0}^{n-1} c_i \pi(\mathrm{diag}(B^i)) = \pi(\sum_{i=0}^{n-1} c_i \mathrm{diag}(B^i))   \sum_{i=0}^{n-1} c_i \mathrm{diag}(B^i) = \sum_{i=0}^{n-1} d_i \mathrm{diag}(B^i)","['linear-algebra', 'matrices']"
75,Solving a certain matrix equation,Solving a certain matrix equation,,"I am trying to solve the (real) matrix equation $X^{-1}AX=Y$ , where the matrix $A_{n\times n}$ is given and the matrices $X_{n\times n}, Y_{n\times n}$ are unknown matrices, with the only restriction that $Y$ is of the form $\begin{bmatrix}       S_{2\times 2} & T_{2\times (n-2)}\\\ \star & \star \end{bmatrix}$ , where $S_{2\times 2}$ has equal row sums and $T_{2\times (n-2)}$ is consisted of two identical rows, i.e., $T_{2\times (n-2)}= \begin{bmatrix}       T_1\\\ T_1\end{bmatrix}$ (Thus we want $Y$ only to be of the given form and no restriction on $X$ except being invertible.) The thing is that I would like to solve this without diagonalising or triangularising $A$ , i.e., no use of the eigenvalues of $A$ , whatsoever. I would very greatly appreciate it if you can direct me to some literature or give me some hints. I have a feeling that, even though we have only one rather weak restriction (i.e., the restriction on the form of $Y$ ), finding an explicit and ""nice"" solution is not possible. (When there is a solution.)","I am trying to solve the (real) matrix equation , where the matrix is given and the matrices are unknown matrices, with the only restriction that is of the form , where has equal row sums and is consisted of two identical rows, i.e., (Thus we want only to be of the given form and no restriction on except being invertible.) The thing is that I would like to solve this without diagonalising or triangularising , i.e., no use of the eigenvalues of , whatsoever. I would very greatly appreciate it if you can direct me to some literature or give me some hints. I have a feeling that, even though we have only one rather weak restriction (i.e., the restriction on the form of ), finding an explicit and ""nice"" solution is not possible. (When there is a solution.)","X^{-1}AX=Y A_{n\times n} X_{n\times n}, Y_{n\times n} Y \begin{bmatrix} 
     S_{2\times 2} & T_{2\times (n-2)}\\\ \star & \star \end{bmatrix} S_{2\times 2} T_{2\times (n-2)} T_{2\times (n-2)}= \begin{bmatrix} 
     T_1\\\ T_1\end{bmatrix} Y X A A Y","['linear-algebra', 'matrices', 'matrix-equations']"
76,Property of villainous matrices,Property of villainous matrices,,"Say that an $N \times N$ binary matrix $M \in \mathrm{Mat}_{N \times N}(\{0,1\})$ is villainous if it does not contain the submatrix $I_2 = \begin{pmatrix} 1& 0\\  0 & 1 \end{pmatrix}$ . Show that for any constant $\kappa < 1/80$ , every villainous $N\times N$ matrix contains a submatrix of size at least $\kappa N\times \kappa N$ such that all of the elements of the submatrix are equal. I've tried to think about the problem and had a few thoughts. For example, a villainous matrix may contain the submatrix $\begin{pmatrix} 0 & 1\\ 1 & 0 \end{pmatrix}$ , and so there's some level of 'invariance' here: the transpose of a villainous matrix is not necessarily villainous, so maybe a symmetry argument will not be helpful here. Next I tried to define precisely what a submatrix is, because the book doesn't mention it at all. It cannot refer to a contiguous block, since the statement wouldn't be true then: if we take a $N \times N$ matrix with $1$ 's on the diagonals directed north-east to south-west and spaced 3 positions apart with all other elements be zeroes, it does not contain $I_2$ as contiguous block and it does not contain (contiguous) $3 \times 3$ submatrix with equal elements. So, by $N \times N$ ""submatrix"", I'm pretty sure we're referring to the intersection of any $N$ rows and $N$ columns. The problem though is that I'm not sure how I might attack the problem. The book is for a first course in linear algebra and only marks it as a medium difficulty problem, so I don't believe we'll absolutely need to resort to any deep theorems in linear algebra to make real progress on the problem. Any help will be great!","Say that an binary matrix is villainous if it does not contain the submatrix . Show that for any constant , every villainous matrix contains a submatrix of size at least such that all of the elements of the submatrix are equal. I've tried to think about the problem and had a few thoughts. For example, a villainous matrix may contain the submatrix , and so there's some level of 'invariance' here: the transpose of a villainous matrix is not necessarily villainous, so maybe a symmetry argument will not be helpful here. Next I tried to define precisely what a submatrix is, because the book doesn't mention it at all. It cannot refer to a contiguous block, since the statement wouldn't be true then: if we take a matrix with 's on the diagonals directed north-east to south-west and spaced 3 positions apart with all other elements be zeroes, it does not contain as contiguous block and it does not contain (contiguous) submatrix with equal elements. So, by ""submatrix"", I'm pretty sure we're referring to the intersection of any rows and columns. The problem though is that I'm not sure how I might attack the problem. The book is for a first course in linear algebra and only marks it as a medium difficulty problem, so I don't believe we'll absolutely need to resort to any deep theorems in linear algebra to make real progress on the problem. Any help will be great!","N \times N M \in \mathrm{Mat}_{N \times N}(\{0,1\}) I_2 = \begin{pmatrix}
1& 0\\
 0 & 1
\end{pmatrix} \kappa < 1/80 N\times N \kappa N\times \kappa N \begin{pmatrix}
0 & 1\\
1 & 0
\end{pmatrix} N \times N 1 I_2 3 \times 3 N \times N N N","['linear-algebra', 'matrices']"
77,Eigenvalues and Eigenvectors of a block matrix,Eigenvalues and Eigenvectors of a block matrix,,"I need to find the eigenvalues and eigenvectors of the following matrix: $$D = \begin{bmatrix} 1 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & -i & 0 & 0 \\ 0 & 0 & i & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & -1  \\ \end{bmatrix} $$ Now I'm sure my professor does not want me to make some sterile computations, therefore I've tried analyzing the matrix first. First of all, I noticed the matrix is Hermitian, which means all its eigenvalues must be real numbers. I've looked up the properties of Hermitian matrices but nothing useful came out. Then I noticed I can rewrite $D$ as a block matrix composed of $2\times2$ blocks: $$ \begin{bmatrix} A & [0] & B \\ [0] & \sigma_2 & [0] \\ C & [0] & F \\ \end{bmatrix}$$ where $\sigma_2$ denotes the second Pauli matrix (I do not believe this to be a mere coincidence). The upside is that the eigenvalues of all the $2 \times 2$ matrices are trivial or I have already computed them (same thing for the eigenvalues). The main problem is that I've never actually worked with block matrices in any of my courses, so my personal knowledge of the topic is close to zero. I've seen something on Wikipedia about the determinant of matrices of the form: $$ \begin{bmatrix} A & B \\ C & F  \end{bmatrix}$$ But this does not apply to my case, so maybe the block matrix route is unlikely the right one. Is there some trick to compute the eigenvalues and eigenvectors of such a  peculiar matrix that I'm most likely missing? I'm honestly at a dead end this time so any input is much appreciated.","I need to find the eigenvalues and eigenvectors of the following matrix: Now I'm sure my professor does not want me to make some sterile computations, therefore I've tried analyzing the matrix first. First of all, I noticed the matrix is Hermitian, which means all its eigenvalues must be real numbers. I've looked up the properties of Hermitian matrices but nothing useful came out. Then I noticed I can rewrite as a block matrix composed of blocks: where denotes the second Pauli matrix (I do not believe this to be a mere coincidence). The upside is that the eigenvalues of all the matrices are trivial or I have already computed them (same thing for the eigenvalues). The main problem is that I've never actually worked with block matrices in any of my courses, so my personal knowledge of the topic is close to zero. I've seen something on Wikipedia about the determinant of matrices of the form: But this does not apply to my case, so maybe the block matrix route is unlikely the right one. Is there some trick to compute the eigenvalues and eigenvectors of such a  peculiar matrix that I'm most likely missing? I'm honestly at a dead end this time so any input is much appreciated.","D = \begin{bmatrix}
1 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & -i & 0 & 0 \\
0 & 0 & i & 0 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & -1  \\
\end{bmatrix}  D 2\times2  \begin{bmatrix}
A & [0] & B \\
[0] & \sigma_2 & [0] \\
C & [0] & F \\
\end{bmatrix} \sigma_2 2 \times 2  \begin{bmatrix} A & B \\
C & F 
\end{bmatrix}","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'block-matrices']"
78,On the rank of product of matrices of order 10,On the rank of product of matrices of order 10,,"I had been asked the following question: Suppose $A$ and $B$ are two $10 \times 10$ matrices over $\mathbb{C}$ with $rank(AB)=6$ . What is the maximum possible value of rank of $BA$ ? First, I realized that both $rank(A)$ and $rank(B)$ are at least 6. Further, from Sylvester's inequality, we must have $$rank(A)+rank(B) \leq 16.$$ Now, counting down all the possibilities (For instance, one possibility is $rank(A)=6$ and $rank(B)=10$ ), I arrived at a possible maximum value of $rank(BA)$ which is 8. For this to happen we must have $rank(A) = rank(B) = 8$ . I tried, but could not get an answer to the question, `do there exist two matrices $A$ and $B$ each with rank $8$ and that $rank(AB)=6$ , $rank(BA)=8$ ?","I had been asked the following question: Suppose and are two matrices over with . What is the maximum possible value of rank of ? First, I realized that both and are at least 6. Further, from Sylvester's inequality, we must have Now, counting down all the possibilities (For instance, one possibility is and ), I arrived at a possible maximum value of which is 8. For this to happen we must have . I tried, but could not get an answer to the question, `do there exist two matrices and each with rank and that , ?",A B 10 \times 10 \mathbb{C} rank(AB)=6 BA rank(A) rank(B) rank(A)+rank(B) \leq 16. rank(A)=6 rank(B)=10 rank(BA) rank(A) = rank(B) = 8 A B 8 rank(AB)=6 rank(BA)=8,"['linear-algebra', 'matrices', 'matrix-rank']"
79,Calculating eigenvectors and eigenvalues of a 3x3 matrix,Calculating eigenvectors and eigenvalues of a 3x3 matrix,,"I am dealing with a task in which I have to calculate the eigenvectors and eigenvalues of the following matrix: $$\begin{pmatrix} 5& 6 &1\\ 1 &3& 1\\ 2 &1 &2\end{pmatrix}$$ So I have computed the characteristic equation, which turns out to be this three-degree polynomial: $$-λ^3+10\lambda^2 -22\lambda +20$$ This cannot be factored by Ruffini, which leaves me stuck as to finding the roots I need. Have I done something wrong? Thanks in advance.","I am dealing with a task in which I have to calculate the eigenvectors and eigenvalues of the following matrix: So I have computed the characteristic equation, which turns out to be this three-degree polynomial: This cannot be factored by Ruffini, which leaves me stuck as to finding the roots I need. Have I done something wrong? Thanks in advance.","\begin{pmatrix}
5& 6 &1\\
1 &3& 1\\
2 &1 &2\end{pmatrix} -λ^3+10\lambda^2 -22\lambda +20","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
80,Unitary group and subgroup are non-isomorphic,Unitary group and subgroup are non-isomorphic,,"Consider the group $U(n)$ of unitary $n\times n$ matrices, as well as its subgroup $V$ of $n\times n$ matrices which are obtainable by permuting rows of diagonal matrices $\mathrm{diag}(a_1,\dots,a_n)$ with $|a_i|=1$ . It is clear that $V\subsetneq U(n)$ . However, I am trying to come up with an argument why we cannot have $V\cong U(n)$ as groups. Note that the fact that $V\subsetneq U(n)$ does not imply this! For instance, we also have $2\mathbb Z\subsetneq\mathbb Z$ , yet $2\mathbb Z\cong\mathbb Z$ . My idea was the following. For every $A\in V$ , the matrix $A^{n!}$ is diagonal. Thus, if we have $A,B\in V$ , then the two matrices $A^{n!}$ and $B^{n!}$ must commute. I expect this to fail for the matrices in $U(n)$ . However, I am yet to find two matrices $A,B\in U(n)$ which do the job, meaning that $A^{n!}B^{n!}\neq B^{n!}A^{n!}$ . Sure, if we fix some $n$ , say $n=2$ , then we can come up with such $A$ and $B$ explicitly. But how to solve the problem for general $n$ ? Any ideas? I am also open for other strategies proving $V\not\cong U(n)$ .","Consider the group of unitary matrices, as well as its subgroup of matrices which are obtainable by permuting rows of diagonal matrices with . It is clear that . However, I am trying to come up with an argument why we cannot have as groups. Note that the fact that does not imply this! For instance, we also have , yet . My idea was the following. For every , the matrix is diagonal. Thus, if we have , then the two matrices and must commute. I expect this to fail for the matrices in . However, I am yet to find two matrices which do the job, meaning that . Sure, if we fix some , say , then we can come up with such and explicitly. But how to solve the problem for general ? Any ideas? I am also open for other strategies proving .","U(n) n\times n V n\times n \mathrm{diag}(a_1,\dots,a_n) |a_i|=1 V\subsetneq U(n) V\cong U(n) V\subsetneq U(n) 2\mathbb Z\subsetneq\mathbb Z 2\mathbb Z\cong\mathbb Z A\in V A^{n!} A,B\in V A^{n!} B^{n!} U(n) A,B\in U(n) A^{n!}B^{n!}\neq B^{n!}A^{n!} n n=2 A B n V\not\cong U(n)","['linear-algebra', 'matrices', 'group-theory', 'unitary-matrices']"
81,What is the mathematical principle that explains the shape of a curve that depicts the mean distance between points on a grid and every other point?,What is the mathematical principle that explains the shape of a curve that depicts the mean distance between points on a grid and every other point?,,"I'm wondering if someone could give me a high-school-level explanation of the mathematical principle at work behind this. I was curious about this, so I designed an experiment to test it, and I'm wondering about the results. Suppose I have a bounded 2D coordinate grid that is a 100 x 100 square, so (0,1), (0,2) ... (99,99). Then, for each point, I calculate the mean Euclidean distance between that point and every other point. Then I sort the points by their mean distance to every other point in the grid and plot it. This is what a bar plot looks like for a 100x100 grid, with the x-axis representing unique coordinates and the y-axis representing the mean Euclidean distance between that coordinate and every other point in the grid: The plot shows a characteristic curve, where the middle section is essentially linear, the low end curves down somewhat, and the high end curves up more distinctly. What causes this? EDIT BASED ON @Eric's RESPONSE: If the corners of the grid are the cause of the accelerations at each end of the plot, then wouldn't it follow that the mean Euclidean distances between points in a cornerless grid (i.e., a circle) would not have these? I created a circle of radius 10 around the origin, generated random radii and angles to create random points within that circle, which I rounded to one place beyond the decimal. I randomly generated 500,000 such points, calculated their distances from one another, and plotted those results. Here's what it looks like: The extremes actually become more pronounced. Was I incorrect to infer that eliminating the corners would eliminate (or reduce) the bends at the end of the plot?","I'm wondering if someone could give me a high-school-level explanation of the mathematical principle at work behind this. I was curious about this, so I designed an experiment to test it, and I'm wondering about the results. Suppose I have a bounded 2D coordinate grid that is a 100 x 100 square, so (0,1), (0,2) ... (99,99). Then, for each point, I calculate the mean Euclidean distance between that point and every other point. Then I sort the points by their mean distance to every other point in the grid and plot it. This is what a bar plot looks like for a 100x100 grid, with the x-axis representing unique coordinates and the y-axis representing the mean Euclidean distance between that coordinate and every other point in the grid: The plot shows a characteristic curve, where the middle section is essentially linear, the low end curves down somewhat, and the high end curves up more distinctly. What causes this? EDIT BASED ON @Eric's RESPONSE: If the corners of the grid are the cause of the accelerations at each end of the plot, then wouldn't it follow that the mean Euclidean distances between points in a cornerless grid (i.e., a circle) would not have these? I created a circle of radius 10 around the origin, generated random radii and angles to create random points within that circle, which I rounded to one place beyond the decimal. I randomly generated 500,000 such points, calculated their distances from one another, and plotted those results. Here's what it looks like: The extremes actually become more pronounced. Was I incorrect to infer that eliminating the corners would eliminate (or reduce) the bends at the end of the plot?",,"['matrices', 'matrix-calculus']"
82,"How to prove that if a $3 \times 3$ matrix has two equal rows, it has no inverse?","How to prove that if a  matrix has two equal rows, it has no inverse?",3 \times 3,"In my maths classes in school we have said that if a matrix has two equal rows then it has no inverse. I can see this by calculating that the determinant of any $3 \times 3$ matrix with two equal rows is always $0$ . I would like to know how if you have some $3 \times 3$ matrix with two equal rows, you can prove that the columns are linearly dependent, and that the matrix therefore has no inverse. I am going off the definition of linear independence being that you cannot have a linear combination that equals the zero vector. PS. I am a total beginner to linear algebra so I may have completely messed up some simple stuff in the question.","In my maths classes in school we have said that if a matrix has two equal rows then it has no inverse. I can see this by calculating that the determinant of any matrix with two equal rows is always . I would like to know how if you have some matrix with two equal rows, you can prove that the columns are linearly dependent, and that the matrix therefore has no inverse. I am going off the definition of linear independence being that you cannot have a linear combination that equals the zero vector. PS. I am a total beginner to linear algebra so I may have completely messed up some simple stuff in the question.",3 \times 3 0 3 \times 3,"['linear-algebra', 'matrices', 'vectors', 'inverse']"
83,"Convergence of the positive operator $ \big( A+ (\lambda)^n B\big)^{\frac{1}{n}},$ where $0< \lambda <1.$",Convergence of the positive operator  where," \big( A+ (\lambda)^n B\big)^{\frac{1}{n}}, 0< \lambda <1.","Let $A, B$ be two positive operators in $M_{n}(\mathbb{C}).$ We know that $\lim_{n\to \infty}A^{\frac{1}{n}}$ converges and $\lim_{n\to \infty}A^{\frac{1}{n}}= P_{\rm ran(A)},$ where $P_{\rm ran(A)}$ denote the projection into range of $A.$ I am interested to know the limit of the sequence $$\lim_{n\to \infty} \big( A+ (\lambda)^n B\big)^{\frac{1}{n}},$$ where $0< \lambda <1.$ Some simple case, we can conclude that the above limit exists. However, I do not know in full generality. Any suggestions and input will be highly appreciated.","Let be two positive operators in We know that converges and where denote the projection into range of I am interested to know the limit of the sequence where Some simple case, we can conclude that the above limit exists. However, I do not know in full generality. Any suggestions and input will be highly appreciated.","A, B M_{n}(\mathbb{C}). \lim_{n\to \infty}A^{\frac{1}{n}} \lim_{n\to \infty}A^{\frac{1}{n}}= P_{\rm ran(A)}, P_{\rm ran(A)} A. \lim_{n\to \infty} \big( A+ (\lambda)^n B\big)^{\frac{1}{n}}, 0< \lambda <1.","['matrices', 'functional-analysis', 'soft-question', 'monotone-functions']"
84,"Using 4 by 4 Lorentz boost matrices to verify the tensor transformation law, $T^{\mu'\nu'}=\Lambda^{\mu'}_\alpha\Lambda^{\nu'}_\beta T^{\alpha\beta}$","Using 4 by 4 Lorentz boost matrices to verify the tensor transformation law,",T^{\mu'\nu'}=\Lambda^{\mu'}_\alpha\Lambda^{\nu'}_\beta T^{\alpha\beta},"In the following question, $K^{\prime}$ is a frame moving in the positive $x$ -direction with speed $v$ relative to frame $K$ : A tensor of type $(2,0)$ is $16$ numbers, $T^{\mu\nu}$ with the transformation property $$T^{\mu^\prime\nu^\prime}=\Lambda^{\mu^{\prime}}_\alpha\Lambda^{\nu^\prime}_\beta T^{\alpha\beta}\tag{1}$$ under the Lorentz transformation $x^{\mu^\prime}=\Lambda^{\mu^{\prime}}_\nu x^\nu$ Suppose that in frame $K$ , $T^{00} = \alpha$ , where $\alpha$ is a constant and the other $15$ components of $T^{\mu\nu}$ are zero. Determine the components $T^{\mu^\prime\nu^\prime}$ in $K^\prime$ . I want to try to answer this using $4\times 4$ Lorentz boost matrix multiplication, even though there is a much faster and simpler method given by the author which will be shown at the end, though I do not understand that method given by the author. Since the $4\times 4$ Lorentz boost matrix is $\Lambda^{\mu^{\prime}}_\alpha=\begin{pmatrix}\gamma & -\beta\gamma & 0 & 0\\-\beta\gamma & \gamma & 0 & 0\\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1\end{pmatrix}$ along the $x$ -direction. Then multiplying two such boosts as in $(1)$ , $\Big(\Lambda^{\mu^{\prime}}_\alpha$ and $\Lambda^{\nu^\prime}_\beta\Big)$ in $4\times 4$ matrix format eqn $(1)$ should read, $$\begin{align}T^{\mu^\prime\nu^\prime}&=\begin{pmatrix}\gamma & -\beta\gamma & 0 & 0\\-\beta\gamma & \gamma & 0 & 0\\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1\end{pmatrix}\begin{pmatrix}\gamma & -\beta\gamma & 0 & 0\\-\beta\gamma & \gamma & 0 & 0\\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1\end{pmatrix}\begin{pmatrix}\alpha & 0 & 0 & 0\\0 & 0 & 0 & 0\\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0\end{pmatrix}\\&=\alpha\begin{pmatrix}\gamma^2(1+\beta^2) & -2\beta\gamma^2 & 0 & 0\\-2\beta\gamma^2 & \gamma^2(1+\beta^2) & 0 & 0\\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1\end{pmatrix}\begin{pmatrix}1 & 0 & 0 & 0\\0 & 0 & 0 & 0\\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0\end{pmatrix}\\&=\alpha\gamma^2\begin{pmatrix}1+\beta^2 & 0& 0 & 0\\-2\beta & 0 & 0 & 0\\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0\end{pmatrix}\tag{2}\end{align}$$ The calculation was performed using wolframalpha.com , where $\beta=\dfrac{v}{c}$ and $\gamma=\left(1-v^2/c^2\right)^{-1/2}$ Addressing the question, the only non-zero components of $T^{\mu^\prime\nu^\prime}$ are $$T^{0^\prime 0^\prime}=\alpha\gamma^2(1+\beta^2)=\alpha\gamma^2\left(1+\frac{v^2}{c^2}\right)$$ and $$T^{0^\prime 1^\prime}=-2\beta\alpha\gamma^2=-2\frac{\gamma^2 v \alpha}{c}$$ Now the problem is that according to the authors solution my answer above is wrong and the correct solution is As the only non-zero component of $T^{\mu\nu}$ is $T^{00}=\alpha$ we have $$T^{\mu^\prime\nu^\prime}=\Lambda^{\mu^{\prime}}_0\Lambda^{\nu^\prime}_0 T^{00}=\Lambda^{\mu^{\prime}}_0\Lambda^{\nu^\prime}_0 \alpha\tag{3}$$ Accordingly, $$T^{0^\prime 0^\prime}=\Big(\Lambda^{0^{\prime}}_0\Big)^2\alpha=\gamma^2\alpha,$$ $$T^{0^\prime 1^\prime}=T^{1^\prime 0^\prime}=\Lambda^{0^{\prime}}_0\Lambda^{1^{\prime}}_0\alpha=-\frac{\gamma^2 v \alpha}{c},$$ $$T^{1^\prime 1^\prime}=\Big(\Lambda^{1^{\prime}}_0\Big)^2\alpha=\frac{\gamma^2 v^2\alpha}{c^2}$$ Looking at the authors' solution, it's clear that he/she is explicitly writing the matrix components of $\Lambda^{\mu{^\prime}}_{\alpha}$ such that $$\Lambda^{\mu^\prime}_\alpha=\Lambda^{\nu^\prime}_\beta=\begin{pmatrix}\Lambda^{0^\prime}_0 & \Lambda^{0^\prime}_1 & \Lambda^{0^\prime}_2 & \Lambda^{0^\prime}_3 \\ \Lambda^{1^\prime}_0 & \Lambda^{1^\prime}_1 & \Lambda^{1^\prime}_2 & \Lambda^{1^\prime}_3 \\ \Lambda^{2^\prime}_0 & \Lambda^{2^\prime}_1 & \Lambda^{2^\prime}_2 & \Lambda^{2^\prime}_3 \\ \Lambda^{3^\prime}_0 & \Lambda^{3^\prime}_1 & \Lambda^{3^\prime}_2 & \Lambda^{3^\prime}_3\end{pmatrix}$$ In fact, just by inspection, the correct matrix that will give the same components for $T^{\mu^\prime\nu^\prime}$ as the authors' is $$T^{\mu^\prime\nu^\prime}=\alpha\gamma^2\begin{pmatrix}1 & -\beta & 0 & 0\\-\beta & \beta^2 & 0 & 0\\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0\end{pmatrix}\tag{4}$$ but how to obtain this matrix is beyond my comprehension. So my questions are, Why is the logic I used in eqn $(2)$ not giving me the same answer as the author (I have only $2$ non-zero components, whereas the author has $4$ ) and how can I obtain the correct matrix form for the components, shown in eqn $(4)$ ? How did the author 'know in advance' that $T^{\mu^\prime\nu^\prime}=\Lambda^{\mu^{\prime}}_0\Lambda^{\nu^\prime}_0 T^{00}$ (from eqn $(3)$ ) would give non-zero components? Put another way, I'm asking how the author knew the lower indices on the $\Lambda$ matrices are zero even though there is a contribution to $T^{0^\prime 1^\prime}=T^{1^\prime 0^\prime}$ ?","In the following question, is a frame moving in the positive -direction with speed relative to frame : A tensor of type is numbers, with the transformation property under the Lorentz transformation Suppose that in frame , , where is a constant and the other components of are zero. Determine the components in . I want to try to answer this using Lorentz boost matrix multiplication, even though there is a much faster and simpler method given by the author which will be shown at the end, though I do not understand that method given by the author. Since the Lorentz boost matrix is along the -direction. Then multiplying two such boosts as in , and in matrix format eqn should read, The calculation was performed using wolframalpha.com , where and Addressing the question, the only non-zero components of are and Now the problem is that according to the authors solution my answer above is wrong and the correct solution is As the only non-zero component of is we have Accordingly, Looking at the authors' solution, it's clear that he/she is explicitly writing the matrix components of such that In fact, just by inspection, the correct matrix that will give the same components for as the authors' is but how to obtain this matrix is beyond my comprehension. So my questions are, Why is the logic I used in eqn not giving me the same answer as the author (I have only non-zero components, whereas the author has ) and how can I obtain the correct matrix form for the components, shown in eqn ? How did the author 'know in advance' that (from eqn ) would give non-zero components? Put another way, I'm asking how the author knew the lower indices on the matrices are zero even though there is a contribution to ?","K^{\prime} x v K (2,0) 16 T^{\mu\nu} T^{\mu^\prime\nu^\prime}=\Lambda^{\mu^{\prime}}_\alpha\Lambda^{\nu^\prime}_\beta T^{\alpha\beta}\tag{1} x^{\mu^\prime}=\Lambda^{\mu^{\prime}}_\nu x^\nu K T^{00} = \alpha \alpha 15 T^{\mu\nu} T^{\mu^\prime\nu^\prime} K^\prime 4\times 4 4\times 4 \Lambda^{\mu^{\prime}}_\alpha=\begin{pmatrix}\gamma & -\beta\gamma & 0 & 0\\-\beta\gamma & \gamma & 0 & 0\\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1\end{pmatrix} x (1) \Big(\Lambda^{\mu^{\prime}}_\alpha \Lambda^{\nu^\prime}_\beta\Big) 4\times 4 (1) \begin{align}T^{\mu^\prime\nu^\prime}&=\begin{pmatrix}\gamma & -\beta\gamma & 0 & 0\\-\beta\gamma & \gamma & 0 & 0\\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1\end{pmatrix}\begin{pmatrix}\gamma & -\beta\gamma & 0 & 0\\-\beta\gamma & \gamma & 0 & 0\\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1\end{pmatrix}\begin{pmatrix}\alpha & 0 & 0 & 0\\0 & 0 & 0 & 0\\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0\end{pmatrix}\\&=\alpha\begin{pmatrix}\gamma^2(1+\beta^2) & -2\beta\gamma^2 & 0 & 0\\-2\beta\gamma^2 & \gamma^2(1+\beta^2) & 0 & 0\\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1\end{pmatrix}\begin{pmatrix}1 & 0 & 0 & 0\\0 & 0 & 0 & 0\\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0\end{pmatrix}\\&=\alpha\gamma^2\begin{pmatrix}1+\beta^2 & 0& 0 & 0\\-2\beta & 0 & 0 & 0\\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0\end{pmatrix}\tag{2}\end{align} \beta=\dfrac{v}{c} \gamma=\left(1-v^2/c^2\right)^{-1/2} T^{\mu^\prime\nu^\prime} T^{0^\prime 0^\prime}=\alpha\gamma^2(1+\beta^2)=\alpha\gamma^2\left(1+\frac{v^2}{c^2}\right) T^{0^\prime 1^\prime}=-2\beta\alpha\gamma^2=-2\frac{\gamma^2 v \alpha}{c} T^{\mu\nu} T^{00}=\alpha T^{\mu^\prime\nu^\prime}=\Lambda^{\mu^{\prime}}_0\Lambda^{\nu^\prime}_0 T^{00}=\Lambda^{\mu^{\prime}}_0\Lambda^{\nu^\prime}_0 \alpha\tag{3} T^{0^\prime 0^\prime}=\Big(\Lambda^{0^{\prime}}_0\Big)^2\alpha=\gamma^2\alpha, T^{0^\prime 1^\prime}=T^{1^\prime 0^\prime}=\Lambda^{0^{\prime}}_0\Lambda^{1^{\prime}}_0\alpha=-\frac{\gamma^2 v \alpha}{c}, T^{1^\prime 1^\prime}=\Big(\Lambda^{1^{\prime}}_0\Big)^2\alpha=\frac{\gamma^2 v^2\alpha}{c^2} \Lambda^{\mu{^\prime}}_{\alpha} \Lambda^{\mu^\prime}_\alpha=\Lambda^{\nu^\prime}_\beta=\begin{pmatrix}\Lambda^{0^\prime}_0 & \Lambda^{0^\prime}_1 & \Lambda^{0^\prime}_2 & \Lambda^{0^\prime}_3 \\ \Lambda^{1^\prime}_0 & \Lambda^{1^\prime}_1 & \Lambda^{1^\prime}_2 & \Lambda^{1^\prime}_3 \\ \Lambda^{2^\prime}_0 & \Lambda^{2^\prime}_1 & \Lambda^{2^\prime}_2 & \Lambda^{2^\prime}_3 \\ \Lambda^{3^\prime}_0 & \Lambda^{3^\prime}_1 & \Lambda^{3^\prime}_2 & \Lambda^{3^\prime}_3\end{pmatrix} T^{\mu^\prime\nu^\prime} T^{\mu^\prime\nu^\prime}=\alpha\gamma^2\begin{pmatrix}1 & -\beta & 0 & 0\\-\beta & \beta^2 & 0 & 0\\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0\end{pmatrix}\tag{4} (2) 2 4 (4) T^{\mu^\prime\nu^\prime}=\Lambda^{\mu^{\prime}}_0\Lambda^{\nu^\prime}_0 T^{00} (3) \Lambda T^{0^\prime 1^\prime}=T^{1^\prime 0^\prime}","['matrices', 'intuition', 'mathematical-physics', 'tensors', 'special-relativity']"
85,Calculate $\mathbf{A}^+\mathbf{y}$ indirectly without accessing $\mathbf{A}$,Calculate  indirectly without accessing,\mathbf{A}^+\mathbf{y} \mathbf{A},"The Original Question (informal): I have a black-box linear system $f:\mathbb{R}^N\rightarrow\mathbb{R}^M~(0\ll M\ll N)$ . It is guaranteed that there exists a full-rank matrix $\mathbf{A}\in\mathbb{R}^{M\times N}$ satisfying $\forall \mathbf{x}\in\mathbb{R}^N,f(\mathbf{x})\equiv\mathbf{Ax}$ . Here I call $\mathbf{A}$ the transformation matrix of $f$ . Now given a vector $\mathbf{y}\in\mathbb{R}^M$ , I want to calculate $\mathbf{z}=\mathbf{A}^+\mathbf{y}\in\mathbb{R}^N$ , where $\mathbf{A}^+\in\mathbb{R}^{N\times M}$ , satisfying $\mathbf{AA}^+=\mathbf{I}_M$ in some cases, is the Moore–Penrose inverse of $\mathbf{A}$ ( Reference 1 ). There are some supplementary information: I may know that there is a way to get the explicit form of $\mathbf{A}$ from a given fixed $f$ ( Reference 2 ). However, since $M$ and $N$ are both very large numbers (about $10^6$ or even $10^9$ ), I would not get $\mathbf{A}$ , explicitly calculate $\mathbf{A}^+$ , and finally calculate $\mathbf{z=A}^+\mathbf{y}$ . (The computational complexity of direct calculation is too high for me.) I may want an indirect way of obtaining $\mathbf{z=A}^+\mathbf{y}$ . The inner structure of linear $f$ is very complicated. Actually, $f$ is a black-box. But in my system, I can conveniently calculate $f(\mathbf{r})$ for any given $\mathbf{r}\in\mathbb{R}^N$ . In other words, the forward pass of $f$ is fast and efficient. I may not need $\mathbf{A}$ , $\mathbf{A}^+$ or some operators about them. I may only want $\mathbf{z=A}^+\mathbf{y}$ , which is known to be unique when $\mathbf{y}$ and $f$ are given and fixed. There are no prior knowledge about $f$ , $\mathbf{A}$ and $\mathbf{y}$ . In other words, their inner values or elements can be random numbers, like some noise. So, how to get $\mathbf{z=A}^+\mathbf{y}$ efficiently? Some of My Efforts: I was trying to search an $\mathbf{x}$ satisfying $\mathbf{Ax=y}$ . To be concrete, I use a tool dubbed PyTorch (a deep learning framework on Python) to optimize a randomly initialized $\mathbf{x}$ with a loss function $\mathcal{L}=\lVert f(\mathbf{x})-\mathbf{y} \rVert _2^2$ . And $f$ is a neural network in my own implementation. When $\mathcal{L}$ hits $0$ , I stop my optimization program and get the estimation. However, since $0\ll M\ll N$ , there may exist $\mathbf{x}_1$ and $\mathbf{x}_2$ satisfying $\mathbf{x}_1\ne \mathbf{x}_2$ and $\mathbf{Ax}_1=\mathbf{Ax}_2=\mathbf{y}$ . Therefore, I think this method could not exactly find the unique $\mathbf{z=A}^+\mathbf{y}$ that I want. Does there exists an efficient way to achieve this? There may be two statements (but in fact, only one of them is true): The answer is ""Yes"". There exists a way to efficiently calculate $\mathbf{z=A}^+\mathbf{y}$ from given fixed $f$ and $\mathbf{y}$ , without accessing the explicit forms of $\mathbf{A}$ or $\mathbf{A}^+$ . In other words, there are some properties of $\mathbf{z=A}^+\mathbf{y}$ can be utilized. But I have still not found them. The answer is ""No"". To get $\mathbf{z=A}^+\mathbf{y}$ , I should try to get the explicit form of $\mathbf{A}^+$ and then calculate $\mathbf{A}^+\mathbf{y}$ . There are no efficient and indirect ways. After a long struggle, I still have no idea about this problem. Now I may tend to believe that the above second statement is true. Any solutions, suggestions and discussions about this problem would be appealing for me. I am still searching, trying and thinking ...","The Original Question (informal): I have a black-box linear system . It is guaranteed that there exists a full-rank matrix satisfying . Here I call the transformation matrix of . Now given a vector , I want to calculate , where , satisfying in some cases, is the Moore–Penrose inverse of ( Reference 1 ). There are some supplementary information: I may know that there is a way to get the explicit form of from a given fixed ( Reference 2 ). However, since and are both very large numbers (about or even ), I would not get , explicitly calculate , and finally calculate . (The computational complexity of direct calculation is too high for me.) I may want an indirect way of obtaining . The inner structure of linear is very complicated. Actually, is a black-box. But in my system, I can conveniently calculate for any given . In other words, the forward pass of is fast and efficient. I may not need , or some operators about them. I may only want , which is known to be unique when and are given and fixed. There are no prior knowledge about , and . In other words, their inner values or elements can be random numbers, like some noise. So, how to get efficiently? Some of My Efforts: I was trying to search an satisfying . To be concrete, I use a tool dubbed PyTorch (a deep learning framework on Python) to optimize a randomly initialized with a loss function . And is a neural network in my own implementation. When hits , I stop my optimization program and get the estimation. However, since , there may exist and satisfying and . Therefore, I think this method could not exactly find the unique that I want. Does there exists an efficient way to achieve this? There may be two statements (but in fact, only one of them is true): The answer is ""Yes"". There exists a way to efficiently calculate from given fixed and , without accessing the explicit forms of or . In other words, there are some properties of can be utilized. But I have still not found them. The answer is ""No"". To get , I should try to get the explicit form of and then calculate . There are no efficient and indirect ways. After a long struggle, I still have no idea about this problem. Now I may tend to believe that the above second statement is true. Any solutions, suggestions and discussions about this problem would be appealing for me. I am still searching, trying and thinking ...","f:\mathbb{R}^N\rightarrow\mathbb{R}^M~(0\ll M\ll N) \mathbf{A}\in\mathbb{R}^{M\times N} \forall \mathbf{x}\in\mathbb{R}^N,f(\mathbf{x})\equiv\mathbf{Ax} \mathbf{A} f \mathbf{y}\in\mathbb{R}^M \mathbf{z}=\mathbf{A}^+\mathbf{y}\in\mathbb{R}^N \mathbf{A}^+\in\mathbb{R}^{N\times M} \mathbf{AA}^+=\mathbf{I}_M \mathbf{A} \mathbf{A} f M N 10^6 10^9 \mathbf{A} \mathbf{A}^+ \mathbf{z=A}^+\mathbf{y} \mathbf{z=A}^+\mathbf{y} f f f(\mathbf{r}) \mathbf{r}\in\mathbb{R}^N f \mathbf{A} \mathbf{A}^+ \mathbf{z=A}^+\mathbf{y} \mathbf{y} f f \mathbf{A} \mathbf{y} \mathbf{z=A}^+\mathbf{y} \mathbf{x} \mathbf{Ax=y} \mathbf{x} \mathcal{L}=\lVert f(\mathbf{x})-\mathbf{y} \rVert _2^2 f \mathcal{L} 0 0\ll M\ll N \mathbf{x}_1 \mathbf{x}_2 \mathbf{x}_1\ne \mathbf{x}_2 \mathbf{Ax}_1=\mathbf{Ax}_2=\mathbf{y} \mathbf{z=A}^+\mathbf{y} \mathbf{z=A}^+\mathbf{y} f \mathbf{y} \mathbf{A} \mathbf{A}^+ \mathbf{z=A}^+\mathbf{y} \mathbf{z=A}^+\mathbf{y} \mathbf{A}^+ \mathbf{A}^+\mathbf{y}","['linear-algebra', 'matrices', 'optimization', 'matrix-equations', 'pseudoinverse']"
86,"Let $A_1, A_2, ..., A_k\in M_n(\mathbb{R})$ be symmetric matrices such that $A^2_1+A^2_2+...+A^2_k=0$. Prove that $A_i=0$ for every $i=1,2,...,k$.",Let  be symmetric matrices such that . Prove that  for every .,"A_1, A_2, ..., A_k\in M_n(\mathbb{R}) A^2_1+A^2_2+...+A^2_k=0 A_i=0 i=1,2,...,k","Let $A_1, A_2, ..., A_k\in M_n(\mathbb{R})$ be symmetric matrices  such that $A^2_1+A^2_2+...+A^2_k=0$ . Prove that $A_i=0$ for every $i=1,2,...,k$ . My attempt: Let $B$ be a ortonormal base of $M_n(\mathbb{R})$ (this base exists becasue $M_n(\mathbb{R}$ ) has finite dimension), $v\in\mathbb{R}^n$ and $[T_i]_B=A_i$ for every $i=1,2,...,k$ , then $0=\left< (T^2_1+...+T^2_k)(v),v\right>=\left< T^2_1(v),v\right>+\left< T^2_2(v),v\right>+...+\left< T^2_k(v),v\right>=\left< T_1(v),T^*_1(v)\right>+\left<T_2(v),T^*_2(v) \right>+...+\left< T_k(v),T^*_k(v)\right>=\left< T_1(v),T^t_1(v)\right>+\left<T_2(v),T^t_2(v) \right>+...+\left< T_k(v),T^t_k(v)\right>=\left< T_1(v),T_1(v)\right>+\left<T_2(v),T_2(v) \right>+...+\left< T_k(v),T_k(v)\right>=||T_1(v)||^2+||T_2(v)||^2+...+||T_k(v)||^2$ . So, we need to have $||T_i(v)||^2=0$ for every $i=1,2,...,k$ ans so $T_i=0$ for every $i=1,2,...,k$ and $A_i=0$ for every $i=1,2,...,k$ . Is this correct? Thanks","Let be symmetric matrices  such that . Prove that for every . My attempt: Let be a ortonormal base of (this base exists becasue ) has finite dimension), and for every , then . So, we need to have for every ans so for every and for every . Is this correct? Thanks","A_1, A_2, ..., A_k\in M_n(\mathbb{R}) A^2_1+A^2_2+...+A^2_k=0 A_i=0 i=1,2,...,k B M_n(\mathbb{R}) M_n(\mathbb{R} v\in\mathbb{R}^n [T_i]_B=A_i i=1,2,...,k 0=\left< (T^2_1+...+T^2_k)(v),v\right>=\left< T^2_1(v),v\right>+\left< T^2_2(v),v\right>+...+\left< T^2_k(v),v\right>=\left< T_1(v),T^*_1(v)\right>+\left<T_2(v),T^*_2(v) \right>+...+\left< T_k(v),T^*_k(v)\right>=\left< T_1(v),T^t_1(v)\right>+\left<T_2(v),T^t_2(v) \right>+...+\left< T_k(v),T^t_k(v)\right>=\left< T_1(v),T_1(v)\right>+\left<T_2(v),T_2(v) \right>+...+\left< T_k(v),T_k(v)\right>=||T_1(v)||^2+||T_2(v)||^2+...+||T_k(v)||^2 ||T_i(v)||^2=0 i=1,2,...,k T_i=0 i=1,2,...,k A_i=0 i=1,2,...,k","['linear-algebra', 'matrices', 'solution-verification', 'vector-spaces']"
87,Linear maps that have the same matrix regardless of the bases chosen for domain and codomain,Linear maps that have the same matrix regardless of the bases chosen for domain and codomain,,"Question: Other than the zero map, what linear map has the same matrix $A_{E,F}$ with respect to all $E$ and $F$ ? For linear map $T:\mathbb{R}^n \rightarrow  \mathbb{R}^n$ , given a basis $E$ for domain and basis $F$ for codomain, I can find a unique corresponding matrix $A_{E,F}$ , where $A_{E,F}$ generally depends on $E$ and $F$ . Note that for any $E$ and $F$ , the matrix corresponding to the zero map is always the zero matrix, since the map sends all vectors in $E$ to $\mathbf{0}$ , and $\mathbf{0}$ can only be represented by $(0,...,0)$ with respect to any basis $F$ .","Question: Other than the zero map, what linear map has the same matrix with respect to all and ? For linear map , given a basis for domain and basis for codomain, I can find a unique corresponding matrix , where generally depends on and . Note that for any and , the matrix corresponding to the zero map is always the zero matrix, since the map sends all vectors in to , and can only be represented by with respect to any basis .","A_{E,F} E F T:\mathbb{R}^n \rightarrow  \mathbb{R}^n E F A_{E,F} A_{E,F} E F E F E \mathbf{0} \mathbf{0} (0,...,0) F","['linear-algebra', 'matrices', 'linear-transformations', 'change-of-basis', 'similar-matrices']"
88,Conditions for generalized projection matrix of size (2x2)? My results seem incorrect...,Conditions for generalized projection matrix of size (2x2)? My results seem incorrect...,,"I am trying to derive the general conditions that must be imposed upon the real-valued entries of a $2 \times 2$ orthogonal projection matrix. However, I am coming to a conclusion that seems wrong and hope someone can point me in the right direction. The prompt reads as follows: Find conditions on $a,b,c,d \in \mathbb{R}$ that guarantee the matrix $\begin{pmatrix} a & b \\ c & d\end{pmatrix}$ defines a rank-1 orthogonal projection. I begin by imposing the conditions that all projection matrices $P$ must fulfill, namely (i) $P = P^T$ (ii) $P = P^2$ Applying condition (i), $\begin{pmatrix} a & b \\ c & d\end{pmatrix} \overset{!}{=} \begin{pmatrix} a & c \\ b & d\end{pmatrix}$ I get $b=c$ and so I continue working with $\begin{pmatrix} a & c \\ c & d\end{pmatrix}$ and move on to condition (ii): $\begin{pmatrix} a & c \\ c & d\end{pmatrix} \overset{!}{=} \begin{pmatrix} a & c \\ c & d\end{pmatrix}\begin{pmatrix} a & c \\ c & d\end{pmatrix}=\begin{pmatrix} a^2 +c^2 & ac + cd \\ ac +cd & d^2 + c^2\end{pmatrix}$ From that, I get three equations: (1) $a = a^2 +c^2$ (2) $c=ac+cd=c(a+d)$ (3) $d=d^2 + c^2$ Further working out (2) by striking $c$ from each side, the system is then (1) $a = a^2 +c^2$ (2) $1=a+d$ (3) $d=d^2+c^2$ I re-arrange (1) and (3) to get (1) $c^2 = a - a^2$ (3) $c^2 = d - d^2$ and thus $a-a^2 = d-d^2$ from which I conclude that $a = d$ . Returning to equation (2), I then get (2) $1 = a+d = 2a$ and so $a = \frac{1}{2}$ and $d = \frac{1}{2}$ . Plugging either one of these into equation (1) or (3) then allows me to solve for $c$ , $\frac{1}{2} = \left ( \frac{1}{2} \right )^2+c^2$ or $c^2 = \frac{1}{4}$ And thus $c=\pm \frac{1}{2}$ . So, according to these results, to be guaranteed an orthogonal projection matrix of rank-1, my matrix $P$ must be one of two possibilities (as indicated by $\pm$ ): $P = \frac{1}{2} \begin{pmatrix} 1 & \pm 1 \\ \pm 1 & 1\end{pmatrix}$ This result is unsettling. First of all, I was expecting a broader range of possibilities for $a,b,c,d$ or at least for one or two variables. It seems odd that $a$ and $d$ are wholly constrained to one value and that $c$ only has two options. Is this correct? It seems like there should be more here that is possible. Aren't matrices such as $\begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix}$ or $\begin{pmatrix} 0 & 0 \\ 0 & 1 \end{pmatrix}$ also to be included here? For example, when I apply $\begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix}$ to a general vector $\begin{pmatrix} x \\ y \end{pmatrix}$ , it's easy for me to see the orthogonal projection: $\begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix} \begin{pmatrix} x \\ y\end{pmatrix} = \begin{pmatrix} x \\ 0\end{pmatrix}$ But when I apply the resulting projection matrix from above to the same generalized vector, I get a result that does reveal orthogonal projection to me (using $c = + \frac{1}{2}$ ): $\begin{pmatrix} \frac{1}{2} & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{2}\end{pmatrix} \begin{pmatrix} x \\ y\end{pmatrix} = \frac{1}{2} \begin{pmatrix} x+y \\ x+y\end{pmatrix}$ How can that be an orthogonal projection? Isn't that simply a change in length along the same direction? Am I not understanding the fundamental concept? Thanks for any help you might be able to provide.","I am trying to derive the general conditions that must be imposed upon the real-valued entries of a orthogonal projection matrix. However, I am coming to a conclusion that seems wrong and hope someone can point me in the right direction. The prompt reads as follows: Find conditions on that guarantee the matrix defines a rank-1 orthogonal projection. I begin by imposing the conditions that all projection matrices must fulfill, namely (i) (ii) Applying condition (i), I get and so I continue working with and move on to condition (ii): From that, I get three equations: (1) (2) (3) Further working out (2) by striking from each side, the system is then (1) (2) (3) I re-arrange (1) and (3) to get (1) (3) and thus from which I conclude that . Returning to equation (2), I then get (2) and so and . Plugging either one of these into equation (1) or (3) then allows me to solve for , or And thus . So, according to these results, to be guaranteed an orthogonal projection matrix of rank-1, my matrix must be one of two possibilities (as indicated by ): This result is unsettling. First of all, I was expecting a broader range of possibilities for or at least for one or two variables. It seems odd that and are wholly constrained to one value and that only has two options. Is this correct? It seems like there should be more here that is possible. Aren't matrices such as or also to be included here? For example, when I apply to a general vector , it's easy for me to see the orthogonal projection: But when I apply the resulting projection matrix from above to the same generalized vector, I get a result that does reveal orthogonal projection to me (using ): How can that be an orthogonal projection? Isn't that simply a change in length along the same direction? Am I not understanding the fundamental concept? Thanks for any help you might be able to provide.","2 \times 2 a,b,c,d \in \mathbb{R} \begin{pmatrix} a & b \\ c & d\end{pmatrix} P P = P^T P = P^2 \begin{pmatrix} a & b \\ c & d\end{pmatrix} \overset{!}{=} \begin{pmatrix} a & c \\ b & d\end{pmatrix} b=c \begin{pmatrix} a & c \\ c & d\end{pmatrix} \begin{pmatrix} a & c \\ c & d\end{pmatrix} \overset{!}{=} \begin{pmatrix} a & c \\ c & d\end{pmatrix}\begin{pmatrix} a & c \\ c & d\end{pmatrix}=\begin{pmatrix} a^2 +c^2 & ac + cd \\ ac +cd & d^2 + c^2\end{pmatrix} a = a^2 +c^2 c=ac+cd=c(a+d) d=d^2 + c^2 c a = a^2 +c^2 1=a+d d=d^2+c^2 c^2 = a - a^2 c^2 = d - d^2 a-a^2 = d-d^2 a = d 1 = a+d = 2a a = \frac{1}{2} d = \frac{1}{2} c \frac{1}{2} = \left ( \frac{1}{2} \right )^2+c^2 c^2 = \frac{1}{4} c=\pm \frac{1}{2} P \pm P = \frac{1}{2} \begin{pmatrix} 1 & \pm 1 \\ \pm 1 & 1\end{pmatrix} a,b,c,d a d c \begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix} \begin{pmatrix} 0 & 0 \\ 0 & 1 \end{pmatrix} \begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} \begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix} \begin{pmatrix} x \\ y\end{pmatrix} = \begin{pmatrix} x \\ 0\end{pmatrix} c = + \frac{1}{2} \begin{pmatrix} \frac{1}{2} & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{2}\end{pmatrix} \begin{pmatrix} x \\ y\end{pmatrix} = \frac{1}{2} \begin{pmatrix} x+y \\ x+y\end{pmatrix}","['linear-algebra', 'matrices', 'vector-spaces', 'vectors', 'projection']"
89,"""Second binomial formula"" for p.d. matrices","""Second binomial formula"" for p.d. matrices",,"Suppose $a,b$ are positive numbers, $a\neq b$ . Then, the relationship $$ \frac{1}{4}\left(\frac{1}{a}+\frac{1}{b}\right)>\frac{1}{a+b} $$ is true because we can rewrite it as $$ a+b>4\frac{ab}{a+b} $$ or $$ (a-b)^2>0 $$ My question: Is there a similar relationship for p.d. matrices (of suitable dimension so that addition and multiplication work), i.e., can one establish that $$ \frac{1}{4}\left(A^{-1}+B^{-1}\right)-(A+B)^{-1}>0? $$ With similar steps to the scalar case, I write the claim as $$ A+B-4B(A+B)^{-1}A>0, $$ from which the analogous steps to the scalar case do not directly go through anymore. Using, e.g., Inverse of the sum of matrices did not help me proceed, either. For context, the assertion would help me establish the general case here (I believe/hope that specific application has no properties that I do not mention in my question here): https://stats.stackexchange.com/questions/588398/help-partitioned-samples-efficiency-in-ols-compared-to-one-sample-regression","Suppose are positive numbers, . Then, the relationship is true because we can rewrite it as or My question: Is there a similar relationship for p.d. matrices (of suitable dimension so that addition and multiplication work), i.e., can one establish that With similar steps to the scalar case, I write the claim as from which the analogous steps to the scalar case do not directly go through anymore. Using, e.g., Inverse of the sum of matrices did not help me proceed, either. For context, the assertion would help me establish the general case here (I believe/hope that specific application has no properties that I do not mention in my question here): https://stats.stackexchange.com/questions/588398/help-partitioned-samples-efficiency-in-ols-compared-to-one-sample-regression","a,b a\neq b 
\frac{1}{4}\left(\frac{1}{a}+\frac{1}{b}\right)>\frac{1}{a+b}
 
a+b>4\frac{ab}{a+b}
 
(a-b)^2>0
 
\frac{1}{4}\left(A^{-1}+B^{-1}\right)-(A+B)^{-1}>0?
 
A+B-4B(A+B)^{-1}A>0,
","['matrices', 'positive-definite']"
90,Let $C \in \mathbb{R}^{4 \times 4}$ satisfy $C^3 + 6C = 5C^2$. Prove that $C$ is diagonalizable over the real numbers,Let  satisfy . Prove that  is diagonalizable over the real numbers,C \in \mathbb{R}^{4 \times 4} C^3 + 6C = 5C^2 C,"Question : Let $C \in \mathbb{R}^{4 \times 4}$ satisfy $C^3 + 6C = 5C^2$ .  Prove that $C$ is diagonalizable over the real numbers. My Attempt : If $C$ satisfies $C^3 + 6C = 5C^2$ then $C$ is a root of the polynomial $p(x) = x^3 - 5x^2 + 6x = x(x-2)(x-3)$ .  The minimal polynomial $m(x)$ for $C$ then must divide $p(x)$ .  Hence $$ m(x) \in \{x, x-2, x-3, x(x-2), x(x-3), (x-2)(x-3), x(x-2)(x-3) \}. $$ In any of these instances, the minimal polynomial for $C$ splits into linear factors over $\mathbb{R}$ , and each of the possibilities for $m(x)$ has distinct roots.  This implies that the matrix $C$ will always have distinct eigenvalues, and that $C$ can be diagonalized. Follow up question : How does the value of $n$ in $\mathbb{R}^{n \times n}$ affect the proof of this solution?  For instance, if $C$ were a 5 by 5 matrix instead, would the proof still hold?","Question : Let satisfy .  Prove that is diagonalizable over the real numbers. My Attempt : If satisfies then is a root of the polynomial .  The minimal polynomial for then must divide .  Hence In any of these instances, the minimal polynomial for splits into linear factors over , and each of the possibilities for has distinct roots.  This implies that the matrix will always have distinct eigenvalues, and that can be diagonalized. Follow up question : How does the value of in affect the proof of this solution?  For instance, if were a 5 by 5 matrix instead, would the proof still hold?","C \in \mathbb{R}^{4 \times 4} C^3 + 6C = 5C^2 C C C^3 + 6C = 5C^2 C p(x) = x^3 - 5x^2 + 6x = x(x-2)(x-3) m(x) C p(x) 
m(x) \in \{x, x-2, x-3, x(x-2), x(x-3), (x-2)(x-3), x(x-2)(x-3) \}.
 C \mathbb{R} m(x) C C n \mathbb{R}^{n \times n} C","['linear-algebra', 'abstract-algebra', 'matrices', 'solution-verification', 'diagonalization']"
91,"If two matrices A and B have the same determinant, are AB and BA similar?","If two matrices A and B have the same determinant, are AB and BA similar?",,"If two matrices A and B have the same determinant, are AB and BA similar? How would I prove it? I'm learning this topic for a data science course, and I don't have a mathematical background. We haven't covered eigenvalues and characteristic polynomials yet. It would be helpful if you can answer without using those concepts. Thanks","If two matrices A and B have the same determinant, are AB and BA similar? How would I prove it? I'm learning this topic for a data science course, and I don't have a mathematical background. We haven't covered eigenvalues and characteristic polynomials yet. It would be helpful if you can answer without using those concepts. Thanks",,"['linear-algebra', 'matrices']"
92,Infinite symmetrical matrix sum (discrete Lyapunov equation),Infinite symmetrical matrix sum (discrete Lyapunov equation),,"I have 2 symmetrical matrices ( $A$ and $B$ ) and I am looking to find the sum $S$ : $$S=A+BAB+B^2AB^2+\ldots$$ Or in summation format: $$S=\sum_{i=0}^\infty B^iAB^i$$ We know that the absolute magnitude of all the eigenvalues $\lambda_i$ of $B$ are less than $1$ so the sum converges. I tried to use the same trick as for a geometric sum but I can't factor out both $B$ 's at the same time. To simplify, we can assume that $B$ is diagonal but I assume that the result would hold for any shape. EDIT: It seems to be similar to the the discrete Lyapunov equation, with: $$BSB-S+A=0$$ as all matrices are symmetrical here. https://en.wikipedia.org/wiki/Lyapunov_equation","I have 2 symmetrical matrices ( and ) and I am looking to find the sum : Or in summation format: We know that the absolute magnitude of all the eigenvalues of are less than so the sum converges. I tried to use the same trick as for a geometric sum but I can't factor out both 's at the same time. To simplify, we can assume that is diagonal but I assume that the result would hold for any shape. EDIT: It seems to be similar to the the discrete Lyapunov equation, with: as all matrices are symmetrical here. https://en.wikipedia.org/wiki/Lyapunov_equation",A B S S=A+BAB+B^2AB^2+\ldots S=\sum_{i=0}^\infty B^iAB^i \lambda_i B 1 B B BSB-S+A=0,"['matrices', 'summation', 'geometric-series']"
93,Let $A = \bigl( \begin{smallmatrix}2 & 2 \\ -1 & 5\end{smallmatrix}\bigr)$. Prove for $n \geq 1$ using induction that $A^n =$ ...,Let . Prove for  using induction that  ...,A = \bigl( \begin{smallmatrix}2 & 2 \\ -1 & 5\end{smallmatrix}\bigr) n \geq 1 A^n =,"Can someone check to see if my proof is correct? Feel free to nitpick, trying to get better at writing proofs. Here's the problem: Let $A = \bigl( \begin{smallmatrix}2 & 2 \\ -1 & 5\end{smallmatrix}\bigr)$ . Prove for $n \geq 1$ that $A^n = 4^n \bigl( \begin{smallmatrix}-1 & 2 \\ -1 & 2\end{smallmatrix}\bigr) + 3^n \bigl( \begin{smallmatrix}2 & -2 \\ 1 & -1\end{smallmatrix}\bigr)$ by induction. Note: $A^n$ means $A$ matrix multiplied by itself $n$ times. So for example, $A^2 = \bigl( \begin{smallmatrix}2 & 2 \\ -1 & 5\end{smallmatrix}\bigr)\bigl( \begin{smallmatrix}2 & 2 \\ -1 & 5\end{smallmatrix}\bigr) = \bigl( \begin{smallmatrix}2 & 14 \\ -7 & 23\end{smallmatrix}\bigr)$ My attempt at the proof: Base case, $n=1:$ $A^1 = \bigl( \begin{smallmatrix}2 & 2 \\ -1 & 5\end{smallmatrix}\bigr) = 4^1\bigl( \begin{smallmatrix}-1 & 2 \\ -1 & 2\end{smallmatrix}\bigr)+ 3^1\bigl( \begin{smallmatrix}2 & -2 \\ 1 & -1\end{smallmatrix}\bigr) \ \checkmark$ Assume the inductive hypothesis, $n=k: A^k = 4^k\bigl( \begin{smallmatrix}-1 & 2 \\ -1 & 2\end{smallmatrix}\bigr) + 3^k\bigl( \begin{smallmatrix}2 & -2 \\ 1 & -1\end{smallmatrix}\bigr)$ WTS that the proposition holds for $n=k+1: A^{k+1} = 4^{k+1}\bigl( \begin{smallmatrix}-1 & 2 \\ -1 & 2\end{smallmatrix}\bigr) + 3^{k+1}\bigl( \begin{smallmatrix}2 & -2 \\ 1 & -1\end{smallmatrix}\bigr)$ \begin{align*} A^k A = \left(4^k\bigl( \begin{smallmatrix}-1 & 2 \\ -1 & 2\end{smallmatrix}\bigr) + 3^k\bigl( \begin{smallmatrix}2 & -2 \\ 1 & -1\end{smallmatrix}\bigr) \right)\bigl( \begin{smallmatrix}2 & 2 \\ -1 & 5\end{smallmatrix}\bigr) &= 4^k\bigl( \begin{smallmatrix}-1 & 2 \\ -1 & 2\end{smallmatrix}\bigr)\bigl( \begin{smallmatrix}2 & 2 \\ -1 & 5\end{smallmatrix}\bigr) + 3^k\bigl( \begin{smallmatrix}2 & -2 \\ 1 & -1\end{smallmatrix}\bigr)\bigl( \begin{smallmatrix}2 & 2 \\ -1 & 5\end{smallmatrix}\bigr) \\ &= 4^k\bigl( \begin{smallmatrix}-4 & 8 \\ -4 & 8\end{smallmatrix}\bigr) + 3^k\bigl( \begin{smallmatrix}6 & -6 \\ 3 & -3\end{smallmatrix}\bigr) \\ &= (4^k)(4)\bigl( \begin{smallmatrix}-1 & 2 \\ -1 & 2\end{smallmatrix}\bigr)  +  (3^k)(3)\bigl( \begin{smallmatrix}2 & -2 \\ 1 & -1\end{smallmatrix}\bigr) \\ A^k A &= 4^{k+1}\bigl( \begin{smallmatrix}-1 & 2 \\ -1 & 2\end{smallmatrix}\bigr) + 3^{k+1}\bigl( \begin{smallmatrix}2 & -2 \\ 1 & -1\end{smallmatrix}\bigr) \\ A^k A &= A^{k+1} \end{align*} By induction the proposition holds for all $n \geq 1$","Can someone check to see if my proof is correct? Feel free to nitpick, trying to get better at writing proofs. Here's the problem: Let . Prove for that by induction. Note: means matrix multiplied by itself times. So for example, My attempt at the proof: Base case, Assume the inductive hypothesis, WTS that the proposition holds for By induction the proposition holds for all","A = \bigl( \begin{smallmatrix}2 & 2 \\ -1 & 5\end{smallmatrix}\bigr) n \geq 1 A^n = 4^n \bigl( \begin{smallmatrix}-1 & 2 \\ -1 & 2\end{smallmatrix}\bigr) + 3^n \bigl( \begin{smallmatrix}2 & -2 \\ 1 & -1\end{smallmatrix}\bigr) A^n A n A^2 = \bigl( \begin{smallmatrix}2 & 2 \\ -1 & 5\end{smallmatrix}\bigr)\bigl( \begin{smallmatrix}2 & 2 \\ -1 & 5\end{smallmatrix}\bigr) = \bigl( \begin{smallmatrix}2 & 14 \\ -7 & 23\end{smallmatrix}\bigr) n=1: A^1 = \bigl( \begin{smallmatrix}2 & 2 \\ -1 & 5\end{smallmatrix}\bigr) = 4^1\bigl( \begin{smallmatrix}-1 & 2 \\ -1 & 2\end{smallmatrix}\bigr)+ 3^1\bigl( \begin{smallmatrix}2 & -2 \\ 1 & -1\end{smallmatrix}\bigr) \ \checkmark n=k: A^k = 4^k\bigl( \begin{smallmatrix}-1 & 2 \\ -1 & 2\end{smallmatrix}\bigr) + 3^k\bigl( \begin{smallmatrix}2 & -2 \\ 1 & -1\end{smallmatrix}\bigr) n=k+1: A^{k+1} = 4^{k+1}\bigl( \begin{smallmatrix}-1 & 2 \\ -1 & 2\end{smallmatrix}\bigr) + 3^{k+1}\bigl( \begin{smallmatrix}2 & -2 \\ 1 & -1\end{smallmatrix}\bigr) \begin{align*}
A^k A = \left(4^k\bigl( \begin{smallmatrix}-1 & 2 \\ -1 & 2\end{smallmatrix}\bigr) + 3^k\bigl( \begin{smallmatrix}2 & -2 \\ 1 & -1\end{smallmatrix}\bigr)
\right)\bigl( \begin{smallmatrix}2 & 2 \\ -1 & 5\end{smallmatrix}\bigr)
&= 4^k\bigl( \begin{smallmatrix}-1 & 2 \\ -1 & 2\end{smallmatrix}\bigr)\bigl( \begin{smallmatrix}2 & 2 \\ -1 & 5\end{smallmatrix}\bigr) + 3^k\bigl( \begin{smallmatrix}2 & -2 \\ 1 & -1\end{smallmatrix}\bigr)\bigl( \begin{smallmatrix}2 & 2 \\ -1 & 5\end{smallmatrix}\bigr) \\
&= 4^k\bigl( \begin{smallmatrix}-4 & 8 \\ -4 & 8\end{smallmatrix}\bigr)
+ 3^k\bigl( \begin{smallmatrix}6 & -6 \\ 3 & -3\end{smallmatrix}\bigr) \\
&= (4^k)(4)\bigl( \begin{smallmatrix}-1 & 2 \\ -1 & 2\end{smallmatrix}\bigr)  + 
(3^k)(3)\bigl( \begin{smallmatrix}2 & -2 \\ 1 & -1\end{smallmatrix}\bigr) \\
A^k A &= 4^{k+1}\bigl( \begin{smallmatrix}-1 & 2 \\ -1 & 2\end{smallmatrix}\bigr) + 3^{k+1}\bigl( \begin{smallmatrix}2 & -2 \\ 1 & -1\end{smallmatrix}\bigr) \\
A^k A &= A^{k+1}
\end{align*} n \geq 1","['matrices', 'solution-verification', 'induction']"
94,Is the space of $n\times n$ real symmetric matrices with strictly positive determinant connected within the vector space of $n\times n$ real matrices?,Is the space of  real symmetric matrices with strictly positive determinant connected within the vector space of  real matrices?,n\times n n\times n,"I want to make clear that I am aware of the connectedness in the case of general real matrices. But here I ask about the subspace of symmetric ones. If it is not the case, which are the connected components of such topological space? If it is the case, what would be the path on such space connecting say a signature matrix $J$ with positive determinant with the identity $I$ ? That is, give a nontrivial path of symmetric matrices with positive determinant from some signature matrix $J\neq I$ with $\det(J)=1>0$ to the identity $I$ . Remember that a signature matrix is a diagonal matrix whose diagonal entries belong to $\{-1,1\}$ . Note also that, as this matrix in my question has to have positive determinant $-1$ appears an even number of times in such diagonal.","I want to make clear that I am aware of the connectedness in the case of general real matrices. But here I ask about the subspace of symmetric ones. If it is not the case, which are the connected components of such topological space? If it is the case, what would be the path on such space connecting say a signature matrix with positive determinant with the identity ? That is, give a nontrivial path of symmetric matrices with positive determinant from some signature matrix with to the identity . Remember that a signature matrix is a diagonal matrix whose diagonal entries belong to . Note also that, as this matrix in my question has to have positive determinant appears an even number of times in such diagonal.","J I J\neq I \det(J)=1>0 I \{-1,1\} -1","['linear-algebra', 'matrices', 'connectedness', 'topological-vector-spaces', 'symmetric-matrices']"
95,Finding all square roots of a matrix when it has distinct eigenvalues,Finding all square roots of a matrix when it has distinct eigenvalues,,"According to https://en.wikipedia.org/wiki/Square_root_of_a_matrix#Matrices_with_distinct_eigenvalues , ""An n×n matrix with $n$ distinct nonzero eigenvalues has $2^n$ square roots."" And we can clearly see that they are given by decomposing $A$ into $SDS^{-1}$ where $D$ is diagonal and taking square roots of the eigenvalues; this generates $2^n$ pairwise distinct square roots. But how did we prove that these are the only matrices $B$ such that $B^2 = A$ ?","According to https://en.wikipedia.org/wiki/Square_root_of_a_matrix#Matrices_with_distinct_eigenvalues , ""An n×n matrix with distinct nonzero eigenvalues has square roots."" And we can clearly see that they are given by decomposing into where is diagonal and taking square roots of the eigenvalues; this generates pairwise distinct square roots. But how did we prove that these are the only matrices such that ?",n 2^n A SDS^{-1} D 2^n B B^2 = A,"['linear-algebra', 'matrices']"
96,3D rotation matrix around a point (not origin),3D rotation matrix around a point (not origin),,"I'm trying to find the rotation matrix for when rotating around a point that is not origin. I read another post and the post had said that we can think about moving the point back to the origin, but I think the question may have been about 2D. (or referring to a situation where the point is (a,b,0) where a,b != 0) I'm not sure how the method will apply to 3D situation or a situation where the point is (a,b,c) where all of a,b,c are nonzero. I would really appreciate help!","I'm trying to find the rotation matrix for when rotating around a point that is not origin. I read another post and the post had said that we can think about moving the point back to the origin, but I think the question may have been about 2D. (or referring to a situation where the point is (a,b,0) where a,b != 0) I'm not sure how the method will apply to 3D situation or a situation where the point is (a,b,c) where all of a,b,c are nonzero. I would really appreciate help!",,"['matrices', 'rotations']"
97,Concepts behind augmented matrix,Concepts behind augmented matrix,,"Apparently the strategy for finding a general solution to the equation $Ax = v$ , where $A$ is some $m\times n$ matrix, $x$ is an unknown vector of dimension n, and $v$ is not the zero vector, is to turn $v$ into an extra column of A (resulting in an augmented matrix, $aA$ ) and then simply proceed with the usual technique on $aA$ by putting $aA$ into row-echelon form. But why does this work ? If I try to answer this question myself, the answer I give is because $Ax = v$ represents a system of equations, and moving $v$ from the right hand side of this system to the left does not change it. But in order to actually do that I think you'd have to subtract $v$ from both sides of the system, which would mean you should have $-v$ as the extra column in $aA$ and not $v$ ; but this is not how it works. Where does my reasoning go wrong? What is the correct explanation for why performing row operations on $aA$ gives you the general solution to $A$ ?","Apparently the strategy for finding a general solution to the equation , where is some matrix, is an unknown vector of dimension n, and is not the zero vector, is to turn into an extra column of A (resulting in an augmented matrix, ) and then simply proceed with the usual technique on by putting into row-echelon form. But why does this work ? If I try to answer this question myself, the answer I give is because represents a system of equations, and moving from the right hand side of this system to the left does not change it. But in order to actually do that I think you'd have to subtract from both sides of the system, which would mean you should have as the extra column in and not ; but this is not how it works. Where does my reasoning go wrong? What is the correct explanation for why performing row operations on gives you the general solution to ?",Ax = v A m\times n x v v aA aA aA Ax = v v v -v aA v aA A,"['linear-algebra', 'matrices', 'systems-of-equations']"
98,Eigenvalues of symmetric matrix AB,Eigenvalues of symmetric matrix AB,,"If matrices $A$ and $B$ are symmetric and matrix $AB$ is also symmetric, then show that every eigenvalue of $AB$ can be written as one eigenvalue of $A$ times one eigenvalue of $B$ . I tried so much but I could not find a good answer for that","If matrices and are symmetric and matrix is also symmetric, then show that every eigenvalue of can be written as one eigenvalue of times one eigenvalue of . I tried so much but I could not find a good answer for that",A B AB AB A B,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'symmetric-matrices']"
99,Derivative of Mahalanobis pairwise distance matrix respect to transformation matrix,Derivative of Mahalanobis pairwise distance matrix respect to transformation matrix,,"For a set of vectorial observations $x_i$ stored in the matrix $X$ , I would like to obtain the gradient of the pairwise Mahalanobis distance matrix $D$ with respect to the Mahalanobis transformation matrix $M$ ( $\frac{\partial D}{\partial M}$ ), given that: $D_{ij} = (x_i - x_j)^TM(x_i - x_j)$ edit: M should be positive semi-definite, or alternatively $M = A^T A$ I have found how the euclidean pairwise distance matrix can be expressed as in a vectorized  form ( Pairwise distance matrix ), and how the Mahalanobis distance between two elements can be derived with respect to the matrix M ( Differentiating mahalanobis distance ), but I have not been able to put everything together.","For a set of vectorial observations stored in the matrix , I would like to obtain the gradient of the pairwise Mahalanobis distance matrix with respect to the Mahalanobis transformation matrix ( ), given that: edit: M should be positive semi-definite, or alternatively I have found how the euclidean pairwise distance matrix can be expressed as in a vectorized  form ( Pairwise distance matrix ), and how the Mahalanobis distance between two elements can be derived with respect to the matrix M ( Differentiating mahalanobis distance ), but I have not been able to put everything together.",x_i X D M \frac{\partial D}{\partial M} D_{ij} = (x_i - x_j)^TM(x_i - x_j) M = A^T A,"['matrices', 'optimization', 'matrix-calculus', 'machine-learning', 'mahalanobis-distance']"
