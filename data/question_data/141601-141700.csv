,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Not sure about the limit of $\frac{\log(1+x^4)}{x^2 \tan^2(2x)}$ for $x$ that goes to zero,Not sure about the limit of  for  that goes to zero,\frac{\log(1+x^4)}{x^2 \tan^2(2x)} x,"I have to find the value of the function $\frac{\log(1+x^4)}{x^2 \tan^2(2x)}$ for $x$ that goes to zero. It's an undefined form $\frac{0}{0}$ so I transformed the function in this way in order to use two known limits $$\frac{\log(1+x^4)}{x^2} \frac{x^2}{x^2} \frac{1}{\tan^2(2x)} \frac{4x^2}{4x^2} $$ and so $$\frac{\log(1+x^4)}{x^4}\frac{4x^2}{\tan^2(2x)}\frac{x^2}{4x^2} $$ It seems to me that the value of the limit in zero is $\frac{1}{4}$ but looking to the graph of the function (using Desmos) it do not have a clear behavior in zero. Can you tell me if my reasoning is right or wrong and, if wrong, where is the mistake? Thank you. ps. I found the problem with Desmos. In my notation I was using log for the logarithm with e as the base. But I should use ln in Desmos for the same logarithm, which otherwise is 10 based.","I have to find the value of the function for that goes to zero. It's an undefined form so I transformed the function in this way in order to use two known limits and so It seems to me that the value of the limit in zero is but looking to the graph of the function (using Desmos) it do not have a clear behavior in zero. Can you tell me if my reasoning is right or wrong and, if wrong, where is the mistake? Thank you. ps. I found the problem with Desmos. In my notation I was using log for the logarithm with e as the base. But I should use ln in Desmos for the same logarithm, which otherwise is 10 based.",\frac{\log(1+x^4)}{x^2 \tan^2(2x)} x \frac{0}{0} \frac{\log(1+x^4)}{x^2} \frac{x^2}{x^2} \frac{1}{\tan^2(2x)} \frac{4x^2}{4x^2}  \frac{\log(1+x^4)}{x^4}\frac{4x^2}{\tan^2(2x)}\frac{x^2}{4x^2}  \frac{1}{4},"['real-analysis', 'limits']"
1,"Are there other, non-probabilistic ways to calculate: $\lim_{n\to\infty}\frac{1}{n}\ln\sum_{m>n\alpha}\frac{(n\lambda)^m}{m!}$?","Are there other, non-probabilistic ways to calculate: ?",\lim_{n\to\infty}\frac{1}{n}\ln\sum_{m>n\alpha}\frac{(n\lambda)^m}{m!},"In section five of this nice exposition of moment generating functions, we prove the following theorem: Take an i.i.d sequence of random variables $(X_n:\Omega\to\Bbb R)_{n\in\Bbb N}$ whose common cumulant generating function $\Lambda$ is finite in a neighbourhood of the origin (the value $+\infty$ is permitted in general). Let $X:=X_1$ . Define $\Lambda^\ast$ to be the ""Fenchel-Legendre transform"" of $\Lambda$ : $$\Lambda^\ast:\Bbb R\to\overline{\Bbb R},\,x\mapsto\sup_{t\in\Bbb R}(x\cdot t-\Lambda(t))$$ And define for every $n\in\Bbb N$ the empirical means: $$Y_n:=\frac{1}{n}\sum_{j=1}^nX_j:\Omega\to\Bbb R$$ If $\alpha>\mathbb{E}(X)$ and $\mathrm{Pr}(X>\alpha)>0$ then $0<\Lambda^\ast(\alpha)<\infty$ and we get the asymptotics of the ""large deviations"": $$\frac{1}{n}\ln\mathrm{Pr}(Y_n>\alpha)\overset{n\to\infty}{\longrightarrow}-\Lambda^\ast(\alpha)=\inf_{t>0}(\Lambda(t)-\alpha\cdot t)$$ With convergence from below. The proof is - to me - a fairly complex and delicate application of probability theory. I don't yet know enough to follow it all the way through. I wanted to ""see it in action"" so I explicitly calculated $\Lambda^\ast$ for the Poisson distribution. Let's fix some $\lambda>0$ . If we take $(X_n)_n$ which follow the Poisson distribution with parameter $\lambda$ , it is easy to check that $\Lambda(t)=\lambda(e^t-1)$ for all $t$ . Some basic calculus optimisation will find: $$\Lambda^\ast(\alpha)=\lambda+\alpha(\ln(\alpha\cdot\lambda^{-1})-1)$$ Whenever $\alpha>\lambda$ . So the theorem predicts that: $$\lim_{n\to\infty}\frac{1}{n}\ln\left(\sum_{m>n\alpha}\frac{(n\lambda)^m}{m!}e^{-n\lambda}\right)=-\lambda+\alpha(1-\ln(\alpha\cdot\lambda^{-1}))$$ A little rearranging brings that to: $$\tag{$\ast$}\forall\,\,0<\lambda<\alpha:\quad\quad\quad\lim_{n\to\infty}\frac{1}{n}\ln\left(\sum_{m>n\alpha}\frac{(n\lambda)^m}{m!}\right)=\alpha(1-\ln(\alpha\cdot\lambda^{-1}))$$ Which seems highly nontrivial to do by other means. I am very curious to see whether or not anyone on the site has the expertise to supply a ""real analytic"" proof of this identity. I know that the probabilistic proof is just a real analytic proof with a particular flavour, but I mean to ask if there are other ways of doing this which aren't purely motivated by ideas from probability or measure theory (the proof seems to involve a ""change-of-measure"" trick, which is a new one on me). Or, if this special Poisson case admits a simpler probabilistic proof, that would be interesting to see too. I think this special case is an interesting problem. Since I don't fully understand the proof I've already seen, I reckon that verifying $(\ast)$ is way above my paygrade. I hope to learn from your answers :) Just one thing I can helpfully observe: By the Stolz-Cesaro theorem, it would suffice to show that (although I do not know if this is true): $$\lim_{n\to\infty}\frac{\sum_{m>(n+1)\alpha}\frac{((n+1)\lambda)^m}{m!}}{\sum_{m>n\alpha}\frac{(n\lambda)^m}{m!}}=\left(\frac{\lambda e}{\alpha}\right)^\alpha$$ Oh, and a related cool (I think) expression can be deduced by applying the same technique to a sequence of binomial variable variables with parameters $n,1/2$ : If $n\in\Bbb N$ and $\frac{n}{2}<\alpha<n$ then: $$\lim_{m\to\infty}\frac{1}{m}\ln\left(\sum_{k>m\alpha}\binom{mn}{k}\right)=n\ln n-\alpha\ln\alpha-(n-\alpha)\ln(n-\alpha)$$ More generally, if $0<p<1$ , $n\in\Bbb N$ and $np<\alpha<n$ : $$\lim_{m\to\infty}\frac{1}{m}\ln\left(\sum_{k>m\alpha}\binom{mn}{k}\left(\frac{p}{1-p}\right)^k\right)=\alpha\ln\left(\frac{p}{1-p}\right)+n\ln n-\alpha\ln\alpha-(n-\alpha)\ln(n-\alpha)$$ This also seems quite nontrivial without this “master theorem”. I would be equally interested to see a proof of that by other means.","In section five of this nice exposition of moment generating functions, we prove the following theorem: Take an i.i.d sequence of random variables whose common cumulant generating function is finite in a neighbourhood of the origin (the value is permitted in general). Let . Define to be the ""Fenchel-Legendre transform"" of : And define for every the empirical means: If and then and we get the asymptotics of the ""large deviations"": With convergence from below. The proof is - to me - a fairly complex and delicate application of probability theory. I don't yet know enough to follow it all the way through. I wanted to ""see it in action"" so I explicitly calculated for the Poisson distribution. Let's fix some . If we take which follow the Poisson distribution with parameter , it is easy to check that for all . Some basic calculus optimisation will find: Whenever . So the theorem predicts that: A little rearranging brings that to: Which seems highly nontrivial to do by other means. I am very curious to see whether or not anyone on the site has the expertise to supply a ""real analytic"" proof of this identity. I know that the probabilistic proof is just a real analytic proof with a particular flavour, but I mean to ask if there are other ways of doing this which aren't purely motivated by ideas from probability or measure theory (the proof seems to involve a ""change-of-measure"" trick, which is a new one on me). Or, if this special Poisson case admits a simpler probabilistic proof, that would be interesting to see too. I think this special case is an interesting problem. Since I don't fully understand the proof I've already seen, I reckon that verifying is way above my paygrade. I hope to learn from your answers :) Just one thing I can helpfully observe: By the Stolz-Cesaro theorem, it would suffice to show that (although I do not know if this is true): Oh, and a related cool (I think) expression can be deduced by applying the same technique to a sequence of binomial variable variables with parameters : If and then: More generally, if , and : This also seems quite nontrivial without this “master theorem”. I would be equally interested to see a proof of that by other means.","(X_n:\Omega\to\Bbb R)_{n\in\Bbb N} \Lambda +\infty X:=X_1 \Lambda^\ast \Lambda \Lambda^\ast:\Bbb R\to\overline{\Bbb R},\,x\mapsto\sup_{t\in\Bbb R}(x\cdot t-\Lambda(t)) n\in\Bbb N Y_n:=\frac{1}{n}\sum_{j=1}^nX_j:\Omega\to\Bbb R \alpha>\mathbb{E}(X) \mathrm{Pr}(X>\alpha)>0 0<\Lambda^\ast(\alpha)<\infty \frac{1}{n}\ln\mathrm{Pr}(Y_n>\alpha)\overset{n\to\infty}{\longrightarrow}-\Lambda^\ast(\alpha)=\inf_{t>0}(\Lambda(t)-\alpha\cdot t) \Lambda^\ast \lambda>0 (X_n)_n \lambda \Lambda(t)=\lambda(e^t-1) t \Lambda^\ast(\alpha)=\lambda+\alpha(\ln(\alpha\cdot\lambda^{-1})-1) \alpha>\lambda \lim_{n\to\infty}\frac{1}{n}\ln\left(\sum_{m>n\alpha}\frac{(n\lambda)^m}{m!}e^{-n\lambda}\right)=-\lambda+\alpha(1-\ln(\alpha\cdot\lambda^{-1})) \tag{\ast}\forall\,\,0<\lambda<\alpha:\quad\quad\quad\lim_{n\to\infty}\frac{1}{n}\ln\left(\sum_{m>n\alpha}\frac{(n\lambda)^m}{m!}\right)=\alpha(1-\ln(\alpha\cdot\lambda^{-1})) (\ast) \lim_{n\to\infty}\frac{\sum_{m>(n+1)\alpha}\frac{((n+1)\lambda)^m}{m!}}{\sum_{m>n\alpha}\frac{(n\lambda)^m}{m!}}=\left(\frac{\lambda e}{\alpha}\right)^\alpha n,1/2 n\in\Bbb N \frac{n}{2}<\alpha<n \lim_{m\to\infty}\frac{1}{m}\ln\left(\sum_{k>m\alpha}\binom{mn}{k}\right)=n\ln n-\alpha\ln\alpha-(n-\alpha)\ln(n-\alpha) 0<p<1 n\in\Bbb N np<\alpha<n \lim_{m\to\infty}\frac{1}{m}\ln\left(\sum_{k>m\alpha}\binom{mn}{k}\left(\frac{p}{1-p}\right)^k\right)=\alpha\ln\left(\frac{p}{1-p}\right)+n\ln n-\alpha\ln\alpha-(n-\alpha)\ln(n-\alpha)","['real-analysis', 'limits']"
2,Computing the limit of $ \frac{1^3+2^3+\cdots+n^3}{\sqrt{4n^8 +1}} $,Computing the limit of, \frac{1^3+2^3+\cdots+n^3}{\sqrt{4n^8 +1}} ,I had this exercise: Compute the limit $$ \lim_{n\to\infty} \frac{1^3+2^3+\cdots+n^3}{\sqrt{4n^8 +1}} $$ I tried two different approaches and got different answers. Approach 1: $$\begin{split} \lim_{n\to\infty} \frac{1^3+2^3+\cdots+n^3}{n^4\sqrt{4 +1/n^8}}&=\lim_{n\to\infty} \frac{\frac{1^3}{n^4}+\frac{2^3}{n^4}+\cdots+\frac{n^3}{n^4}}{\sqrt{4 +1/n^8}}\\ &=\lim_{n\to\infty} \frac{\frac{1^3}{n^4}+\frac{2^3}{n^4}+\cdots+\frac{1}{n}}{\sqrt{4 +1/n^8}}\\ &= \frac{0}{\sqrt{4+0}}\\ &= 0\\ \end{split}$$ Approach 2: We substitute sum of cubes of $n$ natural numbers in the numerator and get $$\begin{split} \lim_{n\to\infty} \frac{n^2(n+1)^2}{4n^4\sqrt{4 +1/n^8}}&= \lim_{n\to\infty} \frac{n^2(n+1)^2}{4n^4\sqrt{4 +1/n^8}}\\ &= \lim_{n\to\infty} \frac{n^4(1+\frac{1}{n})^2}{4n^4\sqrt{4 +1/n^8}}\\ &= \lim_{n\to\infty} \frac{(1+\frac{1}{n})^2}{4\sqrt{4 +1/n^8}}\\  &= \lim_{n\to\infty} \frac{1}{4\sqrt{4}}\\ &= \frac{1}{8}\\ \end{split}$$ I'm not sure why the two methods are giving me different answers and which one is correct?,I had this exercise: Compute the limit I tried two different approaches and got different answers. Approach 1: Approach 2: We substitute sum of cubes of natural numbers in the numerator and get I'm not sure why the two methods are giving me different answers and which one is correct?," \lim_{n\to\infty} \frac{1^3+2^3+\cdots+n^3}{\sqrt{4n^8 +1}}  \begin{split}
\lim_{n\to\infty} \frac{1^3+2^3+\cdots+n^3}{n^4\sqrt{4 +1/n^8}}&=\lim_{n\to\infty} \frac{\frac{1^3}{n^4}+\frac{2^3}{n^4}+\cdots+\frac{n^3}{n^4}}{\sqrt{4 +1/n^8}}\\
&=\lim_{n\to\infty} \frac{\frac{1^3}{n^4}+\frac{2^3}{n^4}+\cdots+\frac{1}{n}}{\sqrt{4 +1/n^8}}\\
&= \frac{0}{\sqrt{4+0}}\\
&= 0\\
\end{split} n \begin{split}
\lim_{n\to\infty} \frac{n^2(n+1)^2}{4n^4\sqrt{4 +1/n^8}}&= \lim_{n\to\infty} \frac{n^2(n+1)^2}{4n^4\sqrt{4 +1/n^8}}\\
&= \lim_{n\to\infty} \frac{n^4(1+\frac{1}{n})^2}{4n^4\sqrt{4 +1/n^8}}\\
&= \lim_{n\to\infty} \frac{(1+\frac{1}{n})^2}{4\sqrt{4 +1/n^8}}\\ 
&= \lim_{n\to\infty} \frac{1}{4\sqrt{4}}\\
&= \frac{1}{8}\\
\end{split}","['real-analysis', 'calculus', 'limits', 'limits-without-lhopital']"
3,Find the value of $\lim _{x\to \infty }\left(1+\frac{4}{\ln x}\right)^{\ln\left(x^2+x\right)}$,Find the value of,\lim _{x\to \infty }\left(1+\frac{4}{\ln x}\right)^{\ln\left(x^2+x\right)},"So this is my attempt to the question: Let, $$y=\left(1+\frac{4}{\ln x}\right)^{\ln\left(x^2+x\right)}$$ $$\ln y=\ln(x^2+x)\ln\left(1+\frac{4}{\ln x}\right)$$ $$\lim _{x\to \infty }\ln y=\lim _{x\to \infty }\ln(x^2+x)\ln\left(1+\frac{4}{\ln x}\right)$$ Solving for the right side, since it is in the form of $\infty\cdot0$ , we can change this into $$\lim _{x\to \infty }\frac{\ln(x^2+x)}{\frac{1}{\ln\left(1+\frac{4}{\ln x}\right)}}$$ So that we can then use the L'Hopital rule. However, after using the L'Hopital rule once, I found the form to be even more complicated and still becomes an indeterminate form. Is there another way to solve this problem?","So this is my attempt to the question: Let, Solving for the right side, since it is in the form of , we can change this into So that we can then use the L'Hopital rule. However, after using the L'Hopital rule once, I found the form to be even more complicated and still becomes an indeterminate form. Is there another way to solve this problem?",y=\left(1+\frac{4}{\ln x}\right)^{\ln\left(x^2+x\right)} \ln y=\ln(x^2+x)\ln\left(1+\frac{4}{\ln x}\right) \lim _{x\to \infty }\ln y=\lim _{x\to \infty }\ln(x^2+x)\ln\left(1+\frac{4}{\ln x}\right) \infty\cdot0 \lim _{x\to \infty }\frac{\ln(x^2+x)}{\frac{1}{\ln\left(1+\frac{4}{\ln x}\right)}},"['calculus', 'limits']"
4,What is $\lim_{n\to\infty} n (\sum_{k = 1}^{n} \frac{1}{\sqrt {n^2 + k}} - 1)$?,What is ?,\lim_{n\to\infty} n (\sum_{k = 1}^{n} \frac{1}{\sqrt {n^2 + k}} - 1),"I am trying to find this limit: $\lim_{n \to \infty} a_n^{b_n}$ , where $a_n = \sum_{k = 1}^{n} \frac{1}{\sqrt{n^2+ k}}$ and $b_n = n$ . I have managed to prove that $\lim_{n \to \infty} a_n = 1$ , by using the squeeze lemma (bounding $a_n$ between $1$ and $\frac{n}{\sqrt {n^2 + n}}$ ) and I want to use this theorem: if $\lim_{n \to \infty} = 0$ , then $\lim_{n\to\infty} (1 + a_n) ^ {\frac{1}{a_n}} = e$ . To apply this theorem, I did: $$\lim_{n\to\infty}a_n^{b_n} = \lim_{n\to\infty}((1 + a_n - 1) ^ \frac{1}{a_n - 1}) ^ {b_n(a_n - 1)} = e^{\lim_{n\to\infty}b_n(a_n - 1)}$$ . Now I am having problems finding the limit in the exponent. Could you help?","I am trying to find this limit: , where and . I have managed to prove that , by using the squeeze lemma (bounding between and ) and I want to use this theorem: if , then . To apply this theorem, I did: . Now I am having problems finding the limit in the exponent. Could you help?",\lim_{n \to \infty} a_n^{b_n} a_n = \sum_{k = 1}^{n} \frac{1}{\sqrt{n^2+ k}} b_n = n \lim_{n \to \infty} a_n = 1 a_n 1 \frac{n}{\sqrt {n^2 + n}} \lim_{n \to \infty} = 0 \lim_{n\to\infty} (1 + a_n) ^ {\frac{1}{a_n}} = e \lim_{n\to\infty}a_n^{b_n} = \lim_{n\to\infty}((1 + a_n - 1) ^ \frac{1}{a_n - 1}) ^ {b_n(a_n - 1)} = e^{\lim_{n\to\infty}b_n(a_n - 1)},"['calculus', 'limits']"
5,"$\lim_{x\to\infty}\left(1+\frac{1}{x}\right)^{x} = e,\ $ and $\lim_{x\to\infty}\left(1-\frac{1}{x}\right)^{x} = \frac{1}{e}.\ $ Is this generalisable?",and  Is this generalisable?,"\lim_{x\to\infty}\left(1+\frac{1}{x}\right)^{x} = e,\  \lim_{x\to\infty}\left(1-\frac{1}{x}\right)^{x} = \frac{1}{e}.\ ","Suppose $f(x)\to\infty,\ g(x)\to\infty,\ $ both as $\ x\to\infty,\ $ and $\lim_{x\to\infty}\left(1+\frac{1}{f(x)}\right)^{g(x)} = c>1.\ $ Then does $\lim_{x\to\infty}\left(1-\frac{1}{f(x)}\right)^{g(x)} = \frac{1}{c}\ ?$ The proof I know/like for $\ f(x) = x\ $ and $\ g(x)=x\ $ is as follows: Suppose $\lim_{x\to\infty}\left(1+\frac{1}{x}\right)^{x} = e.\ $ Then \begin{align} \lim_{x\to\infty}\frac{1}{\left(1-\frac{1}{x}\right)^x}=\lim_{x\to\infty}\left(\frac{x}{x-1}\right)^x=\lim_{(x+1)\to\infty}\left(\frac{x+1}{x}\right)^{x+1}\\ \\ =\lim_{(x+1)\to\infty}\left(\left(\frac{x+1}{x}\right)^x\ \cdot\ \left(\frac{x+1}{x}\right)\right)=\lim_{x\to\infty}\left(\left(1+\frac{1}{x}\right)^x\ \cdot\ \left(1+\frac{1}{x}\right)\right) = e\cdot1 = e\\ \end{align} and we are done. I tried the same method with the generalisation: Suppose $\lim_{x\to\infty}\left(1+\frac{1}{f(x)}\right)^{g(x)} = c>1.\ $ Then $$ \lim_{x\to\infty}\frac{1}{\left(1-\frac{1}{f(x)}\right)^{g(x)}}=\lim_{x\to\infty}\left(\frac{f(x)}{f(x)-1}\right)^{g(x)}=\lim_{(x+1)\to\infty}\left(\frac{f(x+1)}{f(x+1)-1}\right)^{g(x+1)}=\ ?$$ Maybe this works with some Binomial or Taylor expansion, but I doubt it? Also I'm not even sure if the result it true...","Suppose both as and Then does The proof I know/like for and is as follows: Suppose Then and we are done. I tried the same method with the generalisation: Suppose Then Maybe this works with some Binomial or Taylor expansion, but I doubt it? Also I'm not even sure if the result it true...","f(x)\to\infty,\ g(x)\to\infty,\  \ x\to\infty,\  \lim_{x\to\infty}\left(1+\frac{1}{f(x)}\right)^{g(x)} = c>1.\  \lim_{x\to\infty}\left(1-\frac{1}{f(x)}\right)^{g(x)} = \frac{1}{c}\ ? \ f(x) = x\  \ g(x)=x\  \lim_{x\to\infty}\left(1+\frac{1}{x}\right)^{x} = e.\  \begin{align} \lim_{x\to\infty}\frac{1}{\left(1-\frac{1}{x}\right)^x}=\lim_{x\to\infty}\left(\frac{x}{x-1}\right)^x=\lim_{(x+1)\to\infty}\left(\frac{x+1}{x}\right)^{x+1}\\
\\
=\lim_{(x+1)\to\infty}\left(\left(\frac{x+1}{x}\right)^x\ \cdot\ \left(\frac{x+1}{x}\right)\right)=\lim_{x\to\infty}\left(\left(1+\frac{1}{x}\right)^x\ \cdot\ \left(1+\frac{1}{x}\right)\right) = e\cdot1 = e\\
\end{align} \lim_{x\to\infty}\left(1+\frac{1}{f(x)}\right)^{g(x)} = c>1.\   \lim_{x\to\infty}\frac{1}{\left(1-\frac{1}{f(x)}\right)^{g(x)}}=\lim_{x\to\infty}\left(\frac{f(x)}{f(x)-1}\right)^{g(x)}=\lim_{(x+1)\to\infty}\left(\frac{f(x+1)}{f(x+1)-1}\right)^{g(x+1)}=\ ?","['real-analysis', 'limits', 'functions', 'taylor-expansion']"
6,Continuously extend this function to the unit sphere.,Continuously extend this function to the unit sphere.,,"Given a nonzero vector $u$ , I can define the map $$ f_{u} : S^2 \to \mathbb{R}^3 \\ f_{u}(x) = \frac{u-(u\cdot x)x}{({(u \cdot x)^2}/(u\cdot u))-1} $$ which is well-defined everywhere except when $u$ and $x$ are parallel. So, I have two questions: Does $f_{u}$ have limits at those two points where it is not well-defined, and what are those limits? Is there a simple geometric interpretation of what $f_{u}$ does to the unit sphere?","Given a nonzero vector , I can define the map which is well-defined everywhere except when and are parallel. So, I have two questions: Does have limits at those two points where it is not well-defined, and what are those limits? Is there a simple geometric interpretation of what does to the unit sphere?","u 
f_{u} : S^2 \to \mathbb{R}^3 \\
f_{u}(x) = \frac{u-(u\cdot x)x}{({(u \cdot x)^2}/(u\cdot u))-1}
 u x f_{u} f_{u}","['limits', 'vectors', '3d']"
7,Prove that $\lim_{x \to 2} 5x^2 = 20$ using $\epsilon - \delta$ definition.,Prove that  using  definition.,\lim_{x \to 2} 5x^2 = 20 \epsilon - \delta,"Working on the book: Richard Hammack. "" Book of Proof "" (p. 259) Example 13.2 Prove that $\lim_{x \to 2} 5x^2 = 20$ . Proof. Suppose $\epsilon$ > 0. Notice that $$ | f(x) - L| = |5x^2 - 20| = |5(x^2 - 4)| = |5(x - 2)(x + 2)| = 5 · |x-2| · |x + 2|. $$ Now we have a factor of $|x-2|$ in $|f(x)-L|$ , but it is accompanied with $|x+2|$ . But if $|x-2|$ is small, then $x$ is close to 2, so $|x+2|$ should be close to 4. Now, the author assumes $|x-2| \leq 1$ In fact, if $|x-2| \leq 1$ , then | $x+2| = |(x-2)+4| \leq |x-2|+|4| \leq 1+4 = 5$ . (Here we applied the inequality (13.2) from page 245.) In other words, if $|x - 2| \leq 1, \text{then } |x + 2| \leq 5$ , and the above equation yields $$ | f (x) - L| = |5x^2 - 20| = 5 · |x - 2| · |x + 2| < 5 · |x - 2| · 5 = 25|x - 2|. $$ Take $\delta$ to be smaller than both 1 and $\frac{\epsilon}{25}$ . Then $0<|x-2|<\delta$ implies $|5x^2-20|<25·|x-2|<25\delta<25\frac{\epsilon}{25}=\epsilon$ . By Definition 13.2, $\lim_{x \to 2} 5x^2 = 20$ My questions are: Where does the assumption $|x-2| \leq 1$ come from and how it gets discharged ? I wonder if the author is really proving $\forall \epsilon > 0 ( \exists \delta > 0(|x-c| < \delta \Rightarrow (|x-c| \leq 1 \Rightarrow |f(x) - f(c)| < \epsilon)))$ . Perhaps, I am wrong but it is not possible to add that assumption as a premise, as arbitrary variable $x$ appears after the introduction of $\delta$ .","Working on the book: Richard Hammack. "" Book of Proof "" (p. 259) Example 13.2 Prove that . Proof. Suppose > 0. Notice that Now we have a factor of in , but it is accompanied with . But if is small, then is close to 2, so should be close to 4. Now, the author assumes In fact, if , then | . (Here we applied the inequality (13.2) from page 245.) In other words, if , and the above equation yields Take to be smaller than both 1 and . Then implies . By Definition 13.2, My questions are: Where does the assumption come from and how it gets discharged ? I wonder if the author is really proving . Perhaps, I am wrong but it is not possible to add that assumption as a premise, as arbitrary variable appears after the introduction of .","\lim_{x \to 2} 5x^2 = 20 \epsilon 
| f(x) - L| = |5x^2 - 20| = |5(x^2 - 4)| = |5(x - 2)(x + 2)| = 5 · |x-2| · |x + 2|.
 |x-2| |f(x)-L| |x+2| |x-2| x |x+2| |x-2| \leq 1 |x-2| \leq 1 x+2| = |(x-2)+4| \leq |x-2|+|4| \leq 1+4 = 5 |x - 2| \leq 1, \text{then } |x + 2| \leq 5 
| f (x) - L| = |5x^2 - 20| = 5 · |x - 2| · |x + 2| < 5 · |x - 2| · 5 = 25|x - 2|.
 \delta \frac{\epsilon}{25} 0<|x-2|<\delta |5x^2-20|<25·|x-2|<25\delta<25\frac{\epsilon}{25}=\epsilon \lim_{x \to 2} 5x^2 = 20 |x-2| \leq 1 \forall \epsilon > 0 ( \exists \delta > 0(|x-c| < \delta \Rightarrow (|x-c| \leq 1 \Rightarrow |f(x) - f(c)| < \epsilon))) x \delta","['real-analysis', 'limits', 'logic', 'proof-explanation', 'epsilon-delta']"
8,L'hopital rule fails with limits to infinity?,L'hopital rule fails with limits to infinity?,,"$$  \lim_{n \to \infty} \frac{1 +cn^2}{(2n+3 + 2 \sin n)^2} = ? $$ if I  factor the $n^2$ out of denominator, $$ \lim_{n \to \infty} \frac{ 1 + cn^2}{ n^2 ( 2 + 3n^{-1} + 2 \frac{ \sin n}{n} )^2}$$ And take limit directly, I get the answer as $$ \frac{c}{4}$$ However, If I apply l'hopital rule, Iget $$ \lim_{ n \to \infty} \frac{ 2cn}{2 (2n + 3 + 2 \sin n)( 2 + 2 \cos n)} $$ However this new limit gives a different value than original according to wolfram.. and neither am I able to compute it by hand, what am I missing? Some people say of limit existing and not existing, but then suppose $$ \lim_{x \to 0} \frac{1}{x} = \infty$$ Does this limit exist? how do you define a limit to be existing as in what is sufficent condition for it","if I  factor the out of denominator, And take limit directly, I get the answer as However, If I apply l'hopital rule, Iget However this new limit gives a different value than original according to wolfram.. and neither am I able to compute it by hand, what am I missing? Some people say of limit existing and not existing, but then suppose Does this limit exist? how do you define a limit to be existing as in what is sufficent condition for it",  \lim_{n \to \infty} \frac{1 +cn^2}{(2n+3 + 2 \sin n)^2} = ?  n^2  \lim_{n \to \infty} \frac{ 1 + cn^2}{ n^2 ( 2 + 3n^{-1} + 2 \frac{ \sin n}{n} )^2}  \frac{c}{4}  \lim_{ n \to \infty} \frac{ 2cn}{2 (2n + 3 + 2 \sin n)( 2 + 2 \cos n)}   \lim_{x \to 0} \frac{1}{x} = \infty,"['calculus', 'limits']"
9,Can $f(x)$ defined on an open interval have limits at the endpoints? Can $f(z)$ defined on an open disk have limits at the boundaries?,Can  defined on an open interval have limits at the endpoints? Can  defined on an open disk have limits at the boundaries?,f(x) f(z),"For suppose $f$ a function from $(0,1)$ onto $(0,1)$ defined by $f(x)=x$ , then is the limit of $f$ exist as $x$ tends to $0$ or $1$ , I know there only one side limit is finitely defined, but from the other side, it is not defined. Now my question is that is the limit exist still (as vacuously)? Also for a linear complex function $f$ defined on a region $\{ z : |z|<1\}$ , does $f$ have limit value at the boundary points? Since, we can't reach the boundary points from all the directions, as it is not defined! Now what about the existence of limit, is it vacuously exist from all the othe directions(not defined in domain!). And my last question is little different from above. For suppose limit of function have existed at all points of its domain. Then is it possible to find such function that have limit value as limit point of codomain or outside of codomain? Please anyone help , any kind of help would be appreciated!","For suppose a function from onto defined by , then is the limit of exist as tends to or , I know there only one side limit is finitely defined, but from the other side, it is not defined. Now my question is that is the limit exist still (as vacuously)? Also for a linear complex function defined on a region , does have limit value at the boundary points? Since, we can't reach the boundary points from all the directions, as it is not defined! Now what about the existence of limit, is it vacuously exist from all the othe directions(not defined in domain!). And my last question is little different from above. For suppose limit of function have existed at all points of its domain. Then is it possible to find such function that have limit value as limit point of codomain or outside of codomain? Please anyone help , any kind of help would be appreciated!","f (0,1) (0,1) f(x)=x f x 0 1 f \{ z : |z|<1\} f","['real-analysis', 'limits', 'functions']"
10,Riemann sum of $\int_1^2 {1\over x^2} dx$.,Riemann sum of .,\int_1^2 {1\over x^2} dx,"I've spent quite a time solving the following problem: Evaluate using Riemann's sum: $$ I = \int_1^2{1\over x^2} dx $$ I was first trying the following approach, which didn't work since the summation seems undoable to me: $$ \Delta x = {1\over n}\\ I = \lim_{n\to\infty}\sum_{k=1}^nf\left(1+{k\over n}\right)\Delta x \\ = \lim_{n\to\infty}\sum_{k=1}^n{n^2\over (k+n)^2} {1\over n} \\ = \lim_{n\to\infty}\sum_{k=1}^n{n\over (k+n)^2} $$ Wolfram evaluates this sum in terms of digamma function which is too advanced. Several hours has passed before I decided to reconsider the point to choose in each partition. Let: $$ \Delta x = {1\over n}\\ x_k = 1 + {k\over n}\\ \begin{align} I &= \lim_{n\to\infty}\sum_{k=1}^nf\left(\sqrt{x_k x_{k-1}}\right)\Delta x \\ &= \lim_{n\to\infty}\sum_{k=1}^n{1 \over x_k x_{k-1}}\Delta x \\ &= \lim_{n\to\infty}\sum_{k=1}^n{1 \over \left(1+{k\over n}\right)\left(1+{k-1\over n}\right)}\Delta x \\ &= \lim_{n\to\infty}\sum_{k=1}^n{n^2 \over (n+k)(n+k-1)}{1\over n}\\ &=\lim_{n\to\infty}\sum_{k=1}^n{n \over (n+k)(n+k-1)} \\ &=\lim_{n\to\infty}\sum_{k=1}^n\left({n \over (n+k-1)} - {n \over (n+k)}\right)\\ &= {n\over n} - {n\over 2n}\\ &= \boxed{{1\over 2}} \end{align} $$ This sum telescopes nicely. Now I'm wondering whether the first approach is even doable. I've met some other questions but the first one lists a hint I don't really understand and the second one is closed as a duplicate. What would be the way to finish the initial approach? In the first approach, the problem is actually reduced to finding the limit which I couldn't handle. Also is there some intuition in choosing the ""right"" points in the partitions?","I've spent quite a time solving the following problem: Evaluate using Riemann's sum: I was first trying the following approach, which didn't work since the summation seems undoable to me: Wolfram evaluates this sum in terms of digamma function which is too advanced. Several hours has passed before I decided to reconsider the point to choose in each partition. Let: This sum telescopes nicely. Now I'm wondering whether the first approach is even doable. I've met some other questions but the first one lists a hint I don't really understand and the second one is closed as a duplicate. What would be the way to finish the initial approach? In the first approach, the problem is actually reduced to finding the limit which I couldn't handle. Also is there some intuition in choosing the ""right"" points in the partitions?","
I = \int_1^2{1\over x^2} dx
 
\Delta x = {1\over n}\\
I = \lim_{n\to\infty}\sum_{k=1}^nf\left(1+{k\over n}\right)\Delta x \\
= \lim_{n\to\infty}\sum_{k=1}^n{n^2\over (k+n)^2} {1\over n} \\
= \lim_{n\to\infty}\sum_{k=1}^n{n\over (k+n)^2}
 
\Delta x = {1\over n}\\
x_k = 1 + {k\over n}\\
\begin{align}
I &= \lim_{n\to\infty}\sum_{k=1}^nf\left(\sqrt{x_k x_{k-1}}\right)\Delta x \\
&= \lim_{n\to\infty}\sum_{k=1}^n{1 \over x_k x_{k-1}}\Delta x \\
&= \lim_{n\to\infty}\sum_{k=1}^n{1 \over \left(1+{k\over n}\right)\left(1+{k-1\over n}\right)}\Delta x \\
&= \lim_{n\to\infty}\sum_{k=1}^n{n^2 \over (n+k)(n+k-1)}{1\over n}\\
&=\lim_{n\to\infty}\sum_{k=1}^n{n \over (n+k)(n+k-1)} \\
&=\lim_{n\to\infty}\sum_{k=1}^n\left({n \over (n+k-1)} - {n \over (n+k)}\right)\\
&= {n\over n} - {n\over 2n}\\
&= \boxed{{1\over 2}}
\end{align}
","['real-analysis', 'calculus', 'limits', 'riemann-sum']"
11,Find $\lim_{n \to \infty} \prod_{k=1}^{n} \frac{(k+1)^2}{k(k+2)}$,Find,\lim_{n \to \infty} \prod_{k=1}^{n} \frac{(k+1)^2}{k(k+2)},"I have to find the following limit: $$\lim\limits_{n \to \infty} {\displaystyle \prod_{k=1}^{n} \dfrac{(k+1)^2}{k(k+2)}}$$ This is what I tried: $$\lim\limits_{n \to \infty} {\displaystyle \prod_{k=1}^{n} \dfrac{(k+1)^2}{k(k+2)}} =  \lim\limits_{n \to \infty} {\displaystyle \prod_{k=1}^{n} \dfrac{k^2+2k+1}{k^2+2k}} =  \lim\limits_{n \to \infty} {\displaystyle \prod_{k=1}^{n} \bigg(\dfrac{k^2+2k}{k^2+2k}} + \dfrac{1}{k^2+2k} \bigg ) = $$ $$ = \lim\limits_{n \to \infty} {\displaystyle \prod_{k=1}^{n} \bigg(1 } + \dfrac{1}{k^2+2k} \bigg ) =  \lim\limits_{n \to \infty} {\displaystyle \prod_{k=1}^{n} 1 }  + \lim\limits_{n \to \infty} {\displaystyle \prod_{k=1}^{n} \dfrac{1}{k^2+2k} }$$ Now, $\lim\limits_{n \to \infty} {\displaystyle \prod_{k=1}^{n} 1 } = 1$ and: $\lim\limits_{n \to \infty} {\displaystyle \prod_{k=1}^{n} \dfrac{1}{k^2+2k} } = 0$ I think the above equals $0$ , since this is a product and the limit of the last term of the product is $0$ , so the whole thing would be $0$ , but I am not exactly sure if my intuition is right. So that means: $$\lim\limits_{n \to \infty} {\displaystyle \prod_{k=1}^{n} \dfrac{(k+1)^2}{k(k+2)}} = \lim\limits_{n \to \infty} {\displaystyle \prod_{k=1}^{n} 1 }  + \lim\limits_{n \to \infty} {\displaystyle \prod_{k=1}^{n} \dfrac{1}{k^2+2k} } = 1 + 0 = 1$$ The problem I have is that my textbook claims that the correct answer is $2$ , not $1$ . So I did something wrong, however, I can't spot my mistake/mistakes.","I have to find the following limit: This is what I tried: Now, and: I think the above equals , since this is a product and the limit of the last term of the product is , so the whole thing would be , but I am not exactly sure if my intuition is right. So that means: The problem I have is that my textbook claims that the correct answer is , not . So I did something wrong, however, I can't spot my mistake/mistakes.","\lim\limits_{n \to \infty} {\displaystyle \prod_{k=1}^{n} \dfrac{(k+1)^2}{k(k+2)}} \lim\limits_{n \to \infty} {\displaystyle \prod_{k=1}^{n} \dfrac{(k+1)^2}{k(k+2)}} = 
\lim\limits_{n \to \infty} {\displaystyle \prod_{k=1}^{n} \dfrac{k^2+2k+1}{k^2+2k}} = 
\lim\limits_{n \to \infty} {\displaystyle \prod_{k=1}^{n} \bigg(\dfrac{k^2+2k}{k^2+2k}} + \dfrac{1}{k^2+2k} \bigg ) =   = \lim\limits_{n \to \infty} {\displaystyle \prod_{k=1}^{n} \bigg(1 } + \dfrac{1}{k^2+2k} \bigg ) = 
\lim\limits_{n \to \infty} {\displaystyle \prod_{k=1}^{n} 1 }  + \lim\limits_{n \to \infty} {\displaystyle \prod_{k=1}^{n} \dfrac{1}{k^2+2k} } \lim\limits_{n \to \infty} {\displaystyle \prod_{k=1}^{n} 1 } = 1 \lim\limits_{n \to \infty} {\displaystyle \prod_{k=1}^{n} \dfrac{1}{k^2+2k} } = 0 0 0 0 \lim\limits_{n \to \infty} {\displaystyle \prod_{k=1}^{n} \dfrac{(k+1)^2}{k(k+2)}} = \lim\limits_{n \to \infty} {\displaystyle \prod_{k=1}^{n} 1 }  + \lim\limits_{n \to \infty} {\displaystyle \prod_{k=1}^{n} \dfrac{1}{k^2+2k} } = 1 + 0 = 1 2 1",['calculus']
12,Prove $\lim_{x\to 0} f(x)$ exists $\implies\lim_{x\to 0} f(x^2)$ exists,Prove  exists  exists,\lim_{x\to 0} f(x) \implies\lim_{x\to 0} f(x^2),"Want to show $\lim_{x\to 0} f(x)$ exists $\implies \lim_{x\to 0} f(x^2)$ exists.  (I recently posted a similar variation of the question, where I asked if it was true for $a$ rather than $0$ which can be found here: Prove that $\lim_{x\to0} f(bx)$ exists, if $\lim_{x\to0} f(x)$ exists )  Anyway, here is my attempted proof: Assume $\lim_{x\to 0} f(x)=l$ .  Then $\forall\epsilon_0 \exists\delta_0 \forall x:0<|x|<\delta_0 \implies |f(x)-l|<\epsilon_0$ It follows that $\forall\epsilon_0 \exists\delta_0 \forall x:0<|x^2|<\delta_0 \implies |f(x^2)-l|<\epsilon_0$ (this is the part I was most questionable about - am I allowed to do this?). Now let $g(x)=x^2$ , and it can be shown that $\lim_{x\to 0} x^2 = 0$ .  Hence $\forall\epsilon_1 \exists\delta_1 \forall x:0<|x|<\delta_1 \implies |x^2|<\epsilon_1$ .  Then let $e_1 = \delta_0.$ And hence from the statement in the last line, there exists a $\delta_1$ for which $|x^2|<\epsilon_1 = \delta_0$ Then for a given $\epsilon$ I take my $\delta = \delta_1$ .  Then it follows that $\forall x:0<|x|<\delta \implies |f(x^2)-l|<\epsilon$","Want to show exists exists.  (I recently posted a similar variation of the question, where I asked if it was true for rather than which can be found here: Prove that $\lim_{x\to0} f(bx)$ exists, if $\lim_{x\to0} f(x)$ exists )  Anyway, here is my attempted proof: Assume .  Then It follows that (this is the part I was most questionable about - am I allowed to do this?). Now let , and it can be shown that .  Hence .  Then let And hence from the statement in the last line, there exists a for which Then for a given I take my .  Then it follows that",\lim_{x\to 0} f(x) \implies \lim_{x\to 0} f(x^2) a 0 \lim_{x\to 0} f(x)=l \forall\epsilon_0 \exists\delta_0 \forall x:0<|x|<\delta_0 \implies |f(x)-l|<\epsilon_0 \forall\epsilon_0 \exists\delta_0 \forall x:0<|x^2|<\delta_0 \implies |f(x^2)-l|<\epsilon_0 g(x)=x^2 \lim_{x\to 0} x^2 = 0 \forall\epsilon_1 \exists\delta_1 \forall x:0<|x|<\delta_1 \implies |x^2|<\epsilon_1 e_1 = \delta_0. \delta_1 |x^2|<\epsilon_1 = \delta_0 \epsilon \delta = \delta_1 \forall x:0<|x|<\delta \implies |f(x^2)-l|<\epsilon,"['limits', 'proof-verification']"
13,"Exercise asks to show that $\sum_{n=1}^{\infty}\frac{n+1}{n}\frac{1}{3^n}x^n$ converges for $x\in(-1,1)$. I show $x\in(-3,3)$. Is something wrong?",Exercise asks to show that  converges for . I show . Is something wrong?,"\sum_{n=1}^{\infty}\frac{n+1}{n}\frac{1}{3^n}x^n x\in(-1,1) x\in(-3,3)","Consider the power series $\sum_{n=1}^{\infty} \frac{n+1}{n}\frac{1}{3^n}x^n$ . 1. Show that the series converges for all $x\in (-1,1)$ . If we let $a_n = \frac{n+1}{n}\frac{1}{3^n}x^n$ then using the ratio test we get \begin{equation*} \begin{split} \lim_{n\to\infty} \left|\frac{a_{n+1}}{a_n}\right| &= \lim_{n\to\infty} \left|\frac{\frac{n+2}{n+1}\frac{1}{3^{n+1}}x^{n+1}}{\frac{n+1}{n}\frac{1}{3^n}x^n}\right| \\ &= \lim_{n\to\infty} \left|\frac{n+2}{n+1}\frac{1}{3^{n+1}}x^{n+1}\times \frac{n}{(n+1)x^n}3^n\right| \\ &= \left|\frac{x}{3}\right|\lim_{n\to\infty} \left|\frac{n(n+2)}{(n+1)^2}\right| \\ &= \frac{|x|}{3}. \end{split} \end{equation*} So if $\frac{|x|}{3} < 1$ , then this series will converge. Since $\frac{|x|}{3} < 1 \Longleftrightarrow |x| < 3$ , the radius of convergence is $3$ , i.e. the series converges for all $x\in (-3,3)$ . Have I gone wrong somewhere or is the question wrong???","Consider the power series . 1. Show that the series converges for all . If we let then using the ratio test we get So if , then this series will converge. Since , the radius of convergence is , i.e. the series converges for all . Have I gone wrong somewhere or is the question wrong???","\sum_{n=1}^{\infty} \frac{n+1}{n}\frac{1}{3^n}x^n x\in (-1,1) a_n = \frac{n+1}{n}\frac{1}{3^n}x^n \begin{equation*}
\begin{split}
\lim_{n\to\infty} \left|\frac{a_{n+1}}{a_n}\right| &= \lim_{n\to\infty} \left|\frac{\frac{n+2}{n+1}\frac{1}{3^{n+1}}x^{n+1}}{\frac{n+1}{n}\frac{1}{3^n}x^n}\right| \\
&= \lim_{n\to\infty} \left|\frac{n+2}{n+1}\frac{1}{3^{n+1}}x^{n+1}\times \frac{n}{(n+1)x^n}3^n\right| \\
&= \left|\frac{x}{3}\right|\lim_{n\to\infty} \left|\frac{n(n+2)}{(n+1)^2}\right| \\
&= \frac{|x|}{3}.
\end{split}
\end{equation*} \frac{|x|}{3} < 1 \frac{|x|}{3} < 1 \Longleftrightarrow |x| < 3 3 x\in (-3,3)","['limits', 'convergence-divergence']"
14,Proving differentiability at a point if limit of derivative exists at that point,Proving differentiability at a point if limit of derivative exists at that point,,"Suppose that $f:\mathbb{R} \rightarrow \mathbb{R}$ is continuous, is known to be differentiable everywhere but $a \in \mathbb{R}$ , but that $\lim_{x \rightarrow a}f'(x)=L$ .  Is it permissible to apply the mean value theorem as follows to show that $f$ is differentiable at $a$ ? $\frac{f(a+h)-f(a)}{h}=f'(\theta)$ for some $\theta \in (a, h+a) $ implies that $ \lim_{h \rightarrow 0} \frac{f(a+h)-f(a)}{h} = \lim_{\theta \rightarrow a}f'(\theta) = L$ since $\theta$ is sandwiched in between $a$ and $a+h$ .","Suppose that is continuous, is known to be differentiable everywhere but , but that .  Is it permissible to apply the mean value theorem as follows to show that is differentiable at ? for some implies that since is sandwiched in between and .","f:\mathbb{R} \rightarrow \mathbb{R} a \in \mathbb{R} \lim_{x \rightarrow a}f'(x)=L f a \frac{f(a+h)-f(a)}{h}=f'(\theta) \theta \in (a, h+a)   \lim_{h \rightarrow 0} \frac{f(a+h)-f(a)}{h} = \lim_{\theta \rightarrow a}f'(\theta) = L \theta a a+h","['calculus', 'limits', 'analysis', 'derivatives']"
15,"When does the limit $\lim_{(x,y)\to(0,0)} \frac{x^ky^l}{x^{2p}+y^{2q}}$ exist?",When does the limit  exist?,"\lim_{(x,y)\to(0,0)} \frac{x^ky^l}{x^{2p}+y^{2q}}","In this case, $k,l,p,q\geq0$ and are integers. I have attempted the substitution $u=x^p$ and $v=y^q$ $$\lim_{(x,y)\to(0,0)} \frac{x^ky^l}{x^{2p}+y^{2q}}=\lim_{(u,v)\to(0,0)} \frac{u^{k/p}v^{l/q}}{u^2+v^2}$$ Then, note that $|u|<\sqrt{|u|^2+|v|^2}$ and $|v|<\sqrt{|u|^2+|v|^2}$ . So we have $$\bigg|\frac{u^{k/p}v^{l/q}}{u^2+v^2}\bigg|=\frac{|u|^{k/p}|v|^{l/q}}{|u|^2+|v|^2}<\frac{(|u|^2+|v|^2)^{k/(2p)+l/(2q)}}{|u|^2+|v|^2}$$ Thus, the limit equals $0$ when $\frac{k}{p}+\frac{l}{q}>2$ . We need to show that otherwise, the limit does not exist. If $\frac{k}{p}+\frac{l}{q}=2$ , taking the limit along the axes yields $0$ but letting $x^p=y^q$ gives $$\frac{|u|^{k/p}|v|^{l/q}}{|u|^2+|v|^2}=\frac{|u|^{k/p+l/q}}{2|u|^2}\to\frac{1}{2}$$ so the limit does not exist. Similarly, if $\frac{k}{p}+\frac{l}{q}<2$ , taking the limit along the axes still yields $0$ , but letting $x^p=y^q$ instead gives $$\frac{|u|^{k/p}|v|^{l/q}}{|u|^2+|v|^2}>\frac{|u|^{2}}{2|u|^2}\to\frac{1}{2}$$ showing that the limit does not exist. I'm not sure if all of the steps I've taken are correct - especially some of the inequalities as $(u,v)\to(0,0)$ . Also, for the last two cases, I'm not exactly sure if the absolute values around $u$ and $v$ should be there or whether I should have let $|x|^p=|y|^q$ .","In this case, and are integers. I have attempted the substitution and Then, note that and . So we have Thus, the limit equals when . We need to show that otherwise, the limit does not exist. If , taking the limit along the axes yields but letting gives so the limit does not exist. Similarly, if , taking the limit along the axes still yields , but letting instead gives showing that the limit does not exist. I'm not sure if all of the steps I've taken are correct - especially some of the inequalities as . Also, for the last two cases, I'm not exactly sure if the absolute values around and should be there or whether I should have let .","k,l,p,q\geq0 u=x^p v=y^q \lim_{(x,y)\to(0,0)} \frac{x^ky^l}{x^{2p}+y^{2q}}=\lim_{(u,v)\to(0,0)} \frac{u^{k/p}v^{l/q}}{u^2+v^2} |u|<\sqrt{|u|^2+|v|^2} |v|<\sqrt{|u|^2+|v|^2} \bigg|\frac{u^{k/p}v^{l/q}}{u^2+v^2}\bigg|=\frac{|u|^{k/p}|v|^{l/q}}{|u|^2+|v|^2}<\frac{(|u|^2+|v|^2)^{k/(2p)+l/(2q)}}{|u|^2+|v|^2} 0 \frac{k}{p}+\frac{l}{q}>2 \frac{k}{p}+\frac{l}{q}=2 0 x^p=y^q \frac{|u|^{k/p}|v|^{l/q}}{|u|^2+|v|^2}=\frac{|u|^{k/p+l/q}}{2|u|^2}\to\frac{1}{2} \frac{k}{p}+\frac{l}{q}<2 0 x^p=y^q \frac{|u|^{k/p}|v|^{l/q}}{|u|^2+|v|^2}>\frac{|u|^{2}}{2|u|^2}\to\frac{1}{2} (u,v)\to(0,0) u v |x|^p=|y|^q","['limits', 'multivariable-calculus']"
16,Evaluate $\lim_{x \to 0}[\frac{\ln(x+\sqrt{1+x^2})}{x}]^{1/x^2}$.,Evaluate .,\lim_{x \to 0}[\frac{\ln(x+\sqrt{1+x^2})}{x}]^{1/x^2},Let $\ln(x+\sqrt{1+x^2})=:y$ ，then $x=\dfrac{1}{2}(e^y-e^{-y}).$ Therefore \begin{align*} \lim_{x \to 0}\left[\frac{\ln(x+\sqrt{1+x^2})}{x}\right]^{\frac{1}{x^2}}&=\lim_{y \to 0}\left(\frac{2ye^y}{e^{2y}-1}\right)^{\frac{4}{e^{2y}+e^{-2y}-2}}\\ &=\lim_{y \to 0}\left(1+\frac{2ye^y-e^{2y}+1}{e^{2y}-1}\right)^{\frac{e^{2y}-1}{2ye^y-e^{2y}+1}\cdot\frac{2ye^y-e^{2y}+1}{e^{2y}-1}\cdot\frac{4}{e^{2y}+e^{-2y}-2}}\\ &=\exp \lim_{y \to 0}\left(\frac{2ye^y-e^{2y}+1}{e^{2y}-1}\cdot\frac{4}{e^{2y}+e^{-2y}-2}\right)\\ &=\exp \lim_{y \to 0}\left(\frac{2ye^y-e^{2y}+1}{2y}\cdot\frac{4e^{2y}}{e^{4y}-2e^{2y}+1}\right)\\ &=\exp \left(2\lim_{y \to 0}\frac{2ye^y-e^{2y}+1}{ye^{4y}-2ye^{2y}+y}\right)\\ &=\exp \left[2\lim_{y \to 0}\frac{2e^y(y-e^y+1)}{(e^{2y}-1)(e^{2y}(4y+1)-1)}\right]\\ &=\exp \left[2\lim_{y \to 0}\frac{y-e^y+1}{e^{2y}(4y^2+y)-y}\right]\\ &=\exp \left[2\lim_{y \to 0}\frac{1-e^y}{e^{2y}(8y^2+10y+1)-1}\right]\\ &=\exp \left[-2\lim_{y \to 0}\frac{y}{e^{2y}(8y^2+10y+1)-1}\right]\\ &=\exp \left[-2\lim_{y \to 0}\frac{1}{e^{2y}(16y^2+36y+12)}\right]\\ &=\exp\left(-\frac{1}{6}\right). \end{align*} Please correct me if I'm wrong! Are there simpler solutions?,Let ，then Therefore Please correct me if I'm wrong! Are there simpler solutions?,"\ln(x+\sqrt{1+x^2})=:y x=\dfrac{1}{2}(e^y-e^{-y}). \begin{align*}
\lim_{x \to 0}\left[\frac{\ln(x+\sqrt{1+x^2})}{x}\right]^{\frac{1}{x^2}}&=\lim_{y \to 0}\left(\frac{2ye^y}{e^{2y}-1}\right)^{\frac{4}{e^{2y}+e^{-2y}-2}}\\
&=\lim_{y \to 0}\left(1+\frac{2ye^y-e^{2y}+1}{e^{2y}-1}\right)^{\frac{e^{2y}-1}{2ye^y-e^{2y}+1}\cdot\frac{2ye^y-e^{2y}+1}{e^{2y}-1}\cdot\frac{4}{e^{2y}+e^{-2y}-2}}\\
&=\exp \lim_{y \to 0}\left(\frac{2ye^y-e^{2y}+1}{e^{2y}-1}\cdot\frac{4}{e^{2y}+e^{-2y}-2}\right)\\
&=\exp \lim_{y \to 0}\left(\frac{2ye^y-e^{2y}+1}{2y}\cdot\frac{4e^{2y}}{e^{4y}-2e^{2y}+1}\right)\\
&=\exp \left(2\lim_{y \to 0}\frac{2ye^y-e^{2y}+1}{ye^{4y}-2ye^{2y}+y}\right)\\
&=\exp \left[2\lim_{y \to 0}\frac{2e^y(y-e^y+1)}{(e^{2y}-1)(e^{2y}(4y+1)-1)}\right]\\
&=\exp \left[2\lim_{y \to 0}\frac{y-e^y+1}{e^{2y}(4y^2+y)-y}\right]\\
&=\exp \left[2\lim_{y \to 0}\frac{1-e^y}{e^{2y}(8y^2+10y+1)-1}\right]\\
&=\exp \left[-2\lim_{y \to 0}\frac{y}{e^{2y}(8y^2+10y+1)-1}\right]\\
&=\exp \left[-2\lim_{y \to 0}\frac{1}{e^{2y}(16y^2+36y+12)}\right]\\
&=\exp\left(-\frac{1}{6}\right).
\end{align*}",['limits']
17,Evaluating limits of nested functions,Evaluating limits of nested functions,,"I am interested in how we can approximate limits of functions of the form: $$g(x)=f\circ f\circ f\circ f\circ f\circ ...(x)$$ If we take the function $f(x)=\sin(x)$ for example we get: $$g(x)=\sin(\sin(\sin(\sin...(x))))$$ as far as I can tell, for $x\in\Re$ the function $g(x)\to0$ How could I prove this? My only thought so far is that $|\sin(x)|\le1$ and so $|\sin(\sin(x))|\le\sin(1)<1$ Can I continue this to prove the limit?","I am interested in how we can approximate limits of functions of the form: If we take the function for example we get: as far as I can tell, for the function How could I prove this? My only thought so far is that and so Can I continue this to prove the limit?",g(x)=f\circ f\circ f\circ f\circ f\circ ...(x) f(x)=\sin(x) g(x)=\sin(\sin(\sin(\sin...(x)))) x\in\Re g(x)\to0 |\sin(x)|\le1 |\sin(\sin(x))|\le\sin(1)<1,"['calculus', 'limits']"
18,$\lim_{x \to \infty} e^x - \frac{e^x}{x+1}$ Application of L'Hopital's Rule,Application of L'Hopital's Rule,\lim_{x \to \infty} e^x - \frac{e^x}{x+1},"I want to know if I can ""use"" a limit after I've used L'hopital's rule on it? I'm not sure how to better word it, but I can show you what I tried, maybe you can tell me if it is right or why it is wrong. $$\lim_{x \to \infty} e^x - \frac{e^x}{x+1}$$ We can split this into two limits $$\lim_{x \to \infty} e^x - \lim_{x \to \infty} \frac{e^x}{x+1}$$ Now since the limit on the right side is infinity over infinity, we can apply L'Hopital's rule $$\lim_{x \to \infty} e^x - \lim_{x \to \infty} \frac{e^x}{1}$$ Now we can join the two limits back (I am ""reusing"" the limit after applying L'hopital...is this allowed?) $$\lim_{x \to \infty} e^x - e^x$$ Subtracting we have $$\lim_{x \to \infty} 0 = 0$$","I want to know if I can ""use"" a limit after I've used L'hopital's rule on it? I'm not sure how to better word it, but I can show you what I tried, maybe you can tell me if it is right or why it is wrong. We can split this into two limits Now since the limit on the right side is infinity over infinity, we can apply L'Hopital's rule Now we can join the two limits back (I am ""reusing"" the limit after applying L'hopital...is this allowed?) Subtracting we have",\lim_{x \to \infty} e^x - \frac{e^x}{x+1} \lim_{x \to \infty} e^x - \lim_{x \to \infty} \frac{e^x}{x+1} \lim_{x \to \infty} e^x - \lim_{x \to \infty} \frac{e^x}{1} \lim_{x \to \infty} e^x - e^x \lim_{x \to \infty} 0 = 0,"['calculus', 'limits', 'derivatives']"
19,Proof that $\exp(x+y) = \exp(x)*\exp(y)$ using limit definition of $\exp(x)$,Proof that  using limit definition of,\exp(x+y) = \exp(x)*\exp(y) \exp(x),"I want to prove: $\exp(x+y) = \exp(x)\cdot \exp(y)$ using the definition: $\exp(x) = \lim_{n\to\infty} (1+\frac{x}{n})^n$ I am having trouble completing the proof, but here is my idea so far: $$\lim_{n\to\infty} \left(1+\frac{x+y}{n}\right)^n = \lim_{n\to\infty} \left(1+\frac{x}{n}\right)^n \cdot \lim_{n\to\infty} \left(1+\frac{y}{n}\right)^n =     \lim_{n\to\infty} \left(\left(1+\frac{x}{n}\right)^n \cdot \left(1+\frac{y}{n}\right)^n  \right) $$ Now I rearrange the last expression: $$\lim_{n\to\infty} \left(1+\frac{x+y+\frac{xy}{n}}{n}\right)^n $$ From here my idea is to somehow show that this limit is equal to $$\lim_{n\to\infty} \left(1+\frac{x+y}{n}\right)^n = \exp(x+y)$$ using the Squeeze Theorem and perhaps Bernoulli's Inequality, but I am at a loss as to how exactly to do it. I'd appreciate your help.","I want to prove: using the definition: I am having trouble completing the proof, but here is my idea so far: Now I rearrange the last expression: From here my idea is to somehow show that this limit is equal to using the Squeeze Theorem and perhaps Bernoulli's Inequality, but I am at a loss as to how exactly to do it. I'd appreciate your help.","\exp(x+y) = \exp(x)\cdot \exp(y) \exp(x) = \lim_{n\to\infty} (1+\frac{x}{n})^n \lim_{n\to\infty} \left(1+\frac{x+y}{n}\right)^n = \lim_{n\to\infty} \left(1+\frac{x}{n}\right)^n \cdot \lim_{n\to\infty} \left(1+\frac{y}{n}\right)^n =   
 \lim_{n\to\infty} \left(\left(1+\frac{x}{n}\right)^n \cdot \left(1+\frac{y}{n}\right)^n  \right)  \lim_{n\to\infty} \left(1+\frac{x+y+\frac{xy}{n}}{n}\right)^n  \lim_{n\to\infty} \left(1+\frac{x+y}{n}\right)^n = \exp(x+y)","['real-analysis', 'limits', 'exponential-function']"
20,Evaluate $\lim\limits_{x \to 0+}\left[\frac{x^{\sin x}-(\sin x)^{x}}{x^3}+\frac{\ln x}{6}\right].$,Evaluate,\lim\limits_{x \to 0+}\left[\frac{x^{\sin x}-(\sin x)^{x}}{x^3}+\frac{\ln x}{6}\right].,"Problem Evaluate $$\lim_{x \to 0+}\left[\frac{x^{\sin x}-(\sin x)^{x}}{x^3}+\frac{\ln x}{6}\right].$$ Attempt First, we may obtain $$\lim_{x \to 0+}\left[\frac{x^{\sin x}-(\sin x)^{x}}{x^3}+\frac{\ln x}{6}\right]=\lim_{x \to 0+}\frac{6e^{\sin x\ln x}-6e^{x\ln\sin x}+x^3\ln x}{6x^3}.$$ Here, you can apply L'Hôpital's rule , but it's too complicated. Moreover, you can also apply Taylor's formula , for example $$e^{\sin x\ln x}=1+\sin x\ln x+\frac{1}{2}(\sin x\ln x)^2+\cdots,\\e^{x\ln\sin x}=1+x\ln\sin x+\frac{1}{2}(x\ln\sin x)^2+\cdots,$$ but you cannot cancel the terms, thus you cannot avoid differentiating, either. Is there any elegant solution? P.S. Please don't suspect the existence of the limit. The result equals $\dfrac{1}{6}.$","Problem Evaluate Attempt First, we may obtain Here, you can apply L'Hôpital's rule , but it's too complicated. Moreover, you can also apply Taylor's formula , for example but you cannot cancel the terms, thus you cannot avoid differentiating, either. Is there any elegant solution? P.S. Please don't suspect the existence of the limit. The result equals","\lim_{x \to 0+}\left[\frac{x^{\sin x}-(\sin x)^{x}}{x^3}+\frac{\ln x}{6}\right]. \lim_{x \to 0+}\left[\frac{x^{\sin x}-(\sin x)^{x}}{x^3}+\frac{\ln x}{6}\right]=\lim_{x \to 0+}\frac{6e^{\sin x\ln x}-6e^{x\ln\sin x}+x^3\ln x}{6x^3}. e^{\sin x\ln x}=1+\sin x\ln x+\frac{1}{2}(\sin x\ln x)^2+\cdots,\\e^{x\ln\sin x}=1+x\ln\sin x+\frac{1}{2}(x\ln\sin x)^2+\cdots, \dfrac{1}{6}.","['calculus', 'limits']"
21,Attempt to generalize a self-made theorem on polynomials,Attempt to generalize a self-made theorem on polynomials,,"This is a follow-up of this question . In that question, I brought up a theorem I had discovered: For any complex polynomial $P$ degree $n$ : $$ \sum\limits_{k=0}^{n+1}(-1)^k\binom{n+1}{k}P(a+kb) = 0\quad \forall a,b \in\mathbb{C}$$ In an attempt to generalize it to non-polynomials, I conjecture: For any function $F:\ \mathbb{R} \rightarrow \mathbb{R}$ that is smooth on $[a, b]$ : $$ \lim_{n\to\infty} \sum\limits_{k=0}^{n}(-1)^k\binom{n}{k}F\Big(a+\frac{k(b-a)}{n}\Big) = 0 $$ My rationale for this conjecture is, since any function can be approximated by a polynomial, and the theorem works for all polynomials, taking the limits of the sum of any function to infinity (treat $F$ as an infinite degree polynomial) should result in $0$ as well. My questions: If this conjecture is incorrect, can you disprove it? If possible, can you construct a correct version of this theorem (i.e., $F$ has to satisfy some other conditions)? If this is indeed correct, either a sketch or a hint of a proof is appreciated.","This is a follow-up of this question . In that question, I brought up a theorem I had discovered: For any complex polynomial degree : In an attempt to generalize it to non-polynomials, I conjecture: For any function that is smooth on : My rationale for this conjecture is, since any function can be approximated by a polynomial, and the theorem works for all polynomials, taking the limits of the sum of any function to infinity (treat as an infinite degree polynomial) should result in as well. My questions: If this conjecture is incorrect, can you disprove it? If possible, can you construct a correct version of this theorem (i.e., has to satisfy some other conditions)? If this is indeed correct, either a sketch or a hint of a proof is appreciated.","P n  \sum\limits_{k=0}^{n+1}(-1)^k\binom{n+1}{k}P(a+kb) = 0\quad \forall a,b \in\mathbb{C} F:\ \mathbb{R} \rightarrow \mathbb{R} [a, b]  \lim_{n\to\infty} \sum\limits_{k=0}^{n}(-1)^k\binom{n}{k}F\Big(a+\frac{k(b-a)}{n}\Big) = 0  F 0 F","['real-analysis', 'limits']"
22,Finding value of infinite series limit,Finding value of infinite series limit,,Finding value of $\displaystyle \lim_{n\rightarrow \infty}\frac{2-\underbrace{\sqrt{2+\sqrt{2+\sqrt{2+........+\sqrt{2}}}}}_{\bf{n\; times}}}{4^{-n}}$ Try: I am trying to convert it into $\cos$ ine series sum Let $$\displaystyle \sqrt{2+2\cos \theta } = 2\cos \frac{\theta}{2}$$ and $$\displaystyle \sqrt{2+\sqrt{2+2\cos \theta}} = 2\cos \frac{\theta}{4}$$ $$\displaystyle \sqrt{2+\sqrt{2+\sqrt{2+2\cos \theta}}} = 2\cos \frac{\theta}{8}$$ could some help me how i write $$\underbrace{\sqrt{2+\sqrt{2+\sqrt{2+........+\sqrt{2}}}}}_{\bf{n\; times}}$$ into cosine series form. thanks,Finding value of Try: I am trying to convert it into ine series sum Let and could some help me how i write into cosine series form. thanks,\displaystyle \lim_{n\rightarrow \infty}\frac{2-\underbrace{\sqrt{2+\sqrt{2+\sqrt{2+........+\sqrt{2}}}}}_{\bf{n\; times}}}{4^{-n}} \cos \displaystyle \sqrt{2+2\cos \theta } = 2\cos \frac{\theta}{2} \displaystyle \sqrt{2+\sqrt{2+2\cos \theta}} = 2\cos \frac{\theta}{4} \displaystyle \sqrt{2+\sqrt{2+\sqrt{2+2\cos \theta}}} = 2\cos \frac{\theta}{8} \underbrace{\sqrt{2+\sqrt{2+\sqrt{2+........+\sqrt{2}}}}}_{\bf{n\; times}},['limits']
23,Existence of infinite iteration of functions $f_\infty$?,Existence of infinite iteration of functions ?,f_\infty,"Given a sequence of functions $\{f_n\}$ satisifying an iterated relation such as $f_n(x)=g(x+f_{n-1}(x))$ $f_n(x)=g(xf_{n-1}(x))$ $f_n(x)=g(x/f_{n-1}(x))$ Where $g:=f_1$ is continuous on the interval $[a, b]$ (or differentiable on $(a,b)$ for stronger assumptions) Question: How to prove the existence of $f_\infty(x):=\lim\limits_{n\to \infty}f_n(x)$ ? AND Are there any methods to prove such $f_\infty$ does not exist? The question comes from the problems $\displaystyle\int_0^\pi\sin(x+\sin(x+\sin(x+\cdots)))\,\mathrm dx=2$ and $\displaystyle\int_0^\pi\sin(x\sin(x\sin(x\cdots)))\,\mathrm dx$ and $\displaystyle\int_{-\frac\pi2}^{\frac\pi2}\sin\frac x{\sin\frac x{\sin\frac x{\sin\cdots}}}\,\mathrm dx=\frac34 \pi$ . Let $g(x)=\sin x$ . I ""proved"" the $1^{\rm{st}}$ and  the $3^{\rm{rd}}$ integral by assuming the exsistence of $f_\infty$ . @Sangchul Lee think $f_\infty$ in the $2^{\rm{nd}}$ integral does not exist due to the chaotic behavior . If $f_\infty$ in the $2^{\rm{nd}}$ integral exists, then $$L=\int_0^\alpha \sin y\,\mathrm d\left(\frac y{\sin y}\right) =1.86006...$$ where $\alpha=2.31373...$ is the positive root of $\dfrac t{\sin t}= \pi$ . Some thoughts so far: If we could prove $f(t)=g(x_0+t)$ is a contraction mapping on $[a,b]$ for every $x_0\in[a,b]$ , that is, if $t_0$ (depending on $x_0$ ) is the only fixed point on $[a,b]$ , then the result is intuitively true from Banach Fixed Point Theorem (similar to the case $f(t)=g(x_0t)$ and $f(t)=g(x_0/t)$ ). However, we could not apply the theorem for any $f$ , one example is $f(t)=\sin(x_0+t)$ in the $1^{\rm{st}}$ integral.","Given a sequence of functions satisifying an iterated relation such as Where is continuous on the interval (or differentiable on for stronger assumptions) Question: How to prove the existence of ? AND Are there any methods to prove such does not exist? The question comes from the problems and and . Let . I ""proved"" the and  the integral by assuming the exsistence of . @Sangchul Lee think in the integral does not exist due to the chaotic behavior . If in the integral exists, then where is the positive root of . Some thoughts so far: If we could prove is a contraction mapping on for every , that is, if (depending on ) is the only fixed point on , then the result is intuitively true from Banach Fixed Point Theorem (similar to the case and ). However, we could not apply the theorem for any , one example is in the integral.","\{f_n\} f_n(x)=g(x+f_{n-1}(x)) f_n(x)=g(xf_{n-1}(x)) f_n(x)=g(x/f_{n-1}(x)) g:=f_1 [a, b] (a,b) f_\infty(x):=\lim\limits_{n\to \infty}f_n(x) f_\infty \displaystyle\int_0^\pi\sin(x+\sin(x+\sin(x+\cdots)))\,\mathrm dx=2 \displaystyle\int_0^\pi\sin(x\sin(x\sin(x\cdots)))\,\mathrm dx \displaystyle\int_{-\frac\pi2}^{\frac\pi2}\sin\frac x{\sin\frac x{\sin\frac x{\sin\cdots}}}\,\mathrm dx=\frac34 \pi g(x)=\sin x 1^{\rm{st}} 3^{\rm{rd}} f_\infty f_\infty 2^{\rm{nd}} f_\infty 2^{\rm{nd}} L=\int_0^\alpha \sin y\,\mathrm d\left(\frac y{\sin y}\right) =1.86006... \alpha=2.31373... \dfrac t{\sin t}= \pi f(t)=g(x_0+t) [a,b] x_0\in[a,b] t_0 x_0 [a,b] f(t)=g(x_0t) f(t)=g(x_0/t) f f(t)=\sin(x_0+t) 1^{\rm{st}}","['limits', 'convergence-divergence', 'fixed-point-theorems', 'chaos-theory']"
24,Compute $\lim_{n\to\infty}3\frac {{n!}^{\frac 1n}}{n}.$,Compute,\lim_{n\to\infty}3\frac {{n!}^{\frac 1n}}{n}.,Evaluate $\lim_{n\to\infty}3\frac {{n!}^{\frac 1n}}{n}.$ I tried forcing a riemann sum: rewrite $\lim_{n\to\infty}3\frac {{n!}^{\frac 1n}}{n}=\lim_{n\to\infty}3(\frac{n!}{n^n})^{\frac 1n}=L.$ Apply $\ln$ on both sides and get: $$\lim_{n\to\infty}\ln3+ \frac 1n\ln(\frac {n!}{n^n})=\lim_{n\to\infty} \ln3+ \sum_{k=1}^{k=n}\frac1n\ln(\frac kn)=\ln3+\int_0^1\ln(x)dx=\ln(\frac 3e)\to L=\frac 3e.$$ I actually did right I had a typo... Sorry for wasting your time.,Evaluate $\lim_{n\to\infty}3\frac {{n!}^{\frac 1n}}{n}.$ I tried forcing a riemann sum: rewrite $\lim_{n\to\infty}3\frac {{n!}^{\frac 1n}}{n}=\lim_{n\to\infty}3(\frac{n!}{n^n})^{\frac 1n}=L.$ Apply $\ln$ on both sides and get: $$\lim_{n\to\infty}\ln3+ \frac 1n\ln(\frac {n!}{n^n})=\lim_{n\to\infty} \ln3+ \sum_{k=1}^{k=n}\frac1n\ln(\frac kn)=\ln3+\int_0^1\ln(x)dx=\ln(\frac 3e)\to L=\frac 3e.$$ I actually did right I had a typo... Sorry for wasting your time.,,"['calculus', 'real-analysis', 'limits', 'limits-without-lhopital']"
25,Computing $\lim_{n \rightarrow \infty} \int_{0}^{\pi/3} \frac{\sin^{n}x}{\sin^{n}x+\cos^{n}x}dx$ using Dominated Convergence Theorem,Computing  using Dominated Convergence Theorem,\lim_{n \rightarrow \infty} \int_{0}^{\pi/3} \frac{\sin^{n}x}{\sin^{n}x+\cos^{n}x}dx,So recently I found this integral:    $$\lim_{n \rightarrow \infty} \int_{0}^{\pi/3} \frac{\sin^{n}x}{\sin^{n}x+\cos^{n}x}dx$$ I know the answer should be $ \frac{\pi}{12} $ and I saw it can be solved using the Dominated Convergence Theorem. I managed to get the integral to this form: $$\lim_{n \rightarrow \infty} \frac{\pi}{3}- \int_{0}^{\pi/3} \frac{1}{1+\operatorname{tg}^{n}x}dx$$ The new integral should be  $ \frac{\pi}{4} $. But I can't find the function which bounds the function inside the integral.  Can you explain me how can I find the answer using DCT? Thanks in advance.,So recently I found this integral:    $$\lim_{n \rightarrow \infty} \int_{0}^{\pi/3} \frac{\sin^{n}x}{\sin^{n}x+\cos^{n}x}dx$$ I know the answer should be $ \frac{\pi}{12} $ and I saw it can be solved using the Dominated Convergence Theorem. I managed to get the integral to this form: $$\lim_{n \rightarrow \infty} \frac{\pi}{3}- \int_{0}^{\pi/3} \frac{1}{1+\operatorname{tg}^{n}x}dx$$ The new integral should be  $ \frac{\pi}{4} $. But I can't find the function which bounds the function inside the integral.  Can you explain me how can I find the answer using DCT? Thanks in advance.,,"['calculus', 'real-analysis', 'limits', 'convergence-divergence', 'trigonometric-integrals']"
26,Trying to figure out $\mu(\liminf_{n\to \infty}A_n) \le \liminf_{n\to \infty}\mu(A_n)$,Trying to figure out,\mu(\liminf_{n\to \infty}A_n) \le \liminf_{n\to \infty}\mu(A_n),"Let $(\Bbb{X}, \Sigma, \mu)$ be a measure space and Let $(A_n) \subset \Sigma$. The following property is satisfied: $\mu(\liminf_{n\to \infty}A_n) \le \liminf_{n\to \infty}\mu(A_n)$ where $\liminf_{n\to \infty}A_n = \displaystyle \bigcup_{n=1}^\infty \bigcap_{k=n}^\infty A_k$. I noticed that this property appears all the time (in Folland's book anyway) and I'm not sure I have grasped the notion correctly - or if to make it concrete and dumb - why aren't the two sides equal? Can anyone give an illustration (or a link to one), that will clarify this. Either a numerical illustration or a graphic one. Thanks.","Let $(\Bbb{X}, \Sigma, \mu)$ be a measure space and Let $(A_n) \subset \Sigma$. The following property is satisfied: $\mu(\liminf_{n\to \infty}A_n) \le \liminf_{n\to \infty}\mu(A_n)$ where $\liminf_{n\to \infty}A_n = \displaystyle \bigcup_{n=1}^\infty \bigcap_{k=n}^\infty A_k$. I noticed that this property appears all the time (in Folland's book anyway) and I'm not sure I have grasped the notion correctly - or if to make it concrete and dumb - why aren't the two sides equal? Can anyone give an illustration (or a link to one), that will clarify this. Either a numerical illustration or a graphic one. Thanks.",,"['limits', 'measure-theory', 'supremum-and-infimum', 'limsup-and-liminf']"
27,$\lim_{n\to\infty} \frac{n!!}{(n+1)!!}=0$ please check my proof,please check my proof,\lim_{n\to\infty} \frac{n!!}{(n+1)!!}=0,"for  $a_n = \frac {n!!}{(n+1)!!}$, prove that $\lim a_n = 0$ putting $a_n$ into logarithm, we get $$\ln a_n=\sum_{k=1}^n \ln \frac k {k+1} = -\sum_{k=1}^n \ln (1+\frac1 k)$$ by taylor series, $$\ln(1+1)=\frac 1 1 - \frac 1 2 + \frac 1 3+...$$ $$\ln(1+\frac1 2)=\frac 1 2 - \frac 1 8 + \frac 1 {24}-...$$ $$...$$ $$\ln(1+\frac1 k)=\frac 1 k - \frac 1 {2k^2} + \frac 1 {3k^3}-...$$ if ""$\lim \ln a_n$"" exists, $\sum_1^\infty \ln(1+\frac 1 k)$  (absolutely) converges. so we know that any rearrangement series of $\ln(1+\frac 1 k)$ converges and has the same value. furthermore, $\ln(1+\frac 1 k) =\sum_1^\infty\frac {(-1)^{k+1}} k$ converges absolutely if $k>1$ so, by assuning $\ln a_n$ converges, expand $\ln(1+\frac 1 k)$ by taylor series for $k>1$ and rearrange column by column. $$\sum_{k=1}^{\infty} \ln (1+\frac1 k)=(\ln2 + (\frac1 2 + \frac 1 3 + ...) - \frac 1 2 (\frac 1 4 + \frac 1 9 + ...) +\frac1 3(\frac 1 8 +\frac 1 {27} +...)+...)$$ however the second term $(\frac 1 2 + \frac 1 3 +...)$ doesn't converge while all the other terms converge. this contradiction proves that $\ln a_n$ doesn't converge. so we have $$\lim_{n\to\infty}\ln a_n=-\sum_{k=1}^{\infty} \ln (1+\frac1 k)=-\infty$$ which implies $\lim a_n =0$. Is this proof correct? Can you give me more simple proof?","for  $a_n = \frac {n!!}{(n+1)!!}$, prove that $\lim a_n = 0$ putting $a_n$ into logarithm, we get $$\ln a_n=\sum_{k=1}^n \ln \frac k {k+1} = -\sum_{k=1}^n \ln (1+\frac1 k)$$ by taylor series, $$\ln(1+1)=\frac 1 1 - \frac 1 2 + \frac 1 3+...$$ $$\ln(1+\frac1 2)=\frac 1 2 - \frac 1 8 + \frac 1 {24}-...$$ $$...$$ $$\ln(1+\frac1 k)=\frac 1 k - \frac 1 {2k^2} + \frac 1 {3k^3}-...$$ if ""$\lim \ln a_n$"" exists, $\sum_1^\infty \ln(1+\frac 1 k)$  (absolutely) converges. so we know that any rearrangement series of $\ln(1+\frac 1 k)$ converges and has the same value. furthermore, $\ln(1+\frac 1 k) =\sum_1^\infty\frac {(-1)^{k+1}} k$ converges absolutely if $k>1$ so, by assuning $\ln a_n$ converges, expand $\ln(1+\frac 1 k)$ by taylor series for $k>1$ and rearrange column by column. $$\sum_{k=1}^{\infty} \ln (1+\frac1 k)=(\ln2 + (\frac1 2 + \frac 1 3 + ...) - \frac 1 2 (\frac 1 4 + \frac 1 9 + ...) +\frac1 3(\frac 1 8 +\frac 1 {27} +...)+...)$$ however the second term $(\frac 1 2 + \frac 1 3 +...)$ doesn't converge while all the other terms converge. this contradiction proves that $\ln a_n$ doesn't converge. so we have $$\lim_{n\to\infty}\ln a_n=-\sum_{k=1}^{\infty} \ln (1+\frac1 k)=-\infty$$ which implies $\lim a_n =0$. Is this proof correct? Can you give me more simple proof?",,"['real-analysis', 'limits']"
28,What's the limit of a square root function?,What's the limit of a square root function?,,"$$\lim_{x \to \sqrt{3}^{-}} \sqrt{x^2-3}$$ What's the answer of this limit? There are two hyppothesis: $0$ and undefined. Undefined Because the square root of a negative number is undefined, $0$ because if we plug $\sqrt{3}$, we obtain $0$; I am not so sure. Please help Edit: Why do we calculate this limit? According to my teacher, a function limit at sqrt(3) exists if limit at sqrt(3)- and limit at sqrt(3)+ both exists and are the same. So that's why we tried to find it . Is that right?","$$\lim_{x \to \sqrt{3}^{-}} \sqrt{x^2-3}$$ What's the answer of this limit? There are two hyppothesis: $0$ and undefined. Undefined Because the square root of a negative number is undefined, $0$ because if we plug $\sqrt{3}$, we obtain $0$; I am not so sure. Please help Edit: Why do we calculate this limit? According to my teacher, a function limit at sqrt(3) exists if limit at sqrt(3)- and limit at sqrt(3)+ both exists and are the same. So that's why we tried to find it . Is that right?",,['limits']
29,Asymptotic to a sequence of algebraic numbers.,Asymptotic to a sequence of algebraic numbers.,,Let $f(n)$ be the largest real solution of $$x^n - x^{n-1} = 1 $$ As $n$ grows to positive infinity we get the asymptotic : $$ f(n) = 1 + \frac{\ln(n)}{n} + \frac{\exp(2)}{n^2} + ...$$ Where the value $\exp(2)$ is optimal ! ( and $...$ means smaller term(s) ) Notice $f(2)$ is the golden mean. How to show this asymptotic  ? Edit Corrected the formula.,Let $f(n)$ be the largest real solution of $$x^n - x^{n-1} = 1 $$ As $n$ grows to positive infinity we get the asymptotic : $$ f(n) = 1 + \frac{\ln(n)}{n} + \frac{\exp(2)}{n^2} + ...$$ Where the value $\exp(2)$ is optimal ! ( and $...$ means smaller term(s) ) Notice $f(2)$ is the golden mean. How to show this asymptotic  ? Edit Corrected the formula.,,"['limits', 'polynomials', 'asymptotics', 'exponential-function', 'roots']"
30,Where is the mistake in this solution of $\lim_{x \to 1}{\frac{1-x^2}{\sin (\pi x)}}$?,Where is the mistake in this solution of ?,\lim_{x \to 1}{\frac{1-x^2}{\sin (\pi x)}},"I'm trying to solve this limit: $$\lim_{x \to 1}{\frac{1-x^2}{\sin (\pi x)}}$$ The answer ought to be $\frac{2}{\pi}$, but I end up with $0$: $\lim\limits_{x \to 1}{\frac{1-x^2}{\sin (\pi x)}} = $ $\lim\limits_{y \to 0}{\frac{1-(y+1)^2}{\sin (\pi (y+1))}} = $ $\lim\limits_{y \to 0}{\frac{\pi(y+1)}{\sin (\pi (y+1))} \frac{1-(y+1)^2}{\pi(y+1)}} = $ $\lim\limits_{y \to 0}{\frac{1-(y+1)^2}{\pi(y+1)}} =  0$ Where and why is my solution incorrect? Note: I'm aware of this post , however I believe mine is different because I'm asking where and why my solution went wrong, not why my answer was wrong.","I'm trying to solve this limit: $$\lim_{x \to 1}{\frac{1-x^2}{\sin (\pi x)}}$$ The answer ought to be $\frac{2}{\pi}$, but I end up with $0$: $\lim\limits_{x \to 1}{\frac{1-x^2}{\sin (\pi x)}} = $ $\lim\limits_{y \to 0}{\frac{1-(y+1)^2}{\sin (\pi (y+1))}} = $ $\lim\limits_{y \to 0}{\frac{\pi(y+1)}{\sin (\pi (y+1))} \frac{1-(y+1)^2}{\pi(y+1)}} = $ $\lim\limits_{y \to 0}{\frac{1-(y+1)^2}{\pi(y+1)}} =  0$ Where and why is my solution incorrect? Note: I'm aware of this post , however I believe mine is different because I'm asking where and why my solution went wrong, not why my answer was wrong.",,"['calculus', 'limits', 'proof-verification', 'trigonometry', 'limits-without-lhopital']"
31,Limit without L'Hopital rule,Limit without L'Hopital rule,,How to compute the limits for the following functions without using L'Hopital rule? 1) $\displaystyle\underset{x\to 0^{+}}{\lim} \frac{e^{1/x} + 2 e ^{-1/x} + \ln x}{3e^{1/x} + 5e^{-1/x} + 7\ln x}  $ 2) $\displaystyle\underset{x\to 0^{+}}{\lim} \frac{x+e^{-1/x}}{x-e^{-1/x}}$ I wouldn't know where to begin. Thank you very much.,How to compute the limits for the following functions without using L'Hopital rule? 1) $\displaystyle\underset{x\to 0^{+}}{\lim} \frac{e^{1/x} + 2 e ^{-1/x} + \ln x}{3e^{1/x} + 5e^{-1/x} + 7\ln x}  $ 2) $\displaystyle\underset{x\to 0^{+}}{\lim} \frac{x+e^{-1/x}}{x-e^{-1/x}}$ I wouldn't know where to begin. Thank you very much.,,"['calculus', 'limits', 'limits-without-lhopital']"
32,Limit of ${n^2 \choose n}/{n^2+n-1 \choose n}$,Limit of,{n^2 \choose n}/{n^2+n-1 \choose n},I am trying to work out $$\lim_{n \to \infty} \frac{{n^2 \choose n}}{{n^2+n-1 \choose n}}.$$ Numerically it appears to be approximately $0.377$.  Is it possible to get an exact answer?,I am trying to work out $$\lim_{n \to \infty} \frac{{n^2 \choose n}}{{n^2+n-1 \choose n}}.$$ Numerically it appears to be approximately $0.377$.  Is it possible to get an exact answer?,,['limits']
33,"Using Mean Value Theorem, Prove that ${\tan x\over x}>{x\over\sin x}$","Using Mean Value Theorem, Prove that",{\tan x\over x}>{x\over\sin x},"Prove using Mean Value Theorem, $${\tan x\over x}>{x\over\sin x} \space\forall \space x \space\in(0, \pi/2) $$ Attempt:: $f(x) = x-\sin x$ $f^\prime(x) = 1-\cos x > 0 $ Hence $x-\sin x>0, {x\over\sin x}>1$ Similarly, I got ${\tan x\over x}>1$ But how do I compare them and get the required inequality?","Prove using Mean Value Theorem, Attempt:: Hence Similarly, I got But how do I compare them and get the required inequality?","{\tan x\over x}>{x\over\sin x} \space\forall \space x \space\in(0, \pi/2)  f(x) = x-\sin x f^\prime(x) = 1-\cos x > 0  x-\sin x>0, {x\over\sin x}>1 {\tan x\over x}>1","['calculus', 'limits', 'inequality']"
34,How to evaluate $\lim\limits_{x\to 0}\frac{e^{\arctan{(x)}}-xe^{\pi x}-1}{(\ln{(1+x)})^2}$?,How to evaluate ?,\lim\limits_{x\to 0}\frac{e^{\arctan{(x)}}-xe^{\pi x}-1}{(\ln{(1+x)})^2},How to evaluate $\lim\limits_{x\to 0}\frac{e^{\arctan{(x)}}-xe^{\pi x}-1}{(\ln{(1+x)})^2}$? So I think we expand to $x^2$ since the lowest term for $\ln(1+x)$ is $x$ Let $u=\arctan{(x)}$ $\lim\limits_{x\to 0}\frac{e^{\arctan{(x)}}-xe^{\pi x}-1}{(\ln{(1+x)})^2}=\lim\limits_{x\to 0}\frac{1+u+\frac{u^2}{2}+o(u^2)-(x+\pi x^2+o())-1}{x^2+o()}=\lim\limits_{x\to 0}\frac{1+x+o(x^3)+\frac{x^2+o()}{2}+o(u^2)-(x+\pi x^2+o())-1}{x^2+o()}=\frac12-\pi$ My answer is rather messy and likely incorrect. Could someone provide an easier way to solve such problem?,How to evaluate $\lim\limits_{x\to 0}\frac{e^{\arctan{(x)}}-xe^{\pi x}-1}{(\ln{(1+x)})^2}$? So I think we expand to $x^2$ since the lowest term for $\ln(1+x)$ is $x$ Let $u=\arctan{(x)}$ $\lim\limits_{x\to 0}\frac{e^{\arctan{(x)}}-xe^{\pi x}-1}{(\ln{(1+x)})^2}=\lim\limits_{x\to 0}\frac{1+u+\frac{u^2}{2}+o(u^2)-(x+\pi x^2+o())-1}{x^2+o()}=\lim\limits_{x\to 0}\frac{1+x+o(x^3)+\frac{x^2+o()}{2}+o(u^2)-(x+\pi x^2+o())-1}{x^2+o()}=\frac12-\pi$ My answer is rather messy and likely incorrect. Could someone provide an easier way to solve such problem?,,"['calculus', 'limits', 'summation', 'power-series', 'taylor-expansion']"
35,"If $\lim_{x\to\infty}f(x)$ and $\lim_{x\to\infty}f^{\prime}(x)$ both exist, then $\lim_{x\to\infty}f^{\prime}(x) = 0$","If  and  both exist, then",\lim_{x\to\infty}f(x) \lim_{x\to\infty}f^{\prime}(x) \lim_{x\to\infty}f^{\prime}(x) = 0,"Suppose $f:\mathbb{R}\to\mathbb{R}$ is everywhere differentiable, and suppose that $\lim_{x\to\infty}f(x)$ and $\lim_{x\to\infty}f^{\prime}(x)$ both exist. I am trying to prove that the latter limit is necessarily $0$. I have the following argument, but I'm not sure if it's completely sound. Since $f$ is differentiable everywhere, we can apply the Mean Value Theorem to $f$ on $[x,x+1]$ for all relevant $x$. This guarantees an $\alpha_{x}\in(x,x+1)$ such that $$f^{\prime}(\alpha_{x}) = \frac{f(x+1)-f(x)}{x+1-x} = f(x+1)-f(x).$$ Now, the limit as $x\to\infty$ of the right-hand side of this expression must be $0$, since $\lim_{x\to\infty}f(x)$ exists by assumption (and must equal $\lim_{x\to\infty}f(x+1)$). On the left hand side, we notice that $\alpha_{x}\to\infty$ as $x\to\infty$, since $\alpha_{x}>x$ always, so that: \begin{eqnarray*} 0 & = & \lim_{x\to\infty}[f(x+1)-f(x)]\\ & = & \lim_{x\to\infty}f^{\prime}(\alpha_{x})\\ & = & \lim_{y\to\infty}f^{\prime}(y), \end{eqnarray*} proving the result. I took inspiration for this argument from other sources which use the same trick of ""use the Mean Value Theorem to introduce a quantity $\alpha_{x}$ which we have some bounds on, then take limits"". However, this style of argument seems dodgy to me: we haven't actually defined a function $\alpha$ to take the limit of as $x\to\infty$, and it's not clear to me that defining such a function is always possible. For example, we can't just say ""take the least such value and call it $\alpha_{x}$"", because we haven't shown that there will always be a least such value. Here are my questions: In the above, where have we used the fact that $\lim_{x\to\infty}f^{\prime}(x)$ exists? This is an important assumption: consider for example the function $x\mapsto\sin{(x^{2})}/x$. My guess is that it's used in the last line, where we must assume this fact to use the chain rule, but I'd like confirmation of this. Does the ""$\alpha_{x}$ trick"" require something like the Axiom of Choice in general? In particular, the thing which makes me slightly anxious about just saying ""choose an $\alpha_{x}$ for every $x$"" is that we have to make (uncountably) infinitely many ""choices"", and we have no prescribed method of doing this. EDIT : It turns out this has been answered in other questions on this site, see link in the comments below. EDIT : Note that the first question is different to others on related topics because here I am asking very specifically about this argument and why it works.","Suppose $f:\mathbb{R}\to\mathbb{R}$ is everywhere differentiable, and suppose that $\lim_{x\to\infty}f(x)$ and $\lim_{x\to\infty}f^{\prime}(x)$ both exist. I am trying to prove that the latter limit is necessarily $0$. I have the following argument, but I'm not sure if it's completely sound. Since $f$ is differentiable everywhere, we can apply the Mean Value Theorem to $f$ on $[x,x+1]$ for all relevant $x$. This guarantees an $\alpha_{x}\in(x,x+1)$ such that $$f^{\prime}(\alpha_{x}) = \frac{f(x+1)-f(x)}{x+1-x} = f(x+1)-f(x).$$ Now, the limit as $x\to\infty$ of the right-hand side of this expression must be $0$, since $\lim_{x\to\infty}f(x)$ exists by assumption (and must equal $\lim_{x\to\infty}f(x+1)$). On the left hand side, we notice that $\alpha_{x}\to\infty$ as $x\to\infty$, since $\alpha_{x}>x$ always, so that: \begin{eqnarray*} 0 & = & \lim_{x\to\infty}[f(x+1)-f(x)]\\ & = & \lim_{x\to\infty}f^{\prime}(\alpha_{x})\\ & = & \lim_{y\to\infty}f^{\prime}(y), \end{eqnarray*} proving the result. I took inspiration for this argument from other sources which use the same trick of ""use the Mean Value Theorem to introduce a quantity $\alpha_{x}$ which we have some bounds on, then take limits"". However, this style of argument seems dodgy to me: we haven't actually defined a function $\alpha$ to take the limit of as $x\to\infty$, and it's not clear to me that defining such a function is always possible. For example, we can't just say ""take the least such value and call it $\alpha_{x}$"", because we haven't shown that there will always be a least such value. Here are my questions: In the above, where have we used the fact that $\lim_{x\to\infty}f^{\prime}(x)$ exists? This is an important assumption: consider for example the function $x\mapsto\sin{(x^{2})}/x$. My guess is that it's used in the last line, where we must assume this fact to use the chain rule, but I'd like confirmation of this. Does the ""$\alpha_{x}$ trick"" require something like the Axiom of Choice in general? In particular, the thing which makes me slightly anxious about just saying ""choose an $\alpha_{x}$ for every $x$"" is that we have to make (uncountably) infinitely many ""choices"", and we have no prescribed method of doing this. EDIT : It turns out this has been answered in other questions on this site, see link in the comments below. EDIT : Note that the first question is different to others on related topics because here I am asking very specifically about this argument and why it works.",,"['calculus', 'real-analysis', 'limits']"
36,"Elementary question about the limit $\big( 1 - \frac 1 {\sqrt n}\big )^n$, $n\to\infty$.","Elementary question about the limit , .",\big( 1 - \frac 1 {\sqrt n}\big )^n n\to\infty,"When calculating the limit $L=\big( 1 - \frac 1 {\sqrt n}\big)^n$, $n\to\infty$, what allows me to do the following: $$ L=\lim \left(\left(1-\frac {1}{\sqrt n}\right)^\sqrt{n} \right)^\sqrt{n} $$ As the term inside the outer parenthesis goes to $e^{-1}$, we have $L=\lim e^{-\sqrt n}=0$ . It's like we're distributing the parenthesis somehow: $$\lim \left(\left(1-\frac {1}{\sqrt n}\right)^\sqrt{n} \right)^\sqrt{n}=\lim \left(\lim\left (1-\frac {1}{\sqrt n}\right)^\sqrt{n} \right)^\sqrt{n}=\lim e^{-\sqrt n}$$ The question is: Why can we do this? Which property are we using?","When calculating the limit $L=\big( 1 - \frac 1 {\sqrt n}\big)^n$, $n\to\infty$, what allows me to do the following: $$ L=\lim \left(\left(1-\frac {1}{\sqrt n}\right)^\sqrt{n} \right)^\sqrt{n} $$ As the term inside the outer parenthesis goes to $e^{-1}$, we have $L=\lim e^{-\sqrt n}=0$ . It's like we're distributing the parenthesis somehow: $$\lim \left(\left(1-\frac {1}{\sqrt n}\right)^\sqrt{n} \right)^\sqrt{n}=\lim \left(\lim\left (1-\frac {1}{\sqrt n}\right)^\sqrt{n} \right)^\sqrt{n}=\lim e^{-\sqrt n}$$ The question is: Why can we do this? Which property are we using?",,"['calculus', 'limits']"
37,Solving this limit: $\lim\limits_{x\to\infty}\frac{x^{x+1/x}}{(x+1/x)^x}$,Solving this limit:,\lim\limits_{x\to\infty}\frac{x^{x+1/x}}{(x+1/x)^x},"$$\lim\limits_{x\to\infty}\frac{x^{x+1/x}}{(x+1/x)^x}$$ I have tried a lot of things, like: transforming those terms to: $$\frac{e^{(x+1/x)\ln(x)}}{e^{x\ln(x+1/x)}}$$ then I tried L'Hôpital's rule but it was just getting more complex I also made them one, like: $$e^{(x+1/x)\ln(x)-x\ln(x+1/x)}$$ At last, I tried to ""squeeze"" them but I couldn't find the perfect function for that. I hope that this is not a duplicate because I searched but I couldn't find a similar post.","$$\lim\limits_{x\to\infty}\frac{x^{x+1/x}}{(x+1/x)^x}$$ I have tried a lot of things, like: transforming those terms to: $$\frac{e^{(x+1/x)\ln(x)}}{e^{x\ln(x+1/x)}}$$ then I tried L'Hôpital's rule but it was just getting more complex I also made them one, like: $$e^{(x+1/x)\ln(x)-x\ln(x+1/x)}$$ At last, I tried to ""squeeze"" them but I couldn't find the perfect function for that. I hope that this is not a duplicate because I searched but I couldn't find a similar post.",,"['calculus', 'limits']"
38,Find the limit of $\lim_{x\to \infty}(\frac{x}{x})^x+(\frac{x-1}{x})^x+(\frac{x-2}{x})^x......+(\frac{1}{x})^x$,Find the limit of,\lim_{x\to \infty}(\frac{x}{x})^x+(\frac{x-1}{x})^x+(\frac{x-2}{x})^x......+(\frac{1}{x})^x,"Find the limit of $\lim_{x\to \infty}(\frac{x}{x})^x+(\frac{x-1}{x})^x+(\frac{x-2}{x})^x......+(\frac{1}{x})^x$ $\lim_{x\to \infty}(\frac{x}{x})^x+(\frac{x-1}{x})^x+(\frac{x-2}{x})^x......+(\frac{1}{x})^x$ $=\lim_{x\to \infty}\frac{1^x+2^x+3^x+.....+x^x}{x^x}$ This is in $\frac{\infty}{\infty}$ form ,so i applied L hospital rule. $=\lim_{x\to \infty}\frac{2^x\log 2+3^x\log 3+.....+x^x(1+\log x)}{x^x(1+\log x)}$ But i am stuck here and could not solve further.Please help.","Find the limit of $\lim_{x\to \infty}(\frac{x}{x})^x+(\frac{x-1}{x})^x+(\frac{x-2}{x})^x......+(\frac{1}{x})^x$ $\lim_{x\to \infty}(\frac{x}{x})^x+(\frac{x-1}{x})^x+(\frac{x-2}{x})^x......+(\frac{1}{x})^x$ $=\lim_{x\to \infty}\frac{1^x+2^x+3^x+.....+x^x}{x^x}$ This is in $\frac{\infty}{\infty}$ form ,so i applied L hospital rule. $=\lim_{x\to \infty}\frac{2^x\log 2+3^x\log 3+.....+x^x(1+\log x)}{x^x(1+\log x)}$ But i am stuck here and could not solve further.Please help.",,['limits']
39,Limit of a sequence using Hölder's inequality,Limit of a sequence using Hölder's inequality,,"Let $a_1,\ldots,a_p$ be positive real numbers. Find the limit of $$\left(\frac{a_1^n+\cdots+a_p^n}{p}\right)^{1/n}$$ My attempt: I applied Hölder's and have obtained that this term is bounded below by $\frac{a_1+\cdots+a_p}{p}$ and since $a_1^p+\cdots+a_n^p \le (a_1+\cdots+a_p)^n$ is valid, applying log of limits technique, I get that it is bounded above by $a_1+\cdots+a_p$. However, I haven't obtained an actual limit. In fact I don't know the actual answer. I think it could be the lower bound I obtained, because that answer is validated for certain examples, like, by putting all values of $a_i$'s as a constant $k$, but I am not able to get a suitable idea to conclude that. Any help?","Let $a_1,\ldots,a_p$ be positive real numbers. Find the limit of $$\left(\frac{a_1^n+\cdots+a_p^n}{p}\right)^{1/n}$$ My attempt: I applied Hölder's and have obtained that this term is bounded below by $\frac{a_1+\cdots+a_p}{p}$ and since $a_1^p+\cdots+a_n^p \le (a_1+\cdots+a_p)^n$ is valid, applying log of limits technique, I get that it is bounded above by $a_1+\cdots+a_p$. However, I haven't obtained an actual limit. In fact I don't know the actual answer. I think it could be the lower bound I obtained, because that answer is validated for certain examples, like, by putting all values of $a_i$'s as a constant $k$, but I am not able to get a suitable idea to conclude that. Any help?",,"['real-analysis', 'limits', 'inequality']"
40,Differentiability of this picewise function,Differentiability of this picewise function,,"$$f(x,y) = \left\{\begin{array}{cc} \frac{xy}{x^2+y^2} & (x,y)\neq(0,0) \\ f(x,y) = 0 & (x,y)=(0,0) \end{array}\right.$$ In order to verify if this function is differentiable, I tried to prove it by the theorem that says that if $\frac{∂f}{∂x}$ and $\frac{∂f}{∂y}$ exist and are continuous at the point $(x_0,y_0)$, then the function is differentiable at this point. So I did: $$\frac{\partial f}{\partial x}(0,0) = \lim_{h\to 0}\frac{f(0+h,0)-f(0,0)}{h} = 0$$ $$\frac{\partial f}{\partial y}(0,0) = \lim_{h\to 0}\frac{f(0,0+h)-f(0,0)}{h} = 0$$ so we have that the partial derivatives at point $(0,0)$ is $0$. Now, if we take the derivative at $(x,y)\neq (0,0)$ and then take the limit of it as $(x,y)\to(0,0)$, we can see if the derivatives are continuous or not. So here it is: $$\frac{\partial f}{\partial x}(x,y) = \frac{y(y^2-x^2)}{(x^2+y^2)}$$ but $$\lim_{(x,y)\to(0,0)} \frac{y(y^2-x^2)}{(x^2+y^2)} $$ does not exist (by wolfram alpha... but can anybody tell me an easy way to prove this limit does not exist? easier than taking the limit in different directions?), therefore the derivative is not continuous at $(0,0)$, so we can't say $f$ is differentiable at $(0,0)$, but for $(x,y)\neq (0,0)$ the function is continuous, as it is a quotient of continuous functions. So $f$ is at least differentiable at $(x,y)\neq (0,0)$. Now, to verify differentiability at $(0,0)$ I think we must use the limit definition of differentiablity: A function is differentiable at $(0,0)$ iff: $$\lim_{(h,k)\to (0,0)} \frac{f(0+h,0+k)-f(0,0)-\frac{\partial f}{\partial x}(0,0)-\frac{\partial f}{\partial y}(0,0)}{\|(h,k)\|} = 0$$ Let's calculate this limit: $$\lim_{(h,k)\to (0,0)} \frac{f(0+h,0+k)-f(0,0)-\frac{\partial f}{\partial x}(0,0)-\frac{\partial f}{\partial y}(0,0)}{\|(h,k)\|} = \\ \lim_{(h,k)\to (0,0)} \frac{\frac{hk}{h^2+k^2}}{\sqrt{h^2+k^2}} = \\ \lim_{(h,k)\to (0,0)} \frac{hk}{(h^2+k^2)\sqrt{h^2+k^2}}$$ which I think, it's a limit that does not exist, therefore the function isn't differentiable at $(0,0)$","$$f(x,y) = \left\{\begin{array}{cc} \frac{xy}{x^2+y^2} & (x,y)\neq(0,0) \\ f(x,y) = 0 & (x,y)=(0,0) \end{array}\right.$$ In order to verify if this function is differentiable, I tried to prove it by the theorem that says that if $\frac{∂f}{∂x}$ and $\frac{∂f}{∂y}$ exist and are continuous at the point $(x_0,y_0)$, then the function is differentiable at this point. So I did: $$\frac{\partial f}{\partial x}(0,0) = \lim_{h\to 0}\frac{f(0+h,0)-f(0,0)}{h} = 0$$ $$\frac{\partial f}{\partial y}(0,0) = \lim_{h\to 0}\frac{f(0,0+h)-f(0,0)}{h} = 0$$ so we have that the partial derivatives at point $(0,0)$ is $0$. Now, if we take the derivative at $(x,y)\neq (0,0)$ and then take the limit of it as $(x,y)\to(0,0)$, we can see if the derivatives are continuous or not. So here it is: $$\frac{\partial f}{\partial x}(x,y) = \frac{y(y^2-x^2)}{(x^2+y^2)}$$ but $$\lim_{(x,y)\to(0,0)} \frac{y(y^2-x^2)}{(x^2+y^2)} $$ does not exist (by wolfram alpha... but can anybody tell me an easy way to prove this limit does not exist? easier than taking the limit in different directions?), therefore the derivative is not continuous at $(0,0)$, so we can't say $f$ is differentiable at $(0,0)$, but for $(x,y)\neq (0,0)$ the function is continuous, as it is a quotient of continuous functions. So $f$ is at least differentiable at $(x,y)\neq (0,0)$. Now, to verify differentiability at $(0,0)$ I think we must use the limit definition of differentiablity: A function is differentiable at $(0,0)$ iff: $$\lim_{(h,k)\to (0,0)} \frac{f(0+h,0+k)-f(0,0)-\frac{\partial f}{\partial x}(0,0)-\frac{\partial f}{\partial y}(0,0)}{\|(h,k)\|} = 0$$ Let's calculate this limit: $$\lim_{(h,k)\to (0,0)} \frac{f(0+h,0+k)-f(0,0)-\frac{\partial f}{\partial x}(0,0)-\frac{\partial f}{\partial y}(0,0)}{\|(h,k)\|} = \\ \lim_{(h,k)\to (0,0)} \frac{\frac{hk}{h^2+k^2}}{\sqrt{h^2+k^2}} = \\ \lim_{(h,k)\to (0,0)} \frac{hk}{(h^2+k^2)\sqrt{h^2+k^2}}$$ which I think, it's a limit that does not exist, therefore the function isn't differentiable at $(0,0)$",,"['calculus', 'limits', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
41,$ \lim_{x \to 0^+} \frac{f(f(x)) }{f^{-1}(x)}$,, \lim_{x \to 0^+} \frac{f(f(x)) }{f^{-1}(x)},"Suppose that $f \in \mathcal{C} ^1 \ ([0,1])$ and that $\displaystyle\lim_{x \to  0^+} \frac{f(2x^2)}{\sqrt {3}x^2} = 1$. Find $\displaystyle \lim_{x \to  0^+} \frac{f(f(x)) }{f^{-1}(x)}$. I don't know were to begin. They don't say if $\lim_{x \to  0^+} f(2x^2) = 0$, so is $f(2x^2) \sim \sqrt {3} x^2 $? I also don't know how to deal with the inverse function. Any help would be greatly appreciated, especially explaining the reasoning behind the method.","Suppose that $f \in \mathcal{C} ^1 \ ([0,1])$ and that $\displaystyle\lim_{x \to  0^+} \frac{f(2x^2)}{\sqrt {3}x^2} = 1$. Find $\displaystyle \lim_{x \to  0^+} \frac{f(f(x)) }{f^{-1}(x)}$. I don't know were to begin. They don't say if $\lim_{x \to  0^+} f(2x^2) = 0$, so is $f(2x^2) \sim \sqrt {3} x^2 $? I also don't know how to deal with the inverse function. Any help would be greatly appreciated, especially explaining the reasoning behind the method.",,"['real-analysis', 'limits']"
42,Are these two limits equal to each other?,Are these two limits equal to each other?,,"I'm curious about whether these two limits are the same (well I know they are equal since Wolfram Alpha confirms it, but I want to know whether the reasoning is justified): $$ \lim_{x\rightarrow \infty} \frac{\ln{x}}{x} \;\; \text{ is equivalent to  } \;\; \lim_{x\rightarrow 0}\;x\ln{x}   $$ I've already found that: $$ \lim_{x\rightarrow \infty}\frac{\ln{x}}{x} =0  $$ and then tried to use this to find the second limit: $$ \lim_{x\rightarrow \infty}\frac{\ln{x}}{x} = \lim_{x\rightarrow \infty}\; \frac{1}{x}\times \ln{ \left( \frac{1}{x} \right)^{-1} }= -\lim_{x\rightarrow \infty}\; \frac{1}{x}\times \ln{ \left( \frac{1}{x} \right) } $$ since $1/x$ tends to $0$ as $x$ tends to $\infty$ then I let: $y=1/x$ and thus: $$= -\lim_{1/y\rightarrow\infty} \; y\ln{y} $$ and given that $1/y \rightarrow \infty$ would imply that $y\rightarrow 0$: $$=-\lim_{y\rightarrow 0}y\ln{y}$$ So the limit would be: $$-\lim_{y\rightarrow 0}y\ln{y}=\lim_{x\rightarrow \infty}\frac{\ln{x}}{x} =0$$ I'm just not quite sure whether this is strictly correct, since it seems to me that $1/x$ would approach zero at a different 'rate' than $x$ would and thus the limits wouldn't necessarily have to be the same.","I'm curious about whether these two limits are the same (well I know they are equal since Wolfram Alpha confirms it, but I want to know whether the reasoning is justified): $$ \lim_{x\rightarrow \infty} \frac{\ln{x}}{x} \;\; \text{ is equivalent to  } \;\; \lim_{x\rightarrow 0}\;x\ln{x}   $$ I've already found that: $$ \lim_{x\rightarrow \infty}\frac{\ln{x}}{x} =0  $$ and then tried to use this to find the second limit: $$ \lim_{x\rightarrow \infty}\frac{\ln{x}}{x} = \lim_{x\rightarrow \infty}\; \frac{1}{x}\times \ln{ \left( \frac{1}{x} \right)^{-1} }= -\lim_{x\rightarrow \infty}\; \frac{1}{x}\times \ln{ \left( \frac{1}{x} \right) } $$ since $1/x$ tends to $0$ as $x$ tends to $\infty$ then I let: $y=1/x$ and thus: $$= -\lim_{1/y\rightarrow\infty} \; y\ln{y} $$ and given that $1/y \rightarrow \infty$ would imply that $y\rightarrow 0$: $$=-\lim_{y\rightarrow 0}y\ln{y}$$ So the limit would be: $$-\lim_{y\rightarrow 0}y\ln{y}=\lim_{x\rightarrow \infty}\frac{\ln{x}}{x} =0$$ I'm just not quite sure whether this is strictly correct, since it seems to me that $1/x$ would approach zero at a different 'rate' than $x$ would and thus the limits wouldn't necessarily have to be the same.",,"['calculus', 'limits', 'limits-without-lhopital']"
43,Pointwise convergence implies uniform convergence under concavity?,Pointwise convergence implies uniform convergence under concavity?,,"Suppose that $X$ is a subset of a Euclidean space and $f_n:X\to\mathbb{R}$ converges pointwise to $f:X\to\mathbb{R}$ . A book I'm reading seems to say that: if $X$ is convex and compact and each $f_n$ is continuous and concave, then the convergence is uniform. Main question : is the above true and, if so, could you please provide a proof (or a reference to a proof)? I know that if $f_n$ is continuous, then it is uniformly continuous but I can't proceed further. Edit 2 : as Nate's answer below demonstrates, the claim is false given the premises as they are. What if we further assume that $f$ is continuous? Sub-question if you have time: can you please give a few references concerning sufficient conditions that allow the implication from pointwise convergence to uniform convergence? I know of Dini's Theorem and Arzelà-Ascoli Theorem. Thank you! Edit 1 to add context : on page 390 of (Monfort and Gourieroux (1995, Vol II)), it is stated: a family $Q_n(\omega,\cdot)$ of concave functions converging pointwise to a function $Q_\infty(\cdot)$ , which is necessarily concave, converges uniformly to that function over any compact subset Here, $\omega$ can be taken as given and $\cdot$ is a place holder for $\theta\in\Theta$ with $\Theta\subset\mathbb{R}^p$ is compact and convex. Hence, the question above.","Suppose that is a subset of a Euclidean space and converges pointwise to . A book I'm reading seems to say that: if is convex and compact and each is continuous and concave, then the convergence is uniform. Main question : is the above true and, if so, could you please provide a proof (or a reference to a proof)? I know that if is continuous, then it is uniformly continuous but I can't proceed further. Edit 2 : as Nate's answer below demonstrates, the claim is false given the premises as they are. What if we further assume that is continuous? Sub-question if you have time: can you please give a few references concerning sufficient conditions that allow the implication from pointwise convergence to uniform convergence? I know of Dini's Theorem and Arzelà-Ascoli Theorem. Thank you! Edit 1 to add context : on page 390 of (Monfort and Gourieroux (1995, Vol II)), it is stated: a family of concave functions converging pointwise to a function , which is necessarily concave, converges uniformly to that function over any compact subset Here, can be taken as given and is a place holder for with is compact and convex. Hence, the question above.","X f_n:X\to\mathbb{R} f:X\to\mathbb{R} X f_n f_n f Q_n(\omega,\cdot) Q_\infty(\cdot) \omega \cdot \theta\in\Theta \Theta\subset\mathbb{R}^p","['analysis', 'limits', 'convergence-divergence', 'convex-analysis', 'uniform-convergence']"
44,Riemann integral confusion 2,Riemann integral confusion 2,,"I evaluate the following limit with Riemann integral $\displaystyle u_n=\frac{1}{n}\sum_{k=1}^n \log(1+\frac{k^2-2k+2}{n^2}),\:\ $$\lim _{n\to \infty } u_n=lim _{n\to \infty\ }\frac{1}{n}\sum_{k=1}^n \log(1+\frac{k^2-2k+2}{n^2})=lim _{n\to \infty\ }\frac{1}{n}\sum _{k=1}^nf\left(\frac{k}{n}\right)$$=\int _0^1\:f\left(x\right)dx$ I know what it is about but at the final I always have problem because I don't know to get the function... and if I put a function how verify if that is correct? I'm total confused  Please make me to understand that, I apreciate! For example, I get this function: f:[0,1]-->R, f(x)= ln(1+x), how I verify if that is correct, because if I put k/n instead x we obtain ln(1+k/n) but we have $$log(1+\frac{k^2-2k+2}{n^2})$$ we must have same value? In general how we find the function ? how we understand which is function?","I evaluate the following limit with Riemann integral $\displaystyle u_n=\frac{1}{n}\sum_{k=1}^n \log(1+\frac{k^2-2k+2}{n^2}),\:\ $$\lim _{n\to \infty } u_n=lim _{n\to \infty\ }\frac{1}{n}\sum_{k=1}^n \log(1+\frac{k^2-2k+2}{n^2})=lim _{n\to \infty\ }\frac{1}{n}\sum _{k=1}^nf\left(\frac{k}{n}\right)$$=\int _0^1\:f\left(x\right)dx$ I know what it is about but at the final I always have problem because I don't know to get the function... and if I put a function how verify if that is correct? I'm total confused  Please make me to understand that, I apreciate! For example, I get this function: f:[0,1]-->R, f(x)= ln(1+x), how I verify if that is correct, because if I put k/n instead x we obtain ln(1+k/n) but we have $$log(1+\frac{k^2-2k+2}{n^2})$$ we must have same value? In general how we find the function ? how we understand which is function?",,"['limits', 'riemann-sum']"
45,Calculate limit without de l'Hospital rule,Calculate limit without de l'Hospital rule,,I have problem with showing that $\displaystyle \lim_{n \to \infty} n (\frac{e}{(1+\frac{1}{n})^n}-1)=\frac{1}{2} $ without de l'Hospital rule I thought to use the rule $\displaystyle \lim_{n\to \infty} n(\ln a-1)=a$,I have problem with showing that $\displaystyle \lim_{n \to \infty} n (\frac{e}{(1+\frac{1}{n})^n}-1)=\frac{1}{2} $ without de l'Hospital rule I thought to use the rule $\displaystyle \lim_{n\to \infty} n(\ln a-1)=a$,,"['calculus', 'limits', 'limits-without-lhopital']"
46,Show that $\lim_{t \to 0} \int_{\mathbb{R}^d}|f(x)-f(x-t)|dx = 0$,Show that,\lim_{t \to 0} \int_{\mathbb{R}^d}|f(x)-f(x-t)|dx = 0,"Let $f \in L^1(\mathbb{R}^d)$. Show that $\lim_{t \to 0} \int_{\mathbb{R}^d}|f(x)-f(x-t)|dx = 0$. What I want to do is bound $|f(x)-f(x-t)|$ above by something and then use the Lebesgue Dominated Convergence theorem to move the limit inside the integral sign, which would solve the problem. I know that $f$ is finite for almost every $x \in \mathbb{R}^d$, but I don't know that it's bounded, so I'm not sure if this is a feasible approach. In a previous question on this homework set I showed that $\int_{\mathbb{R}^d} f(x)dx = \int_{\mathbb{R}^d} f(x+a)dx$, for all $a \in \mathbb{R}^d$, but I'm not sure if that can be applied to this problem. Any hints will be appreciated (it is a homework problem, so I would prefer hints over a complete solution).","Let $f \in L^1(\mathbb{R}^d)$. Show that $\lim_{t \to 0} \int_{\mathbb{R}^d}|f(x)-f(x-t)|dx = 0$. What I want to do is bound $|f(x)-f(x-t)|$ above by something and then use the Lebesgue Dominated Convergence theorem to move the limit inside the integral sign, which would solve the problem. I know that $f$ is finite for almost every $x \in \mathbb{R}^d$, but I don't know that it's bounded, so I'm not sure if this is a feasible approach. In a previous question on this homework set I showed that $\int_{\mathbb{R}^d} f(x)dx = \int_{\mathbb{R}^d} f(x+a)dx$, for all $a \in \mathbb{R}^d$, but I'm not sure if that can be applied to this problem. Any hints will be appreciated (it is a homework problem, so I would prefer hints over a complete solution).",,"['real-analysis', 'limits', 'lebesgue-integral']"
47,Show that $\lim_{x \rightarrow 1} \frac{x^4-2x+1}{x-1} + \sqrt{x} =3$,Show that,\lim_{x \rightarrow 1} \frac{x^4-2x+1}{x-1} + \sqrt{x} =3,"Show that $\lim_{x \rightarrow 1} \frac{x^4-2x+1}{x-1} + \sqrt{x} =3$ from the definition (using $\epsilon-\delta$) Why can't I do something like this? We want: $|\frac{x^4-2x+1}{x-1} + \sqrt{x}-3| = |\frac{x^4-2x+1}{x-1} + \frac{x-9}{\sqrt{x}+3}| \le |\frac{x^4-2x+1}{x-1}| + |\frac{x-9}{\sqrt{x}+3}| \le |x^3+x^2+x-1| + |x-9| \le |x|^3 + |x|^2+|x-1| + |x-9| < \epsilon$ Now suppose $|x-1| < 1$, then $|x|\le 2$ and $|x-9| \le 9$, so $$|x|^3 + |x|^2+|x-1| + |x-9| < 2^3+2^2+9+|x-1| < \epsilon$$ Now take $\delta = \min\{1, \epsilon -21\}$. But this clearly is wrong because $\delta$ needs to be positive for every $\epsilon>0$, what part of my working is incorrect? (I know how to get the answer properly, just wondering why this way is incorrect, I'm quite new to $\epsilon-\delta$ proofs)","Show that $\lim_{x \rightarrow 1} \frac{x^4-2x+1}{x-1} + \sqrt{x} =3$ from the definition (using $\epsilon-\delta$) Why can't I do something like this? We want: $|\frac{x^4-2x+1}{x-1} + \sqrt{x}-3| = |\frac{x^4-2x+1}{x-1} + \frac{x-9}{\sqrt{x}+3}| \le |\frac{x^4-2x+1}{x-1}| + |\frac{x-9}{\sqrt{x}+3}| \le |x^3+x^2+x-1| + |x-9| \le |x|^3 + |x|^2+|x-1| + |x-9| < \epsilon$ Now suppose $|x-1| < 1$, then $|x|\le 2$ and $|x-9| \le 9$, so $$|x|^3 + |x|^2+|x-1| + |x-9| < 2^3+2^2+9+|x-1| < \epsilon$$ Now take $\delta = \min\{1, \epsilon -21\}$. But this clearly is wrong because $\delta$ needs to be positive for every $\epsilon>0$, what part of my working is incorrect? (I know how to get the answer properly, just wondering why this way is incorrect, I'm quite new to $\epsilon-\delta$ proofs)",,"['real-analysis', 'limits', 'self-learning']"
48,"$\lim_{x \to 0} \frac{f(x)-g(x)}{g^{-1}(x)-f^{-1}(x)} = 1$ for any $f,g \in C^1$ that are tangent to $\text{id}$ at $0$ with some simple condition",for any  that are tangent to  at  with some simple condition,"\lim_{x \to 0} \frac{f(x)-g(x)}{g^{-1}(x)-f^{-1}(x)} = 1 f,g \in C^1 \text{id} 0","Theorem For any real functions $f,g \in C^1$ such that $f(0) = g(0) = 0$ and $f'(0) = g'(0) = 1$ and $x$ is strictly between $f(x)$ and $g(x)$ for any $x \ne 0$: $f,g$ are invertible on some open neighbourhood of $0$ $\dfrac{f(x)-g(x)}{g^{-1}(x)-f^{-1}(x)} \to 1$ as $x \to 0$ Questions What is the simplest proof you can think of? I've given mine below. Motivation This theorem was inspired by user8286 's solution to a special case , and I wanted to find weaker conditions on the functions under which the limit in his proof would hold.","Theorem For any real functions $f,g \in C^1$ such that $f(0) = g(0) = 0$ and $f'(0) = g'(0) = 1$ and $x$ is strictly between $f(x)$ and $g(x)$ for any $x \ne 0$: $f,g$ are invertible on some open neighbourhood of $0$ $\dfrac{f(x)-g(x)}{g^{-1}(x)-f^{-1}(x)} \to 1$ as $x \to 0$ Questions What is the simplest proof you can think of? I've given mine below. Motivation This theorem was inspired by user8286 's solution to a special case , and I wanted to find weaker conditions on the functions under which the limit in his proof would hold.",,"['analysis', 'limits']"
49,How do I solve this indeterminate limit without the L'hospital rule?,How do I solve this indeterminate limit without the L'hospital rule?,,"I've been trying to solve this limit without L'Hospital's rule because I don't know how to use derivates yet. So I tried rationalizing the denominator and numerator but it didn't work. $$\lim\limits_{x\to 4} \frac{ \sqrt{2x+1}-3 }{ \sqrt{x-2}-\sqrt{2} }$$ By the way, the answer is supposed to be $\frac{2\sqrt{2}}{3}$.","I've been trying to solve this limit without L'Hospital's rule because I don't know how to use derivates yet. So I tried rationalizing the denominator and numerator but it didn't work. $$\lim\limits_{x\to 4} \frac{ \sqrt{2x+1}-3 }{ \sqrt{x-2}-\sqrt{2} }$$ By the way, the answer is supposed to be $\frac{2\sqrt{2}}{3}$.",,"['calculus', 'limits']"
50,Does the derivative of a bounded smooth monotone function have a limit at infinity?,Does the derivative of a bounded smooth monotone function have a limit at infinity?,,"Let $f \in C^1(\mathbb{R})$ a monotonic function such that $$\lim_{x \to \infty} f(x) = m \in \mathbb{R}$$ Does this imply $\displaystyle\lim_{x \to \infty} f'(x) = 0$? If so, can the hypothesis be relaxed? If not, which additional condition we must require on $f(x)$ so that it works? EDIT: Let's require also that $\displaystyle\lim_{x \to \infty} f'(x)$ exists. How can one prove the claim?","Let $f \in C^1(\mathbb{R})$ a monotonic function such that $$\lim_{x \to \infty} f(x) = m \in \mathbb{R}$$ Does this imply $\displaystyle\lim_{x \to \infty} f'(x) = 0$? If so, can the hypothesis be relaxed? If not, which additional condition we must require on $f(x)$ so that it works? EDIT: Let's require also that $\displaystyle\lim_{x \to \infty} f'(x)$ exists. How can one prove the claim?",,"['calculus', 'real-analysis', 'limits', 'derivatives']"
51,Finding points on graph with tangent lines perpendicular to a line,Finding points on graph with tangent lines perpendicular to a line,,"Find all points $(x,y)$ on the graph of $y=\frac{x}{x-3}$ with tangent lines perpendicular to the line $y=3x-1.$ My thoughts on this problem: First I should find the slope of the given line and the tangent to the given curve. I'm unsure of how to proceed with this though. I know that the slope of the tangent line is equal to $\frac{dy}{dx}$ at any point on the curve. So the slope of the tangent line would be: $$y'=\frac{(x-3)(1)-(x)(1)}{(x-3)^2}=\frac{-3}{(x-3)^2}$$ I also know that the product of the slopes of two perpendicular lines is $-1.$ I'm not sure how to apply this to the problem, and also I'm not sure about how to find $x$- and $y$-coordinates. Overall I'm not sure about how to set this problem up step by step. Thank you.","Find all points $(x,y)$ on the graph of $y=\frac{x}{x-3}$ with tangent lines perpendicular to the line $y=3x-1.$ My thoughts on this problem: First I should find the slope of the given line and the tangent to the given curve. I'm unsure of how to proceed with this though. I know that the slope of the tangent line is equal to $\frac{dy}{dx}$ at any point on the curve. So the slope of the tangent line would be: $$y'=\frac{(x-3)(1)-(x)(1)}{(x-3)^2}=\frac{-3}{(x-3)^2}$$ I also know that the product of the slopes of two perpendicular lines is $-1.$ I'm not sure how to apply this to the problem, and also I'm not sure about how to find $x$- and $y$-coordinates. Overall I'm not sure about how to set this problem up step by step. Thank you.",,"['calculus', 'limits', 'functions', 'derivatives']"
52,A question on $\liminf$ and $\limsup$ and interchanging limits,A question on  and  and interchanging limits,\liminf \limsup,"I have a real function, $f(n,m)$, which is not necessarily bounded nor necessarily non-negative,  but has point-wise convergence of: \begin{equation*} g(m) = \sum\limits_{n=1}^{\infty} f(n,m) \end{equation*} and $g(m)$ is finite for all integers $m$. I am examining the interchange of limits in \begin{equation*} \sum\limits_{m=1}^{\infty} \sum\limits_{n=1}^{\infty} f(n,m) \sim  \sum\limits_{n=1}^{\infty} \sum\limits_{m=1}^{\infty} f(n,m) \end{equation*} and will settle for bounding the summation within a (possibly infinite) range. If I define: \begin{equation*} g_{N}(m) = \inf_{K \ge N} \sum\limits_{k=1}^{K} f(k,m) \le g(m) \end{equation*} and  \begin{equation*} h_{N}(m) = \sup_{K \ge N} \sum\limits_{k=1}^{K} f(k,m) \ge g(m) \end{equation*} Then, can I state the following? IF $\sum\limits_{m=1}^{\infty} g(m)$ converges, then the limit is within the (possibly infinite) range of: \begin{equation*} \begin{aligned} \liminf_{N \to \infty} \liminf_{M \to \infty} \sum\limits_{m=1}^{M} \sum\limits_{n=1}^{N} f(n,m)    &\le \lim_{M \to \infty} \sum\limits_{m=1}^{M} g(m) \\ &\le \limsup_{N \to \infty} \limsup_{M \to \infty} \sum\limits_{m=1}^{M} \sum\limits_{N=1}^{N} f(n,m) \end{aligned} \end{equation*} My reasoning is thus: \begin{equation*} \begin{aligned} \liminf_{N \to \infty} \liminf_{M \to \infty} \sum\limits_{m=1}^{M} \sum\limits_{n=1}^{N} f(n,m)    &\le \liminf_{M \to \infty} \liminf_{N \to \infty} \sum\limits_{m=1}^{M} \sum\limits_{n=1}^{N} f(n,m) \\ &\le \liminf_{M \to \infty} \sum\limits_{m=1}^{M} \sum\limits_{N=1}^{\infty} g_{N}(m) \\ &\le \liminf_{M \to \infty} \sum\limits_{m=1}^{M} g(m) \\ &\le \lim_{M \to \infty} \sum\limits_{m=1}^{M} g(m) \\ &\le \limsup_{M \to \infty} \sum\limits_{m=1}^{M} g(m) \\ &\le \limsup_{M \to \infty} \sum\limits_{m=1}^{M} \sum\limits_{N=1}^{\infty} h_{N}(m) \\ &\le \limsup_{M \to \infty} \limsup_{N \to \infty} \sum\limits_{m=1}^{M} \sum\limits_{N=1}^{N} f(n,m) \\ &\le \limsup_{N \to \infty} \limsup_{M \to \infty} \sum\limits_{m=1}^{M} \sum\limits_{N=1}^{N} f(n,m) \end{aligned} \end{equation*} With the understanding that the  $\liminf$ or $\limsup$ operations could diverge  to $\pm \infty$ or converge to different values, the statement above is similar to Fatou's Lemmas, but with the counting measure and with a much , much weaker convergence statement. My question: Is this weak, weak convergence statement true for all such $f(n,m)$? If it is not true, can a counter example be provided?","I have a real function, $f(n,m)$, which is not necessarily bounded nor necessarily non-negative,  but has point-wise convergence of: \begin{equation*} g(m) = \sum\limits_{n=1}^{\infty} f(n,m) \end{equation*} and $g(m)$ is finite for all integers $m$. I am examining the interchange of limits in \begin{equation*} \sum\limits_{m=1}^{\infty} \sum\limits_{n=1}^{\infty} f(n,m) \sim  \sum\limits_{n=1}^{\infty} \sum\limits_{m=1}^{\infty} f(n,m) \end{equation*} and will settle for bounding the summation within a (possibly infinite) range. If I define: \begin{equation*} g_{N}(m) = \inf_{K \ge N} \sum\limits_{k=1}^{K} f(k,m) \le g(m) \end{equation*} and  \begin{equation*} h_{N}(m) = \sup_{K \ge N} \sum\limits_{k=1}^{K} f(k,m) \ge g(m) \end{equation*} Then, can I state the following? IF $\sum\limits_{m=1}^{\infty} g(m)$ converges, then the limit is within the (possibly infinite) range of: \begin{equation*} \begin{aligned} \liminf_{N \to \infty} \liminf_{M \to \infty} \sum\limits_{m=1}^{M} \sum\limits_{n=1}^{N} f(n,m)    &\le \lim_{M \to \infty} \sum\limits_{m=1}^{M} g(m) \\ &\le \limsup_{N \to \infty} \limsup_{M \to \infty} \sum\limits_{m=1}^{M} \sum\limits_{N=1}^{N} f(n,m) \end{aligned} \end{equation*} My reasoning is thus: \begin{equation*} \begin{aligned} \liminf_{N \to \infty} \liminf_{M \to \infty} \sum\limits_{m=1}^{M} \sum\limits_{n=1}^{N} f(n,m)    &\le \liminf_{M \to \infty} \liminf_{N \to \infty} \sum\limits_{m=1}^{M} \sum\limits_{n=1}^{N} f(n,m) \\ &\le \liminf_{M \to \infty} \sum\limits_{m=1}^{M} \sum\limits_{N=1}^{\infty} g_{N}(m) \\ &\le \liminf_{M \to \infty} \sum\limits_{m=1}^{M} g(m) \\ &\le \lim_{M \to \infty} \sum\limits_{m=1}^{M} g(m) \\ &\le \limsup_{M \to \infty} \sum\limits_{m=1}^{M} g(m) \\ &\le \limsup_{M \to \infty} \sum\limits_{m=1}^{M} \sum\limits_{N=1}^{\infty} h_{N}(m) \\ &\le \limsup_{M \to \infty} \limsup_{N \to \infty} \sum\limits_{m=1}^{M} \sum\limits_{N=1}^{N} f(n,m) \\ &\le \limsup_{N \to \infty} \limsup_{M \to \infty} \sum\limits_{m=1}^{M} \sum\limits_{N=1}^{N} f(n,m) \end{aligned} \end{equation*} With the understanding that the  $\liminf$ or $\limsup$ operations could diverge  to $\pm \infty$ or converge to different values, the statement above is similar to Fatou's Lemmas, but with the counting measure and with a much , much weaker convergence statement. My question: Is this weak, weak convergence statement true for all such $f(n,m)$? If it is not true, can a counter example be provided?",,"['limits', 'limsup-and-liminf']"
53,"Limit involving the Riemann zeta function, why is this identity trivial?","Limit involving the Riemann zeta function, why is this identity trivial?",,"Mathematica knows that: $$n^k=\lim_{s\to 1} \, \frac{\zeta (s) \left(1-\frac{1}{\exp ^{s^{n^k}-1}(n)}\right)}{n}$$ Why is the above a trivial identity? What is it about the Zeta function that makes it obvious? I know experimentally that one can test a zeta zero with the integral: $$\int_0^{\infty } \frac{1}{\exp \left(x^{\frac{1}{\rho _1}}\right)+1} \, dx$$ which resembles the expression inside the parentheses in the limit a little bit. If any one knows how to rewrite the latex of the limit to make it more readable, feel free to edit. As a Mathematica program this limit is: Clear[s, n] Limit[Zeta[s]*(1 - 1/Exp[n]^(s^n^k - 1))/n, s -> 1]","Mathematica knows that: $$n^k=\lim_{s\to 1} \, \frac{\zeta (s) \left(1-\frac{1}{\exp ^{s^{n^k}-1}(n)}\right)}{n}$$ Why is the above a trivial identity? What is it about the Zeta function that makes it obvious? I know experimentally that one can test a zeta zero with the integral: $$\int_0^{\infty } \frac{1}{\exp \left(x^{\frac{1}{\rho _1}}\right)+1} \, dx$$ which resembles the expression inside the parentheses in the limit a little bit. If any one knows how to rewrite the latex of the limit to make it more readable, feel free to edit. As a Mathematica program this limit is: Clear[s, n] Limit[Zeta[s]*(1 - 1/Exp[n]^(s^n^k - 1))/n, s -> 1]",,"['limits', 'riemann-zeta']"
54,How can I find the value of this limit,How can I find the value of this limit,,$$\lim _{ { x }\to { 0 } }{ \frac { \sin x-\arctan x }{  {x }^{ 2 }\log(1+x) } }$$ this log is natural logarithm,$$\lim _{ { x }\to { 0 } }{ \frac { \sin x-\arctan x }{  {x }^{ 2 }\log(1+x) } }$$ this log is natural logarithm,,"['calculus', 'limits']"
55,Limit of a sum (no probabilities),Limit of a sum (no probabilities),,Show that $$\lim_{n\to+\infty}\left(\frac{2}{3}\right)^n\sum_{k=0}^{[n/3]}\binom{n}{k}2^{-k}=\frac{1}{2}$$ without using probabilities. $[\;\cdot\;]$ denotes the integer part.,Show that $$\lim_{n\to+\infty}\left(\frac{2}{3}\right)^n\sum_{k=0}^{[n/3]}\binom{n}{k}2^{-k}=\frac{1}{2}$$ without using probabilities. $[\;\cdot\;]$ denotes the integer part.,,"['limits', 'summation', 'asymptotics', 'binomial-coefficients']"
56,Help $\lim_{k \to \infty} \frac{1-e^{-kt}}{k}=? $,Help,\lim_{k \to \infty} \frac{1-e^{-kt}}{k}=? ,"what is the lime of $ \frac{1-e^{-kt}}{k}$ as $k \to \infty$? Is that just equal $\frac{1}{\infty}=0$? Does any one can help, I am not sure if We can apply L'Hopital's rule. S","what is the lime of $ \frac{1-e^{-kt}}{k}$ as $k \to \infty$? Is that just equal $\frac{1}{\infty}=0$? Does any one can help, I am not sure if We can apply L'Hopital's rule. S",,['limits']
57,"Proving a limit to show an inequality, $\frac{\ln t}{t^{1/3}}$","Proving a limit to show an inequality,",\frac{\ln t}{t^{1/3}},"This is a question on an assignment so please no full solutions, but if anyone could guide me through answering this question I'd be very greatful. Thanks Show that $\displaystyle \frac{\ln t}{t^{1/3}} \rightarrow 0$ as $t \rightarrow \infty$. From the deﬁnition of limit (taking $\epsilon = 1$), deduce that for sufﬁciently large $t$, $(\ln t) ^3 \leq t$.","This is a question on an assignment so please no full solutions, but if anyone could guide me through answering this question I'd be very greatful. Thanks Show that $\displaystyle \frac{\ln t}{t^{1/3}} \rightarrow 0$ as $t \rightarrow \infty$. From the deﬁnition of limit (taking $\epsilon = 1$), deduce that for sufﬁciently large $t$, $(\ln t) ^3 \leq t$.",,['limits']
58,"Limits using epsilon delta definition $f(x,y)=xy$ for functions of two variables",Limits using epsilon delta definition  for functions of two variables,"f(x,y)=xy","Prove: using $\epsilon$-$\delta$ definition, the limit of both $f$ and $g$ as $(x,y)\to (0,0)$ is $0$. $f(x,y)=xy$ $g(x,y)=\frac{xy}{x^2 +y^2+1}$ Also, for Q2 can I convert $g(x,y)$ to $m(x,y)/n(x,y)=g(x,y)$ using arithmetic of limits, then prove using $\epsilon$-$\delta$ definition the limit of function $m$ and $n$ separately; then combine the two? Thanks :) I wonder if this is correct: $|xy-0|<\epsilon$ given $|x-0|< \delta $ and $|y-0|< \delta $ $|xy-0|< |x-0||y-0|<\delta^2=\epsilon$ therefore: $\delta<\epsilon^{1/2}$","Prove: using $\epsilon$-$\delta$ definition, the limit of both $f$ and $g$ as $(x,y)\to (0,0)$ is $0$. $f(x,y)=xy$ $g(x,y)=\frac{xy}{x^2 +y^2+1}$ Also, for Q2 can I convert $g(x,y)$ to $m(x,y)/n(x,y)=g(x,y)$ using arithmetic of limits, then prove using $\epsilon$-$\delta$ definition the limit of function $m$ and $n$ separately; then combine the two? Thanks :) I wonder if this is correct: $|xy-0|<\epsilon$ given $|x-0|< \delta $ and $|y-0|< \delta $ $|xy-0|< |x-0||y-0|<\delta^2=\epsilon$ therefore: $\delta<\epsilon^{1/2}$",,"['limits', 'multivariable-calculus']"
59,Evaluate the sum $\lim_{t\to{1-0}}(1-t)(\frac{t}{1+t}+\frac{t^2}{1+t^2}+...+\frac{t^n}{1+t^n}+...)$,Evaluate the sum,\lim_{t\to{1-0}}(1-t)(\frac{t}{1+t}+\frac{t^2}{1+t^2}+...+\frac{t^n}{1+t^n}+...),I have a question about this limit. Calculate$$\lim_{t\to{1-0}}(1-t)(\frac{t}{1+t}+\frac{t^2}{1+t^2}+...+\frac{t^n}{1+t^n}+...)$$ Can anyone help?,I have a question about this limit. Calculate$$\lim_{t\to{1-0}}(1-t)(\frac{t}{1+t}+\frac{t^2}{1+t^2}+...+\frac{t^n}{1+t^n}+...)$$ Can anyone help?,,"['calculus', 'limits']"
60,"Calculating $f(0), f'(0), f''(0)$ for a function $f$ satisfying $\lim_{x \to 0} (1 + x + f(x)/x)^{1/x} = e^3$.",Calculating  for a function  satisfying .,"f(0), f'(0), f''(0) f \lim_{x \to 0} (1 + x + f(x)/x)^{1/x} = e^3","I'm trying to do the following problem and could use some help(from Apostol, Calculus, Volume I , 7.11 Ex. 33 p. 291): A funtion $f$ has a continuous third derivative everywhere and   satisfies the relation $$ \lim_{x \to 0} \left(1 + x + \dfrac{f(x)}{x}\right)^{1/x} = e^3.$$ Compute $f(0), f'(0), f''(0),$ and $\lim_{x \to 0} \left(1 + \frac{f(x)}{x}\right)^{1/x}$. [ Hint: If $\lim_{x \to 0} g(x) = A$, then $g(x) = A + o(1)$ as $x \to 0$.] The book gives the following as answers: $f(0) = 0, f'(0) = 0, f''(0) = 4$ and the limit $= e^2$. I cannot seem to make any forward progress on this.  It is the last Exercise in a section of exercises on taking limits by using polynomial expansions of functions.  (This set of Exercises is immediately before the section on L'Hopital's rule... I don't know if that is applicable here, but a solution without it is appreciated since Apostol intends this to be done without it.) My initial attempts involve writing: $$\begin{align*} & \lim_{x \to 0} \left(1 + x + \dfrac{f(x)}{x}\right)^{1/x} &= e^3.\\ \implies & \lim_{x \to 0} \left(e^{(1/x)\log(1 + x + f(x)/x)}\right) &= e^3.\\ \implies & \lim_{x \to 0} \left(e^{-\frac{\log x}{x} + \frac{1}{x} \log (x + x^2 + f(x))}\right) &= e^3. \end{align*}$$ I wanted to do this to attempt to get to a point that I could write a polynomial expansion of $\log$ at $0$, but I can't seem to make any progress on that front.  Maybe there is a better way to simplify things? Thanks for any help.  Full solutions or hints are equally welcome.","I'm trying to do the following problem and could use some help(from Apostol, Calculus, Volume I , 7.11 Ex. 33 p. 291): A funtion $f$ has a continuous third derivative everywhere and   satisfies the relation $$ \lim_{x \to 0} \left(1 + x + \dfrac{f(x)}{x}\right)^{1/x} = e^3.$$ Compute $f(0), f'(0), f''(0),$ and $\lim_{x \to 0} \left(1 + \frac{f(x)}{x}\right)^{1/x}$. [ Hint: If $\lim_{x \to 0} g(x) = A$, then $g(x) = A + o(1)$ as $x \to 0$.] The book gives the following as answers: $f(0) = 0, f'(0) = 0, f''(0) = 4$ and the limit $= e^2$. I cannot seem to make any forward progress on this.  It is the last Exercise in a section of exercises on taking limits by using polynomial expansions of functions.  (This set of Exercises is immediately before the section on L'Hopital's rule... I don't know if that is applicable here, but a solution without it is appreciated since Apostol intends this to be done without it.) My initial attempts involve writing: $$\begin{align*} & \lim_{x \to 0} \left(1 + x + \dfrac{f(x)}{x}\right)^{1/x} &= e^3.\\ \implies & \lim_{x \to 0} \left(e^{(1/x)\log(1 + x + f(x)/x)}\right) &= e^3.\\ \implies & \lim_{x \to 0} \left(e^{-\frac{\log x}{x} + \frac{1}{x} \log (x + x^2 + f(x))}\right) &= e^3. \end{align*}$$ I wanted to do this to attempt to get to a point that I could write a polynomial expansion of $\log$ at $0$, but I can't seem to make any progress on that front.  Maybe there is a better way to simplify things? Thanks for any help.  Full solutions or hints are equally welcome.",,['calculus']
61,Question about finding the limit at an undefined point.,Question about finding the limit at an undefined point.,,"This may be braindead, but I'm trying! If I have a function $f$ and that function is not defined at some x, then asking for the derivative of the function at $x$ makes no sense since there is no $f(x)$ at $x$. But if I want to find a gradient for that function as close as possible to x, then how does that work?  Isn't that the same as the derivative at x?  It's like, I can do the same calculation but I have to disregard the result because I'm asking for something that doesn't exist. For example, if $f(x)=\frac{1}{x−2}$, then $f(x)$ is not defined at $x=2$. So I can't find the derivative at that point since it doesn't exist, But the limit is 2. But the limit is the derivative, and the derivative doesn't exist! I'm confused. I felt like I understood this but I woke up this morning with no idea.  Last week I was happily finding the volume of cylindrical wedges, now I can't understand limits O_O Please set me straight. EDIT: I think my problem is the way I'm thinking about limits.  It seems that there are two limits and I'm confusing them.  The limit that $f(x)$ approaches and the limit that $x+h$ approaches.  In the above example where $f(x)=\frac{1}{x-2}$, $2+h$ approaches $2$ and $f'(x)$ is undefined since the numerator contains a division by zero. Is that my answer? 2nd EDIT:  This is what I'm really asking: How do I find $lim_{x\to a}f'(x)$?","This may be braindead, but I'm trying! If I have a function $f$ and that function is not defined at some x, then asking for the derivative of the function at $x$ makes no sense since there is no $f(x)$ at $x$. But if I want to find a gradient for that function as close as possible to x, then how does that work?  Isn't that the same as the derivative at x?  It's like, I can do the same calculation but I have to disregard the result because I'm asking for something that doesn't exist. For example, if $f(x)=\frac{1}{x−2}$, then $f(x)$ is not defined at $x=2$. So I can't find the derivative at that point since it doesn't exist, But the limit is 2. But the limit is the derivative, and the derivative doesn't exist! I'm confused. I felt like I understood this but I woke up this morning with no idea.  Last week I was happily finding the volume of cylindrical wedges, now I can't understand limits O_O Please set me straight. EDIT: I think my problem is the way I'm thinking about limits.  It seems that there are two limits and I'm confusing them.  The limit that $f(x)$ approaches and the limit that $x+h$ approaches.  In the above example where $f(x)=\frac{1}{x-2}$, $2+h$ approaches $2$ and $f'(x)$ is undefined since the numerator contains a division by zero. Is that my answer? 2nd EDIT:  This is what I'm really asking: How do I find $lim_{x\to a}f'(x)$?",,"['calculus', 'limits', 'derivatives']"
62,Can you verify my ideas here with the chain rule using limits?,Can you verify my ideas here with the chain rule using limits?,,"I've been working on understanding limits thoroughly, so I'm rewriting how I understand the chain rule.  Please help me fill in my gaps in understanding. $f$ is some function. Then $f'(x) = \lim\limits_{h \to 0}\frac{f(x+h)-f(x)}{h}$ Now I might want to evaluate something like $$\left(f(g(x))\right)' = \lim\limits_{h \to 0}\frac{f(g(x+h))-f(g(x))}{h}$$ Evaluating this is tricky, so we need a way to do it. $\left(f(g(x))\right)' = (\lim\limits_{h \to 0}\frac{f(g(x+h))-f(g(x))}{g(x+h)-g(x)}\cdot\frac{g(x+h)-g(x)}{h}$) $\left(f(g(x))\right)' = (\lim\limits_{h \to 0}\frac{f(g(x+h))-f(g(x))}{g(x+h)-g(x)})\cdot\lim\limits_{h \to 0}(\frac{g(x+h)-g(x)}{h})$ if $k=g(x+h)-g(x)$, then $g(x+h)=k+g(x)$, so $\left(f(g(x))\right)' = (\lim\limits_{h \to 0}\frac{f(g(x)+k)-f(g(x))}{k})\cdot(\lim\limits_{h \to 0}\frac{g(x+h)-g(x)}{h})$ and, assuming I can go ahead and just change the limit variable on the left term, then $=(\lim\limits_{k \to 0}\frac{f(g(x)+k)-f(g(x))}{k}) \cdot (\lim\limits_{h \to 0}\frac{g(x+h)-g(x)}{h})$ $=f'(g(x))\cdot g'(x)$ Which is easier to figure out, and is also the chain rule. Is that correct?","I've been working on understanding limits thoroughly, so I'm rewriting how I understand the chain rule.  Please help me fill in my gaps in understanding. $f$ is some function. Then $f'(x) = \lim\limits_{h \to 0}\frac{f(x+h)-f(x)}{h}$ Now I might want to evaluate something like $$\left(f(g(x))\right)' = \lim\limits_{h \to 0}\frac{f(g(x+h))-f(g(x))}{h}$$ Evaluating this is tricky, so we need a way to do it. $\left(f(g(x))\right)' = (\lim\limits_{h \to 0}\frac{f(g(x+h))-f(g(x))}{g(x+h)-g(x)}\cdot\frac{g(x+h)-g(x)}{h}$) $\left(f(g(x))\right)' = (\lim\limits_{h \to 0}\frac{f(g(x+h))-f(g(x))}{g(x+h)-g(x)})\cdot\lim\limits_{h \to 0}(\frac{g(x+h)-g(x)}{h})$ if $k=g(x+h)-g(x)$, then $g(x+h)=k+g(x)$, so $\left(f(g(x))\right)' = (\lim\limits_{h \to 0}\frac{f(g(x)+k)-f(g(x))}{k})\cdot(\lim\limits_{h \to 0}\frac{g(x+h)-g(x)}{h})$ and, assuming I can go ahead and just change the limit variable on the left term, then $=(\lim\limits_{k \to 0}\frac{f(g(x)+k)-f(g(x))}{k}) \cdot (\lim\limits_{h \to 0}\frac{g(x+h)-g(x)}{h})$ $=f'(g(x))\cdot g'(x)$ Which is easier to figure out, and is also the chain rule. Is that correct?",,"['calculus', 'limits', 'derivatives']"
63,Lower bounding a ratio of gamma functions,Lower bounding a ratio of gamma functions,,"I am trying to show that the following function has a lower bound of $\ \frac{1}{2}$ for all $c\geq 2$ .  Or, alternatively, that that function increases with $\ c$ : $$\frac{\Gamma\left[1+\frac{1}{c}\right] \Gamma[1+c]}{\Gamma\left[1+\frac{1}{c}+c\right]}$$ Note that this can be rewritten a few ways, such as $\ c\text{ B}[1+\frac{1}{c}, c]$ Plotting it out to 10,000 or so, it seems very clear that it is increasing, approaching 1, but I can't figure out how to prove it either through integration or properties of the special functions.  Can anyone help me out?  Thank you! Note:  I found in this document in Theorem 3, if $$(a-1)(b-1) \geq 0$$ then $$\text B(a,b) \geq \frac{1}{ab}$$ So, I thought that if I  took the function as $\ c \text B(1+ \frac{1}{c},c)$ where $\ (\frac{1}{c})(c-1) \geq 0$ we have $$c \text B(1+ \frac{1}{c},c) \geq  \frac{c}{\frac{c+1}{c} c} = \frac{c}{c+1} > \frac{1}{2}$$ However, I must be doing something wrong, as the value of $\ c \text B(1+ \frac{1}{c},c)$ when $c=2$ is actually less than $\frac{2}{3}$ (though greater than $\frac{1}{2}$ )","I am trying to show that the following function has a lower bound of for all .  Or, alternatively, that that function increases with : Note that this can be rewritten a few ways, such as Plotting it out to 10,000 or so, it seems very clear that it is increasing, approaching 1, but I can't figure out how to prove it either through integration or properties of the special functions.  Can anyone help me out?  Thank you! Note:  I found in this document in Theorem 3, if then So, I thought that if I  took the function as where we have However, I must be doing something wrong, as the value of when is actually less than (though greater than )","\ \frac{1}{2} c\geq 2 \ c \frac{\Gamma\left[1+\frac{1}{c}\right] \Gamma[1+c]}{\Gamma\left[1+\frac{1}{c}+c\right]} \ c\text{ B}[1+\frac{1}{c}, c] (a-1)(b-1) \geq 0 \text B(a,b) \geq \frac{1}{ab} \ c \text B(1+ \frac{1}{c},c) \ (\frac{1}{c})(c-1) \geq 0 c \text B(1+ \frac{1}{c},c) \geq  \frac{c}{\frac{c+1}{c} c} = \frac{c}{c+1} > \frac{1}{2} \ c \text B(1+ \frac{1}{c},c) c=2 \frac{2}{3} \frac{1}{2}","['inequality', 'limits', 'special-functions', 'gamma-function']"
64,Solving $\frac{df}{dx} = 2x$ using the definition of a derivative. [closed],Solving  using the definition of a derivative. [closed],\frac{df}{dx} = 2x,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 6 months ago . Improve this question Although it's pretty straightforward to solve the above equation but I was wondering, how about if we use the limit definition of a derivative and then try to solve it. So, the equation looks like $$\lim_{h\to 0}\frac{f(x+h)-f(x)}{h} = 2x$$ Now how should I go about solving it? IMO guessing might be one of the ways but I am not aware of any of the ""formal"" ones. Any kind of help will be appreciated.","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 6 months ago . Improve this question Although it's pretty straightforward to solve the above equation but I was wondering, how about if we use the limit definition of a derivative and then try to solve it. So, the equation looks like Now how should I go about solving it? IMO guessing might be one of the ways but I am not aware of any of the ""formal"" ones. Any kind of help will be appreciated.",\lim_{h\to 0}\frac{f(x+h)-f(x)}{h} = 2x,"['limits', 'derivatives', 'limits-without-lhopital']"
65,Calculating a limit of a sequence of integrals that approaches the catalan constant,Calculating a limit of a sequence of integrals that approaches the catalan constant,,"Let $f\colon[0, 1] \to \mathbb{R}$ , $f(x)=\frac{\arctan(x)}{x}, x \in (0, 1], f(0)=1$ . Prove that $$\lim_{n \to \infty} n \left(\frac{\pi}{4}-n\int_0^1\frac{x^n}{1+x^{2n}}dx \right)=\int_0^1f(x)dx$$ My approach was to make the change of variables $u=x^n, \text{ which is bijective on [0, 1]. Therefore } x=u^{\frac{1}{n}} \implies dx=\frac{1}{n}u^{\frac{1}{n}-1}; \frac{\pi}{4}=\int_0^1\frac{1}{1+x^2}dx$ . Now we rewrite the requested limit: $$n \left(\frac{\pi}{4}-n\int_0^1\frac{x^n}{1+x^{2n}}dx \right)=n \left(\int_0^1\frac{1}{1+u^2}du-\int_0^1\frac{u^{\frac{1}{n}}}{1+u^{2}}dx \right)=\int_0^1\frac{-\frac{u^{\frac{1}{n}}-1}{\frac{1}{n}}}{1+u^2}du$$ . My question is now can we interchange the limit with the integral? $\frac{u^h-1}{h}$ approaches $\ln(u)$ when $h$ approaches $0$ , so it would make sense that $$\lim_{n \to \infty}\int_0^1\frac{-\frac{u^{\frac{1}{n}}-1}{\frac{1}{n}}}{1+u^2}du=\int_0^1\frac{-\ln(u)}{1+u^2}du$$ . Now if we do integration by parts on $\int_0^1f(x)dx$ we obtain $$\int_0^1f(x)dx=\arctan(x)\ln(x)\mid_0^1-\int_0^1\frac{\ln(x)}{1+x^2}dx=\int_0^1\frac{-\ln(u)}{1+u^2}du$$ I have not yet learned in school about uniform convergence, but I read by myself that if a sequence of functions is uniformly convergent to some function, then we can interchange the limit. Is it true that $\frac{u^h-1}{h}$ converges uniformly to $\ln(u)$ ? It seems obvious, but I can't seem to prove it, since $u=0$ makes the function behave badly. But again $u=0$ is the only problem, it is only one point of discontinuity so it should not influence the value that much. I do not know if this is relevant, but I looked up and found that $\int_0^1f(x)dx$ is the catalan constant. Any help in solving this problem regarding the interchange of limits would be gladly appreciated.","Let , . Prove that My approach was to make the change of variables . Now we rewrite the requested limit: . My question is now can we interchange the limit with the integral? approaches when approaches , so it would make sense that . Now if we do integration by parts on we obtain I have not yet learned in school about uniform convergence, but I read by myself that if a sequence of functions is uniformly convergent to some function, then we can interchange the limit. Is it true that converges uniformly to ? It seems obvious, but I can't seem to prove it, since makes the function behave badly. But again is the only problem, it is only one point of discontinuity so it should not influence the value that much. I do not know if this is relevant, but I looked up and found that is the catalan constant. Any help in solving this problem regarding the interchange of limits would be gladly appreciated.","f\colon[0, 1] \to \mathbb{R} f(x)=\frac{\arctan(x)}{x}, x \in (0, 1], f(0)=1 \lim_{n \to \infty} n \left(\frac{\pi}{4}-n\int_0^1\frac{x^n}{1+x^{2n}}dx \right)=\int_0^1f(x)dx u=x^n, \text{ which is bijective on [0, 1]. Therefore } x=u^{\frac{1}{n}} \implies dx=\frac{1}{n}u^{\frac{1}{n}-1}; \frac{\pi}{4}=\int_0^1\frac{1}{1+x^2}dx n \left(\frac{\pi}{4}-n\int_0^1\frac{x^n}{1+x^{2n}}dx \right)=n \left(\int_0^1\frac{1}{1+u^2}du-\int_0^1\frac{u^{\frac{1}{n}}}{1+u^{2}}dx \right)=\int_0^1\frac{-\frac{u^{\frac{1}{n}}-1}{\frac{1}{n}}}{1+u^2}du \frac{u^h-1}{h} \ln(u) h 0 \lim_{n \to \infty}\int_0^1\frac{-\frac{u^{\frac{1}{n}}-1}{\frac{1}{n}}}{1+u^2}du=\int_0^1\frac{-\ln(u)}{1+u^2}du \int_0^1f(x)dx \int_0^1f(x)dx=\arctan(x)\ln(x)\mid_0^1-\int_0^1\frac{\ln(x)}{1+x^2}dx=\int_0^1\frac{-\ln(u)}{1+u^2}du \frac{u^h-1}{h} \ln(u) u=0 u=0 \int_0^1f(x)dx","['real-analysis', 'limits', 'improper-integrals', 'uniform-convergence', 'catalans-constant']"
66,"given $xf''(x) + f'(x)+f(x)\leq 0\,\forall x > 0.$ Find $\lim\limits_{x\to\infty} f(x)$.",given  Find .,"xf''(x) + f'(x)+f(x)\leq 0\,\forall x > 0. \lim\limits_{x\to\infty} f(x)","Let $f:\mathbb{R}^+\to\mathbb{R}^+$ be a twice-differentiable function so that $xf''(x) + f'(x)+f(x)\leq 0\,\forall x > 0.$ Find $\lim\limits_{x\to\infty} f(x)$ . It may be useful to consider a function $g(x)$ defined in terms of $f,f',$ and/or $f''$ . We have $(\dfrac{f'(x)}{f(x)})' = \dfrac{f''(x)f(x)-f'(x)^2}{f(x)^2}, (x\dfrac{f'(x)}{f(x)})' = \dfrac{f'(x)}{f(x)} +x\dfrac{f''(x)f(x)-f'(x)^2}{f(x)^2} = \dfrac{f'(x)f(x) + xf''(x)f(x)-f'(x)^2 x}{f(x)^2}.$ Adding 1 to the latter derivative gives $\dfrac{f(x)(xf''(x)+f'(x)+f(x)) - f'(x)^2 x}{f(x)^2}.$ The MVT or Taylor's theorem might be useful too. I was also thinking of finding specific functions that actually satisfy the inequality to have a better idea of what approach to use. Note that no linear polynomial or constant $f(x)$ satisfies the inequality in the question. If $f=ax^2 + bx+c$ then $xf''(x) + f'(x) + f(x) = x(2a) + 2ax+b + ax^2 + bx+c$ and so by the inequality we must have $a < 0.$ But this is impossible since $f(x)>0\,\forall x.$ Hence f cannot be a quadratic either.",Let be a twice-differentiable function so that Find . It may be useful to consider a function defined in terms of and/or . We have Adding 1 to the latter derivative gives The MVT or Taylor's theorem might be useful too. I was also thinking of finding specific functions that actually satisfy the inequality to have a better idea of what approach to use. Note that no linear polynomial or constant satisfies the inequality in the question. If then and so by the inequality we must have But this is impossible since Hence f cannot be a quadratic either.,"f:\mathbb{R}^+\to\mathbb{R}^+ xf''(x) + f'(x)+f(x)\leq 0\,\forall x > 0. \lim\limits_{x\to\infty} f(x) g(x) f,f', f'' (\dfrac{f'(x)}{f(x)})' = \dfrac{f''(x)f(x)-f'(x)^2}{f(x)^2}, (x\dfrac{f'(x)}{f(x)})' = \dfrac{f'(x)}{f(x)} +x\dfrac{f''(x)f(x)-f'(x)^2}{f(x)^2} = \dfrac{f'(x)f(x) + xf''(x)f(x)-f'(x)^2 x}{f(x)^2}. \dfrac{f(x)(xf''(x)+f'(x)+f(x)) - f'(x)^2 x}{f(x)^2}. f(x) f=ax^2 + bx+c xf''(x) + f'(x) + f(x) = x(2a) + 2ax+b + ax^2 + bx+c a < 0. f(x)>0\,\forall x.","['real-analysis', 'calculus', 'limits', 'derivatives', 'continuity']"
67,How to rigorously fill in certain steps in the proof of L'Hôpital's Rule as it appears in Spivak's Calculus? [duplicate],How to rigorously fill in certain steps in the proof of L'Hôpital's Rule as it appears in Spivak's Calculus? [duplicate],,"This question already has answers here : Understanding the Proof of L'Hopital's Rule (2 answers) Closed 2 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved The following is L'Hôpital's Rule as it appears in Spivak's Calculus Suppose that $\lim\limits_{x \to a} f(x) = \lim\limits_{x \to a}  g(x)=0$ and suppose also that $\lim\limits_{x \to a} \frac{f'(x)}{g'(x)}$ exists. Then $\lim\limits_{x \to a} \frac{f(x)}{g(x)}$ exists, and $$\lim\limits_{x \to a} \frac{f(x)}{g(x)}= \lim\limits_{x \to a}  \frac{f'(x)}{g'(x)}$$ Spivak then provides a proof of the theorem, and at the very end writes (Once again, the reader is invited to supply the details of this part of the argument) The following is my attempt at supplying details of certain steps in the proof. There are two steps which I understand at an intuitive level, but not at a formal mathematical level. The hypothesis that $\lim\limits_{x \to a} \frac{f'(x)}{g'(x)}$ exists contains two implicit assumptions (1) there is an interval $(a-\delta, a+\delta)$ such that $f'(x)$ and $g'(x)$ exist for all $x$ in $(a-\delta, a+\delta)$ except, perhaps, for $x=a$ (2) in this interval $g'(x) \neq 0$ with, once again, the possible exception of $x=a$ To show this, recall that $\lim\limits_{x \to a} \frac{f'(x)}{g'(x)}=l$ means $$\forall \epsilon>0\ \exists \delta>0\ \forall x,\ 0<|x-a|<\delta \implies \left | \frac{f'(x)}{g'(x)} -l \right |<\epsilon$$ Therefore, $\frac{f'(x)}{g'(x)}$ is defined in $(a-\delta) \cup (a+\delta)$ , so $f'(x)$ and $g'(x)$ both exist and $g'(x) \neq 0$ in this interval. Note that since this is a limit for $x$ approaching $a$ nothing is said about $f'$ and $g'$ at $a$ . On the other hand, $f$ and $g$ are not even assumed to be defined at $a$ That is, all we assumed was $\lim\limits_{x \to a} f(x) = \lim\limits_{x \to a}  g(x)=0$ . This says something about values of $f$ and $g$ near $a$ but not at $a$ . If we define $f(a)=g(a)=0$ (changing the previous values of $f(a)$ and $g(a)$ , if necessary), then $f$ and $g$ are continuous at $a$ . If $a<x<a+\delta$ , then the Mean Value Theorem and the Cauchy Mean Value Theorem apply to $f$ and $g$ on the interval $[a,x]$ (and a similar statement holds for $a-\delta<x<a$ . Why is the following step necessary First applying the Mean Value Theorem to $g$ , we see that $g(x) \neq > 0$ , for if $g(x)=0$ there would be some $x_1$ in $(a,x)$ with $g'(x_1)=0$ , contradicting $(2)$ . Didn't we already know this fact, precisely from $(2)$ ? Now applying the Cauchy Mean Value Theorem to $f$ and $g$ , we see that there is a number $\alpha_x$ in $(a,x)$ such that $$[f(x)-0]g'(\alpha_x)=[g(x)-0]f'(\alpha_x)$$ or $$\frac{f(x)}{g(x)}=\frac{f'(\alpha_x)}{g'(\alpha_x)}$$ Now $\alpha_x$ approaches $a$ as $x$ approaches $a$ , because $\alpha_x$ is in $(a,x)$ . I understand this as: since $|\alpha_x-a|<|x-a|$ so $0<|x-a|<\delta \implies 0<|\alpha_x-a|<\delta$ Since we are assuming that $\lim\limits_{y \to a} \frac{f'(y)}{g'(y)}$ exists, it follows that $$\lim\limits_{x \to a} \frac{f(x)}{g(x)}=\lim\limits_{x \to a}  \frac{f'(\alpha_x)}{g'(\alpha_x)}=\lim\limits_{y \to a}  \frac{f'(y)}{g'(y)}$$ I don't quite understand how to prove this last step. $\lim\limits_{x \to a} \frac{f'(\alpha_x)}{g'(\alpha_x)}=l$ means $$\forall \epsilon\ \exists \delta>0\ \forall x, 0<|x-a|<\delta  \implies \left | \frac{f'(\alpha_x)}{g'(\alpha_x)}-l \right |<\epsilon$$ Where is the implication from $0<|\alpha_x-a|<\delta$ to $\left | \frac{f'(\alpha_x)}{g'(\alpha_x)}-l \right |$ ?","This question already has answers here : Understanding the Proof of L'Hopital's Rule (2 answers) Closed 2 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved The following is L'Hôpital's Rule as it appears in Spivak's Calculus Suppose that and suppose also that exists. Then exists, and Spivak then provides a proof of the theorem, and at the very end writes (Once again, the reader is invited to supply the details of this part of the argument) The following is my attempt at supplying details of certain steps in the proof. There are two steps which I understand at an intuitive level, but not at a formal mathematical level. The hypothesis that exists contains two implicit assumptions (1) there is an interval such that and exist for all in except, perhaps, for (2) in this interval with, once again, the possible exception of To show this, recall that means Therefore, is defined in , so and both exist and in this interval. Note that since this is a limit for approaching nothing is said about and at . On the other hand, and are not even assumed to be defined at That is, all we assumed was . This says something about values of and near but not at . If we define (changing the previous values of and , if necessary), then and are continuous at . If , then the Mean Value Theorem and the Cauchy Mean Value Theorem apply to and on the interval (and a similar statement holds for . Why is the following step necessary First applying the Mean Value Theorem to , we see that , for if there would be some in with , contradicting . Didn't we already know this fact, precisely from ? Now applying the Cauchy Mean Value Theorem to and , we see that there is a number in such that or Now approaches as approaches , because is in . I understand this as: since so Since we are assuming that exists, it follows that I don't quite understand how to prove this last step. means Where is the implication from to ?","\lim\limits_{x \to a} f(x) = \lim\limits_{x \to a}
 g(x)=0 \lim\limits_{x \to a} \frac{f'(x)}{g'(x)} \lim\limits_{x \to a} \frac{f(x)}{g(x)} \lim\limits_{x \to a} \frac{f(x)}{g(x)}= \lim\limits_{x \to a}
 \frac{f'(x)}{g'(x)} \lim\limits_{x \to a} \frac{f'(x)}{g'(x)} (a-\delta, a+\delta) f'(x) g'(x) x (a-\delta, a+\delta) x=a g'(x) \neq 0 x=a \lim\limits_{x \to a} \frac{f'(x)}{g'(x)}=l \forall \epsilon>0\ \exists \delta>0\ \forall x,\ 0<|x-a|<\delta \implies \left | \frac{f'(x)}{g'(x)} -l \right |<\epsilon \frac{f'(x)}{g'(x)} (a-\delta) \cup (a+\delta) f'(x) g'(x) g'(x) \neq 0 x a f' g' a f g a \lim\limits_{x \to a} f(x) = \lim\limits_{x \to a}
 g(x)=0 f g a a f(a)=g(a)=0 f(a) g(a) f g a a<x<a+\delta f g [a,x] a-\delta<x<a g g(x) \neq
> 0 g(x)=0 x_1 (a,x) g'(x_1)=0 (2) (2) f g \alpha_x (a,x) [f(x)-0]g'(\alpha_x)=[g(x)-0]f'(\alpha_x) \frac{f(x)}{g(x)}=\frac{f'(\alpha_x)}{g'(\alpha_x)} \alpha_x a x a \alpha_x (a,x) |\alpha_x-a|<|x-a| 0<|x-a|<\delta \implies 0<|\alpha_x-a|<\delta \lim\limits_{y \to a} \frac{f'(y)}{g'(y)} \lim\limits_{x \to a} \frac{f(x)}{g(x)}=\lim\limits_{x \to a}
 \frac{f'(\alpha_x)}{g'(\alpha_x)}=\lim\limits_{y \to a}
 \frac{f'(y)}{g'(y)} \lim\limits_{x \to a} \frac{f'(\alpha_x)}{g'(\alpha_x)}=l \forall \epsilon\ \exists \delta>0\ \forall x, 0<|x-a|<\delta  \implies \left | \frac{f'(\alpha_x)}{g'(\alpha_x)}-l \right |<\epsilon 0<|\alpha_x-a|<\delta \left | \frac{f'(\alpha_x)}{g'(\alpha_x)}-l \right |","['calculus', 'limits', 'derivatives', 'proof-explanation']"
68,"If $a$ is not a power of $10$, when $\lim\limits_{n\to\infty} S(a^n)=\infty$? ($s(m)$ is the sum of the digits of $m$.) [closed]","If  is not a power of , when ? ( is the sum of the digits of .) [closed]",a 10 \lim\limits_{n\to\infty} S(a^n)=\infty s(m) m,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question If $a$ is not a power of $10$ , when $$\lim\limits_{n\to\infty} S(a^n)=\infty?$$ ( $s(m)$ is the sum of the digits of $m$ .) (I don't have any ideas to prove this.)","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question If is not a power of , when ( is the sum of the digits of .) (I don't have any ideas to prove this.)",a 10 \lim\limits_{n\to\infty} S(a^n)=\infty? s(m) m,"['limits', 'number-theory']"
69,"Let $f(x)\triangleq\ln (1+x)$, and for all integer $n\ge 2$, let $f^n(x)\triangleq f(f^{n-1}(x))$. Calculate $\lim_{x\to\infty\\n\to\infty} f^n(x)=?$","Let , and for all integer , let . Calculate",f(x)\triangleq\ln (1+x) n\ge 2 f^n(x)\triangleq f(f^{n-1}(x)) \lim_{x\to\infty\\n\to\infty} f^n(x)=?,"Let $f\colon [0,\infty)\to [0,\infty)$ be defined by $f(x)\triangleq\ln (1+x)$ . For all integer $n\ge 2$ , let $f^n(x)\triangleq f(f^{n-1}(x))$ , where $f^{1}(x)\triangleq f(x)$ . How can we calculate the following limit? \begin{equation} \lim_{x\to\infty\\n\to\infty} f^n(x)=? \end{equation} Here is my attempt: Since $\ln x$ is much slower than $x$ , I think this limit is 0 but I do not know how to calculate it.","Let be defined by . For all integer , let , where . How can we calculate the following limit? Here is my attempt: Since is much slower than , I think this limit is 0 but I do not know how to calculate it.","f\colon [0,\infty)\to [0,\infty) f(x)\triangleq\ln (1+x) n\ge 2 f^n(x)\triangleq f(f^{n-1}(x)) f^{1}(x)\triangleq f(x) \begin{equation}
\lim_{x\to\infty\\n\to\infty} f^n(x)=?
\end{equation} \ln x x","['real-analysis', 'limits', 'multivariable-calculus']"
70,$f:\mathbb{R} \mapsto \mathbb{R}$ convex and increasing. Show there is $a \in \mathbb{R}$ such that $\lim\limits_{x\to -\infty}\frac{f(x)}{x}=a$,convex and increasing. Show there is  such that,f:\mathbb{R} \mapsto \mathbb{R} a \in \mathbb{R} \lim\limits_{x\to -\infty}\frac{f(x)}{x}=a,"Let $f:\mathbb{R} \to \mathbb{R}$ be a convex and increasing function such that $\lim\limits_{x\to -\infty}f(x)\to-\infty$ Show that there is an $a \in \mathbb{R}$ such that $\lim\limits_{x\to -\infty}\frac{f(x)}{x}=a$ . First of all, i can't really find an example respecting the hypothesis. Is there a convex function which is increasing with $\lim\limits_{x\to -\infty}f(x)=-\infty$ ? If $f$ was supposed concave, it appears true to me. Then, if the statement holds, I don't really see how to prove it. I tried to apply the following property of convex function on the interval $x<a<b$ : $$\frac{f(a)-f(x)}{a-x}\le\frac{f(b)-f(x)}{b-x}\le\frac{f(b)-f(a)}{b-a}.$$ But, even if i suceed to ""sandwich"" ${f(x) \over x}$ , i don't really see how it could help.. Thank you in advance for help.","Let be a convex and increasing function such that Show that there is an such that . First of all, i can't really find an example respecting the hypothesis. Is there a convex function which is increasing with ? If was supposed concave, it appears true to me. Then, if the statement holds, I don't really see how to prove it. I tried to apply the following property of convex function on the interval : But, even if i suceed to ""sandwich"" , i don't really see how it could help.. Thank you in advance for help.",f:\mathbb{R} \to \mathbb{R} \lim\limits_{x\to -\infty}f(x)\to-\infty a \in \mathbb{R} \lim\limits_{x\to -\infty}\frac{f(x)}{x}=a \lim\limits_{x\to -\infty}f(x)=-\infty f x<a<b \frac{f(a)-f(x)}{a-x}\le\frac{f(b)-f(x)}{b-x}\le\frac{f(b)-f(a)}{b-a}. {f(x) \over x},"['real-analysis', 'limits', 'functions', 'convex-analysis']"
71,find $\lim_{x \to \infty} \frac{\ln(x+1)}{x}$,find,\lim_{x \to \infty} \frac{\ln(x+1)}{x},"I need to find: $$\lim_{x \to \infty} \frac{\ln(x+1)}{x}$$ without using L'Hospital, integrals or derivative! I would like your help in order to know i'm not doing something invalid. I was thinking of the following way: $$ \lim_{x \to \infty } \frac{\ln(x+1)}{x}  =  \lim_{x \to \infty } \frac{1}{x} \ln(x+1)  =  \lim_{x \to \infty } \ln(x+1)^\frac{1}{x} $$ and because $\frac{1}{x}$ tend to $0$ as $x$ approaches $\infty$ , than $(x+1)^\frac{1}{x}$ tends to $1$ and $\ln(1) = 0$ and the limit is $0$ . But this way sounds off. I was trying also to express $x$ with $t$ and use $e$ (euler) somehow, but it didn't work. Is my way valid? If not, do you have a better way without using L'Hospital's rule? Thanks! Edit: While reading answers I was thinking of another way, $$\lim_{x \to \infty} \frac{\ln(x+1)}{x}  =  \lim_{x \to \infty} \frac{\ln(x(1+\frac{1}{x}))}{x} = \lim_{x \to \infty} \frac{\ln(x)}{x} + \frac{\ln(1+\frac{1}{x})}{x} = $$ $$\lim_{x \to \infty} \ln(\sqrt[x]{x}) + \frac{\ln(1+\frac{1}{x})}{x}$$ now $\sqrt[x]{x}$ tends to $1$ as x approaches to $\infty$ , hence $\ln(\sqrt[x]{x})$ tends to $0$ (is it valid to say so?) and $\frac{\ln(1+\frac{1}{x})}{x}$ tends to $0$ is pretty obvious. Am on missing something in this one?","I need to find: without using L'Hospital, integrals or derivative! I would like your help in order to know i'm not doing something invalid. I was thinking of the following way: and because tend to as approaches , than tends to and and the limit is . But this way sounds off. I was trying also to express with and use (euler) somehow, but it didn't work. Is my way valid? If not, do you have a better way without using L'Hospital's rule? Thanks! Edit: While reading answers I was thinking of another way, now tends to as x approaches to , hence tends to (is it valid to say so?) and tends to is pretty obvious. Am on missing something in this one?","\lim_{x \to \infty} \frac{\ln(x+1)}{x} 
\lim_{x \to \infty } \frac{\ln(x+1)}{x}  =  \lim_{x \to \infty } \frac{1}{x} \ln(x+1)  =  \lim_{x \to \infty } \ln(x+1)^\frac{1}{x}
 \frac{1}{x} 0 x \infty (x+1)^\frac{1}{x} 1 \ln(1) = 0 0 x t e \lim_{x \to \infty} \frac{\ln(x+1)}{x}  =  \lim_{x \to \infty} \frac{\ln(x(1+\frac{1}{x}))}{x} = \lim_{x \to \infty} \frac{\ln(x)}{x} + \frac{\ln(1+\frac{1}{x})}{x} =  \lim_{x \to \infty} \ln(\sqrt[x]{x}) + \frac{\ln(1+\frac{1}{x})}{x} \sqrt[x]{x} 1 \infty \ln(\sqrt[x]{x}) 0 \frac{\ln(1+\frac{1}{x})}{x} 0","['limits', 'functions', 'logarithms', 'infinity']"
72,Another limit points task.,Another limit points task.,,"Let $x_n, y_n$ -- two numerical sequences with pairwise distinct terms (i.e $x_i \neq x_j$ and $y_i \neq y_j$ for all $i \neq j$ ). We decided that $\lim_{n \to \infty} (x_ny_n) = \lim_{n \to \infty} (x_n + y_n) = 5$ . The question is to find all limit points of the set $A$ , where: $A = \{e^{x_n} + e^{y_n} \ | \ n \in \mathbb{N}\}$ . Definition of limit point which I know: $a \in X, A \subset X$ is limit point if for every $\varepsilon > 0$ punctured ball $B_{\varepsilon}(a)$ contains an element from $A$ . In our case $X = \mathbb{R}$ . I found that: $x = \lim_{n \to \infty} (x_n) = \frac{5 + \sqrt{5}}{2}$ $y =\lim_{n \to \infty} (y_n) = \frac{5 - \sqrt{5}}{2}$ With solving a system $xy=5$ $x+y=5$ . (We've got case with swapped values of x and y, but it's easy to show that we can do the same for them if we find solution). My idea is to find $z = \lim z_n$ , where $z_n = e^{x_n} + e^{y_n}$ . And then we can say that limit point is only $z$ (from the limit defenition). But I've got some troubles with finding this limit.","Let -- two numerical sequences with pairwise distinct terms (i.e and for all ). We decided that . The question is to find all limit points of the set , where: . Definition of limit point which I know: is limit point if for every punctured ball contains an element from . In our case . I found that: With solving a system . (We've got case with swapped values of x and y, but it's easy to show that we can do the same for them if we find solution). My idea is to find , where . And then we can say that limit point is only (from the limit defenition). But I've got some troubles with finding this limit.","x_n, y_n x_i \neq x_j y_i \neq y_j i \neq j \lim_{n \to \infty} (x_ny_n) = \lim_{n \to \infty} (x_n + y_n) = 5 A A = \{e^{x_n} + e^{y_n} \ | \ n \in \mathbb{N}\} a \in X, A \subset X \varepsilon > 0 B_{\varepsilon}(a) A X = \mathbb{R} x = \lim_{n \to \infty} (x_n) = \frac{5 + \sqrt{5}}{2} y =\lim_{n \to \infty} (y_n) = \frac{5 - \sqrt{5}}{2} xy=5 x+y=5 z = \lim z_n z_n = e^{x_n} + e^{y_n} z","['limits', 'metric-spaces']"
73,"If $ \lim_{x \to +\infty}f(x) = A $ and $ \lim_{x \to +\infty}f'(x) = B $, prove that $B = 0$","If  and , prove that", \lim_{x \to +\infty}f(x) = A   \lim_{x \to +\infty}f'(x) = B  B = 0,"Problem: Let $ f: \mathbb{R} \to \mathbb{R} $ be a function of class $ C^1 $ such that $ \lim_{x \to +\infty}f(x) = A $ and $ \lim_{x \to +\infty}f'(x) = B $ for $ A, B \in \mathbb{R} $ . Prove that $B = 0$ . I need help in validating my proof. Here it goes: Suppose that $ B \neq 0 $ . Take $ \epsilon = B+1 $ . From the definition of limit to infinity, the following holds: $ (\exists M > 0)(\forall x > M) | f'(x) - B | < B+1 $ From there, $ 1 < f'(x) < 2B + 1 $ (for all x greater than M). Now, take an interval $ [M+1, M+2] $ . $ f $ is continuous on that segment, so it's bounded and reaches it's maximum and minimum. Function $ f $ is also differentiable on that segment, and if we apply Fermat's theorem (on local maximum/minimum), we'll get a contradiction, since $ f'(x) > 0 $ for all $ x \in [M+1, M+2] $ . Therefore $ B = 0 $ .","Problem: Let be a function of class such that and for . Prove that . I need help in validating my proof. Here it goes: Suppose that . Take . From the definition of limit to infinity, the following holds: From there, (for all x greater than M). Now, take an interval . is continuous on that segment, so it's bounded and reaches it's maximum and minimum. Function is also differentiable on that segment, and if we apply Fermat's theorem (on local maximum/minimum), we'll get a contradiction, since for all . Therefore ."," f: \mathbb{R} \to \mathbb{R}   C^1   \lim_{x \to +\infty}f(x) = A   \lim_{x \to +\infty}f'(x) = B   A, B \in \mathbb{R}  B = 0  B \neq 0   \epsilon = B+1   (\exists M > 0)(\forall x > M) | f'(x) - B | < B+1   1 < f'(x) < 2B + 1   [M+1, M+2]   f   f   f'(x) > 0   x \in [M+1, M+2]   B = 0 ","['real-analysis', 'limits']"
74,"Show that for any $a \ne 0$, $f(x)=1/x$ approaches $1/a$ as $x$ approaches $a$.","Show that for any ,  approaches  as  approaches .",a \ne 0 f(x)=1/x 1/a x a,"To show in general that $f$ approaches $1/a$ near $a$ for any $a$ we proceed in basically the same way, except that,again, we have to be a little more careful in formulating our initial stipulation. It's not good enough simply to require that $|x-a|$ should be less than $1$ , or any other particular number, because if $a$ is close to $0$ this would allow values of $x$ that are negative (not to mention the embarrassing possibility that $x=0$ , so that $f(x)$ isn't even defined!). The trick in this case is to first require that $$|x-a|<\frac{|a|}{2};$$ in other words, we require that $x$ be less than half as far from $a$ as $a$ is from $0$ . You should be able to check first that $x \ne 0$ and that $1/|x|$ < $2/|a|$ , and then work out the rest of the argument. In order to check that $x \ne 0$ I checked both cases $a \ge 0$ and $a<0$ : $a > 0$ : $|x-a|<\dfrac{|a|}{2}$ $\implies$ $-\dfrac{|a|}{2}<x-a<\dfrac{|a|}{2}$ $\implies$ $-\dfrac{a}{2}<x-a<\dfrac{a}{2}$ $\implies$ $-\dfrac{a}{2}+a<x<\dfrac{a}{2}+a$ and so $\dfrac{2a-a}{2}<x<\dfrac{a+2a}{2}$ . Thus $x>0$ since $0<\dfrac{a}{2}<x$ . $a<0$ : $|x-a|<\dfrac{|a|}{2}$ $\implies$ $-\dfrac{(-a)}{2}<x-a<\dfrac{(-a)}{2}$ $\implies$ $-\dfrac{(-a)}{2} + a <x<\dfrac{(-a)}{2}+a$ $\implies$ $\dfrac{a}{2}+a=\dfrac{2a +a}{2}=\dfrac{3a}{2}<x<\dfrac{(-a)}{2}+a=\dfrac{2a-a}{2}=\dfrac{a}{2}$ . Therefore $x<0$ since $x<\dfrac{a}{2}<0$ . In order to get that $1/|x|<2/|a|$ , I did the following: $|x-a|<\dfrac{|a|}{2}$ $\implies$ $-\dfrac{|a|}{2}<x-a<\dfrac{|a|}{2}$ and so $-\dfrac{|a|}{2}+a<x$ $\implies$ $-\dfrac{3|a|}{2}=-\dfrac{|a|}{2}-|a|\le-\dfrac{|a|}{2} + a< x$ which implies that $\dfrac{1}{x}<-\dfrac{2}{3|a|}<\dfrac{2}{3|a|}<\dfrac{2}{|a|}.$ Thus verifying that $\dfrac{1}{|x|}<\dfrac{2}{|a|}$ . Now using this information, I am to show that in general, $f(x)=1/x$ approaches $1/a$ for $x$ near $a$ . My approach so far: $\biggl|\dfrac{1}{x} - \dfrac{1}{a}\biggr|=\biggl|\dfrac{a-x}{ax}\biggr|=\dfrac{1}{a}\cdot\dfrac{1}{|x|}\cdot |x-a|<\dfrac{1}{a}\cdot \dfrac{2}{|a|}\cdot |x-a|$ and so requiring that $|x-a|<\dfrac{|a|\cdot a}{2}\epsilon$ gives us our desired result. What is the problem with allowing negative values of $x$ ?","To show in general that approaches near for any we proceed in basically the same way, except that,again, we have to be a little more careful in formulating our initial stipulation. It's not good enough simply to require that should be less than , or any other particular number, because if is close to this would allow values of that are negative (not to mention the embarrassing possibility that , so that isn't even defined!). The trick in this case is to first require that in other words, we require that be less than half as far from as is from . You should be able to check first that and that < , and then work out the rest of the argument. In order to check that I checked both cases and : : and so . Thus since . : . Therefore since . In order to get that , I did the following: and so which implies that Thus verifying that . Now using this information, I am to show that in general, approaches for near . My approach so far: and so requiring that gives us our desired result. What is the problem with allowing negative values of ?",f 1/a a a |x-a| 1 a 0 x x=0 f(x) |x-a|<\frac{|a|}{2}; x a a 0 x \ne 0 1/|x| 2/|a| x \ne 0 a \ge 0 a<0 a > 0 |x-a|<\dfrac{|a|}{2} \implies -\dfrac{|a|}{2}<x-a<\dfrac{|a|}{2} \implies -\dfrac{a}{2}<x-a<\dfrac{a}{2} \implies -\dfrac{a}{2}+a<x<\dfrac{a}{2}+a \dfrac{2a-a}{2}<x<\dfrac{a+2a}{2} x>0 0<\dfrac{a}{2}<x a<0 |x-a|<\dfrac{|a|}{2} \implies -\dfrac{(-a)}{2}<x-a<\dfrac{(-a)}{2} \implies -\dfrac{(-a)}{2} + a <x<\dfrac{(-a)}{2}+a \implies \dfrac{a}{2}+a=\dfrac{2a +a}{2}=\dfrac{3a}{2}<x<\dfrac{(-a)}{2}+a=\dfrac{2a-a}{2}=\dfrac{a}{2} x<0 x<\dfrac{a}{2}<0 1/|x|<2/|a| |x-a|<\dfrac{|a|}{2} \implies -\dfrac{|a|}{2}<x-a<\dfrac{|a|}{2} -\dfrac{|a|}{2}+a<x \implies -\dfrac{3|a|}{2}=-\dfrac{|a|}{2}-|a|\le-\dfrac{|a|}{2} + a< x \dfrac{1}{x}<-\dfrac{2}{3|a|}<\dfrac{2}{3|a|}<\dfrac{2}{|a|}. \dfrac{1}{|x|}<\dfrac{2}{|a|} f(x)=1/x 1/a x a \biggl|\dfrac{1}{x} - \dfrac{1}{a}\biggr|=\biggl|\dfrac{a-x}{ax}\biggr|=\dfrac{1}{a}\cdot\dfrac{1}{|x|}\cdot |x-a|<\dfrac{1}{a}\cdot \dfrac{2}{|a|}\cdot |x-a| |x-a|<\dfrac{|a|\cdot a}{2}\epsilon x,"['real-analysis', 'limits', 'analysis', 'proof-verification', 'epsilon-delta']"
75,Limit of log of norm of exponential of Hamiltonian Matrix equals maximal eigenvalue,Limit of log of norm of exponential of Hamiltonian Matrix equals maximal eigenvalue,,"Let $A$ be a $2n \times 2n$ Hamiltonian matrix (i.e. $JA$ is symmetric with $J=\begin{pmatrix} 0 & I_n \\ -I_n & 0\\ \end{pmatrix}$ ). Is it true that $$\lim_{t\to \infty}\frac{1}{t} \log \Vert{e^{At}}\Vert=\max_i(\text{Re}(\lambda_i))$$ where $\lambda_i$ are are all the eigenvalues of $A$ and $\Vert \cdot \Vert$ is the spectral norm? I'm interested in computing the Lyapunov exponent of critical points of Hamiltonian systems. This type of limit appears in such cases. From numerical calculations this seems to be true. I've been able to prove this result for the $2\times2$ case, where $A=\begin{pmatrix} a & b\\ c & -a\\ \end{pmatrix}.$ In this case, setting $\lambda=\sqrt{a^2 + b c}$ , $M=\frac{1}{\lambda}A$ is an involutory matrix. Then the exponential $\Vert{e^{\lambda M t}}\Vert$ may be written in terms of hyperbolic trigonometric functions, which are bounded in the $\lambda$ imaginary case, and may be approximated in terms of exponentials in the $\lambda$ real case. The above result follows easily then, and it is actually independent of the norm. I'm having problems in the general case. I wanted to find a way to write $A$ as a block diagonal matrix consisting of smaller Hamiltonian matrices via a unitary transformation, and then prove it inductively over $n$ , but I've had no luck finding such result. Any suggestions?","Let be a Hamiltonian matrix (i.e. is symmetric with ). Is it true that where are are all the eigenvalues of and is the spectral norm? I'm interested in computing the Lyapunov exponent of critical points of Hamiltonian systems. This type of limit appears in such cases. From numerical calculations this seems to be true. I've been able to prove this result for the case, where In this case, setting , is an involutory matrix. Then the exponential may be written in terms of hyperbolic trigonometric functions, which are bounded in the imaginary case, and may be approximated in terms of exponentials in the real case. The above result follows easily then, and it is actually independent of the norm. I'm having problems in the general case. I wanted to find a way to write as a block diagonal matrix consisting of smaller Hamiltonian matrices via a unitary transformation, and then prove it inductively over , but I've had no luck finding such result. Any suggestions?","A 2n \times 2n JA J=\begin{pmatrix}
0 & I_n \\
-I_n & 0\\
\end{pmatrix} \lim_{t\to \infty}\frac{1}{t} \log \Vert{e^{At}}\Vert=\max_i(\text{Re}(\lambda_i)) \lambda_i A \Vert \cdot \Vert 2\times2 A=\begin{pmatrix}
a & b\\
c & -a\\
\end{pmatrix}. \lambda=\sqrt{a^2 + b c} M=\frac{1}{\lambda}A \Vert{e^{\lambda M t}}\Vert \lambda \lambda A n","['limits', 'dynamical-systems', 'matrix-exponential', 'spectral-norm']"
76,Proof of exponential law using limit definition of exponential function?,Proof of exponential law using limit definition of exponential function?,,"For fun, I tried to prove the well-known exponential property $e^{a+b} = e^a e^b$ using the limit definition of the exponential function, below. Definition. The exponential function is defined as follows. $$e^x := \lim_{\epsilon \rightarrow 0} \left( 1 + \epsilon x \right)^{1/\epsilon}$$ I was able to outline the majority of the proof, however I do not have sufficient justification to go from line $\eqref 1$ to step $\eqref 2$ . What limit properties might be used to fill in the blanks? I'd prefer not to use the binomial theorem or calculus-based argument, if possible (though if an expansion like that seems necessary, that is OK!). Proof. $$e^{a+b} = \lim_{\epsilon \rightarrow 0} \left( 1 + \epsilon (a+b) \right)^{1/\epsilon}$$ $$e^{a+b} = \lim_{\epsilon \rightarrow 0} \left( 1 + \epsilon a + \epsilon b \right)^{1/\epsilon} \tag 1 \label 1$$ $$\vdots$$ $$e^{a+b} = \lim_{\epsilon \rightarrow 0} \left( 1 + \epsilon a + \epsilon b  + \epsilon^2 ab\right)^{1/\epsilon} \tag 2 \label 2$$ $$e^{a+b} = \lim_{\epsilon \rightarrow 0} \left( (1 + \epsilon a)(1 + \epsilon b)\right)^{1/\epsilon}$$ $$e^{a+b} = \lim_{\epsilon \rightarrow 0} \left(1 + \epsilon a \right)^{1/\epsilon} \left(1 + \epsilon b\right)^{1/\epsilon}$$ $$e^{a+b} = e^a e^b$$","For fun, I tried to prove the well-known exponential property using the limit definition of the exponential function, below. Definition. The exponential function is defined as follows. I was able to outline the majority of the proof, however I do not have sufficient justification to go from line to step . What limit properties might be used to fill in the blanks? I'd prefer not to use the binomial theorem or calculus-based argument, if possible (though if an expansion like that seems necessary, that is OK!). Proof.","e^{a+b} = e^a e^b e^x := \lim_{\epsilon \rightarrow 0} \left( 1 + \epsilon x \right)^{1/\epsilon} \eqref 1 \eqref 2 e^{a+b} = \lim_{\epsilon \rightarrow 0} \left( 1 + \epsilon (a+b) \right)^{1/\epsilon} e^{a+b} = \lim_{\epsilon \rightarrow 0} \left( 1 + \epsilon a + \epsilon b \right)^{1/\epsilon} \tag 1 \label 1 \vdots e^{a+b} = \lim_{\epsilon \rightarrow 0} \left( 1 + \epsilon a + \epsilon b 
+ \epsilon^2 ab\right)^{1/\epsilon} \tag 2 \label 2 e^{a+b} = \lim_{\epsilon \rightarrow 0} \left( (1 + \epsilon a)(1 + \epsilon b)\right)^{1/\epsilon} e^{a+b} = \lim_{\epsilon \rightarrow 0} \left(1 + \epsilon a \right)^{1/\epsilon} \left(1 + \epsilon b\right)^{1/\epsilon} e^{a+b} = e^a e^b","['limits', 'exponential-function', 'alternative-proof']"
77,Does $\lim\limits_{x \to 0}\frac{\sin(x\sin\frac{1}{x})}{x\sin\frac{1}{x}}$ exist or not?,Does  exist or not?,\lim\limits_{x \to 0}\frac{\sin(x\sin\frac{1}{x})}{x\sin\frac{1}{x}},"Denote $$f(x)=\frac{\sin \left(x\sin\dfrac{1}{x}\right)}{x\sin\dfrac{1}{x}}.$$ We want to research the limit $$\lim_{x \to 0}f(x).$$ According to most of the textbooks for common calculus, especially in China which is named Advanced Mathematics not Mathematical Analysis, the existence of the functional limit at some point requires that the funciton has the definition over some deleted neighborhood of that point. Since the domain of $f(x)$ is $$D=\mathbb{R} \backslash M,M=\left\{x\bigg|x=0,~\text{or}~~  x=\pm\frac{1}{k\pi}(k\in \mathbb{N_+})\right\},$$ then there exists no deleted neighborhood of $x=0$ such that $f(x)$ has definition over it. That is because: $$\forall \delta>0, \exists x_0 \in M: 0<|x_0-0|<\delta.$$ Thus, the limit seems not to exist, for the reason that $f(x)$ can not satisfy the fundamental condition which is the premise for the definition of the functional limit. But, according to Mathematical Analysis, the functional limit can be defined at the condensation point. Thus, we have to acknowledge that the limit really exists and may readily evaluate the limit $$\lim_{x \to 0}f(x)=1.$$ Now, the question comes out. Does the limit exist or not on earth? How to reconcile this contradiction especially in teaching and examination?","Denote We want to research the limit According to most of the textbooks for common calculus, especially in China which is named Advanced Mathematics not Mathematical Analysis, the existence of the functional limit at some point requires that the funciton has the definition over some deleted neighborhood of that point. Since the domain of is then there exists no deleted neighborhood of such that has definition over it. That is because: Thus, the limit seems not to exist, for the reason that can not satisfy the fundamental condition which is the premise for the definition of the functional limit. But, according to Mathematical Analysis, the functional limit can be defined at the condensation point. Thus, we have to acknowledge that the limit really exists and may readily evaluate the limit Now, the question comes out. Does the limit exist or not on earth? How to reconcile this contradiction especially in teaching and examination?","f(x)=\frac{\sin \left(x\sin\dfrac{1}{x}\right)}{x\sin\dfrac{1}{x}}. \lim_{x \to 0}f(x). f(x) D=\mathbb{R} \backslash M,M=\left\{x\bigg|x=0,~\text{or}~~  x=\pm\frac{1}{k\pi}(k\in \mathbb{N_+})\right\}, x=0 f(x) \forall \delta>0, \exists x_0 \in M: 0<|x_0-0|<\delta. f(x) \lim_{x \to 0}f(x)=1.",['limits']
78,An unorthodox way of converting a recurrent sequence into a non-recurrent one,An unorthodox way of converting a recurrent sequence into a non-recurrent one,,"The problem is: A sequence is given with: $$ a_1 = 2016, a_{n+1} = \sqrt{a_n^2 + 4 + \frac{1}{a_n^{222}}}$$ Find: $$ (a)  \quad \lim_{n \to \infty}{a_n} \quad (b) \lim_{n \to \infty}{\frac{a_n^2}{n}} \quad (c) \lim_{n \to \infty}{\frac{\sum_{k=1}^{n}{a_k}}{n \sqrt{n}}}$$ The (a) part I was able to solve by proving that the sequence is increasing and not bounded by above. Therefore $$ \lim_{n \to \infty}a_n = +\infty$$ Then for (b) I applied the Stolz theorem: $$ \lim_{n \to \infty}{\frac{a_n^2}{n}} = \lim_{n \to \infty}{\frac{a_{n+1}^2 - a_n^2}{n+1-n}} = \lim_{n \to \infty}{(a_n^2 + 4 + \frac{1}{a_n^{222}} - a_n^2)} = \lim_{n \to \infty}{(4 + \frac{1}{a_n^{222}})} = 4$$ But the (c) part is where I get confused. I've tried applying the Stolz theorem there as well but no luck since the n's don't cancel out like they did in (b). I did have one thing on my mind though but I'm not sure how formally correct it is. Let's look at the first few sequence members: $$a_1 = 2016, a_2 = \sqrt{2016^2 + 4 + \frac{1}{2016^{222}}}, a_3 = \sqrt{2016^2 + 4 + \frac{1}{2016^{222}}+ 4 + \frac{1}{a_2^{222}}} = \sqrt{2016^2 + 8 + \frac{1}{2016^{222}} + \frac{1}{a_2^{222}}}$$ And now as we keep going we are going to get a new $$ \frac{1}{a_n^{222}}$$ member inside for each n. So that still makes this sequence recursive. But can I simply disregard that part since it obviously always converges to zero, not matter what the value of n is? Or maybe I could say something like $$ \text{Let } \frac{1}{a_n^{222}} = Z(n) \quad \text{ Then } \lim_{n \to \infty}Z(n) = 0 \quad (\forall n)$$ And now I rewrite my sequence as: $$ a_n = \sqrt{(2016)^2 + 4(n-1) + \sum_{i=1}^{n-1}Z(i)} \quad \forall n \geq 2$$ Now let's finally look at the (c) problem: $$\lim_{n \to \infty}{\frac{\sum_{k=1}^{n}{a_k}}{n \sqrt{n}}} = \lim_{n \to \infty}{\frac{a_1 + a_2 + ... +a_n}{n \sqrt{n}}} = \lim_{n \to \infty}{\frac{2016  + \sqrt{2016^2 + 4 + Z(1)} + ... + \sqrt{2016^2 + 4(n-1) + \sum{Z(n})}   }{n \sqrt{n}}}$$ And now I apply Stolz theorem: $$ = \lim_{n \to \infty}{\frac{\sqrt{2016^2 + 4n + \sum{Z(n+1)}}}{(n+1)\sqrt{n+1} - n\sqrt{n}}} = 0  $$ Since the top degree of $n$ is $\frac{1}{2}$ and the bottom one is $ \frac{3}{2}$ . So can anyone tell me whether this solutions is correct? If it isn't, how would you solve it? If it is correct but my process is wrong I'd also like to know where I went wrong. Thanks in advance.","The problem is: A sequence is given with: Find: The (a) part I was able to solve by proving that the sequence is increasing and not bounded by above. Therefore Then for (b) I applied the Stolz theorem: But the (c) part is where I get confused. I've tried applying the Stolz theorem there as well but no luck since the n's don't cancel out like they did in (b). I did have one thing on my mind though but I'm not sure how formally correct it is. Let's look at the first few sequence members: And now as we keep going we are going to get a new member inside for each n. So that still makes this sequence recursive. But can I simply disregard that part since it obviously always converges to zero, not matter what the value of n is? Or maybe I could say something like And now I rewrite my sequence as: Now let's finally look at the (c) problem: And now I apply Stolz theorem: Since the top degree of is and the bottom one is . So can anyone tell me whether this solutions is correct? If it isn't, how would you solve it? If it is correct but my process is wrong I'd also like to know where I went wrong. Thanks in advance."," a_1 = 2016, a_{n+1} = \sqrt{a_n^2 + 4 + \frac{1}{a_n^{222}}}  (a)  \quad \lim_{n \to \infty}{a_n} \quad (b) \lim_{n \to \infty}{\frac{a_n^2}{n}} \quad (c) \lim_{n \to \infty}{\frac{\sum_{k=1}^{n}{a_k}}{n \sqrt{n}}}  \lim_{n \to \infty}a_n = +\infty  \lim_{n \to \infty}{\frac{a_n^2}{n}} = \lim_{n \to \infty}{\frac{a_{n+1}^2 - a_n^2}{n+1-n}} = \lim_{n \to \infty}{(a_n^2 + 4 + \frac{1}{a_n^{222}} - a_n^2)} = \lim_{n \to \infty}{(4 + \frac{1}{a_n^{222}})} = 4 a_1 = 2016, a_2 = \sqrt{2016^2 + 4 + \frac{1}{2016^{222}}}, a_3 = \sqrt{2016^2 + 4 + \frac{1}{2016^{222}}+ 4 + \frac{1}{a_2^{222}}} = \sqrt{2016^2 + 8 + \frac{1}{2016^{222}} + \frac{1}{a_2^{222}}}  \frac{1}{a_n^{222}}  \text{Let } \frac{1}{a_n^{222}} = Z(n) \quad \text{ Then } \lim_{n \to \infty}Z(n) = 0 \quad (\forall n)  a_n = \sqrt{(2016)^2 + 4(n-1) + \sum_{i=1}^{n-1}Z(i)} \quad \forall n \geq 2 \lim_{n \to \infty}{\frac{\sum_{k=1}^{n}{a_k}}{n \sqrt{n}}} = \lim_{n \to \infty}{\frac{a_1 + a_2 + ... +a_n}{n \sqrt{n}}} = \lim_{n \to \infty}{\frac{2016  + \sqrt{2016^2 + 4 + Z(1)} + ... + \sqrt{2016^2 + 4(n-1) + \sum{Z(n})}   }{n \sqrt{n}}}  = \lim_{n \to \infty}{\frac{\sqrt{2016^2 + 4n + \sum{Z(n+1)}}}{(n+1)\sqrt{n+1} - n\sqrt{n}}} = 0   n \frac{1}{2}  \frac{3}{2}","['real-analysis', 'limits', 'convergence-divergence']"
79,Proof Verification: Squeeze theorem,Proof Verification: Squeeze theorem,,"We are given that $g(x) < f(x) < h(x)$ over some interval, $$\lim _{x\rightarrow a} g(x)=L$$ and $$\lim _{x\rightarrow a} h(x)=L$$ Through this we can infer that for every $ε_h$ there exists a $δ_h$ such that  $|h(x)-L|<ε_h$ when $0<|x−a|<δ_h$ and that for every  $ε_g$ there exists a $δ_g$ such that  $|g(x)-L|<ε_g$ when $0<|x−a|<δ_g$ Let $δ$ be defined as $\min(δ_h,δ_g)$ We can subtract $L$ from each term of the given inequality to get $g(x)-L < f(x)-L < h(x)-L$ We get $-ε_g<g(x)-L < f(x)-L < h(x)-L<ε_h$. Now we define $ε$ as $\max(ε_h,ε_g)$. Using this we get $|f(x)−L|<ε$ if $0<|x−a|<δ$ This Completes the proof. I wanted to ask 1) whether the proof is correct? 2) whether it's easy to follow ?","We are given that $g(x) < f(x) < h(x)$ over some interval, $$\lim _{x\rightarrow a} g(x)=L$$ and $$\lim _{x\rightarrow a} h(x)=L$$ Through this we can infer that for every $ε_h$ there exists a $δ_h$ such that  $|h(x)-L|<ε_h$ when $0<|x−a|<δ_h$ and that for every  $ε_g$ there exists a $δ_g$ such that  $|g(x)-L|<ε_g$ when $0<|x−a|<δ_g$ Let $δ$ be defined as $\min(δ_h,δ_g)$ We can subtract $L$ from each term of the given inequality to get $g(x)-L < f(x)-L < h(x)-L$ We get $-ε_g<g(x)-L < f(x)-L < h(x)-L<ε_h$. Now we define $ε$ as $\max(ε_h,ε_g)$. Using this we get $|f(x)−L|<ε$ if $0<|x−a|<δ$ This Completes the proof. I wanted to ask 1) whether the proof is correct? 2) whether it's easy to follow ?",,"['real-analysis', 'limits', 'proof-verification', 'epsilon-delta']"
80,Compute $\lim_{n\to\infty} \frac {1}{n^k}\binom{n}{k}$,Compute,\lim_{n\to\infty} \frac {1}{n^k}\binom{n}{k},"Evaluate $$\lim_{n\to\infty} \frac {1}{n^k}\binom{n}{k} \,, \quad k\in\mathbb{Z}_+^*$$ What I tried so far: I rewrote: $$\lim_{n\to\infty} \frac {1}{n^k}\binom{n}{k}=\frac 1{k!}\lim_{n\to\infty}\frac {n(n-1)\cdots(n-k+1)}{n^k}=L$$ Applying $\ln$ to both sides we get: $$\lim_{n\to\infty}\ln\left(\frac 1{k!}\right)+\sum_{i=1}^{k-1}\ln\left(\frac{ n-i}{ n^k}\right)$$ And thought maybe I could get a Riemann somewhere but it doesn't get me anywhere... EDIT: Solved it with Surb's hint the product of the limit is the limits of the product: we get: $$\frac 1{k!}\lim_{n\to\infty}\frac {n(n-1)\cdots(n-k+1)}{n^k}=\frac {1}{k!}\lim_{n\to\infty}\frac {n}{n}\lim_{n\to\infty}\frac{n-1}{n}\cdots\lim_{n\to\infty}\frac{n-k+1}{n}=\frac 1{k!}$$",Evaluate What I tried so far: I rewrote: Applying to both sides we get: And thought maybe I could get a Riemann somewhere but it doesn't get me anywhere... EDIT: Solved it with Surb's hint the product of the limit is the limits of the product: we get:,"\lim_{n\to\infty} \frac {1}{n^k}\binom{n}{k} \,, \quad k\in\mathbb{Z}_+^* \lim_{n\to\infty} \frac {1}{n^k}\binom{n}{k}=\frac 1{k!}\lim_{n\to\infty}\frac {n(n-1)\cdots(n-k+1)}{n^k}=L \ln \lim_{n\to\infty}\ln\left(\frac 1{k!}\right)+\sum_{i=1}^{k-1}\ln\left(\frac{ n-i}{ n^k}\right) \frac 1{k!}\lim_{n\to\infty}\frac {n(n-1)\cdots(n-k+1)}{n^k}=\frac {1}{k!}\lim_{n\to\infty}\frac {n}{n}\lim_{n\to\infty}\frac{n-1}{n}\cdots\lim_{n\to\infty}\frac{n-k+1}{n}=\frac 1{k!}","['calculus', 'real-analysis', 'limits', 'limits-without-lhopital']"
81,"If $\lim\limits_{x \to \infty} f(f(x))= \infty$, $\lim\limits_{x \to \infty} f(x)=\pm \infty$","If ,",\lim\limits_{x \to \infty} f(f(x))= \infty \lim\limits_{x \to \infty} f(x)=\pm \infty,"Let $f: \mathbb{R} \to \mathbb{R}$ such that  $\displaystyle \lim_{x \to \infty} f(f(x))= \infty$ and $\displaystyle \lim_{x \to -\infty} f(f(x))= -\infty$ and $f$ has the intermediate value property (it is not necessarily continuous). Prove that $\displaystyle\lim_{x \to \infty}f(x)$ and $\displaystyle\lim_{x \to -\infty}f(x)$ exist and are infinite. I managed to prove that those two limits exist. But I have a really hard time proving that they are infinite. I supposed that $\displaystyle \lim_{x \to \infty} f(x)=a \in \mathbb{R}$ and tried to get a contradiction from here, but since $f$ is not continuous, I must somehow use the intermediate value property and. I see that, for all $\epsilon>0$, there is $\delta>0$ such that when $x>\delta$, $f(x)$ is getting bounded in $(a-\epsilon, a+\epsilon)$ and so $(f\circ f)((\delta,\infty))\subset f((a-\epsilon,a+\epsilon))$, but I couldn't proceed further.","Let $f: \mathbb{R} \to \mathbb{R}$ such that  $\displaystyle \lim_{x \to \infty} f(f(x))= \infty$ and $\displaystyle \lim_{x \to -\infty} f(f(x))= -\infty$ and $f$ has the intermediate value property (it is not necessarily continuous). Prove that $\displaystyle\lim_{x \to \infty}f(x)$ and $\displaystyle\lim_{x \to -\infty}f(x)$ exist and are infinite. I managed to prove that those two limits exist. But I have a really hard time proving that they are infinite. I supposed that $\displaystyle \lim_{x \to \infty} f(x)=a \in \mathbb{R}$ and tried to get a contradiction from here, but since $f$ is not continuous, I must somehow use the intermediate value property and. I see that, for all $\epsilon>0$, there is $\delta>0$ such that when $x>\delta$, $f(x)$ is getting bounded in $(a-\epsilon, a+\epsilon)$ and so $(f\circ f)((\delta,\infty))\subset f((a-\epsilon,a+\epsilon))$, but I couldn't proceed further.",,"['calculus', 'real-analysis', 'limits', 'functions']"
82,Do right-to-left readers also reverse mathematical concepts like number line?,Do right-to-left readers also reverse mathematical concepts like number line?,,"I come from a culture which uses left-to-right language. In my mind, the following is true: On number line, negative numbers are to the left of 0, positive are to the right. In Cartesian coordinate system, X axis points to the right (same as number line). Limit from the left is '-' and limit from the right is '+'. Does the same hold true for mathematicians who come from cultures using right-to-left languages?","I come from a culture which uses left-to-right language. In my mind, the following is true: On number line, negative numbers are to the left of 0, positive are to the right. In Cartesian coordinate system, X axis points to the right (same as number line). Limit from the left is '-' and limit from the right is '+'. Does the same hold true for mathematicians who come from cultures using right-to-left languages?",,"['limits', 'number-line']"
83,"Differentiability of a function $\mathbb{R}^2\to\mathbb{R}$ at $(0,0)$",Differentiability of a function  at,"\mathbb{R}^2\to\mathbb{R} (0,0)","I am trying to solve this question from a past paper. Determine how the differentiability of $f(x,y)$ below depends on the real parameter $\alpha > 0$ $$f(x,y)=\left\{ \begin{array}{ll} \displaystyle \frac{\log(1+|x|^\alpha y^2)}{x^2+y^4} & \text{if } (x,y) \neq (0,0) \\ 0 & \text{if } (x,y)  = (0,0)\end{array} \right.$$ It's easy to see that $f$ is differentiable at all $(x,y) \neq (0,0) $ for all $\alpha >0$. So we need to study what happens at $(0,0)$. As we have $\frac{\partial f}{\partial x}(0,0)=\frac{\partial f}{\partial y}(0,0)=0$, the definition of differentiability reduces to showing that the limit: $$\lim_{(x,y)\to(0,0)} \frac{\log(1+|x|^\alpha y^2)}{\sqrt{x^2+y^2}(x^2+y^4)}  $$ equals $0$. Using Young's inequality for products, we know this is the case for all $\alpha > 2$. On the curve $x=y^2$, the above limit is not $0$ for $\alpha < \tfrac{3}{2}$. I am not sure how to proceed from here. I have tried other paths, but to no avail. I would appreciate any tips or pointers.","I am trying to solve this question from a past paper. Determine how the differentiability of $f(x,y)$ below depends on the real parameter $\alpha > 0$ $$f(x,y)=\left\{ \begin{array}{ll} \displaystyle \frac{\log(1+|x|^\alpha y^2)}{x^2+y^4} & \text{if } (x,y) \neq (0,0) \\ 0 & \text{if } (x,y)  = (0,0)\end{array} \right.$$ It's easy to see that $f$ is differentiable at all $(x,y) \neq (0,0) $ for all $\alpha >0$. So we need to study what happens at $(0,0)$. As we have $\frac{\partial f}{\partial x}(0,0)=\frac{\partial f}{\partial y}(0,0)=0$, the definition of differentiability reduces to showing that the limit: $$\lim_{(x,y)\to(0,0)} \frac{\log(1+|x|^\alpha y^2)}{\sqrt{x^2+y^2}(x^2+y^4)}  $$ equals $0$. Using Young's inequality for products, we know this is the case for all $\alpha > 2$. On the curve $x=y^2$, the above limit is not $0$ for $\alpha < \tfrac{3}{2}$. I am not sure how to proceed from here. I have tried other paths, but to no avail. I would appreciate any tips or pointers.",,"['real-analysis', 'limits', 'multivariable-calculus', 'derivatives']"
84,Calculus determine whether a function is differentiable (hard),Calculus determine whether a function is differentiable (hard),,I know that i need to prove  $$\lim_{ h\to0^+}\dfrac{ g(x+h) - g(x) }{h}$$ exists but i cannot find a way to simplify the expression ...,I know that i need to prove  $$\lim_{ h\to0^+}\dfrac{ g(x+h) - g(x) }{h}$$ exists but i cannot find a way to simplify the expression ...,,"['calculus', 'limits', 'derivatives']"
85,"Prove that every converging limit $\lim_{n \to \infty} \sum_{k=1}^{a(n)} f(k,n)$ is essentially a riemann sum.",Prove that every converging limit  is essentially a riemann sum.,"\lim_{n \to \infty} \sum_{k=1}^{a(n)} f(k,n)","Let $a(n)$ be a strictly increasing function of $n$. Proof that every converging limit $$\lim_{n \to \infty} \sum_{k=1}^{a(n)} f(k,n)$$ is essentially a Riemann sum.","Let $a(n)$ be a strictly increasing function of $n$. Proof that every converging limit $$\lim_{n \to \infty} \sum_{k=1}^{a(n)} f(k,n)$$ is essentially a Riemann sum.",,"['calculus', 'limits', 'summation', 'proof-writing', 'riemann-sum']"
86,Evaluating limits at $-\infty$,Evaluating limits at,-\infty,"$$\lim_{x\to-\infty}\frac{\sqrt{x^6+729}}{4x^3+\sqrt{2x^6+1}}=\lim_{x\to-\infty}\frac{-\sqrt{1+\frac{729}{x^6}}}{4-\sqrt{2+\frac{1}{x^6}}}=\frac{-1}{4-\sqrt{2}}$$ $$\lim_{x\to\infty}\frac{\sqrt{x^6+729}}{4x^3+\sqrt{2x^6+1}}=\lim_{x\to-\infty}\frac{\sqrt{1+\frac{729}{x^6}}}{4+\sqrt{2+\frac{1}{x^6}}}=\frac{1}{4+\sqrt{2}}$$ Is this correct? Can someone please explain to me the distribution of the negatives in regards to the radicals when dealing with limits to $-\infty?$ Does it depend on the powers whether or not a negative stays outside of the radicals? Also, why does a negative not get applied to the $4$? Any help would be greatly appreciated!","$$\lim_{x\to-\infty}\frac{\sqrt{x^6+729}}{4x^3+\sqrt{2x^6+1}}=\lim_{x\to-\infty}\frac{-\sqrt{1+\frac{729}{x^6}}}{4-\sqrt{2+\frac{1}{x^6}}}=\frac{-1}{4-\sqrt{2}}$$ $$\lim_{x\to\infty}\frac{\sqrt{x^6+729}}{4x^3+\sqrt{2x^6+1}}=\lim_{x\to-\infty}\frac{\sqrt{1+\frac{729}{x^6}}}{4+\sqrt{2+\frac{1}{x^6}}}=\frac{1}{4+\sqrt{2}}$$ Is this correct? Can someone please explain to me the distribution of the negatives in regards to the radicals when dealing with limits to $-\infty?$ Does it depend on the powers whether or not a negative stays outside of the radicals? Also, why does a negative not get applied to the $4$? Any help would be greatly appreciated!",,"['calculus', 'limits']"
87,Show that:$\lim\limits_{x\to \infty}{\gamma\over x}+e^{-\gamma/x}\prod\limits_{y=1}^{\infty}\left({xy\over 1+xy}\right){e^{1/(xy)}}=1$,Show that:,\lim\limits_{x\to \infty}{\gamma\over x}+e^{-\gamma/x}\prod\limits_{y=1}^{\infty}\left({xy\over 1+xy}\right){e^{1/(xy)}}=1,I always want to know if this limit hold or not. Please show that $$\lim_{x\to \infty}\left[{\gamma\over x}+e^{-{\gamma\over x}}\prod_{y=1}^{\infty}\left({xy\over 1+xy}\right){e^{{1\over xy}}}\right]=1\tag1$$ Where $\gamma$ is Euler's constant; $\gamma=0.577216...$ My try: Re write $$\left(1-{\gamma\over x}\right)e^{\gamma \over x}=\prod_{y=1}^{\infty}\left({xy\over 1+xy}\right){e^{1\over xy}}$$ Take the ln $${\gamma\over x}+\ln{\left(1-{\gamma\over x}\right)}=\sum_{y=1}^{\infty}\ln{\left(xy\over 1+xy\right){e^{1\over xy}}}$$ $${\gamma\over x}+\ln{\left(1-{\gamma\over x}\right)}=\ln{\left({x\over 1+x}\right){e^{1\over x}}}+\ln{\left({2x\over 1+2x}\right){e^{1\over 2x}}}+ \ln{\left({3x\over 1+3x}\right){e^{1\over 3x}}}+\cdots$$ any hints what to do next? Or else prove $(1)$,I always want to know if this limit hold or not. Please show that $$\lim_{x\to \infty}\left[{\gamma\over x}+e^{-{\gamma\over x}}\prod_{y=1}^{\infty}\left({xy\over 1+xy}\right){e^{{1\over xy}}}\right]=1\tag1$$ Where $\gamma$ is Euler's constant; $\gamma=0.577216...$ My try: Re write $$\left(1-{\gamma\over x}\right)e^{\gamma \over x}=\prod_{y=1}^{\infty}\left({xy\over 1+xy}\right){e^{1\over xy}}$$ Take the ln $${\gamma\over x}+\ln{\left(1-{\gamma\over x}\right)}=\sum_{y=1}^{\infty}\ln{\left(xy\over 1+xy\right){e^{1\over xy}}}$$ $${\gamma\over x}+\ln{\left(1-{\gamma\over x}\right)}=\ln{\left({x\over 1+x}\right){e^{1\over x}}}+\ln{\left({2x\over 1+2x}\right){e^{1\over 2x}}}+ \ln{\left({3x\over 1+3x}\right){e^{1\over 3x}}}+\cdots$$ any hints what to do next? Or else prove $(1)$,,"['limits', 'infinite-product', 'euler-mascheroni-constant']"
88,"For $x$ being irrational, why is $\lim_{m\to\infty}\lim_{n\to\infty}[\cos(n!\pi x)]^{2m}=0$?","For  being irrational, why is ?",x \lim_{m\to\infty}\lim_{n\to\infty}[\cos(n!\pi x)]^{2m}=0,"Let $x \in \mathbb{R} \setminus \mathbb{Q}$. What is the value of $$\lim_{m \to \infty} \lim_{n \to \infty} \left[ \cos(n!\pi x) \right]^{2m}, \qquad (m,n \in \mathbb{N})$$ The answer given is $0$. I don't understand why it is so.","Let $x \in \mathbb{R} \setminus \mathbb{Q}$. What is the value of $$\lim_{m \to \infty} \lim_{n \to \infty} \left[ \cos(n!\pi x) \right]^{2m}, \qquad (m,n \in \mathbb{N})$$ The answer given is $0$. I don't understand why it is so.",,"['real-analysis', 'limits', 'trigonometry']"
89,Measure theory: Counterexample for a limit of Lebesgue measurable sets,Measure theory: Counterexample for a limit of Lebesgue measurable sets,,"Let $(A_k)_{k \in \Bbb {N}}$ be a sequence of Lebesgue measurable subsets of $\Bbb {R}^n$.  $A_k \in \mathscr {A}_n {} ,(A_k) \subset \Bbb{R}^n \ \forall k \in \Bbb {N}$ Let $ A^* = \{ x \in \Bbb {R}^n \mid x \in A_k \text{ for infinitely many } k \in \Bbb {N} \} $ I'm trying to find a sequence $A_k$ to disprove the following statement if $ \lim_{k \to \infty } (\mathscr{L}^n(A_k)) = 0 $ then $ \mathscr{L}^n(A^*) = 0 $ with $ \mathscr{A}_n = \{ A \in \Bbb {R}^n \mid \text{A is Lebesgue-measurable}\}$ and $ \mathscr{L}^n $ is the n-dimensional Lebesgue-measure. Thanks for any help","Let $(A_k)_{k \in \Bbb {N}}$ be a sequence of Lebesgue measurable subsets of $\Bbb {R}^n$.  $A_k \in \mathscr {A}_n {} ,(A_k) \subset \Bbb{R}^n \ \forall k \in \Bbb {N}$ Let $ A^* = \{ x \in \Bbb {R}^n \mid x \in A_k \text{ for infinitely many } k \in \Bbb {N} \} $ I'm trying to find a sequence $A_k$ to disprove the following statement if $ \lim_{k \to \infty } (\mathscr{L}^n(A_k)) = 0 $ then $ \mathscr{L}^n(A^*) = 0 $ with $ \mathscr{A}_n = \{ A \in \Bbb {R}^n \mid \text{A is Lebesgue-measurable}\}$ and $ \mathscr{L}^n $ is the n-dimensional Lebesgue-measure. Thanks for any help",,"['limits', 'measure-theory', 'lebesgue-measure']"
90,Fractional anti-derivatives and derivatives of the logarithm,Fractional anti-derivatives and derivatives of the logarithm,,"Anti-derivatives and derivatives of the natural logarithm are well defined until we attempt to evaluate the fractional derivative and anti-derivatives. The background to this problem was that I was trying to evaluate fractional derivatives of $\frac1x$ , which is usually given as $D^n\frac1x=\frac{\Gamma(0)}{\Gamma(-n)}x^{-1-n}$ , but that is defined only for $n=0$ and nowhere else.  You can define it for $n\in\mathbb N$ , but this is not the best definition one could have.  ( $D^n$ is the $n$ th derivative with respect to $x$ ) I have the general $n$ th anti-derivative ( $I^n$ ) of the natural logarithm as $$I^n\ln(x)=\frac{x^n\left(\ln(x)-\int_0^1\frac{t^n-1}{t-1}dt\right)}{\Gamma(n+1)}$$ Proved by induction: $\frac d{dx}I^n\ln(x)=I^{n-1}\ln(x)$ , and holds true for $n=1$ . I have this graphed on Desmos. (I really like that it has an integral feature.  And if it is too slow, click the little circles on the right to turn off those functions.) While this is a great formula, it doesn't really work for derivatives ( $D^n=I^{-n}$ ), or at least, Desmos stops graphing at $n=-0.99$ , but before that, it appears as though $\lim_{n\to-1}I^n\ln(x)=\frac1x$ I attempted to evaluate it for $n=-1$ $$\frac1x=D^1\ln(x)=I^{-1}\ln(x)=\lim_{n\to-1}\frac{x^n\left(\ln(x)-\int_0^1\frac{t^n-1}{t-1}dt\right)}{\Gamma(n+1)}$$ If you try to directly substitute, you get an indefinite form, so I attempted to do a limit method instead.  I would like to apply L'Hostpital's rule, but I don't quite know how to deal with either the numerator nor the denominator. I have found that $$I^{1/2}\ln(x)=\frac{2\sqrt x\left(\ln(x)-2+\ln(4)\right)}{\sqrt\pi}$$ The $2-\ln(4)$ is wolframalpha's evaluation of $\int_0^1\frac{t^{1/2}-1}{t-1}dt$ From here, you can differentiate like normal to get $D^{(2n-1)/2}\ln(x)$ , $n\in\mathbb N$ . More importantly, if the limit from above is correct, then my formula works for $I^{-n}\ln(x)$ with $n\in\mathbb N$ and I can finally define what $D^n\frac1x$ is equal to! So the question: How can we evaluate the limit: $\lim_{n\to-1}I^n\ln(x)$ ?  What does it equal? Is the fractional derivative of the natural logarithm and $\frac1x$ already known?  I've made posts on this before, and it doesn't appear many people have tackled this problem. Here and Here .  Both have received little attention.  Attempting to use regular fractional derivative formulas are extremely messy, so I could not actually use them.  If someone more experienced can, that'd be great.","Anti-derivatives and derivatives of the natural logarithm are well defined until we attempt to evaluate the fractional derivative and anti-derivatives. The background to this problem was that I was trying to evaluate fractional derivatives of , which is usually given as , but that is defined only for and nowhere else.  You can define it for , but this is not the best definition one could have.  ( is the th derivative with respect to ) I have the general th anti-derivative ( ) of the natural logarithm as Proved by induction: , and holds true for . I have this graphed on Desmos. (I really like that it has an integral feature.  And if it is too slow, click the little circles on the right to turn off those functions.) While this is a great formula, it doesn't really work for derivatives ( ), or at least, Desmos stops graphing at , but before that, it appears as though I attempted to evaluate it for If you try to directly substitute, you get an indefinite form, so I attempted to do a limit method instead.  I would like to apply L'Hostpital's rule, but I don't quite know how to deal with either the numerator nor the denominator. I have found that The is wolframalpha's evaluation of From here, you can differentiate like normal to get , . More importantly, if the limit from above is correct, then my formula works for with and I can finally define what is equal to! So the question: How can we evaluate the limit: ?  What does it equal? Is the fractional derivative of the natural logarithm and already known?  I've made posts on this before, and it doesn't appear many people have tackled this problem. Here and Here .  Both have received little attention.  Attempting to use regular fractional derivative formulas are extremely messy, so I could not actually use them.  If someone more experienced can, that'd be great.",\frac1x D^n\frac1x=\frac{\Gamma(0)}{\Gamma(-n)}x^{-1-n} n=0 n\in\mathbb N D^n n x n I^n I^n\ln(x)=\frac{x^n\left(\ln(x)-\int_0^1\frac{t^n-1}{t-1}dt\right)}{\Gamma(n+1)} \frac d{dx}I^n\ln(x)=I^{n-1}\ln(x) n=1 D^n=I^{-n} n=-0.99 \lim_{n\to-1}I^n\ln(x)=\frac1x n=-1 \frac1x=D^1\ln(x)=I^{-1}\ln(x)=\lim_{n\to-1}\frac{x^n\left(\ln(x)-\int_0^1\frac{t^n-1}{t-1}dt\right)}{\Gamma(n+1)} I^{1/2}\ln(x)=\frac{2\sqrt x\left(\ln(x)-2+\ln(4)\right)}{\sqrt\pi} 2-\ln(4) \int_0^1\frac{t^{1/2}-1}{t-1}dt D^{(2n-1)/2}\ln(x) n\in\mathbb N I^{-n}\ln(x) n\in\mathbb N D^n\frac1x \lim_{n\to-1}I^n\ln(x) \frac1x,"['calculus', 'limits', 'fractional-calculus']"
91,Limit involving the inverse beta regularized function,Limit involving the inverse beta regularized function,,"Let $0<p<\frac{1}{2}$. I am looking for the limit: $$\lim_{t \to \infty} \left(\frac{t}{\frac{t}{I_{2 p}^{-1}\left(\frac{t}{2},\frac{1}{2}\right)}-2 \sqrt{t} \sqrt{\frac{1}{I_{2 p}^{-1}\left(\frac{t}{2},\frac{1}{2}\right)}-1}+1}\right)^{t+1}$$ where $I^{-1}_{(.)}(.,.)$ is the Inverse Regularized Beta Function. Numerically the convergence is extremely fast:","Let $0<p<\frac{1}{2}$. I am looking for the limit: $$\lim_{t \to \infty} \left(\frac{t}{\frac{t}{I_{2 p}^{-1}\left(\frac{t}{2},\frac{1}{2}\right)}-2 \sqrt{t} \sqrt{\frac{1}{I_{2 p}^{-1}\left(\frac{t}{2},\frac{1}{2}\right)}-1}+1}\right)^{t+1}$$ where $I^{-1}_{(.)}(.,.)$ is the Inverse Regularized Beta Function. Numerically the convergence is extremely fast:",,"['limits', 'special-functions', 'beta-function']"
92,Show that $\lim_{n\to\infty}n+n^2 \log\left(\frac{n}{n+1}\right)= 1/2$,Show that,\lim_{n\to\infty}n+n^2 \log\left(\frac{n}{n+1}\right)= 1/2,Show that  $$\lim_{n\to\infty}\left[n+n^2 \log\left(\frac{n}{n+1}\right)\right]= \frac{1}{2}$$ I can't get how this limit comes about. It says that it comes from the sequential criterion of limits.,Show that  $$\lim_{n\to\infty}\left[n+n^2 \log\left(\frac{n}{n+1}\right)\right]= \frac{1}{2}$$ I can't get how this limit comes about. It says that it comes from the sequential criterion of limits.,,"['calculus', 'limits']"
93,How to compute $\lim _{x\to \infty }\left(\ln\left(\frac{e^x-1}{x}\right)-x\right)$?,How to compute ?,\lim _{x\to \infty }\left(\ln\left(\frac{e^x-1}{x}\right)-x\right),"I have a problem with this limit, I don't know what method to use. I have no idea how to compute it. Can you explain the method and the steps used? Thanks $$\lim _{x\to \infty }\left(\ln\left(\frac{e^x-1}{x}\right)-x\right)$$","I have a problem with this limit, I don't know what method to use. I have no idea how to compute it. Can you explain the method and the steps used? Thanks $$\lim _{x\to \infty }\left(\ln\left(\frac{e^x-1}{x}\right)-x\right)$$",,"['calculus', 'limits']"
94,How to compute $\lim\limits_{x \to 0} \left(\frac{e^{x^2} -1}{x^2}\right)^\frac{1}{x^2}$?,How to compute ?,\lim\limits_{x \to 0} \left(\frac{e^{x^2} -1}{x^2}\right)^\frac{1}{x^2},"I have a problem with this limit, I don't know what method to use. I have no idea how to compute it. Can you explain the method and the steps used? Thanks $$\lim\limits_{x \to 0} \left(\frac{e^{x^2} -1}{x^2}\right)^\frac{1}{x^2}$$ Note: In a previous version of this question the limit was written as $\left(\frac{(e^{x})^2 -1}{x^2}\right)^\frac{1}{x^2}$.","I have a problem with this limit, I don't know what method to use. I have no idea how to compute it. Can you explain the method and the steps used? Thanks $$\lim\limits_{x \to 0} \left(\frac{e^{x^2} -1}{x^2}\right)^\frac{1}{x^2}$$ Note: In a previous version of this question the limit was written as $\left(\frac{(e^{x})^2 -1}{x^2}\right)^\frac{1}{x^2}$.",,['limits']
95,L'Hôpital's rule and Difference Quotients,L'Hôpital's rule and Difference Quotients,,"Consider the general difference quotient for a function $f(x)$ that is differentiable at $x = a$: $$f'(a) = \lim_{x \to a} \frac{f(x) - f(a)}{x - a}$$ Since both the numerator and denominator of the difference quotient are differentiable and approach 0, it seems like we should be able to apply l'Hospital's rule, differentiating with respect to $x$: \begin{align} f'(a) & = \lim_{x \to a} \, \frac{f(x) - f(a)}{x - a} \\ & = \lim_{x \to a} \, f'(x), \text{ given we fix $a$ and have assumed $f$ is differentiable} \end{align} But this seems to imply that $f'(x)$ is continuous or $f(x)$ is continuously differentiable at $x = a$. I don't understand where this additional condition comes from: I suspect the problem may be in applying l'Hospital's rule (perhaps it's circular?). But to use l'Hospital's and know the limit existed, it was sufficient to assume $f(x)$ was differentiable at $x = a$. Nowhere did we have to make the assumption that the function be continuously differentiable. I must be overlooking some small detail, but I just cannot put my finger on it. Any help would be appreciated.","Consider the general difference quotient for a function $f(x)$ that is differentiable at $x = a$: $$f'(a) = \lim_{x \to a} \frac{f(x) - f(a)}{x - a}$$ Since both the numerator and denominator of the difference quotient are differentiable and approach 0, it seems like we should be able to apply l'Hospital's rule, differentiating with respect to $x$: \begin{align} f'(a) & = \lim_{x \to a} \, \frac{f(x) - f(a)}{x - a} \\ & = \lim_{x \to a} \, f'(x), \text{ given we fix $a$ and have assumed $f$ is differentiable} \end{align} But this seems to imply that $f'(x)$ is continuous or $f(x)$ is continuously differentiable at $x = a$. I don't understand where this additional condition comes from: I suspect the problem may be in applying l'Hospital's rule (perhaps it's circular?). But to use l'Hospital's and know the limit existed, it was sufficient to assume $f(x)$ was differentiable at $x = a$. Nowhere did we have to make the assumption that the function be continuously differentiable. I must be overlooking some small detail, but I just cannot put my finger on it. Any help would be appreciated.",,"['calculus', 'limits', 'derivatives']"
96,limit of $x \cot x$ as $x\to 0$.,limit of  as .,x \cot x x\to 0,I was asked to calculate $$\lim_{x \to 0}x\cot x  $$  I did it as following (using L'Hôpital's rule): $$\lim_{x\to 0} x\cot x = \lim_{x\to 0} \frac{x \cos x}{\sin x} $$ We can now use L'Hospital's rule since the limit has indeterminate form $\frac{0}{0}$. Hence $$\begin{align}\lim_{x\to 0}\frac{(x \cos x)'}{(\sin x)'}   &= \lim_{x\to 0}\frac{-x\sin x + \cos x}{\cos x} \\ &= \lim_{x\to 0}\frac{-x\sin x}{\cos x} + 1 \\[4pt ]&= \lim_{x\to 0} - x \tan x + 1 \\[4pt] &= 1 \end{align}$$ I think that the result is correct but are the arguments correct?,I was asked to calculate $$\lim_{x \to 0}x\cot x  $$  I did it as following (using L'Hôpital's rule): $$\lim_{x\to 0} x\cot x = \lim_{x\to 0} \frac{x \cos x}{\sin x} $$ We can now use L'Hospital's rule since the limit has indeterminate form $\frac{0}{0}$. Hence $$\begin{align}\lim_{x\to 0}\frac{(x \cos x)'}{(\sin x)'}   &= \lim_{x\to 0}\frac{-x\sin x + \cos x}{\cos x} \\ &= \lim_{x\to 0}\frac{-x\sin x}{\cos x} + 1 \\[4pt ]&= \lim_{x\to 0} - x \tan x + 1 \\[4pt] &= 1 \end{align}$$ I think that the result is correct but are the arguments correct?,,"['calculus', 'limits']"
97,Limit of definite integral sequence,Limit of definite integral sequence,,"This is taken from an admission exam textbook at our local construction university : $$ \lim_{n\to\infty} n\int_{1}^{2} \frac{1}{x^2(1+x^n)}dx = ?$$ I tried finding tight bounds in order to use the sandwich theorem, but that got me nowhere. I also tried using Lebesgue's Dominated Convergence Theorem, but I ended up with nothing once more. Do you have any ideas?","This is taken from an admission exam textbook at our local construction university : $$ \lim_{n\to\infty} n\int_{1}^{2} \frac{1}{x^2(1+x^n)}dx = ?$$ I tried finding tight bounds in order to use the sandwich theorem, but that got me nowhere. I also tried using Lebesgue's Dominated Convergence Theorem, but I ended up with nothing once more. Do you have any ideas?",,"['calculus', 'limits', 'definite-integrals']"
98,How to calculate this limit as $x\rightarrow 0$?,How to calculate this limit as ?,x\rightarrow 0,"Not sure how to evaluate this one any hints?$$\lim_{x\rightarrow 0} \frac{\tan^3(x)}{x}$$ I think I have an answer, would it be $0$. Since we can write is as $$\frac{\sin(x)}{x} \frac{\sin^2(x)}{\cos^3(x)}$$ and then use algebra of limit and established result for left hand limit?","Not sure how to evaluate this one any hints?$$\lim_{x\rightarrow 0} \frac{\tan^3(x)}{x}$$ I think I have an answer, would it be $0$. Since we can write is as $$\frac{\sin(x)}{x} \frac{\sin^2(x)}{\cos^3(x)}$$ and then use algebra of limit and established result for left hand limit?",,['limits']
99,Limits and Series in Smooth Infinitesimal Analysis,Limits and Series in Smooth Infinitesimal Analysis,,"I just learned a tiny bit about SIA. While it is interesting, that it handles derivatives so easily, I wonder: Can we still recover the concepts of limits (of sequences) and especially series, to make statements like: $\lim_{n\to \infty} (1+\frac{1}{n})^n = e$ $\sum_{k=1}^\infty \frac{1}{k} = \infty$ $e^x = \sum_{k=0}^\infty \frac{x^k}{k!}$ and so on and so forth? I didn't find anything about this.","I just learned a tiny bit about SIA. While it is interesting, that it handles derivatives so easily, I wonder: Can we still recover the concepts of limits (of sequences) and especially series, to make statements like: $\lim_{n\to \infty} (1+\frac{1}{n})^n = e$ $\sum_{k=1}^\infty \frac{1}{k} = \infty$ $e^x = \sum_{k=0}^\infty \frac{x^k}{k!}$ and so on and so forth? I didn't find anything about this.",,"['real-analysis', 'limits', 'convergence-divergence', 'nonstandard-analysis', 'constructive-mathematics']"
