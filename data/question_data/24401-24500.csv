,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Show that $L^nv\rightarrow 0$.,Show that .,L^nv\rightarrow 0,"Let $V$ be a finite-dimensional normed vector space. Let $L:V\rightarrow V$ be a linear operator and let $v\in V$ . Assume that there is a sequence $\{n_i\}_{i=1}^\infty\subset\mathbb{Z}$ such that $L^{n_i}v\rightarrow 0$ . Show that $L^nv\rightarrow 0$ as $n\rightarrow \infty$ . My try: If $v$ is an eigenvector of $L$ , then we have $$ 0=\lim L^{n_i}v=\lim \lambda^{n_i}v $$ which implies that $|\lambda|<1$ . Then we have $$ \lim L^nv=\lim \lambda^nv=0. $$ How to prove when $v$ is not eigenvector?","Let be a finite-dimensional normed vector space. Let be a linear operator and let . Assume that there is a sequence such that . Show that as . My try: If is an eigenvector of , then we have which implies that . Then we have How to prove when is not eigenvector?","V L:V\rightarrow V v\in V \{n_i\}_{i=1}^\infty\subset\mathbb{Z} L^{n_i}v\rightarrow 0 L^nv\rightarrow 0 n\rightarrow \infty v L 
0=\lim L^{n_i}v=\lim \lambda^{n_i}v
 |\lambda|<1 
\lim L^nv=\lim \lambda^nv=0.
 v",['linear-algebra']
1,Structure Constants for $ 2 $ by $ 2 $ Matrices,Structure Constants for  by  Matrices, 2   2 ,"The space of $2 \times 2$ matrices is $4$ -dimensional. For a generic choice of $3$ matrices $X$ , $Y$ , and $Z$ , we can take a basis to be $\{\mathrm{Id}_2, X, Y, Z\}$ . The following fact seems, experimentally, to be true: Expand the three products $YX$ , $ZY$ , and $XZ$ in this basis: $$ \begin {align*} YX &= a_{yx} \mathrm{Id} + b_{yx}X + c_{yx}Y + d_{yx}Z \\[1.2ex] ZY &= a_{zy} \mathrm{Id} + b_{zy}X + c_{zy}Y + d_{zy}Z \\[1.2ex] XZ &= a_{xz} \mathrm{Id} + b_{xz}X + c_{xz}Y + d_{xz}Z \end {align*} $$ Then it seems that we always have the following relations among these structure constants: $$ b_{yx} = d_{zy}, ~~ c_{yx} = d_{xz}, ~~ c_{zy} = b_{xz} $$ I do not see why this is true. Does anyone know a proof or explanation?","The space of matrices is -dimensional. For a generic choice of matrices , , and , we can take a basis to be . The following fact seems, experimentally, to be true: Expand the three products , , and in this basis: Then it seems that we always have the following relations among these structure constants: I do not see why this is true. Does anyone know a proof or explanation?","2 \times 2 4 3 X Y Z \{\mathrm{Id}_2, X, Y, Z\} YX ZY XZ  \begin {align*}
YX &= a_{yx} \mathrm{Id} + b_{yx}X + c_{yx}Y + d_{yx}Z \\[1.2ex]
ZY &= a_{zy} \mathrm{Id} + b_{zy}X + c_{zy}Y + d_{zy}Z \\[1.2ex]
XZ &= a_{xz} \mathrm{Id} + b_{xz}X + c_{xz}Y + d_{xz}Z
\end {align*}
  b_{yx} = d_{zy}, ~~ c_{yx} = d_{xz}, ~~ c_{zy} = b_{xz} ","['linear-algebra', 'abstract-algebra', 'matrices']"
2,$\det(xA - B) = 0$ and diagonalization,and diagonalization,\det(xA - B) = 0,"Let $A, B$ be two $3 \times 3$ (complex) symmetric matrices and suppose the equation $\det(xA - B) = 0$ has three distinct solutions. Prove that $A$ is invertible. Any help appreciated!",Let be two (complex) symmetric matrices and suppose the equation has three distinct solutions. Prove that is invertible. Any help appreciated!,"A, B 3 \times 3 \det(xA - B) = 0 A","['linear-algebra', 'matrices']"
3,Let $T: V \to V$ be a linear map such that $T^2-3T+2I=0$.,Let  be a linear map such that .,T: V \to V T^2-3T+2I=0,"Let $T: V \to V$ be a linear map such that $T^2-3T+2I=0$ , where $I$ is the identity map. question: a) Prove that $V=\ker(T-2I) \oplus\ker(T-I)$ b) let $A$ be an $n \times n$ matrix such that $A^2-3A+2I_n=0$ Where $I_n$ is the $n\times n$ identity matrix. True or false: $A$ is diagonalizable I attempted $\ker(T-2I)=(T-2I)V=0$ $\ker(T-I)=(T-I)V=0$ I know the direct sum should be the join of $\ker(T-2T)$ and $\ker(T-I)$ is $0$ , and I am not sure how to prove it I am a first year student from McGill U. I am doing linear mapping on linear algebra. The textbook I am using is Linear Algebre edition sixth by SEYMOUR LIPAXHUTZ","Let be a linear map such that , where is the identity map. question: a) Prove that b) let be an matrix such that Where is the identity matrix. True or false: is diagonalizable I attempted I know the direct sum should be the join of and is , and I am not sure how to prove it I am a first year student from McGill U. I am doing linear mapping on linear algebra. The textbook I am using is Linear Algebre edition sixth by SEYMOUR LIPAXHUTZ",T: V \to V T^2-3T+2I=0 I V=\ker(T-2I) \oplus\ker(T-I) A n \times n A^2-3A+2I_n=0 I_n n\times n A \ker(T-2I)=(T-2I)V=0 \ker(T-I)=(T-I)V=0 \ker(T-2T) \ker(T-I) 0,"['linear-algebra', 'vector-spaces', 'direct-sum']"
4,Does a norm ball around a real matrix contain all possible pairs of complex spectrum?,Does a norm ball around a real matrix contain all possible pairs of complex spectrum?,,"We consider the normed vector space $(M_n(\mathbb R), \|\cdot\|_F)$ , i.e., real matrices with Frobenius norm. Let $A \in M_n(\mathbb R)$ be a diagonalizable matrix and have all eigenvalues to be real. Let $B_A(\varepsilon)$ denote the open norm ball with radius $\varepsilon > 0$ , i.e., \begin{align*} B_A(\varepsilon) =\{ E \in M_n(\mathbb R): \|A-E\|_F < \varepsilon\}. \end{align*} We know the complex eigenvalues of a real matrix must be conjugate pairs. My question is: for any combination of real or complex conjugate pairs of eigenvalues, is there always a matrix $E \in B_A(\varepsilon)$ has the spectrum with the same number of real and complex conjugate pairs. To be more clear, let $n = 2k$ be even, I would like to know whether there are always matrices in the norm ball such that have eigenvalues with $1$ complex conjugate pair, $2$ pairs, and so on until $k$ pairs of conjugate eigenvalues.","We consider the normed vector space , i.e., real matrices with Frobenius norm. Let be a diagonalizable matrix and have all eigenvalues to be real. Let denote the open norm ball with radius , i.e., We know the complex eigenvalues of a real matrix must be conjugate pairs. My question is: for any combination of real or complex conjugate pairs of eigenvalues, is there always a matrix has the spectrum with the same number of real and complex conjugate pairs. To be more clear, let be even, I would like to know whether there are always matrices in the norm ball such that have eigenvalues with complex conjugate pair, pairs, and so on until pairs of conjugate eigenvalues.","(M_n(\mathbb R), \|\cdot\|_F) A \in M_n(\mathbb R) B_A(\varepsilon) \varepsilon > 0 \begin{align*}
B_A(\varepsilon) =\{ E \in M_n(\mathbb R): \|A-E\|_F < \varepsilon\}.
\end{align*} E \in B_A(\varepsilon) n = 2k 1 2 k","['linear-algebra', 'eigenvalues-eigenvectors', 'perturbation-theory']"
5,Miscalculating the determinant,Miscalculating the determinant,,"I am learning linear algebra and am getting stuck when trying to calculate the determinant using elementary row operations. Consider the matrix A. \begin{vmatrix} 0 & 1 & 2 & 3 \\ 1 & 1 & 1 & 1 \\ -2 & -2 & 3 & 3 \\ 1 & 2 & -2 & -3 \\ \end{vmatrix} According to the solution in my textbook and Matlab the determinant should be 10. I however find -20. Here is what I did. I first interchanged row 1 and row 3. \begin{vmatrix} 1 & 2 & -2 & -3 \\ 1 & 1 & 1 & 1 \\ -2 & -2 & 3 & 3 \\ 0 & 1 & 2 & 3 \end{vmatrix} I then substracted row 1 from row two. I also added the first row twice to the third row. \begin{vmatrix} 1 & 2 & -2 & -3 \\ 0 & -1 & 3 & -2 \\ 0 & 2 & -1 & 9 \\ 0 & 1 & 2 & 3 \end{vmatrix} Then, I added the second row twice to the third row and once to the fourth row. \begin{vmatrix} 1 & 2 & -2 & -3 \\ 0 & -1 & 3 & -2 \\ 0 & 0 & 5 & 5 \\ 0 & 0 & 5 & 1 \end{vmatrix} My final operation was to substract the third row from the fourth row, which gave: \begin{vmatrix} 1 & 2 & -2 & -3 \\ 0 & -1 & 3 & -2 \\ 0 & 0 & 5 & 5 \\ 0 & 0 & 0 & -4 \end{vmatrix} Finally, I calculated the determinant: $(-1)^1 \cdot 1 \cdot -1 \cdot 5 \cdot -4 = -20$ The $(-1)^1$ is there since I did one operation in which I interchanged two rows. I would really appreciate if you could tell me what I did wrong. Martijn","I am learning linear algebra and am getting stuck when trying to calculate the determinant using elementary row operations. Consider the matrix A. According to the solution in my textbook and Matlab the determinant should be 10. I however find -20. Here is what I did. I first interchanged row 1 and row 3. I then substracted row 1 from row two. I also added the first row twice to the third row. Then, I added the second row twice to the third row and once to the fourth row. My final operation was to substract the third row from the fourth row, which gave: Finally, I calculated the determinant: The is there since I did one operation in which I interchanged two rows. I would really appreciate if you could tell me what I did wrong. Martijn","\begin{vmatrix}
0 & 1 & 2 & 3 \\
1 & 1 & 1 & 1 \\
-2 & -2 & 3 & 3 \\
1 & 2 & -2 & -3 \\
\end{vmatrix} \begin{vmatrix}
1 & 2 & -2 & -3 \\
1 & 1 & 1 & 1 \\
-2 & -2 & 3 & 3 \\
0 & 1 & 2 & 3
\end{vmatrix} \begin{vmatrix}
1 & 2 & -2 & -3 \\
0 & -1 & 3 & -2 \\
0 & 2 & -1 & 9 \\
0 & 1 & 2 & 3
\end{vmatrix} \begin{vmatrix}
1 & 2 & -2 & -3 \\
0 & -1 & 3 & -2 \\
0 & 0 & 5 & 5 \\
0 & 0 & 5 & 1
\end{vmatrix} \begin{vmatrix}
1 & 2 & -2 & -3 \\
0 & -1 & 3 & -2 \\
0 & 0 & 5 & 5 \\
0 & 0 & 0 & -4
\end{vmatrix} (-1)^1 \cdot 1 \cdot -1 \cdot 5 \cdot -4 = -20 (-1)^1","['linear-algebra', 'determinant']"
6,Reflections along root vectors in a basis generate the Weyl group?,Reflections along root vectors in a basis generate the Weyl group?,,"Let $\Phi$ be a root system of type ADE, $\Lambda$ be the lattice in the Euclidean space spanned by $\Phi$ . If $\Lambda$ is spanned by $\{v_i\}\subset \Phi$ (as a $\mathbb Z$ -module), and let $\sigma_i$ be the reflection along hyperplane associated to $v_i$ . Is it true that $\{\sigma_i\}$ generate the Weyl group $W(\Phi)$ ? My approach I know there is a proposition says reflections along vectors in a base (of a root system, or in some book called simple system) generate the full $W(\Phi)$ . So I tried to show $\{v_i\}$ contains a base, but have no idea how to do this. More precisely, I don't know how to use the condition $\{v_i\}$ generate $\Lambda$ . If this too general, what about the case $\Phi=E_6$ ? If $6$ vectors $v_1,\ldots,v_6$ in the root system generate the lattice, is it true that after possibally replacing $v_i$ by $-v_i$ for some $i$ , they will form a base of $E_6$ ?","Let be a root system of type ADE, be the lattice in the Euclidean space spanned by . If is spanned by (as a -module), and let be the reflection along hyperplane associated to . Is it true that generate the Weyl group ? My approach I know there is a proposition says reflections along vectors in a base (of a root system, or in some book called simple system) generate the full . So I tried to show contains a base, but have no idea how to do this. More precisely, I don't know how to use the condition generate . If this too general, what about the case ? If vectors in the root system generate the lattice, is it true that after possibally replacing by for some , they will form a base of ?","\Phi \Lambda \Phi \Lambda \{v_i\}\subset \Phi \mathbb Z \sigma_i v_i \{\sigma_i\} W(\Phi) W(\Phi) \{v_i\} \{v_i\} \Lambda \Phi=E_6 6 v_1,\ldots,v_6 v_i -v_i i E_6","['linear-algebra', 'lie-algebras', 'integer-lattices', 'root-systems']"
7,How to find the analytical representation of eigenvalues of the matrix $G$?,How to find the analytical representation of eigenvalues of the matrix ?,G,"I have the following matrix arising when I tried to discretize the Green function， now to show the convergence of my algorithm I need to find the eigenvalues of the matrix $G$ and show it has absolute value less than 1 for certain choices of $N$. Note that the explicit formula for entry $(i,j)$ is $-i(N+1-j)$ when $i\le j$ and it is symmetric, so we can get the formulas for $i>j$ by interchanging $i$ and $j$ in the $i\le j$ case. Any one has any ideas about how to find the analytical representation of eigenvalues of the matrix $G$, i,e, the eigenvalues represented by $N$? Thank you so much for any help! $\begin{pmatrix}  - N & - N + 1 & -N+2 & -N+3 &\ldots & 1(-2) & 1(-1) \\  - N + 1 & 2( - N + 1) & 2(-N+2) & 2(-N+3) &\ddots & 2(-2) & 2(-1) \\  - N + 2 & 2( - N + 2) & 3(-N+2) & 3(-N+3) &\ddots & 3(-2) & 3(-1) \\  - N + 3 & 2( - N + 3) & 3(-N+3) & 4(-N+3) &\ddots & 4(-2) & 4(-1) \\  \vdots & \vdots & \ddots & \vdots & \vdots \\  - 2 & 2(-2) & 3(-2) & 4(-2) &\ddots & ( - 1 + N)( - 2) & ( - 1 + N)( - 1) \\  - 1 & 2(-1) & 3(-1) & 4(-1) &\ldots & ( - 1 + N)( - 1) & N( - 1) \\ \end{pmatrix}$","I have the following matrix arising when I tried to discretize the Green function， now to show the convergence of my algorithm I need to find the eigenvalues of the matrix $G$ and show it has absolute value less than 1 for certain choices of $N$. Note that the explicit formula for entry $(i,j)$ is $-i(N+1-j)$ when $i\le j$ and it is symmetric, so we can get the formulas for $i>j$ by interchanging $i$ and $j$ in the $i\le j$ case. Any one has any ideas about how to find the analytical representation of eigenvalues of the matrix $G$, i,e, the eigenvalues represented by $N$? Thank you so much for any help! $\begin{pmatrix}  - N & - N + 1 & -N+2 & -N+3 &\ldots & 1(-2) & 1(-1) \\  - N + 1 & 2( - N + 1) & 2(-N+2) & 2(-N+3) &\ddots & 2(-2) & 2(-1) \\  - N + 2 & 2( - N + 2) & 3(-N+2) & 3(-N+3) &\ddots & 3(-2) & 3(-1) \\  - N + 3 & 2( - N + 3) & 3(-N+3) & 4(-N+3) &\ddots & 4(-2) & 4(-1) \\  \vdots & \vdots & \ddots & \vdots & \vdots \\  - 2 & 2(-2) & 3(-2) & 4(-2) &\ddots & ( - 1 + N)( - 2) & ( - 1 + N)( - 1) \\  - 1 & 2(-1) & 3(-1) & 4(-1) &\ldots & ( - 1 + N)( - 1) & N( - 1) \\ \end{pmatrix}$",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
8,The relationship between spectral decomposition / eigendecomposition and projection operators,The relationship between spectral decomposition / eigendecomposition and projection operators,,"I am trying to clarify the relationship between the spectral decomposition / eigendecomposition of a matrix and projection operators. I understand that there is a connection between diagonalizability of a linear operator / matrix and projection operators in the following sense: Given a finite-dimensional vector space $V$ ($\dim V = n$) and a linear operator $T \in L(V)$ that has $k \leq n$ distinct eigenvalues $\lambda_1, \dots, \lambda_k$, if $T$ is diagonalizable, then there exist $k$ linear operators $E_1, \dots, E_k$ on $V$ such that the $E_i$ are projection operators, $I = \sum_i E_i$, and $T = \sum_i \lambda_i E_i$, etc. (and an analogous converse also holds). I also understand that the spectral decomposition / eigendecomposition of an $n \times n$ matrix $\mathbf{A}$ with $n$ linearly independent eigenvectors can be written as $\mathbf{A = Q \boldsymbol{\Lambda} Q^{-1}}$, where $\mathbf{Q}$ is the matrix whose $i$th column is the eigenvector $q_i$ of $\mathbf{A}$, and $\Lambda_{ii} = \lambda_i$. If $\mathbf{A}$ is a normal matrix, then $\mathbf{Q}$ is unitary, so that  $$\mathbf{A} = \sum_{i=1}^n\lambda_iq_i q_i^*,$$ where $q_i^*$ is the adjoint of $q_i$. Does this mean, then, that the projection operator associated with $\lambda_i$ can be related to the sum of outer products of eigenvectors with the same eigenvalue: $$ E_i = \sum_{j=1}^{r_i} q_j q_j^*,$$ where $r_i$ is the algebraic multiplicity of $\lambda_i$? And then all of the associated properties of the projection operators hold? Or are there additional assumptions/conditions (other than $\mathbf{A}$ being normal) that need to be taken to be true for this relationship to hold (for instance, I believe that $V$ needs to be an inner product space for the spectral decomposition to be formulated)? Any additional information on the relationship between these two paradigms would be greatly appreciated. Thank you!","I am trying to clarify the relationship between the spectral decomposition / eigendecomposition of a matrix and projection operators. I understand that there is a connection between diagonalizability of a linear operator / matrix and projection operators in the following sense: Given a finite-dimensional vector space $V$ ($\dim V = n$) and a linear operator $T \in L(V)$ that has $k \leq n$ distinct eigenvalues $\lambda_1, \dots, \lambda_k$, if $T$ is diagonalizable, then there exist $k$ linear operators $E_1, \dots, E_k$ on $V$ such that the $E_i$ are projection operators, $I = \sum_i E_i$, and $T = \sum_i \lambda_i E_i$, etc. (and an analogous converse also holds). I also understand that the spectral decomposition / eigendecomposition of an $n \times n$ matrix $\mathbf{A}$ with $n$ linearly independent eigenvectors can be written as $\mathbf{A = Q \boldsymbol{\Lambda} Q^{-1}}$, where $\mathbf{Q}$ is the matrix whose $i$th column is the eigenvector $q_i$ of $\mathbf{A}$, and $\Lambda_{ii} = \lambda_i$. If $\mathbf{A}$ is a normal matrix, then $\mathbf{Q}$ is unitary, so that  $$\mathbf{A} = \sum_{i=1}^n\lambda_iq_i q_i^*,$$ where $q_i^*$ is the adjoint of $q_i$. Does this mean, then, that the projection operator associated with $\lambda_i$ can be related to the sum of outer products of eigenvectors with the same eigenvalue: $$ E_i = \sum_{j=1}^{r_i} q_j q_j^*,$$ where $r_i$ is the algebraic multiplicity of $\lambda_i$? And then all of the associated properties of the projection operators hold? Or are there additional assumptions/conditions (other than $\mathbf{A}$ being normal) that need to be taken to be true for this relationship to hold (for instance, I believe that $V$ needs to be an inner product space for the spectral decomposition to be formulated)? Any additional information on the relationship between these two paradigms would be greatly appreciated. Thank you!",,"['linear-algebra', 'eigenvalues-eigenvectors', 'spectral-theory', 'matrix-decomposition', 'projection-matrices']"
9,operator concavity of a function involving trace and logarithm,operator concavity of a function involving trace and logarithm,,"I want to see if  $$f(A)= \operatorname{trace}(C\log(I+\sqrt{A}B\sqrt{A}))$$ is operator concave with respect to a Hermitian positive definite matrices $A$?  $\log$ is matrix logarithm,  $B$ and $C$ are arbitrary  Hermitian positive definite matrices, and $I$ is unit matrix. I checked it numerically with many randomly generated positive definite matrices, using the following condition from Bhatia's matrix analysis: $$f\left({\frac{X+Y}{2}}\right)>\frac{f(X)}{2}+\frac{f(Y)}{2}$$ and it seems that the condition is satisfied. I want to prove it using the same condition. I am trying to show: $$ \operatorname{trace}\left(C\log\left(I+\sqrt{\frac{X+Y}{2}}B\sqrt{\frac{X+Y}{2}}\right)\right)>\frac{\operatorname{trace}\left(C\left(\log(I+\sqrt{X}B\sqrt{X})+\log(I+\sqrt{Y}B\sqrt{Y})\right)\right)}{2}, (1)$$ Update: First, I solve it for special case $C=I$ (i.e. $C$ is identity matrix). Using $\operatorname{trace}(\log x)=\log \det(x)$, left  side of (1) is, $$\operatorname{trace}\left(\log\left(I+\sqrt{\frac{X+Y}{2}}B\sqrt{\frac{X+Y}{2}}\right)\right) \\=\log\left(\det\left(I+\sqrt{\frac{X+Y}{2}}B\sqrt{\frac{X+Y}{2}}\right)\right) \\=\log\left(\det\left(I+{\frac{X+Y}{2}}B\right)\right), (2)$$ Third line is from Sylvester's determinant identity( https://en.wikipedia.org/wiki/Sylvester%27s_determinant_identity ). Right  side of (1) becomes $$\frac{1}{2}\operatorname{trace}\left(\left(\log(I+\sqrt{X}B\sqrt{X})+\log(I+\sqrt{Y}B\sqrt{Y})\right)\right)\\ =\frac{1}{2}\left(\log\left(\det(I+\sqrt{X}B\sqrt{X})\right)+\log\left(\det(I+\sqrt{Y}B\sqrt{Y})\right)\right)\\ =\frac{1}{2}\left(\log\left(\det(I+XB)\right)+\log\left(\det(I+YB)\right)\right), (3)$$ From concavity of log-determinant ( Log-Determinant Concavity Proof ), I conclude (2) is greater than(3)  and thus the condition (1) is satisfied for $C=I$. I have two questions: 1) Is my conclusion correct?(I am not 100% sure) 2)How can I extend this result (if it is correct) to a general positive definite $C$? Update 2: I also tried the approach suggested in the answer to this question: Is the trace of inverse matrix convex? ...but the second derivative was too complicated. Any help is very appreciated. If you are aware of any other way to prove concavity of $f(A)= \operatorname{trace}(C\log(I+\sqrt{A}B\sqrt{A}))$ please let me know.","I want to see if  $$f(A)= \operatorname{trace}(C\log(I+\sqrt{A}B\sqrt{A}))$$ is operator concave with respect to a Hermitian positive definite matrices $A$?  $\log$ is matrix logarithm,  $B$ and $C$ are arbitrary  Hermitian positive definite matrices, and $I$ is unit matrix. I checked it numerically with many randomly generated positive definite matrices, using the following condition from Bhatia's matrix analysis: $$f\left({\frac{X+Y}{2}}\right)>\frac{f(X)}{2}+\frac{f(Y)}{2}$$ and it seems that the condition is satisfied. I want to prove it using the same condition. I am trying to show: $$ \operatorname{trace}\left(C\log\left(I+\sqrt{\frac{X+Y}{2}}B\sqrt{\frac{X+Y}{2}}\right)\right)>\frac{\operatorname{trace}\left(C\left(\log(I+\sqrt{X}B\sqrt{X})+\log(I+\sqrt{Y}B\sqrt{Y})\right)\right)}{2}, (1)$$ Update: First, I solve it for special case $C=I$ (i.e. $C$ is identity matrix). Using $\operatorname{trace}(\log x)=\log \det(x)$, left  side of (1) is, $$\operatorname{trace}\left(\log\left(I+\sqrt{\frac{X+Y}{2}}B\sqrt{\frac{X+Y}{2}}\right)\right) \\=\log\left(\det\left(I+\sqrt{\frac{X+Y}{2}}B\sqrt{\frac{X+Y}{2}}\right)\right) \\=\log\left(\det\left(I+{\frac{X+Y}{2}}B\right)\right), (2)$$ Third line is from Sylvester's determinant identity( https://en.wikipedia.org/wiki/Sylvester%27s_determinant_identity ). Right  side of (1) becomes $$\frac{1}{2}\operatorname{trace}\left(\left(\log(I+\sqrt{X}B\sqrt{X})+\log(I+\sqrt{Y}B\sqrt{Y})\right)\right)\\ =\frac{1}{2}\left(\log\left(\det(I+\sqrt{X}B\sqrt{X})\right)+\log\left(\det(I+\sqrt{Y}B\sqrt{Y})\right)\right)\\ =\frac{1}{2}\left(\log\left(\det(I+XB)\right)+\log\left(\det(I+YB)\right)\right), (3)$$ From concavity of log-determinant ( Log-Determinant Concavity Proof ), I conclude (2) is greater than(3)  and thus the condition (1) is satisfied for $C=I$. I have two questions: 1) Is my conclusion correct?(I am not 100% sure) 2)How can I extend this result (if it is correct) to a general positive definite $C$? Update 2: I also tried the approach suggested in the answer to this question: Is the trace of inverse matrix convex? ...but the second derivative was too complicated. Any help is very appreciated. If you are aware of any other way to prove concavity of $f(A)= \operatorname{trace}(C\log(I+\sqrt{A}B\sqrt{A}))$ please let me know.",,"['linear-algebra', 'matrices', 'convex-analysis', 'trace', 'positive-definite']"
10,Sufficient conditions for difference of matrices A-B to be positive definite,Sufficient conditions for difference of matrices A-B to be positive definite,,"Define $A>B\iff A-B\succ 0$, i.e. $A-B$ is positive definite. A well-known sufficient condition for $A>B$ is that the largest eigenvalue of $B$ is smaller than the smallest eigenvalue of $A$. Are there any other useful sufficient conditions for $A>B$? Are there any equivalent properties (i.e. $A>B$ if and only if $A,B$ satisfy some property $P$)?","Define $A>B\iff A-B\succ 0$, i.e. $A-B$ is positive definite. A well-known sufficient condition for $A>B$ is that the largest eigenvalue of $B$ is smaller than the smallest eigenvalue of $A$. Are there any other useful sufficient conditions for $A>B$? Are there any equivalent properties (i.e. $A>B$ if and only if $A,B$ satisfy some property $P$)?",,"['linear-algebra', 'abstract-algebra', 'matrices', 'functional-analysis', 'order-theory']"
11,Matrix with $A_{ij} = a_i a_j$ for some vector $a\in\mathbb{R}^n$,Matrix with  for some vector,A_{ij} = a_i a_j a\in\mathbb{R}^n,"If $f(x) = g(\langle a,x\rangle)$ for $a,x\in\mathbb{R}^n$ and $g: \mathbb{R}\to\mathbb{R}$, with the usual inner product, then the matrix of second partials is $D^2f(x) = g''(\langle a,x\rangle) A$, where $A$ has components $A_{ij} = a_i a_j$. Is there a name for such a matrix constructed from a vector in this manner, or more generally $A_{ij} = a_i b_j$ for $b\in\mathbb{R}^m$ with $m$ not necessarily equal to $n$?","If $f(x) = g(\langle a,x\rangle)$ for $a,x\in\mathbb{R}^n$ and $g: \mathbb{R}\to\mathbb{R}$, with the usual inner product, then the matrix of second partials is $D^2f(x) = g''(\langle a,x\rangle) A$, where $A$ has components $A_{ij} = a_i a_j$. Is there a name for such a matrix constructed from a vector in this manner, or more generally $A_{ij} = a_i b_j$ for $b\in\mathbb{R}^m$ with $m$ not necessarily equal to $n$?",,"['linear-algebra', 'matrices', 'multivariable-calculus']"
12,Is there a relation between the solutions to these two Lyapunov matrix equations?,Is there a relation between the solutions to these two Lyapunov matrix equations?,,"Let $A \in \mathcal M(n \times n; \mathbb R)$ with $\rho(A) < 1$. Let $X, Y$ be the solutions to the following Lyapunov matrix equations \begin{align*} X &= A^T X A + Q \\ Y &= A Y A^T + Q \end{align*} where positive definite matrix $Q$ is given. By the assumptions on $A$, the solutions have closed form \begin{align*} X &= \sum_{k=0}^{\infty} (A^T)^kQA^k,\\ Y &= \sum_{k=0}^{\infty} A^k Q (A^T)^k \end{align*} I am wondering whether there are relationships between $X$ and $Y$, such as spectrum, norms, etc.","Let $A \in \mathcal M(n \times n; \mathbb R)$ with $\rho(A) < 1$. Let $X, Y$ be the solutions to the following Lyapunov matrix equations \begin{align*} X &= A^T X A + Q \\ Y &= A Y A^T + Q \end{align*} where positive definite matrix $Q$ is given. By the assumptions on $A$, the solutions have closed form \begin{align*} X &= \sum_{k=0}^{\infty} (A^T)^kQA^k,\\ Y &= \sum_{k=0}^{\infty} A^k Q (A^T)^k \end{align*} I am wondering whether there are relationships between $X$ and $Y$, such as spectrum, norms, etc.",,"['linear-algebra', 'matrices', 'matrix-equations', 'matrix-analysis']"
13,Connectedness of a subset of structured matrices satisfying prescribed conditions on roots of polynomials,Connectedness of a subset of structured matrices satisfying prescribed conditions on roots of polynomials,,"I am extracting information of me proving an application problem, if there is anything not clear, please let me know. Suppose we parameterize a structured matrix in following form \begin{align} \label{matrix:1} \tag{$\star$} \begin{pmatrix} 0 & -a_1 & 0 & 0 & -b_1 \\ 1 & -a_2 & 0 & 0 & -b_2 \\ 0 & -a_3 & 0 & 0 & -b_3 \\ 0 & -a_4 & 1 & 0 & -b_4 \\ 0 & -a_5 & 0 &1 & -b_5 \end{pmatrix}, \end{align} where the upper left $2 \times 2$ and lower right $3 \times 3$ blocks are in companion form . I would like to claim that the set of matrices of above form and with eigenvalues modules bounded in some interval $(-M, M)$ is connected. If we let     \begin{align*}  E = \{ A \in \mathcal{M}(5 \times 5; \mathbb R): \text{ $A$ is of form \eqref{matrix:1} and } \rho(A) < M \}, \end{align*}     we want to prove $E$ is connected. My approach is to first prove the set \begin{align*}  F = \{ B \in \mathcal{M}(5 \times 5; \mathbb R): \text{ structure of $B$ is of the form \eqref{matrix:2} and } \rho(B) < M \} \end{align*} is connected, where \begin{align} \label{matrix:2} \tag{$\ast$} \begin{pmatrix} 0 & -a_1 & 0 & 0 & 0 \\ 1 & -a_2 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & -b_3 \\ 0 & 0 & 1 & 0 & -b_4 \\ 0 &  0 & 0 &1 & -b_5 \end{pmatrix}. \end{align} If a matrix $C$ has the form \eqref{matrix:2}, then the characteristic polynomial $p_C(t) = (t^2+a_2 t + a_1)(t^3 + b_5 t^2 + b_4 t +b_3)$. If I am not mistaken, the set $F$ should be a continuous image of roots lying in $(-M, M)$ which should be connected. Now I would like to show for a general element in $E$, we can find a continuous path to the set $F$. Since for any $A \in E$, the characteristic polynomial  $$p_A(t) = (t^2+a_2 t + a_1)(t^3 + b_5 t^2 + b_4 t +b_3) + \text{ polynomial terms involving entries of } a, b.$$ Intuitively, we can first choose some element $D \in F$ with $p_D(t) = p_A(t)$ and then continuously transform the coefficients there. But now I could not prove the whole path will only contain elements of $E$. Would anyone help me with this approach or provide some other approaches? Or given examples to show the set is actually not connected?","I am extracting information of me proving an application problem, if there is anything not clear, please let me know. Suppose we parameterize a structured matrix in following form \begin{align} \label{matrix:1} \tag{$\star$} \begin{pmatrix} 0 & -a_1 & 0 & 0 & -b_1 \\ 1 & -a_2 & 0 & 0 & -b_2 \\ 0 & -a_3 & 0 & 0 & -b_3 \\ 0 & -a_4 & 1 & 0 & -b_4 \\ 0 & -a_5 & 0 &1 & -b_5 \end{pmatrix}, \end{align} where the upper left $2 \times 2$ and lower right $3 \times 3$ blocks are in companion form . I would like to claim that the set of matrices of above form and with eigenvalues modules bounded in some interval $(-M, M)$ is connected. If we let     \begin{align*}  E = \{ A \in \mathcal{M}(5 \times 5; \mathbb R): \text{ $A$ is of form \eqref{matrix:1} and } \rho(A) < M \}, \end{align*}     we want to prove $E$ is connected. My approach is to first prove the set \begin{align*}  F = \{ B \in \mathcal{M}(5 \times 5; \mathbb R): \text{ structure of $B$ is of the form \eqref{matrix:2} and } \rho(B) < M \} \end{align*} is connected, where \begin{align} \label{matrix:2} \tag{$\ast$} \begin{pmatrix} 0 & -a_1 & 0 & 0 & 0 \\ 1 & -a_2 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & -b_3 \\ 0 & 0 & 1 & 0 & -b_4 \\ 0 &  0 & 0 &1 & -b_5 \end{pmatrix}. \end{align} If a matrix $C$ has the form \eqref{matrix:2}, then the characteristic polynomial $p_C(t) = (t^2+a_2 t + a_1)(t^3 + b_5 t^2 + b_4 t +b_3)$. If I am not mistaken, the set $F$ should be a continuous image of roots lying in $(-M, M)$ which should be connected. Now I would like to show for a general element in $E$, we can find a continuous path to the set $F$. Since for any $A \in E$, the characteristic polynomial  $$p_A(t) = (t^2+a_2 t + a_1)(t^3 + b_5 t^2 + b_4 t +b_3) + \text{ polynomial terms involving entries of } a, b.$$ Intuitively, we can first choose some element $D \in F$ with $p_D(t) = p_A(t)$ and then continuously transform the coefficients there. But now I could not prove the whole path will only contain elements of $E$. Would anyone help me with this approach or provide some other approaches? Or given examples to show the set is actually not connected?",,"['linear-algebra', 'abstract-algebra', 'general-topology', 'polynomials', 'connectedness']"
14,A problem about matrix eigenvalues: the eigenvalues of $A_n$ are all positive and no more than $3+2\sqrt 2$,A problem about matrix eigenvalues: the eigenvalues of  are all positive and no more than,A_n 3+2\sqrt 2,"Today I was asked by a friend to have a look over a linear algebra problem, which is quite easy at a first glance: For $n\in\mathbb N$, put $$A_n=\left[\begin{matrix}1 &\frac{1}{2} &\frac{1}{3} &\cdots &\frac{1}{n}\\ \frac{1}{2} &\frac{1}{2} &\frac{1}{3} &\cdots &\frac{1}{n}\\ \frac{1}{3} &\frac{1}{3} &\frac{1}{3} &\cdots &\frac{1}{n}\\ \vdots& & &\ddots & \vdots\\ \frac{1}{n}& \frac{1}{n}& \frac{1}{n}& \cdots &\frac{1}{n}\end{matrix}\right].$$ Then the eigenvalues of $A_n$ are all positive and no more than $3+2\sqrt2$. To the positivity of eigenvalues, we may consider the matrix $P=\left[\begin{smallmatrix} 1 \\ -1 & 1\\ & \ddots&  \ddots \\& & & 1\\& & & -1 &1\end{smallmatrix}\right]$ and it follows that $P^\top AP=\left[\begin{smallmatrix} 1/2 \\ & 1/6& &\ast \\ & &\ddots\\ & &  & 1/(n-1)n\\ & O& & & 1/n\end{smallmatrix}\right]$. Now the problem is, the wierd bound $3+2\sqrt 2$ cannot be seen. Diagonalizing this real symmetric matrix does not seem to work because it usually needs the eigenvalues beforehand. I thought about Gerschgorin disk but unfortunately it does no help here. So I would like to ask for some ideas about this problem, and any help is appreciated.","Today I was asked by a friend to have a look over a linear algebra problem, which is quite easy at a first glance: For $n\in\mathbb N$, put $$A_n=\left[\begin{matrix}1 &\frac{1}{2} &\frac{1}{3} &\cdots &\frac{1}{n}\\ \frac{1}{2} &\frac{1}{2} &\frac{1}{3} &\cdots &\frac{1}{n}\\ \frac{1}{3} &\frac{1}{3} &\frac{1}{3} &\cdots &\frac{1}{n}\\ \vdots& & &\ddots & \vdots\\ \frac{1}{n}& \frac{1}{n}& \frac{1}{n}& \cdots &\frac{1}{n}\end{matrix}\right].$$ Then the eigenvalues of $A_n$ are all positive and no more than $3+2\sqrt2$. To the positivity of eigenvalues, we may consider the matrix $P=\left[\begin{smallmatrix} 1 \\ -1 & 1\\ & \ddots&  \ddots \\& & & 1\\& & & -1 &1\end{smallmatrix}\right]$ and it follows that $P^\top AP=\left[\begin{smallmatrix} 1/2 \\ & 1/6& &\ast \\ & &\ddots\\ & &  & 1/(n-1)n\\ & O& & & 1/n\end{smallmatrix}\right]$. Now the problem is, the wierd bound $3+2\sqrt 2$ cannot be seen. Diagonalizing this real symmetric matrix does not seem to work because it usually needs the eigenvalues beforehand. I thought about Gerschgorin disk but unfortunately it does no help here. So I would like to ask for some ideas about this problem, and any help is appreciated.",,"['linear-algebra', 'eigenvalues-eigenvectors']"
15,How to find the Jordan canonical form of tensor products.,How to find the Jordan canonical form of tensor products.,,"Let $k$ be a field, $ A \in M_{m\times m}(k)$ be a single Jordan block with eigenvalue $a$, and $B \in M_{n\times n}(k)$ be a single Jordan block with eigenvalue $b$. $A$ and $B$ together define a linear transformation $$A\otimes B : k^{m\times n} \to k^{m\times n} $$ and my question is what is the Jordan canonical form of $A\otimes B$? (The Jordan canonical form of $A\otimes B$ exists because the characteristic polynomial of $A\otimes B$ splits and equals $(t-ab)^{mn}$) One can tickle this question directly by finding the $A\otimes B$ invariant subspaces of $k^{m\times n}$: Assume $m\geq n$, let $\{e_i\}$ be a basis of $k^m$, and $\{f_j\}$ be a basis of $k^n$, then $\{e_i\otimes f_j\}$ form a basis of $k^{m\times n}$. In the case $a=b=0$, $k^{m\times n}$ decomposes into the direct sum $$\bigoplus_{l=n-m}^{m-n} V_l$$ where $V_l=\mathrm{span}\{e_i\otimes e_j|i-j=l\}$, and $A\otimes B$ restricted to each $V_l$ is a single Jordan block. The case exactly one of $a,b$ equals zero is not much harder. But the case $ab\neq 0$ is much more difficult. Another way to do find the elementary divisors of the map  $$k[t]^{m\times n}\xrightarrow{tI-A\otimes B} k[t]^{m\times n}$$  using the formula $$d_i=\gcd(\{i\times i\text{ minor of }A\otimes B\})$$ but I can't find a good way to calculate the $\gcd$'s. Any help or hints would be appreciated. Thank you!","Let $k$ be a field, $ A \in M_{m\times m}(k)$ be a single Jordan block with eigenvalue $a$, and $B \in M_{n\times n}(k)$ be a single Jordan block with eigenvalue $b$. $A$ and $B$ together define a linear transformation $$A\otimes B : k^{m\times n} \to k^{m\times n} $$ and my question is what is the Jordan canonical form of $A\otimes B$? (The Jordan canonical form of $A\otimes B$ exists because the characteristic polynomial of $A\otimes B$ splits and equals $(t-ab)^{mn}$) One can tickle this question directly by finding the $A\otimes B$ invariant subspaces of $k^{m\times n}$: Assume $m\geq n$, let $\{e_i\}$ be a basis of $k^m$, and $\{f_j\}$ be a basis of $k^n$, then $\{e_i\otimes f_j\}$ form a basis of $k^{m\times n}$. In the case $a=b=0$, $k^{m\times n}$ decomposes into the direct sum $$\bigoplus_{l=n-m}^{m-n} V_l$$ where $V_l=\mathrm{span}\{e_i\otimes e_j|i-j=l\}$, and $A\otimes B$ restricted to each $V_l$ is a single Jordan block. The case exactly one of $a,b$ equals zero is not much harder. But the case $ab\neq 0$ is much more difficult. Another way to do find the elementary divisors of the map  $$k[t]^{m\times n}\xrightarrow{tI-A\otimes B} k[t]^{m\times n}$$  using the formula $$d_i=\gcd(\{i\times i\text{ minor of }A\otimes B\})$$ but I can't find a good way to calculate the $\gcd$'s. Any help or hints would be appreciated. Thank you!",,"['linear-algebra', 'matrices', 'modules', 'tensor-products', 'jordan-normal-form']"
16,Is there geometric Intuition for singular (2x2) matrices in space of (2x2) matrices?,Is there geometric Intuition for singular (2x2) matrices in space of (2x2) matrices?,,"Consider the vector space of 2x2 matrices $M_{2x2}$ over the reals. There's a clear isomorphism from the space to $\mathbb{R}^4$. Take a singular matrix in this space, say $\left[\begin{array}{ccc} a & 0 \\ 0 & 0 \end{array}\right]$, $a \in \mathbb{R}$. Taking the set of all matrices of this form clearly forms a subspace, which can be thought of as a line in $\mathbb{R}^4$, yet the set of all singular matrices is not a subspace. My question is thus: in general, do sets of singular matrices of certain forms always have nice geometric intuitions like this? Furthermore, in this space of matrices, can we always form subspaces of singular matrices, and if so, are they always one or two dimensional? Clearly $\left[\begin{array}{ccc} a & b \\ 0 & 0 \end{array}\right]$, $a, b \in \mathbb{R}$ is a two-dimensional subspace consisting entirely of singular matrices in $M_{2x2}$, which can be interpreted as plane in $\mathbb{R}^4$, but $\left[\begin{array}{ccc} a & b \\ c & 0 \end{array}\right]$, $a, b, c \in \mathbb{R}$ is $not$ a subspace of singular matrices. Is there a general theory for subspaces of nxn singular matrices of certain forms in $M_{nxn}$?","Consider the vector space of 2x2 matrices $M_{2x2}$ over the reals. There's a clear isomorphism from the space to $\mathbb{R}^4$. Take a singular matrix in this space, say $\left[\begin{array}{ccc} a & 0 \\ 0 & 0 \end{array}\right]$, $a \in \mathbb{R}$. Taking the set of all matrices of this form clearly forms a subspace, which can be thought of as a line in $\mathbb{R}^4$, yet the set of all singular matrices is not a subspace. My question is thus: in general, do sets of singular matrices of certain forms always have nice geometric intuitions like this? Furthermore, in this space of matrices, can we always form subspaces of singular matrices, and if so, are they always one or two dimensional? Clearly $\left[\begin{array}{ccc} a & b \\ 0 & 0 \end{array}\right]$, $a, b \in \mathbb{R}$ is a two-dimensional subspace consisting entirely of singular matrices in $M_{2x2}$, which can be interpreted as plane in $\mathbb{R}^4$, but $\left[\begin{array}{ccc} a & b \\ c & 0 \end{array}\right]$, $a, b, c \in \mathbb{R}$ is $not$ a subspace of singular matrices. Is there a general theory for subspaces of nxn singular matrices of certain forms in $M_{nxn}$?",,"['linear-algebra', 'matrices', 'vector-spaces', 'intuition']"
17,Can the theory of determinants be derived using the definition by row operations?,Can the theory of determinants be derived using the definition by row operations?,,"This year I'm teaching an elementary course on linear algebra for physics students. Because of that I have been researching the different ways of presenting the definition of the determinant (one of the hardest topics for such a course, in my opinion). In the lecture notes by Terry Loring a definition using elementary row operations is given http://www.math.unm.edu/~loring/links/linear_s06/det_def.pdf I like this definition since seems to me very algorithmic, and uses an idea which is familiar to the students. Compare this with the other more popular definitions which are The explicit formula using the sign of permutations The recursive formula using the Laplace development The axiomatic one as the unique multilinear alternate form by rows (or by columns) that takes the value 1 at the identity matrix. Of course, this definition by row elimination seems very close to this last axiomatic definition. However my question is, do you know if there is some way of deriving the whole theory of determinants from the definition based on row elimination? Indeed, it even seems hard to prove directly that the definition is correct (non ambiguous). Do you know some book using this approach?","This year I'm teaching an elementary course on linear algebra for physics students. Because of that I have been researching the different ways of presenting the definition of the determinant (one of the hardest topics for such a course, in my opinion). In the lecture notes by Terry Loring a definition using elementary row operations is given http://www.math.unm.edu/~loring/links/linear_s06/det_def.pdf I like this definition since seems to me very algorithmic, and uses an idea which is familiar to the students. Compare this with the other more popular definitions which are The explicit formula using the sign of permutations The recursive formula using the Laplace development The axiomatic one as the unique multilinear alternate form by rows (or by columns) that takes the value 1 at the identity matrix. Of course, this definition by row elimination seems very close to this last axiomatic definition. However my question is, do you know if there is some way of deriving the whole theory of determinants from the definition based on row elimination? Indeed, it even seems hard to prove directly that the definition is correct (non ambiguous). Do you know some book using this approach?",,"['linear-algebra', 'education', 'book-recommendation']"
18,Direct calculation of the canonical bundle of the complex projective space,Direct calculation of the canonical bundle of the complex projective space,,"Let $\mathrm{P}_{\mathbb{C}}^{n}$ be the n-dimensional projective space, we pick the local charts \begin{equation} U_i=\lbrace [z]\in\mathrm{P}_{\mathbb{C}}^n\mid z_i\ne0\rbrace,\quad\phi_i([z_0:\dots:z_n])=(z_0/z_i,\dots,z_{i-1}/z_i,z_{i+1}/z_i,\dots,z_n/z_i) \end{equation} then the transition maps are $(i>j)$ \begin{equation} \phi_{ij}(u_1,\dots,u_n)=(u_1/u_i,\dots,u_{j-1}/u_i,1/u_i,u_j/u_i,\dots,u_{i-1}/u_i,u_{i+1}/u_i,\dots, u_n/u_i). \end{equation} If instead we consider \begin{equation} \tilde{\phi}_{ij}(u_1,\dots,u_n)=(u_1/u_i,\dots,u_{j-1}/u_i,u_j/u_i,\dots,u_{i-1}/u_i,1/u_i,u_{i+1}/u_i,\dots, u_n/u_i) \end{equation} then Huybrechts claims that $\phi_{ij}$ is the composition of $\tilde{\phi}_{ij}$ with the   permutation $(j+1,i)$ of parity $(-1)^{i-j-1}$. Huybrechts, Daniel , Complex geometry. An introduction , Universitext. Berlin: Springer (ISBN 3-540-21290-6/pbk). xii, 309 p. (2005). ZBL1055.14001 . See page 92, Remark and Proposition 2.4.3. Is this claim true? To obtain $\phi_{ij}$ from $\tilde{\phi}_{ij}$ I need to move $1/u_i$ from the $i$-th place to the $j$-th place, which is done by $i-j$ subsequent transpositions. E.g.: Moving $7$ to the 3rd place: \begin{equation} 123456789\to 123457689\to 123475689\to 123745689\to127345689 \end{equation} is achieved in $7-3=4$ steps. The previous fact is used to prove that the determinant of the jacobian is \begin{equation} \det  \operatorname{J}\phi_{ij}=(-1)^{i-j+1}\det \operatorname{J}\tilde{\phi}_{ij}=(-1)^{i-j}(1/u_i)^{n+1}, \end{equation}   where $\det \operatorname{J}\tilde{\phi}_{ij}=-(1/u_i)^{n+1}$. \begin{equation} \lvert \operatorname{J}\tilde{\phi}_{ij}\rvert=\begin{vmatrix} u_i^{-1} &            &            &             &          &            &-u_1u_i^{-2}         &\\          &\ddots      &            &             &          &            &\vdots    &\\          &            & u_{i}^{-1} &             &          &            &-u_{j-1}u_i^{-2 }    &\\          &            &            &  u_{i}^{-1} &          &            & -u_j u_{i}^{-2}            &\\          &            &            &             &\ddots    &            & \vdots                &\\          &            &            &             &          & u_{i}^{-1} & - u_{i-1}u_{i}^{-2}\\          &            &            & 0           &          &            & \fbox{$-u_i^{-2}$} &           &\\          &            &            &             &          &            &  -u_{i+1}u_{i}^{-2}    &  u_{i}^{-1}\\          &            &            &             &          &            &                               \vdots&  &            &\ddots      &\\          &            &            &             &          &            &  -u_nu_i^{-2}                             &        &            &            &u_{i}^{-1}\\ \end{vmatrix}=-(u_i)^{-(n+1)}. \end{equation} If instead I calculate $\det \operatorname{J}\phi_{ij}$ \begin{equation} \operatorname{J}\phi_{ij}=\begin{bmatrix} u_i^{-1} &            &            &             &          &            &-u_1u_i^{-2}         &\\          &\ddots      &            &             &          &            &\vdots    &\\          &            & u_{i}^{-1} &             &          &            &-u_{j-1}u_i^{-2 }    &\\          &            &            & 0           &          &            & \fbox{$-u_i^{-2}$} &           &\\          &            &            &  u_{i}^{-1} &          &            & -u_j u_{i}^{-2}            &\\          &            &            &             &\ddots    &            & \vdots                &\\          &            &            &             &          & u_{i}^{-1} & - u_{i-1}u_{i}^{-2}\\          &            &            &             &          &            &  -u_{i+1}u_{i}^{-2}    &  u_{i}^{-1}\\          &            &            &             &          &            &                               \vdots&  &            &\ddots      &\\          &            &            &             &          &            &  -u_nu_i^{-2}                             &        &            &            &u_{i}^{-1}\\ \end{bmatrix} \end{equation} I expand with respect to the row of the $(j,i)$-element, which yields \begin{equation} \det \operatorname{J}\phi_{ij}=(-1)^{i+j}(-u_i^{-2})(u_i^{-1})^{n-1}=(-1)^{i-j+1}(u_i)^{-(n+1)}. \end{equation} This differs from Huybrechts' result by a factor $(-1)$. Moreover the jacobian of $\phi_{ij}$ should be related to that of $\tilde{\phi}_{ij}$ by moving the $j$-th row below the $i$-th one (i.e. below $-u_{i-1}u_i^{-2}$) which requires $i-j$ switches, consistently with what I said before. So I am probably making some mistakes in both the computation of the parity of the permutation and that of the determinant, which should be embarrassingly easy, but nonetheless I can't see where. I'm so concerned about this $(-1)^{i-j}$ because it is interpreted as the Čech coboundary of $\lbrace U_i,(-1)^i\rbrace$ so that the cocycle of $K_{\mathrm{P}_{\mathbb{C}}^{n}}$ and that $\mathcal{O}_{\mathrm{P}_{\mathbb{C}}^{n}}(-n-1)$ are equal in Čech cohmology. Can someone help?","Let $\mathrm{P}_{\mathbb{C}}^{n}$ be the n-dimensional projective space, we pick the local charts \begin{equation} U_i=\lbrace [z]\in\mathrm{P}_{\mathbb{C}}^n\mid z_i\ne0\rbrace,\quad\phi_i([z_0:\dots:z_n])=(z_0/z_i,\dots,z_{i-1}/z_i,z_{i+1}/z_i,\dots,z_n/z_i) \end{equation} then the transition maps are $(i>j)$ \begin{equation} \phi_{ij}(u_1,\dots,u_n)=(u_1/u_i,\dots,u_{j-1}/u_i,1/u_i,u_j/u_i,\dots,u_{i-1}/u_i,u_{i+1}/u_i,\dots, u_n/u_i). \end{equation} If instead we consider \begin{equation} \tilde{\phi}_{ij}(u_1,\dots,u_n)=(u_1/u_i,\dots,u_{j-1}/u_i,u_j/u_i,\dots,u_{i-1}/u_i,1/u_i,u_{i+1}/u_i,\dots, u_n/u_i) \end{equation} then Huybrechts claims that $\phi_{ij}$ is the composition of $\tilde{\phi}_{ij}$ with the   permutation $(j+1,i)$ of parity $(-1)^{i-j-1}$. Huybrechts, Daniel , Complex geometry. An introduction , Universitext. Berlin: Springer (ISBN 3-540-21290-6/pbk). xii, 309 p. (2005). ZBL1055.14001 . See page 92, Remark and Proposition 2.4.3. Is this claim true? To obtain $\phi_{ij}$ from $\tilde{\phi}_{ij}$ I need to move $1/u_i$ from the $i$-th place to the $j$-th place, which is done by $i-j$ subsequent transpositions. E.g.: Moving $7$ to the 3rd place: \begin{equation} 123456789\to 123457689\to 123475689\to 123745689\to127345689 \end{equation} is achieved in $7-3=4$ steps. The previous fact is used to prove that the determinant of the jacobian is \begin{equation} \det  \operatorname{J}\phi_{ij}=(-1)^{i-j+1}\det \operatorname{J}\tilde{\phi}_{ij}=(-1)^{i-j}(1/u_i)^{n+1}, \end{equation}   where $\det \operatorname{J}\tilde{\phi}_{ij}=-(1/u_i)^{n+1}$. \begin{equation} \lvert \operatorname{J}\tilde{\phi}_{ij}\rvert=\begin{vmatrix} u_i^{-1} &            &            &             &          &            &-u_1u_i^{-2}         &\\          &\ddots      &            &             &          &            &\vdots    &\\          &            & u_{i}^{-1} &             &          &            &-u_{j-1}u_i^{-2 }    &\\          &            &            &  u_{i}^{-1} &          &            & -u_j u_{i}^{-2}            &\\          &            &            &             &\ddots    &            & \vdots                &\\          &            &            &             &          & u_{i}^{-1} & - u_{i-1}u_{i}^{-2}\\          &            &            & 0           &          &            & \fbox{$-u_i^{-2}$} &           &\\          &            &            &             &          &            &  -u_{i+1}u_{i}^{-2}    &  u_{i}^{-1}\\          &            &            &             &          &            &                               \vdots&  &            &\ddots      &\\          &            &            &             &          &            &  -u_nu_i^{-2}                             &        &            &            &u_{i}^{-1}\\ \end{vmatrix}=-(u_i)^{-(n+1)}. \end{equation} If instead I calculate $\det \operatorname{J}\phi_{ij}$ \begin{equation} \operatorname{J}\phi_{ij}=\begin{bmatrix} u_i^{-1} &            &            &             &          &            &-u_1u_i^{-2}         &\\          &\ddots      &            &             &          &            &\vdots    &\\          &            & u_{i}^{-1} &             &          &            &-u_{j-1}u_i^{-2 }    &\\          &            &            & 0           &          &            & \fbox{$-u_i^{-2}$} &           &\\          &            &            &  u_{i}^{-1} &          &            & -u_j u_{i}^{-2}            &\\          &            &            &             &\ddots    &            & \vdots                &\\          &            &            &             &          & u_{i}^{-1} & - u_{i-1}u_{i}^{-2}\\          &            &            &             &          &            &  -u_{i+1}u_{i}^{-2}    &  u_{i}^{-1}\\          &            &            &             &          &            &                               \vdots&  &            &\ddots      &\\          &            &            &             &          &            &  -u_nu_i^{-2}                             &        &            &            &u_{i}^{-1}\\ \end{bmatrix} \end{equation} I expand with respect to the row of the $(j,i)$-element, which yields \begin{equation} \det \operatorname{J}\phi_{ij}=(-1)^{i+j}(-u_i^{-2})(u_i^{-1})^{n-1}=(-1)^{i-j+1}(u_i)^{-(n+1)}. \end{equation} This differs from Huybrechts' result by a factor $(-1)$. Moreover the jacobian of $\phi_{ij}$ should be related to that of $\tilde{\phi}_{ij}$ by moving the $j$-th row below the $i$-th one (i.e. below $-u_{i-1}u_i^{-2}$) which requires $i-j$ switches, consistently with what I said before. So I am probably making some mistakes in both the computation of the parity of the permutation and that of the determinant, which should be embarrassingly easy, but nonetheless I can't see where. I'm so concerned about this $(-1)^{i-j}$ because it is interpreted as the Čech coboundary of $\lbrace U_i,(-1)^i\rbrace$ so that the cocycle of $K_{\mathrm{P}_{\mathbb{C}}^{n}}$ and that $\mathcal{O}_{\mathrm{P}_{\mathbb{C}}^{n}}(-n-1)$ are equal in Čech cohmology. Can someone help?",,"['linear-algebra', 'proof-verification', 'algebraic-geometry', 'proof-explanation', 'complex-geometry']"
19,"If $M$ maps all probability vectors on a subspace to some probability vectors, is $M$ the restriction of a column stochastic matrix?","If  maps all probability vectors on a subspace to some probability vectors, is  the restriction of a column stochastic matrix?",M M,"For $n \ge 1$, let  $$\Delta^{n-1} := \left\{ (x_1,\dots,x_{n}) \in \mathbb{R}^{n} \mid \sum_{i=1}^{n} x_i=1,~x_i \ge 0 \right\}$$  and let $\mathcal{S},~\mathcal{S}'\subset\mathbb{R}^n$ be subspaces such that $\mathcal{S} \cap \Delta^{n-1} \ne \emptyset$ and $\mathcal{S}' \cap \Delta^{n-1} \ne \emptyset$. Consider a linear map $M:\mathcal{S}\to\mathcal{S}'$ such that $M(\mathcal{S} \cap \Delta^{n-1}) \subseteq \mathcal{S}' \cap \Delta^{n-1}$. The problem is to prove whether or not there exists a linear map $T_Q:\mathbb{R}^n\to\mathbb{R}^n$  given by some column stochastic matrix ($Q_{ij}\geq0$, $\sum_iQ_{ij}=1$, in the standard basis), such that $Q v=M(v)$ for every $v\in \mathcal{S} \cap \Delta^{n-1}$. Has this extension problem been already studied?","For $n \ge 1$, let  $$\Delta^{n-1} := \left\{ (x_1,\dots,x_{n}) \in \mathbb{R}^{n} \mid \sum_{i=1}^{n} x_i=1,~x_i \ge 0 \right\}$$  and let $\mathcal{S},~\mathcal{S}'\subset\mathbb{R}^n$ be subspaces such that $\mathcal{S} \cap \Delta^{n-1} \ne \emptyset$ and $\mathcal{S}' \cap \Delta^{n-1} \ne \emptyset$. Consider a linear map $M:\mathcal{S}\to\mathcal{S}'$ such that $M(\mathcal{S} \cap \Delta^{n-1}) \subseteq \mathcal{S}' \cap \Delta^{n-1}$. The problem is to prove whether or not there exists a linear map $T_Q:\mathbb{R}^n\to\mathbb{R}^n$  given by some column stochastic matrix ($Q_{ij}\geq0$, $\sum_iQ_{ij}=1$, in the standard basis), such that $Q v=M(v)$ for every $v\in \mathcal{S} \cap \Delta^{n-1}$. Has this extension problem been already studied?",,"['linear-algebra', 'matrices', 'stochastic-processes', 'markov-chains', 'markov-process']"
20,Is the determinant of a covariance matrix always zero?,Is the determinant of a covariance matrix always zero?,,"My understanding is that given matrix X, I can find its corresponding covariance matrix by: finding the means of each column subtracting each mean from each value in its respective column and multiplying the resulting matrix by its own transpose. Let's call this matrix C. Here is what it would look like in Python: Y = X - numpy.mean(X, axis = 0)  C = numpy.dot(Y, Y.T) If I do this, I can prove mathematically (and experimentally using some simple Python code) that det(C) = 0 always. However, a colleague tells me that using the inverse of a covariance matrix is common in his field and he showed me some R code to demonstrate. > det(cov(swiss)) [1] 244394171542 I notice that R has several ways of calculating the covariance matrix that leads to different results. I also notice from Googling that some people say the covariance matrix is always singular (eg here ) whereas others say it is not. So, my question is: why the differences of opinion and what's the true answer? EDIT : I discovered that the determinant is only zero if the matrix is square. If anybody knows the proof for this or can throw some further light on the matter, I'd be grateful.","My understanding is that given matrix X, I can find its corresponding covariance matrix by: finding the means of each column subtracting each mean from each value in its respective column and multiplying the resulting matrix by its own transpose. Let's call this matrix C. Here is what it would look like in Python: Y = X - numpy.mean(X, axis = 0)  C = numpy.dot(Y, Y.T) If I do this, I can prove mathematically (and experimentally using some simple Python code) that det(C) = 0 always. However, a colleague tells me that using the inverse of a covariance matrix is common in his field and he showed me some R code to demonstrate. > det(cov(swiss)) [1] 244394171542 I notice that R has several ways of calculating the covariance matrix that leads to different results. I also notice from Googling that some people say the covariance matrix is always singular (eg here ) whereas others say it is not. So, my question is: why the differences of opinion and what's the true answer? EDIT : I discovered that the determinant is only zero if the matrix is square. If anybody knows the proof for this or can throw some further light on the matter, I'd be grateful.",,"['linear-algebra', 'statistics']"
21,Kernel of the map $\bigoplus_{k\geq 0} \Lambda^k \pi: \bigoplus_{k\geq 0} \Lambda^k V\longrightarrow \bigoplus_{l\geq 0} \Lambda^l(V/W)$?,Kernel of the map ?,\bigoplus_{k\geq 0} \Lambda^k \pi: \bigoplus_{k\geq 0} \Lambda^k V\longrightarrow \bigoplus_{l\geq 0} \Lambda^l(V/W),"Let $V$ be a vector space over a field $\mathbb K$ and $U\subseteq V$ a subspace. Let $$\pi:V\longrightarrow V/U,$$ the cannonical projection. For every $k\geq 0$ this induces a linear map $$\Lambda^k \pi:\Lambda^k V\longrightarrow \Lambda^k (V/U),$$ where $\Lambda^0 \pi=\pi$. This allows use to define a map $$\bigoplus_{k\geq 0}\Lambda^k \pi: \bigoplus_{k\geq 0} \Lambda^k V\longrightarrow \bigoplus_{k\geq 0} \Lambda^k (V/U).$$ This is the only linear make which make the diagram below to commute for every $k$: where $\jmath_k$ and $\jmath_k^\prime$ are the inclusions. What is the kernel of the map $\displaystyle\bigoplus_{k\geq 0} \Lambda^k \pi$? Well, I know that: $$\mathsf{Ker}(\bigoplus_{k\geq 0} \Lambda^k \pi)=\bigoplus_{k\geq 0} \mathsf{Ker}(\Lambda^k \pi),$$ so the problem boils down to determining $\mathsf{Ker}(\Lambda^k \pi)$. Thanks.","Let $V$ be a vector space over a field $\mathbb K$ and $U\subseteq V$ a subspace. Let $$\pi:V\longrightarrow V/U,$$ the cannonical projection. For every $k\geq 0$ this induces a linear map $$\Lambda^k \pi:\Lambda^k V\longrightarrow \Lambda^k (V/U),$$ where $\Lambda^0 \pi=\pi$. This allows use to define a map $$\bigoplus_{k\geq 0}\Lambda^k \pi: \bigoplus_{k\geq 0} \Lambda^k V\longrightarrow \bigoplus_{k\geq 0} \Lambda^k (V/U).$$ This is the only linear make which make the diagram below to commute for every $k$: where $\jmath_k$ and $\jmath_k^\prime$ are the inclusions. What is the kernel of the map $\displaystyle\bigoplus_{k\geq 0} \Lambda^k \pi$? Well, I know that: $$\mathsf{Ker}(\bigoplus_{k\geq 0} \Lambda^k \pi)=\bigoplus_{k\geq 0} \mathsf{Ker}(\Lambda^k \pi),$$ so the problem boils down to determining $\mathsf{Ker}(\Lambda^k \pi)$. Thanks.",,"['linear-algebra', 'linear-transformations']"
22,How to normalize the matrix?,How to normalize the matrix?,,"I have this matrix, \begin{equation} T=\begin{bmatrix}a&b\\-b&-a\end{bmatrix} \end{equation} To normalize it, the matrix $T$ must satisfy this condition: $T^2=1$ and $1$ is the identity matrix. To solve that I set $x^2T^2=1$ and solve for x which is $\frac{1}{\sqrt{a^2-b^2}}$.  The normalized matrix is  \begin{equation} T=\frac{1}{\sqrt{a^2-b^2}}\begin{bmatrix}a&b\\-b&-a\end{bmatrix} \end{equation} The next matrix P is a bit different, \begin{equation} P=\begin{bmatrix}c+a&b\\-b&c-a\end{bmatrix} \end{equation} Can this matrix P be normalized for the same condition $P^2=1$?","I have this matrix, \begin{equation} T=\begin{bmatrix}a&b\\-b&-a\end{bmatrix} \end{equation} To normalize it, the matrix $T$ must satisfy this condition: $T^2=1$ and $1$ is the identity matrix. To solve that I set $x^2T^2=1$ and solve for x which is $\frac{1}{\sqrt{a^2-b^2}}$.  The normalized matrix is  \begin{equation} T=\frac{1}{\sqrt{a^2-b^2}}\begin{bmatrix}a&b\\-b&-a\end{bmatrix} \end{equation} The next matrix P is a bit different, \begin{equation} P=\begin{bmatrix}c+a&b\\-b&c-a\end{bmatrix} \end{equation} Can this matrix P be normalized for the same condition $P^2=1$?",,['linear-algebra']
23,Proving symmetric matrices are diagonalizable,Proving symmetric matrices are diagonalizable,,"I'm at a loss of how to show this. I know that this is implied by the Spectral Theorem but not sure exactly how to show this in a simple, straightforward proof. I've tried laying down what I know about symmetric matrices and diagonalizable ones but I'm not too sure what to use when. My guess is I'd have to reason with regards to the eigenvalues but again I'm not sure. A matrix is symmetric if $A = A^T$. A matrix $A$ is symmetric if it can be expressed in the form $A = QDQ^{T}$. A square matrix $A$ is called diagonalizable if $\exists$ invertible P such that $P^{-1}AP$ is a diagonal matrix. Would really appreciate any help.","I'm at a loss of how to show this. I know that this is implied by the Spectral Theorem but not sure exactly how to show this in a simple, straightforward proof. I've tried laying down what I know about symmetric matrices and diagonalizable ones but I'm not too sure what to use when. My guess is I'd have to reason with regards to the eigenvalues but again I'm not sure. A matrix is symmetric if $A = A^T$. A matrix $A$ is symmetric if it can be expressed in the form $A = QDQ^{T}$. A square matrix $A$ is called diagonalizable if $\exists$ invertible P such that $P^{-1}AP$ is a diagonal matrix. Would really appreciate any help.",,"['linear-algebra', 'matrices', 'linear-transformations']"
24,Bounding Plane and Projective Occlusion of Cone and Ellipsoid Intersection,Bounding Plane and Projective Occlusion of Cone and Ellipsoid Intersection,,"We position a view cone in euclidean 3-space at origin and orient it towards $z$. Through this cone we observe an ellipsoid of arbitrary size, position and orientation which may or may not be intersecting with our cone volume. Now we wish to find out Is this ellipsoid intersecting our view cone volume at all? (i.e. can we see it?) If yes, what are the distances of the two planes orthogonal to $z$ that bound  not the entire ellipsoid, but only the intersection? (giving us a one-dimensional range along the cone ray that covers the visible part of the ellipsoid) Does the visible part of the ellipsoid projected to the far plane cover 100% of the circular intersection of the view cone and the far plane? (i.e. Does it fully occlude any space behind it from the view cone's perspective) If yes, what is the nearest distance of a plane at which 100% of that circle is covered? (giving us a depth horizon after which any information further away can be discarded because it is guaranteed to be invisible) For these questions, I'm not only interested in solutions, but also approaches and general guidance, even if it's just about the 2-space analog case. Are there direct analytical solutions or at least iterative methods that converge quickly, precise or approximate? Would it be easier to use a 4-sided view pyramid rather than a cone? My background is in linear algebra and implicit surface distance estimation. Projective geometry is very new to me so any pointers are appreciated.","We position a view cone in euclidean 3-space at origin and orient it towards $z$. Through this cone we observe an ellipsoid of arbitrary size, position and orientation which may or may not be intersecting with our cone volume. Now we wish to find out Is this ellipsoid intersecting our view cone volume at all? (i.e. can we see it?) If yes, what are the distances of the two planes orthogonal to $z$ that bound  not the entire ellipsoid, but only the intersection? (giving us a one-dimensional range along the cone ray that covers the visible part of the ellipsoid) Does the visible part of the ellipsoid projected to the far plane cover 100% of the circular intersection of the view cone and the far plane? (i.e. Does it fully occlude any space behind it from the view cone's perspective) If yes, what is the nearest distance of a plane at which 100% of that circle is covered? (giving us a depth horizon after which any information further away can be discarded because it is guaranteed to be invisible) For these questions, I'm not only interested in solutions, but also approaches and general guidance, even if it's just about the 2-space analog case. Are there direct analytical solutions or at least iterative methods that converge quickly, precise or approximate? Would it be easier to use a 4-sided view pyramid rather than a cone? My background is in linear algebra and implicit surface distance estimation. Projective geometry is very new to me so any pointers are appreciated.",,"['linear-algebra', 'euclidean-geometry', 'projective-geometry']"
25,"properties of, 3x3 matrix, determinant 1, real eigenvalues","properties of, 3x3 matrix, determinant 1, real eigenvalues",,"Let A be a 3x3 matrix with determinant 1. Suppose there exists x such that  $$\lim_{n\to\infty} (A^n x) = v$$ and $x\neq v$, $v \neq 0$. Then I can prove that $A$ must have real eigenvalues. I want to show that there exists $y$ such that $$\lim_{n\to\infty}(A^{-n}y) = v,$$ $y \neq v$. From the condition on $x$ above, we know $Av = v$, so one eigenvalue of $A$ is 1. As I mentioned above, the others are real, and so the 3 eigenvalues of $A$ are $1, \lambda, 1/\lambda$. If $\lambda > 1$, then $1/\lambda < 1,$ so let $w$ be the eigenvector corresponding to $\lambda$. $$ \lim_{n\to\infty} (A^{-n}(v + w)) = lim_{n\to\infty} v + \lambda^{-n}w = v.$$ In other words, this choice of $y$ works. However, I cannot see how to find a $y$ when all the eigenvalues of $A$ are 1. Assuming there is always a $y$ the question I am trying to solve says that $y,x,v$ are linearly independent. How can I prove this?","Let A be a 3x3 matrix with determinant 1. Suppose there exists x such that  $$\lim_{n\to\infty} (A^n x) = v$$ and $x\neq v$, $v \neq 0$. Then I can prove that $A$ must have real eigenvalues. I want to show that there exists $y$ such that $$\lim_{n\to\infty}(A^{-n}y) = v,$$ $y \neq v$. From the condition on $x$ above, we know $Av = v$, so one eigenvalue of $A$ is 1. As I mentioned above, the others are real, and so the 3 eigenvalues of $A$ are $1, \lambda, 1/\lambda$. If $\lambda > 1$, then $1/\lambda < 1,$ so let $w$ be the eigenvector corresponding to $\lambda$. $$ \lim_{n\to\infty} (A^{-n}(v + w)) = lim_{n\to\infty} v + \lambda^{-n}w = v.$$ In other words, this choice of $y$ works. However, I cannot see how to find a $y$ when all the eigenvalues of $A$ are 1. Assuming there is always a $y$ the question I am trying to solve says that $y,x,v$ are linearly independent. How can I prove this?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
26,Diagonalizability via forms,Diagonalizability via forms,,"Bored at a lecture, I began tinkering with 2-by-2 matrices and observed the following. Suppose we want to diagonalize the matrix $$\begin{pmatrix}a & b\\c & d\end{pmatrix}$$ in the most naive way possible.  We seek a matrix $$\begin{pmatrix}\alpha & \beta \\ \gamma & \delta \end{pmatrix}$$ with $\alpha\delta - \beta\gamma \ne 0$ such that  $$\frac{1}{\alpha\delta - \beta\gamma} \begin{pmatrix} \delta & -\beta \\ -\gamma & \alpha \end{pmatrix} \begin{pmatrix}a & b\\c & d\end{pmatrix} \begin{pmatrix}\alpha & \beta \\ \gamma & \delta \end{pmatrix} = \begin{pmatrix}* & 0 \\ 0 & *\end{pmatrix}.$$ Grinding through the algebra, it turns out that the off-diagonal terms are $Q(\alpha, \gamma)$ and $-Q(\beta, \delta)$, up to a scalar multiple, where $$Q(x, y) = cx^2 + (a-d)xy - by^2.$$ So to find a diagonalizing matrix, we seek solutions to $Q(x, y) = 0$. Because $Q$ is homogeneous, we are actually seeking solutions in $\mathbb{P}^1$. The condition $\alpha\delta - \beta\gamma \ne 0$ is equivalent to $$[\alpha:\gamma] \ne [\beta:\delta].$$ This viewpoint is totally new to me. For $n$-by-$n$ matrices, are there analogous homogeneous forms coming from trying to diagonalize in this way?","Bored at a lecture, I began tinkering with 2-by-2 matrices and observed the following. Suppose we want to diagonalize the matrix $$\begin{pmatrix}a & b\\c & d\end{pmatrix}$$ in the most naive way possible.  We seek a matrix $$\begin{pmatrix}\alpha & \beta \\ \gamma & \delta \end{pmatrix}$$ with $\alpha\delta - \beta\gamma \ne 0$ such that  $$\frac{1}{\alpha\delta - \beta\gamma} \begin{pmatrix} \delta & -\beta \\ -\gamma & \alpha \end{pmatrix} \begin{pmatrix}a & b\\c & d\end{pmatrix} \begin{pmatrix}\alpha & \beta \\ \gamma & \delta \end{pmatrix} = \begin{pmatrix}* & 0 \\ 0 & *\end{pmatrix}.$$ Grinding through the algebra, it turns out that the off-diagonal terms are $Q(\alpha, \gamma)$ and $-Q(\beta, \delta)$, up to a scalar multiple, where $$Q(x, y) = cx^2 + (a-d)xy - by^2.$$ So to find a diagonalizing matrix, we seek solutions to $Q(x, y) = 0$. Because $Q$ is homogeneous, we are actually seeking solutions in $\mathbb{P}^1$. The condition $\alpha\delta - \beta\gamma \ne 0$ is equivalent to $$[\alpha:\gamma] \ne [\beta:\delta].$$ This viewpoint is totally new to me. For $n$-by-$n$ matrices, are there analogous homogeneous forms coming from trying to diagonalize in this way?",,"['linear-algebra', 'matrices', 'projective-space']"
27,Conjecture: Any unit of a set of matrices under a commutative ring with identity has a solution to Ax = b and conversely,Conjecture: Any unit of a set of matrices under a commutative ring with identity has a solution to Ax = b and conversely,,"Conjecture: Given a $M(R)$ the set of matrices under a commutative ring with identity, $R$, a matrix $A$ in $M(R)$ is a unit if and only if for any vector $b$ there exists a vector $x$ such that $Ax = b$. I have conjectured the previous statement, and could prove the forward implication; If $A$ is a unit, $A^{-1}$ exists such that $AA^{-1} = I.$ For any $b$, $Ib = b$. Then $(AA^{-1})b = b$. Then $A(A^{-1}b) = b$. Now suppose for any $b$, there exists $x$ such that $Ax = b$. Take $e_1, ... e_n$, the unit vectors of $I$. then $AB = I$ exists. However, I am not able to show $AB = I$ implies $BA = I$, because the existence of $B$ such that $BA = I$ does not guarantee existence of $C$ such that $CA = I$. If C exists, the result is immediate. Is the conjecture true, or does there exist a counterexample?","Conjecture: Given a $M(R)$ the set of matrices under a commutative ring with identity, $R$, a matrix $A$ in $M(R)$ is a unit if and only if for any vector $b$ there exists a vector $x$ such that $Ax = b$. I have conjectured the previous statement, and could prove the forward implication; If $A$ is a unit, $A^{-1}$ exists such that $AA^{-1} = I.$ For any $b$, $Ib = b$. Then $(AA^{-1})b = b$. Then $A(A^{-1}b) = b$. Now suppose for any $b$, there exists $x$ such that $Ax = b$. Take $e_1, ... e_n$, the unit vectors of $I$. then $AB = I$ exists. However, I am not able to show $AB = I$ implies $BA = I$, because the existence of $B$ such that $BA = I$ does not guarantee existence of $C$ such that $CA = I$. If C exists, the result is immediate. Is the conjecture true, or does there exist a counterexample?",,"['linear-algebra', 'abstract-algebra', 'matrices']"
28,$n+1$ points in $\mathbb{R}^n$ with pairwise rational distances are linearly dependent,points in  with pairwise rational distances are linearly dependent,n+1 \mathbb{R}^n,"Let $v_0$ be the zero vector in $\mathbb{R}^n$ and let $v_1, v_2, . . . , v_{n+1}$ be vectors in $\mathbb{R}^n$ such that the Euclidean norm $|v_i − v_j|$ is rational for every $0 ≤ i, j ≤ n + 1$. Prove that $v_1, . . . , v_{n+1}$ are linearly dependent over the rationals. I was reading an ingenious proof of this result, an outline of which I present here: ""by passing to a subspace"", we may assume that $v_1,...,v_n$ are linearly independent over $\mathbb{R}$ since $v_1,...,v_{n+1}$ is a family of $n+1$ vectors in the $n$- dimmensional space $\mathbb{R}^n$ (considered over the field $\mathbb{R}$), they are linearly dependent over $\mathbb{R}$. Hence write $v_{n+1}=\sum_{k=1}^n\lambda_kv_k$. The goal is to show that $\lambda_k$ is rational. by the parrallelogram inequality, all the scalar products $<v_i,v_j>$ are rational. Hence the Grammian matrix $G$ of $v_1,..,v_n$ has all entries rational. Furthermore, since $v_1,...,v_n$ are linearly independent $G$ is invertible (well-known property of the Grammian) Let $w$ denote the column vector of size $n$ with coordinate $k$ given by $<v_{n+1},v_k>$. Let $\lambda$ denote the column vector of size $n$ with coordinate $k$ given by $\lambda_k$. Then we have $w=G\lambda$. From the above, $\lambda=G^{-1}w$ which is a product of matrices with rational entries, thus a rational matrix. done. The only step I don't understand is step 1. Why do we have the right to assume $v_1,...,v_n$ are linearly independent over $\mathbb{R}$, and which subspace are we ""passing to""?","Let $v_0$ be the zero vector in $\mathbb{R}^n$ and let $v_1, v_2, . . . , v_{n+1}$ be vectors in $\mathbb{R}^n$ such that the Euclidean norm $|v_i − v_j|$ is rational for every $0 ≤ i, j ≤ n + 1$. Prove that $v_1, . . . , v_{n+1}$ are linearly dependent over the rationals. I was reading an ingenious proof of this result, an outline of which I present here: ""by passing to a subspace"", we may assume that $v_1,...,v_n$ are linearly independent over $\mathbb{R}$ since $v_1,...,v_{n+1}$ is a family of $n+1$ vectors in the $n$- dimmensional space $\mathbb{R}^n$ (considered over the field $\mathbb{R}$), they are linearly dependent over $\mathbb{R}$. Hence write $v_{n+1}=\sum_{k=1}^n\lambda_kv_k$. The goal is to show that $\lambda_k$ is rational. by the parrallelogram inequality, all the scalar products $<v_i,v_j>$ are rational. Hence the Grammian matrix $G$ of $v_1,..,v_n$ has all entries rational. Furthermore, since $v_1,...,v_n$ are linearly independent $G$ is invertible (well-known property of the Grammian) Let $w$ denote the column vector of size $n$ with coordinate $k$ given by $<v_{n+1},v_k>$. Let $\lambda$ denote the column vector of size $n$ with coordinate $k$ given by $\lambda_k$. Then we have $w=G\lambda$. From the above, $\lambda=G^{-1}w$ which is a product of matrices with rational entries, thus a rational matrix. done. The only step I don't understand is step 1. Why do we have the right to assume $v_1,...,v_n$ are linearly independent over $\mathbb{R}$, and which subspace are we ""passing to""?",,['linear-algebra']
29,Finding the relation between identity and identical functions,Finding the relation between identity and identical functions,,"Consider the function $f(x,a,b,c,d) = (((x \oplus a) + b) \oplus c) + d$ defined on $\mathbb{Z}_ {2^n}$, where $\oplus$ denotes bitwise exclusive-OR (XOR) and $+$ denotes addition modulo $2^n$. Let's start with two definitions: $f(\cdot,a,b,c,d)$ is an identity function on $\mathbb{Z}_{2^n}$ if $\forall x \in \mathbb{Z}_{2^n}, f(x,a,b,c,d) = x$. $f(\cdot,a',b',c',d')$ is identical to $f(\cdot,a,b,c,d)$ if $\forall x \in \mathbb{Z}_{2^n}, f(x,a',b',c',d') = f(x,a,b,c,d)$. In answer to this question , it has been shown that the number of identity functions is equal to $n \times 2^{n+2}$. Furthemore, when looking in practice for several $n$, it seems that $n \times 2^{n+2}$ is also the number of identical functions for a given $(a,b,c,d) \in (\mathbb{Z}_{2^n})^4$. In fact, identity functions are the specific case where $(a,b,c,d) \in (\mathbb{Z}_{2^n})^4$ is such that $f(x,a,b,c,d) = x$. Is there a way to prove that the number of identity functions is equal to the number of identical functions? In other terms, how can we extend the demonstration for all $(a,b,c,d) \in (\mathbb{Z}_{2^n})^4$?","Consider the function $f(x,a,b,c,d) = (((x \oplus a) + b) \oplus c) + d$ defined on $\mathbb{Z}_ {2^n}$, where $\oplus$ denotes bitwise exclusive-OR (XOR) and $+$ denotes addition modulo $2^n$. Let's start with two definitions: $f(\cdot,a,b,c,d)$ is an identity function on $\mathbb{Z}_{2^n}$ if $\forall x \in \mathbb{Z}_{2^n}, f(x,a,b,c,d) = x$. $f(\cdot,a',b',c',d')$ is identical to $f(\cdot,a,b,c,d)$ if $\forall x \in \mathbb{Z}_{2^n}, f(x,a',b',c',d') = f(x,a,b,c,d)$. In answer to this question , it has been shown that the number of identity functions is equal to $n \times 2^{n+2}$. Furthemore, when looking in practice for several $n$, it seems that $n \times 2^{n+2}$ is also the number of identical functions for a given $(a,b,c,d) \in (\mathbb{Z}_{2^n})^4$. In fact, identity functions are the specific case where $(a,b,c,d) \in (\mathbb{Z}_{2^n})^4$ is such that $f(x,a,b,c,d) = x$. Is there a way to prove that the number of identity functions is equal to the number of identical functions? In other terms, how can we extend the demonstration for all $(a,b,c,d) \in (\mathbb{Z}_{2^n})^4$?",,"['linear-algebra', 'finite-groups', 'proof-writing', 'modular-arithmetic', 'binary']"
30,Linearly independent subset of polynomials,Linearly independent subset of polynomials,,"Let $S$ be a set of non-zero polynomials over a field $F$. If no two elements of $S$ have the same degree, show that $S$ is an independent subset of $F[x]$. I tried to prove the statement by induction on the number of elements of any finite subset of $S$.  Please let me know if my proof is correct or not. Besides, is there other method to prove the statement? Thanks! Let n be the number of elements in a finite subset of S. Base case $n = 1$ $\alpha f = 0 \implies \alpha = 0$ because $f \ne 0$. Assume the statement is true for $n=k$. When $n = k+1$, suppose $$\sigma= \alpha_1 f_1+....+\alpha_k f_k+ \alpha_{k+1} f_{k+1}=0$$ WLOG, we may assume that $f_{k+1}$ has the highest degree among all $f_i${i=1,2,...,k+1}. Let $m= \deg f_{k+1}$ Denote the leading coefficient of $f_{k+1}$ by $\lambda$. The coefficient of $x^{m}$ will be zero for all other $f_i$. Note that the coefficient of $x^{m}$ in $\sigma$ will then be $$\alpha_{k+1}\lambda=0$$ By the definition of leading coefficient, $$\lambda  \ne 0$$. So $$\alpha_{k+1}=0$$ This implies $$\alpha_1 f_1+....+\alpha_k f_k=0$$ By induction assumption, $$\alpha_1 = \alpha_2=...=\alpha_{k}=0$$ Hence, $f_1,f_2,....,f_{k+1}$ are linearly independent. By the principle of M.I., the statement is true for all natural numbers n. Best regards, Michael","Let $S$ be a set of non-zero polynomials over a field $F$. If no two elements of $S$ have the same degree, show that $S$ is an independent subset of $F[x]$. I tried to prove the statement by induction on the number of elements of any finite subset of $S$.  Please let me know if my proof is correct or not. Besides, is there other method to prove the statement? Thanks! Let n be the number of elements in a finite subset of S. Base case $n = 1$ $\alpha f = 0 \implies \alpha = 0$ because $f \ne 0$. Assume the statement is true for $n=k$. When $n = k+1$, suppose $$\sigma= \alpha_1 f_1+....+\alpha_k f_k+ \alpha_{k+1} f_{k+1}=0$$ WLOG, we may assume that $f_{k+1}$ has the highest degree among all $f_i${i=1,2,...,k+1}. Let $m= \deg f_{k+1}$ Denote the leading coefficient of $f_{k+1}$ by $\lambda$. The coefficient of $x^{m}$ will be zero for all other $f_i$. Note that the coefficient of $x^{m}$ in $\sigma$ will then be $$\alpha_{k+1}\lambda=0$$ By the definition of leading coefficient, $$\lambda  \ne 0$$. So $$\alpha_{k+1}=0$$ This implies $$\alpha_1 f_1+....+\alpha_k f_k=0$$ By induction assumption, $$\alpha_1 = \alpha_2=...=\alpha_{k}=0$$ Hence, $f_1,f_2,....,f_{k+1}$ are linearly independent. By the principle of M.I., the statement is true for all natural numbers n. Best regards, Michael",,['linear-algebra']
31,Upper Bound for Vertices of Intersection of Subspace and Simplex,Upper Bound for Vertices of Intersection of Subspace and Simplex,,"Let $S$ be a simplex in $\mathbb R^m$. It can be expressed as the convex hull of the columns of some affinely independent $m \times m + 1$ matrix $A$: $$S=\left\{A\vec\alpha: \vec\alpha\in\mathbb R^{m+1},\alpha_i\ge0,\sum\alpha_i=1\right\}$$ Or alternately by the intersection of a set of $m + 1$ half-spaces defined by the rows of a matrix $C$, where each row $\vec c$ gives $\vec c^T \vec x \le 1$. (This requires that the origin be in the interior of the simplex, but that can be done without loss of generality). $$S=\left\{\vec x\in\mathbb R^{m}:C\vec x\leqq 1\right\}$$ Let $Q$ be a subspace of dimension $n<m$, expressed  in terms of a linearly independent basis: $$Q=span\left\{\vec v_1,\vec v_2,...,\vec v_n\right\}=\left\{V\vec y:\vec y\in\mathbb R^n\right\}$$ If the intersection $Q\cap S$ contains a point in the interior of $S$, then it can be expressed as a convex hull in $Q$. From low dimensional cases, it is clear that there are some restrictions on its geometry. In particular, I am interested in the number of vertices. I believe the best lower bound is $n+1$; that is the minimum for any convex hull in $n$ dimensions, and one can be obtained in any dimension by construction. The the upper bound is much less clear. At $m=3, n=2$ (a plane intersecting a tetrahedron), the lower bound is 3 and the upper bound is 4, since the planar cross sections of a tetrahedron are either triangles or quadrilaterals. However, it's not clear to me how this argument generalizes to higher dimensions. The best general upper bound I could come up with is $\binom{m+1}{n}$, which follows from this argument: The set of $\vec y$ coordinates has the same topology as the hull in $\mathbb R^m$, so we can look at the hull in that space, again assuming that the origin is an interior point. $$Q\cap S = \left\{V\vec y: \vec y \in \mathbb R^n, CV\vec y\leqq 1\right\}$$ $CV$ is an $m+1\times n$ matrix, so this equation describes the intersection of $m+1$ half spaces in $\mathbb R^n$. Some of them could be redundant, but we can assume that none are to obtain an upper bound on vertices. Since this hull exists in $n$ dimensions, each vertex lies at the intersection of the boundaries of at least $n$ half spaces in order to be fully determined. If it is in the intersection of more than $n$ half spaces, we can find a subset of exactly $n$ whose intersection is the vertex. Thus if we consider every combination of $n$ half spaces, we will obtain all the vertices, which gives an upper bound of $\binom{m + 1}{n}$ vertices. It seems like there ought to exist a better upper bound. This one gives more vertices than can possibly be attained for the low dimensional cases $(m=2, n=1)$, $(m=3, n=1)$, and($m=3, n=2)$. Any input on how best to improve this bound (or show it can't be inproved) would be much appreciated. A more general result would be preferred, but I'm primarily interested in the case $m>>n>>1$.","Let $S$ be a simplex in $\mathbb R^m$. It can be expressed as the convex hull of the columns of some affinely independent $m \times m + 1$ matrix $A$: $$S=\left\{A\vec\alpha: \vec\alpha\in\mathbb R^{m+1},\alpha_i\ge0,\sum\alpha_i=1\right\}$$ Or alternately by the intersection of a set of $m + 1$ half-spaces defined by the rows of a matrix $C$, where each row $\vec c$ gives $\vec c^T \vec x \le 1$. (This requires that the origin be in the interior of the simplex, but that can be done without loss of generality). $$S=\left\{\vec x\in\mathbb R^{m}:C\vec x\leqq 1\right\}$$ Let $Q$ be a subspace of dimension $n<m$, expressed  in terms of a linearly independent basis: $$Q=span\left\{\vec v_1,\vec v_2,...,\vec v_n\right\}=\left\{V\vec y:\vec y\in\mathbb R^n\right\}$$ If the intersection $Q\cap S$ contains a point in the interior of $S$, then it can be expressed as a convex hull in $Q$. From low dimensional cases, it is clear that there are some restrictions on its geometry. In particular, I am interested in the number of vertices. I believe the best lower bound is $n+1$; that is the minimum for any convex hull in $n$ dimensions, and one can be obtained in any dimension by construction. The the upper bound is much less clear. At $m=3, n=2$ (a plane intersecting a tetrahedron), the lower bound is 3 and the upper bound is 4, since the planar cross sections of a tetrahedron are either triangles or quadrilaterals. However, it's not clear to me how this argument generalizes to higher dimensions. The best general upper bound I could come up with is $\binom{m+1}{n}$, which follows from this argument: The set of $\vec y$ coordinates has the same topology as the hull in $\mathbb R^m$, so we can look at the hull in that space, again assuming that the origin is an interior point. $$Q\cap S = \left\{V\vec y: \vec y \in \mathbb R^n, CV\vec y\leqq 1\right\}$$ $CV$ is an $m+1\times n$ matrix, so this equation describes the intersection of $m+1$ half spaces in $\mathbb R^n$. Some of them could be redundant, but we can assume that none are to obtain an upper bound on vertices. Since this hull exists in $n$ dimensions, each vertex lies at the intersection of the boundaries of at least $n$ half spaces in order to be fully determined. If it is in the intersection of more than $n$ half spaces, we can find a subset of exactly $n$ whose intersection is the vertex. Thus if we consider every combination of $n$ half spaces, we will obtain all the vertices, which gives an upper bound of $\binom{m + 1}{n}$ vertices. It seems like there ought to exist a better upper bound. This one gives more vertices than can possibly be attained for the low dimensional cases $(m=2, n=1)$, $(m=3, n=1)$, and($m=3, n=2)$. Any input on how best to improve this bound (or show it can't be inproved) would be much appreciated. A more general result would be preferred, but I'm primarily interested in the case $m>>n>>1$.",,"['linear-algebra', 'combinatorial-geometry', 'simplex']"
32,Why do i have to transpose the coefficients of a system of linear transformations to get its matrix form?,Why do i have to transpose the coefficients of a system of linear transformations to get its matrix form?,,"I have been reading a book about linear algebra that is very popular in my country and  in the chapter about the matrix of a linear transformation he states the following: $``$ Be $\ U\ $ and $\ V\ $ two vector spaces, over $\ {\rm I\!R} \ $, of dimensions $n$ and $m$, respectively. Consider the linear transformation $\ F:U \rightarrow V \ $. Given the basis $\ B=\{u_1, ...,u_n\}\ $ of $\ U \ $ and the basis $\ C = \{v_1,...,v_m\} \ $  of $\ V \ $ ,  therefore each one of the vectors $\ F(u_1),...,F(u_n)\ $ are in $\ V \ $ and consequently are linear combinations of the basis $\ C \ $: $$F(u_1) = \alpha_{11}v_1 + \alpha_{21}v_2 + ... + \alpha_{m1}v_m$$ $$F(u_2) = \alpha_{12}v_1 + \alpha_{22}v_2 + ... + \alpha_{m2}v_m$$ $$\vdots$$ $$F(u_n) = \alpha_{1n}v_1 + \alpha_{2n}v_2 + ... + \alpha_{mn}v_m$$ $Definition \ \ 4$ - A matrix $\ m\times n \ $ over $\ {\rm I\!R} \ $. $$ \qquad \quad \begin{pmatrix}     \alpha_{11} & \alpha_{12} & \dots  & \alpha_{1n} \\     \alpha_{21} & \alpha_{22}  & \dots  & \alpha_{2n} \\     \vdots & \vdots  & \ddots & \vdots \\     \alpha_{m1} & \alpha_{m2}  & \dots  & \alpha_{mn} \end{pmatrix}=(\alpha_{ij})$$ that is obtained from the previous considerations is called matrix of $F$ in relation to the basis $B$ and $C$ . $""$ So why is it the transpose of the matrix of the coefficients of the linear relations instead of the usual matrix ?","I have been reading a book about linear algebra that is very popular in my country and  in the chapter about the matrix of a linear transformation he states the following: $``$ Be $\ U\ $ and $\ V\ $ two vector spaces, over $\ {\rm I\!R} \ $, of dimensions $n$ and $m$, respectively. Consider the linear transformation $\ F:U \rightarrow V \ $. Given the basis $\ B=\{u_1, ...,u_n\}\ $ of $\ U \ $ and the basis $\ C = \{v_1,...,v_m\} \ $  of $\ V \ $ ,  therefore each one of the vectors $\ F(u_1),...,F(u_n)\ $ are in $\ V \ $ and consequently are linear combinations of the basis $\ C \ $: $$F(u_1) = \alpha_{11}v_1 + \alpha_{21}v_2 + ... + \alpha_{m1}v_m$$ $$F(u_2) = \alpha_{12}v_1 + \alpha_{22}v_2 + ... + \alpha_{m2}v_m$$ $$\vdots$$ $$F(u_n) = \alpha_{1n}v_1 + \alpha_{2n}v_2 + ... + \alpha_{mn}v_m$$ $Definition \ \ 4$ - A matrix $\ m\times n \ $ over $\ {\rm I\!R} \ $. $$ \qquad \quad \begin{pmatrix}     \alpha_{11} & \alpha_{12} & \dots  & \alpha_{1n} \\     \alpha_{21} & \alpha_{22}  & \dots  & \alpha_{2n} \\     \vdots & \vdots  & \ddots & \vdots \\     \alpha_{m1} & \alpha_{m2}  & \dots  & \alpha_{mn} \end{pmatrix}=(\alpha_{ij})$$ that is obtained from the previous considerations is called matrix of $F$ in relation to the basis $B$ and $C$ . $""$ So why is it the transpose of the matrix of the coefficients of the linear relations instead of the usual matrix ?",,"['linear-algebra', 'matrices', 'linear-transformations']"
33,"If $e^\lambda$ is an eigenvalue of $e^A$, then $\lambda$ is an eigenvalue of $A$","If  is an eigenvalue of , then  is an eigenvalue of",e^\lambda e^A \lambda A,"Let $A$ be an $n\times n$ matrix. I want to show that if $e^\lambda$ is an eigenvalue of $e^A$, then $\lambda$ is an eigenvalue of $A$. I know the converse is true, but I'm not sure how to go the other way. Our assumption seems to be equating power series to each other, which seem more difficult to work with. Thanks!","Let $A$ be an $n\times n$ matrix. I want to show that if $e^\lambda$ is an eigenvalue of $e^A$, then $\lambda$ is an eigenvalue of $A$. I know the converse is true, but I'm not sure how to go the other way. Our assumption seems to be equating power series to each other, which seem more difficult to work with. Thanks!",,"['linear-algebra', 'matrix-exponential']"
34,Staircase spectrum: is there a known solution for this problem?,Staircase spectrum: is there a known solution for this problem?,,"•   Let $A$ be a (real) dense (non-sparse) matrix with size $rp\times M$  ($M < rp$) with orthonormal columns (i.e. $A^TA = I_M$). •   Similarly, let $B$ be a dense matrix with size $rq\times N$  ($N < rq$) with orthonormal columns. •   Let $V=\bigoplus_{i=1}^qA.$ Equivalently, $V=I_q\bigotimes A,$ where $\bigoplus$ denotes the direct sum, and $\bigotimes$ denotes the Kronecker product. •   Let $W=\bigoplus_{j=1}^pB.$ Equivalently, $W=I_p\bigotimes B.$ •   Let $R=V^TW,$ and therefore,  $$ R=\big(\bigoplus_{i=1}^qA^T\big)\big(\bigoplus_{j=1}^pB\big) $$ $$ =\big(I_q\bigotimes A^T\big)\big(I_p\bigotimes B\big)  $$ •   Assume:  $rpq > qM + pN$ (so that the augmented matrix $\left[\begin{array}{c|c}V & W\end{array}\right]$ is a tall matrix). •   Note:  $p\neq q$, $p>1$ and $q>1$. Prove that the singular values of $R$ come in $n$-tuples, with $$ n=\text{GCD}(p,q), $$ where $\text{GCD}(p,q)$ is the greatest common divisor of $p$ and $q$. In other words, prove that a singular value of $R$ has a multiplicity of $\text{GCD}(p,q)$. Note: I am ONLY interested in proving that the largest singular value of $R$ is distinct when $p$ and $q$ are relatively prime ($\text{GCD}(p,q) = 1$). However, I think giving the larger picture of the behavior of the singular values of $R$ is far more interesting, and probably even helpful in proving the specific case I'm interested in. Please refer to the Matlab code below to verify that $n=\text{GCD}(p,q)$. Matlab code: clear all; close all;  r = 3; % integer >= 1  p = 12; q = 16; %17 % for the singular values of R to be distinct, p and q must be relatively prime integers and larger than 1 (i.e. GCD(p,q) = 1 and p > 1 and q > 1)   M = 8; % # of columns of A N = 14; % # of columns of B. M and N MUST satisfy 1 < M < rp, 1 < N < rq AND they must satisfy assumption that r*p*q/(q*M+p*N) > 1.  [A,~] = qr(rand(r*p,M),0); % size = rp X M (M < rp, A'A = I) % Note: you can orthonormalize (e.g. via Gram Schmidt or SVD) any arbitrary                         % (full rank) data matrix other than noise, I'm only using                        % noise as a data matrix because it's conevniently easy to generate. [B,~] = qr(rand(r*q,N),0); % size = rq X N (N < rq, B'B = I)  V = kron(eye(q),A); % size = q(rp) X qM W = kron(eye(p),B); % size = p(rq) X pN  R = V'*W; % i.e. R = direct sum (over q) of (A') X direct sum (over p) of (B)  % assumption r*p*q/(q*M+p*N) % check is greater than 1 % This assumption is required for the augmented matrix [V W] to be tall.  gcd(p,q) % check for relative primeness of p and q. % This assumption is required for the singular values of R to be distinct.  figure;stem(svd(R));    title 'note how the singular values of R come in GCD(p,q)-tuples'","•   Let $A$ be a (real) dense (non-sparse) matrix with size $rp\times M$  ($M < rp$) with orthonormal columns (i.e. $A^TA = I_M$). •   Similarly, let $B$ be a dense matrix with size $rq\times N$  ($N < rq$) with orthonormal columns. •   Let $V=\bigoplus_{i=1}^qA.$ Equivalently, $V=I_q\bigotimes A,$ where $\bigoplus$ denotes the direct sum, and $\bigotimes$ denotes the Kronecker product. •   Let $W=\bigoplus_{j=1}^pB.$ Equivalently, $W=I_p\bigotimes B.$ •   Let $R=V^TW,$ and therefore,  $$ R=\big(\bigoplus_{i=1}^qA^T\big)\big(\bigoplus_{j=1}^pB\big) $$ $$ =\big(I_q\bigotimes A^T\big)\big(I_p\bigotimes B\big)  $$ •   Assume:  $rpq > qM + pN$ (so that the augmented matrix $\left[\begin{array}{c|c}V & W\end{array}\right]$ is a tall matrix). •   Note:  $p\neq q$, $p>1$ and $q>1$. Prove that the singular values of $R$ come in $n$-tuples, with $$ n=\text{GCD}(p,q), $$ where $\text{GCD}(p,q)$ is the greatest common divisor of $p$ and $q$. In other words, prove that a singular value of $R$ has a multiplicity of $\text{GCD}(p,q)$. Note: I am ONLY interested in proving that the largest singular value of $R$ is distinct when $p$ and $q$ are relatively prime ($\text{GCD}(p,q) = 1$). However, I think giving the larger picture of the behavior of the singular values of $R$ is far more interesting, and probably even helpful in proving the specific case I'm interested in. Please refer to the Matlab code below to verify that $n=\text{GCD}(p,q)$. Matlab code: clear all; close all;  r = 3; % integer >= 1  p = 12; q = 16; %17 % for the singular values of R to be distinct, p and q must be relatively prime integers and larger than 1 (i.e. GCD(p,q) = 1 and p > 1 and q > 1)   M = 8; % # of columns of A N = 14; % # of columns of B. M and N MUST satisfy 1 < M < rp, 1 < N < rq AND they must satisfy assumption that r*p*q/(q*M+p*N) > 1.  [A,~] = qr(rand(r*p,M),0); % size = rp X M (M < rp, A'A = I) % Note: you can orthonormalize (e.g. via Gram Schmidt or SVD) any arbitrary                         % (full rank) data matrix other than noise, I'm only using                        % noise as a data matrix because it's conevniently easy to generate. [B,~] = qr(rand(r*q,N),0); % size = rq X N (N < rq, B'B = I)  V = kron(eye(q),A); % size = q(rp) X qM W = kron(eye(p),B); % size = p(rq) X pN  R = V'*W; % i.e. R = direct sum (over q) of (A') X direct sum (over p) of (B)  % assumption r*p*q/(q*M+p*N) % check is greater than 1 % This assumption is required for the augmented matrix [V W] to be tall.  gcd(p,q) % check for relative primeness of p and q. % This assumption is required for the singular values of R to be distinct.  figure;stem(svd(R));    title 'note how the singular values of R come in GCD(p,q)-tuples'",,"['linear-algebra', 'matrices']"
35,Non-equivalent norms on finite dimensional vector spaces over a non-complete field,Non-equivalent norms on finite dimensional vector spaces over a non-complete field,,"It is widely known that on a finite-dimensional vector space over a complete field, every norm is equivalent. However, I'm trying (and failing) to find a counterexample over a field which is not complete. My first try was to treat $\mathbb{Q}$ as a vector space over itself and find two non-equivalent norms. As for Ostrowski's theorem we know that, for example, the absolute value and the 2-adic norm are not equivalent. However, I noticed that the $p$-adic norm, though being a norm on the field $\mathbb{Q}$, it is not a norm on the vector space $(\mathbb{Q},|\cdot |)$. I have tried to find such norms in $\mathbb{Q}^2$ and other (not complete) fields to no avail. I'm content with any example, but it would be great if it's accompanied by the proof of their non-equivalence.","It is widely known that on a finite-dimensional vector space over a complete field, every norm is equivalent. However, I'm trying (and failing) to find a counterexample over a field which is not complete. My first try was to treat $\mathbb{Q}$ as a vector space over itself and find two non-equivalent norms. As for Ostrowski's theorem we know that, for example, the absolute value and the 2-adic norm are not equivalent. However, I noticed that the $p$-adic norm, though being a norm on the field $\mathbb{Q}$, it is not a norm on the vector space $(\mathbb{Q},|\cdot |)$. I have tried to find such norms in $\mathbb{Q}^2$ and other (not complete) fields to no avail. I'm content with any example, but it would be great if it's accompanied by the proof of their non-equivalence.",,"['linear-algebra', 'abstract-algebra', 'normed-spaces', 'complete-spaces']"
36,Dimension of an object,Dimension of an object,,"I know the dimension of a straight line, a circle is 1. But how can we prove it by using vector space? The dimension a vector space is the number of elements in the basis.","I know the dimension of a straight line, a circle is 1. But how can we prove it by using vector space? The dimension a vector space is the number of elements in the basis.",,['linear-algebra']
37,Kernels of powers of linear transformation,Kernels of powers of linear transformation,,"Suppose $T:V\to V$ is a linear transformation and $dim(V)=n$. It is well known that there is an integer $m$, where $0\le m\le n$ such that $$\{\textbf{0}\}=K(T^0)\subsetneq K(T^1)\subsetneq K(T^2) \subsetneq \cdots \subsetneq K(T^m)=K(T^{m+1})=K(T^{m+2})=\cdots$$ Here $K$ denotes kernel. Let the nullity of $T^i$ be $n_i$. Then we have a sequence $$0, n_1, n_2, \cdots,n_{m-1},n_m,n_m,n_m,\cdots$$ Denote $s_i=n_i-n_{i-1}$, which can be thought of as how much ""bigger"" $K(T^i)$ is larger than $K(T^{i-1})$. It is clear that $s_i>0$ for $0 \le i \le m$. I am reading a proof of some theorem and I think the author has implicitly assumed $s_{i-1} \ge s_i$. I come up with a proof of this but am not sure about its correctness. Consider $K(T^i)$. Since $K(T^{i-1})\subsetneq K(T^i)$, we can choose $s_i$ vectors, called $z_{i,1}, z_{i,2},\cdots,z_{i,s_i}$, all not in $K(T^{i-1})$ so that $$K(T^i)=K(T^{i-1})\oplus \langle z_{i,1},z_{i,2},\cdots, z_{i,s_i}\rangle$$ Now consider the $s_i$ vectors $T(z_{i,1}),T(z_{i,2}), \cdots, T(z_{i,s_i})$. First let us shown that they are linearly independent. Suppose $$\textbf{0}=a_1T(z_{i,1})+a_2T(z_{i,2})+\dots+a_{s_i}T(z_{i,s_i})$$ Then $$\textbf{0}=T(a_1 z_{i,1}+a_2z_{i,2}+\dots+a_{s_i} z_{i,s_i})$$ $$\implies a_1 z_{i,1}+a_2z_{i,2}+\dots+a_{s_i} z_{i,s_i}\in K(T)\subsetneq K(T^{i-1})$$ Then by definition of $z_{i,1},z_{i,2},\cdots,z_{i,s_i}$, we must have $$a_1 z_{i,1}+a_2z_{i,2}+\dots+a_{s_i} z_{i,s_i}=\textbf{0}$$ and hence $$a_1=a_2 = \dots =a_{s_i}=0.$$ Now because $$T^{i-1}(T(z_{i,j}))=T^i(z_{i,j})=\textbf{0}$$ we have  $$T(z_{i,j})\in K(T^{i-1}).$$ Next, by the way $z_{i,j}$ are chosen, we have $$T^{i-1}(z_{i,j})\ne\textbf{0}$$ $$T^{i-2}(T(z_{i,j}))\ne\textbf{0}$$ $$z_{i,j}\notin K(T^{i-2})$$ In conclusion, the $s_i$ vectors $$T(z_{i,1}), T(z_{i,2}), \cdots, T(z_{i,s_i})$$ are linearly independent, $\in K(T^{i-1})$ but $\notin K(T^{i-2})$. Hence, $$s_{i-1} \ge s_i$$ Is the above proof correct?","Suppose $T:V\to V$ is a linear transformation and $dim(V)=n$. It is well known that there is an integer $m$, where $0\le m\le n$ such that $$\{\textbf{0}\}=K(T^0)\subsetneq K(T^1)\subsetneq K(T^2) \subsetneq \cdots \subsetneq K(T^m)=K(T^{m+1})=K(T^{m+2})=\cdots$$ Here $K$ denotes kernel. Let the nullity of $T^i$ be $n_i$. Then we have a sequence $$0, n_1, n_2, \cdots,n_{m-1},n_m,n_m,n_m,\cdots$$ Denote $s_i=n_i-n_{i-1}$, which can be thought of as how much ""bigger"" $K(T^i)$ is larger than $K(T^{i-1})$. It is clear that $s_i>0$ for $0 \le i \le m$. I am reading a proof of some theorem and I think the author has implicitly assumed $s_{i-1} \ge s_i$. I come up with a proof of this but am not sure about its correctness. Consider $K(T^i)$. Since $K(T^{i-1})\subsetneq K(T^i)$, we can choose $s_i$ vectors, called $z_{i,1}, z_{i,2},\cdots,z_{i,s_i}$, all not in $K(T^{i-1})$ so that $$K(T^i)=K(T^{i-1})\oplus \langle z_{i,1},z_{i,2},\cdots, z_{i,s_i}\rangle$$ Now consider the $s_i$ vectors $T(z_{i,1}),T(z_{i,2}), \cdots, T(z_{i,s_i})$. First let us shown that they are linearly independent. Suppose $$\textbf{0}=a_1T(z_{i,1})+a_2T(z_{i,2})+\dots+a_{s_i}T(z_{i,s_i})$$ Then $$\textbf{0}=T(a_1 z_{i,1}+a_2z_{i,2}+\dots+a_{s_i} z_{i,s_i})$$ $$\implies a_1 z_{i,1}+a_2z_{i,2}+\dots+a_{s_i} z_{i,s_i}\in K(T)\subsetneq K(T^{i-1})$$ Then by definition of $z_{i,1},z_{i,2},\cdots,z_{i,s_i}$, we must have $$a_1 z_{i,1}+a_2z_{i,2}+\dots+a_{s_i} z_{i,s_i}=\textbf{0}$$ and hence $$a_1=a_2 = \dots =a_{s_i}=0.$$ Now because $$T^{i-1}(T(z_{i,j}))=T^i(z_{i,j})=\textbf{0}$$ we have  $$T(z_{i,j})\in K(T^{i-1}).$$ Next, by the way $z_{i,j}$ are chosen, we have $$T^{i-1}(z_{i,j})\ne\textbf{0}$$ $$T^{i-2}(T(z_{i,j}))\ne\textbf{0}$$ $$z_{i,j}\notin K(T^{i-2})$$ In conclusion, the $s_i$ vectors $$T(z_{i,1}), T(z_{i,2}), \cdots, T(z_{i,s_i})$$ are linearly independent, $\in K(T^{i-1})$ but $\notin K(T^{i-2})$. Hence, $$s_{i-1} \ge s_i$$ Is the above proof correct?",,"['linear-algebra', 'linear-transformations']"
38,Exponentiation of Gell-Mann Matrices,Exponentiation of Gell-Mann Matrices,,"The exponentiation of Pauli vector $\vec \sigma=(\sigma_x,\sigma_y,\sigma_z)$ is trivial as we have the identity:$$e^{ia(\vec n\cdot \vec \sigma)}=I cos(a)+i(\vec n \cdot \vec \sigma)sin(a)$$ I have been studying the properties of Gell-Mann matrices and was wondering whether a similar exponentiation  is possible or not. I used Mathematica to explicitly compute the exponentials of individual Gell-Mann matrices and I am getting exact solutions. Though, the exponentials of Gell-Mann matrices can be obtained explicitely I was trying to obtain an identity similar to the above one for Pauli matrices. The major property of Pauli matrices which enable the derivation of the above identity is that $\sigma_i^2=I$. But, the Gell-Mann matrices in general does not have this property. Is it possible to derive a similar identity? If yes, I would like to receive some hints on how to achieve the same.","The exponentiation of Pauli vector $\vec \sigma=(\sigma_x,\sigma_y,\sigma_z)$ is trivial as we have the identity:$$e^{ia(\vec n\cdot \vec \sigma)}=I cos(a)+i(\vec n \cdot \vec \sigma)sin(a)$$ I have been studying the properties of Gell-Mann matrices and was wondering whether a similar exponentiation  is possible or not. I used Mathematica to explicitly compute the exponentials of individual Gell-Mann matrices and I am getting exact solutions. Though, the exponentials of Gell-Mann matrices can be obtained explicitely I was trying to obtain an identity similar to the above one for Pauli matrices. The major property of Pauli matrices which enable the derivation of the above identity is that $\sigma_i^2=I$. But, the Gell-Mann matrices in general does not have this property. Is it possible to derive a similar identity? If yes, I would like to receive some hints on how to achieve the same.",,"['linear-algebra', 'matrices', 'exponentiation']"
39,$\det(ABC) = \det(B)\det(AC)$?,?,\det(ABC) = \det(B)\det(AC),"Suppose $A$, $B$, and $C$ are $(n \times m)$, $(m \times m)$, and $(m \times n)$ matrices respectively, with $m\gt n$. What are the most general conditions under which $$ \det(ABC) = \det(B)\det(AC)$$ holds?","Suppose $A$, $B$, and $C$ are $(n \times m)$, $(m \times m)$, and $(m \times n)$ matrices respectively, with $m\gt n$. What are the most general conditions under which $$ \det(ABC) = \det(B)\det(AC)$$ holds?",,"['linear-algebra', 'matrices', 'determinant']"
40,"Matrix norm inequality : $\| Ax\| \leq |\lambda| \|x\|$, proof verification","Matrix norm inequality : , proof verification",\| Ax\| \leq |\lambda| \|x\|,"Suppose that $A$ is a normal $n\times n$ matrix.  Show that $\|Ax \| \geq |\lambda_n|\|x\|$ for all    $x \in \mathbf{C}^n$, if $\lambda_n$ is the eigenvalue to $A$ of smallest absolute value. Is this also always true when $A$   is not normal?  Give a proof   or a counterexample! First question, does this depend on which norm I choose? Since $x\in\mathbf{C}^n$, then I am inclined to choose the induced norm (whatever its real name is), but I do not know where $A$ exists. If I can choose the 'natural one', then $$ \|Ax\|^2 = (Ax,Ax) = x^*A^*Ax.$$ Since $A$ is normal, it is unitarily similar to diagonal matrix $\Lambda$, so if $y = Ux$ (isometry) one gets $$ x^*A^*Ax = x^*U^*\Lambda^*\Lambda U x = y^*\Lambda^*\Lambda y =  \sum_i^n |\lambda_i|^2 |y_i|^2 \geq |\lambda_n|^2 \|y\|^2$$ and because $\|y\| = \|x\|$ one gets $\|Ax\| \geq |\lambda_n|\|x\|$. Is this correct? Suppose $A$ not normal, then one can obtain the SVD $A = W\Sigma V^*$ and apply a similar proof $$ \|Ax\|^2 = \dots = x^*V\Sigma^* \Sigma V^* x. $$ Take $v = V^*x$, isometric, and singular values $\sigma_1 \geq \dots \geq \sigma_n \geq 0$, one gets $$ x^*V\Sigma^*\Sigma V^*x = v^*\Sigma^*\Sigma v = \sum_i^n \sigma_i^2 |v_i|^2 \geq \sigma_n^2 \|v\|^2 $$ so, $\|v\| = \|x\|$ one gets $\|Ax\| \geq \sigma_n\|x\|$. And since  $\sigma_n = \sqrt{\lambda_n} \geq \lambda_n$ for $\lambda_n \leq 1$, the statement is probably not always true, thus I can find a counterexample. I still feel like the not-normal case should be true, since it is on verge of being true, and because of my insecurity with norms and singular values. Any improvements, criticism and hints are appreciated.","Suppose that $A$ is a normal $n\times n$ matrix.  Show that $\|Ax \| \geq |\lambda_n|\|x\|$ for all    $x \in \mathbf{C}^n$, if $\lambda_n$ is the eigenvalue to $A$ of smallest absolute value. Is this also always true when $A$   is not normal?  Give a proof   or a counterexample! First question, does this depend on which norm I choose? Since $x\in\mathbf{C}^n$, then I am inclined to choose the induced norm (whatever its real name is), but I do not know where $A$ exists. If I can choose the 'natural one', then $$ \|Ax\|^2 = (Ax,Ax) = x^*A^*Ax.$$ Since $A$ is normal, it is unitarily similar to diagonal matrix $\Lambda$, so if $y = Ux$ (isometry) one gets $$ x^*A^*Ax = x^*U^*\Lambda^*\Lambda U x = y^*\Lambda^*\Lambda y =  \sum_i^n |\lambda_i|^2 |y_i|^2 \geq |\lambda_n|^2 \|y\|^2$$ and because $\|y\| = \|x\|$ one gets $\|Ax\| \geq |\lambda_n|\|x\|$. Is this correct? Suppose $A$ not normal, then one can obtain the SVD $A = W\Sigma V^*$ and apply a similar proof $$ \|Ax\|^2 = \dots = x^*V\Sigma^* \Sigma V^* x. $$ Take $v = V^*x$, isometric, and singular values $\sigma_1 \geq \dots \geq \sigma_n \geq 0$, one gets $$ x^*V\Sigma^*\Sigma V^*x = v^*\Sigma^*\Sigma v = \sum_i^n \sigma_i^2 |v_i|^2 \geq \sigma_n^2 \|v\|^2 $$ so, $\|v\| = \|x\|$ one gets $\|Ax\| \geq \sigma_n\|x\|$. And since  $\sigma_n = \sqrt{\lambda_n} \geq \lambda_n$ for $\lambda_n \leq 1$, the statement is probably not always true, thus I can find a counterexample. I still feel like the not-normal case should be true, since it is on verge of being true, and because of my insecurity with norms and singular values. Any improvements, criticism and hints are appreciated.",,"['linear-algebra', 'matrices']"
41,Why do Clifford bivectors represent the orthogonal Lie algebra?,Why do Clifford bivectors represent the orthogonal Lie algebra?,,"It takes a long, painful, but straightforward calculation to see that the commutators of grade one elements $[e_i, e_j]$ of a Clifford algebra $\mathrm{Cl}(p,q)$ have exactly the same commutation relations as the rotations in the $(ij)$ plane that form the basis of $\mathfrak{so}(p,q)$. There is a different path to this result that I have yet to understand thoroughly, but on the surface it seems to involve a similarly unilluminating calculation to prove that $\mathrm{ad}_{[e_i, e_j]}$ maps vectors into vectors and (linear combinations of) bivectors into bivectors. Is there a way to understand why it couldn’t be otherwise? (Not necessarily a complete proof, but at least some motivation to carry out the calculation knowing what to expect.)","It takes a long, painful, but straightforward calculation to see that the commutators of grade one elements $[e_i, e_j]$ of a Clifford algebra $\mathrm{Cl}(p,q)$ have exactly the same commutation relations as the rotations in the $(ij)$ plane that form the basis of $\mathfrak{so}(p,q)$. There is a different path to this result that I have yet to understand thoroughly, but on the surface it seems to involve a similarly unilluminating calculation to prove that $\mathrm{ad}_{[e_i, e_j]}$ maps vectors into vectors and (linear combinations of) bivectors into bivectors. Is there a way to understand why it couldn’t be otherwise? (Not necessarily a complete proof, but at least some motivation to carry out the calculation knowing what to expect.)",,"['linear-algebra', 'lie-algebras', 'clifford-algebras']"
42,"More than basis vectors in a space are dependent, less can't span the space proof","More than basis vectors in a space are dependent, less can't span the space proof",,"Let $V$ be a vector space, $ B = \{ \mathbf{b}_{1}, \ldots, \mathbf{b}_{n} \}$ a basis for $V$ (independent and spans $V$). Prove that fewer vectors than $n$ can't span $V$, while more vectors are necessarily dependent. I am looking for a clearer proof than the one on this page: https://math.kennesaw.edu/~plaval/math3260/basis.pdf (middle of third page, Theorem 306), and/or an explanation as to why that system of equations helps prove linear dependence (says to consider said linear combination, but doesn't really explain anything). Thanks!","Let $V$ be a vector space, $ B = \{ \mathbf{b}_{1}, \ldots, \mathbf{b}_{n} \}$ a basis for $V$ (independent and spans $V$). Prove that fewer vectors than $n$ can't span $V$, while more vectors are necessarily dependent. I am looking for a clearer proof than the one on this page: https://math.kennesaw.edu/~plaval/math3260/basis.pdf (middle of third page, Theorem 306), and/or an explanation as to why that system of equations helps prove linear dependence (says to consider said linear combination, but doesn't really explain anything). Thanks!",,"['linear-algebra', 'vector-spaces']"
43,$T^3=\frac{1}{2}(T+T^*) \rightarrow$ T is self adjoint,T is self adjoint,T^3=\frac{1}{2}(T+T^*) \rightarrow,"Let $T$ be a normal transformation on a finite-dimensional Hilbert space; that is, $TT^*=T^*T$, where $T^*$ is the adjoint of $T$. Prove that if $T^3=\frac{1}{2}(T+T^*)$, then $T$ is self adjoint. I have tried to do some math on $(Tv,u)$ but I was not successful in proving the following: $(Tu,v)=(u,Tv)$ which is what I need for self-adjoint transformation. Edit: $(T^3,v)=\frac{1}{2}\left(\left(T+T^*\right),u \right)= \left( Tu,v\right)+  \left( T^*u,v\right)=\left( u,T^*v\right)+\left( u,Tv\right)=\left( u,T^3v\right)$ Therefore, $T^3$ is self adjoint. Does it mean that $T$ is self adjoint? Thanks!","Let $T$ be a normal transformation on a finite-dimensional Hilbert space; that is, $TT^*=T^*T$, where $T^*$ is the adjoint of $T$. Prove that if $T^3=\frac{1}{2}(T+T^*)$, then $T$ is self adjoint. I have tried to do some math on $(Tv,u)$ but I was not successful in proving the following: $(Tu,v)=(u,Tv)$ which is what I need for self-adjoint transformation. Edit: $(T^3,v)=\frac{1}{2}\left(\left(T+T^*\right),u \right)= \left( Tu,v\right)+  \left( T^*u,v\right)=\left( u,T^*v\right)+\left( u,Tv\right)=\left( u,T^3v\right)$ Therefore, $T^3$ is self adjoint. Does it mean that $T$ is self adjoint? Thanks!",,"['linear-algebra', 'linear-transformations']"
44,Differentiation as Rotation,Differentiation as Rotation,,I am trying to make a connection between linear algebra and the Fourier transform. Functions form a vector space and differentiation is an operator. Fourier transforming a function from what i understand is writing the function in the basis of complex exponentials. Differentiation in the Fourier domain is multiplication by the imaginary unit which is also the eigenvalue of the 90 degree rotation matrix of the 2x2 rotational matrix. Is it possible to say that the the differential operator is diagonalised in the Fourier domain? and Is the differential operator the equivalent of the rotation matrix in finite dimensional matrices?,I am trying to make a connection between linear algebra and the Fourier transform. Functions form a vector space and differentiation is an operator. Fourier transforming a function from what i understand is writing the function in the basis of complex exponentials. Differentiation in the Fourier domain is multiplication by the imaginary unit which is also the eigenvalue of the 90 degree rotation matrix of the 2x2 rotational matrix. Is it possible to say that the the differential operator is diagonalised in the Fourier domain? and Is the differential operator the equivalent of the rotation matrix in finite dimensional matrices?,,"['linear-algebra', 'functional-analysis']"
45,"Suppose $L$ and $M$ are two vector spaces, how to find a basis of $L+M$ and $L \cap M$?","Suppose  and  are two vector spaces, how to find a basis of  and ?",L M L+M L \cap M,"Let $(1+t−t^3,1+t+t^2,1−t)$ and $(t^3+t,2−t^3,2+t^3)$ be two basis of subspaces $L$ and $M$. Find one basis of $L+M$ and $L\cap M$. I think $(1+t−t^3,\,1+t+t^2,\,1−t)$ could probably be expressed as $$ \begin{pmatrix}  1 & t & t^2 & t^3  \end{pmatrix} \begin{pmatrix}   \phantom{-}1 & 1 & \phantom{-}1 \\   \phantom{-}1 & 1 &           -1 \\   \phantom{-}0 & 1 & \phantom{-}0 \\             -1 & 0 & \phantom{-}0  \end{pmatrix} $$ and $(t^3+t,2−t^3,2+t^3)$ could be expressed as $$ \begin{pmatrix}  1 & t & t^2 & t^3 \end{pmatrix} \begin{pmatrix}  0 & \phantom{-}2 & 2 \\  1 & \phantom{-}0 & 0 \\  0 & \phantom{-}0 & 0 \\  1 &           -1 & 1 \end{pmatrix} $$ But I am stuck then.","Let $(1+t−t^3,1+t+t^2,1−t)$ and $(t^3+t,2−t^3,2+t^3)$ be two basis of subspaces $L$ and $M$. Find one basis of $L+M$ and $L\cap M$. I think $(1+t−t^3,\,1+t+t^2,\,1−t)$ could probably be expressed as $$ \begin{pmatrix}  1 & t & t^2 & t^3  \end{pmatrix} \begin{pmatrix}   \phantom{-}1 & 1 & \phantom{-}1 \\   \phantom{-}1 & 1 &           -1 \\   \phantom{-}0 & 1 & \phantom{-}0 \\             -1 & 0 & \phantom{-}0  \end{pmatrix} $$ and $(t^3+t,2−t^3,2+t^3)$ could be expressed as $$ \begin{pmatrix}  1 & t & t^2 & t^3 \end{pmatrix} \begin{pmatrix}  0 & \phantom{-}2 & 2 \\  1 & \phantom{-}0 & 0 \\  0 & \phantom{-}0 & 0 \\  1 &           -1 & 1 \end{pmatrix} $$ But I am stuck then.",,"['linear-algebra', 'vector-spaces']"
46,Relationship between row space and orthogonal component of kernel of complex vector space,Relationship between row space and orthogonal component of kernel of complex vector space,,"When we consider the real vector space, row space is equal to orthogonal complement of the null space (kernel). This fact can be proved as follows. Let's consider linear map $A : \mathbb{R}^n \rightarrow \mathbb{R}^m$. Row space of the matrix is $\mathrm{Row}(A) := \mathrm{span}(\{\mathbf{r}_i\}_{i=1,2,...,m})$. Where, $\{\mathbf{r}_i\}_{i=1,2,...,n}$ is row vector of matrix A. On the other hand, the kernel of the matrix is  $\mathrm{Kernel}(A) := \{ \mathbf{v}\in\mathbb{R}^n | A\mathbf{v}=0 \} = \{ \mathbf{v}\in\mathbb{R}^n | \mathbf{r}_i\cdot\mathbf{v}=0 \mathrm{\ for\ all\ i }\}$ Then, orthogonal subspace of the kernel is nothing but row space. However, when we consider complex vector space $A : \mathbb{C}^n \rightarrow \mathbb{C}^m$, inner product of row vector is $\mathbf{r}_i^{*}\cdot\mathbf{v}$. So, in this case, the condition $\mathbf{r}_i^{*}\cdot\mathbf{v}=0$ is different from the definition of the kernel. Is there some relationship between row space and orthogonal space of the kernel of complex vector space? Ref: https://en.wikipedia.org/wiki/Row_and_column_spaces#Relation_to_the_null_space","When we consider the real vector space, row space is equal to orthogonal complement of the null space (kernel). This fact can be proved as follows. Let's consider linear map $A : \mathbb{R}^n \rightarrow \mathbb{R}^m$. Row space of the matrix is $\mathrm{Row}(A) := \mathrm{span}(\{\mathbf{r}_i\}_{i=1,2,...,m})$. Where, $\{\mathbf{r}_i\}_{i=1,2,...,n}$ is row vector of matrix A. On the other hand, the kernel of the matrix is  $\mathrm{Kernel}(A) := \{ \mathbf{v}\in\mathbb{R}^n | A\mathbf{v}=0 \} = \{ \mathbf{v}\in\mathbb{R}^n | \mathbf{r}_i\cdot\mathbf{v}=0 \mathrm{\ for\ all\ i }\}$ Then, orthogonal subspace of the kernel is nothing but row space. However, when we consider complex vector space $A : \mathbb{C}^n \rightarrow \mathbb{C}^m$, inner product of row vector is $\mathbf{r}_i^{*}\cdot\mathbf{v}$. So, in this case, the condition $\mathbf{r}_i^{*}\cdot\mathbf{v}=0$ is different from the definition of the kernel. Is there some relationship between row space and orthogonal space of the kernel of complex vector space? Ref: https://en.wikipedia.org/wiki/Row_and_column_spaces#Relation_to_the_null_space",,"['linear-algebra', 'matrices']"
47,QR(pivot) vs SVD for low rank approximation,QR(pivot) vs SVD for low rank approximation,,"Define the low rank problem as finding the approximation of matrix A, B: where we want to minimize rank(B) and we want the 2 norm of the residu of A-B to be less than epsilon. Could someone help me understand what the benefits of both methods are when you use them for low rank approximation? I did some research and found some points: QR is supposed to be faster than SVD (especially for low rank probs) When matrices have 'gaps' in singular values -> QR better than SVD When there are no gaps in singular values, SVD is preferred. I have no idea why 1) holds.  I also have only a guess of why 2) works: the gap in singular values could make our estimation of the new matrix off because if epsilon should fall between a huge gap of singular values, the matrix we would obtain would be too rough of an approximation (we lose data we did not anticipate) ? Another point I'd like to understand better is that I found somewhere that SVD would work better than QR when lower k was required. The example that was given was as follows: A matrix with rank only 1: \begin{bmatrix}1&1\\2&2\end{bmatrix} A matrix with theoretical rank of 2: \begin{bmatrix}1.000&0.9999\\2&2\end{bmatrix} And a matrix $\sum$ was given from $ A = U * \sum * V^T$ (SVD of A). With its singular values on the diagonal: \begin{bmatrix}2&0\\0&10^-8\end{bmatrix} In this case, SVD would actually tell us that the rank of the second matrix was only 1, and not really 2. Which I guess aids in a low rank approximation? I cannot explain how this helps.","Define the low rank problem as finding the approximation of matrix A, B: where we want to minimize rank(B) and we want the 2 norm of the residu of A-B to be less than epsilon. Could someone help me understand what the benefits of both methods are when you use them for low rank approximation? I did some research and found some points: QR is supposed to be faster than SVD (especially for low rank probs) When matrices have 'gaps' in singular values -> QR better than SVD When there are no gaps in singular values, SVD is preferred. I have no idea why 1) holds.  I also have only a guess of why 2) works: the gap in singular values could make our estimation of the new matrix off because if epsilon should fall between a huge gap of singular values, the matrix we would obtain would be too rough of an approximation (we lose data we did not anticipate) ? Another point I'd like to understand better is that I found somewhere that SVD would work better than QR when lower k was required. The example that was given was as follows: A matrix with rank only 1: \begin{bmatrix}1&1\\2&2\end{bmatrix} A matrix with theoretical rank of 2: \begin{bmatrix}1.000&0.9999\\2&2\end{bmatrix} And a matrix $\sum$ was given from $ A = U * \sum * V^T$ (SVD of A). With its singular values on the diagonal: \begin{bmatrix}2&0\\0&10^-8\end{bmatrix} In this case, SVD would actually tell us that the rank of the second matrix was only 1, and not really 2. Which I guess aids in a low rank approximation? I cannot explain how this helps.",,"['linear-algebra', 'approximation', 'matrix-decomposition', 'svd']"
48,Uniqueness of the killing form,Uniqueness of the killing form,,"I would like to consider/prove the following problems: let $k$ be a field, $g$ a finite-dimensional simple Lie algebra over $k$ with Killing form $B$. If $\sigma:g\times g\rightarrow k$ is a symmetric bilinear form such that $\sigma([x,y],z)=\sigma(x,[y,z])$, then there is $c\in k$ with $B=c\sigma$. I know that for $k$ algebraically closed, this follow from a version of Schur's lemma. Problem 1: what about the case $k=\mathbb{R}$? Is this result still true, and how can it be proven if it is? By some kind of base change argument perhaps? Apparently this can be generalized to semisimple $g$; in this case the obvious base change argument might actually work (as $g$ is semisimple iff its complexification is, which is, as far as I know, false for simple $g$). However, it is still not obvious why $c\in\mathbb{R}$ instead of merely $c\in\mathbb{C}$? Actually I found the following exercise in Bourbaki's book: it suggests that the result might be false. Problem 2 if $g$ is a simple subalgebra of $gl_n(\mathbb{R})$ and $\sigma(x,y)=\mathrm{Tr}(xy)$, then $B=c\sigma$ for some $c\in\mathbb{R}$. Is Problem 2 not even true?","I would like to consider/prove the following problems: let $k$ be a field, $g$ a finite-dimensional simple Lie algebra over $k$ with Killing form $B$. If $\sigma:g\times g\rightarrow k$ is a symmetric bilinear form such that $\sigma([x,y],z)=\sigma(x,[y,z])$, then there is $c\in k$ with $B=c\sigma$. I know that for $k$ algebraically closed, this follow from a version of Schur's lemma. Problem 1: what about the case $k=\mathbb{R}$? Is this result still true, and how can it be proven if it is? By some kind of base change argument perhaps? Apparently this can be generalized to semisimple $g$; in this case the obvious base change argument might actually work (as $g$ is semisimple iff its complexification is, which is, as far as I know, false for simple $g$). However, it is still not obvious why $c\in\mathbb{R}$ instead of merely $c\in\mathbb{C}$? Actually I found the following exercise in Bourbaki's book: it suggests that the result might be false. Problem 2 if $g$ is a simple subalgebra of $gl_n(\mathbb{R})$ and $\sigma(x,y)=\mathrm{Tr}(xy)$, then $B=c\sigma$ for some $c\in\mathbb{R}$. Is Problem 2 not even true?",,"['linear-algebra', 'representation-theory', 'lie-algebras']"
49,How can I find the Nash-equilibrium of the following zero sum game?,How can I find the Nash-equilibrium of the following zero sum game?,,"I want to find the Nash-equilibrium of the following zero sum game.  $$A=\begin{bmatrix}0&2&-1\\-2&0&3\\1&-3&0\end{bmatrix}$$ I used the Minimax Theorem. $$min_{x \in X} max_{y \in Y} E(x,y)= min_{(u,x)} \{u; u \ge \sum^{n}_{i=1}a_{ij}x_i, \sum^{n}_{i=1}x_i=1, x_i \ge 0 \} $$ $$max_{y \in Y} min_{x \in X} E(x,y)=max_{(w,y)} \{w; w \le \sum^{m}_{j=1}a_{ij}y_j, \sum^{m}_{j=1}y_j=1, y_j \ge 0 \} $$ $$E(x,y)=x^TAy$$ I can get two linear programs $$min \, u$$ $$-u-2x_2+x_3 \le 0 $$ $$-u+2x_1-3x_3 \le 0 $$ $$-u-x_1+3x_2 \le 0 $$ $$ x_1+x_2+x_3 =1 $$ $$ x_1,x_2,x_3 \ge 0 $$ and $$max \, w $$ $$ w-2y_2+y_3 \le 0 $$ $$w+2y_1-3y_3 \le 0 $$ $$w-y_1+3y_2 \le 0 $$ $$ y_1+y_2+y_3 =1 $$ $$ y_1,y_2,y_3 \ge 0 $$ Then, how can I find the Nash-equilibrium?","I want to find the Nash-equilibrium of the following zero sum game.  $$A=\begin{bmatrix}0&2&-1\\-2&0&3\\1&-3&0\end{bmatrix}$$ I used the Minimax Theorem. $$min_{x \in X} max_{y \in Y} E(x,y)= min_{(u,x)} \{u; u \ge \sum^{n}_{i=1}a_{ij}x_i, \sum^{n}_{i=1}x_i=1, x_i \ge 0 \} $$ $$max_{y \in Y} min_{x \in X} E(x,y)=max_{(w,y)} \{w; w \le \sum^{m}_{j=1}a_{ij}y_j, \sum^{m}_{j=1}y_j=1, y_j \ge 0 \} $$ $$E(x,y)=x^TAy$$ I can get two linear programs $$min \, u$$ $$-u-2x_2+x_3 \le 0 $$ $$-u+2x_1-3x_3 \le 0 $$ $$-u-x_1+3x_2 \le 0 $$ $$ x_1+x_2+x_3 =1 $$ $$ x_1,x_2,x_3 \ge 0 $$ and $$max \, w $$ $$ w-2y_2+y_3 \le 0 $$ $$w+2y_1-3y_3 \le 0 $$ $$w-y_1+3y_2 \le 0 $$ $$ y_1+y_2+y_3 =1 $$ $$ y_1,y_2,y_3 \ge 0 $$ Then, how can I find the Nash-equilibrium?",,"['linear-algebra', 'matrices', 'game-theory', 'nash-equilibrium']"
50,Three lines are concurrent (or parallel) $\iff$ the determinant of its coordinates vanishes.,Three lines are concurrent (or parallel)  the determinant of its coordinates vanishes.,\iff,"I'm trying to prove the concurrency condition for three lines lying on a plane. This condition says that: Let    \begin{cases} ax + by + cz=0 \\ a'x – b'y + c'z=0 \\ a''x + b''y + c''z=0 \end{cases}   be three lines (barycentric coordinates), with the cordinates of the lines not all equal. Prove that they are concurrent or parallel iff $$ \left| \begin{array}{ccc} a & b & c \\ a' & b' & c' \\ a'' & b'' & c'' \end{array} \right| =0. $$ My try: $$\left| \begin{array}{ccc} a & b & c \\ a' & b' & c' \\ a'' & b'' & c'' \end{array} \right| =0 \iff \exists P=(x,y,z)\neq(0,0,0),$$ and we've found a common $P$ in those three lines, different of $(0,0,0)$ and with $(x,y)\neq (0,0)$, because it would follow that $P=(0,0,0)$. So the lines are concurrent. Is it correct? How I see that they're parallel?","I'm trying to prove the concurrency condition for three lines lying on a plane. This condition says that: Let    \begin{cases} ax + by + cz=0 \\ a'x – b'y + c'z=0 \\ a''x + b''y + c''z=0 \end{cases}   be three lines (barycentric coordinates), with the cordinates of the lines not all equal. Prove that they are concurrent or parallel iff $$ \left| \begin{array}{ccc} a & b & c \\ a' & b' & c' \\ a'' & b'' & c'' \end{array} \right| =0. $$ My try: $$\left| \begin{array}{ccc} a & b & c \\ a' & b' & c' \\ a'' & b'' & c'' \end{array} \right| =0 \iff \exists P=(x,y,z)\neq(0,0,0),$$ and we've found a common $P$ in those three lines, different of $(0,0,0)$ and with $(x,y)\neq (0,0)$, because it would follow that $P=(0,0,0)$. So the lines are concurrent. Is it correct? How I see that they're parallel?",,"['linear-algebra', 'geometry', 'determinant', 'affine-geometry', 'barycentric-coordinates']"
51,Does everywhere positive definite Hessian imply bijective gradient?,Does everywhere positive definite Hessian imply bijective gradient?,,"If $f:\mathbb R^n\to\mathbb R$ is strictly convex, i.e. its hessian is everywhere positive definite, does that imply its gradient is bijective? To ensure the well-definedness of the Légendre transform, which is usually used on s.c. functions, I need the gradient to be bijective, so as to be sure that for any $p\in\mathbb R^n$ there exists one and only one $x\in\mathbb R^n$ for which $p=\nabla f(x)$...","If $f:\mathbb R^n\to\mathbb R$ is strictly convex, i.e. its hessian is everywhere positive definite, does that imply its gradient is bijective? To ensure the well-definedness of the Légendre transform, which is usually used on s.c. functions, I need the gradient to be bijective, so as to be sure that for any $p\in\mathbb R^n$ there exists one and only one $x\in\mathbb R^n$ for which $p=\nabla f(x)$...",,"['linear-algebra', 'multivariable-calculus']"
52,Intuition: why distinct eigenvalues $\implies$ linearly independent eigen vectors?,Intuition: why distinct eigenvalues  linearly independent eigen vectors?,\implies,"Suppose you have an $~n\times n~$ matrix with $~n~$ distinct (not repeated) eigenvalues. There is a theorem telling us that the eigen vectors corresponding to these eigenvalues must be linearly independent . I can basically follow the proof, but I am looking for an intuitive explanation of why this is the case. Can anyone offer some insight?","Suppose you have an matrix with distinct (not repeated) eigenvalues. There is a theorem telling us that the eigen vectors corresponding to these eigenvalues must be linearly independent . I can basically follow the proof, but I am looking for an intuitive explanation of why this is the case. Can anyone offer some insight?",~n\times n~ ~n~,"['linear-algebra', 'eigenvalues-eigenvectors']"
53,"Three different notions for ""bigness"" of a module (length, rank and ""mass"").","Three different notions for ""bigness"" of a module (length, rank and ""mass"").",,"Let $M$ be an $R-$module where $R$ is an integral domain. I'm trying to understand the relations between these three notions of size: Let $S \subset M$ be a generating set of minimal cardinality . Define $|S| = mass(M)$ (I don't know any formal term for this so i'll use this one. Sorry.). Let $T \subset M$ be a maximal linearly independent set (A set $T$ is linearly independent iff the natural homomorphism from $R^{\oplus T}$ to $M$ is injective). Define $|T| = {rk}_R(M)$. By Jordan–Hölder for modules, all composition series of a module have the same length. Define $length(M)$ as that length. For vector spaces (i .e. if $R$ is a field) all the notions coincide. (please correct me if i'm wrong). Suppose $M$ is free and of finite length. We know $M$ is free iff it admits a basis. So $M \cong R^{\oplus B}$ where $B \subset M$ is a linearly independent subset. By splitting the composition series we find that $M$ is isomorphic to a sum of simple modules. That is: $$M \cong \bigoplus_{j \in J} N_j \cong R^{\oplus B}$$ Can we now deduce that $length(M)=mass(M)={rk}_R(M)$? Suppose $M$ is projective and of finite length.  We have again, by splitting, that $M \cong \bigoplus_{j \in J} N_j $. Do we have $length(M)=mass(M)$? What about ${rk}_R(M)$? Suppose $M$ is noetherian . Clearly $mass(M)$ is finite. Must $length(M)$ be finite? What if we add that $M$ is projective (/free)? I'd like to have a more organized picture of how these notions interact with each other on different modules. Comprehensive answers would be greatly appreciated (and rewarded ;)).","Let $M$ be an $R-$module where $R$ is an integral domain. I'm trying to understand the relations between these three notions of size: Let $S \subset M$ be a generating set of minimal cardinality . Define $|S| = mass(M)$ (I don't know any formal term for this so i'll use this one. Sorry.). Let $T \subset M$ be a maximal linearly independent set (A set $T$ is linearly independent iff the natural homomorphism from $R^{\oplus T}$ to $M$ is injective). Define $|T| = {rk}_R(M)$. By Jordan–Hölder for modules, all composition series of a module have the same length. Define $length(M)$ as that length. For vector spaces (i .e. if $R$ is a field) all the notions coincide. (please correct me if i'm wrong). Suppose $M$ is free and of finite length. We know $M$ is free iff it admits a basis. So $M \cong R^{\oplus B}$ where $B \subset M$ is a linearly independent subset. By splitting the composition series we find that $M$ is isomorphic to a sum of simple modules. That is: $$M \cong \bigoplus_{j \in J} N_j \cong R^{\oplus B}$$ Can we now deduce that $length(M)=mass(M)={rk}_R(M)$? Suppose $M$ is projective and of finite length.  We have again, by splitting, that $M \cong \bigoplus_{j \in J} N_j $. Do we have $length(M)=mass(M)$? What about ${rk}_R(M)$? Suppose $M$ is noetherian . Clearly $mass(M)$ is finite. Must $length(M)$ be finite? What if we add that $M$ is projective (/free)? I'd like to have a more organized picture of how these notions interact with each other on different modules. Comprehensive answers would be greatly appreciated (and rewarded ;)).",,"['linear-algebra', 'modules', 'projective-module', 'noetherian']"
54,Levi-Civita tensor,Levi-Civita tensor,,"Show that $\epsilon_{ijk} A_{il}  A_{jm}  A_{kn}  = \det(A) \epsilon_{lmn}$ where $\epsilon$ epsilon is the standard Levi-Civita symbol and A is a three dimensional matrix. I found the above property in a book used for my course about turbulence, where it is used to prove that the $\epsilon$ is isotropic. The proof for the property however is not provided, and I'm struggling to prove it myself.  Thanks.","Show that $\epsilon_{ijk} A_{il}  A_{jm}  A_{kn}  = \det(A) \epsilon_{lmn}$ where $\epsilon$ epsilon is the standard Levi-Civita symbol and A is a three dimensional matrix. I found the above property in a book used for my course about turbulence, where it is used to prove that the $\epsilon$ is isotropic. The proof for the property however is not provided, and I'm struggling to prove it myself.  Thanks.",,"['linear-algebra', 'tensors']"
55,Calculating the intersection of two spaces of polynomials,Calculating the intersection of two spaces of polynomials,,"This problem is driving me nuts. I feel like there should be an elementary argument, yet I have failed to find one. Consider the vector space $V_n=\mathbb Q[x]/{x^{2n+1}}=\mathbb Q\{1,x,x^2,\ldots, x^{2n}\}$. Define polynomials $p_m=x^m+(-1-x)^m+(-1-x)^{2n-m}$. Consider the subspaces of $V_n$ given by $$I=\mathbb Q\{p_m\,:\, 0\leq m\leq 2n\},$$ $$J=\mathbb Q\{x^{2i}\,:\,0\leq i\leq n\}$$ and $$K=\mathbb Q\{x^{2i}-x^{2n-2i}\,:\,0\leq i\leq n\}.$$ I want to prove that $$I\cap J= K.$$ Inclusion in the reverse direction is clear: $p_m-p_{2n-m}=x^m-x^{2n-m}$. The hard part is showing the inclusion $I\cap J\subset K$. Computer calculations show this is true for all $n$ I have checked. Update: I have asked this question at mathoverflow. Update 2: This question now has an answer at mathoverflow.","This problem is driving me nuts. I feel like there should be an elementary argument, yet I have failed to find one. Consider the vector space $V_n=\mathbb Q[x]/{x^{2n+1}}=\mathbb Q\{1,x,x^2,\ldots, x^{2n}\}$. Define polynomials $p_m=x^m+(-1-x)^m+(-1-x)^{2n-m}$. Consider the subspaces of $V_n$ given by $$I=\mathbb Q\{p_m\,:\, 0\leq m\leq 2n\},$$ $$J=\mathbb Q\{x^{2i}\,:\,0\leq i\leq n\}$$ and $$K=\mathbb Q\{x^{2i}-x^{2n-2i}\,:\,0\leq i\leq n\}.$$ I want to prove that $$I\cap J= K.$$ Inclusion in the reverse direction is clear: $p_m-p_{2n-m}=x^m-x^{2n-m}$. The hard part is showing the inclusion $I\cap J\subset K$. Computer calculations show this is true for all $n$ I have checked. Update: I have asked this question at mathoverflow. Update 2: This question now has an answer at mathoverflow.",,"['linear-algebra', 'abstract-algebra', 'polynomials', 'vector-spaces']"
56,An explanation for a Jordan normal form proof from the Kaye and Wilson book,An explanation for a Jordan normal form proof from the Kaye and Wilson book,,"In this proof of Jordan normal form in the Kaye and Wilson book, then for a transformation $T$ with minimal polynomial $m(x) = (x-e)^k$, they take a basis of $\texttt{ker}\;T$, extend it to a basis of $\texttt{ker}\;T^2$, ..., extend it to a basis of $\texttt{ker}\;T^k$. They then take the elements $a_1,...,a_n$ in $\texttt{ker}\;T^k$ but not $\texttt{ker}\;T^{k-1}$ then take $b_i = T(a_i)$ and claim the $b_i$,$a_j$ form a linearly independent set, so then they extend the list of the $b_i$ so that the span of the $a_j$, $b_i$ is $\texttt{ker}\;T^{k-1}$ not $\texttt{ker}\;T^{k-2}$. They then take $c_i = T(b_i)$ and carry on this process. After the proof they say the crucial point is that the basis modification does give a basis. Then the lemma they prove is: If $ \{u_1,...,u_r \}$ is a basis for $\texttt{ker}\;T^j$ is extended to a basis of $\texttt{ker}\; T^{j+1}$ $\{u_1,...,u_r,v_1,...,v_s\}$ and to a basis $\{u_1,...,u_r,v_1,...,v_s,w_1,...,w_t\}$ of $\texttt{ker}\;T^{j+2}$ then $\{u_1,...,u_r,T(w_1),...,T(w_t)\}$ is a linearly independent subset of $\texttt{ker}\;T^{j+1}$. I can't work out why they prove $\{u_1,...,u_r,T(w_1),...,T(w_t)\}$ is linearly independent - where does this come into the proof for Jordan normal form? For the proof for Jordan normal form surely they'd want to show that $\{w_1,...,w_t,T(w_1),...,T(w_t)\}$ is linearly independent since in the proof they claim the $b_i$,$a_j$ form a linearly independent set (in which case they wouldn't need to extend the basis of $u_i$ to a basis of $u_i$,$v_j$ and then to a basis $u_i$,$v_j$,$w_k$ - they wouldn't need the $u_i$ at all).","In this proof of Jordan normal form in the Kaye and Wilson book, then for a transformation $T$ with minimal polynomial $m(x) = (x-e)^k$, they take a basis of $\texttt{ker}\;T$, extend it to a basis of $\texttt{ker}\;T^2$, ..., extend it to a basis of $\texttt{ker}\;T^k$. They then take the elements $a_1,...,a_n$ in $\texttt{ker}\;T^k$ but not $\texttt{ker}\;T^{k-1}$ then take $b_i = T(a_i)$ and claim the $b_i$,$a_j$ form a linearly independent set, so then they extend the list of the $b_i$ so that the span of the $a_j$, $b_i$ is $\texttt{ker}\;T^{k-1}$ not $\texttt{ker}\;T^{k-2}$. They then take $c_i = T(b_i)$ and carry on this process. After the proof they say the crucial point is that the basis modification does give a basis. Then the lemma they prove is: If $ \{u_1,...,u_r \}$ is a basis for $\texttt{ker}\;T^j$ is extended to a basis of $\texttt{ker}\; T^{j+1}$ $\{u_1,...,u_r,v_1,...,v_s\}$ and to a basis $\{u_1,...,u_r,v_1,...,v_s,w_1,...,w_t\}$ of $\texttt{ker}\;T^{j+2}$ then $\{u_1,...,u_r,T(w_1),...,T(w_t)\}$ is a linearly independent subset of $\texttt{ker}\;T^{j+1}$. I can't work out why they prove $\{u_1,...,u_r,T(w_1),...,T(w_t)\}$ is linearly independent - where does this come into the proof for Jordan normal form? For the proof for Jordan normal form surely they'd want to show that $\{w_1,...,w_t,T(w_1),...,T(w_t)\}$ is linearly independent since in the proof they claim the $b_i$,$a_j$ form a linearly independent set (in which case they wouldn't need to extend the basis of $u_i$ to a basis of $u_i$,$v_j$ and then to a basis $u_i$,$v_j$,$w_k$ - they wouldn't need the $u_i$ at all).",,"['linear-algebra', 'proof-explanation', 'jordan-normal-form']"
57,Invariant subspaces and eigenvectors,Invariant subspaces and eigenvectors,,"Let $T:V \rightarrow V$ be a linear transformation where $V$ is a finite dimensional vector space. Let $W \subset V$ be such that $T(W) \subset W$. If $v_1, v_2, \ldots ,v_n \in V$ are eigenvectors corresponding to distinct eigenvalues $\lambda_1, \lambda_2, \ldots ,\lambda_n$ of $T$ and $v_1 + v_2 + \ldots + v_n \in W$, prove that each $v_i \in W$. I could prove that all the above eigenvectors are linearly independent and that atleast one $v_i$ lies in $W$ but how do I show all $v_i's \in W$? Any hint is appreciated. Note : Even though this has a homework tag, I should stress that this isn't my homework. It's been a few years since I last took a university course. I found this question while going through some (really) old papers in my desk!","Let $T:V \rightarrow V$ be a linear transformation where $V$ is a finite dimensional vector space. Let $W \subset V$ be such that $T(W) \subset W$. If $v_1, v_2, \ldots ,v_n \in V$ are eigenvectors corresponding to distinct eigenvalues $\lambda_1, \lambda_2, \ldots ,\lambda_n$ of $T$ and $v_1 + v_2 + \ldots + v_n \in W$, prove that each $v_i \in W$. I could prove that all the above eigenvectors are linearly independent and that atleast one $v_i$ lies in $W$ but how do I show all $v_i's \in W$? Any hint is appreciated. Note : Even though this has a homework tag, I should stress that this isn't my homework. It's been a few years since I last took a university course. I found this question while going through some (really) old papers in my desk!",,['linear-algebra']
58,$\lim_{k \to \infty} A^k$ where $A$ is diagonalizable,where  is diagonalizable,\lim_{k \to \infty} A^k A,"I'm reviewing diagonalization and am wondering if the following makes sense. Let $A \in \mathcal{M}_{n \times n}(\mathbb{R})$ be a diagonalizable matrix. That is, there exist matrices $D$ and $P$ such that $$ A = PDP^{-1} $$ where the columns of $P$ are linearly independent eigenvectors of $A$ and the $D$ is a diagonal matrix whose diagonal entries are the eigenvalues of $A$ (repeated based on their respective multiplicities). Does it follow that $$ \lim_{k \to \infty} A^k = 0 $$ if each eigenvalue of $A$ is in the range $(-1, 1)$? My reasoning is that (from elementary linear algebra) we can show that $$ A = PD^kP^{-1} $$ if $A$ is diagonalizable. Since $P$ and $P^{-1}$ are finite, the product should approach zero since each (diagonal) entry of $D$ will approach zero since $$ \lim_{k \to \infty} \lambda^k = 0 $$ if $-1 < \lambda < 1$. Does this make sense or is my reasoning flawed?","I'm reviewing diagonalization and am wondering if the following makes sense. Let $A \in \mathcal{M}_{n \times n}(\mathbb{R})$ be a diagonalizable matrix. That is, there exist matrices $D$ and $P$ such that $$ A = PDP^{-1} $$ where the columns of $P$ are linearly independent eigenvectors of $A$ and the $D$ is a diagonal matrix whose diagonal entries are the eigenvalues of $A$ (repeated based on their respective multiplicities). Does it follow that $$ \lim_{k \to \infty} A^k = 0 $$ if each eigenvalue of $A$ is in the range $(-1, 1)$? My reasoning is that (from elementary linear algebra) we can show that $$ A = PD^kP^{-1} $$ if $A$ is diagonalizable. Since $P$ and $P^{-1}$ are finite, the product should approach zero since each (diagonal) entry of $D$ will approach zero since $$ \lim_{k \to \infty} \lambda^k = 0 $$ if $-1 < \lambda < 1$. Does this make sense or is my reasoning flawed?",,"['linear-algebra', 'limits', 'diagonalization']"
59,Geometric meaning of minors,Geometric meaning of minors,,This is a bit silly question I found on another discussion forum. I know that determinants can be used to compute volumes of parallelepiped. I also know that determinants can be computed by linear combination of its minors. Is there any geometric meaning of minors or some proof/explanation why there is no such geometric meaning.,This is a bit silly question I found on another discussion forum. I know that determinants can be used to compute volumes of parallelepiped. I also know that determinants can be computed by linear combination of its minors. Is there any geometric meaning of minors or some proof/explanation why there is no such geometric meaning.,,"['linear-algebra', 'geometry', 'determinant']"
60,Why are quadratic forms so special and why not investigate in higher forms?,Why are quadratic forms so special and why not investigate in higher forms?,,"Ok, this is a soft question. If $K$ is a field of characteristic different from $2$, one can use the polarization identity to get a one-to-one correspondence between homogeneous polynomials of degree $2$ in $K[X_1,...X_n]$ variables, symmetric bilinear forms $K^n\times K^n\to K$, symmetric $n\times n$-matrices with $K$-entries. Quadratic forms are extensively studied. Why not to study 'cubic forms' or more generally '$n$-forms' in the same intensity? Perhaps one gets a correspondence of cubic forms with $3$-dimensional $n\times n\times n$-matrices. Is it just not investigated so much as quadratic forms just because these 'higer dimensional matrices' are more difficult to handle?","Ok, this is a soft question. If $K$ is a field of characteristic different from $2$, one can use the polarization identity to get a one-to-one correspondence between homogeneous polynomials of degree $2$ in $K[X_1,...X_n]$ variables, symmetric bilinear forms $K^n\times K^n\to K$, symmetric $n\times n$-matrices with $K$-entries. Quadratic forms are extensively studied. Why not to study 'cubic forms' or more generally '$n$-forms' in the same intensity? Perhaps one gets a correspondence of cubic forms with $3$-dimensional $n\times n\times n$-matrices. Is it just not investigated so much as quadratic forms just because these 'higer dimensional matrices' are more difficult to handle?",,"['linear-algebra', 'soft-question', 'quadratic-forms']"
61,Best Books to learn Proof-Based Linear Algebra and Matrices [duplicate],Best Books to learn Proof-Based Linear Algebra and Matrices [duplicate],,"This question already has answers here : Where to start learning Linear Algebra? [closed] (16 answers) Prerequisites/Books for A First Course in Linear Algebra (6 answers) Closed last year . So I'm in a really serious problem. It's my first year at university and I'm doing a CS major. The math is already getting serious and I'm lost, really lost. It's all about matrices so far and the thing is I really can't do the proofs (of determinants). High school(A-level) was math was pie and it didn't even involve any proofs and that's where I'm lacking now and I'm stressed out. I don't know where to start. So I went online and looked at all the books that have been listed around and  here are some I found: Linear Algebra Done Right - Axler Intro. to Linear algebra - Gilbert Strang Linear Algebra - Hoffman and Kunze These are some, but I'm not sure where to start. I need a book which will teach me some basic proofs and how to think to solve these proofs. So anyone could help me  out, where to start?","This question already has answers here : Where to start learning Linear Algebra? [closed] (16 answers) Prerequisites/Books for A First Course in Linear Algebra (6 answers) Closed last year . So I'm in a really serious problem. It's my first year at university and I'm doing a CS major. The math is already getting serious and I'm lost, really lost. It's all about matrices so far and the thing is I really can't do the proofs (of determinants). High school(A-level) was math was pie and it didn't even involve any proofs and that's where I'm lacking now and I'm stressed out. I don't know where to start. So I went online and looked at all the books that have been listed around and  here are some I found: Linear Algebra Done Right - Axler Intro. to Linear algebra - Gilbert Strang Linear Algebra - Hoffman and Kunze These are some, but I'm not sure where to start. I need a book which will teach me some basic proofs and how to think to solve these proofs. So anyone could help me  out, where to start?",,"['linear-algebra', 'matrices', 'reference-request', 'book-recommendation']"
62,"In Levenberg–Marquardt, is forcing the Hessian to be positive definite OK?","In Levenberg–Marquardt, is forcing the Hessian to be positive definite OK?",,"I am often doing parameter estimation using the Levenberg-Marquardt method, which involves solving the following linear system at each step: $$(H + \lambda I) \delta = r_{i}$$ where $H$ is a square Hessian matrix, $I$ is an identity matrix, $r_{i}$ is residual vector (at $i$ -th iteration), $\lambda$ is a damping factor, $\delta$ is improvement step to compute. The $\lambda$ value is decreased when the step improved solution (reduced objective value) and increased otherwise. The $\lambda$ parameter can allow solving ill-posed problems as it makes the Hessian positive definite. In most cases $H$ is positive definite by itself, but sometimes not. What to do in that case? Should I stop the iteration completely or increase lambda until $H$ becomes positive definite and solve the problem normally?","I am often doing parameter estimation using the Levenberg-Marquardt method, which involves solving the following linear system at each step: where is a square Hessian matrix, is an identity matrix, is residual vector (at -th iteration), is a damping factor, is improvement step to compute. The value is decreased when the step improved solution (reduced objective value) and increased otherwise. The parameter can allow solving ill-posed problems as it makes the Hessian positive definite. In most cases is positive definite by itself, but sometimes not. What to do in that case? Should I stop the iteration completely or increase lambda until becomes positive definite and solve the problem normally?",(H + \lambda I) \delta = r_{i} H I r_{i} i \lambda \delta \lambda \lambda H H,"['linear-algebra', 'optimization', 'least-squares', 'numerical-optimization']"
63,Show that $T-iI$ is invertible when $T$ is self-adjoint,Show that  is invertible when  is self-adjoint,T-iI T,"Let $T$ be a self adjoint operator on a finite dimensional inner product space $V$.    Then $ \| T(x)\pm ix \|^2=\| T(x) \|^2+\| x\|^2$ for all $x \in V$.   Deduce that $T-iI$ is inverible. Since $\| T(x)\pm ix \|=0$ iff $\| T(x)\|=0$ and $\| x \|=0$ and it means $N(T)=0$ and it is one-to-one.   Since $V$ is finite dimensional, $T$ is onto. Thus $T$ is invertible.    Hence, $T-iI$  is invertible. Is this proof complete?","Let $T$ be a self adjoint operator on a finite dimensional inner product space $V$.    Then $ \| T(x)\pm ix \|^2=\| T(x) \|^2+\| x\|^2$ for all $x \in V$.   Deduce that $T-iI$ is inverible. Since $\| T(x)\pm ix \|=0$ iff $\| T(x)\|=0$ and $\| x \|=0$ and it means $N(T)=0$ and it is one-to-one.   Since $V$ is finite dimensional, $T$ is onto. Thus $T$ is invertible.    Hence, $T-iI$  is invertible. Is this proof complete?",,['linear-algebra']
64,Can you define a vector space in terms of a pre-existing projective space?,Can you define a vector space in terms of a pre-existing projective space?,,"Projective spaces are usually defined as the quotient of a vector space (by the equivalence relation that identifies collinear vectors). However, in my opinion, projective spaces seem intuitively less structured and more primitive than vector spaces. Is there a way to reverse the order of definition? Projective spaces can be defined axiomatically (albeit in a more geometric way than vector spaces). Can you extract the structure of a vector space from an underlying projective space?","Projective spaces are usually defined as the quotient of a vector space (by the equivalence relation that identifies collinear vectors). However, in my opinion, projective spaces seem intuitively less structured and more primitive than vector spaces. Is there a way to reverse the order of definition? Projective spaces can be defined axiomatically (albeit in a more geometric way than vector spaces). Can you extract the structure of a vector space from an underlying projective space?",,"['linear-algebra', 'vector-spaces', 'definition', 'projective-geometry', 'projective-space']"
65,small rank solution of a matrix equation,small rank solution of a matrix equation,,"Consider the matrix equation $$AX-XA = R$$ where $A$ and $R$ are given square matrices such that $\operatorname{rank}(R)=r$. How to establish conditions (necessary, sufficient, or both) on $A$ and (or) on $R$ which ensure that there exists a solution $X$ of rank smaller or equal than $r$? Some observations: The equation involves the commutator $[A,X]=AX-XA$ between $A$ and $X$, thus the solution can not be unique. In fact if $X_0$ is a particular solution, than all the possible solutions are of the form $$X_0 + B$$ where $B$ is a matrix which commutes with $A$, i.e. $[A,B]=O$. If we assume that $A$ is normal with simple eigenvalues, then a unitary $U$ and an invertible diagonal $D$ exist such that $A=UDU^*$. Therefore, we can vectorize the equation as $$(U\otimes U)(D\otimes I -I \otimes D)(U\otimes U)^* vec(X)=vec(R)$$ The $n^2 \times n^2$ diagonal matrix $D\otimes I -I \otimes D$ has exactly $n$ zero eigenvalues. Does this ""vectorized"" equation ($n^2$ linear system) give us some more information? What can we ask about $D$  to ensure that a solution $vec(X)$,  or equivalently $(U\otimes U)^*vec(X)$, has rank less or equal than $r$?","Consider the matrix equation $$AX-XA = R$$ where $A$ and $R$ are given square matrices such that $\operatorname{rank}(R)=r$. How to establish conditions (necessary, sufficient, or both) on $A$ and (or) on $R$ which ensure that there exists a solution $X$ of rank smaller or equal than $r$? Some observations: The equation involves the commutator $[A,X]=AX-XA$ between $A$ and $X$, thus the solution can not be unique. In fact if $X_0$ is a particular solution, than all the possible solutions are of the form $$X_0 + B$$ where $B$ is a matrix which commutes with $A$, i.e. $[A,B]=O$. If we assume that $A$ is normal with simple eigenvalues, then a unitary $U$ and an invertible diagonal $D$ exist such that $A=UDU^*$. Therefore, we can vectorize the equation as $$(U\otimes U)(D\otimes I -I \otimes D)(U\otimes U)^* vec(X)=vec(R)$$ The $n^2 \times n^2$ diagonal matrix $D\otimes I -I \otimes D$ has exactly $n$ zero eigenvalues. Does this ""vectorized"" equation ($n^2$ linear system) give us some more information? What can we ask about $D$  to ensure that a solution $vec(X)$,  or equivalently $(U\otimes U)^*vec(X)$, has rank less or equal than $r$?",,"['linear-algebra', 'matrices', 'numerical-linear-algebra']"
66,Cross-ratio relations,Cross-ratio relations,,"The way I define the cross-ratio in projectve geometry: Let $P_0,P_1,P_2,P_3$ being four points on a projective line G, such that $P_0,P_1,P_2$ are pairwise distinct. Let $\pi:\mathbb KP^1\rightarrow G$ the unique projective map $\pi([1:0])=P_0,\pi([0:1])=P_1, \pi([1:1])=P_2, \pi([x_0:x_1])=P_3$ The cross-ratio $CR(P_0,P_1,P_2,P_3):=\frac{x_1}{x_0}$ Question: Is there a fast way showing the relations $CR(P_1,P_0,P_2,P_3)=\frac{1}{CR(P_0,P_1,P_2,P_3)}=CR(P_0,P_1,P_3,P_2)$ and $CR(P_2,P_1,P_0,P_3)=1-CR(P_0,P_1,P_2,P_3)=CR(P_0,P_3,P_2,P_1)$","The way I define the cross-ratio in projectve geometry: Let $P_0,P_1,P_2,P_3$ being four points on a projective line G, such that $P_0,P_1,P_2$ are pairwise distinct. Let $\pi:\mathbb KP^1\rightarrow G$ the unique projective map $\pi([1:0])=P_0,\pi([0:1])=P_1, \pi([1:1])=P_2, \pi([x_0:x_1])=P_3$ The cross-ratio $CR(P_0,P_1,P_2,P_3):=\frac{x_1}{x_0}$ Question: Is there a fast way showing the relations $CR(P_1,P_0,P_2,P_3)=\frac{1}{CR(P_0,P_1,P_2,P_3)}=CR(P_0,P_1,P_3,P_2)$ and $CR(P_2,P_1,P_0,P_3)=1-CR(P_0,P_1,P_2,P_3)=CR(P_0,P_3,P_2,P_1)$",,"['linear-algebra', 'projective-geometry', 'projective-space', 'affine-geometry']"
67,Complexity of a quadratic program,Complexity of a quadratic program,,"I have a quadratic program: $$\displaystyle\min_{\mathbf{X}} (\mathbf{X^TQX +C^TX}) \quad{} \text{subject to} \quad{} \mathbf{A X \leq Y}$$ $\mathbf{Q}$ is positive definite and is $N \times N$, $\mathbf{A}$ is $M \times N$ and $\mathbf{X}$ is an $N \times 1$ vector. I'm trying to compute the complexity in terms of multiplications, but can not figure out how to approach it. I'd appreciate it if anyone can provide help/guidance or any references I can check. I'm using the interior point convex algorithm.","I have a quadratic program: $$\displaystyle\min_{\mathbf{X}} (\mathbf{X^TQX +C^TX}) \quad{} \text{subject to} \quad{} \mathbf{A X \leq Y}$$ $\mathbf{Q}$ is positive definite and is $N \times N$, $\mathbf{A}$ is $M \times N$ and $\mathbf{X}$ is an $N \times 1$ vector. I'm trying to compute the complexity in terms of multiplications, but can not figure out how to approach it. I'd appreciate it if anyone can provide help/guidance or any references I can check. I'm using the interior point convex algorithm.",,"['linear-algebra', 'optimization', 'computational-complexity']"
68,QR decomposition help,QR decomposition help,,What do Q and R stand for? Why must the diagonal entries of R be positive instead of just nonzero?,What do Q and R stand for? Why must the diagonal entries of R be positive instead of just nonzero?,,"['linear-algebra', 'matrices', 'numerical-linear-algebra']"
69,Obtaining the discriminant of the characteristic polynomial directly from the matrix,Obtaining the discriminant of the characteristic polynomial directly from the matrix,,"Let $M \in \mathbb{Z}_{n \times n}$ be a square matrix with integer coefficients. Let $P(x)$ be its characteristic polynomial $$     P(x) = \det\left(x \cdot \mathbb{I}_{n \times n}- M\right) $$ I would like to compute the discriminant of $P(x)$, and I am wondering if it can be obtained from $M$ directly. The intent is to determine whether $M$ has distinct eigenvalues. I am looking for references, ideas, algorithms. Thank you.","Let $M \in \mathbb{Z}_{n \times n}$ be a square matrix with integer coefficients. Let $P(x)$ be its characteristic polynomial $$     P(x) = \det\left(x \cdot \mathbb{I}_{n \times n}- M\right) $$ I would like to compute the discriminant of $P(x)$, and I am wondering if it can be obtained from $M$ directly. The intent is to determine whether $M$ has distinct eigenvalues. I am looking for references, ideas, algorithms. Thank you.",,"['linear-algebra', 'reference-request', 'polynomials']"
70,Matrix Chain Multiplication?,Matrix Chain Multiplication?,,"The following are questions about using dynamic programming for matrix chain multiplication. Pseudocode can be found in the Wikipedia article on matrix chain multiplication . 1) Why is the time complexity for trying all bracket permutations $\mathcal{O}(2^n)$, where $n$ is the number of matrices?? 2) Why are there $\frac{n^2}{2}$ unique subsequences? (This question is refering to memoization where all unique subproblems are stored. A rephrasing of this question would be: Why are there $\frac{n^2}{2}$ unique subproblems to be stored?) 3) Why does using memoization reduce the time to $O(n^3)$? I don't have made any progress on the above questions, therefore there is no work for me to show.","The following are questions about using dynamic programming for matrix chain multiplication. Pseudocode can be found in the Wikipedia article on matrix chain multiplication . 1) Why is the time complexity for trying all bracket permutations $\mathcal{O}(2^n)$, where $n$ is the number of matrices?? 2) Why are there $\frac{n^2}{2}$ unique subsequences? (This question is refering to memoization where all unique subproblems are stored. A rephrasing of this question would be: Why are there $\frac{n^2}{2}$ unique subproblems to be stored?) 3) Why does using memoization reduce the time to $O(n^3)$? I don't have made any progress on the above questions, therefore there is no work for me to show.",,['linear-algebra']
71,Standard terminology for the relation between $A$ and $B$ if $B= Q^t A P$?,Standard terminology for the relation between  and  if ?,A B B= Q^t A P,"Let $A,B$ be two rectangular $m\times n$ matrices related by  $$B= Q^t A P$$ with $P$ an $n\times n$ and $Q$ an $m\times m $ matrix. Is there a standard terminolgy for this relation? If instead of the transposed $Q^t$ one takes the inverse $Q^{-1}$ above, they are just called ""equivalent"" according to http://en.wikipedia.org/wiki/Matrix_equivalence I know that when $m=n$ ( Edit 1 : and P=Q) one uses ""congruent"" (transposed case)  and ""similar"" (inverse case). Edit 2 : $P$ and $Q$ are assumed both to be invertible (sorry for forgetting to write it). As Marc van Leeuwen pointed out, there is no point in distinguishing among the cases $Q$, $Q^t$ and $Q^{-1}$ since $Q$ is arbitrary ( and invertible). It only makes sense when interpreting $Q$ as coordinate change matrix and $A$ as a linear operator (   -> $Q^{-1}$) or bilinear form   (-> $Q^t$) (see explanations of Paul Garrett). Thanks to everybody who contributed to clarify my confused question.","Let $A,B$ be two rectangular $m\times n$ matrices related by  $$B= Q^t A P$$ with $P$ an $n\times n$ and $Q$ an $m\times m $ matrix. Is there a standard terminolgy for this relation? If instead of the transposed $Q^t$ one takes the inverse $Q^{-1}$ above, they are just called ""equivalent"" according to http://en.wikipedia.org/wiki/Matrix_equivalence I know that when $m=n$ ( Edit 1 : and P=Q) one uses ""congruent"" (transposed case)  and ""similar"" (inverse case). Edit 2 : $P$ and $Q$ are assumed both to be invertible (sorry for forgetting to write it). As Marc van Leeuwen pointed out, there is no point in distinguishing among the cases $Q$, $Q^t$ and $Q^{-1}$ since $Q$ is arbitrary ( and invertible). It only makes sense when interpreting $Q$ as coordinate change matrix and $A$ as a linear operator (   -> $Q^{-1}$) or bilinear form   (-> $Q^t$) (see explanations of Paul Garrett). Thanks to everybody who contributed to clarify my confused question.",,"['linear-algebra', 'matrices', 'terminology']"
72,Triangularizing a Matrix,Triangularizing a Matrix,,"In Hoffman & Kunze, it is mentioned several times that any matrix with a minimal polynomial that splits over your field into linear factors is similar to an upper triangular matrix.  A proof is given, but it does not give an algorithm for finding such a matrix.  Indeed, they prove that any such linear operator can be decomposed into the sum of a nilpotent linear operator and a diagonalizable linear operator which commute, and can therefore be simultaneously triangularized, but do not seem to give a direct method as to how to actually choose an appropriate basis in practice. I was wondering if there is an algorithm for triangularizing such an operator that does not rely on Jordan canonical form (since triangularization appears before Jordan form in their book) or noticing some clever choice of basis?","In Hoffman & Kunze, it is mentioned several times that any matrix with a minimal polynomial that splits over your field into linear factors is similar to an upper triangular matrix.  A proof is given, but it does not give an algorithm for finding such a matrix.  Indeed, they prove that any such linear operator can be decomposed into the sum of a nilpotent linear operator and a diagonalizable linear operator which commute, and can therefore be simultaneously triangularized, but do not seem to give a direct method as to how to actually choose an appropriate basis in practice. I was wondering if there is an algorithm for triangularizing such an operator that does not rely on Jordan canonical form (since triangularization appears before Jordan form in their book) or noticing some clever choice of basis?",,"['linear-algebra', 'matrices', 'matrix-decomposition', 'triangularization']"
73,Set of finite subsets as vector space: Double dual?,Set of finite subsets as vector space: Double dual?,,"Some problem I've found while thinking about duals of vector spaces: Be $S$ an arbitrary set. Denote by $F(S)$ the set of finite subsets of $S$, and by $P(S)$ its power set. Now it is easy to see that $F(S)$ forms a vector space over $\mathbb{Z}/2\mathbb{Z}$ if you define vector addition as symmetric difference and multiplication with scalar by the simple relations $0A=\emptyset$ and $1A=A$. A particular basis of that vector space is $\{\{s\}: s\in S\}$. From that, it is also easy to construct the dual space $F(S)^*$: Its members are simply given by the elements of $P(S)$ with the following application rule: If $\alpha\in P(S)$ and $v\in F(S)$, then $$\alpha(v) = \begin{cases}1 & \text{if $\alpha\cap v$ has an odd number of elements}\\0 & \text{if $\alpha\cap v$ has an even number of elements}\end{cases}$$ (or simply, express the number of elements in $\alpha\cup v$ in $\mathbb{Z}/2\mathbb{Z}$. It is also not hard to show that vector addition in $F(S)^*$ is again the symmetric difference. So one can say that, given the application rule above, $F(S)^* = P(S)$ Note that for finite $S$, $F(S)=P(S)$, while for infinite $S$, $F(S)$ is smaller than $P(S)$. So far, so good. However, what about the double-dual $F(S)^{**} = P(S)^*$? Since for finite $S$, $P(S)=F(S)$, one would conclude that the very same construction works again. However for infinite $S$, it cannot work, because you cannot say whether an infinite set has an even or odd number of elements. Also, $P(S)$ already contains all subsets of $S$, and as far as I understand, $P(S)^*$ should then be larger than $P(S)$. Therefore my question: Does there exist a simple representation of $F(S)^{**}=P(S)^*$, and if so, what does it look like? Ideally one should easily see the equivalence to $F(S)$ in the case of finite $S$.","Some problem I've found while thinking about duals of vector spaces: Be $S$ an arbitrary set. Denote by $F(S)$ the set of finite subsets of $S$, and by $P(S)$ its power set. Now it is easy to see that $F(S)$ forms a vector space over $\mathbb{Z}/2\mathbb{Z}$ if you define vector addition as symmetric difference and multiplication with scalar by the simple relations $0A=\emptyset$ and $1A=A$. A particular basis of that vector space is $\{\{s\}: s\in S\}$. From that, it is also easy to construct the dual space $F(S)^*$: Its members are simply given by the elements of $P(S)$ with the following application rule: If $\alpha\in P(S)$ and $v\in F(S)$, then $$\alpha(v) = \begin{cases}1 & \text{if $\alpha\cap v$ has an odd number of elements}\\0 & \text{if $\alpha\cap v$ has an even number of elements}\end{cases}$$ (or simply, express the number of elements in $\alpha\cup v$ in $\mathbb{Z}/2\mathbb{Z}$. It is also not hard to show that vector addition in $F(S)^*$ is again the symmetric difference. So one can say that, given the application rule above, $F(S)^* = P(S)$ Note that for finite $S$, $F(S)=P(S)$, while for infinite $S$, $F(S)$ is smaller than $P(S)$. So far, so good. However, what about the double-dual $F(S)^{**} = P(S)^*$? Since for finite $S$, $P(S)=F(S)$, one would conclude that the very same construction works again. However for infinite $S$, it cannot work, because you cannot say whether an infinite set has an even or odd number of elements. Also, $P(S)$ already contains all subsets of $S$, and as far as I understand, $P(S)^*$ should then be larger than $P(S)$. Therefore my question: Does there exist a simple representation of $F(S)^{**}=P(S)^*$, and if so, what does it look like? Ideally one should easily see the equivalence to $F(S)$ in the case of finite $S$.",,['linear-algebra']
74,Solving $Ax = B$ when $A$ has a large condition number.,Solving  when  has a large condition number.,Ax = B A,"There's two parts to this problem the first is implementation related the second is theoretical. Part 1:  I've been given $A$ & $B$ and need to solve for $x$. To do this I've been using an SVD. The problem is when $A$ has a large condition number the SVD loses enough precision that returned matrix becomes useless (filled with nan values). The condition number is quotient of the absolute value of largest and smallest eigenvalues. Part of the reason the condition number is so large is the min value is on the order of $10^{-19}$. I used the Jacobi method to calculate the eigenvalues of $A$. Both the SVD & Jacobi implementations come from Numerical Recipes in C 3rd ed. I've heard that you can threshold the SVD to ignore such small values but can't find reference to it in the NR and LAPACK implementations or on papers on SVD implementation. Am I missing something does this not exist? Part 2: What other methods are used to solve $Ax = B$? Thanks, Jon","There's two parts to this problem the first is implementation related the second is theoretical. Part 1:  I've been given $A$ & $B$ and need to solve for $x$. To do this I've been using an SVD. The problem is when $A$ has a large condition number the SVD loses enough precision that returned matrix becomes useless (filled with nan values). The condition number is quotient of the absolute value of largest and smallest eigenvalues. Part of the reason the condition number is so large is the min value is on the order of $10^{-19}$. I used the Jacobi method to calculate the eigenvalues of $A$. Both the SVD & Jacobi implementations come from Numerical Recipes in C 3rd ed. I've heard that you can threshold the SVD to ignore such small values but can't find reference to it in the NR and LAPACK implementations or on papers on SVD implementation. Am I missing something does this not exist? Part 2: What other methods are used to solve $Ax = B$? Thanks, Jon",,"['linear-algebra', 'reference-request']"
75,what are the conditions for the product of 2 symmetric matrices being symmetric,what are the conditions for the product of 2 symmetric matrices being symmetric,,"In generally, the product of two symmetric matrices is not symmetric, so I am wondering under what conditions the product is symmetric. Likewise, over complex space, what are the conditions for the product of 2 Hermitian matrices being Hermitian? Thanks!","In generally, the product of two symmetric matrices is not symmetric, so I am wondering under what conditions the product is symmetric. Likewise, over complex space, what are the conditions for the product of 2 Hermitian matrices being Hermitian? Thanks!",,"['linear-algebra', 'matrices']"
76,Dimension of the range of $T$ is equal to the codimension of $\ker T$,Dimension of the range of  is equal to the codimension of,T \ker T,"Given a linear transformation, $T:U\rightarrow V$ , I am asked to show that the dimension of the range of $T$ is the same as the codimension of the kernel of $T$. I am told that $U$ is not necessarily a finite dimensional vector space so I cannot assume that the dimension theory holds. As a matter of fact, I know nothing about codimensions and so I have no idea how to go about this question. I need some help.","Given a linear transformation, $T:U\rightarrow V$ , I am asked to show that the dimension of the range of $T$ is the same as the codimension of the kernel of $T$. I am told that $U$ is not necessarily a finite dimensional vector space so I cannot assume that the dimension theory holds. As a matter of fact, I know nothing about codimensions and so I have no idea how to go about this question. I need some help.",,['linear-algebra']
77,What are applications of Frobenius inequality?,What are applications of Frobenius inequality?,,Frobenius inequality states that $\operatorname{rank} AB + \operatorname{rank} BC \leq \operatorname{rank} ABC + \operatorname{rank} B$ whenever this has a meaning. I remember being told that this was sometimes useful. Do you know of any example? Thank you.,Frobenius inequality states that whenever this has a meaning. I remember being told that this was sometimes useful. Do you know of any example? Thank you.,\operatorname{rank} AB + \operatorname{rank} BC \leq \operatorname{rank} ABC + \operatorname{rank} B,"['linear-algebra', 'matrices']"
78,Non-degenerate alternating bilinear form on a finite abelian group,Non-degenerate alternating bilinear form on a finite abelian group,,"Ciao! Let $A$ be a finite abelian group, and let $ \psi : A \times A \to \mathbb{Q}/\mathbb{Z} $ be an alternating, non-degenerate bilinear form on $A$. Maybe I should say what I mean by these words; bilinear means it is linear in each argument separately; alternating means that $\psi(a,a) = 0$ for all $a$; non-degenerate means that, if $\psi(a,b) = 0$ for all $b$, then $a$ must be $0$. Why must $A$ have square cardinality? I believe it will follow from the following theorem in Linear algebra: Theorem. Let $V$ be a finite dimensional vector space over a field $K$ that has an alternating, non-degenerate bilinear form on it (from $V \times V \to K$). Then dim $V$ is even. My idea was to proceed as follows: If the size of $A$ is not square, then for some prime $p$, $A(p)$ is not square, where $A(p)$ means the $p$-primary part of $A$. The original $\psi$ induces a map on $A(p)$ that is non-degenerate, alternating and bilinear. I then wanted to say that $A(p)$ is an $\mathbb{F}_p$-vector space, and then applying the theorem I am done, but this is not true, e.g, $\mathbb{Z}/25\mathbb{Z}$ is not an $\mathbb{F}_5$-vector space. Any pointers anyone?","Ciao! Let $A$ be a finite abelian group, and let $ \psi : A \times A \to \mathbb{Q}/\mathbb{Z} $ be an alternating, non-degenerate bilinear form on $A$. Maybe I should say what I mean by these words; bilinear means it is linear in each argument separately; alternating means that $\psi(a,a) = 0$ for all $a$; non-degenerate means that, if $\psi(a,b) = 0$ for all $b$, then $a$ must be $0$. Why must $A$ have square cardinality? I believe it will follow from the following theorem in Linear algebra: Theorem. Let $V$ be a finite dimensional vector space over a field $K$ that has an alternating, non-degenerate bilinear form on it (from $V \times V \to K$). Then dim $V$ is even. My idea was to proceed as follows: If the size of $A$ is not square, then for some prime $p$, $A(p)$ is not square, where $A(p)$ means the $p$-primary part of $A$. The original $\psi$ induces a map on $A(p)$ that is non-degenerate, alternating and bilinear. I then wanted to say that $A(p)$ is an $\mathbb{F}_p$-vector space, and then applying the theorem I am done, but this is not true, e.g, $\mathbb{Z}/25\mathbb{Z}$ is not an $\mathbb{F}_5$-vector space. Any pointers anyone?",,"['linear-algebra', 'abstract-algebra', 'abelian-groups']"
79,Inequality involving a symmetric matrix and minors of an orthogonal matrix,Inequality involving a symmetric matrix and minors of an orthogonal matrix,,"Fix $n \geq 3$ and take any orthonormal vectors $x,y,z \in \mathbb{R}^n$ . Let also $A \in M_n(\mathbb{R})$ be a symmetric matrix with positive entries ( $A_{ij} = A_{ji} > 0$ ). Is the following inequality true? $$\sum_{i<j}A_{ij}\begin{vmatrix}x_i & x_j \newline y_i & y_j \end{vmatrix}^2 \leq \sum_{i<j<k}\max(A_{ij},A_{ik},A_{jk})\begin{vmatrix}x_i & x_j & x_k \newline y_i & y_j & y_k \newline z_i & z_j & z_k \end{vmatrix}^2$$ For $n=3$ the proof is easy: The sum on the LHS has just three terms and one can simply write $$A_{12}\begin{vmatrix}x_1 & x_2 \newline y_1 & y_2 \end{vmatrix}^2 + A_{13}\begin{vmatrix}x_1 & x_3 \newline y_1 & y_3 \end{vmatrix}^2 + A_{23}\begin{vmatrix}x_2 & x_3 \newline y_2 & y_3 \end{vmatrix}^2 \leq \max(A_{12},A_{13},A_{23})\Bigg(\begin{vmatrix}x_1 & x_2 \newline y_1 & y_2 \end{vmatrix}^2 + \begin{vmatrix}x_1 & x_3 \newline y_1 & y_3 \end{vmatrix}^2 + \begin{vmatrix}x_2 & x_3 \newline y_2 & y_3 \end{vmatrix}^2\Bigg) = \max(A_{12},A_{13},A_{23}) = RHS$$ Numerical experiments suggest that the inequality holds for $n>3$ as well, but how to prove it? Any hint would be much appreciated!","Fix and take any orthonormal vectors . Let also be a symmetric matrix with positive entries ( ). Is the following inequality true? For the proof is easy: The sum on the LHS has just three terms and one can simply write Numerical experiments suggest that the inequality holds for as well, but how to prove it? Any hint would be much appreciated!","n \geq 3 x,y,z \in \mathbb{R}^n A \in M_n(\mathbb{R}) A_{ij} = A_{ji} > 0 \sum_{i<j}A_{ij}\begin{vmatrix}x_i & x_j \newline y_i & y_j \end{vmatrix}^2 \leq \sum_{i<j<k}\max(A_{ij},A_{ik},A_{jk})\begin{vmatrix}x_i & x_j & x_k \newline y_i & y_j & y_k \newline z_i & z_j & z_k \end{vmatrix}^2 n=3 A_{12}\begin{vmatrix}x_1 & x_2 \newline y_1 & y_2 \end{vmatrix}^2 + A_{13}\begin{vmatrix}x_1 & x_3 \newline y_1 & y_3 \end{vmatrix}^2 + A_{23}\begin{vmatrix}x_2 & x_3 \newline y_2 & y_3 \end{vmatrix}^2 \leq \max(A_{12},A_{13},A_{23})\Bigg(\begin{vmatrix}x_1 & x_2 \newline y_1 & y_2 \end{vmatrix}^2 + \begin{vmatrix}x_1 & x_3 \newline y_1 & y_3 \end{vmatrix}^2 + \begin{vmatrix}x_2 & x_3 \newline y_2 & y_3 \end{vmatrix}^2\Bigg) = \max(A_{12},A_{13},A_{23}) = RHS n>3","['linear-algebra', 'inequality', 'determinant', 'symmetric-matrices', 'orthonormal']"
80,Proof of first step in SVD,Proof of first step in SVD,,"I have to proof the following statement: Prove that for a given $A\in \mathcal M _{n \times m}(\mathbb R)$ there exist two orthogonal matrices $U \in \mathcal O(n)$ , $V \in \mathcal O(m)$ such that: $$UAV^T=\begin{pmatrix}||A||_2 & 0\\ 0 & A_1 \end{pmatrix},$$ where $||A||_2$ is the 2 norm of the matrix and $A_1\in \mathcal M _{n-1 \times m-1}(\mathbb R)$ . The professor told me to use Gram-Schmidt orthonormalization process as a hint. My try so far has been the following. I need to find two rotation matrices that rewrite $A$ in the way above. To find the first one, I've thought of generating a set of orthonormal vectors $\{v_1, v_2, ..., v_m\}$ where $v_1\in \mathbb R^m$ is a vector that fulfills $||Av_1||_2=||A||_2$ . To find the second matrix, I've thought of using another rotation matrix that changes from the canonic basis in $\mathbb R^n$ to a orthogonal basis containing $\frac{Av_1}{||Av_1||_2}=\frac{Av_1}{||A||_2}$ as one of the vectors. This way, I have two orthogonal matrices that applyed to the original matrix give the first column as I want it to be. However, I can't find a way to prove that the rest of the first row has to be al zeros other than the first element. I think the solution has something to do with which linearly independent vectors $\{x_1,..., x_m\}$ I choose to perform Gram-Schmidt. If i could somehow find a way to find those vectors sutch that $Av_1 \perp \{Av_2, ..., Av_m\}$ id then have that the rest of the first row are zeros too.","I have to proof the following statement: Prove that for a given there exist two orthogonal matrices , such that: where is the 2 norm of the matrix and . The professor told me to use Gram-Schmidt orthonormalization process as a hint. My try so far has been the following. I need to find two rotation matrices that rewrite in the way above. To find the first one, I've thought of generating a set of orthonormal vectors where is a vector that fulfills . To find the second matrix, I've thought of using another rotation matrix that changes from the canonic basis in to a orthogonal basis containing as one of the vectors. This way, I have two orthogonal matrices that applyed to the original matrix give the first column as I want it to be. However, I can't find a way to prove that the rest of the first row has to be al zeros other than the first element. I think the solution has something to do with which linearly independent vectors I choose to perform Gram-Schmidt. If i could somehow find a way to find those vectors sutch that id then have that the rest of the first row are zeros too.","A\in \mathcal M _{n \times m}(\mathbb R) U \in \mathcal O(n) V \in \mathcal O(m) UAV^T=\begin{pmatrix}||A||_2 & 0\\ 0 & A_1 \end{pmatrix}, ||A||_2 A_1\in \mathcal M _{n-1 \times m-1}(\mathbb R) A \{v_1, v_2, ..., v_m\} v_1\in \mathbb R^m ||Av_1||_2=||A||_2 \mathbb R^n \frac{Av_1}{||Av_1||_2}=\frac{Av_1}{||A||_2} \{x_1,..., x_m\} Av_1 \perp \{Av_2, ..., Av_m\}","['linear-algebra', 'matrices', 'linear-transformations', 'svd']"
81,(Real) Vector space axioms: Simple example for necessity of axiom of associativity of multiplication,(Real) Vector space axioms: Simple example for necessity of axiom of associativity of multiplication,,"I'm writing some notes on Linear Algebra and am trying to show the independence of an ""optimal list"" for vector space axioms. I am using the following definition: A vector space is a structure $(V,0,+,\cdot)$ , where $V$ is a set, $0$ is an element of $V$ and $+\colon V\times V\to V$ and $\cdot\colon\mathbb{R}\times V\to\mathbb{V}$ are binary operations such that $v+(u+w)=(v+u)+w$ $v+0=v$ For all $v$ there exists $w$ such that $v+w=0$ . $\alpha(\beta v)=(\alpha\beta)v$ $\alpha(v+w)=\alpha v+\alpha w$ . $(\alpha+\beta)v=\alpha v+\beta v$ $1v=v$ Question : Is there a simple example of a structure which satisfies all axioms except only 4? A few remarks: I used just the ""right-identity/right-inverses"" version of the group axioms in 1.-3., which is well-known to be sufficient for having a group ( see here ) I added "" $0$ "" as a constant because axiom 3. depends on the given $0$ which satisfies 2. This is a way to avoid the usual ambiguities with the way axiom 3. is commonly stated ( see this question and the accepted answer ) It is well known that commutativity is implied by the other axioms (ignoring 4., even), so I ommited it. I am considering only real vector spaces. (So the example in this answer , for example, does not apply) For each axiom in the list except 5. , I have already managed to find a structure which satisfies all other axioms except it. However, my example for axiom 4. is quite complicated: Let $V=\mathbb{R}$ as a $\mathbb{Q}$ -vector space, and $L_{\mathbb{Q}}(\mathbb{R})$ be the $\mathbb{Q}$ -vector space of $\mathbb{Q}$ -endomorphisms of $\mathbb{R}$ . Take any $\mathbb{Q}$ -linear map $\Phi\colon\mathbb{R}\to L_\mathbb{Q}(\mathbb{R})$ which takes $1$ to the identity and $\sqrt{2}$ to the zero map (which can be done by difining $\Phi$ on a basis of $\mathbb{R}$ over $\mathbb{Q}$ which contains $1$ and $\sqrt{2}$ ), and let $\lambda x = \Phi(\lambda)(x)$ for $\lambda\in\mathbb{R}$ and $x\in V$ . Then axioms 1.-3. are immediate for $V$ ; axiom 5. follows from $\Phi(\alpha)$ being $\mathbb{Q}$ -linear (additive) for each $\alpha$ , 6. follows from $\Phi$ itself being $\mathbb{Q}$ -linear, and 7. from $\Phi(1)$ being the identity. Axiom 4. is not true: Taking $\alpha=\beta=\sqrt{2}$ and $v=1$ , we have $\alpha(\beta v)=0$ but $(\alpha\beta)v=2v=2$ . However, this example depends on the axiom of choice (for having a basis of $\mathbb{R}$ as a $\mathbb{Q}$ -vector space) and some linear algebraic facts as well. If we allowed for vector spaces over other fields, then this would give us an example which does not depend on choice simply by taking $\mathbb{Q}[\sqrt{2}]$ instead of $\mathbb{R}$ , but this is not what I want. Is there any other simpler example of a structure which satisfies all axioms except 4.? The distributivity axioms actually allow us to prove that such a given structure will be a divisible group , and that in fact the product will be associative for integers (although not necessarily for rationals). It's also not clear to me whether the subjacent group will be torsion-free.","I'm writing some notes on Linear Algebra and am trying to show the independence of an ""optimal list"" for vector space axioms. I am using the following definition: A vector space is a structure , where is a set, is an element of and and are binary operations such that For all there exists such that . . Question : Is there a simple example of a structure which satisfies all axioms except only 4? A few remarks: I used just the ""right-identity/right-inverses"" version of the group axioms in 1.-3., which is well-known to be sufficient for having a group ( see here ) I added "" "" as a constant because axiom 3. depends on the given which satisfies 2. This is a way to avoid the usual ambiguities with the way axiom 3. is commonly stated ( see this question and the accepted answer ) It is well known that commutativity is implied by the other axioms (ignoring 4., even), so I ommited it. I am considering only real vector spaces. (So the example in this answer , for example, does not apply) For each axiom in the list except 5. , I have already managed to find a structure which satisfies all other axioms except it. However, my example for axiom 4. is quite complicated: Let as a -vector space, and be the -vector space of -endomorphisms of . Take any -linear map which takes to the identity and to the zero map (which can be done by difining on a basis of over which contains and ), and let for and . Then axioms 1.-3. are immediate for ; axiom 5. follows from being -linear (additive) for each , 6. follows from itself being -linear, and 7. from being the identity. Axiom 4. is not true: Taking and , we have but . However, this example depends on the axiom of choice (for having a basis of as a -vector space) and some linear algebraic facts as well. If we allowed for vector spaces over other fields, then this would give us an example which does not depend on choice simply by taking instead of , but this is not what I want. Is there any other simpler example of a structure which satisfies all axioms except 4.? The distributivity axioms actually allow us to prove that such a given structure will be a divisible group , and that in fact the product will be associative for integers (although not necessarily for rationals). It's also not clear to me whether the subjacent group will be torsion-free.","(V,0,+,\cdot) V 0 V +\colon V\times V\to V \cdot\colon\mathbb{R}\times V\to\mathbb{V} v+(u+w)=(v+u)+w v+0=v v w v+w=0 \alpha(\beta v)=(\alpha\beta)v \alpha(v+w)=\alpha v+\alpha w (\alpha+\beta)v=\alpha v+\beta v 1v=v 0 0 V=\mathbb{R} \mathbb{Q} L_{\mathbb{Q}}(\mathbb{R}) \mathbb{Q} \mathbb{Q} \mathbb{R} \mathbb{Q} \Phi\colon\mathbb{R}\to L_\mathbb{Q}(\mathbb{R}) 1 \sqrt{2} \Phi \mathbb{R} \mathbb{Q} 1 \sqrt{2} \lambda x = \Phi(\lambda)(x) \lambda\in\mathbb{R} x\in V V \Phi(\alpha) \mathbb{Q} \alpha \Phi \mathbb{Q} \Phi(1) \alpha=\beta=\sqrt{2} v=1 \alpha(\beta v)=0 (\alpha\beta)v=2v=2 \mathbb{R} \mathbb{Q} \mathbb{Q}[\sqrt{2}] \mathbb{R}","['linear-algebra', 'axioms']"
82,What matrices $M$ satisfy $M_{il} M_{jm} M_{kn} \delta_{lmn} = \delta_{ijk}$?,What matrices  satisfy ?,M M_{il} M_{jm} M_{kn} \delta_{lmn} = \delta_{ijk},"I'm trying to better understand matrices that are defined by preserving some other tensor. I recently asked a more involved question , and I realized it might be better to learn simpler examples first. As an example, consider the set of $n$ by $n$ real matrices $M$ that satisfy $M_{ik} M_{jl} \delta_{kl} = \delta_{ik}$ . Here $\delta_{ij}$ is the $n \times n$ Kronecker delta. This is merely a restatement of $M M^T = I$ which is the defining relation of orthogonal matrices. By exploring $M$ close to the identity, we can deduce that up to a reflection, $M$ will be the exponential of a real antisymmetric matrix. I want to understand generalizations where the $M$ 's instead preserve some trilinear form, not a bilinear one. Consider for example $\delta_{ijk}$ , the $n\times n \times n$ tensor that is $1$ if $i=j=k$ and $0$ otherwise. How can we characterize the set of real matrices $M$ that satisfy $M_{il} M_{jm} M_{kn} \delta_{lmn} = \delta_{ijk}$ ? In particular, is a complete enumeration of these $M$ 's known? By inspection, one can see that if $M_1$ and $M_2$ satisfy this constraint, then so does $M_1 M_2$ . Similarly, if $M$ satisfies this constraint and is invertible, then so too does $M^{-1}$ . However, I'm finding it hard to say general statements about the properties of matrices $M$ satisfying $M_{il} M_{jm} M_{kn} \delta_{lmn} = \delta_{ijk}$ . For example, it's not even obvious to me whether all $M$ that satisfy this constraint must be invertible. By inspection, we can deduce some explicit solutions. If $M$ is a permutation matrix, it will satisfy the above equation. However, I'm having trouble guessing any more real matrices that satisfy the constraint. As an aside, if I weaken the constraint and allow $M$ to be complex, then note that I get additional solutions from the set of diagonal $M$ whose cube is the identity, but again I suspect I might be missing solutions. I suspect and hope that there are techniques that will give a complete enumeration of the kinds of matrices $M$ satisfying this constraint.","I'm trying to better understand matrices that are defined by preserving some other tensor. I recently asked a more involved question , and I realized it might be better to learn simpler examples first. As an example, consider the set of by real matrices that satisfy . Here is the Kronecker delta. This is merely a restatement of which is the defining relation of orthogonal matrices. By exploring close to the identity, we can deduce that up to a reflection, will be the exponential of a real antisymmetric matrix. I want to understand generalizations where the 's instead preserve some trilinear form, not a bilinear one. Consider for example , the tensor that is if and otherwise. How can we characterize the set of real matrices that satisfy ? In particular, is a complete enumeration of these 's known? By inspection, one can see that if and satisfy this constraint, then so does . Similarly, if satisfies this constraint and is invertible, then so too does . However, I'm finding it hard to say general statements about the properties of matrices satisfying . For example, it's not even obvious to me whether all that satisfy this constraint must be invertible. By inspection, we can deduce some explicit solutions. If is a permutation matrix, it will satisfy the above equation. However, I'm having trouble guessing any more real matrices that satisfy the constraint. As an aside, if I weaken the constraint and allow to be complex, then note that I get additional solutions from the set of diagonal whose cube is the identity, but again I suspect I might be missing solutions. I suspect and hope that there are techniques that will give a complete enumeration of the kinds of matrices satisfying this constraint.",n n M M_{ik} M_{jl} \delta_{kl} = \delta_{ik} \delta_{ij} n \times n M M^T = I M M M \delta_{ijk} n\times n \times n 1 i=j=k 0 M M_{il} M_{jm} M_{kn} \delta_{lmn} = \delta_{ijk} M M_1 M_2 M_1 M_2 M M^{-1} M M_{il} M_{jm} M_{kn} \delta_{lmn} = \delta_{ijk} M M M M M,"['linear-algebra', 'matrices', 'tensors', 'orthogonal-matrices']"
83,Gershgorin Circle Theorem in Infinite Dimensional Vector Spaces,Gershgorin Circle Theorem in Infinite Dimensional Vector Spaces,,Do there exist any generalizations of the Gershgorin Circle Theorem to infinite dimensional vector spaces?,Do there exist any generalizations of the Gershgorin Circle Theorem to infinite dimensional vector spaces?,,"['linear-algebra', 'reference-request']"
84,Geometric interpretation of a non-symmetric matrix having only real eigenvalues?,Geometric interpretation of a non-symmetric matrix having only real eigenvalues?,,"Is there a geometric interpretation of a non-symmetric matrix having only real eigenvalues? It appears that multiplying random matrices with IID random entries eventually produces a matrix with only real eigenvalues, wondering if this can be turned into a statement about how random linear maps transform a vector. For instance, code below multiples $10\ 2\times 2$ random matrices, eigenvalues are almost always real. n = 2; depth = 10; dist = NormalDistribution[]; sample := RandomVariate[dist, {n, n}]/Sqrt[n]; sampled := Nest[sample . # &, sample, depth - 1]; Eigenvalues[sampled] (* {-0.859186, 0.613002} *) Some trajectories of 1,1 vector evolving in accordance with random 2D linear transformation. It appears that power iteration converges to a line for all matrices with real eigenvalues but not for complex-valued ones ( code ) Real-valued eigenvalues Complex valued eigenvalues","Is there a geometric interpretation of a non-symmetric matrix having only real eigenvalues? It appears that multiplying random matrices with IID random entries eventually produces a matrix with only real eigenvalues, wondering if this can be turned into a statement about how random linear maps transform a vector. For instance, code below multiples random matrices, eigenvalues are almost always real. n = 2; depth = 10; dist = NormalDistribution[]; sample := RandomVariate[dist, {n, n}]/Sqrt[n]; sampled := Nest[sample . # &, sample, depth - 1]; Eigenvalues[sampled] (* {-0.859186, 0.613002} *) Some trajectories of 1,1 vector evolving in accordance with random 2D linear transformation. It appears that power iteration converges to a line for all matrices with real eigenvalues but not for complex-valued ones ( code ) Real-valued eigenvalues Complex valued eigenvalues",10\ 2\times 2,['linear-algebra']
85,A natural (?) proof of linear indepedence of eigenvectors of distinct eigenvalues.,A natural (?) proof of linear indepedence of eigenvectors of distinct eigenvalues.,,"Proposition. Let $T\colon V \to V$ be a linear operator. If $v_1, v_2, \ldots, v_m$ are eigenvectors of $T$ that belong to distinct eigenvalues, then they are linearly independent. The usual proofs are (1) by induction, (2) via a Vandermonde matrix. I would like to suggest a proof which looks much more intuitive to me, see below. I did not find it in the literature and would be grateful for any reference and for any comments (are you agree that this proof is more intuitive and natural than the ""classical"" proofs?). Proof. Assume that $v_1, v_2, \ldots, v_m$ are linearly dependent. Let $W:=\mathsf{span}(v_1, v_2, \ldots, v_m)$ . Denote $k:=\mathsf{dim}(W)$ , then we have $k<m$ . Let $S\colon W \to W$ be the restriction of $T$ on $W$ . Then $v_1, v_2, \ldots, v_m$ are eigenvectors of $S$ . This means that $S$ , a linear operator in a $k$ -dimensional vector space, has $m$ eigenvalues, where $m>k$ , which is not possible.","Proposition. Let be a linear operator. If are eigenvectors of that belong to distinct eigenvalues, then they are linearly independent. The usual proofs are (1) by induction, (2) via a Vandermonde matrix. I would like to suggest a proof which looks much more intuitive to me, see below. I did not find it in the literature and would be grateful for any reference and for any comments (are you agree that this proof is more intuitive and natural than the ""classical"" proofs?). Proof. Assume that are linearly dependent. Let . Denote , then we have . Let be the restriction of on . Then are eigenvectors of . This means that , a linear operator in a -dimensional vector space, has eigenvalues, where , which is not possible.","T\colon V \to V v_1, v_2, \ldots, v_m T v_1, v_2, \ldots, v_m W:=\mathsf{span}(v_1, v_2, \ldots, v_m) k:=\mathsf{dim}(W) k<m S\colon W \to W T W v_1, v_2, \ldots, v_m S S k m m>k","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'linear-independence']"
86,Prove: $\dim(W_1 \cap W_2) \ge \dim V - 2\dim W $,Prove:,\dim(W_1 \cap W_2) \ge \dim V - 2\dim W ,"Let $ V $ be a linear space over a field $ F $ and let $ W, W_1, W_2 $ be subspaces of $ V$ . Assume that $ V = W \oplus W_1 = W \oplus W_2. $ Prove that $$\dim(W_1 \cap W_2) \ge \dim V - 2 \dim W.$$ Could you, please, verify the following proof? $$\dim W_1 + \dim W = \dim V \\ \dim W_2 + \dim W = \dim V$$ \begin{align} \dim(W_1 + W_2)  &= \dim W_1 + \dim W_2 - \dim (W_1 \cap W_2) \\ &= \dim V - \dim W + \dim V - \dim W - \dim(W_1 \cap W_2) \\ &= 2 \dim V - 2 \dim W - \dim(W_1 \cap W_2) \end{align} $$\dim(W_1 + W_2) + \dim(W_1 \cap W_2) = 2 \dim V - 2 \dim W$$ Since $W_1 + W_2$ is a subspace of $V$ , we have $\dim(W_1 + W_2) \le \dim V$ and, therefore, we get: $$\dim(W_1 \cap W_2) \ge \dim V - 2 \dim W.$$","Let be a linear space over a field and let be subspaces of . Assume that Prove that Could you, please, verify the following proof? Since is a subspace of , we have and, therefore, we get:"," V   F   W, W_1, W_2   V  V = W \oplus W_1 = W \oplus W_2.  \dim(W_1 \cap W_2) \ge \dim V - 2 \dim W. \dim W_1 + \dim W = \dim V \\
\dim W_2 + \dim W = \dim V \begin{align}
\dim(W_1 + W_2) 
&= \dim W_1 + \dim W_2 - \dim (W_1 \cap W_2) \\
&= \dim V - \dim W + \dim V - \dim W - \dim(W_1 \cap W_2) \\
&= 2 \dim V - 2 \dim W - \dim(W_1 \cap W_2)
\end{align} \dim(W_1 + W_2) + \dim(W_1 \cap W_2) = 2 \dim V - 2 \dim W W_1 + W_2 V \dim(W_1 + W_2) \le \dim V \dim(W_1 \cap W_2) \ge \dim V - 2 \dim W.","['linear-algebra', 'solution-verification', 'vector-spaces']"
87,Is it true that an arbitrary rotation $R\in SO(3)$ can be decomposed into rotations about any two fixed axes?,Is it true that an arbitrary rotation  can be decomposed into rotations about any two fixed axes?,R\in SO(3),"This is from exercise 4.11 in Nielsen and Chuang's Quantum Computation and Quantum Information (10th Anniversary Edition), which I think might be in error. Background A quantum unitary operator on the state of a single qubit (a 2-level system whose state space is represented by a 2D complex Hilbert space) is a member of $SU(2)$. $SU(2)$ is related to $SO(3)$ in such a way that we can think of members of $SU(2)$ as rotations for the present application. In fact, they have the effect of rotating the Bloch vector representation of the quantum state: http://www.vcpc.univie.ac.at/~ian/hotlist/qc/talks/bloch-sphere-rotations.pdf It is known that an arbitrary such operation $U$ may be represented as follows: \begin{eqnarray*}U &\equiv& R_{\hat{n}}(\theta) \equiv e^{i \alpha} e^{i \frac{\theta}{2} \hat{n} \cdot \vec{\sigma}} \\ &=& \cos\left(\theta/2\right) I + i \sin\left(\theta/2\right) \hat{n} \cdot \vec{\sigma} \\ &=& \cos\left(\theta/2\right) I + i \sin\left(\theta/2\right) (n_x \sigma_x + n_y \sigma_y + n_z \sigma_z) \, \, , \end{eqnarray*} where the $\sigma_j$ are the Pauli matrices for $j \in \{x, y, z\}$, $\hat{n}$ is a unit vector in $\mathbb{R}^3$, and $\alpha$ is some phase angle (that incidentally is irrelevant to quantum mechanics, but its relevance might not be a consideration for this question). Note that this rotates the Bloch vector representation of a quantum state (let's call it $\vec{q}$) by an angle $\theta$ around $\hat{n}$, as is described in the link above and elsewhere. A theorem states that an arbitrary rotation may be decomposed into $z$ and $y$ rotations as follows: $U = e^{i\alpha} R_{\hat{z}}(\beta)\, R_{\hat{y}}(\gamma)\, R_\hat{z}(\delta) \equiv e^{i\alpha} e^{i\beta \sigma_z} e^{i\gamma \sigma_y} e^{i\delta \sigma_z}$, where $\alpha$, $\beta$, $\gamma$, and $\delta$ are phase or rotation angles. Claim The claim (which the exercise asks the reader to prove using the theorem mentioned in the previous paragraph) is that given some non-equal fixed axes $\hat{n}$ and $\hat{m}$, an arbitrary operation $U \equiv R_{\hat{r}}(\theta) = e^{i \phi} e^{i \frac{\theta}{2} \hat{r} \cdot \vec{\sigma}}$ may be decomposed as follows: \begin{equation*} U = e^{i\alpha} R_{\hat{m}}(\beta)\, R_{\hat{n}}(\gamma)\, R_\hat{m}(\delta) \, \, , \end{equation*} for some angles $\alpha$, $\beta$, $\gamma$, and $\delta$. Contention Consider given $\hat{m}$ and $\hat{n}$ that are very close to each other (e.g. their dot product is very close to but not equal to 1, perhaps 0.99). Consider $\hat{r}$ that is very far away from either $\hat{m}$ or $\hat{n}$ (e.g. their dot product is close to 0, perhaps 0 or 0.01). Consider trying to rotate a vector $\vec{v}$ around $\hat{r}$, $\vec{v}$ being initially very close to $\hat{m}$ and $\hat{n}$ prior to the rotation (where $\vec{v}$ may be thought of as a Bloch vector in the quantum context we're discussing). Surely, we can think of rotations around $\hat{r}$ that cannot be achieved by rotations around $\hat{m}$ and $\hat{n}$ as claimed in the exercise. Note This set of errata mention this exercise having an error, but it refers to the printing from the year 2000 (see erratum referring to p. 176), and doesn't show what the original exercise had stated: http://www.michaelnielsen.org/qcqi/errata/errata/errata.html . I have the 2010 edition and so would expect that the error would have been corrected in my edition (I can't tell if the original erroneous exercise was the same as printed in the later 2010 edition). Thanks!","This is from exercise 4.11 in Nielsen and Chuang's Quantum Computation and Quantum Information (10th Anniversary Edition), which I think might be in error. Background A quantum unitary operator on the state of a single qubit (a 2-level system whose state space is represented by a 2D complex Hilbert space) is a member of $SU(2)$. $SU(2)$ is related to $SO(3)$ in such a way that we can think of members of $SU(2)$ as rotations for the present application. In fact, they have the effect of rotating the Bloch vector representation of the quantum state: http://www.vcpc.univie.ac.at/~ian/hotlist/qc/talks/bloch-sphere-rotations.pdf It is known that an arbitrary such operation $U$ may be represented as follows: \begin{eqnarray*}U &\equiv& R_{\hat{n}}(\theta) \equiv e^{i \alpha} e^{i \frac{\theta}{2} \hat{n} \cdot \vec{\sigma}} \\ &=& \cos\left(\theta/2\right) I + i \sin\left(\theta/2\right) \hat{n} \cdot \vec{\sigma} \\ &=& \cos\left(\theta/2\right) I + i \sin\left(\theta/2\right) (n_x \sigma_x + n_y \sigma_y + n_z \sigma_z) \, \, , \end{eqnarray*} where the $\sigma_j$ are the Pauli matrices for $j \in \{x, y, z\}$, $\hat{n}$ is a unit vector in $\mathbb{R}^3$, and $\alpha$ is some phase angle (that incidentally is irrelevant to quantum mechanics, but its relevance might not be a consideration for this question). Note that this rotates the Bloch vector representation of a quantum state (let's call it $\vec{q}$) by an angle $\theta$ around $\hat{n}$, as is described in the link above and elsewhere. A theorem states that an arbitrary rotation may be decomposed into $z$ and $y$ rotations as follows: $U = e^{i\alpha} R_{\hat{z}}(\beta)\, R_{\hat{y}}(\gamma)\, R_\hat{z}(\delta) \equiv e^{i\alpha} e^{i\beta \sigma_z} e^{i\gamma \sigma_y} e^{i\delta \sigma_z}$, where $\alpha$, $\beta$, $\gamma$, and $\delta$ are phase or rotation angles. Claim The claim (which the exercise asks the reader to prove using the theorem mentioned in the previous paragraph) is that given some non-equal fixed axes $\hat{n}$ and $\hat{m}$, an arbitrary operation $U \equiv R_{\hat{r}}(\theta) = e^{i \phi} e^{i \frac{\theta}{2} \hat{r} \cdot \vec{\sigma}}$ may be decomposed as follows: \begin{equation*} U = e^{i\alpha} R_{\hat{m}}(\beta)\, R_{\hat{n}}(\gamma)\, R_\hat{m}(\delta) \, \, , \end{equation*} for some angles $\alpha$, $\beta$, $\gamma$, and $\delta$. Contention Consider given $\hat{m}$ and $\hat{n}$ that are very close to each other (e.g. their dot product is very close to but not equal to 1, perhaps 0.99). Consider $\hat{r}$ that is very far away from either $\hat{m}$ or $\hat{n}$ (e.g. their dot product is close to 0, perhaps 0 or 0.01). Consider trying to rotate a vector $\vec{v}$ around $\hat{r}$, $\vec{v}$ being initially very close to $\hat{m}$ and $\hat{n}$ prior to the rotation (where $\vec{v}$ may be thought of as a Bloch vector in the quantum context we're discussing). Surely, we can think of rotations around $\hat{r}$ that cannot be achieved by rotations around $\hat{m}$ and $\hat{n}$ as claimed in the exercise. Note This set of errata mention this exercise having an error, but it refers to the printing from the year 2000 (see erratum referring to p. 176), and doesn't show what the original exercise had stated: http://www.michaelnielsen.org/qcqi/errata/errata/errata.html . I have the 2010 edition and so would expect that the error would have been corrected in my edition (I can't tell if the original erroneous exercise was the same as printed in the later 2010 edition). Thanks!",,"['abstract-algebra', 'rotations', 'quantum-mechanics', 'quantum-information']"
88,"If $A\geq S^TS$ and $A\geq \lambda$ for symplectic $S$, is $A\geq R^TR\geq \lambda$ for some symplectic $R$?","If  and  for symplectic , is  for some symplectic ?",A\geq S^TS A\geq \lambda S A\geq R^TR\geq \lambda R,"Consider a $2n$ -by- $2n$ real matrix $A$ such that there exists symplectic matrix $S \in Sp(2n,\mathbb{R})$ with $A \geq S^TS$ and $A \geq \lambda \mathbb{1}$ for some $\lambda \in [0,1]$ . Does there exist symplectic matrix $R \in Sp(2n,\mathbb{R})$ such that $A \geq R^T R \geq \lambda \mathbb{1}$ ? Note that this is equivalent to asking: For PSD $A$ with symplectic eigenvalues greater than $1$ and eigenvalues greater than $\lambda$ , does there exist PSD $B$ with unit symplectic spectrum and $A \geq B \geq \lambda \mathbb{1}$ ? For more on symplectic eigenvalues, see e.g. On Symplectic eigenvalues... by Bhatia and Jain.","Consider a -by- real matrix such that there exists symplectic matrix with and for some . Does there exist symplectic matrix such that ? Note that this is equivalent to asking: For PSD with symplectic eigenvalues greater than and eigenvalues greater than , does there exist PSD with unit symplectic spectrum and ? For more on symplectic eigenvalues, see e.g. On Symplectic eigenvalues... by Bhatia and Jain.","2n 2n A S \in Sp(2n,\mathbb{R}) A \geq S^TS A \geq \lambda \mathbb{1} \lambda \in [0,1] R \in Sp(2n,\mathbb{R}) A \geq R^T R \geq \lambda \mathbb{1} A 1 \lambda B A \geq B \geq \lambda \mathbb{1}","['linear-algebra', 'positive-definite', 'symplectic-linear-algebra']"
89,Proving that the $n \times n$ Hilbert matrix is positive definite,Proving that the  Hilbert matrix is positive definite,n \times n,"Prove that the following matrix is positive definite. $$ A = \begin{bmatrix} 1 & \frac12 & \dots & \frac1n \\ \frac12 & \frac13 & \dots & \frac1{n+1} \\ \vdots  & \vdots & \ddots & \vdots \\ \frac1n & \frac1{n+1} & \dots & \frac1{2n-1} \end{bmatrix} $$ I tried to do a couple of thing without succes (it's immediate that $A=A^t$ ): First: I tried to show this by definition: In this case, the matrix is real so, let $v^t=(x_1,...,x_n)^t$ be a real vector, I tried to show that $v^tAv>0$ . I then arrive to $$x_1^2+x_1x_2+\frac{2x_1x_3}{3}+\dots+\frac{x_2x_n}{n+1}+\frac{x_n^2}{2n-1}$$ but I don't know how to show that this last equation is $>0$ , is there some identity to show this? Second , I tried to define the matrix as a inner product, that is $a_{ij} = \langle v_j, v_i \rangle$ where $\{v_1,\dots,v_n\}$ is a ortonormal base for $\mathbb{R}^n$ . How can I define those $v_j$ 's? Third , I tried to show that the minors of the matrix are all positives, I tried to do it by induction over the matrix size, for $n=1$ is trivial because $A$ is $1>0$ in this case, for $n=2$ , we have $$A=\begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \\ \end{bmatrix}$$ and then $$\begin{vmatrix} 1 & 1/2 \\ 1/2 & 1/3 \\ \end{vmatrix}=1/3-1/4>0$$ If we suposse that for size $n-1$ we have determinant positive then I tried to calculate $\det(A)$ using determinants of size $n-1$ all positives. (I did it expanding by the first column): $$\det(A)=1\det\begin{bmatrix} 1/3 & \dots & 1/n+1 \\ \vdots & \vdots & \vdots \\ 1/n+1 & \dots & 1/2n-1 \\ \end{bmatrix}-\frac{1}{2}\det\begin{bmatrix} 1/2 & \dots & 1/n \\ \vdots & \vdots & \vdots \\ 1/n+1 & \dots & 1/2n-1 \\ \end{bmatrix}+\dots+(-1)^n\frac{1}{n}\det\begin{bmatrix} 1/2 & \dots & 1/n \\ \vdots & \ddots & \vdots \\ 1/n & \dots & 1/2n-2 \\ \end{bmatrix}$$ Then, how can I do to show $\det(A) > 0$ keeping in mind that every determinant is positive?","Prove that the following matrix is positive definite. I tried to do a couple of thing without succes (it's immediate that ): First: I tried to show this by definition: In this case, the matrix is real so, let be a real vector, I tried to show that . I then arrive to but I don't know how to show that this last equation is , is there some identity to show this? Second , I tried to define the matrix as a inner product, that is where is a ortonormal base for . How can I define those 's? Third , I tried to show that the minors of the matrix are all positives, I tried to do it by induction over the matrix size, for is trivial because is in this case, for , we have and then If we suposse that for size we have determinant positive then I tried to calculate using determinants of size all positives. (I did it expanding by the first column): Then, how can I do to show keeping in mind that every determinant is positive?"," A = \begin{bmatrix} 1 & \frac12 & \dots & \frac1n \\ \frac12 & \frac13 & \dots & \frac1{n+1} \\ \vdots  & \vdots & \ddots & \vdots \\ \frac1n & \frac1{n+1} & \dots & \frac1{2n-1} \end{bmatrix}  A=A^t v^t=(x_1,...,x_n)^t v^tAv>0 x_1^2+x_1x_2+\frac{2x_1x_3}{3}+\dots+\frac{x_2x_n}{n+1}+\frac{x_n^2}{2n-1} >0 a_{ij} = \langle v_j, v_i \rangle \{v_1,\dots,v_n\} \mathbb{R}^n v_j n=1 A 1>0 n=2 A=\begin{bmatrix}
1 & 1/2 \\
1/2 & 1/3 \\
\end{bmatrix} \begin{vmatrix}
1 & 1/2 \\
1/2 & 1/3 \\
\end{vmatrix}=1/3-1/4>0 n-1 \det(A) n-1 \det(A)=1\det\begin{bmatrix}
1/3 & \dots & 1/n+1 \\
\vdots & \vdots & \vdots \\
1/n+1 & \dots & 1/2n-1 \\
\end{bmatrix}-\frac{1}{2}\det\begin{bmatrix}
1/2 & \dots & 1/n \\
\vdots & \vdots & \vdots \\
1/n+1 & \dots & 1/2n-1 \\
\end{bmatrix}+\dots+(-1)^n\frac{1}{n}\det\begin{bmatrix}
1/2 & \dots & 1/n \\
\vdots & \ddots & \vdots \\
1/n & \dots & 1/2n-2 \\
\end{bmatrix} \det(A) > 0","['linear-algebra', 'matrices', 'positive-definite', 'hankel-matrices', 'hilbert-matrices']"
90,Are all entrywise nonnegative positive semidefinite matrices has a rank-1 decomposition with nonnegative vectors?,Are all entrywise nonnegative positive semidefinite matrices has a rank-1 decomposition with nonnegative vectors?,,"I know that all positive semidefinite matrices has a rank-1 decomposition. (Equivalently, all quadratic nonnegative polynomial is sum of squares of linear function.) $$A = \sum_{i=1}^r x_i x_i^T = X X^T$$ . I am wondering if there is an entrywise nonnegative X satisfying $A=X X^T$ if $A$ is a entrywise nonnegative positive semidefinite matrix. The converse is obviously true.. I think an important tool is that solution set of $A=X X^T$ is closed with the operation $X \rightarrow XP$ where the $P$ is an orthonormal matrix. For example, $$a^T \begin{pmatrix} 2 & 1 & 0 \\ 1 & 1 & 1 \\ 0 & 1 & 2 \\ \end{pmatrix}a$$ can be represented as $(a_1+a_2+a_3)^2 + (a_1 - a_3)^2$ , but with multiplying by $$\frac{1}{\sqrt{2}} \begin{pmatrix} 1 & 1 \\ 1 & -1 \\ \end{pmatrix}, $$ we get an 'positive representation', $\frac{1}{2}(2a_1+a_2)^2 + \frac{1}{2}(a_2 + 2a_3)^2$ .","I know that all positive semidefinite matrices has a rank-1 decomposition. (Equivalently, all quadratic nonnegative polynomial is sum of squares of linear function.) . I am wondering if there is an entrywise nonnegative X satisfying if is a entrywise nonnegative positive semidefinite matrix. The converse is obviously true.. I think an important tool is that solution set of is closed with the operation where the is an orthonormal matrix. For example, can be represented as , but with multiplying by we get an 'positive representation', .","A = \sum_{i=1}^r x_i x_i^T = X X^T A=X X^T A A=X X^T X \rightarrow XP P a^T
\begin{pmatrix}
2 & 1 & 0 \\
1 & 1 & 1 \\
0 & 1 & 2 \\
\end{pmatrix}a (a_1+a_2+a_3)^2 + (a_1 - a_3)^2 \frac{1}{\sqrt{2}}
\begin{pmatrix}
1 & 1 \\
1 & -1 \\
\end{pmatrix},
 \frac{1}{2}(2a_1+a_2)^2 + \frac{1}{2}(a_2 + 2a_3)^2","['linear-algebra', 'matrices', 'positive-semidefinite', 'positive-matrices']"
91,When can a representation of a finite group be defined over certain cyclotomic fields?,When can a representation of a finite group be defined over certain cyclotomic fields?,,"Let $G$ be a finite group and $\rho \colon G \to \mathrm{GL}(n, \overline{\mathbb Q})$ be a representation. A theorem of Frobenius says that $\rho(G)$ is conjugate (in $\mathrm{GL}(n, \overline{\mathbb Q})$ ) to a subgroup of $\mathrm{GL}(n, \mathbb Q(\zeta_e))$ , where $e$ is the exponent of $G$ and $\zeta_e$ is a primitive $e$ th root of unity. We say that $\rho$ is defined over $\mathbb Q(\zeta_e)$ . Considering a cyclic group of prime order shows that Frobenius' field is in general optimal. Sometimes however we are lucky and can do better than this: consider for instance the dihedral group $D_4$ of order $8$ . Its unique irreducible representation of degree $2$ maps the rotation $r$ to $\begin{pmatrix} 0 & -1 \\ 1 & 0 \end{pmatrix}$ and the symmetry $s$ to $\begin{pmatrix} 1 & 0 \\ 0 & -1\end{pmatrix}$ , i.e., this representation is real . We can decide whether an irreducible representation can be defined over $\mathbb R$ by calculating the Frobenius-Schur indicator of its character: if the Frobenius-Schur indicator is $1$ , then the given irreducible representation can be defined over the real numbers and if it's different from $1$ , then the representation is not real. If we calculate the Frobenius-Schur indicator for the irreducible representation of the quaternionic group $Q_8 = \langle a,b \ | \ a^4 = 1, \ a^2 = b^2, \ ab = b^{-1}a\rangle$ given by $$a \mapsto \begin{pmatrix} i & 0 \\ 0 & -i \end{pmatrix}, \qquad b \mapsto \begin{pmatrix}0 & -1 \\ 1 & 0\end{pmatrix},$$ we end up with $-1$ , showing that this representation cannot be defined over $\mathbb R$ (in fact, it is quaternionic, which is not surprising, since we have a quaternion group). Frobenius' theorem says that we can define it over $\mathbb Q(i)$ , which I already did by giving the matrices. The following has been bothering me for a while: we can define the above representation of $Q_8$ over $\mathbb Q(\omega)$ for a primitive third root $\omega$ ! In fact, it is equivalent to the representation given by $$a \mapsto \begin{pmatrix} 1+2\omega & -1 \\ -2 & -1-2\omega \end{pmatrix}, \qquad b \mapsto \begin{pmatrix}-1 & \omega^2 \\ -2\omega & 1\end{pmatrix}.$$ This seems odd to me. My question is therefore: Given a representation of a finite group and an integer $d \geq 3$ , how can we decide whether it is defined over $\mathbb Q(\zeta_d)$ ? For instance: is the above representation of $Q_8$ definable over $\mathbb Q(\zeta_5)$ ? There is of course the obvious necessary condition that the character must take values in $\mathbb Z[\zeta_d]$ . Also, as explained above, taking $d$ as the exponent of the group works. I appreciate any help!","Let be a finite group and be a representation. A theorem of Frobenius says that is conjugate (in ) to a subgroup of , where is the exponent of and is a primitive th root of unity. We say that is defined over . Considering a cyclic group of prime order shows that Frobenius' field is in general optimal. Sometimes however we are lucky and can do better than this: consider for instance the dihedral group of order . Its unique irreducible representation of degree maps the rotation to and the symmetry to , i.e., this representation is real . We can decide whether an irreducible representation can be defined over by calculating the Frobenius-Schur indicator of its character: if the Frobenius-Schur indicator is , then the given irreducible representation can be defined over the real numbers and if it's different from , then the representation is not real. If we calculate the Frobenius-Schur indicator for the irreducible representation of the quaternionic group given by we end up with , showing that this representation cannot be defined over (in fact, it is quaternionic, which is not surprising, since we have a quaternion group). Frobenius' theorem says that we can define it over , which I already did by giving the matrices. The following has been bothering me for a while: we can define the above representation of over for a primitive third root ! In fact, it is equivalent to the representation given by This seems odd to me. My question is therefore: Given a representation of a finite group and an integer , how can we decide whether it is defined over ? For instance: is the above representation of definable over ? There is of course the obvious necessary condition that the character must take values in . Also, as explained above, taking as the exponent of the group works. I appreciate any help!","G \rho \colon G \to \mathrm{GL}(n, \overline{\mathbb Q}) \rho(G) \mathrm{GL}(n, \overline{\mathbb Q}) \mathrm{GL}(n, \mathbb Q(\zeta_e)) e G \zeta_e e \rho \mathbb Q(\zeta_e) D_4 8 2 r \begin{pmatrix} 0 & -1 \\ 1 & 0 \end{pmatrix} s \begin{pmatrix} 1 & 0 \\ 0 & -1\end{pmatrix} \mathbb R 1 1 Q_8 = \langle a,b \ | \ a^4 = 1, \ a^2 = b^2, \ ab = b^{-1}a\rangle a \mapsto \begin{pmatrix} i & 0 \\ 0 & -i \end{pmatrix}, \qquad b \mapsto \begin{pmatrix}0 & -1 \\ 1 & 0\end{pmatrix}, -1 \mathbb R \mathbb Q(i) Q_8 \mathbb Q(\omega) \omega a \mapsto \begin{pmatrix} 1+2\omega & -1 \\ -2 & -1-2\omega \end{pmatrix}, \qquad b \mapsto \begin{pmatrix}-1 & \omega^2 \\ -2\omega & 1\end{pmatrix}. d \geq 3 \mathbb Q(\zeta_d) Q_8 \mathbb Q(\zeta_5) \mathbb Z[\zeta_d] d","['linear-algebra', 'group-theory', 'representation-theory', 'cyclotomic-fields']"
92,Distance between point and convex hull in high dimensions,Distance between point and convex hull in high dimensions,,"I am trying to develop an intuition for the properties of the convex hull of a set of points in high ( $d>20$ ) dimensions. Consider a set of $n$ data points which are iid distributed according to some simple distribution (e.g. uniform hypercube, multivariate normal with mean $\mathbf{0}$ and identity covariance matrix $\mathbf{I}_d$ , or similar). Now suppose we draw an $n+1$ 'th data point $\mathbf{x}$ from that same distribution. What can we say about the relationship between $\mathbf{x}$ and the convex hull of the first $n$ points? My expectation $^1$ is that as dimensionality increases, the distance between $\mathbf{x}$ and the convex hull should grow for fixed $n$ . I also expect the volume of the convex hull to, in some sense, shrink relative to the volume of the domain or something similar. To make this more concrete, I would like some expression (or bound) on the expected distance between $\mathbf{x}$ and the convex hull for some simple data distribution as a function of $n$ and $d$ . Does such an example exist (either analytic or numeric) which could aid with my intuition? Disclaimer: this is not my area of expertise, so even simple examples or half-answers would be very helpful. $^1$ Inspired by section 2.5 of The Elements of Statistical Learning where the authors demonstrate the curse of dimensionality (e.g. points tend to be further apart as dimensions increase, side-length of the subcube needed to capture a fraction r of the volume of the data increases with dimension).","I am trying to develop an intuition for the properties of the convex hull of a set of points in high ( ) dimensions. Consider a set of data points which are iid distributed according to some simple distribution (e.g. uniform hypercube, multivariate normal with mean and identity covariance matrix , or similar). Now suppose we draw an 'th data point from that same distribution. What can we say about the relationship between and the convex hull of the first points? My expectation is that as dimensionality increases, the distance between and the convex hull should grow for fixed . I also expect the volume of the convex hull to, in some sense, shrink relative to the volume of the domain or something similar. To make this more concrete, I would like some expression (or bound) on the expected distance between and the convex hull for some simple data distribution as a function of and . Does such an example exist (either analytic or numeric) which could aid with my intuition? Disclaimer: this is not my area of expertise, so even simple examples or half-answers would be very helpful. Inspired by section 2.5 of The Elements of Statistical Learning where the authors demonstrate the curse of dimensionality (e.g. points tend to be further apart as dimensions increase, side-length of the subcube needed to capture a fraction r of the volume of the data increases with dimension).",d>20 n \mathbf{0} \mathbf{I}_d n+1 \mathbf{x} \mathbf{x} n ^1 \mathbf{x} n \mathbf{x} n d ^1,"['linear-algebra', 'geometry', 'convex-analysis', 'machine-learning', 'convex-hulls']"
93,"linear independence of $\{x_j-x_i\}_{j=1 ,\\j \ne i}^{k+1}$? [closed]",linear independence of ? [closed],"\{x_j-x_i\}_{j=1 ,\\j \ne i}^{k+1}","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Show that if the set $\{x_2-x_1,...,x_{k+1}-x_1\} \subset \mathbb{R}^n$ is linearly independent then the following set is also linearly independent: $$\{x_1-x_i,...,x_{i-1}-x_i,x_{i+1}-x_i,...,x_{k+1}-x_i\}$$","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Show that if the set is linearly independent then the following set is also linearly independent:","\{x_2-x_1,...,x_{k+1}-x_1\} \subset \mathbb{R}^n \{x_1-x_i,...,x_{i-1}-x_i,x_{i+1}-x_i,...,x_{k+1}-x_i\}","['linear-algebra', 'vector-spaces', 'convex-analysis', 'convex-hulls']"
94,Spectral norm inequality: Applying lipschitz continous function elementwise,Spectral norm inequality: Applying lipschitz continous function elementwise,,"Let $A, B$ be some $m\times n$ matrices in $\mathbb{R}$ and $f:\mathbb{R} \to \mathbb{R}$ a Lipschitz continuous function, i.e. $\|f(x) - f(y)\|_2 \leq L\|x-y\|_2$ for some $L$ . For the Frobenius norm $\|\cdot\|_F$ , we get $$ \| f(A) - f(B) \|_F \leq L\|A - B\|_F, $$ where $f(A)_{ij} = f(a_{ij})$ . Is there some similar statement for the spectral norm? Would it help if $f$ had a bounded derivative $f'$ ? Then I could do something like $$ \| f(A) - f(B) \| = \| f'(\Xi) \odot (A-B)\|$$ where $\Xi = (\xi)_{ij}$ is such that $f(a_{ij}) - f(b_{ij}) = f'(\xi_{ij})(a_{ij} - b_{ij})$ and $\odot$ is the Hadamard product. I know that there are some cases (I think $f'(\Xi)$ psd could be enough), where one could do $$  f'(\Xi) \text{ has some structure } \implies \| f'(\Xi) \odot (A-B)\| \leq \max_{ij} |f'(\xi_{ij})| \cdot \|(A-B)\|, $$ but I don't think that I can show $f'(\Xi)$ psd in my case.","Let be some matrices in and a Lipschitz continuous function, i.e. for some . For the Frobenius norm , we get where . Is there some similar statement for the spectral norm? Would it help if had a bounded derivative ? Then I could do something like where is such that and is the Hadamard product. I know that there are some cases (I think psd could be enough), where one could do but I don't think that I can show psd in my case.","A, B m\times n \mathbb{R} f:\mathbb{R} \to \mathbb{R} \|f(x) - f(y)\|_2 \leq L\|x-y\|_2 L \|\cdot\|_F 
\| f(A) - f(B) \|_F \leq L\|A - B\|_F,
 f(A)_{ij} = f(a_{ij}) f f'  \| f(A) - f(B) \| = \| f'(\Xi) \odot (A-B)\| \Xi = (\xi)_{ij} f(a_{ij}) - f(b_{ij}) = f'(\xi_{ij})(a_{ij} - b_{ij}) \odot f'(\Xi)  
f'(\Xi) \text{ has some structure } \implies \| f'(\Xi) \odot (A-B)\| \leq \max_{ij} |f'(\xi_{ij})| \cdot \|(A-B)\|,
 f'(\Xi)","['linear-algebra', 'matrices', 'matrix-equations']"
95,Abstract symmetric definition of duality in linear algebra?,Abstract symmetric definition of duality in linear algebra?,,"In his Linear Algebra , 4th ed. from 1975, Greub presents (p. 65) an abstract, symmetric definition of duality, in which two vector spaces $E^*,E$ over a field $\Gamma$ are said to be dual if there is a non-degenerate bilinear form $\langle-,-\rangle:E^*\times E\to\Gamma$ defined between them. This differs from most standard linear algebra books I've seen which simply define ""the"" dual space of a vector space $E$ to be the space $L(E)$ of linear functionals on $E$ . Of course, with Greub's definition, there is an embedding $E^*\to L(E)$ defined by $x^*\mapsto\langle x^*,-\rangle$ , which is an isomorphism when $E$ is finite-dimensional, but the definition itself is more general. I prefer Greub's approach to the standard approach, and see it akin to many other uses of abstract definitions in mathematics -- like using the abstract definition of a group even though any group is isomorphic to a group of permutations (Cayley's Theorem), etc. I am curious who first used this approach in linear algebra. I've found the following sources so far: Greub used it as early as his first German edition in 1958. In Fundamental Concepts of Algebra (1956), Chevalley uses it (p. 106) when defining ""vector spaces in duality"", although he also defines ""the"" dual module of a module earlier in the book (p. 67). In Lectures in Abstract Algebra, Vol. II: Linear Algebra (1953), Jacobson uses it (p. 141, 253). I was expecting to find this approach used in Bourbaki, but I did not. Does anyone have an earlier source?","In his Linear Algebra , 4th ed. from 1975, Greub presents (p. 65) an abstract, symmetric definition of duality, in which two vector spaces over a field are said to be dual if there is a non-degenerate bilinear form defined between them. This differs from most standard linear algebra books I've seen which simply define ""the"" dual space of a vector space to be the space of linear functionals on . Of course, with Greub's definition, there is an embedding defined by , which is an isomorphism when is finite-dimensional, but the definition itself is more general. I prefer Greub's approach to the standard approach, and see it akin to many other uses of abstract definitions in mathematics -- like using the abstract definition of a group even though any group is isomorphic to a group of permutations (Cayley's Theorem), etc. I am curious who first used this approach in linear algebra. I've found the following sources so far: Greub used it as early as his first German edition in 1958. In Fundamental Concepts of Algebra (1956), Chevalley uses it (p. 106) when defining ""vector spaces in duality"", although he also defines ""the"" dual module of a module earlier in the book (p. 67). In Lectures in Abstract Algebra, Vol. II: Linear Algebra (1953), Jacobson uses it (p. 141, 253). I was expecting to find this approach used in Bourbaki, but I did not. Does anyone have an earlier source?","E^*,E \Gamma \langle-,-\rangle:E^*\times E\to\Gamma E L(E) E E^*\to L(E) x^*\mapsto\langle x^*,-\rangle E","['linear-algebra', 'reference-request', 'math-history', 'duality-theorems', 'dual-spaces']"
96,"Quadratic form, from independence to normality","Quadratic form, from independence to normality",,"Suppose we have a $k$ -dimensional random variable $x$ , and for any given idempotent matrix $A$ , $x'Ax$ and $x'(I-A)x$ are independent to each other. Is $x$ normally distributed? Far as I know, for $2$ -dimensional $x$ and idempotent and symmetric $A$ , once $Ax$ is always independent to $(I-A)x$ , $x$ must have the $2$ -dimensional normal distribution. And I think it's also true for higher dimensions. I know the fact that an idempotent matrix can always be diagonalized by an orthogonal matrix to a diagonalized matrix with eigenvalues $0,1$ . But I know little about the quadratic form. Thanks in advance.","Suppose we have a -dimensional random variable , and for any given idempotent matrix , and are independent to each other. Is normally distributed? Far as I know, for -dimensional and idempotent and symmetric , once is always independent to , must have the -dimensional normal distribution. And I think it's also true for higher dimensions. I know the fact that an idempotent matrix can always be diagonalized by an orthogonal matrix to a diagonalized matrix with eigenvalues . But I know little about the quadratic form. Thanks in advance.","k x A x'Ax x'(I-A)x x 2 x A Ax (I-A)x x 2 0,1","['linear-algebra', 'probability-theory', 'normal-distribution', 'independence', 'quadratic-forms']"
97,Eigenvalues and roots of unity for integer coefficients matrix,Eigenvalues and roots of unity for integer coefficients matrix,,"I read the following exercise and I am quite stuck: Let $A \in GL_n(\mathbb{R})$ , whose coefficients are in $\mathbb{Z}$ . We assume that $A$ has $n$ distinct complex eigenvalues, and that the moduli of these eigenvalues are $\leq 1$ . Prove that the eigenvalues are roots of unity. My try : of course $A$ is diagonalizable. I can see that the fact that the moduli of the eigenvalues are $\leq 1$ implies that $(A^k)_{k \in \mathbb{N}}$ is bounded, and I think we can deduce from the fact that $A$ has integer coefficients that the eigenvalues have modulus $=1$ . But I am not really sure how, and how to deduce that the eigenvalues are roots of unity ? Can we maybe prove that $A^k=I_n$ for some $k$ ? Thanks in advance !","I read the following exercise and I am quite stuck: Let , whose coefficients are in . We assume that has distinct complex eigenvalues, and that the moduli of these eigenvalues are . Prove that the eigenvalues are roots of unity. My try : of course is diagonalizable. I can see that the fact that the moduli of the eigenvalues are implies that is bounded, and I think we can deduce from the fact that has integer coefficients that the eigenvalues have modulus . But I am not really sure how, and how to deduce that the eigenvalues are roots of unity ? Can we maybe prove that for some ? Thanks in advance !",A \in GL_n(\mathbb{R}) \mathbb{Z} A n \leq 1 A \leq 1 (A^k)_{k \in \mathbb{N}} A =1 A^k=I_n k,"['linear-algebra', 'eigenvalues-eigenvectors', 'roots-of-unity']"
98,Find all functions $f: \mathbb{R} \to \mathbb{R}$ such that $f(f(x+y)) = f(x)+f(y)$,Find all functions  such that,f: \mathbb{R} \to \mathbb{R} f(f(x+y)) = f(x)+f(y),"The problem is to find the set of all functions $f: \mathbb{R} \to \mathbb{R}$ such that for all $x,y \in \mathbb{R}$ we have $f(f(x+y)) = f(x)+f(y)$ . We first notice that $f(x)=x+c$ is a solution. With some work it turns out that it is enough to find solutions with $f(0)=0$ and from that to find all solutions, and moreover, it is not that hard to show that in this case of $f(0)=0$ the functional equation is equivilent to the two conditions: $$(1): f(x+y)=f(x)+f(y) \\ (2) :f(f(x))=f(x) $$ I was expecting that the only solutions will be $f(x) = x, f=0$ but with a bit more thought I think the set of solutions is bigger. I would like to know if I'm right. Here's what I thought: take a basis $\{e_\alpha\}_{\alpha \in I}$ of $\mathbb{R}$ as a vector space over $\mathbb{Q}$ . Consider a division of this basis to disctint pairs $(e_\alpha, e_\beta)$ [each element of the basis appears in one and exactly one of those pairs). Define $f$ on the basis elements as $f(e_\alpha) = e_\beta, f(e_\beta)=e_\beta$ . Thus we defined $f$ on the basis elements. Now, we simply continue $f$ linearly, to define it on all $\mathbb{R}$ : $f(\sum a_i e_i) = \sum a_i f(e_i)$ . This makes sure $(1)$ holds and $f$ is additive. By definition, it is trivial that $(2)$ also works if $x$ is a basis element,and therefore, because $f$ is additive, it also holds for any real number (just write it as a linear combination of the basis elements and do the algebra). Therefore I think this is a construction which shows that there are plenty of pathological solutions. Am I right? Is there a nice way to characterize all solutions?","The problem is to find the set of all functions such that for all we have . We first notice that is a solution. With some work it turns out that it is enough to find solutions with and from that to find all solutions, and moreover, it is not that hard to show that in this case of the functional equation is equivilent to the two conditions: I was expecting that the only solutions will be but with a bit more thought I think the set of solutions is bigger. I would like to know if I'm right. Here's what I thought: take a basis of as a vector space over . Consider a division of this basis to disctint pairs [each element of the basis appears in one and exactly one of those pairs). Define on the basis elements as . Thus we defined on the basis elements. Now, we simply continue linearly, to define it on all : . This makes sure holds and is additive. By definition, it is trivial that also works if is a basis element,and therefore, because is additive, it also holds for any real number (just write it as a linear combination of the basis elements and do the algebra). Therefore I think this is a construction which shows that there are plenty of pathological solutions. Am I right? Is there a nice way to characterize all solutions?","f: \mathbb{R} \to \mathbb{R} x,y \in \mathbb{R} f(f(x+y)) = f(x)+f(y) f(x)=x+c f(0)=0 f(0)=0 (1): f(x+y)=f(x)+f(y) \\ (2) :f(f(x))=f(x)  f(x) = x, f=0 \{e_\alpha\}_{\alpha \in I} \mathbb{R} \mathbb{Q} (e_\alpha, e_\beta) f f(e_\alpha) = e_\beta, f(e_\beta)=e_\beta f f \mathbb{R} f(\sum a_i e_i) = \sum a_i f(e_i) (1) f (2) x f","['linear-algebra', 'functions', 'vector-spaces', 'contest-math']"
99,Making sense of Strang's proof for the Perron-Frobenius theorem,Making sense of Strang's proof for the Perron-Frobenius theorem,,"A proof of Perron-Frobenius theorem is given in ""Introduction to Linear Algebra"" by Gilbert Strang as: This is asked before, check Perron-Frobenius theorem proof , but the explanation given by @Kavi Rama Murthy only addresses part of the proof which is: If $y>z$ then $y_j>z_j$ for all $j$ , therefore $\sum_{j=1}^{n} a_{ij} (y_j-z_j)>0\implies A(y-z)>0$ since all elements of $A$ is positive. ie., if $Ax\geq t_{\max}x$ is not an equality then $Ax>t_{\max}x\implies A^2x>t_{\max}Ax$ ie., $Ay>t_{\max}y$ for some positive vector $y$ . Similarly we can also say that for positive matrices, $A(y-z)>0\implies \sum_{j=1}^{n} a_{ij} (y_j-z_j)>0\implies y=z$ I think this much is clear. How can this tell me $t_{\max}$ can be increased ? Thanks @user8675309, due to the reasoning above when $Ax\geq t_{\max}x$ is not an equality we get the strict inequality $Ay>t_{\max}y$ for a positive matrix $A$ . ie., we can increase $t_{\max}$ by a small $\delta t$ till atleast one of the corresponding elements of $Ax$ and $t_{\max}x$ are equal. This leads to the conclusion that $Ax=t_{\max}x$ . But where does the assumption $x\neq 0$ being a non-negative vector affect in this proof ? And how can one make sense of the rest of the proof ?","A proof of Perron-Frobenius theorem is given in ""Introduction to Linear Algebra"" by Gilbert Strang as: This is asked before, check Perron-Frobenius theorem proof , but the explanation given by @Kavi Rama Murthy only addresses part of the proof which is: If then for all , therefore since all elements of is positive. ie., if is not an equality then ie., for some positive vector . Similarly we can also say that for positive matrices, I think this much is clear. How can this tell me can be increased ? Thanks @user8675309, due to the reasoning above when is not an equality we get the strict inequality for a positive matrix . ie., we can increase by a small till atleast one of the corresponding elements of and are equal. This leads to the conclusion that . But where does the assumption being a non-negative vector affect in this proof ? And how can one make sense of the rest of the proof ?",y>z y_j>z_j j \sum_{j=1}^{n} a_{ij} (y_j-z_j)>0\implies A(y-z)>0 A Ax\geq t_{\max}x Ax>t_{\max}x\implies A^2x>t_{\max}Ax Ay>t_{\max}y y A(y-z)>0\implies \sum_{j=1}^{n} a_{ij} (y_j-z_j)>0\implies y=z t_{\max} Ax\geq t_{\max}x Ay>t_{\max}y A t_{\max} \delta t Ax t_{\max}x Ax=t_{\max}x x\neq 0,"['linear-algebra', 'proof-explanation', 'positive-matrices']"
