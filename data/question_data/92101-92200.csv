,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Specific Calculation of the Germs of a Holomorphic Function,Specific Calculation of the Germs of a Holomorphic Function,,"As a specific example of this question and a follow up to this one does anyone know a nice way to calculate the germs at $z=1$ of $$f(z)=\sqrt{1+\sqrt{z}}$$ My attempts have been messy at best, and I'd rather avoid trying to wade through Taylor series if I can! Any ideas would be most welcome!","As a specific example of this question and a follow up to this one does anyone know a nice way to calculate the germs at $z=1$ of $$f(z)=\sqrt{1+\sqrt{z}}$$ My attempts have been messy at best, and I'd rather avoid trying to wade through Taylor series if I can! Any ideas would be most welcome!",,"['complex-analysis', 'riemann-surfaces']"
1,"A proper definition of $i$, the imaginary unit [duplicate]","A proper definition of , the imaginary unit [duplicate]",i,"This question already has answers here : Refining my knowledge of the imaginary number (8 answers) Closed 8 years ago . Back when I was in high school, which was a long time ago, I recall my math teacher telling me that the definition of $i$, the imaginary unit, is $\sqrt{-1}$. Knowing little, at the time, I accepted it without thinking twice. Several years later when I was in my Complex Analysis class, one of my classmates asked for the distinction between defining $i$ in the normal way, i.e., $i=\sqrt{-1}$, and more ambiguous way, $i^2=-1$. Now that I thought deeply about it, I have some suspicions about what my high school teacher taught me at the time. Firstly, at least at the level of high school, one normally defines a square root of $x$, $\sqrt{x}$ as the positive quantity of either number that satisfies the property $\sqrt{x}\sqrt{x}=x$. Clearly, when $x<0$, such notion of sign makes no sense, so one cannot honestly talk about $\sqrt{-1}$ with the naive definition of the square root. Fine, we are better than that, and we may say that $i$ is the principal root of the equation $x^2=-1$. We then just denote it by $\sqrt{-1}$. But this way of denoting $i$ brings with its convenience a litany of disasters, including the famous $1=-1$ fallacy. Namely, one can show that $1=-1$ by $1=\sqrt{1}=\sqrt{(-1)(-1)}=\sqrt{-1}\sqrt{-1}=i^2=-1$. Many a people have pointed out the haphazardness of assuming that the familiar law $\sqrt{x}\sqrt{y}=\sqrt{xy}$ holds when $x,y<0$. But at the same time we have no shame in writing $\sqrt{-5}$ as $\sqrt{5}i$ (in fact, I think this is the very reason of inventing the imaginary unit ). Is it not terribly unnatural that the law holds for odd number of negative factors, and not so for the even ones? In fact, is there an example where this native rule (that when you have a negative radicand, you can pretty much apply the familiar laws of exponents)? (I guess it makes the first question.) Secondly (so this officially marks the second, and the last question), which ought to be the definition of $i$, in your opinion? I believe that many people choose to write $i=\sqrt{-1}$ as it gives some illusion of determinancy, whereas $i^2=-1$ does not. But I still prefer the latter definition, and it seems to be the consensus of every complex analysis textbook that I've ever laid my hands on. Better yet, I believe that the complex numbers shold be defined as the algebraic completion of reals or an isomorphic field to $\mathbb{R}\times\mathbb{R}$, with some special addition and multiplication rules, but I guess it is a little bit out of high school students' league (at least for most of them). EDIT: Thank you all for your insightful responds, but there still one thing none of you has yet answered... Is there a conunter example to the law where we have $\sqrt{-A}$ for $A>0$, we have $\sqrt{A}i$?","This question already has answers here : Refining my knowledge of the imaginary number (8 answers) Closed 8 years ago . Back when I was in high school, which was a long time ago, I recall my math teacher telling me that the definition of $i$, the imaginary unit, is $\sqrt{-1}$. Knowing little, at the time, I accepted it without thinking twice. Several years later when I was in my Complex Analysis class, one of my classmates asked for the distinction between defining $i$ in the normal way, i.e., $i=\sqrt{-1}$, and more ambiguous way, $i^2=-1$. Now that I thought deeply about it, I have some suspicions about what my high school teacher taught me at the time. Firstly, at least at the level of high school, one normally defines a square root of $x$, $\sqrt{x}$ as the positive quantity of either number that satisfies the property $\sqrt{x}\sqrt{x}=x$. Clearly, when $x<0$, such notion of sign makes no sense, so one cannot honestly talk about $\sqrt{-1}$ with the naive definition of the square root. Fine, we are better than that, and we may say that $i$ is the principal root of the equation $x^2=-1$. We then just denote it by $\sqrt{-1}$. But this way of denoting $i$ brings with its convenience a litany of disasters, including the famous $1=-1$ fallacy. Namely, one can show that $1=-1$ by $1=\sqrt{1}=\sqrt{(-1)(-1)}=\sqrt{-1}\sqrt{-1}=i^2=-1$. Many a people have pointed out the haphazardness of assuming that the familiar law $\sqrt{x}\sqrt{y}=\sqrt{xy}$ holds when $x,y<0$. But at the same time we have no shame in writing $\sqrt{-5}$ as $\sqrt{5}i$ (in fact, I think this is the very reason of inventing the imaginary unit ). Is it not terribly unnatural that the law holds for odd number of negative factors, and not so for the even ones? In fact, is there an example where this native rule (that when you have a negative radicand, you can pretty much apply the familiar laws of exponents)? (I guess it makes the first question.) Secondly (so this officially marks the second, and the last question), which ought to be the definition of $i$, in your opinion? I believe that many people choose to write $i=\sqrt{-1}$ as it gives some illusion of determinancy, whereas $i^2=-1$ does not. But I still prefer the latter definition, and it seems to be the consensus of every complex analysis textbook that I've ever laid my hands on. Better yet, I believe that the complex numbers shold be defined as the algebraic completion of reals or an isomorphic field to $\mathbb{R}\times\mathbb{R}$, with some special addition and multiplication rules, but I guess it is a little bit out of high school students' league (at least for most of them). EDIT: Thank you all for your insightful responds, but there still one thing none of you has yet answered... Is there a conunter example to the law where we have $\sqrt{-A}$ for $A>0$, we have $\sqrt{A}i$?",,"['complex-analysis', 'soft-question', 'complex-numbers']"
2,Solve $\sin(z) = z$ in complex numbers,Solve  in complex numbers,\sin(z) = z,"Show that $\sin(z) = z$ has infinitely many solutions in complex numbers. Little Picard theorem should help, but using big Picard theorem is undesirable. Thanks a lot!","Show that $\sin(z) = z$ has infinitely many solutions in complex numbers. Little Picard theorem should help, but using big Picard theorem is undesirable. Thanks a lot!",,"['complex-analysis', 'complex-numbers', 'roots']"
3,Goursat's Theorem,Goursat's Theorem,,"A very first theorem that is proved in the first course of Complex Analysis would be the Gousart Theorem. Here it is: Theorem (Goursat). Let $f:U\rightarrow\mathbb{C}$ be an analytic function. Then the integral $\displaystyle\int_{\partial R}f(z)dz=0$ , where $R$ is a rectangle given by { $z=x+iy : a\leq x\leq b$ and $ c\leq y\leq d$ }. A lot of books give a rather complicated proof using quite a lot of estimation. I am just wondering whether the following proof, which looks completely natural for me, makes sense. Proof. Let $f(z)=u(x,y)+iv(x,y)$ . $\displaystyle\int_{\partial R}f(z)dz=\int_{L_1}f(z)dz+\int_{L_2}f(z)dz+\int_{L_3}f(z)dz+\int_{L_4}f(z)dz$ , where $L_1,L_2,L_3,L_4$ are the four sides of the rectangle. One can show that by explicit calculations that $\displaystyle \int_{L_1}f(z)dz+\int_{L_2}f(z)dz+\int_{L_3}f(z)dz+\int_{L_4}f(z)dz =I_1+iI_2$ , where $I_1=\displaystyle\int_{a}^b u(x,c)-u(x,d)dx-\int_{c}^d v(b,y)-v(a,y)dy$ and $I_2=\displaystyle\int_{a}^b v(x,c)-v(x,d)dx+\int_{c}^d u(b,y)-u(a,y)dy$ . By Fundamental Theorem of Calculus and Fubini's Theorem, we have $I_1=\displaystyle\int_{a}^b\int_{c}^{d} -\dfrac{\partial u}{\partial y}-\dfrac{\partial v}{\partial x}dydx$ and $I_2=\displaystyle\int_{a}^b\int_{c}^{d} -\dfrac{\partial v}{\partial y}+\dfrac{\partial u}{\partial x}dydx$ Since $f$ is analytic, it satisfies the Cauchy-Riemann Equations: $-\dfrac{\partial u}{\partial y}-\dfrac{\partial v}{\partial x}=-\dfrac{\partial v}{\partial y}+\dfrac{\partial u}{\partial x}=0$ So $I_1=I_2=0$ . We are done. I am just wondering whether this proof is valid. But I have never seen any classics on Complex Analysis adopting this proof.","A very first theorem that is proved in the first course of Complex Analysis would be the Gousart Theorem. Here it is: Theorem (Goursat). Let be an analytic function. Then the integral , where is a rectangle given by { and }. A lot of books give a rather complicated proof using quite a lot of estimation. I am just wondering whether the following proof, which looks completely natural for me, makes sense. Proof. Let . , where are the four sides of the rectangle. One can show that by explicit calculations that , where and . By Fundamental Theorem of Calculus and Fubini's Theorem, we have and Since is analytic, it satisfies the Cauchy-Riemann Equations: So . We are done. I am just wondering whether this proof is valid. But I have never seen any classics on Complex Analysis adopting this proof.","f:U\rightarrow\mathbb{C} \displaystyle\int_{\partial R}f(z)dz=0 R z=x+iy : a\leq x\leq b  c\leq y\leq d f(z)=u(x,y)+iv(x,y) \displaystyle\int_{\partial R}f(z)dz=\int_{L_1}f(z)dz+\int_{L_2}f(z)dz+\int_{L_3}f(z)dz+\int_{L_4}f(z)dz L_1,L_2,L_3,L_4 \displaystyle
\int_{L_1}f(z)dz+\int_{L_2}f(z)dz+\int_{L_3}f(z)dz+\int_{L_4}f(z)dz
=I_1+iI_2 I_1=\displaystyle\int_{a}^b u(x,c)-u(x,d)dx-\int_{c}^d v(b,y)-v(a,y)dy I_2=\displaystyle\int_{a}^b v(x,c)-v(x,d)dx+\int_{c}^d u(b,y)-u(a,y)dy I_1=\displaystyle\int_{a}^b\int_{c}^{d} -\dfrac{\partial u}{\partial y}-\dfrac{\partial v}{\partial x}dydx I_2=\displaystyle\int_{a}^b\int_{c}^{d} -\dfrac{\partial v}{\partial y}+\dfrac{\partial u}{\partial x}dydx f -\dfrac{\partial u}{\partial y}-\dfrac{\partial v}{\partial x}=-\dfrac{\partial v}{\partial y}+\dfrac{\partial u}{\partial x}=0 I_1=I_2=0",['complex-analysis']
4,Integral with two different answers using real and complex analysis,Integral with two different answers using real and complex analysis,,"The integral is $$\int_0^{2\pi}\frac{\mathrm dθ}{2-\cosθ}.$$ Just to skip time, the answer of the indefinite integral is $\dfrac2{\sqrt{3}}\tan^{-1}\left(\sqrt3\tan\left(\dfracθ2\right)\right)$ . Evaluating it from $0$ to $ 2 \pi$ yields $$\frac2{\sqrt3}\tan^{-1}(\sqrt3 \tanπ)-\frac2{\sqrt3}\tan^{-1}(\sqrt3 \tan0)=0-0=0.$$ But using complex analysis, the integral is transformed into $$2i\int_C\frac{\mathrm dz}{z^2-4z+1}=2i\int_C\frac{\mathrm dz}{(z-2+\sqrt3)(z-2-\sqrt3)},$$ where $C$ is the boundary of the circle $|z|=1$ . Then by Cauchy's integral formula, since $z=2-\sqrt3$ is inside the domain of the region bounded by $C$ , then: $$2i\int_C\frac{\mathrm dz}{(z-2+\sqrt3)(z-2-\sqrt3)}=2πi\frac{2i}{2-\sqrt3-2-\sqrt3}=2πi\frac{2i}{-2\sqrt3}=\frac{2π}{\sqrt3}.$$ Using real analysis I get $0$ , using complex analysis I get $\dfrac{2π}{\sqrt3}$ . What is wrong?","The integral is Just to skip time, the answer of the indefinite integral is . Evaluating it from to yields But using complex analysis, the integral is transformed into where is the boundary of the circle . Then by Cauchy's integral formula, since is inside the domain of the region bounded by , then: Using real analysis I get , using complex analysis I get . What is wrong?","\int_0^{2\pi}\frac{\mathrm dθ}{2-\cosθ}. \dfrac2{\sqrt{3}}\tan^{-1}\left(\sqrt3\tan\left(\dfracθ2\right)\right) 0  2 \pi \frac2{\sqrt3}\tan^{-1}(\sqrt3 \tanπ)-\frac2{\sqrt3}\tan^{-1}(\sqrt3 \tan0)=0-0=0. 2i\int_C\frac{\mathrm dz}{z^2-4z+1}=2i\int_C\frac{\mathrm dz}{(z-2+\sqrt3)(z-2-\sqrt3)}, C |z|=1 z=2-\sqrt3 C 2i\int_C\frac{\mathrm dz}{(z-2+\sqrt3)(z-2-\sqrt3)}=2πi\frac{2i}{2-\sqrt3-2-\sqrt3}=2πi\frac{2i}{-2\sqrt3}=\frac{2π}{\sqrt3}. 0 \dfrac{2π}{\sqrt3}","['complex-analysis', 'definite-integrals', 'cauchy-integral-formula']"
5,$x+y=xy=w \in \mathbb{R}^+$. Is $x^w+y^w$ real?,. Is  real?,x+y=xy=w \in \mathbb{R}^+ x^w+y^w,"Question: For $x,y \in \mathbb{C}$, suppose $x+y=xy=w \in \mathbb{R}^+$. Is $x^w+y^w$ necessarily real? For instance, if $x+y=xy=3$, then one solution is $x = \frac{3 \pm i \sqrt{3}}{2}$, $y = \frac{3 \mp i \sqrt{3}}{2}$, but $x^3 + y^3 = 0$, which is real. I've checked this numerically for many values of $w$ that give complex $x$ and $y$ (namely, $w \in (0,4)$.)","Question: For $x,y \in \mathbb{C}$, suppose $x+y=xy=w \in \mathbb{R}^+$. Is $x^w+y^w$ necessarily real? For instance, if $x+y=xy=3$, then one solution is $x = \frac{3 \pm i \sqrt{3}}{2}$, $y = \frac{3 \mp i \sqrt{3}}{2}$, but $x^3 + y^3 = 0$, which is real. I've checked this numerically for many values of $w$ that give complex $x$ and $y$ (namely, $w \in (0,4)$.)",,"['complex-analysis', 'complex-numbers', 'recreational-mathematics']"
6,Confusion about contour integration of constant function: intuition vs. Residue Theorem,Confusion about contour integration of constant function: intuition vs. Residue Theorem,,"Let's say we have the holomorphic function $$f(z) = 1.$$ Because $f(z)$ has no poles, according the Residue Theorem we have $$\oint_\gamma f(z)\,dz = 0$$ for any closed counterclockwise path $\gamma$. But let's say that $\gamma$ is a circle around the origin of radius $r$. Then shouldn't we have $$\oint_\gamma f(z)\,dz = 2 \pi r$$ because $$\oint_\gamma f(z)\,dz = \oint_\gamma dz = \text{arclength}\,\gamma$$ ? I'm pretty sure the result using the Residue Theorem is correct, so then my reasoning must be incorrect for the second way of looking at it. Where is my reasoning incorrect?","Let's say we have the holomorphic function $$f(z) = 1.$$ Because $f(z)$ has no poles, according the Residue Theorem we have $$\oint_\gamma f(z)\,dz = 0$$ for any closed counterclockwise path $\gamma$. But let's say that $\gamma$ is a circle around the origin of radius $r$. Then shouldn't we have $$\oint_\gamma f(z)\,dz = 2 \pi r$$ because $$\oint_\gamma f(z)\,dz = \oint_\gamma dz = \text{arclength}\,\gamma$$ ? I'm pretty sure the result using the Residue Theorem is correct, so then my reasoning must be incorrect for the second way of looking at it. Where is my reasoning incorrect?",,"['complex-analysis', 'contour-integration']"
7,All continuous functions are analytic,All continuous functions are analytic,,"This might be very silly to ask, but somehow this sequence of results are leading me to this wrong result. I am dealing with complex analysis and the mistake I am making might be because I am using some results from real analysis. If a function, $f(z)$, is continuous in simply connected domain, then it will be Riemann integrable and hence its antiderivitive, $F(z)$, will exist and moreover the antiderivative will be differentiable in the domain. This implies that $F(Z)$ is analytic since it is differentiable in the neighborhood of all points. Which also means that it is infinitely times differentiable. And hence even $f(z)$ is infinitely times differentiable and hence, $f(z)$ is also analytic.","This might be very silly to ask, but somehow this sequence of results are leading me to this wrong result. I am dealing with complex analysis and the mistake I am making might be because I am using some results from real analysis. If a function, $f(z)$, is continuous in simply connected domain, then it will be Riemann integrable and hence its antiderivitive, $F(z)$, will exist and moreover the antiderivative will be differentiable in the domain. This implies that $F(Z)$ is analytic since it is differentiable in the neighborhood of all points. Which also means that it is infinitely times differentiable. And hence even $f(z)$ is infinitely times differentiable and hence, $f(z)$ is also analytic.",,['complex-analysis']
8,Prerequisites to learn Complex Analysis,Prerequisites to learn Complex Analysis,,"I really wanna learn Complex Analysis but I don't know where to start. Basically I can do olympiad problems, but I don't know calculus that well, so I would appreciate it if someone can post a list of topics (preferably in chronological order, if that makes sense) for me to learn before I am absolutely 100% ready to start learning the complex analysis, which I've read about and think is A-M-A-Z-I-N-G. Thank you for your time everyone! :D","I really wanna learn Complex Analysis but I don't know where to start. Basically I can do olympiad problems, but I don't know calculus that well, so I would appreciate it if someone can post a list of topics (preferably in chronological order, if that makes sense) for me to learn before I am absolutely 100% ready to start learning the complex analysis, which I've read about and think is A-M-A-Z-I-N-G. Thank you for your time everyone! :D",,"['complex-analysis', 'self-learning', 'big-list']"
9,Integrate: $\int_0^{\infty} \frac{\sin (ax)}{e^{\pi x} \sinh(\pi x)}dx$,Integrate:,\int_0^{\infty} \frac{\sin (ax)}{e^{\pi x} \sinh(\pi x)}dx,"How to evaluate the following  $$\int_0^{\infty} \frac{\sin (ax)}{e^{\pi x} \sinh(\pi x)} dx $$ Given hints says to construct a rectangle $0\to R\to R+i\to i \to 0$ and consider $\displaystyle f(z):=\frac{e^{iaz}}{e^{2\pi z}-1} $ and evaluate around it but that does not help. ADDED:: I need to evaluate it with method of contour . Particularly using given hint on book, when I integrate $i \to 0$, the integral does not converge. Also, real part of $\displaystyle f(z):=\frac{e^{iaz}}{e^{2\pi z}-1} $ does not converge around $0$. Also I considered $\displaystyle f(z):=\frac{e^{iaz}-e^{-iaz}}{e^{2\pi x}-1} $, and integrate from $-R \to R \to R+i \to -R+ i \to R$ and I get $\displaystyle \int_{-\infty}^\infty f(z) dz$ but the function is not symmetric about $0$ . If I consider $\displaystyle f(z):=\frac{e^{iaz}}{e^{2\pi x}-1} $ around the path $-R - i \to R - i \to R + i \to -R +i \to -R -i $, I again end up with $\displaystyle \int_{-\infty}^{\infty}f(z) dz$ and I cannot extract $\int_0^{\infty}$ due to  function is not symmetric at $0$ because of $e^{2\pi z}$ at the denominator. Please help!!","How to evaluate the following  $$\int_0^{\infty} \frac{\sin (ax)}{e^{\pi x} \sinh(\pi x)} dx $$ Given hints says to construct a rectangle $0\to R\to R+i\to i \to 0$ and consider $\displaystyle f(z):=\frac{e^{iaz}}{e^{2\pi z}-1} $ and evaluate around it but that does not help. ADDED:: I need to evaluate it with method of contour . Particularly using given hint on book, when I integrate $i \to 0$, the integral does not converge. Also, real part of $\displaystyle f(z):=\frac{e^{iaz}}{e^{2\pi z}-1} $ does not converge around $0$. Also I considered $\displaystyle f(z):=\frac{e^{iaz}-e^{-iaz}}{e^{2\pi x}-1} $, and integrate from $-R \to R \to R+i \to -R+ i \to R$ and I get $\displaystyle \int_{-\infty}^\infty f(z) dz$ but the function is not symmetric about $0$ . If I consider $\displaystyle f(z):=\frac{e^{iaz}}{e^{2\pi x}-1} $ around the path $-R - i \to R - i \to R + i \to -R +i \to -R -i $, I again end up with $\displaystyle \int_{-\infty}^{\infty}f(z) dz$ and I cannot extract $\int_0^{\infty}$ due to  function is not symmetric at $0$ because of $e^{2\pi z}$ at the denominator. Please help!!",,"['complex-analysis', 'contour-integration']"
10,references for learning about branch cuts/ branch points in complex analysis,references for learning about branch cuts/ branch points in complex analysis,,Are there any good books/online resources for learning about branch cuts at the level of  introductory undergraduate complex analysis. Thanks in advance.,Are there any good books/online resources for learning about branch cuts at the level of  introductory undergraduate complex analysis. Thanks in advance.,,"['complex-analysis', 'reference-request', 'book-recommendation']"
11,How to solve this integral using the method of residues?,How to solve this integral using the method of residues?,,"Using the method of residues, verify the following. $$\int_0^{\pi} \frac{d \theta}{(3+2cos \theta)^2} = \frac{3 \pi \sqrt{5}}{25}$$ I tried doing this but can't get the correct answer, here is my attempt: first I substituted $cos \theta = \frac{z+ \frac{1}{z}}{2}$ and $d \theta = \frac{dz}{iz}$ and got $\int_0^{\pi} \frac{dz}{iz(3+z+\frac{1}{z})^2}$ then I multiplied top and bottom my $iz$ so that the $i$ moves to the top and on the bottom I'll have $z^2$ so I can distribute it into the rest of it and get $-i \int_0^{\pi} \frac{zdz}{(3z+z^2+1)^2}$ the denominator has two zeroes at $-\frac{3}{2} \pm \sqrt{\frac{5}{4}}$, since the denominator is squared the function has poles here of order two. The I used Cauchy's Residue theorem but first I needed to find the residue at $-\frac{3}{2} + \sqrt{\frac{5}{4}}$ because only this pole is inside the unit circle. I found the residue using theorem 1 on pdf page 324/579 from this textbook http://english-c.tongji.edu.cn/_SiteConf/files/2014/05/05/file_53676237d7159.pdf After finding the residue using that formula and applying cauchy's residue theorem I do not get an answer close to $\frac{3 \pi \sqrt{5}}{25}$ I am not sure how to handle the fact that the integral is from $0$ to $\pi$ rather then from $0$ to $2\pi$, and I am not sure if I made any mistakes when finding the poles and residues.","Using the method of residues, verify the following. $$\int_0^{\pi} \frac{d \theta}{(3+2cos \theta)^2} = \frac{3 \pi \sqrt{5}}{25}$$ I tried doing this but can't get the correct answer, here is my attempt: first I substituted $cos \theta = \frac{z+ \frac{1}{z}}{2}$ and $d \theta = \frac{dz}{iz}$ and got $\int_0^{\pi} \frac{dz}{iz(3+z+\frac{1}{z})^2}$ then I multiplied top and bottom my $iz$ so that the $i$ moves to the top and on the bottom I'll have $z^2$ so I can distribute it into the rest of it and get $-i \int_0^{\pi} \frac{zdz}{(3z+z^2+1)^2}$ the denominator has two zeroes at $-\frac{3}{2} \pm \sqrt{\frac{5}{4}}$, since the denominator is squared the function has poles here of order two. The I used Cauchy's Residue theorem but first I needed to find the residue at $-\frac{3}{2} + \sqrt{\frac{5}{4}}$ because only this pole is inside the unit circle. I found the residue using theorem 1 on pdf page 324/579 from this textbook http://english-c.tongji.edu.cn/_SiteConf/files/2014/05/05/file_53676237d7159.pdf After finding the residue using that formula and applying cauchy's residue theorem I do not get an answer close to $\frac{3 \pi \sqrt{5}}{25}$ I am not sure how to handle the fact that the integral is from $0$ to $\pi$ rather then from $0$ to $2\pi$, and I am not sure if I made any mistakes when finding the poles and residues.",,"['complex-analysis', 'residue-calculus', 'complex-integration']"
12,$f(z)=\bar{z}$ has no primitive,has no primitive,f(z)=\bar{z},"As a consequence of Goursat's Theorem, we can prove that every holomorphic function on an open disk has primitive . Question: Is it true that every continuous function $f\colon D\rightarrow \mathbb{C}$ has primitive? [D=open disc in $\mathbb{C}$] The answer I think is ""NO"". But my explanation involves use of some important theorems. The example I thought is $f(z)=\overline{z}$. If this $f$ has primitive, then $f$ has to be holomorphic, a contradiction. The problem I would concern here is the following: Problem: Can we give an elementary argument that $f(z)=\overline{z}$ has no primitive in any open disc? (I want to avoid the theorem that ""a complex function which is once differentiable is infinitely many times differentiable"").","As a consequence of Goursat's Theorem, we can prove that every holomorphic function on an open disk has primitive . Question: Is it true that every continuous function $f\colon D\rightarrow \mathbb{C}$ has primitive? [D=open disc in $\mathbb{C}$] The answer I think is ""NO"". But my explanation involves use of some important theorems. The example I thought is $f(z)=\overline{z}$. If this $f$ has primitive, then $f$ has to be holomorphic, a contradiction. The problem I would concern here is the following: Problem: Can we give an elementary argument that $f(z)=\overline{z}$ has no primitive in any open disc? (I want to avoid the theorem that ""a complex function which is once differentiable is infinitely many times differentiable"").",,['complex-analysis']
13,What is the value of $\sum_{m=1}^{19} \frac{1}{\zeta^{3m}+\zeta^{2m}+\zeta^{m}+1}$ with $\zeta=e^{2\pi i/19}$?,What is the value of  with ?,\sum_{m=1}^{19} \frac{1}{\zeta^{3m}+\zeta^{2m}+\zeta^{m}+1} \zeta=e^{2\pi i/19},"Given that $\zeta=e^{2\pi i/19}$, how to find the value of $$S=\sum_{m=1}^{19} \dfrac{1}{\zeta^{3m}+\zeta^{2m}+\zeta^{m}+1}$$? All I could think of was to somehow factorize the denominator and apply some sort of a partial fraction method, but I didn't succeed in that too. Answers and hints in the right direction appreciated.","Given that $\zeta=e^{2\pi i/19}$, how to find the value of $$S=\sum_{m=1}^{19} \dfrac{1}{\zeta^{3m}+\zeta^{2m}+\zeta^{m}+1}$$? All I could think of was to somehow factorize the denominator and apply some sort of a partial fraction method, but I didn't succeed in that too. Answers and hints in the right direction appreciated.",,"['complex-analysis', 'summation', 'roots-of-unity']"
14,Asymptotic evaluation of integral method of steepest descent,Asymptotic evaluation of integral method of steepest descent,,The question asks to show that the leading term of the integral $$ \int_{-\infty}^\infty (1+t^2)^{-1}\exp\left(ik(t^5/5+t)\right) dt $$ for large $k$ using the method of steepest descent is equal to $$ \sqrt{\frac{\pi}{k}} e^{\frac{-4k}{5\sqrt2}} \cos\left(\frac{4k}{5\sqrt2} - \frac{3\pi}{8}\right) $$ ...I don't even know how to pick my contour for this problem.  Thanks for the help!,The question asks to show that the leading term of the integral $$ \int_{-\infty}^\infty (1+t^2)^{-1}\exp\left(ik(t^5/5+t)\right) dt $$ for large $k$ using the method of steepest descent is equal to $$ \sqrt{\frac{\pi}{k}} e^{\frac{-4k}{5\sqrt2}} \cos\left(\frac{4k}{5\sqrt2} - \frac{3\pi}{8}\right) $$ ...I don't even know how to pick my contour for this problem.  Thanks for the help!,,"['complex-analysis', 'asymptotics']"
15,Non-existence of a bijective analytic function between annulus and punctured disk,Non-existence of a bijective analytic function between annulus and punctured disk,,Suppose $A=\{z\in \mathbb{C}: 0<|z|<1\}$ and $B=\{z\in \mathbb{C}: 2<|z|<3\}$. Show that there is no one -to-one analytic function from A to B. Any hints? Thanks!,Suppose $A=\{z\in \mathbb{C}: 0<|z|<1\}$ and $B=\{z\in \mathbb{C}: 2<|z|<3\}$. Show that there is no one -to-one analytic function from A to B. Any hints? Thanks!,,['complex-analysis']
16,Understanding the branch cut and discontinuity of the hypergeometric function,Understanding the branch cut and discontinuity of the hypergeometric function,,"I am going through this paper, and I am having trouble understanding page 20. I am still learning my way around managing multi valued complex functions, so I'd like your help in understanding what's happening there. I have the definition of the hypergeometric series $_2F_1$ as $$ _2F_1(a,b;c;z)=\sum_{n=0}^\infty \frac{(a)_n(b)_n}{(c)_n}\frac{z^n}{n!}. $$ Here $(a)_n$ is the rising Pochhammer symbol, $(a)_n=\Gamma(a+n)/\Gamma(a)$ (well defined whenever $a$ is not a negative integer or zero). By elementary computations, the radius of convergence of this series is 1. It is then said that $_2F_1$ can be continued analitically in the whole complex plane along any curve not passing through $[1;+\infty)$, and $1$ itself is a branching point and the function has a cut on the previous segment. Furthermore, equation $2.115$ of the paper computes the discontinuity when crossing the branch cut as (with $x\geq4/3$ to have the real part of the argument bigger than one) $$ _2F_1\left(\frac16,\frac56;1;\frac{3x}4+i\epsilon\right)-_2F_1\left(\frac16,\frac56;1;\frac{3x}4-i\epsilon\right)=i\, _2F_1\left(\frac16,\frac56,1;1-\frac{3x}4\right). $$ I am trying to understand those results. It is not clear to me how to understand from the power series form the behaviour on the boundary of the disk of convergence. It is clear to me that the series in $z=1$ diverges, as the coefficients do not go to zero, so there should be no analytical continuation. A way to study that function would be its integral representation: $$ _2F_1(a,b;c;z)=\frac{\Gamma(c)}{\Gamma(b)\Gamma(c-b)}\int_0^1t^{b-1}(1-t)^{c-b-1}(1-zt)^{-a}dt. $$ Here it is understood that $\arg t=0=\arg(1-t)$. From this representation, it is clear to me that the function can have branches whenever $a$ is not a positive integer, and if $z\in[1;\infty)$ we have $0$ in the integration path, so we can have multiple branches. And that's exactly my case. The problem is that I can't use this form to prove the previous equation about the discontinuity: all I manage to write is (here I write z=x with x real number bigger than one, and I am neglecting some numerical factors that can easily be reinserted) $$ \lim_{\epsilon\to0}((1-xt-i\epsilon t)^{-a}-(1-xt+i\epsilon t)^{-a})=-(1-e^{-2i\pi a})(1-xt)^{-a}. $$ This is different from the line that I would like to prove (as an example, I do not see why I should change the argument, but I also see that with that argument changing the function is at least evaluated in a point where well defineteness is guaranteed). To summarize, I have two questions: How to prove the discontinuity from the integral form? More in general: how to treat power series in order to understand if their singularities are poles or branch cuts, and the discontinuity of their analytic continuations? I know that this is a very broad topic, and I'd also like some references to continue my study. Thanks everybody for the time you took reading this.","I am going through this paper, and I am having trouble understanding page 20. I am still learning my way around managing multi valued complex functions, so I'd like your help in understanding what's happening there. I have the definition of the hypergeometric series $_2F_1$ as $$ _2F_1(a,b;c;z)=\sum_{n=0}^\infty \frac{(a)_n(b)_n}{(c)_n}\frac{z^n}{n!}. $$ Here $(a)_n$ is the rising Pochhammer symbol, $(a)_n=\Gamma(a+n)/\Gamma(a)$ (well defined whenever $a$ is not a negative integer or zero). By elementary computations, the radius of convergence of this series is 1. It is then said that $_2F_1$ can be continued analitically in the whole complex plane along any curve not passing through $[1;+\infty)$, and $1$ itself is a branching point and the function has a cut on the previous segment. Furthermore, equation $2.115$ of the paper computes the discontinuity when crossing the branch cut as (with $x\geq4/3$ to have the real part of the argument bigger than one) $$ _2F_1\left(\frac16,\frac56;1;\frac{3x}4+i\epsilon\right)-_2F_1\left(\frac16,\frac56;1;\frac{3x}4-i\epsilon\right)=i\, _2F_1\left(\frac16,\frac56,1;1-\frac{3x}4\right). $$ I am trying to understand those results. It is not clear to me how to understand from the power series form the behaviour on the boundary of the disk of convergence. It is clear to me that the series in $z=1$ diverges, as the coefficients do not go to zero, so there should be no analytical continuation. A way to study that function would be its integral representation: $$ _2F_1(a,b;c;z)=\frac{\Gamma(c)}{\Gamma(b)\Gamma(c-b)}\int_0^1t^{b-1}(1-t)^{c-b-1}(1-zt)^{-a}dt. $$ Here it is understood that $\arg t=0=\arg(1-t)$. From this representation, it is clear to me that the function can have branches whenever $a$ is not a positive integer, and if $z\in[1;\infty)$ we have $0$ in the integration path, so we can have multiple branches. And that's exactly my case. The problem is that I can't use this form to prove the previous equation about the discontinuity: all I manage to write is (here I write z=x with x real number bigger than one, and I am neglecting some numerical factors that can easily be reinserted) $$ \lim_{\epsilon\to0}((1-xt-i\epsilon t)^{-a}-(1-xt+i\epsilon t)^{-a})=-(1-e^{-2i\pi a})(1-xt)^{-a}. $$ This is different from the line that I would like to prove (as an example, I do not see why I should change the argument, but I also see that with that argument changing the function is at least evaluated in a point where well defineteness is guaranteed). To summarize, I have two questions: How to prove the discontinuity from the integral form? More in general: how to treat power series in order to understand if their singularities are poles or branch cuts, and the discontinuity of their analytic continuations? I know that this is a very broad topic, and I'd also like some references to continue my study. Thanks everybody for the time you took reading this.",,"['complex-analysis', 'power-series', 'hypergeometric-function', 'branch-cuts', 'branch-points']"
17,Entire functions such that $\frac{f(z+1)-f(z-1)}{2}=f'(z)$,Entire functions such that,\frac{f(z+1)-f(z-1)}{2}=f'(z),"Consider the equation $$\frac{f(z+1)-f(z-1)}{2}=f'(z)\tag{*}.$$ Any polynomial of degree $\leq 2$ satisfies $(*)$ for all $z$.  My question is: If $f:\mathbb{C}\to\mathbb{C}$ is holomorphic and satisfies $(*)$ for all $z$, must $f$ be a polynomial of degree $\leq 2$? I would also be interested in answers to weakened versions of the question, for instance where $f$ is only holomorphic on a strip $\{z:-c<\operatorname{Im}z<c\}$, or where $f$ is allowed to have isolated singularities. Here are some things I know about this question so far.  If we instead consider smooth functions $f:\mathbb{R}\to\mathbb{R}$, then there are lots more solutions to $(*)$ besides polynomials of degree $\leq 2$ (see Functions $f$ such that $f(x+1)-f(x-1)=2f'(x)$. ). On the other hand, if $f$ is a polynomial which satisfies $(*)$, it must have degree $\leq 2$.  Indeed, if $f$ satisfies $(*)$, so does $f'$ (by differentiating the equation), so if a polynomial of degree $>2$ satisfied $(*)$, we could repeatedly differentiate to get a cubic.  Subtracting the quadratic part (since $(*)$ is linear), we would conclude that $f(z)=z^3$ satisfies $(*)$, which is false. You could attempt to build solutions to $(*)$ using Taylor series.  Specifically, suppose $f(z)=\sum a_n z^n$ and let $g(z)=f'(z)-\frac{f(z+1)-f(z-1)}{2}$.  To verify that $f$ satisifies $(*)$, it suffices to check that $g^{(n)}(0)=0$ for all $n\in\mathbb{N}$.  This can be written as an infinite list of (infinitary) linear conditions on the $a_n$.  For instance, the condition that $g(0)=0$ says that $$\sum_{n=1}^\infty a_{2n+1}=0$$ and the condition that $g'(0)=0$ says that $$\sum_{n=2}^\infty2na_{2n}=0.$$  In general, the equations for even derivatives involve only the $a_n$ for $n$ odd and the equations for odd derivatives involve only the $a_n$ for $n$ even, so you can consider odd $n$ and even $n$ separately.  You could try to inductively construct the $a_n$ to make all these equations true one at a time.  For instance, you might start by defining $a_3=1$ and $a_5=-1$, and then define $a_7$ and $a_9$ so that $g(0)=0$ remains true but $g''(0)=0$ becomes true.  Then you could try to define $a_{11}$, $a_{13}$, and $a_{15}$ so that $g(0)=0$ and $g''(0)=0$ remain true and $g''''(0)=0$ becomes true.  However, this has convergence issues: I don't know how to prove that such a construction will make the series $\sum_{n=1}^\infty a_{2n+1}$ actually converge (all the construction gives is that infinitely many of the partial sums are $0$), let alone that the $a_n$ shrink fast enough so that $\sum a_nz^n$ is entire.","Consider the equation $$\frac{f(z+1)-f(z-1)}{2}=f'(z)\tag{*}.$$ Any polynomial of degree $\leq 2$ satisfies $(*)$ for all $z$.  My question is: If $f:\mathbb{C}\to\mathbb{C}$ is holomorphic and satisfies $(*)$ for all $z$, must $f$ be a polynomial of degree $\leq 2$? I would also be interested in answers to weakened versions of the question, for instance where $f$ is only holomorphic on a strip $\{z:-c<\operatorname{Im}z<c\}$, or where $f$ is allowed to have isolated singularities. Here are some things I know about this question so far.  If we instead consider smooth functions $f:\mathbb{R}\to\mathbb{R}$, then there are lots more solutions to $(*)$ besides polynomials of degree $\leq 2$ (see Functions $f$ such that $f(x+1)-f(x-1)=2f'(x)$. ). On the other hand, if $f$ is a polynomial which satisfies $(*)$, it must have degree $\leq 2$.  Indeed, if $f$ satisfies $(*)$, so does $f'$ (by differentiating the equation), so if a polynomial of degree $>2$ satisfied $(*)$, we could repeatedly differentiate to get a cubic.  Subtracting the quadratic part (since $(*)$ is linear), we would conclude that $f(z)=z^3$ satisfies $(*)$, which is false. You could attempt to build solutions to $(*)$ using Taylor series.  Specifically, suppose $f(z)=\sum a_n z^n$ and let $g(z)=f'(z)-\frac{f(z+1)-f(z-1)}{2}$.  To verify that $f$ satisifies $(*)$, it suffices to check that $g^{(n)}(0)=0$ for all $n\in\mathbb{N}$.  This can be written as an infinite list of (infinitary) linear conditions on the $a_n$.  For instance, the condition that $g(0)=0$ says that $$\sum_{n=1}^\infty a_{2n+1}=0$$ and the condition that $g'(0)=0$ says that $$\sum_{n=2}^\infty2na_{2n}=0.$$  In general, the equations for even derivatives involve only the $a_n$ for $n$ odd and the equations for odd derivatives involve only the $a_n$ for $n$ even, so you can consider odd $n$ and even $n$ separately.  You could try to inductively construct the $a_n$ to make all these equations true one at a time.  For instance, you might start by defining $a_3=1$ and $a_5=-1$, and then define $a_7$ and $a_9$ so that $g(0)=0$ remains true but $g''(0)=0$ becomes true.  Then you could try to define $a_{11}$, $a_{13}$, and $a_{15}$ so that $g(0)=0$ and $g''(0)=0$ remain true and $g''''(0)=0$ becomes true.  However, this has convergence issues: I don't know how to prove that such a construction will make the series $\sum_{n=1}^\infty a_{2n+1}$ actually converge (all the construction gives is that infinitely many of the partial sums are $0$), let alone that the $a_n$ shrink fast enough so that $\sum a_nz^n$ is entire.",,"['complex-analysis', 'ordinary-differential-equations', 'functional-equations']"
18,Book on quasiconformal mappings?,Book on quasiconformal mappings?,,"I am looking an introductory book on ""quasiconformal mappings"" for self-study. Also I would like to know about motivation and history behind this concept (I am a beginner of this subject). I really appreciate any help you can provide.","I am looking an introductory book on ""quasiconformal mappings"" for self-study. Also I would like to know about motivation and history behind this concept (I am a beginner of this subject). I really appreciate any help you can provide.",,"['complex-analysis', 'reference-request', 'self-learning', 'book-recommendation', 'quasiconformal-maps']"
19,Relative density of primes under extension,Relative density of primes under extension,,"Let $\mathbb{P}_{\mathbb{C}}$ be the set of Gaussian primes and $\mathbb{P}_{\mathbb{N}}$ the set of primes in $\mathbb{N}$. Let $\pi_{\mathbf{C}}(\sqrt{n})$ be the number of Gaussian primes with norm $\leq \sqrt{n}$ and $\pi_{\mathbf{N}}(n)$ be, as usual, the number of primes $\leq n$ in $\mathbb{N}$. Recall that norm($x+iy$)=N($x+iy$)=$x^2 +y^2$; hence, my taking of a square root above. I am interested to know what the order of magnitude is for $$\frac{\pi_{\mathbf{N}}(n)}{\pi_{\mathbf{C}}(\sqrt{n})}$$i.e. Has the extension of the definition of primes increased/decreased the relative density of primes with respect to their set of definition? A rather quixotic question could be "" Is there a general asymptotic for the number of primes in an arbitrary infinite field with the definition of being a prime as usual?"" Fact : Prime numbers of the form $4n + 3$ are also Gaussian primes. Gauss's circle problem which asks for the number of Gaussian integers with norm less than a given value is presently unresolved. I think this is tangentially related to the asymptotic I am looking for.","Let $\mathbb{P}_{\mathbb{C}}$ be the set of Gaussian primes and $\mathbb{P}_{\mathbb{N}}$ the set of primes in $\mathbb{N}$. Let $\pi_{\mathbf{C}}(\sqrt{n})$ be the number of Gaussian primes with norm $\leq \sqrt{n}$ and $\pi_{\mathbf{N}}(n)$ be, as usual, the number of primes $\leq n$ in $\mathbb{N}$. Recall that norm($x+iy$)=N($x+iy$)=$x^2 +y^2$; hence, my taking of a square root above. I am interested to know what the order of magnitude is for $$\frac{\pi_{\mathbf{N}}(n)}{\pi_{\mathbf{C}}(\sqrt{n})}$$i.e. Has the extension of the definition of primes increased/decreased the relative density of primes with respect to their set of definition? A rather quixotic question could be "" Is there a general asymptotic for the number of primes in an arbitrary infinite field with the definition of being a prime as usual?"" Fact : Prime numbers of the form $4n + 3$ are also Gaussian primes. Gauss's circle problem which asks for the number of Gaussian integers with norm less than a given value is presently unresolved. I think this is tangentially related to the asymptotic I am looking for.",,"['number-theory', 'complex-analysis', 'prime-numbers']"
20,Let $h:\mathbb{C}\to\mathbb{C}$ in $C^k(\mathbb{C})$ with compact support. Find solutions to the equation $f_x + if_y = h$.,Let  in  with compact support. Find solutions to the equation .,h:\mathbb{C}\to\mathbb{C} C^k(\mathbb{C}) f_x + if_y = h,Let $h:\mathbb{C}\to\mathbb{C}$ be a function in $C^k(\mathbb{C})$ and has compact support. What are the solutions in $C^k(\mathbb{C})$ to the equation $$ \frac{\partial f}{\partial x} + i\frac{\partial f}{\partial y} = h? $$ Can this be solved without using techniques from PDE?,Let be a function in and has compact support. What are the solutions in to the equation Can this be solved without using techniques from PDE?,"h:\mathbb{C}\to\mathbb{C} C^k(\mathbb{C}) C^k(\mathbb{C}) 
\frac{\partial f}{\partial x} + i\frac{\partial f}{\partial y} = h?
",['complex-analysis']
21,Understanding the chain rule in the Wirtinger calculus,Understanding the chain rule in the Wirtinger calculus,,"The Wirtinger differential operators are defined by: \begin{equation} \frac{\partial}{\partial z} = \frac{1}{2}\left(\frac{\partial}{\partial x} - i\frac{\partial}{\partial y}\right) \\ \frac{\partial}{\partial \bar{z}} = \frac{1}{2}\left(\frac{\partial}{\partial x} + i\frac{\partial}{\partial y}\right) \end{equation} These satisfy the following chain rule: \begin{equation} \frac{\partial}{\partial z}(f \circ g) = \left(\frac{\partial f}{\partial z}\circ g\right)\frac{\partial g}{\partial z} + \left(\frac{\partial f}{\partial \bar{z}}\circ g\right)\frac{\partial \bar{g}}{\partial z}. \end{equation} Usually I think about partial derivatives as forming the components of the Jacobian (a.k.a differential/total derivative) and the chain rule for them as a matrix representation of the relation: \begin{equation}  \mathbf{D}(f \circ g) = (\mathbf{D}f \circ g)\cdot\mathbf{D}g. \end{equation} How can one interpret the chain rule for the Writinger differential operators in this light? I would particularly enjoy a formalism that allows me to understand why $\frac{\partial{f}}{\partial \bar{z}} = 0$ iff $f$ is analytic, or makes this seem like a really natural definition to begin investigation of the properties of analytic functions. Alternatively I would like formalism that is closely related to the idea of complexifying the tangent bundle of $\mathbb{R}^2$ with its standard complex structure, with an explanation of the connection.","The Wirtinger differential operators are defined by: These satisfy the following chain rule: Usually I think about partial derivatives as forming the components of the Jacobian (a.k.a differential/total derivative) and the chain rule for them as a matrix representation of the relation: How can one interpret the chain rule for the Writinger differential operators in this light? I would particularly enjoy a formalism that allows me to understand why iff is analytic, or makes this seem like a really natural definition to begin investigation of the properties of analytic functions. Alternatively I would like formalism that is closely related to the idea of complexifying the tangent bundle of with its standard complex structure, with an explanation of the connection.","\begin{equation}
\frac{\partial}{\partial z} = \frac{1}{2}\left(\frac{\partial}{\partial x} - i\frac{\partial}{\partial y}\right) \\
\frac{\partial}{\partial \bar{z}} = \frac{1}{2}\left(\frac{\partial}{\partial x} + i\frac{\partial}{\partial y}\right)
\end{equation} \begin{equation}
\frac{\partial}{\partial z}(f \circ g) = \left(\frac{\partial f}{\partial z}\circ g\right)\frac{\partial g}{\partial z} + \left(\frac{\partial f}{\partial \bar{z}}\circ g\right)\frac{\partial \bar{g}}{\partial z}.
\end{equation} \begin{equation}
 \mathbf{D}(f \circ g) = (\mathbf{D}f \circ g)\cdot\mathbf{D}g.
\end{equation} \frac{\partial{f}}{\partial \bar{z}} = 0 f \mathbb{R}^2","['complex-analysis', 'chain-rule']"
22,Straight Line Equation in Complex Plane,Straight Line Equation in Complex Plane,,"I'm confused about the straight line equation in complex plane: how does  $0 = Re((m+i)z + b)$ come from $y = mx + b$?  I mean when I see $y = mx + b$, I can draw a graph in my mind, but when I see $0 = Re((m+i)z + b)$, there is nothing on my mind. How can I connect the two equatinos? Does anyone could help me, thanks!","I'm confused about the straight line equation in complex plane: how does  $0 = Re((m+i)z + b)$ come from $y = mx + b$?  I mean when I see $y = mx + b$, I can draw a graph in my mind, but when I see $0 = Re((m+i)z + b)$, there is nothing on my mind. How can I connect the two equatinos? Does anyone could help me, thanks!",,"['complex-analysis', 'complex-numbers', 'analytic-geometry']"
23,Finite Blaschke product and proper maps on the unit disc,Finite Blaschke product and proper maps on the unit disc,,"I have to present tomorrow on an introductory section in several complex variables about proper maps, and they gloss over a fact that seems important to me, but I do not know how to prove it. Suppose $f: \Bbb D \to \Bbb D$ is an analytic proper map. Prove that $f$ is a finite Blaschke product . That is, prove that $$f(z) = e^{i \theta} \prod_{j=1}^{k} {{z-a_j} \over {1- \overline a_jz}}$$ where, $\theta$ is real, and $a_j \in \Bbb D$ . They mention that to show this, you should consider the fiber of the origin. I'm not sure what to do with this information, though.","I have to present tomorrow on an introductory section in several complex variables about proper maps, and they gloss over a fact that seems important to me, but I do not know how to prove it. Suppose is an analytic proper map. Prove that is a finite Blaschke product . That is, prove that where, is real, and . They mention that to show this, you should consider the fiber of the origin. I'm not sure what to do with this information, though.",f: \Bbb D \to \Bbb D f f(z) = e^{i \theta} \prod_{j=1}^{k} {{z-a_j} \over {1- \overline a_jz}} \theta a_j \in \Bbb D,['complex-analysis']
24,Invariance of cross ratio under Möbius transformation and another problem related to cross ratio.,Invariance of cross ratio under Möbius transformation and another problem related to cross ratio.,,"Problem statement: Given $z_1,z_2,z_3,z_4$ different points of $\overline {\mathbb C}$, we define the cross ratio $(z_1,z_2,z_3,z_4)$ as $(z_1,z_2,z_3,z_4)=\dfrac{z_1-z_2}{z_1-z_4}\dfrac{z_3-z_4}{z_3-z_2}$. Note that $(z_1,z_2,z_3,z_4)$ is the image of $z_1$ under the Möbius transformation $T$ such that $T(z_2)=0$, $T(z_3)=1$, $T(z_4)=\infty$. a) Prove that if $T \in \mathcal H$ then $(T(z_1),T(z_2),T(z_3),T(z_4))=(z_1,z_2,z_3,z_4)$. b) Show that $z_1,z_2,z_3,z_4$ lie in a line or circle if and only if $(z_1,z_2,z_3,z_4) \in \mathbb R$ My attempt at a solution: For a), using the ""hint"" they give, if $T \in \mathcal H$, I did the following: If I call $H=(z,,z_2,z_3,z_4)$, I can consider $H \circ T^{-1} (z)$. Note that $H \circ T^{-1}(T(z_2))=0$, $H \circ T^{-1} (T(z_3))=1$ and $H \circ T^{-1} (T(z_4))=\infty$. This means that $(z_1,z_2,z_3,z_4)=H \circ T^{-1}(T(z_1))=(T(z_1),T(z_2),T(z_3),T(z_4))$. I don't know if my answer is correct, I would like to check it, and if anyone has a better or different answer, he/she is very welcome to post it. For b) I am lost, for the forward implication, I've tried to show that $(z_1,z_2,z_3,z_4)=\overline {(z_1,z_2,z_3,z_4)}$ or that $arg((z_1,z_2,z_3,z_4))$ is a multiple of $\pi$ but I couldn't conclude anything. I would appreciate some help with this point.","Problem statement: Given $z_1,z_2,z_3,z_4$ different points of $\overline {\mathbb C}$, we define the cross ratio $(z_1,z_2,z_3,z_4)$ as $(z_1,z_2,z_3,z_4)=\dfrac{z_1-z_2}{z_1-z_4}\dfrac{z_3-z_4}{z_3-z_2}$. Note that $(z_1,z_2,z_3,z_4)$ is the image of $z_1$ under the Möbius transformation $T$ such that $T(z_2)=0$, $T(z_3)=1$, $T(z_4)=\infty$. a) Prove that if $T \in \mathcal H$ then $(T(z_1),T(z_2),T(z_3),T(z_4))=(z_1,z_2,z_3,z_4)$. b) Show that $z_1,z_2,z_3,z_4$ lie in a line or circle if and only if $(z_1,z_2,z_3,z_4) \in \mathbb R$ My attempt at a solution: For a), using the ""hint"" they give, if $T \in \mathcal H$, I did the following: If I call $H=(z,,z_2,z_3,z_4)$, I can consider $H \circ T^{-1} (z)$. Note that $H \circ T^{-1}(T(z_2))=0$, $H \circ T^{-1} (T(z_3))=1$ and $H \circ T^{-1} (T(z_4))=\infty$. This means that $(z_1,z_2,z_3,z_4)=H \circ T^{-1}(T(z_1))=(T(z_1),T(z_2),T(z_3),T(z_4))$. I don't know if my answer is correct, I would like to check it, and if anyone has a better or different answer, he/she is very welcome to post it. For b) I am lost, for the forward implication, I've tried to show that $(z_1,z_2,z_3,z_4)=\overline {(z_1,z_2,z_3,z_4)}$ or that $arg((z_1,z_2,z_3,z_4))$ is a multiple of $\pi$ but I couldn't conclude anything. I would appreciate some help with this point.",,['complex-analysis']
25,An analytic function with a simple pole,An analytic function with a simple pole,,"Let $f(z)$ be analytic in the disk $|z|<R \ \ \ (R>1)$ except for a simple pole at a point $z_0$, $|z_0|=1$. Consider the expansion $f(z)=a_0+a_1 z+ \cdots$, and show that $$\lim_{n \to \infty} \frac {a_n} {a_{n+1}}=z_0$$ All my attempts failed. I wanted to use the Laurent series at $z_0$ but the problem needs expansion at $0$.","Let $f(z)$ be analytic in the disk $|z|<R \ \ \ (R>1)$ except for a simple pole at a point $z_0$, $|z_0|=1$. Consider the expansion $f(z)=a_0+a_1 z+ \cdots$, and show that $$\lim_{n \to \infty} \frac {a_n} {a_{n+1}}=z_0$$ All my attempts failed. I wanted to use the Laurent series at $z_0$ but the problem needs expansion at $0$.",,['complex-analysis']
26,"Laurent series, am I correct in this reasoning?","Laurent series, am I correct in this reasoning?",,"A problem in my book asks: In the Laurent series for $\displaystyle f(z) = \frac{1}{(z-4)}$ centered at $z=1$, what is the coefficient of $(z-1)^{-2}$? The book's solution gives $$\frac{1}{4-z} = \frac{1}{z-1-3} = \frac{\frac{1}{z-1}}{(1-\frac{3}{(z-1)})} = \frac{1}{z-1} \sum_{n=0}^\infty (\frac{3}{z-1})^n,$$ so the cofficient will be $3$. However, I don't think this is correct (I don't know complex variables that well, so I'm hesitant to say the book is wrong). First, $f(z)$ is analytic at $z=1$, so shouldn't we get a unique power series expansion about $1$, and hence no negative coefficients? Second, I'm not sure their expansion of $\displaystyle 1/(1-\frac{3}{(z-1)})$ is valid near $1$, since $\displaystyle |\frac{3}{(z-1)}|$ is not less than $1$ for $z$ close to $1$. Can anyone confirm my reasoning, or explain why it's wrong? The way I did this problem was: $$\frac{1}{z-4} = \frac{1}{(z-1)-3} = \frac{-1}{3-(z-1)} = \frac{-1/3}{1-(z-1)/3}$$ and so we get $$f(z) = -\frac{1}{3} \sum_{n=0}^\infty \left(\frac{z-1}{3}\right)^n$$","A problem in my book asks: In the Laurent series for $\displaystyle f(z) = \frac{1}{(z-4)}$ centered at $z=1$, what is the coefficient of $(z-1)^{-2}$? The book's solution gives $$\frac{1}{4-z} = \frac{1}{z-1-3} = \frac{\frac{1}{z-1}}{(1-\frac{3}{(z-1)})} = \frac{1}{z-1} \sum_{n=0}^\infty (\frac{3}{z-1})^n,$$ so the cofficient will be $3$. However, I don't think this is correct (I don't know complex variables that well, so I'm hesitant to say the book is wrong). First, $f(z)$ is analytic at $z=1$, so shouldn't we get a unique power series expansion about $1$, and hence no negative coefficients? Second, I'm not sure their expansion of $\displaystyle 1/(1-\frac{3}{(z-1)})$ is valid near $1$, since $\displaystyle |\frac{3}{(z-1)}|$ is not less than $1$ for $z$ close to $1$. Can anyone confirm my reasoning, or explain why it's wrong? The way I did this problem was: $$\frac{1}{z-4} = \frac{1}{(z-1)-3} = \frac{-1}{3-(z-1)} = \frac{-1/3}{1-(z-1)/3}$$ and so we get $$f(z) = -\frac{1}{3} \sum_{n=0}^\infty \left(\frac{z-1}{3}\right)^n$$",,['complex-analysis']
27,What number is written?,What number is written?,,In each of the 28 blank hexagons in the figure real numbers are written  in such a way that the number in each inner hexagon is equal to the arithmetic mean of the numbers in the six adjacent ones. Find the value in the central hexagon (where ?). I solved that problem head-on by composing a system of 28 linear algebraic equations and solving it with help of Maple. My answer is $\frac {4986619541155196219}{4026303401170889720}=1.238510625\dots$ (if I am not  mistaken).  This is a training problem from the course of discrete complex analysis which is used to model magnetism and seepage in porous media. The comments to the problem hint that there is another solution that does not require cumbersome calculations.,In each of the 28 blank hexagons in the figure real numbers are written  in such a way that the number in each inner hexagon is equal to the arithmetic mean of the numbers in the six adjacent ones. Find the value in the central hexagon (where ?). I solved that problem head-on by composing a system of 28 linear algebraic equations and solving it with help of Maple. My answer is (if I am not  mistaken).  This is a training problem from the course of discrete complex analysis which is used to model magnetism and seepage in porous media. The comments to the problem hint that there is another solution that does not require cumbersome calculations.,\frac {4986619541155196219}{4026303401170889720}=1.238510625\dots,['complex-analysis']
28,"Compute $\int_{0}^{\infty}\frac{x\sin 2x}{9+x^{2}} \, dx$",Compute,"\int_{0}^{\infty}\frac{x\sin 2x}{9+x^{2}} \, dx","$$\int_{0}^{\infty}\frac{x\sin 2x}{9+x^{2}} \, dx$$ Some rearranging eventually gives $$\int_{0}^{\infty}\frac{x\sin 2x}{9+x^{2}} \, dx = \frac{-i}{2}\int_{-\infty}^{\infty} \frac{xe^{2ix}}{9+x^{2}}$$ Consider $f(z) = \frac{ze^{2iz}}{9+z^{2}}$ and the contour $\gamma$ of the semicircle laying in the upper half of the plane: Let $\gamma_{R}$ denote the circular part with radius $R$ and $\gamma_{L}$ denote the part lying on the real axis with length $2R$. Computing the residue at the only pole, $z = 3i$, we have that $$\oint_{\gamma}f(z) \, dz = \frac{i\pi}{e^{6}} $$ On the other hand, \begin{align*} \oint_{\gamma}f(z) \, dz &= \oint_{\gamma_{R}}f(z) \, dz + \oint_{\gamma_{L}} f(z) \, dz \\ &= \oint_{\gamma_{R}} f(z) \, dz + \int_{-R}^{R} \frac{xe^{2ix}}{9+x^{2}} \, dx \end{align*} We may evaluate the first integral in the usual way by parameterizing the contour and taking $z = Re^{i\theta}$. \begin{align*} \oint_{\gamma_{R}} f(z) \, dz = \int_{0}^{\pi} \, \frac{R^{2}e^{2i\theta}e^{2i\cos\theta}e^{-2R\sin\theta}}{(9+R^{2}e^{2i\theta})} d\theta \end{align*} I'm aiming to show that this integral goes to $0$ as $R$ goes to infinity. This gives the desired result as the factor of $\frac{-i}{2}$ is all that is missing according to WolframAlpha. I'm not sure how to finish it though.","$$\int_{0}^{\infty}\frac{x\sin 2x}{9+x^{2}} \, dx$$ Some rearranging eventually gives $$\int_{0}^{\infty}\frac{x\sin 2x}{9+x^{2}} \, dx = \frac{-i}{2}\int_{-\infty}^{\infty} \frac{xe^{2ix}}{9+x^{2}}$$ Consider $f(z) = \frac{ze^{2iz}}{9+z^{2}}$ and the contour $\gamma$ of the semicircle laying in the upper half of the plane: Let $\gamma_{R}$ denote the circular part with radius $R$ and $\gamma_{L}$ denote the part lying on the real axis with length $2R$. Computing the residue at the only pole, $z = 3i$, we have that $$\oint_{\gamma}f(z) \, dz = \frac{i\pi}{e^{6}} $$ On the other hand, \begin{align*} \oint_{\gamma}f(z) \, dz &= \oint_{\gamma_{R}}f(z) \, dz + \oint_{\gamma_{L}} f(z) \, dz \\ &= \oint_{\gamma_{R}} f(z) \, dz + \int_{-R}^{R} \frac{xe^{2ix}}{9+x^{2}} \, dx \end{align*} We may evaluate the first integral in the usual way by parameterizing the contour and taking $z = Re^{i\theta}$. \begin{align*} \oint_{\gamma_{R}} f(z) \, dz = \int_{0}^{\pi} \, \frac{R^{2}e^{2i\theta}e^{2i\cos\theta}e^{-2R\sin\theta}}{(9+R^{2}e^{2i\theta})} d\theta \end{align*} I'm aiming to show that this integral goes to $0$ as $R$ goes to infinity. This gives the desired result as the factor of $\frac{-i}{2}$ is all that is missing according to WolframAlpha. I'm not sure how to finish it though.",,['complex-analysis']
29,Good description of orbits of upper half plane under $SL_2 (Z)$,Good description of orbits of upper half plane under,SL_2 (Z),"It's known that $SL_2(Z)$ acts on $H=\{z\, |\, Im(z)>0\}$, is there a good description of orbits of $i$ and $w$, other than directly write down $=\{ \frac{ac|z|^2+bc\bar z+adz+bd}{c^2|z|^2+dc\bar z +dcz+d^2}; ad-bc=1 \}$?","It's known that $SL_2(Z)$ acts on $H=\{z\, |\, Im(z)>0\}$, is there a good description of orbits of $i$ and $w$, other than directly write down $=\{ \frac{ac|z|^2+bc\bar z+adz+bd}{c^2|z|^2+dc\bar z +dcz+d^2}; ad-bc=1 \}$?",,"['complex-analysis', 'number-theory', 'modular-arithmetic', 'modular-forms']"
30,Proving pseudo-hyperbolic distance is distance,Proving pseudo-hyperbolic distance is distance,,"The pseudo-hyperbolic distance on the unit disk is defined as: $$\rho(z,w)=\left|\dfrac{z-w}{1-\bar wz}\right|.$$ I'd like to prove it's a distance. The real problem is, as always, the triangle inequality, because the other properties are mostly obvious. That is, I need to prove: $$\rho(z,w)\leq\rho(z,t)+\rho(t,w),$$ for all $z,w,t\in\mathbb{D}$. I tried writing $z,t,w$ as real part plus $i$ times imaginary part, and ended up with a messy expression Wolfram can't handle. I tried polar coordinates, and the mess is even worse, and Wolfram's help is even less. I Googled first, but only found stuff about the Hyperbolic distance, and a pdf having this as an exercise, suggesting to also show that: $$\rho(z,w)\leq\frac{\rho(z,t)+\rho(t,w)}{1+\rho(z,t)\rho(t,w)}.$$ But that didn't help. So here I am. How do I solve this?","The pseudo-hyperbolic distance on the unit disk is defined as: $$\rho(z,w)=\left|\dfrac{z-w}{1-\bar wz}\right|.$$ I'd like to prove it's a distance. The real problem is, as always, the triangle inequality, because the other properties are mostly obvious. That is, I need to prove: $$\rho(z,w)\leq\rho(z,t)+\rho(t,w),$$ for all $z,w,t\in\mathbb{D}$. I tried writing $z,t,w$ as real part plus $i$ times imaginary part, and ended up with a messy expression Wolfram can't handle. I tried polar coordinates, and the mess is even worse, and Wolfram's help is even less. I Googled first, but only found stuff about the Hyperbolic distance, and a pdf having this as an exercise, suggesting to also show that: $$\rho(z,w)\leq\frac{\rho(z,t)+\rho(t,w)}{1+\rho(z,t)\rho(t,w)}.$$ But that didn't help. So here I am. How do I solve this?",,"['complex-analysis', 'hyperbolic-geometry']"
31,"Let $M_1,M_2,M_3,M_4$ be the suprema of $|f|$ on the edges of a square. Show that $|f(0)|\le \sqrt[4]{M_1M_2M_3M_4}$",Let  be the suprema of  on the edges of a square. Show that,"M_1,M_2,M_3,M_4 |f| |f(0)|\le \sqrt[4]{M_1M_2M_3M_4}","Let $G$ denote the interior of the square with vertices $1,i,-1,-i$. Suppose $f$ is holomorphic on $G$ extends continuously to $\overline{G}$, and $M_1,M_2,M_3,M_4$ are the suprema of $|f|$ on the edges of $G$. Show that  $$|f(0)|\le \sqrt[4]{M_1M_2M_3M_4}$$  I have an idea if we assume $f$ is never zero. By Schwarz-Christoffel formula, we can map the unit cicle to the square ,fixing the origin and the vertices. Then define $h=f(g)$ and log $|h|$, we have log$|h(0)|={1\over{2\pi}}\int_0^{2\pi}$log$|h(e^{i\theta})|d\theta$ and then we can get the inequality. However, if $f$ is zero somewhere, we can't define log $h$ then I'm stuck. Any help would be appreciated!","Let $G$ denote the interior of the square with vertices $1,i,-1,-i$. Suppose $f$ is holomorphic on $G$ extends continuously to $\overline{G}$, and $M_1,M_2,M_3,M_4$ are the suprema of $|f|$ on the edges of $G$. Show that  $$|f(0)|\le \sqrt[4]{M_1M_2M_3M_4}$$  I have an idea if we assume $f$ is never zero. By Schwarz-Christoffel formula, we can map the unit cicle to the square ,fixing the origin and the vertices. Then define $h=f(g)$ and log $|h|$, we have log$|h(0)|={1\over{2\pi}}\int_0^{2\pi}$log$|h(e^{i\theta})|d\theta$ and then we can get the inequality. However, if $f$ is zero somewhere, we can't define log $h$ then I'm stuck. Any help would be appreciated!",,['complex-analysis']
32,Composition of holomorphic functions is holomorphic,Composition of holomorphic functions is holomorphic,,"If $g(w)$ and $f(z)$ are holomorphic functions, show that $g(f(z))$ is also holomorphic. From the assumptions we have that for some $A,B$, $$\lim_{h\rightarrow 0}\frac{g(f(z)+h)-g(f(z))}{h}=A,$$ $$\lim_{h\rightarrow 0}\frac{f(z+h)-f(z)}{h}=B.$$ We want to prove that there exists $C$ with $$\lim_{h\rightarrow 0}\frac{g(f(z+h))-g(f(z))}{h}=C.$$ The second equation means, by definition of the limit, that for any $\varepsilon > 0$, there exists $\delta$ such that $0<|h|<\delta$ implies $$\left|\frac{f(z+h)-f(z)}{h}-B\right|<\varepsilon.$$ I don't know how to get to $\dfrac{g(f(z+h))-g(f(z))}{h}$ from here.","If $g(w)$ and $f(z)$ are holomorphic functions, show that $g(f(z))$ is also holomorphic. From the assumptions we have that for some $A,B$, $$\lim_{h\rightarrow 0}\frac{g(f(z)+h)-g(f(z))}{h}=A,$$ $$\lim_{h\rightarrow 0}\frac{f(z+h)-f(z)}{h}=B.$$ We want to prove that there exists $C$ with $$\lim_{h\rightarrow 0}\frac{g(f(z+h))-g(f(z))}{h}=C.$$ The second equation means, by definition of the limit, that for any $\varepsilon > 0$, there exists $\delta$ such that $0<|h|<\delta$ implies $$\left|\frac{f(z+h)-f(z)}{h}-B\right|<\varepsilon.$$ I don't know how to get to $\dfrac{g(f(z+h))-g(f(z))}{h}$ from here.",,['complex-analysis']
33,Are there functions that have $\Re(f(z))$ periodic but $f(z)$ is not periodic?,Are there functions that have  periodic but  is not periodic?,\Re(f(z)) f(z),"Let $f(z)$ be a function meromorphic in a simply connected convex domain $D$ (subset of the complex plane with positive area or the whole complex plane) where $z$ is a complex number. Are there such functions $f(z)$ where $\Re(f(z))$ is periodic in the domain (no periods larger than the domain please :p ) but $f(z)$ is not periodic? (if $D\subset \mathbb C$ it is clear that $f(z)$ is not periodic but $\Re(f(z))$ might still be for some shapes of $D$). In particular the case when $D = \mathbb C$ is interesting. (in other words $f(z)$ meromorphic over $\mathbb C$) I guess it is a similar question to ask about $\Im$ , $\operatorname{Arg}$ or $|\cdot|$ instead of $\Re$. I read about double periodic functions and Cauchy-Riemann equations but I still don't know. I can't find such a function in the literature ( i mean the one i search here , i don't mean i can't find a double periodic one in the literature of course ) and I don't know how to construct them or even if they exist.","Let $f(z)$ be a function meromorphic in a simply connected convex domain $D$ (subset of the complex plane with positive area or the whole complex plane) where $z$ is a complex number. Are there such functions $f(z)$ where $\Re(f(z))$ is periodic in the domain (no periods larger than the domain please :p ) but $f(z)$ is not periodic? (if $D\subset \mathbb C$ it is clear that $f(z)$ is not periodic but $\Re(f(z))$ might still be for some shapes of $D$). In particular the case when $D = \mathbb C$ is interesting. (in other words $f(z)$ meromorphic over $\mathbb C$) I guess it is a similar question to ask about $\Im$ , $\operatorname{Arg}$ or $|\cdot|$ instead of $\Re$. I read about double periodic functions and Cauchy-Riemann equations but I still don't know. I can't find such a function in the literature ( i mean the one i search here , i don't mean i can't find a double periodic one in the literature of course ) and I don't know how to construct them or even if they exist.",,"['complex-analysis', 'ordinary-differential-equations', 'periodic-functions']"
34,Complex valued function $ \cos\sqrt z$,Complex valued function, \cos\sqrt z,"I'm looking for the right argument why the function $ \cos\sqrt z$ is analytic on the whole complex plane. As far as I understand, a holomorphic branch of $\sqrt z$ can only be found on the cut plane (without negative numbers) since the Argument function isn't continuous everywhere. Hence $ \cos\sqrt z$ is at least holomorphic on the same domain, but how to justify that it is actually holomorphic everywhere?","I'm looking for the right argument why the function $ \cos\sqrt z$ is analytic on the whole complex plane. As far as I understand, a holomorphic branch of $\sqrt z$ can only be found on the cut plane (without negative numbers) since the Argument function isn't continuous everywhere. Hence $ \cos\sqrt z$ is at least holomorphic on the same domain, but how to justify that it is actually holomorphic everywhere?",,['complex-analysis']
35,Use rectangular contour to integrate $\int_{0}^{\infty}\frac{\sin(ax)}{e^{2\pi x}-1}dx$,Use rectangular contour to integrate,\int_{0}^{\infty}\frac{\sin(ax)}{e^{2\pi x}-1}dx,"I have been self-studying CA and find it very interesting. So, working through problems in a book I have, I ran across $$\int_{0}^{\infty}\frac{\sin(ax)}{e^{2\pi x}-1}dx=\frac{1}{4}\coth(a/2)-\frac{1}{2a}$$ and $$\int_{0}^{\infty}\frac{\sin(ax)}{e^{x}+1}dx=\frac{1}{2a}-\frac{\pi}{2\sinh(\pi a)}$$ For the former, I wrote it as $\frac{e^{aiz}}{e^{2\pi z}-1}$ and used a rectangular contour with vertices $0, \;\ R. \;\ R+i, \;\ i$ $e^{2\pi z}-1$ has poles at $ni$. Of these, I think $i$ only lies within the contour. Unless I am in error, I then calculated the residue at $i$ to be $\frac{e^{-a}}{2\pi}$ So, $2\pi i(\frac{e^{-a}}{2\pi})=ie^{-a}$ Now, where I get hung up is setting up the integrals around the contour. Two of which should tend to 0 as $R\to \infty$. Here is what I done. $$\int_{0}^{R}\frac{e^{iax}}{e^{2\pi x}-1}dx+\int_{0}^{\infty}\frac{e^{ai(R+iy)}}{e^{2\pi (R+iy)}-1}idy+\int_{R}^{0}\frac{e^{ai(x+i)}}{e^{2\pi (x+i)}-1}dx+\int_{\infty}^{0}\frac{e^{ai(iy)}}{e^{2\pi iy}-1}dy=-\sinh(a)$$ I am unsure of the limits on the second and fourth integrals. I am not so sure this is correct. The second and fourth ones, which represent the vertical sides,  should tend to 0 as $R\to\infty$. I hope :). This than gave me $(1-e^{-a})\int\frac{e^{iax}}{e^{2\pi x}-1}dx=-\sinh(a)$. Which does not look correct. I did manage to solve this using series and $\pi csc(\pi z)$, but the contour I am unsure of. Can someone lend a hand here?. Any advice on either would be appreciated. For the other one, I only posted it because it looks similar, but I believe is actually more involved and 'tougher' if you will. If I can get one, perhaps I can manage to evaluate the other. With that one, I think the same rectangle with the same vertices can be used, but $\pi i$ would have to be avoided. Perhaps a Principal Value in there somewhere. But, I was told to use vertices $0, \;\ R, \;\ R+2\pi i, \;\ 2\pi i$ Since I am relatively new to CA, setting up those integrals is the confusing part. I can do easier types of contour integrals, but want to learn more about these more challenging ones. Thanks for any assistance.","I have been self-studying CA and find it very interesting. So, working through problems in a book I have, I ran across $$\int_{0}^{\infty}\frac{\sin(ax)}{e^{2\pi x}-1}dx=\frac{1}{4}\coth(a/2)-\frac{1}{2a}$$ and $$\int_{0}^{\infty}\frac{\sin(ax)}{e^{x}+1}dx=\frac{1}{2a}-\frac{\pi}{2\sinh(\pi a)}$$ For the former, I wrote it as $\frac{e^{aiz}}{e^{2\pi z}-1}$ and used a rectangular contour with vertices $0, \;\ R. \;\ R+i, \;\ i$ $e^{2\pi z}-1$ has poles at $ni$. Of these, I think $i$ only lies within the contour. Unless I am in error, I then calculated the residue at $i$ to be $\frac{e^{-a}}{2\pi}$ So, $2\pi i(\frac{e^{-a}}{2\pi})=ie^{-a}$ Now, where I get hung up is setting up the integrals around the contour. Two of which should tend to 0 as $R\to \infty$. Here is what I done. $$\int_{0}^{R}\frac{e^{iax}}{e^{2\pi x}-1}dx+\int_{0}^{\infty}\frac{e^{ai(R+iy)}}{e^{2\pi (R+iy)}-1}idy+\int_{R}^{0}\frac{e^{ai(x+i)}}{e^{2\pi (x+i)}-1}dx+\int_{\infty}^{0}\frac{e^{ai(iy)}}{e^{2\pi iy}-1}dy=-\sinh(a)$$ I am unsure of the limits on the second and fourth integrals. I am not so sure this is correct. The second and fourth ones, which represent the vertical sides,  should tend to 0 as $R\to\infty$. I hope :). This than gave me $(1-e^{-a})\int\frac{e^{iax}}{e^{2\pi x}-1}dx=-\sinh(a)$. Which does not look correct. I did manage to solve this using series and $\pi csc(\pi z)$, but the contour I am unsure of. Can someone lend a hand here?. Any advice on either would be appreciated. For the other one, I only posted it because it looks similar, but I believe is actually more involved and 'tougher' if you will. If I can get one, perhaps I can manage to evaluate the other. With that one, I think the same rectangle with the same vertices can be used, but $\pi i$ would have to be avoided. Perhaps a Principal Value in there somewhere. But, I was told to use vertices $0, \;\ R, \;\ R+2\pi i, \;\ 2\pi i$ Since I am relatively new to CA, setting up those integrals is the confusing part. I can do easier types of contour integrals, but want to learn more about these more challenging ones. Thanks for any assistance.",,"['complex-analysis', 'contour-integration']"
36,Mathematical Olympiad question (complex variable),Mathematical Olympiad question (complex variable),,"I found this question in an old Mathematical Olympiad: Let $0<a<1$ be a real number, and let $f(z)$ be a complex polynomial such that $$|f(z)|\leq \frac{1}{|z-a|}$$ on the unit disk $|z|\leq 1$ . Prove that $$|f(a)| \leq \frac{1}{1-a^2}.$$ My attempt: Since $f$ is analytic in $\{z\in\mathbb{C}:|z|\leq 1\}$ , we have that $$|f(a)| \leq \max_{|z|=1}|f(z)|.$$ Because of the triangle inequality, for all $z\in \mathbb{C}$ with $|z|=1$ we have $$|z-a|\geq |z|-a = 1-a.$$ By applying the hypothesis, we get $$|f(a)| \leq \max_{|z|=1}|f(z)| \leq \max_{|z|=1}\frac{1}{|z-a|}\leq \frac{1}{1-a}.$$ Nevertheless, we know that $1-a^2>1-a$ since $a\in (0,1)$ . Therefore, we cannot get the desired result from the above inequality. What can I apply to complete the proof?","I found this question in an old Mathematical Olympiad: Let be a real number, and let be a complex polynomial such that on the unit disk . Prove that My attempt: Since is analytic in , we have that Because of the triangle inequality, for all with we have By applying the hypothesis, we get Nevertheless, we know that since . Therefore, we cannot get the desired result from the above inequality. What can I apply to complete the proof?","0<a<1 f(z) |f(z)|\leq \frac{1}{|z-a|} |z|\leq 1 |f(a)| \leq \frac{1}{1-a^2}. f \{z\in\mathbb{C}:|z|\leq 1\} |f(a)| \leq \max_{|z|=1}|f(z)|. z\in \mathbb{C} |z|=1 |z-a|\geq |z|-a = 1-a. |f(a)| \leq \max_{|z|=1}|f(z)| \leq \max_{|z|=1}\frac{1}{|z-a|}\leq \frac{1}{1-a}. 1-a^2>1-a a\in (0,1)","['complex-analysis', 'complex-numbers', 'contest-math']"
37,Generators for the ideal of entire functions vanishing on $\mathbb Z\times\mathbb Z\subset \mathbb C\times\mathbb C$,Generators for the ideal of entire functions vanishing on,\mathbb Z\times\mathbb Z\subset \mathbb C\times\mathbb C,"For $n=1,2$ , let $R_n$ be the ring of entire complex functions in $n$ complex variables.  Let $I$ be the ideal $R_2$ of functions $f$ such that $f(\mathbb Z\times\mathbb Z)=\{0\}$ . Is $I$ generated by the two functions $(x,y)\mapsto\sin(\pi x)$ and $(x,y)\mapsto\sin(\pi y)$ ? The reason why I am asking this is the analogous case in $R_1$ : If $J$ is the  ideal $R_1$ of functions $f$ such that $f(\mathbb Z)=\{0\}$ then, for $f\in J$ , $\frac{f(x)}{\sin(\pi x)}$ seems to be an entire function.  This implies $J=\sin(\pi x)R_1$ .","For , let be the ring of entire complex functions in complex variables.  Let be the ideal of functions such that . Is generated by the two functions and ? The reason why I am asking this is the analogous case in : If is the  ideal of functions such that then, for , seems to be an entire function.  This implies .","n=1,2 R_n n I R_2 f f(\mathbb Z\times\mathbb Z)=\{0\} I (x,y)\mapsto\sin(\pi x) (x,y)\mapsto\sin(\pi y) R_1 J R_1 f f(\mathbb Z)=\{0\} f\in J \frac{f(x)}{\sin(\pi x)} J=\sin(\pi x)R_1","['complex-analysis', 'analytic-geometry', 'ideals', 'several-complex-variables']"
38,$g(g(f(z)))=z$ and $\prod\limits_{f(x)=0}^{} g(x)=1$.,and .,g(g(f(z)))=z \prod\limits_{f(x)=0}^{} g(x)=1,"My friend recently gave me this system of functional equations, asking me if I could find holomorphic $f,g: \mathbb{C} \to \mathbb{C}$ satisfying: $g(g(f(z))) = z$ $\displaystyle\prod_{f(x)=0}^{} g(x)=1$ To which I promptly said, “no.” Thoughts? Hints?","My friend recently gave me this system of functional equations, asking me if I could find holomorphic satisfying: To which I promptly said, “no.” Thoughts? Hints?","f,g: \mathbb{C} \to \mathbb{C} g(g(f(z))) = z \displaystyle\prod_{f(x)=0}^{} g(x)=1","['complex-analysis', 'functional-equations']"
39,computing $A_2=\sum_{k=1}^{n}\frac{1}{(z_k-1)^2} $ and $\sum_{k=1}^n \cot^2\left( \frac{k\pi}{n+1}\right)$,computing  and,A_2=\sum_{k=1}^{n}\frac{1}{(z_k-1)^2}  \sum_{k=1}^n \cot^2\left( \frac{k\pi}{n+1}\right),"Assume that  $z_1,z_2,...,z_n$ are  roots of the equation $z^n+z^{n-1}+...+z+1=0$. I was asked  to compute the expressions    $$A_1=\sum_{k=1}^{n}\frac{1}{(z_k-1)} ~~~~~~and~~~~~~A_2=\sum_{k=1}^{n}\frac{1}{(z_k-1)^2} $$   Then deduces  $$B_2=\sum_{k=1}^n \cot^2\left( \frac{k\pi}{n+1}\right)$$ I managed with $A_1$ and proved that $$A_1=\sum_{k=1}^{n}\frac{1}{(z_k-1)}=-\frac{n}{2}$$ I  used the fact that $$\frac{z^{n+1}-1}{z-1}=z^n+z^{n-1}+...+z+1$$ Actually I couldn't see an apparent link between  $A_1$,  $A_2$ and $B_2$.  Can anyone help with $A_2$ and $B_2$. k","Assume that  $z_1,z_2,...,z_n$ are  roots of the equation $z^n+z^{n-1}+...+z+1=0$. I was asked  to compute the expressions    $$A_1=\sum_{k=1}^{n}\frac{1}{(z_k-1)} ~~~~~~and~~~~~~A_2=\sum_{k=1}^{n}\frac{1}{(z_k-1)^2} $$   Then deduces  $$B_2=\sum_{k=1}^n \cot^2\left( \frac{k\pi}{n+1}\right)$$ I managed with $A_1$ and proved that $$A_1=\sum_{k=1}^{n}\frac{1}{(z_k-1)}=-\frac{n}{2}$$ I  used the fact that $$\frac{z^{n+1}-1}{z-1}=z^n+z^{n-1}+...+z+1$$ Actually I couldn't see an apparent link between  $A_1$,  $A_2$ and $B_2$.  Can anyone help with $A_2$ and $B_2$. k",,"['complex-analysis', 'algebra-precalculus', 'complex-numbers', 'summation', 'roots']"
40,Complex Derivatives in Polar Form,Complex Derivatives in Polar Form,,"How do i write  $\frac{\partial f}{\partial z}$ and $\frac{\partial f}{\partial \bar z}$ in polar form. From my textbook I know, $$\frac{\partial f}{\partial z} = \frac{e^{-i\theta}}{2} \left(\frac{\partial }{\partial r} - \frac {i}{r} \frac{\partial }{\partial \theta} \right)$$ $$\frac{\partial f}{\partial \bar z} = \frac{e^{i\theta}}{2} \left(\frac{\partial }{\partial r} + \frac {i}{r} \frac{\partial }{\partial \theta} \right)$$ how do i derive these answers? i know $$ z = re^{i\theta}$$ and, $$ z = u(r,\theta) + iv(r,\theta)$$","How do i write  $\frac{\partial f}{\partial z}$ and $\frac{\partial f}{\partial \bar z}$ in polar form. From my textbook I know, $$\frac{\partial f}{\partial z} = \frac{e^{-i\theta}}{2} \left(\frac{\partial }{\partial r} - \frac {i}{r} \frac{\partial }{\partial \theta} \right)$$ $$\frac{\partial f}{\partial \bar z} = \frac{e^{i\theta}}{2} \left(\frac{\partial }{\partial r} + \frac {i}{r} \frac{\partial }{\partial \theta} \right)$$ how do i derive these answers? i know $$ z = re^{i\theta}$$ and, $$ z = u(r,\theta) + iv(r,\theta)$$",,"['complex-analysis', 'derivatives', 'proof-explanation']"
41,How many values does $\sqrt{\sqrt{i}}$ have?,How many values does  have?,\sqrt{\sqrt{i}},"Wolfram says, there are only two roots, but $\sqrt{i}$ already gives two roots. So if we express them in Cartesian form we can take square roots of them separately and end up with four roots. $$\sqrt{i}=e^{\frac{i\pi}{4}}=\frac{1}{\sqrt2}+i\frac{1}{\sqrt2}$$ But also $$\sqrt{i}=e^{\frac{-3i\pi}{4}}=-\frac{1}{\sqrt2}-i\frac{1}{\sqrt2}$$ Then take square roots of each of those and you end up with  $$\sqrt{\sqrt{i}}=e^{\frac{i\pi}{8}},e^{\frac{5i\pi}{4}},e^{\frac{-3i\pi}{4}},e^{\frac{-7i\pi}{4}}$$ It works, doesn't it? Raise each of those to the power of 4 and you get $i$. What am I missing here?","Wolfram says, there are only two roots, but $\sqrt{i}$ already gives two roots. So if we express them in Cartesian form we can take square roots of them separately and end up with four roots. $$\sqrt{i}=e^{\frac{i\pi}{4}}=\frac{1}{\sqrt2}+i\frac{1}{\sqrt2}$$ But also $$\sqrt{i}=e^{\frac{-3i\pi}{4}}=-\frac{1}{\sqrt2}-i\frac{1}{\sqrt2}$$ Then take square roots of each of those and you end up with  $$\sqrt{\sqrt{i}}=e^{\frac{i\pi}{8}},e^{\frac{5i\pi}{4}},e^{\frac{-3i\pi}{4}},e^{\frac{-7i\pi}{4}}$$ It works, doesn't it? Raise each of those to the power of 4 and you get $i$. What am I missing here?",,"['complex-analysis', 'radicals', 'wolfram-alpha']"
42,Holomorphic functions: is it true that $f(\bar{z})=\overline {f(z)}$?,Holomorphic functions: is it true that ?,f(\bar{z})=\overline {f(z)},"Is it true that  $ f(\bar{z})=\overline {f(z)}$, Where z is complex? I think it holds when $f(z)$ is holomorphic since we have $f(z)=p(x,y)+iq(x,y)=p(z,0)+iq(z,0)$ Any help...","Is it true that  $ f(\bar{z})=\overline {f(z)}$, Where z is complex? I think it holds when $f(z)$ is holomorphic since we have $f(z)=p(x,y)+iq(x,y)=p(z,0)+iq(z,0)$ Any help...",,['complex-analysis']
43,Prove that $\cos(z)$ and $\sin(z)$ are surjective over the complex numbers. [duplicate],Prove that  and  are surjective over the complex numbers. [duplicate],\cos(z) \sin(z),"This question already has answers here : Proving surjectivity of $\cos(z)$ and $\sin(z)$ and find all $z : \cos(z) \in \mathbb R$ and all $z: \sin(z) \in \mathbb R$ (3 answers) Closed 10 years ago . I have an exercise that says: (a) Prove that $\cos(z)$ and $\sin(z)$ are surjective functions from $\mathbb C \to \mathbb C$. (b) Find the solutions of the equation $\cos(z)=\dfrac{5}{4}$. As far as part (a) goes, I have no idea how to show surjectivity. I know by definition that $\cos(z)=\dfrac{e^{iz}+e^{-iz}}{2}$ and $\sin(z)=\dfrac{e^{iz}-e^{-iz}}{2i}$. I want to show that given $w \in \mathbb C$, there are $z_1, z_2 \in \mathbb C$ such that $\cos(z_1)=w$ and $\sin(z_2)=w$. I've tried to play with the expressions from above but I couldn't get to anything. I need help on this. For part (b), once I've proved (a), then I know (b) makes sense, i.e., there exist solutions for that equation. $\cos(z)=\dfrac{5}{4}$ iff $\dfrac{e^{iz}+e^{-iz}}{2}=\dfrac{5}{4}$ Again, I have no idea what can I get out from this equation.","This question already has answers here : Proving surjectivity of $\cos(z)$ and $\sin(z)$ and find all $z : \cos(z) \in \mathbb R$ and all $z: \sin(z) \in \mathbb R$ (3 answers) Closed 10 years ago . I have an exercise that says: (a) Prove that $\cos(z)$ and $\sin(z)$ are surjective functions from $\mathbb C \to \mathbb C$. (b) Find the solutions of the equation $\cos(z)=\dfrac{5}{4}$. As far as part (a) goes, I have no idea how to show surjectivity. I know by definition that $\cos(z)=\dfrac{e^{iz}+e^{-iz}}{2}$ and $\sin(z)=\dfrac{e^{iz}-e^{-iz}}{2i}$. I want to show that given $w \in \mathbb C$, there are $z_1, z_2 \in \mathbb C$ such that $\cos(z_1)=w$ and $\sin(z_2)=w$. I've tried to play with the expressions from above but I couldn't get to anything. I need help on this. For part (b), once I've proved (a), then I know (b) makes sense, i.e., there exist solutions for that equation. $\cos(z)=\dfrac{5}{4}$ iff $\dfrac{e^{iz}+e^{-iz}}{2}=\dfrac{5}{4}$ Again, I have no idea what can I get out from this equation.",,"['complex-analysis', 'complex-numbers']"
44,Show $f$ is analytic if $f^8$ is analytic,Show  is analytic if  is analytic,f f^8,"This is from Gamelin's book on Complex Analysis. Problem : Show that if $f(z)$ is continuous on a domain $D$, and if $f(z)^8$ is analytic on $D$, then $f(z)$ is analytic on $D$. (I assume the intention is that $D$ is nice, i.e. open and connected) I'm not exactly sure how to approach this. I'm guessing it has something to do with zeroes of $f$. For example, around non-zeroes, $f$ is analytic since $z^{1/8}$ is nonzero in a neighborhood. Or something along those lines. The details are eluding me. Any help would be appreciated!","This is from Gamelin's book on Complex Analysis. Problem : Show that if $f(z)$ is continuous on a domain $D$, and if $f(z)^8$ is analytic on $D$, then $f(z)$ is analytic on $D$. (I assume the intention is that $D$ is nice, i.e. open and connected) I'm not exactly sure how to approach this. I'm guessing it has something to do with zeroes of $f$. For example, around non-zeroes, $f$ is analytic since $z^{1/8}$ is nonzero in a neighborhood. Or something along those lines. The details are eluding me. Any help would be appreciated!",,"['complex-analysis', 'analysis', 'complex-numbers']"
45,An entire function of strict order 2,An entire function of strict order 2,,"Here is a problem from Stein and Shakarchi Complex Analysis , can somebody help me to solve it? I guess we can use Phragmen-Lindelof theorem but I don't know the exact way. Suppose $f(z)$ is an entire function s.t. $f(z)=O(e^{c_1|z|^2})$ for some $c_1>0$, and for $x$ real $f(x)=O(e^{-c_2|x|^2})$ for some $c_2>0$. Then $f(x+iy)=O(e^{-ax^2+by^2})$ for some $a,b>0$.","Here is a problem from Stein and Shakarchi Complex Analysis , can somebody help me to solve it? I guess we can use Phragmen-Lindelof theorem but I don't know the exact way. Suppose $f(z)$ is an entire function s.t. $f(z)=O(e^{c_1|z|^2})$ for some $c_1>0$, and for $x$ real $f(x)=O(e^{-c_2|x|^2})$ for some $c_2>0$. Then $f(x+iy)=O(e^{-ax^2+by^2})$ for some $a,b>0$.",,"['complex-analysis', 'asymptotics']"
46,Why is it clear from this formulation that f is continuous wherever it is holomorphic?,Why is it clear from this formulation that f is continuous wherever it is holomorphic?,,"Hi I am new on here so not sure if this is right place to post but quick and presumably easy question: So holomorphic at a point $z_0 \in \Omega$ is defined as the limit as $h\rightarrow 0$ of  $\frac{f(z_0+h)-f(z_0)}{h}$. My textbook says that we can rewrite this as $f(z_0+h)-f(z_0)-ah=h\psi(h)$, where $\psi$ is a function defined for all small $h$ and the limit as $h\rightarrow 0$ of $\psi(h)=0$ and $a=f'(z_0)$. Why does this reformulation that $f$ is continuous wherever it is holomorphic? Thanks JG","Hi I am new on here so not sure if this is right place to post but quick and presumably easy question: So holomorphic at a point $z_0 \in \Omega$ is defined as the limit as $h\rightarrow 0$ of  $\frac{f(z_0+h)-f(z_0)}{h}$. My textbook says that we can rewrite this as $f(z_0+h)-f(z_0)-ah=h\psi(h)$, where $\psi$ is a function defined for all small $h$ and the limit as $h\rightarrow 0$ of $\psi(h)=0$ and $a=f'(z_0)$. Why does this reformulation that $f$ is continuous wherever it is holomorphic? Thanks JG",,"['complex-analysis', 'limits', 'derivatives', 'continuity']"
47,How to compute the monodromy group for a given differential equation.,How to compute the monodromy group for a given differential equation.,,"Given a differential equation, say  $\frac{d^2f}{dz^2}+\frac{f}{z-5}=0$. We know that this equation has two linearly independent solutions $f_1(z), f_2(z)$. By analytic continuation, the solution $f_i$ is taken to $g_i$, $i=1, 2$, and $g_i=c_{i1}f_1+c_{i2}f_2$. The set of matrices $(c_{ij})$ form a group called monodromy group. My question is how to compute the monodromy group explicitly?","Given a differential equation, say  $\frac{d^2f}{dz^2}+\frac{f}{z-5}=0$. We know that this equation has two linearly independent solutions $f_1(z), f_2(z)$. By analytic continuation, the solution $f_i$ is taken to $g_i$, $i=1, 2$, and $g_i=c_{i1}f_1+c_{i2}f_2$. The set of matrices $(c_{ij})$ form a group called monodromy group. My question is how to compute the monodromy group explicitly?",,"['complex-analysis', 'ordinary-differential-equations']"
48,Understanding the Schwarz reflection principle,Understanding the Schwarz reflection principle,,"I am currently reading Stein and Shakarchi's Complex Analysis, and I think there is something I am not quite understanding about the Schwarz reflection principle. Here is my problem: Suppose $f$ is a holomorphic function on $\Omega^+$ (an open subset of the upper complex plane) that extends continuously to $I$ (a subset of $\mathbb{R}$). Let $\Omega^-$ be the reflection of $\Omega^+$ across the real axis. Take $F(z) = f(z)$ if $z \in \Omega^+$ and $F(z) = f(\overline{z})$ is $z \in \Omega^-$. We can extend $F$ continuously to $I$. Why isn't the function $F$ holomorphic on $\Omega^+ \cup I \cup \Omega^-$? I think there's some detail of a proof that I overlooked. My intuition tells me that $F$ isn't holomorphic for the same reason that a function defined on $\mathbb{R}^+$ isn't necessarily differentiable at zero if you extend it to be an even function.","I am currently reading Stein and Shakarchi's Complex Analysis, and I think there is something I am not quite understanding about the Schwarz reflection principle. Here is my problem: Suppose $f$ is a holomorphic function on $\Omega^+$ (an open subset of the upper complex plane) that extends continuously to $I$ (a subset of $\mathbb{R}$). Let $\Omega^-$ be the reflection of $\Omega^+$ across the real axis. Take $F(z) = f(z)$ if $z \in \Omega^+$ and $F(z) = f(\overline{z})$ is $z \in \Omega^-$. We can extend $F$ continuously to $I$. Why isn't the function $F$ holomorphic on $\Omega^+ \cup I \cup \Omega^-$? I think there's some detail of a proof that I overlooked. My intuition tells me that $F$ isn't holomorphic for the same reason that a function defined on $\mathbb{R}^+$ isn't necessarily differentiable at zero if you extend it to be an even function.",,['complex-analysis']
49,"Proof that $\frac{1}{2\pi i}\oint f'(z)/f(z) \, dz = n$",Proof that,"\frac{1}{2\pi i}\oint f'(z)/f(z) \, dz = n","My text gives a much more complicated proof of this result, which makes me wonder if the argument I have in my head for this has something wrong with it.  Does this work, or have I made a bad assumption somewhere along the line? Let $U \subseteq \mathbb{C}$ be open, $\overline{D}(z_0, r) \subset U$, and $f : U \rightarrow \mathbb{C}$ be holomorphic with a zero of order $n$ at $z_0$ and no other zeroes in $U$. Taking a power series expansion at $z_0$, $$f(z) = a_n(z-z_0)^n + o((z-z_0)^{n+1}).$$ Differentiating, $$f'(z) = n a_n(z-z_0)^{n-1} + o((z-z_0)^n),$$ so we have $$\lim_{z \rightarrow z_0} \frac{(z-z_0) f'(z)}{f(z)} = \lim_{z \rightarrow z_0} \frac{n a_n(z-z_0)^n + o((z-z_0)^{n+1})}{a_n(z-z_0)^n + o((z-z_0)^{n+1})} = n.$$ Define the function $g : U \rightarrow \mathbb{C}$ by $$g(z) = \begin{cases} (z-z_0) f'(z)/f(z) & \text{if } z \neq z_0 \\ n & \text{otherwise} \end{cases}$$ By the Cauchy integral formula, $$n = g(z_0) = \frac{1}{2 \pi i} \oint_{\partial\overline{D}(z_0, r)} \frac{g(z)}{z - z_0}  dz =  \frac{1}{2 \pi i} \oint_{\partial\overline{D}(z_0, r)} \frac{(z-z_0)f'(z)}{(z-z_0)f(z)} dz=  \frac{1}{2 \pi i} \oint_{\partial\overline{D}(z_0, r)} \frac{f'(z)}{f(z)} dz$$ as desired.","My text gives a much more complicated proof of this result, which makes me wonder if the argument I have in my head for this has something wrong with it.  Does this work, or have I made a bad assumption somewhere along the line? Let $U \subseteq \mathbb{C}$ be open, $\overline{D}(z_0, r) \subset U$, and $f : U \rightarrow \mathbb{C}$ be holomorphic with a zero of order $n$ at $z_0$ and no other zeroes in $U$. Taking a power series expansion at $z_0$, $$f(z) = a_n(z-z_0)^n + o((z-z_0)^{n+1}).$$ Differentiating, $$f'(z) = n a_n(z-z_0)^{n-1} + o((z-z_0)^n),$$ so we have $$\lim_{z \rightarrow z_0} \frac{(z-z_0) f'(z)}{f(z)} = \lim_{z \rightarrow z_0} \frac{n a_n(z-z_0)^n + o((z-z_0)^{n+1})}{a_n(z-z_0)^n + o((z-z_0)^{n+1})} = n.$$ Define the function $g : U \rightarrow \mathbb{C}$ by $$g(z) = \begin{cases} (z-z_0) f'(z)/f(z) & \text{if } z \neq z_0 \\ n & \text{otherwise} \end{cases}$$ By the Cauchy integral formula, $$n = g(z_0) = \frac{1}{2 \pi i} \oint_{\partial\overline{D}(z_0, r)} \frac{g(z)}{z - z_0}  dz =  \frac{1}{2 \pi i} \oint_{\partial\overline{D}(z_0, r)} \frac{(z-z_0)f'(z)}{(z-z_0)f(z)} dz=  \frac{1}{2 \pi i} \oint_{\partial\overline{D}(z_0, r)} \frac{f'(z)}{f(z)} dz$$ as desired.",,['complex-analysis']
50,What do they mean by radius of convergence?,What do they mean by radius of convergence?,,"I am looking at a couple different page on the definition of Radius of Convergence, specifically for Taylor series. I first learned it as follows: For a power series $$\sum_{k=0}^\infty a_k (z-z_0)^k$$ the radius of convergence is a unique real number $R\in\mathbb R \cup \{0,\infty\}$ where the sum converges when $|z-z_0|<R$ and diverges when $|z-z_0|>R$ . However, I was told a different definition/convention specific to Taylor series: For a function $f:U\to\mathbb C$ and any $z_0\in U$ we say the radius of convergence for the Taylor series centered at $z_0$ is the largest $R$ for which the Taylor series converges to $f$ on $D(z_0;R)$ . So it not only needs to converge, it has to converge to $f$ . This two definitions are clearly different, for example, consider the function $g:\mathbb C\setminus\{1\}$ where $g(z)=0$ . The Taylor series of $g$ centered at $z_0=0$ is $0$ . Using the first definition we know that the radius of convergence of this Taylor series is $\infty$ . Using the second definition we see that the radius of convergence is actually $1$ , since it does not converge to $g$ at $z=1$ . This is confusing already but upon a bit of searching, it seems like both of these definition contradicts with this fact in the wikipedia article: https://en.wikipedia.org/wiki/Analyticity_of_holomorphic_functions which says: ""the radius of convergence is always the distance from the center $a$ to the nearest non-removable singularity;"" Using the same $g$ as above, this fact produces contradictory result with the second definition. Consider $h:\mathbb C\setminus \mathbb R^-\to \mathbb C$ where $h(z)=0$ , $h$ has no removable singularity since none of them are isolated. Now the Taylor series of $h$ at $z_0=1$ is $0$ and has radius of convergence $\infty$ . However, the distance between $z_0$ and the nearest non-removable singularity is $1$ , since $\mathbb R^-$ is a set of non-removable singularities. So $h$ shows that the fact is contradictory with the first definition. Given those, I have the following questions: Is anything I have stated incorrect? Is it a convention to define radius of convergence differently for Taylor series? (instead of the series converging, it has to converge to the function where the series is defined on) Is the fact listed in wikipedia correct? Is it yet another convention for ""radius of convergence""?","I am looking at a couple different page on the definition of Radius of Convergence, specifically for Taylor series. I first learned it as follows: For a power series the radius of convergence is a unique real number where the sum converges when and diverges when . However, I was told a different definition/convention specific to Taylor series: For a function and any we say the radius of convergence for the Taylor series centered at is the largest for which the Taylor series converges to on . So it not only needs to converge, it has to converge to . This two definitions are clearly different, for example, consider the function where . The Taylor series of centered at is . Using the first definition we know that the radius of convergence of this Taylor series is . Using the second definition we see that the radius of convergence is actually , since it does not converge to at . This is confusing already but upon a bit of searching, it seems like both of these definition contradicts with this fact in the wikipedia article: https://en.wikipedia.org/wiki/Analyticity_of_holomorphic_functions which says: ""the radius of convergence is always the distance from the center to the nearest non-removable singularity;"" Using the same as above, this fact produces contradictory result with the second definition. Consider where , has no removable singularity since none of them are isolated. Now the Taylor series of at is and has radius of convergence . However, the distance between and the nearest non-removable singularity is , since is a set of non-removable singularities. So shows that the fact is contradictory with the first definition. Given those, I have the following questions: Is anything I have stated incorrect? Is it a convention to define radius of convergence differently for Taylor series? (instead of the series converging, it has to converge to the function where the series is defined on) Is the fact listed in wikipedia correct? Is it yet another convention for ""radius of convergence""?","\sum_{k=0}^\infty a_k (z-z_0)^k R\in\mathbb R \cup \{0,\infty\} |z-z_0|<R |z-z_0|>R f:U\to\mathbb C z_0\in U z_0 R f D(z_0;R) f g:\mathbb C\setminus\{1\} g(z)=0 g z_0=0 0 \infty 1 g z=1 a g h:\mathbb C\setminus \mathbb R^-\to \mathbb C h(z)=0 h h z_0=1 0 \infty z_0 1 \mathbb R^- h",['complex-analysis']
51,Fourier Transform of $\frac{1}{\sqrt{|x|}}$ [duplicate],Fourier Transform of  [duplicate],\frac{1}{\sqrt{|x|}},"This question already has answers here : Fourier transform of $ |x|^{s} $ and $\log|x| $ (5 answers) Closed last month . I want to find the fourier transform of $\frac{1}{\sqrt{|x|}}$. I checked the table of common fourier transforms in Wikipedia, and I know the answer should be $$\sqrt{\frac{2\pi}{|\omega|}}$$ What I can't find out, however, is why that is the answer. I tried  $$ \hat{f}(\omega) = \int_{-\infty}^{\infty} \frac{1}{\sqrt{|x|}} e^{-i\omega x} dx$$ $$ = \int_{0}^{\infty} \frac{1}{\sqrt{x}} e^{-i\omega x} dx + \int_{0}^{\infty} \frac{1}{\sqrt{x}} e^{i\omega x} dx$$ but that just gives me two unsolvable exponential integrals. I also tried finding the answer through residue calculus, as the function has a single singularity at 0, which yields $$ \hat{f}(\omega) = 2\pi i \ Res_{z = 0} \frac{e^{-i \omega z}}{\sqrt{|z|}} = 2\pi i \lim_{z \to 0} (e^{-i \omega z}) = 2\pi i$$ What am I doing wrong? Or am I thinking completely in the wrong direction? Thanks in advance!","This question already has answers here : Fourier transform of $ |x|^{s} $ and $\log|x| $ (5 answers) Closed last month . I want to find the fourier transform of $\frac{1}{\sqrt{|x|}}$. I checked the table of common fourier transforms in Wikipedia, and I know the answer should be $$\sqrt{\frac{2\pi}{|\omega|}}$$ What I can't find out, however, is why that is the answer. I tried  $$ \hat{f}(\omega) = \int_{-\infty}^{\infty} \frac{1}{\sqrt{|x|}} e^{-i\omega x} dx$$ $$ = \int_{0}^{\infty} \frac{1}{\sqrt{x}} e^{-i\omega x} dx + \int_{0}^{\infty} \frac{1}{\sqrt{x}} e^{i\omega x} dx$$ but that just gives me two unsolvable exponential integrals. I also tried finding the answer through residue calculus, as the function has a single singularity at 0, which yields $$ \hat{f}(\omega) = 2\pi i \ Res_{z = 0} \frac{e^{-i \omega z}}{\sqrt{|z|}} = 2\pi i \lim_{z \to 0} (e^{-i \omega z}) = 2\pi i$$ What am I doing wrong? Or am I thinking completely in the wrong direction? Thanks in advance!",,"['complex-analysis', 'fourier-analysis', 'fourier-transform']"
52,Holomorphic Parameter Integral,Holomorphic Parameter Integral,,"Let $U\subseteq\mathbb{C}$ be open, $\gamma$ a way in $\mathbb{C}$ which is picewise differentiable continiously and $f\colon rg(\gamma)\times U\to\mathbb{C}$ a continious function. Consider the parameter integral     $$ F(z):=\int_{\gamma}f(\omega,z), d\omega, z\in U. $$     Show: Let the function $z\mapsto f(\omega,z)$ be holomorphic in $U$ for every $\omega\in rg(\gamma)$ with continious derivation $\frac{\partial}{\partial z}f(\omega,z)$ on $rg(\gamma)\times U$. Then $F(z)$ is holomorphic in $U$, and one can differentiate under the integral:     $$ F'(z)=\int_{\gamma}\frac{\partial}{\partial z}f(\omega,z)\, d\omega, z\in U $$ Unfortunately I do not know how to show that. I know the proof when $f$ is a function with values in $\mathbb{R}$ but cannot proof it here when $f$ has values in $\mathbb{C}$.","Let $U\subseteq\mathbb{C}$ be open, $\gamma$ a way in $\mathbb{C}$ which is picewise differentiable continiously and $f\colon rg(\gamma)\times U\to\mathbb{C}$ a continious function. Consider the parameter integral     $$ F(z):=\int_{\gamma}f(\omega,z), d\omega, z\in U. $$     Show: Let the function $z\mapsto f(\omega,z)$ be holomorphic in $U$ for every $\omega\in rg(\gamma)$ with continious derivation $\frac{\partial}{\partial z}f(\omega,z)$ on $rg(\gamma)\times U$. Then $F(z)$ is holomorphic in $U$, and one can differentiate under the integral:     $$ F'(z)=\int_{\gamma}\frac{\partial}{\partial z}f(\omega,z)\, d\omega, z\in U $$ Unfortunately I do not know how to show that. I know the proof when $f$ is a function with values in $\mathbb{R}$ but cannot proof it here when $f$ has values in $\mathbb{C}$.",,[]
53,Show that an entire function is a polynomial,Show that an entire function is a polynomial,,"There is a question in the book that asks me to show that if f is an entire function such that $|f(z)| \le L|z|^m$ where $|z| \ge R$ , then $f$ is a polynomial of degree of at most $m$ . The problem gives me a hint that I should use the Cauchy estimates for n>m and $r \to \infty$ The below is from a post https://math.stackexchange.com/a/143881/64742 Since $f$ is entire, it is equal to a power series centered at zero with radius of convergence $\infty$ , which must match its Taylor series there. $$f(z)=\sum_{n=0}^\infty \frac{f^{(n)}(0)}{n!}z^n$$ Since $|f(z)|\leq k|z|^m$ , Cauchy's estimate gives us $$|f^{(n)}(0)|\leq \frac{n!k|z|^m}{R^n}$$ for all $|z|=R$ . For $n>m$ , letting $R\rightarrow\infty$ , we see that $|f^{(n)}|=0$ . It follows that $f$ is a polynomial of degree $\leq m$ . Now, I follow what the above answer says except where ""It follows that f is a polynomial of degree $ \le m$ . Why do we arrive at that conclusion? The preceding statement merely says that $ |f^{(n)}|=0 $","There is a question in the book that asks me to show that if f is an entire function such that where , then is a polynomial of degree of at most . The problem gives me a hint that I should use the Cauchy estimates for n>m and The below is from a post https://math.stackexchange.com/a/143881/64742 Since is entire, it is equal to a power series centered at zero with radius of convergence , which must match its Taylor series there. Since , Cauchy's estimate gives us for all . For , letting , we see that . It follows that is a polynomial of degree . Now, I follow what the above answer says except where ""It follows that f is a polynomial of degree . Why do we arrive at that conclusion? The preceding statement merely says that",|f(z)| \le L|z|^m |z| \ge R f m r \to \infty f \infty f(z)=\sum_{n=0}^\infty \frac{f^{(n)}(0)}{n!}z^n |f(z)|\leq k|z|^m |f^{(n)}(0)|\leq \frac{n!k|z|^m}{R^n} |z|=R n>m R\rightarrow\infty |f^{(n)}|=0 f \leq m  \le m  |f^{(n)}|=0 ,['complex-analysis']
54,Problem $13.7$ in Apostol's Mathematical Analysis.,Problem  in Apostol's Mathematical Analysis.,13.7,"I am looking at problem $13.7$ in Apostol's Mathematical Analysis. Let $D=\{|z|<1\}$ and let $f=u+iv$ be such that $(\rm i)$ $u,v\in{\mathcal C}^1(D)$ $(\rm ii)$ $f$ is continuous on $\overline D$ $(\rm iii)$ $f(z)=z$ on $\partial D$ . $(\rm iv)$ $J_f(z)>0$ if $z\in D$ . Show that $(1)$ For every open $\Omega\subseteq D$ , $f(\Omega)$ is open in $f(D)$ . $(2)$ $f(D)$ is an open ball of radius $1$ . $(3)$ For each $w\in f(D)$ , $D\cap f^{-1}(w)$ is finite. I have only proven $(1)$ : PROOF1 Since $u,v$ are of class $\mathcal C^1$ over $D$ , so is $f$ , and since the Jacobian doesn't vanish, the inverse function theorem says that for each $z\in D$ there exists an open nbhd $N_z$ such that $f\mid_{N_z}$ is a diffeomorphism of class $C_1$ , thus $f\mid_{N_z}$ is a homeomorphism, so it is an open map. Let $\Omega\subseteq D$ be open, and choose $z\in\Omega$ . Then there exists $N_z$ as above. Let $\widetilde N_z=N_z\cap \Omega$ . This is open and lies inside $N_z$ so its image under $f\mid_{N_z}$ is open by the above. Then $f(\Omega)=f(\Omega)\cap f(D)=\bigcup_{z\in\Omega}f\left(N_z\right)\cap f(D)$ is the intersection of the union of open sets with $f(D)$ , so it is open in $f(D)$ . $\blacktriangle$ . PROOF2 Suppose that $f(D)\not\subseteq D$ . Then certainly $f(\bar D)\not\subseteq \bar D$ . Thus, there exists $q\in\bar D$ such that $|f(q)|>1$ . But $|f|$ is continuous on the compact $\bar D$ ; thus it attains its maximum on some $p\in\bar D$ . But we cannot have $p\in\partial D$ since we would have $|f(p)|=1$ , so it must be the case $p\in D$ . Let $K=\overline B$ where $B=B(0,r)\; ;\; r=|f(p)|$ . Then $f(\bar D)\subseteq \bar B$ , and $f(p)\in\partial B$ . Let $N_p$ be an open nbhd of $p$ in $D$ . Then we see $f(N_p)$ is not open in $f(D)$ , contrary to what was proven in $(1)$ . It follows that $f(D)\subseteq D$ . PROOF3 Suppose for the sake of a contradiction that there exists $w\in f(D)=D$ such that $D \cap f^{-1}(w)$ is infinite. Since this is a subset of the compact $\bar D$ , it must have an accumulation point in $\alpha \in \bar D$ . Suppose first that $\alpha\in D$ . Then for each $\epsilon >0$ the ball $B(\alpha;\epsilon)$ contains infinitely many points of $\{z\in D:f(z)=w\}$ , in contradiction with the fact $f$ is a local diffeomorphism in $\alpha$ . Suppose $\alpha\in \partial D$ . Then $f(\alpha)=\alpha$ , and by continuity, $f(\alpha)=w$ , which is absurd since $D\cap\partial D=\varnothing$ . $\blacktriangle$ . Chapter $13$ is ""Implicit functions and extremum problems"". I am not sure what theorems to use to prove $(2)$ , and I haven't thought about $(3)$ , so I will worry about $(3)$ later. I want to prove $(2)$ now. Any hints? ADD If $A$ is any set, $\partial A$ denotes its boundary and $\overline A$ its closure.","I am looking at problem in Apostol's Mathematical Analysis. Let and let be such that is continuous on on . if . Show that For every open , is open in . is an open ball of radius . For each , is finite. I have only proven : PROOF1 Since are of class over , so is , and since the Jacobian doesn't vanish, the inverse function theorem says that for each there exists an open nbhd such that is a diffeomorphism of class , thus is a homeomorphism, so it is an open map. Let be open, and choose . Then there exists as above. Let . This is open and lies inside so its image under is open by the above. Then is the intersection of the union of open sets with , so it is open in . . PROOF2 Suppose that . Then certainly . Thus, there exists such that . But is continuous on the compact ; thus it attains its maximum on some . But we cannot have since we would have , so it must be the case . Let where . Then , and . Let be an open nbhd of in . Then we see is not open in , contrary to what was proven in . It follows that . PROOF3 Suppose for the sake of a contradiction that there exists such that is infinite. Since this is a subset of the compact , it must have an accumulation point in . Suppose first that . Then for each the ball contains infinitely many points of , in contradiction with the fact is a local diffeomorphism in . Suppose . Then , and by continuity, , which is absurd since . . Chapter is ""Implicit functions and extremum problems"". I am not sure what theorems to use to prove , and I haven't thought about , so I will worry about later. I want to prove now. Any hints? ADD If is any set, denotes its boundary and its closure.","13.7 D=\{|z|<1\} f=u+iv (\rm i) u,v\in{\mathcal C}^1(D) (\rm ii) f \overline D (\rm iii) f(z)=z \partial D (\rm iv) J_f(z)>0 z\in D (1) \Omega\subseteq D f(\Omega) f(D) (2) f(D) 1 (3) w\in f(D) D\cap f^{-1}(w) (1) u,v \mathcal C^1 D f z\in D N_z f\mid_{N_z} C_1 f\mid_{N_z} \Omega\subseteq D z\in\Omega N_z \widetilde N_z=N_z\cap \Omega N_z f\mid_{N_z} f(\Omega)=f(\Omega)\cap f(D)=\bigcup_{z\in\Omega}f\left(N_z\right)\cap f(D) f(D) f(D) \blacktriangle f(D)\not\subseteq D f(\bar D)\not\subseteq \bar D q\in\bar D |f(q)|>1 |f| \bar D p\in\bar D p\in\partial D |f(p)|=1 p\in D K=\overline B B=B(0,r)\; ;\; r=|f(p)| f(\bar D)\subseteq \bar B f(p)\in\partial B N_p p D f(N_p) f(D) (1) f(D)\subseteq D w\in f(D)=D D \cap f^{-1}(w) \bar D \alpha \in \bar D \alpha\in D \epsilon >0 B(\alpha;\epsilon) \{z\in D:f(z)=w\} f \alpha \alpha\in \partial D f(\alpha)=\alpha f(\alpha)=w D\cap\partial D=\varnothing \blacktriangle 13 (2) (3) (3) (2) A \partial A \overline A","['complex-analysis', 'multivariable-calculus']"
55,Is a curve homologous to zero according to Ahlfors actually homologous to zero?,Is a curve homologous to zero according to Ahlfors actually homologous to zero?,,"The presentation of the homology version of Cauchy's theorem in Ahlfors is slick, but sweeps a lot of the topology under the rug using clever arguments. This question is an attempt to reconcile Ahlfors' analytic notion of a curve being homologous to zero (presented in his book Complex Analysis and originally due to E. Artin, I believe) with the standard definition in homology as found in Hatcher. We work in the complex plane and fix $a\in \mathbb C$. Let $\gamma$ be a continuous map $[0,1]\rightarrow C\backslash \{ a\}$. Following Munkres in his book Topology , we define the winding number of $\gamma$ with respect to the point $a$ by considering $$g(t)=\frac{\gamma(t)-a}{|\gamma(t)-a|}.$$ This is clearly a loop in $S^1$ and corresponds to some multiple of of the generator of the fundamental group of $S^1$. If the generator is $\tau$ and $g(t)$ corresponds to $m\tau$, $m\in\mathbb Z$, we define the winding number $n(\gamma, a)$ to be $m$. This is the definition of winding number I will use in this question, but in case $\gamma$ is piecewise differentiable, it corresponds to the analytic definition by integration given in Ahlfors. Ahlfors calls a curve contained in an open region $\Omega$ homologous to zero if $n(\gamma,a)=0$ for all $a\in \Omega^c$. In homology theory, as I understand it, we would call a curve homologous to zero if it represents the zero element in $H_1(\Omega, \mathbb Z)$. That is, $\gamma$ is the boundary of some singular $2$-chain. My question: Why do all of these notions agree? Why is Ahlfors' definition of being homologous to zero (using Munkres' definition of winding number) agree with the usual homological one? I would like to use Munkres' definition because it works for continuous curves, not just piecewise differentiable ones, and it seems to me the equivalence should hold in this generality. Edit: This result appears as proposition 1.9.13 in Berenstein and Gay's book on complex analysis.","The presentation of the homology version of Cauchy's theorem in Ahlfors is slick, but sweeps a lot of the topology under the rug using clever arguments. This question is an attempt to reconcile Ahlfors' analytic notion of a curve being homologous to zero (presented in his book Complex Analysis and originally due to E. Artin, I believe) with the standard definition in homology as found in Hatcher. We work in the complex plane and fix $a\in \mathbb C$. Let $\gamma$ be a continuous map $[0,1]\rightarrow C\backslash \{ a\}$. Following Munkres in his book Topology , we define the winding number of $\gamma$ with respect to the point $a$ by considering $$g(t)=\frac{\gamma(t)-a}{|\gamma(t)-a|}.$$ This is clearly a loop in $S^1$ and corresponds to some multiple of of the generator of the fundamental group of $S^1$. If the generator is $\tau$ and $g(t)$ corresponds to $m\tau$, $m\in\mathbb Z$, we define the winding number $n(\gamma, a)$ to be $m$. This is the definition of winding number I will use in this question, but in case $\gamma$ is piecewise differentiable, it corresponds to the analytic definition by integration given in Ahlfors. Ahlfors calls a curve contained in an open region $\Omega$ homologous to zero if $n(\gamma,a)=0$ for all $a\in \Omega^c$. In homology theory, as I understand it, we would call a curve homologous to zero if it represents the zero element in $H_1(\Omega, \mathbb Z)$. That is, $\gamma$ is the boundary of some singular $2$-chain. My question: Why do all of these notions agree? Why is Ahlfors' definition of being homologous to zero (using Munkres' definition of winding number) agree with the usual homological one? I would like to use Munkres' definition because it works for continuous curves, not just piecewise differentiable ones, and it seems to me the equivalence should hold in this generality. Edit: This result appears as proposition 1.9.13 in Berenstein and Gay's book on complex analysis.",,"['complex-analysis', 'algebraic-topology', 'homology-cohomology']"
56,Location of zeros of a sum of exponentials,Location of zeros of a sum of exponentials,,Describe the approximate locations of the zeros of the function $$ f(z) = e^{iz}+e^{-iz}+e^z $$ lying outside the circle $|z|=R >>1$. Another prelim problem.  For Rouche's theorem we need to find a bounded domain.  I tried for $R<|z|<r$ and then looked in the left half plane so that $|e^z|<1$ but then I cannot apply the triangle inequality to the other side. Thanks!,Describe the approximate locations of the zeros of the function $$ f(z) = e^{iz}+e^{-iz}+e^z $$ lying outside the circle $|z|=R >>1$. Another prelim problem.  For Rouche's theorem we need to find a bounded domain.  I tried for $R<|z|<r$ and then looked in the left half plane so that $|e^z|<1$ but then I cannot apply the triangle inequality to the other side. Thanks!,,"['complex-analysis', 'roots']"
57,Proof that a certain entire function is a polynomial,Proof that a certain entire function is a polynomial,,"Let $n\in\mathbf{N}$ be fixed, and $f$ entire and $|f^{-1}(\left\lbrace w\right\rbrace)|\leq n$ for every $w\in\mathbf{C}$. Then $f$ is a polynomial of degree at most $n$. I try to prove this statement, and I think one can prove it as follows: consider $f(1/z)$. $0$ cannot be an essential singularity of $f(1/z)$, for the big Picard theorem would imply that on any neighborhood of $0$ $f(1/z)$ takes on all possible complex values (with at most one exception) infinitely often, but this is contrary to $|f^{-1}(\left\lbrace w\right\rbrace)|\leq n$. Then $f(1/z)$ has a pole of order $k$ say, and since $f$ is holomorphic it is a polynomial of degree $k$ (for its principal part vanishes). By the fundamental theorem of algebra, $k\leq n$. Is it possible here to avoid using Picard's theorem?","Let $n\in\mathbf{N}$ be fixed, and $f$ entire and $|f^{-1}(\left\lbrace w\right\rbrace)|\leq n$ for every $w\in\mathbf{C}$. Then $f$ is a polynomial of degree at most $n$. I try to prove this statement, and I think one can prove it as follows: consider $f(1/z)$. $0$ cannot be an essential singularity of $f(1/z)$, for the big Picard theorem would imply that on any neighborhood of $0$ $f(1/z)$ takes on all possible complex values (with at most one exception) infinitely often, but this is contrary to $|f^{-1}(\left\lbrace w\right\rbrace)|\leq n$. Then $f(1/z)$ has a pole of order $k$ say, and since $f$ is holomorphic it is a polynomial of degree $k$ (for its principal part vanishes). By the fundamental theorem of algebra, $k\leq n$. Is it possible here to avoid using Picard's theorem?",,['complex-analysis']
58,Blaschke Factor mapping holomorphic,Blaschke Factor mapping holomorphic,,"In Stein and Shakarchi's book, Chapter 1, Exercise 7 asks us to show that $$\left|\frac{w-z}{1-\overline{w}{z}}\right|<1$$ if $|z|<1$ and $|w|<1$, with equality if either $|z|=1$ or $|w|=1$. I was able to show this much, and for the second part of the question, most of it is immediate from the above, except showing that for a fixed $w\in\mathbb{D}=\{z\in\mathbb{C}:|z|<1\}$ $$F(z)=\frac{w-z}{1-\overline{w}{z}}$$ is holomorphic. I can do it if I completely expand everything and get it to the form $F(x,y)=u(x,y)+iv(x,y)$ (I haven't actually written everything out, only assured myself that I could actually get it to such a form). From here I should be able to check the Cauchy-Riemann equations and verify the partial derivatives are continuous, which is enough to show that $F$ is holomorphic, but I was wondering if there was a better/more elegant way of showing it? Thanks!","In Stein and Shakarchi's book, Chapter 1, Exercise 7 asks us to show that $$\left|\frac{w-z}{1-\overline{w}{z}}\right|<1$$ if $|z|<1$ and $|w|<1$, with equality if either $|z|=1$ or $|w|=1$. I was able to show this much, and for the second part of the question, most of it is immediate from the above, except showing that for a fixed $w\in\mathbb{D}=\{z\in\mathbb{C}:|z|<1\}$ $$F(z)=\frac{w-z}{1-\overline{w}{z}}$$ is holomorphic. I can do it if I completely expand everything and get it to the form $F(x,y)=u(x,y)+iv(x,y)$ (I haven't actually written everything out, only assured myself that I could actually get it to such a form). From here I should be able to check the Cauchy-Riemann equations and verify the partial derivatives are continuous, which is enough to show that $F$ is holomorphic, but I was wondering if there was a better/more elegant way of showing it? Thanks!",,['complex-analysis']
59,Why are the phase portrait of the simple plane pendulum and a domain coloring of sin(z) so similar?,Why are the phase portrait of the simple plane pendulum and a domain coloring of sin(z) so similar?,,"The simple plane pendulum $$\frac{d^2\theta}{dt^2} + \frac{g}{l}\sin{\theta} = 0$$ has the very perdy phase portrait Meanwhile, a domain coloring of $\sin(z)$ in the complex plane is Why are these so similar?","The simple plane pendulum $$\frac{d^2\theta}{dt^2} + \frac{g}{l}\sin{\theta} = 0$$ has the very perdy phase portrait Meanwhile, a domain coloring of $\sin(z)$ in the complex plane is Why are these so similar?",,"['complex-analysis', 'physics', 'dynamical-systems']"
60,Expressing a complex function in terms of z,Expressing a complex function in terms of z,,"Use the Cauchy-Riemann equations to determine all differentiable   functions that satisfy $Re(f(z))=xy$ I think I know how to do this problem. If we let $z=x+iy$, then $f(z)=u(x,y)+iv(x,y)$. We are given $u(x,y)=xy$. The Cauchy-Riemann equations give us, after some calculations, that $f(x,y)=xy+i(\frac{y^2-x^2}{2}+C)$, $C \in \mathbb{R}$. I'm somewhat unsatisfied that my answer is expressed in terms of $x$ and $y$; I'd like to have it in terms of one complex variable $z$ rather than two real variables $x$ and $y$ [even if they are equivalent]. I've tried doing this with the identities $x=\frac{z+\bar{z}}{2}$ and $y=\frac{z-\bar{z}}{2i}$, but I arrive at something involving $z$ and $\bar{z}$. It's my understanding that differentiable (and hence analytic) functions shouldn't have a $\bar{z}$ in their formulas, so what am I doing wrong?","Use the Cauchy-Riemann equations to determine all differentiable   functions that satisfy $Re(f(z))=xy$ I think I know how to do this problem. If we let $z=x+iy$, then $f(z)=u(x,y)+iv(x,y)$. We are given $u(x,y)=xy$. The Cauchy-Riemann equations give us, after some calculations, that $f(x,y)=xy+i(\frac{y^2-x^2}{2}+C)$, $C \in \mathbb{R}$. I'm somewhat unsatisfied that my answer is expressed in terms of $x$ and $y$; I'd like to have it in terms of one complex variable $z$ rather than two real variables $x$ and $y$ [even if they are equivalent]. I've tried doing this with the identities $x=\frac{z+\bar{z}}{2}$ and $y=\frac{z-\bar{z}}{2i}$, but I arrive at something involving $z$ and $\bar{z}$. It's my understanding that differentiable (and hence analytic) functions shouldn't have a $\bar{z}$ in their formulas, so what am I doing wrong?",,"['complex-analysis', 'complex-numbers']"
61,Homework Problem: Complex Analysis Chain Rule,Homework Problem: Complex Analysis Chain Rule,,"My classmates and I were given that we had to verify, \begin{eqnarray} \frac{\partial}{\partial z} (f \circ g) = (\frac{\partial f}{\partial z} \circ g)(\frac{\partial g}{\partial z}) + (\frac{\partial f}{\partial \bar{z}} \circ g)(\frac{\partial \bar{g}}{\partial z}) \end{eqnarray} We are given the definition that $ \frac{\partial}{\partial z} = \frac{1}{2}(\frac{\partial}{\partial x} - i \frac{\partial}{\partial y})$. This computation is quite tedious to verify and we end up with about 32 terms that needs to cancel out. For example, if $g = s(x,y) + it(x,y)$we have that the term $\frac{\partial g}{\partial z} = \frac{\partial g}{\partial x} - i \frac{\partial g}{\partial y} = \frac{\partial s}{\partial x} + i \frac{\partial t}{\partial x} + i \frac{\partial s}{\partial y} - \frac{\partial t}{\partial y}$. So, as you can see many terms are introduced very quickly. However, our professor said this was a simple two line proof. Is there an alternative approach that will yield the verification much more quickly?","My classmates and I were given that we had to verify, \begin{eqnarray} \frac{\partial}{\partial z} (f \circ g) = (\frac{\partial f}{\partial z} \circ g)(\frac{\partial g}{\partial z}) + (\frac{\partial f}{\partial \bar{z}} \circ g)(\frac{\partial \bar{g}}{\partial z}) \end{eqnarray} We are given the definition that $ \frac{\partial}{\partial z} = \frac{1}{2}(\frac{\partial}{\partial x} - i \frac{\partial}{\partial y})$. This computation is quite tedious to verify and we end up with about 32 terms that needs to cancel out. For example, if $g = s(x,y) + it(x,y)$we have that the term $\frac{\partial g}{\partial z} = \frac{\partial g}{\partial x} - i \frac{\partial g}{\partial y} = \frac{\partial s}{\partial x} + i \frac{\partial t}{\partial x} + i \frac{\partial s}{\partial y} - \frac{\partial t}{\partial y}$. So, as you can see many terms are introduced very quickly. However, our professor said this was a simple two line proof. Is there an alternative approach that will yield the verification much more quickly?",,['complex-analysis']
62,A boundary version of Cauchy's theorem,A boundary version of Cauchy's theorem,,"I am looking for a reference for the following theorem (or something like it) that is not Kodaira's book. Let $D$ be a domain and $\overline{D}$ be it's closure. Suppose that $f:\overline{D} \rightarrow \mathbb{C}$ is holomorphic in $D$ and continuous on $\overline{D}$. If the boundary of $D$, denoted $\partial D$ is composed of piecewise $C^1$ curves then     $$f(w) = \frac{1}{2\pi i}\oint_{\partial D}\frac{f(z)}{z-w}dz$$     for all $w\in D$. This is stronger than the classical Cauchy integral theorem, because here we can allow the path we integrate over to be the boundary. This appears in Kodaira's book, but the book seems sloppily written and I found an egregious error in the first few pages, so I'd like another reference. This does not appear in Ahlfors or any other standard text I looked at. I am looking for any proof that extends to usual Cauchy's theorem to allow paths that lie on the boundary of a region, instead of requiring them to lie entirely in the open region like the usual version does. I managed to find a paper that sketches a high-powered proof when the boundary is a rectifiable Jordan arc (or some multiply connected region where each boundary component is) and gives a reference to a more elementary proof of the same result. I think a simpler proof should be possible in this case, where the boundary is assumed to be $C^1$. This is closely related to an old question that did not get much attention.","I am looking for a reference for the following theorem (or something like it) that is not Kodaira's book. Let $D$ be a domain and $\overline{D}$ be it's closure. Suppose that $f:\overline{D} \rightarrow \mathbb{C}$ is holomorphic in $D$ and continuous on $\overline{D}$. If the boundary of $D$, denoted $\partial D$ is composed of piecewise $C^1$ curves then     $$f(w) = \frac{1}{2\pi i}\oint_{\partial D}\frac{f(z)}{z-w}dz$$     for all $w\in D$. This is stronger than the classical Cauchy integral theorem, because here we can allow the path we integrate over to be the boundary. This appears in Kodaira's book, but the book seems sloppily written and I found an egregious error in the first few pages, so I'd like another reference. This does not appear in Ahlfors or any other standard text I looked at. I am looking for any proof that extends to usual Cauchy's theorem to allow paths that lie on the boundary of a region, instead of requiring them to lie entirely in the open region like the usual version does. I managed to find a paper that sketches a high-powered proof when the boundary is a rectifiable Jordan arc (or some multiply connected region where each boundary component is) and gives a reference to a more elementary proof of the same result. I think a simpler proof should be possible in this case, where the boundary is assumed to be $C^1$. This is closely related to an old question that did not get much attention.",,"['complex-analysis', 'reference-request']"
63,Non-isolated singularity points,Non-isolated singularity points,,"I am having trouble understanding non-isolated singularity points. An isolated singularity point I do kind of understand, it is when: a point $z_0$ is said to be isolated if $z_0$ is a singular point and has a neighborhood throughout which $f$ is analytic except at $z_0$. For example, why would $\text{tan}(1/z),\ \text{log}(z),\text{or even}\ \frac{1}{\sin(\frac{\pi}{z})}$ have a non-isolated singularity point?","I am having trouble understanding non-isolated singularity points. An isolated singularity point I do kind of understand, it is when: a point $z_0$ is said to be isolated if $z_0$ is a singular point and has a neighborhood throughout which $f$ is analytic except at $z_0$. For example, why would $\text{tan}(1/z),\ \text{log}(z),\text{or even}\ \frac{1}{\sin(\frac{\pi}{z})}$ have a non-isolated singularity point?",,['complex-analysis']
64,Interpolation with entire function,Interpolation with entire function,,Is there any simple  way to construct an entiere function $f$ such that : $$\forall p \in {\mathbb N} \quad f(2^p)=(-1)^p$$,Is there any simple  way to construct an entiere function $f$ such that : $$\forall p \in {\mathbb N} \quad f(2^p)=(-1)^p$$,,['complex-analysis']
65,"Schwarz's Lemma, fixed points question","Schwarz's Lemma, fixed points question",,This is from an old qualifying examination question. If f is holomorphic in the unit disk $D$ and $|f(z)|<1$ for all $z\in D$. Suppose also that $f$ has two distinct fixed points in $D$ then $f(z)=z$ for every $z\in D$ I know that I have to use the Schwarz's lemma and may be make use of Möbius transformations. I tried setting $g(z)=\phi_a\circ f\circ \phi_{-a}$. But that does not seem to work because I don't see why $|g(z)|=|z|$ for some non zero $z$. Any helpful hints are greatly appreciated. Edit: Of course the $a$ above is one of the fixed points.,This is from an old qualifying examination question. If f is holomorphic in the unit disk $D$ and $|f(z)|<1$ for all $z\in D$. Suppose also that $f$ has two distinct fixed points in $D$ then $f(z)=z$ for every $z\in D$ I know that I have to use the Schwarz's lemma and may be make use of Möbius transformations. I tried setting $g(z)=\phi_a\circ f\circ \phi_{-a}$. But that does not seem to work because I don't see why $|g(z)|=|z|$ for some non zero $z$. Any helpful hints are greatly appreciated. Edit: Of course the $a$ above is one of the fixed points.,,"['complex-analysis', 'fixed-point-theorems']"
66,"When does an ""infinite polynomial"" make sense?","When does an ""infinite polynomial"" make sense?",,"Suppose I pick a collection $A \subset \mathbb{C}$ of points in the complex plane and attempt to construct a ""polynomial"" with those roots via, $$f(z):=\Pi_{\alpha \in A} (z-\alpha).$$ If $A$ is finite, we get a polynomial. If $A=\{n\pi:n \in \mathbb{Z}\}$, according to Euler we get $f(x)=\sin(x)$. Edit: this example is not right as Qiaochu has pointed out; see his answer for more details. What about other subsets of the complex plane? Other countable subsets without accumulation points? Countable sub with accumulation points like $A=\{1/n:n \in \mathbb{Z}\}$? Uncountable subsets?? When does the product converge, and if it does how does the spatial distribution of $A$ effect the properties of $f$? This question was motivated by the question here: Determining the density of roots to an infinite polynomial","Suppose I pick a collection $A \subset \mathbb{C}$ of points in the complex plane and attempt to construct a ""polynomial"" with those roots via, $$f(z):=\Pi_{\alpha \in A} (z-\alpha).$$ If $A$ is finite, we get a polynomial. If $A=\{n\pi:n \in \mathbb{Z}\}$, according to Euler we get $f(x)=\sin(x)$. Edit: this example is not right as Qiaochu has pointed out; see his answer for more details. What about other subsets of the complex plane? Other countable subsets without accumulation points? Countable sub with accumulation points like $A=\{1/n:n \in \mathbb{Z}\}$? Uncountable subsets?? When does the product converge, and if it does how does the spatial distribution of $A$ effect the properties of $f$? This question was motivated by the question here: Determining the density of roots to an infinite polynomial",,"['complex-analysis', 'polynomials']"
67,Riemann's theorem on removable singularities,Riemann's theorem on removable singularities,,"Theorem Let $\Omega\subseteq \mathbb{C}$ open , $ a\in\Omega,\ f\in H(\Omega\backslash \{a\})$ and there is $r>0$ with $f$ is bounded on $C(a,r)\backslash \{a\}$ ($C(a,r)$ is the circle with origin $a$ and radius $r$), then $a$ is a removable singularity. Proof Let $h:\Omega\rightarrow \mathbb{C}$ be defined as: $$ h(z)=\left\{\begin{array}{l} 0,\ \ \ \ \ \ \ \ \  \  \ \ \ \ \ \ \ \ \ \ z=a \\ (z-a)^{2}f(z), \ z\in\Omega\backslash \{a\} \end{array}\right. $$ Then we have: $$ \lim_{z\rightarrow a}\frac{h(z)-h(a)}{z-a}=\lim_{z\rightarrow a}(z-a)f(z)=0\Rightarrow h'(a)=0.  $$ $\color{red}{\text{ Why is} \lim\limits_{z\rightarrow a}(z-a)f(z)=0?}$ So we have $h\in H(\Omega)$ and therefore $(h(a)=h'(a)=0)$. $$ h(z)=\sum_{n=2}^{\infty}c_{n}(z-a)^{n}\ (z\in K(a,\ r)). $$ Letting $f(a):=c_{2}$, it follows: $\color{red}{\text{ Why do we have} f(a):=c_{2}?}$ $$ f(z)=\sum_{n=0}^{\infty}c_{n+2}(z-a)^{n}\ (z\in K(a,\ r)), $$ so $f\in H(\Omega).\ \square $","Theorem Let $\Omega\subseteq \mathbb{C}$ open , $ a\in\Omega,\ f\in H(\Omega\backslash \{a\})$ and there is $r>0$ with $f$ is bounded on $C(a,r)\backslash \{a\}$ ($C(a,r)$ is the circle with origin $a$ and radius $r$), then $a$ is a removable singularity. Proof Let $h:\Omega\rightarrow \mathbb{C}$ be defined as: $$ h(z)=\left\{\begin{array}{l} 0,\ \ \ \ \ \ \ \ \  \  \ \ \ \ \ \ \ \ \ \ z=a \\ (z-a)^{2}f(z), \ z\in\Omega\backslash \{a\} \end{array}\right. $$ Then we have: $$ \lim_{z\rightarrow a}\frac{h(z)-h(a)}{z-a}=\lim_{z\rightarrow a}(z-a)f(z)=0\Rightarrow h'(a)=0.  $$ $\color{red}{\text{ Why is} \lim\limits_{z\rightarrow a}(z-a)f(z)=0?}$ So we have $h\in H(\Omega)$ and therefore $(h(a)=h'(a)=0)$. $$ h(z)=\sum_{n=2}^{\infty}c_{n}(z-a)^{n}\ (z\in K(a,\ r)). $$ Letting $f(a):=c_{2}$, it follows: $\color{red}{\text{ Why do we have} f(a):=c_{2}?}$ $$ f(z)=\sum_{n=0}^{\infty}c_{n+2}(z-a)^{n}\ (z\in K(a,\ r)), $$ so $f\in H(\Omega).\ \square $",,['complex-analysis']
68,Complex structure v.s. conformal structure in more than 1 complex dimension,Complex structure v.s. conformal structure in more than 1 complex dimension,,"I've recently been learning some complex geometry, mostly for my own edification. In the course of my studies I came across the following statement: If $X$ is a Riemann surface then a choice of complex structure is equivalent to choice of conformal structure. That is, complex structures and conformal structures on a Riemann surface are in bijection. I haven't seen an analog of this statement for higher dimensional manifolds. Given that the intuition that one develops in 1-complex variable falls apart alarmingly fast in several complex variables, I suspect that complex and conformal structures are not equivalent in higher dimensions. My question is as follows. Are complex structures and conformal structures inequivalent in dimension >1? If so how can I understand this (why aren't they)? Furthermore, does the correspondence fall apart in any controlled way. For instance does conformal structure imply a complex structure but that the converse no longer holds? Or are conformal and complex structures simply incomparable in higher dimensions?","I've recently been learning some complex geometry, mostly for my own edification. In the course of my studies I came across the following statement: If $X$ is a Riemann surface then a choice of complex structure is equivalent to choice of conformal structure. That is, complex structures and conformal structures on a Riemann surface are in bijection. I haven't seen an analog of this statement for higher dimensional manifolds. Given that the intuition that one develops in 1-complex variable falls apart alarmingly fast in several complex variables, I suspect that complex and conformal structures are not equivalent in higher dimensions. My question is as follows. Are complex structures and conformal structures inequivalent in dimension >1? If so how can I understand this (why aren't they)? Furthermore, does the correspondence fall apart in any controlled way. For instance does conformal structure imply a complex structure but that the converse no longer holds? Or are conformal and complex structures simply incomparable in higher dimensions?",,"['complex-analysis', 'complex-geometry', 'conformal-geometry']"
69,Complex integral over a circle,Complex integral over a circle,,"In an exercise, I'm supposed to assume that $|a| < r < |b|$ and prove that $\displaystyle\int_{\gamma} \frac{1}{(z - a)(z - b)} dz = \frac{2 \pi i}{a - b}$, where $\gamma$ is a circle of radius $r$ centered at the origin with positive orientation. So I had the idea to express this integral as $\frac{1}{a - b} \displaystyle\int_{\gamma} \frac{1}{z - a} - \frac{1}{z - b} dz$. Then I tried to evaluate each of these separately, but I don't really know what to do. I get $\displaystyle\int_{0}^{2 \pi} \frac{i r e^{it}}{r e^{it} - a} dt$ for the first term. How am I supposed to integrate this? I can't substitute $u = re^{it}$, can I?","In an exercise, I'm supposed to assume that $|a| < r < |b|$ and prove that $\displaystyle\int_{\gamma} \frac{1}{(z - a)(z - b)} dz = \frac{2 \pi i}{a - b}$, where $\gamma$ is a circle of radius $r$ centered at the origin with positive orientation. So I had the idea to express this integral as $\frac{1}{a - b} \displaystyle\int_{\gamma} \frac{1}{z - a} - \frac{1}{z - b} dz$. Then I tried to evaluate each of these separately, but I don't really know what to do. I get $\displaystyle\int_{0}^{2 \pi} \frac{i r e^{it}}{r e^{it} - a} dt$ for the first term. How am I supposed to integrate this? I can't substitute $u = re^{it}$, can I?",,['complex-analysis']
70,Proof for integral representation of Lambert W function,Proof for integral representation of Lambert W function,,"The Lambert W function satisfies the identity $$W(z)e^{W(z)}=z.$$ How do you prove that $$ W(z) = \frac{z}{2\pi} \int_{-\pi}^{\pi} \frac{(1-\nu \cot(\nu))^2+\nu^2}{z+\nu \csc(\nu)e^{-\nu\cot(\nu)}} \, \mathrm{d}\nu $$ where $z$ is a real number and $z\geq-\frac{1}{e}$ ?",The Lambert W function satisfies the identity How do you prove that where is a real number and ?,"W(z)e^{W(z)}=z. 
W(z)
= \frac{z}{2\pi} \int_{-\pi}^{\pi} \frac{(1-\nu \cot(\nu))^2+\nu^2}{z+\nu \csc(\nu)e^{-\nu\cot(\nu)}} \, \mathrm{d}\nu
 z z\geq-\frac{1}{e}","['complex-analysis', 'definite-integrals', 'lambert-w']"
71,$|\log (1 + z)| \leq 2 |z|$ Complex inequality,Complex inequality,|\log (1 + z)| \leq 2 |z|,"Prove that for $|z|\leq0.5$, $|\log (1 + z)| \leq 2 |z|$. I know that $|\log (1 + z)|=|\log|1+z|+i\arg(1+z)|$ and $|\arg(1+z)|\leq\pi/6$ for $|z|\leq0.5$, but then I don't know how to proceed. It seems that the it attains ""="" when $z=0$? Thanks!","Prove that for $|z|\leq0.5$, $|\log (1 + z)| \leq 2 |z|$. I know that $|\log (1 + z)|=|\log|1+z|+i\arg(1+z)|$ and $|\arg(1+z)|\leq\pi/6$ for $|z|\leq0.5$, but then I don't know how to proceed. It seems that the it attains ""="" when $z=0$? Thanks!",,['complex-analysis']
72,How and when to do exercises from books?,How and when to do exercises from books?,,"Most math books I have seen seem to be structured in the same way; each chapter is a few dozen pages, and after each chapter there are a few dozen exercises. I often find myself not knowing what exercises to do and when to do them. If I have read the first 10 pages, I'm not sure what $x$ is in ""the first $x$ exercises should be answerable after reading the first $10$ pages."" A prime example is Tristan Needham's ""Visual Complex Analysis"". It's a very good book; however, the first chapter is about $50$ pages long, after which follows a splatter of $50$ exercises. I'm not sure when I should interrupt my reading and go the exercises. And I'm not sure when to interrupt my exercises and go back to the reading.","Most math books I have seen seem to be structured in the same way; each chapter is a few dozen pages, and after each chapter there are a few dozen exercises. I often find myself not knowing what exercises to do and when to do them. If I have read the first 10 pages, I'm not sure what $x$ is in ""the first $x$ exercises should be answerable after reading the first $10$ pages."" A prime example is Tristan Needham's ""Visual Complex Analysis"". It's a very good book; however, the first chapter is about $50$ pages long, after which follows a splatter of $50$ exercises. I'm not sure when I should interrupt my reading and go the exercises. And I'm not sure when to interrupt my exercises and go back to the reading.",,"['complex-analysis', 'soft-question', 'self-learning', 'problem-solving']"
73,Proof of Sum of inverse squares Equaling $\pi^2/6$,Proof of Sum of inverse squares Equaling,\pi^2/6,"I'm a 15 year old interested in higher level mathematics. I've recently been studying Complex Analysis from notes of my math teacher from his college math classes. I've understood everything up until the very last 2 pages, when he provides part of a proof that the sum of $1/(n^2)$ from $n=1\to \infty$ is equal to $\pi^2/6$ using residues and the Residue Theorem. He has written the Residues of the function  $$ f(z) = \frac{\pi \cot(\pi z)}{z^2} $$  But I want to understand how he got them, so that I feel that I fully understand it myself. I got all the residues at $z = 0, 1, -1, 2, -2, 3, -3, \ldots$ but what I don't understand is this : Res at the pole $z=0$ is $\pi^2/3$. Why? Please explain how he got this to me. I've tried working it out using several methods but can not figure it out. Thanks.","I'm a 15 year old interested in higher level mathematics. I've recently been studying Complex Analysis from notes of my math teacher from his college math classes. I've understood everything up until the very last 2 pages, when he provides part of a proof that the sum of $1/(n^2)$ from $n=1\to \infty$ is equal to $\pi^2/6$ using residues and the Residue Theorem. He has written the Residues of the function  $$ f(z) = \frac{\pi \cot(\pi z)}{z^2} $$  But I want to understand how he got them, so that I feel that I fully understand it myself. I got all the residues at $z = 0, 1, -1, 2, -2, 3, -3, \ldots$ but what I don't understand is this : Res at the pole $z=0$ is $\pi^2/3$. Why? Please explain how he got this to me. I've tried working it out using several methods but can not figure it out. Thanks.",,['complex-analysis']
74,Function holomorphic except for real line and continuous everywhere is entire,Function holomorphic except for real line and continuous everywhere is entire,,"1) I've already shown that if $f:\mathbb{C}\rightarrow\mathbb{C}$ is holomorphic everywhere except for a single point and if it continuous on whole $\mathbb{C}$ then it is entire. It was quite easy. But now I have to generalize it onto a whole real axis in $\mathbb{C}$. Can you help me with that? 2) On the other hand I wonder if it can be generalized even more, for even bigger sets? I mean what are the ""biggest"" sets $S$ that apply to the following statement? If $f:\mathbb{C}\rightarrow\mathbb{C}$ is holomorphic on $\mathbb{C} \setminus S$ and continuous on $\mathbb{C}$ then it is holomorphic everywhere on $\mathbb{C}$.","1) I've already shown that if $f:\mathbb{C}\rightarrow\mathbb{C}$ is holomorphic everywhere except for a single point and if it continuous on whole $\mathbb{C}$ then it is entire. It was quite easy. But now I have to generalize it onto a whole real axis in $\mathbb{C}$. Can you help me with that? 2) On the other hand I wonder if it can be generalized even more, for even bigger sets? I mean what are the ""biggest"" sets $S$ that apply to the following statement? If $f:\mathbb{C}\rightarrow\mathbb{C}$ is holomorphic on $\mathbb{C} \setminus S$ and continuous on $\mathbb{C}$ then it is holomorphic everywhere on $\mathbb{C}$.",,"['complex-analysis', 'analysis', 'complex-integration']"
75,Generalization of Cauchy Residue theorem to Multi-dimensional holomorphic functions,Generalization of Cauchy Residue theorem to Multi-dimensional holomorphic functions,,"We know Cauchy Residue theorem from the Complex analysis. however I wonder if there is a kind of Generalization of Cauchy integral and Residue theorem to the complex multidimensional holomorphic function too ?  if such theorem exists, please mention any possible application of this theorem too. Can this theorem help in Iterative elimination of roots of a polynomial equations system ?","We know Cauchy Residue theorem from the Complex analysis. however I wonder if there is a kind of Generalization of Cauchy integral and Residue theorem to the complex multidimensional holomorphic function too ?  if such theorem exists, please mention any possible application of this theorem too. Can this theorem help in Iterative elimination of roots of a polynomial equations system ?",,"['complex-analysis', 'algebraic-geometry', 'several-complex-variables']"
76,What mapping does Escher use?,What mapping does Escher use?,,"In Escher's hyperbolic tesselations, he takes (effectively) a tesselation of the plane and maps it to a tesselation of the unit disk, by a mapping that takes straight lines to circles meeting the disk boundary at right angles.  What precisely is this mapping (with a formula)?  I am aware that this is effectively a mapping from the hyperbolic plane to the Poincare disk, taking a mapping from a tesselation of the hyperbolic plane to the disk.  However, what is it as a map from $R^2$ to $D$ , with formula.  I believe that it is a conformal map. What is the correct formula in higher dimensions, also?","In Escher's hyperbolic tesselations, he takes (effectively) a tesselation of the plane and maps it to a tesselation of the unit disk, by a mapping that takes straight lines to circles meeting the disk boundary at right angles.  What precisely is this mapping (with a formula)?  I am aware that this is effectively a mapping from the hyperbolic plane to the Poincare disk, taking a mapping from a tesselation of the hyperbolic plane to the disk.  However, what is it as a map from to , with formula.  I believe that it is a conformal map. What is the correct formula in higher dimensions, also?",R^2 D,"['complex-analysis', 'geometry', 'tessellations']"
77,Is there a reason why Harmonic functions are defined on open sets?,Is there a reason why Harmonic functions are defined on open sets?,,"Whenever I see a definition of a harmonic function, it's always defined as follows A function $f : U \to \Bbb{R}$ is called harmonic (where $U$ is an open subset of $\Bbb{R}^n$) iff it is twice continuously differentiable on $U$ and satisfies Laplace's equation $\Delta f = 0$. Is there a reason why $U$ is always chosen to be open and what are the ramifications if $U$ is chosen to not be an open set in the context of the Laplace operator (in which we take the second partial derivatives)? Edit: I have recently read that for a ""good"" subset $V$ of $\Bbb{R}^n$ i.e. a submanifold of codimension 1 with boundary, we can extend the notion of differentiability by extending $V$ for any point $v \in V$ to involve a neighbourhood around $v$ over which we can differentiate. Then the derivative of $V$ is just the derivative of the extension. Supposedly, unless $V$ is ""good"", then the derivative needn't be uniquely defined. Why does this happen to be the case and what's the necessity in defining ""good"" as we have done? Thanks in advance!","Whenever I see a definition of a harmonic function, it's always defined as follows A function $f : U \to \Bbb{R}$ is called harmonic (where $U$ is an open subset of $\Bbb{R}^n$) iff it is twice continuously differentiable on $U$ and satisfies Laplace's equation $\Delta f = 0$. Is there a reason why $U$ is always chosen to be open and what are the ramifications if $U$ is chosen to not be an open set in the context of the Laplace operator (in which we take the second partial derivatives)? Edit: I have recently read that for a ""good"" subset $V$ of $\Bbb{R}^n$ i.e. a submanifold of codimension 1 with boundary, we can extend the notion of differentiability by extending $V$ for any point $v \in V$ to involve a neighbourhood around $v$ over which we can differentiate. Then the derivative of $V$ is just the derivative of the extension. Supposedly, unless $V$ is ""good"", then the derivative needn't be uniquely defined. Why does this happen to be the case and what's the necessity in defining ""good"" as we have done? Thanks in advance!",,"['complex-analysis', 'differential-geometry', 'differential-topology', 'harmonic-functions']"
78,What type of singularity $\sin(\frac{1}{\cos(\frac{1}{z})})$ has? (GATE 2009),What type of singularity  has? (GATE 2009),\sin(\frac{1}{\cos(\frac{1}{z})}),Determine the type of singularity at $z = 0$ of the following function and why? $$f(z) = \sin \left(\frac{1}{\cos (\frac{1}{z})}\right).$$ I have no idea. Thank you for your answer.,Determine the type of singularity at $z = 0$ of the following function and why? $$f(z) = \sin \left(\frac{1}{\cos (\frac{1}{z})}\right).$$ I have no idea. Thank you for your answer.,,['complex-analysis']
79,"Where does Klein's j-invariant take the values 0 and 1, and with what multiplicities?","Where does Klein's j-invariant take the values 0 and 1, and with what multiplicities?",,"I tried to solve the following problem from Ahlfors' text, please verify my solution: Where does the function $$J(\tau)=\frac{4}{27} \frac{(1-\lambda+\lambda^2)^3}{\lambda^2(1-\lambda)^2} $$   take the values $0$ and $1$, and with what multiplicities? Here $\lambda$ is the modular $\lambda$ function, defined by $$\lambda(\tau)=\frac{e_3-e_2}{e_1-e_2} $$ where $e_1=\wp(\omega_1/2),e_2=\wp(\omega_2/2),e_3=\wp((\omega_1+\omega_2)/2).$ My attempt: First part - solving $J(\tau)=0$ Since $\lambda$ is holomorphic in the upper half plane, and never takes the values $0,1$ it follows that $J(\tau)=0 \Longleftrightarrow 1-\lambda+\lambda^2=0 \Longleftrightarrow \tau \in \lambda^{-1} \{\exp(\pm 2 \pi i/6) \}$. After examining this picture from Wikipedia: I've speculated that $\lambda(\exp(2\pi i/6))=\exp(2 \pi i/6),\lambda(\exp(2 \pi i/3))=\exp(- 2 \pi i/6)$. ( However, I have no idea how to prove it ) Since the region bounded by the lines $\Re \tau= \pm 1$ and the upper semi circles $|\tau \mp 1/2|=1/2$ is a fundamental region for $\lambda$ (""half"" of the boundary needs to be included) it follows that $\exp(2 \pi i/6)$ and $\exp(2 \pi i/3)$ are the only two zeros there, and due to the cube in the numerator of $J(\tau)$ it's clear that they are zeros of order $3$ each. Using additional symmetries of the j-invariant, such as $1$-periodicity, and the Schwarz reflection principle about the upper semi-circles $|\tau-n-1/2|=1/2$ all other zeros are found and they are all of order $3$ as well. Second part - solving $J(\tau)=1$ Well, $J(\tau)=1 \Longleftrightarrow 4(1-\lambda+\lambda^2)^3-27 \lambda^2 (1-\lambda)^2=0 \Longleftrightarrow (2 \lambda-1)^2(\lambda-2)^2 (\lambda+1)^2$. This shows that $J(\tau)=1 \Longleftrightarrow \tau \in \lambda^{-1} \{1/2,2,-1\}$, and at each such $\tau$ the value $1$ is taken with multiplicity 2. Now, in the fundamental region I suppose we have $\lambda(i)=\frac{1}{2},\lambda(1/2+1/2 i)=2,\lambda(1+i)=-1$, and all other solutions can be found by the symmetries of $J$. My doubts: I am not entirely sure that my approach is correct. I don't know how to prove any of the equations of the form $\lambda(\tau_0)=w_0$ above. Please tell me if the above is correct, and help me prove the equations $\lambda(\tau_0)=w_0$. Thanks! EDIT: The end of this file might have some value. However, there are references in there that I don't follow.","I tried to solve the following problem from Ahlfors' text, please verify my solution: Where does the function $$J(\tau)=\frac{4}{27} \frac{(1-\lambda+\lambda^2)^3}{\lambda^2(1-\lambda)^2} $$   take the values $0$ and $1$, and with what multiplicities? Here $\lambda$ is the modular $\lambda$ function, defined by $$\lambda(\tau)=\frac{e_3-e_2}{e_1-e_2} $$ where $e_1=\wp(\omega_1/2),e_2=\wp(\omega_2/2),e_3=\wp((\omega_1+\omega_2)/2).$ My attempt: First part - solving $J(\tau)=0$ Since $\lambda$ is holomorphic in the upper half plane, and never takes the values $0,1$ it follows that $J(\tau)=0 \Longleftrightarrow 1-\lambda+\lambda^2=0 \Longleftrightarrow \tau \in \lambda^{-1} \{\exp(\pm 2 \pi i/6) \}$. After examining this picture from Wikipedia: I've speculated that $\lambda(\exp(2\pi i/6))=\exp(2 \pi i/6),\lambda(\exp(2 \pi i/3))=\exp(- 2 \pi i/6)$. ( However, I have no idea how to prove it ) Since the region bounded by the lines $\Re \tau= \pm 1$ and the upper semi circles $|\tau \mp 1/2|=1/2$ is a fundamental region for $\lambda$ (""half"" of the boundary needs to be included) it follows that $\exp(2 \pi i/6)$ and $\exp(2 \pi i/3)$ are the only two zeros there, and due to the cube in the numerator of $J(\tau)$ it's clear that they are zeros of order $3$ each. Using additional symmetries of the j-invariant, such as $1$-periodicity, and the Schwarz reflection principle about the upper semi-circles $|\tau-n-1/2|=1/2$ all other zeros are found and they are all of order $3$ as well. Second part - solving $J(\tau)=1$ Well, $J(\tau)=1 \Longleftrightarrow 4(1-\lambda+\lambda^2)^3-27 \lambda^2 (1-\lambda)^2=0 \Longleftrightarrow (2 \lambda-1)^2(\lambda-2)^2 (\lambda+1)^2$. This shows that $J(\tau)=1 \Longleftrightarrow \tau \in \lambda^{-1} \{1/2,2,-1\}$, and at each such $\tau$ the value $1$ is taken with multiplicity 2. Now, in the fundamental region I suppose we have $\lambda(i)=\frac{1}{2},\lambda(1/2+1/2 i)=2,\lambda(1+i)=-1$, and all other solutions can be found by the symmetries of $J$. My doubts: I am not entirely sure that my approach is correct. I don't know how to prove any of the equations of the form $\lambda(\tau_0)=w_0$ above. Please tell me if the above is correct, and help me prove the equations $\lambda(\tau_0)=w_0$. Thanks! EDIT: The end of this file might have some value. However, there are references in there that I don't follow.",,"['complex-analysis', 'modular-forms', 'elliptic-functions']"
80,Find all the values of $(1+i)^{(1-i)}$,Find all the values of,(1+i)^{(1-i)},"The question says to find all the values of $(1+i)^{(1-i)}$ I have trouble figuring out firstly, exactly what values are being looked for. I can toy around with the equation a bit to try to make it look ""acceptable"" (i.e $ax + byi$ format) but get stuck along the way. So I need help with: a) figuring out what values are needed. i.e. what does the question $mean$ and some brief background or diagram that explains, in a practical sense, what I'm supposed to be looking for. b) the algebra that can lead me to a reasonable solution. -- MY ATTEMPT: $$ (1+i)^{(1-i)} = (1+i)^{(1-i)}.\frac{(1+i)^{(1+i)}}{(1+i)^{(1+i)}} = \frac{(1+i)^2}{(1+i)^{(1+i)}} = *$$ *is where I get stuck.","The question says to find all the values of $(1+i)^{(1-i)}$ I have trouble figuring out firstly, exactly what values are being looked for. I can toy around with the equation a bit to try to make it look ""acceptable"" (i.e $ax + byi$ format) but get stuck along the way. So I need help with: a) figuring out what values are needed. i.e. what does the question $mean$ and some brief background or diagram that explains, in a practical sense, what I'm supposed to be looking for. b) the algebra that can lead me to a reasonable solution. -- MY ATTEMPT: $$ (1+i)^{(1-i)} = (1+i)^{(1-i)}.\frac{(1+i)^{(1+i)}}{(1+i)^{(1+i)}} = \frac{(1+i)^2}{(1+i)^{(1+i)}} = *$$ *is where I get stuck.",,"['complex-analysis', 'algebra-precalculus', 'complex-numbers']"
81,Introductory books on complex analysis? [duplicate],Introductory books on complex analysis? [duplicate],,"This question already has answers here : What is a good complex analysis textbook, barring Ahlfors's? (28 answers) Closed 11 years ago . I'm a senior in my undergrad. years of college, and I haven't taken Complex Analysis yet. I have taken Real Analysis I (covered properties of $\mathbb{R}$, set theory, limits of sequences and functions, series, (uniform) continuity, uniform convergence) and Abstract Algebra I (covered $\mathbb{Z}_{n}$, an intro to group theory (groups, subgroups, quotient groups, isomorphism theorems, semidirect products), and an intro to ring theory (fields, ideals)). The book that we use at the university I attend isn't very analytical, from my understanding. (The book is Fundamentals of Complex Analysis with Applications to Engineering, Science, and Mathematics , 3rd ed. by Saff.) Of the courses I've taken in my undergrad, Real Analysis I has definitely been my favorite course so far, and I will be taking Real Analysis II (covers integration and differentiation in $\mathbb{R}^n$, Riemman-Stieltjes, and some other topics that I don't know about) this upcoming fall. Are there any books on complex analysis that you would suggest given my background? Thank you! Edit : Other courses I have taken: I have taken Calculus I through III (nothing on Differential Equations - although I do know what a first-order linear differential equation is), actuarial science courses (Probability (Calculus-based), Statistics (Calculus-based), Life Contingencies), and Linear Algebra (one semester using Larson's Elementary Linear Algebra and a second semester independent study using Axler).","This question already has answers here : What is a good complex analysis textbook, barring Ahlfors's? (28 answers) Closed 11 years ago . I'm a senior in my undergrad. years of college, and I haven't taken Complex Analysis yet. I have taken Real Analysis I (covered properties of $\mathbb{R}$, set theory, limits of sequences and functions, series, (uniform) continuity, uniform convergence) and Abstract Algebra I (covered $\mathbb{Z}_{n}$, an intro to group theory (groups, subgroups, quotient groups, isomorphism theorems, semidirect products), and an intro to ring theory (fields, ideals)). The book that we use at the university I attend isn't very analytical, from my understanding. (The book is Fundamentals of Complex Analysis with Applications to Engineering, Science, and Mathematics , 3rd ed. by Saff.) Of the courses I've taken in my undergrad, Real Analysis I has definitely been my favorite course so far, and I will be taking Real Analysis II (covers integration and differentiation in $\mathbb{R}^n$, Riemman-Stieltjes, and some other topics that I don't know about) this upcoming fall. Are there any books on complex analysis that you would suggest given my background? Thank you! Edit : Other courses I have taken: I have taken Calculus I through III (nothing on Differential Equations - although I do know what a first-order linear differential equation is), actuarial science courses (Probability (Calculus-based), Statistics (Calculus-based), Life Contingencies), and Linear Algebra (one semester using Larson's Elementary Linear Algebra and a second semester independent study using Axler).",,"['complex-analysis', 'reference-request']"
82,"Find all entire functions $f$ such that for all $z\in \mathbb{C}$, $|f(z)|\ge \frac{1}{|z|+1}$","Find all entire functions  such that for all ,",f z\in \mathbb{C} |f(z)|\ge \frac{1}{|z|+1},"Find all entire functions $f$ such that for all $z\in \mathbb{C}$, $|f(z)|\ge \frac{1}{|z|+1}$ This is one of the past qualifying exams that I was working on and I think that I have to find the function that involved with $f$ that is bounded and use Louiville's theorem to say that the function that is found is constant and conclude something about $f$. I can only think of using $1/f$ so that $\frac{1}{|f(z)|} \le |z|+1$ but $|z|+1$ is not really bounded so I would like to ask you for some hint or idea. Any hint/ idea would be appreciated. Thank you in advance.","Find all entire functions $f$ such that for all $z\in \mathbb{C}$, $|f(z)|\ge \frac{1}{|z|+1}$ This is one of the past qualifying exams that I was working on and I think that I have to find the function that involved with $f$ that is bounded and use Louiville's theorem to say that the function that is found is constant and conclude something about $f$. I can only think of using $1/f$ so that $\frac{1}{|f(z)|} \le |z|+1$ but $|z|+1$ is not really bounded so I would like to ask you for some hint or idea. Any hint/ idea would be appreciated. Thank you in advance.",,['complex-analysis']
83,Dolbeault cohomology of the complex projective space.,Dolbeault cohomology of the complex projective space.,,"Let $X=\mathbb{CP}^n$.  We proved using the hodge decomposition that $H^0(X,\Omega^p)=0$ if $p\neq 0$. But I do not understand why I cannot have global holomorphic differential p-forms not even constants. I want to understand why $H^0(X,\Omega^p)=0$ without using the Hodge decomposition. And without using GAGA I would like to see a proof of $H^0(Proj(\mathbb{C}[x_0,..,x_n]),\Omega_{X/\mathbb{C}}^p)=0$ for $p\neq 0$","Let $X=\mathbb{CP}^n$.  We proved using the hodge decomposition that $H^0(X,\Omega^p)=0$ if $p\neq 0$. But I do not understand why I cannot have global holomorphic differential p-forms not even constants. I want to understand why $H^0(X,\Omega^p)=0$ without using the Hodge decomposition. And without using GAGA I would like to see a proof of $H^0(Proj(\mathbb{C}[x_0,..,x_n]),\Omega_{X/\mathbb{C}}^p)=0$ for $p\neq 0$",,"['complex-analysis', 'algebraic-geometry']"
84,Zeroes of a holomorphic function,Zeroes of a holomorphic function,,"Where can I find a proof/reference for the following fact? Let $f$ be a holomorphic function with a zero of order $n$ at $z = 0$. Then for sufficiently small $\epsilon > 0$, there exists $\delta > 0$ such that for all $a$ with $0 < |a| < \delta$, $f(z) = a$ has exactly $n$ roots in the disc $|z| < \epsilon$.","Where can I find a proof/reference for the following fact? Let $f$ be a holomorphic function with a zero of order $n$ at $z = 0$. Then for sufficiently small $\epsilon > 0$, there exists $\delta > 0$ such that for all $a$ with $0 < |a| < \delta$, $f(z) = a$ has exactly $n$ roots in the disc $|z| < \epsilon$.",,['complex-analysis']
85,Is Complex plane more than just a topological space with $\mathbb{R}^2$ topology?,Is Complex plane more than just a topological space with  topology?,\mathbb{R}^2,The Set of Complex numbers is a field as well as a nice topological space homeomorphic to $\mathbb{R}^2$. But why such a particular interest for this space? For instance what is more special about it than any other $\mathbb{R}^n$? I agree that each function on a complex domain will look like a single variable function and may have a nice definition for a derivative with respect to that single variable where as in $\mathbb{R}^3$ we cannot(?) have such a single variable to differentiate a function with.Particularly Holomorphic functions are as many times differentiable as we want. But is that all? I believe there is more to it but couldn't just get a picture of it. Any insights? Thanks,The Set of Complex numbers is a field as well as a nice topological space homeomorphic to $\mathbb{R}^2$. But why such a particular interest for this space? For instance what is more special about it than any other $\mathbb{R}^n$? I agree that each function on a complex domain will look like a single variable function and may have a nice definition for a derivative with respect to that single variable where as in $\mathbb{R}^3$ we cannot(?) have such a single variable to differentiate a function with.Particularly Holomorphic functions are as many times differentiable as we want. But is that all? I believe there is more to it but couldn't just get a picture of it. Any insights? Thanks,,"['complex-analysis', 'soft-question']"
86,A Better Proof for the converse of Cauchy's Theorem for Rectangles?,A Better Proof for the converse of Cauchy's Theorem for Rectangles?,,"So the question asked to prove the following:  ""Prove that if f is a continuous complex valued function in the open subset G of the complex plane, and if the integral of $f(z)dz = 0$ over every rectangle R, with edges parallel to the coordinate axes, contained with its interior in G, then f is holomorphic"" (Sarason Complex Function Theory 2nd edition). Following the logic the book used to show this was true for triangular regions, I came up with this seemingly roundabout way of proving it, which I know works, but seems very roundabout. Basically the book proves the other direction for triangles, so I can just modify that proof, and then the book uses the triangle theorem to prove Cauchy's Theorem for a convex region. It is easy to substitute the rectangle theorem, for the triangle theorem. While I'm glad this works, and its almost directly lifted from the textbook, it is over 2 pages long, so I was wondering if a more direct approach could be found. Proof Summary: ""Our approach will be to first that if f is holomorphic the integral around every rectangle of $f(z)dz$ equals 0. We will then apply this fact to prove Cauchy's Theorem for a convex region. Once this is complete we can note that this new proof of Cauchy's Theorem allows us to say that if f has integral zero around every rectangle contained in G, f has a primitive in G. If f has a primitive in G, it is holomorphic."" Edit: Also I was thinking that since we've gone through the proof in the textbook and it works for triangles, if we can somehow divide the rectangle into 2 triangles and show that the integral around each of these triangles is equal to zero, then the rest follows from the book's derivations based on triangles, though honestly this seems very difficult to do.","So the question asked to prove the following:  ""Prove that if f is a continuous complex valued function in the open subset G of the complex plane, and if the integral of $f(z)dz = 0$ over every rectangle R, with edges parallel to the coordinate axes, contained with its interior in G, then f is holomorphic"" (Sarason Complex Function Theory 2nd edition). Following the logic the book used to show this was true for triangular regions, I came up with this seemingly roundabout way of proving it, which I know works, but seems very roundabout. Basically the book proves the other direction for triangles, so I can just modify that proof, and then the book uses the triangle theorem to prove Cauchy's Theorem for a convex region. It is easy to substitute the rectangle theorem, for the triangle theorem. While I'm glad this works, and its almost directly lifted from the textbook, it is over 2 pages long, so I was wondering if a more direct approach could be found. Proof Summary: ""Our approach will be to first that if f is holomorphic the integral around every rectangle of $f(z)dz$ equals 0. We will then apply this fact to prove Cauchy's Theorem for a convex region. Once this is complete we can note that this new proof of Cauchy's Theorem allows us to say that if f has integral zero around every rectangle contained in G, f has a primitive in G. If f has a primitive in G, it is holomorphic."" Edit: Also I was thinking that since we've gone through the proof in the textbook and it works for triangles, if we can somehow divide the rectangle into 2 triangles and show that the integral around each of these triangles is equal to zero, then the rest follows from the book's derivations based on triangles, though honestly this seems very difficult to do.",,['complex-analysis']
87,What can the multiset of zeros of a meromorphic function look like?,What can the multiset of zeros of a meromorphic function look like?,,"Suppose I have a multiset $S$ of complex numbers. Under what conditions is there a meromorphic function $f$ whose zeros are precisely the elements of $S$, and have the same multiplicities? I know that $S$ must be a discrete set (unless it is the entire complex plane with infinite multiplicity, for which $f(x) = 0$ has $S$ as its multiset of zeros), but as far as I know, there are no other conditions.","Suppose I have a multiset $S$ of complex numbers. Under what conditions is there a meromorphic function $f$ whose zeros are precisely the elements of $S$, and have the same multiplicities? I know that $S$ must be a discrete set (unless it is the entire complex plane with infinite multiplicity, for which $f(x) = 0$ has $S$ as its multiset of zeros), but as far as I know, there are no other conditions.",,['complex-analysis']
88,Conjecture: all complex roots of $\sum_{k=0}^\infty \frac{z^k}{\left(nk\right)!}$ are real,Conjecture: all complex roots of  are real,\sum_{k=0}^\infty \frac{z^k}{\left(nk\right)!},"Conjecture: $$\left[n\in\mathbb{Z}^+,z\in\mathbb{C},0=\sum_{k=0}^\infty \frac{z^k}{\left(nk\right)!}\right]\Rightarrow z\in\mathbb{R}$$ This conjecture has been verified for $n\in\{1,2,4\}$ . The motivation for this conjecture arose during the study of the exponential sum function which has applications to exponentiation in rings with abelian multiplication: $$\text{rues}_n\left(z\right)=\sum_{k=0}^\infty \frac{z^{nk}}{\left(nk\right)!}=\frac{1}{n}\sum _{k=1}^n \exp\left(ze^{2ki\pi/n}\right)$$",Conjecture: This conjecture has been verified for . The motivation for this conjecture arose during the study of the exponential sum function which has applications to exponentiation in rings with abelian multiplication:,"\left[n\in\mathbb{Z}^+,z\in\mathbb{C},0=\sum_{k=0}^\infty \frac{z^k}{\left(nk\right)!}\right]\Rightarrow z\in\mathbb{R} n\in\{1,2,4\} \text{rues}_n\left(z\right)=\sum_{k=0}^\infty \frac{z^{nk}}{\left(nk\right)!}=\frac{1}{n}\sum _{k=1}^n \exp\left(ze^{2ki\pi/n}\right)","['complex-analysis', 'roots', 'conjectures', 'mittag-leffler-function']"
89,"Evaluate $\int _0^{2\pi }\frac{\cos (n\theta) }{a+\cos\theta}\,d\theta$ with $a>1$, $n\in \mathbb{N}-\left\{0\right\}$","Evaluate  with ,","\int _0^{2\pi }\frac{\cos (n\theta) }{a+\cos\theta}\,d\theta a>1 n\in \mathbb{N}-\left\{0\right\}","Evaluate $$\int _0^{2\pi }\frac{\cos (n\theta) }{a+\cos\theta}\,d\theta,\quad\,a>1$$ I wrote $$f\left(z\right)=\frac{\frac{1}{2}\left(z^n+z^{-n}\right)}{\frac{iz^2}{2}+aiz+\frac{i}{2}}$$ The discriminant is $\Delta =a^2-1>0$ Poles are $z=-a\pm \sqrt{a^2-1}$ and $z=0$ $z=-a+\sqrt{a^2-1}$ and $z=0$ are within the unit circle. Applying the residue theorem, we compute: $2i\pi \lim _{z\to -a+\sqrt{a^2-1}}\left(\frac{-\frac{i}{2}\left(z^n+z^{-n}\right)}{\frac{1}{2}\left(z+a+\sqrt{a^2-1}\right)}\right)$ The first residue is $\pi \frac{\left(-a+\sqrt{a^2-1}\right)^n+\left(-a+\sqrt{a^2-1}\right)^{-n}}{\sqrt{a^2-1}}$ Now $2i\pi \lim _{z\to 0}\left(\frac{-\frac{i}{2}\left(z^{n+1}+z^{-n+1}\right)}{\frac{1}{2}\left(z+a-\sqrt{a^2-1}\right)\left(z+a+\sqrt{a^2-1}\right)}\right)$ $=\lim _{z\to 0}\left(2\pi \left(z^{n+1}+z^{-n+1}\right)\right)\:$","Evaluate I wrote The discriminant is Poles are and and are within the unit circle. Applying the residue theorem, we compute: The first residue is Now","\int _0^{2\pi }\frac{\cos (n\theta) }{a+\cos\theta}\,d\theta,\quad\,a>1 f\left(z\right)=\frac{\frac{1}{2}\left(z^n+z^{-n}\right)}{\frac{iz^2}{2}+aiz+\frac{i}{2}} \Delta =a^2-1>0 z=-a\pm \sqrt{a^2-1} z=0 z=-a+\sqrt{a^2-1} z=0 2i\pi \lim _{z\to -a+\sqrt{a^2-1}}\left(\frac{-\frac{i}{2}\left(z^n+z^{-n}\right)}{\frac{1}{2}\left(z+a+\sqrt{a^2-1}\right)}\right) \pi \frac{\left(-a+\sqrt{a^2-1}\right)^n+\left(-a+\sqrt{a^2-1}\right)^{-n}}{\sqrt{a^2-1}} 2i\pi \lim _{z\to 0}\left(\frac{-\frac{i}{2}\left(z^{n+1}+z^{-n+1}\right)}{\frac{1}{2}\left(z+a-\sqrt{a^2-1}\right)\left(z+a+\sqrt{a^2-1}\right)}\right) =\lim _{z\to 0}\left(2\pi \left(z^{n+1}+z^{-n+1}\right)\right)\:","['complex-analysis', 'contour-integration']"
90,Holomorphic function satisfying $f^{-1}(\Bbb R)=\Bbb R$ is of the form $f(z)=az+b$,Holomorphic function satisfying  is of the form,f^{-1}(\Bbb R)=\Bbb R f(z)=az+b,"Let $f$ be a holomorphic function defined on $\Bbb C$ such that $f^{-1}(\Bbb R)=\Bbb R$. Prove that there exists $a,b \in \Bbb R, \; a \neq 0 \;$ such that $f(z)=az+b \;$ for every $z \in \Bbb C$. I was given a hint: define the function $g(z)=\frac{f(z)-f(0)}{z}$. such $g$ is also holomorphic in $\Bbb C$ since the only singularity is in $0$, and it is a removable singularity since we can define $g(0)=f'(0)$. Next I thought about proving $g$ is bounded, and thus constant (using Liouville's theorem), but pretty much out of ideas. How can I finish the proof/solve in another way?","Let $f$ be a holomorphic function defined on $\Bbb C$ such that $f^{-1}(\Bbb R)=\Bbb R$. Prove that there exists $a,b \in \Bbb R, \; a \neq 0 \;$ such that $f(z)=az+b \;$ for every $z \in \Bbb C$. I was given a hint: define the function $g(z)=\frac{f(z)-f(0)}{z}$. such $g$ is also holomorphic in $\Bbb C$ since the only singularity is in $0$, and it is a removable singularity since we can define $g(0)=f'(0)$. Next I thought about proving $g$ is bounded, and thus constant (using Liouville's theorem), but pretty much out of ideas. How can I finish the proof/solve in another way?",,"['complex-analysis', 'holomorphic-functions']"
91,Relationship between Cauchy-Goursat Theorem and conservative vector fields,Relationship between Cauchy-Goursat Theorem and conservative vector fields,,"In vector calculus we know that if $\,\,\underline{F}\,\,$ is a $\,\,\underline{conservative}\,\,$ vector field, then: $\oint_\gamma{\textbf{F}\cdot d{\textbf{r}}} = 0$, where $\gamma$ is a closed curve. $\nabla\times \textbf{F} = 0$, i.e. the Curl Vanishes $\oint_{\gamma_1}{\textbf{F}\cdot d{\textbf{r}}}= \oint_{\gamma_2}{\textbf{F}\cdot d{\textbf{r}}}$, where $\gamma_i$ starts at $A$ and ends at $B$, Path Independent $\textbf{F} = \nabla\phi$, Existence of Potential, $\phi$ is a scalar field. However in Complex Analysis we know that if $f$ is holomorphic along and inside a simple closed contour $C$, then: $$\int_Cf(z)dz = 0$$ where $z\in\mathbb{C}$. I wanted to know what is the relationship between these two properties in $\mathbb{R}$ and in $\mathbb{C}$. Indeed in both we have a closed path that does not intersect itself (simple). However, in $\mathbb{R}$ we require that the vector field is conservative, whereas in $\mathbb{C}$ we require that the function is holomorphic inside and into the contour. My idea was that the property of the conservative vector field that the curl vanishes, somehow connects to $f$ being holomorphic. Indeed by the sufficiency theorem we know that $f$ is holomorphic if the four partial derivatives $u_x,u_y,v_x,v_y$ exists and are continuous on and in the contour, furthermore the Cauchy Riemann equations hold, i.e. $u_x = v_y$ and $v_x = -u_y$. The Cauchy-Riemann equations look like they could come out of the cross product between $\nabla$ and $\mathbf{F}$ somehow. Even though I couldn't figure it out. I tried to make some kind of function that transforms $f: \mathbb{C}\to\mathbb{C}$ to a function $g:\mathbb{R}^2\to\mathbb{R}^2$, where $u(x,y)\to x'$ and $v(x,y) \to y'$, where $g(x',y') = (u(x,y),v(x,y))$. However I have no idea if what I am doing even makes sense. So I would like to know if you could explain to me how the conditions of a conservative vector field translate to those in $\mathbb{C}$ and vice versa. Indeed in both cases we have that the integral around the path is zero, so there could be some relationship. Please let me know how you would go about showing the link between these two apparently separate properties.","In vector calculus we know that if $\,\,\underline{F}\,\,$ is a $\,\,\underline{conservative}\,\,$ vector field, then: $\oint_\gamma{\textbf{F}\cdot d{\textbf{r}}} = 0$, where $\gamma$ is a closed curve. $\nabla\times \textbf{F} = 0$, i.e. the Curl Vanishes $\oint_{\gamma_1}{\textbf{F}\cdot d{\textbf{r}}}= \oint_{\gamma_2}{\textbf{F}\cdot d{\textbf{r}}}$, where $\gamma_i$ starts at $A$ and ends at $B$, Path Independent $\textbf{F} = \nabla\phi$, Existence of Potential, $\phi$ is a scalar field. However in Complex Analysis we know that if $f$ is holomorphic along and inside a simple closed contour $C$, then: $$\int_Cf(z)dz = 0$$ where $z\in\mathbb{C}$. I wanted to know what is the relationship between these two properties in $\mathbb{R}$ and in $\mathbb{C}$. Indeed in both we have a closed path that does not intersect itself (simple). However, in $\mathbb{R}$ we require that the vector field is conservative, whereas in $\mathbb{C}$ we require that the function is holomorphic inside and into the contour. My idea was that the property of the conservative vector field that the curl vanishes, somehow connects to $f$ being holomorphic. Indeed by the sufficiency theorem we know that $f$ is holomorphic if the four partial derivatives $u_x,u_y,v_x,v_y$ exists and are continuous on and in the contour, furthermore the Cauchy Riemann equations hold, i.e. $u_x = v_y$ and $v_x = -u_y$. The Cauchy-Riemann equations look like they could come out of the cross product between $\nabla$ and $\mathbf{F}$ somehow. Even though I couldn't figure it out. I tried to make some kind of function that transforms $f: \mathbb{C}\to\mathbb{C}$ to a function $g:\mathbb{R}^2\to\mathbb{R}^2$, where $u(x,y)\to x'$ and $v(x,y) \to y'$, where $g(x',y') = (u(x,y),v(x,y))$. However I have no idea if what I am doing even makes sense. So I would like to know if you could explain to me how the conditions of a conservative vector field translate to those in $\mathbb{C}$ and vice versa. Indeed in both cases we have that the integral around the path is zero, so there could be some relationship. Please let me know how you would go about showing the link between these two apparently separate properties.",,"['complex-analysis', 'vector-analysis', 'vector-fields']"
92,Proving surjectivity of $\cos(z)$ and $\sin(z)$ and find all $z : \cos(z) \in \mathbb R$ and all $z: \sin(z) \in \mathbb R$,Proving surjectivity of  and  and find all  and all,\cos(z) \sin(z) z : \cos(z) \in \mathbb R z: \sin(z) \in \mathbb R,"I am trying to solve the following two problems: 1) Prove that the functions $\cos(z)$, $\sin(z)$ are surjective over the complex numbers. 2) Find all $z \in \mathbb C$: $cos(z) \in \mathbb R$ and find all $z \in \mathbb C$: $\sin(z) \in \mathbb R$. For 1),I've tried to prove it for the function $\cos(z)$ (I suppose the other one is analogue)  so I've used the fact that $\cos(z)=\dfrac{e^{iz}+e^{-iz}}{2}$. Let $w \in \mathbb C$, I want to show there exists $z \in \mathbb C : f(z)=w$, i.e., $\dfrac{e^{iz}+e^{-iz}}{2}=w$, multiplying by $2$ and then by $e^{iz}$ yields $e^{2iz}+1=2we^{iz}$ iff $e^{2iz}-2we^{iz}+1=0$. I don't know if this approach is the correct one but here I've replaced $e^{iz}$ by $x$, so the solutions of the equation would be the roots of the polynomial $p(x)=x^2-2wx+1$, by the quadratic formula, I get that $x \in \{\dfrac{2w+w_0}{2},\dfrac{2w-w_0}{2}\}$, where $w_0^2=4w^2-4$. Then, $e^{iz} \in \{\dfrac{2w+w_0}{2},\dfrac{2w-w_0}{2}\}$. At this point I got lost, I would like to explicitly show that $z$ exists and I can't see existence directly from the fact that $e^{iz}=\dfrac{2w+w_0}{2}$ or $e^{iz}=\dfrac{2w-w_0}{2}$. First I thought of taking logarithm of both sides of the equation ir order to solve for $z$, but this is not a legitimate operation unless $e^{iz} \in \mathbb R$. I couldn't go any farther. For point 2) I have no idea what to do, should I use the identity $\cos(z)=\dfrac{e^{iz}+e^{-iz}}{2}$? I would appreciate some help with the two points (specially with point 2), at least in 1) I could do something). Btw, Happy new year!","I am trying to solve the following two problems: 1) Prove that the functions $\cos(z)$, $\sin(z)$ are surjective over the complex numbers. 2) Find all $z \in \mathbb C$: $cos(z) \in \mathbb R$ and find all $z \in \mathbb C$: $\sin(z) \in \mathbb R$. For 1),I've tried to prove it for the function $\cos(z)$ (I suppose the other one is analogue)  so I've used the fact that $\cos(z)=\dfrac{e^{iz}+e^{-iz}}{2}$. Let $w \in \mathbb C$, I want to show there exists $z \in \mathbb C : f(z)=w$, i.e., $\dfrac{e^{iz}+e^{-iz}}{2}=w$, multiplying by $2$ and then by $e^{iz}$ yields $e^{2iz}+1=2we^{iz}$ iff $e^{2iz}-2we^{iz}+1=0$. I don't know if this approach is the correct one but here I've replaced $e^{iz}$ by $x$, so the solutions of the equation would be the roots of the polynomial $p(x)=x^2-2wx+1$, by the quadratic formula, I get that $x \in \{\dfrac{2w+w_0}{2},\dfrac{2w-w_0}{2}\}$, where $w_0^2=4w^2-4$. Then, $e^{iz} \in \{\dfrac{2w+w_0}{2},\dfrac{2w-w_0}{2}\}$. At this point I got lost, I would like to explicitly show that $z$ exists and I can't see existence directly from the fact that $e^{iz}=\dfrac{2w+w_0}{2}$ or $e^{iz}=\dfrac{2w-w_0}{2}$. First I thought of taking logarithm of both sides of the equation ir order to solve for $z$, but this is not a legitimate operation unless $e^{iz} \in \mathbb R$. I couldn't go any farther. For point 2) I have no idea what to do, should I use the identity $\cos(z)=\dfrac{e^{iz}+e^{-iz}}{2}$? I would appreciate some help with the two points (specially with point 2), at least in 1) I could do something). Btw, Happy new year!",,"['complex-analysis', 'complex-numbers']"
93,Any even elliptic function can be written in terms of the Weierstrass $\wp$ function,Any even elliptic function can be written in terms of the Weierstrass  function,\wp,"Given two nonzero complex numbers $\omega_1, \omega_2$, with nonreal ratio, we define the period module $$M= \omega_1 \mathbb Z+ \omega_2 \mathbb Z= \{n_1 \omega_1+ n_2 \omega_2:n_1,n_2 \in \mathbb Z \} $$  and the Weierstrass $\wp$ function $$\wp(z; \omega_1, \omega_2) \equiv \wp(z;M)= \frac{1}{z^2}+ \sum_{\omega \in M \setminus \{0 \}} \frac{1}{(z- \omega)^2}-\frac{1}{\omega^2} .$$ I want to solve the following exercise from Ahlfors' complex analysis text (page 274): Show that any even elliptic function with periods $\omega_1$, $\omega_2$ can be expressed in the form   $$C \prod_{k=1}^n \frac{\wp(z)-\wp(a_k)}{\wp(z)-\wp(b_k)} \text{ ($C$=const.)} $$   provided  that $0$ is neither a zero nor a pole. What is the corresponding form if the function either vanishes or becomes infinite at the origin? My attempt: Let $f$ be an even elliptic function with periods $\omega_1,\omega_2$, and suppose for the moment that $f$ has neither a zero nor a pole at the origin. If $f$ is constant, we have an empty product representation $$f(z)= C \prod_{k=1}^0 \left( \dots \right). $$ Suppose now that $f$ isn't constant. As an elliptic function $f$ has equal number of (congruent) zeros and poles. Denote its zeros by $a_1, \dots,a_n$ and its poles by $b_1, \dots ,b_n$ (multiple points being repeated), and define $$g(z)=f(z) \bigg/ \prod_{k=1}^n \frac{\wp(z)-\wp(a_k)}{\wp(z)-\wp(b_k)} $$  What I want to say is that any numerator $$\wp(z)-\wp(a_k) $$ has a simple zero at $a_k$, and any denominator $$\frac{1}{\wp(z)-\wp(b_k)}$$ has a simple pole at $b_k$. If that's true then $g$ is a holomorphic elliptic function, which reduces to a constant $C$. If $f$ has a zero of order $2m$ at the origin we repeat the proof for $\tilde{f}= \wp^m f$ and we obtain the representation $$f(z)=C \wp(z)^{-m} \prod_{k=1}^n \frac{\wp(z)-\wp(a_k)}{\wp(z)-\wp(b_k)} \text{ ($C$=const.)} $$ If $f$ has a pole of order $2m$ at the origin we repeat the proof for $\tilde{f}= \wp^{-m} f$ and we obtain the representation $$f(z)=C \wp(z)^{+m} \prod_{k=1}^n \frac{\wp(z)-\wp(a_k)}{\wp(z)-\wp(b_k)} \text{ ($C$=const.)} .$$ My question: Why are all values of $\wp$ (except $\infty$) taken ""simply"" (that is with non-vanishing derivative at the point)? I tried considering the ""fundamental parallelogram"" with vertices at $a,a+\omega_1,a+\omega_2,a+\omega_1+\omega_2 $ where $a=-\frac{1}{2} \omega_1-\frac{1}{2} \omega_2$, and WLOG the uppermost and rightmost edges are included. It is known that in this parallelogram all complex values are taken twice. Since $\wp$ is even, if $a$ is an interior point, the value $\wp(a)$ is taken at least twice, at the points $\pm a$. However, if $a$ lies on the part of the boundary of the parallelogram which is included, I can't use the evenness argument, and as far as I can tell $a$ might be a double value of $\wp$ (?) Is my solution correct so far? and can you please help me with the question in boldface? Thanks!","Given two nonzero complex numbers $\omega_1, \omega_2$, with nonreal ratio, we define the period module $$M= \omega_1 \mathbb Z+ \omega_2 \mathbb Z= \{n_1 \omega_1+ n_2 \omega_2:n_1,n_2 \in \mathbb Z \} $$  and the Weierstrass $\wp$ function $$\wp(z; \omega_1, \omega_2) \equiv \wp(z;M)= \frac{1}{z^2}+ \sum_{\omega \in M \setminus \{0 \}} \frac{1}{(z- \omega)^2}-\frac{1}{\omega^2} .$$ I want to solve the following exercise from Ahlfors' complex analysis text (page 274): Show that any even elliptic function with periods $\omega_1$, $\omega_2$ can be expressed in the form   $$C \prod_{k=1}^n \frac{\wp(z)-\wp(a_k)}{\wp(z)-\wp(b_k)} \text{ ($C$=const.)} $$   provided  that $0$ is neither a zero nor a pole. What is the corresponding form if the function either vanishes or becomes infinite at the origin? My attempt: Let $f$ be an even elliptic function with periods $\omega_1,\omega_2$, and suppose for the moment that $f$ has neither a zero nor a pole at the origin. If $f$ is constant, we have an empty product representation $$f(z)= C \prod_{k=1}^0 \left( \dots \right). $$ Suppose now that $f$ isn't constant. As an elliptic function $f$ has equal number of (congruent) zeros and poles. Denote its zeros by $a_1, \dots,a_n$ and its poles by $b_1, \dots ,b_n$ (multiple points being repeated), and define $$g(z)=f(z) \bigg/ \prod_{k=1}^n \frac{\wp(z)-\wp(a_k)}{\wp(z)-\wp(b_k)} $$  What I want to say is that any numerator $$\wp(z)-\wp(a_k) $$ has a simple zero at $a_k$, and any denominator $$\frac{1}{\wp(z)-\wp(b_k)}$$ has a simple pole at $b_k$. If that's true then $g$ is a holomorphic elliptic function, which reduces to a constant $C$. If $f$ has a zero of order $2m$ at the origin we repeat the proof for $\tilde{f}= \wp^m f$ and we obtain the representation $$f(z)=C \wp(z)^{-m} \prod_{k=1}^n \frac{\wp(z)-\wp(a_k)}{\wp(z)-\wp(b_k)} \text{ ($C$=const.)} $$ If $f$ has a pole of order $2m$ at the origin we repeat the proof for $\tilde{f}= \wp^{-m} f$ and we obtain the representation $$f(z)=C \wp(z)^{+m} \prod_{k=1}^n \frac{\wp(z)-\wp(a_k)}{\wp(z)-\wp(b_k)} \text{ ($C$=const.)} .$$ My question: Why are all values of $\wp$ (except $\infty$) taken ""simply"" (that is with non-vanishing derivative at the point)? I tried considering the ""fundamental parallelogram"" with vertices at $a,a+\omega_1,a+\omega_2,a+\omega_1+\omega_2 $ where $a=-\frac{1}{2} \omega_1-\frac{1}{2} \omega_2$, and WLOG the uppermost and rightmost edges are included. It is known that in this parallelogram all complex values are taken twice. Since $\wp$ is even, if $a$ is an interior point, the value $\wp(a)$ is taken at least twice, at the points $\pm a$. However, if $a$ lies on the part of the boundary of the parallelogram which is included, I can't use the evenness argument, and as far as I can tell $a$ might be a double value of $\wp$ (?) Is my solution correct so far? and can you please help me with the question in boldface? Thanks!",,"['complex-analysis', 'analysis', 'special-functions', 'elliptic-functions']"
94,Dogbone contour integral/branch cuts/residue at infinity,Dogbone contour integral/branch cuts/residue at infinity,,"I am trying to compute: $$\int_0^1 \frac{\sqrt{x-x^2}}{x+2} dx$$ by contour integration. I define $f(z) = \sqrt{z-z^2}$ with a branch cut on $[0,1]$ in such a way that $f(-1)=\sqrt{2}i$, then define $g(z)=\frac{f(z)}{z+2}$. Then I integrate $g$ clockwise around a dogbone contour from 0 to 1. The two arcs' contributions go to zero as the radius goes to zero, simply because the integrand is bounded if $z$ is away from $-2$. The choice of branch cut means that the integrand is of opposite sign on the two sides of the branch cut. This combined with the different orientation (the top integral is traversed going to the right, the bottom to the left) means that the top and bottom integrals are the same, both equal in the limit to the value that is desired. Consequently: $$2 \int_0^1 \frac{\sqrt{x-x^2}}{x+2} dx = 2 \pi i ( Res(g,-2) + Res(g,\infty) )$$ by the exterior domain residue theorem. The first one is by the rules of residue calculus $f(-2)=\sqrt{6}i$, from how $f$ is defined. This is fine (it turns out the way it should). My problem is with $Res(g,\infty)$. The definition of this is $Res \left (-\frac{g(1/z)}{z^2},0 \right )$. So this is $Res \left (-\frac{f(1/z)}{z^2(1/z+2)},0 \right )$. My only idea for computing this was to try to break up $f(1/z)$ using the branch cut and ultimately binomial expand the square root. Doing this gives the correct answer, namely $-5/2i$, provided that $(-1)^{1/2}=i$ in the binomial expansion formula. I assume this comes from the fact that the binomial expansion holds for the principal square root, but am not sure. Overall plugging these back in gives $$\int_0^1 \frac{\sqrt{x-x^2}}{x+2} dx = \pi (5/2 - \sqrt{6})$$ which is correct. But is there a better way of thinking about this?","I am trying to compute: $$\int_0^1 \frac{\sqrt{x-x^2}}{x+2} dx$$ by contour integration. I define $f(z) = \sqrt{z-z^2}$ with a branch cut on $[0,1]$ in such a way that $f(-1)=\sqrt{2}i$, then define $g(z)=\frac{f(z)}{z+2}$. Then I integrate $g$ clockwise around a dogbone contour from 0 to 1. The two arcs' contributions go to zero as the radius goes to zero, simply because the integrand is bounded if $z$ is away from $-2$. The choice of branch cut means that the integrand is of opposite sign on the two sides of the branch cut. This combined with the different orientation (the top integral is traversed going to the right, the bottom to the left) means that the top and bottom integrals are the same, both equal in the limit to the value that is desired. Consequently: $$2 \int_0^1 \frac{\sqrt{x-x^2}}{x+2} dx = 2 \pi i ( Res(g,-2) + Res(g,\infty) )$$ by the exterior domain residue theorem. The first one is by the rules of residue calculus $f(-2)=\sqrt{6}i$, from how $f$ is defined. This is fine (it turns out the way it should). My problem is with $Res(g,\infty)$. The definition of this is $Res \left (-\frac{g(1/z)}{z^2},0 \right )$. So this is $Res \left (-\frac{f(1/z)}{z^2(1/z+2)},0 \right )$. My only idea for computing this was to try to break up $f(1/z)$ using the branch cut and ultimately binomial expand the square root. Doing this gives the correct answer, namely $-5/2i$, provided that $(-1)^{1/2}=i$ in the binomial expansion formula. I assume this comes from the fact that the binomial expansion holds for the principal square root, but am not sure. Overall plugging these back in gives $$\int_0^1 \frac{\sqrt{x-x^2}}{x+2} dx = \pi (5/2 - \sqrt{6})$$ which is correct. But is there a better way of thinking about this?",,"['complex-analysis', 'contour-integration']"
95,Does holomorphic a.e. and continuous imply holomorphic everywhere?,Does holomorphic a.e. and continuous imply holomorphic everywhere?,,"Suppose $D$ is a domain in $\mathbb{C}$, $f:D\rightarrow \mathbb{C}$ is a continuous function. Suppose $f$ is holomorphic outside the zero set $f^{-1}(0)$, and $f^{-1}(0)$ has Lebesgue measure zero. Question : Is $f$ holomorphic on the whole domain $D$ or not? The point that I'm confused with is that it seems that $f$ is a weakly holomorphic function, but I cannot prove it. Weakly holomorphic corresponds to $\int_D f\cdot\partial_{\bar{z}}\phi=0$ for every $\phi\in C_c^\infty(D)$, but I can only prove that $\int_D f\cdot\partial_{\bar{z}}\phi=0$ for every $\phi\in C_c^\infty(D-K)$, where $K:=f^{-1}(0)$. Any answer or comment is welcome. I really appreciate your help.","Suppose $D$ is a domain in $\mathbb{C}$, $f:D\rightarrow \mathbb{C}$ is a continuous function. Suppose $f$ is holomorphic outside the zero set $f^{-1}(0)$, and $f^{-1}(0)$ has Lebesgue measure zero. Question : Is $f$ holomorphic on the whole domain $D$ or not? The point that I'm confused with is that it seems that $f$ is a weakly holomorphic function, but I cannot prove it. Weakly holomorphic corresponds to $\int_D f\cdot\partial_{\bar{z}}\phi=0$ for every $\phi\in C_c^\infty(D)$, but I can only prove that $\int_D f\cdot\partial_{\bar{z}}\phi=0$ for every $\phi\in C_c^\infty(D-K)$, where $K:=f^{-1}(0)$. Any answer or comment is welcome. I really appreciate your help.",,"['complex-analysis', 'partial-differential-equations']"
96,"Prove that $\int_0^{\pi/2} \cos^{p+q-2}(\theta) \cos((p-q)\theta)d\theta = \frac{\pi}{(p+q-1)2^{p+q-1}B(p,q)}$",Prove that,"\int_0^{\pi/2} \cos^{p+q-2}(\theta) \cos((p-q)\theta)d\theta = \frac{\pi}{(p+q-1)2^{p+q-1}B(p,q)}","Does anybody know how to prove this identity $?$ : \begin{align*} & \int_{0}^{\pi/2} \cos^{p + q - 2}\,\left(\,{\theta}\,\right) \cos\left(\,{\left[\,{p - q}\,\right] \theta}\,\right) \,{\rm d}\theta \\[2mm] = & \ \frac{\pi}{\left(\,{p + q - 1}\,\right) 2^{p + q - 1}\, \operatorname{B}\left(\,{p,q}\,\right)}\ , \qquad p+q>1,\ q<1 \end{align*} where $\operatorname{B}\left(\,{x,y}\,\right)$ denotes Beta Function . I am confused because the result contains beta function in the denominator. I did it using Cauchy's Beta Integral but my friend says there's another method using contour integration: I can't figure it out.",Does anybody know how to prove this identity : where denotes Beta Function . I am confused because the result contains beta function in the denominator. I did it using Cauchy's Beta Integral but my friend says there's another method using contour integration: I can't figure it out.,"? \begin{align*}
& \int_{0}^{\pi/2}
\cos^{p + q - 2}\,\left(\,{\theta}\,\right)
\cos\left(\,{\left[\,{p - q}\,\right]
\theta}\,\right)
\,{\rm d}\theta
\\[2mm] = & \
\frac{\pi}{\left(\,{p + q - 1}\,\right)
2^{p + q - 1}\,
\operatorname{B}\left(\,{p,q}\,\right)}\ ,
\qquad p+q>1,\ q<1
\end{align*} \operatorname{B}\left(\,{x,y}\,\right)","['complex-analysis', 'special-functions', 'definite-integrals']"
97,Clarification on the Weierstrass factorization theorem,Clarification on the Weierstrass factorization theorem,,"Given any entire function $f(z)$, with zeros on $\{a_n\}$, it must be of the form $$f(z) = z^m e^{g(z)} \prod_1^\infty E_{p_n}(z/a_n)$$ where the $E_{p_n}$ denote elementary factors, and $g$ is entire. Is this factorization unique or is there a counterexample? Can we always choose the $p_n$ so that the same zeros $a_n$ give the same values of $p_n$?","Given any entire function $f(z)$, with zeros on $\{a_n\}$, it must be of the form $$f(z) = z^m e^{g(z)} \prod_1^\infty E_{p_n}(z/a_n)$$ where the $E_{p_n}$ denote elementary factors, and $g$ is entire. Is this factorization unique or is there a counterexample? Can we always choose the $p_n$ so that the same zeros $a_n$ give the same values of $p_n$?",,['complex-analysis']
98,Serre's proof that zeta function is meromorphic,Serre's proof that zeta function is meromorphic,,"I try to understand the proof of Chap. VI, n° 3.1, Prop. 10 in Serre's ""A course in arithmetic"" (page 70). The goal is to prove that zeta-function can be written as \begin{align*} \zeta(s)=\frac{1}{s-1}+\phi(s) \end{align*} with an holomorphic $\phi$ for any $s\in\mathbb{C}$ with $\mathfrak{Re}(s)>0$. I understand the proof except for one detail: $\phi$ is given explicitly by $\phi=\sum_{n=1}^\infty \phi_n$ with \begin{align*} \phi_n(s)=\int_n^{n+1} \left(n^{-s}-t^{-s}\right) dt. \end{align*} To prove the convergence of $\phi$, we want to show that  \begin{align*} \left|\phi_n(s)\right|\leq\frac{|s|}{n^{x+1}} \quad \text{with}\quad x=\mathfrak{Re}(s)\quad(*) \end{align*} For this Serre first notes that \begin{align*} \left|\phi_n(s)\right| \leq \sup_{n\leq t\leq n+1}\left| n^{-s}-t^{-s}\right| \end{align*} Which is clear since the range of integration is $(n+1)-n=1$. Then he sais that the derivative of $n^{-s}-t^{-s}$ is equal to $\frac{s}{t^{s+1}}$. And from this somehow follows (*). My question is: How exactly does this follow? I noted the following statements for $s=x+iy$ where $x=\mathfrak{Re}(s)$ and $y=\mathfrak{Im}(s)$ and $f(t)=n^{-s}-t^{-s}$ Because $|t^{iy}|=|e^{iy\ln(t)}|=1$ (since it is on the unit circle) and $|t^{x+1}|=t^{x+1}$ (since $t$ and $x$ are real) we see that \begin{align*} \left|f'(t)\right|=\left|\frac{s}{t^{x+1+iy}}\right|=\frac{|s|}{|t^{x+1}| |t^{iy}|}=\frac{|s|}{t^{x+1}} \end{align*} Because $f(n)=0$ we can calculate \begin{align*} |f(n+1)|=|f(n+1)-f(n)|=\left|\int_n^{n+1}f'(t)dt\right|\leq\sup_{n\leq t\leq n+1}\left|f'(t)\right|=\sup_{n\leq t\leq n+1}\frac{|s|}{t^{x+1}}=\frac{|s|}{n^{x+1}} \end{align*} Am I close? :-)","I try to understand the proof of Chap. VI, n° 3.1, Prop. 10 in Serre's ""A course in arithmetic"" (page 70). The goal is to prove that zeta-function can be written as \begin{align*} \zeta(s)=\frac{1}{s-1}+\phi(s) \end{align*} with an holomorphic $\phi$ for any $s\in\mathbb{C}$ with $\mathfrak{Re}(s)>0$. I understand the proof except for one detail: $\phi$ is given explicitly by $\phi=\sum_{n=1}^\infty \phi_n$ with \begin{align*} \phi_n(s)=\int_n^{n+1} \left(n^{-s}-t^{-s}\right) dt. \end{align*} To prove the convergence of $\phi$, we want to show that  \begin{align*} \left|\phi_n(s)\right|\leq\frac{|s|}{n^{x+1}} \quad \text{with}\quad x=\mathfrak{Re}(s)\quad(*) \end{align*} For this Serre first notes that \begin{align*} \left|\phi_n(s)\right| \leq \sup_{n\leq t\leq n+1}\left| n^{-s}-t^{-s}\right| \end{align*} Which is clear since the range of integration is $(n+1)-n=1$. Then he sais that the derivative of $n^{-s}-t^{-s}$ is equal to $\frac{s}{t^{s+1}}$. And from this somehow follows (*). My question is: How exactly does this follow? I noted the following statements for $s=x+iy$ where $x=\mathfrak{Re}(s)$ and $y=\mathfrak{Im}(s)$ and $f(t)=n^{-s}-t^{-s}$ Because $|t^{iy}|=|e^{iy\ln(t)}|=1$ (since it is on the unit circle) and $|t^{x+1}|=t^{x+1}$ (since $t$ and $x$ are real) we see that \begin{align*} \left|f'(t)\right|=\left|\frac{s}{t^{x+1+iy}}\right|=\frac{|s|}{|t^{x+1}| |t^{iy}|}=\frac{|s|}{t^{x+1}} \end{align*} Because $f(n)=0$ we can calculate \begin{align*} |f(n+1)|=|f(n+1)-f(n)|=\left|\int_n^{n+1}f'(t)dt\right|\leq\sup_{n\leq t\leq n+1}\left|f'(t)\right|=\sup_{n\leq t\leq n+1}\frac{|s|}{t^{x+1}}=\frac{|s|}{n^{x+1}} \end{align*} Am I close? :-)",,"['complex-analysis', 'riemann-zeta', 'zeta-functions']"
99,Rouché's Theorem for $p(z)=z^7-5z^3+12$,Rouché's Theorem for,p(z)=z^7-5z^3+12,"Let $f$ and $g$ be differentiable on a domain $D$ and suppose that $\gamma$ is a simple closed contour whose inside is contained in D. If $|f(z)-g(z)|<|f(z)|$ for all $z$ on $\gamma$, then $f$ and $g$ have the same number of zeros inside $\gamma$ (counted including their order). I was reading an example of application of Rouché's Theorem, where Rouché's theorem was used to show that the polynomial $p(z)=z^7-5z^3+12$ has $0$ roots in $\{z:\mathbb{C}:|z|<1\}$. What was done was: Let $g(z)=z^7-5z^3+12$ and let $f(z)=12$. Then for $|z|=1$, $|f(z)-g(z)|=|z^7-5z^3| \\ \le|z|^7+5|z| \\=1+5\\=6<12=|f(z)|$ Hence by Rouché's Theorem $p(z)=z^7-5z^3+12$ has $7$ roots in $\{z:\mathbb{C}:|z|<2\}$. I was wondering, what is the purpose to do the step $\le|z|^7+5|z|$? Can't I just jump straight from  $|f(z)-g(z)|=|z^7-5z^3| \\=|1-5|\\=4<12=|f(z)|?$ Secondly, $|f(z)-g(z)|=|-z^7+5z^3|$, is there a reason why they used $|f(z)-g(z)|=|z^7-5z^3|$? Thirdly, what does it mean by ""(counted including their order)""? (From the definition above)","Let $f$ and $g$ be differentiable on a domain $D$ and suppose that $\gamma$ is a simple closed contour whose inside is contained in D. If $|f(z)-g(z)|<|f(z)|$ for all $z$ on $\gamma$, then $f$ and $g$ have the same number of zeros inside $\gamma$ (counted including their order). I was reading an example of application of Rouché's Theorem, where Rouché's theorem was used to show that the polynomial $p(z)=z^7-5z^3+12$ has $0$ roots in $\{z:\mathbb{C}:|z|<1\}$. What was done was: Let $g(z)=z^7-5z^3+12$ and let $f(z)=12$. Then for $|z|=1$, $|f(z)-g(z)|=|z^7-5z^3| \\ \le|z|^7+5|z| \\=1+5\\=6<12=|f(z)|$ Hence by Rouché's Theorem $p(z)=z^7-5z^3+12$ has $7$ roots in $\{z:\mathbb{C}:|z|<2\}$. I was wondering, what is the purpose to do the step $\le|z|^7+5|z|$? Can't I just jump straight from  $|f(z)-g(z)|=|z^7-5z^3| \\=|1-5|\\=4<12=|f(z)|?$ Secondly, $|f(z)-g(z)|=|-z^7+5z^3|$, is there a reason why they used $|f(z)-g(z)|=|z^7-5z^3|$? Thirdly, what does it mean by ""(counted including their order)""? (From the definition above)",,['complex-analysis']
