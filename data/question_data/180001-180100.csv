,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Find the point on the graph of $z= x^{2} +y^{2} +10$ nearest to the plane $x+2y-z=0$.,Find the point on the graph of  nearest to the plane .,z= x^{2} +y^{2} +10 x+2y-z=0,"Find the point on the graph of $z= x^{2} +y^{2} +10$ nearest to the plane $x+2y-z=0$ . So, any point on the given surface will be $(x,y,x^{2} +y^{2} +10)$ . I need to minimize the function $(x +2y-x^{2}-y^{2}-10)/(\sqrt{6})$ The only critical point is $(1/2,1)$ . But this point gives maximum of the function. How would I find the nearest point. What I think is that, I should change the sign of the function, to keep the distance positive, then I'll get the same critical point, but the value will be minimum. So I'll get the nearest point. In this case if I am asked to find the maximum distance, what it should be then $?$","Find the point on the graph of nearest to the plane . So, any point on the given surface will be . I need to minimize the function The only critical point is . But this point gives maximum of the function. How would I find the nearest point. What I think is that, I should change the sign of the function, to keep the distance positive, then I'll get the same critical point, but the value will be minimum. So I'll get the nearest point. In this case if I am asked to find the maximum distance, what it should be then","z= x^{2} +y^{2} +10 x+2y-z=0 (x,y,x^{2} +y^{2} +10) (x +2y-x^{2}-y^{2}-10)/(\sqrt{6}) (1/2,1) ?","['calculus', 'multivariable-calculus']"
1,"compute the limit of $xy(x+y-2)$ as $(x,y)$ approaches infinity for two domains",compute the limit of  as  approaches infinity for two domains,"xy(x+y-2) (x,y)","Problem is to compute the limit of $xy(x+y-2)$ as $(x,y)$ approaches infinity for the domain (a) $D_f=R^2$ (b) $D_f=\{(x,y): |y-x|<1, x,y>0\}$ For (a) I have found that when approaching infinity along the line $x=y$ we have that the values for the function goes to infinity. Along the $x$ -axis the function becomes $0$ and therefor the limit becomes $0 $ as well. So for the first domain (which is all of the points in $R^2$ ) there exists no limit. For (b) I am not sure how to approach the problem. I know that $|y-x|<1$ is the same as $-1<y-x <1$ . The problem becomes reduced to only look at points in $R^2$ that satisfied that the distance between them are not more than 1, and are positive. I am thinking that maybe it is possible to estimate the function to be less than another function of one variabel and somehow use the squeeze theorem. My problem is that I am no good at inequalities like that.","Problem is to compute the limit of as approaches infinity for the domain (a) (b) For (a) I have found that when approaching infinity along the line we have that the values for the function goes to infinity. Along the -axis the function becomes and therefor the limit becomes as well. So for the first domain (which is all of the points in ) there exists no limit. For (b) I am not sure how to approach the problem. I know that is the same as . The problem becomes reduced to only look at points in that satisfied that the distance between them are not more than 1, and are positive. I am thinking that maybe it is possible to estimate the function to be less than another function of one variabel and somehow use the squeeze theorem. My problem is that I am no good at inequalities like that.","xy(x+y-2) (x,y) D_f=R^2 D_f=\{(x,y): |y-x|<1, x,y>0\} x=y x 0 0  R^2 |y-x|<1 -1<y-x <1 R^2","['limits', 'multivariable-calculus']"
2,"Find the tangent plane to the image of $\phi(u,v)=(u^2,u\sin e^v,\frac{1}{3}u\cos e^v)$ at $(13,-2,1)$",Find the tangent plane to the image of  at,"\phi(u,v)=(u^2,u\sin e^v,\frac{1}{3}u\cos e^v) (13,-2,1)","It's clear we need to find the normal to the tangent vectors $T_u,T_v$ , etc. But if we have to compute $T_u \times T_v$ at the specific point mentioned, we have to solve for $u,v$ in $(1)$ : $$\begin{cases} 13 = u^2 \\ -2 = u \sin e^v \\ 1 = \frac{1}{3} u \cos e^v \end{cases} \tag 1$$ Is this right? If there is no way around this, then how do we solve for $u,v$ in $(1)$ ?","It's clear we need to find the normal to the tangent vectors , etc. But if we have to compute at the specific point mentioned, we have to solve for in : Is this right? If there is no way around this, then how do we solve for in ?","T_u,T_v T_u \times T_v u,v (1) \begin{cases} 13 = u^2 \\ -2 = u \sin e^v \\ 1 = \frac{1}{3} u \cos e^v \end{cases} \tag 1 u,v (1)","['multivariable-calculus', 'vector-analysis']"
3,Local Extrema of Function & Supremum,Local Extrema of Function & Supremum,,"Let $f(x, y) :=−3x^4−y^2+ 4x^2y$ . Prove that, for any $v∈R^2$ , the function $h_v:R→R$ given by $h_v(t):=f(tv)$ has a local maximum at t= 0.  Then find all local extrema of f, as well as $sup_{R^2} f$ and $inf_{R^2} f$ . To find the local maximum, i will have to find partial second order derivative for f(x,y) but is this the same as f(tv)? I know that If f has a local max or min at a then for h the function g(t)=f(a + th) has a local max or min at t = 0, but i dont know how to prove that. In addition, how owuld i find all local extrema, sup and inf? Thanks.","Let . Prove that, for any , the function given by has a local maximum at t= 0.  Then find all local extrema of f, as well as and . To find the local maximum, i will have to find partial second order derivative for f(x,y) but is this the same as f(tv)? I know that If f has a local max or min at a then for h the function g(t)=f(a + th) has a local max or min at t = 0, but i dont know how to prove that. In addition, how owuld i find all local extrema, sup and inf? Thanks.","f(x, y) :=−3x^4−y^2+ 4x^2y v∈R^2 h_v:R→R h_v(t):=f(tv) sup_{R^2} f inf_{R^2} f","['real-analysis', 'calculus', 'multivariable-calculus', 'maxima-minima']"
4,Diffgeo question on curves and properties of dot product,Diffgeo question on curves and properties of dot product,,"Let $\gamma$ be a curve such that $\gamma(a)=p$ and $\gamma(b)=q$ and let $u$ be a unit vector. A) Prove $\gamma'(t) \cdot u$ $\leq$ $\|\gamma’(t)\|$ For this one I use the properties of dot product, noting that $u=\frac{\gamma’}{\|\gamma\|}$ but I always end up just showing that the quantities are equal. B) Use path integrals to show ( $\gamma(b)-\gamma(a)) \cdot u$ $\leq$ $\int_{a}^{b} \|\gamma'(t)\| dt$ For this one I observe that $u=\frac{(q-p)}{\|(q-p)\|}$ and sloppily reach a conclusion. C) Prove the length of gamma from $p$ to $q$ is larger than or equal to the length of the straight line between $p$ and $q$ . This one is intuitively obvious, but for the proof I observe the straight line dist is just $\|p-q\|$ and the length of gamma is the arc length. I had myself convinced I had this part the other day, but now I’m confused. Hopefully someone can make this all more clear for me. I apologize for the terrible (lack of) formatting Thx Nick","Let be a curve such that and and let be a unit vector. A) Prove For this one I use the properties of dot product, noting that but I always end up just showing that the quantities are equal. B) Use path integrals to show ( For this one I observe that and sloppily reach a conclusion. C) Prove the length of gamma from to is larger than or equal to the length of the straight line between and . This one is intuitively obvious, but for the proof I observe the straight line dist is just and the length of gamma is the arc length. I had myself convinced I had this part the other day, but now I’m confused. Hopefully someone can make this all more clear for me. I apologize for the terrible (lack of) formatting Thx Nick",\gamma \gamma(a)=p \gamma(b)=q u \gamma'(t) \cdot u \leq \|\gamma’(t)\| u=\frac{\gamma’}{\|\gamma\|} \gamma(b)-\gamma(a)) \cdot u \leq \int_{a}^{b} \|\gamma'(t)\| dt u=\frac{(q-p)}{\|(q-p)\|} p q p q \|p-q\|,"['linear-algebra', 'multivariable-calculus', 'differential-geometry']"
5,Deriving polar substitution formula for double integrals,Deriving polar substitution formula for double integrals,,"I'm studying double integrals (the theory part, not the problem solving). We're  trying to derive the formula for the polar substitution in a double integral: $$\iint_{D}f(x, y)dxdy = \iint_{D_1}f(r\cos\theta, r\sin\theta)rdrd\theta$$ where $x = r\cos\theta, y = r\sin\theta$ maps $D_1 \to D$ First I want to say that it is explicitly stated in the textbook that we're not going to prove the general formula for substitutions in the integral (the one that uses the Jacobian) but rather, just for now, the formula for the polar substitution. It starts by defining a set $S := \big\{(r, \theta)|\bar{r_1} \leq r\leq \bar{r_2}, \bar{\theta_1} \leq \theta \leq \bar{\theta_2}\big\}$ and dividing this set using lines/curves $r = r_j$ and $\theta = \theta_j$ where: $$r_j := \bar{r_1} + j\triangle r, \text{where } \triangle r := \frac{\bar{r_2} - \bar{r_1}}{n} \\\theta_j:= \bar{\theta_1}+j\triangle \theta, \text{where } \triangle\theta := \frac{\bar{\theta_2} - \bar{\theta_1}}{n} $$ and $j=0, 1, ..., n$ This part is clear, it's pretty similar to dividing a rectangular region when defining a double integral itself. So an illustration would look something like this (sorry for the bad picture, couldn't find a way to do it in Desmos): The following sentence really confuses me because it's stated as a given (without any explanation whatsoever): If we look at the region $\big\{ (r, \theta)| r_0 \leq r \leq r_0 + \triangle r, \theta_0 \leq \theta \leq \theta_0 + \triangle \theta  \big\}$ we notice that the area of that region is $$\frac{1}{2}\triangle \theta ((r_0 + \triangle r)^2 - r_0^2) = (r_0 + \frac{\triangle r}{2})\triangle r \triangle \theta$$ The region that we're talking about is, as far as I can see, the 'bottom-right' sub-region of the set $S$ , since $r_0 = \bar{r_1}, \theta_0 = \bar{\theta_1}$ . What I don't see is how what they said is the area of that region? Speaking in polar terms the region defined should actually be a rectangle, so I guess the area should be just $\triangle r \triangle \theta$ , but apparently I'm missing something. After this statement it goes on to show the formula I've mention above and it's all clear from here on, it's just this one part that I can't understand. Any ideas? Thanks.","I'm studying double integrals (the theory part, not the problem solving). We're  trying to derive the formula for the polar substitution in a double integral: where maps First I want to say that it is explicitly stated in the textbook that we're not going to prove the general formula for substitutions in the integral (the one that uses the Jacobian) but rather, just for now, the formula for the polar substitution. It starts by defining a set and dividing this set using lines/curves and where: and This part is clear, it's pretty similar to dividing a rectangular region when defining a double integral itself. So an illustration would look something like this (sorry for the bad picture, couldn't find a way to do it in Desmos): The following sentence really confuses me because it's stated as a given (without any explanation whatsoever): If we look at the region we notice that the area of that region is The region that we're talking about is, as far as I can see, the 'bottom-right' sub-region of the set , since . What I don't see is how what they said is the area of that region? Speaking in polar terms the region defined should actually be a rectangle, so I guess the area should be just , but apparently I'm missing something. After this statement it goes on to show the formula I've mention above and it's all clear from here on, it's just this one part that I can't understand. Any ideas? Thanks.","\iint_{D}f(x, y)dxdy = \iint_{D_1}f(r\cos\theta, r\sin\theta)rdrd\theta x = r\cos\theta, y = r\sin\theta D_1 \to D S := \big\{(r, \theta)|\bar{r_1} \leq r\leq \bar{r_2}, \bar{\theta_1} \leq \theta \leq \bar{\theta_2}\big\} r = r_j \theta = \theta_j r_j := \bar{r_1} + j\triangle r, \text{where } \triangle r := \frac{\bar{r_2} - \bar{r_1}}{n}
\\\theta_j:= \bar{\theta_1}+j\triangle \theta, \text{where } \triangle\theta := \frac{\bar{\theta_2} - \bar{\theta_1}}{n}
 j=0, 1, ..., n \big\{ (r, \theta)| r_0 \leq r \leq r_0 + \triangle r, \theta_0 \leq \theta \leq \theta_0 + \triangle \theta  \big\} \frac{1}{2}\triangle \theta ((r_0 + \triangle r)^2 - r_0^2) = (r_0 + \frac{\triangle r}{2})\triangle r \triangle \theta S r_0 = \bar{r_1}, \theta_0 = \bar{\theta_1} \triangle r \triangle \theta","['integration', 'multivariable-calculus', 'polar-coordinates']"
6,"Maximizing the line integral $\int \mathbf{F}\cdot d\mathbf{r}$ for the vector field $\mathbf{F}=\langle x^2 y+y^3-y,3x+2y^2 x+e^y\rangle$.",Maximizing the line integral  for the vector field .,"\int \mathbf{F}\cdot d\mathbf{r} \mathbf{F}=\langle x^2 y+y^3-y,3x+2y^2 x+e^y\rangle","Consider the vector field $\mathbf{F}=\langle x^2 y+y^3-y,3x+2y^2 x+e^y\rangle$ . For which closed non-self-intersecting curve in the plane does the line integral over this vector field have the maximal value? What is this value? Let $\mathbf{F}=\langle xy,y^2\rangle$ , let $C$ be the unit circle centered at the origin, and consider $\int_C \mathbf{F}\cdot d\mathbf{r}$ . Which portions of $C$ contribute positively to this integral? Calculate the integral two ways, first directly and then by using Green's theorem. For the first question. I need to maximize the line integral  how do I do that? How can I apply Maxima minima concept to this?","Consider the vector field . For which closed non-self-intersecting curve in the plane does the line integral over this vector field have the maximal value? What is this value? Let , let be the unit circle centered at the origin, and consider . Which portions of contribute positively to this integral? Calculate the integral two ways, first directly and then by using Green's theorem. For the first question. I need to maximize the line integral  how do I do that? How can I apply Maxima minima concept to this?","\mathbf{F}=\langle x^2 y+y^3-y,3x+2y^2 x+e^y\rangle \mathbf{F}=\langle xy,y^2\rangle C \int_C \mathbf{F}\cdot d\mathbf{r} C","['calculus', 'integration', 'multivariable-calculus']"
7,"For the function $f(x,y,z)= xyz$ how close to the point $(0,0,0)$ should one take the point $(x,y,z)$ in order to make $|f(x,y,z)-f(0,0,0)|<0.008$?",For the function  how close to the point  should one take the point  in order to make ?,"f(x,y,z)= xyz (0,0,0) (x,y,z) |f(x,y,z)-f(0,0,0)|<0.008","For the function $$f(x,y,z)= xyz$$ how close to the point $(0,0,0)$ should one take the point $(x,y,z)$ in order to make $|f(x,y,z)-f(0,0,0)|<0.008$ ? here is my solution: after substitution in the absolute value inequality we get $|xyz|<0.008$ $\Rightarrow$ $\sqrt {x^{2}y^{2}z^{2}}<0.008$ but for $x,y,z <1$ $\sqrt {x^{2}y^{2}z^{2}} < \sqrt{x^{2}+y^{2}+z^{2}}$ And If we choosed the distance to be less than $\delta = 0.008$ the condition will hold. the solution of the book is $\delta = 0.2\sqrt{3}$ my questions are : Is my solution correct? how to obtain the solution of the book?",For the function how close to the point should one take the point in order to make ? here is my solution: after substitution in the absolute value inequality we get but for And If we choosed the distance to be less than the condition will hold. the solution of the book is my questions are : Is my solution correct? how to obtain the solution of the book?,"f(x,y,z)= xyz (0,0,0) (x,y,z) |f(x,y,z)-f(0,0,0)|<0.008 |xyz|<0.008 \Rightarrow \sqrt {x^{2}y^{2}z^{2}}<0.008 x,y,z <1 \sqrt {x^{2}y^{2}z^{2}} < \sqrt{x^{2}+y^{2}+z^{2}} \delta = 0.008 \delta = 0.2\sqrt{3}","['limits', 'multivariable-calculus']"
8,Visualizing Lagrange multipliers,Visualizing Lagrange multipliers,,"Sorry if this seems like a very basic question but I am having trouble visualizing Lagrange multipliers. Particularly the equation: $ \nabla f = \lambda * \nabla g $ f = function to maximise. g = constraint. I don't understand why equating the gradients in such a way produces the extremum. I watched a Khan Academy video and the explanation was as follows: Plotting Contours of f and g My question is: Why does the extremum occur only where the contours of f touch the constraint at one point? Why can they not occur where there are multiple points? For example: Ex: Contours of f meeting g at two points Also, why is it that equating the gradients in such a way produces the point where they touch at only one point? My understanding is $ \nabla $ f is the gradient vector of f and $ \nabla $ g is the gradient vector for g. It seems to be there may be infinitely many points which may satisfy the equation but are not necessarily the extremum. Kindly help me visualise what is going on here. Thanks in advance. EDIT: My understanding of gradients","Sorry if this seems like a very basic question but I am having trouble visualizing Lagrange multipliers. Particularly the equation: f = function to maximise. g = constraint. I don't understand why equating the gradients in such a way produces the extremum. I watched a Khan Academy video and the explanation was as follows: Plotting Contours of f and g My question is: Why does the extremum occur only where the contours of f touch the constraint at one point? Why can they not occur where there are multiple points? For example: Ex: Contours of f meeting g at two points Also, why is it that equating the gradients in such a way produces the point where they touch at only one point? My understanding is f is the gradient vector of f and g is the gradient vector for g. It seems to be there may be infinitely many points which may satisfy the equation but are not necessarily the extremum. Kindly help me visualise what is going on here. Thanks in advance. EDIT: My understanding of gradients", \nabla f = \lambda * \nabla g   \nabla   \nabla ,"['multivariable-calculus', 'lagrange-multiplier']"
9,"Multivariable limit $\lim_{(x,y,z) \to (0,0,0)} \frac{2x^2+3y^2+z^2}{x^2+y^2+z^2}$",Multivariable limit,"\lim_{(x,y,z) \to (0,0,0)} \frac{2x^2+3y^2+z^2}{x^2+y^2+z^2}","I am studying multivariable limits from the text Vector Calculus by S.J. Colley. I practiced computing limiting values of the functions of two-variables $f(x,y)$ . Here's a problem involving a function of three variables. Compute the limit $$\lim_{(x,y,z) \to (0,0,0)} \frac{2x^2+3y^2+z^2}{x^2+y^2+z^2}$$ I thought to myself, this could be for example, the scalar potential of a field in space. So, it's fun to see if the function has a limit as we get closer and closer to the origin. I would like to ask, if my proof is mathematically correct. Solution. From solid analytical geometry, a straight line in $\mathbb{R^3}$ is: $\frac{x-x_0}{l}=\frac{y-y_0}{m}=\frac{z-z_0}{n}=r$ The straight line passing through the origin and having direction numbers $l,m,n$ is: $\frac{x}{l}=\frac{y}{m}=\frac{z}{n}=r$ So, along this straight line with direction numbers $l,m,n$ , our function takes on values $f(x,y,z)=f(r)=\frac{2r^2l^2+3r^2m^2+r^2n^2}{r^2l^2+r^2m^2+r^2n^2}=\frac{2l^2+3m^2+n^2}{l^2+m^2+n^2}$ Along the line $x=y,z=0$ , $f(x,y,z)=\frac{5}{2}$ Along the line $y=z,x=0$ , $f(x,y,z)=\frac{4}{2}=2$ Along the line $x=z,y=0$ , $f(x,y,z)=\frac{3}{2}$ Along the line $x=y=z$ , $f(x,y,z)=\frac{6}{3}=2$ Hence, $\lim_{(x,y,z) \to (0,0,0)\\\text{ along }x=y,z=0}\frac{2x^2+3y^2+z^2}{x^2+y^2+z^2}=\frac{5}{2}$ $\lim_{(x,y,z) \to (0,0,0)\\\text{ along }y=z,x=0}\frac{2x^2+3y^2+z^2}{x^2+y^2+z^2}=2$ $\lim_{(x,y,z) \to (0,0,0)\\\text{ along }x=z,y=0}\frac{2x^2+3y^2+z^2}{x^2+y^2+z^2}=\frac{3}{2}$ $\lim_{(x,y,z) \to (0,0,0)\\\text{ along }x=y=z}\frac{2x^2+3y^2+z^2}{x^2+y^2+z^2}=3$ Thus, the limit of this function does not exist as $(x,y,z) \to (0,0,0)$ .","I am studying multivariable limits from the text Vector Calculus by S.J. Colley. I practiced computing limiting values of the functions of two-variables . Here's a problem involving a function of three variables. Compute the limit I thought to myself, this could be for example, the scalar potential of a field in space. So, it's fun to see if the function has a limit as we get closer and closer to the origin. I would like to ask, if my proof is mathematically correct. Solution. From solid analytical geometry, a straight line in is: The straight line passing through the origin and having direction numbers is: So, along this straight line with direction numbers , our function takes on values Along the line , Along the line , Along the line , Along the line , Hence, Thus, the limit of this function does not exist as .","f(x,y) \lim_{(x,y,z) \to (0,0,0)} \frac{2x^2+3y^2+z^2}{x^2+y^2+z^2} \mathbb{R^3} \frac{x-x_0}{l}=\frac{y-y_0}{m}=\frac{z-z_0}{n}=r l,m,n \frac{x}{l}=\frac{y}{m}=\frac{z}{n}=r l,m,n f(x,y,z)=f(r)=\frac{2r^2l^2+3r^2m^2+r^2n^2}{r^2l^2+r^2m^2+r^2n^2}=\frac{2l^2+3m^2+n^2}{l^2+m^2+n^2} x=y,z=0 f(x,y,z)=\frac{5}{2} y=z,x=0 f(x,y,z)=\frac{4}{2}=2 x=z,y=0 f(x,y,z)=\frac{3}{2} x=y=z f(x,y,z)=\frac{6}{3}=2 \lim_{(x,y,z) \to (0,0,0)\\\text{ along }x=y,z=0}\frac{2x^2+3y^2+z^2}{x^2+y^2+z^2}=\frac{5}{2} \lim_{(x,y,z) \to (0,0,0)\\\text{ along }y=z,x=0}\frac{2x^2+3y^2+z^2}{x^2+y^2+z^2}=2 \lim_{(x,y,z) \to (0,0,0)\\\text{ along }x=z,y=0}\frac{2x^2+3y^2+z^2}{x^2+y^2+z^2}=\frac{3}{2} \lim_{(x,y,z) \to (0,0,0)\\\text{ along }x=y=z}\frac{2x^2+3y^2+z^2}{x^2+y^2+z^2}=3 (x,y,z) \to (0,0,0)","['multivariable-calculus', 'proof-verification']"
10,Total derivative in high dimension,Total derivative in high dimension,,"I recently came to know the process of differentiating in high dimensions and also the chain rule in this situation. Consider a function $g:\mathbb R^2\rightarrow\mathbb R$ as $(x,y)\mapsto x^2+y^2-1 $ . Now, we observe that $g(x,y)=g(-x,-y)$ . We totally differentiate both sides with respect to the vector $(x,y)$ . $LHS$ becomes $Dg_{(x,y)}=(2x,2y)$ and for $RHS$ , we define $h:\mathbb R^2\rightarrow\mathbb R^2$ as $(x,y)\mapsto (-x,-y)$ . $RHS$ is $g\circ h(x,y)$ , so we apply chain rule and get $$Dg\circ h_{(x,y)}=Dg_{h(x,y)} \cdot Dh_{(x,y)}$$ $$=(-2x,-2y)\times\left(\begin{matrix} -1 & 0 \\ 0 & -1\end{matrix}\right)$$ $$=(2x,2y)$$ So, we evaluate them to be same, as expected. But the doubt is : For the case of real numbers, we have $x\mapsto x^2$ , this function also satisfies the same equality but the linear map for derivative at $x=a$ is $x\mapsto 2ax$ and that for $x=-a$ is $x\mapsto -2ax$ ? So, why aren't they same in this situation? Have I done some wrong calculation for high dimensional case? If yes, then can't we differentiate both sides in high dimensional case? Please help","I recently came to know the process of differentiating in high dimensions and also the chain rule in this situation. Consider a function as . Now, we observe that . We totally differentiate both sides with respect to the vector . becomes and for , we define as . is , so we apply chain rule and get So, we evaluate them to be same, as expected. But the doubt is : For the case of real numbers, we have , this function also satisfies the same equality but the linear map for derivative at is and that for is ? So, why aren't they same in this situation? Have I done some wrong calculation for high dimensional case? If yes, then can't we differentiate both sides in high dimensional case? Please help","g:\mathbb R^2\rightarrow\mathbb R (x,y)\mapsto x^2+y^2-1  g(x,y)=g(-x,-y) (x,y) LHS Dg_{(x,y)}=(2x,2y) RHS h:\mathbb R^2\rightarrow\mathbb R^2 (x,y)\mapsto (-x,-y) RHS g\circ h(x,y) Dg\circ h_{(x,y)}=Dg_{h(x,y)} \cdot Dh_{(x,y)} =(-2x,-2y)\times\left(\begin{matrix} -1 & 0 \\ 0 & -1\end{matrix}\right) =(2x,2y) x\mapsto x^2 x=a x\mapsto 2ax x=-a x\mapsto -2ax","['multivariable-calculus', 'derivatives']"
11,Qual problem on the intersection of two surfaces,Qual problem on the intersection of two surfaces,,"This is a qual problem that has been giving me some difficulty: Let $$  \begin{align*}      f(x, y, z) &= x^2 + y^4 + z^6 - 3 \\      g(x, y, z) &= x + y + z,  \end{align*}  $$ and $S = \{(x, y, z) \in \mathbb{R}^3\ \mid f(\vec{x}) = g(\vec{x}) = 0\}$ .   Show that for every $\vec{v} \in S$ there exists a neighborhood $U$ of $\vec{v}$ such that: There exists a differentiable, injective map $\gamma \colon (-\epsilon, \epsilon) \to U$ . $\gamma'$ does not vanish on $(-\epsilon, \epsilon)$ . The image of $\gamma$ is $U \cap S$ . This seems like a good problem for the implicit function theorem. I am fairly certain that the problem is solved if the Jacobian matrix $$ J = \begin{bmatrix}    2x & 4y^3 & 6z^5 \\     1 &  1   & 1 \end{bmatrix} $$ has rank $2$ for all points in $S$ . (In this case, we could choose two variables to solve in terms of the remaining third with the implicit function theorem.) However, the rank of $J$ does not seem easy to compute in $S$ (or anywhere else). The rank of $J$ is $2$ precisely if a solution $(x, y, z) \in S$ cannot satisfy $2x = 4y^3 = 6z^5$ . For this to work, we would want the equations \begin{align*}     2x = 4y^3 &= 6z^5 \\     x^2 + y^4 + z^6 &= 3 \\     x + y + z &= 0 \end{align*} to have no solutions. Eliminating a variable with $x + y + z = 0$ doesn't seem immensely helpful. For example, substituting $z = -x - y$ gives $x^2 + y^4 + (x + y)^6 = 3$ , which doesn't have any obviously nice properties. Is there a better way to solve this question? If not, how should I proceed with the implicit function theorem?","This is a qual problem that has been giving me some difficulty: Let and .   Show that for every there exists a neighborhood of such that: There exists a differentiable, injective map . does not vanish on . The image of is . This seems like a good problem for the implicit function theorem. I am fairly certain that the problem is solved if the Jacobian matrix has rank for all points in . (In this case, we could choose two variables to solve in terms of the remaining third with the implicit function theorem.) However, the rank of does not seem easy to compute in (or anywhere else). The rank of is precisely if a solution cannot satisfy . For this to work, we would want the equations to have no solutions. Eliminating a variable with doesn't seem immensely helpful. For example, substituting gives , which doesn't have any obviously nice properties. Is there a better way to solve this question? If not, how should I proceed with the implicit function theorem?","
 \begin{align*}
     f(x, y, z) &= x^2 + y^4 + z^6 - 3 \\
     g(x, y, z) &= x + y + z,
 \end{align*}
  S = \{(x, y, z) \in \mathbb{R}^3\ \mid f(\vec{x}) = g(\vec{x}) = 0\} \vec{v} \in S U \vec{v} \gamma \colon (-\epsilon, \epsilon) \to U \gamma' (-\epsilon, \epsilon) \gamma U \cap S 
J =
\begin{bmatrix}
   2x & 4y^3 & 6z^5 \\
    1 &  1   & 1
\end{bmatrix}
 2 S J S J 2 (x, y, z) \in S 2x = 4y^3 = 6z^5 \begin{align*}
    2x = 4y^3 &= 6z^5 \\
    x^2 + y^4 + z^6 &= 3 \\
    x + y + z &= 0
\end{align*} x + y + z = 0 z = -x - y x^2 + y^4 + (x + y)^6 = 3","['real-analysis', 'multivariable-calculus']"
12,Solution to variational problem,Solution to variational problem,,"I am struggling to see how to get to the solution of the equation below. The problem equation and solution come from here (see eqn 3 and eqn4) . They state that this is a variational problem, $$ p^{*}(a | w)=\underset{p(a | w)}{\arg \max } \sum_{a} \left[ p(a | w) U(w, a)-\frac{1}{\beta}  p(a | w) \log \frac{p(a | w)}{p_{0}(a)} \right] $$ given that $0<p(a | w)<1$ and $\sum_{a} p(a | w) =1$ The answer is: $$ p^{*}(a | w)=\frac{1}{Z(w)} p_{0}(a) e^{\beta U(w, a)} $$ where $Z$ is the normalisation constant $$ Z(w)=\Sigma_{a} p_{0}(a) e^{\beta U(w, a)} $$ Edit: Added the constraint that p lies between 0 and 1 (as it's a probability)","I am struggling to see how to get to the solution of the equation below. The problem equation and solution come from here (see eqn 3 and eqn4) . They state that this is a variational problem, given that and The answer is: where is the normalisation constant Edit: Added the constraint that p lies between 0 and 1 (as it's a probability)","
p^{*}(a | w)=\underset{p(a | w)}{\arg \max } \sum_{a} \left[ p(a | w) U(w, a)-\frac{1}{\beta}  p(a | w) \log \frac{p(a | w)}{p_{0}(a)} \right]
 0<p(a | w)<1 \sum_{a} p(a | w) =1 
p^{*}(a | w)=\frac{1}{Z(w)} p_{0}(a) e^{\beta U(w, a)}
 Z 
Z(w)=\Sigma_{a} p_{0}(a) e^{\beta U(w, a)}
","['multivariable-calculus', 'optimization', 'lagrange-multiplier', 'karush-kuhn-tucker']"
13,What is the surface area of given surface .,What is the surface area of given surface .,,"Compute the area of that part of plane $x+y+z=2a$ which lies in the first octant and is bounded by the cylinder $x^2+ y^2 =a^2$ Now $z = 2a -x -y$ , $\dfrac{\partial{z}}{\partial{x}} = -1$ $\dfrac{\partial{z}}{\partial{y}} = -1$ , $ds  = \sqrt{3}dxdy$ Area is given by : $\int\int_{A} \sqrt{3}dxdy$ where A is the area of $x^2+ y^2 =a^2$ in first quadrant. so Surface area  = $\dfrac{\sqrt{3}\pi a^2}{4}$ .  However the answer in my book is $\dfrac{3\pi a^2}{4}$ . can anyone tell me what is wrong with my solution ? and why is my answer is incorrect ?","Compute the area of that part of plane which lies in the first octant and is bounded by the cylinder Now , , Area is given by : where A is the area of in first quadrant. so Surface area  = .  However the answer in my book is . can anyone tell me what is wrong with my solution ? and why is my answer is incorrect ?",x+y+z=2a x^2+ y^2 =a^2 z = 2a -x -y \dfrac{\partial{z}}{\partial{x}} = -1 \dfrac{\partial{z}}{\partial{y}} = -1 ds  = \sqrt{3}dxdy \int\int_{A} \sqrt{3}dxdy x^2+ y^2 =a^2 \dfrac{\sqrt{3}\pi a^2}{4} \dfrac{3\pi a^2}{4},"['integration', 'multivariable-calculus']"
14,Sums preserve the property that all stationary points are global minima?,Sums preserve the property that all stationary points are global minima?,,"Let $f: \mathbb R^n \rightarrow \mathbb R$ and $g: \mathbb R^n \rightarrow \mathbb R$ be continuously differentiable functions whose sublevel sets are compact. Suppose that $\nabla f(x) = 0$ for some $x \in \mathbb R^n$ if and only if $x$ is a global minimum of $f$ . Suppose the same for $g$ . Then, is it true that $\nabla(f + g)(x) = 0$ for some $x$ if and only if $x$ is a global minimum of $f + g$ ? It seems to be true if both $f$ and $g$ are convex. What if one of them is not convex?","Let and be continuously differentiable functions whose sublevel sets are compact. Suppose that for some if and only if is a global minimum of . Suppose the same for . Then, is it true that for some if and only if is a global minimum of ? It seems to be true if both and are convex. What if one of them is not convex?",f: \mathbb R^n \rightarrow \mathbb R g: \mathbb R^n \rightarrow \mathbb R \nabla f(x) = 0 x \in \mathbb R^n x f g \nabla(f + g)(x) = 0 x x f + g f g,"['real-analysis', 'multivariable-calculus', 'optimization']"
15,Cauchy Repeated Integral Formula with Root upper-bounds?,Cauchy Repeated Integral Formula with Root upper-bounds?,,"Cauchy's formula for repeated integration states that for any continuous function on $[0,1]$ we have that the $n$ -fold integral can be represented by a single integral as follows $$ \int_a^x \int_a^{\sigma_1} \cdots \int_a^{\sigma_{n-1}} f(\sigma_{n}) \, \mathrm{d}\sigma_{n} \cdots \, \mathrm{d}\sigma_2 \, \mathrm{d}\sigma_1 = \frac{1}{(n-1)!} \int_a^x\left(x-t\right)^{n-1} f(t)\,\mathrm{d}t. $$ Following this question , I'm wondering if there is a ""known"" analogue of the formula for the following variant $$ \int_a^{\sqrt{x}} \int_a^{\sqrt{\sigma_1}} \cdots \int_a^{\sqrt{\sigma_{n-1}}} f(\sigma_{n}) \, \mathrm{d}\sigma_{n} \cdots \, \mathrm{d}\sigma_2 \, \mathrm{d}\sigma_1 = \int_a^x k(t,x,a) f(t)dt, $$ for some locally-integrable function $k(t,x,a)\in L_{loc}^1(\mathbb{R}^3)$ ?, where $\sigma_1\leq ...\leq \sigma_{n-1}\leq x$ .","Cauchy's formula for repeated integration states that for any continuous function on we have that the -fold integral can be represented by a single integral as follows Following this question , I'm wondering if there is a ""known"" analogue of the formula for the following variant for some locally-integrable function ?, where .","[0,1] n 
\int_a^x \int_a^{\sigma_1} \cdots \int_a^{\sigma_{n-1}} f(\sigma_{n}) \, \mathrm{d}\sigma_{n} \cdots \, \mathrm{d}\sigma_2 \, \mathrm{d}\sigma_1
=
\frac{1}{(n-1)!} \int_a^x\left(x-t\right)^{n-1} f(t)\,\mathrm{d}t.
 
\int_a^{\sqrt{x}} \int_a^{\sqrt{\sigma_1}} \cdots \int_a^{\sqrt{\sigma_{n-1}}} f(\sigma_{n}) \, \mathrm{d}\sigma_{n} \cdots \, \mathrm{d}\sigma_2 \, \mathrm{d}\sigma_1
= \int_a^x k(t,x,a) f(t)dt,
 k(t,x,a)\in L_{loc}^1(\mathbb{R}^3) \sigma_1\leq ...\leq \sigma_{n-1}\leq x","['real-analysis', 'calculus']"
16,Is the gradient vector tangent to the surface?,Is the gradient vector tangent to the surface?,,"I understand the reason why the gradient vector is always orthogonal to the level sets of f, but I just cannot find any notes saying the gradient vector is tangent to the surface. But it seems to be reasonable if I imagine that when climbing the hill, the steepest path is actually tangent to the hill!!! So I am now a bit confusing...","I understand the reason why the gradient vector is always orthogonal to the level sets of f, but I just cannot find any notes saying the gradient vector is tangent to the surface. But it seems to be reasonable if I imagine that when climbing the hill, the steepest path is actually tangent to the hill!!! So I am now a bit confusing...",,"['calculus', 'multivariable-calculus', 'vectors', 'vector-analysis', 'tangent-line']"
17,"$\lim\limits_{h\to(0,0)}\frac{\sqrt[3]{h_1h_2+h_2+h_1+1}-1-\frac{h_1}{3}-\frac{h_2}{3}}{\sqrt{h_1^2+h_2^2}}$",,"\lim\limits_{h\to(0,0)}\frac{\sqrt[3]{h_1h_2+h_2+h_1+1}-1-\frac{h_1}{3}-\frac{h_2}{3}}{\sqrt{h_1^2+h_2^2}}","I am trying to show that a function has a total derivative at $(1,1)$ but I got stuck trying to show $$\lim\limits_{h\to(0,0)}\frac{\sqrt[3]{h_1h_2+h_2+h_1+1}-1-\frac{h_1}{3}-\frac{h_2}{3}}{\sqrt{h_1^2+h_2^2}}=0$$ Where $h=(h_1,h_2)\in\mathbb{R}^2$ . Since $h$ approaches $(0,0)$ $h_1$ and $h_2$ have to approach $0$ aswell but I am relatively new to multivariable limits so I need some help.",I am trying to show that a function has a total derivative at but I got stuck trying to show Where . Since approaches and have to approach aswell but I am relatively new to multivariable limits so I need some help.,"(1,1) \lim\limits_{h\to(0,0)}\frac{\sqrt[3]{h_1h_2+h_2+h_1+1}-1-\frac{h_1}{3}-\frac{h_2}{3}}{\sqrt{h_1^2+h_2^2}}=0 h=(h_1,h_2)\in\mathbb{R}^2 h (0,0) h_1 h_2 0",['multivariable-calculus']
18,transform differential equation using variable substitution,transform differential equation using variable substitution,,"task is to transform the differential equation $$x\frac{\partial^2 f}{\partial x^2}+\frac{\partial^2 f}{\partial x\partial y}+\frac{\partial f}{\partial x}=xe^{-2y}$$ using the variabel substitution $$u=xe^{-y}, v=y.$$ I have started with the chain rule for the partial derivatives of the first order, I get $$\frac{\partial f}{\partial x}=e^{-y}\cdot\frac{\partial f}{\partial u}$$ $$\frac{\partial f}{\partial y}= -xe^{-y}\cdot\frac{\partial f}{\partial u}+\frac{\partial f}{\partial v}$$ Then when trying to transform $\frac{\partial^2 f}{\partial x^2}$ I get $$\frac{\partial^2 f}{\partial x^2}=\frac{\partial }{\partial x}(\frac{\partial f}{\partial x})=\frac{\partial }{\partial x}(e^{-y}\frac{\partial f}{\partial u})$$ But now I am stuck. I don't think I can use the product rule for derivatives here since I want to take the derivative with respect to x and $e^{-y}$ is not a function of x. EDIT: I have now manage to transform all the partial derivatives that are included in the equation, I get $$\frac{\partial^2 f}{\partial x^2}=\frac{\partial^2 f}{\partial u^2}\cdot e^{-2y}$$ $$\frac{\partial^2 f}{\partial x \partial y}=\frac{\partial }{\partial x}(-xe^{-y}\cdot \frac{\partial f}{\partial u}) + \frac{\partial }{\partial x}(\frac{\partial f}{\partial v}) = -e^{-y}\cdot \frac{\partial f}{\partial u}-xe^{-2y}\cdot \frac{\partial^2 f}{\partial u^2}+\frac{\partial^2 }{\partial u \partial v}$$ If i insert that to the equation I get $$xe^{-2y}\frac{\partial^2 f}{\partial u^2}-e^{-y}\frac{\partial f}{\partial u}-xe^{-2y}\cdot \frac{\partial^2 f }{\partial u^2}+\frac{\partial^2 f}{\partial u \partial v}+e^{-y}\frac{\partial f}{\partial u}=xe^{-2y}$$ which simplifies to $$\frac{\partial^2 }{\partial u \partial v} = xe^{-2y}$$ My textbook has written the answer as $$\frac{\partial^2 }{\partial u \partial v} = u$$ Did I do something wrong along the way or is my answer correct?","task is to transform the differential equation using the variabel substitution I have started with the chain rule for the partial derivatives of the first order, I get Then when trying to transform I get But now I am stuck. I don't think I can use the product rule for derivatives here since I want to take the derivative with respect to x and is not a function of x. EDIT: I have now manage to transform all the partial derivatives that are included in the equation, I get If i insert that to the equation I get which simplifies to My textbook has written the answer as Did I do something wrong along the way or is my answer correct?","x\frac{\partial^2 f}{\partial x^2}+\frac{\partial^2 f}{\partial x\partial y}+\frac{\partial f}{\partial x}=xe^{-2y} u=xe^{-y}, v=y. \frac{\partial f}{\partial x}=e^{-y}\cdot\frac{\partial f}{\partial u} \frac{\partial f}{\partial y}= -xe^{-y}\cdot\frac{\partial f}{\partial u}+\frac{\partial f}{\partial v} \frac{\partial^2 f}{\partial x^2} \frac{\partial^2 f}{\partial x^2}=\frac{\partial }{\partial x}(\frac{\partial f}{\partial x})=\frac{\partial }{\partial x}(e^{-y}\frac{\partial f}{\partial u}) e^{-y} \frac{\partial^2 f}{\partial x^2}=\frac{\partial^2 f}{\partial u^2}\cdot e^{-2y} \frac{\partial^2 f}{\partial x \partial y}=\frac{\partial }{\partial x}(-xe^{-y}\cdot \frac{\partial f}{\partial u}) + \frac{\partial }{\partial x}(\frac{\partial f}{\partial v}) = -e^{-y}\cdot \frac{\partial f}{\partial u}-xe^{-2y}\cdot \frac{\partial^2 f}{\partial u^2}+\frac{\partial^2 }{\partial u \partial v} xe^{-2y}\frac{\partial^2 f}{\partial u^2}-e^{-y}\frac{\partial f}{\partial u}-xe^{-2y}\cdot \frac{\partial^2 f }{\partial u^2}+\frac{\partial^2 f}{\partial u \partial v}+e^{-y}\frac{\partial f}{\partial u}=xe^{-2y} \frac{\partial^2 }{\partial u \partial v} = xe^{-2y} \frac{\partial^2 }{\partial u \partial v} = u","['multivariable-calculus', 'partial-differential-equations', 'partial-derivative']"
19,"If $T(X) = AX $, $A \subset \mathbb{R^{m \times 1}}, \forall X \in \mathbb{R^{n}},$ Then $T$ is continuous.","If ,  Then  is continuous.","T(X) = AX  A \subset \mathbb{R^{m \times 1}}, \forall X \in \mathbb{R^{n}}, T","If $T(X) = AX $ , $A \subset \mathbb{R^{m \times 1}}, \forall X \in \mathbb{R^{n}},$ Then $T$ is continuous. My thoughts: I have proved (or I was helped to prove ) it using the facts that every linear operator is continuous and that every linear operator is bounded in a finite dimensional space. Now, I want to prove this statement using more elementary tools, like $\epsilon - \delta$ definition, could anyone help me in doing this please?","If , Then is continuous. My thoughts: I have proved (or I was helped to prove ) it using the facts that every linear operator is continuous and that every linear operator is bounded in a finite dimensional space. Now, I want to prove this statement using more elementary tools, like definition, could anyone help me in doing this please?","T(X) = AX  A \subset \mathbb{R^{m \times 1}}, \forall X \in \mathbb{R^{n}}, T \epsilon - \delta","['calculus', 'linear-algebra', 'matrices', 'multivariable-calculus', 'continuity']"
20,How to solve this multivariable exponential equation?,How to solve this multivariable exponential equation?,,I searched if this was asked before but couldn't find a solution. I have this equation $y^{70} = x + 500 $ $y^{50} = x + 1 $ Is it possible to solve this equation? The only thing I could do is to bring it into this form and then cross-multiply which didn't yield many results. $y^{20} = \frac{x+500}{x+1} $,I searched if this was asked before but couldn't find a solution. I have this equation Is it possible to solve this equation? The only thing I could do is to bring it into this form and then cross-multiply which didn't yield many results.,y^{70} = x + 500  y^{50} = x + 1  y^{20} = \frac{x+500}{x+1} ,"['multivariable-calculus', 'functions', 'logarithms', 'exponential-function']"
21,"Compute $\lim_{(x,y) \to (0,0)} \frac{x^2 - 2\cos(y) + 2}{y^2 - 2\cos(x) + 2}$",Compute,"\lim_{(x,y) \to (0,0)} \frac{x^2 - 2\cos(y) + 2}{y^2 - 2\cos(x) + 2}","Does the limit $$\lim_{(x,y) \to (0,0)} \left( \frac{x^2 - 2\cos(y) + 2}{y^2 - 2\cos(x) + 2} \right) $$ exist? I think it does and it's equal to $1$ , but I don't know how to prove it. I tried to use Taylor expansion of $\cos(x)$ and $\cos(y)$ , but it doesn't help me to compute the limit.","Does the limit exist? I think it does and it's equal to , but I don't know how to prove it. I tried to use Taylor expansion of and , but it doesn't help me to compute the limit.","\lim_{(x,y) \to (0,0)} \left( \frac{x^2 - 2\cos(y) + 2}{y^2 - 2\cos(x) + 2} \right)  1 \cos(x) \cos(y)","['limits', 'multivariable-calculus', 'taylor-expansion']"
22,How is the Jacobian derived using this method?.,How is the Jacobian derived using this method?.,,"My course notes say the following: $$\left[\begin{array}{l}{d x} \\ {d y}\end{array}\right]=\left[\begin{array}{ll}{x_{u}} & {x_{v}} \\ {y_{u}} & {y_{v}}\end{array}\right]\left[\begin{array}{l}{d u} \\ {d v}\end{array}\right]\\$$ $$d A=\left|\begin{array}{ll}{x_{u}} & {x_{v}} \\ {y_{u}} & {y_{v}}\end{array}\right| d u d v\\$$ Apparently the second line follows from the first, but expanding out the algebra of the two equations from the first line and multiplying both equations together (as $dA = dx dy$ ) I don't get the same as calculating the determinant in the second line and multiplying by $dudv$ . So how does the second line follow from the first?","My course notes say the following: Apparently the second line follows from the first, but expanding out the algebra of the two equations from the first line and multiplying both equations together (as ) I don't get the same as calculating the determinant in the second line and multiplying by . So how does the second line follow from the first?",\left[\begin{array}{l}{d x} \\ {d y}\end{array}\right]=\left[\begin{array}{ll}{x_{u}} & {x_{v}} \\ {y_{u}} & {y_{v}}\end{array}\right]\left[\begin{array}{l}{d u} \\ {d v}\end{array}\right]\\ d A=\left|\begin{array}{ll}{x_{u}} & {x_{v}} \\ {y_{u}} & {y_{v}}\end{array}\right| d u d v\\ dA = dx dy dudv,"['multivariable-calculus', 'jacobian']"
23,What happened to the limit of integration when using Tonelli's Theorem?,What happened to the limit of integration when using Tonelli's Theorem?,,"I recently had encounter the integral $$\int_0^{\infty}{\Bigg[\int_0^x{1}dy\Bigg]f(x)dx}.$$ My question is why is the integral above equal to $$\int_0^{\infty}1{\Bigg[\int_y^{\infty}{f(x)}dx\Bigg]dy}?$$ In particular, I am interested on how to get the limit of integration with $[y,\infty]$ for $dy$ ? My understanding is that when using Tonelli's Theorem, the integrals are interchangeable. That is, I must have $$\int_0^{\infty}{\Bigg[\int_0^x{1}dy\Bigg]f(x)dx}=\int_0^x{\Bigg[\int_0^{\infty}{f(x)dx}\Bigg]dy}$$ Can you please help me realize where did I go wrong and how to get the correct limit of integration? Thanks in advance.","I recently had encounter the integral My question is why is the integral above equal to In particular, I am interested on how to get the limit of integration with for ? My understanding is that when using Tonelli's Theorem, the integrals are interchangeable. That is, I must have Can you please help me realize where did I go wrong and how to get the correct limit of integration? Thanks in advance.","\int_0^{\infty}{\Bigg[\int_0^x{1}dy\Bigg]f(x)dx}. \int_0^{\infty}1{\Bigg[\int_y^{\infty}{f(x)}dx\Bigg]dy}? [y,\infty] dy \int_0^{\infty}{\Bigg[\int_0^x{1}dy\Bigg]f(x)dx}=\int_0^x{\Bigg[\int_0^{\infty}{f(x)dx}\Bigg]dy}","['real-analysis', 'analysis', 'probability-theory', 'multivariable-calculus']"
24,"Find $\lim_{(x,y)\to(0,0)} \frac{xy^2}{(x^2+y^4)\sqrt{x^2+y^2}}$",Find,"\lim_{(x,y)\to(0,0)} \frac{xy^2}{(x^2+y^4)\sqrt{x^2+y^2}}","I'm trying to prove that $$\lim_{(x,y)\to(0,0)} \frac{xy^2}{(x^2+y^4)\sqrt{x^2+y^2}}$$ doesn't exist. I've tried different paths like $(x,y)=(t^2,t)$ , and everything seems to work fine but the term $\sqrt{x^2+y^2}$ in the denominator. Do you know some other useful composition for this? Thanks in advance.","I'm trying to prove that doesn't exist. I've tried different paths like , and everything seems to work fine but the term in the denominator. Do you know some other useful composition for this? Thanks in advance.","\lim_{(x,y)\to(0,0)} \frac{xy^2}{(x^2+y^4)\sqrt{x^2+y^2}} (x,y)=(t^2,t) \sqrt{x^2+y^2}","['limits', 'multivariable-calculus']"
25,"How to bound $\sum_{ \substack{\mathbf{x} \in \mathbb{Z}^n \\ \mathrm{dist}(\mathbf{x}, B) > \delta }} \frac{1}{\mathrm{dist}(\mathbf{x}, B)^m }$?",How to bound ?,"\sum_{ \substack{\mathbf{x} \in \mathbb{Z}^n \\ \mathrm{dist}(\mathbf{x}, B) > \delta }} \frac{1}{\mathrm{dist}(\mathbf{x}, B)^m }","I am interested in bounding the following sum $$\sum_{ \substack{\mathbf{x} \in \mathbb{Z}^n \\ \operatorname{dist}(\mathbf{x}, B) > \delta }} \frac{1}{\operatorname{dist}(\mathbf{x}, B)^m }, $$ where $B$ is a compact subset of $\mathbb{R}^n$ and $\operatorname{dist}(x, B)  = \min_{\mathbf{y} \in B} \| \mathbf{x} - \mathbf{y}  \|_{\infty}$ and $\delta > 0$ . I am wondering for what values of $m$ will this sum converge, and how I can show it. Any comments would be appreciated.  Thank you.","I am interested in bounding the following sum where is a compact subset of and and . I am wondering for what values of will this sum converge, and how I can show it. Any comments would be appreciated.  Thank you.","\sum_{ \substack{\mathbf{x} \in \mathbb{Z}^n \\ \operatorname{dist}(\mathbf{x}, B) > \delta }} \frac{1}{\operatorname{dist}(\mathbf{x}, B)^m },
 B \mathbb{R}^n \operatorname{dist}(x, B)  = \min_{\mathbf{y} \in B} \| \mathbf{x} - \mathbf{y}  \|_{\infty} \delta > 0 m","['real-analysis', 'elementary-number-theory', 'multivariable-calculus', 'summation', 'metric-spaces']"
26,"Prove $f(a)^2\sin(2a)=f(b)^2\sin(2b)$ for every closed curve given by $(f(t)\cos(t),f(t)\sin(t))$",Prove  for every closed curve given by,"f(a)^2\sin(2a)=f(b)^2\sin(2b) (f(t)\cos(t),f(t)\sin(t))","I stumbled upon this question recently, and actually managed to solve it. But, if to be honest - I don't like my solution. It feels too long, not very smart, and it is divided to cases (which is very unpleasant to me). I was wondering if there was a smarter solution to the problem; That's why I came here. The Question : Let $\gamma\subset\mathbb{R}^2$ be a closed, simple and piecewise-smooth planar curve, and let $f:\Bbb R\to \Bbb R$ be a differentiable function. Given $a,b\in\Bbb R$ , the parametrization of $\gamma$ is given by: $$\gamma(t)=(f(t)\cos(t),f(t)\sin(t))\\ t\in[a,b]$$ Prove the following equality: $$f(a)^2\sin(2a)=f(b)^2\sin(2b)$$ My Solution : Let $k_1,k_2\in\Bbb Z$ . We know that $\gamma$ is closed. Thus: $$ \left\{ \begin{aligned} f(a)\cos(a) &= f(b)\cos(b)\\ f(a)\sin(a) &= f(b)\sin(b)\\ \end{aligned} \right. $$ Case 1 : $f(a)=0$ . If $f(a)=0$ , then we get from the system of equations that $f(b)=0$ too (since $\sin$ and $\cos$ are never $0$ for the same value), proving the desired equality. Case 2 : $a$ can be given by $a=\pi k_1$ . Thus: $$ \left\{ \begin{aligned} f(a)(-1)^{k_1} &= f(b)\cos(b)\\ 0 &= f(b)\sin(b)\\ \end{aligned} \right. $$ Assuming $f(b)\neq0$ , we'll get (according to the second equation) that $\sin(b)=0$ . In other words, $b$ can also be given by $b=\pi k_2$ . Therefore, $\sin(2a)=\sin(2b)=0$ , which proves the desired equality. Case 3 : $a$ can be given by $a=\frac \pi 2+\pi k_1$ . Thus: $$ \left\{ \begin{aligned} 0 &= f(b)\cos(b)\\ f(a)(-1)^{k_1} &= f(b)\sin(b)\\ \end{aligned} \right. $$ This case is very similar to the previous one: we'll assume again that $f(b)\neq0$ , and then we'll get from the first equation that $b$ can be given by $b=\frac \pi 2+\pi k_2$ , implying again that $\sin(2a)=\sin(2b)=0$ , which proves the desired equality. $(*)$ For cases 1,2 and 3, we have symmetry between $a$ and $b$ , of course. Thus, choosing the inital condition on $b$ (rather than on $a$ as we did) won't change the solution. Case 4: All other options: For all the other options, we will get that neither of the elements in the system of equations can be $0$ . Thus, we can divide one equation by the other and get: $$\tan(a)=\tan(b)\implies a=b+\pi k_1$$ Plugging this equality to one of the equations, we will get: $$f(a)(-1)^{k_1}\cos(b)=f(b)\cos(b)$$ And since $\cos(b)\neq0$ : $$f(a)(-1)^{k_1}=f(b)\implies f(a)^2=f(b)^2\tag{1}$$ Also, note that: $$\sin(2a)=\sin(2b+2\pi k_1)=\sin(2b)\tag{2}$$ Multiplying equation $(1)$ by $(2)$ , we will get the desired equation. $\blacksquare$ Thanks!","I stumbled upon this question recently, and actually managed to solve it. But, if to be honest - I don't like my solution. It feels too long, not very smart, and it is divided to cases (which is very unpleasant to me). I was wondering if there was a smarter solution to the problem; That's why I came here. The Question : Let be a closed, simple and piecewise-smooth planar curve, and let be a differentiable function. Given , the parametrization of is given by: Prove the following equality: My Solution : Let . We know that is closed. Thus: Case 1 : . If , then we get from the system of equations that too (since and are never for the same value), proving the desired equality. Case 2 : can be given by . Thus: Assuming , we'll get (according to the second equation) that . In other words, can also be given by . Therefore, , which proves the desired equality. Case 3 : can be given by . Thus: This case is very similar to the previous one: we'll assume again that , and then we'll get from the first equation that can be given by , implying again that , which proves the desired equality. For cases 1,2 and 3, we have symmetry between and , of course. Thus, choosing the inital condition on (rather than on as we did) won't change the solution. Case 4: All other options: For all the other options, we will get that neither of the elements in the system of equations can be . Thus, we can divide one equation by the other and get: Plugging this equality to one of the equations, we will get: And since : Also, note that: Multiplying equation by , we will get the desired equation. Thanks!","\gamma\subset\mathbb{R}^2 f:\Bbb R\to \Bbb R a,b\in\Bbb R \gamma \gamma(t)=(f(t)\cos(t),f(t)\sin(t))\\ t\in[a,b] f(a)^2\sin(2a)=f(b)^2\sin(2b) k_1,k_2\in\Bbb Z \gamma 
\left\{
\begin{aligned}
f(a)\cos(a) &= f(b)\cos(b)\\
f(a)\sin(a) &= f(b)\sin(b)\\
\end{aligned}
\right.
 f(a)=0 f(a)=0 f(b)=0 \sin \cos 0 a a=\pi k_1 
\left\{
\begin{aligned}
f(a)(-1)^{k_1} &= f(b)\cos(b)\\
0 &= f(b)\sin(b)\\
\end{aligned}
\right.
 f(b)\neq0 \sin(b)=0 b b=\pi k_2 \sin(2a)=\sin(2b)=0 a a=\frac \pi 2+\pi k_1 
\left\{
\begin{aligned}
0 &= f(b)\cos(b)\\
f(a)(-1)^{k_1} &= f(b)\sin(b)\\
\end{aligned}
\right.
 f(b)\neq0 b b=\frac \pi 2+\pi k_2 \sin(2a)=\sin(2b)=0 (*) a b b a 0 \tan(a)=\tan(b)\implies a=b+\pi k_1 f(a)(-1)^{k_1}\cos(b)=f(b)\cos(b) \cos(b)\neq0 f(a)(-1)^{k_1}=f(b)\implies f(a)^2=f(b)^2\tag{1} \sin(2a)=\sin(2b+2\pi k_1)=\sin(2b)\tag{2} (1) (2) \blacksquare","['calculus', 'multivariable-calculus', 'differential-geometry']"
27,"Find $\lim_{(x,y) \to (1,0)} \frac{(x-1)\sin y}{y \ln x }$",Find,"\lim_{(x,y) \to (1,0)} \frac{(x-1)\sin y}{y \ln x }","I want to compute $$\lim_{(x,y) \to (1,0)} \frac{(x-1)\sin y}{y \ln x }$$ I dont know how to do it; I just evaluated $x$ and $y$ limits separately and got $1$ . I'm not sure about this, though.","I want to compute I dont know how to do it; I just evaluated and limits separately and got . I'm not sure about this, though.","\lim_{(x,y) \to (1,0)} \frac{(x-1)\sin y}{y \ln x } x y 1","['limits', 'multivariable-calculus']"
28,show that $u(x) = \frac{1}{2\pi} \int_{R^2} log(|y-x|)\Delta u(y)dy$,show that,u(x) = \frac{1}{2\pi} \int_{R^2} log(|y-x|)\Delta u(y)dy,"Let $u:R^2 \rightarrow R$ be a $C^2$ function with a compact support. I want to show that $u(x) = \frac{1}{2\pi} \int_{R^2} log(|y-x|)\Delta u(y)dy$ , when $x, y \in R^2$ . This look like it should be solved with harmonic functions and with Green's formulas. I declare $v(x,y) = log(|y-x|)$ which is harmonic in $R^2$ \ $\{ x \}$ . Let $G_{R,\epsilon}$ be $B(x,R)$ \ $\bar B(x,\epsilon)$ . From Green's 3rd formula I get: $$ \int_{\partial G_{R,\epsilon}} u<\nabla v, N> - v<\nabla u,N> dS = \int_{\partial G_{R,\epsilon}} u\Delta v - v\Delta u  = -\int_{\partial G_{R,\epsilon}} v\Delta u$$ when $N$ is the outer unit normal. Now: $$ \int_{\partial G_{R,\epsilon}} u<\nabla v, N> - v<\nabla u,N> dS =  \int_{|y-x| = R} (u<\nabla v, N> - v<\nabla u,N>)dS -  \int_{|y-x| = \epsilon} (u<\nabla v, N> - v<\nabla u,N>)dS$$ I need to somehow evaluate those integrals so that when I take $R \rightarrow \infty$ and $\epsilon \rightarrow 0$ I will get $-2\pi u(x) $ . However, the only integral I managed to evaluate is $\int_{|y-x| = \epsilon} v<\nabla u,N>)$ which I showed that goes to $0$ . But with the others I didn't have much luck. Because I got stuck with $log(R)$ who doesn't go to zero and because $<\nabla, N>$ ends being not so nice of an expression. I fit is needed: $\nabla v = (\frac{y_1-x_1}{(y_1-x_1)^2 + (y_2-x_2)^2}, \frac{y_2-x_2}{(y_1-x_1)^2 + (y_2-x_2)^2}$ and the outer Normals are $\frac{y}{R}$ and $\frac{y}{\epsilon}$ Help would be appreciated.","Let be a function with a compact support. I want to show that , when . This look like it should be solved with harmonic functions and with Green's formulas. I declare which is harmonic in \ . Let be \ . From Green's 3rd formula I get: when is the outer unit normal. Now: I need to somehow evaluate those integrals so that when I take and I will get . However, the only integral I managed to evaluate is which I showed that goes to . But with the others I didn't have much luck. Because I got stuck with who doesn't go to zero and because ends being not so nice of an expression. I fit is needed: and the outer Normals are and Help would be appreciated.","u:R^2 \rightarrow R C^2 u(x) = \frac{1}{2\pi} \int_{R^2} log(|y-x|)\Delta u(y)dy x, y \in R^2 v(x,y) = log(|y-x|) R^2 \{ x \} G_{R,\epsilon} B(x,R) \bar B(x,\epsilon)  \int_{\partial G_{R,\epsilon}} u<\nabla v, N> - v<\nabla u,N> dS = \int_{\partial G_{R,\epsilon}} u\Delta v - v\Delta u  = -\int_{\partial G_{R,\epsilon}} v\Delta u N  \int_{\partial G_{R,\epsilon}} u<\nabla v, N> - v<\nabla u,N> dS =  \int_{|y-x| = R} (u<\nabla v, N> - v<\nabla u,N>)dS -  \int_{|y-x| = \epsilon} (u<\nabla v, N> - v<\nabla u,N>)dS R \rightarrow \infty \epsilon \rightarrow 0 -2\pi u(x)  \int_{|y-x| = \epsilon} v<\nabla u,N>) 0 log(R) <\nabla, N> \nabla v = (\frac{y_1-x_1}{(y_1-x_1)^2 + (y_2-x_2)^2}, \frac{y_2-x_2}{(y_1-x_1)^2 + (y_2-x_2)^2} \frac{y}{R} \frac{y}{\epsilon}","['multivariable-calculus', 'vector-analysis', 'harmonic-functions']"
29,$DF(a)$ is invertible and $F(a)=0$. Imply $C^1$?,is invertible and . Imply ?,DF(a) F(a)=0 C^1,"Suppose $F:\mathbb{R}^3\mapsto\mathbb{R}^3$ via $F(x)=(f_1(x),f_2(x),f_3(x))$ . Assume $DF(a)$ is invertible and $F(a)=0$ . Prove that there are infinitely many values $x\in\mathbb{R}^3$ such that $f_1(x)=f_2(x)=f_3(x)$ . I was just asked this question on a quiz and I don't know where to start , especially because there is no assumption about continuity. My first instinct is to use inverse function theorem but I know this isn't right.","Suppose via . Assume is invertible and . Prove that there are infinitely many values such that . I was just asked this question on a quiz and I don't know where to start , especially because there is no assumption about continuity. My first instinct is to use inverse function theorem but I know this isn't right.","F:\mathbb{R}^3\mapsto\mathbb{R}^3 F(x)=(f_1(x),f_2(x),f_3(x)) DF(a) F(a)=0 x\in\mathbb{R}^3 f_1(x)=f_2(x)=f_3(x)",['multivariable-calculus']
30,Prove $\iiint_V\frac{1}{\sqrt{(x-a)^2+(y-b)^2+(z-c)^2}}dV$ is constant inside a ball,Prove  is constant inside a ball,\iiint_V\frac{1}{\sqrt{(x-a)^2+(y-b)^2+(z-c)^2}}dV,"I have been struggling with this problem for a while: Let $V$ be the volume: $$V=\{(x,y,z)| R_1^2\leq x^2+y^2+z^2\leq R_2^2\}$$ Such that $0<R_1 <R_2$ . We will define a new function $\phi(a,b,c)$ , which is defined for every $(a,b,c)\notin V$ : $$\phi(a,b,c)=\iiint_V\frac{1}{\sqrt{(x-a)^2+(y-b)^2+(z-c)^2}}dxdydz$$ Our task is to prove that $\phi(a,b,c)$ is constant inside the ball $B((0,0,0),R_1)$ . I tried to change the variables using spherical coordinates: $$x=r\cos\varphi\sin\theta$$ $$y=r\sin\varphi\sin\theta$$ $$z=r\cos\theta$$ $$r\in[R_1,R_2], \theta \in[0,\pi] ,\varphi \in[0,2\pi]$$ And then solve the integral, proving it is constant when $r<R_1$ , but the integral was a bit hard to solve. I assume there's an easier way - but I couldn't think of one. Thanks! P.S. - Yes, I noticed the Physics here - electric potential inside a ball! But unforunately this is not the course - I have to be rigorous.","I have been struggling with this problem for a while: Let be the volume: Such that . We will define a new function , which is defined for every : Our task is to prove that is constant inside the ball . I tried to change the variables using spherical coordinates: And then solve the integral, proving it is constant when , but the integral was a bit hard to solve. I assume there's an easier way - but I couldn't think of one. Thanks! P.S. - Yes, I noticed the Physics here - electric potential inside a ball! But unforunately this is not the course - I have to be rigorous.","V V=\{(x,y,z)| R_1^2\leq x^2+y^2+z^2\leq R_2^2\} 0<R_1 <R_2 \phi(a,b,c) (a,b,c)\notin V \phi(a,b,c)=\iiint_V\frac{1}{\sqrt{(x-a)^2+(y-b)^2+(z-c)^2}}dxdydz \phi(a,b,c) B((0,0,0),R_1) x=r\cos\varphi\sin\theta y=r\sin\varphi\sin\theta z=r\cos\theta r\in[R_1,R_2], \theta \in[0,\pi] ,\varphi \in[0,2\pi] r<R_1",['multivariable-calculus']
31,Deciphering a problem - calculus; maps,Deciphering a problem - calculus; maps,,"A map from an open set $\mathcal{D} \subset \mathbb{R}^n$ to $\mathbb{R}^m$ , denoted $f : \mathcal{D} \subset \mathbb{R}^n \longrightarrow \mathbb{R}^m$ , is a rule that assigns each $x \in \mathcal{D}$ a value $f(x) \in \mathbb{R}^m$ . Decomposing $f(x)$ into components, we can write such a map as $$f(x) = \left( f_1(x), f_2(x), \dots, f_m(x) \right), \qquad x = \left( x_1, x_2, \dots, x_n \right) \in \mathcal{D} \subset \mathbb{R}^n, $$ where each component $f_i(x), i=1, 2, \dots, m$ , defines a function on $\mathcal{D} \subset \mathbb{R}^n$ . We say that $f(x)$ is differentiable on $\mathcal{D}$ if each one of the component functions $f_i(x), i=1, 2, \dots, m$ , are differentiable on $\mathcal{D}$ . Suppose that $f : \mathcal{D} \subset \mathbb{R}^n \longrightarrow \mathbb{R}^m$ is differentiable map on $\mathcal{D}$ and $$r(t) = \left( r_1(t), r_2(t), \dots, r_n(t) \right), \qquad -1\lt t \lt 1,$$ is a differentiable curve on $\mathbb{R}^n$ that lies in $\mathcal{D}$ . We can use the map $f(x)$ to define a differentiable curve on $\mathbb{R}^m$ by setting $$\gamma(t) = f(r(t)),$$ or equivalently, $$ \gamma(t) = \left( \gamma_1(t), \gamma_2(t), \dots, \gamma_m(t) \right) $$ where $$\gamma_i(t) = f_i \left( r_1(t), r_2(t), \dots, r_n(t) \right), \qquad i = \left( 1, 2, \dots, m \right). $$ We then know the vectors $$v = r'(0) \quad \text{and} \quad w = \gamma'(0)$$ area tangent to the curves $r(t)$ and $\gamma(t)$ at the points $x_0 = r(0) \in \mathcal{D} \subset \mathbb{R}^n$ and $\gamma_0 \in \mathbb{R}^m$ , respectively. Using column notating to denote the tangent vectors $v$ and $w$ , that is, $$ v = \left(\begin{array}{c} v_1 \\ v_2 \\ \vdots \\ v_n \end{array}\right) \quad \text{and} \quad w = \left(\begin{array}{c} w_1 \\ w_2 \\ \vdots \\ w_n \end{array}\right), $$ show that vectors $v$ and $w$ are related by the linear relation $$ w = Df(x_0)v, $$ where $Df(x)$ , for $x \in \mathcal{D} \subset \mathbb{R}^n$ , is the matrix defined by $$ Df(x) = \left(\begin{array}{ccccc} \dfrac{\partial f_1}{\partial x_1}(x) & \dfrac{\partial f_1}{\partial x_2}(x) & \dfrac{\partial f_1}{\partial x_3}(x) &\cdots & \dfrac{\partial f_1}{\partial x_n}(x) \\ \dfrac{\partial f_2}{\partial x_1}(x) & \dfrac{\partial f_2}{\partial x_2}(x) & \dfrac{\partial f_2}{\partial x_3}(x) &\cdots & \dfrac{\partial f_2}{\partial x_n}(x) \\ \dfrac{\partial f_3}{\partial x_1}(x) & \dfrac{\partial f_3}{\partial x_2}(x) & \dfrac{\partial f_3}{\partial x_3}(x) &\cdots & \dfrac{\partial f_3}{\partial x_n}(x)  \\ \vdots & \vdots & \vdots & & \vdots \\ \dfrac{\partial f_m}{\partial x_1}(x) & \dfrac{\partial f_m}{\partial x_2}(x) & \dfrac{\partial f_m}{\partial x_3}(x) &\cdots & \dfrac{\partial f_m}{\partial x_n}(x) \end{array}\right).$$ [Hint: Use the chain rule.] I understand the individual concepts in this problem, but I can't get started on actually answering it. How would one go about putting the pieces together here? Sorry that I haven't done any working, but I need something to get started with.","A map from an open set to , denoted , is a rule that assigns each a value . Decomposing into components, we can write such a map as where each component , defines a function on . We say that is differentiable on if each one of the component functions , are differentiable on . Suppose that is differentiable map on and is a differentiable curve on that lies in . We can use the map to define a differentiable curve on by setting or equivalently, where We then know the vectors area tangent to the curves and at the points and , respectively. Using column notating to denote the tangent vectors and , that is, show that vectors and are related by the linear relation where , for , is the matrix defined by [Hint: Use the chain rule.] I understand the individual concepts in this problem, but I can't get started on actually answering it. How would one go about putting the pieces together here? Sorry that I haven't done any working, but I need something to get started with.","\mathcal{D} \subset \mathbb{R}^n \mathbb{R}^m f : \mathcal{D} \subset \mathbb{R}^n \longrightarrow \mathbb{R}^m x \in \mathcal{D} f(x) \in \mathbb{R}^m f(x) f(x) = \left( f_1(x), f_2(x), \dots, f_m(x) \right), \qquad x = \left( x_1, x_2, \dots, x_n \right) \in \mathcal{D} \subset \mathbb{R}^n,  f_i(x), i=1, 2, \dots, m \mathcal{D} \subset \mathbb{R}^n f(x) \mathcal{D} f_i(x), i=1, 2, \dots, m \mathcal{D} f : \mathcal{D} \subset \mathbb{R}^n \longrightarrow \mathbb{R}^m \mathcal{D} r(t) = \left( r_1(t), r_2(t), \dots, r_n(t) \right), \qquad -1\lt t \lt 1, \mathbb{R}^n \mathcal{D} f(x) \mathbb{R}^m \gamma(t) = f(r(t)),  \gamma(t) = \left( \gamma_1(t), \gamma_2(t), \dots, \gamma_m(t) \right)  \gamma_i(t) = f_i \left( r_1(t), r_2(t), \dots, r_n(t) \right), \qquad i = \left( 1, 2, \dots, m \right).  v = r'(0) \quad \text{and} \quad w = \gamma'(0) r(t) \gamma(t) x_0 = r(0) \in \mathcal{D} \subset \mathbb{R}^n \gamma_0 \in \mathbb{R}^m v w  v = \left(\begin{array}{c} v_1 \\ v_2 \\ \vdots \\ v_n \end{array}\right) \quad \text{and} \quad w = \left(\begin{array}{c} w_1 \\ w_2 \\ \vdots \\ w_n \end{array}\right),  v w  w = Df(x_0)v,  Df(x) x \in \mathcal{D} \subset \mathbb{R}^n  Df(x) = \left(\begin{array}{ccccc} \dfrac{\partial f_1}{\partial x_1}(x) & \dfrac{\partial f_1}{\partial x_2}(x) & \dfrac{\partial f_1}{\partial x_3}(x) &\cdots & \dfrac{\partial f_1}{\partial x_n}(x) \\ \dfrac{\partial f_2}{\partial x_1}(x) & \dfrac{\partial f_2}{\partial x_2}(x) & \dfrac{\partial f_2}{\partial x_3}(x) &\cdots & \dfrac{\partial f_2}{\partial x_n}(x) \\ \dfrac{\partial f_3}{\partial x_1}(x) & \dfrac{\partial f_3}{\partial x_2}(x) & \dfrac{\partial f_3}{\partial x_3}(x) &\cdots & \dfrac{\partial f_3}{\partial x_n}(x)  \\ \vdots & \vdots & \vdots & & \vdots \\ \dfrac{\partial f_m}{\partial x_1}(x) & \dfrac{\partial f_m}{\partial x_2}(x) & \dfrac{\partial f_m}{\partial x_3}(x) &\cdots & \dfrac{\partial f_m}{\partial x_n}(x) \end{array}\right).","['linear-algebra', 'multivariable-calculus', 'linear-transformations', 'chain-rule']"
32,"$\{x_{k}\}$ has an accumulation point and $z_k=||x_{k}-x_{k+1}||^2$ is summable, then does $\{x_{n}\}$ converge?","has an accumulation point and  is summable, then does  converge?",\{x_{k}\} z_k=||x_{k}-x_{k+1}||^2 \{x_{n}\},"Let $\{x_k\}\subset \mathbb{R}^n$ and suppose $\{x_{k}\}$ has an accumulation point and $z_k=||x_{k}-x_{k+1}||^2$ is summable. I was trying to prove that $\{x_{n}\}$ converges, but the best I can prove is  that $$\sum^{\infty}_{n=1} \frac{\left(\sum^{n}_{j=1}||x_{j+1}-x_{j}||\right)^2}{n^2}<\infty$$ using Hardy's Inequality. I appreciate any help.","Let and suppose has an accumulation point and is summable. I was trying to prove that converges, but the best I can prove is  that using Hardy's Inequality. I appreciate any help.",\{x_k\}\subset \mathbb{R}^n \{x_{k}\} z_k=||x_{k}-x_{k+1}||^2 \{x_{n}\} \sum^{\infty}_{n=1} \frac{\left(\sum^{n}_{j=1}||x_{j+1}-x_{j}||\right)^2}{n^2}<\infty,"['sequences-and-series', 'multivariable-calculus']"
33,Existence of directional derivative,Existence of directional derivative,,"For a two variable function, does the existence of continuous partial derivatives of order 1 with respect to $x$ and $y$ at a point $(x,y)$ imply the existence of the directional derivative in any direction at the point $(x,y)$ ?","For a two variable function, does the existence of continuous partial derivatives of order 1 with respect to and at a point imply the existence of the directional derivative in any direction at the point ?","x y (x,y) (x,y)","['real-analysis', 'multivariable-calculus', 'partial-derivative']"
34,Vector proof of shortest distance from a point to a line: flaw in my reasoning.,Vector proof of shortest distance from a point to a line: flaw in my reasoning.,,"Let $E$ an euclidean plane, $P$ a point in $E$ , and $d$ a straight line in $E$ with a fixed point $A$ and a direction vector $\vec{V}$ , so that any other point $X \in d$ can be described through a real parameter $t$ by $$\vec{AX}=t\vec{V}$$ . Consequently, we can write $$\vec{PX}=\vec{PA}+t\vec{V} \,\,\,\,\,\,\,\,\,\,\,\,\    [1]$$ In this mathexchange question, I have a problem with a proof to show the shortest distance from $P$ to $d$ . In fact, I am able to calculate it by minimizing the quantity $$||\vec{PX}||^2$$ seen as a function of $t$ . That expression is purely scalar so it's easy to avoid mistake when minimizing it through differentiation. But just out of curiosity I tried to get to the same result by minimizing $\vec{PX}$ instead of its square, and I have a problem: I can show the shortest distance is perpendicular, but I get the magnitude wrong. Here is the reasoning: First I write $\vec{PX}=|\vec{PX}|.e_{\vec{PX}}$ , where $e$ is the unit vector along the direction of $\vec{PX}$ . Minimizing equation $[1]$ with respect to $t$ means two things: 1/ First I must differentiate both sides of $[1]$ : $$(\partial_t|\vec{PX}|).e_{\vec{PX}}+|\vec{PX}|.(\partial_te_{\vec{PX}})=0+\vec{V}\,\,\,\,\,[2]$$ 2/ Now I impose the minimization condition on the distance: this means that the object $\partial_t|\vec{PX}|=0$ , so the only part that survives in the expression $[2]$ is $$|\vec{PX}|.(\partial_te_{\vec{PX}})=\vec{V}\,\,\,\,\,[3]$$ Now there is a reasoning of vector calculus that tells us that the differential of a unit vector is perpendicular to it. The LHS of $[3]$ tells us that $\partial_te_{\vec{PX}}$ is perpendiculat to $\vec{PX}$ but the RHS of $[3]$ tells us that this perpendicular object is also parallel to $\vec{V}$ . Hence the shortest distance $|\vec{PX}|$ is perpendicular to the straight line $d$ . But then I get stuck and I don't see how to get the magnitude of that distance. At first I would want to write $$|\vec{PX}|=\frac{|\vec{V}|}{|\partial_te_{\vec{PX}}|} \,\,\,\,\,\, [4]$$ I do not see how to calculate the actual magnitude of $|\vec{PX}|$ from there. In fact, I suspect there is a mistake somewhere, because the direction vector $\vec{V}$ can be arbitrarily small or large, while $\vec{PX}$ is fixed by the geometry. I am probably missing something very silly. I have explained the problem the most clearly I can, any insight would be appreciated. Thanks.","Let an euclidean plane, a point in , and a straight line in with a fixed point and a direction vector , so that any other point can be described through a real parameter by . Consequently, we can write In this mathexchange question, I have a problem with a proof to show the shortest distance from to . In fact, I am able to calculate it by minimizing the quantity seen as a function of . That expression is purely scalar so it's easy to avoid mistake when minimizing it through differentiation. But just out of curiosity I tried to get to the same result by minimizing instead of its square, and I have a problem: I can show the shortest distance is perpendicular, but I get the magnitude wrong. Here is the reasoning: First I write , where is the unit vector along the direction of . Minimizing equation with respect to means two things: 1/ First I must differentiate both sides of : 2/ Now I impose the minimization condition on the distance: this means that the object , so the only part that survives in the expression is Now there is a reasoning of vector calculus that tells us that the differential of a unit vector is perpendicular to it. The LHS of tells us that is perpendiculat to but the RHS of tells us that this perpendicular object is also parallel to . Hence the shortest distance is perpendicular to the straight line . But then I get stuck and I don't see how to get the magnitude of that distance. At first I would want to write I do not see how to calculate the actual magnitude of from there. In fact, I suspect there is a mistake somewhere, because the direction vector can be arbitrarily small or large, while is fixed by the geometry. I am probably missing something very silly. I have explained the problem the most clearly I can, any insight would be appreciated. Thanks.","E P E d E A \vec{V} X \in d t \vec{AX}=t\vec{V} \vec{PX}=\vec{PA}+t\vec{V} \,\,\,\,\,\,\,\,\,\,\,\,\    [1] P d ||\vec{PX}||^2 t \vec{PX} \vec{PX}=|\vec{PX}|.e_{\vec{PX}} e \vec{PX} [1] t [1] (\partial_t|\vec{PX}|).e_{\vec{PX}}+|\vec{PX}|.(\partial_te_{\vec{PX}})=0+\vec{V}\,\,\,\,\,[2] \partial_t|\vec{PX}|=0 [2] |\vec{PX}|.(\partial_te_{\vec{PX}})=\vec{V}\,\,\,\,\,[3] [3] \partial_te_{\vec{PX}} \vec{PX} [3] \vec{V} |\vec{PX}| d |\vec{PX}|=\frac{|\vec{V}|}{|\partial_te_{\vec{PX}}|} \,\,\,\,\,\, [4] |\vec{PX}| \vec{V} \vec{PX}","['geometry', 'multivariable-calculus', 'optimization', 'vectors', 'analytic-geometry']"
35,"Gradient and Hessian of $f(x,y) := a^T \left( x \odot \left[ \exp\left( \mu \ (y \ \oslash \ x) \right) - 1 \right] \right)$, wr.t. $x$ and $y$","Gradient and Hessian of , wr.t.  and","f(x,y) := a^T \left( x \odot \left[ \exp\left( \mu \ (y \ \oslash \ x) \right) - 1 \right] \right) x y","How to find the Gradient and Hessian of \begin{align} f(x,y) := a^T \left( x \odot \left[ \exp\left( \mu \  (y \ \oslash \ x)  \right) - 1 \right] \right) \ , \end{align} where $a, x, y \in \mathbb{R}^n$ , all-ones vector $1 \in \mathbb{R}^n$ , and $\mu  \in \mathbb{R}$ ? Also, $\odot$ and $\oslash$ means elementwise multiplication and division, respectively.","How to find the Gradient and Hessian of where , all-ones vector , and ? Also, and means elementwise multiplication and division, respectively.","\begin{align}
f(x,y) := a^T \left( x \odot \left[ \exp\left( \mu \  (y \ \oslash \ x)  \right) - 1 \right] \right) \ ,
\end{align} a, x, y \in \mathbb{R}^n 1 \in \mathbb{R}^n \mu  \in \mathbb{R} \odot \oslash","['multivariable-calculus', 'matrix-calculus']"
36,Spivak's Calculus on Manifolds problem 2-15(c),Spivak's Calculus on Manifolds problem 2-15(c),,"2-15. Regard an $n \times n $ matrix as a point in the $n$ -fold product $\Bbb{R^n} \times \cdots\times \Bbb{R^n}$ by considering each row as a    member of $\Bbb{R^n}$ . (a) Prove that $det: \Bbb{R^n} \times \cdots \times \Bbb{R^n} \to \Bbb{R}$ is differentiable and $  D(\mathrm{det})(a_1,\ldots,a_n)(x_1,\ldots,x_n)=\sum_{n=i}^{n} \mathrm{det} \begin{bmatrix}  a_1 \\ \vdots \\ x_i\\ \vdots\\ a_n \end{bmatrix}$ (b) if $a_{ij}: \Bbb{R} \to \Bbb{R} $ are differentiable and $f(t)=det(a_{ij}(t))$ , show that $f'(t)= \sum_{j=1}^{n} det \begin{bmatrix}  a_{11}(t),\ldots, a_{1n}(t)\\ \vdots\\ a_{j1}'(t),\ldots, a_{jn}'(t)\\ \vdots \\ a_{n1}(t),\ldots, a_{nn}(t) \end{bmatrix}$ (c) if $\mathrm{det} (a_{ij}(t)) \neq 0$ for all $t$ and $b_1,...,b_n: \Bbb{R} \to \Bbb{R}$ are differentiable, let $s_1,\ldots,s_n: \Bbb{R}  \to \Bbb{R}$ be the functions such that $s_1(t),\ldots,s_n(t)$ are the solutions of the equations $\sum_{j=1}^{n}  a_{ij}(t)s_j(t)=b_i(t)$ $i=1,\ldots,n$ . show that $s_i$ is differentiable and find $s_i'(t)$ . Problem 2-40 asks to redo problem 2-15(c) using the implicit function theorem. 2-12 theorem (implicit function theorem). Suppose $ f: \Bbb{R^n} \times \Bbb{R^m} \to \Bbb{R^m} $ is continuously   differentiable in an open set containing $(a,b)$ and $f(a,b) =0$ . Let  be the $m \times m$ matrix. $$(D_{n+j}f^i(a,b))~, \quad 1 \leq i,~j \leq m~.$$ If $\mathrm{det} M \neq 0$ , there is an open set $A \subset R^n$ containing $a$ and an open set $B$ subset $R^m$ containing $b$ , with the following   property: for each $x \in A$ there is a unique $g(x)$ in $B$ such that $f(x,g(x)) =0$ . the function $g$ is differentiable. Call $i^{th}$ row of $(a_{ji}(t))$ as $R_i(t)$ . Let me define $g(t)=  \begin{bmatrix}  s_1(t) \\ s_2 (t)\\ \vdots   \\ s_n(t)  \end{bmatrix}$ . thus $R_i(t)g(t)=b_i(t)$ . If I define $f:\Bbb{R^n} \times \Bbb{R^n} \to \Bbb{R^n}$ such that $f(R_i(t),g(t)) = R_i(t)g(t)=b_i(t)$ , the first difficulty I face is that the theorem will provide $g(t)$ is differentiable only if $b_i(t)=0$ , which may not be true. To overcome this difficulty, how should I define $f$ so that the theorem will provide me that $g(t)$ is differentiable?","2-15. Regard an matrix as a point in the -fold product by considering each row as a    member of . (a) Prove that is differentiable and (b) if are differentiable and , show that (c) if for all and are differentiable, let be the functions such that are the solutions of the equations . show that is differentiable and find . Problem 2-40 asks to redo problem 2-15(c) using the implicit function theorem. 2-12 theorem (implicit function theorem). Suppose is continuously   differentiable in an open set containing and . Let  be the matrix. If , there is an open set containing and an open set subset containing , with the following   property: for each there is a unique in such that . the function is differentiable. Call row of as . Let me define . thus . If I define such that , the first difficulty I face is that the theorem will provide is differentiable only if , which may not be true. To overcome this difficulty, how should I define so that the theorem will provide me that is differentiable?","n \times n  n \Bbb{R^n} \times \cdots\times \Bbb{R^n} \Bbb{R^n} det: \Bbb{R^n} \times \cdots \times \Bbb{R^n} \to
\Bbb{R} 
 D(\mathrm{det})(a_1,\ldots,a_n)(x_1,\ldots,x_n)=\sum_{n=i}^{n} \mathrm{det} \begin{bmatrix} 
a_1 \\ \vdots \\ x_i\\ \vdots\\ a_n \end{bmatrix} a_{ij}: \Bbb{R} \to \Bbb{R}  f(t)=det(a_{ij}(t)) f'(t)= \sum_{j=1}^{n} det
\begin{bmatrix}  a_{11}(t),\ldots, a_{1n}(t)\\ \vdots\\ a_{j1}'(t),\ldots,
a_{jn}'(t)\\ \vdots \\ a_{n1}(t),\ldots, a_{nn}(t) \end{bmatrix} \mathrm{det} (a_{ij}(t)) \neq 0 t b_1,...,b_n: \Bbb{R}
\to \Bbb{R} s_1,\ldots,s_n: \Bbb{R}
 \to \Bbb{R} s_1(t),\ldots,s_n(t) \sum_{j=1}^{n}
 a_{ij}(t)s_j(t)=b_i(t) i=1,\ldots,n s_i s_i'(t)  f: \Bbb{R^n}
\times \Bbb{R^m} \to \Bbb{R^m}  (a,b) f(a,b) =0 m \times m (D_{n+j}f^i(a,b))~, \quad 1 \leq i,~j \leq m~. \mathrm{det} M \neq 0 A \subset R^n a B R^m b x \in A g(x) B f(x,g(x)) =0 g i^{th} (a_{ji}(t)) R_i(t) g(t)= 
\begin{bmatrix} 
s_1(t) \\
s_2 (t)\\
\vdots   \\
s_n(t) 
\end{bmatrix} R_i(t)g(t)=b_i(t) f:\Bbb{R^n} \times \Bbb{R^n} \to \Bbb{R^n} f(R_i(t),g(t)) = R_i(t)g(t)=b_i(t) g(t) b_i(t)=0 f g(t)","['real-analysis', 'multivariable-calculus']"
37,"Gradient of $f(W)=\sum_{j \neq {t}} \left[ \max\left(0, [ W x ]_j - \left[ W x \right]_{t} + \delta \right) \right] + \lambda \left\| W \right\|_F^2$",Gradient of,"f(W)=\sum_{j \neq {t}} \left[ \max\left(0, [ W x ]_j - \left[ W x \right]_{t} + \delta \right) \right] + \lambda \left\| W \right\|_F^2","How to find the gradient of the following function \begin{align} f(W) := \sum_{j \neq {t}} \left[ \max\left(0, [ W x ]_j - \left[ W x \right]_{t} + \delta \right) \right] + \lambda \left\| W \right\|_F^2 \ , \end{align} w.r.t. $W \in \mathbb{R}^{m \times n}$ matrix, where $x \in \mathbb{R}^n$ and $\delta, \lambda$ are known variables. The $j$ th element of a vector $y \in \mathbb{R}^n$ is denoted as $[y]_j$ .","How to find the gradient of the following function w.r.t. matrix, where and are known variables. The th element of a vector is denoted as .","\begin{align}
f(W) := \sum_{j \neq {t}} \left[ \max\left(0, [ W x ]_j - \left[ W x \right]_{t} + \delta \right) \right] + \lambda \left\| W \right\|_F^2 \ ,
\end{align} W \in \mathbb{R}^{m \times n} x \in \mathbb{R}^n \delta, \lambda j y \in \mathbb{R}^n [y]_j","['multivariable-calculus', 'matrix-calculus']"
38,Proving $\int_{S^{n-1}}x_1^2dS =\int_{S^{n-1}}x_k^2dS$,Proving,\int_{S^{n-1}}x_1^2dS =\int_{S^{n-1}}x_k^2dS,"Denote $x = (x_1,...,x_n)$ . I'm trying to prove the following: $$\int_{S^{n-1}}x_1^2dS =\int_{S^{n-1}}x_k^2dS \; , \;  2\leq k\leq n $$ Intuitively this equality is due to the symmetry of the sphere, but I'm looking for a formal explanation. I thought about using the definition with a parametrization, but I'm not sure how to find a good paramterization of the sphere for that purpose.","Denote . I'm trying to prove the following: Intuitively this equality is due to the symmetry of the sphere, but I'm looking for a formal explanation. I thought about using the definition with a parametrization, but I'm not sure how to find a good paramterization of the sphere for that purpose.","x = (x_1,...,x_n) \int_{S^{n-1}}x_1^2dS =\int_{S^{n-1}}x_k^2dS \; , \;  2\leq k\leq n ","['integration', 'multivariable-calculus', 'manifolds', 'spheres']"
39,Doubt from Spivak's proof of inverse function theorem.,Doubt from Spivak's proof of inverse function theorem.,,why must $y^i-f^i(x)=0$ for all $i$ ?,why must for all ?,y^i-f^i(x)=0 i,"['real-analysis', 'multivariable-calculus', 'inverse-function-theorem']"
40,"Prove $\exists\theta\in(0,1)$ s.t. $\Delta f=\frac{\partial f}{\partial x}\Delta x+\frac{\partial f}{\partial y}\Delta y$",Prove  s.t.,"\exists\theta\in(0,1) \Delta f=\frac{\partial f}{\partial x}\Delta x+\frac{\partial f}{\partial y}\Delta y","Let $f(x,y)\in C^1$ in $\mathbb{R^2}$ and let $(x_0+\Delta x,y_0+\Delta y)$ and $(x_0,y_0)$ be points in $\mathbb{R^2}$ . Prove that $\exists\theta\in(0,1)$ such that: $$f(x_0+\Delta x,y_0+\Delta y)-f(x_0,y_0)=\\\frac{\partial f}{\partial x}(x_0+\theta\Delta x,y_0+\theta\Delta y)\Delta x+\frac{\partial f}{\partial y}(x_0+\theta\Delta x,y_0+\theta\Delta y)\Delta y$$ At first glance, this seemed like the differentiability definition, but I tried to make the connection and unfortunately failed. I guess that MVT hides here, but I don't see how to rigorously reach it. Thanks! Note : I found a solution to this problem online (not here) but I couldn't understand it, so I'd really appreciate a somewhat detailed solution.","Let in and let and be points in . Prove that such that: At first glance, this seemed like the differentiability definition, but I tried to make the connection and unfortunately failed. I guess that MVT hides here, but I don't see how to rigorously reach it. Thanks! Note : I found a solution to this problem online (not here) but I couldn't understand it, so I'd really appreciate a somewhat detailed solution.","f(x,y)\in C^1 \mathbb{R^2} (x_0+\Delta x,y_0+\Delta y) (x_0,y_0) \mathbb{R^2} \exists\theta\in(0,1) f(x_0+\Delta x,y_0+\Delta y)-f(x_0,y_0)=\\\frac{\partial f}{\partial x}(x_0+\theta\Delta x,y_0+\theta\Delta y)\Delta x+\frac{\partial f}{\partial y}(x_0+\theta\Delta x,y_0+\theta\Delta y)\Delta y","['calculus', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
41,Will the product of a real valued matrix and its transpose always have a real eigenvector?,Will the product of a real valued matrix and its transpose always have a real eigenvector?,,"I'm trying to solve this really interesting problem, taken from ""Berkeley Problmes in Mathematics"" by Souza and Silva https://www.amazon.com/Berkeley-Problems-Mathematics-Problem-Books/dp/0387204296 Trying to make headway, I've noticed that if the last two statements are correct, then $M^T(Mu)=\sigma^2 u$ , so in other words, there exists an eigenvector $u$ with the eigenvalue $\sigma^2$ . Therefore, I have two questions, and I would dearly love a proof or counter-example: Is it true that $M^TM$ must have at least a (real) eigenvector? I strongly suspect this $\sigma$ they talk of is the operator norm of $M$ . Is this true? Is this a dead-end way to try to solve this problem. Should I try something else? Of course, everything is real-valued here.","I'm trying to solve this really interesting problem, taken from ""Berkeley Problmes in Mathematics"" by Souza and Silva https://www.amazon.com/Berkeley-Problems-Mathematics-Problem-Books/dp/0387204296 Trying to make headway, I've noticed that if the last two statements are correct, then , so in other words, there exists an eigenvector with the eigenvalue . Therefore, I have two questions, and I would dearly love a proof or counter-example: Is it true that must have at least a (real) eigenvector? I strongly suspect this they talk of is the operator norm of . Is this true? Is this a dead-end way to try to solve this problem. Should I try something else? Of course, everything is real-valued here.",M^T(Mu)=\sigma^2 u u \sigma^2 M^TM \sigma M,"['linear-algebra', 'multivariable-calculus', 'linear-transformations']"
42,Show that there are at least two points on a manifold to which a vector is normal,Show that there are at least two points on a manifold to which a vector is normal,,"Let $M \subset \mathbb R^3$ be a $2$ -dimensional manifold which is also a compact set. And let $v \in \mathbb R^3$ be a vector which satisfies $ ||v|| = 1$ . The task is to prove that there are at least two points $x,y \in M$ such that $v$ is a normal vector to their tangent spaces. There was given a hint to look at the extremum points of the function $f(x) = <x,v>$ which means $f(x) = x_1v_1 + x_2v_2 + x_3v_3$ . Since the functions is continuous and $M$ is compact then the function get a minimum and maximum on $M$ . Let's say the $x$ is the maximum and $y$ is a the minimum. Moreover, I noticed that $\nabla f = (v_1,v_2,v_3) = v$ . However, I am not so sure how to continue from here. I am supposed to show that $v$ is normal to both $T_xM$ and $T_yM$ but I don't see how exactly. I still haven't used the fact that $M$ is a manifold so it probably uses that. Help would be appreciated","Let be a -dimensional manifold which is also a compact set. And let be a vector which satisfies . The task is to prove that there are at least two points such that is a normal vector to their tangent spaces. There was given a hint to look at the extremum points of the function which means . Since the functions is continuous and is compact then the function get a minimum and maximum on . Let's say the is the maximum and is a the minimum. Moreover, I noticed that . However, I am not so sure how to continue from here. I am supposed to show that is normal to both and but I don't see how exactly. I still haven't used the fact that is a manifold so it probably uses that. Help would be appreciated","M \subset \mathbb R^3 2 v \in \mathbb R^3  ||v|| = 1 x,y \in M v f(x) = <x,v> f(x) = x_1v_1 + x_2v_2 + x_3v_3 M M x y \nabla f = (v_1,v_2,v_3) = v v T_xM T_yM M","['calculus', 'multivariable-calculus', 'differential-geometry', 'manifolds']"
43,Injective mapping theorem proof discrepancy.,Injective mapping theorem proof discrepancy.,,"I do not understand from where the 2 comes in the proof of letter(a) of the statement of the theorem: And this is proposition 12.2.4 I think the author has used the mean value inequality, but I do not know how, could anyone explain this for me please?","I do not understand from where the 2 comes in the proof of letter(a) of the statement of the theorem: And this is proposition 12.2.4 I think the author has used the mean value inequality, but I do not know how, could anyone explain this for me please?",,"['real-analysis', 'calculus', 'analysis', 'multivariable-calculus', 'proof-explanation']"
44,"Show that if $f$ is a smooth function, $M$ is a manifold and $x$ is a local extremum of $f$ on $M$, then $D_f(x)(v) = 0$ in the tangent space.","Show that if  is a smooth function,  is a manifold and  is a local extremum of  on , then  in the tangent space.",f M x f M D_f(x)(v) = 0,"Let $M \subset R^n$ be a $k$ dimensional manifold. Let $f: R^n \to R$ be a smooth function. Let $x \in M$ be a local extremum of $f$ on $M$ . The task is to prove that $\nabla f_x (v) = 0$ for every $v \in T_xM$ (the tangent space at $x$ ). I am not really sure fro where to start. What I find weird is that I remember that if $y$ is a local extremum of $f$ , then $\nabla f_y = 0$ (it is the gradient at the point $y$ ). But then the question is trivial, so I am probably getting something wrong here. Can some one show me what is the problem and give me a direction on how to solve it? Help would be appreciated. I also need to such an example of such a function so that $\nabla f_x \ne 0$ . Edit: Let $M \subset R^n$ be a $k$ -dimensional manifold and let $ f : R^n \to R$ be a smooth function s.t $ x_0 /in M $ is a local extremum point of $f$ on $M$ (i) Show that $\nabla f_{x_0}(h) = 0$ for every $h \in T_{x_0}M$ . (ii) Find such a function with $\nabla f{x_0} \ne 0$","Let be a dimensional manifold. Let be a smooth function. Let be a local extremum of on . The task is to prove that for every (the tangent space at ). I am not really sure fro where to start. What I find weird is that I remember that if is a local extremum of , then (it is the gradient at the point ). But then the question is trivial, so I am probably getting something wrong here. Can some one show me what is the problem and give me a direction on how to solve it? Help would be appreciated. I also need to such an example of such a function so that . Edit: Let be a -dimensional manifold and let be a smooth function s.t is a local extremum point of on (i) Show that for every . (ii) Find such a function with",M \subset R^n k f: R^n \to R x \in M f M \nabla f_x (v) = 0 v \in T_xM x y f \nabla f_y = 0 y \nabla f_x \ne 0 M \subset R^n k  f : R^n \to R  x_0 /in M  f M \nabla f_{x_0}(h) = 0 h \in T_{x_0}M \nabla f{x_0} \ne 0,"['calculus', 'multivariable-calculus', 'manifolds', 'tangent-spaces']"
45,Surface integral of an intersection cone-plane,Surface integral of an intersection cone-plane,,"Find $\iint_S ydS$ , where $s$ is the part of the cone $z = \sqrt{2(x^2 + y^2)}$ that lies below the plane $z = 1 + y$ The intersection of these two is an ellipse of area $A = \pi\sqrt {2}$ Note that this problem has been solved here: Surface integral problem . However, I found a slightly different approach. Let's calculate dS: $$dS = \sqrt{1 + \frac{4(x^2 + y^2)}{z^2}}dxdy$$ Plugging $z = \sqrt{2(x^2 + y^2)}$ into it you get: $$dS = \sqrt{3}dxdy$$ Where A = $dxdy = \pi\sqrt {2}$ Then: $$\iint_S ydS = \sqrt {6}\pi $$ Note this is not the same result robjohn got. I don't understand why $y$ is treated as it wasn't there. I understand the following: $\iint_S dS = \sqrt {6}\pi $ But we have $\iint_S ydS$ and not $\iint_S dS$ . I guess there must be a symmetry argument to justify this but I don't see it... Thanks.","Find , where is the part of the cone that lies below the plane The intersection of these two is an ellipse of area Note that this problem has been solved here: Surface integral problem . However, I found a slightly different approach. Let's calculate dS: Plugging into it you get: Where A = Then: Note this is not the same result robjohn got. I don't understand why is treated as it wasn't there. I understand the following: But we have and not . I guess there must be a symmetry argument to justify this but I don't see it... Thanks.",\iint_S ydS s z = \sqrt{2(x^2 + y^2)} z = 1 + y A = \pi\sqrt {2} dS = \sqrt{1 + \frac{4(x^2 + y^2)}{z^2}}dxdy z = \sqrt{2(x^2 + y^2)} dS = \sqrt{3}dxdy dxdy = \pi\sqrt {2} \iint_S ydS = \sqrt {6}\pi  y \iint_S dS = \sqrt {6}\pi  \iint_S ydS \iint_S dS,"['calculus', 'multivariable-calculus', 'surface-integrals']"
46,$\iint_{\Sigma} \frac{d\sigma}{\sqrt{x^2+y^2+(z+R)^2}}$ with $\Sigma$ the upper half of the sphere $x^2+y^2+z^2=R^2$,with  the upper half of the sphere,\iint_{\Sigma} \frac{d\sigma}{\sqrt{x^2+y^2+(z+R)^2}} \Sigma x^2+y^2+z^2=R^2,"My attempt: $K = \{(r,\theta): 0 \le r  \le R, 0 \le \theta \le 2\pi \}$ . I chose following parameterization: $$ \vec{\varphi}(r,\theta)=(r\cos\theta, r\sin\theta,\sqrt{R^2-r^2}).$$ And after further calculations, I got $$ \left\|\frac{\partial{\vec{\varphi}}}{\partial r} \times\frac{\partial{\vec{\varphi}}}{\partial \theta}\right\| = \frac{rR}{\sqrt{R^2-r^2}}.$$ And now, $$\iint_{\Sigma} \frac{d\sigma}{\sqrt{x^2+y^2+(z+R)^2}} = \int_0^{2\pi}d\theta\int_0^R \frac{r^2R}{\sqrt{(2R^2+2R\sqrt{R^2-r^2})(R^2-r^2)}}dr.$$ Applying the substitution $t = \sqrt{R^2-r^2}$ , we get $$ 2\pi R\int_R^0\frac{R^2-t^2}{\sqrt{(2R^2+2Rt)t^2}}\frac{-t}{\sqrt{R^2-t^2}}dt=\sqrt{2R}\pi\int_0^R\sqrt{R-t}dt=\frac23\sqrt2\pi R^2.$$ The correct answer, however, should be $2\pi R(2-\sqrt2)$ . Can somebody spot my mistake?","My attempt: . I chose following parameterization: And after further calculations, I got And now, Applying the substitution , we get The correct answer, however, should be . Can somebody spot my mistake?","K = \{(r,\theta): 0 \le r  \le R, 0 \le \theta \le 2\pi \}  \vec{\varphi}(r,\theta)=(r\cos\theta, r\sin\theta,\sqrt{R^2-r^2}).  \left\|\frac{\partial{\vec{\varphi}}}{\partial r} \times\frac{\partial{\vec{\varphi}}}{\partial \theta}\right\| = \frac{rR}{\sqrt{R^2-r^2}}. \iint_{\Sigma} \frac{d\sigma}{\sqrt{x^2+y^2+(z+R)^2}} = \int_0^{2\pi}d\theta\int_0^R \frac{r^2R}{\sqrt{(2R^2+2R\sqrt{R^2-r^2})(R^2-r^2)}}dr. t = \sqrt{R^2-r^2}  2\pi R\int_R^0\frac{R^2-t^2}{\sqrt{(2R^2+2Rt)t^2}}\frac{-t}{\sqrt{R^2-t^2}}dt=\sqrt{2R}\pi\int_0^R\sqrt{R-t}dt=\frac23\sqrt2\pi R^2. 2\pi R(2-\sqrt2)",['multivariable-calculus']
47,What does it mean for partial derivative to exist but not continuous?,What does it mean for partial derivative to exist but not continuous?,,What does it mean for partial derivative to exist but not continuous? Continuous partial derivatives imply differentiability and differentiability imply continuity of a function and the existence of partial derivatives. I also understand that any other implication of this statement is false.,What does it mean for partial derivative to exist but not continuous? Continuous partial derivatives imply differentiability and differentiability imply continuity of a function and the existence of partial derivatives. I also understand that any other implication of this statement is false.,,"['multivariable-calculus', 'continuity']"
48,calculate the surface area of the part of a cylinder $ x^2 + (y-1)^2 = 1 $ that is inside the sphere $ x^2 + y^2 + z^2 = 4 $.,calculate the surface area of the part of a cylinder  that is inside the sphere ., x^2 + (y-1)^2 = 1   x^2 + y^2 + z^2 = 4 ,"calculate the surface area of the part of a cylinder $ x^2 + (y-1)^2 = 1 $ that is inside the sphere $  x^2 + y^2 + z^2 = 4 $ . my trial : The Domain of integration on the YZ plane is : solving : (*) $  x^2 + (y-1)^2 = 1 $ $ x^2 + (y)^2 + z^2 \leq 4 $ We get : $ 0 \leq z \leq \sqrt{4-2y}$ and $ -2 \leq y \leq 2 $ $||\nabla{Cylinder(x,y,z)}|| = ||(2x,2(y-1),0)|| = 2 ~$ see(*) S = $2~\int_{-2}^{2}\int_{0}^{\sqrt{4-2y}}~~2~dydz$ ( by symmetry *2) I also tried solving by parmetrization and i didn't get the same answer i really need HELP is this way alright ? or is there something wrong",calculate the surface area of the part of a cylinder that is inside the sphere . my trial : The Domain of integration on the YZ plane is : solving : (*) We get : and see(*) S = ( by symmetry *2) I also tried solving by parmetrization and i didn't get the same answer i really need HELP is this way alright ? or is there something wrong," x^2 + (y-1)^2 = 1    x^2 + y^2 + z^2 = 4    x^2 + (y-1)^2 = 1   x^2 + (y)^2 + z^2 \leq 4   0 \leq z \leq \sqrt{4-2y}  -2 \leq y \leq 2  ||\nabla{Cylinder(x,y,z)}|| = ||(2x,2(y-1),0)|| = 2 ~ 2~\int_{-2}^{2}\int_{0}^{\sqrt{4-2y}}~~2~dydz","['multivariable-calculus', 'surfaces', 'surface-integrals']"
49,Does the following mean value theorem type statement hold in $\mathbb{R}^{n}$,Does the following mean value theorem type statement hold in,\mathbb{R}^{n},"Let $c: [0,1] \rightarrow \mathbb{R}^{n}$ be a $C^1$ curve. Suppose $c$ passes through two points $c(x_1), c(x_2) \in \mathbb{R}^{n}$ with $0 < x_1 < x_1 < 1$ . Does there exist a point $x_0 \in (x_1, x_2)$ such that $Dc(x_0)$ lies on the line $c(x_2)-c(x_1)$ ? I'm not sure if my question makes sense as written, let me know if it doesn't! I will provide some intuition/example for what I want below. This requirement seems much less strict than let's say the false, directly generalized MVT in this question: counterexample for direct generalization of the one-dimensional Mean Value Theorem . In the case of the circle, as in the example in the answer to the above question, one could just slide the secant line determined by $c(x_1)$ and $c(x_2)$ until it became a tangent line, which would correspond to the point $x_0$ we are looking for. In $\mathbb{R}^{3}$ this becomes a bit more confusing. For instance, the curve $c$ can move away from being ""above"" the segment $c(x_1)c(x_2)$ . However, in that case what I'd like to say is something along the lines of: suppose the origin, $c(x_1)$ , and $c(x_2)$ are not collinear. Then the origin and $c(x_1)$ , the origin and $c(x_2)$ determine a plane. Does there exist a time point $x_0$ such that $Dc(t_0)$ lies in that plane? In general, the idea is similar to the mean value theorem, but most generalizations deal with the magnitude of the derivative, whereas here we just want the tangent to be parallel to the secant. This is probably very simple but given the discussion above, I have trouble formulating exactly what I want.","Let be a curve. Suppose passes through two points with . Does there exist a point such that lies on the line ? I'm not sure if my question makes sense as written, let me know if it doesn't! I will provide some intuition/example for what I want below. This requirement seems much less strict than let's say the false, directly generalized MVT in this question: counterexample for direct generalization of the one-dimensional Mean Value Theorem . In the case of the circle, as in the example in the answer to the above question, one could just slide the secant line determined by and until it became a tangent line, which would correspond to the point we are looking for. In this becomes a bit more confusing. For instance, the curve can move away from being ""above"" the segment . However, in that case what I'd like to say is something along the lines of: suppose the origin, , and are not collinear. Then the origin and , the origin and determine a plane. Does there exist a time point such that lies in that plane? In general, the idea is similar to the mean value theorem, but most generalizations deal with the magnitude of the derivative, whereas here we just want the tangent to be parallel to the secant. This is probably very simple but given the discussion above, I have trouble formulating exactly what I want.","c: [0,1] \rightarrow \mathbb{R}^{n} C^1 c c(x_1), c(x_2) \in \mathbb{R}^{n} 0 < x_1 < x_1 < 1 x_0 \in (x_1, x_2) Dc(x_0) c(x_2)-c(x_1) c(x_1) c(x_2) x_0 \mathbb{R}^{3} c c(x_1)c(x_2) c(x_1) c(x_2) c(x_1) c(x_2) x_0 Dc(t_0)","['real-analysis', 'calculus', 'multivariable-calculus', 'vector-analysis']"
50,How to show that origin is a saddle point for the following function.,How to show that origin is a saddle point for the following function.,,"Let $f:\mathbb{R}^2\to \mathbb{R}$ be defined by $f(x, y)= x^6-2x^2y-x^4y+2y^2$ . Then I need to show that $(0, 0)$ is a saddle point of $f$ . My effort: We have $f_{xx}= 30x^4-4y-12x^2y$ , $f_{xy}= -4x-4x^3$ , $f_{yy}= 4$ . At $(0, 0)$ , we have $$f_{xy}^2-4f_{xx}f_{yy}= 0$$ at $(0, 0)$ . Also $f_{xx}=0$ , means that second order derivative test is inconclusive. So, further test is required. Observe that $f(h, k)-f(0, 0) = h^6-2h^2k-h^4k+2k^2$ . For $h = k$ , we have $$f(h, h)-f(0, 0)= h^6-2h^3-h^5+2h^2$$ As $h$ is near $0$ , $$f(h, h)-f(0, 0)>0.$$ For $h \neq k$ , $$f(h, k)-f(0, 0) = h^6-2h^2k-h^4k+2k^2 = h^2(h^4-2k-h^2k)+2k^2$$ How to proceed further? Am i proceeding in right direction or not?","Let be defined by . Then I need to show that is a saddle point of . My effort: We have , , . At , we have at . Also , means that second order derivative test is inconclusive. So, further test is required. Observe that . For , we have As is near , For , How to proceed further? Am i proceeding in right direction or not?","f:\mathbb{R}^2\to \mathbb{R} f(x, y)= x^6-2x^2y-x^4y+2y^2 (0, 0) f f_{xx}= 30x^4-4y-12x^2y f_{xy}= -4x-4x^3 f_{yy}= 4 (0, 0) f_{xy}^2-4f_{xx}f_{yy}= 0 (0, 0) f_{xx}=0 f(h, k)-f(0, 0) = h^6-2h^2k-h^4k+2k^2 h = k f(h, h)-f(0, 0)= h^6-2h^3-h^5+2h^2 h 0 f(h, h)-f(0, 0)>0. h \neq k f(h, k)-f(0, 0) = h^6-2h^2k-h^4k+2k^2 = h^2(h^4-2k-h^2k)+2k^2","['multivariable-calculus', 'maxima-minima']"
51,Implicit function theorem: The result about equivalence of partial derivatives,Implicit function theorem: The result about equivalence of partial derivatives,,"I am trying to understand how one obtains the result of the Implicit Function Theorem which involves the equivalence of the derivatives as stated in the related Wikipedia page ( https://en.wikipedia.org/wiki/Implicit_function_theorem ): Here, $f$ is a continuous differentiable function $f: \mathbb{R}^{n+m} \to \mathbb{R}^{m}$ . At a point $(a,b)$ , we have $f(a,b) = 0 \in \mathbb{R}^{m}$ . Then in a neighborhood $U \in \mathbb{R}^{n}$ around $a$ , we have a $C^1$ function $g: \mathbb{R}^{n} \to \mathbb{R}^{m}$ such that $f(x,g(x))=0$ and $g(a) = b$ in this neighborhood. The above equation of derivatives holds in this neighborhood as well. I tried to replicate the equation above by applying the chain rule straightforwardly to $f(x,g(x))$ . Considering the total derivative of a single component $f_i$ of $f$ with respect to $x_j$ in $U$ , we should have: $$\nabla_{x_j} f_i = \sum_{t=1}^{m}\dfrac{\partial f_t}{\partial g_t}(x,g(x))\dfrac{\partial g_t}{\partial x_j}(x) + \dfrac{\partial f_i}{\partial x_j}(x,g(x))$$ This is simply the sum of all $f_i$ 's components' derivatives with respect to $x_j$ . Generalizing the above to all $f_i$ $(1 \leq i \leq m)$ : $$\left[\nabla_{x_j} f_1, \dots,  \nabla_{x_j} f_m\right]^T_{m \times 1} = [J_{f,y}(x,g(x))]_{m \times m}\left[\dfrac{\partial g_1}{\partial x_j}(x), \dots,  \dfrac{\partial g_m}{\partial x_j}(x)\right]^T_{m \times 1} + \left[\dfrac{\partial f_1}{\partial x_j}(x,g(x)), \dots,  \dfrac{\partial f_m}{\partial x_j}(x,g(x))\right]^T_{m \times 1}$$ Here $J_{f,y}$ is the Jacobian of $f$ with respect to all $g_t$ components. Now, rearrenging I obtain: $$\left[\dfrac{\partial g_1}{\partial x_j}(x), \dots,  \dfrac{\partial g_m}{\partial x_j}(x)\right]^T_{m \times 1} = [J_{f,y}(x,g(x))]_{m \times m}^{-1}\left[\nabla_{x_j} f_1 - \dfrac{\partial f_1}{\partial x_j}(x,g(x)), \dots,  \nabla_{x_j} f_m - \dfrac{\partial f_m}{\partial x_j}(x,g(x))\right]^T_{m \times 1}$$ This is not quite the result shown on the Wikipedia page, as I have a subtraction of the partial derivatives with respect to $x_j$ from the total derivatives. What am I missing here?","I am trying to understand how one obtains the result of the Implicit Function Theorem which involves the equivalence of the derivatives as stated in the related Wikipedia page ( https://en.wikipedia.org/wiki/Implicit_function_theorem ): Here, is a continuous differentiable function . At a point , we have . Then in a neighborhood around , we have a function such that and in this neighborhood. The above equation of derivatives holds in this neighborhood as well. I tried to replicate the equation above by applying the chain rule straightforwardly to . Considering the total derivative of a single component of with respect to in , we should have: This is simply the sum of all 's components' derivatives with respect to . Generalizing the above to all : Here is the Jacobian of with respect to all components. Now, rearrenging I obtain: This is not quite the result shown on the Wikipedia page, as I have a subtraction of the partial derivatives with respect to from the total derivatives. What am I missing here?","f f: \mathbb{R}^{n+m} \to \mathbb{R}^{m} (a,b) f(a,b) = 0 \in \mathbb{R}^{m} U \in \mathbb{R}^{n} a C^1 g: \mathbb{R}^{n} \to \mathbb{R}^{m} f(x,g(x))=0 g(a) = b f(x,g(x)) f_i f x_j U \nabla_{x_j} f_i = \sum_{t=1}^{m}\dfrac{\partial f_t}{\partial g_t}(x,g(x))\dfrac{\partial g_t}{\partial x_j}(x) + \dfrac{\partial f_i}{\partial x_j}(x,g(x)) f_i x_j f_i (1 \leq i \leq m) \left[\nabla_{x_j} f_1, \dots,  \nabla_{x_j} f_m\right]^T_{m \times 1} = [J_{f,y}(x,g(x))]_{m \times m}\left[\dfrac{\partial g_1}{\partial x_j}(x), \dots,  \dfrac{\partial g_m}{\partial x_j}(x)\right]^T_{m \times 1} + \left[\dfrac{\partial f_1}{\partial x_j}(x,g(x)), \dots,  \dfrac{\partial f_m}{\partial x_j}(x,g(x))\right]^T_{m \times 1} J_{f,y} f g_t \left[\dfrac{\partial g_1}{\partial x_j}(x), \dots,  \dfrac{\partial g_m}{\partial x_j}(x)\right]^T_{m \times 1} = [J_{f,y}(x,g(x))]_{m \times m}^{-1}\left[\nabla_{x_j} f_1 - \dfrac{\partial f_1}{\partial x_j}(x,g(x)), \dots,  \nabla_{x_j} f_m - \dfrac{\partial f_m}{\partial x_j}(x,g(x))\right]^T_{m \times 1} x_j","['real-analysis', 'calculus', 'multivariable-calculus', 'implicit-function-theorem']"
52,Limit of permuted numbers when each of them is approaching $0$,Limit of permuted numbers when each of them is approaching,0,"Let $\sigma:N_n \rightarrow N_n$ be arbitrary permutation of $n$ numbers. Does the following limit exist? $$\lim_{x_i \to 0} \frac{x_1+x_2^2+ \cdots x_n^n}{x_{\sigma(1)}+x_{\sigma(2)}^2+ \cdots {x_{\sigma(n)}^n}}$$ I've considered the case where $\sigma(n) = n$ for some $n \in N$ and concluded that then the limit doesn't exist but I'm having trouble figuring out when $\sigma(n) \neq n \ (\forall \ n \in N)$ . I was given a hint to consider that $\sigma (1) >1$ and $\sigma(n) <n.$ The set $N_n$ I believe to be arbitrary set with $n$ elements, not necessarily natural numbers. Don't have much much more context than that but I don't think this matters for the problem.","Let be arbitrary permutation of numbers. Does the following limit exist? I've considered the case where for some and concluded that then the limit doesn't exist but I'm having trouble figuring out when . I was given a hint to consider that and The set I believe to be arbitrary set with elements, not necessarily natural numbers. Don't have much much more context than that but I don't think this matters for the problem.",\sigma:N_n \rightarrow N_n n \lim_{x_i \to 0} \frac{x_1+x_2^2+ \cdots x_n^n}{x_{\sigma(1)}+x_{\sigma(2)}^2+ \cdots {x_{\sigma(n)}^n}} \sigma(n) = n n \in N \sigma(n) \neq n \ (\forall \ n \in N) \sigma (1) >1 \sigma(n) <n. N_n n,"['limits', 'multivariable-calculus']"
53,Proving limit does not exist (multivariable caculus),Proving limit does not exist (multivariable caculus),,"How do I prove that the limit: $$\lim_{(x,y)\to(0,1)} y\sin(1/x)$$ does not exist. I am absolutely lost. I am not sure if I can substitute $(x,y)$ for another function such as $y=x-1,$ leading to the limit being: $$\lim_{(x-1)\to(1)} (x-1)\sin(1/x)$$ but I think this just complicates things more. Any help is appreciated.",How do I prove that the limit: does not exist. I am absolutely lost. I am not sure if I can substitute for another function such as leading to the limit being: but I think this just complicates things more. Any help is appreciated.,"\lim_{(x,y)\to(0,1)} y\sin(1/x) (x,y) y=x-1, \lim_{(x-1)\to(1)} (x-1)\sin(1/x)","['calculus', 'limits', 'multivariable-calculus']"
54,Chain rule where intermediate variable is a matrix,Chain rule where intermediate variable is a matrix,,"How does one calculate the derivative of a scalar with respect to a matrix using the chain rule where the intermediate variable is a matrix ? For example: $$\frac{\partial L}{\partial \mathbf W} = \frac{\partial L}{\partial \mathbf Y} \frac{\partial \mathbf Y}{\partial \mathbf W}$$ If $\mathbf Y$ were a vector ( $\mathbf y$ ), the chain rule would suggest that we need to sum across all the individual elements of $\mathbf y$ , i.e. $$\frac{\partial L}{\partial \mathbf W} = \sum_i \frac{\partial L}{\partial y_i} \frac{\partial y_i}{\partial \mathbf W} \text{, where $y_i$ is an element of vector $\mathbf y$}$$ Is it OK to assume that the extension of that rule to the case where $\mathbf Y$ is a matrix is as follows? $$\frac{\partial L}{\partial \mathbf W} = \sum_{i,j} \frac{\partial L}{\partial \mathbf Y_{i,j}} \frac{\partial \mathbf Y_{i,j}}{\partial \mathbf W}\text{, where $\mathbf Y_{i,j}$ is an element of matrix $\mathbf Y$}$$","How does one calculate the derivative of a scalar with respect to a matrix using the chain rule where the intermediate variable is a matrix ? For example: If were a vector ( ), the chain rule would suggest that we need to sum across all the individual elements of , i.e. Is it OK to assume that the extension of that rule to the case where is a matrix is as follows?","\frac{\partial L}{\partial \mathbf W} = \frac{\partial L}{\partial \mathbf Y} \frac{\partial \mathbf Y}{\partial \mathbf W} \mathbf Y \mathbf y \mathbf y \frac{\partial L}{\partial \mathbf W} = \sum_i \frac{\partial L}{\partial y_i} \frac{\partial y_i}{\partial \mathbf W} \text{, where y_i is an element of vector \mathbf y} \mathbf Y \frac{\partial L}{\partial \mathbf W} = \sum_{i,j} \frac{\partial L}{\partial \mathbf Y_{i,j}} \frac{\partial \mathbf Y_{i,j}}{\partial \mathbf W}\text{, where \mathbf Y_{i,j} is an element of matrix \mathbf Y}","['matrices', 'multivariable-calculus', 'derivatives', 'vectors', 'matrix-calculus']"
55,Approximating $\int_1^3 \int_1^3 \int_1^3 x^{y^z} \mathrm dx\mathrm dy\mathrm dz$?,Approximating ?,\int_1^3 \int_1^3 \int_1^3 x^{y^z} \mathrm dx\mathrm dy\mathrm dz,"I was given a really nasty integral to approximate, which was $$\int_1^3 \int_1^3 \int_1^3 x^{y^z}\mathrm dx\mathrm dy\mathrm dz$$ I was completely clueless; and my interviewer gave me some hints but I got nowhere. I was told that the order of magnitude is roughly $10^9$ . Could someone help me out? Thank you!","I was given a really nasty integral to approximate, which was I was completely clueless; and my interviewer gave me some hints but I got nowhere. I was told that the order of magnitude is roughly . Could someone help me out? Thank you!",\int_1^3 \int_1^3 \int_1^3 x^{y^z}\mathrm dx\mathrm dy\mathrm dz 10^9,"['integration', 'multivariable-calculus', 'approximation', 'volume']"
56,Change of Coordinate and Chain Rule,Change of Coordinate and Chain Rule,,"I think I am confused about something very fundamental about change of coordinates and how it affects computing the derivatives, and I would really appreciate some clarification. Let me set up my question appropriately: Suppose we have a function $f:\mathbb{R}^n \to \mathbb{R}$ , and I want to compute its gradient $\nabla_{x}f(x_0)$ at some point $x_0 \in \mathbb{R}^n$ . However, the expression of $f$ is very complicated in this coordinate system, and I know that in another coordinate system described by $x \mapsto y$ , where we have a nice invertible linear function $Ax=y$ , the gradient $\nabla_{y}f(y_0)$ in that coordinate system has a very simple form. I want to make use of this simple $\nabla_y f(y_0)$ to compute $\nabla_{x}f(x_0)$ . However, it seems like a simple application of the chain rule does not really give me what I want, because $$\nabla_x f(x_0) = \nabla_x f(AA^{-1}x_0) = A^T\nabla_yf(AA^{-1}x_0) = A^T\nabla_yf(x_0)$$ Now I am stuck. Is there no hope of making use of the gradient in one coordinate system to compute the gradient in another coordinate system like I am trying to do? Am I making some basic mistakes? Are there alternatives? Any clarification/help is appreciated! BTW, I am also curious about a related problem on Hessian calculation. In Hessian Matrix Identity , the following is claimed to hold: $$ H(g,x') = A^tH(f, x)A, \text{ where } g(x')=f(Ax)$$ This is not correct is it? I mean take a simple $f(x)=x^2$ and $g(x')=f(ax)=(ax)^2$ , we have $$\frac{d^2g}{dx'}(x')=2, \text{ while } a\times\frac{d^2f}{dx^2}(x)\times a=2a^2$$","I think I am confused about something very fundamental about change of coordinates and how it affects computing the derivatives, and I would really appreciate some clarification. Let me set up my question appropriately: Suppose we have a function , and I want to compute its gradient at some point . However, the expression of is very complicated in this coordinate system, and I know that in another coordinate system described by , where we have a nice invertible linear function , the gradient in that coordinate system has a very simple form. I want to make use of this simple to compute . However, it seems like a simple application of the chain rule does not really give me what I want, because Now I am stuck. Is there no hope of making use of the gradient in one coordinate system to compute the gradient in another coordinate system like I am trying to do? Am I making some basic mistakes? Are there alternatives? Any clarification/help is appreciated! BTW, I am also curious about a related problem on Hessian calculation. In Hessian Matrix Identity , the following is claimed to hold: This is not correct is it? I mean take a simple and , we have","f:\mathbb{R}^n \to \mathbb{R} \nabla_{x}f(x_0) x_0 \in \mathbb{R}^n f x \mapsto y Ax=y \nabla_{y}f(y_0) \nabla_y f(y_0) \nabla_{x}f(x_0) \nabla_x f(x_0) = \nabla_x f(AA^{-1}x_0) = A^T\nabla_yf(AA^{-1}x_0) = A^T\nabla_yf(x_0)  H(g,x') = A^tH(f, x)A, \text{ where } g(x')=f(Ax) f(x)=x^2 g(x')=f(ax)=(ax)^2 \frac{d^2g}{dx'}(x')=2, \text{ while } a\times\frac{d^2f}{dx^2}(x)\times a=2a^2","['real-analysis', 'calculus', 'linear-algebra', 'analysis', 'multivariable-calculus']"
57,Abuse of notation in the chain rule,Abuse of notation in the chain rule,,"I have a function: $f: \mathbb{R}^p \to \mathbb{R}^n$ . Now let's define the functions $x_i : \mathbb{R}^p \to \mathbb{R}$ , and hence we can define the function $\phi : (u_1,..., u_p) \to (x_1(u_1,...,u_p), ..., x_p(u_1,...,u_p))$ Then my book is defining the partial derivative of $f \circ \phi$ at $u_j$ as $$\frac{\partial f\circ \phi}{\partial u_j} = \sum_{i = 1}^p \frac{\partial x_i}{\partial u_j} \frac{\partial f}{\partial x_i}$$ But it doesn't mean anything to take the partial derivative at a function!?  So $\frac{\partial f}{\partial x_i}$ doesn't make sense, since $x_i$ is a function; I mean we can't calculate the partial derivative at a function. For example it doesn't mean anything to say $\frac{\partial (x^2+y^2)}{\partial xy}$ , right? So I guess this is an abuse of notation and that the right formula is $$\frac{\partial f\circ \phi}{\partial u_j} = \sum_{i = 1}^p \frac{\partial x_i}{\partial u_j} \frac{\partial f}{\partial a_i}$$ where the $a_i$ are independent variables and not functions! Am I correct? Thank you!","I have a function: . Now let's define the functions , and hence we can define the function Then my book is defining the partial derivative of at as But it doesn't mean anything to take the partial derivative at a function!?  So doesn't make sense, since is a function; I mean we can't calculate the partial derivative at a function. For example it doesn't mean anything to say , right? So I guess this is an abuse of notation and that the right formula is where the are independent variables and not functions! Am I correct? Thank you!","f: \mathbb{R}^p \to \mathbb{R}^n x_i : \mathbb{R}^p \to \mathbb{R} \phi : (u_1,..., u_p) \to (x_1(u_1,...,u_p), ..., x_p(u_1,...,u_p)) f \circ \phi u_j \frac{\partial f\circ \phi}{\partial u_j} = \sum_{i = 1}^p \frac{\partial x_i}{\partial u_j} \frac{\partial f}{\partial x_i} \frac{\partial f}{\partial x_i} x_i \frac{\partial (x^2+y^2)}{\partial xy} \frac{\partial f\circ \phi}{\partial u_j} = \sum_{i = 1}^p \frac{\partial x_i}{\partial u_j} \frac{\partial f}{\partial a_i} a_i","['real-analysis', 'calculus', 'multivariable-calculus', 'notation', 'chain-rule']"
58,Characterizing critical points of a complex valued function.,Characterizing critical points of a complex valued function.,,"Consider a function $f : \Bbb R^2 \longrightarrow \Bbb R^2$ . A point $(x_0,y_0) \in \Bbb R^2$ is said to be a regular point of $f$ if $Df\ ((x_0,y_0)) : \Bbb R^2 \longrightarrow \Bbb R^2$ is an invertible linear operator. A point $(x_0,y_0) \in \Bbb R^2$ is said to be a critical point of $f$ if it is not a regular point of $f$ . Since $\Bbb C$ is homeomorphic to $\Bbb R^2$ so I was very eager to know the concept of critical points in the complex plane and I found that the critical points of a complex valued function $f$ over the complex plane are precisely those points $z_0 = x_0 + iy_0 \in \Bbb C$ such that $f'(z_0) = 0$ . How do I relate these two concepts? In fact how do I proof the theorem which states that "" Let $f : \Bbb C \longrightarrow \Bbb C$ be a differentiable function. Then a point $z_0 \in \Bbb C$ is a critical point of $f$ (in the multivariable sense) if and only if $f'(z_0) = 0$ . "" Please help me in proving this theorem. Then it will be really very helpful for me. Thank you very much.","Consider a function . A point is said to be a regular point of if is an invertible linear operator. A point is said to be a critical point of if it is not a regular point of . Since is homeomorphic to so I was very eager to know the concept of critical points in the complex plane and I found that the critical points of a complex valued function over the complex plane are precisely those points such that . How do I relate these two concepts? In fact how do I proof the theorem which states that "" Let be a differentiable function. Then a point is a critical point of (in the multivariable sense) if and only if . "" Please help me in proving this theorem. Then it will be really very helpful for me. Thank you very much.","f : \Bbb R^2 \longrightarrow \Bbb R^2 (x_0,y_0) \in \Bbb R^2 f Df\ ((x_0,y_0)) : \Bbb R^2 \longrightarrow \Bbb R^2 (x_0,y_0) \in \Bbb R^2 f f \Bbb C \Bbb R^2 f z_0 = x_0 + iy_0 \in \Bbb C f'(z_0) = 0 f : \Bbb C \longrightarrow \Bbb C z_0 \in \Bbb C f f'(z_0) = 0","['complex-analysis', 'multivariable-calculus']"
59,"Using cylindrical coordinates, find the volume of the region $D =\{y^2+z^2\le5+x^2,4x^2+y^2+z^2\le25\}$","Using cylindrical coordinates, find the volume of the region","D =\{y^2+z^2\le5+x^2,4x^2+y^2+z^2\le25\}","I want to calculate this volume region, using cylindrical coordinates: $$D=\{y^2+z^2\le5+x^2,4x^2+y^2+z^2\le25\}$$ So, I have a hyperboloid and an ellipsoid. Is it correct to calculate the volume like this: $$2\int_{0}^{2\pi}\int_{0}^{3}\int_{2}^{\frac12\sqrt{25-r^2}}rdxdrd\theta  + 2\int_{0}^{2\pi}\int_{0}^{2}\int_{0}^{\sqrt{5+x^2}}rdrdxd\theta$$ Is there a more efficient method ?","I want to calculate this volume region, using cylindrical coordinates: So, I have a hyperboloid and an ellipsoid. Is it correct to calculate the volume like this: Is there a more efficient method ?","D=\{y^2+z^2\le5+x^2,4x^2+y^2+z^2\le25\} 2\int_{0}^{2\pi}\int_{0}^{3}\int_{2}^{\frac12\sqrt{25-r^2}}rdxdrd\theta  + 2\int_{0}^{2\pi}\int_{0}^{2}\int_{0}^{\sqrt{5+x^2}}rdrdxd\theta","['calculus', 'integration', 'multivariable-calculus', 'definite-integrals']"
60,calculate flux through surface,calculate flux through surface,,"I need to calculate the flux of the vector field $\vec{F}$ through the surface $D$ , where $$\vec{F} = \left<z, \, y \sqrt{x^2 + z^2}, \, -x \right> \\ D = \{x^2+6x+z^2\le 0 \,| -1\le y \le 0\}.$$ So there should be a cylinder (height on $y$ axis) shifted by 3 units (center $x=-3$ ), with cut on $ -1\le y \le 0$ . Cylindrical parametrization: $$\begin{cases} x=r\cos\theta -3\\ y=y\\ z=r\sin\theta \end{cases} $$ I think I can do this problem in two ways: The first one by calculating the flux for each of the 3 surfaces (1 cylinder, 2 disks), and the second one by using the divergence theorem. $$\operatorname{div}F = \sqrt{x^2+y^2} \overset{\text{cylindrical}}{\underset{\text{coordinates}}{=}} \sqrt{r^2-9-6r\cos\theta},$$ so with the divergence theorem, $$\int_{0}^{2\pi}\int_{0}^{3\sqrt{3}}\int_{-1}^{0}{r\sqrt{r^2-9-6r\cos\theta} \, dy \, dr \, d\theta}.$$ Is this correct?","I need to calculate the flux of the vector field through the surface , where So there should be a cylinder (height on axis) shifted by 3 units (center ), with cut on . Cylindrical parametrization: I think I can do this problem in two ways: The first one by calculating the flux for each of the 3 surfaces (1 cylinder, 2 disks), and the second one by using the divergence theorem. so with the divergence theorem, Is this correct?","\vec{F} D \vec{F} = \left<z, \, y \sqrt{x^2 + z^2}, \, -x \right> \\
D = \{x^2+6x+z^2\le 0 \,| -1\le y \le 0\}. y x=-3  -1\le y \le 0 \begin{cases}
x=r\cos\theta -3\\
y=y\\
z=r\sin\theta
\end{cases}
 \operatorname{div}F = \sqrt{x^2+y^2} \overset{\text{cylindrical}}{\underset{\text{coordinates}}{=}} \sqrt{r^2-9-6r\cos\theta}, \int_{0}^{2\pi}\int_{0}^{3\sqrt{3}}\int_{-1}^{0}{r\sqrt{r^2-9-6r\cos\theta} \, dy \, dr \, d\theta}.","['integration', 'multivariable-calculus', 'vector-analysis', 'surface-integrals']"
61,Verify Gauss divergence theorem of $F=xi+yj+zk$ over the sphere $x^{2}+y^{2}+z^{2}=a^{2}$,Verify Gauss divergence theorem of  over the sphere,F=xi+yj+zk x^{2}+y^{2}+z^{2}=a^{2},"Verify Gauss divergence theorem of $F=xi+yj+zk$ over the sphere $x^{2}+y^{2}+z^{2}=a^{2}$ When I evaluate taking normal $N=k$ , I get the answer $2\pi a^{3}$ . But when I take normal to the surface by taking out gradient  of $f(x,y,z)= x^{2}+y^{2}+z^{2}$ I am able to verify theorem. But my course instructor told we can take normal $N=k$ when whenever we are able to translate system in $xy$ plane .  And if plane is given then we have to find gradient otherwise not. Please clarify!!!","Verify Gauss divergence theorem of over the sphere When I evaluate taking normal , I get the answer . But when I take normal to the surface by taking out gradient  of I am able to verify theorem. But my course instructor told we can take normal when whenever we are able to translate system in plane .  And if plane is given then we have to find gradient otherwise not. Please clarify!!!","F=xi+yj+zk x^{2}+y^{2}+z^{2}=a^{2} N=k 2\pi a^{3} f(x,y,z)= x^{2}+y^{2}+z^{2} N=k xy","['calculus', 'multivariable-calculus', 'vectors', 'vector-analysis']"
62,Why is it that the surface integral of the flux of a vector field is the same as the surface integral of the vector field itself?,Why is it that the surface integral of the flux of a vector field is the same as the surface integral of the vector field itself?,,"In other words, this: http://www.math.ucla.edu/~archristian/teaching/32b-w17/week-7.pdf Is this just a definition because what we really care about is how much the vectors are ""pushing"" through the surface? Or is it an actual equality?","In other words, this: http://www.math.ucla.edu/~archristian/teaching/32b-w17/week-7.pdf Is this just a definition because what we really care about is how much the vectors are ""pushing"" through the surface? Or is it an actual equality?",,"['multivariable-calculus', 'stokes-theorem']"
63,Definition of smooth functions on arbitrary subsets of $\mathbb{R}^n$ and partial derivatives,Definition of smooth functions on arbitrary subsets of  and partial derivatives,\mathbb{R}^n,"Let $A$ be an arbitrary subset of $\mathbb{R}^n$ , and let $f:A\to \mathbb{R}$ be a function. We say that $f$ is smooth if for each point $p$ in $A$ there exists an open subset $U$ of $\mathbb{R}^n$ containing $p$ and a smooth map $g:U\to \mathbb{R}$ such that $f$ and $g$ agrees on $A \cap U$ . Now let $U$ be an open subset of $\mathbb{H}^n$ , with $\mathbb{H}^n=\{x\in \mathbb{R}^n:x^n\ge0 \}$ Let $f:U\to \mathbb{R}$ be a smooth function in the above sense. Let $p\in U \cap \partial\mathbb{H}^n$ . Thus there are $\tilde U$ open subset of $\mathbb{R}^n$ and a smooth map $g:\tilde U\to \mathbb{R}$ such that $g$ and $f$ agrees on $U \cap \tilde U$ . I want to show that the partial derivatives of $f$ at $p$ are determined by their values in Int $\mathbb{H}^n$ , and therefore in particular are indipendent of the choice of exstension i.e. of the choice of $g$ . Here is my argument. $$\lim_{t\to0^+}\frac{f(p+te_i)-f(p)}{t}=\lim_{t\to0^+}\frac{g(p+te_i)-g(p)}{t}\qquad [1]$$ beacuse $f(p)=g(p)$ and for sufficient positive small $t$ we have $p+te_i \in U \cap \tilde U$ and so $f(p+te_i)=g(p+te_i)$ . Now, since $g$ is smooth at $p$ by hypothesis, we have that the right hand side of the above equation exists, and we have $$\lim_{t\to0^+}\frac{g(p+te_i)-g(p)}{t}=\lim_{t\to0^-}\frac{g(p+te_i)-g(p)}{t}=:\partial_i|_p g$$ So I can define $\partial_i|_p f:=\partial_i|_p g$ and this value is indipendent of the choice of $g$ by $[1]$ . So I have shown the claim in the above yellow box. Is my argument right? In John M. Lee Textbook Introduction to smooth manifolds, on page 27, he says that By continuity , all partial derivatives of $f$ at points of $U\cap \partial\mathbb{H}^n$ are determined by their values in Int $\mathbb{H}^n$ , and therefore in particular are indipendent of the choice of exstension. But it seems I dont'use the continuity of any function, so I'm not convinced of having understood the matter.","Let be an arbitrary subset of , and let be a function. We say that is smooth if for each point in there exists an open subset of containing and a smooth map such that and agrees on . Now let be an open subset of , with Let be a smooth function in the above sense. Let . Thus there are open subset of and a smooth map such that and agrees on . I want to show that the partial derivatives of at are determined by their values in Int , and therefore in particular are indipendent of the choice of exstension i.e. of the choice of . Here is my argument. beacuse and for sufficient positive small we have and so . Now, since is smooth at by hypothesis, we have that the right hand side of the above equation exists, and we have So I can define and this value is indipendent of the choice of by . So I have shown the claim in the above yellow box. Is my argument right? In John M. Lee Textbook Introduction to smooth manifolds, on page 27, he says that By continuity , all partial derivatives of at points of are determined by their values in Int , and therefore in particular are indipendent of the choice of exstension. But it seems I dont'use the continuity of any function, so I'm not convinced of having understood the matter.",A \mathbb{R}^n f:A\to \mathbb{R} f p A U \mathbb{R}^n p g:U\to \mathbb{R} f g A \cap U U \mathbb{H}^n \mathbb{H}^n=\{x\in \mathbb{R}^n:x^n\ge0 \} f:U\to \mathbb{R} p\in U \cap \partial\mathbb{H}^n \tilde U \mathbb{R}^n g:\tilde U\to \mathbb{R} g f U \cap \tilde U f p \mathbb{H}^n g \lim_{t\to0^+}\frac{f(p+te_i)-f(p)}{t}=\lim_{t\to0^+}\frac{g(p+te_i)-g(p)}{t}\qquad [1] f(p)=g(p) t p+te_i \in U \cap \tilde U f(p+te_i)=g(p+te_i) g p \lim_{t\to0^+}\frac{g(p+te_i)-g(p)}{t}=\lim_{t\to0^-}\frac{g(p+te_i)-g(p)}{t}=:\partial_i|_p g \partial_i|_p f:=\partial_i|_p g g [1] f U\cap \partial\mathbb{H}^n \mathbb{H}^n,"['multivariable-calculus', 'differential-geometry', 'smooth-manifolds', 'smooth-functions']"
64,Prove $F(t)^2-G(t)^2=2tF(t)G(t)$,Prove,F(t)^2-G(t)^2=2tF(t)G(t),"Let $F, G:]0,\infty[ \to \mathbb R$ be $$F(t):=\int_{[0,\infty[}e^{-tx^2}\cos{x^2}d\lambda(x)$$ and $$G(t):=\int_{[0,\infty[}e^{-tx^2}\sin{x^2}d\lambda(x)\,.$$ Prove that $$F(t)^2-G(t)^2=2tF(t)G(t)\,.$$ Note: $F(t)^2-G(t)^2=(\int_{[0,\infty[}e^{-tx^2}\cos{x^2}d\lambda(x))^2-(\int_{[0,\infty[}e^{-tx^2}\sin{x^2}d\lambda(x))^2$ Specifically looking at $(\int_{[0,\infty[}e^{-tx^2}\cos{x^2}d\lambda(x))^2=\int_{[0,\infty[}e^{-tx^2}\cos{x^2}d\lambda(x)\times \int_{[0,\infty[}e^{-tx^2}\cos{x^2}d\lambda(x)$ am I allowed to state: $\int_{[0,\infty[}e^{-tx^2}\cos{x^2}d\lambda(x)\times \int_{[0,\infty[}e^{-tx^2}\cos{x^2}d\lambda(x)=\int_{[0,\infty[^2}e^{-tx^2}\cos{x^2}d\lambda^{2}(x)$ ? And then I assume I should substitute $y = x^2$ $\int_{[0,\infty[}2x(\int_{[0,\infty[}e^{-ty}\cos{y}dy)dx$ I can integrate $e^{-y}\cos{y}$ tediously by parts, however, I am not sure what to do with the parameter $t$ in $e^{-ty}\cos{y}$ , as in the above case. Any support is greatly appreciated.","Let be and Prove that Note: Specifically looking at am I allowed to state: ? And then I assume I should substitute I can integrate tediously by parts, however, I am not sure what to do with the parameter in , as in the above case. Any support is greatly appreciated.","F, G:]0,\infty[ \to \mathbb R F(t):=\int_{[0,\infty[}e^{-tx^2}\cos{x^2}d\lambda(x) G(t):=\int_{[0,\infty[}e^{-tx^2}\sin{x^2}d\lambda(x)\,. F(t)^2-G(t)^2=2tF(t)G(t)\,. F(t)^2-G(t)^2=(\int_{[0,\infty[}e^{-tx^2}\cos{x^2}d\lambda(x))^2-(\int_{[0,\infty[}e^{-tx^2}\sin{x^2}d\lambda(x))^2 (\int_{[0,\infty[}e^{-tx^2}\cos{x^2}d\lambda(x))^2=\int_{[0,\infty[}e^{-tx^2}\cos{x^2}d\lambda(x)\times \int_{[0,\infty[}e^{-tx^2}\cos{x^2}d\lambda(x) \int_{[0,\infty[}e^{-tx^2}\cos{x^2}d\lambda(x)\times \int_{[0,\infty[}e^{-tx^2}\cos{x^2}d\lambda(x)=\int_{[0,\infty[^2}e^{-tx^2}\cos{x^2}d\lambda^{2}(x) y = x^2 \int_{[0,\infty[}2x(\int_{[0,\infty[}e^{-ty}\cos{y}dy)dx e^{-y}\cos{y} t e^{-ty}\cos{y}","['real-analysis', 'integration', 'measure-theory', 'multivariable-calculus', 'lebesgue-integral']"
65,tough question in Line integral / Multivariable Calculas,tough question in Line integral / Multivariable Calculas,,"Let $C$ be a curve in the $(x − y)$ -plane. For every point $(x, y)$ of $C$ let $u(x, y)$ denote the unit vector in the direction of the tangent line   to $C$ at $(x, y)$ . Let $S$ be the surface obtained by taking the union of   all straight line segments connecting $(1, 2, 3)$ to points of $C$ . Express   the area of $S$ as an integral of the first type , on the curve $C$ , of some   function of $x$ and $y$ . (hint: try to use the function $u(x, y)$ .) really Hard question , i couldn't understand how to use the fact that line integral will help here since i don't have a function $f(x,y)$ to calculate $ \int_{C} f(x(t),y(t))\sqrt{x'(t)^2 + y'(t)^2 }\ dt$ also i can parameterize $S$ like that : $S=:k(1-x(t),2-y(t),3) , k\in[0,1]$ where $(x(t),y(t),0)$ is the curve $C$ and why $u(x,y)$ is given here. Unit vector","Let be a curve in the -plane. For every point of let denote the unit vector in the direction of the tangent line   to at . Let be the surface obtained by taking the union of   all straight line segments connecting to points of . Express   the area of as an integral of the first type , on the curve , of some   function of and . (hint: try to use the function .) really Hard question , i couldn't understand how to use the fact that line integral will help here since i don't have a function to calculate also i can parameterize like that : where is the curve and why is given here. Unit vector","C (x − y) (x, y) C u(x, y) C (x, y) S (1, 2, 3) C S C x y u(x, y) f(x,y)  \int_{C} f(x(t),y(t))\sqrt{x'(t)^2 + y'(t)^2 }\ dt S S=:k(1-x(t),2-y(t),3) , k\in[0,1] (x(t),y(t),0) C u(x,y)","['integration', 'multivariable-calculus']"
66,prove that null set product $R^n$ is of measure zero,prove that null set product  is of measure zero,R^n,"Let $E$ be a set of measure zero. I want to prove that the Cartesian product of $E$ and $R^n$ is also of measure zero. I thought of maybe using intervals to cover the product, as I know $E$ is of measure zero, but I couldn't really work it out.","Let be a set of measure zero. I want to prove that the Cartesian product of and is also of measure zero. I thought of maybe using intervals to cover the product, as I know is of measure zero, but I couldn't really work it out.",E E R^n E,"['calculus', 'measure-theory', 'multivariable-calculus']"
67,Does the expression of a directional derivative in terms of normal derivatives require continuity?,Does the expression of a directional derivative in terms of normal derivatives require continuity?,,"I read on wikipedia that, if $\mathbf{v}=\langle v_1,v_2\rangle$ is a unit vector in $\mathbb{R}^2$ , then the directional derivative $$\nabla_{\mathbf{v}}f(x,y)=v_1\frac{\partial f}{\partial x}+v_2\frac{\partial f}{\partial y}$$ if the partial deriavtives are continuous at $(x,y)$ . However, nowhere else states that it is required that the partial derivatives be continuous, only that (obviously) they must exist. Can someone clarify whether or not it is true that the derivatives must be continuous, and if it is true, can you please explain why? Wikipeida article that says the derivatives must be continuous: https://en.wikipedia.org/wiki/Derivative#Directional_derivatives Other places that don't: http://mathonline.wikidot.com/higher-order-directional-derivatives http://www.astrosen.unam.mx/~aceves/Metodos/ebooks/hildebrand.pdf (page 275-276) http://mathworld.wolfram.com/DirectionalDerivative.html","I read on wikipedia that, if is a unit vector in , then the directional derivative if the partial deriavtives are continuous at . However, nowhere else states that it is required that the partial derivatives be continuous, only that (obviously) they must exist. Can someone clarify whether or not it is true that the derivatives must be continuous, and if it is true, can you please explain why? Wikipeida article that says the derivatives must be continuous: https://en.wikipedia.org/wiki/Derivative#Directional_derivatives Other places that don't: http://mathonline.wikidot.com/higher-order-directional-derivatives http://www.astrosen.unam.mx/~aceves/Metodos/ebooks/hildebrand.pdf (page 275-276) http://mathworld.wolfram.com/DirectionalDerivative.html","\mathbf{v}=\langle v_1,v_2\rangle \mathbb{R}^2 \nabla_{\mathbf{v}}f(x,y)=v_1\frac{\partial f}{\partial x}+v_2\frac{\partial f}{\partial y} (x,y)","['calculus', 'multivariable-calculus', 'partial-derivative']"
68,"Find vector flux of $v=(yz,y^2z, yz^2)$ through the surface of the cylinder $x^2+y^2=1, 0 \leq z \leq 1$",Find vector flux of  through the surface of the cylinder,"v=(yz,y^2z, yz^2) x^2+y^2=1, 0 \leq z \leq 1","I'm given the following vector field : $v=(yz,y^2z, yz^2)$ and I need to find its flux through the cylinder $x^2+y^2=1, 0 \leq z \leq 1$ . I don't have solutions to this exercise, so I don't know if my results are correct. The divergence of $v$ is simply $4yz$ . So if we set up our integral in cylindrical coordinates, we get $$\int_{0}^{1}\int_{0}^{2\pi}\int_{0}^{1}4zr\sin(\theta) r drd\theta dz$$ . Now, if we evaluate this, we get $0$ because $\int_{0}^{2\pi}sin(\theta)=0$ . So the result will simply be $0$ even if the original vector field could seem a bit intimidating. We could also use a symmetry argument : integrating the variable $y$ , which is an odd function, over a symmetric interval, here a circle, will always yield $0$ . Also, the integrals over the two ""surface circles"", where $z=0$ and $z=1$ , also yield $0$ because for the first one, we integrate over $0$ and for the second, we have $4y$ which is again odd over a symmetric interval. Do you agree with this ? Thanks for your help ! Edit : as was pointed out in the comments, if we use the divergence theorem, we don't need to evaluate over surfaces, and thus the integrals over the circles where z=0 resp. z=1 are not required at all. This step would be required if we proceed without using the divergence theorem. In this case, we would also need to evaluate over the circular surface of the cylinder.","I'm given the following vector field : and I need to find its flux through the cylinder . I don't have solutions to this exercise, so I don't know if my results are correct. The divergence of is simply . So if we set up our integral in cylindrical coordinates, we get . Now, if we evaluate this, we get because . So the result will simply be even if the original vector field could seem a bit intimidating. We could also use a symmetry argument : integrating the variable , which is an odd function, over a symmetric interval, here a circle, will always yield . Also, the integrals over the two ""surface circles"", where and , also yield because for the first one, we integrate over and for the second, we have which is again odd over a symmetric interval. Do you agree with this ? Thanks for your help ! Edit : as was pointed out in the comments, if we use the divergence theorem, we don't need to evaluate over surfaces, and thus the integrals over the circles where z=0 resp. z=1 are not required at all. This step would be required if we proceed without using the divergence theorem. In this case, we would also need to evaluate over the circular surface of the cylinder.","v=(yz,y^2z, yz^2) x^2+y^2=1, 0 \leq z \leq 1 v 4yz \int_{0}^{1}\int_{0}^{2\pi}\int_{0}^{1}4zr\sin(\theta) r drd\theta dz 0 \int_{0}^{2\pi}sin(\theta)=0 0 y 0 z=0 z=1 0 0 4y","['real-analysis', 'multivariable-calculus']"
69,Measure of boundary of discontinuity point of integrable function,Measure of boundary of discontinuity point of integrable function,,"Let $B\subset \mathbb{R}^n$ be a closed rectangle, $f:B\to\mathbb{R}$ Reimann integrable and $A\subset B$ a set of points where $f$ is not continous. I`m trying to show that the measure of $\partial A$ is $0$ ,where $\partial A$ is the boundary of $A$ . I know that the measure of $A$ is $0$ ,and that for general $0$ measure $A$ the statement isn`t correct, However, when $A$ is defined as the set of discontinuity points of an integrable function, I can't find a counter example. My attempt of a proof was: Take $A_\epsilon=\{x\in B : osc(f,x)\geq\epsilon\}$ ,and we have $A=\bigcup_{k=1}^\infty A_{\frac{1}{k}}$ We have $\partial A\subset \bigcup_{k=1}^\infty \partial A_{\frac{1}{k}}$ so it is sufficient to show $\partial A_{\frac{1}{k}}$ is of measure zero. $A_{\frac{1}{k}}$ is compact, and of measure $0$ (Following a proof I saw of the Lebesgue theorem), so we can find a finite cover of rectangles as small as we want, this is also a cover of the interior of $A_{\frac{1}{k}}$ , so the interior is of measure 0. Now we have $int(A_{\frac{1}{k}})\cup\partial A_{\frac{1} {k}}=A_{\frac{1}{k}}$ ( $A_{\frac{1}{k}}$ is closed). (*)Since the measure of $int(A_{\frac{1}{k}})=0$ and also the measure of $A_{\frac{1}{k}}=0$ , we must have $\partial A_{\frac{1}{k}}=0$ I`m unsure of my proof since because honestly, I find it hard to believe the statement is true. I`d be glad if someone can provide a counter example, or verify me and tell me this statement is actually correct.","Let be a closed rectangle, Reimann integrable and a set of points where is not continous. I`m trying to show that the measure of is ,where is the boundary of . I know that the measure of is ,and that for general measure the statement isn`t correct, However, when is defined as the set of discontinuity points of an integrable function, I can't find a counter example. My attempt of a proof was: Take ,and we have We have so it is sufficient to show is of measure zero. is compact, and of measure (Following a proof I saw of the Lebesgue theorem), so we can find a finite cover of rectangles as small as we want, this is also a cover of the interior of , so the interior is of measure 0. Now we have ( is closed). (*)Since the measure of and also the measure of , we must have I`m unsure of my proof since because honestly, I find it hard to believe the statement is true. I`d be glad if someone can provide a counter example, or verify me and tell me this statement is actually correct.","B\subset \mathbb{R}^n f:B\to\mathbb{R} A\subset B f \partial A 0 \partial A A A 0 0 A A A_\epsilon=\{x\in B : osc(f,x)\geq\epsilon\} A=\bigcup_{k=1}^\infty A_{\frac{1}{k}} \partial A\subset \bigcup_{k=1}^\infty \partial A_{\frac{1}{k}} \partial A_{\frac{1}{k}} A_{\frac{1}{k}} 0 A_{\frac{1}{k}} int(A_{\frac{1}{k}})\cup\partial A_{\frac{1} {k}}=A_{\frac{1}{k}} A_{\frac{1}{k}} int(A_{\frac{1}{k}})=0 A_{\frac{1}{k}}=0 \partial A_{\frac{1}{k}}=0","['real-analysis', 'integration', 'measure-theory', 'multivariable-calculus']"
70,How to prove that $ | \Phi_u \times \Phi_v | = \sqrt{ \det g} $?,How to prove that ?, | \Phi_u \times \Phi_v | = \sqrt{ \det g} ,Let be $ \Phi $ a parametrization of a surface $ \in \mathbb{R}^3 $ and $g$ the metric tensor. As the title says...how to show that How to prove that $ | \Phi_u \times \Phi_v | = \sqrt{\det g} $ ? I started like this: $$| \Phi_u \times \Phi_v | = \left| \begin{pmatrix} \frac{\delta \phi_1}{ \delta u} \\ \frac{\delta \phi_2}{ \delta u} \\ \frac{\delta \phi_2}{ \delta u} \end{pmatrix} \times \begin{pmatrix} \frac{\delta \phi_1}{ \delta v} \\ \frac{\delta \phi_2}{ \delta v} \\ \frac{\delta \phi_2}{ \delta v} \end{pmatrix} \right| = \left| \begin{pmatrix} \frac{\delta \phi_2}{ \delta u} \frac{\delta \phi_3}{ \delta v}- \frac{\delta \phi_3}{ \delta u} \frac{\delta \phi_2}{ \delta v}\\ \frac{\delta \phi_1}{ \delta u} \frac{\delta \phi_3}{ \delta v}- \frac{\delta \phi_3}{ \delta u} \frac{\delta \phi_1}{ \delta v}\\ \frac{\delta \phi_1}{ \delta u} \frac{\delta \phi_2}{ \delta v}- \frac{\delta \phi_2}{ \delta u} \frac{\delta \phi_1}{ \delta v}\end{pmatrix} \right|$$ I am not sure how to proceed..If I continue I am coming to a dead end..hope you can help me out a bit :) !,Let be a parametrization of a surface and the metric tensor. As the title says...how to show that How to prove that ? I started like this: I am not sure how to proceed..If I continue I am coming to a dead end..hope you can help me out a bit :) !, \Phi   \in \mathbb{R}^3  g  | \Phi_u \times \Phi_v | = \sqrt{\det g}  | \Phi_u \times \Phi_v | = \left| \begin{pmatrix} \frac{\delta \phi_1}{ \delta u} \\ \frac{\delta \phi_2}{ \delta u} \\ \frac{\delta \phi_2}{ \delta u} \end{pmatrix} \times \begin{pmatrix} \frac{\delta \phi_1}{ \delta v} \\ \frac{\delta \phi_2}{ \delta v} \\ \frac{\delta \phi_2}{ \delta v} \end{pmatrix} \right| = \left| \begin{pmatrix} \frac{\delta \phi_2}{ \delta u} \frac{\delta \phi_3}{ \delta v}- \frac{\delta \phi_3}{ \delta u} \frac{\delta \phi_2}{ \delta v}\\ \frac{\delta \phi_1}{ \delta u} \frac{\delta \phi_3}{ \delta v}- \frac{\delta \phi_3}{ \delta u} \frac{\delta \phi_1}{ \delta v}\\ \frac{\delta \phi_1}{ \delta u} \frac{\delta \phi_2}{ \delta v}- \frac{\delta \phi_2}{ \delta u} \frac{\delta \phi_1}{ \delta v}\end{pmatrix} \right|,"['integration', 'multivariable-calculus', 'parametrization']"
71,Differential forms and order of integration,Differential forms and order of integration,,"I don't understand how $$   \int_{a_2}^{b_2} \int_{a_1}^{b_1} f(t_1,t_2) dt_1 dt_2 =     \int_{a_1}^{b_1} \int_{a_2}^{b_2} f(t_1,t_2) dt_2 dt_1 $$ can agree with the fact that $dt_1 \wedge dt_2 = -dt_2 \wedge dt_1$ . I tried to work with pullbacks, but I must be doing something wrong. I'd really like to see a very low-level step-by-step derivation of the equality above from the point of view of differential forms.","I don't understand how can agree with the fact that . I tried to work with pullbacks, but I must be doing something wrong. I'd really like to see a very low-level step-by-step derivation of the equality above from the point of view of differential forms.","
  \int_{a_2}^{b_2} \int_{a_1}^{b_1} f(t_1,t_2) dt_1 dt_2 =
    \int_{a_1}^{b_1} \int_{a_2}^{b_2} f(t_1,t_2) dt_2 dt_1
 dt_1 \wedge dt_2 = -dt_2 \wedge dt_1","['integration', 'multivariable-calculus', 'differential-forms', 'order-of-integration']"
72,How to calculate the line integral with respect to the circle in counterclockwise direction,How to calculate the line integral with respect to the circle in counterclockwise direction,,"Consider the vector field $F=<y,-x>$ . Compute the line integral $$\int_CF\cdot dr$$ where $C$ is the circle of radius $3$ centered at the origin counterclockwise. My Try: The circle is $x^2+y^2=9$ $$\cases{x=3\cos t \\ y=3\sin t} \text{ for } 0\le t\le2\pi$$ Now how do I calculate $\int_CF\cdot dr$ ? Can anyone explain how to solve this?",Consider the vector field . Compute the line integral where is the circle of radius centered at the origin counterclockwise. My Try: The circle is Now how do I calculate ? Can anyone explain how to solve this?,"F=<y,-x> \int_CF\cdot dr C 3 x^2+y^2=9 \cases{x=3\cos t \\ y=3\sin t} \text{ for } 0\le t\le2\pi \int_CF\cdot dr","['calculus', 'integration', 'multivariable-calculus', 'line-integrals']"
73,$U\subseteq\mathbb{R}^n$ is open and connected $f:U\to \mathbb{R}^m$ differentiable.$Df(x)=0$ $\forall x \in U$. then $f$ is constant.,is open and connected  differentiable. . then  is constant.,U\subseteq\mathbb{R}^n f:U\to \mathbb{R}^m Df(x)=0 \forall x \in U f,"Attempt: Since U is open and connected, U is path connected. then there exists a path $\gamma(t):[0,1]\to U$ between any points $a,b\in U$ such that $\gamma(1)=b$ and $\gamma(0)=a$ Define $h(t)=(f\circ\gamma)(t):\mathbb{R}\to \mathbb{R}^m$ What I want to do: $h(1)-h(0)=\int_0^1 \frac{d}{dx}f(\gamma(t))dt=\int_0^1Df(\gamma(t))\gamma^\prime(t)dt=0$ then $f(b)=f(a)$ $\forall a,b\in U$ so $f$ is constant. But I don't know that this path is differentiable, so this approach doesn't work.","Attempt: Since U is open and connected, U is path connected. then there exists a path between any points such that and Define What I want to do: then so is constant. But I don't know that this path is differentiable, so this approach doesn't work.","\gamma(t):[0,1]\to U a,b\in U \gamma(1)=b \gamma(0)=a h(t)=(f\circ\gamma)(t):\mathbb{R}\to \mathbb{R}^m h(1)-h(0)=\int_0^1 \frac{d}{dx}f(\gamma(t))dt=\int_0^1Df(\gamma(t))\gamma^\prime(t)dt=0 f(b)=f(a) \forall a,b\in U f","['multivariable-calculus', 'connectedness']"
74,A Problem of Lagrange Multiplier,A Problem of Lagrange Multiplier,,"The problem is find the minimum value of $x^2+y^2+z^2$ subject to the condition $x+y+z=1$ and $xyz+1=0$ . Let $f(x,y,z)=x^2+y^2+z^2$ , then after some calculation I got this two equations: $4+6\lambda_1+\lambda_2(1-f)=0 $ and $2f+\lambda_1-3\lambda_2=0$ Now I can solve these two equations to find $\lambda_1$ and $\lambda_2$ in terms for $f$ . Now I cant understand how to proceed. Please help with explanation.","The problem is find the minimum value of subject to the condition and . Let , then after some calculation I got this two equations: and Now I can solve these two equations to find and in terms for . Now I cant understand how to proceed. Please help with explanation.","x^2+y^2+z^2 x+y+z=1 xyz+1=0 f(x,y,z)=x^2+y^2+z^2 4+6\lambda_1+\lambda_2(1-f)=0  2f+\lambda_1-3\lambda_2=0 \lambda_1 \lambda_2 f","['multivariable-calculus', 'lagrange-multiplier', 'maxima-minima']"
75,"Prove that the polynomial is $g(x,y)(x^2 + y^2 -1)^2 + c$",Prove that the polynomial is,"g(x,y)(x^2 + y^2 -1)^2 + c","This is from a Brazilian math contest for college students (OBMU): Let $f(x,y)$ be a polynomial in two real variables such that the polynomials $$\frac{\partial f}{\partial x}(x,y)$$ $$\frac{\partial f}{\partial y}(x,y)$$ are divisible by $x^2+y^2-1$ . Prove that there's a polynomial $g(x,y)$ and a constant $c$ such that $$f(x,y) = g(x,y)(x^2+y^2 -1)^2 +c$$",This is from a Brazilian math contest for college students (OBMU): Let be a polynomial in two real variables such that the polynomials are divisible by . Prove that there's a polynomial and a constant such that,"f(x,y) \frac{\partial f}{\partial x}(x,y) \frac{\partial f}{\partial y}(x,y) x^2+y^2-1 g(x,y) c f(x,y) = g(x,y)(x^2+y^2 -1)^2 +c","['real-analysis', 'multivariable-calculus', 'contest-math', 'multivariate-polynomial']"
76,Specific case of Mean Value Theorem for partial derivatives,Specific case of Mean Value Theorem for partial derivatives,,"Let $f: \Omega \subseteq \mathbb{R}^{n} \longrightarrow \mathbb{R}$ be a continuous function in the closed segment $[x,y] \subset \Omega $ , such that the partial derivative with respect to the j-th variable $( \frac{\partial f(x)}{ \partial x_{j}})$ is defined in the segment $ (x,y) $ . Prove that $ \exists z \in (x,y) $ such that: $$f(y) - f(x) = \frac{\partial f(z)}{ \partial x_{j}} (y_{j} - x_{j})$$ Honestly I'm really surprised that I ended up having to ask this, because at first I thought the prove would just  closely follow the same structure from similar theorems. But my main problem is that the conditions I'm given restrict me from using the theorems that I am comfortable with. Most similar questions I've found (like this or this ) refer to different MVTs (which I actually already know), but can't seemingly be applied here. The proof for the first one (it's Theorem 36, just using this as a reference) doesn't work here because it only proves the existence of a directional derivative, where the direction is the one from the segment, so for example in my case it could only prove the directional derivative in the direction $ \frac{y-x}{||y-x||} $ . In fact, the proof does require that the limit $ \lim_{t \rightarrow t_{0}} g(t)$ is only evaluated for points that are in the segment, because otherwise you can't guarantee that the composition of f and g are continuous, and can't apply the single-variable MVT. The second one does imply existence of all partial derivatives, but it requires differentiability so that's out of the question. What I gathered from both proofs is that they are usually revolved around reducing the multivariable functions down to functions in $ \mathbb{R} $ , where we can use the MVT for the single-variable case. However, I don't know how to do that in this case. I think that my main problem comes from the fact that I don't know what is the direction of the segment $[x,y]$ . Could anyone please give me a hint on how to build a function that lets me reduce this problem to a single-variable case? Or should I take a completely different approach?","Let be a continuous function in the closed segment , such that the partial derivative with respect to the j-th variable is defined in the segment . Prove that such that: Honestly I'm really surprised that I ended up having to ask this, because at first I thought the prove would just  closely follow the same structure from similar theorems. But my main problem is that the conditions I'm given restrict me from using the theorems that I am comfortable with. Most similar questions I've found (like this or this ) refer to different MVTs (which I actually already know), but can't seemingly be applied here. The proof for the first one (it's Theorem 36, just using this as a reference) doesn't work here because it only proves the existence of a directional derivative, where the direction is the one from the segment, so for example in my case it could only prove the directional derivative in the direction . In fact, the proof does require that the limit is only evaluated for points that are in the segment, because otherwise you can't guarantee that the composition of f and g are continuous, and can't apply the single-variable MVT. The second one does imply existence of all partial derivatives, but it requires differentiability so that's out of the question. What I gathered from both proofs is that they are usually revolved around reducing the multivariable functions down to functions in , where we can use the MVT for the single-variable case. However, I don't know how to do that in this case. I think that my main problem comes from the fact that I don't know what is the direction of the segment . Could anyone please give me a hint on how to build a function that lets me reduce this problem to a single-variable case? Or should I take a completely different approach?","f: \Omega \subseteq \mathbb{R}^{n} \longrightarrow \mathbb{R} [x,y] \subset \Omega  ( \frac{\partial f(x)}{ \partial x_{j}})  (x,y)   \exists z \in (x,y)  f(y) - f(x) = \frac{\partial f(z)}{ \partial x_{j}} (y_{j} - x_{j})  \frac{y-x}{||y-x||}   \lim_{t \rightarrow t_{0}} g(t)  \mathbb{R}  [x,y]","['real-analysis', 'multivariable-calculus', 'partial-derivative']"
77,Spherical Laplacians on an Exponential,Spherical Laplacians on an Exponential,,"I looked around a bit and couldn't find a resolution to this. I was curious about the scalar function $u(r) = e^{-r}$ with $r \in [0,\infty)$ and acting Spherical Laplacians on it. $$\Delta u(r) = \frac{1}{r^{2}}\frac{\partial}{\partial r}\Big(r^{2}\frac{\partial u}{\partial r}\Big).\tag{1}$$ Laplacian is straight forward to compute: $$\Delta u = \Big(1 - \frac{2}{r}\Big)u.\tag{2}$$ But computing a second one seems to introduce some ambiguity: $$\Delta^{2}u = \Delta \Delta u = \Delta \Big(1-\frac{2}{r}\Big)u = \Delta\Big(u -\frac{2u}{r}\Big) = \Big(1-\frac{2}{r}\Big)u - \Delta\Big(\frac{2u}{r}\Big) .\tag{3}$$ With the rightmost term being the confusing part to me; If I proceed with distributing the $\Delta$ : $$\Delta\Big(\frac{u}{r}\Big) \overset{?}{=}\  u\ \Delta\Big(\frac{1}{r}\Big) +2\ \nabla u \cdot \nabla\Big(\frac{1}{r}\Big) + \frac{1}{r}\Delta u.\tag{4}$$ As I understand it all of these objects are defined, with: $$\nabla\Big(\frac{1}{r}\Big) = -\frac{1}{r^{2}}\hat{r}\qquad \text{and}\qquad\Delta\Big(\frac{1}{r}\Big) = -4\pi\delta^3(r).\tag{5}$$ Where $\delta^3(r)$ is the 3D Dirac delta distribution. This all seems to suggest that: $$-2\Delta\Big(\frac{u}{r}\Big) = -2\Big(-4\pi\delta^3(r) u + 2\frac{u}{r^{2}} + \frac{1}{r}\Big(1 - \frac{2}{r}\Big)u \Big)\tag{6}$$ $$\require{cancel}-2\Delta\Big(\frac{u}{r}\Big) = -2\Big(-4\pi\delta(r) u + \cancel{2\frac{u}{r^{2}}} + \frac{1}{r}\Big(1 - \cancel{\frac{2}{r}}\Big)u \Big)\tag{7}$$ $$-2\Delta\Big(\frac{u}{r}\Big) = 8\pi\delta^3(r)u - \frac{2u}{r}.\tag{8}$$ Leaving me with: $$\Delta^{2}u = \Big(1 -\frac{4}{r} + 8\pi\delta^3(r)\Big)u.\tag{9}$$ Now the heart of my question is: Are these manipulations correct, or have I assumed something I should have not? Why does $\Delta\Big(\Delta e^{-r}\Big)$ contain a discontinuity (the $\delta^3(r)$ ) when all the $r$ -derivatives exist?","I looked around a bit and couldn't find a resolution to this. I was curious about the scalar function with and acting Spherical Laplacians on it. Laplacian is straight forward to compute: But computing a second one seems to introduce some ambiguity: With the rightmost term being the confusing part to me; If I proceed with distributing the : As I understand it all of these objects are defined, with: Where is the 3D Dirac delta distribution. This all seems to suggest that: Leaving me with: Now the heart of my question is: Are these manipulations correct, or have I assumed something I should have not? Why does contain a discontinuity (the ) when all the -derivatives exist?","u(r) = e^{-r} r \in [0,\infty) \Delta u(r) = \frac{1}{r^{2}}\frac{\partial}{\partial r}\Big(r^{2}\frac{\partial u}{\partial r}\Big).\tag{1} \Delta u = \Big(1 - \frac{2}{r}\Big)u.\tag{2} \Delta^{2}u = \Delta \Delta u = \Delta \Big(1-\frac{2}{r}\Big)u = \Delta\Big(u -\frac{2u}{r}\Big) = \Big(1-\frac{2}{r}\Big)u - \Delta\Big(\frac{2u}{r}\Big) .\tag{3} \Delta \Delta\Big(\frac{u}{r}\Big) \overset{?}{=}\  u\ \Delta\Big(\frac{1}{r}\Big) +2\ \nabla u \cdot \nabla\Big(\frac{1}{r}\Big) + \frac{1}{r}\Delta u.\tag{4} \nabla\Big(\frac{1}{r}\Big) = -\frac{1}{r^{2}}\hat{r}\qquad \text{and}\qquad\Delta\Big(\frac{1}{r}\Big) = -4\pi\delta^3(r).\tag{5} \delta^3(r) -2\Delta\Big(\frac{u}{r}\Big) = -2\Big(-4\pi\delta^3(r) u + 2\frac{u}{r^{2}} + \frac{1}{r}\Big(1 - \frac{2}{r}\Big)u \Big)\tag{6} \require{cancel}-2\Delta\Big(\frac{u}{r}\Big) = -2\Big(-4\pi\delta(r) u + \cancel{2\frac{u}{r^{2}}} + \frac{1}{r}\Big(1 - \cancel{\frac{2}{r}}\Big)u \Big)\tag{7} -2\Delta\Big(\frac{u}{r}\Big) = 8\pi\delta^3(r)u - \frac{2u}{r}.\tag{8} \Delta^{2}u = \Big(1 -\frac{4}{r} + 8\pi\delta^3(r)\Big)u.\tag{9} \Delta\Big(\Delta e^{-r}\Big) \delta^3(r) r","['multivariable-calculus', 'vector-analysis', 'distribution-theory', 'dirac-delta']"
78,What's $\frac{\partial}{\partial A}A?$,What's,\frac{\partial}{\partial A}A?,"What's $\frac{\partial}{\partial A}A?$ In this Python App it says that it's $I\otimes I$ , but how can it be? $\frac{\partial}{\partial A}A=\left[\frac{\partial}{\partial A_{ij}}A\right]=[\frac{\partial}{\partial A_{ij}}A_{lk}]$ that equals to 1 only when $l=i$ and $k=j$ ,i.e. $\frac{\partial}{\partial A_{ij}}A$ is a matrix of $A$ 's dimensions, but only one entry is 1. All the others are zero...","What's In this Python App it says that it's , but how can it be? that equals to 1 only when and ,i.e. is a matrix of 's dimensions, but only one entry is 1. All the others are zero...",\frac{\partial}{\partial A}A? I\otimes I \frac{\partial}{\partial A}A=\left[\frac{\partial}{\partial A_{ij}}A\right]=[\frac{\partial}{\partial A_{ij}}A_{lk}] l=i k=j \frac{\partial}{\partial A_{ij}}A A,"['multivariable-calculus', 'matrix-calculus']"
79,Second derivative of polar coordinates,Second derivative of polar coordinates,,"How do I express $\dfrac{\partial^2z}{\partial\theta^2}$ in terms of Cartesian coordinates given that $(x,y)$ are Cartesian coordinates and $(r,\theta)$ are polar coordinates. Attempt: $$ \frac{\partial^2z}{\partial\theta^2} =  \frac{\partial }{\partial \theta}\left[\frac{\partial z }{\partial x}\frac{\partial x}{\partial \theta}+\frac{\partial z}{\partial y}\frac{\partial y}{\partial \theta}\right] $$ I'm not entirely sure if I am on the right track because z isn't specified so simplifying that expression down isn't possible.",How do I express in terms of Cartesian coordinates given that are Cartesian coordinates and are polar coordinates. Attempt: I'm not entirely sure if I am on the right track because z isn't specified so simplifying that expression down isn't possible.,"\dfrac{\partial^2z}{\partial\theta^2} (x,y) (r,\theta) 
\frac{\partial^2z}{\partial\theta^2} = 
\frac{\partial }{\partial \theta}\left[\frac{\partial z }{\partial x}\frac{\partial x}{\partial \theta}+\frac{\partial z}{\partial y}\frac{\partial y}{\partial \theta}\right]
","['calculus', 'multivariable-calculus', 'partial-derivative']"
80,Conversion of a Vector in a Cartesian Coordinate System to a Cylindrical Coordinate System,Conversion of a Vector in a Cartesian Coordinate System to a Cylindrical Coordinate System,,"I'm having trouble converting a vector from the Cartesian coordinate system to the cylindrical coordinate system (second year vector calculus) Represent the vector $\mathbf A(x,y,z) = z\ \hat i - 2x\ \hat j + y\ \hat k $ in cylindrical coordinates by writing it in the form $$\mathbf A(\rho, \phi, z) = A_1(\rho, \phi, z){\hat \rho} + A_2(\rho, \phi, z){\hat \phi} + A_3(\rho, \phi, z){\hat z}$$ I know that $$x=\rho\cos(\phi) \ \ ,  \ \ y = \rho\sin(\phi) \ \ , \ \ z = z$$ and $$\rho=\sqrt{x^2+y^2} \ \ , \ \ \phi = \arctan\Big(\frac{y}{x}\Big) \ \ , \ \ z = z$$ So I put $$\mathbf A(\rho,\phi,z) = z\ \hat i - 2\rho\cos(\phi) \ \hat j + \rho\sin(\phi) \ \hat k$$ I've tried computing the tangent vectors to the curve: $$\vec \rho = \Big(\frac{\partial A_x}{\partial \rho}, \frac{\partial A_y}{\partial \rho} , \frac{\partial A_z}{\partial \rho} \Big) = (0,-2\cos(\phi),\sin(\phi))$$ $$\vec \phi = \Big(\frac{\partial A_x}{\partial \phi}, \frac{\partial A_y}{\partial \phi} , \frac{\partial A_z}{\partial \phi} \Big) = (0, 2\rho\sin(\phi), \rho\cos(\phi))$$ $$\vec z = \Big(\frac{\partial A_x}{\partial z}, \frac{\partial A_y}{\partial z} , \frac{\partial A_z}{\partial z} \Big) = (1,0,0)$$ Scale factors: $$h_\rho = |\vec \rho| = \sqrt{4\cos^2(\phi) + \sin^2(\phi)}$$ $$h_\phi = |\vec \phi| = \sqrt{4\rho^2\sin^2(\phi) + \rho^2\cos^2(\phi)}$$ $$h_z = |\vec z| = 1$$ I also know that $$\hat \rho = \frac{1}{h_\rho}\vec \rho \ \ , \ \ \hat \phi = \frac{1}{h_\phi}\vec \phi \ \ , \ \ \hat z = \frac{1}{h_z}\vec z$$ But from here I'm not sure how to proceed to the correct answer of $$\mathbf A(\rho,\phi,z) = (z\cos(\phi)-\rho\sin(2\phi))\ \hat \rho - (z\sin(\phi)+2\rho\cos^2(\phi))\ \hat \phi + \rho\sin(\phi)\ \hat z$$ Any help would be greatly appreciated, cheers!","I'm having trouble converting a vector from the Cartesian coordinate system to the cylindrical coordinate system (second year vector calculus) Represent the vector in cylindrical coordinates by writing it in the form I know that and So I put I've tried computing the tangent vectors to the curve: Scale factors: I also know that But from here I'm not sure how to proceed to the correct answer of Any help would be greatly appreciated, cheers!","\mathbf A(x,y,z) = z\ \hat i - 2x\ \hat j + y\ \hat k  \mathbf A(\rho, \phi, z) = A_1(\rho, \phi, z){\hat \rho} + A_2(\rho, \phi, z){\hat \phi} + A_3(\rho, \phi, z){\hat z} x=\rho\cos(\phi) \ \ ,  \ \ y = \rho\sin(\phi) \ \ , \ \ z = z \rho=\sqrt{x^2+y^2} \ \ , \ \ \phi = \arctan\Big(\frac{y}{x}\Big) \ \ , \ \ z = z \mathbf A(\rho,\phi,z) = z\ \hat i - 2\rho\cos(\phi) \ \hat j + \rho\sin(\phi) \ \hat k \vec \rho = \Big(\frac{\partial A_x}{\partial \rho}, \frac{\partial A_y}{\partial \rho} , \frac{\partial A_z}{\partial \rho} \Big) = (0,-2\cos(\phi),\sin(\phi)) \vec \phi = \Big(\frac{\partial A_x}{\partial \phi}, \frac{\partial A_y}{\partial \phi} , \frac{\partial A_z}{\partial \phi} \Big) = (0, 2\rho\sin(\phi), \rho\cos(\phi)) \vec z = \Big(\frac{\partial A_x}{\partial z}, \frac{\partial A_y}{\partial z} , \frac{\partial A_z}{\partial z} \Big) = (1,0,0) h_\rho = |\vec \rho| = \sqrt{4\cos^2(\phi) + \sin^2(\phi)} h_\phi = |\vec \phi| = \sqrt{4\rho^2\sin^2(\phi) + \rho^2\cos^2(\phi)} h_z = |\vec z| = 1 \hat \rho = \frac{1}{h_\rho}\vec \rho \ \ , \ \ \hat \phi = \frac{1}{h_\phi}\vec \phi \ \ , \ \ \hat z = \frac{1}{h_z}\vec z \mathbf A(\rho,\phi,z) = (z\cos(\phi)-\rho\sin(2\phi))\ \hat \rho - (z\sin(\phi)+2\rho\cos^2(\phi))\ \hat \phi + \rho\sin(\phi)\ \hat z","['multivariable-calculus', 'cylindrical-coordinates', 'curvilinear-coordinates']"
81,Calculating the distance between an ellipse and a point.,Calculating the distance between an ellipse and a point.,,"I have the following parametrization of a curve (an ellipse): \begin{align} x(t) &=\frac{-17(\cos(t)-\sqrt{2}\sin(t))+23}{6} \\ y(t) &=\frac{17(\cos{(t)}-\sqrt{2}\sin(t))+23}{6} \\  z(t) &=\frac{17\sqrt{2}\sin(t)+26}{6} \end{align} for $0\leq t\leq2\pi$ . Now how can I calculate the distance from the point $(1,1,s)$ to this parametric curve in function only of $s$ ?",I have the following parametrization of a curve (an ellipse): for . Now how can I calculate the distance from the point to this parametric curve in function only of ?,"\begin{align}
x(t) &=\frac{-17(\cos(t)-\sqrt{2}\sin(t))+23}{6} \\
y(t) &=\frac{17(\cos{(t)}-\sqrt{2}\sin(t))+23}{6} \\ 
z(t) &=\frac{17\sqrt{2}\sin(t)+26}{6}
\end{align} 0\leq t\leq2\pi (1,1,s) s",['multivariable-calculus']
82,Directional derivative and composite functions,Directional derivative and composite functions,,"We have a function $f(p_0)$ with $p_0 \in \mathbb{R}^n$ , and the vector $\vec{v}$ with $||\vec{v}|| = 1$ . The derivative of $f(p)$ to $\lambda$ in the point $p = p_0 + \lambda \vec{v}$ , calculated for $\lambda = 0$ , is called directional derivative along $\vec{v}$ and is indicated with the symbol $$\left( \dfrac{\partial f}{\partial v} \right)_{p_{0}}$$ From the composite function derivation rules, must be $$\left[ \dfrac{d}{d \lambda} f(p_0 + \lambda \vec{v})\right]_{\lambda = 0} = f'_{x_1}(p_0) \cdot v_1 +... f'_{x_n}(p_0) \cdot v_n$$ Although I have consulted several books, this passage is still not clear to me, especially how the function $f(p_0 + \lambda \vec{v})$ should be a composite function. Can you help me? Thank you in advance","We have a function with , and the vector with . The derivative of to in the point , calculated for , is called directional derivative along and is indicated with the symbol From the composite function derivation rules, must be Although I have consulted several books, this passage is still not clear to me, especially how the function should be a composite function. Can you help me? Thank you in advance",f(p_0) p_0 \in \mathbb{R}^n \vec{v} ||\vec{v}|| = 1 f(p) \lambda p = p_0 + \lambda \vec{v} \lambda = 0 \vec{v} \left( \dfrac{\partial f}{\partial v} \right)_{p_{0}} \left[ \dfrac{d}{d \lambda} f(p_0 + \lambda \vec{v})\right]_{\lambda = 0} = f'_{x_1}(p_0) \cdot v_1 +... f'_{x_n}(p_0) \cdot v_n f(p_0 + \lambda \vec{v}),"['multivariable-calculus', 'functions']"
83,Show that $\lim_{\alpha \to \infty} \int_{\mathbb{R}^n} f(x) \phi_{\alpha}(x - x_0) dx = f(x_0)$,Show that,\lim_{\alpha \to \infty} \int_{\mathbb{R}^n} f(x) \phi_{\alpha}(x - x_0) dx = f(x_0),"Given is a integrable function $\phi: \mathbb{R}^n \mapsto \mathbb{R}$ with the property $\int_{\mathbb{R}^n} \phi  dx = 1$ . We define for $\alpha > 0$ the re-scaled function $\phi_\alpha(x):= \alpha^n \phi(\alpha x) $ . Now I should show that for every continuous and bounded function $f: \mathbb{R}^n \mapsto \mathbb{R}$ and for every $x_0 \in \mathbb{R}^n$ the following holds: $\lim_{\alpha \to \infty} \int_{\mathbb{R}^n} f(x) \phi_{\alpha}(x - x_0) dx = f(x_0)$ So I've done some research and found that the properties of $\phi$ are very similar to something called the dirac-delta function $\delta$ . One property of $\delta$ is that $|\alpha| \delta(\alpha x) = \delta(x)$ . I will now prove that something similar holds for $\phi$ , which means I will show that $\alpha^n \phi(\alpha x) = \phi(x)$ ( $\alpha$ already is $>0$ so I can leave out the absolute value). Using multivariable substitution ( $u = \alpha x$ ): $\int_{\mathbb{R}^n} \alpha^n \phi(\alpha x) dx = \int_{\mathbb{R}^n} \alpha^n \phi(u) \frac{1}{\alpha^n} du = \int_{\mathbb{R}^n} \phi(u)du = 1$ If this is correct, my problem becomes easier and I only have to show that $\int_{\mathbb{R}^n} f(x)\phi(x-x_0) = f(x_0)$ I am not sure how to continue from this point on, but my guess would be that using the property of $f$ being bounded might prove useful.  Also please correct me on mistakes I have done thus far. EDIT: Thanks to help in the comments I maybe figured out how to correct my mistakes and continue: Using multivariable substitution ( $u = \alpha x - \alpha x_0$ ): $\lim_{\alpha \to \infty} \int_{\mathbb{R}^n} f(x) \alpha^n \phi(\alpha x) dx = \lim_{\alpha \to \infty} \int_{\mathbb{R}^n} f(x_0 +\frac{u}{\alpha}) \phi(u) du$ Now if it is possible to get $\lim$ inside of the integral we have: $\int_{\mathbb{R}^n} \lim_{\alpha \to \infty}  f(x_0 +\frac{u}{\alpha}) \phi(u) du = \int_{\mathbb{R}^n} f(x_0) \phi(u) du = f(x_0)\int_{\mathbb{R}^n} \phi(u) du = f(x_0)$ Is this correct? If yes, which theorem can I use to get the limit inside the integral?","Given is a integrable function with the property . We define for the re-scaled function . Now I should show that for every continuous and bounded function and for every the following holds: So I've done some research and found that the properties of are very similar to something called the dirac-delta function . One property of is that . I will now prove that something similar holds for , which means I will show that ( already is so I can leave out the absolute value). Using multivariable substitution ( ): If this is correct, my problem becomes easier and I only have to show that I am not sure how to continue from this point on, but my guess would be that using the property of being bounded might prove useful.  Also please correct me on mistakes I have done thus far. EDIT: Thanks to help in the comments I maybe figured out how to correct my mistakes and continue: Using multivariable substitution ( ): Now if it is possible to get inside of the integral we have: Is this correct? If yes, which theorem can I use to get the limit inside the integral?",\phi: \mathbb{R}^n \mapsto \mathbb{R} \int_{\mathbb{R}^n} \phi  dx = 1 \alpha > 0 \phi_\alpha(x):= \alpha^n \phi(\alpha x)  f: \mathbb{R}^n \mapsto \mathbb{R} x_0 \in \mathbb{R}^n \lim_{\alpha \to \infty} \int_{\mathbb{R}^n} f(x) \phi_{\alpha}(x - x_0) dx = f(x_0) \phi \delta \delta |\alpha| \delta(\alpha x) = \delta(x) \phi \alpha^n \phi(\alpha x) = \phi(x) \alpha >0 u = \alpha x \int_{\mathbb{R}^n} \alpha^n \phi(\alpha x) dx = \int_{\mathbb{R}^n} \alpha^n \phi(u) \frac{1}{\alpha^n} du = \int_{\mathbb{R}^n} \phi(u)du = 1 \int_{\mathbb{R}^n} f(x)\phi(x-x_0) = f(x_0) f u = \alpha x - \alpha x_0 \lim_{\alpha \to \infty} \int_{\mathbb{R}^n} f(x) \alpha^n \phi(\alpha x) dx = \lim_{\alpha \to \infty} \int_{\mathbb{R}^n} f(x_0 +\frac{u}{\alpha}) \phi(u) du \lim \int_{\mathbb{R}^n} \lim_{\alpha \to \infty}  f(x_0 +\frac{u}{\alpha}) \phi(u) du = \int_{\mathbb{R}^n} f(x_0) \phi(u) du = f(x_0)\int_{\mathbb{R}^n} \phi(u) du = f(x_0),"['real-analysis', 'integration', 'multivariable-calculus']"
84,Understanding partial derivative involving 3 variables,Understanding partial derivative involving 3 variables,,"I am new to partial derivative and I need some help in understanding if what I have done so far is correct. Let $S$ be the surface given by $x^2 + y^2 - 3z^2 = 5$ I want to calculate the partial derivative: $\frac{\partial z}{\partial x}$ at the point $(2,2,1)$ and $(2,2,-1)$ This is what I have done: $x^2 + y^2 - 3z^2 = 5$ $z^2 = \frac{x^2 + y^2 - 5}{3}$ $z = \pm \sqrt\frac{x^2 + y^2 - 5}{3}$ $\frac{\partial z}{\partial x} = \frac{\frac{1}{2}(x^2 + y^2 - 5)^{-\frac12}(2x)}{\sqrt3}$ $\frac{\partial z}{\partial x} = \frac{2x}{2\sqrt{3}\sqrt{x^2 + y^2 - 5}}$ $\frac{\partial z}{\partial x} = \frac{x}{\sqrt{3}\sqrt{x^2 + y^2 - 5}}$ But I am unsure of how to continue after this, and how to use the points (2,2,1) and (2,2,-1).","I am new to partial derivative and I need some help in understanding if what I have done so far is correct. Let be the surface given by I want to calculate the partial derivative: at the point and This is what I have done: But I am unsure of how to continue after this, and how to use the points (2,2,1) and (2,2,-1).","S x^2 + y^2 - 3z^2 = 5 \frac{\partial z}{\partial x} (2,2,1) (2,2,-1) x^2 + y^2 - 3z^2 = 5 z^2 = \frac{x^2 + y^2 - 5}{3} z = \pm \sqrt\frac{x^2 + y^2 - 5}{3} \frac{\partial z}{\partial x} = \frac{\frac{1}{2}(x^2 + y^2 - 5)^{-\frac12}(2x)}{\sqrt3} \frac{\partial z}{\partial x} = \frac{2x}{2\sqrt{3}\sqrt{x^2 + y^2 - 5}} \frac{\partial z}{\partial x} = \frac{x}{\sqrt{3}\sqrt{x^2 + y^2 - 5}}","['calculus', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
85,"Maximize $x(C-x)A^2+y(D-y)B^2+(x(D-y)+y(C-x))AB$ where $A,B,C,D$ are Constant",Maximize  where  are Constant,"x(C-x)A^2+y(D-y)B^2+(x(D-y)+y(C-x))AB A,B,C,D","I am trying to figure out where the maximum occurs for the following expression of 2 variables ( $x$ and $y$ ): $x(C-x)A^2+y(D-y)B^2+(x(D-y)+y(C-x))AB$ where $-1\leq A,B\leq 1$ , $C\geq 2$ and $D\geq 2$ are all constants. Moreover the range of values allowed for $x$ and $y$ is $0\leq x\leq C$ and $0\leq y\leq D$ . I tried taking partial derivatives but the second order partial derivative test ( https://en.wikipedia.org/wiki/Second_partial_derivative_test ) is inconclusive so I was wondering if there may be another way to approach this.","I am trying to figure out where the maximum occurs for the following expression of 2 variables ( and ): where , and are all constants. Moreover the range of values allowed for and is and . I tried taking partial derivatives but the second order partial derivative test ( https://en.wikipedia.org/wiki/Second_partial_derivative_test ) is inconclusive so I was wondering if there may be another way to approach this.","x y x(C-x)A^2+y(D-y)B^2+(x(D-y)+y(C-x))AB -1\leq A,B\leq 1 C\geq 2 D\geq 2 x y 0\leq x\leq C 0\leq y\leq D","['multivariable-calculus', 'maxima-minima']"
86,Divergence of $1/r$ in cylindrical coordinates,Divergence of  in cylindrical coordinates,1/r,"In classical textbooks, like ""Introduction to Electrodynamics"" by J.D. Griffiths, it is given that $$\nabla\cdot\left(\frac{\widehat{r}}{r^2}\right)=4\pi\delta^3(R).$$ To prove this equality, Griffiths first evaluates the divergence and finds it $0$ for $r \neq 0$ and undefined for $r=0$ . Then evaluates the integral of the given divergence and finds it $4\pi$ . Then, he concludes that the above equality should hold since the integral has a constant value containing origin. Now, I am asked about the following equality : $$\nabla \cdot\left(\frac{\widehat{\rho}}{\rho}\right)=2\pi\delta^2(\rho)$$ (note: the equation is claimed to be valid in cylindrical coordinates). I was unable to work out the integral properly. Any help would be appreciated.","In classical textbooks, like ""Introduction to Electrodynamics"" by J.D. Griffiths, it is given that To prove this equality, Griffiths first evaluates the divergence and finds it for and undefined for . Then evaluates the integral of the given divergence and finds it . Then, he concludes that the above equality should hold since the integral has a constant value containing origin. Now, I am asked about the following equality : (note: the equation is claimed to be valid in cylindrical coordinates). I was unable to work out the integral properly. Any help would be appreciated.",\nabla\cdot\left(\frac{\widehat{r}}{r^2}\right)=4\pi\delta^3(R). 0 r \neq 0 r=0 4\pi \nabla \cdot\left(\frac{\widehat{\rho}}{\rho}\right)=2\pi\delta^2(\rho),"['multivariable-calculus', 'divergence-operator', 'cylindrical-coordinates']"
87,Finding Multivariable limits using polar coordinates,Finding Multivariable limits using polar coordinates,,How do I find the limit of this multivariable function as it goes to zero using polar coordinates? $$	\frac{\sin (x^2 + y^2)}{(x^2 + y^2)^2} $$,How do I find the limit of this multivariable function as it goes to zero using polar coordinates? $$	\frac{\sin (x^2 + y^2)}{(x^2 + y^2)^2} $$,,"['limits', 'multivariable-calculus', 'polar-coordinates']"
88,Double integrals and interchangeable limits,Double integrals and interchangeable limits,,Why do the answers come out to be different in these two cases where only the integrating order has been changed (The first one comes out to be 0.5 and the other one come out to be -0.5). We have learnt that the limits are interchangeable when the limits are constants. This is a little contradicting.: P.S. Sorry for the bad English.,Why do the answers come out to be different in these two cases where only the integrating order has been changed (The first one comes out to be 0.5 and the other one come out to be -0.5). We have learnt that the limits are interchangeable when the limits are constants. This is a little contradicting.: P.S. Sorry for the bad English.,,"['calculus', 'multivariable-calculus']"
89,Continuity of $2$ variable function in $R^2$ when one variable function is differentiable.,Continuity of  variable function in  when one variable function is differentiable.,2 R^2,"I have this question in an assignment and I am unable to figure it out. ""Suppose $f(x,y)$ is a function defined in $R^2$.  Set $g(x) = f(x, 0)$, $h(y) = f(0, y)$.  If $g$ and $h$ are differentiable at $0$ as functions in one variable does it follows that f is continuous at the origin?  (If your answer is ""yes"", provide a proof; if your answer is ""no"", construct a counterexample.)"" I was able to construct a counterexample to show that it does not follow that $f$ is differentiable at the origin but I am not sure if how that relates to the continuity of $f$. Thanks in advance.","I have this question in an assignment and I am unable to figure it out. ""Suppose $f(x,y)$ is a function defined in $R^2$.  Set $g(x) = f(x, 0)$, $h(y) = f(0, y)$.  If $g$ and $h$ are differentiable at $0$ as functions in one variable does it follows that f is continuous at the origin?  (If your answer is ""yes"", provide a proof; if your answer is ""no"", construct a counterexample.)"" I was able to construct a counterexample to show that it does not follow that $f$ is differentiable at the origin but I am not sure if how that relates to the continuity of $f$. Thanks in advance.",,['multivariable-calculus']
90,Proving Multivariable limit Exists,Proving Multivariable limit Exists,,"I am trying to determine and prove the limit for the function $f(x,y)=\frac{y^2-4|y|-2|x|}{|x|+2|y|}$ after setting $y=mx$ I found out that the limit is not dependent on m and -2 is a potential candidate for the limit so I set up my equation for the squeeze theorem $\lim_{(x,y)\to(0,0)} |\frac{y^2-4|y|-2|x|}{|x|+2|y|}+2|=\lim_{(x,y)\to(0,0)}\frac{y^2}{|x|+2|y|}$ and from this point on I'm having a bit of trouble to set up an inequality to satisfy the squeeze theorem. Would I be able to claim that $y^2 \leq y^2(|x|+2|y|)$","I am trying to determine and prove the limit for the function $f(x,y)=\frac{y^2-4|y|-2|x|}{|x|+2|y|}$ after setting $y=mx$ I found out that the limit is not dependent on m and -2 is a potential candidate for the limit so I set up my equation for the squeeze theorem $\lim_{(x,y)\to(0,0)} |\frac{y^2-4|y|-2|x|}{|x|+2|y|}+2|=\lim_{(x,y)\to(0,0)}\frac{y^2}{|x|+2|y|}$ and from this point on I'm having a bit of trouble to set up an inequality to satisfy the squeeze theorem. Would I be able to claim that $y^2 \leq y^2(|x|+2|y|)$",,"['calculus', 'limits', 'multivariable-calculus']"
91,Differentiability on a function from directional derivatives,Differentiability on a function from directional derivatives,,"Let $f:\mathbb{R}^2 \rightarrow \mathbb{R}$ and $a \in \mathbb{R}^2$. Suppose that all $D_v (a)$ exist and that $D_{(v_1, v_2)} f (a)= 5v_1 ^2 + v_2$. Is $f$ differentiable in $a$? I think it is differentiable. The only way I can think of solving this problem is by using the sufficient condition of differentiability: Let $A$ be open in $\mathbb{R}^n$, $a \in A$ and $f:A \rightarrow \mathbb{R}^m$. If the partial derivatives $D_1 f(x),...,D_n f(x)$ exist in an open ball $B(a,r)$ and they are all continuous in $a$ then $f$ is differentiable in $a$. So the values of the partial derivatives in $a$ are $D_1 f (a)= 5 \cdot 1^2+0$ and $D_2 f (a)= 5 \cdot 0^2+1 = 1$  and they exist because all directional derivatives exist by hypothesis. But I don't know how to conclude that the partial derivatives are continuous in $a$ since I don't know their general expression, only their value in $a$.","Let $f:\mathbb{R}^2 \rightarrow \mathbb{R}$ and $a \in \mathbb{R}^2$. Suppose that all $D_v (a)$ exist and that $D_{(v_1, v_2)} f (a)= 5v_1 ^2 + v_2$. Is $f$ differentiable in $a$? I think it is differentiable. The only way I can think of solving this problem is by using the sufficient condition of differentiability: Let $A$ be open in $\mathbb{R}^n$, $a \in A$ and $f:A \rightarrow \mathbb{R}^m$. If the partial derivatives $D_1 f(x),...,D_n f(x)$ exist in an open ball $B(a,r)$ and they are all continuous in $a$ then $f$ is differentiable in $a$. So the values of the partial derivatives in $a$ are $D_1 f (a)= 5 \cdot 1^2+0$ and $D_2 f (a)= 5 \cdot 0^2+1 = 1$  and they exist because all directional derivatives exist by hypothesis. But I don't know how to conclude that the partial derivatives are continuous in $a$ since I don't know their general expression, only their value in $a$.",,"['real-analysis', 'multivariable-calculus']"
92,How to show a real valued function of several variables is analytic?,How to show a real valued function of several variables is analytic?,,"Let $f:\Omega\subset\Bbb{R}^m\to\Bbb{R}^n$ be a given function. In general, how can I show that $f$ is smooth (infinitely differentiable) and analytic. I know this is a bit vaguely stated question, but I just need a general idea about showing smoothness and analyticity. If you can give me some examples that would be great. PDF's and other sources are also welcome.","Let $f:\Omega\subset\Bbb{R}^m\to\Bbb{R}^n$ be a given function. In general, how can I show that $f$ is smooth (infinitely differentiable) and analytic. I know this is a bit vaguely stated question, but I just need a general idea about showing smoothness and analyticity. If you can give me some examples that would be great. PDF's and other sources are also welcome.",,"['real-analysis', 'multivariable-calculus', 'analyticity', 'analytic-functions', 'smooth-functions']"
93,"Implicit Function Theorem to solve $\frac{\partial u}{\partial x}$ when $f\big(x,y,u(x,y),v(x,y)\big)$ mapped to $\mathbb{R^2}$",Implicit Function Theorem to solve  when  mapped to,"\frac{\partial u}{\partial x} f\big(x,y,u(x,y),v(x,y)\big) \mathbb{R^2}","I am given $2$ equations $$f_1=x^2-y^2-u^3+v^2+4=0$$ $$f_2=2xy+y^2-2u^2+3v^4+8=0$$ and asked to show that we can solve the equations for $u$ and $v$ in terms of $x$ and $y$ in a neighbhourood of the solution $(2,-1,2,1)$.  I have applied the implicit function theorem and showed that $det\bigg(\frac{\partial f_1,f_2}{\partial_u,v}\bigg)$ at $(2,-1,2,1)$ is non-zero. My Question They then ask to solve for $\frac{\partial u}{\partial x}$.  This is what I'm having trouble with.  I know that when we, for e.g, have $f(x,g(x))$ mapped to one function then we can solve for $\frac{dy}{x}$ by computing $\frac{dy}{x}=-\frac{f_x}{f_y}$.  I tried applying the same definition to this problem but had some difficulty.  I know that I can derive $f_1$ with respect to $x$ thus getting $\frac{\partial f_1}{\partial x}=2x-3u^2\frac{\partial u}{\partial x} +2v\frac{\partial v}{\partial x}$ and can do the same for $f_2$.  And from there I can eliminate $\frac{\partial v}{\partial x}$ and isolate for $\frac{\partial u}{\partial x}$.  Is this the only procedure to solve for $\frac{\partial u}{\partial x}$ or is there a formula like in the case for one component function. Thanks","I am given $2$ equations $$f_1=x^2-y^2-u^3+v^2+4=0$$ $$f_2=2xy+y^2-2u^2+3v^4+8=0$$ and asked to show that we can solve the equations for $u$ and $v$ in terms of $x$ and $y$ in a neighbhourood of the solution $(2,-1,2,1)$.  I have applied the implicit function theorem and showed that $det\bigg(\frac{\partial f_1,f_2}{\partial_u,v}\bigg)$ at $(2,-1,2,1)$ is non-zero. My Question They then ask to solve for $\frac{\partial u}{\partial x}$.  This is what I'm having trouble with.  I know that when we, for e.g, have $f(x,g(x))$ mapped to one function then we can solve for $\frac{dy}{x}$ by computing $\frac{dy}{x}=-\frac{f_x}{f_y}$.  I tried applying the same definition to this problem but had some difficulty.  I know that I can derive $f_1$ with respect to $x$ thus getting $\frac{\partial f_1}{\partial x}=2x-3u^2\frac{\partial u}{\partial x} +2v\frac{\partial v}{\partial x}$ and can do the same for $f_2$.  And from there I can eliminate $\frac{\partial v}{\partial x}$ and isolate for $\frac{\partial u}{\partial x}$.  Is this the only procedure to solve for $\frac{\partial u}{\partial x}$ or is there a formula like in the case for one component function. Thanks",,"['real-analysis', 'analysis', 'multivariable-calculus', 'proof-writing']"
94,Understanding some definitions of smooth manifolds and the inverse function theorem,Understanding some definitions of smooth manifolds and the inverse function theorem,,"I am an electrical engineering grad student trying to learn some of the basics of differential geometry to better understand some work on signal processing and learning on Grassmanian manifolds. Pretty much I want to understand this paper pretty completely. My relevant background includes a formal undergraduate analysis course, a graduate measure theoretic probability course, and of course a bunch of random engineering mathematics/mathematical physics courses. I'm using the book ""Introduction to Differential Geometry for Engineers"" by Doolin/Martin to try and get up to speed quickly. The first ""definition"" given is the following:  A subset $M$ of $\mathbb{R}^n$ is a k-dimensional manifold if for each $\mathbf{x} \in M$ there are: open subsets $U$ and $V$ of $\mathbb{R}^n$ with $x \in U$  and a diffeomorphism $f$ from $U$ to $V$ such that $f(U \cap M) = \{ \mathbf{y} \in V : y^{k+1}=y^{k+2}=...= y^n = 0 \}$. Thus, each point $y$ in the image of $f$ has a representation like $y = (y^1(x), y^2(x), ... y^k(x),0,...,0)$. The book also quotes an implicit function theorem, which I'll summarize as:  If $F:\mathbb{R}^{n-k}\times\mathbb{R}^{k}\rightarrow\mathbb{R}^k$ is continuously differentiable in an open set containing $(\mathbf{a},\mathbf{b})$ and $F(\mathbf{a},\mathbf{b}) = \mathbf{0}$ then if the Jacobian of F is of rank $k$ then there is an open set $A\in \mathbb{R}^{n-k}$ containing $\mathbf{a}$ and an open set $B \in \mathbb{R}^k$ containing $\mathbf{b}$ such that for each $\mathbf{x}\in A$ there is a unique $g(\mathbf{x})\in B$ such that $F(\mathbf{x}, g(\mathbf{x}))=0$ Furthermore the function $g$ is differentiable. Furthermore, there is a change of coordinates such that assigns $g(\mathbf{x}) = \mathbf{0}$. My question is in regards to the following example from the text: Consider a $C^{\infty}$ function $F$ with the domain $A \in  \mathbb{R}^{n}$ and a range in $\mathbb{R}^k$, $k<n$. Consider the set   $M = \{\mathbf{x}\in \mathbb{R}^n:F(\mathbf{x}) =\mathbf{0}\}$ Let the   rank of the Jacobian of $F$ be equal to $k$ $\forall$ $\mathbf{x}\in M$.   $M$ is an $n-k$ dimensional manifold. They indicate that this is true precisely because in the inverse function theorem there exists the change of coordinates that allows us to set $g(\mathbf{x}) = 0$ above. I don't see why that fact leads to the conclusion that $M$ is a manifold. Am I missing some obvious construction or something? I tried to construct an alternative argument: If I consider the function $f:\mathbb{R}^{n} \rightarrow \mathbb{R}^n$ such that $f( \mathbf{x} ) = [x^{1},x^2,x^3,...,x^n, F(\mathbf{x})]$. I have $f\in C^{\infty}$ and defined on $A$, the domain of $F$, and I think its easy to see that for an open subset of $A$ we have the diffeomorphism as in the ""definition"" above"". Does this argument work?","I am an electrical engineering grad student trying to learn some of the basics of differential geometry to better understand some work on signal processing and learning on Grassmanian manifolds. Pretty much I want to understand this paper pretty completely. My relevant background includes a formal undergraduate analysis course, a graduate measure theoretic probability course, and of course a bunch of random engineering mathematics/mathematical physics courses. I'm using the book ""Introduction to Differential Geometry for Engineers"" by Doolin/Martin to try and get up to speed quickly. The first ""definition"" given is the following:  A subset $M$ of $\mathbb{R}^n$ is a k-dimensional manifold if for each $\mathbf{x} \in M$ there are: open subsets $U$ and $V$ of $\mathbb{R}^n$ with $x \in U$  and a diffeomorphism $f$ from $U$ to $V$ such that $f(U \cap M) = \{ \mathbf{y} \in V : y^{k+1}=y^{k+2}=...= y^n = 0 \}$. Thus, each point $y$ in the image of $f$ has a representation like $y = (y^1(x), y^2(x), ... y^k(x),0,...,0)$. The book also quotes an implicit function theorem, which I'll summarize as:  If $F:\mathbb{R}^{n-k}\times\mathbb{R}^{k}\rightarrow\mathbb{R}^k$ is continuously differentiable in an open set containing $(\mathbf{a},\mathbf{b})$ and $F(\mathbf{a},\mathbf{b}) = \mathbf{0}$ then if the Jacobian of F is of rank $k$ then there is an open set $A\in \mathbb{R}^{n-k}$ containing $\mathbf{a}$ and an open set $B \in \mathbb{R}^k$ containing $\mathbf{b}$ such that for each $\mathbf{x}\in A$ there is a unique $g(\mathbf{x})\in B$ such that $F(\mathbf{x}, g(\mathbf{x}))=0$ Furthermore the function $g$ is differentiable. Furthermore, there is a change of coordinates such that assigns $g(\mathbf{x}) = \mathbf{0}$. My question is in regards to the following example from the text: Consider a $C^{\infty}$ function $F$ with the domain $A \in  \mathbb{R}^{n}$ and a range in $\mathbb{R}^k$, $k<n$. Consider the set   $M = \{\mathbf{x}\in \mathbb{R}^n:F(\mathbf{x}) =\mathbf{0}\}$ Let the   rank of the Jacobian of $F$ be equal to $k$ $\forall$ $\mathbf{x}\in M$.   $M$ is an $n-k$ dimensional manifold. They indicate that this is true precisely because in the inverse function theorem there exists the change of coordinates that allows us to set $g(\mathbf{x}) = 0$ above. I don't see why that fact leads to the conclusion that $M$ is a manifold. Am I missing some obvious construction or something? I tried to construct an alternative argument: If I consider the function $f:\mathbb{R}^{n} \rightarrow \mathbb{R}^n$ such that $f( \mathbf{x} ) = [x^{1},x^2,x^3,...,x^n, F(\mathbf{x})]$. I have $f\in C^{\infty}$ and defined on $A$, the domain of $F$, and I think its easy to see that for an open subset of $A$ we have the diffeomorphism as in the ""definition"" above"". Does this argument work?",,"['multivariable-calculus', 'differential-geometry', 'manifolds', 'smooth-manifolds']"
95,Taking partial derivatives of related variables,Taking partial derivatives of related variables,,"As the title states, say I have a function $f(x,y)$ and I am given $y$ as a function $y(x)$. Say I take the partial derivative of $f$ with respect to $x$: apparently, I am just supposed to let $y$ be a constant as if it were unrelated to $x$, as I can see by googling or from other questions on this site. (For example, this question has an answer which says so, but with no explanation.) My question then, is why? The way I've learnt partial derivatives is always via a sort of geometric intuition: draw the surface described by $z=f(x,y)$ in $\mathbb{R}^3$, and to take $\partial f/\partial x$ is to draw a plane parallel to the $x$ and $z$ axes and orthogonal to the $y$ axis, and look at the slope of the curve along intersection of the surface of the plot and the plane. It seems then, that we can't treat $y$ as constant in the event that they are non-independant, since doing so would imply that some points on the $xy$-plane as in the previous visualisation would not be achievable. (For example, if $y=x^2$ and $f(x,y)=x+y$, it doesn't make sense to consider the point $(1,2,3)$ since it's not achievable.) I would think then that to take $\partial f/\partial x$ would require finding explicitly a function $g(x)$ so that $g(x)=f(x,y)$, then taking $\partial g/\partial x$, but apparently that is not the case. Taken to the extreme, say $x=y$ and we have, say, $f(x,y)=x^2+y^2$. According to what's supposed to be the case,  $$ \frac{\partial f}{\partial x}(x,y) = 2x. $$ If we however first substitute $x=y$ we will get $4x$. Why is my reasoning wrong? And why is the correct reasoning, well, correct? (To be clear: I have no problems with taking partial derivatives if the variables are unrelated. I don't think this will be very useful, but for context, I encountered this in trying to apply the Euler-Lagrange equation in an applied math problem. I came into this issue as the equation required me to take the partial derivative of a function $\mathcal{L}(q,q',t)$ with respect to $q'$, where $q'=dq/dt$.)","As the title states, say I have a function $f(x,y)$ and I am given $y$ as a function $y(x)$. Say I take the partial derivative of $f$ with respect to $x$: apparently, I am just supposed to let $y$ be a constant as if it were unrelated to $x$, as I can see by googling or from other questions on this site. (For example, this question has an answer which says so, but with no explanation.) My question then, is why? The way I've learnt partial derivatives is always via a sort of geometric intuition: draw the surface described by $z=f(x,y)$ in $\mathbb{R}^3$, and to take $\partial f/\partial x$ is to draw a plane parallel to the $x$ and $z$ axes and orthogonal to the $y$ axis, and look at the slope of the curve along intersection of the surface of the plot and the plane. It seems then, that we can't treat $y$ as constant in the event that they are non-independant, since doing so would imply that some points on the $xy$-plane as in the previous visualisation would not be achievable. (For example, if $y=x^2$ and $f(x,y)=x+y$, it doesn't make sense to consider the point $(1,2,3)$ since it's not achievable.) I would think then that to take $\partial f/\partial x$ would require finding explicitly a function $g(x)$ so that $g(x)=f(x,y)$, then taking $\partial g/\partial x$, but apparently that is not the case. Taken to the extreme, say $x=y$ and we have, say, $f(x,y)=x^2+y^2$. According to what's supposed to be the case,  $$ \frac{\partial f}{\partial x}(x,y) = 2x. $$ If we however first substitute $x=y$ we will get $4x$. Why is my reasoning wrong? And why is the correct reasoning, well, correct? (To be clear: I have no problems with taking partial derivatives if the variables are unrelated. I don't think this will be very useful, but for context, I encountered this in trying to apply the Euler-Lagrange equation in an applied math problem. I came into this issue as the equation required me to take the partial derivative of a function $\mathcal{L}(q,q',t)$ with respect to $q'$, where $q'=dq/dt$.)",,"['calculus', 'multivariable-calculus', 'functions']"
96,Prove that Laplace density (standard) lies in Sobolev space of order $\leq 3/2$,Prove that Laplace density (standard) lies in Sobolev space of order,\leq 3/2,"I'm trying to prove, that the function $f(x) = e^{-|x|}$ lies in $H^{s}(\mathbb{R}^2)$ for $s\leq \frac{3}{2}$. Therefore I calculate the functions sobolev norm $$\|f\|_s^2 = \frac{1}{4\pi^2}\int_{\mathbb{R}^2}(1+\| u\|)^s |\mathscr{F}f(u)|^2du$$ Since for its fourier transform holds $\mathscr{F}f(u) = (1+\|u\|^2)^{-1}$, we consider $$\|f\|_s^2 = \int_{\mathbb{R}^2}(1+\|u\|^2)^{s-2}du$$. Now I want to show two things: for $s=\frac{3}{2}$, this integral is not finite (and therefore obviously not for bigger $s$ and secondly, that this is finite for $s<\frac{3}{2}$. I proved the first part as following: $$\|f\|_s^2 =  \int_{\mathbb{R}^2}(1+\|u\|^2)^{-\frac{1}{2}}du \\ \geq  \int_{\mathbb{R}^2}\frac{1}{1+\|u\|}du \geq  \int_{\mathbb{R}^2}\frac{1}{\sum_{i=1}^2|u_i|}du > \infty $$, using $(a+b)^\frac{1}{2} \leq a^\frac{1}{2} + b^\frac{1}{2}$ and the behaviour of $\frac{1}{x}$ for $x\rightarrow 0$. For the second point I still miss any idea of how to calculate an upper bound for the integral. I hope there is somebody seeing the point I'm missing actually in this case.","I'm trying to prove, that the function $f(x) = e^{-|x|}$ lies in $H^{s}(\mathbb{R}^2)$ for $s\leq \frac{3}{2}$. Therefore I calculate the functions sobolev norm $$\|f\|_s^2 = \frac{1}{4\pi^2}\int_{\mathbb{R}^2}(1+\| u\|)^s |\mathscr{F}f(u)|^2du$$ Since for its fourier transform holds $\mathscr{F}f(u) = (1+\|u\|^2)^{-1}$, we consider $$\|f\|_s^2 = \int_{\mathbb{R}^2}(1+\|u\|^2)^{s-2}du$$. Now I want to show two things: for $s=\frac{3}{2}$, this integral is not finite (and therefore obviously not for bigger $s$ and secondly, that this is finite for $s<\frac{3}{2}$. I proved the first part as following: $$\|f\|_s^2 =  \int_{\mathbb{R}^2}(1+\|u\|^2)^{-\frac{1}{2}}du \\ \geq  \int_{\mathbb{R}^2}\frac{1}{1+\|u\|}du \geq  \int_{\mathbb{R}^2}\frac{1}{\sum_{i=1}^2|u_i|}du > \infty $$, using $(a+b)^\frac{1}{2} \leq a^\frac{1}{2} + b^\frac{1}{2}$ and the behaviour of $\frac{1}{x}$ for $x\rightarrow 0$. For the second point I still miss any idea of how to calculate an upper bound for the integral. I hope there is somebody seeing the point I'm missing actually in this case.",,"['calculus', 'multivariable-calculus', 'fourier-analysis', 'sobolev-spaces']"
97,Why is $\sin(x+y)=c$ not a smooth curve for $c = \pm 1$?,Why is  not a smooth curve for ?,\sin(x+y)=c c = \pm 1,"I understand that, for $c= 1$ and $c=-1$, the derivative of the function $f(x,y)=\sin(x+y)-c$ is a zero vector, and hence the locus which is given by $\sin(x+y)-c=0$ fails to satisfy the definition of the smooth curve. However, I have the following doubt: Suppose $c = 1$. Then, $$ \sin(x+y) = 1  \implies y = -x + \arcsin(1)$$ So, $$y = -x + (4n+1)\pi/2, \quad\forall n\in\mathbb{Z}$$ satisfies the equation. These are a set of parallel lines. But, why is it failing to satisfy the definition of a smooth curve? I tried to plot the function for $c=1$ in WolframAlpha, but the plot did not contain any points. Most likely, I am doing some silly mistake in the above calculations. Can anyone help in identifying the flaw in my thinking?","I understand that, for $c= 1$ and $c=-1$, the derivative of the function $f(x,y)=\sin(x+y)-c$ is a zero vector, and hence the locus which is given by $\sin(x+y)-c=0$ fails to satisfy the definition of the smooth curve. However, I have the following doubt: Suppose $c = 1$. Then, $$ \sin(x+y) = 1  \implies y = -x + \arcsin(1)$$ So, $$y = -x + (4n+1)\pi/2, \quad\forall n\in\mathbb{Z}$$ satisfies the equation. These are a set of parallel lines. But, why is it failing to satisfy the definition of a smooth curve? I tried to plot the function for $c=1$ in WolframAlpha, but the plot did not contain any points. Most likely, I am doing some silly mistake in the above calculations. Can anyone help in identifying the flaw in my thinking?",,"['algebra-precalculus', 'multivariable-calculus', 'trigonometry']"
98,"Find extrema (max,min) of a 3 variable function on a given domain","Find extrema (max,min) of a 3 variable function on a given domain",,"$f(x,y,z) = (x^2+y^2)e^z$ with domain = $D=\{x^2+y^2=4 , (x-2)^2+y^2 +z^2<= 4\}$ I found that a possible min/max could be the point $(0,0)$ (grad(f)=0..) I can see that the boundary of $D$ is $z^2-4x+4=0$ which is the intersection between the cylinder $x^2+y^2=4$ and the sphere $(x-2)^2+y^2 +z^2<= 4$ So I need to find the min/max on $D$ and on its boundary separately I checked the min/max using $Lagrange multiplier $ on the cylinder $x^2+y^2=4$, but here is the problem : The LaGrange system has 3 equation with 4 variables: $f_x+\gamma g_x=0$  $f_y+\gamma g_y=0$ $x^2+y^2=4$ I dont know how to behave without a z in my constrain ->$x^2+y^2=4$ I have the same problem with $z^2-4x+4=0$ if I do lagrange I dont know the $y$ Can you help me figure out the system clearly and find min/max constrained and not. Thx. EDIT : I managed to find the max/min on the constrain -> I did the LaGrange system of 5 equations , so I used two constrains inside the system (not just one at a time). and I found $(2,0,+-2)$ which is correct. But I still can't figure out how to calculate max/min inside the domain $D$","$f(x,y,z) = (x^2+y^2)e^z$ with domain = $D=\{x^2+y^2=4 , (x-2)^2+y^2 +z^2<= 4\}$ I found that a possible min/max could be the point $(0,0)$ (grad(f)=0..) I can see that the boundary of $D$ is $z^2-4x+4=0$ which is the intersection between the cylinder $x^2+y^2=4$ and the sphere $(x-2)^2+y^2 +z^2<= 4$ So I need to find the min/max on $D$ and on its boundary separately I checked the min/max using $Lagrange multiplier $ on the cylinder $x^2+y^2=4$, but here is the problem : The LaGrange system has 3 equation with 4 variables: $f_x+\gamma g_x=0$  $f_y+\gamma g_y=0$ $x^2+y^2=4$ I dont know how to behave without a z in my constrain ->$x^2+y^2=4$ I have the same problem with $z^2-4x+4=0$ if I do lagrange I dont know the $y$ Can you help me figure out the system clearly and find min/max constrained and not. Thx. EDIT : I managed to find the max/min on the constrain -> I did the LaGrange system of 5 equations , so I used two constrains inside the system (not just one at a time). and I found $(2,0,+-2)$ which is correct. But I still can't figure out how to calculate max/min inside the domain $D$",,"['calculus', 'multivariable-calculus']"
99,Having trouble proving change of variables from integral of differential forms.,Having trouble proving change of variables from integral of differential forms.,,"From what I understand, the integral of a differential form $dx^1\wedge\cdots\wedge dx^k$ on $[a^1,b^1]\times\cdots\times [a^k,b^k]$ is defined by $$ \int_{[a^1,b^1]\times\cdots\times [a^k,b^k]} dx^1\wedge\cdots\wedge dx^k := \int_{a^k}^{b^k}\cdots\int_{a^1}^{b^1} dx^1 \cdots dx^k $$ Where the right is $k$ repeated Riemann integrals. I am having trouble proving the change of variables rule for Riemann integrals using differential forms. I don’t see where the absolute sign would appear. Here’s what I have so far... Let $$ \phi : [p^1,q^1]\times\cdots\times [p^k,q^k] \to [a^1,b^1]\times\cdots\times [a^k,b^k], (t^1, \dots , t^k) \mapsto \left( \phi^1 (t^1,\dots ,t^k), \dots , \phi^k (t^1,\dots ,t^k)\right) $$ be bijective and its total derivative bijective, too. Then $$ \int_{a^k}^{b^k}\cdots\int_{a^1}^{b^1} dx^1 \cdots dx^k = \int_{[a^1,b^1]\times\cdots\times [a^k,b^k]} dx^1\wedge\cdots\wedge dx^k  \\ = \int_{[p^1,q^1]\times\cdots\times [p^k,q^k]} \phi^* \left(dx^1\wedge\cdots\wedge dx^k\right) = \int_{[p^1,q^1]\times\cdots\times [p^k,q^k]} det( D\phi ) \,\,\, dt^1\wedge\cdots\wedge dt^k $$ where $D\phi$ denotes the total derivative of $\phi$ to $t^1,\dots , t^k$. I feel like the next step is to turn this into a Riemann integral but then there wouldn’t be an absolute sign on that determinant.","From what I understand, the integral of a differential form $dx^1\wedge\cdots\wedge dx^k$ on $[a^1,b^1]\times\cdots\times [a^k,b^k]$ is defined by $$ \int_{[a^1,b^1]\times\cdots\times [a^k,b^k]} dx^1\wedge\cdots\wedge dx^k := \int_{a^k}^{b^k}\cdots\int_{a^1}^{b^1} dx^1 \cdots dx^k $$ Where the right is $k$ repeated Riemann integrals. I am having trouble proving the change of variables rule for Riemann integrals using differential forms. I don’t see where the absolute sign would appear. Here’s what I have so far... Let $$ \phi : [p^1,q^1]\times\cdots\times [p^k,q^k] \to [a^1,b^1]\times\cdots\times [a^k,b^k], (t^1, \dots , t^k) \mapsto \left( \phi^1 (t^1,\dots ,t^k), \dots , \phi^k (t^1,\dots ,t^k)\right) $$ be bijective and its total derivative bijective, too. Then $$ \int_{a^k}^{b^k}\cdots\int_{a^1}^{b^1} dx^1 \cdots dx^k = \int_{[a^1,b^1]\times\cdots\times [a^k,b^k]} dx^1\wedge\cdots\wedge dx^k  \\ = \int_{[p^1,q^1]\times\cdots\times [p^k,q^k]} \phi^* \left(dx^1\wedge\cdots\wedge dx^k\right) = \int_{[p^1,q^1]\times\cdots\times [p^k,q^k]} det( D\phi ) \,\,\, dt^1\wedge\cdots\wedge dt^k $$ where $D\phi$ denotes the total derivative of $\phi$ to $t^1,\dots , t^k$. I feel like the next step is to turn this into a Riemann integral but then there wouldn’t be an absolute sign on that determinant.",,"['multivariable-calculus', 'differential-geometry']"
