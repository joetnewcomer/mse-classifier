,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Convex function is proper when it has at least one finite value in the relative interior of its effective domain,Convex function is proper when it has at least one finite value in the relative interior of its effective domain,,"Problem Statement let $f: \mathbb{E}\mapsto \bar{\mathbb{R}}$ be a convex function, show that if there exists a point $x\in \text{ri}(\text{dom}(f))$ with $f(x)$ taking a finite value, then $f$ must be proper. This is an exercise problem that is highly relevant to Rockafellar's Textbook on Variational Analysis. There was a similar exercise related to improper function, but professor tweaked it. My Take So far this is my take, I don't think it's correct, or rigorous. $\text{epi}(f)$ is convex, then $$ f\left( \sum_{i = 1}^n \lambda_i x_i \right) \le \sum_{i = 1}^n \lambda_i f(x_i) \quad \lambda \in \Delta_n, x_i \in \text{dom}(f) $$ Convexity of Epigraph implies an inequality, relating to the convex combinations of points in the effective domain of the function $f(x)$ . Now consider fixing value of $a \in \text{dom}(f)$ such that $f(a)$ is finite, then ( Not very rigours here ): $$ a = \sum_{i = 1}^n \lambda_i x_i $$ A can be represented as a convex combination of points in the affective domain of the function. Then: $$ -\infty < f(a) = f\left( \sum_{i = 1}^n \lambda_i x_i \right) \le \sum_{i = 1}^n \lambda_i f(x_i) $$ Therefore, for any $x_i$ , $f(x_i)$ is not $-\infty$ , the function is proper. However, I don't understand why the original statement stated the existence of finite value inside the Relative Interior of the Affective Domain, instead of just the effective domain? I am not sure how many points I should choose to construct the convex combinations to be equal to $a$ , nor I am sure whether is possible. Appreciate it.","Problem Statement let be a convex function, show that if there exists a point with taking a finite value, then must be proper. This is an exercise problem that is highly relevant to Rockafellar's Textbook on Variational Analysis. There was a similar exercise related to improper function, but professor tweaked it. My Take So far this is my take, I don't think it's correct, or rigorous. is convex, then Convexity of Epigraph implies an inequality, relating to the convex combinations of points in the effective domain of the function . Now consider fixing value of such that is finite, then ( Not very rigours here ): A can be represented as a convex combination of points in the affective domain of the function. Then: Therefore, for any , is not , the function is proper. However, I don't understand why the original statement stated the existence of finite value inside the Relative Interior of the Affective Domain, instead of just the effective domain? I am not sure how many points I should choose to construct the convex combinations to be equal to , nor I am sure whether is possible. Appreciate it.","f: \mathbb{E}\mapsto \bar{\mathbb{R}} x\in \text{ri}(\text{dom}(f)) f(x) f \text{epi}(f) 
f\left(
\sum_{i = 1}^n \lambda_i x_i
\right) \le \sum_{i = 1}^n \lambda_i f(x_i) \quad \lambda \in \Delta_n, x_i \in \text{dom}(f)
 f(x) a \in \text{dom}(f) f(a) 
a = \sum_{i = 1}^n \lambda_i x_i
 
-\infty < f(a) = f\left(
\sum_{i = 1}^n \lambda_i x_i
\right) \le \sum_{i = 1}^n \lambda_i f(x_i)
 x_i f(x_i) -\infty a","['analysis', 'convex-analysis', 'convex-optimization', 'convex-geometry', 'variational-analysis']"
1,The monotonicity of a function that is of complex form,The monotonicity of a function that is of complex form,,"Consider the following function in $x\geq0$ with a parameter $q\in[0,1]$ : $f(x;q):=\frac{(1-e^{-x})^2}{e^{-x}(1+x)^2(1+qx-e^{qx}(1+x))^2}\bigg[(1+qx)^2+(2+qx)q^{2}x(1+x)^2+e^{qx}(1+x)\big\{(qx+1)(x-1)+q^{2}x(1+x)(qx(1+x)-2)\big\}\bigg]$ For a given $q\in[0,1]$ , when I plot the function $f(x;q)$ in $x\geq0$ , I can see that the function is monotonically (strictly) increasing in $x\geq0$ , so I suspect that this monotonicity holds for $f(x;q)$ in general. I've tried to prove it by taking the first derivative, but as you can see, it is very complicated... I used the symbolic calculation/calculus in Mathematica, but it was not able to resolve the issue. I am just wondering if there could be any way to prove that $f(x;q)$ increases in $x\geq0$ for a given $q\in[0,1]$ . By the way, if $q=0$ or $q=1$ , then the function $f(x;q)$ reduces to a quite simple form and I can easily show its monotonicity. Any suggestions or thoughts would be really appreciated. Thank you so much for your help in advance!","Consider the following function in with a parameter : For a given , when I plot the function in , I can see that the function is monotonically (strictly) increasing in , so I suspect that this monotonicity holds for in general. I've tried to prove it by taking the first derivative, but as you can see, it is very complicated... I used the symbolic calculation/calculus in Mathematica, but it was not able to resolve the issue. I am just wondering if there could be any way to prove that increases in for a given . By the way, if or , then the function reduces to a quite simple form and I can easily show its monotonicity. Any suggestions or thoughts would be really appreciated. Thank you so much for your help in advance!","x\geq0 q\in[0,1] f(x;q):=\frac{(1-e^{-x})^2}{e^{-x}(1+x)^2(1+qx-e^{qx}(1+x))^2}\bigg[(1+qx)^2+(2+qx)q^{2}x(1+x)^2+e^{qx}(1+x)\big\{(qx+1)(x-1)+q^{2}x(1+x)(qx(1+x)-2)\big\}\bigg] q\in[0,1] f(x;q) x\geq0 x\geq0 f(x;q) f(x;q) x\geq0 q\in[0,1] q=0 q=1 f(x;q)","['calculus', 'analysis', 'monotone-functions']"
2,"How to show that for every vector field $v$ there exists a constant $M_{ > 0}$ such that $||v(x)|| \leq M$ $\space \forall x \in \gamma([a,b])$?",How to show that for every vector field  there exists a constant  such that  ?,"v M_{ > 0} ||v(x)|| \leq M \space \forall x \in \gamma([a,b])","Given is that the length $L(\gamma)$ of a $C^1$ -curve $\gamma : [a, b] \longrightarrow \mathbb{R^n}$ is defined as: $L(\gamma) = \int^b_a||\gamma'(t)||dt$ Now I need to show that for every continuous vector field $v$ defined on $\gamma([a, b])$ there exists a constant $M > 0$ such that $||v(x)|| \leq M$ $\space \forall x \in \gamma([a,b])$ . I've been stuck on this question for a while and I'm not sure how to show this correctly. Thank you very much in advance for your time and help!",Given is that the length of a -curve is defined as: Now I need to show that for every continuous vector field defined on there exists a constant such that . I've been stuck on this question for a while and I'm not sure how to show this correctly. Thank you very much in advance for your time and help!,"L(\gamma) C^1 \gamma : [a, b] \longrightarrow \mathbb{R^n} L(\gamma) = \int^b_a||\gamma'(t)||dt v \gamma([a, b]) M > 0 ||v(x)|| \leq M \space \forall x \in \gamma([a,b])",['analysis']
3,When is $\int\frac{dx}{\sqrt{a-bx^n-x^2}}$ solvable in terms of elementary functions and why?,When is  solvable in terms of elementary functions and why?,\int\frac{dx}{\sqrt{a-bx^n-x^2}},"The integral in the question appears in the solution to the orbit of a particle subjected to a central force. It is written in Goldstein's Classical mechanics that the solution is possible in terms of circular trigonometric functions without providing any reason. The wiki page https://en.wikipedia.org/wiki/Exact_solutions_of_classical_central-force_problems refers to Whittaker which says that the expression under the square root must be quadratic at most. I don't understand why it should be quadratic at most, i.e., why other values of n would make the integral stop having solutions with elementary functions only, what's the guarantee that no other integral value of n is possible? Any help/reference will be appreciated. Thanks in advance.","The integral in the question appears in the solution to the orbit of a particle subjected to a central force. It is written in Goldstein's Classical mechanics that the solution is possible in terms of circular trigonometric functions without providing any reason. The wiki page https://en.wikipedia.org/wiki/Exact_solutions_of_classical_central-force_problems refers to Whittaker which says that the expression under the square root must be quadratic at most. I don't understand why it should be quadratic at most, i.e., why other values of n would make the integral stop having solutions with elementary functions only, what's the guarantee that no other integral value of n is possible? Any help/reference will be appreciated. Thanks in advance.",,"['calculus', 'integration', 'analysis', 'indefinite-integrals', 'elementary-functions']"
4,"How to prove that the set $S:=\{(u,v,\sin(u)\cos(v)):u,v \in [a,b]\}$ is connected?",How to prove that the set  is connected?,"S:=\{(u,v,\sin(u)\cos(v)):u,v \in [a,b]\}","Exercise: Given the Set $S:=\{(u,v,\sin(u)\cos(v)):u,v \in [a,b]\}$ show that $S$ is a connected Set. From a intuitive viewing point the connectedness is obvious. My first thought was to use the Definition of connectedness but I tried something else first: My idea is to use the fact that for a continuous function $f:X \rightarrow Y$ and a connected set $X$ the set $f(X)$ is also connected. So one could write the Set S as parametrization $f(x,y)=(x,y,\sin(x)cos(y))$ $f$ is obviously continuous and $f([a,b]^2)=S$ Now $[a,b]$ is path connected so, in particular the set $S$ is connected. Are my calculations correct? Is there another way to show it? I am especially interested in a more ""topological"" prove. Hope someone could help me out with that.","Exercise: Given the Set show that is a connected Set. From a intuitive viewing point the connectedness is obvious. My first thought was to use the Definition of connectedness but I tried something else first: My idea is to use the fact that for a continuous function and a connected set the set is also connected. So one could write the Set S as parametrization is obviously continuous and Now is path connected so, in particular the set is connected. Are my calculations correct? Is there another way to show it? I am especially interested in a more ""topological"" prove. Hope someone could help me out with that.","S:=\{(u,v,\sin(u)\cos(v)):u,v \in [a,b]\} S f:X \rightarrow Y X f(X) f(x,y)=(x,y,\sin(x)cos(y)) f f([a,b]^2)=S [a,b] S","['general-topology', 'analysis']"
5,"$C[0,1]$ and $C^1[0,1]$ continuous bijections in either direction",and  continuous bijections in either direction,"C[0,1] C^1[0,1]","I am trying to find two non homeomorphic topologies with continuous bijections in both directions. I know there are solutions for this online, but I would much rather someone tell me whether I'm thinking along the right lines to solve this well known problem. Please do not post spoilers. Consider $C[0,1]$ and $C^1[0,1]$ with $f(0)=0$ and the uniform norm. $C^1$ denotes the continuously differentiable functions. $C^1$ is not complete but $C$ is so they are not homeomorphic. I think that the maps $F[f]=\frac{d}{dx}f$ and $G[f]=\int_0^xf$ are bijections in either direction and $F[G[f]]=f$ so one of them must not be continuous. This is a shame, I'm considering modifying $F$ , for instance $F[f]=\frac{d}{dx} (xf)$ , maybe that smoothes things out. If I change to another norm I know (Sobolev norm maybe), I lose the nice guarantee that they are nonhomeomorphic. Am I wasting my time or is this going somewhere?","I am trying to find two non homeomorphic topologies with continuous bijections in both directions. I know there are solutions for this online, but I would much rather someone tell me whether I'm thinking along the right lines to solve this well known problem. Please do not post spoilers. Consider and with and the uniform norm. denotes the continuously differentiable functions. is not complete but is so they are not homeomorphic. I think that the maps and are bijections in either direction and so one of them must not be continuous. This is a shame, I'm considering modifying , for instance , maybe that smoothes things out. If I change to another norm I know (Sobolev norm maybe), I lose the nice guarantee that they are nonhomeomorphic. Am I wasting my time or is this going somewhere?","C[0,1] C^1[0,1] f(0)=0 C^1 C^1 C F[f]=\frac{d}{dx}f G[f]=\int_0^xf F[G[f]]=f F F[f]=\frac{d}{dx} (xf)","['general-topology', 'analysis', 'derivatives', 'continuity', 'metric-spaces']"
6,"Definition and properties of $W^{-1,k}_{loc}(\Omega)$ space",Definition and properties of  space,"W^{-1,k}_{loc}(\Omega)","I have a basic understanding of the Sobolev spaces. Let $\Omega \subset \mathbb{R}^n$ and $k=1,2,3... .$ I understand that $W^{-1,k}(\Omega)$ is defined as the dual of $W_0^{1,k'}(\Omega)$ where $k'$ is the conjugate exponent of $k.$ In other words, $W^{-1,k}(\Omega)$ is a normed linear space which consists of all the functionals $f :W_0^{1,k'}(\Omega) \rightarrow \mathbb{R}$ equipped with operator norm defined by $||f||_{W^{-1,k}(\Omega)}= \sup\limits_{||\phi||_{W^{1,k'}(\Omega)}=1}|f(\phi)|.$ I recently came across the space $W^{-1,k}_{loc}(\Omega).$ I would like to understand the followings: 1. Is there a norm (or atleast a metric) on $W^{1,k}_{loc}(\Omega)$ ? 2. What is the definition of $W^{-1,k}_{loc}(\Omega)$ ? How to interpret these spaces? (In view of (1) is it the dual of $W^{1,k}_{loc}(\Omega)$ under the norm/metric ) 3. What do we mean by compact sets and bounded sets in $W^{-1,k}_{loc}(\Omega)$ ?","I have a basic understanding of the Sobolev spaces. Let and I understand that is defined as the dual of where is the conjugate exponent of In other words, is a normed linear space which consists of all the functionals equipped with operator norm defined by I recently came across the space I would like to understand the followings: 1. Is there a norm (or atleast a metric) on ? 2. What is the definition of ? How to interpret these spaces? (In view of (1) is it the dual of under the norm/metric ) 3. What do we mean by compact sets and bounded sets in ?","\Omega \subset \mathbb{R}^n k=1,2,3... . W^{-1,k}(\Omega) W_0^{1,k'}(\Omega) k' k. W^{-1,k}(\Omega) f :W_0^{1,k'}(\Omega) \rightarrow \mathbb{R} ||f||_{W^{-1,k}(\Omega)}= \sup\limits_{||\phi||_{W^{1,k'}(\Omega)}=1}|f(\phi)|. W^{-1,k}_{loc}(\Omega). W^{1,k}_{loc}(\Omega) W^{-1,k}_{loc}(\Omega) W^{1,k}_{loc}(\Omega) W^{-1,k}_{loc}(\Omega)","['functional-analysis', 'analysis', 'partial-differential-equations', 'compactness', 'sobolev-spaces']"
7,Image of a polyhedral set under a linear map,Image of a polyhedral set under a linear map,,"I say that a subset $P$ of $\mathbb{R}^n$ is a polyhedral set iff there exists some positive integer $m$ , a matrix $A\in Mat(m\times n,\mathbb{R})$ and a column vector $b\in\mathbb{R}^m$ such that $P=\{x\in\mathbb{R}^n\mid Ax\le b\}$ . When $x$ and $y$ are two vectors in $\mathbb{R}^n$ I write $x\le y$ to say that $x_i\le y_i$ for all $i=1,\dots, m$ . Now let $T\in Mat(m\times n,\mathbb{R})$ e let $f:\mathbb{R}^n\to \mathbb{R}^m$ be the linear transformation defined by $f(x)=Tx$ for all $x\in\mathbb{R}^n$ . My problem is to prove that if P is a polyhedral set, then also the image of $P$ under f, i.e. $f(P)=\{Tx\mid x\in\mathbb{R}^n\text{ and }Ax\le b\}$ , is a polyhedral set . The idea is to show that there exists a $p\times m$ matrix $M$ , for some positive integer $p$ , and a column vector $\beta$ such that $f(P)=\{y\in\mathbb{R}^m\mid My\le\beta\}$ . But I don't how to show this latter thing. Can anyone help me, please?","I say that a subset of is a polyhedral set iff there exists some positive integer , a matrix and a column vector such that . When and are two vectors in I write to say that for all . Now let e let be the linear transformation defined by for all . My problem is to prove that if P is a polyhedral set, then also the image of under f, i.e. , is a polyhedral set . The idea is to show that there exists a matrix , for some positive integer , and a column vector such that . But I don't how to show this latter thing. Can anyone help me, please?","P \mathbb{R}^n m A\in Mat(m\times n,\mathbb{R}) b\in\mathbb{R}^m P=\{x\in\mathbb{R}^n\mid Ax\le b\} x y \mathbb{R}^n x\le y x_i\le y_i i=1,\dots, m T\in Mat(m\times n,\mathbb{R}) f:\mathbb{R}^n\to \mathbb{R}^m f(x)=Tx x\in\mathbb{R}^n P f(P)=\{Tx\mid x\in\mathbb{R}^n\text{ and }Ax\le b\} p\times m M p \beta f(P)=\{y\in\mathbb{R}^m\mid My\le\beta\}","['linear-algebra', 'geometry', 'analysis', 'linear-transformations', 'convex-analysis']"
8,"Is it true that $d((f+g)(x), (f+g)(y)) \leq d(f(x),f(y)) + d(g(x),g(y))$?",Is it true that ?,"d((f+g)(x), (f+g)(y)) \leq d(f(x),f(y)) + d(g(x),g(y))","In arbitrary metric space $(M, d)$ , is it true that $d((f+g)(x), (f+g)(y)) \leq d(f(x),f(y)) + d(g(x),g(y))$ ? Clearly, in the simple case where $M = \mathbb{R}$ and $d(x,y) = | x - y |$ , we have $$\begin{align} d((f+g)(x), (f+g)(y)) &= |(f+g)(x) - (f+g)(y)| \\ &= |(f(x) + g(x)) - (f(y) + g(y))| \\ &= |(f(x) - f(y)) + (g(x) - g(y))| \tag{1} \\ &\leq |f(x) - f(y)| + |g(x) - g(y)| \\ &= d(f(x), f(y)) + d(g(x), g(y)), \end{align}$$ as desired. So basically, this proof requires the ability to rearrange the values within the absolute values sign, as indicated in line (1), which is certainly possible. But when I try this same type of proof using just the properties of an arbitrary metric, I get $$\begin{align} d((f+g)(x), (f+g)(y)) &= d(f(x) + g(x), f(y) + g(y)) \\ &(=?) \ |d(f(x), f(y)) -  d(g(x), g(y))| \tag{2}\\ &\leq d(f(x),f(y)) + d(g(x), g(y)), \end{align}$$ and I'm not sure I can do the same type of rearrangement in an arbitrary metric space, which is why I've put the question mark in line (2). I feel like I'm missing something obvious! (For what it's worth, I'm trying to use this to prove that the sum of uniformly continuous functions is uniformly continuous.)","In arbitrary metric space , is it true that ? Clearly, in the simple case where and , we have as desired. So basically, this proof requires the ability to rearrange the values within the absolute values sign, as indicated in line (1), which is certainly possible. But when I try this same type of proof using just the properties of an arbitrary metric, I get and I'm not sure I can do the same type of rearrangement in an arbitrary metric space, which is why I've put the question mark in line (2). I feel like I'm missing something obvious! (For what it's worth, I'm trying to use this to prove that the sum of uniformly continuous functions is uniformly continuous.)","(M, d) d((f+g)(x), (f+g)(y)) \leq d(f(x),f(y)) + d(g(x),g(y)) M = \mathbb{R} d(x,y) = | x - y | \begin{align}
d((f+g)(x), (f+g)(y)) &= |(f+g)(x) - (f+g)(y)| \\
&= |(f(x) + g(x)) - (f(y) + g(y))| \\
&= |(f(x) - f(y)) + (g(x) - g(y))| \tag{1} \\
&\leq |f(x) - f(y)| + |g(x) - g(y)| \\
&= d(f(x), f(y)) + d(g(x), g(y)),
\end{align} \begin{align}
d((f+g)(x), (f+g)(y)) &= d(f(x) + g(x), f(y) + g(y)) \\
&(=?) \ |d(f(x), f(y)) -  d(g(x), g(y))| \tag{2}\\
&\leq d(f(x),f(y)) + d(g(x), g(y)),
\end{align}","['real-analysis', 'analysis', 'metric-spaces']"
9,plane to torus projection,plane to torus projection,,"I am struggling to find a way to make a projection of a 2d plane $(x,y)$ onto a section of a torus. Is there a function $f$ that would map it depending on the given tor radii, the radii of its section and angles $\theta$ and $\phi$ ? Torus mapping Image:","I am struggling to find a way to make a projection of a 2d plane onto a section of a torus. Is there a function that would map it depending on the given tor radii, the radii of its section and angles and ? Torus mapping Image:","(x,y) f \theta \phi","['general-topology', 'analysis']"
10,Integral inequality $\int^{\pi}_{0} f^2(x)dx \le \int^{\pi}_{0} (f')^2(x)dx + (\int^{\pi}_{0} f(x)dx)^2$. [closed],Integral inequality . [closed],\int^{\pi}_{0} f^2(x)dx \le \int^{\pi}_{0} (f')^2(x)dx + (\int^{\pi}_{0} f(x)dx)^2,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 2 years ago . Improve this question Let $f \in H^1(0,\pi)$ , show that $\int^{\pi}_{0} f^2(x)dx \le \int^{\pi}_{0} (f')^2(x)dx + (\int^{\pi}_{0} f(x)dx)^2$ . $H^1(0,\pi) = W^{1,2}(0,\pi)$ , Sobolev space. The question is from PDE course, but I guess it's true for all functions as long as integrals in the inequality are finite. I tried IBP, Holder inequality, but cannot get the answer. Would appreciate any suggestions where to start. Thanks!","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 2 years ago . Improve this question Let , show that . , Sobolev space. The question is from PDE course, but I guess it's true for all functions as long as integrals in the inequality are finite. I tried IBP, Holder inequality, but cannot get the answer. Would appreciate any suggestions where to start. Thanks!","f \in H^1(0,\pi) \int^{\pi}_{0} f^2(x)dx \le \int^{\pi}_{0} (f')^2(x)dx + (\int^{\pi}_{0} f(x)dx)^2 H^1(0,\pi) = W^{1,2}(0,\pi)","['analysis', 'partial-differential-equations', 'integral-inequality']"
11,Showing that the corresponding two spaces are homeomorphic.,Showing that the corresponding two spaces are homeomorphic.,,"I want to show that the following map induces a homeomorphism between the two structures. $\require{AMScd}$ \begin{CD}     \mathbb{B}^{n} @>f(x_1,\cdots , x_n) = (x_1 , \cdots x_n , \sqrt{1- \sum_{i=1}^n(x_i)^2})>>  \mathbb{S}^n @>\pi_1(x) = cl(\{x\}) >> \mathbb{S}^{n} / \ \sim_2 \\      @VV\pi_2(x) = cl(x)V \\       \mathbb{B}^{n} / \sim_1  \\ \end{CD} where $\sim_1$ signifies identifying the antipodal points of the boundary circle of $\mathbb{B}^n$ and $\mathbb{S}^n / \sim_2$ signifies identifying the antipodal points of $\mathbb{S}^n$ . I want to show that $\mathbb{S}^n / \sim_2$ and $\mathbb{B}^n / \sim_1$ are homeomorphic using the theorem, $\require{AMScd}$ \begin{CD}     \mathbb{X} @>g(x)>> Y \\      @VV p V  \\      \  \mathbb{X} / \sim  \\ \end{CD} where $g(x)$ is an identification map and $p$ is a projection map. The part where I am stuck at is: 1)Showing that $\pi_1 \circ f (x) $ is a surjective map. 2)Is showing that $\pi_1 \circ f(x)$ surjective enough? Or is there anything else we have to show to prove that the two maps are homeomorphic? Edit $1$ :Let $(x_1, \cdots , x_{n+1}) \in  \mathbb{S}^n/\sim_2$ then, case $1:$ Assume that $x_{n+1} > 0$ then $\pi_1^{-1}((x_1,\cdots ,x_{n+1})) = (x_1, \cdots ,x_{n+1})$ where $\sum_{i=1}^{n+1}(x_i)^2 = 1$ . $f^{-1}(x_1,\cdots,x_{n+1}) = (x_1,\cdots x_n)$ s.t. $\sum_{i=1}^n(x_i)^2 < 1$ . Then $$\pi_2(x_1, \cdots x_n) = (x_1 , \cdots x_n)$$ case $2:$ Assume that $x_{n+1} = 0$ then $(x_1,\cdots x_{n+1})$ is a point on the equator of the sphere. Then, $\pi_1^ {-1} (x_1,\cdots x_{n},0)= \{[x],[x]'\} $ where $[x]= (x_1, \cdots ,x_{n+1})$ and $[x]'$ is the antipodal point of $[x]$ . Also $\sum_{i=1}^n(x_i)^2 = 1$ . I am stuck in this part.","I want to show that the following map induces a homeomorphism between the two structures. where signifies identifying the antipodal points of the boundary circle of and signifies identifying the antipodal points of . I want to show that and are homeomorphic using the theorem, where is an identification map and is a projection map. The part where I am stuck at is: 1)Showing that is a surjective map. 2)Is showing that surjective enough? Or is there anything else we have to show to prove that the two maps are homeomorphic? Edit :Let then, case Assume that then where . s.t. . Then case Assume that then is a point on the equator of the sphere. Then, where and is the antipodal point of . Also . I am stuck in this part.","\require{AMScd} \begin{CD}
    \mathbb{B}^{n} @>f(x_1,\cdots , x_n) = (x_1 , \cdots x_n , \sqrt{1- \sum_{i=1}^n(x_i)^2})>>  \mathbb{S}^n @>\pi_1(x) = cl(\{x\}) >> \mathbb{S}^{n} / \ \sim_2 \\
     @VV\pi_2(x) = cl(x)V \\
      \mathbb{B}^{n} / \sim_1  \\
\end{CD} \sim_1 \mathbb{B}^n \mathbb{S}^n / \sim_2 \mathbb{S}^n \mathbb{S}^n / \sim_2 \mathbb{B}^n / \sim_1 \require{AMScd} \begin{CD}
    \mathbb{X} @>g(x)>> Y \\
     @VV p V  \\
     \  \mathbb{X} / \sim  \\
\end{CD} g(x) p \pi_1 \circ f (x)  \pi_1 \circ f(x) 1 (x_1, \cdots , x_{n+1}) \in  \mathbb{S}^n/\sim_2 1: x_{n+1} > 0 \pi_1^{-1}((x_1,\cdots ,x_{n+1})) = (x_1, \cdots ,x_{n+1}) \sum_{i=1}^{n+1}(x_i)^2 = 1 f^{-1}(x_1,\cdots,x_{n+1}) = (x_1,\cdots x_n) \sum_{i=1}^n(x_i)^2 < 1 \pi_2(x_1, \cdots x_n) = (x_1 , \cdots x_n) 2: x_{n+1} = 0 (x_1,\cdots x_{n+1}) \pi_1^ {-1} (x_1,\cdots x_{n},0)= \{[x],[x]'\}  [x]= (x_1, \cdots ,x_{n+1}) [x]' [x] \sum_{i=1}^n(x_i)^2 = 1","['general-topology', 'analysis']"
12,A uniform lower bound for $\left| |x-y|^{\alpha-2}(x-y)-|x-z|^{\alpha-2}(x-z) \right|$ when $|y-z|\geq 1$,A uniform lower bound for  when,\left| |x-y|^{\alpha-2}(x-y)-|x-z|^{\alpha-2}(x-z) \right| |y-z|\geq 1,"It is shown in the post Is this function on $\mathbb{R}^{2}$ always positive? that the modulus $|f|$ of the function $f:\mathbb{R}^{2} \rightarrow \mathbb{R}^{2} $ given by : $$f(x):=\alpha\left( |x-y|^{\alpha-2}(x-y)-|x-z|^{\alpha-2}(x-z) \right)$$ where $\alpha >0,\alpha \neq 1$ is strictly positive when $y\neq z$ . I am trying to get a uniform lower bound of $|f|$ given that $|y-z|> 1$ . By uniform I mean in uniform in $x$ . Any hints ? Some thoughts: Let $t=x-y$ . Denote $\eta=y-z$ so that $x-z=t+\eta$ . Then it suffices to find a (uniform in $t$ ) lower bound for $$g(t):=\left| |t|^{\alpha-2}t-|t+\eta|^{\alpha-2}(t+\eta) \right|$$ I had earlier attempted the bound $$\left| |t|^{\alpha-2}t-|t+\eta|^{\alpha-2}(t+\eta) \right|\geq \left| |t|^{\alpha-1}-|t+\eta|^{\alpha-1} \right|$$ but the latter is not bounded away from zero because (as @Calvin Khor pointed out) it attains zero at $t=-\eta/2$ . An update : When $x$ lies on the perpendicular bisector $L$ of $\overline{yz}$ we have $|f(x)|\geq \frac{1}{2^{\alpha-2}}|y-z|^{\alpha-1}$ . Why? For every $x$ on the bisector we have $|x-y|=|x-z|$ . Therefore, for any $x\in L$ , $|f(x)|=|x-y|^{\alpha-2}|x-y-(x-z)|=|x-y|^{\alpha-2}|y-z|$ . If $x$ is the midpoint of $\overline{yz}$ then $|x-y|=\frac{1}{2}|y-z|$ . If $x \in L$ then $|x-y|>\frac{1}{2}|y-z|$ ,","It is shown in the post Is this function on $\mathbb{R}^{2}$ always positive? that the modulus of the function given by : where is strictly positive when . I am trying to get a uniform lower bound of given that . By uniform I mean in uniform in . Any hints ? Some thoughts: Let . Denote so that . Then it suffices to find a (uniform in ) lower bound for I had earlier attempted the bound but the latter is not bounded away from zero because (as @Calvin Khor pointed out) it attains zero at . An update : When lies on the perpendicular bisector of we have . Why? For every on the bisector we have . Therefore, for any , . If is the midpoint of then . If then ,","|f| f:\mathbb{R}^{2} \rightarrow \mathbb{R}^{2}  f(x):=\alpha\left( |x-y|^{\alpha-2}(x-y)-|x-z|^{\alpha-2}(x-z) \right) \alpha >0,\alpha \neq 1 y\neq z |f| |y-z|> 1 x t=x-y \eta=y-z x-z=t+\eta t g(t):=\left| |t|^{\alpha-2}t-|t+\eta|^{\alpha-2}(t+\eta) \right| \left| |t|^{\alpha-2}t-|t+\eta|^{\alpha-2}(t+\eta) \right|\geq
\left| |t|^{\alpha-1}-|t+\eta|^{\alpha-1} \right| t=-\eta/2 x L \overline{yz} |f(x)|\geq \frac{1}{2^{\alpha-2}}|y-z|^{\alpha-1} x |x-y|=|x-z| x\in L |f(x)|=|x-y|^{\alpha-2}|x-y-(x-z)|=|x-y|^{\alpha-2}|y-z| x \overline{yz} |x-y|=\frac{1}{2}|y-z| x \in L |x-y|>\frac{1}{2}|y-z|","['real-analysis', 'analysis', 'inequality', 'harmonic-analysis']"
13,Diagonalization argument for convergence in distribution,Diagonalization argument for convergence in distribution,,"Let $Z_{n, N}$ be a sequence of random variable such that, for any fixed $N$ , this sequence of variables converges in distribution to a variable $Z$ as $n \rightarrow \infty$ . There exists a sequence $N(n)$ that goes to infinity ( $N(n) \rightarrow \infty$ as $n \rightarrow \infty$ ) such that $Z_{n, N(n)}$ converges to $Z$ in distribution as $n \rightarrow \infty$ ? How to construct the sequence $N(n)$ and prove the convergence in distribution of the resulting random variables $Z_{n, N(n)}$ ? A little bit of context: An argument similar to the one above is used in Terence Tao, ""Topics in Random Matrix Theory"" book under the name of ""diagonalization argument"" . In Section 2.2.1, the argument is used to show the possibility of considering bounded random variables to prove the central limit theorem without loss of generality. There $N$ is the upper bound for the variables and it is assumed that since central limit theorem yield $Z_{n, N} \rightarrow \mathcal{N}(0, 1)$ , for any fixed $N$ , it hold that $Z_{n, N(n)} \rightarrow \mathcal{N}(0, 1)$ for some sequence of bounds $N(n)$ dependent on $n$ . .","Let be a sequence of random variable such that, for any fixed , this sequence of variables converges in distribution to a variable as . There exists a sequence that goes to infinity ( as ) such that converges to in distribution as ? How to construct the sequence and prove the convergence in distribution of the resulting random variables ? A little bit of context: An argument similar to the one above is used in Terence Tao, ""Topics in Random Matrix Theory"" book under the name of ""diagonalization argument"" . In Section 2.2.1, the argument is used to show the possibility of considering bounded random variables to prove the central limit theorem without loss of generality. There is the upper bound for the variables and it is assumed that since central limit theorem yield , for any fixed , it hold that for some sequence of bounds dependent on . .","Z_{n, N} N Z n \rightarrow \infty N(n) N(n) \rightarrow \infty n \rightarrow \infty Z_{n, N(n)} Z n \rightarrow \infty N(n) Z_{n, N(n)} N Z_{n, N} \rightarrow \mathcal{N}(0, 1) N Z_{n, N(n)} \rightarrow \mathcal{N}(0, 1) N(n) n","['real-analysis', 'probability', 'analysis']"
14,"Prove that for each $f\in L^1([0,1])$ we have $\lim_{n \to\infty}\int_0^1 f(x)g_n(x)dx=0$.",Prove that for each  we have .,"f\in L^1([0,1]) \lim_{n \to\infty}\int_0^1 f(x)g_n(x)dx=0","I would be glad if someone could help me to prove the following exercise. Let $\{g_n\}$ be a sequence of measurable functions on $[0,1]$ such that (a) There exists a constant $C>0$ such that for each $n\in \Bbb N$ , $|g_n(x)|\leq C$ for almost all $x\in[0,1]$ . (b) For every $a\in [0,1]$ , $\lim_{n\to\infty}\int_0^a g_n(x)dx=0$ . Prove that for each $f\in L^1([0,1])$ we have $\lim_{n \to\infty}\int_0^1 f(x)g_n(x)dx=0$ . Attempt . Suppose $f$ is a step function defined on $[0,1]$ . We show that $\lim_{n \to\infty}\int_0^1 f(x)g_n(x)dx=0$ . Say $f=\sum_{i=1}^N c_i\chi_{E_i}$ for some disjoint intervals $E_1,\dots,E_N\subseteq [0,1]$ . Then $$ \lim_{n \to\infty}\int_0^1 f(x)g_n(x)dx=\lim_{n\to\infty}\int_0^1 \left(\sum_{i=1}^N c_i\chi_{E_i}(x)\right) g_n(x)dx=\sum_{i=1}^Nc_i\int_{E_i}g_n(x)dx=0$$ since for each $n\in\Bbb N$ and $a,b\in [0,1]$ with $a\leq b$ we have $$\int_a^b g_n(x)dx=\int_0^b g_n(x)dx -\int_0^a g_n(x)dx \implies \lim_{n\to\infty}\int_a^bg_n(x)dx=0.$$ Question. Does there exist a sequence $\{f_n\}$ of step functions on $[0,1]$ such that $0\leq f_1\leq f_2\leq\dots$ and $f_n$ converges to $|f|$ pointwise almost everywhere on $[0,1]$ ? Even if the answer is yes, I couldn't conclude the proof by monotone convergence theorem. Thanks!","I would be glad if someone could help me to prove the following exercise. Let be a sequence of measurable functions on such that (a) There exists a constant such that for each , for almost all . (b) For every , . Prove that for each we have . Attempt . Suppose is a step function defined on . We show that . Say for some disjoint intervals . Then since for each and with we have Question. Does there exist a sequence of step functions on such that and converges to pointwise almost everywhere on ? Even if the answer is yes, I couldn't conclude the proof by monotone convergence theorem. Thanks!","\{g_n\} [0,1] C>0 n\in \Bbb N |g_n(x)|\leq C x\in[0,1] a\in [0,1] \lim_{n\to\infty}\int_0^a g_n(x)dx=0 f\in L^1([0,1]) \lim_{n \to\infty}\int_0^1 f(x)g_n(x)dx=0 f [0,1] \lim_{n \to\infty}\int_0^1 f(x)g_n(x)dx=0 f=\sum_{i=1}^N c_i\chi_{E_i} E_1,\dots,E_N\subseteq [0,1]  \lim_{n \to\infty}\int_0^1 f(x)g_n(x)dx=\lim_{n\to\infty}\int_0^1 \left(\sum_{i=1}^N c_i\chi_{E_i}(x)\right) g_n(x)dx=\sum_{i=1}^Nc_i\int_{E_i}g_n(x)dx=0 n\in\Bbb N a,b\in [0,1] a\leq b \int_a^b g_n(x)dx=\int_0^b g_n(x)dx -\int_0^a g_n(x)dx \implies \lim_{n\to\infty}\int_a^bg_n(x)dx=0. \{f_n\} [0,1] 0\leq f_1\leq f_2\leq\dots f_n |f| [0,1]","['real-analysis', 'analysis', 'measure-theory']"
15,Dirichlet integral integrates to $\pi$,Dirichlet integral integrates to,\pi,"I think the question has been asked several times on MSE since seems a standard fourier analysis fact, but I don't find any reference for it. Let's consider the Dirichlet Kernel as $$D_n(x) := \sum\limits_{\lvert k \rvert \leq n}e^{ikx}$$ In a further proposition I think it's used something like $$\int_0^{\pi} D_n(x)dx = \pi$$ Is this true? I tried to computed this explicitly or using $$D_n(x) = \frac{\text{sin}\left(\left(n+\frac{1}{2}\right)x\right)}{\text{sin}(\frac{x}{2})} \hspace{0.2cm} \forall x\ne 0 \hspace{0.2cm} \text{mod} 2\pi$$ But I got stuck since the integral of the part relative to $\text{cos}$ is $0$ , since integrate to $\text{sin}$ , and the part relative to $\text{cos}$ depends on $k$ but the $i$ term remains. Where is my mistake? And how to prove $$\int_0^{\pi} D_n(x)dx = \pi$$ if true ?","I think the question has been asked several times on MSE since seems a standard fourier analysis fact, but I don't find any reference for it. Let's consider the Dirichlet Kernel as In a further proposition I think it's used something like Is this true? I tried to computed this explicitly or using But I got stuck since the integral of the part relative to is , since integrate to , and the part relative to depends on but the term remains. Where is my mistake? And how to prove if true ?",D_n(x) := \sum\limits_{\lvert k \rvert \leq n}e^{ikx} \int_0^{\pi} D_n(x)dx = \pi D_n(x) = \frac{\text{sin}\left(\left(n+\frac{1}{2}\right)x\right)}{\text{sin}(\frac{x}{2})} \hspace{0.2cm} \forall x\ne 0 \hspace{0.2cm} \text{mod} 2\pi \text{cos} 0 \text{sin} \text{cos} k i \int_0^{\pi} D_n(x)dx = \pi,"['integration', 'analysis', 'definite-integrals', 'fourier-analysis', 'fourier-series']"
16,Let $f$ be a right continuous function then show that it is continuous from $\mathbb{R}_l \to \mathbb{R}$,Let  be a right continuous function then show that it is continuous from,f \mathbb{R}_l \to \mathbb{R},"The most simplest example of a right continuous function that could come to my mind is $$f(x)= \begin{cases}x &x \le 2 \\  x+1 & x>2\\ \end{cases}$$ This function is discontinuous at the point $x=2$ but it is right continuous .The function is not continuous in $\mathbb{R}$ as if we consider the open set $(1,3)$ then the pre-image of this set is $(1,2] \cup (3,4) $ which is not open in $\mathbb{R}$ . Is the set $(1,2] \cup (3,4)$ open in $\mathbb{R}_l$ ?I am not sure though . This is a question from Munkres and I need to prove this, so may be I am making some mistakes. I think it will be of the form $$(1,2] \cup (3,4) = \cup _{n \ge 2}[1- \frac{1}{n},2) \cup_{n \ge 2} [2,2-\frac{1}{n}) \cup_{n \ge 2} [3-\frac{1}{n},4-\frac{1}{n})$$ Here $\mathbb{R}_l$ denotes the lower limit topology. Is this way of writing it correct? Edit 1:I was thinking of considering two cases : Case 1: when $a \ne f(x_1-)$ and $ b \ne f(x_2-)$ then we consider the open interval $(a,b)$ .So the inverse $f^{-1}(a,b)=(f^{-1}(a),f^{-1}(b))$ Case 2: when $a = f(x_1-)$ then $f^{-1}(a,b)=[x_1,f^{-1}b)$ Case 3: when $b=f(x_2-)$ can also be dealt similarly . Case4: when $a=f(x_1-)$ and $b=f(x_2-)$ (where $x_1 < x_2 $ then $f^{-1}(a,b)=[x_1,x_2)$ . This is how I am thinking about the general proof . Am I in the right path?I would encourage some hints rather than the complete answer.","The most simplest example of a right continuous function that could come to my mind is This function is discontinuous at the point but it is right continuous .The function is not continuous in as if we consider the open set then the pre-image of this set is which is not open in . Is the set open in ?I am not sure though . This is a question from Munkres and I need to prove this, so may be I am making some mistakes. I think it will be of the form Here denotes the lower limit topology. Is this way of writing it correct? Edit 1:I was thinking of considering two cases : Case 1: when and then we consider the open interval .So the inverse Case 2: when then Case 3: when can also be dealt similarly . Case4: when and (where then . This is how I am thinking about the general proof . Am I in the right path?I would encourage some hints rather than the complete answer.","f(x)= \begin{cases}x &x \le 2 \\  x+1 & x>2\\
\end{cases} x=2 \mathbb{R} (1,3) (1,2] \cup (3,4)  \mathbb{R} (1,2] \cup (3,4) \mathbb{R}_l (1,2] \cup (3,4) = \cup _{n \ge 2}[1- \frac{1}{n},2) \cup_{n \ge 2} [2,2-\frac{1}{n}) \cup_{n \ge 2} [3-\frac{1}{n},4-\frac{1}{n}) \mathbb{R}_l a \ne f(x_1-)  b \ne f(x_2-) (a,b) f^{-1}(a,b)=(f^{-1}(a),f^{-1}(b)) a = f(x_1-) f^{-1}(a,b)=[x_1,f^{-1}b) b=f(x_2-) a=f(x_1-) b=f(x_2-) x_1 < x_2  f^{-1}(a,b)=[x_1,x_2)","['general-topology', 'analysis', 'continuity']"
17,"How can curves approximate curves, but lines cannot approximate lines?","How can curves approximate curves, but lines cannot approximate lines?",,"This is my first question on StackExchange, and i have very little knowledge of mathematics to tell if it is a physics issue or a math issue, so any info will be useful. Main question Let's say i want to find the length of a line. Take the following image for reference (1): it is a right triangle, and a naive/wrong way of finding the length of the hypotenuse . Is simple to show how this way of thinking can go wrong with a counterexample, if both smaller sides of the triangle have length $a$ , the hypotenuse has length $a\sqrt{2}$ . While the ""zigzagging"" gives the answer $2a$ . Again, i have a shamefully poor background on Real Analysis, so i could not explain the following main question : How can one of these two ""curvy"" lines approximate the other? My physics professor used this ""zigzagging"" argument (but with curves) to show how to calculate the variation of entropy in a thermodynamic process. And i just could not stop thinking about this problem in the first figure. Some context I had this problem in my mind when i saw this on my thermodynamics book (the book is in Portuguese), it showed how to approximate a small line element with fragments that make up a small Carnot Cycle . I don't think this is important to the discussion, so i will just put info regarding the context, just for curiosity: page 22 of this book (there should be an equivalent in English literature).","This is my first question on StackExchange, and i have very little knowledge of mathematics to tell if it is a physics issue or a math issue, so any info will be useful. Main question Let's say i want to find the length of a line. Take the following image for reference (1): it is a right triangle, and a naive/wrong way of finding the length of the hypotenuse . Is simple to show how this way of thinking can go wrong with a counterexample, if both smaller sides of the triangle have length , the hypotenuse has length . While the ""zigzagging"" gives the answer . Again, i have a shamefully poor background on Real Analysis, so i could not explain the following main question : How can one of these two ""curvy"" lines approximate the other? My physics professor used this ""zigzagging"" argument (but with curves) to show how to calculate the variation of entropy in a thermodynamic process. And i just could not stop thinking about this problem in the first figure. Some context I had this problem in my mind when i saw this on my thermodynamics book (the book is in Portuguese), it showed how to approximate a small line element with fragments that make up a small Carnot Cycle . I don't think this is important to the discussion, so i will just put info regarding the context, just for curiosity: page 22 of this book (there should be an equivalent in English literature).",a a\sqrt{2} 2a,"['calculus', 'geometry', 'analysis']"
18,Dual solutions of the 1d optimal transport problem,Dual solutions of the 1d optimal transport problem,,"Let $\mu$ and $\nu$ be two probability measures on the real line with finite $p$ moment for $p\in [1, \infty)$ . I am interested in the functions $f:\mathbb{R}\rightarrow \mathbb{R}$ and $g:\mathbb{R}\rightarrow \mathbb{R}$ that solve the dual of the optimal transport problem $$ OT(\mu, \nu) = \max\left\{ \int f\,d\mu + \int g\,d\nu,\, \Big|\,f\in L^1(\mu), \, g\in L^1(\nu) \, \text{and}\, f(x)+g(y)\leq h(x-y)\,  \right\}, $$ for an arbitrary convex function $h:\mathbb{R}\rightarrow \mathbb{R}$ . In particular I would be curious to know how $f$ (and $g$ ) look like if $\mu$ and $\nu$ are discrete measures (or in general not absolutely continuous). Thanks in advance for any help :) Update: If $\mu$ and $\nu$ are discrete then, if we set $f(x_1)=0$ , one of the dual solutions is given by $$ f(x_{i}) = \sum_{t=1}^{i-1} h(G^{-1}(F(x_t)) - x_{t+1}) -  h(G^{-1}(F(x_t)), x_{t}) \quad \text{with} \quad i = 2, \dots, n, $$ where $F$ is the cdf of $\mu$ and $G^{-1}$ is the quantile function of $\nu$ . This works even if there is no transport map between $\mu$ and $\nu$ . Moreover if $\mu$ and $\nu$ are continuous and have a connected support and $h$ is differentialble, then: $$ f(x) = - \int_{-\infty}^x h'(G^{-1}(F(t)) - t)\, dt. $$ (More on the motivation of this will come soon in the form of an answer).","Let and be two probability measures on the real line with finite moment for . I am interested in the functions and that solve the dual of the optimal transport problem for an arbitrary convex function . In particular I would be curious to know how (and ) look like if and are discrete measures (or in general not absolutely continuous). Thanks in advance for any help :) Update: If and are discrete then, if we set , one of the dual solutions is given by where is the cdf of and is the quantile function of . This works even if there is no transport map between and . Moreover if and are continuous and have a connected support and is differentialble, then: (More on the motivation of this will come soon in the form of an answer).","\mu \nu p p\in [1, \infty) f:\mathbb{R}\rightarrow \mathbb{R} g:\mathbb{R}\rightarrow \mathbb{R} 
OT(\mu, \nu) = \max\left\{ \int f\,d\mu + \int g\,d\nu,\, \Big|\,f\in L^1(\mu), \, g\in L^1(\nu) \, \text{and}\, f(x)+g(y)\leq h(x-y)\,  \right\},
 h:\mathbb{R}\rightarrow \mathbb{R} f g \mu \nu \mu \nu f(x_1)=0 
f(x_{i}) = \sum_{t=1}^{i-1} h(G^{-1}(F(x_t)) - x_{t+1}) -  h(G^{-1}(F(x_t)), x_{t}) \quad \text{with} \quad i = 2, \dots, n,
 F \mu G^{-1} \nu \mu \nu \mu \nu h 
f(x) = - \int_{-\infty}^x h'(G^{-1}(F(t)) - t)\, dt.
","['analysis', 'optimization', 'optimal-transport']"
19,Change of Variables and Integration Domain,Change of Variables and Integration Domain,,"This is a purely theoretic problem which doesn't involve any specific calculation. Suppose we have two coordinate systems for $\mathbb R^2$ and they are $xy$ -system and $uv$ -system respectively. Suppose we have $x=g(u,v)$ and $y=h(u,v)$ and we know both $g$ and $h$ are continuously differentiable, i.e. $C^1$ . Suppose the mapping $T(u,v):=(g(u,v),h(u,v))=(x,y)$ is injective. Suppose we have a small rectangle $S$ in the $uv$ -plane. The general idea to prove the formula for change of variables is approximating the image of $S$ under $T$ in the $xy$ -plane by a parallelogram in the $xy$ -plane. People use the image of the boundary of $S$ under $T$ to calculate the area of the parallelogram I quoted. By doing this, people use the fact that $T$ preserves boundaries, i.e. $T(\partial S)=\partial [T(S)].$ My question is , how can we assure that $T(\partial S)=\partial [T(S)]$ ? If the equality is not true, then how do we approximate the parallelogram in the $xy$ -plane? I know the equality is true if $T$ is a homeomorphism, but under our assumption, $T$ may not have a continuous inverse(though $T^{-1}$ is always defined on $T(S)$ due to our injective condition). Edit: I think I missed an important condition that the Jacobian determinant $\frac{\partial (x,y)}{\partial (u,v)}$ is nonzero in the interior of the integral domain in the $uv$ -plane. If we have this condition, then we can use the Inverse Function Theorem to guarantee the existence of local homeomorphism, and then our operation is valid.(Am I right?) Second Edit: In my calculus book, the formula for change of variables doesn't require the Jacobian determinant is nonzero. I am stuck here again. Third Edit: Intuitively I feel like the Jacobian must be nonzero and so I am trying to prove it now. Suppose the Jacobian is zero on a set with positive measure, which is denoted by $E$ . The key equation in the proof is that $$dx\,dy= \frac{\partial (x,y)}{\partial (u,v)} du\,dv.$$ On $E$ , we can have some $du,dv$ s.t. $du>0$ and $dv>0$ . Since the Jacobian is zero on $E$ , we have $dx\,dy=0$ . So a rectangle with area of $du\,dv(>0)$ is mapped to a set with zero measure by an injective continuous function $T$ , where $T$ is defined as above. Intuitively I feel like this statement is wrong, but I can not prove it. Any help with deriving a contradiction will be appreciate.","This is a purely theoretic problem which doesn't involve any specific calculation. Suppose we have two coordinate systems for and they are -system and -system respectively. Suppose we have and and we know both and are continuously differentiable, i.e. . Suppose the mapping is injective. Suppose we have a small rectangle in the -plane. The general idea to prove the formula for change of variables is approximating the image of under in the -plane by a parallelogram in the -plane. People use the image of the boundary of under to calculate the area of the parallelogram I quoted. By doing this, people use the fact that preserves boundaries, i.e. My question is , how can we assure that ? If the equality is not true, then how do we approximate the parallelogram in the -plane? I know the equality is true if is a homeomorphism, but under our assumption, may not have a continuous inverse(though is always defined on due to our injective condition). Edit: I think I missed an important condition that the Jacobian determinant is nonzero in the interior of the integral domain in the -plane. If we have this condition, then we can use the Inverse Function Theorem to guarantee the existence of local homeomorphism, and then our operation is valid.(Am I right?) Second Edit: In my calculus book, the formula for change of variables doesn't require the Jacobian determinant is nonzero. I am stuck here again. Third Edit: Intuitively I feel like the Jacobian must be nonzero and so I am trying to prove it now. Suppose the Jacobian is zero on a set with positive measure, which is denoted by . The key equation in the proof is that On , we can have some s.t. and . Since the Jacobian is zero on , we have . So a rectangle with area of is mapped to a set with zero measure by an injective continuous function , where is defined as above. Intuitively I feel like this statement is wrong, but I can not prove it. Any help with deriving a contradiction will be appreciate.","\mathbb R^2 xy uv x=g(u,v) y=h(u,v) g h C^1 T(u,v):=(g(u,v),h(u,v))=(x,y) S uv S T xy xy S T T T(\partial S)=\partial [T(S)]. T(\partial S)=\partial [T(S)] xy T T T^{-1} T(S) \frac{\partial (x,y)}{\partial (u,v)} uv E dx\,dy= \frac{\partial (x,y)}{\partial (u,v)} du\,dv. E du,dv du>0 dv>0 E dx\,dy=0 du\,dv(>0) T T","['calculus', 'general-topology', 'analysis', 'multivariable-calculus', 'change-of-variable']"
20,$\int_{0}^{1} \frac{\sin {xt}}{1+t} \ dt $ Lebesgue Integral,Lebesgue Integral,\int_{0}^{1} \frac{\sin {xt}}{1+t} \ dt ,"could you please help me. And let me notice how bad I am please. Problem: Calculate the derivative $F^\prime(x)$ of the function $F$ defined by the Lebesgue integral $$F(X) = \int_{0}^{1} \dfrac{\sin {xt}}{1+t} \ dt , x\in [0,1]$$ My attempt: Since the function $\dfrac{\sin {xt}}{1 + t}$ is a bounded function on $[0, 1]$ , then the integral (Riemann usual) $\int_{0}^{1} \dfrac{\sin {xt}}{1+t} \ dt $ converges, which implies that the integral of Lebesgue $\int_{0}^{1} \dfrac{\sin {xt}}{1 + t} \ dt $ exists, so $f$ is summable. Then using Theorem 9.6 which tells us that whatever the summable function $f$ in the segment $[a, b] \subset \mathbb{R}$ , in almost all points the equality $$\dfrac{\mathrm{d} }{\mathrm{d} x}\int_{a}^{x}f(t) \ dt = f(x)$$ Then the $F^\prime (x)$ is equal to. $$F^\prime(x) = \dfrac{\mathrm{d} }{\mathrm{d} x}\int_{0}^{1}\frac{\sin {xt}}{1+t} \ dt = \dfrac{\sin {x(1)}}{1+(1)} = \dfrac{\sin {x}}{2}$$ Many thanks","could you please help me. And let me notice how bad I am please. Problem: Calculate the derivative of the function defined by the Lebesgue integral My attempt: Since the function is a bounded function on , then the integral (Riemann usual) converges, which implies that the integral of Lebesgue exists, so is summable. Then using Theorem 9.6 which tells us that whatever the summable function in the segment , in almost all points the equality Then the is equal to. Many thanks","F^\prime(x) F F(X) = \int_{0}^{1} \dfrac{\sin {xt}}{1+t} \ dt , x\in [0,1] \dfrac{\sin {xt}}{1 + t} [0, 1] \int_{0}^{1} \dfrac{\sin {xt}}{1+t} \ dt  \int_{0}^{1} \dfrac{\sin {xt}}{1 + t} \ dt  f f [a, b] \subset \mathbb{R} \dfrac{\mathrm{d} }{\mathrm{d} x}\int_{a}^{x}f(t) \ dt = f(x) F^\prime (x) F^\prime(x) = \dfrac{\mathrm{d} }{\mathrm{d} x}\int_{0}^{1}\frac{\sin {xt}}{1+t} \ dt = \dfrac{\sin {x(1)}}{1+(1)} = \dfrac{\sin {x}}{2}","['analysis', 'definite-integrals', 'lebesgue-integral', 'lebesgue-measure', 'riemann-integration']"
21,Is the quotient map on a locally compact topological group closed?,Is the quotient map on a locally compact topological group closed?,,"Let $G$ be a locally compact group, $H$ a closed subgroup, $G/H$ the space of left cosets with the quotient topology, and $q:G\rightarrow G/H$ the projection map.  Also, let $C(G,\mathbb{C})$ be the space of continuous functions from $G$ to $\mathbb{C}$ .  The author of a book I am reading is attempting to define a subspace of $C(G,\mathbb{C})$ using (among others) the condition "" $q($ supp $(f))$ is compact"".  Here is my problem: unless $q$ is a closed map I do not see how this condition can respect linearity (i.e., if $q($ supp $(f))$ and $q($ supp $(g))$ are compact then so is $q($ supp $(f+g))$ for $f,g$ in this subspace).  If $G$ is simply a topological group and $H$ is compact, then it is true that $q$ is closed.  But in particular for locally compact groups can we say that $q$ is closed?  My belief is that this is not true, but I could be wrong.  Since the other conditions imposed on this ""subspace"" don't seem to help the matter, this would appear to be an error in the text.","Let be a locally compact group, a closed subgroup, the space of left cosets with the quotient topology, and the projection map.  Also, let be the space of continuous functions from to .  The author of a book I am reading is attempting to define a subspace of using (among others) the condition "" supp is compact"".  Here is my problem: unless is a closed map I do not see how this condition can respect linearity (i.e., if supp and supp are compact then so is supp for in this subspace).  If is simply a topological group and is compact, then it is true that is closed.  But in particular for locally compact groups can we say that is closed?  My belief is that this is not true, but I could be wrong.  Since the other conditions imposed on this ""subspace"" don't seem to help the matter, this would appear to be an error in the text.","G H G/H q:G\rightarrow G/H C(G,\mathbb{C}) G \mathbb{C} C(G,\mathbb{C}) q( (f)) q q( (f)) q( (g)) q( (f+g)) f,g G H q q","['functional-analysis', 'analysis', 'topological-groups']"
22,Why is $\dim_{B}F \le n$ in $\mathbb R^n$? (Upper Bound on MinkowskiBouligand dimension),Why is  in ? (Upper Bound on MinkowskiBouligand dimension),\dim_{B}F \le n \mathbb R^n,"(Please skip to the end for a word on notation) For $F \subset\mathbb R^n$ , where $F\ne \varnothing$ , we have $$0 \le \underline\dim_B F \le \overline\dim_B F \le n$$ and hence $$\dim_B F \le n$$ where $\dim_B F$ denotes the box-counting dimension of $F$ . $\underline\dim_B F$ and $\overline\dim_B F$ denote the lower and upper box-counting dimension respectively. I need help understanding the proof of the last inequality, i.e. the $\le n$ part. The proof in the book is: The first two inequalities are obvious; for the third, $F$ may be enclosed in a large cube $C$ so by counting $$ -mesh squares $N_(F)  N_(C)  c^{n}$ for some constant $c$ . Since $F\subset C$ , $N_\delta(F) \le N_\delta(C)$ is clear. Why is $N_\delta(C) \le c\delta^{-n}$ for some $c$ ? Notation: About $\dim_B F$ and $N_\delta(F)$ : The lower and upper box-counting dimensions of a subset $F$ of $^n$ are given by $$\underline{\dim}_B F = \underline{\lim}_{\delta\to 0} \frac{\log N_\delta(F)}{-\log \delta}$$ $$\overline{\dim}_B F = \overline{\lim}_{\delta\to 0} \frac{\log N_\delta(F)}{-\log \delta}$$ and the box-counting dimension of $F$ by $$\dim_B F = \lim_{\delta\to 0} \frac{\log N_\delta(F)}{-\log \delta}$$ (if this limit exists), where $N_\delta(F)$ is any of the following: the smallest number of sets of diameter at most $$ that cover $F$ ; the smallest number of closed balls of radius $$ that cover $F$ ; the smallest number of cubes of side $$ that cover $F$ ; the number of $$ -mesh cubes that intersect $F$ ; the largest number of disjoint balls of radius $$ with centres in $F$ . What is a $\delta$ -mesh? $$[m_1\delta, (m_1+1)\delta] \times [m_2\delta, (m_2+1)\delta] \times \ldots \times [m_n\delta, (m_n+1)\delta]$$ where $m_i \in\mathbb Z$ for every $1 \le i \le n$ , is called a $\delta$ -mesh (or a $\delta$ -grid) in $\mathbb R^n$ .","(Please skip to the end for a word on notation) For , where , we have and hence where denotes the box-counting dimension of . and denote the lower and upper box-counting dimension respectively. I need help understanding the proof of the last inequality, i.e. the part. The proof in the book is: The first two inequalities are obvious; for the third, may be enclosed in a large cube so by counting -mesh squares for some constant . Since , is clear. Why is for some ? Notation: About and : The lower and upper box-counting dimensions of a subset of are given by and the box-counting dimension of by (if this limit exists), where is any of the following: the smallest number of sets of diameter at most that cover ; the smallest number of closed balls of radius that cover ; the smallest number of cubes of side that cover ; the number of -mesh cubes that intersect ; the largest number of disjoint balls of radius with centres in . What is a -mesh? where for every , is called a -mesh (or a -grid) in .","F \subset\mathbb R^n F\ne \varnothing 0 \le \underline\dim_B F \le \overline\dim_B F \le n \dim_B F \le n \dim_B F F \underline\dim_B F \overline\dim_B F \le n F C  N_(F)  N_(C)  c^{n} c F\subset C N_\delta(F) \le N_\delta(C) N_\delta(C) \le c\delta^{-n} c \dim_B F N_\delta(F) F ^n \underline{\dim}_B F = \underline{\lim}_{\delta\to 0} \frac{\log N_\delta(F)}{-\log \delta} \overline{\dim}_B F = \overline{\lim}_{\delta\to 0} \frac{\log N_\delta(F)}{-\log \delta} F \dim_B F = \lim_{\delta\to 0} \frac{\log N_\delta(F)}{-\log \delta} N_\delta(F)  F  F  F  F  F \delta [m_1\delta, (m_1+1)\delta] \times [m_2\delta, (m_2+1)\delta] \times \ldots \times [m_n\delta, (m_n+1)\delta] m_i \in\mathbb Z 1 \le i \le n \delta \delta \mathbb R^n","['real-analysis', 'geometry', 'analysis']"
23,On the Topological Properties of a Solution Set of Equations,On the Topological Properties of a Solution Set of Equations,,"The following is from some qualifying exam. Let $f(x,y,z)=2x^{2}-2xy+5y^{2}+z^{4}-6$ and $g(x,y,z)=xyz-1$ , is the set \begin{align*} S=\{(x,y,z)\in\mathbb{R}^{3}:f(x,y,z)=g(x,y,z)=0\} \end{align*} closed, compact, connected? To tackle this problem, I let $h(x,y,z)=f(x,y,z)-g(x,y,z)$ , then $S=h^{-1}(\{0\})\cap g^{-1}(\{0\})$ . While both $h$ and $g$ are continuous, so $S$ is closed. But I cannot determine if $S$ is compact or connected. For the compactness, one may try to determine if $S$ is a bounded set, but how?","The following is from some qualifying exam. Let and , is the set closed, compact, connected? To tackle this problem, I let , then . While both and are continuous, so is closed. But I cannot determine if is compact or connected. For the compactness, one may try to determine if is a bounded set, but how?","f(x,y,z)=2x^{2}-2xy+5y^{2}+z^{4}-6 g(x,y,z)=xyz-1 \begin{align*}
S=\{(x,y,z)\in\mathbb{R}^{3}:f(x,y,z)=g(x,y,z)=0\}
\end{align*} h(x,y,z)=f(x,y,z)-g(x,y,z) S=h^{-1}(\{0\})\cap g^{-1}(\{0\}) h g S S S","['general-topology', 'analysis']"
24,"Assume that $K$ is a compact subset of $\mathbb{R}$. Prove directly that $K=\{(x,0) :x\in K\}$ is a compact subset of $\mathbb{R}^2$.",Assume that  is a compact subset of . Prove directly that  is a compact subset of .,"K \mathbb{R} K=\{(x,0) :x\in K\} \mathbb{R}^2","Assume that $K$ is a compact subset of $\mathbb{R}$ . Prove directly that $K=\{(x,0) :x\in K\}$ is a compact subset of $\mathbb{R}^2$ . I have been given the following hint: Suppose that $\{U_i\}_{i\in I}$ is a cover of $K$ by open sets. These $U_i$ are subsets of the plane $\mathbb{R}^2$ . Find a collection $\{V_i\}_{i\in I}$ of open subsets of the real line $\mathbb{R}$ that cover $K$ . Be sure to prove that your sets $V_i$ are openyou cant just say that they are open, you have to prove that. It really seems kind of like common sense that if all the values of $x$ form a compact set, then $(x,0)$ forms a compact set since $0$ is just $0$ . I don't even really understand the hint given.","Assume that is a compact subset of . Prove directly that is a compact subset of . I have been given the following hint: Suppose that is a cover of by open sets. These are subsets of the plane . Find a collection of open subsets of the real line that cover . Be sure to prove that your sets are openyou cant just say that they are open, you have to prove that. It really seems kind of like common sense that if all the values of form a compact set, then forms a compact set since is just . I don't even really understand the hint given.","K \mathbb{R} K=\{(x,0) :x\in K\} \mathbb{R}^2 \{U_i\}_{i\in I} K U_i \mathbb{R}^2 \{V_i\}_{i\in I} \mathbb{R} K V_i x (x,0) 0 0","['real-analysis', 'analysis', 'compactness']"
25,Doubt in the geometric interpretation of Newton's method of calculating the roots of the equation $f(x)$.,Doubt in the geometric interpretation of Newton's method of calculating the roots of the equation .,f(x),"This is a question from Rudin(trying to prove the newtons method of calculating the roots of the function) and which aks me to state the geometric intuition of : $$x_n=x_{n-1}-\frac{f(x_{n-1})}{f'(x_{n-1})}$$ I know that the the line to the tangent at the graph at the point $(x_1,f(x_1))$ has the equation $y = f(x_1)+f'(x_1)(x-x_1)$ and it crosses the graph at the point $x_2$ which is given by the above equation. Now, while looking at the equation,I found something like this: $\frac{x_1.f(x_1+h)-f(x_1)(x_1+h)}{f(x_1+h)-f(x_1)}=x_2$ which forces me to think of the exterior division formula where $m=f(x_1+h)$ and $n= f(x_1)$ and the equation assumes the form $\frac{m.x_1-n(x_1+h)}{m-n}=x_2$ .Since $x_1 \in (\alpha,b)$ and $f(\alpha)=0(f(b)>0)$ and the point $x_2$ is the exterior point , so $x_2$ is closer to $\alpha$ .Why am I getting something similar to the exterior division formula?","This is a question from Rudin(trying to prove the newtons method of calculating the roots of the function) and which aks me to state the geometric intuition of : I know that the the line to the tangent at the graph at the point has the equation and it crosses the graph at the point which is given by the above equation. Now, while looking at the equation,I found something like this: which forces me to think of the exterior division formula where and and the equation assumes the form .Since and and the point is the exterior point , so is closer to .Why am I getting something similar to the exterior division formula?","x_n=x_{n-1}-\frac{f(x_{n-1})}{f'(x_{n-1})} (x_1,f(x_1)) y = f(x_1)+f'(x_1)(x-x_1) x_2 \frac{x_1.f(x_1+h)-f(x_1)(x_1+h)}{f(x_1+h)-f(x_1)}=x_2 m=f(x_1+h) n= f(x_1) \frac{m.x_1-n(x_1+h)}{m-n}=x_2 x_1 \in (\alpha,b) f(\alpha)=0(f(b)>0) x_2 x_2 \alpha","['real-analysis', 'analysis']"
26,Inequality regarding L^6 norm of a trigonometric polynomial,Inequality regarding L^6 norm of a trigonometric polynomial,,"I am reading Bourgain's paper ""Fourier transform restriction phenomena for certain lattice subsets and applications to nonlinear evolution equations"", and am stuck on the following inequality (p117): Let $f=\sum_{-N}^{N}a_n e^{i(nx+n^2t)}$ . Then by direct calculation one obtains $$ \|f\|_6^6=\|f^3\|_2^2=\sum_{n,j}\left|\sum_{n_1^2+n_2^2+(n-n_1-n_2)^2=j} a_{n_1} a_{n_2} a_{n-n_1-n_2} \right|^2 $$ The bit I am confused by is he then claims that this is bounded by $$ \max_{|n|<3N \\ |j|<3N^2} r_{n,j} \cdot \left( \sum_{m<N} |a_m|^2 \right)^3 $$ where $$r_{n,j} = \#\{(n_1,n_2): |n_i|<N \text{ and } n_1^2+n_2^2+(n-n_1-n_2)^2=j\}$$ Where does this last inequality come from? I had been trying to use some kind of bound of the form $abc \lesssim a^3+b^3+c^3$ and the embedding $l^3 \subset l^2$ but I don't think the first inequality there is actually true and even with these I don't see what happens to the sum over $(n,j)$ out front. Thanks in advance.","I am reading Bourgain's paper ""Fourier transform restriction phenomena for certain lattice subsets and applications to nonlinear evolution equations"", and am stuck on the following inequality (p117): Let . Then by direct calculation one obtains The bit I am confused by is he then claims that this is bounded by where Where does this last inequality come from? I had been trying to use some kind of bound of the form and the embedding but I don't think the first inequality there is actually true and even with these I don't see what happens to the sum over out front. Thanks in advance.","f=\sum_{-N}^{N}a_n e^{i(nx+n^2t)} 
\|f\|_6^6=\|f^3\|_2^2=\sum_{n,j}\left|\sum_{n_1^2+n_2^2+(n-n_1-n_2)^2=j} a_{n_1} a_{n_2} a_{n-n_1-n_2} \right|^2
 
\max_{|n|<3N \\ |j|<3N^2} r_{n,j} \cdot \left( \sum_{m<N} |a_m|^2 \right)^3
 r_{n,j} = \#\{(n_1,n_2): |n_i|<N \text{ and } n_1^2+n_2^2+(n-n_1-n_2)^2=j\} abc \lesssim a^3+b^3+c^3 l^3 \subset l^2 (n,j)","['analysis', 'inequality', 'partial-differential-equations', 'fourier-series', 'trigonometric-series']"
27,Proof of Lemma 6.3 in Carothers' Real Analysis,Proof of Lemma 6.3 in Carothers' Real Analysis,,"Lemma 6.3 (Chapter 6 - Connectedness) : Let $E$ be a subset of a metric space $(M,d)$ . If $U$ and $V$ are disjoint, open sets in $E$ , then there are disjoint open sets $A$ and $B$ in $M$ such that $U= A \cap E$ and $V = B \cap E$ . The proof roughly goes as follows (I haven't typed it out exactly , but I have captured all the important details) - Since $U$ is open in $E$ , for each $x\in U$ , $\exists\epsilon_x > 0$ such that $E\cap B(x,\epsilon_x) \subset U$ . Since $V$ is open in $E$ , for each $y\in V$ , $\exists\delta_y > 0$ such that $E\cap B(y,\delta_y) \subset V$ . Since $U\cap V = \varnothing$ , we also have $E\cap B(x,\epsilon_x) \cap B(y,\delta_y) = \varnothing$ . Now, the author claims that : For every $x\in U$ and $y\in V$ , $$B\left(x,\frac{\epsilon_x}{2}\right) \cap B\left(y,\frac{\delta_y}{2}\right) = \varnothing$$ Why is this true? I'm trying to show this in the following way. Suppose $z \in B\left(x,\frac{\epsilon_x}{2}\right) \cap B\left(y,\frac{\delta_y}{2}\right)$ . So, $d(x,z) < \epsilon_x/2$ and $d(y,z) = \delta_y/2$ . To show that the intersection is non-empty would amount to showing $d(x,z) < \epsilon_x/2$ and $d(y,z) = \delta_y/2$ cannot hold together. How do I do this? Thus, $$A = \bigcup_{x\in U} B\left(x,\frac{\epsilon_x}{2}\right) \text{ and } B = \bigcup_{y\in V} B\left(y,\frac{\delta_y}{2}\right)$$ work. Why does this choice of $A, B$ work? All I need to show for this, is that $U = A\cap E$ and $V= B\cap E$ right? Is anything else needed to be done? Thank you. Picture for Reference: Attempting a Proof by Contradiction: As mentioned in an answer, $B(x,r/2)\subset B(x,r)$ . Suppose $\exists x \in U, \exists y\in V$ for which $B\left(x,\frac{\epsilon_x}{2}\right) \cap B\left(y,\frac{\delta_y}{2}\right) \ne \varnothing$ . Also, $$E\cap B(x,\epsilon_x) \subset U \implies E\cap B(x,\epsilon_x/2) \subset U$$ and $$E\cap B(y,\delta_y) \subset V \implies E\cap B(y,\delta_y/2) \subset V$$ What's next? I'm not able to find a contradiction. $U\cap V = \varnothing \implies E\cap B(x,\epsilon_x) \cap B(y,\delta_y) = \varnothing$ , and I believe this is what we want to contradict.","Lemma 6.3 (Chapter 6 - Connectedness) : Let be a subset of a metric space . If and are disjoint, open sets in , then there are disjoint open sets and in such that and . The proof roughly goes as follows (I haven't typed it out exactly , but I have captured all the important details) - Since is open in , for each , such that . Since is open in , for each , such that . Since , we also have . Now, the author claims that : For every and , Why is this true? I'm trying to show this in the following way. Suppose . So, and . To show that the intersection is non-empty would amount to showing and cannot hold together. How do I do this? Thus, work. Why does this choice of work? All I need to show for this, is that and right? Is anything else needed to be done? Thank you. Picture for Reference: Attempting a Proof by Contradiction: As mentioned in an answer, . Suppose for which . Also, and What's next? I'm not able to find a contradiction. , and I believe this is what we want to contradict.","E (M,d) U V E A B M U= A \cap E V = B \cap E U E x\in U \exists\epsilon_x > 0 E\cap B(x,\epsilon_x) \subset U V E y\in V \exists\delta_y > 0 E\cap B(y,\delta_y) \subset V U\cap V = \varnothing E\cap B(x,\epsilon_x) \cap B(y,\delta_y) = \varnothing x\in U y\in V B\left(x,\frac{\epsilon_x}{2}\right) \cap B\left(y,\frac{\delta_y}{2}\right) = \varnothing z \in B\left(x,\frac{\epsilon_x}{2}\right) \cap B\left(y,\frac{\delta_y}{2}\right) d(x,z) < \epsilon_x/2 d(y,z) = \delta_y/2 d(x,z) < \epsilon_x/2 d(y,z) = \delta_y/2 A = \bigcup_{x\in U} B\left(x,\frac{\epsilon_x}{2}\right) \text{ and } B = \bigcup_{y\in V} B\left(y,\frac{\delta_y}{2}\right) A, B U = A\cap E V= B\cap E B(x,r/2)\subset B(x,r) \exists x \in U, \exists y\in V B\left(x,\frac{\epsilon_x}{2}\right) \cap B\left(y,\frac{\delta_y}{2}\right) \ne \varnothing E\cap B(x,\epsilon_x) \subset U \implies E\cap B(x,\epsilon_x/2) \subset U E\cap B(y,\delta_y) \subset V \implies E\cap B(y,\delta_y/2) \subset V U\cap V = \varnothing \implies E\cap B(x,\epsilon_x) \cap B(y,\delta_y) = \varnothing","['real-analysis', 'analysis', 'metric-spaces', 'proof-explanation']"
28,"Integral comparison test for $\frac{1}{x \ln(x)^2} $ or $\frac{1}{x \log(x)^2}$ on $(0, \frac{1}{2})$",Integral comparison test for  or  on,"\frac{1}{x \ln(x)^2}  \frac{1}{x \log(x)^2} (0, \frac{1}{2})","I am trying to show $f(x) = \frac{1}{x \ln(x)^2}$ on the interval $(0,\frac{1}{2})$ is in $L^p$ only for $p=1$ and not for $p>1$ . I can show $||f||_{L^1( (0,\frac{1}{2}) )} < \infty $ but for $p>1$ , I have a hard time figuring out how to evaluate $\int_0^{\frac{1}{2}} \frac{1}{x^p \ln(x)^{2p}} dx$ . Change of variable with $u= \ln(x)$ gives me $\int_{-\infty}^{\ln(\frac{1}{2})} \frac{1}{(e^u)^{p-1} u^{2p}} du$ which looks like I should try an integral test instead.","I am trying to show on the interval is in only for and not for . I can show but for , I have a hard time figuring out how to evaluate . Change of variable with gives me which looks like I should try an integral test instead.","f(x) = \frac{1}{x \ln(x)^2} (0,\frac{1}{2}) L^p p=1 p>1 ||f||_{L^1( (0,\frac{1}{2}) )} < \infty  p>1 \int_0^{\frac{1}{2}} \frac{1}{x^p \ln(x)^{2p}} dx u= \ln(x) \int_{-\infty}^{\ln(\frac{1}{2})} \frac{1}{(e^u)^{p-1} u^{2p}} du","['functional-analysis', 'analysis', 'measure-theory', 'partial-differential-equations', 'lebesgue-integral']"
29,"If $X\subset \mathbb{R}$ is bounded and a infinity set, then $X \cap X^\prime \not = \emptyset$.","If  is bounded and a infinity set, then .",X\subset \mathbb{R} X \cap X^\prime \not = \emptyset,"First of all, let us define $X^\prime = \{a \in \mathbb{R} \mid \forall\epsilon>0, ((a - \epsilon, a + \epsilon)-\{a\}) \cap X \not = \emptyset\}$ . The result is true when $X$ is an uncountable set: if $X \cap X^\prime = \emptyset$ , then for all $x \in X$ there is an $\epsilon_x>0$ such that $((x - \epsilon_x, x + \epsilon_x)-\{x\}) \cap X = \emptyset$ . It implies that all points of $X$ are isolated points. Then, $X$ should be a countable set (a contradition). So, $X \cap X^\prime \not = \emptyset$ . But, what happens when $X$ is a countable set?","First of all, let us define . The result is true when is an uncountable set: if , then for all there is an such that . It implies that all points of are isolated points. Then, should be a countable set (a contradition). So, . But, what happens when is a countable set?","X^\prime = \{a \in \mathbb{R} \mid \forall\epsilon>0, ((a - \epsilon, a + \epsilon)-\{a\}) \cap X \not = \emptyset\} X X \cap X^\prime = \emptyset x \in X \epsilon_x>0 ((x - \epsilon_x, x + \epsilon_x)-\{x\}) \cap X = \emptyset X X X \cap X^\prime \not = \emptyset X","['general-topology', 'analysis', 'metric-spaces']"
30,Pointwise and uniform convergence of $f_n(x)=\cos \frac{nx}{1+n^2}$,Pointwise and uniform convergence of,f_n(x)=\cos \frac{nx}{1+n^2},"Study the pointwise and uniform convergence of $f_n(x)=\cos \frac{nx}{1+n^2}$ for $x \in \mathbb{R}$ . If $x=0$ it is $f_n(0)=1$ , for $x \ne 0$ it is $$\lim_{n \to \infty} \cos\frac{nx}{1+n^2}=\cos 0=1$$ So $f_n(x)$ converges pointwise to $f(x)=1$ for all $x \in \mathbb{R}$ . To study the uniform convergence I must evaluate $$\lim_{n \to \infty} \sup_{x \in \mathbb{R}}\left|\cos \frac{nx}{1+n^2}-1\right|=\lim_{n \to \infty} \sup_{x \in \mathbb{R}}\left(1-\cos \frac{nx}{1+n^2}\right)$$ Since the derivative of $1-\cos \frac{nx}{1+n^2}$ exists for all $x\in\mathbb{R}$ , it is $$\frac{d}{dx} \left(1-\cos \frac{nx}{1+n^2}\right)=\frac{n}{1+n^2} \sin \frac{nx}{1+n^2} \geq 0$$ $$\iff 2k\pi \leq \frac{nx}{1+n^2} \leq (2k+1)\pi \iff \frac{1+n^2}{n}2k\pi \leq x \leq \frac{1+n^2}{n}(2k+1)\pi$$ For $k\in\mathbb{Z}$ , so $$\lim_{n \to \infty} \sup_{x \in \mathbb{R}}\left(1-\cos \frac{nx}{1+n^2}\right)=\lim_{n \to \infty} \left(1-\cos \frac{n \left(\frac{1+n^2}{n}(2k+1)\pi\right)}{1+n^2}\right)=\lim_{n\to\infty}(1-\cos\pi)=2$$ So $f_n$ is not uniformly convergent in $\mathbb{R}$ . Is this correct? Some more questions: I've noticed that $f_n(\frac{1+n^2}{n})=\cos 1$ , can I conclude that since there exists at least one $\bar{x}=\frac{1+n^2}{n}$ such that $|f_n(\bar{x})-f(\bar{x})|=1-\cos 1 \ne 0$ so $f_n$ cannot be uniform convergent in whole $\mathbb{R}$ because it it will never be $|f_n(x)-f(x)|<\varepsilon$ for all $x \in \mathbb{R}$ because of $\bar{x}$ . Is this correct? It is $\cos \frac{nx}{1+n^2} \approx 1-\frac{n^2 x^2}{2(1+n^2)^2}$ as $n \to \infty$ , can this be useful to evaluate the limit of the supremum? Or is it useless because the asymptotic behaviour can't give informations about a supremum? Is there a way to identify if there are some subsets of $\mathbb{R}$ where $f_n$ can converge uniformly to $f$ ?","Study the pointwise and uniform convergence of for . If it is , for it is So converges pointwise to for all . To study the uniform convergence I must evaluate Since the derivative of exists for all , it is For , so So is not uniformly convergent in . Is this correct? Some more questions: I've noticed that , can I conclude that since there exists at least one such that so cannot be uniform convergent in whole because it it will never be for all because of . Is this correct? It is as , can this be useful to evaluate the limit of the supremum? Or is it useless because the asymptotic behaviour can't give informations about a supremum? Is there a way to identify if there are some subsets of where can converge uniformly to ?",f_n(x)=\cos \frac{nx}{1+n^2} x \in \mathbb{R} x=0 f_n(0)=1 x \ne 0 \lim_{n \to \infty} \cos\frac{nx}{1+n^2}=\cos 0=1 f_n(x) f(x)=1 x \in \mathbb{R} \lim_{n \to \infty} \sup_{x \in \mathbb{R}}\left|\cos \frac{nx}{1+n^2}-1\right|=\lim_{n \to \infty} \sup_{x \in \mathbb{R}}\left(1-\cos \frac{nx}{1+n^2}\right) 1-\cos \frac{nx}{1+n^2} x\in\mathbb{R} \frac{d}{dx} \left(1-\cos \frac{nx}{1+n^2}\right)=\frac{n}{1+n^2} \sin \frac{nx}{1+n^2} \geq 0 \iff 2k\pi \leq \frac{nx}{1+n^2} \leq (2k+1)\pi \iff \frac{1+n^2}{n}2k\pi \leq x \leq \frac{1+n^2}{n}(2k+1)\pi k\in\mathbb{Z} \lim_{n \to \infty} \sup_{x \in \mathbb{R}}\left(1-\cos \frac{nx}{1+n^2}\right)=\lim_{n \to \infty} \left(1-\cos \frac{n \left(\frac{1+n^2}{n}(2k+1)\pi\right)}{1+n^2}\right)=\lim_{n\to\infty}(1-\cos\pi)=2 f_n \mathbb{R} f_n(\frac{1+n^2}{n})=\cos 1 \bar{x}=\frac{1+n^2}{n} |f_n(\bar{x})-f(\bar{x})|=1-\cos 1 \ne 0 f_n \mathbb{R} |f_n(x)-f(x)|<\varepsilon x \in \mathbb{R} \bar{x} \cos \frac{nx}{1+n^2} \approx 1-\frac{n^2 x^2}{2(1+n^2)^2} n \to \infty \mathbb{R} f_n f,"['sequences-and-series', 'analysis', 'uniform-convergence']"
31,Does AM-GM Follow From the Convexity of Some Function,Does AM-GM Follow From the Convexity of Some Function,,"The AM-GM for $n = 2$ is $$\frac{x+y}{2} \ge (xy)^{1/2}$$ This is very easy to prove with algebra. However, I am wondering if there is a proof using convexity. That is, can we find a convex function $f:\mathbb{R} \to \mathbb{R}$ so that for some $t = t(x, y)$ between $0$ and $1$ we have $$tf(x) + (1-t)f(y) = \frac {x+y}{2}$$ and $$f(tx + (1-t)y) = (xy)^{1/2}$$ Is there such a proof?","The AM-GM for is This is very easy to prove with algebra. However, I am wondering if there is a proof using convexity. That is, can we find a convex function so that for some between and we have and Is there such a proof?","n = 2 \frac{x+y}{2} \ge (xy)^{1/2} f:\mathbb{R} \to \mathbb{R} t = t(x, y) 0 1 tf(x) + (1-t)f(y) = \frac {x+y}{2} f(tx + (1-t)y) = (xy)^{1/2}","['analysis', 'inequality', 'a.m.-g.m.-inequality']"
32,Functions with mutually orthogonal gradients at every point,Functions with mutually orthogonal gradients at every point,,"I am not completely sure if the question is too broad, but here goes. If we are given a function $f:\mathbb{R}^2 \to \mathbb{R}$ , is there any way to find other non-constant functions $g:\mathbb{R}^2 \to \mathbb{R}$ such that at every point their gradients are orthogonal? In other words, given $f$ can we find a non-constant $g$ such that $\nabla f\cdot \nabla{g}=0$ for every $(x,y) \in \mathbb{R}^2$ ? For given $f$ I am interested in a criterion if such functions $g$ exist, and if so how would be a way to find them. As an example, if $f(x,y)=x$ , then $g(x,y)=y$ satisfies that property. If $f(x,y)=x^2+y^2$ ? It would have to be a solution of the PDE $xf_x+yf_y=0$ , which unfortunately I don't remember my PDEs now to try to solve. The broader question is essentially if we can say anything about the following PDE for a given $f$ . $$f_xg_x+f_yg_y=0$$ Assume any nice conditions you may want about $f$ , such as $f\in C^{\infty}$ etc. If the topic is too broad for an answer, I would at least appreciate any pointers for a direction to read further about it. This also relates to products of harmonic functions, specifically a product of two harmonic functions is also harmonic if the above condition on the gradients is satisfied.","I am not completely sure if the question is too broad, but here goes. If we are given a function , is there any way to find other non-constant functions such that at every point their gradients are orthogonal? In other words, given can we find a non-constant such that for every ? For given I am interested in a criterion if such functions exist, and if so how would be a way to find them. As an example, if , then satisfies that property. If ? It would have to be a solution of the PDE , which unfortunately I don't remember my PDEs now to try to solve. The broader question is essentially if we can say anything about the following PDE for a given . Assume any nice conditions you may want about , such as etc. If the topic is too broad for an answer, I would at least appreciate any pointers for a direction to read further about it. This also relates to products of harmonic functions, specifically a product of two harmonic functions is also harmonic if the above condition on the gradients is satisfied.","f:\mathbb{R}^2 \to \mathbb{R} g:\mathbb{R}^2 \to \mathbb{R} f g \nabla f\cdot \nabla{g}=0 (x,y) \in \mathbb{R}^2 f g f(x,y)=x g(x,y)=y f(x,y)=x^2+y^2 xf_x+yf_y=0 f f_xg_x+f_yg_y=0 f f\in C^{\infty}","['analysis', 'partial-differential-equations']"
33,How is the formula for the volume of a body in $\mathbb{R}^n$ derived?,How is the formula for the volume of a body in  derived?,\mathbb{R}^n,"Here is a formula for the volume of a general body in $\mathbb{R}^n$ I came across, that uses spherical polar coordinates. I'm not sure how it came about: Here, $K$ is a body with $0$ in its interior, and for each direction $\theta \in S^{n-1}$ , let $r(\theta)$ be the radius of $K$ in that direction. $$ \text{vol}(K) = nv_n \int_{S^{n-1}} \int_{0}^{r(\theta)} s^{n-1}ds d\sigma = v_n \int_{S^{n-1}} r(\theta)^n d\sigma(\theta)$$ $S^{n-1}$ is the sphere of radius $1$ in $n-1$ dimensions, and $v_n$ is the volume of the Euclidean ball of radius $1$ in $n$ -dimensions. I understand how the first integral goes to the second, but how did this formula come about in the first place? I'm trying to start off with the regular Cartesian integral for volume which looks like $$\int \int ... \int dx_1 dx_2...dx_n$$ but I'm not able to proceed from here. It has been a long while since I posted this. Could someone please add a detailed explanation? It would be really helpful.","Here is a formula for the volume of a general body in I came across, that uses spherical polar coordinates. I'm not sure how it came about: Here, is a body with in its interior, and for each direction , let be the radius of in that direction. is the sphere of radius in dimensions, and is the volume of the Euclidean ball of radius in -dimensions. I understand how the first integral goes to the second, but how did this formula come about in the first place? I'm trying to start off with the regular Cartesian integral for volume which looks like but I'm not able to proceed from here. It has been a long while since I posted this. Could someone please add a detailed explanation? It would be really helpful.",\mathbb{R}^n K 0 \theta \in S^{n-1} r(\theta) K  \text{vol}(K) = nv_n \int_{S^{n-1}} \int_{0}^{r(\theta)} s^{n-1}ds d\sigma = v_n \int_{S^{n-1}} r(\theta)^n d\sigma(\theta) S^{n-1} 1 n-1 v_n 1 n \int \int ... \int dx_1 dx_2...dx_n,"['integration', 'geometry', 'analysis', 'volume', 'spherical-coordinates']"
34,How to use the extended version of Weierstrass's theorem?,How to use the extended version of Weierstrass's theorem?,,"After my question Question for the function $f(x)=\log\left(\frac{x^2}{x-2}\right)$ , I have obtain a very good answer and I remember that I have never studied this theorem during my period at my university (1993) "" extended version of Weierstrass's theorem"" . Obviously there are several pages on the internet that discuss the proof of the theorem. I would like to understand well the hypotheses and when and how to apply the theorem in practical cases especially for the students of an high school . I have seen that are used the sequences and the minimum and maximum limits ( $\lim \sup$ , $\lim \min$ )  and I do not remember their use and the meaning of this concepts.","After my question Question for the function $f(x)=\log\left(\frac{x^2}{x-2}\right)$ , I have obtain a very good answer and I remember that I have never studied this theorem during my period at my university (1993) "" extended version of Weierstrass's theorem"" . Obviously there are several pages on the internet that discuss the proof of the theorem. I would like to understand well the hypotheses and when and how to apply the theorem in practical cases especially for the students of an high school . I have seen that are used the sequences and the minimum and maximum limits ( , )  and I do not remember their use and the meaning of this concepts.",\lim \sup \lim \min,"['real-analysis', 'analysis', 'examples-counterexamples', 'extreme-value-theorem']"
35,Convergence of series using domination,Convergence of series using domination,,"Let $(x_n)_n$ be a sequence of $]0,+\infty[,y_n=\sum_{k=1}^nx_k$ such that $\lim_n y_n=+\infty.$ Let $p>1.$ Prove that $$\sum_{n}\dfrac{x_n}{y_n(\ln(y_n))^p}$$ converges. Maybe the easiest way to prove it is to show that $\sum_n\dfrac{x_n}{y_n(\ln(y_n))^p}$ is dominated with a convergent series. Any ideas ?",Let be a sequence of such that Let Prove that converges. Maybe the easiest way to prove it is to show that is dominated with a convergent series. Any ideas ?,"(x_n)_n ]0,+\infty[,y_n=\sum_{k=1}^nx_k \lim_n y_n=+\infty. p>1. \sum_{n}\dfrac{x_n}{y_n(\ln(y_n))^p} \sum_n\dfrac{x_n}{y_n(\ln(y_n))^p}","['real-analysis', 'sequences-and-series', 'analysis']"
36,"Let $f:\mathbb {R}^2 \to \mathbb {R}$ be a $C^1$-function with $D_2f(0,0)=1$ [closed]",Let  be a -function with  [closed],"f:\mathbb {R}^2 \to \mathbb {R} C^1 D_2f(0,0)=1","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question What's true and what's false? Let $f:\mathbb {R}^2 \to \mathbb {R}$ be a $C^1$ -function with $D_2f(0,0)=1$ . Assume $g:\mathbb {R}\to \mathbb {R}$ fulfills $g(x)=0$ and $f(x,g(x))=0$ , then $g$ is in a neighbourhood of $0$ continuous differentiable. For every function $g:\mathbb {R}^m \to \mathbb {R}^n$ exists a function $f:\mathbb {R}^{m+n} \to \mathbb {R}^n$ such that $(x,g(x))$ exactly describes the solution $f(x,y)=0$ I'm pretty sure that 1. is true, that's just the implicit function theorem. But I don't really know about 2..","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question What's true and what's false? Let be a -function with . Assume fulfills and , then is in a neighbourhood of continuous differentiable. For every function exists a function such that exactly describes the solution I'm pretty sure that 1. is true, that's just the implicit function theorem. But I don't really know about 2..","f:\mathbb {R}^2 \to \mathbb {R} C^1 D_2f(0,0)=1 g:\mathbb {R}\to \mathbb {R} g(x)=0 f(x,g(x))=0 g 0 g:\mathbb {R}^m \to \mathbb {R}^n f:\mathbb {R}^{m+n} \to \mathbb {R}^n (x,g(x)) f(x,y)=0","['real-analysis', 'analysis']"
37,The condition and proof about the integral test for convergence,The condition and proof about the integral test for convergence,,"The proof about the integral test: Suppose $f (x) $ is nonnegative monotone decreasing over $[1,\infty)$ , then the positive series $\sum_{n=1}^{\infty}f(n)$ is convergent if and only if $\lim_{A\rightarrow +\infty}f(x)dx$ exists. Proof: Since $f(x)$ is monotone decreasing, we can get $f(n+1)<\int_{n}^{n+1}f(x)\Bbb{dx}< f(n)$ , sum them up and get $\sum_{k=1}^{n+1}f(x)-f(1)<\int_{1}^{n+1}f(x)\Bbb{dx}<\sum_{k=1}^{n}f(k)$ , when the series is convergent, the integral is bounded, since $f(x)$ is nonnegative, the integral is monotone increasing, the $\lim_{A\rightarrow +\infty}f(x)dx$ exists. When $\lim_{A\rightarrow +\infty}f(x)dx$ exists, the positive series is bounded, so it is convergent. When reading the integral test for positive series, I confused about the condition ""Monotone decreasing"": this condition is used to get $f(n+1)<\int_{n}^{n+1}f(x)\Bbb{dx}< f(n)$ and following that $\sum_{k=1}^{n+1}f(x)-f(1)<\int_{1}^{n+1}f(x)\Bbb{dx}<\sum_{k=1}^{n}f(k)$ , but I am confused because when the condition is monotone increasing we can just change the direction of the sign of the inequality like $f(n+1)>\int_{n}^{n+1}f(x)\Bbb{dx}> f(n)$ , so why the condition ""monotone decreasing"" is necessary? ""Continuous"": the condition says that $f (x) $ is continuous. But when proves it seems that the condition that be used is the integrable. Is the condition continuous not necessary? I think there is something wrong with my understanding. Could you find and point it? Thank you!","The proof about the integral test: Suppose is nonnegative monotone decreasing over , then the positive series is convergent if and only if exists. Proof: Since is monotone decreasing, we can get , sum them up and get , when the series is convergent, the integral is bounded, since is nonnegative, the integral is monotone increasing, the exists. When exists, the positive series is bounded, so it is convergent. When reading the integral test for positive series, I confused about the condition ""Monotone decreasing"": this condition is used to get and following that , but I am confused because when the condition is monotone increasing we can just change the direction of the sign of the inequality like , so why the condition ""monotone decreasing"" is necessary? ""Continuous"": the condition says that is continuous. But when proves it seems that the condition that be used is the integrable. Is the condition continuous not necessary? I think there is something wrong with my understanding. Could you find and point it? Thank you!","f (x)  [1,\infty) \sum_{n=1}^{\infty}f(n) \lim_{A\rightarrow +\infty}f(x)dx f(x) f(n+1)<\int_{n}^{n+1}f(x)\Bbb{dx}< f(n) \sum_{k=1}^{n+1}f(x)-f(1)<\int_{1}^{n+1}f(x)\Bbb{dx}<\sum_{k=1}^{n}f(k) f(x) \lim_{A\rightarrow +\infty}f(x)dx \lim_{A\rightarrow +\infty}f(x)dx f(n+1)<\int_{n}^{n+1}f(x)\Bbb{dx}< f(n) \sum_{k=1}^{n+1}f(x)-f(1)<\int_{1}^{n+1}f(x)\Bbb{dx}<\sum_{k=1}^{n}f(k) f(n+1)>\int_{n}^{n+1}f(x)\Bbb{dx}> f(n) f (x) ","['integration', 'analysis', 'convergence-divergence']"
38,The rearrangement of integrals,The rearrangement of integrals,,"This question comes from Fleming's book ""Functions of Several Variables"", which is a little bit like the rearrangement of convergent series: Let $f$ be continuous on an open set $D$ .Assume that the integrals of $f^+$ and $f^-$ over $D$ both diverge to $+\infty $ .Show that given any number $l$ here is a sequence of compact sets $K_1\subset K_2\subset \cdots $ such that $D=K_1\cup K_2\cup \cdots $ and $lim_{n\to \infty}{\int_{K_n} fdV}=l$ . The following is my thought: Since $\int_D f^+ dV=+\infty$ , there exists compact set $K_{11}\subset D$ such that $$\int_{K_{11}} f^+ dV>l+1$$ Similarly,there exists compact set $K_{12}\subset D$ such that $$ \int_{K_{12}}{f^-dV}>\int_{K_{11}}{f^+}dV-\left( l+1 \right)  $$ We can remove the part on which $f^+>0$ from $K_{12}$ (This part can be open by the continuity of $f^+$ ),and then $K_{12}$ is still compact.The similar procedure should be acted on $K_{11}$ .And then let $K_1=K_{11}\cup K_{12}$ ,we have $$ \int_{K_1}{f}dV=\int_{K_1}{f^+}dV-\int_{K_1}{f^-}dV<l+1 $$ By using the approach similar to the above process,we can get $K_1\subset K_2\subset \cdots$ with $\int_{K_n} fdV<l+\frac{1}{n}$ while $n$ is odd; $>l-\frac{1}{n}$ while $n$ is even. However, it still cannot show that $lim_{n\to \infty}\cdots =l$ and $\bigcup K_i =D$ .","This question comes from Fleming's book ""Functions of Several Variables"", which is a little bit like the rearrangement of convergent series: Let be continuous on an open set .Assume that the integrals of and over both diverge to .Show that given any number here is a sequence of compact sets such that and . The following is my thought: Since , there exists compact set such that Similarly,there exists compact set such that We can remove the part on which from (This part can be open by the continuity of ),and then is still compact.The similar procedure should be acted on .And then let ,we have By using the approach similar to the above process,we can get with while is odd; while is even. However, it still cannot show that and .","f D f^+ f^- D +\infty  l K_1\subset K_2\subset \cdots  D=K_1\cup K_2\cup \cdots  lim_{n\to \infty}{\int_{K_n} fdV}=l \int_D f^+ dV=+\infty K_{11}\subset D \int_{K_{11}} f^+ dV>l+1 K_{12}\subset D 
\int_{K_{12}}{f^-dV}>\int_{K_{11}}{f^+}dV-\left( l+1 \right) 
 f^+>0 K_{12} f^+ K_{12} K_{11} K_1=K_{11}\cup K_{12} 
\int_{K_1}{f}dV=\int_{K_1}{f^+}dV-\int_{K_1}{f^-}dV<l+1
 K_1\subset K_2\subset \cdots \int_{K_n} fdV<l+\frac{1}{n} n >l-\frac{1}{n} n lim_{n\to \infty}\cdots =l \bigcup K_i =D","['real-analysis', 'calculus', 'integration', 'analysis']"
39,Proof that the union of connected sets where the intersection of the closure of one with the other is non-empty.,Proof that the union of connected sets where the intersection of the closure of one with the other is non-empty.,,"The problem says: Prove that if $(X,d)$ is a metric space and $A, B$ are connected subsets of $ X$ , then if $cl(A)\cap B\neq\emptyset$ , $A\cup B$ is connected. To show this, I supposed the contrary, that $A\cup B$ is disconnected and thus, $A\cup B=C\cup D$ , with $cl(C)\cap D=\emptyset$ and $cl(D)\cap C=\emptyset$ . Then I can define the function: $$f:C\cup D\to \{0,1\}$$ $$f(x)=\begin{cases}0 ,& x\in C, \\ 1,& x\in D.\end{cases}$$ Which is continuous and not constant. If $x\in C$ , then $x\in A$ or $x\in B$ . WLOG, suppose it's in $A$ , then because $f$ is continuous, the image of connected sets is connected and thus, $f(a)=f(x), \forall a\in A$ . Because, $cl(A)\cap B\neq\emptyset$ , if $x\in cl(A)\cap B$ there is a sequence $\{x_n\}$ of points in $A$ such that: $$lim_{n\to\infty}x_n=x$$ And, because $f$ is continuous, $$lim_{n\to\infty}f(x_n)=f(x)$$ And because each $x_n\in A$ , we can conclude that $f(x)=1$ . But because $x\in B$ , we can alsio conclude that $f(b)=1\forall b\in B$ . But then, $f$ is constant, which is a contradiction. Is this correct, or am I missing something? I feel that when I use the function, I made a leap of logic by regarding $C\cup D$ as a metric space.","The problem says: Prove that if is a metric space and are connected subsets of , then if , is connected. To show this, I supposed the contrary, that is disconnected and thus, , with and . Then I can define the function: Which is continuous and not constant. If , then or . WLOG, suppose it's in , then because is continuous, the image of connected sets is connected and thus, . Because, , if there is a sequence of points in such that: And, because is continuous, And because each , we can conclude that . But because , we can alsio conclude that . But then, is constant, which is a contradiction. Is this correct, or am I missing something? I feel that when I use the function, I made a leap of logic by regarding as a metric space.","(X,d) A, B  X cl(A)\cap B\neq\emptyset A\cup B A\cup B A\cup B=C\cup D cl(C)\cap D=\emptyset cl(D)\cap C=\emptyset f:C\cup D\to \{0,1\} f(x)=\begin{cases}0 ,& x\in C, \\ 1,& x\in D.\end{cases} x\in C x\in A x\in B A f f(a)=f(x), \forall a\in A cl(A)\cap B\neq\emptyset x\in cl(A)\cap B \{x_n\} A lim_{n\to\infty}x_n=x f lim_{n\to\infty}f(x_n)=f(x) x_n\in A f(x)=1 x\in B f(b)=1\forall b\in B f C\cup D","['real-analysis', 'general-topology', 'analysis', 'metric-spaces']"
40,"How to prove that the inverse of a continuous strictly monotone increasing function is continuous? (Terence Tao Analysis 1, Proposition 9.8.3)","How to prove that the inverse of a continuous strictly monotone increasing function is continuous? (Terence Tao Analysis 1, Proposition 9.8.3)",,"I'm having some problems proving the inverse is continuous. The hint in the book is to use the standard epsilon-delta definition of continuity. I believe the easiest route is a proof by contradiction, but with all of the quantifiers in the statement, I may be incorrectly negating the statement I am trying to prove. Also, I have at my disposal the intermediate value theorem, which most of my proof relies on. Below is the proposition: Let $a < b$ be real numbers, and let $ f:[a, b] \to \mathbb{R} $ be a function which is both continuous and strictly monotone increasing. Then $f$ is a bijection from $[a, b]$ to $[f(a), f(b)]$ , and the inverse $f^{-1}: [f(a), f(b)] \to [a, b]$ is also continuous and strictly monotone increasing. Below is my attempt at a proof: Let $x_1, x_2 \in [a, b]$ be real numbers such that $f(x_1) = f(x_2)$ . From the trichotomy of the real numbers, we have that exactly one of the following is true: $x_1 = x_2$ , $x_1 < x_2$ , or $x_1 > x_2$ . Suppose $x_1 \not = x_2$ . Then, by definition of strictly increasing monotone functions, we have that $f(x_1) \not = f(x_2)$ . Thus, $x_1 = x_2$ , and $f$ is injective. Now let $y \in [f(a), f(b)]$ be a real number. Then, by the intermediate value theorem, there exists a real number $c \in [a, b]$ such that $f(c) = y$ . Thus, $f$ is a surjection from $[a, b]$ to $[f(a), f(b)]$ . Since $f$ is both injective and surjective, we can conclude that $f$ is a bijection from $[a, b]$ to $[f(a), f(b)]$ . To show that $f^{-1}$ is strictly monotone increasing, let $y_1, y_2 \in [f(a), f(b)]$ be real numbers such that $y_1 < y_2$ . Then,by the intermediate value theorem, there exist $x_1, x_2 \in [a, b]$ such that $f(x_1) = y_1$ and $f(x_2) = y_2$ . Since $f$ is strictly monotone increasing, we have $x_1 < x_2$ . Using the definition of an inverse, we have \begin{align*}f^{-1}(y_1) &= f^{-1}(f(x_1)) \\&= x_1 \\&< x_2 \\&= f^{-1}(f(x_2)) \\&=f^{-1}(y_2) \text{,}\end{align*} showing that $f^{-1}$ is strictly monotone increasing. Finally, we will show that $f^{-1}$ is continuous. Let $y_0 \in [f(a), f(b)]$ be a real number, and let $\epsilon > 0 $ be a real number. As before, there exists a real number $x_0 \in [a, b]$ such that $f(x_0) = y_0$ . Likewise, for any real number $y \in [f(a), f(b)]$ , the intermediate value theorem tells us that there exists a real number $x \in [a, b]$ such that $f(x) = y$ . We want to show that there exists a $\delta > 0 $ such that $ | f^{-1}(y) -f^{-1}( y_0) | < \epsilon$ for all $y \in [f(a), f(b)]$ such that $|y - y_0| < \delta$ . This is equivalent to showing that there exists a $\delta > 0 $ such that $ | x - x_0 | < \epsilon$ for all $f(x) \in [f(a), f(b)]$ such that $|f(x) - f(x_0)| < \delta$ . Written in the order we are more accustomed to, this is equivalent to showing that there exists a $\delta > 0 $ such that $|f(x) - f(x_0)| < \delta$ for all $x \in [a, b]$ such that $|x - x_0| < \epsilon$ . Suppose, for the sake of contradiction, that $f^{-1}$ is not continuous. That is, suppose for all $\delta > 0$ , there exists an $\epsilon > 0$ such that $|f(x) - f(x_0)| \ge \delta$ for all $x \in [a, b]$ such that $|x - x_0| < \epsilon$ . I am not really sure where to go from here, and I'm not certain I correctly negated the statement that the inverse of $f$ is continuous. Any help is greatly appreciated. P.S. This is not for any homework, just self study. I've never taken a class in analysis, so please feel free to point out anything I am doing wrong (or that is less than rigorous).","I'm having some problems proving the inverse is continuous. The hint in the book is to use the standard epsilon-delta definition of continuity. I believe the easiest route is a proof by contradiction, but with all of the quantifiers in the statement, I may be incorrectly negating the statement I am trying to prove. Also, I have at my disposal the intermediate value theorem, which most of my proof relies on. Below is the proposition: Let be real numbers, and let be a function which is both continuous and strictly monotone increasing. Then is a bijection from to , and the inverse is also continuous and strictly monotone increasing. Below is my attempt at a proof: Let be real numbers such that . From the trichotomy of the real numbers, we have that exactly one of the following is true: , , or . Suppose . Then, by definition of strictly increasing monotone functions, we have that . Thus, , and is injective. Now let be a real number. Then, by the intermediate value theorem, there exists a real number such that . Thus, is a surjection from to . Since is both injective and surjective, we can conclude that is a bijection from to . To show that is strictly monotone increasing, let be real numbers such that . Then,by the intermediate value theorem, there exist such that and . Since is strictly monotone increasing, we have . Using the definition of an inverse, we have showing that is strictly monotone increasing. Finally, we will show that is continuous. Let be a real number, and let be a real number. As before, there exists a real number such that . Likewise, for any real number , the intermediate value theorem tells us that there exists a real number such that . We want to show that there exists a such that for all such that . This is equivalent to showing that there exists a such that for all such that . Written in the order we are more accustomed to, this is equivalent to showing that there exists a such that for all such that . Suppose, for the sake of contradiction, that is not continuous. That is, suppose for all , there exists an such that for all such that . I am not really sure where to go from here, and I'm not certain I correctly negated the statement that the inverse of is continuous. Any help is greatly appreciated. P.S. This is not for any homework, just self study. I've never taken a class in analysis, so please feel free to point out anything I am doing wrong (or that is less than rigorous).","a < b  f:[a, b] \to \mathbb{R}  f [a, b] [f(a), f(b)] f^{-1}: [f(a), f(b)] \to [a, b] x_1, x_2 \in [a, b] f(x_1) = f(x_2) x_1 = x_2 x_1 < x_2 x_1 > x_2 x_1 \not = x_2 f(x_1) \not = f(x_2) x_1 = x_2 f y \in [f(a), f(b)] c \in [a, b] f(c) = y f [a, b] [f(a), f(b)] f f [a, b] [f(a), f(b)] f^{-1} y_1, y_2 \in [f(a), f(b)] y_1 < y_2 x_1, x_2 \in [a, b] f(x_1) = y_1 f(x_2) = y_2 f x_1 < x_2 \begin{align*}f^{-1}(y_1) &= f^{-1}(f(x_1)) \\&= x_1 \\&< x_2 \\&= f^{-1}(f(x_2)) \\&=f^{-1}(y_2) \text{,}\end{align*} f^{-1} f^{-1} y_0 \in [f(a), f(b)] \epsilon > 0  x_0 \in [a, b] f(x_0) = y_0 y \in [f(a), f(b)] x \in [a, b] f(x) = y \delta > 0   | f^{-1}(y) -f^{-1}( y_0) | < \epsilon y \in [f(a), f(b)] |y - y_0| < \delta \delta > 0   | x - x_0 | < \epsilon f(x) \in [f(a), f(b)] |f(x) - f(x_0)| < \delta \delta > 0  |f(x) - f(x_0)| < \delta x \in [a, b] |x - x_0| < \epsilon f^{-1} \delta > 0 \epsilon > 0 |f(x) - f(x_0)| \ge \delta x \in [a, b] |x - x_0| < \epsilon f","['real-analysis', 'analysis', 'continuity', 'inverse', 'epsilon-delta']"
41,Continuity of the supremum of a continuous function over a closed interval,Continuity of the supremum of a continuous function over a closed interval,,"Proposition: let $F:R\times [0,1]\rightarrow R$ , be a countinuous function. If $g(x):= Sup\{F(x,t):t\in [0,1]\}$ , then $g(x)$ is continuous at all points. My Idea:  Assume $g$ is not continuous at $x_0$ . Then there exists $\epsilon >0$ such that there exists a sequence $\{y_n\}$ which is converges to $x_0$ and $|g(y_n)-g(x_0)|>\epsilon$ and $y_n\ne x_0$ for all $n$ . $g(y_n)=F(y_n,t_n)$ for some $t_n\in [0,1]$ as $[0,1]$ is compact and $F$ is continuous. Thus we have the sequence $\{(y_n,t_n)\}$ . Now since the sequence lies in a closed interval by the BolzanoWeierstrass theorem there is a sub-sequence $\{(y_m,t_m)\}$ which converges. It is easy to note $\{(y_m,t_m)\}$ converges to $(x_0,t_0)$ where $t_0$ is some real number between 0 and 1 . Hence $F(y_m,t_m)>g(x_0)+\epsilon$ or $F(y_m,t_m)<g(x_0)-\epsilon$ as $g(y_m)>g(x_0)+\epsilon$ or $g(y_m)<g(x_0)-\epsilon$ , which implies $F(x_0,t_0)\ge g(x_0)+\epsilon$ or $F(x_0,t_0)\le g(x_0)-\epsilon$ If it were only for the first inequality we would be done, alas it is not the case. I would appreciate if someone helped me prove/disprove the proposition.","Proposition: let , be a countinuous function. If , then is continuous at all points. My Idea:  Assume is not continuous at . Then there exists such that there exists a sequence which is converges to and and for all . for some as is compact and is continuous. Thus we have the sequence . Now since the sequence lies in a closed interval by the BolzanoWeierstrass theorem there is a sub-sequence which converges. It is easy to note converges to where is some real number between 0 and 1 . Hence or as or , which implies or If it were only for the first inequality we would be done, alas it is not the case. I would appreciate if someone helped me prove/disprove the proposition.","F:R\times [0,1]\rightarrow R g(x):= Sup\{F(x,t):t\in [0,1]\} g(x) g x_0 \epsilon >0 \{y_n\} x_0 |g(y_n)-g(x_0)|>\epsilon y_n\ne x_0 n g(y_n)=F(y_n,t_n) t_n\in [0,1] [0,1] F \{(y_n,t_n)\} \{(y_m,t_m)\} \{(y_m,t_m)\} (x_0,t_0) t_0 F(y_m,t_m)>g(x_0)+\epsilon F(y_m,t_m)<g(x_0)-\epsilon g(y_m)>g(x_0)+\epsilon g(y_m)<g(x_0)-\epsilon F(x_0,t_0)\ge g(x_0)+\epsilon F(x_0,t_0)\le g(x_0)-\epsilon","['real-analysis', 'analysis', 'multivariable-calculus', 'continuity']"
42,Proving the continuity of Fourier Transform between Schwartz spaces via sequences,Proving the continuity of Fourier Transform between Schwartz spaces via sequences,,"Prove that if $f_k \rightarrow f$ in the Schwartz space $\mathcal{S}(\mathbb{R}^n)$ , then $\hat{f_k} \rightarrow \hat{f}$ in $\mathcal{S}(\mathbb{R}^n)$ . This is the Exercise 2.2.2  in Loukas Grafakos's book named Classical Fourier Analysis (3 edition) and it's used to prove the Corollary 2.2.15 which says the Fourier transform is a homeomorphism between Schwartz spaces. The convergence in $\mathcal{S}(\mathbb{R}^n)$ is defined by: $f_k \rightarrow f$ in $\mathcal{S}(\mathbb{R}^n)$ if $\rho_{\alpha,\beta}(f_k-f) \rightarrow 0$ as $k \rightarrow \infty$ , $\forall\alpha, \beta$ multi-index, where $$\rho_{\alpha,\beta}(f) = \sup_{x \in \mathbb{R}^n} |x^\alpha \partial^\beta(f)(x)|.$$ My idea is to prove the convergence $\hat{f_k} \rightarrow \hat{f}$ by definition using the folowing identity: $$\xi^\alpha \partial^\beta \hat{g}(\xi) = \frac{(-2\pi i)^{|\beta|}}{(2\pi i)^{|\alpha|}} (\partial^\alpha(x^\beta g(x)))^\wedge(\xi), \;\; \forall g \in\mathcal{S}(\mathbb{R}^n),$$ but I'm can't get the convergence I want.","Prove that if in the Schwartz space , then in . This is the Exercise 2.2.2  in Loukas Grafakos's book named Classical Fourier Analysis (3 edition) and it's used to prove the Corollary 2.2.15 which says the Fourier transform is a homeomorphism between Schwartz spaces. The convergence in is defined by: in if as , multi-index, where My idea is to prove the convergence by definition using the folowing identity: but I'm can't get the convergence I want.","f_k \rightarrow f \mathcal{S}(\mathbb{R}^n) \hat{f_k} \rightarrow \hat{f} \mathcal{S}(\mathbb{R}^n) \mathcal{S}(\mathbb{R}^n) f_k \rightarrow f \mathcal{S}(\mathbb{R}^n) \rho_{\alpha,\beta}(f_k-f) \rightarrow 0 k \rightarrow \infty \forall\alpha, \beta \rho_{\alpha,\beta}(f) = \sup_{x \in \mathbb{R}^n} |x^\alpha \partial^\beta(f)(x)|. \hat{f_k} \rightarrow \hat{f} \xi^\alpha \partial^\beta \hat{g}(\xi) = \frac{(-2\pi i)^{|\beta|}}{(2\pi i)^{|\alpha|}} (\partial^\alpha(x^\beta g(x)))^\wedge(\xi), \;\; \forall g \in\mathcal{S}(\mathbb{R}^n),","['analysis', 'partial-differential-equations', 'fourier-analysis', 'fourier-transform', 'schwartz-space']"
43,Proving slight extension of the Monotone Convergence Theorem,Proving slight extension of the Monotone Convergence Theorem,,"I am trying to do the following exercise in the Measure Theory book by Cohn Prove that the Monotone Convergence Theorem still holds if the assumption that the functions $f_1, f_2, ...$ are non-negative is dropped, and the assumption that $f_1$ is integrable is added (note that in this case the integrals of the functions $f$ and $f_2, f_3, ...$ exist, but may equal $\infty$ .) I think I was able to solve it, but it took two cases, and case $2$ seems a bit contrived. I was wondering if my solution is correct, and if there is a better way? Thank you. $\\$ $\\$ Case 1: $f$ is integrable Since $f_1 \le f_2 \le \cdots$ , the sequence $(f_n-f_1)_{n=0}^{\infty}$ is increasing, non-negative, and converges to $f-f_1$ . Now we may apply the MCT to get $$\lim_{n \to \infty} \int (f_n-f_1) = \int(f-f_1)$$ Since each $f_n$ satisfies $f_1 \le f_n \le f$ and both $f_1$ and $f$ are integrable, each $f_n$ is integrable, so we may split the integral on the left. Since $f$ and $f_1$ is integrable, we may split the integral on the right. Cancelling, we get $$\lim_{n \to \infty} \int f_n = \int f$$ Case 2: $f$ is not integrable Since $f_1$ is integrable, $\int(f_1)_- < \infty$ , so $\int (f_n)_- < \infty$ and $\int (f)_- < \infty$ . Therefore, since $f$ is not integrable, we must have that $\int f = \int f_+ = \infty$ . Now apply the MCT to the sequence $(f_n)_+$ which converges to $f+$ . We get $$\lim_{n \to \infty} \int (f_n)_+ = \int (f_n)_+ = \infty$$ Now $$\lim _{n \to \infty} \int f_n = \lim _{n \to \infty} \left[ \int (f_n)_+ - \int (f_n)_- \right ]= \infty - \lim_{n \to \infty} \int (f_n)_- = \infty.$$","I am trying to do the following exercise in the Measure Theory book by Cohn Prove that the Monotone Convergence Theorem still holds if the assumption that the functions are non-negative is dropped, and the assumption that is integrable is added (note that in this case the integrals of the functions and exist, but may equal .) I think I was able to solve it, but it took two cases, and case seems a bit contrived. I was wondering if my solution is correct, and if there is a better way? Thank you. Case 1: is integrable Since , the sequence is increasing, non-negative, and converges to . Now we may apply the MCT to get Since each satisfies and both and are integrable, each is integrable, so we may split the integral on the left. Since and is integrable, we may split the integral on the right. Cancelling, we get Case 2: is not integrable Since is integrable, , so and . Therefore, since is not integrable, we must have that . Now apply the MCT to the sequence which converges to . We get Now","f_1, f_2, ... f_1 f f_2, f_3, ... \infty 2 \\ \\ f f_1 \le f_2 \le \cdots (f_n-f_1)_{n=0}^{\infty} f-f_1 \lim_{n \to \infty} \int (f_n-f_1) = \int(f-f_1) f_n f_1 \le f_n \le f f_1 f f_n f f_1 \lim_{n \to \infty} \int f_n = \int f f f_1 \int(f_1)_- < \infty \int (f_n)_- < \infty \int (f)_- < \infty f \int f = \int f_+ = \infty (f_n)_+ f+ \lim_{n \to \infty} \int (f_n)_+ = \int (f_n)_+ = \infty \lim _{n \to \infty} \int f_n = \lim _{n \to \infty} \left[ \int (f_n)_+ - \int (f_n)_- \right ]= \infty - \lim_{n \to \infty} \int (f_n)_- = \infty.","['real-analysis', 'analysis', 'measure-theory']"
44,Concativity of entropy without Jensen's inequality,Concativity of entropy without Jensen's inequality,,"In my information theory class I need to prove that entropy is concave (which is usually done with Jensen's inequality). But I want to use only the definition of entropy. And as the result of derivations I get a wrong answer. Here what I do: I need to prove that: $$\lambda {\rm H} \left(p\right)+\left(1-\lambda \right){\rm H} \left(q\right)\le {\rm H} \left(\lambda p+\left(1-\lambda \right)q\right)$$ I use the definitions of entropy (with the summation carried out over $\textit{p}$ or $\textit{q}$ , or ( $\textit{p}$ and $\textit{q}$ )): $$\lambda {\rm H} \left(p\right)={\rm {\mathbb E}}_{p} \log \frac{1}{p^{\lambda } } ={\rm {\mathbb E}}_{p,q} \log \frac{1}{p^{\lambda }}$$ $$\left(1-\lambda \right){\rm H} \left(q\right)={\rm {\mathbb E}}_{q} \log \frac{1}{q^{\left(1-\lambda \right)} } ={\rm {\mathbb E}}_{p,q} \log \frac{1}{q^{\left(1-\lambda \right)} } $$ $${\rm H} \left(\lambda p+\left(1-\lambda \right)q\right)={\rm {\mathbb E}}_{p,q} \log \frac{1}{\lambda p+\left(1-\lambda \right)q} $$ Then collecting everything and moving to the left-hand side: $${\rm {\mathbb E}}_{p,q} \log \frac{1}{p^{\lambda } } +{\rm {\mathbb E}}_{p,q} \log \frac{1}{q^{\left(1-\lambda \right)} } ={\rm {\mathbb E}}_{p,q} \log \frac{1}{p^{\lambda } q^{\left(1-\lambda \right)} } $$ $${\rm {\mathbb E}}_{p,q} \log \frac{1}{p^{\lambda } q^{\left(1-\lambda \right)} } -{\rm {\mathbb E}}_{p,q} \log \frac{1}{\lambda p+\left(1-\lambda \right)q} \le 0$$ Using log properties: $${\rm {\mathbb E}}_{p,q} \log \frac{\lambda p+\left(1-\lambda \right)q}{p^{\lambda } q^{\left(1-\lambda \right)} } \le 0$$ So I have to prove that the ration under log is less than unity (for log to be negative): $$\frac{\lambda p+\left(1-\lambda \right)q}{p^{\lambda } q^{\left(1-\lambda \right)} } \le 1$$ Finally I get the inequality which is definitely wrong (since AM-GM says me so): $$\lambda p+\left(1-\lambda \right)q\le p^{\lambda } q^{\left(1-\lambda \right)} $$ So, where I am wrong?","In my information theory class I need to prove that entropy is concave (which is usually done with Jensen's inequality). But I want to use only the definition of entropy. And as the result of derivations I get a wrong answer. Here what I do: I need to prove that: I use the definitions of entropy (with the summation carried out over or , or ( and )): Then collecting everything and moving to the left-hand side: Using log properties: So I have to prove that the ration under log is less than unity (for log to be negative): Finally I get the inequality which is definitely wrong (since AM-GM says me so): So, where I am wrong?","\lambda {\rm H} \left(p\right)+\left(1-\lambda \right){\rm H} \left(q\right)\le {\rm H} \left(\lambda p+\left(1-\lambda \right)q\right) \textit{p} \textit{q} \textit{p} \textit{q} \lambda {\rm H} \left(p\right)={\rm {\mathbb E}}_{p} \log \frac{1}{p^{\lambda } } ={\rm {\mathbb E}}_{p,q} \log \frac{1}{p^{\lambda }} \left(1-\lambda \right){\rm H} \left(q\right)={\rm {\mathbb E}}_{q} \log \frac{1}{q^{\left(1-\lambda \right)} } ={\rm {\mathbb E}}_{p,q} \log \frac{1}{q^{\left(1-\lambda \right)} }  {\rm H} \left(\lambda p+\left(1-\lambda \right)q\right)={\rm {\mathbb E}}_{p,q} \log \frac{1}{\lambda p+\left(1-\lambda \right)q}  {\rm {\mathbb E}}_{p,q} \log \frac{1}{p^{\lambda } } +{\rm {\mathbb E}}_{p,q} \log \frac{1}{q^{\left(1-\lambda \right)} } ={\rm {\mathbb E}}_{p,q} \log \frac{1}{p^{\lambda } q^{\left(1-\lambda \right)} }  {\rm {\mathbb E}}_{p,q} \log \frac{1}{p^{\lambda } q^{\left(1-\lambda \right)} } -{\rm {\mathbb E}}_{p,q} \log \frac{1}{\lambda p+\left(1-\lambda \right)q} \le 0 {\rm {\mathbb E}}_{p,q} \log \frac{\lambda p+\left(1-\lambda \right)q}{p^{\lambda } q^{\left(1-\lambda \right)} } \le 0 \frac{\lambda p+\left(1-\lambda \right)q}{p^{\lambda } q^{\left(1-\lambda \right)} } \le 1 \lambda p+\left(1-\lambda \right)q\le p^{\lambda } q^{\left(1-\lambda \right)} ","['analysis', 'inequality', 'convex-analysis', 'entropy', 'jensen-inequality']"
45,Inequalities on $L_p$ norm of Bounded Functions,Inequalities on  norm of Bounded Functions,L_p,"Let $\mathcal{F}$ be a set of uniformly bounded measurable functions on interval $[0,1]$ with respect to the Lebesgue measure. Let $\tilde{\mathcal{F}} = \{e^f/\int e^f d\mu: f\in\mathcal{F}\}$ be the transformed density class. Let $\|\cdot\|_p$ be the $L_p$ norm for $1\leq p\leq\infty$ . For any $f\in\mathcal{F}$ , denote $\tilde{f} = e^f/\int e^f$ . Show that for any $f_1$ and $f_2$ in $\mathcal{F}$ , $$\| \tilde{f}_1 - \tilde{f}_2 \|_p\leq c_1 \|f_1 -f_2 \|_p $$ and $$\|\log(\tilde{f}_1) - \log(\tilde{f}_2) \|_p\leq c_2\|\tilde{f}_1 - \tilde{f}_2\|_p$$ for some constants $c_1$ and $c_2$ that only depends on $\sup_{f\in\mathcal{F}}\|f\|_\infty$ . In the paper the author says its ""easy"" to see these two inequalities but I've been struggling to show this explicitly for a while now. Any suggestions would be greatly appreciated. Edit: I think the uniform boundedness and the fact that $\exp(\cdot)$ and $\ln(\cdot)$ are Lipschitz should give the results. Note that $\exp(x)$ on a bounded interval is always Lipschitz; for $\ln(x)$ to be Lipschitz, need $x$ to be on $[a,\infty)$ for $a>0$ .","Let be a set of uniformly bounded measurable functions on interval with respect to the Lebesgue measure. Let be the transformed density class. Let be the norm for . For any , denote . Show that for any and in , and for some constants and that only depends on . In the paper the author says its ""easy"" to see these two inequalities but I've been struggling to show this explicitly for a while now. Any suggestions would be greatly appreciated. Edit: I think the uniform boundedness and the fact that and are Lipschitz should give the results. Note that on a bounded interval is always Lipschitz; for to be Lipschitz, need to be on for .","\mathcal{F} [0,1] \tilde{\mathcal{F}} = \{e^f/\int e^f d\mu: f\in\mathcal{F}\} \|\cdot\|_p L_p 1\leq p\leq\infty f\in\mathcal{F} \tilde{f} = e^f/\int e^f f_1 f_2 \mathcal{F} \| \tilde{f}_1 - \tilde{f}_2 \|_p\leq c_1 \|f_1 -f_2 \|_p  \|\log(\tilde{f}_1) - \log(\tilde{f}_2) \|_p\leq c_2\|\tilde{f}_1 - \tilde{f}_2\|_p c_1 c_2 \sup_{f\in\mathcal{F}}\|f\|_\infty \exp(\cdot) \ln(\cdot) \exp(x) \ln(x) x [a,\infty) a>0","['real-analysis', 'functional-analysis', 'analysis', 'normed-spaces']"
46,Invertibility of the characteristic map,Invertibility of the characteristic map,,"We are in the context of Hamilton Jacobi equations, in particular I was studying the characteristic method. We want to solve the problem of the special form (Hamiltonian only depending on the "" $p$ "" variables) $$ \begin{cases} u_t(x,t)+H\big(D_xu(x,t)\big)= 0, \\ u(x,0)=g, \end{cases}\quad(x,t) \in \mathbb{R}^n \times \mathbb{R}^+ . $$ In this case it's easy to find that the characteristic line starting from $y$ at time $s$ is $$ X(y,s) = y+sDH\big(Dg(y)\big). $$ Now set $$\overline{T} = \sup \Big\{ t : \, \det\big[I+tD^2H\big(Dg(y)\big)D^2g(y)\big] >0, \, \forall y \in \mathbb{R}^n \Big\}$$ The problem is that the textbook says that if $D^2H$ and $D^2g$ (Hessian matrices) are bounded, then for all $s < \overline{T}$ the function $y \mapsto X(y,s)$ is invertible (with $C^1$ inverse, but that's clear from the local inverse function theorem since the Jacobian is inverbile by hypothesis). What I can't do is proving surjectivity and injectivity of this map. Can someone help me? Thanks! P.S. The textbook i'm referring to is P.L. Lions, Generalized solutions of Hamilton Jacobi equations , page 14.","We are in the context of Hamilton Jacobi equations, in particular I was studying the characteristic method. We want to solve the problem of the special form (Hamiltonian only depending on the "" "" variables) In this case it's easy to find that the characteristic line starting from at time is Now set The problem is that the textbook says that if and (Hessian matrices) are bounded, then for all the function is invertible (with inverse, but that's clear from the local inverse function theorem since the Jacobian is inverbile by hypothesis). What I can't do is proving surjectivity and injectivity of this map. Can someone help me? Thanks! P.S. The textbook i'm referring to is P.L. Lions, Generalized solutions of Hamilton Jacobi equations , page 14.","p 
\begin{cases}
u_t(x,t)+H\big(D_xu(x,t)\big)= 0, \\
u(x,0)=g,
\end{cases}\quad(x,t) \in \mathbb{R}^n \times \mathbb{R}^+ .
 y s 
X(y,s) = y+sDH\big(Dg(y)\big).
 \overline{T} = \sup \Big\{ t : \, \det\big[I+tD^2H\big(Dg(y)\big)D^2g(y)\big] >0, \, \forall y \in \mathbb{R}^n \Big\} D^2H D^2g s < \overline{T} y \mapsto X(y,s) C^1","['real-analysis', 'analysis', 'partial-differential-equations']"
47,simple Application of Gram-Schmidt Orthogonalization,simple Application of Gram-Schmidt Orthogonalization,,"I wanted to apply the Gram-Schmidt orthogonalization to the following simple example of polynomials $1,x,x^2,x^3$ in $L^2[-1,1]$ , is it correct? $e_1 = 1$ $e'_2 = x - \int_{-1}^{1}xdx = x \implies e_2 = \frac{x}{\|x\|}$ $e'_3 = x^2 - \int_{-1}^{1}x^2dx -\int_{-1}^{1}x^2xdx = x^2 - \frac{1}{2} \implies e_4 = \frac{x^2 - \frac{1}{2}}{\|x^2 - \frac{1}{2}\|}$ $e'_4 = x^3 - \int_{-1}^{1}x^3dx -\int_{-1}^{1}x^3xdx -\int_{-1}^{1}x^3x^2dx = x^3 - \frac{2}{5} \implies e_3 = \frac{x^3 - \frac{2}{5}}{\|x^3 - \frac{2}{5}\|}$","I wanted to apply the Gram-Schmidt orthogonalization to the following simple example of polynomials in , is it correct?","1,x,x^2,x^3 L^2[-1,1] e_1 = 1 e'_2 = x - \int_{-1}^{1}xdx = x \implies e_2 = \frac{x}{\|x\|} e'_3 = x^2 - \int_{-1}^{1}x^2dx -\int_{-1}^{1}x^2xdx = x^2 - \frac{1}{2} \implies e_4 = \frac{x^2 - \frac{1}{2}}{\|x^2 - \frac{1}{2}\|} e'_4 = x^3 - \int_{-1}^{1}x^3dx -\int_{-1}^{1}x^3xdx -\int_{-1}^{1}x^3x^2dx = x^3 - \frac{2}{5} \implies e_3 = \frac{x^3 - \frac{2}{5}}{\|x^3 - \frac{2}{5}\|}","['linear-algebra', 'functional-analysis', 'analysis', 'solution-verification', 'orthonormal']"
48,Are Minkowski functionals (a.k.a. gauges) *strictly* convex?,Are Minkowski functionals (a.k.a. gauges) *strictly* convex?,,"Take a strictly convex subset $C$ of say, $\mathbb{R}^d$ .  For the sake of simplicity, assume that $C$ is compact and $0$ belongs to its interior. Define the Minkowski functional \begin{align*} f \colon \mathbb{R}^d & \to [0,+\infty)\\ x &\mapsto \min\{\tau\ge 0 : x \in \tau C\} \end{align*} It is known that $f$ is convex. Is its square $f^2$ strictly convex? At least in two dimensions, it feels like it should be.  It looks like a paraboloid whose sections are scaled $\partial C$ 's (rather than ellipses).  However, I have no idea how to prove it I and have no geometric intuition whatsoever for $d\ge 3$ .","Take a strictly convex subset of say, .  For the sake of simplicity, assume that is compact and belongs to its interior. Define the Minkowski functional It is known that is convex. Is its square strictly convex? At least in two dimensions, it feels like it should be.  It looks like a paraboloid whose sections are scaled 's (rather than ellipses).  However, I have no idea how to prove it I and have no geometric intuition whatsoever for .","C \mathbb{R}^d C 0 \begin{align*}
f \colon \mathbb{R}^d & \to [0,+\infty)\\
x &\mapsto \min\{\tau\ge 0 : x \in \tau C\}
\end{align*} f f^2 \partial C d\ge 3","['real-analysis', 'functional-analysis', 'analysis', 'convex-analysis', 'normed-spaces']"
49,An integral estimate regarding tails of solutions in Di CastroKuusiPalatucci,An integral estimate regarding tails of solutions in Di CastroKuusiPalatucci,,"Let $x_0 \in \mathbb{R}^n$ and suppose $u \colon B_r(x_0)\to(0,\infty)$ be an $L^p$ integrable function and let $k=\sup_{x \in B_r(x_0)} u(x)$ . Also let $\phi\in C_c^{\infty}(B_r(x_0))$ be such that $0 \le \phi\le 1$ in $B_r(x_0)$ , $\phi \equiv 1$ on $B_{\frac{r}{2}}(x_0)$ and $|\nabla\phi| \le \frac{c}{r}$ in $B_r(x_0)$ . Then for $y\in \mathbb{R}^n\setminus B_r(x_0)$ and $x\in B_r(x_0)$ , we have \begin{multline} \int_{\mathbb{R}^n\setminus B_r(x_0)}\int_{B_r(x_0)}|u(x)-u(y)|^{p-2}(u(x)-u(y))(u(x)-2k)\phi^p(x)\,dx dy \\ \ge \int_{\mathbb{R}^n\setminus B_r(x_0)}\int_{B_r(x_0)}k(u(y)-k)_{+}^{p-1}\phi^p(x)\,dx dy \\ -\int_{\mathbb{R}^n\setminus B_r(x_0)}\int_{B_r(x_0)}2k\chi_{\{u(y)<k\}}(y) \cdot (u(x)-u(y))_{+}^{p-1}\phi^p(x)\, dx dy. \end{multline} This has been written in this article on page 17. Can someone kindly help with how to get it?","Let and suppose be an integrable function and let . Also let be such that in , on and in . Then for and , we have This has been written in this article on page 17. Can someone kindly help with how to get it?","x_0 \in \mathbb{R}^n u \colon B_r(x_0)\to(0,\infty) L^p k=\sup_{x \in B_r(x_0)} u(x) \phi\in C_c^{\infty}(B_r(x_0)) 0 \le \phi\le 1 B_r(x_0) \phi \equiv 1 B_{\frac{r}{2}}(x_0) |\nabla\phi| \le \frac{c}{r} B_r(x_0) y\in \mathbb{R}^n\setminus B_r(x_0) x\in B_r(x_0) \begin{multline}
\int_{\mathbb{R}^n\setminus B_r(x_0)}\int_{B_r(x_0)}|u(x)-u(y)|^{p-2}(u(x)-u(y))(u(x)-2k)\phi^p(x)\,dx dy \\
\ge \int_{\mathbb{R}^n\setminus B_r(x_0)}\int_{B_r(x_0)}k(u(y)-k)_{+}^{p-1}\phi^p(x)\,dx dy \\
-\int_{\mathbb{R}^n\setminus B_r(x_0)}\int_{B_r(x_0)}2k\chi_{\{u(y)<k\}}(y) \cdot (u(x)-u(y))_{+}^{p-1}\phi^p(x)\, dx dy.
\end{multline}","['integration', 'analysis', 'partial-differential-equations', 'regularity-theory-of-pdes', 'parabolic-pde']"
50,Suppose $U$ is bounded and $p>q$. Does $f_n\rightharpoonup f$ in $L^p(U)$ implies $f_n\to f$ in $L^q(U)$?,Suppose  is bounded and . Does  in  implies  in ?,U p>q f_n\rightharpoonup f L^p(U) f_n\to f L^q(U),Suppose $U$ is bounded and $p>q$ . We know $L^p(U)$ is stronger then $L^q(U)$ in the sense that $f_n\to f$ in $L^p(U)$ implies $f_n\to f$ in $L^q(U)$ . Can we relax the convergence on the stronger space and still retain the result? So is it the case that $f_n\rightharpoonup f$ in $L^p(U)$ implies $f_n\to f$ in $L^q(U)$ ?,Suppose is bounded and . We know is stronger then in the sense that in implies in . Can we relax the convergence on the stronger space and still retain the result? So is it the case that in implies in ?,U p>q L^p(U) L^q(U) f_n\to f L^p(U) f_n\to f L^q(U) f_n\rightharpoonup f L^p(U) f_n\to f L^q(U),"['functional-analysis', 'analysis', 'lp-spaces', 'weak-convergence']"
51,Term by term integration of series on infinite intervals,Term by term integration of series on infinite intervals,,Suppose $f(x)=\sum a_n x^n$ is given by its Taylor series such that $\int_0^{\infty} f(x)dx$ exists. Under what conditions can we integrate term by term by replacing $f(x)$ by its Taylor series?,Suppose is given by its Taylor series such that exists. Under what conditions can we integrate term by term by replacing by its Taylor series?,f(x)=\sum a_n x^n \int_0^{\infty} f(x)dx f(x),"['real-analysis', 'calculus', 'functional-analysis', 'analysis', 'lebesgue-integral']"
52,Conceptual Clarification: Rudin's Definition of a Differential Form of Order k,Conceptual Clarification: Rudin's Definition of a Differential Form of Order k,,"Broad Context As the title suggests, I'm reading Rudin's POMA (3e) and have gotten to chapter 10. Miscellaneous online resources have not been as useful as always, since Rudin (notably, I think) defines differential forms without making any mention whatsoever of manifolds, tensor products, or the exterior algebra (which are all things I've found via. google, and which I do not yet know about). Presumably, Rudin's definition should be understandable without any of these concepts, or else he would have had to introduce them. Narrow Context I expect half of the problem will be notation. Here are the definitions I'm working with. $\mathbf{\mathscr{C}'}$ -Mapping: Let $X$ and $Y$ be normed vector spaces over $\mathbb{C}$ . Let $B(X,Y)$ denote the set of all bounded linear mappings $X \to Y$ . Let $A$ be an open subset of $X$ (with distances measured in terms of the norm of $X$ ) such that every $a \in A$ is a limit point of $A$ . Let $f : A \to Y$ be a map. $f$ is said to be $\mathscr{C}'$ in $A$ if $f$ is Frechet differntiable on $A$ and (letting $\mathcal{D} f(x)$ denote the derivative) the map $A \to B(X,Y) : x \mapsto \mathcal{D} f(x)$ is continuous on $A$ (with distances in $B(X,Y)$ measured in terms of the operator norm). Full disclose, I've expanded the above definition to a slightly more general context than Rudin presents (definition 9.20 POMA). $\mathbf{k}$ -Surface in $\mathbf{A}$ : Let $n$ and $k$ be positive integers. Let $A$ be an open subset of $\mathbb{R}^n$ . Rudin defines a $k$ -surface in $A$ to be a $\mathscr{C}'$ mapping $K \to A$ , where $K$ is a compact subset of $\mathbb{R}^k$ . In particular, Rudin says we are to confine our attention to the situation in which $K$ in the above definition is either a $k$ -cell (by which it is meant a Cartesian product of $k$ non-degenerate closed intervals of real numbers), or is the standard $k$ -simplex. Note, the above is Def 10.10 and the below is Def 10.11 in POMA. Differential Form of Order $\mathbf{k \geq 1}$ : Let $A \subset \mathbb{R}^n$ be open. Let $\Omega(A)$ denote the set of all $k$ -surfaces in $A$ (this is my own notation). Let $\phi_1,\ldots,\phi_n$ denote the component functions of a given $\Phi\in\Omega(A)$ . A differential form of order $k$ is a function $\omega : \Omega(A)\to\mathbb{R}$ determined by the rule $$   \Phi \mapsto \int_{\text{dom}(\Phi)}           \sum a_{i_1}\cdots\hspace{0.5mm}_{i_k}\big(\Phi(\mathbf{u})\big)           \det \big(              \mathcal{D} (\phi_{i_1}(\mathbf{u}),\ldots,\phi_{i_k}(\mathbf{u}))          \big) \mathrm{d} \mathbf{u} $$ where the integral is a classical (what I would call iteratively defined) multiple integral over the compact subset dom $(\Phi)$ of $\mathbb{R}^k$ (that is, the domain of $\Phi$ ). Also, the indices $i_1,\ldots,i_k$ ""range independently from $1$ to $n$ ,"" and ""the functions $a_{i_1}\cdots\hspace{0.5mm}_{i_k}$ are assumed to be real and continuous"" on $A$ .   Moreover, Rudin says that the above rule is ""symbolically represented by the sum"" $$   \omega = \sum a_{i_1}\cdots\hspace{0.5mm}_{i_k}(\mathbf{x})\hspace{1mm}            \mathrm{d}_{x_1} \wedge \cdots \wedge \mathrm{d}_{x_k}. $$ Rudin does not provide any sort of definition of "" $x \wedge y$ ."" The Question Did I happen to get anything wrong in the above definitions? Is the summation taken over $i\in\{1,\ldots,n\}$ , over $(i_1,\ldots,i_k)\in\{1,\ldots,n\}^k$ , or over some other indexing set? In other words, how many of these weirdly indexed functions $a_{i_1}\cdots\hspace{0.5mm}_{i_k} : A \to \mathbb{R}$ are necessary in order to completely determine $\omega$ ? What the heck is this wedge notation supposed to (as Rudin says) ""symbolically represet?"" It would almost seem to me that $\omega$ is entirely determined by the set $A$ and the functions $a_{i_1}\cdots\hspace{0.5mm}_{i_k}$ , is it not? Do the $\mathrm{d}_{x_1} \wedge \cdots \wedge \mathrm{d}_{x_k}$ specify a subset of $\{1,\ldots,n\}^k$ over which the summation is to be taken, or something like that? Again, I'm not familiar with manifolds, nor with tensor products, and Rudin has not developed these subjects, at all. Normally, I prefer rigor to intuition, but as a first step, I'll take what I can get. Suggested supplementary readings are also welcome although, realistically, for the foreseeable future I won't have time to read, e.g., a whole book on a subject which is not part of my school work. Thanks for your time.","Broad Context As the title suggests, I'm reading Rudin's POMA (3e) and have gotten to chapter 10. Miscellaneous online resources have not been as useful as always, since Rudin (notably, I think) defines differential forms without making any mention whatsoever of manifolds, tensor products, or the exterior algebra (which are all things I've found via. google, and which I do not yet know about). Presumably, Rudin's definition should be understandable without any of these concepts, or else he would have had to introduce them. Narrow Context I expect half of the problem will be notation. Here are the definitions I'm working with. -Mapping: Let and be normed vector spaces over . Let denote the set of all bounded linear mappings . Let be an open subset of (with distances measured in terms of the norm of ) such that every is a limit point of . Let be a map. is said to be in if is Frechet differntiable on and (letting denote the derivative) the map is continuous on (with distances in measured in terms of the operator norm). Full disclose, I've expanded the above definition to a slightly more general context than Rudin presents (definition 9.20 POMA). -Surface in : Let and be positive integers. Let be an open subset of . Rudin defines a -surface in to be a mapping , where is a compact subset of . In particular, Rudin says we are to confine our attention to the situation in which in the above definition is either a -cell (by which it is meant a Cartesian product of non-degenerate closed intervals of real numbers), or is the standard -simplex. Note, the above is Def 10.10 and the below is Def 10.11 in POMA. Differential Form of Order : Let be open. Let denote the set of all -surfaces in (this is my own notation). Let denote the component functions of a given . A differential form of order is a function determined by the rule where the integral is a classical (what I would call iteratively defined) multiple integral over the compact subset dom of (that is, the domain of ). Also, the indices ""range independently from to ,"" and ""the functions are assumed to be real and continuous"" on .   Moreover, Rudin says that the above rule is ""symbolically represented by the sum"" Rudin does not provide any sort of definition of "" ."" The Question Did I happen to get anything wrong in the above definitions? Is the summation taken over , over , or over some other indexing set? In other words, how many of these weirdly indexed functions are necessary in order to completely determine ? What the heck is this wedge notation supposed to (as Rudin says) ""symbolically represet?"" It would almost seem to me that is entirely determined by the set and the functions , is it not? Do the specify a subset of over which the summation is to be taken, or something like that? Again, I'm not familiar with manifolds, nor with tensor products, and Rudin has not developed these subjects, at all. Normally, I prefer rigor to intuition, but as a first step, I'll take what I can get. Suggested supplementary readings are also welcome although, realistically, for the foreseeable future I won't have time to read, e.g., a whole book on a subject which is not part of my school work. Thanks for your time.","\mathbf{\mathscr{C}'} X Y \mathbb{C} B(X,Y) X \to Y A X X a \in A A f : A \to Y f \mathscr{C}' A f A \mathcal{D} f(x) A \to B(X,Y) : x \mapsto \mathcal{D} f(x) A B(X,Y) \mathbf{k} \mathbf{A} n k A \mathbb{R}^n k A \mathscr{C}' K \to A K \mathbb{R}^k K k k k \mathbf{k \geq 1} A \subset \mathbb{R}^n \Omega(A) k A \phi_1,\ldots,\phi_n \Phi\in\Omega(A) k \omega : \Omega(A)\to\mathbb{R} 
  \Phi \mapsto \int_{\text{dom}(\Phi)} 
         \sum a_{i_1}\cdots\hspace{0.5mm}_{i_k}\big(\Phi(\mathbf{u})\big) 
         \det \big(
             \mathcal{D} (\phi_{i_1}(\mathbf{u}),\ldots,\phi_{i_k}(\mathbf{u}))
         \big) \mathrm{d} \mathbf{u}
 (\Phi) \mathbb{R}^k \Phi i_1,\ldots,i_k 1 n a_{i_1}\cdots\hspace{0.5mm}_{i_k} A 
  \omega = \sum a_{i_1}\cdots\hspace{0.5mm}_{i_k}(\mathbf{x})\hspace{1mm}
           \mathrm{d}_{x_1} \wedge \cdots \wedge \mathrm{d}_{x_k}.
 x \wedge y i\in\{1,\ldots,n\} (i_1,\ldots,i_k)\in\{1,\ldots,n\}^k a_{i_1}\cdots\hspace{0.5mm}_{i_k} : A \to \mathbb{R} \omega \omega A a_{i_1}\cdots\hspace{0.5mm}_{i_k} \mathrm{d}_{x_1} \wedge \cdots \wedge \mathrm{d}_{x_k} \{1,\ldots,n\}^k","['analysis', 'differential-forms']"
53,Bounding a Riemann Stieltjes integral.,Bounding a Riemann Stieltjes integral.,,I have little experience with Riemann Stieltjes integrals. Any good reading material on it would be much appreciated (specifically a large summary of the material). Suppose $|k|_t$ is the total variation of $k:\mathbb{R}\to \mathbb{R}$ . Where $k$ is non-negative and monotonically increasing.  Consider the Riemann-Stieltjes integral (of a continuous $f:\mathbb{R}\to\mathbb{R}$ ) $$ \int_0^t f(s) d|k|_s $$ Is there any hope in bounding this by something like : $$ \int_0^t f(s) d|k|_s\leq C |k|_t\int_0^tf(s)ds ~~~~?$$ Thanks in advance! I would like to stress that $|k|_t$ is continuous and differentiable a.e. $\textbf{Edit :}$ Motivation : (you can kind of ignore the probability stuff just imagine everything as deterministic if you wish) The reason for studying this is I have an integral $$ \frac{\partial}{\partial t}\mathbb{E}\int_0^t |X_s| d|k|_s. $$ Here $X_s$ is a continuous random variable solution to the Skorohod reflection problem on a convex domain $D$ . $k_t$ is the random ( $X_t$ dependent) process which keeps $X_s$ in the domain. $|k|_t$ its total variation (also random but a.s finite). Now I have bounds on $\mathbb{E}(X_t)$ and can bound $|k|_t$ by $|X_t|$ ! So I was hoping to write : \begin{align*} \frac{\partial}{\partial t}\mathbb{E}\int_0^t |X_s| d|k|_s\leq \frac{\partial}{\partial t} C \mathbb{E} |k|_t \int_0^t |X_s| ds \end{align*} Then use Cauchy Schwartz and my bounds previously mentioned.,I have little experience with Riemann Stieltjes integrals. Any good reading material on it would be much appreciated (specifically a large summary of the material). Suppose is the total variation of . Where is non-negative and monotonically increasing.  Consider the Riemann-Stieltjes integral (of a continuous ) Is there any hope in bounding this by something like : Thanks in advance! I would like to stress that is continuous and differentiable a.e. Motivation : (you can kind of ignore the probability stuff just imagine everything as deterministic if you wish) The reason for studying this is I have an integral Here is a continuous random variable solution to the Skorohod reflection problem on a convex domain . is the random ( dependent) process which keeps in the domain. its total variation (also random but a.s finite). Now I have bounds on and can bound by ! So I was hoping to write : Then use Cauchy Schwartz and my bounds previously mentioned.,"|k|_t k:\mathbb{R}\to \mathbb{R} k f:\mathbb{R}\to\mathbb{R}  \int_0^t f(s) d|k|_s   \int_0^t f(s) d|k|_s\leq C |k|_t\int_0^tf(s)ds ~~~~? |k|_t \textbf{Edit :}  \frac{\partial}{\partial t}\mathbb{E}\int_0^t |X_s| d|k|_s.  X_s D k_t X_t X_s |k|_t \mathbb{E}(X_t) |k|_t |X_t| \begin{align*}
\frac{\partial}{\partial t}\mathbb{E}\int_0^t |X_s| d|k|_s\leq \frac{\partial}{\partial t} C \mathbb{E} |k|_t \int_0^t |X_s| ds
\end{align*}","['real-analysis', 'calculus', 'analysis', 'riemann-integration']"
54,Theorem 7.18 Rudin's,Theorem 7.18 Rudin's,,"I'm studying the example of a continuous function nowhere differentiable on Rudin's book ""Principles of Mathematical analysis"", but I keep missing the point of an assumption.  Here you can see the proof. The point I'm struggling with is the (?) weird assumption of not allowing any integer lying on the interval (formula n.38). Intuitively, I see that avoiding an integer on my interval let us evaluate $|\phi(4^n(x+\delta_m))-\phi(4^nx)|$ ( $\star$ ) on a straight line, and the result will be $\frac{1}{2}$ . In the second case, if we have a integer $q$ , the graph of the function $\phi$ change the slope in a neighborhood of $q$ (we have something like a $\land$ or $\lor$ , with the segments of different measure), but I don't understand whhy in this case the difference ( $\star$ ) change, and why this should be avoid. I tried to figure it out what's going on here, but I really didn't find anything. Any hints or solution would be much appreciate, thanks in advance.","I'm studying the example of a continuous function nowhere differentiable on Rudin's book ""Principles of Mathematical analysis"", but I keep missing the point of an assumption.  Here you can see the proof. The point I'm struggling with is the (?) weird assumption of not allowing any integer lying on the interval (formula n.38). Intuitively, I see that avoiding an integer on my interval let us evaluate ( ) on a straight line, and the result will be . In the second case, if we have a integer , the graph of the function change the slope in a neighborhood of (we have something like a or , with the segments of different measure), but I don't understand whhy in this case the difference ( ) change, and why this should be avoid. I tried to figure it out what's going on here, but I really didn't find anything. Any hints or solution would be much appreciate, thanks in advance.",|\phi(4^n(x+\delta_m))-\phi(4^nx)| \star \frac{1}{2} q \phi q \land \lor \star,"['real-analysis', 'analysis']"
55,An Incomplete Metric Space,An Incomplete Metric Space,,"I am reading Kolmogorov's ""Introductory Real Analysis"" and I have come across this example of an incomplete metric space ( $C^2_{[a,b]}$ ) on page 59: If $$\phi_n(t)= \left\{ \begin{array}{lcc}              -1 & -1 \leq t \leq -\frac{1}{n} \\              nt & -\frac{1}{n}\leq t \leq \frac{1}{n} \\              1 & \frac{1}{n}\leq t \leq 1               \end{array}    \right.$$ then { $\phi_n(t)$ } is a fundamental sequence in $C^2_{[-1,1]}$ , since $\\$ $\int^1_{-1}[\phi_n(t)-\phi_{n^{'}}(t)]^2dt \leq \frac{2}{{min\{n,n^{'}\}}}$ , where $C^2_{[a,b]}$ is the metric space on the set of all functions continuous on the interval [a,b], equipped with the distance function $\rho(x,y)=(\int^a_{b}[x(t)-y(t)]^2dt)^{1/2}$ . It is worth noting that 'fundamental sequence' and 'Cauchy sequence' are used interchangeably here. The problem I have is that I don't understand there this upper bound comes from and why it is chosen here. From desmos: https://www.desmos.com/calculator/mrd9tjyrup , the max distance between 2 elements only ever reaches $\frac{2}{3}$ , but the given bound $\frac{2}{{min\{n,n^{'}\}}}=2$ for ${min\{n,n^{'}\}}=1$ . Also, it is my understanding that $\int^1_{-1}[\phi_n(t)-\phi_{n^{'}}(t)]^2dt = \frac{2}{{min\{n,n^{'}\}}}$ is a consequence of the limit taken as $n^{'} \rightarrow \infty.$ Any help would be much appreciated.","I am reading Kolmogorov's ""Introductory Real Analysis"" and I have come across this example of an incomplete metric space ( ) on page 59: If then { } is a fundamental sequence in , since , where is the metric space on the set of all functions continuous on the interval [a,b], equipped with the distance function . It is worth noting that 'fundamental sequence' and 'Cauchy sequence' are used interchangeably here. The problem I have is that I don't understand there this upper bound comes from and why it is chosen here. From desmos: https://www.desmos.com/calculator/mrd9tjyrup , the max distance between 2 elements only ever reaches , but the given bound for . Also, it is my understanding that is a consequence of the limit taken as Any help would be much appreciated.","C^2_{[a,b]} \phi_n(t)= \left\{ \begin{array}{lcc}
             -1 & -1 \leq t \leq -\frac{1}{n} \\
             nt & -\frac{1}{n}\leq t \leq \frac{1}{n} \\
             1 & \frac{1}{n}\leq t \leq 1 
             \end{array}
   \right. \phi_n(t) C^2_{[-1,1]} \\ \int^1_{-1}[\phi_n(t)-\phi_{n^{'}}(t)]^2dt \leq \frac{2}{{min\{n,n^{'}\}}} C^2_{[a,b]} \rho(x,y)=(\int^a_{b}[x(t)-y(t)]^2dt)^{1/2} \frac{2}{3} \frac{2}{{min\{n,n^{'}\}}}=2 {min\{n,n^{'}\}}=1 \int^1_{-1}[\phi_n(t)-\phi_{n^{'}}(t)]^2dt = \frac{2}{{min\{n,n^{'}\}}} n^{'} \rightarrow \infty.","['real-analysis', 'analysis', 'metric-spaces']"
56,Decomposing a Measurable Set into its interior and boundary.,Decomposing a Measurable Set into its interior and boundary.,,"I recently learn that every open set and closed set is Lebesgue Measurable. However, from what I understand, every set $S$ can be decomposed into the union of its interior, $\text{int}(S)$ , which is open, and its boundary, $\partial{S}$ , which is closed. So ${S}=\text{int}(S)\cup \partial{S}$ is a finite union of measurable sets. Then shouldn't any set S should be measurable too? I know this is incorrect by the existence of non-measurable sets, so what goes wrong? Edit: by correction in comments: $\overline{S}=\text{int}(S)\cup \partial{S}$","I recently learn that every open set and closed set is Lebesgue Measurable. However, from what I understand, every set can be decomposed into the union of its interior, , which is open, and its boundary, , which is closed. So is a finite union of measurable sets. Then shouldn't any set S should be measurable too? I know this is incorrect by the existence of non-measurable sets, so what goes wrong? Edit: by correction in comments:",S \text{int}(S) \partial{S} {S}=\text{int}(S)\cup \partial{S} \overline{S}=\text{int}(S)\cup \partial{S},"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure']"
57,On the $w^{\ast}$ Convergence of a Sequence of Measures,On the  Convergence of a Sequence of Measures,w^{\ast},"Assume that $(\nu_{j})$ is a sequence of positive Radon measures supported in a compact set $K\subseteq\mathbb{R}^{n}$ and \begin{align*} \sup_{x\in{\mathbb{R}}^{n},r>0}\dfrac{1}{r^{n-\lambda}}\int_{B(x,r)}|M_{\alpha}\nu_{j}(y)|^{p}dy\rightarrow 0, \end{align*} where $0<\lambda,\alpha<n$ , and \begin{align*} M_{\alpha}\nu_{j}(y)=\sup_{s>0}\dfrac{1}{s^{n-\alpha}}\nu_{j}(B(y,s)). \end{align*} The author claims that there is a subsequence of $(\nu_{j})$ that $w^{\ast}$ converges to a measure $\nu$ which supported in $K$ . I fail to see why it must be the case. Apparently the author resorts to the Banach-Alaoglu Theorem of the sequential $w^{\ast}$ compact property regarding to the space of all positive Radon measures which supported in $K$ . Therefore we are to bound $(\nu_{j}(K))$ . But how does one bound the quantity $\nu_{j}(K)$ in terms of the integral with respect to the fractional maximal function?","Assume that is a sequence of positive Radon measures supported in a compact set and where , and The author claims that there is a subsequence of that converges to a measure which supported in . I fail to see why it must be the case. Apparently the author resorts to the Banach-Alaoglu Theorem of the sequential compact property regarding to the space of all positive Radon measures which supported in . Therefore we are to bound . But how does one bound the quantity in terms of the integral with respect to the fractional maximal function?","(\nu_{j}) K\subseteq\mathbb{R}^{n} \begin{align*}
\sup_{x\in{\mathbb{R}}^{n},r>0}\dfrac{1}{r^{n-\lambda}}\int_{B(x,r)}|M_{\alpha}\nu_{j}(y)|^{p}dy\rightarrow 0,
\end{align*} 0<\lambda,\alpha<n \begin{align*}
M_{\alpha}\nu_{j}(y)=\sup_{s>0}\dfrac{1}{s^{n-\alpha}}\nu_{j}(B(y,s)).
\end{align*} (\nu_{j}) w^{\ast} \nu K w^{\ast} K (\nu_{j}(K)) \nu_{j}(K)","['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
58,"If $f(x) = x(1 - x^n)^{-1/n}$, how many function compositions does it take for $f(f(f(\cdots f(x_0)))) > 1$?","If , how many function compositions does it take for ?",f(x) = x(1 - x^n)^{-1/n} f(f(f(\cdots f(x_0)))) > 1,"Fix an integer $n \geq 1$ and let $f(x) := x(1 - x^n)^{-1/n}$ . Fix $x_0 \in (0, 1)$ . Then there exists an $N$ (depending on $n$ and $x_0$ ) such that $f(f(f(\cdots f(x_0)))) > 1$ where the function composition occurred $N$ times. Is there a way to estimate the order of magnitude of $N$ depending on $n$ and $x_0$ , like what does $N$ asymptotically look like?","Fix an integer and let . Fix . Then there exists an (depending on and ) such that where the function composition occurred times. Is there a way to estimate the order of magnitude of depending on and , like what does asymptotically look like?","n \geq 1 f(x) := x(1 - x^n)^{-1/n} x_0 \in (0, 1) N n x_0 f(f(f(\cdots f(x_0)))) > 1 N N n x_0 N","['real-analysis', 'analysis', 'asymptotics']"
59,About polynomial harmonic functions,About polynomial harmonic functions,,"I am really struggling to solve the following, I don't even know how to start. I would appreciate if anyone could give me some help. Let $m$ be a positive integer and $u : \mathbb{R}^{n} \rightarrow  \mathbb{R}$ be a harmonic function. If $u(x) = O(\left|x \right|^m)$ when $\left|x \right| \to \infty$ , show that $u$ is polynomial of   degree at most $m$ . Thanks in advance for any help.","I am really struggling to solve the following, I don't even know how to start. I would appreciate if anyone could give me some help. Let be a positive integer and be a harmonic function. If when , show that is polynomial of   degree at most . Thanks in advance for any help.","m u : \mathbb{R}^{n} \rightarrow
 \mathbb{R} u(x) = O(\left|x \right|^m) \left|x \right| \to \infty u m","['analysis', 'polynomials', 'partial-differential-equations', 'harmonic-analysis', 'harmonic-functions']"
60,Symbol of a pseudo-differential operator. Hormander property and principal symbol,Symbol of a pseudo-differential operator. Hormander property and principal symbol,,"Suppose I have a pseudo-differential operator on $\mathbb R$ whose symbol is $a(x,\xi) = e^{-\xi^2/2}$ (notice, no dependence on $x$ ) Question 1 :  Are the following statements correct? $a$ is an Hrmander symbol in the class $\mathcal S^{m}_{1,0},\; \forall\, m\in \mathbb R$ . Since $\mathcal S^{m}_{1,0} \subset \mathcal S^{n}_{1,0}$ for $m\leq n$ holds, then we might say $a\in \mathcal S^{-\infty}_{1,0}$ where we define $\mathcal S^{-\infty}_{1,0}=\cap_{m\in \mathbb R}\mathcal S^{m}_{1,0}$ . Question 2 : Does $a$ have a principal symbol? If it does, what is it? N.B. For the notation one might refer to the Wikipedia page on pseudo-differential operators.","Suppose I have a pseudo-differential operator on whose symbol is (notice, no dependence on ) Question 1 :  Are the following statements correct? is an Hrmander symbol in the class . Since for holds, then we might say where we define . Question 2 : Does have a principal symbol? If it does, what is it? N.B. For the notation one might refer to the Wikipedia page on pseudo-differential operators.","\mathbb R a(x,\xi) = e^{-\xi^2/2} x a \mathcal S^{m}_{1,0},\; \forall\, m\in \mathbb R \mathcal S^{m}_{1,0} \subset \mathcal S^{n}_{1,0} m\leq n a\in \mathcal S^{-\infty}_{1,0} \mathcal S^{-\infty}_{1,0}=\cap_{m\in \mathbb R}\mathcal S^{m}_{1,0} a","['functional-analysis', 'analysis', 'pseudo-differential-operators', 'microlocal-analysis']"
61,Uniquely extended fractional iterations of $\exp$,Uniquely extended fractional iterations of,\exp,"Let us define the following basic conditions for an iterated exponential function: $$\exp^1(x)=e^x\tag{$\forall x$}$$ $$\exp^{a+b}(x)=\exp^a(\exp^b(x))\tag{$\forall a,b,x$}$$ I then pondered what sort of additional conditions could be applied. Using the useful inequality $e^x-1\ge x$ , I considered adding the additional constraint: $$\exp^a(x)-a\ge\exp^b(x)-b\tag{$a\ge b$}$$ which can be seen as a reasonable result of inductively applying the inequality. From this, I noticed that: $$0=\exp^0(0)-0\le\exp^a(0)-a\le\exp^1(0)-1=0\tag{$\forall a\in[0,1]$}$$ $$\exp^a(0)=a\tag{$\forall a\in[0,1]$}$$ From this, one can define $\exp^a(0)$ for any $a$ by repeatedly using $$\exp^{a+1}(0)=e^{\exp^a(0)}$$ One can also easily see that this implies $\exp^a(0)$ attains every real value exactly once, meaning it has a well-defined inverse. Now define the super-logarithm: $$x=\operatorname{slog}(\exp^x(0))=\exp^{\operatorname{slog}(x)}(0)$$ and note that we can then write: $$\exp^a(x)=\exp^{a+\operatorname{slog}(x)}(0)$$ which uniquely defines $\exp^a(x)$ . That is to say, we have: $$\exp^a(x)=\begin{cases}a,&x=0\land a\in[0,1]\\\ln(\exp^{a+1}(0)),&x=0\land a<0\\e^{\exp^{a-1}(0)},&x=0\land a>1\\\exp^{a+\operatorname{slog}(x)}(0),&x\ne0\end{cases}$$ One can then check that this satisfies the imposed inequality restriction as well as the functional equation. For the functional equation: $$\exp^a(\exp^b(x))=\exp^{a+\operatorname{slog}(\exp^{b+\operatorname{slog}(x)}(0))}(0)=\exp^{a+b+\operatorname{slog}(x)}(0)=\exp^{a+b}(x)\tag{$x\ne0\land\exp^b(x)\ne0$}$$ The other cases are even simpler to prove. For the inequality: $$\exp^a(0)-a=0\ge0=\exp^b(0)-b\tag{$\forall a,b\in[0,1]$}$$ For $a,b\notin[0,1]$ , the result follows inductively. We can then see that $$\exp^a(x)-a=\exp^{a+\operatorname{slog}(x)}(0)-(a+\operatorname{slog}(x))+\operatorname{slog}(x)$$ and so it follows for all $x$ . What interests me are conditions that do not seem unreasonable or meaningless that lead to similar uniqueness. And so here are my questions: Is there a nice way to extend this to other bases? It seems the inequality for $e$ gets kind of messy if you try to extend it to other bases. And of course I'm not looking for something as trivial as ""just linearly interpolate $\exp_b^a(0)$ for $a\in[0,1]$ with $\exp_b^1(x)=b^x$ ."" What other conditions can be imposed to produce a uniquely defined iterated exponential function (base $e$ or otherwise)? And hopefully I didn't make any mistakes in the above definitions and proofs. $\ddot\smile$","Let us define the following basic conditions for an iterated exponential function: I then pondered what sort of additional conditions could be applied. Using the useful inequality , I considered adding the additional constraint: which can be seen as a reasonable result of inductively applying the inequality. From this, I noticed that: From this, one can define for any by repeatedly using One can also easily see that this implies attains every real value exactly once, meaning it has a well-defined inverse. Now define the super-logarithm: and note that we can then write: which uniquely defines . That is to say, we have: One can then check that this satisfies the imposed inequality restriction as well as the functional equation. For the functional equation: The other cases are even simpler to prove. For the inequality: For , the result follows inductively. We can then see that and so it follows for all . What interests me are conditions that do not seem unreasonable or meaningless that lead to similar uniqueness. And so here are my questions: Is there a nice way to extend this to other bases? It seems the inequality for gets kind of messy if you try to extend it to other bases. And of course I'm not looking for something as trivial as ""just linearly interpolate for with ."" What other conditions can be imposed to produce a uniquely defined iterated exponential function (base or otherwise)? And hopefully I didn't make any mistakes in the above definitions and proofs.","\exp^1(x)=e^x\tag{\forall x} \exp^{a+b}(x)=\exp^a(\exp^b(x))\tag{\forall a,b,x} e^x-1\ge x \exp^a(x)-a\ge\exp^b(x)-b\tag{a\ge b} 0=\exp^0(0)-0\le\exp^a(0)-a\le\exp^1(0)-1=0\tag{\forall a\in[0,1]} \exp^a(0)=a\tag{\forall a\in[0,1]} \exp^a(0) a \exp^{a+1}(0)=e^{\exp^a(0)} \exp^a(0) x=\operatorname{slog}(\exp^x(0))=\exp^{\operatorname{slog}(x)}(0) \exp^a(x)=\exp^{a+\operatorname{slog}(x)}(0) \exp^a(x) \exp^a(x)=\begin{cases}a,&x=0\land a\in[0,1]\\\ln(\exp^{a+1}(0)),&x=0\land a<0\\e^{\exp^{a-1}(0)},&x=0\land a>1\\\exp^{a+\operatorname{slog}(x)}(0),&x\ne0\end{cases} \exp^a(\exp^b(x))=\exp^{a+\operatorname{slog}(\exp^{b+\operatorname{slog}(x)}(0))}(0)=\exp^{a+b+\operatorname{slog}(x)}(0)=\exp^{a+b}(x)\tag{x\ne0\land\exp^b(x)\ne0} \exp^a(0)-a=0\ge0=\exp^b(0)-b\tag{\forall a,b\in[0,1]} a,b\notin[0,1] \exp^a(x)-a=\exp^{a+\operatorname{slog}(x)}(0)-(a+\operatorname{slog}(x))+\operatorname{slog}(x) x e \exp_b^a(0) a\in[0,1] \exp_b^1(x)=b^x e \ddot\smile","['analysis', 'exponential-function', 'tetration', 'power-towers', 'fractional-iteration']"
62,"Let $x,y \in \mathbb{R}$..Show that $x^{n+1}-y^{n+1}=(x-y) \sum\limits_{k=0}^n x^k y^{n-k}$ for all $n \in \mathbb{N}_0$ [duplicate]",Let ..Show that  for all  [duplicate],"x,y \in \mathbb{R} x^{n+1}-y^{n+1}=(x-y) \sum\limits_{k=0}^n x^k y^{n-k} n \in \mathbb{N}_0","This question already has answers here : Different proofs of $\,a^n-b^n =(a-b)\sum_{i=0}^{n-1} a^i b^{n-1-i} $? (4 answers) Closed 4 years ago . Let $x,y \in \mathbb{R}$ . Show that $$ x^{n+1}-y^{n+1}=(x-y)\sum_{k=0}^n x^k y^{n-k} $$ for all $n \in\mathbb{N}_0$ I need to prove this via induction. My attempt: base case (k=0) = $$x^{0+1}-y^{0+1}=(x-y)(x^0 y^{n-0}) $$ $$x-y=(x-y)(y^n)$$ Here is where I get lost, does this disprove this? This statement is only true if $y^n = 1$ ? Doesn't this mean that this isn't true for all $x,y$ in $\mathbb R$ ?","This question already has answers here : Different proofs of $\,a^n-b^n =(a-b)\sum_{i=0}^{n-1} a^i b^{n-1-i} $? (4 answers) Closed 4 years ago . Let . Show that for all I need to prove this via induction. My attempt: base case (k=0) = Here is where I get lost, does this disprove this? This statement is only true if ? Doesn't this mean that this isn't true for all in ?","x,y \in \mathbb{R}  x^{n+1}-y^{n+1}=(x-y)\sum_{k=0}^n x^k y^{n-k}  n \in\mathbb{N}_0 x^{0+1}-y^{0+1}=(x-y)(x^0 y^{n-0})  x-y=(x-y)(y^n) y^n = 1 x,y \mathbb R","['algebra-precalculus', 'analysis', 'proof-verification', 'induction']"
63,"Prove that $\inf\{d(x,y):x\in\pmb S_1, y\in\pmb S_2\}$ is in the set.",Prove that  is in the set.,"\inf\{d(x,y):x\in\pmb S_1, y\in\pmb S_2\}","Let the distance of two subsets $\pmb S_1$ and $\pmb S_2$ of a given complete metric space $(X, d)$ be defined as $d(\pmb S_1,\pmb S_2)=\inf\{d(x,y):x\in\pmb S_1, y\in\pmb S_2\}$ . Suppose that $\pmb S_1=\{x\}$ and $\pmb S_2$ is closed. Prove $d(\pmb S_1,\pmb S_2)=d(x,y)$ for some $y\in\pmb S_2$ . In my opinion, if $d$ is not specified, how would I know whether the infimum is in the set or not because proving a set being closed involves $d$ . Does anyone have any idea?","Let the distance of two subsets and of a given complete metric space be defined as . Suppose that and is closed. Prove for some . In my opinion, if is not specified, how would I know whether the infimum is in the set or not because proving a set being closed involves . Does anyone have any idea?","\pmb S_1 \pmb S_2 (X, d) d(\pmb S_1,\pmb S_2)=\inf\{d(x,y):x\in\pmb S_1, y\in\pmb S_2\} \pmb S_1=\{x\} \pmb S_2 d(\pmb S_1,\pmb S_2)=d(x,y) y\in\pmb S_2 d d","['real-analysis', 'functional-analysis', 'analysis', 'metric-spaces']"
64,asymptotic expansion of $(1-a)^n$,asymptotic expansion of,(1-a)^n,"On the page 311 in the book of Flajolet appears that $(1-a)^n=e^{-an}\exp(O(na^2))$ . I am trying to understand this formula and, in particular, the term $\exp(O(na^2))$ . I've tried the following. From binomial expansion we have \begin{align*} (1-a)^n&=1+\binom{n}{1}(-a)+\binom{n}{2}(-a)^2+\binom{n}{3}(-a)^3+\ldots\\ &=1+\frac{(-a)n}{1!}+\frac{(-a)^2n^2}{2!}+\frac{(-a)^2(-n)}{2!}+\frac{(-a)^3n^3}{3!}+\frac{(-a^3)(-3n^2+2n)}{3!}+\ldots \end{align*} The last line can be written as \begin{align*} \left(1+\frac{(-a)n}{1!}+\frac{(-a)^2n^2}{2!}+\frac{(-a)^3n^3}{3!}+\ldots\right)\cdot\left(1-\frac{a^2n}{2!}+\frac{-a^3(2n)}{3!}+(-a)^4n^2\left(\frac{11}{4!}-\frac{1}{3}\right)+\ldots\right), \end{align*} so that inside the first parenthesis we get $e^{-na}$ , which means that the infinite sum inside the second parenthesis should produce $\exp(O(na^2))$ . Could you please explain this? How do we obtain this $O$ -expression?","On the page 311 in the book of Flajolet appears that . I am trying to understand this formula and, in particular, the term . I've tried the following. From binomial expansion we have The last line can be written as so that inside the first parenthesis we get , which means that the infinite sum inside the second parenthesis should produce . Could you please explain this? How do we obtain this -expression?","(1-a)^n=e^{-an}\exp(O(na^2)) \exp(O(na^2)) \begin{align*}
(1-a)^n&=1+\binom{n}{1}(-a)+\binom{n}{2}(-a)^2+\binom{n}{3}(-a)^3+\ldots\\
&=1+\frac{(-a)n}{1!}+\frac{(-a)^2n^2}{2!}+\frac{(-a)^2(-n)}{2!}+\frac{(-a)^3n^3}{3!}+\frac{(-a^3)(-3n^2+2n)}{3!}+\ldots
\end{align*} \begin{align*}
\left(1+\frac{(-a)n}{1!}+\frac{(-a)^2n^2}{2!}+\frac{(-a)^3n^3}{3!}+\ldots\right)\cdot\left(1-\frac{a^2n}{2!}+\frac{-a^3(2n)}{3!}+(-a)^4n^2\left(\frac{11}{4!}-\frac{1}{3}\right)+\ldots\right),
\end{align*} e^{-na} \exp(O(na^2)) O","['analysis', 'asymptotics']"
65,Inequality and $\ell^2$-norm,Inequality and -norm,\ell^2,"Let $a_1\geq a_2\geq \ldots \geq a_n$ and $b_1\geq b_2 \geq \ldots \geq b_n$ be two sequences with $\sum_{i=1}^n a_i=0$ and $\sum_{i=1}^n b_i=0.$ I want to prove if there exists $1\leq j\leq n$ such that $(a_j-b_j)^2>m$ , then $$\sum_{i=1}^n (a_i-b_i)^2>2m.$$ I assumed that $\sum_{i=1}^n (a_i-b_i)^2\leq 2m$ and tried to use their property to get the result but it didn't work. Any kind of suggestion is appreciated.  Is there any books about inequalities like these? Thanks to everyone for the help.","Let and be two sequences with and I want to prove if there exists such that , then I assumed that and tried to use their property to get the result but it didn't work. Any kind of suggestion is appreciated.  Is there any books about inequalities like these? Thanks to everyone for the help.",a_1\geq a_2\geq \ldots \geq a_n b_1\geq b_2 \geq \ldots \geq b_n \sum_{i=1}^n a_i=0 \sum_{i=1}^n b_i=0. 1\leq j\leq n (a_j-b_j)^2>m \sum_{i=1}^n (a_i-b_i)^2>2m. \sum_{i=1}^n (a_i-b_i)^2\leq 2m,"['real-analysis', 'calculus', 'analysis', 'inequality', 'cauchy-schwarz-inequality']"
66,Turning a continuous everywhere differentiable nowhere function into a smooth function by infinitely many times definite integration?,Turning a continuous everywhere differentiable nowhere function into a smooth function by infinitely many times definite integration?,,"Let $W(x)$ be a real-vlued function defined on a (possibly infinite) interval $\text{T}\subseteq\mathbb{R}$ containing $0$ that is continuous everywhere differentiable nowhere on $\text{T}$ . Define the sequence of function $f_n:\text{T}\to\mathbb{R}$ as follows: $$f_0(x)\triangleq{W(x)}$$ and $$f_n(x)\triangleq\int_{0}^{x}f_{n-1}(u)du$$ for $n=1,2,3,4,5,...$ . Then it follows the fact that $f_n$ is differentiable exactly $n$ -times everywhere on $\text{T}$ . Question: Does there exist such $W(x)$ and $\text{T}$ so that the sequence of function $f_n$ on $\text{T}$ ""converges"" to a limit function $f_{\infty}$ on $\text{T}$ with $f_{\infty}$ being infinitely many times differentiable (i.e., smooth) on $\text{T}$ ?","Let be a real-vlued function defined on a (possibly infinite) interval containing that is continuous everywhere differentiable nowhere on . Define the sequence of function as follows: and for . Then it follows the fact that is differentiable exactly -times everywhere on . Question: Does there exist such and so that the sequence of function on ""converges"" to a limit function on with being infinitely many times differentiable (i.e., smooth) on ?","W(x) \text{T}\subseteq\mathbb{R} 0 \text{T} f_n:\text{T}\to\mathbb{R} f_0(x)\triangleq{W(x)} f_n(x)\triangleq\int_{0}^{x}f_{n-1}(u)du n=1,2,3,4,5,... f_n n \text{T} W(x) \text{T} f_n \text{T} f_{\infty} \text{T} f_{\infty} \text{T}","['real-analysis', 'analysis']"
67,Supremum and Infimum Proof: $\inf(x) + \sup(y) \leq \sup(x+y)$,Supremum and Infimum Proof:,\inf(x) + \sup(y) \leq \sup(x+y),"Im stuck on how to prove this: $$\inf(x) + \sup(y) \leq \sup(x+y)$$ I know that $$\inf(x) \leq \sup(x)\\ x \leq y\implies \sup(x) \leq \inf(y)\\ \sup(x) + \sup(y) \geq \sup(x+y)\\ \inf(x) + \inf(y) \leq \inf(x+y)\\ x \leq y\implies \sup(x)\leq \sup(y)$$ How can I utilize these to prove this? I've come to the conclusion that $$\inf(x)+\inf(y) \leq \inf(x+y) \leq \sup(x+y) \leq \sup(x) + \sup(y)$$ I've also tried utilizing the $\sup(x) = -\inf(-x)$ proof, but that doesn't seem to work either.","Im stuck on how to prove this: I know that How can I utilize these to prove this? I've come to the conclusion that I've also tried utilizing the proof, but that doesn't seem to work either.","\inf(x) + \sup(y) \leq \sup(x+y) \inf(x) \leq \sup(x)\\
x \leq y\implies \sup(x) \leq \inf(y)\\
\sup(x) + \sup(y) \geq \sup(x+y)\\
\inf(x) + \inf(y) \leq \inf(x+y)\\
x \leq y\implies \sup(x)\leq \sup(y) \inf(x)+\inf(y) \leq \inf(x+y) \leq \sup(x+y) \leq \sup(x) + \sup(y) \sup(x) = -\inf(-x)","['analysis', 'inequality', 'supremum-and-infimum']"
68,Extended convex function - continuity,Extended convex function - continuity,,"I am dealing with a problem concerning a convex function defined on $\mathbb{R}^d$ and taking values on $\mathbb{R}\cup\{+\infty\}$ . I would like to use in my argument that such a convex function is $\lambda^d$ -almost everywhere continuous, but I do not know if this is a valid statement. Does anybody of you know a theorem or a reference which contains this claim?? Thank you in advance !","I am dealing with a problem concerning a convex function defined on and taking values on . I would like to use in my argument that such a convex function is -almost everywhere continuous, but I do not know if this is a valid statement. Does anybody of you know a theorem or a reference which contains this claim?? Thank you in advance !",\mathbb{R}^d \mathbb{R}\cup\{+\infty\} \lambda^d,"['analysis', 'convex-analysis']"
69,"Given three positive numbers $a,b,c\in R_{+}^{*}$ prove the following inequality",Given three positive numbers  prove the following inequality,"a,b,c\in R_{+}^{*}","If $\ abc=1$ then prove that $$\sum_{cyc}\frac{1}{3-a+a^{6}}1$$ where $a,b,c>0$ I think this inequality can be proved by holder ? My attempt using $\ am-gm$ $$3-a+a^{6}3-a \quad(etc)$$ $$\sum_{cyc}\frac{1}{3-a+a^{6}}\displaystyle\sum_{cyc}\frac{1}{3-a}$$ Now I will get tow case if $a>3$ and if $a<3$ . If $a>3,$ the  inequality is true. Now if $a<3$ Using : $3-a>0$ (etc) So: $$\sum_{cyc}\frac{1}{3-a}1$$ Is my work correct?",If then prove that where I think this inequality can be proved by holder ? My attempt using Now I will get tow case if and if . If the  inequality is true. Now if Using : (etc) So: Is my work correct?,"\ abc=1 \sum_{cyc}\frac{1}{3-a+a^{6}}1 a,b,c>0 \ am-gm 3-a+a^{6}3-a \quad(etc) \sum_{cyc}\frac{1}{3-a+a^{6}}\displaystyle\sum_{cyc}\frac{1}{3-a} a>3 a<3 a>3, a<3 3-a>0 \sum_{cyc}\frac{1}{3-a}1","['analysis', 'multivariable-calculus', 'inequality', 'tangent-line-method']"
70,Let $\{f_n\}$ be a sequence of continuous function on a metric space $E$ and $\lim_{n\to \infty} f_n(x_n)=f(x)$ for every $x_n\to x$ and $x\in E $ .,Let  be a sequence of continuous function on a metric space  and  for every  and  .,\{f_n\} E \lim_{n\to \infty} f_n(x_n)=f(x) x_n\to x x\in E ,"Let $E$ be a metric space where every point of $E$ is an accumulation point. Let $\{f_n\}$ be a sequence of continuous function on $E$ and $$\lim_{n\to \infty} f_n(x_n)=f(x)$$ for every sequence of point $x_n \in E$ such that $x_n\to x$ and $x_n\neq x $ . Can we have $f_n \to f $ pointwisely or uniformly ? My attempt: Let $E=(0,1]$ and $$f_n(x) = \begin{cases} n ,  0 < x < \frac1n \\       \frac1x  ,  \frac{1}{n} \le x \le 1 \end{cases}$$ Then we can easily see $f_n(x)$ satisfied the hypothesis above and $f_n \to \frac1x$ , but converges not uniformly . My question : $(1)$ Can we construct $\{f_n\}$ such that $\lim_{n\to \infty} f_n(x) \neq f(x)$ for some $x \in E$ $(2)$ If we assume $E$ is compact , can we show that $f_n \to f$ pointwisely or uniformly ?","Let be a metric space where every point of is an accumulation point. Let be a sequence of continuous function on and for every sequence of point such that and . Can we have pointwisely or uniformly ? My attempt: Let and Then we can easily see satisfied the hypothesis above and , but converges not uniformly . My question : Can we construct such that for some If we assume is compact , can we show that pointwisely or uniformly ?","E E \{f_n\} E \lim_{n\to \infty} f_n(x_n)=f(x) x_n \in E x_n\to x x_n\neq x  f_n \to f  E=(0,1] f_n(x) = \begin{cases} n ,  0 < x < \frac1n \\
      \frac1x  ,  \frac{1}{n} \le x \le 1 \end{cases} f_n(x) f_n \to \frac1x (1) \{f_n\} \lim_{n\to \infty} f_n(x) \neq f(x) x \in E (2) E f_n \to f","['sequences-and-series', 'analysis']"
71,"Prove or disprove: If $a+b \leq \frac{1}{2}$, then $\frac{(1-a)(1-b)}{ab} \geq 1$ for positive $a,b$","Prove or disprove: If , then  for positive","a+b \leq \frac{1}{2} \frac{(1-a)(1-b)}{ab} \geq 1 a,b","Let $a,b$ be two positive numbers. Prove or disprove the statement: If $a+b \leq \frac{1}{2}$ , then $\dfrac{1-a}{a} \dfrac{1-b}{b} \geq 1$ . True. Assume $a+b \leq \frac{1}{2}$ . Then $$\dfrac{1-a}{a} \dfrac{1-b}{b}=\dfrac{1-b-a+ab}{ab}=\dfrac{1}{ab}-\dfrac{a+b}{ab}+1=-\dfrac{a+b}{ab}+\dfrac{1}{ab}+1\geq \dfrac{-1}{ab}+\dfrac{1}{ab}+1=1. $$ Can you check my answer?","Let be two positive numbers. Prove or disprove the statement: If , then . True. Assume . Then Can you check my answer?","a,b a+b \leq \frac{1}{2} \dfrac{1-a}{a} \dfrac{1-b}{b} \geq 1 a+b \leq \frac{1}{2} \dfrac{1-a}{a} \dfrac{1-b}{b}=\dfrac{1-b-a+ab}{ab}=\dfrac{1}{ab}-\dfrac{a+b}{ab}+1=-\dfrac{a+b}{ab}+\dfrac{1}{ab}+1\geq \dfrac{-1}{ab}+\dfrac{1}{ab}+1=1. ",['analysis']
72,"Show that $\Omega \setminus N_A \ni x \mapsto 1_A(x) \cdot \int\limits_{\Upsilon} k(x,t)f(t)d\nu(t)$ is measurable",Show that  is measurable,"\Omega \setminus N_A \ni x \mapsto 1_A(x) \cdot \int\limits_{\Upsilon} k(x,t)f(t)d\nu(t)","Assume $(\Omega,\mathcal{A},\mu)$ and $(\Upsilon,\mathcal{B},\nu)$ are measure spaces with $\sigma$ -finite measures $\mu, \nu$ and $k \in L^2(\Omega \times \Upsilon, \mathcal{A} \otimes \mathcal{B}, \mu \times \nu)$ . I have shown for all $A \in \mathcal{A}$ with $\mu(A) < + \infty$ and for all $f \in L^2(\Upsilon,\mathcal{B},\nu,\mathbb{C})$ that the function $(x,t) \mapsto 1_A(x) \cdot \int\limits_{\Upsilon} k(x,t)f(t) d\nu(t)$ is integrable. What I want to do now is to follow that for a null set $N_A \in \mathcal{A}$ the function $g: \Omega \setminus N_A \ni x \mapsto 1_A(x) \cdot \int\limits_{\Upsilon} k(x,t)f(t)d\nu(t)$ is measurable. My idea is that since the the integral of $g$ is finite it is finite almost everywhere. Now I define the set $N_A$ as the points $x$ for which $g(x)$ is not finite. Is it now guaranteed that $g$ is measurable?",Assume and are measure spaces with -finite measures and . I have shown for all with and for all that the function is integrable. What I want to do now is to follow that for a null set the function is measurable. My idea is that since the the integral of is finite it is finite almost everywhere. Now I define the set as the points for which is not finite. Is it now guaranteed that is measurable?,"(\Omega,\mathcal{A},\mu) (\Upsilon,\mathcal{B},\nu) \sigma \mu, \nu k \in L^2(\Omega \times \Upsilon, \mathcal{A} \otimes \mathcal{B}, \mu \times \nu) A \in \mathcal{A} \mu(A) < + \infty f \in L^2(\Upsilon,\mathcal{B},\nu,\mathbb{C}) (x,t) \mapsto 1_A(x) \cdot \int\limits_{\Upsilon} k(x,t)f(t) d\nu(t) N_A \in \mathcal{A} g: \Omega \setminus N_A \ni x \mapsto 1_A(x) \cdot \int\limits_{\Upsilon} k(x,t)f(t)d\nu(t) g N_A x g(x) g","['integration', 'functional-analysis', 'analysis', 'measure-theory']"
73,"$f(x)$ is uniform continuous and $\{f(nh)\}_{n\in\mathbb N}$ converges,prove $\lim_{x\to \infty}f(x)$ exists","is uniform continuous and  converges,prove  exists",f(x) \{f(nh)\}_{n\in\mathbb N} \lim_{x\to \infty}f(x),"Assume $f(x)\in C[0,+\infty)$ and $f(x)$ is uniform continous,if for any $h>0$ ,the sequence $\{f(nh)\}_{n\in\mathbb N}$ converges,please prove that $\lim\limits_{x\to\infty}f(x)$ exists. I have no idea how to answer this question,though it doesn't seem to be diffcult.Please give me some ideas or solutions,thank you.","Assume and is uniform continous,if for any ,the sequence converges,please prove that exists. I have no idea how to answer this question,though it doesn't seem to be diffcult.Please give me some ideas or solutions,thank you.","f(x)\in C[0,+\infty) f(x) h>0 \{f(nh)\}_{n\in\mathbb N} \lim\limits_{x\to\infty}f(x)","['calculus', 'analysis']"
74,Floor and ceiling functions measurable,Floor and ceiling functions measurable,,"I want to show, that $$f: (\mathbb{R}, B(\mathbb{R})) \rightarrow   (\mathbb{R}, B(\mathbb{R}))$$ $$ f(x)=\left[\frac{1}{x}\right] \forall x\ne 0 $$ $$ 0: x=0$$ is measurable. I have to consider $$f^{-1}(\mathbb{Z} \setminus \{0\}) $$ Is this the right idea?","I want to show, that is measurable. I have to consider Is this the right idea?","f: (\mathbb{R}, B(\mathbb{R})) \rightarrow   (\mathbb{R}, B(\mathbb{R}))  f(x)=\left[\frac{1}{x}\right] \forall x\ne 0   0: x=0 f^{-1}(\mathbb{Z} \setminus \{0\}) ","['real-analysis', 'analysis', 'measure-theory', 'functions']"
75,Analysis and Calculus [closed],Analysis and Calculus [closed],,"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 5 years ago . Improve this question Is there a good book that combines Calculus I, II, III, Analysis I, II with the routine calculation exercises and that also shows rigorous proofs of the theorems? I have looked at Vladimir Zorich's Mathematical Analysis I but does not have many routine questions in it. So what are your suggestions? So the idea is I want to study Calculus with Analysis together.","Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 5 years ago . Improve this question Is there a good book that combines Calculus I, II, III, Analysis I, II with the routine calculation exercises and that also shows rigorous proofs of the theorems? I have looked at Vladimir Zorich's Mathematical Analysis I but does not have many routine questions in it. So what are your suggestions? So the idea is I want to study Calculus with Analysis together.",,"['calculus', 'analysis', 'reference-request']"
76,Approximation of subset of Hilbert space by finite-dimensional functions,Approximation of subset of Hilbert space by finite-dimensional functions,,"I cannot come up with an answer to the following problem, which I came across: Let $H$ be a separable, real Hilbert space with ONB $\{e_n\}_{n \in \mathbb{N}}$ and let $U \subseteq H$ be open (if it helps, we may instead consider $U$ closed). Then it is commonly known that there exist continuous, non-negative functions $f_n: H \to \mathbb{R}$ , $n \in \mathbb{N},$ such that $f_n$ converges monotonically increasing pointwise to $\mathbf{1}_U$ . My question is: Can I choose these functions such that $f_n \in C_b(\mathbb{R}^d)$ for each $n$ , where $d=d_n$ is of course allowed to depend on $n$ ? In saying so, I identify functions $f \ \in C_b(\mathbb{R}^d)$ with the function $f \circ P_d:H \to \mathbb{R},$ where $P_d$ denotes the projection on the linear span of $e_1,...e_d$ . I have tried a number of fruitless attempts. Roughly speaking, most of them failed due to the fact that $\underset{n \in \mathbb{N}}{\cup}span(e_1,...,e_n) \neq H$ . I would appreciate any reasonable input on this! Thank you in advance!","I cannot come up with an answer to the following problem, which I came across: Let be a separable, real Hilbert space with ONB and let be open (if it helps, we may instead consider closed). Then it is commonly known that there exist continuous, non-negative functions , such that converges monotonically increasing pointwise to . My question is: Can I choose these functions such that for each , where is of course allowed to depend on ? In saying so, I identify functions with the function where denotes the projection on the linear span of . I have tried a number of fruitless attempts. Roughly speaking, most of them failed due to the fact that . I would appreciate any reasonable input on this! Thank you in advance!","H \{e_n\}_{n \in \mathbb{N}} U \subseteq H U f_n: H \to \mathbb{R} n \in \mathbb{N}, f_n \mathbf{1}_U f_n \in C_b(\mathbb{R}^d) n d=d_n n f \ \in C_b(\mathbb{R}^d) f \circ P_d:H \to \mathbb{R}, P_d e_1,...e_d \underset{n \in \mathbb{N}}{\cup}span(e_1,...,e_n) \neq H","['functional-analysis', 'analysis', 'hilbert-spaces']"
77,How to prove the limit related to the following infinite series?,How to prove the limit related to the following infinite series?,,I would appreciate it if you can tell me how to prove this limit $$\lim_{x\to 0^{+}}\sum_{n=2}^{\infty}\frac{(-1)^{n}}{(\log n)^{x}}=\frac{1}{2}$$,I would appreciate it if you can tell me how to prove this limit,\lim_{x\to 0^{+}}\sum_{n=2}^{\infty}\frac{(-1)^{n}}{(\log n)^{x}}=\frac{1}{2},"['sequences-and-series', 'analysis', 'summation']"
78,Stronger version of finite additivity of Legesgue measure,Stronger version of finite additivity of Legesgue measure,,"Let $m_*(A)$ denote the Lebesgue outer measure, and when $A$ is measurable, let $m(A)=m_*(A)$ be the Lebesgue measure. Let $U=\{E_1,\cdots,E_N\}$ be a finite collection of pairwise disjoint Lebesgue measurable sets (of $\mathbb{R}^n$ ), and denote $E:=\bigcup_{i=1}^{N} E_i$ . We know that (finite additivity): $$ m(E)=\sum_{i=1}^{N}m(E_i)\tag{1}$$ If we add another set $A$ , which may not be measurable, is it true that $$m_*(A\cap E)=\sum_{i=1}^{N}m_*(A\cap E_i)\tag{$*$}$$ (1) is a special case of ( $*$ ) when $A=E$ . Anyone have any idea on proving  ( $*$ )? I thought $m_*(A\cap E)=m_*(A)-m_*(A\setminus E)$ may be useful but I don't know how to proceed. Note that the definition of a set $E$ is measurable is that $m_*(A)=m_*(A\setminus E)+m_*(A\cap E)$ for all $A$ .","Let denote the Lebesgue outer measure, and when is measurable, let be the Lebesgue measure. Let be a finite collection of pairwise disjoint Lebesgue measurable sets (of ), and denote . We know that (finite additivity): If we add another set , which may not be measurable, is it true that (1) is a special case of ( ) when . Anyone have any idea on proving  ( )? I thought may be useful but I don't know how to proceed. Note that the definition of a set is measurable is that for all .","m_*(A) A m(A)=m_*(A) U=\{E_1,\cdots,E_N\} \mathbb{R}^n E:=\bigcup_{i=1}^{N} E_i  m(E)=\sum_{i=1}^{N}m(E_i)\tag{1} A m_*(A\cap E)=\sum_{i=1}^{N}m_*(A\cap E_i)\tag{*} * A=E * m_*(A\cap E)=m_*(A)-m_*(A\setminus E) E m_*(A)=m_*(A\setminus E)+m_*(A\cap E) A","['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure']"
79,"If series is not uniformly convergent, can we still integrate term by term?","If series is not uniformly convergent, can we still integrate term by term?",,"We know that if $\sum a_nx^n$ converges uniformly, then we can integrate term by term. So this is just a sufficient condition, right? Does there exists a series not converging uniformly and still we can perform term by term integration? I am looking for some general results in this direction.","We know that if converges uniformly, then we can integrate term by term. So this is just a sufficient condition, right? Does there exists a series not converging uniformly and still we can perform term by term integration? I am looking for some general results in this direction.",\sum a_nx^n,"['real-analysis', 'calculus']"
80,"Find asymptotic of the next sequence : $a_n =\sqrt[4]{1}+\sqrt[4]{2}+...+\sqrt[4]{n}, n \to \infty$",Find asymptotic of the next sequence :,"a_n =\sqrt[4]{1}+\sqrt[4]{2}+...+\sqrt[4]{n}, n \to \infty","Find asymptotic of sequence $a_n=\sqrt[4]{1}+\sqrt[4]{2}+...+\sqrt[4]{n}, n \to \infty$ . Here is my solution: $$\sqrt[4]{1}+\sqrt[4]{2}+...+\sqrt[4]{n}=n^\frac{5}{4}\sum\limits_{i=1}^{n} \sqrt[4]\frac{i}{n}\frac{1}{n} =n^\frac{5}{4}\left(\int\limits_{0}^{1} \sqrt[4]{x}dx+o(1)\right)=n^\frac{5}{4}\left(\frac{4}{5}+o(1)\right)$$ But  I'm not sure that it's correct. Could someone please check it and let me know what is wrong?",Find asymptotic of sequence . Here is my solution: But  I'm not sure that it's correct. Could someone please check it and let me know what is wrong?,"a_n=\sqrt[4]{1}+\sqrt[4]{2}+...+\sqrt[4]{n}, n \to \infty \sqrt[4]{1}+\sqrt[4]{2}+...+\sqrt[4]{n}=n^\frac{5}{4}\sum\limits_{i=1}^{n} \sqrt[4]\frac{i}{n}\frac{1}{n} =n^\frac{5}{4}\left(\int\limits_{0}^{1} \sqrt[4]{x}dx+o(1)\right)=n^\frac{5}{4}\left(\frac{4}{5}+o(1)\right)","['analysis', 'proof-verification']"
81,Riemann integral of a function with Banach space values,Riemann integral of a function with Banach space values,,"My question is: how to prove that Riemann integral of a continuous function $f\colon [a,b]\to Y$ , where $Y$ is a Banach space, is independent of the choice of intermediate points? Let $\{t_0,...,t_n\}\subset [a,b]$ be such that $a=t_0\le t_1\le...\le t_n=b$ and $\Delta Z_n=\max_{i\in\{i,...n\}}(t_i-t_{i-1})\to0$ as $n\to\infty$ . Let $S_{Z_{n}}=\sum_{i=1}^{n}f(x_i)(t_i-t_{i-1})$ , where $x_i\in [t_i-t_{i-1}]$ . I define the Riemann integral by $$\int_{a}^{b}f(t)\,dt :=\lim_{n\to\infty}S_{Z_{n}}.$$ First of all we notice that the function $[a,b]\ni t\mapsto\|f(t)\|$ is continuous on a compact set, so it is bounded. Hence $$\lim_{n\to\infty}S_{Z_{n}}\le \sup_{t\in [a,b]}\|f(t)\|\cdot\lambda([a,b])=M<\infty,$$ where $\lambda$ is meant to be Lebesgue measure. In order to justify the definition of Riemann integral I have to do two things. First, I need to prove that the limit $\lim_{n\to\infty}S_{Z_{n}}$ exists and secondly that the integral  is independent of the choice of intermediate points. We can prove it at one go by showing that the sequance $(S_{Z_{n}})$ is Cauchy I suppose. We shall prove that $\|S_{Z_{n}}-S_{Z_{m}}\|\to 0$ . Consider $$\|S_{Z_{n}}-S_{Z_{m}}\|=\|\sum_{i=1}^{n}f(x_{n,i})(t_{n,i}-t_{n, i-1})-\sum_{j=1}^{m}f(x_{m,j})(t_{m,j}-t_{m, i-j})\|\le\\ \le\sum_{i=1}^{n}\sum_{j=1}^{m}\|f(x_{n,i})-f(x_{m,j})\|\,\lambda([t_{n,i}-t_{n,i-1}]\cap [t_{m,j}-t_{m,j-1}])\le\\ \le \sum_{i=1}^{n}\sum_{j=1}^{m}\|f(x_{n,i})-f(x_{m,j})\|\max\{\Delta Z_{n}, \Delta Z'_{m}\}. $$ Unfortunately, I can't deduce the desired convergance, since I have no info about the ratio of convergance of $\max\{\Delta Z_{n}, \Delta Z'_{m}\}$ . As concerns $\|f(x_{n,i})-f(x_{m,j})\|$ , we can easily estimate it from above or maybe it would be more sufficient to use the fact that $t\mapsto\|f(t)\|$ is uniformly continuous. What shall I do?","My question is: how to prove that Riemann integral of a continuous function , where is a Banach space, is independent of the choice of intermediate points? Let be such that and as . Let , where . I define the Riemann integral by First of all we notice that the function is continuous on a compact set, so it is bounded. Hence where is meant to be Lebesgue measure. In order to justify the definition of Riemann integral I have to do two things. First, I need to prove that the limit exists and secondly that the integral  is independent of the choice of intermediate points. We can prove it at one go by showing that the sequance is Cauchy I suppose. We shall prove that . Consider Unfortunately, I can't deduce the desired convergance, since I have no info about the ratio of convergance of . As concerns , we can easily estimate it from above or maybe it would be more sufficient to use the fact that is uniformly continuous. What shall I do?","f\colon [a,b]\to Y Y \{t_0,...,t_n\}\subset [a,b] a=t_0\le t_1\le...\le t_n=b \Delta Z_n=\max_{i\in\{i,...n\}}(t_i-t_{i-1})\to0 n\to\infty S_{Z_{n}}=\sum_{i=1}^{n}f(x_i)(t_i-t_{i-1}) x_i\in [t_i-t_{i-1}] \int_{a}^{b}f(t)\,dt :=\lim_{n\to\infty}S_{Z_{n}}. [a,b]\ni t\mapsto\|f(t)\| \lim_{n\to\infty}S_{Z_{n}}\le \sup_{t\in [a,b]}\|f(t)\|\cdot\lambda([a,b])=M<\infty, \lambda \lim_{n\to\infty}S_{Z_{n}} (S_{Z_{n}}) \|S_{Z_{n}}-S_{Z_{m}}\|\to 0 \|S_{Z_{n}}-S_{Z_{m}}\|=\|\sum_{i=1}^{n}f(x_{n,i})(t_{n,i}-t_{n, i-1})-\sum_{j=1}^{m}f(x_{m,j})(t_{m,j}-t_{m, i-j})\|\le\\
\le\sum_{i=1}^{n}\sum_{j=1}^{m}\|f(x_{n,i})-f(x_{m,j})\|\,\lambda([t_{n,i}-t_{n,i-1}]\cap [t_{m,j}-t_{m,j-1}])\le\\
\le \sum_{i=1}^{n}\sum_{j=1}^{m}\|f(x_{n,i})-f(x_{m,j})\|\max\{\Delta Z_{n}, \Delta Z'_{m}\}.
 \max\{\Delta Z_{n}, \Delta Z'_{m}\} \|f(x_{n,i})-f(x_{m,j})\| t\mapsto\|f(t)\|","['real-analysis', 'integration', 'analysis']"
82,$d_p$ and $d_\infty$ in $\mathbb{C}^n$ are uniformly equivalent,and  in  are uniformly equivalent,d_p d_\infty \mathbb{C}^n,"I need to prove this Show  with the metrics $d_p$ and $d_{\infty}$ in $\mathbb{C}^n$ are uniformly equivalents, with $p \in [1, \infty)$ . So, I have in my book two definitions about equivalence metrics in a metric space $(X,d)$ . 1) Two metrics $d_1$ and $d_2$ are uniformly equivalents if there are constants $a, b >0$ such that $ad_1(x,y) \leq d_2(x,y) \leq bd_1(x,y), \forall x, y \in \mathbb{C}^n$ or equivalently, if $a \leq \dfrac{d_2(x,y)}{d_1(x,y)}\leq b $ for all $x \neq y$ . 2) The second definition is about topology equivalence. We say that two metrics $d_1$ and $d_2$ are topology equivalents if any sequence convergent in space $X$ with the metric $d_1$ also converge in the metric $d_2$ and for the same limit point. I have already proved two facts: a) If two metrics $d_1$ and $d_2$ are uniformly equivalent in $X$ , then a subset $M$ of $X$ is bounded with respect to the metric $d_1$ if, and only if, $M$ is bounded with respect to the metric $d_2$ . b) If two metrics are uniformly equivalent, then they are topologically equivalent. But my problem above continues.  I could this: By definition we know that $$ d_p (x,y) = \left(  \sum_{i=1}^{n} \ |x_i -y_i|^{p} \right)^{1/p} \mbox{e}\;\; d_\infty (x,y) = \sup_{i=1,..., n}{ |x_i - y_i| }. $$ So, by Minkowski's inequality, we have $$ d_p (x,y) = \left(  \sum_{i=1}^{n} \ |x_i -y_i|^{p} \right)^{1/p} \leq  \left(  \sum_{i=1}^{n} \ |x_i|^{p} \right)^{1/p} + \left(  \sum_{i=1}^{n} \ |y_i|^{p} \right)^{1/p} \leq M_1 + M_2 = M $$ and $$ d_\infty = \sup_{i=1,..., n}{ |x_i - y_i| } \leq N. $$ How $0 < d_p(x,y)$ and $0 < d_\infty (x,y)$ for all $x \neq y$ , we have with statements above that $$ 0 \leq \dfrac{d_p(x,y)}{d_{\infty}(x,y)} \leq \dfrac{M}{N}=b, b>0. $$ My problem here is how can I to prove with there is a constant positive $a$ such that $a \leq \dfrac{d_p(x,y)}{d_{\infty}(x,y)}$ . Thanks.","I need to prove this Show  with the metrics and in are uniformly equivalents, with . So, I have in my book two definitions about equivalence metrics in a metric space . 1) Two metrics and are uniformly equivalents if there are constants such that or equivalently, if for all . 2) The second definition is about topology equivalence. We say that two metrics and are topology equivalents if any sequence convergent in space with the metric also converge in the metric and for the same limit point. I have already proved two facts: a) If two metrics and are uniformly equivalent in , then a subset of is bounded with respect to the metric if, and only if, is bounded with respect to the metric . b) If two metrics are uniformly equivalent, then they are topologically equivalent. But my problem above continues.  I could this: By definition we know that So, by Minkowski's inequality, we have and How and for all , we have with statements above that My problem here is how can I to prove with there is a constant positive such that . Thanks.","d_p d_{\infty} \mathbb{C}^n p \in [1, \infty) (X,d) d_1 d_2 a, b >0 ad_1(x,y) \leq d_2(x,y) \leq bd_1(x,y), \forall x, y \in \mathbb{C}^n a \leq \dfrac{d_2(x,y)}{d_1(x,y)}\leq b  x \neq y d_1 d_2 X d_1 d_2 d_1 d_2 X M X d_1 M d_2 
d_p (x,y) = \left(  \sum_{i=1}^{n} \ |x_i -y_i|^{p} \right)^{1/p} \mbox{e}\;\;
d_\infty (x,y) = \sup_{i=1,..., n}{ |x_i - y_i| }.
 
d_p (x,y) = \left(  \sum_{i=1}^{n} \ |x_i -y_i|^{p} \right)^{1/p} \leq 
\left(  \sum_{i=1}^{n} \ |x_i|^{p} \right)^{1/p} +
\left(  \sum_{i=1}^{n} \ |y_i|^{p} \right)^{1/p} \leq M_1 + M_2 = M
 
d_\infty = \sup_{i=1,..., n}{ |x_i - y_i| } \leq N.
 0 < d_p(x,y) 0 < d_\infty (x,y) x \neq y 
0 \leq \dfrac{d_p(x,y)}{d_{\infty}(x,y)} \leq \dfrac{M}{N}=b, b>0.
 a a \leq \dfrac{d_p(x,y)}{d_{\infty}(x,y)}","['functional-analysis', 'analysis', 'metric-spaces']"
83,Fractional Derivative and the Fourier Transform,Fractional Derivative and the Fourier Transform,,"I've recently came across the notion of a fractional derivative of a function $f$ that is defined as $$\big(D^{\frac{1}{2}} f\big)(x)= \frac{1}{\Gamma(\frac{1}{2})}\int_0^x (x-t)^{-\frac{1}{2}}f(t) dt$$ Now, I typically know of functions of differential operators to be defined in terms of the Fourier Transform. So in my case I'd think of the fractional derivative to be defined as $$\big(D^{\frac{1}{2}}f\big)(x) = \frac{1}{2\pi}\int_{\mathbb{R}}e^{i \langle x, \xi \rangle} \sqrt{\xi} \cdot \hat{f}(\xi) d\xi$$ I imagine these should be the same but how would I go about showing this?","I've recently came across the notion of a fractional derivative of a function that is defined as Now, I typically know of functions of differential operators to be defined in terms of the Fourier Transform. So in my case I'd think of the fractional derivative to be defined as I imagine these should be the same but how would I go about showing this?","f \big(D^{\frac{1}{2}} f\big)(x)= \frac{1}{\Gamma(\frac{1}{2})}\int_0^x (x-t)^{-\frac{1}{2}}f(t) dt \big(D^{\frac{1}{2}}f\big)(x) = \frac{1}{2\pi}\int_{\mathbb{R}}e^{i \langle x, \xi \rangle} \sqrt{\xi} \cdot \hat{f}(\xi) d\xi","['real-analysis', 'analysis', 'fourier-analysis', 'fractional-calculus']"
84,Is the inverse of a finite-propagation operator finite-propagation?,Is the inverse of a finite-propagation operator finite-propagation?,,"A bounded operator $T:L^2(\mathbb{R}^n)\rightarrow L^2(\mathbb{R}^n)$ is said to have finite propagation if there exists $r>0$ such that: if $\phi,\psi\in C_c(\mathbb{R}^n)$ have supports that are separated by a distance larger than $r$ , then $$\psi T\phi=0\in\mathcal{B}(L^2(\mathbb{R}^n)).$$ Suppose $T$ is invertible and has finite propagation. Then does it follow that $T^{-1}$ has finite propagation? Remark: I suspect the answer is no, but I'm looking for a concrete example.","A bounded operator is said to have finite propagation if there exists such that: if have supports that are separated by a distance larger than , then Suppose is invertible and has finite propagation. Then does it follow that has finite propagation? Remark: I suspect the answer is no, but I'm looking for a concrete example.","T:L^2(\mathbb{R}^n)\rightarrow L^2(\mathbb{R}^n) r>0 \phi,\psi\in C_c(\mathbb{R}^n) r \psi T\phi=0\in\mathcal{B}(L^2(\mathbb{R}^n)). T T^{-1}","['functional-analysis', 'analysis', 'operator-theory', 'operator-algebras']"
85,Darboux continuity of the function $f(x) = \limsup_{n \to \infty} \frac{(x_{1}+...+x_{n})^{2}}{n^{2}}$,Darboux continuity of the function,f(x) = \limsup_{n \to \infty} \frac{(x_{1}+...+x_{n})^{2}}{n^{2}},"Let $f : [0,1] \to [0,1]$ be a function that assigns to each $x \in [0,1]$ the following value: $$ x = 0.x_{1}x_{2}x_{3} \ \ ... \hspace{0.3cm} \text{be the binary expansion of }x $$ define $$ f(x): = \limsup_{n \to \infty} \frac{(x_{1}+...+x_{n})^{2}}{n^{2}}$$ Prove that $f$ is Darboux continuous but not continuous. Can someone give hint on this problem? Thank you.",Let be a function that assigns to each the following value: define Prove that is Darboux continuous but not continuous. Can someone give hint on this problem? Thank you.,"f : [0,1] \to [0,1] x \in [0,1]  x = 0.x_{1}x_{2}x_{3} \ \ ... \hspace{0.3cm} \text{be the binary expansion of }x   f(x): = \limsup_{n \to \infty} \frac{(x_{1}+...+x_{n})^{2}}{n^{2}} f","['real-analysis', 'analysis', 'continuity', 'binary', 'average']"
86,Show $\lim\limits_{n \to \infty} x_n=a$,Show,\lim\limits_{n \to \infty} x_n=a,"Given $n,k \in \mathbb{N}$ , $t_{n,k} \geq 0$ , $\sum\limits_{k=1}^n t_{n,k}=1$ , $\lim\limits_{n \to \infty}t_{n,k}=0$ . $\lim\limits_{n \to \infty}a_n=a$ and let $x_n := \sum\limits_{k=1}^n t_{n,k}a_k$ . Show $\lim\limits_{n \to \infty} x_n=a$ . I think I can see why intuitively. For large $n$ , in $\sum\limits_{k=1}^n t_{n,k}a_k$ , we are summing big portion of term of the form $\epsilon_j (a\pm\epsilon_i)$ , so it is approximately $(1-\epsilon)a+\epsilon*First\_few\_term\_of\_a_n \approx a$ . Is there clever ways to prove it?","Given , , , . and let . Show . I think I can see why intuitively. For large , in , we are summing big portion of term of the form , so it is approximately . Is there clever ways to prove it?","n,k \in \mathbb{N} t_{n,k} \geq 0 \sum\limits_{k=1}^n t_{n,k}=1 \lim\limits_{n \to \infty}t_{n,k}=0 \lim\limits_{n \to \infty}a_n=a x_n := \sum\limits_{k=1}^n t_{n,k}a_k \lim\limits_{n \to \infty} x_n=a n \sum\limits_{k=1}^n t_{n,k}a_k \epsilon_j (a\pm\epsilon_i) (1-\epsilon)a+\epsilon*First\_few\_term\_of\_a_n \approx a",['analysis']
87,Harmonic functions in the half-plane,Harmonic functions in the half-plane,,Denote by $\mathbb{H}$ the upper half-plane $$ \mathbb{H} := \left\{ x \in \mathbb{R}^n : x_n > 0\right\}.  $$ Suppose that $u \in C^2(\mathbb{H}) \cap C(\bar{\mathbb{H}})$ is a bounded harmonic function such that $u \leq 0$ on $\partial\mathbb{H} = \{ x_n = 0\}$ . Is it possible to conclude that $u \leq 0$ in all of $\mathbb{H}$ ? I know this is the case for $n = 2$ but am unable to establish the general case $n \geq 3$ .,Denote by the upper half-plane Suppose that is a bounded harmonic function such that on . Is it possible to conclude that in all of ? I know this is the case for but am unable to establish the general case .,"\mathbb{H} 
\mathbb{H} := \left\{ x \in \mathbb{R}^n : x_n > 0\right\}. 
 u \in C^2(\mathbb{H}) \cap C(\bar{\mathbb{H}}) u \leq 0 \partial\mathbb{H} = \{ x_n = 0\} u \leq 0 \mathbb{H} n = 2 n \geq 3",['analysis']
88,Absolutely continuous spectrum invariant under unitary equivalence,Absolutely continuous spectrum invariant under unitary equivalence,,"I am doing an exercise calculating the absolutely continuous spectrum of some operator $A$ by calculating the spectrum of a different operator $B$ unitarily equivalent to $A$ , i.e. $$A = U B U^*.$$ I know that $\sigma (A) = \sigma(B)$ as is easily seen by playing with the resolvent. But I cannot as easily conclude $\sigma_{ac}(A) = \sigma_{ac}(B)$ . Am I missing something obvious? How can one show this? I first thought about something along $$\sigma_{ac}(A) = \sigma(A \mid_{\mathcal H_{ac}}) = \sigma(UBU^* \mid_{\mathcal H_{ac}}) \stackrel{?}{=} \sigma(U B \mid_{\stackrel{\sim}{\mathcal H_{ac}} }U^*) = \sigma_{ac}(B),$$ but I don't think the third equality is justifiable, especially we have different Hilbert spaces $\mathcal H, \stackrel{\sim}{\mathcal H}$ .","I am doing an exercise calculating the absolutely continuous spectrum of some operator by calculating the spectrum of a different operator unitarily equivalent to , i.e. I know that as is easily seen by playing with the resolvent. But I cannot as easily conclude . Am I missing something obvious? How can one show this? I first thought about something along but I don't think the third equality is justifiable, especially we have different Hilbert spaces .","A B A A = U B U^*. \sigma (A) = \sigma(B) \sigma_{ac}(A) = \sigma_{ac}(B) \sigma_{ac}(A) = \sigma(A \mid_{\mathcal H_{ac}}) = \sigma(UBU^* \mid_{\mathcal H_{ac}}) \stackrel{?}{=} \sigma(U B \mid_{\stackrel{\sim}{\mathcal H_{ac}}
}U^*) = \sigma_{ac}(B), \mathcal H, \stackrel{\sim}{\mathcal H}","['functional-analysis', 'analysis', 'operator-theory', 'mathematical-physics', 'spectral-theory']"
89,Optimality of Kantorovich potentials for the squared distance,Optimality of Kantorovich potentials for the squared distance,,"This question comes from Villani's book, Optimal Transport: Old and New. Consider the cost function $c(x, y) = |x - y|^2$ on $X  \times Y$ , where $X$ is the right half of the unit ball, and $Y$ is the right half of the unit ball, shifted one unit to the right. Let $\mu$ be the uniform distribution on $X$ and $\nu$ the uniform distribution on $Y$ . The optimal transport map $T$ is given by $(x, y)  \mapsto (x + 1, y)$ ,  and by a theorem from chapter 10 we know that $T(x, y)  =  (x,y) + \nabla \psi(x, y)$ for some $c$ -convex function $\psi$ on $X$ . Since this  means that $\psi(x, y) = (1, 0)$ we have $\psi(x, y) = x$ . My question: Is this $\psi$ equal to the optimal $\psi$ in the Kantorovich dual problem? That is, is the following inequality  true: $$ \int_X c(x, Tx)\, d\mu(x) = \int_Y \psi^c \, d\nu - \int_X \psi\, d\mu, $$ where $\psi^c(y) := \inf_x \psi(x) + c(x, y)$ . I've tried computing the $c$ -transform and as far as I can tell the answer to my question is negative, since I am getting that $\partial_c \psi(x)$ is not the translate by 1, which it should be to guarantee optimality by Kantorovich duality. Any help is much appreciated!","This question comes from Villani's book, Optimal Transport: Old and New. Consider the cost function on , where is the right half of the unit ball, and is the right half of the unit ball, shifted one unit to the right. Let be the uniform distribution on and the uniform distribution on . The optimal transport map is given by ,  and by a theorem from chapter 10 we know that for some -convex function on . Since this  means that we have . My question: Is this equal to the optimal in the Kantorovich dual problem? That is, is the following inequality  true: where . I've tried computing the -transform and as far as I can tell the answer to my question is negative, since I am getting that is not the translate by 1, which it should be to guarantee optimality by Kantorovich duality. Any help is much appreciated!","c(x, y) = |x - y|^2 X  \times Y X Y \mu X \nu Y T (x, y)  \mapsto (x + 1, y) T(x, y)  =  (x,y) + \nabla \psi(x, y) c \psi X \psi(x, y) = (1, 0) \psi(x, y) = x \psi \psi 
\int_X c(x, Tx)\, d\mu(x) = \int_Y \psi^c \, d\nu - \int_X \psi\, d\mu,
 \psi^c(y) := \inf_x \psi(x) + c(x, y) c \partial_c \psi(x)","['analysis', 'optimization', 'optimal-transport']"
90,$\iint_D f = 0$ implies $f(p)=0$ for all $p$.,implies  for all .,\iint_D f = 0 f(p)=0 p,"If $D$ is open, and if $f$ is continuous, bounded, and obeys $f(p) \ge 0$ for all $p \in D$ , then $\iint_D f = 0$ implies $f(p)=0$ for all $p$ . The hint in the back of my book says that There's a neighborhood where $f(p) \ge \delta$ . From the definition in my book, it states that The double integral $\iint_R f$ exists and has value $v$ if and only if for any $\epsilon > 0$ there is a $\delta > 0$ such that $|S(N,f,\{p_{ij}\})-v| < \delta$ . There's another theorem in the book that states that If $f(p) \ge 0$ for all $p\in D$ , $\iint_D f \ge 0$ . So far, from the information I've been given, that would mean that I could use the contrapositive to prove that if $f(p) \ne 0$ for all $p$ , then $\iint_D f \ne 0$ . Since $f(p) \ne 0$ I could use cases. Case 1: If $f(p) > 0$ , then I could use the aforementioned theorem to show that $\iint_D f > 0$ which is not equal to $0$ . Case 2: If $f(p) < 0$ , then $-f(p)=g(p)>0$ , so that $\iint_D g(p) > 0$ . Then $\iint_D g(p) > 0$ is equivalent to - $\iint_D f(p) > 0$ which is not equal to $0$ . I would like to know if I'm approaching this proof the correct way? Also, would it be difficult to instead try a direct proof using the hint and the definition that I stated above?","If is open, and if is continuous, bounded, and obeys for all , then implies for all . The hint in the back of my book says that There's a neighborhood where . From the definition in my book, it states that The double integral exists and has value if and only if for any there is a such that . There's another theorem in the book that states that If for all , . So far, from the information I've been given, that would mean that I could use the contrapositive to prove that if for all , then . Since I could use cases. Case 1: If , then I could use the aforementioned theorem to show that which is not equal to . Case 2: If , then , so that . Then is equivalent to - which is not equal to . I would like to know if I'm approaching this proof the correct way? Also, would it be difficult to instead try a direct proof using the hint and the definition that I stated above?","D f f(p) \ge 0 p \in D \iint_D f = 0 f(p)=0 p f(p) \ge \delta \iint_R f v \epsilon > 0 \delta > 0 |S(N,f,\{p_{ij}\})-v| < \delta f(p) \ge 0 p\in D \iint_D f \ge 0 f(p) \ne 0 p \iint_D f \ne 0 f(p) \ne 0 f(p) > 0 \iint_D f > 0 0 f(p) < 0 -f(p)=g(p)>0 \iint_D g(p) > 0 \iint_D g(p) > 0 \iint_D f(p) > 0 0","['calculus', 'analysis', 'multivariable-calculus']"
91,Origin of Taylor Series,Origin of Taylor Series,,"Historically, the Taylor series representations or truncated Taylor series approximations of a function at a point $x_0$ was first done by taking Newton's form of an interpolation polynomial for points of the form $x_0 + n \Delta$ , where $\Delta$ is a positive real number and $n$ is a natural number, and then taking the limit as $\Delta$ goes to $0$ . Could someone explain in detail how this was done? Let $f$ be a function on an interval of real numbers. Let $\Delta$ be a real number and let $x_0$ be in the domain of $f$ such that $x_0$ , $x_0 + \Delta$ , $\dots$ , $x_0 + n\Delta$ is in the domain of $f$ , where $n$ is a positive integer. Newton's form of the interpolation polynomial of the data $\{ (x_0 + k \Delta, f(x_0 + k\Delta) \}$ is $$f(x_0) + \frac{f(x_0 + \Delta) - g_0(x_0+\Delta)}{\Delta}(x-x_0) + \cdots + \frac{f(x_0 + n\Delta ) - g_{n-1}(x_0 + n\Delta)}{n!\Delta}(x-x_0)\cdots (x-(n-1)\Delta), $$ where $g_k$ is the interpolation polynomial for the first $k+1$ points. Taking the limit as $\Delta$ tends towards $0$ gives $$  f(x_0) + f'(x_0)\cdot x + \cdots + \frac{1}{n!}\cdot (\lim_{\Delta \to 0} \frac{f(x_0 + n\Delta) -g_{n-1}(x_0 + n\Delta)}{\Delta^n})\cdot x^n, $$ as long as $f$ is sufficently smooth and the limit $\lim_{\Delta \to 0} \frac{f(x_0 + n\Delta) -g_{n-1}(x_0 + n\Delta)}{\Delta^n}$ exits. But why does the limit $\lim_{\Delta \to 0} \frac{f(x_0 + n\Delta) -g_{n-1}(x_0 + n\Delta)}{\Delta^n}$ equal $f^{(n)}(x_0)$ ?","Historically, the Taylor series representations or truncated Taylor series approximations of a function at a point was first done by taking Newton's form of an interpolation polynomial for points of the form , where is a positive real number and is a natural number, and then taking the limit as goes to . Could someone explain in detail how this was done? Let be a function on an interval of real numbers. Let be a real number and let be in the domain of such that , , , is in the domain of , where is a positive integer. Newton's form of the interpolation polynomial of the data is where is the interpolation polynomial for the first points. Taking the limit as tends towards gives as long as is sufficently smooth and the limit exits. But why does the limit equal ?","x_0 x_0 + n \Delta \Delta n \Delta 0 f \Delta x_0 f x_0 x_0 + \Delta \dots x_0 + n\Delta f n \{ (x_0 + k \Delta, f(x_0 + k\Delta) \} f(x_0) + \frac{f(x_0 + \Delta) - g_0(x_0+\Delta)}{\Delta}(x-x_0) + \cdots + \frac{f(x_0 + n\Delta ) - g_{n-1}(x_0 + n\Delta)}{n!\Delta}(x-x_0)\cdots (x-(n-1)\Delta),
 g_k k+1 \Delta 0  
f(x_0) + f'(x_0)\cdot x + \cdots + \frac{1}{n!}\cdot (\lim_{\Delta \to 0} \frac{f(x_0 + n\Delta) -g_{n-1}(x_0 + n\Delta)}{\Delta^n})\cdot x^n,
 f \lim_{\Delta \to 0} \frac{f(x_0 + n\Delta) -g_{n-1}(x_0 + n\Delta)}{\Delta^n} \lim_{\Delta \to 0} \frac{f(x_0 + n\Delta) -g_{n-1}(x_0 + n\Delta)}{\Delta^n} f^{(n)}(x_0)","['real-analysis', 'sequences-and-series', 'analysis', 'power-series', 'taylor-expansion']"
92,How is $C_0^k(\mathbb R)$ defined?,How is  defined?,C_0^k(\mathbb R),"I'm sorry for asking such a short question, but I see the space $C_0^k(\mathbb R)$ everywhere being used without a rigorous definition. $C_0(\mathbb R)$ is the space of continuous functions on $\mathbb R$ vanishing at infinity. The question is, is $C_0^k(\mathbb R):=C_0(\mathbb R)\cap C^k(\mathbb R)$ or is $C_0^k(\mathbb R):=\left\{f\in C^k(\mathbb R):f^{(i)}\in C_0(\mathbb R)\text{ for all }i\in\left\{0,\ldots,k\right\}\right\}$ ? Those spaces shouldn't coincide.","I'm sorry for asking such a short question, but I see the space everywhere being used without a rigorous definition. is the space of continuous functions on vanishing at infinity. The question is, is or is ? Those spaces shouldn't coincide.","C_0^k(\mathbb R) C_0(\mathbb R) \mathbb R C_0^k(\mathbb R):=C_0(\mathbb R)\cap C^k(\mathbb R) C_0^k(\mathbb R):=\left\{f\in C^k(\mathbb R):f^{(i)}\in C_0(\mathbb R)\text{ for all }i\in\left\{0,\ldots,k\right\}\right\}","['analysis', 'derivatives']"
93,Properties of Counting Measures Restricted to a Particular Sigma Algebra,Properties of Counting Measures Restricted to a Particular Sigma Algebra,,"Define $\mathcal{F}=\{A\subseteq\mathbb{R} \ \vert \ 0\in A^{\mathrm{o}} \ or \ \ 0 \in (A^c)^{\mathrm{o}} \}$ , where $A^{\mathrm{o}}$ is the interior of $A$ . It can be shown quite easily that $\mathcal{F}$ is an algebra of sets, and that $\sigma(\mathcal{F})$ contains the singletons of $\mathbb{R}$ . If $\gamma$ is the counting measure on $(\mathbb{R},\mathcal{P}(\mathbb{R}))$ , let $\alpha(A)=\gamma(A \ \cap \ \{\frac{1}{n} \ \vert\ n \in \mathbb{N} \})$ and $\beta(A)=\gamma(A \ \cap \ \{\frac{1}{n} \ \vert\ n \in \mathbb{N} \} \ \cup \  \{0\})$ . If we define $\mu := \left.\alpha \ \right|_{\sigma(\mathcal{F})}$ and $\nu := \left.\beta \ \right|_{\sigma(\mathcal{F})}$ (the restrictions of $\mu$ and $\nu$ to $\sigma(\mathcal{F})$ ) then it turns out that: a) both $\mu$ and $\nu$ are $\sigma$ -finite. b) $\mu$ and $\nu$ coincide on $\mathcal{F}$ (that is, $\mu(A)=\nu(A)$ , $\forall A \in \mathcal{F}$ ). c) $\mu \neq \nu(A)$ on $\sigma(\mathcal{F})$ . I am having trouble in proving these 3 facts, and any help/hints to help push me towards the right direction will be much appreciated. In particular, I'm having difficulty in working with any general set $A\in \sigma(\mathcal{F})$ ; I currently have no clue on how to construct a sequence $(A_n)_{n=1}^{\infty}$ in $\sigma(\mathcal{F})$ such that $\bigcup\limits_{n=1}^{\infty}A_n = \mathbb{R}$ with $\mu(A_n)<\infty$ and $\nu(A_n)<\infty$ . EDIT: I've actually made some progress with this since asking: I have solutions to b) and c), and have partial work for a). The struggle in part a) remains with constructing such a sequence of sets. Best I've come up with is $(A_k)_{k=1}^{\infty}=(-k,-\frac{1}{k}] \cup [\frac{1}{k},k)$ , and in this case, $0 \in (A_k^c)^{\mathrm{o}}$ with $\mu(A_k)<\infty$ and $\nu(A_k)<\infty$ . Problem here is that the countable union isn't the whole real line (it misses $\{0\}$ !). Similarly, I've also considered the sequence $(A_k)_{k=1}^{\infty}=(-k,0] \cup [\frac{1}{k},k)$ , whose countable union is all of $\mathbb{R}$ , $\mu$ and $\nu$ are finite $\forall k$ , but this sequence is not in $\mathcal{F}$ .","Define , where is the interior of . It can be shown quite easily that is an algebra of sets, and that contains the singletons of . If is the counting measure on , let and . If we define and (the restrictions of and to ) then it turns out that: a) both and are -finite. b) and coincide on (that is, , ). c) on . I am having trouble in proving these 3 facts, and any help/hints to help push me towards the right direction will be much appreciated. In particular, I'm having difficulty in working with any general set ; I currently have no clue on how to construct a sequence in such that with and . EDIT: I've actually made some progress with this since asking: I have solutions to b) and c), and have partial work for a). The struggle in part a) remains with constructing such a sequence of sets. Best I've come up with is , and in this case, with and . Problem here is that the countable union isn't the whole real line (it misses !). Similarly, I've also considered the sequence , whose countable union is all of , and are finite , but this sequence is not in .","\mathcal{F}=\{A\subseteq\mathbb{R} \ \vert \ 0\in A^{\mathrm{o}} \ or \ \ 0 \in (A^c)^{\mathrm{o}} \} A^{\mathrm{o}} A \mathcal{F} \sigma(\mathcal{F}) \mathbb{R} \gamma (\mathbb{R},\mathcal{P}(\mathbb{R})) \alpha(A)=\gamma(A \ \cap \ \{\frac{1}{n} \ \vert\ n \in \mathbb{N} \}) \beta(A)=\gamma(A \ \cap \ \{\frac{1}{n} \ \vert\ n \in \mathbb{N} \} \ \cup \  \{0\}) \mu := \left.\alpha \ \right|_{\sigma(\mathcal{F})} \nu := \left.\beta \ \right|_{\sigma(\mathcal{F})} \mu \nu \sigma(\mathcal{F}) \mu \nu \sigma \mu \nu \mathcal{F} \mu(A)=\nu(A) \forall A \in \mathcal{F} \mu \neq \nu(A) \sigma(\mathcal{F}) A\in \sigma(\mathcal{F}) (A_n)_{n=1}^{\infty} \sigma(\mathcal{F}) \bigcup\limits_{n=1}^{\infty}A_n = \mathbb{R} \mu(A_n)<\infty \nu(A_n)<\infty (A_k)_{k=1}^{\infty}=(-k,-\frac{1}{k}] \cup [\frac{1}{k},k) 0 \in (A_k^c)^{\mathrm{o}} \mu(A_k)<\infty \nu(A_k)<\infty \{0\} (A_k)_{k=1}^{\infty}=(-k,0] \cup [\frac{1}{k},k) \mathbb{R} \mu \nu \forall k \mathcal{F}","['real-analysis', 'analysis', 'measure-theory']"
94,Applications of the lack of compactness of the closed unit ball in infinite-dimensional Banach spaces,Applications of the lack of compactness of the closed unit ball in infinite-dimensional Banach spaces,,"I am writing a paper on the compactness of closed balls in Banach spaces, with particular attention paid to the following theorem Let $V$ be a Banach space over $\mathbb R$ or $\mathbb C$ . The closed unit ball in $V$ is compact if and only if $V$ is finite-dimensional. I am looking for some consequences or applications of this theorem (probably mostly related to the part which asserts that: if $V$ is infinite-dimensional, then the closed unit ball is not compact). I do prove the immediate corollary of this theorem, which is basically replacing ""the closed unit ball"" with ""the closed ball of radius $r>0$ around $x_0\in V$ "" in the statement of the theorem. I have also been looking at the notion of weak convergence, and how this can allow for compactness (in the weak sense) in infinite-dimensional spaces. Other than those two, I am looking for some other applications of this theorem. In particular, are there any specific interesting examples one can look at that follow from this theorem? Any feedback is appreciated.","I am writing a paper on the compactness of closed balls in Banach spaces, with particular attention paid to the following theorem Let be a Banach space over or . The closed unit ball in is compact if and only if is finite-dimensional. I am looking for some consequences or applications of this theorem (probably mostly related to the part which asserts that: if is infinite-dimensional, then the closed unit ball is not compact). I do prove the immediate corollary of this theorem, which is basically replacing ""the closed unit ball"" with ""the closed ball of radius around "" in the statement of the theorem. I have also been looking at the notion of weak convergence, and how this can allow for compactness (in the weak sense) in infinite-dimensional spaces. Other than those two, I am looking for some other applications of this theorem. In particular, are there any specific interesting examples one can look at that follow from this theorem? Any feedback is appreciated.",V \mathbb R \mathbb C V V V r>0 x_0\in V,"['functional-analysis', 'analysis', 'vector-spaces', 'compactness', 'examples-counterexamples']"
95,Cyclic Behavior of Iterated Simple Rational Functions,Cyclic Behavior of Iterated Simple Rational Functions,,"I noticed that for the (real) function $f(x)=\frac{1}{1-x}$ , $f(f(f(x)=f^3(x)=x$ for all real $x$ . This surprised me, and I was naturally curious about rational functions that elicit the identity after 4 or 5 or, in general, $n$ iterations. I looked at rational functions of the form $f(x)=\frac{1}{z-x}$ for $z\in\mathbb{R}$ and wondered for which $z$ did $f^n(x):=f(f(...f(f(x))...))=x$ . After playing around for a bit on Wolfram and Desmos, I came up with the following conjecture: If $n\in\mathbb{N}$ and $n\ge2$ and $f(x)=\frac{1}{2\cos(\frac{\pi}{n})-x}$ , then $f^n(x)=x$ . Note that I do not claim that $f$ is the only rational function of the form $f(x)=\frac{1}{z-x}$ where $z\in\mathbb{R}$ . So I have a few questions: 1) Is this conjecture true? 2) How would someone go about proving it, if it is true? 3) Is this a well-known theorem or a somewhat-immediate corollary or special case of a well-known theorem? If so, what is that theorem? 4) How would I find all $z\in\mathbb{R}$ such that if $f(x)=\frac{1}{z-x}$ , then $f^n(x)=x$ ? Is such a thing easy to do? 5) What field of math would ask questions like this?","I noticed that for the (real) function , for all real . This surprised me, and I was naturally curious about rational functions that elicit the identity after 4 or 5 or, in general, iterations. I looked at rational functions of the form for and wondered for which did . After playing around for a bit on Wolfram and Desmos, I came up with the following conjecture: If and and , then . Note that I do not claim that is the only rational function of the form where . So I have a few questions: 1) Is this conjecture true? 2) How would someone go about proving it, if it is true? 3) Is this a well-known theorem or a somewhat-immediate corollary or special case of a well-known theorem? If so, what is that theorem? 4) How would I find all such that if , then ? Is such a thing easy to do? 5) What field of math would ask questions like this?",f(x)=\frac{1}{1-x} f(f(f(x)=f^3(x)=x x n f(x)=\frac{1}{z-x} z\in\mathbb{R} z f^n(x):=f(f(...f(f(x))...))=x n\in\mathbb{N} n\ge2 f(x)=\frac{1}{2\cos(\frac{\pi}{n})-x} f^n(x)=x f f(x)=\frac{1}{z-x} z\in\mathbb{R} z\in\mathbb{R} f(x)=\frac{1}{z-x} f^n(x)=x,"['number-theory', 'analysis']"
96,If a sequence of measurable functions $f_{n}$ converge to $f$ almost everywhere then $f$ is measurable,If a sequence of measurable functions  converge to  almost everywhere then  is measurable,f_{n} f f,"$\textbf{Theorem}$ if a sequence of measurable functions $f_{n}$ converge to $f$ almost everywhere then $f$ is measurable $\textbf{proof}$ Let $A=\{ x\in X : \lim f_n(x)=f(x)\}$ . Since $f_n \to f$ a.e, it follows that $\mu(A^{c})=0$ . Thus $A^{c}$ is measurable and hence $A$ is measurable. Now, let $a\in \mathbb{R}$ . Observe that the equality $$A \cap f^{-1}((a,\infty)) =A \cap \left[\bigcup_{n=1}^{\infty} \bigcap_{i=n}^{\infty}\ f_{i}^{-1}\! \left(\left(a+\frac{1}{n},\infty\right)\right)\right]$$ I am not getting that how this above equality is coming? Please explain this with detailed arguments. I would be grateful for it.","if a sequence of measurable functions converge to almost everywhere then is measurable Let . Since a.e, it follows that . Thus is measurable and hence is measurable. Now, let . Observe that the equality I am not getting that how this above equality is coming? Please explain this with detailed arguments. I would be grateful for it.","\textbf{Theorem} f_{n} f f \textbf{proof} A=\{ x\in X : \lim f_n(x)=f(x)\} f_n \to f \mu(A^{c})=0 A^{c} A a\in \mathbb{R} A \cap f^{-1}((a,\infty)) =A \cap \left[\bigcup_{n=1}^{\infty} \bigcap_{i=n}^{\infty}\ f_{i}^{-1}\! \left(\left(a+\frac{1}{n},\infty\right)\right)\right]","['real-analysis', 'analysis', 'measure-theory', 'outer-measure']"
97,"For a two variable function to be differentiable, does its partial derivatives have to be continuous?","For a two variable function to be differentiable, does its partial derivatives have to be continuous?",,"I am currently doing an Analysis-Differential Forms module and one of the questions states: ""Give an informal interpretation of what it means for a function $f(x,y)$ to be differentiable at point $c=(c_1,c_2)$ "" The tutor gave this answer: Taking a neighbourhood about $f(c_1,c_2)$ . The function is differentiable if $f_x$ and $f_y$ (partial derivatives) are continuous and exist for all $x$ and $y$ values in that neighbourhood. My question is, does its partial derivatives have to be continuous so that $f(x,y)$ is differentiable?","I am currently doing an Analysis-Differential Forms module and one of the questions states: ""Give an informal interpretation of what it means for a function to be differentiable at point "" The tutor gave this answer: Taking a neighbourhood about . The function is differentiable if and (partial derivatives) are continuous and exist for all and values in that neighbourhood. My question is, does its partial derivatives have to be continuous so that is differentiable?","f(x,y) c=(c_1,c_2) f(c_1,c_2) f_x f_y x y f(x,y)","['analysis', 'derivatives']"
98,Wasserstein Distance with Translations,Wasserstein Distance with Translations,,"I am studying this book about Optimal Transport, and in Remark 2.19 it talks about translation in Variance, where it is stated that a nice property of Wasserstein Distances is the ability to factor out translations: $$\mathcal{W}_2 (T_{\tau\#}\alpha \; , \; T_{\tau'\#}\beta)^2= \mathcal{W}_2(\alpha,\beta)^2- 	2\langle \tau -\tau' \; , \; \mathbf{m}_\alpha - \mathbf{m}_\beta \rangle + \|\tau -\tau'\|^2$$ where $\mathbf{m}_\alpha \triangleq \int_\mathcal{X}xd\alpha(x) \in \mathbb{R}^d$ is the mean of a measure $\alpha$ . The proceeded to write that: $$	\mathcal{W}_2(\alpha,\beta)^2=\mathcal{W}_2(\bar{\alpha},\bar{\beta})^2+\|\mathbf{m}_\alpha - \mathbf{m}_\beta \|^2$$ Where $\mathcal{W}_2(\bar{\alpha},\bar{\beta})^2=\mathcal{W}_2 (T_{\tau\#}\alpha \; , \; T_{\tau'\#}\beta)^2$ . Now, I can't wrap my head around how the term $\|\mathbf{m}_\alpha - \mathbf{m}_\beta \|^2$ came to be in the equation. I've tried to trace backwards, but I got stuck. Here is my attempt: \begin{align} 		2 \langle \tau-\tau',\mathbf{m}_\alpha-\mathbf{m}_\beta\rangle  - \|\tau -\tau'\|^2 &= \|\mathbf{m}_\alpha-\mathbf{m}_\beta\|^2 \\ 		2 \langle \tau-\tau',\mathbf{m}_\alpha-\mathbf{m}_\beta\rangle  &= \|\tau -\tau'\|^2 + \|\mathbf{m}_\alpha-\mathbf{m}_\beta\|^2 \\ 		  \langle \tau-\tau',\mathbf{m}_\alpha-\mathbf{m}_\beta\rangle &= \frac{\|\tau -\tau'\|^2 + \|\mathbf{m}_\alpha-\mathbf{m}_\beta\|^2}{2} \end{align} I haven't studied analysis or measure theory, but I am trying to understand as much as I could. I thought that this is an inner product and that $\tau-\tau'$ is the difference between translations, which could be viewed as some vector, and similarly for the difference between the means of the discrete measures which I assume that they yield the center of the discrete measures $\alpha$ and $\beta$ . I am pretty sure something is wrong in my understanding, or at least my basics. So, my questions are: where did I go wrong? is that an inner product? what is the definition of this $\langle \cdot , \cdot \rangle$ notation? and what is the definition of these norms $\|\cdot\|^2$ in the context of the Wasserstein distances? Thanks EDIT: I am using $\triangleq$ as the equal with a ""def"" on top. EDIT: The purpose is self-study. I am not enrolled anywhere so it isn't a homework. EDIT: From Remark 2.19 in the book: $(\bar{\alpha},\bar{\beta})$ are the ""centered"" zero mean measures $\bar{\alpha}=T_{\mathbf{m}_\alpha \#} \alpha$","I am studying this book about Optimal Transport, and in Remark 2.19 it talks about translation in Variance, where it is stated that a nice property of Wasserstein Distances is the ability to factor out translations: where is the mean of a measure . The proceeded to write that: Where . Now, I can't wrap my head around how the term came to be in the equation. I've tried to trace backwards, but I got stuck. Here is my attempt: I haven't studied analysis or measure theory, but I am trying to understand as much as I could. I thought that this is an inner product and that is the difference between translations, which could be viewed as some vector, and similarly for the difference between the means of the discrete measures which I assume that they yield the center of the discrete measures and . I am pretty sure something is wrong in my understanding, or at least my basics. So, my questions are: where did I go wrong? is that an inner product? what is the definition of this notation? and what is the definition of these norms in the context of the Wasserstein distances? Thanks EDIT: I am using as the equal with a ""def"" on top. EDIT: The purpose is self-study. I am not enrolled anywhere so it isn't a homework. EDIT: From Remark 2.19 in the book: are the ""centered"" zero mean measures","\mathcal{W}_2 (T_{\tau\#}\alpha \; , \; T_{\tau'\#}\beta)^2= \mathcal{W}_2(\alpha,\beta)^2-
	2\langle \tau -\tau' \; , \; \mathbf{m}_\alpha - \mathbf{m}_\beta \rangle + \|\tau -\tau'\|^2 \mathbf{m}_\alpha \triangleq \int_\mathcal{X}xd\alpha(x) \in \mathbb{R}^d \alpha 	\mathcal{W}_2(\alpha,\beta)^2=\mathcal{W}_2(\bar{\alpha},\bar{\beta})^2+\|\mathbf{m}_\alpha - \mathbf{m}_\beta \|^2 \mathcal{W}_2(\bar{\alpha},\bar{\beta})^2=\mathcal{W}_2 (T_{\tau\#}\alpha \; , \; T_{\tau'\#}\beta)^2 \|\mathbf{m}_\alpha - \mathbf{m}_\beta \|^2 \begin{align}
		2 \langle \tau-\tau',\mathbf{m}_\alpha-\mathbf{m}_\beta\rangle  - \|\tau -\tau'\|^2 &= \|\mathbf{m}_\alpha-\mathbf{m}_\beta\|^2 \\
		2 \langle \tau-\tau',\mathbf{m}_\alpha-\mathbf{m}_\beta\rangle  &= \|\tau -\tau'\|^2 + \|\mathbf{m}_\alpha-\mathbf{m}_\beta\|^2 \\
		  \langle \tau-\tau',\mathbf{m}_\alpha-\mathbf{m}_\beta\rangle &= \frac{\|\tau -\tau'\|^2 + \|\mathbf{m}_\alpha-\mathbf{m}_\beta\|^2}{2}
\end{align} \tau-\tau' \alpha \beta \langle \cdot , \cdot \rangle \|\cdot\|^2 \triangleq (\bar{\alpha},\bar{\beta}) \bar{\alpha}=T_{\mathbf{m}_\alpha \#} \alpha","['analysis', 'measure-theory', 'metric-spaces', 'optimal-transport']"
99,"Does a ""full enough"" convex set have an internal point?","Does a ""full enough"" convex set have an internal point?",,"Let $C \subseteq V$ be a convex subset of a vector space (not necessarily finite dimensional). Suppose that for every $v \in V$ , there are some $a  \mathbb{R}^+$ and $x, y  C$ such that $v = a(x - y)$ . Intuitively, $C$ has some positive ""thickness"" in every direction, somewhere. Does it follow that $C$ has an internal point? Here's a bit more detail. For a vector $v  V$ , call a point $x  C$ $v$ -internal iff for some $ > 0$ , for every $0 <   $ , $x + v$ is in $C$ . Given that $C$ is convex, this is equivalent to just saying that $x + v  C$ for some $ > 0$ . A point $x  C$ is internal iff it is $v$ -internal for every $v  V$ . My question amounts to this: given that, for every $v  V$ , there is some $v$ -internal point $x  C$ , does it follow that there is a point $x  C$ which is $v$ -internal for every $v  V$ ? In the finite dimensional case this checks out. Given a basis $e, , e_n$ , and some points $x, , x_n, y, , y_n  C$ which are $e$ -internal, , $e_n$ -internal, $-e$ -internal, , and $-e_n$ -internal, respectively, the mean of all of these points must be an internal point. But I don't know how things would go in an infinite dimensional vector space.","Let be a convex subset of a vector space (not necessarily finite dimensional). Suppose that for every , there are some and such that . Intuitively, has some positive ""thickness"" in every direction, somewhere. Does it follow that has an internal point? Here's a bit more detail. For a vector , call a point -internal iff for some , for every , is in . Given that is convex, this is equivalent to just saying that for some . A point is internal iff it is -internal for every . My question amounts to this: given that, for every , there is some -internal point , does it follow that there is a point which is -internal for every ? In the finite dimensional case this checks out. Given a basis , and some points which are -internal, , -internal, -internal, , and -internal, respectively, the mean of all of these points must be an internal point. But I don't know how things would go in an infinite dimensional vector space.","C \subseteq V v \in V a  \mathbb{R}^+ x, y  C v = a(x - y) C C v  V x  C v  > 0 0 <    x + v C C x + v  C  > 0 x  C v v  V v  V v x  C x  C v v  V e, , e_n x, , x_n, y, , y_n  C e e_n -e -e_n","['real-analysis', 'linear-algebra', 'analysis', 'convex-analysis']"
