,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"How to show that $\lim_{\varepsilon \rightarrow 0^+} \int_0^\infty \frac{\varepsilon}{\varepsilon^2+x}\sin(1/x) \, dx=0$?",How to show that ?,"\lim_{\varepsilon \rightarrow 0^+} \int_0^\infty \frac{\varepsilon}{\varepsilon^2+x}\sin(1/x) \, dx=0","So I tried integration by parts and substitution but I couldn't bound the integral on the inside so that the limit would tend towards zero. Any hints on how to do this? Here is the work I have done so far \begin{align} \int_0^t \frac{\varepsilon}{\varepsilon^2+x} \sin(1/x) \, dx & = \int_0^t \frac{\varepsilon x}{\varepsilon^2+x^2}\sin(1/x^2) \, dx \\ &= \left.\arctan \left(\frac x {\varepsilon}\right) x\sin(1/x^2)\right|_0^t-\int_0^t\arctan \left(\frac x \varepsilon \right) \frac d {dx}(x\sin(1/x^2)) \, dx\\ &=-\int_0^t\arctan\left(\frac x \varepsilon \right)\frac{d}{dx}(x\sin(1/x^2)) \, dx \end{align}","So I tried integration by parts and substitution but I couldn't bound the integral on the inside so that the limit would tend towards zero. Any hints on how to do this? Here is the work I have done so far \begin{align} \int_0^t \frac{\varepsilon}{\varepsilon^2+x} \sin(1/x) \, dx & = \int_0^t \frac{\varepsilon x}{\varepsilon^2+x^2}\sin(1/x^2) \, dx \\ &= \left.\arctan \left(\frac x {\varepsilon}\right) x\sin(1/x^2)\right|_0^t-\int_0^t\arctan \left(\frac x \varepsilon \right) \frac d {dx}(x\sin(1/x^2)) \, dx\\ &=-\int_0^t\arctan\left(\frac x \varepsilon \right)\frac{d}{dx}(x\sin(1/x^2)) \, dx \end{align}",,"['calculus', 'real-analysis', 'integration', 'analysis', 'improper-integrals']"
1,Let $u \in C^2 (\Omega) \cap C (\bar \Omega) $ be a solution: $\Delta u = f(u)$ in $\Omega$ and $u=0$ on $\partial \Omega$. Show that $u \equiv 0$,Let  be a solution:  in  and  on . Show that,u \in C^2 (\Omega) \cap C (\bar \Omega)  \Delta u = f(u) \Omega u=0 \partial \Omega u \equiv 0,"Let $\Omega = \{ (x,y) \in \mathbb R^2: x^2 + y^2<1 \}$ and assume that $f: \mathbb R \rightarrow \mathbb R$ is a strictly increasing function with $f(0) =0$. Let $u \in C^2 (\Omega) \cap C (\bar \Omega) $ be a solution of the following problem: $$\Delta u = f(u)$$ in $\Omega$ and $u=0$ on $\partial \Omega$. Show that $u \equiv 0$. I have two ideas. The first is to multiply $u$ on the equation first and then integrate, which gives us $\int_{\Omega} |\nabla u|^2 = -\int_{\Omega} uf(u)$,but don't know how to use the condition of $f$. The second one is just integrate the equation which gives us $\int_{\Omega} f(u) = \int_{\partial \Omega} \frac{\partial u}{\partial n}dS$.","Let $\Omega = \{ (x,y) \in \mathbb R^2: x^2 + y^2<1 \}$ and assume that $f: \mathbb R \rightarrow \mathbb R$ is a strictly increasing function with $f(0) =0$. Let $u \in C^2 (\Omega) \cap C (\bar \Omega) $ be a solution of the following problem: $$\Delta u = f(u)$$ in $\Omega$ and $u=0$ on $\partial \Omega$. Show that $u \equiv 0$. I have two ideas. The first is to multiply $u$ on the equation first and then integrate, which gives us $\int_{\Omega} |\nabla u|^2 = -\int_{\Omega} uf(u)$,but don't know how to use the condition of $f$. The second one is just integrate the equation which gives us $\int_{\Omega} f(u) = \int_{\partial \Omega} \frac{\partial u}{\partial n}dS$.",,"['real-analysis', 'analysis', 'partial-differential-equations']"
2,Prove $\prod_1^\infty (1+p_n)$ converges,Prove  converges,\prod_1^\infty (1+p_n),"Let $p_{2n-1} = \frac{-1}{\sqrt{n}}$, and $p_{2n} = \frac{1}{n}+\frac{1}{\sqrt{n}}$. Prove $\prod_1^\infty (1+p_n)$ converges. By numerical simulations, it appears to converge (to something around $0.759$). However, I'm not sure how to prove this. I know we can skip the first term since it's $0$. Then we can write it in the following form. \begin{align*} \prod_1^\infty (1+p_n) &= \prod_1^\infty \left(1+\frac{1}{2n}+\frac{1}{\sqrt{2n}}\right)\left(1-\frac{1}{\sqrt{2n+1}}\right)  \\ &=  \left(1+\frac{1}{2}+\frac{1}{\sqrt{2}}\right)\left(1-\frac{1}{\sqrt{3}}\right)\left(1+\frac{1}{4}+\frac{1}{\sqrt{4}}\right)\left(1-\frac{1}{\sqrt{5}}\right)... \end{align*} Any thoughts?","Let $p_{2n-1} = \frac{-1}{\sqrt{n}}$, and $p_{2n} = \frac{1}{n}+\frac{1}{\sqrt{n}}$. Prove $\prod_1^\infty (1+p_n)$ converges. By numerical simulations, it appears to converge (to something around $0.759$). However, I'm not sure how to prove this. I know we can skip the first term since it's $0$. Then we can write it in the following form. \begin{align*} \prod_1^\infty (1+p_n) &= \prod_1^\infty \left(1+\frac{1}{2n}+\frac{1}{\sqrt{2n}}\right)\left(1-\frac{1}{\sqrt{2n+1}}\right)  \\ &=  \left(1+\frac{1}{2}+\frac{1}{\sqrt{2}}\right)\left(1-\frac{1}{\sqrt{3}}\right)\left(1+\frac{1}{4}+\frac{1}{\sqrt{4}}\right)\left(1-\frac{1}{\sqrt{5}}\right)... \end{align*} Any thoughts?",,"['real-analysis', 'analysis', 'infinite-product']"
3,Solve an inequality $|x| + |x-y| \geq |x_0| + |x_0-y| $ for $y$,Solve an inequality  for,|x| + |x-y| \geq |x_0| + |x_0-y|  y,"Fix $x_0 \in \mathbb R^n$. Describe $y \in \mathbb R^n$ that satisfies $|x| + |x-y| \geq |x_0| + |x_0-y|$ for all $x \in \mathbb R^n$. I have tried triangle inequalities (including reverse) repeatedly, but does not seem to work. How can I describe such $y$?","Fix $x_0 \in \mathbb R^n$. Describe $y \in \mathbb R^n$ that satisfies $|x| + |x-y| \geq |x_0| + |x_0-y|$ for all $x \in \mathbb R^n$. I have tried triangle inequalities (including reverse) repeatedly, but does not seem to work. How can I describe such $y$?",,"['real-analysis', 'linear-algebra', 'analysis', 'inequality', 'arithmetic']"
4,Dominated or Monotone convergence for $\dfrac{1+nx^2}{(1+x)^n}$,Dominated or Monotone convergence for,\dfrac{1+nx^2}{(1+x)^n},"I am having a particularly difficult time working with the sequence of functions given by: $$f_n(x)=\dfrac{1+nx^2}{(1+x)^n}.$$ I am working only on $[0,1]$. It is clear that these are measurable functions which go to $0$ pointwise, (though $f_n(0)=1\;\forall n\in\mathbb{N}$) and I would like to use either the Dominated or Monotone Convergence Theorems to show that: $$\lim\limits_{n\to\infty}\int\limits_0^1f_n(x)=0.$$ I know it is true based on the context in which I received the problem, but it is proving to be a tough nut to crack. I'm usually good at these. A little noodling around on Desmos shows that the sequence is in fact dominated on $[0,1]$ by any constant function greater than 1, and that for $n\geq 3$ it is decreasing on $[0,1]$. For $n=1$ or $n=2$, it has a minimum between $0$ and $1$. But these things are, for the most part, incredibly tedious to show rigorously. This was a question on a prior preliminary exam in my department, and was written to take a few minutes at most. Is there a quicker way? All these details have taken me almost an hour to write out!","I am having a particularly difficult time working with the sequence of functions given by: $$f_n(x)=\dfrac{1+nx^2}{(1+x)^n}.$$ I am working only on $[0,1]$. It is clear that these are measurable functions which go to $0$ pointwise, (though $f_n(0)=1\;\forall n\in\mathbb{N}$) and I would like to use either the Dominated or Monotone Convergence Theorems to show that: $$\lim\limits_{n\to\infty}\int\limits_0^1f_n(x)=0.$$ I know it is true based on the context in which I received the problem, but it is proving to be a tough nut to crack. I'm usually good at these. A little noodling around on Desmos shows that the sequence is in fact dominated on $[0,1]$ by any constant function greater than 1, and that for $n\geq 3$ it is decreasing on $[0,1]$. For $n=1$ or $n=2$, it has a minimum between $0$ and $1$. But these things are, for the most part, incredibly tedious to show rigorously. This was a question on a prior preliminary exam in my department, and was written to take a few minutes at most. Is there a quicker way? All these details have taken me almost an hour to write out!",,"['calculus', 'real-analysis', 'analysis']"
5,Why cadlag Processes have atmost countable discontinuities?,Why cadlag Processes have atmost countable discontinuities?,,"Hi Guys I was trying to understand a proof on mathstack exchange by @Lord_Farin and I cannot really follow the proof. Can some body help me here?(Please see my doubts in bold) Cardinality of set of discontinuities of cadlag functions It clearly suffices to prove that there are only countably many discontinuities on any interval $[0,n]$. The existence of left- and right-hand limits means that for all $x$ and for all $N$, there exists an $\epsilon_{x,N}$ with: $$\forall y,z \in B(x; \epsilon_{x,N}): (y-x)(z-x) > 0 \implies |f(x)-f(z)| <\frac1N$$ where the antecedent ensures that $y$ and $z$ are on the same side of $x$. Why is this last statement  even true? . It would be clear if it was continuous but the author is just using existence of left and right limits. Where do we even use the point $y$ By compactness of $[0,n]$, finitely many of the $B(x;\epsilon_{x,N})$ cover $[0,n]$. That is, except for finitely many points (the centers $x$ for the covering balls), we can be sure that no discontinuity of size bigger than $\frac1N$ exists in $[0,n]$. This last statement is not clear to me at all. I mean the existence of finitely many open balls with center $x$ tells me if I assume (which i don't really understand why) what is written above is true then at these finitely many points there are no jumps/discontinuities? Thus, for each $N$, the set: $$\left\{x \in [0,n]: \left|\lim_{\xi\to x^+}f(\xi) - \lim_{\xi\to x^-}f(\xi)\right| > \frac1N\right\}$$ is finite. It follows that the set of discontinuities on $[0,n]$: $$\left\{x \in [0,n]: \lim_{\xi\to x^+}f(\xi) \ne \lim_{\xi\to x^-}f(\xi)\right\} = \bigcup_{N \in\Bbb N}\left\{x \in [0,n]: \left|\lim_{\xi\to x^+}f(\xi) - \lim_{\xi\to x^-}f(\xi)\right| > \frac1N\right\}$$ is countable, and hence so is: $$\left\{x \in [0,\infty): \lim_{\xi\to x^+}f(\xi) \ne \lim_{\xi\to x^-}f(\xi)\right\} = \bigcup_{n \in \Bbb N}\left\{x \in [0,n]: \lim_{\xi\to x^+}f(\xi) \ne \lim_{\xi\to x^-}f(\xi)\right\}$$","Hi Guys I was trying to understand a proof on mathstack exchange by @Lord_Farin and I cannot really follow the proof. Can some body help me here?(Please see my doubts in bold) Cardinality of set of discontinuities of cadlag functions It clearly suffices to prove that there are only countably many discontinuities on any interval $[0,n]$. The existence of left- and right-hand limits means that for all $x$ and for all $N$, there exists an $\epsilon_{x,N}$ with: $$\forall y,z \in B(x; \epsilon_{x,N}): (y-x)(z-x) > 0 \implies |f(x)-f(z)| <\frac1N$$ where the antecedent ensures that $y$ and $z$ are on the same side of $x$. Why is this last statement  even true? . It would be clear if it was continuous but the author is just using existence of left and right limits. Where do we even use the point $y$ By compactness of $[0,n]$, finitely many of the $B(x;\epsilon_{x,N})$ cover $[0,n]$. That is, except for finitely many points (the centers $x$ for the covering balls), we can be sure that no discontinuity of size bigger than $\frac1N$ exists in $[0,n]$. This last statement is not clear to me at all. I mean the existence of finitely many open balls with center $x$ tells me if I assume (which i don't really understand why) what is written above is true then at these finitely many points there are no jumps/discontinuities? Thus, for each $N$, the set: $$\left\{x \in [0,n]: \left|\lim_{\xi\to x^+}f(\xi) - \lim_{\xi\to x^-}f(\xi)\right| > \frac1N\right\}$$ is finite. It follows that the set of discontinuities on $[0,n]$: $$\left\{x \in [0,n]: \lim_{\xi\to x^+}f(\xi) \ne \lim_{\xi\to x^-}f(\xi)\right\} = \bigcup_{N \in\Bbb N}\left\{x \in [0,n]: \left|\lim_{\xi\to x^+}f(\xi) - \lim_{\xi\to x^-}f(\xi)\right| > \frac1N\right\}$$ is countable, and hence so is: $$\left\{x \in [0,\infty): \lim_{\xi\to x^+}f(\xi) \ne \lim_{\xi\to x^-}f(\xi)\right\} = \bigcup_{n \in \Bbb N}\left\{x \in [0,n]: \lim_{\xi\to x^+}f(\xi) \ne \lim_{\xi\to x^-}f(\xi)\right\}$$",,"['calculus', 'analysis']"
6,$L^p$-norm of the gradient of a harmonic function.,-norm of the gradient of a harmonic function.,L^p,"Let $1<p<\infty$, $\Omega \subsetneq U \subset \mathbb{R}^n$ be bounded open sets in $\mathbb{R}^n$ with smooth boundary. The claim is that there is $c>0$ such that the following inequality holds $$ \| \nabla u \|_{L^p(\Omega^\prime)} \leq c \| u \|_{L^p(U)}$$  for all $u \in L^p(U)$ with $u$ being a weak solution of $\Delta u = 0$, I believe that it can be proved by the mean value property of harmonic functions that such a $u$ is actually smooth and so $\nabla u$ is defined in the usual sense. My question is how can we prove this inequality? The proof I'm reading just states that applying the mean value property to $\nabla u$ and so the $c$ exists. Also, I'm kind of suspicious of for the case $p \in (1,2)$ because I feel we might need to apply Jensen's inequality to $\frac{p}{2}$ somehow.","Let $1<p<\infty$, $\Omega \subsetneq U \subset \mathbb{R}^n$ be bounded open sets in $\mathbb{R}^n$ with smooth boundary. The claim is that there is $c>0$ such that the following inequality holds $$ \| \nabla u \|_{L^p(\Omega^\prime)} \leq c \| u \|_{L^p(U)}$$  for all $u \in L^p(U)$ with $u$ being a weak solution of $\Delta u = 0$, I believe that it can be proved by the mean value property of harmonic functions that such a $u$ is actually smooth and so $\nabla u$ is defined in the usual sense. My question is how can we prove this inequality? The proof I'm reading just states that applying the mean value property to $\nabla u$ and so the $c$ exists. Also, I'm kind of suspicious of for the case $p \in (1,2)$ because I feel we might need to apply Jensen's inequality to $\frac{p}{2}$ somehow.",,"['real-analysis', 'analysis', 'partial-differential-equations', 'harmonic-analysis']"
7,"$f: \mathbb{R} \rightarrow \mathbb{R}, f(0)=1, f(x+y) \leq f(x)f(y)$. Show that if $f$ continuous in $x=0$, $f$ is continuous in $\mathbb{R}$.",". Show that if  continuous in ,  is continuous in .","f: \mathbb{R} \rightarrow \mathbb{R}, f(0)=1, f(x+y) \leq f(x)f(y) f x=0 f \mathbb{R}","So far I have shown that the equality only holds when $x=0$ or $y=0$. I also have found out that $f(nx) \leq f(x)^n$ for natural $n$ (otherwise the summation doesn't make sense). But I do not know how to deduce the continuity.  My idea is to show it with the sequence criterion for continuity, but the only thing I've shown with it is that for $n \rightarrow 0$ the function $f(nx) \rightarrow f(0)=1$, which doesn't help a lot. I also could use that $f$ is continuous in $x=0$ to make any $f(x)$ a product of $f(1)$, but it only holds for natural numbers, thus not leading to a solution. I would appreciate hints, not solutions. Thank you for help.","So far I have shown that the equality only holds when $x=0$ or $y=0$. I also have found out that $f(nx) \leq f(x)^n$ for natural $n$ (otherwise the summation doesn't make sense). But I do not know how to deduce the continuity.  My idea is to show it with the sequence criterion for continuity, but the only thing I've shown with it is that for $n \rightarrow 0$ the function $f(nx) \rightarrow f(0)=1$, which doesn't help a lot. I also could use that $f$ is continuous in $x=0$ to make any $f(x)$ a product of $f(1)$, but it only holds for natural numbers, thus not leading to a solution. I would appreciate hints, not solutions. Thank you for help.",,['analysis']
8,"A convergence problem in real non-negative sequence, $\sum_{n=1}^\infty a_n$.","A convergence problem in real non-negative sequence, .",\sum_{n=1}^\infty a_n,"We now have $a_n\geq 0$, $\forall n=1,2,...,$ and $\sum_{n=1}^\infty a_n <\infty$. Then I guess  that $\lim_{n\to\infty} a_n \cdot n = 0$. But I realized that it is wrong. Since if we let $a_n = 1/n $ if $n = 2^i$ for some $i=1,2,...$ and $a_n = 0$ for the rest of the $n$. Then we have that $\sum_{n=1}^\infty a_n = 1/2 + 1/4 + 1/8 + \cdots < \infty$, and $a_n\cdot n$ does not converges to $0$. Now I add another condition that $a_n$ is non-increasing. Does this result hold this time. i.e. the formal question is as follows: $a_n\geq 0$, $\forall n=1,2,...,$ and $a_n$ is non-increasing, and  $\sum_{n=1}^\infty a_n <\infty$. Then prove $a_n \cdot n \to 0$, or give a counterexample that $a_n\cdot n$ does not necessarily converge to $0$","We now have $a_n\geq 0$, $\forall n=1,2,...,$ and $\sum_{n=1}^\infty a_n <\infty$. Then I guess  that $\lim_{n\to\infty} a_n \cdot n = 0$. But I realized that it is wrong. Since if we let $a_n = 1/n $ if $n = 2^i$ for some $i=1,2,...$ and $a_n = 0$ for the rest of the $n$. Then we have that $\sum_{n=1}^\infty a_n = 1/2 + 1/4 + 1/8 + \cdots < \infty$, and $a_n\cdot n$ does not converges to $0$. Now I add another condition that $a_n$ is non-increasing. Does this result hold this time. i.e. the formal question is as follows: $a_n\geq 0$, $\forall n=1,2,...,$ and $a_n$ is non-increasing, and  $\sum_{n=1}^\infty a_n <\infty$. Then prove $a_n \cdot n \to 0$, or give a counterexample that $a_n\cdot n$ does not necessarily converge to $0$",,['analysis']
9,Examples of short maps (Lipschitz functions with $k=1$) with exactly 2 fixed points.,Examples of short maps (Lipschitz functions with ) with exactly 2 fixed points.,k=1,"I was just reading about the Banach fixed point theorem, which states that a contraction (a function $f$ satisfying $|f(x)-f(y)|\leq k|x-y|$ for $0<k<1$) has a unique fixed point. If we have $k=1$ then $f$ is called a short map or a non-expansive map. I'm wondering how many fixed points such functions can have. Clearly if $f$ is the identity function then every point is a fixed point. If $f$ just translates points then there are no fixed points. Does there exist a function $f\colon\mathbb{R}\to\mathbb{R}$ satisfying $|f(x)-f(y)|\leq|x-y|$, which has exactly two fixed points? If not are there examples of short maps involving different metric spaces with exactly two fixed points?","I was just reading about the Banach fixed point theorem, which states that a contraction (a function $f$ satisfying $|f(x)-f(y)|\leq k|x-y|$ for $0<k<1$) has a unique fixed point. If we have $k=1$ then $f$ is called a short map or a non-expansive map. I'm wondering how many fixed points such functions can have. Clearly if $f$ is the identity function then every point is a fixed point. If $f$ just translates points then there are no fixed points. Does there exist a function $f\colon\mathbb{R}\to\mathbb{R}$ satisfying $|f(x)-f(y)|\leq|x-y|$, which has exactly two fixed points? If not are there examples of short maps involving different metric spaces with exactly two fixed points?",,"['real-analysis', 'analysis', 'metric-spaces']"
10,"Statements that say in another way that ""$S$ is an open set""","Statements that say in another way that "" is an open set""",S,"Suppose that we speak about open sets in $\mathbb R^{n}$ and that they are defined in the usual way with the help of open balls. I know that these statements are another way of saying that ""$S$ is an open set"": 1) The complement of $S$ is closed 2) $S ∩ ∂S = ∅$ 3) each point $s \in S$ is an interior point I would like to know are there any other widely (or not widely) known statements that describe open sets in another way, like the mentioned 1), 2), and 3)? You can add more than one if you know about more than one.","Suppose that we speak about open sets in $\mathbb R^{n}$ and that they are defined in the usual way with the help of open balls. I know that these statements are another way of saying that ""$S$ is an open set"": 1) The complement of $S$ is closed 2) $S ∩ ∂S = ∅$ 3) each point $s \in S$ is an interior point I would like to know are there any other widely (or not widely) known statements that describe open sets in another way, like the mentioned 1), 2), and 3)? You can add more than one if you know about more than one.",,"['analysis', 'elementary-set-theory']"
11,Infimum of distance between point and (closed) set,Infimum of distance between point and (closed) set,,"I'm having a little trouble with the following exercise: Let $V \subset\mathbb{R}^p$ be a non-empty, closed set and $a \in \mathbb{R}^p$. For $x, y \in \mathbb{R}^p$ we note $d(x,y) = \|x-y\|$. Furthermore, let $d(a,V) = \inf \{\, d(a,x) \mid x \in V \,\}$. Show that there exists $b \in V$ such that $d(a,V) = d(a,b)$. Hint: consider the set $V \cap \overline{B(a; R)}$ for a suitably chosen $R > 0$. Now I know that $a \in V \Leftrightarrow d(a,V) = 0$, so if $a \in V$ we can simply choose $b = a$. However, I am not quite sure how to prove this statement for $a \notin V$, and I don't really know how to use the hint that was given. I already looked at similar questions, but they all include theorems about compact spaces, and in this exercise $V$ isn't necessarily compact (or so I think). Any hints would be very much appreciated.","I'm having a little trouble with the following exercise: Let $V \subset\mathbb{R}^p$ be a non-empty, closed set and $a \in \mathbb{R}^p$. For $x, y \in \mathbb{R}^p$ we note $d(x,y) = \|x-y\|$. Furthermore, let $d(a,V) = \inf \{\, d(a,x) \mid x \in V \,\}$. Show that there exists $b \in V$ such that $d(a,V) = d(a,b)$. Hint: consider the set $V \cap \overline{B(a; R)}$ for a suitably chosen $R > 0$. Now I know that $a \in V \Leftrightarrow d(a,V) = 0$, so if $a \in V$ we can simply choose $b = a$. However, I am not quite sure how to prove this statement for $a \notin V$, and I don't really know how to use the hint that was given. I already looked at similar questions, but they all include theorems about compact spaces, and in this exercise $V$ isn't necessarily compact (or so I think). Any hints would be very much appreciated.",,"['analysis', 'supremum-and-infimum']"
12,Convolution of two gaussian functions,Convolution of two gaussian functions,,"I want to calculate the convolution $F * G$ of two Gaussian functions without resorting to Fouritertransforms: $F(t) := \exp(-at^2), G(t) := \exp(-bt^2) \qquad a,b>0$ But intuitively I expected the convolution to result again in a non constant function. Can anyone find my mistake / confirm that this calculation is correct? Let $\Omega = \mathbb R$, then $\begin{align*} (F*G)(x) &= \int_\Omega F(t)G(x-t)dt &\\ & = \int_\Omega e^{-at^2-b(x-t)^2} dt \qquad\qquad \quad  \text{substitute }u = t+\frac{1}{2} x \implies ""du=dt"" \\ &=\int_\Omega e^{-a(u-\frac{1}{2}x)^2-b(\frac{1}{2}x-u)^2}dt \qquad \text{substitute }v = u-\frac{1}{2} x \implies ""du=dv""\\ &=\int_\Omega e^{-(a+b)v^2} dv \qquad\qquad \qquad\,\, \text{substitute } w = \sqrt{a+b}v \implies""dw = \sqrt{a+b}dv"" \\ &=\frac{1}{\sqrt{a+b}}\int_\Omega e^{-w^2}dw \\ &=\frac{\sqrt{\pi}}{\sqrt{a+b}} \end{align*}$","I want to calculate the convolution $F * G$ of two Gaussian functions without resorting to Fouritertransforms: $F(t) := \exp(-at^2), G(t) := \exp(-bt^2) \qquad a,b>0$ But intuitively I expected the convolution to result again in a non constant function. Can anyone find my mistake / confirm that this calculation is correct? Let $\Omega = \mathbb R$, then $\begin{align*} (F*G)(x) &= \int_\Omega F(t)G(x-t)dt &\\ & = \int_\Omega e^{-at^2-b(x-t)^2} dt \qquad\qquad \quad  \text{substitute }u = t+\frac{1}{2} x \implies ""du=dt"" \\ &=\int_\Omega e^{-a(u-\frac{1}{2}x)^2-b(\frac{1}{2}x-u)^2}dt \qquad \text{substitute }v = u-\frac{1}{2} x \implies ""du=dv""\\ &=\int_\Omega e^{-(a+b)v^2} dv \qquad\qquad \qquad\,\, \text{substitute } w = \sqrt{a+b}v \implies""dw = \sqrt{a+b}dv"" \\ &=\frac{1}{\sqrt{a+b}}\int_\Omega e^{-w^2}dw \\ &=\frac{\sqrt{\pi}}{\sqrt{a+b}} \end{align*}$",,"['real-analysis', 'analysis', 'probability-distributions', 'convolution']"
13,On the inner metric of loops in space,On the inner metric of loops in space,,"An injective Lipschitz map $f \colon S^{1} \to \mathbb{R}^n$produces a loop $M:=f(S^{1})$. The inner metric $d_M$ on $M$ is defined by $d_M(x,y):=\inf L(\gamma)$ over all curves $\gamma\subset M$ connecting $x$ with $y$. It is claimed that $$\sup\left\{{d_M(x,y)\over \|x-y\|}\>\biggm| x,y\in M, \ x\ne y\right\}\geq{\pi\over2}\ ,$$ whereby $\|x-y\|$ denotes euclidean distance in ${\mathbb R}^n$. I tried hard resolve this issue and I hope that you can help me. Thank you.","An injective Lipschitz map $f \colon S^{1} \to \mathbb{R}^n$produces a loop $M:=f(S^{1})$. The inner metric $d_M$ on $M$ is defined by $d_M(x,y):=\inf L(\gamma)$ over all curves $\gamma\subset M$ connecting $x$ with $y$. It is claimed that $$\sup\left\{{d_M(x,y)\over \|x-y\|}\>\biggm| x,y\in M, \ x\ne y\right\}\geq{\pi\over2}\ ,$$ whereby $\|x-y\|$ denotes euclidean distance in ${\mathbb R}^n$. I tried hard resolve this issue and I hope that you can help me. Thank you.",,['analysis']
14,"Integrals of the form $\int^1_{0} x^\alpha(1-x)^\beta dx$ where $\alpha,\beta \in \mathbb{R}$.",Integrals of the form  where .,"\int^1_{0} x^\alpha(1-x)^\beta dx \alpha,\beta \in \mathbb{R}","Let $\alpha,\beta \in \mathbb{R}$. I was wondering if there is a systematic way to solve integrals of the following form: \begin{equation} \int^1_{0} x^\alpha(1-x)^\beta dx \end{equation} I have seen similar kind of integrals many times. Any help/hints would be really appreciated. Thanks!","Let $\alpha,\beta \in \mathbb{R}$. I was wondering if there is a systematic way to solve integrals of the following form: \begin{equation} \int^1_{0} x^\alpha(1-x)^\beta dx \end{equation} I have seen similar kind of integrals many times. Any help/hints would be really appreciated. Thanks!",,"['integration', 'analysis']"
15,Is there anything wrong with this definition of discontinuity?,Is there anything wrong with this definition of discontinuity?,,"Is there anything wrong with this definition of discontinuity for a function y = f(x)? $\forall \delta>0\, \exists \varepsilon>0$ such that $\vert x-c\vert < \delta$, but $\vert f(x) - f(c)\vert > \epsilon$.","Is there anything wrong with this definition of discontinuity for a function y = f(x)? $\forall \delta>0\, \exists \varepsilon>0$ such that $\vert x-c\vert < \delta$, but $\vert f(x) - f(c)\vert > \epsilon$.",,"['analysis', 'definition', 'epsilon-delta']"
16,"Showing that $x^2+\frac{1}{4} > x$ on $(-\frac{1}{2}, \frac{1}{2})$",Showing that  on,"x^2+\frac{1}{4} > x (-\frac{1}{2}, \frac{1}{2})","I don't recall having ever done a proof like this analytically, so I would appreciate your advice. Here's what I did, and it seems to be a correct approach, but there could be a better one. Let $x:=\frac{1}{2}-\varepsilon$ $\\$   (for $0<\varepsilon <1$). Now $f(x)=x^2+\frac{1}{4}=(\frac{1}{2}-\varepsilon)^2+\frac{1}{4}=\frac{1}{2}-\varepsilon+\varepsilon^2>\frac{1}{2}-\varepsilon=x$.","I don't recall having ever done a proof like this analytically, so I would appreciate your advice. Here's what I did, and it seems to be a correct approach, but there could be a better one. Let $x:=\frac{1}{2}-\varepsilon$ $\\$   (for $0<\varepsilon <1$). Now $f(x)=x^2+\frac{1}{4}=(\frac{1}{2}-\varepsilon)^2+\frac{1}{4}=\frac{1}{2}-\varepsilon+\varepsilon^2>\frac{1}{2}-\varepsilon=x$.",,"['real-analysis', 'analysis']"
17,"$n$-th derivative of $(x^2-1)^n$ has distinct real roots in $[-1,1]$.",-th derivative of  has distinct real roots in .,"n (x^2-1)^n [-1,1]","For $n=1,2,3,\ldots$, let $$f(x) = (x^2-1)^n .$$ Show that the $n$-th derivative $f^{(n)}$ has distinct real roots in $[-1,1]$. I have no idea about the problem. Could I have a hint?","For $n=1,2,3,\ldots$, let $$f(x) = (x^2-1)^n .$$ Show that the $n$-th derivative $f^{(n)}$ has distinct real roots in $[-1,1]$. I have no idea about the problem. Could I have a hint?",,"['analysis', 'derivatives']"
18,What is the difference between single and double layer potential,What is the difference between single and double layer potential,,I want to know the difference between single layer and double layer potentials. Is there a  link between the choice of   single/double  layer  potential  and the  boundary  condition  of a PDE or an homogeneity?,I want to know the difference between single layer and double layer potentials. Is there a  link between the choice of   single/double  layer  potential  and the  boundary  condition  of a PDE or an homogeneity?,,"['analysis', 'partial-differential-equations', 'boundary-value-problem', 'potential-theory']"
19,"For $n \geq 2$, $\mathbb{R}^n$ and an $n$-dimensional sphere are path-connected","For ,  and an -dimensional sphere are path-connected",n \geq 2 \mathbb{R}^n n,"Let $n \in \mathbb{N}$, $n ≥ 2$. I want to prove that a) $\mathbb{R}^n \setminus \{0\}$ is path-connected. b) the sphere $S^{n-1} := \{x \in \mathbb{R}^n: ||x||_2 = 1\}$ is path-connected. For a), I would need to find a path that connects any two points $x, y \in \mathbb{R}^n$. In case the path that leads directly from $x$ to $y$ doesn't contain $0$, this would be a connection. But if the direct path does contain $0$, I would need to find another way. Now it's easy to imagine that there are plenty of curves from $x$ to $y$ that don't touch $0$, but what would be a most elegant path where it's easy to see/prove that it's indeed one?","Let $n \in \mathbb{N}$, $n ≥ 2$. I want to prove that a) $\mathbb{R}^n \setminus \{0\}$ is path-connected. b) the sphere $S^{n-1} := \{x \in \mathbb{R}^n: ||x||_2 = 1\}$ is path-connected. For a), I would need to find a path that connects any two points $x, y \in \mathbb{R}^n$. In case the path that leads directly from $x$ to $y$ doesn't contain $0$, this would be a connection. But if the direct path does contain $0$, I would need to find another way. Now it's easy to imagine that there are plenty of curves from $x$ to $y$ that don't touch $0$, but what would be a most elegant path where it's easy to see/prove that it's indeed one?",,"['real-analysis', 'analysis', 'curves']"
20,How to prove the elementary inequality?,How to prove the elementary inequality?,,"The inequality is the following: $$\frac{(1+x)^q-1}{x+x^q} \leq C(q),$$ where $q\in [1,+\infty)$  and $x > 0$, and the constant $C$ depends only on $q$. It's very nice if someone can provide the minimal value of $C(q)$, I guess the minimal value is $q$.","The inequality is the following: $$\frac{(1+x)^q-1}{x+x^q} \leq C(q),$$ where $q\in [1,+\infty)$  and $x > 0$, and the constant $C$ depends only on $q$. It's very nice if someone can provide the minimal value of $C(q)$, I guess the minimal value is $q$.",,['analysis']
21,Find an example of a sequence not in $l^1$ satisfying certain boundedness conditions.,Find an example of a sequence not in  satisfying certain boundedness conditions.,l^1,"This question is about getting a concrete example for this question on bounded holomorphic functions posed by @user122916 (something that he really expected as explained in the comments). Give an example of a sequence of complex numbers $(a_n)_{n\ge 0}$ so that  \begin{eqnarray} |\sum_{n\ge 0} {a_n z^n} | &\le &1  \text{ for all }z \in \mathbb{C}, |z| < 1 \\ \sum_{n\ge 0} |a_n| &=& \infty \end{eqnarray} Such sequences exist because there exist bounded holomorphic functions on the unit disk that do not have a continuous extension to the unit circle ( one finds a bounded Blaschke product with zero set that contains the unit circle in its closure). However, a concrete example escapes me. Note that all this is part of the theory of $H^{\infty}$ space, so the specialists might have one at hand.","This question is about getting a concrete example for this question on bounded holomorphic functions posed by @user122916 (something that he really expected as explained in the comments). Give an example of a sequence of complex numbers $(a_n)_{n\ge 0}$ so that  \begin{eqnarray} |\sum_{n\ge 0} {a_n z^n} | &\le &1  \text{ for all }z \in \mathbb{C}, |z| < 1 \\ \sum_{n\ge 0} |a_n| &=& \infty \end{eqnarray} Such sequences exist because there exist bounded holomorphic functions on the unit disk that do not have a continuous extension to the unit circle ( one finds a bounded Blaschke product with zero set that contains the unit circle in its closure). However, a concrete example escapes me. Note that all this is part of the theory of $H^{\infty}$ space, so the specialists might have one at hand.",,['analysis']
22,How can I prove Holder inequality for $0<p<1$? [closed],How can I prove Holder inequality for ? [closed],0<p<1,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved Improve this question Let $0<p<1$ snd $\dfrac{1}{p}+\dfrac{1}{p'}=1$ . Notice that $p'<0$ . If $f \in L^{p}$ and $0<\int_{\Omega}\vert g(x) \vert^{p'}dx < \infty$ then, $$\int_{\Omega}\vert f(x)g(x) \vert dx \geq (\int_{\Omega}\vert f(x) \vert^{p}dx)^{\frac{1}{p}}(\int_{\Omega}\vert g(x) \vert^{p'}dx)^{\frac{1}{p'}} $$ The assumption on $g$ implies that $g>0$ almost surely.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved Improve this question Let snd . Notice that . If and then, The assumption on implies that almost surely.",0<p<1 \dfrac{1}{p}+\dfrac{1}{p'}=1 p'<0 f \in L^{p} 0<\int_{\Omega}\vert g(x) \vert^{p'}dx < \infty \int_{\Omega}\vert f(x)g(x) \vert dx \geq (\int_{\Omega}\vert f(x) \vert^{p}dx)^{\frac{1}{p}}(\int_{\Omega}\vert g(x) \vert^{p'}dx)^{\frac{1}{p'}}  g g>0,"['analysis', 'lebesgue-integral']"
23,to show $\int_{-\infty}^{\infty} x^{2n}e^{-x^2}dx = (2n)!{\sqrt{\pi}}/4^nn!$,to show,\int_{-\infty}^{\infty} x^{2n}e^{-x^2}dx = (2n)!{\sqrt{\pi}}/4^nn!,I was trying to show  the following  $\int_{-\infty}^{\infty} x^{2n}e^{-x^2}dx = (2n)!{\sqrt{\pi}}/4^nn!$ by using $\int_{-\infty}^{\infty} e^{-tx^2}dx = \sqrt{\pi/t}$  thus I differentiated this exponential integral n times to get the following. $\int_{-\infty}^{\infty} \frac{d^ne^{-tx^2}}{dt^n}dx $$=\frac{2^{n}\times \sqrt{\pi}t^{\frac{2n-1}{2}}} {1\times 3\times 5 \times ... \times (2n-1)}$ after applying limit for $t\rightarrow 1} I am not getting the desired result. Where am I going wrong ? Thanks,I was trying to show  the following  $\int_{-\infty}^{\infty} x^{2n}e^{-x^2}dx = (2n)!{\sqrt{\pi}}/4^nn!$ by using $\int_{-\infty}^{\infty} e^{-tx^2}dx = \sqrt{\pi/t}$  thus I differentiated this exponential integral n times to get the following. $\int_{-\infty}^{\infty} \frac{d^ne^{-tx^2}}{dt^n}dx $$=\frac{2^{n}\times \sqrt{\pi}t^{\frac{2n-1}{2}}} {1\times 3\times 5 \times ... \times (2n-1)}$ after applying limit for $t\rightarrow 1} I am not getting the desired result. Where am I going wrong ? Thanks,,"['integration', 'analysis', 'measure-theory']"
24,A question about the integral form of the Cauchy-Schwarz inequality.,A question about the integral form of the Cauchy-Schwarz inequality.,,"I'm confused about the following form of the Cauchy-Schwarz inequality: $$\int{f(x)g(x)} dx\leq \sqrt{\int{f(x)^2dx}}\sqrt{\int{g(x)^2dx}} \tag{A}$$ An analogous form for the inequality is $(ac+bd)\leq (a^2+b^2)(c^2+d^2)$. Assuming that the integral inequality above is the same in spirit as this, we should have something like $$\begin{align}f(x_1)g(x_1)&+f(x_2)g(x_2)+\dots f(x_n)g(x_n)\\&\leq \sqrt{f(x_1)^2+f(x_2)^2+\dots+f(x_n)^2}+\sqrt{g(x_1)^2+g(x_2)^2+\dots+g(x_n)^2}\end{align}$$ Extending the argument to an infinite number of $x_i$'s, we still do not get A. This is because it is not like if we had an infinite number of $x_i$'s on the interval $[a,b]$, we'd have $$f(x_1)+f(x_2)+\dots=\int_a^b{f(x)dx}$$ By my understanding, $\int{f(x)dx}$  is not the summation of $f(x_i)$. It is the summation of $\lim\limits_{\Delta x\to\infty}f(x_i)\Delta x$. Where am I going wrong?","I'm confused about the following form of the Cauchy-Schwarz inequality: $$\int{f(x)g(x)} dx\leq \sqrt{\int{f(x)^2dx}}\sqrt{\int{g(x)^2dx}} \tag{A}$$ An analogous form for the inequality is $(ac+bd)\leq (a^2+b^2)(c^2+d^2)$. Assuming that the integral inequality above is the same in spirit as this, we should have something like $$\begin{align}f(x_1)g(x_1)&+f(x_2)g(x_2)+\dots f(x_n)g(x_n)\\&\leq \sqrt{f(x_1)^2+f(x_2)^2+\dots+f(x_n)^2}+\sqrt{g(x_1)^2+g(x_2)^2+\dots+g(x_n)^2}\end{align}$$ Extending the argument to an infinite number of $x_i$'s, we still do not get A. This is because it is not like if we had an infinite number of $x_i$'s on the interval $[a,b]$, we'd have $$f(x_1)+f(x_2)+\dots=\int_a^b{f(x)dx}$$ By my understanding, $\int{f(x)dx}$  is not the summation of $f(x_i)$. It is the summation of $\lim\limits_{\Delta x\to\infty}f(x_i)\Delta x$. Where am I going wrong?",,"['real-analysis', 'analysis']"
25,"Let $\{a_n\}$ be a sequence with limit $\alpha$, and define $b_n=a_{n+1}$ where $n\in \mathbb{N}$. Show that $\{b_n\}\rightarrow \alpha$.","Let  be a sequence with limit , and define  where . Show that .",\{a_n\} \alpha b_n=a_{n+1} n\in \mathbb{N} \{b_n\}\rightarrow \alpha,"Let $\{a_n\}$ be a sequence with limit $\alpha$, and define $b_n=a_{n+1}$ where $n\in \mathbb{N}$. Show that $\{b_n\}\rightarrow \alpha$. What I have: Since $\{a_n\}\rightarrow \alpha$ we know that for all $\epsilon>0$ there exists $N\in \mathbb{N}$ such that $|a_n-\alpha|<\epsilon$ for all $n\geq N$. I'm stuck trying to figure the rest out. Any help is greatly appreciated, thank you.","Let $\{a_n\}$ be a sequence with limit $\alpha$, and define $b_n=a_{n+1}$ where $n\in \mathbb{N}$. Show that $\{b_n\}\rightarrow \alpha$. What I have: Since $\{a_n\}\rightarrow \alpha$ we know that for all $\epsilon>0$ there exists $N\in \mathbb{N}$ such that $|a_n-\alpha|<\epsilon$ for all $n\geq N$. I'm stuck trying to figure the rest out. Any help is greatly appreciated, thank you.",,"['calculus', 'real-analysis', 'analysis']"
26,Inequality involving $\frac{\sin x}{x}$,Inequality involving,\frac{\sin x}{x},"Can anybody explain me, why the following inequality is true? $$\sum_{k=0}^{\infty} \int_{k \pi + \frac{\pi}{4}}^{(k+1)\pi-\frac{\pi}{4}} \left| \frac{\sin \xi}{\xi} \right| \, \text{d} \xi \geq  \sum_{k=0}^{\infty} \frac{\left| \sin\left( (k+1) \pi - \frac{\pi}{4} \right)\right|}{(k+1) \pi - \frac{\pi}{4}} \  \frac{\pi}{2} $$ The question is motivated from the following calculation $$ \sum_{k=0}^{\infty} \int_{k \pi}^{(k+1) \pi} \left| \frac{\sin \xi}{\xi} \right| \, \text{d} \xi \geq \sum_{k=0}^{\infty} \int_{k \pi + \frac{\pi}{4}}^{(k+1)\pi-\frac{\pi}{4}} \left| \frac{\sin \xi}{\xi} \right| \, \text{d} \xi \geq \\ \geq \sum_{k=0}^{\infty} \frac{\left| \sin\left( (k+1) \pi - \frac{\pi}{4} \right)\right|}{(k+1) \pi - \frac{\pi}{4}} \  \frac{\pi}{2} =  \sum_{k=0}^{\infty} \frac{\left| \sin\left( (k+1) \pi - \frac{\pi}{4} \right)\right|}{2(k+1)  - \frac{1}{2}} = \\ = \sum_{k=0}^{\infty}  \frac{\sqrt{2}}{2} \frac{1}{2(k+1)  - \frac{1}{2}} = \sum_{k=0}^{\infty} \frac{\sqrt{2}}{4k+3} \geq \sum_{k=0}^{\infty} \frac{\sqrt{2}}{4k} = \\ = \frac{\sqrt{2}}{4}\sum_{k=0}^{\infty} \frac{1}{k} = \infty $$ which shows that $\frac{\sin \xi}{\xi} \notin \mathcal{L}^1(\mathbb{R})$.","Can anybody explain me, why the following inequality is true? $$\sum_{k=0}^{\infty} \int_{k \pi + \frac{\pi}{4}}^{(k+1)\pi-\frac{\pi}{4}} \left| \frac{\sin \xi}{\xi} \right| \, \text{d} \xi \geq  \sum_{k=0}^{\infty} \frac{\left| \sin\left( (k+1) \pi - \frac{\pi}{4} \right)\right|}{(k+1) \pi - \frac{\pi}{4}} \  \frac{\pi}{2} $$ The question is motivated from the following calculation $$ \sum_{k=0}^{\infty} \int_{k \pi}^{(k+1) \pi} \left| \frac{\sin \xi}{\xi} \right| \, \text{d} \xi \geq \sum_{k=0}^{\infty} \int_{k \pi + \frac{\pi}{4}}^{(k+1)\pi-\frac{\pi}{4}} \left| \frac{\sin \xi}{\xi} \right| \, \text{d} \xi \geq \\ \geq \sum_{k=0}^{\infty} \frac{\left| \sin\left( (k+1) \pi - \frac{\pi}{4} \right)\right|}{(k+1) \pi - \frac{\pi}{4}} \  \frac{\pi}{2} =  \sum_{k=0}^{\infty} \frac{\left| \sin\left( (k+1) \pi - \frac{\pi}{4} \right)\right|}{2(k+1)  - \frac{1}{2}} = \\ = \sum_{k=0}^{\infty}  \frac{\sqrt{2}}{2} \frac{1}{2(k+1)  - \frac{1}{2}} = \sum_{k=0}^{\infty} \frac{\sqrt{2}}{4k+3} \geq \sum_{k=0}^{\infty} \frac{\sqrt{2}}{4k} = \\ = \frac{\sqrt{2}}{4}\sum_{k=0}^{\infty} \frac{1}{k} = \infty $$ which shows that $\frac{\sin \xi}{\xi} \notin \mathcal{L}^1(\mathbb{R})$.",,"['calculus', 'real-analysis', 'analysis', 'inequality']"
27,Continuous function positive at a point is positive in a neighborhood of that point,Continuous function positive at a point is positive in a neighborhood of that point,,"Pretty much the problem asks if a function is continuous at the point $c$ and $f(c) > 0$ then there exists a $d > 0$ such that $\forall x$, $f(x) > 0$ with $|x-c| < d$. I can understand what the problem means. That if a function is positive at a point then there is another point that's really close that'll also be positive. I can not prove this in a formal way though. I've tried using the intermediate value theorem and I do not know hot to implement it. Any ideas?","Pretty much the problem asks if a function is continuous at the point $c$ and $f(c) > 0$ then there exists a $d > 0$ such that $\forall x$, $f(x) > 0$ with $|x-c| < d$. I can understand what the problem means. That if a function is positive at a point then there is another point that's really close that'll also be positive. I can not prove this in a formal way though. I've tried using the intermediate value theorem and I do not know hot to implement it. Any ideas?",,"['analysis', 'continuity']"
28,how to prove that $k+1 \ge (1+\frac{1}{k})^{k}$?,how to prove that ?,k+1 \ge (1+\frac{1}{k})^{k},How to prove that $$k+1\ge \bigg(1+\frac{1}{k}\bigg)^{k} $$    when $k>2$,How to prove that $$k+1\ge \bigg(1+\frac{1}{k}\bigg)^{k} $$    when $k>2$,,['analysis']
29,Prove that $\int_0^1f'(x)dx \leq f(1) - f(0)$.,Prove that .,\int_0^1f'(x)dx \leq f(1) - f(0),"Let $f(x)$ be a non-decreasing function on $[0, 1].$ You may assume that $f$ is differentiable almost everywhere.  Prove that $\int_0^1f'(x)dx \leq f(1) - f(0)$. I am having a hard time with this question.  Obviously we know that $f$ is continuous.  It looks a lot like absolute continuity. Thanks for any help","Let $f(x)$ be a non-decreasing function on $[0, 1].$ You may assume that $f$ is differentiable almost everywhere.  Prove that $\int_0^1f'(x)dx \leq f(1) - f(0)$. I am having a hard time with this question.  Obviously we know that $f$ is continuous.  It looks a lot like absolute continuity. Thanks for any help",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
30,"Showing a set is nowhere dense in $C([0,1])$",Showing a set is nowhere dense in,"C([0,1])","Let $E_n$ be the set of all $f \in C\big([0,1]\big)$ for which there exists $x_0 \in [0,1]$ (depending on $f$) such that \begin{align*} \lvert\, f(x)-f(x_0)\rvert \leq n\lvert x-x_0\rvert, \end{align*} for all $x \in [0,1]$. Why is $E_n$ nowhere dense in $C\big([0,1]\big)$. I have been able to show that $E_n$ is closed in $C\big([0,1]\big)$, but haven't been able to show why the interior is empty. Any advice?...","Let $E_n$ be the set of all $f \in C\big([0,1]\big)$ for which there exists $x_0 \in [0,1]$ (depending on $f$) such that \begin{align*} \lvert\, f(x)-f(x_0)\rvert \leq n\lvert x-x_0\rvert, \end{align*} for all $x \in [0,1]$. Why is $E_n$ nowhere dense in $C\big([0,1]\big)$. I have been able to show that $E_n$ is closed in $C\big([0,1]\big)$, but haven't been able to show why the interior is empty. Any advice?...",,"['real-analysis', 'analysis', 'banach-spaces', 'baire-category', 'holder-spaces']"
31,Fourier transform of a compactly supported function.,Fourier transform of a compactly supported function.,,"Can someone help me the question below? Is there a positive-valued compactly supported function $f$ such that the Fourier transform ${{f}^{\operatorname{ft}}}\left( t \right)=\int_{-\infty }^{\infty }{f\left( x \right){{e}^{itx}}dx}, t\in\mathbb{R}$, and derivative of the Fourier transform, $\frac{d}{dt}{{f}^{\operatorname{ft}}}\left( t \right)$, decay with exponential rates at infinity ? Thank so much for helping.","Can someone help me the question below? Is there a positive-valued compactly supported function $f$ such that the Fourier transform ${{f}^{\operatorname{ft}}}\left( t \right)=\int_{-\infty }^{\infty }{f\left( x \right){{e}^{itx}}dx}, t\in\mathbb{R}$, and derivative of the Fourier transform, $\frac{d}{dt}{{f}^{\operatorname{ft}}}\left( t \right)$, decay with exponential rates at infinity ? Thank so much for helping.",,['analysis']
32,Intuition about Taking an Integral,Intuition about Taking an Integral,,"My hope is to personally develop some further intuition for taking an integral (measuring the area under a curve). Consider a normal distribution and I need the area under the curve from $a$ to $b$. I know from calculus that the answer is given by: $$P(a\le X \le b)  = \int_{a}^{b} f(x)dx = \int_{a}^{b}\frac{1}{\sigma\sqrt{2\pi}}e^{−(y−\mu)^2/ 2\sigma^2} dx$$ My class instructor then draws a normal curve, indicates $a$ and $b$ on the horizontal axis (number line) and draws a line up from each point $a, b$ to the density function, connects the two crossing points and showing a square asks us, ""How do we get the area of a square?"" (Answer: base times height.) It is then shown that the area of the square underestimates the area under the curve and then to get a better approximation the squares are redrawn as two rectangles and then four rectangles and then eight rectangles and this process shows that the area of the (smaller and smaller width) rectangles approximates the area under the curve better and better. Next the instructor said that the ""$f(x)$"" part can be thought of as the height of the rectangle and the ""$dx$"" part can be thought of as the base (width) of the rectangle and that we want the base to be really small, in fact, infinitely small. The instructor then says something like, ""Taking an integral or measuring the are under a curve is like summing the areas of rectangles with infinitely small width."" My questions: Are there other intuitive explanations of what is happening when we take an integral out there and would you please provide them? How would a pure mathematician explain an integral? Would the explanations (intuitive and mathematical) be fully consistent? Multiple explanations or points of view would be appreciated.","My hope is to personally develop some further intuition for taking an integral (measuring the area under a curve). Consider a normal distribution and I need the area under the curve from $a$ to $b$. I know from calculus that the answer is given by: $$P(a\le X \le b)  = \int_{a}^{b} f(x)dx = \int_{a}^{b}\frac{1}{\sigma\sqrt{2\pi}}e^{−(y−\mu)^2/ 2\sigma^2} dx$$ My class instructor then draws a normal curve, indicates $a$ and $b$ on the horizontal axis (number line) and draws a line up from each point $a, b$ to the density function, connects the two crossing points and showing a square asks us, ""How do we get the area of a square?"" (Answer: base times height.) It is then shown that the area of the square underestimates the area under the curve and then to get a better approximation the squares are redrawn as two rectangles and then four rectangles and then eight rectangles and this process shows that the area of the (smaller and smaller width) rectangles approximates the area under the curve better and better. Next the instructor said that the ""$f(x)$"" part can be thought of as the height of the rectangle and the ""$dx$"" part can be thought of as the base (width) of the rectangle and that we want the base to be really small, in fact, infinitely small. The instructor then says something like, ""Taking an integral or measuring the are under a curve is like summing the areas of rectangles with infinitely small width."" My questions: Are there other intuitive explanations of what is happening when we take an integral out there and would you please provide them? How would a pure mathematician explain an integral? Would the explanations (intuitive and mathematical) be fully consistent? Multiple explanations or points of view would be appreciated.",,"['real-analysis', 'integration', 'analysis', 'self-learning']"
33,"Example of a function f that is Generalized Riemann Integrable, but its square is NOT Generalized Riemann Integrable.","Example of a function f that is Generalized Riemann Integrable, but its square is NOT Generalized Riemann Integrable.",,"I am reading a section about Generalized Riemann Integral (Kurzweil-Henstock), and there was a problem on that section to provide an example of a function $f$ on $[0,1]$ that is Generalized Riemann Integrable, but its square $f^2$ is NOT Generalized Riemann Integrable. I couldn't come up with a single function that does the job. Therefore, I appreciate if anyone can provide me with an example of a function $f$ that satisfies the conditions mentioned above. Thanks!","I am reading a section about Generalized Riemann Integral (Kurzweil-Henstock), and there was a problem on that section to provide an example of a function $f$ on $[0,1]$ that is Generalized Riemann Integrable, but its square $f^2$ is NOT Generalized Riemann Integrable. I couldn't come up with a single function that does the job. Therefore, I appreciate if anyone can provide me with an example of a function $f$ that satisfies the conditions mentioned above. Thanks!",,"['real-analysis', 'analysis', 'examples-counterexamples', 'gauge-integral']"
34,"is it possible to find out a partition of $[a,b]$",is it possible to find out a partition of,"[a,b]","Let, $f:[a,b]$$\rightarrow$$\mathbb{R}$ be a continuous function. Is it possible to find out a partition of $[a,b]$ such that $f$ is monotone there? I am stuck here. How to proceed from here?","Let, $f:[a,b]$$\rightarrow$$\mathbb{R}$ be a continuous function. Is it possible to find out a partition of $[a,b]$ such that $f$ is monotone there? I am stuck here. How to proceed from here?",,['analysis']
35,Show that $\lim_{x→0+ } x^{−1/2}f(x)$ exists and determine the value of this limit.,Show that  exists and determine the value of this limit.,\lim_{x→0+ } x^{−1/2}f(x),"Let $f : [0,1] → \mathbb{R}$ be absolutely continuous, satisfy $f(0) = 0$ and $f′ ∈ L_2([0,1]).$ Show that $\lim_{x→0+ } x^{−1/2}f(x)$ exists and determine the value of this limit. From absolute continuity we have that $f'$ exists almost everywhere.  I wanted to say that $f(1) - \lim_{x \rightarrow 0^+}x^{-1/2}f(x)$ = $\int$$_{0}^1$$\frac{d}{dx}(x^{-1/2}$$f(x))$$dx$ and the show this has $L_1$ norm finite showing the limit exists.  But this hasn't worked yet. Thanks for any help.","Let $f : [0,1] → \mathbb{R}$ be absolutely continuous, satisfy $f(0) = 0$ and $f′ ∈ L_2([0,1]).$ Show that $\lim_{x→0+ } x^{−1/2}f(x)$ exists and determine the value of this limit. From absolute continuity we have that $f'$ exists almost everywhere.  I wanted to say that $f(1) - \lim_{x \rightarrow 0^+}x^{-1/2}f(x)$ = $\int$$_{0}^1$$\frac{d}{dx}(x^{-1/2}$$f(x))$$dx$ and the show this has $L_1$ norm finite showing the limit exists.  But this hasn't worked yet. Thanks for any help.",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
36,"$f'$ is bounded and isn't continuous on $(a,b)$, so there's a point $y\in(a,b)$ such that $\lim_{x\to y}f'$ does not exist","is bounded and isn't continuous on , so there's a point  such that  does not exist","f' (a,b) y\in(a,b) \lim_{x\to y}f'","Prove/disprove: $f$ has a bounded derivative and $f'$ isn't continuous on $(a,b)$, so there's a point $y\in(a,b)$ such that $\displaystyle\lim_{x\to y}f'$ does not exist. I think that if $f'$ isn't continuous on the interval, then maybe we could have two disjoint sub-intervals, like for example $(a,c), (d,b)$ such that $d-c=\dfrac {a+b} 3$ so there's a substantial gap in the interval $(a,b)$ where $f'$ isn't defined so it follows that it won't have a limit there, for example: on $\dfrac {c+d}2$.","Prove/disprove: $f$ has a bounded derivative and $f'$ isn't continuous on $(a,b)$, so there's a point $y\in(a,b)$ such that $\displaystyle\lim_{x\to y}f'$ does not exist. I think that if $f'$ isn't continuous on the interval, then maybe we could have two disjoint sub-intervals, like for example $(a,c), (d,b)$ such that $d-c=\dfrac {a+b} 3$ so there's a substantial gap in the interval $(a,b)$ where $f'$ isn't defined so it follows that it won't have a limit there, for example: on $\dfrac {c+d}2$.",,"['calculus', 'analysis', 'functions', 'proof-verification']"
37,Infinite differentiability and power series expansion,Infinite differentiability and power series expansion,,Does every infinitely differentiable function have a power series expansion?Is this a theorem? Or is this an open question?,Does every infinitely differentiable function have a power series expansion?Is this a theorem? Or is this an open question?,,"['analysis', 'power-series']"
38,How can I check the convergence of the sequence? Does it diverge?,How can I check the convergence of the sequence? Does it diverge?,,"How can I check the convergence of the sequence $\frac{1}{\sqrt{n^2+1}}+\frac{2}{\sqrt{n^2+2}}+\cdots+\frac{n}{\sqrt{n^2+n}}$? I think that it diverges,because it is bounded below from $\frac{n(n+1)}{2\sqrt{n^2+n}} $ and above from $\frac{n(n+1)}{2\sqrt{n^2+1}}$..Is this correct?","How can I check the convergence of the sequence $\frac{1}{\sqrt{n^2+1}}+\frac{2}{\sqrt{n^2+2}}+\cdots+\frac{n}{\sqrt{n^2+n}}$? I think that it diverges,because it is bounded below from $\frac{n(n+1)}{2\sqrt{n^2+n}} $ and above from $\frac{n(n+1)}{2\sqrt{n^2+1}}$..Is this correct?",,"['calculus', 'analysis']"
39,How prove there exists a real number $y$ with $0<y<1$ such that $a_0+a_1y+\cdots+a_ny^n=0.$,How prove there exists a real number  with  such that,y 0<y<1 a_0+a_1y+\cdots+a_ny^n=0.,"Suppose that the real numbers $a_0,a_1,\dots,a_n$ and $x,$ with $0<x<1,$ satisfy $$\frac{a_0}{1-x}+\frac{a_1}{1-x^2}+\cdots+\frac{a_n}{1-x^{n+1}}=0.$$Prove that there exists a real number $y$ with $0<y<1$ such that $$a_0+a_1y+\cdots+a_ny^n=0.$$ My try: let $$f(x)=a_{n}x^n+a_{n-1}x^{n-1}+\cdots+a_{1}x+a_{0}$$ then  $$f(0)=a_{0}$$ $$f(1)=a_{0}+a_{1}+\cdots+a_{n}$$ But How can prove  $$f(0)f(1)<0?$$ Thank you","Suppose that the real numbers $a_0,a_1,\dots,a_n$ and $x,$ with $0<x<1,$ satisfy $$\frac{a_0}{1-x}+\frac{a_1}{1-x^2}+\cdots+\frac{a_n}{1-x^{n+1}}=0.$$Prove that there exists a real number $y$ with $0<y<1$ such that $$a_0+a_1y+\cdots+a_ny^n=0.$$ My try: let $$f(x)=a_{n}x^n+a_{n-1}x^{n-1}+\cdots+a_{1}x+a_{0}$$ then  $$f(0)=a_{0}$$ $$f(1)=a_{0}+a_{1}+\cdots+a_{n}$$ But How can prove  $$f(0)f(1)<0?$$ Thank you",,['analysis']
40,Prove that $\mu\left(\cup_{k=1}^\infty A_k\right)=\sum_{k\ge1}\mu(A_k)$,Prove that,\mu\left(\cup_{k=1}^\infty A_k\right)=\sum_{k\ge1}\mu(A_k),"Suppose that the measurable sets $A_1,A_2,...$ are ""almost disjoint"" in the sense that $\mu(A_i\cap A_j) = 0$ if $i\neq j$. Prove that $$\mu\left(\cup_{k=1}^\infty A_k\right)=\sum_{k\ge1}\mu(A_k)$$ Conversely, suppose that the measurable sets $A_1,A_2,...$ satisfy $$\mu\left(\cup_{k=1}^\infty A_k\right) = \sum_{k=1}^\infty\mu(A_k)<\infty$$ Prove that the sets are almost disjoint. Here $\mu(A)$ denotes the Lebesgue measure of $A$. I know that if the sets $S_1,S_2,...$ are all measurable, then $$\mu(\cup_{k=1}^\infty S_k)\le\sum_{k=1}^\infty \mu(S_k)$$ and equality holds if the sets are disjoint. How can I accommodate this for almost disjoint sets?","Suppose that the measurable sets $A_1,A_2,...$ are ""almost disjoint"" in the sense that $\mu(A_i\cap A_j) = 0$ if $i\neq j$. Prove that $$\mu\left(\cup_{k=1}^\infty A_k\right)=\sum_{k\ge1}\mu(A_k)$$ Conversely, suppose that the measurable sets $A_1,A_2,...$ satisfy $$\mu\left(\cup_{k=1}^\infty A_k\right) = \sum_{k=1}^\infty\mu(A_k)<\infty$$ Prove that the sets are almost disjoint. Here $\mu(A)$ denotes the Lebesgue measure of $A$. I know that if the sets $S_1,S_2,...$ are all measurable, then $$\mu(\cup_{k=1}^\infty S_k)\le\sum_{k=1}^\infty \mu(S_k)$$ and equality holds if the sets are disjoint. How can I accommodate this for almost disjoint sets?",,"['real-analysis', 'analysis', 'measure-theory', 'geometric-measure-theory', 'lebesgue-measure']"
41,Solve the integral? [closed],Solve the integral? [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 10 years ago . Improve this question Help me? please How to solve this integral? $$\int\frac{1+x^2}{1+x^4}\,dx$$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 10 years ago . Improve this question Help me? please How to solve this integral? $$\int\frac{1+x^2}{1+x^4}\,dx$$",,"['analysis', 'real-numbers']"
42,"There exist $\{a_{n}\},\{b_{n}\}$ such $\lim_{n\to\infty}\frac{a_{n}}{b_{n}}=c？$",There exist  such,"\{a_{n}\},\{b_{n}\} \lim_{n\to\infty}\frac{a_{n}}{b_{n}}=c？","Let $ A$ and $ B$ are two infinite subsets of the natural numbers $\mathbb{N}$,  such that $$A\cap B=\emptyset \qquad A\cup B=\mathbb N$$ Question: is it true that for every natural $c>0$,   there exist two increasing sequences $\{a_{n}\},\{b_{n}\}$ such that  $\{a_{n}\}\subseteq  A,\{b_{n}\}\subseteq  B$ and  $$\lim_{n\to\infty}\dfrac{a_{n}}{b_{n}}=c?$$ My try: maybe I guess  there doesn't exist such this condition $\{a_{n}\},\{b_{n}\}$ Thank you for you help","Let $ A$ and $ B$ are two infinite subsets of the natural numbers $\mathbb{N}$,  such that $$A\cap B=\emptyset \qquad A\cup B=\mathbb N$$ Question: is it true that for every natural $c>0$,   there exist two increasing sequences $\{a_{n}\},\{b_{n}\}$ such that  $\{a_{n}\}\subseteq  A,\{b_{n}\}\subseteq  B$ and  $$\lim_{n\to\infty}\dfrac{a_{n}}{b_{n}}=c?$$ My try: maybe I guess  there doesn't exist such this condition $\{a_{n}\},\{b_{n}\}$ Thank you for you help",,"['real-analysis', 'analysis', 'ramsey-theory']"
43,"In calculus of variations, what is a functional?","In calculus of variations, what is a functional?",,"I am reading a bit about calculus of variations, and I've encountered the following. Suppose the given function $F(\cdot,\cdot,\cdot)$ is twice continuously differentiable with respect to all of its arguments. Among all functions/paths $y=y(x)$ , which are twice continuously differentiable on the internal $[a,b]$ with $y(a)$ and $y(b)$ specified, find the one which extremizes the functional defined by $$J(y) := \int_a^b F(x, y, y_x) \, {\rm d} x$$ I have a bit of trouble understanding what this means exactly, as a lot of the lingo is quite new. As I currently understand it, the function $F$ is a function, of which is second derivative is constant (like for example $y=x^2)$ , and it looks as if $F$ is a function of 3 variables? I'm not too sure what $F(x,y, y_x)$ means, since $y_x$ was earlier specified to mean $y'(x)$ . Also, what is this 'functional' intuitively? I have some trouble understanding it, mainly because of the aforementioned confusion.","I am reading a bit about calculus of variations, and I've encountered the following. Suppose the given function is twice continuously differentiable with respect to all of its arguments. Among all functions/paths , which are twice continuously differentiable on the internal with and specified, find the one which extremizes the functional defined by I have a bit of trouble understanding what this means exactly, as a lot of the lingo is quite new. As I currently understand it, the function is a function, of which is second derivative is constant (like for example , and it looks as if is a function of 3 variables? I'm not too sure what means, since was earlier specified to mean . Also, what is this 'functional' intuitively? I have some trouble understanding it, mainly because of the aforementioned confusion.","F(\cdot,\cdot,\cdot) y=y(x) [a,b] y(a) y(b) J(y) := \int_a^b F(x, y, y_x) \, {\rm d} x F y=x^2) F F(x,y, y_x) y_x y'(x)","['analysis', 'optimization', 'intuition', 'calculus-of-variations']"
44,Minimum of set $\{\frac{m}{n} + \frac{4n}{m}\}$,Minimum of set,\{\frac{m}{n} + \frac{4n}{m}\},"We have the following set: $\mathcal{A} = \{ \frac{m}{n} + \frac{4n}{m};\ \ m, n \in \mathbb{N} \} $ Attempting to prove that the set's minimum is 4 yields: $$\frac{m}{n}+\frac{4n}{m} = \frac{m^2 + 4 n ^2}{mn} \geq 4$$ $$m^2 + 4n^2 \geq 4mn$$ $$m^2 + 4n^2 - 4mn \geq 0$$ $$(2n-m)^2 \geq 0$$ I do not know how to proceed beyond this point, although I suspect induction may be required. Thank you for your time. Edit: how can it be shown that the set has no upper-bound?","We have the following set: $\mathcal{A} = \{ \frac{m}{n} + \frac{4n}{m};\ \ m, n \in \mathbb{N} \} $ Attempting to prove that the set's minimum is 4 yields: $$\frac{m}{n}+\frac{4n}{m} = \frac{m^2 + 4 n ^2}{mn} \geq 4$$ $$m^2 + 4n^2 \geq 4mn$$ $$m^2 + 4n^2 - 4mn \geq 0$$ $$(2n-m)^2 \geq 0$$ I do not know how to proceed beyond this point, although I suspect induction may be required. Thank you for your time. Edit: how can it be shown that the set has no upper-bound?",,['analysis']
45,Estimate divergence by gradient in H1,Estimate divergence by gradient in H1,,"I am currently trying to fully understand the stationary Stokes equations of incompressible fluid. In the mixed form (homogeneous boundary data), for $\Omega\subset\mathbb{R}^d$, $d\in\{2,3\}$, a system like $$ \left\{ \begin{array}{rl} a(\mathbf{u},\mathbf{v}) + b(\mathbf{v},p) &= 0\\ b(\mathbf{u},q) &= 0 \end{array} \right. $$ for all $\mathbf v\in (H^1(\Omega))^d$ and $q\in L^2(\Omega)$ with $b(\mathbf v,p) = -(\nabla\cdot \mathbf v,p)_{L^2(\Omega)}$ arises. For proving existence and uniqueness of a solution one can follow the book of V. Girault and P.-A. Raviart. However, they do not mention how to prove continuity of these two bilinear forms. For $a(\cdot,\cdot)$ I have managed to do the proof by using Korns inequality. Concerning the second bilinear form $b(\cdot,\cdot)$ I have found the estimate $$ \|\nabla\cdot \mathbf{v}\|_{L^2(\Omega)}\leq \sqrt{d}\|\nabla\mathbf{v}\|_{L^2(\Omega)}. $$ But it remains totally unclear to me, how to show this estimate. Help is very welcome and thanks in advance! Update: I have gotten so far, that $$ \|\nabla\mathbf{v}\|^2_{L^2(\Omega)} = \int_\Omega \sum_{i,j=1}^d (\frac{\partial v_i}{\partial x_j})^2 $$ and $$ \|\nabla\cdot\mathbf{v}\|^2_{L^2(\Omega)} = \int_\Omega (\sum_{i=1}^d \frac{\partial v_i}{\partial x_i})^2, $$ but how does the $\sqrt{d}$ come in? I can see that some terms of the Jacobian term appear in the divergence term, but what about the mixed terms?","I am currently trying to fully understand the stationary Stokes equations of incompressible fluid. In the mixed form (homogeneous boundary data), for $\Omega\subset\mathbb{R}^d$, $d\in\{2,3\}$, a system like $$ \left\{ \begin{array}{rl} a(\mathbf{u},\mathbf{v}) + b(\mathbf{v},p) &= 0\\ b(\mathbf{u},q) &= 0 \end{array} \right. $$ for all $\mathbf v\in (H^1(\Omega))^d$ and $q\in L^2(\Omega)$ with $b(\mathbf v,p) = -(\nabla\cdot \mathbf v,p)_{L^2(\Omega)}$ arises. For proving existence and uniqueness of a solution one can follow the book of V. Girault and P.-A. Raviart. However, they do not mention how to prove continuity of these two bilinear forms. For $a(\cdot,\cdot)$ I have managed to do the proof by using Korns inequality. Concerning the second bilinear form $b(\cdot,\cdot)$ I have found the estimate $$ \|\nabla\cdot \mathbf{v}\|_{L^2(\Omega)}\leq \sqrt{d}\|\nabla\mathbf{v}\|_{L^2(\Omega)}. $$ But it remains totally unclear to me, how to show this estimate. Help is very welcome and thanks in advance! Update: I have gotten so far, that $$ \|\nabla\mathbf{v}\|^2_{L^2(\Omega)} = \int_\Omega \sum_{i,j=1}^d (\frac{\partial v_i}{\partial x_j})^2 $$ and $$ \|\nabla\cdot\mathbf{v}\|^2_{L^2(\Omega)} = \int_\Omega (\sum_{i=1}^d \frac{\partial v_i}{\partial x_i})^2, $$ but how does the $\sqrt{d}$ come in? I can see that some terms of the Jacobian term appear in the divergence term, but what about the mixed terms?",,"['analysis', 'sobolev-spaces', 'estimation']"
46,Modulus of continuity of a continuous function,Modulus of continuity of a continuous function,,"Let $f : I\subset\mathbb{R} → \mathbb{C}$ be a continuous function on the closed interval $I$.  A modulus of continuity of $f$ is any real-extended valued function $\omega: [0, ∞] → [0, ∞]$, vanishing at $0$ and continuous at $0$, that is $\lim_{t\to0}\omega(t)=\omega(0)=0.$ We say $f$ admits $\omega$ as modulus of continuity if and only if, $$\forall x,x'\in I: \|f(x)-f(x')\|\leq\omega(|x-x'|).$$ Here is my question: Does any function $f$ as above admit a modulus of continuity?","Let $f : I\subset\mathbb{R} → \mathbb{C}$ be a continuous function on the closed interval $I$.  A modulus of continuity of $f$ is any real-extended valued function $\omega: [0, ∞] → [0, ∞]$, vanishing at $0$ and continuous at $0$, that is $\lim_{t\to0}\omega(t)=\omega(0)=0.$ We say $f$ admits $\omega$ as modulus of continuity if and only if, $$\forall x,x'\in I: \|f(x)-f(x')\|\leq\omega(|x-x'|).$$ Here is my question: Does any function $f$ as above admit a modulus of continuity?",,"['analysis', 'uniform-continuity']"
47,Convergence of partial sums of real sequences,Convergence of partial sums of real sequences,,"For all $i\in\mathbb{N}$, let $(a_{i,n})_{n\in\mathbb{N}}$ be a real sequence that tends to $0$ for $n\rightarrow\infty$. It holds also that $|a_{i,n}|\leq1$ for all $i,n\in\mathbb{N}$. Is it possible to show that \begin{align*}  c_n:=\frac{1}{n}\sum_{i=1}^{n}a_{i,n}\xrightarrow{n\rightarrow\infty}0?\end{align*} Thanks!","For all $i\in\mathbb{N}$, let $(a_{i,n})_{n\in\mathbb{N}}$ be a real sequence that tends to $0$ for $n\rightarrow\infty$. It holds also that $|a_{i,n}|\leq1$ for all $i,n\in\mathbb{N}$. Is it possible to show that \begin{align*}  c_n:=\frac{1}{n}\sum_{i=1}^{n}a_{i,n}\xrightarrow{n\rightarrow\infty}0?\end{align*} Thanks!",,"['real-analysis', 'analysis', 'asymptotics']"
48,Prove that a function f is continuous (2),Prove that a function f is continuous (2),,$$f:\mathbb{R} \rightarrow \mathbb{R}:$$ $$f(x) = \begin{cases}  \cos\left(\frac{1}{x}\right) & \text{if $x\neq0$} \\  0 & \text{if $x=0$} \end{cases}$$ Is the function $f$ continuous in $x=0$? 1) $\displaystyle\lim_{x \to 0^-} \cos\left(\frac{1}{x}\right)$ with the sequence: $x_n=-\frac{1}{n}$ $\displaystyle\lim_{x \to \infty}-\frac{1}{n}= 0^-$ $\Rightarrow \displaystyle\lim_{x \to \infty} \cos\left(\frac{1}{-\frac{1}{n}}\right)$ $\displaystyle\lim_{x \to \infty} \cos(-n)$ which doesn't exist. 2) $\displaystyle\lim_{x \to 0^+} \cos\left(\frac{1}{x}\right)$ with the sequence: $x_n=-\frac{1}{n}$ $\displaystyle\lim_{x \to \infty}-\frac{1}{n}= 0^+$ $\Rightarrow \displaystyle\lim_{x \to \infty} \cos\left(\frac{1}{-\frac{1}{n}}\right)$ $\displaystyle\lim_{x \to \infty} \cos(-n)$ which doesn't exist. $f$ is not continuous in $x=0$ because after (1) & (2) the both one-side limits for $0$ don't exist. Is this sufficient to answer the question?,$$f:\mathbb{R} \rightarrow \mathbb{R}:$$ $$f(x) = \begin{cases}  \cos\left(\frac{1}{x}\right) & \text{if $x\neq0$} \\  0 & \text{if $x=0$} \end{cases}$$ Is the function $f$ continuous in $x=0$? 1) $\displaystyle\lim_{x \to 0^-} \cos\left(\frac{1}{x}\right)$ with the sequence: $x_n=-\frac{1}{n}$ $\displaystyle\lim_{x \to \infty}-\frac{1}{n}= 0^-$ $\Rightarrow \displaystyle\lim_{x \to \infty} \cos\left(\frac{1}{-\frac{1}{n}}\right)$ $\displaystyle\lim_{x \to \infty} \cos(-n)$ which doesn't exist. 2) $\displaystyle\lim_{x \to 0^+} \cos\left(\frac{1}{x}\right)$ with the sequence: $x_n=-\frac{1}{n}$ $\displaystyle\lim_{x \to \infty}-\frac{1}{n}= 0^+$ $\Rightarrow \displaystyle\lim_{x \to \infty} \cos\left(\frac{1}{-\frac{1}{n}}\right)$ $\displaystyle\lim_{x \to \infty} \cos(-n)$ which doesn't exist. $f$ is not continuous in $x=0$ because after (1) & (2) the both one-side limits for $0$ don't exist. Is this sufficient to answer the question?,,"['real-analysis', 'analysis']"
49,Question regarding $\lim_{x \to 0} \left(\exp(\sin (x)) + \exp \left(\frac{1}{\sin (x)}\right)\right)$,Question regarding,\lim_{x \to 0} \left(\exp(\sin (x)) + \exp \left(\frac{1}{\sin (x)}\right)\right),"I wanted to find out whether the following limit exists, and find the value if it does.  $$\lim_{x \to 0} \left(\exp(\sin (x)) + \exp \left(\frac{1}{\sin (x)}\right)\right).$$ Attempt After many attempt to prove that the limit exists, I looked up Wolfram Alpha, and it turns out that it doesn't. So I set out to prove that. By the algebra of limits we have $$\lim_{x \to 0} \left(\exp(\sin (x)) + \exp \left(\frac{1}{\sin (x)}\right)\right)= \lim_{x \to 0} \exp(\sin (x)) + \lim_{x \to 0} \exp \left(\frac{1}{\sin (x)}\right).$$ Now, let $$f(x)=\frac{1}{\sin (x)} \ \text{and} \ g(x)=\exp (x),$$ and consider the sequences $$(y_n)=\dfrac{1}{\frac{\pi}{2}+ 2n\pi} \ \text{and} \ (z_n)=\dfrac{1}{-\frac{\pi}{2}+ 2n\pi }\text{.}$$ Then $(y_n) \rightarrow 0$ and $(z_n) \rightarrow 0$ as $n \rightarrow 0$, however $f(y_n)=1$ and  $f(z_n)=-1$ for all $n \in \mathbb{N}$. Hence $\lim_{n \to \infty}f(y_n) \neq \lim_{n \to \infty}f(z_n)$ and $\lim_{x \to 0} \frac{1}{\sin (x)}$ does not exist. If the working above is correct, I then would like to show that since $\lim_{x \to 0} \frac{1}{\sin (x)}$, then $\lim_{x \to 0} \exp \left(\frac{1}{\sin (x)}\right)$ does not exist, if $\lim_{x \to 0} \exp \left(\frac{1}{\sin (x)}\right)$ does not exist, then $\lim_{x \to 0} \exp(\sin (x)) + \lim_{x \to 0} \exp \left(\frac{1}{\sin (x)}\right)$ does not exist. Question A. Is my working above correct? B. How do I show (1) and (2)? I know the rule for finding the limit of the composition of two functions and the addition of limits, if the limit exists, but not if the limit doesn't exist. C. Is there any other way to tackle this question? D. Could you give me some tips to identify if the limit of a function does not exist, without any aid of computers? (In order to not waste time trying to prove the existence of the limit when it actually doesn't.) Thank you for your time.","I wanted to find out whether the following limit exists, and find the value if it does.  $$\lim_{x \to 0} \left(\exp(\sin (x)) + \exp \left(\frac{1}{\sin (x)}\right)\right).$$ Attempt After many attempt to prove that the limit exists, I looked up Wolfram Alpha, and it turns out that it doesn't. So I set out to prove that. By the algebra of limits we have $$\lim_{x \to 0} \left(\exp(\sin (x)) + \exp \left(\frac{1}{\sin (x)}\right)\right)= \lim_{x \to 0} \exp(\sin (x)) + \lim_{x \to 0} \exp \left(\frac{1}{\sin (x)}\right).$$ Now, let $$f(x)=\frac{1}{\sin (x)} \ \text{and} \ g(x)=\exp (x),$$ and consider the sequences $$(y_n)=\dfrac{1}{\frac{\pi}{2}+ 2n\pi} \ \text{and} \ (z_n)=\dfrac{1}{-\frac{\pi}{2}+ 2n\pi }\text{.}$$ Then $(y_n) \rightarrow 0$ and $(z_n) \rightarrow 0$ as $n \rightarrow 0$, however $f(y_n)=1$ and  $f(z_n)=-1$ for all $n \in \mathbb{N}$. Hence $\lim_{n \to \infty}f(y_n) \neq \lim_{n \to \infty}f(z_n)$ and $\lim_{x \to 0} \frac{1}{\sin (x)}$ does not exist. If the working above is correct, I then would like to show that since $\lim_{x \to 0} \frac{1}{\sin (x)}$, then $\lim_{x \to 0} \exp \left(\frac{1}{\sin (x)}\right)$ does not exist, if $\lim_{x \to 0} \exp \left(\frac{1}{\sin (x)}\right)$ does not exist, then $\lim_{x \to 0} \exp(\sin (x)) + \lim_{x \to 0} \exp \left(\frac{1}{\sin (x)}\right)$ does not exist. Question A. Is my working above correct? B. How do I show (1) and (2)? I know the rule for finding the limit of the composition of two functions and the addition of limits, if the limit exists, but not if the limit doesn't exist. C. Is there any other way to tackle this question? D. Could you give me some tips to identify if the limit of a function does not exist, without any aid of computers? (In order to not waste time trying to prove the existence of the limit when it actually doesn't.) Thank you for your time.",,['analysis']
50,Questions on Bolzano-Weierstrass theorem,Questions on Bolzano-Weierstrass theorem,,"Here is how it is worded in Apostol: Theorem. If a bounded set $S$ in $\mathbb{R}^n$ contains infinitely many points, then there is at least one point in $\mathbb{R}^n$ which is an accumulation point of $S$. Proof. (for $\mathbb{R}^1$) Since $S$ is bounded, it lies in some interval $[-a, a]$. At least one of the subintervals $[-a, 0]$ or $[0, a]$ contains an infinite subset of $S$. Call one such subinterval $[a_1, b_1]$. Bisect $[a_1, b_1]$ and obtain a subinterval $[a_2, b_2]$ containing an infinite subset of $S$, and continue this process. In this way a countable collection of intervals is obtained, the $n$th interval $[a_n, b_n]$ being of length $b_n -a_n = a/2^{n-1}$. Clearly the sup of the left endpoints $a_n$ and the inf of the right endpoints $b_n$ must be equal, say to $x$. The point $x$ will be an accumulation point of $S$ because, if $r$ is any positive number, the interval $[a_n, b_n]$ will be contained in $B(x;r)$ as soon as $n$ is large enough so that $b_n -a_n < r/2$. The interval $B(x;r)$ contains a point of $S$ distinct from $x$ and hence $x$ is an accumulation point of $S$. My questions: 1) Why bother with the first 3 sentences? What's wrong with just saying ""Since $S$ is bounded, there is an interval $[a_1, b_1]$ containing an infinite subset of $S$"". Actually, now I'm not entirely sure why it has to be bounded. Couldn't you always find some infinite subset to do this with? 2) Why does $b_n -a_n < r/2$? I would think it just has to be less than $r$. For example, these intervals could just ""converge"" to one end point ($x = a_1$ or $x = b_1$), in which case the length just has to be less than the radius of the ball (neighborhood). 3) I'm not entirely sure why we kept bisecting the intervals. I'm confused, but I don't really have a good concrete question.","Here is how it is worded in Apostol: Theorem. If a bounded set $S$ in $\mathbb{R}^n$ contains infinitely many points, then there is at least one point in $\mathbb{R}^n$ which is an accumulation point of $S$. Proof. (for $\mathbb{R}^1$) Since $S$ is bounded, it lies in some interval $[-a, a]$. At least one of the subintervals $[-a, 0]$ or $[0, a]$ contains an infinite subset of $S$. Call one such subinterval $[a_1, b_1]$. Bisect $[a_1, b_1]$ and obtain a subinterval $[a_2, b_2]$ containing an infinite subset of $S$, and continue this process. In this way a countable collection of intervals is obtained, the $n$th interval $[a_n, b_n]$ being of length $b_n -a_n = a/2^{n-1}$. Clearly the sup of the left endpoints $a_n$ and the inf of the right endpoints $b_n$ must be equal, say to $x$. The point $x$ will be an accumulation point of $S$ because, if $r$ is any positive number, the interval $[a_n, b_n]$ will be contained in $B(x;r)$ as soon as $n$ is large enough so that $b_n -a_n < r/2$. The interval $B(x;r)$ contains a point of $S$ distinct from $x$ and hence $x$ is an accumulation point of $S$. My questions: 1) Why bother with the first 3 sentences? What's wrong with just saying ""Since $S$ is bounded, there is an interval $[a_1, b_1]$ containing an infinite subset of $S$"". Actually, now I'm not entirely sure why it has to be bounded. Couldn't you always find some infinite subset to do this with? 2) Why does $b_n -a_n < r/2$? I would think it just has to be less than $r$. For example, these intervals could just ""converge"" to one end point ($x = a_1$ or $x = b_1$), in which case the length just has to be less than the radius of the ball (neighborhood). 3) I'm not entirely sure why we kept bisecting the intervals. I'm confused, but I don't really have a good concrete question.",,"['real-analysis', 'analysis']"
51,Cauchy Sequence Convergence Prove or Give Counterexample,Cauchy Sequence Convergence Prove or Give Counterexample,,"Prove or give a counterexample: if a sequence of real numbers $\{x_n\}$ from $n=1$ to $\infty$ has the property that for all $\epsilon >0$, there exists $N \in \mathbb N$ such that for all $n \ge N$ we have $|x_{n+1} - x_n| < \epsilon$, then $\{x_n\}$ is a convergent sequence. How is this different from the definition of a Cauchy sequence? Attempt: For the second part of the problem, I know that it's different from the definition of a Cauchy sequence as it's taking the next part of the sequence and subtracting it from the current part of the sequence. For the first part, I'm not sure how to go about doing this; originally, I thought I could do something such as $|x_{n+1} - x_n| < \epsilon/2$, in then use the triangle inequality. But I'm not sure we can do that. Thoughts and comments? And apologies ahead of time for the lack of formatting.","Prove or give a counterexample: if a sequence of real numbers $\{x_n\}$ from $n=1$ to $\infty$ has the property that for all $\epsilon >0$, there exists $N \in \mathbb N$ such that for all $n \ge N$ we have $|x_{n+1} - x_n| < \epsilon$, then $\{x_n\}$ is a convergent sequence. How is this different from the definition of a Cauchy sequence? Attempt: For the second part of the problem, I know that it's different from the definition of a Cauchy sequence as it's taking the next part of the sequence and subtracting it from the current part of the sequence. For the first part, I'm not sure how to go about doing this; originally, I thought I could do something such as $|x_{n+1} - x_n| < \epsilon/2$, in then use the triangle inequality. But I'm not sure we can do that. Thoughts and comments? And apologies ahead of time for the lack of formatting.",,"['calculus', 'real-analysis', 'analysis']"
52,Understanding the Banach fixed point theorem,Understanding the Banach fixed point theorem,,"The Banach fixed point theorem is stated in my book ( Applied Asymptotic Analysis by Miller) as Let $\mathcal B$ be a Banach space with norm $\|\cdot\|$.  Let $X$ be a nonempty bounded subset of $\mathcal B$ and suppose that $T \colon X \to X$ is a mapping that satisfies, for some $0 < \rho < 1$, the inequality   $$ \|T(f) - T(g)\| \leq \rho \|f-g\| $$   for all $f$ and $g$ in $X$.  Then there exists a unique element $f^\infty \in X$ such that (i) the sequence of iterates $\{T^k(f)\}_{k \geq 0}$ converges to $f^\infty$ whenever $f \in X$ and (ii) $f^\infty = T(f^\infty)$. I'm having some trouble understanding this result. Suppose $X_R$ is the ball of radius $R$, i.e. $$ X_R = \{f \in \mathcal B \colon \|f\| \leq R\}, $$ and suppose that $T$ is a contraction mapping on $X_R$.  The theorem says that $T$ has a unique fixed point $f^\infty \in X_R$. But isn't $T$ also a contraction mapping in every ball $X_S$ with $0 < S < R$?  Does the theorem then imply that $T$ has a unique fixed point in $X_S$?  It then seems to me that we must have $\|f^\infty\| = 0$, for otherwise it would be outside of some such ball. Where am I going wrong?","The Banach fixed point theorem is stated in my book ( Applied Asymptotic Analysis by Miller) as Let $\mathcal B$ be a Banach space with norm $\|\cdot\|$.  Let $X$ be a nonempty bounded subset of $\mathcal B$ and suppose that $T \colon X \to X$ is a mapping that satisfies, for some $0 < \rho < 1$, the inequality   $$ \|T(f) - T(g)\| \leq \rho \|f-g\| $$   for all $f$ and $g$ in $X$.  Then there exists a unique element $f^\infty \in X$ such that (i) the sequence of iterates $\{T^k(f)\}_{k \geq 0}$ converges to $f^\infty$ whenever $f \in X$ and (ii) $f^\infty = T(f^\infty)$. I'm having some trouble understanding this result. Suppose $X_R$ is the ball of radius $R$, i.e. $$ X_R = \{f \in \mathcal B \colon \|f\| \leq R\}, $$ and suppose that $T$ is a contraction mapping on $X_R$.  The theorem says that $T$ has a unique fixed point $f^\infty \in X_R$. But isn't $T$ also a contraction mapping in every ball $X_S$ with $0 < S < R$?  Does the theorem then imply that $T$ has a unique fixed point in $X_S$?  It then seems to me that we must have $\|f^\infty\| = 0$, for otherwise it would be outside of some such ball. Where am I going wrong?",,"['analysis', 'fixed-point-theorems']"
53,For what values of $a$ will the following sequence converge?,For what values of  will the following sequence converge?,a,"$a \in \mathbb R$ has the decimal expansion $a = a_0.a_1a_2a_3 \ldots a_n \ldots$ Find all values for $a$ for which the sequence $\{a_n\}_{n=1}^{\infty}$ converges. I rule out irrationals first, because if they can't be represented with integer numerator and denominator, they can't have convergent decimal expansions. But how do I separate the rationals, say differ between say $1/7$ and $1/3$?","$a \in \mathbb R$ has the decimal expansion $a = a_0.a_1a_2a_3 \ldots a_n \ldots$ Find all values for $a$ for which the sequence $\{a_n\}_{n=1}^{\infty}$ converges. I rule out irrationals first, because if they can't be represented with integer numerator and denominator, they can't have convergent decimal expansions. But how do I separate the rationals, say differ between say $1/7$ and $1/3$?",,['analysis']
54,Riemann-Stieltjes Integrable,Riemann-Stieltjes Integrable,,"If $f$ and $g$ are both Riemann-Stieltjes Integrable with respect to a monotonic function $\alpha$ , is it true that $f(g(x))$ is still integrable with respect to $\alpha$ ?","If and are both Riemann-Stieltjes Integrable with respect to a monotonic function , is it true that is still integrable with respect to ?",f g \alpha f(g(x)) \alpha,"['real-analysis', 'integration', 'analysis', 'riemann-integration', 'stieltjes-integral']"
55,"""limit along a path"" equivalent to usual definition of limit?","""limit along a path"" equivalent to usual definition of limit?",,"At the institution where I teach, regrettably, we do not teach students the real definition of the limit in our calculus classes.  I am teaching a little complex analysis, and I would like to use the notion of $\lim_{z \to z_0} f(z)$, with at least a little rigor.  If I mention $\epsilon$ and $\delta$ my students will not understand it at all.  Is $\lim_{z \to z_0} f(z) =v$ equivalent to the assertion that $f(r(t)) \to v$ as $t:0\to 1$ along any path $r:[0,1) \to \mathbb{C}$ with $r(t) \to z_0$ as $t \to 1^-$?  My students might appreciate the idea of the pathwise definition.  It is clear to me that the standard, $\epsilon$-$\delta$ definition implies the pathwise criterion, but I embarassed to admit that I do not know if the converse is true.","At the institution where I teach, regrettably, we do not teach students the real definition of the limit in our calculus classes.  I am teaching a little complex analysis, and I would like to use the notion of $\lim_{z \to z_0} f(z)$, with at least a little rigor.  If I mention $\epsilon$ and $\delta$ my students will not understand it at all.  Is $\lim_{z \to z_0} f(z) =v$ equivalent to the assertion that $f(r(t)) \to v$ as $t:0\to 1$ along any path $r:[0,1) \to \mathbb{C}$ with $r(t) \to z_0$ as $t \to 1^-$?  My students might appreciate the idea of the pathwise definition.  It is clear to me that the standard, $\epsilon$-$\delta$ definition implies the pathwise criterion, but I embarassed to admit that I do not know if the converse is true.",,"['analysis', 'education']"
56,Fourier transform of the derivative - insufficient hypotheses?,Fourier transform of the derivative - insufficient hypotheses?,,"An exercise in Carlos ISNARD's Introdução à medida e integração : Show that if $f$ and $f'$ $\in\mathscr{L}^1(\mathbb{R},\lambda,\mathbb{C})$ and $\lim_{x\to\pm\infty}f(x)=0$ then $\hat{(f')}(\zeta)=i\zeta\hat{f}(\zeta)$. ($\lambda$ is the Lebesgue measure on $\mathbb{R}$.) I'm tempted to apply integration by parts on the integral from $-N$ to $N$ and then take limit as $N\to\infty$. But to obtain the result I seemingly need $f'e^{-i\zeta x}$ to be Riemann-integrable so as to use the fundamental theorem of Calculus. What am I missing here? Thank you.","An exercise in Carlos ISNARD's Introdução à medida e integração : Show that if $f$ and $f'$ $\in\mathscr{L}^1(\mathbb{R},\lambda,\mathbb{C})$ and $\lim_{x\to\pm\infty}f(x)=0$ then $\hat{(f')}(\zeta)=i\zeta\hat{f}(\zeta)$. ($\lambda$ is the Lebesgue measure on $\mathbb{R}$.) I'm tempted to apply integration by parts on the integral from $-N$ to $N$ and then take limit as $N\to\infty$. But to obtain the result I seemingly need $f'e^{-i\zeta x}$ to be Riemann-integrable so as to use the fundamental theorem of Calculus. What am I missing here? Thank you.",,"['analysis', 'measure-theory']"
57,Lebesgue Integration Question,Lebesgue Integration Question,,"(Just read the bolded statements if you want to get straight to the point) This question comes as an extension to one posed in Stein and Sakarchi's Real Analysis, and it is related to the notion that an integral of a positive function is equal to the volume bounded by its graph. The text proves that $\int_{\mathbb{R}^{d}}|f(x)|dx=m(A)$ where $A:=\{(x,\alpha)\in\mathbb{R}^{d}\times\mathbb{R} : 0\leq\alpha\leq|f(x)|\}$.  Assuming both $A$ and $f$ are measurable in the appropriate contexts, the proof is a simple computation: \begin{equation*}\int_{\mathbb{R}^{d}}|f(x)|dx=\int_{\mathbb{R}^{d}}m(A_{x})dx=\int_{\mathbb{R}^{d}}\chi_{A_{x}}(x)dx=\int\limits_{0}^{\infty}\int_{\mathbb{R}^{d}}\chi_{A}(x,\alpha)dxd\alpha=m(A)\end{equation*} (we are of course applying Tonelli's Theorem). Now, here is the question from Stein and Sakarchi: If $f$ is integrable on $\mathbb{R}^{d}$, then define for each $\alpha>0$ the set $E_{\alpha}:=\{x:|f(x)|>\alpha\}$, and prove \begin{equation*}\int_{\mathbb{R}^{d}}|f(x)|dx=\int\limits_{0}^{\infty}m(E_{\alpha})d\alpha.\end{equation*} The proof is basically an immediate consequence of what was already proven above, except that we use the slices $A_{\alpha}$ instead of $A_{x}$.  In other words, we have: \begin{equation*}\int_{\mathbb{R}^{d}}|f(x)|dx=\int_{\mathbb{R}^{d}}m(A_{x})dx=\int\limits_{0}^{\infty}m(A_{\alpha})d\alpha=m(A).\end{equation*}  Since $A_{\alpha}$=$E_{\alpha}$, the problem is solved.  A nice geometric meaning of integrating the separate slices is that integrating $A_{x}dx$ is akin to partitioning the domain, and integrating $A_{\alpha}d\alpha$ is akin to partitioning the range.  Again, the rigorous justifications are from the Fubini/Tonelli theorem. Now, here is the part where I am having difficulty. I want to prove the statement \begin{equation*}\int_{\mathbb{R}^{d}}|f(x)|^{p}dx=\int_{0}^{\infty}p\alpha^{p-1}m(E_{\alpha})d\alpha\end{equation*} where everything is as above, and $1<p<\infty$.  The above case is $p=1$.  Since we are not using $\{x: 0<\alpha<|f(x)^{p}\}$, the above proof technique cannot be used exactly, and I'm not sure how to proceed. It is interesting that \begin{equation*}$|f(x)|^{p}=\int\limits_{0}^{|f(x)|}p\alpha^{p-1}d\alpha\end{equation*} so that we get something like \begin{equation*}\int_{\mathbb{R}^{d}}|f(x)|^{p}dx=\int_{\mathbb{R}^{d}}\int_{0}^{|f(x)|}p\alpha^{p-1}d\alpha.\end{equation*}  However, I'm still not sure where to take this.","(Just read the bolded statements if you want to get straight to the point) This question comes as an extension to one posed in Stein and Sakarchi's Real Analysis, and it is related to the notion that an integral of a positive function is equal to the volume bounded by its graph. The text proves that $\int_{\mathbb{R}^{d}}|f(x)|dx=m(A)$ where $A:=\{(x,\alpha)\in\mathbb{R}^{d}\times\mathbb{R} : 0\leq\alpha\leq|f(x)|\}$.  Assuming both $A$ and $f$ are measurable in the appropriate contexts, the proof is a simple computation: \begin{equation*}\int_{\mathbb{R}^{d}}|f(x)|dx=\int_{\mathbb{R}^{d}}m(A_{x})dx=\int_{\mathbb{R}^{d}}\chi_{A_{x}}(x)dx=\int\limits_{0}^{\infty}\int_{\mathbb{R}^{d}}\chi_{A}(x,\alpha)dxd\alpha=m(A)\end{equation*} (we are of course applying Tonelli's Theorem). Now, here is the question from Stein and Sakarchi: If $f$ is integrable on $\mathbb{R}^{d}$, then define for each $\alpha>0$ the set $E_{\alpha}:=\{x:|f(x)|>\alpha\}$, and prove \begin{equation*}\int_{\mathbb{R}^{d}}|f(x)|dx=\int\limits_{0}^{\infty}m(E_{\alpha})d\alpha.\end{equation*} The proof is basically an immediate consequence of what was already proven above, except that we use the slices $A_{\alpha}$ instead of $A_{x}$.  In other words, we have: \begin{equation*}\int_{\mathbb{R}^{d}}|f(x)|dx=\int_{\mathbb{R}^{d}}m(A_{x})dx=\int\limits_{0}^{\infty}m(A_{\alpha})d\alpha=m(A).\end{equation*}  Since $A_{\alpha}$=$E_{\alpha}$, the problem is solved.  A nice geometric meaning of integrating the separate slices is that integrating $A_{x}dx$ is akin to partitioning the domain, and integrating $A_{\alpha}d\alpha$ is akin to partitioning the range.  Again, the rigorous justifications are from the Fubini/Tonelli theorem. Now, here is the part where I am having difficulty. I want to prove the statement \begin{equation*}\int_{\mathbb{R}^{d}}|f(x)|^{p}dx=\int_{0}^{\infty}p\alpha^{p-1}m(E_{\alpha})d\alpha\end{equation*} where everything is as above, and $1<p<\infty$.  The above case is $p=1$.  Since we are not using $\{x: 0<\alpha<|f(x)^{p}\}$, the above proof technique cannot be used exactly, and I'm not sure how to proceed. It is interesting that \begin{equation*}$|f(x)|^{p}=\int\limits_{0}^{|f(x)|}p\alpha^{p-1}d\alpha\end{equation*} so that we get something like \begin{equation*}\int_{\mathbb{R}^{d}}|f(x)|^{p}dx=\int_{\mathbb{R}^{d}}\int_{0}^{|f(x)|}p\alpha^{p-1}d\alpha.\end{equation*}  However, I'm still not sure where to take this.",,"['analysis', 'integration']"
58,Tensor product of Hilbert Spaces,Tensor product of Hilbert Spaces,,"I am following this link under ""definitions"" I need to see why the suggested inner product on the pre-Hilbert space $H_1$ tensor $H_2$ is well defined.  Recall that the fundamental tensors are a spanning set, but not a linearly independent one, hence not free in the category of vector spaces.  How do I know that this definition is consistent no matter what representation of an element of the tensor space I give?","I am following this link under ""definitions"" I need to see why the suggested inner product on the pre-Hilbert space $H_1$ tensor $H_2$ is well defined.  Recall that the fundamental tensors are a spanning set, but not a linearly independent one, hence not free in the category of vector spaces.  How do I know that this definition is consistent no matter what representation of an element of the tensor space I give?",,"['linear-algebra', 'abstract-algebra', 'analysis', 'operator-theory']"
59,Find a sequence $\{a_n\}$ of real numbers such that $\sum a_n$ converges but $\prod (1+ a_n)$ diverges.,Find a sequence  of real numbers such that  converges but  diverges.,\{a_n\} \sum a_n \prod (1+ a_n),"Find a sequence $\{a_n\}$ of real numbers such that $\sum a_n$ converges but $\prod (1+ a_n)$  diverges. The converse is trivial, just make all the $a_n=-1$.","Find a sequence $\{a_n\}$ of real numbers such that $\sum a_n$ converges but $\prod (1+ a_n)$  diverges. The converse is trivial, just make all the $a_n=-1$.",,['analysis']
60,Not so obvious calculus question,Not so obvious calculus question,,"You have $f \in C^\infty([0,1])$ with $f > 0$. Then $\sqrt{f}$ is easily seen to be differentiable . Prove that there exists a constant $C$ independent of $f$ such that: $$\sup_{x\in[0,1]}\left\lvert \left(\sqrt f\right)'(x)\right\rvert \leq C \left(1 + \sup_{x\in[0,1]}\lvert f(x)\rvert + \sup_{x\in[0,1]}\lvert f'(x)\rvert +  \sup_{x\in[0,1]} \lvert f''(x)\rvert\right).$$ I wonder if someone can give a nice proof of this.","You have $f \in C^\infty([0,1])$ with $f > 0$. Then $\sqrt{f}$ is easily seen to be differentiable . Prove that there exists a constant $C$ independent of $f$ such that: $$\sup_{x\in[0,1]}\left\lvert \left(\sqrt f\right)'(x)\right\rvert \leq C \left(1 + \sup_{x\in[0,1]}\lvert f(x)\rvert + \sup_{x\in[0,1]}\lvert f'(x)\rvert +  \sup_{x\in[0,1]} \lvert f''(x)\rvert\right).$$ I wonder if someone can give a nice proof of this.",,"['calculus', 'real-analysis', 'analysis', 'sobolev-spaces']"
61,derivative of a map of vector space of matrices,derivative of a map of vector space of matrices,,"Question: Let $A_{n\times  n}$ be the vector space of all real  $n\times  n$ matrices. If I define a map $$g:A_{n\times  n}\rightarrow A_{n\times  n}$$ such that: $$g\left ( X \right )=X^{2}$$ In this case, what is the derivative of $g$? I am thinking whether the formula: $g'\left ( X \right )=2XX{'}$ works. But I have no idea how the derivative of a matrix is defined. please help?","Question: Let $A_{n\times  n}$ be the vector space of all real  $n\times  n$ matrices. If I define a map $$g:A_{n\times  n}\rightarrow A_{n\times  n}$$ such that: $$g\left ( X \right )=X^{2}$$ In this case, what is the derivative of $g$? I am thinking whether the formula: $g'\left ( X \right )=2XX{'}$ works. But I have no idea how the derivative of a matrix is defined. please help?",,"['linear-algebra', 'real-analysis', 'analysis']"
62,Why solve polynomial equations?,Why solve polynomial equations?,,"Most people learn in linear algebra that its possible to calculate the eigenvalues of a matrix by finding the roots of its characteristic polynomial. However, this method is actually very slow, and while its easy to remember and its possible for a person to use this method by hand, there are many better techniques available (which do not rely on factoring a polynomial). So I was wondering, why on earth is it actually important to have techniques available to solve polynomial equations? (to be specific, I mean solving over $\mathbb{C}$) I actually used to be fairly interested in how to do it, and I know a lot of the different methods that people use. I was just thinking about it though, and I'm actually not sure what sort of applications there are for those techniques.","Most people learn in linear algebra that its possible to calculate the eigenvalues of a matrix by finding the roots of its characteristic polynomial. However, this method is actually very slow, and while its easy to remember and its possible for a person to use this method by hand, there are many better techniques available (which do not rely on factoring a polynomial). So I was wondering, why on earth is it actually important to have techniques available to solve polynomial equations? (to be specific, I mean solving over $\mathbb{C}$) I actually used to be fairly interested in how to do it, and I know a lot of the different methods that people use. I was just thinking about it though, and I'm actually not sure what sort of applications there are for those techniques.",,"['linear-algebra', 'analysis', 'polynomials']"
63,Given $f\notin L^p$ find $g\in L^q$ s.t. $fg\notin L^1$,Given  find  s.t.,f\notin L^p g\in L^q fg\notin L^1,"This is the second part of the Exercise 2 in Chapter 8 of Measure and Integral by Zygmund and Wheeden: Show also that for real valued $f\notin L^p(E)$, there exist a function $g\in L^q(E)$, $\frac{1}{p}+\frac{1}{q}=1$, s.t. $fg\notin L^1(E)$. (Construct $g$ of the form $\sum a_kg_k$ for appropriate $a_k$ and $g_k$ satisfying $\int_E fg_k\to \infty$.) In the following the integrals are taken over $E$. Trying to follow the hint, if we assume $1\lt p\lt\infty$ note that $a_k=k^{-(q+1)}$, $b_k=k$ satisfies $$\begin{cases} \sum a_kb_k\lt\infty\\ \sum a_kb_k^q=\infty\end{cases}$$ In view of $$\begin{align*} &\Vert g\Vert_q=\Vert \sum a_kg_k\Vert_q\leq \sum a_k\Vert g_k\Vert_q\\ &\Vert fg \Vert_1=\int fg =\sum a_k\int fg_k \end{align*}$$ we just need $g_k$ such that $\Vert g_k\Vert_q=k$ and $\int fg_k=k^q$. Suppose that there exist a set $A_k\subseteq E$ such that $\int_{A_k} \vert f\vert^p=k^q$, then the sequence given by $$g_k=\vert f\vert^{p-1}\chi_{A_k}=\vert f\vert^{p/q}\chi_{A_k}$$ will do the job. I need to justify the existence of the $A_k$s. So, I'm looking for something like the following. Let $E\subseteq \mathbb{R}^d$. Let $f$ a function with $$\int_E \vert f\vert=\infty.$$ My question is: Given $t\in[0,\infty[$, does there exist a set $A_t\subseteq E$ s.t. $$\int_{A_t}\vert f\vert=t?$$ Let $C_t$ the cube around the origin with volume $t\geq 0$. I was trying to prove that the function $F:[0,\infty[\to\mathbb{R}\cup\{\infty\}$ given by $$F(t)=\int_{C_t\cap E}\vert f\vert$$ is continuous. My idea seems to have no future, because for example $f$ could be $\infty$ in any subset of positive measure. If this is not possible, how can I approach this problem?","This is the second part of the Exercise 2 in Chapter 8 of Measure and Integral by Zygmund and Wheeden: Show also that for real valued $f\notin L^p(E)$, there exist a function $g\in L^q(E)$, $\frac{1}{p}+\frac{1}{q}=1$, s.t. $fg\notin L^1(E)$. (Construct $g$ of the form $\sum a_kg_k$ for appropriate $a_k$ and $g_k$ satisfying $\int_E fg_k\to \infty$.) In the following the integrals are taken over $E$. Trying to follow the hint, if we assume $1\lt p\lt\infty$ note that $a_k=k^{-(q+1)}$, $b_k=k$ satisfies $$\begin{cases} \sum a_kb_k\lt\infty\\ \sum a_kb_k^q=\infty\end{cases}$$ In view of $$\begin{align*} &\Vert g\Vert_q=\Vert \sum a_kg_k\Vert_q\leq \sum a_k\Vert g_k\Vert_q\\ &\Vert fg \Vert_1=\int fg =\sum a_k\int fg_k \end{align*}$$ we just need $g_k$ such that $\Vert g_k\Vert_q=k$ and $\int fg_k=k^q$. Suppose that there exist a set $A_k\subseteq E$ such that $\int_{A_k} \vert f\vert^p=k^q$, then the sequence given by $$g_k=\vert f\vert^{p-1}\chi_{A_k}=\vert f\vert^{p/q}\chi_{A_k}$$ will do the job. I need to justify the existence of the $A_k$s. So, I'm looking for something like the following. Let $E\subseteq \mathbb{R}^d$. Let $f$ a function with $$\int_E \vert f\vert=\infty.$$ My question is: Given $t\in[0,\infty[$, does there exist a set $A_t\subseteq E$ s.t. $$\int_{A_t}\vert f\vert=t?$$ Let $C_t$ the cube around the origin with volume $t\geq 0$. I was trying to prove that the function $F:[0,\infty[\to\mathbb{R}\cup\{\infty\}$ given by $$F(t)=\int_{C_t\cap E}\vert f\vert$$ is continuous. My idea seems to have no future, because for example $f$ could be $\infty$ in any subset of positive measure. If this is not possible, how can I approach this problem?",,"['analysis', 'measure-theory']"
64,Separability of $l^\infty$,Separability of,l^\infty,"I am trying to prove that $l^\infty$ is not separable.  However, I have proved it is separable.  Can you help find the flaw in my logic? (In case my teacher uses different definitions from everyone else, $l^\infty$ is defined here to be the set of all bounded sequences, and the metric used is $d((x_n), (y_n)) = sup\{|x_n - y_n|\}$.) My logic is consider $A$, the set of all bounded sequences with all terms in $\mathbb{Q}$.  We show that all bounded sequences $(x_n)$ are in the closure of this set.  To show this, we must show that for all $\epsilon$, an $\epsilon$-ball around $(x_n)$ intersects $A$.  But since you can find a rational number arbitrarily close to any number in the reals, you can just find a rational sequence $(a_n)$ such that $|a_n - x_n| < \epsilon$ for all $n$.  Thus $d((x_n), (a_n)) < \epsilon$, and $B_\epsilon((x_n)) \cap A \neq \emptyset$ for all $\epsilon$.  Therefore $l^\infty$ is the closure of $A$, and since $A$ is countable, $l^\infty$ is separable. Can you help find the flaw in my logic? Thank you!","I am trying to prove that $l^\infty$ is not separable.  However, I have proved it is separable.  Can you help find the flaw in my logic? (In case my teacher uses different definitions from everyone else, $l^\infty$ is defined here to be the set of all bounded sequences, and the metric used is $d((x_n), (y_n)) = sup\{|x_n - y_n|\}$.) My logic is consider $A$, the set of all bounded sequences with all terms in $\mathbb{Q}$.  We show that all bounded sequences $(x_n)$ are in the closure of this set.  To show this, we must show that for all $\epsilon$, an $\epsilon$-ball around $(x_n)$ intersects $A$.  But since you can find a rational number arbitrarily close to any number in the reals, you can just find a rational sequence $(a_n)$ such that $|a_n - x_n| < \epsilon$ for all $n$.  Thus $d((x_n), (a_n)) < \epsilon$, and $B_\epsilon((x_n)) \cap A \neq \emptyset$ for all $\epsilon$.  Therefore $l^\infty$ is the closure of $A$, and since $A$ is countable, $l^\infty$ is separable. Can you help find the flaw in my logic? Thank you!",,['analysis']
65,Step functions and the characteristic function of rationals,Step functions and the characteristic function of rationals,,"A function $t: [a, b] \rightarrow \mathbb{R}$ is called a step function when a $k \in \mathbb{N}$ and numbers $z_0,...,z_k$ with $a = z_0 \leq z_1 \leq ... \leq z_k = b$ exist, such that for all $i \in \{1,2,...k\}$ the restriction $t |_{(z_{i-1},z_{i})}$ is constant. Let $f: [0,1] \rightarrow \mathbb{R}$ be defined by $$f(x) = \left\{ \begin{array}{rcl} 1, & \mbox{if} & x \in \mathbb{Q} \\ 0, & \mbox{if} & x \notin \mathbb{Q} \end{array} \right.$$ Show: (i) The function $f$ is a point-wise limit of step functions. (ii) There is no uniform convergent series of step functions, whose limit-function is $f$. So the book I'm reading mentions this Dirichlet function all the time. Still I'm having trouble finding a solution to this exercise. All help is very much appreciated!","A function $t: [a, b] \rightarrow \mathbb{R}$ is called a step function when a $k \in \mathbb{N}$ and numbers $z_0,...,z_k$ with $a = z_0 \leq z_1 \leq ... \leq z_k = b$ exist, such that for all $i \in \{1,2,...k\}$ the restriction $t |_{(z_{i-1},z_{i})}$ is constant. Let $f: [0,1] \rightarrow \mathbb{R}$ be defined by $$f(x) = \left\{ \begin{array}{rcl} 1, & \mbox{if} & x \in \mathbb{Q} \\ 0, & \mbox{if} & x \notin \mathbb{Q} \end{array} \right.$$ Show: (i) The function $f$ is a point-wise limit of step functions. (ii) There is no uniform convergent series of step functions, whose limit-function is $f$. So the book I'm reading mentions this Dirichlet function all the time. Still I'm having trouble finding a solution to this exercise. All help is very much appreciated!",,['analysis']
66,Lower bound of sum of cosine of angle difference,Lower bound of sum of cosine of angle difference,,"Let $n\geq 4$ . (1) why $$\sum_{i\neq j\in[n]}\cos^2(\theta_i-\theta_j)\geq n^2/2$$ holds? Note that this corresponds to the formula $$n^2/2\leq \|X\|_F^2$$ in the paper, where $X$ is matrix with $X_{ij}=\cos(\theta_i-\theta_j)$ . Note that $i<j$ , $i>j$ are considered separatly thus there are $n^2-n$ terms in the summation. (2) does it also hold $$\sum_{i\neq j\in[n]}\cos(\theta_i-\theta_j)\geq n^2/2$$","Let . (1) why holds? Note that this corresponds to the formula in the paper, where is matrix with . Note that , are considered separatly thus there are terms in the summation. (2) does it also hold",n\geq 4 \sum_{i\neq j\in[n]}\cos^2(\theta_i-\theta_j)\geq n^2/2 n^2/2\leq \|X\|_F^2 X X_{ij}=\cos(\theta_i-\theta_j) i<j i>j n^2-n \sum_{i\neq j\in[n]}\cos(\theta_i-\theta_j)\geq n^2/2,['analysis']
67,"Integral inequality : for $f\in C^{4}[0,1]$ with $f(0)=f(1)=f^\prime(0)=f^\prime(1)=0$, show that $\int_0^1|\frac{f^{(4)}(x)}{f(x)}|dx\geq 192$","Integral inequality : for  with , show that","f\in C^{4}[0,1] f(0)=f(1)=f^\prime(0)=f^\prime(1)=0 \int_0^1|\frac{f^{(4)}(x)}{f(x)}|dx\geq 192","I was interested in the problem stated below: Problem 0 . $f\in C^{4}[0,1]$ with $f(0)=f(1)=f^\prime(0)=f^\prime(1)=0$ , show that $\int_0^1|\frac{f^{(4)}(x)}{f(x)}|dx\geq 192$ This problem is assigned as supplementary problems based on the easier one : Problem 1 : $f\in C^{2}[0,1]$ with $f(0)=f(1)=0$ , show that $\int_0^1|\frac{f^{\prime\prime}(x)}{f(x)}|dx\geq 4$ I proved the easier one in this way: Proof of Problem 1: $$\int_{[0,1]}|f''(x)|dx\geq \int_{[\theta_1,\theta_2]}|f''(x)|dx\geq \left|\int_{[\theta_1,\theta_2]}f''(x)dx\right|=|f^\prime(\theta_2)-f^\prime(\theta_1)|$$ As $f$ is continuous on a compact set, we can find $x_0$ such that $f(x_0)>f(x)$ for any $x\in [0,1]$ . By the mean value theorem, assign $\theta_1$ and $\theta_2$ above to be: $$\frac{f(x_0)-f(0)}{x_0-0}=f^\prime(\theta_1);\quad \frac{f(x_0)-f(1)}{x_0-1}=f^\prime(\theta_2)$$ Then we have: $$|f^\prime(\theta_2)-f^\prime(\theta_1)|=\left|\frac{f(x_0)}{x_0(1-x_0)}\right|\geq 4|f(x_0)|$$ In sum, we have: $$\int_{[0,1]}\left|\frac{f^{\prime\prime}(x)}{f(x)}\right|\geq \int_{[0,1]}\left|\frac{f^{\prime\prime}(x)}{f(x_0)}\right|\geq 4$$ However for Problem 0 , I tried MVT and Taylor expansions on $0$ and $1$ but find no clues to continue. Any reasonable thoughts are welcome! Thank you!","I was interested in the problem stated below: Problem 0 . with , show that This problem is assigned as supplementary problems based on the easier one : Problem 1 : with , show that I proved the easier one in this way: Proof of Problem 1: As is continuous on a compact set, we can find such that for any . By the mean value theorem, assign and above to be: Then we have: In sum, we have: However for Problem 0 , I tried MVT and Taylor expansions on and but find no clues to continue. Any reasonable thoughts are welcome! Thank you!","f\in C^{4}[0,1] f(0)=f(1)=f^\prime(0)=f^\prime(1)=0 \int_0^1|\frac{f^{(4)}(x)}{f(x)}|dx\geq 192 f\in C^{2}[0,1] f(0)=f(1)=0 \int_0^1|\frac{f^{\prime\prime}(x)}{f(x)}|dx\geq 4 \int_{[0,1]}|f''(x)|dx\geq \int_{[\theta_1,\theta_2]}|f''(x)|dx\geq \left|\int_{[\theta_1,\theta_2]}f''(x)dx\right|=|f^\prime(\theta_2)-f^\prime(\theta_1)| f x_0 f(x_0)>f(x) x\in [0,1] \theta_1 \theta_2 \frac{f(x_0)-f(0)}{x_0-0}=f^\prime(\theta_1);\quad \frac{f(x_0)-f(1)}{x_0-1}=f^\prime(\theta_2) |f^\prime(\theta_2)-f^\prime(\theta_1)|=\left|\frac{f(x_0)}{x_0(1-x_0)}\right|\geq 4|f(x_0)| \int_{[0,1]}\left|\frac{f^{\prime\prime}(x)}{f(x)}\right|\geq \int_{[0,1]}\left|\frac{f^{\prime\prime}(x)}{f(x_0)}\right|\geq 4 0 1","['analysis', 'numerical-methods', 'taylor-expansion', 'numerical-calculus', 'mean-value-theorem']"
68,Is the study on the theory of the integral of Denjoy's and Perron's an active field nowadays?,Is the study on the theory of the integral of Denjoy's and Perron's an active field nowadays?,,"I am an undergraduate student knowing the basic definition on the integral of Denjoy's and Perron's. I wonder if this topic is still active nowadays. Is the integral of Denjoy's and Perron's worth devoting time to study for a student planning to read a master/ PhD degree in analysis? Is the study on the theory of the integral of Denjoy's and Perron's an active field nowadays? What problem interests researchers on these two integrals most? Is there some ""big"" open problems students with the knowledge of measure theory and functional analysis can understand? What branch of analysis uses the integral of Denjoy's and Perron's frequently?","I am an undergraduate student knowing the basic definition on the integral of Denjoy's and Perron's. I wonder if this topic is still active nowadays. Is the integral of Denjoy's and Perron's worth devoting time to study for a student planning to read a master/ PhD degree in analysis? Is the study on the theory of the integral of Denjoy's and Perron's an active field nowadays? What problem interests researchers on these two integrals most? Is there some ""big"" open problems students with the knowledge of measure theory and functional analysis can understand? What branch of analysis uses the integral of Denjoy's and Perron's frequently?",,"['real-analysis', 'integration', 'analysis', 'soft-question']"
69,Prove $x^2 + x \cos (x) -2 \cos ^2 (x) = 0$ has exactly two real roots,Prove  has exactly two real roots,x^2 + x \cos (x) -2 \cos ^2 (x) = 0,"Given the equation $$x^2 + x \cos (x) - 2 \cos ^2 (x) = 0$$ prove it has exactly two real roots. My attempt In order two prove the equation indeed just has two real roots, first of all I need to prove they exists and then that they are the only ones. For the existance part notice that denoting $f(x) = x^2 + x \cos (x) - 2 \cos ^2 (x) = (x-\cos (x))(x+2\cos (x))$ $$f(-2) = 0.0036 > 0, f(0) = -2 < 0, f(1) = 0.00045 > 0$$ By Bolzano's theorem, there is at least one root within each interval $(-2,0)$ and $(0,1)$ Where I am stuck is with the uniqueness part. I tried to find the derivate $$f'(x) = 2x + \cos (x) - x \sin (x) + 4 \cos (x) \sin (x) = 2x + \cos (x) - x \sin (x) + 2\sin (2x)$$ and putting it equal to $0$ : $$2x + \cos (x) - x \sin (x) + 2\sin (2x) = 0$$ If I see that this equation only has a root (which is true, checked by graphing it), then by Rolle's theorem a function at most has one root more than it derivative; therefore the two roots I found are the only ones. However, this is where I got stuck since I can't figure out how to solve this new equation. I think I may be going in the wrong direction, but I can't think of other way to solve the problem. Any help is truly appreciated.","Given the equation prove it has exactly two real roots. My attempt In order two prove the equation indeed just has two real roots, first of all I need to prove they exists and then that they are the only ones. For the existance part notice that denoting By Bolzano's theorem, there is at least one root within each interval and Where I am stuck is with the uniqueness part. I tried to find the derivate and putting it equal to : If I see that this equation only has a root (which is true, checked by graphing it), then by Rolle's theorem a function at most has one root more than it derivative; therefore the two roots I found are the only ones. However, this is where I got stuck since I can't figure out how to solve this new equation. I think I may be going in the wrong direction, but I can't think of other way to solve the problem. Any help is truly appreciated.","x^2 + x \cos (x) - 2 \cos ^2 (x) = 0 f(x) = x^2 + x \cos (x) - 2 \cos ^2 (x) = (x-\cos (x))(x+2\cos (x)) f(-2) = 0.0036 > 0, f(0) = -2 < 0, f(1) = 0.00045 > 0 (-2,0) (0,1) f'(x) = 2x + \cos (x) - x \sin (x) + 4 \cos (x) \sin (x) = 2x + \cos (x) - x \sin (x) + 2\sin (2x) 0 2x + \cos (x) - x \sin (x) + 2\sin (2x) = 0","['real-analysis', 'analysis', 'continuity', 'numerical-methods', 'rolles-theorem']"
70,A counting function that is Borel measurable,A counting function that is Borel measurable,,"Question: Let $F:\mathbb{R^2\to R}$ be a continuous function. Define $p(x)$ as the number of $y$ such that $F(x,y)=0$ ,i.e. $p(x)=\#\{y\in\mathbb{R}|F(x,y)=0\}$ . Prove that $p(x)$ is (extended) Borel measurable(as $p(x)$ valued in $[0,+\infty]$ ). Attempt: It's not hard to see that $\{(x,y)|F(x,y)=0\}$ and $\{y|F(x_0,y)=0\}$ are both closed set.But I don't know how to deal with the 'counting function' $p(x).$ Thanks in advance!","Question: Let be a continuous function. Define as the number of such that ,i.e. . Prove that is (extended) Borel measurable(as valued in ). Attempt: It's not hard to see that and are both closed set.But I don't know how to deal with the 'counting function' Thanks in advance!","F:\mathbb{R^2\to R} p(x) y F(x,y)=0 p(x)=\#\{y\in\mathbb{R}|F(x,y)=0\} p(x) p(x) [0,+\infty] \{(x,y)|F(x,y)=0\} \{y|F(x_0,y)=0\} p(x).","['real-analysis', 'analysis', 'measure-theory']"
71,A problem on compositions of functions,A problem on compositions of functions,,"$$f(x) = \begin{cases} x-1 & x \geq 0 \\ 1-x & x <0 \end{cases}$$ $$g(x) = \begin{cases} x& x\geq 0 \\ x^2 & x < 0 \end{cases}$$ It asks me for the compositions $f(g(x))$ . What I did: it seemed easy to me, since $$f(g(x)) = \begin{cases} x-1 & x \geq 0 \\ 1-x^2 & x < 0 \end{cases}$$ $$g(f(x)) = \begin{cases} x-1 & x \geq 0 \\ (1-x)^2 & x < 0 \end{cases}$$ Instead in the solution there is: $$g(f(x)) = \begin{cases} 1-x & x < 0 \\ (x-1)^2 & 0 \leq x < 1 \\ x-1 & x \geq 1 \end{cases}$$ I don't get that solution. Some clarifications?","It asks me for the compositions . What I did: it seemed easy to me, since Instead in the solution there is: I don't get that solution. Some clarifications?",f(x) = \begin{cases} x-1 & x \geq 0 \\ 1-x & x <0 \end{cases} g(x) = \begin{cases} x& x\geq 0 \\ x^2 & x < 0 \end{cases} f(g(x)) f(g(x)) = \begin{cases} x-1 & x \geq 0 \\ 1-x^2 & x < 0 \end{cases} g(f(x)) = \begin{cases} x-1 & x \geq 0 \\ (1-x)^2 & x < 0 \end{cases} g(f(x)) = \begin{cases} 1-x & x < 0 \\ (x-1)^2 & 0 \leq x < 1 \\ x-1 & x \geq 1 \end{cases},"['calculus', 'analysis', 'elementary-functions']"
72,generalized vector multiplication without identity element,generalized vector multiplication without identity element,,"Is it possible to define (ideally infinitely) differentiable functions $f_n : \mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R}^n$ and $g_n : \mathbb{R}^n \times (\mathbb{R}^n \setminus \{0\}) \rightarrow \mathbb{R}^n$ where $f_n(x, g_n(y, x)) = y$ and $f_n(x, y) = 0 \iff (x = 0 \vee y = 0)$ ? It's possible for $n = 1$ using $f_1(x, y) = xy$ and $g_1(x, y) = \frac{x}{y}$ . By interpretation of $\mathbb{R}^2$ as complex numbers, it's also possible for $n = 2$ using $f_2(x, y) = \left( \begin{array}{c}  Re (x_1 + x_2 i) (y_1 + y_2 i) \\  Im (x_1 + x_2 i) (y_1 + y_2 i) \\ \end{array} \right)$ and $g_2(x, y) = \left( \begin{array}{c}  Re \frac{x_1 + x_2 i}{y_1 + y_2 i} \\  Im \frac{x_1 + x_2 i}{y_1 + y_2 i} \\ \end{array} \right)$ . $n = 4$ should also work using a similar approach using quaternions. But is it possible for the general case? I'm not sure whether this is related to this ([1]) question since it might not be necessary to have full algebraic characteristics in order to define $f_n$ and $g_n$ . [1] Is there a third dimension of numbers?","Is it possible to define (ideally infinitely) differentiable functions and where and ? It's possible for using and . By interpretation of as complex numbers, it's also possible for using and . should also work using a similar approach using quaternions. But is it possible for the general case? I'm not sure whether this is related to this ([1]) question since it might not be necessary to have full algebraic characteristics in order to define and . [1] Is there a third dimension of numbers?","f_n : \mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R}^n g_n : \mathbb{R}^n \times (\mathbb{R}^n \setminus \{0\}) \rightarrow \mathbb{R}^n f_n(x, g_n(y, x)) = y f_n(x, y) = 0 \iff (x = 0 \vee y = 0) n = 1 f_1(x, y) = xy g_1(x, y) = \frac{x}{y} \mathbb{R}^2 n = 2 f_2(x, y) = \left(
\begin{array}{c}
 Re (x_1 + x_2 i) (y_1 + y_2 i) \\
 Im (x_1 + x_2 i) (y_1 + y_2 i) \\
\end{array}
\right) g_2(x, y) = \left(
\begin{array}{c}
 Re \frac{x_1 + x_2 i}{y_1 + y_2 i} \\
 Im \frac{x_1 + x_2 i}{y_1 + y_2 i} \\
\end{array}
\right) n = 4 f_n g_n","['abstract-algebra', 'analysis']"
73,How can i write $f(x)=\cos(x)$ as the difference of two monotonically increasing functions?,How can i write  as the difference of two monotonically increasing functions?,f(x)=\cos(x),This is a Question from an Analysis 1 exam. The question is as follows: Decide if the functions $f: \mathbb{R} \longrightarrow \mathbb{R}$ can be written as the difference of two monotonically increasing functions a) $f(x) = \cos(x)$ b) $f(x) = x^2$ For the moment I’m working on a) my first thought would be to use the MVT and receive something in the form of $\cos(x)+2x = -\sin(x)-2x$ but as we see - $\sin(x)$ is not monotonically increasing. Obviously one could also answer with $\cos(x) = (\cos(x)+2x) - 2x$ but I fear this answer would not be accepted by my professor. If you have any tips or answers for either a) or b) id be grateful,This is a Question from an Analysis 1 exam. The question is as follows: Decide if the functions can be written as the difference of two monotonically increasing functions a) b) For the moment I’m working on a) my first thought would be to use the MVT and receive something in the form of but as we see - is not monotonically increasing. Obviously one could also answer with but I fear this answer would not be accepted by my professor. If you have any tips or answers for either a) or b) id be grateful,f: \mathbb{R} \longrightarrow \mathbb{R} f(x) = \cos(x) f(x) = x^2 \cos(x)+2x = -\sin(x)-2x \sin(x) \cos(x) = (\cos(x)+2x) - 2x,['analysis']
74,"Can a pair of complex numbers be ""completely"" algebraically independent?","Can a pair of complex numbers be ""completely"" algebraically independent?",,"Two complex numbers $\alpha,\beta$ are called algebraically independent if there is no polynomial $p(x,y)\in\mathbb{Q}[x,y]$ such that $p(\alpha,\beta)=0$ . For a complex number $x$ , let $A(x)=\{z\in\mathbb{C}:x \text{ is algebraic over } \mathbb{Q}(z)\}$ Can there exist a pair of complex numbers $x,y$ such that $A(x)\cap A(y)$ is empty? Such a pair would also be algebraically independent. A necessary condition for it to be empty is that both of them must be transcendental over the rationals. For all $x\in\mathbb{C}$ , $x$ is contained in $A(x)$ , so $A(x)$ is never empty. If $x$ is algebraic over the rationals, then $A(x)$ would be the whole of $\mathbb{C}$ . It follows that if either $x$ or $y$ is algebraic over the rationals, then the intersection would not be empty.","Two complex numbers are called algebraically independent if there is no polynomial such that . For a complex number , let Can there exist a pair of complex numbers such that is empty? Such a pair would also be algebraically independent. A necessary condition for it to be empty is that both of them must be transcendental over the rationals. For all , is contained in , so is never empty. If is algebraic over the rationals, then would be the whole of . It follows that if either or is algebraic over the rationals, then the intersection would not be empty.","\alpha,\beta p(x,y)\in\mathbb{Q}[x,y] p(\alpha,\beta)=0 x A(x)=\{z\in\mathbb{C}:x \text{ is algebraic over } \mathbb{Q}(z)\} x,y A(x)\cap A(y) x\in\mathbb{C} x A(x) A(x) x A(x) \mathbb{C} x y","['analysis', 'field-theory', 'transcendental-numbers']"
75,"Reference Book - Analysis with a ""General View""","Reference Book - Analysis with a ""General View""",,"When studying Analysis (Real, $\mathbb R^n$ , Metric ...) it's common to see similar (or even the same) theorem, sometimes with the same (or similar) proof. I was wondering if there is any book out there that tries to present ""Analysis"" with results from a more general spaces to more specific spaces. For example, if a result is valid for every complete metric space, then it's presented and proved with this generic view. While, if the result is specific to $\mathbb R$ , then the result is shown for that case. I understand that this might be perhaps too much to ask, but you never know. Maybe someone wrote such book already. Just to make clear where this comes from. I'm writing some notes on courses I've taken as a graduate student, and I was trying to organize my notes from ""more generic to more specific"". Hence, I started with stuff like Topological Spaces, Metric Spaces, and moved to Complex and Euclidean Spaces. I realized that many of the results I proved for Euclidean Spaces were pretty much the same in more general spaces, and I was repeating a bunch of my theorems, with slight changes.","When studying Analysis (Real, , Metric ...) it's common to see similar (or even the same) theorem, sometimes with the same (or similar) proof. I was wondering if there is any book out there that tries to present ""Analysis"" with results from a more general spaces to more specific spaces. For example, if a result is valid for every complete metric space, then it's presented and proved with this generic view. While, if the result is specific to , then the result is shown for that case. I understand that this might be perhaps too much to ask, but you never know. Maybe someone wrote such book already. Just to make clear where this comes from. I'm writing some notes on courses I've taken as a graduate student, and I was trying to organize my notes from ""more generic to more specific"". Hence, I started with stuff like Topological Spaces, Metric Spaces, and moved to Complex and Euclidean Spaces. I realized that many of the results I proved for Euclidean Spaces were pretty much the same in more general spaces, and I was repeating a bunch of my theorems, with slight changes.",\mathbb R^n \mathbb R,"['analysis', 'reference-request']"
76,"Understanding $\lim\limits_{m(B) \rightarrow 0, x \in B}$ (Lebesgue Differentiation)",Understanding  (Lebesgue Differentiation),"\lim\limits_{m(B) \rightarrow 0, x \in B}","My question concerns the following excerpt from section 3.1 (Differentiation of the Lebesgue Integral) of Stein and Shakarchi's Real Analysis : Suppose $f$ is integrable on $\mathbb{R}^d$ . Is it true that $$\lim\limits_{m(B) \rightarrow 0 \\ x \in B} \frac{1}{m(B)} \int_B f(y) \,dy = f(x), \quad \text{for a.e. } x?\label{1}\tag{$\ast$} $$ The limit is taken as the volume of open balls $B$ containing $x$ goes to $0$ . My question is: what is the precise meaning of $\lim\limits_{m(B) \rightarrow 0, \, x \in B}$ ? I am familiar with the following notions of limits: If $(a_n)_{n=1}^{\infty}$ is a sequence, then  "" $\lim\limits_{n \to \infty} a_n = a$ "" means: for each $\epsilon > 0$ , there exists some $N \in \mathbb{N}$ such that $|a_n - a| < \epsilon$ whenever $n \geq N$ . If $f: \mathbb{R}^d \rightarrow \mathbb{R}$ , then "" $\lim\limits_{x \rightarrow x_0} f(x) = y$ "" means: for each $\epsilon > 0$ , there is some $\delta > 0$ such that $|f(x) - y| < \epsilon$ whenever $\|x - x_0\| < \delta$ . But \eqref{1} does not seem to match either of the above notions of a limit. My best guess is that \eqref{1} means something like the following: $$\lim_{n \to \infty} \frac{1}{m(B_n)} \int_{B_n} f(y)\,dy = f(x) \quad \text{for a.e. } x$$ for every sequence of balls $\{B_n\}_{n=1}^{\infty}$ satisfying (1) $\lim\limits_{n \to \infty} m(B_n) = 0$ , and (2) $x \in B_n$ for all $n$ ...Is this correct?","My question concerns the following excerpt from section 3.1 (Differentiation of the Lebesgue Integral) of Stein and Shakarchi's Real Analysis : Suppose is integrable on . Is it true that The limit is taken as the volume of open balls containing goes to . My question is: what is the precise meaning of ? I am familiar with the following notions of limits: If is a sequence, then  "" "" means: for each , there exists some such that whenever . If , then "" "" means: for each , there is some such that whenever . But \eqref{1} does not seem to match either of the above notions of a limit. My best guess is that \eqref{1} means something like the following: for every sequence of balls satisfying (1) , and (2) for all ...Is this correct?","f \mathbb{R}^d \lim\limits_{m(B) \rightarrow 0 \\ x \in B} \frac{1}{m(B)} \int_B f(y) \,dy = f(x), \quad \text{for a.e. } x?\label{1}\tag{\ast}  B x 0 \lim\limits_{m(B) \rightarrow 0, \, x \in B} (a_n)_{n=1}^{\infty} \lim\limits_{n \to \infty} a_n = a \epsilon > 0 N \in \mathbb{N} |a_n - a| < \epsilon n \geq N f: \mathbb{R}^d \rightarrow \mathbb{R} \lim\limits_{x \rightarrow x_0} f(x) = y \epsilon > 0 \delta > 0 |f(x) - y| < \epsilon \|x - x_0\| < \delta \lim_{n \to \infty} \frac{1}{m(B_n)} \int_{B_n} f(y)\,dy = f(x) \quad \text{for a.e. } x \{B_n\}_{n=1}^{\infty} \lim\limits_{n \to \infty} m(B_n) = 0 x \in B_n n","['real-analysis', 'calculus', 'analysis', 'derivatives', 'lebesgue-integral']"
77,Counterexample for a differentiable structure,Counterexample for a differentiable structure,,"I'm trying to understand the definition of a differentiable structure. Is it correct that $x\rightarrow x$ and $x \rightarrow x^3$ doesn't form a diffeomorphism, since $x\rightarrow x^{1/3}$ isn't differentiable in $0$ ?","I'm trying to understand the definition of a differentiable structure. Is it correct that and doesn't form a diffeomorphism, since isn't differentiable in ?",x\rightarrow x x \rightarrow x^3 x\rightarrow x^{1/3} 0,['analysis']
78,Definition of Frobenius-Perron (transfer) operator,Definition of Frobenius-Perron (transfer) operator,,"I came across two definitions for the transfer operator. The two definitions seem to me to be very similar, but I couldn't deduce one of them from the other. The first definition is the classic one, which I have already found in several books. Let $(X,\mu)$ be a $\sigma$ -finite measure space. For $f\in L^{1}(X,\mu)$ , define a signed measure $\mu_{f}$ on $X$ by $$\mu_{f}(A)=\int_{T^{-1}(A)}f \ d\mu$$ for all measurable $A\subset X$ . By the Radon-Nikodym theorem, there exists a unique element $P_{T}f\in L^{1}(X,\mu)$ such that $$\mu_{f}(A)=\int_{A}P_{T}f \ d\mu$$ for all measurable $A\subset X$ . The operator $P_{T}\colon L^{1}(X,\mu)\to L^{1}(X,\mu)$ define by $$\int_A P_{T}f \ d\mu = \int_{T^{-1}(A)} f\ d\mu\qquad \forall f\in L^{1}(X,\mu)$$ is called the Perron-Frobenius or transfer operator . The second definition can be found, for example, in the book An introduction to infinite ergodic theory by  J. Aaronson. The transfer operator $\widehat{T}:L^1(X,\mu)\to L^1(X,\mu)$ is given by, $$\int_X g (\widehat{T}f)\ d\mu = \int_X (g\circ T)f\ d\mu \qquad \forall f\in L^1(X,\mu),\ g\in L^{\infty}(X,\mu)$$ this equality is deduced from the following equality, $$\widehat{T}f=\frac{d\nu_f \circ T^{-1}}{d\mu}\qquad \text{where } \nu_f (A)= \int_A f \ d\mu$$ So I would like to know if $P_{T}$ and $\widehat{T}$ are the same operator. It seems so, but I couldn't get to prove it.","I came across two definitions for the transfer operator. The two definitions seem to me to be very similar, but I couldn't deduce one of them from the other. The first definition is the classic one, which I have already found in several books. Let be a -finite measure space. For , define a signed measure on by for all measurable . By the Radon-Nikodym theorem, there exists a unique element such that for all measurable . The operator define by is called the Perron-Frobenius or transfer operator . The second definition can be found, for example, in the book An introduction to infinite ergodic theory by  J. Aaronson. The transfer operator is given by, this equality is deduced from the following equality, So I would like to know if and are the same operator. It seems so, but I couldn't get to prove it.","(X,\mu) \sigma f\in L^{1}(X,\mu) \mu_{f} X \mu_{f}(A)=\int_{T^{-1}(A)}f \ d\mu A\subset X P_{T}f\in L^{1}(X,\mu) \mu_{f}(A)=\int_{A}P_{T}f \ d\mu A\subset X P_{T}\colon L^{1}(X,\mu)\to L^{1}(X,\mu) \int_A P_{T}f \ d\mu = \int_{T^{-1}(A)} f\ d\mu\qquad \forall f\in L^{1}(X,\mu) \widehat{T}:L^1(X,\mu)\to L^1(X,\mu) \int_X g (\widehat{T}f)\ d\mu = \int_X (g\circ T)f\ d\mu \qquad \forall f\in L^1(X,\mu),\ g\in L^{\infty}(X,\mu) \widehat{T}f=\frac{d\nu_f \circ T^{-1}}{d\mu}\qquad \text{where } \nu_f (A)= \int_A f \ d\mu P_{T} \widehat{T}","['analysis', 'measure-theory', 'operator-theory', 'dynamical-systems', 'ergodic-theory']"
79,"Evaluating $\int_{0}^{1} \ln \left( \sec \left( \frac{\pi x}{2} \right) \right) \ln (\sin (\pi x)) \, dx$",Evaluating,"\int_{0}^{1} \ln \left( \sec \left( \frac{\pi x}{2} \right) \right) \ln (\sin (\pi x)) \, dx","Other than being generally interested in this integral, it also appears in my $\zeta$ approach to my question here. Is it possible to evaluate the following integral? I was thinking to perhaps use an infinite product approach for $\sec$ , however, I couldn’t really get far with it. $$\int_{0}^{1} \ln \left( \sec \left( \frac{\pi x}{2} \right) \right) \ln (\sin (\pi x)) \, dx$$ If this integral is possible, is it also possible to evaluate the following two other integrals? $$\int_{0}^{1} \ln \left( \sec \left( \frac{\pi x}{2} \right) \right)^2 \ln (\sin (\pi x)) \, dx$$ $$\int_{0}^{1} x \ln \left( \sec \left( \frac{\pi x}{2} \right) \right) \ln (\sin (\pi x)) \, dx$$","Other than being generally interested in this integral, it also appears in my approach to my question here. Is it possible to evaluate the following integral? I was thinking to perhaps use an infinite product approach for , however, I couldn’t really get far with it. If this integral is possible, is it also possible to evaluate the following two other integrals?","\zeta \sec \int_{0}^{1} \ln \left( \sec \left( \frac{\pi x}{2} \right) \right) \ln (\sin (\pi x)) \, dx \int_{0}^{1} \ln \left( \sec \left( \frac{\pi x}{2} \right) \right)^2 \ln (\sin (\pi x)) \, dx \int_{0}^{1} x \ln \left( \sec \left( \frac{\pi x}{2} \right) \right) \ln (\sin (\pi x)) \, dx","['calculus', 'integration', 'analysis', 'definite-integrals', 'trigonometric-integrals']"
80,Folland Theorem 1.14 extending premeasure to a measure,Folland Theorem 1.14 extending premeasure to a measure,,"1.14 Theorem Let $\mathcal A \subset \mathcal{P}(X)$ be an algebra, $\mu_0$ be a premeasure on $\mathcal A$ , and $\mathcal{M}$ the $\sigma$ -algebra generated by $\mathcal{A}$ . There exists a measure $\mu$ on $\mathcal{M}$ whose restriction to $\mathcal{A}$ is $\mu_0$ , namely $\mu=\mu^\star|\mathcal{M}$ where $\mu^\star$ is given by (1.12). If $v$ is another measure on $\mathcal{M}$ that extends $\mu_0$ , then $v(E)\leq \mu(E)$ for all $E\in \mathcal{M}$ , with equality when $\mu(E)\leq \infty$ . If $\mu_0$ is $\sigma$ -finite, then $\mu$ is the unique extension of $\mu_0$ to a measure on $\mathcal{M}$ . My question is on the second assertion for which the proof reads: As for the second assertion, if $E\in \mathcal{M}$ and $E\subset \bigcup_1^\infty A_j$ where $A_j\in \mathcal{A}$ , then $v(E)\leq \sum_1^\infty v(A_j)=\sum_1^\infty \mu_0(A_j)$ whence $v(E)\leq \mu(E)$ . The only step I can see is that by monotonicity, we have that $v(E)\leq v(\bigcup_1^\infty A_j)$ . I understand that $v=\mu_0$ on $\mathcal{A}$ however, how can I get to $v(E)\leq \sum_1^\infty v(A_j)=\sum_1^\infty \mu_0(A_j)$ and moreover, why does this imply that $v(E) \leq \mu(E)$ ?","1.14 Theorem Let be an algebra, be a premeasure on , and the -algebra generated by . There exists a measure on whose restriction to is , namely where is given by (1.12). If is another measure on that extends , then for all , with equality when . If is -finite, then is the unique extension of to a measure on . My question is on the second assertion for which the proof reads: As for the second assertion, if and where , then whence . The only step I can see is that by monotonicity, we have that . I understand that on however, how can I get to and moreover, why does this imply that ?",\mathcal A \subset \mathcal{P}(X) \mu_0 \mathcal A \mathcal{M} \sigma \mathcal{A} \mu \mathcal{M} \mathcal{A} \mu_0 \mu=\mu^\star|\mathcal{M} \mu^\star v \mathcal{M} \mu_0 v(E)\leq \mu(E) E\in \mathcal{M} \mu(E)\leq \infty \mu_0 \sigma \mu \mu_0 \mathcal{M} E\in \mathcal{M} E\subset \bigcup_1^\infty A_j A_j\in \mathcal{A} v(E)\leq \sum_1^\infty v(A_j)=\sum_1^\infty \mu_0(A_j) v(E)\leq \mu(E) v(E)\leq v(\bigcup_1^\infty A_j) v=\mu_0 \mathcal{A} v(E)\leq \sum_1^\infty v(A_j)=\sum_1^\infty \mu_0(A_j) v(E) \leq \mu(E),"['real-analysis', 'analysis', 'measure-theory']"
81,General Sobolev inequalities for $L^\infty$ norm,General Sobolev inequalities for  norm,L^\infty,"In Evans' book on PDEs , section 5.6.3 states the general Sobolev inequalities. Specifically, let $U$ be a bounded open subset of $\mathbb{R}^n$ with $C^1$ boundary. Assuming $u \in W^{k,p}(U)$ , and $k<\frac{n}{p}$ , the proof states that: Since $D^{\alpha}u \in L^p(U)$ for all $|\alpha| \leq k$ (using multi-index notation here), the Gagliardo-Nirenberg-Sobolev inequality implies: $$ ||D^{\beta} u ||_{L^{p^*}(U)} \leq C ||u||_{W^{k,p}(U)} \quad \text{if } |\beta| \leq k-1, $$ where $p^* = \frac{np}{n - p}$ is the Sobolev conjugate of $p$ if $1 \leq p < n$ . What does the equation above look like in case of $L^{\infty}$ for the norm on the left, i.e. when $p^* = \infty$ ? Using the definition of the Sobolev conjugate, this would lead to $p=n$ , but then how would you satisfy $k<\frac{n}{p}$ in that case? And how would the norm on the right look like? Also, is it possible to extend this inequality for $u \in C^{\infty}(U)$ ?","In Evans' book on PDEs , section 5.6.3 states the general Sobolev inequalities. Specifically, let be a bounded open subset of with boundary. Assuming , and , the proof states that: Since for all (using multi-index notation here), the Gagliardo-Nirenberg-Sobolev inequality implies: where is the Sobolev conjugate of if . What does the equation above look like in case of for the norm on the left, i.e. when ? Using the definition of the Sobolev conjugate, this would lead to , but then how would you satisfy in that case? And how would the norm on the right look like? Also, is it possible to extend this inequality for ?","U \mathbb{R}^n C^1 u \in W^{k,p}(U) k<\frac{n}{p} D^{\alpha}u \in L^p(U) |\alpha| \leq k 
||D^{\beta} u ||_{L^{p^*}(U)} \leq C ||u||_{W^{k,p}(U)} \quad \text{if } |\beta| \leq k-1,
 p^* = \frac{np}{n - p} p 1 \leq p < n L^{\infty} p^* = \infty p=n k<\frac{n}{p} u \in C^{\infty}(U)","['analysis', 'partial-differential-equations', 'normed-spaces', 'sobolev-spaces']"
82,"Prob. 15, Chap. 2, in Royden's REAL ANALYSIS: For any set of positive measure and $\epsilon > 0$, there are finitely many disjoint measurable sets ...","Prob. 15, Chap. 2, in Royden's REAL ANALYSIS: For any set of positive measure and , there are finitely many disjoint measurable sets ...",\epsilon > 0,"Here is Prob. 15, Chap. 2, in the book Real Analysis by H.L. Royden and P.M. Fitzpatrick, 4th edition: Show that if $E$ has finite measure and $\epsilon > 0$ , then $E$ is the disjoint union of a finite number of measurable sets, each of which has measure at most $\epsilon$ . Here $E \subset \mathbb{R}$ of course, and $m^*(E) < \infty$ , where $m^*(E)$ is the infimum of the the set of all the sums of the form $\sum_{k = 1}^\infty l \left( I_k \right)$ , where $\left\{ I_k \right\}_{k=1}^\infty$ is a countable collection of non-empty, bounded open intervals which cover $E$ and, for each $k$ , $l \left( I_k \right)$ denotes the length of the interval $I_k$ . How to proceed from here?","Here is Prob. 15, Chap. 2, in the book Real Analysis by H.L. Royden and P.M. Fitzpatrick, 4th edition: Show that if has finite measure and , then is the disjoint union of a finite number of measurable sets, each of which has measure at most . Here of course, and , where is the infimum of the the set of all the sums of the form , where is a countable collection of non-empty, bounded open intervals which cover and, for each , denotes the length of the interval . How to proceed from here?",E \epsilon > 0 E \epsilon E \subset \mathbb{R} m^*(E) < \infty m^*(E) \sum_{k = 1}^\infty l \left( I_k \right) \left\{ I_k \right\}_{k=1}^\infty E k l \left( I_k \right) I_k,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure']"
83,Showing two measures are equal up to scalar multiplication.,Showing two measures are equal up to scalar multiplication.,,"I've been stuck on the following question and was wondering if it'd be possible to get some help. Let $\mu, \nu$ be two finite Borel measures on a metric space $X$ s.t. $\mu << \nu$ and let $c>0$ and suppose that $\nu$ is doubling and that $$\lim_{r \to 0} \frac{\mu(B_r(x))}{\nu(B_r(x))} = c$$ for $\nu$ almost every $x\in X$ . By using the Vitali Covering theorem to show that $\mu = c\nu$ . (We assume the balls are closed) I've been able to prove this using Radon-Nikodym combined with the lebesgue differentiation theorem but wish to prove it solely using Vital's covering theorem but have been unsuccessful in even finding an appropriate Vitali cover. Theorem (Vitali Covering Theorem) Let X be a metric space and let $\nu$ be a doubling measure on $X$ and let $\mathcal{B}$ be a Vitali cover for $S \subset X$ then there exists a countable $\mathcal{B}' \subset \mathcal{B}$ such that all elements of $\mathcal{B}'$ are disjoint and $$\nu \bigg(S \setminus \bigcup_{B' \in \mathcal{B}'}B'\bigg)=0$$ Definition (Vitali Cover) Let $S \subset X$ then $\mathcal{B}$ a collection of closed balls such that $\forall \epsilon >0 , \forall x \in S$ there exists $B \in \mathcal{B}$ such that $x \in B$ and $\rm{rad}(B) < \epsilon$ then we call $\mathcal{B}$ a Vitali cover for S. Many Thanks.",I've been stuck on the following question and was wondering if it'd be possible to get some help. Let be two finite Borel measures on a metric space s.t. and let and suppose that is doubling and that for almost every . By using the Vitali Covering theorem to show that . (We assume the balls are closed) I've been able to prove this using Radon-Nikodym combined with the lebesgue differentiation theorem but wish to prove it solely using Vital's covering theorem but have been unsuccessful in even finding an appropriate Vitali cover. Theorem (Vitali Covering Theorem) Let X be a metric space and let be a doubling measure on and let be a Vitali cover for then there exists a countable such that all elements of are disjoint and Definition (Vitali Cover) Let then a collection of closed balls such that there exists such that and then we call a Vitali cover for S. Many Thanks.,"\mu, \nu X \mu << \nu c>0 \nu \lim_{r \to 0} \frac{\mu(B_r(x))}{\nu(B_r(x))} = c \nu x\in X \mu = c\nu \nu X \mathcal{B} S \subset X \mathcal{B}' \subset \mathcal{B} \mathcal{B}' \nu \bigg(S \setminus \bigcup_{B' \in \mathcal{B}'}B'\bigg)=0 S \subset X \mathcal{B} \forall \epsilon >0 , \forall x \in S B \in \mathcal{B} x \in B \rm{rad}(B) < \epsilon \mathcal{B}","['analysis', 'measure-theory', 'geometric-measure-theory']"
84,Prove $\frac{( 4n+5 ) ( ( 2n+2 )! ) ^2}{2}( \int_{-1}^1{f( x ) \text{d}x} ) ^2\le \int_{-1}^1{( f^{( 2n+2 )}( x ) )^2 \text{d}x}$,Prove,\frac{( 4n+5 ) ( ( 2n+2 )! ) ^2}{2}( \int_{-1}^1{f( x ) \text{d}x} ) ^2\le \int_{-1}^1{( f^{( 2n+2 )}( x ) )^2 \text{d}x},"Let $f \in C^{2n+2}[-1,1]$ , and $$ f\left( 0 \right) =f''\left( 0 \right) =\cdots =f^{\left( 2n+2 \right)}\left( 0 \right) =0 $$ Prove that $$ \frac{\left( 4n+5 \right) \left( \left( 2n+2 \right) ! \right) ^2}{2}\left( \int_{-1}^1{f\left( x \right) \text{d}x} \right) ^2\le \int_{-1}^1{\left( f^{\left( 2n+2 \right)}\left( x \right) \right)^2 \text{d}x} $$ I even don't know how to deal with the basic circumstances, such as when $n=0$ , we have $$ 10\left( \int_{-1}^1{f\left( x \right) \text{d}x} \right) ^2\le \int_{-1}^1{\left( f''\left( x \right) \right) ^2\text{d}x} $$ I tried to apply Cauchy–Schwarz inequality and Integration by parts , but they didn't work. Also I think the general formula may relate to Taylor series since it has factorials on the left side. Can anyone help?","Let , and Prove that I even don't know how to deal with the basic circumstances, such as when , we have I tried to apply Cauchy–Schwarz inequality and Integration by parts , but they didn't work. Also I think the general formula may relate to Taylor series since it has factorials on the left side. Can anyone help?","f \in C^{2n+2}[-1,1] 
f\left( 0 \right) =f''\left( 0 \right) =\cdots =f^{\left( 2n+2 \right)}\left( 0 \right) =0
 
\frac{\left( 4n+5 \right) \left( \left( 2n+2 \right) ! \right) ^2}{2}\left( \int_{-1}^1{f\left( x \right) \text{d}x} \right) ^2\le \int_{-1}^1{\left( f^{\left( 2n+2 \right)}\left( x \right) \right)^2 \text{d}x}
 n=0 
10\left( \int_{-1}^1{f\left( x \right) \text{d}x} \right) ^2\le \int_{-1}^1{\left( f''\left( x \right) \right) ^2\text{d}x}
","['real-analysis', 'analysis', 'inequality']"
85,Why is the cone measurable?,Why is the cone measurable?,,"Problem Denote by $\lambda_n$ the Lebesgue measure on $\mathbb {R}^n$ . Let $h \in \mathbb {R}_{>0}$ . Let $A \subset \mathbb {R}^{n-1}$ be Lebesgue-measurable with finite measure. Define the cone $C(A,h)$ as the union of all straight lines connecting a point in $A \times\{0\}$ with the point $(0, h) \in \mathbb {R}^{n-1} \times \mathbb{R}$ . Prove that $\lambda_n (C(A,h))=\frac{h}{n}\cdot \lambda_{n-1}(A)$ . Where I struggle Assuming that $C(A,h)$ is (Lebesgue-)measurable, this is a simple application of Fubini. How Fubini is used is all clear to me. However, why is $C(A,h)$ measurable? It seems one could roughly argue as follows: As $A$ and $[0,h]$ are measurable so is $A \times [0,h]$ . As $A \times [0,h]$ is measurable it is the union of a null set and a Borel set. $C(A,h)$ is the image of $A \times [0,h]$ under a certain diffeomorphism (c.f. Deducing a formula for the volume of a cone over a $J$-measurable set. ). Diffeomorphisms map null sets to null sets, and Borel sets to Borel sets. Hence, $C(A,h)$ is (as a finite union of measurable sets) measurable. Unfortunately, I am not allowed to use the above properties of diffeomorphisms. What are other ways to prove the claim? Please only give a hint to set me thinking.","Problem Denote by the Lebesgue measure on . Let . Let be Lebesgue-measurable with finite measure. Define the cone as the union of all straight lines connecting a point in with the point . Prove that . Where I struggle Assuming that is (Lebesgue-)measurable, this is a simple application of Fubini. How Fubini is used is all clear to me. However, why is measurable? It seems one could roughly argue as follows: As and are measurable so is . As is measurable it is the union of a null set and a Borel set. is the image of under a certain diffeomorphism (c.f. Deducing a formula for the volume of a cone over a $J$-measurable set. ). Diffeomorphisms map null sets to null sets, and Borel sets to Borel sets. Hence, is (as a finite union of measurable sets) measurable. Unfortunately, I am not allowed to use the above properties of diffeomorphisms. What are other ways to prove the claim? Please only give a hint to set me thinking.","\lambda_n \mathbb {R}^n h \in \mathbb {R}_{>0} A \subset \mathbb {R}^{n-1} C(A,h) A \times\{0\} (0, h) \in \mathbb {R}^{n-1} \times \mathbb{R} \lambda_n (C(A,h))=\frac{h}{n}\cdot \lambda_{n-1}(A) C(A,h) C(A,h) A [0,h] A \times [0,h] A \times [0,h] C(A,h) A \times [0,h] C(A,h)","['analysis', 'lebesgue-integral', 'lebesgue-measure', 'geometric-measure-theory', 'fubini-tonelli-theorems']"
86,The concept of differential and derivative,The concept of differential and derivative,,"The concept of derivative and differential has always caused me confusion. So, I was reviewing analysis of several variables, and in a book I found the following definition Let $X, Y$ be Banach space, $U\subset X$ an open set and $f: U\to Y$ an application, differentiable at a point $a\in U$ . The (unique) linear application $A: X\to Y$ , which satisfies \eqref{eq1} is called the derivative of $f$ at point $a$ , denoted by $df(a):= A: X\to Y$ . If $f$ is differentiable at any point in $U$ , then the application $$df:U\to\mathcal{L}(X,Y) \qquad a\mapsto df(a)$$ is called the differential of $f$ . $$\forall\;\varepsilon>0\;\exists\:\delta>0\;\forall\: h\in X, \text{ such that } 0<\left\lVert h  \right\lVert_{X}<\delta \Rightarrow a+h\in U \text{ and } \frac{\left\lVert f(a+h)-f(a)-Ah \right\lVert_{Y}}{\left\lVert h\right\lVert_{X}}<\varepsilon\tag{1}\label{eq1}$$ Through the site there are several posts about the difference between derivative and differential, to to quote a few (as a reference): What is the practical difference between a differential and a derivative? Differential vs Derivative Are the differential and derivative of a single-variable function exactly the same thing? But despite the good answers, I was still having trouble understanding the difference between these two concepts, until I came across the definition above. So, I would like to see if I really understand these concepts (my interest is restricted to functions $f:\mathbb{R}^m\to\mathbb{R}^n$ ). I thought of the following example: Let $f:\mathbb{R}^2\to\mathbb{R}^2$ , be defined by $f(x,y)=e^x(\cos y,\sin y)$ . So, the differential is given by $$df:\mathbb{R}^2\to\mathcal{L}(\mathbb{R}^2 ,\mathbb{R}^2)\quad\text{where}\quad df=     \begin{pmatrix}     e^x\cos y & -e^x\sin y \\     e^x\sin y & e^x\cos y \\     \end{pmatrix} $$ And the derivative "" only makes sense "" to speak, when talking about a derivative at a point, so, considering the point $(0,2\pi)$ , the derivative at this point is given by $$df(0,2\pi)=     \begin{pmatrix}     1 & 0 \\     0 & 1 \\     \end{pmatrix} $$ which is a linear transformation from $\mathbb{R}^2$ to $\mathbb{R}^2$ . I would like to know if my example is correct, that is, if I managed to understand the difference between these concepts.","The concept of derivative and differential has always caused me confusion. So, I was reviewing analysis of several variables, and in a book I found the following definition Let be Banach space, an open set and an application, differentiable at a point . The (unique) linear application , which satisfies \eqref{eq1} is called the derivative of at point , denoted by . If is differentiable at any point in , then the application is called the differential of . Through the site there are several posts about the difference between derivative and differential, to to quote a few (as a reference): What is the practical difference between a differential and a derivative? Differential vs Derivative Are the differential and derivative of a single-variable function exactly the same thing? But despite the good answers, I was still having trouble understanding the difference between these two concepts, until I came across the definition above. So, I would like to see if I really understand these concepts (my interest is restricted to functions ). I thought of the following example: Let , be defined by . So, the differential is given by And the derivative "" only makes sense "" to speak, when talking about a derivative at a point, so, considering the point , the derivative at this point is given by which is a linear transformation from to . I would like to know if my example is correct, that is, if I managed to understand the difference between these concepts.","X, Y U\subset X f: U\to Y a\in U A: X\to Y f a df(a):= A: X\to Y f U df:U\to\mathcal{L}(X,Y) \qquad a\mapsto df(a) f \forall\;\varepsilon>0\;\exists\:\delta>0\;\forall\: h\in X, \text{ such that } 0<\left\lVert h  \right\lVert_{X}<\delta \Rightarrow a+h\in U \text{ and } \frac{\left\lVert f(a+h)-f(a)-Ah \right\lVert_{Y}}{\left\lVert h\right\lVert_{X}}<\varepsilon\tag{1}\label{eq1} f:\mathbb{R}^m\to\mathbb{R}^n f:\mathbb{R}^2\to\mathbb{R}^2 f(x,y)=e^x(\cos y,\sin y) df:\mathbb{R}^2\to\mathcal{L}(\mathbb{R}^2 ,\mathbb{R}^2)\quad\text{where}\quad df=
    \begin{pmatrix}
    e^x\cos y & -e^x\sin y \\
    e^x\sin y & e^x\cos y \\
    \end{pmatrix}
 (0,2\pi) df(0,2\pi)=
    \begin{pmatrix}
    1 & 0 \\
    0 & 1 \\
    \end{pmatrix}
 \mathbb{R}^2 \mathbb{R}^2","['calculus', 'analysis', 'multivariable-calculus', 'derivatives', 'terminology']"
87,Why is this integral bounded?,Why is this integral bounded?,,"Someone told me earlier that $$\int_{\delta}^{\infty} u^{\beta} e^{-cu^{2}} du <\infty$$ for positive $\beta$ and $c$ . How could I prove this is true? The only way I can think of to do this is by recognizing it as an incomplete gamma function, but is there any another way?","Someone told me earlier that for positive and . How could I prove this is true? The only way I can think of to do this is by recognizing it as an incomplete gamma function, but is there any another way?",\int_{\delta}^{\infty} u^{\beta} e^{-cu^{2}} du <\infty \beta c,"['integration', 'analysis', 'inequality']"
88,"If $\int_1^xf(t)\,\mathrm{d}t\leqslant f^2(x)$ for all $x\geqslant 1$, prove that $f(x)\geqslant\frac{1}{2}\,(x-1).$","If  for all , prove that","\int_1^xf(t)\,\mathrm{d}t\leqslant f^2(x) x\geqslant 1 f(x)\geqslant\frac{1}{2}\,(x-1).","Let $f:[1,+\infty)\to \mathbb{R}$ be a positive, continuous function.    If $\int_1^xf(t)\,\mathrm{d}t\leqslant f^2(x)$ for all $x\geqslant 1$ , prove that $f(x)\geqslant\frac{1}{2}\,(x-1).$ Attempt. Usual procedures, like working on the monotonicity of the function $$g(x):=f(x)-\frac{1}{2}\,(x-1)$$ (in order to get $g$ increasing and therefore $g(x)\geqslant g(1)=f(1)>0$ for $x\geqslant 1$ ), do not work. Thanks for the help.","Let be a positive, continuous function.    If for all , prove that Attempt. Usual procedures, like working on the monotonicity of the function (in order to get increasing and therefore for ), do not work. Thanks for the help.","f:[1,+\infty)\to \mathbb{R} \int_1^xf(t)\,\mathrm{d}t\leqslant f^2(x) x\geqslant 1 f(x)\geqslant\frac{1}{2}\,(x-1). g(x):=f(x)-\frac{1}{2}\,(x-1) g g(x)\geqslant g(1)=f(1)>0 x\geqslant 1","['real-analysis', 'calculus', 'analysis', 'riemann-integration']"
89,Which mappings preserve convex bodies?,Which mappings preserve convex bodies?,,"Let $$f:\mathbb{R}^n\to\mathbb{R}^n,$$ $n\geq 2$ , be a mapping which maps every convex body (compact convex set with nonempty interior) to a convex body. If we assume $f$ to be a homeomorphism, it needs to be affine. Is there something we can say without this assumption?","Let , be a mapping which maps every convex body (compact convex set with nonempty interior) to a convex body. If we assume to be a homeomorphism, it needs to be affine. Is there something we can say without this assumption?","f:\mathbb{R}^n\to\mathbb{R}^n, n\geq 2 f","['real-analysis', 'analysis']"
90,What if there are infinite stationary points?,What if there are infinite stationary points?,,"I want to calculate extremes of certain multivariable function $f(x,y)=(6−x−y)x^2y^3$ . After solving system of derivatives $f_x=0$ and $f_y=0$ I got something like this: $P_1=(x,0),x\in \mathbb R$ $P_2=(0,y),y\in \mathbb R$ $P_3=(2,3)$ First two conditions are satisfied with infinite number of $x$ and $y$ . How am I supposed to act in such situation? Do I have to check the first two points in some way? If so, how should I do this?","I want to calculate extremes of certain multivariable function . After solving system of derivatives and I got something like this: First two conditions are satisfied with infinite number of and . How am I supposed to act in such situation? Do I have to check the first two points in some way? If so, how should I do this?","f(x,y)=(6−x−y)x^2y^3 f_x=0 f_y=0 P_1=(x,0),x\in \mathbb R P_2=(0,y),y\in \mathbb R P_3=(2,3) x y","['analysis', 'extreme-value-analysis']"
91,"Bessel function integral: $\int_{0}^{\infty} e^{-ax}x^{m}\left(J_{0}(bx) \right)^{2} \, \mathrm dx$",Bessel function integral:,"\int_{0}^{\infty} e^{-ax}x^{m}\left(J_{0}(bx) \right)^{2} \, \mathrm dx","I am  working on a  problem. Solving the PDE for my problem, these Bessel integrals arise: $$\int^\infty_0 e^{-ax}x^m(J_0(bx))^2dx,\quad \int^\infty_0 e^{-ax}x^m(J_1(bx))^2dx\qquad \text{and} \qquad\int^\infty_0 e^{-ax}x^mJ_0(bx)J_1(bx)dx$$ where $~J_0~$ and $~J_1~$ are the Bessel functions of first kind. I haven't found the solution in any table or book, and due to my limited background in applied mathematics I don't know how to integrate it by myself. Does anybody know the solution? Thanks a lot in advance","I am  working on a  problem. Solving the PDE for my problem, these Bessel integrals arise: where and are the Bessel functions of first kind. I haven't found the solution in any table or book, and due to my limited background in applied mathematics I don't know how to integrate it by myself. Does anybody know the solution? Thanks a lot in advance","\int^\infty_0 e^{-ax}x^m(J_0(bx))^2dx,\quad \int^\infty_0 e^{-ax}x^m(J_1(bx))^2dx\qquad \text{and} \qquad\int^\infty_0 e^{-ax}x^mJ_0(bx)J_1(bx)dx ~J_0~ ~J_1~","['integration', 'analysis', 'improper-integrals', 'bessel-functions']"
92,Riemann integral of two same functions except at finitely many points.,Riemann integral of two same functions except at finitely many points.,,"Here is my proof. Theorem: If $f$ is Riemann integrable on $[a,b],c\in[a,b]$ , and $g(x)=f(x)$ except for finitely many points $c_1,\cdots,c_k$ in $[a,b]$ , then $g$ is Riemann integrable on $[a,b]$ , and $\int_{a}^{b}f(x)dx=\int_{a}^{b}g(x)dx.$ Suppose another function $g$ such that $g(x)=f(x)$ except for finitely many points $c_1,\cdots,c_k$ . Since $f$ is Riemann integrable, for any $\epsilon>0$ , there exists $\delta=\frac{\epsilon}{4n\sum_{i=1}^{k}|g(c_i)-f(c_i)|}$ such that for any partition $P=(a=x_0,\cdots,x_n=b)$ with $\left\lVert P\right\rVert<\delta$ , we have $\left|R(f,P)-\int_{a}^{b}f\right|<\frac{\epsilon}{2}.$ Now, \begin{align*} \left|R(g,P)-\int_{a}^{b}f\right|&=\left|R(g,P)-R(f,P)+R(f,P)-\int_{a}^{b}f\right|\\ &\leq\left|R(g,P)-R(f,P)\right|+\left|R(f,P)-\int_{a}^{b}f\right|\\ &<\left|R(g,P)-R(f,P)\right|+\frac{\epsilon}{2}\\ &<\frac{\epsilon}{2}+\frac{\epsilon}{2}=\epsilon.\\ \end{align*} where \begin{align*} |R(g,P)-R(f,P)|&=\left|\sum_{i=1}^{n}g(x_i*)(x_i-x_{i-1})-\sum_{i=1}^{n}f(x_i*)(x_i-x_{i-1})\right|\\ &=\left|\sum_{i=1}^{n}(g(x_i^*)-f(x_i^*))(x_i-x_{i-1})\right|\\ &<\sum_{i=1}^{k}|g(c_i)-f(c_i)|(2n\delta)\\ &=2n\sum_{i=1}^{k}|g(c_i)-f(c_i)|\left(\frac{\epsilon}{4n\sum_{i=1}^{k}|g(c_i)-f(c_i)|}\right)\\ &<\frac{\epsilon}{2} \end{align*} Is my proof of this theorem correct?","Here is my proof. Theorem: If is Riemann integrable on , and except for finitely many points in , then is Riemann integrable on , and Suppose another function such that except for finitely many points . Since is Riemann integrable, for any , there exists such that for any partition with , we have Now, where Is my proof of this theorem correct?","f [a,b],c\in[a,b] g(x)=f(x) c_1,\cdots,c_k [a,b] g [a,b] \int_{a}^{b}f(x)dx=\int_{a}^{b}g(x)dx. g g(x)=f(x) c_1,\cdots,c_k f \epsilon>0 \delta=\frac{\epsilon}{4n\sum_{i=1}^{k}|g(c_i)-f(c_i)|} P=(a=x_0,\cdots,x_n=b) \left\lVert P\right\rVert<\delta \left|R(f,P)-\int_{a}^{b}f\right|<\frac{\epsilon}{2}. \begin{align*}
\left|R(g,P)-\int_{a}^{b}f\right|&=\left|R(g,P)-R(f,P)+R(f,P)-\int_{a}^{b}f\right|\\
&\leq\left|R(g,P)-R(f,P)\right|+\left|R(f,P)-\int_{a}^{b}f\right|\\
&<\left|R(g,P)-R(f,P)\right|+\frac{\epsilon}{2}\\
&<\frac{\epsilon}{2}+\frac{\epsilon}{2}=\epsilon.\\
\end{align*} \begin{align*}
|R(g,P)-R(f,P)|&=\left|\sum_{i=1}^{n}g(x_i*)(x_i-x_{i-1})-\sum_{i=1}^{n}f(x_i*)(x_i-x_{i-1})\right|\\
&=\left|\sum_{i=1}^{n}(g(x_i^*)-f(x_i^*))(x_i-x_{i-1})\right|\\
&<\sum_{i=1}^{k}|g(c_i)-f(c_i)|(2n\delta)\\
&=2n\sum_{i=1}^{k}|g(c_i)-f(c_i)|\left(\frac{\epsilon}{4n\sum_{i=1}^{k}|g(c_i)-f(c_i)|}\right)\\
&<\frac{\epsilon}{2}
\end{align*}","['real-analysis', 'analysis', 'proof-verification', 'riemann-integration']"
93,"Show that the open interval (a, b) is Lebesgue measurable","Show that the open interval (a, b) is Lebesgue measurable",,"I have to show that an open interval in the form $(a,b)$ , where $a,b \in {\mathbb R}$ and $a < b$ is Lebesgue measurable.  I think I'm supposed to show, that the subset $(a,b)$ is Lebesgue measurable, if and only if: $$m(A) = m(A ∩ S) + m(A ∩ S^c)$$ where $S \subseteq {\mathbb R}^n$ and $S^c$ is the complement of $S$ .  But how do I actually prove that the open interval $(a,b)$ is Lebesgue measurable?","I have to show that an open interval in the form , where and is Lebesgue measurable.  I think I'm supposed to show, that the subset is Lebesgue measurable, if and only if: where and is the complement of .  But how do I actually prove that the open interval is Lebesgue measurable?","(a,b) a,b \in {\mathbb R} a < b (a,b) m(A) = m(A ∩ S) + m(A ∩ S^c) S \subseteq {\mathbb R}^n S^c S (a,b)","['analysis', 'measure-theory', 'lebesgue-measure']"
94,Is this function of bounded variation?,Is this function of bounded variation?,,"Consider Riemann's function defined on $\mathbb{R}$ , $$ R(x) = \sum_{n=1}^\infty \frac{\sin n^2 x}{n^2} . $$ If you graph it, you can see that it shows a lot of zigzags. Hence, the question is, is this function of bounded variation in $(0,2 \pi )$ ?","Consider Riemann's function defined on , If you graph it, you can see that it shows a lot of zigzags. Hence, the question is, is this function of bounded variation in ?","\mathbb{R}  R(x) = \sum_{n=1}^\infty \frac{\sin n^2 x}{n^2} .  (0,2 \pi )","['real-analysis', 'analysis', 'fourier-analysis', 'fourier-series']"
95,Constructive Intermediate Value Theorem (IVT),Constructive Intermediate Value Theorem (IVT),,"I'm trying to learn a bit about intuitionistic/constructive mathematics, as I want to understand a little about topos theory and homotopy type theory (HoTT).  I'm confused as to why the intermediate value theorem doesn't hold in constructive analysis. Suppose $f: \mathbb{R} \to \mathbb{R}$ is a continuous function on the interval $(a, b)$ .  Further, suppose $f(a) < 0$ and $f(b) > 0$ .  By classical analysis, there exists a point $c \in (a, b)$ such that $f(c) = 0$ .  I'm told that this doesn't generally hold in constructive or intuitionistic logic.  I would like to know why the following ""construction"" does not constitute an outline of a proof. Let $a_n$ be a sequence of points defined as follows: $a_0 = \frac{a + b}{2}$ $a_n = \frac{a + a_{n-1}}{2}$ if $f(a_{n-1}) > 0$ $a_n = \frac{b + a_{n-1}}{2}$ if $f(a_{n-1}) < 0$ $a_n = a_{n-1}$ if $f(a_{n-1}) = 0$ . Define $c = \lim_{n \rightarrow \infty} a_n$ .  Then $f(c) = 0$ .  This provides an algorithmic construction (using the bisection algorithm) which not only shows that a 0 exists but actually gives a way to compute a zero.  Why is this outline of a proof not valid in constructive logic?  What am I missing?","I'm trying to learn a bit about intuitionistic/constructive mathematics, as I want to understand a little about topos theory and homotopy type theory (HoTT).  I'm confused as to why the intermediate value theorem doesn't hold in constructive analysis. Suppose is a continuous function on the interval .  Further, suppose and .  By classical analysis, there exists a point such that .  I'm told that this doesn't generally hold in constructive or intuitionistic logic.  I would like to know why the following ""construction"" does not constitute an outline of a proof. Let be a sequence of points defined as follows: if if if . Define .  Then .  This provides an algorithmic construction (using the bisection algorithm) which not only shows that a 0 exists but actually gives a way to compute a zero.  Why is this outline of a proof not valid in constructive logic?  What am I missing?","f: \mathbb{R} \to \mathbb{R} (a, b) f(a) < 0 f(b) > 0 c \in (a, b) f(c) = 0 a_n a_0 = \frac{a + b}{2} a_n = \frac{a + a_{n-1}}{2} f(a_{n-1}) > 0 a_n = \frac{b + a_{n-1}}{2} f(a_{n-1}) < 0 a_n = a_{n-1} f(a_{n-1}) = 0 c = \lim_{n \rightarrow \infty} a_n f(c) = 0","['real-analysis', 'analysis', 'constructive-mathematics', 'intuitionistic-logic']"
96,Why isn't this problem in Real and Complex Analysis really easy?,Why isn't this problem in Real and Complex Analysis really easy?,,"In Real and Complex Analysis, Walter Rudin proposes the following problem: Let $\{f_n\}$ be a sequence of continuous functions in $[0,1]$ with $0\leq f_n\leq 1$ and such that for all $x\in [0,1]$ , $f_n(x)\to 0$ . Show that $$\lim_{n\to\infty}\int_0^1 f_n(x)\:\mathrm{d}x=0.$$ Also try to prove this without using anything from Lebesgue's theory. (This exercice is meant to show how powerful Lebesgue's theory is.) (This is my translation since I own the french version of the book.) Since Rudin does this remark in the exercice, it seems to me that this ought to be a hard problem to do without Lebesgue's machinery. However, I thought about the following solution: Since $[0,1]$ is compact and $f_n\to 0$ pointwise, we have that $f_n\to 0$ uniformly. Then $$\lim_{n\to\infty}\int_0^1 f_n(x)\:\mathrm{d}x=\int_0^1\lim_{n\to\infty}f_n(x)\:\mathrm{d}x=0.$$ Is this wrong? If so, why?","In Real and Complex Analysis, Walter Rudin proposes the following problem: Let be a sequence of continuous functions in with and such that for all , . Show that Also try to prove this without using anything from Lebesgue's theory. (This exercice is meant to show how powerful Lebesgue's theory is.) (This is my translation since I own the french version of the book.) Since Rudin does this remark in the exercice, it seems to me that this ought to be a hard problem to do without Lebesgue's machinery. However, I thought about the following solution: Since is compact and pointwise, we have that uniformly. Then Is this wrong? If so, why?","\{f_n\} [0,1] 0\leq f_n\leq 1 x\in [0,1] f_n(x)\to 0 \lim_{n\to\infty}\int_0^1 f_n(x)\:\mathrm{d}x=0. [0,1] f_n\to 0 f_n\to 0 \lim_{n\to\infty}\int_0^1 f_n(x)\:\mathrm{d}x=\int_0^1\lim_{n\to\infty}f_n(x)\:\mathrm{d}x=0.","['real-analysis', 'analysis']"
97,MVT inequality problem,MVT inequality problem,,"I just sat a real analysis exam and this was a question in it that I couldn't answer... Prove that $\left|e^\frac{-x^2}{2t}-e^\frac{-y^2}{2t}\right| \leq \frac{|x-y|}{t}$ for $x,y \in [-1,1] ,t>0$ I ended up trying to set $f(x,y)=e^\frac{-x^2}{2t}-e^\frac{-y^2}{2t}$, then attempted trying $f(-1,-1) =f(1,1)$ but never ended up getting anywhere. Any tips on how this is actually solved? I've never seen an inequality problem like this before.","I just sat a real analysis exam and this was a question in it that I couldn't answer... Prove that $\left|e^\frac{-x^2}{2t}-e^\frac{-y^2}{2t}\right| \leq \frac{|x-y|}{t}$ for $x,y \in [-1,1] ,t>0$ I ended up trying to set $f(x,y)=e^\frac{-x^2}{2t}-e^\frac{-y^2}{2t}$, then attempted trying $f(-1,-1) =f(1,1)$ but never ended up getting anywhere. Any tips on how this is actually solved? I've never seen an inequality problem like this before.",,"['real-analysis', 'analysis']"
98,Proving $f'$=$g'$ has some c such that $g=f+c$,Proving = has some c such that,f' g' g=f+c,"Suppose that $f$ and $g$ are differentiable functions on $(a,b)$ and suppose that $g'(x)=f'(x)$ for all $x \in (a,b)$. Prove that there is some $c \in \mathbb{R}$ such that $g(x) = f(x)+c$. So far, I started with this: Let $h'(x)=f'(x)-g'(x)=0$, then MVT implies $\exists$ c $\in \mathbb{R}$ such that $h'(c) = \frac{h(b)-h(a)}{b-a} =0$. Then $h'(c)=0 \implies h(c)=c$ After this i'm not sure where to go, or if this is correct at all, any hints? This is also my first post in Latex so sorry if there's any mistakes!","Suppose that $f$ and $g$ are differentiable functions on $(a,b)$ and suppose that $g'(x)=f'(x)$ for all $x \in (a,b)$. Prove that there is some $c \in \mathbb{R}$ such that $g(x) = f(x)+c$. So far, I started with this: Let $h'(x)=f'(x)-g'(x)=0$, then MVT implies $\exists$ c $\in \mathbb{R}$ such that $h'(c) = \frac{h(b)-h(a)}{b-a} =0$. Then $h'(c)=0 \implies h(c)=c$ After this i'm not sure where to go, or if this is correct at all, any hints? This is also my first post in Latex so sorry if there's any mistakes!",,"['real-analysis', 'analysis', 'derivatives']"
99,Limit of divided differences.,Limit of divided differences.,,"I'm trying to show that the limit of Newton's interpolation formula as $x_i\to x_0$ ( $i=1,\ldots,n$ ) gives the Taylor's formula (knowing that $f\in \mathcal{C}^{n+1}[x_0-\delta,x_0+\delta]$ ) $$T(x)=f(x_0)+f'(x_0)(x-x_0)+\ldots+\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n+\frac{f^{(n+1)}(\xi)}{(n+1)!}(x-x_0)^{n+1}$$ The Newton's interpolation formula is $$P_n(x)=f(x_0)+f(x_0,x_1)(x-x_0)+\ldots+f(x_0,\ldots,x_n)(x-x_0)\ldots (x-x_{n-1})$$ So basically I must show that $$\lim_{(x_1,\ldots,x_n)\to(x_0,\ldots,x_0)}P_n(x)=T(x)$$ $f(x_0,\ldots,x_k)$ is defined as $$f(x_0,\ldots,x_k)=\frac{f(x_1,\ldots,x_k)-f(x_0,\ldots,x_{k-1})}{x_k-x_0}$$ From that definition it's easy to show that $$\lim_{(x_1,\ldots,x_n)\to(x_0,\ldots,x_0)}f(x_0,x_1)=\lim_{(x_1,\ldots,x_n)\to(x_0,\ldots,x_0)}\frac{f(x_1)-f(x_0)}{x_1-x_0}\overset{def}{=}f'(x_0)$$ But I get stuck when I have to find the limit of $f(x_0,x_1,x_2)$ . $$\lim_{(x_1,\ldots,x_n)\to(x_0,\ldots,x_0)}f(x_0,x_1,x_2)=\lim_{(x_1,\ldots,x_n)\to(x_0,\ldots,x_0)}\frac{f(x_1,x_2)-f(x_0,x_1)}{x_2-x_0}=???=\frac{1}{2}f''(x_0)$$ So my question is what should I do at $(???)$ ? I'm guessing that if I get this step figured out, finding the limit of $f(x_0,\ldots,x_{n-1})$ is analogous?","I'm trying to show that the limit of Newton's interpolation formula as ( ) gives the Taylor's formula (knowing that ) The Newton's interpolation formula is So basically I must show that is defined as From that definition it's easy to show that But I get stuck when I have to find the limit of . So my question is what should I do at ? I'm guessing that if I get this step figured out, finding the limit of is analogous?","x_i\to x_0 i=1,\ldots,n f\in \mathcal{C}^{n+1}[x_0-\delta,x_0+\delta] T(x)=f(x_0)+f'(x_0)(x-x_0)+\ldots+\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n+\frac{f^{(n+1)}(\xi)}{(n+1)!}(x-x_0)^{n+1} P_n(x)=f(x_0)+f(x_0,x_1)(x-x_0)+\ldots+f(x_0,\ldots,x_n)(x-x_0)\ldots (x-x_{n-1}) \lim_{(x_1,\ldots,x_n)\to(x_0,\ldots,x_0)}P_n(x)=T(x) f(x_0,\ldots,x_k) f(x_0,\ldots,x_k)=\frac{f(x_1,\ldots,x_k)-f(x_0,\ldots,x_{k-1})}{x_k-x_0} \lim_{(x_1,\ldots,x_n)\to(x_0,\ldots,x_0)}f(x_0,x_1)=\lim_{(x_1,\ldots,x_n)\to(x_0,\ldots,x_0)}\frac{f(x_1)-f(x_0)}{x_1-x_0}\overset{def}{=}f'(x_0) f(x_0,x_1,x_2) \lim_{(x_1,\ldots,x_n)\to(x_0,\ldots,x_0)}f(x_0,x_1,x_2)=\lim_{(x_1,\ldots,x_n)\to(x_0,\ldots,x_0)}\frac{f(x_1,x_2)-f(x_0,x_1)}{x_2-x_0}=???=\frac{1}{2}f''(x_0) (???) f(x_0,\ldots,x_{n-1})","['real-analysis', 'analysis', 'numerical-methods', 'interpolation']"
