,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Strict positiveness on a C*-algebra given by generators and relations.,Strict positiveness on a C*-algebra given by generators and relations.,,"Let $A$ be a C*-algebra with generators $a_1,a_2,\ldots,a_n$ and some (non-important) relations (the relations imply that $\|a_i\|\leq 1$, so that $A$ exists). Among the given relations we have that $$ a_1 = \sum_{1 \leq i \leq n} a_i a_i^*. $$ This implies that $a_1$ is positive; I would like to prove that $a_1$ is strictly positive. So far my attempts to prove this have been in vain. If $\phi$ is a multiplicative linear positive functional, then $\phi(a_1)=0$ implies $\phi(a_i)=0$ and then $\phi = 0$; I've no clue on the general case. Thanks in advance.","Let $A$ be a C*-algebra with generators $a_1,a_2,\ldots,a_n$ and some (non-important) relations (the relations imply that $\|a_i\|\leq 1$, so that $A$ exists). Among the given relations we have that $$ a_1 = \sum_{1 \leq i \leq n} a_i a_i^*. $$ This implies that $a_1$ is positive; I would like to prove that $a_1$ is strictly positive. So far my attempts to prove this have been in vain. If $\phi$ is a multiplicative linear positive functional, then $\phi(a_1)=0$ implies $\phi(a_i)=0$ and then $\phi = 0$; I've no clue on the general case. Thanks in advance.",,"['functional-analysis', 'operator-algebras', 'c-star-algebras']"
1,"Is the supremum norm the only $ C^{*} $-norm on $ {C_{c}}(X) $, equipped with the usual pointwise operations?","Is the supremum norm the only -norm on , equipped with the usual pointwise operations?", C^{*}   {C_{c}}(X) ,"Let $ X $ be a locally compact Hausdorff space. Then $ {C_{c}}(X) $ is a commutative $ * $-algebra with respect to addition, multiplication, scalar multiplication and conjugation (all pointwise operations). Question. If $ \| \cdot \|: {C_{c}}(X) \to \Bbb{R}_{\geq 0} $ is a (not-necessarily-complete) $ C^{*} $-norm on this $ * $-algebra, then is it true that $ \| \cdot \| $ is actually the supremum norm on $ {C_{c}}(X) $? If so, then $ {C_{0}}(X) $ is the only $ C^{*} $-completion of this $ * $-algebra. This question seemed trivial at first sight, but after a while, I realized that it was not at all. Thanks for your help!","Let $ X $ be a locally compact Hausdorff space. Then $ {C_{c}}(X) $ is a commutative $ * $-algebra with respect to addition, multiplication, scalar multiplication and conjugation (all pointwise operations). Question. If $ \| \cdot \|: {C_{c}}(X) \to \Bbb{R}_{\geq 0} $ is a (not-necessarily-complete) $ C^{*} $-norm on this $ * $-algebra, then is it true that $ \| \cdot \| $ is actually the supremum norm on $ {C_{c}}(X) $? If so, then $ {C_{0}}(X) $ is the only $ C^{*} $-completion of this $ * $-algebra. This question seemed trivial at first sight, but after a while, I realized that it was not at all. Thanks for your help!",,"['functional-analysis', 'normed-spaces', 'operator-algebras', 'c-star-algebras']"
2,Isometry between $X^\ast/M^\perp$ and $M^\ast$.,Isometry between  and .,X^\ast/M^\perp M^\ast,"Let $X$ be a normed linear space, $M \subset X$ be a subspace, $M^\perp = \{x^\ast \in X^\ast \mid x^\ast\big|_M = 0\}$ be the annihilator of $M$, $X^\ast$ the topological dual of $X$, and let's define $\Phi: X^\ast/M^\perp \to M^*$ by $\Phi(x^\ast+M^\perp) = x^\ast\big|_M$. I have already proved that $M^\perp$ is closed, and that $\Phi$ is well-defined, linear, and surjective. I finally want to prove that $\Phi$ is an isometry. On one hand, we take $y^\ast \in M^\perp$ and now: $$\begin{align} \|\Phi(x^\ast+M^\perp)\| &= \|x^\ast\big|_M\| = \|x^\ast\big|_M+y^\ast\big|_M\|  \\  &=\|(x^\ast+y^\ast)\big|_M\| \leq \|x^\ast+y^\ast\|,\end{align}$$and taking the infimum on $y^\ast$ gives $\|\Phi(x^\ast+M^\perp)\| \leq \|x^\ast+M^\perp\|$. I'm having trouble on the other inequality. One first attempt was to fix $m \in M$, take any $y^\ast \in M^\perp$ and do: $$\|(x^\ast+y^\ast)(m)\| \leq \|x^\ast(m)\|+\|y^\ast(m)\| = \|x^\ast(m)\| \leq \|x^\ast\big|_M\|\|m\|,$$and this gives $\|x^\ast\big|_M  + y^\ast\big|_M\| \leq \|x^\ast\big|_M\|$ instead of the $\|x^\ast  + y^\ast\|\leq \|x^\ast\big|_M\|$ that I wanted. Can someone help?","Let $X$ be a normed linear space, $M \subset X$ be a subspace, $M^\perp = \{x^\ast \in X^\ast \mid x^\ast\big|_M = 0\}$ be the annihilator of $M$, $X^\ast$ the topological dual of $X$, and let's define $\Phi: X^\ast/M^\perp \to M^*$ by $\Phi(x^\ast+M^\perp) = x^\ast\big|_M$. I have already proved that $M^\perp$ is closed, and that $\Phi$ is well-defined, linear, and surjective. I finally want to prove that $\Phi$ is an isometry. On one hand, we take $y^\ast \in M^\perp$ and now: $$\begin{align} \|\Phi(x^\ast+M^\perp)\| &= \|x^\ast\big|_M\| = \|x^\ast\big|_M+y^\ast\big|_M\|  \\  &=\|(x^\ast+y^\ast)\big|_M\| \leq \|x^\ast+y^\ast\|,\end{align}$$and taking the infimum on $y^\ast$ gives $\|\Phi(x^\ast+M^\perp)\| \leq \|x^\ast+M^\perp\|$. I'm having trouble on the other inequality. One first attempt was to fix $m \in M$, take any $y^\ast \in M^\perp$ and do: $$\|(x^\ast+y^\ast)(m)\| \leq \|x^\ast(m)\|+\|y^\ast(m)\| = \|x^\ast(m)\| \leq \|x^\ast\big|_M\|\|m\|,$$and this gives $\|x^\ast\big|_M  + y^\ast\big|_M\| \leq \|x^\ast\big|_M\|$ instead of the $\|x^\ast  + y^\ast\|\leq \|x^\ast\big|_M\|$ that I wanted. Can someone help?",,"['linear-algebra', 'analysis', 'functional-analysis', 'quotient-spaces', 'isometry']"
3,A question on maximal monotone operators,A question on maximal monotone operators,,"Definition : Let $H$ a Hilbert space. An unbounded linear operator $A: D(A) \subseteq H \to H$ is said to be monotone if it satisfies   $$\forall v \in D(A),\ (Av, v) \ge 0 $$ It is called maximal monotone if, in addition, $R(I + A) = H$, i.e.,   $\forall f \in H,\ \exists  u \in D(A) \text{ such that }u + Au = f$ If $A $ is maximal monotone then $\lambda A $ is also maximal monotone for every $\lambda > 0$. However, if $A$ and $B$ are maximal monotone operators, then $A + B$, defined on $D(A) \cap D(B)$, need not be maximal monotone. Why?","Definition : Let $H$ a Hilbert space. An unbounded linear operator $A: D(A) \subseteq H \to H$ is said to be monotone if it satisfies   $$\forall v \in D(A),\ (Av, v) \ge 0 $$ It is called maximal monotone if, in addition, $R(I + A) = H$, i.e.,   $\forall f \in H,\ \exists  u \in D(A) \text{ such that }u + Au = f$ If $A $ is maximal monotone then $\lambda A $ is also maximal monotone for every $\lambda > 0$. However, if $A$ and $B$ are maximal monotone operators, then $A + B$, defined on $D(A) \cap D(B)$, need not be maximal monotone. Why?",,"['functional-analysis', 'monotone-operator-theory']"
4,"Is $T \colon \left( W^{2,2}(\mathbb{R}), \| \cdot \|_{L^2} \right) \longrightarrow \left( L^2(\mathbb{R}), \| \cdot \|_{L^2} \right)$ unbounded?",Is  unbounded?,"T \colon \left( W^{2,2}(\mathbb{R}), \| \cdot \|_{L^2} \right) \longrightarrow \left( L^2(\mathbb{R}), \| \cdot \|_{L^2} \right)","I am currently considering the following operator (""modified Laplacian""): $$T \colon \left( W^{2,2}(\mathbb{R}), \| \cdot \|_{L^2} \right) \longrightarrow \left( L^2(\mathbb{R}), \| \cdot \|_{L^2} \right)$$ definied by $$u \mapsto Tu := -u^{\prime \prime}$$ Of course $-u^{\prime \prime}$ refers to the second weak-derivative of $u$. Please note, that I have equipped the domain of the operator with the $\| \cdot \|_{L^2}$-Norm (the operator would clearly be bounded, when considering the Sobolev-Norm)! Is this operator an unbounded one? I have picked an arbitrary $f \in W^{2,2}(\mathbb R)$.","I am currently considering the following operator (""modified Laplacian""): $$T \colon \left( W^{2,2}(\mathbb{R}), \| \cdot \|_{L^2} \right) \longrightarrow \left( L^2(\mathbb{R}), \| \cdot \|_{L^2} \right)$$ definied by $$u \mapsto Tu := -u^{\prime \prime}$$ Of course $-u^{\prime \prime}$ refers to the second weak-derivative of $u$. Please note, that I have equipped the domain of the operator with the $\| \cdot \|_{L^2}$-Norm (the operator would clearly be bounded, when considering the Sobolev-Norm)! Is this operator an unbounded one? I have picked an arbitrary $f \in W^{2,2}(\mathbb R)$.",,"['functional-analysis', 'operator-theory']"
5,to define hilbert spaces how we use them in real world problem? [closed],to define hilbert spaces how we use them in real world problem? [closed],,Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 9 years ago . Improve this question I know all the definition of Hilbert spaces? How can we say here we have to use $L^p$ space here we have to use Hilbert space or Banach space? I do not understand why we use these spaces?,Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 9 years ago . Improve this question I know all the definition of Hilbert spaces? How can we say here we have to use $L^p$ space here we have to use Hilbert space or Banach space? I do not understand why we use these spaces?,,"['real-analysis', 'functional-analysis']"
6,Multiplication of a distribution by a continuous function,Multiplication of a distribution by a continuous function,,"In my work, I am encountering the issue of having to multiply a continuous function (not necessarily differentiable) by a distribution. It seems to me that if $f(x)$ is a continuous function on $\mathbb{R}$ and if $d(x)$ is a distribution on $\mathbb{R}$, then the product $f(x) d(x)$ makes sense and can be interpreted as a distribution. If someone knows an example of a distribution, which cannot be multiplied by a continuous function, I would like to see it. If there is no such example, then my question is this: is there a reference discussing the extent with which distributions can be multiplied together? Hopefully, in such a discussion, a particular case would be the case permitting the multiplication of a distribution by a continuous function. My goal being to justify my operations with theory.","In my work, I am encountering the issue of having to multiply a continuous function (not necessarily differentiable) by a distribution. It seems to me that if $f(x)$ is a continuous function on $\mathbb{R}$ and if $d(x)$ is a distribution on $\mathbb{R}$, then the product $f(x) d(x)$ makes sense and can be interpreted as a distribution. If someone knows an example of a distribution, which cannot be multiplied by a continuous function, I would like to see it. If there is no such example, then my question is this: is there a reference discussing the extent with which distributions can be multiplied together? Hopefully, in such a discussion, a particular case would be the case permitting the multiplication of a distribution by a continuous function. My goal being to justify my operations with theory.",,"['functional-analysis', 'distribution-theory', 'dirac-delta']"
7,Lemma 3.5-3 in Erwine Kreyszig's INTRODUCTORY FUNCTIONAL ANALYSIS WITH APPLICATIONS: Is the set of non-zere Fourier co-efficients uncountable too?,Lemma 3.5-3 in Erwine Kreyszig's INTRODUCTORY FUNCTIONAL ANALYSIS WITH APPLICATIONS: Is the set of non-zere Fourier co-efficients uncountable too?,,"Let $X$ be an inner product space, let $x \in X$ be non-zero, and let $M$ be an uncountable orthonormal subset of $X$. Then what can we say about the cardinality of the following set?  $$ \{ \ v \in M \colon \ \langle x, v \rangle \neq 0 \ \}.$$ I know that, for any orthonormal sequence $(e_k)$ in $X$, the Bessel inequality  $$\sum_{k=1}^\infty \vert \langle x, e_k \rangle \vert^2 \leq \Vert x \Vert^2$$ holds and hence, for all $m \in \mathbb{N}$, the the set  $$\{ \ k \in \mathbb{N} \ \colon \ \vert \langle x, e_k \rangle \vert > \frac{1}{m} \ \}$$  is at most finite. But I'm struggling to proceed from the countable case to the uncountable one.","Let $X$ be an inner product space, let $x \in X$ be non-zero, and let $M$ be an uncountable orthonormal subset of $X$. Then what can we say about the cardinality of the following set?  $$ \{ \ v \in M \colon \ \langle x, v \rangle \neq 0 \ \}.$$ I know that, for any orthonormal sequence $(e_k)$ in $X$, the Bessel inequality  $$\sum_{k=1}^\infty \vert \langle x, e_k \rangle \vert^2 \leq \Vert x \Vert^2$$ holds and hence, for all $m \in \mathbb{N}$, the the set  $$\{ \ k \in \mathbb{N} \ \colon \ \vert \langle x, e_k \rangle \vert > \frac{1}{m} \ \}$$  is at most finite. But I'm struggling to proceed from the countable case to the uncountable one.",,"['real-analysis', 'analysis', 'functional-analysis', 'hilbert-spaces', 'inner-products']"
8,"Show $\langle u,v \rangle = -\frac{1}{2}$ when $u+v+w=0$ and $\|u\|=\|v\|=\|w\|=1$?",Show  when  and ?,"\langle u,v \rangle = -\frac{1}{2} u+v+w=0 \|u\|=\|v\|=\|w\|=1","Show $\langle u,v \rangle = -\frac{1}{2}$ when $u+v+w=0$ and $\|u\|=\|v\|=\|w\|=1$? My thinking is: $\langle u+v+w,v \rangle =0 \iff \langle u,v \rangle + \langle w,v \rangle = -1$ How do i continue?","Show $\langle u,v \rangle = -\frac{1}{2}$ when $u+v+w=0$ and $\|u\|=\|v\|=\|w\|=1$? My thinking is: $\langle u+v+w,v \rangle =0 \iff \langle u,v \rangle + \langle w,v \rangle = -1$ How do i continue?",,"['linear-algebra', 'functional-analysis', 'inner-products']"
9,Powers of compact operators,Powers of compact operators,,"Consider a Hilbert space $H$ and a compact self-adjoint operator $T : H \to H$. I want to prove that all positive powers (especially fractional powers) of $T$ are compact. From the spectral theorem, I see that powers of bounded operators are bounded. But I am unsure how to prove the corresponding statement for compact operators.","Consider a Hilbert space $H$ and a compact self-adjoint operator $T : H \to H$. I want to prove that all positive powers (especially fractional powers) of $T$ are compact. From the spectral theorem, I see that powers of bounded operators are bounded. But I am unsure how to prove the corresponding statement for compact operators.",,"['functional-analysis', 'operator-theory']"
10,"What does ""up to a subsequence"" mean?","What does ""up to a subsequence"" mean?",,"English is my second language. Now I have to read papers written in English, and I can't understand the phrase. Well, I get a vague idea, but that's all. What have I done? I Googled with ""up to a subsequence"" (yes, with quotes), but I still can't figure out the meaning from the results. If context is needed, this link may help.","English is my second language. Now I have to read papers written in English, and I can't understand the phrase. Well, I get a vague idea, but that's all. What have I done? I Googled with ""up to a subsequence"" (yes, with quotes), but I still can't figure out the meaning from the results. If context is needed, this link may help.",,"['real-analysis', 'functional-analysis', 'terminology']"
11,Uncountable sums convergence doubt,Uncountable sums convergence doubt,,"Hi I am having certain doubts about the sum of uncountable numbers In my class on functional analysis we proved that if $\sum_{\alpha \in A} X_{\alpha}$ converges then $X_{\alpha}$ is not equal to zero at most a countably many times. I know that there is a similar question on the Math Stack exchange website but I am having serious troubles understanding the logic behind the proof in the class The proof given by @Benji makes  complete sense. The sum of an uncountable number of positive numbers We had a very similar proof in the class which was partly discussed in the comment section . We used the definition that A sequence $\sum_{\alpha \in A} X_{\alpha}$ is convergent iff $\sup_{F \subset A} \sum_{\alpha \in F}X_{\alpha} < \infty$ where $F$ refers to a finite subset of $A$ The proof is as follows: Proof: By Contradiction. We assume that $\sum_{\alpha \in A} X_{\alpha}$ converges and $\{ \alpha |X_{\alpha}>0 \}$ is uncountable we defined sets $S_n=\{\alpha |X_\alpha >\frac{1}{n}\}$ but the we have  $\bigcup_{n \in \mathbb{N}} S_n=\{\alpha |X_\alpha >0\}$ But by assumption the RHS is uncountable and since LHS is countable union of sets , then it implies that $\exists N \in \mathbb{N}$ such that $S_N$ is uncountable (Please note in the following whenever I use $F$ , it is a finite subset of A) Now the prof concludes that $\sup_{F \subset A} \sum_{\alpha \in F}X_{\alpha} = \infty$ And from the definition in bond it follows that $\sum _{\alpha \in A}X_{\alpha}$ does not converge and we get a contradiction Now if I consider the set $S_N=\{\alpha_1,\alpha_2,\dots \dots\}$ defined as before i.e $X_{\alpha_j}>\frac{1}{N}$ $\forall \alpha_j  \in S_N$ and consider a generic $F=\{\alpha_1,\alpha_2,\dots ,\alpha_k \}$ Then $\sum_{\alpha \in F \subset S_N} X_{\alpha}>\frac{k}{N}$ for any k  as cardinality of F needs to be finite So therefore I have that $\sup_{F \in \mathcal{F}} \sum_{\alpha \in F} X_{\alpha} <\sup_{k \in \mathbb{N}} \frac{k}{N}=\infty$ where $\mathcal{F}$ is the set of all finite subsets of $A$ **I don't understand why does $S_N$ need to be uncountable for this thing thing to work. I mean wouldn't it work as long as it is infinite(countably)? ** Please let me know if this question is not clear.","Hi I am having certain doubts about the sum of uncountable numbers In my class on functional analysis we proved that if $\sum_{\alpha \in A} X_{\alpha}$ converges then $X_{\alpha}$ is not equal to zero at most a countably many times. I know that there is a similar question on the Math Stack exchange website but I am having serious troubles understanding the logic behind the proof in the class The proof given by @Benji makes  complete sense. The sum of an uncountable number of positive numbers We had a very similar proof in the class which was partly discussed in the comment section . We used the definition that A sequence $\sum_{\alpha \in A} X_{\alpha}$ is convergent iff $\sup_{F \subset A} \sum_{\alpha \in F}X_{\alpha} < \infty$ where $F$ refers to a finite subset of $A$ The proof is as follows: Proof: By Contradiction. We assume that $\sum_{\alpha \in A} X_{\alpha}$ converges and $\{ \alpha |X_{\alpha}>0 \}$ is uncountable we defined sets $S_n=\{\alpha |X_\alpha >\frac{1}{n}\}$ but the we have  $\bigcup_{n \in \mathbb{N}} S_n=\{\alpha |X_\alpha >0\}$ But by assumption the RHS is uncountable and since LHS is countable union of sets , then it implies that $\exists N \in \mathbb{N}$ such that $S_N$ is uncountable (Please note in the following whenever I use $F$ , it is a finite subset of A) Now the prof concludes that $\sup_{F \subset A} \sum_{\alpha \in F}X_{\alpha} = \infty$ And from the definition in bond it follows that $\sum _{\alpha \in A}X_{\alpha}$ does not converge and we get a contradiction Now if I consider the set $S_N=\{\alpha_1,\alpha_2,\dots \dots\}$ defined as before i.e $X_{\alpha_j}>\frac{1}{N}$ $\forall \alpha_j  \in S_N$ and consider a generic $F=\{\alpha_1,\alpha_2,\dots ,\alpha_k \}$ Then $\sum_{\alpha \in F \subset S_N} X_{\alpha}>\frac{k}{N}$ for any k  as cardinality of F needs to be finite So therefore I have that $\sup_{F \in \mathcal{F}} \sum_{\alpha \in F} X_{\alpha} <\sup_{k \in \mathbb{N}} \frac{k}{N}=\infty$ where $\mathcal{F}$ is the set of all finite subsets of $A$ **I don't understand why does $S_N$ need to be uncountable for this thing thing to work. I mean wouldn't it work as long as it is infinite(countably)? ** Please let me know if this question is not clear.",,"['analysis', 'functional-analysis', 'summation']"
12,Uniform Convergence on Compact Sets Means Uniform Convergence on the whole Set,Uniform Convergence on Compact Sets Means Uniform Convergence on the whole Set,,"Let $\Omega\in Open(\mathbb{R}^n)$ for some $n\in\mathbb{N}_{\geq1}$. Then we know that $\exists \{K_n\}_{n\in\mathbb{N}_{\geq1}}$, a collection of compact sets, such that $\Omega = \bigcup_{n\in\mathbb{N}_{\geq1}}K_n$ and $K_n \subseteq int(K_{n+1})\forall n\in\mathbb{N}_{\geq1}$ (in particular, define $K_n := \{x\in\Omega\,|\,||x||\leq n\}\cap\{x\in\Omega\,|\,d(x,\,\Omega^c)\geq\frac{1}{n}\}$). Suppose there is a sequence of continuous functions $f_i:\Omega\to\mathbb{C}$ which we somehow know converge uniformly to the pointwise limit function $f$ on $K_n$, for all $n\in\mathbb{N}_{\geq1}$. Prove that $\{f_i\}$ then converges uniformly on $\Omega$. (This came up while studying Example 1.44 in Rudin's Functional Analysis--it is a fact which he takes for granted and I couldn't figure out why it holds)","Let $\Omega\in Open(\mathbb{R}^n)$ for some $n\in\mathbb{N}_{\geq1}$. Then we know that $\exists \{K_n\}_{n\in\mathbb{N}_{\geq1}}$, a collection of compact sets, such that $\Omega = \bigcup_{n\in\mathbb{N}_{\geq1}}K_n$ and $K_n \subseteq int(K_{n+1})\forall n\in\mathbb{N}_{\geq1}$ (in particular, define $K_n := \{x\in\Omega\,|\,||x||\leq n\}\cap\{x\in\Omega\,|\,d(x,\,\Omega^c)\geq\frac{1}{n}\}$). Suppose there is a sequence of continuous functions $f_i:\Omega\to\mathbb{C}$ which we somehow know converge uniformly to the pointwise limit function $f$ on $K_n$, for all $n\in\mathbb{N}_{\geq1}$. Prove that $\{f_i\}$ then converges uniformly on $\Omega$. (This came up while studying Example 1.44 in Rudin's Functional Analysis--it is a fact which he takes for granted and I couldn't figure out why it holds)",,"['real-analysis', 'analysis', 'functional-analysis']"
13,How do I prove a differential operator has no purely imaginary eigenvalues?,How do I prove a differential operator has no purely imaginary eigenvalues?,,"Anyone who has taken a course in linear algebra knows how to prove the eigenvalues of a self-adjoint operator are real or the eigenvalues of a skew-self-adjoint operator are purely imaginary. This is easily extended to the spectrum of differential operators on Hilbert spaces. Conversely, how does one go about proving that a differential operator has no purely imaginary eigenvalues? Indeed, what condition can be imposed on a first order differential operator $L$:  \begin{equation*} Lu =A \frac{du}{dt} + Bu \end{equation*} for $u: S^1 \to \mathbb{R}^n$ that ensures the spectrum of $L$ contains no purely imaginary eigenvalues? If $n=2m$ and \begin{equation*} A= \left( \begin{array}{cc} 0 & -I  \\ I & 0  \end{array} \right) \end{equation*} then the first order part of $L$ is self-adjoint, so what condition can be imposed on $B$ so that the eigenvalues are not imaginary?","Anyone who has taken a course in linear algebra knows how to prove the eigenvalues of a self-adjoint operator are real or the eigenvalues of a skew-self-adjoint operator are purely imaginary. This is easily extended to the spectrum of differential operators on Hilbert spaces. Conversely, how does one go about proving that a differential operator has no purely imaginary eigenvalues? Indeed, what condition can be imposed on a first order differential operator $L$:  \begin{equation*} Lu =A \frac{du}{dt} + Bu \end{equation*} for $u: S^1 \to \mathbb{R}^n$ that ensures the spectrum of $L$ contains no purely imaginary eigenvalues? If $n=2m$ and \begin{equation*} A= \left( \begin{array}{cc} 0 & -I  \\ I & 0  \end{array} \right) \end{equation*} then the first order part of $L$ is self-adjoint, so what condition can be imposed on $B$ so that the eigenvalues are not imaginary?",,"['functional-analysis', 'ordinary-differential-equations', 'hilbert-spaces']"
14,$\lambda f(x)+(1-\lambda)f(y)=f(\lambda x+(1-\lambda)y)$ implies $f $ linear?,implies  linear?,\lambda f(x)+(1-\lambda)f(y)=f(\lambda x+(1-\lambda)y) f ,"Let $X$ be a Banach space. $f:X \to X$ a continuous function. If we assume that $f$ satisfies the following convexity condition: $$\lambda f(x)+(1-\lambda)f(y)=f(\lambda x+(1-\lambda)y),$$ for all $x,y\in X$ and $0\leq \lambda \leq 1$. Does this imply that $f$ is automatically linear ?","Let $X$ be a Banach space. $f:X \to X$ a continuous function. If we assume that $f$ satisfies the following convexity condition: $$\lambda f(x)+(1-\lambda)f(y)=f(\lambda x+(1-\lambda)y),$$ for all $x,y\in X$ and $0\leq \lambda \leq 1$. Does this imply that $f$ is automatically linear ?",,"['real-analysis', 'functional-analysis', 'convex-analysis', 'banach-spaces', 'functional-equations']"
15,"Prob. 1, Sec. 3.3, in Erwine Kreyszig's INTRODUCTORY FUNCTIONAL ANALYSIS WITH APPLICATIONS","Prob. 1, Sec. 3.3, in Erwine Kreyszig's INTRODUCTORY FUNCTIONAL ANALYSIS WITH APPLICATIONS",,"Let $H$ be a Hilbert space, $M \subset H$ a convex subset, and $\left( x_n \right)$ a sequence in $M$ such that $\left\lVert x_n \right\rVert \to d$ , where $d = \inf_{x \in M} \lVert x \rVert$ . How to show that $\left( x_n \right)$ converges in $H$ ? What are the most surprising facts about the plane $\mathbb{R}^2$ and the space $\mathbb{R}^3$ that one can derive using the above result?","Let be a Hilbert space, a convex subset, and a sequence in such that , where . How to show that converges in ? What are the most surprising facts about the plane and the space that one can derive using the above result?",H M \subset H \left( x_n \right) M \left\lVert x_n \right\rVert \to d d = \inf_{x \in M} \lVert x \rVert \left( x_n \right) H \mathbb{R}^2 \mathbb{R}^3,"['real-analysis', 'analysis', 'functional-analysis', 'hilbert-spaces', 'inner-products']"
16,All derivations are directional derivatives [duplicate],All derivations are directional derivatives [duplicate],,"This question already has an answer here : Does a linear operator obeying Leibniz rule imply it is a differential operator? (1 answer) Closed 9 years ago . Let $X : C^{\infty}(\mathbb{R}^n) \rightarrow \mathbb{R}$ be a derivation, so i.e. linear and satisfying the Leibniz Rule  $$X(fg)=X(f) \cdot  g(a)+X(g) \cdot f(a)$$ for some fixed $a \in \mathbb{R}^n.$ Then I would like to see that $X = \sum_{i=1}^{n} a_i \frac{\partial}{\partial x_i}|_a.$ So I want to see that $X$ is essentially a directional derivative.","This question already has an answer here : Does a linear operator obeying Leibniz rule imply it is a differential operator? (1 answer) Closed 9 years ago . Let $X : C^{\infty}(\mathbb{R}^n) \rightarrow \mathbb{R}$ be a derivation, so i.e. linear and satisfying the Leibniz Rule  $$X(fg)=X(f) \cdot  g(a)+X(g) \cdot f(a)$$ for some fixed $a \in \mathbb{R}^n.$ Then I would like to see that $X = \sum_{i=1}^{n} a_i \frac{\partial}{\partial x_i}|_a.$ So I want to see that $X$ is essentially a directional derivative.",,"['calculus', 'real-analysis', 'analysis', 'functional-analysis', 'manifolds']"
17,Strong convergence of convex combinations of a weakly convergent sequence,Strong convergence of convex combinations of a weakly convergent sequence,,"Consider the Mazur's Lemma (H. Brezis - ""Functional analysis, ...""): Assume $(x_n)$ converges weakly to $x$. Then there exists a sequence $(y_n)$ made up of convex combinations of the $x_n$'s that converges strongly to $x$. The Lemma says that ""there exists a sequence...''. Is it true that every sequence $(y_n)$ made up of convex combinations of the $x_n$'s converges strongly to $x$? For example, if we consider  $$y_n = \frac{1}{n}(x_1 + x_2 + ... +x_n),$$ is it true that $y_n$ converges strongly to $x$?","Consider the Mazur's Lemma (H. Brezis - ""Functional analysis, ...""): Assume $(x_n)$ converges weakly to $x$. Then there exists a sequence $(y_n)$ made up of convex combinations of the $x_n$'s that converges strongly to $x$. The Lemma says that ""there exists a sequence...''. Is it true that every sequence $(y_n)$ made up of convex combinations of the $x_n$'s converges strongly to $x$? For example, if we consider  $$y_n = \frac{1}{n}(x_1 + x_2 + ... +x_n),$$ is it true that $y_n$ converges strongly to $x$?",,"['functional-analysis', 'convex-analysis', 'weak-convergence']"
18,If $T^{*}$ is injective then $T$ is surjective?,If  is injective then  is surjective?,T^{*} T,"If $T$ is a bounded linear map from the Hilbert space $H_1$ to the Hilbert space $H_2$, and $T^{*}$ is injective, then I know that $H_2$ is the closure of the range of $T$. But can I conclude that $T$ is surjective? Any hint would help. (Sorry if this is trivial.)","If $T$ is a bounded linear map from the Hilbert space $H_1$ to the Hilbert space $H_2$, and $T^{*}$ is injective, then I know that $H_2$ is the closure of the range of $T$. But can I conclude that $T$ is surjective? Any hint would help. (Sorry if this is trivial.)",,"['real-analysis', 'functional-analysis', 'hilbert-spaces']"
19,A Lemma about the operator space,A Lemma about the operator space,,"The following lemma comes from the book ""C*-algebras Finite-Dimensional Approximations"" by N.P. Brown and N. Ozawa P379 Lemma 13.2.3 Let $X_{i}\in B(H_{i})$ (i=1,2) be unital operator subspaces and let $\phi: X_{1}\rightarrow X_{2}$ be a unital complete isomery. Suppose that $X_{2}$ is spanned by unitary elements in $B(H_{2})$ . Then, $\phi$ uniquely extends to a -homomorphism between the C -subalgebras $C^{*}(X_{1})$ generated by $X_{i}$ in $B(H_{i})$ . Proof By Arveson's Extension Theorem, $\phi$ extends to a complete contractive map (i.e. $||\phi||_{cb}\leq1$ ) from $B(H_{1})\rightarrow B(H_{2})$ , which we still denote by $\phi$ . Since $\phi$ is unital, it has to be a u.c.p map. Since $\phi|_{X_{1}}$ is isometric and $X_{2}$ is spanned by unitary elements, $X_{1}$ is contained in the multiplicative domain of $\phi$ . Hence, $\phi$ is a *-homomorphism on $C^{*}(X_{1})$ . In the proof of this lemma, the author first use the ""operator space"" version of Arveson's Extension Theorem to get a c.c map $\phi$ from $B(H_{1})\rightarrow B(H_{2})$ . And then comes my question : The author says ""Since $\phi$ is unital, it has to be a u.c.p."" Why? The author says ""Since $\phi|_{X_{1}}$ is isometric and $X_{2}$ is spanned by unitary elements, $X_{1}$ is contained in the multiplicative domain of $\phi$ ."" Why?","The following lemma comes from the book ""C*-algebras Finite-Dimensional Approximations"" by N.P. Brown and N. Ozawa P379 Lemma 13.2.3 Let (i=1,2) be unital operator subspaces and let be a unital complete isomery. Suppose that is spanned by unitary elements in . Then, uniquely extends to a -homomorphism between the C -subalgebras generated by in . Proof By Arveson's Extension Theorem, extends to a complete contractive map (i.e. ) from , which we still denote by . Since is unital, it has to be a u.c.p map. Since is isometric and is spanned by unitary elements, is contained in the multiplicative domain of . Hence, is a *-homomorphism on . In the proof of this lemma, the author first use the ""operator space"" version of Arveson's Extension Theorem to get a c.c map from . And then comes my question : The author says ""Since is unital, it has to be a u.c.p."" Why? The author says ""Since is isometric and is spanned by unitary elements, is contained in the multiplicative domain of ."" Why?",X_{i}\in B(H_{i}) \phi: X_{1}\rightarrow X_{2} X_{2} B(H_{2}) \phi C^{*}(X_{1}) X_{i} B(H_{i}) \phi ||\phi||_{cb}\leq1 B(H_{1})\rightarrow B(H_{2}) \phi \phi \phi|_{X_{1}} X_{2} X_{1} \phi \phi C^{*}(X_{1}) \phi B(H_{1})\rightarrow B(H_{2}) \phi \phi|_{X_{1}} X_{2} X_{1} \phi,"['functional-analysis', 'operator-algebras', 'c-star-algebras']"
20,Weak convergence and lim inf and lim sup of the sequence of norms,Weak convergence and lim inf and lim sup of the sequence of norms,,"Assume $x_n$ is a sequence in a Banach space that converges weakly to $x$. Then we know that $\|x\| \leq \lim \inf \|x_n\|$. 1)But can we say that $\lim \inf \|x_n\| < \infty$ or is this in general wrong? 2) In a Hilbert space $X$ if $x_n$ converges weakly to $x$, then we know that $\lim \sup \|x_n\| \leq \|x\|$ iff $x_n$ converges to $x$ (in norm). But in general i guess it can be that $\lim \sup \|x_n\| = \infty$, right? Maybe you could give me some illuminating basic examples to illustrate these matters? Thank you.","Assume $x_n$ is a sequence in a Banach space that converges weakly to $x$. Then we know that $\|x\| \leq \lim \inf \|x_n\|$. 1)But can we say that $\lim \inf \|x_n\| < \infty$ or is this in general wrong? 2) In a Hilbert space $X$ if $x_n$ converges weakly to $x$, then we know that $\lim \sup \|x_n\| \leq \|x\|$ iff $x_n$ converges to $x$ (in norm). But in general i guess it can be that $\lim \sup \|x_n\| = \infty$, right? Maybe you could give me some illuminating basic examples to illustrate these matters? Thank you.",,"['functional-analysis', 'weak-convergence']"
21,What is the adjoint of the wave operator $\square_{g}$ in Sobolev Spaces?,What is the adjoint of the wave operator  in Sobolev Spaces?,\square_{g},"I have a three part question. The Laplace-Beltrami operator is an operator which is the typical example of a self-adjoint operator in $L^{2}$. I am wondering if this is also true for other Hilbert spaces $W^{k,2}$. The second part of the question and in which I am more interested is the following: How much the results are true for metrics with are not positive definitive. For example, the D'Alambertian (wave operator) in a general Pseudo-riemannian space. Below I am trying to calculate explicitly the adjoint for $\square_{g}$ in $H^{1}({{\cal{U}}_{t_{1}}^{+}},\nu_{g})$. Any comments will be appreciated. The wave operator is self-adjoint in $L^{2}({{\cal{U}}_{t_{1}}^{+}},\nu_{g})$ which means that for all $\psi,\omega \in C_{c}^{\infty}({{\cal{U}}_{t_{1}}^{+}})$ it is true that: $$(\psi,\square\omega)_{L^2}=(\square\psi,\omega)_{L^2}$$. which allow us to write: $$\int_{{{\cal{U}}_{t_{1}}^{+}}} \square_{g}\psi\omega \nu_{g} = \int_{{{\cal{U}}_{t_{1}}^{+}}} \psi\square_{g}\omega \nu_{g}$$ Now taking into account the contracted Ricci identity: $$\square(\psi_{,i})=(\square\psi)_{,i}+R^{j}_{i}\psi_{,j}$$ we have the following equality: $$ \int_{{{\cal{U}}_{t_{1}}^{+}}} (\square_{g}\psi)_{,i}\omega_{,i} \nu_{g} = \int_{{{\cal{U}}_{t_{1}}^{+}}} (\square_{g}(\psi_{,i})-R^{j}_{i}\psi_{,j})\omega_{,i}  \nu_{g} $$ Now the first term using the self adjointness of $\square_{g}$ can be rewritten as: $$ \int_{{{\cal{U}}_{t_{1}}^{+}}}\square_{g}(\psi_{,i})\omega_{,i} \nu_{g} = \int_{{{\cal{U}}_{t_{1}}^{+}}} \psi_{,_{i}}\square_{g}(\omega_{,i}) \nu_{g} $$ which again using the contracted Ricci identities gives: $$ \int_{{{\cal{U}}_{t_{1}}^{+}}}\psi_{,_{i}}\square_{g}(\omega_{,i}) \nu_{g} = \int_{{{\cal{U}}_{t_{1}}^{+}}} \psi_{,_{i}}((\square_{g}\omega)_{,i}+R^{j}_{i}\omega_{,j}) \nu_{g} $$ Now putting together the above equalities we have that: $$   \int_{{{\cal{U}}_{t_{1}}^{+}}}\square\psi\omega \nu_{g}+ \int_{{{\cal{U}}_{t_{1}}^{+}}}(\square\psi)_{i}\omega_{i} \nu_{g}=\int_{{{\cal{U}}_{t_{1}}^{+}}}\psi\square\omega \nu_{g}+ \int_{{{\cal{U}}_{t_{1}}^{+}}}\psi_{i}(\square\omega)_{i} \nu_{g}+\int_{{{\cal{U}}_{t_{1}}^{+}}}(R^{i}_{j}-R^{j}_{i})\psi_{,j}\omega_{,i}   \nu_{g} $$ It seems that in the case $R^{i}_{j}=0$ then the $\square_{g}$ is self adjoint. Is that correct? So to sum up. My main question is: How can I calculate the adjoint of the wave operator $\square_{g}$ in Sobolev Spaces?","I have a three part question. The Laplace-Beltrami operator is an operator which is the typical example of a self-adjoint operator in $L^{2}$. I am wondering if this is also true for other Hilbert spaces $W^{k,2}$. The second part of the question and in which I am more interested is the following: How much the results are true for metrics with are not positive definitive. For example, the D'Alambertian (wave operator) in a general Pseudo-riemannian space. Below I am trying to calculate explicitly the adjoint for $\square_{g}$ in $H^{1}({{\cal{U}}_{t_{1}}^{+}},\nu_{g})$. Any comments will be appreciated. The wave operator is self-adjoint in $L^{2}({{\cal{U}}_{t_{1}}^{+}},\nu_{g})$ which means that for all $\psi,\omega \in C_{c}^{\infty}({{\cal{U}}_{t_{1}}^{+}})$ it is true that: $$(\psi,\square\omega)_{L^2}=(\square\psi,\omega)_{L^2}$$. which allow us to write: $$\int_{{{\cal{U}}_{t_{1}}^{+}}} \square_{g}\psi\omega \nu_{g} = \int_{{{\cal{U}}_{t_{1}}^{+}}} \psi\square_{g}\omega \nu_{g}$$ Now taking into account the contracted Ricci identity: $$\square(\psi_{,i})=(\square\psi)_{,i}+R^{j}_{i}\psi_{,j}$$ we have the following equality: $$ \int_{{{\cal{U}}_{t_{1}}^{+}}} (\square_{g}\psi)_{,i}\omega_{,i} \nu_{g} = \int_{{{\cal{U}}_{t_{1}}^{+}}} (\square_{g}(\psi_{,i})-R^{j}_{i}\psi_{,j})\omega_{,i}  \nu_{g} $$ Now the first term using the self adjointness of $\square_{g}$ can be rewritten as: $$ \int_{{{\cal{U}}_{t_{1}}^{+}}}\square_{g}(\psi_{,i})\omega_{,i} \nu_{g} = \int_{{{\cal{U}}_{t_{1}}^{+}}} \psi_{,_{i}}\square_{g}(\omega_{,i}) \nu_{g} $$ which again using the contracted Ricci identities gives: $$ \int_{{{\cal{U}}_{t_{1}}^{+}}}\psi_{,_{i}}\square_{g}(\omega_{,i}) \nu_{g} = \int_{{{\cal{U}}_{t_{1}}^{+}}} \psi_{,_{i}}((\square_{g}\omega)_{,i}+R^{j}_{i}\omega_{,j}) \nu_{g} $$ Now putting together the above equalities we have that: $$   \int_{{{\cal{U}}_{t_{1}}^{+}}}\square\psi\omega \nu_{g}+ \int_{{{\cal{U}}_{t_{1}}^{+}}}(\square\psi)_{i}\omega_{i} \nu_{g}=\int_{{{\cal{U}}_{t_{1}}^{+}}}\psi\square\omega \nu_{g}+ \int_{{{\cal{U}}_{t_{1}}^{+}}}\psi_{i}(\square\omega)_{i} \nu_{g}+\int_{{{\cal{U}}_{t_{1}}^{+}}}(R^{i}_{j}-R^{j}_{i})\psi_{,j}\omega_{,i}   \nu_{g} $$ It seems that in the case $R^{i}_{j}=0$ then the $\square_{g}$ is self adjoint. Is that correct? So to sum up. My main question is: How can I calculate the adjoint of the wave operator $\square_{g}$ in Sobolev Spaces?",,"['functional-analysis', 'differential-geometry', 'hilbert-spaces']"
22,Domain of square root of a self-adjoint positive operator,Domain of square root of a self-adjoint positive operator,,"Let $A \geq 0$ be a densely defined self-adjoint positive operator on a Hilbert space $H$ obtained by Friedrichs extension, and let $Q$ be the densely defined quadratic form associated to $A$, that is, $Q(x, y) = \langle Ax, y\rangle$. Now it is clear that $x \in D(A) \Rightarrow Q(x, x) \in \mathbb{R}$. I remember reading somewhere that $D(A^{1/2})$ is defined as $\{x \in H : Q(x, x) \in \mathbb{R}\}$. If this is true, can I please have a reference? Thanks.","Let $A \geq 0$ be a densely defined self-adjoint positive operator on a Hilbert space $H$ obtained by Friedrichs extension, and let $Q$ be the densely defined quadratic form associated to $A$, that is, $Q(x, y) = \langle Ax, y\rangle$. Now it is clear that $x \in D(A) \Rightarrow Q(x, x) \in \mathbb{R}$. I remember reading somewhere that $D(A^{1/2})$ is defined as $\{x \in H : Q(x, x) \in \mathbb{R}\}$. If this is true, can I please have a reference? Thanks.",,"['functional-analysis', 'reference-request', 'operator-theory']"
23,"Projection on closed subspace of $L^1$, $L^{\infty}$","Projection on closed subspace of ,",L^1 L^{\infty},"For $p=1,\infty$ let $K$ be a closed subspace of $L^p(\mathbb{R},m)$. According to this question, it should be easy to find examples of $K$ and $f\in L^p(\mathbb{R},m)$ such that there exists a non-unique function $h\in K$ which minimizes $||f-g||_p ,g\in K.$ I'd appreciate some help finding such examples.","For $p=1,\infty$ let $K$ be a closed subspace of $L^p(\mathbb{R},m)$. According to this question, it should be easy to find examples of $K$ and $f\in L^p(\mathbb{R},m)$ such that there exists a non-unique function $h\in K$ which minimizes $||f-g||_p ,g\in K.$ I'd appreciate some help finding such examples.",,"['real-analysis', 'functional-analysis', 'hilbert-spaces', 'banach-spaces']"
24,What is abstraction of direction in considering vectors such as used in Engineering & Physics?,What is abstraction of direction in considering vectors such as used in Engineering & Physics?,,"In the use of vectors of engineering and physics, we encounter objects that obey the axioms of a vector space but also have two new attributes of length (or, magnitude) and direction (e.g. direction in space). In the study of vector spaces, you do not encounter these notions of length and direction.  To obtain the idea of length, you need to consider a vector space with added structure to include the concept of normed vector elements as commonly displayed in Banach Space or Hilbert space (or others).  Indeed, other attributes such as a metric (e.g. Banach), and inner product can be included. But, where does the notion of direction come in?  The closest I get is the idea of using direction cosines derived from the inner product of an N-dimensional space of vectors.  But, is this the only abstraction of direction (which is not much of an abstraction to me). I am not sure what I am looking for but I guess maybe if someone could name a formal algebraic structure (extension of Banach Space?) where direction is included as one of the added structures and then I could use that to study.","In the use of vectors of engineering and physics, we encounter objects that obey the axioms of a vector space but also have two new attributes of length (or, magnitude) and direction (e.g. direction in space). In the study of vector spaces, you do not encounter these notions of length and direction.  To obtain the idea of length, you need to consider a vector space with added structure to include the concept of normed vector elements as commonly displayed in Banach Space or Hilbert space (or others).  Indeed, other attributes such as a metric (e.g. Banach), and inner product can be included. But, where does the notion of direction come in?  The closest I get is the idea of using direction cosines derived from the inner product of an N-dimensional space of vectors.  But, is this the only abstraction of direction (which is not much of an abstraction to me). I am not sure what I am looking for but I guess maybe if someone could name a formal algebraic structure (extension of Banach Space?) where direction is included as one of the added structures and then I could use that to study.",,"['functional-analysis', 'vector-spaces', 'banach-spaces']"
25,What is the norm of the pre-multiplication by a fixed matrix operator?,What is the norm of the pre-multiplication by a fixed matrix operator?,,"Let $A \colon= \left(\alpha_{ij} \right)_{m\times n}$ be a given $m \times n$ matrix of complex numbers, and let the operator $T \colon \mathbb{C}^n \to \mathbb{C}^m$ be defined by  $$T(x) \colon= Ax \ \ \ \mbox{ for all } \  x \in \mathbb{C}^n,$$ where $\mathbb{C}$ denotes the set of all complex numbers, all vectors are to be understood as column vectors, and $Ax$ denotes the usual matrix product. Then $T$ is of course a linear operator. Let $r$ and $k$ be given real numbers such that $1 \leq r < +\infty$ and $1 \leq k < +\infty$. Then what is the norm of $T$ (i) if the norm on $\mathbb{C}^n$ is given by  $$\Vert x \Vert_k \colon= \left( \vert \xi_1\vert^k+ \cdots + \vert \xi_n \vert^k \right)^{\frac{1}{k}} \ \ \ \mbox{ for all }  \ x \colon= (\xi_1, \ldots, \xi_n) \in \mathbb{C}^n$$  and the norm on $\mathbb{C}^m$ is given by  $$\Vert y \Vert_r \colon= \left( \vert \eta_1 \vert^r + \cdots + \vert \eta_m \vert^r \right)^{\frac{1}{r}} \ \ \ \mbox{ for all } \ y \colon= (\eta_1, \ldots, \eta_m) \in \mathbb{C}^m?$$ (ii) if $\mathbb{C}^n$ is given the same norm as in (i) above  but $\mathbb{C}^m$ is given the maximum norm  $$\Vert y \Vert_{\infty} \colon= \max \left( \vert \eta_1 \vert, \ldots, \vert \eta_m \vert \right) \ \ \ \mbox{ for all } \ y \colon = (\eta_1, \ldots, \eta_m ) \in \mathbb{C}^m?$$ (iii) if $\mathbb{C}^n$ is given the maximum norm  $$\Vert x \Vert_{\infty} \colon= \max \left( \vert \xi_1 \vert, \ldots, \vert \xi_n \vert \right) \ \ \ \mbox{ for all } \ x \colon= (\xi_1, \ldots, \xi_n ) \in \mathbb{C}^n,$$ but $\mathbb{C}^m$ is given the same norm as in (i) above? (iv) if both $\mathbb{C}^n$ and $\mathbb{C}^m$ are given their respective  maximum norms, as in (ii) and (iii) above? Definition: Let $X$ and $Y$ be normed spaces both real or both complex, and let $T \colon X \to Y$ be a linear operator. Then $T$ is said to be bounded if there is a non-negative real number $c$ such that  $$\Vert T(x) \Vert_{Y} \leq c \ \Vert x \Vert_{X}  \ \ \ \mbox{ for all } x \in X,$$ and then the norm $\Vert T \Vert$ of $T$ is given by  the formula  $$\Vert T \Vert \colon= \sup \left\{ \ \frac{\Vert T(x) \Vert_Y}{\Vert x \Vert_X} \ \colon \ x \in X,  \ x \neq \theta_X \ \right\},$$ where $\theta_X$ denotes the zero vector in $X$. Or, equivalently,  $$\Vert T \Vert = \sup \left\{ \ \Vert T(x) \Vert_Y \ \colon \ x \in X, \ \Vert x \Vert_X = 1 \ \right\}.$$ It can be shown that if $X$ is finite-dimensional, then $T$ is bounded.","Let $A \colon= \left(\alpha_{ij} \right)_{m\times n}$ be a given $m \times n$ matrix of complex numbers, and let the operator $T \colon \mathbb{C}^n \to \mathbb{C}^m$ be defined by  $$T(x) \colon= Ax \ \ \ \mbox{ for all } \  x \in \mathbb{C}^n,$$ where $\mathbb{C}$ denotes the set of all complex numbers, all vectors are to be understood as column vectors, and $Ax$ denotes the usual matrix product. Then $T$ is of course a linear operator. Let $r$ and $k$ be given real numbers such that $1 \leq r < +\infty$ and $1 \leq k < +\infty$. Then what is the norm of $T$ (i) if the norm on $\mathbb{C}^n$ is given by  $$\Vert x \Vert_k \colon= \left( \vert \xi_1\vert^k+ \cdots + \vert \xi_n \vert^k \right)^{\frac{1}{k}} \ \ \ \mbox{ for all }  \ x \colon= (\xi_1, \ldots, \xi_n) \in \mathbb{C}^n$$  and the norm on $\mathbb{C}^m$ is given by  $$\Vert y \Vert_r \colon= \left( \vert \eta_1 \vert^r + \cdots + \vert \eta_m \vert^r \right)^{\frac{1}{r}} \ \ \ \mbox{ for all } \ y \colon= (\eta_1, \ldots, \eta_m) \in \mathbb{C}^m?$$ (ii) if $\mathbb{C}^n$ is given the same norm as in (i) above  but $\mathbb{C}^m$ is given the maximum norm  $$\Vert y \Vert_{\infty} \colon= \max \left( \vert \eta_1 \vert, \ldots, \vert \eta_m \vert \right) \ \ \ \mbox{ for all } \ y \colon = (\eta_1, \ldots, \eta_m ) \in \mathbb{C}^m?$$ (iii) if $\mathbb{C}^n$ is given the maximum norm  $$\Vert x \Vert_{\infty} \colon= \max \left( \vert \xi_1 \vert, \ldots, \vert \xi_n \vert \right) \ \ \ \mbox{ for all } \ x \colon= (\xi_1, \ldots, \xi_n ) \in \mathbb{C}^n,$$ but $\mathbb{C}^m$ is given the same norm as in (i) above? (iv) if both $\mathbb{C}^n$ and $\mathbb{C}^m$ are given their respective  maximum norms, as in (ii) and (iii) above? Definition: Let $X$ and $Y$ be normed spaces both real or both complex, and let $T \colon X \to Y$ be a linear operator. Then $T$ is said to be bounded if there is a non-negative real number $c$ such that  $$\Vert T(x) \Vert_{Y} \leq c \ \Vert x \Vert_{X}  \ \ \ \mbox{ for all } x \in X,$$ and then the norm $\Vert T \Vert$ of $T$ is given by  the formula  $$\Vert T \Vert \colon= \sup \left\{ \ \frac{\Vert T(x) \Vert_Y}{\Vert x \Vert_X} \ \colon \ x \in X,  \ x \neq \theta_X \ \right\},$$ where $\theta_X$ denotes the zero vector in $X$. Or, equivalently,  $$\Vert T \Vert = \sup \left\{ \ \Vert T(x) \Vert_Y \ \colon \ x \in X, \ \Vert x \Vert_X = 1 \ \right\}.$$ It can be shown that if $X$ is finite-dimensional, then $T$ is bounded.",,"['real-analysis', 'analysis', 'functional-analysis', 'normed-spaces']"
26,"if we change the norm, is it possible to make it complete ?","if we change the norm, is it possible to make it complete ?",,"There are many examples about $C^1[0,1]$ that is not complete under supremum norm. if we change the norm, is it possible to make it complete ? Are there any examples about this ?  Thank you for your help .","There are many examples about $C^1[0,1]$ that is not complete under supremum norm. if we change the norm, is it possible to make it complete ? Are there any examples about this ?  Thank you for your help .",,"['real-analysis', 'general-topology', 'functional-analysis']"
27,"Show that {$\phi (x-n), n\in \Bbb{Z}$} is an orthonormal sequence in $L^2(\Bbb{R})$",Show that {} is an orthonormal sequence in,"\phi (x-n), n\in \Bbb{Z} L^2(\Bbb{R})","Let $\phi$ be a compactly supported continuous function such that $\phi (x)=0$ outside of some finite interval. $\phi$ satisfies the following refinable equation: $$\phi (x)=\sum_{k=0}^{M}c_k\phi (2x-k)$$ and $\hat{\phi}(0)=1$ . $\hat{\phi}(\omega)$ is the Fourier transform of $\phi (x)$. I've shown that $\sum_{k=0}^{M}c_k=2$ Show that {$\phi (x-n), n\in \Bbb{Z}$} is an orthonormal sequence in $L^2(\Bbb{R})$ only if $$\sum_{k\in \Bbb{Z}} |\hat{\phi}(\omega + 2k\pi)|^2 = 1, \forall \omega \in \Bbb{R}$$ The question is asking for a biconditional proof so we have to prove both directions. For the forward direction, Since {$\phi (x-n), n\in \Bbb{Z}$} is an orthonormal sequence, we know that $$ \int_{-\infty}^{\infty} \phi (x-n)\phi (x-m) dx = \left\{\begin{array}{ll} 1 & : n=m\\ 0 & : n\neq m  \end{array} \right.\ \ \ (*)$$ We have that $$\int_{-\infty}^{\infty} \phi (x-n) e^{-i\omega (x-n)} dx = \hat{\phi}(\omega)\\\Rightarrow \int_{-\infty}^{\infty} \phi (x-n) e^{-i\omega x} dx = e^{-i\omega n}\hat{\phi}(\omega)$$ I'm not sure what to do next. Can I square both sides to try and get to the form of $(*)$ for the case $n=m$? Or am I not doing this correctly? Also I replaced $\omega$ with $\omega + 2k\pi$ but still nothing comes to mind about how to get to the required identity. Will greatly appreciate if someone can work out the reverse direction too, ie given $$ \sum_{k\in \Bbb{Z}} |\hat{\phi}(\omega + 2k\pi)|^2 = 1, \forall \omega \in \Bbb{R}$$ show that {$\phi (x-n), n\in \Bbb{Z}$} is an orthonormal sequence.","Let $\phi$ be a compactly supported continuous function such that $\phi (x)=0$ outside of some finite interval. $\phi$ satisfies the following refinable equation: $$\phi (x)=\sum_{k=0}^{M}c_k\phi (2x-k)$$ and $\hat{\phi}(0)=1$ . $\hat{\phi}(\omega)$ is the Fourier transform of $\phi (x)$. I've shown that $\sum_{k=0}^{M}c_k=2$ Show that {$\phi (x-n), n\in \Bbb{Z}$} is an orthonormal sequence in $L^2(\Bbb{R})$ only if $$\sum_{k\in \Bbb{Z}} |\hat{\phi}(\omega + 2k\pi)|^2 = 1, \forall \omega \in \Bbb{R}$$ The question is asking for a biconditional proof so we have to prove both directions. For the forward direction, Since {$\phi (x-n), n\in \Bbb{Z}$} is an orthonormal sequence, we know that $$ \int_{-\infty}^{\infty} \phi (x-n)\phi (x-m) dx = \left\{\begin{array}{ll} 1 & : n=m\\ 0 & : n\neq m  \end{array} \right.\ \ \ (*)$$ We have that $$\int_{-\infty}^{\infty} \phi (x-n) e^{-i\omega (x-n)} dx = \hat{\phi}(\omega)\\\Rightarrow \int_{-\infty}^{\infty} \phi (x-n) e^{-i\omega x} dx = e^{-i\omega n}\hat{\phi}(\omega)$$ I'm not sure what to do next. Can I square both sides to try and get to the form of $(*)$ for the case $n=m$? Or am I not doing this correctly? Also I replaced $\omega$ with $\omega + 2k\pi$ but still nothing comes to mind about how to get to the required identity. Will greatly appreciate if someone can work out the reverse direction too, ie given $$ \sum_{k\in \Bbb{Z}} |\hat{\phi}(\omega + 2k\pi)|^2 = 1, \forall \omega \in \Bbb{R}$$ show that {$\phi (x-n), n\in \Bbb{Z}$} is an orthonormal sequence.",,"['functional-analysis', 'fourier-analysis']"
28,Suppose $f$ fulfils a unity condition. Prove $\hat{f}(0)=1$,Suppose  fulfils a unity condition. Prove,f \hat{f}(0)=1,"Suppose that $ f \in L^1(\Bbb{R})\cap L^2(\Bbb{R})$ satisfies the following unity condition $$\sum_{k\in \Bbb{Z}} f(x-k) = 1\ \ \ \ , \forall x\in\Bbb{R}$$ Prove that $\hat{f}(0)=1$ Here $\hat{f}$is the fourier transform of $f$. I tried writing $\int_{-\infty}^{\infty} f(t-k)e^{-i\omega t}dt = e^{-i\omega k}\hat{f}(\omega)$, which implies that  $e^{i\omega k}\int_{-\infty}^{\infty} f(t-k)e^{-i\omega t}dt = \hat{f}(\omega)$ Therefore sub $\omega=0$ gives $\int_{-\infty}^{\infty} f(t-k)dt =\hat{f}(0)$ I am unsure how to proceed from there. Anyone can help me please?","Suppose that $ f \in L^1(\Bbb{R})\cap L^2(\Bbb{R})$ satisfies the following unity condition $$\sum_{k\in \Bbb{Z}} f(x-k) = 1\ \ \ \ , \forall x\in\Bbb{R}$$ Prove that $\hat{f}(0)=1$ Here $\hat{f}$is the fourier transform of $f$. I tried writing $\int_{-\infty}^{\infty} f(t-k)e^{-i\omega t}dt = e^{-i\omega k}\hat{f}(\omega)$, which implies that  $e^{i\omega k}\int_{-\infty}^{\infty} f(t-k)e^{-i\omega t}dt = \hat{f}(\omega)$ Therefore sub $\omega=0$ gives $\int_{-\infty}^{\infty} f(t-k)dt =\hat{f}(0)$ I am unsure how to proceed from there. Anyone can help me please?",,"['functional-analysis', 'fourier-analysis']"
29,Disjoint convex sets which cannot be separated by any continuous linear functional,Disjoint convex sets which cannot be separated by any continuous linear functional,,"This problem is out of Rudin's Functional analysis exercise 3.2. The problem is stated below. I'm really struggling with this chapter in general. It has a lot of new topics I have not seen before. Any help is appreciated. I am attaching some of my current thoughts on the problem. Suppose $L^2 = L^2([-1,1])$, with respect to Lebesgue measure. For each scalar $\alpha$, let $E_\alpha$, be the set of all continuous functions $f$ on $[-1,1]$ such that $ f(0) = \alpha$. Show that $E_\alpha$ is convex and that each is dense in $L^2$. Thus $E_\alpha$ and $E_\beta$ are disjoint convex sets (if $\alpha \neq \beta$) which cannot be seperated by any continusous linear functionals $\Lambda$ on $L^2$. Hint: What is $\Lambda(E_\alpha)$. proof: Suppose the given hypothesis above. To show convexity let functions $f, g \in E_\alpha$ be arbitrary. Then we observe that $h(x) = tf(x) + (1-t) g(x)$ for $0 \leq t \leq 1$ is a continuous function and that $h(0) = t f(0) + (1-t)g(0) = t\alpha + (1-t)\alpha = \alpha.$ Therefore $h(x) \in E_\alpha$ and we have $E_\alpha$ convex. To show that $E_\alpha$ is dense in $L^2$. Let $A$ be a collection of continuous functions in $L^2$. Need to show that $E_\alpha \cap A \neq \emptyset$, but not sure how... I assume that given $\epsilon > 0$ we need to take an arbitrary function $f \in A$ and show there exists a function $g \in E_\alpha$ such that for $h(x) = f(x) - g(x)$, $$ \|h(x)\| = \left ( \int_{-1}^{1}|h(x)|^2 d\mu \right )^{1/2} < \epsilon $$ It makes sense that the sets $E_\alpha$ and $E_\beta$ are disjoint because every function in each of them will have a different value at $0$. But the last statement confuses me. Also I am struggling with the hint, is $\Lambda(E_\alpha) = 0$? I know that $L^2$ is its own dual space, since $p=2$ and its conjugate $q=2$ as well, so $\Lambda \in L^2$. It seems like symmetry of interval may have something to do with it, along with convexity or $E_\alpha$. Thanks again for any help in advance.","This problem is out of Rudin's Functional analysis exercise 3.2. The problem is stated below. I'm really struggling with this chapter in general. It has a lot of new topics I have not seen before. Any help is appreciated. I am attaching some of my current thoughts on the problem. Suppose $L^2 = L^2([-1,1])$, with respect to Lebesgue measure. For each scalar $\alpha$, let $E_\alpha$, be the set of all continuous functions $f$ on $[-1,1]$ such that $ f(0) = \alpha$. Show that $E_\alpha$ is convex and that each is dense in $L^2$. Thus $E_\alpha$ and $E_\beta$ are disjoint convex sets (if $\alpha \neq \beta$) which cannot be seperated by any continusous linear functionals $\Lambda$ on $L^2$. Hint: What is $\Lambda(E_\alpha)$. proof: Suppose the given hypothesis above. To show convexity let functions $f, g \in E_\alpha$ be arbitrary. Then we observe that $h(x) = tf(x) + (1-t) g(x)$ for $0 \leq t \leq 1$ is a continuous function and that $h(0) = t f(0) + (1-t)g(0) = t\alpha + (1-t)\alpha = \alpha.$ Therefore $h(x) \in E_\alpha$ and we have $E_\alpha$ convex. To show that $E_\alpha$ is dense in $L^2$. Let $A$ be a collection of continuous functions in $L^2$. Need to show that $E_\alpha \cap A \neq \emptyset$, but not sure how... I assume that given $\epsilon > 0$ we need to take an arbitrary function $f \in A$ and show there exists a function $g \in E_\alpha$ such that for $h(x) = f(x) - g(x)$, $$ \|h(x)\| = \left ( \int_{-1}^{1}|h(x)|^2 d\mu \right )^{1/2} < \epsilon $$ It makes sense that the sets $E_\alpha$ and $E_\beta$ are disjoint because every function in each of them will have a different value at $0$. But the last statement confuses me. Also I am struggling with the hint, is $\Lambda(E_\alpha) = 0$? I know that $L^2$ is its own dual space, since $p=2$ and its conjugate $q=2$ as well, so $\Lambda \in L^2$. It seems like symmetry of interval may have something to do with it, along with convexity or $E_\alpha$. Thanks again for any help in advance.",,"['general-topology', 'functional-analysis', 'lebesgue-integral']"
30,Projection-valued measure property,Projection-valued measure property,,"I am reading the book functional analysis from Dirk Werner. In this book he introduces a Projection-valued measure (PVM) as follows. Let $L(H)$ denote the set of linear,bounded operators $T:H\to H$ on a   Hilbertspace $H$. Denote with $\Sigma$ the Borel-$\sigma$-algebra on   $\mathbb{R}$. A map $$ E : \Sigma \to L(H)$$ is called PVM if all   $E(A)$ are orthognal projections with $$ E(\emptyset) =0, \quad E(\mathbb{R})= Id $$    and for pairwise disjoint $A_1,A_2,\dots \in  \Sigma$ it holds $$ \sum_{i=1}^\infty E(A_i) (x) = E(\cup A_i)(x)  \quad \forall x \in H .$$ Now the author claims that is very easy to show that $$E(A) E(B) = E(B)E(A) = E(A\cap B)$$ holds, but he does not give a hint. Why is this true?","I am reading the book functional analysis from Dirk Werner. In this book he introduces a Projection-valued measure (PVM) as follows. Let $L(H)$ denote the set of linear,bounded operators $T:H\to H$ on a   Hilbertspace $H$. Denote with $\Sigma$ the Borel-$\sigma$-algebra on   $\mathbb{R}$. A map $$ E : \Sigma \to L(H)$$ is called PVM if all   $E(A)$ are orthognal projections with $$ E(\emptyset) =0, \quad E(\mathbb{R})= Id $$    and for pairwise disjoint $A_1,A_2,\dots \in  \Sigma$ it holds $$ \sum_{i=1}^\infty E(A_i) (x) = E(\cup A_i)(x)  \quad \forall x \in H .$$ Now the author claims that is very easy to show that $$E(A) E(B) = E(B)E(A) = E(A\cap B)$$ holds, but he does not give a hint. Why is this true?",,"['functional-analysis', 'measure-theory']"
31,Hahn-Banach theorem and complex linear functional,Hahn-Banach theorem and complex linear functional,,"I find this exercise but I cannot prove it. If $ X $ is a complex topological vector space and $ f \colon X \to \mathbb C  $ is nonzero continuous linear function, show that $ X \setminus  \ker f $ is connected.","I find this exercise but I cannot prove it. If $ X $ is a complex topological vector space and $ f \colon X \to \mathbb C  $ is nonzero continuous linear function, show that $ X \setminus  \ker f $ is connected.",,"['functional-analysis', 'topological-vector-spaces']"
32,Prove the operator is positive,Prove the operator is positive,,"I'm searching for an alternative proof of the following: Let $U$ be a self-adjoint operator on a Hilbert space $H$, define $m=\inf_{\|x\|=1}\langle Ux,x\rangle$ and $M=\sup_{\|x\|=1}\langle Ux,x\rangle$. If $P$ is a   polynomial such that $P(x)\geq 0$ for $x\in[m,M]$, prove that $P(U)$   is a positive operator. The usual proof is by using that $\sigma(P(U))=P(\sigma(U))$ ( Spectral mapping theorem for polynomials ). The intention is to prove this result without this theorem. Edit: We say that $U$ is a positive operator if $\langle Ux, x\rangle\geq 0$ for every $x\in H$.","I'm searching for an alternative proof of the following: Let $U$ be a self-adjoint operator on a Hilbert space $H$, define $m=\inf_{\|x\|=1}\langle Ux,x\rangle$ and $M=\sup_{\|x\|=1}\langle Ux,x\rangle$. If $P$ is a   polynomial such that $P(x)\geq 0$ for $x\in[m,M]$, prove that $P(U)$   is a positive operator. The usual proof is by using that $\sigma(P(U))=P(\sigma(U))$ ( Spectral mapping theorem for polynomials ). The intention is to prove this result without this theorem. Edit: We say that $U$ is a positive operator if $\langle Ux, x\rangle\geq 0$ for every $x\in H$.",,"['functional-analysis', 'operator-theory', 'alternative-proof']"
33,"If $f_n\to f$ a.e. and are bounded in $L^p$ norm, then $\int f_n g\to \int fg$ for any $g\in L^q$","If  a.e. and are bounded in  norm, then  for any",f_n\to f L^p \int f_n g\to \int fg g\in L^q,"Suppose $p>1$ and $q$ is its conjugate exponent. Suppose $f_n \rightarrow f~a.e.$ and $\sup_n\|f_n\|_p < \infty $ . prove that if $g \in L^q,$ then $\lim_{n \rightarrow \infty} \int f_ng=\int fg.$ Does this extend to the case where $p=1$ and $q=\infty$ ? If not, give a counter example. Progress I know I need to prove $|\int (f_ng-fg)| < \epsilon$ and $|\int (f_ng-fg)| < |\int (f_n-f)g|$ , if $f_n \rightarrow f$ in $ L^p $ . I can use Holder's inequality to get the result. My question is how to get $f_n \rightarrow f $ in $L^p $ by the hypothesis $f_n \rightarrow f~a.e.$ and $\sup_n\|f_n\|_p < \infty$ or use other method to get the result. thanks","Suppose and is its conjugate exponent. Suppose and . prove that if then Does this extend to the case where and ? If not, give a counter example. Progress I know I need to prove and , if in . I can use Holder's inequality to get the result. My question is how to get in by the hypothesis and or use other method to get the result. thanks","p>1 q f_n \rightarrow f~a.e. \sup_n\|f_n\|_p < \infty  g \in L^q, \lim_{n \rightarrow \infty} \int f_ng=\int fg. p=1 q=\infty |\int (f_ng-fg)| < \epsilon |\int (f_ng-fg)| < |\int (f_n-f)g| f_n \rightarrow f  L^p  f_n \rightarrow f  L^p  f_n \rightarrow f~a.e. \sup_n\|f_n\|_p < \infty","['real-analysis', 'functional-analysis', 'convergence-divergence', 'lp-spaces']"
34,a question about Riesz's theorem,a question about Riesz's theorem,,"Riesz's theorem:      Let $(V,\|\cdot\|)$ be a normed vector space, and suppose $C$ is a compact subset of $V$, moreover, $C$'s interior is not empty, and then please prove $\dim(V)<\infty$. Because I am not sure whether this statement is true or not, I need to prove it. Actually, I found a Riesz's lemma which states that Let X be a normed vector space and Y be a closed linear subspace of X such that Y does not equal to X and $\alpha$$\in R$,$0<\alpha<1$. Then there is $x_{\alpha}\in X$ such that $\|x_{\alpha}\|=1$ and $\|x_{\alpha}-y\|>\alpha$ for all y$\in Y$, are there any relation between them? How to prove Riesz's theorem? Thanks!","Riesz's theorem:      Let $(V,\|\cdot\|)$ be a normed vector space, and suppose $C$ is a compact subset of $V$, moreover, $C$'s interior is not empty, and then please prove $\dim(V)<\infty$. Because I am not sure whether this statement is true or not, I need to prove it. Actually, I found a Riesz's lemma which states that Let X be a normed vector space and Y be a closed linear subspace of X such that Y does not equal to X and $\alpha$$\in R$,$0<\alpha<1$. Then there is $x_{\alpha}\in X$ such that $\|x_{\alpha}\|=1$ and $\|x_{\alpha}-y\|>\alpha$ for all y$\in Y$, are there any relation between them? How to prove Riesz's theorem? Thanks!",,"['real-analysis', 'general-topology', 'analysis', 'functional-analysis']"
35,Bochner Integral: Derivative,Bochner Integral: Derivative,,"Given a Banach space $E$. Consider a continuous derivative: $$F'\in\mathcal{C}(\mathbb{R},E):\quad\int_\mathbb{R}\|F'(s)\|\mathrm{d}s<\infty$$ Then its integral computes as: $$\int_a^bF'(s)\mathrm{d}s=F(b)-F(a)$$ How to prove this by modern tools?","Given a Banach space $E$. Consider a continuous derivative: $$F'\in\mathcal{C}(\mathbb{R},E):\quad\int_\mathbb{R}\|F'(s)\|\mathrm{d}s<\infty$$ Then its integral computes as: $$\int_a^bF'(s)\mathrm{d}s=F(b)-F(a)$$ How to prove this by modern tools?",,"['real-analysis', 'functional-analysis', 'banach-spaces']"
36,Application of Banach Steinhaus theorem,Application of Banach Steinhaus theorem,,"Let $X,Y$ be Banach spaces and let $A:X\rightarrow Y,B:Y^*\rightarrow X^*$ be linear maps. Show that if for all $x\in X,y\in Y^*,y^*(Ax)=By^*(x)$, then both $A$ and $B$ are continuous","Let $X,Y$ be Banach spaces and let $A:X\rightarrow Y,B:Y^*\rightarrow X^*$ be linear maps. Show that if for all $x\in X,y\in Y^*,y^*(Ax)=By^*(x)$, then both $A$ and $B$ are continuous",,"['functional-analysis', 'banach-spaces']"
37,Orthonormal Sets and Compactness,Orthonormal Sets and Compactness,,"1) Let $\{u_n\}$ $(n=1,2,\ldots)$ be an orthonormal set in Hilbert space $H$. Show that this set is closed and bounded but not compact. 2) Let $Q$ be the set of all $x\in H$ of the form $$x=\sum\limits_{n = 1}^\infty c_n u_n$$ where $|c_n|\leq1/n$. Prove $Q$ is compact. 3) More generally, let $\{\delta_n\}$ be a sequence of positive numbers, and let $S$ be the set of all $x\in H$ of the form $$x=\sum_1^\infty c_n u_n$$ where $|c_n| \leq \delta_n$. Prove that $S$ is compact iff $$\sum_1^\infty\delta_n^2<\infty$$ 4) Prove that $H$ is not locally compact. I don't really even know where to start with this.","1) Let $\{u_n\}$ $(n=1,2,\ldots)$ be an orthonormal set in Hilbert space $H$. Show that this set is closed and bounded but not compact. 2) Let $Q$ be the set of all $x\in H$ of the form $$x=\sum\limits_{n = 1}^\infty c_n u_n$$ where $|c_n|\leq1/n$. Prove $Q$ is compact. 3) More generally, let $\{\delta_n\}$ be a sequence of positive numbers, and let $S$ be the set of all $x\in H$ of the form $$x=\sum_1^\infty c_n u_n$$ where $|c_n| \leq \delta_n$. Prove that $S$ is compact iff $$\sum_1^\infty\delta_n^2<\infty$$ 4) Prove that $H$ is not locally compact. I don't really even know where to start with this.",,"['complex-analysis', 'functional-analysis']"
38,Quaternions as a counterexample to the Gelfand–Mazur theorem,Quaternions as a counterexample to the Gelfand–Mazur theorem,,It seems that by the Gelfand–Mazur theorem quaternions are isomorphic to complex numbers. That is clearly wrong. So where is the catch? I think that I found the problem but it seams so subtle that would like to have confirmation from someone else and I would like to know where the proof of Gelfand–Mazur blows up.,It seems that by the Gelfand–Mazur theorem quaternions are isomorphic to complex numbers. That is clearly wrong. So where is the catch? I think that I found the problem but it seams so subtle that would like to have confirmation from someone else and I would like to know where the proof of Gelfand–Mazur blows up.,,"['functional-analysis', 'banach-algebras']"
39,Definition of the norm of a bounded linear operator.,Definition of the norm of a bounded linear operator.,,"I have a somewhat basic but confusing question regarding the definition of the norm for bounded linear operator. Suppose $f$ is a bounded linear operator, that is, there exists $M>0$ such that $\|f(x)\| \leq M\|x\|$. We define $\mathcal{M}=\{M\geq 0:\|f(x)\|\leq M\|x\|, \forall x\}$ $$\|f\|:=\inf\{M\geq 0:\|f(x)\|\leq M\|x\|, \forall x\}=\inf\{\mathcal{M}\}.$$ Now, the question is: how do we know that $\|f\|\in \mathcal{M}$, i.e. that $\|f(x)\|\leq \|f\|\|x\|$ for all $x$? This might seem a rather dumb question, but I've noticed a lot of books skirt around the technicalities of this, and even when they don't, the arguments don't strike me as convincing. The argument I've seen a lot goes like this: Since $\|f\| =\inf\{\mathcal{M}\}$, for all $n\in\mathbb{N}$, $$\|f\|+\frac1n \in \mathcal{M}.$$ Otherwise $\|f\|$ would not be the infimum. Therefore $$\|f(x)\| \leq \left( \|f\|+\frac1n \right)\|x\|,$$ for all $x$ and for all $n\in \mathbb{N}$. Letting $n\rightarrow \infty$ while keeping everything else fixed we obtain: $$\|f(x)\| \leq \|f\|\|x\|.$$  How exactly is $\|f(x)\| \leq \|f\|\|x\|$ a consequence of $\|f(x)\| \leq \left( \|f\|+\frac1n \right)\|x\|$?  I tried obtaining a proof by contradiction: let us replace $\frac1n$ by $\epsilon>0$ and let's assume that the latter is true but the conclusion is not. This would imply the existence of $x_0$ such that  $$\|f(x_0)\| > \|f\|\|x_0\|$$ while simultaneusly having $$\left( \|f\|+\epsilon \right)\|x_0\|\geq \|f(x_0)\|.$$ Without loss of generality we can assume $x_0\neq 0$, and putting this all together yields: $$\|f\|+\epsilon> \|f\|,$$ which is not a contradiction. More generally, the fact that $\|f\|+\epsilon \in \mathcal{M}$ amounts to saying that $\|f\|$ is a limit (or accumulation, depending on your definition) point of $\mathcal{M}$. It does not follow that the limit point must belong to the set in question (that would be saying $\mathcal{M}$ is closed (which is what we're trying to prove). Thanks in advance for any insights into this.","I have a somewhat basic but confusing question regarding the definition of the norm for bounded linear operator. Suppose $f$ is a bounded linear operator, that is, there exists $M>0$ such that $\|f(x)\| \leq M\|x\|$. We define $\mathcal{M}=\{M\geq 0:\|f(x)\|\leq M\|x\|, \forall x\}$ $$\|f\|:=\inf\{M\geq 0:\|f(x)\|\leq M\|x\|, \forall x\}=\inf\{\mathcal{M}\}.$$ Now, the question is: how do we know that $\|f\|\in \mathcal{M}$, i.e. that $\|f(x)\|\leq \|f\|\|x\|$ for all $x$? This might seem a rather dumb question, but I've noticed a lot of books skirt around the technicalities of this, and even when they don't, the arguments don't strike me as convincing. The argument I've seen a lot goes like this: Since $\|f\| =\inf\{\mathcal{M}\}$, for all $n\in\mathbb{N}$, $$\|f\|+\frac1n \in \mathcal{M}.$$ Otherwise $\|f\|$ would not be the infimum. Therefore $$\|f(x)\| \leq \left( \|f\|+\frac1n \right)\|x\|,$$ for all $x$ and for all $n\in \mathbb{N}$. Letting $n\rightarrow \infty$ while keeping everything else fixed we obtain: $$\|f(x)\| \leq \|f\|\|x\|.$$  How exactly is $\|f(x)\| \leq \|f\|\|x\|$ a consequence of $\|f(x)\| \leq \left( \|f\|+\frac1n \right)\|x\|$?  I tried obtaining a proof by contradiction: let us replace $\frac1n$ by $\epsilon>0$ and let's assume that the latter is true but the conclusion is not. This would imply the existence of $x_0$ such that  $$\|f(x_0)\| > \|f\|\|x_0\|$$ while simultaneusly having $$\left( \|f\|+\epsilon \right)\|x_0\|\geq \|f(x_0)\|.$$ Without loss of generality we can assume $x_0\neq 0$, and putting this all together yields: $$\|f\|+\epsilon> \|f\|,$$ which is not a contradiction. More generally, the fact that $\|f\|+\epsilon \in \mathcal{M}$ amounts to saying that $\|f\|$ is a limit (or accumulation, depending on your definition) point of $\mathcal{M}$. It does not follow that the limit point must belong to the set in question (that would be saying $\mathcal{M}$ is closed (which is what we're trying to prove). Thanks in advance for any insights into this.",,"['functional-analysis', 'normed-spaces']"
40,What's the difference between a Banach Algebra and a C*-algebra?,What's the difference between a Banach Algebra and a C*-algebra?,,I'm currently looking at going into a PhD program in mathematics and need to decide on a specialization. In meeting with my advisor he pushed me into looking at C*-algebras based on my interests. However specialists in the field seem to be rather rare and in expanding my search it seems that Banach Algebra's are closely related but I'm not entirely sure on the distinction (other than C*-algebras seem to be a specific form of Banach's). Can someone tell me the difference?,I'm currently looking at going into a PhD program in mathematics and need to decide on a specialization. In meeting with my advisor he pushed me into looking at C*-algebras based on my interests. However specialists in the field seem to be rather rare and in expanding my search it seems that Banach Algebra's are closely related but I'm not entirely sure on the distinction (other than C*-algebras seem to be a specific form of Banach's). Can someone tell me the difference?,,"['analysis', 'functional-analysis', 'operator-algebras', 'banach-algebras']"
41,"Properties of the multiplication operator, self-ajointness","Properties of the multiplication operator, self-ajointness",,"Let $(\Omega, \Sigma,\mu)$ a measurable space, $f:\Omega\to \mathbb{R}$ $\mu$-measurable. a.My first question: What does ""f $\mu$-measurable"" mean?I only know, what it means that ""f is measurable"" but including the measure ""$\mu$"", $\mu$-measurability? Now consider the operator $$M_f:D(M_f)=\{u\in L^2(\Omega); f\cdot u\in L^2(\Omega)\}\subseteq L^2(\Omega)\to L^2(\Omega),\; u\mapsto f\cdot u$$ $D(M_f)$ means the domain of the multiplicationoperator $M_f$. $M_f$ is always linear and if $f\in L^{\infty}(\Omega)$, $M_f$ is bounded. Now, let $f:\Omega\to \mathbb{R}$ be only measurable. I want to know, why $M_f$ is self-adjoint. Therefore I have to check the following properties: $$1. M_f\; \text{ is densely defined}$$ $$2. M_f\; \text{ is symmetric}$$ $$3. Im(M_f+iId)=L^2(\Omega);\; Im(M_f-iId)=L^2(\Omega)\; \text{(I think, it is enough to prove $Im(M_f+iId)=L^2(\Omega))$}$$ The second property is no problem. I stuck with 1. and 3. We had this proof in lecture and we said if we proved 1.:It is $\Omega=\bigcup_{n\in\mathbb{N}}\Omega_n$ with $\Omega_n=\{x\in\Omega; |f(x)|\le n\}$ and $\{u\in L^2(\Omega);\exists  n\in\mathbb{N}:$ such that $u=0$ almost everywhere in $\Omega\setminus\Omega_n$ $\}\subseteq D(M_f)$. And the set $\{u\in L^2(\Omega);\exists  n\in\mathbb{N}:$ such that $u=0$ almost everywhere in $\Omega\setminus\Omega_n$ $\}$ is dense in $L^2(\Omega)$. b. I don't understand this. Why is the set $\{u\in L^2(\Omega);\exists  n\in\mathbb{N}:$ such that $u=0$ almost everywhere in $\Omega\setminus\Omega_n$ $\}$ dense in $L^2(\Omega)$? And the proof of the 3. property we had in lecture: We only want to prove $Im(M_f+iId)=L^2(\Omega)$ (in other words: $M_f+iId$ is surjective). Let $g\in L^2(\Omega)$. It is $u:=\frac{g}{f+i}\in L^2(\Omega)$ (auxiliary calculation: $(M_f+iId)(u)=g\iff fu+iu=g\iff u=\frac{g}{f+i}$) and $f\cdot u\in L^2(\Omega)$. Therefore it is $u\in D(M_f)$ and $M_fu+iu=g$, so it is $g\in Im(M_f+iId)$. My questions: c. Why is  $u:=\frac{g}{f+i}\in L^2(\Omega)$ and $f\cdot u\in L^2(\Omega)$? Any help would be appreciated. Regards","Let $(\Omega, \Sigma,\mu)$ a measurable space, $f:\Omega\to \mathbb{R}$ $\mu$-measurable. a.My first question: What does ""f $\mu$-measurable"" mean?I only know, what it means that ""f is measurable"" but including the measure ""$\mu$"", $\mu$-measurability? Now consider the operator $$M_f:D(M_f)=\{u\in L^2(\Omega); f\cdot u\in L^2(\Omega)\}\subseteq L^2(\Omega)\to L^2(\Omega),\; u\mapsto f\cdot u$$ $D(M_f)$ means the domain of the multiplicationoperator $M_f$. $M_f$ is always linear and if $f\in L^{\infty}(\Omega)$, $M_f$ is bounded. Now, let $f:\Omega\to \mathbb{R}$ be only measurable. I want to know, why $M_f$ is self-adjoint. Therefore I have to check the following properties: $$1. M_f\; \text{ is densely defined}$$ $$2. M_f\; \text{ is symmetric}$$ $$3. Im(M_f+iId)=L^2(\Omega);\; Im(M_f-iId)=L^2(\Omega)\; \text{(I think, it is enough to prove $Im(M_f+iId)=L^2(\Omega))$}$$ The second property is no problem. I stuck with 1. and 3. We had this proof in lecture and we said if we proved 1.:It is $\Omega=\bigcup_{n\in\mathbb{N}}\Omega_n$ with $\Omega_n=\{x\in\Omega; |f(x)|\le n\}$ and $\{u\in L^2(\Omega);\exists  n\in\mathbb{N}:$ such that $u=0$ almost everywhere in $\Omega\setminus\Omega_n$ $\}\subseteq D(M_f)$. And the set $\{u\in L^2(\Omega);\exists  n\in\mathbb{N}:$ such that $u=0$ almost everywhere in $\Omega\setminus\Omega_n$ $\}$ is dense in $L^2(\Omega)$. b. I don't understand this. Why is the set $\{u\in L^2(\Omega);\exists  n\in\mathbb{N}:$ such that $u=0$ almost everywhere in $\Omega\setminus\Omega_n$ $\}$ dense in $L^2(\Omega)$? And the proof of the 3. property we had in lecture: We only want to prove $Im(M_f+iId)=L^2(\Omega)$ (in other words: $M_f+iId$ is surjective). Let $g\in L^2(\Omega)$. It is $u:=\frac{g}{f+i}\in L^2(\Omega)$ (auxiliary calculation: $(M_f+iId)(u)=g\iff fu+iu=g\iff u=\frac{g}{f+i}$) and $f\cdot u\in L^2(\Omega)$. Therefore it is $u\in D(M_f)$ and $M_fu+iu=g$, so it is $g\in Im(M_f+iId)$. My questions: c. Why is  $u:=\frac{g}{f+i}\in L^2(\Omega)$ and $f\cdot u\in L^2(\Omega)$? Any help would be appreciated. Regards",,"['functional-analysis', 'operator-theory']"
42,A consequence of the open mapping theorem,A consequence of the open mapping theorem,,"We let $f$ be a bounded and surjective linear map from the Banach space $X$ onto the Banach space $Y$ and put $$ r_0=\inf\{r: f(B^X(0,r)\supset B^Y(0,1)\}. $$ Using the open mapping theorem, I have shown that $0<r_0<\infty$ (in fact $||f||\geq 1/r_0$) and that  $$ f(B^X(0,r_0)\supset B^Y(0,1). $$  But I'm still wondering if $$ f(\overline{B^X(0,r_0)}\supset \overline{B^Y(0,1)} $$ holds or not?","We let $f$ be a bounded and surjective linear map from the Banach space $X$ onto the Banach space $Y$ and put $$ r_0=\inf\{r: f(B^X(0,r)\supset B^Y(0,1)\}. $$ Using the open mapping theorem, I have shown that $0<r_0<\infty$ (in fact $||f||\geq 1/r_0$) and that  $$ f(B^X(0,r_0)\supset B^Y(0,1). $$  But I'm still wondering if $$ f(\overline{B^X(0,r_0)}\supset \overline{B^Y(0,1)} $$ holds or not?",,"['real-analysis', 'functional-analysis', 'banach-spaces', 'normed-spaces']"
43,Example of maximal monotone operators in non-reflexive Banach spaces with applications in PDE,Example of maximal monotone operators in non-reflexive Banach spaces with applications in PDE,,"My question is about examples of maximal monotone operators that are defined in non-reflexive Banach spaces and have applications in PDEs, variational inequalities, etc (any application actually)? If possible I would like to exclude the convex subdifferential since it is a well-known example.","My question is about examples of maximal monotone operators that are defined in non-reflexive Banach spaces and have applications in PDEs, variational inequalities, etc (any application actually)? If possible I would like to exclude the convex subdifferential since it is a well-known example.",,"['functional-analysis', 'partial-differential-equations', 'convex-analysis', 'nonlinear-optimization', 'monotone-operator-theory']"
44,Is this map uniformly continuous? continuous?,Is this map uniformly continuous? continuous?,,"Let $s$ denote the metric space of all sequences of complex numbers with the metric  $$d(x,y) \colon= \sum_{j=1}^\infty \frac{1}{2^j} \frac{\vert \xi_j(x) - \xi_j(y)\vert}{1+\vert \xi_j(x) - \xi_j(y) \vert} $$  for all $x, y \in s$. Here $\xi_j(x)$ denotes the $j$-th term of $x$. Let $k$ be a fixed natural number. Then is the map $T_k \colon s \to \mathbb{C}$ defined by  $$T_k x \colon= \xi_k(x) \qquad \mbox{ for all } x \in s$$  uniformly continuous? continuous?","Let $s$ denote the metric space of all sequences of complex numbers with the metric  $$d(x,y) \colon= \sum_{j=1}^\infty \frac{1}{2^j} \frac{\vert \xi_j(x) - \xi_j(y)\vert}{1+\vert \xi_j(x) - \xi_j(y) \vert} $$  for all $x, y \in s$. Here $\xi_j(x)$ denotes the $j$-th term of $x$. Let $k$ be a fixed natural number. Then is the map $T_k \colon s \to \mathbb{C}$ defined by  $$T_k x \colon= \xi_k(x) \qquad \mbox{ for all } x \in s$$  uniformly continuous? continuous?",,"['real-analysis', 'analysis', 'functional-analysis', 'metric-spaces', 'continuity']"
45,If the scalar product are equal then the operators are equal.,If the scalar product are equal then the operators are equal.,,"I want to show the following: Let H be a $\mathbb C$ -hilbert space and $S,T\in L(X)$ If $\langle Sx,x \rangle = \langle Tx,x \rangle$ for all $x\in H$, then $S=T$ Any hints for me?","I want to show the following: Let H be a $\mathbb C$ -hilbert space and $S,T\in L(X)$ If $\langle Sx,x \rangle = \langle Tx,x \rangle$ for all $x\in H$, then $S=T$ Any hints for me?",,"['functional-analysis', 'operator-theory', 'hilbert-spaces']"
46,"Is $L^2(0,\infty;L^2(\Omega)) = L^2((0,\infty)\times \Omega)$?",Is ?,"L^2(0,\infty;L^2(\Omega)) = L^2((0,\infty)\times \Omega)","If $\Omega$ is a bounded $C^1$ domain, is $L^2(0,\infty;L^2(\Omega)) = L^2((0,\infty)\times \Omega)$? Are they the same? I know this is true when instead of $(0,\infty)$ we have a bounded interval.","If $\Omega$ is a bounded $C^1$ domain, is $L^2(0,\infty;L^2(\Omega)) = L^2((0,\infty)\times \Omega)$? Are they the same? I know this is true when instead of $(0,\infty)$ we have a bounded interval.",,"['functional-analysis', 'lp-spaces', 'bochner-spaces']"
47,Understanding a proof from Conway: showing existence of idempotents using functional calculus,Understanding a proof from Conway: showing existence of idempotents using functional calculus,,"I am studying the spectral theory of operators on Banach and Hilbert spaces, making use of Conway's A Course in Functional Analysis. In section VII.4, Conway states Proposition 4.11 as a consequence of the spectral mapping theorem. I am having trouble understanding a crucial step in this argument. I have summarized Conway's proof below. Proposition 4.11 Let $\mathcal{A}$ be a Banach algebra with identity. Suppose that $a \in \mathcal{A}$ with $\sigma(a) = F_1 \cup F_2$, where $F_1,F_2$ are disjoint, non-empty closed subsets of $\mathbb{C}$. Then there exists a nontrivial idempotent $e \in \mathcal{A}$ such that (i) If $b \in \mathcal{A}$ commutes with $a$, then $b$ commutes with $e$. (ii) If $a_1 = ae$ and $a_2 = a(1-e)$, then $a = a_1 + a_2$ and $a_1a_2 = a_2a_1 = 0$. (iii) $\sigma(a_1) = F_1 \cup \{0\}$, $\sigma(a_2) = F_2 \cup \{0\}$. This theorem is achieved by the following argument, with utilizes the Riesz functional calculus. We let $G_1, G_2$ be disjoint open subsets of $\mathbb{C}$ such that $G_1 \supset F_1, G_2 \supset F_2$. Then we choose $\Gamma$, a positively oriented system of closed curves such that $F_2$ lies in the inside of $\Gamma$ and $F_2$ lies on the outside of $\Gamma$. We then let $\chi_1$ be the characteristic function of $G_1$ an observe that $\chi_1$ is a holomorphic function on $G_1 \cup G_2$, hence holomorphic on a neighborhood of $\sigma(a) = F_1 \cup F_2 \subset G_1 \cup G_2$. We then use the Riesz functional calculus to define: $$\rho(\chi_1) = \int_{\Gamma} \chi_1(z) (z - a)^{-1} dz,$$ which is an element of $\mathcal{A}$. The claim is that, since $\rho$ is a homomorphism and $\chi_1^2 = \chi_1$, $\rho(\chi_1)$ is the idempotent we are looking for. The spectral mapping theorem allows us to prove (iii). My concern with the above proof is about the selection of the contour $\Gamma$. According to the definition of the functional calculus for an element $a \in \mathcal{A}$ (see, for instance (4.5) in the same section of Conway), we must choose $\Gamma$ so that the entire spectrum $\sigma(a)$ lies on the inside of $\Gamma$. But here it is specifically stated that $F_2$ (which is part of $\sigma(a)$) lies on the outside of $\Gamma$. How, then, can the contour integral above be considered as an instance of the functional calculus? Any clarification is greatly appreciated.","I am studying the spectral theory of operators on Banach and Hilbert spaces, making use of Conway's A Course in Functional Analysis. In section VII.4, Conway states Proposition 4.11 as a consequence of the spectral mapping theorem. I am having trouble understanding a crucial step in this argument. I have summarized Conway's proof below. Proposition 4.11 Let $\mathcal{A}$ be a Banach algebra with identity. Suppose that $a \in \mathcal{A}$ with $\sigma(a) = F_1 \cup F_2$, where $F_1,F_2$ are disjoint, non-empty closed subsets of $\mathbb{C}$. Then there exists a nontrivial idempotent $e \in \mathcal{A}$ such that (i) If $b \in \mathcal{A}$ commutes with $a$, then $b$ commutes with $e$. (ii) If $a_1 = ae$ and $a_2 = a(1-e)$, then $a = a_1 + a_2$ and $a_1a_2 = a_2a_1 = 0$. (iii) $\sigma(a_1) = F_1 \cup \{0\}$, $\sigma(a_2) = F_2 \cup \{0\}$. This theorem is achieved by the following argument, with utilizes the Riesz functional calculus. We let $G_1, G_2$ be disjoint open subsets of $\mathbb{C}$ such that $G_1 \supset F_1, G_2 \supset F_2$. Then we choose $\Gamma$, a positively oriented system of closed curves such that $F_2$ lies in the inside of $\Gamma$ and $F_2$ lies on the outside of $\Gamma$. We then let $\chi_1$ be the characteristic function of $G_1$ an observe that $\chi_1$ is a holomorphic function on $G_1 \cup G_2$, hence holomorphic on a neighborhood of $\sigma(a) = F_1 \cup F_2 \subset G_1 \cup G_2$. We then use the Riesz functional calculus to define: $$\rho(\chi_1) = \int_{\Gamma} \chi_1(z) (z - a)^{-1} dz,$$ which is an element of $\mathcal{A}$. The claim is that, since $\rho$ is a homomorphism and $\chi_1^2 = \chi_1$, $\rho(\chi_1)$ is the idempotent we are looking for. The spectral mapping theorem allows us to prove (iii). My concern with the above proof is about the selection of the contour $\Gamma$. According to the definition of the functional calculus for an element $a \in \mathcal{A}$ (see, for instance (4.5) in the same section of Conway), we must choose $\Gamma$ so that the entire spectrum $\sigma(a)$ lies on the inside of $\Gamma$. But here it is specifically stated that $F_2$ (which is part of $\sigma(a)$) lies on the outside of $\Gamma$. How, then, can the contour integral above be considered as an instance of the functional calculus? Any clarification is greatly appreciated.",,"['functional-analysis', 'spectral-theory', 'banach-algebras']"
48,Logic behind a proof in Topological Vector Spaces,Logic behind a proof in Topological Vector Spaces,,"I found the following result at the beginning of some notes on topological vector spaces (TVS). This is a quite fundamental result, that apparently is considered the corresponding version of the triangle inequality in TVS. Here there is the result with two proofs. Theorem: Let $X$ be a TVS. Then, for every $W \in \mathcal{N}_0$ there exists a $U \in \mathcal{N}_{0}^\text{sym}$ such that $U+U \subset W$. Proof 1: For any neighborhood $W \in \mathcal{N}_o$, there are neighborhoods $U_1, U_2$ of $0$ such that $U_1 + U_2 \subseteq W$. The result follows by setting $U = U_1 \cap U_2 \cap (-U_1) \cap (-U_2)$. Proof 2: Since $0+0 = 0$ and addition is continuous, there exist neighborhoods $U_1, U_2 \in \mathcal{N}_0$ such that $U_1 + U_2 \subseteq W$. The result follows by setting $U = U_1 \cap U_2 \cap (-U_1) \cap (-U_2)$, by noting that $U$ is symmetric. Here $\mathcal{N}_{0}$ denotes the neighborhood of $0$, and $\mathcal{N}_{0}^\text{sym}$ the symmetric neighborhood of $0$. Now, in both cases I am not sure about the logic behind them. In particular, here there are my doubts: 1) In proof 1 how do we come up with the fact that there are neighborhoods $U_1, U_2$ of $0$ – and not of something else! – such that $U_1 + U_2 \subseteq W$? 2) In proof 2 why do we focus on $0 + 0=0$? Is it because if we would have focused on – let's say– $2 + (-2) = 0$, we would have got still a neighborhood of $0$ by translation invariance, i.e. $\mathcal{N}_0 +2$? As you could notice, both questions are fairly similar and are concerned, in my opinion, with the inner logic of TVS, that completely baffles me right now. Thus, any feedback or help will be most welcome. Thank you for your time.","I found the following result at the beginning of some notes on topological vector spaces (TVS). This is a quite fundamental result, that apparently is considered the corresponding version of the triangle inequality in TVS. Here there is the result with two proofs. Theorem: Let $X$ be a TVS. Then, for every $W \in \mathcal{N}_0$ there exists a $U \in \mathcal{N}_{0}^\text{sym}$ such that $U+U \subset W$. Proof 1: For any neighborhood $W \in \mathcal{N}_o$, there are neighborhoods $U_1, U_2$ of $0$ such that $U_1 + U_2 \subseteq W$. The result follows by setting $U = U_1 \cap U_2 \cap (-U_1) \cap (-U_2)$. Proof 2: Since $0+0 = 0$ and addition is continuous, there exist neighborhoods $U_1, U_2 \in \mathcal{N}_0$ such that $U_1 + U_2 \subseteq W$. The result follows by setting $U = U_1 \cap U_2 \cap (-U_1) \cap (-U_2)$, by noting that $U$ is symmetric. Here $\mathcal{N}_{0}$ denotes the neighborhood of $0$, and $\mathcal{N}_{0}^\text{sym}$ the symmetric neighborhood of $0$. Now, in both cases I am not sure about the logic behind them. In particular, here there are my doubts: 1) In proof 1 how do we come up with the fact that there are neighborhoods $U_1, U_2$ of $0$ – and not of something else! – such that $U_1 + U_2 \subseteq W$? 2) In proof 2 why do we focus on $0 + 0=0$? Is it because if we would have focused on – let's say– $2 + (-2) = 0$, we would have got still a neighborhood of $0$ by translation invariance, i.e. $\mathcal{N}_0 +2$? As you could notice, both questions are fairly similar and are concerned, in my opinion, with the inner logic of TVS, that completely baffles me right now. Thus, any feedback or help will be most welcome. Thank you for your time.",,"['real-analysis', 'general-topology', 'functional-analysis', 'self-learning', 'topological-vector-spaces']"
49,Questions about the Fourier transform as a unitary transform,Questions about the Fourier transform as a unitary transform,,"As far as I know, the Fourier transform is a (linear) unitary transform: $T: \textbf{L}^2(-\infty, +\infty) \rightarrow \textbf{L}^2(-\infty, +\infty)$ where the basis functions {$e^{i \omega x} | \omega \in \textbf{R}, x \in \textbf{R}$} are not $L^2$-functions. Question 1 : does there exist such set of $\textbf{L}^2$ integrable basis in the $\textbf{L}^2(-\infty, +\infty)$? If it does exist, e.g. {$f(x) | x \in \textbf{R}$}, the Fourier transform of this set of basis must be a transform of basis in $\textbf{L}^2(-\infty, +\infty)$, since the $T$ is unitary - am I right? ( a little stretch on question 1: if there exist infinite number of sets of such basis, is there any general constructing/generating method to find one set of such basis? ) Question 2 : the basis element $e^{i \omega x} \notin \textbf{L}^2(-\infty, +\infty)$, however, the Dirac $\delta$-function as follows: $\delta(x - y) = \int_{-\infty}^{+\infty} { \frac{e^{i \omega x}}{\sqrt{2\pi}} \frac{e^{-i \omega y}}{\sqrt{2\pi}} d \omega}$ can be viewed as the Fourier (unitary) transform of the very own basis {$e^{i \omega x}$}. The Dirac functions {$\delta (x - y)$} can be viewed as the sort of ""singular"" basis (with the ""singular"" support). Interestingly, the $\delta (x - y)$ is $\textbf{L}^2$ integrable. Now the question about the uniqueness - is this the only possible ""basis transformation"" on the basis {$e^{i \omega x}$}? If not, what else can we have? Given that I am not familiar with the relationship of the Fourier analysis and Hilbert spaces (yet willing to learn), what resources/books would you recommend, if I need to learn more?","As far as I know, the Fourier transform is a (linear) unitary transform: $T: \textbf{L}^2(-\infty, +\infty) \rightarrow \textbf{L}^2(-\infty, +\infty)$ where the basis functions {$e^{i \omega x} | \omega \in \textbf{R}, x \in \textbf{R}$} are not $L^2$-functions. Question 1 : does there exist such set of $\textbf{L}^2$ integrable basis in the $\textbf{L}^2(-\infty, +\infty)$? If it does exist, e.g. {$f(x) | x \in \textbf{R}$}, the Fourier transform of this set of basis must be a transform of basis in $\textbf{L}^2(-\infty, +\infty)$, since the $T$ is unitary - am I right? ( a little stretch on question 1: if there exist infinite number of sets of such basis, is there any general constructing/generating method to find one set of such basis? ) Question 2 : the basis element $e^{i \omega x} \notin \textbf{L}^2(-\infty, +\infty)$, however, the Dirac $\delta$-function as follows: $\delta(x - y) = \int_{-\infty}^{+\infty} { \frac{e^{i \omega x}}{\sqrt{2\pi}} \frac{e^{-i \omega y}}{\sqrt{2\pi}} d \omega}$ can be viewed as the Fourier (unitary) transform of the very own basis {$e^{i \omega x}$}. The Dirac functions {$\delta (x - y)$} can be viewed as the sort of ""singular"" basis (with the ""singular"" support). Interestingly, the $\delta (x - y)$ is $\textbf{L}^2$ integrable. Now the question about the uniqueness - is this the only possible ""basis transformation"" on the basis {$e^{i \omega x}$}? If not, what else can we have? Given that I am not familiar with the relationship of the Fourier analysis and Hilbert spaces (yet willing to learn), what resources/books would you recommend, if I need to learn more?",,"['functional-analysis', 'fourier-analysis', 'hilbert-spaces', 'online-resources']"
50,Strict Bessel inequality in $\mathcal{l}^2$,Strict Bessel inequality in,\mathcal{l}^2,"I'm asked to give an example of an $x\in \mathcal{l}^2$ s.t. $$\sum_{j=1}^{\infty}|<e_j,x>|^2<||x||^2$$ Where $(e_j)$ is some orthonormal sequence. However, I think this question is more about finding a sequence $(e_j)$ s.t. there exist $x$ for which strict inequality hold then about finding an $x$ for which strict inequality holds. Because finding such $x$ is not always possible, most notably this is (I believe) impossible when the $(e_j)$ form a total basis for $\mathcal{l}^2$. So asking to find such $x$ with no regard to what sequence $(e_j)$ we are working with seems strange? More concretely, if we take $(e_j)=\delta_j$ then we have: $$\sum_{j=1}^{\infty}|<e_j,x>|^2=\sum_{j=1}^{\infty}|\sum_{i=1}^{\infty}e_{j_i}\bar{x_i}|^2=\sum_{j=1}^{\infty}|x_j|^2=||x||^2$$ So that we will always have equality. However if we take a sequence like $(e_j)=\delta_{2j}$ then we will have inequality for every $x$ that has a non-zero odd term. So I'm asking if I'm correct that it makes little sense to ask for an $x$ for which inequality hold with no regard for the sequence $(e_j)$?","I'm asked to give an example of an $x\in \mathcal{l}^2$ s.t. $$\sum_{j=1}^{\infty}|<e_j,x>|^2<||x||^2$$ Where $(e_j)$ is some orthonormal sequence. However, I think this question is more about finding a sequence $(e_j)$ s.t. there exist $x$ for which strict inequality hold then about finding an $x$ for which strict inequality holds. Because finding such $x$ is not always possible, most notably this is (I believe) impossible when the $(e_j)$ form a total basis for $\mathcal{l}^2$. So asking to find such $x$ with no regard to what sequence $(e_j)$ we are working with seems strange? More concretely, if we take $(e_j)=\delta_j$ then we have: $$\sum_{j=1}^{\infty}|<e_j,x>|^2=\sum_{j=1}^{\infty}|\sum_{i=1}^{\infty}e_{j_i}\bar{x_i}|^2=\sum_{j=1}^{\infty}|x_j|^2=||x||^2$$ So that we will always have equality. However if we take a sequence like $(e_j)=\delta_{2j}$ then we will have inequality for every $x$ that has a non-zero odd term. So I'm asking if I'm correct that it makes little sense to ask for an $x$ for which inequality hold with no regard for the sequence $(e_j)$?",,"['functional-analysis', 'hilbert-spaces']"
51,Finite rank volterra operator,Finite rank volterra operator,,"I am wondering when a Volterra integral operator $V_K:L_2(0,1)\to L_2(0,1)$ is a finite rank operator: $$V_Kf=\int_0^xK(x,y)f(y)dy$$ thanks in advance for your help","I am wondering when a Volterra integral operator $V_K:L_2(0,1)\to L_2(0,1)$ is a finite rank operator: $$V_Kf=\int_0^xK(x,y)f(y)dy$$ thanks in advance for your help",,"['analysis', 'functional-analysis', 'operator-theory', 'compact-operators']"
52,Invariant subspace of self-adjoint operator,Invariant subspace of self-adjoint operator,,"Assume that we have a self-adjoint operator $T: H \rightarrow H$ with a representation $T(x) = \sum_{n=0}^{\infty} \lambda_n \langle x , x_n \rangle x_n,$ so we have pure point spectrum. Now, I was wondering whether this classical Linear Algebra result still holds: Let $V \subset H$ be a finite-dimensional subspace such that $T(V) \subset V$, then $T|_V$ is a diagonizable matrix? Somehow, this sounds very natural, since $V$ is closed, so we can decompose the operator in $T: V \oplus V^{\perp} \rightarrow H$, but I have difficulties to characterize the invariant spaces.","Assume that we have a self-adjoint operator $T: H \rightarrow H$ with a representation $T(x) = \sum_{n=0}^{\infty} \lambda_n \langle x , x_n \rangle x_n,$ so we have pure point spectrum. Now, I was wondering whether this classical Linear Algebra result still holds: Let $V \subset H$ be a finite-dimensional subspace such that $T(V) \subset V$, then $T|_V$ is a diagonizable matrix? Somehow, this sounds very natural, since $V$ is closed, so we can decompose the operator in $T: V \oplus V^{\perp} \rightarrow H$, but I have difficulties to characterize the invariant spaces.",,"['real-analysis', 'analysis', 'functional-analysis', 'operator-theory', 'spectral-theory']"
53,"If $u \in L^1(0,\infty)$, then $|u(x)| \to 0$ as $x \to \infty$?","If , then  as ?","u \in L^1(0,\infty) |u(x)| \to 0 x \to \infty","Let $u \in L^1(0,\infty)$. Does this mean necessarily that $|u(x)| \to 0$ as $x \to \infty$? I think it has to decay otherwise the integral will be infinite. Can I get a hint on how to prove this? Thank you.","Let $u \in L^1(0,\infty)$. Does this mean necessarily that $|u(x)| \to 0$ as $x \to \infty$? I think it has to decay otherwise the integral will be infinite. Can I get a hint on how to prove this? Thank you.",,"['functional-analysis', 'lp-spaces']"
54,Is there a non-dense subspace of $\ell_1$ which is dense in every finite-dimensional subspace?,Is there a non-dense subspace of  which is dense in every finite-dimensional subspace?,\ell_1,"We want to find a subspace $F \subseteq \ell_1(\mathbb{R})$ such that for any finite $n \in \mathbb{N}$ and tuple $x=(x_1,\dots,x_n) \in \mathbb{R}^n$ we can find a sequence $(f^k)_k \subseteq F$ with $f^k|_n = (f^k_1,\dots,f^k_n) \to x$ for $k \to \infty$. Actually, I would expect that this does not exist, but I can't quite find a way to prove this. The usual approach for me would be to say that as $F$ is not dense, there is $h \in \ell^\infty$ orthogonal to $F$, but every initial segment can be approximated by a sequence in $F$ and then try to use a diagonal argument to obtain a contradiction, but I can't make that work. I hope somebody has some more intuition on that problem than I do.","We want to find a subspace $F \subseteq \ell_1(\mathbb{R})$ such that for any finite $n \in \mathbb{N}$ and tuple $x=(x_1,\dots,x_n) \in \mathbb{R}^n$ we can find a sequence $(f^k)_k \subseteq F$ with $f^k|_n = (f^k_1,\dots,f^k_n) \to x$ for $k \to \infty$. Actually, I would expect that this does not exist, but I can't quite find a way to prove this. The usual approach for me would be to say that as $F$ is not dense, there is $h \in \ell^\infty$ orthogonal to $F$, but every initial segment can be approximated by a sequence in $F$ and then try to use a diagonal argument to obtain a contradiction, but I can't make that work. I hope somebody has some more intuition on that problem than I do.",,"['sequences-and-series', 'functional-analysis', 'banach-spaces']"
55,"if a point is a root of all linear functionals on a normed space, then it's zero","if a point is a root of all linear functionals on a normed space, then it's zero",,"I've been trying to do some work as literal and detailed as possible in order to see I know my analysis for every detail. I tried to explain my self why if there is $x_0\in X$ ($X$ is a normed space) such that $\forall\varphi\in X^*:\:\varphi(x_0)=0$ then $x_0=0$. I could justify it using some high theorems, like consequences of the hahn-banach theorem, but I really don't feel like this is necessary and I would love a farily easy explanation if anyone is familiar with. If you feel like all the reasoning for this claim do require these kind of theorem please let me know that. Thanks a lot","I've been trying to do some work as literal and detailed as possible in order to see I know my analysis for every detail. I tried to explain my self why if there is $x_0\in X$ ($X$ is a normed space) such that $\forall\varphi\in X^*:\:\varphi(x_0)=0$ then $x_0=0$. I could justify it using some high theorems, like consequences of the hahn-banach theorem, but I really don't feel like this is necessary and I would love a farily easy explanation if anyone is familiar with. If you feel like all the reasoning for this claim do require these kind of theorem please let me know that. Thanks a lot",,['real-analysis']
56,If Banach space is algebraic sum of its two linear subspaces with mutually separated unit spheres then these subspaces are closed.,If Banach space is algebraic sum of its two linear subspaces with mutually separated unit spheres then these subspaces are closed.,,"Let $X$ be a Banach space, $Y$ and $Z$ be its linear subspaces such that $X = Y+Z$, $\ Y\cap Z = \{0\}$ and the unit spheres in $Y$ and $Z$ are separated, i.e. $$ \exists r>0 \ \ \lVert y-z\rVert \geq r \quad \forall y\in Y \ \ \forall z\in Z \ \text{ s.t. } \lVert y\rVert = \lVert z\rVert = 1 $$ The problem is to prove that $Y$ and $Z$ is closed in $X$. My idea was to estimate norms of $y\in Y$ and $z\in Z$ with norm of their sum, because by Banach's isomorphism theorem it is enough to show that $$ \exists C>0 \ \ \ \lVert y\rVert + \lVert z\rVert \leq C\lVert y+z\rVert \quad \forall y\in Y \ \forall z\in Z $$ what implies $X \cong Y\oplus Z$. Could anybody help me?","Let $X$ be a Banach space, $Y$ and $Z$ be its linear subspaces such that $X = Y+Z$, $\ Y\cap Z = \{0\}$ and the unit spheres in $Y$ and $Z$ are separated, i.e. $$ \exists r>0 \ \ \lVert y-z\rVert \geq r \quad \forall y\in Y \ \ \forall z\in Z \ \text{ s.t. } \lVert y\rVert = \lVert z\rVert = 1 $$ The problem is to prove that $Y$ and $Z$ is closed in $X$. My idea was to estimate norms of $y\in Y$ and $z\in Z$ with norm of their sum, because by Banach's isomorphism theorem it is enough to show that $$ \exists C>0 \ \ \ \lVert y\rVert + \lVert z\rVert \leq C\lVert y+z\rVert \quad \forall y\in Y \ \forall z\in Z $$ what implies $X \cong Y\oplus Z$. Could anybody help me?",,"['functional-analysis', 'banach-spaces']"
57,Is the Fractional integral operator well-defined?,Is the Fractional integral operator well-defined?,,"How to prove the fractional integral operator $J_{\alpha}:L^p(\Bbb R^+)\rightarrow L^p(\Bbb R^+)$ (of order $\alpha>0$) which is defined for each $f\in L^p(\Bbb R^+)$ by $$J_{\alpha}f(x):={1\over \Gamma(\alpha)}\int_0^x(x-y)^{\alpha-1}f(y)\,dy~~~~~x\in\Bbb R^+$$  is well-defined?","How to prove the fractional integral operator $J_{\alpha}:L^p(\Bbb R^+)\rightarrow L^p(\Bbb R^+)$ (of order $\alpha>0$) which is defined for each $f\in L^p(\Bbb R^+)$ by $$J_{\alpha}f(x):={1\over \Gamma(\alpha)}\int_0^x(x-y)^{\alpha-1}f(y)\,dy~~~~~x\in\Bbb R^+$$  is well-defined?",,"['functional-analysis', 'fractional-calculus']"
58,Proving existence of a linear functional,Proving existence of a linear functional,,"Let $(X, \| \cdot \|)$ be a normed space, and let $A, B ⊂ X$ be disjoint convex sets such that $B$ is closed and $A$ is compact. Prove that there exists $\varphi ∈ X^*$ such that $$\sup_{a\in A} \operatorname{Re}(\varphi)(a) < \inf_{b\in B} \operatorname{Re}(\varphi)(b).$$ I've been struggling a lot with this question, and would love some guide or help. My attempt was to use the geometric version of Hahn-Banach theorem, but I couldn't quite get this result. The problem is obviously showing the existence of a functional such that we have ""$<$"" instead of ""$\leq$"" using the extra properties of $A,B$. Any help would be blessed. thanks!","Let $(X, \| \cdot \|)$ be a normed space, and let $A, B ⊂ X$ be disjoint convex sets such that $B$ is closed and $A$ is compact. Prove that there exists $\varphi ∈ X^*$ such that $$\sup_{a\in A} \operatorname{Re}(\varphi)(a) < \inf_{b\in B} \operatorname{Re}(\varphi)(b).$$ I've been struggling a lot with this question, and would love some guide or help. My attempt was to use the geometric version of Hahn-Banach theorem, but I couldn't quite get this result. The problem is obviously showing the existence of a functional such that we have ""$<$"" instead of ""$\leq$"" using the extra properties of $A,B$. Any help would be blessed. thanks!",,['real-analysis']
59,Functional Derivative (Gateaux variation) of functional with convolution,Functional Derivative (Gateaux variation) of functional with convolution,,"I have the  following functional \begin{align*} F[f]=\int f(x) \log(g(x)) dx \end{align*} where $g(x)$ is given by convolution $g(x)=y(x) * f(x)=\int y(\tau) f(x-\tau) d\tau$ , so \begin{align*} F[f]&=\int f(x) \log(g(x)) dx=F[f]=\int f(x) \log(y(x) * f(x)) dx\\ &=\int f(x) \log \left(\int y(\tau) f(x-\tau) d\tau \right) dx \end{align*} assume that $y(x)$ is fixed. My Goal is to find functional derivative or Gateaux derivative with respect to $f$ . My issue is that I don't know how to deal with convolution when I take the derivative. What I did The variation is given by $\left[ \frac{d}{d\epsilon} F[f+\epsilon \theta] \right]_{\epsilon=0} $ . My question is do I work with the convolution term or no? Here are the two possibilities 1) Don't work with the convolution term \begin{align*} \left[ \frac{d}{d\epsilon} F[f+\epsilon \theta(x)] \right]_{\epsilon=0} =\left[ \frac{d}{d\epsilon} \int (f(x)+\epsilon \theta(x)) \log(g(x)) dx \right]_{\epsilon=0}= \int \theta \log(g(x)) dx \end{align*} 2) Work with the convolution term \begin{align*} \left[ \frac{d}{d\epsilon} F[f+\epsilon \theta] \right]_{\epsilon=0} =\left[ \frac{d}{d\epsilon} \int (f(x)+\epsilon \theta(x)) \log \left(\int y(\tau) (f(x-\tau)+\epsilon \theta(\tau)) d\tau \right) dx \right]_{\epsilon=0} \end{align*} In the integral inside convolution should it be $\theta(\tau)$ of $\theta(x)$ ?? Which way is correct? if the second way is correct how do I proceed next? I was also thinking about using chain rule of derivatives for functional \begin{align*} \frac{\delta F[G[f]]}{\delta f}=\frac{\delta F[G[f]]}{\delta G}\frac{\delta G[f]}{\delta f} \end{align*} but I don't think convolution is a functional, so I am not sure if this applies. Thank you fro any help in advance.","I have the  following functional where is given by convolution , so assume that is fixed. My Goal is to find functional derivative or Gateaux derivative with respect to . My issue is that I don't know how to deal with convolution when I take the derivative. What I did The variation is given by . My question is do I work with the convolution term or no? Here are the two possibilities 1) Don't work with the convolution term 2) Work with the convolution term In the integral inside convolution should it be of ?? Which way is correct? if the second way is correct how do I proceed next? I was also thinking about using chain rule of derivatives for functional but I don't think convolution is a functional, so I am not sure if this applies. Thank you fro any help in advance.","\begin{align*}
F[f]=\int f(x) \log(g(x)) dx
\end{align*} g(x) g(x)=y(x) * f(x)=\int y(\tau) f(x-\tau) d\tau \begin{align*}
F[f]&=\int f(x) \log(g(x)) dx=F[f]=\int f(x) \log(y(x) * f(x)) dx\\
&=\int f(x) \log \left(\int y(\tau) f(x-\tau) d\tau \right) dx
\end{align*} y(x) f \left[ \frac{d}{d\epsilon} F[f+\epsilon \theta] \right]_{\epsilon=0}
 \begin{align*}
\left[ \frac{d}{d\epsilon} F[f+\epsilon \theta(x)] \right]_{\epsilon=0}
=\left[ \frac{d}{d\epsilon} \int (f(x)+\epsilon \theta(x)) \log(g(x)) dx \right]_{\epsilon=0}=
\int \theta \log(g(x)) dx
\end{align*} \begin{align*}
\left[ \frac{d}{d\epsilon} F[f+\epsilon \theta] \right]_{\epsilon=0}
=\left[ \frac{d}{d\epsilon} \int (f(x)+\epsilon \theta(x)) \log \left(\int y(\tau) (f(x-\tau)+\epsilon \theta(\tau)) d\tau \right) dx \right]_{\epsilon=0}
\end{align*} \theta(\tau) \theta(x) \begin{align*}
\frac{\delta F[G[f]]}{\delta f}=\frac{\delta F[G[f]]}{\delta G}\frac{\delta G[f]}{\delta f}
\end{align*}","['functional-analysis', 'derivatives', 'calculus-of-variations', 'gateaux-derivative']"
60,If the dual unit ball of a normed space $X$ is metrizable in the weak-$*$ topology then $X$ is separable,If the dual unit ball of a normed space  is metrizable in the weak- topology then  is separable,X * X,"Let $X$ be a normed space and $(B_{X^*},w^*)$ be the unit ball of the dual space $X^*$ endowed with the weak-$*$ topology. Here is a proof a the fact that if $(B_{X^*},w^*)$ is metrizable then $X$ is separable : Set $K :=(B_{X^*},w^*)$. Since $K$ is metrizable we have that $C(K)$ -   the space of continuous functions over $K$  - is separable (proved as   a lemma). Consider the function $\Lambda : X \rightarrow C(K)$ defined   by $x \mapsto \widehat{x}\big|_K$ where $\widehat{x}\big|_K(x^*) =  x^*(x)$ for all $x \in X$. This function is well-defined and $$  \big\|\widehat{x}\big|_K\big\|_\infty = \sup\big\{|x^*(x)| : x^* \in > B_{X^*}\big\} = \|x\|$$ by Hahn-Banach. Hence $X$ is separable. I don't get why it should be obvious that $X$ is separable. Is the function $\Lambda$ onto ?","Let $X$ be a normed space and $(B_{X^*},w^*)$ be the unit ball of the dual space $X^*$ endowed with the weak-$*$ topology. Here is a proof a the fact that if $(B_{X^*},w^*)$ is metrizable then $X$ is separable : Set $K :=(B_{X^*},w^*)$. Since $K$ is metrizable we have that $C(K)$ -   the space of continuous functions over $K$  - is separable (proved as   a lemma). Consider the function $\Lambda : X \rightarrow C(K)$ defined   by $x \mapsto \widehat{x}\big|_K$ where $\widehat{x}\big|_K(x^*) =  x^*(x)$ for all $x \in X$. This function is well-defined and $$  \big\|\widehat{x}\big|_K\big\|_\infty = \sup\big\{|x^*(x)| : x^* \in > B_{X^*}\big\} = \|x\|$$ by Hahn-Banach. Hence $X$ is separable. I don't get why it should be obvious that $X$ is separable. Is the function $\Lambda$ onto ?",,"['general-topology', 'functional-analysis', 'normed-spaces']"
61,Convergence of truncation in $L^{p}$,Convergence of truncation in,L^{p},"If you have a truncation $T_{k}u$ defined as: $$ T_{k}u :=  \begin{cases} u,&  \text{ if }~ |u(x)| \leq 1\\ k\frac{u}{|u(x)|}, &   \text{ if }~|u(x)| > k  \end{cases}  $$ If you consider the truncation of a function $u \in L^{p}(\Omega)$, $p > 1$ and $\Omega$ is bounded. Then how would you show that $T_{k}u \rightarrow u$ in $L^{p}(\Omega)$. It is clear that $T_{k}u \rightarrow u$ pointwise a.e. and by the Dominated convergence Theorem you could show that $T_{k}u \rightarrow u$ in $L^{1}(\Omega)$. I'm finding it hard to show convergence of truncation in $L^{p}(\Omega)$. Does anyone have any idea how this can be shown?","If you have a truncation $T_{k}u$ defined as: $$ T_{k}u :=  \begin{cases} u,&  \text{ if }~ |u(x)| \leq 1\\ k\frac{u}{|u(x)|}, &   \text{ if }~|u(x)| > k  \end{cases}  $$ If you consider the truncation of a function $u \in L^{p}(\Omega)$, $p > 1$ and $\Omega$ is bounded. Then how would you show that $T_{k}u \rightarrow u$ in $L^{p}(\Omega)$. It is clear that $T_{k}u \rightarrow u$ pointwise a.e. and by the Dominated convergence Theorem you could show that $T_{k}u \rightarrow u$ in $L^{1}(\Omega)$. I'm finding it hard to show convergence of truncation in $L^{p}(\Omega)$. Does anyone have any idea how this can be shown?",,"['real-analysis', 'functional-analysis']"
62,$f_n \rightarrow 0$ in $L^1$ $\implies \sqrt{f_n} \rightarrow 0$ also?,in   also?,f_n \rightarrow 0 L^1 \implies \sqrt{f_n} \rightarrow 0,"Let $(X,\Sigma,\mu)$ be a finite measure space, and let $\{f_n : n \in \mathbb{N} \}$ be a sequence of non-negative measurable functions converging in the $L^1$ sense to the zero function. Show that the sequence $\{\sqrt{f_n}:n \in \mathbb{N} \}$ also converges in the $L^1$ sense to the zero function. So I have to somehow show that $$ \lim_{n \to \infty}\int_X\lvert\sqrt{f_n(x)}\rvert\;\mathbb{d}\mu(x) = 0 $$ If I'm honest I don't really know where to start. I think it's an easy question, but I'm new to this stuff. Any help appreciated!","Let $(X,\Sigma,\mu)$ be a finite measure space, and let $\{f_n : n \in \mathbb{N} \}$ be a sequence of non-negative measurable functions converging in the $L^1$ sense to the zero function. Show that the sequence $\{\sqrt{f_n}:n \in \mathbb{N} \}$ also converges in the $L^1$ sense to the zero function. So I have to somehow show that $$ \lim_{n \to \infty}\int_X\lvert\sqrt{f_n(x)}\rvert\;\mathbb{d}\mu(x) = 0 $$ If I'm honest I don't really know where to start. I think it's an easy question, but I'm new to this stuff. Any help appreciated!",,"['functional-analysis', 'measure-theory']"
63,Closed map on Banach Space,Closed map on Banach Space,,"Let $X,Y$ be Banach spaces and $T \in B(X,Y)$. Show that if $T$ sends every bounded closed subsets of $X$ onto closed sets of $Y$ then $T(X)$ is closed. It's true when the map is injective, but is it true in general?","Let $X,Y$ be Banach spaces and $T \in B(X,Y)$. Show that if $T$ sends every bounded closed subsets of $X$ onto closed sets of $Y$ then $T(X)$ is closed. It's true when the map is injective, but is it true in general?",,['functional-analysis']
64,norm of bounded linear operator restricted to dense subspace,norm of bounded linear operator restricted to dense subspace,,"I have no idea how to do this question I was given in class. Let $E$ and $F$ be normed spaces and let $T \in \mathcal{L}(E,F)$. Suppose that $E_0 \subseteq E$ is a dense subspace. Show that $\parallel T_{E_0} \parallel = \parallel T \parallel$. I know we can approximate every $x \in E$ by a sequence in $E_0$ but I can't see how to use it to get the result. Any clues would be a big help","I have no idea how to do this question I was given in class. Let $E$ and $F$ be normed spaces and let $T \in \mathcal{L}(E,F)$. Suppose that $E_0 \subseteq E$ is a dense subspace. Show that $\parallel T_{E_0} \parallel = \parallel T \parallel$. I know we can approximate every $x \in E$ by a sequence in $E_0$ but I can't see how to use it to get the result. Any clues would be a big help",,['functional-analysis']
65,$A y= b$ in $C(X)$,in,A y= b C(X),"Let $X$ be a compact Hausdorff topological space, and $C(X)$ denote the ring of all complex valued continuous functions on $X$. If $A\in C(X)^{m\times n}$, $b\in C(X)^{m\times 1}$, and for all $x\in X$, $b(x)$ belongs to the range of $A(x)$, then does there exist a $y\in C(X)^{n \times 1}$ such that $Ay =b$?","Let $X$ be a compact Hausdorff topological space, and $C(X)$ denote the ring of all complex valued continuous functions on $X$. If $A\in C(X)^{m\times n}$, $b\in C(X)^{m\times 1}$, and for all $x\in X$, $b(x)$ belongs to the range of $A(x)$, then does there exist a $y\in C(X)^{n \times 1}$ such that $Ay =b$?",,"['linear-algebra', 'functional-analysis', 'banach-algebras']"
66,Weak Convergence and Weak Topology,Weak Convergence and Weak Topology,,"In discussing weak topology of a normed space $X$, a lemma is given as follows. If $(x_n)$ is a sequence in $X$ converging weakly to $x$, then $x_n$ is bounded. I understand the proof of this lemma. My question here is why this lemma is given in the context of weak topology. That is, how are weak topology and weak convergence related ? Thank you!","In discussing weak topology of a normed space $X$, a lemma is given as follows. If $(x_n)$ is a sequence in $X$ converging weakly to $x$, then $x_n$ is bounded. I understand the proof of this lemma. My question here is why this lemma is given in the context of weak topology. That is, how are weak topology and weak convergence related ? Thank you!",,"['general-topology', 'analysis', 'functional-analysis', 'banach-spaces', 'weak-convergence']"
67,A question about maximal and minimal tensor product,A question about maximal and minimal tensor product,,"Let $A, B$ be two C*-algebras and $\pi: A\otimes_{\max} B\rightarrow M_{n}(\mathbb{C})$ be a representations, then  this $\pi$ can factor through the minimal tensor product $A\otimes_{\min} B$ ? (That is, do there exist two representations $\phi:A\otimes_{\max} B\rightarrow A\otimes_{min}B$ and $\psi: A\otimes_{\min} B\rightarrow M_{n}(\mathbb{C})$ such that $\pi=\psi\circ\phi$.)","Let $A, B$ be two C*-algebras and $\pi: A\otimes_{\max} B\rightarrow M_{n}(\mathbb{C})$ be a representations, then  this $\pi$ can factor through the minimal tensor product $A\otimes_{\min} B$ ? (That is, do there exist two representations $\phi:A\otimes_{\max} B\rightarrow A\otimes_{min}B$ and $\psi: A\otimes_{\min} B\rightarrow M_{n}(\mathbb{C})$ such that $\pi=\psi\circ\phi$.)",,"['functional-analysis', 'operator-algebras', 'c-star-algebras']"
68,Classification of operators,Classification of operators,,"I have a collection of questions about the limit point/circle concept and self-adjointness that are kind of connected, so I would like to ask them in a row. Apparently, an operator that is limit point/ limit circle has either deficiency indices one or two. This means for me that in Sturm-Liouville theory we do not want to study self-adjoint operators that would always have deficiency indices zero, but only these minimal operators, but I don't quite see why? 1.) I found the following definition: A minimal operator is a densely-defined, closed and symmetric operator (i.e. $T \subset T^{**} \subset T^*$). Thus, this operator cannot be unique, right? Especially, a self-adjoint extension (if one exists) of this operator would also be a minimal operator by this definition? Does this mean that the maximal operator is just the adjoint of the minimal operator (this would mean that the maximal operator is also closed, densely defined and an extension of the minimal operator)? 2.) Apparently for 2nd order Sturm-Liouville operators, it is said that there is this l.c. /l.p. dichotomy (so an operator is either l.p. or l.c.), but the self-adjoint extension would be neither l.c. or l.p. as the deficiency indices are clearly zero. This somehow contradicts the dichotomy in that way. I think there I am probably wrong about the definitions, so maybe anybody could help me with that? 3.) If we take a domain for a Sturm-Liouville operator in the l.c. case for example, then we have to impose boundary conditions sometimes in order to exclude unwanted solutions ( as we do for Legendre's differential equation). Now, if we study its self-adjoint extension ( which has a larger domain(!)), then I don't see how we could a priori know that this extension has not reintroduced the solutions that we tried to exclude or can this never happen? 4.) If we know that a Sturm-Liouville operator on a finite interval is l.p. and we know that the spectrum is purely discrete, does this always mean that there is an orthonormal basis of eigenvectors that span $L^2$? ( I mean I know that it is true for l.c. operators in general and we know in the l.c. case a priori that the spectrum is discrete, but can we conclude the existence of an ONB in the l.p. case, if we know that all elements in the spectrum must be discrete)?","I have a collection of questions about the limit point/circle concept and self-adjointness that are kind of connected, so I would like to ask them in a row. Apparently, an operator that is limit point/ limit circle has either deficiency indices one or two. This means for me that in Sturm-Liouville theory we do not want to study self-adjoint operators that would always have deficiency indices zero, but only these minimal operators, but I don't quite see why? 1.) I found the following definition: A minimal operator is a densely-defined, closed and symmetric operator (i.e. $T \subset T^{**} \subset T^*$). Thus, this operator cannot be unique, right? Especially, a self-adjoint extension (if one exists) of this operator would also be a minimal operator by this definition? Does this mean that the maximal operator is just the adjoint of the minimal operator (this would mean that the maximal operator is also closed, densely defined and an extension of the minimal operator)? 2.) Apparently for 2nd order Sturm-Liouville operators, it is said that there is this l.c. /l.p. dichotomy (so an operator is either l.p. or l.c.), but the self-adjoint extension would be neither l.c. or l.p. as the deficiency indices are clearly zero. This somehow contradicts the dichotomy in that way. I think there I am probably wrong about the definitions, so maybe anybody could help me with that? 3.) If we take a domain for a Sturm-Liouville operator in the l.c. case for example, then we have to impose boundary conditions sometimes in order to exclude unwanted solutions ( as we do for Legendre's differential equation). Now, if we study its self-adjoint extension ( which has a larger domain(!)), then I don't see how we could a priori know that this extension has not reintroduced the solutions that we tried to exclude or can this never happen? 4.) If we know that a Sturm-Liouville operator on a finite interval is l.p. and we know that the spectrum is purely discrete, does this always mean that there is an orthonormal basis of eigenvectors that span $L^2$? ( I mean I know that it is true for l.c. operators in general and we know in the l.c. case a priori that the spectrum is discrete, but can we conclude the existence of an ONB in the l.p. case, if we know that all elements in the spectrum must be discrete)?",,"['real-analysis', 'analysis']"
69,An example of a Banach space whose evaluation map is not surjective?,An example of a Banach space whose evaluation map is not surjective?,,"I have been giving the following corollary while studying functional analysis Let $X$ be a normed vector space. Then the evaluation map $$ev : X \to X'' ,  x \mapsto (f \mapsto fx) $$ is an isometry. I know this is a consquence of the Hahn-Banach Theorem I have been asked to find an example of a Banach space $X$ whose evaluation map is not surjective. But since the evaluation is a distance persevering function and is automatically injective how would one find an example of a space whose evaluation map is not a surjective?  I can't seem to think of a Banach space where this is true. Maybe my understanding of the double dual is too limited ?","I have been giving the following corollary while studying functional analysis Let $X$ be a normed vector space. Then the evaluation map $$ev : X \to X'' ,  x \mapsto (f \mapsto fx) $$ is an isometry. I know this is a consquence of the Hahn-Banach Theorem I have been asked to find an example of a Banach space $X$ whose evaluation map is not surjective. But since the evaluation is a distance persevering function and is automatically injective how would one find an example of a space whose evaluation map is not a surjective?  I can't seem to think of a Banach space where this is true. Maybe my understanding of the double dual is too limited ?",,"['functional-analysis', 'examples-counterexamples']"
70,Prove an integral operator is compact,Prove an integral operator is compact,,"The statement is like this, $K\subset\mathbb{R}$ is compact, the operator $A:L^\infty(K)\mapsto L^\infty(K)$ is defined by $f(x)\mapsto\int_K k(x,y)f(y)dy$. For $x\neq y$, $|k(x,y)|\leq M|x-y|^{\alpha-2}$, where $\alpha\in(0,2]$ and $M$ is constant. Show $A$ is a compact operator. I thought about Hilbert-Schmidt operator first. However, I noted that, although we can relax $L^\infty(K)$ to $L^2(K)$ (because $L^\infty\subset L^2$, is that correct?), $k\notin L^2(K\times K)$. So the condition for Hilbert-Schmidt is not satisfied. Information about $k$ is not enough, so it seems difficult to construct $\{k_n\}$ that makes $A_n$ compact and converge to $A$ - so that $A$ is compact. Therefore, now I am thinking about Arzela-Ascoli. So, I think I should take a bounded sequence $f_n(x)$ in $L^\infty(K)$, and prove that $Af_n$ is uniformly bounded and equicontinuous. For uniformly boundedness, $$ ||Af_n||_\infty=\sup_{x\in K}|\int_Kk(x,y)f_n(y)dy|\leq\sup_{x\in K}\int_KM|x-y|^{\alpha-2}|f_n(y)|dy $$ Then I get stuck in how to obtain the bound. For equicontinuity, $$ ||Af_n(x_1)-Af_n(x_2)||_\infty=\sup_{x_1,x_2\in K}|\int_K[k(x_1,y)-k(x_2,y)]f_n(y)dy|\leq\sup_{x_1,x_2\in K}\int_K|k(x_1,y)-k(x_2,y)||f_n(y)|dy $$ Again I got stuck in bounding the integral. So, is my general idea correct? If so, how should I proceed to bound the two integrals? Any helpful suggestion will be appreciated! Thanks in advance! Update: I got an idea in using sequence of compact operators. Maybe we can consider $k_n(x,y)$ such that $|k_n|\leq \frac{M}{|x-y|^{2-\alpha}+\delta}$, where $\delta>0$. $A_n$ can be defined by using $k_n$ as kernel. It seems $k_n\in L^2(K\times K)$ and $A_n$ is Hilbert-Schmidt integral operator, then $A_n$ is compact. Then we only need to prove that $A_n\rightarrow A$. I am working on details of that idea. By the way, the bound for $k(x,y)$ in the statement is correct. Actually, that form is common in Green's functions.","The statement is like this, $K\subset\mathbb{R}$ is compact, the operator $A:L^\infty(K)\mapsto L^\infty(K)$ is defined by $f(x)\mapsto\int_K k(x,y)f(y)dy$. For $x\neq y$, $|k(x,y)|\leq M|x-y|^{\alpha-2}$, where $\alpha\in(0,2]$ and $M$ is constant. Show $A$ is a compact operator. I thought about Hilbert-Schmidt operator first. However, I noted that, although we can relax $L^\infty(K)$ to $L^2(K)$ (because $L^\infty\subset L^2$, is that correct?), $k\notin L^2(K\times K)$. So the condition for Hilbert-Schmidt is not satisfied. Information about $k$ is not enough, so it seems difficult to construct $\{k_n\}$ that makes $A_n$ compact and converge to $A$ - so that $A$ is compact. Therefore, now I am thinking about Arzela-Ascoli. So, I think I should take a bounded sequence $f_n(x)$ in $L^\infty(K)$, and prove that $Af_n$ is uniformly bounded and equicontinuous. For uniformly boundedness, $$ ||Af_n||_\infty=\sup_{x\in K}|\int_Kk(x,y)f_n(y)dy|\leq\sup_{x\in K}\int_KM|x-y|^{\alpha-2}|f_n(y)|dy $$ Then I get stuck in how to obtain the bound. For equicontinuity, $$ ||Af_n(x_1)-Af_n(x_2)||_\infty=\sup_{x_1,x_2\in K}|\int_K[k(x_1,y)-k(x_2,y)]f_n(y)dy|\leq\sup_{x_1,x_2\in K}\int_K|k(x_1,y)-k(x_2,y)||f_n(y)|dy $$ Again I got stuck in bounding the integral. So, is my general idea correct? If so, how should I proceed to bound the two integrals? Any helpful suggestion will be appreciated! Thanks in advance! Update: I got an idea in using sequence of compact operators. Maybe we can consider $k_n(x,y)$ such that $|k_n|\leq \frac{M}{|x-y|^{2-\alpha}+\delta}$, where $\delta>0$. $A_n$ can be defined by using $k_n$ as kernel. It seems $k_n\in L^2(K\times K)$ and $A_n$ is Hilbert-Schmidt integral operator, then $A_n$ is compact. Then we only need to prove that $A_n\rightarrow A$. I am working on details of that idea. By the way, the bound for $k(x,y)$ in the statement is correct. Actually, that form is common in Green's functions.",,"['real-analysis', 'functional-analysis', 'compact-operators', 'integral-operators']"
71,QD C*-algebra's representation theorem,QD C*-algebra's representation theorem,,"Here is a question from the proof of the ""QD C*-algebra's representation theorem"" in P245 of book ""C*-algebras and Finite-Dimensional Approximations"" by Nate and Taka. For a separable unital C*-algebra $A$,  let $\phi_{n}: A\rightarrow M_{k(n)}(\mathbb{C})$ be u.c.p maps which are asymptotically multiplicative (i.e., $||\phi_{n}(ab)-\phi_{n}(a)\phi_{n}(b)||\rightarrow 0$ for all $a, b\in A$) and isometric (i.e., $||a||=\lim_{n\rightarrow\infty}||\phi_{n}(a)||$ for all $a\in A$). Consider the u.c.p. map   $$\Phi: A\rightarrow \prod_{n=1}^{\infty}M_{k(n)}(\mathbb{C})\subset B(\bigoplus_{n=1}^{\infty}\ell_{k(n)}^{2}),~~\Phi(a)=\bigoplus_{n=1}^{\infty}\phi_{n}(a).$$ Then, $\Phi$ is a faithful representation modulo the compact. My question are : How to verify $\Phi$ is a representation modulo the compact? (I think the ""faithful"" is easy from the isometric.) In the proof, the author says $\Phi$ obviously has finite-rank projections which commute with its image and tend strongly to one. What are the ""obvious"" finite-rank projections? Now, I attach the definition of representation modulo the compact in P19 of this book. If $\pi: B(H)\rightarrow Q(H)$ is the canonical mapping onto the Calkin algebra, $A$ is a unital C*-algebra and $\phi: A\rightarrow B(H)$ is a unital completely positive map, then we say that $\phi$ is a representation modulo the compacts if $\pi\circ\phi: A\rightarrow Q(H)$ is a *-homomorphism.","Here is a question from the proof of the ""QD C*-algebra's representation theorem"" in P245 of book ""C*-algebras and Finite-Dimensional Approximations"" by Nate and Taka. For a separable unital C*-algebra $A$,  let $\phi_{n}: A\rightarrow M_{k(n)}(\mathbb{C})$ be u.c.p maps which are asymptotically multiplicative (i.e., $||\phi_{n}(ab)-\phi_{n}(a)\phi_{n}(b)||\rightarrow 0$ for all $a, b\in A$) and isometric (i.e., $||a||=\lim_{n\rightarrow\infty}||\phi_{n}(a)||$ for all $a\in A$). Consider the u.c.p. map   $$\Phi: A\rightarrow \prod_{n=1}^{\infty}M_{k(n)}(\mathbb{C})\subset B(\bigoplus_{n=1}^{\infty}\ell_{k(n)}^{2}),~~\Phi(a)=\bigoplus_{n=1}^{\infty}\phi_{n}(a).$$ Then, $\Phi$ is a faithful representation modulo the compact. My question are : How to verify $\Phi$ is a representation modulo the compact? (I think the ""faithful"" is easy from the isometric.) In the proof, the author says $\Phi$ obviously has finite-rank projections which commute with its image and tend strongly to one. What are the ""obvious"" finite-rank projections? Now, I attach the definition of representation modulo the compact in P19 of this book. If $\pi: B(H)\rightarrow Q(H)$ is the canonical mapping onto the Calkin algebra, $A$ is a unital C*-algebra and $\phi: A\rightarrow B(H)$ is a unital completely positive map, then we say that $\phi$ is a representation modulo the compacts if $\pi\circ\phi: A\rightarrow Q(H)$ is a *-homomorphism.",,"['functional-analysis', 'operator-algebras', 'c-star-algebras', 'von-neumann-algebras']"
72,Hypothesis of Riesz's representation theorem,Hypothesis of Riesz's representation theorem,,"The Riesz representation states that Let $(H, \langle \cdot, \cdot \rangle)$ be a Hilbert space. Then for any linear and continuous functional $x^{\ast}:H \to \mathbb{K}$ there is a unique element $y \in H$ such that $x^\ast(x) = \langle x, y\rangle\ \forall x \in X$, and conversely, if $y \in H$, then $x^\ast:H \to K, x^\ast(x) = \langle x, y \rangle\ \forall x \in H$, is linear and continuous, with $\|x^\ast\|=\|y\|$. Now the question is, can we replace ""Hilbert space"" with ""prehilbertian space""? Well, for the moment I don't think so. My thoughts so far are that I have to find a non complete space $X$, such as $C[a, b]$ with the inner product $\langle f, g \rangle = \int_a^b f(t)g(t)dt$, and in that space, find a functional for which there exists $x \in X$ such that $x^\ast(x) \neq \langle x, y\rangle\ \forall y \in X$. Something else that I could do, is find a functional for which there exists $y, y' \in X$, $y \neq y'$ such that $\forall x \in X$, $x^\ast(x) = \langle x, y\rangle = \langle x, y' \rangle$. For the moment, I didn't manage to find a counter example, so if you could give me directions for where to look, that would be great. Tanks in advance for any answer.","The Riesz representation states that Let $(H, \langle \cdot, \cdot \rangle)$ be a Hilbert space. Then for any linear and continuous functional $x^{\ast}:H \to \mathbb{K}$ there is a unique element $y \in H$ such that $x^\ast(x) = \langle x, y\rangle\ \forall x \in X$, and conversely, if $y \in H$, then $x^\ast:H \to K, x^\ast(x) = \langle x, y \rangle\ \forall x \in H$, is linear and continuous, with $\|x^\ast\|=\|y\|$. Now the question is, can we replace ""Hilbert space"" with ""prehilbertian space""? Well, for the moment I don't think so. My thoughts so far are that I have to find a non complete space $X$, such as $C[a, b]$ with the inner product $\langle f, g \rangle = \int_a^b f(t)g(t)dt$, and in that space, find a functional for which there exists $x \in X$ such that $x^\ast(x) \neq \langle x, y\rangle\ \forall y \in X$. Something else that I could do, is find a functional for which there exists $y, y' \in X$, $y \neq y'$ such that $\forall x \in X$, $x^\ast(x) = \langle x, y\rangle = \langle x, y' \rangle$. For the moment, I didn't manage to find a counter example, so if you could give me directions for where to look, that would be great. Tanks in advance for any answer.",,"['functional-analysis', 'hilbert-spaces', 'riesz-representation-theorem']"
73,Generalised derivative and derivative of functions of bounded variation,Generalised derivative and derivative of functions of bounded variation,,"Let $f:\mathbb{R}\to\mathbb{C}$ be a function Lebesgue-integrable on any finite interval and let $K$ be the space of infinitely differentiable equal to 0 outside a given finite interval. Be the generalised derivative of $f$ defined as the distribution $T':K\to\mathbb{C}$ defined by $$T'(\varphi)=-\int_{\mathbb{R}}f(x)\varphi'(x)d\mu$$where the integral is Lebesgue's. I read in Kolmogorov-Fomin's Элементы теории функций и функционального анализа that if $f$ is a function of bounded variation and its derivative as a function corresponds to its generalised derivative as a distribution, i.e. if $\int_{\mathbb{R}}f'(x)\varphi(x)d\mu=-\int_{\mathbb{R}}f(x)\varphi'(x)d\mu$ for any $\varphi\in K$, then $f$ is absolutely continuous. I think I have been able to prove, by using the decomposition $f=H+\psi+\chi$ where $H$ is a step function, $\psi$ is absolutely continuous and $\chi$ is singular (i.e. with $\chi'(x)=0$ for almost all $x\in\mathbb{R}$), that, if the derivative of $f$ as a function corresponds to its generalised derivative as a distribution, then $f$ is identical to an absolutely continuous function almost everywhere , but I am not sure it holds everywhere and, if it did, I am not able to prove it. Does anybody know more about this fact? $\infty$ thanks! I apologise in advance if I had used some scarcely formal and rigourous language, but the book I am following is quite informal and I fear I am assimilating not-so-good customs.","Let $f:\mathbb{R}\to\mathbb{C}$ be a function Lebesgue-integrable on any finite interval and let $K$ be the space of infinitely differentiable equal to 0 outside a given finite interval. Be the generalised derivative of $f$ defined as the distribution $T':K\to\mathbb{C}$ defined by $$T'(\varphi)=-\int_{\mathbb{R}}f(x)\varphi'(x)d\mu$$where the integral is Lebesgue's. I read in Kolmogorov-Fomin's Элементы теории функций и функционального анализа that if $f$ is a function of bounded variation and its derivative as a function corresponds to its generalised derivative as a distribution, i.e. if $\int_{\mathbb{R}}f'(x)\varphi(x)d\mu=-\int_{\mathbb{R}}f(x)\varphi'(x)d\mu$ for any $\varphi\in K$, then $f$ is absolutely continuous. I think I have been able to prove, by using the decomposition $f=H+\psi+\chi$ where $H$ is a step function, $\psi$ is absolutely continuous and $\chi$ is singular (i.e. with $\chi'(x)=0$ for almost all $x\in\mathbb{R}$), that, if the derivative of $f$ as a function corresponds to its generalised derivative as a distribution, then $f$ is identical to an absolutely continuous function almost everywhere , but I am not sure it holds everywhere and, if it did, I am not able to prove it. Does anybody know more about this fact? $\infty$ thanks! I apologise in advance if I had used some scarcely formal and rigourous language, but the book I am following is quite informal and I fear I am assimilating not-so-good customs.",,"['functional-analysis', 'lebesgue-integral']"
74,$p$-summable series in a Banach space,-summable series in a Banach space,p,"Let $E$ be a Banach space and denote its dual space by $E^*$. Let $p \in [1, \infty)$ and $x : \mathbb{N}\rightarrow E$ be such that for every $\phi \in E^*$, $$\left( \sum_{n=1}^{\infty} \lvert \phi(x(n))\rvert^p \right)^{1/p}$$ is finite. Why is $$\sup_{\phi \in E^*, \lVert \phi \rVert \leq 1} \left( \sum_{n=1}^{\infty} \lvert \phi(x(n))\rvert^p \right)^{1/p}$$ finite.","Let $E$ be a Banach space and denote its dual space by $E^*$. Let $p \in [1, \infty)$ and $x : \mathbb{N}\rightarrow E$ be such that for every $\phi \in E^*$, $$\left( \sum_{n=1}^{\infty} \lvert \phi(x(n))\rvert^p \right)^{1/p}$$ is finite. Why is $$\sup_{\phi \in E^*, \lVert \phi \rVert \leq 1} \left( \sum_{n=1}^{\infty} \lvert \phi(x(n))\rvert^p \right)^{1/p}$$ finite.",,"['functional-analysis', 'banach-spaces']"
75,"Norm of the Linear (Integral) Operator on $C[0,1]$",Norm of the Linear (Integral) Operator on,"C[0,1]","There may be some similar questions being asked but I didn't see anything that was quite what I was looking for so I'll ask the question here. Let $X = (C[0,1],||.||_{\max})$ and $T:X \to X = \int^{t}_{0}x(\tau)d\tau$. The norm is defined in the following way: for $x(t) \in C[0,1]$, $||x(t)||_{max}= \textrm{max}\{|x(t)|:t \in [0,1]\}$. I want to show that $T$ is a bounded linear operator and find $||T||= \textrm{sup}\left\{\frac{||T(x(t))||}{||x(t)||}:x(t) \in C[0,1], x(t) \neq 0\right\}$. Showing that $T$ is a bounded linear operator is simple but I'm not sure about finding the norm of $T$. By the Extreme Value Theorem, we know that a maximum exists for $x(t)$ and $T(x(t))$ on $[0,1]$. But the supremum of the quotient of all such values may be infinite. How do we show this is not the case?","There may be some similar questions being asked but I didn't see anything that was quite what I was looking for so I'll ask the question here. Let $X = (C[0,1],||.||_{\max})$ and $T:X \to X = \int^{t}_{0}x(\tau)d\tau$. The norm is defined in the following way: for $x(t) \in C[0,1]$, $||x(t)||_{max}= \textrm{max}\{|x(t)|:t \in [0,1]\}$. I want to show that $T$ is a bounded linear operator and find $||T||= \textrm{sup}\left\{\frac{||T(x(t))||}{||x(t)||}:x(t) \in C[0,1], x(t) \neq 0\right\}$. Showing that $T$ is a bounded linear operator is simple but I'm not sure about finding the norm of $T$. By the Extreme Value Theorem, we know that a maximum exists for $x(t)$ and $T(x(t))$ on $[0,1]$. But the supremum of the quotient of all such values may be infinite. How do we show this is not the case?",,"['functional-analysis', 'operator-theory', 'normed-spaces']"
76,"Proving that the 1-norm, $||x||_1$ is not generated by inner products on $\mathbb{C}^n$","Proving that the 1-norm,  is not generated by inner products on",||x||_1 \mathbb{C}^n,"Proving that the 1-norm, $||x||_1$ is not generated by inner products on $\mathbb{C}^n$. Is it sufficient to take $x=(1,0)$, $y=(0,1)$ in $\mathbb{C}^2$ and just showing that  \begin{align} ||x+y||^2+||x-y||^2=8\\ 2||x||^2+2||y||^2=4 \end{align} As a counter example to show that it does not satisfy the parallelogram law? Or in proving this must it be an actual formal proof?","Proving that the 1-norm, $||x||_1$ is not generated by inner products on $\mathbb{C}^n$. Is it sufficient to take $x=(1,0)$, $y=(0,1)$ in $\mathbb{C}^2$ and just showing that  \begin{align} ||x+y||^2+||x-y||^2=8\\ 2||x||^2+2||y||^2=4 \end{align} As a counter example to show that it does not satisfy the parallelogram law? Or in proving this must it be an actual formal proof?",,"['analysis', 'functional-analysis']"
77,To sum $1+2+3+\cdots$ to $-\frac1{12}$,To sum  to,1+2+3+\cdots -\frac1{12},$$\sum_{n=1}^\infty\frac1{n^s}$$ only converges to $\zeta(s)$ if $\text{Re}(s)>1$ . Why should analytically continuing to $\zeta(-1)$ give the right answer?,only converges to if . Why should analytically continuing to give the right answer?,\sum_{n=1}^\infty\frac1{n^s} \zeta(s) \text{Re}(s)>1 \zeta(-1),"['analysis', 'complex-analysis']"
78,What do modern-day analysts actually do?,What do modern-day analysts actually do?,,"In an abstract algebra class, one learns about groups, rings, and fields, and (perhaps naively) conceives of a modern-day algebraist as someone who studies these sorts of structures.  One learns about the classification of finite simple groups , and gains some slight sense of what a group theorist might wonder about. In a topology class, one learns about topological spaces, and conceives of an algebraic topologist as someone who studies topological spaces, their algebraic invariants, and wonders about ways of classifying such spaces.  Similarly, a differential geometer might be described as someone who studies manifolds and their invariants, and an algebraic geometer as someone who studies varieties and schemes and their invariants. Now, obviously these sorts of one-sentence descriptions are rather simplistic, especially since many (most?) mathematicians work at the interface of a variety of different areas. That being said, I feel that I have absolutely no conception of what contemporary analysts actually do.  My sense is that contemporary analysis does not , for example, resemble the material found in (say) Folland's text . To be slightly more concrete, my question boils down to these: What areas of analysis are at the center of active research? What sort of questions are analysts concerned with?  What are some major themes that each subject is concerned with?  What are the big-picture goals of each subject? My sense is that current areas of research include: Harmonic analysis (and Fourier analysis ) Operator theory Partial differential equations (PDE) Several complex variables (SCV) Geometric measure theory (GMT) My sense is that analysts care about things like regularity, growth, and oscillations, and might be concerned with: Approximation problems Interpolation problems Optimization problems Boundary-value problems However, all of this is really the extent of my understanding. Note on motivation: Just to be clear, I am someone who really likes analysis.  Part of my motivation for asking (other than curiosity) is that I seem to meet very few American undergraduates or first-year graduate students who are interested in pursuing analysis, and sometimes wonder if this is because few of us seem to have any idea what analysts actually do. Note also: Saying that analysts are mathematicians who really like estimates does not count :-) Apologies if this question is too vague or too broad.","In an abstract algebra class, one learns about groups, rings, and fields, and (perhaps naively) conceives of a modern-day algebraist as someone who studies these sorts of structures.  One learns about the classification of finite simple groups , and gains some slight sense of what a group theorist might wonder about. In a topology class, one learns about topological spaces, and conceives of an algebraic topologist as someone who studies topological spaces, their algebraic invariants, and wonders about ways of classifying such spaces.  Similarly, a differential geometer might be described as someone who studies manifolds and their invariants, and an algebraic geometer as someone who studies varieties and schemes and their invariants. Now, obviously these sorts of one-sentence descriptions are rather simplistic, especially since many (most?) mathematicians work at the interface of a variety of different areas. That being said, I feel that I have absolutely no conception of what contemporary analysts actually do.  My sense is that contemporary analysis does not , for example, resemble the material found in (say) Folland's text . To be slightly more concrete, my question boils down to these: What areas of analysis are at the center of active research? What sort of questions are analysts concerned with?  What are some major themes that each subject is concerned with?  What are the big-picture goals of each subject? My sense is that current areas of research include: Harmonic analysis (and Fourier analysis ) Operator theory Partial differential equations (PDE) Several complex variables (SCV) Geometric measure theory (GMT) My sense is that analysts care about things like regularity, growth, and oscillations, and might be concerned with: Approximation problems Interpolation problems Optimization problems Boundary-value problems However, all of this is really the extent of my understanding. Note on motivation: Just to be clear, I am someone who really likes analysis.  Part of my motivation for asking (other than curiosity) is that I seem to meet very few American undergraduates or first-year graduate students who are interested in pursuing analysis, and sometimes wonder if this is because few of us seem to have any idea what analysts actually do. Note also: Saying that analysts are mathematicians who really like estimates does not count :-) Apologies if this question is too vague or too broad.",,"['analysis', 'complex-analysis', 'soft-question', 'partial-differential-equations', 'harmonic-analysis']"
79,"What is a good complex analysis textbook, barring Ahlfors's?","What is a good complex analysis textbook, barring Ahlfors's?",,"I'm out of college, and trying to self-learn complex analysis. I'm finding Ahlfors' text difficult. Any recommendations? I'm probably at an intermediate sophistication level for an undergrad. (Bonus points if the text has a section on the Riemann Zeta function and the Prime Number Theorem.)","I'm out of college, and trying to self-learn complex analysis. I'm finding Ahlfors' text difficult. Any recommendations? I'm probably at an intermediate sophistication level for an undergrad. (Bonus points if the text has a section on the Riemann Zeta function and the Prime Number Theorem.)",,"['complex-analysis', 'reference-request', 'book-recommendation']"
80,How to prove Euler's formula: $e^{i\varphi}=\cos(\varphi) +i\sin(\varphi)$?,How to prove Euler's formula: ?,e^{i\varphi}=\cos(\varphi) +i\sin(\varphi),Could you provide a proof of Euler's formula: $e^{i\varphi}=\cos(\varphi) +i\sin(\varphi)$?,Could you provide a proof of Euler's formula: $e^{i\varphi}=\cos(\varphi) +i\sin(\varphi)$?,,"['complex-analysis', 'complex-numbers']"
81,What is the Riemann-Zeta function?,What is the Riemann-Zeta function?,,"In laymen's terms, as much as possible: What is the Riemann-Zeta function, and why does it come up so often with relation to prime numbers?","In laymen's terms, as much as possible: What is the Riemann-Zeta function, and why does it come up so often with relation to prime numbers?",,"['terminology', 'complex-analysis', 'prime-numbers', 'riemann-zeta']"
82,Intuitive explanation of Cauchy's Integral Formula in Complex Analysis,Intuitive explanation of Cauchy's Integral Formula in Complex Analysis,,"There is a theorem that states that if $f$ is analytic in a domain $D$, and the closed disc {$  z:|z-\alpha|\leq r$} contained in $D$, and $C$ denotes the disc's boundary followed in the positive direction, then for every $z$ in the disc we can write: $$f(z)=\frac{1}{2\pi i}\int\frac{f(\zeta)}{\zeta-z}d\zeta$$ My question is: What is the intuitive explanation of this formula? (For example, but not necessary, geometrically.) (Just to clarify - I know the proof of this theorem, I'm just trying to understand where does this exact formula come from.)","There is a theorem that states that if $f$ is analytic in a domain $D$, and the closed disc {$  z:|z-\alpha|\leq r$} contained in $D$, and $C$ denotes the disc's boundary followed in the positive direction, then for every $z$ in the disc we can write: $$f(z)=\frac{1}{2\pi i}\int\frac{f(\zeta)}{\zeta-z}d\zeta$$ My question is: What is the intuitive explanation of this formula? (For example, but not necessary, geometrically.) (Just to clarify - I know the proof of this theorem, I'm just trying to understand where does this exact formula come from.)",,"['intuition', 'complex-analysis']"
83,What's the difference between $\mathbb{R}^2$ and the complex plane?,What's the difference between  and the complex plane?,\mathbb{R}^2,"I haven't taken any complex analysis course yet, but now I have this question that relates to it. Let's have a look at a very simple example. Suppose $x,y$ and $z$ are the Cartesian coordinates and we have a function $z=f(x,y)=\cos(x)+\sin(y)$. However, now I change the $\mathbb{R}^2$ plane $x,y$ to complex plane and make a new function, $z=\cos(t)+i\sin(t)$. So, can anyone tell me some famous and fundamental differences between complex plane and $\mathbb{R}^2$ by this example, like some features $\mathbb{R}^2 $ has but complex plane doesn't or the other way around. (Actually I am trying to understand why electrical engineers always want to put signal into the complex numbers rather than $\mathbb{R}^2$, if a signal is affected by 2 components) Thanks for help me out!","I haven't taken any complex analysis course yet, but now I have this question that relates to it. Let's have a look at a very simple example. Suppose $x,y$ and $z$ are the Cartesian coordinates and we have a function $z=f(x,y)=\cos(x)+\sin(y)$. However, now I change the $\mathbb{R}^2$ plane $x,y$ to complex plane and make a new function, $z=\cos(t)+i\sin(t)$. So, can anyone tell me some famous and fundamental differences between complex plane and $\mathbb{R}^2$ by this example, like some features $\mathbb{R}^2 $ has but complex plane doesn't or the other way around. (Actually I am trying to understand why electrical engineers always want to put signal into the complex numbers rather than $\mathbb{R}^2$, if a signal is affected by 2 components) Thanks for help me out!",,"['complex-analysis', 'complex-numbers', 'signal-processing']"
84,"""Where"" exactly are complex numbers used ""in the real world""?","""Where"" exactly are complex numbers used ""in the real world""?",,"I've always enjoyed solving problems in the complex numbers during my undergrad. However, I've always wondered where are they used and for what? In my domain (computer science) I've rarely seen it be used/applied and hence am curious. So what practical applications of complex numbers exist and what are the ways in which complex  transformation helps address the problem that wasn't immediately addressable? Way back in undergrad when I asked my professor this he mentioned that ""the folks in mechanical and aerospace engineering use it a lot"" but for what? (Don't other domains use it too?). I'm well aware of its use in Fourier analysis but that's the farthest I got to a 'real world application'. I'm sure that's not it. PS: I'm not looking for the ability to make one problem easier to solve, but a bigger picture where the result of the complex analysis is used for something meaningful in the real world. A naive analogy is deciding the height of tower based on trigonometry. That's going from paper to the real world. Similarly, what is it that is analyzed in the complex world and the result is used in the real world without imaginaries clouding the problem? The question: Interesting results easily achieved using complex numbers is nice but covers a more mathematical perspective on interim results that make solving a problem easier. It covers different ground IMHO.","I've always enjoyed solving problems in the complex numbers during my undergrad. However, I've always wondered where are they used and for what? In my domain (computer science) I've rarely seen it be used/applied and hence am curious. So what practical applications of complex numbers exist and what are the ways in which complex  transformation helps address the problem that wasn't immediately addressable? Way back in undergrad when I asked my professor this he mentioned that ""the folks in mechanical and aerospace engineering use it a lot"" but for what? (Don't other domains use it too?). I'm well aware of its use in Fourier analysis but that's the farthest I got to a 'real world application'. I'm sure that's not it. PS: I'm not looking for the ability to make one problem easier to solve, but a bigger picture where the result of the complex analysis is used for something meaningful in the real world. A naive analogy is deciding the height of tower based on trigonometry. That's going from paper to the real world. Similarly, what is it that is analyzed in the complex world and the result is used in the real world without imaginaries clouding the problem? The question: Interesting results easily achieved using complex numbers is nice but covers a more mathematical perspective on interim results that make solving a problem easier. It covers different ground IMHO.",,"['complex-analysis', 'soft-question', 'complex-numbers', 'big-list', 'big-picture']"
85,Why are differentiable complex functions infinitely differentiable?,Why are differentiable complex functions infinitely differentiable?,,"When I studied complex analysis, I could never understand how once-differentiable complex functions could be possibly be infinitely differentiable. After all, this doesn't hold for functions from $\mathbb R ^2$ to $\mathbb R ^2$. Can anyone explain what is different about complex numbers?","When I studied complex analysis, I could never understand how once-differentiable complex functions could be possibly be infinitely differentiable. After all, this doesn't hold for functions from $\mathbb R ^2$ to $\mathbb R ^2$. Can anyone explain what is different about complex numbers?",,['complex-analysis']
86,Can one deduce Liouville's theorem (in complex analysis) from the non-emptiness of spectra in complex Banach algebras?,Can one deduce Liouville's theorem (in complex analysis) from the non-emptiness of spectra in complex Banach algebras?,,"As you probably know, the classical proof of the non-emptiness of the spectrum for an element $x$ in a general Banach algebra over $\mathbb{C}$ can be proven quite easily using Liouville's theorem in complex analysis: every bounded, entire function $\mathbb{C} \to \mathbb{C}$ is constant. As these two theorems seem closely related and are certainly strong and non-trivial (for instance, both of them easily imply the fundamental theorem of algebra), I wonder if it is also possible to deduce Liouville's theorem from the non-emptiness of spectra for elements in complex Banach algebras. I guess one would like to to apply the Gelfand-Mazur theorem (which is a simple corollary of the above non-emptiness) to the Banach algebra of bounded, entire functions on $\mathbb{C}$ but showing that this is a division algebra is basically the same as showing that it is equal to $\mathbb{C}$ to begin with.","As you probably know, the classical proof of the non-emptiness of the spectrum for an element $x$ in a general Banach algebra over $\mathbb{C}$ can be proven quite easily using Liouville's theorem in complex analysis: every bounded, entire function $\mathbb{C} \to \mathbb{C}$ is constant. As these two theorems seem closely related and are certainly strong and non-trivial (for instance, both of them easily imply the fundamental theorem of algebra), I wonder if it is also possible to deduce Liouville's theorem from the non-emptiness of spectra for elements in complex Banach algebras. I guess one would like to to apply the Gelfand-Mazur theorem (which is a simple corollary of the above non-emptiness) to the Banach algebra of bounded, entire functions on $\mathbb{C}$ but showing that this is a division algebra is basically the same as showing that it is equal to $\mathbb{C}$ to begin with.",,"['complex-analysis', 'banach-algebras']"
87,Why does being holomorphic imply so much about a function?,Why does being holomorphic imply so much about a function?,,"I haven't yet started my complex analysis course (soon!), but recently (inspired by you guys) I've been looking into holomorphic functions. And wow, they're cool! There's so much stuff that's true about them... But my question is: why is being holomorphic such a strong condition? Is there some intuitive reason why this seemingly simple condition implies so much about a function? edit: may I add that I am thinking in particular about entire functions. e.g. it is not all obvious to me why being differentiable on the complex plane would imply Picard's theorem, that the function takes every value except at most one.","I haven't yet started my complex analysis course (soon!), but recently (inspired by you guys) I've been looking into holomorphic functions. And wow, they're cool! There's so much stuff that's true about them... But my question is: why is being holomorphic such a strong condition? Is there some intuitive reason why this seemingly simple condition implies so much about a function? edit: may I add that I am thinking in particular about entire functions. e.g. it is not all obvious to me why being differentiable on the complex plane would imply Picard's theorem, that the function takes every value except at most one.",,"['complex-analysis', 'soft-question']"
88,Why is $i! = 0.498015668 - 0.154949828i$?,Why is ?,i! = 0.498015668 - 0.154949828i,"While moving my laptop the other day, I ended up mashing the keyboard a little, and by pure chance managed to do a google search for i! . Curiously, Google's calculator dutifully informed me that $i!$ was, in fact, $0.498015668 - 0.154949828i$. Why is this? I know what a factorial is, so what does it actually mean to take the factorial of a complex number? Also, are those parts of the complex answer rational or irrational? Do complex factorials give rise to any interesting geometric shapes/curves on the complex plane?","While moving my laptop the other day, I ended up mashing the keyboard a little, and by pure chance managed to do a google search for i! . Curiously, Google's calculator dutifully informed me that $i!$ was, in fact, $0.498015668 - 0.154949828i$. Why is this? I know what a factorial is, so what does it actually mean to take the factorial of a complex number? Also, are those parts of the complex answer rational or irrational? Do complex factorials give rise to any interesting geometric shapes/curves on the complex plane?",,"['complex-analysis', 'complex-numbers', 'factorial']"
89,Why isn't several complex variables as fundamental as multivariable calculus?,Why isn't several complex variables as fundamental as multivariable calculus?,,One typically studies analysis in $\mathbb{R}^n$ after studying analysis in $\mathbb{R}$ .  Why can't the same be said of $\mathbb{C}$ ?,One typically studies analysis in after studying analysis in .  Why can't the same be said of ?,\mathbb{R}^n \mathbb{R} \mathbb{C},"['complex-analysis', 'several-complex-variables']"
90,What is so interesting about the zeroes of the Riemann $\zeta$ function?,What is so interesting about the zeroes of the Riemann  function?,\zeta,The Riemann $\zeta$ function plays a significant role in number theory and is defined by $$\zeta(s) = \sum_{n=1}^\infty \frac{1}{n^s} \qquad \text{ for } \sigma > 1 \text{ and } s= \sigma + it$$ The Riemann hypothesis asserts that all the non-trivial zeroes of the $\zeta$ function  lie on the line $\text{Re}(s) = \frac{1}{2}$. My question is: Why are we interested in the zeroes of the $\zeta$ function? Does it give any information about something? What is the use of writing $$\zeta(s) = \prod_{p} \biggl(1-\frac{1}{p^s}\biggr)^{-1}$$,The Riemann $\zeta$ function plays a significant role in number theory and is defined by $$\zeta(s) = \sum_{n=1}^\infty \frac{1}{n^s} \qquad \text{ for } \sigma > 1 \text{ and } s= \sigma + it$$ The Riemann hypothesis asserts that all the non-trivial zeroes of the $\zeta$ function  lie on the line $\text{Re}(s) = \frac{1}{2}$. My question is: Why are we interested in the zeroes of the $\zeta$ function? Does it give any information about something? What is the use of writing $$\zeta(s) = \prod_{p} \biggl(1-\frac{1}{p^s}\biggr)^{-1}$$,,"['complex-analysis', 'number-theory', 'prime-numbers', 'analytic-number-theory', 'riemann-zeta']"
91,Difference between Analytic and Holomorphic function,Difference between Analytic and Holomorphic function,,"A function $f : \mathbb{C} \rightarrow \mathbb{C}$ is said to be holomorphic in an open set $A \subset \mathbb{C}$ if it is differentiable at each point of the set $A$. The function $f : \mathbb{C} \rightarrow \mathbb{C}$ is said to be analytic if it has power series representation. We can prove that the two concepts are same for a single variable complex functions. So why these two different terms? Is there any difference between these two concepts in general, please give example. Thank you for your help.","A function $f : \mathbb{C} \rightarrow \mathbb{C}$ is said to be holomorphic in an open set $A \subset \mathbb{C}$ if it is differentiable at each point of the set $A$. The function $f : \mathbb{C} \rightarrow \mathbb{C}$ is said to be analytic if it has power series representation. We can prove that the two concepts are same for a single variable complex functions. So why these two different terms? Is there any difference between these two concepts in general, please give example. Thank you for your help.",,"['complex-analysis', 'definition']"
92,Fractal behavior along the boundary of convergence?,Fractal behavior along the boundary of convergence?,,"The complex power series $$\sum_{n=1}^{\infty}\frac{z^{n^2}}{n^2}$$ has radius $1$ (Ratio Test) and is absolutely convergent along $|z|=1$.  Recalling something that my calculus professor (Ray Mayer, emeritus of Reed College) showed me 15 years ago, I started looking at a ""graph"" of this function. More precisely, here are plots of the images of $z$ with constant magnitude under this power series. (The mapped curves are the images of $|z|=1$, $|z|=0.9$, and $|z|=0.8$.  At the far right you can see $\sum\limits_{n=1}^{\infty}\frac1{n^2}=\frac{\pi^2}{6}$.) So... what the heck is going on with the fractal behavior of the image of the boundary?  Is there any understanding of this kind of behavior from power series?  For instance, rotating $z$ by some angles might leave you with roughly the original series after a bit of rotation, scaling and translation.  But I haven't been able to see how that would all come together. I have a hunch that the ""berries"" along the inside of the leaf happen around values of $z$ with interesting arguments, but I haven't sat down to map out what those arguments might be. EDIT: Indeed, the ""leaftips"" and ""berries"" seem to happen at regular $z$ values.  Starting at the largest leaftip in quadrant I and moving clockwise, the leaftips are the images of $\exp\left(\frac{\pi}{2}i\right), \exp\left(\frac{\pi}{4}i\right), \exp\left(\frac{\pi}{6}i\right)$,... Similarly, starting from the largest berry and moving clockwise the ""seeds"" of the berries are the images of $\exp\left(\frac{\pi}{3}i\right), \exp\left(\frac{\pi}{5}i\right), \exp\left(\frac{\pi}{7}i\right)$,... It appears that the ""arc"" from the large berry in quadrant IV to the large berry in quadrant I is parametrized by $\exp(it)$ with $t\in\left(-\frac{\pi}{3},\frac{\pi}{3}\right)$.  Further, that the large leaf in quadrant I that crosses into quadrant II is parametrized by $t\in\left(\frac{3\pi}{7},\frac{3\pi}{5}\right)$.  These two sections (and indeed any of the ""leaves"", ""subleaves"", etc.) ought to be similar in the fractal sense.","The complex power series $$\sum_{n=1}^{\infty}\frac{z^{n^2}}{n^2}$$ has radius $1$ (Ratio Test) and is absolutely convergent along $|z|=1$.  Recalling something that my calculus professor (Ray Mayer, emeritus of Reed College) showed me 15 years ago, I started looking at a ""graph"" of this function. More precisely, here are plots of the images of $z$ with constant magnitude under this power series. (The mapped curves are the images of $|z|=1$, $|z|=0.9$, and $|z|=0.8$.  At the far right you can see $\sum\limits_{n=1}^{\infty}\frac1{n^2}=\frac{\pi^2}{6}$.) So... what the heck is going on with the fractal behavior of the image of the boundary?  Is there any understanding of this kind of behavior from power series?  For instance, rotating $z$ by some angles might leave you with roughly the original series after a bit of rotation, scaling and translation.  But I haven't been able to see how that would all come together. I have a hunch that the ""berries"" along the inside of the leaf happen around values of $z$ with interesting arguments, but I haven't sat down to map out what those arguments might be. EDIT: Indeed, the ""leaftips"" and ""berries"" seem to happen at regular $z$ values.  Starting at the largest leaftip in quadrant I and moving clockwise, the leaftips are the images of $\exp\left(\frac{\pi}{2}i\right), \exp\left(\frac{\pi}{4}i\right), \exp\left(\frac{\pi}{6}i\right)$,... Similarly, starting from the largest berry and moving clockwise the ""seeds"" of the berries are the images of $\exp\left(\frac{\pi}{3}i\right), \exp\left(\frac{\pi}{5}i\right), \exp\left(\frac{\pi}{7}i\right)$,... It appears that the ""arc"" from the large berry in quadrant IV to the large berry in quadrant I is parametrized by $\exp(it)$ with $t\in\left(-\frac{\pi}{3},\frac{\pi}{3}\right)$.  Further, that the large leaf in quadrant I that crosses into quadrant II is parametrized by $t\in\left(\frac{3\pi}{7},\frac{3\pi}{5}\right)$.  These two sections (and indeed any of the ""leaves"", ""subleaves"", etc.) ought to be similar in the fractal sense.",,"['complex-analysis', 'power-series', 'plane-curves', 'fractals']"
93,Can someone please explain the Riemann Hypothesis to me... in English?,Can someone please explain the Riemann Hypothesis to me... in English?,,"I've read so much about it but none of it makes a lot of sense. Also, what's so unsolvable about it?","I've read so much about it but none of it makes a lot of sense. Also, what's so unsolvable about it?",,"['number-theory', 'soft-question', 'complex-analysis', 'riemann-hypothesis']"
94,Entire one-to-one functions are linear,Entire one-to-one functions are linear,,Can we prove that every entire one-to-one function is linear?,Can we prove that every entire one-to-one function is linear?,,['complex-analysis']
95,Would a proof to the Riemann Hypothesis affect security?,Would a proof to the Riemann Hypothesis affect security?,,"If a solution was found to the Riemann Hypothesis, would it have any effect on the security of things such as RSA protection? Would it make cracking large numbers easier?","If a solution was found to the Riemann Hypothesis, would it have any effect on the security of things such as RSA protection? Would it make cracking large numbers easier?",,"['number-theory', 'complex-analysis', 'soft-question', 'cryptography', 'riemann-hypothesis']"
96,Interesting results easily achieved using complex numbers,Interesting results easily achieved using complex numbers,,"I was just looking at a calculus textbook preparing my class for next week on complex numbers. I found it interesting to see as an exercise a way to calculate the usual freshman calculus integrals $\int e^{ax}\cos{bx}\ dx$ and $\int e^{ax}\sin{bx}\ dx$ by taking the real and imaginary parts of the ""complex"" integral $\int e^{(a + bi)x} \ dx$. So my question is if you know of other ""relatively interesting"" results that can be obtained easily by using complex numbers. It may be something that one can present to engineering students taking the usual calculus sequence, but I'm also interested in somewhat more advanced examples (if they're available under the condition that the process to get them is somewhat easy, or not too long). Thank you all.","I was just looking at a calculus textbook preparing my class for next week on complex numbers. I found it interesting to see as an exercise a way to calculate the usual freshman calculus integrals $\int e^{ax}\cos{bx}\ dx$ and $\int e^{ax}\sin{bx}\ dx$ by taking the real and imaginary parts of the ""complex"" integral $\int e^{(a + bi)x} \ dx$. So my question is if you know of other ""relatively interesting"" results that can be obtained easily by using complex numbers. It may be something that one can present to engineering students taking the usual calculus sequence, but I'm also interested in somewhat more advanced examples (if they're available under the condition that the process to get them is somewhat easy, or not too long). Thank you all.",,"['complex-analysis', 'complex-numbers']"
97,Does the complex conjugate of an integral equal the integral of the conjugate?,Does the complex conjugate of an integral equal the integral of the conjugate?,,"Let $f$ be a complex valued function of a complex variable. Does $$ \overline{\int f(z) dz} = \int \overline{f(z)}dz \text{ ?} $$ If $f$ is a function of a real variable, the answer is yes as $$ \int f(t) dt = \int \text{Re}(f(t))dt + i\text{Im}(f(t))dt. $$ If $f$ is a complex valued function of a complex variable and belong to $L^2$, the answer is also yes as $L^2$ is a Hilbert space and, by conjugate symmetry of the inner product, $$  \overline{\langle f,g\rangle}=\langle g,f\rangle $$ where $g(z)=1$ is the identity function. Apart from these two cases, is it otherwise true? Is it true in $L^1$?","Let $f$ be a complex valued function of a complex variable. Does $$ \overline{\int f(z) dz} = \int \overline{f(z)}dz \text{ ?} $$ If $f$ is a function of a real variable, the answer is yes as $$ \int f(t) dt = \int \text{Re}(f(t))dt + i\text{Im}(f(t))dt. $$ If $f$ is a complex valued function of a complex variable and belong to $L^2$, the answer is also yes as $L^2$ is a Hilbert space and, by conjugate symmetry of the inner product, $$  \overline{\langle f,g\rangle}=\langle g,f\rangle $$ where $g(z)=1$ is the identity function. Apart from these two cases, is it otherwise true? Is it true in $L^1$?",,[]
98,Trigonometric sums related to the Verlinde formula,Trigonometric sums related to the Verlinde formula,,"Original question (see also the revised, possibly simpler, version below): Let $g > 1, r > 1$ be integers. Playing around with the Verlinde formula (see below), I came across the expression $$\sum_{n=1}^{r-1} \sin(\pi n/r)^{2-2g} (e^{2\pi i n^2/r}-1).$$ My goal is to reduce the complexity in $r$ of this expression; that is, to find a closed form of the sum avoiding the dependence on $r$ in the number of summands. Is this possible? Here's a related example: The Verlinde formula, which e.g. has applications in conformal field theory, algebraic geometry, and quantum topology, is $$(r/2)^{g-1}\sum_{n=1}^{r-1} \sin(\pi n/r)^{2-2g}.$$ In this case, one can use a trick by Szenes to reduce the complexity of the sum: The sum can be written as $$\sum_{n=1}^{r-1} f(z_n).$$ where $z_n = e^{\pi i n/r}$, for a suitable meromorphic function $f : \mathbb{C} \to \mathbb{C}$ having poles only at $1$ and $-1$. Now the trick essentially is to find a suitable meromorphic form $\mu_r$ having poles at $2r$'th roots of unity and apply the Residue Theorem to $f\mu_k$ to rewrite the above sum as $$\sum_{n=1}^{r-1} f(z_n) = -\text{Res}_{z=1} f\mu_r,$$ which then turns out to be a polynomial in $r$ of degree $2g-2$. This trick doesn't seem to apply to my slightly more complicated sum though. Another possibility might be to somehow rewrite the sum as a Gauss sum, but that doesn't quite seem to work either. ""Revised"" question : So, maybe the question above does not have a straightforward answer, but I believe it might suffice to be able to work out the following (at least, it's a similar problem). Say we just have a sum like $$\sum_{n=1}^{r-1} e^{\pi i n^2/(2r)}$$ as above (almost, anyway). Then we may apply a quadratic reciprocity theorem to simplify matters. But say now that we throw in a power of $n$ to get something like $$\sum_{n=1}^{r-1} n^k e^{\pi i n^2/(2r)},$$ for $k > 0$. Can sums like these be treated in a similar manner as the quadratic Gauss sum above (perhaps just in special cases like $k = 1$, or $k = 2$); can we somehow describe the large $r$ asymptotics? Standard tricks in this field seem to involve summation by parts and the Euler--Maclaurin formula but it doesn't seem to quite work out. For example, in the case $k = 1$, summation by parts (or elementary combinatorial considerations) will imply $$\sum_{n=1}^{r-1} n e^{\pi i n^2/(2r)} = (r-1)\sum_{n=1}^{r-1} e^{\pi in^2/(2r)} - \sum_{j=1}^{r-1}\sum_{n=1}^j e^{\pi in^2/(2r)}.$$ Now, the first term is simple to handle as mentioned above, but the second one seems to be worse. Any suggestions?","Original question (see also the revised, possibly simpler, version below): Let $g > 1, r > 1$ be integers. Playing around with the Verlinde formula (see below), I came across the expression $$\sum_{n=1}^{r-1} \sin(\pi n/r)^{2-2g} (e^{2\pi i n^2/r}-1).$$ My goal is to reduce the complexity in $r$ of this expression; that is, to find a closed form of the sum avoiding the dependence on $r$ in the number of summands. Is this possible? Here's a related example: The Verlinde formula, which e.g. has applications in conformal field theory, algebraic geometry, and quantum topology, is $$(r/2)^{g-1}\sum_{n=1}^{r-1} \sin(\pi n/r)^{2-2g}.$$ In this case, one can use a trick by Szenes to reduce the complexity of the sum: The sum can be written as $$\sum_{n=1}^{r-1} f(z_n).$$ where $z_n = e^{\pi i n/r}$, for a suitable meromorphic function $f : \mathbb{C} \to \mathbb{C}$ having poles only at $1$ and $-1$. Now the trick essentially is to find a suitable meromorphic form $\mu_r$ having poles at $2r$'th roots of unity and apply the Residue Theorem to $f\mu_k$ to rewrite the above sum as $$\sum_{n=1}^{r-1} f(z_n) = -\text{Res}_{z=1} f\mu_r,$$ which then turns out to be a polynomial in $r$ of degree $2g-2$. This trick doesn't seem to apply to my slightly more complicated sum though. Another possibility might be to somehow rewrite the sum as a Gauss sum, but that doesn't quite seem to work either. ""Revised"" question : So, maybe the question above does not have a straightforward answer, but I believe it might suffice to be able to work out the following (at least, it's a similar problem). Say we just have a sum like $$\sum_{n=1}^{r-1} e^{\pi i n^2/(2r)}$$ as above (almost, anyway). Then we may apply a quadratic reciprocity theorem to simplify matters. But say now that we throw in a power of $n$ to get something like $$\sum_{n=1}^{r-1} n^k e^{\pi i n^2/(2r)},$$ for $k > 0$. Can sums like these be treated in a similar manner as the quadratic Gauss sum above (perhaps just in special cases like $k = 1$, or $k = 2$); can we somehow describe the large $r$ asymptotics? Standard tricks in this field seem to involve summation by parts and the Euler--Maclaurin formula but it doesn't seem to quite work out. For example, in the case $k = 1$, summation by parts (or elementary combinatorial considerations) will imply $$\sum_{n=1}^{r-1} n e^{\pi i n^2/(2r)} = (r-1)\sum_{n=1}^{r-1} e^{\pi in^2/(2r)} - \sum_{j=1}^{r-1}\sum_{n=1}^j e^{\pi in^2/(2r)}.$$ Now, the first term is simple to handle as mentioned above, but the second one seems to be worse. Any suggestions?",,"['number-theory', 'complex-analysis', 'algebraic-geometry', 'trigonometry', 'geometric-topology']"
99,What is the intuition behind the Wirtinger derivatives?,What is the intuition behind the Wirtinger derivatives?,,"The Wirtinger differential operators are introduced in complex analysis to simplify differentiation in complex variables. Most textbooks introduce them as if it were a natural thing to do. However, I fail to see the intuition behind this. Most of the time, I even think they tend to make calculations harder. Is there a simple interpretation of these operators? What mental picture do you have when you use them?","The Wirtinger differential operators are introduced in complex analysis to simplify differentiation in complex variables. Most textbooks introduce them as if it were a natural thing to do. However, I fail to see the intuition behind this. Most of the time, I even think they tend to make calculations harder. Is there a simple interpretation of these operators? What mental picture do you have when you use them?",,"['complex-analysis', 'multivariable-calculus', 'intuition']"
