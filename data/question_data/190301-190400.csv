,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,What does $d\log\left(\frac{y}{x}\right)$ mean mathematically?,What does  mean mathematically?,d\log\left(\frac{y}{x}\right),"I am used to seeing derivatives written as $$\frac{df}{dx}.$$ But my economics professor keeps using notation like $$ d\log\left(\frac{y}{x}\right)$$ and I have no idea what this means. What does this notation signify? If it's a derivative, with respect to what? There is no denominator term. And I thought modern calculus used standard analysis with limits and not infinitesimals. my question: What does  $d\log\left(\frac{y}{x}\right)$ signify? How do I take this derivative and with respect to what?","I am used to seeing derivatives written as $$\frac{df}{dx}.$$ But my economics professor keeps using notation like $$ d\log\left(\frac{y}{x}\right)$$ and I have no idea what this means. What does this notation signify? If it's a derivative, with respect to what? There is no denominator term. And I thought modern calculus used standard analysis with limits and not infinitesimals. my question: What does  $d\log\left(\frac{y}{x}\right)$ signify? How do I take this derivative and with respect to what?",,"['calculus', 'derivatives', 'notation', 'logarithms', 'differential']"
1,"Mean value theorem for the second derivative, when the first derivative is zero at endpoints [duplicate]","Mean value theorem for the second derivative, when the first derivative is zero at endpoints [duplicate]",,"This question already has answers here : Prove that $|f''(\xi)|\geqslant\frac{4|f(a)-f(b)|}{(b-a)^2}$ (2 answers) Closed 8 years ago . Suppose $f:[a,b]\to \mathbb R$ has derivative up to order $2$, and  $f'(a)=f'(b)=0$. Prove there is  a point $c$ at $(a,b)$ such that  $$ |f''(c)|\geq 4\frac{|f(b)-f(a)|}{(b-a)^2}. $$ If it was factor 2, not 4, then I could use a Taylor expansion with Lagrange residue.","This question already has answers here : Prove that $|f''(\xi)|\geqslant\frac{4|f(a)-f(b)|}{(b-a)^2}$ (2 answers) Closed 8 years ago . Suppose $f:[a,b]\to \mathbb R$ has derivative up to order $2$, and  $f'(a)=f'(b)=0$. Prove there is  a point $c$ at $(a,b)$ such that  $$ |f''(c)|\geq 4\frac{|f(b)-f(a)|}{(b-a)^2}. $$ If it was factor 2, not 4, then I could use a Taylor expansion with Lagrange residue.",,"['calculus', 'inequality', 'derivatives']"
2,"Differentiability of $\mathrm{max}(x, y)$ at $x=y$",Differentiability of  at,"\mathrm{max}(x, y) x=y","I am trying to figure out differentiability of $\mathrm{max}(x, y)$. Intuitively, it should not be differentiable at $x=y$, since it changes direction ""non-smoothly"" at those points. I can not, however, find a way to prove this algebraically.","I am trying to figure out differentiability of $\mathrm{max}(x, y)$. Intuitively, it should not be differentiable at $x=y$, since it changes direction ""non-smoothly"" at those points. I can not, however, find a way to prove this algebraically.",,['calculus']
3,Prove $S^2(x) + C^2(x) = 1$,Prove,S^2(x) + C^2(x) = 1,"Suppose $S, C : \Bbb R \to \Bbb R$ are differentiable and that (1) $S'(x) = C(x)$ and $C'(x) = −S(x)$ for all $x \in \Bbb R$ , and (2) $S(0) = 0$ and $C(0) = 1$ . Prove that $S^2(x) + C^2(x) = 1$ for all $x \in \Bbb R$ . I tried using the limit definition of differentiable in setting the limit as $x$ approaches $a$ of $((S(X)-S(a))/x-1)=C(x)$ and setting the limit as $x$ approaches $a$ of $((C(X)-C(a))/x-1)=-S(x)$ but I don't know if that gets me anywhere. Hints or anything would be greatly appreciated!","Suppose are differentiable and that (1) and for all , and (2) and . Prove that for all . I tried using the limit definition of differentiable in setting the limit as approaches of and setting the limit as approaches of but I don't know if that gets me anywhere. Hints or anything would be greatly appreciated!","S, C : \Bbb R \to \Bbb R S'(x) = C(x) C'(x) = −S(x) x \in \Bbb R S(0) = 0 C(0) = 1 S^2(x) + C^2(x) = 1 x \in \Bbb R x a ((S(X)-S(a))/x-1)=C(x) x a ((C(X)-C(a))/x-1)=-S(x)","['real-analysis', 'derivatives']"
4,"Prove that if $f$ differentiable in $x^*$ and $f(x^*)=0$, then $\liminf_{x\to x^*}\frac{|f(x)|}{||x-x^*||}=0$ if $n>1$","Prove that if  differentiable in  and , then  if",f x^* f(x^*)=0 \liminf_{x\to x^*}\frac{|f(x)|}{||x-x^*||}=0 n>1,"Let $f\colon\mathbb{R}^n\to\mathbb{R}$ be differentiable at $x^*$ and $f(x^*)=0$. Prove that if $n>1$, then $$\liminf_{x\to x^*}\frac{|f(x)|}{||x-x^*||}=0.$$ Is this true for $n=1$? I know that because $f$ is differentiable in $x^*$, there is a linear transformation $L\colon \mathbb{R}^n\to\mathbb{R}$ such that $$0=\lim_{x\to x^*}\frac{|f(x)-f(x^*)-L(x-x^*)|}{||x-x^*||}=\lim_{x\to x^*}\frac{|f(x)-L(x-x^*)|}{||x-x^*||}.$$ How to proceed from there? And what is special about $n=1$?","Let $f\colon\mathbb{R}^n\to\mathbb{R}$ be differentiable at $x^*$ and $f(x^*)=0$. Prove that if $n>1$, then $$\liminf_{x\to x^*}\frac{|f(x)|}{||x-x^*||}=0.$$ Is this true for $n=1$? I know that because $f$ is differentiable in $x^*$, there is a linear transformation $L\colon \mathbb{R}^n\to\mathbb{R}$ such that $$0=\lim_{x\to x^*}\frac{|f(x)-f(x^*)-L(x-x^*)|}{||x-x^*||}=\lim_{x\to x^*}\frac{|f(x)-L(x-x^*)|}{||x-x^*||}.$$ How to proceed from there? And what is special about $n=1$?",,"['real-analysis', 'derivatives', 'limsup-and-liminf']"
5,Finding extreme values where second derivative is zero,Finding extreme values where second derivative is zero,,"Consider this function: $$f(x)= 5x^6 - 18x^5 + 15x^4 - 10$$ I am told to find the extreme values of this function. So at first, I take the first derivative and set it zero. $$f'(x)=30x^5-90x^4+60x^3=0(say)$$ $$=x^5-3x^4+2x^3=0$$ If I evaluate for $x$, I find $x= 0, 1, 2$. Now let's take the second derivative of $f(x)$: $$f''(x)= 150x^4-360x^3+180x^2$$ I am Ok with $f''(1)$ and $f''(2)$ because they give a negative and positive value respectively. Now, $f''(0) = 0$. So I don't know if I have a maxima or minima at $x=0$. I have found one way to solve this on internet that is named as first derivative test. Here I take two values of $x$, one $<0$, other $>0$.  Let me take $0.5$ and $-1$. $$f'(-1)= -180$$ $$f'(0.5)=45/16$$ So if $x<0$ then the function is decreasing. But when $0 < x <1$ (less than one because we have another maxima at $x=1$, but I am worrying only about that at $x=0$), it's increasing. Thus I can say we have a minima at $x=0$. The main reason behind my asking is, after finding $f''(x)=0$, my solution book has shown that $f^{(iii)}(0)=0$ but $f^{(iv)}(0)= 360>0$. Thus we have a minima at $x=0$. But this is the INFLECTION POINT test, isn't it? But a point being an inflection point doesn't necessarily mean that it is a local maxima or local minima. So I would like to know if my work is right as I am not deeply familiar with the solution that I gave above and if my book's solution is also allowable or not? And one more thing I am not sure about: Can a point be an inflection point and at the same time a local maxima or minima? If yes, an example would be very helpful.","Consider this function: $$f(x)= 5x^6 - 18x^5 + 15x^4 - 10$$ I am told to find the extreme values of this function. So at first, I take the first derivative and set it zero. $$f'(x)=30x^5-90x^4+60x^3=0(say)$$ $$=x^5-3x^4+2x^3=0$$ If I evaluate for $x$, I find $x= 0, 1, 2$. Now let's take the second derivative of $f(x)$: $$f''(x)= 150x^4-360x^3+180x^2$$ I am Ok with $f''(1)$ and $f''(2)$ because they give a negative and positive value respectively. Now, $f''(0) = 0$. So I don't know if I have a maxima or minima at $x=0$. I have found one way to solve this on internet that is named as first derivative test. Here I take two values of $x$, one $<0$, other $>0$.  Let me take $0.5$ and $-1$. $$f'(-1)= -180$$ $$f'(0.5)=45/16$$ So if $x<0$ then the function is decreasing. But when $0 < x <1$ (less than one because we have another maxima at $x=1$, but I am worrying only about that at $x=0$), it's increasing. Thus I can say we have a minima at $x=0$. The main reason behind my asking is, after finding $f''(x)=0$, my solution book has shown that $f^{(iii)}(0)=0$ but $f^{(iv)}(0)= 360>0$. Thus we have a minima at $x=0$. But this is the INFLECTION POINT test, isn't it? But a point being an inflection point doesn't necessarily mean that it is a local maxima or local minima. So I would like to know if my work is right as I am not deeply familiar with the solution that I gave above and if my book's solution is also allowable or not? And one more thing I am not sure about: Can a point be an inflection point and at the same time a local maxima or minima? If yes, an example would be very helpful.",,"['calculus', 'derivatives', 'extreme-value-theorem']"
6,Can one find a line that is tangent to a cubic polynomial more than once?,Can one find a line that is tangent to a cubic polynomial more than once?,,"I know that any line cannot be tangent to the graph of $y=Ax^3+Bx^2+Cx+D$ at more than one point. Question: how can one show this, or even prove it?","I know that any line cannot be tangent to the graph of $y=Ax^3+Bx^2+Cx+D$ at more than one point. Question: how can one show this, or even prove it?",,"['calculus', 'derivatives']"
7,Conceptual question on differentiation in calculus?,Conceptual question on differentiation in calculus?,,"In Calc class, my teacher told us that the only solution to $y' = y$ is $y = ce^x$, with $c$ being a real number. I am having difficulty understanding the only part. Is there a proof of this? Or am I missing something obvious?","In Calc class, my teacher told us that the only solution to $y' = y$ is $y = ce^x$, with $c$ being a real number. I am having difficulty understanding the only part. Is there a proof of this? Or am I missing something obvious?",,"['calculus', 'derivatives']"
8,Prove that $\frac{d^n}{dx^n} \cos(ax) = a^n \cos (ax + \frac{n\pi}{2})$,Prove that,\frac{d^n}{dx^n} \cos(ax) = a^n \cos (ax + \frac{n\pi}{2}),"Question Prove that $\frac{d^n}{dx^n} \cos(ax) =  a^n \cos (ax + \frac{n\pi}{2})$ I could calculate the first few derivatives of $\cos(ax)$ and consequently observe the pattern that unfolds, which in turn I can then correlate with $a^n \cos (ax + \frac{n\pi}{2})$ . However, I'm curious for a proof that does not rely on so many words , rather something more concrete(could I put it that way?). I would include some of my attempts, but it's all rather trivial.","Question Prove that I could calculate the first few derivatives of and consequently observe the pattern that unfolds, which in turn I can then correlate with . However, I'm curious for a proof that does not rely on so many words , rather something more concrete(could I put it that way?). I would include some of my attempts, but it's all rather trivial.",\frac{d^n}{dx^n} \cos(ax) =  a^n \cos (ax + \frac{n\pi}{2}) \cos(ax) a^n \cos (ax + \frac{n\pi}{2}),"['calculus', 'trigonometry', 'derivatives']"
9,"Prove that there exist $a \in [-1,1]$, such that $f'''(a)=3f(1)-3f(-1)-6f'(0)$","Prove that there exist , such that","a \in [-1,1] f'''(a)=3f(1)-3f(-1)-6f'(0)","Let $f: \mathbb{R} \rightarrow \mathbb{R}$ be a three times differentiable function. Prove that there exists $a \in [-1,1]$ such that $$ f'''(a) = 3f(1)-3f(-1)-6f'(0)$$ Any hint/idea's how to approach this problem? Thanks!","Let $f: \mathbb{R} \rightarrow \mathbb{R}$ be a three times differentiable function. Prove that there exists $a \in [-1,1]$ such that $$ f'''(a) = 3f(1)-3f(-1)-6f'(0)$$ Any hint/idea's how to approach this problem? Thanks!",,"['calculus', 'real-analysis', 'derivatives']"
10,formal proof from calulus,formal proof from calulus,,"Given $f:R \to R$, $f$ is differentiable on $R$ and $\lim_{x \to \infty}(f(x)-f(-x))=0$. I need to show that there is $x_0 \in R$ such that $f'(x_0)=0$ I am trying to prove it by contradiction .... so i assume there is no $x_0 \in R$ such that  $f'(x_0)=0$ this means this function is strictly monotonic because the derivative ""respects"" the mean value theorem so it cannot be negative and positive without passing a zero value ....  (from what i understand the derivative function of a continuous/differentiable functions isn't necessarily continuous but it still respect the mean value theorem i think it's called darboux's theorem ) now from $\lim_{x \to \infty}(f(x)-f(-x))=0$ ,we get $\lim_{x \to \infty}f(x)=\lim_{x \to \infty}f(-x)$ now the last step is the contradiction its very intuitive and even maybe obvious that we can't get $\lim_{x \to \infty}f(x)=\lim_{x \to \infty}f(-x)$ for strictly monotonic function but i cant think of a formal way to prove it .","Given $f:R \to R$, $f$ is differentiable on $R$ and $\lim_{x \to \infty}(f(x)-f(-x))=0$. I need to show that there is $x_0 \in R$ such that $f'(x_0)=0$ I am trying to prove it by contradiction .... so i assume there is no $x_0 \in R$ such that  $f'(x_0)=0$ this means this function is strictly monotonic because the derivative ""respects"" the mean value theorem so it cannot be negative and positive without passing a zero value ....  (from what i understand the derivative function of a continuous/differentiable functions isn't necessarily continuous but it still respect the mean value theorem i think it's called darboux's theorem ) now from $\lim_{x \to \infty}(f(x)-f(-x))=0$ ,we get $\lim_{x \to \infty}f(x)=\lim_{x \to \infty}f(-x)$ now the last step is the contradiction its very intuitive and even maybe obvious that we can't get $\lim_{x \to \infty}f(x)=\lim_{x \to \infty}f(-x)$ for strictly monotonic function but i cant think of a formal way to prove it .",,"['calculus', 'derivatives', 'proof-writing']"
11,Finding the derivative of $2^{x}$ from first terms?,Finding the derivative of  from first terms?,2^{x},"I was trying to understand why $e^{x}$ is special by finding the derivatives of other exponential functions and comparing the results. So I tried ${\rm f}\left(x\right) = 2^{x}$, but now I'm stuck. Here's my final step: $\displaystyle{{\rm f}'\left(x\right) = \lim_{h \to 0}{2^{x}\left(2^{h} - 1\right) \over h}}$.","I was trying to understand why $e^{x}$ is special by finding the derivatives of other exponential functions and comparing the results. So I tried ${\rm f}\left(x\right) = 2^{x}$, but now I'm stuck. Here's my final step: $\displaystyle{{\rm f}'\left(x\right) = \lim_{h \to 0}{2^{x}\left(2^{h} - 1\right) \over h}}$.",,"['calculus', 'derivatives', 'exponential-function']"
12,"Differentiating $x+2x^2\sin(1/x)$ near $0$, discontinuity of the derivative","Differentiating  near , discontinuity of the derivative",x+2x^2\sin(1/x) 0,"I have this function $$ \begin{array}{l} f:\mathbb{R}\rightarrow\mathbb{R}\\ x\rightarrow\left\{\begin{array}{ll} x+2x^2\sin\left(\frac{1}{x}\right)&x\neq 0\\ 0&x=0 \end{array}\right. \end{array} $$ I need to calculate its derivative in $x=0$. I'm not sure how can I compute it at zero, since the limit of its derivative goes to 1 when x tends to zero but at zero I thought $f'(0)$ must be zero (therefore its derivative is not continuous at the origin). If is not zero then it is invertible around the origin via the Inverse Function Theorem, right?","I have this function $$ \begin{array}{l} f:\mathbb{R}\rightarrow\mathbb{R}\\ x\rightarrow\left\{\begin{array}{ll} x+2x^2\sin\left(\frac{1}{x}\right)&x\neq 0\\ 0&x=0 \end{array}\right. \end{array} $$ I need to calculate its derivative in $x=0$. I'm not sure how can I compute it at zero, since the limit of its derivative goes to 1 when x tends to zero but at zero I thought $f'(0)$ must be zero (therefore its derivative is not continuous at the origin). If is not zero then it is invertible around the origin via the Inverse Function Theorem, right?",,"['real-analysis', 'derivatives']"
13,Uniform Continuity and Differentiation,Uniform Continuity and Differentiation,,"Is the following true or false?: Let $f\colon  [0,1) \to \mathbb{R}$ be a function differentiable in $[0,1)$ (where the derivative at zero means ""right derivative"") such that both $f$ and $f'$ are uniformly continuous in $(0,1)$. Then $f'$ is continuous. Note that the mistery lies at $x=0$. So the question is: can we say with these hypotheses that $f'(0)=\lim_{x\to 0^{+}}f'(x)$ (which exists thanks to the uniform continuity of $f'_{\mid (0,1)}$). Note also that the uniform continuity of $f'_{\mid (0,1)}$  makes redundant the analogous requirement for $f$ (which will even more become a Lipschitz function).","Is the following true or false?: Let $f\colon  [0,1) \to \mathbb{R}$ be a function differentiable in $[0,1)$ (where the derivative at zero means ""right derivative"") such that both $f$ and $f'$ are uniformly continuous in $(0,1)$. Then $f'$ is continuous. Note that the mistery lies at $x=0$. So the question is: can we say with these hypotheses that $f'(0)=\lim_{x\to 0^{+}}f'(x)$ (which exists thanks to the uniform continuity of $f'_{\mid (0,1)}$). Note also that the uniform continuity of $f'_{\mid (0,1)}$  makes redundant the analogous requirement for $f$ (which will even more become a Lipschitz function).",,"['calculus', 'real-analysis', 'derivatives']"
14,Proof of Product Rule for Derivatives using Proof by Induction,Proof of Product Rule for Derivatives using Proof by Induction,,"I am trying to understand the proof of the General Result for the Product Rule for Derivatives by reading this . Relevant parts are as follows: Basis for the induction $$ D_x \left({f_1 \left({x}\right) f_2 \left({x}\right)}\right) = D_x \left({f_1 \left({x}\right)}\right) f_2 \left({x}\right) + f_1 \left({x}\right) D_x \left({f_2 \left({x}\right)}\right) $$ Induction Hypothesis $$ D_x \left({\prod_{i=1}^k f_i \left({x}\right)}\right) = \sum_{i=1}^k \left({D_x \left({f_i \left({x}\right)}\right) \prod_{j \ne i} f_i \left({x}\right)}\right) $$ Induction Step $$ \begin{align} \tag{1} \kern-30pt D_x \left({\textstyle\prod\limits_{i=1}^{k+1} f_i \left({x}\right)}\right)  &=  D_x \left({\left({\textstyle\prod\limits_{i=1}^k f_i \left({x}\right)}\right) f_{k+1} \left({x}\right)}\right) \\   &= \tag{2}  D_x \left({f_{k+1} \left({x}\right)}\right) \left({\textstyle\prod\limits_{i=1}^k f_i \left({x}\right)}\right) + D_x \left({\textstyle\prod\limits_{i=1}^k f_i \left({x}\right)}\right) f_{k+1} \left({x}\right) \\  &=\tag{3}  D_x \left({f_{k+1} \left({x}\right)}\right) \left({\textstyle\prod\limits_{i=1}^k f_i \left({x}\right)}\right) + \left({\sum_{i=1}^k \left({D_x \left({f_i \left({x}\right)}\right) \textstyle\prod\limits_{j \ne i} f_i \left({x}\right)}\right)}\right) f_{k+1} \left({x}\right)  \\  &= \tag{4}  \sum_{i=1}^{k+1} \left({D_x \left({f_i \left({x}\right)}\right)\textstyle \prod\limits_{j \ne i} f_i \left({x}\right)}\right)    \end{align} $$ Question I am stuck at (3). How do I go from (3) to (4)? Specifically, what sort of algebraic manipulations need to be done and what are the motivations for doing those algebraic manipulations in order to arrive at (4)? To put it in another way, I would like to know that is the thought process that one goes through when simplifying (3) to (4). Note: I hope someone can correct my LaTeX typesetting. I was under the impression that the align environment would automatically number the formulas I write. Thanks in advance.","I am trying to understand the proof of the General Result for the Product Rule for Derivatives by reading this . Relevant parts are as follows: Basis for the induction $$ D_x \left({f_1 \left({x}\right) f_2 \left({x}\right)}\right) = D_x \left({f_1 \left({x}\right)}\right) f_2 \left({x}\right) + f_1 \left({x}\right) D_x \left({f_2 \left({x}\right)}\right) $$ Induction Hypothesis $$ D_x \left({\prod_{i=1}^k f_i \left({x}\right)}\right) = \sum_{i=1}^k \left({D_x \left({f_i \left({x}\right)}\right) \prod_{j \ne i} f_i \left({x}\right)}\right) $$ Induction Step $$ \begin{align} \tag{1} \kern-30pt D_x \left({\textstyle\prod\limits_{i=1}^{k+1} f_i \left({x}\right)}\right)  &=  D_x \left({\left({\textstyle\prod\limits_{i=1}^k f_i \left({x}\right)}\right) f_{k+1} \left({x}\right)}\right) \\   &= \tag{2}  D_x \left({f_{k+1} \left({x}\right)}\right) \left({\textstyle\prod\limits_{i=1}^k f_i \left({x}\right)}\right) + D_x \left({\textstyle\prod\limits_{i=1}^k f_i \left({x}\right)}\right) f_{k+1} \left({x}\right) \\  &=\tag{3}  D_x \left({f_{k+1} \left({x}\right)}\right) \left({\textstyle\prod\limits_{i=1}^k f_i \left({x}\right)}\right) + \left({\sum_{i=1}^k \left({D_x \left({f_i \left({x}\right)}\right) \textstyle\prod\limits_{j \ne i} f_i \left({x}\right)}\right)}\right) f_{k+1} \left({x}\right)  \\  &= \tag{4}  \sum_{i=1}^{k+1} \left({D_x \left({f_i \left({x}\right)}\right)\textstyle \prod\limits_{j \ne i} f_i \left({x}\right)}\right)    \end{align} $$ Question I am stuck at (3). How do I go from (3) to (4)? Specifically, what sort of algebraic manipulations need to be done and what are the motivations for doing those algebraic manipulations in order to arrive at (4)? To put it in another way, I would like to know that is the thought process that one goes through when simplifying (3) to (4). Note: I hope someone can correct my LaTeX typesetting. I was under the impression that the align environment would automatically number the formulas I write. Thanks in advance.",,"['calculus', 'induction', 'derivatives']"
15,Minimize function $\sqrt{x^2-4x+5}+\sqrt{4+x^2}$,Minimize function,\sqrt{x^2-4x+5}+\sqrt{4+x^2},"Suppose I want to find $$ \min f(x) = \min(\sqrt{x^2-4x+5}+\sqrt{4+x^2}) $$ I'd start with computing derivative and set it to $0$ $$ f'(x) = \frac{x-2}{\sqrt{x^2-4x+5}} + \frac{x}{\sqrt{4+x^2}} $$ Then $$ \frac{x-2}{\sqrt{x^2-4x+5}} + \frac{x}{\sqrt{4+x^2}} = 0 $$ $$ (x-2)\sqrt{4+x^2} + x\sqrt{x^2-4x+5}=0 $$ My first instinct is to rewrite it as $$ (2-x)\sqrt{4+x^2} = x\sqrt{x^2-4x+5} $$ and square both sides. The thing is, I get $$ 3x^2 -16x +16=0 $$ with $x_1 = 4/3$ and $x_2 = 4$ . So $f'(x_1^-) <0$ and $f'(x_1^+) > 0$ thus in $x = 4/3$ there is a minimum of our function. Question: is it valid to square both sides of an equation as I did above? Is there a possibility that because of that I lose or introduce a solution that shouldn't exist and might mess everything up?","Suppose I want to find I'd start with computing derivative and set it to Then My first instinct is to rewrite it as and square both sides. The thing is, I get with and . So and thus in there is a minimum of our function. Question: is it valid to square both sides of an equation as I did above? Is there a possibility that because of that I lose or introduce a solution that shouldn't exist and might mess everything up?","
\min f(x) = \min(\sqrt{x^2-4x+5}+\sqrt{4+x^2})
 0 
f'(x) = \frac{x-2}{\sqrt{x^2-4x+5}} + \frac{x}{\sqrt{4+x^2}}
 
\frac{x-2}{\sqrt{x^2-4x+5}} + \frac{x}{\sqrt{4+x^2}} = 0
 
(x-2)\sqrt{4+x^2} + x\sqrt{x^2-4x+5}=0
 
(2-x)\sqrt{4+x^2} = x\sqrt{x^2-4x+5}
 
3x^2 -16x +16=0
 x_1 = 4/3 x_2 = 4 f'(x_1^-) <0 f'(x_1^+) > 0 x = 4/3","['derivatives', 'optimization', 'solution-verification', 'maxima-minima']"
16,Differentiating $x^{x^{x^{...}}}$,Differentiating,x^{x^{x^{...}}},How do I differentiate $$ x^{x^{x^{...}}}$$ with respect to $x$ ? (Note that $x$ is raised infinitely many times.) My attempt: Let $y = x^{x^{x^{...}}}$ . Taking logarithm of both sides we get $\ln y = y \ln x$ and let $f = y \ln x - \ln y$ . Now $$\frac{dy}{dx} = -\frac{\frac{\partial f}{\partial x}}{\frac{\partial f}{\partial y}} = \frac{y^2}{x(1 - y \ln x)}$$ Is this approach correct? If not how do I proceed ? Help would be appreciated. Thanks.,How do I differentiate with respect to ? (Note that is raised infinitely many times.) My attempt: Let . Taking logarithm of both sides we get and let . Now Is this approach correct? If not how do I proceed ? Help would be appreciated. Thanks., x^{x^{x^{...}}} x x y = x^{x^{x^{...}}} \ln y = y \ln x f = y \ln x - \ln y \frac{dy}{dx} = -\frac{\frac{\partial f}{\partial x}}{\frac{\partial f}{\partial y}} = \frac{y^2}{x(1 - y \ln x)},"['real-analysis', 'calculus', 'derivatives']"
17,Help finding the derivative of $f(x) = \cos{\left(\sqrt{e^{x^5} \sin{x}}\right)}$,Help finding the derivative of,f(x) = \cos{\left(\sqrt{e^{x^5} \sin{x}}\right)},"I am trying to find the derivative of $f(x) = \cos(\sqrt{(e^{x^5} \sin(x)})$ . I keep getting the wrong answer, and I'm not sure what I'm doing wrong. $$\frac{d}{dx} e^{x^5} = e^{x^5} \cdot 5x^4$$ $$\frac{d}{dx} \sin(x) = \cos(x)$$ $$\frac{d}{dx} (e^{x ^5} \cdot \sin(x)) = [(e^{x^5} \cdot 5x^4) \cdot \sin(x)] + [\cos(x) \cdot e^{x^5}]$$ $$\frac{d}{dx} \sqrt{x} = \frac{1}{2\sqrt{x}}$$ $$\frac{d}{dx} \sqrt{(e^{x^5} \sin(x))} = \frac{[(e^{x^5} \cdot 5x^4) \cdot \sin(x)] + [\cos(x) \cdot e^{x^5}]}{2 \sqrt{e^{x^5} \sin(x)}}$$ Therefore, since $\frac{d}{dx} \cos(x) = -\sin(x)$ , I have $$ f'(x) = -\sin(\sqrt{e^{x^5} \sin(x)}) \cdot \frac{[(e^{x^5} \cdot 5x^4) \cdot \sin(x)] + [\cos(x) \cdot e^{x^5}]}{2 \sqrt{e^{x^5} \sin(x)}}$$ However, the website I'm using, ""WeBWorK"", says this is incorrect.","I am trying to find the derivative of . I keep getting the wrong answer, and I'm not sure what I'm doing wrong. Therefore, since , I have However, the website I'm using, ""WeBWorK"", says this is incorrect.",f(x) = \cos(\sqrt{(e^{x^5} \sin(x)}) \frac{d}{dx} e^{x^5} = e^{x^5} \cdot 5x^4 \frac{d}{dx} \sin(x) = \cos(x) \frac{d}{dx} (e^{x ^5} \cdot \sin(x)) = [(e^{x^5} \cdot 5x^4) \cdot \sin(x)] + [\cos(x) \cdot e^{x^5}] \frac{d}{dx} \sqrt{x} = \frac{1}{2\sqrt{x}} \frac{d}{dx} \sqrt{(e^{x^5} \sin(x))} = \frac{[(e^{x^5} \cdot 5x^4) \cdot \sin(x)] + [\cos(x) \cdot e^{x^5}]}{2 \sqrt{e^{x^5} \sin(x)}} \frac{d}{dx} \cos(x) = -\sin(x)  f'(x) = -\sin(\sqrt{e^{x^5} \sin(x)}) \cdot \frac{[(e^{x^5} \cdot 5x^4) \cdot \sin(x)] + [\cos(x) \cdot e^{x^5}]}{2 \sqrt{e^{x^5} \sin(x)}},"['calculus', 'derivatives', 'chain-rule']"
18,Derivative of sigmoid function that contains vectors,Derivative of sigmoid function that contains vectors,,Could someone show me how to take the derivative of this function with respect to $w_i$ ? $f(w) = \frac{1}{1+e^{-w^Tx}}$ $w$ and $x$ are both vectors $\in \mathbb{R}^D$ How would this be different from taking the derivative with respect to $w$ itself?,Could someone show me how to take the derivative of this function with respect to ? and are both vectors How would this be different from taking the derivative with respect to itself?,w_i f(w) = \frac{1}{1+e^{-w^Tx}} w x \in \mathbb{R}^D w,"['calculus', 'derivatives', 'matrix-calculus']"
19,Possible mistake finding the maximum volume of a box with the AM-GM inequality?,Possible mistake finding the maximum volume of a box with the AM-GM inequality?,,"I have found the following problem: What is the box (without a top) of largest volume which can be constructed from   a square piece of tin of edge length $2a$ by cutting a square from each corner   and folding up the edges I have tried to solve it with the AM-GM inequality: I have the sides $(2a-2h), (2a-2h),h$ and then: $$\frac{(2a-2h)+ (2a-2h) + h}{3}\geq \sqrt[3]{(2a-2h)^2h}$$ Equality holds when $(2a-2h)=(2a-2h)=h$, when I try to solve, I find: $$h=\frac{2a}{3}$$ When I try to do the same with derivatives, I find that the roots of the derivative of $(2a-2h)^2h$ are $a$ and $\frac{a}{3}$. I may be doing something extremely silly but I can't figure out what is wrong.","I have found the following problem: What is the box (without a top) of largest volume which can be constructed from   a square piece of tin of edge length $2a$ by cutting a square from each corner   and folding up the edges I have tried to solve it with the AM-GM inequality: I have the sides $(2a-2h), (2a-2h),h$ and then: $$\frac{(2a-2h)+ (2a-2h) + h}{3}\geq \sqrt[3]{(2a-2h)^2h}$$ Equality holds when $(2a-2h)=(2a-2h)=h$, when I try to solve, I find: $$h=\frac{2a}{3}$$ When I try to do the same with derivatives, I find that the roots of the derivative of $(2a-2h)^2h$ are $a$ and $\frac{a}{3}$. I may be doing something extremely silly but I can't figure out what is wrong.",,"['derivatives', 'inequality', 'optimization']"
20,Intuition behind the difference between $\frac{\partial f}{\partial t}$ and $\frac{df}{dt}$,Intuition behind the difference between  and,\frac{\partial f}{\partial t} \frac{df}{dt},"Let $f=f(x,y,t)$. Using the definition of the differential $df$, we can easily conclude that $\frac{df}{dt} = \frac{\partial f}{\partial t} + (\vec{v} \cdot \vec{\nabla})f$ $\hspace{2mm} (1)$ So my question is: We refer to both $\frac{df}{dt}$ and $\frac{\partial f}{\partial t}$ as a change of the quantity $f$ with respect to $t$. But what does each time derivative symbolize in eq$(1)$? How could I measure them? (I know the definition of partial derivative and the role of $(\vec{v} \cdot \vec{\nabla})f$ in eq$(1)$. All I want is to understand the connection the $2$ derivatives in eq$(1)$.)","Let $f=f(x,y,t)$. Using the definition of the differential $df$, we can easily conclude that $\frac{df}{dt} = \frac{\partial f}{\partial t} + (\vec{v} \cdot \vec{\nabla})f$ $\hspace{2mm} (1)$ So my question is: We refer to both $\frac{df}{dt}$ and $\frac{\partial f}{\partial t}$ as a change of the quantity $f$ with respect to $t$. But what does each time derivative symbolize in eq$(1)$? How could I measure them? (I know the definition of partial derivative and the role of $(\vec{v} \cdot \vec{\nabla})f$ in eq$(1)$. All I want is to understand the connection the $2$ derivatives in eq$(1)$.)",,"['derivatives', 'soft-question', 'partial-derivative', 'vector-analysis']"
21,A smooth function can not be transformed into another smooth function without changing the value of every open interval.,A smooth function can not be transformed into another smooth function without changing the value of every open interval.,,"Take any $C^\infty$ (smooth) function $f: R \to R$ . For any arbitrary function $t:R\to R$ , define $g :R\to R$ as $g(x)= (t\circ f)(x)$ Conjecture : For any such $g$ , if $g$ is smooth ( $g\in C^\infty$ ), the following must necessarily hold: $(i)$ : Either: $s(x) = x$ (identity function), or $(ii)$ : There exists no open ( $O_R$ ) interval $U$ on the domain of $f, g$ , for which holds: $f(U)=g(U)$ . i.e.: $$\forall U\in O_R:\exists x\in U:f(x)\neq g(x)$$ In plain English: A smooth function cannot be transformed into another smooth function, without changing the values in all its intervals: Only isolated points may remain unchanged. Here is an incomplete argument why it seems to me must be true: Assume we have a an arbitrary smooth function $f$ , and an arbitrary function $s$ , and $g=s\circ f$ . Assume that $s$ is not the identity function (contradicting condition $i$ ), and that for some interval $(a,b)$ , $f(x)=g(x)$ for all $x\in (a,b)$ , (contradicting condition $ii$ ). Take $b$ here to be the largest $b$ , such that this holds (which is possible by the Completeness Axiom on $R$ ). Now denote by $f_n, g_n$ the $n$ th derivative of $f, g$ respectively. Since by assumption, $f$ is smooth on $b$ , we know that $$(1): \underset{\delta \to 0^-}{\text{Lim}}\left(\frac{f_{n-1}(b+\delta)-f_{n-1}(b)}{\delta}\right)=:L_{f_n}^-=L_{f_n}^+:=\underset{\delta \to 0^+}{\text{Lim}}\left(\frac{f_{n-1}(b+\delta)-f_{n-1}(b)}{\delta}\right)$$ ( $L$ will denote the limit with respect to the point $b$ ). $(2):$ Since $f$ and $g$ are identical on $(a,b)$ , we also know that $L_{f_n}^-=L_{g_n}^-$ , for all $n\in \mathbb N$ . $(3):$ Now assume (in order to derive a contradiction) that $g$ is smooth on $b$ , so that $L_{g_n}^-=L_{g_n}^+$ for all $n \in \mathbb N$ . Then using $(1,2)$ it also holds that $L_{f_n}^+=L_{g_n}^+$ for all $n \in \mathbb N$ . However, since $b$ is the largest value such that $f(x)=g(x)$ on $(a,b)$ , that means that either $f(b)\neq g(b)$ (in which case $g$ is discontinuous and not smooth, completing the proof for that case), or for some $c>b$ , it is the case that $f(x)\neq g(x)$ for all $x\in (b,c)$ . Now here comes a bit of a leap: Given that $f(x)\neq g(x)$ for all $x\in (b,c)$ , we also know that there is an interval $(b,\beta _1)$ , where $\beta_1\leq c$ , in which for all $x$ : $f_1(x)\neq g_1(x)$ . Similarly, given interval $(b, \beta_i)$ in which for all $x: f_i(x)\neq g_i(x)$ , there is an interval $(b, \beta_{i+1})$ , where $\beta_{i+1}\leq \beta_i$ , in which for all $x: f_{i+1}(x)\neq g_{i+1}(x)$ Again a leap: Hence we know that for any $n\in \mathbb N$ , there is a $\beta \in \mathbb N$ , such that for all $x\in (b, \beta), f_{n}(x)\neq g_{n}(x)$ . Hence there exists an $n\in \mathbb N$ , such that $L_{g_n}^+ \neq L_{f_n}^+$ . This contradicts $(3)$ , therefore, $g$ is not smooth. Discussion: Is this conjecture correct? Is the first part of the proof correct? Is there a way to fill in the ""leaps"" at the end? Are there better ways to prove it (or if the conjecture is false, to restate it into a correct one)? ps. note , I have no formal maths training, and I came up with this conjecture myself based on intuition, so if this is a stupid conjecture or proof, understand that.","Take any (smooth) function . For any arbitrary function , define as Conjecture : For any such , if is smooth ( ), the following must necessarily hold: : Either: (identity function), or : There exists no open ( ) interval on the domain of , for which holds: . i.e.: In plain English: A smooth function cannot be transformed into another smooth function, without changing the values in all its intervals: Only isolated points may remain unchanged. Here is an incomplete argument why it seems to me must be true: Assume we have a an arbitrary smooth function , and an arbitrary function , and . Assume that is not the identity function (contradicting condition ), and that for some interval , for all , (contradicting condition ). Take here to be the largest , such that this holds (which is possible by the Completeness Axiom on ). Now denote by the th derivative of respectively. Since by assumption, is smooth on , we know that ( will denote the limit with respect to the point ). Since and are identical on , we also know that , for all . Now assume (in order to derive a contradiction) that is smooth on , so that for all . Then using it also holds that for all . However, since is the largest value such that on , that means that either (in which case is discontinuous and not smooth, completing the proof for that case), or for some , it is the case that for all . Now here comes a bit of a leap: Given that for all , we also know that there is an interval , where , in which for all : . Similarly, given interval in which for all , there is an interval , where , in which for all Again a leap: Hence we know that for any , there is a , such that for all . Hence there exists an , such that . This contradicts , therefore, is not smooth. Discussion: Is this conjecture correct? Is the first part of the proof correct? Is there a way to fill in the ""leaps"" at the end? Are there better ways to prove it (or if the conjecture is false, to restate it into a correct one)? ps. note , I have no formal maths training, and I came up with this conjecture myself based on intuition, so if this is a stupid conjecture or proof, understand that.","C^\infty f: R \to R t:R\to R g :R\to R g(x)= (t\circ f)(x) g g g\in C^\infty (i) s(x) = x (ii) O_R U f, g f(U)=g(U) \forall U\in O_R:\exists x\in U:f(x)\neq g(x) f s g=s\circ f s i (a,b) f(x)=g(x) x\in (a,b) ii b b R f_n, g_n n f, g f b (1): \underset{\delta \to 0^-}{\text{Lim}}\left(\frac{f_{n-1}(b+\delta)-f_{n-1}(b)}{\delta}\right)=:L_{f_n}^-=L_{f_n}^+:=\underset{\delta \to 0^+}{\text{Lim}}\left(\frac{f_{n-1}(b+\delta)-f_{n-1}(b)}{\delta}\right) L b (2): f g (a,b) L_{f_n}^-=L_{g_n}^- n\in \mathbb N (3): g b L_{g_n}^-=L_{g_n}^+ n \in \mathbb N (1,2) L_{f_n}^+=L_{g_n}^+ n \in \mathbb N b f(x)=g(x) (a,b) f(b)\neq g(b) g c>b f(x)\neq g(x) x\in (b,c) f(x)\neq g(x) x\in (b,c) (b,\beta _1) \beta_1\leq c x f_1(x)\neq g_1(x) (b, \beta_i) x: f_i(x)\neq g_i(x) (b, \beta_{i+1}) \beta_{i+1}\leq \beta_i x: f_{i+1}(x)\neq g_{i+1}(x) n\in \mathbb N \beta \in \mathbb N x\in (b, \beta), f_{n}(x)\neq g_{n}(x) n\in \mathbb N L_{g_n}^+ \neq L_{f_n}^+ (3) g","['real-analysis', 'derivatives', 'proof-verification']"
22,Subdifferential of a convex differentiable function,Subdifferential of a convex differentiable function,,"Let us consider $f:\, \mathbb{R}^n \rightarrow \mathbb{R}$ be a convex function and differentiable at a point $x_0$. If $\partial f(x_0)$ denotes the subdifferential of $f$ I would like to prove that the only element in it is given by the gradient of f in $x_0$, i.e. $$ \partial f(x_0)=\left\lbrace\nabla f(x_0)\right\rbrace $$ I know that it should be trivial but i can't prove the inclusion: $\subset$ ...any suggestion?","Let us consider $f:\, \mathbb{R}^n \rightarrow \mathbb{R}$ be a convex function and differentiable at a point $x_0$. If $\partial f(x_0)$ denotes the subdifferential of $f$ I would like to prove that the only element in it is given by the gradient of f in $x_0$, i.e. $$ \partial f(x_0)=\left\lbrace\nabla f(x_0)\right\rbrace $$ I know that it should be trivial but i can't prove the inclusion: $\subset$ ...any suggestion?",,"['real-analysis', 'derivatives', 'convex-analysis', 'subgradient']"
23,Prove with use of derivative [closed],Prove with use of derivative [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question How to prove this inequality using derivative ? For each  $x>4$ , $$\displaystyle \sqrt[3]{x} - \sqrt[3]{4} < \sqrt[3]{x-4} $$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question How to prove this inequality using derivative ? For each  $x>4$ , $$\displaystyle \sqrt[3]{x} - \sqrt[3]{4} < \sqrt[3]{x-4} $$",,"['derivatives', 'inequality']"
24,How can I calculate the derivative of a Catmull-Rom spline with nonuniform parameterization?,How can I calculate the derivative of a Catmull-Rom spline with nonuniform parameterization?,,"Allow me to preface this by saying I am not a trained mathematician in any sense, so it's entirely possible I'm missing something rather fundamental. That said, I'm trying to take the derivative of a centripetal or chordal Catmull-Rom spline . Using the uniform equation calculating the tangents for the control points as well as the derivative is very simple. But when you want to make a spline with nonuniform $t$ values, you have to go through this: Arrows indicate that you multiply by the coefficient and add to the adjacent to form the next coefficient up in the pyramid. When you get up to $C$ you have your coordinate on the curve. $P$ values are your control points and $t$ values are calculated with the following: If I'm just plugging values into $t$ then I can get the curve coordinates, but I need the derivative to do some other calculations. I tried to expand the entire thing manually and take the derivative at $t_1$ and $t_2$ , but it was a mess, and I think I did it completely wrong, so I ask here. Looking at the answer here , this person claims to have done it and got these very simple formulas, but the only methodology listed is ""mathematica."" How can I get a usable, general derivative? Is it necessary to choose an alpha value beforehand, or can it be done easily without choosing a parameterization type? Images in this question were borrowed from an answer here .","Allow me to preface this by saying I am not a trained mathematician in any sense, so it's entirely possible I'm missing something rather fundamental. That said, I'm trying to take the derivative of a centripetal or chordal Catmull-Rom spline . Using the uniform equation calculating the tangents for the control points as well as the derivative is very simple. But when you want to make a spline with nonuniform values, you have to go through this: Arrows indicate that you multiply by the coefficient and add to the adjacent to form the next coefficient up in the pyramid. When you get up to you have your coordinate on the curve. values are your control points and values are calculated with the following: If I'm just plugging values into then I can get the curve coordinates, but I need the derivative to do some other calculations. I tried to expand the entire thing manually and take the derivative at and , but it was a mess, and I think I did it completely wrong, so I ask here. Looking at the answer here , this person claims to have done it and got these very simple formulas, but the only methodology listed is ""mathematica."" How can I get a usable, general derivative? Is it necessary to choose an alpha value beforehand, or can it be done easily without choosing a parameterization type? Images in this question were borrowed from an answer here .",t C P t t t_1 t_2,"['derivatives', 'numerical-methods', 'recursive-algorithms', 'spline']"
25,How do you work with infinitesimal exponents in synthetic differential geometry?,How do you work with infinitesimal exponents in synthetic differential geometry?,,"I just read this paper by Andrej Bauer , which discusses the basic tenets of synthetic differential geometry. Namely, that for any function $f$, any real number $x$, and any infinitesimal $\epsilon$ (a number which squares to $0$), there is a unique number $f'(x)$ such that $$f(x + \epsilon) = f(x) + f'(x)\epsilon$$ This allows us to simply prove some of the most useful high-school deriviatives. But, I am wondering about differentiating the function $f(x) = e^x$. Plugging it in, I get $f(x + \epsilon) = e^{x + \epsilon}$, but I'm not sure how to reduce this into the form $e^x + e^x\epsilon$. In general, how do we handle infinitesimal exponents $a^\epsilon$?","I just read this paper by Andrej Bauer , which discusses the basic tenets of synthetic differential geometry. Namely, that for any function $f$, any real number $x$, and any infinitesimal $\epsilon$ (a number which squares to $0$), there is a unique number $f'(x)$ such that $$f(x + \epsilon) = f(x) + f'(x)\epsilon$$ This allows us to simply prove some of the most useful high-school deriviatives. But, I am wondering about differentiating the function $f(x) = e^x$. Plugging it in, I get $f(x + \epsilon) = e^{x + \epsilon}$, but I'm not sure how to reduce this into the form $e^x + e^x\epsilon$. In general, how do we handle infinitesimal exponents $a^\epsilon$?",,"['calculus', 'derivatives', 'topos-theory']"
26,Multivariable calculus - Implicit function theorem,Multivariable calculus - Implicit function theorem,,"we are given the function $F: \mathbb R^3 \to \mathbb R^2$, $F(x,y,z)=\begin{pmatrix} x+yz-z^3-1 \\ x^3-xz+y^3\end{pmatrix}$ Show that around $(1,-1,0)$ we can represent $x$ and $y$ as functions of $z$, and find $\frac{dx}{dz}$ and $\frac{dy}{dz}$ What I did: The differential of $F$ is: $\begin{pmatrix} 1 & z &y-3z^2\\3x^2-z & 3y^2 &-x\end{pmatrix}$, insert $x=1,y=-1,z=0$ to get: $\begin{pmatrix} 1 & 0 &-1 \\3&3&-1\end{pmatrix}$ The matrix of the partial derivatives with respect to x and y is the first 2 columns, and it is invertible, and so the requirements of the implicit function theorem are met. How do i find the differential of $x$ and $y$ with respect to $z$ tho? One would expect that $\frac{dx}{dz} = -\frac{dF}{dz}(\frac{dF}{dx})^{-1}$ but...those are vectors. what is the inverse of a vector? how do you multiply vectors? there's a size mismatch.","we are given the function $F: \mathbb R^3 \to \mathbb R^2$, $F(x,y,z)=\begin{pmatrix} x+yz-z^3-1 \\ x^3-xz+y^3\end{pmatrix}$ Show that around $(1,-1,0)$ we can represent $x$ and $y$ as functions of $z$, and find $\frac{dx}{dz}$ and $\frac{dy}{dz}$ What I did: The differential of $F$ is: $\begin{pmatrix} 1 & z &y-3z^2\\3x^2-z & 3y^2 &-x\end{pmatrix}$, insert $x=1,y=-1,z=0$ to get: $\begin{pmatrix} 1 & 0 &-1 \\3&3&-1\end{pmatrix}$ The matrix of the partial derivatives with respect to x and y is the first 2 columns, and it is invertible, and so the requirements of the implicit function theorem are met. How do i find the differential of $x$ and $y$ with respect to $z$ tho? One would expect that $\frac{dx}{dz} = -\frac{dF}{dz}(\frac{dF}{dx})^{-1}$ but...those are vectors. what is the inverse of a vector? how do you multiply vectors? there's a size mismatch.",,"['calculus', 'derivatives', 'implicit-differentiation', 'implicit-function-theorem']"
27,"$F(x) =\int \limits_{\tan x}^{\cot x}\sqrt{1+t^2}\,\mathrm{d}t\,\,$ then $F'( \pi/4) =?$",then,"F(x) =\int \limits_{\tan x}^{\cot x}\sqrt{1+t^2}\,\mathrm{d}t\,\, F'( \pi/4) =?","$F(x) =\int \limits_{\tan x}^{\cot x}\sqrt{1+t^2}\,\mathrm{d}t\,\,$   then $F'(\pi/4) =?$ $$F(x) =\int \limits_{\tan x}^{\cot x}\sqrt{1+t^2}\,\mathrm{d}t$$ $$F(x) =-\int \limits_{0}^{\cot x}\sqrt{1+t^2}\,\mathrm{d}t + \int \limits_{0}^{\tan x}\sqrt{1+t^2}\,\mathrm{d}t$$ According to Fundamental Theorem Part 1 : \begin{align}Old\\ F'(x) & = f(b(x))\cdot b'(x)-f(a(x))\cdot a'(x) \\ F'(x) & = \sqrt{1+x^2}(-\csc^2x) - \sqrt{1+x^2}(\sec^2x) \\ F'(x) & = -\sqrt{1+x^2}(\csc^2x + \sec^2x) \\ F'(x) & = -\sqrt{1+x^2}(1) \\ F'(\pi/4) & = -\sqrt{1+(\pi/4)^2} \\ \end{align} New  $F'(x)  = \sqrt{1+x^2}(d/dx(\sqrt{1+\cot^2x}) - \sqrt{1+x^2}(d/dx(\sqrt{1+\tan^2x})$ $F'(x)  = \sqrt{1+x^2}(d/dx(\sqrt{\csc^2x}) - \sqrt{1+x^2}(d/dx(\sqrt{\sec^2x})$ $F'(x)  = \sqrt{1+x^2}[(-\csc{x}\cot{x}) - (-\sec{x}\tan{x}))$ $F'(\pi/4)  = \sqrt{1+(\pi/4)^2}[(-\csc{(\pi/4)}\cot{(\pi/4)}) - (\sec{(\pi/4)}\tan{(\pi/4)}))$ ?","$F(x) =\int \limits_{\tan x}^{\cot x}\sqrt{1+t^2}\,\mathrm{d}t\,\,$   then $F'(\pi/4) =?$ $$F(x) =\int \limits_{\tan x}^{\cot x}\sqrt{1+t^2}\,\mathrm{d}t$$ $$F(x) =-\int \limits_{0}^{\cot x}\sqrt{1+t^2}\,\mathrm{d}t + \int \limits_{0}^{\tan x}\sqrt{1+t^2}\,\mathrm{d}t$$ According to Fundamental Theorem Part 1 : \begin{align}Old\\ F'(x) & = f(b(x))\cdot b'(x)-f(a(x))\cdot a'(x) \\ F'(x) & = \sqrt{1+x^2}(-\csc^2x) - \sqrt{1+x^2}(\sec^2x) \\ F'(x) & = -\sqrt{1+x^2}(\csc^2x + \sec^2x) \\ F'(x) & = -\sqrt{1+x^2}(1) \\ F'(\pi/4) & = -\sqrt{1+(\pi/4)^2} \\ \end{align} New  $F'(x)  = \sqrt{1+x^2}(d/dx(\sqrt{1+\cot^2x}) - \sqrt{1+x^2}(d/dx(\sqrt{1+\tan^2x})$ $F'(x)  = \sqrt{1+x^2}(d/dx(\sqrt{\csc^2x}) - \sqrt{1+x^2}(d/dx(\sqrt{\sec^2x})$ $F'(x)  = \sqrt{1+x^2}[(-\csc{x}\cot{x}) - (-\sec{x}\tan{x}))$ $F'(\pi/4)  = \sqrt{1+(\pi/4)^2}[(-\csc{(\pi/4)}\cot{(\pi/4)}) - (\sec{(\pi/4)}\tan{(\pi/4)}))$ ?",,"['calculus', 'derivatives']"
28,Smooth sequence of functions converging pointwise to a smooth function and limit of derivatives,Smooth sequence of functions converging pointwise to a smooth function and limit of derivatives,,"In the wikipedia page of uniform convergence, it says that given a sequence $\{ f_n \}$ of differentiable real functions (say, over the reals) with the property that it converges pointwise to some function $f$ , the limit of $\{f_{n}' \}$ need not be equal to $f'$ . It then gives an example where $\{ f_n \} $ converges uniformly to a differentiable $f$ , but $\{f_n '\}$ does not converge even pointwise. My question is, what if we assume that each $f_n$ and their limit $f$ are, say, $C^\infty$ , and $\{ f_n '\}$ converges pointwise to some $C^\infty$ function $g$ as well. Is it now enough to show that $f' = g$ ? Or do we further need to assume uniform convergence? Is there a classic counterexample to this question as well?","In the wikipedia page of uniform convergence, it says that given a sequence of differentiable real functions (say, over the reals) with the property that it converges pointwise to some function , the limit of need not be equal to . It then gives an example where converges uniformly to a differentiable , but does not converge even pointwise. My question is, what if we assume that each and their limit are, say, , and converges pointwise to some function as well. Is it now enough to show that ? Or do we further need to assume uniform convergence? Is there a classic counterexample to this question as well?",\{ f_n \} f \{f_{n}' \} f' \{ f_n \}  f \{f_n '\} f_n f C^\infty \{ f_n '\} C^\infty g f' = g,"['real-analysis', 'derivatives', 'uniform-convergence']"
29,Is the total derivative defined for a function of multiple independent variables?,Is the total derivative defined for a function of multiple independent variables?,,"It seems clear enough to define the total derivative of a function $f=f(x(t),y(t))$ of multiple ""intermediate variables"" $x$ and $y$ , who themselves depend on one independent variable $t$ , to be: $$\frac{df}{dt}=\frac{\partial f}{\partial x} \frac{dx}{dt} + \frac{\partial f}{\partial y} \frac{dy}{dt}$$ (One could say this is simply an application of the multivariable chain rule.) I also find it reasonable when multiple independent variables $t$ and $u$ are present (yielding $f=f(x(t,u),y(t,u))$ to calculate the partial derivatives by: $$\frac{\partial f}{\partial t}=\frac{\partial f}{\partial x} \frac{\partial x}{\partial t} + \frac{\partial f}{\partial y} \frac{\partial y}{\partial t}$$ (similarly for $\frac{\partial f}{\partial u}$ .) However, I am confused if such as thing as the ""total derivative"" $\frac{d f}{d t}$ can be defined when their are multiple independent variables $t$ and $u$ present, and how it would relate to the partial derivative $\frac{\partial f}{\partial t}$ . Note: I understand that in the simple case of a function depending explicitly on multiple independent variables, such as $f=f(t,u)$ , we do have total derivatives with respect to each variable: $$\frac{df}{dt}=\frac{\partial f}{\partial t} + \frac{\partial f}{\partial u} \frac{du}{dt}  \text{  and  }  \frac{df}{du}=\frac{\partial f}{\partial t} \frac{dt}{du} + \frac{\partial f}{\partial u}$$ I'm not sure how this can be generalized to the case with $f$ depending on intermediate variables $x(t,u)$ and $y(t,u)$ . I attempted to solve this myself, but I ended up with a strange result. Here was my approach: Take the total differential of $f$ : $$df=\frac{\partial f}{\partial x} dx + \frac{\partial f}{\partial y} dy$$ being aware that since $x=x(t,u)$ , then $dx=\frac{\partial x}{\partial t} dt + \frac{\partial x}{\partial u} du$ and similarly for $dy$ . Dividing the total differential by $dt$ (not rigorous, I know), we obtain: $$\frac{df}{dt}=\frac{\partial f}{\partial x} \frac{dx}{dt} + \frac{\partial f}{\partial y} \frac{dy}{dt}$$ We can evaluate $\frac{dx}{dt}$ as $\frac{dx}{dt}= \frac{\partial x}{\partial t} \frac{dt}{dt} + \frac{\partial x}{\partial u} \frac{du}{dt} = \frac{\partial x}{\partial t}$ because $\frac{du}{dt}=0$ , and similarly for $\frac{dy}{dt}$ : $$\frac{df}{dt}=\frac{\partial f}{\partial x} \frac{\partial x}{\partial t} + \frac{\partial f}{\partial y} \frac{\partial y}{\partial t}$$ But I must have made a mistake somewhere, because this expression is equal to $\frac{\partial f}{\partial t}$ , which should only be true when $t$ is the only independent variable. What have I done wrong, and what is the true expression for $\frac{df}{dt}$ (if it exists)? Thank you for your time.","It seems clear enough to define the total derivative of a function of multiple ""intermediate variables"" and , who themselves depend on one independent variable , to be: (One could say this is simply an application of the multivariable chain rule.) I also find it reasonable when multiple independent variables and are present (yielding to calculate the partial derivatives by: (similarly for .) However, I am confused if such as thing as the ""total derivative"" can be defined when their are multiple independent variables and present, and how it would relate to the partial derivative . Note: I understand that in the simple case of a function depending explicitly on multiple independent variables, such as , we do have total derivatives with respect to each variable: I'm not sure how this can be generalized to the case with depending on intermediate variables and . I attempted to solve this myself, but I ended up with a strange result. Here was my approach: Take the total differential of : being aware that since , then and similarly for . Dividing the total differential by (not rigorous, I know), we obtain: We can evaluate as because , and similarly for : But I must have made a mistake somewhere, because this expression is equal to , which should only be true when is the only independent variable. What have I done wrong, and what is the true expression for (if it exists)? Thank you for your time.","f=f(x(t),y(t)) x y t \frac{df}{dt}=\frac{\partial f}{\partial x} \frac{dx}{dt} + \frac{\partial f}{\partial y} \frac{dy}{dt} t u f=f(x(t,u),y(t,u)) \frac{\partial f}{\partial t}=\frac{\partial f}{\partial x} \frac{\partial x}{\partial t} + \frac{\partial f}{\partial y} \frac{\partial y}{\partial t} \frac{\partial f}{\partial u} \frac{d f}{d t} t u \frac{\partial f}{\partial t} f=f(t,u) \frac{df}{dt}=\frac{\partial f}{\partial t} + \frac{\partial f}{\partial u} \frac{du}{dt}  \text{  and  }  \frac{df}{du}=\frac{\partial f}{\partial t} \frac{dt}{du} + \frac{\partial f}{\partial u} f x(t,u) y(t,u) f df=\frac{\partial f}{\partial x} dx + \frac{\partial f}{\partial y} dy x=x(t,u) dx=\frac{\partial x}{\partial t} dt + \frac{\partial x}{\partial u} du dy dt \frac{df}{dt}=\frac{\partial f}{\partial x} \frac{dx}{dt} + \frac{\partial f}{\partial y} \frac{dy}{dt} \frac{dx}{dt} \frac{dx}{dt}= \frac{\partial x}{\partial t} \frac{dt}{dt} + \frac{\partial x}{\partial u} \frac{du}{dt} = \frac{\partial x}{\partial t} \frac{du}{dt}=0 \frac{dy}{dt} \frac{df}{dt}=\frac{\partial f}{\partial x} \frac{\partial x}{\partial t} + \frac{\partial f}{\partial y} \frac{\partial y}{\partial t} \frac{\partial f}{\partial t} t \frac{df}{dt}","['derivatives', 'partial-derivative', 'differential']"
30,Derivative of argmin in a constrained problem,Derivative of argmin in a constrained problem,,"Let $f(x,y)$ be a continuously differentiable function from $\mathbb{R}^2$ to $\mathbb{R}$ . Suppose that for every $y$ the function $g_y(x)=f(x,y)$ is strictly convex.  Define $$ h(y) = \arg\min_{x\leq b} f(x,y) $$ where $b\in \mathbb{R}$ is a parameter that constraint the set of possible $x$ . My question is: Can we say anything about the derivative of the argmin $h'(y)$ ? I know from this question that $h(y)$ is continuous. In addition, the answer to this question shows that when the problem is unconstrained ( $b=\infty$ ) we can differentiate the optimality condition $$ \frac{ \partial f(h(y),y)}{\partial x} = 0 $$ with respect to $y$ to obtain an expression for $h'(y)$ . But when the problem is constrained ( $b<\infty$ ) that expression becomes $$ \frac{ \partial f(h(y),y)}{\partial x} = \lambda(y) $$ where $\lambda(y)$ is the Lagrange multiplier on the constraint $x\leq b$ and its hard to say anything about $h'(y)$ without knowing $\lambda'(y)$ .","Let be a continuously differentiable function from to . Suppose that for every the function is strictly convex.  Define where is a parameter that constraint the set of possible . My question is: Can we say anything about the derivative of the argmin ? I know from this question that is continuous. In addition, the answer to this question shows that when the problem is unconstrained ( ) we can differentiate the optimality condition with respect to to obtain an expression for . But when the problem is constrained ( ) that expression becomes where is the Lagrange multiplier on the constraint and its hard to say anything about without knowing .","f(x,y) \mathbb{R}^2 \mathbb{R} y g_y(x)=f(x,y) 
h(y) = \arg\min_{x\leq b} f(x,y)
 b\in \mathbb{R} x h'(y) h(y) b=\infty 
\frac{ \partial f(h(y),y)}{\partial x} = 0
 y h'(y) b<\infty 
\frac{ \partial f(h(y),y)}{\partial x} = \lambda(y)
 \lambda(y) x\leq b h'(y) \lambda'(y)","['real-analysis', 'derivatives', 'optimization', 'convex-optimization', 'partial-derivative']"
31,"Why $(\max\{0, x\})^n$ is a differentiable function?",Why  is a differentiable function?,"(\max\{0, x\})^n","I am reading this post that proves why $\max\{0,x\}$ is not differentiable. But why $(\max\{0, x\})^n$ for $n>1$ is differentiable?",I am reading this post that proves why is not differentiable. But why for is differentiable?,"\max\{0,x\} (\max\{0, x\})^n n>1",['derivatives']
32,I want to know what was the motivation/intuition behind weierstrass function?,I want to know what was the motivation/intuition behind weierstrass function?,,I know that it came as an example of a function that is continuous everywhere but differentiable nowhere.But how did the idea of such an unusual construction came?,I know that it came as an example of a function that is continuous everywhere but differentiable nowhere.But how did the idea of such an unusual construction came?,,"['real-analysis', 'derivatives']"
33,Derivative of absolute value of complex-valued function,Derivative of absolute value of complex-valued function,,"I was wondering whether there was a nice formula for something like $$ \dfrac{\partial}{\partial x} \left| e^x + (1+i)e^{-x} \right|. $$ (Note that the function is chosen on purpose to have no discontinuities in the derivative, as the argument to the absolute value function never passes through zero.) So I typed into WolframAlpha: D[|exp(x) + (1+i)*exp(-x)|, x] and it said: Huh!? What imaginary component?","I was wondering whether there was a nice formula for something like $$ \dfrac{\partial}{\partial x} \left| e^x + (1+i)e^{-x} \right|. $$ (Note that the function is chosen on purpose to have no discontinuities in the derivative, as the argument to the absolute value function never passes through zero.) So I typed into WolframAlpha: D[|exp(x) + (1+i)*exp(-x)|, x] and it said: Huh!? What imaginary component?",,"['derivatives', 'complex-numbers', 'absolute-value']"
34,Second derivative of absolute value function proportional to Dirac delta function?,Second derivative of absolute value function proportional to Dirac delta function?,,"I have recently discovered the relation \begin{equation}   \frac{\mathrm d^2}{\mathrm dx^2} \big| x \big| = 2\delta (x). \end{equation} I was very intrigued when I found this expression, and as it makes sense to me intuitively. However, I am aware that Dirac's delta ""function"" is defined as a distribution, not a function in the conventional sense, so there is probably more going on here than meets the untrained eye. So my question is, what caveats are necessary for the above equation for it to be formally correct? Can it be taken literally? I would have presented a definition of the delta function for completeness, but I am also not certain which definition is appropriate in this context. Am I correct in my understanding that the piecewise definition \begin{equation}   \delta (x) \equiv    \begin{cases}      \infty, & \operatorname{if} x = 0 \\      0,      & \operatorname{if} x \neq 0   \end{cases}  \end{equation} Is only an informal, intuitive definition and actually an ""abuse of notation"" of proper functions? I have knowledge up to multivariable/vector calculus and differential equations, but no formal or distribution theory, analysis, number theory, etc. training. Please accommodate if appropriate. [I apologize for the formatting issues, I am genuinely trying to learn. Can someone please point me in the direction of a crash course for LaTeX (particularly math)? I've looked around a lot on SE and elsewhere on the web and I'm having trouble finding tutorials that explain the simplest of things, like when to use the dollar sign and when to use the double dollar, how to write dollar signs without them taking effect, when to use a package in the preamble, if the preamble is a specific location or just anything before the expression to be formatted, etc. Sorry for the meta question.]","I have recently discovered the relation \begin{equation}   \frac{\mathrm d^2}{\mathrm dx^2} \big| x \big| = 2\delta (x). \end{equation} I was very intrigued when I found this expression, and as it makes sense to me intuitively. However, I am aware that Dirac's delta ""function"" is defined as a distribution, not a function in the conventional sense, so there is probably more going on here than meets the untrained eye. So my question is, what caveats are necessary for the above equation for it to be formally correct? Can it be taken literally? I would have presented a definition of the delta function for completeness, but I am also not certain which definition is appropriate in this context. Am I correct in my understanding that the piecewise definition \begin{equation}   \delta (x) \equiv    \begin{cases}      \infty, & \operatorname{if} x = 0 \\      0,      & \operatorname{if} x \neq 0   \end{cases}  \end{equation} Is only an informal, intuitive definition and actually an ""abuse of notation"" of proper functions? I have knowledge up to multivariable/vector calculus and differential equations, but no formal or distribution theory, analysis, number theory, etc. training. Please accommodate if appropriate. [I apologize for the formatting issues, I am genuinely trying to learn. Can someone please point me in the direction of a crash course for LaTeX (particularly math)? I've looked around a lot on SE and elsewhere on the web and I'm having trouble finding tutorials that explain the simplest of things, like when to use the dollar sign and when to use the double dollar, how to write dollar signs without them taking effect, when to use a package in the preamble, if the preamble is a specific location or just anything before the expression to be formatted, etc. Sorry for the meta question.]",,"['derivatives', 'distribution-theory', 'absolute-value', 'dirac-delta']"
35,Newton's method vs. gradient descent with exact line search,Newton's method vs. gradient descent with exact line search,,"tl;dr: When is gradient descent with exact line search preferred over Newton's method? I simply don't understand why exact line search is ever useful, and here's my reasoning. Let's say I have a function $f$ that I want to minimize. Gradient descent performs the updates: $$\vec{x} \gets \vec{x} - t \vec{\nabla}f(\vec{x})$$ The optimal step size $t$ must satisfy: $$\frac{d}{d t} f(\vec{x} - t\vec{h}) = 0$$ To solve this, I approximate $f$ with its Taylor polynomial: $$f(\vec{x} - t\vec{h}) \approx f(\vec{x}) - t\vec{h}^\top \vec{\nabla}f(\vec{x}) + \frac{t^2}{2}\vec{h}^\top \vec{\nabla}^2f(\vec{x})\ \vec{h} + \ldots$$ ...and then differentiate it with respect to $t$ and set it equal to $0$ : $$\frac{d}{d t} f(\vec{x} - t\vec{h}) \approx -\vec{h}^\top \vec{\nabla}f(\vec{x}) + t\vec{h}^\top \vec{\nabla}^2f(\vec{x})\ \vec{h} = 0$$ $$t = \frac{\vec{h}^\top \vec{\nabla}f(\vec{x})}{\vec{h}^\top \vec{\nabla}^2f(\vec{x})\ \vec{h}}$$ Since $\vec{h} = \vec{\nabla} f(\vec{x})$ , we have: $$t = \frac{\vec{\nabla}f(\vec{x})^\top \vec{\nabla}f(\vec{x})}{\vec{\nabla}f(\vec{x})^\top \vec{\nabla}^2f(\vec{x})\ \vec{\nabla}f(\vec{x})}$$ This is great, except that we can see that computing $t$ requires the Hessian $\vec{\nabla}^2f$ (expensive)! But if I could really compute the Hessian, then wouldn't I be using Newton's method instead? Why would I ever choose to do exact line search?","tl;dr: When is gradient descent with exact line search preferred over Newton's method? I simply don't understand why exact line search is ever useful, and here's my reasoning. Let's say I have a function that I want to minimize. Gradient descent performs the updates: The optimal step size must satisfy: To solve this, I approximate with its Taylor polynomial: ...and then differentiate it with respect to and set it equal to : Since , we have: This is great, except that we can see that computing requires the Hessian (expensive)! But if I could really compute the Hessian, then wouldn't I be using Newton's method instead? Why would I ever choose to do exact line search?",f \vec{x} \gets \vec{x} - t \vec{\nabla}f(\vec{x}) t \frac{d}{d t} f(\vec{x} - t\vec{h}) = 0 f f(\vec{x} - t\vec{h}) \approx f(\vec{x}) - t\vec{h}^\top \vec{\nabla}f(\vec{x}) + \frac{t^2}{2}\vec{h}^\top \vec{\nabla}^2f(\vec{x})\ \vec{h} + \ldots t 0 \frac{d}{d t} f(\vec{x} - t\vec{h}) \approx -\vec{h}^\top \vec{\nabla}f(\vec{x}) + t\vec{h}^\top \vec{\nabla}^2f(\vec{x})\ \vec{h} = 0 t = \frac{\vec{h}^\top \vec{\nabla}f(\vec{x})}{\vec{h}^\top \vec{\nabla}^2f(\vec{x})\ \vec{h}} \vec{h} = \vec{\nabla} f(\vec{x}) t = \frac{\vec{\nabla}f(\vec{x})^\top \vec{\nabla}f(\vec{x})}{\vec{\nabla}f(\vec{x})^\top \vec{\nabla}^2f(\vec{x})\ \vec{\nabla}f(\vec{x})} t \vec{\nabla}^2f,"['derivatives', 'optimization', 'convex-optimization', 'newton-raphson']"
36,How do computers draw function graphs?,How do computers draw function graphs?,,"Could anyone give me a general overview of the algorithm computers use to draw graphs? I'm guessing it is either by graphing many, many points and just connecting them or by doing a general study of the function with the derivative. However, just taking a derivative is not enough to know the exact behavior of the function in an interval, since you would have to evaluate each point of the interval in the derivative, leaving us with the first method again... So how is it done?","Could anyone give me a general overview of the algorithm computers use to draw graphs? I'm guessing it is either by graphing many, many points and just connecting them or by doing a general study of the function with the derivative. However, just taking a derivative is not enough to know the exact behavior of the function in an interval, since you would have to evaluate each point of the interval in the derivative, leaving us with the first method again... So how is it done?",,"['calculus', 'algorithms', 'derivatives', 'graphing-functions']"
37,Closed form for $n$-th derivative of exponential: $\exp\left(-\frac{\pi^2a^2}{x}\right)$,Closed form for -th derivative of exponential:,n \exp\left(-\frac{\pi^2a^2}{x}\right),"I need the closed-form for the $n$-th derivative ($n\geq0 $): $$\frac{\partial^n}{\partial x^n}\exp\left(-\frac{\pi^2a^2}{x}\right)$$ Thanks! By following the suggestion of Hermite polynomials: $$H_n(x)=(-1)^ne^{x^2}\frac{\partial^n}{\partial x^n}e^{-x^2}$$ and doing the variable change $x=\pi a y^{-\frac{1}{2}}$, I obtain: $$\frac{\partial^n}{\partial x^n}=-2\left(\frac{y^{\frac{3}{2}}}{\pi a}\right)^n\frac{\partial^n}{\partial y^n}$$ and therefore $$H_n(\pi a y^{-\frac{1}{2}})=(-1)^{n+1}e^{\frac{\pi^2a^2}{y}}2\left(\frac{\pi a}{y^{\frac{3}{2}}}\right)^n\frac{\partial^n}{\partial y^n}e^{-\frac{\pi^2a^2}{y}}$$ Finally $$\frac{\partial^n}{\partial y^n}e^{-\frac{\pi^2a^2}{y}}=\frac{1}{2}e^{-\frac{\pi^2a^2}{y}}(-1)^{n+1}H_n(\pi a y^{-\frac{1}{2}})\left(\frac{y^{\frac{3}{2}}}{\pi a}\right)^n$$ Is this correct?","I need the closed-form for the $n$-th derivative ($n\geq0 $): $$\frac{\partial^n}{\partial x^n}\exp\left(-\frac{\pi^2a^2}{x}\right)$$ Thanks! By following the suggestion of Hermite polynomials: $$H_n(x)=(-1)^ne^{x^2}\frac{\partial^n}{\partial x^n}e^{-x^2}$$ and doing the variable change $x=\pi a y^{-\frac{1}{2}}$, I obtain: $$\frac{\partial^n}{\partial x^n}=-2\left(\frac{y^{\frac{3}{2}}}{\pi a}\right)^n\frac{\partial^n}{\partial y^n}$$ and therefore $$H_n(\pi a y^{-\frac{1}{2}})=(-1)^{n+1}e^{\frac{\pi^2a^2}{y}}2\left(\frac{\pi a}{y^{\frac{3}{2}}}\right)^n\frac{\partial^n}{\partial y^n}e^{-\frac{\pi^2a^2}{y}}$$ Finally $$\frac{\partial^n}{\partial y^n}e^{-\frac{\pi^2a^2}{y}}=\frac{1}{2}e^{-\frac{\pi^2a^2}{y}}(-1)^{n+1}H_n(\pi a y^{-\frac{1}{2}})\left(\frac{y^{\frac{3}{2}}}{\pi a}\right)^n$$ Is this correct?",,"['derivatives', 'exponential-function', 'closed-form']"
38,Local minima and local maxima of a univariate polynomial,Local minima and local maxima of a univariate polynomial,,"Let $$f(x) = a_n x^n + a_{n-1} x^{n-1} + \ldots + a_1 x + a_0$$ be a real polynomial of degree $n > 0$ . Using the derivative test, the values of $x$ for which the function $f(x)$ attains local minima and local maxima can be determined. However, considering this does not immediately give the values of $f(x)$ at those local minima and local maxima, I was wondering the following: Is it possible for $f(x)$ to attain a higher value at a local minimum than at a local maximum? If so, what is the lowest degree in which this can happen? For the second question, I was thinking about degree $5$ . Such a polynomial can have two local maxima and two local minima, since its derivative has degree $4$ . Order the extrema in increasing value of $x$ at which it is attained. First suppose $a_n > 0$ . I think it could have the first local maximum be lower than the last local minimum if the increase from the first local minimum to the second local maximum is more than the decreases from the local maxima to the next local minima combined (vice versa if $a_n < 0$ ). However, I could not find an example, so I am not sure if the first question is even true.","Let be a real polynomial of degree . Using the derivative test, the values of for which the function attains local minima and local maxima can be determined. However, considering this does not immediately give the values of at those local minima and local maxima, I was wondering the following: Is it possible for to attain a higher value at a local minimum than at a local maximum? If so, what is the lowest degree in which this can happen? For the second question, I was thinking about degree . Such a polynomial can have two local maxima and two local minima, since its derivative has degree . Order the extrema in increasing value of at which it is attained. First suppose . I think it could have the first local maximum be lower than the last local minimum if the increase from the first local minimum to the second local maximum is more than the decreases from the local maxima to the next local minima combined (vice versa if ). However, I could not find an example, so I am not sure if the first question is even true.",f(x) = a_n x^n + a_{n-1} x^{n-1} + \ldots + a_1 x + a_0 n > 0 x f(x) f(x) f(x) 5 4 x a_n > 0 a_n < 0,"['real-analysis', 'calculus', 'derivatives', 'polynomials', 'maxima-minima']"
39,The role of machine precision in the derivative approximation,The role of machine precision in the derivative approximation,,"You wish to compute of $f^{\prime}(x)$ using the approximation $$\widehat{f^{\prime}}(x)=\frac{f({x}+h)-{f}(x)}{h}$$ for some fixed $0<h<0.01$ . Suppose that the computer already contains a built-in function for computing $f$ , with relative errors less than machine precision, $t$ , and you know the value of a constant $A$ satisfying $\left|f^{\prime \prime}(y)\right| \leq A$ for all $y$ with $|y-x|<0.01$ . (a)  Estimate the total error incurred when computing $f^{\prime}(x)$ . Give your answer in terms of $f(x), f^{\prime}(x)$ , $h$ and $t$ . My attempt: Taylors remainder theorem says $$f(x+h)=f(x)+hf'(x)+(h^2/2!)f''(\zeta),~$$ for some $\zeta \in (x,x+h)$ . Now $$ \widehat{f^{\prime}}(x)=\frac{f({x}+h)-{f}(x)}{h}=f'(x)+(h/2)f''(\zeta), $$ implies $$|\widehat{f^{\prime}}(x)-f'(x)| \leq (h/2)A,$$ since $|\zeta-x|<0.01$ . Doubt: What exactly is the role of $t$ here? How the relative error in computing $f$ using the  built in function comes into play?","You wish to compute of using the approximation for some fixed . Suppose that the computer already contains a built-in function for computing , with relative errors less than machine precision, , and you know the value of a constant satisfying for all with . (a)  Estimate the total error incurred when computing . Give your answer in terms of , and . My attempt: Taylors remainder theorem says for some . Now implies since . Doubt: What exactly is the role of here? How the relative error in computing using the  built in function comes into play?","f^{\prime}(x) \widehat{f^{\prime}}(x)=\frac{f({x}+h)-{f}(x)}{h} 0<h<0.01 f t A \left|f^{\prime \prime}(y)\right| \leq A y |y-x|<0.01 f^{\prime}(x) f(x), f^{\prime}(x) h t f(x+h)=f(x)+hf'(x)+(h^2/2!)f''(\zeta),~ \zeta \in (x,x+h) 
\widehat{f^{\prime}}(x)=\frac{f({x}+h)-{f}(x)}{h}=f'(x)+(h/2)f''(\zeta),
 |\widehat{f^{\prime}}(x)-f'(x)| \leq (h/2)A, |\zeta-x|<0.01 t f","['real-analysis', 'derivatives', 'numerical-methods']"
40,"Why does the technique to find derivative of $y=\frac{ax^2+bx+c}{a'x^2+b'x+c'}$ quickly, works?","Why does the technique to find derivative of  quickly, works?",y=\frac{ax^2+bx+c}{a'x^2+b'x+c'},"I just watched a video about a very good technique to evaluate derivative of $ y=\dfrac{ax^2+bx+c}{a'x^2+b'x+c'} ,\quad a'\neq0$ quickly: $$\large y'=\large\dfrac{{ \begin{vmatrix}{a} && {b} \\ {a'} && {b'}\end{vmatrix} }x^2+\color{red}2{ \begin{vmatrix}{a} && {c} \\ {a'} && {c'}\end{vmatrix} }x+{ \begin{vmatrix}{b} && {c} \\ {b'} && {c'}\end{vmatrix} }}{(a'x^2+b'x+c')^2}$$ I wonder is it possible to prove this is true? Of course we can calculate derivative of $y$ and expanding the terms and compare it with the formula that I mentioned. But is it possible to check this is true intuitively Or without expanding? ( it is obvious that denominator should be $(a'x^2+b'x+c')^2$ )",I just watched a video about a very good technique to evaluate derivative of quickly: I wonder is it possible to prove this is true? Of course we can calculate derivative of and expanding the terms and compare it with the formula that I mentioned. But is it possible to check this is true intuitively Or without expanding? ( it is obvious that denominator should be )," y=\dfrac{ax^2+bx+c}{a'x^2+b'x+c'} ,\quad a'\neq0 \large y'=\large\dfrac{{ \begin{vmatrix}{a} && {b} \\ {a'} && {b'}\end{vmatrix} }x^2+\color{red}2{ \begin{vmatrix}{a} && {c} \\ {a'} && {c'}\end{vmatrix} }x+{ \begin{vmatrix}{b} && {c} \\ {b'} && {c'}\end{vmatrix} }}{(a'x^2+b'x+c')^2} y (a'x^2+b'x+c')^2","['calculus', 'derivatives']"
41,Differentiate $\arcsin(\frac{1}{\sqrt{1 + x^2}})$,Differentiate,\arcsin(\frac{1}{\sqrt{1 + x^2}}),"This is from Calculus by Michael Spivak, 3rd Edition, Chapter 15, problem 1 (iv): Differentiate $f(x) = \arcsin\left(\frac{1}{\sqrt{1 + x^2}}\right).$ Here's my attempt: \begin{align}f'(x) &= \left[\arcsin\left(\frac{1}{\sqrt{1 + x^2}}\right)\right]' \\ &= \arcsin'\left(\frac{1}{\sqrt{1 + x^2}}\right)\cdot \left(\frac{1}{\sqrt{1 + x^2}}\right)' \\ &= \frac{1}{\sqrt{1 - \left(\frac{1}{\sqrt{1 + x^2}}\right)^2}} \cdot \frac{-x}{(1+x^2)^{3/2}} \\ &= \frac{1}{\sqrt{1 - \frac{1}{1+x^2}}} \cdot \frac{-x}{(1+x^2)^{3/2}} \\ &= \frac{1}{\sqrt{\frac{1 + x^2 - 1}{1+x^2}}} \cdot \frac{-x}{(1+x^2)^{3/2}} \\ &= \frac{-x}{\frac{\sqrt{x^2}}{\sqrt{1+x^2}} \cdot \sqrt{1 + x^2}(1+x^2)} \\ &=\frac{-x}{|x|} \cdot \frac{1}{1+x^2}.\end{align} Therefore $$f'(x) = \begin{cases} \dfrac{-1}{1+x^2},  & \text{if $x > 0$} \\[1ex] \dfrac{1}{1+x^2}, & \text{if $x < 0$} \end{cases} $$ In the Answer Book the published solution is $\frac{-1}{1+x^2}$ . It seems Spivak is discounting the possibility that $x < 0$ . Why might this be? Is there a reason $x$ cannot be negative here? Did I make a mistake? Perhaps with this part of simplifying the denominator? $$\sqrt{\frac{x^2}{1+x^2}} \cdot \sqrt{1 + x^2} = \sqrt {x^2} = |x|$$ I suspect there's something simple I'm not seeing. I'm just now encountering these inverse trigonometric functions, so my understanding isn't very solid yet. Edit: This error could have become a misleading, possibly demoralizing waste of time, but thanks your insightful answers it's instead proven to be more instructive than the first 3 (omitted, correct) parts of the problem.","This is from Calculus by Michael Spivak, 3rd Edition, Chapter 15, problem 1 (iv): Differentiate Here's my attempt: Therefore In the Answer Book the published solution is . It seems Spivak is discounting the possibility that . Why might this be? Is there a reason cannot be negative here? Did I make a mistake? Perhaps with this part of simplifying the denominator? I suspect there's something simple I'm not seeing. I'm just now encountering these inverse trigonometric functions, so my understanding isn't very solid yet. Edit: This error could have become a misleading, possibly demoralizing waste of time, but thanks your insightful answers it's instead proven to be more instructive than the first 3 (omitted, correct) parts of the problem.","f(x) = \arcsin\left(\frac{1}{\sqrt{1 + x^2}}\right). \begin{align}f'(x) &= \left[\arcsin\left(\frac{1}{\sqrt{1 + x^2}}\right)\right]' \\
&= \arcsin'\left(\frac{1}{\sqrt{1 + x^2}}\right)\cdot \left(\frac{1}{\sqrt{1 + x^2}}\right)' \\
&= \frac{1}{\sqrt{1 - \left(\frac{1}{\sqrt{1 + x^2}}\right)^2}} \cdot \frac{-x}{(1+x^2)^{3/2}} \\
&= \frac{1}{\sqrt{1 - \frac{1}{1+x^2}}} \cdot \frac{-x}{(1+x^2)^{3/2}} \\
&= \frac{1}{\sqrt{\frac{1 + x^2 - 1}{1+x^2}}} \cdot \frac{-x}{(1+x^2)^{3/2}} \\
&= \frac{-x}{\frac{\sqrt{x^2}}{\sqrt{1+x^2}} \cdot \sqrt{1 + x^2}(1+x^2)} \\
&=\frac{-x}{|x|} \cdot \frac{1}{1+x^2}.\end{align} f'(x) =
\begin{cases}
\dfrac{-1}{1+x^2},  & \text{if x > 0} \\[1ex]
\dfrac{1}{1+x^2}, & \text{if x < 0}
\end{cases}
 \frac{-1}{1+x^2} x < 0 x \sqrt{\frac{x^2}{1+x^2}} \cdot \sqrt{1 + x^2} = \sqrt {x^2} = |x|","['calculus', 'derivatives', 'trigonometry', 'inverse-function', 'chain-rule']"
42,Inequality for function of $\arctan(x)$,Inequality for function of,\arctan(x),"I want to show that $$f(x) = \frac{1}{\arctan(x)} - \frac{1}{x} $$ is increasing on $(0, \infty)$ .  I can see this clearly by plotting it, but I'm struggling to write it out rigorously.  It obviously suffices to show its derivative is always positive in this range (which is also clear from plotting it).  We have $$f'(x) = \frac{(1+x^2)\arctan^2(x) -x^2}{x^2(1+x^2)\arctan^2(x)}$$ so again it suffices to show that $$g(x) \equiv (1+x^2)\arctan^2(x) -x^2 \ge 0 \quad \forall x >0$$ (and, yet again, this is clear from plotting it).  I've jumped down the rabbit hole of taking the derivative of $g$ as well (since it is $0$ at $x = 0$ so it would again suffice to show that $g' \ge 0$ ) and it doesn't yield anything immediately useful for me.  Please help if you can","I want to show that is increasing on .  I can see this clearly by plotting it, but I'm struggling to write it out rigorously.  It obviously suffices to show its derivative is always positive in this range (which is also clear from plotting it).  We have so again it suffices to show that (and, yet again, this is clear from plotting it).  I've jumped down the rabbit hole of taking the derivative of as well (since it is at so it would again suffice to show that ) and it doesn't yield anything immediately useful for me.  Please help if you can","f(x) = \frac{1}{\arctan(x)} - \frac{1}{x}  (0, \infty) f'(x) = \frac{(1+x^2)\arctan^2(x) -x^2}{x^2(1+x^2)\arctan^2(x)} g(x) \equiv (1+x^2)\arctan^2(x) -x^2 \ge 0 \quad \forall x >0 g 0 x = 0 g' \ge 0","['calculus', 'derivatives', 'inequality']"
43,Why does logarithmic differentiation work even if logs are not defined for negative numbers?,Why does logarithmic differentiation work even if logs are not defined for negative numbers?,,"Say for instance, we have a function $y = x^3$, the derivative of which is $\frac{\mathrm{d}y}{\mathrm{d}x} = 3x^2$. Now, say I want to do a logarithmic differentiation: $$ \ln(y) = \ln(x^3) \hspace{10pt}\implies\hspace{10pt} \ln(y) = 3\ln(x), $$ so by implicit differentiation with respect to $x$, $$ \frac{1}{y}\frac{\mathrm{d}y}{\mathrm{d}x} = 3\cdot\frac{1}{x} \implies \frac{\mathrm{d}y}{\mathrm{d}x} = y\cdot 3 \cdot \frac{1}{x} = x^3\cdot \frac{3}{x} = 3 x^2.  $$ Why does this work, even though $\ln(x)$ function is not defined for negative values of $x$? Or does doing differentiation using logarithms limit the result to positive values for $x$ only which in this case is yielding the correct result nonetheless? For some reason, the authors of articles I found regarding the same, don't seem to be too concerned regarding this  problem. Any help would be appreciated, Thankyou.","Say for instance, we have a function $y = x^3$, the derivative of which is $\frac{\mathrm{d}y}{\mathrm{d}x} = 3x^2$. Now, say I want to do a logarithmic differentiation: $$ \ln(y) = \ln(x^3) \hspace{10pt}\implies\hspace{10pt} \ln(y) = 3\ln(x), $$ so by implicit differentiation with respect to $x$, $$ \frac{1}{y}\frac{\mathrm{d}y}{\mathrm{d}x} = 3\cdot\frac{1}{x} \implies \frac{\mathrm{d}y}{\mathrm{d}x} = y\cdot 3 \cdot \frac{1}{x} = x^3\cdot \frac{3}{x} = 3 x^2.  $$ Why does this work, even though $\ln(x)$ function is not defined for negative values of $x$? Or does doing differentiation using logarithms limit the result to positive values for $x$ only which in this case is yielding the correct result nonetheless? For some reason, the authors of articles I found regarding the same, don't seem to be too concerned regarding this  problem. Any help would be appreciated, Thankyou.",,"['calculus', 'derivatives', 'logarithms']"
44,Differentiate $2^x$ from first principles.,Differentiate  from first principles.,2^x,"I am trying to differentiate $2^x$ from first principles. This is what I have so far: \begin{align} f'(x) &= \underset{h\rightarrow 0}{\textrm{lim}} \frac{f(x+h) - f(x)}{h}\\ \implies \frac{d2^x}{dx} &= \underset{h\rightarrow 0}{\textrm{lim}}\frac{2^{x+h}-2^x}{h}\\ &= \underset{h\rightarrow 0}{\textrm{lim}}\frac{2^x(2^h -1)}{h} \end{align} From that point on, as the limit is of type 0/0, I was thinking of using L'Hôpital's rule, but this gives \begin{equation} \frac{d2^x}{dx} = 2^x\frac{d2^h}{dh}\bigg\rvert_{h=0}. \end{equation} Not sure how to go from there.","I am trying to differentiate $2^x$ from first principles. This is what I have so far: \begin{align} f'(x) &= \underset{h\rightarrow 0}{\textrm{lim}} \frac{f(x+h) - f(x)}{h}\\ \implies \frac{d2^x}{dx} &= \underset{h\rightarrow 0}{\textrm{lim}}\frac{2^{x+h}-2^x}{h}\\ &= \underset{h\rightarrow 0}{\textrm{lim}}\frac{2^x(2^h -1)}{h} \end{align} From that point on, as the limit is of type 0/0, I was thinking of using L'Hôpital's rule, but this gives \begin{equation} \frac{d2^x}{dx} = 2^x\frac{d2^h}{dh}\bigg\rvert_{h=0}. \end{equation} Not sure how to go from there.",,"['calculus', 'derivatives']"
45,Continuous function with non-negative derivative a.e. implies non-decreasing?,Continuous function with non-negative derivative a.e. implies non-decreasing?,,"Let $f \colon [a,b] \rightarrow \mathbb{R}$ be a continuous function on a compact interval of the real line. Suppose that $f$ is differentiable almost everywhere and that $f'(x) \geq 0$ at every point of differentiability. Is it true that $f$ is non-decreasing on [a,b]? (If $f$ is $\textit{absolutely}$ continuous, this is certainly true, but I'm not so sure what happens if you weaken the assumption to mere continuity.)","Let $f \colon [a,b] \rightarrow \mathbb{R}$ be a continuous function on a compact interval of the real line. Suppose that $f$ is differentiable almost everywhere and that $f'(x) \geq 0$ at every point of differentiability. Is it true that $f$ is non-decreasing on [a,b]? (If $f$ is $\textit{absolutely}$ continuous, this is certainly true, but I'm not so sure what happens if you weaken the assumption to mere continuity.)",,"['real-analysis', 'derivatives', 'continuity', 'monotone-functions']"
46,derivative of a function divided by the same function,derivative of a function divided by the same function,,I've been trying to understand and look for a proof that for example (1) $$\frac{\frac{d}{dx}f(x)}{f(x)}$$ is equal to (2) $$\frac{d}{dx}ln[f(x)]$$ Can someone help me understand why 1 & 2 are equal?,I've been trying to understand and look for a proof that for example (1) $$\frac{\frac{d}{dx}f(x)}{f(x)}$$ is equal to (2) $$\frac{d}{dx}ln[f(x)]$$ Can someone help me understand why 1 & 2 are equal?,,"['calculus', 'derivatives']"
47,Is $\dot u/\dot \phi$ is the same as $\mathrm du / \mathrm d\phi$?,Is  is the same as ?,\dot u/\dot \phi \mathrm du / \mathrm d\phi,"I have two functions $u$ and $\phi$ given. I am not sure what they depend on, but I think that it is a common variable $\tau$. So $u(\tau)$ and $\phi(\tau)$. Then $\dot u$ is the derivative of $u$ with respect to $\tau$. The derivation of a problem works if $$\frac{\dot u}{\dot \phi} = \frac{\mathrm du}{\mathrm d\phi}.$$ Is that legitimate? Within thermodynamics, where an equation of state like $f(u, \phi) = 0$ holds, I learned that I the reciprocal of a derivative is the derivative the other way around, allowing to “cancel chain rules”. I am not sure whether this would hold here as well. This is probably just a duplicate of how to calculate $\frac{d\dot{x}}{dx}$ ? I can just cancel the $\mathrm d\tau$?","I have two functions $u$ and $\phi$ given. I am not sure what they depend on, but I think that it is a common variable $\tau$. So $u(\tau)$ and $\phi(\tau)$. Then $\dot u$ is the derivative of $u$ with respect to $\tau$. The derivation of a problem works if $$\frac{\dot u}{\dot \phi} = \frac{\mathrm du}{\mathrm d\phi}.$$ Is that legitimate? Within thermodynamics, where an equation of state like $f(u, \phi) = 0$ holds, I learned that I the reciprocal of a derivative is the derivative the other way around, allowing to “cancel chain rules”. I am not sure whether this would hold here as well. This is probably just a duplicate of how to calculate $\frac{d\dot{x}}{dx}$ ? I can just cancel the $\mathrm d\tau$?",,['derivatives']
48,Rigorously prove that the tangent line is indeed tangent?,Rigorously prove that the tangent line is indeed tangent?,,"Let $f$ be a function of $x$ and $f'$ be the derivative, at a point $(x_0, f(x_0))$ the slope is $f'(x_0)$, we know from any calculus book that the line $g(x) = f(x_0) + f'(x_0)(x - x_0)$ is tangent to the curve of $f$,  but how do you rigorously prove it?","Let $f$ be a function of $x$ and $f'$ be the derivative, at a point $(x_0, f(x_0))$ the slope is $f'(x_0)$, we know from any calculus book that the line $g(x) = f(x_0) + f'(x_0)(x - x_0)$ is tangent to the curve of $f$,  but how do you rigorously prove it?",,"['calculus', 'derivatives']"
49,Performing a differentiation on a Fourier series,Performing a differentiation on a Fourier series,,"I'm tutoring a set of problem sheet to do with Fourier series and one problem is as follows: The Fourier series for a sawtooth wave is, $f(x)=x=-\sum^{\infty}_{n=1}\frac{2(-1)^n\sin(nx)}{n}$ for $-\pi < x<\pi$. If you differentiate this you get $1=-2\sum^{\infty}_{n=1}(-1)^n\cos(nx)$ again for $-\pi < x<\pi$ What is wrong with this? I have the solutions sheet and it says that it does not converge to 1 (fair enough, I plotted it to large $n$ and it sort of converges but oscillates between 0 and 2 in the interval) and then states ... An assumption has been made that you can interchange the order of summation    and differentiation in the result stated. It then goes to note that you can interchange the order of summation and integration I don't understand the argument, can anyone shed some light on this?","I'm tutoring a set of problem sheet to do with Fourier series and one problem is as follows: The Fourier series for a sawtooth wave is, $f(x)=x=-\sum^{\infty}_{n=1}\frac{2(-1)^n\sin(nx)}{n}$ for $-\pi < x<\pi$. If you differentiate this you get $1=-2\sum^{\infty}_{n=1}(-1)^n\cos(nx)$ again for $-\pi < x<\pi$ What is wrong with this? I have the solutions sheet and it says that it does not converge to 1 (fair enough, I plotted it to large $n$ and it sort of converges but oscillates between 0 and 2 in the interval) and then states ... An assumption has been made that you can interchange the order of summation    and differentiation in the result stated. It then goes to note that you can interchange the order of summation and integration I don't understand the argument, can anyone shed some light on this?",,"['fourier-series', 'derivatives']"
50,Is the right-limit of left-derivatives equal to the right-derivative?,Is the right-limit of left-derivatives equal to the right-derivative?,,"Let's say $f$ is a convex function, so that $f_L'$ and $f_R'$ exist everywhere. I was wondering if the following is true: $$\lim_{t\to a^+}f_L'(t)=f_R'(a).$$ We know that $f'_L(t)$ decreases as $t\to a^+$ , so $$\lim_{t\to a^+}f'_L(t)=\inf_{t>a}f_L'(t).$$ On the other hand, convexity implies $$f_R'(a)\leq f_L'(t)\qquad\text{for all}\qquad t>a.$$ As such $$f_R'(a)\leq\lim_{t\to a^+}f_L'(t).$$ Should this also be an equality?","Let's say is a convex function, so that and exist everywhere. I was wondering if the following is true: We know that decreases as , so On the other hand, convexity implies As such Should this also be an equality?",f f_L' f_R' \lim_{t\to a^+}f_L'(t)=f_R'(a). f'_L(t) t\to a^+ \lim_{t\to a^+}f'_L(t)=\inf_{t>a}f_L'(t). f_R'(a)\leq f_L'(t)\qquad\text{for all}\qquad t>a. f_R'(a)\leq\lim_{t\to a^+}f_L'(t).,"['real-analysis', 'derivatives', 'convex-analysis']"
51,How to determine a derivative if the derivative is dependent itself?,How to determine a derivative if the derivative is dependent itself?,,"Let's suppose I've got a function $f(x)$ where I'd like to differentiate with respect to $t$ , but $t$ depends on $x$ : $t(x)$ . Thus the whole linked derivative thing: $\dfrac{\mathrm{d}f(x)}{\mathrm{d}t(x)}$ . Is this possible at all? Alternatively I had to find $t^{-1}(x)$ : $x(t)$ and then calculate the derivative fairly easy: $\mathrm{d} f(x(t))/\mathrm{d}t$ . But finding the inverse is not always possible. Is there another way? At all?","Let's suppose I've got a function where I'd like to differentiate with respect to , but depends on : . Thus the whole linked derivative thing: . Is this possible at all? Alternatively I had to find : and then calculate the derivative fairly easy: . But finding the inverse is not always possible. Is there another way? At all?",f(x) t t x t(x) \dfrac{\mathrm{d}f(x)}{\mathrm{d}t(x)} t^{-1}(x) x(t) \mathrm{d} f(x(t))/\mathrm{d}t,['derivatives']
52,"Find the derivative of $h(x) = \min_{u \in [x,b]} f(u)$",Find the derivative of,"h(x) = \min_{u \in [x,b]} f(u)","Let $f:\mathbb{R}\rightarrow \mathbb{R}$ be a differentiable function. And let $b\in \mathbb{R}$ be a fixed number, consider the function $h:(-\infty,b]\rightarrow \mathbb{R} $ given by \begin{equation}  h(x) = \min_{u \in [x,b]} f(u) \end{equation} I want to find $h'(x)$ . Following the definition \begin{equation} h'(x) = \lim_{\epsilon \to 0} \frac{h(x+\epsilon)-h(x)}{\epsilon} \end{equation} then \begin{equation} h'(x) = \lim_{ \epsilon \to 0} \frac{\min_{u \in [x+\epsilon,b]} f(u)-\min_{u \in [x,b]} f(u)}{\epsilon} \end{equation} But at this point I don't know how to continue. Do you think is possible to find $h'(x)$ without knowing an expression for $f(u)$ ? or Do you know how to find $h'(x)$ using other technique? Thanks in advance.","Let be a differentiable function. And let be a fixed number, consider the function given by I want to find . Following the definition then But at this point I don't know how to continue. Do you think is possible to find without knowing an expression for ? or Do you know how to find using other technique? Thanks in advance.","f:\mathbb{R}\rightarrow \mathbb{R} b\in \mathbb{R} h:(-\infty,b]\rightarrow \mathbb{R}  \begin{equation} 
h(x) = \min_{u \in [x,b]} f(u) \end{equation} h'(x) \begin{equation}
h'(x) = \lim_{\epsilon \to 0} \frac{h(x+\epsilon)-h(x)}{\epsilon}
\end{equation} \begin{equation}
h'(x) = \lim_{ \epsilon \to 0} \frac{\min_{u \in [x+\epsilon,b]} f(u)-\min_{u \in [x,b]} f(u)}{\epsilon}
\end{equation} h'(x) f(u) h'(x)","['calculus', 'derivatives']"
53,Isn't my book using logarithm wrongly to differentiate?,Isn't my book using logarithm wrongly to differentiate?,,"Problem: Differentiate with respect to $x$ : $\frac{e^{-3x}(3x+5)}{7x-1}$ My book's attempt: Let, $$y=\frac{e^{-3x}(3x+5)}{7x-1}$$ $$\ln(y)=\ln(\frac{e^{-3x}(3x+5)}{7x-1})\tag{1}$$ $$\text{rest of the math...}$$ Question: Isn't taking $\ln()$ unfounded in $(1)$ : $\frac{e^{-3x}(3x+5)}{7x-1}$ could a negative number? Don't we have to just use the product and quotient rule to find the derivative here?","Problem: Differentiate with respect to : My book's attempt: Let, Question: Isn't taking unfounded in : could a negative number? Don't we have to just use the product and quotient rule to find the derivative here?",x \frac{e^{-3x}(3x+5)}{7x-1} y=\frac{e^{-3x}(3x+5)}{7x-1} \ln(y)=\ln(\frac{e^{-3x}(3x+5)}{7x-1})\tag{1} \text{rest of the math...} \ln() (1) \frac{e^{-3x}(3x+5)}{7x-1},['calculus']
54,"Are there ""tight"" (as possible) upper bounds for $\max_t{\left\{ \left|\frac{d f(t)}{dt}\right|\right\}}$ for time-limited functions???","Are there ""tight"" (as possible) upper bounds for  for time-limited functions???",\max_t{\left\{ \left|\frac{d f(t)}{dt}\right|\right\}},"The main objective is to find some upper bound for the maximum rate of change as ""tight"" as possible, hopefully related to characteristics of the functions as its signal energy, or its higher Fourier coefficient, or something easy to obtain from the function itself. Also, figure out what is needed for a finite-energy time-limited signal (so, with infinite bandwidth), to have a bounded rate of change $\max_t |\frac{df(t)}{dt}|<\infty$ . For some partial answers you can go directly to my 2nd answer here Following the notation of exercise 4.49 of the book ""Signals and Systems, 2nd Edition"" (Alan V. Oppenheim, Alan S. Willsky, with S. Hamid) [ 1 ], the Fourier Transform is defined as $F(j \omega) = \int_{-\infty}^{\infty} f(t) e^{-j \omega t} dt$ , so the function can be described as $f(t) = \frac{1}{2 \pi} \int_{-\infty}^{\infty} F(j \omega) e^{j \omega t} d\omega$ , where $j = \sqrt{-1}$ . Let $f(t)$ being a function which fulfill the conditions to have a Fourier Transform $F(j \omega)$ . Then using the composition of a complex number in their amplitude and phase, and the “triangle inequality”, I can establish the following (here $|\cdot |$ is the absolute value): $$ \begin{equation}\begin{split} M = \max_{t}\left\{\left|{\frac{d f(t)}{dt}}\right|\right\} & = \max_{t}\left\{ \left|{\frac{1}{2\pi}} \int_{-\infty}^{\infty}j\omega F(j\omega)e^{j \omega t} d\omega \right|\right\} \texttt{     (Eq. 1)} \\ & =\max_{t}\left\{ \left|{\frac{1}{2\pi}} \int_{-\infty}^{\infty}\left|j\omega F(j\omega)\right| e^{j\sphericalangle\left(j\omega F(j\omega )\right)} e^{j \omega t} d\omega \right| \right\} \\ & \leq \max_{t}\left\{ {\frac{1}{2\pi}} \int_{-\infty}^{\infty}\left| \left|j\omega F(j\omega)\right| e^{j\sphericalangle\left(j\omega F(j\omega )\right)} e^{j \omega t}\right| d\omega \right\} \\ & \leq \max_{t}\left\{ {\frac{1}{2\pi}} \int_{-\infty}^{\infty}\left|j \right|\left|\omega F(j\omega)\right|\left| e^{j\sphericalangle\left(j\omega F(j\omega )\right)}\right| \left| e^{j \omega t}\right| d\omega \right\} \\ & = {\frac{1}{2\pi}} \int_{-\infty}^{\infty}\left|\omega F(j\omega)\right| d\omega \texttt{     (Eq. 2)}\end{split}\end{equation}$$ since $1 = |e^{j\phi}|, \forall \phi \in \mathbb{R}$ (note that $j = e^{j\frac{\pi}{2}}, \omega t \in \mathbb{R},$ and angle $ \sphericalangle\left(j\omega F(j\omega )\right) \in \mathbb{R}$ ), and the remaining integral is independent of $t$ . In the book ""Fourier Series: A Modern Introduction - Volume 1 (2nd Edition)"" (R. E. Edwards) [ 2 ], on Chapter 2 point 2.3.6 (point (3) of the ""Remarks"") is proved that if $f(t)$ is of bounded variation, and $\hat{f}(n)$ is its Fourier transform (defined differently from here), then: $$ \left|{n \hat{f}(n)}\right| \leq V(f), \texttt{     (Eq. 5)}$$ with $V(f)$ the total variation of $f(t)$ . Since the total variation for a Riemann integrable function $f(t)$ with $M < \infty$ can be defined as: $$ V(f) = \int_{-\infty}^{\infty} \left| \frac{df(t)}{dt} \right| dt $$ I believe that the bound $\max_{t}\left\{\left|{\frac{d f(t)}{dt}}\right|\right\} \leq V(f)$ is going to be ""too loose"", because is the same situation as considering the sum of a series of positive coefficients $a_n \geq 0$ and asking to compare $\max_{n}\{a_n\} \leq \sum_{-\infty}^{\infty}a_n$ (!) . (!): Caution is needed, because the sum is under an integral, so the "" $dt$ "" could change the intuition. As example, consider the ramp function that starts at the origin and ends at the point $(1/2,\pi)$ , its max value is $\pi$ but the sum of the area under the curve is $\sqrt{\pi/2}/2 < \pi$ , but on converse, if the edge is on $(2,\pi)$ it’s area under the curve will be $2 \sqrt{\pi} > \pi$ ). So I want to know: A. Is the bound ${\frac{1}{2\pi}} \int_{-\infty}^{\infty}\left|\omega F(j\omega)\right| d\omega$ tight ""enough""? or It will be a ""loose one"" as $V(f)$ ? Here I know that: $$ \frac{df(t)}{dt}\Big\vert_{t=0} = {\frac{1}{2\pi}} \int_{-\infty}^{\infty} j\omega F(j\omega) d\omega $$ by making $t=0$ in the exponent $e^{j \omega t} = e^0 = 1$ of the inverse Fourier transform definition, so I have hope is not ""too loose"". For avoiding any kind of ""strange behavior"" like continuous functions nowhere differentiable or differentiable functions nowhere continuous, and the full zoo of functions in between, please consider that the functions $f(t)$ is as following: $$f(t) = x(t) \cdot (\theta(t-t_0) - \theta(t-t_F))$$ is a non-constant one-variable real-valued function defined for every $t \in (-\infty; \infty)$ with $\theta(t)$ the unitary step function and $t_0<t_F$ , so $f(t)$ haves a beginning at $t_0$ and an end at $t_F$ , being $f(t) = 0 \text{ if } t\leq t_0 \text{ or } t_F \geq t$ , letting the property $\mathbb{F}\left\{\frac{df(t)}{dt}\right\} = j\omega F(j \omega)$ being true. Let $f(t)$ be a Lebesgue integrable function: $\int_{-\infty}^{\infty}|f(t)|dt < \infty$ , and also a finite energy function: $\int_{-\infty}^{\infty}|f(t)|^2dt < \infty$ . If needed, also Riemann integrable. Consider that function $x(t)$ is continuous, also smooth so all derivatives exists and are bounded (or at least, one time differentiable), so using $\max$ or $\sup$ or else is equivalent (same for $f(t)$ except at the points $f(t_0)$ and $f(t_F)$ ), and also that $f(t)$ is of bounded variation. In the same mentioned point of [2], at ""Remarks"" section, is said that Wiener have proved that for a bounded function to be continuous if and only if it behave as: $$ \lim_{N \to \infty} \frac{1}{N} \sum_{|n| \leq N} \left|n \hat{f}(n) \right| = 0 \texttt{     (Eq. 6)}$$ Also assume that the function $f(t)$ follows the Rieman-Lebesgue Lema [ 3 ], and the conditions needed to have a Fourier transform $F(jw)$ described by a function -not by a distribution as Dirac's delta $\delta(t)$ or others- so that the Paley–Wiener theorem is fulfilled [ 4 ]. I would like to represent ""naively"" physically possible time-limited phenomena with $f(t)$ , so I don't want in principle, to put restrictions to $f(t_0)$ or $f(t_F)$ , but if needed, first start with $f(t_F)=0$ , and as last resource, add $f(t_0)=0$ , making $f(t)$ compact-supported but not necessarily $f(t) \in C_c^{\infty}$ , since to be a Bump function it also requires that $\lim_{t->\partial t} \frac{d^n f(t)}{dt^n} = 0$ so every derivative is continuous at the boundaries - and just if nothing else is possible, let $f(t)$ be a Bump function $f(t) \in C_c^{\infty}$ . I don't know if there exists a space of non-analytic $C_c^{\infty}$ -alike functions that can have $f(t_0)\neq 0$ and/or $f(t_F)\neq 0$ or both, if exists, please let me know how is called and any reference to search for them (I left it as a separated question in here ). Since $\frac{df(t)}{dt} = \frac{dx(t)}{dt}\cdot (\theta(t-t_0) - \theta(t-t_F)) + x(t)\cdot\delta(t-t_0) -  x(t)\cdot\delta(t-t_F)$ , when looking for $\max_t |\cdot|$ , it will be ""infinite"" because $\delta(t) = \infty$ at $t=0$ . Because of this, I am explicitly avoiding the discontinuity at the edges, so $t_0 < t < t_F$ could let me work with $\frac{df(t)}{dt} = \frac{dx(t)}{dt}\cdot (\theta(t-t_0) - \theta(t-t_F))$ , so $\max_{t_0 < t < t_F} |\frac{df(t)}{dt}| = \max_{t_0 < t < t_F} |\frac{dx(t)}{dt}|$ . If the bound is applicable for more general functions, please let me know which constraints you have removed. B) What other tight bounds for $\max_{t}\left\{\left|{\frac{d f(t)}{dt}}\right|\right\}$ are known?? Since using the same argument of the main equations I will have that $\max_{t}\left\{\left|{f(t)}\right|\right\} \leq {\frac{1}{2\pi}} \int_{-\infty}^{\infty}\left|F(j\omega)\right| d\omega \leq \int_{-\infty}^{\infty}\left|f(t)\right| dt \textit{  ¿}\leq\textit{?} \sqrt{\int_{-\infty}^{\infty}\left|f(t)\right|^2 dt} = \sqrt{E_0}$ , so I am trying to find something proportional somehow to the energy $E_0 = \int_{-\infty}^{\infty}\left|f(t)\right|^2 dt$ of the function $f(t)$ , even tried to multiply by $\frac{E_0}{E_0}$ to form things of the fashion of $\int_{-\infty}^{\infty}\frac{\left|F(j \omega)\right|^2}{E_0} d\omega = 1$ so $g(\omega)= \frac{\left|F(j \omega)\right|^2}{E_0}$ could be think as a probability distribution and use bounds for the expected value $E_g[\omega]$ and $E_g[\omega^2]$ with unsuccessful results. I have found on internet some bounds as the Kalman-Rota or the Landau-Kolmogorov-Hadamard inequalities that states that $||f'||_2 \leq \sqrt{2} ||f||_2^{1/2}||f''||_2^{1/2}$ [ 5 ], but my intuition says that commonly $\max_{t}|f'| \ll \max_{t}|f''|$ . I also found other inequalities like Poincare's, Sobolev's, Friedrichs's, or Uncertainty Principle relations, but the inequality goes on the other direction $\textit{something}(f) \leq \sup |f'|$ . On the comments were mentioned some bounds applicable to band-limited functions (Bernstein inequality [ 11 ], Markov brothers' inequality [ 12 ], Others [ 13 ], follow the main article [ 14 ]), but since here I am asking about time-limited functions , which are going to have unbounded domain on the frequencies [ 10 ], I believe they are not applicable. C) What restrictions have to fulfill $f(t)$ so it happen to be true that $\max_{t}\left\{\left|{\frac{d f(t)}{dt}}\right|\right\} \leq \max_{\omega}\left\{\left|\omega \cdot F(j \omega)\right|\right\}$ ???? I have tried with a few functions and it happen to be true, so if you know of any demonstration related please share any reference. I also found counterexamples for finite-energy time-limited, so I want to know which conditions must happen to make it “useful”. This bound ""conjecture"" come from the following mistake: $$ \begin{equation}\begin{split} M = \max_{t}\left\{\left|{\frac{d f(t)}{dt}}\right|\right\} & = \max_{t}\left\{ \left|{\frac{1}{2\pi}} \int_{-\infty}^{\infty}j\omega F(j\omega)e^{j \omega t} d\omega \right|\right\} \texttt{     (Eq. 1)} \\ & =\max_{t}\left\{ \left|{\frac{1}{2\pi}} \int_{-\infty}^{\infty}\left|j\omega F(j\omega)\right| e^{j\sphericalangle\left(j\omega F(j\omega )\right)} e^{j \omega t} d\omega \right| \right\} \end{split}\end{equation}$$ Where, if I let $M_\omega^* = \max_\omega |j\omega F(j \omega)|$ which happens at $\omega^* = \arg \max |j\omega F(j \omega)|$ , then at this I will have that $e^{j\sphericalangle\left(j\omega F(j\omega )\right)} = e^{j \phi^*}$ for some $\phi^* \in \mathbb{R}$ , so: $$ \begin{equation}\begin{split} M = \max_{t}\left\{\left|{\frac{d f(t)}{dt}}\right|\right\} & \leq? \max_{t}\left\{ \left|{\frac{M_\omega^* \cdot e^{j \phi^*}}{2\pi}} \int_{-\infty}^{\infty} e^{j \omega t} d\omega \right|\right\} \texttt{     (Eq. 3)} \\ & \leq \max_{t}\left\{ M_\omega^* |e^{j \phi^*}|\left|{\frac{1}{2\pi}} \int_{-\infty}^{\infty}e^{j \omega t} d\omega \right| \right\} \\ & = M_\omega^* \max_{t}\left\{\left|\delta(t) \right| \right\} \\ &  = \max_\omega |j\omega F(j \omega)| \max_{t}\left\{\left|\delta(t) \right| \right\} \texttt{     (Eq. 4)}\end{split}\end{equation}$$ In Eq. 4 the result will be $0$ for $t \neq 0$ , and "" $\infty$ "" if $t=0$ , so certainly is not a rightfully obtained bound, but ignoring somehow the delta function makes me wonder about the value of $$M_\omega^* = \max_\omega |j\omega F(j \omega)| \texttt{     (Eq. 7)}$$ . Added later: Following a suggestion, I am going to review some examples. (!!) I solved these examples using Wolfram-Alpha website [ 7 ], which works by default with a different definition of the Fourier transform, so be careful about it. I don't review if the results are ""theoretically"" right, and I have already found examples where Wolfram-Alpha gives numerically wrong results. Gaussian function example_______________________ First one, the case of the Gaussian function: Let $f(t)$ a less restricted function that is not time-limited, but vanishes at infinity, $f(t)=\frac{1}{\sqrt{2\pi}} e^{-\frac{t^2}{2}}$ the standard Gaussian function distribution so $\int_{-\infty}^{\infty} f(t) dt = \int_{-\infty}^{\infty} |f(t)| dt = 1$ and its signal energy is finite $\int_{-\infty}^{\infty} |f(t)|^2 dt = \frac{1}{2\sqrt{\pi}} \approx 0.282 \ll \infty$ . Also, following “our” notation, the non-unitary Fourier Transform for the angular frequency $\omega$ of $\mathbb{F_t}\left\{ e^{-a t^2}\right\}(\omega) = \sqrt{\frac{\pi}{a}} e^{\frac{-\omega^2}{4a}}\,$ , so for this $f(t)$ we have $F(j\omega)=e^{-\frac{\omega^2}{2}}$ (the property that the Fourier Transform of Gaussians is also Gaussian - unbound domain in time and frequency, vanishing at infinity in both). Then, the following is true: $$ \begin{equation}\begin{split} M_{std.gauss} & = & \max_{t}\left\{\left|{\frac{d f(t)}{dt}}\right|\right\} = \frac{1}{\sqrt{2\pi e}}\text{ on } t^* = \pm 1 & \approx 0.24197 \\ & \leq & {\frac{1}{2\pi}} \int_{-\infty}^{\infty}\left|\omega F(j\omega)\right| d\omega  = \frac{1}{\pi}  & \approx 0.31831 \\ & \leq & \max_{\omega}\left\{\left|{\omega F(j\omega)}\right|\right\}  = \frac{1}{\sqrt{e}} \text{ on } \omega^* = \pm 1  & \approx 0.60653 \\ & \leq & V(f) = \int_{-\infty}^{\infty} \left| \frac{df(t)}{dt} \right| dt  = \sqrt{\frac{2}{\pi}} & \approx 0.79788 \end{split}\end{equation}$$ Is interesting to note that both bounds of Eq. 2 and Eq. 7  have worked better than $V(f)$ . Also note that the bound Eq. 2 is “much tighter” than the bound of Eq. 7, so it’s happened what was commented on (!) . Is interesting to think that the Gaussian is the function that maximizes the Uncertainty Principle, so no other one-variable function with the same energy (since is a one parameter function), can be more concentrated in both time and frequency domains at once ([ 9 ] and [ 10 ]), so my intuition says that time-limited functions (which can be thought as convolution on the frequency domain of a standard function with a ""sinc function""), are going to be even more spread in the frequencies, so it is going to be less likely to found a higher peak for $|\omega F(j\omega)|$ that the ones is achieved by the Gaussian function with the same energy. But anyway, in this example it can be seen that this ""conjecture"" is not a total nonsense. Classic functions examples_____________________ Here I review the simplest cases of traditional functions which its Fourier transforms are tabulated in [1] and in Wikipedia [4]. Knowing beforehand they don't fulfill my requirements, it will be a logic start I think, since many people had worked with them before. The following notation is used from now on: "" $E°$ "" is used for each signal energy (definition on each table), $\Pi(t) = 1, |t|\leq \frac{1}{2}$ is the standard rectangular function ( Unitbox(t) in Wolphram-Alpha ), $\delta(t)$ is the Dirac's delta distribution ( Diracdelta(t) in Wolphram-Alpha ), $\theta(t)=1, t \geq 0$ is the standard step function ( Unitstep(t) in Wolphram-Alpha ), $\Lambda (t)=1-|t|, |t|\leq 1$ is the standard triangular function ( Unittriangle(t) in Wolphram-Alpha ), $H_1(t)=2\cdot t\,$ is the Hermite polynomial of the first kind, which fulfill the equation $H_n(t) = (-1)^n\,e^{t^2}\frac{d^n}{dt^n}(e^{-t^2})$ ( HermiteH(1,t) in Wolphram-Alpha ), and $\mathscr{C}=0.91596559\cdots\,$ is the Catalan's constant. I have let with (*) the results I believe have questionable accuracy. $$  \begin{array}{|c:c|c:c|c|c:c:c|c:c:c:c|}  \hline  f(t) & \text{dom}(f(t)) & F(j\omega)=\mathbb{F}\{f(t)\}(\omega) & \text{dom}(F(j\omega)) & \max_t |f'(t)| & \frac{1}{2 \pi} \int_{-\infty}^{\infty} |j\omega F(j\omega)|d\omega & \max_\omega |j\omega F(j \omega)| & V(f) = \int_{-\infty}^{\infty} |f'(t)|dt & \max_t |f(t)| & ||f||_1 = \int_{-\infty}^{\infty} |f(t)|dt & E° = \int_{-\infty}^{\infty} |f(t)|^2 dt & ||f||_2 = \sqrt{E°} \\  \hline \Pi (t) &  [-\frac{1}{2}; \frac{1}{2}] & \text{sinc}(\frac{\omega}{2}) & (-\infty; \infty) & \infty^* & \infty^* & 2 & 0^* & 1 & 1 & 1 & 1 \\ \hdashline \text{sinc}(\frac{t}{2}) &  (-\infty; \infty) & 2\pi \cdot \Pi (\omega) & [-\frac{1}{2}; \frac{1}{2}] & 0.218 & \frac{1}{4} = 0.25 & \pi = 3.1416 & \infty & 1 & \infty & 2\pi = 6.2831 & 2.5066 \\ \hdashline \text{sinc}^2(\frac{t}{2}) &  (-\infty; \infty) & \Lambda (\omega) & [-1; 1] & 0.27 & \frac{1}{3} = 0.33 & \frac{\pi}{2} = 1.57079  & \infty & 1 & 2\pi = 6.2831 & \frac{4\pi}{3} = 4.18879 & 2.046 \\ \hdashline \Lambda (t) &  [-1; 1] & \text{sinc}^2(\frac{\omega}{2}) & (-\infty; \infty) & undefined & \infty & 1.44922 & 2^* & 1 & 1 & \frac{2}{3} = 0.66 & 0.816 \\ \hdashline e^{-t} \cdot \theta (t) &  [0; \infty) & \frac{1}{(1+j\omega)} & (-\infty; \infty) & 1^* & \infty & 1^* & 1^* & 1 & 1 & \frac{1}{2} = 0.5 & 0.707 \\ \hdashline t\cdot  e^{-t} \cdot \theta (t)&  [0; \infty) & \frac{1}{(1+j\omega)^2} & (-\infty; \infty) & 1^* & \infty & \frac{1}{2} = 0.5 & \frac{2}{e}^* = 0.735759^* & \frac{1}{e} = 0.3678 & 1 & \frac{1}{4} = 0.25 & \frac{1}{2} = 0.5 \\ \hdashline \frac{1}{\sqrt{2\pi}}e^{-\frac{t^2}{2}} &  (-\infty; \infty) & e^{-\frac{\omega^2}{2}} & (-\infty; \infty) & \frac{1}{\sqrt{2 e \pi}} = 0.24 & \frac{1}{\pi} =  0.31831 & \frac{1}{\sqrt{e}} = 0.606 & \sqrt{\frac{2}{\pi}} = 0.797 & \frac{1}{\sqrt{2\pi}} = 0.39 & 1 & \frac{1}{2\sqrt{\pi}} = 0.282 & 0.531 \\ \hdashline \frac{1}{\sqrt{\pi}}e^{-j\frac{t^2}{2}} &  (-\infty; \infty) & (1-j)\cdot e^{j\frac{\omega^2}{2}} & (-\infty; \infty) & \infty & \infty & \infty & \infty & \frac{1}{\sqrt{\pi}} = 0.564 & \infty & \infty & \infty \\ \hdashline e^{-|t|} &  (-\infty; \infty) & \frac{2}{(1+\omega^2)} & (-\infty; \infty) & 1^* & \infty & 1 & \infty^* & 1 & 2 & 1 & 1 \\ \hdashline e^{-\frac{t^2}{2}}H_1(t) &  (-\infty; \infty) & -j\omega \cdot 2\sqrt{2\pi}\cdot e^{-\frac{\omega^2}{2}} & (-\infty; \infty) & 2 & 2 & \frac{4\sqrt{2\pi}}{e} = 3.68 & \frac{8}{\sqrt{e}} = 4.8522 & \frac{2}{\sqrt{e}} = 1.213 & 4 & 2\sqrt{\pi} = 3.5449 & 1.883 \\ \hdashline \text{sech}(t) & (-\infty; \infty) & \pi \cdot \text{sech}(\frac{\pi \omega}{2}) & (-\infty; \infty) & \frac{1}{2} = 0.5 & \frac{8 \mathscr{C}}{\pi^2} = 0.74245 & 1.32549 & 2 & 1 & \pi = 3.1416 & 2 & 1.414 \\ \hline \end{array} $$ Unfortunately, the upper bound of Eq. 2 diverges on many examples (I wasn´t expecting it to work for these examples anyway), but for the cases where it results to be finite, it shows to be much better than the bound given by Eq. 5. On the other hand, the upper bound of Eq. 7 results to be finite on much more examples, and also lower than the bound of Eq. 5, but much closer to it. Also, for the example $f(t) = t\,e^{-t}\theta(t)$ it shows to be lower than the maximum rate of change. This is why I am asking for which conditions this bound have to fulfill to become a valid upper bound (I don't expect that this bound is going to be valid for every possible function, but it could be an alternative when the bound of Eq. 2 doesn't converges). Also note that for the same case of $f(t) = t\,e^{-t}\,\theta(t)$ the maximum rate of change results to be higher than the Total Variation $V(f)$ , going against my intuition at least, so maybe the validity of the bound of Eq. 5 is not universal (could be related to the Fourier transform definition used in [2] being different from the one I am using here, or also a numerical issue). Bounded domain functions examples________________ Here I review some simple cases of time limited signals. I have tried to find simple cases of continuous functions: signals that starts from $0$ and rises/decline “slowly” ( $f(t)=f’(t)=0$ ), others that starts “sharply”, other that starts from a point different than $0$ , odd and even signals, signals with a discontinuity, the same signal with different compact domain, positive signals, signals with flat-top, etc. (some signals which I could found a simple Fourier transform to work with). Unfortunately there is no case of a proper “smooth function” since I don’t found any simple “bump function” $\in C_c^\infty$ with a simple Fourier transform, both in “closed form” (I left the question here , and already test all the functions of here on Wolfram-Alpha with negative results). I extend the previous table notation with these: $J_1(t)$ is the Bessel function of the first kind of order 1 ( BesselJ(1,t) in Wolphram-Alpha ), $\text{Si}(t)$ is the “Sine integral” function ( SinIntegral(t) in Wolphram-Alpha ) Also, since for time limited functions the domain is restricted to $t_0 \leq t \leq t_F$ , the definitions for the Fourier transform changes to $F(j \omega) = \int_{t_0 }^{t_F} f(t) e^{-j \omega t} dt$ , and also all the time domain integrals change its integration limits correspondingly (see headers of the table 2). Be attentive of this, since the Fourier transform of the functions are different with other integration limits. Also related, as explained in point (5) for avoiding the problem at the domain ""edges"", I use $\max_{t_0 < t < t_F} |f'(t)|$ without including the boundaries. Again, I have let with (*) the results I believe have questionable accuracy. $$  \begin{array}{|c:c|c:c|c|c:c:c|c:c:c:c|}  \hline  f(t) & \text{dom}(f) = [a\,;\,b] & F(j\omega)=\mathbb{F}_{[a\,;\,b]}\{f(t)\}(\omega) & \text{dom}(F(j\omega)) & \max_{a < t < b} |f'(t)| & \frac{1}{2 \pi} \int_{-\infty}^{\infty} |j\omega F(j\omega)|d\omega & \max_\omega |j\omega F(j \omega)| & V_a^b(f) = \int_{a}^{b} |f'(t)|dt & \max_{a\leq t \leq b} |f(t)| & ||f||_1 = \int_{a}^{b} |f(t)|dt & E° = \int_{a}^{b} |f(t)|^2 dt & ||f||_2 = \sqrt{E°} \\  \hline \sqrt{1-t^2} & [-1; 1] & \pi \cdot \frac{J_1(\omega)}{\omega} & (-\infty; \infty) & \infty & \infty^* & 1.82798 & 2 & 1 & \frac{\pi}{2} = 1.57079 & \frac{4}{3} = 1.33 & 1.1547 \\ \hdashline \sin(\frac{t\pi}{2}) & [-1; 1] & -j\frac{8\,\omega\cos(\omega)}{(\pi^2-4\,\omega^2)} & (-\infty; \infty) & \frac{\pi}{2} = 1.57079 & \infty^* & 2.75144 & 2 & 1 & \frac{4}{\pi} =1.2732 & 1 & 1 \\ \hdashline \sin^2(\frac{t\pi}{2}) & [-1; 1] & \frac{(\pi^2-2\,\omega^2)\sin(\omega)}{(\pi^2\,\omega-\omega^3)} & (-\infty; \infty) & \frac{\pi}{2} = 1.57079 & \infty^* & 2.8929 & 2 & 1 & 1 & \frac{3}{4} = 0.75 & 0.866 \\ \hdashline \cos^2(\frac{t\pi}{2}) &  [0; 1] & j\frac{(\pi^2(1-e^{-j\omega})-2\,\omega^2)}{2\,\omega\,(\omega^2-\pi^2)} & (-\infty; \infty) & \frac{\pi}{2} = 1.57079 & \infty^* & 1.4562 & 1 & 1 & \frac{1}{2} = 0.5 & \frac{3}{8} = 0.375 & 0.612 \\ \hdashline \cos^2(\frac{t\pi}{2}) & [-1; 1] & \frac{\pi^2\sin(\omega)}{(\pi^2\omega-\omega^3)} & (-\infty; \infty) & \frac{\pi}{2} = 1.57079 & 2.28547^* & 1.63641 & 2 & 1 & 1 & \frac{3}{4} = 0.75 & 0.866 \\ \hdashline \frac{(1+\cos(t\pi))^2}{4} & [-1; 1] & \frac{3\,\pi^4\sin(\omega)}{(\omega^5-5\pi^2\omega^3+4\pi^4\omega)} & (-\infty; \infty) & \frac{3\sqrt{3}\pi}{8} = 2.0405 & 2.61265^* & 1.58242 & 2 & 1 & \frac{3}{4} = 0.75 & \frac{35}{64} = 0.546875 & 0.7395 \\ \hdashline \sin(\frac{t\pi}{2})\cos^2(\frac{t\pi}{2}) & [-1; 1] & j\frac{16\, \pi^2\, \omega \cos(\omega)}{(16\, \omega^4-40\, \pi^2 \omega^2+9\,\pi^4)} & (-\infty; \infty) & 1.5708 & 1.93647^* & 1.23244 & 1.5396^* & \frac{2}{3\sqrt{3}} = 0.3849 & \frac{4}{3\sqrt{\pi}} = 0.42441 & \frac{1}{8} = 0.125 & 0.354 \\ \hdashline \text{sinc}(t\pi)\cos(\frac{t\pi}{2}) & [-1; 1] & \frac{1}{2\pi}\left(\text{Si}(\frac{\pi}{2}-\omega)+\text{Si}(\frac{3\pi}{2}-\omega)+\text{Si}(\frac{\pi}{2}+\omega)+\text{Si}(\frac{3\pi}{2}+\omega)\right) & (-\infty; \infty) & 1.62897 & \infty^* & 1.61724 & 2 & 1 & \frac{2\,\text{Si}(\pi)}{8} = 1.17898 & \frac{2\,\text{Si}(2\pi)}{\pi} = 0.9028 & 0.9501 \\ \hdashline 1-\sin^4(\frac{t\pi}{2}) & [-1; 1] & \frac{\pi^2(5\pi^2 - 2\,\omega^2) \sin(\omega)}{(\omega^5 - 5\pi^2\omega^3 + 4\pi^4\omega)} & (-\infty; \infty) & \frac{3\sqrt{3}\pi}{8} = 2.0405 & 3.01547^* & 1.8225 & 2 & 1 & \frac{5}{4} = 1.25 & \frac{67}{64} =  1.04688 & 1.023 \\ \hdashline \sin(|t|\pi) & [-1; 1] & \frac{2\pi\,(1+\cos(\omega))}{(\pi^2-\omega^2)} & (-\infty; \infty) & \pi = 3.1416 & \infty^* & 2.90769 & 4^* & 1 & \frac{4}{\pi} =1.2732 & 1 & 1 \\ \hline \end{array} $$ Against my expectation, the time limited functions behave much worst I intended, but because they are following my intuition: it decays slowly than the Gaussian function, but so slowly that the integral bound of Eq. 2 diverges in many cases, and also so spread, that the peak given by the bound of Eq. 7 is lower than the maximum rate of change. In many cases, the maximum rate of change results higher than the Total Variation $V_a^b(f)$ , against Eq. 5. The positive side, when the integral of Eq. 2 converges, it results to be a proper bound of the maximum rate of change, even when it was higher than the Total Variation. Unfortunately, all the results for the bound of Eq. 2 have “*” since its validity is questionable, and its value were obtained by numerical approximation through Nintegrate in Wolfram-Alpha, so I will review another bound to ""double check"". Using Hölder’s inequality, it can be stated that for two functions $f(t)$ and $g(t)$ the following is true: $$\int_{-\infty}^\infty |f(t) \cdot g(t)|\,dt \leq \int_{-\infty}^\infty |f(t)|\,dt \cdot \sup_t |g(t)|$$ I am going to applied it to the bound of Eq. 2 to avoid the multiplication: $$\frac{1}{2 \pi} \int_{-\infty}^\infty |\omega \, g(\omega) \cdot \frac{F(j \omega)}{g(\omega)}|\,d\omega \leq \frac{1}{2 \pi} \int_{-\infty}^\infty |\omega\, g(\omega)|\,d\omega \cdot \sup_\omega |\frac{F(j \omega)}{g(\omega)}| \texttt{     (Eq. 8)}$$ So, I tested some functions $g(\omega)$ that makes the integral of the right side converge, and then see if they have a supremum when tested against the function $f(t) = \cos^2(\frac{t\pi}{2}),\,|t| \leq 1\,$ : $$\begin{array}{|c|c:c|} \hline  g(\omega) & \int_{-\infty}^\infty |\omega\, g(\omega)|\,d\omega &  \texttt{(Eq. 8)} \\ \hline  e^{-\sqrt{|w|}} & 24 & 12.1623 \\ \hline  \end{array} $$ Here is really interesting that having a ""clear result"" for $g(\omega) = e^{-\sqrt{|w|}}$ proves that the results of table 2, maybe numerical inaccurate, are still valid (at list for the chosen $f(t)$ ), so there exists time-limited functions for which the integral of Eq. 2 converges. But not only this, also it proves something maybe obvious: There exists TIME-LIMITED functions (with unbound domain on the frequencies), which have a maximum rate of change bounded , (for functions with bounded domain in the frequencies, the results is known and presented in [14]) but better, seen the same problem for a different “not so obvious angle”, I believe it could be reinterpreted as: There exists some “ $\text{mysterious conditions}\,\mathbb{X}$ ” that makes that some time-limited signals with unlimited bandwidth will have a bounded maximum rate of change , conditions I am trying to figure out . Additionally, there other issue I found reviewing the results of table 2: for every function a tried with values at the “edges” of its domain different from zero, the integral of Eq. 2 diverges. This is confusing for me, especially for the functions $f(t) = \cos^2(\frac{t\pi}{2})$ when changing the domain from $[-1,\,1]$ to $[0,\,1]$ : technically there is no differences in the achieved maximum slopes of the curves, however for the reduced domain one, the integral of Eq. 2 diverges. I think it could be related to the phenomena I am avoiding according point (5), but somehow it manifests on the frequency domain. I tried to solve it by changing the function to $f(t) = \theta(-t)+\cos^2(\frac{t\pi}{2}),\, 0 \leq t \leq 1\,$ so no disruptive changes happen to the function, but as I was expecting its Fourier Transform in $[0,\,1]$ was the same to the previous functions (it was adding just a zero-measure point, so it doesn’t change the integral). I have tried many other different functions and combinations on $[-1,\,1]$ with $f(-1) \neq 0$ , and I couldn’t find anyone where the integral of Eq. 2 converges. It doesn’t mean that its maximum rate of change was unbounded (Eq. 2 is an upper bound), but its seems as a new conjecture: for a time-limited function $f(t)$ to have a finite ${\frac{1}{2\pi}} \int_{-\infty}^{\infty}\left|\omega F(j\omega)\right| d\omega < \infty $ it must have value zero at its domain boundaries $f(t_0) = f(t_F) = 0$ (don’t meaning this, it need to be a bump function $\in C_c^\infty$ , which it’s much more restrictive). I left this into another question in here . Continues in answers section...","The main objective is to find some upper bound for the maximum rate of change as ""tight"" as possible, hopefully related to characteristics of the functions as its signal energy, or its higher Fourier coefficient, or something easy to obtain from the function itself. Also, figure out what is needed for a finite-energy time-limited signal (so, with infinite bandwidth), to have a bounded rate of change . For some partial answers you can go directly to my 2nd answer here Following the notation of exercise 4.49 of the book ""Signals and Systems, 2nd Edition"" (Alan V. Oppenheim, Alan S. Willsky, with S. Hamid) [ 1 ], the Fourier Transform is defined as , so the function can be described as , where . Let being a function which fulfill the conditions to have a Fourier Transform . Then using the composition of a complex number in their amplitude and phase, and the “triangle inequality”, I can establish the following (here is the absolute value): since (note that and angle ), and the remaining integral is independent of . In the book ""Fourier Series: A Modern Introduction - Volume 1 (2nd Edition)"" (R. E. Edwards) [ 2 ], on Chapter 2 point 2.3.6 (point (3) of the ""Remarks"") is proved that if is of bounded variation, and is its Fourier transform (defined differently from here), then: with the total variation of . Since the total variation for a Riemann integrable function with can be defined as: I believe that the bound is going to be ""too loose"", because is the same situation as considering the sum of a series of positive coefficients and asking to compare (!) . (!): Caution is needed, because the sum is under an integral, so the "" "" could change the intuition. As example, consider the ramp function that starts at the origin and ends at the point , its max value is but the sum of the area under the curve is , but on converse, if the edge is on it’s area under the curve will be ). So I want to know: A. Is the bound tight ""enough""? or It will be a ""loose one"" as ? Here I know that: by making in the exponent of the inverse Fourier transform definition, so I have hope is not ""too loose"". For avoiding any kind of ""strange behavior"" like continuous functions nowhere differentiable or differentiable functions nowhere continuous, and the full zoo of functions in between, please consider that the functions is as following: is a non-constant one-variable real-valued function defined for every with the unitary step function and , so haves a beginning at and an end at , being , letting the property being true. Let be a Lebesgue integrable function: , and also a finite energy function: . If needed, also Riemann integrable. Consider that function is continuous, also smooth so all derivatives exists and are bounded (or at least, one time differentiable), so using or or else is equivalent (same for except at the points and ), and also that is of bounded variation. In the same mentioned point of [2], at ""Remarks"" section, is said that Wiener have proved that for a bounded function to be continuous if and only if it behave as: Also assume that the function follows the Rieman-Lebesgue Lema [ 3 ], and the conditions needed to have a Fourier transform described by a function -not by a distribution as Dirac's delta or others- so that the Paley–Wiener theorem is fulfilled [ 4 ]. I would like to represent ""naively"" physically possible time-limited phenomena with , so I don't want in principle, to put restrictions to or , but if needed, first start with , and as last resource, add , making compact-supported but not necessarily , since to be a Bump function it also requires that so every derivative is continuous at the boundaries - and just if nothing else is possible, let be a Bump function . I don't know if there exists a space of non-analytic -alike functions that can have and/or or both, if exists, please let me know how is called and any reference to search for them (I left it as a separated question in here ). Since , when looking for , it will be ""infinite"" because at . Because of this, I am explicitly avoiding the discontinuity at the edges, so could let me work with , so . If the bound is applicable for more general functions, please let me know which constraints you have removed. B) What other tight bounds for are known?? Since using the same argument of the main equations I will have that , so I am trying to find something proportional somehow to the energy of the function , even tried to multiply by to form things of the fashion of so could be think as a probability distribution and use bounds for the expected value and with unsuccessful results. I have found on internet some bounds as the Kalman-Rota or the Landau-Kolmogorov-Hadamard inequalities that states that [ 5 ], but my intuition says that commonly . I also found other inequalities like Poincare's, Sobolev's, Friedrichs's, or Uncertainty Principle relations, but the inequality goes on the other direction . On the comments were mentioned some bounds applicable to band-limited functions (Bernstein inequality [ 11 ], Markov brothers' inequality [ 12 ], Others [ 13 ], follow the main article [ 14 ]), but since here I am asking about time-limited functions , which are going to have unbounded domain on the frequencies [ 10 ], I believe they are not applicable. C) What restrictions have to fulfill so it happen to be true that ???? I have tried with a few functions and it happen to be true, so if you know of any demonstration related please share any reference. I also found counterexamples for finite-energy time-limited, so I want to know which conditions must happen to make it “useful”. This bound ""conjecture"" come from the following mistake: Where, if I let which happens at , then at this I will have that for some , so: In Eq. 4 the result will be for , and "" "" if , so certainly is not a rightfully obtained bound, but ignoring somehow the delta function makes me wonder about the value of . Added later: Following a suggestion, I am going to review some examples. (!!) I solved these examples using Wolfram-Alpha website [ 7 ], which works by default with a different definition of the Fourier transform, so be careful about it. I don't review if the results are ""theoretically"" right, and I have already found examples where Wolfram-Alpha gives numerically wrong results. Gaussian function example_______________________ First one, the case of the Gaussian function: Let a less restricted function that is not time-limited, but vanishes at infinity, the standard Gaussian function distribution so and its signal energy is finite . Also, following “our” notation, the non-unitary Fourier Transform for the angular frequency of , so for this we have (the property that the Fourier Transform of Gaussians is also Gaussian - unbound domain in time and frequency, vanishing at infinity in both). Then, the following is true: Is interesting to note that both bounds of Eq. 2 and Eq. 7  have worked better than . Also note that the bound Eq. 2 is “much tighter” than the bound of Eq. 7, so it’s happened what was commented on (!) . Is interesting to think that the Gaussian is the function that maximizes the Uncertainty Principle, so no other one-variable function with the same energy (since is a one parameter function), can be more concentrated in both time and frequency domains at once ([ 9 ] and [ 10 ]), so my intuition says that time-limited functions (which can be thought as convolution on the frequency domain of a standard function with a ""sinc function""), are going to be even more spread in the frequencies, so it is going to be less likely to found a higher peak for that the ones is achieved by the Gaussian function with the same energy. But anyway, in this example it can be seen that this ""conjecture"" is not a total nonsense. Classic functions examples_____________________ Here I review the simplest cases of traditional functions which its Fourier transforms are tabulated in [1] and in Wikipedia [4]. Knowing beforehand they don't fulfill my requirements, it will be a logic start I think, since many people had worked with them before. The following notation is used from now on: "" "" is used for each signal energy (definition on each table), is the standard rectangular function ( Unitbox(t) in Wolphram-Alpha ), is the Dirac's delta distribution ( Diracdelta(t) in Wolphram-Alpha ), is the standard step function ( Unitstep(t) in Wolphram-Alpha ), is the standard triangular function ( Unittriangle(t) in Wolphram-Alpha ), is the Hermite polynomial of the first kind, which fulfill the equation ( HermiteH(1,t) in Wolphram-Alpha ), and is the Catalan's constant. I have let with (*) the results I believe have questionable accuracy. Unfortunately, the upper bound of Eq. 2 diverges on many examples (I wasn´t expecting it to work for these examples anyway), but for the cases where it results to be finite, it shows to be much better than the bound given by Eq. 5. On the other hand, the upper bound of Eq. 7 results to be finite on much more examples, and also lower than the bound of Eq. 5, but much closer to it. Also, for the example it shows to be lower than the maximum rate of change. This is why I am asking for which conditions this bound have to fulfill to become a valid upper bound (I don't expect that this bound is going to be valid for every possible function, but it could be an alternative when the bound of Eq. 2 doesn't converges). Also note that for the same case of the maximum rate of change results to be higher than the Total Variation , going against my intuition at least, so maybe the validity of the bound of Eq. 5 is not universal (could be related to the Fourier transform definition used in [2] being different from the one I am using here, or also a numerical issue). Bounded domain functions examples________________ Here I review some simple cases of time limited signals. I have tried to find simple cases of continuous functions: signals that starts from and rises/decline “slowly” ( ), others that starts “sharply”, other that starts from a point different than , odd and even signals, signals with a discontinuity, the same signal with different compact domain, positive signals, signals with flat-top, etc. (some signals which I could found a simple Fourier transform to work with). Unfortunately there is no case of a proper “smooth function” since I don’t found any simple “bump function” with a simple Fourier transform, both in “closed form” (I left the question here , and already test all the functions of here on Wolfram-Alpha with negative results). I extend the previous table notation with these: is the Bessel function of the first kind of order 1 ( BesselJ(1,t) in Wolphram-Alpha ), is the “Sine integral” function ( SinIntegral(t) in Wolphram-Alpha ) Also, since for time limited functions the domain is restricted to , the definitions for the Fourier transform changes to , and also all the time domain integrals change its integration limits correspondingly (see headers of the table 2). Be attentive of this, since the Fourier transform of the functions are different with other integration limits. Also related, as explained in point (5) for avoiding the problem at the domain ""edges"", I use without including the boundaries. Again, I have let with (*) the results I believe have questionable accuracy. Against my expectation, the time limited functions behave much worst I intended, but because they are following my intuition: it decays slowly than the Gaussian function, but so slowly that the integral bound of Eq. 2 diverges in many cases, and also so spread, that the peak given by the bound of Eq. 7 is lower than the maximum rate of change. In many cases, the maximum rate of change results higher than the Total Variation , against Eq. 5. The positive side, when the integral of Eq. 2 converges, it results to be a proper bound of the maximum rate of change, even when it was higher than the Total Variation. Unfortunately, all the results for the bound of Eq. 2 have “*” since its validity is questionable, and its value were obtained by numerical approximation through Nintegrate in Wolfram-Alpha, so I will review another bound to ""double check"". Using Hölder’s inequality, it can be stated that for two functions and the following is true: I am going to applied it to the bound of Eq. 2 to avoid the multiplication: So, I tested some functions that makes the integral of the right side converge, and then see if they have a supremum when tested against the function : Here is really interesting that having a ""clear result"" for proves that the results of table 2, maybe numerical inaccurate, are still valid (at list for the chosen ), so there exists time-limited functions for which the integral of Eq. 2 converges. But not only this, also it proves something maybe obvious: There exists TIME-LIMITED functions (with unbound domain on the frequencies), which have a maximum rate of change bounded , (for functions with bounded domain in the frequencies, the results is known and presented in [14]) but better, seen the same problem for a different “not so obvious angle”, I believe it could be reinterpreted as: There exists some “ ” that makes that some time-limited signals with unlimited bandwidth will have a bounded maximum rate of change , conditions I am trying to figure out . Additionally, there other issue I found reviewing the results of table 2: for every function a tried with values at the “edges” of its domain different from zero, the integral of Eq. 2 diverges. This is confusing for me, especially for the functions when changing the domain from to : technically there is no differences in the achieved maximum slopes of the curves, however for the reduced domain one, the integral of Eq. 2 diverges. I think it could be related to the phenomena I am avoiding according point (5), but somehow it manifests on the frequency domain. I tried to solve it by changing the function to so no disruptive changes happen to the function, but as I was expecting its Fourier Transform in was the same to the previous functions (it was adding just a zero-measure point, so it doesn’t change the integral). I have tried many other different functions and combinations on with , and I couldn’t find anyone where the integral of Eq. 2 converges. It doesn’t mean that its maximum rate of change was unbounded (Eq. 2 is an upper bound), but its seems as a new conjecture: for a time-limited function to have a finite it must have value zero at its domain boundaries (don’t meaning this, it need to be a bump function , which it’s much more restrictive). I left this into another question in here . Continues in answers section...","\max_t |\frac{df(t)}{dt}|<\infty F(j \omega) = \int_{-\infty}^{\infty} f(t) e^{-j \omega t} dt f(t) = \frac{1}{2 \pi} \int_{-\infty}^{\infty} F(j \omega) e^{j \omega t} d\omega j = \sqrt{-1} f(t) F(j \omega) |\cdot |  \begin{equation}\begin{split} M = \max_{t}\left\{\left|{\frac{d f(t)}{dt}}\right|\right\} & = \max_{t}\left\{ \left|{\frac{1}{2\pi}} \int_{-\infty}^{\infty}j\omega F(j\omega)e^{j \omega t} d\omega \right|\right\} \texttt{     (Eq. 1)} \\ & =\max_{t}\left\{ \left|{\frac{1}{2\pi}} \int_{-\infty}^{\infty}\left|j\omega F(j\omega)\right| e^{j\sphericalangle\left(j\omega F(j\omega )\right)} e^{j \omega t} d\omega \right| \right\} \\ & \leq \max_{t}\left\{ {\frac{1}{2\pi}} \int_{-\infty}^{\infty}\left| \left|j\omega F(j\omega)\right| e^{j\sphericalangle\left(j\omega F(j\omega )\right)} e^{j \omega t}\right| d\omega \right\} \\ & \leq \max_{t}\left\{ {\frac{1}{2\pi}} \int_{-\infty}^{\infty}\left|j \right|\left|\omega F(j\omega)\right|\left| e^{j\sphericalangle\left(j\omega F(j\omega )\right)}\right| \left| e^{j \omega t}\right| d\omega \right\} \\ & = {\frac{1}{2\pi}} \int_{-\infty}^{\infty}\left|\omega F(j\omega)\right| d\omega \texttt{     (Eq. 2)}\end{split}\end{equation} 1 = |e^{j\phi}|, \forall \phi \in \mathbb{R} j = e^{j\frac{\pi}{2}}, \omega t \in \mathbb{R},  \sphericalangle\left(j\omega F(j\omega )\right) \in \mathbb{R} t f(t) \hat{f}(n)  \left|{n \hat{f}(n)}\right| \leq V(f), \texttt{     (Eq. 5)} V(f) f(t) f(t) M < \infty  V(f) = \int_{-\infty}^{\infty} \left| \frac{df(t)}{dt} \right| dt  \max_{t}\left\{\left|{\frac{d f(t)}{dt}}\right|\right\} \leq V(f) a_n \geq 0 \max_{n}\{a_n\} \leq \sum_{-\infty}^{\infty}a_n dt (1/2,\pi) \pi \sqrt{\pi/2}/2 < \pi (2,\pi) 2 \sqrt{\pi} > \pi {\frac{1}{2\pi}} \int_{-\infty}^{\infty}\left|\omega F(j\omega)\right| d\omega V(f)  \frac{df(t)}{dt}\Big\vert_{t=0} = {\frac{1}{2\pi}} \int_{-\infty}^{\infty} j\omega F(j\omega) d\omega  t=0 e^{j \omega t} = e^0 = 1 f(t) f(t) = x(t) \cdot (\theta(t-t_0) - \theta(t-t_F)) t \in (-\infty; \infty) \theta(t) t_0<t_F f(t) t_0 t_F f(t) = 0 \text{ if } t\leq t_0 \text{ or } t_F \geq t \mathbb{F}\left\{\frac{df(t)}{dt}\right\} = j\omega F(j \omega) f(t) \int_{-\infty}^{\infty}|f(t)|dt < \infty \int_{-\infty}^{\infty}|f(t)|^2dt < \infty x(t) \max \sup f(t) f(t_0) f(t_F) f(t)  \lim_{N \to \infty} \frac{1}{N} \sum_{|n| \leq N} \left|n \hat{f}(n) \right| = 0 \texttt{     (Eq. 6)} f(t) F(jw) \delta(t) f(t) f(t_0) f(t_F) f(t_F)=0 f(t_0)=0 f(t) f(t) \in C_c^{\infty} \lim_{t->\partial t} \frac{d^n f(t)}{dt^n} = 0 f(t) f(t) \in C_c^{\infty} C_c^{\infty} f(t_0)\neq 0 f(t_F)\neq 0 \frac{df(t)}{dt} = \frac{dx(t)}{dt}\cdot (\theta(t-t_0) - \theta(t-t_F)) + x(t)\cdot\delta(t-t_0) -  x(t)\cdot\delta(t-t_F) \max_t |\cdot| \delta(t) = \infty t=0 t_0 < t < t_F \frac{df(t)}{dt} = \frac{dx(t)}{dt}\cdot (\theta(t-t_0) - \theta(t-t_F)) \max_{t_0 < t < t_F} |\frac{df(t)}{dt}| = \max_{t_0 < t < t_F} |\frac{dx(t)}{dt}| \max_{t}\left\{\left|{\frac{d f(t)}{dt}}\right|\right\} \max_{t}\left\{\left|{f(t)}\right|\right\} \leq {\frac{1}{2\pi}} \int_{-\infty}^{\infty}\left|F(j\omega)\right| d\omega \leq \int_{-\infty}^{\infty}\left|f(t)\right| dt \textit{  ¿}\leq\textit{?} \sqrt{\int_{-\infty}^{\infty}\left|f(t)\right|^2 dt} = \sqrt{E_0} E_0 = \int_{-\infty}^{\infty}\left|f(t)\right|^2 dt f(t) \frac{E_0}{E_0} \int_{-\infty}^{\infty}\frac{\left|F(j \omega)\right|^2}{E_0} d\omega = 1 g(\omega)= \frac{\left|F(j \omega)\right|^2}{E_0} E_g[\omega] E_g[\omega^2] ||f'||_2 \leq \sqrt{2} ||f||_2^{1/2}||f''||_2^{1/2} \max_{t}|f'| \ll \max_{t}|f''| \textit{something}(f) \leq \sup |f'| f(t) \max_{t}\left\{\left|{\frac{d f(t)}{dt}}\right|\right\} \leq \max_{\omega}\left\{\left|\omega \cdot F(j \omega)\right|\right\}  \begin{equation}\begin{split} M = \max_{t}\left\{\left|{\frac{d f(t)}{dt}}\right|\right\} & = \max_{t}\left\{ \left|{\frac{1}{2\pi}} \int_{-\infty}^{\infty}j\omega F(j\omega)e^{j \omega t} d\omega \right|\right\} \texttt{     (Eq. 1)} \\ & =\max_{t}\left\{ \left|{\frac{1}{2\pi}} \int_{-\infty}^{\infty}\left|j\omega F(j\omega)\right| e^{j\sphericalangle\left(j\omega F(j\omega )\right)} e^{j \omega t} d\omega \right| \right\} \end{split}\end{equation} M_\omega^* = \max_\omega |j\omega F(j \omega)| \omega^* = \arg \max |j\omega F(j \omega)| e^{j\sphericalangle\left(j\omega F(j\omega )\right)} = e^{j \phi^*} \phi^* \in \mathbb{R}  \begin{equation}\begin{split} M = \max_{t}\left\{\left|{\frac{d f(t)}{dt}}\right|\right\} & \leq? \max_{t}\left\{ \left|{\frac{M_\omega^* \cdot e^{j \phi^*}}{2\pi}} \int_{-\infty}^{\infty} e^{j \omega t} d\omega \right|\right\} \texttt{     (Eq. 3)} \\ & \leq \max_{t}\left\{ M_\omega^* |e^{j \phi^*}|\left|{\frac{1}{2\pi}} \int_{-\infty}^{\infty}e^{j \omega t} d\omega \right| \right\} \\ & = M_\omega^* \max_{t}\left\{\left|\delta(t) \right| \right\} \\ &  = \max_\omega |j\omega F(j \omega)| \max_{t}\left\{\left|\delta(t) \right| \right\} \texttt{     (Eq. 4)}\end{split}\end{equation} 0 t \neq 0 \infty t=0 M_\omega^* = \max_\omega |j\omega F(j \omega)| \texttt{     (Eq. 7)} f(t) f(t)=\frac{1}{\sqrt{2\pi}} e^{-\frac{t^2}{2}} \int_{-\infty}^{\infty} f(t) dt = \int_{-\infty}^{\infty} |f(t)| dt = 1 \int_{-\infty}^{\infty} |f(t)|^2 dt = \frac{1}{2\sqrt{\pi}} \approx 0.282 \ll \infty \omega \mathbb{F_t}\left\{ e^{-a t^2}\right\}(\omega) = \sqrt{\frac{\pi}{a}} e^{\frac{-\omega^2}{4a}}\, f(t) F(j\omega)=e^{-\frac{\omega^2}{2}}  \begin{equation}\begin{split} M_{std.gauss} & = & \max_{t}\left\{\left|{\frac{d f(t)}{dt}}\right|\right\} = \frac{1}{\sqrt{2\pi e}}\text{ on } t^* = \pm 1 & \approx 0.24197 \\ & \leq & {\frac{1}{2\pi}} \int_{-\infty}^{\infty}\left|\omega F(j\omega)\right| d\omega  = \frac{1}{\pi}  & \approx 0.31831 \\ & \leq & \max_{\omega}\left\{\left|{\omega F(j\omega)}\right|\right\}  = \frac{1}{\sqrt{e}} \text{ on } \omega^* = \pm 1  & \approx 0.60653 \\ & \leq & V(f) = \int_{-\infty}^{\infty} \left| \frac{df(t)}{dt} \right| dt  = \sqrt{\frac{2}{\pi}} & \approx 0.79788 \end{split}\end{equation} V(f) |\omega F(j\omega)| E° \Pi(t) = 1, |t|\leq \frac{1}{2} \delta(t) \theta(t)=1, t \geq 0 \Lambda (t)=1-|t|, |t|\leq 1 H_1(t)=2\cdot t\, H_n(t) = (-1)^n\,e^{t^2}\frac{d^n}{dt^n}(e^{-t^2}) \mathscr{C}=0.91596559\cdots\,  
\begin{array}{|c:c|c:c|c|c:c:c|c:c:c:c|} 
\hline 
f(t) & \text{dom}(f(t)) & F(j\omega)=\mathbb{F}\{f(t)\}(\omega) & \text{dom}(F(j\omega)) & \max_t |f'(t)| & \frac{1}{2 \pi} \int_{-\infty}^{\infty} |j\omega F(j\omega)|d\omega & \max_\omega |j\omega F(j \omega)| & V(f) = \int_{-\infty}^{\infty} |f'(t)|dt & \max_t |f(t)| & ||f||_1 = \int_{-\infty}^{\infty} |f(t)|dt & E° = \int_{-\infty}^{\infty} |f(t)|^2 dt & ||f||_2 = \sqrt{E°} \\  \hline
\Pi (t) &  [-\frac{1}{2}; \frac{1}{2}] & \text{sinc}(\frac{\omega}{2}) & (-\infty; \infty) & \infty^* & \infty^* & 2 & 0^* & 1 & 1 & 1 & 1 \\ \hdashline
\text{sinc}(\frac{t}{2}) &  (-\infty; \infty) & 2\pi \cdot \Pi (\omega) & [-\frac{1}{2}; \frac{1}{2}] & 0.218 & \frac{1}{4} = 0.25 & \pi = 3.1416 & \infty & 1 & \infty & 2\pi = 6.2831 & 2.5066 \\ \hdashline
\text{sinc}^2(\frac{t}{2}) &  (-\infty; \infty) & \Lambda (\omega) & [-1; 1] & 0.27 & \frac{1}{3} = 0.33 & \frac{\pi}{2} = 1.57079  & \infty & 1 & 2\pi = 6.2831 & \frac{4\pi}{3} = 4.18879 & 2.046 \\ \hdashline
\Lambda (t) &  [-1; 1] & \text{sinc}^2(\frac{\omega}{2}) & (-\infty; \infty) & undefined & \infty & 1.44922 & 2^* & 1 & 1 & \frac{2}{3} = 0.66 & 0.816 \\ \hdashline
e^{-t} \cdot \theta (t) &  [0; \infty) & \frac{1}{(1+j\omega)} & (-\infty; \infty) & 1^* & \infty & 1^* & 1^* & 1 & 1 & \frac{1}{2} = 0.5 & 0.707 \\ \hdashline
t\cdot  e^{-t} \cdot \theta (t)&  [0; \infty) & \frac{1}{(1+j\omega)^2} & (-\infty; \infty) & 1^* & \infty & \frac{1}{2} = 0.5 & \frac{2}{e}^* = 0.735759^* & \frac{1}{e} = 0.3678 & 1 & \frac{1}{4} = 0.25 & \frac{1}{2} = 0.5 \\ \hdashline
\frac{1}{\sqrt{2\pi}}e^{-\frac{t^2}{2}} &  (-\infty; \infty) & e^{-\frac{\omega^2}{2}} & (-\infty; \infty) & \frac{1}{\sqrt{2 e \pi}} = 0.24 & \frac{1}{\pi} =  0.31831 & \frac{1}{\sqrt{e}} = 0.606 & \sqrt{\frac{2}{\pi}} = 0.797 & \frac{1}{\sqrt{2\pi}} = 0.39 & 1 & \frac{1}{2\sqrt{\pi}} = 0.282 & 0.531 \\ \hdashline
\frac{1}{\sqrt{\pi}}e^{-j\frac{t^2}{2}} &  (-\infty; \infty) & (1-j)\cdot e^{j\frac{\omega^2}{2}} & (-\infty; \infty) & \infty & \infty & \infty & \infty & \frac{1}{\sqrt{\pi}} = 0.564 & \infty & \infty & \infty \\ \hdashline
e^{-|t|} &  (-\infty; \infty) & \frac{2}{(1+\omega^2)} & (-\infty; \infty) & 1^* & \infty & 1 & \infty^* & 1 & 2 & 1 & 1 \\ \hdashline
e^{-\frac{t^2}{2}}H_1(t) &  (-\infty; \infty) & -j\omega \cdot 2\sqrt{2\pi}\cdot e^{-\frac{\omega^2}{2}} & (-\infty; \infty) & 2 & 2 & \frac{4\sqrt{2\pi}}{e} = 3.68 & \frac{8}{\sqrt{e}} = 4.8522 & \frac{2}{\sqrt{e}} = 1.213 & 4 & 2\sqrt{\pi} = 3.5449 & 1.883 \\ \hdashline
\text{sech}(t) & (-\infty; \infty) & \pi \cdot \text{sech}(\frac{\pi \omega}{2}) & (-\infty; \infty) & \frac{1}{2} = 0.5 & \frac{8 \mathscr{C}}{\pi^2} = 0.74245 & 1.32549 & 2 & 1 & \pi = 3.1416 & 2 & 1.414 \\ \hline
\end{array}
 f(t) = t\,e^{-t}\theta(t) f(t) = t\,e^{-t}\,\theta(t) V(f) 0 f(t)=f’(t)=0 0 \in C_c^\infty J_1(t) \text{Si}(t) t_0 \leq t \leq t_F F(j \omega) = \int_{t_0 }^{t_F} f(t) e^{-j \omega t} dt \max_{t_0 < t < t_F} |f'(t)|  
\begin{array}{|c:c|c:c|c|c:c:c|c:c:c:c|} 
\hline 
f(t) & \text{dom}(f) = [a\,;\,b] & F(j\omega)=\mathbb{F}_{[a\,;\,b]}\{f(t)\}(\omega) & \text{dom}(F(j\omega)) & \max_{a < t < b} |f'(t)| & \frac{1}{2 \pi} \int_{-\infty}^{\infty} |j\omega F(j\omega)|d\omega & \max_\omega |j\omega F(j \omega)| & V_a^b(f) = \int_{a}^{b} |f'(t)|dt & \max_{a\leq t \leq b} |f(t)| & ||f||_1 = \int_{a}^{b} |f(t)|dt & E° = \int_{a}^{b} |f(t)|^2 dt & ||f||_2 = \sqrt{E°} \\  \hline
\sqrt{1-t^2} & [-1; 1] & \pi \cdot \frac{J_1(\omega)}{\omega} & (-\infty; \infty) & \infty & \infty^* & 1.82798 & 2 & 1 & \frac{\pi}{2} = 1.57079 & \frac{4}{3} = 1.33 & 1.1547 \\ \hdashline
\sin(\frac{t\pi}{2}) & [-1; 1] & -j\frac{8\,\omega\cos(\omega)}{(\pi^2-4\,\omega^2)} & (-\infty; \infty) & \frac{\pi}{2} = 1.57079 & \infty^* & 2.75144 & 2 & 1 & \frac{4}{\pi} =1.2732 & 1 & 1 \\ \hdashline
\sin^2(\frac{t\pi}{2}) & [-1; 1] & \frac{(\pi^2-2\,\omega^2)\sin(\omega)}{(\pi^2\,\omega-\omega^3)} & (-\infty; \infty) & \frac{\pi}{2} = 1.57079 & \infty^* & 2.8929 & 2 & 1 & 1 & \frac{3}{4} = 0.75 & 0.866 \\ \hdashline
\cos^2(\frac{t\pi}{2}) &  [0; 1] & j\frac{(\pi^2(1-e^{-j\omega})-2\,\omega^2)}{2\,\omega\,(\omega^2-\pi^2)} & (-\infty; \infty) & \frac{\pi}{2} = 1.57079 & \infty^* & 1.4562 & 1 & 1 & \frac{1}{2} = 0.5 & \frac{3}{8} = 0.375 & 0.612 \\ \hdashline
\cos^2(\frac{t\pi}{2}) & [-1; 1] & \frac{\pi^2\sin(\omega)}{(\pi^2\omega-\omega^3)} & (-\infty; \infty) & \frac{\pi}{2} = 1.57079 & 2.28547^* & 1.63641 & 2 & 1 & 1 & \frac{3}{4} = 0.75 & 0.866 \\ \hdashline
\frac{(1+\cos(t\pi))^2}{4} & [-1; 1] & \frac{3\,\pi^4\sin(\omega)}{(\omega^5-5\pi^2\omega^3+4\pi^4\omega)} & (-\infty; \infty) & \frac{3\sqrt{3}\pi}{8} = 2.0405 & 2.61265^* & 1.58242 & 2 & 1 & \frac{3}{4} = 0.75 & \frac{35}{64} = 0.546875 & 0.7395 \\ \hdashline
\sin(\frac{t\pi}{2})\cos^2(\frac{t\pi}{2}) & [-1; 1] & j\frac{16\, \pi^2\, \omega \cos(\omega)}{(16\, \omega^4-40\, \pi^2 \omega^2+9\,\pi^4)} & (-\infty; \infty) & 1.5708 & 1.93647^* & 1.23244 & 1.5396^* & \frac{2}{3\sqrt{3}} = 0.3849 & \frac{4}{3\sqrt{\pi}} = 0.42441 & \frac{1}{8} = 0.125 & 0.354 \\ \hdashline
\text{sinc}(t\pi)\cos(\frac{t\pi}{2}) & [-1; 1] & \frac{1}{2\pi}\left(\text{Si}(\frac{\pi}{2}-\omega)+\text{Si}(\frac{3\pi}{2}-\omega)+\text{Si}(\frac{\pi}{2}+\omega)+\text{Si}(\frac{3\pi}{2}+\omega)\right) & (-\infty; \infty) & 1.62897 & \infty^* & 1.61724 & 2 & 1 & \frac{2\,\text{Si}(\pi)}{8} = 1.17898 & \frac{2\,\text{Si}(2\pi)}{\pi} = 0.9028 & 0.9501 \\ \hdashline
1-\sin^4(\frac{t\pi}{2}) & [-1; 1] & \frac{\pi^2(5\pi^2 - 2\,\omega^2) \sin(\omega)}{(\omega^5 - 5\pi^2\omega^3 + 4\pi^4\omega)} & (-\infty; \infty) & \frac{3\sqrt{3}\pi}{8} = 2.0405 & 3.01547^* & 1.8225 & 2 & 1 & \frac{5}{4} = 1.25 & \frac{67}{64} =  1.04688 & 1.023 \\ \hdashline
\sin(|t|\pi) & [-1; 1] & \frac{2\pi\,(1+\cos(\omega))}{(\pi^2-\omega^2)} & (-\infty; \infty) & \pi = 3.1416 & \infty^* & 2.90769 & 4^* & 1 & \frac{4}{\pi} =1.2732 & 1 & 1 \\ \hline
\end{array}
 V_a^b(f) f(t) g(t) \int_{-\infty}^\infty |f(t) \cdot g(t)|\,dt \leq \int_{-\infty}^\infty |f(t)|\,dt \cdot \sup_t |g(t)| \frac{1}{2 \pi} \int_{-\infty}^\infty |\omega \, g(\omega) \cdot \frac{F(j \omega)}{g(\omega)}|\,d\omega \leq \frac{1}{2 \pi} \int_{-\infty}^\infty |\omega\, g(\omega)|\,d\omega \cdot \sup_\omega |\frac{F(j \omega)}{g(\omega)}| \texttt{     (Eq. 8)} g(\omega) f(t) = \cos^2(\frac{t\pi}{2}),\,|t| \leq 1\, \begin{array}{|c|c:c|} \hline 
g(\omega) & \int_{-\infty}^\infty |\omega\, g(\omega)|\,d\omega &  \texttt{(Eq. 8)} \\ \hline 
e^{-\sqrt{|w|}} & 24 & 12.1623 \\ \hline 
\end{array}  g(\omega) = e^{-\sqrt{|w|}} f(t) \text{mysterious conditions}\,\mathbb{X} f(t) = \cos^2(\frac{t\pi}{2}) [-1,\,1] [0,\,1] f(t) = \theta(-t)+\cos^2(\frac{t\pi}{2}),\, 0 \leq t \leq 1\, [0,\,1] [-1,\,1] f(-1) \neq 0 f(t) {\frac{1}{2\pi}} \int_{-\infty}^{\infty}\left|\omega F(j\omega)\right| d\omega < \infty  f(t_0) = f(t_F) = 0 \in C_c^\infty","['real-analysis', 'derivatives', 'fourier-analysis', 'upper-lower-bounds', 'piecewise-continuity']"
55,Why should we use the fundamental definition of derivative while checking differentiability?,Why should we use the fundamental definition of derivative while checking differentiability?,,"I have seen in many examples that using the fundamental definition of derivative only gives the correct results while checking for differentiability at a point. But why do we get wrong results while we directly apply the rules of differentiation and taking the limit at the point. Here is an example $$y = \begin{cases} x^2, &-\infty < x < 1\\ 2x, &1\leq x < \infty \end{cases} $$ If we check the differentiability at the point $x=1$ then when we apply the rules of differentiation left hand and right hand derivative becomes same. But by using fundamental definition of derivative it will be different. But if I am right the rules of differentiation are in fact derived from the fundamental definition of derivative? So both of these should give correct result. Or it seems like I am missing some basic concept. I have seen same question like this one here . The answer to this question only says it gives wrong result.But it doesn't answer the question of why?",I have seen in many examples that using the fundamental definition of derivative only gives the correct results while checking for differentiability at a point. But why do we get wrong results while we directly apply the rules of differentiation and taking the limit at the point. Here is an example If we check the differentiability at the point then when we apply the rules of differentiation left hand and right hand derivative becomes same. But by using fundamental definition of derivative it will be different. But if I am right the rules of differentiation are in fact derived from the fundamental definition of derivative? So both of these should give correct result. Or it seems like I am missing some basic concept. I have seen same question like this one here . The answer to this question only says it gives wrong result.But it doesn't answer the question of why?,"y = \begin{cases}
x^2, &-\infty < x < 1\\
2x, &1\leq x < \infty
\end{cases}
 x=1","['calculus', 'derivatives']"
56,"$f$ is continuous and differentiable in $(-1,1)$, $f(1) = 0$, $f(x)>0 (x∈(-1,1))$. Show $∃c∈(-1,1); mf(c)f'(-c) = nf'(c)f(-c)$ in which $m∈N, n∈N$","is continuous and differentiable in , , . Show  in which","f (-1,1) f(1) = 0 f(x)>0 (x∈(-1,1)) ∃c∈(-1,1); mf(c)f'(-c) = nf'(c)f(-c) m∈N, n∈N","My problem is the following: ""Suppose that $f\colon[-1,1]\to\mathbb{R}$ is continuous and differentiable on $(-1,1)$ , $f(1) = 0$ and $f(x)>0$ for $x\in(-1,1)$ . Show that there exists $c\in(-1,1)$ such that $mf(c)f'(-c) = nf'(c)f(-c)$ , in which $m\in N$ and $n\in N$ ."" I can prove if when $M=N$ : there exists $c\in(-1,1)$ for which $f(c)f'(-c) = f'(c)f(-c)$ using $g(x) = f(x)f(-x)$ , which satisfies $g(1) = g(-1) = 0$ and Rolle's theorem, but I can't prove it when $m ≠ n$ . Can someone please tell me how to prove it?","My problem is the following: ""Suppose that is continuous and differentiable on , and for . Show that there exists such that , in which and ."" I can prove if when : there exists for which using , which satisfies and Rolle's theorem, but I can't prove it when . Can someone please tell me how to prove it?","f\colon[-1,1]\to\mathbb{R} (-1,1) f(1) = 0 f(x)>0 x\in(-1,1) c\in(-1,1) mf(c)f'(-c) = nf'(c)f(-c) m\in N n\in N M=N c\in(-1,1) f(c)f'(-c) = f'(c)f(-c) g(x) = f(x)f(-x) g(1) = g(-1) = 0 m ≠ n","['calculus', 'derivatives']"
57,Find dV/dA in terms of r.,Find dV/dA in terms of r.,,"The volume of a sphere, $V$ cm³, of radius $r$ is given by the formula $V = \frac{4}{3} \pi r^3$ . The surface area of a sphere $A$ cm² of radius $r$ cm is given by the formula $A=4\pi r^2$ . Find $dV/dA$ in terms of $r$ . Here's my workings to the question: $$V= \frac{\frac{4}{3}\pi r^{3}}{4\pi r^2}A = \frac{1}{3}rA$$ So, $$\frac{dV}{dA} = \frac{1}{3}r =\frac{r}{3}.$$ I am not sure about this answer, so it would help to know if anyone got the same answer. Thank you!","The volume of a sphere, cm³, of radius is given by the formula . The surface area of a sphere cm² of radius cm is given by the formula . Find in terms of . Here's my workings to the question: So, I am not sure about this answer, so it would help to know if anyone got the same answer. Thank you!",V r V = \frac{4}{3} \pi r^3 A r A=4\pi r^2 dV/dA r V= \frac{\frac{4}{3}\pi r^{3}}{4\pi r^2}A = \frac{1}{3}rA \frac{dV}{dA} = \frac{1}{3}r =\frac{r}{3}.,"['calculus', 'derivatives', 'related-rates']"
58,"If $f(x)≤x$ , then $f′(x)≤1$?","If  , then ?",f(x)≤x f′(x)≤1,"I'm studying Calculus and having a trouble solving this question. 1) If $f(x)\leq x$ , then $f′(x)\leq 1$ for all $x$ ? 2) What if $f(0)=0$ , $f′(x)$ exists for all $x$ ? I could easily find the counter example for 1) (Therefore it is false) But I'm not sure about 2) If $f(0)=0$ and $f′(x)$ exists for all $x$ & $f(x)\leq x$ , then $f′(x)\leq 1$ for all $x$ ? Please leave a comment if you don't mind :)","I'm studying Calculus and having a trouble solving this question. 1) If , then for all ? 2) What if , exists for all ? I could easily find the counter example for 1) (Therefore it is false) But I'm not sure about 2) If and exists for all & , then for all ? Please leave a comment if you don't mind :)",f(x)\leq x f′(x)\leq 1 x f(0)=0 f′(x) x f(0)=0 f′(x) x f(x)\leq x f′(x)\leq 1 x,"['calculus', 'derivatives']"
59,"Given second and first derivatives at 2 points, prove that some point in between them has third derivative greater than or equal to 24.","Given second and first derivatives at 2 points, prove that some point in between them has third derivative greater than or equal to 24.",,"Let $f$ be a function that is $C^3$ on an open interval containing $[0,1]$ - that is,  the third derivative $f'''$ exists and is continuous on an open interval containing $[0,1]$ . Assume that $f(0) = f'(0) = f''(0) = 0$ and that $f'(1) = f''(1) = 0$ . If $f(1) = 1$ , prove that there is some $c\in\left(0,1\right)$ such that $f'''(c) \geq 24$ . Not sure how to approach this problem, been stuck on it for a while. Any help would be nice. I tried using the Mean Value Theorem, and got that at some point $c_1\in\left[0,1\right], f'(c_1) = \frac{f(1)-f(0)}{1-0}=1$ and there is some point $c_2 \in [c_1,1]$ such that $f''(c_2) = \frac{f'(1)-f'(c_1)}{1-c_1}=\frac{-1}{1-c_1}<-1$ and there is some point $c_3\in[c_2,1]$ such that $f'''(c_3) = \frac{f''(1)-f''(c_2)}{1-c_2}>1$ . I'm not sure how to extend this to $24$ though, or even if this method will work. Is there a way to use Taylor Polynomials perhaps?","Let be a function that is on an open interval containing - that is,  the third derivative exists and is continuous on an open interval containing . Assume that and that . If , prove that there is some such that . Not sure how to approach this problem, been stuck on it for a while. Any help would be nice. I tried using the Mean Value Theorem, and got that at some point and there is some point such that and there is some point such that . I'm not sure how to extend this to though, or even if this method will work. Is there a way to use Taylor Polynomials perhaps?","f C^3 [0,1] f''' [0,1] f(0) = f'(0) = f''(0) = 0 f'(1) = f''(1) = 0 f(1) = 1 c\in\left(0,1\right) f'''(c) \geq 24 c_1\in\left[0,1\right], f'(c_1) = \frac{f(1)-f(0)}{1-0}=1 c_2 \in [c_1,1] f''(c_2) = \frac{f'(1)-f'(c_1)}{1-c_1}=\frac{-1}{1-c_1}<-1 c_3\in[c_2,1] f'''(c_3) = \frac{f''(1)-f''(c_2)}{1-c_2}>1 24",['derivatives']
60,$\left|\sin(x)\right|$ derivative at $\pi$,derivative at,\left|\sin(x)\right| \pi,"Let $f$ be a function, $f:\mathbb{R}\to\mathbb{R}$ $$f(x)=\begin{cases}        e^{x^2}-2 & x< 0 \\ x^3+x-1 & 0\leq x \leq 1 \\       \left|\sin(x)\right|  & x>1.     \end{cases}$$ Check if $f$ is continuous and differentiable at $a$ , when $a=0,1,\frac{\pi}{2}, \pi$ . If $f$ is differentiable at $a$ , find $f'(a)$ . What I've been doing: I found that: $f$ is continuous at $0$ but not differentiable. $f$ is not continuous at $1$ so it's not differentiable. And then I thought that $f$ was differentiable at $\pi$ and $\frac{\pi}{2}$ because $f$ is continuous at $(1, +\infty)$ ( $\left|\sin(x)\right|$ ), so I looked for: $f'(\frac{\pi}{2})=\left|\cos(\frac{\pi}{2})\right|=0$ (by the solution my prof gave me this is correct). $f'(\pi)=\left|\cos(\pi)\right|=-1$ Now this is wrong. The solution they gave me says that $f$ is not differentiable at $\pi$ , and I'm really lost. Why is it differentiable at $\frac{\pi}{2}$ and not $\pi$ ?","Let be a function, Check if is continuous and differentiable at , when . If is differentiable at , find . What I've been doing: I found that: is continuous at but not differentiable. is not continuous at so it's not differentiable. And then I thought that was differentiable at and because is continuous at ( ), so I looked for: (by the solution my prof gave me this is correct). Now this is wrong. The solution they gave me says that is not differentiable at , and I'm really lost. Why is it differentiable at and not ?","f f:\mathbb{R}\to\mathbb{R} f(x)=\begin{cases} 
      e^{x^2}-2 & x< 0 \\
x^3+x-1 & 0\leq x \leq 1 \\ 
     \left|\sin(x)\right|  & x>1. 
   \end{cases} f a a=0,1,\frac{\pi}{2}, \pi f a f'(a) f 0 f 1 f \pi \frac{\pi}{2} f (1, +\infty) \left|\sin(x)\right| f'(\frac{\pi}{2})=\left|\cos(\frac{\pi}{2})\right|=0 f'(\pi)=\left|\cos(\pi)\right|=-1 f \pi \frac{\pi}{2} \pi","['calculus', 'real-analysis', 'derivatives', 'continuity']"
61,Is $\frac{dy}{dx} = \frac{d(y+c)}{dx}$?,Is ?,\frac{dy}{dx} = \frac{d(y+c)}{dx},"Is the following true? $$\frac{dy}{dx} = \frac{d(y+c)}{dx}$$ where $c$ is an arbitrary real constant. I believe it is true, and my reasoning goes like this: $dy$ is an infinitesimal, so the addition of another constant would still be an infinitesimal. I do not know if my reasoning is correct. Do note that I'm not familiar with epilson delta and university calculus. I would appreciate it if someone could explain the above simply. EDIT: I couldn't see why $d(y+c)= dy+dc$. What is $d$? Is it a number, or a function?","Is the following true? $$\frac{dy}{dx} = \frac{d(y+c)}{dx}$$ where $c$ is an arbitrary real constant. I believe it is true, and my reasoning goes like this: $dy$ is an infinitesimal, so the addition of another constant would still be an infinitesimal. I do not know if my reasoning is correct. Do note that I'm not familiar with epilson delta and university calculus. I would appreciate it if someone could explain the above simply. EDIT: I couldn't see why $d(y+c)= dy+dc$. What is $d$? Is it a number, or a function?",,"['calculus', 'derivatives']"
62,Function $f(x)$ such that $\frac{f'(x) \cdot x}{f(x)}$ is linear,Function  such that  is linear,f(x) \frac{f'(x) \cdot x}{f(x)},"I've just learnt about price elasticity of demand. Since we're only doing 10th grade economics, our teacher tried to make it simple by using an example where the demand curve is linear (i.e. quantity of demand is in linear relationship with the price), and we were supposed to treat the price elasticity of demand $e_d$ as having a linear relationship with the price (but it is not ). However, as the teacher pointed out to a few curious students, $e_d$ is in fact not having such a linear relationship at all, as seen in its formula: \begin{align} e_d = \frac{\text{variation in demand}}{\text{variation in price}} = \frac{\Delta Q}{\Delta P} \times \frac{P}{Q} \end{align} where $Q$ stands for quantity of demand and $P$ stands for price, and $\Delta Q$ and $\Delta P$ respectively stand for change in demand and price. I then wanted to find a case where $e_d$ is actually linear. By assuming the demand $q$ for a certain price $p$ is given by a function $q = f(p)$, I could express the price elasticity at that price $p$ as \begin{align} e_d = \lim_{h\rightarrow 0} \frac{f(x+h) - f(x)}{h} \times \frac{x}{f(x)}  = \frac{f'(x)x}{f(x)} \end{align} This where I got stuck. My limited knowledge in calculus and in mathematics in general does not give an explicit way to find a $f(x)$ that makes $e_d$ a linear equation, i.e. $\frac{f'(x) \cdot x}{f(x)} = ax + c$, where $a$ and $c$ are real numbers. I might have mixed too much mathematics in economics here, but I am just curious to know if this could actually happen.","I've just learnt about price elasticity of demand. Since we're only doing 10th grade economics, our teacher tried to make it simple by using an example where the demand curve is linear (i.e. quantity of demand is in linear relationship with the price), and we were supposed to treat the price elasticity of demand $e_d$ as having a linear relationship with the price (but it is not ). However, as the teacher pointed out to a few curious students, $e_d$ is in fact not having such a linear relationship at all, as seen in its formula: \begin{align} e_d = \frac{\text{variation in demand}}{\text{variation in price}} = \frac{\Delta Q}{\Delta P} \times \frac{P}{Q} \end{align} where $Q$ stands for quantity of demand and $P$ stands for price, and $\Delta Q$ and $\Delta P$ respectively stand for change in demand and price. I then wanted to find a case where $e_d$ is actually linear. By assuming the demand $q$ for a certain price $p$ is given by a function $q = f(p)$, I could express the price elasticity at that price $p$ as \begin{align} e_d = \lim_{h\rightarrow 0} \frac{f(x+h) - f(x)}{h} \times \frac{x}{f(x)}  = \frac{f'(x)x}{f(x)} \end{align} This where I got stuck. My limited knowledge in calculus and in mathematics in general does not give an explicit way to find a $f(x)$ that makes $e_d$ a linear equation, i.e. $\frac{f'(x) \cdot x}{f(x)} = ax + c$, where $a$ and $c$ are real numbers. I might have mixed too much mathematics in economics here, but I am just curious to know if this could actually happen.",,"['derivatives', 'economics']"
63,"Differences and relations between $\partial x, dx, \delta x, \Delta x$.",Differences and relations between .,"\partial x, dx, \delta x, \Delta x","$\partial x, dx, \delta x, \Delta x$ all describe the change of something, but what distinguishes them, and when is this distinction irrelevant? For instance, sometimes $\dfrac{\mathrm df(x)}{\mathrm dx} = \dfrac{\partial f(x)}{\partial x}$, and in thermodynamics the equality $pdV = \delta W$ might hold. So when is it important to distinguish these, and when can they be used interchangeably?","$\partial x, dx, \delta x, \Delta x$ all describe the change of something, but what distinguishes them, and when is this distinction irrelevant? For instance, sometimes $\dfrac{\mathrm df(x)}{\mathrm dx} = \dfrac{\partial f(x)}{\partial x}$, and in thermodynamics the equality $pdV = \delta W$ might hold. So when is it important to distinguish these, and when can they be used interchangeably?",,"['derivatives', 'notation', 'partial-derivative', 'differential-forms']"
64,Partial Derivative of the one variable function,Partial Derivative of the one variable function,,"This is from my exam: 1) Calculate partial derivative $f'(10)$ of the function: $$f(x)=\frac{1-\log x}{1+\log x}.$$ This is a function of only one variable, why do they use the term 'partial' ? Are the terms derivative and differential interchangeable for one variable functions. Is it correct to write (for this example): $f'(10)=\frac{\mathrm{d}f}{\mathrm{d}x}(10)$ or $f'(10)=\frac{\partial f}{\partial x}(10)$","This is from my exam: 1) Calculate partial derivative $f'(10)$ of the function: $$f(x)=\frac{1-\log x}{1+\log x}.$$ This is a function of only one variable, why do they use the term 'partial' ? Are the terms derivative and differential interchangeable for one variable functions. Is it correct to write (for this example): $f'(10)=\frac{\mathrm{d}f}{\mathrm{d}x}(10)$ or $f'(10)=\frac{\partial f}{\partial x}(10)$",,['derivatives']
65,Speediness and correctness when graphing by hand .,Speediness and correctness when graphing by hand .,,"First of all thank you for visiting this question! I believe it's a pretty simple problem but get's kinda hairy and time consuming on each step as I have done it, so my question (the one you are here for) is the following : What is the fastest and most reliable way to sketch a given equation determining zeroes, asymptotes, extreme values, and points of inﬂection? (While doing this problem here , I realized of other doubts I had about the procedure I applied, I've listed those doubts at the bottom) I was given by the university I'm looking forward to join an example of my math admission exam and on the last section and the one that it's worth the most I encountered the following : Let $f(x) = \frac{x}{x^2+1}$ Determine the zeroes, asymptotes, extreme values, and points of inﬂection of the graph. Use this information to sketch the graph of $f(x)$ supported by complete arguments. My first idea was to create a table that I would be filling while doing this problem, the table was the following : +--------------------------------+ |Known Information               | +--------------------------------+ |Zeroes :                        | |Vertical Asymptote :            | |Horizontal Asymptote :          | |Extreme Values :                | |Points of Inflection :          | +--------------------------------+ Now I will separate each procedure by sections on the post , in the same order I did the problem. 1.- Determining the zeros I proceeded to find the zeroes by doing the following $$\frac{x}{x^2+1}=0$$ $$·(x^2+1) ·(x^2+1)$$ $$x=0$$ So when $x=0$ the equation is $0$ as well (Yes, I could have determined this by just looking at the pure fraction, but I'm trying to be as general as possible , my test might contain some other than just x) We proceed to fill the table +--------------------------------+ |Known Information               | +--------------------------------+ |Zeroes : 0                      | |Vertical Asymptote :            | |Horizontal Asymptote :          | |Extreme Values :                | |Points of Inflection :          | +--------------------------------+ 2.- Determining Vertical Asymptote I proceeded to determine the vertical asymptote by setting the denominator to zero as I illustrate next $$x^2+1=0$$ $$-1 -1$$ $$x^2=-1$$ By having this I determined that there was no vertical asymptote . +--------------------------------+ |Known Information               | +--------------------------------+ |Zeroes : 0                      | |Vertical Asymptote : None       | |Horizontal Asymptote :          | |Extreme Values :                | |Points of Inflection :          | +--------------------------------+ 3.- Determining the horizontal asymptote. To do this, I knew that Given $$\frac{t^a}{d^b}$$ if $b>a$ then we have an horizontal asymptote at $y=0$ Consequently +--------------------------------+ |Known Information               | +--------------------------------+ |Zeroes : 0                      | |Vertical Asymptote : None       | |Horizontal Asymptote : y=1      | |Extreme Values :                | |Points of Inflection :          | +--------------------------------+ 4.- Determining the extreme values. This is where it gets time consuming and complicated , to proceed I differentiated my $f(x)$ as illustrated next $$y = \frac{x}{x^2+1}$$ $$\frac{dy}{dx}=\frac{dy}{dx}[\frac{x}{x^2+1}]$$ $$\frac{dy}{dx}[\frac{x}{x^2+1}] = \frac{dy}{dx}[x·((x^2+1)^-1)]$$ Note : On the last illustration the last term on the right is raised to the negative one power, which is equal to $\frac{1}{x^2+1}$ (I do not know why I can't express the negative one power here..) And I also knew that Given $$\frac{dy}{dx}[f(x)g(x)]=f'(x)g(x)+g'(x)f(x)$$ So I proceeded first to find the derivatives of each term being multiplied , the first one resulting as below : $$\frac{dy}{dx}[x] = 1$$ The second one resulting as below as well (Using chain rule) $$\frac{dy}{dx}[(x^2+1)^-1] = -1((x^2+1)^-2)·2x$$ Or equivalently $$\frac{dy}{dx}[(x^2+1)^-1] = -\frac{2x}{(x^2+1)^2}$$ So ended up having $$\frac{dy}{dx}[x] = 1$$ and $$\frac{dy}{dx}[(x^2+1)^-1] = -\frac{2x}{(x^2+1)^2}$$ Proceeded to use the product rule as illustrated below $$\frac{dy}{dx}[f(x)g(x)]=f'(x)g(x)+g'(x)f(x)$$ $$\frac{dy}{dx}[x·\frac{1}{(x^2+1)}]=(1·(\frac{1}{(x^2+1)}))+(-\frac{2x}{(x^2+1)^2} · x)$$ $$\frac{dy}{dx}[x·\frac{1}{(x^2+1)}]=(\frac{1}{(x^2+1)})-(\frac{2x^2}{(x^2+1)^2})$$ I proceeded to find a common denominator, and accomplished this by multiplying $\frac{1}{(x^2+1)}$ by $(x^2+1)$ as illustrated next $$\frac{1}{(x^2+1)}·\frac{x^2+1}{x^2+1} = \frac{x^2+1}{(x^2+1)^2}$$ Substituting it and resulting in $$\frac{dy}{dx}[x·\frac{1}{(x^2+1)}]=(\frac{x^2+1}{(x^2+1)^2})-(\frac{2x^2}{(x^2+1)^2})$$ $$\frac{dy}{dx}[x·\frac{1}{(x^2+1)}]=\frac{(x^2+1)-(2x^2)}{(x^2+1)^2}$$ So our final derivative is $$\frac{dy}{dx}[x·\frac{1}{(x^2+1)}]=\frac{-x^2+1}{(x^2+1)^2}$$ To find it's extreme values we set the derivative equation equal to zero $$0=\frac{-x^2+1}{(x^2+1)^2}$$ And solve for x doing the following $$0=\frac{-x^2+1}{(x^2+1)^2}$$ $$·(x^2+1)^2·(x^2+1)^2$$ $$0=-x^2+1$$ $$-1 -1$$ $$-1=-x^2$$ $$·-1 ·-1$$ $$1=x^2$$ $$\sqrt(1)\sqrt(x^2)$$ $$\pm1=x$$ So finally, we have determined the extreme values which are $x=1$ and $x=-1$ +--------------------------------+ |Known Information               | +--------------------------------+ |Zeroes : 0                      | |Vertical Asymptote : None       | |Horizontal Asymptote : y=1      | |Extreme Values : x=1 & x=-1     | |Points of Inflection :          | +--------------------------------+ 5.- Determining nature of extreme values. This is were I was completely frustrated on how long it was taking for me to determine the nature of the extreme values I had obtained (Maxima or minima), I firstly remembered that to determine this I needed to determine the second derivative of the first equation and evaluate it at the obtained extreme values , if when doing so the value resulted positive that meant that It would be a minimum value , if negative it would be a maximum value . So I would need to do as illustrated next $$y=\frac{-x^2+1}{(x^2+1)^2}$$ $$\frac{dy}{dx}=\frac{dy}{dx}[\frac{-x^2+1}{(x^2+1)^2}]$$ To do this I decided to use the quotient rule as It was easier for me to derive with no negative terms on this more complicated functions, so the quotient rule states $$\frac{dy}{dx}[\frac{f(x)}{g(x)}] = \frac{g(x)f'(x)-f(x)g'(x)}{(g(x))^2}$$ So I proceeded to derive each term of my equation, the top term resulting in $$\frac{dy}{dx}[-x^2+1] = -2x$$ And the bottom term resulting in $$\frac{dy}{dx}[(x^2+1)^2] = 2·(x^2+1)·2x$$ or $$\frac{dy}{dx}[(x^2+1)^2] = 4x(x^2+1)$$ So ended up having $$\frac{dy}{dx}[-x^2+1] = -2x$$ and $$\frac{dy}{dx}[(x^2+1)^2] = 4x(x^2+1)$$ Now proceeded to use the quotient rule $$\frac{dy}{dx}[\frac{f(x)}{g(x)}] = \frac{g(x)f'(x)-f(x)g'(x)}{(g(x))^2}$$ $$\frac{dy}{dx}[\frac{-x^2+1}{(x^2+1)^2}] = \frac{(x^2+1)^2·(-2x)-(-x^2+1)·(4x(x^2+1))}{((x^2+1)^2)^2}$$ Evaluate at our extreme values $x=1$ and $x=-1$ $$\frac{((1)^2+1)^2·(-2(1))-(-(1)^2+1)·(4(1)((1)^2+1))}{(((1)^2+1)^2)^2} = -\frac{1}{2}$$ So we can say that at point $x=1$ theres a maximum value. $$\frac{((-1)^2+-1)^2·(-2(-1))-(-(-1)^2+-1)·(4(-1)((-1)^2+-1))}{(((-1)^2+-1)^2)^2} = \frac{1}{2}$$ So we can say that at point $x=-1$ theres a minimum value. +--------------------------------+ |Known Information               | +--------------------------------+ |Zeroes : 0                      | |Vertical Asymptote : None       | |Horizontal Asymptote : y=1      | |Extreme Values:x=1 Max x=-1 Min | |Points of Inflection :          | +--------------------------------+ 6.- Determining Inflection Points It's taking me longer than expected to write this post so I'll leave for anyone interested this part as a practice, what I basically did was simplify the second derivative , in this case the following $$0 = \frac{(x^2+1)^2·(-2x)-(-x^2+1)·(4x(x^2+1))}{((x^2+1)^2)^2}$$ After taking a bunch of time simplifying it I ended up with $$2x^4+6=0$$ and guessed that for the function to be equal to zero, then 2x^4 should be equal to exactly -6 $$2x^4=-6$$ I ended up with an imaginary number so guessed that there was no points of inflection ? NOTE: This part of determining the inflection points , I'm not pretty sure that I could have come with such a value (Imaginary) , please let me know of my mistakes. +--------------------------------+ |Known Information               | +--------------------------------+ |Zeroes : 0                      | |Vertical Asymptote : None       | |Horizontal Asymptote : y=1      | |Extreme Values:x=1 Max x=-1 Min | |Points of Inflection : None?    | +--------------------------------+ 6.- Graphing With my final information table +--------------------------------+ |Known Information               | +--------------------------------+ |Zeroes : 0                      | |Vertical Asymptote : None       | |Horizontal Asymptote : y=1      | |Extreme Values:x=1 Max x=-1 Min | |Points of Inflection : None?    | +--------------------------------+ I graphed the function by using the extreme values (evaluating it's y value on the function) and the zeroes (zero) , and ended up having the following that was actually a good result. My doubts left after this entire procedure in conjunction with the main question (which I'm excluding here on the bottom) of this topic are the following Is it quite possible that I could have gotten an imaginary number as my inflection point? Is trying close numbers to the found extreme values with the purpose of determining if the point is a maxima or minima a valid argument ? And thank you very much for reading the question! (:","First of all thank you for visiting this question! I believe it's a pretty simple problem but get's kinda hairy and time consuming on each step as I have done it, so my question (the one you are here for) is the following : What is the fastest and most reliable way to sketch a given equation determining zeroes, asymptotes, extreme values, and points of inﬂection? (While doing this problem here , I realized of other doubts I had about the procedure I applied, I've listed those doubts at the bottom) I was given by the university I'm looking forward to join an example of my math admission exam and on the last section and the one that it's worth the most I encountered the following : Let Determine the zeroes, asymptotes, extreme values, and points of inﬂection of the graph. Use this information to sketch the graph of supported by complete arguments. My first idea was to create a table that I would be filling while doing this problem, the table was the following : +--------------------------------+ |Known Information               | +--------------------------------+ |Zeroes :                        | |Vertical Asymptote :            | |Horizontal Asymptote :          | |Extreme Values :                | |Points of Inflection :          | +--------------------------------+ Now I will separate each procedure by sections on the post , in the same order I did the problem. 1.- Determining the zeros I proceeded to find the zeroes by doing the following So when the equation is as well (Yes, I could have determined this by just looking at the pure fraction, but I'm trying to be as general as possible , my test might contain some other than just x) We proceed to fill the table +--------------------------------+ |Known Information               | +--------------------------------+ |Zeroes : 0                      | |Vertical Asymptote :            | |Horizontal Asymptote :          | |Extreme Values :                | |Points of Inflection :          | +--------------------------------+ 2.- Determining Vertical Asymptote I proceeded to determine the vertical asymptote by setting the denominator to zero as I illustrate next By having this I determined that there was no vertical asymptote . +--------------------------------+ |Known Information               | +--------------------------------+ |Zeroes : 0                      | |Vertical Asymptote : None       | |Horizontal Asymptote :          | |Extreme Values :                | |Points of Inflection :          | +--------------------------------+ 3.- Determining the horizontal asymptote. To do this, I knew that Given if then we have an horizontal asymptote at Consequently +--------------------------------+ |Known Information               | +--------------------------------+ |Zeroes : 0                      | |Vertical Asymptote : None       | |Horizontal Asymptote : y=1      | |Extreme Values :                | |Points of Inflection :          | +--------------------------------+ 4.- Determining the extreme values. This is where it gets time consuming and complicated , to proceed I differentiated my as illustrated next Note : On the last illustration the last term on the right is raised to the negative one power, which is equal to (I do not know why I can't express the negative one power here..) And I also knew that Given So I proceeded first to find the derivatives of each term being multiplied , the first one resulting as below : The second one resulting as below as well (Using chain rule) Or equivalently So ended up having and Proceeded to use the product rule as illustrated below I proceeded to find a common denominator, and accomplished this by multiplying by as illustrated next Substituting it and resulting in So our final derivative is To find it's extreme values we set the derivative equation equal to zero And solve for x doing the following So finally, we have determined the extreme values which are and +--------------------------------+ |Known Information               | +--------------------------------+ |Zeroes : 0                      | |Vertical Asymptote : None       | |Horizontal Asymptote : y=1      | |Extreme Values : x=1 & x=-1     | |Points of Inflection :          | +--------------------------------+ 5.- Determining nature of extreme values. This is were I was completely frustrated on how long it was taking for me to determine the nature of the extreme values I had obtained (Maxima or minima), I firstly remembered that to determine this I needed to determine the second derivative of the first equation and evaluate it at the obtained extreme values , if when doing so the value resulted positive that meant that It would be a minimum value , if negative it would be a maximum value . So I would need to do as illustrated next To do this I decided to use the quotient rule as It was easier for me to derive with no negative terms on this more complicated functions, so the quotient rule states So I proceeded to derive each term of my equation, the top term resulting in And the bottom term resulting in or So ended up having and Now proceeded to use the quotient rule Evaluate at our extreme values and So we can say that at point theres a maximum value. So we can say that at point theres a minimum value. +--------------------------------+ |Known Information               | +--------------------------------+ |Zeroes : 0                      | |Vertical Asymptote : None       | |Horizontal Asymptote : y=1      | |Extreme Values:x=1 Max x=-1 Min | |Points of Inflection :          | +--------------------------------+ 6.- Determining Inflection Points It's taking me longer than expected to write this post so I'll leave for anyone interested this part as a practice, what I basically did was simplify the second derivative , in this case the following After taking a bunch of time simplifying it I ended up with and guessed that for the function to be equal to zero, then 2x^4 should be equal to exactly -6 I ended up with an imaginary number so guessed that there was no points of inflection ? NOTE: This part of determining the inflection points , I'm not pretty sure that I could have come with such a value (Imaginary) , please let me know of my mistakes. +--------------------------------+ |Known Information               | +--------------------------------+ |Zeroes : 0                      | |Vertical Asymptote : None       | |Horizontal Asymptote : y=1      | |Extreme Values:x=1 Max x=-1 Min | |Points of Inflection : None?    | +--------------------------------+ 6.- Graphing With my final information table +--------------------------------+ |Known Information               | +--------------------------------+ |Zeroes : 0                      | |Vertical Asymptote : None       | |Horizontal Asymptote : y=1      | |Extreme Values:x=1 Max x=-1 Min | |Points of Inflection : None?    | +--------------------------------+ I graphed the function by using the extreme values (evaluating it's y value on the function) and the zeroes (zero) , and ended up having the following that was actually a good result. My doubts left after this entire procedure in conjunction with the main question (which I'm excluding here on the bottom) of this topic are the following Is it quite possible that I could have gotten an imaginary number as my inflection point? Is trying close numbers to the found extreme values with the purpose of determining if the point is a maxima or minima a valid argument ? And thank you very much for reading the question! (:",f(x) = \frac{x}{x^2+1} f(x) \frac{x}{x^2+1}=0 ·(x^2+1) ·(x^2+1) x=0 x=0 0 x^2+1=0 -1 -1 x^2=-1 \frac{t^a}{d^b} b>a y=0 f(x) y = \frac{x}{x^2+1} \frac{dy}{dx}=\frac{dy}{dx}[\frac{x}{x^2+1}] \frac{dy}{dx}[\frac{x}{x^2+1}] = \frac{dy}{dx}[x·((x^2+1)^-1)] \frac{1}{x^2+1} \frac{dy}{dx}[f(x)g(x)]=f'(x)g(x)+g'(x)f(x) \frac{dy}{dx}[x] = 1 \frac{dy}{dx}[(x^2+1)^-1] = -1((x^2+1)^-2)·2x \frac{dy}{dx}[(x^2+1)^-1] = -\frac{2x}{(x^2+1)^2} \frac{dy}{dx}[x] = 1 \frac{dy}{dx}[(x^2+1)^-1] = -\frac{2x}{(x^2+1)^2} \frac{dy}{dx}[f(x)g(x)]=f'(x)g(x)+g'(x)f(x) \frac{dy}{dx}[x·\frac{1}{(x^2+1)}]=(1·(\frac{1}{(x^2+1)}))+(-\frac{2x}{(x^2+1)^2} · x) \frac{dy}{dx}[x·\frac{1}{(x^2+1)}]=(\frac{1}{(x^2+1)})-(\frac{2x^2}{(x^2+1)^2}) \frac{1}{(x^2+1)} (x^2+1) \frac{1}{(x^2+1)}·\frac{x^2+1}{x^2+1} = \frac{x^2+1}{(x^2+1)^2} \frac{dy}{dx}[x·\frac{1}{(x^2+1)}]=(\frac{x^2+1}{(x^2+1)^2})-(\frac{2x^2}{(x^2+1)^2}) \frac{dy}{dx}[x·\frac{1}{(x^2+1)}]=\frac{(x^2+1)-(2x^2)}{(x^2+1)^2} \frac{dy}{dx}[x·\frac{1}{(x^2+1)}]=\frac{-x^2+1}{(x^2+1)^2} 0=\frac{-x^2+1}{(x^2+1)^2} 0=\frac{-x^2+1}{(x^2+1)^2} ·(x^2+1)^2·(x^2+1)^2 0=-x^2+1 -1 -1 -1=-x^2 ·-1 ·-1 1=x^2 \sqrt(1)\sqrt(x^2) \pm1=x x=1 x=-1 y=\frac{-x^2+1}{(x^2+1)^2} \frac{dy}{dx}=\frac{dy}{dx}[\frac{-x^2+1}{(x^2+1)^2}] \frac{dy}{dx}[\frac{f(x)}{g(x)}] = \frac{g(x)f'(x)-f(x)g'(x)}{(g(x))^2} \frac{dy}{dx}[-x^2+1] = -2x \frac{dy}{dx}[(x^2+1)^2] = 2·(x^2+1)·2x \frac{dy}{dx}[(x^2+1)^2] = 4x(x^2+1) \frac{dy}{dx}[-x^2+1] = -2x \frac{dy}{dx}[(x^2+1)^2] = 4x(x^2+1) \frac{dy}{dx}[\frac{f(x)}{g(x)}] = \frac{g(x)f'(x)-f(x)g'(x)}{(g(x))^2} \frac{dy}{dx}[\frac{-x^2+1}{(x^2+1)^2}] = \frac{(x^2+1)^2·(-2x)-(-x^2+1)·(4x(x^2+1))}{((x^2+1)^2)^2} x=1 x=-1 \frac{((1)^2+1)^2·(-2(1))-(-(1)^2+1)·(4(1)((1)^2+1))}{(((1)^2+1)^2)^2} = -\frac{1}{2} x=1 \frac{((-1)^2+-1)^2·(-2(-1))-(-(-1)^2+-1)·(4(-1)((-1)^2+-1))}{(((-1)^2+-1)^2)^2} = \frac{1}{2} x=-1 0 = \frac{(x^2+1)^2·(-2x)-(-x^2+1)·(4x(x^2+1))}{((x^2+1)^2)^2} 2x^4+6=0 2x^4=-6,"['calculus', 'derivatives', 'graphing-functions']"
66,Pointwise boundedness is uniform for a sequence of derivatives,Pointwise boundedness is uniform for a sequence of derivatives,,"Let $f \in C^\infty ([a, b])$ be an infinitely differentiable function defined on a closed interval $[a, b]$ with the following property: for any $x \in [a, b]$ the sequence $|f^{(n)}(x)|$ is bounded, i.e.: $$ \forall x \in [a, b] \;\; \exists C_x \in \mathbb R \;\; \forall n \geq 0 \;\; |f^{(n)}(x)| \leq C_x $$ Prove that there is a constant $C$ such that $|f^{(n)}(x)| \leq C$ for all $n \geq 0$ and $x \in [a, b]$. Clarification: the derivatives are defined on closed intervals by taking one-side limits. My ideas: $C_x$ is actually a function if we set: $$ C_x := \sup_{n \geq 0} |f^{(n)}(x)| $$ The task is to prove that this function is bounded on $[a, b]$. By contradiction? Suppose $C_x$ is unbounded. Then there is a sequence $(n_k, x_k)$ such that $$ \lim_{k \to \infty} |f^{(n_k)}(x_k)| = \infty $$ $n_k$ cannot be bounded since otherwise there would be infinitely many pairs $(n_0, x_k)$ for some $n_0$. The limit above would still be infinity for that subsequence, but for a fixed function $f^{(n_0)}$ that is impossible since it is a continuous function defined on a closed interval. The sequence $(n_k, x_k)$ can be chosen in such way that: $n_k$ are strictly increasing $x_k$ converge to some $x_0 \in [a, b]$ However, I have no idea how to use this sequence. Any advice? What direction should I look in?","Let $f \in C^\infty ([a, b])$ be an infinitely differentiable function defined on a closed interval $[a, b]$ with the following property: for any $x \in [a, b]$ the sequence $|f^{(n)}(x)|$ is bounded, i.e.: $$ \forall x \in [a, b] \;\; \exists C_x \in \mathbb R \;\; \forall n \geq 0 \;\; |f^{(n)}(x)| \leq C_x $$ Prove that there is a constant $C$ such that $|f^{(n)}(x)| \leq C$ for all $n \geq 0$ and $x \in [a, b]$. Clarification: the derivatives are defined on closed intervals by taking one-side limits. My ideas: $C_x$ is actually a function if we set: $$ C_x := \sup_{n \geq 0} |f^{(n)}(x)| $$ The task is to prove that this function is bounded on $[a, b]$. By contradiction? Suppose $C_x$ is unbounded. Then there is a sequence $(n_k, x_k)$ such that $$ \lim_{k \to \infty} |f^{(n_k)}(x_k)| = \infty $$ $n_k$ cannot be bounded since otherwise there would be infinitely many pairs $(n_0, x_k)$ for some $n_0$. The limit above would still be infinity for that subsequence, but for a fixed function $f^{(n_0)}$ that is impossible since it is a continuous function defined on a closed interval. The sequence $(n_k, x_k)$ can be chosen in such way that: $n_k$ are strictly increasing $x_k$ converge to some $x_0 \in [a, b]$ However, I have no idea how to use this sequence. Any advice? What direction should I look in?",,"['real-analysis', 'derivatives']"
67,"Let P(x) be a polynomial of degree 4 , having extremum at $x=1,x=2$ and $\lim_{x\to 0}\frac{x^2+P(x)}{x^2}=2$ Then the value of P(2)","Let P(x) be a polynomial of degree 4 , having extremum at  and  Then the value of P(2)","x=1,x=2 \lim_{x\to 0}\frac{x^2+P(x)}{x^2}=2","Let $P(x)$ be a polynomial of degree 4 , having extremum at $x=1,x=2$ and $$\lim_{x\to 0}\frac{x^2+P(x)}{x^2}=2.$$ Then what is the value of $P(2)$? I worked out the limit using L'Hospital got a relation in terms of second derivative of $P$; the other derivative relations are that first derivatives are zero at 1,2. How can we interpretate these derivative equations to find the function? Any help is welcome.","Let $P(x)$ be a polynomial of degree 4 , having extremum at $x=1,x=2$ and $$\lim_{x\to 0}\frac{x^2+P(x)}{x^2}=2.$$ Then what is the value of $P(2)$? I worked out the limit using L'Hospital got a relation in terms of second derivative of $P$; the other derivative relations are that first derivatives are zero at 1,2. How can we interpretate these derivative equations to find the function? Any help is welcome.",,"['calculus', 'derivatives', 'polynomials']"
68,"$\frac{d^{\frac{1}{2}}}{dx^{\frac{1}{2}}}$, what does it mean?",", what does it mean?",\frac{d^{\frac{1}{2}}}{dx^{\frac{1}{2}}},"I would like to know what these fractional exponents means in a derivative $\frac{d}{dx}$ operator. Like, I've seen $\frac{d^2}{dx^2}$ but I don't know what $\frac{d^{\frac{1}{2}}}{dx^{\frac{1}{2}}}$ means.","I would like to know what these fractional exponents means in a derivative $\frac{d}{dx}$ operator. Like, I've seen $\frac{d^2}{dx^2}$ but I don't know what $\frac{d^{\frac{1}{2}}}{dx^{\frac{1}{2}}}$ means.",,"['derivatives', 'fractional-calculus']"
69,"Prove that there are $n$ pairwise distinct numbers $t_1,t_2,...,t_n$ in $(0,1)$, such that $\sum_{i=1}^n\frac{k_i}{f'(t_i)}=\sum_{i=1}^n k_i$","Prove that there are  pairwise distinct numbers  in , such that","n t_1,t_2,...,t_n (0,1) \sum_{i=1}^n\frac{k_i}{f'(t_i)}=\sum_{i=1}^n k_i","Let $f$ be a function that is continuous on $[0,1]$ and differentiable on $(0,1)$ with $f(0)=0$ and $f(1)=1$ and let $k_1,k_2,...,k_n$ be any positive numbers. Prove that there are $n$ pairwise distinct numbers $t_1,t_2,...,t_n$ in $(0,1)$ such that $$\sum_{i=1}^{n}{\frac{k_i}{f^{\prime}(t_i)}}=\sum_{i=1}^{n}{k_i}$$ My attempt: By MVT, there exists $c \in (0,1)$ such that $$f^{\prime}(c)=\dfrac{f(1)-f(0)}{1-0}=1$$ My aim is to show that for all $t_i$, $1 \leq i \leq n$, $f^{\prime}(t_i)=1$ because this seems to be the most natural way to prove it. But I can only prove for one number only. Also I don't understand what $t_i$ intuitively is. Can anyone guide me on this question?","Let $f$ be a function that is continuous on $[0,1]$ and differentiable on $(0,1)$ with $f(0)=0$ and $f(1)=1$ and let $k_1,k_2,...,k_n$ be any positive numbers. Prove that there are $n$ pairwise distinct numbers $t_1,t_2,...,t_n$ in $(0,1)$ such that $$\sum_{i=1}^{n}{\frac{k_i}{f^{\prime}(t_i)}}=\sum_{i=1}^{n}{k_i}$$ My attempt: By MVT, there exists $c \in (0,1)$ such that $$f^{\prime}(c)=\dfrac{f(1)-f(0)}{1-0}=1$$ My aim is to show that for all $t_i$, $1 \leq i \leq n$, $f^{\prime}(t_i)=1$ because this seems to be the most natural way to prove it. But I can only prove for one number only. Also I don't understand what $t_i$ intuitively is. Can anyone guide me on this question?",,"['calculus', 'derivatives']"
70,Does there exist a bounded function of class $C^{\infty}$ such that all of its derivatives at $0$ coincide with the derivatives of $e^{-x} +x+1$?,Does there exist a bounded function of class  such that all of its derivatives at  coincide with the derivatives of ?,C^{\infty} 0 e^{-x} +x+1,"Consider the function $g:\mathbb{R}\to\mathbb{R}$ where $g(x) = e^{-x} + x + 1$. Does there exist a bounded function $f:\mathbb{R}\to\mathbb{R}$ of class $C^{\infty}$ such that $f^{(n)}(0) = g^{(n)}(0)$ for all $n = 0, 1, 2, \ldots$? Here, $f^{(n)}(x)$ denotes the nth derivative of $f$ at $x$, and $f^{(0)}(x) = f(x)$.","Consider the function $g:\mathbb{R}\to\mathbb{R}$ where $g(x) = e^{-x} + x + 1$. Does there exist a bounded function $f:\mathbb{R}\to\mathbb{R}$ of class $C^{\infty}$ such that $f^{(n)}(0) = g^{(n)}(0)$ for all $n = 0, 1, 2, \ldots$? Here, $f^{(n)}(x)$ denotes the nth derivative of $f$ at $x$, and $f^{(0)}(x) = f(x)$.",,"['real-analysis', 'derivatives']"
71,"$f$ is differentiable at any quadratic irrational $a\in (0,\infty)\setminus\mathbb{Q}$. (""How to learn mathematics"" edited by Kunihiko Kodaira.)","is differentiable at any quadratic irrational . (""How to learn mathematics"" edited by Kunihiko Kodaira.)","f a\in (0,\infty)\setminus\mathbb{Q}","I am reading ""How to learn mathematics"" (in Japanese) edited by Kunihiko Kodaira. Kodaira wrote as follows (I used ChatGPT3.5 for this English translation.): I attended Professor Teiji Takagi's lecture on introductory analysis during my first year of university. One of the practice problems was about the continuity of a function $f(x)$ defined as follows: for the interval $x > 0$ , if $x$ is irrational, then $f(x) = 0$ , and if $x = p/q$ is a rational number (where $p/q$ is in reduced form and $q > 0$ ), then $f(x) = 1/q$ . As is well known, $f(x)$ is discontinuous at $x = a$ if $a$ is rational and continuous at $x = a$ if $a$ is irrational. I remember realizing that by slightly modifying the definition of $f(x)$ to $f(x) = 0$ for irrational $x$ and $f(x) = 1/q^3$ for $x = p/q$ (where $p/q$ is a rational number in reduced form with $q > 0$ ), $f(x)$ is discontinuous at $x = a$ if $a$ is rational, continuous at $x = a$ if $a$ is irrational, and further, differentiable at $x = a$ if $a$ is a quadratic irrational. Since then, I began to consider alternative proofs for theorems and to explore variations of problems. Let $f:(0,\infty)\to\mathbb{R}$ be the function such that $f(x) = 0$ for $x\in (0,\infty)\setminus\mathbb{Q}$ and $f(x) =\frac{1}{q^3}$ for $x\in (0,\infty)\cap\mathbb{Q}$ , where $x=\frac{p}{q}$ and $\frac{p}{q}$ is in reduced form. It is obvious that $f$ is not continuous at $a\in (0,\infty)\cap\mathbb{Q}$ . Let $a\in (0,\infty)\setminus\mathbb{Q}$ . Let $\varepsilon$ be an arbitrary positive real number. Let $N$ be a positive integer such that $\frac{1}{N^3}<\varepsilon$ . Let $d_i$ be the distance between $a$ and the rational number whose denominator is $i$ and which is closest to $a$ for each positive integer $i$ . Let $d:=\min\{d_1,\dots,d_N\}$ . Let $\delta$ be a positive real number such that $\delta<d$ . If $|x-a|<\delta$ , then $|f(x)-f(a)|=|f(x)|<\frac{1}{N^3}<\varepsilon$ . So, $f$ is continuous at $a\in (0,\infty)\setminus\mathbb{Q}$ . But I cannot prove that $f$ is differentiable at any quadratic irrational $a\in (0,\infty)\setminus\mathbb{Q}$ . Please tell me a proof of this fact.","I am reading ""How to learn mathematics"" (in Japanese) edited by Kunihiko Kodaira. Kodaira wrote as follows (I used ChatGPT3.5 for this English translation.): I attended Professor Teiji Takagi's lecture on introductory analysis during my first year of university. One of the practice problems was about the continuity of a function defined as follows: for the interval , if is irrational, then , and if is a rational number (where is in reduced form and ), then . As is well known, is discontinuous at if is rational and continuous at if is irrational. I remember realizing that by slightly modifying the definition of to for irrational and for (where is a rational number in reduced form with ), is discontinuous at if is rational, continuous at if is irrational, and further, differentiable at if is a quadratic irrational. Since then, I began to consider alternative proofs for theorems and to explore variations of problems. Let be the function such that for and for , where and is in reduced form. It is obvious that is not continuous at . Let . Let be an arbitrary positive real number. Let be a positive integer such that . Let be the distance between and the rational number whose denominator is and which is closest to for each positive integer . Let . Let be a positive real number such that . If , then . So, is continuous at . But I cannot prove that is differentiable at any quadratic irrational . Please tell me a proof of this fact.","f(x) x > 0 x f(x) = 0 x = p/q p/q q > 0 f(x) = 1/q f(x) x = a a x = a a f(x) f(x) = 0 x f(x) = 1/q^3 x = p/q p/q q > 0 f(x) x = a a x = a a x = a a f:(0,\infty)\to\mathbb{R} f(x) = 0 x\in (0,\infty)\setminus\mathbb{Q} f(x) =\frac{1}{q^3} x\in (0,\infty)\cap\mathbb{Q} x=\frac{p}{q} \frac{p}{q} f a\in (0,\infty)\cap\mathbb{Q} a\in (0,\infty)\setminus\mathbb{Q} \varepsilon N \frac{1}{N^3}<\varepsilon d_i a i a i d:=\min\{d_1,\dots,d_N\} \delta \delta<d |x-a|<\delta |f(x)-f(a)|=|f(x)|<\frac{1}{N^3}<\varepsilon f a\in (0,\infty)\setminus\mathbb{Q} f a\in (0,\infty)\setminus\mathbb{Q}","['calculus', 'derivatives', 'continuity']"
72,Is differentiability the same as continuity of the derivative?,Is differentiability the same as continuity of the derivative?,,"I've seen an answer or two on this, but they don't fully make sense with me. One example is that $sin\frac{1}{x}$ is not differentiable at $x=0$ , but then isn't the derivative: $$-\frac{cos\frac{1}{x}}{x^2}$$ This is not continuous at $x=0$ as well. Can someone explain this, or give an example of when the differentiability of a function is not the same as the continuity of the derivative?","I've seen an answer or two on this, but they don't fully make sense with me. One example is that is not differentiable at , but then isn't the derivative: This is not continuous at as well. Can someone explain this, or give an example of when the differentiability of a function is not the same as the continuity of the derivative?",sin\frac{1}{x} x=0 -\frac{cos\frac{1}{x}}{x^2} x=0,"['calculus', 'derivatives', 'continuity']"
73,Does the Zygmund class on a closed interval include all everywhere differentiable functions?,Does the Zygmund class on a closed interval include all everywhere differentiable functions?,,"Let $\mathcal{C}$ be the class of continuous functions in the Zygmund class on the closed interval $[0, 1]$ . In this question, a continuous function $f$ is in the Zygmund class if there is a constant $c>0$ such that— $$|f(x) + f(y) - 2f((x+y)/2)| \le c\epsilon, $$ for every $\epsilon>0$ , whenever $x$ and $y$ are points in $f$ 's domain such that $|x-y|\le\epsilon$ . Then, does $\mathcal{C}$ include every function that is differentiable everywhere on $[0, 1]$ ? (I know the answer is yes for functions that are continuously differentiable everywhere on $[0, 1]$ .) If not, what is an example of an everywhere differentiable function on a closed interval that is not in the Zygmund class? To be clear, this question is not homework or a self-study assignment.","Let be the class of continuous functions in the Zygmund class on the closed interval . In this question, a continuous function is in the Zygmund class if there is a constant such that— for every , whenever and are points in 's domain such that . Then, does include every function that is differentiable everywhere on ? (I know the answer is yes for functions that are continuously differentiable everywhere on .) If not, what is an example of an everywhere differentiable function on a closed interval that is not in the Zygmund class? To be clear, this question is not homework or a self-study assignment.","\mathcal{C} [0, 1] f c>0 |f(x) + f(y) - 2f((x+y)/2)| \le c\epsilon,  \epsilon>0 x y f |x-y|\le\epsilon \mathcal{C} [0, 1] [0, 1]","['real-analysis', 'derivatives']"
74,Last attempt about the Cirtoaje's inequality $ x^{\left(2\left(1-x\right)\right)^{k}}+(1-x)^{\left(2\left(x\right)\right)^{k}}\leq 1$,Last attempt about the Cirtoaje's inequality, x^{\left(2\left(1-x\right)\right)^{k}}+(1-x)^{\left(2\left(x\right)\right)^{k}}\leq 1,"Claim : Let $0.5-\varepsilon_k\leq x\leq 0.5+\varepsilon_k$ and $0<\varepsilon_k\leq 0.25$ with $k\geq 2$ then prove or disprove that : $$\frac{\left(2^{-\left(2\left(1-x\right)\right)^{k}}\cdot x\right)}{1-2^{\left(k-1\right)\ln\left(2\right)}\left(\left(1-x\right)\cdot2\cdot x\right)^{\left(\left(k-1\right)\ln\left(2\right)+1\right)}}\geq x^{\left(2\left(1-x\right)\right)^{k}}$$ Background : Again and I can say it's my last attempt on it I try to show : Let $0<x<1$ and $k\geq 1$ then we have : $$ x^{\left(2\left(1-x\right)\right)^{k}}+(1-x)^{\left(2\left(x\right)\right)^{k}}\leq 1$$ To show it we can directly use the Bernoulli's inequality on $0.5+\beta_k\leq x\leq1$ and use the claim if proved on $0.5-\varepsilon_k\leq x\leq 0.5+\varepsilon_k$ so the two inequalities complements each other .And the last inequality resulting from the claim is really easier than the Cirtoaje's inequality . To finish I have tried to show the claim using logarithm and derivatives without reach the goal Case $k=2$ : As first step I simply use convexity because we have the inequality on $x\in[0.5,0.55]$ : $$\left(\frac{\left(r\left(0.5\right)-r\left(0.55\right)\right)}{-0.05}\left(x-0.5\right)+r\left(0.5\right)\right)^{2}\cdot x\leq \left(2^{-\left(2\left(1-x\right)\right)^{2}}\cdot x\right)$$ Where : $$r\left(x\right)=\left(2^{-\left(2^{0.5}\left(1-x\right)\right)^{2}}\right)$$ Then it seems we have on $x\in[0.54,0.55]$ : $$\left(\frac{\left(r\left(0.5\right)-r\left(0.55\right)\right)}{-0.05}\left(x-0.5\right)+r\left(0.5\right)\right)^{2}\geq \left(1-2^{\left(2-1\right)\ln\left(2\right)}\left(\left(1-x\right)\cdot2\cdot x\right)^{\left(\left(2-1\right)\ln\left(2\right)+1\right)}\right)\cdot x^{\left(4\left(1-x\right)^{2}\right)-1}$$ But It's really not convincing...because the derivatives are not equal at $x=0.5$ A better way would be to re-write the inequality as : $$f\left(x\right)=\frac{0.5}{1-2^{\left(2-1\right)\ln\left(2\right)}\left(\left(1-x\right)\cdot2\cdot x\right)^{\left(\left(2-1\right)\ln\left(2\right)+1\right)}}\geq g(x)=\left(\left(2x\right)^{\left(\left(2\left(1-x\right)\right)^{2}-1\right)}\right)$$ Then take the logarithm ,make the difference and see what happens with : $$\frac{d}{dx}(\ln\left(f\left(x)\right)-\ln\left(g\left(x\right)\right)\right)$$ It seems that the derivative of the difference admits three roots whose at $x=0.5$ .It seems it's also the case in general . We can also substitute $y=2x$ . Last edit in this case ( $k=2$ ) : Using concavity and chord  we have $x\in[0.5,0.55]$ : $\left(\frac{\left(f\left(0.5\right)-f\left(0.55\right)\right)}{-0.05}\left(x-0.5\right)+f\left(0.5\right)\right)^{\frac{1}{a\ln^{2}\left(1-x\right)}}\leq \frac{1}{1-2^{\left(2-1\right)\ln\left(2\right)}\left(\left(1-x\right)\cdot2\cdot x\right)^{\left(\left(2-1\right)\ln\left(2\right)+1\right)}}\quad (I)$ Where : $$f\left(x\right)=\left(\frac{1}{1-2^{\left(2-1\right)\ln\left(2\right)}\left(\left(1-x\right)\cdot2\cdot x\right)^{\left(\left(2-1\right)\ln\left(2\right)+1\right)}}\right)^{a\ln^{2}\left(1-x\right)}$$ And $a$ evaluate as follow let: $$g\left(x\right)=\left(\frac{\left(f\left(0.5\right)-f\left(0.55\right)\right)}{-0.05}\left(x-0.5\right)+f\left(0.5\right)\right)^{\frac{1}{a\ln^{2}\left(1-x\right)}}$$ $$h(x)=(g(x))'$$ Then : $$h(0.5)=0$$ Now it seems we have $x\in[0.5,0.55]$ : $$\left(\frac{\left(f\left(0.5\right)-f\left(0.55\right)\right)}{-0.05}\left(x-0.5\right)+f\left(0.5\right)\right)^{\frac{1}{a\ln^{2}\left(1-x\right)}}\cdot0.5\geq  \left(2x\right)^{\left(\left(2\left(1-x\right)\right)^{2}-1\right)}$$ Ps : I change the last edit because before it was useless and not interesting now it's better . PPS: The inequality $(I)$ must be restricted on $x\in[0.5,0.515]$ where the function $f(x)$ seems to be concave . Question : How to prove or disprove the claim ? How to determine $\varepsilon_k,\beta_k$ with some accuracy  ? Thanks for your effort in this sense .","Claim : Let and with then prove or disprove that : Background : Again and I can say it's my last attempt on it I try to show : Let and then we have : To show it we can directly use the Bernoulli's inequality on and use the claim if proved on so the two inequalities complements each other .And the last inequality resulting from the claim is really easier than the Cirtoaje's inequality . To finish I have tried to show the claim using logarithm and derivatives without reach the goal Case : As first step I simply use convexity because we have the inequality on : Where : Then it seems we have on : But It's really not convincing...because the derivatives are not equal at A better way would be to re-write the inequality as : Then take the logarithm ,make the difference and see what happens with : It seems that the derivative of the difference admits three roots whose at .It seems it's also the case in general . We can also substitute . Last edit in this case ( ) : Using concavity and chord  we have : Where : And evaluate as follow let: Then : Now it seems we have : Ps : I change the last edit because before it was useless and not interesting now it's better . PPS: The inequality must be restricted on where the function seems to be concave . Question : How to prove or disprove the claim ? How to determine with some accuracy  ? Thanks for your effort in this sense .","0.5-\varepsilon_k\leq x\leq 0.5+\varepsilon_k 0<\varepsilon_k\leq 0.25 k\geq 2 \frac{\left(2^{-\left(2\left(1-x\right)\right)^{k}}\cdot x\right)}{1-2^{\left(k-1\right)\ln\left(2\right)}\left(\left(1-x\right)\cdot2\cdot x\right)^{\left(\left(k-1\right)\ln\left(2\right)+1\right)}}\geq x^{\left(2\left(1-x\right)\right)^{k}} 0<x<1 k\geq 1  x^{\left(2\left(1-x\right)\right)^{k}}+(1-x)^{\left(2\left(x\right)\right)^{k}}\leq 1 0.5+\beta_k\leq x\leq1 0.5-\varepsilon_k\leq x\leq 0.5+\varepsilon_k k=2 x\in[0.5,0.55] \left(\frac{\left(r\left(0.5\right)-r\left(0.55\right)\right)}{-0.05}\left(x-0.5\right)+r\left(0.5\right)\right)^{2}\cdot x\leq \left(2^{-\left(2\left(1-x\right)\right)^{2}}\cdot x\right) r\left(x\right)=\left(2^{-\left(2^{0.5}\left(1-x\right)\right)^{2}}\right) x\in[0.54,0.55] \left(\frac{\left(r\left(0.5\right)-r\left(0.55\right)\right)}{-0.05}\left(x-0.5\right)+r\left(0.5\right)\right)^{2}\geq \left(1-2^{\left(2-1\right)\ln\left(2\right)}\left(\left(1-x\right)\cdot2\cdot x\right)^{\left(\left(2-1\right)\ln\left(2\right)+1\right)}\right)\cdot x^{\left(4\left(1-x\right)^{2}\right)-1} x=0.5 f\left(x\right)=\frac{0.5}{1-2^{\left(2-1\right)\ln\left(2\right)}\left(\left(1-x\right)\cdot2\cdot x\right)^{\left(\left(2-1\right)\ln\left(2\right)+1\right)}}\geq g(x)=\left(\left(2x\right)^{\left(\left(2\left(1-x\right)\right)^{2}-1\right)}\right) \frac{d}{dx}(\ln\left(f\left(x)\right)-\ln\left(g\left(x\right)\right)\right) x=0.5 y=2x k=2 x\in[0.5,0.55] \left(\frac{\left(f\left(0.5\right)-f\left(0.55\right)\right)}{-0.05}\left(x-0.5\right)+f\left(0.5\right)\right)^{\frac{1}{a\ln^{2}\left(1-x\right)}}\leq \frac{1}{1-2^{\left(2-1\right)\ln\left(2\right)}\left(\left(1-x\right)\cdot2\cdot x\right)^{\left(\left(2-1\right)\ln\left(2\right)+1\right)}}\quad (I) f\left(x\right)=\left(\frac{1}{1-2^{\left(2-1\right)\ln\left(2\right)}\left(\left(1-x\right)\cdot2\cdot x\right)^{\left(\left(2-1\right)\ln\left(2\right)+1\right)}}\right)^{a\ln^{2}\left(1-x\right)} a g\left(x\right)=\left(\frac{\left(f\left(0.5\right)-f\left(0.55\right)\right)}{-0.05}\left(x-0.5\right)+f\left(0.5\right)\right)^{\frac{1}{a\ln^{2}\left(1-x\right)}} h(x)=(g(x))' h(0.5)=0 x\in[0.5,0.55] \left(\frac{\left(f\left(0.5\right)-f\left(0.55\right)\right)}{-0.05}\left(x-0.5\right)+f\left(0.5\right)\right)^{\frac{1}{a\ln^{2}\left(1-x\right)}}\cdot0.5\geq  \left(2x\right)^{\left(\left(2\left(1-x\right)\right)^{2}-1\right)} (I) x\in[0.5,0.515] f(x) \varepsilon_k,\beta_k","['derivatives', 'inequality']"
75,Calculating of derivative,Calculating of derivative,,I'm a little confused with using derivatives. I understand that derivatives respond to question how result of function changes when input of function changes However I do not understand why we calculate derivative of $f(x) = x^2$ instead of calculating $f(2)$ and $f(2.001)$ and subtracting these two values of $f(2.001)$ from $f(2.0)$ Could someone explain that?,I'm a little confused with using derivatives. I understand that derivatives respond to question how result of function changes when input of function changes However I do not understand why we calculate derivative of instead of calculating and and subtracting these two values of from Could someone explain that?,f(x) = x^2 f(2) f(2.001) f(2.001) f(2.0),['derivatives']
76,Find derivative of $\lfloor{x}\rfloor$ in distribution,Find derivative of  in distribution,\lfloor{x}\rfloor,Find derivative in distribution of $f(x)=\lfloor{x}\rfloor=E(x)$ $$E(x)≤x≤E(x+1)$$ Answer is : $$\lfloor{x}\rfloor '=\displaystyle\sum_{k=-\infty}^{\infty}\delta_{k}$$ I don't have any idea about how to. Can you assist? I'm too thankful,Find derivative in distribution of Answer is : I don't have any idea about how to. Can you assist? I'm too thankful,f(x)=\lfloor{x}\rfloor=E(x) E(x)≤x≤E(x+1) \lfloor{x}\rfloor '=\displaystyle\sum_{k=-\infty}^{\infty}\delta_{k},"['real-analysis', 'derivatives', 'distribution-theory']"
77,function defined by its derivative at a specific point,function defined by its derivative at a specific point,,"I was solving a physics problem about a falling element that gets air resistance. In that case, the acceleration, which is the derivative of velocity, is defined as $$a=\frac{mg-bv}{m}$$ And, m,g,b is all a constant. Let's define a function $v(t)=\text{the velocity at the time t}$ Then, we can define the derivative of velocity( $a$ ) with the expression written above. Like this. $$\frac{dv(t)}{dt}=\frac{mg-bv(t)}{m}$$ Now, we can define the physics problem I was solving by pure math. Abstracted problem function v(t) is defined as $$v(t)=\frac{mg-m\frac{dv(t)}{dt}}{b}$$ Can v(t) be defined without using it's derivative? If it can't, why? If it can, how can it be done, and what is the definition? I am new pretty new to calculus and also physics. This question might be a stupid or a simple question. In that case, sorry. Thanks in advance.","I was solving a physics problem about a falling element that gets air resistance. In that case, the acceleration, which is the derivative of velocity, is defined as And, m,g,b is all a constant. Let's define a function Then, we can define the derivative of velocity( ) with the expression written above. Like this. Now, we can define the physics problem I was solving by pure math. Abstracted problem function v(t) is defined as Can v(t) be defined without using it's derivative? If it can't, why? If it can, how can it be done, and what is the definition? I am new pretty new to calculus and also physics. This question might be a stupid or a simple question. In that case, sorry. Thanks in advance.",a=\frac{mg-bv}{m} v(t)=\text{the velocity at the time t} a \frac{dv(t)}{dt}=\frac{mg-bv(t)}{m} v(t)=\frac{mg-m\frac{dv(t)}{dt}}{b},"['calculus', 'derivatives', 'physics']"
78,Rolle's theorem for second derivative,Rolle's theorem for second derivative,,"A problem asks the following $f$ is a twice-differentiable function on some segment $[a,b]$ such that $f(a)=f(b)$ and $f'(a)f'(b)<0$ .  it asks to prove that the second derivative of $f$ vanishes at some point between $a$ and $b$ (strictly). What about this situation",A problem asks the following is a twice-differentiable function on some segment such that and .  it asks to prove that the second derivative of vanishes at some point between and (strictly). What about this situation,"f [a,b] f(a)=f(b) f'(a)f'(b)<0 f a b","['derivatives', 'rolles-theorem']"
79,"In the derivative notation $f'(x)$, does the $(x)$ mean ""with respect to $x$"" or something else?","In the derivative notation , does the  mean ""with respect to "" or something else?",f'(x) (x) x,"I know $d/dx$ means derivative with respect to x (perhaps I am a little unclear on what precisely that means - I'm not quite sure - but I do think I have at least some sense of its meaning). I know (I think!) that... $$\frac{d}{dx} f(x)=f'(x)$$ Here is my question (it's been bugging me incessantly for the last couple weeks): What does $f'(x)$ mean? Does it mean the derivative of $f$ with respect to x ? Or, rather, does it mean the derivative of $f$ with respect to x , evaluated at the point x ? Or something totally different? To make my question more clear, let me ask this too: What does $f'(a)$ mean? Does it mean the derivative with respect to $a$ ? Or the derivative with respect to $x$ evaluated at some point $a$ ? (so that the ""with respect to x "" is actually ""encoded"" in the $f'$ part of the notation!). Etc. Even worse, take $f'(ax)$ , which appears in some derivative computation rules. Does that mean derivative with respect to $ax$ ?? I suspect this confusion may be somewhat related to the maddeningly persistent confusion -- both for me and virtually everyone else -- between a function and its value at a point . Also, teachers tend to use somewhat imprecise notation and language, so I, being someone who likes precision, can sometimes get confused. While I'm at it, I'll note that this confusion may be related to my additional confusion over language like ""the derivative of the sum of two functions,"" where the two ""functions"" are, say, $x^2$ and $x^3$ . But I thought those were mere polynomials , not functions. One might say $f(x)=x^2$ , but one would never say the function itself , $f$ , was equal to $x^2$ , right? I'm confused. We shouldn't say things like ""the function $x^2$ ,"" right? :(","I know means derivative with respect to x (perhaps I am a little unclear on what precisely that means - I'm not quite sure - but I do think I have at least some sense of its meaning). I know (I think!) that... Here is my question (it's been bugging me incessantly for the last couple weeks): What does mean? Does it mean the derivative of with respect to x ? Or, rather, does it mean the derivative of with respect to x , evaluated at the point x ? Or something totally different? To make my question more clear, let me ask this too: What does mean? Does it mean the derivative with respect to ? Or the derivative with respect to evaluated at some point ? (so that the ""with respect to x "" is actually ""encoded"" in the part of the notation!). Etc. Even worse, take , which appears in some derivative computation rules. Does that mean derivative with respect to ?? I suspect this confusion may be somewhat related to the maddeningly persistent confusion -- both for me and virtually everyone else -- between a function and its value at a point . Also, teachers tend to use somewhat imprecise notation and language, so I, being someone who likes precision, can sometimes get confused. While I'm at it, I'll note that this confusion may be related to my additional confusion over language like ""the derivative of the sum of two functions,"" where the two ""functions"" are, say, and . But I thought those were mere polynomials , not functions. One might say , but one would never say the function itself , , was equal to , right? I'm confused. We shouldn't say things like ""the function ,"" right? :(",d/dx \frac{d}{dx} f(x)=f'(x) f'(x) f f f'(a) a x a f' f'(ax) ax x^2 x^3 f(x)=x^2 f x^2 x^2,"['calculus', 'derivatives', 'notation']"
80,Usage of mean value theorem ; bounded derivative and open interval,Usage of mean value theorem ; bounded derivative and open interval,,"Let $f : (0,1) \to \mathbb{R}$ be a function such that $ |f'(x)| \leq 5 $ on the open interval $(0,1)$. Prove that $\lim_{x \to 1^-} f(x)$ exists. It involves the derivative and the actual function itself, so I think I have to somehow use the mean value theorem.. Also, $f$ is continuous on $(0,1)$ and differentiable on $(0,1)$ ( because the derivative exists there ). But then, the function is defined on the open interval, so the requirements for the mean value theorem aren't satisfied. I'm guessing we have to consider intervals of the form $(a,b)$ with $a > 0$ and $b < 0$. Lastly, I don't see the significance of the $5$ ... Is it only there to establish that the derivative is bounded, or does the number itself have some signifiance ( would the same thing hold if we had $3$ for example? ). Please give me a hint, not the solution. Something like ""consider the mean value theorem on intervals of the form ... "" would be very helpful.","Let $f : (0,1) \to \mathbb{R}$ be a function such that $ |f'(x)| \leq 5 $ on the open interval $(0,1)$. Prove that $\lim_{x \to 1^-} f(x)$ exists. It involves the derivative and the actual function itself, so I think I have to somehow use the mean value theorem.. Also, $f$ is continuous on $(0,1)$ and differentiable on $(0,1)$ ( because the derivative exists there ). But then, the function is defined on the open interval, so the requirements for the mean value theorem aren't satisfied. I'm guessing we have to consider intervals of the form $(a,b)$ with $a > 0$ and $b < 0$. Lastly, I don't see the significance of the $5$ ... Is it only there to establish that the derivative is bounded, or does the number itself have some signifiance ( would the same thing hold if we had $3$ for example? ). Please give me a hint, not the solution. Something like ""consider the mean value theorem on intervals of the form ... "" would be very helpful.",,"['calculus', 'real-analysis', 'derivatives']"
81,How to derive the sensitivity of the roots of a polynomial to its coefficients?,How to derive the sensitivity of the roots of a polynomial to its coefficients?,,"I have a cubic polynomial $$f(x)=a_o x^3+a_1x^2+a_2x+a_0=(x-x_0)(x-x_1)(x-x_2)$$ but no closed form for its roots, i.e., I do not have closed form expressions for $x_1$, $x_2$ and $x_3$ in terms of the coefficients $a_0$ to $a_3$. However is there a means to derive the sensitivity of the roots with respect to the coefficients, i.e., $\frac{\partial x_1}{\partial a_1}$, etc?","I have a cubic polynomial $$f(x)=a_o x^3+a_1x^2+a_2x+a_0=(x-x_0)(x-x_1)(x-x_2)$$ but no closed form for its roots, i.e., I do not have closed form expressions for $x_1$, $x_2$ and $x_3$ in terms of the coefficients $a_0$ to $a_3$. However is there a means to derive the sensitivity of the roots with respect to the coefficients, i.e., $\frac{\partial x_1}{\partial a_1}$, etc?",,"['derivatives', 'polynomials']"
82,Why do we take positive square roots when proving the derivatives of the trig functions?,Why do we take positive square roots when proving the derivatives of the trig functions?,,"The proofs of the derivatives of the trig functions typically use the identity $$\cos^2(x) + \sin^2(x) = 1$$ but I usually see something like this $$\sin^2(x) = 1 - \cos^2(x)$$ $$\implies \sin(x) = \sqrt{1-\cos^2(x)}$$ to prove, say, the derivative of $\cos^{-1}(x)$. Why do we take the positive square root of $\sin(x)$? I'm sure I'm missing something really obvious. (For one thing, the RHS must be non negative ... ) Thanks.","The proofs of the derivatives of the trig functions typically use the identity $$\cos^2(x) + \sin^2(x) = 1$$ but I usually see something like this $$\sin^2(x) = 1 - \cos^2(x)$$ $$\implies \sin(x) = \sqrt{1-\cos^2(x)}$$ to prove, say, the derivative of $\cos^{-1}(x)$. Why do we take the positive square root of $\sin(x)$? I'm sure I'm missing something really obvious. (For one thing, the RHS must be non negative ... ) Thanks.",,"['calculus', 'trigonometry', 'derivatives']"
83,Higher dimensional total derivative,Higher dimensional total derivative,,"I'm teaching myself some higher dimensional calculus, and I am currently stuck on the definition of the total derivative. The book I'm using is Principles of Mathematical Analysis by Walter Rudin, and he defines the derivative as Suppose $E$ is an open set in $\mathbb{R}^n$, $f$ maps $E$ into   $\mathbb{R}^m$, and $x \in E$. If there exists a linear transformation   $A$ of $\mathbb{R}^n$ into $\mathbb{R}^m$ such that   $$ \lim_{h \to 0} \frac{|f(x + h) - f(x) - Ah|}{|h|} = 0, $$ then we say that $f$ is differentiable at $x$ and we write $$ f'(x) = A. $$ My confusion stems from the $Ah$ term for two reasons: other texts define this term as $L(v)$ for some linear function $L$, and because Rudin also writes that ""since $A \in L(\mathbb{R}^n, \mathbb{R}^m), Ah \in \mathbb{R}^m$"" which both indicate that $A$ is taking $h$ as an argument. This doesn't make sense to me, for suppose that $f$ were to also be linear, then the limit implies that the linear function $A$ is $f$ itself: If we write $A(h)$ to emphasise that $A$ is a function of $h$, then by the linearity of $f$ the numerator of the limit becomes $$ f(x + h) - f(x) - A(h) = f(h) - A(h) $$ so that the limit is zero precisely when $A = f$. However, the derivative of a linear function shouldn't be the same linear function! What is the correct way to understand this definition? And how might I go about finding the derivative of an arbitrary function? For example, what is the derivative of the function $$ g : \mathbb{R}^2 \to \mathbb{R}, g(x, y) = xy $$ according to this definition?","I'm teaching myself some higher dimensional calculus, and I am currently stuck on the definition of the total derivative. The book I'm using is Principles of Mathematical Analysis by Walter Rudin, and he defines the derivative as Suppose $E$ is an open set in $\mathbb{R}^n$, $f$ maps $E$ into   $\mathbb{R}^m$, and $x \in E$. If there exists a linear transformation   $A$ of $\mathbb{R}^n$ into $\mathbb{R}^m$ such that   $$ \lim_{h \to 0} \frac{|f(x + h) - f(x) - Ah|}{|h|} = 0, $$ then we say that $f$ is differentiable at $x$ and we write $$ f'(x) = A. $$ My confusion stems from the $Ah$ term for two reasons: other texts define this term as $L(v)$ for some linear function $L$, and because Rudin also writes that ""since $A \in L(\mathbb{R}^n, \mathbb{R}^m), Ah \in \mathbb{R}^m$"" which both indicate that $A$ is taking $h$ as an argument. This doesn't make sense to me, for suppose that $f$ were to also be linear, then the limit implies that the linear function $A$ is $f$ itself: If we write $A(h)$ to emphasise that $A$ is a function of $h$, then by the linearity of $f$ the numerator of the limit becomes $$ f(x + h) - f(x) - A(h) = f(h) - A(h) $$ so that the limit is zero precisely when $A = f$. However, the derivative of a linear function shouldn't be the same linear function! What is the correct way to understand this definition? And how might I go about finding the derivative of an arbitrary function? For example, what is the derivative of the function $$ g : \mathbb{R}^2 \to \mathbb{R}, g(x, y) = xy $$ according to this definition?",,"['calculus', 'derivatives']"
84,If $f: \mathbb{R}^n\to \mathbb{R}$ has a continuous derivative at a point,If  has a continuous derivative at a point,f: \mathbb{R}^n\to \mathbb{R},"There are a lot of well-known examples for ""continuous at point but not continuous on a neighbourhood"", ""differentiable at a point but not continous on a neighbourhood"" (of course then not differentiable or not continuously differentiable). Is it correct that for a function having a continuous derivative at a point, it must have a continuous derivative on a neighborhood around that point? I can't think of any counter-examples.","There are a lot of well-known examples for ""continuous at point but not continuous on a neighbourhood"", ""differentiable at a point but not continous on a neighbourhood"" (of course then not differentiable or not continuously differentiable). Is it correct that for a function having a continuous derivative at a point, it must have a continuous derivative on a neighborhood around that point? I can't think of any counter-examples.",,"['real-analysis', 'derivatives', 'examples-counterexamples']"
85,evans; estimate on derivatives,evans; estimate on derivatives,,"I have troubles understanding a certain necessity in Evans' proof of a theorem on the estimates for derivatives of harmonic functions. So consider the following, (this is the same theorem as discussed here , however i am asking a different question) Theorem 7 (Estimates on derivatives).  Assume u is harmonic in U. Then   \begin{align} |D^\alpha u(x_0)| \le \frac{C_k}{r^{n+k}} \|u\|_{L^1(B(x_0,r))} \tag{18} \end{align}   for each ball $B(x_0,r) \subseteq U$ and each multiindex $\alpha$ of order $|\alpha| = k$. Here    \begin{align} C_0 = \frac{1}{\alpha(n)}, C_k = \frac{(2^{n+1}nk)^k}{\alpha (n)} \text{ for } k=1,2,\ldots \tag{19} \end{align} Proof. We establish $(\text{18}), (\text{19})$ by induction on $k$, with the case $k=0$ being immediate from the mean value formula $u(x) = \frac{1}{\alpha(n)r^n} \int_{B(x_0,r)} u \, dx = \frac{1}{\alpha(n)r^{n-1}} \int_{\partial B(x,r)} u \, dS$ (which denote average values of $u$ over the ball and sphere, respectively). $$ \, $$ For $k = 1$, we note upon differentiating Laplace's equation that $u_{x_i}$ (for $i=1,...n$) is harmonic. Consequently, \begin{align} \left|u_{x_i}(x_0)\right|  &\le \frac{2n}{r} \|u\|_{L^\infty(\partial B(x_0,\frac{r}{2})} \end{align} Now if $x \in \partial B(x_0,\frac{r}{2})$, then $B(x,\frac{r}{2}) \subseteq B(x_0,r) \subseteq U$, and so \begin{align} |u(x)| \le \frac{1}{\alpha(n)} \left(\frac{2}{r}\right)^n \|u\|_{L^1(B(x_0,r))} \end{align} by (18), (19) for $k=0$. Combining the inequalities above, we get \begin{align}|D^\alpha u(x_0)| &\le \frac{2^{n+1}n}{\alpha(n)} \frac{1}{r^{n+1}} \|u\|_{L^1(B(x_0,r))} \\ &= \frac{2^{n+1}n}{r^{n+1}} \|u\|_{L^1(B(x_0,r))} \\ \end{align} if $|\alpha| = 1$. This verifies $(\text{18})$ and $(\text{19})$ for $k = 1$. $$\,$$ Now in the induction step we proceed analogously: assume $k \geq 2$ and (18), (19) are valid for all balls in $U$ and each multiindex of order less than or equal to $k-1$. Fix $B(x_0,r) \subset U$ and let $\alpha$ be a multiindex with $|\alpha| = k$. Then $D^\alpha u = (D^\beta u)_{x_i}$ for some $i \in \{1, \cdots,n\}$, $|\beta|=k-1$. By calculations similar to those in (20) we establish that $$|D^\alpha u(x)| \leq \frac{n\,k} r \, \|D^\beta u\|_{L^\infty(\partial B(x_0,\frac r k))}.$$ If $x \in \partial B(x_0,\frac r k)$ then $B(x,\frac {k-1} k \, r) \subset B(x_0,r) \subset U$. Thus (18), (19) for $k-1$ imply $$|D^\beta u(x)| \leq \frac{(2^{n+1} \, n \, (k-1) )^{k-1} } {\alpha(n) \, (\frac{k-1} k \, r)^{n+k-1} } \, \|u\|_{L^1(B(x_0,r))}.$$ Now combine the two results to conclude the theorem. Question Why don't we just use $\frac r 2$ (as before) instead of $\frac r k$ in the induction step? What is the use of $\frac r k$? I don't see any reason why $\frac r 2$ wouldn't work. Im very glad for any thoughts on this!","I have troubles understanding a certain necessity in Evans' proof of a theorem on the estimates for derivatives of harmonic functions. So consider the following, (this is the same theorem as discussed here , however i am asking a different question) Theorem 7 (Estimates on derivatives).  Assume u is harmonic in U. Then   \begin{align} |D^\alpha u(x_0)| \le \frac{C_k}{r^{n+k}} \|u\|_{L^1(B(x_0,r))} \tag{18} \end{align}   for each ball $B(x_0,r) \subseteq U$ and each multiindex $\alpha$ of order $|\alpha| = k$. Here    \begin{align} C_0 = \frac{1}{\alpha(n)}, C_k = \frac{(2^{n+1}nk)^k}{\alpha (n)} \text{ for } k=1,2,\ldots \tag{19} \end{align} Proof. We establish $(\text{18}), (\text{19})$ by induction on $k$, with the case $k=0$ being immediate from the mean value formula $u(x) = \frac{1}{\alpha(n)r^n} \int_{B(x_0,r)} u \, dx = \frac{1}{\alpha(n)r^{n-1}} \int_{\partial B(x,r)} u \, dS$ (which denote average values of $u$ over the ball and sphere, respectively). $$ \, $$ For $k = 1$, we note upon differentiating Laplace's equation that $u_{x_i}$ (for $i=1,...n$) is harmonic. Consequently, \begin{align} \left|u_{x_i}(x_0)\right|  &\le \frac{2n}{r} \|u\|_{L^\infty(\partial B(x_0,\frac{r}{2})} \end{align} Now if $x \in \partial B(x_0,\frac{r}{2})$, then $B(x,\frac{r}{2}) \subseteq B(x_0,r) \subseteq U$, and so \begin{align} |u(x)| \le \frac{1}{\alpha(n)} \left(\frac{2}{r}\right)^n \|u\|_{L^1(B(x_0,r))} \end{align} by (18), (19) for $k=0$. Combining the inequalities above, we get \begin{align}|D^\alpha u(x_0)| &\le \frac{2^{n+1}n}{\alpha(n)} \frac{1}{r^{n+1}} \|u\|_{L^1(B(x_0,r))} \\ &= \frac{2^{n+1}n}{r^{n+1}} \|u\|_{L^1(B(x_0,r))} \\ \end{align} if $|\alpha| = 1$. This verifies $(\text{18})$ and $(\text{19})$ for $k = 1$. $$\,$$ Now in the induction step we proceed analogously: assume $k \geq 2$ and (18), (19) are valid for all balls in $U$ and each multiindex of order less than or equal to $k-1$. Fix $B(x_0,r) \subset U$ and let $\alpha$ be a multiindex with $|\alpha| = k$. Then $D^\alpha u = (D^\beta u)_{x_i}$ for some $i \in \{1, \cdots,n\}$, $|\beta|=k-1$. By calculations similar to those in (20) we establish that $$|D^\alpha u(x)| \leq \frac{n\,k} r \, \|D^\beta u\|_{L^\infty(\partial B(x_0,\frac r k))}.$$ If $x \in \partial B(x_0,\frac r k)$ then $B(x,\frac {k-1} k \, r) \subset B(x_0,r) \subset U$. Thus (18), (19) for $k-1$ imply $$|D^\beta u(x)| \leq \frac{(2^{n+1} \, n \, (k-1) )^{k-1} } {\alpha(n) \, (\frac{k-1} k \, r)^{n+k-1} } \, \|u\|_{L^1(B(x_0,r))}.$$ Now combine the two results to conclude the theorem. Question Why don't we just use $\frac r 2$ (as before) instead of $\frac r k$ in the induction step? What is the use of $\frac r k$? I don't see any reason why $\frac r 2$ wouldn't work. Im very glad for any thoughts on this!",,"['real-analysis', 'derivatives', 'partial-differential-equations', 'partial-derivative']"
86,Is it logically correct to differentiate and check continuity of derivative without knowing before hand if the function is differentiable or not?,Is it logically correct to differentiate and check continuity of derivative without knowing before hand if the function is differentiable or not?,,"Suppose I want to check if a certain function is differentiable or not in a certain domain of $x$. For example, if $$f(x)=\int_{0}^{x} t \sin(\frac{1}{t})dt$$ in $(0,π)$ then is it okay to directly differentiate $f(x)$ and check if $f′(x)$ is continuous? Or it would be wrong to differentiate using Leibniz rule without knowing if $f(x)$ is differentiable ?","Suppose I want to check if a certain function is differentiable or not in a certain domain of $x$. For example, if $$f(x)=\int_{0}^{x} t \sin(\frac{1}{t})dt$$ in $(0,π)$ then is it okay to directly differentiate $f(x)$ and check if $f′(x)$ is continuous? Or it would be wrong to differentiate using Leibniz rule without knowing if $f(x)$ is differentiable ?",,['calculus']
87,"What's wrong with this ""proof"" that $\frac {de^x}{dx} \neq e^x$?","What's wrong with this ""proof"" that ?",\frac {de^x}{dx} \neq e^x,"What's wrong with this ""proof"" that $\frac {de^x}{dx} \neq e^x$? $$e^x = \sum _{n=0}^\infty \frac{1}{n!}x^n $$ Therefore  $$\frac{d e ^x}{d x}= \sum _{n=0}^\infty \frac{n}{n!}x^{n-1}=  \frac{1}{(-1)!}x^{-1}+\sum _{n=0}^\infty \frac{1}{n!}x^n\neq e^x$$ The mistake must be somewhere in the fact that I pull the derivative operator ""through"" the sum operator. That is possible if the sum is finite, but given that it's infinite it must be invalid, but why?","What's wrong with this ""proof"" that $\frac {de^x}{dx} \neq e^x$? $$e^x = \sum _{n=0}^\infty \frac{1}{n!}x^n $$ Therefore  $$\frac{d e ^x}{d x}= \sum _{n=0}^\infty \frac{n}{n!}x^{n-1}=  \frac{1}{(-1)!}x^{-1}+\sum _{n=0}^\infty \frac{1}{n!}x^n\neq e^x$$ The mistake must be somewhere in the fact that I pull the derivative operator ""through"" the sum operator. That is possible if the sum is finite, but given that it's infinite it must be invalid, but why?",,"['derivatives', 'exponential-function', 'fake-proofs']"
88,n-th derivative where $n$ is a real number?,n-th derivative where  is a real number?,n,"We know that  $$\frac{d^n}{dt^n} e^{at}= a^n e^{at}; \, n\in \mathbb N.$$ I want to know if the result is true if $n$ is a real number, i.e., $n\in \mathbb R$ ?","We know that  $$\frac{d^n}{dt^n} e^{at}= a^n e^{at}; \, n\in \mathbb N.$$ I want to know if the result is true if $n$ is a real number, i.e., $n\in \mathbb R$ ?",,"['real-analysis', 'derivatives', 'fractional-calculus']"
89,Is $f(x)=\frac{\sin x}{x}$ for all $x\neq 0$ differentiable?,Is  for all  differentiable?,f(x)=\frac{\sin x}{x} x\neq 0,"The function $f(x)$ is defined by $$f(x)=\frac{\sin x}{x}$$ for any $x≠0$. For $x=0$, $f(x)=1$. My work: Determine if the function is continuous, differentiable and if the latter, find its derivative at $0$. $$f(x) =\begin{cases}\dfrac{\sin x}{x}, & x \ne 0 \\       1 & x = 0 \end{cases}$$ I proved the continuous condition using L'Hopital's rule on the following \begin{equation} f(0) = \lim_{x\to 0} \frac{\sin x}x = 1 \end{equation} For the defferentiable condition I think I proved it \begin{align*} \lim_{x\to0} \frac{f(x) - f(0)}{x-0} &= \lim_{x\to0} \frac{\frac{\sin x}x - 1}{x-0} \\ &= \lim_{x\to0} \frac{\sin x - x}{x^2} \\ &= \lim_{x\to0} \frac{\cos x-1}{2x} \\ &= \lim_{x\to0} \frac{-\sin x}{2} \\ &= 0  \end{align*} Now the derivative of $f(x)$ is \begin{equation} \frac{x\cos x - \sin x}{x^2} \end{equation} But what does it mean ""find its derivative at $0$"" ? The only thing that came to my mind is to find its limit as $x\to 0$ \begin{equation} \lim_{x\to0}\frac{x\cos x - \sin x}{x^2} = 0 \end{equation} Did I understand and do everything correctly?","The function $f(x)$ is defined by $$f(x)=\frac{\sin x}{x}$$ for any $x≠0$. For $x=0$, $f(x)=1$. My work: Determine if the function is continuous, differentiable and if the latter, find its derivative at $0$. $$f(x) =\begin{cases}\dfrac{\sin x}{x}, & x \ne 0 \\       1 & x = 0 \end{cases}$$ I proved the continuous condition using L'Hopital's rule on the following \begin{equation} f(0) = \lim_{x\to 0} \frac{\sin x}x = 1 \end{equation} For the defferentiable condition I think I proved it \begin{align*} \lim_{x\to0} \frac{f(x) - f(0)}{x-0} &= \lim_{x\to0} \frac{\frac{\sin x}x - 1}{x-0} \\ &= \lim_{x\to0} \frac{\sin x - x}{x^2} \\ &= \lim_{x\to0} \frac{\cos x-1}{2x} \\ &= \lim_{x\to0} \frac{-\sin x}{2} \\ &= 0  \end{align*} Now the derivative of $f(x)$ is \begin{equation} \frac{x\cos x - \sin x}{x^2} \end{equation} But what does it mean ""find its derivative at $0$"" ? The only thing that came to my mind is to find its limit as $x\to 0$ \begin{equation} \lim_{x\to0}\frac{x\cos x - \sin x}{x^2} = 0 \end{equation} Did I understand and do everything correctly?",,"['calculus', 'derivatives', 'continuity']"
90,Does the existence of the derivative at a point imply the existence of the left and right derivative?,Does the existence of the derivative at a point imply the existence of the left and right derivative?,,"I'm asking this because I've seen ""results"" in the internet that state: A function $f$ is differentiable at $x = a$ if and only if both the right-hand derivative and left-hand derivative at $x = a$ exist and both of these derivatives are equal. And the counter implication appears in passing in my notes: If a function $f$ is differentiable at $x = a$ then both the right-hand derivative and left-hand derivative at $x = a$ exist and both of these derivatives are equal. However, the function $x^2 \sin \frac{1}{x}$ is differentiable in $\mathbb{R}$ even thought the lateral derivatives don't exist at $0$. This example appears in my notes, which is distressing because (unless I'm committing a logical fallacy) then it's not true that the existence of the derivative at a point imply the existence of the left and right derivative at that point (which would not be consistent with the previous statement).","I'm asking this because I've seen ""results"" in the internet that state: A function $f$ is differentiable at $x = a$ if and only if both the right-hand derivative and left-hand derivative at $x = a$ exist and both of these derivatives are equal. And the counter implication appears in passing in my notes: If a function $f$ is differentiable at $x = a$ then both the right-hand derivative and left-hand derivative at $x = a$ exist and both of these derivatives are equal. However, the function $x^2 \sin \frac{1}{x}$ is differentiable in $\mathbb{R}$ even thought the lateral derivatives don't exist at $0$. This example appears in my notes, which is distressing because (unless I'm committing a logical fallacy) then it's not true that the existence of the derivative at a point imply the existence of the left and right derivative at that point (which would not be consistent with the previous statement).",,"['calculus', 'real-analysis', 'derivatives']"
91,Derivative of $f(x) = \frac{\cos{(x^2 - 1)}}{2x}$,Derivative of,f(x) = \frac{\cos{(x^2 - 1)}}{2x},Find the derivative of the function $$f(x) = \frac{\cos{(x^2 - 1)}}{2x}$$ This is my step-by-step solution: $$f'(x) = \frac{-\sin{(x^2 - 1)}2x - 2\cos{(x^2 -1)}}{4x^2} = \frac{2x\sin{(1 - x^2)} - 2\cos{(1 - x^2)}}{4x^2} = \frac{x\sin{(1 - x^2)} - \cos{(1 - x^2)}}{2x^2} = \frac{\sin{(1 - x^2)}}{2x} - \frac{\cos{(1-x^2)}}{2x^2}$$ and this is the output of WolphramAlpha: http://www.wolframalpha.com/input/?i=derivative+cos(x^2+-+1)%2F(2x) Where is the mistakes?,Find the derivative of the function $$f(x) = \frac{\cos{(x^2 - 1)}}{2x}$$ This is my step-by-step solution: $$f'(x) = \frac{-\sin{(x^2 - 1)}2x - 2\cos{(x^2 -1)}}{4x^2} = \frac{2x\sin{(1 - x^2)} - 2\cos{(1 - x^2)}}{4x^2} = \frac{x\sin{(1 - x^2)} - \cos{(1 - x^2)}}{2x^2} = \frac{\sin{(1 - x^2)}}{2x} - \frac{\cos{(1-x^2)}}{2x^2}$$ and this is the output of WolphramAlpha: http://www.wolframalpha.com/input/?i=derivative+cos(x^2+-+1)%2F(2x) Where is the mistakes?,,['calculus']
92,Problem with real differentiable function involving both Mean Value Theorem and Intermediate Value Theorem,Problem with real differentiable function involving both Mean Value Theorem and Intermediate Value Theorem,,"Problem: Let $a,b \in \Bbb R$, $a<b$, and let $f$ be a differentiable real-valued function on an open subset of $\Bbb R$ that contains $[a,b]$. Show that if $\gamma$ is any real number between $f'(a)$ and $f'(b)$ then there exists a number $c \in (a,b)$ such that $\gamma = f'(c)$ . So I was trying to use the Mean Value Theorem and the Intermediate Value Theorem for the function $\frac {f(x_1) -f(x_2)}{x_1 - x_2}$ on this set: $\{(x_1,x_2) \in E^2: a \le x_1 < x_2 \le b \}$ but I am stuck how do you go from here (if my thinking is correct).","Problem: Let $a,b \in \Bbb R$, $a<b$, and let $f$ be a differentiable real-valued function on an open subset of $\Bbb R$ that contains $[a,b]$. Show that if $\gamma$ is any real number between $f'(a)$ and $f'(b)$ then there exists a number $c \in (a,b)$ such that $\gamma = f'(c)$ . So I was trying to use the Mean Value Theorem and the Intermediate Value Theorem for the function $\frac {f(x_1) -f(x_2)}{x_1 - x_2}$ on this set: $\{(x_1,x_2) \in E^2: a \le x_1 < x_2 \le b \}$ but I am stuck how do you go from here (if my thinking is correct).",,"['real-analysis', 'derivatives']"
93,Regarding the derivative of the $j$-invariant,Regarding the derivative of the -invariant,j,"Is anyone aware of a formula for the derivative of the $j$-invariant $j(\tau)$ with respect to $\tau$? Here, $\tau$ is in the upper half-plane. I would image there are probably quite a few formulae for $j'(\tau)$, but they are not well-known. I looked through a few books but I could not find a single formula. Any help would be greatly appreciated.","Is anyone aware of a formula for the derivative of the $j$-invariant $j(\tau)$ with respect to $\tau$? Here, $\tau$ is in the upper half-plane. I would image there are probably quite a few formulae for $j'(\tau)$, but they are not well-known. I looked through a few books but I could not find a single formula. Any help would be greatly appreciated.",,"['derivatives', 'modular-forms']"
94,"Prove that if $f$ is differentiable at $x=0$, then $f$ is differentiable on $\mathbb{R}$.","Prove that if  is differentiable at , then  is differentiable on .",f x=0 f \mathbb{R},"$Conj:$ Suppose that a function $f:\mathbb{R}\rightarrow\mathbb{R}$ is differentiable at $x=0$, satisfies $f(a + b) = f(a)f(b)$ for all $a,b,\in\mathbb{R}$, and is not identically zero ($\exists ~x$ such that $f(x) \neq 0$). Then $f$ is differentiable and  $f'(x) = f'(0)f(x)$. I think I need to show that $f(0) = 1$, $f(x)\neq 0$ for all $x$ and that $f$ is continuous if continuous at $f(0)$. Here's what I got so far: $Proof:$ $f$ not identically zero implies that there exists $x \in \mathbb{R}$ such that $f(x) \neq 0$. Then: \begin{align} f(x) &= f(x +a - a) & \text{(where $a \in \mathbb{R}$).}\\ & = f(x)f(a)f(-a) \\ & \neq 0 \end{align} Thus, $f(x)\neq 0$ implies that $f(a)\neq 0$ for all $a \in \mathbb{R}$. Furthermore, \begin{align} f(0) & = f(0 + 0)\\ & = f(0)^2 \end{align} Thus, $f(0)$ must be either 0 or 1. We just showed that $f(x)\neq 0$ for all $x$, so $f(0)=1$. Now I need the continuity piece (I think) but I'm stuck. Thanks in advance!","$Conj:$ Suppose that a function $f:\mathbb{R}\rightarrow\mathbb{R}$ is differentiable at $x=0$, satisfies $f(a + b) = f(a)f(b)$ for all $a,b,\in\mathbb{R}$, and is not identically zero ($\exists ~x$ such that $f(x) \neq 0$). Then $f$ is differentiable and  $f'(x) = f'(0)f(x)$. I think I need to show that $f(0) = 1$, $f(x)\neq 0$ for all $x$ and that $f$ is continuous if continuous at $f(0)$. Here's what I got so far: $Proof:$ $f$ not identically zero implies that there exists $x \in \mathbb{R}$ such that $f(x) \neq 0$. Then: \begin{align} f(x) &= f(x +a - a) & \text{(where $a \in \mathbb{R}$).}\\ & = f(x)f(a)f(-a) \\ & \neq 0 \end{align} Thus, $f(x)\neq 0$ implies that $f(a)\neq 0$ for all $a \in \mathbb{R}$. Furthermore, \begin{align} f(0) & = f(0 + 0)\\ & = f(0)^2 \end{align} Thus, $f(0)$ must be either 0 or 1. We just showed that $f(x)\neq 0$ for all $x$, so $f(0)=1$. Now I need the continuity piece (I think) but I'm stuck. Thanks in advance!",,"['real-analysis', 'derivatives']"
95,Generalized power rule for derivatives,Generalized power rule for derivatives,,"Background This background is not really necessary to answer my question, but I included it here to provide context. This question has some programming aspects to it as well, but since my question is mainly about math, I decided to ask it here. I'm trying to extend the implementation of automatic differentiation found here . This implementation, assuming I read it properly, does not work for functions of the form $F(x)=f(x)^{g(x)}$. I'm trying to modify it so that it does work for such functions. Question I'm trying to find a derivative for functions of the form $F(x)=f(x)^{g(x)}$. I specifically only care about the ""normal"" cases where $g(x)$ is an integral constant, or $f(x)$ is positive. Wikipedia has provided me with a ""Generalized Power Rule"": $$(f^g)^\prime = f^g\left(f^\prime\frac{g}{f}+g^\prime\ln f\right)$$ The generalized rule however does not work for $f\leq 0$. In my implementation it is difficult to tell which of the two cases I'm working with, so I would rather not need to implement this generalized rule for the latter case, and the basic power rule for the former. Is there a rule that works for both cases?","Background This background is not really necessary to answer my question, but I included it here to provide context. This question has some programming aspects to it as well, but since my question is mainly about math, I decided to ask it here. I'm trying to extend the implementation of automatic differentiation found here . This implementation, assuming I read it properly, does not work for functions of the form $F(x)=f(x)^{g(x)}$. I'm trying to modify it so that it does work for such functions. Question I'm trying to find a derivative for functions of the form $F(x)=f(x)^{g(x)}$. I specifically only care about the ""normal"" cases where $g(x)$ is an integral constant, or $f(x)$ is positive. Wikipedia has provided me with a ""Generalized Power Rule"": $$(f^g)^\prime = f^g\left(f^\prime\frac{g}{f}+g^\prime\ln f\right)$$ The generalized rule however does not work for $f\leq 0$. In my implementation it is difficult to tell which of the two cases I'm working with, so I would rather not need to implement this generalized rule for the latter case, and the basic power rule for the former. Is there a rule that works for both cases?",,"['derivatives', 'exponentiation']"
96,"If $f$ is increasing on an open interval and continuous at endpoints, it's increasing on the closed interval.","If  is increasing on an open interval and continuous at endpoints, it's increasing on the closed interval.",f,"Prove that if $f$ is increasing on $(a,b)$ and continuous at $a$ and $b$, then $f$ is increasing on $[a,b]$. The question then clarifies: ""In particular, if $f$ is continuous on $[a,b]$ and $f'>0$ on $(a,b)$ then $f$ is increasing on $[a,b]$. An idea I had was to consider $\varepsilon > 0$, and to note that $f$ is increasing on $[a+\varepsilon, b-\varepsilon]$. Then, since $\lim_{x \to a}f(x)=f(a)$ and $\lim_{x \to b}f(x)=f(b)$, we can get some contradiction that it's strictly increasing by using the intermedate value theorem for some $c$ s.t. if $a<c<b \Rightarrow f(a)<f(c)<f(b)$. Sorry if this attempt is confusing.","Prove that if $f$ is increasing on $(a,b)$ and continuous at $a$ and $b$, then $f$ is increasing on $[a,b]$. The question then clarifies: ""In particular, if $f$ is continuous on $[a,b]$ and $f'>0$ on $(a,b)$ then $f$ is increasing on $[a,b]$. An idea I had was to consider $\varepsilon > 0$, and to note that $f$ is increasing on $[a+\varepsilon, b-\varepsilon]$. Then, since $\lim_{x \to a}f(x)=f(a)$ and $\lim_{x \to b}f(x)=f(b)$, we can get some contradiction that it's strictly increasing by using the intermedate value theorem for some $c$ s.t. if $a<c<b \Rightarrow f(a)<f(c)<f(b)$. Sorry if this attempt is confusing.",,"['calculus', 'real-analysis', 'derivatives']"
97,"Show that there is $\eta\in(0,1)$ with $f''(\eta)=f(\eta)$",Show that there is  with,"\eta\in(0,1) f''(\eta)=f(\eta)","Suppose $f:[0,1]\to\mathbb{R}$ is twice differentiable with $$\lim_{x\to0^+}\frac{f(x)}{x}=1,\lim_{x\to 1^-}\frac{f(x)}{x-1}=2$$ Show that there is $\eta\in(0,1)$ s.t. $f''(\eta)=f(\eta)$. It is equivalent to prove that $f''(x)-f(x)$ has a root in $(0,1)$. Then I tried to construct function. First I noted that the ODE $$y''-y=0$$ has solution of the form $\alpha e^x+\beta e^{-x}$, then I consider the following function : $$(f-f')e^{x}$$ and apply mean value theorem to is. But thing does not behave that nice, and I was stuck.","Suppose $f:[0,1]\to\mathbb{R}$ is twice differentiable with $$\lim_{x\to0^+}\frac{f(x)}{x}=1,\lim_{x\to 1^-}\frac{f(x)}{x-1}=2$$ Show that there is $\eta\in(0,1)$ s.t. $f''(\eta)=f(\eta)$. It is equivalent to prove that $f''(x)-f(x)$ has a root in $(0,1)$. Then I tried to construct function. First I noted that the ODE $$y''-y=0$$ has solution of the form $\alpha e^x+\beta e^{-x}$, then I consider the following function : $$(f-f')e^{x}$$ and apply mean value theorem to is. But thing does not behave that nice, and I was stuck.",,"['calculus', 'real-analysis', 'derivatives']"
98,Why do we write second derivatives like $\frac{d^2x}{dt^2}$ [duplicate],Why do we write second derivatives like  [duplicate],\frac{d^2x}{dt^2},"This question already has answers here : What is the use of, and intuition behind, writing $\frac{d^2}{dx^2}$ for the second derivative? (4 answers) Closed 11 years ago . Why do we write the second derivative of $x$ with respect to $t$ as $\frac{d^2x}{dt^2}$? It's never been explained to me, and I've never found a particularly good explanation. What's up with this weird derivative notation?","This question already has answers here : What is the use of, and intuition behind, writing $\frac{d^2}{dx^2}$ for the second derivative? (4 answers) Closed 11 years ago . Why do we write the second derivative of $x$ with respect to $t$ as $\frac{d^2x}{dt^2}$? It's never been explained to me, and I've never found a particularly good explanation. What's up with this weird derivative notation?",,"['notation', 'derivatives']"
99,The derivative of $e^x$,The derivative of,e^x,We all know that the derivative of $e^x$ equals $e^x$. I found this notation on Wikipedia: $$\dfrac{d}{dx} e^x = e^x$$ Why isn't this expression $$\dfrac{dy}{dx} e^x = e^x$$ Is it because $$\dfrac{d}{dx} e^x = \dfrac{d \space e^x}{dx}  $$ I'm asking this because I have only encountered $\dfrac{dy}{dx}$ when differentiating (mainly because I always use the $f'(x)$ method.,We all know that the derivative of $e^x$ equals $e^x$. I found this notation on Wikipedia: $$\dfrac{d}{dx} e^x = e^x$$ Why isn't this expression $$\dfrac{dy}{dx} e^x = e^x$$ Is it because $$\dfrac{d}{dx} e^x = \dfrac{d \space e^x}{dx}  $$ I'm asking this because I have only encountered $\dfrac{dy}{dx}$ when differentiating (mainly because I always use the $f'(x)$ method.,,"['notation', 'derivatives']"
